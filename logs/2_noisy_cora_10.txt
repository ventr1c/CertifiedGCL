Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11648])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10568])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.92522430419922 = 1.9565902948379517 + 10.0 * 8.596863746643066
Epoch 0, val loss: 1.9608893394470215
Epoch 10, training loss: 87.90937805175781 = 1.9455745220184326 + 10.0 * 8.596380233764648
Epoch 10, val loss: 1.949275016784668
Epoch 20, training loss: 87.85366821289062 = 1.9317442178726196 + 10.0 * 8.592191696166992
Epoch 20, val loss: 1.9344764947891235
Epoch 30, training loss: 87.54806518554688 = 1.9133185148239136 + 10.0 * 8.563474655151367
Epoch 30, val loss: 1.9148175716400146
Epoch 40, training loss: 86.14241027832031 = 1.8914048671722412 + 10.0 * 8.425100326538086
Epoch 40, val loss: 1.8928420543670654
Epoch 50, training loss: 82.16185760498047 = 1.8681416511535645 + 10.0 * 8.02937126159668
Epoch 50, val loss: 1.86998450756073
Epoch 60, training loss: 79.19100189208984 = 1.847968578338623 + 10.0 * 7.7343034744262695
Epoch 60, val loss: 1.8516656160354614
Epoch 70, training loss: 75.83292388916016 = 1.8350892066955566 + 10.0 * 7.399783134460449
Epoch 70, val loss: 1.8403550386428833
Epoch 80, training loss: 73.66316223144531 = 1.8260079622268677 + 10.0 * 7.183714866638184
Epoch 80, val loss: 1.8320749998092651
Epoch 90, training loss: 71.45285034179688 = 1.818796992301941 + 10.0 * 6.963405609130859
Epoch 90, val loss: 1.8252975940704346
Epoch 100, training loss: 70.177001953125 = 1.8119428157806396 + 10.0 * 6.836505889892578
Epoch 100, val loss: 1.8184465169906616
Epoch 110, training loss: 69.19649505615234 = 1.803789496421814 + 10.0 * 6.7392706871032715
Epoch 110, val loss: 1.8108841180801392
Epoch 120, training loss: 68.57919311523438 = 1.7956143617630005 + 10.0 * 6.6783576011657715
Epoch 120, val loss: 1.8034780025482178
Epoch 130, training loss: 68.15779113769531 = 1.7870124578475952 + 10.0 * 6.637077808380127
Epoch 130, val loss: 1.7961097955703735
Epoch 140, training loss: 67.8038558959961 = 1.77788245677948 + 10.0 * 6.602596759796143
Epoch 140, val loss: 1.7884998321533203
Epoch 150, training loss: 67.51567840576172 = 1.768280267715454 + 10.0 * 6.574739933013916
Epoch 150, val loss: 1.7804878950119019
Epoch 160, training loss: 67.22069549560547 = 1.7581453323364258 + 10.0 * 6.546254634857178
Epoch 160, val loss: 1.7721253633499146
Epoch 170, training loss: 66.98210144042969 = 1.747543454170227 + 10.0 * 6.523455619812012
Epoch 170, val loss: 1.7636035680770874
Epoch 180, training loss: 66.74365997314453 = 1.7361572980880737 + 10.0 * 6.5007500648498535
Epoch 180, val loss: 1.7545009851455688
Epoch 190, training loss: 66.54209899902344 = 1.7236748933792114 + 10.0 * 6.481842041015625
Epoch 190, val loss: 1.744629979133606
Epoch 200, training loss: 66.40706634521484 = 1.7099668979644775 + 10.0 * 6.469709873199463
Epoch 200, val loss: 1.7337554693222046
Epoch 210, training loss: 66.2425308227539 = 1.6949536800384521 + 10.0 * 6.4547576904296875
Epoch 210, val loss: 1.72188138961792
Epoch 220, training loss: 66.10983276367188 = 1.678865671157837 + 10.0 * 6.44309663772583
Epoch 220, val loss: 1.7091312408447266
Epoch 230, training loss: 65.98794555664062 = 1.661576747894287 + 10.0 * 6.4326372146606445
Epoch 230, val loss: 1.6955894231796265
Epoch 240, training loss: 65.88263702392578 = 1.6432214975357056 + 10.0 * 6.4239420890808105
Epoch 240, val loss: 1.6811928749084473
Epoch 250, training loss: 65.77053833007812 = 1.6238150596618652 + 10.0 * 6.414672374725342
Epoch 250, val loss: 1.666090726852417
Epoch 260, training loss: 65.6978759765625 = 1.6034700870513916 + 10.0 * 6.409440517425537
Epoch 260, val loss: 1.6504219770431519
Epoch 270, training loss: 65.59970092773438 = 1.5822384357452393 + 10.0 * 6.4017462730407715
Epoch 270, val loss: 1.6341158151626587
Epoch 280, training loss: 65.49519348144531 = 1.5605125427246094 + 10.0 * 6.393467903137207
Epoch 280, val loss: 1.6176469326019287
Epoch 290, training loss: 65.42111206054688 = 1.5382758378982544 + 10.0 * 6.388283729553223
Epoch 290, val loss: 1.60105562210083
Epoch 300, training loss: 65.3783187866211 = 1.5156737565994263 + 10.0 * 6.386264324188232
Epoch 300, val loss: 1.5844221115112305
Epoch 310, training loss: 65.27173614501953 = 1.4930393695831299 + 10.0 * 6.377869606018066
Epoch 310, val loss: 1.567999005317688
Epoch 320, training loss: 65.19058227539062 = 1.4704965353012085 + 10.0 * 6.372008800506592
Epoch 320, val loss: 1.55209219455719
Epoch 330, training loss: 65.12097930908203 = 1.4481099843978882 + 10.0 * 6.367286682128906
Epoch 330, val loss: 1.536535382270813
Epoch 340, training loss: 65.07740020751953 = 1.4258419275283813 + 10.0 * 6.365156173706055
Epoch 340, val loss: 1.5214877128601074
Epoch 350, training loss: 64.99187469482422 = 1.4039095640182495 + 10.0 * 6.3587965965271
Epoch 350, val loss: 1.5068144798278809
Epoch 360, training loss: 64.92051696777344 = 1.3823773860931396 + 10.0 * 6.353814125061035
Epoch 360, val loss: 1.4930256605148315
Epoch 370, training loss: 64.86788177490234 = 1.36114501953125 + 10.0 * 6.350673675537109
Epoch 370, val loss: 1.4796595573425293
Epoch 380, training loss: 64.80022430419922 = 1.3400962352752686 + 10.0 * 6.346013069152832
Epoch 380, val loss: 1.466888189315796
Epoch 390, training loss: 64.75439453125 = 1.3193169832229614 + 10.0 * 6.343507766723633
Epoch 390, val loss: 1.4542276859283447
Epoch 400, training loss: 64.67498016357422 = 1.2986586093902588 + 10.0 * 6.337632179260254
Epoch 400, val loss: 1.4421212673187256
Epoch 410, training loss: 64.62613677978516 = 1.278059482574463 + 10.0 * 6.334807872772217
Epoch 410, val loss: 1.4301320314407349
Epoch 420, training loss: 64.57583618164062 = 1.2574008703231812 + 10.0 * 6.331843376159668
Epoch 420, val loss: 1.4182795286178589
Epoch 430, training loss: 64.52873992919922 = 1.23671555519104 + 10.0 * 6.329202175140381
Epoch 430, val loss: 1.4067240953445435
Epoch 440, training loss: 64.51188659667969 = 1.2158229351043701 + 10.0 * 6.329606533050537
Epoch 440, val loss: 1.394562840461731
Epoch 450, training loss: 64.41462707519531 = 1.194770097732544 + 10.0 * 6.321985721588135
Epoch 450, val loss: 1.3828386068344116
Epoch 460, training loss: 64.35725402832031 = 1.1737020015716553 + 10.0 * 6.318355083465576
Epoch 460, val loss: 1.3710631132125854
Epoch 470, training loss: 64.3258285522461 = 1.1524786949157715 + 10.0 * 6.31733512878418
Epoch 470, val loss: 1.3592561483383179
Epoch 480, training loss: 64.28318786621094 = 1.1309452056884766 + 10.0 * 6.3152241706848145
Epoch 480, val loss: 1.3469140529632568
Epoch 490, training loss: 64.22374725341797 = 1.1094833612442017 + 10.0 * 6.311426639556885
Epoch 490, val loss: 1.3348387479782104
Epoch 500, training loss: 64.1808090209961 = 1.087976336479187 + 10.0 * 6.309283256530762
Epoch 500, val loss: 1.3225829601287842
Epoch 510, training loss: 64.17253875732422 = 1.0664607286453247 + 10.0 * 6.310607433319092
Epoch 510, val loss: 1.310198187828064
Epoch 520, training loss: 64.11190032958984 = 1.0449990034103394 + 10.0 * 6.306690216064453
Epoch 520, val loss: 1.2980856895446777
Epoch 530, training loss: 64.09095001220703 = 1.0236097574234009 + 10.0 * 6.306734085083008
Epoch 530, val loss: 1.286085844039917
Epoch 540, training loss: 64.02433776855469 = 1.0024181604385376 + 10.0 * 6.302191734313965
Epoch 540, val loss: 1.2738679647445679
Epoch 550, training loss: 63.97533416748047 = 0.9815362691879272 + 10.0 * 6.299379825592041
Epoch 550, val loss: 1.2620211839675903
Epoch 560, training loss: 63.935726165771484 = 0.9609652757644653 + 10.0 * 6.297476291656494
Epoch 560, val loss: 1.2505159378051758
Epoch 570, training loss: 63.91862106323242 = 0.9407092332839966 + 10.0 * 6.297791481018066
Epoch 570, val loss: 1.2391024827957153
Epoch 580, training loss: 63.88404846191406 = 0.9208354949951172 + 10.0 * 6.296320915222168
Epoch 580, val loss: 1.228304147720337
Epoch 590, training loss: 63.85490417480469 = 0.9013661742210388 + 10.0 * 6.295353889465332
Epoch 590, val loss: 1.217926263809204
Epoch 600, training loss: 63.79806900024414 = 0.8824506998062134 + 10.0 * 6.291562080383301
Epoch 600, val loss: 1.2078856229782104
Epoch 610, training loss: 63.76679229736328 = 0.864020824432373 + 10.0 * 6.290277004241943
Epoch 610, val loss: 1.1986116170883179
Epoch 620, training loss: 63.776954650878906 = 0.8460118174552917 + 10.0 * 6.293094158172607
Epoch 620, val loss: 1.189786434173584
Epoch 630, training loss: 63.74966812133789 = 0.8284587264060974 + 10.0 * 6.292120933532715
Epoch 630, val loss: 1.1811381578445435
Epoch 640, training loss: 63.67985153198242 = 0.8112460970878601 + 10.0 * 6.286860466003418
Epoch 640, val loss: 1.1730669736862183
Epoch 650, training loss: 63.64394760131836 = 0.7945376038551331 + 10.0 * 6.28494119644165
Epoch 650, val loss: 1.1656758785247803
Epoch 660, training loss: 63.65287780761719 = 0.7782067060470581 + 10.0 * 6.287467002868652
Epoch 660, val loss: 1.1588798761367798
Epoch 670, training loss: 63.59861755371094 = 0.7620260715484619 + 10.0 * 6.283658981323242
Epoch 670, val loss: 1.151681900024414
Epoch 680, training loss: 63.56242370605469 = 0.7462080717086792 + 10.0 * 6.28162145614624
Epoch 680, val loss: 1.145646572113037
Epoch 690, training loss: 63.53559875488281 = 0.7305964827537537 + 10.0 * 6.280500411987305
Epoch 690, val loss: 1.1396163702011108
Epoch 700, training loss: 63.54994201660156 = 0.7151119709014893 + 10.0 * 6.283483028411865
Epoch 700, val loss: 1.1334927082061768
Epoch 710, training loss: 63.492637634277344 = 0.6996839046478271 + 10.0 * 6.279295444488525
Epoch 710, val loss: 1.1279897689819336
Epoch 720, training loss: 63.44757080078125 = 0.6844321489334106 + 10.0 * 6.276313781738281
Epoch 720, val loss: 1.1221970319747925
Epoch 730, training loss: 63.422523498535156 = 0.6692669987678528 + 10.0 * 6.275325775146484
Epoch 730, val loss: 1.1167422533035278
Epoch 740, training loss: 63.39540481567383 = 0.6541179418563843 + 10.0 * 6.2741289138793945
Epoch 740, val loss: 1.111401915550232
Epoch 750, training loss: 63.4644889831543 = 0.6389237642288208 + 10.0 * 6.282556533813477
Epoch 750, val loss: 1.1061538457870483
Epoch 760, training loss: 63.36029815673828 = 0.6235883235931396 + 10.0 * 6.2736711502075195
Epoch 760, val loss: 1.100595235824585
Epoch 770, training loss: 63.3289909362793 = 0.6082726716995239 + 10.0 * 6.272071838378906
Epoch 770, val loss: 1.095170021057129
Epoch 780, training loss: 63.297508239746094 = 0.5930013060569763 + 10.0 * 6.270450592041016
Epoch 780, val loss: 1.0899882316589355
Epoch 790, training loss: 63.30122375488281 = 0.5776830315589905 + 10.0 * 6.2723541259765625
Epoch 790, val loss: 1.0845880508422852
Epoch 800, training loss: 63.259437561035156 = 0.5622647404670715 + 10.0 * 6.269717216491699
Epoch 800, val loss: 1.0798695087432861
Epoch 810, training loss: 63.23897933959961 = 0.5468397736549377 + 10.0 * 6.269213676452637
Epoch 810, val loss: 1.0746374130249023
Epoch 820, training loss: 63.21162033081055 = 0.531485915184021 + 10.0 * 6.2680134773254395
Epoch 820, val loss: 1.0702128410339355
Epoch 830, training loss: 63.179420471191406 = 0.5162136554718018 + 10.0 * 6.266320705413818
Epoch 830, val loss: 1.0654666423797607
Epoch 840, training loss: 63.15518569946289 = 0.5009996294975281 + 10.0 * 6.265418529510498
Epoch 840, val loss: 1.0612616539001465
Epoch 850, training loss: 63.17163848876953 = 0.4858761131763458 + 10.0 * 6.268576145172119
Epoch 850, val loss: 1.0570580959320068
Epoch 860, training loss: 63.13023376464844 = 0.47075599431991577 + 10.0 * 6.2659478187561035
Epoch 860, val loss: 1.0526807308197021
Epoch 870, training loss: 63.091224670410156 = 0.45589861273765564 + 10.0 * 6.263532638549805
Epoch 870, val loss: 1.0492349863052368
Epoch 880, training loss: 63.08620071411133 = 0.44122058153152466 + 10.0 * 6.264498233795166
Epoch 880, val loss: 1.0457642078399658
Epoch 890, training loss: 63.045135498046875 = 0.4267973005771637 + 10.0 * 6.261834144592285
Epoch 890, val loss: 1.042650580406189
Epoch 900, training loss: 63.01468276977539 = 0.4125850796699524 + 10.0 * 6.260209560394287
Epoch 900, val loss: 1.0398502349853516
Epoch 910, training loss: 63.000980377197266 = 0.3987041115760803 + 10.0 * 6.260227680206299
Epoch 910, val loss: 1.037308931350708
Epoch 920, training loss: 63.03437805175781 = 0.3850500285625458 + 10.0 * 6.264932632446289
Epoch 920, val loss: 1.0349855422973633
Epoch 930, training loss: 62.956756591796875 = 0.37163421511650085 + 10.0 * 6.258512020111084
Epoch 930, val loss: 1.0327168703079224
Epoch 940, training loss: 62.938636779785156 = 0.35856908559799194 + 10.0 * 6.258006572723389
Epoch 940, val loss: 1.0310174226760864
Epoch 950, training loss: 62.914207458496094 = 0.34587886929512024 + 10.0 * 6.256833076477051
Epoch 950, val loss: 1.0295684337615967
Epoch 960, training loss: 62.966644287109375 = 0.3334585428237915 + 10.0 * 6.2633185386657715
Epoch 960, val loss: 1.028518557548523
Epoch 970, training loss: 62.87879943847656 = 0.32128089666366577 + 10.0 * 6.255751609802246
Epoch 970, val loss: 1.0271559953689575
Epoch 980, training loss: 62.87968063354492 = 0.3095208704471588 + 10.0 * 6.257016181945801
Epoch 980, val loss: 1.026170015335083
Epoch 990, training loss: 62.832889556884766 = 0.2981399595737457 + 10.0 * 6.253474712371826
Epoch 990, val loss: 1.0260019302368164
Epoch 1000, training loss: 62.82475662231445 = 0.2871118187904358 + 10.0 * 6.2537641525268555
Epoch 1000, val loss: 1.0260143280029297
Epoch 1010, training loss: 62.82296371459961 = 0.27639374136924744 + 10.0 * 6.254656791687012
Epoch 1010, val loss: 1.026051640510559
Epoch 1020, training loss: 62.86952590942383 = 0.2660152018070221 + 10.0 * 6.260351181030273
Epoch 1020, val loss: 1.0263922214508057
Epoch 1030, training loss: 62.78326416015625 = 0.25593307614326477 + 10.0 * 6.25273323059082
Epoch 1030, val loss: 1.0270909070968628
Epoch 1040, training loss: 62.74894332885742 = 0.24627803266048431 + 10.0 * 6.2502665519714355
Epoch 1040, val loss: 1.0278655290603638
Epoch 1050, training loss: 62.73353576660156 = 0.23696637153625488 + 10.0 * 6.249657154083252
Epoch 1050, val loss: 1.028813123703003
Epoch 1060, training loss: 62.75889205932617 = 0.2279874086380005 + 10.0 * 6.2530903816223145
Epoch 1060, val loss: 1.029906153678894
Epoch 1070, training loss: 62.73446273803711 = 0.21933451294898987 + 10.0 * 6.2515130043029785
Epoch 1070, val loss: 1.0312262773513794
Epoch 1080, training loss: 62.712318420410156 = 0.21098381280899048 + 10.0 * 6.250133514404297
Epoch 1080, val loss: 1.0328068733215332
Epoch 1090, training loss: 62.67667770385742 = 0.20301686227321625 + 10.0 * 6.247365951538086
Epoch 1090, val loss: 1.03469979763031
Epoch 1100, training loss: 62.672489166259766 = 0.19537633657455444 + 10.0 * 6.247711181640625
Epoch 1100, val loss: 1.0368225574493408
Epoch 1110, training loss: 62.710262298583984 = 0.18802213668823242 + 10.0 * 6.252223968505859
Epoch 1110, val loss: 1.0390855073928833
Epoch 1120, training loss: 62.68017578125 = 0.180930957198143 + 10.0 * 6.249924659729004
Epoch 1120, val loss: 1.0409753322601318
Epoch 1130, training loss: 62.634193420410156 = 0.17417824268341064 + 10.0 * 6.246001243591309
Epoch 1130, val loss: 1.0435576438903809
Epoch 1140, training loss: 62.62226104736328 = 0.16772520542144775 + 10.0 * 6.245453834533691
Epoch 1140, val loss: 1.0462909936904907
Epoch 1150, training loss: 62.66355895996094 = 0.1615581512451172 + 10.0 * 6.250199794769287
Epoch 1150, val loss: 1.0491312742233276
Epoch 1160, training loss: 62.61125183105469 = 0.1555955559015274 + 10.0 * 6.245565891265869
Epoch 1160, val loss: 1.0515550374984741
Epoch 1170, training loss: 62.587459564208984 = 0.14994406700134277 + 10.0 * 6.243751525878906
Epoch 1170, val loss: 1.0546313524246216
Epoch 1180, training loss: 62.579368591308594 = 0.1445290595293045 + 10.0 * 6.243484020233154
Epoch 1180, val loss: 1.0579028129577637
Epoch 1190, training loss: 62.64542770385742 = 0.1393025815486908 + 10.0 * 6.250612735748291
Epoch 1190, val loss: 1.0610194206237793
Epoch 1200, training loss: 62.56678009033203 = 0.13430537283420563 + 10.0 * 6.2432475090026855
Epoch 1200, val loss: 1.0641074180603027
Epoch 1210, training loss: 62.55661392211914 = 0.12954212725162506 + 10.0 * 6.242707252502441
Epoch 1210, val loss: 1.067368507385254
Epoch 1220, training loss: 62.53055953979492 = 0.12500347197055817 + 10.0 * 6.240555763244629
Epoch 1220, val loss: 1.0709997415542603
Epoch 1230, training loss: 62.535682678222656 = 0.12065746635198593 + 10.0 * 6.241502285003662
Epoch 1230, val loss: 1.0747302770614624
Epoch 1240, training loss: 62.54283142089844 = 0.11646288633346558 + 10.0 * 6.242636680603027
Epoch 1240, val loss: 1.0782318115234375
Epoch 1250, training loss: 62.50730514526367 = 0.11244446784257889 + 10.0 * 6.239485740661621
Epoch 1250, val loss: 1.0818164348602295
Epoch 1260, training loss: 62.50969696044922 = 0.10861896723508835 + 10.0 * 6.240107536315918
Epoch 1260, val loss: 1.0855275392532349
Epoch 1270, training loss: 62.53563690185547 = 0.10494255274534225 + 10.0 * 6.243069648742676
Epoch 1270, val loss: 1.0890072584152222
Epoch 1280, training loss: 62.504798889160156 = 0.1014307290315628 + 10.0 * 6.240336894989014
Epoch 1280, val loss: 1.0929017066955566
Epoch 1290, training loss: 62.48065948486328 = 0.09806652367115021 + 10.0 * 6.238259315490723
Epoch 1290, val loss: 1.0968648195266724
Epoch 1300, training loss: 62.49773406982422 = 0.0948416143655777 + 10.0 * 6.240289211273193
Epoch 1300, val loss: 1.100655198097229
Epoch 1310, training loss: 62.46353530883789 = 0.09174174070358276 + 10.0 * 6.237179279327393
Epoch 1310, val loss: 1.1047011613845825
Epoch 1320, training loss: 62.46468734741211 = 0.0887707993388176 + 10.0 * 6.237591743469238
Epoch 1320, val loss: 1.1087214946746826
Epoch 1330, training loss: 62.474517822265625 = 0.0859217494726181 + 10.0 * 6.2388596534729
Epoch 1330, val loss: 1.1125333309173584
Epoch 1340, training loss: 62.46063232421875 = 0.08320268243551254 + 10.0 * 6.237742900848389
Epoch 1340, val loss: 1.1172764301300049
Epoch 1350, training loss: 62.43871307373047 = 0.08056782186031342 + 10.0 * 6.235814571380615
Epoch 1350, val loss: 1.121192216873169
Epoch 1360, training loss: 62.45453643798828 = 0.07806482166051865 + 10.0 * 6.23764705657959
Epoch 1360, val loss: 1.1256442070007324
Epoch 1370, training loss: 62.439239501953125 = 0.07564603537321091 + 10.0 * 6.236359596252441
Epoch 1370, val loss: 1.1297098398208618
Epoch 1380, training loss: 62.41648864746094 = 0.0733155608177185 + 10.0 * 6.234317302703857
Epoch 1380, val loss: 1.1336555480957031
Epoch 1390, training loss: 62.41033172607422 = 0.07108969986438751 + 10.0 * 6.23392391204834
Epoch 1390, val loss: 1.1381301879882812
Epoch 1400, training loss: 62.485931396484375 = 0.06896377354860306 + 10.0 * 6.241696834564209
Epoch 1400, val loss: 1.1425271034240723
Epoch 1410, training loss: 62.41949462890625 = 0.0668792799115181 + 10.0 * 6.2352614402771
Epoch 1410, val loss: 1.146190881729126
Epoch 1420, training loss: 62.40227508544922 = 0.06489655375480652 + 10.0 * 6.233737945556641
Epoch 1420, val loss: 1.1506397724151611
Epoch 1430, training loss: 62.42792892456055 = 0.0629996582865715 + 10.0 * 6.236493110656738
Epoch 1430, val loss: 1.1549861431121826
Epoch 1440, training loss: 62.384552001953125 = 0.061171870678663254 + 10.0 * 6.232337951660156
Epoch 1440, val loss: 1.1592150926589966
Epoch 1450, training loss: 62.36940383911133 = 0.05941937491297722 + 10.0 * 6.230998516082764
Epoch 1450, val loss: 1.163486361503601
Epoch 1460, training loss: 62.3881721496582 = 0.05774344876408577 + 10.0 * 6.2330427169799805
Epoch 1460, val loss: 1.1677615642547607
Epoch 1470, training loss: 62.38137435913086 = 0.05610239505767822 + 10.0 * 6.232527256011963
Epoch 1470, val loss: 1.1719131469726562
Epoch 1480, training loss: 62.36101531982422 = 0.05452726036310196 + 10.0 * 6.230648994445801
Epoch 1480, val loss: 1.1761407852172852
Epoch 1490, training loss: 62.3553466796875 = 0.05301898345351219 + 10.0 * 6.2302327156066895
Epoch 1490, val loss: 1.1805049180984497
Epoch 1500, training loss: 62.368690490722656 = 0.051571644842624664 + 10.0 * 6.2317118644714355
Epoch 1500, val loss: 1.1844122409820557
Epoch 1510, training loss: 62.35798645019531 = 0.05017165467143059 + 10.0 * 6.230781555175781
Epoch 1510, val loss: 1.1887394189834595
Epoch 1520, training loss: 62.33885192871094 = 0.04882225766777992 + 10.0 * 6.229002952575684
Epoch 1520, val loss: 1.1931794881820679
Epoch 1530, training loss: 62.34468078613281 = 0.047524239867925644 + 10.0 * 6.229715824127197
Epoch 1530, val loss: 1.1972405910491943
Epoch 1540, training loss: 62.360267639160156 = 0.04627693444490433 + 10.0 * 6.231399059295654
Epoch 1540, val loss: 1.2013098001480103
Epoch 1550, training loss: 62.336708068847656 = 0.04506463557481766 + 10.0 * 6.2291646003723145
Epoch 1550, val loss: 1.2060585021972656
Epoch 1560, training loss: 62.32353210449219 = 0.04390416294336319 + 10.0 * 6.227962970733643
Epoch 1560, val loss: 1.2101510763168335
Epoch 1570, training loss: 62.38425064086914 = 0.04278021678328514 + 10.0 * 6.234147071838379
Epoch 1570, val loss: 1.21444833278656
Epoch 1580, training loss: 62.33552551269531 = 0.04169581085443497 + 10.0 * 6.2293829917907715
Epoch 1580, val loss: 1.2184253931045532
Epoch 1590, training loss: 62.31187438964844 = 0.040639426559209824 + 10.0 * 6.227123737335205
Epoch 1590, val loss: 1.2225689888000488
Epoch 1600, training loss: 62.30421447753906 = 0.03964001312851906 + 10.0 * 6.226457595825195
Epoch 1600, val loss: 1.2268309593200684
Epoch 1610, training loss: 62.362430572509766 = 0.038674212992191315 + 10.0 * 6.232375621795654
Epoch 1610, val loss: 1.23110830783844
Epoch 1620, training loss: 62.310211181640625 = 0.03771840035915375 + 10.0 * 6.2272491455078125
Epoch 1620, val loss: 1.2347583770751953
Epoch 1630, training loss: 62.29816436767578 = 0.036812674254179 + 10.0 * 6.22613525390625
Epoch 1630, val loss: 1.2391899824142456
Epoch 1640, training loss: 62.30697250366211 = 0.035937756299972534 + 10.0 * 6.227103233337402
Epoch 1640, val loss: 1.2431610822677612
Epoch 1650, training loss: 62.33710479736328 = 0.03508667275309563 + 10.0 * 6.230201721191406
Epoch 1650, val loss: 1.246703028678894
Epoch 1660, training loss: 62.294822692871094 = 0.03425019979476929 + 10.0 * 6.226057529449463
Epoch 1660, val loss: 1.250815987586975
Epoch 1670, training loss: 62.28108596801758 = 0.03345973789691925 + 10.0 * 6.224762916564941
Epoch 1670, val loss: 1.2550973892211914
Epoch 1680, training loss: 62.27177810668945 = 0.032696232199668884 + 10.0 * 6.223908424377441
Epoch 1680, val loss: 1.259105920791626
Epoch 1690, training loss: 62.26578903198242 = 0.03195977583527565 + 10.0 * 6.223382949829102
Epoch 1690, val loss: 1.263244390487671
Epoch 1700, training loss: 62.34751510620117 = 0.031245699152350426 + 10.0 * 6.231626987457275
Epoch 1700, val loss: 1.2671427726745605
Epoch 1710, training loss: 62.299320220947266 = 0.030544467270374298 + 10.0 * 6.226877689361572
Epoch 1710, val loss: 1.2709825038909912
Epoch 1720, training loss: 62.29285430908203 = 0.029855752363801003 + 10.0 * 6.22629976272583
Epoch 1720, val loss: 1.2747759819030762
Epoch 1730, training loss: 62.26093292236328 = 0.029203714802861214 + 10.0 * 6.223172664642334
Epoch 1730, val loss: 1.2785199880599976
Epoch 1740, training loss: 62.256988525390625 = 0.02857683040201664 + 10.0 * 6.222841262817383
Epoch 1740, val loss: 1.2825987339019775
Epoch 1750, training loss: 62.245147705078125 = 0.027966534718871117 + 10.0 * 6.2217183113098145
Epoch 1750, val loss: 1.2865493297576904
Epoch 1760, training loss: 62.276878356933594 = 0.027382414788007736 + 10.0 * 6.224949836730957
Epoch 1760, val loss: 1.2903623580932617
Epoch 1770, training loss: 62.27603530883789 = 0.02679981291294098 + 10.0 * 6.224923610687256
Epoch 1770, val loss: 1.2935870885849
Epoch 1780, training loss: 62.24451446533203 = 0.02623499371111393 + 10.0 * 6.221827983856201
Epoch 1780, val loss: 1.2980968952178955
Epoch 1790, training loss: 62.24052047729492 = 0.025693364441394806 + 10.0 * 6.221482753753662
Epoch 1790, val loss: 1.301407814025879
Epoch 1800, training loss: 62.25678253173828 = 0.025171132758259773 + 10.0 * 6.223161220550537
Epoch 1800, val loss: 1.3053719997406006
Epoch 1810, training loss: 62.231651306152344 = 0.024662340059876442 + 10.0 * 6.220698833465576
Epoch 1810, val loss: 1.3090558052062988
Epoch 1820, training loss: 62.241214752197266 = 0.02417290024459362 + 10.0 * 6.221704006195068
Epoch 1820, val loss: 1.3128607273101807
Epoch 1830, training loss: 62.24203109741211 = 0.023693377152085304 + 10.0 * 6.2218337059021
Epoch 1830, val loss: 1.3166753053665161
Epoch 1840, training loss: 62.2647819519043 = 0.02323058806359768 + 10.0 * 6.224154949188232
Epoch 1840, val loss: 1.3204131126403809
Epoch 1850, training loss: 62.226585388183594 = 0.022772207856178284 + 10.0 * 6.220381259918213
Epoch 1850, val loss: 1.324114203453064
Epoch 1860, training loss: 62.21261215209961 = 0.022328900173306465 + 10.0 * 6.219028472900391
Epoch 1860, val loss: 1.3273818492889404
Epoch 1870, training loss: 62.20942687988281 = 0.02191038802266121 + 10.0 * 6.218751430511475
Epoch 1870, val loss: 1.331223964691162
Epoch 1880, training loss: 62.214630126953125 = 0.02150186337530613 + 10.0 * 6.21931266784668
Epoch 1880, val loss: 1.3348397016525269
Epoch 1890, training loss: 62.25286102294922 = 0.02110402286052704 + 10.0 * 6.223176002502441
Epoch 1890, val loss: 1.3381645679473877
Epoch 1900, training loss: 62.24510192871094 = 0.02070724032819271 + 10.0 * 6.222439765930176
Epoch 1900, val loss: 1.3421106338500977
Epoch 1910, training loss: 62.230045318603516 = 0.020323269069194794 + 10.0 * 6.220972061157227
Epoch 1910, val loss: 1.345434546470642
Epoch 1920, training loss: 62.199745178222656 = 0.019951578229665756 + 10.0 * 6.217979431152344
Epoch 1920, val loss: 1.3489480018615723
Epoch 1930, training loss: 62.19997787475586 = 0.019593263044953346 + 10.0 * 6.218038558959961
Epoch 1930, val loss: 1.3523322343826294
Epoch 1940, training loss: 62.209659576416016 = 0.019246693700551987 + 10.0 * 6.219041347503662
Epoch 1940, val loss: 1.3560534715652466
Epoch 1950, training loss: 62.22154998779297 = 0.018904026597738266 + 10.0 * 6.220264434814453
Epoch 1950, val loss: 1.3592627048492432
Epoch 1960, training loss: 62.211143493652344 = 0.018566295504570007 + 10.0 * 6.219257831573486
Epoch 1960, val loss: 1.3620625734329224
Epoch 1970, training loss: 62.19367218017578 = 0.01824479177594185 + 10.0 * 6.21754264831543
Epoch 1970, val loss: 1.365635871887207
Epoch 1980, training loss: 62.19085693359375 = 0.01793019473552704 + 10.0 * 6.217292785644531
Epoch 1980, val loss: 1.3688901662826538
Epoch 1990, training loss: 62.21402359008789 = 0.01762845553457737 + 10.0 * 6.219639778137207
Epoch 1990, val loss: 1.3720126152038574
Epoch 2000, training loss: 62.17845153808594 = 0.017328832298517227 + 10.0 * 6.21611213684082
Epoch 2000, val loss: 1.3755319118499756
Epoch 2010, training loss: 62.20917510986328 = 0.017044873908162117 + 10.0 * 6.219213008880615
Epoch 2010, val loss: 1.3791364431381226
Epoch 2020, training loss: 62.186607360839844 = 0.016755416989326477 + 10.0 * 6.21698522567749
Epoch 2020, val loss: 1.3818800449371338
Epoch 2030, training loss: 62.20072937011719 = 0.016478391364216805 + 10.0 * 6.2184247970581055
Epoch 2030, val loss: 1.3850376605987549
Epoch 2040, training loss: 62.18441390991211 = 0.016209730878472328 + 10.0 * 6.21682071685791
Epoch 2040, val loss: 1.3884550333023071
Epoch 2050, training loss: 62.17206573486328 = 0.01594454236328602 + 10.0 * 6.215611934661865
Epoch 2050, val loss: 1.3915064334869385
Epoch 2060, training loss: 62.16348648071289 = 0.01569286175072193 + 10.0 * 6.214779376983643
Epoch 2060, val loss: 1.3949105739593506
Epoch 2070, training loss: 62.16986846923828 = 0.015446823090314865 + 10.0 * 6.215442180633545
Epoch 2070, val loss: 1.3981302976608276
Epoch 2080, training loss: 62.201236724853516 = 0.01520317792892456 + 10.0 * 6.218603610992432
Epoch 2080, val loss: 1.4009791612625122
Epoch 2090, training loss: 62.158897399902344 = 0.014959513209760189 + 10.0 * 6.214393615722656
Epoch 2090, val loss: 1.4038050174713135
Epoch 2100, training loss: 62.15591049194336 = 0.014726928435266018 + 10.0 * 6.214118003845215
Epoch 2100, val loss: 1.4070149660110474
Epoch 2110, training loss: 62.17121887207031 = 0.014505509287118912 + 10.0 * 6.215671062469482
Epoch 2110, val loss: 1.410287618637085
Epoch 2120, training loss: 62.17854690551758 = 0.014285869896411896 + 10.0 * 6.216425895690918
Epoch 2120, val loss: 1.4132506847381592
Epoch 2130, training loss: 62.177818298339844 = 0.014065208844840527 + 10.0 * 6.216375350952148
Epoch 2130, val loss: 1.4158776998519897
Epoch 2140, training loss: 62.16170120239258 = 0.01385432668030262 + 10.0 * 6.214784622192383
Epoch 2140, val loss: 1.4189269542694092
Epoch 2150, training loss: 62.1569938659668 = 0.013650189153850079 + 10.0 * 6.214334487915039
Epoch 2150, val loss: 1.4216398000717163
Epoch 2160, training loss: 62.18122482299805 = 0.013450384140014648 + 10.0 * 6.216777324676514
Epoch 2160, val loss: 1.4244658946990967
Epoch 2170, training loss: 62.14925003051758 = 0.013250604271888733 + 10.0 * 6.213599681854248
Epoch 2170, val loss: 1.4275810718536377
Epoch 2180, training loss: 62.133384704589844 = 0.013061024248600006 + 10.0 * 6.212032318115234
Epoch 2180, val loss: 1.4303520917892456
Epoch 2190, training loss: 62.14155960083008 = 0.012877965345978737 + 10.0 * 6.2128682136535645
Epoch 2190, val loss: 1.4336020946502686
Epoch 2200, training loss: 62.20176315307617 = 0.012698751874268055 + 10.0 * 6.218906402587891
Epoch 2200, val loss: 1.4365564584732056
Epoch 2210, training loss: 62.160675048828125 = 0.012512020766735077 + 10.0 * 6.214816093444824
Epoch 2210, val loss: 1.4384466409683228
Epoch 2220, training loss: 62.139373779296875 = 0.012336491607129574 + 10.0 * 6.212703704833984
Epoch 2220, val loss: 1.4419668912887573
Epoch 2230, training loss: 62.18113327026367 = 0.01217144075781107 + 10.0 * 6.216896057128906
Epoch 2230, val loss: 1.4446786642074585
Epoch 2240, training loss: 62.12693405151367 = 0.011995714157819748 + 10.0 * 6.211493492126465
Epoch 2240, val loss: 1.4469319581985474
Epoch 2250, training loss: 62.12871170043945 = 0.01183092687278986 + 10.0 * 6.211688041687012
Epoch 2250, val loss: 1.4496864080429077
Epoch 2260, training loss: 62.156951904296875 = 0.011675236746668816 + 10.0 * 6.214527606964111
Epoch 2260, val loss: 1.4527103900909424
Epoch 2270, training loss: 62.11973571777344 = 0.011514432728290558 + 10.0 * 6.210822105407715
Epoch 2270, val loss: 1.455121636390686
Epoch 2280, training loss: 62.11796951293945 = 0.01136273704469204 + 10.0 * 6.210660457611084
Epoch 2280, val loss: 1.4578020572662354
Epoch 2290, training loss: 62.12668991088867 = 0.01121506281197071 + 10.0 * 6.211547374725342
Epoch 2290, val loss: 1.4605231285095215
Epoch 2300, training loss: 62.183074951171875 = 0.011067185550928116 + 10.0 * 6.217200756072998
Epoch 2300, val loss: 1.4629135131835938
Epoch 2310, training loss: 62.13002014160156 = 0.010920177213847637 + 10.0 * 6.211909770965576
Epoch 2310, val loss: 1.4655041694641113
Epoch 2320, training loss: 62.11130905151367 = 0.010779534466564655 + 10.0 * 6.210052967071533
Epoch 2320, val loss: 1.4682538509368896
Epoch 2330, training loss: 62.10860824584961 = 0.010644285008311272 + 10.0 * 6.20979642868042
Epoch 2330, val loss: 1.4707328081130981
Epoch 2340, training loss: 62.196266174316406 = 0.010512147098779678 + 10.0 * 6.218575477600098
Epoch 2340, val loss: 1.4725927114486694
Epoch 2350, training loss: 62.13114929199219 = 0.010370912030339241 + 10.0 * 6.212077617645264
Epoch 2350, val loss: 1.4759820699691772
Epoch 2360, training loss: 62.10847854614258 = 0.010242733173072338 + 10.0 * 6.2098236083984375
Epoch 2360, val loss: 1.4784321784973145
Epoch 2370, training loss: 62.15166091918945 = 0.010119117796421051 + 10.0 * 6.214154243469238
Epoch 2370, val loss: 1.4808101654052734
Epoch 2380, training loss: 62.10671615600586 = 0.00998690165579319 + 10.0 * 6.209672927856445
Epoch 2380, val loss: 1.4828652143478394
Epoch 2390, training loss: 62.1013069152832 = 0.009867649525403976 + 10.0 * 6.20914363861084
Epoch 2390, val loss: 1.4854223728179932
Epoch 2400, training loss: 62.128501892089844 = 0.009748964570462704 + 10.0 * 6.211874961853027
Epoch 2400, val loss: 1.4880390167236328
Epoch 2410, training loss: 62.09242630004883 = 0.009630688466131687 + 10.0 * 6.208279609680176
Epoch 2410, val loss: 1.4900145530700684
Epoch 2420, training loss: 62.10995101928711 = 0.009520242922008038 + 10.0 * 6.210042953491211
Epoch 2420, val loss: 1.492497444152832
Epoch 2430, training loss: 62.12124252319336 = 0.00940485019236803 + 10.0 * 6.211184024810791
Epoch 2430, val loss: 1.4946763515472412
Epoch 2440, training loss: 62.09711456298828 = 0.009292670525610447 + 10.0 * 6.208782196044922
Epoch 2440, val loss: 1.4970439672470093
Epoch 2450, training loss: 62.10987854003906 = 0.009187055751681328 + 10.0 * 6.210069179534912
Epoch 2450, val loss: 1.499358892440796
Epoch 2460, training loss: 62.10645294189453 = 0.009078558534383774 + 10.0 * 6.209737300872803
Epoch 2460, val loss: 1.501861810684204
Epoch 2470, training loss: 62.111961364746094 = 0.008976428769528866 + 10.0 * 6.210298538208008
Epoch 2470, val loss: 1.5041993856430054
Epoch 2480, training loss: 62.08476257324219 = 0.008871377445757389 + 10.0 * 6.207589149475098
Epoch 2480, val loss: 1.506409764289856
Epoch 2490, training loss: 62.07600784301758 = 0.008771342225372791 + 10.0 * 6.206723690032959
Epoch 2490, val loss: 1.5085270404815674
Epoch 2500, training loss: 62.08363723754883 = 0.008675767108798027 + 10.0 * 6.207496166229248
Epoch 2500, val loss: 1.5110654830932617
Epoch 2510, training loss: 62.13750076293945 = 0.008580767549574375 + 10.0 * 6.212892055511475
Epoch 2510, val loss: 1.5130550861358643
Epoch 2520, training loss: 62.10176086425781 = 0.008481984958052635 + 10.0 * 6.209327697753906
Epoch 2520, val loss: 1.5150507688522339
Epoch 2530, training loss: 62.08718490600586 = 0.008389241993427277 + 10.0 * 6.207879543304443
Epoch 2530, val loss: 1.5175484418869019
Epoch 2540, training loss: 62.0914192199707 = 0.008297044783830643 + 10.0 * 6.208312034606934
Epoch 2540, val loss: 1.5195118188858032
Epoch 2550, training loss: 62.089881896972656 = 0.008207965642213821 + 10.0 * 6.20816707611084
Epoch 2550, val loss: 1.5216587781906128
Epoch 2560, training loss: 62.10945129394531 = 0.008119585923850536 + 10.0 * 6.210133075714111
Epoch 2560, val loss: 1.5238202810287476
Epoch 2570, training loss: 62.072635650634766 = 0.008031196892261505 + 10.0 * 6.206460475921631
Epoch 2570, val loss: 1.5258644819259644
Epoch 2580, training loss: 62.0685920715332 = 0.007946206256747246 + 10.0 * 6.206064701080322
Epoch 2580, val loss: 1.5279334783554077
Epoch 2590, training loss: 62.064231872558594 = 0.007863595150411129 + 10.0 * 6.205636978149414
Epoch 2590, val loss: 1.5300467014312744
Epoch 2600, training loss: 62.0712890625 = 0.007784382440149784 + 10.0 * 6.206350326538086
Epoch 2600, val loss: 1.5322233438491821
Epoch 2610, training loss: 62.088233947753906 = 0.007704786024987698 + 10.0 * 6.208052635192871
Epoch 2610, val loss: 1.5342369079589844
Epoch 2620, training loss: 62.08527755737305 = 0.0076223029755055904 + 10.0 * 6.207765579223633
Epoch 2620, val loss: 1.5358070135116577
Epoch 2630, training loss: 62.10804748535156 = 0.007545993663370609 + 10.0 * 6.210050106048584
Epoch 2630, val loss: 1.5382076501846313
Epoch 2640, training loss: 62.09868621826172 = 0.007465477101504803 + 10.0 * 6.209122180938721
Epoch 2640, val loss: 1.5396173000335693
Epoch 2650, training loss: 62.07046890258789 = 0.007386975921690464 + 10.0 * 6.206308364868164
Epoch 2650, val loss: 1.5421676635742188
Epoch 2660, training loss: 62.05363464355469 = 0.00731391366571188 + 10.0 * 6.20463228225708
Epoch 2660, val loss: 1.5440897941589355
Epoch 2670, training loss: 62.06210708618164 = 0.007244651671499014 + 10.0 * 6.205486297607422
Epoch 2670, val loss: 1.546446681022644
Epoch 2680, training loss: 62.09086990356445 = 0.007173947524279356 + 10.0 * 6.208369255065918
Epoch 2680, val loss: 1.548309087753296
Epoch 2690, training loss: 62.0543327331543 = 0.00710091320797801 + 10.0 * 6.204723358154297
Epoch 2690, val loss: 1.5495028495788574
Epoch 2700, training loss: 62.04450225830078 = 0.007032736670225859 + 10.0 * 6.203746795654297
Epoch 2700, val loss: 1.5518532991409302
Epoch 2710, training loss: 62.0687141418457 = 0.006968326400965452 + 10.0 * 6.206174373626709
Epoch 2710, val loss: 1.5539829730987549
Epoch 2720, training loss: 62.05207061767578 = 0.00690012751147151 + 10.0 * 6.204516887664795
Epoch 2720, val loss: 1.5551341772079468
Epoch 2730, training loss: 62.065643310546875 = 0.006834451109170914 + 10.0 * 6.205880641937256
Epoch 2730, val loss: 1.5571168661117554
Epoch 2740, training loss: 62.08293914794922 = 0.006769426632672548 + 10.0 * 6.207616806030273
Epoch 2740, val loss: 1.5592015981674194
Epoch 2750, training loss: 62.06516647338867 = 0.006703158374875784 + 10.0 * 6.205846309661865
Epoch 2750, val loss: 1.5606414079666138
Epoch 2760, training loss: 62.03977966308594 = 0.006641400512307882 + 10.0 * 6.203313827514648
Epoch 2760, val loss: 1.5628107786178589
Epoch 2770, training loss: 62.04574203491211 = 0.006580880377441645 + 10.0 * 6.203916072845459
Epoch 2770, val loss: 1.5644832849502563
Epoch 2780, training loss: 62.0767936706543 = 0.0065209632739424706 + 10.0 * 6.207027435302734
Epoch 2780, val loss: 1.5664509534835815
Epoch 2790, training loss: 62.08948516845703 = 0.0064627183601260185 + 10.0 * 6.2083024978637695
Epoch 2790, val loss: 1.5680620670318604
Epoch 2800, training loss: 62.05217742919922 = 0.00640081288293004 + 10.0 * 6.204577445983887
Epoch 2800, val loss: 1.5698697566986084
Epoch 2810, training loss: 62.02656173706055 = 0.006342705804854631 + 10.0 * 6.202021598815918
Epoch 2810, val loss: 1.5714343786239624
Epoch 2820, training loss: 62.026363372802734 = 0.0062868124805390835 + 10.0 * 6.20200777053833
Epoch 2820, val loss: 1.5733438730239868
Epoch 2830, training loss: 62.027408599853516 = 0.006232978776097298 + 10.0 * 6.202117443084717
Epoch 2830, val loss: 1.5750354528427124
Epoch 2840, training loss: 62.07442855834961 = 0.006180388852953911 + 10.0 * 6.206824779510498
Epoch 2840, val loss: 1.5762381553649902
Epoch 2850, training loss: 62.05992126464844 = 0.006125334650278091 + 10.0 * 6.205379486083984
Epoch 2850, val loss: 1.5787602663040161
Epoch 2860, training loss: 62.0305290222168 = 0.006068236194550991 + 10.0 * 6.202445983886719
Epoch 2860, val loss: 1.5796393156051636
Epoch 2870, training loss: 62.026248931884766 = 0.006015408784151077 + 10.0 * 6.202023506164551
Epoch 2870, val loss: 1.5817898511886597
Epoch 2880, training loss: 62.018531799316406 = 0.00596531992778182 + 10.0 * 6.20125675201416
Epoch 2880, val loss: 1.583626389503479
Epoch 2890, training loss: 62.02421951293945 = 0.005915287882089615 + 10.0 * 6.2018303871154785
Epoch 2890, val loss: 1.5851612091064453
Epoch 2900, training loss: 62.0722541809082 = 0.005865565966814756 + 10.0 * 6.206638813018799
Epoch 2900, val loss: 1.5867127180099487
Epoch 2910, training loss: 62.028751373291016 = 0.005814934149384499 + 10.0 * 6.202293872833252
Epoch 2910, val loss: 1.5882683992385864
Epoch 2920, training loss: 62.022071838378906 = 0.005766505375504494 + 10.0 * 6.201630592346191
Epoch 2920, val loss: 1.5899754762649536
Epoch 2930, training loss: 62.0392951965332 = 0.005718800704926252 + 10.0 * 6.203357696533203
Epoch 2930, val loss: 1.5916105508804321
Epoch 2940, training loss: 62.05655288696289 = 0.005671968217939138 + 10.0 * 6.205088138580322
Epoch 2940, val loss: 1.5935790538787842
Epoch 2950, training loss: 62.02425003051758 = 0.005623380187898874 + 10.0 * 6.201862812042236
Epoch 2950, val loss: 1.594441533088684
Epoch 2960, training loss: 62.01553726196289 = 0.005576884374022484 + 10.0 * 6.200995922088623
Epoch 2960, val loss: 1.596186637878418
Epoch 2970, training loss: 62.0084342956543 = 0.005533054005354643 + 10.0 * 6.200290203094482
Epoch 2970, val loss: 1.5982102155685425
Epoch 2980, training loss: 62.016441345214844 = 0.005490296054631472 + 10.0 * 6.201095104217529
Epoch 2980, val loss: 1.5995615720748901
Epoch 2990, training loss: 62.03443145751953 = 0.00544752785935998 + 10.0 * 6.2028985023498535
Epoch 2990, val loss: 1.6010892391204834
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 87.92787170410156 = 1.9591048955917358 + 10.0 * 8.59687614440918
Epoch 0, val loss: 1.9667527675628662
Epoch 10, training loss: 87.91325378417969 = 1.9482505321502686 + 10.0 * 8.596500396728516
Epoch 10, val loss: 1.9558035135269165
Epoch 20, training loss: 87.86891174316406 = 1.9347797632217407 + 10.0 * 8.593413352966309
Epoch 20, val loss: 1.9413830041885376
Epoch 30, training loss: 87.62678527832031 = 1.917712688446045 + 10.0 * 8.570907592773438
Epoch 30, val loss: 1.9223709106445312
Epoch 40, training loss: 86.36714172363281 = 1.897831916809082 + 10.0 * 8.446930885314941
Epoch 40, val loss: 1.9004756212234497
Epoch 50, training loss: 81.61968994140625 = 1.878584623336792 + 10.0 * 7.974110126495361
Epoch 50, val loss: 1.8798584938049316
Epoch 60, training loss: 78.20321655273438 = 1.8629902601242065 + 10.0 * 7.6340227127075195
Epoch 60, val loss: 1.8643910884857178
Epoch 70, training loss: 73.936767578125 = 1.849753499031067 + 10.0 * 7.2087016105651855
Epoch 70, val loss: 1.8513931035995483
Epoch 80, training loss: 71.453857421875 = 1.8369816541671753 + 10.0 * 6.9616875648498535
Epoch 80, val loss: 1.839389681816101
Epoch 90, training loss: 70.19219207763672 = 1.8239551782608032 + 10.0 * 6.8368239402771
Epoch 90, val loss: 1.8270854949951172
Epoch 100, training loss: 69.31269836425781 = 1.8112587928771973 + 10.0 * 6.750144004821777
Epoch 100, val loss: 1.8147826194763184
Epoch 110, training loss: 68.61646270751953 = 1.7997969388961792 + 10.0 * 6.681666374206543
Epoch 110, val loss: 1.8032301664352417
Epoch 120, training loss: 68.1382064819336 = 1.7888826131820679 + 10.0 * 6.634932994842529
Epoch 120, val loss: 1.7924450635910034
Epoch 130, training loss: 67.71992492675781 = 1.7778815031051636 + 10.0 * 6.594204425811768
Epoch 130, val loss: 1.7818248271942139
Epoch 140, training loss: 67.3375244140625 = 1.7665464878082275 + 10.0 * 6.557097911834717
Epoch 140, val loss: 1.771066665649414
Epoch 150, training loss: 67.04833221435547 = 1.7543855905532837 + 10.0 * 6.529394626617432
Epoch 150, val loss: 1.7599056959152222
Epoch 160, training loss: 66.82352447509766 = 1.7412489652633667 + 10.0 * 6.508227825164795
Epoch 160, val loss: 1.7478004693984985
Epoch 170, training loss: 66.6269760131836 = 1.7268553972244263 + 10.0 * 6.490012168884277
Epoch 170, val loss: 1.7348310947418213
Epoch 180, training loss: 66.45291137695312 = 1.7111186981201172 + 10.0 * 6.474178791046143
Epoch 180, val loss: 1.720679521560669
Epoch 190, training loss: 66.29734802246094 = 1.6939505338668823 + 10.0 * 6.4603400230407715
Epoch 190, val loss: 1.7053478956222534
Epoch 200, training loss: 66.15001678466797 = 1.6753427982330322 + 10.0 * 6.44746732711792
Epoch 200, val loss: 1.6886670589447021
Epoch 210, training loss: 66.03116607666016 = 1.6550512313842773 + 10.0 * 6.437611103057861
Epoch 210, val loss: 1.6705126762390137
Epoch 220, training loss: 65.9073257446289 = 1.6331186294555664 + 10.0 * 6.4274210929870605
Epoch 220, val loss: 1.6508601903915405
Epoch 230, training loss: 65.85948944091797 = 1.6095002889633179 + 10.0 * 6.424999237060547
Epoch 230, val loss: 1.6296879053115845
Epoch 240, training loss: 65.69905090332031 = 1.584108591079712 + 10.0 * 6.411494731903076
Epoch 240, val loss: 1.6071141958236694
Epoch 250, training loss: 65.58414459228516 = 1.557293176651001 + 10.0 * 6.402684688568115
Epoch 250, val loss: 1.5833357572555542
Epoch 260, training loss: 65.47853088378906 = 1.5291666984558105 + 10.0 * 6.394936561584473
Epoch 260, val loss: 1.5584124326705933
Epoch 270, training loss: 65.40333557128906 = 1.4999139308929443 + 10.0 * 6.3903422355651855
Epoch 270, val loss: 1.5325653553009033
Epoch 280, training loss: 65.29994201660156 = 1.4695929288864136 + 10.0 * 6.383035182952881
Epoch 280, val loss: 1.5060709714889526
Epoch 290, training loss: 65.20862579345703 = 1.438853144645691 + 10.0 * 6.376977443695068
Epoch 290, val loss: 1.479161024093628
Epoch 300, training loss: 65.12039947509766 = 1.4076087474822998 + 10.0 * 6.371278762817383
Epoch 300, val loss: 1.4521886110305786
Epoch 310, training loss: 65.05347442626953 = 1.3763169050216675 + 10.0 * 6.367715358734131
Epoch 310, val loss: 1.4253747463226318
Epoch 320, training loss: 64.94852447509766 = 1.3450833559036255 + 10.0 * 6.360344409942627
Epoch 320, val loss: 1.398925542831421
Epoch 330, training loss: 64.86331176757812 = 1.3140816688537598 + 10.0 * 6.354922771453857
Epoch 330, val loss: 1.372937560081482
Epoch 340, training loss: 64.82420349121094 = 1.2834055423736572 + 10.0 * 6.354079723358154
Epoch 340, val loss: 1.3476256132125854
Epoch 350, training loss: 64.86192321777344 = 1.253217339515686 + 10.0 * 6.360870838165283
Epoch 350, val loss: 1.3229557275772095
Epoch 360, training loss: 64.68914031982422 = 1.2234832048416138 + 10.0 * 6.3465657234191895
Epoch 360, val loss: 1.2992784976959229
Epoch 370, training loss: 64.59098052978516 = 1.1944506168365479 + 10.0 * 6.3396525382995605
Epoch 370, val loss: 1.276419997215271
Epoch 380, training loss: 64.52799987792969 = 1.16604745388031 + 10.0 * 6.33619499206543
Epoch 380, val loss: 1.2545794248580933
Epoch 390, training loss: 64.46615600585938 = 1.138245940208435 + 10.0 * 6.332791328430176
Epoch 390, val loss: 1.2336193323135376
Epoch 400, training loss: 64.40814208984375 = 1.1109715700149536 + 10.0 * 6.32971715927124
Epoch 400, val loss: 1.2134697437286377
Epoch 410, training loss: 64.35216522216797 = 1.0842317342758179 + 10.0 * 6.326793670654297
Epoch 410, val loss: 1.1941078901290894
Epoch 420, training loss: 64.3017349243164 = 1.0580395460128784 + 10.0 * 6.32436990737915
Epoch 420, val loss: 1.1755620241165161
Epoch 430, training loss: 64.27274322509766 = 1.0323373079299927 + 10.0 * 6.324040412902832
Epoch 430, val loss: 1.1579442024230957
Epoch 440, training loss: 64.22203063964844 = 1.0073307752609253 + 10.0 * 6.321469783782959
Epoch 440, val loss: 1.1410157680511475
Epoch 450, training loss: 64.15866088867188 = 0.9830325841903687 + 10.0 * 6.317563056945801
Epoch 450, val loss: 1.124986171722412
Epoch 460, training loss: 64.10932922363281 = 0.9593441486358643 + 10.0 * 6.314998149871826
Epoch 460, val loss: 1.1097878217697144
Epoch 470, training loss: 64.0746841430664 = 0.9362385272979736 + 10.0 * 6.313844203948975
Epoch 470, val loss: 1.0953757762908936
Epoch 480, training loss: 64.04700469970703 = 0.9138372540473938 + 10.0 * 6.313316822052002
Epoch 480, val loss: 1.0817724466323853
Epoch 490, training loss: 63.98093032836914 = 0.892078161239624 + 10.0 * 6.308885097503662
Epoch 490, val loss: 1.069090485572815
Epoch 500, training loss: 63.932796478271484 = 0.8709761500358582 + 10.0 * 6.306181907653809
Epoch 500, val loss: 1.0572086572647095
Epoch 510, training loss: 63.985008239746094 = 0.850532591342926 + 10.0 * 6.31344747543335
Epoch 510, val loss: 1.0461491346359253
Epoch 520, training loss: 63.85559844970703 = 0.8306288123130798 + 10.0 * 6.302496910095215
Epoch 520, val loss: 1.0358084440231323
Epoch 530, training loss: 63.821495056152344 = 0.8114265203475952 + 10.0 * 6.30100679397583
Epoch 530, val loss: 1.0262930393218994
Epoch 540, training loss: 63.84209060668945 = 0.7928301095962524 + 10.0 * 6.304925918579102
Epoch 540, val loss: 1.0175660848617554
Epoch 550, training loss: 63.74970245361328 = 0.7748118042945862 + 10.0 * 6.297489166259766
Epoch 550, val loss: 1.0095001459121704
Epoch 560, training loss: 63.71403121948242 = 0.7573530673980713 + 10.0 * 6.29566764831543
Epoch 560, val loss: 1.0022245645523071
Epoch 570, training loss: 63.68285369873047 = 0.7404422760009766 + 10.0 * 6.294240951538086
Epoch 570, val loss: 0.9956651329994202
Epoch 580, training loss: 63.73268508911133 = 0.724012017250061 + 10.0 * 6.300867557525635
Epoch 580, val loss: 0.9895903468132019
Epoch 590, training loss: 63.658477783203125 = 0.7080024480819702 + 10.0 * 6.295047283172607
Epoch 590, val loss: 0.9840456247329712
Epoch 600, training loss: 63.59776306152344 = 0.6925050616264343 + 10.0 * 6.290525913238525
Epoch 600, val loss: 0.9791967272758484
Epoch 610, training loss: 63.56093978881836 = 0.6774862408638 + 10.0 * 6.2883453369140625
Epoch 610, val loss: 0.974843442440033
Epoch 620, training loss: 63.52996826171875 = 0.6628579497337341 + 10.0 * 6.2867112159729
Epoch 620, val loss: 0.9709935188293457
Epoch 630, training loss: 63.499290466308594 = 0.6485704779624939 + 10.0 * 6.285071849822998
Epoch 630, val loss: 0.9675464630126953
Epoch 640, training loss: 63.537696838378906 = 0.6345514059066772 + 10.0 * 6.290314674377441
Epoch 640, val loss: 0.9645311236381531
Epoch 650, training loss: 63.468013763427734 = 0.620795488357544 + 10.0 * 6.284721851348877
Epoch 650, val loss: 0.9615668654441833
Epoch 660, training loss: 63.43402099609375 = 0.6073665618896484 + 10.0 * 6.282665729522705
Epoch 660, val loss: 0.9589846134185791
Epoch 670, training loss: 63.4022331237793 = 0.5942773818969727 + 10.0 * 6.280795574188232
Epoch 670, val loss: 0.9567493796348572
Epoch 680, training loss: 63.37225341796875 = 0.5814902186393738 + 10.0 * 6.27907657623291
Epoch 680, val loss: 0.9547695517539978
Epoch 690, training loss: 63.34709167480469 = 0.568937361240387 + 10.0 * 6.277815341949463
Epoch 690, val loss: 0.9530599117279053
Epoch 700, training loss: 63.44942855834961 = 0.5565752983093262 + 10.0 * 6.289285182952881
Epoch 700, val loss: 0.9514393210411072
Epoch 710, training loss: 63.30589294433594 = 0.5444011688232422 + 10.0 * 6.276149272918701
Epoch 710, val loss: 0.9500237107276917
Epoch 720, training loss: 63.28666305541992 = 0.5324551463127136 + 10.0 * 6.275420665740967
Epoch 720, val loss: 0.948921263217926
Epoch 730, training loss: 63.2592887878418 = 0.5207575559616089 + 10.0 * 6.273853302001953
Epoch 730, val loss: 0.9478628635406494
Epoch 740, training loss: 63.240203857421875 = 0.5092620849609375 + 10.0 * 6.273094177246094
Epoch 740, val loss: 0.9469954967498779
Epoch 750, training loss: 63.24576187133789 = 0.49790123105049133 + 10.0 * 6.274785995483398
Epoch 750, val loss: 0.9462186098098755
Epoch 760, training loss: 63.21929168701172 = 0.48666566610336304 + 10.0 * 6.273262977600098
Epoch 760, val loss: 0.9453138709068298
Epoch 770, training loss: 63.18395233154297 = 0.4756893217563629 + 10.0 * 6.27082633972168
Epoch 770, val loss: 0.9448524117469788
Epoch 780, training loss: 63.15562057495117 = 0.46490493416786194 + 10.0 * 6.269071578979492
Epoch 780, val loss: 0.9444228410720825
Epoch 790, training loss: 63.13146209716797 = 0.45428526401519775 + 10.0 * 6.2677178382873535
Epoch 790, val loss: 0.9440853595733643
Epoch 800, training loss: 63.1254768371582 = 0.44378167390823364 + 10.0 * 6.268169403076172
Epoch 800, val loss: 0.9437569975852966
Epoch 810, training loss: 63.110313415527344 = 0.4333513379096985 + 10.0 * 6.267696380615234
Epoch 810, val loss: 0.9434829950332642
Epoch 820, training loss: 63.08063888549805 = 0.42309272289276123 + 10.0 * 6.265754699707031
Epoch 820, val loss: 0.9432404637336731
Epoch 830, training loss: 63.0663948059082 = 0.41300132870674133 + 10.0 * 6.265339374542236
Epoch 830, val loss: 0.9431176781654358
Epoch 840, training loss: 63.05944061279297 = 0.4030323922634125 + 10.0 * 6.265640735626221
Epoch 840, val loss: 0.9429506063461304
Epoch 850, training loss: 63.01905059814453 = 0.393206387758255 + 10.0 * 6.262584209442139
Epoch 850, val loss: 0.943040132522583
Epoch 860, training loss: 63.04914093017578 = 0.3835304379463196 + 10.0 * 6.266561031341553
Epoch 860, val loss: 0.9430832266807556
Epoch 870, training loss: 62.998146057128906 = 0.37386566400527954 + 10.0 * 6.262427806854248
Epoch 870, val loss: 0.9429152607917786
Epoch 880, training loss: 62.98088836669922 = 0.36444199085235596 + 10.0 * 6.2616448402404785
Epoch 880, val loss: 0.9429547786712646
Epoch 890, training loss: 62.95126724243164 = 0.3551574647426605 + 10.0 * 6.259611129760742
Epoch 890, val loss: 0.9430034756660461
Epoch 900, training loss: 62.93174743652344 = 0.34604522585868835 + 10.0 * 6.258570194244385
Epoch 900, val loss: 0.9431223273277283
Epoch 910, training loss: 62.92771530151367 = 0.3370647132396698 + 10.0 * 6.2590651512146
Epoch 910, val loss: 0.9432713985443115
Epoch 920, training loss: 62.93815231323242 = 0.32818368077278137 + 10.0 * 6.2609968185424805
Epoch 920, val loss: 0.9435116052627563
Epoch 930, training loss: 62.88938522338867 = 0.31948432326316833 + 10.0 * 6.2569899559021
Epoch 930, val loss: 0.9437034130096436
Epoch 940, training loss: 62.89622116088867 = 0.31096142530441284 + 10.0 * 6.258525848388672
Epoch 940, val loss: 0.9442246556282043
Epoch 950, training loss: 62.8631591796875 = 0.30258774757385254 + 10.0 * 6.256056785583496
Epoch 950, val loss: 0.9445922374725342
Epoch 960, training loss: 62.848297119140625 = 0.2943926453590393 + 10.0 * 6.255390644073486
Epoch 960, val loss: 0.9451441168785095
Epoch 970, training loss: 62.87666320800781 = 0.28637176752090454 + 10.0 * 6.259028911590576
Epoch 970, val loss: 0.9458170533180237
Epoch 980, training loss: 62.8206787109375 = 0.2785177230834961 + 10.0 * 6.254216194152832
Epoch 980, val loss: 0.9465572237968445
Epoch 990, training loss: 62.7939453125 = 0.27083665132522583 + 10.0 * 6.252310752868652
Epoch 990, val loss: 0.9474091529846191
Epoch 1000, training loss: 62.78638458251953 = 0.2633574903011322 + 10.0 * 6.252302646636963
Epoch 1000, val loss: 0.9483902454376221
Epoch 1010, training loss: 62.79709243774414 = 0.2560689449310303 + 10.0 * 6.2541022300720215
Epoch 1010, val loss: 0.9495223164558411
Epoch 1020, training loss: 62.768890380859375 = 0.24890543520450592 + 10.0 * 6.251998424530029
Epoch 1020, val loss: 0.9506350755691528
Epoch 1030, training loss: 62.7578010559082 = 0.24192409217357635 + 10.0 * 6.251587867736816
Epoch 1030, val loss: 0.9519397616386414
Epoch 1040, training loss: 62.75925064086914 = 0.2351352423429489 + 10.0 * 6.252411842346191
Epoch 1040, val loss: 0.9532884359359741
Epoch 1050, training loss: 62.7354621887207 = 0.22853878140449524 + 10.0 * 6.250692367553711
Epoch 1050, val loss: 0.9548605680465698
Epoch 1060, training loss: 62.717594146728516 = 0.22215089201927185 + 10.0 * 6.249544143676758
Epoch 1060, val loss: 0.956568717956543
Epoch 1070, training loss: 62.697147369384766 = 0.2159312516450882 + 10.0 * 6.248121738433838
Epoch 1070, val loss: 0.9584023952484131
Epoch 1080, training loss: 62.71693801879883 = 0.2098984271287918 + 10.0 * 6.250703811645508
Epoch 1080, val loss: 0.9603508710861206
Epoch 1090, training loss: 62.7062873840332 = 0.20400117337703705 + 10.0 * 6.250228404998779
Epoch 1090, val loss: 0.9623227715492249
Epoch 1100, training loss: 62.66706848144531 = 0.19828666746616364 + 10.0 * 6.246878147125244
Epoch 1100, val loss: 0.9644883871078491
Epoch 1110, training loss: 62.650970458984375 = 0.19274599850177765 + 10.0 * 6.245822429656982
Epoch 1110, val loss: 0.9669131636619568
Epoch 1120, training loss: 62.64170455932617 = 0.1873890608549118 + 10.0 * 6.245431423187256
Epoch 1120, val loss: 0.9694061279296875
Epoch 1130, training loss: 62.67349624633789 = 0.1821935921907425 + 10.0 * 6.2491302490234375
Epoch 1130, val loss: 0.9720278382301331
Epoch 1140, training loss: 62.66273498535156 = 0.17711666226387024 + 10.0 * 6.248561859130859
Epoch 1140, val loss: 0.9746280312538147
Epoch 1150, training loss: 62.622859954833984 = 0.17217974364757538 + 10.0 * 6.245068073272705
Epoch 1150, val loss: 0.9774742722511292
Epoch 1160, training loss: 62.61363983154297 = 0.16740958392620087 + 10.0 * 6.244623184204102
Epoch 1160, val loss: 0.9803865551948547
Epoch 1170, training loss: 62.59080123901367 = 0.16280193626880646 + 10.0 * 6.242799758911133
Epoch 1170, val loss: 0.983403205871582
Epoch 1180, training loss: 62.612449645996094 = 0.15834419429302216 + 10.0 * 6.245410442352295
Epoch 1180, val loss: 0.9866583943367004
Epoch 1190, training loss: 62.58036422729492 = 0.15397295355796814 + 10.0 * 6.242639064788818
Epoch 1190, val loss: 0.9897075295448303
Epoch 1200, training loss: 62.57088851928711 = 0.14973145723342896 + 10.0 * 6.2421159744262695
Epoch 1200, val loss: 0.9930356740951538
Epoch 1210, training loss: 62.55874252319336 = 0.14566022157669067 + 10.0 * 6.241308212280273
Epoch 1210, val loss: 0.9964128136634827
Epoch 1220, training loss: 62.57085037231445 = 0.14169877767562866 + 10.0 * 6.242915153503418
Epoch 1220, val loss: 0.9998725056648254
Epoch 1230, training loss: 62.54292297363281 = 0.13783179223537445 + 10.0 * 6.240509033203125
Epoch 1230, val loss: 1.0034278631210327
Epoch 1240, training loss: 62.527217864990234 = 0.1340917944908142 + 10.0 * 6.239312648773193
Epoch 1240, val loss: 1.0071059465408325
Epoch 1250, training loss: 62.52352523803711 = 0.130471870303154 + 10.0 * 6.23930549621582
Epoch 1250, val loss: 1.0109601020812988
Epoch 1260, training loss: 62.59231948852539 = 0.12696118652820587 + 10.0 * 6.246535778045654
Epoch 1260, val loss: 1.0147958993911743
Epoch 1270, training loss: 62.527687072753906 = 0.12349833548069 + 10.0 * 6.240418910980225
Epoch 1270, val loss: 1.0185097455978394
Epoch 1280, training loss: 62.50273513793945 = 0.12015671283006668 + 10.0 * 6.238257884979248
Epoch 1280, val loss: 1.022365927696228
Epoch 1290, training loss: 62.48936462402344 = 0.11695057898759842 + 10.0 * 6.237241268157959
Epoch 1290, val loss: 1.0263457298278809
Epoch 1300, training loss: 62.49108123779297 = 0.11385194957256317 + 10.0 * 6.237722873687744
Epoch 1300, val loss: 1.0305360555648804
Epoch 1310, training loss: 62.511390686035156 = 0.11083335429430008 + 10.0 * 6.240056037902832
Epoch 1310, val loss: 1.0346143245697021
Epoch 1320, training loss: 62.48414993286133 = 0.10790631175041199 + 10.0 * 6.237624168395996
Epoch 1320, val loss: 1.0388600826263428
Epoch 1330, training loss: 62.4664306640625 = 0.10505689680576324 + 10.0 * 6.236137390136719
Epoch 1330, val loss: 1.0429855585098267
Epoch 1340, training loss: 62.50572967529297 = 0.10230398178100586 + 10.0 * 6.240342617034912
Epoch 1340, val loss: 1.0472559928894043
Epoch 1350, training loss: 62.466827392578125 = 0.0996318906545639 + 10.0 * 6.236719608306885
Epoch 1350, val loss: 1.05136239528656
Epoch 1360, training loss: 62.450809478759766 = 0.09702304750680923 + 10.0 * 6.235378742218018
Epoch 1360, val loss: 1.0556483268737793
Epoch 1370, training loss: 62.446533203125 = 0.09452080726623535 + 10.0 * 6.235201358795166
Epoch 1370, val loss: 1.0600354671478271
Epoch 1380, training loss: 62.44010925292969 = 0.09207833558320999 + 10.0 * 6.234803199768066
Epoch 1380, val loss: 1.0643149614334106
Epoch 1390, training loss: 62.4255485534668 = 0.08970257639884949 + 10.0 * 6.233584403991699
Epoch 1390, val loss: 1.0686100721359253
Epoch 1400, training loss: 62.46717071533203 = 0.08740939944982529 + 10.0 * 6.23797607421875
Epoch 1400, val loss: 1.0729565620422363
Epoch 1410, training loss: 62.44021224975586 = 0.0851641371846199 + 10.0 * 6.235505104064941
Epoch 1410, val loss: 1.077330470085144
Epoch 1420, training loss: 62.40837478637695 = 0.08298805356025696 + 10.0 * 6.23253870010376
Epoch 1420, val loss: 1.0815919637680054
Epoch 1430, training loss: 62.39904022216797 = 0.08089301735162735 + 10.0 * 6.231814384460449
Epoch 1430, val loss: 1.086022973060608
Epoch 1440, training loss: 62.429630279541016 = 0.07886552065610886 + 10.0 * 6.235076427459717
Epoch 1440, val loss: 1.0902361869812012
Epoch 1450, training loss: 62.44054412841797 = 0.07686083018779755 + 10.0 * 6.236368179321289
Epoch 1450, val loss: 1.0945665836334229
Epoch 1460, training loss: 62.396484375 = 0.0749368816614151 + 10.0 * 6.232154846191406
Epoch 1460, val loss: 1.0988495349884033
Epoch 1470, training loss: 62.37919998168945 = 0.07306288927793503 + 10.0 * 6.230613708496094
Epoch 1470, val loss: 1.1032332181930542
Epoch 1480, training loss: 62.367958068847656 = 0.07127337902784348 + 10.0 * 6.229668617248535
Epoch 1480, val loss: 1.1076209545135498
Epoch 1490, training loss: 62.366851806640625 = 0.06953427940607071 + 10.0 * 6.229731559753418
Epoch 1490, val loss: 1.11204993724823
Epoch 1500, training loss: 62.48348617553711 = 0.067858025431633 + 10.0 * 6.241562843322754
Epoch 1500, val loss: 1.116299033164978
Epoch 1510, training loss: 62.36832046508789 = 0.06615894287824631 + 10.0 * 6.230216026306152
Epoch 1510, val loss: 1.120273232460022
Epoch 1520, training loss: 62.356956481933594 = 0.06455143541097641 + 10.0 * 6.229240417480469
Epoch 1520, val loss: 1.1248763799667358
Epoch 1530, training loss: 62.347084045410156 = 0.06300495564937592 + 10.0 * 6.228407859802246
Epoch 1530, val loss: 1.1291614770889282
Epoch 1540, training loss: 62.35258102416992 = 0.061513762921094894 + 10.0 * 6.229106903076172
Epoch 1540, val loss: 1.133571982383728
Epoch 1550, training loss: 62.3627815246582 = 0.06005680933594704 + 10.0 * 6.23027229309082
Epoch 1550, val loss: 1.1378319263458252
Epoch 1560, training loss: 62.34769821166992 = 0.058633048087358475 + 10.0 * 6.228906631469727
Epoch 1560, val loss: 1.141947865486145
Epoch 1570, training loss: 62.383018493652344 = 0.05726775527000427 + 10.0 * 6.232575416564941
Epoch 1570, val loss: 1.1463836431503296
Epoch 1580, training loss: 62.326416015625 = 0.05590303987264633 + 10.0 * 6.227051258087158
Epoch 1580, val loss: 1.1502361297607422
Epoch 1590, training loss: 62.31822204589844 = 0.05460270494222641 + 10.0 * 6.2263617515563965
Epoch 1590, val loss: 1.1546558141708374
Epoch 1600, training loss: 62.31261444091797 = 0.053352974355220795 + 10.0 * 6.225926399230957
Epoch 1600, val loss: 1.1588413715362549
Epoch 1610, training loss: 62.30914306640625 = 0.052141256630420685 + 10.0 * 6.2256999015808105
Epoch 1610, val loss: 1.163092017173767
Epoch 1620, training loss: 62.32585906982422 = 0.05097343400120735 + 10.0 * 6.2274885177612305
Epoch 1620, val loss: 1.1673510074615479
Epoch 1630, training loss: 62.3590202331543 = 0.049812719225883484 + 10.0 * 6.230920791625977
Epoch 1630, val loss: 1.1714485883712769
Epoch 1640, training loss: 62.32707977294922 = 0.04867101460695267 + 10.0 * 6.227840900421143
Epoch 1640, val loss: 1.175339937210083
Epoch 1650, training loss: 62.30250549316406 = 0.04757853224873543 + 10.0 * 6.22549295425415
Epoch 1650, val loss: 1.1796495914459229
Epoch 1660, training loss: 62.28943634033203 = 0.046532273292541504 + 10.0 * 6.224290370941162
Epoch 1660, val loss: 1.1837974786758423
Epoch 1670, training loss: 62.28721237182617 = 0.04552123695611954 + 10.0 * 6.2241692543029785
Epoch 1670, val loss: 1.18796706199646
Epoch 1680, training loss: 62.346317291259766 = 0.04454544559121132 + 10.0 * 6.230177402496338
Epoch 1680, val loss: 1.1921358108520508
Epoch 1690, training loss: 62.321956634521484 = 0.04356085881590843 + 10.0 * 6.227839469909668
Epoch 1690, val loss: 1.1960700750350952
Epoch 1700, training loss: 62.29414367675781 = 0.042612362653017044 + 10.0 * 6.225152969360352
Epoch 1700, val loss: 1.2000298500061035
Epoch 1710, training loss: 62.28057861328125 = 0.04169906675815582 + 10.0 * 6.223887920379639
Epoch 1710, val loss: 1.204100251197815
Epoch 1720, training loss: 62.2762336730957 = 0.04082157462835312 + 10.0 * 6.223541259765625
Epoch 1720, val loss: 1.208074927330017
Epoch 1730, training loss: 62.307518005371094 = 0.039965249598026276 + 10.0 * 6.226755142211914
Epoch 1730, val loss: 1.2120124101638794
Epoch 1740, training loss: 62.27186584472656 = 0.039129771292209625 + 10.0 * 6.223273277282715
Epoch 1740, val loss: 1.2160935401916504
Epoch 1750, training loss: 62.27043151855469 = 0.03831935673952103 + 10.0 * 6.223211288452148
Epoch 1750, val loss: 1.2199782133102417
Epoch 1760, training loss: 62.27736282348633 = 0.03752937912940979 + 10.0 * 6.223983287811279
Epoch 1760, val loss: 1.223922610282898
Epoch 1770, training loss: 62.26020431518555 = 0.03676460310816765 + 10.0 * 6.222343921661377
Epoch 1770, val loss: 1.2278460264205933
Epoch 1780, training loss: 62.25199508666992 = 0.03602344170212746 + 10.0 * 6.221597194671631
Epoch 1780, val loss: 1.2317626476287842
Epoch 1790, training loss: 62.29471206665039 = 0.03531178459525108 + 10.0 * 6.225939750671387
Epoch 1790, val loss: 1.235629916191101
Epoch 1800, training loss: 62.25742721557617 = 0.0345890186727047 + 10.0 * 6.222283840179443
Epoch 1800, val loss: 1.2393522262573242
Epoch 1810, training loss: 62.249671936035156 = 0.033896684646606445 + 10.0 * 6.2215776443481445
Epoch 1810, val loss: 1.2429560422897339
Epoch 1820, training loss: 62.249393463134766 = 0.03322954475879669 + 10.0 * 6.221616268157959
Epoch 1820, val loss: 1.2468969821929932
Epoch 1830, training loss: 62.277366638183594 = 0.03258813917636871 + 10.0 * 6.224477767944336
Epoch 1830, val loss: 1.2504140138626099
Epoch 1840, training loss: 62.23601150512695 = 0.03195247799158096 + 10.0 * 6.2204060554504395
Epoch 1840, val loss: 1.254272699356079
Epoch 1850, training loss: 62.23207092285156 = 0.031342215836048126 + 10.0 * 6.2200727462768555
Epoch 1850, val loss: 1.2580057382583618
Epoch 1860, training loss: 62.32419967651367 = 0.030758647248148918 + 10.0 * 6.229344367980957
Epoch 1860, val loss: 1.2616907358169556
Epoch 1870, training loss: 62.26142883300781 = 0.030150284990668297 + 10.0 * 6.223127841949463
Epoch 1870, val loss: 1.2651509046554565
Epoch 1880, training loss: 62.22593307495117 = 0.029578767716884613 + 10.0 * 6.219635486602783
Epoch 1880, val loss: 1.268741488456726
Epoch 1890, training loss: 62.216697692871094 = 0.02902708388864994 + 10.0 * 6.218767166137695
Epoch 1890, val loss: 1.2723835706710815
Epoch 1900, training loss: 62.2137451171875 = 0.028497321531176567 + 10.0 * 6.218524932861328
Epoch 1900, val loss: 1.2759720087051392
Epoch 1910, training loss: 62.2440299987793 = 0.027987459674477577 + 10.0 * 6.221604347229004
Epoch 1910, val loss: 1.2794783115386963
Epoch 1920, training loss: 62.21554946899414 = 0.02746925689280033 + 10.0 * 6.218808174133301
Epoch 1920, val loss: 1.2831422090530396
Epoch 1930, training loss: 62.21272659301758 = 0.02697197161614895 + 10.0 * 6.218575477600098
Epoch 1930, val loss: 1.286589503288269
Epoch 1940, training loss: 62.233184814453125 = 0.026488961651921272 + 10.0 * 6.220669746398926
Epoch 1940, val loss: 1.2901328802108765
Epoch 1950, training loss: 62.206974029541016 = 0.02601657062768936 + 10.0 * 6.218095779418945
Epoch 1950, val loss: 1.293591022491455
Epoch 1960, training loss: 62.238983154296875 = 0.025562835857272148 + 10.0 * 6.221342086791992
Epoch 1960, val loss: 1.2970523834228516
Epoch 1970, training loss: 62.21352767944336 = 0.025107132270932198 + 10.0 * 6.218842029571533
Epoch 1970, val loss: 1.300278902053833
Epoch 1980, training loss: 62.21562957763672 = 0.024661559611558914 + 10.0 * 6.219096660614014
Epoch 1980, val loss: 1.3037264347076416
Epoch 1990, training loss: 62.20052719116211 = 0.02423950284719467 + 10.0 * 6.2176289558410645
Epoch 1990, val loss: 1.307074785232544
Epoch 2000, training loss: 62.18901062011719 = 0.023827286437153816 + 10.0 * 6.216518402099609
Epoch 2000, val loss: 1.3105944395065308
Epoch 2010, training loss: 62.19256591796875 = 0.023429643362760544 + 10.0 * 6.21691370010376
Epoch 2010, val loss: 1.3138757944107056
Epoch 2020, training loss: 62.255592346191406 = 0.02304334007203579 + 10.0 * 6.223254680633545
Epoch 2020, val loss: 1.3172012567520142
Epoch 2030, training loss: 62.20745849609375 = 0.02264438010752201 + 10.0 * 6.218481540679932
Epoch 2030, val loss: 1.320353388786316
Epoch 2040, training loss: 62.186031341552734 = 0.022263994440436363 + 10.0 * 6.216376781463623
Epoch 2040, val loss: 1.3235937356948853
Epoch 2050, training loss: 62.21919250488281 = 0.021895909681916237 + 10.0 * 6.219729423522949
Epoch 2050, val loss: 1.3268667459487915
Epoch 2060, training loss: 62.183319091796875 = 0.021535465493798256 + 10.0 * 6.2161784172058105
Epoch 2060, val loss: 1.330086588859558
Epoch 2070, training loss: 62.18480682373047 = 0.021184774115681648 + 10.0 * 6.216362476348877
Epoch 2070, val loss: 1.3332103490829468
Epoch 2080, training loss: 62.173194885253906 = 0.020843584090471268 + 10.0 * 6.215235233306885
Epoch 2080, val loss: 1.3365319967269897
Epoch 2090, training loss: 62.17487335205078 = 0.020513640716671944 + 10.0 * 6.215435981750488
Epoch 2090, val loss: 1.3397119045257568
Epoch 2100, training loss: 62.228763580322266 = 0.020190611481666565 + 10.0 * 6.2208571434021
Epoch 2100, val loss: 1.3428891897201538
Epoch 2110, training loss: 62.17507553100586 = 0.019868770614266396 + 10.0 * 6.215520858764648
Epoch 2110, val loss: 1.345977783203125
Epoch 2120, training loss: 62.17800521850586 = 0.0195587407797575 + 10.0 * 6.215844631195068
Epoch 2120, val loss: 1.349084496498108
Epoch 2130, training loss: 62.18430709838867 = 0.019254636019468307 + 10.0 * 6.216505527496338
Epoch 2130, val loss: 1.3523321151733398
Epoch 2140, training loss: 62.16459274291992 = 0.01895453967154026 + 10.0 * 6.214563846588135
Epoch 2140, val loss: 1.3551603555679321
Epoch 2150, training loss: 62.16627883911133 = 0.01866581290960312 + 10.0 * 6.214761257171631
Epoch 2150, val loss: 1.3582884073257446
Epoch 2160, training loss: 62.16799545288086 = 0.018384654074907303 + 10.0 * 6.214961051940918
Epoch 2160, val loss: 1.3614835739135742
Epoch 2170, training loss: 62.192012786865234 = 0.018108487129211426 + 10.0 * 6.217390537261963
Epoch 2170, val loss: 1.3642305135726929
Epoch 2180, training loss: 62.16791534423828 = 0.017835335806012154 + 10.0 * 6.21500825881958
Epoch 2180, val loss: 1.3673957586288452
Epoch 2190, training loss: 62.17389678955078 = 0.017568835988640785 + 10.0 * 6.215632438659668
Epoch 2190, val loss: 1.370269775390625
Epoch 2200, training loss: 62.174556732177734 = 0.017307698726654053 + 10.0 * 6.215724945068359
Epoch 2200, val loss: 1.3731709718704224
Epoch 2210, training loss: 62.14620590209961 = 0.017053944990038872 + 10.0 * 6.212914943695068
Epoch 2210, val loss: 1.37613046169281
Epoch 2220, training loss: 62.14656066894531 = 0.01680983230471611 + 10.0 * 6.212975025177002
Epoch 2220, val loss: 1.3791385889053345
Epoch 2230, training loss: 62.16079330444336 = 0.016572117805480957 + 10.0 * 6.214422225952148
Epoch 2230, val loss: 1.3819081783294678
Epoch 2240, training loss: 62.149627685546875 = 0.016332607716321945 + 10.0 * 6.213329792022705
Epoch 2240, val loss: 1.3847899436950684
Epoch 2250, training loss: 62.150634765625 = 0.01609896868467331 + 10.0 * 6.213453769683838
Epoch 2250, val loss: 1.3875473737716675
Epoch 2260, training loss: 62.1495246887207 = 0.015872789546847343 + 10.0 * 6.213365077972412
Epoch 2260, val loss: 1.3905396461486816
Epoch 2270, training loss: 62.14003372192383 = 0.015650993213057518 + 10.0 * 6.212438106536865
Epoch 2270, val loss: 1.3933944702148438
Epoch 2280, training loss: 62.17707061767578 = 0.015440455637872219 + 10.0 * 6.21616268157959
Epoch 2280, val loss: 1.3963655233383179
Epoch 2290, training loss: 62.15987777709961 = 0.01522153615951538 + 10.0 * 6.214465618133545
Epoch 2290, val loss: 1.3987091779708862
Epoch 2300, training loss: 62.12983322143555 = 0.01500590518116951 + 10.0 * 6.211482524871826
Epoch 2300, val loss: 1.401667594909668
Epoch 2310, training loss: 62.12166213989258 = 0.01480222586542368 + 10.0 * 6.210686206817627
Epoch 2310, val loss: 1.4044075012207031
Epoch 2320, training loss: 62.12087631225586 = 0.014606036245822906 + 10.0 * 6.21062707901001
Epoch 2320, val loss: 1.407195806503296
Epoch 2330, training loss: 62.142845153808594 = 0.014414247125387192 + 10.0 * 6.21284294128418
Epoch 2330, val loss: 1.4098960161209106
Epoch 2340, training loss: 62.14927673339844 = 0.014220869168639183 + 10.0 * 6.213505744934082
Epoch 2340, val loss: 1.412521481513977
Epoch 2350, training loss: 62.13164138793945 = 0.014032314531505108 + 10.0 * 6.211760997772217
Epoch 2350, val loss: 1.4152650833129883
Epoch 2360, training loss: 62.15957260131836 = 0.013847931288182735 + 10.0 * 6.214572429656982
Epoch 2360, val loss: 1.4180047512054443
Epoch 2370, training loss: 62.11418914794922 = 0.013665232807397842 + 10.0 * 6.210052490234375
Epoch 2370, val loss: 1.4203743934631348
Epoch 2380, training loss: 62.115325927734375 = 0.013490358367562294 + 10.0 * 6.210183620452881
Epoch 2380, val loss: 1.423065423965454
Epoch 2390, training loss: 62.111961364746094 = 0.013320805504918098 + 10.0 * 6.209864139556885
Epoch 2390, val loss: 1.4257339239120483
Epoch 2400, training loss: 62.17198181152344 = 0.013159885071218014 + 10.0 * 6.215882301330566
Epoch 2400, val loss: 1.428241491317749
Epoch 2410, training loss: 62.11839294433594 = 0.012985434383153915 + 10.0 * 6.210540771484375
Epoch 2410, val loss: 1.4308946132659912
Epoch 2420, training loss: 62.10868835449219 = 0.012822471559047699 + 10.0 * 6.2095866203308105
Epoch 2420, val loss: 1.433374047279358
Epoch 2430, training loss: 62.17778396606445 = 0.012667950242757797 + 10.0 * 6.2165117263793945
Epoch 2430, val loss: 1.4360334873199463
Epoch 2440, training loss: 62.111328125 = 0.012502073310315609 + 10.0 * 6.209882736206055
Epoch 2440, val loss: 1.4382810592651367
Epoch 2450, training loss: 62.10378646850586 = 0.012347002513706684 + 10.0 * 6.20914363861084
Epoch 2450, val loss: 1.4409255981445312
Epoch 2460, training loss: 62.09706115722656 = 0.012199767865240574 + 10.0 * 6.208486080169678
Epoch 2460, val loss: 1.4435009956359863
Epoch 2470, training loss: 62.11001968383789 = 0.012055628933012486 + 10.0 * 6.20979642868042
Epoch 2470, val loss: 1.4460062980651855
Epoch 2480, training loss: 62.1572151184082 = 0.011912482790648937 + 10.0 * 6.214529991149902
Epoch 2480, val loss: 1.448445439338684
Epoch 2490, training loss: 62.12115478515625 = 0.01176503486931324 + 10.0 * 6.210938930511475
Epoch 2490, val loss: 1.4504961967468262
Epoch 2500, training loss: 62.099853515625 = 0.011626136489212513 + 10.0 * 6.208822727203369
Epoch 2500, val loss: 1.453070878982544
Epoch 2510, training loss: 62.095333099365234 = 0.011492210440337658 + 10.0 * 6.208384037017822
Epoch 2510, val loss: 1.455518364906311
Epoch 2520, training loss: 62.11553192138672 = 0.011362822726368904 + 10.0 * 6.210416793823242
Epoch 2520, val loss: 1.4578437805175781
Epoch 2530, training loss: 62.14440155029297 = 0.011228185147047043 + 10.0 * 6.213317394256592
Epoch 2530, val loss: 1.4600989818572998
Epoch 2540, training loss: 62.0998649597168 = 0.011090505868196487 + 10.0 * 6.2088775634765625
Epoch 2540, val loss: 1.4625333547592163
Epoch 2550, training loss: 62.087921142578125 = 0.010963338427245617 + 10.0 * 6.207695960998535
Epoch 2550, val loss: 1.4647216796875
Epoch 2560, training loss: 62.08483123779297 = 0.010839911177754402 + 10.0 * 6.207398891448975
Epoch 2560, val loss: 1.4671872854232788
Epoch 2570, training loss: 62.1025505065918 = 0.010721039026975632 + 10.0 * 6.209183216094971
Epoch 2570, val loss: 1.4695779085159302
Epoch 2580, training loss: 62.09395980834961 = 0.01059986837208271 + 10.0 * 6.208335876464844
Epoch 2580, val loss: 1.4717350006103516
Epoch 2590, training loss: 62.0882568359375 = 0.010483699850738049 + 10.0 * 6.207777500152588
Epoch 2590, val loss: 1.4740742444992065
Epoch 2600, training loss: 62.083282470703125 = 0.01036794949322939 + 10.0 * 6.207291603088379
Epoch 2600, val loss: 1.4763579368591309
Epoch 2610, training loss: 62.11427688598633 = 0.01025922317057848 + 10.0 * 6.210402011871338
Epoch 2610, val loss: 1.4783813953399658
Epoch 2620, training loss: 62.088043212890625 = 0.010144746862351894 + 10.0 * 6.207789897918701
Epoch 2620, val loss: 1.4809306859970093
Epoch 2630, training loss: 62.15083694458008 = 0.010037685744464397 + 10.0 * 6.214079856872559
Epoch 2630, val loss: 1.482839822769165
Epoch 2640, training loss: 62.08967971801758 = 0.009921727702021599 + 10.0 * 6.2079758644104
Epoch 2640, val loss: 1.4852744340896606
Epoch 2650, training loss: 62.0748176574707 = 0.009814468212425709 + 10.0 * 6.206500053405762
Epoch 2650, val loss: 1.487191081047058
Epoch 2660, training loss: 62.06871032714844 = 0.009711899794638157 + 10.0 * 6.205899715423584
Epoch 2660, val loss: 1.489615559577942
Epoch 2670, training loss: 62.070274353027344 = 0.009611864574253559 + 10.0 * 6.206066131591797
Epoch 2670, val loss: 1.49168860912323
Epoch 2680, training loss: 62.12089157104492 = 0.009514005854725838 + 10.0 * 6.211137771606445
Epoch 2680, val loss: 1.4937444925308228
Epoch 2690, training loss: 62.07889175415039 = 0.009412002749741077 + 10.0 * 6.206948280334473
Epoch 2690, val loss: 1.4959688186645508
Epoch 2700, training loss: 62.08324432373047 = 0.009312506765127182 + 10.0 * 6.207393169403076
Epoch 2700, val loss: 1.4979671239852905
Epoch 2710, training loss: 62.0721435546875 = 0.009217028506100178 + 10.0 * 6.206292629241943
Epoch 2710, val loss: 1.5002108812332153
Epoch 2720, training loss: 62.06945037841797 = 0.009124471805989742 + 10.0 * 6.206032752990723
Epoch 2720, val loss: 1.502353549003601
Epoch 2730, training loss: 62.081878662109375 = 0.009033824317157269 + 10.0 * 6.207284450531006
Epoch 2730, val loss: 1.5044395923614502
Epoch 2740, training loss: 62.06254196166992 = 0.008941077627241611 + 10.0 * 6.205359935760498
Epoch 2740, val loss: 1.5065655708312988
Epoch 2750, training loss: 62.10855484008789 = 0.008853242732584476 + 10.0 * 6.209969997406006
Epoch 2750, val loss: 1.5085515975952148
Epoch 2760, training loss: 62.07077407836914 = 0.008762907236814499 + 10.0 * 6.206201076507568
Epoch 2760, val loss: 1.510433316230774
Epoch 2770, training loss: 62.06081008911133 = 0.008677122183144093 + 10.0 * 6.2052130699157715
Epoch 2770, val loss: 1.5124084949493408
Epoch 2780, training loss: 62.06086730957031 = 0.00859153363853693 + 10.0 * 6.205227851867676
Epoch 2780, val loss: 1.5146169662475586
Epoch 2790, training loss: 62.11893081665039 = 0.008510765619575977 + 10.0 * 6.2110419273376465
Epoch 2790, val loss: 1.5166298151016235
Epoch 2800, training loss: 62.06221389770508 = 0.008424907922744751 + 10.0 * 6.205378532409668
Epoch 2800, val loss: 1.5183669328689575
Epoch 2810, training loss: 62.047393798828125 = 0.008344443514943123 + 10.0 * 6.20390510559082
Epoch 2810, val loss: 1.5204893350601196
Epoch 2820, training loss: 62.04817581176758 = 0.008266335353255272 + 10.0 * 6.203990936279297
Epoch 2820, val loss: 1.5225372314453125
Epoch 2830, training loss: 62.13311767578125 = 0.008192497305572033 + 10.0 * 6.2124924659729
Epoch 2830, val loss: 1.524587631225586
Epoch 2840, training loss: 62.10783004760742 = 0.008113100193440914 + 10.0 * 6.2099714279174805
Epoch 2840, val loss: 1.5261379480361938
Epoch 2850, training loss: 62.04999542236328 = 0.008031017147004604 + 10.0 * 6.204196453094482
Epoch 2850, val loss: 1.5280293226242065
Epoch 2860, training loss: 62.04054260253906 = 0.007955530658364296 + 10.0 * 6.203258991241455
Epoch 2860, val loss: 1.5298683643341064
Epoch 2870, training loss: 62.041194915771484 = 0.007883934304118156 + 10.0 * 6.203330993652344
Epoch 2870, val loss: 1.5319329500198364
Epoch 2880, training loss: 62.07537078857422 = 0.007814178243279457 + 10.0 * 6.206755638122559
Epoch 2880, val loss: 1.5338484048843384
Epoch 2890, training loss: 62.06315612792969 = 0.007740687113255262 + 10.0 * 6.205541610717773
Epoch 2890, val loss: 1.5355172157287598
Epoch 2900, training loss: 62.056243896484375 = 0.007670206017792225 + 10.0 * 6.204857349395752
Epoch 2900, val loss: 1.5373417139053345
Epoch 2910, training loss: 62.0565299987793 = 0.007600854616612196 + 10.0 * 6.204892635345459
Epoch 2910, val loss: 1.5393377542495728
Epoch 2920, training loss: 62.0421028137207 = 0.007532884832471609 + 10.0 * 6.203456878662109
Epoch 2920, val loss: 1.5410635471343994
Epoch 2930, training loss: 62.035186767578125 = 0.007466226350516081 + 10.0 * 6.20277214050293
Epoch 2930, val loss: 1.5430095195770264
Epoch 2940, training loss: 62.07687759399414 = 0.007402855437248945 + 10.0 * 6.206947326660156
Epoch 2940, val loss: 1.5448558330535889
Epoch 2950, training loss: 62.03507995605469 = 0.007335549686104059 + 10.0 * 6.202774524688721
Epoch 2950, val loss: 1.5463335514068604
Epoch 2960, training loss: 62.0489501953125 = 0.007271221373230219 + 10.0 * 6.20416784286499
Epoch 2960, val loss: 1.5481200218200684
Epoch 2970, training loss: 62.05851364135742 = 0.007207571994513273 + 10.0 * 6.205130577087402
Epoch 2970, val loss: 1.5500825643539429
Epoch 2980, training loss: 62.0426025390625 = 0.007146838121116161 + 10.0 * 6.203545570373535
Epoch 2980, val loss: 1.5515499114990234
Epoch 2990, training loss: 62.057403564453125 = 0.007085664663463831 + 10.0 * 6.205031871795654
Epoch 2990, val loss: 1.5533782243728638
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 87.92817687988281 = 1.9594860076904297 + 10.0 * 8.596868515014648
Epoch 0, val loss: 1.955186367034912
Epoch 10, training loss: 87.91300964355469 = 1.9490491151809692 + 10.0 * 8.596395492553711
Epoch 10, val loss: 1.9445985555648804
Epoch 20, training loss: 87.85892486572266 = 1.9368340969085693 + 10.0 * 8.592208862304688
Epoch 20, val loss: 1.932043433189392
Epoch 30, training loss: 87.56011199951172 = 1.921549677848816 + 10.0 * 8.56385612487793
Epoch 30, val loss: 1.9164797067642212
Epoch 40, training loss: 86.22648620605469 = 1.9035917520523071 + 10.0 * 8.432289123535156
Epoch 40, val loss: 1.8994522094726562
Epoch 50, training loss: 82.62541198730469 = 1.8847401142120361 + 10.0 * 8.074067115783691
Epoch 50, val loss: 1.881866216659546
Epoch 60, training loss: 80.89189910888672 = 1.8666925430297852 + 10.0 * 7.902521133422852
Epoch 60, val loss: 1.866070032119751
Epoch 70, training loss: 77.6029281616211 = 1.8535610437393188 + 10.0 * 7.574936866760254
Epoch 70, val loss: 1.855044960975647
Epoch 80, training loss: 74.3394546508789 = 1.8432421684265137 + 10.0 * 7.249621868133545
Epoch 80, val loss: 1.8470250368118286
Epoch 90, training loss: 71.74848937988281 = 1.8350167274475098 + 10.0 * 6.991347789764404
Epoch 90, val loss: 1.8395824432373047
Epoch 100, training loss: 70.2896957397461 = 1.8267887830734253 + 10.0 * 6.846290588378906
Epoch 100, val loss: 1.8317625522613525
Epoch 110, training loss: 69.37898254394531 = 1.8175524473190308 + 10.0 * 6.756143569946289
Epoch 110, val loss: 1.8231195211410522
Epoch 120, training loss: 68.7164077758789 = 1.809055209159851 + 10.0 * 6.690735340118408
Epoch 120, val loss: 1.815629243850708
Epoch 130, training loss: 68.30986022949219 = 1.8011571168899536 + 10.0 * 6.650870323181152
Epoch 130, val loss: 1.8086302280426025
Epoch 140, training loss: 67.95496368408203 = 1.793028712272644 + 10.0 * 6.6161932945251465
Epoch 140, val loss: 1.801422119140625
Epoch 150, training loss: 67.64622497558594 = 1.7850784063339233 + 10.0 * 6.586114883422852
Epoch 150, val loss: 1.794188380241394
Epoch 160, training loss: 67.36595153808594 = 1.7771481275558472 + 10.0 * 6.558880805969238
Epoch 160, val loss: 1.7869809865951538
Epoch 170, training loss: 67.09242248535156 = 1.7689450979232788 + 10.0 * 6.532347679138184
Epoch 170, val loss: 1.7796233892440796
Epoch 180, training loss: 67.05447387695312 = 1.7602208852767944 + 10.0 * 6.529425144195557
Epoch 180, val loss: 1.7721070051193237
Epoch 190, training loss: 66.67903900146484 = 1.7506701946258545 + 10.0 * 6.4928364753723145
Epoch 190, val loss: 1.7635648250579834
Epoch 200, training loss: 66.50863647460938 = 1.7401001453399658 + 10.0 * 6.476853847503662
Epoch 200, val loss: 1.754590392112732
Epoch 210, training loss: 66.3554458618164 = 1.7286913394927979 + 10.0 * 6.46267557144165
Epoch 210, val loss: 1.7448809146881104
Epoch 220, training loss: 66.2112045288086 = 1.7161569595336914 + 10.0 * 6.449504852294922
Epoch 220, val loss: 1.7342479228973389
Epoch 230, training loss: 66.0794677734375 = 1.7024999856948853 + 10.0 * 6.437696933746338
Epoch 230, val loss: 1.7226688861846924
Epoch 240, training loss: 65.98871612548828 = 1.687600016593933 + 10.0 * 6.430111408233643
Epoch 240, val loss: 1.7101198434829712
Epoch 250, training loss: 65.84898376464844 = 1.6714235544204712 + 10.0 * 6.417756080627441
Epoch 250, val loss: 1.6965088844299316
Epoch 260, training loss: 65.75296783447266 = 1.6540701389312744 + 10.0 * 6.409890174865723
Epoch 260, val loss: 1.6819713115692139
Epoch 270, training loss: 65.63902282714844 = 1.6351865530014038 + 10.0 * 6.400383949279785
Epoch 270, val loss: 1.6662967205047607
Epoch 280, training loss: 65.55233001708984 = 1.6150320768356323 + 10.0 * 6.3937296867370605
Epoch 280, val loss: 1.6496353149414062
Epoch 290, training loss: 65.45634460449219 = 1.593529224395752 + 10.0 * 6.386281490325928
Epoch 290, val loss: 1.6320080757141113
Epoch 300, training loss: 65.37271118164062 = 1.570695400238037 + 10.0 * 6.38020133972168
Epoch 300, val loss: 1.6134226322174072
Epoch 310, training loss: 65.2969741821289 = 1.5465466976165771 + 10.0 * 6.375042915344238
Epoch 310, val loss: 1.593888759613037
Epoch 320, training loss: 65.21568298339844 = 1.5214496850967407 + 10.0 * 6.369423866271973
Epoch 320, val loss: 1.5737055540084839
Epoch 330, training loss: 65.1467514038086 = 1.4956083297729492 + 10.0 * 6.365114688873291
Epoch 330, val loss: 1.5531527996063232
Epoch 340, training loss: 65.0709228515625 = 1.4689141511917114 + 10.0 * 6.360200881958008
Epoch 340, val loss: 1.5321661233901978
Epoch 350, training loss: 65.00008392333984 = 1.4415768384933472 + 10.0 * 6.355850696563721
Epoch 350, val loss: 1.5108752250671387
Epoch 360, training loss: 64.94806671142578 = 1.413667917251587 + 10.0 * 6.353439807891846
Epoch 360, val loss: 1.4894732236862183
Epoch 370, training loss: 64.87525939941406 = 1.3855113983154297 + 10.0 * 6.348974704742432
Epoch 370, val loss: 1.467979907989502
Epoch 380, training loss: 64.80705261230469 = 1.3571771383285522 + 10.0 * 6.344987392425537
Epoch 380, val loss: 1.446666955947876
Epoch 390, training loss: 64.74276733398438 = 1.328749418258667 + 10.0 * 6.341402053833008
Epoch 390, val loss: 1.4255776405334473
Epoch 400, training loss: 64.72427368164062 = 1.3002402782440186 + 10.0 * 6.342403411865234
Epoch 400, val loss: 1.4046963453292847
Epoch 410, training loss: 64.65869140625 = 1.2718634605407715 + 10.0 * 6.338682651519775
Epoch 410, val loss: 1.384072184562683
Epoch 420, training loss: 64.57763671875 = 1.2439098358154297 + 10.0 * 6.333372592926025
Epoch 420, val loss: 1.3641207218170166
Epoch 430, training loss: 64.51542663574219 = 1.2161771059036255 + 10.0 * 6.329925060272217
Epoch 430, val loss: 1.344689965248108
Epoch 440, training loss: 64.49114227294922 = 1.188743233680725 + 10.0 * 6.330239772796631
Epoch 440, val loss: 1.3258436918258667
Epoch 450, training loss: 64.41919708251953 = 1.1618620157241821 + 10.0 * 6.325733184814453
Epoch 450, val loss: 1.3075840473175049
Epoch 460, training loss: 64.3714599609375 = 1.1352216005325317 + 10.0 * 6.3236236572265625
Epoch 460, val loss: 1.289965033531189
Epoch 470, training loss: 64.31539916992188 = 1.1093591451644897 + 10.0 * 6.320603847503662
Epoch 470, val loss: 1.2731143236160278
Epoch 480, training loss: 64.26695251464844 = 1.0837957859039307 + 10.0 * 6.3183159828186035
Epoch 480, val loss: 1.2568918466567993
Epoch 490, training loss: 64.21723175048828 = 1.0587713718414307 + 10.0 * 6.3158464431762695
Epoch 490, val loss: 1.2412904500961304
Epoch 500, training loss: 64.18093872070312 = 1.034393310546875 + 10.0 * 6.314654350280762
Epoch 500, val loss: 1.2264986038208008
Epoch 510, training loss: 64.1566390991211 = 1.0106807947158813 + 10.0 * 6.314595699310303
Epoch 510, val loss: 1.2124731540679932
Epoch 520, training loss: 64.09657287597656 = 0.9872564077377319 + 10.0 * 6.31093168258667
Epoch 520, val loss: 1.1988720893859863
Epoch 530, training loss: 64.0478286743164 = 0.9647072553634644 + 10.0 * 6.30831241607666
Epoch 530, val loss: 1.1864516735076904
Epoch 540, training loss: 63.999237060546875 = 0.9426259994506836 + 10.0 * 6.305661201477051
Epoch 540, val loss: 1.1745959520339966
Epoch 550, training loss: 63.9697265625 = 0.9211037755012512 + 10.0 * 6.304862022399902
Epoch 550, val loss: 1.1634366512298584
Epoch 560, training loss: 64.00074768066406 = 0.9001162052154541 + 10.0 * 6.310063362121582
Epoch 560, val loss: 1.1530402898788452
Epoch 570, training loss: 63.91355514526367 = 0.8793017268180847 + 10.0 * 6.303425312042236
Epoch 570, val loss: 1.1430078744888306
Epoch 580, training loss: 63.849578857421875 = 0.8593930006027222 + 10.0 * 6.299018383026123
Epoch 580, val loss: 1.1340837478637695
Epoch 590, training loss: 63.81786346435547 = 0.8398801684379578 + 10.0 * 6.297798156738281
Epoch 590, val loss: 1.1257258653640747
Epoch 600, training loss: 63.82096481323242 = 0.8208023309707642 + 10.0 * 6.300016403198242
Epoch 600, val loss: 1.1180404424667358
Epoch 610, training loss: 63.752166748046875 = 0.8021014332771301 + 10.0 * 6.29500675201416
Epoch 610, val loss: 1.1107032299041748
Epoch 620, training loss: 63.712730407714844 = 0.7838136553764343 + 10.0 * 6.292891502380371
Epoch 620, val loss: 1.1041501760482788
Epoch 630, training loss: 63.67725372314453 = 0.7659286260604858 + 10.0 * 6.29113245010376
Epoch 630, val loss: 1.0981028079986572
Epoch 640, training loss: 63.75810623168945 = 0.748263955116272 + 10.0 * 6.3009843826293945
Epoch 640, val loss: 1.0922579765319824
Epoch 650, training loss: 63.671119689941406 = 0.7309860587120056 + 10.0 * 6.294013023376465
Epoch 650, val loss: 1.0872235298156738
Epoch 660, training loss: 63.602073669433594 = 0.7140777707099915 + 10.0 * 6.28879976272583
Epoch 660, val loss: 1.0825741291046143
Epoch 670, training loss: 63.55835723876953 = 0.6975202560424805 + 10.0 * 6.286083698272705
Epoch 670, val loss: 1.0784389972686768
Epoch 680, training loss: 63.52695083618164 = 0.6813340187072754 + 10.0 * 6.284561634063721
Epoch 680, val loss: 1.0746393203735352
Epoch 690, training loss: 63.496150970458984 = 0.6653813719749451 + 10.0 * 6.283076763153076
Epoch 690, val loss: 1.071201205253601
Epoch 700, training loss: 63.467411041259766 = 0.6496310234069824 + 10.0 * 6.281777858734131
Epoch 700, val loss: 1.0680646896362305
Epoch 710, training loss: 63.578407287597656 = 0.6340884566307068 + 10.0 * 6.294432163238525
Epoch 710, val loss: 1.065329670906067
Epoch 720, training loss: 63.44217300415039 = 0.6185934543609619 + 10.0 * 6.282357692718506
Epoch 720, val loss: 1.0621566772460938
Epoch 730, training loss: 63.39371109008789 = 0.6033284664154053 + 10.0 * 6.279038429260254
Epoch 730, val loss: 1.059890627861023
Epoch 740, training loss: 63.36065673828125 = 0.5882583260536194 + 10.0 * 6.277239799499512
Epoch 740, val loss: 1.057692527770996
Epoch 750, training loss: 63.34455871582031 = 0.5734673738479614 + 10.0 * 6.277109146118164
Epoch 750, val loss: 1.0557280778884888
Epoch 760, training loss: 63.315773010253906 = 0.5588758587837219 + 10.0 * 6.275689601898193
Epoch 760, val loss: 1.0543144941329956
Epoch 770, training loss: 63.30096435546875 = 0.5445619821548462 + 10.0 * 6.27564001083374
Epoch 770, val loss: 1.0528515577316284
Epoch 780, training loss: 63.30984878540039 = 0.5305300354957581 + 10.0 * 6.277932167053223
Epoch 780, val loss: 1.0519758462905884
Epoch 790, training loss: 63.23995590209961 = 0.5165724754333496 + 10.0 * 6.272338390350342
Epoch 790, val loss: 1.0509121417999268
Epoch 800, training loss: 63.21584701538086 = 0.5030090808868408 + 10.0 * 6.2712836265563965
Epoch 800, val loss: 1.0504062175750732
Epoch 810, training loss: 63.18687057495117 = 0.4896913170814514 + 10.0 * 6.269717693328857
Epoch 810, val loss: 1.0502934455871582
Epoch 820, training loss: 63.23593521118164 = 0.4765922427177429 + 10.0 * 6.275934219360352
Epoch 820, val loss: 1.050504446029663
Epoch 830, training loss: 63.19597244262695 = 0.46352478861808777 + 10.0 * 6.273244857788086
Epoch 830, val loss: 1.0502287149429321
Epoch 840, training loss: 63.132232666015625 = 0.4507894217967987 + 10.0 * 6.268144130706787
Epoch 840, val loss: 1.050341010093689
Epoch 850, training loss: 63.10470962524414 = 0.438265323638916 + 10.0 * 6.266644477844238
Epoch 850, val loss: 1.0509753227233887
Epoch 860, training loss: 63.09688949584961 = 0.4259730577468872 + 10.0 * 6.267091751098633
Epoch 860, val loss: 1.0514979362487793
Epoch 870, training loss: 63.07971954345703 = 0.41388019919395447 + 10.0 * 6.2665839195251465
Epoch 870, val loss: 1.0523431301116943
Epoch 880, training loss: 63.04966735839844 = 0.4020997881889343 + 10.0 * 6.264756679534912
Epoch 880, val loss: 1.0535541772842407
Epoch 890, training loss: 63.02279281616211 = 0.39041730761528015 + 10.0 * 6.263237476348877
Epoch 890, val loss: 1.0545059442520142
Epoch 900, training loss: 62.99796676635742 = 0.3790322542190552 + 10.0 * 6.261893272399902
Epoch 900, val loss: 1.056006908416748
Epoch 910, training loss: 63.01939392089844 = 0.3678432106971741 + 10.0 * 6.265154838562012
Epoch 910, val loss: 1.0576798915863037
Epoch 920, training loss: 62.991947174072266 = 0.3568127453327179 + 10.0 * 6.263513565063477
Epoch 920, val loss: 1.0589396953582764
Epoch 930, training loss: 62.991188049316406 = 0.3460816442966461 + 10.0 * 6.264510631561279
Epoch 930, val loss: 1.0608142614364624
Epoch 940, training loss: 62.932281494140625 = 0.33545708656311035 + 10.0 * 6.259682655334473
Epoch 940, val loss: 1.0624135732650757
Epoch 950, training loss: 62.912784576416016 = 0.32521334290504456 + 10.0 * 6.2587571144104
Epoch 950, val loss: 1.0645654201507568
Epoch 960, training loss: 62.891632080078125 = 0.31519076228141785 + 10.0 * 6.257644176483154
Epoch 960, val loss: 1.0668799877166748
Epoch 970, training loss: 62.887332916259766 = 0.3054024279117584 + 10.0 * 6.258193016052246
Epoch 970, val loss: 1.0691733360290527
Epoch 980, training loss: 62.92368698120117 = 0.29579877853393555 + 10.0 * 6.262788772583008
Epoch 980, val loss: 1.0714720487594604
Epoch 990, training loss: 62.85538864135742 = 0.2865375280380249 + 10.0 * 6.256885051727295
Epoch 990, val loss: 1.0743372440338135
Epoch 1000, training loss: 62.825286865234375 = 0.2774433195590973 + 10.0 * 6.25478458404541
Epoch 1000, val loss: 1.0769758224487305
Epoch 1010, training loss: 62.80656433105469 = 0.26869261264801025 + 10.0 * 6.253787040710449
Epoch 1010, val loss: 1.0799570083618164
Epoch 1020, training loss: 62.809329986572266 = 0.2601895034313202 + 10.0 * 6.254914283752441
Epoch 1020, val loss: 1.0832536220550537
Epoch 1030, training loss: 62.78531265258789 = 0.2518404722213745 + 10.0 * 6.253347396850586
Epoch 1030, val loss: 1.0858962535858154
Epoch 1040, training loss: 62.78445816040039 = 0.24379368126392365 + 10.0 * 6.254066467285156
Epoch 1040, val loss: 1.0896419286727905
Epoch 1050, training loss: 62.751556396484375 = 0.23600882291793823 + 10.0 * 6.2515549659729
Epoch 1050, val loss: 1.0930367708206177
Epoch 1060, training loss: 62.74496841430664 = 0.22847050428390503 + 10.0 * 6.251649856567383
Epoch 1060, val loss: 1.0964280366897583
Epoch 1070, training loss: 62.8297004699707 = 0.221120685338974 + 10.0 * 6.260858058929443
Epoch 1070, val loss: 1.1000539064407349
Epoch 1080, training loss: 62.72726821899414 = 0.214075967669487 + 10.0 * 6.25131893157959
Epoch 1080, val loss: 1.1041957139968872
Epoch 1090, training loss: 62.69759750366211 = 0.20721791684627533 + 10.0 * 6.249037742614746
Epoch 1090, val loss: 1.1083911657333374
Epoch 1100, training loss: 62.68732833862305 = 0.20062795281410217 + 10.0 * 6.2486701011657715
Epoch 1100, val loss: 1.112261414527893
Epoch 1110, training loss: 62.673397064208984 = 0.19428476691246033 + 10.0 * 6.247910976409912
Epoch 1110, val loss: 1.1168115139007568
Epoch 1120, training loss: 62.66608428955078 = 0.18812939524650574 + 10.0 * 6.247795581817627
Epoch 1120, val loss: 1.1211276054382324
Epoch 1130, training loss: 62.712276458740234 = 0.18214534223079681 + 10.0 * 6.2530131340026855
Epoch 1130, val loss: 1.125414252281189
Epoch 1140, training loss: 62.6540412902832 = 0.17635604739189148 + 10.0 * 6.247768402099609
Epoch 1140, val loss: 1.1300721168518066
Epoch 1150, training loss: 62.68636703491211 = 0.17078687250614166 + 10.0 * 6.25155782699585
Epoch 1150, val loss: 1.134672999382019
Epoch 1160, training loss: 62.629234313964844 = 0.16545365750789642 + 10.0 * 6.246377944946289
Epoch 1160, val loss: 1.1396644115447998
Epoch 1170, training loss: 62.61145782470703 = 0.16029976308345795 + 10.0 * 6.245115756988525
Epoch 1170, val loss: 1.1445749998092651
Epoch 1180, training loss: 62.60033416748047 = 0.15532463788986206 + 10.0 * 6.244501113891602
Epoch 1180, val loss: 1.1495537757873535
Epoch 1190, training loss: 62.61894226074219 = 0.15051402151584625 + 10.0 * 6.246842861175537
Epoch 1190, val loss: 1.1545456647872925
Epoch 1200, training loss: 62.60694885253906 = 0.14584535360336304 + 10.0 * 6.246110439300537
Epoch 1200, val loss: 1.1598392724990845
Epoch 1210, training loss: 62.57811737060547 = 0.14138352870941162 + 10.0 * 6.243673324584961
Epoch 1210, val loss: 1.1650009155273438
Epoch 1220, training loss: 62.56455612182617 = 0.1370517909526825 + 10.0 * 6.242750644683838
Epoch 1220, val loss: 1.1702579259872437
Epoch 1230, training loss: 62.55453109741211 = 0.13291838765144348 + 10.0 * 6.242161273956299
Epoch 1230, val loss: 1.1757142543792725
Epoch 1240, training loss: 62.594913482666016 = 0.12892693281173706 + 10.0 * 6.246598720550537
Epoch 1240, val loss: 1.180842638015747
Epoch 1250, training loss: 62.56167984008789 = 0.1250564157962799 + 10.0 * 6.243662357330322
Epoch 1250, val loss: 1.186676263809204
Epoch 1260, training loss: 62.5461540222168 = 0.12129682302474976 + 10.0 * 6.242486000061035
Epoch 1260, val loss: 1.191774845123291
Epoch 1270, training loss: 62.523216247558594 = 0.11772076785564423 + 10.0 * 6.240549564361572
Epoch 1270, val loss: 1.1975343227386475
Epoch 1280, training loss: 62.52531051635742 = 0.11428510397672653 + 10.0 * 6.241102695465088
Epoch 1280, val loss: 1.2031387090682983
Epoch 1290, training loss: 62.56306457519531 = 0.1109740138053894 + 10.0 * 6.245209217071533
Epoch 1290, val loss: 1.2085827589035034
Epoch 1300, training loss: 62.52178192138672 = 0.10769542306661606 + 10.0 * 6.241408348083496
Epoch 1300, val loss: 1.2138493061065674
Epoch 1310, training loss: 62.49627685546875 = 0.10461146384477615 + 10.0 * 6.239166736602783
Epoch 1310, val loss: 1.2198406457901
Epoch 1320, training loss: 62.49699783325195 = 0.10163338482379913 + 10.0 * 6.239536285400391
Epoch 1320, val loss: 1.2252920866012573
Epoch 1330, training loss: 62.4944953918457 = 0.09875194728374481 + 10.0 * 6.239574432373047
Epoch 1330, val loss: 1.2311023473739624
Epoch 1340, training loss: 62.48926544189453 = 0.09595878422260284 + 10.0 * 6.239330768585205
Epoch 1340, val loss: 1.2368413209915161
Epoch 1350, training loss: 62.4718017578125 = 0.09327781200408936 + 10.0 * 6.237852573394775
Epoch 1350, val loss: 1.2424927949905396
Epoch 1360, training loss: 62.4644660949707 = 0.09068725258111954 + 10.0 * 6.237378120422363
Epoch 1360, val loss: 1.2479804754257202
Epoch 1370, training loss: 62.48552703857422 = 0.08821898698806763 + 10.0 * 6.2397308349609375
Epoch 1370, val loss: 1.2540340423583984
Epoch 1380, training loss: 62.45600128173828 = 0.08578023314476013 + 10.0 * 6.2370219230651855
Epoch 1380, val loss: 1.2593787908554077
Epoch 1390, training loss: 62.43610382080078 = 0.08342792838811874 + 10.0 * 6.235267639160156
Epoch 1390, val loss: 1.264896273612976
Epoch 1400, training loss: 62.432090759277344 = 0.08118907362222672 + 10.0 * 6.235090255737305
Epoch 1400, val loss: 1.2707610130310059
Epoch 1410, training loss: 62.452484130859375 = 0.0790223777294159 + 10.0 * 6.237346172332764
Epoch 1410, val loss: 1.2761733531951904
Epoch 1420, training loss: 62.437583923339844 = 0.07691750675439835 + 10.0 * 6.236066818237305
Epoch 1420, val loss: 1.282052755355835
Epoch 1430, training loss: 62.43355178833008 = 0.07490035146474838 + 10.0 * 6.235865116119385
Epoch 1430, val loss: 1.28750479221344
Epoch 1440, training loss: 62.421504974365234 = 0.0729399248957634 + 10.0 * 6.234856605529785
Epoch 1440, val loss: 1.2932673692703247
Epoch 1450, training loss: 62.418128967285156 = 0.07104466110467911 + 10.0 * 6.234708309173584
Epoch 1450, val loss: 1.2986016273498535
Epoch 1460, training loss: 62.41253662109375 = 0.0692133754491806 + 10.0 * 6.234332084655762
Epoch 1460, val loss: 1.3042112588882446
Epoch 1470, training loss: 62.40574264526367 = 0.06744162738323212 + 10.0 * 6.233830451965332
Epoch 1470, val loss: 1.3095828294754028
Epoch 1480, training loss: 62.42193603515625 = 0.06572863459587097 + 10.0 * 6.235620975494385
Epoch 1480, val loss: 1.314914345741272
Epoch 1490, training loss: 62.39562225341797 = 0.06408093869686127 + 10.0 * 6.233154296875
Epoch 1490, val loss: 1.320918321609497
Epoch 1500, training loss: 62.41078567504883 = 0.06247876212000847 + 10.0 * 6.234830856323242
Epoch 1500, val loss: 1.3259564638137817
Epoch 1510, training loss: 62.40078353881836 = 0.06091783940792084 + 10.0 * 6.233986854553223
Epoch 1510, val loss: 1.3313961029052734
Epoch 1520, training loss: 62.37187576293945 = 0.05942295119166374 + 10.0 * 6.231245517730713
Epoch 1520, val loss: 1.3366340398788452
Epoch 1530, training loss: 62.36451721191406 = 0.05798329785466194 + 10.0 * 6.230653285980225
Epoch 1530, val loss: 1.3423513174057007
Epoch 1540, training loss: 62.362510681152344 = 0.056596558541059494 + 10.0 * 6.230591773986816
Epoch 1540, val loss: 1.34763765335083
Epoch 1550, training loss: 62.40139389038086 = 0.055258892476558685 + 10.0 * 6.234613418579102
Epoch 1550, val loss: 1.3529192209243774
Epoch 1560, training loss: 62.35929870605469 = 0.05391966551542282 + 10.0 * 6.2305378913879395
Epoch 1560, val loss: 1.3580440282821655
Epoch 1570, training loss: 62.35421371459961 = 0.052646368741989136 + 10.0 * 6.230156898498535
Epoch 1570, val loss: 1.3631008863449097
Epoch 1580, training loss: 62.38046646118164 = 0.05143262445926666 + 10.0 * 6.232903480529785
Epoch 1580, val loss: 1.3681751489639282
Epoch 1590, training loss: 62.335025787353516 = 0.05023021250963211 + 10.0 * 6.228479385375977
Epoch 1590, val loss: 1.3734419345855713
Epoch 1600, training loss: 62.32734680175781 = 0.04907825589179993 + 10.0 * 6.2278265953063965
Epoch 1600, val loss: 1.3783984184265137
Epoch 1610, training loss: 62.33083724975586 = 0.04797344282269478 + 10.0 * 6.228286266326904
Epoch 1610, val loss: 1.3833175897598267
Epoch 1620, training loss: 62.40205764770508 = 0.04689560830593109 + 10.0 * 6.235516548156738
Epoch 1620, val loss: 1.3879034519195557
Epoch 1630, training loss: 62.33887481689453 = 0.04585287719964981 + 10.0 * 6.229302406311035
Epoch 1630, val loss: 1.3936614990234375
Epoch 1640, training loss: 62.35413360595703 = 0.04482990875840187 + 10.0 * 6.230930328369141
Epoch 1640, val loss: 1.3979709148406982
Epoch 1650, training loss: 62.31532669067383 = 0.043843671679496765 + 10.0 * 6.227148532867432
Epoch 1650, val loss: 1.4031115770339966
Epoch 1660, training loss: 62.312435150146484 = 0.042886584997177124 + 10.0 * 6.226954936981201
Epoch 1660, val loss: 1.4078643321990967
Epoch 1670, training loss: 62.30134201049805 = 0.04197125509381294 + 10.0 * 6.2259368896484375
Epoch 1670, val loss: 1.412689208984375
Epoch 1680, training loss: 62.30806350708008 = 0.041085030883550644 + 10.0 * 6.22669792175293
Epoch 1680, val loss: 1.4173316955566406
Epoch 1690, training loss: 62.3365364074707 = 0.040219951421022415 + 10.0 * 6.2296319007873535
Epoch 1690, val loss: 1.4219506978988647
Epoch 1700, training loss: 62.3125114440918 = 0.03936927020549774 + 10.0 * 6.227314472198486
Epoch 1700, val loss: 1.4267910718917847
Epoch 1710, training loss: 62.32255935668945 = 0.038542501628398895 + 10.0 * 6.2284016609191895
Epoch 1710, val loss: 1.4309855699539185
Epoch 1720, training loss: 62.29050064086914 = 0.037747059017419815 + 10.0 * 6.22527551651001
Epoch 1720, val loss: 1.435795783996582
Epoch 1730, training loss: 62.28578186035156 = 0.03698219358921051 + 10.0 * 6.224879741668701
Epoch 1730, val loss: 1.440181851387024
Epoch 1740, training loss: 62.278018951416016 = 0.03624068945646286 + 10.0 * 6.224177837371826
Epoch 1740, val loss: 1.4447948932647705
Epoch 1750, training loss: 62.340301513671875 = 0.03553078696131706 + 10.0 * 6.230477333068848
Epoch 1750, val loss: 1.4494640827178955
Epoch 1760, training loss: 62.283897399902344 = 0.03480341657996178 + 10.0 * 6.22490930557251
Epoch 1760, val loss: 1.4534285068511963
Epoch 1770, training loss: 62.26542282104492 = 0.03411358967423439 + 10.0 * 6.223130702972412
Epoch 1770, val loss: 1.4580971002578735
Epoch 1780, training loss: 62.261268615722656 = 0.03345156088471413 + 10.0 * 6.222781658172607
Epoch 1780, val loss: 1.4621104001998901
Epoch 1790, training loss: 62.30069351196289 = 0.03281276300549507 + 10.0 * 6.22678804397583
Epoch 1790, val loss: 1.4661157131195068
Epoch 1800, training loss: 62.33472442626953 = 0.03218906372785568 + 10.0 * 6.23025369644165
Epoch 1800, val loss: 1.4709322452545166
Epoch 1810, training loss: 62.27968215942383 = 0.03154292702674866 + 10.0 * 6.224813938140869
Epoch 1810, val loss: 1.4743143320083618
Epoch 1820, training loss: 62.256412506103516 = 0.03095533326268196 + 10.0 * 6.222545623779297
Epoch 1820, val loss: 1.4792096614837646
Epoch 1830, training loss: 62.24491882324219 = 0.03037841245532036 + 10.0 * 6.221453666687012
Epoch 1830, val loss: 1.4830076694488525
Epoch 1840, training loss: 62.24211120605469 = 0.02982369437813759 + 10.0 * 6.22122859954834
Epoch 1840, val loss: 1.487226128578186
Epoch 1850, training loss: 62.286808013916016 = 0.029284130781888962 + 10.0 * 6.225752353668213
Epoch 1850, val loss: 1.4909754991531372
Epoch 1860, training loss: 62.266822814941406 = 0.02874755673110485 + 10.0 * 6.223807334899902
Epoch 1860, val loss: 1.4952753782272339
Epoch 1870, training loss: 62.25843048095703 = 0.02821977250277996 + 10.0 * 6.223021030426025
Epoch 1870, val loss: 1.49891996383667
Epoch 1880, training loss: 62.240943908691406 = 0.027711456641554832 + 10.0 * 6.221323490142822
Epoch 1880, val loss: 1.503201961517334
Epoch 1890, training loss: 62.229835510253906 = 0.027219844982028008 + 10.0 * 6.220261573791504
Epoch 1890, val loss: 1.5069222450256348
Epoch 1900, training loss: 62.24460983276367 = 0.026751497760415077 + 10.0 * 6.221785545349121
Epoch 1900, val loss: 1.5109195709228516
Epoch 1910, training loss: 62.29003143310547 = 0.02628941275179386 + 10.0 * 6.22637414932251
Epoch 1910, val loss: 1.5146173238754272
Epoch 1920, training loss: 62.24315643310547 = 0.02581017278134823 + 10.0 * 6.221734523773193
Epoch 1920, val loss: 1.5178762674331665
Epoch 1930, training loss: 62.22536087036133 = 0.025371253490447998 + 10.0 * 6.219998836517334
Epoch 1930, val loss: 1.5220668315887451
Epoch 1940, training loss: 62.21601867675781 = 0.024941461160779 + 10.0 * 6.219107627868652
Epoch 1940, val loss: 1.525718331336975
Epoch 1950, training loss: 62.2181396484375 = 0.02452833764255047 + 10.0 * 6.219361305236816
Epoch 1950, val loss: 1.5294742584228516
Epoch 1960, training loss: 62.274810791015625 = 0.02411840669810772 + 10.0 * 6.225069522857666
Epoch 1960, val loss: 1.5328272581100464
Epoch 1970, training loss: 62.23291778564453 = 0.023709390312433243 + 10.0 * 6.220921039581299
Epoch 1970, val loss: 1.5361114740371704
Epoch 1980, training loss: 62.222434997558594 = 0.023323146626353264 + 10.0 * 6.219911098480225
Epoch 1980, val loss: 1.5398997068405151
Epoch 1990, training loss: 62.204322814941406 = 0.022941507399082184 + 10.0 * 6.218138217926025
Epoch 1990, val loss: 1.5433344841003418
Epoch 2000, training loss: 62.199832916259766 = 0.022575249895453453 + 10.0 * 6.21772575378418
Epoch 2000, val loss: 1.5469415187835693
Epoch 2010, training loss: 62.24197769165039 = 0.0222255177795887 + 10.0 * 6.221975326538086
Epoch 2010, val loss: 1.5505465269088745
Epoch 2020, training loss: 62.214412689208984 = 0.0218629352748394 + 10.0 * 6.219254970550537
Epoch 2020, val loss: 1.5535322427749634
Epoch 2030, training loss: 62.201114654541016 = 0.021514173597097397 + 10.0 * 6.217959880828857
Epoch 2030, val loss: 1.557085633277893
Epoch 2040, training loss: 62.18859100341797 = 0.021173326298594475 + 10.0 * 6.216742038726807
Epoch 2040, val loss: 1.5604099035263062
Epoch 2050, training loss: 62.18526840209961 = 0.020847907289862633 + 10.0 * 6.216442108154297
Epoch 2050, val loss: 1.5636053085327148
Epoch 2060, training loss: 62.19686508178711 = 0.020533503964543343 + 10.0 * 6.217633247375488
Epoch 2060, val loss: 1.56702721118927
Epoch 2070, training loss: 62.25067901611328 = 0.020217550918459892 + 10.0 * 6.22304630279541
Epoch 2070, val loss: 1.5698754787445068
Epoch 2080, training loss: 62.19961166381836 = 0.019910698756575584 + 10.0 * 6.217970371246338
Epoch 2080, val loss: 1.5735350847244263
Epoch 2090, training loss: 62.180267333984375 = 0.019604185596108437 + 10.0 * 6.216066360473633
Epoch 2090, val loss: 1.5765570402145386
Epoch 2100, training loss: 62.18080139160156 = 0.019316861405968666 + 10.0 * 6.216148376464844
Epoch 2100, val loss: 1.579854130744934
Epoch 2110, training loss: 62.18754196166992 = 0.019034387543797493 + 10.0 * 6.216850757598877
Epoch 2110, val loss: 1.5829836130142212
Epoch 2120, training loss: 62.17927932739258 = 0.0187532939016819 + 10.0 * 6.216052532196045
Epoch 2120, val loss: 1.5857791900634766
Epoch 2130, training loss: 62.1920280456543 = 0.01848090998828411 + 10.0 * 6.217354774475098
Epoch 2130, val loss: 1.588746190071106
Epoch 2140, training loss: 62.17190170288086 = 0.018216803669929504 + 10.0 * 6.215368747711182
Epoch 2140, val loss: 1.5920462608337402
Epoch 2150, training loss: 62.17110824584961 = 0.017955131828784943 + 10.0 * 6.215315341949463
Epoch 2150, val loss: 1.5949662923812866
Epoch 2160, training loss: 62.23811721801758 = 0.01769929938018322 + 10.0 * 6.222041606903076
Epoch 2160, val loss: 1.5977108478546143
Epoch 2170, training loss: 62.2041015625 = 0.017440613359212875 + 10.0 * 6.218666076660156
Epoch 2170, val loss: 1.600486159324646
Epoch 2180, training loss: 62.16162109375 = 0.01719190552830696 + 10.0 * 6.214442729949951
Epoch 2180, val loss: 1.6035209894180298
Epoch 2190, training loss: 62.160011291503906 = 0.016954854130744934 + 10.0 * 6.214305400848389
Epoch 2190, val loss: 1.6064753532409668
Epoch 2200, training loss: 62.157649993896484 = 0.016727270558476448 + 10.0 * 6.214092254638672
Epoch 2200, val loss: 1.6094378232955933
Epoch 2210, training loss: 62.17351531982422 = 0.016502827405929565 + 10.0 * 6.215701103210449
Epoch 2210, val loss: 1.6121405363082886
Epoch 2220, training loss: 62.176395416259766 = 0.01627608947455883 + 10.0 * 6.216012001037598
Epoch 2220, val loss: 1.6146734952926636
Epoch 2230, training loss: 62.15793228149414 = 0.016050022095441818 + 10.0 * 6.214188575744629
Epoch 2230, val loss: 1.6173293590545654
Epoch 2240, training loss: 62.15736389160156 = 0.015834426507353783 + 10.0 * 6.214152812957764
Epoch 2240, val loss: 1.6200376749038696
Epoch 2250, training loss: 62.1464958190918 = 0.015627095475792885 + 10.0 * 6.2130866050720215
Epoch 2250, val loss: 1.623012900352478
Epoch 2260, training loss: 62.16179656982422 = 0.015424325130879879 + 10.0 * 6.214637279510498
Epoch 2260, val loss: 1.625603199005127
Epoch 2270, training loss: 62.198909759521484 = 0.015223247930407524 + 10.0 * 6.2183685302734375
Epoch 2270, val loss: 1.6282241344451904
Epoch 2280, training loss: 62.15726852416992 = 0.015017418190836906 + 10.0 * 6.214224815368652
Epoch 2280, val loss: 1.6308867931365967
Epoch 2290, training loss: 62.14523696899414 = 0.01482181716710329 + 10.0 * 6.21304178237915
Epoch 2290, val loss: 1.6333011388778687
Epoch 2300, training loss: 62.14884948730469 = 0.014633233658969402 + 10.0 * 6.213421821594238
Epoch 2300, val loss: 1.6359529495239258
Epoch 2310, training loss: 62.13896560668945 = 0.014447348192334175 + 10.0 * 6.212451934814453
Epoch 2310, val loss: 1.6384310722351074
Epoch 2320, training loss: 62.13343811035156 = 0.014266210608184338 + 10.0 * 6.211916923522949
Epoch 2320, val loss: 1.6413806676864624
Epoch 2330, training loss: 62.24562454223633 = 0.014101970009505749 + 10.0 * 6.223152160644531
Epoch 2330, val loss: 1.6440601348876953
Epoch 2340, training loss: 62.16044235229492 = 0.013903575949370861 + 10.0 * 6.214653968811035
Epoch 2340, val loss: 1.6453027725219727
Epoch 2350, training loss: 62.13263702392578 = 0.013732329942286015 + 10.0 * 6.21189022064209
Epoch 2350, val loss: 1.648703694343567
Epoch 2360, training loss: 62.120155334472656 = 0.013564091175794601 + 10.0 * 6.210659027099609
Epoch 2360, val loss: 1.6507824659347534
Epoch 2370, training loss: 62.12237548828125 = 0.013403758406639099 + 10.0 * 6.210896968841553
Epoch 2370, val loss: 1.653381586074829
Epoch 2380, training loss: 62.185211181640625 = 0.01324489526450634 + 10.0 * 6.217196464538574
Epoch 2380, val loss: 1.6555750370025635
Epoch 2390, training loss: 62.1312255859375 = 0.013081716373562813 + 10.0 * 6.2118144035339355
Epoch 2390, val loss: 1.6577314138412476
Epoch 2400, training loss: 62.111724853515625 = 0.012925544753670692 + 10.0 * 6.2098798751831055
Epoch 2400, val loss: 1.6602400541305542
Epoch 2410, training loss: 62.10944747924805 = 0.01277437899261713 + 10.0 * 6.209667205810547
Epoch 2410, val loss: 1.6624215841293335
Epoch 2420, training loss: 62.1168098449707 = 0.012627938762307167 + 10.0 * 6.210418224334717
Epoch 2420, val loss: 1.6647151708602905
Epoch 2430, training loss: 62.16969680786133 = 0.012487458065152168 + 10.0 * 6.2157206535339355
Epoch 2430, val loss: 1.6669926643371582
Epoch 2440, training loss: 62.10806655883789 = 0.012331665493547916 + 10.0 * 6.209573268890381
Epoch 2440, val loss: 1.6691173315048218
Epoch 2450, training loss: 62.1611213684082 = 0.012193401344120502 + 10.0 * 6.214892864227295
Epoch 2450, val loss: 1.6714009046554565
Epoch 2460, training loss: 62.132904052734375 = 0.012046685442328453 + 10.0 * 6.212085723876953
Epoch 2460, val loss: 1.6730009317398071
Epoch 2470, training loss: 62.12065505981445 = 0.011910066939890385 + 10.0 * 6.210874557495117
Epoch 2470, val loss: 1.675577998161316
Epoch 2480, training loss: 62.099945068359375 = 0.01177529338747263 + 10.0 * 6.208817005157471
Epoch 2480, val loss: 1.6775907278060913
Epoch 2490, training loss: 62.098140716552734 = 0.011645532213151455 + 10.0 * 6.208649635314941
Epoch 2490, val loss: 1.679634690284729
Epoch 2500, training loss: 62.15141677856445 = 0.011518917977809906 + 10.0 * 6.213989734649658
Epoch 2500, val loss: 1.681674599647522
Epoch 2510, training loss: 62.09397506713867 = 0.01138845644891262 + 10.0 * 6.208258628845215
Epoch 2510, val loss: 1.6836532354354858
Epoch 2520, training loss: 62.103458404541016 = 0.011266282759606838 + 10.0 * 6.209219455718994
Epoch 2520, val loss: 1.6857713460922241
Epoch 2530, training loss: 62.11290740966797 = 0.011142251081764698 + 10.0 * 6.210176467895508
Epoch 2530, val loss: 1.6875165700912476
Epoch 2540, training loss: 62.12196350097656 = 0.011022571474313736 + 10.0 * 6.211093902587891
Epoch 2540, val loss: 1.6895859241485596
Epoch 2550, training loss: 62.09980392456055 = 0.010904447175562382 + 10.0 * 6.208889961242676
Epoch 2550, val loss: 1.6920032501220703
Epoch 2560, training loss: 62.10091781616211 = 0.010788255371153355 + 10.0 * 6.209012985229492
Epoch 2560, val loss: 1.6936829090118408
Epoch 2570, training loss: 62.128578186035156 = 0.010678085498511791 + 10.0 * 6.211790084838867
Epoch 2570, val loss: 1.695465087890625
Epoch 2580, training loss: 62.09052658081055 = 0.010561151430010796 + 10.0 * 6.207996368408203
Epoch 2580, val loss: 1.6975769996643066
Epoch 2590, training loss: 62.08518600463867 = 0.01044942531734705 + 10.0 * 6.2074737548828125
Epoch 2590, val loss: 1.699215292930603
Epoch 2600, training loss: 62.08860397338867 = 0.010343333706259727 + 10.0 * 6.207826137542725
Epoch 2600, val loss: 1.7011765241622925
Epoch 2610, training loss: 62.11661911010742 = 0.010240154340863228 + 10.0 * 6.210638046264648
Epoch 2610, val loss: 1.7028863430023193
Epoch 2620, training loss: 62.076560974121094 = 0.010132933035492897 + 10.0 * 6.206643104553223
Epoch 2620, val loss: 1.704870581626892
Epoch 2630, training loss: 62.09309387207031 = 0.010033741593360901 + 10.0 * 6.208306312561035
Epoch 2630, val loss: 1.7065707445144653
Epoch 2640, training loss: 62.1326789855957 = 0.009933780878782272 + 10.0 * 6.212274551391602
Epoch 2640, val loss: 1.708222508430481
Epoch 2650, training loss: 62.11246871948242 = 0.009828918613493443 + 10.0 * 6.210263729095459
Epoch 2650, val loss: 1.7096061706542969
Epoch 2660, training loss: 62.07710647583008 = 0.009725497104227543 + 10.0 * 6.206738471984863
Epoch 2660, val loss: 1.7117767333984375
Epoch 2670, training loss: 62.069435119628906 = 0.009629850275814533 + 10.0 * 6.2059807777404785
Epoch 2670, val loss: 1.7132090330123901
Epoch 2680, training loss: 62.069679260253906 = 0.00953811127692461 + 10.0 * 6.206014156341553
Epoch 2680, val loss: 1.7149877548217773
Epoch 2690, training loss: 62.11736297607422 = 0.00944652408361435 + 10.0 * 6.21079158782959
Epoch 2690, val loss: 1.716054081916809
Epoch 2700, training loss: 62.08456802368164 = 0.009354870766401291 + 10.0 * 6.207521438598633
Epoch 2700, val loss: 1.7184003591537476
Epoch 2710, training loss: 62.08102798461914 = 0.009262328036129475 + 10.0 * 6.207176685333252
Epoch 2710, val loss: 1.719965934753418
Epoch 2720, training loss: 62.06718063354492 = 0.009173989295959473 + 10.0 * 6.205800533294678
Epoch 2720, val loss: 1.7217379808425903
Epoch 2730, training loss: 62.05901336669922 = 0.009087279438972473 + 10.0 * 6.204992771148682
Epoch 2730, val loss: 1.723189353942871
Epoch 2740, training loss: 62.07509994506836 = 0.009004217572510242 + 10.0 * 6.206609725952148
Epoch 2740, val loss: 1.7246415615081787
Epoch 2750, training loss: 62.12334442138672 = 0.008919624611735344 + 10.0 * 6.211442470550537
Epoch 2750, val loss: 1.7259764671325684
Epoch 2760, training loss: 62.09342956542969 = 0.008834716863930225 + 10.0 * 6.208459377288818
Epoch 2760, val loss: 1.7279144525527954
Epoch 2770, training loss: 62.064666748046875 = 0.008749795146286488 + 10.0 * 6.205591678619385
Epoch 2770, val loss: 1.729090929031372
Epoch 2780, training loss: 62.05472183227539 = 0.008671140298247337 + 10.0 * 6.2046051025390625
Epoch 2780, val loss: 1.7307419776916504
Epoch 2790, training loss: 62.05632400512695 = 0.008593126200139523 + 10.0 * 6.20477294921875
Epoch 2790, val loss: 1.7321401834487915
Epoch 2800, training loss: 62.111385345458984 = 0.008518547751009464 + 10.0 * 6.210286617279053
Epoch 2800, val loss: 1.7331056594848633
Epoch 2810, training loss: 62.10057830810547 = 0.008439114317297935 + 10.0 * 6.209214210510254
Epoch 2810, val loss: 1.7351678609848022
Epoch 2820, training loss: 62.06694030761719 = 0.008361348882317543 + 10.0 * 6.205857753753662
Epoch 2820, val loss: 1.736311674118042
Epoch 2830, training loss: 62.05271911621094 = 0.00828621257096529 + 10.0 * 6.204443454742432
Epoch 2830, val loss: 1.7378276586532593
Epoch 2840, training loss: 62.055938720703125 = 0.00821589957922697 + 10.0 * 6.204771995544434
Epoch 2840, val loss: 1.7389569282531738
Epoch 2850, training loss: 62.16150665283203 = 0.008150377310812473 + 10.0 * 6.215335369110107
Epoch 2850, val loss: 1.7401221990585327
Epoch 2860, training loss: 62.07217788696289 = 0.008067971095442772 + 10.0 * 6.206410884857178
Epoch 2860, val loss: 1.7417495250701904
Epoch 2870, training loss: 62.04920959472656 = 0.007995267398655415 + 10.0 * 6.2041215896606445
Epoch 2870, val loss: 1.7429602146148682
Epoch 2880, training loss: 62.03833770751953 = 0.007929280400276184 + 10.0 * 6.203040599822998
Epoch 2880, val loss: 1.744449496269226
Epoch 2890, training loss: 62.03831481933594 = 0.007864006794989109 + 10.0 * 6.203044891357422
Epoch 2890, val loss: 1.7456973791122437
Epoch 2900, training loss: 62.058109283447266 = 0.007801263127475977 + 10.0 * 6.205030918121338
Epoch 2900, val loss: 1.746926188468933
Epoch 2910, training loss: 62.081539154052734 = 0.007736331317573786 + 10.0 * 6.207380294799805
Epoch 2910, val loss: 1.7480076551437378
Epoch 2920, training loss: 62.055259704589844 = 0.0076675452291965485 + 10.0 * 6.204759120941162
Epoch 2920, val loss: 1.7494471073150635
Epoch 2930, training loss: 62.035701751708984 = 0.007603254169225693 + 10.0 * 6.202809810638428
Epoch 2930, val loss: 1.7505508661270142
Epoch 2940, training loss: 62.05524826049805 = 0.007543517276644707 + 10.0 * 6.204770565032959
Epoch 2940, val loss: 1.751624584197998
Epoch 2950, training loss: 62.056156158447266 = 0.007480840664356947 + 10.0 * 6.204867362976074
Epoch 2950, val loss: 1.7528698444366455
Epoch 2960, training loss: 62.03099060058594 = 0.007417590823024511 + 10.0 * 6.202357292175293
Epoch 2960, val loss: 1.7540018558502197
Epoch 2970, training loss: 62.07770538330078 = 0.007361565716564655 + 10.0 * 6.207034111022949
Epoch 2970, val loss: 1.7554881572723389
Epoch 2980, training loss: 62.059242248535156 = 0.00729952659457922 + 10.0 * 6.205193996429443
Epoch 2980, val loss: 1.7560638189315796
Epoch 2990, training loss: 62.03650665283203 = 0.007235676981508732 + 10.0 * 6.202927112579346
Epoch 2990, val loss: 1.756850004196167
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.812335266209805
The final CL Acc:0.70988, 0.00761, The final GNN Acc:0.81269, 0.00090
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13094])
remove edge: torch.Size([2, 8004])
updated graph: torch.Size([2, 10542])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.92281341552734 = 1.9540361166000366 + 10.0 * 8.596878051757812
Epoch 0, val loss: 1.957829475402832
Epoch 10, training loss: 87.90864562988281 = 1.9434821605682373 + 10.0 * 8.596516609191895
Epoch 10, val loss: 1.9469200372695923
Epoch 20, training loss: 87.86528778076172 = 1.9306501150131226 + 10.0 * 8.593463897705078
Epoch 20, val loss: 1.9334313869476318
Epoch 30, training loss: 87.60678100585938 = 1.9141119718551636 + 10.0 * 8.569267272949219
Epoch 30, val loss: 1.9162869453430176
Epoch 40, training loss: 86.10631561279297 = 1.8936514854431152 + 10.0 * 8.421266555786133
Epoch 40, val loss: 1.896125078201294
Epoch 50, training loss: 80.69242095947266 = 1.8713314533233643 + 10.0 * 7.88210916519165
Epoch 50, val loss: 1.8752386569976807
Epoch 60, training loss: 76.70743560791016 = 1.8523104190826416 + 10.0 * 7.4855122566223145
Epoch 60, val loss: 1.8587311506271362
Epoch 70, training loss: 73.42090606689453 = 1.8398141860961914 + 10.0 * 7.158109188079834
Epoch 70, val loss: 1.8469090461730957
Epoch 80, training loss: 71.65306091308594 = 1.8288427591323853 + 10.0 * 6.982421875
Epoch 80, val loss: 1.8360787630081177
Epoch 90, training loss: 70.40674591064453 = 1.817861557006836 + 10.0 * 6.858888626098633
Epoch 90, val loss: 1.824925184249878
Epoch 100, training loss: 69.51648712158203 = 1.8060139417648315 + 10.0 * 6.771047115325928
Epoch 100, val loss: 1.813183307647705
Epoch 110, training loss: 68.89559936523438 = 1.7942055463790894 + 10.0 * 6.710139751434326
Epoch 110, val loss: 1.8016325235366821
Epoch 120, training loss: 68.40423583984375 = 1.782728672027588 + 10.0 * 6.662150859832764
Epoch 120, val loss: 1.7904821634292603
Epoch 130, training loss: 68.01714324951172 = 1.7716777324676514 + 10.0 * 6.624547004699707
Epoch 130, val loss: 1.7796586751937866
Epoch 140, training loss: 67.7179183959961 = 1.7602390050888062 + 10.0 * 6.595767974853516
Epoch 140, val loss: 1.7683966159820557
Epoch 150, training loss: 67.45468139648438 = 1.7480798959732056 + 10.0 * 6.57066011428833
Epoch 150, val loss: 1.7566092014312744
Epoch 160, training loss: 67.21919250488281 = 1.7351373434066772 + 10.0 * 6.548405170440674
Epoch 160, val loss: 1.744347095489502
Epoch 170, training loss: 66.99537658691406 = 1.7212764024734497 + 10.0 * 6.52741003036499
Epoch 170, val loss: 1.731473684310913
Epoch 180, training loss: 66.81558227539062 = 1.7061166763305664 + 10.0 * 6.510946273803711
Epoch 180, val loss: 1.7174959182739258
Epoch 190, training loss: 66.61174774169922 = 1.68975830078125 + 10.0 * 6.492198944091797
Epoch 190, val loss: 1.702433705329895
Epoch 200, training loss: 66.43367004394531 = 1.6720046997070312 + 10.0 * 6.476166248321533
Epoch 200, val loss: 1.6861947774887085
Epoch 210, training loss: 66.34800720214844 = 1.6526484489440918 + 10.0 * 6.469535827636719
Epoch 210, val loss: 1.6686564683914185
Epoch 220, training loss: 66.13463592529297 = 1.6315999031066895 + 10.0 * 6.450303554534912
Epoch 220, val loss: 1.6497231721878052
Epoch 230, training loss: 65.998046875 = 1.609056830406189 + 10.0 * 6.438899040222168
Epoch 230, val loss: 1.6295324563980103
Epoch 240, training loss: 65.87466430664062 = 1.5849940776824951 + 10.0 * 6.428966999053955
Epoch 240, val loss: 1.6082234382629395
Epoch 250, training loss: 65.75837707519531 = 1.5594127178192139 + 10.0 * 6.419896602630615
Epoch 250, val loss: 1.585861325263977
Epoch 260, training loss: 65.6566390991211 = 1.5326309204101562 + 10.0 * 6.412400722503662
Epoch 260, val loss: 1.5628629922866821
Epoch 270, training loss: 65.5505142211914 = 1.5050126314163208 + 10.0 * 6.404550552368164
Epoch 270, val loss: 1.5396760702133179
Epoch 280, training loss: 65.49544525146484 = 1.476770043373108 + 10.0 * 6.401867389678955
Epoch 280, val loss: 1.5165807008743286
Epoch 290, training loss: 65.35838317871094 = 1.4477688074111938 + 10.0 * 6.391061782836914
Epoch 290, val loss: 1.4936439990997314
Epoch 300, training loss: 65.25415802001953 = 1.4188979864120483 + 10.0 * 6.38352632522583
Epoch 300, val loss: 1.4714300632476807
Epoch 310, training loss: 65.1590347290039 = 1.3899983167648315 + 10.0 * 6.376904010772705
Epoch 310, val loss: 1.4500826597213745
Epoch 320, training loss: 65.08750915527344 = 1.361253023147583 + 10.0 * 6.372625350952148
Epoch 320, val loss: 1.4296437501907349
Epoch 330, training loss: 65.02491760253906 = 1.3327085971832275 + 10.0 * 6.369221210479736
Epoch 330, val loss: 1.4101676940917969
Epoch 340, training loss: 64.90888214111328 = 1.304620385169983 + 10.0 * 6.360426425933838
Epoch 340, val loss: 1.3916386365890503
Epoch 350, training loss: 64.8306884765625 = 1.2768958806991577 + 10.0 * 6.355379104614258
Epoch 350, val loss: 1.3741965293884277
Epoch 360, training loss: 64.77920532226562 = 1.2493170499801636 + 10.0 * 6.352989196777344
Epoch 360, val loss: 1.3573154211044312
Epoch 370, training loss: 64.67849731445312 = 1.2223193645477295 + 10.0 * 6.345617771148682
Epoch 370, val loss: 1.3412631750106812
Epoch 380, training loss: 64.60299682617188 = 1.1956493854522705 + 10.0 * 6.340734958648682
Epoch 380, val loss: 1.3257783651351929
Epoch 390, training loss: 64.59212493896484 = 1.1691200733184814 + 10.0 * 6.342300891876221
Epoch 390, val loss: 1.3106142282485962
Epoch 400, training loss: 64.48286437988281 = 1.1429343223571777 + 10.0 * 6.333992958068848
Epoch 400, val loss: 1.2957550287246704
Epoch 410, training loss: 64.41014862060547 = 1.1169426441192627 + 10.0 * 6.329320907592773
Epoch 410, val loss: 1.2810765504837036
Epoch 420, training loss: 64.35619354248047 = 1.0910801887512207 + 10.0 * 6.326511383056641
Epoch 420, val loss: 1.2665067911148071
Epoch 430, training loss: 64.32633972167969 = 1.0651171207427979 + 10.0 * 6.326121807098389
Epoch 430, val loss: 1.2515475749969482
Epoch 440, training loss: 64.25674438476562 = 1.0394304990768433 + 10.0 * 6.3217315673828125
Epoch 440, val loss: 1.236554503440857
Epoch 450, training loss: 64.19100952148438 = 1.0139539241790771 + 10.0 * 6.3177056312561035
Epoch 450, val loss: 1.221798300743103
Epoch 460, training loss: 64.15777587890625 = 0.9886146187782288 + 10.0 * 6.316915988922119
Epoch 460, val loss: 1.206937551498413
Epoch 470, training loss: 64.09413146972656 = 0.9634838104248047 + 10.0 * 6.3130645751953125
Epoch 470, val loss: 1.1922085285186768
Epoch 480, training loss: 64.03936767578125 = 0.9385339021682739 + 10.0 * 6.310082912445068
Epoch 480, val loss: 1.177251935005188
Epoch 490, training loss: 63.985843658447266 = 0.9139597415924072 + 10.0 * 6.307188510894775
Epoch 490, val loss: 1.162588119506836
Epoch 500, training loss: 64.00555419921875 = 0.8895384669303894 + 10.0 * 6.311601161956787
Epoch 500, val loss: 1.1479973793029785
Epoch 510, training loss: 63.8963623046875 = 0.8654723167419434 + 10.0 * 6.303089141845703
Epoch 510, val loss: 1.1333715915679932
Epoch 520, training loss: 63.844268798828125 = 0.8418375849723816 + 10.0 * 6.300242900848389
Epoch 520, val loss: 1.1190733909606934
Epoch 530, training loss: 63.8112678527832 = 0.8186028599739075 + 10.0 * 6.299266338348389
Epoch 530, val loss: 1.1052247285842896
Epoch 540, training loss: 63.769126892089844 = 0.7955747246742249 + 10.0 * 6.2973551750183105
Epoch 540, val loss: 1.0909298658370972
Epoch 550, training loss: 63.71595764160156 = 0.7731717824935913 + 10.0 * 6.294278621673584
Epoch 550, val loss: 1.0773195028305054
Epoch 560, training loss: 63.674259185791016 = 0.7512657046318054 + 10.0 * 6.292299270629883
Epoch 560, val loss: 1.0643521547317505
Epoch 570, training loss: 63.63670349121094 = 0.7298814654350281 + 10.0 * 6.290682315826416
Epoch 570, val loss: 1.0517295598983765
Epoch 580, training loss: 63.59445571899414 = 0.7089383602142334 + 10.0 * 6.2885518074035645
Epoch 580, val loss: 1.039439082145691
Epoch 590, training loss: 63.56047058105469 = 0.6884275078773499 + 10.0 * 6.287204265594482
Epoch 590, val loss: 1.0274115800857544
Epoch 600, training loss: 63.54541015625 = 0.6682349443435669 + 10.0 * 6.287717342376709
Epoch 600, val loss: 1.0157487392425537
Epoch 610, training loss: 63.51342010498047 = 0.6485611200332642 + 10.0 * 6.2864861488342285
Epoch 610, val loss: 1.0041112899780273
Epoch 620, training loss: 63.46426010131836 = 0.6294834613800049 + 10.0 * 6.283477783203125
Epoch 620, val loss: 0.9935147762298584
Epoch 630, training loss: 63.42460250854492 = 0.6109352707862854 + 10.0 * 6.28136682510376
Epoch 630, val loss: 0.9833264350891113
Epoch 640, training loss: 63.422218322753906 = 0.5928515195846558 + 10.0 * 6.282937049865723
Epoch 640, val loss: 0.9735745787620544
Epoch 650, training loss: 63.37925720214844 = 0.5752565264701843 + 10.0 * 6.280400276184082
Epoch 650, val loss: 0.9639222025871277
Epoch 660, training loss: 63.35354232788086 = 0.5579566955566406 + 10.0 * 6.2795586585998535
Epoch 660, val loss: 0.9550459384918213
Epoch 670, training loss: 63.307308197021484 = 0.5412408113479614 + 10.0 * 6.276606559753418
Epoch 670, val loss: 0.9462246894836426
Epoch 680, training loss: 63.30308151245117 = 0.5249291658401489 + 10.0 * 6.277815341949463
Epoch 680, val loss: 0.9380664825439453
Epoch 690, training loss: 63.24967575073242 = 0.5091150999069214 + 10.0 * 6.274056434631348
Epoch 690, val loss: 0.9306536912918091
Epoch 700, training loss: 63.218780517578125 = 0.4936806559562683 + 10.0 * 6.272510051727295
Epoch 700, val loss: 0.9234159588813782
Epoch 710, training loss: 63.24238204956055 = 0.4786202609539032 + 10.0 * 6.276376247406006
Epoch 710, val loss: 0.9166925549507141
Epoch 720, training loss: 63.17707061767578 = 0.46377289295196533 + 10.0 * 6.271329879760742
Epoch 720, val loss: 0.9099282026290894
Epoch 730, training loss: 63.1472282409668 = 0.4493933916091919 + 10.0 * 6.2697834968566895
Epoch 730, val loss: 0.9038870334625244
Epoch 740, training loss: 63.147605895996094 = 0.43531161546707153 + 10.0 * 6.2712297439575195
Epoch 740, val loss: 0.8982037305831909
Epoch 750, training loss: 63.096195220947266 = 0.4215241074562073 + 10.0 * 6.267467021942139
Epoch 750, val loss: 0.8924336433410645
Epoch 760, training loss: 63.06683349609375 = 0.4081425070762634 + 10.0 * 6.265869140625
Epoch 760, val loss: 0.8875152468681335
Epoch 770, training loss: 63.09296798706055 = 0.39504584670066833 + 10.0 * 6.269792079925537
Epoch 770, val loss: 0.8828626871109009
Epoch 780, training loss: 63.047733306884766 = 0.3821929693222046 + 10.0 * 6.26655387878418
Epoch 780, val loss: 0.8783268928527832
Epoch 790, training loss: 62.99888229370117 = 0.36967819929122925 + 10.0 * 6.262920379638672
Epoch 790, val loss: 0.8740548491477966
Epoch 800, training loss: 62.9760856628418 = 0.35750025510787964 + 10.0 * 6.2618584632873535
Epoch 800, val loss: 0.8702531456947327
Epoch 810, training loss: 62.9867057800293 = 0.34563466906547546 + 10.0 * 6.2641072273254395
Epoch 810, val loss: 0.8666518330574036
Epoch 820, training loss: 62.97308349609375 = 0.33407217264175415 + 10.0 * 6.263901233673096
Epoch 820, val loss: 0.8636753559112549
Epoch 830, training loss: 62.93595886230469 = 0.32273945212364197 + 10.0 * 6.261322021484375
Epoch 830, val loss: 0.8604128360748291
Epoch 840, training loss: 62.89310073852539 = 0.31186118721961975 + 10.0 * 6.258123874664307
Epoch 840, val loss: 0.8579785823822021
Epoch 850, training loss: 62.86805725097656 = 0.30130061507225037 + 10.0 * 6.256675720214844
Epoch 850, val loss: 0.8556362986564636
Epoch 860, training loss: 62.85615539550781 = 0.2910479009151459 + 10.0 * 6.2565107345581055
Epoch 860, val loss: 0.8536186218261719
Epoch 870, training loss: 62.90105056762695 = 0.2810458540916443 + 10.0 * 6.262000560760498
Epoch 870, val loss: 0.8516854643821716
Epoch 880, training loss: 62.81786346435547 = 0.27132678031921387 + 10.0 * 6.254653453826904
Epoch 880, val loss: 0.8501346111297607
Epoch 890, training loss: 62.80378723144531 = 0.26194027066230774 + 10.0 * 6.254184722900391
Epoch 890, val loss: 0.848952054977417
Epoch 900, training loss: 62.82328796386719 = 0.25287318229675293 + 10.0 * 6.2570414543151855
Epoch 900, val loss: 0.8479679226875305
Epoch 910, training loss: 62.80099868774414 = 0.24408553540706635 + 10.0 * 6.255691051483154
Epoch 910, val loss: 0.8467760682106018
Epoch 920, training loss: 62.75621795654297 = 0.23558759689331055 + 10.0 * 6.252062797546387
Epoch 920, val loss: 0.8462949991226196
Epoch 930, training loss: 62.74372482299805 = 0.22741174697875977 + 10.0 * 6.251631259918213
Epoch 930, val loss: 0.8459821343421936
Epoch 940, training loss: 62.766143798828125 = 0.21955008804798126 + 10.0 * 6.254659175872803
Epoch 940, val loss: 0.8459097146987915
Epoch 950, training loss: 62.71198654174805 = 0.21188075840473175 + 10.0 * 6.2500104904174805
Epoch 950, val loss: 0.8455406427383423
Epoch 960, training loss: 62.699405670166016 = 0.20452630519866943 + 10.0 * 6.24948787689209
Epoch 960, val loss: 0.845976710319519
Epoch 970, training loss: 62.695072174072266 = 0.1974509209394455 + 10.0 * 6.249762058258057
Epoch 970, val loss: 0.8464182019233704
Epoch 980, training loss: 62.74980545043945 = 0.19064779579639435 + 10.0 * 6.255915641784668
Epoch 980, val loss: 0.8469052314758301
Epoch 990, training loss: 62.67042922973633 = 0.18396462500095367 + 10.0 * 6.2486467361450195
Epoch 990, val loss: 0.8473740816116333
Epoch 1000, training loss: 62.6422233581543 = 0.1776701658964157 + 10.0 * 6.246455192565918
Epoch 1000, val loss: 0.848498523235321
Epoch 1010, training loss: 62.62492752075195 = 0.17157858610153198 + 10.0 * 6.245335102081299
Epoch 1010, val loss: 0.8496025204658508
Epoch 1020, training loss: 62.62137222290039 = 0.1657610684633255 + 10.0 * 6.245561122894287
Epoch 1020, val loss: 0.8508915305137634
Epoch 1030, training loss: 62.63746643066406 = 0.16008883714675903 + 10.0 * 6.247737884521484
Epoch 1030, val loss: 0.8521754741668701
Epoch 1040, training loss: 62.58903121948242 = 0.1546221226453781 + 10.0 * 6.243441104888916
Epoch 1040, val loss: 0.8534172773361206
Epoch 1050, training loss: 62.5886344909668 = 0.14940786361694336 + 10.0 * 6.243922710418701
Epoch 1050, val loss: 0.855071485042572
Epoch 1060, training loss: 62.644657135009766 = 0.14442922174930573 + 10.0 * 6.250022888183594
Epoch 1060, val loss: 0.8569505214691162
Epoch 1070, training loss: 62.577205657958984 = 0.13957148790359497 + 10.0 * 6.243763446807861
Epoch 1070, val loss: 0.8584640622138977
Epoch 1080, training loss: 62.551273345947266 = 0.13496164977550507 + 10.0 * 6.241631507873535
Epoch 1080, val loss: 0.8605623841285706
Epoch 1090, training loss: 62.541385650634766 = 0.13053719699382782 + 10.0 * 6.241084575653076
Epoch 1090, val loss: 0.8626077175140381
Epoch 1100, training loss: 62.6091423034668 = 0.1262672394514084 + 10.0 * 6.248287677764893
Epoch 1100, val loss: 0.8646886348724365
Epoch 1110, training loss: 62.55265808105469 = 0.1221524029970169 + 10.0 * 6.243050575256348
Epoch 1110, val loss: 0.8672939538955688
Epoch 1120, training loss: 62.51999282836914 = 0.11819776147603989 + 10.0 * 6.240179538726807
Epoch 1120, val loss: 0.8694065809249878
Epoch 1130, training loss: 62.510597229003906 = 0.11441559344530106 + 10.0 * 6.239618301391602
Epoch 1130, val loss: 0.8721821308135986
Epoch 1140, training loss: 62.565982818603516 = 0.11077015846967697 + 10.0 * 6.245521068572998
Epoch 1140, val loss: 0.8748549222946167
Epoch 1150, training loss: 62.48860549926758 = 0.10724975913763046 + 10.0 * 6.23813533782959
Epoch 1150, val loss: 0.8770228624343872
Epoch 1160, training loss: 62.48139953613281 = 0.10386696457862854 + 10.0 * 6.237753391265869
Epoch 1160, val loss: 0.8798385262489319
Epoch 1170, training loss: 62.46819305419922 = 0.10064326226711273 + 10.0 * 6.236754894256592
Epoch 1170, val loss: 0.8826155066490173
Epoch 1180, training loss: 62.480201721191406 = 0.09754004329442978 + 10.0 * 6.2382659912109375
Epoch 1180, val loss: 0.8854538202285767
Epoch 1190, training loss: 62.469966888427734 = 0.09453849494457245 + 10.0 * 6.237542629241943
Epoch 1190, val loss: 0.8882505893707275
Epoch 1200, training loss: 62.44392776489258 = 0.09165330976247787 + 10.0 * 6.235227584838867
Epoch 1200, val loss: 0.8914152979850769
Epoch 1210, training loss: 62.46404266357422 = 0.08888358622789383 + 10.0 * 6.237515926361084
Epoch 1210, val loss: 0.8942554593086243
Epoch 1220, training loss: 62.43193054199219 = 0.0862138494849205 + 10.0 * 6.234571933746338
Epoch 1220, val loss: 0.8972926735877991
Epoch 1230, training loss: 62.421173095703125 = 0.08367178589105606 + 10.0 * 6.233750343322754
Epoch 1230, val loss: 0.9004992246627808
Epoch 1240, training loss: 62.4158935546875 = 0.0812193751335144 + 10.0 * 6.2334675788879395
Epoch 1240, val loss: 0.9037458300590515
Epoch 1250, training loss: 62.44716262817383 = 0.07886628806591034 + 10.0 * 6.23682975769043
Epoch 1250, val loss: 0.9070814847946167
Epoch 1260, training loss: 62.43003845214844 = 0.07656382024288177 + 10.0 * 6.235347270965576
Epoch 1260, val loss: 0.9099467992782593
Epoch 1270, training loss: 62.413326263427734 = 0.07434507459402084 + 10.0 * 6.233898162841797
Epoch 1270, val loss: 0.913083016872406
Epoch 1280, training loss: 62.39917755126953 = 0.07224193215370178 + 10.0 * 6.232693672180176
Epoch 1280, val loss: 0.9165194630622864
Epoch 1290, training loss: 62.41817855834961 = 0.0702001303434372 + 10.0 * 6.234797477722168
Epoch 1290, val loss: 0.9197782278060913
Epoch 1300, training loss: 62.37364959716797 = 0.06824122369289398 + 10.0 * 6.230540752410889
Epoch 1300, val loss: 0.9232587814331055
Epoch 1310, training loss: 62.387939453125 = 0.06635003536939621 + 10.0 * 6.23215913772583
Epoch 1310, val loss: 0.926842987537384
Epoch 1320, training loss: 62.386112213134766 = 0.06452279537916183 + 10.0 * 6.23215913772583
Epoch 1320, val loss: 0.9301496744155884
Epoch 1330, training loss: 62.36752700805664 = 0.06276760250329971 + 10.0 * 6.230475902557373
Epoch 1330, val loss: 0.9335096478462219
Epoch 1340, training loss: 62.35500717163086 = 0.061070334166288376 + 10.0 * 6.22939395904541
Epoch 1340, val loss: 0.936812698841095
Epoch 1350, training loss: 62.35625457763672 = 0.05944652855396271 + 10.0 * 6.22968053817749
Epoch 1350, val loss: 0.9402191638946533
Epoch 1360, training loss: 62.388675689697266 = 0.057871684432029724 + 10.0 * 6.2330803871154785
Epoch 1360, val loss: 0.9435200095176697
Epoch 1370, training loss: 62.359737396240234 = 0.056349754333496094 + 10.0 * 6.2303385734558105
Epoch 1370, val loss: 0.9473032355308533
Epoch 1380, training loss: 62.33631896972656 = 0.05488627031445503 + 10.0 * 6.228143215179443
Epoch 1380, val loss: 0.9505113363265991
Epoch 1390, training loss: 62.32963180541992 = 0.05347700044512749 + 10.0 * 6.2276153564453125
Epoch 1390, val loss: 0.9541553854942322
Epoch 1400, training loss: 62.34565353393555 = 0.0521206334233284 + 10.0 * 6.229353427886963
Epoch 1400, val loss: 0.9575335383415222
Epoch 1410, training loss: 62.32166290283203 = 0.05080384016036987 + 10.0 * 6.227086067199707
Epoch 1410, val loss: 0.9611690044403076
Epoch 1420, training loss: 62.344688415527344 = 0.04953928291797638 + 10.0 * 6.229515075683594
Epoch 1420, val loss: 0.9648908972740173
Epoch 1430, training loss: 62.32425308227539 = 0.04831376299262047 + 10.0 * 6.227593898773193
Epoch 1430, val loss: 0.9682813882827759
Epoch 1440, training loss: 62.314910888671875 = 0.04712095856666565 + 10.0 * 6.226778984069824
Epoch 1440, val loss: 0.9717228412628174
Epoch 1450, training loss: 62.29636764526367 = 0.045978620648384094 + 10.0 * 6.225039005279541
Epoch 1450, val loss: 0.9752865433692932
Epoch 1460, training loss: 62.29922103881836 = 0.044879477471113205 + 10.0 * 6.225434303283691
Epoch 1460, val loss: 0.9787147641181946
Epoch 1470, training loss: 62.36235427856445 = 0.04380412399768829 + 10.0 * 6.2318549156188965
Epoch 1470, val loss: 0.9820614457130432
Epoch 1480, training loss: 62.30877685546875 = 0.04277675971388817 + 10.0 * 6.22659969329834
Epoch 1480, val loss: 0.9859305024147034
Epoch 1490, training loss: 62.28810501098633 = 0.041768837720155716 + 10.0 * 6.224633693695068
Epoch 1490, val loss: 0.9892873167991638
Epoch 1500, training loss: 62.28118133544922 = 0.04081308841705322 + 10.0 * 6.224036693572998
Epoch 1500, val loss: 0.992881178855896
Epoch 1510, training loss: 62.32991409301758 = 0.03987393528223038 + 10.0 * 6.22900390625
Epoch 1510, val loss: 0.9961583018302917
Epoch 1520, training loss: 62.30425262451172 = 0.03897351399064064 + 10.0 * 6.226527690887451
Epoch 1520, val loss: 0.9998747706413269
Epoch 1530, training loss: 62.281734466552734 = 0.038085564970970154 + 10.0 * 6.224364757537842
Epoch 1530, val loss: 1.0032148361206055
Epoch 1540, training loss: 62.25953674316406 = 0.037238214164972305 + 10.0 * 6.222229957580566
Epoch 1540, val loss: 1.0069339275360107
Epoch 1550, training loss: 62.25088119506836 = 0.03641972318291664 + 10.0 * 6.2214460372924805
Epoch 1550, val loss: 1.0105079412460327
Epoch 1560, training loss: 62.262847900390625 = 0.03563137724995613 + 10.0 * 6.222721576690674
Epoch 1560, val loss: 1.014146327972412
Epoch 1570, training loss: 62.275123596191406 = 0.034857407212257385 + 10.0 * 6.224026679992676
Epoch 1570, val loss: 1.0174574851989746
Epoch 1580, training loss: 62.26586151123047 = 0.03410586714744568 + 10.0 * 6.223175525665283
Epoch 1580, val loss: 1.0208568572998047
Epoch 1590, training loss: 62.250125885009766 = 0.03336915746331215 + 10.0 * 6.221675395965576
Epoch 1590, val loss: 1.0240533351898193
Epoch 1600, training loss: 62.23554229736328 = 0.032667044550180435 + 10.0 * 6.220287799835205
Epoch 1600, val loss: 1.0275726318359375
Epoch 1610, training loss: 62.23965835571289 = 0.03199325501918793 + 10.0 * 6.220766544342041
Epoch 1610, val loss: 1.0308258533477783
Epoch 1620, training loss: 62.274696350097656 = 0.03133075311779976 + 10.0 * 6.224336624145508
Epoch 1620, val loss: 1.0341744422912598
Epoch 1630, training loss: 62.24690628051758 = 0.030686940997838974 + 10.0 * 6.221621990203857
Epoch 1630, val loss: 1.0380134582519531
Epoch 1640, training loss: 62.249053955078125 = 0.030061373487114906 + 10.0 * 6.221899509429932
Epoch 1640, val loss: 1.0410369634628296
Epoch 1650, training loss: 62.2464485168457 = 0.029452869668602943 + 10.0 * 6.2216997146606445
Epoch 1650, val loss: 1.0445504188537598
Epoch 1660, training loss: 62.222389221191406 = 0.028861379250884056 + 10.0 * 6.219352722167969
Epoch 1660, val loss: 1.0478161573410034
Epoch 1670, training loss: 62.21162033081055 = 0.028293384239077568 + 10.0 * 6.218332767486572
Epoch 1670, val loss: 1.0512559413909912
Epoch 1680, training loss: 62.205204010009766 = 0.027745340019464493 + 10.0 * 6.217745780944824
Epoch 1680, val loss: 1.0546761751174927
Epoch 1690, training loss: 62.253963470458984 = 0.02721240743994713 + 10.0 * 6.22267484664917
Epoch 1690, val loss: 1.0579354763031006
Epoch 1700, training loss: 62.20597457885742 = 0.026683758944272995 + 10.0 * 6.217928886413574
Epoch 1700, val loss: 1.0612190961837769
Epoch 1710, training loss: 62.217464447021484 = 0.02617238648235798 + 10.0 * 6.2191290855407715
Epoch 1710, val loss: 1.0645248889923096
Epoch 1720, training loss: 62.20214080810547 = 0.025674624368548393 + 10.0 * 6.217646598815918
Epoch 1720, val loss: 1.067712664604187
Epoch 1730, training loss: 62.21019744873047 = 0.02519756369292736 + 10.0 * 6.218500137329102
Epoch 1730, val loss: 1.0709638595581055
Epoch 1740, training loss: 62.2042350769043 = 0.02472618781030178 + 10.0 * 6.217950820922852
Epoch 1740, val loss: 1.0742522478103638
Epoch 1750, training loss: 62.19196319580078 = 0.024269819259643555 + 10.0 * 6.216769218444824
Epoch 1750, val loss: 1.0775483846664429
Epoch 1760, training loss: 62.25543975830078 = 0.023825230076909065 + 10.0 * 6.223161220550537
Epoch 1760, val loss: 1.0805046558380127
Epoch 1770, training loss: 62.1983757019043 = 0.02339417301118374 + 10.0 * 6.217497825622559
Epoch 1770, val loss: 1.083983302116394
Epoch 1780, training loss: 62.180538177490234 = 0.02296765334904194 + 10.0 * 6.215756893157959
Epoch 1780, val loss: 1.0871723890304565
Epoch 1790, training loss: 62.19148254394531 = 0.022565636783838272 + 10.0 * 6.216891765594482
Epoch 1790, val loss: 1.0904985666275024
Epoch 1800, training loss: 62.212825775146484 = 0.022167906165122986 + 10.0 * 6.2190656661987305
Epoch 1800, val loss: 1.0935457944869995
Epoch 1810, training loss: 62.18901443481445 = 0.021770864725112915 + 10.0 * 6.216724395751953
Epoch 1810, val loss: 1.096154808998108
Epoch 1820, training loss: 62.17304229736328 = 0.02139420062303543 + 10.0 * 6.215165138244629
Epoch 1820, val loss: 1.0996540784835815
Epoch 1830, training loss: 62.181060791015625 = 0.021025601774454117 + 10.0 * 6.21600341796875
Epoch 1830, val loss: 1.1026772260665894
Epoch 1840, training loss: 62.1714973449707 = 0.020663635805249214 + 10.0 * 6.215083122253418
Epoch 1840, val loss: 1.105930209159851
Epoch 1850, training loss: 62.177337646484375 = 0.020313607528805733 + 10.0 * 6.215702533721924
Epoch 1850, val loss: 1.1090284585952759
Epoch 1860, training loss: 62.16471481323242 = 0.019974999129772186 + 10.0 * 6.214474201202393
Epoch 1860, val loss: 1.111839771270752
Epoch 1870, training loss: 62.203125 = 0.019646551460027695 + 10.0 * 6.218348026275635
Epoch 1870, val loss: 1.1150707006454468
Epoch 1880, training loss: 62.15696716308594 = 0.01930631883442402 + 10.0 * 6.213766098022461
Epoch 1880, val loss: 1.1177263259887695
Epoch 1890, training loss: 62.157432556152344 = 0.018991565331816673 + 10.0 * 6.213843822479248
Epoch 1890, val loss: 1.1207103729248047
Epoch 1900, training loss: 62.159847259521484 = 0.018680879846215248 + 10.0 * 6.21411657333374
Epoch 1900, val loss: 1.123503565788269
Epoch 1910, training loss: 62.14213562011719 = 0.01837734691798687 + 10.0 * 6.212375640869141
Epoch 1910, val loss: 1.1265872716903687
Epoch 1920, training loss: 62.18162536621094 = 0.018084585666656494 + 10.0 * 6.216353893280029
Epoch 1920, val loss: 1.1293838024139404
Epoch 1930, training loss: 62.170196533203125 = 0.017791172489523888 + 10.0 * 6.215240478515625
Epoch 1930, val loss: 1.1320148706436157
Epoch 1940, training loss: 62.13788986206055 = 0.017507752403616905 + 10.0 * 6.212038040161133
Epoch 1940, val loss: 1.1353625059127808
Epoch 1950, training loss: 62.12880325317383 = 0.017234094440937042 + 10.0 * 6.211156845092773
Epoch 1950, val loss: 1.1380494832992554
Epoch 1960, training loss: 62.13331985473633 = 0.016972539946436882 + 10.0 * 6.211634635925293
Epoch 1960, val loss: 1.1409380435943604
Epoch 1970, training loss: 62.18893814086914 = 0.01671711727976799 + 10.0 * 6.217222213745117
Epoch 1970, val loss: 1.1439067125320435
Epoch 1980, training loss: 62.13238525390625 = 0.0164494588971138 + 10.0 * 6.2115936279296875
Epoch 1980, val loss: 1.146459937095642
Epoch 1990, training loss: 62.14263153076172 = 0.016197046265006065 + 10.0 * 6.212643623352051
Epoch 1990, val loss: 1.1490148305892944
Epoch 2000, training loss: 62.14421081542969 = 0.015951156616210938 + 10.0 * 6.212826251983643
Epoch 2000, val loss: 1.1519023180007935
Epoch 2010, training loss: 62.123077392578125 = 0.01571051776409149 + 10.0 * 6.2107367515563965
Epoch 2010, val loss: 1.1546876430511475
Epoch 2020, training loss: 62.11370086669922 = 0.015480651520192623 + 10.0 * 6.209822177886963
Epoch 2020, val loss: 1.1573293209075928
Epoch 2030, training loss: 62.119144439697266 = 0.015255635604262352 + 10.0 * 6.210389137268066
Epoch 2030, val loss: 1.160139799118042
Epoch 2040, training loss: 62.14946365356445 = 0.015035031363368034 + 10.0 * 6.213442802429199
Epoch 2040, val loss: 1.1630650758743286
Epoch 2050, training loss: 62.14146041870117 = 0.014811782166361809 + 10.0 * 6.212664604187012
Epoch 2050, val loss: 1.1653623580932617
Epoch 2060, training loss: 62.13898468017578 = 0.01459523756057024 + 10.0 * 6.212439060211182
Epoch 2060, val loss: 1.1675796508789062
Epoch 2070, training loss: 62.11845016479492 = 0.014384999871253967 + 10.0 * 6.210406303405762
Epoch 2070, val loss: 1.1704423427581787
Epoch 2080, training loss: 62.127952575683594 = 0.01418143231421709 + 10.0 * 6.211377143859863
Epoch 2080, val loss: 1.1727726459503174
Epoch 2090, training loss: 62.098480224609375 = 0.013977373018860817 + 10.0 * 6.2084503173828125
Epoch 2090, val loss: 1.175528883934021
Epoch 2100, training loss: 62.102298736572266 = 0.013784432783722878 + 10.0 * 6.208851337432861
Epoch 2100, val loss: 1.178058385848999
Epoch 2110, training loss: 62.14200210571289 = 0.01359696127474308 + 10.0 * 6.212840557098389
Epoch 2110, val loss: 1.1804906129837036
Epoch 2120, training loss: 62.113197326660156 = 0.013404388912022114 + 10.0 * 6.209979057312012
Epoch 2120, val loss: 1.183099389076233
Epoch 2130, training loss: 62.09578323364258 = 0.013217642903327942 + 10.0 * 6.208256721496582
Epoch 2130, val loss: 1.1854010820388794
Epoch 2140, training loss: 62.15424346923828 = 0.013035359792411327 + 10.0 * 6.214120864868164
Epoch 2140, val loss: 1.187743902206421
Epoch 2150, training loss: 62.097267150878906 = 0.012862040661275387 + 10.0 * 6.20844030380249
Epoch 2150, val loss: 1.1904075145721436
Epoch 2160, training loss: 62.08319854736328 = 0.0126860486343503 + 10.0 * 6.2070512771606445
Epoch 2160, val loss: 1.1928389072418213
Epoch 2170, training loss: 62.081214904785156 = 0.012519639916718006 + 10.0 * 6.206869602203369
Epoch 2170, val loss: 1.195341944694519
Epoch 2180, training loss: 62.07835388183594 = 0.01235735323280096 + 10.0 * 6.206599712371826
Epoch 2180, val loss: 1.1978683471679688
Epoch 2190, training loss: 62.178104400634766 = 0.012198932468891144 + 10.0 * 6.216590404510498
Epoch 2190, val loss: 1.200241208076477
Epoch 2200, training loss: 62.131935119628906 = 0.012033318169414997 + 10.0 * 6.2119903564453125
Epoch 2200, val loss: 1.2019511461257935
Epoch 2210, training loss: 62.09330368041992 = 0.011871104128658772 + 10.0 * 6.20814323425293
Epoch 2210, val loss: 1.2045421600341797
Epoch 2220, training loss: 62.070594787597656 = 0.011718844063580036 + 10.0 * 6.205887794494629
Epoch 2220, val loss: 1.2071017026901245
Epoch 2230, training loss: 62.0797119140625 = 0.011574066244065762 + 10.0 * 6.206813812255859
Epoch 2230, val loss: 1.2094391584396362
Epoch 2240, training loss: 62.114898681640625 = 0.011429139412939548 + 10.0 * 6.2103471755981445
Epoch 2240, val loss: 1.2116878032684326
Epoch 2250, training loss: 62.09773635864258 = 0.011278599500656128 + 10.0 * 6.208645820617676
Epoch 2250, val loss: 1.2136708498001099
Epoch 2260, training loss: 62.09115982055664 = 0.011138259433209896 + 10.0 * 6.208002090454102
Epoch 2260, val loss: 1.2158135175704956
Epoch 2270, training loss: 62.06231689453125 = 0.010998438112437725 + 10.0 * 6.205132007598877
Epoch 2270, val loss: 1.2183812856674194
Epoch 2280, training loss: 62.092124938964844 = 0.010864964686334133 + 10.0 * 6.208126068115234
Epoch 2280, val loss: 1.2206881046295166
Epoch 2290, training loss: 62.095008850097656 = 0.01072845607995987 + 10.0 * 6.208428382873535
Epoch 2290, val loss: 1.222589135169983
Epoch 2300, training loss: 62.059654235839844 = 0.010596595704555511 + 10.0 * 6.2049055099487305
Epoch 2300, val loss: 1.2249177694320679
Epoch 2310, training loss: 62.05778121948242 = 0.010469496250152588 + 10.0 * 6.204730987548828
Epoch 2310, val loss: 1.2271583080291748
Epoch 2320, training loss: 62.052730560302734 = 0.010346402414143085 + 10.0 * 6.204238414764404
Epoch 2320, val loss: 1.2293528318405151
Epoch 2330, training loss: 62.074771881103516 = 0.010227502323687077 + 10.0 * 6.206454277038574
Epoch 2330, val loss: 1.2318171262741089
Epoch 2340, training loss: 62.090370178222656 = 0.010103539563715458 + 10.0 * 6.20802640914917
Epoch 2340, val loss: 1.2339096069335938
Epoch 2350, training loss: 62.062660217285156 = 0.0099802790209651 + 10.0 * 6.205267906188965
Epoch 2350, val loss: 1.2354540824890137
Epoch 2360, training loss: 62.058677673339844 = 0.009862788952887058 + 10.0 * 6.20488166809082
Epoch 2360, val loss: 1.2376724481582642
Epoch 2370, training loss: 62.047882080078125 = 0.009749091230332851 + 10.0 * 6.203813076019287
Epoch 2370, val loss: 1.2396996021270752
Epoch 2380, training loss: 62.058624267578125 = 0.009639685042202473 + 10.0 * 6.204898357391357
Epoch 2380, val loss: 1.2417844533920288
Epoch 2390, training loss: 62.08762741088867 = 0.009530534967780113 + 10.0 * 6.207809925079346
Epoch 2390, val loss: 1.2439380884170532
Epoch 2400, training loss: 62.05159378051758 = 0.00941698718816042 + 10.0 * 6.204217433929443
Epoch 2400, val loss: 1.2458049058914185
Epoch 2410, training loss: 62.04413986206055 = 0.00931200198829174 + 10.0 * 6.203482627868652
Epoch 2410, val loss: 1.2476661205291748
Epoch 2420, training loss: 62.06754684448242 = 0.009210350923240185 + 10.0 * 6.205833911895752
Epoch 2420, val loss: 1.2497392892837524
Epoch 2430, training loss: 62.076942443847656 = 0.009106980636715889 + 10.0 * 6.206783771514893
Epoch 2430, val loss: 1.2520513534545898
Epoch 2440, training loss: 62.064849853515625 = 0.009002135135233402 + 10.0 * 6.205584526062012
Epoch 2440, val loss: 1.2536567449569702
Epoch 2450, training loss: 62.03828430175781 = 0.008901630528271198 + 10.0 * 6.202938079833984
Epoch 2450, val loss: 1.2554212808609009
Epoch 2460, training loss: 62.03717041015625 = 0.008805183693766594 + 10.0 * 6.202836513519287
Epoch 2460, val loss: 1.2573957443237305
Epoch 2470, training loss: 62.09415054321289 = 0.00871328730136156 + 10.0 * 6.20854377746582
Epoch 2470, val loss: 1.2590190172195435
Epoch 2480, training loss: 62.03881072998047 = 0.008614953607320786 + 10.0 * 6.203019618988037
Epoch 2480, val loss: 1.2614421844482422
Epoch 2490, training loss: 62.02650451660156 = 0.008523153141140938 + 10.0 * 6.201798439025879
Epoch 2490, val loss: 1.2630282640457153
Epoch 2500, training loss: 62.033538818359375 = 0.008434999734163284 + 10.0 * 6.202510356903076
Epoch 2500, val loss: 1.2651591300964355
Epoch 2510, training loss: 62.108116149902344 = 0.008349885232746601 + 10.0 * 6.209976673126221
Epoch 2510, val loss: 1.2672425508499146
Epoch 2520, training loss: 62.05023193359375 = 0.008253500796854496 + 10.0 * 6.204197883605957
Epoch 2520, val loss: 1.2683688402175903
Epoch 2530, training loss: 62.026302337646484 = 0.008168178610503674 + 10.0 * 6.201813697814941
Epoch 2530, val loss: 1.270591139793396
Epoch 2540, training loss: 62.04683303833008 = 0.008083670400083065 + 10.0 * 6.2038750648498535
Epoch 2540, val loss: 1.2721275091171265
Epoch 2550, training loss: 62.03229904174805 = 0.007999321445822716 + 10.0 * 6.20242977142334
Epoch 2550, val loss: 1.2740464210510254
Epoch 2560, training loss: 62.02957534790039 = 0.007918168790638447 + 10.0 * 6.202165603637695
Epoch 2560, val loss: 1.2758044004440308
Epoch 2570, training loss: 62.025020599365234 = 0.007837706245481968 + 10.0 * 6.201718330383301
Epoch 2570, val loss: 1.2775393724441528
Epoch 2580, training loss: 62.04821014404297 = 0.007760926615446806 + 10.0 * 6.204045295715332
Epoch 2580, val loss: 1.2795199155807495
Epoch 2590, training loss: 62.017852783203125 = 0.007680537644773722 + 10.0 * 6.201017379760742
Epoch 2590, val loss: 1.2810178995132446
Epoch 2600, training loss: 62.02324676513672 = 0.007604328449815512 + 10.0 * 6.201564311981201
Epoch 2600, val loss: 1.2828540802001953
Epoch 2610, training loss: 62.0445556640625 = 0.007531024049967527 + 10.0 * 6.203702449798584
Epoch 2610, val loss: 1.284833312034607
Epoch 2620, training loss: 62.031551361083984 = 0.0074568199925124645 + 10.0 * 6.202409267425537
Epoch 2620, val loss: 1.2859750986099243
Epoch 2630, training loss: 62.04176712036133 = 0.007382772862911224 + 10.0 * 6.203438758850098
Epoch 2630, val loss: 1.2875123023986816
Epoch 2640, training loss: 62.010772705078125 = 0.007307596504688263 + 10.0 * 6.20034646987915
Epoch 2640, val loss: 1.28933584690094
Epoch 2650, training loss: 62.01487350463867 = 0.007237613666802645 + 10.0 * 6.200763702392578
Epoch 2650, val loss: 1.2909226417541504
Epoch 2660, training loss: 62.02882385253906 = 0.007169239688664675 + 10.0 * 6.202165603637695
Epoch 2660, val loss: 1.2925631999969482
Epoch 2670, training loss: 62.01869201660156 = 0.007101298775523901 + 10.0 * 6.2011590003967285
Epoch 2670, val loss: 1.2942235469818115
Epoch 2680, training loss: 62.008914947509766 = 0.00703482935205102 + 10.0 * 6.200188159942627
Epoch 2680, val loss: 1.2962315082550049
Epoch 2690, training loss: 62.0332145690918 = 0.006971034687012434 + 10.0 * 6.202624320983887
Epoch 2690, val loss: 1.2978110313415527
Epoch 2700, training loss: 62.02369689941406 = 0.006902654189616442 + 10.0 * 6.201679706573486
Epoch 2700, val loss: 1.299165964126587
Epoch 2710, training loss: 62.003292083740234 = 0.006837307009845972 + 10.0 * 6.199645519256592
Epoch 2710, val loss: 1.3006778955459595
Epoch 2720, training loss: 62.01852798461914 = 0.006777268368750811 + 10.0 * 6.201174736022949
Epoch 2720, val loss: 1.3024795055389404
Epoch 2730, training loss: 62.06069564819336 = 0.006714895833283663 + 10.0 * 6.205398082733154
Epoch 2730, val loss: 1.3038136959075928
Epoch 2740, training loss: 62.01287841796875 = 0.006648879032582045 + 10.0 * 6.200623035430908
Epoch 2740, val loss: 1.3051400184631348
Epoch 2750, training loss: 62.026973724365234 = 0.006587883457541466 + 10.0 * 6.202038764953613
Epoch 2750, val loss: 1.3068519830703735
Epoch 2760, training loss: 61.998565673828125 = 0.006528290919959545 + 10.0 * 6.199203968048096
Epoch 2760, val loss: 1.3081631660461426
Epoch 2770, training loss: 61.9952278137207 = 0.006469080224633217 + 10.0 * 6.198875904083252
Epoch 2770, val loss: 1.3099302053451538
Epoch 2780, training loss: 61.98955535888672 = 0.00641381973400712 + 10.0 * 6.198314189910889
Epoch 2780, val loss: 1.3114465475082397
Epoch 2790, training loss: 61.99052810668945 = 0.006358593702316284 + 10.0 * 6.198416709899902
Epoch 2790, val loss: 1.3130264282226562
Epoch 2800, training loss: 62.03467559814453 = 0.006303795613348484 + 10.0 * 6.202836990356445
Epoch 2800, val loss: 1.3145865201950073
Epoch 2810, training loss: 62.0483283996582 = 0.006246532313525677 + 10.0 * 6.2042083740234375
Epoch 2810, val loss: 1.3153074979782104
Epoch 2820, training loss: 62.009586334228516 = 0.006190495565533638 + 10.0 * 6.200339317321777
Epoch 2820, val loss: 1.316908597946167
Epoch 2830, training loss: 61.992984771728516 = 0.006136104464530945 + 10.0 * 6.1986846923828125
Epoch 2830, val loss: 1.318189263343811
Epoch 2840, training loss: 61.98603439331055 = 0.006083847489207983 + 10.0 * 6.197995185852051
Epoch 2840, val loss: 1.3199419975280762
Epoch 2850, training loss: 61.98862838745117 = 0.0060340482741594315 + 10.0 * 6.198259353637695
Epoch 2850, val loss: 1.3211058378219604
Epoch 2860, training loss: 62.02527618408203 = 0.005983419716358185 + 10.0 * 6.201929092407227
Epoch 2860, val loss: 1.3224164247512817
Epoch 2870, training loss: 61.98905944824219 = 0.0059315417893230915 + 10.0 * 6.198312759399414
Epoch 2870, val loss: 1.3240300416946411
Epoch 2880, training loss: 61.99748229980469 = 0.005882615223526955 + 10.0 * 6.199160099029541
Epoch 2880, val loss: 1.325254201889038
Epoch 2890, training loss: 61.98916244506836 = 0.005834178999066353 + 10.0 * 6.198332786560059
Epoch 2890, val loss: 1.3265469074249268
Epoch 2900, training loss: 61.971519470214844 = 0.0057865590788424015 + 10.0 * 6.196573257446289
Epoch 2900, val loss: 1.3282073736190796
Epoch 2910, training loss: 61.99764633178711 = 0.005741788540035486 + 10.0 * 6.199190616607666
Epoch 2910, val loss: 1.3298094272613525
Epoch 2920, training loss: 62.02313995361328 = 0.005693426821380854 + 10.0 * 6.20174503326416
Epoch 2920, val loss: 1.3307371139526367
Epoch 2930, training loss: 62.00168991088867 = 0.0056424615904688835 + 10.0 * 6.1996049880981445
Epoch 2930, val loss: 1.3317222595214844
Epoch 2940, training loss: 61.97281265258789 = 0.005597312469035387 + 10.0 * 6.19672155380249
Epoch 2940, val loss: 1.3331650495529175
Epoch 2950, training loss: 61.96708679199219 = 0.005552771035581827 + 10.0 * 6.196153163909912
Epoch 2950, val loss: 1.3345173597335815
Epoch 2960, training loss: 61.97226333618164 = 0.005511189345270395 + 10.0 * 6.1966753005981445
Epoch 2960, val loss: 1.3358992338180542
Epoch 2970, training loss: 62.021427154541016 = 0.0054709212854504585 + 10.0 * 6.201595783233643
Epoch 2970, val loss: 1.3370171785354614
Epoch 2980, training loss: 61.96871566772461 = 0.005424177274107933 + 10.0 * 6.196329116821289
Epoch 2980, val loss: 1.3385443687438965
Epoch 2990, training loss: 61.988243103027344 = 0.0053824265487492085 + 10.0 * 6.198286056518555
Epoch 2990, val loss: 1.339595913887024
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 87.907958984375 = 1.9393649101257324 + 10.0 * 8.5968599319458
Epoch 0, val loss: 1.940751314163208
Epoch 10, training loss: 87.89227294921875 = 1.9292525053024292 + 10.0 * 8.596302032470703
Epoch 10, val loss: 1.9299770593643188
Epoch 20, training loss: 87.83332061767578 = 1.9169148206710815 + 10.0 * 8.59164047241211
Epoch 20, val loss: 1.9164196252822876
Epoch 30, training loss: 87.47891998291016 = 1.9010075330734253 + 10.0 * 8.557790756225586
Epoch 30, val loss: 1.8988527059555054
Epoch 40, training loss: 85.60204315185547 = 1.8817516565322876 + 10.0 * 8.372029304504395
Epoch 40, val loss: 1.8785861730575562
Epoch 50, training loss: 78.74314880371094 = 1.8609052896499634 + 10.0 * 7.688223838806152
Epoch 50, val loss: 1.856652855873108
Epoch 60, training loss: 76.11659240722656 = 1.8431366682052612 + 10.0 * 7.427345275878906
Epoch 60, val loss: 1.8395543098449707
Epoch 70, training loss: 73.86927795410156 = 1.8300087451934814 + 10.0 * 7.203927040100098
Epoch 70, val loss: 1.8272584676742554
Epoch 80, training loss: 71.75176239013672 = 1.8156743049621582 + 10.0 * 6.9936089515686035
Epoch 80, val loss: 1.8136597871780396
Epoch 90, training loss: 70.37252044677734 = 1.8034205436706543 + 10.0 * 6.85690975189209
Epoch 90, val loss: 1.8024922609329224
Epoch 100, training loss: 69.5068359375 = 1.7924964427947998 + 10.0 * 6.7714338302612305
Epoch 100, val loss: 1.79231595993042
Epoch 110, training loss: 68.79767608642578 = 1.7828456163406372 + 10.0 * 6.701482772827148
Epoch 110, val loss: 1.7830157279968262
Epoch 120, training loss: 68.24929809570312 = 1.772156000137329 + 10.0 * 6.647714614868164
Epoch 120, val loss: 1.773084044456482
Epoch 130, training loss: 67.85835266113281 = 1.7606195211410522 + 10.0 * 6.609773635864258
Epoch 130, val loss: 1.7626608610153198
Epoch 140, training loss: 67.56396484375 = 1.748426914215088 + 10.0 * 6.5815534591674805
Epoch 140, val loss: 1.7518833875656128
Epoch 150, training loss: 67.33206939697266 = 1.7352986335754395 + 10.0 * 6.5596771240234375
Epoch 150, val loss: 1.7401245832443237
Epoch 160, training loss: 67.1434097290039 = 1.721055507659912 + 10.0 * 6.542235851287842
Epoch 160, val loss: 1.7274004220962524
Epoch 170, training loss: 66.9628677368164 = 1.705600380897522 + 10.0 * 6.525726795196533
Epoch 170, val loss: 1.7136260271072388
Epoch 180, training loss: 66.78113555908203 = 1.6889641284942627 + 10.0 * 6.509217262268066
Epoch 180, val loss: 1.6989994049072266
Epoch 190, training loss: 66.62432098388672 = 1.6711201667785645 + 10.0 * 6.4953203201293945
Epoch 190, val loss: 1.6834359169006348
Epoch 200, training loss: 66.43026733398438 = 1.6517486572265625 + 10.0 * 6.477851867675781
Epoch 200, val loss: 1.6664447784423828
Epoch 210, training loss: 66.25286102294922 = 1.6306567192077637 + 10.0 * 6.462221145629883
Epoch 210, val loss: 1.6482895612716675
Epoch 220, training loss: 66.0998306274414 = 1.6078615188598633 + 10.0 * 6.449196815490723
Epoch 220, val loss: 1.6285518407821655
Epoch 230, training loss: 65.98710632324219 = 1.583066463470459 + 10.0 * 6.440403461456299
Epoch 230, val loss: 1.607225775718689
Epoch 240, training loss: 65.83001708984375 = 1.5566246509552002 + 10.0 * 6.42733907699585
Epoch 240, val loss: 1.5844998359680176
Epoch 250, training loss: 65.70301818847656 = 1.52853262424469 + 10.0 * 6.417448043823242
Epoch 250, val loss: 1.5603461265563965
Epoch 260, training loss: 65.60310363769531 = 1.4988635778427124 + 10.0 * 6.41042423248291
Epoch 260, val loss: 1.5350358486175537
Epoch 270, training loss: 65.49740600585938 = 1.4677925109863281 + 10.0 * 6.402961730957031
Epoch 270, val loss: 1.5085853338241577
Epoch 280, training loss: 65.38361358642578 = 1.4357203245162964 + 10.0 * 6.394789218902588
Epoch 280, val loss: 1.481613039970398
Epoch 290, training loss: 65.3026123046875 = 1.4028544425964355 + 10.0 * 6.389975547790527
Epoch 290, val loss: 1.4542397260665894
Epoch 300, training loss: 65.19112396240234 = 1.3693279027938843 + 10.0 * 6.382179260253906
Epoch 300, val loss: 1.4265700578689575
Epoch 310, training loss: 65.09312438964844 = 1.3356502056121826 + 10.0 * 6.375747203826904
Epoch 310, val loss: 1.3992537260055542
Epoch 320, training loss: 65.01781463623047 = 1.301894187927246 + 10.0 * 6.371592044830322
Epoch 320, val loss: 1.3724300861358643
Epoch 330, training loss: 64.95140838623047 = 1.2680670022964478 + 10.0 * 6.3683342933654785
Epoch 330, val loss: 1.3454066514968872
Epoch 340, training loss: 64.846923828125 = 1.2346099615097046 + 10.0 * 6.361231327056885
Epoch 340, val loss: 1.3193145990371704
Epoch 350, training loss: 64.75237274169922 = 1.2015990018844604 + 10.0 * 6.355077743530273
Epoch 350, val loss: 1.2939066886901855
Epoch 360, training loss: 64.70036315917969 = 1.1690627336502075 + 10.0 * 6.353129863739014
Epoch 360, val loss: 1.269194483757019
Epoch 370, training loss: 64.63091278076172 = 1.1367275714874268 + 10.0 * 6.3494181632995605
Epoch 370, val loss: 1.2448334693908691
Epoch 380, training loss: 64.52644348144531 = 1.105406641960144 + 10.0 * 6.342103958129883
Epoch 380, val loss: 1.2216870784759521
Epoch 390, training loss: 64.45358276367188 = 1.0747578144073486 + 10.0 * 6.337882041931152
Epoch 390, val loss: 1.1992429494857788
Epoch 400, training loss: 64.38607025146484 = 1.0448002815246582 + 10.0 * 6.334126949310303
Epoch 400, val loss: 1.1776080131530762
Epoch 410, training loss: 64.32770538330078 = 1.0155246257781982 + 10.0 * 6.3312177658081055
Epoch 410, val loss: 1.1566476821899414
Epoch 420, training loss: 64.32406616210938 = 0.986707329750061 + 10.0 * 6.333735942840576
Epoch 420, val loss: 1.1364672183990479
Epoch 430, training loss: 64.21797943115234 = 0.958902895450592 + 10.0 * 6.3259077072143555
Epoch 430, val loss: 1.1170896291732788
Epoch 440, training loss: 64.13994598388672 = 0.9319435954093933 + 10.0 * 6.320799827575684
Epoch 440, val loss: 1.0984041690826416
Epoch 450, training loss: 64.08250427246094 = 0.9057648777961731 + 10.0 * 6.317674160003662
Epoch 450, val loss: 1.080695629119873
Epoch 460, training loss: 64.04551696777344 = 0.8802884817123413 + 10.0 * 6.31652307510376
Epoch 460, val loss: 1.0637813806533813
Epoch 470, training loss: 64.02790069580078 = 0.8554574847221375 + 10.0 * 6.317244052886963
Epoch 470, val loss: 1.0472828149795532
Epoch 480, training loss: 63.93464660644531 = 0.8315993547439575 + 10.0 * 6.310304641723633
Epoch 480, val loss: 1.0319486856460571
Epoch 490, training loss: 63.88154220581055 = 0.8084450960159302 + 10.0 * 6.307309627532959
Epoch 490, val loss: 1.0171699523925781
Epoch 500, training loss: 63.841548919677734 = 0.7860198616981506 + 10.0 * 6.305552959442139
Epoch 500, val loss: 1.0032418966293335
Epoch 510, training loss: 63.85367202758789 = 0.764151930809021 + 10.0 * 6.3089518547058105
Epoch 510, val loss: 0.9898994565010071
Epoch 520, training loss: 63.75034713745117 = 0.7431081533432007 + 10.0 * 6.300724029541016
Epoch 520, val loss: 0.9772570729255676
Epoch 530, training loss: 63.71818923950195 = 0.7226836085319519 + 10.0 * 6.299550533294678
Epoch 530, val loss: 0.965538740158081
Epoch 540, training loss: 63.683345794677734 = 0.7028108239173889 + 10.0 * 6.29805326461792
Epoch 540, val loss: 0.9541332125663757
Epoch 550, training loss: 63.647071838378906 = 0.6834851503372192 + 10.0 * 6.296358585357666
Epoch 550, val loss: 0.9436540603637695
Epoch 560, training loss: 63.60723876953125 = 0.6646870970726013 + 10.0 * 6.294255256652832
Epoch 560, val loss: 0.9335500597953796
Epoch 570, training loss: 63.57086944580078 = 0.6464607119560242 + 10.0 * 6.292440891265869
Epoch 570, val loss: 0.9241841435432434
Epoch 580, training loss: 63.54251480102539 = 0.628753662109375 + 10.0 * 6.291376113891602
Epoch 580, val loss: 0.915328860282898
Epoch 590, training loss: 63.504066467285156 = 0.611438512802124 + 10.0 * 6.289262771606445
Epoch 590, val loss: 0.9070557355880737
Epoch 600, training loss: 63.47395706176758 = 0.5945437550544739 + 10.0 * 6.2879414558410645
Epoch 600, val loss: 0.8993334770202637
Epoch 610, training loss: 63.443607330322266 = 0.5782135725021362 + 10.0 * 6.286539554595947
Epoch 610, val loss: 0.8920359015464783
Epoch 620, training loss: 63.431243896484375 = 0.5621347427368164 + 10.0 * 6.2869110107421875
Epoch 620, val loss: 0.8850923776626587
Epoch 630, training loss: 63.39318084716797 = 0.5465548038482666 + 10.0 * 6.28466272354126
Epoch 630, val loss: 0.8788796067237854
Epoch 640, training loss: 63.34914779663086 = 0.5313511490821838 + 10.0 * 6.2817792892456055
Epoch 640, val loss: 0.8731371164321899
Epoch 650, training loss: 63.31716537475586 = 0.5166075229644775 + 10.0 * 6.280055522918701
Epoch 650, val loss: 0.8679355978965759
Epoch 660, training loss: 63.28668212890625 = 0.5022137761116028 + 10.0 * 6.278447151184082
Epoch 660, val loss: 0.8631080389022827
Epoch 670, training loss: 63.33027648925781 = 0.4881351590156555 + 10.0 * 6.284214019775391
Epoch 670, val loss: 0.8583437204360962
Epoch 680, training loss: 63.26795959472656 = 0.47424253821372986 + 10.0 * 6.279371738433838
Epoch 680, val loss: 0.8545683026313782
Epoch 690, training loss: 63.22590255737305 = 0.46081122756004333 + 10.0 * 6.2765092849731445
Epoch 690, val loss: 0.8508591055870056
Epoch 700, training loss: 63.195743560791016 = 0.44779661297798157 + 10.0 * 6.274794578552246
Epoch 700, val loss: 0.8475399017333984
Epoch 710, training loss: 63.170860290527344 = 0.43505793809890747 + 10.0 * 6.273580074310303
Epoch 710, val loss: 0.8445286750793457
Epoch 720, training loss: 63.19575881958008 = 0.4226413667201996 + 10.0 * 6.2773118019104
Epoch 720, val loss: 0.8417690992355347
Epoch 730, training loss: 63.16159439086914 = 0.4103524386882782 + 10.0 * 6.275124549865723
Epoch 730, val loss: 0.8394201993942261
Epoch 740, training loss: 63.1035270690918 = 0.3985493779182434 + 10.0 * 6.270497798919678
Epoch 740, val loss: 0.8373692035675049
Epoch 750, training loss: 63.081459045410156 = 0.38702863454818726 + 10.0 * 6.269443035125732
Epoch 750, val loss: 0.8355525732040405
Epoch 760, training loss: 63.05882263183594 = 0.37574583292007446 + 10.0 * 6.268307685852051
Epoch 760, val loss: 0.8341989517211914
Epoch 770, training loss: 63.069549560546875 = 0.36478158831596375 + 10.0 * 6.270476818084717
Epoch 770, val loss: 0.8329280614852905
Epoch 780, training loss: 63.0318489074707 = 0.3541373610496521 + 10.0 * 6.267771244049072
Epoch 780, val loss: 0.8318749070167542
Epoch 790, training loss: 63.00537109375 = 0.34368517994880676 + 10.0 * 6.266168594360352
Epoch 790, val loss: 0.8312662243843079
Epoch 800, training loss: 62.98579025268555 = 0.3336380124092102 + 10.0 * 6.2652153968811035
Epoch 800, val loss: 0.8304645419120789
Epoch 810, training loss: 62.95860290527344 = 0.32380738854408264 + 10.0 * 6.263479709625244
Epoch 810, val loss: 0.8304114937782288
Epoch 820, training loss: 63.0297966003418 = 0.3142606019973755 + 10.0 * 6.2715535163879395
Epoch 820, val loss: 0.8305262923240662
Epoch 830, training loss: 62.92849349975586 = 0.3050159811973572 + 10.0 * 6.26234769821167
Epoch 830, val loss: 0.8303421139717102
Epoch 840, training loss: 62.90485382080078 = 0.29606324434280396 + 10.0 * 6.260879039764404
Epoch 840, val loss: 0.8305264711380005
Epoch 850, training loss: 62.89683532714844 = 0.2873840928077698 + 10.0 * 6.2609453201293945
Epoch 850, val loss: 0.831099271774292
Epoch 860, training loss: 62.90202713012695 = 0.27892833948135376 + 10.0 * 6.262310028076172
Epoch 860, val loss: 0.8318214416503906
Epoch 870, training loss: 62.85563659667969 = 0.27083486318588257 + 10.0 * 6.258480072021484
Epoch 870, val loss: 0.8325421810150146
Epoch 880, training loss: 62.83824157714844 = 0.26294374465942383 + 10.0 * 6.2575297355651855
Epoch 880, val loss: 0.8336735367774963
Epoch 890, training loss: 62.81949996948242 = 0.2553307116031647 + 10.0 * 6.256417274475098
Epoch 890, val loss: 0.8350359797477722
Epoch 900, training loss: 62.81416320800781 = 0.24795639514923096 + 10.0 * 6.25662088394165
Epoch 900, val loss: 0.8366474509239197
Epoch 910, training loss: 62.80842590332031 = 0.24077412486076355 + 10.0 * 6.256765365600586
Epoch 910, val loss: 0.8381150960922241
Epoch 920, training loss: 62.79045104980469 = 0.23384793102741241 + 10.0 * 6.255660057067871
Epoch 920, val loss: 0.8396655917167664
Epoch 930, training loss: 62.77018356323242 = 0.2271684855222702 + 10.0 * 6.25430154800415
Epoch 930, val loss: 0.8417209386825562
Epoch 940, training loss: 62.74863052368164 = 0.2207239717245102 + 10.0 * 6.252790927886963
Epoch 940, val loss: 0.8439024686813354
Epoch 950, training loss: 62.747711181640625 = 0.21450598537921906 + 10.0 * 6.253320693969727
Epoch 950, val loss: 0.8461405634880066
Epoch 960, training loss: 62.739356994628906 = 0.20841644704341888 + 10.0 * 6.25309419631958
Epoch 960, val loss: 0.8486658334732056
Epoch 970, training loss: 62.822662353515625 = 0.20259994268417358 + 10.0 * 6.2620062828063965
Epoch 970, val loss: 0.8506038784980774
Epoch 980, training loss: 62.72822189331055 = 0.19683365523815155 + 10.0 * 6.253138542175293
Epoch 980, val loss: 0.8532043099403381
Epoch 990, training loss: 62.692481994628906 = 0.19138453900814056 + 10.0 * 6.250109672546387
Epoch 990, val loss: 0.8559053540229797
Epoch 1000, training loss: 62.67465591430664 = 0.18610307574272156 + 10.0 * 6.248855113983154
Epoch 1000, val loss: 0.8589003682136536
Epoch 1010, training loss: 62.66262435913086 = 0.18100257217884064 + 10.0 * 6.248162269592285
Epoch 1010, val loss: 0.8620250225067139
Epoch 1020, training loss: 62.65644454956055 = 0.17604660987854004 + 10.0 * 6.248039722442627
Epoch 1020, val loss: 0.8651012182235718
Epoch 1030, training loss: 62.69428634643555 = 0.17121367156505585 + 10.0 * 6.252306938171387
Epoch 1030, val loss: 0.8680732846260071
Epoch 1040, training loss: 62.633548736572266 = 0.16654177010059357 + 10.0 * 6.246700763702393
Epoch 1040, val loss: 0.8714659214019775
Epoch 1050, training loss: 62.67610549926758 = 0.16204363107681274 + 10.0 * 6.251406192779541
Epoch 1050, val loss: 0.8747466206550598
Epoch 1060, training loss: 62.60988998413086 = 0.15761467814445496 + 10.0 * 6.245227336883545
Epoch 1060, val loss: 0.878366231918335
Epoch 1070, training loss: 62.601985931396484 = 0.15338553488254547 + 10.0 * 6.2448601722717285
Epoch 1070, val loss: 0.8818947076797485
Epoch 1080, training loss: 62.593021392822266 = 0.14930623769760132 + 10.0 * 6.24437141418457
Epoch 1080, val loss: 0.8856703042984009
Epoch 1090, training loss: 62.5989875793457 = 0.14534008502960205 + 10.0 * 6.245364665985107
Epoch 1090, val loss: 0.8895130753517151
Epoch 1100, training loss: 62.60856246948242 = 0.14145860075950623 + 10.0 * 6.246710300445557
Epoch 1100, val loss: 0.89351886510849
Epoch 1110, training loss: 62.58221435546875 = 0.13770881295204163 + 10.0 * 6.244450569152832
Epoch 1110, val loss: 0.8970447778701782
Epoch 1120, training loss: 62.56509780883789 = 0.13408833742141724 + 10.0 * 6.243101119995117
Epoch 1120, val loss: 0.9012191891670227
Epoch 1130, training loss: 62.543575286865234 = 0.1305907517671585 + 10.0 * 6.241298198699951
Epoch 1130, val loss: 0.9053407907485962
Epoch 1140, training loss: 62.53838348388672 = 0.1272115707397461 + 10.0 * 6.241117000579834
Epoch 1140, val loss: 0.9095649123191833
Epoch 1150, training loss: 62.568763732910156 = 0.12392222136259079 + 10.0 * 6.2444844245910645
Epoch 1150, val loss: 0.9138206243515015
Epoch 1160, training loss: 62.553958892822266 = 0.1207173690199852 + 10.0 * 6.243324279785156
Epoch 1160, val loss: 0.9183691740036011
Epoch 1170, training loss: 62.54439926147461 = 0.11760947853326797 + 10.0 * 6.242678642272949
Epoch 1170, val loss: 0.922265350818634
Epoch 1180, training loss: 62.52770233154297 = 0.1145850196480751 + 10.0 * 6.241311550140381
Epoch 1180, val loss: 0.9268816709518433
Epoch 1190, training loss: 62.50175476074219 = 0.11163437366485596 + 10.0 * 6.239012241363525
Epoch 1190, val loss: 0.9313753843307495
Epoch 1200, training loss: 62.48996353149414 = 0.1088099479675293 + 10.0 * 6.238115310668945
Epoch 1200, val loss: 0.9359355568885803
Epoch 1210, training loss: 62.49250030517578 = 0.10605862736701965 + 10.0 * 6.238644123077393
Epoch 1210, val loss: 0.9406378269195557
Epoch 1220, training loss: 62.50176239013672 = 0.10337211191654205 + 10.0 * 6.23983907699585
Epoch 1220, val loss: 0.945442259311676
Epoch 1230, training loss: 62.49805450439453 = 0.10076246410608292 + 10.0 * 6.239729404449463
Epoch 1230, val loss: 0.9501921534538269
Epoch 1240, training loss: 62.46767807006836 = 0.09821723401546478 + 10.0 * 6.236946105957031
Epoch 1240, val loss: 0.9548478722572327
Epoch 1250, training loss: 62.453617095947266 = 0.09576750546693802 + 10.0 * 6.235785007476807
Epoch 1250, val loss: 0.9595757126808167
Epoch 1260, training loss: 62.45245361328125 = 0.09339133650064468 + 10.0 * 6.23590612411499
Epoch 1260, val loss: 0.964765191078186
Epoch 1270, training loss: 62.474002838134766 = 0.09107706695795059 + 10.0 * 6.238292694091797
Epoch 1270, val loss: 0.9697274565696716
Epoch 1280, training loss: 62.44055938720703 = 0.08881551772356033 + 10.0 * 6.235174655914307
Epoch 1280, val loss: 0.9743732810020447
Epoch 1290, training loss: 62.42837905883789 = 0.0866287499666214 + 10.0 * 6.234175205230713
Epoch 1290, val loss: 0.9796117544174194
Epoch 1300, training loss: 62.421051025390625 = 0.084508016705513 + 10.0 * 6.233654499053955
Epoch 1300, val loss: 0.984600841999054
Epoch 1310, training loss: 62.541568756103516 = 0.08242734521627426 + 10.0 * 6.245913982391357
Epoch 1310, val loss: 0.9900018572807312
Epoch 1320, training loss: 62.44847869873047 = 0.08038469403982162 + 10.0 * 6.236809730529785
Epoch 1320, val loss: 0.9942412376403809
Epoch 1330, training loss: 62.406612396240234 = 0.07840751111507416 + 10.0 * 6.232820510864258
Epoch 1330, val loss: 0.999300479888916
Epoch 1340, training loss: 62.39285659790039 = 0.07653108239173889 + 10.0 * 6.231632709503174
Epoch 1340, val loss: 1.0046279430389404
Epoch 1350, training loss: 62.38714599609375 = 0.0747002363204956 + 10.0 * 6.2312445640563965
Epoch 1350, val loss: 1.0099554061889648
Epoch 1360, training loss: 62.37952423095703 = 0.072921022772789 + 10.0 * 6.230660438537598
Epoch 1360, val loss: 1.0152440071105957
Epoch 1370, training loss: 62.38283920288086 = 0.07118519395589828 + 10.0 * 6.231165409088135
Epoch 1370, val loss: 1.0204682350158691
Epoch 1380, training loss: 62.46525192260742 = 0.06947321444749832 + 10.0 * 6.239577770233154
Epoch 1380, val loss: 1.025610327720642
Epoch 1390, training loss: 62.38762664794922 = 0.06780825555324554 + 10.0 * 6.2319817543029785
Epoch 1390, val loss: 1.030508041381836
Epoch 1400, training loss: 62.374698638916016 = 0.06617886573076248 + 10.0 * 6.230852127075195
Epoch 1400, val loss: 1.0356568098068237
Epoch 1410, training loss: 62.35601806640625 = 0.06462801247835159 + 10.0 * 6.2291388511657715
Epoch 1410, val loss: 1.0407663583755493
Epoch 1420, training loss: 62.34846115112305 = 0.06312059611082077 + 10.0 * 6.228533744812012
Epoch 1420, val loss: 1.046124815940857
Epoch 1430, training loss: 62.3580207824707 = 0.06165598705410957 + 10.0 * 6.229636192321777
Epoch 1430, val loss: 1.0514460802078247
Epoch 1440, training loss: 62.356788635253906 = 0.06021815538406372 + 10.0 * 6.229657173156738
Epoch 1440, val loss: 1.0565900802612305
Epoch 1450, training loss: 62.398773193359375 = 0.05883249640464783 + 10.0 * 6.233994007110596
Epoch 1450, val loss: 1.0613312721252441
Epoch 1460, training loss: 62.33903503417969 = 0.057428084313869476 + 10.0 * 6.228160858154297
Epoch 1460, val loss: 1.0668963193893433
Epoch 1470, training loss: 62.336647033691406 = 0.05610182136297226 + 10.0 * 6.228054523468018
Epoch 1470, val loss: 1.0716300010681152
Epoch 1480, training loss: 62.32604217529297 = 0.05482489988207817 + 10.0 * 6.227121829986572
Epoch 1480, val loss: 1.0770881175994873
Epoch 1490, training loss: 62.32322311401367 = 0.053584109991788864 + 10.0 * 6.226963996887207
Epoch 1490, val loss: 1.0823187828063965
Epoch 1500, training loss: 62.375606536865234 = 0.05238264054059982 + 10.0 * 6.2323222160339355
Epoch 1500, val loss: 1.087499976158142
Epoch 1510, training loss: 62.321170806884766 = 0.05117909237742424 + 10.0 * 6.226999282836914
Epoch 1510, val loss: 1.0925521850585938
Epoch 1520, training loss: 62.30343246459961 = 0.05003131553530693 + 10.0 * 6.225340366363525
Epoch 1520, val loss: 1.097845435142517
Epoch 1530, training loss: 62.30007553100586 = 0.04891772195696831 + 10.0 * 6.225115776062012
Epoch 1530, val loss: 1.103049635887146
Epoch 1540, training loss: 62.40459442138672 = 0.0478503480553627 + 10.0 * 6.2356743812561035
Epoch 1540, val loss: 1.108101487159729
Epoch 1550, training loss: 62.33140182495117 = 0.04676533862948418 + 10.0 * 6.228463649749756
Epoch 1550, val loss: 1.1130256652832031
Epoch 1560, training loss: 62.28670120239258 = 0.04572248458862305 + 10.0 * 6.224097728729248
Epoch 1560, val loss: 1.1180627346038818
Epoch 1570, training loss: 62.28544235229492 = 0.044732704758644104 + 10.0 * 6.224071025848389
Epoch 1570, val loss: 1.1231495141983032
Epoch 1580, training loss: 62.279850006103516 = 0.04377088323235512 + 10.0 * 6.223608016967773
Epoch 1580, val loss: 1.1283513307571411
Epoch 1590, training loss: 62.334468841552734 = 0.04283372685313225 + 10.0 * 6.22916316986084
Epoch 1590, val loss: 1.1335939168930054
Epoch 1600, training loss: 62.29362869262695 = 0.04190906509757042 + 10.0 * 6.22517204284668
Epoch 1600, val loss: 1.1381903886795044
Epoch 1610, training loss: 62.2812614440918 = 0.041008178144693375 + 10.0 * 6.224025249481201
Epoch 1610, val loss: 1.1430895328521729
Epoch 1620, training loss: 62.323123931884766 = 0.040144987404346466 + 10.0 * 6.228297710418701
Epoch 1620, val loss: 1.1481174230575562
Epoch 1630, training loss: 62.27366256713867 = 0.03929908946156502 + 10.0 * 6.22343635559082
Epoch 1630, val loss: 1.1527806520462036
Epoch 1640, training loss: 62.25739288330078 = 0.03847808018326759 + 10.0 * 6.221891403198242
Epoch 1640, val loss: 1.157791018486023
Epoch 1650, training loss: 62.259056091308594 = 0.03768547996878624 + 10.0 * 6.222136974334717
Epoch 1650, val loss: 1.1629157066345215
Epoch 1660, training loss: 62.32423400878906 = 0.03691104054450989 + 10.0 * 6.228732109069824
Epoch 1660, val loss: 1.1677600145339966
Epoch 1670, training loss: 62.29669189453125 = 0.03614528104662895 + 10.0 * 6.226054668426514
Epoch 1670, val loss: 1.1719398498535156
Epoch 1680, training loss: 62.25387954711914 = 0.03540617600083351 + 10.0 * 6.221847057342529
Epoch 1680, val loss: 1.1767131090164185
Epoch 1690, training loss: 62.24067687988281 = 0.0346929095685482 + 10.0 * 6.220598220825195
Epoch 1690, val loss: 1.1816409826278687
Epoch 1700, training loss: 62.23443603515625 = 0.03400583937764168 + 10.0 * 6.220043182373047
Epoch 1700, val loss: 1.1864672899246216
Epoch 1710, training loss: 62.23508834838867 = 0.03333655744791031 + 10.0 * 6.220175266265869
Epoch 1710, val loss: 1.1913905143737793
Epoch 1720, training loss: 62.308170318603516 = 0.032680969685316086 + 10.0 * 6.227549076080322
Epoch 1720, val loss: 1.1960749626159668
Epoch 1730, training loss: 62.24610900878906 = 0.03203867748379707 + 10.0 * 6.221406936645508
Epoch 1730, val loss: 1.2004671096801758
Epoch 1740, training loss: 62.23434066772461 = 0.031404923647642136 + 10.0 * 6.2202935218811035
Epoch 1740, val loss: 1.2050635814666748
Epoch 1750, training loss: 62.24365997314453 = 0.030799759551882744 + 10.0 * 6.221285820007324
Epoch 1750, val loss: 1.2097983360290527
Epoch 1760, training loss: 62.215354919433594 = 0.030206525698304176 + 10.0 * 6.218514919281006
Epoch 1760, val loss: 1.2142618894577026
Epoch 1770, training loss: 62.21163558959961 = 0.02963423915207386 + 10.0 * 6.218200206756592
Epoch 1770, val loss: 1.2190353870391846
Epoch 1780, training loss: 62.236610412597656 = 0.029081426560878754 + 10.0 * 6.220752716064453
Epoch 1780, val loss: 1.223795771598816
Epoch 1790, training loss: 62.22085189819336 = 0.02852555178105831 + 10.0 * 6.219232559204102
Epoch 1790, val loss: 1.2277778387069702
Epoch 1800, training loss: 62.22902297973633 = 0.0279806237667799 + 10.0 * 6.220104217529297
Epoch 1800, val loss: 1.2322875261306763
Epoch 1810, training loss: 62.205238342285156 = 0.027459006756544113 + 10.0 * 6.217778205871582
Epoch 1810, val loss: 1.23676335811615
Epoch 1820, training loss: 62.19660186767578 = 0.02695617265999317 + 10.0 * 6.2169647216796875
Epoch 1820, val loss: 1.2415461540222168
Epoch 1830, training loss: 62.2120475769043 = 0.026469411328434944 + 10.0 * 6.218557834625244
Epoch 1830, val loss: 1.2461355924606323
Epoch 1840, training loss: 62.226768493652344 = 0.025987543165683746 + 10.0 * 6.220078468322754
Epoch 1840, val loss: 1.2503777742385864
Epoch 1850, training loss: 62.20836639404297 = 0.025520341470837593 + 10.0 * 6.218284606933594
Epoch 1850, val loss: 1.254388451576233
Epoch 1860, training loss: 62.18674087524414 = 0.025061415508389473 + 10.0 * 6.21616792678833
Epoch 1860, val loss: 1.258973240852356
Epoch 1870, training loss: 62.21436309814453 = 0.024621140211820602 + 10.0 * 6.2189741134643555
Epoch 1870, val loss: 1.2633098363876343
Epoch 1880, training loss: 62.19355010986328 = 0.024188315495848656 + 10.0 * 6.216936111450195
Epoch 1880, val loss: 1.2673219442367554
Epoch 1890, training loss: 62.197574615478516 = 0.0237631443887949 + 10.0 * 6.217381477355957
Epoch 1890, val loss: 1.271651029586792
Epoch 1900, training loss: 62.189422607421875 = 0.023346954956650734 + 10.0 * 6.216607570648193
Epoch 1900, val loss: 1.276023268699646
Epoch 1910, training loss: 62.187496185302734 = 0.022944586351513863 + 10.0 * 6.216454982757568
Epoch 1910, val loss: 1.280063509941101
Epoch 1920, training loss: 62.17506408691406 = 0.0225524939596653 + 10.0 * 6.2152509689331055
Epoch 1920, val loss: 1.284239411354065
Epoch 1930, training loss: 62.20045852661133 = 0.02217356488108635 + 10.0 * 6.217828273773193
Epoch 1930, val loss: 1.2884762287139893
Epoch 1940, training loss: 62.17356872558594 = 0.02179364487528801 + 10.0 * 6.215177536010742
Epoch 1940, val loss: 1.2925959825515747
Epoch 1950, training loss: 62.18767547607422 = 0.021429572254419327 + 10.0 * 6.2166242599487305
Epoch 1950, val loss: 1.2964640855789185
Epoch 1960, training loss: 62.18691635131836 = 0.02106849104166031 + 10.0 * 6.2165846824646
Epoch 1960, val loss: 1.300794005393982
Epoch 1970, training loss: 62.16310119628906 = 0.020723680034279823 + 10.0 * 6.214237689971924
Epoch 1970, val loss: 1.30441415309906
Epoch 1980, training loss: 62.151615142822266 = 0.02038489654660225 + 10.0 * 6.213122844696045
Epoch 1980, val loss: 1.3085756301879883
Epoch 1990, training loss: 62.15115737915039 = 0.020057402551174164 + 10.0 * 6.213109970092773
Epoch 1990, val loss: 1.3127641677856445
Epoch 2000, training loss: 62.174495697021484 = 0.019739490002393723 + 10.0 * 6.215475559234619
Epoch 2000, val loss: 1.3167401552200317
Epoch 2010, training loss: 62.2203483581543 = 0.019424201920628548 + 10.0 * 6.220092296600342
Epoch 2010, val loss: 1.3204901218414307
Epoch 2020, training loss: 62.16389465332031 = 0.01910051703453064 + 10.0 * 6.214479446411133
Epoch 2020, val loss: 1.3243894577026367
Epoch 2030, training loss: 62.1495361328125 = 0.018796565011143684 + 10.0 * 6.213074207305908
Epoch 2030, val loss: 1.3282766342163086
Epoch 2040, training loss: 62.13644790649414 = 0.018504519015550613 + 10.0 * 6.211794376373291
Epoch 2040, val loss: 1.3325209617614746
Epoch 2050, training loss: 62.14155960083008 = 0.018221624195575714 + 10.0 * 6.212333679199219
Epoch 2050, val loss: 1.336610198020935
Epoch 2060, training loss: 62.193904876708984 = 0.01794292777776718 + 10.0 * 6.217596054077148
Epoch 2060, val loss: 1.340468168258667
Epoch 2070, training loss: 62.184181213378906 = 0.017661454156041145 + 10.0 * 6.216651916503906
Epoch 2070, val loss: 1.343612551689148
Epoch 2080, training loss: 62.13079071044922 = 0.01738904044032097 + 10.0 * 6.211340427398682
Epoch 2080, val loss: 1.3476592302322388
Epoch 2090, training loss: 62.12925720214844 = 0.01712987758219242 + 10.0 * 6.211213111877441
Epoch 2090, val loss: 1.3514971733093262
Epoch 2100, training loss: 62.12663650512695 = 0.01687575876712799 + 10.0 * 6.2109761238098145
Epoch 2100, val loss: 1.3552523851394653
Epoch 2110, training loss: 62.16043472290039 = 0.01663004234433174 + 10.0 * 6.214380741119385
Epoch 2110, val loss: 1.3589468002319336
Epoch 2120, training loss: 62.132083892822266 = 0.016375727951526642 + 10.0 * 6.211570739746094
Epoch 2120, val loss: 1.3626813888549805
Epoch 2130, training loss: 62.15400695800781 = 0.016132107004523277 + 10.0 * 6.21378755569458
Epoch 2130, val loss: 1.3663705587387085
Epoch 2140, training loss: 62.13045120239258 = 0.01589719019830227 + 10.0 * 6.211455345153809
Epoch 2140, val loss: 1.369546890258789
Epoch 2150, training loss: 62.12004852294922 = 0.01566723920404911 + 10.0 * 6.210438251495361
Epoch 2150, val loss: 1.3732688426971436
Epoch 2160, training loss: 62.113189697265625 = 0.015446324832737446 + 10.0 * 6.209774494171143
Epoch 2160, val loss: 1.3769866228103638
Epoch 2170, training loss: 62.1789436340332 = 0.015231369063258171 + 10.0 * 6.216371059417725
Epoch 2170, val loss: 1.380394458770752
Epoch 2180, training loss: 62.13494110107422 = 0.015008095651865005 + 10.0 * 6.211993217468262
Epoch 2180, val loss: 1.3840159177780151
Epoch 2190, training loss: 62.11946105957031 = 0.014795585535466671 + 10.0 * 6.210466384887695
Epoch 2190, val loss: 1.3875339031219482
Epoch 2200, training loss: 62.1115837097168 = 0.014590072445571423 + 10.0 * 6.2096991539001465
Epoch 2200, val loss: 1.3910868167877197
Epoch 2210, training loss: 62.1443977355957 = 0.014391535893082619 + 10.0 * 6.213000297546387
Epoch 2210, val loss: 1.3944544792175293
Epoch 2220, training loss: 62.14834976196289 = 0.014192909933626652 + 10.0 * 6.213415622711182
Epoch 2220, val loss: 1.397718071937561
Epoch 2230, training loss: 62.101646423339844 = 0.013990753330290318 + 10.0 * 6.208765506744385
Epoch 2230, val loss: 1.4011881351470947
Epoch 2240, training loss: 62.100406646728516 = 0.013799953274428844 + 10.0 * 6.20866060256958
Epoch 2240, val loss: 1.4047623872756958
Epoch 2250, training loss: 62.09657669067383 = 0.013615036383271217 + 10.0 * 6.208296298980713
Epoch 2250, val loss: 1.408286452293396
Epoch 2260, training loss: 62.145816802978516 = 0.013434239663183689 + 10.0 * 6.21323823928833
Epoch 2260, val loss: 1.4118084907531738
Epoch 2270, training loss: 62.099708557128906 = 0.013254189863801003 + 10.0 * 6.208645343780518
Epoch 2270, val loss: 1.4144188165664673
Epoch 2280, training loss: 62.0943717956543 = 0.013078972697257996 + 10.0 * 6.208128929138184
Epoch 2280, val loss: 1.418140172958374
Epoch 2290, training loss: 62.162940979003906 = 0.012910573743283749 + 10.0 * 6.21500301361084
Epoch 2290, val loss: 1.4213080406188965
Epoch 2300, training loss: 62.09132385253906 = 0.012733017094433308 + 10.0 * 6.207859039306641
Epoch 2300, val loss: 1.4245620965957642
Epoch 2310, training loss: 62.086483001708984 = 0.012566279619932175 + 10.0 * 6.207391738891602
Epoch 2310, val loss: 1.427829623222351
Epoch 2320, training loss: 62.12313461303711 = 0.012406021356582642 + 10.0 * 6.21107292175293
Epoch 2320, val loss: 1.4312673807144165
Epoch 2330, training loss: 62.08143615722656 = 0.01224462129175663 + 10.0 * 6.206919193267822
Epoch 2330, val loss: 1.4341037273406982
Epoch 2340, training loss: 62.0836296081543 = 0.012091058306396008 + 10.0 * 6.207153797149658
Epoch 2340, val loss: 1.4373087882995605
Epoch 2350, training loss: 62.07597732543945 = 0.011941247619688511 + 10.0 * 6.206403732299805
Epoch 2350, val loss: 1.4405741691589355
Epoch 2360, training loss: 62.115760803222656 = 0.011797147803008556 + 10.0 * 6.2103962898254395
Epoch 2360, val loss: 1.4437189102172852
Epoch 2370, training loss: 62.110408782958984 = 0.011642283760011196 + 10.0 * 6.209876537322998
Epoch 2370, val loss: 1.4471046924591064
Epoch 2380, training loss: 62.09746551513672 = 0.011497999541461468 + 10.0 * 6.208596706390381
Epoch 2380, val loss: 1.4495052099227905
Epoch 2390, training loss: 62.075294494628906 = 0.011353874579071999 + 10.0 * 6.206394195556641
Epoch 2390, val loss: 1.452808141708374
Epoch 2400, training loss: 62.07151412963867 = 0.011219429783523083 + 10.0 * 6.206029415130615
Epoch 2400, val loss: 1.4558520317077637
Epoch 2410, training loss: 62.116146087646484 = 0.011087201535701752 + 10.0 * 6.210505962371826
Epoch 2410, val loss: 1.4587254524230957
Epoch 2420, training loss: 62.1207275390625 = 0.01095044519752264 + 10.0 * 6.210977554321289
Epoch 2420, val loss: 1.4617923498153687
Epoch 2430, training loss: 62.07928466796875 = 0.01081707514822483 + 10.0 * 6.206846714019775
Epoch 2430, val loss: 1.4644447565078735
Epoch 2440, training loss: 62.062782287597656 = 0.010687598958611488 + 10.0 * 6.205209255218506
Epoch 2440, val loss: 1.4676717519760132
Epoch 2450, training loss: 62.06070327758789 = 0.010565809905529022 + 10.0 * 6.205013751983643
Epoch 2450, val loss: 1.4705740213394165
Epoch 2460, training loss: 62.0989875793457 = 0.010447073727846146 + 10.0 * 6.208853721618652
Epoch 2460, val loss: 1.4735088348388672
Epoch 2470, training loss: 62.0738410949707 = 0.010320974513888359 + 10.0 * 6.2063517570495605
Epoch 2470, val loss: 1.4764987230300903
Epoch 2480, training loss: 62.056880950927734 = 0.01019842829555273 + 10.0 * 6.2046685218811035
Epoch 2480, val loss: 1.479151964187622
Epoch 2490, training loss: 62.04976272583008 = 0.010082503780722618 + 10.0 * 6.203968048095703
Epoch 2490, val loss: 1.481952428817749
Epoch 2500, training loss: 62.05231475830078 = 0.009969959035515785 + 10.0 * 6.2042341232299805
Epoch 2500, val loss: 1.484940528869629
Epoch 2510, training loss: 62.085243225097656 = 0.009860984049737453 + 10.0 * 6.20753812789917
Epoch 2510, val loss: 1.4876580238342285
Epoch 2520, training loss: 62.05605697631836 = 0.009746671654284 + 10.0 * 6.2046308517456055
Epoch 2520, val loss: 1.4906811714172363
Epoch 2530, training loss: 62.11205291748047 = 0.009639228694140911 + 10.0 * 6.210241317749023
Epoch 2530, val loss: 1.4935424327850342
Epoch 2540, training loss: 62.05683898925781 = 0.009527720510959625 + 10.0 * 6.204730987548828
Epoch 2540, val loss: 1.495708703994751
Epoch 2550, training loss: 62.05055236816406 = 0.009423765353858471 + 10.0 * 6.204113006591797
Epoch 2550, val loss: 1.4985101222991943
Epoch 2560, training loss: 62.05232620239258 = 0.00932181254029274 + 10.0 * 6.204300403594971
Epoch 2560, val loss: 1.5014617443084717
Epoch 2570, training loss: 62.06565475463867 = 0.009221195243299007 + 10.0 * 6.205643653869629
Epoch 2570, val loss: 1.5041825771331787
Epoch 2580, training loss: 62.050167083740234 = 0.009122264571487904 + 10.0 * 6.204104423522949
Epoch 2580, val loss: 1.506775975227356
Epoch 2590, training loss: 62.04803466796875 = 0.009024594910442829 + 10.0 * 6.2039008140563965
Epoch 2590, val loss: 1.5096158981323242
Epoch 2600, training loss: 62.096466064453125 = 0.008929557166993618 + 10.0 * 6.20875358581543
Epoch 2600, val loss: 1.5123933553695679
Epoch 2610, training loss: 62.056392669677734 = 0.008832693099975586 + 10.0 * 6.204756259918213
Epoch 2610, val loss: 1.5149675607681274
Epoch 2620, training loss: 62.04179000854492 = 0.008740304969251156 + 10.0 * 6.203305244445801
Epoch 2620, val loss: 1.5171105861663818
Epoch 2630, training loss: 62.0621223449707 = 0.00864886399358511 + 10.0 * 6.205347537994385
Epoch 2630, val loss: 1.5200927257537842
Epoch 2640, training loss: 62.03493118286133 = 0.008558557368814945 + 10.0 * 6.202637195587158
Epoch 2640, val loss: 1.5225211381912231
Epoch 2650, training loss: 62.03280258178711 = 0.008470539003610611 + 10.0 * 6.202433109283447
Epoch 2650, val loss: 1.5250890254974365
Epoch 2660, training loss: 62.03135299682617 = 0.008384963497519493 + 10.0 * 6.202296733856201
Epoch 2660, val loss: 1.5279737710952759
Epoch 2670, training loss: 62.06572723388672 = 0.00830142479389906 + 10.0 * 6.205742835998535
Epoch 2670, val loss: 1.5304617881774902
Epoch 2680, training loss: 62.057960510253906 = 0.00821553636342287 + 10.0 * 6.204974174499512
Epoch 2680, val loss: 1.5325621366500854
Epoch 2690, training loss: 62.04275894165039 = 0.008131914772093296 + 10.0 * 6.203462600708008
Epoch 2690, val loss: 1.534875750541687
Epoch 2700, training loss: 62.02882766723633 = 0.008049294352531433 + 10.0 * 6.202077865600586
Epoch 2700, val loss: 1.5373799800872803
Epoch 2710, training loss: 62.01853942871094 = 0.00797055009752512 + 10.0 * 6.201056957244873
Epoch 2710, val loss: 1.5399487018585205
Epoch 2720, training loss: 62.027774810791016 = 0.007894436828792095 + 10.0 * 6.201988220214844
Epoch 2720, val loss: 1.5424888134002686
Epoch 2730, training loss: 62.07915496826172 = 0.007816697470843792 + 10.0 * 6.207133769989014
Epoch 2730, val loss: 1.544962763786316
Epoch 2740, training loss: 62.0579719543457 = 0.007739729713648558 + 10.0 * 6.205023288726807
Epoch 2740, val loss: 1.547235131263733
Epoch 2750, training loss: 62.060462951660156 = 0.007663808763027191 + 10.0 * 6.20527982711792
Epoch 2750, val loss: 1.5493144989013672
Epoch 2760, training loss: 62.01539611816406 = 0.007587700616568327 + 10.0 * 6.200780868530273
Epoch 2760, val loss: 1.5517405271530151
Epoch 2770, training loss: 62.01295852661133 = 0.007516131270676851 + 10.0 * 6.200544357299805
Epoch 2770, val loss: 1.5542550086975098
Epoch 2780, training loss: 62.00996398925781 = 0.007447036448866129 + 10.0 * 6.200251579284668
Epoch 2780, val loss: 1.556790828704834
Epoch 2790, training loss: 62.06547927856445 = 0.007379466202110052 + 10.0 * 6.205810070037842
Epoch 2790, val loss: 1.5592830181121826
Epoch 2800, training loss: 62.011512756347656 = 0.007307912223041058 + 10.0 * 6.200420379638672
Epoch 2800, val loss: 1.5612034797668457
Epoch 2810, training loss: 62.01915740966797 = 0.007239934988319874 + 10.0 * 6.2011919021606445
Epoch 2810, val loss: 1.5634785890579224
Epoch 2820, training loss: 62.01509475708008 = 0.0071721794083714485 + 10.0 * 6.20079231262207
Epoch 2820, val loss: 1.5656193494796753
Epoch 2830, training loss: 62.039154052734375 = 0.007109036669135094 + 10.0 * 6.203204154968262
Epoch 2830, val loss: 1.5677226781845093
Epoch 2840, training loss: 62.004364013671875 = 0.007040800992399454 + 10.0 * 6.199732303619385
Epoch 2840, val loss: 1.5701289176940918
Epoch 2850, training loss: 62.0079460144043 = 0.006978408899158239 + 10.0 * 6.20009708404541
Epoch 2850, val loss: 1.572708249092102
Epoch 2860, training loss: 62.05891418457031 = 0.0069181048311293125 + 10.0 * 6.205199241638184
Epoch 2860, val loss: 1.5747774839401245
Epoch 2870, training loss: 62.018558502197266 = 0.006854798179119825 + 10.0 * 6.201170444488525
Epoch 2870, val loss: 1.5765300989151
Epoch 2880, training loss: 62.014156341552734 = 0.006792338099330664 + 10.0 * 6.200736045837402
Epoch 2880, val loss: 1.5788702964782715
Epoch 2890, training loss: 61.99591827392578 = 0.006730938330292702 + 10.0 * 6.19891881942749
Epoch 2890, val loss: 1.5810496807098389
Epoch 2900, training loss: 61.99251937866211 = 0.006673220545053482 + 10.0 * 6.19858455657959
Epoch 2900, val loss: 1.583396077156067
Epoch 2910, training loss: 62.00343704223633 = 0.00661676749587059 + 10.0 * 6.199681758880615
Epoch 2910, val loss: 1.5857857465744019
Epoch 2920, training loss: 62.02912521362305 = 0.006559213623404503 + 10.0 * 6.202256679534912
Epoch 2920, val loss: 1.5879406929016113
Epoch 2930, training loss: 62.01988220214844 = 0.006503731943666935 + 10.0 * 6.201337814331055
Epoch 2930, val loss: 1.589751124382019
Epoch 2940, training loss: 62.042816162109375 = 0.006445789709687233 + 10.0 * 6.20363712310791
Epoch 2940, val loss: 1.5920958518981934
Epoch 2950, training loss: 62.03275680541992 = 0.006389983929693699 + 10.0 * 6.20263671875
Epoch 2950, val loss: 1.5938600301742554
Epoch 2960, training loss: 61.99235916137695 = 0.0063341883942484856 + 10.0 * 6.198602676391602
Epoch 2960, val loss: 1.5957671403884888
Epoch 2970, training loss: 61.98525619506836 = 0.006281951442360878 + 10.0 * 6.197897434234619
Epoch 2970, val loss: 1.5981415510177612
Epoch 2980, training loss: 61.984718322753906 = 0.006231208331882954 + 10.0 * 6.197848796844482
Epoch 2980, val loss: 1.6003425121307373
Epoch 2990, training loss: 62.011260986328125 = 0.006181672681123018 + 10.0 * 6.200508117675781
Epoch 2990, val loss: 1.6025874614715576
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 87.91390991210938 = 1.9455844163894653 + 10.0 * 8.596832275390625
Epoch 0, val loss: 1.943365216255188
Epoch 10, training loss: 87.8960189819336 = 1.935526967048645 + 10.0 * 8.596049308776855
Epoch 10, val loss: 1.9330250024795532
Epoch 20, training loss: 87.81681060791016 = 1.923259973526001 + 10.0 * 8.589354515075684
Epoch 20, val loss: 1.9198834896087646
Epoch 30, training loss: 87.31272888183594 = 1.9076915979385376 + 10.0 * 8.54050350189209
Epoch 30, val loss: 1.902943730354309
Epoch 40, training loss: 84.3275146484375 = 1.8893989324569702 + 10.0 * 8.24381160736084
Epoch 40, val loss: 1.883459210395813
Epoch 50, training loss: 78.3310775756836 = 1.8697359561920166 + 10.0 * 7.646133899688721
Epoch 50, val loss: 1.8638075590133667
Epoch 60, training loss: 74.48323822021484 = 1.8554742336273193 + 10.0 * 7.262775897979736
Epoch 60, val loss: 1.8509100675582886
Epoch 70, training loss: 71.79378509521484 = 1.8422178030014038 + 10.0 * 6.995157241821289
Epoch 70, val loss: 1.8383046388626099
Epoch 80, training loss: 70.38377380371094 = 1.8307424783706665 + 10.0 * 6.8553032875061035
Epoch 80, val loss: 1.8276127576828003
Epoch 90, training loss: 69.50183868408203 = 1.8192853927612305 + 10.0 * 6.768255233764648
Epoch 90, val loss: 1.8171885013580322
Epoch 100, training loss: 68.74919128417969 = 1.8090851306915283 + 10.0 * 6.694011211395264
Epoch 100, val loss: 1.80824613571167
Epoch 110, training loss: 68.33635711669922 = 1.800074577331543 + 10.0 * 6.653628826141357
Epoch 110, val loss: 1.8002281188964844
Epoch 120, training loss: 67.96068572998047 = 1.7913342714309692 + 10.0 * 6.616934776306152
Epoch 120, val loss: 1.7922416925430298
Epoch 130, training loss: 67.60591125488281 = 1.7826725244522095 + 10.0 * 6.582324028015137
Epoch 130, val loss: 1.784461498260498
Epoch 140, training loss: 67.28323364257812 = 1.7741845846176147 + 10.0 * 6.550905227661133
Epoch 140, val loss: 1.7769742012023926
Epoch 150, training loss: 67.00582885742188 = 1.7654539346694946 + 10.0 * 6.524036884307861
Epoch 150, val loss: 1.7695012092590332
Epoch 160, training loss: 66.74093627929688 = 1.7561413049697876 + 10.0 * 6.49847936630249
Epoch 160, val loss: 1.76158607006073
Epoch 170, training loss: 66.5526123046875 = 1.7458319664001465 + 10.0 * 6.480677604675293
Epoch 170, val loss: 1.7527943849563599
Epoch 180, training loss: 66.36344909667969 = 1.7345291376113892 + 10.0 * 6.462892055511475
Epoch 180, val loss: 1.7430214881896973
Epoch 190, training loss: 66.2093276977539 = 1.722130298614502 + 10.0 * 6.448719501495361
Epoch 190, val loss: 1.7323849201202393
Epoch 200, training loss: 66.08885192871094 = 1.7085742950439453 + 10.0 * 6.438027858734131
Epoch 200, val loss: 1.7207850217819214
Epoch 210, training loss: 65.9609603881836 = 1.693794846534729 + 10.0 * 6.4267168045043945
Epoch 210, val loss: 1.7081578969955444
Epoch 220, training loss: 65.8335189819336 = 1.6777657270431519 + 10.0 * 6.4155755043029785
Epoch 220, val loss: 1.6945774555206299
Epoch 230, training loss: 65.77214050292969 = 1.6603866815567017 + 10.0 * 6.411175727844238
Epoch 230, val loss: 1.679895043373108
Epoch 240, training loss: 65.62088775634766 = 1.6416285037994385 + 10.0 * 6.39792537689209
Epoch 240, val loss: 1.664074420928955
Epoch 250, training loss: 65.50906372070312 = 1.6214548349380493 + 10.0 * 6.388761043548584
Epoch 250, val loss: 1.647254228591919
Epoch 260, training loss: 65.40787506103516 = 1.5997720956802368 + 10.0 * 6.380810737609863
Epoch 260, val loss: 1.6292521953582764
Epoch 270, training loss: 65.35786437988281 = 1.5764741897583008 + 10.0 * 6.378138542175293
Epoch 270, val loss: 1.6100342273712158
Epoch 280, training loss: 65.2358169555664 = 1.5515509843826294 + 10.0 * 6.368426322937012
Epoch 280, val loss: 1.589486002922058
Epoch 290, training loss: 65.1482162475586 = 1.5250948667526245 + 10.0 * 6.362311840057373
Epoch 290, val loss: 1.5679001808166504
Epoch 300, training loss: 65.06155395507812 = 1.4971717596054077 + 10.0 * 6.356438636779785
Epoch 300, val loss: 1.545268177986145
Epoch 310, training loss: 64.99690246582031 = 1.4677820205688477 + 10.0 * 6.352911949157715
Epoch 310, val loss: 1.5216052532196045
Epoch 320, training loss: 64.92586517333984 = 1.4368987083435059 + 10.0 * 6.348896503448486
Epoch 320, val loss: 1.496726393699646
Epoch 330, training loss: 64.84380340576172 = 1.4048267602920532 + 10.0 * 6.343897819519043
Epoch 330, val loss: 1.4711310863494873
Epoch 340, training loss: 64.75732421875 = 1.3718128204345703 + 10.0 * 6.338551044464111
Epoch 340, val loss: 1.4450042247772217
Epoch 350, training loss: 64.68577575683594 = 1.3379641771316528 + 10.0 * 6.334780693054199
Epoch 350, val loss: 1.4183831214904785
Epoch 360, training loss: 64.62770080566406 = 1.3034076690673828 + 10.0 * 6.3324294090271
Epoch 360, val loss: 1.391445517539978
Epoch 370, training loss: 64.59392547607422 = 1.2683058977127075 + 10.0 * 6.33256196975708
Epoch 370, val loss: 1.3642576932907104
Epoch 380, training loss: 64.49879455566406 = 1.2330763339996338 + 10.0 * 6.326571941375732
Epoch 380, val loss: 1.3372232913970947
Epoch 390, training loss: 64.4220962524414 = 1.1979970932006836 + 10.0 * 6.322409629821777
Epoch 390, val loss: 1.3106656074523926
Epoch 400, training loss: 64.3849105834961 = 1.163236141204834 + 10.0 * 6.32216739654541
Epoch 400, val loss: 1.2846096754074097
Epoch 410, training loss: 64.31574249267578 = 1.1290768384933472 + 10.0 * 6.318666458129883
Epoch 410, val loss: 1.2594294548034668
Epoch 420, training loss: 64.2403793334961 = 1.0957399606704712 + 10.0 * 6.3144636154174805
Epoch 420, val loss: 1.2350739240646362
Epoch 430, training loss: 64.18064880371094 = 1.0633128881454468 + 10.0 * 6.311733722686768
Epoch 430, val loss: 1.2117046117782593
Epoch 440, training loss: 64.1467056274414 = 1.0319621562957764 + 10.0 * 6.311474323272705
Epoch 440, val loss: 1.1893943548202515
Epoch 450, training loss: 64.07791137695312 = 1.0018142461776733 + 10.0 * 6.307610034942627
Epoch 450, val loss: 1.1683599948883057
Epoch 460, training loss: 64.02633666992188 = 0.9728209376335144 + 10.0 * 6.305351734161377
Epoch 460, val loss: 1.1483690738677979
Epoch 470, training loss: 63.97514343261719 = 0.94508957862854 + 10.0 * 6.303005218505859
Epoch 470, val loss: 1.1295374631881714
Epoch 480, training loss: 63.933345794677734 = 0.9183637499809265 + 10.0 * 6.301497936248779
Epoch 480, val loss: 1.111812710762024
Epoch 490, training loss: 63.884422302246094 = 0.8928083777427673 + 10.0 * 6.299161434173584
Epoch 490, val loss: 1.094989538192749
Epoch 500, training loss: 63.846038818359375 = 0.8683089017868042 + 10.0 * 6.2977728843688965
Epoch 500, val loss: 1.0793424844741821
Epoch 510, training loss: 63.79093933105469 = 0.8447039723396301 + 10.0 * 6.294623374938965
Epoch 510, val loss: 1.0644123554229736
Epoch 520, training loss: 63.75432205200195 = 0.8220406770706177 + 10.0 * 6.2932281494140625
Epoch 520, val loss: 1.0505542755126953
Epoch 530, training loss: 63.70641326904297 = 0.8002098202705383 + 10.0 * 6.29062032699585
Epoch 530, val loss: 1.0375373363494873
Epoch 540, training loss: 63.6683235168457 = 0.7791455984115601 + 10.0 * 6.2889180183410645
Epoch 540, val loss: 1.0251497030258179
Epoch 550, training loss: 63.64737319946289 = 0.7586718797683716 + 10.0 * 6.288870334625244
Epoch 550, val loss: 1.0133357048034668
Epoch 560, training loss: 63.612579345703125 = 0.7388765811920166 + 10.0 * 6.287370204925537
Epoch 560, val loss: 1.0021798610687256
Epoch 570, training loss: 63.558929443359375 = 0.7197737097740173 + 10.0 * 6.2839155197143555
Epoch 570, val loss: 0.9919171333312988
Epoch 580, training loss: 63.543766021728516 = 0.7012248039245605 + 10.0 * 6.28425407409668
Epoch 580, val loss: 0.981957197189331
Epoch 590, training loss: 63.523521423339844 = 0.6831141710281372 + 10.0 * 6.284040927886963
Epoch 590, val loss: 0.9722769260406494
Epoch 600, training loss: 63.468360900878906 = 0.665582001209259 + 10.0 * 6.280278205871582
Epoch 600, val loss: 0.9632856845855713
Epoch 610, training loss: 63.433841705322266 = 0.6486088633537292 + 10.0 * 6.2785234451293945
Epoch 610, val loss: 0.9549670219421387
Epoch 620, training loss: 63.396339416503906 = 0.6320419311523438 + 10.0 * 6.276429653167725
Epoch 620, val loss: 0.9467916488647461
Epoch 630, training loss: 63.367305755615234 = 0.6159125566482544 + 10.0 * 6.275139331817627
Epoch 630, val loss: 0.939245343208313
Epoch 640, training loss: 63.4629020690918 = 0.6001325249671936 + 10.0 * 6.286276817321777
Epoch 640, val loss: 0.9318767786026001
Epoch 650, training loss: 63.3287353515625 = 0.5846499800682068 + 10.0 * 6.27440881729126
Epoch 650, val loss: 0.9248734712600708
Epoch 660, training loss: 63.29201889038086 = 0.5696619749069214 + 10.0 * 6.272235870361328
Epoch 660, val loss: 0.918688178062439
Epoch 670, training loss: 63.26047897338867 = 0.5550627708435059 + 10.0 * 6.270541667938232
Epoch 670, val loss: 0.9127451777458191
Epoch 680, training loss: 63.23110580444336 = 0.5408243536949158 + 10.0 * 6.269028186798096
Epoch 680, val loss: 0.9072034955024719
Epoch 690, training loss: 63.22237777709961 = 0.5268922448158264 + 10.0 * 6.269548416137695
Epoch 690, val loss: 0.9021363854408264
Epoch 700, training loss: 63.29167175292969 = 0.5132198333740234 + 10.0 * 6.2778449058532715
Epoch 700, val loss: 0.897232174873352
Epoch 710, training loss: 63.25022888183594 = 0.4996550679206848 + 10.0 * 6.275057315826416
Epoch 710, val loss: 0.8919330835342407
Epoch 720, training loss: 63.1529426574707 = 0.4866827726364136 + 10.0 * 6.266625881195068
Epoch 720, val loss: 0.887848436832428
Epoch 730, training loss: 63.122718811035156 = 0.47408926486968994 + 10.0 * 6.264863014221191
Epoch 730, val loss: 0.8841338753700256
Epoch 740, training loss: 63.09183120727539 = 0.4617922306060791 + 10.0 * 6.263003826141357
Epoch 740, val loss: 0.8806326985359192
Epoch 750, training loss: 63.068458557128906 = 0.4497291147708893 + 10.0 * 6.2618727684021
Epoch 750, val loss: 0.8775676488876343
Epoch 760, training loss: 63.04651641845703 = 0.4378797113895416 + 10.0 * 6.260863304138184
Epoch 760, val loss: 0.8748087286949158
Epoch 770, training loss: 63.02555465698242 = 0.4262552857398987 + 10.0 * 6.25993013381958
Epoch 770, val loss: 0.8723151683807373
Epoch 780, training loss: 63.10063552856445 = 0.4148528575897217 + 10.0 * 6.26857852935791
Epoch 780, val loss: 0.8699269890785217
Epoch 790, training loss: 62.995033264160156 = 0.40359431505203247 + 10.0 * 6.259143829345703
Epoch 790, val loss: 0.8679129481315613
Epoch 800, training loss: 62.97213363647461 = 0.3926292657852173 + 10.0 * 6.257950782775879
Epoch 800, val loss: 0.8662286400794983
Epoch 810, training loss: 62.94912338256836 = 0.3819478452205658 + 10.0 * 6.256717681884766
Epoch 810, val loss: 0.864929735660553
Epoch 820, training loss: 62.94525146484375 = 0.3714967668056488 + 10.0 * 6.257375240325928
Epoch 820, val loss: 0.8638052940368652
Epoch 830, training loss: 62.92117691040039 = 0.3612011671066284 + 10.0 * 6.255997657775879
Epoch 830, val loss: 0.8627532720565796
Epoch 840, training loss: 62.94414138793945 = 0.35113081336021423 + 10.0 * 6.25930118560791
Epoch 840, val loss: 0.8621276021003723
Epoch 850, training loss: 62.886749267578125 = 0.3413536846637726 + 10.0 * 6.254539489746094
Epoch 850, val loss: 0.8620759844779968
Epoch 860, training loss: 62.856449127197266 = 0.33177638053894043 + 10.0 * 6.252467155456543
Epoch 860, val loss: 0.862010657787323
Epoch 870, training loss: 62.84796142578125 = 0.3224320709705353 + 10.0 * 6.2525529861450195
Epoch 870, val loss: 0.8622145056724548
Epoch 880, training loss: 62.874244689941406 = 0.31328749656677246 + 10.0 * 6.256095886230469
Epoch 880, val loss: 0.8625817894935608
Epoch 890, training loss: 62.83675003051758 = 0.3043975830078125 + 10.0 * 6.253235340118408
Epoch 890, val loss: 0.8636670708656311
Epoch 900, training loss: 62.79589080810547 = 0.29565346240997314 + 10.0 * 6.25002384185791
Epoch 900, val loss: 0.8644303679466248
Epoch 910, training loss: 62.77467346191406 = 0.28720349073410034 + 10.0 * 6.248746871948242
Epoch 910, val loss: 0.8657832145690918
Epoch 920, training loss: 62.76328659057617 = 0.2789693772792816 + 10.0 * 6.24843168258667
Epoch 920, val loss: 0.8673205971717834
Epoch 930, training loss: 62.777164459228516 = 0.27093270421028137 + 10.0 * 6.2506232261657715
Epoch 930, val loss: 0.8689330816268921
Epoch 940, training loss: 62.79473114013672 = 0.26303762197494507 + 10.0 * 6.253169059753418
Epoch 940, val loss: 0.8703619837760925
Epoch 950, training loss: 62.72745132446289 = 0.25539544224739075 + 10.0 * 6.24720573425293
Epoch 950, val loss: 0.8724002242088318
Epoch 960, training loss: 62.71302795410156 = 0.24800868332386017 + 10.0 * 6.246501922607422
Epoch 960, val loss: 0.8749054670333862
Epoch 970, training loss: 62.69011688232422 = 0.24085064232349396 + 10.0 * 6.244926452636719
Epoch 970, val loss: 0.877255380153656
Epoch 980, training loss: 62.68229675292969 = 0.23391032218933105 + 10.0 * 6.244838714599609
Epoch 980, val loss: 0.8799471259117126
Epoch 990, training loss: 62.77349090576172 = 0.22718791663646698 + 10.0 * 6.254630088806152
Epoch 990, val loss: 0.8828827738761902
Epoch 1000, training loss: 62.67560958862305 = 0.22051165997982025 + 10.0 * 6.245509624481201
Epoch 1000, val loss: 0.8851836919784546
Epoch 1010, training loss: 62.6406364440918 = 0.21415938436985016 + 10.0 * 6.242647647857666
Epoch 1010, val loss: 0.8886722326278687
Epoch 1020, training loss: 62.62678527832031 = 0.20800085365772247 + 10.0 * 6.241878509521484
Epoch 1020, val loss: 0.8918766379356384
Epoch 1030, training loss: 62.618839263916016 = 0.20203883945941925 + 10.0 * 6.241680145263672
Epoch 1030, val loss: 0.895449697971344
Epoch 1040, training loss: 62.67787170410156 = 0.19621935486793518 + 10.0 * 6.248165130615234
Epoch 1040, val loss: 0.8988714218139648
Epoch 1050, training loss: 62.61091232299805 = 0.19056645035743713 + 10.0 * 6.242034435272217
Epoch 1050, val loss: 0.9027758836746216
Epoch 1060, training loss: 62.58195877075195 = 0.18512579798698425 + 10.0 * 6.239683151245117
Epoch 1060, val loss: 0.9067330360412598
Epoch 1070, training loss: 62.57447814941406 = 0.1798803061246872 + 10.0 * 6.23945951461792
Epoch 1070, val loss: 0.9107568860054016
Epoch 1080, training loss: 62.561710357666016 = 0.17479552328586578 + 10.0 * 6.238691329956055
Epoch 1080, val loss: 0.9148175120353699
Epoch 1090, training loss: 62.67359161376953 = 0.16985732316970825 + 10.0 * 6.250373363494873
Epoch 1090, val loss: 0.9187175631523132
Epoch 1100, training loss: 62.5885124206543 = 0.16504472494125366 + 10.0 * 6.24234676361084
Epoch 1100, val loss: 0.9231822490692139
Epoch 1110, training loss: 62.53480911254883 = 0.16039179265499115 + 10.0 * 6.237441539764404
Epoch 1110, val loss: 0.9277819395065308
Epoch 1120, training loss: 62.52842330932617 = 0.15591205656528473 + 10.0 * 6.237251281738281
Epoch 1120, val loss: 0.9320997595787048
Epoch 1130, training loss: 62.51866149902344 = 0.15159578621387482 + 10.0 * 6.236706733703613
Epoch 1130, val loss: 0.9367866516113281
Epoch 1140, training loss: 62.541316986083984 = 0.1473998874425888 + 10.0 * 6.239391803741455
Epoch 1140, val loss: 0.9414225816726685
Epoch 1150, training loss: 62.50583267211914 = 0.14330936968326569 + 10.0 * 6.236252307891846
Epoch 1150, val loss: 0.9460582733154297
Epoch 1160, training loss: 62.521183013916016 = 0.13938069343566895 + 10.0 * 6.238180160522461
Epoch 1160, val loss: 0.9509938359260559
Epoch 1170, training loss: 62.50415802001953 = 0.13553357124328613 + 10.0 * 6.236862659454346
Epoch 1170, val loss: 0.9556032419204712
Epoch 1180, training loss: 62.4960823059082 = 0.1318267583847046 + 10.0 * 6.236425399780273
Epoch 1180, val loss: 0.9605107307434082
Epoch 1190, training loss: 62.466102600097656 = 0.12822172045707703 + 10.0 * 6.233788013458252
Epoch 1190, val loss: 0.9652219414710999
Epoch 1200, training loss: 62.45854949951172 = 0.1247585117816925 + 10.0 * 6.233378887176514
Epoch 1200, val loss: 0.97010737657547
Epoch 1210, training loss: 62.488037109375 = 0.12139569967985153 + 10.0 * 6.236664295196533
Epoch 1210, val loss: 0.9748469591140747
Epoch 1220, training loss: 62.44765853881836 = 0.11811824887990952 + 10.0 * 6.232954025268555
Epoch 1220, val loss: 0.9798765778541565
Epoch 1230, training loss: 62.43107223510742 = 0.11494538933038712 + 10.0 * 6.231612682342529
Epoch 1230, val loss: 0.9849279522895813
Epoch 1240, training loss: 62.424217224121094 = 0.11188438534736633 + 10.0 * 6.2312331199646
Epoch 1240, val loss: 0.9899835586547852
Epoch 1250, training loss: 62.425174713134766 = 0.10892961174249649 + 10.0 * 6.231624603271484
Epoch 1250, val loss: 0.9950296878814697
Epoch 1260, training loss: 62.4609489440918 = 0.10605260729789734 + 10.0 * 6.235489845275879
Epoch 1260, val loss: 0.9999610185623169
Epoch 1270, training loss: 62.433040618896484 = 0.10324490815401077 + 10.0 * 6.232979774475098
Epoch 1270, val loss: 1.0049622058868408
Epoch 1280, training loss: 62.43413162231445 = 0.10052988678216934 + 10.0 * 6.233360290527344
Epoch 1280, val loss: 1.0100831985473633
Epoch 1290, training loss: 62.399417877197266 = 0.09787597507238388 + 10.0 * 6.230154037475586
Epoch 1290, val loss: 1.0146301984786987
Epoch 1300, training loss: 62.38716125488281 = 0.09533168375492096 + 10.0 * 6.229182720184326
Epoch 1300, val loss: 1.0198261737823486
Epoch 1310, training loss: 62.39158630371094 = 0.09287380427122116 + 10.0 * 6.2298712730407715
Epoch 1310, val loss: 1.0247986316680908
Epoch 1320, training loss: 62.44139862060547 = 0.09048212319612503 + 10.0 * 6.235091686248779
Epoch 1320, val loss: 1.0299540758132935
Epoch 1330, training loss: 62.38848114013672 = 0.08812708407640457 + 10.0 * 6.230035305023193
Epoch 1330, val loss: 1.0347042083740234
Epoch 1340, training loss: 62.363155364990234 = 0.08585046231746674 + 10.0 * 6.227730751037598
Epoch 1340, val loss: 1.039807677268982
Epoch 1350, training loss: 62.35580062866211 = 0.08367525041103363 + 10.0 * 6.227212429046631
Epoch 1350, val loss: 1.0446548461914062
Epoch 1360, training loss: 62.350894927978516 = 0.08156546205282211 + 10.0 * 6.226933002471924
Epoch 1360, val loss: 1.0498204231262207
Epoch 1370, training loss: 62.410362243652344 = 0.0795111134648323 + 10.0 * 6.2330851554870605
Epoch 1370, val loss: 1.054410457611084
Epoch 1380, training loss: 62.371498107910156 = 0.07750790566205978 + 10.0 * 6.22939920425415
Epoch 1380, val loss: 1.0597553253173828
Epoch 1390, training loss: 62.3592529296875 = 0.07555411756038666 + 10.0 * 6.22836971282959
Epoch 1390, val loss: 1.0646566152572632
Epoch 1400, training loss: 62.343135833740234 = 0.07367899268865585 + 10.0 * 6.226945400238037
Epoch 1400, val loss: 1.069482684135437
Epoch 1410, training loss: 62.32526397705078 = 0.07185165584087372 + 10.0 * 6.225341320037842
Epoch 1410, val loss: 1.0745047330856323
Epoch 1420, training loss: 62.32026672363281 = 0.07009288668632507 + 10.0 * 6.225017547607422
Epoch 1420, val loss: 1.0793187618255615
Epoch 1430, training loss: 62.35218811035156 = 0.06838398426771164 + 10.0 * 6.2283806800842285
Epoch 1430, val loss: 1.084157109260559
Epoch 1440, training loss: 62.32889938354492 = 0.06670678406953812 + 10.0 * 6.226219177246094
Epoch 1440, val loss: 1.089073896408081
Epoch 1450, training loss: 62.30964660644531 = 0.06508523225784302 + 10.0 * 6.224455833435059
Epoch 1450, val loss: 1.094054102897644
Epoch 1460, training loss: 62.30467224121094 = 0.06351644545793533 + 10.0 * 6.22411584854126
Epoch 1460, val loss: 1.0988675355911255
Epoch 1470, training loss: 62.33332061767578 = 0.062011923640966415 + 10.0 * 6.227130889892578
Epoch 1470, val loss: 1.1037378311157227
Epoch 1480, training loss: 62.29669189453125 = 0.060524869710206985 + 10.0 * 6.223616600036621
Epoch 1480, val loss: 1.1083608865737915
Epoch 1490, training loss: 62.29878234863281 = 0.0591028667986393 + 10.0 * 6.223968029022217
Epoch 1490, val loss: 1.1132365465164185
Epoch 1500, training loss: 62.309757232666016 = 0.05771148204803467 + 10.0 * 6.2252044677734375
Epoch 1500, val loss: 1.1178189516067505
Epoch 1510, training loss: 62.306602478027344 = 0.056366775184869766 + 10.0 * 6.2250237464904785
Epoch 1510, val loss: 1.122738242149353
Epoch 1520, training loss: 62.28042221069336 = 0.0550561249256134 + 10.0 * 6.222536563873291
Epoch 1520, val loss: 1.127563238143921
Epoch 1530, training loss: 62.27200698852539 = 0.05379224568605423 + 10.0 * 6.2218217849731445
Epoch 1530, val loss: 1.1321969032287598
Epoch 1540, training loss: 62.304141998291016 = 0.05256880819797516 + 10.0 * 6.225157260894775
Epoch 1540, val loss: 1.136783242225647
Epoch 1550, training loss: 62.29356384277344 = 0.05136461928486824 + 10.0 * 6.224219799041748
Epoch 1550, val loss: 1.1411103010177612
Epoch 1560, training loss: 62.270362854003906 = 0.050199951976537704 + 10.0 * 6.222016334533691
Epoch 1560, val loss: 1.1461607217788696
Epoch 1570, training loss: 62.25035095214844 = 0.04907337203621864 + 10.0 * 6.220128059387207
Epoch 1570, val loss: 1.1506423950195312
Epoch 1580, training loss: 62.24785232543945 = 0.04799214377999306 + 10.0 * 6.2199859619140625
Epoch 1580, val loss: 1.1552590131759644
Epoch 1590, training loss: 62.29294204711914 = 0.046947792172431946 + 10.0 * 6.224599361419678
Epoch 1590, val loss: 1.1600536108016968
Epoch 1600, training loss: 62.28414535522461 = 0.04590241238474846 + 10.0 * 6.223824501037598
Epoch 1600, val loss: 1.1643251180648804
Epoch 1610, training loss: 62.256500244140625 = 0.044893961399793625 + 10.0 * 6.221160411834717
Epoch 1610, val loss: 1.168620228767395
Epoch 1620, training loss: 62.235347747802734 = 0.04392145574092865 + 10.0 * 6.219142436981201
Epoch 1620, val loss: 1.1732730865478516
Epoch 1630, training loss: 62.234825134277344 = 0.04299087077379227 + 10.0 * 6.219183444976807
Epoch 1630, val loss: 1.1777045726776123
Epoch 1640, training loss: 62.30998229980469 = 0.04208926483988762 + 10.0 * 6.226789474487305
Epoch 1640, val loss: 1.1822484731674194
Epoch 1650, training loss: 62.24974822998047 = 0.04117697849869728 + 10.0 * 6.2208571434021
Epoch 1650, val loss: 1.1862969398498535
Epoch 1660, training loss: 62.22307205200195 = 0.04031096771359444 + 10.0 * 6.218276023864746
Epoch 1660, val loss: 1.1907756328582764
Epoch 1670, training loss: 62.21613311767578 = 0.039474811404943466 + 10.0 * 6.217665672302246
Epoch 1670, val loss: 1.1951820850372314
Epoch 1680, training loss: 62.24116516113281 = 0.038671813905239105 + 10.0 * 6.220249176025391
Epoch 1680, val loss: 1.1997281312942505
Epoch 1690, training loss: 62.230194091796875 = 0.0378655344247818 + 10.0 * 6.21923303604126
Epoch 1690, val loss: 1.2035490274429321
Epoch 1700, training loss: 62.212852478027344 = 0.03708551451563835 + 10.0 * 6.217576503753662
Epoch 1700, val loss: 1.2079391479492188
Epoch 1710, training loss: 62.20857238769531 = 0.03633496165275574 + 10.0 * 6.217223644256592
Epoch 1710, val loss: 1.2119466066360474
Epoch 1720, training loss: 62.200469970703125 = 0.035610124468803406 + 10.0 * 6.216485977172852
Epoch 1720, val loss: 1.216386079788208
Epoch 1730, training loss: 62.22555160522461 = 0.03490979224443436 + 10.0 * 6.219064235687256
Epoch 1730, val loss: 1.2206116914749146
Epoch 1740, training loss: 62.25294494628906 = 0.034218329936265945 + 10.0 * 6.221872806549072
Epoch 1740, val loss: 1.224698781967163
Epoch 1750, training loss: 62.20443344116211 = 0.033524516969919205 + 10.0 * 6.217091083526611
Epoch 1750, val loss: 1.228329062461853
Epoch 1760, training loss: 62.19017791748047 = 0.03287604823708534 + 10.0 * 6.2157301902771
Epoch 1760, val loss: 1.2326302528381348
Epoch 1770, training loss: 62.18234634399414 = 0.0322442352771759 + 10.0 * 6.215010166168213
Epoch 1770, val loss: 1.2365422248840332
Epoch 1780, training loss: 62.17940902709961 = 0.03163529187440872 + 10.0 * 6.21477746963501
Epoch 1780, val loss: 1.2406361103057861
Epoch 1790, training loss: 62.198734283447266 = 0.031040184199810028 + 10.0 * 6.216769218444824
Epoch 1790, val loss: 1.2444783449172974
Epoch 1800, training loss: 62.19120788574219 = 0.030452854931354523 + 10.0 * 6.216075420379639
Epoch 1800, val loss: 1.2485536336898804
Epoch 1810, training loss: 62.179298400878906 = 0.029871495440602303 + 10.0 * 6.214942455291748
Epoch 1810, val loss: 1.2521836757659912
Epoch 1820, training loss: 62.18056106567383 = 0.029320308938622475 + 10.0 * 6.215124130249023
Epoch 1820, val loss: 1.256384015083313
Epoch 1830, training loss: 62.20709228515625 = 0.028781002387404442 + 10.0 * 6.217831134796143
Epoch 1830, val loss: 1.2599049806594849
Epoch 1840, training loss: 62.18722915649414 = 0.02825443632900715 + 10.0 * 6.215897560119629
Epoch 1840, val loss: 1.263731837272644
Epoch 1850, training loss: 62.181522369384766 = 0.02773914858698845 + 10.0 * 6.215378284454346
Epoch 1850, val loss: 1.2674380540847778
Epoch 1860, training loss: 62.161903381347656 = 0.027240872383117676 + 10.0 * 6.213466167449951
Epoch 1860, val loss: 1.2714077234268188
Epoch 1870, training loss: 62.15829086303711 = 0.026760850101709366 + 10.0 * 6.213152885437012
Epoch 1870, val loss: 1.2752678394317627
Epoch 1880, training loss: 62.16975402832031 = 0.026295192539691925 + 10.0 * 6.214345932006836
Epoch 1880, val loss: 1.2790850400924683
Epoch 1890, training loss: 62.191280364990234 = 0.025837214663624763 + 10.0 * 6.216544151306152
Epoch 1890, val loss: 1.2826049327850342
Epoch 1900, training loss: 62.163063049316406 = 0.025377724319696426 + 10.0 * 6.213768482208252
Epoch 1900, val loss: 1.2859553098678589
Epoch 1910, training loss: 62.16495132446289 = 0.024942876771092415 + 10.0 * 6.214000701904297
Epoch 1910, val loss: 1.289595603942871
Epoch 1920, training loss: 62.18824005126953 = 0.024514028802514076 + 10.0 * 6.216372489929199
Epoch 1920, val loss: 1.2930748462677002
Epoch 1930, training loss: 62.16176986694336 = 0.024094803258776665 + 10.0 * 6.2137675285339355
Epoch 1930, val loss: 1.296714425086975
Epoch 1940, training loss: 62.150272369384766 = 0.023688768967986107 + 10.0 * 6.212658405303955
Epoch 1940, val loss: 1.3002241849899292
Epoch 1950, training loss: 62.14049530029297 = 0.023294953629374504 + 10.0 * 6.211719989776611
Epoch 1950, val loss: 1.303565502166748
Epoch 1960, training loss: 62.16693115234375 = 0.022912926971912384 + 10.0 * 6.214402198791504
Epoch 1960, val loss: 1.3067967891693115
Epoch 1970, training loss: 62.15483093261719 = 0.022535290569067 + 10.0 * 6.213229656219482
Epoch 1970, val loss: 1.31049644947052
Epoch 1980, training loss: 62.155399322509766 = 0.022167179733514786 + 10.0 * 6.21332311630249
Epoch 1980, val loss: 1.3138450384140015
Epoch 1990, training loss: 62.13130187988281 = 0.021798474714159966 + 10.0 * 6.2109503746032715
Epoch 1990, val loss: 1.3169581890106201
Epoch 2000, training loss: 62.12464141845703 = 0.021452153101563454 + 10.0 * 6.210318565368652
Epoch 2000, val loss: 1.3204001188278198
Epoch 2010, training loss: 62.12503433227539 = 0.021115267649292946 + 10.0 * 6.210391998291016
Epoch 2010, val loss: 1.3237097263336182
Epoch 2020, training loss: 62.178585052490234 = 0.02078760415315628 + 10.0 * 6.215779781341553
Epoch 2020, val loss: 1.327067494392395
Epoch 2030, training loss: 62.15517807006836 = 0.020457236096262932 + 10.0 * 6.21347188949585
Epoch 2030, val loss: 1.3301206827163696
Epoch 2040, training loss: 62.1544189453125 = 0.020136887207627296 + 10.0 * 6.213428020477295
Epoch 2040, val loss: 1.333503007888794
Epoch 2050, training loss: 62.116703033447266 = 0.019819876179099083 + 10.0 * 6.209688186645508
Epoch 2050, val loss: 1.3363198041915894
Epoch 2060, training loss: 62.11890411376953 = 0.019517233595252037 + 10.0 * 6.209939002990723
Epoch 2060, val loss: 1.3395239114761353
Epoch 2070, training loss: 62.13570022583008 = 0.019222933799028397 + 10.0 * 6.211647987365723
Epoch 2070, val loss: 1.3424433469772339
Epoch 2080, training loss: 62.12472915649414 = 0.01893315650522709 + 10.0 * 6.210579872131348
Epoch 2080, val loss: 1.3456426858901978
Epoch 2090, training loss: 62.15009307861328 = 0.018653783947229385 + 10.0 * 6.213143825531006
Epoch 2090, val loss: 1.3490415811538696
Epoch 2100, training loss: 62.1119499206543 = 0.01837029680609703 + 10.0 * 6.209357738494873
Epoch 2100, val loss: 1.3517916202545166
Epoch 2110, training loss: 62.11699295043945 = 0.01809908077120781 + 10.0 * 6.2098894119262695
Epoch 2110, val loss: 1.3548015356063843
Epoch 2120, training loss: 62.11204147338867 = 0.01783549226820469 + 10.0 * 6.209420680999756
Epoch 2120, val loss: 1.3578276634216309
Epoch 2130, training loss: 62.11425018310547 = 0.01757645420730114 + 10.0 * 6.209667205810547
Epoch 2130, val loss: 1.360674500465393
Epoch 2140, training loss: 62.11332321166992 = 0.017323311418294907 + 10.0 * 6.20959997177124
Epoch 2140, val loss: 1.363480567932129
Epoch 2150, training loss: 62.1352424621582 = 0.01707383245229721 + 10.0 * 6.211816787719727
Epoch 2150, val loss: 1.3660199642181396
Epoch 2160, training loss: 62.13317108154297 = 0.016830062493681908 + 10.0 * 6.211634159088135
Epoch 2160, val loss: 1.3691596984863281
Epoch 2170, training loss: 62.10090255737305 = 0.01658787578344345 + 10.0 * 6.208431720733643
Epoch 2170, val loss: 1.3720892667770386
Epoch 2180, training loss: 62.089088439941406 = 0.016352875158190727 + 10.0 * 6.207273483276367
Epoch 2180, val loss: 1.3747411966323853
Epoch 2190, training loss: 62.08552932739258 = 0.01612767018377781 + 10.0 * 6.206940174102783
Epoch 2190, val loss: 1.3775715827941895
Epoch 2200, training loss: 62.087059020996094 = 0.015910474583506584 + 10.0 * 6.2071146965026855
Epoch 2200, val loss: 1.3803449869155884
Epoch 2210, training loss: 62.129112243652344 = 0.015695421025156975 + 10.0 * 6.211341857910156
Epoch 2210, val loss: 1.3829338550567627
Epoch 2220, training loss: 62.10197448730469 = 0.015479168854653835 + 10.0 * 6.208649635314941
Epoch 2220, val loss: 1.3855884075164795
Epoch 2230, training loss: 62.14237594604492 = 0.015268823131918907 + 10.0 * 6.212710380554199
Epoch 2230, val loss: 1.3880712985992432
Epoch 2240, training loss: 62.09608840942383 = 0.015061191283166409 + 10.0 * 6.208102703094482
Epoch 2240, val loss: 1.3911020755767822
Epoch 2250, training loss: 62.079036712646484 = 0.014856938272714615 + 10.0 * 6.206418037414551
Epoch 2250, val loss: 1.3934608697891235
Epoch 2260, training loss: 62.075496673583984 = 0.014663404785096645 + 10.0 * 6.206083297729492
Epoch 2260, val loss: 1.3960686922073364
Epoch 2270, training loss: 62.070865631103516 = 0.014474681578576565 + 10.0 * 6.205639362335205
Epoch 2270, val loss: 1.398699164390564
Epoch 2280, training loss: 62.13434982299805 = 0.014291241765022278 + 10.0 * 6.212006092071533
Epoch 2280, val loss: 1.4011986255645752
Epoch 2290, training loss: 62.10226058959961 = 0.014101908542215824 + 10.0 * 6.208815574645996
Epoch 2290, val loss: 1.403443455696106
Epoch 2300, training loss: 62.093387603759766 = 0.013917290605604649 + 10.0 * 6.207947254180908
Epoch 2300, val loss: 1.4059375524520874
Epoch 2310, training loss: 62.09046173095703 = 0.013739364221692085 + 10.0 * 6.207672119140625
Epoch 2310, val loss: 1.4082770347595215
Epoch 2320, training loss: 62.06888961791992 = 0.01356477476656437 + 10.0 * 6.205532550811768
Epoch 2320, val loss: 1.4106957912445068
Epoch 2330, training loss: 62.09284210205078 = 0.013396857306361198 + 10.0 * 6.207944393157959
Epoch 2330, val loss: 1.4130598306655884
Epoch 2340, training loss: 62.09001922607422 = 0.013228380121290684 + 10.0 * 6.20767879486084
Epoch 2340, val loss: 1.4151626825332642
Epoch 2350, training loss: 62.076072692871094 = 0.013062793761491776 + 10.0 * 6.206301212310791
Epoch 2350, val loss: 1.417739748954773
Epoch 2360, training loss: 62.05853271484375 = 0.01290182676166296 + 10.0 * 6.204563140869141
Epoch 2360, val loss: 1.4201831817626953
Epoch 2370, training loss: 62.06003952026367 = 0.012747474014759064 + 10.0 * 6.204729080200195
Epoch 2370, val loss: 1.4224486351013184
Epoch 2380, training loss: 62.07622528076172 = 0.012596690095961094 + 10.0 * 6.206362724304199
Epoch 2380, val loss: 1.4247795343399048
Epoch 2390, training loss: 62.080665588378906 = 0.012446615844964981 + 10.0 * 6.206821918487549
Epoch 2390, val loss: 1.4272065162658691
Epoch 2400, training loss: 62.07762908935547 = 0.012294303625822067 + 10.0 * 6.206533432006836
Epoch 2400, val loss: 1.4291871786117554
Epoch 2410, training loss: 62.06008529663086 = 0.012145533226430416 + 10.0 * 6.204793930053711
Epoch 2410, val loss: 1.431174635887146
Epoch 2420, training loss: 62.071598052978516 = 0.012004760093986988 + 10.0 * 6.205959320068359
Epoch 2420, val loss: 1.4334731101989746
Epoch 2430, training loss: 62.06444549560547 = 0.01186355296522379 + 10.0 * 6.205258369445801
Epoch 2430, val loss: 1.4355032444000244
Epoch 2440, training loss: 62.051326751708984 = 0.011725954711437225 + 10.0 * 6.203959941864014
Epoch 2440, val loss: 1.4378880262374878
Epoch 2450, training loss: 62.0512580871582 = 0.011592511087656021 + 10.0 * 6.2039666175842285
Epoch 2450, val loss: 1.4400355815887451
Epoch 2460, training loss: 62.091148376464844 = 0.0114598348736763 + 10.0 * 6.207968711853027
Epoch 2460, val loss: 1.4419523477554321
Epoch 2470, training loss: 62.079612731933594 = 0.011326750740408897 + 10.0 * 6.206828594207764
Epoch 2470, val loss: 1.443756103515625
Epoch 2480, training loss: 62.07061767578125 = 0.01119619607925415 + 10.0 * 6.205942153930664
Epoch 2480, val loss: 1.445882797241211
Epoch 2490, training loss: 62.04142379760742 = 0.011070833541452885 + 10.0 * 6.203035354614258
Epoch 2490, val loss: 1.448193907737732
Epoch 2500, training loss: 62.03544998168945 = 0.010949033312499523 + 10.0 * 6.202450275421143
Epoch 2500, val loss: 1.450131893157959
Epoch 2510, training loss: 62.03334045410156 = 0.010829602368175983 + 10.0 * 6.202250957489014
Epoch 2510, val loss: 1.4521117210388184
Epoch 2520, training loss: 62.05752944946289 = 0.010714277625083923 + 10.0 * 6.204681396484375
Epoch 2520, val loss: 1.4541136026382446
Epoch 2530, training loss: 62.06454849243164 = 0.010598395019769669 + 10.0 * 6.205395221710205
Epoch 2530, val loss: 1.4561289548873901
Epoch 2540, training loss: 62.04367446899414 = 0.01047872006893158 + 10.0 * 6.203319549560547
Epoch 2540, val loss: 1.4577336311340332
Epoch 2550, training loss: 62.049720764160156 = 0.010363168083131313 + 10.0 * 6.203935623168945
Epoch 2550, val loss: 1.459497332572937
Epoch 2560, training loss: 62.02901840209961 = 0.010251728817820549 + 10.0 * 6.201876640319824
Epoch 2560, val loss: 1.4613367319107056
Epoch 2570, training loss: 62.030487060546875 = 0.010144727304577827 + 10.0 * 6.2020344734191895
Epoch 2570, val loss: 1.4633122682571411
Epoch 2580, training loss: 62.08544921875 = 0.010038567706942558 + 10.0 * 6.207540988922119
Epoch 2580, val loss: 1.4649686813354492
Epoch 2590, training loss: 62.040802001953125 = 0.009932279586791992 + 10.0 * 6.203086853027344
Epoch 2590, val loss: 1.4667315483093262
Epoch 2600, training loss: 62.02717208862305 = 0.009828422218561172 + 10.0 * 6.20173454284668
Epoch 2600, val loss: 1.46871018409729
Epoch 2610, training loss: 62.02193832397461 = 0.009729239158332348 + 10.0 * 6.201220989227295
Epoch 2610, val loss: 1.4705690145492554
Epoch 2620, training loss: 62.06390380859375 = 0.009633549489080906 + 10.0 * 6.205427169799805
Epoch 2620, val loss: 1.472382664680481
Epoch 2630, training loss: 62.02009582519531 = 0.00952960830181837 + 10.0 * 6.201056480407715
Epoch 2630, val loss: 1.4737722873687744
Epoch 2640, training loss: 62.039737701416016 = 0.009434527717530727 + 10.0 * 6.203030586242676
Epoch 2640, val loss: 1.475655198097229
Epoch 2650, training loss: 62.09781265258789 = 0.009340977296233177 + 10.0 * 6.2088470458984375
Epoch 2650, val loss: 1.4774142503738403
Epoch 2660, training loss: 62.036556243896484 = 0.009240318089723587 + 10.0 * 6.202731609344482
Epoch 2660, val loss: 1.4787026643753052
Epoch 2670, training loss: 62.01653289794922 = 0.009147226810455322 + 10.0 * 6.200738430023193
Epoch 2670, val loss: 1.4802922010421753
Epoch 2680, training loss: 62.00814437866211 = 0.009059197269380093 + 10.0 * 6.199908256530762
Epoch 2680, val loss: 1.481942057609558
Epoch 2690, training loss: 62.01927185058594 = 0.008972429670393467 + 10.0 * 6.2010297775268555
Epoch 2690, val loss: 1.4833533763885498
Epoch 2700, training loss: 62.06889724731445 = 0.008885064162313938 + 10.0 * 6.206001281738281
Epoch 2700, val loss: 1.4847787618637085
Epoch 2710, training loss: 62.024009704589844 = 0.008798631839454174 + 10.0 * 6.201520919799805
Epoch 2710, val loss: 1.4868426322937012
Epoch 2720, training loss: 62.009708404541016 = 0.008712765760719776 + 10.0 * 6.200099468231201
Epoch 2720, val loss: 1.4881311655044556
Epoch 2730, training loss: 62.014652252197266 = 0.008632047101855278 + 10.0 * 6.200602054595947
Epoch 2730, val loss: 1.4897462129592896
Epoch 2740, training loss: 62.034912109375 = 0.008550835773348808 + 10.0 * 6.202635765075684
Epoch 2740, val loss: 1.4910696744918823
Epoch 2750, training loss: 62.011863708496094 = 0.008470110595226288 + 10.0 * 6.200339317321777
Epoch 2750, val loss: 1.4927729368209839
Epoch 2760, training loss: 62.005592346191406 = 0.008390942588448524 + 10.0 * 6.1997199058532715
Epoch 2760, val loss: 1.4942327737808228
Epoch 2770, training loss: 62.04250717163086 = 0.00831447634845972 + 10.0 * 6.203419208526611
Epoch 2770, val loss: 1.4955168962478638
Epoch 2780, training loss: 62.03938674926758 = 0.008235206827521324 + 10.0 * 6.203114986419678
Epoch 2780, val loss: 1.4966598749160767
Epoch 2790, training loss: 62.00289535522461 = 0.00815768726170063 + 10.0 * 6.199473857879639
Epoch 2790, val loss: 1.4984955787658691
Epoch 2800, training loss: 61.99540328979492 = 0.008083269000053406 + 10.0 * 6.198731899261475
Epoch 2800, val loss: 1.4996813535690308
Epoch 2810, training loss: 61.988800048828125 = 0.008012083359062672 + 10.0 * 6.1980791091918945
Epoch 2810, val loss: 1.501242995262146
Epoch 2820, training loss: 61.98979949951172 = 0.007942529395222664 + 10.0 * 6.198185920715332
Epoch 2820, val loss: 1.5026366710662842
Epoch 2830, training loss: 62.05873107910156 = 0.007875163108110428 + 10.0 * 6.205085754394531
Epoch 2830, val loss: 1.5039955377578735
Epoch 2840, training loss: 62.01959228515625 = 0.007799999322742224 + 10.0 * 6.201179027557373
Epoch 2840, val loss: 1.5050389766693115
Epoch 2850, training loss: 62.007850646972656 = 0.007729083299636841 + 10.0 * 6.20001220703125
Epoch 2850, val loss: 1.5064599514007568
Epoch 2860, training loss: 61.99079132080078 = 0.007661295123398304 + 10.0 * 6.198313236236572
Epoch 2860, val loss: 1.5078341960906982
Epoch 2870, training loss: 61.98873519897461 = 0.007596952840685844 + 10.0 * 6.198113918304443
Epoch 2870, val loss: 1.5092353820800781
Epoch 2880, training loss: 62.05388641357422 = 0.007536198012530804 + 10.0 * 6.204634666442871
Epoch 2880, val loss: 1.5107641220092773
Epoch 2890, training loss: 62.026615142822266 = 0.007464245893061161 + 10.0 * 6.2019147872924805
Epoch 2890, val loss: 1.5112942457199097
Epoch 2900, training loss: 61.99713897705078 = 0.007397043518722057 + 10.0 * 6.198974132537842
Epoch 2900, val loss: 1.5128213167190552
Epoch 2910, training loss: 61.9815673828125 = 0.007333224173635244 + 10.0 * 6.197423458099365
Epoch 2910, val loss: 1.5139076709747314
Epoch 2920, training loss: 61.97642517089844 = 0.0072738006711006165 + 10.0 * 6.196915149688721
Epoch 2920, val loss: 1.515188217163086
Epoch 2930, training loss: 61.97490692138672 = 0.007215444464236498 + 10.0 * 6.1967692375183105
Epoch 2930, val loss: 1.5164662599563599
Epoch 2940, training loss: 61.972904205322266 = 0.007157421205192804 + 10.0 * 6.196574687957764
Epoch 2940, val loss: 1.5176931619644165
Epoch 2950, training loss: 61.994956970214844 = 0.007100964896380901 + 10.0 * 6.198785781860352
Epoch 2950, val loss: 1.519020438194275
Epoch 2960, training loss: 62.00652313232422 = 0.007043045945465565 + 10.0 * 6.199948310852051
Epoch 2960, val loss: 1.519887924194336
Epoch 2970, training loss: 62.00603485107422 = 0.006978853605687618 + 10.0 * 6.199905872344971
Epoch 2970, val loss: 1.520875096321106
Epoch 2980, training loss: 61.981788635253906 = 0.006922231055796146 + 10.0 * 6.197486400604248
Epoch 2980, val loss: 1.521997094154358
Epoch 2990, training loss: 61.96928024291992 = 0.006866803392767906 + 10.0 * 6.19624137878418
Epoch 2990, val loss: 1.5230653285980225
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8371112282551397
The final CL Acc:0.77037, 0.01048, The final GNN Acc:0.84027, 0.00342
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10578])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.90448760986328 = 1.9362034797668457 + 10.0 * 8.59682846069336
Epoch 0, val loss: 1.9287124872207642
Epoch 10, training loss: 87.88848876953125 = 1.927374005317688 + 10.0 * 8.596111297607422
Epoch 10, val loss: 1.920568823814392
Epoch 20, training loss: 87.81721496582031 = 1.9166338443756104 + 10.0 * 8.590058326721191
Epoch 20, val loss: 1.910243034362793
Epoch 30, training loss: 87.33657836914062 = 1.9026833772659302 + 10.0 * 8.543389320373535
Epoch 30, val loss: 1.8964974880218506
Epoch 40, training loss: 84.34050750732422 = 1.886249303817749 + 10.0 * 8.245426177978516
Epoch 40, val loss: 1.8806171417236328
Epoch 50, training loss: 78.77596282958984 = 1.8691558837890625 + 10.0 * 7.690680503845215
Epoch 50, val loss: 1.8649790287017822
Epoch 60, training loss: 74.2021484375 = 1.8576747179031372 + 10.0 * 7.234447479248047
Epoch 60, val loss: 1.8546639680862427
Epoch 70, training loss: 71.47240447998047 = 1.8482728004455566 + 10.0 * 6.9624128341674805
Epoch 70, val loss: 1.8461804389953613
Epoch 80, training loss: 70.06144714355469 = 1.839939832687378 + 10.0 * 6.822150230407715
Epoch 80, val loss: 1.8380337953567505
Epoch 90, training loss: 69.28849029541016 = 1.8313230276107788 + 10.0 * 6.745716571807861
Epoch 90, val loss: 1.8296643495559692
Epoch 100, training loss: 68.63488006591797 = 1.8228726387023926 + 10.0 * 6.6812005043029785
Epoch 100, val loss: 1.8218650817871094
Epoch 110, training loss: 68.03849029541016 = 1.8156781196594238 + 10.0 * 6.622280597686768
Epoch 110, val loss: 1.815284252166748
Epoch 120, training loss: 67.5752182006836 = 1.809287190437317 + 10.0 * 6.576592922210693
Epoch 120, val loss: 1.8092479705810547
Epoch 130, training loss: 67.23779296875 = 1.8029531240463257 + 10.0 * 6.543483734130859
Epoch 130, val loss: 1.8030461072921753
Epoch 140, training loss: 66.99710845947266 = 1.7961276769638062 + 10.0 * 6.5200982093811035
Epoch 140, val loss: 1.7964298725128174
Epoch 150, training loss: 66.78636932373047 = 1.7888115644454956 + 10.0 * 6.499755859375
Epoch 150, val loss: 1.789547324180603
Epoch 160, training loss: 66.61238098144531 = 1.7812623977661133 + 10.0 * 6.483111381530762
Epoch 160, val loss: 1.7824194431304932
Epoch 170, training loss: 66.48188781738281 = 1.7732114791870117 + 10.0 * 6.47086763381958
Epoch 170, val loss: 1.7748974561691284
Epoch 180, training loss: 66.33819580078125 = 1.764508605003357 + 10.0 * 6.457368850708008
Epoch 180, val loss: 1.7667734622955322
Epoch 190, training loss: 66.20466613769531 = 1.7550350427627563 + 10.0 * 6.444963455200195
Epoch 190, val loss: 1.7580560445785522
Epoch 200, training loss: 66.09840393066406 = 1.7446043491363525 + 10.0 * 6.435379981994629
Epoch 200, val loss: 1.7485684156417847
Epoch 210, training loss: 65.98159790039062 = 1.7331851720809937 + 10.0 * 6.424841403961182
Epoch 210, val loss: 1.738294005393982
Epoch 220, training loss: 65.85941314697266 = 1.720708966255188 + 10.0 * 6.413870334625244
Epoch 220, val loss: 1.726901888847351
Epoch 230, training loss: 65.73817443847656 = 1.7069438695907593 + 10.0 * 6.403122425079346
Epoch 230, val loss: 1.7145509719848633
Epoch 240, training loss: 65.63699340820312 = 1.691794753074646 + 10.0 * 6.394519805908203
Epoch 240, val loss: 1.7010645866394043
Epoch 250, training loss: 65.5635986328125 = 1.6753095388412476 + 10.0 * 6.388828754425049
Epoch 250, val loss: 1.6861783266067505
Epoch 260, training loss: 65.45307159423828 = 1.6572437286376953 + 10.0 * 6.37958288192749
Epoch 260, val loss: 1.6701120138168335
Epoch 270, training loss: 65.36243438720703 = 1.6377034187316895 + 10.0 * 6.372472763061523
Epoch 270, val loss: 1.6528719663619995
Epoch 280, training loss: 65.28165435791016 = 1.6167242527008057 + 10.0 * 6.366492748260498
Epoch 280, val loss: 1.6343027353286743
Epoch 290, training loss: 65.20576477050781 = 1.5942987203598022 + 10.0 * 6.361146450042725
Epoch 290, val loss: 1.6147297620773315
Epoch 300, training loss: 65.13783264160156 = 1.570709228515625 + 10.0 * 6.356712818145752
Epoch 300, val loss: 1.5941275358200073
Epoch 310, training loss: 65.05204010009766 = 1.5458794832229614 + 10.0 * 6.350615978240967
Epoch 310, val loss: 1.5727200508117676
Epoch 320, training loss: 65.00257110595703 = 1.5201380252838135 + 10.0 * 6.348243236541748
Epoch 320, val loss: 1.5506314039230347
Epoch 330, training loss: 64.91568756103516 = 1.4935258626937866 + 10.0 * 6.342216491699219
Epoch 330, val loss: 1.5281505584716797
Epoch 340, training loss: 64.85684204101562 = 1.466348648071289 + 10.0 * 6.339049339294434
Epoch 340, val loss: 1.505398154258728
Epoch 350, training loss: 64.76544952392578 = 1.4387069940567017 + 10.0 * 6.332674503326416
Epoch 350, val loss: 1.4826445579528809
Epoch 360, training loss: 64.7194595336914 = 1.410859227180481 + 10.0 * 6.330859661102295
Epoch 360, val loss: 1.459936261177063
Epoch 370, training loss: 64.66060638427734 = 1.3827682733535767 + 10.0 * 6.327783584594727
Epoch 370, val loss: 1.437377691268921
Epoch 380, training loss: 64.58492279052734 = 1.3548657894134521 + 10.0 * 6.323005676269531
Epoch 380, val loss: 1.4152621030807495
Epoch 390, training loss: 64.52278137207031 = 1.3270161151885986 + 10.0 * 6.319576740264893
Epoch 390, val loss: 1.3935229778289795
Epoch 400, training loss: 64.51032257080078 = 1.2993272542953491 + 10.0 * 6.321099758148193
Epoch 400, val loss: 1.3723101615905762
Epoch 410, training loss: 64.4164047241211 = 1.272011637687683 + 10.0 * 6.314439296722412
Epoch 410, val loss: 1.3515418767929077
Epoch 420, training loss: 64.3531494140625 = 1.244836688041687 + 10.0 * 6.310831069946289
Epoch 420, val loss: 1.3314634561538696
Epoch 430, training loss: 64.35380554199219 = 1.2178981304168701 + 10.0 * 6.3135905265808105
Epoch 430, val loss: 1.3119187355041504
Epoch 440, training loss: 64.25756072998047 = 1.1915401220321655 + 10.0 * 6.306601524353027
Epoch 440, val loss: 1.2929812669754028
Epoch 450, training loss: 64.194580078125 = 1.165563702583313 + 10.0 * 6.302901744842529
Epoch 450, val loss: 1.2747737169265747
Epoch 460, training loss: 64.15055084228516 = 1.1398969888687134 + 10.0 * 6.301065444946289
Epoch 460, val loss: 1.2571899890899658
Epoch 470, training loss: 64.11664581298828 = 1.114488124847412 + 10.0 * 6.300215721130371
Epoch 470, val loss: 1.2400599718093872
Epoch 480, training loss: 64.05316925048828 = 1.0896010398864746 + 10.0 * 6.296357154846191
Epoch 480, val loss: 1.2236847877502441
Epoch 490, training loss: 64.01686096191406 = 1.0651451349258423 + 10.0 * 6.29517126083374
Epoch 490, val loss: 1.207997441291809
Epoch 500, training loss: 63.966835021972656 = 1.0410305261611938 + 10.0 * 6.292580604553223
Epoch 500, val loss: 1.1930330991744995
Epoch 510, training loss: 63.930946350097656 = 1.017371416091919 + 10.0 * 6.291357517242432
Epoch 510, val loss: 1.1787230968475342
Epoch 520, training loss: 63.87724304199219 = 0.9943234920501709 + 10.0 * 6.288291931152344
Epoch 520, val loss: 1.1650744676589966
Epoch 530, training loss: 63.868896484375 = 0.9716148376464844 + 10.0 * 6.289728164672852
Epoch 530, val loss: 1.1522144079208374
Epoch 540, training loss: 63.81502151489258 = 0.9494837522506714 + 10.0 * 6.286553859710693
Epoch 540, val loss: 1.1400891542434692
Epoch 550, training loss: 63.75112533569336 = 0.9279507994651794 + 10.0 * 6.282317161560059
Epoch 550, val loss: 1.1285450458526611
Epoch 560, training loss: 63.73372268676758 = 0.9069215655326843 + 10.0 * 6.282680034637451
Epoch 560, val loss: 1.1178547143936157
Epoch 570, training loss: 63.70204162597656 = 0.8863182067871094 + 10.0 * 6.281572341918945
Epoch 570, val loss: 1.1076515913009644
Epoch 580, training loss: 63.64461898803711 = 0.8664092421531677 + 10.0 * 6.277821063995361
Epoch 580, val loss: 1.0983043909072876
Epoch 590, training loss: 63.6105842590332 = 0.8470422029495239 + 10.0 * 6.2763543128967285
Epoch 590, val loss: 1.0895910263061523
Epoch 600, training loss: 63.593074798583984 = 0.828178882598877 + 10.0 * 6.276489734649658
Epoch 600, val loss: 1.0814225673675537
Epoch 610, training loss: 63.54198455810547 = 0.8096898198127747 + 10.0 * 6.273229598999023
Epoch 610, val loss: 1.0739102363586426
Epoch 620, training loss: 63.579811096191406 = 0.7915933728218079 + 10.0 * 6.27882194519043
Epoch 620, val loss: 1.0667275190353394
Epoch 630, training loss: 63.48588943481445 = 0.7742860913276672 + 10.0 * 6.27116060256958
Epoch 630, val loss: 1.0602978467941284
Epoch 640, training loss: 63.45377731323242 = 0.7573210597038269 + 10.0 * 6.269645690917969
Epoch 640, val loss: 1.054347038269043
Epoch 650, training loss: 63.41980743408203 = 0.7407512664794922 + 10.0 * 6.267905235290527
Epoch 650, val loss: 1.0489344596862793
Epoch 660, training loss: 63.40907287597656 = 0.7247166037559509 + 10.0 * 6.268435478210449
Epoch 660, val loss: 1.04399573802948
Epoch 670, training loss: 63.37805938720703 = 0.7088451385498047 + 10.0 * 6.266921043395996
Epoch 670, val loss: 1.0392051935195923
Epoch 680, training loss: 63.38074493408203 = 0.6934807896614075 + 10.0 * 6.268726348876953
Epoch 680, val loss: 1.0349693298339844
Epoch 690, training loss: 63.331512451171875 = 0.6783968806266785 + 10.0 * 6.265311241149902
Epoch 690, val loss: 1.0311039686203003
Epoch 700, training loss: 63.291744232177734 = 0.6637392044067383 + 10.0 * 6.262800693511963
Epoch 700, val loss: 1.027624487876892
Epoch 710, training loss: 63.26926040649414 = 0.6494272947311401 + 10.0 * 6.261983394622803
Epoch 710, val loss: 1.024536371231079
Epoch 720, training loss: 63.279903411865234 = 0.6353733539581299 + 10.0 * 6.264452934265137
Epoch 720, val loss: 1.021726131439209
Epoch 730, training loss: 63.24046325683594 = 0.621728241443634 + 10.0 * 6.261873722076416
Epoch 730, val loss: 1.019258737564087
Epoch 740, training loss: 63.21220397949219 = 0.6081982254981995 + 10.0 * 6.260400295257568
Epoch 740, val loss: 1.0169824361801147
Epoch 750, training loss: 63.18374252319336 = 0.5950816869735718 + 10.0 * 6.258866310119629
Epoch 750, val loss: 1.0152491331100464
Epoch 760, training loss: 63.16974639892578 = 0.5821042656898499 + 10.0 * 6.258764266967773
Epoch 760, val loss: 1.013533115386963
Epoch 770, training loss: 63.15304946899414 = 0.5694745182991028 + 10.0 * 6.258357524871826
Epoch 770, val loss: 1.012304663658142
Epoch 780, training loss: 63.12801742553711 = 0.5570477843284607 + 10.0 * 6.257096767425537
Epoch 780, val loss: 1.010949969291687
Epoch 790, training loss: 63.10415267944336 = 0.5448935031890869 + 10.0 * 6.25592565536499
Epoch 790, val loss: 1.0102230310440063
Epoch 800, training loss: 63.074981689453125 = 0.5328758955001831 + 10.0 * 6.254210472106934
Epoch 800, val loss: 1.0095094442367554
Epoch 810, training loss: 63.099952697753906 = 0.5211615562438965 + 10.0 * 6.257879257202148
Epoch 810, val loss: 1.009082555770874
Epoch 820, training loss: 63.04991149902344 = 0.5095943212509155 + 10.0 * 6.254031658172607
Epoch 820, val loss: 1.0087544918060303
Epoch 830, training loss: 63.018428802490234 = 0.4981699585914612 + 10.0 * 6.252026081085205
Epoch 830, val loss: 1.0086880922317505
Epoch 840, training loss: 63.00921630859375 = 0.48705923557281494 + 10.0 * 6.252215385437012
Epoch 840, val loss: 1.008863091468811
Epoch 850, training loss: 62.9941291809082 = 0.4760870337486267 + 10.0 * 6.251804351806641
Epoch 850, val loss: 1.0090886354446411
Epoch 860, training loss: 62.972408294677734 = 0.46527230739593506 + 10.0 * 6.25071382522583
Epoch 860, val loss: 1.0096087455749512
Epoch 870, training loss: 62.95558547973633 = 0.4546358287334442 + 10.0 * 6.250094890594482
Epoch 870, val loss: 1.0102102756500244
Epoch 880, training loss: 62.925743103027344 = 0.4441644251346588 + 10.0 * 6.248157978057861
Epoch 880, val loss: 1.0110167264938354
Epoch 890, training loss: 62.93171310424805 = 0.43387892842292786 + 10.0 * 6.249783515930176
Epoch 890, val loss: 1.0120514631271362
Epoch 900, training loss: 62.89161682128906 = 0.42371243238449097 + 10.0 * 6.246790409088135
Epoch 900, val loss: 1.0128942728042603
Epoch 910, training loss: 62.878726959228516 = 0.41375112533569336 + 10.0 * 6.246497631072998
Epoch 910, val loss: 1.0141414403915405
Epoch 920, training loss: 62.90619659423828 = 0.40393081307411194 + 10.0 * 6.2502264976501465
Epoch 920, val loss: 1.0152544975280762
Epoch 930, training loss: 62.869564056396484 = 0.3941675126552582 + 10.0 * 6.247539520263672
Epoch 930, val loss: 1.0165518522262573
Epoch 940, training loss: 62.83236312866211 = 0.3846672475337982 + 10.0 * 6.24476957321167
Epoch 940, val loss: 1.0179401636123657
Epoch 950, training loss: 62.81742858886719 = 0.37527304887771606 + 10.0 * 6.244215488433838
Epoch 950, val loss: 1.019473671913147
Epoch 960, training loss: 62.84307098388672 = 0.3660188317298889 + 10.0 * 6.247704982757568
Epoch 960, val loss: 1.0209661722183228
Epoch 970, training loss: 62.78756332397461 = 0.3569321036338806 + 10.0 * 6.243062973022461
Epoch 970, val loss: 1.022504448890686
Epoch 980, training loss: 62.796329498291016 = 0.3479996919631958 + 10.0 * 6.244832992553711
Epoch 980, val loss: 1.0242360830307007
Epoch 990, training loss: 62.76339340209961 = 0.33914950489997864 + 10.0 * 6.242424488067627
Epoch 990, val loss: 1.025794267654419
Epoch 1000, training loss: 62.763580322265625 = 0.33050602674484253 + 10.0 * 6.243307590484619
Epoch 1000, val loss: 1.0275012254714966
Epoch 1010, training loss: 62.728206634521484 = 0.3220290243625641 + 10.0 * 6.240617752075195
Epoch 1010, val loss: 1.0293515920639038
Epoch 1020, training loss: 62.71200942993164 = 0.3137182593345642 + 10.0 * 6.239829063415527
Epoch 1020, val loss: 1.031211495399475
Epoch 1030, training loss: 62.711669921875 = 0.3055501878261566 + 10.0 * 6.240612030029297
Epoch 1030, val loss: 1.032990574836731
Epoch 1040, training loss: 62.726417541503906 = 0.297489196062088 + 10.0 * 6.242892742156982
Epoch 1040, val loss: 1.0347416400909424
Epoch 1050, training loss: 62.67676544189453 = 0.2896624207496643 + 10.0 * 6.238710403442383
Epoch 1050, val loss: 1.0366092920303345
Epoch 1060, training loss: 62.66659927368164 = 0.28198710083961487 + 10.0 * 6.238461494445801
Epoch 1060, val loss: 1.0387822389602661
Epoch 1070, training loss: 62.67792892456055 = 0.2745324373245239 + 10.0 * 6.240339756011963
Epoch 1070, val loss: 1.0408618450164795
Epoch 1080, training loss: 62.63740158081055 = 0.2671467661857605 + 10.0 * 6.237025260925293
Epoch 1080, val loss: 1.0428082942962646
Epoch 1090, training loss: 62.62786102294922 = 0.2600008547306061 + 10.0 * 6.236785888671875
Epoch 1090, val loss: 1.0451816320419312
Epoch 1100, training loss: 62.637596130371094 = 0.25298136472702026 + 10.0 * 6.238461494445801
Epoch 1100, val loss: 1.0474133491516113
Epoch 1110, training loss: 62.61213302612305 = 0.24618902802467346 + 10.0 * 6.236594200134277
Epoch 1110, val loss: 1.0496748685836792
Epoch 1120, training loss: 62.611236572265625 = 0.23952028155326843 + 10.0 * 6.237171649932861
Epoch 1120, val loss: 1.052030086517334
Epoch 1130, training loss: 62.60725402832031 = 0.23302428424358368 + 10.0 * 6.237422943115234
Epoch 1130, val loss: 1.0544689893722534
Epoch 1140, training loss: 62.59432601928711 = 0.2267046570777893 + 10.0 * 6.236762046813965
Epoch 1140, val loss: 1.0569424629211426
Epoch 1150, training loss: 62.56052780151367 = 0.2205919325351715 + 10.0 * 6.2339935302734375
Epoch 1150, val loss: 1.0597261190414429
Epoch 1160, training loss: 62.552223205566406 = 0.21466203033924103 + 10.0 * 6.233756065368652
Epoch 1160, val loss: 1.0625503063201904
Epoch 1170, training loss: 62.57149124145508 = 0.20888839662075043 + 10.0 * 6.236260414123535
Epoch 1170, val loss: 1.0654339790344238
Epoch 1180, training loss: 62.56138229370117 = 0.20320725440979004 + 10.0 * 6.2358174324035645
Epoch 1180, val loss: 1.068055272102356
Epoch 1190, training loss: 62.51780319213867 = 0.19764569401741028 + 10.0 * 6.232015609741211
Epoch 1190, val loss: 1.0710864067077637
Epoch 1200, training loss: 62.507896423339844 = 0.19233164191246033 + 10.0 * 6.231556415557861
Epoch 1200, val loss: 1.0739734172821045
Epoch 1210, training loss: 62.4962043762207 = 0.18716944754123688 + 10.0 * 6.230903625488281
Epoch 1210, val loss: 1.0772194862365723
Epoch 1220, training loss: 62.516239166259766 = 0.18215487897396088 + 10.0 * 6.233408451080322
Epoch 1220, val loss: 1.0803724527359009
Epoch 1230, training loss: 62.49549865722656 = 0.17725473642349243 + 10.0 * 6.2318243980407715
Epoch 1230, val loss: 1.083446741104126
Epoch 1240, training loss: 62.497066497802734 = 0.17246702313423157 + 10.0 * 6.232460021972656
Epoch 1240, val loss: 1.086661458015442
Epoch 1250, training loss: 62.470458984375 = 0.16789233684539795 + 10.0 * 6.2302565574646
Epoch 1250, val loss: 1.0901399850845337
Epoch 1260, training loss: 62.46145248413086 = 0.16344358026981354 + 10.0 * 6.229800701141357
Epoch 1260, val loss: 1.0936940908432007
Epoch 1270, training loss: 62.49945068359375 = 0.15914084017276764 + 10.0 * 6.234030723571777
Epoch 1270, val loss: 1.0971630811691284
Epoch 1280, training loss: 62.47767639160156 = 0.15488651394844055 + 10.0 * 6.232278823852539
Epoch 1280, val loss: 1.1007506847381592
Epoch 1290, training loss: 62.43611526489258 = 0.1508135050535202 + 10.0 * 6.228529930114746
Epoch 1290, val loss: 1.1046066284179688
Epoch 1300, training loss: 62.42878723144531 = 0.14686575531959534 + 10.0 * 6.228192329406738
Epoch 1300, val loss: 1.108413577079773
Epoch 1310, training loss: 62.4552116394043 = 0.14303885400295258 + 10.0 * 6.231217384338379
Epoch 1310, val loss: 1.1122822761535645
Epoch 1320, training loss: 62.413578033447266 = 0.13932840526103973 + 10.0 * 6.2274250984191895
Epoch 1320, val loss: 1.1161736249923706
Epoch 1330, training loss: 62.40992736816406 = 0.13573327660560608 + 10.0 * 6.227419376373291
Epoch 1330, val loss: 1.120245337486267
Epoch 1340, training loss: 62.40805435180664 = 0.1322253942489624 + 10.0 * 6.227582931518555
Epoch 1340, val loss: 1.1242907047271729
Epoch 1350, training loss: 62.41740417480469 = 0.1288442760705948 + 10.0 * 6.228856086730957
Epoch 1350, val loss: 1.1284675598144531
Epoch 1360, training loss: 62.40841293334961 = 0.12556609511375427 + 10.0 * 6.22828483581543
Epoch 1360, val loss: 1.132595181465149
Epoch 1370, training loss: 62.388092041015625 = 0.12238240987062454 + 10.0 * 6.226571083068848
Epoch 1370, val loss: 1.1368008852005005
Epoch 1380, training loss: 62.3730354309082 = 0.11928173154592514 + 10.0 * 6.225375175476074
Epoch 1380, val loss: 1.1411361694335938
Epoch 1390, training loss: 62.38229751586914 = 0.11629944294691086 + 10.0 * 6.22659969329834
Epoch 1390, val loss: 1.1455005407333374
Epoch 1400, training loss: 62.388458251953125 = 0.11338891088962555 + 10.0 * 6.2275071144104
Epoch 1400, val loss: 1.1497818231582642
Epoch 1410, training loss: 62.380558013916016 = 0.11052753031253815 + 10.0 * 6.22700309753418
Epoch 1410, val loss: 1.1541415452957153
Epoch 1420, training loss: 62.35541534423828 = 0.10781392455101013 + 10.0 * 6.224760055541992
Epoch 1420, val loss: 1.1586518287658691
Epoch 1430, training loss: 62.35442352294922 = 0.10517092794179916 + 10.0 * 6.2249250411987305
Epoch 1430, val loss: 1.1632276773452759
Epoch 1440, training loss: 62.34910583496094 = 0.1026017814874649 + 10.0 * 6.2246503829956055
Epoch 1440, val loss: 1.1678177118301392
Epoch 1450, training loss: 62.33423614501953 = 0.10010232776403427 + 10.0 * 6.223413467407227
Epoch 1450, val loss: 1.1725666522979736
Epoch 1460, training loss: 62.35834503173828 = 0.09767266362905502 + 10.0 * 6.226067543029785
Epoch 1460, val loss: 1.1770901679992676
Epoch 1470, training loss: 62.346160888671875 = 0.09532244503498077 + 10.0 * 6.225083827972412
Epoch 1470, val loss: 1.181780219078064
Epoch 1480, training loss: 62.32147216796875 = 0.09303580224514008 + 10.0 * 6.222843647003174
Epoch 1480, val loss: 1.1865026950836182
Epoch 1490, training loss: 62.3076286315918 = 0.09082619845867157 + 10.0 * 6.221680164337158
Epoch 1490, val loss: 1.1913353204727173
Epoch 1500, training loss: 62.308834075927734 = 0.08868976682424545 + 10.0 * 6.222014427185059
Epoch 1500, val loss: 1.19605553150177
Epoch 1510, training loss: 62.31169128417969 = 0.08661762624979019 + 10.0 * 6.222507476806641
Epoch 1510, val loss: 1.200844407081604
Epoch 1520, training loss: 62.301841735839844 = 0.08459439873695374 + 10.0 * 6.221724510192871
Epoch 1520, val loss: 1.205743432044983
Epoch 1530, training loss: 62.333740234375 = 0.08262941986322403 + 10.0 * 6.22511100769043
Epoch 1530, val loss: 1.2105498313903809
Epoch 1540, training loss: 62.312625885009766 = 0.08072708547115326 + 10.0 * 6.223189830780029
Epoch 1540, val loss: 1.2154085636138916
Epoch 1550, training loss: 62.31194305419922 = 0.07885244488716125 + 10.0 * 6.22330904006958
Epoch 1550, val loss: 1.2200208902359009
Epoch 1560, training loss: 62.31316375732422 = 0.0770563930273056 + 10.0 * 6.223610877990723
Epoch 1560, val loss: 1.224997639656067
Epoch 1570, training loss: 62.26649856567383 = 0.07530653476715088 + 10.0 * 6.219119071960449
Epoch 1570, val loss: 1.2299047708511353
Epoch 1580, training loss: 62.26103210449219 = 0.07362616062164307 + 10.0 * 6.218740463256836
Epoch 1580, val loss: 1.2347240447998047
Epoch 1590, training loss: 62.25690460205078 = 0.07199512422084808 + 10.0 * 6.218491077423096
Epoch 1590, val loss: 1.2396650314331055
Epoch 1600, training loss: 62.257049560546875 = 0.07041899114847183 + 10.0 * 6.218663215637207
Epoch 1600, val loss: 1.2445920705795288
Epoch 1610, training loss: 62.317203521728516 = 0.06889054924249649 + 10.0 * 6.224831581115723
Epoch 1610, val loss: 1.2493705749511719
Epoch 1620, training loss: 62.26193618774414 = 0.0673595443367958 + 10.0 * 6.219457626342773
Epoch 1620, val loss: 1.254539132118225
Epoch 1630, training loss: 62.27910614013672 = 0.06591007858514786 + 10.0 * 6.221319675445557
Epoch 1630, val loss: 1.2594945430755615
Epoch 1640, training loss: 62.24955368041992 = 0.06446053832769394 + 10.0 * 6.218509197235107
Epoch 1640, val loss: 1.2641677856445312
Epoch 1650, training loss: 62.252017974853516 = 0.06306334584951401 + 10.0 * 6.218895435333252
Epoch 1650, val loss: 1.2689883708953857
Epoch 1660, training loss: 62.25469970703125 = 0.061736419796943665 + 10.0 * 6.219296455383301
Epoch 1660, val loss: 1.2741085290908813
Epoch 1670, training loss: 62.237728118896484 = 0.060427118092775345 + 10.0 * 6.2177300453186035
Epoch 1670, val loss: 1.278897762298584
Epoch 1680, training loss: 62.250850677490234 = 0.059159137308597565 + 10.0 * 6.2191691398620605
Epoch 1680, val loss: 1.2837904691696167
Epoch 1690, training loss: 62.228858947753906 = 0.05792035162448883 + 10.0 * 6.2170939445495605
Epoch 1690, val loss: 1.2885746955871582
Epoch 1700, training loss: 62.226165771484375 = 0.05671865865588188 + 10.0 * 6.216944694519043
Epoch 1700, val loss: 1.2935059070587158
Epoch 1710, training loss: 62.260902404785156 = 0.05554423853754997 + 10.0 * 6.220535755157471
Epoch 1710, val loss: 1.298253059387207
Epoch 1720, training loss: 62.23047637939453 = 0.05441707372665405 + 10.0 * 6.217606067657471
Epoch 1720, val loss: 1.3031866550445557
Epoch 1730, training loss: 62.24959945678711 = 0.05330510810017586 + 10.0 * 6.219629287719727
Epoch 1730, val loss: 1.3078844547271729
Epoch 1740, training loss: 62.20650863647461 = 0.052225735038518906 + 10.0 * 6.215428352355957
Epoch 1740, val loss: 1.3128623962402344
Epoch 1750, training loss: 62.19877624511719 = 0.05117715150117874 + 10.0 * 6.214759826660156
Epoch 1750, val loss: 1.317794919013977
Epoch 1760, training loss: 62.20852279663086 = 0.050173740833997726 + 10.0 * 6.215834617614746
Epoch 1760, val loss: 1.3227109909057617
Epoch 1770, training loss: 62.23697280883789 = 0.04918511584401131 + 10.0 * 6.218778610229492
Epoch 1770, val loss: 1.3275115489959717
Epoch 1780, training loss: 62.205413818359375 = 0.048195015639066696 + 10.0 * 6.21572208404541
Epoch 1780, val loss: 1.3321717977523804
Epoch 1790, training loss: 62.184200286865234 = 0.04725334420800209 + 10.0 * 6.2136945724487305
Epoch 1790, val loss: 1.3370440006256104
Epoch 1800, training loss: 62.178680419921875 = 0.04633604362607002 + 10.0 * 6.2132344245910645
Epoch 1800, val loss: 1.3417891263961792
Epoch 1810, training loss: 62.180458068847656 = 0.045453570783138275 + 10.0 * 6.213500499725342
Epoch 1810, val loss: 1.3465415239334106
Epoch 1820, training loss: 62.240638732910156 = 0.04459826648235321 + 10.0 * 6.219604015350342
Epoch 1820, val loss: 1.3510481119155884
Epoch 1830, training loss: 62.243408203125 = 0.04374568909406662 + 10.0 * 6.219965934753418
Epoch 1830, val loss: 1.3559330701828003
Epoch 1840, training loss: 62.18397903442383 = 0.04288289323449135 + 10.0 * 6.214109897613525
Epoch 1840, val loss: 1.3604274988174438
Epoch 1850, training loss: 62.16550064086914 = 0.04208454117178917 + 10.0 * 6.212341785430908
Epoch 1850, val loss: 1.3651869297027588
Epoch 1860, training loss: 62.18012619018555 = 0.04130960628390312 + 10.0 * 6.213881492614746
Epoch 1860, val loss: 1.3697915077209473
Epoch 1870, training loss: 62.17859649658203 = 0.040543753653764725 + 10.0 * 6.213805198669434
Epoch 1870, val loss: 1.374198317527771
Epoch 1880, training loss: 62.17609786987305 = 0.03980495035648346 + 10.0 * 6.213629245758057
Epoch 1880, val loss: 1.3790721893310547
Epoch 1890, training loss: 62.19031524658203 = 0.039075739681720734 + 10.0 * 6.215124130249023
Epoch 1890, val loss: 1.383410930633545
Epoch 1900, training loss: 62.160362243652344 = 0.0383647158741951 + 10.0 * 6.212199687957764
Epoch 1900, val loss: 1.3879936933517456
Epoch 1910, training loss: 62.18328857421875 = 0.037673041224479675 + 10.0 * 6.214561462402344
Epoch 1910, val loss: 1.3923368453979492
Epoch 1920, training loss: 62.14609909057617 = 0.03699951618909836 + 10.0 * 6.210909843444824
Epoch 1920, val loss: 1.3969236612319946
Epoch 1930, training loss: 62.14601135253906 = 0.03635071590542793 + 10.0 * 6.210966110229492
Epoch 1930, val loss: 1.4014531373977661
Epoch 1940, training loss: 62.16558837890625 = 0.03572066128253937 + 10.0 * 6.212986946105957
Epoch 1940, val loss: 1.405819296836853
Epoch 1950, training loss: 62.164459228515625 = 0.035087861120700836 + 10.0 * 6.212937355041504
Epoch 1950, val loss: 1.410090684890747
Epoch 1960, training loss: 62.14537811279297 = 0.03446198254823685 + 10.0 * 6.2110915184021
Epoch 1960, val loss: 1.414427638053894
Epoch 1970, training loss: 62.13856506347656 = 0.033876851201057434 + 10.0 * 6.210468769073486
Epoch 1970, val loss: 1.4188871383666992
Epoch 1980, training loss: 62.18477249145508 = 0.033305272459983826 + 10.0 * 6.215146541595459
Epoch 1980, val loss: 1.4231535196304321
Epoch 1990, training loss: 62.141273498535156 = 0.03272949904203415 + 10.0 * 6.210854530334473
Epoch 1990, val loss: 1.427207589149475
Epoch 2000, training loss: 62.139339447021484 = 0.032172344624996185 + 10.0 * 6.210716724395752
Epoch 2000, val loss: 1.4315307140350342
Epoch 2010, training loss: 62.14503479003906 = 0.0316341370344162 + 10.0 * 6.211339950561523
Epoch 2010, val loss: 1.4356262683868408
Epoch 2020, training loss: 62.1510009765625 = 0.03109864890575409 + 10.0 * 6.2119903564453125
Epoch 2020, val loss: 1.4397003650665283
Epoch 2030, training loss: 62.12990951538086 = 0.030590424314141273 + 10.0 * 6.20993185043335
Epoch 2030, val loss: 1.444153904914856
Epoch 2040, training loss: 62.11408996582031 = 0.030088340863585472 + 10.0 * 6.208400249481201
Epoch 2040, val loss: 1.4482219219207764
Epoch 2050, training loss: 62.119544982910156 = 0.029603397473692894 + 10.0 * 6.208994388580322
Epoch 2050, val loss: 1.4522641897201538
Epoch 2060, training loss: 62.16576385498047 = 0.029130492359399796 + 10.0 * 6.213663578033447
Epoch 2060, val loss: 1.4561588764190674
Epoch 2070, training loss: 62.1361198425293 = 0.02865508571267128 + 10.0 * 6.2107462882995605
Epoch 2070, val loss: 1.4605427980422974
Epoch 2080, training loss: 62.13795471191406 = 0.02819332480430603 + 10.0 * 6.2109761238098145
Epoch 2080, val loss: 1.4642646312713623
Epoch 2090, training loss: 62.13151931762695 = 0.02775292657315731 + 10.0 * 6.210376739501953
Epoch 2090, val loss: 1.4684696197509766
Epoch 2100, training loss: 62.13138198852539 = 0.027311043813824654 + 10.0 * 6.210407257080078
Epoch 2100, val loss: 1.4722778797149658
Epoch 2110, training loss: 62.10625076293945 = 0.026874514296650887 + 10.0 * 6.207937717437744
Epoch 2110, val loss: 1.4762519598007202
Epoch 2120, training loss: 62.09741973876953 = 0.026460014283657074 + 10.0 * 6.207096099853516
Epoch 2120, val loss: 1.4801448583602905
Epoch 2130, training loss: 62.10830307006836 = 0.02605482190847397 + 10.0 * 6.208224773406982
Epoch 2130, val loss: 1.4839541912078857
Epoch 2140, training loss: 62.14008712768555 = 0.025655945762991905 + 10.0 * 6.211442947387695
Epoch 2140, val loss: 1.4878308773040771
Epoch 2150, training loss: 62.11484146118164 = 0.02527119219303131 + 10.0 * 6.208956718444824
Epoch 2150, val loss: 1.4919116497039795
Epoch 2160, training loss: 62.102500915527344 = 0.024884412065148354 + 10.0 * 6.207761764526367
Epoch 2160, val loss: 1.4955146312713623
Epoch 2170, training loss: 62.11948776245117 = 0.02451615408062935 + 10.0 * 6.209496974945068
Epoch 2170, val loss: 1.499495506286621
Epoch 2180, training loss: 62.096988677978516 = 0.02414253167808056 + 10.0 * 6.207284450531006
Epoch 2180, val loss: 1.5030757188796997
Epoch 2190, training loss: 62.093692779541016 = 0.023784637451171875 + 10.0 * 6.206990718841553
Epoch 2190, val loss: 1.5069153308868408
Epoch 2200, training loss: 62.08198165893555 = 0.023440061137080193 + 10.0 * 6.2058539390563965
Epoch 2200, val loss: 1.5108181238174438
Epoch 2210, training loss: 62.0845832824707 = 0.023103024810552597 + 10.0 * 6.206148147583008
Epoch 2210, val loss: 1.5144400596618652
Epoch 2220, training loss: 62.13473129272461 = 0.02277621626853943 + 10.0 * 6.211195468902588
Epoch 2220, val loss: 1.51799738407135
Epoch 2230, training loss: 62.102020263671875 = 0.022435570135712624 + 10.0 * 6.207958698272705
Epoch 2230, val loss: 1.5217797756195068
Epoch 2240, training loss: 62.0823974609375 = 0.022115757688879967 + 10.0 * 6.206027984619141
Epoch 2240, val loss: 1.5254594087600708
Epoch 2250, training loss: 62.08549118041992 = 0.021801188588142395 + 10.0 * 6.206368923187256
Epoch 2250, val loss: 1.529112696647644
Epoch 2260, training loss: 62.11326599121094 = 0.02149621956050396 + 10.0 * 6.209177017211914
Epoch 2260, val loss: 1.5326828956604004
Epoch 2270, training loss: 62.094886779785156 = 0.021184159442782402 + 10.0 * 6.207370281219482
Epoch 2270, val loss: 1.536201000213623
Epoch 2280, training loss: 62.07526397705078 = 0.020891603082418442 + 10.0 * 6.205437183380127
Epoch 2280, val loss: 1.5397717952728271
Epoch 2290, training loss: 62.063682556152344 = 0.020605705678462982 + 10.0 * 6.204307556152344
Epoch 2290, val loss: 1.5435459613800049
Epoch 2300, training loss: 62.06575393676758 = 0.020328320562839508 + 10.0 * 6.204542636871338
Epoch 2300, val loss: 1.5471482276916504
Epoch 2310, training loss: 62.09849166870117 = 0.02005857415497303 + 10.0 * 6.20784330368042
Epoch 2310, val loss: 1.5505207777023315
Epoch 2320, training loss: 62.084495544433594 = 0.019782209768891335 + 10.0 * 6.2064714431762695
Epoch 2320, val loss: 1.553941249847412
Epoch 2330, training loss: 62.063560485839844 = 0.01950772851705551 + 10.0 * 6.204405307769775
Epoch 2330, val loss: 1.5574373006820679
Epoch 2340, training loss: 62.079227447509766 = 0.01924649439752102 + 10.0 * 6.205998420715332
Epoch 2340, val loss: 1.5607712268829346
Epoch 2350, training loss: 62.055294036865234 = 0.018988175317645073 + 10.0 * 6.203630447387695
Epoch 2350, val loss: 1.5642329454421997
Epoch 2360, training loss: 62.05500030517578 = 0.018739350140094757 + 10.0 * 6.2036261558532715
Epoch 2360, val loss: 1.5676716566085815
Epoch 2370, training loss: 62.06875991821289 = 0.01849665865302086 + 10.0 * 6.205026149749756
Epoch 2370, val loss: 1.5709882974624634
Epoch 2380, training loss: 62.05632019042969 = 0.018257640302181244 + 10.0 * 6.203806400299072
Epoch 2380, val loss: 1.5743324756622314
Epoch 2390, training loss: 62.07373046875 = 0.018025318160653114 + 10.0 * 6.205570697784424
Epoch 2390, val loss: 1.5777628421783447
Epoch 2400, training loss: 62.0521125793457 = 0.017791546881198883 + 10.0 * 6.203432083129883
Epoch 2400, val loss: 1.5811059474945068
Epoch 2410, training loss: 62.07997512817383 = 0.017568396404385567 + 10.0 * 6.206240653991699
Epoch 2410, val loss: 1.5844817161560059
Epoch 2420, training loss: 62.093414306640625 = 0.017346011474728584 + 10.0 * 6.207606792449951
Epoch 2420, val loss: 1.5876365900039673
Epoch 2430, training loss: 62.04252624511719 = 0.017114834859967232 + 10.0 * 6.202540874481201
Epoch 2430, val loss: 1.5904853343963623
Epoch 2440, training loss: 62.038326263427734 = 0.016900209710001945 + 10.0 * 6.202142715454102
Epoch 2440, val loss: 1.5939522981643677
Epoch 2450, training loss: 62.034629821777344 = 0.016692467033863068 + 10.0 * 6.201793670654297
Epoch 2450, val loss: 1.597296118736267
Epoch 2460, training loss: 62.078983306884766 = 0.016491366550326347 + 10.0 * 6.206249237060547
Epoch 2460, val loss: 1.600370168685913
Epoch 2470, training loss: 62.03335189819336 = 0.016283821314573288 + 10.0 * 6.201706886291504
Epoch 2470, val loss: 1.6032487154006958
Epoch 2480, training loss: 62.03853988647461 = 0.016084160655736923 + 10.0 * 6.202245712280273
Epoch 2480, val loss: 1.6066267490386963
Epoch 2490, training loss: 62.055335998535156 = 0.015891943126916885 + 10.0 * 6.203944206237793
Epoch 2490, val loss: 1.609864354133606
Epoch 2500, training loss: 62.052860260009766 = 0.015700845047831535 + 10.0 * 6.203715801239014
Epoch 2500, val loss: 1.612831950187683
Epoch 2510, training loss: 62.039215087890625 = 0.015508942306041718 + 10.0 * 6.202370643615723
Epoch 2510, val loss: 1.6159279346466064
Epoch 2520, training loss: 62.02568435668945 = 0.015322343446314335 + 10.0 * 6.201035976409912
Epoch 2520, val loss: 1.618820071220398
Epoch 2530, training loss: 62.052024841308594 = 0.015145240351557732 + 10.0 * 6.203688144683838
Epoch 2530, val loss: 1.6218070983886719
Epoch 2540, training loss: 62.04851150512695 = 0.014967448078095913 + 10.0 * 6.203354358673096
Epoch 2540, val loss: 1.6249058246612549
Epoch 2550, training loss: 62.05074691772461 = 0.014791350811719894 + 10.0 * 6.2035956382751465
Epoch 2550, val loss: 1.6277326345443726
Epoch 2560, training loss: 62.031288146972656 = 0.014615325257182121 + 10.0 * 6.201667308807373
Epoch 2560, val loss: 1.6306536197662354
Epoch 2570, training loss: 62.036231994628906 = 0.01444525271654129 + 10.0 * 6.202178478240967
Epoch 2570, val loss: 1.633655309677124
Epoch 2580, training loss: 62.03104782104492 = 0.014279612340033054 + 10.0 * 6.201676845550537
Epoch 2580, val loss: 1.6365113258361816
Epoch 2590, training loss: 62.028438568115234 = 0.01411779597401619 + 10.0 * 6.201432228088379
Epoch 2590, val loss: 1.639328956604004
Epoch 2600, training loss: 62.03955841064453 = 0.013960327953100204 + 10.0 * 6.202559471130371
Epoch 2600, val loss: 1.6422661542892456
Epoch 2610, training loss: 62.02241516113281 = 0.01380297914147377 + 10.0 * 6.20086145401001
Epoch 2610, val loss: 1.645161509513855
Epoch 2620, training loss: 62.011207580566406 = 0.013647033832967281 + 10.0 * 6.199756145477295
Epoch 2620, val loss: 1.6481950283050537
Epoch 2630, training loss: 62.01262664794922 = 0.01349644735455513 + 10.0 * 6.199913024902344
Epoch 2630, val loss: 1.650964617729187
Epoch 2640, training loss: 62.07884216308594 = 0.013356002978980541 + 10.0 * 6.206548690795898
Epoch 2640, val loss: 1.6536877155303955
Epoch 2650, training loss: 62.05546951293945 = 0.013198492117226124 + 10.0 * 6.204226970672607
Epoch 2650, val loss: 1.6555719375610352
Epoch 2660, training loss: 62.02046203613281 = 0.013048041611909866 + 10.0 * 6.200741767883301
Epoch 2660, val loss: 1.6588953733444214
Epoch 2670, training loss: 62.006160736083984 = 0.01290571317076683 + 10.0 * 6.1993255615234375
Epoch 2670, val loss: 1.661705493927002
Epoch 2680, training loss: 62.0534782409668 = 0.012768610380589962 + 10.0 * 6.204071044921875
Epoch 2680, val loss: 1.663977026939392
Epoch 2690, training loss: 62.00193405151367 = 0.012631826102733612 + 10.0 * 6.198930263519287
Epoch 2690, val loss: 1.6666756868362427
Epoch 2700, training loss: 62.0145263671875 = 0.012496765702962875 + 10.0 * 6.200202941894531
Epoch 2700, val loss: 1.6693493127822876
Epoch 2710, training loss: 62.01319122314453 = 0.012361802160739899 + 10.0 * 6.200082778930664
Epoch 2710, val loss: 1.6718347072601318
Epoch 2720, training loss: 62.0081672668457 = 0.012233265675604343 + 10.0 * 6.199593544006348
Epoch 2720, val loss: 1.6746935844421387
Epoch 2730, training loss: 62.0139274597168 = 0.012105085887014866 + 10.0 * 6.2001824378967285
Epoch 2730, val loss: 1.6770662069320679
Epoch 2740, training loss: 62.003299713134766 = 0.011980955488979816 + 10.0 * 6.199131965637207
Epoch 2740, val loss: 1.6797220706939697
Epoch 2750, training loss: 62.01486587524414 = 0.011860400438308716 + 10.0 * 6.200300693511963
Epoch 2750, val loss: 1.682251214981079
Epoch 2760, training loss: 61.996986389160156 = 0.011735498905181885 + 10.0 * 6.198525428771973
Epoch 2760, val loss: 1.6845617294311523
Epoch 2770, training loss: 62.025146484375 = 0.011617688462138176 + 10.0 * 6.201352596282959
Epoch 2770, val loss: 1.6869796514511108
Epoch 2780, training loss: 62.02225112915039 = 0.011500784195959568 + 10.0 * 6.201075077056885
Epoch 2780, val loss: 1.6895229816436768
Epoch 2790, training loss: 61.99647903442383 = 0.01137749757617712 + 10.0 * 6.19851016998291
Epoch 2790, val loss: 1.6918048858642578
Epoch 2800, training loss: 62.0120849609375 = 0.011267222464084625 + 10.0 * 6.200081825256348
Epoch 2800, val loss: 1.694443941116333
Epoch 2810, training loss: 62.025001525878906 = 0.011152271181344986 + 10.0 * 6.201384544372559
Epoch 2810, val loss: 1.6964854001998901
Epoch 2820, training loss: 61.98857879638672 = 0.011038453318178654 + 10.0 * 6.19775390625
Epoch 2820, val loss: 1.699081301689148
Epoch 2830, training loss: 61.984840393066406 = 0.010928383097052574 + 10.0 * 6.197391033172607
Epoch 2830, val loss: 1.7015581130981445
Epoch 2840, training loss: 61.98100280761719 = 0.010822860524058342 + 10.0 * 6.197018146514893
Epoch 2840, val loss: 1.7040324211120605
Epoch 2850, training loss: 62.015113830566406 = 0.010719827376306057 + 10.0 * 6.200439453125
Epoch 2850, val loss: 1.7059943675994873
Epoch 2860, training loss: 61.99169158935547 = 0.010612141340970993 + 10.0 * 6.198107719421387
Epoch 2860, val loss: 1.7083375453948975
Epoch 2870, training loss: 61.9898796081543 = 0.01050960086286068 + 10.0 * 6.19793701171875
Epoch 2870, val loss: 1.7106053829193115
Epoch 2880, training loss: 62.001556396484375 = 0.010408061556518078 + 10.0 * 6.199114799499512
Epoch 2880, val loss: 1.7128273248672485
Epoch 2890, training loss: 61.978057861328125 = 0.010308879427611828 + 10.0 * 6.196774959564209
Epoch 2890, val loss: 1.7150870561599731
Epoch 2900, training loss: 61.98466873168945 = 0.010213383473455906 + 10.0 * 6.197445869445801
Epoch 2900, val loss: 1.7173938751220703
Epoch 2910, training loss: 61.99717712402344 = 0.010118934325873852 + 10.0 * 6.198705673217773
Epoch 2910, val loss: 1.719468593597412
Epoch 2920, training loss: 61.98563766479492 = 0.010023334063589573 + 10.0 * 6.197561264038086
Epoch 2920, val loss: 1.7218378782272339
Epoch 2930, training loss: 61.99517822265625 = 0.00993055198341608 + 10.0 * 6.1985249519348145
Epoch 2930, val loss: 1.7239770889282227
Epoch 2940, training loss: 62.017574310302734 = 0.009837910532951355 + 10.0 * 6.2007737159729
Epoch 2940, val loss: 1.7260620594024658
Epoch 2950, training loss: 61.98024368286133 = 0.0097455233335495 + 10.0 * 6.197049617767334
Epoch 2950, val loss: 1.7280657291412354
Epoch 2960, training loss: 61.98666000366211 = 0.009656860493123531 + 10.0 * 6.197700500488281
Epoch 2960, val loss: 1.730301856994629
Epoch 2970, training loss: 61.9924430847168 = 0.009568015113472939 + 10.0 * 6.198287487030029
Epoch 2970, val loss: 1.732240080833435
Epoch 2980, training loss: 61.96844482421875 = 0.00948045402765274 + 10.0 * 6.195896625518799
Epoch 2980, val loss: 1.734371542930603
Epoch 2990, training loss: 61.96621322631836 = 0.009396565146744251 + 10.0 * 6.195681571960449
Epoch 2990, val loss: 1.7363802194595337
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.662962962962963
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 87.91558837890625 = 1.9475241899490356 + 10.0 * 8.596806526184082
Epoch 0, val loss: 1.9404405355453491
Epoch 10, training loss: 87.89588165283203 = 1.9371552467346191 + 10.0 * 8.59587287902832
Epoch 10, val loss: 1.9303977489471436
Epoch 20, training loss: 87.81101989746094 = 1.9244564771652222 + 10.0 * 8.588656425476074
Epoch 20, val loss: 1.9177942276000977
Epoch 30, training loss: 87.26898193359375 = 1.9084813594818115 + 10.0 * 8.536050796508789
Epoch 30, val loss: 1.9019354581832886
Epoch 40, training loss: 83.34915161132812 = 1.8899939060211182 + 10.0 * 8.145915985107422
Epoch 40, val loss: 1.884247064590454
Epoch 50, training loss: 76.19248962402344 = 1.8716689348220825 + 10.0 * 7.432082653045654
Epoch 50, val loss: 1.8681128025054932
Epoch 60, training loss: 73.49835205078125 = 1.8587806224822998 + 10.0 * 7.163957118988037
Epoch 60, val loss: 1.8561934232711792
Epoch 70, training loss: 71.67262268066406 = 1.846389651298523 + 10.0 * 6.982623100280762
Epoch 70, val loss: 1.8447926044464111
Epoch 80, training loss: 70.48921966552734 = 1.8353776931762695 + 10.0 * 6.865384101867676
Epoch 80, val loss: 1.8342115879058838
Epoch 90, training loss: 69.6809310913086 = 1.8254073858261108 + 10.0 * 6.785552024841309
Epoch 90, val loss: 1.8247445821762085
Epoch 100, training loss: 69.15791320800781 = 1.8163862228393555 + 10.0 * 6.734152793884277
Epoch 100, val loss: 1.8162906169891357
Epoch 110, training loss: 68.55896759033203 = 1.807962417602539 + 10.0 * 6.675100326538086
Epoch 110, val loss: 1.8086172342300415
Epoch 120, training loss: 67.98670196533203 = 1.8004918098449707 + 10.0 * 6.618621349334717
Epoch 120, val loss: 1.8018395900726318
Epoch 130, training loss: 67.55966186523438 = 1.7936455011367798 + 10.0 * 6.576601505279541
Epoch 130, val loss: 1.7954943180084229
Epoch 140, training loss: 67.2502212524414 = 1.786609172821045 + 10.0 * 6.546361446380615
Epoch 140, val loss: 1.7890541553497314
Epoch 150, training loss: 66.94159698486328 = 1.779451847076416 + 10.0 * 6.516214847564697
Epoch 150, val loss: 1.7824729681015015
Epoch 160, training loss: 66.6811294555664 = 1.7722575664520264 + 10.0 * 6.49088716506958
Epoch 160, val loss: 1.7759497165679932
Epoch 170, training loss: 66.51317596435547 = 1.7646842002868652 + 10.0 * 6.474849224090576
Epoch 170, val loss: 1.7691963911056519
Epoch 180, training loss: 66.29646301269531 = 1.7562731504440308 + 10.0 * 6.454019069671631
Epoch 180, val loss: 1.761932611465454
Epoch 190, training loss: 66.14259338378906 = 1.7473074197769165 + 10.0 * 6.439528942108154
Epoch 190, val loss: 1.7541449069976807
Epoch 200, training loss: 65.99754333496094 = 1.7375887632369995 + 10.0 * 6.425995826721191
Epoch 200, val loss: 1.7458597421646118
Epoch 210, training loss: 65.92252349853516 = 1.7270771265029907 + 10.0 * 6.4195451736450195
Epoch 210, val loss: 1.7368628978729248
Epoch 220, training loss: 65.79177856445312 = 1.715297818183899 + 10.0 * 6.407647609710693
Epoch 220, val loss: 1.7269445657730103
Epoch 230, training loss: 65.66769409179688 = 1.7026249170303345 + 10.0 * 6.396507263183594
Epoch 230, val loss: 1.716298222541809
Epoch 240, training loss: 65.57128143310547 = 1.688894271850586 + 10.0 * 6.388238430023193
Epoch 240, val loss: 1.7046869993209839
Epoch 250, training loss: 65.48490905761719 = 1.6739683151245117 + 10.0 * 6.381094455718994
Epoch 250, val loss: 1.692173719406128
Epoch 260, training loss: 65.43426513671875 = 1.6578679084777832 + 10.0 * 6.3776397705078125
Epoch 260, val loss: 1.6786452531814575
Epoch 270, training loss: 65.36610412597656 = 1.640252947807312 + 10.0 * 6.372585296630859
Epoch 270, val loss: 1.6639894247055054
Epoch 280, training loss: 65.26600646972656 = 1.6214535236358643 + 10.0 * 6.364455223083496
Epoch 280, val loss: 1.6483033895492554
Epoch 290, training loss: 65.18724060058594 = 1.601509928703308 + 10.0 * 6.358572959899902
Epoch 290, val loss: 1.6317485570907593
Epoch 300, training loss: 65.11775970458984 = 1.5803587436676025 + 10.0 * 6.3537397384643555
Epoch 300, val loss: 1.6142915487289429
Epoch 310, training loss: 65.05938720703125 = 1.5580110549926758 + 10.0 * 6.350137233734131
Epoch 310, val loss: 1.5960437059402466
Epoch 320, training loss: 65.00369262695312 = 1.534698724746704 + 10.0 * 6.346899509429932
Epoch 320, val loss: 1.5767810344696045
Epoch 330, training loss: 64.92353820800781 = 1.5103487968444824 + 10.0 * 6.3413190841674805
Epoch 330, val loss: 1.5571907758712769
Epoch 340, training loss: 64.86177825927734 = 1.4852499961853027 + 10.0 * 6.337652683258057
Epoch 340, val loss: 1.5371284484863281
Epoch 350, training loss: 64.80847930908203 = 1.459505319595337 + 10.0 * 6.334897518157959
Epoch 350, val loss: 1.5167524814605713
Epoch 360, training loss: 64.7689437866211 = 1.433255910873413 + 10.0 * 6.333568572998047
Epoch 360, val loss: 1.4960665702819824
Epoch 370, training loss: 64.6883773803711 = 1.406564474105835 + 10.0 * 6.328181266784668
Epoch 370, val loss: 1.4754680395126343
Epoch 380, training loss: 64.65438079833984 = 1.3796114921569824 + 10.0 * 6.327476978302002
Epoch 380, val loss: 1.4550819396972656
Epoch 390, training loss: 64.58659362792969 = 1.3526333570480347 + 10.0 * 6.3233962059021
Epoch 390, val loss: 1.4350082874298096
Epoch 400, training loss: 64.51805114746094 = 1.3256735801696777 + 10.0 * 6.31923770904541
Epoch 400, val loss: 1.4152666330337524
Epoch 410, training loss: 64.45658874511719 = 1.2988747358322144 + 10.0 * 6.315771579742432
Epoch 410, val loss: 1.3959180116653442
Epoch 420, training loss: 64.40992736816406 = 1.2721678018569946 + 10.0 * 6.313775539398193
Epoch 420, val loss: 1.3770604133605957
Epoch 430, training loss: 64.41236114501953 = 1.2454439401626587 + 10.0 * 6.316691875457764
Epoch 430, val loss: 1.35837984085083
Epoch 440, training loss: 64.32472229003906 = 1.2192552089691162 + 10.0 * 6.310546875
Epoch 440, val loss: 1.340590238571167
Epoch 450, training loss: 64.2565689086914 = 1.193508267402649 + 10.0 * 6.306305885314941
Epoch 450, val loss: 1.3233489990234375
Epoch 460, training loss: 64.20074462890625 = 1.1680960655212402 + 10.0 * 6.30326509475708
Epoch 460, val loss: 1.3066271543502808
Epoch 470, training loss: 64.15328216552734 = 1.1429312229156494 + 10.0 * 6.301034927368164
Epoch 470, val loss: 1.2904235124588013
Epoch 480, training loss: 64.1485595703125 = 1.1179734468460083 + 10.0 * 6.303058624267578
Epoch 480, val loss: 1.2746593952178955
Epoch 490, training loss: 64.0802230834961 = 1.0933763980865479 + 10.0 * 6.298684597015381
Epoch 490, val loss: 1.2593406438827515
Epoch 500, training loss: 64.02517700195312 = 1.0691128969192505 + 10.0 * 6.2956061363220215
Epoch 500, val loss: 1.2445931434631348
Epoch 510, training loss: 63.9954833984375 = 1.0451819896697998 + 10.0 * 6.295030117034912
Epoch 510, val loss: 1.2303705215454102
Epoch 520, training loss: 63.952091217041016 = 1.0216025114059448 + 10.0 * 6.293048858642578
Epoch 520, val loss: 1.2164075374603271
Epoch 530, training loss: 63.9072265625 = 0.9982901215553284 + 10.0 * 6.2908935546875
Epoch 530, val loss: 1.20294189453125
Epoch 540, training loss: 63.860416412353516 = 0.9754593372344971 + 10.0 * 6.2884955406188965
Epoch 540, val loss: 1.189943552017212
Epoch 550, training loss: 63.87643051147461 = 0.9530312418937683 + 10.0 * 6.292340278625488
Epoch 550, val loss: 1.1773104667663574
Epoch 560, training loss: 63.81944274902344 = 0.9305289387702942 + 10.0 * 6.288891315460205
Epoch 560, val loss: 1.1652023792266846
Epoch 570, training loss: 63.750675201416016 = 0.908733606338501 + 10.0 * 6.284193992614746
Epoch 570, val loss: 1.1534239053726196
Epoch 580, training loss: 63.7136116027832 = 0.8871657252311707 + 10.0 * 6.282644748687744
Epoch 580, val loss: 1.1422199010849
Epoch 590, training loss: 63.70045471191406 = 0.8660580515861511 + 10.0 * 6.283439636230469
Epoch 590, val loss: 1.1314184665679932
Epoch 600, training loss: 63.6566276550293 = 0.8452803492546082 + 10.0 * 6.281134605407715
Epoch 600, val loss: 1.1211661100387573
Epoch 610, training loss: 63.62111282348633 = 0.824809193611145 + 10.0 * 6.279630184173584
Epoch 610, val loss: 1.1110963821411133
Epoch 620, training loss: 63.57477951049805 = 0.8048740029335022 + 10.0 * 6.2769904136657715
Epoch 620, val loss: 1.1017175912857056
Epoch 630, training loss: 63.54636001586914 = 0.7853447794914246 + 10.0 * 6.276101589202881
Epoch 630, val loss: 1.0927202701568604
Epoch 640, training loss: 63.540287017822266 = 0.7662023305892944 + 10.0 * 6.277408599853516
Epoch 640, val loss: 1.0842506885528564
Epoch 650, training loss: 63.489566802978516 = 0.7474522590637207 + 10.0 * 6.274211406707764
Epoch 650, val loss: 1.0762397050857544
Epoch 660, training loss: 63.451393127441406 = 0.7291319370269775 + 10.0 * 6.272225856781006
Epoch 660, val loss: 1.0688371658325195
Epoch 670, training loss: 63.41783142089844 = 0.7114484310150146 + 10.0 * 6.270638465881348
Epoch 670, val loss: 1.0619373321533203
Epoch 680, training loss: 63.45539474487305 = 0.6941044926643372 + 10.0 * 6.276129245758057
Epoch 680, val loss: 1.0554990768432617
Epoch 690, training loss: 63.37548828125 = 0.6771572828292847 + 10.0 * 6.269833087921143
Epoch 690, val loss: 1.049656867980957
Epoch 700, training loss: 63.34720993041992 = 0.6605895161628723 + 10.0 * 6.268661975860596
Epoch 700, val loss: 1.0441542863845825
Epoch 710, training loss: 63.3341178894043 = 0.6444844603538513 + 10.0 * 6.26896333694458
Epoch 710, val loss: 1.0390878915786743
Epoch 720, training loss: 63.30656433105469 = 0.6287531852722168 + 10.0 * 6.2677812576293945
Epoch 720, val loss: 1.0345293283462524
Epoch 730, training loss: 63.26731872558594 = 0.6134341955184937 + 10.0 * 6.265388488769531
Epoch 730, val loss: 1.0303337574005127
Epoch 740, training loss: 63.29492950439453 = 0.5984157919883728 + 10.0 * 6.269651412963867
Epoch 740, val loss: 1.026548147201538
Epoch 750, training loss: 63.22712707519531 = 0.5839297771453857 + 10.0 * 6.26431941986084
Epoch 750, val loss: 1.023095965385437
Epoch 760, training loss: 63.1944580078125 = 0.5696244835853577 + 10.0 * 6.2624831199646
Epoch 760, val loss: 1.0201337337493896
Epoch 770, training loss: 63.1836051940918 = 0.5557899475097656 + 10.0 * 6.262781620025635
Epoch 770, val loss: 1.0175856351852417
Epoch 780, training loss: 63.16316604614258 = 0.5421186089515686 + 10.0 * 6.2621049880981445
Epoch 780, val loss: 1.0152543783187866
Epoch 790, training loss: 63.12419891357422 = 0.5289135575294495 + 10.0 * 6.259528160095215
Epoch 790, val loss: 1.0133609771728516
Epoch 800, training loss: 63.098941802978516 = 0.5159720182418823 + 10.0 * 6.258296966552734
Epoch 800, val loss: 1.0117294788360596
Epoch 810, training loss: 63.08692169189453 = 0.5034209489822388 + 10.0 * 6.258349895477295
Epoch 810, val loss: 1.0105935335159302
Epoch 820, training loss: 63.09426498413086 = 0.4910200536251068 + 10.0 * 6.260324478149414
Epoch 820, val loss: 1.009600281715393
Epoch 830, training loss: 63.06917190551758 = 0.47888481616973877 + 10.0 * 6.259028434753418
Epoch 830, val loss: 1.0087816715240479
Epoch 840, training loss: 63.02185821533203 = 0.4670080840587616 + 10.0 * 6.2554850578308105
Epoch 840, val loss: 1.008297085762024
Epoch 850, training loss: 63.00255584716797 = 0.45552778244018555 + 10.0 * 6.254702568054199
Epoch 850, val loss: 1.0082491636276245
Epoch 860, training loss: 62.99211120605469 = 0.4442662000656128 + 10.0 * 6.25478458404541
Epoch 860, val loss: 1.0083461999893188
Epoch 870, training loss: 62.99203109741211 = 0.4332384467124939 + 10.0 * 6.2558794021606445
Epoch 870, val loss: 1.0086208581924438
Epoch 880, training loss: 62.95729064941406 = 0.4224841892719269 + 10.0 * 6.253480434417725
Epoch 880, val loss: 1.0093406438827515
Epoch 890, training loss: 62.93021774291992 = 0.4119224548339844 + 10.0 * 6.251829624176025
Epoch 890, val loss: 1.0101356506347656
Epoch 900, training loss: 62.914512634277344 = 0.4016393721103668 + 10.0 * 6.251287460327148
Epoch 900, val loss: 1.0111443996429443
Epoch 910, training loss: 62.94926452636719 = 0.39156657457351685 + 10.0 * 6.255769729614258
Epoch 910, val loss: 1.0124990940093994
Epoch 920, training loss: 62.93407440185547 = 0.38156938552856445 + 10.0 * 6.255250453948975
Epoch 920, val loss: 1.0135585069656372
Epoch 930, training loss: 62.866798400878906 = 0.37193435430526733 + 10.0 * 6.249486446380615
Epoch 930, val loss: 1.0152292251586914
Epoch 940, training loss: 62.845115661621094 = 0.36257311701774597 + 10.0 * 6.248254299163818
Epoch 940, val loss: 1.0171682834625244
Epoch 950, training loss: 62.8253059387207 = 0.3534049391746521 + 10.0 * 6.247189998626709
Epoch 950, val loss: 1.0193262100219727
Epoch 960, training loss: 62.8221549987793 = 0.34444019198417664 + 10.0 * 6.247771263122559
Epoch 960, val loss: 1.021474003791809
Epoch 970, training loss: 62.83977127075195 = 0.33563297986984253 + 10.0 * 6.25041389465332
Epoch 970, val loss: 1.0237849950790405
Epoch 980, training loss: 62.81169509887695 = 0.327114075422287 + 10.0 * 6.248457908630371
Epoch 980, val loss: 1.0263665914535522
Epoch 990, training loss: 62.766727447509766 = 0.3187372088432312 + 10.0 * 6.2447991371154785
Epoch 990, val loss: 1.0290852785110474
Epoch 1000, training loss: 62.754051208496094 = 0.31061601638793945 + 10.0 * 6.2443437576293945
Epoch 1000, val loss: 1.0319929122924805
Epoch 1010, training loss: 62.73801803588867 = 0.30273008346557617 + 10.0 * 6.243528842926025
Epoch 1010, val loss: 1.0351277589797974
Epoch 1020, training loss: 62.74115753173828 = 0.29501935839653015 + 10.0 * 6.2446136474609375
Epoch 1020, val loss: 1.0383638143539429
Epoch 1030, training loss: 62.71634292602539 = 0.28744563460350037 + 10.0 * 6.242889881134033
Epoch 1030, val loss: 1.0415986776351929
Epoch 1040, training loss: 62.730323791503906 = 0.28006261587142944 + 10.0 * 6.245026111602783
Epoch 1040, val loss: 1.0450050830841064
Epoch 1050, training loss: 62.693885803222656 = 0.27291157841682434 + 10.0 * 6.2420973777771
Epoch 1050, val loss: 1.0486631393432617
Epoch 1060, training loss: 62.682159423828125 = 0.265984445810318 + 10.0 * 6.241617679595947
Epoch 1060, val loss: 1.0525062084197998
Epoch 1070, training loss: 62.72367858886719 = 0.25924843549728394 + 10.0 * 6.246443271636963
Epoch 1070, val loss: 1.0563888549804688
Epoch 1080, training loss: 62.67987060546875 = 0.25254523754119873 + 10.0 * 6.242732524871826
Epoch 1080, val loss: 1.0602178573608398
Epoch 1090, training loss: 62.66111755371094 = 0.2461395561695099 + 10.0 * 6.241497993469238
Epoch 1090, val loss: 1.0645077228546143
Epoch 1100, training loss: 62.6387825012207 = 0.23990149796009064 + 10.0 * 6.2398881912231445
Epoch 1100, val loss: 1.0688246488571167
Epoch 1110, training loss: 62.62508010864258 = 0.23384298384189606 + 10.0 * 6.239123344421387
Epoch 1110, val loss: 1.0732944011688232
Epoch 1120, training loss: 62.63700485229492 = 0.2279128134250641 + 10.0 * 6.240909099578857
Epoch 1120, val loss: 1.0777437686920166
Epoch 1130, training loss: 62.64149475097656 = 0.22213463485240936 + 10.0 * 6.241936206817627
Epoch 1130, val loss: 1.0822350978851318
Epoch 1140, training loss: 62.598445892333984 = 0.2165297418832779 + 10.0 * 6.238191604614258
Epoch 1140, val loss: 1.0872802734375
Epoch 1150, training loss: 62.58760070800781 = 0.21106284856796265 + 10.0 * 6.237653732299805
Epoch 1150, val loss: 1.0919758081436157
Epoch 1160, training loss: 62.618465423583984 = 0.2057604193687439 + 10.0 * 6.241270542144775
Epoch 1160, val loss: 1.0969830751419067
Epoch 1170, training loss: 62.56930160522461 = 0.20058748126029968 + 10.0 * 6.236871242523193
Epoch 1170, val loss: 1.1017738580703735
Epoch 1180, training loss: 62.55580520629883 = 0.1955549418926239 + 10.0 * 6.236024856567383
Epoch 1180, val loss: 1.1068263053894043
Epoch 1190, training loss: 62.60087585449219 = 0.19064396619796753 + 10.0 * 6.241023063659668
Epoch 1190, val loss: 1.1117337942123413
Epoch 1200, training loss: 62.550270080566406 = 0.1859065592288971 + 10.0 * 6.236436367034912
Epoch 1200, val loss: 1.117231845855713
Epoch 1210, training loss: 62.5236930847168 = 0.1812431961297989 + 10.0 * 6.2342448234558105
Epoch 1210, val loss: 1.1223785877227783
Epoch 1220, training loss: 62.52010726928711 = 0.1767619103193283 + 10.0 * 6.234334468841553
Epoch 1220, val loss: 1.1277562379837036
Epoch 1230, training loss: 62.545291900634766 = 0.1723581701517105 + 10.0 * 6.237293243408203
Epoch 1230, val loss: 1.1331511735916138
Epoch 1240, training loss: 62.507591247558594 = 0.16808389127254486 + 10.0 * 6.233950614929199
Epoch 1240, val loss: 1.138535976409912
Epoch 1250, training loss: 62.48299789428711 = 0.1639270931482315 + 10.0 * 6.231906890869141
Epoch 1250, val loss: 1.144100546836853
Epoch 1260, training loss: 62.47861862182617 = 0.15989063680171967 + 10.0 * 6.231873035430908
Epoch 1260, val loss: 1.1495367288589478
Epoch 1270, training loss: 62.528358459472656 = 0.15597502887248993 + 10.0 * 6.23723840713501
Epoch 1270, val loss: 1.1550315618515015
Epoch 1280, training loss: 62.48054122924805 = 0.15209564566612244 + 10.0 * 6.232844352722168
Epoch 1280, val loss: 1.1606812477111816
Epoch 1290, training loss: 62.46859359741211 = 0.1483571082353592 + 10.0 * 6.2320237159729
Epoch 1290, val loss: 1.1662979125976562
Epoch 1300, training loss: 62.45945739746094 = 0.14471673965454102 + 10.0 * 6.231473922729492
Epoch 1300, val loss: 1.1719970703125
Epoch 1310, training loss: 62.45269012451172 = 0.14116623997688293 + 10.0 * 6.231152534484863
Epoch 1310, val loss: 1.1774098873138428
Epoch 1320, training loss: 62.50441360473633 = 0.1377132683992386 + 10.0 * 6.236670017242432
Epoch 1320, val loss: 1.1827518939971924
Epoch 1330, training loss: 62.42953109741211 = 0.1343727558851242 + 10.0 * 6.22951602935791
Epoch 1330, val loss: 1.1887493133544922
Epoch 1340, training loss: 62.41626739501953 = 0.13111303746700287 + 10.0 * 6.228515625
Epoch 1340, val loss: 1.1944630146026611
Epoch 1350, training loss: 62.40788650512695 = 0.1279534101486206 + 10.0 * 6.227993488311768
Epoch 1350, val loss: 1.1999529600143433
Epoch 1360, training loss: 62.41664123535156 = 0.12489296495914459 + 10.0 * 6.229174613952637
Epoch 1360, val loss: 1.205612301826477
Epoch 1370, training loss: 62.41782760620117 = 0.12188003212213516 + 10.0 * 6.229594707489014
Epoch 1370, val loss: 1.2111622095108032
Epoch 1380, training loss: 62.391300201416016 = 0.11897080391645432 + 10.0 * 6.227232933044434
Epoch 1380, val loss: 1.2169157266616821
Epoch 1390, training loss: 62.383304595947266 = 0.11612288653850555 + 10.0 * 6.226717948913574
Epoch 1390, val loss: 1.2226812839508057
Epoch 1400, training loss: 62.40362548828125 = 0.11336156725883484 + 10.0 * 6.2290263175964355
Epoch 1400, val loss: 1.228303074836731
Epoch 1410, training loss: 62.37327194213867 = 0.11066794395446777 + 10.0 * 6.226260185241699
Epoch 1410, val loss: 1.2341704368591309
Epoch 1420, training loss: 62.38899230957031 = 0.10805107653141022 + 10.0 * 6.228094100952148
Epoch 1420, val loss: 1.239643931388855
Epoch 1430, training loss: 62.36958694458008 = 0.10549231618642807 + 10.0 * 6.226409435272217
Epoch 1430, val loss: 1.2456353902816772
Epoch 1440, training loss: 62.37814712524414 = 0.10303298383951187 + 10.0 * 6.227511405944824
Epoch 1440, val loss: 1.2513210773468018
Epoch 1450, training loss: 62.36555099487305 = 0.10061520338058472 + 10.0 * 6.2264933586120605
Epoch 1450, val loss: 1.2568233013153076
Epoch 1460, training loss: 62.35298156738281 = 0.0982755795121193 + 10.0 * 6.225470542907715
Epoch 1460, val loss: 1.262381672859192
Epoch 1470, training loss: 62.3453369140625 = 0.09598561376333237 + 10.0 * 6.224935054779053
Epoch 1470, val loss: 1.2680039405822754
Epoch 1480, training loss: 62.34551239013672 = 0.09377459436655045 + 10.0 * 6.2251739501953125
Epoch 1480, val loss: 1.273512840270996
Epoch 1490, training loss: 62.334739685058594 = 0.09161920845508575 + 10.0 * 6.2243123054504395
Epoch 1490, val loss: 1.2791619300842285
Epoch 1500, training loss: 62.33110809326172 = 0.08952947705984116 + 10.0 * 6.224157810211182
Epoch 1500, val loss: 1.2848118543624878
Epoch 1510, training loss: 62.37971496582031 = 0.08749201148748398 + 10.0 * 6.229222297668457
Epoch 1510, val loss: 1.2904517650604248
Epoch 1520, training loss: 62.31576919555664 = 0.08544840663671494 + 10.0 * 6.223031997680664
Epoch 1520, val loss: 1.2955607175827026
Epoch 1530, training loss: 62.30148696899414 = 0.08351639658212662 + 10.0 * 6.221796989440918
Epoch 1530, val loss: 1.3011908531188965
Epoch 1540, training loss: 62.29454803466797 = 0.08164709806442261 + 10.0 * 6.221290111541748
Epoch 1540, val loss: 1.3067392110824585
Epoch 1550, training loss: 62.303646087646484 = 0.07982760667800903 + 10.0 * 6.222382068634033
Epoch 1550, val loss: 1.3120241165161133
Epoch 1560, training loss: 62.316864013671875 = 0.07804245501756668 + 10.0 * 6.22388219833374
Epoch 1560, val loss: 1.3171547651290894
Epoch 1570, training loss: 62.29629135131836 = 0.07630855590105057 + 10.0 * 6.22199821472168
Epoch 1570, val loss: 1.322769284248352
Epoch 1580, training loss: 62.30894088745117 = 0.07462000101804733 + 10.0 * 6.2234320640563965
Epoch 1580, val loss: 1.3280304670333862
Epoch 1590, training loss: 62.28256607055664 = 0.0729731097817421 + 10.0 * 6.220959663391113
Epoch 1590, val loss: 1.3335418701171875
Epoch 1600, training loss: 62.28181457519531 = 0.07137908041477203 + 10.0 * 6.221043586730957
Epoch 1600, val loss: 1.3388736248016357
Epoch 1610, training loss: 62.268653869628906 = 0.06983156502246857 + 10.0 * 6.219882011413574
Epoch 1610, val loss: 1.3442175388336182
Epoch 1620, training loss: 62.25851058959961 = 0.06832168996334076 + 10.0 * 6.219018936157227
Epoch 1620, val loss: 1.3495713472366333
Epoch 1630, training loss: 62.30895233154297 = 0.06686945259571075 + 10.0 * 6.224208354949951
Epoch 1630, val loss: 1.3550840616226196
Epoch 1640, training loss: 62.354026794433594 = 0.06541857123374939 + 10.0 * 6.228860855102539
Epoch 1640, val loss: 1.3597667217254639
Epoch 1650, training loss: 62.27688980102539 = 0.0639766976237297 + 10.0 * 6.221291542053223
Epoch 1650, val loss: 1.3647960424423218
Epoch 1660, training loss: 62.24546813964844 = 0.06262602657079697 + 10.0 * 6.2182841300964355
Epoch 1660, val loss: 1.3701796531677246
Epoch 1670, training loss: 62.23786926269531 = 0.0613018237054348 + 10.0 * 6.21765661239624
Epoch 1670, val loss: 1.3750396966934204
Epoch 1680, training loss: 62.241477966308594 = 0.060025058686733246 + 10.0 * 6.218145370483398
Epoch 1680, val loss: 1.3800002336502075
Epoch 1690, training loss: 62.28517150878906 = 0.05876864865422249 + 10.0 * 6.222640037536621
Epoch 1690, val loss: 1.3848161697387695
Epoch 1700, training loss: 62.24271774291992 = 0.05754199996590614 + 10.0 * 6.218517780303955
Epoch 1700, val loss: 1.3901917934417725
Epoch 1710, training loss: 62.244667053222656 = 0.056350890547037125 + 10.0 * 6.218831539154053
Epoch 1710, val loss: 1.3950433731079102
Epoch 1720, training loss: 62.224708557128906 = 0.055183012038469315 + 10.0 * 6.216952323913574
Epoch 1720, val loss: 1.3998548984527588
Epoch 1730, training loss: 62.22862243652344 = 0.05405296012759209 + 10.0 * 6.217456817626953
Epoch 1730, val loss: 1.404623031616211
Epoch 1740, training loss: 62.23649597167969 = 0.05295779928565025 + 10.0 * 6.218353748321533
Epoch 1740, val loss: 1.4094516038894653
Epoch 1750, training loss: 62.2471923828125 = 0.05188941955566406 + 10.0 * 6.21953010559082
Epoch 1750, val loss: 1.414500117301941
Epoch 1760, training loss: 62.25108337402344 = 0.05083633214235306 + 10.0 * 6.220025062561035
Epoch 1760, val loss: 1.4188817739486694
Epoch 1770, training loss: 62.21967697143555 = 0.04979337379336357 + 10.0 * 6.216988563537598
Epoch 1770, val loss: 1.4235087633132935
Epoch 1780, training loss: 62.202537536621094 = 0.048814963549375534 + 10.0 * 6.215372085571289
Epoch 1780, val loss: 1.4284389019012451
Epoch 1790, training loss: 62.19282531738281 = 0.04785199463367462 + 10.0 * 6.2144975662231445
Epoch 1790, val loss: 1.4330387115478516
Epoch 1800, training loss: 62.213783264160156 = 0.046922892332077026 + 10.0 * 6.216685771942139
Epoch 1800, val loss: 1.4375978708267212
Epoch 1810, training loss: 62.202392578125 = 0.046003635972738266 + 10.0 * 6.215638637542725
Epoch 1810, val loss: 1.442091464996338
Epoch 1820, training loss: 62.195987701416016 = 0.045100897550582886 + 10.0 * 6.215088844299316
Epoch 1820, val loss: 1.446554183959961
Epoch 1830, training loss: 62.186580657958984 = 0.044229909777641296 + 10.0 * 6.214234828948975
Epoch 1830, val loss: 1.4512722492218018
Epoch 1840, training loss: 62.17890930175781 = 0.04338313639163971 + 10.0 * 6.213552474975586
Epoch 1840, val loss: 1.4559383392333984
Epoch 1850, training loss: 62.19464874267578 = 0.042563170194625854 + 10.0 * 6.215208530426025
Epoch 1850, val loss: 1.4601490497589111
Epoch 1860, training loss: 62.21218490600586 = 0.04175179824233055 + 10.0 * 6.217043399810791
Epoch 1860, val loss: 1.4643826484680176
Epoch 1870, training loss: 62.19529342651367 = 0.04097399488091469 + 10.0 * 6.215432167053223
Epoch 1870, val loss: 1.4692829847335815
Epoch 1880, training loss: 62.17577362060547 = 0.040192801505327225 + 10.0 * 6.213558197021484
Epoch 1880, val loss: 1.4735113382339478
Epoch 1890, training loss: 62.16400146484375 = 0.03945267200469971 + 10.0 * 6.212454795837402
Epoch 1890, val loss: 1.4778742790222168
Epoch 1900, training loss: 62.16484832763672 = 0.03872813284397125 + 10.0 * 6.212612152099609
Epoch 1900, val loss: 1.4821585416793823
Epoch 1910, training loss: 62.2310791015625 = 0.038027748465538025 + 10.0 * 6.219305038452148
Epoch 1910, val loss: 1.4863628149032593
Epoch 1920, training loss: 62.198020935058594 = 0.03732305392622948 + 10.0 * 6.21606969833374
Epoch 1920, val loss: 1.4904553890228271
Epoch 1930, training loss: 62.17095947265625 = 0.03665212169289589 + 10.0 * 6.213430881500244
Epoch 1930, val loss: 1.4945471286773682
Epoch 1940, training loss: 62.17324447631836 = 0.03598814830183983 + 10.0 * 6.213725566864014
Epoch 1940, val loss: 1.4987659454345703
Epoch 1950, training loss: 62.15464782714844 = 0.035350728780031204 + 10.0 * 6.211929798126221
Epoch 1950, val loss: 1.5031211376190186
Epoch 1960, training loss: 62.1927375793457 = 0.03472902625799179 + 10.0 * 6.215800762176514
Epoch 1960, val loss: 1.5071622133255005
Epoch 1970, training loss: 62.147518157958984 = 0.034105200320482254 + 10.0 * 6.211341381072998
Epoch 1970, val loss: 1.5110714435577393
Epoch 1980, training loss: 62.14374923706055 = 0.033513084053993225 + 10.0 * 6.211023807525635
Epoch 1980, val loss: 1.5154494047164917
Epoch 1990, training loss: 62.15394592285156 = 0.032940804958343506 + 10.0 * 6.212100505828857
Epoch 1990, val loss: 1.5195138454437256
Epoch 2000, training loss: 62.167572021484375 = 0.03237362578511238 + 10.0 * 6.21351957321167
Epoch 2000, val loss: 1.523298740386963
Epoch 2010, training loss: 62.145469665527344 = 0.03180038183927536 + 10.0 * 6.211367130279541
Epoch 2010, val loss: 1.5268027782440186
Epoch 2020, training loss: 62.13621520996094 = 0.0312674306333065 + 10.0 * 6.210494518280029
Epoch 2020, val loss: 1.5311063528060913
Epoch 2030, training loss: 62.148406982421875 = 0.030739545822143555 + 10.0 * 6.211766719818115
Epoch 2030, val loss: 1.5348440408706665
Epoch 2040, training loss: 62.13447952270508 = 0.030224431306123734 + 10.0 * 6.21042537689209
Epoch 2040, val loss: 1.538668155670166
Epoch 2050, training loss: 62.133697509765625 = 0.02971959114074707 + 10.0 * 6.210397720336914
Epoch 2050, val loss: 1.5424946546554565
Epoch 2060, training loss: 62.146610260009766 = 0.029227692633867264 + 10.0 * 6.211738109588623
Epoch 2060, val loss: 1.5460525751113892
Epoch 2070, training loss: 62.13783645629883 = 0.028749529272317886 + 10.0 * 6.21090841293335
Epoch 2070, val loss: 1.5502632856369019
Epoch 2080, training loss: 62.1555290222168 = 0.028283022344112396 + 10.0 * 6.212724685668945
Epoch 2080, val loss: 1.553975224494934
Epoch 2090, training loss: 62.125911712646484 = 0.027807215228676796 + 10.0 * 6.209810733795166
Epoch 2090, val loss: 1.5573533773422241
Epoch 2100, training loss: 62.17698287963867 = 0.027364442124962807 + 10.0 * 6.214962005615234
Epoch 2100, val loss: 1.5611933469772339
Epoch 2110, training loss: 62.11988830566406 = 0.026918508112430573 + 10.0 * 6.209296703338623
Epoch 2110, val loss: 1.5647917985916138
Epoch 2120, training loss: 62.10823440551758 = 0.026493441313505173 + 10.0 * 6.208174228668213
Epoch 2120, val loss: 1.5685844421386719
Epoch 2130, training loss: 62.136539459228516 = 0.026082372292876244 + 10.0 * 6.211045742034912
Epoch 2130, val loss: 1.572229027748108
Epoch 2140, training loss: 62.12236785888672 = 0.025671428069472313 + 10.0 * 6.209669589996338
Epoch 2140, val loss: 1.5757125616073608
Epoch 2150, training loss: 62.10507583618164 = 0.02525932528078556 + 10.0 * 6.207981586456299
Epoch 2150, val loss: 1.5790880918502808
Epoch 2160, training loss: 62.098026275634766 = 0.02487172745168209 + 10.0 * 6.207315444946289
Epoch 2160, val loss: 1.5828351974487305
Epoch 2170, training loss: 62.102943420410156 = 0.02449115924537182 + 10.0 * 6.207845211029053
Epoch 2170, val loss: 1.5862352848052979
Epoch 2180, training loss: 62.14035415649414 = 0.02412215620279312 + 10.0 * 6.211623191833496
Epoch 2180, val loss: 1.5894609689712524
Epoch 2190, training loss: 62.11825180053711 = 0.023759132251143456 + 10.0 * 6.209449291229248
Epoch 2190, val loss: 1.5935893058776855
Epoch 2200, training loss: 62.117225646972656 = 0.023398086428642273 + 10.0 * 6.2093825340271
Epoch 2200, val loss: 1.5966405868530273
Epoch 2210, training loss: 62.08538818359375 = 0.02304215170443058 + 10.0 * 6.206234931945801
Epoch 2210, val loss: 1.6002976894378662
Epoch 2220, training loss: 62.087196350097656 = 0.02269802987575531 + 10.0 * 6.20644998550415
Epoch 2220, val loss: 1.6034576892852783
Epoch 2230, training loss: 62.17860412597656 = 0.022368062287569046 + 10.0 * 6.215623378753662
Epoch 2230, val loss: 1.6064938306808472
Epoch 2240, training loss: 62.11564254760742 = 0.022042103111743927 + 10.0 * 6.209360122680664
Epoch 2240, val loss: 1.6104307174682617
Epoch 2250, training loss: 62.08860397338867 = 0.02171100489795208 + 10.0 * 6.206689357757568
Epoch 2250, val loss: 1.6135231256484985
Epoch 2260, training loss: 62.075531005859375 = 0.021401992067694664 + 10.0 * 6.205412864685059
Epoch 2260, val loss: 1.6169415712356567
Epoch 2270, training loss: 62.07532501220703 = 0.021095702424645424 + 10.0 * 6.205422878265381
Epoch 2270, val loss: 1.620032548904419
Epoch 2280, training loss: 62.145389556884766 = 0.020799627527594566 + 10.0 * 6.212459087371826
Epoch 2280, val loss: 1.6229923963546753
Epoch 2290, training loss: 62.11482620239258 = 0.02050410956144333 + 10.0 * 6.209432125091553
Epoch 2290, val loss: 1.626358151435852
Epoch 2300, training loss: 62.10825729370117 = 0.020208869129419327 + 10.0 * 6.208804607391357
Epoch 2300, val loss: 1.6292603015899658
Epoch 2310, training loss: 62.07494354248047 = 0.01992894522845745 + 10.0 * 6.205501556396484
Epoch 2310, val loss: 1.6328662633895874
Epoch 2320, training loss: 62.065059661865234 = 0.019649839028716087 + 10.0 * 6.204541206359863
Epoch 2320, val loss: 1.6360880136489868
Epoch 2330, training loss: 62.07151412963867 = 0.01938251033425331 + 10.0 * 6.2052130699157715
Epoch 2330, val loss: 1.639244794845581
Epoch 2340, training loss: 62.116127014160156 = 0.019118789583444595 + 10.0 * 6.209700584411621
Epoch 2340, val loss: 1.6422734260559082
Epoch 2350, training loss: 62.07681655883789 = 0.018854929134249687 + 10.0 * 6.205796241760254
Epoch 2350, val loss: 1.6451056003570557
Epoch 2360, training loss: 62.0675163269043 = 0.018599877133965492 + 10.0 * 6.204891681671143
Epoch 2360, val loss: 1.6483709812164307
Epoch 2370, training loss: 62.0841064453125 = 0.018352434039115906 + 10.0 * 6.206575393676758
Epoch 2370, val loss: 1.6514137983322144
Epoch 2380, training loss: 62.05193328857422 = 0.01810591109097004 + 10.0 * 6.203382968902588
Epoch 2380, val loss: 1.6542320251464844
Epoch 2390, training loss: 62.105674743652344 = 0.017867311835289 + 10.0 * 6.208780765533447
Epoch 2390, val loss: 1.656764030456543
Epoch 2400, training loss: 62.06596755981445 = 0.017628207802772522 + 10.0 * 6.204833984375
Epoch 2400, val loss: 1.660188913345337
Epoch 2410, training loss: 62.05268859863281 = 0.01739649288356304 + 10.0 * 6.203529357910156
Epoch 2410, val loss: 1.663026213645935
Epoch 2420, training loss: 62.04294967651367 = 0.0171707421541214 + 10.0 * 6.202578067779541
Epoch 2420, val loss: 1.6661497354507446
Epoch 2430, training loss: 62.04046630859375 = 0.016952162608504295 + 10.0 * 6.2023515701293945
Epoch 2430, val loss: 1.6690078973770142
Epoch 2440, training loss: 62.04841232299805 = 0.0167382825165987 + 10.0 * 6.20316743850708
Epoch 2440, val loss: 1.6716835498809814
Epoch 2450, training loss: 62.12648391723633 = 0.016529664397239685 + 10.0 * 6.210995674133301
Epoch 2450, val loss: 1.6743022203445435
Epoch 2460, training loss: 62.05983352661133 = 0.016316605731844902 + 10.0 * 6.204351902008057
Epoch 2460, val loss: 1.677290439605713
Epoch 2470, training loss: 62.04166793823242 = 0.016108518466353416 + 10.0 * 6.2025556564331055
Epoch 2470, val loss: 1.6801514625549316
Epoch 2480, training loss: 62.052581787109375 = 0.015911439433693886 + 10.0 * 6.203667163848877
Epoch 2480, val loss: 1.6830276250839233
Epoch 2490, training loss: 62.045814514160156 = 0.015715107321739197 + 10.0 * 6.203009605407715
Epoch 2490, val loss: 1.68595290184021
Epoch 2500, training loss: 62.06373596191406 = 0.015524090267717838 + 10.0 * 6.2048211097717285
Epoch 2500, val loss: 1.6889930963516235
Epoch 2510, training loss: 62.05229187011719 = 0.015329043380916119 + 10.0 * 6.203696250915527
Epoch 2510, val loss: 1.691182255744934
Epoch 2520, training loss: 62.035587310791016 = 0.015135691501200199 + 10.0 * 6.20204496383667
Epoch 2520, val loss: 1.6937105655670166
Epoch 2530, training loss: 62.03273391723633 = 0.01495564915239811 + 10.0 * 6.201777458190918
Epoch 2530, val loss: 1.6966744661331177
Epoch 2540, training loss: 62.060523986816406 = 0.014777171425521374 + 10.0 * 6.2045745849609375
Epoch 2540, val loss: 1.6990705728530884
Epoch 2550, training loss: 62.041236877441406 = 0.014603804796934128 + 10.0 * 6.202663421630859
Epoch 2550, val loss: 1.7020505666732788
Epoch 2560, training loss: 62.03901290893555 = 0.014430267736315727 + 10.0 * 6.202458381652832
Epoch 2560, val loss: 1.7045221328735352
Epoch 2570, training loss: 62.02838897705078 = 0.014257636852562428 + 10.0 * 6.201413154602051
Epoch 2570, val loss: 1.7071788311004639
Epoch 2580, training loss: 62.06698989868164 = 0.014092079363763332 + 10.0 * 6.205289840698242
Epoch 2580, val loss: 1.7097761631011963
Epoch 2590, training loss: 62.02215576171875 = 0.013927336782217026 + 10.0 * 6.200822830200195
Epoch 2590, val loss: 1.712073564529419
Epoch 2600, training loss: 62.0261344909668 = 0.013767286203801632 + 10.0 * 6.201236724853516
Epoch 2600, val loss: 1.7148802280426025
Epoch 2610, training loss: 62.029205322265625 = 0.01361055951565504 + 10.0 * 6.201559543609619
Epoch 2610, val loss: 1.7174683809280396
Epoch 2620, training loss: 62.03858184814453 = 0.013455657288432121 + 10.0 * 6.202512741088867
Epoch 2620, val loss: 1.7197425365447998
Epoch 2630, training loss: 62.02820587158203 = 0.013303338550031185 + 10.0 * 6.20149040222168
Epoch 2630, val loss: 1.7222912311553955
Epoch 2640, training loss: 62.03720474243164 = 0.013155972585082054 + 10.0 * 6.202404975891113
Epoch 2640, val loss: 1.725017786026001
Epoch 2650, training loss: 62.06687927246094 = 0.01301142293959856 + 10.0 * 6.205386638641357
Epoch 2650, val loss: 1.72725510597229
Epoch 2660, training loss: 62.01691436767578 = 0.012858055531978607 + 10.0 * 6.200405597686768
Epoch 2660, val loss: 1.7290863990783691
Epoch 2670, training loss: 62.009742736816406 = 0.012710796669125557 + 10.0 * 6.199703216552734
Epoch 2670, val loss: 1.7317028045654297
Epoch 2680, training loss: 62.00461196899414 = 0.012573049403727055 + 10.0 * 6.199203968048096
Epoch 2680, val loss: 1.7341670989990234
Epoch 2690, training loss: 62.00096893310547 = 0.012438388541340828 + 10.0 * 6.198853015899658
Epoch 2690, val loss: 1.736466884613037
Epoch 2700, training loss: 62.01426315307617 = 0.012307213619351387 + 10.0 * 6.200195789337158
Epoch 2700, val loss: 1.7387290000915527
Epoch 2710, training loss: 62.045074462890625 = 0.012176880612969398 + 10.0 * 6.203289985656738
Epoch 2710, val loss: 1.7408807277679443
Epoch 2720, training loss: 62.02389907836914 = 0.012046833522617817 + 10.0 * 6.20118522644043
Epoch 2720, val loss: 1.743449330329895
Epoch 2730, training loss: 62.04383087158203 = 0.011918461881577969 + 10.0 * 6.20319128036499
Epoch 2730, val loss: 1.7457640171051025
Epoch 2740, training loss: 62.00010681152344 = 0.011786255985498428 + 10.0 * 6.198832035064697
Epoch 2740, val loss: 1.7476567029953003
Epoch 2750, training loss: 62.00306701660156 = 0.011660503223538399 + 10.0 * 6.199140548706055
Epoch 2750, val loss: 1.7497379779815674
Epoch 2760, training loss: 62.0220832824707 = 0.011541740037500858 + 10.0 * 6.201054096221924
Epoch 2760, val loss: 1.7520506381988525
Epoch 2770, training loss: 62.023101806640625 = 0.011420822702348232 + 10.0 * 6.201168060302734
Epoch 2770, val loss: 1.754077434539795
Epoch 2780, training loss: 62.02206802368164 = 0.011302883736789227 + 10.0 * 6.201076507568359
Epoch 2780, val loss: 1.7566455602645874
Epoch 2790, training loss: 62.00459671020508 = 0.01118354219943285 + 10.0 * 6.199341297149658
Epoch 2790, val loss: 1.7585113048553467
Epoch 2800, training loss: 61.994773864746094 = 0.011069229803979397 + 10.0 * 6.198370456695557
Epoch 2800, val loss: 1.7605822086334229
Epoch 2810, training loss: 61.992271423339844 = 0.01095886155962944 + 10.0 * 6.198131084442139
Epoch 2810, val loss: 1.7629737854003906
Epoch 2820, training loss: 62.035377502441406 = 0.010850016959011555 + 10.0 * 6.202452659606934
Epoch 2820, val loss: 1.7647236585617065
Epoch 2830, training loss: 61.98832321166992 = 0.01074066013097763 + 10.0 * 6.197758197784424
Epoch 2830, val loss: 1.7668019533157349
Epoch 2840, training loss: 61.99395751953125 = 0.010634804144501686 + 10.0 * 6.1983323097229
Epoch 2840, val loss: 1.7691951990127563
Epoch 2850, training loss: 62.04481887817383 = 0.010532122105360031 + 10.0 * 6.203428745269775
Epoch 2850, val loss: 1.7710622549057007
Epoch 2860, training loss: 61.99382400512695 = 0.010424087755382061 + 10.0 * 6.198339939117432
Epoch 2860, val loss: 1.7730587720870972
Epoch 2870, training loss: 61.99601745605469 = 0.01032213494181633 + 10.0 * 6.198569297790527
Epoch 2870, val loss: 1.7751392126083374
Epoch 2880, training loss: 62.0072135925293 = 0.01022209320217371 + 10.0 * 6.1996989250183105
Epoch 2880, val loss: 1.7770614624023438
Epoch 2890, training loss: 61.989044189453125 = 0.010125381872057915 + 10.0 * 6.197892189025879
Epoch 2890, val loss: 1.7794798612594604
Epoch 2900, training loss: 62.0301513671875 = 0.010032812133431435 + 10.0 * 6.202012062072754
Epoch 2900, val loss: 1.7815881967544556
Epoch 2910, training loss: 61.99776077270508 = 0.009929722175002098 + 10.0 * 6.198782920837402
Epoch 2910, val loss: 1.7826734781265259
Epoch 2920, training loss: 61.977256774902344 = 0.009834835305809975 + 10.0 * 6.196742057800293
Epoch 2920, val loss: 1.7849531173706055
Epoch 2930, training loss: 61.96971893310547 = 0.009742594324052334 + 10.0 * 6.195997714996338
Epoch 2930, val loss: 1.787036418914795
Epoch 2940, training loss: 61.97111129760742 = 0.009652744978666306 + 10.0 * 6.196146011352539
Epoch 2940, val loss: 1.7888368368148804
Epoch 2950, training loss: 62.040985107421875 = 0.009564029984176159 + 10.0 * 6.203142166137695
Epoch 2950, val loss: 1.790358066558838
Epoch 2960, training loss: 62.001747131347656 = 0.009474499151110649 + 10.0 * 6.199227333068848
Epoch 2960, val loss: 1.7922035455703735
Epoch 2970, training loss: 61.985416412353516 = 0.009383167140185833 + 10.0 * 6.197603225708008
Epoch 2970, val loss: 1.7943226099014282
Epoch 2980, training loss: 61.97896957397461 = 0.009298207238316536 + 10.0 * 6.196967124938965
Epoch 2980, val loss: 1.7962510585784912
Epoch 2990, training loss: 61.990135192871094 = 0.009214766323566437 + 10.0 * 6.198091983795166
Epoch 2990, val loss: 1.7979817390441895
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6444444444444445
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 87.92123413085938 = 1.9530155658721924 + 10.0 * 8.596821784973145
Epoch 0, val loss: 1.9588801860809326
Epoch 10, training loss: 87.90283966064453 = 1.9422643184661865 + 10.0 * 8.596057891845703
Epoch 10, val loss: 1.9476810693740845
Epoch 20, training loss: 87.82713317871094 = 1.9284347295761108 + 10.0 * 8.589869499206543
Epoch 20, val loss: 1.9331674575805664
Epoch 30, training loss: 87.3711929321289 = 1.910417079925537 + 10.0 * 8.546077728271484
Epoch 30, val loss: 1.9146368503570557
Epoch 40, training loss: 84.5914306640625 = 1.890364646911621 + 10.0 * 8.270106315612793
Epoch 40, val loss: 1.8948317766189575
Epoch 50, training loss: 78.32910919189453 = 1.8695000410079956 + 10.0 * 7.645960807800293
Epoch 50, val loss: 1.8752604722976685
Epoch 60, training loss: 74.66594696044922 = 1.856501579284668 + 10.0 * 7.28094482421875
Epoch 60, val loss: 1.8632762432098389
Epoch 70, training loss: 72.06572723388672 = 1.8463939428329468 + 10.0 * 7.021933078765869
Epoch 70, val loss: 1.853874921798706
Epoch 80, training loss: 70.64733123779297 = 1.837684988975525 + 10.0 * 6.880964756011963
Epoch 80, val loss: 1.8449448347091675
Epoch 90, training loss: 69.82467651367188 = 1.8279720544815063 + 10.0 * 6.799670219421387
Epoch 90, val loss: 1.8351891040802002
Epoch 100, training loss: 69.02017974853516 = 1.8189020156860352 + 10.0 * 6.720128059387207
Epoch 100, val loss: 1.8262709379196167
Epoch 110, training loss: 68.41056060791016 = 1.8113000392913818 + 10.0 * 6.65992546081543
Epoch 110, val loss: 1.81849205493927
Epoch 120, training loss: 67.85449981689453 = 1.8045086860656738 + 10.0 * 6.604998588562012
Epoch 120, val loss: 1.8112772703170776
Epoch 130, training loss: 67.4853286743164 = 1.798157811164856 + 10.0 * 6.568717002868652
Epoch 130, val loss: 1.804404377937317
Epoch 140, training loss: 67.19617462158203 = 1.7914493083953857 + 10.0 * 6.540472507476807
Epoch 140, val loss: 1.7974194288253784
Epoch 150, training loss: 66.96745300292969 = 1.7844489812850952 + 10.0 * 6.518300533294678
Epoch 150, val loss: 1.7903635501861572
Epoch 160, training loss: 66.76644134521484 = 1.7771152257919312 + 10.0 * 6.498932361602783
Epoch 160, val loss: 1.7831411361694336
Epoch 170, training loss: 66.59092712402344 = 1.769465684890747 + 10.0 * 6.482146739959717
Epoch 170, val loss: 1.775755524635315
Epoch 180, training loss: 66.45398712158203 = 1.761284351348877 + 10.0 * 6.469270706176758
Epoch 180, val loss: 1.7681101560592651
Epoch 190, training loss: 66.28935241699219 = 1.7525596618652344 + 10.0 * 6.453678607940674
Epoch 190, val loss: 1.7601126432418823
Epoch 200, training loss: 66.14219665527344 = 1.743323802947998 + 10.0 * 6.439887046813965
Epoch 200, val loss: 1.7517051696777344
Epoch 210, training loss: 66.00979614257812 = 1.7332922220230103 + 10.0 * 6.427650451660156
Epoch 210, val loss: 1.7427079677581787
Epoch 220, training loss: 65.96678161621094 = 1.722287654876709 + 10.0 * 6.4244489669799805
Epoch 220, val loss: 1.7329425811767578
Epoch 230, training loss: 65.7913818359375 = 1.7102375030517578 + 10.0 * 6.408114910125732
Epoch 230, val loss: 1.722248911857605
Epoch 240, training loss: 65.68212127685547 = 1.697297215461731 + 10.0 * 6.398482322692871
Epoch 240, val loss: 1.7107994556427002
Epoch 250, training loss: 65.58209228515625 = 1.683380126953125 + 10.0 * 6.389871120452881
Epoch 250, val loss: 1.6986029148101807
Epoch 260, training loss: 65.51671600341797 = 1.6683547496795654 + 10.0 * 6.384836196899414
Epoch 260, val loss: 1.6854393482208252
Epoch 270, training loss: 65.41259002685547 = 1.6521327495574951 + 10.0 * 6.3760457038879395
Epoch 270, val loss: 1.6713472604751587
Epoch 280, training loss: 65.31255340576172 = 1.6347841024398804 + 10.0 * 6.367777347564697
Epoch 280, val loss: 1.6563752889633179
Epoch 290, training loss: 65.23552703857422 = 1.6162930727005005 + 10.0 * 6.361923694610596
Epoch 290, val loss: 1.6404566764831543
Epoch 300, training loss: 65.16107177734375 = 1.5966739654541016 + 10.0 * 6.356439590454102
Epoch 300, val loss: 1.6235636472702026
Epoch 310, training loss: 65.15937042236328 = 1.5758754014968872 + 10.0 * 6.358349323272705
Epoch 310, val loss: 1.6055811643600464
Epoch 320, training loss: 65.0383071899414 = 1.5538933277130127 + 10.0 * 6.3484416007995605
Epoch 320, val loss: 1.5868651866912842
Epoch 330, training loss: 64.95343017578125 = 1.5312082767486572 + 10.0 * 6.342222213745117
Epoch 330, val loss: 1.5676813125610352
Epoch 340, training loss: 64.87971496582031 = 1.507696270942688 + 10.0 * 6.3372015953063965
Epoch 340, val loss: 1.5479005575180054
Epoch 350, training loss: 64.83097076416016 = 1.4834593534469604 + 10.0 * 6.334751605987549
Epoch 350, val loss: 1.5276063680648804
Epoch 360, training loss: 64.80253601074219 = 1.4585661888122559 + 10.0 * 6.334396839141846
Epoch 360, val loss: 1.5069243907928467
Epoch 370, training loss: 64.6987075805664 = 1.4332109689712524 + 10.0 * 6.326549530029297
Epoch 370, val loss: 1.4859063625335693
Epoch 380, training loss: 64.63370513916016 = 1.4074691534042358 + 10.0 * 6.322623252868652
Epoch 380, val loss: 1.4648693799972534
Epoch 390, training loss: 64.57266998291016 = 1.3813879489898682 + 10.0 * 6.319128036499023
Epoch 390, val loss: 1.4439388513565063
Epoch 400, training loss: 64.621826171875 = 1.355087399482727 + 10.0 * 6.32667350769043
Epoch 400, val loss: 1.4228997230529785
Epoch 410, training loss: 64.46046447753906 = 1.3281846046447754 + 10.0 * 6.313227653503418
Epoch 410, val loss: 1.401932954788208
Epoch 420, training loss: 64.41085815429688 = 1.301361322402954 + 10.0 * 6.310949325561523
Epoch 420, val loss: 1.3812901973724365
Epoch 430, training loss: 64.35908508300781 = 1.2744166851043701 + 10.0 * 6.308466911315918
Epoch 430, val loss: 1.3608733415603638
Epoch 440, training loss: 64.39118957519531 = 1.2473641633987427 + 10.0 * 6.314382553100586
Epoch 440, val loss: 1.3405135869979858
Epoch 450, training loss: 64.25779724121094 = 1.2201589345932007 + 10.0 * 6.303763389587402
Epoch 450, val loss: 1.3204643726348877
Epoch 460, training loss: 64.20697784423828 = 1.19303560256958 + 10.0 * 6.301393985748291
Epoch 460, val loss: 1.3008421659469604
Epoch 470, training loss: 64.15755462646484 = 1.1659810543060303 + 10.0 * 6.299157619476318
Epoch 470, val loss: 1.2815810441970825
Epoch 480, training loss: 64.1086196899414 = 1.138999104499817 + 10.0 * 6.296962261199951
Epoch 480, val loss: 1.2626450061798096
Epoch 490, training loss: 64.16297149658203 = 1.1121900081634521 + 10.0 * 6.305078506469727
Epoch 490, val loss: 1.244110345840454
Epoch 500, training loss: 64.03527069091797 = 1.0854159593582153 + 10.0 * 6.294985294342041
Epoch 500, val loss: 1.2259036302566528
Epoch 510, training loss: 63.99170684814453 = 1.0591514110565186 + 10.0 * 6.293255805969238
Epoch 510, val loss: 1.208409309387207
Epoch 520, training loss: 63.94380187988281 = 1.0332882404327393 + 10.0 * 6.291051387786865
Epoch 520, val loss: 1.191599726676941
Epoch 530, training loss: 63.93959426879883 = 1.0078670978546143 + 10.0 * 6.293172836303711
Epoch 530, val loss: 1.1753846406936646
Epoch 540, training loss: 63.87188720703125 = 0.9828082919120789 + 10.0 * 6.288908004760742
Epoch 540, val loss: 1.1598565578460693
Epoch 550, training loss: 63.826717376708984 = 0.9584648609161377 + 10.0 * 6.286825180053711
Epoch 550, val loss: 1.1450583934783936
Epoch 560, training loss: 63.77853012084961 = 0.9346429705619812 + 10.0 * 6.284388542175293
Epoch 560, val loss: 1.1309475898742676
Epoch 570, training loss: 63.749019622802734 = 0.9113978743553162 + 10.0 * 6.283762454986572
Epoch 570, val loss: 1.1175520420074463
Epoch 580, training loss: 63.71780014038086 = 0.8887531161308289 + 10.0 * 6.282904624938965
Epoch 580, val loss: 1.1049472093582153
Epoch 590, training loss: 63.705379486083984 = 0.8667477965354919 + 10.0 * 6.283863067626953
Epoch 590, val loss: 1.0930379629135132
Epoch 600, training loss: 63.65876007080078 = 0.8450946807861328 + 10.0 * 6.28136682510376
Epoch 600, val loss: 1.0817004442214966
Epoch 610, training loss: 63.60198974609375 = 0.8243404626846313 + 10.0 * 6.277764797210693
Epoch 610, val loss: 1.0711833238601685
Epoch 620, training loss: 63.57541275024414 = 0.8042688965797424 + 10.0 * 6.277114391326904
Epoch 620, val loss: 1.0614054203033447
Epoch 630, training loss: 63.57475280761719 = 0.784712016582489 + 10.0 * 6.279004096984863
Epoch 630, val loss: 1.0520808696746826
Epoch 640, training loss: 63.518402099609375 = 0.7657074928283691 + 10.0 * 6.275269508361816
Epoch 640, val loss: 1.0434397459030151
Epoch 650, training loss: 63.49793243408203 = 0.747329831123352 + 10.0 * 6.275060176849365
Epoch 650, val loss: 1.0354286432266235
Epoch 660, training loss: 63.45874786376953 = 0.7293294072151184 + 10.0 * 6.272942066192627
Epoch 660, val loss: 1.0277236700057983
Epoch 670, training loss: 63.42934036254883 = 0.7119014263153076 + 10.0 * 6.2717437744140625
Epoch 670, val loss: 1.0207135677337646
Epoch 680, training loss: 63.394737243652344 = 0.6949902772903442 + 10.0 * 6.269974708557129
Epoch 680, val loss: 1.0141181945800781
Epoch 690, training loss: 63.36526870727539 = 0.6785708069801331 + 10.0 * 6.268670082092285
Epoch 690, val loss: 1.0080374479293823
Epoch 700, training loss: 63.35139465332031 = 0.6624980568885803 + 10.0 * 6.268889427185059
Epoch 700, val loss: 1.0021963119506836
Epoch 710, training loss: 63.34201431274414 = 0.6467400789260864 + 10.0 * 6.269527435302734
Epoch 710, val loss: 0.9964948892593384
Epoch 720, training loss: 63.30179977416992 = 0.6313374638557434 + 10.0 * 6.2670464515686035
Epoch 720, val loss: 0.9913055300712585
Epoch 730, training loss: 63.270870208740234 = 0.6163671612739563 + 10.0 * 6.265450477600098
Epoch 730, val loss: 0.9862174987792969
Epoch 740, training loss: 63.26935577392578 = 0.6017924547195435 + 10.0 * 6.266756534576416
Epoch 740, val loss: 0.9814055562019348
Epoch 750, training loss: 63.24291229248047 = 0.5873516798019409 + 10.0 * 6.2655558586120605
Epoch 750, val loss: 0.9769213795661926
Epoch 760, training loss: 63.21153259277344 = 0.5732648372650146 + 10.0 * 6.263826847076416
Epoch 760, val loss: 0.9725193977355957
Epoch 770, training loss: 63.18867111206055 = 0.5594343543052673 + 10.0 * 6.262923717498779
Epoch 770, val loss: 0.968431830406189
Epoch 780, training loss: 63.16600799560547 = 0.5458657145500183 + 10.0 * 6.262014389038086
Epoch 780, val loss: 0.9645249247550964
Epoch 790, training loss: 63.16468048095703 = 0.5325211882591248 + 10.0 * 6.263216018676758
Epoch 790, val loss: 0.9607494473457336
Epoch 800, training loss: 63.13785934448242 = 0.519331157207489 + 10.0 * 6.261852741241455
Epoch 800, val loss: 0.9570943117141724
Epoch 810, training loss: 63.103031158447266 = 0.5064416527748108 + 10.0 * 6.2596588134765625
Epoch 810, val loss: 0.953610360622406
Epoch 820, training loss: 63.07020950317383 = 0.4937446415424347 + 10.0 * 6.257646560668945
Epoch 820, val loss: 0.9502581357955933
Epoch 830, training loss: 63.060302734375 = 0.4812985360622406 + 10.0 * 6.257900238037109
Epoch 830, val loss: 0.9471259713172913
Epoch 840, training loss: 63.089256286621094 = 0.4689704477787018 + 10.0 * 6.262028694152832
Epoch 840, val loss: 0.9438996315002441
Epoch 850, training loss: 63.0244140625 = 0.45677831768989563 + 10.0 * 6.256763458251953
Epoch 850, val loss: 0.9408689737319946
Epoch 860, training loss: 62.99839401245117 = 0.44487857818603516 + 10.0 * 6.255351543426514
Epoch 860, val loss: 0.9379225969314575
Epoch 870, training loss: 62.97492218017578 = 0.43326520919799805 + 10.0 * 6.2541656494140625
Epoch 870, val loss: 0.935112714767456
Epoch 880, training loss: 62.95597839355469 = 0.4218543767929077 + 10.0 * 6.253412246704102
Epoch 880, val loss: 0.9326449036598206
Epoch 890, training loss: 62.96949768066406 = 0.41065195202827454 + 10.0 * 6.255884647369385
Epoch 890, val loss: 0.9301235675811768
Epoch 900, training loss: 62.9504508972168 = 0.39955639839172363 + 10.0 * 6.25508975982666
Epoch 900, val loss: 0.9273862838745117
Epoch 910, training loss: 62.91950607299805 = 0.3887268602848053 + 10.0 * 6.253077983856201
Epoch 910, val loss: 0.9252665638923645
Epoch 920, training loss: 62.88931655883789 = 0.3781736493110657 + 10.0 * 6.251114368438721
Epoch 920, val loss: 0.9230003356933594
Epoch 930, training loss: 62.87129211425781 = 0.3678836226463318 + 10.0 * 6.250340938568115
Epoch 930, val loss: 0.9211501479148865
Epoch 940, training loss: 62.91977310180664 = 0.3577865660190582 + 10.0 * 6.256198406219482
Epoch 940, val loss: 0.9194387197494507
Epoch 950, training loss: 62.872520446777344 = 0.34793704748153687 + 10.0 * 6.252458095550537
Epoch 950, val loss: 0.9173369407653809
Epoch 960, training loss: 62.823455810546875 = 0.3383375406265259 + 10.0 * 6.248511791229248
Epoch 960, val loss: 0.9158998131752014
Epoch 970, training loss: 62.80889892578125 = 0.3290351927280426 + 10.0 * 6.247986316680908
Epoch 970, val loss: 0.9145286679267883
Epoch 980, training loss: 62.78891372680664 = 0.3200272023677826 + 10.0 * 6.246888637542725
Epoch 980, val loss: 0.9133005738258362
Epoch 990, training loss: 62.774925231933594 = 0.31124594807624817 + 10.0 * 6.246367931365967
Epoch 990, val loss: 0.9123944044113159
Epoch 1000, training loss: 62.84979248046875 = 0.30273276567459106 + 10.0 * 6.254705905914307
Epoch 1000, val loss: 0.9116296172142029
Epoch 1010, training loss: 62.79775619506836 = 0.2942792475223541 + 10.0 * 6.25034761428833
Epoch 1010, val loss: 0.9106199741363525
Epoch 1020, training loss: 62.73831558227539 = 0.28616762161254883 + 10.0 * 6.245214939117432
Epoch 1020, val loss: 0.9101781845092773
Epoch 1030, training loss: 62.72421646118164 = 0.2783609926700592 + 10.0 * 6.2445855140686035
Epoch 1030, val loss: 0.9097711443901062
Epoch 1040, training loss: 62.710384368896484 = 0.2707669138908386 + 10.0 * 6.243961811065674
Epoch 1040, val loss: 0.9097403883934021
Epoch 1050, training loss: 62.773345947265625 = 0.26338452100753784 + 10.0 * 6.250996112823486
Epoch 1050, val loss: 0.9096180200576782
Epoch 1060, training loss: 62.709381103515625 = 0.25615060329437256 + 10.0 * 6.245323181152344
Epoch 1060, val loss: 0.9096317291259766
Epoch 1070, training loss: 62.7082633972168 = 0.24919264018535614 + 10.0 * 6.245907306671143
Epoch 1070, val loss: 0.9099941253662109
Epoch 1080, training loss: 62.66133499145508 = 0.24245685338974 + 10.0 * 6.24188756942749
Epoch 1080, val loss: 0.910366952419281
Epoch 1090, training loss: 62.65238571166992 = 0.2359502762556076 + 10.0 * 6.24164342880249
Epoch 1090, val loss: 0.9110081195831299
Epoch 1100, training loss: 62.6538200378418 = 0.22965136170387268 + 10.0 * 6.242416858673096
Epoch 1100, val loss: 0.911886990070343
Epoch 1110, training loss: 62.64704895019531 = 0.22351397573947906 + 10.0 * 6.242353439331055
Epoch 1110, val loss: 0.9125242233276367
Epoch 1120, training loss: 62.61721420288086 = 0.21753984689712524 + 10.0 * 6.239967346191406
Epoch 1120, val loss: 0.9134804010391235
Epoch 1130, training loss: 62.606971740722656 = 0.211770161986351 + 10.0 * 6.239520072937012
Epoch 1130, val loss: 0.9147509932518005
Epoch 1140, training loss: 62.61857986450195 = 0.20616774260997772 + 10.0 * 6.241240978240967
Epoch 1140, val loss: 0.915959358215332
Epoch 1150, training loss: 62.599124908447266 = 0.20070350170135498 + 10.0 * 6.239842414855957
Epoch 1150, val loss: 0.9172423481941223
Epoch 1160, training loss: 62.59963607788086 = 0.19543035328388214 + 10.0 * 6.240420341491699
Epoch 1160, val loss: 0.9187434911727905
Epoch 1170, training loss: 62.60348129272461 = 0.19028285145759583 + 10.0 * 6.24131965637207
Epoch 1170, val loss: 0.920026421546936
Epoch 1180, training loss: 62.559242248535156 = 0.1853020340204239 + 10.0 * 6.237393856048584
Epoch 1180, val loss: 0.9216117858886719
Epoch 1190, training loss: 62.549964904785156 = 0.18048176169395447 + 10.0 * 6.236948490142822
Epoch 1190, val loss: 0.9235758185386658
Epoch 1200, training loss: 62.535179138183594 = 0.17582915723323822 + 10.0 * 6.235934734344482
Epoch 1200, val loss: 0.9253588914871216
Epoch 1210, training loss: 62.527748107910156 = 0.17131638526916504 + 10.0 * 6.235642910003662
Epoch 1210, val loss: 0.9274397492408752
Epoch 1220, training loss: 62.551265716552734 = 0.16692611575126648 + 10.0 * 6.238433837890625
Epoch 1220, val loss: 0.9293957948684692
Epoch 1230, training loss: 62.5218620300293 = 0.16263054311275482 + 10.0 * 6.235922813415527
Epoch 1230, val loss: 0.9315972328186035
Epoch 1240, training loss: 62.50734329223633 = 0.1584579348564148 + 10.0 * 6.234888553619385
Epoch 1240, val loss: 0.933599591255188
Epoch 1250, training loss: 62.52033233642578 = 0.15444332361221313 + 10.0 * 6.236588954925537
Epoch 1250, val loss: 0.935928463935852
Epoch 1260, training loss: 62.531673431396484 = 0.15054772794246674 + 10.0 * 6.238112449645996
Epoch 1260, val loss: 0.9383261203765869
Epoch 1270, training loss: 62.48667526245117 = 0.1467200219631195 + 10.0 * 6.23399543762207
Epoch 1270, val loss: 0.9406442046165466
Epoch 1280, training loss: 62.46636962890625 = 0.1430521160364151 + 10.0 * 6.2323317527771
Epoch 1280, val loss: 0.9430769085884094
Epoch 1290, training loss: 62.4587516784668 = 0.13949891924858093 + 10.0 * 6.231925010681152
Epoch 1290, val loss: 0.945607602596283
Epoch 1300, training loss: 62.50437545776367 = 0.13604186475276947 + 10.0 * 6.236833095550537
Epoch 1300, val loss: 0.948150634765625
Epoch 1310, training loss: 62.454254150390625 = 0.1326747089624405 + 10.0 * 6.2321577072143555
Epoch 1310, val loss: 0.9506261944770813
Epoch 1320, training loss: 62.441246032714844 = 0.1293904036283493 + 10.0 * 6.231185436248779
Epoch 1320, val loss: 0.9531062245368958
Epoch 1330, training loss: 62.432254791259766 = 0.12623801827430725 + 10.0 * 6.2306013107299805
Epoch 1330, val loss: 0.9558117985725403
Epoch 1340, training loss: 62.4405517578125 = 0.12317046523094177 + 10.0 * 6.231738090515137
Epoch 1340, val loss: 0.9585440158843994
Epoch 1350, training loss: 62.451019287109375 = 0.12016694247722626 + 10.0 * 6.2330851554870605
Epoch 1350, val loss: 0.9612178206443787
Epoch 1360, training loss: 62.437808990478516 = 0.11722780764102936 + 10.0 * 6.232058048248291
Epoch 1360, val loss: 0.9636878371238708
Epoch 1370, training loss: 62.41818618774414 = 0.11441180109977722 + 10.0 * 6.230377674102783
Epoch 1370, val loss: 0.9666655659675598
Epoch 1380, training loss: 62.40578079223633 = 0.11166901141405106 + 10.0 * 6.2294111251831055
Epoch 1380, val loss: 0.9694288372993469
Epoch 1390, training loss: 62.40009689331055 = 0.10901419818401337 + 10.0 * 6.2291083335876465
Epoch 1390, val loss: 0.9724277257919312
Epoch 1400, training loss: 62.40436935424805 = 0.10642867535352707 + 10.0 * 6.229794025421143
Epoch 1400, val loss: 0.9754371047019958
Epoch 1410, training loss: 62.38615417480469 = 0.10391383618116379 + 10.0 * 6.228224277496338
Epoch 1410, val loss: 0.9784222841262817
Epoch 1420, training loss: 62.457374572753906 = 0.1014777272939682 + 10.0 * 6.235589504241943
Epoch 1420, val loss: 0.9813217520713806
Epoch 1430, training loss: 62.385986328125 = 0.0990559384226799 + 10.0 * 6.228693008422852
Epoch 1430, val loss: 0.9840894937515259
Epoch 1440, training loss: 62.36747741699219 = 0.09673462808132172 + 10.0 * 6.22707462310791
Epoch 1440, val loss: 0.9874567985534668
Epoch 1450, training loss: 62.35516357421875 = 0.0944952443242073 + 10.0 * 6.226067066192627
Epoch 1450, val loss: 0.9904106855392456
Epoch 1460, training loss: 62.34678268432617 = 0.0923134982585907 + 10.0 * 6.225447177886963
Epoch 1460, val loss: 0.9934658408164978
Epoch 1470, training loss: 62.356441497802734 = 0.09019903838634491 + 10.0 * 6.226624488830566
Epoch 1470, val loss: 0.9967133402824402
Epoch 1480, training loss: 62.3494873046875 = 0.08810485899448395 + 10.0 * 6.226138114929199
Epoch 1480, val loss: 0.9995240569114685
Epoch 1490, training loss: 62.36631774902344 = 0.08608502149581909 + 10.0 * 6.228023052215576
Epoch 1490, val loss: 1.0025014877319336
Epoch 1500, training loss: 62.33160400390625 = 0.08411793410778046 + 10.0 * 6.224748611450195
Epoch 1500, val loss: 1.005700707435608
Epoch 1510, training loss: 62.317657470703125 = 0.08222717046737671 + 10.0 * 6.223543167114258
Epoch 1510, val loss: 1.008974313735962
Epoch 1520, training loss: 62.31553649902344 = 0.08039151877164841 + 10.0 * 6.223514556884766
Epoch 1520, val loss: 1.0120329856872559
Epoch 1530, training loss: 62.37848663330078 = 0.07860523462295532 + 10.0 * 6.229988098144531
Epoch 1530, val loss: 1.0150562524795532
Epoch 1540, training loss: 62.33918762207031 = 0.07684481143951416 + 10.0 * 6.226234436035156
Epoch 1540, val loss: 1.018276572227478
Epoch 1550, training loss: 62.33444595336914 = 0.0751226544380188 + 10.0 * 6.2259321212768555
Epoch 1550, val loss: 1.0212860107421875
Epoch 1560, training loss: 62.299224853515625 = 0.07346934825181961 + 10.0 * 6.2225751876831055
Epoch 1560, val loss: 1.0246360301971436
Epoch 1570, training loss: 62.3130989074707 = 0.07186747342348099 + 10.0 * 6.224123001098633
Epoch 1570, val loss: 1.0278103351593018
Epoch 1580, training loss: 62.29983901977539 = 0.07029266655445099 + 10.0 * 6.222954750061035
Epoch 1580, val loss: 1.0309503078460693
Epoch 1590, training loss: 62.30812072753906 = 0.06876398622989655 + 10.0 * 6.223935604095459
Epoch 1590, val loss: 1.034178614616394
Epoch 1600, training loss: 62.31471252441406 = 0.06728380173444748 + 10.0 * 6.224742889404297
Epoch 1600, val loss: 1.0372389554977417
Epoch 1610, training loss: 62.2795295715332 = 0.06581665575504303 + 10.0 * 6.221371650695801
Epoch 1610, val loss: 1.0406112670898438
Epoch 1620, training loss: 62.28087615966797 = 0.06441383808851242 + 10.0 * 6.221646308898926
Epoch 1620, val loss: 1.0436745882034302
Epoch 1630, training loss: 62.34488296508789 = 0.06305427104234695 + 10.0 * 6.228182792663574
Epoch 1630, val loss: 1.046907663345337
Epoch 1640, training loss: 62.28447723388672 = 0.06168559938669205 + 10.0 * 6.222279071807861
Epoch 1640, val loss: 1.049912929534912
Epoch 1650, training loss: 62.26537322998047 = 0.060383737087249756 + 10.0 * 6.220499038696289
Epoch 1650, val loss: 1.0532194375991821
Epoch 1660, training loss: 62.25607681274414 = 0.059116821736097336 + 10.0 * 6.219696044921875
Epoch 1660, val loss: 1.0564210414886475
Epoch 1670, training loss: 62.27376174926758 = 0.05788849666714668 + 10.0 * 6.221587181091309
Epoch 1670, val loss: 1.0596083402633667
Epoch 1680, training loss: 62.2763671875 = 0.05667707324028015 + 10.0 * 6.221968650817871
Epoch 1680, val loss: 1.062805414199829
Epoch 1690, training loss: 62.26304244995117 = 0.0555068776011467 + 10.0 * 6.2207536697387695
Epoch 1690, val loss: 1.065836787223816
Epoch 1700, training loss: 62.2462043762207 = 0.054351869970560074 + 10.0 * 6.2191853523254395
Epoch 1700, val loss: 1.0690056085586548
Epoch 1710, training loss: 62.236629486083984 = 0.05324329435825348 + 10.0 * 6.218338966369629
Epoch 1710, val loss: 1.072265386581421
Epoch 1720, training loss: 62.22763442993164 = 0.05216706171631813 + 10.0 * 6.2175469398498535
Epoch 1720, val loss: 1.0754224061965942
Epoch 1730, training loss: 62.24125289916992 = 0.05111953243613243 + 10.0 * 6.219013214111328
Epoch 1730, val loss: 1.0785917043685913
Epoch 1740, training loss: 62.25376510620117 = 0.050092700868844986 + 10.0 * 6.220366954803467
Epoch 1740, val loss: 1.0818160772323608
Epoch 1750, training loss: 62.251617431640625 = 0.0490824431180954 + 10.0 * 6.2202534675598145
Epoch 1750, val loss: 1.0850683450698853
Epoch 1760, training loss: 62.24699783325195 = 0.048098284751176834 + 10.0 * 6.2198896408081055
Epoch 1760, val loss: 1.087904930114746
Epoch 1770, training loss: 62.22053146362305 = 0.047129929065704346 + 10.0 * 6.217339992523193
Epoch 1770, val loss: 1.0911815166473389
Epoch 1780, training loss: 62.2136116027832 = 0.04620889946818352 + 10.0 * 6.216740608215332
Epoch 1780, val loss: 1.0943800210952759
Epoch 1790, training loss: 62.20491027832031 = 0.045309390872716904 + 10.0 * 6.2159600257873535
Epoch 1790, val loss: 1.097466230392456
Epoch 1800, training loss: 62.21324920654297 = 0.044437628239393234 + 10.0 * 6.216881275177002
Epoch 1800, val loss: 1.1007530689239502
Epoch 1810, training loss: 62.23654556274414 = 0.04358123242855072 + 10.0 * 6.219296455383301
Epoch 1810, val loss: 1.1036866903305054
Epoch 1820, training loss: 62.25929260253906 = 0.04274364188313484 + 10.0 * 6.221654891967773
Epoch 1820, val loss: 1.106975793838501
Epoch 1830, training loss: 62.193214416503906 = 0.04190806299448013 + 10.0 * 6.215130805969238
Epoch 1830, val loss: 1.1098021268844604
Epoch 1840, training loss: 62.19485855102539 = 0.04111291840672493 + 10.0 * 6.21537446975708
Epoch 1840, val loss: 1.112925410270691
Epoch 1850, training loss: 62.18632125854492 = 0.04034237191081047 + 10.0 * 6.214597702026367
Epoch 1850, val loss: 1.1160635948181152
Epoch 1860, training loss: 62.25227355957031 = 0.03960028663277626 + 10.0 * 6.221267223358154
Epoch 1860, val loss: 1.1190401315689087
Epoch 1870, training loss: 62.19829559326172 = 0.03885122761130333 + 10.0 * 6.215944290161133
Epoch 1870, val loss: 1.122090458869934
Epoch 1880, training loss: 62.18186569213867 = 0.03812841325998306 + 10.0 * 6.214373588562012
Epoch 1880, val loss: 1.1250720024108887
Epoch 1890, training loss: 62.178794860839844 = 0.03742876648902893 + 10.0 * 6.214136600494385
Epoch 1890, val loss: 1.1280403137207031
Epoch 1900, training loss: 62.2580451965332 = 0.03674659505486488 + 10.0 * 6.222129821777344
Epoch 1900, val loss: 1.1309196949005127
Epoch 1910, training loss: 62.21323776245117 = 0.03607761487364769 + 10.0 * 6.217715740203857
Epoch 1910, val loss: 1.1340210437774658
Epoch 1920, training loss: 62.18069076538086 = 0.035415858030319214 + 10.0 * 6.214527606964111
Epoch 1920, val loss: 1.13687264919281
Epoch 1930, training loss: 62.1708984375 = 0.03478620573878288 + 10.0 * 6.213611125946045
Epoch 1930, val loss: 1.139815330505371
Epoch 1940, training loss: 62.19656753540039 = 0.03417251259088516 + 10.0 * 6.2162394523620605
Epoch 1940, val loss: 1.1426212787628174
Epoch 1950, training loss: 62.16325378417969 = 0.033569417893886566 + 10.0 * 6.212968349456787
Epoch 1950, val loss: 1.145894169807434
Epoch 1960, training loss: 62.15176010131836 = 0.0329834446310997 + 10.0 * 6.211877822875977
Epoch 1960, val loss: 1.1486457586288452
Epoch 1970, training loss: 62.17547607421875 = 0.032420188188552856 + 10.0 * 6.214305400848389
Epoch 1970, val loss: 1.1514935493469238
Epoch 1980, training loss: 62.178340911865234 = 0.031855739653110504 + 10.0 * 6.214648246765137
Epoch 1980, val loss: 1.1544287204742432
Epoch 1990, training loss: 62.165462493896484 = 0.031298741698265076 + 10.0 * 6.21341609954834
Epoch 1990, val loss: 1.157206416130066
Epoch 2000, training loss: 62.143123626708984 = 0.03075915202498436 + 10.0 * 6.211236476898193
Epoch 2000, val loss: 1.160131812095642
Epoch 2010, training loss: 62.139827728271484 = 0.030241651460528374 + 10.0 * 6.210958480834961
Epoch 2010, val loss: 1.1629300117492676
Epoch 2020, training loss: 62.15060806274414 = 0.029736248776316643 + 10.0 * 6.212087154388428
Epoch 2020, val loss: 1.165876865386963
Epoch 2030, training loss: 62.21158981323242 = 0.029236258938908577 + 10.0 * 6.218235492706299
Epoch 2030, val loss: 1.1684855222702026
Epoch 2040, training loss: 62.16917419433594 = 0.028750134631991386 + 10.0 * 6.2140421867370605
Epoch 2040, val loss: 1.1712791919708252
Epoch 2050, training loss: 62.16117477416992 = 0.028270237147808075 + 10.0 * 6.213290214538574
Epoch 2050, val loss: 1.174187183380127
Epoch 2060, training loss: 62.15956115722656 = 0.027807334437966347 + 10.0 * 6.213175296783447
Epoch 2060, val loss: 1.1768211126327515
Epoch 2070, training loss: 62.14719772338867 = 0.027351390570402145 + 10.0 * 6.211984634399414
Epoch 2070, val loss: 1.179868221282959
Epoch 2080, training loss: 62.124366760253906 = 0.02690866030752659 + 10.0 * 6.20974588394165
Epoch 2080, val loss: 1.182313084602356
Epoch 2090, training loss: 62.12659454345703 = 0.026480285450816154 + 10.0 * 6.2100114822387695
Epoch 2090, val loss: 1.1853505373001099
Epoch 2100, training loss: 62.1591911315918 = 0.026064876466989517 + 10.0 * 6.21331262588501
Epoch 2100, val loss: 1.1881911754608154
Epoch 2110, training loss: 62.16019821166992 = 0.025646032765507698 + 10.0 * 6.2134552001953125
Epoch 2110, val loss: 1.1906368732452393
Epoch 2120, training loss: 62.14870834350586 = 0.025235529989004135 + 10.0 * 6.212347507476807
Epoch 2120, val loss: 1.1933780908584595
Epoch 2130, training loss: 62.118309020996094 = 0.024835551157593727 + 10.0 * 6.209347724914551
Epoch 2130, val loss: 1.195957064628601
Epoch 2140, training loss: 62.11112976074219 = 0.02445131354033947 + 10.0 * 6.208667755126953
Epoch 2140, val loss: 1.1987242698669434
Epoch 2150, training loss: 62.11635971069336 = 0.02407878451049328 + 10.0 * 6.209228038787842
Epoch 2150, val loss: 1.2012999057769775
Epoch 2160, training loss: 62.17485809326172 = 0.023713398724794388 + 10.0 * 6.215114593505859
Epoch 2160, val loss: 1.2038606405258179
Epoch 2170, training loss: 62.151611328125 = 0.023350536823272705 + 10.0 * 6.212826251983643
Epoch 2170, val loss: 1.2064603567123413
Epoch 2180, training loss: 62.13263702392578 = 0.022990457713603973 + 10.0 * 6.210964679718018
Epoch 2180, val loss: 1.209084153175354
Epoch 2190, training loss: 62.108306884765625 = 0.022640682756900787 + 10.0 * 6.208566665649414
Epoch 2190, val loss: 1.2117871046066284
Epoch 2200, training loss: 62.101078033447266 = 0.022304123267531395 + 10.0 * 6.207877159118652
Epoch 2200, val loss: 1.2142672538757324
Epoch 2210, training loss: 62.101383209228516 = 0.02197692170739174 + 10.0 * 6.207940578460693
Epoch 2210, val loss: 1.2169047594070435
Epoch 2220, training loss: 62.12949752807617 = 0.021656019613146782 + 10.0 * 6.210783958435059
Epoch 2220, val loss: 1.2193306684494019
Epoch 2230, training loss: 62.107086181640625 = 0.021341271698474884 + 10.0 * 6.2085747718811035
Epoch 2230, val loss: 1.2220841646194458
Epoch 2240, training loss: 62.13393783569336 = 0.021029146388173103 + 10.0 * 6.2112908363342285
Epoch 2240, val loss: 1.2243118286132812
Epoch 2250, training loss: 62.145713806152344 = 0.020718900486826897 + 10.0 * 6.212499618530273
Epoch 2250, val loss: 1.2267534732818604
Epoch 2260, training loss: 62.09657287597656 = 0.020417900756001472 + 10.0 * 6.207615852355957
Epoch 2260, val loss: 1.2291163206100464
Epoch 2270, training loss: 62.0889778137207 = 0.02012859471142292 + 10.0 * 6.206884860992432
Epoch 2270, val loss: 1.231832504272461
Epoch 2280, training loss: 62.0833625793457 = 0.019844893366098404 + 10.0 * 6.2063517570495605
Epoch 2280, val loss: 1.2343640327453613
Epoch 2290, training loss: 62.08323287963867 = 0.019571570679545403 + 10.0 * 6.206366062164307
Epoch 2290, val loss: 1.2367668151855469
Epoch 2300, training loss: 62.128013610839844 = 0.019302191212773323 + 10.0 * 6.21087121963501
Epoch 2300, val loss: 1.2390632629394531
Epoch 2310, training loss: 62.08197784423828 = 0.019033629447221756 + 10.0 * 6.206294059753418
Epoch 2310, val loss: 1.2415618896484375
Epoch 2320, training loss: 62.10029983520508 = 0.018772901967167854 + 10.0 * 6.208152770996094
Epoch 2320, val loss: 1.2438760995864868
Epoch 2330, training loss: 62.09178161621094 = 0.018511805683374405 + 10.0 * 6.207326889038086
Epoch 2330, val loss: 1.246023178100586
Epoch 2340, training loss: 62.09275436401367 = 0.018262963742017746 + 10.0 * 6.207448959350586
Epoch 2340, val loss: 1.2484657764434814
Epoch 2350, training loss: 62.076908111572266 = 0.018016202375292778 + 10.0 * 6.2058892250061035
Epoch 2350, val loss: 1.2509597539901733
Epoch 2360, training loss: 62.07291030883789 = 0.017778096720576286 + 10.0 * 6.2055134773254395
Epoch 2360, val loss: 1.253292202949524
Epoch 2370, training loss: 62.151798248291016 = 0.01754562370479107 + 10.0 * 6.213425636291504
Epoch 2370, val loss: 1.2557047605514526
Epoch 2380, training loss: 62.09592056274414 = 0.017305897548794746 + 10.0 * 6.207861423492432
Epoch 2380, val loss: 1.2573747634887695
Epoch 2390, training loss: 62.076881408691406 = 0.017078887671232224 + 10.0 * 6.20598030090332
Epoch 2390, val loss: 1.2601726055145264
Epoch 2400, training loss: 62.06766891479492 = 0.016855202615261078 + 10.0 * 6.205081462860107
Epoch 2400, val loss: 1.2622833251953125
Epoch 2410, training loss: 62.102561950683594 = 0.016641836613416672 + 10.0 * 6.208591938018799
Epoch 2410, val loss: 1.2645976543426514
Epoch 2420, training loss: 62.07960510253906 = 0.016424821689724922 + 10.0 * 6.206317901611328
Epoch 2420, val loss: 1.266745686531067
Epoch 2430, training loss: 62.0792236328125 = 0.016210850328207016 + 10.0 * 6.206301212310791
Epoch 2430, val loss: 1.268916130065918
Epoch 2440, training loss: 62.0544319152832 = 0.01600578799843788 + 10.0 * 6.203842639923096
Epoch 2440, val loss: 1.2712359428405762
Epoch 2450, training loss: 62.0588264465332 = 0.015807412564754486 + 10.0 * 6.204301834106445
Epoch 2450, val loss: 1.2733359336853027
Epoch 2460, training loss: 62.07447814941406 = 0.015612335875630379 + 10.0 * 6.205886363983154
Epoch 2460, val loss: 1.2755508422851562
Epoch 2470, training loss: 62.06776428222656 = 0.015416434034705162 + 10.0 * 6.205235004425049
Epoch 2470, val loss: 1.2776904106140137
Epoch 2480, training loss: 62.12220764160156 = 0.015226409770548344 + 10.0 * 6.210698127746582
Epoch 2480, val loss: 1.279767632484436
Epoch 2490, training loss: 62.057804107666016 = 0.015037313103675842 + 10.0 * 6.2042765617370605
Epoch 2490, val loss: 1.281986117362976
Epoch 2500, training loss: 62.0509147644043 = 0.01485175359994173 + 10.0 * 6.203606605529785
Epoch 2500, val loss: 1.2841676473617554
Epoch 2510, training loss: 62.069705963134766 = 0.014675012789666653 + 10.0 * 6.205502986907959
Epoch 2510, val loss: 1.286239504814148
Epoch 2520, training loss: 62.054744720458984 = 0.01449758093804121 + 10.0 * 6.204024791717529
Epoch 2520, val loss: 1.2883551120758057
Epoch 2530, training loss: 62.04623794555664 = 0.014325002208352089 + 10.0 * 6.20319128036499
Epoch 2530, val loss: 1.2904804944992065
Epoch 2540, training loss: 62.056541442871094 = 0.014157514087855816 + 10.0 * 6.204238414764404
Epoch 2540, val loss: 1.2924768924713135
Epoch 2550, training loss: 62.06290817260742 = 0.01399166788905859 + 10.0 * 6.204891681671143
Epoch 2550, val loss: 1.2944836616516113
Epoch 2560, training loss: 62.04813003540039 = 0.013825535774230957 + 10.0 * 6.203430652618408
Epoch 2560, val loss: 1.2964818477630615
Epoch 2570, training loss: 62.06783676147461 = 0.01366457436233759 + 10.0 * 6.205417156219482
Epoch 2570, val loss: 1.2983866930007935
Epoch 2580, training loss: 62.0439453125 = 0.013505842536687851 + 10.0 * 6.2030439376831055
Epoch 2580, val loss: 1.3004021644592285
Epoch 2590, training loss: 62.034751892089844 = 0.013350739143788815 + 10.0 * 6.202139854431152
Epoch 2590, val loss: 1.3025296926498413
Epoch 2600, training loss: 62.043975830078125 = 0.01319815032184124 + 10.0 * 6.203077793121338
Epoch 2600, val loss: 1.304423451423645
Epoch 2610, training loss: 62.089107513427734 = 0.013054289855062962 + 10.0 * 6.207605361938477
Epoch 2610, val loss: 1.3062670230865479
Epoch 2620, training loss: 62.04701614379883 = 0.012898572720587254 + 10.0 * 6.203412055969238
Epoch 2620, val loss: 1.30843985080719
Epoch 2630, training loss: 62.03278350830078 = 0.01275700144469738 + 10.0 * 6.20200252532959
Epoch 2630, val loss: 1.3102003335952759
Epoch 2640, training loss: 62.06759262084961 = 0.012617872096598148 + 10.0 * 6.2054972648620605
Epoch 2640, val loss: 1.3120278120040894
Epoch 2650, training loss: 62.02755355834961 = 0.012475740164518356 + 10.0 * 6.201508045196533
Epoch 2650, val loss: 1.3140963315963745
Epoch 2660, training loss: 62.03363800048828 = 0.012336901389062405 + 10.0 * 6.202130317687988
Epoch 2660, val loss: 1.3160595893859863
Epoch 2670, training loss: 62.027950286865234 = 0.012205902487039566 + 10.0 * 6.201574325561523
Epoch 2670, val loss: 1.3179380893707275
Epoch 2680, training loss: 62.08599090576172 = 0.012078327126801014 + 10.0 * 6.207391262054443
Epoch 2680, val loss: 1.319820523262024
Epoch 2690, training loss: 62.05812072753906 = 0.011940669268369675 + 10.0 * 6.204617977142334
Epoch 2690, val loss: 1.3212974071502686
Epoch 2700, training loss: 62.028770446777344 = 0.011811509728431702 + 10.0 * 6.201695919036865
Epoch 2700, val loss: 1.3232076168060303
Epoch 2710, training loss: 62.01587677001953 = 0.011686575599014759 + 10.0 * 6.200418949127197
Epoch 2710, val loss: 1.3250808715820312
Epoch 2720, training loss: 62.013221740722656 = 0.011565866880118847 + 10.0 * 6.200165748596191
Epoch 2720, val loss: 1.3268134593963623
Epoch 2730, training loss: 62.064064025878906 = 0.01144914049655199 + 10.0 * 6.205261707305908
Epoch 2730, val loss: 1.328694224357605
Epoch 2740, training loss: 62.02047348022461 = 0.011327006854116917 + 10.0 * 6.2009148597717285
Epoch 2740, val loss: 1.3302627801895142
Epoch 2750, training loss: 62.02610778808594 = 0.01120891235768795 + 10.0 * 6.2014899253845215
Epoch 2750, val loss: 1.332045316696167
Epoch 2760, training loss: 62.02083206176758 = 0.011092488653957844 + 10.0 * 6.200973987579346
Epoch 2760, val loss: 1.3337881565093994
Epoch 2770, training loss: 62.03760528564453 = 0.010979080572724342 + 10.0 * 6.202662467956543
Epoch 2770, val loss: 1.3353270292282104
Epoch 2780, training loss: 62.012939453125 = 0.01086645107716322 + 10.0 * 6.200207233428955
Epoch 2780, val loss: 1.3373184204101562
Epoch 2790, training loss: 62.012054443359375 = 0.01075727865099907 + 10.0 * 6.200129508972168
Epoch 2790, val loss: 1.3390053510665894
Epoch 2800, training loss: 62.0311279296875 = 0.01065011229366064 + 10.0 * 6.202047824859619
Epoch 2800, val loss: 1.3404853343963623
Epoch 2810, training loss: 62.02479553222656 = 0.01054137758910656 + 10.0 * 6.201425552368164
Epoch 2810, val loss: 1.3420779705047607
Epoch 2820, training loss: 62.01696014404297 = 0.010434610769152641 + 10.0 * 6.200652599334717
Epoch 2820, val loss: 1.343600869178772
Epoch 2830, training loss: 62.041656494140625 = 0.010332001373171806 + 10.0 * 6.203132629394531
Epoch 2830, val loss: 1.3452930450439453
Epoch 2840, training loss: 62.00130844116211 = 0.010229418985545635 + 10.0 * 6.199107646942139
Epoch 2840, val loss: 1.346989631652832
Epoch 2850, training loss: 62.002769470214844 = 0.010131938382983208 + 10.0 * 6.199263572692871
Epoch 2850, val loss: 1.3487526178359985
Epoch 2860, training loss: 62.003231048583984 = 0.010036121122539043 + 10.0 * 6.199319362640381
Epoch 2860, val loss: 1.3502103090286255
Epoch 2870, training loss: 62.05812072753906 = 0.009941549971699715 + 10.0 * 6.204817771911621
Epoch 2870, val loss: 1.3515585660934448
Epoch 2880, training loss: 62.010581970214844 = 0.009843173436820507 + 10.0 * 6.200074195861816
Epoch 2880, val loss: 1.353371262550354
Epoch 2890, training loss: 62.022029876708984 = 0.009747413918375969 + 10.0 * 6.201228141784668
Epoch 2890, val loss: 1.3547444343566895
Epoch 2900, training loss: 62.006282806396484 = 0.009654349647462368 + 10.0 * 6.199662685394287
Epoch 2900, val loss: 1.3563003540039062
Epoch 2910, training loss: 62.0047492980957 = 0.009563871659338474 + 10.0 * 6.19951868057251
Epoch 2910, val loss: 1.358006477355957
Epoch 2920, training loss: 62.000465393066406 = 0.009474632330238819 + 10.0 * 6.199099063873291
Epoch 2920, val loss: 1.3594579696655273
Epoch 2930, training loss: 61.99374008178711 = 0.009386430494487286 + 10.0 * 6.198435306549072
Epoch 2930, val loss: 1.3609586954116821
Epoch 2940, training loss: 62.02460861206055 = 0.009300868958234787 + 10.0 * 6.201530933380127
Epoch 2940, val loss: 1.3621914386749268
Epoch 2950, training loss: 61.99382019042969 = 0.009215672500431538 + 10.0 * 6.198460578918457
Epoch 2950, val loss: 1.3638938665390015
Epoch 2960, training loss: 62.01090621948242 = 0.009134557098150253 + 10.0 * 6.200177192687988
Epoch 2960, val loss: 1.365281343460083
Epoch 2970, training loss: 62.00558853149414 = 0.009048432111740112 + 10.0 * 6.1996541023254395
Epoch 2970, val loss: 1.3667651414871216
Epoch 2980, training loss: 61.98726272583008 = 0.008965730667114258 + 10.0 * 6.197829723358154
Epoch 2980, val loss: 1.3680830001831055
Epoch 2990, training loss: 61.98993682861328 = 0.008886572904884815 + 10.0 * 6.1981048583984375
Epoch 2990, val loss: 1.369814395904541
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.812335266209805
The final CL Acc:0.66914, 0.02310, The final GNN Acc:0.81040, 0.00151
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13116])
remove edge: torch.Size([2, 7996])
updated graph: torch.Size([2, 10556])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.90811920166016 = 1.939950704574585 + 10.0 * 8.596817016601562
Epoch 0, val loss: 1.9408400058746338
Epoch 10, training loss: 87.88977813720703 = 1.9303961992263794 + 10.0 * 8.595937728881836
Epoch 10, val loss: 1.930953860282898
Epoch 20, training loss: 87.80652618408203 = 1.9182212352752686 + 10.0 * 8.588830947875977
Epoch 20, val loss: 1.9182696342468262
Epoch 30, training loss: 87.32207489013672 = 1.9026813507080078 + 10.0 * 8.541939735412598
Epoch 30, val loss: 1.9022095203399658
Epoch 40, training loss: 84.94658660888672 = 1.8855525255203247 + 10.0 * 8.306103706359863
Epoch 40, val loss: 1.8848545551300049
Epoch 50, training loss: 79.99069213867188 = 1.86703622341156 + 10.0 * 7.812365531921387
Epoch 50, val loss: 1.8660151958465576
Epoch 60, training loss: 76.2218246459961 = 1.850101113319397 + 10.0 * 7.4371724128723145
Epoch 60, val loss: 1.8493009805679321
Epoch 70, training loss: 72.60435485839844 = 1.8348206281661987 + 10.0 * 7.076953411102295
Epoch 70, val loss: 1.8342645168304443
Epoch 80, training loss: 70.95586395263672 = 1.820443868637085 + 10.0 * 6.913541793823242
Epoch 80, val loss: 1.8206744194030762
Epoch 90, training loss: 69.68560791015625 = 1.8095825910568237 + 10.0 * 6.787602424621582
Epoch 90, val loss: 1.8103365898132324
Epoch 100, training loss: 69.04058837890625 = 1.8000714778900146 + 10.0 * 6.7240519523620605
Epoch 100, val loss: 1.8013379573822021
Epoch 110, training loss: 68.48419189453125 = 1.7897354364395142 + 10.0 * 6.669445514678955
Epoch 110, val loss: 1.7919100522994995
Epoch 120, training loss: 68.01055908203125 = 1.7799105644226074 + 10.0 * 6.623064994812012
Epoch 120, val loss: 1.7830954790115356
Epoch 130, training loss: 67.64227294921875 = 1.770628809928894 + 10.0 * 6.587164402008057
Epoch 130, val loss: 1.7745907306671143
Epoch 140, training loss: 67.31861114501953 = 1.760917067527771 + 10.0 * 6.555769443511963
Epoch 140, val loss: 1.7656456232070923
Epoch 150, training loss: 67.05217742919922 = 1.7504976987838745 + 10.0 * 6.530168056488037
Epoch 150, val loss: 1.7561249732971191
Epoch 160, training loss: 66.77574920654297 = 1.7391170263290405 + 10.0 * 6.503663063049316
Epoch 160, val loss: 1.7461436986923218
Epoch 170, training loss: 66.5313949584961 = 1.7267496585845947 + 10.0 * 6.480464458465576
Epoch 170, val loss: 1.7356185913085938
Epoch 180, training loss: 66.34297180175781 = 1.7131834030151367 + 10.0 * 6.462979316711426
Epoch 180, val loss: 1.7240478992462158
Epoch 190, training loss: 66.18710327148438 = 1.6981512308120728 + 10.0 * 6.448895454406738
Epoch 190, val loss: 1.711081624031067
Epoch 200, training loss: 66.01610565185547 = 1.6814345121383667 + 10.0 * 6.433466911315918
Epoch 200, val loss: 1.6967262029647827
Epoch 210, training loss: 65.87171173095703 = 1.6631478071212769 + 10.0 * 6.420856475830078
Epoch 210, val loss: 1.6810576915740967
Epoch 220, training loss: 65.76172637939453 = 1.643302083015442 + 10.0 * 6.411842346191406
Epoch 220, val loss: 1.6641485691070557
Epoch 230, training loss: 65.62617492675781 = 1.6218605041503906 + 10.0 * 6.400432109832764
Epoch 230, val loss: 1.6459743976593018
Epoch 240, training loss: 65.5098648071289 = 1.5989971160888672 + 10.0 * 6.391086578369141
Epoch 240, val loss: 1.626561164855957
Epoch 250, training loss: 65.40270233154297 = 1.5747714042663574 + 10.0 * 6.382793426513672
Epoch 250, val loss: 1.6060327291488647
Epoch 260, training loss: 65.31893920898438 = 1.5490626096725464 + 10.0 * 6.376987457275391
Epoch 260, val loss: 1.5843522548675537
Epoch 270, training loss: 65.21686553955078 = 1.5223243236541748 + 10.0 * 6.369454383850098
Epoch 270, val loss: 1.5617436170578003
Epoch 280, training loss: 65.11943054199219 = 1.4946643114089966 + 10.0 * 6.362476348876953
Epoch 280, val loss: 1.5385146141052246
Epoch 290, training loss: 65.0345230102539 = 1.466321349143982 + 10.0 * 6.356820106506348
Epoch 290, val loss: 1.5146857500076294
Epoch 300, training loss: 65.0077133178711 = 1.4373314380645752 + 10.0 * 6.3570380210876465
Epoch 300, val loss: 1.4907050132751465
Epoch 310, training loss: 64.88512420654297 = 1.4082332849502563 + 10.0 * 6.347689151763916
Epoch 310, val loss: 1.4664777517318726
Epoch 320, training loss: 64.79765319824219 = 1.3791191577911377 + 10.0 * 6.341853141784668
Epoch 320, val loss: 1.4426382780075073
Epoch 330, training loss: 64.7208023071289 = 1.3499963283538818 + 10.0 * 6.337080478668213
Epoch 330, val loss: 1.4190436601638794
Epoch 340, training loss: 64.71143341064453 = 1.3210066556930542 + 10.0 * 6.339043140411377
Epoch 340, val loss: 1.3959431648254395
Epoch 350, training loss: 64.59548950195312 = 1.2922834157943726 + 10.0 * 6.330320358276367
Epoch 350, val loss: 1.3732469081878662
Epoch 360, training loss: 64.52471160888672 = 1.264015555381775 + 10.0 * 6.326069355010986
Epoch 360, val loss: 1.3513034582138062
Epoch 370, training loss: 64.49421691894531 = 1.2361944913864136 + 10.0 * 6.325802326202393
Epoch 370, val loss: 1.3301929235458374
Epoch 380, training loss: 64.40607452392578 = 1.2088820934295654 + 10.0 * 6.319719314575195
Epoch 380, val loss: 1.309875249862671
Epoch 390, training loss: 64.3330307006836 = 1.1821600198745728 + 10.0 * 6.315086841583252
Epoch 390, val loss: 1.2905304431915283
Epoch 400, training loss: 64.29454040527344 = 1.1560139656066895 + 10.0 * 6.313852787017822
Epoch 400, val loss: 1.2719712257385254
Epoch 410, training loss: 64.26520538330078 = 1.1303952932357788 + 10.0 * 6.313480854034424
Epoch 410, val loss: 1.2542377710342407
Epoch 420, training loss: 64.18270111083984 = 1.1053911447525024 + 10.0 * 6.3077311515808105
Epoch 420, val loss: 1.2373560667037964
Epoch 430, training loss: 64.12039947509766 = 1.0809074640274048 + 10.0 * 6.303948879241943
Epoch 430, val loss: 1.2211555242538452
Epoch 440, training loss: 64.06890106201172 = 1.0569192171096802 + 10.0 * 6.3011980056762695
Epoch 440, val loss: 1.2056533098220825
Epoch 450, training loss: 64.10161590576172 = 1.0333014726638794 + 10.0 * 6.3068318367004395
Epoch 450, val loss: 1.190651774406433
Epoch 460, training loss: 64.00758361816406 = 1.0100548267364502 + 10.0 * 6.299753189086914
Epoch 460, val loss: 1.1761811971664429
Epoch 470, training loss: 63.940555572509766 = 0.9873403310775757 + 10.0 * 6.295321464538574
Epoch 470, val loss: 1.1622343063354492
Epoch 480, training loss: 63.89004135131836 = 0.9650015830993652 + 10.0 * 6.29250431060791
Epoch 480, val loss: 1.1487749814987183
Epoch 490, training loss: 63.874847412109375 = 0.9429392218589783 + 10.0 * 6.293190956115723
Epoch 490, val loss: 1.135810375213623
Epoch 500, training loss: 63.84471130371094 = 0.9212482571601868 + 10.0 * 6.292346000671387
Epoch 500, val loss: 1.1228814125061035
Epoch 510, training loss: 63.76900100708008 = 0.8999579548835754 + 10.0 * 6.286904335021973
Epoch 510, val loss: 1.1106539964675903
Epoch 520, training loss: 63.72404861450195 = 0.8790537118911743 + 10.0 * 6.284499168395996
Epoch 520, val loss: 1.0988726615905762
Epoch 530, training loss: 63.68633270263672 = 0.8585667610168457 + 10.0 * 6.282776832580566
Epoch 530, val loss: 1.0875540971755981
Epoch 540, training loss: 63.658721923828125 = 0.8383779525756836 + 10.0 * 6.282034397125244
Epoch 540, val loss: 1.0765635967254639
Epoch 550, training loss: 63.614418029785156 = 0.8187058568000793 + 10.0 * 6.279571056365967
Epoch 550, val loss: 1.0659894943237305
Epoch 560, training loss: 63.57756423950195 = 0.7995167374610901 + 10.0 * 6.277804374694824
Epoch 560, val loss: 1.0559769868850708
Epoch 570, training loss: 63.5406379699707 = 0.7807477116584778 + 10.0 * 6.275989055633545
Epoch 570, val loss: 1.0464671850204468
Epoch 580, training loss: 63.505367279052734 = 0.762395441532135 + 10.0 * 6.27429723739624
Epoch 580, val loss: 1.0373985767364502
Epoch 590, training loss: 63.567596435546875 = 0.74446702003479 + 10.0 * 6.282312870025635
Epoch 590, val loss: 1.028857707977295
Epoch 600, training loss: 63.452293395996094 = 0.7267547249794006 + 10.0 * 6.27255392074585
Epoch 600, val loss: 1.0203598737716675
Epoch 610, training loss: 63.4162712097168 = 0.7096598744392395 + 10.0 * 6.270661354064941
Epoch 610, val loss: 1.0125409364700317
Epoch 620, training loss: 63.38499450683594 = 0.6929421424865723 + 10.0 * 6.269205093383789
Epoch 620, val loss: 1.0052496194839478
Epoch 630, training loss: 63.41449737548828 = 0.6765439510345459 + 10.0 * 6.273795127868652
Epoch 630, val loss: 0.998359739780426
Epoch 640, training loss: 63.338417053222656 = 0.6605512499809265 + 10.0 * 6.267786502838135
Epoch 640, val loss: 0.9916646480560303
Epoch 650, training loss: 63.29310607910156 = 0.6448290348052979 + 10.0 * 6.264827728271484
Epoch 650, val loss: 0.9855172634124756
Epoch 660, training loss: 63.26701354980469 = 0.6294348835945129 + 10.0 * 6.263757705688477
Epoch 660, val loss: 0.9796539545059204
Epoch 670, training loss: 63.31656265258789 = 0.6142836213111877 + 10.0 * 6.270227909088135
Epoch 670, val loss: 0.9739068150520325
Epoch 680, training loss: 63.220375061035156 = 0.5993799567222595 + 10.0 * 6.262099266052246
Epoch 680, val loss: 0.9688034653663635
Epoch 690, training loss: 63.199607849121094 = 0.5847432017326355 + 10.0 * 6.261486530303955
Epoch 690, val loss: 0.963989794254303
Epoch 700, training loss: 63.182682037353516 = 0.5703760385513306 + 10.0 * 6.26123046875
Epoch 700, val loss: 0.9592645764350891
Epoch 710, training loss: 63.136329650878906 = 0.5561779737472534 + 10.0 * 6.258015155792236
Epoch 710, val loss: 0.9548778533935547
Epoch 720, training loss: 63.125648498535156 = 0.5422291159629822 + 10.0 * 6.2583417892456055
Epoch 720, val loss: 0.9509516954421997
Epoch 730, training loss: 63.10051345825195 = 0.5284749269485474 + 10.0 * 6.257203578948975
Epoch 730, val loss: 0.9469356536865234
Epoch 740, training loss: 63.08250045776367 = 0.5149943828582764 + 10.0 * 6.256750583648682
Epoch 740, val loss: 0.9434551000595093
Epoch 750, training loss: 63.04396057128906 = 0.5017628073692322 + 10.0 * 6.254220008850098
Epoch 750, val loss: 0.9402468800544739
Epoch 760, training loss: 63.02246856689453 = 0.48877692222595215 + 10.0 * 6.253369331359863
Epoch 760, val loss: 0.9372034668922424
Epoch 770, training loss: 63.084625244140625 = 0.47595903277397156 + 10.0 * 6.260866641998291
Epoch 770, val loss: 0.9344062209129333
Epoch 780, training loss: 62.990013122558594 = 0.46330133080482483 + 10.0 * 6.252671241760254
Epoch 780, val loss: 0.931783139705658
Epoch 790, training loss: 62.965065002441406 = 0.4509563744068146 + 10.0 * 6.251410484313965
Epoch 790, val loss: 0.9293799996376038
Epoch 800, training loss: 62.93885803222656 = 0.43884092569351196 + 10.0 * 6.250001430511475
Epoch 800, val loss: 0.9271677136421204
Epoch 810, training loss: 62.96174621582031 = 0.42696958780288696 + 10.0 * 6.253477573394775
Epoch 810, val loss: 0.9250842332839966
Epoch 820, training loss: 62.98170471191406 = 0.41517746448516846 + 10.0 * 6.25665283203125
Epoch 820, val loss: 0.923348069190979
Epoch 830, training loss: 62.89011001586914 = 0.40366145968437195 + 10.0 * 6.248644828796387
Epoch 830, val loss: 0.921493411064148
Epoch 840, training loss: 62.86534118652344 = 0.39247119426727295 + 10.0 * 6.247286796569824
Epoch 840, val loss: 0.9200725555419922
Epoch 850, training loss: 62.8424072265625 = 0.3814679682254791 + 10.0 * 6.24609375
Epoch 850, val loss: 0.9187856316566467
Epoch 860, training loss: 62.82437515258789 = 0.3706945478916168 + 10.0 * 6.245368003845215
Epoch 860, val loss: 0.9177166819572449
Epoch 870, training loss: 62.83171844482422 = 0.36012449860572815 + 10.0 * 6.247159481048584
Epoch 870, val loss: 0.9168475270271301
Epoch 880, training loss: 62.79442596435547 = 0.3497679829597473 + 10.0 * 6.2444658279418945
Epoch 880, val loss: 0.9157377481460571
Epoch 890, training loss: 62.792823791503906 = 0.3396260440349579 + 10.0 * 6.245319843292236
Epoch 890, val loss: 0.9151596426963806
Epoch 900, training loss: 62.76744079589844 = 0.3297874331474304 + 10.0 * 6.243765354156494
Epoch 900, val loss: 0.914413571357727
Epoch 910, training loss: 62.743072509765625 = 0.3201596140861511 + 10.0 * 6.242291450500488
Epoch 910, val loss: 0.9141757488250732
Epoch 920, training loss: 62.72640609741211 = 0.31081944704055786 + 10.0 * 6.241559028625488
Epoch 920, val loss: 0.9139591455459595
Epoch 930, training loss: 62.75871276855469 = 0.3017139434814453 + 10.0 * 6.245699882507324
Epoch 930, val loss: 0.9138118028640747
Epoch 940, training loss: 62.711509704589844 = 0.2927864193916321 + 10.0 * 6.241872310638428
Epoch 940, val loss: 0.9139221906661987
Epoch 950, training loss: 62.68827819824219 = 0.2841443121433258 + 10.0 * 6.240413188934326
Epoch 950, val loss: 0.9140526652336121
Epoch 960, training loss: 62.66807556152344 = 0.2757466733455658 + 10.0 * 6.239233016967773
Epoch 960, val loss: 0.9144909381866455
Epoch 970, training loss: 62.68497085571289 = 0.2676262855529785 + 10.0 * 6.241734504699707
Epoch 970, val loss: 0.9151095747947693
Epoch 980, training loss: 62.70116424560547 = 0.2596031725406647 + 10.0 * 6.244156360626221
Epoch 980, val loss: 0.9153271317481995
Epoch 990, training loss: 62.630592346191406 = 0.2519194185733795 + 10.0 * 6.23786735534668
Epoch 990, val loss: 0.9161677956581116
Epoch 1000, training loss: 62.61221694946289 = 0.24450907111167908 + 10.0 * 6.2367706298828125
Epoch 1000, val loss: 0.9171952605247498
Epoch 1010, training loss: 62.603294372558594 = 0.23733988404273987 + 10.0 * 6.236595630645752
Epoch 1010, val loss: 0.9183858036994934
Epoch 1020, training loss: 62.65321350097656 = 0.23036326467990875 + 10.0 * 6.242285251617432
Epoch 1020, val loss: 0.919639527797699
Epoch 1030, training loss: 62.60061264038086 = 0.22364114224910736 + 10.0 * 6.237697124481201
Epoch 1030, val loss: 0.9208838939666748
Epoch 1040, training loss: 62.57265853881836 = 0.21706750988960266 + 10.0 * 6.235558986663818
Epoch 1040, val loss: 0.9224959015846252
Epoch 1050, training loss: 62.56565856933594 = 0.2107902318239212 + 10.0 * 6.23548698425293
Epoch 1050, val loss: 0.9239791035652161
Epoch 1060, training loss: 62.571250915527344 = 0.2046511024236679 + 10.0 * 6.236660003662109
Epoch 1060, val loss: 0.9257166385650635
Epoch 1070, training loss: 62.59373092651367 = 0.1987154185771942 + 10.0 * 6.239501476287842
Epoch 1070, val loss: 0.9276706576347351
Epoch 1080, training loss: 62.530860900878906 = 0.19296790659427643 + 10.0 * 6.233789443969727
Epoch 1080, val loss: 0.9297628998756409
Epoch 1090, training loss: 62.51382827758789 = 0.18745246529579163 + 10.0 * 6.232637405395508
Epoch 1090, val loss: 0.9318148493766785
Epoch 1100, training loss: 62.500728607177734 = 0.18212199211120605 + 10.0 * 6.231860637664795
Epoch 1100, val loss: 0.9341912865638733
Epoch 1110, training loss: 62.490806579589844 = 0.1769661158323288 + 10.0 * 6.231383800506592
Epoch 1110, val loss: 0.9366106986999512
Epoch 1120, training loss: 62.53117370605469 = 0.17197281122207642 + 10.0 * 6.235919952392578
Epoch 1120, val loss: 0.939207136631012
Epoch 1130, training loss: 62.513648986816406 = 0.1670820713043213 + 10.0 * 6.23465633392334
Epoch 1130, val loss: 0.9415613412857056
Epoch 1140, training loss: 62.474517822265625 = 0.16238899528980255 + 10.0 * 6.231213092803955
Epoch 1140, val loss: 0.9442889094352722
Epoch 1150, training loss: 62.47322082519531 = 0.15785309672355652 + 10.0 * 6.231536865234375
Epoch 1150, val loss: 0.9471606612205505
Epoch 1160, training loss: 62.446571350097656 = 0.15346643328666687 + 10.0 * 6.229310512542725
Epoch 1160, val loss: 0.9499785304069519
Epoch 1170, training loss: 62.434417724609375 = 0.14922654628753662 + 10.0 * 6.228518962860107
Epoch 1170, val loss: 0.9531815052032471
Epoch 1180, training loss: 62.50543212890625 = 0.14511968195438385 + 10.0 * 6.236031532287598
Epoch 1180, val loss: 0.9564656615257263
Epoch 1190, training loss: 62.46135330200195 = 0.14117540419101715 + 10.0 * 6.232017993927002
Epoch 1190, val loss: 0.9594536423683167
Epoch 1200, training loss: 62.41205596923828 = 0.13729380071163177 + 10.0 * 6.227476119995117
Epoch 1200, val loss: 0.9628074169158936
Epoch 1210, training loss: 62.40691375732422 = 0.1335783749818802 + 10.0 * 6.2273335456848145
Epoch 1210, val loss: 0.9663327932357788
Epoch 1220, training loss: 62.40876007080078 = 0.13000445067882538 + 10.0 * 6.227875709533691
Epoch 1220, val loss: 0.9698874950408936
Epoch 1230, training loss: 62.4071044921875 = 0.12652315199375153 + 10.0 * 6.228058338165283
Epoch 1230, val loss: 0.9735049605369568
Epoch 1240, training loss: 62.47128677368164 = 0.12316711246967316 + 10.0 * 6.234811782836914
Epoch 1240, val loss: 0.9770747423171997
Epoch 1250, training loss: 62.402034759521484 = 0.1198587715625763 + 10.0 * 6.228217601776123
Epoch 1250, val loss: 0.9807887077331543
Epoch 1260, training loss: 62.366798400878906 = 0.11670637875795364 + 10.0 * 6.225008964538574
Epoch 1260, val loss: 0.9845519065856934
Epoch 1270, training loss: 62.35572814941406 = 0.11365792155265808 + 10.0 * 6.224206924438477
Epoch 1270, val loss: 0.988484799861908
Epoch 1280, training loss: 62.35996627807617 = 0.11071330308914185 + 10.0 * 6.2249250411987305
Epoch 1280, val loss: 0.9923102259635925
Epoch 1290, training loss: 62.366485595703125 = 0.10782440751791 + 10.0 * 6.225865840911865
Epoch 1290, val loss: 0.9962420463562012
Epoch 1300, training loss: 62.337345123291016 = 0.10502776503562927 + 10.0 * 6.223231792449951
Epoch 1300, val loss: 1.0002543926239014
Epoch 1310, training loss: 62.33543014526367 = 0.10232328623533249 + 10.0 * 6.223310947418213
Epoch 1310, val loss: 1.0042874813079834
Epoch 1320, training loss: 62.32687759399414 = 0.09970803558826447 + 10.0 * 6.222716808319092
Epoch 1320, val loss: 1.0083853006362915
Epoch 1330, training loss: 62.36225128173828 = 0.09716176986694336 + 10.0 * 6.226509094238281
Epoch 1330, val loss: 1.012614369392395
Epoch 1340, training loss: 62.42070770263672 = 0.09465688467025757 + 10.0 * 6.23260498046875
Epoch 1340, val loss: 1.0165700912475586
Epoch 1350, training loss: 62.317134857177734 = 0.0922565832734108 + 10.0 * 6.222487449645996
Epoch 1350, val loss: 1.0205304622650146
Epoch 1360, training loss: 62.310298919677734 = 0.08992457389831543 + 10.0 * 6.222037315368652
Epoch 1360, val loss: 1.0248534679412842
Epoch 1370, training loss: 62.292232513427734 = 0.08768501877784729 + 10.0 * 6.220454692840576
Epoch 1370, val loss: 1.029068946838379
Epoch 1380, training loss: 62.288543701171875 = 0.08551793545484543 + 10.0 * 6.220302581787109
Epoch 1380, val loss: 1.0333166122436523
Epoch 1390, training loss: 62.299076080322266 = 0.08341433852910995 + 10.0 * 6.221566200256348
Epoch 1390, val loss: 1.0375299453735352
Epoch 1400, training loss: 62.293914794921875 = 0.08133364468812943 + 10.0 * 6.221258163452148
Epoch 1400, val loss: 1.0418481826782227
Epoch 1410, training loss: 62.28620147705078 = 0.07932233065366745 + 10.0 * 6.2206878662109375
Epoch 1410, val loss: 1.0461833477020264
Epoch 1420, training loss: 62.27583694458008 = 0.07737495750188828 + 10.0 * 6.219846248626709
Epoch 1420, val loss: 1.050458312034607
Epoch 1430, training loss: 62.261165618896484 = 0.0754983052611351 + 10.0 * 6.21856689453125
Epoch 1430, val loss: 1.0547175407409668
Epoch 1440, training loss: 62.25846481323242 = 0.0736793577671051 + 10.0 * 6.218478202819824
Epoch 1440, val loss: 1.0591965913772583
Epoch 1450, training loss: 62.299468994140625 = 0.07190877944231033 + 10.0 * 6.222756385803223
Epoch 1450, val loss: 1.0635013580322266
Epoch 1460, training loss: 62.282962799072266 = 0.07017549127340317 + 10.0 * 6.221278667449951
Epoch 1460, val loss: 1.0674068927764893
Epoch 1470, training loss: 62.2653923034668 = 0.06849036365747452 + 10.0 * 6.219690322875977
Epoch 1470, val loss: 1.0721046924591064
Epoch 1480, training loss: 62.27484893798828 = 0.06686969846487045 + 10.0 * 6.220798015594482
Epoch 1480, val loss: 1.0761563777923584
Epoch 1490, training loss: 62.232627868652344 = 0.06527617573738098 + 10.0 * 6.216734886169434
Epoch 1490, val loss: 1.0806156396865845
Epoch 1500, training loss: 62.23521041870117 = 0.06375156342983246 + 10.0 * 6.217145919799805
Epoch 1500, val loss: 1.0850117206573486
Epoch 1510, training loss: 62.23701477050781 = 0.06227536126971245 + 10.0 * 6.217473983764648
Epoch 1510, val loss: 1.0893014669418335
Epoch 1520, training loss: 62.237022399902344 = 0.06083584576845169 + 10.0 * 6.217618465423584
Epoch 1520, val loss: 1.0935851335525513
Epoch 1530, training loss: 62.223995208740234 = 0.05943996459245682 + 10.0 * 6.216455459594727
Epoch 1530, val loss: 1.0977075099945068
Epoch 1540, training loss: 62.256195068359375 = 0.05808074027299881 + 10.0 * 6.21981143951416
Epoch 1540, val loss: 1.1019058227539062
Epoch 1550, training loss: 62.22222137451172 = 0.056749895215034485 + 10.0 * 6.216547012329102
Epoch 1550, val loss: 1.106110692024231
Epoch 1560, training loss: 62.22977828979492 = 0.05547439306974411 + 10.0 * 6.217430591583252
Epoch 1560, val loss: 1.1103134155273438
Epoch 1570, training loss: 62.207427978515625 = 0.05421967804431915 + 10.0 * 6.215321063995361
Epoch 1570, val loss: 1.1144376993179321
Epoch 1580, training loss: 62.2040901184082 = 0.05301565304398537 + 10.0 * 6.215107440948486
Epoch 1580, val loss: 1.1187328100204468
Epoch 1590, training loss: 62.21356201171875 = 0.05185447633266449 + 10.0 * 6.216170787811279
Epoch 1590, val loss: 1.1228421926498413
Epoch 1600, training loss: 62.207763671875 = 0.05070845037698746 + 10.0 * 6.215705394744873
Epoch 1600, val loss: 1.1269115209579468
Epoch 1610, training loss: 62.19483184814453 = 0.049583278596401215 + 10.0 * 6.214524745941162
Epoch 1610, val loss: 1.1310137510299683
Epoch 1620, training loss: 62.193824768066406 = 0.048519428819417953 + 10.0 * 6.2145304679870605
Epoch 1620, val loss: 1.1354329586029053
Epoch 1630, training loss: 62.21174621582031 = 0.04747408255934715 + 10.0 * 6.216427326202393
Epoch 1630, val loss: 1.1391438245773315
Epoch 1640, training loss: 62.18745422363281 = 0.046458013355731964 + 10.0 * 6.214099407196045
Epoch 1640, val loss: 1.1434391736984253
Epoch 1650, training loss: 62.18586349487305 = 0.04547388106584549 + 10.0 * 6.214038848876953
Epoch 1650, val loss: 1.1473944187164307
Epoch 1660, training loss: 62.21137237548828 = 0.04451293870806694 + 10.0 * 6.216685771942139
Epoch 1660, val loss: 1.151660680770874
Epoch 1670, training loss: 62.18865203857422 = 0.04357483610510826 + 10.0 * 6.214507579803467
Epoch 1670, val loss: 1.155658483505249
Epoch 1680, training loss: 62.16374206542969 = 0.042671121656894684 + 10.0 * 6.212107181549072
Epoch 1680, val loss: 1.1594499349594116
Epoch 1690, training loss: 62.157203674316406 = 0.04180118069052696 + 10.0 * 6.211540222167969
Epoch 1690, val loss: 1.163672924041748
Epoch 1700, training loss: 62.18501663208008 = 0.04095715656876564 + 10.0 * 6.2144060134887695
Epoch 1700, val loss: 1.1675864458084106
Epoch 1710, training loss: 62.15999984741211 = 0.0401168130338192 + 10.0 * 6.21198844909668
Epoch 1710, val loss: 1.1715283393859863
Epoch 1720, training loss: 62.15093231201172 = 0.03930279240012169 + 10.0 * 6.21116304397583
Epoch 1720, val loss: 1.1754229068756104
Epoch 1730, training loss: 62.15530014038086 = 0.03852448612451553 + 10.0 * 6.211677551269531
Epoch 1730, val loss: 1.1795117855072021
Epoch 1740, training loss: 62.21272659301758 = 0.037757907062768936 + 10.0 * 6.217496871948242
Epoch 1740, val loss: 1.1832002401351929
Epoch 1750, training loss: 62.16456985473633 = 0.03700476512312889 + 10.0 * 6.212756156921387
Epoch 1750, val loss: 1.1873549222946167
Epoch 1760, training loss: 62.134342193603516 = 0.03627999871969223 + 10.0 * 6.209805965423584
Epoch 1760, val loss: 1.1910356283187866
Epoch 1770, training loss: 62.133056640625 = 0.03558498993515968 + 10.0 * 6.209747314453125
Epoch 1770, val loss: 1.1949071884155273
Epoch 1780, training loss: 62.12993621826172 = 0.0349108912050724 + 10.0 * 6.209502220153809
Epoch 1780, val loss: 1.1988221406936646
Epoch 1790, training loss: 62.16478729248047 = 0.03426215425133705 + 10.0 * 6.213052272796631
Epoch 1790, val loss: 1.2024253606796265
Epoch 1800, training loss: 62.12749481201172 = 0.03360147774219513 + 10.0 * 6.209389686584473
Epoch 1800, val loss: 1.2063581943511963
Epoch 1810, training loss: 62.14366149902344 = 0.0329664871096611 + 10.0 * 6.211069583892822
Epoch 1810, val loss: 1.2100145816802979
Epoch 1820, training loss: 62.11460494995117 = 0.0323479101061821 + 10.0 * 6.208225727081299
Epoch 1820, val loss: 1.213808536529541
Epoch 1830, training loss: 62.11540603637695 = 0.03175096586346626 + 10.0 * 6.208365440368652
Epoch 1830, val loss: 1.2173686027526855
Epoch 1840, training loss: 62.111568450927734 = 0.031176216900348663 + 10.0 * 6.208039283752441
Epoch 1840, val loss: 1.2212061882019043
Epoch 1850, training loss: 62.1547737121582 = 0.030615614727139473 + 10.0 * 6.21241569519043
Epoch 1850, val loss: 1.224879503250122
Epoch 1860, training loss: 62.144981384277344 = 0.030054759234189987 + 10.0 * 6.211492538452148
Epoch 1860, val loss: 1.228291392326355
Epoch 1870, training loss: 62.12031555175781 = 0.029508965089917183 + 10.0 * 6.209080696105957
Epoch 1870, val loss: 1.2319729328155518
Epoch 1880, training loss: 62.106754302978516 = 0.028984002768993378 + 10.0 * 6.20777702331543
Epoch 1880, val loss: 1.2356324195861816
Epoch 1890, training loss: 62.100128173828125 = 0.028477201238274574 + 10.0 * 6.207165241241455
Epoch 1890, val loss: 1.239311933517456
Epoch 1900, training loss: 62.112300872802734 = 0.02798844501376152 + 10.0 * 6.208431243896484
Epoch 1900, val loss: 1.2429028749465942
Epoch 1910, training loss: 62.12248229980469 = 0.02750382386147976 + 10.0 * 6.209497928619385
Epoch 1910, val loss: 1.2461810111999512
Epoch 1920, training loss: 62.1120719909668 = 0.027019791305065155 + 10.0 * 6.208505153656006
Epoch 1920, val loss: 1.249519944190979
Epoch 1930, training loss: 62.10186767578125 = 0.026552565395832062 + 10.0 * 6.207531452178955
Epoch 1930, val loss: 1.2533677816390991
Epoch 1940, training loss: 62.11362075805664 = 0.02609865553677082 + 10.0 * 6.208752155303955
Epoch 1940, val loss: 1.2568880319595337
Epoch 1950, training loss: 62.08826446533203 = 0.025660661980509758 + 10.0 * 6.2062602043151855
Epoch 1950, val loss: 1.2598272562026978
Epoch 1960, training loss: 62.08426284790039 = 0.025234902277588844 + 10.0 * 6.205903053283691
Epoch 1960, val loss: 1.2635730504989624
Epoch 1970, training loss: 62.08849334716797 = 0.02481880411505699 + 10.0 * 6.206367492675781
Epoch 1970, val loss: 1.266883134841919
Epoch 1980, training loss: 62.11466979980469 = 0.024414746090769768 + 10.0 * 6.2090253829956055
Epoch 1980, val loss: 1.2702659368515015
Epoch 1990, training loss: 62.08402633666992 = 0.024014830589294434 + 10.0 * 6.206001281738281
Epoch 1990, val loss: 1.27344810962677
Epoch 2000, training loss: 62.11492156982422 = 0.023626243695616722 + 10.0 * 6.209129810333252
Epoch 2000, val loss: 1.276782751083374
Epoch 2010, training loss: 62.10322570800781 = 0.023232094943523407 + 10.0 * 6.207999229431152
Epoch 2010, val loss: 1.2801604270935059
Epoch 2020, training loss: 62.073944091796875 = 0.022860528901219368 + 10.0 * 6.205108165740967
Epoch 2020, val loss: 1.283570408821106
Epoch 2030, training loss: 62.0690803527832 = 0.022499341517686844 + 10.0 * 6.204658031463623
Epoch 2030, val loss: 1.286566138267517
Epoch 2040, training loss: 62.07041931152344 = 0.022152123972773552 + 10.0 * 6.204826831817627
Epoch 2040, val loss: 1.2900348901748657
Epoch 2050, training loss: 62.08544158935547 = 0.021808871999382973 + 10.0 * 6.206363201141357
Epoch 2050, val loss: 1.293231725692749
Epoch 2060, training loss: 62.06698226928711 = 0.02146606333553791 + 10.0 * 6.204551696777344
Epoch 2060, val loss: 1.2964693307876587
Epoch 2070, training loss: 62.078208923339844 = 0.0211337860673666 + 10.0 * 6.205707550048828
Epoch 2070, val loss: 1.2999638319015503
Epoch 2080, training loss: 62.0839729309082 = 0.02080778405070305 + 10.0 * 6.2063164710998535
Epoch 2080, val loss: 1.3030612468719482
Epoch 2090, training loss: 62.06820297241211 = 0.020490260794758797 + 10.0 * 6.204771518707275
Epoch 2090, val loss: 1.305873990058899
Epoch 2100, training loss: 62.07748794555664 = 0.020179403945803642 + 10.0 * 6.20573091506958
Epoch 2100, val loss: 1.3089847564697266
Epoch 2110, training loss: 62.08442306518555 = 0.01987709105014801 + 10.0 * 6.206454277038574
Epoch 2110, val loss: 1.3119515180587769
Epoch 2120, training loss: 62.05793380737305 = 0.019580667838454247 + 10.0 * 6.203835487365723
Epoch 2120, val loss: 1.3150551319122314
Epoch 2130, training loss: 62.046844482421875 = 0.019289713352918625 + 10.0 * 6.202755451202393
Epoch 2130, val loss: 1.318185806274414
Epoch 2140, training loss: 62.044464111328125 = 0.01901080459356308 + 10.0 * 6.202545166015625
Epoch 2140, val loss: 1.3213320970535278
Epoch 2150, training loss: 62.091461181640625 = 0.018740136176347733 + 10.0 * 6.207272052764893
Epoch 2150, val loss: 1.3241462707519531
Epoch 2160, training loss: 62.065582275390625 = 0.01846703141927719 + 10.0 * 6.204711437225342
Epoch 2160, val loss: 1.327017068862915
Epoch 2170, training loss: 62.05150604248047 = 0.0181948933750391 + 10.0 * 6.203330993652344
Epoch 2170, val loss: 1.3299249410629272
Epoch 2180, training loss: 62.05164337158203 = 0.0179327093064785 + 10.0 * 6.203371047973633
Epoch 2180, val loss: 1.3330023288726807
Epoch 2190, training loss: 62.04878234863281 = 0.017679274082183838 + 10.0 * 6.203110218048096
Epoch 2190, val loss: 1.3359391689300537
Epoch 2200, training loss: 62.03899002075195 = 0.017435071989893913 + 10.0 * 6.202155590057373
Epoch 2200, val loss: 1.3387699127197266
Epoch 2210, training loss: 62.073429107666016 = 0.01719624362885952 + 10.0 * 6.205623149871826
Epoch 2210, val loss: 1.3417147397994995
Epoch 2220, training loss: 62.05158996582031 = 0.016949186101555824 + 10.0 * 6.203464031219482
Epoch 2220, val loss: 1.3444350957870483
Epoch 2230, training loss: 62.03632354736328 = 0.016715869307518005 + 10.0 * 6.201960563659668
Epoch 2230, val loss: 1.3474040031433105
Epoch 2240, training loss: 62.027984619140625 = 0.016487302258610725 + 10.0 * 6.201149940490723
Epoch 2240, val loss: 1.3498203754425049
Epoch 2250, training loss: 62.025733947753906 = 0.016264846548438072 + 10.0 * 6.200946807861328
Epoch 2250, val loss: 1.3526725769042969
Epoch 2260, training loss: 62.057167053222656 = 0.01605042815208435 + 10.0 * 6.204111576080322
Epoch 2260, val loss: 1.355624794960022
Epoch 2270, training loss: 62.022193908691406 = 0.015830865129828453 + 10.0 * 6.200636386871338
Epoch 2270, val loss: 1.3579106330871582
Epoch 2280, training loss: 62.02101135253906 = 0.015620642341673374 + 10.0 * 6.2005391120910645
Epoch 2280, val loss: 1.3606702089309692
Epoch 2290, training loss: 62.02151107788086 = 0.015418056398630142 + 10.0 * 6.20060920715332
Epoch 2290, val loss: 1.3632830381393433
Epoch 2300, training loss: 62.109039306640625 = 0.01522449217736721 + 10.0 * 6.209381580352783
Epoch 2300, val loss: 1.3656563758850098
Epoch 2310, training loss: 62.03583526611328 = 0.015014059841632843 + 10.0 * 6.20208215713501
Epoch 2310, val loss: 1.3684834241867065
Epoch 2320, training loss: 62.01323699951172 = 0.01481662131845951 + 10.0 * 6.1998419761657715
Epoch 2320, val loss: 1.371035099029541
Epoch 2330, training loss: 62.01921463012695 = 0.014631137251853943 + 10.0 * 6.200458526611328
Epoch 2330, val loss: 1.373702883720398
Epoch 2340, training loss: 62.038944244384766 = 0.014444497413933277 + 10.0 * 6.202449798583984
Epoch 2340, val loss: 1.3761931657791138
Epoch 2350, training loss: 62.027462005615234 = 0.014260324649512768 + 10.0 * 6.201320171356201
Epoch 2350, val loss: 1.3782432079315186
Epoch 2360, training loss: 62.044578552246094 = 0.014082107692956924 + 10.0 * 6.203049659729004
Epoch 2360, val loss: 1.3809715509414673
Epoch 2370, training loss: 62.00779342651367 = 0.013903039507567883 + 10.0 * 6.1993889808654785
Epoch 2370, val loss: 1.3830643892288208
Epoch 2380, training loss: 62.007484436035156 = 0.013730733655393124 + 10.0 * 6.199375629425049
Epoch 2380, val loss: 1.3857707977294922
Epoch 2390, training loss: 62.0012092590332 = 0.01356360875070095 + 10.0 * 6.198764324188232
Epoch 2390, val loss: 1.3880900144577026
Epoch 2400, training loss: 62.03465270996094 = 0.013401350937783718 + 10.0 * 6.202125072479248
Epoch 2400, val loss: 1.3908265829086304
Epoch 2410, training loss: 62.02738952636719 = 0.013232008554041386 + 10.0 * 6.201415538787842
Epoch 2410, val loss: 1.392364501953125
Epoch 2420, training loss: 62.00727462768555 = 0.013070936314761639 + 10.0 * 6.19942045211792
Epoch 2420, val loss: 1.3950531482696533
Epoch 2430, training loss: 61.99684524536133 = 0.012912999838590622 + 10.0 * 6.19839334487915
Epoch 2430, val loss: 1.3969802856445312
Epoch 2440, training loss: 61.99516677856445 = 0.012760860845446587 + 10.0 * 6.198240756988525
Epoch 2440, val loss: 1.399496078491211
Epoch 2450, training loss: 62.07276916503906 = 0.012610545381903648 + 10.0 * 6.206015586853027
Epoch 2450, val loss: 1.4016488790512085
Epoch 2460, training loss: 62.00993347167969 = 0.012463738210499287 + 10.0 * 6.199747085571289
Epoch 2460, val loss: 1.403752326965332
Epoch 2470, training loss: 61.99578857421875 = 0.012313068844377995 + 10.0 * 6.198347568511963
Epoch 2470, val loss: 1.406018614768982
Epoch 2480, training loss: 62.00223922729492 = 0.012173506431281567 + 10.0 * 6.1990065574646
Epoch 2480, val loss: 1.4081097841262817
Epoch 2490, training loss: 62.00896453857422 = 0.01203227136284113 + 10.0 * 6.199693202972412
Epoch 2490, val loss: 1.410109043121338
Epoch 2500, training loss: 61.986873626708984 = 0.01189329382032156 + 10.0 * 6.197497844696045
Epoch 2500, val loss: 1.4122505187988281
Epoch 2510, training loss: 62.026344299316406 = 0.011757946573197842 + 10.0 * 6.201458930969238
Epoch 2510, val loss: 1.4142265319824219
Epoch 2520, training loss: 61.988216400146484 = 0.01162472553551197 + 10.0 * 6.197659492492676
Epoch 2520, val loss: 1.4165536165237427
Epoch 2530, training loss: 61.9813232421875 = 0.011493375524878502 + 10.0 * 6.1969828605651855
Epoch 2530, val loss: 1.4185374975204468
Epoch 2540, training loss: 62.031375885009766 = 0.011364853009581566 + 10.0 * 6.202001094818115
Epoch 2540, val loss: 1.4206767082214355
Epoch 2550, training loss: 61.98836135864258 = 0.011235596612095833 + 10.0 * 6.1977128982543945
Epoch 2550, val loss: 1.4223167896270752
Epoch 2560, training loss: 61.972930908203125 = 0.011109813116490841 + 10.0 * 6.1961822509765625
Epoch 2560, val loss: 1.4242180585861206
Epoch 2570, training loss: 61.97017288208008 = 0.01099010556936264 + 10.0 * 6.195918083190918
Epoch 2570, val loss: 1.4264564514160156
Epoch 2580, training loss: 61.968631744384766 = 0.010872033424675465 + 10.0 * 6.195775985717773
Epoch 2580, val loss: 1.4282695055007935
Epoch 2590, training loss: 61.980587005615234 = 0.010757275857031345 + 10.0 * 6.1969828605651855
Epoch 2590, val loss: 1.4303054809570312
Epoch 2600, training loss: 62.02213668823242 = 0.010640318505465984 + 10.0 * 6.201149940490723
Epoch 2600, val loss: 1.4322540760040283
Epoch 2610, training loss: 61.973243713378906 = 0.010520738549530506 + 10.0 * 6.196272373199463
Epoch 2610, val loss: 1.4336763620376587
Epoch 2620, training loss: 61.97158432006836 = 0.010406137444078922 + 10.0 * 6.196117877960205
Epoch 2620, val loss: 1.4357613325119019
Epoch 2630, training loss: 61.96997833251953 = 0.010297060944139957 + 10.0 * 6.195968151092529
Epoch 2630, val loss: 1.4376908540725708
Epoch 2640, training loss: 62.006500244140625 = 0.010189123451709747 + 10.0 * 6.199631214141846
Epoch 2640, val loss: 1.4396909475326538
Epoch 2650, training loss: 61.963077545166016 = 0.010082103312015533 + 10.0 * 6.1952996253967285
Epoch 2650, val loss: 1.4409692287445068
Epoch 2660, training loss: 61.996482849121094 = 0.009978335350751877 + 10.0 * 6.198650360107422
Epoch 2660, val loss: 1.4428118467330933
Epoch 2670, training loss: 61.956809997558594 = 0.009871036745607853 + 10.0 * 6.194693565368652
Epoch 2670, val loss: 1.444602131843567
Epoch 2680, training loss: 61.958282470703125 = 0.009770512580871582 + 10.0 * 6.194851398468018
Epoch 2680, val loss: 1.44635009765625
Epoch 2690, training loss: 61.95703125 = 0.009672628715634346 + 10.0 * 6.194735527038574
Epoch 2690, val loss: 1.4481574296951294
Epoch 2700, training loss: 61.986244201660156 = 0.0095772510394454 + 10.0 * 6.197666645050049
Epoch 2700, val loss: 1.4497567415237427
Epoch 2710, training loss: 61.985511779785156 = 0.009476211853325367 + 10.0 * 6.197603702545166
Epoch 2710, val loss: 1.451350212097168
Epoch 2720, training loss: 61.965999603271484 = 0.009380371309816837 + 10.0 * 6.195662021636963
Epoch 2720, val loss: 1.45270574092865
Epoch 2730, training loss: 61.957244873046875 = 0.009285707026720047 + 10.0 * 6.194796085357666
Epoch 2730, val loss: 1.4545987844467163
Epoch 2740, training loss: 61.948455810546875 = 0.009193368256092072 + 10.0 * 6.1939263343811035
Epoch 2740, val loss: 1.4563279151916504
Epoch 2750, training loss: 61.9568977355957 = 0.009104827418923378 + 10.0 * 6.194779396057129
Epoch 2750, val loss: 1.4581451416015625
Epoch 2760, training loss: 61.965606689453125 = 0.009017113596200943 + 10.0 * 6.1956586837768555
Epoch 2760, val loss: 1.459619402885437
Epoch 2770, training loss: 61.96401596069336 = 0.008929063566029072 + 10.0 * 6.1955084800720215
Epoch 2770, val loss: 1.460845947265625
Epoch 2780, training loss: 61.995418548583984 = 0.008840947411954403 + 10.0 * 6.198657512664795
Epoch 2780, val loss: 1.4625498056411743
Epoch 2790, training loss: 61.95063781738281 = 0.008752173744142056 + 10.0 * 6.194188594818115
Epoch 2790, val loss: 1.4639614820480347
Epoch 2800, training loss: 61.93911361694336 = 0.008666844107210636 + 10.0 * 6.193044662475586
Epoch 2800, val loss: 1.4655330181121826
Epoch 2810, training loss: 61.942745208740234 = 0.008585943840444088 + 10.0 * 6.193415641784668
Epoch 2810, val loss: 1.4670276641845703
Epoch 2820, training loss: 61.976158142089844 = 0.008507783524692059 + 10.0 * 6.196764945983887
Epoch 2820, val loss: 1.4681490659713745
Epoch 2830, training loss: 61.94659423828125 = 0.008425033651292324 + 10.0 * 6.193816661834717
Epoch 2830, val loss: 1.4699431657791138
Epoch 2840, training loss: 61.94156265258789 = 0.008344744332134724 + 10.0 * 6.193321704864502
Epoch 2840, val loss: 1.4714998006820679
Epoch 2850, training loss: 61.94247817993164 = 0.008267686702311039 + 10.0 * 6.193421363830566
Epoch 2850, val loss: 1.4729315042495728
Epoch 2860, training loss: 61.9638786315918 = 0.008191659115254879 + 10.0 * 6.195569038391113
Epoch 2860, val loss: 1.4745208024978638
Epoch 2870, training loss: 61.94429016113281 = 0.008116189390420914 + 10.0 * 6.193617343902588
Epoch 2870, val loss: 1.4759361743927002
Epoch 2880, training loss: 61.949153900146484 = 0.008042886853218079 + 10.0 * 6.194111347198486
Epoch 2880, val loss: 1.4773142337799072
Epoch 2890, training loss: 61.93867874145508 = 0.007968796417117119 + 10.0 * 6.193070888519287
Epoch 2890, val loss: 1.4783854484558105
Epoch 2900, training loss: 61.963165283203125 = 0.00789810810238123 + 10.0 * 6.195527076721191
Epoch 2900, val loss: 1.4798414707183838
Epoch 2910, training loss: 61.93537521362305 = 0.00782717950642109 + 10.0 * 6.192754745483398
Epoch 2910, val loss: 1.4812848567962646
Epoch 2920, training loss: 61.93476867675781 = 0.007756978739053011 + 10.0 * 6.19270133972168
Epoch 2920, val loss: 1.4824442863464355
Epoch 2930, training loss: 61.94027328491211 = 0.007689598482102156 + 10.0 * 6.193258285522461
Epoch 2930, val loss: 1.4837634563446045
Epoch 2940, training loss: 61.94990539550781 = 0.007622203323990107 + 10.0 * 6.194228172302246
Epoch 2940, val loss: 1.4850736856460571
Epoch 2950, training loss: 61.940643310546875 = 0.0075533888302743435 + 10.0 * 6.1933088302612305
Epoch 2950, val loss: 1.4864137172698975
Epoch 2960, training loss: 61.95003128051758 = 0.007488066330552101 + 10.0 * 6.194254398345947
Epoch 2960, val loss: 1.4876794815063477
Epoch 2970, training loss: 61.937469482421875 = 0.007421083748340607 + 10.0 * 6.193005084991455
Epoch 2970, val loss: 1.488816261291504
Epoch 2980, training loss: 61.92915725708008 = 0.007355725858360529 + 10.0 * 6.192180156707764
Epoch 2980, val loss: 1.490281105041504
Epoch 2990, training loss: 61.928855895996094 = 0.007294030860066414 + 10.0 * 6.1921563148498535
Epoch 2990, val loss: 1.491428017616272
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 87.89820861816406 = 1.9299519062042236 + 10.0 * 8.59682559967041
Epoch 0, val loss: 1.9365204572677612
Epoch 10, training loss: 87.88124084472656 = 1.920622706413269 + 10.0 * 8.596061706542969
Epoch 10, val loss: 1.9268922805786133
Epoch 20, training loss: 87.81117248535156 = 1.9092216491699219 + 10.0 * 8.590194702148438
Epoch 20, val loss: 1.9150385856628418
Epoch 30, training loss: 87.41644287109375 = 1.8944714069366455 + 10.0 * 8.552197456359863
Epoch 30, val loss: 1.8994413614273071
Epoch 40, training loss: 85.45210266113281 = 1.8768911361694336 + 10.0 * 8.357521057128906
Epoch 40, val loss: 1.8810460567474365
Epoch 50, training loss: 78.72354888916016 = 1.8584575653076172 + 10.0 * 7.686508655548096
Epoch 50, val loss: 1.8613204956054688
Epoch 60, training loss: 75.01227569580078 = 1.8401631116867065 + 10.0 * 7.317211151123047
Epoch 60, val loss: 1.8426979780197144
Epoch 70, training loss: 72.69876861572266 = 1.8239141702651978 + 10.0 * 7.087485313415527
Epoch 70, val loss: 1.8271491527557373
Epoch 80, training loss: 71.57237243652344 = 1.8111655712127686 + 10.0 * 6.976120948791504
Epoch 80, val loss: 1.8149223327636719
Epoch 90, training loss: 70.46121978759766 = 1.8006974458694458 + 10.0 * 6.866052150726318
Epoch 90, val loss: 1.8050872087478638
Epoch 100, training loss: 69.72887420654297 = 1.7915103435516357 + 10.0 * 6.793735980987549
Epoch 100, val loss: 1.796389102935791
Epoch 110, training loss: 68.9911117553711 = 1.782614827156067 + 10.0 * 6.720849514007568
Epoch 110, val loss: 1.7879546880722046
Epoch 120, training loss: 68.39775085449219 = 1.7748137712478638 + 10.0 * 6.662293910980225
Epoch 120, val loss: 1.7804404497146606
Epoch 130, training loss: 67.9768295288086 = 1.7667630910873413 + 10.0 * 6.621006488800049
Epoch 130, val loss: 1.7724425792694092
Epoch 140, training loss: 67.55953979492188 = 1.757765531539917 + 10.0 * 6.580177307128906
Epoch 140, val loss: 1.7639890909194946
Epoch 150, training loss: 67.1971435546875 = 1.7489120960235596 + 10.0 * 6.54482364654541
Epoch 150, val loss: 1.7560094594955444
Epoch 160, training loss: 66.91940307617188 = 1.7397156953811646 + 10.0 * 6.517969131469727
Epoch 160, val loss: 1.7478818893432617
Epoch 170, training loss: 66.69634246826172 = 1.7290393114089966 + 10.0 * 6.496729850769043
Epoch 170, val loss: 1.738417387008667
Epoch 180, training loss: 66.51338195800781 = 1.7167267799377441 + 10.0 * 6.479665279388428
Epoch 180, val loss: 1.7275440692901611
Epoch 190, training loss: 66.35729217529297 = 1.7029589414596558 + 10.0 * 6.465433597564697
Epoch 190, val loss: 1.7154899835586548
Epoch 200, training loss: 66.22014617919922 = 1.6878122091293335 + 10.0 * 6.453233242034912
Epoch 200, val loss: 1.7023026943206787
Epoch 210, training loss: 66.09387969970703 = 1.6712863445281982 + 10.0 * 6.442258834838867
Epoch 210, val loss: 1.6880196332931519
Epoch 220, training loss: 66.02173614501953 = 1.6530511379241943 + 10.0 * 6.436868190765381
Epoch 220, val loss: 1.6722867488861084
Epoch 230, training loss: 65.87628936767578 = 1.6331892013549805 + 10.0 * 6.424310684204102
Epoch 230, val loss: 1.655385971069336
Epoch 240, training loss: 65.75493621826172 = 1.6119439601898193 + 10.0 * 6.414299011230469
Epoch 240, val loss: 1.637297511100769
Epoch 250, training loss: 65.64353942871094 = 1.5891152620315552 + 10.0 * 6.405442237854004
Epoch 250, val loss: 1.6177515983581543
Epoch 260, training loss: 65.53630065917969 = 1.5646499395370483 + 10.0 * 6.397165298461914
Epoch 260, val loss: 1.5970592498779297
Epoch 270, training loss: 65.43827819824219 = 1.5384987592697144 + 10.0 * 6.389977931976318
Epoch 270, val loss: 1.5748767852783203
Epoch 280, training loss: 65.3399887084961 = 1.5109978914260864 + 10.0 * 6.382898807525635
Epoch 280, val loss: 1.5517221689224243
Epoch 290, training loss: 65.24971008300781 = 1.4822824001312256 + 10.0 * 6.376742362976074
Epoch 290, val loss: 1.527704119682312
Epoch 300, training loss: 65.16073608398438 = 1.4524827003479004 + 10.0 * 6.370825290679932
Epoch 300, val loss: 1.5029456615447998
Epoch 310, training loss: 65.1189956665039 = 1.4219623804092407 + 10.0 * 6.36970329284668
Epoch 310, val loss: 1.4775886535644531
Epoch 320, training loss: 65.00846862792969 = 1.3905601501464844 + 10.0 * 6.361791133880615
Epoch 320, val loss: 1.4517455101013184
Epoch 330, training loss: 64.9197006225586 = 1.359094262123108 + 10.0 * 6.356060981750488
Epoch 330, val loss: 1.4257993698120117
Epoch 340, training loss: 64.83610534667969 = 1.327378273010254 + 10.0 * 6.350872993469238
Epoch 340, val loss: 1.3998446464538574
Epoch 350, training loss: 64.76092529296875 = 1.2956762313842773 + 10.0 * 6.346525192260742
Epoch 350, val loss: 1.374003291130066
Epoch 360, training loss: 64.71847534179688 = 1.2639673948287964 + 10.0 * 6.345450401306152
Epoch 360, val loss: 1.3482306003570557
Epoch 370, training loss: 64.62419128417969 = 1.2324100732803345 + 10.0 * 6.339178085327148
Epoch 370, val loss: 1.3229490518569946
Epoch 380, training loss: 64.54988861083984 = 1.2012078762054443 + 10.0 * 6.33486795425415
Epoch 380, val loss: 1.2980362176895142
Epoch 390, training loss: 64.487548828125 = 1.1703543663024902 + 10.0 * 6.331719398498535
Epoch 390, val loss: 1.273568868637085
Epoch 400, training loss: 64.46265411376953 = 1.1398407220840454 + 10.0 * 6.332281589508057
Epoch 400, val loss: 1.2496857643127441
Epoch 410, training loss: 64.3781967163086 = 1.109897255897522 + 10.0 * 6.3268303871154785
Epoch 410, val loss: 1.2264130115509033
Epoch 420, training loss: 64.32060241699219 = 1.0806833505630493 + 10.0 * 6.3239922523498535
Epoch 420, val loss: 1.20402193069458
Epoch 430, training loss: 64.25564575195312 = 1.052125096321106 + 10.0 * 6.320352077484131
Epoch 430, val loss: 1.1825268268585205
Epoch 440, training loss: 64.20237731933594 = 1.0242362022399902 + 10.0 * 6.317813873291016
Epoch 440, val loss: 1.1619213819503784
Epoch 450, training loss: 64.23422241210938 = 0.9971241354942322 + 10.0 * 6.323709487915039
Epoch 450, val loss: 1.142105221748352
Epoch 460, training loss: 64.12020874023438 = 0.9704158902168274 + 10.0 * 6.314979553222656
Epoch 460, val loss: 1.1231894493103027
Epoch 470, training loss: 64.05876159667969 = 0.9447116255760193 + 10.0 * 6.311405181884766
Epoch 470, val loss: 1.105353832244873
Epoch 480, training loss: 64.00568389892578 = 0.9197133779525757 + 10.0 * 6.308597087860107
Epoch 480, val loss: 1.088482141494751
Epoch 490, training loss: 63.961082458496094 = 0.8954000473022461 + 10.0 * 6.306568145751953
Epoch 490, val loss: 1.072458028793335
Epoch 500, training loss: 63.93817901611328 = 0.8716720342636108 + 10.0 * 6.306650638580322
Epoch 500, val loss: 1.0572454929351807
Epoch 510, training loss: 63.8740348815918 = 0.8488595485687256 + 10.0 * 6.302517890930176
Epoch 510, val loss: 1.0429002046585083
Epoch 520, training loss: 63.832244873046875 = 0.8266634345054626 + 10.0 * 6.300558090209961
Epoch 520, val loss: 1.029468059539795
Epoch 530, training loss: 63.7904052734375 = 0.805142879486084 + 10.0 * 6.298526287078857
Epoch 530, val loss: 1.0168079137802124
Epoch 540, training loss: 63.76714324951172 = 0.7842766046524048 + 10.0 * 6.2982869148254395
Epoch 540, val loss: 1.0048491954803467
Epoch 550, training loss: 63.7199821472168 = 0.7639786601066589 + 10.0 * 6.295600414276123
Epoch 550, val loss: 0.9937649369239807
Epoch 560, training loss: 63.67897033691406 = 0.7444010972976685 + 10.0 * 6.29345703125
Epoch 560, val loss: 0.9834033846855164
Epoch 570, training loss: 63.64069366455078 = 0.7254619598388672 + 10.0 * 6.291523456573486
Epoch 570, val loss: 0.9738184213638306
Epoch 580, training loss: 63.67512893676758 = 0.7071081399917603 + 10.0 * 6.296802043914795
Epoch 580, val loss: 0.9647958278656006
Epoch 590, training loss: 63.582698822021484 = 0.6890029907226562 + 10.0 * 6.289369583129883
Epoch 590, val loss: 0.9562975764274597
Epoch 600, training loss: 63.54058837890625 = 0.6717137098312378 + 10.0 * 6.286887168884277
Epoch 600, val loss: 0.9485111832618713
Epoch 610, training loss: 63.50851821899414 = 0.6549807786941528 + 10.0 * 6.285353660583496
Epoch 610, val loss: 0.9413024187088013
Epoch 620, training loss: 63.5284309387207 = 0.6385913491249084 + 10.0 * 6.2889838218688965
Epoch 620, val loss: 0.9345205426216125
Epoch 630, training loss: 63.4434928894043 = 0.6227494478225708 + 10.0 * 6.282074451446533
Epoch 630, val loss: 0.9282761812210083
Epoch 640, training loss: 63.41378402709961 = 0.6073564291000366 + 10.0 * 6.280642509460449
Epoch 640, val loss: 0.9226554036140442
Epoch 650, training loss: 63.386077880859375 = 0.5924198627471924 + 10.0 * 6.2793660163879395
Epoch 650, val loss: 0.9175330400466919
Epoch 660, training loss: 63.360565185546875 = 0.5778383016586304 + 10.0 * 6.27827262878418
Epoch 660, val loss: 0.9127702116966248
Epoch 670, training loss: 63.33355712890625 = 0.5636270642280579 + 10.0 * 6.276993274688721
Epoch 670, val loss: 0.9084827899932861
Epoch 680, training loss: 63.31592559814453 = 0.5497751832008362 + 10.0 * 6.276615142822266
Epoch 680, val loss: 0.9045962691307068
Epoch 690, training loss: 63.280757904052734 = 0.5363308191299438 + 10.0 * 6.274442672729492
Epoch 690, val loss: 0.901140034198761
Epoch 700, training loss: 63.25155258178711 = 0.5231581926345825 + 10.0 * 6.272839546203613
Epoch 700, val loss: 0.8979690670967102
Epoch 710, training loss: 63.22914123535156 = 0.5102578997612 + 10.0 * 6.271888256072998
Epoch 710, val loss: 0.8950482606887817
Epoch 720, training loss: 63.19993209838867 = 0.497733473777771 + 10.0 * 6.270219802856445
Epoch 720, val loss: 0.8924732208251953
Epoch 730, training loss: 63.169315338134766 = 0.48550093173980713 + 10.0 * 6.268381595611572
Epoch 730, val loss: 0.8902430534362793
Epoch 740, training loss: 63.140167236328125 = 0.4735744297504425 + 10.0 * 6.266659259796143
Epoch 740, val loss: 0.888346791267395
Epoch 750, training loss: 63.1153564453125 = 0.46182015538215637 + 10.0 * 6.265353679656982
Epoch 750, val loss: 0.8867073655128479
Epoch 760, training loss: 63.102134704589844 = 0.4502408802509308 + 10.0 * 6.265189170837402
Epoch 760, val loss: 0.8852762579917908
Epoch 770, training loss: 63.093875885009766 = 0.4387786388397217 + 10.0 * 6.265509605407715
Epoch 770, val loss: 0.8839566707611084
Epoch 780, training loss: 63.05187225341797 = 0.4275316596031189 + 10.0 * 6.262434005737305
Epoch 780, val loss: 0.8828489780426025
Epoch 790, training loss: 63.029197692871094 = 0.41655460000038147 + 10.0 * 6.261264324188232
Epoch 790, val loss: 0.8820147514343262
Epoch 800, training loss: 63.00736618041992 = 0.40574872493743896 + 10.0 * 6.260161399841309
Epoch 800, val loss: 0.8814036250114441
Epoch 810, training loss: 63.011837005615234 = 0.3950856029987335 + 10.0 * 6.2616753578186035
Epoch 810, val loss: 0.8810027241706848
Epoch 820, training loss: 62.98402786254883 = 0.38456958532333374 + 10.0 * 6.259945869445801
Epoch 820, val loss: 0.880681037902832
Epoch 830, training loss: 62.97044372558594 = 0.3742436170578003 + 10.0 * 6.25961971282959
Epoch 830, val loss: 0.8804784417152405
Epoch 840, training loss: 62.923545837402344 = 0.3641304671764374 + 10.0 * 6.255941390991211
Epoch 840, val loss: 0.8804851174354553
Epoch 850, training loss: 62.908687591552734 = 0.35423004627227783 + 10.0 * 6.255445957183838
Epoch 850, val loss: 0.8806893825531006
Epoch 860, training loss: 62.91444396972656 = 0.344527930021286 + 10.0 * 6.256991386413574
Epoch 860, val loss: 0.8810303211212158
Epoch 870, training loss: 62.87324905395508 = 0.33487552404403687 + 10.0 * 6.2538371086120605
Epoch 870, val loss: 0.8815522193908691
Epoch 880, training loss: 62.8642578125 = 0.32553085684776306 + 10.0 * 6.253872871398926
Epoch 880, val loss: 0.8822084665298462
Epoch 890, training loss: 62.84026336669922 = 0.31636616587638855 + 10.0 * 6.252389907836914
Epoch 890, val loss: 0.8830724358558655
Epoch 900, training loss: 62.8275260925293 = 0.3074612021446228 + 10.0 * 6.252006530761719
Epoch 900, val loss: 0.8841533064842224
Epoch 910, training loss: 62.8073616027832 = 0.2987205684185028 + 10.0 * 6.250864028930664
Epoch 910, val loss: 0.8854408264160156
Epoch 920, training loss: 62.78205871582031 = 0.29020580649375916 + 10.0 * 6.249185085296631
Epoch 920, val loss: 0.886924147605896
Epoch 930, training loss: 62.767635345458984 = 0.28193333745002747 + 10.0 * 6.248570442199707
Epoch 930, val loss: 0.8886464238166809
Epoch 940, training loss: 62.77170181274414 = 0.2738676369190216 + 10.0 * 6.249783515930176
Epoch 940, val loss: 0.8905308842658997
Epoch 950, training loss: 62.79564666748047 = 0.26603636145591736 + 10.0 * 6.252961158752441
Epoch 950, val loss: 0.8925856351852417
Epoch 960, training loss: 62.741214752197266 = 0.2583036720752716 + 10.0 * 6.248291015625
Epoch 960, val loss: 0.8946564793586731
Epoch 970, training loss: 62.711875915527344 = 0.2509818971157074 + 10.0 * 6.246089458465576
Epoch 970, val loss: 0.8970112204551697
Epoch 980, training loss: 62.689388275146484 = 0.2438652366399765 + 10.0 * 6.244552135467529
Epoch 980, val loss: 0.8995736241340637
Epoch 990, training loss: 62.67521667480469 = 0.23699189722537994 + 10.0 * 6.2438225746154785
Epoch 990, val loss: 0.9022563099861145
Epoch 1000, training loss: 62.66918182373047 = 0.23030664026737213 + 10.0 * 6.243887424468994
Epoch 1000, val loss: 0.9050456285476685
Epoch 1010, training loss: 62.6609001159668 = 0.22379842400550842 + 10.0 * 6.243710517883301
Epoch 1010, val loss: 0.9078755378723145
Epoch 1020, training loss: 62.6623420715332 = 0.21751758456230164 + 10.0 * 6.244482517242432
Epoch 1020, val loss: 0.9108292460441589
Epoch 1030, training loss: 62.64990997314453 = 0.21147644519805908 + 10.0 * 6.2438435554504395
Epoch 1030, val loss: 0.9138481020927429
Epoch 1040, training loss: 62.61678695678711 = 0.20558908581733704 + 10.0 * 6.241119861602783
Epoch 1040, val loss: 0.9170403480529785
Epoch 1050, training loss: 62.60272216796875 = 0.19993972778320312 + 10.0 * 6.240278244018555
Epoch 1050, val loss: 0.9203922152519226
Epoch 1060, training loss: 62.602745056152344 = 0.19446434080600739 + 10.0 * 6.240828037261963
Epoch 1060, val loss: 0.9238957762718201
Epoch 1070, training loss: 62.594093322753906 = 0.1891557276248932 + 10.0 * 6.2404937744140625
Epoch 1070, val loss: 0.9273844361305237
Epoch 1080, training loss: 62.57011413574219 = 0.18405945599079132 + 10.0 * 6.238605499267578
Epoch 1080, val loss: 0.9310010671615601
Epoch 1090, training loss: 62.55881118774414 = 0.17909374833106995 + 10.0 * 6.237971782684326
Epoch 1090, val loss: 0.9347233772277832
Epoch 1100, training loss: 62.56180953979492 = 0.174320787191391 + 10.0 * 6.238749027252197
Epoch 1100, val loss: 0.9385839700698853
Epoch 1110, training loss: 62.55263137817383 = 0.16961993277072906 + 10.0 * 6.2383012771606445
Epoch 1110, val loss: 0.9423943758010864
Epoch 1120, training loss: 62.54264450073242 = 0.16511015594005585 + 10.0 * 6.237753391265869
Epoch 1120, val loss: 0.9463227391242981
Epoch 1130, training loss: 62.515438079833984 = 0.1607561856508255 + 10.0 * 6.23546838760376
Epoch 1130, val loss: 0.9503408074378967
Epoch 1140, training loss: 62.51062774658203 = 0.15654578804969788 + 10.0 * 6.235407829284668
Epoch 1140, val loss: 0.9544463157653809
Epoch 1150, training loss: 62.5584602355957 = 0.15243855118751526 + 10.0 * 6.240602016448975
Epoch 1150, val loss: 0.9585065841674805
Epoch 1160, training loss: 62.49981689453125 = 0.14849282801151276 + 10.0 * 6.235132217407227
Epoch 1160, val loss: 0.962661623954773
Epoch 1170, training loss: 62.47775650024414 = 0.1446422040462494 + 10.0 * 6.233311653137207
Epoch 1170, val loss: 0.9668179154396057
Epoch 1180, training loss: 62.46590805053711 = 0.14094065129756927 + 10.0 * 6.232496738433838
Epoch 1180, val loss: 0.9710738658905029
Epoch 1190, training loss: 62.469573974609375 = 0.1373475342988968 + 10.0 * 6.233222484588623
Epoch 1190, val loss: 0.9753327369689941
Epoch 1200, training loss: 62.467933654785156 = 0.13382142782211304 + 10.0 * 6.2334113121032715
Epoch 1200, val loss: 0.9795692563056946
Epoch 1210, training loss: 62.443275451660156 = 0.13040822744369507 + 10.0 * 6.231286525726318
Epoch 1210, val loss: 0.9837681651115417
Epoch 1220, training loss: 62.43827438354492 = 0.12712007761001587 + 10.0 * 6.231115341186523
Epoch 1220, val loss: 0.9881165623664856
Epoch 1230, training loss: 62.427303314208984 = 0.12393229454755783 + 10.0 * 6.230337142944336
Epoch 1230, val loss: 0.9925174117088318
Epoch 1240, training loss: 62.53694534301758 = 0.12080658227205276 + 10.0 * 6.241613864898682
Epoch 1240, val loss: 0.99683678150177
Epoch 1250, training loss: 62.41582489013672 = 0.11779499053955078 + 10.0 * 6.229803085327148
Epoch 1250, val loss: 1.001151442527771
Epoch 1260, training loss: 62.41170120239258 = 0.11487779021263123 + 10.0 * 6.229681968688965
Epoch 1260, val loss: 1.0054441690444946
Epoch 1270, training loss: 62.39777755737305 = 0.11205850541591644 + 10.0 * 6.228571891784668
Epoch 1270, val loss: 1.009870171546936
Epoch 1280, training loss: 62.38704299926758 = 0.10931647568941116 + 10.0 * 6.2277727127075195
Epoch 1280, val loss: 1.0143561363220215
Epoch 1290, training loss: 62.39364242553711 = 0.10665083676576614 + 10.0 * 6.228699207305908
Epoch 1290, val loss: 1.018836498260498
Epoch 1300, training loss: 62.39857864379883 = 0.10402099788188934 + 10.0 * 6.229455947875977
Epoch 1300, val loss: 1.023252248764038
Epoch 1310, training loss: 62.375152587890625 = 0.10150788724422455 + 10.0 * 6.227364540100098
Epoch 1310, val loss: 1.0275723934173584
Epoch 1320, training loss: 62.367889404296875 = 0.09906288981437683 + 10.0 * 6.226882457733154
Epoch 1320, val loss: 1.0320470333099365
Epoch 1330, training loss: 62.35366439819336 = 0.09669465571641922 + 10.0 * 6.225697040557861
Epoch 1330, val loss: 1.0365650653839111
Epoch 1340, training loss: 62.34767150878906 = 0.09439793974161148 + 10.0 * 6.225327491760254
Epoch 1340, val loss: 1.0410255193710327
Epoch 1350, training loss: 62.39363098144531 = 0.0921647846698761 + 10.0 * 6.230146884918213
Epoch 1350, val loss: 1.045472502708435
Epoch 1360, training loss: 62.3612174987793 = 0.08996126800775528 + 10.0 * 6.227125644683838
Epoch 1360, val loss: 1.0498687028884888
Epoch 1370, training loss: 62.34540557861328 = 0.08783562481403351 + 10.0 * 6.225756645202637
Epoch 1370, val loss: 1.054335117340088
Epoch 1380, training loss: 62.345909118652344 = 0.08576960116624832 + 10.0 * 6.226014137268066
Epoch 1380, val loss: 1.058727741241455
Epoch 1390, training loss: 62.32612228393555 = 0.08378071337938309 + 10.0 * 6.224234104156494
Epoch 1390, val loss: 1.0631017684936523
Epoch 1400, training loss: 62.31477737426758 = 0.08183986693620682 + 10.0 * 6.223293781280518
Epoch 1400, val loss: 1.0676376819610596
Epoch 1410, training loss: 62.31249237060547 = 0.07996337860822678 + 10.0 * 6.223252773284912
Epoch 1410, val loss: 1.0720404386520386
Epoch 1420, training loss: 62.34357833862305 = 0.07812986522912979 + 10.0 * 6.2265448570251465
Epoch 1420, val loss: 1.076449990272522
Epoch 1430, training loss: 62.31345748901367 = 0.07631129026412964 + 10.0 * 6.223714351654053
Epoch 1430, val loss: 1.0806869268417358
Epoch 1440, training loss: 62.305057525634766 = 0.07456835359334946 + 10.0 * 6.223048686981201
Epoch 1440, val loss: 1.085020899772644
Epoch 1450, training loss: 62.28949737548828 = 0.07288942486047745 + 10.0 * 6.221660614013672
Epoch 1450, val loss: 1.08938467502594
Epoch 1460, training loss: 62.28307342529297 = 0.07125551998615265 + 10.0 * 6.221181869506836
Epoch 1460, val loss: 1.0938037633895874
Epoch 1470, training loss: 62.292999267578125 = 0.06967182457447052 + 10.0 * 6.222332954406738
Epoch 1470, val loss: 1.0982080698013306
Epoch 1480, training loss: 62.28873062133789 = 0.06811758875846863 + 10.0 * 6.2220611572265625
Epoch 1480, val loss: 1.1024152040481567
Epoch 1490, training loss: 62.29319763183594 = 0.06659214198589325 + 10.0 * 6.222660541534424
Epoch 1490, val loss: 1.1066542863845825
Epoch 1500, training loss: 62.27703094482422 = 0.06512518972158432 + 10.0 * 6.221190452575684
Epoch 1500, val loss: 1.1109623908996582
Epoch 1510, training loss: 62.261592864990234 = 0.06369372457265854 + 10.0 * 6.219789981842041
Epoch 1510, val loss: 1.1152288913726807
Epoch 1520, training loss: 62.25765609741211 = 0.06231166422367096 + 10.0 * 6.219534397125244
Epoch 1520, val loss: 1.1195449829101562
Epoch 1530, training loss: 62.268619537353516 = 0.06096351519227028 + 10.0 * 6.220765590667725
Epoch 1530, val loss: 1.1237393617630005
Epoch 1540, training loss: 62.26362609863281 = 0.059641528874635696 + 10.0 * 6.220398426055908
Epoch 1540, val loss: 1.1278433799743652
Epoch 1550, training loss: 62.267478942871094 = 0.05836421251296997 + 10.0 * 6.220911502838135
Epoch 1550, val loss: 1.1318691968917847
Epoch 1560, training loss: 62.24501419067383 = 0.05711379274725914 + 10.0 * 6.218790054321289
Epoch 1560, val loss: 1.136023998260498
Epoch 1570, training loss: 62.2340202331543 = 0.05591340363025665 + 10.0 * 6.21781063079834
Epoch 1570, val loss: 1.1401102542877197
Epoch 1580, training loss: 62.22831726074219 = 0.05474327877163887 + 10.0 * 6.217357158660889
Epoch 1580, val loss: 1.1442335844039917
Epoch 1590, training loss: 62.24982833862305 = 0.053612589836120605 + 10.0 * 6.219621658325195
Epoch 1590, val loss: 1.1483017206192017
Epoch 1600, training loss: 62.21906661987305 = 0.052475281059741974 + 10.0 * 6.216659069061279
Epoch 1600, val loss: 1.1523419618606567
Epoch 1610, training loss: 62.222904205322266 = 0.051384419202804565 + 10.0 * 6.217152118682861
Epoch 1610, val loss: 1.1563528776168823
Epoch 1620, training loss: 62.26922607421875 = 0.050326865166425705 + 10.0 * 6.221889972686768
Epoch 1620, val loss: 1.1603785753250122
Epoch 1630, training loss: 62.2121696472168 = 0.04928786680102348 + 10.0 * 6.216288089752197
Epoch 1630, val loss: 1.1641658544540405
Epoch 1640, training loss: 62.20072555541992 = 0.04828878119587898 + 10.0 * 6.215243339538574
Epoch 1640, val loss: 1.1682393550872803
Epoch 1650, training loss: 62.19734573364258 = 0.04732424020767212 + 10.0 * 6.215002059936523
Epoch 1650, val loss: 1.1722596883773804
Epoch 1660, training loss: 62.20466613769531 = 0.04638027027249336 + 10.0 * 6.215828895568848
Epoch 1660, val loss: 1.1762648820877075
Epoch 1670, training loss: 62.21294021606445 = 0.045448318123817444 + 10.0 * 6.21674919128418
Epoch 1670, val loss: 1.1801468133926392
Epoch 1680, training loss: 62.187286376953125 = 0.04454799368977547 + 10.0 * 6.214273929595947
Epoch 1680, val loss: 1.183986783027649
Epoch 1690, training loss: 62.190792083740234 = 0.04367612674832344 + 10.0 * 6.214711666107178
Epoch 1690, val loss: 1.187918782234192
Epoch 1700, training loss: 62.2083740234375 = 0.042827058583498 + 10.0 * 6.216554641723633
Epoch 1700, val loss: 1.191693902015686
Epoch 1710, training loss: 62.20064163208008 = 0.042001545429229736 + 10.0 * 6.215864181518555
Epoch 1710, val loss: 1.1954742670059204
Epoch 1720, training loss: 62.18202590942383 = 0.04118639975786209 + 10.0 * 6.214083671569824
Epoch 1720, val loss: 1.1994508504867554
Epoch 1730, training loss: 62.17182540893555 = 0.04039596766233444 + 10.0 * 6.2131428718566895
Epoch 1730, val loss: 1.203135371208191
Epoch 1740, training loss: 62.170005798339844 = 0.03963267803192139 + 10.0 * 6.213037490844727
Epoch 1740, val loss: 1.2069438695907593
Epoch 1750, training loss: 62.181148529052734 = 0.03888600319623947 + 10.0 * 6.214226245880127
Epoch 1750, val loss: 1.2107133865356445
Epoch 1760, training loss: 62.17354202270508 = 0.03815208747982979 + 10.0 * 6.213539123535156
Epoch 1760, val loss: 1.2143609523773193
Epoch 1770, training loss: 62.16608428955078 = 0.03744681179523468 + 10.0 * 6.212863922119141
Epoch 1770, val loss: 1.2179292440414429
Epoch 1780, training loss: 62.194580078125 = 0.03675317391753197 + 10.0 * 6.215782642364502
Epoch 1780, val loss: 1.2217293977737427
Epoch 1790, training loss: 62.202083587646484 = 0.036066990345716476 + 10.0 * 6.216601371765137
Epoch 1790, val loss: 1.2250980138778687
Epoch 1800, training loss: 62.155433654785156 = 0.03539933264255524 + 10.0 * 6.212003231048584
Epoch 1800, val loss: 1.228662371635437
Epoch 1810, training loss: 62.14488220214844 = 0.034759312868118286 + 10.0 * 6.211012363433838
Epoch 1810, val loss: 1.2322618961334229
Epoch 1820, training loss: 62.141780853271484 = 0.03413938358426094 + 10.0 * 6.210764408111572
Epoch 1820, val loss: 1.235801100730896
Epoch 1830, training loss: 62.13974380493164 = 0.03353305906057358 + 10.0 * 6.210621356964111
Epoch 1830, val loss: 1.239269733428955
Epoch 1840, training loss: 62.226097106933594 = 0.03293725103139877 + 10.0 * 6.219316005706787
Epoch 1840, val loss: 1.2426495552062988
Epoch 1850, training loss: 62.15389633178711 = 0.03234720230102539 + 10.0 * 6.212154865264893
Epoch 1850, val loss: 1.2461315393447876
Epoch 1860, training loss: 62.13359832763672 = 0.03178038448095322 + 10.0 * 6.210181713104248
Epoch 1860, val loss: 1.2495120763778687
Epoch 1870, training loss: 62.14661407470703 = 0.031231112778186798 + 10.0 * 6.211538314819336
Epoch 1870, val loss: 1.252812147140503
Epoch 1880, training loss: 62.140193939208984 = 0.030688928440213203 + 10.0 * 6.2109503746032715
Epoch 1880, val loss: 1.256265640258789
Epoch 1890, training loss: 62.13336181640625 = 0.03016214817762375 + 10.0 * 6.210319995880127
Epoch 1890, val loss: 1.25956392288208
Epoch 1900, training loss: 62.138282775878906 = 0.029651116579771042 + 10.0 * 6.21086311340332
Epoch 1900, val loss: 1.2628523111343384
Epoch 1910, training loss: 62.12095260620117 = 0.029146360233426094 + 10.0 * 6.2091803550720215
Epoch 1910, val loss: 1.2661678791046143
Epoch 1920, training loss: 62.12929153442383 = 0.028657469898462296 + 10.0 * 6.210063457489014
Epoch 1920, val loss: 1.2693498134613037
Epoch 1930, training loss: 62.13224792480469 = 0.028176480904221535 + 10.0 * 6.210407257080078
Epoch 1930, val loss: 1.2725862264633179
Epoch 1940, training loss: 62.12677764892578 = 0.02770872786641121 + 10.0 * 6.209906578063965
Epoch 1940, val loss: 1.2757428884506226
Epoch 1950, training loss: 62.11442184448242 = 0.02725212275981903 + 10.0 * 6.208716869354248
Epoch 1950, val loss: 1.27879798412323
Epoch 1960, training loss: 62.11241912841797 = 0.02680918015539646 + 10.0 * 6.208560943603516
Epoch 1960, val loss: 1.2819041013717651
Epoch 1970, training loss: 62.13377380371094 = 0.02637716755270958 + 10.0 * 6.210739612579346
Epoch 1970, val loss: 1.2849786281585693
Epoch 1980, training loss: 62.11137008666992 = 0.025947021320462227 + 10.0 * 6.208542346954346
Epoch 1980, val loss: 1.2880163192749023
Epoch 1990, training loss: 62.103946685791016 = 0.025526221841573715 + 10.0 * 6.207841873168945
Epoch 1990, val loss: 1.2911456823349
Epoch 2000, training loss: 62.101768493652344 = 0.025122717022895813 + 10.0 * 6.207664489746094
Epoch 2000, val loss: 1.2941107749938965
Epoch 2010, training loss: 62.136680603027344 = 0.024727173149585724 + 10.0 * 6.211195468902588
Epoch 2010, val loss: 1.2972157001495361
Epoch 2020, training loss: 62.0999755859375 = 0.024327009916305542 + 10.0 * 6.207564830780029
Epoch 2020, val loss: 1.3000577688217163
Epoch 2030, training loss: 62.09502410888672 = 0.02394714578986168 + 10.0 * 6.2071075439453125
Epoch 2030, val loss: 1.3030585050582886
Epoch 2040, training loss: 62.09856033325195 = 0.023576008155941963 + 10.0 * 6.207498550415039
Epoch 2040, val loss: 1.3060506582260132
Epoch 2050, training loss: 62.094051361083984 = 0.023212984204292297 + 10.0 * 6.207083702087402
Epoch 2050, val loss: 1.3088852167129517
Epoch 2060, training loss: 62.083988189697266 = 0.022860536351799965 + 10.0 * 6.206112861633301
Epoch 2060, val loss: 1.311728835105896
Epoch 2070, training loss: 62.09879684448242 = 0.02251678705215454 + 10.0 * 6.207627773284912
Epoch 2070, val loss: 1.3145374059677124
Epoch 2080, training loss: 62.128318786621094 = 0.022176921367645264 + 10.0 * 6.210614204406738
Epoch 2080, val loss: 1.317350149154663
Epoch 2090, training loss: 62.09836959838867 = 0.021834703162312508 + 10.0 * 6.207653522491455
Epoch 2090, val loss: 1.3199620246887207
Epoch 2100, training loss: 62.084922790527344 = 0.021506115794181824 + 10.0 * 6.206341743469238
Epoch 2100, val loss: 1.3227843046188354
Epoch 2110, training loss: 62.072998046875 = 0.021190987899899483 + 10.0 * 6.205180644989014
Epoch 2110, val loss: 1.3255641460418701
Epoch 2120, training loss: 62.06900405883789 = 0.020884044468402863 + 10.0 * 6.204812049865723
Epoch 2120, val loss: 1.328321099281311
Epoch 2130, training loss: 62.08271408081055 = 0.020583683624863625 + 10.0 * 6.206212997436523
Epoch 2130, val loss: 1.3309887647628784
Epoch 2140, training loss: 62.07640838623047 = 0.02028105966746807 + 10.0 * 6.205612659454346
Epoch 2140, val loss: 1.3336430788040161
Epoch 2150, training loss: 62.090049743652344 = 0.01998547650873661 + 10.0 * 6.207006454467773
Epoch 2150, val loss: 1.3363198041915894
Epoch 2160, training loss: 62.09632873535156 = 0.01969912089407444 + 10.0 * 6.207663059234619
Epoch 2160, val loss: 1.3389157056808472
Epoch 2170, training loss: 62.06719970703125 = 0.019417934119701385 + 10.0 * 6.20477819442749
Epoch 2170, val loss: 1.3413488864898682
Epoch 2180, training loss: 62.05850601196289 = 0.019144132733345032 + 10.0 * 6.2039361000061035
Epoch 2180, val loss: 1.3440450429916382
Epoch 2190, training loss: 62.052947998046875 = 0.018878985196352005 + 10.0 * 6.203406810760498
Epoch 2190, val loss: 1.346659779548645
Epoch 2200, training loss: 62.06114196777344 = 0.0186192374676466 + 10.0 * 6.204252243041992
Epoch 2200, val loss: 1.349246621131897
Epoch 2210, training loss: 62.09519958496094 = 0.018358832225203514 + 10.0 * 6.20768404006958
Epoch 2210, val loss: 1.3516780138015747
Epoch 2220, training loss: 62.07099914550781 = 0.01810065656900406 + 10.0 * 6.205289840698242
Epoch 2220, val loss: 1.3538471460342407
Epoch 2230, training loss: 62.04896926879883 = 0.017849110066890717 + 10.0 * 6.2031121253967285
Epoch 2230, val loss: 1.3563218116760254
Epoch 2240, training loss: 62.04732894897461 = 0.017608564347028732 + 10.0 * 6.202971935272217
Epoch 2240, val loss: 1.3588800430297852
Epoch 2250, training loss: 62.04381561279297 = 0.0173733402043581 + 10.0 * 6.202644348144531
Epoch 2250, val loss: 1.3613086938858032
Epoch 2260, training loss: 62.07148742675781 = 0.017141349613666534 + 10.0 * 6.205434322357178
Epoch 2260, val loss: 1.3636422157287598
Epoch 2270, training loss: 62.04037857055664 = 0.016910281032323837 + 10.0 * 6.2023468017578125
Epoch 2270, val loss: 1.3659756183624268
Epoch 2280, training loss: 62.050472259521484 = 0.016686098650097847 + 10.0 * 6.203378677368164
Epoch 2280, val loss: 1.3682249784469604
Epoch 2290, training loss: 62.070655822753906 = 0.01646368019282818 + 10.0 * 6.205419063568115
Epoch 2290, val loss: 1.3705703020095825
Epoch 2300, training loss: 62.037742614746094 = 0.016242142766714096 + 10.0 * 6.202149868011475
Epoch 2300, val loss: 1.372679352760315
Epoch 2310, training loss: 62.03037643432617 = 0.01603402942419052 + 10.0 * 6.201434135437012
Epoch 2310, val loss: 1.375061273574829
Epoch 2320, training loss: 62.02769470214844 = 0.015830187126994133 + 10.0 * 6.201186180114746
Epoch 2320, val loss: 1.3773987293243408
Epoch 2330, training loss: 62.03152847290039 = 0.015630632638931274 + 10.0 * 6.201590061187744
Epoch 2330, val loss: 1.3796323537826538
Epoch 2340, training loss: 62.089534759521484 = 0.015430792234838009 + 10.0 * 6.2074103355407715
Epoch 2340, val loss: 1.3818342685699463
Epoch 2350, training loss: 62.05014419555664 = 0.015231458470225334 + 10.0 * 6.2034912109375
Epoch 2350, val loss: 1.3839740753173828
Epoch 2360, training loss: 62.02932357788086 = 0.015041175298392773 + 10.0 * 6.201428413391113
Epoch 2360, val loss: 1.3860336542129517
Epoch 2370, training loss: 62.023441314697266 = 0.014856241643428802 + 10.0 * 6.2008585929870605
Epoch 2370, val loss: 1.3883204460144043
Epoch 2380, training loss: 62.035823822021484 = 0.01467407401651144 + 10.0 * 6.202115058898926
Epoch 2380, val loss: 1.390458106994629
Epoch 2390, training loss: 62.038719177246094 = 0.01449042372405529 + 10.0 * 6.202422618865967
Epoch 2390, val loss: 1.3925186395645142
Epoch 2400, training loss: 62.02640151977539 = 0.01431042980402708 + 10.0 * 6.20120906829834
Epoch 2400, val loss: 1.3945121765136719
Epoch 2410, training loss: 62.01850128173828 = 0.014137462712824345 + 10.0 * 6.200436592102051
Epoch 2410, val loss: 1.3967386484146118
Epoch 2420, training loss: 62.038272857666016 = 0.013970336876809597 + 10.0 * 6.202430248260498
Epoch 2420, val loss: 1.398686170578003
Epoch 2430, training loss: 62.01738357543945 = 0.013797729276120663 + 10.0 * 6.2003583908081055
Epoch 2430, val loss: 1.4009145498275757
Epoch 2440, training loss: 62.01115417480469 = 0.013631160371005535 + 10.0 * 6.199752330780029
Epoch 2440, val loss: 1.4027513265609741
Epoch 2450, training loss: 62.00696563720703 = 0.013472096994519234 + 10.0 * 6.199349403381348
Epoch 2450, val loss: 1.4048839807510376
Epoch 2460, training loss: 62.01046371459961 = 0.013316120021045208 + 10.0 * 6.199714660644531
Epoch 2460, val loss: 1.4069193601608276
Epoch 2470, training loss: 62.06427001953125 = 0.01316145621240139 + 10.0 * 6.205111026763916
Epoch 2470, val loss: 1.4089168310165405
Epoch 2480, training loss: 62.02348709106445 = 0.013002866879105568 + 10.0 * 6.201048374176025
Epoch 2480, val loss: 1.4106751680374146
Epoch 2490, training loss: 62.00844192504883 = 0.012852330692112446 + 10.0 * 6.199559211730957
Epoch 2490, val loss: 1.4126088619232178
Epoch 2500, training loss: 62.008514404296875 = 0.012707577086985111 + 10.0 * 6.199580669403076
Epoch 2500, val loss: 1.4147177934646606
Epoch 2510, training loss: 62.036834716796875 = 0.012564321979880333 + 10.0 * 6.202426910400391
Epoch 2510, val loss: 1.4166526794433594
Epoch 2520, training loss: 62.01309585571289 = 0.012419791892170906 + 10.0 * 6.200067520141602
Epoch 2520, val loss: 1.418521761894226
Epoch 2530, training loss: 62.008323669433594 = 0.012279048562049866 + 10.0 * 6.199604511260986
Epoch 2530, val loss: 1.420407772064209
Epoch 2540, training loss: 62.00114059448242 = 0.012140313163399696 + 10.0 * 6.198899745941162
Epoch 2540, val loss: 1.422391653060913
Epoch 2550, training loss: 62.01235580444336 = 0.012005208060145378 + 10.0 * 6.200035095214844
Epoch 2550, val loss: 1.424250841140747
Epoch 2560, training loss: 61.993534088134766 = 0.011871151626110077 + 10.0 * 6.198166370391846
Epoch 2560, val loss: 1.4261173009872437
Epoch 2570, training loss: 61.99418258666992 = 0.011741518042981625 + 10.0 * 6.198244094848633
Epoch 2570, val loss: 1.4279547929763794
Epoch 2580, training loss: 61.995765686035156 = 0.011613643728196621 + 10.0 * 6.198415279388428
Epoch 2580, val loss: 1.4297910928726196
Epoch 2590, training loss: 62.02648162841797 = 0.01148846186697483 + 10.0 * 6.201498985290527
Epoch 2590, val loss: 1.4316701889038086
Epoch 2600, training loss: 62.01858139038086 = 0.011360044591128826 + 10.0 * 6.2007222175598145
Epoch 2600, val loss: 1.433495283126831
Epoch 2610, training loss: 61.994232177734375 = 0.011235258542001247 + 10.0 * 6.198299884796143
Epoch 2610, val loss: 1.4350568056106567
Epoch 2620, training loss: 61.998931884765625 = 0.011116818524897099 + 10.0 * 6.198781490325928
Epoch 2620, val loss: 1.4368178844451904
Epoch 2630, training loss: 61.988094329833984 = 0.010997078381478786 + 10.0 * 6.197709560394287
Epoch 2630, val loss: 1.4388295412063599
Epoch 2640, training loss: 61.9801025390625 = 0.010880114510655403 + 10.0 * 6.196922302246094
Epoch 2640, val loss: 1.4405553340911865
Epoch 2650, training loss: 61.980716705322266 = 0.010766354389488697 + 10.0 * 6.196995258331299
Epoch 2650, val loss: 1.4423834085464478
Epoch 2660, training loss: 62.00114059448242 = 0.010655181482434273 + 10.0 * 6.1990485191345215
Epoch 2660, val loss: 1.444159984588623
Epoch 2670, training loss: 61.993865966796875 = 0.010543882846832275 + 10.0 * 6.1983323097229
Epoch 2670, val loss: 1.445572018623352
Epoch 2680, training loss: 61.99826431274414 = 0.010433179326355457 + 10.0 * 6.198782920837402
Epoch 2680, val loss: 1.447272777557373
Epoch 2690, training loss: 61.97731018066406 = 0.01032275054603815 + 10.0 * 6.1966986656188965
Epoch 2690, val loss: 1.4489572048187256
Epoch 2700, training loss: 61.97308349609375 = 0.010216495022177696 + 10.0 * 6.196286678314209
Epoch 2700, val loss: 1.4507752656936646
Epoch 2710, training loss: 61.98155212402344 = 0.010115237906575203 + 10.0 * 6.1971435546875
Epoch 2710, val loss: 1.4523370265960693
Epoch 2720, training loss: 61.9923210144043 = 0.010012639686465263 + 10.0 * 6.198230743408203
Epoch 2720, val loss: 1.4540634155273438
Epoch 2730, training loss: 61.979190826416016 = 0.009908699430525303 + 10.0 * 6.196928024291992
Epoch 2730, val loss: 1.4555984735488892
Epoch 2740, training loss: 61.96579360961914 = 0.009808988310396671 + 10.0 * 6.195598602294922
Epoch 2740, val loss: 1.4571946859359741
Epoch 2750, training loss: 61.96805953979492 = 0.00971288699656725 + 10.0 * 6.195834636688232
Epoch 2750, val loss: 1.458865761756897
Epoch 2760, training loss: 61.98600387573242 = 0.009618047624826431 + 10.0 * 6.197638511657715
Epoch 2760, val loss: 1.4604939222335815
Epoch 2770, training loss: 61.993324279785156 = 0.009521432220935822 + 10.0 * 6.198380470275879
Epoch 2770, val loss: 1.4620426893234253
Epoch 2780, training loss: 61.96564483642578 = 0.009423178620636463 + 10.0 * 6.195622444152832
Epoch 2780, val loss: 1.4635666608810425
Epoch 2790, training loss: 61.96092224121094 = 0.009331222623586655 + 10.0 * 6.195158958435059
Epoch 2790, val loss: 1.4650942087173462
Epoch 2800, training loss: 61.97281265258789 = 0.00924223754554987 + 10.0 * 6.196356773376465
Epoch 2800, val loss: 1.466709017753601
Epoch 2810, training loss: 61.96100997924805 = 0.009152633138000965 + 10.0 * 6.195185661315918
Epoch 2810, val loss: 1.46829354763031
Epoch 2820, training loss: 61.970489501953125 = 0.009065422229468822 + 10.0 * 6.196142673492432
Epoch 2820, val loss: 1.469895601272583
Epoch 2830, training loss: 61.973236083984375 = 0.008978116326034069 + 10.0 * 6.196425437927246
Epoch 2830, val loss: 1.4713283777236938
Epoch 2840, training loss: 61.960819244384766 = 0.008892684243619442 + 10.0 * 6.195192813873291
Epoch 2840, val loss: 1.472975730895996
Epoch 2850, training loss: 61.967384338378906 = 0.008809477090835571 + 10.0 * 6.195857524871826
Epoch 2850, val loss: 1.4744809865951538
Epoch 2860, training loss: 61.97226333618164 = 0.008725046180188656 + 10.0 * 6.196353912353516
Epoch 2860, val loss: 1.4759626388549805
Epoch 2870, training loss: 61.95458221435547 = 0.008644850924611092 + 10.0 * 6.194593906402588
Epoch 2870, val loss: 1.4773582220077515
Epoch 2880, training loss: 61.95124435424805 = 0.00856519490480423 + 10.0 * 6.194268226623535
Epoch 2880, val loss: 1.47891366481781
Epoch 2890, training loss: 61.96749496459961 = 0.008486511185765266 + 10.0 * 6.195900917053223
Epoch 2890, val loss: 1.480495572090149
Epoch 2900, training loss: 61.957679748535156 = 0.008407535031437874 + 10.0 * 6.194927215576172
Epoch 2900, val loss: 1.4818379878997803
Epoch 2910, training loss: 61.970733642578125 = 0.008329847827553749 + 10.0 * 6.196240425109863
Epoch 2910, val loss: 1.4831982851028442
Epoch 2920, training loss: 61.9472770690918 = 0.008252489380538464 + 10.0 * 6.193902492523193
Epoch 2920, val loss: 1.4848134517669678
Epoch 2930, training loss: 61.94799041748047 = 0.008177932351827621 + 10.0 * 6.193981170654297
Epoch 2930, val loss: 1.486263632774353
Epoch 2940, training loss: 61.95602035522461 = 0.008104309439659119 + 10.0 * 6.194791793823242
Epoch 2940, val loss: 1.4877210855484009
Epoch 2950, training loss: 61.98387145996094 = 0.008031346835196018 + 10.0 * 6.19758415222168
Epoch 2950, val loss: 1.4891127347946167
Epoch 2960, training loss: 61.94261932373047 = 0.007957597263157368 + 10.0 * 6.1934661865234375
Epoch 2960, val loss: 1.49057137966156
Epoch 2970, training loss: 61.93465042114258 = 0.007888137362897396 + 10.0 * 6.192676067352295
Epoch 2970, val loss: 1.491840124130249
Epoch 2980, training loss: 61.93254089355469 = 0.007820041850209236 + 10.0 * 6.192471981048584
Epoch 2980, val loss: 1.493430733680725
Epoch 2990, training loss: 61.94060516357422 = 0.007753157056868076 + 10.0 * 6.1932854652404785
Epoch 2990, val loss: 1.4948962926864624
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 87.90992736816406 = 1.9415624141693115 + 10.0 * 8.596837043762207
Epoch 0, val loss: 1.948298454284668
Epoch 10, training loss: 87.89318084716797 = 1.9320399761199951 + 10.0 * 8.596114158630371
Epoch 10, val loss: 1.9384987354278564
Epoch 20, training loss: 87.82260131835938 = 1.9207181930541992 + 10.0 * 8.590188980102539
Epoch 20, val loss: 1.9266117811203003
Epoch 30, training loss: 87.40010070800781 = 1.9065146446228027 + 10.0 * 8.549358367919922
Epoch 30, val loss: 1.9116687774658203
Epoch 40, training loss: 85.21006774902344 = 1.8888814449310303 + 10.0 * 8.33211898803711
Epoch 40, val loss: 1.893560767173767
Epoch 50, training loss: 79.24883270263672 = 1.8686997890472412 + 10.0 * 7.738013744354248
Epoch 50, val loss: 1.8730636835098267
Epoch 60, training loss: 75.26814270019531 = 1.853520393371582 + 10.0 * 7.341462135314941
Epoch 60, val loss: 1.8577595949172974
Epoch 70, training loss: 72.60291290283203 = 1.841800570487976 + 10.0 * 7.076111316680908
Epoch 70, val loss: 1.8457379341125488
Epoch 80, training loss: 71.44560241699219 = 1.8292806148529053 + 10.0 * 6.96163272857666
Epoch 80, val loss: 1.833053708076477
Epoch 90, training loss: 70.28340148925781 = 1.8189252614974976 + 10.0 * 6.846447467803955
Epoch 90, val loss: 1.822687029838562
Epoch 100, training loss: 69.44524383544922 = 1.8108782768249512 + 10.0 * 6.763436794281006
Epoch 100, val loss: 1.8141511678695679
Epoch 110, training loss: 68.88461303710938 = 1.8030906915664673 + 10.0 * 6.708151817321777
Epoch 110, val loss: 1.805573582649231
Epoch 120, training loss: 68.3203353881836 = 1.7950053215026855 + 10.0 * 6.652532577514648
Epoch 120, val loss: 1.796809434890747
Epoch 130, training loss: 67.84074401855469 = 1.787545084953308 + 10.0 * 6.605319976806641
Epoch 130, val loss: 1.7889411449432373
Epoch 140, training loss: 67.47334289550781 = 1.7805297374725342 + 10.0 * 6.569281101226807
Epoch 140, val loss: 1.7815418243408203
Epoch 150, training loss: 67.11344146728516 = 1.7730847597122192 + 10.0 * 6.5340352058410645
Epoch 150, val loss: 1.7735432386398315
Epoch 160, training loss: 66.83335876464844 = 1.7649390697479248 + 10.0 * 6.506842136383057
Epoch 160, val loss: 1.7651634216308594
Epoch 170, training loss: 66.58894348144531 = 1.7558478116989136 + 10.0 * 6.483310222625732
Epoch 170, val loss: 1.7559808492660522
Epoch 180, training loss: 66.41470336914062 = 1.7456527948379517 + 10.0 * 6.466905117034912
Epoch 180, val loss: 1.7458834648132324
Epoch 190, training loss: 66.21587371826172 = 1.7345197200775146 + 10.0 * 6.4481353759765625
Epoch 190, val loss: 1.7348082065582275
Epoch 200, training loss: 66.0616226196289 = 1.7222182750701904 + 10.0 * 6.433940410614014
Epoch 200, val loss: 1.7226897478103638
Epoch 210, training loss: 65.93942260742188 = 1.7085587978363037 + 10.0 * 6.423086643218994
Epoch 210, val loss: 1.7094312906265259
Epoch 220, training loss: 65.824462890625 = 1.6933759450912476 + 10.0 * 6.413108825683594
Epoch 220, val loss: 1.6948472261428833
Epoch 230, training loss: 65.70231628417969 = 1.676680088043213 + 10.0 * 6.402563095092773
Epoch 230, val loss: 1.678772211074829
Epoch 240, training loss: 65.60931396484375 = 1.6584540605545044 + 10.0 * 6.395086288452148
Epoch 240, val loss: 1.66129469871521
Epoch 250, training loss: 65.50643920898438 = 1.6386178731918335 + 10.0 * 6.386782169342041
Epoch 250, val loss: 1.6423975229263306
Epoch 260, training loss: 65.43753051757812 = 1.617156982421875 + 10.0 * 6.38203763961792
Epoch 260, val loss: 1.6220341920852661
Epoch 270, training loss: 65.32090759277344 = 1.5940029621124268 + 10.0 * 6.372690677642822
Epoch 270, val loss: 1.6003077030181885
Epoch 280, training loss: 65.2375717163086 = 1.5693954229354858 + 10.0 * 6.366817474365234
Epoch 280, val loss: 1.5773022174835205
Epoch 290, training loss: 65.16995239257812 = 1.5434298515319824 + 10.0 * 6.36265230178833
Epoch 290, val loss: 1.5532751083374023
Epoch 300, training loss: 65.10124969482422 = 1.5161991119384766 + 10.0 * 6.3585052490234375
Epoch 300, val loss: 1.5282407999038696
Epoch 310, training loss: 64.99578857421875 = 1.4878730773925781 + 10.0 * 6.350791931152344
Epoch 310, val loss: 1.502644419670105
Epoch 320, training loss: 64.92475128173828 = 1.458725094795227 + 10.0 * 6.346602439880371
Epoch 320, val loss: 1.4765546321868896
Epoch 330, training loss: 64.84132385253906 = 1.4290995597839355 + 10.0 * 6.341222763061523
Epoch 330, val loss: 1.4502836465835571
Epoch 340, training loss: 64.76840209960938 = 1.3991892337799072 + 10.0 * 6.336921215057373
Epoch 340, val loss: 1.4240460395812988
Epoch 350, training loss: 64.72402954101562 = 1.3689123392105103 + 10.0 * 6.335511684417725
Epoch 350, val loss: 1.3977084159851074
Epoch 360, training loss: 64.63603210449219 = 1.338417887687683 + 10.0 * 6.329761505126953
Epoch 360, val loss: 1.3716719150543213
Epoch 370, training loss: 64.5794906616211 = 1.308107852935791 + 10.0 * 6.327138423919678
Epoch 370, val loss: 1.346187710762024
Epoch 380, training loss: 64.50048065185547 = 1.2779994010925293 + 10.0 * 6.3222479820251465
Epoch 380, val loss: 1.321107029914856
Epoch 390, training loss: 64.43232727050781 = 1.2480038404464722 + 10.0 * 6.318432807922363
Epoch 390, val loss: 1.2963405847549438
Epoch 400, training loss: 64.39503479003906 = 1.2182530164718628 + 10.0 * 6.317678451538086
Epoch 400, val loss: 1.2724049091339111
Epoch 410, training loss: 64.31916809082031 = 1.1891381740570068 + 10.0 * 6.313002586364746
Epoch 410, val loss: 1.2488943338394165
Epoch 420, training loss: 64.24891662597656 = 1.1604598760604858 + 10.0 * 6.308845520019531
Epoch 420, val loss: 1.2260960340499878
Epoch 430, training loss: 64.19900512695312 = 1.132273554801941 + 10.0 * 6.306673526763916
Epoch 430, val loss: 1.2040584087371826
Epoch 440, training loss: 64.15437316894531 = 1.1043636798858643 + 10.0 * 6.305001258850098
Epoch 440, val loss: 1.1826825141906738
Epoch 450, training loss: 64.08814239501953 = 1.077086329460144 + 10.0 * 6.301105499267578
Epoch 450, val loss: 1.1618504524230957
Epoch 460, training loss: 64.0348892211914 = 1.0503737926483154 + 10.0 * 6.2984514236450195
Epoch 460, val loss: 1.1418641805648804
Epoch 470, training loss: 64.03327178955078 = 1.0241601467132568 + 10.0 * 6.300910949707031
Epoch 470, val loss: 1.1225700378417969
Epoch 480, training loss: 63.959312438964844 = 0.998298704624176 + 10.0 * 6.2961015701293945
Epoch 480, val loss: 1.1037795543670654
Epoch 490, training loss: 63.907081604003906 = 0.9731670022010803 + 10.0 * 6.293391227722168
Epoch 490, val loss: 1.0858795642852783
Epoch 500, training loss: 63.85225296020508 = 0.9486976265907288 + 10.0 * 6.290355682373047
Epoch 500, val loss: 1.068764090538025
Epoch 510, training loss: 63.841182708740234 = 0.9247371554374695 + 10.0 * 6.29164457321167
Epoch 510, val loss: 1.052284836769104
Epoch 520, training loss: 63.76664352416992 = 0.9011611342430115 + 10.0 * 6.286548137664795
Epoch 520, val loss: 1.0366097688674927
Epoch 530, training loss: 63.72318649291992 = 0.8782607316970825 + 10.0 * 6.284492492675781
Epoch 530, val loss: 1.0216498374938965
Epoch 540, training loss: 63.756099700927734 = 0.8559724688529968 + 10.0 * 6.290012836456299
Epoch 540, val loss: 1.00735604763031
Epoch 550, training loss: 63.65336990356445 = 0.8339587450027466 + 10.0 * 6.2819414138793945
Epoch 550, val loss: 0.9937416315078735
Epoch 560, training loss: 63.616024017333984 = 0.8126627206802368 + 10.0 * 6.280335903167725
Epoch 560, val loss: 0.9808836579322815
Epoch 570, training loss: 63.571754455566406 = 0.7919070720672607 + 10.0 * 6.277984619140625
Epoch 570, val loss: 0.9686617255210876
Epoch 580, training loss: 63.61049270629883 = 0.7715038657188416 + 10.0 * 6.283898830413818
Epoch 580, val loss: 0.957079291343689
Epoch 590, training loss: 63.51493835449219 = 0.7516425251960754 + 10.0 * 6.276329517364502
Epoch 590, val loss: 0.9456633925437927
Epoch 600, training loss: 63.471317291259766 = 0.732221245765686 + 10.0 * 6.273909568786621
Epoch 600, val loss: 0.935209333896637
Epoch 610, training loss: 63.43072509765625 = 0.7132899165153503 + 10.0 * 6.271743297576904
Epoch 610, val loss: 0.9252794981002808
Epoch 620, training loss: 63.39868927001953 = 0.6946956515312195 + 10.0 * 6.270399570465088
Epoch 620, val loss: 0.9157638549804688
Epoch 630, training loss: 63.450138092041016 = 0.6763430833816528 + 10.0 * 6.277379512786865
Epoch 630, val loss: 0.9065436124801636
Epoch 640, training loss: 63.35757064819336 = 0.6582762598991394 + 10.0 * 6.2699294090271
Epoch 640, val loss: 0.8977495431900024
Epoch 650, training loss: 63.30786895751953 = 0.6406130194664001 + 10.0 * 6.266725540161133
Epoch 650, val loss: 0.8892947435379028
Epoch 660, training loss: 63.27775573730469 = 0.6233367919921875 + 10.0 * 6.26544189453125
Epoch 660, val loss: 0.8813269138336182
Epoch 670, training loss: 63.25266647338867 = 0.6062802076339722 + 10.0 * 6.264638423919678
Epoch 670, val loss: 0.8736380338668823
Epoch 680, training loss: 63.24736022949219 = 0.5893634557723999 + 10.0 * 6.265799522399902
Epoch 680, val loss: 0.8660517334938049
Epoch 690, training loss: 63.22465896606445 = 0.572851836681366 + 10.0 * 6.265180587768555
Epoch 690, val loss: 0.8587905168533325
Epoch 700, training loss: 63.167259216308594 = 0.5567234754562378 + 10.0 * 6.261053562164307
Epoch 700, val loss: 0.8519064784049988
Epoch 710, training loss: 63.13920593261719 = 0.5408700704574585 + 10.0 * 6.259833335876465
Epoch 710, val loss: 0.845258355140686
Epoch 720, training loss: 63.116451263427734 = 0.5252119898796082 + 10.0 * 6.259123802185059
Epoch 720, val loss: 0.8388508558273315
Epoch 730, training loss: 63.10721206665039 = 0.5097053647041321 + 10.0 * 6.259750843048096
Epoch 730, val loss: 0.8325784802436829
Epoch 740, training loss: 63.08065414428711 = 0.4945497214794159 + 10.0 * 6.258610725402832
Epoch 740, val loss: 0.8265627026557922
Epoch 750, training loss: 63.04188537597656 = 0.47969987988471985 + 10.0 * 6.256218910217285
Epoch 750, val loss: 0.8208111524581909
Epoch 760, training loss: 63.01824951171875 = 0.46515360474586487 + 10.0 * 6.255309581756592
Epoch 760, val loss: 0.8153882026672363
Epoch 770, training loss: 63.03840637207031 = 0.45085588097572327 + 10.0 * 6.258755207061768
Epoch 770, val loss: 0.8101089596748352
Epoch 780, training loss: 62.97946548461914 = 0.4367973208427429 + 10.0 * 6.254266738891602
Epoch 780, val loss: 0.8050434589385986
Epoch 790, training loss: 62.99129867553711 = 0.42305418848991394 + 10.0 * 6.256824493408203
Epoch 790, val loss: 0.8002178072929382
Epoch 800, training loss: 62.94063949584961 = 0.40978461503982544 + 10.0 * 6.253085136413574
Epoch 800, val loss: 0.795619010925293
Epoch 810, training loss: 62.91200256347656 = 0.3967114984989166 + 10.0 * 6.251528739929199
Epoch 810, val loss: 0.7913930416107178
Epoch 820, training loss: 62.889381408691406 = 0.38398152589797974 + 10.0 * 6.250540256500244
Epoch 820, val loss: 0.7874709367752075
Epoch 830, training loss: 62.926212310791016 = 0.3715401887893677 + 10.0 * 6.255467414855957
Epoch 830, val loss: 0.7837144136428833
Epoch 840, training loss: 62.87297439575195 = 0.35941728949546814 + 10.0 * 6.2513556480407715
Epoch 840, val loss: 0.7801742553710938
Epoch 850, training loss: 62.85000228881836 = 0.34759649634361267 + 10.0 * 6.250240802764893
Epoch 850, val loss: 0.7769821882247925
Epoch 860, training loss: 62.81822967529297 = 0.3361945152282715 + 10.0 * 6.248203754425049
Epoch 860, val loss: 0.7739262580871582
Epoch 870, training loss: 62.801361083984375 = 0.32518166303634644 + 10.0 * 6.247618198394775
Epoch 870, val loss: 0.7712696194648743
Epoch 880, training loss: 62.77114486694336 = 0.3144119381904602 + 10.0 * 6.245673179626465
Epoch 880, val loss: 0.7690158486366272
Epoch 890, training loss: 62.82077407836914 = 0.3039703369140625 + 10.0 * 6.251680374145508
Epoch 890, val loss: 0.7669287323951721
Epoch 900, training loss: 62.777000427246094 = 0.29388558864593506 + 10.0 * 6.248311519622803
Epoch 900, val loss: 0.7650160193443298
Epoch 910, training loss: 62.721031188964844 = 0.28402847051620483 + 10.0 * 6.2437005043029785
Epoch 910, val loss: 0.7634861469268799
Epoch 920, training loss: 62.71196365356445 = 0.27455684542655945 + 10.0 * 6.243741035461426
Epoch 920, val loss: 0.7623220086097717
Epoch 930, training loss: 62.77389144897461 = 0.26538193225860596 + 10.0 * 6.250851154327393
Epoch 930, val loss: 0.7613529562950134
Epoch 940, training loss: 62.693397521972656 = 0.256590873003006 + 10.0 * 6.243680477142334
Epoch 940, val loss: 0.7603854537010193
Epoch 950, training loss: 62.66038513183594 = 0.24801765382289886 + 10.0 * 6.241236686706543
Epoch 950, val loss: 0.7598957419395447
Epoch 960, training loss: 62.65476608276367 = 0.2398313283920288 + 10.0 * 6.2414937019348145
Epoch 960, val loss: 0.7596824765205383
Epoch 970, training loss: 62.662113189697266 = 0.2318771779537201 + 10.0 * 6.243023872375488
Epoch 970, val loss: 0.7595910429954529
Epoch 980, training loss: 62.62565994262695 = 0.22415398061275482 + 10.0 * 6.240150451660156
Epoch 980, val loss: 0.7598055601119995
Epoch 990, training loss: 62.60664749145508 = 0.2168101966381073 + 10.0 * 6.238984107971191
Epoch 990, val loss: 0.7602747678756714
Epoch 1000, training loss: 62.59171676635742 = 0.20972192287445068 + 10.0 * 6.238199710845947
Epoch 1000, val loss: 0.7610707879066467
Epoch 1010, training loss: 62.587615966796875 = 0.2029145210981369 + 10.0 * 6.238470077514648
Epoch 1010, val loss: 0.7619985342025757
Epoch 1020, training loss: 62.607078552246094 = 0.19628378748893738 + 10.0 * 6.241079330444336
Epoch 1020, val loss: 0.7631147503852844
Epoch 1030, training loss: 62.59291458129883 = 0.18987984955310822 + 10.0 * 6.2403035163879395
Epoch 1030, val loss: 0.7645214796066284
Epoch 1040, training loss: 62.54157257080078 = 0.18378904461860657 + 10.0 * 6.235778331756592
Epoch 1040, val loss: 0.766019344329834
Epoch 1050, training loss: 62.53432083129883 = 0.1779617816209793 + 10.0 * 6.235635757446289
Epoch 1050, val loss: 0.7678965926170349
Epoch 1060, training loss: 62.51898956298828 = 0.1723794788122177 + 10.0 * 6.234661102294922
Epoch 1060, val loss: 0.7700484395027161
Epoch 1070, training loss: 62.5069465637207 = 0.16701781749725342 + 10.0 * 6.233992576599121
Epoch 1070, val loss: 0.7723363041877747
Epoch 1080, training loss: 62.53709411621094 = 0.16185316443443298 + 10.0 * 6.237524032592773
Epoch 1080, val loss: 0.7746968269348145
Epoch 1090, training loss: 62.531822204589844 = 0.1568050980567932 + 10.0 * 6.237501621246338
Epoch 1090, val loss: 0.7772811055183411
Epoch 1100, training loss: 62.4796257019043 = 0.15200549364089966 + 10.0 * 6.232762336730957
Epoch 1100, val loss: 0.7799602746963501
Epoch 1110, training loss: 62.475467681884766 = 0.14741423726081848 + 10.0 * 6.232805252075195
Epoch 1110, val loss: 0.7828150987625122
Epoch 1120, training loss: 62.47762680053711 = 0.14300106465816498 + 10.0 * 6.233462333679199
Epoch 1120, val loss: 0.7859060764312744
Epoch 1130, training loss: 62.455936431884766 = 0.13873076438903809 + 10.0 * 6.231720924377441
Epoch 1130, val loss: 0.7890222668647766
Epoch 1140, training loss: 62.485652923583984 = 0.13464999198913574 + 10.0 * 6.235100269317627
Epoch 1140, val loss: 0.7922216057777405
Epoch 1150, training loss: 62.43731689453125 = 0.13062679767608643 + 10.0 * 6.230669021606445
Epoch 1150, val loss: 0.7955033183097839
Epoch 1160, training loss: 62.43419647216797 = 0.12684425711631775 + 10.0 * 6.2307353019714355
Epoch 1160, val loss: 0.7988609671592712
Epoch 1170, training loss: 62.42068099975586 = 0.12316469848155975 + 10.0 * 6.2297515869140625
Epoch 1170, val loss: 0.8024483919143677
Epoch 1180, training loss: 62.40801239013672 = 0.11967214196920395 + 10.0 * 6.22883415222168
Epoch 1180, val loss: 0.8061330318450928
Epoch 1190, training loss: 62.40402603149414 = 0.11627517640590668 + 10.0 * 6.2287750244140625
Epoch 1190, val loss: 0.8098143339157104
Epoch 1200, training loss: 62.45110321044922 = 0.11297811567783356 + 10.0 * 6.23381233215332
Epoch 1200, val loss: 0.8135258555412292
Epoch 1210, training loss: 62.38779067993164 = 0.10979931056499481 + 10.0 * 6.227799415588379
Epoch 1210, val loss: 0.8173024654388428
Epoch 1220, training loss: 62.383262634277344 = 0.10672997683286667 + 10.0 * 6.2276530265808105
Epoch 1220, val loss: 0.821246325969696
Epoch 1230, training loss: 62.403900146484375 = 0.10379527509212494 + 10.0 * 6.230010509490967
Epoch 1230, val loss: 0.8252028226852417
Epoch 1240, training loss: 62.39885711669922 = 0.10087994486093521 + 10.0 * 6.229797840118408
Epoch 1240, val loss: 0.8290507197380066
Epoch 1250, training loss: 62.378623962402344 = 0.09812496602535248 + 10.0 * 6.2280497550964355
Epoch 1250, val loss: 0.8330346941947937
Epoch 1260, training loss: 62.35652542114258 = 0.09547027200460434 + 10.0 * 6.226105690002441
Epoch 1260, val loss: 0.8370935320854187
Epoch 1270, training loss: 62.34613800048828 = 0.09289490431547165 + 10.0 * 6.2253241539001465
Epoch 1270, val loss: 0.8411740064620972
Epoch 1280, training loss: 62.3455810546875 = 0.09042468667030334 + 10.0 * 6.225515842437744
Epoch 1280, val loss: 0.8452718257904053
Epoch 1290, training loss: 62.38890838623047 = 0.0880330353975296 + 10.0 * 6.230087757110596
Epoch 1290, val loss: 0.8493673801422119
Epoch 1300, training loss: 62.3534049987793 = 0.0856463760137558 + 10.0 * 6.226775646209717
Epoch 1300, val loss: 0.853369951248169
Epoch 1310, training loss: 62.361331939697266 = 0.08340300619602203 + 10.0 * 6.227792739868164
Epoch 1310, val loss: 0.8575044870376587
Epoch 1320, training loss: 62.319297790527344 = 0.08118050545454025 + 10.0 * 6.223811626434326
Epoch 1320, val loss: 0.8616282343864441
Epoch 1330, training loss: 62.32136535644531 = 0.07906913757324219 + 10.0 * 6.224229335784912
Epoch 1330, val loss: 0.8657439947128296
Epoch 1340, training loss: 62.33279037475586 = 0.07701672613620758 + 10.0 * 6.225577354431152
Epoch 1340, val loss: 0.8698599338531494
Epoch 1350, training loss: 62.30760955810547 = 0.0750378966331482 + 10.0 * 6.223257064819336
Epoch 1350, val loss: 0.8740043640136719
Epoch 1360, training loss: 62.300872802734375 = 0.0731225460767746 + 10.0 * 6.222774982452393
Epoch 1360, val loss: 0.8780812621116638
Epoch 1370, training loss: 62.36638641357422 = 0.07127874344587326 + 10.0 * 6.22951078414917
Epoch 1370, val loss: 0.882212221622467
Epoch 1380, training loss: 62.30358123779297 = 0.0694180354475975 + 10.0 * 6.223416328430176
Epoch 1380, val loss: 0.8861127495765686
Epoch 1390, training loss: 62.28139114379883 = 0.06768468022346497 + 10.0 * 6.221370697021484
Epoch 1390, val loss: 0.8902375102043152
Epoch 1400, training loss: 62.271934509277344 = 0.06599545478820801 + 10.0 * 6.2205939292907715
Epoch 1400, val loss: 0.8942995667457581
Epoch 1410, training loss: 62.31181335449219 = 0.06436370313167572 + 10.0 * 6.22474479675293
Epoch 1410, val loss: 0.8983607292175293
Epoch 1420, training loss: 62.29159927368164 = 0.06277153640985489 + 10.0 * 6.2228827476501465
Epoch 1420, val loss: 0.902318000793457
Epoch 1430, training loss: 62.26439666748047 = 0.061221957206726074 + 10.0 * 6.220317363739014
Epoch 1430, val loss: 0.9063479900360107
Epoch 1440, training loss: 62.257240295410156 = 0.05974481627345085 + 10.0 * 6.219749450683594
Epoch 1440, val loss: 0.9104771614074707
Epoch 1450, training loss: 62.27721405029297 = 0.05831366032361984 + 10.0 * 6.221889972686768
Epoch 1450, val loss: 0.914472222328186
Epoch 1460, training loss: 62.252113342285156 = 0.05688918009400368 + 10.0 * 6.219522476196289
Epoch 1460, val loss: 0.9185633659362793
Epoch 1470, training loss: 62.263492584228516 = 0.055525995790958405 + 10.0 * 6.220796585083008
Epoch 1470, val loss: 0.9224991202354431
Epoch 1480, training loss: 62.263694763183594 = 0.054224733263254166 + 10.0 * 6.220946788787842
Epoch 1480, val loss: 0.9265075922012329
Epoch 1490, training loss: 62.2307243347168 = 0.05293097719550133 + 10.0 * 6.217779159545898
Epoch 1490, val loss: 0.9304546117782593
Epoch 1500, training loss: 62.23190689086914 = 0.05169637128710747 + 10.0 * 6.218020915985107
Epoch 1500, val loss: 0.9344748258590698
Epoch 1510, training loss: 62.22787094116211 = 0.05050041899085045 + 10.0 * 6.217737197875977
Epoch 1510, val loss: 0.9384452700614929
Epoch 1520, training loss: 62.25331115722656 = 0.04933764040470123 + 10.0 * 6.220396995544434
Epoch 1520, val loss: 0.9423056244850159
Epoch 1530, training loss: 62.24139404296875 = 0.048216111958026886 + 10.0 * 6.219317436218262
Epoch 1530, val loss: 0.9462532997131348
Epoch 1540, training loss: 62.21717071533203 = 0.047103043645620346 + 10.0 * 6.217006683349609
Epoch 1540, val loss: 0.9500047564506531
Epoch 1550, training loss: 62.22113800048828 = 0.04604244977235794 + 10.0 * 6.2175092697143555
Epoch 1550, val loss: 0.9539254903793335
Epoch 1560, training loss: 62.2167854309082 = 0.045011065900325775 + 10.0 * 6.217177391052246
Epoch 1560, val loss: 0.9577279686927795
Epoch 1570, training loss: 62.23431396484375 = 0.04401512071490288 + 10.0 * 6.219029903411865
Epoch 1570, val loss: 0.9616782665252686
Epoch 1580, training loss: 62.20176696777344 = 0.04305092990398407 + 10.0 * 6.215871334075928
Epoch 1580, val loss: 0.965362548828125
Epoch 1590, training loss: 62.196407318115234 = 0.04211606830358505 + 10.0 * 6.215429306030273
Epoch 1590, val loss: 0.969255805015564
Epoch 1600, training loss: 62.249298095703125 = 0.041213829070329666 + 10.0 * 6.220808506011963
Epoch 1600, val loss: 0.9729517102241516
Epoch 1610, training loss: 62.22468948364258 = 0.04029503092169762 + 10.0 * 6.21843957901001
Epoch 1610, val loss: 0.9764907360076904
Epoch 1620, training loss: 62.192115783691406 = 0.0394391231238842 + 10.0 * 6.215267658233643
Epoch 1620, val loss: 0.9803394675254822
Epoch 1630, training loss: 62.18232727050781 = 0.03860563039779663 + 10.0 * 6.214372158050537
Epoch 1630, val loss: 0.9839544296264648
Epoch 1640, training loss: 62.17526626586914 = 0.03780052438378334 + 10.0 * 6.213746547698975
Epoch 1640, val loss: 0.9877006411552429
Epoch 1650, training loss: 62.20944595336914 = 0.03701978176832199 + 10.0 * 6.217242240905762
Epoch 1650, val loss: 0.9913201332092285
Epoch 1660, training loss: 62.18426513671875 = 0.03625384345650673 + 10.0 * 6.214800834655762
Epoch 1660, val loss: 0.9948044419288635
Epoch 1670, training loss: 62.17827606201172 = 0.03549336642026901 + 10.0 * 6.214278221130371
Epoch 1670, val loss: 0.9983938932418823
Epoch 1680, training loss: 62.19026565551758 = 0.03476478159427643 + 10.0 * 6.215550422668457
Epoch 1680, val loss: 1.0018497705459595
Epoch 1690, training loss: 62.18359375 = 0.03404538705945015 + 10.0 * 6.214954853057861
Epoch 1690, val loss: 1.0053070783615112
Epoch 1700, training loss: 62.165992736816406 = 0.0333685576915741 + 10.0 * 6.213262557983398
Epoch 1700, val loss: 1.0087965726852417
Epoch 1710, training loss: 62.16522979736328 = 0.03270252048969269 + 10.0 * 6.213252544403076
Epoch 1710, val loss: 1.0122841596603394
Epoch 1720, training loss: 62.17173385620117 = 0.032055050134658813 + 10.0 * 6.213967800140381
Epoch 1720, val loss: 1.0157034397125244
Epoch 1730, training loss: 62.157737731933594 = 0.03141622245311737 + 10.0 * 6.212632179260254
Epoch 1730, val loss: 1.0191023349761963
Epoch 1740, training loss: 62.184898376464844 = 0.03079059347510338 + 10.0 * 6.2154107093811035
Epoch 1740, val loss: 1.0223500728607178
Epoch 1750, training loss: 62.15516662597656 = 0.030194591730833054 + 10.0 * 6.212497234344482
Epoch 1750, val loss: 1.0256460905075073
Epoch 1760, training loss: 62.14057159423828 = 0.029616255313158035 + 10.0 * 6.211095333099365
Epoch 1760, val loss: 1.0289833545684814
Epoch 1770, training loss: 62.134765625 = 0.029050542041659355 + 10.0 * 6.210571765899658
Epoch 1770, val loss: 1.0323879718780518
Epoch 1780, training loss: 62.15229034423828 = 0.02850479632616043 + 10.0 * 6.21237850189209
Epoch 1780, val loss: 1.035536289215088
Epoch 1790, training loss: 62.14788818359375 = 0.027969054877758026 + 10.0 * 6.211991786956787
Epoch 1790, val loss: 1.0388730764389038
Epoch 1800, training loss: 62.17022705078125 = 0.0274417232722044 + 10.0 * 6.214278221130371
Epoch 1800, val loss: 1.0420434474945068
Epoch 1810, training loss: 62.122520446777344 = 0.02693406119942665 + 10.0 * 6.209558486938477
Epoch 1810, val loss: 1.0451512336730957
Epoch 1820, training loss: 62.122371673583984 = 0.0264423917979002 + 10.0 * 6.209592819213867
Epoch 1820, val loss: 1.0483767986297607
Epoch 1830, training loss: 62.122802734375 = 0.025962231680750847 + 10.0 * 6.209683895111084
Epoch 1830, val loss: 1.0515918731689453
Epoch 1840, training loss: 62.18975830078125 = 0.025489700958132744 + 10.0 * 6.216426849365234
Epoch 1840, val loss: 1.0546345710754395
Epoch 1850, training loss: 62.14288330078125 = 0.025035036727786064 + 10.0 * 6.211784839630127
Epoch 1850, val loss: 1.0576802492141724
Epoch 1860, training loss: 62.123138427734375 = 0.024580707773566246 + 10.0 * 6.209855556488037
Epoch 1860, val loss: 1.0607625246047974
Epoch 1870, training loss: 62.15153884887695 = 0.024152405560016632 + 10.0 * 6.212738990783691
Epoch 1870, val loss: 1.0637109279632568
Epoch 1880, training loss: 62.11363983154297 = 0.023721911013126373 + 10.0 * 6.208991527557373
Epoch 1880, val loss: 1.0667974948883057
Epoch 1890, training loss: 62.10615158081055 = 0.02331307716667652 + 10.0 * 6.2082839012146
Epoch 1890, val loss: 1.0697171688079834
Epoch 1900, training loss: 62.107948303222656 = 0.02291279472410679 + 10.0 * 6.208503723144531
Epoch 1900, val loss: 1.072782278060913
Epoch 1910, training loss: 62.15888595581055 = 0.022521469742059708 + 10.0 * 6.21363639831543
Epoch 1910, val loss: 1.0756412744522095
Epoch 1920, training loss: 62.11241149902344 = 0.02214115858078003 + 10.0 * 6.209027290344238
Epoch 1920, val loss: 1.0786024332046509
Epoch 1930, training loss: 62.0991325378418 = 0.02176727168262005 + 10.0 * 6.207736492156982
Epoch 1930, val loss: 1.0814166069030762
Epoch 1940, training loss: 62.094486236572266 = 0.021404247730970383 + 10.0 * 6.207308292388916
Epoch 1940, val loss: 1.0842862129211426
Epoch 1950, training loss: 62.10093688964844 = 0.021054847165942192 + 10.0 * 6.207988262176514
Epoch 1950, val loss: 1.0871455669403076
Epoch 1960, training loss: 62.10433578491211 = 0.020709766075015068 + 10.0 * 6.208362579345703
Epoch 1960, val loss: 1.0899142026901245
Epoch 1970, training loss: 62.11320877075195 = 0.020368464291095734 + 10.0 * 6.20928430557251
Epoch 1970, val loss: 1.0927214622497559
Epoch 1980, training loss: 62.119564056396484 = 0.020034657791256905 + 10.0 * 6.2099528312683105
Epoch 1980, val loss: 1.0952937602996826
Epoch 1990, training loss: 62.09340286254883 = 0.019712107256054878 + 10.0 * 6.207369327545166
Epoch 1990, val loss: 1.0979266166687012
Epoch 2000, training loss: 62.08150863647461 = 0.019397491589188576 + 10.0 * 6.206211090087891
Epoch 2000, val loss: 1.100751519203186
Epoch 2010, training loss: 62.076568603515625 = 0.019095126539468765 + 10.0 * 6.205747127532959
Epoch 2010, val loss: 1.1034048795700073
Epoch 2020, training loss: 62.11164474487305 = 0.018801193684339523 + 10.0 * 6.20928430557251
Epoch 2020, val loss: 1.1060518026351929
Epoch 2030, training loss: 62.07975769042969 = 0.01849975995719433 + 10.0 * 6.206125736236572
Epoch 2030, val loss: 1.1085635423660278
Epoch 2040, training loss: 62.102535247802734 = 0.018205629661679268 + 10.0 * 6.208433151245117
Epoch 2040, val loss: 1.1109435558319092
Epoch 2050, training loss: 62.0737419128418 = 0.017931273207068443 + 10.0 * 6.205580711364746
Epoch 2050, val loss: 1.113783359527588
Epoch 2060, training loss: 62.0844841003418 = 0.017662299796938896 + 10.0 * 6.206682205200195
Epoch 2060, val loss: 1.1162595748901367
Epoch 2070, training loss: 62.08599090576172 = 0.01739194430410862 + 10.0 * 6.206860065460205
Epoch 2070, val loss: 1.1187160015106201
Epoch 2080, training loss: 62.06855773925781 = 0.017128314822912216 + 10.0 * 6.205142974853516
Epoch 2080, val loss: 1.1212533712387085
Epoch 2090, training loss: 62.0662841796875 = 0.016877826303243637 + 10.0 * 6.2049407958984375
Epoch 2090, val loss: 1.123717188835144
Epoch 2100, training loss: 62.099727630615234 = 0.016637472435832024 + 10.0 * 6.208309173583984
Epoch 2100, val loss: 1.1261775493621826
Epoch 2110, training loss: 62.102081298828125 = 0.016390448436141014 + 10.0 * 6.208569049835205
Epoch 2110, val loss: 1.1285576820373535
Epoch 2120, training loss: 62.080806732177734 = 0.01614220440387726 + 10.0 * 6.206466197967529
Epoch 2120, val loss: 1.130730390548706
Epoch 2130, training loss: 62.06278991699219 = 0.015904035419225693 + 10.0 * 6.204688549041748
Epoch 2130, val loss: 1.1330980062484741
Epoch 2140, training loss: 62.05005645751953 = 0.01567964442074299 + 10.0 * 6.203437805175781
Epoch 2140, val loss: 1.135434865951538
Epoch 2150, training loss: 62.05458450317383 = 0.015460606664419174 + 10.0 * 6.203912258148193
Epoch 2150, val loss: 1.1378518342971802
Epoch 2160, training loss: 62.102012634277344 = 0.015244989655911922 + 10.0 * 6.208676815032959
Epoch 2160, val loss: 1.1399719715118408
Epoch 2170, training loss: 62.073936462402344 = 0.015021874569356441 + 10.0 * 6.2058916091918945
Epoch 2170, val loss: 1.142163872718811
Epoch 2180, training loss: 62.059600830078125 = 0.014811900444328785 + 10.0 * 6.204478740692139
Epoch 2180, val loss: 1.1444482803344727
Epoch 2190, training loss: 62.078125 = 0.014605077914893627 + 10.0 * 6.2063517570495605
Epoch 2190, val loss: 1.1466892957687378
Epoch 2200, training loss: 62.04035568237305 = 0.014408723451197147 + 10.0 * 6.202594757080078
Epoch 2200, val loss: 1.1488276720046997
Epoch 2210, training loss: 62.04938888549805 = 0.014211280271410942 + 10.0 * 6.203517913818359
Epoch 2210, val loss: 1.1510595083236694
Epoch 2220, training loss: 62.10151290893555 = 0.014015288092195988 + 10.0 * 6.208749771118164
Epoch 2220, val loss: 1.1530691385269165
Epoch 2230, training loss: 62.05495071411133 = 0.013827175833284855 + 10.0 * 6.2041120529174805
Epoch 2230, val loss: 1.155132532119751
Epoch 2240, training loss: 62.04044723510742 = 0.013640375807881355 + 10.0 * 6.202680587768555
Epoch 2240, val loss: 1.1572866439819336
Epoch 2250, training loss: 62.03340148925781 = 0.013460403308272362 + 10.0 * 6.201993942260742
Epoch 2250, val loss: 1.159453272819519
Epoch 2260, training loss: 62.064945220947266 = 0.013285080902278423 + 10.0 * 6.205165863037109
Epoch 2260, val loss: 1.161460041999817
Epoch 2270, training loss: 62.033851623535156 = 0.013111455366015434 + 10.0 * 6.20207405090332
Epoch 2270, val loss: 1.163318157196045
Epoch 2280, training loss: 62.04407501220703 = 0.01293950155377388 + 10.0 * 6.203113555908203
Epoch 2280, val loss: 1.1652392148971558
Epoch 2290, training loss: 62.05078887939453 = 0.012768777087330818 + 10.0 * 6.203802108764648
Epoch 2290, val loss: 1.1672332286834717
Epoch 2300, training loss: 62.02720642089844 = 0.012609775178134441 + 10.0 * 6.2014594078063965
Epoch 2300, val loss: 1.1693660020828247
Epoch 2310, training loss: 62.02357864379883 = 0.01245008036494255 + 10.0 * 6.201112747192383
Epoch 2310, val loss: 1.171370267868042
Epoch 2320, training loss: 62.046844482421875 = 0.012294921092689037 + 10.0 * 6.203454971313477
Epoch 2320, val loss: 1.17317533493042
Epoch 2330, training loss: 62.04849624633789 = 0.012136300094425678 + 10.0 * 6.203636169433594
Epoch 2330, val loss: 1.175000786781311
Epoch 2340, training loss: 62.02152633666992 = 0.01198555063456297 + 10.0 * 6.200953960418701
Epoch 2340, val loss: 1.176895260810852
Epoch 2350, training loss: 62.02334976196289 = 0.011836822144687176 + 10.0 * 6.201151371002197
Epoch 2350, val loss: 1.1787471771240234
Epoch 2360, training loss: 62.047794342041016 = 0.011690894141793251 + 10.0 * 6.203610420227051
Epoch 2360, val loss: 1.1805520057678223
Epoch 2370, training loss: 62.017906188964844 = 0.011547294445335865 + 10.0 * 6.20063591003418
Epoch 2370, val loss: 1.1824020147323608
Epoch 2380, training loss: 62.021209716796875 = 0.011412297375500202 + 10.0 * 6.200979709625244
Epoch 2380, val loss: 1.184316635131836
Epoch 2390, training loss: 62.07282638549805 = 0.011281193234026432 + 10.0 * 6.206154823303223
Epoch 2390, val loss: 1.1861469745635986
Epoch 2400, training loss: 62.03643035888672 = 0.011132916435599327 + 10.0 * 6.2025299072265625
Epoch 2400, val loss: 1.1875554323196411
Epoch 2410, training loss: 62.011409759521484 = 0.01100311428308487 + 10.0 * 6.200040817260742
Epoch 2410, val loss: 1.1893965005874634
Epoch 2420, training loss: 62.006324768066406 = 0.010871315374970436 + 10.0 * 6.199545383453369
Epoch 2420, val loss: 1.1911042928695679
Epoch 2430, training loss: 62.00541305541992 = 0.010747802443802357 + 10.0 * 6.199466705322266
Epoch 2430, val loss: 1.1928596496582031
Epoch 2440, training loss: 62.048770904541016 = 0.010622628964483738 + 10.0 * 6.203814506530762
Epoch 2440, val loss: 1.1944574117660522
Epoch 2450, training loss: 62.042091369628906 = 0.010497610084712505 + 10.0 * 6.203159332275391
Epoch 2450, val loss: 1.1962509155273438
Epoch 2460, training loss: 62.01019287109375 = 0.010377811267971992 + 10.0 * 6.199981689453125
Epoch 2460, val loss: 1.1978172063827515
Epoch 2470, training loss: 62.002193450927734 = 0.010256115347146988 + 10.0 * 6.199193477630615
Epoch 2470, val loss: 1.1996099948883057
Epoch 2480, training loss: 62.001373291015625 = 0.010142557322978973 + 10.0 * 6.199122905731201
Epoch 2480, val loss: 1.201283574104309
Epoch 2490, training loss: 62.04121398925781 = 0.010031246580183506 + 10.0 * 6.203118324279785
Epoch 2490, val loss: 1.2030460834503174
Epoch 2500, training loss: 62.01213073730469 = 0.00991821102797985 + 10.0 * 6.200221061706543
Epoch 2500, val loss: 1.2043838500976562
Epoch 2510, training loss: 61.990257263183594 = 0.009805313311517239 + 10.0 * 6.198045253753662
Epoch 2510, val loss: 1.206088900566101
Epoch 2520, training loss: 61.98981475830078 = 0.009696800261735916 + 10.0 * 6.198011875152588
Epoch 2520, val loss: 1.2076176404953003
Epoch 2530, training loss: 62.04468536376953 = 0.009590373374521732 + 10.0 * 6.203509330749512
Epoch 2530, val loss: 1.2091206312179565
Epoch 2540, training loss: 62.00651931762695 = 0.009486873634159565 + 10.0 * 6.199703216552734
Epoch 2540, val loss: 1.2107113599777222
Epoch 2550, training loss: 61.99639892578125 = 0.00938284583389759 + 10.0 * 6.19870138168335
Epoch 2550, val loss: 1.2122889757156372
Epoch 2560, training loss: 61.990478515625 = 0.009281322360038757 + 10.0 * 6.198119640350342
Epoch 2560, val loss: 1.2138935327529907
Epoch 2570, training loss: 61.98834228515625 = 0.00918398704379797 + 10.0 * 6.197915554046631
Epoch 2570, val loss: 1.215432047843933
Epoch 2580, training loss: 62.02647399902344 = 0.009087562561035156 + 10.0 * 6.2017388343811035
Epoch 2580, val loss: 1.216960072517395
Epoch 2590, training loss: 62.00392150878906 = 0.008986124768853188 + 10.0 * 6.199493408203125
Epoch 2590, val loss: 1.218051791191101
Epoch 2600, training loss: 61.975791931152344 = 0.008892644196748734 + 10.0 * 6.196690082550049
Epoch 2600, val loss: 1.2196009159088135
Epoch 2610, training loss: 61.97791290283203 = 0.008799707517027855 + 10.0 * 6.196911334991455
Epoch 2610, val loss: 1.2211802005767822
Epoch 2620, training loss: 62.00087356567383 = 0.008710416033864021 + 10.0 * 6.199216365814209
Epoch 2620, val loss: 1.2225592136383057
Epoch 2630, training loss: 61.99187469482422 = 0.008619003929197788 + 10.0 * 6.1983256340026855
Epoch 2630, val loss: 1.2239940166473389
Epoch 2640, training loss: 61.9890022277832 = 0.008529556915163994 + 10.0 * 6.198047161102295
Epoch 2640, val loss: 1.2253490686416626
Epoch 2650, training loss: 61.990047454833984 = 0.008441956713795662 + 10.0 * 6.198160648345947
Epoch 2650, val loss: 1.2267324924468994
Epoch 2660, training loss: 61.994384765625 = 0.00835612416267395 + 10.0 * 6.198602676391602
Epoch 2660, val loss: 1.228196144104004
Epoch 2670, training loss: 61.978912353515625 = 0.0082760164514184 + 10.0 * 6.197063446044922
Epoch 2670, val loss: 1.2296339273452759
Epoch 2680, training loss: 61.98743438720703 = 0.00819377414882183 + 10.0 * 6.1979241371154785
Epoch 2680, val loss: 1.231157660484314
Epoch 2690, training loss: 61.97740936279297 = 0.008109156042337418 + 10.0 * 6.196929931640625
Epoch 2690, val loss: 1.2323963642120361
Epoch 2700, training loss: 61.97598648071289 = 0.008029398508369923 + 10.0 * 6.196795463562012
Epoch 2700, val loss: 1.2336206436157227
Epoch 2710, training loss: 61.99476623535156 = 0.007951631210744381 + 10.0 * 6.198681831359863
Epoch 2710, val loss: 1.2350385189056396
Epoch 2720, training loss: 61.979347229003906 = 0.007870718836784363 + 10.0 * 6.197147846221924
Epoch 2720, val loss: 1.236330270767212
Epoch 2730, training loss: 61.98628234863281 = 0.007796505466103554 + 10.0 * 6.197848320007324
Epoch 2730, val loss: 1.237702488899231
Epoch 2740, training loss: 61.972286224365234 = 0.0077210343442857265 + 10.0 * 6.196456432342529
Epoch 2740, val loss: 1.2389336824417114
Epoch 2750, training loss: 61.96404266357422 = 0.007645684760063887 + 10.0 * 6.195639610290527
Epoch 2750, val loss: 1.2403709888458252
Epoch 2760, training loss: 61.960670471191406 = 0.007574805989861488 + 10.0 * 6.195309638977051
Epoch 2760, val loss: 1.241715908050537
Epoch 2770, training loss: 62.02526092529297 = 0.007507815957069397 + 10.0 * 6.201775550842285
Epoch 2770, val loss: 1.242999792098999
Epoch 2780, training loss: 61.97819137573242 = 0.007428587879985571 + 10.0 * 6.197076320648193
Epoch 2780, val loss: 1.2437397241592407
Epoch 2790, training loss: 61.96597671508789 = 0.0073599135503172874 + 10.0 * 6.19586181640625
Epoch 2790, val loss: 1.2452863454818726
Epoch 2800, training loss: 61.95330047607422 = 0.007288375403732061 + 10.0 * 6.194601058959961
Epoch 2800, val loss: 1.2464845180511475
Epoch 2810, training loss: 61.95954132080078 = 0.007223421707749367 + 10.0 * 6.1952314376831055
Epoch 2810, val loss: 1.2477785348892212
Epoch 2820, training loss: 61.998077392578125 = 0.007155958563089371 + 10.0 * 6.199091911315918
Epoch 2820, val loss: 1.2489099502563477
Epoch 2830, training loss: 61.977638244628906 = 0.00708938529714942 + 10.0 * 6.197054862976074
Epoch 2830, val loss: 1.2500430345535278
Epoch 2840, training loss: 61.95471954345703 = 0.007024792488664389 + 10.0 * 6.194769382476807
Epoch 2840, val loss: 1.251230239868164
Epoch 2850, training loss: 61.949398040771484 = 0.006961526349186897 + 10.0 * 6.194243431091309
Epoch 2850, val loss: 1.2525099515914917
Epoch 2860, training loss: 61.98005294799805 = 0.006902662571519613 + 10.0 * 6.197315216064453
Epoch 2860, val loss: 1.253807544708252
Epoch 2870, training loss: 61.959686279296875 = 0.006836575921624899 + 10.0 * 6.195284843444824
Epoch 2870, val loss: 1.254758358001709
Epoch 2880, training loss: 61.948917388916016 = 0.0067771305330097675 + 10.0 * 6.1942138671875
Epoch 2880, val loss: 1.255853533744812
Epoch 2890, training loss: 61.95335006713867 = 0.0067176055163145065 + 10.0 * 6.194663047790527
Epoch 2890, val loss: 1.2571219205856323
Epoch 2900, training loss: 61.94768142700195 = 0.006658448837697506 + 10.0 * 6.1941022872924805
Epoch 2900, val loss: 1.2581607103347778
Epoch 2910, training loss: 61.94526290893555 = 0.006601368077099323 + 10.0 * 6.193865776062012
Epoch 2910, val loss: 1.259489893913269
Epoch 2920, training loss: 62.016624450683594 = 0.0065491292625665665 + 10.0 * 6.20100736618042
Epoch 2920, val loss: 1.260620355606079
Epoch 2930, training loss: 61.9826774597168 = 0.006487115751951933 + 10.0 * 6.1976189613342285
Epoch 2930, val loss: 1.2612788677215576
Epoch 2940, training loss: 61.95331954956055 = 0.006428245920687914 + 10.0 * 6.1946892738342285
Epoch 2940, val loss: 1.2624953985214233
Epoch 2950, training loss: 61.936092376708984 = 0.006373290903866291 + 10.0 * 6.192971706390381
Epoch 2950, val loss: 1.263577938079834
Epoch 2960, training loss: 61.93381118774414 = 0.006320890039205551 + 10.0 * 6.1927490234375
Epoch 2960, val loss: 1.2646756172180176
Epoch 2970, training loss: 62.0321159362793 = 0.006268398370593786 + 10.0 * 6.202584743499756
Epoch 2970, val loss: 1.2655421495437622
Epoch 2980, training loss: 61.98072052001953 = 0.006216263398528099 + 10.0 * 6.197450160980225
Epoch 2980, val loss: 1.2667099237442017
Epoch 2990, training loss: 61.94097137451172 = 0.006159969139844179 + 10.0 * 6.193480968475342
Epoch 2990, val loss: 1.2676204442977905
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8360569319978914
The final CL Acc:0.77037, 0.01889, The final GNN Acc:0.83869, 0.00215
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11666])
remove edge: torch.Size([2, 9570])
updated graph: torch.Size([2, 10680])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.90753173828125 = 1.9388978481292725 + 10.0 * 8.596863746643066
Epoch 0, val loss: 1.9325649738311768
Epoch 10, training loss: 87.89241790771484 = 1.9292192459106445 + 10.0 * 8.596319198608398
Epoch 10, val loss: 1.9234580993652344
Epoch 20, training loss: 87.83334350585938 = 1.917598009109497 + 10.0 * 8.591574668884277
Epoch 20, val loss: 1.9121782779693604
Epoch 30, training loss: 87.4754638671875 = 1.9030373096466064 + 10.0 * 8.557242393493652
Epoch 30, val loss: 1.8979123830795288
Epoch 40, training loss: 85.57083129882812 = 1.8853709697723389 + 10.0 * 8.368546485900879
Epoch 40, val loss: 1.881144404411316
Epoch 50, training loss: 80.57600402832031 = 1.865558385848999 + 10.0 * 7.871044635772705
Epoch 50, val loss: 1.8627961874008179
Epoch 60, training loss: 77.75513458251953 = 1.8487907648086548 + 10.0 * 7.590633869171143
Epoch 60, val loss: 1.848364233970642
Epoch 70, training loss: 75.3659896850586 = 1.8378705978393555 + 10.0 * 7.35281229019165
Epoch 70, val loss: 1.8388677835464478
Epoch 80, training loss: 73.14752960205078 = 1.8271417617797852 + 10.0 * 7.1320390701293945
Epoch 80, val loss: 1.8289095163345337
Epoch 90, training loss: 71.58836364746094 = 1.817163348197937 + 10.0 * 6.977120399475098
Epoch 90, val loss: 1.820003628730774
Epoch 100, training loss: 70.3927230834961 = 1.8066638708114624 + 10.0 * 6.858605861663818
Epoch 100, val loss: 1.8107022047042847
Epoch 110, training loss: 69.64212799072266 = 1.7958132028579712 + 10.0 * 6.784631729125977
Epoch 110, val loss: 1.8009499311447144
Epoch 120, training loss: 69.05370330810547 = 1.7847819328308105 + 10.0 * 6.726892471313477
Epoch 120, val loss: 1.7908470630645752
Epoch 130, training loss: 68.6253433227539 = 1.7732458114624023 + 10.0 * 6.685209274291992
Epoch 130, val loss: 1.780565857887268
Epoch 140, training loss: 68.34184265136719 = 1.7613006830215454 + 10.0 * 6.658054351806641
Epoch 140, val loss: 1.7701199054718018
Epoch 150, training loss: 67.9041519165039 = 1.7485523223876953 + 10.0 * 6.615560054779053
Epoch 150, val loss: 1.75931715965271
Epoch 160, training loss: 67.54692077636719 = 1.7353254556655884 + 10.0 * 6.5811591148376465
Epoch 160, val loss: 1.748041033744812
Epoch 170, training loss: 67.24703979492188 = 1.7211947441101074 + 10.0 * 6.552585124969482
Epoch 170, val loss: 1.736114740371704
Epoch 180, training loss: 67.00227355957031 = 1.7054933309555054 + 10.0 * 6.5296783447265625
Epoch 180, val loss: 1.7229459285736084
Epoch 190, training loss: 66.76593780517578 = 1.6880549192428589 + 10.0 * 6.507787704467773
Epoch 190, val loss: 1.7084643840789795
Epoch 200, training loss: 66.57588958740234 = 1.6690447330474854 + 10.0 * 6.490684509277344
Epoch 200, val loss: 1.6927651166915894
Epoch 210, training loss: 66.47427368164062 = 1.6484405994415283 + 10.0 * 6.482583999633789
Epoch 210, val loss: 1.6758805513381958
Epoch 220, training loss: 66.27952575683594 = 1.6262911558151245 + 10.0 * 6.4653239250183105
Epoch 220, val loss: 1.657672643661499
Epoch 230, training loss: 66.12915802001953 = 1.6026216745376587 + 10.0 * 6.452653408050537
Epoch 230, val loss: 1.6384048461914062
Epoch 240, training loss: 65.99886322021484 = 1.577467441558838 + 10.0 * 6.442139625549316
Epoch 240, val loss: 1.6179628372192383
Epoch 250, training loss: 65.90226745605469 = 1.5507957935333252 + 10.0 * 6.435147285461426
Epoch 250, val loss: 1.5965697765350342
Epoch 260, training loss: 65.788330078125 = 1.5225367546081543 + 10.0 * 6.426578998565674
Epoch 260, val loss: 1.573954701423645
Epoch 270, training loss: 65.66629028320312 = 1.4933539628982544 + 10.0 * 6.417293548583984
Epoch 270, val loss: 1.5507627725601196
Epoch 280, training loss: 65.56233215332031 = 1.4631332159042358 + 10.0 * 6.409919738769531
Epoch 280, val loss: 1.5270719528198242
Epoch 290, training loss: 65.47357940673828 = 1.4321151971817017 + 10.0 * 6.404146671295166
Epoch 290, val loss: 1.5029269456863403
Epoch 300, training loss: 65.40596008300781 = 1.4003939628601074 + 10.0 * 6.400556564331055
Epoch 300, val loss: 1.4785454273223877
Epoch 310, training loss: 65.28424835205078 = 1.3683037757873535 + 10.0 * 6.391594409942627
Epoch 310, val loss: 1.4542443752288818
Epoch 320, training loss: 65.19447326660156 = 1.3359085321426392 + 10.0 * 6.385856628417969
Epoch 320, val loss: 1.4299381971359253
Epoch 330, training loss: 65.1114501953125 = 1.303284764289856 + 10.0 * 6.380816459655762
Epoch 330, val loss: 1.4055438041687012
Epoch 340, training loss: 65.02324676513672 = 1.2705039978027344 + 10.0 * 6.375274181365967
Epoch 340, val loss: 1.3812553882598877
Epoch 350, training loss: 64.95960998535156 = 1.2375853061676025 + 10.0 * 6.372202396392822
Epoch 350, val loss: 1.3570363521575928
Epoch 360, training loss: 64.87049865722656 = 1.2045419216156006 + 10.0 * 6.366595268249512
Epoch 360, val loss: 1.3327656984329224
Epoch 370, training loss: 64.79135131835938 = 1.1716217994689941 + 10.0 * 6.361973285675049
Epoch 370, val loss: 1.30858314037323
Epoch 380, training loss: 64.7286376953125 = 1.138683795928955 + 10.0 * 6.35899543762207
Epoch 380, val loss: 1.2844880819320679
Epoch 390, training loss: 64.71039581298828 = 1.1057665348052979 + 10.0 * 6.360462665557861
Epoch 390, val loss: 1.2607299089431763
Epoch 400, training loss: 64.59917449951172 = 1.0732486248016357 + 10.0 * 6.352592468261719
Epoch 400, val loss: 1.2370437383651733
Epoch 410, training loss: 64.5205078125 = 1.0411912202835083 + 10.0 * 6.347931861877441
Epoch 410, val loss: 1.2136001586914062
Epoch 420, training loss: 64.45751953125 = 1.0096204280853271 + 10.0 * 6.344789981842041
Epoch 420, val loss: 1.1906505823135376
Epoch 430, training loss: 64.4643325805664 = 0.9785981178283691 + 10.0 * 6.348573684692383
Epoch 430, val loss: 1.1681158542633057
Epoch 440, training loss: 64.35620880126953 = 0.9482330679893494 + 10.0 * 6.3407979011535645
Epoch 440, val loss: 1.1468144655227661
Epoch 450, training loss: 64.28807067871094 = 0.9187219142913818 + 10.0 * 6.336935043334961
Epoch 450, val loss: 1.1261576414108276
Epoch 460, training loss: 64.23755645751953 = 0.8901204466819763 + 10.0 * 6.334743499755859
Epoch 460, val loss: 1.1063569784164429
Epoch 470, training loss: 64.19036102294922 = 0.8623772859573364 + 10.0 * 6.332798004150391
Epoch 470, val loss: 1.087523341178894
Epoch 480, training loss: 64.13113403320312 = 0.8356167078018188 + 10.0 * 6.329551696777344
Epoch 480, val loss: 1.0698635578155518
Epoch 490, training loss: 64.08134460449219 = 0.809921145439148 + 10.0 * 6.327142715454102
Epoch 490, val loss: 1.0531470775604248
Epoch 500, training loss: 64.09677124023438 = 0.7852596044540405 + 10.0 * 6.331151008605957
Epoch 500, val loss: 1.0377309322357178
Epoch 510, training loss: 63.99577331542969 = 0.7614365220069885 + 10.0 * 6.323433876037598
Epoch 510, val loss: 1.0234146118164062
Epoch 520, training loss: 63.94573974609375 = 0.7386838793754578 + 10.0 * 6.320705413818359
Epoch 520, val loss: 1.0098737478256226
Epoch 530, training loss: 63.9085807800293 = 0.7168603539466858 + 10.0 * 6.319171905517578
Epoch 530, val loss: 0.997553288936615
Epoch 540, training loss: 63.902652740478516 = 0.6958242058753967 + 10.0 * 6.320683002471924
Epoch 540, val loss: 0.9863701462745667
Epoch 550, training loss: 63.82444763183594 = 0.6756891012191772 + 10.0 * 6.314875602722168
Epoch 550, val loss: 0.9757580757141113
Epoch 560, training loss: 63.78563690185547 = 0.6563678979873657 + 10.0 * 6.312926769256592
Epoch 560, val loss: 0.9666298031806946
Epoch 570, training loss: 63.73958969116211 = 0.6377292275428772 + 10.0 * 6.31018590927124
Epoch 570, val loss: 0.9579480290412903
Epoch 580, training loss: 63.76060485839844 = 0.6197695136070251 + 10.0 * 6.314083576202393
Epoch 580, val loss: 0.9500657916069031
Epoch 590, training loss: 63.734920501708984 = 0.6021466851234436 + 10.0 * 6.313277244567871
Epoch 590, val loss: 0.9426539540290833
Epoch 600, training loss: 63.64859390258789 = 0.5852372646331787 + 10.0 * 6.306335926055908
Epoch 600, val loss: 0.9360964894294739
Epoch 610, training loss: 63.59868621826172 = 0.5688768625259399 + 10.0 * 6.302980899810791
Epoch 610, val loss: 0.930017352104187
Epoch 620, training loss: 63.570369720458984 = 0.5529506206512451 + 10.0 * 6.301741600036621
Epoch 620, val loss: 0.9245017766952515
Epoch 630, training loss: 63.61947250366211 = 0.5373731851577759 + 10.0 * 6.3082098960876465
Epoch 630, val loss: 0.9194711446762085
Epoch 640, training loss: 63.55451583862305 = 0.5221455097198486 + 10.0 * 6.303236961364746
Epoch 640, val loss: 0.9146393537521362
Epoch 650, training loss: 63.484127044677734 = 0.5073586702346802 + 10.0 * 6.297677040100098
Epoch 650, val loss: 0.9106764793395996
Epoch 660, training loss: 63.4478759765625 = 0.49296799302101135 + 10.0 * 6.295490741729736
Epoch 660, val loss: 0.9067825078964233
Epoch 670, training loss: 63.417579650878906 = 0.4789256751537323 + 10.0 * 6.29386568069458
Epoch 670, val loss: 0.9032901525497437
Epoch 680, training loss: 63.481693267822266 = 0.4652230441570282 + 10.0 * 6.301647186279297
Epoch 680, val loss: 0.900208055973053
Epoch 690, training loss: 63.37131881713867 = 0.4517764747142792 + 10.0 * 6.291954517364502
Epoch 690, val loss: 0.8973203897476196
Epoch 700, training loss: 63.37019729614258 = 0.43872252106666565 + 10.0 * 6.293147563934326
Epoch 700, val loss: 0.895107626914978
Epoch 710, training loss: 63.31908416748047 = 0.4259983003139496 + 10.0 * 6.289308547973633
Epoch 710, val loss: 0.8927431702613831
Epoch 720, training loss: 63.29270935058594 = 0.4136390686035156 + 10.0 * 6.287907123565674
Epoch 720, val loss: 0.8908206224441528
Epoch 730, training loss: 63.27585220336914 = 0.40162894129753113 + 10.0 * 6.287422180175781
Epoch 730, val loss: 0.8890834450721741
Epoch 740, training loss: 63.27946472167969 = 0.38992300629615784 + 10.0 * 6.288954257965088
Epoch 740, val loss: 0.8878806233406067
Epoch 750, training loss: 63.24571228027344 = 0.37850236892700195 + 10.0 * 6.286721229553223
Epoch 750, val loss: 0.8871170878410339
Epoch 760, training loss: 63.19350814819336 = 0.3673819899559021 + 10.0 * 6.2826128005981445
Epoch 760, val loss: 0.8862407803535461
Epoch 770, training loss: 63.181297302246094 = 0.3566102981567383 + 10.0 * 6.282468795776367
Epoch 770, val loss: 0.8857939839363098
Epoch 780, training loss: 63.19620132446289 = 0.3461102545261383 + 10.0 * 6.285008907318115
Epoch 780, val loss: 0.8856145143508911
Epoch 790, training loss: 63.14897537231445 = 0.3359260559082031 + 10.0 * 6.281304836273193
Epoch 790, val loss: 0.8857144117355347
Epoch 800, training loss: 63.11840057373047 = 0.3260086476802826 + 10.0 * 6.279239177703857
Epoch 800, val loss: 0.8860334157943726
Epoch 810, training loss: 63.11957931518555 = 0.31638261675834656 + 10.0 * 6.280319690704346
Epoch 810, val loss: 0.8865754008293152
Epoch 820, training loss: 63.10963439941406 = 0.3070104420185089 + 10.0 * 6.280262470245361
Epoch 820, val loss: 0.8872045874595642
Epoch 830, training loss: 63.07786560058594 = 0.2978677451610565 + 10.0 * 6.2779998779296875
Epoch 830, val loss: 0.8883269429206848
Epoch 840, training loss: 63.03483200073242 = 0.28903254866600037 + 10.0 * 6.274580001831055
Epoch 840, val loss: 0.8897241950035095
Epoch 850, training loss: 63.025390625 = 0.28045621514320374 + 10.0 * 6.274493217468262
Epoch 850, val loss: 0.8912865519523621
Epoch 860, training loss: 63.108795166015625 = 0.272076278924942 + 10.0 * 6.283671855926514
Epoch 860, val loss: 0.8932079076766968
Epoch 870, training loss: 63.01908874511719 = 0.26384299993515015 + 10.0 * 6.275524616241455
Epoch 870, val loss: 0.8947323560714722
Epoch 880, training loss: 62.98829650878906 = 0.25591322779655457 + 10.0 * 6.273238182067871
Epoch 880, val loss: 0.8964053392410278
Epoch 890, training loss: 62.95399856567383 = 0.24818670749664307 + 10.0 * 6.270581245422363
Epoch 890, val loss: 0.8987799286842346
Epoch 900, training loss: 62.94181442260742 = 0.24068720638751984 + 10.0 * 6.27011251449585
Epoch 900, val loss: 0.9009262919425964
Epoch 910, training loss: 62.95545196533203 = 0.23338130116462708 + 10.0 * 6.272206783294678
Epoch 910, val loss: 0.90334153175354
Epoch 920, training loss: 62.90718078613281 = 0.22626619040966034 + 10.0 * 6.268091678619385
Epoch 920, val loss: 0.9058125019073486
Epoch 930, training loss: 62.90676498413086 = 0.2193530946969986 + 10.0 * 6.268741130828857
Epoch 930, val loss: 0.9085074067115784
Epoch 940, training loss: 62.91878890991211 = 0.2126263529062271 + 10.0 * 6.270616054534912
Epoch 940, val loss: 0.9110599756240845
Epoch 950, training loss: 62.88914108276367 = 0.20610733330249786 + 10.0 * 6.268303394317627
Epoch 950, val loss: 0.9137990474700928
Epoch 960, training loss: 62.86098098754883 = 0.19976484775543213 + 10.0 * 6.266121864318848
Epoch 960, val loss: 0.917282223701477
Epoch 970, training loss: 62.84755325317383 = 0.1936493217945099 + 10.0 * 6.265390396118164
Epoch 970, val loss: 0.9202020168304443
Epoch 980, training loss: 62.85920333862305 = 0.18770886957645416 + 10.0 * 6.267149448394775
Epoch 980, val loss: 0.923760175704956
Epoch 990, training loss: 62.842041015625 = 0.18189454078674316 + 10.0 * 6.266014575958252
Epoch 990, val loss: 0.9266312718391418
Epoch 1000, training loss: 62.81370162963867 = 0.17626360058784485 + 10.0 * 6.263743877410889
Epoch 1000, val loss: 0.9302980303764343
Epoch 1010, training loss: 62.79329299926758 = 0.17084211111068726 + 10.0 * 6.262245178222656
Epoch 1010, val loss: 0.9333726167678833
Epoch 1020, training loss: 62.78615951538086 = 0.16559262573719025 + 10.0 * 6.262056827545166
Epoch 1020, val loss: 0.93724524974823
Epoch 1030, training loss: 62.813297271728516 = 0.16047729551792145 + 10.0 * 6.265282154083252
Epoch 1030, val loss: 0.9408034682273865
Epoch 1040, training loss: 62.78689193725586 = 0.15549975633621216 + 10.0 * 6.263139247894287
Epoch 1040, val loss: 0.9448719620704651
Epoch 1050, training loss: 62.77687454223633 = 0.1506776362657547 + 10.0 * 6.262619972229004
Epoch 1050, val loss: 0.9486324787139893
Epoch 1060, training loss: 62.74307632446289 = 0.14602699875831604 + 10.0 * 6.259705066680908
Epoch 1060, val loss: 0.9527530074119568
Epoch 1070, training loss: 62.73755645751953 = 0.14151829481124878 + 10.0 * 6.259603977203369
Epoch 1070, val loss: 0.956998348236084
Epoch 1080, training loss: 62.7239990234375 = 0.1371341198682785 + 10.0 * 6.258686542510986
Epoch 1080, val loss: 0.9610053300857544
Epoch 1090, training loss: 62.72578811645508 = 0.13288559019565582 + 10.0 * 6.2592902183532715
Epoch 1090, val loss: 0.9649541974067688
Epoch 1100, training loss: 62.754730224609375 = 0.12876005470752716 + 10.0 * 6.26259708404541
Epoch 1100, val loss: 0.9692379236221313
Epoch 1110, training loss: 62.68765640258789 = 0.12476619333028793 + 10.0 * 6.256289005279541
Epoch 1110, val loss: 0.9739518761634827
Epoch 1120, training loss: 62.672367095947266 = 0.12090160697698593 + 10.0 * 6.255146503448486
Epoch 1120, val loss: 0.9781139492988586
Epoch 1130, training loss: 62.668190002441406 = 0.11716717481613159 + 10.0 * 6.255102157592773
Epoch 1130, val loss: 0.9826548099517822
Epoch 1140, training loss: 62.74919509887695 = 0.11354176700115204 + 10.0 * 6.263565540313721
Epoch 1140, val loss: 0.9869799613952637
Epoch 1150, training loss: 62.65437698364258 = 0.10999368131160736 + 10.0 * 6.254438400268555
Epoch 1150, val loss: 0.9920303821563721
Epoch 1160, training loss: 62.63453674316406 = 0.10659799724817276 + 10.0 * 6.252793788909912
Epoch 1160, val loss: 0.9966370463371277
Epoch 1170, training loss: 62.62504959106445 = 0.1033230572938919 + 10.0 * 6.252172470092773
Epoch 1170, val loss: 1.0014071464538574
Epoch 1180, training loss: 62.73912811279297 = 0.10015708953142166 + 10.0 * 6.263896942138672
Epoch 1180, val loss: 1.006072998046875
Epoch 1190, training loss: 62.67978286743164 = 0.09706582874059677 + 10.0 * 6.25827169418335
Epoch 1190, val loss: 1.0110485553741455
Epoch 1200, training loss: 62.606971740722656 = 0.09407909214496613 + 10.0 * 6.251289367675781
Epoch 1200, val loss: 1.0159215927124023
Epoch 1210, training loss: 62.591915130615234 = 0.09121908247470856 + 10.0 * 6.250069618225098
Epoch 1210, val loss: 1.020946979522705
Epoch 1220, training loss: 62.58125686645508 = 0.08846878260374069 + 10.0 * 6.249278545379639
Epoch 1220, val loss: 1.0261132717132568
Epoch 1230, training loss: 62.57264709472656 = 0.08581401407718658 + 10.0 * 6.248682975769043
Epoch 1230, val loss: 1.0312986373901367
Epoch 1240, training loss: 62.62464141845703 = 0.08325446397066116 + 10.0 * 6.254138469696045
Epoch 1240, val loss: 1.0369563102722168
Epoch 1250, training loss: 62.61720275878906 = 0.08072502166032791 + 10.0 * 6.253647804260254
Epoch 1250, val loss: 1.041174054145813
Epoch 1260, training loss: 62.57433319091797 = 0.07830128073692322 + 10.0 * 6.249603271484375
Epoch 1260, val loss: 1.0462571382522583
Epoch 1270, training loss: 62.59040069580078 = 0.07596232742071152 + 10.0 * 6.251443862915039
Epoch 1270, val loss: 1.0515660047531128
Epoch 1280, training loss: 62.549522399902344 = 0.07371284067630768 + 10.0 * 6.2475810050964355
Epoch 1280, val loss: 1.0567727088928223
Epoch 1290, training loss: 62.53460693359375 = 0.07153885811567307 + 10.0 * 6.246306896209717
Epoch 1290, val loss: 1.061648964881897
Epoch 1300, training loss: 62.52387237548828 = 0.0694434642791748 + 10.0 * 6.245442867279053
Epoch 1300, val loss: 1.0670127868652344
Epoch 1310, training loss: 62.51524353027344 = 0.06741935014724731 + 10.0 * 6.244782447814941
Epoch 1310, val loss: 1.0721460580825806
Epoch 1320, training loss: 62.58427429199219 = 0.06546627730131149 + 10.0 * 6.251880645751953
Epoch 1320, val loss: 1.0769603252410889
Epoch 1330, training loss: 62.56371307373047 = 0.06356265395879745 + 10.0 * 6.250014781951904
Epoch 1330, val loss: 1.0828505754470825
Epoch 1340, training loss: 62.531982421875 = 0.06170842424035072 + 10.0 * 6.247027397155762
Epoch 1340, val loss: 1.0874754190444946
Epoch 1350, training loss: 62.501312255859375 = 0.059938229620456696 + 10.0 * 6.244137763977051
Epoch 1350, val loss: 1.0922857522964478
Epoch 1360, training loss: 62.48780822753906 = 0.058245234191417694 + 10.0 * 6.242956161499023
Epoch 1360, val loss: 1.0979058742523193
Epoch 1370, training loss: 62.48348617553711 = 0.056611038744449615 + 10.0 * 6.242687702178955
Epoch 1370, val loss: 1.1031874418258667
Epoch 1380, training loss: 62.49729919433594 = 0.05503538250923157 + 10.0 * 6.244226455688477
Epoch 1380, val loss: 1.1084885597229004
Epoch 1390, training loss: 62.49291229248047 = 0.05349866673350334 + 10.0 * 6.243941307067871
Epoch 1390, val loss: 1.113358736038208
Epoch 1400, training loss: 62.49472427368164 = 0.052014000713825226 + 10.0 * 6.244271278381348
Epoch 1400, val loss: 1.117936611175537
Epoch 1410, training loss: 62.485172271728516 = 0.05058745667338371 + 10.0 * 6.2434587478637695
Epoch 1410, val loss: 1.1236662864685059
Epoch 1420, training loss: 62.517086029052734 = 0.04920802637934685 + 10.0 * 6.2467875480651855
Epoch 1420, val loss: 1.128306269645691
Epoch 1430, training loss: 62.46834182739258 = 0.04785462096333504 + 10.0 * 6.242048740386963
Epoch 1430, val loss: 1.1331819295883179
Epoch 1440, training loss: 62.44464874267578 = 0.046576905995607376 + 10.0 * 6.23980712890625
Epoch 1440, val loss: 1.1384804248809814
Epoch 1450, training loss: 62.436641693115234 = 0.0453435443341732 + 10.0 * 6.239129543304443
Epoch 1450, val loss: 1.1434533596038818
Epoch 1460, training loss: 62.43375015258789 = 0.04415634274482727 + 10.0 * 6.238959312438965
Epoch 1460, val loss: 1.1485451459884644
Epoch 1470, training loss: 62.552799224853516 = 0.043018437922000885 + 10.0 * 6.250977993011475
Epoch 1470, val loss: 1.1539262533187866
Epoch 1480, training loss: 62.457000732421875 = 0.04186251014471054 + 10.0 * 6.241513729095459
Epoch 1480, val loss: 1.1579619646072388
Epoch 1490, training loss: 62.429927825927734 = 0.04078001156449318 + 10.0 * 6.238914966583252
Epoch 1490, val loss: 1.1630828380584717
Epoch 1500, training loss: 62.41486358642578 = 0.03974100947380066 + 10.0 * 6.237512111663818
Epoch 1500, val loss: 1.1679461002349854
Epoch 1510, training loss: 62.410301208496094 = 0.038740165531635284 + 10.0 * 6.237155914306641
Epoch 1510, val loss: 1.1728425025939941
Epoch 1520, training loss: 62.453189849853516 = 0.03777981549501419 + 10.0 * 6.241540908813477
Epoch 1520, val loss: 1.1777348518371582
Epoch 1530, training loss: 62.408302307128906 = 0.03682704269886017 + 10.0 * 6.237147331237793
Epoch 1530, val loss: 1.1822468042373657
Epoch 1540, training loss: 62.41480255126953 = 0.035915814340114594 + 10.0 * 6.237888813018799
Epoch 1540, val loss: 1.186787486076355
Epoch 1550, training loss: 62.410560607910156 = 0.0350324772298336 + 10.0 * 6.237552642822266
Epoch 1550, val loss: 1.1911718845367432
Epoch 1560, training loss: 62.393341064453125 = 0.03418176248669624 + 10.0 * 6.235915660858154
Epoch 1560, val loss: 1.1962571144104004
Epoch 1570, training loss: 62.38677978515625 = 0.033362895250320435 + 10.0 * 6.235341548919678
Epoch 1570, val loss: 1.2011524438858032
Epoch 1580, training loss: 62.417503356933594 = 0.032571934163570404 + 10.0 * 6.238492965698242
Epoch 1580, val loss: 1.2056708335876465
Epoch 1590, training loss: 62.42610168457031 = 0.03180360421538353 + 10.0 * 6.239429950714111
Epoch 1590, val loss: 1.2097673416137695
Epoch 1600, training loss: 62.376827239990234 = 0.03105299361050129 + 10.0 * 6.234577655792236
Epoch 1600, val loss: 1.215059518814087
Epoch 1610, training loss: 62.37187576293945 = 0.03033515252172947 + 10.0 * 6.234154224395752
Epoch 1610, val loss: 1.219415307044983
Epoch 1620, training loss: 62.36077117919922 = 0.029640674591064453 + 10.0 * 6.2331132888793945
Epoch 1620, val loss: 1.2239224910736084
Epoch 1630, training loss: 62.35782241821289 = 0.02897254377603531 + 10.0 * 6.232884883880615
Epoch 1630, val loss: 1.2284289598464966
Epoch 1640, training loss: 62.428245544433594 = 0.028325483202934265 + 10.0 * 6.239992141723633
Epoch 1640, val loss: 1.232509970664978
Epoch 1650, training loss: 62.43671417236328 = 0.02769462764263153 + 10.0 * 6.240901947021484
Epoch 1650, val loss: 1.237573266029358
Epoch 1660, training loss: 62.363922119140625 = 0.02706756256520748 + 10.0 * 6.233685493469238
Epoch 1660, val loss: 1.2418148517608643
Epoch 1670, training loss: 62.342567443847656 = 0.026477091014385223 + 10.0 * 6.231608867645264
Epoch 1670, val loss: 1.2461020946502686
Epoch 1680, training loss: 62.34034729003906 = 0.025908583775162697 + 10.0 * 6.231443881988525
Epoch 1680, val loss: 1.2504806518554688
Epoch 1690, training loss: 62.43208312988281 = 0.02535977028310299 + 10.0 * 6.2406721115112305
Epoch 1690, val loss: 1.2542401552200317
Epoch 1700, training loss: 62.38731002807617 = 0.02480994164943695 + 10.0 * 6.236249923706055
Epoch 1700, val loss: 1.2593692541122437
Epoch 1710, training loss: 62.34972381591797 = 0.02427988313138485 + 10.0 * 6.232544422149658
Epoch 1710, val loss: 1.2630703449249268
Epoch 1720, training loss: 62.326229095458984 = 0.023777130991220474 + 10.0 * 6.230245113372803
Epoch 1720, val loss: 1.2674469947814941
Epoch 1730, training loss: 62.32266616821289 = 0.023289969190955162 + 10.0 * 6.229937553405762
Epoch 1730, val loss: 1.271676778793335
Epoch 1740, training loss: 62.360233306884766 = 0.022822989150881767 + 10.0 * 6.23374080657959
Epoch 1740, val loss: 1.2758703231811523
Epoch 1750, training loss: 62.317562103271484 = 0.022354576736688614 + 10.0 * 6.229520797729492
Epoch 1750, val loss: 1.2802406549453735
Epoch 1760, training loss: 62.34084701538086 = 0.02190592885017395 + 10.0 * 6.231894016265869
Epoch 1760, val loss: 1.2844818830490112
Epoch 1770, training loss: 62.32453918457031 = 0.021470023319125175 + 10.0 * 6.230307102203369
Epoch 1770, val loss: 1.288330078125
Epoch 1780, training loss: 62.30894088745117 = 0.02104371041059494 + 10.0 * 6.228789806365967
Epoch 1780, val loss: 1.2920079231262207
Epoch 1790, training loss: 62.309268951416016 = 0.02063741162419319 + 10.0 * 6.22886323928833
Epoch 1790, val loss: 1.2958471775054932
Epoch 1800, training loss: 62.336509704589844 = 0.02024163119494915 + 10.0 * 6.231626987457275
Epoch 1800, val loss: 1.2994996309280396
Epoch 1810, training loss: 62.314292907714844 = 0.019849324598908424 + 10.0 * 6.22944450378418
Epoch 1810, val loss: 1.3039759397506714
Epoch 1820, training loss: 62.341796875 = 0.019469190388917923 + 10.0 * 6.232232570648193
Epoch 1820, val loss: 1.307453989982605
Epoch 1830, training loss: 62.2988166809082 = 0.019101334735751152 + 10.0 * 6.22797155380249
Epoch 1830, val loss: 1.3120430707931519
Epoch 1840, training loss: 62.29083251953125 = 0.01874445751309395 + 10.0 * 6.227208614349365
Epoch 1840, val loss: 1.3155438899993896
Epoch 1850, training loss: 62.30156707763672 = 0.018402285873889923 + 10.0 * 6.228316307067871
Epoch 1850, val loss: 1.3195321559906006
Epoch 1860, training loss: 62.319580078125 = 0.018067311495542526 + 10.0 * 6.230151176452637
Epoch 1860, val loss: 1.3233882188796997
Epoch 1870, training loss: 62.2908935546875 = 0.01773611083626747 + 10.0 * 6.227315902709961
Epoch 1870, val loss: 1.3268965482711792
Epoch 1880, training loss: 62.29838562011719 = 0.017418455332517624 + 10.0 * 6.228096961975098
Epoch 1880, val loss: 1.3302315473556519
Epoch 1890, training loss: 62.320858001708984 = 0.01710757240653038 + 10.0 * 6.230374813079834
Epoch 1890, val loss: 1.3343156576156616
Epoch 1900, training loss: 62.28310012817383 = 0.01680774800479412 + 10.0 * 6.226629257202148
Epoch 1900, val loss: 1.3382478952407837
Epoch 1910, training loss: 62.263790130615234 = 0.01651398465037346 + 10.0 * 6.224727630615234
Epoch 1910, val loss: 1.3415911197662354
Epoch 1920, training loss: 62.26675033569336 = 0.01623171754181385 + 10.0 * 6.2250518798828125
Epoch 1920, val loss: 1.3453559875488281
Epoch 1930, training loss: 62.3062858581543 = 0.01596115715801716 + 10.0 * 6.229032516479492
Epoch 1930, val loss: 1.349161982536316
Epoch 1940, training loss: 62.28868103027344 = 0.015686433762311935 + 10.0 * 6.227299690246582
Epoch 1940, val loss: 1.3522623777389526
Epoch 1950, training loss: 62.28782653808594 = 0.015417483635246754 + 10.0 * 6.227240562438965
Epoch 1950, val loss: 1.355384349822998
Epoch 1960, training loss: 62.277015686035156 = 0.015159770846366882 + 10.0 * 6.2261857986450195
Epoch 1960, val loss: 1.3589783906936646
Epoch 1970, training loss: 62.256141662597656 = 0.014910932630300522 + 10.0 * 6.224123001098633
Epoch 1970, val loss: 1.3630142211914062
Epoch 1980, training loss: 62.2513542175293 = 0.014668880961835384 + 10.0 * 6.223668575286865
Epoch 1980, val loss: 1.3663216829299927
Epoch 1990, training loss: 62.26163864135742 = 0.014434844255447388 + 10.0 * 6.224720478057861
Epoch 1990, val loss: 1.36996591091156
Epoch 2000, training loss: 62.286537170410156 = 0.01420509722083807 + 10.0 * 6.227232933044434
Epoch 2000, val loss: 1.3732622861862183
Epoch 2010, training loss: 62.267574310302734 = 0.0139743248000741 + 10.0 * 6.225359916687012
Epoch 2010, val loss: 1.3761489391326904
Epoch 2020, training loss: 62.24802780151367 = 0.013753422535955906 + 10.0 * 6.223427772521973
Epoch 2020, val loss: 1.379806399345398
Epoch 2030, training loss: 62.29231643676758 = 0.013545660302042961 + 10.0 * 6.227877140045166
Epoch 2030, val loss: 1.3833390474319458
Epoch 2040, training loss: 62.246849060058594 = 0.01332850195467472 + 10.0 * 6.223351955413818
Epoch 2040, val loss: 1.3866313695907593
Epoch 2050, training loss: 62.22985076904297 = 0.013121678493916988 + 10.0 * 6.221673011779785
Epoch 2050, val loss: 1.3892701864242554
Epoch 2060, training loss: 62.2265739440918 = 0.01292508002370596 + 10.0 * 6.221364974975586
Epoch 2060, val loss: 1.39262855052948
Epoch 2070, training loss: 62.223716735839844 = 0.012734404765069485 + 10.0 * 6.22109842300415
Epoch 2070, val loss: 1.395909309387207
Epoch 2080, training loss: 62.251834869384766 = 0.01254790835082531 + 10.0 * 6.223928928375244
Epoch 2080, val loss: 1.3986873626708984
Epoch 2090, training loss: 62.23701477050781 = 0.012361024506390095 + 10.0 * 6.222465515136719
Epoch 2090, val loss: 1.4021618366241455
Epoch 2100, training loss: 62.23630905151367 = 0.012181262485682964 + 10.0 * 6.222413063049316
Epoch 2100, val loss: 1.4053049087524414
Epoch 2110, training loss: 62.240169525146484 = 0.012002363801002502 + 10.0 * 6.2228169441223145
Epoch 2110, val loss: 1.408431053161621
Epoch 2120, training loss: 62.22453689575195 = 0.01183110848069191 + 10.0 * 6.221270561218262
Epoch 2120, val loss: 1.411834478378296
Epoch 2130, training loss: 62.28787612915039 = 0.011665365658700466 + 10.0 * 6.227621078491211
Epoch 2130, val loss: 1.4153105020523071
Epoch 2140, training loss: 62.23261642456055 = 0.011493056081235409 + 10.0 * 6.22211217880249
Epoch 2140, val loss: 1.417113184928894
Epoch 2150, training loss: 62.212284088134766 = 0.011333471164107323 + 10.0 * 6.220095157623291
Epoch 2150, val loss: 1.4208366870880127
Epoch 2160, training loss: 62.20374298095703 = 0.011178162880241871 + 10.0 * 6.219256401062012
Epoch 2160, val loss: 1.4235721826553345
Epoch 2170, training loss: 62.21818923950195 = 0.011028600856661797 + 10.0 * 6.2207159996032715
Epoch 2170, val loss: 1.4265174865722656
Epoch 2180, training loss: 62.242103576660156 = 0.010878761298954487 + 10.0 * 6.223122596740723
Epoch 2180, val loss: 1.4294427633285522
Epoch 2190, training loss: 62.21505355834961 = 0.010727660730481148 + 10.0 * 6.220432758331299
Epoch 2190, val loss: 1.4326348304748535
Epoch 2200, training loss: 62.20143508911133 = 0.010584807954728603 + 10.0 * 6.219084739685059
Epoch 2200, val loss: 1.4352056980133057
Epoch 2210, training loss: 62.31486892700195 = 0.010446500033140182 + 10.0 * 6.230442523956299
Epoch 2210, val loss: 1.4381346702575684
Epoch 2220, training loss: 62.22736740112305 = 0.01030400488525629 + 10.0 * 6.221706390380859
Epoch 2220, val loss: 1.4410921335220337
Epoch 2230, training loss: 62.20059585571289 = 0.010167363099753857 + 10.0 * 6.219042778015137
Epoch 2230, val loss: 1.4441365003585815
Epoch 2240, training loss: 62.21100997924805 = 0.010038905777037144 + 10.0 * 6.220097064971924
Epoch 2240, val loss: 1.4471821784973145
Epoch 2250, training loss: 62.19472122192383 = 0.00990945752710104 + 10.0 * 6.218481063842773
Epoch 2250, val loss: 1.449707269668579
Epoch 2260, training loss: 62.18252182006836 = 0.009781180880963802 + 10.0 * 6.217274188995361
Epoch 2260, val loss: 1.4520971775054932
Epoch 2270, training loss: 62.2255744934082 = 0.009661167860031128 + 10.0 * 6.221590995788574
Epoch 2270, val loss: 1.4546840190887451
Epoch 2280, training loss: 62.196659088134766 = 0.009539170190691948 + 10.0 * 6.218711853027344
Epoch 2280, val loss: 1.4579757452011108
Epoch 2290, training loss: 62.181678771972656 = 0.009417611174285412 + 10.0 * 6.217226028442383
Epoch 2290, val loss: 1.4603410959243774
Epoch 2300, training loss: 62.18141174316406 = 0.009302516467869282 + 10.0 * 6.21721076965332
Epoch 2300, val loss: 1.463303804397583
Epoch 2310, training loss: 62.19459533691406 = 0.00919202622026205 + 10.0 * 6.218540191650391
Epoch 2310, val loss: 1.4661179780960083
Epoch 2320, training loss: 62.194637298583984 = 0.009080007672309875 + 10.0 * 6.218555927276611
Epoch 2320, val loss: 1.4684879779815674
Epoch 2330, training loss: 62.21921157836914 = 0.008968621492385864 + 10.0 * 6.221024513244629
Epoch 2330, val loss: 1.470399022102356
Epoch 2340, training loss: 62.174468994140625 = 0.008859694004058838 + 10.0 * 6.2165608406066895
Epoch 2340, val loss: 1.4733941555023193
Epoch 2350, training loss: 62.16373825073242 = 0.008756092749536037 + 10.0 * 6.215498447418213
Epoch 2350, val loss: 1.475990891456604
Epoch 2360, training loss: 62.16801071166992 = 0.008655603043735027 + 10.0 * 6.215935707092285
Epoch 2360, val loss: 1.4785430431365967
Epoch 2370, training loss: 62.250858306884766 = 0.008558563888072968 + 10.0 * 6.22422981262207
Epoch 2370, val loss: 1.4808958768844604
Epoch 2380, training loss: 62.19501495361328 = 0.008454350754618645 + 10.0 * 6.218656063079834
Epoch 2380, val loss: 1.4832457304000854
Epoch 2390, training loss: 62.19329833984375 = 0.008356792852282524 + 10.0 * 6.218493938446045
Epoch 2390, val loss: 1.4855029582977295
Epoch 2400, training loss: 62.182273864746094 = 0.008261168375611305 + 10.0 * 6.217401027679443
Epoch 2400, val loss: 1.4880870580673218
Epoch 2410, training loss: 62.15659713745117 = 0.008169078268110752 + 10.0 * 6.214842796325684
Epoch 2410, val loss: 1.4908239841461182
Epoch 2420, training loss: 62.158851623535156 = 0.00808088667690754 + 10.0 * 6.2150774002075195
Epoch 2420, val loss: 1.4934782981872559
Epoch 2430, training loss: 62.23258972167969 = 0.00799625925719738 + 10.0 * 6.222459316253662
Epoch 2430, val loss: 1.4962598085403442
Epoch 2440, training loss: 62.18656539916992 = 0.007901693694293499 + 10.0 * 6.21786642074585
Epoch 2440, val loss: 1.4977232217788696
Epoch 2450, training loss: 62.1699333190918 = 0.007817238569259644 + 10.0 * 6.216211795806885
Epoch 2450, val loss: 1.5003846883773804
Epoch 2460, training loss: 62.15349578857422 = 0.007731852121651173 + 10.0 * 6.214576244354248
Epoch 2460, val loss: 1.502542495727539
Epoch 2470, training loss: 62.18080139160156 = 0.007650352548807859 + 10.0 * 6.217315196990967
Epoch 2470, val loss: 1.5044878721237183
Epoch 2480, training loss: 62.17918395996094 = 0.007567708846181631 + 10.0 * 6.217161655426025
Epoch 2480, val loss: 1.5066298246383667
Epoch 2490, training loss: 62.152198791503906 = 0.007485582958906889 + 10.0 * 6.214471340179443
Epoch 2490, val loss: 1.509360432624817
Epoch 2500, training loss: 62.14446258544922 = 0.007409988436847925 + 10.0 * 6.213705539703369
Epoch 2500, val loss: 1.5116771459579468
Epoch 2510, training loss: 62.1397590637207 = 0.007334548514336348 + 10.0 * 6.213242530822754
Epoch 2510, val loss: 1.5139108896255493
Epoch 2520, training loss: 62.171756744384766 = 0.007263594772666693 + 10.0 * 6.21644926071167
Epoch 2520, val loss: 1.5161844491958618
Epoch 2530, training loss: 62.1734733581543 = 0.007188207004219294 + 10.0 * 6.216628551483154
Epoch 2530, val loss: 1.5184571743011475
Epoch 2540, training loss: 62.172523498535156 = 0.007111189421266317 + 10.0 * 6.216541290283203
Epoch 2540, val loss: 1.5196555852890015
Epoch 2550, training loss: 62.13999938964844 = 0.007040149532258511 + 10.0 * 6.213295936584473
Epoch 2550, val loss: 1.5223116874694824
Epoch 2560, training loss: 62.13238525390625 = 0.00697131035849452 + 10.0 * 6.212541580200195
Epoch 2560, val loss: 1.5246881246566772
Epoch 2570, training loss: 62.13848876953125 = 0.006904960609972477 + 10.0 * 6.21315860748291
Epoch 2570, val loss: 1.5270545482635498
Epoch 2580, training loss: 62.16856002807617 = 0.006840065587311983 + 10.0 * 6.216172218322754
Epoch 2580, val loss: 1.5293235778808594
Epoch 2590, training loss: 62.12999725341797 = 0.006770963780581951 + 10.0 * 6.21232271194458
Epoch 2590, val loss: 1.5307273864746094
Epoch 2600, training loss: 62.151145935058594 = 0.006706085987389088 + 10.0 * 6.214444160461426
Epoch 2600, val loss: 1.5322452783584595
Epoch 2610, training loss: 62.16041564941406 = 0.0066420407965779305 + 10.0 * 6.215377330780029
Epoch 2610, val loss: 1.535048007965088
Epoch 2620, training loss: 62.13447952270508 = 0.006579143460839987 + 10.0 * 6.212790012359619
Epoch 2620, val loss: 1.5371564626693726
Epoch 2630, training loss: 62.156166076660156 = 0.006516382098197937 + 10.0 * 6.214964866638184
Epoch 2630, val loss: 1.5387892723083496
Epoch 2640, training loss: 62.13642883300781 = 0.006454771850258112 + 10.0 * 6.2129974365234375
Epoch 2640, val loss: 1.5407254695892334
Epoch 2650, training loss: 62.123329162597656 = 0.00639625359326601 + 10.0 * 6.211693286895752
Epoch 2650, val loss: 1.5428576469421387
Epoch 2660, training loss: 62.11688232421875 = 0.006338897626847029 + 10.0 * 6.21105432510376
Epoch 2660, val loss: 1.5451278686523438
Epoch 2670, training loss: 62.12369155883789 = 0.0062835621647536755 + 10.0 * 6.211740970611572
Epoch 2670, val loss: 1.5470938682556152
Epoch 2680, training loss: 62.17000961303711 = 0.006228821352124214 + 10.0 * 6.216378211975098
Epoch 2680, val loss: 1.5490537881851196
Epoch 2690, training loss: 62.16559982299805 = 0.006168345920741558 + 10.0 * 6.215943336486816
Epoch 2690, val loss: 1.550381064414978
Epoch 2700, training loss: 62.131473541259766 = 0.006113623268902302 + 10.0 * 6.212535858154297
Epoch 2700, val loss: 1.5526005029678345
Epoch 2710, training loss: 62.11655807495117 = 0.006059004459530115 + 10.0 * 6.211050033569336
Epoch 2710, val loss: 1.5543212890625
Epoch 2720, training loss: 62.15492248535156 = 0.006008070893585682 + 10.0 * 6.21489143371582
Epoch 2720, val loss: 1.555718183517456
Epoch 2730, training loss: 62.12322998046875 = 0.005953686777502298 + 10.0 * 6.211727619171143
Epoch 2730, val loss: 1.5581284761428833
Epoch 2740, training loss: 62.11467361450195 = 0.00590352201834321 + 10.0 * 6.210876941680908
Epoch 2740, val loss: 1.5600862503051758
Epoch 2750, training loss: 62.10908889770508 = 0.005852903705090284 + 10.0 * 6.210323810577393
Epoch 2750, val loss: 1.561600923538208
Epoch 2760, training loss: 62.15420150756836 = 0.005805101711302996 + 10.0 * 6.214839458465576
Epoch 2760, val loss: 1.5632222890853882
Epoch 2770, training loss: 62.109745025634766 = 0.005755054298788309 + 10.0 * 6.2103986740112305
Epoch 2770, val loss: 1.5656012296676636
Epoch 2780, training loss: 62.10824966430664 = 0.005707310978323221 + 10.0 * 6.210254192352295
Epoch 2780, val loss: 1.5673562288284302
Epoch 2790, training loss: 62.11944580078125 = 0.005660306196659803 + 10.0 * 6.211378574371338
Epoch 2790, val loss: 1.5689276456832886
Epoch 2800, training loss: 62.111656188964844 = 0.005613773595541716 + 10.0 * 6.210604190826416
Epoch 2800, val loss: 1.5707241296768188
Epoch 2810, training loss: 62.09811782836914 = 0.0055689457803964615 + 10.0 * 6.209254741668701
Epoch 2810, val loss: 1.572606086730957
Epoch 2820, training loss: 62.108642578125 = 0.005525743123143911 + 10.0 * 6.210311412811279
Epoch 2820, val loss: 1.5745325088500977
Epoch 2830, training loss: 62.18345642089844 = 0.005483853630721569 + 10.0 * 6.21779727935791
Epoch 2830, val loss: 1.5768194198608398
Epoch 2840, training loss: 62.10932540893555 = 0.005434750579297543 + 10.0 * 6.210389137268066
Epoch 2840, val loss: 1.5772532224655151
Epoch 2850, training loss: 62.095191955566406 = 0.005391241051256657 + 10.0 * 6.208980083465576
Epoch 2850, val loss: 1.5793951749801636
Epoch 2860, training loss: 62.090179443359375 = 0.0053496588952839375 + 10.0 * 6.2084832191467285
Epoch 2860, val loss: 1.5810914039611816
Epoch 2870, training loss: 62.11383056640625 = 0.005311275366693735 + 10.0 * 6.210852146148682
Epoch 2870, val loss: 1.5830144882202148
Epoch 2880, training loss: 62.1069221496582 = 0.005269655957818031 + 10.0 * 6.210165500640869
Epoch 2880, val loss: 1.584580898284912
Epoch 2890, training loss: 62.0930061340332 = 0.005227559246122837 + 10.0 * 6.208777904510498
Epoch 2890, val loss: 1.5859407186508179
Epoch 2900, training loss: 62.08478927612305 = 0.005188660696148872 + 10.0 * 6.20796012878418
Epoch 2900, val loss: 1.5875827074050903
Epoch 2910, training loss: 62.12560272216797 = 0.005150999873876572 + 10.0 * 6.212045192718506
Epoch 2910, val loss: 1.588780164718628
Epoch 2920, training loss: 62.10628890991211 = 0.005111550446599722 + 10.0 * 6.210117816925049
Epoch 2920, val loss: 1.590887188911438
Epoch 2930, training loss: 62.088714599609375 = 0.005071757361292839 + 10.0 * 6.208364009857178
Epoch 2930, val loss: 1.5919091701507568
Epoch 2940, training loss: 62.084259033203125 = 0.005033555440604687 + 10.0 * 6.207922458648682
Epoch 2940, val loss: 1.593898057937622
Epoch 2950, training loss: 62.10395812988281 = 0.004997561685740948 + 10.0 * 6.209896087646484
Epoch 2950, val loss: 1.5947808027267456
Epoch 2960, training loss: 62.093910217285156 = 0.004961579106748104 + 10.0 * 6.208894729614258
Epoch 2960, val loss: 1.5967636108398438
Epoch 2970, training loss: 62.080474853515625 = 0.0049271839670836926 + 10.0 * 6.207554817199707
Epoch 2970, val loss: 1.5988305807113647
Epoch 2980, training loss: 62.08193588256836 = 0.00489279069006443 + 10.0 * 6.207704067230225
Epoch 2980, val loss: 1.6004027128219604
Epoch 2990, training loss: 62.08744812011719 = 0.00485864607617259 + 10.0 * 6.208258628845215
Epoch 2990, val loss: 1.60158371925354
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8002108592514497
=== training gcn model ===
Epoch 0, training loss: 87.91946411132812 = 1.9509098529815674 + 10.0 * 8.596855163574219
Epoch 0, val loss: 1.9556920528411865
Epoch 10, training loss: 87.904052734375 = 1.9409905672073364 + 10.0 * 8.596305847167969
Epoch 10, val loss: 1.9449132680892944
Epoch 20, training loss: 87.8456802368164 = 1.9293243885040283 + 10.0 * 8.591635704040527
Epoch 20, val loss: 1.9318630695343018
Epoch 30, training loss: 87.4997329711914 = 1.9145563840866089 + 10.0 * 8.558517456054688
Epoch 30, val loss: 1.9151495695114136
Epoch 40, training loss: 85.67300415039062 = 1.8974164724349976 + 10.0 * 8.377558708190918
Epoch 40, val loss: 1.8967769145965576
Epoch 50, training loss: 80.32772064208984 = 1.8792744874954224 + 10.0 * 7.844844818115234
Epoch 50, val loss: 1.877027988433838
Epoch 60, training loss: 76.89368438720703 = 1.8641718626022339 + 10.0 * 7.502951145172119
Epoch 60, val loss: 1.8630250692367554
Epoch 70, training loss: 73.89883422851562 = 1.8510898351669312 + 10.0 * 7.204774379730225
Epoch 70, val loss: 1.8512113094329834
Epoch 80, training loss: 71.22284698486328 = 1.840000867843628 + 10.0 * 6.938284397125244
Epoch 80, val loss: 1.8413043022155762
Epoch 90, training loss: 69.67329406738281 = 1.830622911453247 + 10.0 * 6.784267425537109
Epoch 90, val loss: 1.832176923751831
Epoch 100, training loss: 68.95765686035156 = 1.8210676908493042 + 10.0 * 6.713658809661865
Epoch 100, val loss: 1.8225642442703247
Epoch 110, training loss: 68.44602966308594 = 1.8112492561340332 + 10.0 * 6.663478374481201
Epoch 110, val loss: 1.8131296634674072
Epoch 120, training loss: 68.08090209960938 = 1.802080750465393 + 10.0 * 6.627882480621338
Epoch 120, val loss: 1.80442476272583
Epoch 130, training loss: 67.66478729248047 = 1.7932504415512085 + 10.0 * 6.587153434753418
Epoch 130, val loss: 1.796052098274231
Epoch 140, training loss: 67.3858413696289 = 1.784727931022644 + 10.0 * 6.560111045837402
Epoch 140, val loss: 1.7879548072814941
Epoch 150, training loss: 67.12581634521484 = 1.7761117219924927 + 10.0 * 6.534969806671143
Epoch 150, val loss: 1.7799031734466553
Epoch 160, training loss: 66.98310852050781 = 1.7670072317123413 + 10.0 * 6.521610260009766
Epoch 160, val loss: 1.771738052368164
Epoch 170, training loss: 66.75379180908203 = 1.7570147514343262 + 10.0 * 6.499677658081055
Epoch 170, val loss: 1.7630736827850342
Epoch 180, training loss: 66.59038543701172 = 1.74617338180542 + 10.0 * 6.484421253204346
Epoch 180, val loss: 1.7539818286895752
Epoch 190, training loss: 66.4495849609375 = 1.7344324588775635 + 10.0 * 6.47151517868042
Epoch 190, val loss: 1.7441644668579102
Epoch 200, training loss: 66.35421752929688 = 1.7216084003448486 + 10.0 * 6.463260650634766
Epoch 200, val loss: 1.733610987663269
Epoch 210, training loss: 66.20319366455078 = 1.7075304985046387 + 10.0 * 6.44956636428833
Epoch 210, val loss: 1.7221851348876953
Epoch 220, training loss: 66.0908203125 = 1.6922255754470825 + 10.0 * 6.439859867095947
Epoch 220, val loss: 1.7098978757858276
Epoch 230, training loss: 66.03338623046875 = 1.6755229234695435 + 10.0 * 6.435786247253418
Epoch 230, val loss: 1.6962722539901733
Epoch 240, training loss: 65.89141082763672 = 1.6572355031967163 + 10.0 * 6.423417568206787
Epoch 240, val loss: 1.6819944381713867
Epoch 250, training loss: 65.79400634765625 = 1.637538194656372 + 10.0 * 6.415647029876709
Epoch 250, val loss: 1.6663916110992432
Epoch 260, training loss: 65.70233154296875 = 1.6163175106048584 + 10.0 * 6.408601760864258
Epoch 260, val loss: 1.6497865915298462
Epoch 270, training loss: 65.61022186279297 = 1.5935779809951782 + 10.0 * 6.401664733886719
Epoch 270, val loss: 1.632110595703125
Epoch 280, training loss: 65.52235412597656 = 1.5693180561065674 + 10.0 * 6.395303726196289
Epoch 280, val loss: 1.6133531332015991
Epoch 290, training loss: 65.44817352294922 = 1.543623924255371 + 10.0 * 6.3904547691345215
Epoch 290, val loss: 1.5935585498809814
Epoch 300, training loss: 65.36491394042969 = 1.5166661739349365 + 10.0 * 6.384825229644775
Epoch 300, val loss: 1.5731521844863892
Epoch 310, training loss: 65.28720092773438 = 1.4886770248413086 + 10.0 * 6.379852294921875
Epoch 310, val loss: 1.5521941184997559
Epoch 320, training loss: 65.22649383544922 = 1.459767460823059 + 10.0 * 6.376672267913818
Epoch 320, val loss: 1.5308750867843628
Epoch 330, training loss: 65.14666748046875 = 1.4302200078964233 + 10.0 * 6.371644496917725
Epoch 330, val loss: 1.5092203617095947
Epoch 340, training loss: 65.0660400390625 = 1.4002737998962402 + 10.0 * 6.366576671600342
Epoch 340, val loss: 1.487697720527649
Epoch 350, training loss: 65.01136016845703 = 1.370115876197815 + 10.0 * 6.364124774932861
Epoch 350, val loss: 1.4664669036865234
Epoch 360, training loss: 64.94818115234375 = 1.3399497270584106 + 10.0 * 6.360823154449463
Epoch 360, val loss: 1.4451854228973389
Epoch 370, training loss: 64.87950897216797 = 1.3099329471588135 + 10.0 * 6.35695743560791
Epoch 370, val loss: 1.4247654676437378
Epoch 380, training loss: 64.80493927001953 = 1.2803528308868408 + 10.0 * 6.352458953857422
Epoch 380, val loss: 1.404785394668579
Epoch 390, training loss: 64.77971649169922 = 1.2512521743774414 + 10.0 * 6.352846145629883
Epoch 390, val loss: 1.385424256324768
Epoch 400, training loss: 64.69734954833984 = 1.222584843635559 + 10.0 * 6.347476482391357
Epoch 400, val loss: 1.366982340812683
Epoch 410, training loss: 64.6640396118164 = 1.1946465969085693 + 10.0 * 6.3469390869140625
Epoch 410, val loss: 1.3493003845214844
Epoch 420, training loss: 64.59209442138672 = 1.1673341989517212 + 10.0 * 6.3424763679504395
Epoch 420, val loss: 1.332305669784546
Epoch 430, training loss: 64.5194320678711 = 1.1407995223999023 + 10.0 * 6.337862968444824
Epoch 430, val loss: 1.3161654472351074
Epoch 440, training loss: 64.47592163085938 = 1.114959955215454 + 10.0 * 6.336095809936523
Epoch 440, val loss: 1.3010308742523193
Epoch 450, training loss: 64.50733184814453 = 1.0897490978240967 + 10.0 * 6.341757774353027
Epoch 450, val loss: 1.2867194414138794
Epoch 460, training loss: 64.36984252929688 = 1.0652501583099365 + 10.0 * 6.3304595947265625
Epoch 460, val loss: 1.2726837396621704
Epoch 470, training loss: 64.31291961669922 = 1.0414336919784546 + 10.0 * 6.3271484375
Epoch 470, val loss: 1.2596603631973267
Epoch 480, training loss: 64.26644897460938 = 1.0182676315307617 + 10.0 * 6.3248186111450195
Epoch 480, val loss: 1.2474135160446167
Epoch 490, training loss: 64.26055145263672 = 0.9956032037734985 + 10.0 * 6.3264946937561035
Epoch 490, val loss: 1.2357542514801025
Epoch 500, training loss: 64.17345428466797 = 0.9734997153282166 + 10.0 * 6.319994926452637
Epoch 500, val loss: 1.2247178554534912
Epoch 510, training loss: 64.13141632080078 = 0.9519026279449463 + 10.0 * 6.317951202392578
Epoch 510, val loss: 1.2141393423080444
Epoch 520, training loss: 64.13372802734375 = 0.9307644367218018 + 10.0 * 6.320296287536621
Epoch 520, val loss: 1.2047317028045654
Epoch 530, training loss: 64.09259033203125 = 0.9099480509757996 + 10.0 * 6.318264007568359
Epoch 530, val loss: 1.1948330402374268
Epoch 540, training loss: 64.01757049560547 = 0.8896275162696838 + 10.0 * 6.312794208526611
Epoch 540, val loss: 1.1861108541488647
Epoch 550, training loss: 63.963287353515625 = 0.8698019981384277 + 10.0 * 6.3093485832214355
Epoch 550, val loss: 1.1777946949005127
Epoch 560, training loss: 63.919822692871094 = 0.8503732681274414 + 10.0 * 6.306944847106934
Epoch 560, val loss: 1.1701409816741943
Epoch 570, training loss: 63.97376251220703 = 0.8312675952911377 + 10.0 * 6.314249515533447
Epoch 570, val loss: 1.1626583337783813
Epoch 580, training loss: 63.85480499267578 = 0.8123575448989868 + 10.0 * 6.304244518280029
Epoch 580, val loss: 1.1557694673538208
Epoch 590, training loss: 63.81159591674805 = 0.7938327193260193 + 10.0 * 6.30177640914917
Epoch 590, val loss: 1.1494029760360718
Epoch 600, training loss: 63.78011703491211 = 0.7757208347320557 + 10.0 * 6.300439357757568
Epoch 600, val loss: 1.1435868740081787
Epoch 610, training loss: 63.82723617553711 = 0.7578575611114502 + 10.0 * 6.3069376945495605
Epoch 610, val loss: 1.1381609439849854
Epoch 620, training loss: 63.72685241699219 = 0.7402026057243347 + 10.0 * 6.2986650466918945
Epoch 620, val loss: 1.1324881315231323
Epoch 630, training loss: 63.67209243774414 = 0.722831130027771 + 10.0 * 6.294926166534424
Epoch 630, val loss: 1.1276205778121948
Epoch 640, training loss: 63.63896942138672 = 0.7057895660400391 + 10.0 * 6.293317794799805
Epoch 640, val loss: 1.1235172748565674
Epoch 650, training loss: 63.6153564453125 = 0.6890254616737366 + 10.0 * 6.292633056640625
Epoch 650, val loss: 1.119385838508606
Epoch 660, training loss: 63.57447052001953 = 0.6723200678825378 + 10.0 * 6.290215015411377
Epoch 660, val loss: 1.1153513193130493
Epoch 670, training loss: 63.56414794921875 = 0.655975341796875 + 10.0 * 6.2908172607421875
Epoch 670, val loss: 1.112105131149292
Epoch 680, training loss: 63.519290924072266 = 0.6399361491203308 + 10.0 * 6.287935733795166
Epoch 680, val loss: 1.109129548072815
Epoch 690, training loss: 63.48562240600586 = 0.6241767406463623 + 10.0 * 6.286144733428955
Epoch 690, val loss: 1.106539249420166
Epoch 700, training loss: 63.50508117675781 = 0.6085984706878662 + 10.0 * 6.289648532867432
Epoch 700, val loss: 1.1039553880691528
Epoch 710, training loss: 63.442195892333984 = 0.593247652053833 + 10.0 * 6.284894943237305
Epoch 710, val loss: 1.1021250486373901
Epoch 720, training loss: 63.41682434082031 = 0.5781644582748413 + 10.0 * 6.283865928649902
Epoch 720, val loss: 1.1003162860870361
Epoch 730, training loss: 63.3737678527832 = 0.5633650422096252 + 10.0 * 6.281040191650391
Epoch 730, val loss: 1.0991592407226562
Epoch 740, training loss: 63.34539031982422 = 0.548869788646698 + 10.0 * 6.279652118682861
Epoch 740, val loss: 1.0983353853225708
Epoch 750, training loss: 63.42264938354492 = 0.5346652269363403 + 10.0 * 6.2887983322143555
Epoch 750, val loss: 1.097440481185913
Epoch 760, training loss: 63.307891845703125 = 0.5204362273216248 + 10.0 * 6.278745651245117
Epoch 760, val loss: 1.0972257852554321
Epoch 770, training loss: 63.2811164855957 = 0.5066726803779602 + 10.0 * 6.277444362640381
Epoch 770, val loss: 1.097127079963684
Epoch 780, training loss: 63.249961853027344 = 0.49319642782211304 + 10.0 * 6.275676727294922
Epoch 780, val loss: 1.097366213798523
Epoch 790, training loss: 63.22121810913086 = 0.47998419404029846 + 10.0 * 6.274123191833496
Epoch 790, val loss: 1.0979310274124146
Epoch 800, training loss: 63.298004150390625 = 0.46696820855140686 + 10.0 * 6.2831034660339355
Epoch 800, val loss: 1.0990186929702759
Epoch 810, training loss: 63.191680908203125 = 0.45413777232170105 + 10.0 * 6.273754596710205
Epoch 810, val loss: 1.0994360446929932
Epoch 820, training loss: 63.15696334838867 = 0.4415714144706726 + 10.0 * 6.271539211273193
Epoch 820, val loss: 1.1005852222442627
Epoch 830, training loss: 63.1714973449707 = 0.42925724387168884 + 10.0 * 6.274224281311035
Epoch 830, val loss: 1.1022926568984985
Epoch 840, training loss: 63.12039566040039 = 0.4171322286128998 + 10.0 * 6.270326137542725
Epoch 840, val loss: 1.1037684679031372
Epoch 850, training loss: 63.11166763305664 = 0.4052213728427887 + 10.0 * 6.270644664764404
Epoch 850, val loss: 1.1053861379623413
Epoch 860, training loss: 63.091102600097656 = 0.39359456300735474 + 10.0 * 6.269751071929932
Epoch 860, val loss: 1.1074475049972534
Epoch 870, training loss: 63.055850982666016 = 0.3822088837623596 + 10.0 * 6.267364025115967
Epoch 870, val loss: 1.1099306344985962
Epoch 880, training loss: 63.05121612548828 = 0.37105128169059753 + 10.0 * 6.268016338348389
Epoch 880, val loss: 1.1123170852661133
Epoch 890, training loss: 63.04304885864258 = 0.3600776493549347 + 10.0 * 6.26829719543457
Epoch 890, val loss: 1.1145133972167969
Epoch 900, training loss: 63.0032844543457 = 0.34936413168907166 + 10.0 * 6.265391826629639
Epoch 900, val loss: 1.1170915365219116
Epoch 910, training loss: 62.981536865234375 = 0.33886465430259705 + 10.0 * 6.264267444610596
Epoch 910, val loss: 1.1203129291534424
Epoch 920, training loss: 62.96695327758789 = 0.3286301791667938 + 10.0 * 6.2638325691223145
Epoch 920, val loss: 1.1233394145965576
Epoch 930, training loss: 62.962398529052734 = 0.3186189532279968 + 10.0 * 6.264378070831299
Epoch 930, val loss: 1.12652587890625
Epoch 940, training loss: 62.95453643798828 = 0.3088169991970062 + 10.0 * 6.264571666717529
Epoch 940, val loss: 1.1294560432434082
Epoch 950, training loss: 62.947425842285156 = 0.2991677224636078 + 10.0 * 6.264825820922852
Epoch 950, val loss: 1.1331303119659424
Epoch 960, training loss: 62.90935134887695 = 0.28985780477523804 + 10.0 * 6.26194953918457
Epoch 960, val loss: 1.136887788772583
Epoch 970, training loss: 62.8714714050293 = 0.2808283269405365 + 10.0 * 6.259064674377441
Epoch 970, val loss: 1.1405431032180786
Epoch 980, training loss: 62.863258361816406 = 0.27207353711128235 + 10.0 * 6.259118556976318
Epoch 980, val loss: 1.144718885421753
Epoch 990, training loss: 62.88105392456055 = 0.26355239748954773 + 10.0 * 6.261750221252441
Epoch 990, val loss: 1.148863434791565
Epoch 1000, training loss: 62.8311882019043 = 0.2552136480808258 + 10.0 * 6.25759744644165
Epoch 1000, val loss: 1.1535578966140747
Epoch 1010, training loss: 62.82426834106445 = 0.24719175696372986 + 10.0 * 6.257707595825195
Epoch 1010, val loss: 1.1581274271011353
Epoch 1020, training loss: 62.817501068115234 = 0.23935779929161072 + 10.0 * 6.257814407348633
Epoch 1020, val loss: 1.1624910831451416
Epoch 1030, training loss: 62.798702239990234 = 0.23177194595336914 + 10.0 * 6.256692886352539
Epoch 1030, val loss: 1.1673896312713623
Epoch 1040, training loss: 62.781455993652344 = 0.224434033036232 + 10.0 * 6.255702018737793
Epoch 1040, val loss: 1.1722568273544312
Epoch 1050, training loss: 62.76344680786133 = 0.21736744046211243 + 10.0 * 6.254607677459717
Epoch 1050, val loss: 1.1775846481323242
Epoch 1060, training loss: 62.75782775878906 = 0.21053099632263184 + 10.0 * 6.254729747772217
Epoch 1060, val loss: 1.182629108428955
Epoch 1070, training loss: 62.76168441772461 = 0.20391596853733063 + 10.0 * 6.255776882171631
Epoch 1070, val loss: 1.1879478693008423
Epoch 1080, training loss: 62.76783752441406 = 0.1974891871213913 + 10.0 * 6.257034778594971
Epoch 1080, val loss: 1.1937189102172852
Epoch 1090, training loss: 62.710838317871094 = 0.1912490725517273 + 10.0 * 6.251958847045898
Epoch 1090, val loss: 1.1991864442825317
Epoch 1100, training loss: 62.69793701171875 = 0.1852707862854004 + 10.0 * 6.2512664794921875
Epoch 1100, val loss: 1.2050516605377197
Epoch 1110, training loss: 62.67707061767578 = 0.17952798306941986 + 10.0 * 6.249754428863525
Epoch 1110, val loss: 1.2109687328338623
Epoch 1120, training loss: 62.67426300048828 = 0.1739911288022995 + 10.0 * 6.250027179718018
Epoch 1120, val loss: 1.2168242931365967
Epoch 1130, training loss: 62.73518753051758 = 0.168614000082016 + 10.0 * 6.256657600402832
Epoch 1130, val loss: 1.2230277061462402
Epoch 1140, training loss: 62.6514778137207 = 0.16338475048542023 + 10.0 * 6.248809337615967
Epoch 1140, val loss: 1.228735327720642
Epoch 1150, training loss: 62.63724899291992 = 0.15836797654628754 + 10.0 * 6.247888088226318
Epoch 1150, val loss: 1.2350414991378784
Epoch 1160, training loss: 62.62567138671875 = 0.15355780720710754 + 10.0 * 6.247211456298828
Epoch 1160, val loss: 1.2413537502288818
Epoch 1170, training loss: 62.65850830078125 = 0.14892615377902985 + 10.0 * 6.250958442687988
Epoch 1170, val loss: 1.2476375102996826
Epoch 1180, training loss: 62.65040969848633 = 0.14438524842262268 + 10.0 * 6.2506022453308105
Epoch 1180, val loss: 1.2542104721069336
Epoch 1190, training loss: 62.61551284790039 = 0.1400240659713745 + 10.0 * 6.247549057006836
Epoch 1190, val loss: 1.2605558633804321
Epoch 1200, training loss: 62.58771896362305 = 0.13584302365779877 + 10.0 * 6.245187759399414
Epoch 1200, val loss: 1.267429232597351
Epoch 1210, training loss: 62.575950622558594 = 0.13184045255184174 + 10.0 * 6.244410991668701
Epoch 1210, val loss: 1.2741655111312866
Epoch 1220, training loss: 62.582183837890625 = 0.12797647714614868 + 10.0 * 6.245420932769775
Epoch 1220, val loss: 1.2811088562011719
Epoch 1230, training loss: 62.59138870239258 = 0.12419114261865616 + 10.0 * 6.246719837188721
Epoch 1230, val loss: 1.2877906560897827
Epoch 1240, training loss: 62.56727600097656 = 0.12053719907999039 + 10.0 * 6.244673728942871
Epoch 1240, val loss: 1.294785737991333
Epoch 1250, training loss: 62.552154541015625 = 0.11702059954404831 + 10.0 * 6.243513584136963
Epoch 1250, val loss: 1.3017785549163818
Epoch 1260, training loss: 62.535919189453125 = 0.11365728080272675 + 10.0 * 6.2422261238098145
Epoch 1260, val loss: 1.3088971376419067
Epoch 1270, training loss: 62.60598373413086 = 0.11042662709951401 + 10.0 * 6.249555587768555
Epoch 1270, val loss: 1.315817952156067
Epoch 1280, training loss: 62.537071228027344 = 0.10721929371356964 + 10.0 * 6.242985248565674
Epoch 1280, val loss: 1.3234119415283203
Epoch 1290, training loss: 62.51344680786133 = 0.10418042540550232 + 10.0 * 6.240926742553711
Epoch 1290, val loss: 1.3303271532058716
Epoch 1300, training loss: 62.53664779663086 = 0.10125097632408142 + 10.0 * 6.243539810180664
Epoch 1300, val loss: 1.3375383615493774
Epoch 1310, training loss: 62.49964141845703 = 0.09838244318962097 + 10.0 * 6.240126132965088
Epoch 1310, val loss: 1.3448185920715332
Epoch 1320, training loss: 62.49335479736328 = 0.09562867879867554 + 10.0 * 6.239772796630859
Epoch 1320, val loss: 1.3519515991210938
Epoch 1330, training loss: 62.483924865722656 = 0.09298668801784515 + 10.0 * 6.239093780517578
Epoch 1330, val loss: 1.3593053817749023
Epoch 1340, training loss: 62.478538513183594 = 0.09044329822063446 + 10.0 * 6.238809585571289
Epoch 1340, val loss: 1.3666026592254639
Epoch 1350, training loss: 62.55119323730469 = 0.08797046542167664 + 10.0 * 6.246322154998779
Epoch 1350, val loss: 1.3738293647766113
Epoch 1360, training loss: 62.4857063293457 = 0.08556945621967316 + 10.0 * 6.240013599395752
Epoch 1360, val loss: 1.3814016580581665
Epoch 1370, training loss: 62.4615592956543 = 0.08324789255857468 + 10.0 * 6.237831115722656
Epoch 1370, val loss: 1.388548493385315
Epoch 1380, training loss: 62.48356628417969 = 0.08102448284626007 + 10.0 * 6.2402544021606445
Epoch 1380, val loss: 1.3961540460586548
Epoch 1390, training loss: 62.45382308959961 = 0.07886168360710144 + 10.0 * 6.237496376037598
Epoch 1390, val loss: 1.403014063835144
Epoch 1400, training loss: 62.45560073852539 = 0.07677137851715088 + 10.0 * 6.2378830909729
Epoch 1400, val loss: 1.4103879928588867
Epoch 1410, training loss: 62.43434143066406 = 0.07476062327623367 + 10.0 * 6.235958099365234
Epoch 1410, val loss: 1.417907953262329
Epoch 1420, training loss: 62.425052642822266 = 0.07282958179712296 + 10.0 * 6.235222339630127
Epoch 1420, val loss: 1.4251892566680908
Epoch 1430, training loss: 62.46131896972656 = 0.07097484916448593 + 10.0 * 6.239034175872803
Epoch 1430, val loss: 1.4324907064437866
Epoch 1440, training loss: 62.42325973510742 = 0.06912777572870255 + 10.0 * 6.235413551330566
Epoch 1440, val loss: 1.4401392936706543
Epoch 1450, training loss: 62.428897857666016 = 0.06736409664154053 + 10.0 * 6.236153602600098
Epoch 1450, val loss: 1.4470361471176147
Epoch 1460, training loss: 62.42292022705078 = 0.06566085666418076 + 10.0 * 6.2357258796691895
Epoch 1460, val loss: 1.4545420408248901
Epoch 1470, training loss: 62.402122497558594 = 0.06402505934238434 + 10.0 * 6.233809471130371
Epoch 1470, val loss: 1.4616140127182007
Epoch 1480, training loss: 62.4024772644043 = 0.06244209408760071 + 10.0 * 6.23400354385376
Epoch 1480, val loss: 1.468802809715271
Epoch 1490, training loss: 62.43174743652344 = 0.06090784817934036 + 10.0 * 6.237083911895752
Epoch 1490, val loss: 1.4757649898529053
Epoch 1500, training loss: 62.435646057128906 = 0.05941260606050491 + 10.0 * 6.23762321472168
Epoch 1500, val loss: 1.4829038381576538
Epoch 1510, training loss: 62.398887634277344 = 0.05796588584780693 + 10.0 * 6.2340922355651855
Epoch 1510, val loss: 1.4901150465011597
Epoch 1520, training loss: 62.411048889160156 = 0.05657335743308067 + 10.0 * 6.235447883605957
Epoch 1520, val loss: 1.4973281621932983
Epoch 1530, training loss: 62.37344741821289 = 0.0552162267267704 + 10.0 * 6.231822967529297
Epoch 1530, val loss: 1.5043362379074097
Epoch 1540, training loss: 62.376243591308594 = 0.05391797423362732 + 10.0 * 6.232232570648193
Epoch 1540, val loss: 1.5114165544509888
Epoch 1550, training loss: 62.38728332519531 = 0.05266653373837471 + 10.0 * 6.233461856842041
Epoch 1550, val loss: 1.5183708667755127
Epoch 1560, training loss: 62.35675048828125 = 0.0514310784637928 + 10.0 * 6.230532169342041
Epoch 1560, val loss: 1.5254909992218018
Epoch 1570, training loss: 62.380035400390625 = 0.050251323729753494 + 10.0 * 6.232978343963623
Epoch 1570, val loss: 1.532446026802063
Epoch 1580, training loss: 62.3946647644043 = 0.04909900203347206 + 10.0 * 6.234556674957275
Epoch 1580, val loss: 1.5394203662872314
Epoch 1590, training loss: 62.36566925048828 = 0.04797497019171715 + 10.0 * 6.231769561767578
Epoch 1590, val loss: 1.5459874868392944
Epoch 1600, training loss: 62.341758728027344 = 0.04689440876245499 + 10.0 * 6.229486465454102
Epoch 1600, val loss: 1.552800178527832
Epoch 1610, training loss: 62.35248947143555 = 0.04585804045200348 + 10.0 * 6.230663299560547
Epoch 1610, val loss: 1.5595110654830933
Epoch 1620, training loss: 62.36980438232422 = 0.04484149441123009 + 10.0 * 6.23249626159668
Epoch 1620, val loss: 1.5661342144012451
Epoch 1630, training loss: 62.352474212646484 = 0.04385027661919594 + 10.0 * 6.230862617492676
Epoch 1630, val loss: 1.5730116367340088
Epoch 1640, training loss: 62.337459564208984 = 0.04289880022406578 + 10.0 * 6.229455947875977
Epoch 1640, val loss: 1.5793545246124268
Epoch 1650, training loss: 62.354393005371094 = 0.04197840765118599 + 10.0 * 6.231241703033447
Epoch 1650, val loss: 1.585983157157898
Epoch 1660, training loss: 62.317264556884766 = 0.04108090326189995 + 10.0 * 6.227618217468262
Epoch 1660, val loss: 1.5925575494766235
Epoch 1670, training loss: 62.33786392211914 = 0.040217526257038116 + 10.0 * 6.229764461517334
Epoch 1670, val loss: 1.5992246866226196
Epoch 1680, training loss: 62.313682556152344 = 0.03936705365777016 + 10.0 * 6.227431297302246
Epoch 1680, val loss: 1.6054993867874146
Epoch 1690, training loss: 62.309974670410156 = 0.038548532873392105 + 10.0 * 6.227142810821533
Epoch 1690, val loss: 1.612040638923645
Epoch 1700, training loss: 62.30561828613281 = 0.03775668144226074 + 10.0 * 6.226786136627197
Epoch 1700, val loss: 1.6185431480407715
Epoch 1710, training loss: 62.35022735595703 = 0.03699612244963646 + 10.0 * 6.2313232421875
Epoch 1710, val loss: 1.6250239610671997
Epoch 1720, training loss: 62.3206787109375 = 0.036228302866220474 + 10.0 * 6.228445053100586
Epoch 1720, val loss: 1.63101327419281
Epoch 1730, training loss: 62.29464340209961 = 0.035490017384290695 + 10.0 * 6.225915431976318
Epoch 1730, val loss: 1.6370054483413696
Epoch 1740, training loss: 62.30385971069336 = 0.03478773310780525 + 10.0 * 6.226907253265381
Epoch 1740, val loss: 1.6432147026062012
Epoch 1750, training loss: 62.3050422668457 = 0.034095946699380875 + 10.0 * 6.227094650268555
Epoch 1750, val loss: 1.6495956182479858
Epoch 1760, training loss: 62.30250549316406 = 0.03343331068754196 + 10.0 * 6.226907253265381
Epoch 1760, val loss: 1.6553027629852295
Epoch 1770, training loss: 62.34147644042969 = 0.032785963267087936 + 10.0 * 6.230868816375732
Epoch 1770, val loss: 1.6611783504486084
Epoch 1780, training loss: 62.28240966796875 = 0.03213941678404808 + 10.0 * 6.225027084350586
Epoch 1780, val loss: 1.6675164699554443
Epoch 1790, training loss: 62.26798629760742 = 0.03152972087264061 + 10.0 * 6.2236456871032715
Epoch 1790, val loss: 1.6732946634292603
Epoch 1800, training loss: 62.26713180541992 = 0.030941598117351532 + 10.0 * 6.223618984222412
Epoch 1800, val loss: 1.6791589260101318
Epoch 1810, training loss: 62.33088302612305 = 0.03036847710609436 + 10.0 * 6.230051517486572
Epoch 1810, val loss: 1.6848454475402832
Epoch 1820, training loss: 62.30539321899414 = 0.029789982363581657 + 10.0 * 6.227560520172119
Epoch 1820, val loss: 1.6903183460235596
Epoch 1830, training loss: 62.289947509765625 = 0.029236547648906708 + 10.0 * 6.226071357727051
Epoch 1830, val loss: 1.6959646940231323
Epoch 1840, training loss: 62.264015197753906 = 0.02869771607220173 + 10.0 * 6.223531723022461
Epoch 1840, val loss: 1.7017762660980225
Epoch 1850, training loss: 62.252838134765625 = 0.028181452304124832 + 10.0 * 6.222465515136719
Epoch 1850, val loss: 1.7075114250183105
Epoch 1860, training loss: 62.25691604614258 = 0.02768067643046379 + 10.0 * 6.222923755645752
Epoch 1860, val loss: 1.7131259441375732
Epoch 1870, training loss: 62.29560470581055 = 0.027189958840608597 + 10.0 * 6.226841449737549
Epoch 1870, val loss: 1.7185699939727783
Epoch 1880, training loss: 62.28848648071289 = 0.026706144213676453 + 10.0 * 6.226178169250488
Epoch 1880, val loss: 1.7234878540039062
Epoch 1890, training loss: 62.265235900878906 = 0.02622932940721512 + 10.0 * 6.22390079498291
Epoch 1890, val loss: 1.7291759252548218
Epoch 1900, training loss: 62.24864959716797 = 0.02577376551926136 + 10.0 * 6.222287654876709
Epoch 1900, val loss: 1.734358310699463
Epoch 1910, training loss: 62.23744201660156 = 0.02533199079334736 + 10.0 * 6.221210956573486
Epoch 1910, val loss: 1.7398561239242554
Epoch 1920, training loss: 62.261661529541016 = 0.024907095357775688 + 10.0 * 6.223675727844238
Epoch 1920, val loss: 1.7449945211410522
Epoch 1930, training loss: 62.23958206176758 = 0.024482371285557747 + 10.0 * 6.22150993347168
Epoch 1930, val loss: 1.7502427101135254
Epoch 1940, training loss: 62.2581787109375 = 0.024072136729955673 + 10.0 * 6.223410606384277
Epoch 1940, val loss: 1.7551507949829102
Epoch 1950, training loss: 62.252620697021484 = 0.023662688210606575 + 10.0 * 6.222895622253418
Epoch 1950, val loss: 1.760224461555481
Epoch 1960, training loss: 62.23247146606445 = 0.02327350713312626 + 10.0 * 6.220919609069824
Epoch 1960, val loss: 1.7652206420898438
Epoch 1970, training loss: 62.21874237060547 = 0.022893579676747322 + 10.0 * 6.219584941864014
Epoch 1970, val loss: 1.7705576419830322
Epoch 1980, training loss: 62.26156997680664 = 0.022533560171723366 + 10.0 * 6.223903656005859
Epoch 1980, val loss: 1.7755658626556396
Epoch 1990, training loss: 62.21877670288086 = 0.022160664200782776 + 10.0 * 6.219661712646484
Epoch 1990, val loss: 1.780004620552063
Epoch 2000, training loss: 62.21369934082031 = 0.021805908530950546 + 10.0 * 6.219189643859863
Epoch 2000, val loss: 1.7847416400909424
Epoch 2010, training loss: 62.22766876220703 = 0.021462716162204742 + 10.0 * 6.220620632171631
Epoch 2010, val loss: 1.7899398803710938
Epoch 2020, training loss: 62.245155334472656 = 0.021127644926309586 + 10.0 * 6.222403049468994
Epoch 2020, val loss: 1.7943278551101685
Epoch 2030, training loss: 62.22903060913086 = 0.020787980407476425 + 10.0 * 6.220824241638184
Epoch 2030, val loss: 1.7989933490753174
Epoch 2040, training loss: 62.23688507080078 = 0.020467663183808327 + 10.0 * 6.221642017364502
Epoch 2040, val loss: 1.803688645362854
Epoch 2050, training loss: 62.20006561279297 = 0.02015335112810135 + 10.0 * 6.217991352081299
Epoch 2050, val loss: 1.8082623481750488
Epoch 2060, training loss: 62.20303726196289 = 0.019852641969919205 + 10.0 * 6.218318462371826
Epoch 2060, val loss: 1.8129098415374756
Epoch 2070, training loss: 62.23423385620117 = 0.01955748163163662 + 10.0 * 6.2214674949646
Epoch 2070, val loss: 1.8173468112945557
Epoch 2080, training loss: 62.219974517822266 = 0.019257906824350357 + 10.0 * 6.220071792602539
Epoch 2080, val loss: 1.8215960264205933
Epoch 2090, training loss: 62.196876525878906 = 0.018973803147673607 + 10.0 * 6.217790126800537
Epoch 2090, val loss: 1.826159954071045
Epoch 2100, training loss: 62.19426345825195 = 0.01869705505669117 + 10.0 * 6.217556953430176
Epoch 2100, val loss: 1.8305559158325195
Epoch 2110, training loss: 62.2269401550293 = 0.018426813185214996 + 10.0 * 6.220851421356201
Epoch 2110, val loss: 1.8347828388214111
Epoch 2120, training loss: 62.23157501220703 = 0.018155165016651154 + 10.0 * 6.221342086791992
Epoch 2120, val loss: 1.8394495248794556
Epoch 2130, training loss: 62.197166442871094 = 0.017890779301524162 + 10.0 * 6.2179274559021
Epoch 2130, val loss: 1.8433018922805786
Epoch 2140, training loss: 62.17969512939453 = 0.017634177580475807 + 10.0 * 6.216206073760986
Epoch 2140, val loss: 1.847619652748108
Epoch 2150, training loss: 62.1734619140625 = 0.01738903671503067 + 10.0 * 6.215607643127441
Epoch 2150, val loss: 1.8519142866134644
Epoch 2160, training loss: 62.1868782043457 = 0.017152711749076843 + 10.0 * 6.216972827911377
Epoch 2160, val loss: 1.8559602499008179
Epoch 2170, training loss: 62.249717712402344 = 0.016918666660785675 + 10.0 * 6.22327995300293
Epoch 2170, val loss: 1.860015630722046
Epoch 2180, training loss: 62.2138557434082 = 0.0166732519865036 + 10.0 * 6.219717979431152
Epoch 2180, val loss: 1.8640891313552856
Epoch 2190, training loss: 62.178810119628906 = 0.016439635306596756 + 10.0 * 6.2162370681762695
Epoch 2190, val loss: 1.868074893951416
Epoch 2200, training loss: 62.169532775878906 = 0.016218457370996475 + 10.0 * 6.215331077575684
Epoch 2200, val loss: 1.872135877609253
Epoch 2210, training loss: 62.17094039916992 = 0.016004057601094246 + 10.0 * 6.215493679046631
Epoch 2210, val loss: 1.8760825395584106
Epoch 2220, training loss: 62.23640823364258 = 0.015792204067111015 + 10.0 * 6.222061634063721
Epoch 2220, val loss: 1.879765510559082
Epoch 2230, training loss: 62.1879768371582 = 0.015581654384732246 + 10.0 * 6.2172393798828125
Epoch 2230, val loss: 1.8838727474212646
Epoch 2240, training loss: 62.18821334838867 = 0.015369648113846779 + 10.0 * 6.217284202575684
Epoch 2240, val loss: 1.8876217603683472
Epoch 2250, training loss: 62.15735626220703 = 0.01516803354024887 + 10.0 * 6.214219093322754
Epoch 2250, val loss: 1.8912734985351562
Epoch 2260, training loss: 62.15495681762695 = 0.014973550103604794 + 10.0 * 6.213998317718506
Epoch 2260, val loss: 1.895078182220459
Epoch 2270, training loss: 62.174949645996094 = 0.014786912128329277 + 10.0 * 6.2160162925720215
Epoch 2270, val loss: 1.8988064527511597
Epoch 2280, training loss: 62.16023254394531 = 0.014594686217606068 + 10.0 * 6.214563846588135
Epoch 2280, val loss: 1.9024736881256104
Epoch 2290, training loss: 62.150062561035156 = 0.014406312257051468 + 10.0 * 6.213565349578857
Epoch 2290, val loss: 1.906162977218628
Epoch 2300, training loss: 62.15565490722656 = 0.01422758586704731 + 10.0 * 6.214142799377441
Epoch 2300, val loss: 1.9098458290100098
Epoch 2310, training loss: 62.21686935424805 = 0.014055089093744755 + 10.0 * 6.220281600952148
Epoch 2310, val loss: 1.9134440422058105
Epoch 2320, training loss: 62.156951904296875 = 0.013875347562134266 + 10.0 * 6.21430778503418
Epoch 2320, val loss: 1.9168741703033447
Epoch 2330, training loss: 62.14265441894531 = 0.013706088066101074 + 10.0 * 6.212894916534424
Epoch 2330, val loss: 1.9205352067947388
Epoch 2340, training loss: 62.173545837402344 = 0.01354294829070568 + 10.0 * 6.216000556945801
Epoch 2340, val loss: 1.924272060394287
Epoch 2350, training loss: 62.14704132080078 = 0.01337496843189001 + 10.0 * 6.213366508483887
Epoch 2350, val loss: 1.9271728992462158
Epoch 2360, training loss: 62.145347595214844 = 0.013213427737355232 + 10.0 * 6.2132134437561035
Epoch 2360, val loss: 1.9304910898208618
Epoch 2370, training loss: 62.156097412109375 = 0.013058545999228954 + 10.0 * 6.214303970336914
Epoch 2370, val loss: 1.9340863227844238
Epoch 2380, training loss: 62.150386810302734 = 0.012904634699225426 + 10.0 * 6.213747978210449
Epoch 2380, val loss: 1.9372169971466064
Epoch 2390, training loss: 62.160396575927734 = 0.01275552622973919 + 10.0 * 6.21476411819458
Epoch 2390, val loss: 1.940449595451355
Epoch 2400, training loss: 62.16239929199219 = 0.012606183998286724 + 10.0 * 6.21497917175293
Epoch 2400, val loss: 1.9436721801757812
Epoch 2410, training loss: 62.1395149230957 = 0.012456262484192848 + 10.0 * 6.212706089019775
Epoch 2410, val loss: 1.946905255317688
Epoch 2420, training loss: 62.12990951538086 = 0.01231494452804327 + 10.0 * 6.211759567260742
Epoch 2420, val loss: 1.950280785560608
Epoch 2430, training loss: 62.13595199584961 = 0.01217897143214941 + 10.0 * 6.212377071380615
Epoch 2430, val loss: 1.953628659248352
Epoch 2440, training loss: 62.18428421020508 = 0.012044984847307205 + 10.0 * 6.217223644256592
Epoch 2440, val loss: 1.9567075967788696
Epoch 2450, training loss: 62.15742874145508 = 0.011905542574822903 + 10.0 * 6.214552402496338
Epoch 2450, val loss: 1.9595413208007812
Epoch 2460, training loss: 62.12986373901367 = 0.011770505458116531 + 10.0 * 6.211809158325195
Epoch 2460, val loss: 1.9626424312591553
Epoch 2470, training loss: 62.12103271484375 = 0.011641590856015682 + 10.0 * 6.210938930511475
Epoch 2470, val loss: 1.9658185243606567
Epoch 2480, training loss: 62.1352424621582 = 0.011517993174493313 + 10.0 * 6.212372779846191
Epoch 2480, val loss: 1.9688377380371094
Epoch 2490, training loss: 62.15970230102539 = 0.011393291875720024 + 10.0 * 6.2148308753967285
Epoch 2490, val loss: 1.9718010425567627
Epoch 2500, training loss: 62.14453887939453 = 0.011266661807894707 + 10.0 * 6.213327407836914
Epoch 2500, val loss: 1.9743162393569946
Epoch 2510, training loss: 62.12471389770508 = 0.011144135147333145 + 10.0 * 6.211357116699219
Epoch 2510, val loss: 1.977273941040039
Epoch 2520, training loss: 62.14617156982422 = 0.011027928441762924 + 10.0 * 6.21351432800293
Epoch 2520, val loss: 1.9798882007598877
Epoch 2530, training loss: 62.120059967041016 = 0.01090999972075224 + 10.0 * 6.2109150886535645
Epoch 2530, val loss: 1.9831737279891968
Epoch 2540, training loss: 62.14118194580078 = 0.010796613991260529 + 10.0 * 6.213038444519043
Epoch 2540, val loss: 1.9859235286712646
Epoch 2550, training loss: 62.1218147277832 = 0.010682868771255016 + 10.0 * 6.211113452911377
Epoch 2550, val loss: 1.9886027574539185
Epoch 2560, training loss: 62.115447998046875 = 0.010572933591902256 + 10.0 * 6.210487365722656
Epoch 2560, val loss: 1.9916305541992188
Epoch 2570, training loss: 62.1262092590332 = 0.010467039421200752 + 10.0 * 6.211574077606201
Epoch 2570, val loss: 1.9941496849060059
Epoch 2580, training loss: 62.118797302246094 = 0.010359554551541805 + 10.0 * 6.210843563079834
Epoch 2580, val loss: 1.9967840909957886
Epoch 2590, training loss: 62.126060485839844 = 0.010254113003611565 + 10.0 * 6.211580753326416
Epoch 2590, val loss: 1.999481201171875
Epoch 2600, training loss: 62.107276916503906 = 0.01015107799321413 + 10.0 * 6.209712505340576
Epoch 2600, val loss: 2.001981258392334
Epoch 2610, training loss: 62.13582229614258 = 0.010051109828054905 + 10.0 * 6.212576866149902
Epoch 2610, val loss: 2.0047614574432373
Epoch 2620, training loss: 62.11367416381836 = 0.0099497614428401 + 10.0 * 6.210372447967529
Epoch 2620, val loss: 2.0072479248046875
Epoch 2630, training loss: 62.11614227294922 = 0.009849407710134983 + 10.0 * 6.210629463195801
Epoch 2630, val loss: 2.009951114654541
Epoch 2640, training loss: 62.111480712890625 = 0.009752370417118073 + 10.0 * 6.210172653198242
Epoch 2640, val loss: 2.0121519565582275
Epoch 2650, training loss: 62.12559127807617 = 0.009658112190663815 + 10.0 * 6.211593151092529
Epoch 2650, val loss: 2.014838695526123
Epoch 2660, training loss: 62.09479522705078 = 0.009565042331814766 + 10.0 * 6.208523273468018
Epoch 2660, val loss: 2.0171406269073486
Epoch 2670, training loss: 62.1131706237793 = 0.009475702419877052 + 10.0 * 6.21036958694458
Epoch 2670, val loss: 2.019857883453369
Epoch 2680, training loss: 62.10478973388672 = 0.009385709650814533 + 10.0 * 6.209540367126465
Epoch 2680, val loss: 2.0220534801483154
Epoch 2690, training loss: 62.10611343383789 = 0.009296354837715626 + 10.0 * 6.209681510925293
Epoch 2690, val loss: 2.0246896743774414
Epoch 2700, training loss: 62.11128234863281 = 0.009209689684212208 + 10.0 * 6.210207462310791
Epoch 2700, val loss: 2.0269341468811035
Epoch 2710, training loss: 62.09300994873047 = 0.009123251773416996 + 10.0 * 6.208388328552246
Epoch 2710, val loss: 2.029021739959717
Epoch 2720, training loss: 62.11227798461914 = 0.009041257202625275 + 10.0 * 6.210323810577393
Epoch 2720, val loss: 2.0314319133758545
Epoch 2730, training loss: 62.110782623291016 = 0.008954519405961037 + 10.0 * 6.210183143615723
Epoch 2730, val loss: 2.0336780548095703
Epoch 2740, training loss: 62.0999755859375 = 0.008872730657458305 + 10.0 * 6.209110260009766
Epoch 2740, val loss: 2.035501718521118
Epoch 2750, training loss: 62.079833984375 = 0.008790400810539722 + 10.0 * 6.207104682922363
Epoch 2750, val loss: 2.037926197052002
Epoch 2760, training loss: 62.08208084106445 = 0.008713986724615097 + 10.0 * 6.207336902618408
Epoch 2760, val loss: 2.0402746200561523
Epoch 2770, training loss: 62.10370635986328 = 0.008639021776616573 + 10.0 * 6.209506511688232
Epoch 2770, val loss: 2.042642831802368
Epoch 2780, training loss: 62.08729553222656 = 0.008561446331441402 + 10.0 * 6.207873344421387
Epoch 2780, val loss: 2.0443661212921143
Epoch 2790, training loss: 62.09282684326172 = 0.008484907448291779 + 10.0 * 6.208434104919434
Epoch 2790, val loss: 2.0462849140167236
Epoch 2800, training loss: 62.10914611816406 = 0.008411942049860954 + 10.0 * 6.210073471069336
Epoch 2800, val loss: 2.0487148761749268
Epoch 2810, training loss: 62.0800895690918 = 0.008336828090250492 + 10.0 * 6.207175254821777
Epoch 2810, val loss: 2.0505285263061523
Epoch 2820, training loss: 62.105552673339844 = 0.008266350254416466 + 10.0 * 6.209728717803955
Epoch 2820, val loss: 2.052307605743408
Epoch 2830, training loss: 62.08641815185547 = 0.008193221874535084 + 10.0 * 6.207822322845459
Epoch 2830, val loss: 2.0545973777770996
Epoch 2840, training loss: 62.07258224487305 = 0.008124101907014847 + 10.0 * 6.206445693969727
Epoch 2840, val loss: 2.056579113006592
Epoch 2850, training loss: 62.08927917480469 = 0.008057947270572186 + 10.0 * 6.208122253417969
Epoch 2850, val loss: 2.0588040351867676
Epoch 2860, training loss: 62.0781364440918 = 0.007990313693881035 + 10.0 * 6.207014560699463
Epoch 2860, val loss: 2.0605645179748535
Epoch 2870, training loss: 62.08361053466797 = 0.007923856377601624 + 10.0 * 6.207568645477295
Epoch 2870, val loss: 2.062239170074463
Epoch 2880, training loss: 62.099056243896484 = 0.00785895437002182 + 10.0 * 6.20911979675293
Epoch 2880, val loss: 2.0643084049224854
Epoch 2890, training loss: 62.07609939575195 = 0.007793145254254341 + 10.0 * 6.2068305015563965
Epoch 2890, val loss: 2.06595778465271
Epoch 2900, training loss: 62.07695770263672 = 0.007728466298431158 + 10.0 * 6.206923007965088
Epoch 2900, val loss: 2.068054676055908
Epoch 2910, training loss: 62.05559158325195 = 0.007665805984288454 + 10.0 * 6.204792499542236
Epoch 2910, val loss: 2.069685459136963
Epoch 2920, training loss: 62.073604583740234 = 0.007606541737914085 + 10.0 * 6.206599712371826
Epoch 2920, val loss: 2.071617364883423
Epoch 2930, training loss: 62.103092193603516 = 0.007543579675257206 + 10.0 * 6.209554672241211
Epoch 2930, val loss: 2.0733389854431152
Epoch 2940, training loss: 62.08636474609375 = 0.007483232766389847 + 10.0 * 6.207888126373291
Epoch 2940, val loss: 2.0750765800476074
Epoch 2950, training loss: 62.08161163330078 = 0.007421503309160471 + 10.0 * 6.207418918609619
Epoch 2950, val loss: 2.076554775238037
Epoch 2960, training loss: 62.06496810913086 = 0.007362894713878632 + 10.0 * 6.205760478973389
Epoch 2960, val loss: 2.0786027908325195
Epoch 2970, training loss: 62.06243896484375 = 0.007306220009922981 + 10.0 * 6.2055134773254395
Epoch 2970, val loss: 2.080411434173584
Epoch 2980, training loss: 62.06387710571289 = 0.007249913178384304 + 10.0 * 6.205662727355957
Epoch 2980, val loss: 2.0821852684020996
Epoch 2990, training loss: 62.06747817993164 = 0.007194587495177984 + 10.0 * 6.206028461456299
Epoch 2990, val loss: 2.083656072616577
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.674074074074074
0.797575118608329
=== training gcn model ===
Epoch 0, training loss: 87.91192626953125 = 1.9433449506759644 + 10.0 * 8.596858024597168
Epoch 0, val loss: 1.9407709836959839
Epoch 10, training loss: 87.8954086303711 = 1.932647466659546 + 10.0 * 8.59627628326416
Epoch 10, val loss: 1.9295310974121094
Epoch 20, training loss: 87.83335876464844 = 1.919517993927002 + 10.0 * 8.591383934020996
Epoch 20, val loss: 1.9155200719833374
Epoch 30, training loss: 87.42289733886719 = 1.9030373096466064 + 10.0 * 8.551985740661621
Epoch 30, val loss: 1.898008942604065
Epoch 40, training loss: 84.53776550292969 = 1.884884238243103 + 10.0 * 8.265287399291992
Epoch 40, val loss: 1.8793835639953613
Epoch 50, training loss: 77.05043029785156 = 1.866995096206665 + 10.0 * 7.518343448638916
Epoch 50, val loss: 1.8619887828826904
Epoch 60, training loss: 73.35746002197266 = 1.8539392948150635 + 10.0 * 7.150351524353027
Epoch 60, val loss: 1.8496904373168945
Epoch 70, training loss: 71.32428741455078 = 1.8418340682983398 + 10.0 * 6.948245048522949
Epoch 70, val loss: 1.8381081819534302
Epoch 80, training loss: 70.4109115600586 = 1.8309247493743896 + 10.0 * 6.857998847961426
Epoch 80, val loss: 1.828333854675293
Epoch 90, training loss: 69.58158111572266 = 1.8208786249160767 + 10.0 * 6.7760701179504395
Epoch 90, val loss: 1.81942880153656
Epoch 100, training loss: 68.782958984375 = 1.8120054006576538 + 10.0 * 6.6970953941345215
Epoch 100, val loss: 1.811703085899353
Epoch 110, training loss: 68.26376342773438 = 1.8031222820281982 + 10.0 * 6.646064281463623
Epoch 110, val loss: 1.8039493560791016
Epoch 120, training loss: 67.8643569946289 = 1.7937684059143066 + 10.0 * 6.607059001922607
Epoch 120, val loss: 1.7959147691726685
Epoch 130, training loss: 67.51529693603516 = 1.7842600345611572 + 10.0 * 6.573103427886963
Epoch 130, val loss: 1.7877559661865234
Epoch 140, training loss: 67.20539855957031 = 1.7745155096054077 + 10.0 * 6.543088436126709
Epoch 140, val loss: 1.7794328927993774
Epoch 150, training loss: 66.97129821777344 = 1.764148473739624 + 10.0 * 6.520715236663818
Epoch 150, val loss: 1.7705150842666626
Epoch 160, training loss: 66.76139831542969 = 1.7529014348983765 + 10.0 * 6.500849723815918
Epoch 160, val loss: 1.760809063911438
Epoch 170, training loss: 66.58317565917969 = 1.7406647205352783 + 10.0 * 6.484251022338867
Epoch 170, val loss: 1.7503316402435303
Epoch 180, training loss: 66.46965026855469 = 1.7272294759750366 + 10.0 * 6.474242687225342
Epoch 180, val loss: 1.7389851808547974
Epoch 190, training loss: 66.28787231445312 = 1.7125364542007446 + 10.0 * 6.45753288269043
Epoch 190, val loss: 1.7266557216644287
Epoch 200, training loss: 66.1505126953125 = 1.69648277759552 + 10.0 * 6.445403099060059
Epoch 200, val loss: 1.713349461555481
Epoch 210, training loss: 66.01811218261719 = 1.6788760423660278 + 10.0 * 6.433923244476318
Epoch 210, val loss: 1.6988376379013062
Epoch 220, training loss: 65.92807006835938 = 1.6596721410751343 + 10.0 * 6.426839351654053
Epoch 220, val loss: 1.6832265853881836
Epoch 230, training loss: 65.81228637695312 = 1.6389575004577637 + 10.0 * 6.417333126068115
Epoch 230, val loss: 1.6664998531341553
Epoch 240, training loss: 65.6778335571289 = 1.6167057752609253 + 10.0 * 6.4061126708984375
Epoch 240, val loss: 1.6486295461654663
Epoch 250, training loss: 65.5687255859375 = 1.5929511785507202 + 10.0 * 6.39757776260376
Epoch 250, val loss: 1.629726529121399
Epoch 260, training loss: 65.47356414794922 = 1.5677669048309326 + 10.0 * 6.390580177307129
Epoch 260, val loss: 1.6099659204483032
Epoch 270, training loss: 65.37126159667969 = 1.5414597988128662 + 10.0 * 6.382979869842529
Epoch 270, val loss: 1.5895459651947021
Epoch 280, training loss: 65.2768783569336 = 1.5141493082046509 + 10.0 * 6.376272678375244
Epoch 280, val loss: 1.568688988685608
Epoch 290, training loss: 65.18453216552734 = 1.4860363006591797 + 10.0 * 6.369849681854248
Epoch 290, val loss: 1.547565221786499
Epoch 300, training loss: 65.18262481689453 = 1.4573601484298706 + 10.0 * 6.372526168823242
Epoch 300, val loss: 1.5263372659683228
Epoch 310, training loss: 65.04410552978516 = 1.4283134937286377 + 10.0 * 6.361578941345215
Epoch 310, val loss: 1.5051816701889038
Epoch 320, training loss: 64.9444351196289 = 1.399172067642212 + 10.0 * 6.354526519775391
Epoch 320, val loss: 1.4842875003814697
Epoch 330, training loss: 64.93706512451172 = 1.3699860572814941 + 10.0 * 6.356707572937012
Epoch 330, val loss: 1.4636445045471191
Epoch 340, training loss: 64.8056411743164 = 1.3410793542861938 + 10.0 * 6.346456527709961
Epoch 340, val loss: 1.4435112476348877
Epoch 350, training loss: 64.73904418945312 = 1.31231689453125 + 10.0 * 6.342672348022461
Epoch 350, val loss: 1.4237178564071655
Epoch 360, training loss: 64.66600799560547 = 1.2838925123214722 + 10.0 * 6.338212013244629
Epoch 360, val loss: 1.4043385982513428
Epoch 370, training loss: 64.68396759033203 = 1.255738377571106 + 10.0 * 6.342822551727295
Epoch 370, val loss: 1.3852488994598389
Epoch 380, training loss: 64.57442474365234 = 1.2280369997024536 + 10.0 * 6.334639072418213
Epoch 380, val loss: 1.3666956424713135
Epoch 390, training loss: 64.4962387084961 = 1.20079505443573 + 10.0 * 6.3295440673828125
Epoch 390, val loss: 1.3485444784164429
Epoch 400, training loss: 64.43207550048828 = 1.174039602279663 + 10.0 * 6.325803756713867
Epoch 400, val loss: 1.330877423286438
Epoch 410, training loss: 64.3748550415039 = 1.147756576538086 + 10.0 * 6.322709560394287
Epoch 410, val loss: 1.3136980533599854
Epoch 420, training loss: 64.44976043701172 = 1.1218181848526 + 10.0 * 6.332794189453125
Epoch 420, val loss: 1.296830177307129
Epoch 430, training loss: 64.29528045654297 = 1.0965243577957153 + 10.0 * 6.319875240325928
Epoch 430, val loss: 1.2806649208068848
Epoch 440, training loss: 64.2291488647461 = 1.0717625617980957 + 10.0 * 6.315738201141357
Epoch 440, val loss: 1.2650671005249023
Epoch 450, training loss: 64.1736831665039 = 1.0475400686264038 + 10.0 * 6.312614440917969
Epoch 450, val loss: 1.2499804496765137
Epoch 460, training loss: 64.12748718261719 = 1.0238195657730103 + 10.0 * 6.310366630554199
Epoch 460, val loss: 1.2355585098266602
Epoch 470, training loss: 64.11019897460938 = 1.0005627870559692 + 10.0 * 6.3109636306762695
Epoch 470, val loss: 1.2217459678649902
Epoch 480, training loss: 64.08561706542969 = 0.9778540730476379 + 10.0 * 6.310776233673096
Epoch 480, val loss: 1.2082289457321167
Epoch 490, training loss: 64.03363800048828 = 0.9556747078895569 + 10.0 * 6.307796001434326
Epoch 490, val loss: 1.1955755949020386
Epoch 500, training loss: 63.9627799987793 = 0.9339826703071594 + 10.0 * 6.302879810333252
Epoch 500, val loss: 1.183502197265625
Epoch 510, training loss: 63.94214630126953 = 0.9128876328468323 + 10.0 * 6.302926063537598
Epoch 510, val loss: 1.1721575260162354
Epoch 520, training loss: 63.92781066894531 = 0.892399251461029 + 10.0 * 6.30354118347168
Epoch 520, val loss: 1.1617958545684814
Epoch 530, training loss: 63.86250686645508 = 0.872423529624939 + 10.0 * 6.299008369445801
Epoch 530, val loss: 1.1516661643981934
Epoch 540, training loss: 63.80779266357422 = 0.8530811071395874 + 10.0 * 6.29547119140625
Epoch 540, val loss: 1.142442226409912
Epoch 550, training loss: 63.772823333740234 = 0.8342406153678894 + 10.0 * 6.293858528137207
Epoch 550, val loss: 1.1339547634124756
Epoch 560, training loss: 63.78892517089844 = 0.8159047961235046 + 10.0 * 6.297301769256592
Epoch 560, val loss: 1.1257046461105347
Epoch 570, training loss: 63.72659683227539 = 0.7980982661247253 + 10.0 * 6.292849540710449
Epoch 570, val loss: 1.118808627128601
Epoch 580, training loss: 63.67763137817383 = 0.7807040810585022 + 10.0 * 6.2896928787231445
Epoch 580, val loss: 1.112207055091858
Epoch 590, training loss: 63.64274978637695 = 0.7637765407562256 + 10.0 * 6.28789758682251
Epoch 590, val loss: 1.1059426069259644
Epoch 600, training loss: 63.63389587402344 = 0.7472375631332397 + 10.0 * 6.288665771484375
Epoch 600, val loss: 1.1003410816192627
Epoch 610, training loss: 63.62696075439453 = 0.7309927344322205 + 10.0 * 6.289597034454346
Epoch 610, val loss: 1.0953294038772583
Epoch 620, training loss: 63.56450271606445 = 0.7150607109069824 + 10.0 * 6.2849440574646
Epoch 620, val loss: 1.0905882120132446
Epoch 630, training loss: 63.57106399536133 = 0.6994025707244873 + 10.0 * 6.287166118621826
Epoch 630, val loss: 1.0861347913742065
Epoch 640, training loss: 63.51053237915039 = 0.6840876936912537 + 10.0 * 6.282644748687744
Epoch 640, val loss: 1.0821980237960815
Epoch 650, training loss: 63.47600173950195 = 0.6690565943717957 + 10.0 * 6.280694484710693
Epoch 650, val loss: 1.0787770748138428
Epoch 660, training loss: 63.44267654418945 = 0.6541644334793091 + 10.0 * 6.278851509094238
Epoch 660, val loss: 1.0756268501281738
Epoch 670, training loss: 63.41830062866211 = 0.6394302845001221 + 10.0 * 6.277886867523193
Epoch 670, val loss: 1.0727012157440186
Epoch 680, training loss: 63.47688293457031 = 0.6248706579208374 + 10.0 * 6.285201072692871
Epoch 680, val loss: 1.0701303482055664
Epoch 690, training loss: 63.391727447509766 = 0.6102831363677979 + 10.0 * 6.278144359588623
Epoch 690, val loss: 1.0673631429672241
Epoch 700, training loss: 63.3487434387207 = 0.5959050059318542 + 10.0 * 6.2752838134765625
Epoch 700, val loss: 1.0654449462890625
Epoch 710, training loss: 63.32234191894531 = 0.5816506743431091 + 10.0 * 6.274069309234619
Epoch 710, val loss: 1.063336730003357
Epoch 720, training loss: 63.356895446777344 = 0.5674181580543518 + 10.0 * 6.278947830200195
Epoch 720, val loss: 1.0613726377487183
Epoch 730, training loss: 63.32840347290039 = 0.5533060431480408 + 10.0 * 6.277509689331055
Epoch 730, val loss: 1.0599653720855713
Epoch 740, training loss: 63.25019454956055 = 0.5392622351646423 + 10.0 * 6.271093368530273
Epoch 740, val loss: 1.0585826635360718
Epoch 750, training loss: 63.228614807128906 = 0.5253362655639648 + 10.0 * 6.270327568054199
Epoch 750, val loss: 1.057475209236145
Epoch 760, training loss: 63.202938079833984 = 0.5114787817001343 + 10.0 * 6.269145965576172
Epoch 760, val loss: 1.0565015077590942
Epoch 770, training loss: 63.25225830078125 = 0.49764665961265564 + 10.0 * 6.275461196899414
Epoch 770, val loss: 1.0554543733596802
Epoch 780, training loss: 63.20149230957031 = 0.4839693605899811 + 10.0 * 6.27175235748291
Epoch 780, val loss: 1.0553187131881714
Epoch 790, training loss: 63.14608383178711 = 0.4703161418437958 + 10.0 * 6.267576694488525
Epoch 790, val loss: 1.0548858642578125
Epoch 800, training loss: 63.11223220825195 = 0.4568385183811188 + 10.0 * 6.265539646148682
Epoch 800, val loss: 1.054726481437683
Epoch 810, training loss: 63.09321594238281 = 0.4434736371040344 + 10.0 * 6.264974117279053
Epoch 810, val loss: 1.0549829006195068
Epoch 820, training loss: 63.141910552978516 = 0.4301702678203583 + 10.0 * 6.27117395401001
Epoch 820, val loss: 1.055013656616211
Epoch 830, training loss: 63.07817840576172 = 0.4170984625816345 + 10.0 * 6.26610803604126
Epoch 830, val loss: 1.0559120178222656
Epoch 840, training loss: 63.07904815673828 = 0.40408244729042053 + 10.0 * 6.267496585845947
Epoch 840, val loss: 1.056618094444275
Epoch 850, training loss: 63.028743743896484 = 0.39131754636764526 + 10.0 * 6.263742446899414
Epoch 850, val loss: 1.0576859712600708
Epoch 860, training loss: 62.99869918823242 = 0.378794401884079 + 10.0 * 6.261990547180176
Epoch 860, val loss: 1.0594463348388672
Epoch 870, training loss: 62.96923828125 = 0.366461843252182 + 10.0 * 6.26027774810791
Epoch 870, val loss: 1.061063528060913
Epoch 880, training loss: 62.94938278198242 = 0.35435375571250916 + 10.0 * 6.25950288772583
Epoch 880, val loss: 1.063040852546692
Epoch 890, training loss: 63.04570007324219 = 0.34244734048843384 + 10.0 * 6.270325183868408
Epoch 890, val loss: 1.0647883415222168
Epoch 900, training loss: 62.9630241394043 = 0.3308223783969879 + 10.0 * 6.263220310211182
Epoch 900, val loss: 1.0677064657211304
Epoch 910, training loss: 62.90108108520508 = 0.3194827437400818 + 10.0 * 6.25816011428833
Epoch 910, val loss: 1.070317268371582
Epoch 920, training loss: 62.8791389465332 = 0.30846911668777466 + 10.0 * 6.2570672035217285
Epoch 920, val loss: 1.0735504627227783
Epoch 930, training loss: 62.8914909362793 = 0.2978008985519409 + 10.0 * 6.259368896484375
Epoch 930, val loss: 1.0772148370742798
Epoch 940, training loss: 62.84973907470703 = 0.28736183047294617 + 10.0 * 6.256237983703613
Epoch 940, val loss: 1.0804585218429565
Epoch 950, training loss: 62.835933685302734 = 0.2772670388221741 + 10.0 * 6.255866527557373
Epoch 950, val loss: 1.084316611289978
Epoch 960, training loss: 62.862632751464844 = 0.2676157057285309 + 10.0 * 6.2595014572143555
Epoch 960, val loss: 1.0889594554901123
Epoch 970, training loss: 62.80679702758789 = 0.2581747770309448 + 10.0 * 6.254862308502197
Epoch 970, val loss: 1.0931189060211182
Epoch 980, training loss: 62.78471755981445 = 0.24916739761829376 + 10.0 * 6.253554821014404
Epoch 980, val loss: 1.0984326601028442
Epoch 990, training loss: 62.796363830566406 = 0.24043944478034973 + 10.0 * 6.255592346191406
Epoch 990, val loss: 1.1031806468963623
Epoch 1000, training loss: 62.74956512451172 = 0.23209154605865479 + 10.0 * 6.2517476081848145
Epoch 1000, val loss: 1.10914945602417
Epoch 1010, training loss: 62.7348747253418 = 0.2240704447031021 + 10.0 * 6.251080513000488
Epoch 1010, val loss: 1.1147396564483643
Epoch 1020, training loss: 62.72846603393555 = 0.21639366447925568 + 10.0 * 6.25120735168457
Epoch 1020, val loss: 1.1208643913269043
Epoch 1030, training loss: 62.76299285888672 = 0.20903058350086212 + 10.0 * 6.255396366119385
Epoch 1030, val loss: 1.1270930767059326
Epoch 1040, training loss: 62.7011604309082 = 0.20203492045402527 + 10.0 * 6.249912738800049
Epoch 1040, val loss: 1.133973479270935
Epoch 1050, training loss: 62.69239044189453 = 0.19531938433647156 + 10.0 * 6.249707221984863
Epoch 1050, val loss: 1.1407817602157593
Epoch 1060, training loss: 62.71163558959961 = 0.18886491656303406 + 10.0 * 6.25227689743042
Epoch 1060, val loss: 1.1476287841796875
Epoch 1070, training loss: 62.67634963989258 = 0.18267859518527985 + 10.0 * 6.2493672370910645
Epoch 1070, val loss: 1.1548124551773071
Epoch 1080, training loss: 62.662620544433594 = 0.17678947746753693 + 10.0 * 6.2485833168029785
Epoch 1080, val loss: 1.162177324295044
Epoch 1090, training loss: 62.6715087890625 = 0.17115162312984467 + 10.0 * 6.250035762786865
Epoch 1090, val loss: 1.1697179079055786
Epoch 1100, training loss: 62.662879943847656 = 0.1657804399728775 + 10.0 * 6.2497100830078125
Epoch 1100, val loss: 1.1781396865844727
Epoch 1110, training loss: 62.63862228393555 = 0.1605750322341919 + 10.0 * 6.247804641723633
Epoch 1110, val loss: 1.1854702234268188
Epoch 1120, training loss: 62.62556457519531 = 0.15560060739517212 + 10.0 * 6.2469964027404785
Epoch 1120, val loss: 1.1933132410049438
Epoch 1130, training loss: 62.599700927734375 = 0.15085329115390778 + 10.0 * 6.244884967803955
Epoch 1130, val loss: 1.2013089656829834
Epoch 1140, training loss: 62.6097297668457 = 0.1463121771812439 + 10.0 * 6.246341705322266
Epoch 1140, val loss: 1.2091736793518066
Epoch 1150, training loss: 62.610233306884766 = 0.14193759858608246 + 10.0 * 6.246829509735107
Epoch 1150, val loss: 1.217118263244629
Epoch 1160, training loss: 62.584407806396484 = 0.1377541869878769 + 10.0 * 6.244665622711182
Epoch 1160, val loss: 1.225908875465393
Epoch 1170, training loss: 62.57207489013672 = 0.1337365359067917 + 10.0 * 6.243834018707275
Epoch 1170, val loss: 1.2342418432235718
Epoch 1180, training loss: 62.554664611816406 = 0.12987585365772247 + 10.0 * 6.242478847503662
Epoch 1180, val loss: 1.242402195930481
Epoch 1190, training loss: 62.58465576171875 = 0.12615838646888733 + 10.0 * 6.245849609375
Epoch 1190, val loss: 1.2505961656570435
Epoch 1200, training loss: 62.584835052490234 = 0.12256857752799988 + 10.0 * 6.2462263107299805
Epoch 1200, val loss: 1.258728265762329
Epoch 1210, training loss: 62.549800872802734 = 0.11913354694843292 + 10.0 * 6.243066787719727
Epoch 1210, val loss: 1.267600655555725
Epoch 1220, training loss: 62.5302619934082 = 0.11581643670797348 + 10.0 * 6.2414445877075195
Epoch 1220, val loss: 1.2759487628936768
Epoch 1230, training loss: 62.5155143737793 = 0.11263526231050491 + 10.0 * 6.240287780761719
Epoch 1230, val loss: 1.284367561340332
Epoch 1240, training loss: 62.55020523071289 = 0.10958074778318405 + 10.0 * 6.244062423706055
Epoch 1240, val loss: 1.2924516201019287
Epoch 1250, training loss: 62.52496337890625 = 0.10660271346569061 + 10.0 * 6.241836071014404
Epoch 1250, val loss: 1.3013664484024048
Epoch 1260, training loss: 62.53211212158203 = 0.10373418778181076 + 10.0 * 6.242837905883789
Epoch 1260, val loss: 1.3086374998092651
Epoch 1270, training loss: 62.50069808959961 = 0.10096748918294907 + 10.0 * 6.239973068237305
Epoch 1270, val loss: 1.31786048412323
Epoch 1280, training loss: 62.48648452758789 = 0.09832257032394409 + 10.0 * 6.238816261291504
Epoch 1280, val loss: 1.326036810874939
Epoch 1290, training loss: 62.518760681152344 = 0.0957576334476471 + 10.0 * 6.242300510406494
Epoch 1290, val loss: 1.334078073501587
Epoch 1300, training loss: 62.50873565673828 = 0.09327796846628189 + 10.0 * 6.241545677185059
Epoch 1300, val loss: 1.3424618244171143
Epoch 1310, training loss: 62.46857452392578 = 0.0908963680267334 + 10.0 * 6.237767696380615
Epoch 1310, val loss: 1.3511203527450562
Epoch 1320, training loss: 62.46131134033203 = 0.08859048783779144 + 10.0 * 6.237272262573242
Epoch 1320, val loss: 1.3597819805145264
Epoch 1330, training loss: 62.47140884399414 = 0.08637431263923645 + 10.0 * 6.238503456115723
Epoch 1330, val loss: 1.3678141832351685
Epoch 1340, training loss: 62.473182678222656 = 0.08419759571552277 + 10.0 * 6.238898277282715
Epoch 1340, val loss: 1.3759784698486328
Epoch 1350, training loss: 62.46870803833008 = 0.08206535130739212 + 10.0 * 6.238664150238037
Epoch 1350, val loss: 1.3833050727844238
Epoch 1360, training loss: 62.44106674194336 = 0.08005138486623764 + 10.0 * 6.2361016273498535
Epoch 1360, val loss: 1.392195224761963
Epoch 1370, training loss: 62.427398681640625 = 0.07808271050453186 + 10.0 * 6.234931468963623
Epoch 1370, val loss: 1.40041983127594
Epoch 1380, training loss: 62.45158386230469 = 0.07618704438209534 + 10.0 * 6.237539768218994
Epoch 1380, val loss: 1.4082913398742676
Epoch 1390, training loss: 62.42668151855469 = 0.07433287054300308 + 10.0 * 6.23523473739624
Epoch 1390, val loss: 1.4164828062057495
Epoch 1400, training loss: 62.420448303222656 = 0.07254061102867126 + 10.0 * 6.234790802001953
Epoch 1400, val loss: 1.4243897199630737
Epoch 1410, training loss: 62.42169952392578 = 0.07081272453069687 + 10.0 * 6.23508882522583
Epoch 1410, val loss: 1.4324978590011597
Epoch 1420, training loss: 62.46172332763672 = 0.06916077435016632 + 10.0 * 6.239256381988525
Epoch 1420, val loss: 1.4407787322998047
Epoch 1430, training loss: 62.396820068359375 = 0.06748221814632416 + 10.0 * 6.23293399810791
Epoch 1430, val loss: 1.4479743242263794
Epoch 1440, training loss: 62.38873291015625 = 0.06590599566698074 + 10.0 * 6.232282638549805
Epoch 1440, val loss: 1.4559502601623535
Epoch 1450, training loss: 62.39090347290039 = 0.06437613815069199 + 10.0 * 6.23265266418457
Epoch 1450, val loss: 1.4639226198196411
Epoch 1460, training loss: 62.413116455078125 = 0.06288964301347733 + 10.0 * 6.23502254486084
Epoch 1460, val loss: 1.4713386297225952
Epoch 1470, training loss: 62.389095306396484 = 0.06144464761018753 + 10.0 * 6.232765197753906
Epoch 1470, val loss: 1.479386806488037
Epoch 1480, training loss: 62.37687683105469 = 0.06003773584961891 + 10.0 * 6.231683731079102
Epoch 1480, val loss: 1.4871653318405151
Epoch 1490, training loss: 62.37229537963867 = 0.058680932968854904 + 10.0 * 6.231361389160156
Epoch 1490, val loss: 1.49483060836792
Epoch 1500, training loss: 62.419921875 = 0.057380735874176025 + 10.0 * 6.2362542152404785
Epoch 1500, val loss: 1.5025116205215454
Epoch 1510, training loss: 62.39274215698242 = 0.05608236789703369 + 10.0 * 6.233665943145752
Epoch 1510, val loss: 1.5100880861282349
Epoch 1520, training loss: 62.3756103515625 = 0.05483287200331688 + 10.0 * 6.232077598571777
Epoch 1520, val loss: 1.5173760652542114
Epoch 1530, training loss: 62.364967346191406 = 0.053620412945747375 + 10.0 * 6.23113489151001
Epoch 1530, val loss: 1.5251837968826294
Epoch 1540, training loss: 62.36299133300781 = 0.05244925990700722 + 10.0 * 6.231054306030273
Epoch 1540, val loss: 1.532286524772644
Epoch 1550, training loss: 62.33964920043945 = 0.051293861120939255 + 10.0 * 6.228835582733154
Epoch 1550, val loss: 1.5394648313522339
Epoch 1560, training loss: 62.376670837402344 = 0.050183508545160294 + 10.0 * 6.232648849487305
Epoch 1560, val loss: 1.5460008382797241
Epoch 1570, training loss: 62.336795806884766 = 0.04910125210881233 + 10.0 * 6.228769302368164
Epoch 1570, val loss: 1.5545333623886108
Epoch 1580, training loss: 62.32563781738281 = 0.048055026680231094 + 10.0 * 6.227758407592773
Epoch 1580, val loss: 1.5613162517547607
Epoch 1590, training loss: 62.3231315612793 = 0.047031745314598083 + 10.0 * 6.227610111236572
Epoch 1590, val loss: 1.5687639713287354
Epoch 1600, training loss: 62.34296417236328 = 0.046052973717451096 + 10.0 * 6.229691028594971
Epoch 1600, val loss: 1.5762187242507935
Epoch 1610, training loss: 62.33806228637695 = 0.04507800564169884 + 10.0 * 6.2292985916137695
Epoch 1610, val loss: 1.5826988220214844
Epoch 1620, training loss: 62.33177185058594 = 0.04413658753037453 + 10.0 * 6.228763580322266
Epoch 1620, val loss: 1.5901917219161987
Epoch 1630, training loss: 62.33137893676758 = 0.04322474077343941 + 10.0 * 6.22881555557251
Epoch 1630, val loss: 1.5967111587524414
Epoch 1640, training loss: 62.30915451049805 = 0.0423332042992115 + 10.0 * 6.226682186126709
Epoch 1640, val loss: 1.6042699813842773
Epoch 1650, training loss: 62.29680633544922 = 0.0414789654314518 + 10.0 * 6.2255330085754395
Epoch 1650, val loss: 1.6112101078033447
Epoch 1660, training loss: 62.310523986816406 = 0.04064696654677391 + 10.0 * 6.226987838745117
Epoch 1660, val loss: 1.6181674003601074
Epoch 1670, training loss: 62.3134765625 = 0.03983452916145325 + 10.0 * 6.227364540100098
Epoch 1670, val loss: 1.6249734163284302
Epoch 1680, training loss: 62.33086395263672 = 0.039048194885253906 + 10.0 * 6.22918176651001
Epoch 1680, val loss: 1.6321241855621338
Epoch 1690, training loss: 62.3439826965332 = 0.03826925531029701 + 10.0 * 6.230571269989014
Epoch 1690, val loss: 1.638314127922058
Epoch 1700, training loss: 62.288516998291016 = 0.03749356418848038 + 10.0 * 6.225102424621582
Epoch 1700, val loss: 1.644923448562622
Epoch 1710, training loss: 62.277034759521484 = 0.036768339574337006 + 10.0 * 6.224026679992676
Epoch 1710, val loss: 1.6516919136047363
Epoch 1720, training loss: 62.27168273925781 = 0.036060236394405365 + 10.0 * 6.223562240600586
Epoch 1720, val loss: 1.658326506614685
Epoch 1730, training loss: 62.28426742553711 = 0.03537256643176079 + 10.0 * 6.224889278411865
Epoch 1730, val loss: 1.6646714210510254
Epoch 1740, training loss: 62.309879302978516 = 0.034695420414209366 + 10.0 * 6.227518558502197
Epoch 1740, val loss: 1.6705539226531982
Epoch 1750, training loss: 62.30095291137695 = 0.03403719142079353 + 10.0 * 6.226691246032715
Epoch 1750, val loss: 1.6780600547790527
Epoch 1760, training loss: 62.268035888671875 = 0.03339501842856407 + 10.0 * 6.223464012145996
Epoch 1760, val loss: 1.6840490102767944
Epoch 1770, training loss: 62.26538848876953 = 0.0327746719121933 + 10.0 * 6.22326135635376
Epoch 1770, val loss: 1.6913138628005981
Epoch 1780, training loss: 62.32672119140625 = 0.0321793369948864 + 10.0 * 6.229454040527344
Epoch 1780, val loss: 1.697467565536499
Epoch 1790, training loss: 62.278892517089844 = 0.03157349303364754 + 10.0 * 6.224731922149658
Epoch 1790, val loss: 1.7031108140945435
Epoch 1800, training loss: 62.26687240600586 = 0.031001033261418343 + 10.0 * 6.2235870361328125
Epoch 1800, val loss: 1.7101689577102661
Epoch 1810, training loss: 62.287540435791016 = 0.0304427407681942 + 10.0 * 6.225709915161133
Epoch 1810, val loss: 1.7162115573883057
Epoch 1820, training loss: 62.2420654296875 = 0.029881754890084267 + 10.0 * 6.221218585968018
Epoch 1820, val loss: 1.7218776941299438
Epoch 1830, training loss: 62.285194396972656 = 0.029351357370615005 + 10.0 * 6.225584506988525
Epoch 1830, val loss: 1.7276448011398315
Epoch 1840, training loss: 62.24651336669922 = 0.028830548748373985 + 10.0 * 6.221768379211426
Epoch 1840, val loss: 1.734441876411438
Epoch 1850, training loss: 62.23136901855469 = 0.02831985056400299 + 10.0 * 6.2203049659729
Epoch 1850, val loss: 1.7398860454559326
Epoch 1860, training loss: 62.2454833984375 = 0.02782563306391239 + 10.0 * 6.221765995025635
Epoch 1860, val loss: 1.7456252574920654
Epoch 1870, training loss: 62.26284408569336 = 0.027347080409526825 + 10.0 * 6.223549842834473
Epoch 1870, val loss: 1.751708745956421
Epoch 1880, training loss: 62.24580001831055 = 0.026873644441366196 + 10.0 * 6.221892356872559
Epoch 1880, val loss: 1.7578051090240479
Epoch 1890, training loss: 62.24111557006836 = 0.02641146630048752 + 10.0 * 6.221470355987549
Epoch 1890, val loss: 1.7629427909851074
Epoch 1900, training loss: 62.225650787353516 = 0.02596382610499859 + 10.0 * 6.219968795776367
Epoch 1900, val loss: 1.7692595720291138
Epoch 1910, training loss: 62.214385986328125 = 0.025530992075800896 + 10.0 * 6.21888542175293
Epoch 1910, val loss: 1.7752360105514526
Epoch 1920, training loss: 62.22902297973633 = 0.0251124557107687 + 10.0 * 6.220391273498535
Epoch 1920, val loss: 1.781172513961792
Epoch 1930, training loss: 62.26530075073242 = 0.02470502257347107 + 10.0 * 6.224059581756592
Epoch 1930, val loss: 1.7864408493041992
Epoch 1940, training loss: 62.234107971191406 = 0.024278098717331886 + 10.0 * 6.220983028411865
Epoch 1940, val loss: 1.791428565979004
Epoch 1950, training loss: 62.230857849121094 = 0.023884112015366554 + 10.0 * 6.220697402954102
Epoch 1950, val loss: 1.7973828315734863
Epoch 1960, training loss: 62.22831726074219 = 0.023495901376008987 + 10.0 * 6.220482349395752
Epoch 1960, val loss: 1.8028144836425781
Epoch 1970, training loss: 62.20851135253906 = 0.023116445168852806 + 10.0 * 6.218539237976074
Epoch 1970, val loss: 1.8078025579452515
Epoch 1980, training loss: 62.21187210083008 = 0.022749315947294235 + 10.0 * 6.218912124633789
Epoch 1980, val loss: 1.8132567405700684
Epoch 1990, training loss: 62.21692657470703 = 0.022388048470020294 + 10.0 * 6.219453811645508
Epoch 1990, val loss: 1.8189659118652344
Epoch 2000, training loss: 62.207725524902344 = 0.02204161509871483 + 10.0 * 6.218568325042725
Epoch 2000, val loss: 1.824528694152832
Epoch 2010, training loss: 62.22114181518555 = 0.021698959171772003 + 10.0 * 6.219944477081299
Epoch 2010, val loss: 1.829610824584961
Epoch 2020, training loss: 62.211490631103516 = 0.02135534957051277 + 10.0 * 6.219013690948486
Epoch 2020, val loss: 1.8338544368743896
Epoch 2030, training loss: 62.23180389404297 = 0.02102694846689701 + 10.0 * 6.221077919006348
Epoch 2030, val loss: 1.8396692276000977
Epoch 2040, training loss: 62.19526672363281 = 0.02070683054625988 + 10.0 * 6.217455863952637
Epoch 2040, val loss: 1.8449751138687134
Epoch 2050, training loss: 62.19054412841797 = 0.020395109429955482 + 10.0 * 6.217015266418457
Epoch 2050, val loss: 1.850093960762024
Epoch 2060, training loss: 62.204654693603516 = 0.020090516656637192 + 10.0 * 6.218456268310547
Epoch 2060, val loss: 1.8550561666488647
Epoch 2070, training loss: 62.1762580871582 = 0.0197877399623394 + 10.0 * 6.215647220611572
Epoch 2070, val loss: 1.85994553565979
Epoch 2080, training loss: 62.18958282470703 = 0.019497253000736237 + 10.0 * 6.217008590698242
Epoch 2080, val loss: 1.8647270202636719
Epoch 2090, training loss: 62.194087982177734 = 0.019210543483495712 + 10.0 * 6.217487812042236
Epoch 2090, val loss: 1.8695560693740845
Epoch 2100, training loss: 62.20592498779297 = 0.01893002726137638 + 10.0 * 6.2186994552612305
Epoch 2100, val loss: 1.8747223615646362
Epoch 2110, training loss: 62.215606689453125 = 0.018660468980669975 + 10.0 * 6.2196946144104
Epoch 2110, val loss: 1.8798460960388184
Epoch 2120, training loss: 62.22929763793945 = 0.01839209720492363 + 10.0 * 6.221090793609619
Epoch 2120, val loss: 1.884400725364685
Epoch 2130, training loss: 62.17741775512695 = 0.018114468082785606 + 10.0 * 6.215929985046387
Epoch 2130, val loss: 1.8884756565093994
Epoch 2140, training loss: 62.161781311035156 = 0.017861394211649895 + 10.0 * 6.214392185211182
Epoch 2140, val loss: 1.8937908411026
Epoch 2150, training loss: 62.15538787841797 = 0.017613159492611885 + 10.0 * 6.213777542114258
Epoch 2150, val loss: 1.8984689712524414
Epoch 2160, training loss: 62.16053009033203 = 0.0173732228577137 + 10.0 * 6.214315891265869
Epoch 2160, val loss: 1.9032936096191406
Epoch 2170, training loss: 62.22084045410156 = 0.01714264042675495 + 10.0 * 6.220369815826416
Epoch 2170, val loss: 1.9078845977783203
Epoch 2180, training loss: 62.17779541015625 = 0.016897570341825485 + 10.0 * 6.216089725494385
Epoch 2180, val loss: 1.9122920036315918
Epoch 2190, training loss: 62.17295455932617 = 0.016663286834955215 + 10.0 * 6.2156291007995605
Epoch 2190, val loss: 1.9162895679473877
Epoch 2200, training loss: 62.15478515625 = 0.016433563083410263 + 10.0 * 6.2138352394104
Epoch 2200, val loss: 1.9208778142929077
Epoch 2210, training loss: 62.15488815307617 = 0.0162135548889637 + 10.0 * 6.213867664337158
Epoch 2210, val loss: 1.925374984741211
Epoch 2220, training loss: 62.18278503417969 = 0.01600036583840847 + 10.0 * 6.216678619384766
Epoch 2220, val loss: 1.9300824403762817
Epoch 2230, training loss: 62.161598205566406 = 0.01579180732369423 + 10.0 * 6.214580535888672
Epoch 2230, val loss: 1.9348151683807373
Epoch 2240, training loss: 62.14710998535156 = 0.01558071281760931 + 10.0 * 6.213152885437012
Epoch 2240, val loss: 1.9391260147094727
Epoch 2250, training loss: 62.142608642578125 = 0.015376114286482334 + 10.0 * 6.212723255157471
Epoch 2250, val loss: 1.9428387880325317
Epoch 2260, training loss: 62.14019012451172 = 0.015179609879851341 + 10.0 * 6.212501049041748
Epoch 2260, val loss: 1.9475075006484985
Epoch 2270, training loss: 62.179134368896484 = 0.01498816441744566 + 10.0 * 6.216414451599121
Epoch 2270, val loss: 1.951791524887085
Epoch 2280, training loss: 62.186988830566406 = 0.014795131981372833 + 10.0 * 6.217219352722168
Epoch 2280, val loss: 1.9562004804611206
Epoch 2290, training loss: 62.143821716308594 = 0.014601748436689377 + 10.0 * 6.212922096252441
Epoch 2290, val loss: 1.959742784500122
Epoch 2300, training loss: 62.148189544677734 = 0.01441534049808979 + 10.0 * 6.213377475738525
Epoch 2300, val loss: 1.9636298418045044
Epoch 2310, training loss: 62.19921875 = 0.014234103262424469 + 10.0 * 6.218498706817627
Epoch 2310, val loss: 1.9674493074417114
Epoch 2320, training loss: 62.14576721191406 = 0.014061056077480316 + 10.0 * 6.213170528411865
Epoch 2320, val loss: 1.9724547863006592
Epoch 2330, training loss: 62.123558044433594 = 0.01388198509812355 + 10.0 * 6.210967540740967
Epoch 2330, val loss: 1.9761887788772583
Epoch 2340, training loss: 62.12165832519531 = 0.013715356588363647 + 10.0 * 6.210794448852539
Epoch 2340, val loss: 1.9801524877548218
Epoch 2350, training loss: 62.12643051147461 = 0.013551361858844757 + 10.0 * 6.211287975311279
Epoch 2350, val loss: 1.9842113256454468
Epoch 2360, training loss: 62.19749069213867 = 0.013391843996942043 + 10.0 * 6.218409538269043
Epoch 2360, val loss: 1.9880269765853882
Epoch 2370, training loss: 62.153900146484375 = 0.013230612501502037 + 10.0 * 6.214066982269287
Epoch 2370, val loss: 1.9918160438537598
Epoch 2380, training loss: 62.145633697509766 = 0.013070957735180855 + 10.0 * 6.213256359100342
Epoch 2380, val loss: 1.995863676071167
Epoch 2390, training loss: 62.13886642456055 = 0.012916230596601963 + 10.0 * 6.212594985961914
Epoch 2390, val loss: 1.9996618032455444
Epoch 2400, training loss: 62.119102478027344 = 0.012760882265865803 + 10.0 * 6.210634231567383
Epoch 2400, val loss: 2.0035831928253174
Epoch 2410, training loss: 62.114688873291016 = 0.012615126557648182 + 10.0 * 6.210207462310791
Epoch 2410, val loss: 2.0074501037597656
Epoch 2420, training loss: 62.14201736450195 = 0.012476262636482716 + 10.0 * 6.212954044342041
Epoch 2420, val loss: 2.0118443965911865
Epoch 2430, training loss: 62.16513442993164 = 0.012330976314842701 + 10.0 * 6.215280532836914
Epoch 2430, val loss: 2.014491081237793
Epoch 2440, training loss: 62.117530822753906 = 0.012178786098957062 + 10.0 * 6.210535049438477
Epoch 2440, val loss: 2.018117666244507
Epoch 2450, training loss: 62.1058235168457 = 0.01204221136868 + 10.0 * 6.209378242492676
Epoch 2450, val loss: 2.02166485786438
Epoch 2460, training loss: 62.100120544433594 = 0.011908191256225109 + 10.0 * 6.2088212966918945
Epoch 2460, val loss: 2.0257744789123535
Epoch 2470, training loss: 62.10599136352539 = 0.011778370477259159 + 10.0 * 6.209421157836914
Epoch 2470, val loss: 2.029130220413208
Epoch 2480, training loss: 62.1638298034668 = 0.011649606749415398 + 10.0 * 6.2152180671691895
Epoch 2480, val loss: 2.0325193405151367
Epoch 2490, training loss: 62.12517547607422 = 0.011520706117153168 + 10.0 * 6.211365699768066
Epoch 2490, val loss: 2.0356340408325195
Epoch 2500, training loss: 62.129886627197266 = 0.011392952874302864 + 10.0 * 6.211849212646484
Epoch 2500, val loss: 2.039216995239258
Epoch 2510, training loss: 62.101070404052734 = 0.011267907917499542 + 10.0 * 6.208980083465576
Epoch 2510, val loss: 2.0431089401245117
Epoch 2520, training loss: 62.11519241333008 = 0.01115098875015974 + 10.0 * 6.210404396057129
Epoch 2520, val loss: 2.046783924102783
Epoch 2530, training loss: 62.11796188354492 = 0.011031610891222954 + 10.0 * 6.210692882537842
Epoch 2530, val loss: 2.050297498703003
Epoch 2540, training loss: 62.11692810058594 = 0.010915598832070827 + 10.0 * 6.210601329803467
Epoch 2540, val loss: 2.0535268783569336
Epoch 2550, training loss: 62.13459396362305 = 0.010802553966641426 + 10.0 * 6.212378978729248
Epoch 2550, val loss: 2.057224988937378
Epoch 2560, training loss: 62.08888244628906 = 0.010682585649192333 + 10.0 * 6.207819938659668
Epoch 2560, val loss: 2.059678316116333
Epoch 2570, training loss: 62.08921432495117 = 0.010571659542620182 + 10.0 * 6.207864284515381
Epoch 2570, val loss: 2.063203811645508
Epoch 2580, training loss: 62.08961868286133 = 0.010465613566339016 + 10.0 * 6.207915306091309
Epoch 2580, val loss: 2.066466808319092
Epoch 2590, training loss: 62.12629318237305 = 0.01036012452095747 + 10.0 * 6.211593151092529
Epoch 2590, val loss: 2.0692858695983887
Epoch 2600, training loss: 62.13944625854492 = 0.010254687629640102 + 10.0 * 6.212919235229492
Epoch 2600, val loss: 2.0729458332061768
Epoch 2610, training loss: 62.099342346191406 = 0.01014832966029644 + 10.0 * 6.208919525146484
Epoch 2610, val loss: 2.075890302658081
Epoch 2620, training loss: 62.082706451416016 = 0.01004263386130333 + 10.0 * 6.207266330718994
Epoch 2620, val loss: 2.079115390777588
Epoch 2630, training loss: 62.08073425292969 = 0.009945577941834927 + 10.0 * 6.20707893371582
Epoch 2630, val loss: 2.082409381866455
Epoch 2640, training loss: 62.12860107421875 = 0.009851391427218914 + 10.0 * 6.211874961853027
Epoch 2640, val loss: 2.0857186317443848
Epoch 2650, training loss: 62.08268737792969 = 0.00974902231246233 + 10.0 * 6.207293510437012
Epoch 2650, val loss: 2.0886142253875732
Epoch 2660, training loss: 62.085838317871094 = 0.009654873982071877 + 10.0 * 6.207618236541748
Epoch 2660, val loss: 2.0915637016296387
Epoch 2670, training loss: 62.12181091308594 = 0.00955916941165924 + 10.0 * 6.2112250328063965
Epoch 2670, val loss: 2.0940258502960205
Epoch 2680, training loss: 62.120460510253906 = 0.009465809911489487 + 10.0 * 6.211099624633789
Epoch 2680, val loss: 2.096683979034424
Epoch 2690, training loss: 62.080909729003906 = 0.009373903274536133 + 10.0 * 6.207153797149658
Epoch 2690, val loss: 2.100665330886841
Epoch 2700, training loss: 62.06851577758789 = 0.009283795952796936 + 10.0 * 6.205923080444336
Epoch 2700, val loss: 2.102968454360962
Epoch 2710, training loss: 62.064205169677734 = 0.009197461418807507 + 10.0 * 6.205500602722168
Epoch 2710, val loss: 2.1063385009765625
Epoch 2720, training loss: 62.068931579589844 = 0.009113769978284836 + 10.0 * 6.205981731414795
Epoch 2720, val loss: 2.109307289123535
Epoch 2730, training loss: 62.128623962402344 = 0.009035085327923298 + 10.0 * 6.211958885192871
Epoch 2730, val loss: 2.112323760986328
Epoch 2740, training loss: 62.0857048034668 = 0.008947161957621574 + 10.0 * 6.207675933837891
Epoch 2740, val loss: 2.11452054977417
Epoch 2750, training loss: 62.066410064697266 = 0.008861122652888298 + 10.0 * 6.20575475692749
Epoch 2750, val loss: 2.1173882484436035
Epoch 2760, training loss: 62.06932067871094 = 0.008780937641859055 + 10.0 * 6.206053733825684
Epoch 2760, val loss: 2.1202430725097656
Epoch 2770, training loss: 62.07804489135742 = 0.008703074418008327 + 10.0 * 6.206934452056885
Epoch 2770, val loss: 2.1231579780578613
Epoch 2780, training loss: 62.07426452636719 = 0.008624442853033543 + 10.0 * 6.206563949584961
Epoch 2780, val loss: 2.1260311603546143
Epoch 2790, training loss: 62.08068084716797 = 0.008545979857444763 + 10.0 * 6.207213401794434
Epoch 2790, val loss: 2.128525972366333
Epoch 2800, training loss: 62.070068359375 = 0.008467943407595158 + 10.0 * 6.206160068511963
Epoch 2800, val loss: 2.131073474884033
Epoch 2810, training loss: 62.0696907043457 = 0.008394939824938774 + 10.0 * 6.206129550933838
Epoch 2810, val loss: 2.133363723754883
Epoch 2820, training loss: 62.0921745300293 = 0.008325089700520039 + 10.0 * 6.208384990692139
Epoch 2820, val loss: 2.136873483657837
Epoch 2830, training loss: 62.08185958862305 = 0.008247214369475842 + 10.0 * 6.207361221313477
Epoch 2830, val loss: 2.1382012367248535
Epoch 2840, training loss: 62.05522918701172 = 0.008172662928700447 + 10.0 * 6.204705715179443
Epoch 2840, val loss: 2.140928268432617
Epoch 2850, training loss: 62.047725677490234 = 0.00810328684747219 + 10.0 * 6.203962326049805
Epoch 2850, val loss: 2.1437923908233643
Epoch 2860, training loss: 62.043060302734375 = 0.008034654892981052 + 10.0 * 6.203502655029297
Epoch 2860, val loss: 2.1461029052734375
Epoch 2870, training loss: 62.06796646118164 = 0.00796815287321806 + 10.0 * 6.205999851226807
Epoch 2870, val loss: 2.148423671722412
Epoch 2880, training loss: 62.079322814941406 = 0.00790046900510788 + 10.0 * 6.207142353057861
Epoch 2880, val loss: 2.150376319885254
Epoch 2890, training loss: 62.059837341308594 = 0.007833252660930157 + 10.0 * 6.205200672149658
Epoch 2890, val loss: 2.1536073684692383
Epoch 2900, training loss: 62.04545593261719 = 0.007766233757138252 + 10.0 * 6.203768730163574
Epoch 2900, val loss: 2.1555542945861816
Epoch 2910, training loss: 62.048988342285156 = 0.007704274728894234 + 10.0 * 6.204128265380859
Epoch 2910, val loss: 2.158282518386841
Epoch 2920, training loss: 62.103759765625 = 0.0076422118581831455 + 10.0 * 6.209611892700195
Epoch 2920, val loss: 2.160243034362793
Epoch 2930, training loss: 62.05763626098633 = 0.007579805329442024 + 10.0 * 6.205005645751953
Epoch 2930, val loss: 2.162757635116577
Epoch 2940, training loss: 62.06489181518555 = 0.00751858064904809 + 10.0 * 6.205737113952637
Epoch 2940, val loss: 2.1654891967773438
Epoch 2950, training loss: 62.073116302490234 = 0.0074596949853003025 + 10.0 * 6.2065653800964355
Epoch 2950, val loss: 2.1671862602233887
Epoch 2960, training loss: 62.05324935913086 = 0.007394655607640743 + 10.0 * 6.204585552215576
Epoch 2960, val loss: 2.169013738632202
Epoch 2970, training loss: 62.058204650878906 = 0.007338110823184252 + 10.0 * 6.205086708068848
Epoch 2970, val loss: 2.1716387271881104
Epoch 2980, training loss: 62.03514099121094 = 0.007279552053660154 + 10.0 * 6.202786445617676
Epoch 2980, val loss: 2.1738858222961426
Epoch 2990, training loss: 62.050777435302734 = 0.0072244517505168915 + 10.0 * 6.204355239868164
Epoch 2990, val loss: 2.176603317260742
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8039008961518187
The final CL Acc:0.70494, 0.02310, The final GNN Acc:0.80056, 0.00259
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13222])
remove edge: torch.Size([2, 7942])
updated graph: torch.Size([2, 10608])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.91799926757812 = 1.9493130445480347 + 10.0 * 8.596868515014648
Epoch 0, val loss: 1.9459609985351562
Epoch 10, training loss: 87.90306854248047 = 1.938896894454956 + 10.0 * 8.596417427062988
Epoch 10, val loss: 1.9353739023208618
Epoch 20, training loss: 87.85346984863281 = 1.9261817932128906 + 10.0 * 8.592728614807129
Epoch 20, val loss: 1.9224315881729126
Epoch 30, training loss: 87.57102966308594 = 1.9097814559936523 + 10.0 * 8.56612491607666
Epoch 30, val loss: 1.905613660812378
Epoch 40, training loss: 86.08160400390625 = 1.8894535303115845 + 10.0 * 8.419215202331543
Epoch 40, val loss: 1.8850221633911133
Epoch 50, training loss: 80.25216674804688 = 1.8668941259384155 + 10.0 * 7.838526725769043
Epoch 50, val loss: 1.8624670505523682
Epoch 60, training loss: 76.94432830810547 = 1.848745346069336 + 10.0 * 7.509557723999023
Epoch 60, val loss: 1.8458096981048584
Epoch 70, training loss: 74.7060546875 = 1.834662675857544 + 10.0 * 7.287139415740967
Epoch 70, val loss: 1.8324215412139893
Epoch 80, training loss: 72.60337829589844 = 1.8224660158157349 + 10.0 * 7.078091144561768
Epoch 80, val loss: 1.821131706237793
Epoch 90, training loss: 71.08951568603516 = 1.8120291233062744 + 10.0 * 6.927748680114746
Epoch 90, val loss: 1.8116563558578491
Epoch 100, training loss: 69.85281372070312 = 1.8022422790527344 + 10.0 * 6.805056571960449
Epoch 100, val loss: 1.8030444383621216
Epoch 110, training loss: 69.08515167236328 = 1.7912957668304443 + 10.0 * 6.7293853759765625
Epoch 110, val loss: 1.793549656867981
Epoch 120, training loss: 68.5240249633789 = 1.7791792154312134 + 10.0 * 6.6744842529296875
Epoch 120, val loss: 1.7826746702194214
Epoch 130, training loss: 68.03311920166016 = 1.7671027183532715 + 10.0 * 6.626601219177246
Epoch 130, val loss: 1.7718161344528198
Epoch 140, training loss: 67.639892578125 = 1.7547953128814697 + 10.0 * 6.5885090827941895
Epoch 140, val loss: 1.7606346607208252
Epoch 150, training loss: 67.32220458984375 = 1.7412171363830566 + 10.0 * 6.558098316192627
Epoch 150, val loss: 1.74821937084198
Epoch 160, training loss: 67.0485610961914 = 1.7261377573013306 + 10.0 * 6.532242298126221
Epoch 160, val loss: 1.7345459461212158
Epoch 170, training loss: 66.8115234375 = 1.7096467018127441 + 10.0 * 6.51018762588501
Epoch 170, val loss: 1.7196636199951172
Epoch 180, training loss: 66.60619354248047 = 1.6916476488113403 + 10.0 * 6.491454601287842
Epoch 180, val loss: 1.7034679651260376
Epoch 190, training loss: 66.50760650634766 = 1.67173171043396 + 10.0 * 6.483587265014648
Epoch 190, val loss: 1.6856005191802979
Epoch 200, training loss: 66.312744140625 = 1.6501003503799438 + 10.0 * 6.466264247894287
Epoch 200, val loss: 1.6661640405654907
Epoch 210, training loss: 66.15614318847656 = 1.6267560720443726 + 10.0 * 6.452938556671143
Epoch 210, val loss: 1.6454813480377197
Epoch 220, training loss: 66.02510833740234 = 1.6016677618026733 + 10.0 * 6.442343711853027
Epoch 220, val loss: 1.6232186555862427
Epoch 230, training loss: 65.97459411621094 = 1.575034260749817 + 10.0 * 6.439955711364746
Epoch 230, val loss: 1.5998681783676147
Epoch 240, training loss: 65.79318237304688 = 1.5467967987060547 + 10.0 * 6.424638748168945
Epoch 240, val loss: 1.575421929359436
Epoch 250, training loss: 65.66337585449219 = 1.5174739360809326 + 10.0 * 6.414590358734131
Epoch 250, val loss: 1.5503660440444946
Epoch 260, training loss: 65.55033874511719 = 1.4872329235076904 + 10.0 * 6.406310558319092
Epoch 260, val loss: 1.524813175201416
Epoch 270, training loss: 65.46527099609375 = 1.4562993049621582 + 10.0 * 6.400897026062012
Epoch 270, val loss: 1.4991117715835571
Epoch 280, training loss: 65.37445068359375 = 1.4248073101043701 + 10.0 * 6.394964694976807
Epoch 280, val loss: 1.473435640335083
Epoch 290, training loss: 65.25597381591797 = 1.3935192823410034 + 10.0 * 6.386245250701904
Epoch 290, val loss: 1.4483429193496704
Epoch 300, training loss: 65.16185760498047 = 1.3622342348098755 + 10.0 * 6.379961967468262
Epoch 300, val loss: 1.423699975013733
Epoch 310, training loss: 65.15068054199219 = 1.3311820030212402 + 10.0 * 6.3819499015808105
Epoch 310, val loss: 1.3995840549468994
Epoch 320, training loss: 64.99887084960938 = 1.2999639511108398 + 10.0 * 6.3698906898498535
Epoch 320, val loss: 1.3757753372192383
Epoch 330, training loss: 64.91899871826172 = 1.2691932916641235 + 10.0 * 6.364980220794678
Epoch 330, val loss: 1.3524343967437744
Epoch 340, training loss: 64.83963012695312 = 1.238346815109253 + 10.0 * 6.360127925872803
Epoch 340, val loss: 1.3293076753616333
Epoch 350, training loss: 64.77024841308594 = 1.2074263095855713 + 10.0 * 6.3562822341918945
Epoch 350, val loss: 1.306488275527954
Epoch 360, training loss: 64.68861389160156 = 1.1768032312393188 + 10.0 * 6.3511810302734375
Epoch 360, val loss: 1.283948302268982
Epoch 370, training loss: 64.61368560791016 = 1.146181344985962 + 10.0 * 6.346750736236572
Epoch 370, val loss: 1.2615653276443481
Epoch 380, training loss: 64.542724609375 = 1.1154963970184326 + 10.0 * 6.3427228927612305
Epoch 380, val loss: 1.2393269538879395
Epoch 390, training loss: 64.50873565673828 = 1.0846412181854248 + 10.0 * 6.342409610748291
Epoch 390, val loss: 1.217097282409668
Epoch 400, training loss: 64.41386413574219 = 1.054138422012329 + 10.0 * 6.335972785949707
Epoch 400, val loss: 1.195176362991333
Epoch 410, training loss: 64.3412857055664 = 1.023952841758728 + 10.0 * 6.331732749938965
Epoch 410, val loss: 1.173615574836731
Epoch 420, training loss: 64.27576446533203 = 0.9939759373664856 + 10.0 * 6.328178882598877
Epoch 420, val loss: 1.152475118637085
Epoch 430, training loss: 64.28838348388672 = 0.9643365144729614 + 10.0 * 6.332404136657715
Epoch 430, val loss: 1.1318061351776123
Epoch 440, training loss: 64.16272735595703 = 0.9354283213615417 + 10.0 * 6.32273006439209
Epoch 440, val loss: 1.1118108034133911
Epoch 450, training loss: 64.09922790527344 = 0.9072445034980774 + 10.0 * 6.3191986083984375
Epoch 450, val loss: 1.0926625728607178
Epoch 460, training loss: 64.04011535644531 = 0.8797474503517151 + 10.0 * 6.316036701202393
Epoch 460, val loss: 1.074433445930481
Epoch 470, training loss: 64.00192260742188 = 0.8530442714691162 + 10.0 * 6.314888000488281
Epoch 470, val loss: 1.0570987462997437
Epoch 480, training loss: 63.95566940307617 = 0.8274180889129639 + 10.0 * 6.3128252029418945
Epoch 480, val loss: 1.0408399105072021
Epoch 490, training loss: 63.890933990478516 = 0.8026710748672485 + 10.0 * 6.308826446533203
Epoch 490, val loss: 1.0257004499435425
Epoch 500, training loss: 63.836936950683594 = 0.7788623571395874 + 10.0 * 6.305807590484619
Epoch 500, val loss: 1.0116653442382812
Epoch 510, training loss: 63.93421173095703 = 0.755800187587738 + 10.0 * 6.317841529846191
Epoch 510, val loss: 0.9985976815223694
Epoch 520, training loss: 63.771873474121094 = 0.733871340751648 + 10.0 * 6.303800106048584
Epoch 520, val loss: 0.9864307641983032
Epoch 530, training loss: 63.71598815917969 = 0.7129145860671997 + 10.0 * 6.300307273864746
Epoch 530, val loss: 0.9753292798995972
Epoch 540, training loss: 63.67741775512695 = 0.6926817297935486 + 10.0 * 6.298473834991455
Epoch 540, val loss: 0.9652819633483887
Epoch 550, training loss: 63.634552001953125 = 0.673145592212677 + 10.0 * 6.296140670776367
Epoch 550, val loss: 0.9560726881027222
Epoch 560, training loss: 63.59677505493164 = 0.6542651653289795 + 10.0 * 6.294250965118408
Epoch 560, val loss: 0.9476600885391235
Epoch 570, training loss: 63.64975357055664 = 0.636030912399292 + 10.0 * 6.301372051239014
Epoch 570, val loss: 0.9399374127388
Epoch 580, training loss: 63.553985595703125 = 0.6181353330612183 + 10.0 * 6.293585300445557
Epoch 580, val loss: 0.9328067302703857
Epoch 590, training loss: 63.51103973388672 = 0.6010438799858093 + 10.0 * 6.290999412536621
Epoch 590, val loss: 0.926348865032196
Epoch 600, training loss: 63.46597671508789 = 0.5844346284866333 + 10.0 * 6.288154125213623
Epoch 600, val loss: 0.9205165505409241
Epoch 610, training loss: 63.43281555175781 = 0.5682864189147949 + 10.0 * 6.286452770233154
Epoch 610, val loss: 0.915273129940033
Epoch 620, training loss: 63.57762145996094 = 0.5524728894233704 + 10.0 * 6.302515029907227
Epoch 620, val loss: 0.9104838371276855
Epoch 630, training loss: 63.391944885253906 = 0.5371690392494202 + 10.0 * 6.285477638244629
Epoch 630, val loss: 0.9060506224632263
Epoch 640, training loss: 63.35966491699219 = 0.5223129391670227 + 10.0 * 6.283735275268555
Epoch 640, val loss: 0.9022171497344971
Epoch 650, training loss: 63.32289123535156 = 0.5078408718109131 + 10.0 * 6.281505107879639
Epoch 650, val loss: 0.8989208936691284
Epoch 660, training loss: 63.29292297363281 = 0.4937524199485779 + 10.0 * 6.279917240142822
Epoch 660, val loss: 0.8960722088813782
Epoch 670, training loss: 63.315223693847656 = 0.48006463050842285 + 10.0 * 6.283515930175781
Epoch 670, val loss: 0.8936948776245117
Epoch 680, training loss: 63.274932861328125 = 0.4665139317512512 + 10.0 * 6.280841827392578
Epoch 680, val loss: 0.8915748000144958
Epoch 690, training loss: 63.22929000854492 = 0.4534957706928253 + 10.0 * 6.277579307556152
Epoch 690, val loss: 0.889870822429657
Epoch 700, training loss: 63.194236755371094 = 0.44072091579437256 + 10.0 * 6.275351524353027
Epoch 700, val loss: 0.8887055516242981
Epoch 710, training loss: 63.1917839050293 = 0.4283037483692169 + 10.0 * 6.276348114013672
Epoch 710, val loss: 0.8879590034484863
Epoch 720, training loss: 63.16444778442383 = 0.4161525070667267 + 10.0 * 6.274829387664795
Epoch 720, val loss: 0.887499988079071
Epoch 730, training loss: 63.13350296020508 = 0.4042847156524658 + 10.0 * 6.272921562194824
Epoch 730, val loss: 0.8873914480209351
Epoch 740, training loss: 63.11101150512695 = 0.39271435141563416 + 10.0 * 6.271829605102539
Epoch 740, val loss: 0.8876820206642151
Epoch 750, training loss: 63.12557601928711 = 0.3814025819301605 + 10.0 * 6.274417400360107
Epoch 750, val loss: 0.8883193135261536
Epoch 760, training loss: 63.079315185546875 = 0.3704812228679657 + 10.0 * 6.270883560180664
Epoch 760, val loss: 0.8892545700073242
Epoch 770, training loss: 63.0460205078125 = 0.35967057943344116 + 10.0 * 6.268635272979736
Epoch 770, val loss: 0.8906108736991882
Epoch 780, training loss: 63.052268981933594 = 0.34919559955596924 + 10.0 * 6.2703070640563965
Epoch 780, val loss: 0.8923539519309998
Epoch 790, training loss: 63.010032653808594 = 0.3390340507030487 + 10.0 * 6.267099857330322
Epoch 790, val loss: 0.8943835496902466
Epoch 800, training loss: 62.979454040527344 = 0.32903966307640076 + 10.0 * 6.265041351318359
Epoch 800, val loss: 0.896873414516449
Epoch 810, training loss: 63.0046501159668 = 0.3193706274032593 + 10.0 * 6.268527984619141
Epoch 810, val loss: 0.899681806564331
Epoch 820, training loss: 62.96831512451172 = 0.3098520040512085 + 10.0 * 6.265846252441406
Epoch 820, val loss: 0.9027353525161743
Epoch 830, training loss: 62.93108367919922 = 0.3006070852279663 + 10.0 * 6.263047695159912
Epoch 830, val loss: 0.9061007499694824
Epoch 840, training loss: 62.91342544555664 = 0.29162225127220154 + 10.0 * 6.262180328369141
Epoch 840, val loss: 0.909692645072937
Epoch 850, training loss: 62.89036560058594 = 0.2828722596168518 + 10.0 * 6.260749340057373
Epoch 850, val loss: 0.9135696291923523
Epoch 860, training loss: 62.88679122924805 = 0.27435216307640076 + 10.0 * 6.26124382019043
Epoch 860, val loss: 0.9177131652832031
Epoch 870, training loss: 62.86214828491211 = 0.2660738527774811 + 10.0 * 6.259607315063477
Epoch 870, val loss: 0.9220858812332153
Epoch 880, training loss: 62.880435943603516 = 0.25800850987434387 + 10.0 * 6.262242794036865
Epoch 880, val loss: 0.9266689419746399
Epoch 890, training loss: 62.825477600097656 = 0.2501390874385834 + 10.0 * 6.257534027099609
Epoch 890, val loss: 0.9314423203468323
Epoch 900, training loss: 62.80384063720703 = 0.24249394237995148 + 10.0 * 6.256134986877441
Epoch 900, val loss: 0.9365211129188538
Epoch 910, training loss: 62.781280517578125 = 0.23512400686740875 + 10.0 * 6.254615783691406
Epoch 910, val loss: 0.9416869878768921
Epoch 920, training loss: 62.7657356262207 = 0.22793656587600708 + 10.0 * 6.253779888153076
Epoch 920, val loss: 0.9470818042755127
Epoch 930, training loss: 62.84492111206055 = 0.2209840714931488 + 10.0 * 6.262393474578857
Epoch 930, val loss: 0.9525615572929382
Epoch 940, training loss: 62.7598876953125 = 0.21410933136940002 + 10.0 * 6.254578113555908
Epoch 940, val loss: 0.9580445289611816
Epoch 950, training loss: 62.72465896606445 = 0.20750682055950165 + 10.0 * 6.251715183258057
Epoch 950, val loss: 0.9637596011161804
Epoch 960, training loss: 62.790748596191406 = 0.20112422108650208 + 10.0 * 6.258962154388428
Epoch 960, val loss: 0.969680905342102
Epoch 970, training loss: 62.728858947753906 = 0.19490432739257812 + 10.0 * 6.2533955574035645
Epoch 970, val loss: 0.975470781326294
Epoch 980, training loss: 62.68950271606445 = 0.18889616429805756 + 10.0 * 6.250060558319092
Epoch 980, val loss: 0.9816039800643921
Epoch 990, training loss: 62.67090606689453 = 0.18308906257152557 + 10.0 * 6.248781681060791
Epoch 990, val loss: 0.987815260887146
Epoch 1000, training loss: 62.72438430786133 = 0.17748066782951355 + 10.0 * 6.254690647125244
Epoch 1000, val loss: 0.9940658211708069
Epoch 1010, training loss: 62.76437759399414 = 0.1720322072505951 + 10.0 * 6.259234428405762
Epoch 1010, val loss: 1.0003774166107178
Epoch 1020, training loss: 62.64739990234375 = 0.16667808592319489 + 10.0 * 6.248072147369385
Epoch 1020, val loss: 1.0066338777542114
Epoch 1030, training loss: 62.622825622558594 = 0.1615886688232422 + 10.0 * 6.246123790740967
Epoch 1030, val loss: 1.0131123065948486
Epoch 1040, training loss: 62.614768981933594 = 0.1566721498966217 + 10.0 * 6.245809555053711
Epoch 1040, val loss: 1.0196925401687622
Epoch 1050, training loss: 62.59755325317383 = 0.15189506113529205 + 10.0 * 6.244565963745117
Epoch 1050, val loss: 1.0263928174972534
Epoch 1060, training loss: 62.600154876708984 = 0.14729681611061096 + 10.0 * 6.245285987854004
Epoch 1060, val loss: 1.0330307483673096
Epoch 1070, training loss: 62.621498107910156 = 0.14282001554965973 + 10.0 * 6.247868061065674
Epoch 1070, val loss: 1.0397199392318726
Epoch 1080, training loss: 62.569480895996094 = 0.13847306370735168 + 10.0 * 6.243100643157959
Epoch 1080, val loss: 1.0463221073150635
Epoch 1090, training loss: 62.581214904785156 = 0.13428613543510437 + 10.0 * 6.244692802429199
Epoch 1090, val loss: 1.0532433986663818
Epoch 1100, training loss: 62.59807205200195 = 0.13027672469615936 + 10.0 * 6.246779441833496
Epoch 1100, val loss: 1.0600098371505737
Epoch 1110, training loss: 62.55491256713867 = 0.12634655833244324 + 10.0 * 6.242856502532959
Epoch 1110, val loss: 1.0667411088943481
Epoch 1120, training loss: 62.52922439575195 = 0.12258604913949966 + 10.0 * 6.240664005279541
Epoch 1120, val loss: 1.0737117528915405
Epoch 1130, training loss: 62.53529357910156 = 0.11897287517786026 + 10.0 * 6.241631984710693
Epoch 1130, val loss: 1.0806446075439453
Epoch 1140, training loss: 62.59583282470703 = 0.11544806510210037 + 10.0 * 6.248038291931152
Epoch 1140, val loss: 1.0874522924423218
Epoch 1150, training loss: 62.5163459777832 = 0.11201514303684235 + 10.0 * 6.240433216094971
Epoch 1150, val loss: 1.0943589210510254
Epoch 1160, training loss: 62.5008659362793 = 0.1087392270565033 + 10.0 * 6.239212989807129
Epoch 1160, val loss: 1.1012803316116333
Epoch 1170, training loss: 62.488765716552734 = 0.10557015985250473 + 10.0 * 6.238319396972656
Epoch 1170, val loss: 1.1082749366760254
Epoch 1180, training loss: 62.49765396118164 = 0.10252244025468826 + 10.0 * 6.239512920379639
Epoch 1180, val loss: 1.1152596473693848
Epoch 1190, training loss: 62.50180435180664 = 0.09955882281064987 + 10.0 * 6.240224361419678
Epoch 1190, val loss: 1.1220858097076416
Epoch 1200, training loss: 62.48624801635742 = 0.0966804251074791 + 10.0 * 6.238956928253174
Epoch 1200, val loss: 1.1289949417114258
Epoch 1210, training loss: 62.45982360839844 = 0.09391772747039795 + 10.0 * 6.236590385437012
Epoch 1210, val loss: 1.1357606649398804
Epoch 1220, training loss: 62.448097229003906 = 0.09126336127519608 + 10.0 * 6.235683441162109
Epoch 1220, val loss: 1.1425912380218506
Epoch 1230, training loss: 62.45695495605469 = 0.08870039880275726 + 10.0 * 6.236825466156006
Epoch 1230, val loss: 1.149371862411499
Epoch 1240, training loss: 62.485198974609375 = 0.08621499687433243 + 10.0 * 6.239898204803467
Epoch 1240, val loss: 1.1562145948410034
Epoch 1250, training loss: 62.5047492980957 = 0.08379653841257095 + 10.0 * 6.242094993591309
Epoch 1250, val loss: 1.1628506183624268
Epoch 1260, training loss: 62.444923400878906 = 0.08144354820251465 + 10.0 * 6.2363481521606445
Epoch 1260, val loss: 1.1695317029953003
Epoch 1270, training loss: 62.41694259643555 = 0.07920254021883011 + 10.0 * 6.233774185180664
Epoch 1270, val loss: 1.1762157678604126
Epoch 1280, training loss: 62.409027099609375 = 0.07704923301935196 + 10.0 * 6.2331976890563965
Epoch 1280, val loss: 1.1828709840774536
Epoch 1290, training loss: 62.40283966064453 = 0.0749591588973999 + 10.0 * 6.2327880859375
Epoch 1290, val loss: 1.189563274383545
Epoch 1300, training loss: 62.42387771606445 = 0.07294957339763641 + 10.0 * 6.235093116760254
Epoch 1300, val loss: 1.1961174011230469
Epoch 1310, training loss: 62.428279876708984 = 0.07100296765565872 + 10.0 * 6.235727787017822
Epoch 1310, val loss: 1.2028276920318604
Epoch 1320, training loss: 62.3828010559082 = 0.0690883994102478 + 10.0 * 6.2313714027404785
Epoch 1320, val loss: 1.2090562582015991
Epoch 1330, training loss: 62.3848762512207 = 0.06725515425205231 + 10.0 * 6.231761932373047
Epoch 1330, val loss: 1.2156445980072021
Epoch 1340, training loss: 62.3696403503418 = 0.06549502164125443 + 10.0 * 6.230414390563965
Epoch 1340, val loss: 1.2220884561538696
Epoch 1350, training loss: 62.36343002319336 = 0.06380149722099304 + 10.0 * 6.2299628257751465
Epoch 1350, val loss: 1.2286195755004883
Epoch 1360, training loss: 62.456298828125 = 0.062171824276447296 + 10.0 * 6.239412784576416
Epoch 1360, val loss: 1.2351330518722534
Epoch 1370, training loss: 62.407737731933594 = 0.06053916737437248 + 10.0 * 6.234719753265381
Epoch 1370, val loss: 1.241037130355835
Epoch 1380, training loss: 62.41353225708008 = 0.058989040553569794 + 10.0 * 6.235454082489014
Epoch 1380, val loss: 1.247621774673462
Epoch 1390, training loss: 62.339744567871094 = 0.05750152841210365 + 10.0 * 6.228224277496338
Epoch 1390, val loss: 1.253639578819275
Epoch 1400, training loss: 62.34617233276367 = 0.05606066808104515 + 10.0 * 6.229011058807373
Epoch 1400, val loss: 1.2599560022354126
Epoch 1410, training loss: 62.33903884887695 = 0.05466898903250694 + 10.0 * 6.2284369468688965
Epoch 1410, val loss: 1.2662478685379028
Epoch 1420, training loss: 62.35398864746094 = 0.053321968764066696 + 10.0 * 6.230066776275635
Epoch 1420, val loss: 1.272462010383606
Epoch 1430, training loss: 62.35456085205078 = 0.05202348530292511 + 10.0 * 6.23025369644165
Epoch 1430, val loss: 1.278630256652832
Epoch 1440, training loss: 62.332584381103516 = 0.050747279077768326 + 10.0 * 6.228183746337891
Epoch 1440, val loss: 1.2846406698226929
Epoch 1450, training loss: 62.35301971435547 = 0.049514126032590866 + 10.0 * 6.230350494384766
Epoch 1450, val loss: 1.2906098365783691
Epoch 1460, training loss: 62.31694412231445 = 0.04832806810736656 + 10.0 * 6.226861476898193
Epoch 1460, val loss: 1.2967280149459839
Epoch 1470, training loss: 62.30217742919922 = 0.04717351496219635 + 10.0 * 6.225500583648682
Epoch 1470, val loss: 1.302741289138794
Epoch 1480, training loss: 62.3006477355957 = 0.04606464132666588 + 10.0 * 6.225458145141602
Epoch 1480, val loss: 1.3086827993392944
Epoch 1490, training loss: 62.35032653808594 = 0.045003991574048996 + 10.0 * 6.230532169342041
Epoch 1490, val loss: 1.314603328704834
Epoch 1500, training loss: 62.32976531982422 = 0.043941959738731384 + 10.0 * 6.228582382202148
Epoch 1500, val loss: 1.3203946352005005
Epoch 1510, training loss: 62.299659729003906 = 0.04291443154215813 + 10.0 * 6.225674629211426
Epoch 1510, val loss: 1.326055884361267
Epoch 1520, training loss: 62.282379150390625 = 0.04194024205207825 + 10.0 * 6.224043846130371
Epoch 1520, val loss: 1.3319876194000244
Epoch 1530, training loss: 62.275516510009766 = 0.04098916053771973 + 10.0 * 6.223452568054199
Epoch 1530, val loss: 1.3377882242202759
Epoch 1540, training loss: 62.325809478759766 = 0.040075648576021194 + 10.0 * 6.228573322296143
Epoch 1540, val loss: 1.343719482421875
Epoch 1550, training loss: 62.274749755859375 = 0.03918096795678139 + 10.0 * 6.223556995391846
Epoch 1550, val loss: 1.3488132953643799
Epoch 1560, training loss: 62.29811096191406 = 0.038314417004585266 + 10.0 * 6.225979804992676
Epoch 1560, val loss: 1.3546907901763916
Epoch 1570, training loss: 62.256534576416016 = 0.03746762126684189 + 10.0 * 6.221906661987305
Epoch 1570, val loss: 1.3600940704345703
Epoch 1580, training loss: 62.25914764404297 = 0.03665243089199066 + 10.0 * 6.222249507904053
Epoch 1580, val loss: 1.3657820224761963
Epoch 1590, training loss: 62.273658752441406 = 0.03586148843169212 + 10.0 * 6.223779678344727
Epoch 1590, val loss: 1.3711885213851929
Epoch 1600, training loss: 62.27836990356445 = 0.035089656710624695 + 10.0 * 6.22432804107666
Epoch 1600, val loss: 1.3764744997024536
Epoch 1610, training loss: 62.24169921875 = 0.03434033319354057 + 10.0 * 6.220736026763916
Epoch 1610, val loss: 1.3818212747573853
Epoch 1620, training loss: 62.23905563354492 = 0.0336252897977829 + 10.0 * 6.220542907714844
Epoch 1620, val loss: 1.3873189687728882
Epoch 1630, training loss: 62.234554290771484 = 0.03292199224233627 + 10.0 * 6.220163345336914
Epoch 1630, val loss: 1.3925378322601318
Epoch 1640, training loss: 62.244911193847656 = 0.0322490893304348 + 10.0 * 6.221266269683838
Epoch 1640, val loss: 1.397891879081726
Epoch 1650, training loss: 62.279964447021484 = 0.03158963471651077 + 10.0 * 6.224837303161621
Epoch 1650, val loss: 1.4029291868209839
Epoch 1660, training loss: 62.246070861816406 = 0.03093358874320984 + 10.0 * 6.221513748168945
Epoch 1660, val loss: 1.4080586433410645
Epoch 1670, training loss: 62.22520446777344 = 0.030305752530694008 + 10.0 * 6.219490051269531
Epoch 1670, val loss: 1.413117527961731
Epoch 1680, training loss: 62.2408561706543 = 0.029698504135012627 + 10.0 * 6.221116065979004
Epoch 1680, val loss: 1.418187141418457
Epoch 1690, training loss: 62.21605682373047 = 0.029107023030519485 + 10.0 * 6.218695163726807
Epoch 1690, val loss: 1.423285722732544
Epoch 1700, training loss: 62.22893524169922 = 0.02854250557720661 + 10.0 * 6.220039367675781
Epoch 1700, val loss: 1.4283740520477295
Epoch 1710, training loss: 62.26225662231445 = 0.02798200026154518 + 10.0 * 6.223427772521973
Epoch 1710, val loss: 1.4331544637680054
Epoch 1720, training loss: 62.21342468261719 = 0.02742587774991989 + 10.0 * 6.218599796295166
Epoch 1720, val loss: 1.4381860494613647
Epoch 1730, training loss: 62.199501037597656 = 0.026902247220277786 + 10.0 * 6.217259883880615
Epoch 1730, val loss: 1.4432514905929565
Epoch 1740, training loss: 62.19902038574219 = 0.02639227919280529 + 10.0 * 6.2172627449035645
Epoch 1740, val loss: 1.4480699300765991
Epoch 1750, training loss: 62.30035400390625 = 0.02590150572359562 + 10.0 * 6.227445125579834
Epoch 1750, val loss: 1.4531363248825073
Epoch 1760, training loss: 62.212520599365234 = 0.025396637618541718 + 10.0 * 6.218712329864502
Epoch 1760, val loss: 1.4572780132293701
Epoch 1770, training loss: 62.18422317504883 = 0.024920498952269554 + 10.0 * 6.215929985046387
Epoch 1770, val loss: 1.4624449014663696
Epoch 1780, training loss: 62.178489685058594 = 0.02445870079100132 + 10.0 * 6.215403079986572
Epoch 1780, val loss: 1.4670556783676147
Epoch 1790, training loss: 62.181034088134766 = 0.02401234768331051 + 10.0 * 6.215702056884766
Epoch 1790, val loss: 1.4718552827835083
Epoch 1800, training loss: 62.300865173339844 = 0.02357056364417076 + 10.0 * 6.227729320526123
Epoch 1800, val loss: 1.4764153957366943
Epoch 1810, training loss: 62.22560501098633 = 0.023147014901041985 + 10.0 * 6.220245838165283
Epoch 1810, val loss: 1.480721354484558
Epoch 1820, training loss: 62.17613220214844 = 0.022722043097019196 + 10.0 * 6.215341091156006
Epoch 1820, val loss: 1.4853439331054688
Epoch 1830, training loss: 62.16402816772461 = 0.022321125492453575 + 10.0 * 6.214170932769775
Epoch 1830, val loss: 1.4900741577148438
Epoch 1840, training loss: 62.164955139160156 = 0.021929990500211716 + 10.0 * 6.2143025398254395
Epoch 1840, val loss: 1.494644284248352
Epoch 1850, training loss: 62.22019577026367 = 0.02155318669974804 + 10.0 * 6.219864368438721
Epoch 1850, val loss: 1.4992657899856567
Epoch 1860, training loss: 62.203731536865234 = 0.0211708415299654 + 10.0 * 6.218255996704102
Epoch 1860, val loss: 1.5031920671463013
Epoch 1870, training loss: 62.192909240722656 = 0.020796999335289 + 10.0 * 6.2172112464904785
Epoch 1870, val loss: 1.5076197385787964
Epoch 1880, training loss: 62.178524017333984 = 0.020436547696590424 + 10.0 * 6.215808868408203
Epoch 1880, val loss: 1.5118106603622437
Epoch 1890, training loss: 62.15498352050781 = 0.020093770697712898 + 10.0 * 6.213489055633545
Epoch 1890, val loss: 1.5162718296051025
Epoch 1900, training loss: 62.1514778137207 = 0.019752781838178635 + 10.0 * 6.213172435760498
Epoch 1900, val loss: 1.5205833911895752
Epoch 1910, training loss: 62.15068817138672 = 0.019425038248300552 + 10.0 * 6.213126182556152
Epoch 1910, val loss: 1.5248934030532837
Epoch 1920, training loss: 62.1671028137207 = 0.01910620927810669 + 10.0 * 6.214799404144287
Epoch 1920, val loss: 1.5291662216186523
Epoch 1930, training loss: 62.19288635253906 = 0.018792370334267616 + 10.0 * 6.217409610748291
Epoch 1930, val loss: 1.5332696437835693
Epoch 1940, training loss: 62.15122985839844 = 0.018473410978913307 + 10.0 * 6.21327543258667
Epoch 1940, val loss: 1.537231683731079
Epoch 1950, training loss: 62.138187408447266 = 0.01817072369158268 + 10.0 * 6.212001800537109
Epoch 1950, val loss: 1.5414602756500244
Epoch 1960, training loss: 62.14030075073242 = 0.01787993311882019 + 10.0 * 6.212242126464844
Epoch 1960, val loss: 1.5456197261810303
Epoch 1970, training loss: 62.159122467041016 = 0.017592431977391243 + 10.0 * 6.214152812957764
Epoch 1970, val loss: 1.5496748685836792
Epoch 1980, training loss: 62.13085174560547 = 0.017310133203864098 + 10.0 * 6.2113542556762695
Epoch 1980, val loss: 1.5536940097808838
Epoch 1990, training loss: 62.13785171508789 = 0.017035529017448425 + 10.0 * 6.212081432342529
Epoch 1990, val loss: 1.5576212406158447
Epoch 2000, training loss: 62.225276947021484 = 0.01676337793469429 + 10.0 * 6.220851421356201
Epoch 2000, val loss: 1.561555027961731
Epoch 2010, training loss: 62.138545989990234 = 0.016503509134054184 + 10.0 * 6.212204456329346
Epoch 2010, val loss: 1.565405011177063
Epoch 2020, training loss: 62.118961334228516 = 0.016241375356912613 + 10.0 * 6.210271835327148
Epoch 2020, val loss: 1.5693000555038452
Epoch 2030, training loss: 62.115238189697266 = 0.0159919373691082 + 10.0 * 6.209924697875977
Epoch 2030, val loss: 1.5731992721557617
Epoch 2040, training loss: 62.160911560058594 = 0.01575164869427681 + 10.0 * 6.2145161628723145
Epoch 2040, val loss: 1.5770514011383057
Epoch 2050, training loss: 62.12678527832031 = 0.015507999807596207 + 10.0 * 6.211127758026123
Epoch 2050, val loss: 1.5808749198913574
Epoch 2060, training loss: 62.143184661865234 = 0.015275895595550537 + 10.0 * 6.2127909660339355
Epoch 2060, val loss: 1.5847539901733398
Epoch 2070, training loss: 62.104522705078125 = 0.015040762722492218 + 10.0 * 6.208948135375977
Epoch 2070, val loss: 1.5881476402282715
Epoch 2080, training loss: 62.10429763793945 = 0.014816509559750557 + 10.0 * 6.208948135375977
Epoch 2080, val loss: 1.5917214155197144
Epoch 2090, training loss: 62.11820983886719 = 0.014599519781768322 + 10.0 * 6.210361003875732
Epoch 2090, val loss: 1.5953161716461182
Epoch 2100, training loss: 62.14550018310547 = 0.014383670873939991 + 10.0 * 6.213111400604248
Epoch 2100, val loss: 1.5988998413085938
Epoch 2110, training loss: 62.1334228515625 = 0.014174438081681728 + 10.0 * 6.2119245529174805
Epoch 2110, val loss: 1.6026259660720825
Epoch 2120, training loss: 62.112701416015625 = 0.013967033475637436 + 10.0 * 6.209873676300049
Epoch 2120, val loss: 1.6058484315872192
Epoch 2130, training loss: 62.09652328491211 = 0.013765554875135422 + 10.0 * 6.20827579498291
Epoch 2130, val loss: 1.6094807386398315
Epoch 2140, training loss: 62.103797912597656 = 0.013569672591984272 + 10.0 * 6.2090229988098145
Epoch 2140, val loss: 1.61294686794281
Epoch 2150, training loss: 62.14337158203125 = 0.013376859948039055 + 10.0 * 6.21299934387207
Epoch 2150, val loss: 1.6162077188491821
Epoch 2160, training loss: 62.11781692504883 = 0.01319075096398592 + 10.0 * 6.21046257019043
Epoch 2160, val loss: 1.6197141408920288
Epoch 2170, training loss: 62.08944320678711 = 0.01300265546888113 + 10.0 * 6.207643985748291
Epoch 2170, val loss: 1.6229056119918823
Epoch 2180, training loss: 62.117767333984375 = 0.012824016623198986 + 10.0 * 6.210494041442871
Epoch 2180, val loss: 1.6260303258895874
Epoch 2190, training loss: 62.09722900390625 = 0.012644401751458645 + 10.0 * 6.208458423614502
Epoch 2190, val loss: 1.6295733451843262
Epoch 2200, training loss: 62.085811614990234 = 0.012471283785998821 + 10.0 * 6.207334041595459
Epoch 2200, val loss: 1.632761001586914
Epoch 2210, training loss: 62.084590911865234 = 0.012300709262490273 + 10.0 * 6.207228660583496
Epoch 2210, val loss: 1.6359381675720215
Epoch 2220, training loss: 62.113731384277344 = 0.012135311029851437 + 10.0 * 6.210159778594971
Epoch 2220, val loss: 1.6390857696533203
Epoch 2230, training loss: 62.0849723815918 = 0.01197099406272173 + 10.0 * 6.207300186157227
Epoch 2230, val loss: 1.6421972513198853
Epoch 2240, training loss: 62.09467697143555 = 0.011813926510512829 + 10.0 * 6.208286285400391
Epoch 2240, val loss: 1.6453138589859009
Epoch 2250, training loss: 62.06961441040039 = 0.011655383743345737 + 10.0 * 6.205796241760254
Epoch 2250, val loss: 1.6483851671218872
Epoch 2260, training loss: 62.08616638183594 = 0.01150268130004406 + 10.0 * 6.2074666023254395
Epoch 2260, val loss: 1.6512209177017212
Epoch 2270, training loss: 62.1158447265625 = 0.011353345587849617 + 10.0 * 6.21044921875
Epoch 2270, val loss: 1.6543151140213013
Epoch 2280, training loss: 62.088050842285156 = 0.011208211071789265 + 10.0 * 6.207684516906738
Epoch 2280, val loss: 1.6573960781097412
Epoch 2290, training loss: 62.08430480957031 = 0.011060970835387707 + 10.0 * 6.207324028015137
Epoch 2290, val loss: 1.6602554321289062
Epoch 2300, training loss: 62.08320999145508 = 0.010920204222202301 + 10.0 * 6.207228660583496
Epoch 2300, val loss: 1.6633620262145996
Epoch 2310, training loss: 62.057273864746094 = 0.010779223404824734 + 10.0 * 6.204649448394775
Epoch 2310, val loss: 1.6661350727081299
Epoch 2320, training loss: 62.05459976196289 = 0.010643319226801395 + 10.0 * 6.204395771026611
Epoch 2320, val loss: 1.6690261363983154
Epoch 2330, training loss: 62.08401107788086 = 0.010511916130781174 + 10.0 * 6.20734977722168
Epoch 2330, val loss: 1.6719005107879639
Epoch 2340, training loss: 62.093441009521484 = 0.010377955622971058 + 10.0 * 6.208306312561035
Epoch 2340, val loss: 1.6746498346328735
Epoch 2350, training loss: 62.05292892456055 = 0.01025026012212038 + 10.0 * 6.204267978668213
Epoch 2350, val loss: 1.6771245002746582
Epoch 2360, training loss: 62.04850769042969 = 0.010122740641236305 + 10.0 * 6.203838348388672
Epoch 2360, val loss: 1.6799430847167969
Epoch 2370, training loss: 62.056209564208984 = 0.010001028887927532 + 10.0 * 6.204620838165283
Epoch 2370, val loss: 1.6829807758331299
Epoch 2380, training loss: 62.106414794921875 = 0.009881438687443733 + 10.0 * 6.209653377532959
Epoch 2380, val loss: 1.685564637184143
Epoch 2390, training loss: 62.063804626464844 = 0.009758862666785717 + 10.0 * 6.205404758453369
Epoch 2390, val loss: 1.6880502700805664
Epoch 2400, training loss: 62.04543685913086 = 0.009642708115279675 + 10.0 * 6.203579425811768
Epoch 2400, val loss: 1.6907670497894287
Epoch 2410, training loss: 62.041526794433594 = 0.009528645314276218 + 10.0 * 6.203199863433838
Epoch 2410, val loss: 1.6934032440185547
Epoch 2420, training loss: 62.07596206665039 = 0.00941943284124136 + 10.0 * 6.2066545486450195
Epoch 2420, val loss: 1.6962379217147827
Epoch 2430, training loss: 62.05695343017578 = 0.009309724904596806 + 10.0 * 6.204764366149902
Epoch 2430, val loss: 1.6984190940856934
Epoch 2440, training loss: 62.083133697509766 = 0.009200450032949448 + 10.0 * 6.207393169403076
Epoch 2440, val loss: 1.700932264328003
Epoch 2450, training loss: 62.050453186035156 = 0.00908766221255064 + 10.0 * 6.204136848449707
Epoch 2450, val loss: 1.7029391527175903
Epoch 2460, training loss: 62.0321044921875 = 0.008985662832856178 + 10.0 * 6.2023115158081055
Epoch 2460, val loss: 1.7058061361312866
Epoch 2470, training loss: 62.03013610839844 = 0.00888415239751339 + 10.0 * 6.202125072479248
Epoch 2470, val loss: 1.7081924676895142
Epoch 2480, training loss: 62.05830001831055 = 0.008785189129412174 + 10.0 * 6.204951286315918
Epoch 2480, val loss: 1.7107807397842407
Epoch 2490, training loss: 62.04127502441406 = 0.00868444237858057 + 10.0 * 6.203258991241455
Epoch 2490, val loss: 1.712804913520813
Epoch 2500, training loss: 62.058048248291016 = 0.008586625568568707 + 10.0 * 6.204946041107178
Epoch 2500, val loss: 1.7150840759277344
Epoch 2510, training loss: 62.04336166381836 = 0.008490988984704018 + 10.0 * 6.203486919403076
Epoch 2510, val loss: 1.7175084352493286
Epoch 2520, training loss: 62.02748489379883 = 0.008397351950407028 + 10.0 * 6.201909065246582
Epoch 2520, val loss: 1.719825029373169
Epoch 2530, training loss: 62.024620056152344 = 0.008307150565087795 + 10.0 * 6.20163106918335
Epoch 2530, val loss: 1.7222665548324585
Epoch 2540, training loss: 62.05675506591797 = 0.008220509625971317 + 10.0 * 6.204853534698486
Epoch 2540, val loss: 1.7245784997940063
Epoch 2550, training loss: 62.046722412109375 = 0.00812863651663065 + 10.0 * 6.203859329223633
Epoch 2550, val loss: 1.726439356803894
Epoch 2560, training loss: 62.01762008666992 = 0.008038691245019436 + 10.0 * 6.200958251953125
Epoch 2560, val loss: 1.7285631895065308
Epoch 2570, training loss: 62.01932144165039 = 0.007952867075800896 + 10.0 * 6.201136589050293
Epoch 2570, val loss: 1.7308799028396606
Epoch 2580, training loss: 62.02730178833008 = 0.007869909517467022 + 10.0 * 6.201943397521973
Epoch 2580, val loss: 1.7329316139221191
Epoch 2590, training loss: 62.041866302490234 = 0.007788284216076136 + 10.0 * 6.2034077644348145
Epoch 2590, val loss: 1.7351380586624146
Epoch 2600, training loss: 62.0837516784668 = 0.007709095254540443 + 10.0 * 6.20760440826416
Epoch 2600, val loss: 1.7375751733779907
Epoch 2610, training loss: 62.02061462402344 = 0.0076235271990299225 + 10.0 * 6.20129919052124
Epoch 2610, val loss: 1.7389812469482422
Epoch 2620, training loss: 62.0071907043457 = 0.007545171305537224 + 10.0 * 6.19996452331543
Epoch 2620, val loss: 1.7413891553878784
Epoch 2630, training loss: 62.02507019042969 = 0.007468092255294323 + 10.0 * 6.201760292053223
Epoch 2630, val loss: 1.7434148788452148
Epoch 2640, training loss: 62.043792724609375 = 0.007391734980046749 + 10.0 * 6.203639984130859
Epoch 2640, val loss: 1.745311975479126
Epoch 2650, training loss: 62.01072311401367 = 0.007318404503166676 + 10.0 * 6.200340747833252
Epoch 2650, val loss: 1.7471871376037598
Epoch 2660, training loss: 61.997657775878906 = 0.007245173212140799 + 10.0 * 6.199041366577148
Epoch 2660, val loss: 1.7492722272872925
Epoch 2670, training loss: 62.02465057373047 = 0.007175684906542301 + 10.0 * 6.201747417449951
Epoch 2670, val loss: 1.7513660192489624
Epoch 2680, training loss: 62.02397537231445 = 0.00710242660716176 + 10.0 * 6.201687335968018
Epoch 2680, val loss: 1.7529222965240479
Epoch 2690, training loss: 61.99836349487305 = 0.007029241882264614 + 10.0 * 6.199133396148682
Epoch 2690, val loss: 1.7547754049301147
Epoch 2700, training loss: 61.99160385131836 = 0.006961238104850054 + 10.0 * 6.198464393615723
Epoch 2700, val loss: 1.7567541599273682
Epoch 2710, training loss: 61.98737716674805 = 0.006894870661199093 + 10.0 * 6.1980485916137695
Epoch 2710, val loss: 1.7586044073104858
Epoch 2720, training loss: 61.989742279052734 = 0.0068301488645374775 + 10.0 * 6.198291301727295
Epoch 2720, val loss: 1.760387659072876
Epoch 2730, training loss: 62.092918395996094 = 0.006769677624106407 + 10.0 * 6.208614826202393
Epoch 2730, val loss: 1.7623133659362793
Epoch 2740, training loss: 62.04438781738281 = 0.0066999043338000774 + 10.0 * 6.203768730163574
Epoch 2740, val loss: 1.7635741233825684
Epoch 2750, training loss: 62.00068664550781 = 0.006635061465203762 + 10.0 * 6.199405193328857
Epoch 2750, val loss: 1.7654293775558472
Epoch 2760, training loss: 61.98872756958008 = 0.006572310347110033 + 10.0 * 6.198215484619141
Epoch 2760, val loss: 1.767298698425293
Epoch 2770, training loss: 61.98606491088867 = 0.006512362509965897 + 10.0 * 6.197955131530762
Epoch 2770, val loss: 1.768941879272461
Epoch 2780, training loss: 62.05469512939453 = 0.006455553695559502 + 10.0 * 6.204823970794678
Epoch 2780, val loss: 1.770720362663269
Epoch 2790, training loss: 62.01111602783203 = 0.006392508279532194 + 10.0 * 6.200472354888916
Epoch 2790, val loss: 1.7718325853347778
Epoch 2800, training loss: 61.988868713378906 = 0.006333325989544392 + 10.0 * 6.198253631591797
Epoch 2800, val loss: 1.7735772132873535
Epoch 2810, training loss: 61.9876708984375 = 0.006276173051446676 + 10.0 * 6.198139667510986
Epoch 2810, val loss: 1.7752231359481812
Epoch 2820, training loss: 62.00111770629883 = 0.006221143063157797 + 10.0 * 6.199489593505859
Epoch 2820, val loss: 1.7767786979675293
Epoch 2830, training loss: 62.01301956176758 = 0.006166006904095411 + 10.0 * 6.200685501098633
Epoch 2830, val loss: 1.7782642841339111
Epoch 2840, training loss: 61.98600387573242 = 0.006108580157160759 + 10.0 * 6.197989463806152
Epoch 2840, val loss: 1.7797272205352783
Epoch 2850, training loss: 61.98445510864258 = 0.006054117809981108 + 10.0 * 6.197840213775635
Epoch 2850, val loss: 1.7812179327011108
Epoch 2860, training loss: 61.97766876220703 = 0.0060019055381417274 + 10.0 * 6.197166919708252
Epoch 2860, val loss: 1.7828454971313477
Epoch 2870, training loss: 62.064727783203125 = 0.0059542059898376465 + 10.0 * 6.205877304077148
Epoch 2870, val loss: 1.7847055196762085
Epoch 2880, training loss: 61.98768615722656 = 0.00589777110144496 + 10.0 * 6.198178768157959
Epoch 2880, val loss: 1.7852270603179932
Epoch 2890, training loss: 61.97395706176758 = 0.005845849867910147 + 10.0 * 6.196811199188232
Epoch 2890, val loss: 1.786971926689148
Epoch 2900, training loss: 61.968414306640625 = 0.005796824581921101 + 10.0 * 6.196261405944824
Epoch 2900, val loss: 1.7884236574172974
Epoch 2910, training loss: 61.97087478637695 = 0.00574856624007225 + 10.0 * 6.196512699127197
Epoch 2910, val loss: 1.78984797000885
Epoch 2920, training loss: 62.05229187011719 = 0.005700491368770599 + 10.0 * 6.204659461975098
Epoch 2920, val loss: 1.7911179065704346
Epoch 2930, training loss: 61.99797058105469 = 0.005652945023030043 + 10.0 * 6.1992316246032715
Epoch 2930, val loss: 1.7921561002731323
Epoch 2940, training loss: 61.97101593017578 = 0.005604538135230541 + 10.0 * 6.1965413093566895
Epoch 2940, val loss: 1.7936409711837769
Epoch 2950, training loss: 61.96181869506836 = 0.005559039302170277 + 10.0 * 6.195626258850098
Epoch 2950, val loss: 1.7949023246765137
Epoch 2960, training loss: 61.987632751464844 = 0.0055152131244540215 + 10.0 * 6.198211669921875
Epoch 2960, val loss: 1.7963097095489502
Epoch 2970, training loss: 61.971038818359375 = 0.0054696290753781796 + 10.0 * 6.19655704498291
Epoch 2970, val loss: 1.7973936796188354
Epoch 2980, training loss: 61.97117614746094 = 0.005424924660474062 + 10.0 * 6.196575164794922
Epoch 2980, val loss: 1.7985162734985352
Epoch 2990, training loss: 62.00366973876953 = 0.005383618175983429 + 10.0 * 6.199828624725342
Epoch 2990, val loss: 1.799963355064392
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 87.90252685546875 = 1.9338667392730713 + 10.0 * 8.5968656539917
Epoch 0, val loss: 1.921925663948059
Epoch 10, training loss: 87.88786315917969 = 1.9244093894958496 + 10.0 * 8.596345901489258
Epoch 10, val loss: 1.9120750427246094
Epoch 20, training loss: 87.83324432373047 = 1.9131978750228882 + 10.0 * 8.592004776000977
Epoch 20, val loss: 1.9002914428710938
Epoch 30, training loss: 87.49149322509766 = 1.8994104862213135 + 10.0 * 8.559207916259766
Epoch 30, val loss: 1.8859634399414062
Epoch 40, training loss: 85.57415771484375 = 1.8826624155044556 + 10.0 * 8.369150161743164
Epoch 40, val loss: 1.869064211845398
Epoch 50, training loss: 79.86083221435547 = 1.8647551536560059 + 10.0 * 7.79960823059082
Epoch 50, val loss: 1.8516168594360352
Epoch 60, training loss: 76.48159790039062 = 1.8505353927612305 + 10.0 * 7.463106632232666
Epoch 60, val loss: 1.839253544807434
Epoch 70, training loss: 73.64022064208984 = 1.8400490283966064 + 10.0 * 7.180017471313477
Epoch 70, val loss: 1.8298712968826294
Epoch 80, training loss: 71.79676818847656 = 1.8287492990493774 + 10.0 * 6.996801376342773
Epoch 80, val loss: 1.8194652795791626
Epoch 90, training loss: 70.40750122070312 = 1.8168118000030518 + 10.0 * 6.859068870544434
Epoch 90, val loss: 1.8085055351257324
Epoch 100, training loss: 69.63977813720703 = 1.8059040307998657 + 10.0 * 6.783387660980225
Epoch 100, val loss: 1.7985433340072632
Epoch 110, training loss: 69.171875 = 1.794679880142212 + 10.0 * 6.737720012664795
Epoch 110, val loss: 1.7883745431900024
Epoch 120, training loss: 68.74858856201172 = 1.7828707695007324 + 10.0 * 6.6965718269348145
Epoch 120, val loss: 1.7779827117919922
Epoch 130, training loss: 68.40188598632812 = 1.7715437412261963 + 10.0 * 6.663033962249756
Epoch 130, val loss: 1.7680792808532715
Epoch 140, training loss: 68.07089233398438 = 1.7603559494018555 + 10.0 * 6.631053924560547
Epoch 140, val loss: 1.7582836151123047
Epoch 150, training loss: 67.72106170654297 = 1.7486728429794312 + 10.0 * 6.597239017486572
Epoch 150, val loss: 1.7481403350830078
Epoch 160, training loss: 67.36905670166016 = 1.7366267442703247 + 10.0 * 6.5632429122924805
Epoch 160, val loss: 1.7375978231430054
Epoch 170, training loss: 67.11145782470703 = 1.7236651182174683 + 10.0 * 6.5387797355651855
Epoch 170, val loss: 1.7261216640472412
Epoch 180, training loss: 66.86748504638672 = 1.7088404893875122 + 10.0 * 6.515864849090576
Epoch 180, val loss: 1.7132712602615356
Epoch 190, training loss: 66.67286682128906 = 1.6924116611480713 + 10.0 * 6.498045444488525
Epoch 190, val loss: 1.699026346206665
Epoch 200, training loss: 66.50984191894531 = 1.6742641925811768 + 10.0 * 6.48355770111084
Epoch 200, val loss: 1.6834450960159302
Epoch 210, training loss: 66.38057708740234 = 1.654442548751831 + 10.0 * 6.472613334655762
Epoch 210, val loss: 1.6663748025894165
Epoch 220, training loss: 66.23846435546875 = 1.6327272653579712 + 10.0 * 6.460573673248291
Epoch 220, val loss: 1.6478101015090942
Epoch 230, training loss: 66.10558319091797 = 1.60924232006073 + 10.0 * 6.449634075164795
Epoch 230, val loss: 1.6278210878372192
Epoch 240, training loss: 66.04251098632812 = 1.5839791297912598 + 10.0 * 6.445853233337402
Epoch 240, val loss: 1.6063371896743774
Epoch 250, training loss: 65.87030029296875 = 1.5567456483840942 + 10.0 * 6.431354999542236
Epoch 250, val loss: 1.5831990242004395
Epoch 260, training loss: 65.7405776977539 = 1.5276914834976196 + 10.0 * 6.42128849029541
Epoch 260, val loss: 1.5587519407272339
Epoch 270, training loss: 65.6112289428711 = 1.4970284700393677 + 10.0 * 6.411419868469238
Epoch 270, val loss: 1.533056616783142
Epoch 280, training loss: 65.58989715576172 = 1.464714765548706 + 10.0 * 6.41251802444458
Epoch 280, val loss: 1.5060312747955322
Epoch 290, training loss: 65.40674591064453 = 1.4305306673049927 + 10.0 * 6.3976216316223145
Epoch 290, val loss: 1.477774739265442
Epoch 300, training loss: 65.29229736328125 = 1.3952345848083496 + 10.0 * 6.389706134796143
Epoch 300, val loss: 1.4487589597702026
Epoch 310, training loss: 65.18904876708984 = 1.3589036464691162 + 10.0 * 6.38301420211792
Epoch 310, val loss: 1.4190815687179565
Epoch 320, training loss: 65.10700225830078 = 1.3216959238052368 + 10.0 * 6.378530979156494
Epoch 320, val loss: 1.389030933380127
Epoch 330, training loss: 65.00458526611328 = 1.2838283777236938 + 10.0 * 6.372076034545898
Epoch 330, val loss: 1.3585984706878662
Epoch 340, training loss: 64.91887664794922 = 1.2458503246307373 + 10.0 * 6.367302894592285
Epoch 340, val loss: 1.3282781839370728
Epoch 350, training loss: 64.82254028320312 = 1.2079620361328125 + 10.0 * 6.361457824707031
Epoch 350, val loss: 1.2982497215270996
Epoch 360, training loss: 64.74349975585938 = 1.1703529357910156 + 10.0 * 6.3573150634765625
Epoch 360, val loss: 1.2686899900436401
Epoch 370, training loss: 64.68195343017578 = 1.1332582235336304 + 10.0 * 6.354869842529297
Epoch 370, val loss: 1.2396572828292847
Epoch 380, training loss: 64.59476470947266 = 1.0969226360321045 + 10.0 * 6.349783897399902
Epoch 380, val loss: 1.211551308631897
Epoch 390, training loss: 64.52427673339844 = 1.0617350339889526 + 10.0 * 6.346253871917725
Epoch 390, val loss: 1.1845579147338867
Epoch 400, training loss: 64.45006561279297 = 1.027655005455017 + 10.0 * 6.342240810394287
Epoch 400, val loss: 1.1587470769882202
Epoch 410, training loss: 64.42601013183594 = 0.9946547150611877 + 10.0 * 6.343135356903076
Epoch 410, val loss: 1.13405179977417
Epoch 420, training loss: 64.34760284423828 = 0.9629424214363098 + 10.0 * 6.338466167449951
Epoch 420, val loss: 1.1104605197906494
Epoch 430, training loss: 64.267822265625 = 0.9325608611106873 + 10.0 * 6.333525657653809
Epoch 430, val loss: 1.0882569551467896
Epoch 440, training loss: 64.20298767089844 = 0.9034039378166199 + 10.0 * 6.329958438873291
Epoch 440, val loss: 1.0673378705978394
Epoch 450, training loss: 64.17784881591797 = 0.8753499984741211 + 10.0 * 6.330249786376953
Epoch 450, val loss: 1.0475270748138428
Epoch 460, training loss: 64.14906311035156 = 0.8485302329063416 + 10.0 * 6.330052852630615
Epoch 460, val loss: 1.0288506746292114
Epoch 470, training loss: 64.06066131591797 = 0.8227055072784424 + 10.0 * 6.323795318603516
Epoch 470, val loss: 1.0111421346664429
Epoch 480, training loss: 64.00068664550781 = 0.7978677749633789 + 10.0 * 6.320281982421875
Epoch 480, val loss: 0.9945229887962341
Epoch 490, training loss: 63.95288848876953 = 0.7738695740699768 + 10.0 * 6.317902088165283
Epoch 490, val loss: 0.9787150025367737
Epoch 500, training loss: 64.0014419555664 = 0.7507110834121704 + 10.0 * 6.325072765350342
Epoch 500, val loss: 0.963668704032898
Epoch 510, training loss: 63.88078308105469 = 0.7278600335121155 + 10.0 * 6.3152923583984375
Epoch 510, val loss: 0.9490207433700562
Epoch 520, training loss: 63.84251022338867 = 0.7059541940689087 + 10.0 * 6.313655376434326
Epoch 520, val loss: 0.9353798627853394
Epoch 530, training loss: 63.783294677734375 = 0.6846213340759277 + 10.0 * 6.3098673820495605
Epoch 530, val loss: 0.9225212335586548
Epoch 540, training loss: 63.74067687988281 = 0.6637977361679077 + 10.0 * 6.307687759399414
Epoch 540, val loss: 0.9101688861846924
Epoch 550, training loss: 63.71141052246094 = 0.643371045589447 + 10.0 * 6.3068037033081055
Epoch 550, val loss: 0.8983197212219238
Epoch 560, training loss: 63.686038970947266 = 0.623303234577179 + 10.0 * 6.306273460388184
Epoch 560, val loss: 0.8868800401687622
Epoch 570, training loss: 63.626216888427734 = 0.6036809682846069 + 10.0 * 6.302253723144531
Epoch 570, val loss: 0.8760243058204651
Epoch 580, training loss: 63.5921745300293 = 0.5845363736152649 + 10.0 * 6.3007636070251465
Epoch 580, val loss: 0.8658593893051147
Epoch 590, training loss: 63.618202209472656 = 0.5658543109893799 + 10.0 * 6.305234909057617
Epoch 590, val loss: 0.8562594652175903
Epoch 600, training loss: 63.542964935302734 = 0.5474056005477905 + 10.0 * 6.299555778503418
Epoch 600, val loss: 0.8468790650367737
Epoch 610, training loss: 63.48986053466797 = 0.5295632481575012 + 10.0 * 6.296029567718506
Epoch 610, val loss: 0.8382582068443298
Epoch 620, training loss: 63.45597839355469 = 0.5121012330055237 + 10.0 * 6.2943878173828125
Epoch 620, val loss: 0.8301562070846558
Epoch 630, training loss: 63.44232177734375 = 0.4950692355632782 + 10.0 * 6.29472541809082
Epoch 630, val loss: 0.8225738406181335
Epoch 640, training loss: 63.41401290893555 = 0.47841358184814453 + 10.0 * 6.293560028076172
Epoch 640, val loss: 0.8153702020645142
Epoch 650, training loss: 63.36982727050781 = 0.46219587326049805 + 10.0 * 6.290762901306152
Epoch 650, val loss: 0.8085916042327881
Epoch 660, training loss: 63.37820053100586 = 0.44649648666381836 + 10.0 * 6.29317045211792
Epoch 660, val loss: 0.8024349212646484
Epoch 670, training loss: 63.30224609375 = 0.43134498596191406 + 10.0 * 6.287090301513672
Epoch 670, val loss: 0.7968699336051941
Epoch 680, training loss: 63.268898010253906 = 0.4166576862335205 + 10.0 * 6.285223960876465
Epoch 680, val loss: 0.7917377352714539
Epoch 690, training loss: 63.238258361816406 = 0.40247029066085815 + 10.0 * 6.283578872680664
Epoch 690, val loss: 0.7871343493461609
Epoch 700, training loss: 63.28700256347656 = 0.38872700929641724 + 10.0 * 6.289827823638916
Epoch 700, val loss: 0.782929003238678
Epoch 710, training loss: 63.19569778442383 = 0.3753979206085205 + 10.0 * 6.28203010559082
Epoch 710, val loss: 0.778993546962738
Epoch 720, training loss: 63.16163635253906 = 0.36252516508102417 + 10.0 * 6.279911041259766
Epoch 720, val loss: 0.7755870223045349
Epoch 730, training loss: 63.14461898803711 = 0.350187212228775 + 10.0 * 6.279443264007568
Epoch 730, val loss: 0.7727267146110535
Epoch 740, training loss: 63.134857177734375 = 0.3382991552352905 + 10.0 * 6.279655933380127
Epoch 740, val loss: 0.7701987028121948
Epoch 750, training loss: 63.099510192871094 = 0.3268955647945404 + 10.0 * 6.277261257171631
Epoch 750, val loss: 0.7681138515472412
Epoch 760, training loss: 63.07218551635742 = 0.3157978951931 + 10.0 * 6.275638580322266
Epoch 760, val loss: 0.7662716507911682
Epoch 770, training loss: 63.059871673583984 = 0.30516111850738525 + 10.0 * 6.275471210479736
Epoch 770, val loss: 0.7649057507514954
Epoch 780, training loss: 63.01900100708008 = 0.2948688566684723 + 10.0 * 6.27241325378418
Epoch 780, val loss: 0.7638921737670898
Epoch 790, training loss: 62.997520446777344 = 0.28501108288764954 + 10.0 * 6.2712507247924805
Epoch 790, val loss: 0.7633256912231445
Epoch 800, training loss: 63.05314254760742 = 0.2755132019519806 + 10.0 * 6.2777628898620605
Epoch 800, val loss: 0.7628828883171082
Epoch 810, training loss: 62.97081756591797 = 0.2662905752658844 + 10.0 * 6.270452976226807
Epoch 810, val loss: 0.7627444267272949
Epoch 820, training loss: 62.93755340576172 = 0.25746163725852966 + 10.0 * 6.268009185791016
Epoch 820, val loss: 0.762999951839447
Epoch 830, training loss: 62.96705627441406 = 0.24896812438964844 + 10.0 * 6.271808624267578
Epoch 830, val loss: 0.7635952234268188
Epoch 840, training loss: 62.906333923339844 = 0.24082094430923462 + 10.0 * 6.2665510177612305
Epoch 840, val loss: 0.764517605304718
Epoch 850, training loss: 62.876155853271484 = 0.23289230465888977 + 10.0 * 6.264326572418213
Epoch 850, val loss: 0.7656903266906738
Epoch 860, training loss: 62.8636589050293 = 0.22530697286128998 + 10.0 * 6.2638349533081055
Epoch 860, val loss: 0.7672004103660583
Epoch 870, training loss: 62.91686248779297 = 0.2179994285106659 + 10.0 * 6.269886493682861
Epoch 870, val loss: 0.7690839767456055
Epoch 880, training loss: 62.83189010620117 = 0.2108432501554489 + 10.0 * 6.2621049880981445
Epoch 880, val loss: 0.7707768678665161
Epoch 890, training loss: 62.815608978271484 = 0.20403209328651428 + 10.0 * 6.261157512664795
Epoch 890, val loss: 0.7730476260185242
Epoch 900, training loss: 62.79829025268555 = 0.19748146831989288 + 10.0 * 6.260080814361572
Epoch 900, val loss: 0.7755814790725708
Epoch 910, training loss: 62.838253021240234 = 0.19115692377090454 + 10.0 * 6.26470947265625
Epoch 910, val loss: 0.7781977653503418
Epoch 920, training loss: 62.80702209472656 = 0.18498922884464264 + 10.0 * 6.262203216552734
Epoch 920, val loss: 0.7810568809509277
Epoch 930, training loss: 62.77231979370117 = 0.1790812760591507 + 10.0 * 6.259324073791504
Epoch 930, val loss: 0.7840356826782227
Epoch 940, training loss: 62.74203109741211 = 0.17341046035289764 + 10.0 * 6.256861686706543
Epoch 940, val loss: 0.7873725295066833
Epoch 950, training loss: 62.73406982421875 = 0.16795992851257324 + 10.0 * 6.256610870361328
Epoch 950, val loss: 0.7907955646514893
Epoch 960, training loss: 62.752227783203125 = 0.16267937421798706 + 10.0 * 6.258955001831055
Epoch 960, val loss: 0.7942724227905273
Epoch 970, training loss: 62.74406051635742 = 0.15751171112060547 + 10.0 * 6.258654594421387
Epoch 970, val loss: 0.7978401184082031
Epoch 980, training loss: 62.71436309814453 = 0.1525864452123642 + 10.0 * 6.2561774253845215
Epoch 980, val loss: 0.8018689751625061
Epoch 990, training loss: 62.69197463989258 = 0.14780566096305847 + 10.0 * 6.2544169425964355
Epoch 990, val loss: 0.8056828379631042
Epoch 1000, training loss: 62.665199279785156 = 0.1432323157787323 + 10.0 * 6.252196788787842
Epoch 1000, val loss: 0.8098123669624329
Epoch 1010, training loss: 62.66367721557617 = 0.13881048560142517 + 10.0 * 6.252486705780029
Epoch 1010, val loss: 0.8139987587928772
Epoch 1020, training loss: 62.67856216430664 = 0.13451141119003296 + 10.0 * 6.2544050216674805
Epoch 1020, val loss: 0.8182430863380432
Epoch 1030, training loss: 62.682491302490234 = 0.13034500181674957 + 10.0 * 6.255214691162109
Epoch 1030, val loss: 0.822796642780304
Epoch 1040, training loss: 62.631988525390625 = 0.12636005878448486 + 10.0 * 6.25056266784668
Epoch 1040, val loss: 0.8272778987884521
Epoch 1050, training loss: 62.61668014526367 = 0.12249541282653809 + 10.0 * 6.24941873550415
Epoch 1050, val loss: 0.8319357633590698
Epoch 1060, training loss: 62.62448501586914 = 0.11879602074623108 + 10.0 * 6.250568866729736
Epoch 1060, val loss: 0.8366789817810059
Epoch 1070, training loss: 62.609981536865234 = 0.11519049108028412 + 10.0 * 6.249479293823242
Epoch 1070, val loss: 0.8411770462989807
Epoch 1080, training loss: 62.61387252807617 = 0.11173026263713837 + 10.0 * 6.250214576721191
Epoch 1080, val loss: 0.8461703062057495
Epoch 1090, training loss: 62.58129119873047 = 0.10836808383464813 + 10.0 * 6.247292518615723
Epoch 1090, val loss: 0.8508713841438293
Epoch 1100, training loss: 62.57077407836914 = 0.10514293611049652 + 10.0 * 6.246562957763672
Epoch 1100, val loss: 0.8558545708656311
Epoch 1110, training loss: 62.642005920410156 = 0.10202731937170029 + 10.0 * 6.253997802734375
Epoch 1110, val loss: 0.8605050444602966
Epoch 1120, training loss: 62.58243179321289 = 0.09898146241903305 + 10.0 * 6.248345375061035
Epoch 1120, val loss: 0.8656951785087585
Epoch 1130, training loss: 62.54712677001953 = 0.09606703370809555 + 10.0 * 6.245106220245361
Epoch 1130, val loss: 0.8705922961235046
Epoch 1140, training loss: 62.53159713745117 = 0.09326466917991638 + 10.0 * 6.243833065032959
Epoch 1140, val loss: 0.8756741285324097
Epoch 1150, training loss: 62.53363037109375 = 0.0905786082148552 + 10.0 * 6.24430513381958
Epoch 1150, val loss: 0.8808002471923828
Epoch 1160, training loss: 62.54827117919922 = 0.08795320242643356 + 10.0 * 6.246031761169434
Epoch 1160, val loss: 0.8857665657997131
Epoch 1170, training loss: 62.51906967163086 = 0.08540135622024536 + 10.0 * 6.2433671951293945
Epoch 1170, val loss: 0.8908637762069702
Epoch 1180, training loss: 62.5302848815918 = 0.0829654186964035 + 10.0 * 6.244731903076172
Epoch 1180, val loss: 0.8958463668823242
Epoch 1190, training loss: 62.512691497802734 = 0.08059635013341904 + 10.0 * 6.243209362030029
Epoch 1190, val loss: 0.9009760618209839
Epoch 1200, training loss: 62.49457931518555 = 0.07829133421182632 + 10.0 * 6.241628646850586
Epoch 1200, val loss: 0.9058891534805298
Epoch 1210, training loss: 62.479671478271484 = 0.07610072195529938 + 10.0 * 6.240357398986816
Epoch 1210, val loss: 0.9111353754997253
Epoch 1220, training loss: 62.47488021850586 = 0.07399318367242813 + 10.0 * 6.24008846282959
Epoch 1220, val loss: 0.9163472056388855
Epoch 1230, training loss: 62.487892150878906 = 0.07196617871522903 + 10.0 * 6.2415924072265625
Epoch 1230, val loss: 0.921614944934845
Epoch 1240, training loss: 62.505828857421875 = 0.06996313482522964 + 10.0 * 6.243586540222168
Epoch 1240, val loss: 0.9264888763427734
Epoch 1250, training loss: 62.45698928833008 = 0.06802113354206085 + 10.0 * 6.23889684677124
Epoch 1250, val loss: 0.9315357208251953
Epoch 1260, training loss: 62.45484161376953 = 0.06615717709064484 + 10.0 * 6.238868236541748
Epoch 1260, val loss: 0.9365302324295044
Epoch 1270, training loss: 62.44612121582031 = 0.0643979087471962 + 10.0 * 6.2381720542907715
Epoch 1270, val loss: 0.9418123960494995
Epoch 1280, training loss: 62.4537239074707 = 0.06267700344324112 + 10.0 * 6.239104747772217
Epoch 1280, val loss: 0.9467625617980957
Epoch 1290, training loss: 62.439964294433594 = 0.06100407987833023 + 10.0 * 6.237895965576172
Epoch 1290, val loss: 0.9518024325370789
Epoch 1300, training loss: 62.43170166015625 = 0.059370264410972595 + 10.0 * 6.2372331619262695
Epoch 1300, val loss: 0.9567995667457581
Epoch 1310, training loss: 62.4249153137207 = 0.05781512334942818 + 10.0 * 6.236710071563721
Epoch 1310, val loss: 0.9618698954582214
Epoch 1320, training loss: 62.41309356689453 = 0.05631853640079498 + 10.0 * 6.235677242279053
Epoch 1320, val loss: 0.9668444395065308
Epoch 1330, training loss: 62.458675384521484 = 0.054877277463674545 + 10.0 * 6.240379810333252
Epoch 1330, val loss: 0.9718605279922485
Epoch 1340, training loss: 62.481834411621094 = 0.05344152823090553 + 10.0 * 6.242839336395264
Epoch 1340, val loss: 0.9765081405639648
Epoch 1350, training loss: 62.398868560791016 = 0.052061647176742554 + 10.0 * 6.234680652618408
Epoch 1350, val loss: 0.9815180897712708
Epoch 1360, training loss: 62.400184631347656 = 0.050745639950037 + 10.0 * 6.234943866729736
Epoch 1360, val loss: 0.9864048957824707
Epoch 1370, training loss: 62.391578674316406 = 0.04948282614350319 + 10.0 * 6.2342095375061035
Epoch 1370, val loss: 0.9913162589073181
Epoch 1380, training loss: 62.381744384765625 = 0.04826228693127632 + 10.0 * 6.2333478927612305
Epoch 1380, val loss: 0.9961857795715332
Epoch 1390, training loss: 62.387725830078125 = 0.04708212986588478 + 10.0 * 6.23406457901001
Epoch 1390, val loss: 1.0009444952011108
Epoch 1400, training loss: 62.4251823425293 = 0.045931149274110794 + 10.0 * 6.237925052642822
Epoch 1400, val loss: 1.005663514137268
Epoch 1410, training loss: 62.380611419677734 = 0.04479500651359558 + 10.0 * 6.23358154296875
Epoch 1410, val loss: 1.0104049444198608
Epoch 1420, training loss: 62.385650634765625 = 0.0437152199447155 + 10.0 * 6.234193325042725
Epoch 1420, val loss: 1.0153067111968994
Epoch 1430, training loss: 62.37968063354492 = 0.04266786947846413 + 10.0 * 6.233701229095459
Epoch 1430, val loss: 1.020019292831421
Epoch 1440, training loss: 62.364959716796875 = 0.041665218770504 + 10.0 * 6.232329368591309
Epoch 1440, val loss: 1.02458655834198
Epoch 1450, training loss: 62.39605712890625 = 0.040690094232559204 + 10.0 * 6.235536575317383
Epoch 1450, val loss: 1.0292755365371704
Epoch 1460, training loss: 62.366886138916016 = 0.03973635658621788 + 10.0 * 6.232714653015137
Epoch 1460, val loss: 1.0340231657028198
Epoch 1470, training loss: 62.34550094604492 = 0.03882528468966484 + 10.0 * 6.230667591094971
Epoch 1470, val loss: 1.0386191606521606
Epoch 1480, training loss: 62.34986877441406 = 0.0379474051296711 + 10.0 * 6.231192111968994
Epoch 1480, val loss: 1.0433247089385986
Epoch 1490, training loss: 62.41757583618164 = 0.03708714246749878 + 10.0 * 6.238049030303955
Epoch 1490, val loss: 1.0479482412338257
Epoch 1500, training loss: 62.361236572265625 = 0.03622254356741905 + 10.0 * 6.232501029968262
Epoch 1500, val loss: 1.0521687269210815
Epoch 1510, training loss: 62.336402893066406 = 0.035422589629888535 + 10.0 * 6.230097770690918
Epoch 1510, val loss: 1.0568934679031372
Epoch 1520, training loss: 62.35206604003906 = 0.034641068428754807 + 10.0 * 6.2317423820495605
Epoch 1520, val loss: 1.0613210201263428
Epoch 1530, training loss: 62.33287811279297 = 0.03387300297617912 + 10.0 * 6.229900360107422
Epoch 1530, val loss: 1.0657355785369873
Epoch 1540, training loss: 62.31561279296875 = 0.03313055261969566 + 10.0 * 6.228248119354248
Epoch 1540, val loss: 1.070167899131775
Epoch 1550, training loss: 62.310787200927734 = 0.03242350369691849 + 10.0 * 6.2278361320495605
Epoch 1550, val loss: 1.0746514797210693
Epoch 1560, training loss: 62.312278747558594 = 0.031733494251966476 + 10.0 * 6.228054523468018
Epoch 1560, val loss: 1.0790200233459473
Epoch 1570, training loss: 62.3786735534668 = 0.03107631579041481 + 10.0 * 6.23475980758667
Epoch 1570, val loss: 1.083513855934143
Epoch 1580, training loss: 62.33797073364258 = 0.030398942530155182 + 10.0 * 6.230757236480713
Epoch 1580, val loss: 1.0873280763626099
Epoch 1590, training loss: 62.3132438659668 = 0.029761791229248047 + 10.0 * 6.228348255157471
Epoch 1590, val loss: 1.0919264554977417
Epoch 1600, training loss: 62.31404495239258 = 0.029140781611204147 + 10.0 * 6.228490352630615
Epoch 1600, val loss: 1.0961276292800903
Epoch 1610, training loss: 62.31538009643555 = 0.02853911742568016 + 10.0 * 6.228684425354004
Epoch 1610, val loss: 1.100185751914978
Epoch 1620, training loss: 62.28844451904297 = 0.02795320563018322 + 10.0 * 6.226048946380615
Epoch 1620, val loss: 1.1045591831207275
Epoch 1630, training loss: 62.28499984741211 = 0.027394253760576248 + 10.0 * 6.225760459899902
Epoch 1630, val loss: 1.1086498498916626
Epoch 1640, training loss: 62.27717208862305 = 0.02685135416686535 + 10.0 * 6.225031852722168
Epoch 1640, val loss: 1.1129283905029297
Epoch 1650, training loss: 62.28034591674805 = 0.02633107453584671 + 10.0 * 6.225401401519775
Epoch 1650, val loss: 1.117066502571106
Epoch 1660, training loss: 62.366641998291016 = 0.025821790099143982 + 10.0 * 6.234082221984863
Epoch 1660, val loss: 1.1211891174316406
Epoch 1670, training loss: 62.28554916381836 = 0.025294966995716095 + 10.0 * 6.226025581359863
Epoch 1670, val loss: 1.124948501586914
Epoch 1680, training loss: 62.26192855834961 = 0.024806741625070572 + 10.0 * 6.223711967468262
Epoch 1680, val loss: 1.1291075944900513
Epoch 1690, training loss: 62.257938385009766 = 0.024337060749530792 + 10.0 * 6.223360061645508
Epoch 1690, val loss: 1.1331768035888672
Epoch 1700, training loss: 62.27432632446289 = 0.02388267032802105 + 10.0 * 6.225044250488281
Epoch 1700, val loss: 1.1370618343353271
Epoch 1710, training loss: 62.26987838745117 = 0.023426691070199013 + 10.0 * 6.224645137786865
Epoch 1710, val loss: 1.1409181356430054
Epoch 1720, training loss: 62.26343536376953 = 0.02298436313867569 + 10.0 * 6.224045276641846
Epoch 1720, val loss: 1.1448547840118408
Epoch 1730, training loss: 62.278953552246094 = 0.022559532895684242 + 10.0 * 6.225639343261719
Epoch 1730, val loss: 1.1488043069839478
Epoch 1740, training loss: 62.25244140625 = 0.022145826369524002 + 10.0 * 6.223029613494873
Epoch 1740, val loss: 1.1527138948440552
Epoch 1750, training loss: 62.246482849121094 = 0.02174667827785015 + 10.0 * 6.222473621368408
Epoch 1750, val loss: 1.1565827131271362
Epoch 1760, training loss: 62.27104568481445 = 0.021364036947488785 + 10.0 * 6.224968433380127
Epoch 1760, val loss: 1.1604207754135132
Epoch 1770, training loss: 62.23862838745117 = 0.02096843719482422 + 10.0 * 6.221765995025635
Epoch 1770, val loss: 1.1640312671661377
Epoch 1780, training loss: 62.25497055053711 = 0.020591476932168007 + 10.0 * 6.223437786102295
Epoch 1780, val loss: 1.1676998138427734
Epoch 1790, training loss: 62.238800048828125 = 0.020227717235684395 + 10.0 * 6.221857070922852
Epoch 1790, val loss: 1.1713664531707764
Epoch 1800, training loss: 62.234107971191406 = 0.01988115720450878 + 10.0 * 6.2214226722717285
Epoch 1800, val loss: 1.1751302480697632
Epoch 1810, training loss: 62.250057220458984 = 0.019541524350643158 + 10.0 * 6.22305154800415
Epoch 1810, val loss: 1.1786853075027466
Epoch 1820, training loss: 62.238704681396484 = 0.019202355295419693 + 10.0 * 6.221950054168701
Epoch 1820, val loss: 1.1823171377182007
Epoch 1830, training loss: 62.234745025634766 = 0.018873140215873718 + 10.0 * 6.221587181091309
Epoch 1830, val loss: 1.1859744787216187
Epoch 1840, training loss: 62.22355270385742 = 0.018557412549853325 + 10.0 * 6.220499515533447
Epoch 1840, val loss: 1.1896172761917114
Epoch 1850, training loss: 62.273193359375 = 0.018258940428495407 + 10.0 * 6.225493431091309
Epoch 1850, val loss: 1.193377137184143
Epoch 1860, training loss: 62.24803924560547 = 0.017938366159796715 + 10.0 * 6.223010063171387
Epoch 1860, val loss: 1.1963390111923218
Epoch 1870, training loss: 62.22592544555664 = 0.017639923840761185 + 10.0 * 6.220828533172607
Epoch 1870, val loss: 1.200104832649231
Epoch 1880, training loss: 62.21145248413086 = 0.017348257824778557 + 10.0 * 6.219410419464111
Epoch 1880, val loss: 1.2034260034561157
Epoch 1890, training loss: 62.20768737792969 = 0.017074700444936752 + 10.0 * 6.219061374664307
Epoch 1890, val loss: 1.2069604396820068
Epoch 1900, training loss: 62.249691009521484 = 0.016802437603473663 + 10.0 * 6.223288536071777
Epoch 1900, val loss: 1.2102088928222656
Epoch 1910, training loss: 62.21192932128906 = 0.016531243920326233 + 10.0 * 6.219539642333984
Epoch 1910, val loss: 1.2136863470077515
Epoch 1920, training loss: 62.1964225769043 = 0.016267430037260056 + 10.0 * 6.218015670776367
Epoch 1920, val loss: 1.2169915437698364
Epoch 1930, training loss: 62.20069122314453 = 0.01601666398346424 + 10.0 * 6.2184672355651855
Epoch 1930, val loss: 1.2204408645629883
Epoch 1940, training loss: 62.26313018798828 = 0.015779444947838783 + 10.0 * 6.224735260009766
Epoch 1940, val loss: 1.2236418724060059
Epoch 1950, training loss: 62.213836669921875 = 0.01551789790391922 + 10.0 * 6.219831943511963
Epoch 1950, val loss: 1.2269964218139648
Epoch 1960, training loss: 62.19947052001953 = 0.015286222100257874 + 10.0 * 6.218418598175049
Epoch 1960, val loss: 1.2302567958831787
Epoch 1970, training loss: 62.204627990722656 = 0.01505252905189991 + 10.0 * 6.218957424163818
Epoch 1970, val loss: 1.2335035800933838
Epoch 1980, training loss: 62.199153900146484 = 0.014827978797256947 + 10.0 * 6.218432426452637
Epoch 1980, val loss: 1.23677659034729
Epoch 1990, training loss: 62.200923919677734 = 0.014607925899326801 + 10.0 * 6.218631744384766
Epoch 1990, val loss: 1.2399336099624634
Epoch 2000, training loss: 62.192771911621094 = 0.014388440176844597 + 10.0 * 6.217838287353516
Epoch 2000, val loss: 1.2430919408798218
Epoch 2010, training loss: 62.189056396484375 = 0.01417844183743 + 10.0 * 6.217487812042236
Epoch 2010, val loss: 1.2462846040725708
Epoch 2020, training loss: 62.216732025146484 = 0.013972684741020203 + 10.0 * 6.22027587890625
Epoch 2020, val loss: 1.2494139671325684
Epoch 2030, training loss: 62.187156677246094 = 0.013768861070275307 + 10.0 * 6.217339038848877
Epoch 2030, val loss: 1.2526271343231201
Epoch 2040, training loss: 62.18602752685547 = 0.0135733000934124 + 10.0 * 6.217245578765869
Epoch 2040, val loss: 1.255688190460205
Epoch 2050, training loss: 62.19724655151367 = 0.013383930549025536 + 10.0 * 6.218386173248291
Epoch 2050, val loss: 1.2588263750076294
Epoch 2060, training loss: 62.191429138183594 = 0.013191831298172474 + 10.0 * 6.2178239822387695
Epoch 2060, val loss: 1.2617160081863403
Epoch 2070, training loss: 62.18600082397461 = 0.01300159189850092 + 10.0 * 6.217299938201904
Epoch 2070, val loss: 1.2646503448486328
Epoch 2080, training loss: 62.16901779174805 = 0.012822137214243412 + 10.0 * 6.2156195640563965
Epoch 2080, val loss: 1.2677289247512817
Epoch 2090, training loss: 62.171669006347656 = 0.012648910284042358 + 10.0 * 6.215901851654053
Epoch 2090, val loss: 1.2707831859588623
Epoch 2100, training loss: 62.20850372314453 = 0.012476633302867413 + 10.0 * 6.219602584838867
Epoch 2100, val loss: 1.2735828161239624
Epoch 2110, training loss: 62.187381744384766 = 0.012301157228648663 + 10.0 * 6.217507839202881
Epoch 2110, val loss: 1.2763571739196777
Epoch 2120, training loss: 62.19602966308594 = 0.012137631885707378 + 10.0 * 6.21838903427124
Epoch 2120, val loss: 1.2793489694595337
Epoch 2130, training loss: 62.16775894165039 = 0.011972630396485329 + 10.0 * 6.215578556060791
Epoch 2130, val loss: 1.2822751998901367
Epoch 2140, training loss: 62.18027877807617 = 0.011819088831543922 + 10.0 * 6.216845989227295
Epoch 2140, val loss: 1.28522527217865
Epoch 2150, training loss: 62.167686462402344 = 0.011660258285701275 + 10.0 * 6.215602397918701
Epoch 2150, val loss: 1.2878799438476562
Epoch 2160, training loss: 62.16126251220703 = 0.011503593064844608 + 10.0 * 6.214975833892822
Epoch 2160, val loss: 1.2907572984695435
Epoch 2170, training loss: 62.14767837524414 = 0.01135706901550293 + 10.0 * 6.213632106781006
Epoch 2170, val loss: 1.2936521768569946
Epoch 2180, training loss: 62.15259552001953 = 0.011214231140911579 + 10.0 * 6.214138031005859
Epoch 2180, val loss: 1.2964272499084473
Epoch 2190, training loss: 62.204566955566406 = 0.011071033775806427 + 10.0 * 6.2193498611450195
Epoch 2190, val loss: 1.298970341682434
Epoch 2200, training loss: 62.165435791015625 = 0.010929632931947708 + 10.0 * 6.215450763702393
Epoch 2200, val loss: 1.3019380569458008
Epoch 2210, training loss: 62.160526275634766 = 0.01079059299081564 + 10.0 * 6.214973449707031
Epoch 2210, val loss: 1.3044989109039307
Epoch 2220, training loss: 62.18576431274414 = 0.010657728649675846 + 10.0 * 6.21751070022583
Epoch 2220, val loss: 1.3071749210357666
Epoch 2230, training loss: 62.1612434387207 = 0.010519790463149548 + 10.0 * 6.215072154998779
Epoch 2230, val loss: 1.309815526008606
Epoch 2240, training loss: 62.14482116699219 = 0.01039193756878376 + 10.0 * 6.213442802429199
Epoch 2240, val loss: 1.3128231763839722
Epoch 2250, training loss: 62.149169921875 = 0.010268351063132286 + 10.0 * 6.213890075683594
Epoch 2250, val loss: 1.3153340816497803
Epoch 2260, training loss: 62.178707122802734 = 0.010148836299777031 + 10.0 * 6.216856002807617
Epoch 2260, val loss: 1.3179415464401245
Epoch 2270, training loss: 62.15214920043945 = 0.010015184991061687 + 10.0 * 6.2142133712768555
Epoch 2270, val loss: 1.3204784393310547
Epoch 2280, training loss: 62.15266799926758 = 0.009900529868900776 + 10.0 * 6.2142767906188965
Epoch 2280, val loss: 1.3231216669082642
Epoch 2290, training loss: 62.12844467163086 = 0.009779542684555054 + 10.0 * 6.21186637878418
Epoch 2290, val loss: 1.3257118463516235
Epoch 2300, training loss: 62.123634338378906 = 0.009666630066931248 + 10.0 * 6.21139669418335
Epoch 2300, val loss: 1.328242301940918
Epoch 2310, training loss: 62.14581298828125 = 0.00955817848443985 + 10.0 * 6.213625431060791
Epoch 2310, val loss: 1.330926537513733
Epoch 2320, training loss: 62.17829132080078 = 0.00944705493748188 + 10.0 * 6.216884613037109
Epoch 2320, val loss: 1.3335211277008057
Epoch 2330, training loss: 62.124420166015625 = 0.009330584667623043 + 10.0 * 6.211508750915527
Epoch 2330, val loss: 1.3355538845062256
Epoch 2340, training loss: 62.11406707763672 = 0.009223845787346363 + 10.0 * 6.210484504699707
Epoch 2340, val loss: 1.3382203578948975
Epoch 2350, training loss: 62.112998962402344 = 0.009121695533394814 + 10.0 * 6.210387706756592
Epoch 2350, val loss: 1.3407220840454102
Epoch 2360, training loss: 62.124366760253906 = 0.009022369049489498 + 10.0 * 6.21153450012207
Epoch 2360, val loss: 1.3431612253189087
Epoch 2370, training loss: 62.15668869018555 = 0.008921532891690731 + 10.0 * 6.214776515960693
Epoch 2370, val loss: 1.3454298973083496
Epoch 2380, training loss: 62.13542938232422 = 0.008817276917397976 + 10.0 * 6.212661266326904
Epoch 2380, val loss: 1.3478116989135742
Epoch 2390, training loss: 62.11632537841797 = 0.008721002377569675 + 10.0 * 6.210760593414307
Epoch 2390, val loss: 1.3502440452575684
Epoch 2400, training loss: 62.1054801940918 = 0.0086263008415699 + 10.0 * 6.209685325622559
Epoch 2400, val loss: 1.3526008129119873
Epoch 2410, training loss: 62.152626037597656 = 0.008538275957107544 + 10.0 * 6.214408874511719
Epoch 2410, val loss: 1.3550506830215454
Epoch 2420, training loss: 62.106224060058594 = 0.00844072550535202 + 10.0 * 6.209778308868408
Epoch 2420, val loss: 1.3572076559066772
Epoch 2430, training loss: 62.10403060913086 = 0.00834733061492443 + 10.0 * 6.209568500518799
Epoch 2430, val loss: 1.3595428466796875
Epoch 2440, training loss: 62.102664947509766 = 0.008258834481239319 + 10.0 * 6.2094407081604
Epoch 2440, val loss: 1.3619027137756348
Epoch 2450, training loss: 62.1255989074707 = 0.008175165392458439 + 10.0 * 6.211742401123047
Epoch 2450, val loss: 1.3642523288726807
Epoch 2460, training loss: 62.11836624145508 = 0.008089406415820122 + 10.0 * 6.2110276222229
Epoch 2460, val loss: 1.366429090499878
Epoch 2470, training loss: 62.10586929321289 = 0.008005588315427303 + 10.0 * 6.209786415100098
Epoch 2470, val loss: 1.368622899055481
Epoch 2480, training loss: 62.096160888671875 = 0.007920732721686363 + 10.0 * 6.208824157714844
Epoch 2480, val loss: 1.3708401918411255
Epoch 2490, training loss: 62.09856414794922 = 0.007842113263905048 + 10.0 * 6.209072113037109
Epoch 2490, val loss: 1.3730148077011108
Epoch 2500, training loss: 62.13380432128906 = 0.00776535551995039 + 10.0 * 6.212603569030762
Epoch 2500, val loss: 1.3752065896987915
Epoch 2510, training loss: 62.09526443481445 = 0.007685069926083088 + 10.0 * 6.2087578773498535
Epoch 2510, val loss: 1.3774664402008057
Epoch 2520, training loss: 62.13654708862305 = 0.007610792759805918 + 10.0 * 6.212893486022949
Epoch 2520, val loss: 1.3797969818115234
Epoch 2530, training loss: 62.099639892578125 = 0.007531443610787392 + 10.0 * 6.2092108726501465
Epoch 2530, val loss: 1.3815826177597046
Epoch 2540, training loss: 62.09356689453125 = 0.0074539449997246265 + 10.0 * 6.208611488342285
Epoch 2540, val loss: 1.3836785554885864
Epoch 2550, training loss: 62.08517074584961 = 0.007384029682725668 + 10.0 * 6.207778453826904
Epoch 2550, val loss: 1.3859481811523438
Epoch 2560, training loss: 62.17790985107422 = 0.007317621260881424 + 10.0 * 6.217059135437012
Epoch 2560, val loss: 1.387892484664917
Epoch 2570, training loss: 62.121482849121094 = 0.007241728715598583 + 10.0 * 6.211424350738525
Epoch 2570, val loss: 1.3898857831954956
Epoch 2580, training loss: 62.086055755615234 = 0.007168823853135109 + 10.0 * 6.207888603210449
Epoch 2580, val loss: 1.392050862312317
Epoch 2590, training loss: 62.076866149902344 = 0.007101637311279774 + 10.0 * 6.206976413726807
Epoch 2590, val loss: 1.3941487073898315
Epoch 2600, training loss: 62.084136962890625 = 0.00703665055334568 + 10.0 * 6.207709789276123
Epoch 2600, val loss: 1.396201729774475
Epoch 2610, training loss: 62.161380767822266 = 0.006971732713282108 + 10.0 * 6.21544075012207
Epoch 2610, val loss: 1.3980931043624878
Epoch 2620, training loss: 62.10885238647461 = 0.006907089613378048 + 10.0 * 6.2101945877075195
Epoch 2620, val loss: 1.3998908996582031
Epoch 2630, training loss: 62.13487243652344 = 0.006839368958026171 + 10.0 * 6.212803363800049
Epoch 2630, val loss: 1.401747226715088
Epoch 2640, training loss: 62.080810546875 = 0.0067769312299788 + 10.0 * 6.207403182983398
Epoch 2640, val loss: 1.4039233922958374
Epoch 2650, training loss: 62.08318328857422 = 0.006715255789458752 + 10.0 * 6.20764684677124
Epoch 2650, val loss: 1.4058947563171387
Epoch 2660, training loss: 62.087467193603516 = 0.006655230186879635 + 10.0 * 6.208081245422363
Epoch 2660, val loss: 1.4076919555664062
Epoch 2670, training loss: 62.09912872314453 = 0.006595754064619541 + 10.0 * 6.209253311157227
Epoch 2670, val loss: 1.409660816192627
Epoch 2680, training loss: 62.0714111328125 = 0.006537817884236574 + 10.0 * 6.20648717880249
Epoch 2680, val loss: 1.411645770072937
Epoch 2690, training loss: 62.0786247253418 = 0.0064804465509951115 + 10.0 * 6.20721435546875
Epoch 2690, val loss: 1.4134694337844849
Epoch 2700, training loss: 62.13884735107422 = 0.006423210725188255 + 10.0 * 6.213242530822754
Epoch 2700, val loss: 1.4150079488754272
Epoch 2710, training loss: 62.087703704833984 = 0.006366563495248556 + 10.0 * 6.208133697509766
Epoch 2710, val loss: 1.4171768426895142
Epoch 2720, training loss: 62.0731086730957 = 0.006310678087174892 + 10.0 * 6.206679821014404
Epoch 2720, val loss: 1.418966293334961
Epoch 2730, training loss: 62.08510208129883 = 0.006260262802243233 + 10.0 * 6.207884311676025
Epoch 2730, val loss: 1.420788049697876
Epoch 2740, training loss: 62.08755111694336 = 0.006204843055456877 + 10.0 * 6.208134651184082
Epoch 2740, val loss: 1.4224886894226074
Epoch 2750, training loss: 62.07379150390625 = 0.006149088963866234 + 10.0 * 6.206764221191406
Epoch 2750, val loss: 1.424296498298645
Epoch 2760, training loss: 62.06950378417969 = 0.006098750978708267 + 10.0 * 6.206340312957764
Epoch 2760, val loss: 1.426137924194336
Epoch 2770, training loss: 62.07941436767578 = 0.006047162692993879 + 10.0 * 6.207336902618408
Epoch 2770, val loss: 1.427789568901062
Epoch 2780, training loss: 62.096435546875 = 0.005996581632643938 + 10.0 * 6.209043979644775
Epoch 2780, val loss: 1.4292882680892944
Epoch 2790, training loss: 62.07364273071289 = 0.005945990793406963 + 10.0 * 6.2067694664001465
Epoch 2790, val loss: 1.4312013387680054
Epoch 2800, training loss: 62.05774688720703 = 0.005898637231439352 + 10.0 * 6.2051849365234375
Epoch 2800, val loss: 1.4330649375915527
Epoch 2810, training loss: 62.060943603515625 = 0.005851939786225557 + 10.0 * 6.205509185791016
Epoch 2810, val loss: 1.4347548484802246
Epoch 2820, training loss: 62.087825775146484 = 0.005804982502013445 + 10.0 * 6.208201885223389
Epoch 2820, val loss: 1.4363189935684204
Epoch 2830, training loss: 62.08497619628906 = 0.005756175145506859 + 10.0 * 6.207921981811523
Epoch 2830, val loss: 1.4378420114517212
Epoch 2840, training loss: 62.0596923828125 = 0.0057112909853458405 + 10.0 * 6.205398082733154
Epoch 2840, val loss: 1.4397764205932617
Epoch 2850, training loss: 62.05757141113281 = 0.005665957927703857 + 10.0 * 6.205190658569336
Epoch 2850, val loss: 1.4415067434310913
Epoch 2860, training loss: 62.0963134765625 = 0.0056219035759568214 + 10.0 * 6.20906925201416
Epoch 2860, val loss: 1.4430749416351318
Epoch 2870, training loss: 62.07101058959961 = 0.005578864831477404 + 10.0 * 6.206543445587158
Epoch 2870, val loss: 1.4445198774337769
Epoch 2880, training loss: 62.06571960449219 = 0.005533581133931875 + 10.0 * 6.206018447875977
Epoch 2880, val loss: 1.4463716745376587
Epoch 2890, training loss: 62.04673385620117 = 0.00549106253311038 + 10.0 * 6.204124450683594
Epoch 2890, val loss: 1.447950005531311
Epoch 2900, training loss: 62.04695510864258 = 0.005449201446026564 + 10.0 * 6.204150676727295
Epoch 2900, val loss: 1.449571967124939
Epoch 2910, training loss: 62.05565643310547 = 0.005409530829638243 + 10.0 * 6.205024719238281
Epoch 2910, val loss: 1.4512102603912354
Epoch 2920, training loss: 62.084693908691406 = 0.0053713261149823666 + 10.0 * 6.207932472229004
Epoch 2920, val loss: 1.4527716636657715
Epoch 2930, training loss: 62.058406829833984 = 0.005328429862856865 + 10.0 * 6.205307960510254
Epoch 2930, val loss: 1.4542871713638306
Epoch 2940, training loss: 62.04661178588867 = 0.0052879732102155685 + 10.0 * 6.204132556915283
Epoch 2940, val loss: 1.4559261798858643
Epoch 2950, training loss: 62.07450866699219 = 0.005250037182122469 + 10.0 * 6.206925868988037
Epoch 2950, val loss: 1.4574846029281616
Epoch 2960, training loss: 62.0382080078125 = 0.005209664348512888 + 10.0 * 6.203299522399902
Epoch 2960, val loss: 1.4588165283203125
Epoch 2970, training loss: 62.06525802612305 = 0.0051734051667153835 + 10.0 * 6.206008434295654
Epoch 2970, val loss: 1.4604374170303345
Epoch 2980, training loss: 62.04003143310547 = 0.00513403071090579 + 10.0 * 6.203489780426025
Epoch 2980, val loss: 1.4618438482284546
Epoch 2990, training loss: 62.0367317199707 = 0.005096052773296833 + 10.0 * 6.2031636238098145
Epoch 2990, val loss: 1.4633541107177734
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 87.92484283447266 = 1.9561446905136108 + 10.0 * 8.596869468688965
Epoch 0, val loss: 1.9633095264434814
Epoch 10, training loss: 87.90968322753906 = 1.946431040763855 + 10.0 * 8.596324920654297
Epoch 10, val loss: 1.9533610343933105
Epoch 20, training loss: 87.84432983398438 = 1.9346046447753906 + 10.0 * 8.590971946716309
Epoch 20, val loss: 1.9410145282745361
Epoch 30, training loss: 87.4167251586914 = 1.919089436531067 + 10.0 * 8.549763679504395
Epoch 30, val loss: 1.9246114492416382
Epoch 40, training loss: 85.05419921875 = 1.9008104801177979 + 10.0 * 8.315339088439941
Epoch 40, val loss: 1.905776858329773
Epoch 50, training loss: 80.39335632324219 = 1.8830604553222656 + 10.0 * 7.851029872894287
Epoch 50, val loss: 1.8873369693756104
Epoch 60, training loss: 76.29383850097656 = 1.8677505254745483 + 10.0 * 7.44260835647583
Epoch 60, val loss: 1.8717148303985596
Epoch 70, training loss: 72.84441375732422 = 1.852734923362732 + 10.0 * 7.099167346954346
Epoch 70, val loss: 1.8562629222869873
Epoch 80, training loss: 71.0466537475586 = 1.838777780532837 + 10.0 * 6.920787811279297
Epoch 80, val loss: 1.841827392578125
Epoch 90, training loss: 70.1180419921875 = 1.8249011039733887 + 10.0 * 6.829314708709717
Epoch 90, val loss: 1.8272967338562012
Epoch 100, training loss: 69.24668884277344 = 1.810625433921814 + 10.0 * 6.7436065673828125
Epoch 100, val loss: 1.8127210140228271
Epoch 110, training loss: 68.58304595947266 = 1.7969931364059448 + 10.0 * 6.678605556488037
Epoch 110, val loss: 1.7988383769989014
Epoch 120, training loss: 68.1248550415039 = 1.7836138010025024 + 10.0 * 6.634124279022217
Epoch 120, val loss: 1.7851358652114868
Epoch 130, training loss: 67.7297134399414 = 1.7697150707244873 + 10.0 * 6.5960001945495605
Epoch 130, val loss: 1.7711002826690674
Epoch 140, training loss: 67.45687103271484 = 1.7554967403411865 + 10.0 * 6.570137977600098
Epoch 140, val loss: 1.7567439079284668
Epoch 150, training loss: 67.12913513183594 = 1.7405301332473755 + 10.0 * 6.538860321044922
Epoch 150, val loss: 1.742276668548584
Epoch 160, training loss: 66.88743591308594 = 1.7247117757797241 + 10.0 * 6.51627254486084
Epoch 160, val loss: 1.7270557880401611
Epoch 170, training loss: 66.6723403930664 = 1.7076812982559204 + 10.0 * 6.496465682983398
Epoch 170, val loss: 1.710915446281433
Epoch 180, training loss: 66.51014709472656 = 1.6891363859176636 + 10.0 * 6.4821014404296875
Epoch 180, val loss: 1.6934696435928345
Epoch 190, training loss: 66.31607055664062 = 1.6688963174819946 + 10.0 * 6.464716911315918
Epoch 190, val loss: 1.6744142770767212
Epoch 200, training loss: 66.15251922607422 = 1.646933674812317 + 10.0 * 6.450558662414551
Epoch 200, val loss: 1.6539865732192993
Epoch 210, training loss: 65.99037170410156 = 1.6232380867004395 + 10.0 * 6.436713695526123
Epoch 210, val loss: 1.632081389427185
Epoch 220, training loss: 65.87109375 = 1.597625732421875 + 10.0 * 6.427346706390381
Epoch 220, val loss: 1.6087262630462646
Epoch 230, training loss: 65.72572326660156 = 1.5701366662979126 + 10.0 * 6.415558815002441
Epoch 230, val loss: 1.583646535873413
Epoch 240, training loss: 65.6081314086914 = 1.5408539772033691 + 10.0 * 6.406727313995361
Epoch 240, val loss: 1.5573084354400635
Epoch 250, training loss: 65.50467681884766 = 1.5099568367004395 + 10.0 * 6.399471759796143
Epoch 250, val loss: 1.5297787189483643
Epoch 260, training loss: 65.41331481933594 = 1.4777357578277588 + 10.0 * 6.393558025360107
Epoch 260, val loss: 1.5012528896331787
Epoch 270, training loss: 65.2834243774414 = 1.4443347454071045 + 10.0 * 6.383908748626709
Epoch 270, val loss: 1.4721299409866333
Epoch 280, training loss: 65.18368530273438 = 1.4101722240447998 + 10.0 * 6.3773512840271
Epoch 280, val loss: 1.442628264427185
Epoch 290, training loss: 65.09803009033203 = 1.375455379486084 + 10.0 * 6.372257709503174
Epoch 290, val loss: 1.4129693508148193
Epoch 300, training loss: 65.04021453857422 = 1.3404383659362793 + 10.0 * 6.369977951049805
Epoch 300, val loss: 1.383299469947815
Epoch 310, training loss: 64.93585968017578 = 1.3055689334869385 + 10.0 * 6.363028526306152
Epoch 310, val loss: 1.3538707494735718
Epoch 320, training loss: 64.88310241699219 = 1.2707303762435913 + 10.0 * 6.361237049102783
Epoch 320, val loss: 1.3249523639678955
Epoch 330, training loss: 64.78111267089844 = 1.236422061920166 + 10.0 * 6.354468822479248
Epoch 330, val loss: 1.2967668771743774
Epoch 340, training loss: 64.70867156982422 = 1.20258629322052 + 10.0 * 6.3506083488464355
Epoch 340, val loss: 1.2693631649017334
Epoch 350, training loss: 64.71292114257812 = 1.1695491075515747 + 10.0 * 6.354337215423584
Epoch 350, val loss: 1.2427740097045898
Epoch 360, training loss: 64.58641815185547 = 1.137026309967041 + 10.0 * 6.344939231872559
Epoch 360, val loss: 1.2172646522521973
Epoch 370, training loss: 64.50590515136719 = 1.105722188949585 + 10.0 * 6.340018272399902
Epoch 370, val loss: 1.1929104328155518
Epoch 380, training loss: 64.4429931640625 = 1.0753960609436035 + 10.0 * 6.336759567260742
Epoch 380, val loss: 1.1696969270706177
Epoch 390, training loss: 64.3864517211914 = 1.0461031198501587 + 10.0 * 6.3340349197387695
Epoch 390, val loss: 1.1475926637649536
Epoch 400, training loss: 64.35145568847656 = 1.0177186727523804 + 10.0 * 6.3333740234375
Epoch 400, val loss: 1.1267863512039185
Epoch 410, training loss: 64.29740142822266 = 0.9905642867088318 + 10.0 * 6.330683708190918
Epoch 410, val loss: 1.106837511062622
Epoch 420, training loss: 64.22219848632812 = 0.9645041823387146 + 10.0 * 6.325769424438477
Epoch 420, val loss: 1.0883126258850098
Epoch 430, training loss: 64.19629669189453 = 0.9395994544029236 + 10.0 * 6.325669765472412
Epoch 430, val loss: 1.0709315538406372
Epoch 440, training loss: 64.12914276123047 = 0.9155394434928894 + 10.0 * 6.321360111236572
Epoch 440, val loss: 1.0545467138290405
Epoch 450, training loss: 64.07864379882812 = 0.8927275538444519 + 10.0 * 6.318591594696045
Epoch 450, val loss: 1.0392628908157349
Epoch 460, training loss: 64.0216293334961 = 0.8707795739173889 + 10.0 * 6.315084934234619
Epoch 460, val loss: 1.0250236988067627
Epoch 470, training loss: 64.00830841064453 = 0.8498455286026001 + 10.0 * 6.3158464431762695
Epoch 470, val loss: 1.0116946697235107
Epoch 480, training loss: 63.959251403808594 = 0.8296193480491638 + 10.0 * 6.312963008880615
Epoch 480, val loss: 0.9991921782493591
Epoch 490, training loss: 63.89842224121094 = 0.810263454914093 + 10.0 * 6.308815956115723
Epoch 490, val loss: 0.9873874187469482
Epoch 500, training loss: 63.88093566894531 = 0.7916349768638611 + 10.0 * 6.308930397033691
Epoch 500, val loss: 0.9764571785926819
Epoch 510, training loss: 63.830345153808594 = 0.7735744714736938 + 10.0 * 6.3056769371032715
Epoch 510, val loss: 0.9660868048667908
Epoch 520, training loss: 63.812110900878906 = 0.7560366988182068 + 10.0 * 6.305607795715332
Epoch 520, val loss: 0.95623779296875
Epoch 530, training loss: 63.76886749267578 = 0.7390368580818176 + 10.0 * 6.302983283996582
Epoch 530, val loss: 0.9468390941619873
Epoch 540, training loss: 63.7103157043457 = 0.7225108742713928 + 10.0 * 6.29878044128418
Epoch 540, val loss: 0.9380483627319336
Epoch 550, training loss: 63.6728401184082 = 0.7063713073730469 + 10.0 * 6.296647071838379
Epoch 550, val loss: 0.9296924471855164
Epoch 560, training loss: 63.63863754272461 = 0.6905009150505066 + 10.0 * 6.294813632965088
Epoch 560, val loss: 0.9217797517776489
Epoch 570, training loss: 63.68288040161133 = 0.6747828722000122 + 10.0 * 6.300809860229492
Epoch 570, val loss: 0.9142594933509827
Epoch 580, training loss: 63.60391616821289 = 0.6592970490455627 + 10.0 * 6.294461727142334
Epoch 580, val loss: 0.9066701531410217
Epoch 590, training loss: 63.56162643432617 = 0.6439245939254761 + 10.0 * 6.291769981384277
Epoch 590, val loss: 0.8995383381843567
Epoch 600, training loss: 63.51662826538086 = 0.6287736296653748 + 10.0 * 6.288785457611084
Epoch 600, val loss: 0.8928347229957581
Epoch 610, training loss: 63.48182678222656 = 0.613750696182251 + 10.0 * 6.286807537078857
Epoch 610, val loss: 0.8864461183547974
Epoch 620, training loss: 63.51206970214844 = 0.5988429188728333 + 10.0 * 6.291322708129883
Epoch 620, val loss: 0.8804247975349426
Epoch 630, training loss: 63.45838928222656 = 0.5840218663215637 + 10.0 * 6.287436485290527
Epoch 630, val loss: 0.8741108179092407
Epoch 640, training loss: 63.41046905517578 = 0.5693042278289795 + 10.0 * 6.284116268157959
Epoch 640, val loss: 0.8683857321739197
Epoch 650, training loss: 63.48963165283203 = 0.5547170639038086 + 10.0 * 6.293491363525391
Epoch 650, val loss: 0.8630240559577942
Epoch 660, training loss: 63.345829010009766 = 0.5403720140457153 + 10.0 * 6.280545711517334
Epoch 660, val loss: 0.8578717112541199
Epoch 670, training loss: 63.32428741455078 = 0.5261663198471069 + 10.0 * 6.279812335968018
Epoch 670, val loss: 0.8531574010848999
Epoch 680, training loss: 63.2862548828125 = 0.5121827721595764 + 10.0 * 6.277407169342041
Epoch 680, val loss: 0.8487629294395447
Epoch 690, training loss: 63.2607536315918 = 0.49839654564857483 + 10.0 * 6.276235580444336
Epoch 690, val loss: 0.8446518778800964
Epoch 700, training loss: 63.32424545288086 = 0.4848272502422333 + 10.0 * 6.283941745758057
Epoch 700, val loss: 0.8407702445983887
Epoch 710, training loss: 63.22370910644531 = 0.4712613821029663 + 10.0 * 6.27524471282959
Epoch 710, val loss: 0.8371819853782654
Epoch 720, training loss: 63.199241638183594 = 0.45807451009750366 + 10.0 * 6.2741169929504395
Epoch 720, val loss: 0.8339521288871765
Epoch 730, training loss: 63.16749572753906 = 0.44513601064682007 + 10.0 * 6.272235870361328
Epoch 730, val loss: 0.8310846090316772
Epoch 740, training loss: 63.14036560058594 = 0.4324577748775482 + 10.0 * 6.270791053771973
Epoch 740, val loss: 0.8286172151565552
Epoch 750, training loss: 63.16170883178711 = 0.4199742376804352 + 10.0 * 6.274173259735107
Epoch 750, val loss: 0.8265411257743835
Epoch 760, training loss: 63.12958908081055 = 0.40775051712989807 + 10.0 * 6.272183895111084
Epoch 760, val loss: 0.8243088126182556
Epoch 770, training loss: 63.08848571777344 = 0.3956979513168335 + 10.0 * 6.269278526306152
Epoch 770, val loss: 0.8227903842926025
Epoch 780, training loss: 63.072174072265625 = 0.38394874334335327 + 10.0 * 6.26882266998291
Epoch 780, val loss: 0.8215727210044861
Epoch 790, training loss: 63.03690719604492 = 0.372422456741333 + 10.0 * 6.266448497772217
Epoch 790, val loss: 0.8206605911254883
Epoch 800, training loss: 63.02635192871094 = 0.3611434996128082 + 10.0 * 6.2665205001831055
Epoch 800, val loss: 0.8198915123939514
Epoch 810, training loss: 63.031036376953125 = 0.3500617444515228 + 10.0 * 6.268097400665283
Epoch 810, val loss: 0.8194866180419922
Epoch 820, training loss: 62.986610412597656 = 0.33911001682281494 + 10.0 * 6.264750003814697
Epoch 820, val loss: 0.8193047046661377
Epoch 830, training loss: 62.983245849609375 = 0.32846957445144653 + 10.0 * 6.265477657318115
Epoch 830, val loss: 0.8192854523658752
Epoch 840, training loss: 62.96809005737305 = 0.31798622012138367 + 10.0 * 6.265010356903076
Epoch 840, val loss: 0.8196771144866943
Epoch 850, training loss: 62.93918228149414 = 0.30774256587028503 + 10.0 * 6.263144016265869
Epoch 850, val loss: 0.8199983835220337
Epoch 860, training loss: 62.90168380737305 = 0.29772549867630005 + 10.0 * 6.2603960037231445
Epoch 860, val loss: 0.8207613229751587
Epoch 870, training loss: 62.880348205566406 = 0.28795555233955383 + 10.0 * 6.259239196777344
Epoch 870, val loss: 0.8217765092849731
Epoch 880, training loss: 62.86513137817383 = 0.27840638160705566 + 10.0 * 6.25867223739624
Epoch 880, val loss: 0.8229987025260925
Epoch 890, training loss: 63.043609619140625 = 0.26900583505630493 + 10.0 * 6.27746057510376
Epoch 890, val loss: 0.8245093822479248
Epoch 900, training loss: 62.84611129760742 = 0.25987908244132996 + 10.0 * 6.258623123168945
Epoch 900, val loss: 0.8258175849914551
Epoch 910, training loss: 62.81577682495117 = 0.2510267496109009 + 10.0 * 6.25647497177124
Epoch 910, val loss: 0.8274563550949097
Epoch 920, training loss: 62.80329895019531 = 0.24245132505893707 + 10.0 * 6.25608491897583
Epoch 920, val loss: 0.8294982314109802
Epoch 930, training loss: 62.78328323364258 = 0.23414336144924164 + 10.0 * 6.254914283752441
Epoch 930, val loss: 0.8316042423248291
Epoch 940, training loss: 62.77530288696289 = 0.22607330977916718 + 10.0 * 6.254922866821289
Epoch 940, val loss: 0.8340030908584595
Epoch 950, training loss: 62.78192138671875 = 0.21820670366287231 + 10.0 * 6.25637149810791
Epoch 950, val loss: 0.8365016579627991
Epoch 960, training loss: 62.772029876708984 = 0.21063247323036194 + 10.0 * 6.256139755249023
Epoch 960, val loss: 0.8389863967895508
Epoch 970, training loss: 62.724185943603516 = 0.2033270299434662 + 10.0 * 6.2520856857299805
Epoch 970, val loss: 0.8416539430618286
Epoch 980, training loss: 62.71513366699219 = 0.1963074505329132 + 10.0 * 6.251882553100586
Epoch 980, val loss: 0.8445863127708435
Epoch 990, training loss: 62.72276306152344 = 0.18955522775650024 + 10.0 * 6.253320693969727
Epoch 990, val loss: 0.8477137088775635
Epoch 1000, training loss: 62.718040466308594 = 0.18298442661762238 + 10.0 * 6.253505706787109
Epoch 1000, val loss: 0.8509346842765808
Epoch 1010, training loss: 62.69827651977539 = 0.17661768198013306 + 10.0 * 6.252165794372559
Epoch 1010, val loss: 0.8542752861976624
Epoch 1020, training loss: 62.666046142578125 = 0.17055435478687286 + 10.0 * 6.24954891204834
Epoch 1020, val loss: 0.8577496409416199
Epoch 1030, training loss: 62.65031433105469 = 0.1647539585828781 + 10.0 * 6.248556137084961
Epoch 1030, val loss: 0.8614246249198914
Epoch 1040, training loss: 62.64255905151367 = 0.15916019678115845 + 10.0 * 6.248339653015137
Epoch 1040, val loss: 0.8652163147926331
Epoch 1050, training loss: 62.7098274230957 = 0.15378312766551971 + 10.0 * 6.2556047439575195
Epoch 1050, val loss: 0.8690643310546875
Epoch 1060, training loss: 62.63032150268555 = 0.14849908649921417 + 10.0 * 6.24818229675293
Epoch 1060, val loss: 0.8731200098991394
Epoch 1070, training loss: 62.60826873779297 = 0.14349742233753204 + 10.0 * 6.246477127075195
Epoch 1070, val loss: 0.8771227598190308
Epoch 1080, training loss: 62.68235778808594 = 0.13872288167476654 + 10.0 * 6.254363536834717
Epoch 1080, val loss: 0.8814278841018677
Epoch 1090, training loss: 62.615718841552734 = 0.13402114808559418 + 10.0 * 6.248169898986816
Epoch 1090, val loss: 0.8855685591697693
Epoch 1100, training loss: 62.58369827270508 = 0.12957145273685455 + 10.0 * 6.245412826538086
Epoch 1100, val loss: 0.8898409605026245
Epoch 1110, training loss: 62.566429138183594 = 0.12529942393302917 + 10.0 * 6.244112968444824
Epoch 1110, val loss: 0.8941977620124817
Epoch 1120, training loss: 62.57185745239258 = 0.12119898200035095 + 10.0 * 6.245065689086914
Epoch 1120, val loss: 0.8986663818359375
Epoch 1130, training loss: 62.57956314086914 = 0.1172294020652771 + 10.0 * 6.2462334632873535
Epoch 1130, val loss: 0.9031848311424255
Epoch 1140, training loss: 62.55158233642578 = 0.11336660385131836 + 10.0 * 6.243821620941162
Epoch 1140, val loss: 0.9078061580657959
Epoch 1150, training loss: 62.5480842590332 = 0.10971016436815262 + 10.0 * 6.243837356567383
Epoch 1150, val loss: 0.9123368859291077
Epoch 1160, training loss: 62.530338287353516 = 0.10615719109773636 + 10.0 * 6.24241828918457
Epoch 1160, val loss: 0.9171440005302429
Epoch 1170, training loss: 62.558685302734375 = 0.10274264961481094 + 10.0 * 6.245594501495361
Epoch 1170, val loss: 0.9217774271965027
Epoch 1180, training loss: 62.51224136352539 = 0.09945221245288849 + 10.0 * 6.241278648376465
Epoch 1180, val loss: 0.9264278411865234
Epoch 1190, training loss: 62.50558090209961 = 0.09628743678331375 + 10.0 * 6.24092960357666
Epoch 1190, val loss: 0.9312727451324463
Epoch 1200, training loss: 62.48537063598633 = 0.09326817840337753 + 10.0 * 6.23921012878418
Epoch 1200, val loss: 0.9360705614089966
Epoch 1210, training loss: 62.47665023803711 = 0.09037566930055618 + 10.0 * 6.2386274337768555
Epoch 1210, val loss: 0.9408590197563171
Epoch 1220, training loss: 62.52267074584961 = 0.0875980332493782 + 10.0 * 6.243507385253906
Epoch 1220, val loss: 0.9457342028617859
Epoch 1230, training loss: 62.50741958618164 = 0.08486540615558624 + 10.0 * 6.242255210876465
Epoch 1230, val loss: 0.9503987431526184
Epoch 1240, training loss: 62.46687698364258 = 0.08224772661924362 + 10.0 * 6.238462924957275
Epoch 1240, val loss: 0.9552727341651917
Epoch 1250, training loss: 62.45293045043945 = 0.07975540310144424 + 10.0 * 6.2373175621032715
Epoch 1250, val loss: 0.9601060748100281
Epoch 1260, training loss: 62.44660186767578 = 0.07736462354660034 + 10.0 * 6.236923694610596
Epoch 1260, val loss: 0.9649653434753418
Epoch 1270, training loss: 62.5383415222168 = 0.07505553215742111 + 10.0 * 6.246328830718994
Epoch 1270, val loss: 0.9700395464897156
Epoch 1280, training loss: 62.473045349121094 = 0.07284215092658997 + 10.0 * 6.240020275115967
Epoch 1280, val loss: 0.9743427634239197
Epoch 1290, training loss: 62.43559646606445 = 0.07067207247018814 + 10.0 * 6.23649263381958
Epoch 1290, val loss: 0.9793252348899841
Epoch 1300, training loss: 62.41501235961914 = 0.0686311274766922 + 10.0 * 6.234638214111328
Epoch 1300, val loss: 0.9840565919876099
Epoch 1310, training loss: 62.411285400390625 = 0.06666320562362671 + 10.0 * 6.234462261199951
Epoch 1310, val loss: 0.9890048503875732
Epoch 1320, training loss: 62.49946212768555 = 0.06477067619562149 + 10.0 * 6.24346923828125
Epoch 1320, val loss: 0.9937957525253296
Epoch 1330, training loss: 62.42938995361328 = 0.06291338056325912 + 10.0 * 6.236647605895996
Epoch 1330, val loss: 0.9984433650970459
Epoch 1340, training loss: 62.39573287963867 = 0.06114286184310913 + 10.0 * 6.233458995819092
Epoch 1340, val loss: 1.003173828125
Epoch 1350, training loss: 62.40768051147461 = 0.05944555625319481 + 10.0 * 6.234823703765869
Epoch 1350, val loss: 1.0079827308654785
Epoch 1360, training loss: 62.39894485473633 = 0.05780916288495064 + 10.0 * 6.234113693237305
Epoch 1360, val loss: 1.0127580165863037
Epoch 1370, training loss: 62.38697052001953 = 0.056225258857011795 + 10.0 * 6.23307466506958
Epoch 1370, val loss: 1.0173888206481934
Epoch 1380, training loss: 62.3804817199707 = 0.054705534130334854 + 10.0 * 6.232577323913574
Epoch 1380, val loss: 1.0221095085144043
Epoch 1390, training loss: 62.39794921875 = 0.05323721095919609 + 10.0 * 6.234471321105957
Epoch 1390, val loss: 1.026760220527649
Epoch 1400, training loss: 62.380245208740234 = 0.05180671811103821 + 10.0 * 6.23284387588501
Epoch 1400, val loss: 1.0314544439315796
Epoch 1410, training loss: 62.430355072021484 = 0.05043985694646835 + 10.0 * 6.2379913330078125
Epoch 1410, val loss: 1.0360665321350098
Epoch 1420, training loss: 62.35764694213867 = 0.04912842437624931 + 10.0 * 6.230851650238037
Epoch 1420, val loss: 1.0404505729675293
Epoch 1430, training loss: 62.34346389770508 = 0.04785839095711708 + 10.0 * 6.229560375213623
Epoch 1430, val loss: 1.04497230052948
Epoch 1440, training loss: 62.34138488769531 = 0.04663936793804169 + 10.0 * 6.2294745445251465
Epoch 1440, val loss: 1.0495787858963013
Epoch 1450, training loss: 62.349952697753906 = 0.04547036811709404 + 10.0 * 6.230448246002197
Epoch 1450, val loss: 1.0540611743927002
Epoch 1460, training loss: 62.388980865478516 = 0.044332414865493774 + 10.0 * 6.2344651222229
Epoch 1460, val loss: 1.0585525035858154
Epoch 1470, training loss: 62.34779739379883 = 0.043231066316366196 + 10.0 * 6.230456352233887
Epoch 1470, val loss: 1.0625905990600586
Epoch 1480, training loss: 62.33908462524414 = 0.04215976223349571 + 10.0 * 6.229692459106445
Epoch 1480, val loss: 1.0669981241226196
Epoch 1490, training loss: 62.329769134521484 = 0.04113820195198059 + 10.0 * 6.22886323928833
Epoch 1490, val loss: 1.0712840557098389
Epoch 1500, training loss: 62.342132568359375 = 0.04015654698014259 + 10.0 * 6.230197429656982
Epoch 1500, val loss: 1.0756700038909912
Epoch 1510, training loss: 62.310489654541016 = 0.039195094257593155 + 10.0 * 6.2271294593811035
Epoch 1510, val loss: 1.0798282623291016
Epoch 1520, training loss: 62.32789993286133 = 0.0382738932967186 + 10.0 * 6.2289628982543945
Epoch 1520, val loss: 1.084007740020752
Epoch 1530, training loss: 62.340003967285156 = 0.03738327696919441 + 10.0 * 6.23026180267334
Epoch 1530, val loss: 1.0883809328079224
Epoch 1540, training loss: 62.32999038696289 = 0.0365043580532074 + 10.0 * 6.229348659515381
Epoch 1540, val loss: 1.0925462245941162
Epoch 1550, training loss: 62.296897888183594 = 0.0356738306581974 + 10.0 * 6.2261223793029785
Epoch 1550, val loss: 1.0964194536209106
Epoch 1560, training loss: 62.290340423583984 = 0.03487064316868782 + 10.0 * 6.225546836853027
Epoch 1560, val loss: 1.1004241704940796
Epoch 1570, training loss: 62.288185119628906 = 0.034099843353033066 + 10.0 * 6.225408554077148
Epoch 1570, val loss: 1.1044847965240479
Epoch 1580, training loss: 62.34400177001953 = 0.033358413726091385 + 10.0 * 6.231064319610596
Epoch 1580, val loss: 1.1083885431289673
Epoch 1590, training loss: 62.30997085571289 = 0.032611969858407974 + 10.0 * 6.227735996246338
Epoch 1590, val loss: 1.11262845993042
Epoch 1600, training loss: 62.28202819824219 = 0.031894054263830185 + 10.0 * 6.225013256072998
Epoch 1600, val loss: 1.1162443161010742
Epoch 1610, training loss: 62.27733612060547 = 0.031212586909532547 + 10.0 * 6.224612236022949
Epoch 1610, val loss: 1.1202208995819092
Epoch 1620, training loss: 62.344810485839844 = 0.03056671842932701 + 10.0 * 6.231424331665039
Epoch 1620, val loss: 1.1238762140274048
Epoch 1630, training loss: 62.289119720458984 = 0.029898889362812042 + 10.0 * 6.225922107696533
Epoch 1630, val loss: 1.1279301643371582
Epoch 1640, training loss: 62.274085998535156 = 0.02927313931286335 + 10.0 * 6.224481105804443
Epoch 1640, val loss: 1.131587028503418
Epoch 1650, training loss: 62.26026153564453 = 0.028667796403169632 + 10.0 * 6.223159313201904
Epoch 1650, val loss: 1.1353996992111206
Epoch 1660, training loss: 62.255340576171875 = 0.028089448809623718 + 10.0 * 6.2227253913879395
Epoch 1660, val loss: 1.1390413045883179
Epoch 1670, training loss: 62.31351852416992 = 0.02752608247101307 + 10.0 * 6.2285990715026855
Epoch 1670, val loss: 1.1426748037338257
Epoch 1680, training loss: 62.320960998535156 = 0.026971030980348587 + 10.0 * 6.22939920425415
Epoch 1680, val loss: 1.1465502977371216
Epoch 1690, training loss: 62.26328659057617 = 0.026423268020153046 + 10.0 * 6.223686218261719
Epoch 1690, val loss: 1.1498030424118042
Epoch 1700, training loss: 62.25155258178711 = 0.02590266615152359 + 10.0 * 6.222565174102783
Epoch 1700, val loss: 1.1534903049468994
Epoch 1710, training loss: 62.324588775634766 = 0.02539839968085289 + 10.0 * 6.229918956756592
Epoch 1710, val loss: 1.157196283340454
Epoch 1720, training loss: 62.25896453857422 = 0.024911699816584587 + 10.0 * 6.223405361175537
Epoch 1720, val loss: 1.160369873046875
Epoch 1730, training loss: 62.23714065551758 = 0.02443191409111023 + 10.0 * 6.221270561218262
Epoch 1730, val loss: 1.1639224290847778
Epoch 1740, training loss: 62.2308464050293 = 0.02397589571774006 + 10.0 * 6.220686912536621
Epoch 1740, val loss: 1.1672741174697876
Epoch 1750, training loss: 62.24405288696289 = 0.023535335436463356 + 10.0 * 6.222051620483398
Epoch 1750, val loss: 1.1706956624984741
Epoch 1760, training loss: 62.27427291870117 = 0.0231001153588295 + 10.0 * 6.225117206573486
Epoch 1760, val loss: 1.1741821765899658
Epoch 1770, training loss: 62.2469596862793 = 0.022664397954940796 + 10.0 * 6.2224297523498535
Epoch 1770, val loss: 1.1775892972946167
Epoch 1780, training loss: 62.227046966552734 = 0.02224891446530819 + 10.0 * 6.220479965209961
Epoch 1780, val loss: 1.1806848049163818
Epoch 1790, training loss: 62.262088775634766 = 0.021847380325198174 + 10.0 * 6.224024295806885
Epoch 1790, val loss: 1.1842454671859741
Epoch 1800, training loss: 62.22107696533203 = 0.021455518901348114 + 10.0 * 6.219962120056152
Epoch 1800, val loss: 1.1871010065078735
Epoch 1810, training loss: 62.218963623046875 = 0.021079951897263527 + 10.0 * 6.219788551330566
Epoch 1810, val loss: 1.190165400505066
Epoch 1820, training loss: 62.21244812011719 = 0.020711323246359825 + 10.0 * 6.219173908233643
Epoch 1820, val loss: 1.1934794187545776
Epoch 1830, training loss: 62.2327995300293 = 0.02036057598888874 + 10.0 * 6.221243858337402
Epoch 1830, val loss: 1.1965296268463135
Epoch 1840, training loss: 62.226715087890625 = 0.02000747062265873 + 10.0 * 6.220670700073242
Epoch 1840, val loss: 1.1995697021484375
Epoch 1850, training loss: 62.23685836791992 = 0.019662022590637207 + 10.0 * 6.221719741821289
Epoch 1850, val loss: 1.2027207612991333
Epoch 1860, training loss: 62.209259033203125 = 0.019319050014019012 + 10.0 * 6.218994140625
Epoch 1860, val loss: 1.2057493925094604
Epoch 1870, training loss: 62.21149826049805 = 0.018998919054865837 + 10.0 * 6.219250202178955
Epoch 1870, val loss: 1.2088314294815063
Epoch 1880, training loss: 62.211788177490234 = 0.018684426322579384 + 10.0 * 6.219310283660889
Epoch 1880, val loss: 1.2116855382919312
Epoch 1890, training loss: 62.2006721496582 = 0.018378963693976402 + 10.0 * 6.218229293823242
Epoch 1890, val loss: 1.2148164510726929
Epoch 1900, training loss: 62.207862854003906 = 0.018080465495586395 + 10.0 * 6.218977928161621
Epoch 1900, val loss: 1.2178505659103394
Epoch 1910, training loss: 62.23399353027344 = 0.017786487936973572 + 10.0 * 6.221620559692383
Epoch 1910, val loss: 1.2208055257797241
Epoch 1920, training loss: 62.20677947998047 = 0.017503345385193825 + 10.0 * 6.21892786026001
Epoch 1920, val loss: 1.2234207391738892
Epoch 1930, training loss: 62.191505432128906 = 0.01722090132534504 + 10.0 * 6.217428684234619
Epoch 1930, val loss: 1.2265199422836304
Epoch 1940, training loss: 62.21397399902344 = 0.01695656217634678 + 10.0 * 6.219701766967773
Epoch 1940, val loss: 1.2292567491531372
Epoch 1950, training loss: 62.20085144042969 = 0.016691455617547035 + 10.0 * 6.2184157371521
Epoch 1950, val loss: 1.2319782972335815
Epoch 1960, training loss: 62.2086296081543 = 0.016429400071501732 + 10.0 * 6.219220161437988
Epoch 1960, val loss: 1.2347513437271118
Epoch 1970, training loss: 62.18608856201172 = 0.01617628149688244 + 10.0 * 6.216990947723389
Epoch 1970, val loss: 1.237506628036499
Epoch 1980, training loss: 62.184627532958984 = 0.015931522473692894 + 10.0 * 6.216869831085205
Epoch 1980, val loss: 1.2401002645492554
Epoch 1990, training loss: 62.20652770996094 = 0.015693867579102516 + 10.0 * 6.219083309173584
Epoch 1990, val loss: 1.2430752515792847
Epoch 2000, training loss: 62.1853141784668 = 0.0154583640396595 + 10.0 * 6.216985702514648
Epoch 2000, val loss: 1.245774507522583
Epoch 2010, training loss: 62.18722152709961 = 0.015227746218442917 + 10.0 * 6.217199325561523
Epoch 2010, val loss: 1.2483888864517212
Epoch 2020, training loss: 62.19173049926758 = 0.015003107488155365 + 10.0 * 6.217672824859619
Epoch 2020, val loss: 1.251153826713562
Epoch 2030, training loss: 62.1614990234375 = 0.014786342158913612 + 10.0 * 6.2146711349487305
Epoch 2030, val loss: 1.2536016702651978
Epoch 2040, training loss: 62.17082977294922 = 0.014577110297977924 + 10.0 * 6.215625286102295
Epoch 2040, val loss: 1.2561219930648804
Epoch 2050, training loss: 62.2140998840332 = 0.014371930621564388 + 10.0 * 6.219972610473633
Epoch 2050, val loss: 1.25888192653656
Epoch 2060, training loss: 62.19976806640625 = 0.014158381149172783 + 10.0 * 6.218560695648193
Epoch 2060, val loss: 1.261621117591858
Epoch 2070, training loss: 62.16811752319336 = 0.013960931450128555 + 10.0 * 6.2154154777526855
Epoch 2070, val loss: 1.2637784481048584
Epoch 2080, training loss: 62.1560173034668 = 0.013764831237494946 + 10.0 * 6.2142252922058105
Epoch 2080, val loss: 1.266336441040039
Epoch 2090, training loss: 62.15886306762695 = 0.013576348312199116 + 10.0 * 6.214528560638428
Epoch 2090, val loss: 1.2689661979675293
Epoch 2100, training loss: 62.17781448364258 = 0.013392146676778793 + 10.0 * 6.216442108154297
Epoch 2100, val loss: 1.271579384803772
Epoch 2110, training loss: 62.19902420043945 = 0.013209383003413677 + 10.0 * 6.218581199645996
Epoch 2110, val loss: 1.2740051746368408
Epoch 2120, training loss: 62.1543083190918 = 0.013028217479586601 + 10.0 * 6.214128017425537
Epoch 2120, val loss: 1.2761731147766113
Epoch 2130, training loss: 62.150360107421875 = 0.012855464592576027 + 10.0 * 6.21375036239624
Epoch 2130, val loss: 1.2784703969955444
Epoch 2140, training loss: 62.16368865966797 = 0.012686099857091904 + 10.0 * 6.215100288391113
Epoch 2140, val loss: 1.2810250520706177
Epoch 2150, training loss: 62.14518356323242 = 0.012515587732195854 + 10.0 * 6.213266849517822
Epoch 2150, val loss: 1.2834885120391846
Epoch 2160, training loss: 62.16374969482422 = 0.012353071942925453 + 10.0 * 6.215139865875244
Epoch 2160, val loss: 1.285842776298523
Epoch 2170, training loss: 62.15632629394531 = 0.012193217873573303 + 10.0 * 6.214413642883301
Epoch 2170, val loss: 1.2882384061813354
Epoch 2180, training loss: 62.195159912109375 = 0.012037412263453007 + 10.0 * 6.2183122634887695
Epoch 2180, val loss: 1.2904335260391235
Epoch 2190, training loss: 62.149444580078125 = 0.011880460195243359 + 10.0 * 6.213756561279297
Epoch 2190, val loss: 1.2924062013626099
Epoch 2200, training loss: 62.130088806152344 = 0.011730358004570007 + 10.0 * 6.211835861206055
Epoch 2200, val loss: 1.2948306798934937
Epoch 2210, training loss: 62.12436294555664 = 0.011584690771996975 + 10.0 * 6.211277961730957
Epoch 2210, val loss: 1.2969924211502075
Epoch 2220, training loss: 62.135398864746094 = 0.011443358846008778 + 10.0 * 6.212395668029785
Epoch 2220, val loss: 1.2992981672286987
Epoch 2230, training loss: 62.194679260253906 = 0.011301849037408829 + 10.0 * 6.218337535858154
Epoch 2230, val loss: 1.3016483783721924
Epoch 2240, training loss: 62.16574478149414 = 0.011159504763782024 + 10.0 * 6.215458869934082
Epoch 2240, val loss: 1.3037523031234741
Epoch 2250, training loss: 62.12519836425781 = 0.011020306497812271 + 10.0 * 6.2114176750183105
Epoch 2250, val loss: 1.3055297136306763
Epoch 2260, training loss: 62.117042541503906 = 0.010888198390603065 + 10.0 * 6.210615634918213
Epoch 2260, val loss: 1.3078564405441284
Epoch 2270, training loss: 62.11821365356445 = 0.010760572738945484 + 10.0 * 6.210745334625244
Epoch 2270, val loss: 1.3100197315216064
Epoch 2280, training loss: 62.15685272216797 = 0.010636438615620136 + 10.0 * 6.214621543884277
Epoch 2280, val loss: 1.3122680187225342
Epoch 2290, training loss: 62.1214714050293 = 0.010511727072298527 + 10.0 * 6.211095809936523
Epoch 2290, val loss: 1.3141140937805176
Epoch 2300, training loss: 62.15929412841797 = 0.010391629301011562 + 10.0 * 6.214890480041504
Epoch 2300, val loss: 1.316038966178894
Epoch 2310, training loss: 62.15496063232422 = 0.01026539970189333 + 10.0 * 6.2144694328308105
Epoch 2310, val loss: 1.3183391094207764
Epoch 2320, training loss: 62.11155319213867 = 0.010142454877495766 + 10.0 * 6.210141181945801
Epoch 2320, val loss: 1.3204044103622437
Epoch 2330, training loss: 62.106544494628906 = 0.010027885437011719 + 10.0 * 6.209651470184326
Epoch 2330, val loss: 1.322123408317566
Epoch 2340, training loss: 62.12592697143555 = 0.009918595664203167 + 10.0 * 6.2116007804870605
Epoch 2340, val loss: 1.3243266344070435
Epoch 2350, training loss: 62.1112060546875 = 0.009806573390960693 + 10.0 * 6.210139751434326
Epoch 2350, val loss: 1.3261027336120605
Epoch 2360, training loss: 62.10877227783203 = 0.009697609581053257 + 10.0 * 6.209907531738281
Epoch 2360, val loss: 1.3282800912857056
Epoch 2370, training loss: 62.14719009399414 = 0.009594990871846676 + 10.0 * 6.213759422302246
Epoch 2370, val loss: 1.329864263534546
Epoch 2380, training loss: 62.11667251586914 = 0.009486392140388489 + 10.0 * 6.210718631744385
Epoch 2380, val loss: 1.3321843147277832
Epoch 2390, training loss: 62.09849166870117 = 0.00937735103070736 + 10.0 * 6.208911418914795
Epoch 2390, val loss: 1.334086537361145
Epoch 2400, training loss: 62.09013366699219 = 0.009276366792619228 + 10.0 * 6.208085536956787
Epoch 2400, val loss: 1.3360741138458252
Epoch 2410, training loss: 62.08994674682617 = 0.0091782845556736 + 10.0 * 6.2080769538879395
Epoch 2410, val loss: 1.3380147218704224
Epoch 2420, training loss: 62.1196174621582 = 0.009082583710551262 + 10.0 * 6.211053371429443
Epoch 2420, val loss: 1.3398191928863525
Epoch 2430, training loss: 62.12562942504883 = 0.008986430242657661 + 10.0 * 6.211664199829102
Epoch 2430, val loss: 1.3420106172561646
Epoch 2440, training loss: 62.0987663269043 = 0.008890809491276741 + 10.0 * 6.208987236022949
Epoch 2440, val loss: 1.3433023691177368
Epoch 2450, training loss: 62.08795166015625 = 0.008794992230832577 + 10.0 * 6.207915306091309
Epoch 2450, val loss: 1.3454558849334717
Epoch 2460, training loss: 62.080833435058594 = 0.00870608165860176 + 10.0 * 6.207212924957275
Epoch 2460, val loss: 1.3470754623413086
Epoch 2470, training loss: 62.12306213378906 = 0.00861944817006588 + 10.0 * 6.211443901062012
Epoch 2470, val loss: 1.348806381225586
Epoch 2480, training loss: 62.0964241027832 = 0.008529102429747581 + 10.0 * 6.208789348602295
Epoch 2480, val loss: 1.3510818481445312
Epoch 2490, training loss: 62.0905876159668 = 0.008441392332315445 + 10.0 * 6.20821475982666
Epoch 2490, val loss: 1.352504849433899
Epoch 2500, training loss: 62.08051681518555 = 0.008356462232768536 + 10.0 * 6.207215785980225
Epoch 2500, val loss: 1.3542982339859009
Epoch 2510, training loss: 62.09205627441406 = 0.008275683969259262 + 10.0 * 6.208378314971924
Epoch 2510, val loss: 1.3559470176696777
Epoch 2520, training loss: 62.128257751464844 = 0.008198262192308903 + 10.0 * 6.212006092071533
Epoch 2520, val loss: 1.3577388525009155
Epoch 2530, training loss: 62.0895881652832 = 0.008112438954412937 + 10.0 * 6.2081475257873535
Epoch 2530, val loss: 1.3594765663146973
Epoch 2540, training loss: 62.0769157409668 = 0.00803257804363966 + 10.0 * 6.206888198852539
Epoch 2540, val loss: 1.361088752746582
Epoch 2550, training loss: 62.08235168457031 = 0.007957345806062222 + 10.0 * 6.207439422607422
Epoch 2550, val loss: 1.362736463546753
Epoch 2560, training loss: 62.1412467956543 = 0.00788364838808775 + 10.0 * 6.213335990905762
Epoch 2560, val loss: 1.3642125129699707
Epoch 2570, training loss: 62.090171813964844 = 0.00780294556170702 + 10.0 * 6.208237171173096
Epoch 2570, val loss: 1.3660290241241455
Epoch 2580, training loss: 62.07170104980469 = 0.007727072108536959 + 10.0 * 6.20639705657959
Epoch 2580, val loss: 1.3676046133041382
Epoch 2590, training loss: 62.06331253051758 = 0.007655529771000147 + 10.0 * 6.205565452575684
Epoch 2590, val loss: 1.3693130016326904
Epoch 2600, training loss: 62.07307815551758 = 0.007586252875626087 + 10.0 * 6.206549167633057
Epoch 2600, val loss: 1.3708776235580444
Epoch 2610, training loss: 62.10514450073242 = 0.007517140358686447 + 10.0 * 6.2097625732421875
Epoch 2610, val loss: 1.3726032972335815
Epoch 2620, training loss: 62.08414840698242 = 0.0074468269012868404 + 10.0 * 6.207670211791992
Epoch 2620, val loss: 1.3742121458053589
Epoch 2630, training loss: 62.118614196777344 = 0.007376904599368572 + 10.0 * 6.211123466491699
Epoch 2630, val loss: 1.3759324550628662
Epoch 2640, training loss: 62.08290481567383 = 0.007309518288820982 + 10.0 * 6.207559585571289
Epoch 2640, val loss: 1.3769956827163696
Epoch 2650, training loss: 62.06026077270508 = 0.007242798805236816 + 10.0 * 6.205301761627197
Epoch 2650, val loss: 1.3786697387695312
Epoch 2660, training loss: 62.05370330810547 = 0.007177439983934164 + 10.0 * 6.204652309417725
Epoch 2660, val loss: 1.3802016973495483
Epoch 2670, training loss: 62.04882049560547 = 0.007116061635315418 + 10.0 * 6.2041707038879395
Epoch 2670, val loss: 1.3817105293273926
Epoch 2680, training loss: 62.05896759033203 = 0.007055891677737236 + 10.0 * 6.205191135406494
Epoch 2680, val loss: 1.3832520246505737
Epoch 2690, training loss: 62.12153244018555 = 0.006993740331381559 + 10.0 * 6.211453914642334
Epoch 2690, val loss: 1.384853720664978
Epoch 2700, training loss: 62.0666389465332 = 0.006929755210876465 + 10.0 * 6.205970764160156
Epoch 2700, val loss: 1.3862055540084839
Epoch 2710, training loss: 62.04954528808594 = 0.006869237869977951 + 10.0 * 6.204267501831055
Epoch 2710, val loss: 1.3876612186431885
Epoch 2720, training loss: 62.07235336303711 = 0.006810596678406 + 10.0 * 6.206554412841797
Epoch 2720, val loss: 1.389052152633667
Epoch 2730, training loss: 62.104087829589844 = 0.0067520746961236 + 10.0 * 6.209733486175537
Epoch 2730, val loss: 1.3908276557922363
Epoch 2740, training loss: 62.06773376464844 = 0.006695298012346029 + 10.0 * 6.206103801727295
Epoch 2740, val loss: 1.391826868057251
Epoch 2750, training loss: 62.051395416259766 = 0.006637181155383587 + 10.0 * 6.2044758796691895
Epoch 2750, val loss: 1.3932291269302368
Epoch 2760, training loss: 62.05841064453125 = 0.006583078298717737 + 10.0 * 6.2051825523376465
Epoch 2760, val loss: 1.39485502243042
Epoch 2770, training loss: 62.056278228759766 = 0.006529276259243488 + 10.0 * 6.20497465133667
Epoch 2770, val loss: 1.3960932493209839
Epoch 2780, training loss: 62.03687286376953 = 0.006476101465523243 + 10.0 * 6.203039646148682
Epoch 2780, val loss: 1.3973729610443115
Epoch 2790, training loss: 62.05717849731445 = 0.00642637861892581 + 10.0 * 6.205075263977051
Epoch 2790, val loss: 1.39870285987854
Epoch 2800, training loss: 62.09850311279297 = 0.006375738885253668 + 10.0 * 6.209212779998779
Epoch 2800, val loss: 1.3999022245407104
Epoch 2810, training loss: 62.050743103027344 = 0.006318063475191593 + 10.0 * 6.204442501068115
Epoch 2810, val loss: 1.4017362594604492
Epoch 2820, training loss: 62.046142578125 = 0.006267762742936611 + 10.0 * 6.2039875984191895
Epoch 2820, val loss: 1.4028549194335938
Epoch 2830, training loss: 62.04551696777344 = 0.006218346301466227 + 10.0 * 6.203929901123047
Epoch 2830, val loss: 1.4042888879776
Epoch 2840, training loss: 62.05418395996094 = 0.00617035198956728 + 10.0 * 6.204801082611084
Epoch 2840, val loss: 1.4057704210281372
Epoch 2850, training loss: 62.07011795043945 = 0.006122074089944363 + 10.0 * 6.206399440765381
Epoch 2850, val loss: 1.4071460962295532
Epoch 2860, training loss: 62.04998779296875 = 0.006074899807572365 + 10.0 * 6.2043914794921875
Epoch 2860, val loss: 1.4081658124923706
Epoch 2870, training loss: 62.033607482910156 = 0.00602704705670476 + 10.0 * 6.202757835388184
Epoch 2870, val loss: 1.4093080759048462
Epoch 2880, training loss: 62.03516387939453 = 0.00598167534917593 + 10.0 * 6.20291805267334
Epoch 2880, val loss: 1.4107520580291748
Epoch 2890, training loss: 62.0645637512207 = 0.0059387874789536 + 10.0 * 6.205862522125244
Epoch 2890, val loss: 1.4119746685028076
Epoch 2900, training loss: 62.04595184326172 = 0.0058914003893733025 + 10.0 * 6.204006195068359
Epoch 2900, val loss: 1.413366675376892
Epoch 2910, training loss: 62.026458740234375 = 0.005845833104103804 + 10.0 * 6.202061653137207
Epoch 2910, val loss: 1.4145594835281372
Epoch 2920, training loss: 62.066001892089844 = 0.005802035331726074 + 10.0 * 6.206019878387451
Epoch 2920, val loss: 1.4159380197525024
Epoch 2930, training loss: 62.051055908203125 = 0.005759136751294136 + 10.0 * 6.204529762268066
Epoch 2930, val loss: 1.4172173738479614
Epoch 2940, training loss: 62.03059387207031 = 0.005715505685657263 + 10.0 * 6.202487945556641
Epoch 2940, val loss: 1.4183237552642822
Epoch 2950, training loss: 62.02253341674805 = 0.005674333311617374 + 10.0 * 6.201685905456543
Epoch 2950, val loss: 1.4193546772003174
Epoch 2960, training loss: 62.10725021362305 = 0.005636656191200018 + 10.0 * 6.210161209106445
Epoch 2960, val loss: 1.4203633069992065
Epoch 2970, training loss: 62.03999328613281 = 0.005592477507889271 + 10.0 * 6.203440189361572
Epoch 2970, val loss: 1.422094702720642
Epoch 2980, training loss: 62.02415084838867 = 0.005551961250603199 + 10.0 * 6.201859951019287
Epoch 2980, val loss: 1.4227782487869263
Epoch 2990, training loss: 62.02146911621094 = 0.005514124874025583 + 10.0 * 6.201595783233643
Epoch 2990, val loss: 1.4242209196090698
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8371112282551397
The final CL Acc:0.76173, 0.02058, The final GNN Acc:0.83676, 0.00090
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10568])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.93518829345703 = 1.966950535774231 + 10.0 * 8.596823692321777
Epoch 0, val loss: 1.9701699018478394
Epoch 10, training loss: 87.91705322265625 = 1.9561198949813843 + 10.0 * 8.59609317779541
Epoch 10, val loss: 1.9587311744689941
Epoch 20, training loss: 87.84785461425781 = 1.942946195602417 + 10.0 * 8.590490341186523
Epoch 20, val loss: 1.9444769620895386
Epoch 30, training loss: 87.46233367919922 = 1.925984501838684 + 10.0 * 8.553634643554688
Epoch 30, val loss: 1.926184058189392
Epoch 40, training loss: 85.51451110839844 = 1.9066396951675415 + 10.0 * 8.360787391662598
Epoch 40, val loss: 1.9066822528839111
Epoch 50, training loss: 81.13009643554688 = 1.8846486806869507 + 10.0 * 7.924544811248779
Epoch 50, val loss: 1.8849765062332153
Epoch 60, training loss: 77.22039031982422 = 1.8654078245162964 + 10.0 * 7.535498142242432
Epoch 60, val loss: 1.867664098739624
Epoch 70, training loss: 73.82917785644531 = 1.8529669046401978 + 10.0 * 7.197620868682861
Epoch 70, val loss: 1.8566608428955078
Epoch 80, training loss: 71.9198989868164 = 1.840739130973816 + 10.0 * 7.007916450500488
Epoch 80, val loss: 1.8448673486709595
Epoch 90, training loss: 70.30596160888672 = 1.8283145427703857 + 10.0 * 6.847764492034912
Epoch 90, val loss: 1.8334099054336548
Epoch 100, training loss: 69.40744018554688 = 1.8170421123504639 + 10.0 * 6.759039878845215
Epoch 100, val loss: 1.8232859373092651
Epoch 110, training loss: 68.81678771972656 = 1.8061261177062988 + 10.0 * 6.701065540313721
Epoch 110, val loss: 1.8135569095611572
Epoch 120, training loss: 68.3699951171875 = 1.796347737312317 + 10.0 * 6.657364368438721
Epoch 120, val loss: 1.8045587539672852
Epoch 130, training loss: 68.05059051513672 = 1.7871484756469727 + 10.0 * 6.626344203948975
Epoch 130, val loss: 1.7961022853851318
Epoch 140, training loss: 67.71907043457031 = 1.7778425216674805 + 10.0 * 6.594122886657715
Epoch 140, val loss: 1.7875810861587524
Epoch 150, training loss: 67.42986297607422 = 1.76852548122406 + 10.0 * 6.566133975982666
Epoch 150, val loss: 1.7794090509414673
Epoch 160, training loss: 67.18133544921875 = 1.7589000463485718 + 10.0 * 6.542243003845215
Epoch 160, val loss: 1.7710036039352417
Epoch 170, training loss: 67.03958892822266 = 1.7483046054840088 + 10.0 * 6.529128551483154
Epoch 170, val loss: 1.7618972063064575
Epoch 180, training loss: 66.81256103515625 = 1.7366747856140137 + 10.0 * 6.507588863372803
Epoch 180, val loss: 1.7520121335983276
Epoch 190, training loss: 66.65515899658203 = 1.7241183519363403 + 10.0 * 6.493103504180908
Epoch 190, val loss: 1.7414478063583374
Epoch 200, training loss: 66.51325225830078 = 1.7104893922805786 + 10.0 * 6.480276107788086
Epoch 200, val loss: 1.7301145792007446
Epoch 210, training loss: 66.39627075195312 = 1.6956048011779785 + 10.0 * 6.470067024230957
Epoch 210, val loss: 1.7178561687469482
Epoch 220, training loss: 66.25894165039062 = 1.679336667060852 + 10.0 * 6.457960605621338
Epoch 220, val loss: 1.7044533491134644
Epoch 230, training loss: 66.14900970458984 = 1.6617274284362793 + 10.0 * 6.448728084564209
Epoch 230, val loss: 1.690054178237915
Epoch 240, training loss: 66.10501098632812 = 1.6426982879638672 + 10.0 * 6.446231365203857
Epoch 240, val loss: 1.6745336055755615
Epoch 250, training loss: 65.94062042236328 = 1.6220837831497192 + 10.0 * 6.431853294372559
Epoch 250, val loss: 1.6577874422073364
Epoch 260, training loss: 65.82136535644531 = 1.6000791788101196 + 10.0 * 6.422128677368164
Epoch 260, val loss: 1.6400474309921265
Epoch 270, training loss: 65.71488952636719 = 1.5765870809555054 + 10.0 * 6.413830280303955
Epoch 270, val loss: 1.6212420463562012
Epoch 280, training loss: 65.6912612915039 = 1.5515995025634766 + 10.0 * 6.413966655731201
Epoch 280, val loss: 1.601170539855957
Epoch 290, training loss: 65.53819274902344 = 1.5251872539520264 + 10.0 * 6.40130090713501
Epoch 290, val loss: 1.5802242755889893
Epoch 300, training loss: 65.42670440673828 = 1.4977563619613647 + 10.0 * 6.392895221710205
Epoch 300, val loss: 1.5586464405059814
Epoch 310, training loss: 65.33663177490234 = 1.4692963361740112 + 10.0 * 6.386733531951904
Epoch 310, val loss: 1.5363026857376099
Epoch 320, training loss: 65.34011840820312 = 1.4400416612625122 + 10.0 * 6.390007972717285
Epoch 320, val loss: 1.513413429260254
Epoch 330, training loss: 65.19786071777344 = 1.4100520610809326 + 10.0 * 6.378780841827393
Epoch 330, val loss: 1.4902743101119995
Epoch 340, training loss: 65.09370422363281 = 1.3798227310180664 + 10.0 * 6.371387958526611
Epoch 340, val loss: 1.4671376943588257
Epoch 350, training loss: 65.031005859375 = 1.3495173454284668 + 10.0 * 6.368149280548096
Epoch 350, val loss: 1.4441707134246826
Epoch 360, training loss: 64.9527816772461 = 1.319233775138855 + 10.0 * 6.363354682922363
Epoch 360, val loss: 1.4215999841690063
Epoch 370, training loss: 64.91352081298828 = 1.2893749475479126 + 10.0 * 6.362414360046387
Epoch 370, val loss: 1.399515986442566
Epoch 380, training loss: 64.8218765258789 = 1.2599976062774658 + 10.0 * 6.3561882972717285
Epoch 380, val loss: 1.3781970739364624
Epoch 390, training loss: 64.75528717041016 = 1.2313112020492554 + 10.0 * 6.352397441864014
Epoch 390, val loss: 1.3577880859375
Epoch 400, training loss: 64.69615173339844 = 1.2033565044403076 + 10.0 * 6.349279403686523
Epoch 400, val loss: 1.3382102251052856
Epoch 410, training loss: 64.65463256835938 = 1.1760262250900269 + 10.0 * 6.347860336303711
Epoch 410, val loss: 1.3193882703781128
Epoch 420, training loss: 64.58644104003906 = 1.1496318578720093 + 10.0 * 6.3436808586120605
Epoch 420, val loss: 1.3016705513000488
Epoch 430, training loss: 64.52130126953125 = 1.1240346431732178 + 10.0 * 6.33972692489624
Epoch 430, val loss: 1.2849563360214233
Epoch 440, training loss: 64.46166229248047 = 1.099204659461975 + 10.0 * 6.336246013641357
Epoch 440, val loss: 1.2691404819488525
Epoch 450, training loss: 64.4640121459961 = 1.0750393867492676 + 10.0 * 6.338897228240967
Epoch 450, val loss: 1.2542697191238403
Epoch 460, training loss: 64.36441802978516 = 1.0513906478881836 + 10.0 * 6.331302642822266
Epoch 460, val loss: 1.2397630214691162
Epoch 470, training loss: 64.33345031738281 = 1.028584361076355 + 10.0 * 6.330486297607422
Epoch 470, val loss: 1.2262523174285889
Epoch 480, training loss: 64.2716293334961 = 1.0063360929489136 + 10.0 * 6.326529502868652
Epoch 480, val loss: 1.2133398056030273
Epoch 490, training loss: 64.22071838378906 = 0.9845960140228271 + 10.0 * 6.323612689971924
Epoch 490, val loss: 1.2010738849639893
Epoch 500, training loss: 64.1782455444336 = 0.9631603956222534 + 10.0 * 6.321508884429932
Epoch 500, val loss: 1.1893779039382935
Epoch 510, training loss: 64.15351104736328 = 0.9416694045066833 + 10.0 * 6.321184158325195
Epoch 510, val loss: 1.1778223514556885
Epoch 520, training loss: 64.09703826904297 = 0.9206185936927795 + 10.0 * 6.3176422119140625
Epoch 520, val loss: 1.166865348815918
Epoch 530, training loss: 64.05650329589844 = 0.8991777896881104 + 10.0 * 6.315732479095459
Epoch 530, val loss: 1.1560323238372803
Epoch 540, training loss: 64.01416015625 = 0.8779948353767395 + 10.0 * 6.3136162757873535
Epoch 540, val loss: 1.1457099914550781
Epoch 550, training loss: 63.97160339355469 = 0.8569759726524353 + 10.0 * 6.311462879180908
Epoch 550, val loss: 1.1358250379562378
Epoch 560, training loss: 64.02130889892578 = 0.8360375761985779 + 10.0 * 6.318526744842529
Epoch 560, val loss: 1.1261796951293945
Epoch 570, training loss: 63.9384765625 = 0.8152730464935303 + 10.0 * 6.312320232391357
Epoch 570, val loss: 1.1171084642410278
Epoch 580, training loss: 63.86532211303711 = 0.7948199510574341 + 10.0 * 6.3070502281188965
Epoch 580, val loss: 1.1085429191589355
Epoch 590, training loss: 63.82731628417969 = 0.7746596336364746 + 10.0 * 6.3052659034729
Epoch 590, val loss: 1.1006193161010742
Epoch 600, training loss: 63.78641128540039 = 0.7547498941421509 + 10.0 * 6.303166389465332
Epoch 600, val loss: 1.0930756330490112
Epoch 610, training loss: 63.7494010925293 = 0.7349974513053894 + 10.0 * 6.301440238952637
Epoch 610, val loss: 1.0859613418579102
Epoch 620, training loss: 63.78035354614258 = 0.7154385447502136 + 10.0 * 6.306491374969482
Epoch 620, val loss: 1.079146385192871
Epoch 630, training loss: 63.70985412597656 = 0.6960703134536743 + 10.0 * 6.30137825012207
Epoch 630, val loss: 1.072932481765747
Epoch 640, training loss: 63.67002868652344 = 0.677090585231781 + 10.0 * 6.2992939949035645
Epoch 640, val loss: 1.0670585632324219
Epoch 650, training loss: 63.620094299316406 = 0.6584585905075073 + 10.0 * 6.296163558959961
Epoch 650, val loss: 1.0616663694381714
Epoch 660, training loss: 63.598331451416016 = 0.6401180028915405 + 10.0 * 6.295821189880371
Epoch 660, val loss: 1.0566985607147217
Epoch 670, training loss: 63.56181335449219 = 0.6220313310623169 + 10.0 * 6.293978214263916
Epoch 670, val loss: 1.0523128509521484
Epoch 680, training loss: 63.5321044921875 = 0.6042308807373047 + 10.0 * 6.292787075042725
Epoch 680, val loss: 1.0483434200286865
Epoch 690, training loss: 63.54866027832031 = 0.5867331027984619 + 10.0 * 6.296192646026611
Epoch 690, val loss: 1.0447596311569214
Epoch 700, training loss: 63.50656509399414 = 0.569553554058075 + 10.0 * 6.293701171875
Epoch 700, val loss: 1.0414828062057495
Epoch 710, training loss: 63.43168640136719 = 0.5526794195175171 + 10.0 * 6.287900447845459
Epoch 710, val loss: 1.0387723445892334
Epoch 720, training loss: 63.40876770019531 = 0.5362035036087036 + 10.0 * 6.287256240844727
Epoch 720, val loss: 1.0366368293762207
Epoch 730, training loss: 63.402095794677734 = 0.5200251936912537 + 10.0 * 6.288207054138184
Epoch 730, val loss: 1.0349839925765991
Epoch 740, training loss: 63.382755279541016 = 0.5041758418083191 + 10.0 * 6.287858009338379
Epoch 740, val loss: 1.0332692861557007
Epoch 750, training loss: 63.33365249633789 = 0.488628625869751 + 10.0 * 6.2845025062561035
Epoch 750, val loss: 1.0324549674987793
Epoch 760, training loss: 63.302024841308594 = 0.4735078513622284 + 10.0 * 6.282851696014404
Epoch 760, val loss: 1.032037615776062
Epoch 770, training loss: 63.28135299682617 = 0.45872780680656433 + 10.0 * 6.282262325286865
Epoch 770, val loss: 1.0319899320602417
Epoch 780, training loss: 63.274070739746094 = 0.4442325234413147 + 10.0 * 6.282983779907227
Epoch 780, val loss: 1.0322684049606323
Epoch 790, training loss: 63.22661209106445 = 0.430021733045578 + 10.0 * 6.279658794403076
Epoch 790, val loss: 1.0331271886825562
Epoch 800, training loss: 63.20444107055664 = 0.41617125272750854 + 10.0 * 6.278826713562012
Epoch 800, val loss: 1.0343149900436401
Epoch 810, training loss: 63.185794830322266 = 0.4026157557964325 + 10.0 * 6.278317928314209
Epoch 810, val loss: 1.0358963012695312
Epoch 820, training loss: 63.18159866333008 = 0.389354407787323 + 10.0 * 6.279224395751953
Epoch 820, val loss: 1.03777015209198
Epoch 830, training loss: 63.14814376831055 = 0.37636327743530273 + 10.0 * 6.2771782875061035
Epoch 830, val loss: 1.0399513244628906
Epoch 840, training loss: 63.15068054199219 = 0.36364343762397766 + 10.0 * 6.278703689575195
Epoch 840, val loss: 1.0422930717468262
Epoch 850, training loss: 63.107357025146484 = 0.35125577449798584 + 10.0 * 6.275609970092773
Epoch 850, val loss: 1.0451090335845947
Epoch 860, training loss: 63.063232421875 = 0.33914291858673096 + 10.0 * 6.272408962249756
Epoch 860, val loss: 1.0483157634735107
Epoch 870, training loss: 63.0411376953125 = 0.3273858428001404 + 10.0 * 6.2713751792907715
Epoch 870, val loss: 1.0518306493759155
Epoch 880, training loss: 63.021236419677734 = 0.3159135580062866 + 10.0 * 6.270532131195068
Epoch 880, val loss: 1.0556738376617432
Epoch 890, training loss: 63.14692306518555 = 0.3047408163547516 + 10.0 * 6.2842183113098145
Epoch 890, val loss: 1.0596405267715454
Epoch 900, training loss: 62.989498138427734 = 0.29372406005859375 + 10.0 * 6.269577503204346
Epoch 900, val loss: 1.064124345779419
Epoch 910, training loss: 62.9777717590332 = 0.28312358260154724 + 10.0 * 6.26946496963501
Epoch 910, val loss: 1.0688165426254272
Epoch 920, training loss: 62.94090270996094 = 0.2728838324546814 + 10.0 * 6.266801834106445
Epoch 920, val loss: 1.0736944675445557
Epoch 930, training loss: 62.928043365478516 = 0.26294082403182983 + 10.0 * 6.266510486602783
Epoch 930, val loss: 1.0788737535476685
Epoch 940, training loss: 63.02495193481445 = 0.2532261908054352 + 10.0 * 6.277172565460205
Epoch 940, val loss: 1.0843636989593506
Epoch 950, training loss: 62.92207336425781 = 0.2438526302576065 + 10.0 * 6.267821788787842
Epoch 950, val loss: 1.0898584127426147
Epoch 960, training loss: 62.87851333618164 = 0.2348087579011917 + 10.0 * 6.264370441436768
Epoch 960, val loss: 1.0958845615386963
Epoch 970, training loss: 62.85448455810547 = 0.2261352837085724 + 10.0 * 6.2628350257873535
Epoch 970, val loss: 1.1021206378936768
Epoch 980, training loss: 62.84037780761719 = 0.21775326132774353 + 10.0 * 6.262262344360352
Epoch 980, val loss: 1.1086241006851196
Epoch 990, training loss: 62.827537536621094 = 0.20964543521404266 + 10.0 * 6.261789321899414
Epoch 990, val loss: 1.115302324295044
Epoch 1000, training loss: 62.861881256103516 = 0.20180106163024902 + 10.0 * 6.266007900238037
Epoch 1000, val loss: 1.12211275100708
Epoch 1010, training loss: 62.82479476928711 = 0.1942484825849533 + 10.0 * 6.263054847717285
Epoch 1010, val loss: 1.1290985345840454
Epoch 1020, training loss: 62.78016662597656 = 0.18700659275054932 + 10.0 * 6.2593159675598145
Epoch 1020, val loss: 1.1364103555679321
Epoch 1030, training loss: 62.76830291748047 = 0.18005292117595673 + 10.0 * 6.258824825286865
Epoch 1030, val loss: 1.1439272165298462
Epoch 1040, training loss: 62.86257553100586 = 0.17336566746234894 + 10.0 * 6.2689208984375
Epoch 1040, val loss: 1.1516327857971191
Epoch 1050, training loss: 62.767677307128906 = 0.16683657467365265 + 10.0 * 6.26008415222168
Epoch 1050, val loss: 1.1592788696289062
Epoch 1060, training loss: 62.73261260986328 = 0.16063269972801208 + 10.0 * 6.257197856903076
Epoch 1060, val loss: 1.1670740842819214
Epoch 1070, training loss: 62.71787643432617 = 0.1547081470489502 + 10.0 * 6.256316661834717
Epoch 1070, val loss: 1.1751158237457275
Epoch 1080, training loss: 62.73325729370117 = 0.14903408288955688 + 10.0 * 6.258422374725342
Epoch 1080, val loss: 1.1832234859466553
Epoch 1090, training loss: 62.70580291748047 = 0.1435070037841797 + 10.0 * 6.256229877471924
Epoch 1090, val loss: 1.1913877725601196
Epoch 1100, training loss: 62.70294952392578 = 0.13826175034046173 + 10.0 * 6.256468772888184
Epoch 1100, val loss: 1.199676752090454
Epoch 1110, training loss: 62.68135070800781 = 0.1332140415906906 + 10.0 * 6.2548136711120605
Epoch 1110, val loss: 1.2081406116485596
Epoch 1120, training loss: 62.659507751464844 = 0.12840259075164795 + 10.0 * 6.253110408782959
Epoch 1120, val loss: 1.2166614532470703
Epoch 1130, training loss: 62.68939971923828 = 0.12378934025764465 + 10.0 * 6.256560802459717
Epoch 1130, val loss: 1.225310206413269
Epoch 1140, training loss: 62.64683532714844 = 0.1193399652838707 + 10.0 * 6.252749443054199
Epoch 1140, val loss: 1.2338730096817017
Epoch 1150, training loss: 62.637142181396484 = 0.11509018391370773 + 10.0 * 6.2522053718566895
Epoch 1150, val loss: 1.242688536643982
Epoch 1160, training loss: 62.637474060058594 = 0.11103583872318268 + 10.0 * 6.252644062042236
Epoch 1160, val loss: 1.251420021057129
Epoch 1170, training loss: 62.63347244262695 = 0.10714513063430786 + 10.0 * 6.252633094787598
Epoch 1170, val loss: 1.2603117227554321
Epoch 1180, training loss: 62.604278564453125 = 0.10343630611896515 + 10.0 * 6.250084400177002
Epoch 1180, val loss: 1.2692290544509888
Epoch 1190, training loss: 62.591041564941406 = 0.09989888221025467 + 10.0 * 6.249114036560059
Epoch 1190, val loss: 1.2782747745513916
Epoch 1200, training loss: 62.582237243652344 = 0.09652779251337051 + 10.0 * 6.248570919036865
Epoch 1200, val loss: 1.2874053716659546
Epoch 1210, training loss: 62.680843353271484 = 0.09329266846179962 + 10.0 * 6.258755207061768
Epoch 1210, val loss: 1.2965506315231323
Epoch 1220, training loss: 62.602333068847656 = 0.0901881754398346 + 10.0 * 6.251214504241943
Epoch 1220, val loss: 1.3054535388946533
Epoch 1230, training loss: 62.57465362548828 = 0.08722792565822601 + 10.0 * 6.248742580413818
Epoch 1230, val loss: 1.3146495819091797
Epoch 1240, training loss: 62.54855728149414 = 0.08440594375133514 + 10.0 * 6.246415138244629
Epoch 1240, val loss: 1.3237981796264648
Epoch 1250, training loss: 62.54317092895508 = 0.08171386271715164 + 10.0 * 6.246145725250244
Epoch 1250, val loss: 1.3329917192459106
Epoch 1260, training loss: 62.594276428222656 = 0.07914923131465912 + 10.0 * 6.2515130043029785
Epoch 1260, val loss: 1.3420675992965698
Epoch 1270, training loss: 62.548919677734375 = 0.07666917145252228 + 10.0 * 6.247225284576416
Epoch 1270, val loss: 1.3513742685317993
Epoch 1280, training loss: 62.53266525268555 = 0.07428489625453949 + 10.0 * 6.245838165283203
Epoch 1280, val loss: 1.3603416681289673
Epoch 1290, training loss: 62.51872634887695 = 0.07202403247356415 + 10.0 * 6.2446699142456055
Epoch 1290, val loss: 1.3694947957992554
Epoch 1300, training loss: 62.54240798950195 = 0.069859579205513 + 10.0 * 6.247254848480225
Epoch 1300, val loss: 1.378454566001892
Epoch 1310, training loss: 62.5036735534668 = 0.06776605546474457 + 10.0 * 6.243590831756592
Epoch 1310, val loss: 1.3874096870422363
Epoch 1320, training loss: 62.505699157714844 = 0.06575953960418701 + 10.0 * 6.243994235992432
Epoch 1320, val loss: 1.3963067531585693
Epoch 1330, training loss: 62.49779510498047 = 0.06384578347206116 + 10.0 * 6.24339485168457
Epoch 1330, val loss: 1.4053598642349243
Epoch 1340, training loss: 62.49225616455078 = 0.0620027631521225 + 10.0 * 6.243025302886963
Epoch 1340, val loss: 1.4140667915344238
Epoch 1350, training loss: 62.50650405883789 = 0.06023753806948662 + 10.0 * 6.244626522064209
Epoch 1350, val loss: 1.4227310419082642
Epoch 1360, training loss: 62.474674224853516 = 0.05854222550988197 + 10.0 * 6.241613388061523
Epoch 1360, val loss: 1.4314439296722412
Epoch 1370, training loss: 62.46865463256836 = 0.056921590119600296 + 10.0 * 6.241173267364502
Epoch 1370, val loss: 1.4401910305023193
Epoch 1380, training loss: 62.50236129760742 = 0.05536278337240219 + 10.0 * 6.244699954986572
Epoch 1380, val loss: 1.4487017393112183
Epoch 1390, training loss: 62.45649719238281 = 0.05385114252567291 + 10.0 * 6.240264415740967
Epoch 1390, val loss: 1.457253336906433
Epoch 1400, training loss: 62.44315719604492 = 0.052410028874874115 + 10.0 * 6.23907470703125
Epoch 1400, val loss: 1.4657617807388306
Epoch 1410, training loss: 62.529335021972656 = 0.051023852080106735 + 10.0 * 6.247830867767334
Epoch 1410, val loss: 1.474073052406311
Epoch 1420, training loss: 62.47431945800781 = 0.049683500081300735 + 10.0 * 6.2424635887146
Epoch 1420, val loss: 1.482546329498291
Epoch 1430, training loss: 62.43856430053711 = 0.048375412821769714 + 10.0 * 6.23901891708374
Epoch 1430, val loss: 1.4908039569854736
Epoch 1440, training loss: 62.42393493652344 = 0.047142330557107925 + 10.0 * 6.237679481506348
Epoch 1440, val loss: 1.4990942478179932
Epoch 1450, training loss: 62.43339920043945 = 0.0459568090736866 + 10.0 * 6.238744258880615
Epoch 1450, val loss: 1.5072777271270752
Epoch 1460, training loss: 62.429054260253906 = 0.04479920119047165 + 10.0 * 6.238425254821777
Epoch 1460, val loss: 1.515317678451538
Epoch 1470, training loss: 62.408546447753906 = 0.04368413984775543 + 10.0 * 6.236485958099365
Epoch 1470, val loss: 1.523565649986267
Epoch 1480, training loss: 62.429931640625 = 0.04261629655957222 + 10.0 * 6.238731384277344
Epoch 1480, val loss: 1.5316448211669922
Epoch 1490, training loss: 62.40999984741211 = 0.04157756268978119 + 10.0 * 6.236842155456543
Epoch 1490, val loss: 1.5395479202270508
Epoch 1500, training loss: 62.395057678222656 = 0.040569111704826355 + 10.0 * 6.235448837280273
Epoch 1500, val loss: 1.547559142112732
Epoch 1510, training loss: 62.38742446899414 = 0.0396062433719635 + 10.0 * 6.234781742095947
Epoch 1510, val loss: 1.5554051399230957
Epoch 1520, training loss: 62.38145446777344 = 0.03868755325675011 + 10.0 * 6.23427677154541
Epoch 1520, val loss: 1.5633063316345215
Epoch 1530, training loss: 62.38418197631836 = 0.037795040756464005 + 10.0 * 6.234638690948486
Epoch 1530, val loss: 1.570949673652649
Epoch 1540, training loss: 62.4422492980957 = 0.03693489357829094 + 10.0 * 6.2405314445495605
Epoch 1540, val loss: 1.578524112701416
Epoch 1550, training loss: 62.39407730102539 = 0.03607654944062233 + 10.0 * 6.235800266265869
Epoch 1550, val loss: 1.5862289667129517
Epoch 1560, training loss: 62.37910079956055 = 0.035268183797597885 + 10.0 * 6.234383583068848
Epoch 1560, val loss: 1.593998670578003
Epoch 1570, training loss: 62.3835334777832 = 0.03448743000626564 + 10.0 * 6.2349042892456055
Epoch 1570, val loss: 1.6015251874923706
Epoch 1580, training loss: 62.36112976074219 = 0.03372453153133392 + 10.0 * 6.23274040222168
Epoch 1580, val loss: 1.608887791633606
Epoch 1590, training loss: 62.35688018798828 = 0.03298711031675339 + 10.0 * 6.232389450073242
Epoch 1590, val loss: 1.616052508354187
Epoch 1600, training loss: 62.400550842285156 = 0.032278966158628464 + 10.0 * 6.2368268966674805
Epoch 1600, val loss: 1.623249888420105
Epoch 1610, training loss: 62.36698532104492 = 0.0315912589430809 + 10.0 * 6.233539581298828
Epoch 1610, val loss: 1.6308177709579468
Epoch 1620, training loss: 62.346473693847656 = 0.030910298228263855 + 10.0 * 6.231556415557861
Epoch 1620, val loss: 1.637725830078125
Epoch 1630, training loss: 62.33739471435547 = 0.030265646055340767 + 10.0 * 6.230712890625
Epoch 1630, val loss: 1.6450400352478027
Epoch 1640, training loss: 62.36273193359375 = 0.02964588813483715 + 10.0 * 6.233308792114258
Epoch 1640, val loss: 1.6519882678985596
Epoch 1650, training loss: 62.33668518066406 = 0.02902783453464508 + 10.0 * 6.2307658195495605
Epoch 1650, val loss: 1.6590465307235718
Epoch 1660, training loss: 62.33292770385742 = 0.028432974591851234 + 10.0 * 6.230449199676514
Epoch 1660, val loss: 1.6660242080688477
Epoch 1670, training loss: 62.3267822265625 = 0.027861744165420532 + 10.0 * 6.229891777038574
Epoch 1670, val loss: 1.673064947128296
Epoch 1680, training loss: 62.32351303100586 = 0.027310030534863472 + 10.0 * 6.229620456695557
Epoch 1680, val loss: 1.6798019409179688
Epoch 1690, training loss: 62.396697998046875 = 0.026783913373947144 + 10.0 * 6.2369914054870605
Epoch 1690, val loss: 1.686694860458374
Epoch 1700, training loss: 62.34621047973633 = 0.02624931000173092 + 10.0 * 6.231996059417725
Epoch 1700, val loss: 1.69330632686615
Epoch 1710, training loss: 62.32345962524414 = 0.025737758725881577 + 10.0 * 6.229772090911865
Epoch 1710, val loss: 1.700015664100647
Epoch 1720, training loss: 62.30516815185547 = 0.025245072320103645 + 10.0 * 6.227992057800293
Epoch 1720, val loss: 1.7066470384597778
Epoch 1730, training loss: 62.33633804321289 = 0.024769574403762817 + 10.0 * 6.231156826019287
Epoch 1730, val loss: 1.7132774591445923
Epoch 1740, training loss: 62.31398391723633 = 0.024297423660755157 + 10.0 * 6.228968620300293
Epoch 1740, val loss: 1.7194093465805054
Epoch 1750, training loss: 62.30483627319336 = 0.023843320086598396 + 10.0 * 6.228099346160889
Epoch 1750, val loss: 1.726426362991333
Epoch 1760, training loss: 62.300758361816406 = 0.02340519241988659 + 10.0 * 6.2277350425720215
Epoch 1760, val loss: 1.7326735258102417
Epoch 1770, training loss: 62.29243469238281 = 0.022977016866207123 + 10.0 * 6.226945877075195
Epoch 1770, val loss: 1.7390813827514648
Epoch 1780, training loss: 62.321678161621094 = 0.022564811632037163 + 10.0 * 6.2299113273620605
Epoch 1780, val loss: 1.7452808618545532
Epoch 1790, training loss: 62.28339385986328 = 0.022157257422804832 + 10.0 * 6.226123809814453
Epoch 1790, val loss: 1.7518216371536255
Epoch 1800, training loss: 62.306331634521484 = 0.021764878183603287 + 10.0 * 6.228456497192383
Epoch 1800, val loss: 1.7579456567764282
Epoch 1810, training loss: 62.28489685058594 = 0.021376242861151695 + 10.0 * 6.226351737976074
Epoch 1810, val loss: 1.7640044689178467
Epoch 1820, training loss: 62.292118072509766 = 0.021003812551498413 + 10.0 * 6.227111339569092
Epoch 1820, val loss: 1.7702900171279907
Epoch 1830, training loss: 62.312095642089844 = 0.02063802443444729 + 10.0 * 6.2291460037231445
Epoch 1830, val loss: 1.7761949300765991
Epoch 1840, training loss: 62.28266525268555 = 0.02028864435851574 + 10.0 * 6.2262372970581055
Epoch 1840, val loss: 1.7827931642532349
Epoch 1850, training loss: 62.26872253417969 = 0.019938921555876732 + 10.0 * 6.224878311157227
Epoch 1850, val loss: 1.7886326313018799
Epoch 1860, training loss: 62.28694152832031 = 0.019608590751886368 + 10.0 * 6.226733207702637
Epoch 1860, val loss: 1.7946693897247314
Epoch 1870, training loss: 62.263427734375 = 0.019276754930615425 + 10.0 * 6.224415302276611
Epoch 1870, val loss: 1.800533652305603
Epoch 1880, training loss: 62.270389556884766 = 0.018957139924168587 + 10.0 * 6.225142955780029
Epoch 1880, val loss: 1.806489109992981
Epoch 1890, training loss: 62.2887077331543 = 0.01864929124712944 + 10.0 * 6.227005958557129
Epoch 1890, val loss: 1.8123962879180908
Epoch 1900, training loss: 62.25885009765625 = 0.018339823931455612 + 10.0 * 6.224050998687744
Epoch 1900, val loss: 1.8179755210876465
Epoch 1910, training loss: 62.25181579589844 = 0.018044820055365562 + 10.0 * 6.223377227783203
Epoch 1910, val loss: 1.823746919631958
Epoch 1920, training loss: 62.244041442871094 = 0.017755335196852684 + 10.0 * 6.222628593444824
Epoch 1920, val loss: 1.8296406269073486
Epoch 1930, training loss: 62.28339385986328 = 0.017481854185461998 + 10.0 * 6.226591110229492
Epoch 1930, val loss: 1.835465908050537
Epoch 1940, training loss: 62.24172592163086 = 0.017199203372001648 + 10.0 * 6.222452640533447
Epoch 1940, val loss: 1.840747356414795
Epoch 1950, training loss: 62.23637771606445 = 0.016927827149629593 + 10.0 * 6.221944808959961
Epoch 1950, val loss: 1.8464714288711548
Epoch 1960, training loss: 62.244468688964844 = 0.016666961833834648 + 10.0 * 6.222780227661133
Epoch 1960, val loss: 1.8520225286483765
Epoch 1970, training loss: 62.28049087524414 = 0.016410531476140022 + 10.0 * 6.226408004760742
Epoch 1970, val loss: 1.8575338125228882
Epoch 1980, training loss: 62.2467041015625 = 0.016160225495696068 + 10.0 * 6.2230544090271
Epoch 1980, val loss: 1.8630578517913818
Epoch 1990, training loss: 62.230445861816406 = 0.015915917232632637 + 10.0 * 6.2214531898498535
Epoch 1990, val loss: 1.8684496879577637
Epoch 2000, training loss: 62.25471496582031 = 0.01568000204861164 + 10.0 * 6.223903656005859
Epoch 2000, val loss: 1.873619556427002
Epoch 2010, training loss: 62.2220573425293 = 0.015447600744664669 + 10.0 * 6.220661163330078
Epoch 2010, val loss: 1.879301905632019
Epoch 2020, training loss: 62.2412109375 = 0.015220969915390015 + 10.0 * 6.222599029541016
Epoch 2020, val loss: 1.884563684463501
Epoch 2030, training loss: 62.22919845581055 = 0.014997494406998158 + 10.0 * 6.221419811248779
Epoch 2030, val loss: 1.8897655010223389
Epoch 2040, training loss: 62.2116584777832 = 0.014780101366341114 + 10.0 * 6.219687461853027
Epoch 2040, val loss: 1.8951764106750488
Epoch 2050, training loss: 62.269676208496094 = 0.014571353793144226 + 10.0 * 6.225510597229004
Epoch 2050, val loss: 1.9001623392105103
Epoch 2060, training loss: 62.22815704345703 = 0.014357461594045162 + 10.0 * 6.22137975692749
Epoch 2060, val loss: 1.9053617715835571
Epoch 2070, training loss: 62.20993423461914 = 0.014154760167002678 + 10.0 * 6.219577789306641
Epoch 2070, val loss: 1.9106886386871338
Epoch 2080, training loss: 62.24555969238281 = 0.013957647606730461 + 10.0 * 6.223160266876221
Epoch 2080, val loss: 1.9151538610458374
Epoch 2090, training loss: 62.2104377746582 = 0.013761659152805805 + 10.0 * 6.219667434692383
Epoch 2090, val loss: 1.9207357168197632
Epoch 2100, training loss: 62.200767517089844 = 0.013571741990745068 + 10.0 * 6.218719482421875
Epoch 2100, val loss: 1.9257699251174927
Epoch 2110, training loss: 62.212615966796875 = 0.01338825561106205 + 10.0 * 6.2199225425720215
Epoch 2110, val loss: 1.9308186769485474
Epoch 2120, training loss: 62.229488372802734 = 0.013206861913204193 + 10.0 * 6.221628189086914
Epoch 2120, val loss: 1.935522437095642
Epoch 2130, training loss: 62.20466232299805 = 0.013023747131228447 + 10.0 * 6.21916389465332
Epoch 2130, val loss: 1.9405744075775146
Epoch 2140, training loss: 62.18928146362305 = 0.012852462939918041 + 10.0 * 6.217642784118652
Epoch 2140, val loss: 1.9454224109649658
Epoch 2150, training loss: 62.21766662597656 = 0.012686178088188171 + 10.0 * 6.220498085021973
Epoch 2150, val loss: 1.9503511190414429
Epoch 2160, training loss: 62.199893951416016 = 0.01251593604683876 + 10.0 * 6.218737602233887
Epoch 2160, val loss: 1.9551043510437012
Epoch 2170, training loss: 62.195098876953125 = 0.012351294979453087 + 10.0 * 6.2182745933532715
Epoch 2170, val loss: 1.9599593877792358
Epoch 2180, training loss: 62.19103240966797 = 0.01218972634524107 + 10.0 * 6.217884540557861
Epoch 2180, val loss: 1.9644442796707153
Epoch 2190, training loss: 62.186866760253906 = 0.012032585218548775 + 10.0 * 6.2174835205078125
Epoch 2190, val loss: 1.9691931009292603
Epoch 2200, training loss: 62.181427001953125 = 0.011880689300596714 + 10.0 * 6.216954708099365
Epoch 2200, val loss: 1.9738353490829468
Epoch 2210, training loss: 62.19059371948242 = 0.011732594110071659 + 10.0 * 6.217885971069336
Epoch 2210, val loss: 1.9783856868743896
Epoch 2220, training loss: 62.19324493408203 = 0.011585555970668793 + 10.0 * 6.218165874481201
Epoch 2220, val loss: 1.9829872846603394
Epoch 2230, training loss: 62.18454360961914 = 0.011440250091254711 + 10.0 * 6.217310428619385
Epoch 2230, val loss: 1.9876459836959839
Epoch 2240, training loss: 62.17972946166992 = 0.011299815028905869 + 10.0 * 6.216843128204346
Epoch 2240, val loss: 1.9921101331710815
Epoch 2250, training loss: 62.23219299316406 = 0.011159433051943779 + 10.0 * 6.222103595733643
Epoch 2250, val loss: 1.996379017829895
Epoch 2260, training loss: 62.17291259765625 = 0.011017192155122757 + 10.0 * 6.216189384460449
Epoch 2260, val loss: 2.000837564468384
Epoch 2270, training loss: 62.15865707397461 = 0.01088449265807867 + 10.0 * 6.21477746963501
Epoch 2270, val loss: 2.005437135696411
Epoch 2280, training loss: 62.15225601196289 = 0.010755482129752636 + 10.0 * 6.214150428771973
Epoch 2280, val loss: 2.0096311569213867
Epoch 2290, training loss: 62.15156936645508 = 0.010630199685692787 + 10.0 * 6.2140936851501465
Epoch 2290, val loss: 2.014012336730957
Epoch 2300, training loss: 62.239437103271484 = 0.010508952662348747 + 10.0 * 6.222892761230469
Epoch 2300, val loss: 2.0184876918792725
Epoch 2310, training loss: 62.18069839477539 = 0.010380181483924389 + 10.0 * 6.217031955718994
Epoch 2310, val loss: 2.022282838821411
Epoch 2320, training loss: 62.16425704956055 = 0.010256796143949032 + 10.0 * 6.215399742126465
Epoch 2320, val loss: 2.026954412460327
Epoch 2330, training loss: 62.152259826660156 = 0.010137815028429031 + 10.0 * 6.214211940765381
Epoch 2330, val loss: 2.030954360961914
Epoch 2340, training loss: 62.19893264770508 = 0.01002274639904499 + 10.0 * 6.218891143798828
Epoch 2340, val loss: 2.0348048210144043
Epoch 2350, training loss: 62.14711380004883 = 0.009908485226333141 + 10.0 * 6.213720798492432
Epoch 2350, val loss: 2.0393850803375244
Epoch 2360, training loss: 62.15351867675781 = 0.009795811027288437 + 10.0 * 6.214372158050537
Epoch 2360, val loss: 2.0432076454162598
Epoch 2370, training loss: 62.15275192260742 = 0.009684753604233265 + 10.0 * 6.214306831359863
Epoch 2370, val loss: 2.047335147857666
Epoch 2380, training loss: 62.16029739379883 = 0.00957957562059164 + 10.0 * 6.215071678161621
Epoch 2380, val loss: 2.0514540672302246
Epoch 2390, training loss: 62.156211853027344 = 0.009473701938986778 + 10.0 * 6.21467399597168
Epoch 2390, val loss: 2.055363178253174
Epoch 2400, training loss: 62.148651123046875 = 0.009368357248604298 + 10.0 * 6.21392822265625
Epoch 2400, val loss: 2.0596084594726562
Epoch 2410, training loss: 62.135345458984375 = 0.009267187677323818 + 10.0 * 6.2126078605651855
Epoch 2410, val loss: 2.0635018348693848
Epoch 2420, training loss: 62.17050552368164 = 0.009167148731648922 + 10.0 * 6.216134071350098
Epoch 2420, val loss: 2.0670621395111084
Epoch 2430, training loss: 62.13629913330078 = 0.009067125618457794 + 10.0 * 6.212723255157471
Epoch 2430, val loss: 2.0713815689086914
Epoch 2440, training loss: 62.129032135009766 = 0.008970675989985466 + 10.0 * 6.212006092071533
Epoch 2440, val loss: 2.075162649154663
Epoch 2450, training loss: 62.15898895263672 = 0.008876788429915905 + 10.0 * 6.215011119842529
Epoch 2450, val loss: 2.078843593597412
Epoch 2460, training loss: 62.148902893066406 = 0.008782666176557541 + 10.0 * 6.214012145996094
Epoch 2460, val loss: 2.0831220149993896
Epoch 2470, training loss: 62.13206481933594 = 0.008687346242368221 + 10.0 * 6.212337970733643
Epoch 2470, val loss: 2.0866174697875977
Epoch 2480, training loss: 62.12113571166992 = 0.008595314808189869 + 10.0 * 6.211254119873047
Epoch 2480, val loss: 2.0905580520629883
Epoch 2490, training loss: 62.190494537353516 = 0.008509901352226734 + 10.0 * 6.218198299407959
Epoch 2490, val loss: 2.0939786434173584
Epoch 2500, training loss: 62.128536224365234 = 0.008419056423008442 + 10.0 * 6.212011814117432
Epoch 2500, val loss: 2.0977776050567627
Epoch 2510, training loss: 62.11878967285156 = 0.00833271723240614 + 10.0 * 6.211045742034912
Epoch 2510, val loss: 2.1016948223114014
Epoch 2520, training loss: 62.12434005737305 = 0.008248643018305302 + 10.0 * 6.211609363555908
Epoch 2520, val loss: 2.105048179626465
Epoch 2530, training loss: 62.16775131225586 = 0.008165953680872917 + 10.0 * 6.215958595275879
Epoch 2530, val loss: 2.1088340282440186
Epoch 2540, training loss: 62.128631591796875 = 0.00808387715369463 + 10.0 * 6.21205472946167
Epoch 2540, val loss: 2.112595558166504
Epoch 2550, training loss: 62.11195373535156 = 0.008003631606698036 + 10.0 * 6.210394859313965
Epoch 2550, val loss: 2.116081953048706
Epoch 2560, training loss: 62.1413688659668 = 0.00792922917753458 + 10.0 * 6.213343620300293
Epoch 2560, val loss: 2.119753122329712
Epoch 2570, training loss: 62.1031494140625 = 0.007848056964576244 + 10.0 * 6.209530353546143
Epoch 2570, val loss: 2.1230976581573486
Epoch 2580, training loss: 62.11357498168945 = 0.007771651726216078 + 10.0 * 6.210580348968506
Epoch 2580, val loss: 2.1263535022735596
Epoch 2590, training loss: 62.15670394897461 = 0.00770052894949913 + 10.0 * 6.214900016784668
Epoch 2590, val loss: 2.129819631576538
Epoch 2600, training loss: 62.11737060546875 = 0.007622134406119585 + 10.0 * 6.21097469329834
Epoch 2600, val loss: 2.1333212852478027
Epoch 2610, training loss: 62.12285232543945 = 0.0075493245385587215 + 10.0 * 6.2115302085876465
Epoch 2610, val loss: 2.1364941596984863
Epoch 2620, training loss: 62.09352493286133 = 0.007476820610463619 + 10.0 * 6.20860481262207
Epoch 2620, val loss: 2.140054225921631
Epoch 2630, training loss: 62.093318939208984 = 0.007408385165035725 + 10.0 * 6.208590984344482
Epoch 2630, val loss: 2.1434884071350098
Epoch 2640, training loss: 62.10416793823242 = 0.007341696880757809 + 10.0 * 6.209682464599609
Epoch 2640, val loss: 2.146712303161621
Epoch 2650, training loss: 62.13649368286133 = 0.007273831870406866 + 10.0 * 6.212922096252441
Epoch 2650, val loss: 2.1498923301696777
Epoch 2660, training loss: 62.1313362121582 = 0.007205302361398935 + 10.0 * 6.2124128341674805
Epoch 2660, val loss: 2.1530909538269043
Epoch 2670, training loss: 62.096900939941406 = 0.00713949603959918 + 10.0 * 6.208975791931152
Epoch 2670, val loss: 2.1565380096435547
Epoch 2680, training loss: 62.092769622802734 = 0.007075265049934387 + 10.0 * 6.208569526672363
Epoch 2680, val loss: 2.1597704887390137
Epoch 2690, training loss: 62.1390495300293 = 0.007013663183897734 + 10.0 * 6.213203430175781
Epoch 2690, val loss: 2.1630406379699707
Epoch 2700, training loss: 62.10244369506836 = 0.006947046611458063 + 10.0 * 6.209549903869629
Epoch 2700, val loss: 2.1659204959869385
Epoch 2710, training loss: 62.0992317199707 = 0.006886116694658995 + 10.0 * 6.209234714508057
Epoch 2710, val loss: 2.1693859100341797
Epoch 2720, training loss: 62.1273193359375 = 0.006827306468039751 + 10.0 * 6.2120490074157715
Epoch 2720, val loss: 2.1725287437438965
Epoch 2730, training loss: 62.093814849853516 = 0.00676354905590415 + 10.0 * 6.208704948425293
Epoch 2730, val loss: 2.1754848957061768
Epoch 2740, training loss: 62.074405670166016 = 0.006705128122121096 + 10.0 * 6.206769943237305
Epoch 2740, val loss: 2.178650379180908
Epoch 2750, training loss: 62.07637023925781 = 0.0066489484161138535 + 10.0 * 6.206972122192383
Epoch 2750, val loss: 2.1815073490142822
Epoch 2760, training loss: 62.10251998901367 = 0.006592830643057823 + 10.0 * 6.209592819213867
Epoch 2760, val loss: 2.1842095851898193
Epoch 2770, training loss: 62.085208892822266 = 0.006535449530929327 + 10.0 * 6.207867622375488
Epoch 2770, val loss: 2.1876566410064697
Epoch 2780, training loss: 62.15073013305664 = 0.006480850744992495 + 10.0 * 6.214425086975098
Epoch 2780, val loss: 2.190708875656128
Epoch 2790, training loss: 62.09054946899414 = 0.006421583704650402 + 10.0 * 6.208413124084473
Epoch 2790, val loss: 2.193526268005371
Epoch 2800, training loss: 62.06890106201172 = 0.006368197035044432 + 10.0 * 6.206253528594971
Epoch 2800, val loss: 2.1967484951019287
Epoch 2810, training loss: 62.067726135253906 = 0.00631628418341279 + 10.0 * 6.206140995025635
Epoch 2810, val loss: 2.1995625495910645
Epoch 2820, training loss: 62.12069320678711 = 0.006267543416470289 + 10.0 * 6.211442470550537
Epoch 2820, val loss: 2.202342987060547
Epoch 2830, training loss: 62.06541061401367 = 0.006212554406374693 + 10.0 * 6.2059197425842285
Epoch 2830, val loss: 2.2054080963134766
Epoch 2840, training loss: 62.06565856933594 = 0.006161706522107124 + 10.0 * 6.205949783325195
Epoch 2840, val loss: 2.208151340484619
Epoch 2850, training loss: 62.109859466552734 = 0.006111844442784786 + 10.0 * 6.21037483215332
Epoch 2850, val loss: 2.210836172103882
Epoch 2860, training loss: 62.0590934753418 = 0.00606205128133297 + 10.0 * 6.205303192138672
Epoch 2860, val loss: 2.2137961387634277
Epoch 2870, training loss: 62.07631301879883 = 0.006014478392899036 + 10.0 * 6.207029819488525
Epoch 2870, val loss: 2.2166430950164795
Epoch 2880, training loss: 62.09642791748047 = 0.005965687800198793 + 10.0 * 6.209046363830566
Epoch 2880, val loss: 2.219147205352783
Epoch 2890, training loss: 62.05967712402344 = 0.005918580573052168 + 10.0 * 6.205375671386719
Epoch 2890, val loss: 2.2223734855651855
Epoch 2900, training loss: 62.05415725708008 = 0.005873202811926603 + 10.0 * 6.204828262329102
Epoch 2900, val loss: 2.2250618934631348
Epoch 2910, training loss: 62.10102081298828 = 0.005828710738569498 + 10.0 * 6.209519386291504
Epoch 2910, val loss: 2.2274250984191895
Epoch 2920, training loss: 62.0721321105957 = 0.005782668478786945 + 10.0 * 6.206634998321533
Epoch 2920, val loss: 2.230489492416382
Epoch 2930, training loss: 62.06195068359375 = 0.005734273698180914 + 10.0 * 6.205621719360352
Epoch 2930, val loss: 2.2330169677734375
Epoch 2940, training loss: 62.05195617675781 = 0.00569281866773963 + 10.0 * 6.204626560211182
Epoch 2940, val loss: 2.236020803451538
Epoch 2950, training loss: 62.047088623046875 = 0.005649828817695379 + 10.0 * 6.20414400100708
Epoch 2950, val loss: 2.2384393215179443
Epoch 2960, training loss: 62.047218322753906 = 0.00560976704582572 + 10.0 * 6.204160690307617
Epoch 2960, val loss: 2.241197347640991
Epoch 2970, training loss: 62.16343688964844 = 0.005570323206484318 + 10.0 * 6.215786933898926
Epoch 2970, val loss: 2.2436070442199707
Epoch 2980, training loss: 62.11363220214844 = 0.005523210391402245 + 10.0 * 6.210810661315918
Epoch 2980, val loss: 2.2461533546447754
Epoch 2990, training loss: 62.05580139160156 = 0.005480126943439245 + 10.0 * 6.205031871795654
Epoch 2990, val loss: 2.2491097450256348
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7037037037037037
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 87.93083190917969 = 1.9624824523925781 + 10.0 * 8.596835136413574
Epoch 0, val loss: 1.9580141305923462
Epoch 10, training loss: 87.91429138183594 = 1.9517784118652344 + 10.0 * 8.596250534057617
Epoch 10, val loss: 1.9466910362243652
Epoch 20, training loss: 87.85910034179688 = 1.93892240524292 + 10.0 * 8.592018127441406
Epoch 20, val loss: 1.9327031373977661
Epoch 30, training loss: 87.53955841064453 = 1.9230209589004517 + 10.0 * 8.561654090881348
Epoch 30, val loss: 1.9151986837387085
Epoch 40, training loss: 85.70484924316406 = 1.9058102369308472 + 10.0 * 8.379903793334961
Epoch 40, val loss: 1.8969660997390747
Epoch 50, training loss: 79.40678405761719 = 1.8880794048309326 + 10.0 * 7.751870632171631
Epoch 50, val loss: 1.8784111738204956
Epoch 60, training loss: 75.16824340820312 = 1.8735941648483276 + 10.0 * 7.329464435577393
Epoch 60, val loss: 1.8653186559677124
Epoch 70, training loss: 72.70086669921875 = 1.8615092039108276 + 10.0 * 7.083935260772705
Epoch 70, val loss: 1.8545091152191162
Epoch 80, training loss: 71.28540802001953 = 1.8488008975982666 + 10.0 * 6.943660736083984
Epoch 80, val loss: 1.842942476272583
Epoch 90, training loss: 70.04154968261719 = 1.837998628616333 + 10.0 * 6.82035493850708
Epoch 90, val loss: 1.8330069780349731
Epoch 100, training loss: 69.15824890136719 = 1.828230857849121 + 10.0 * 6.733001708984375
Epoch 100, val loss: 1.8238359689712524
Epoch 110, training loss: 68.56851196289062 = 1.8179523944854736 + 10.0 * 6.675055503845215
Epoch 110, val loss: 1.8142040967941284
Epoch 120, training loss: 68.1583023071289 = 1.8076794147491455 + 10.0 * 6.6350626945495605
Epoch 120, val loss: 1.804753065109253
Epoch 130, training loss: 67.8147201538086 = 1.7978159189224243 + 10.0 * 6.601690769195557
Epoch 130, val loss: 1.7957531213760376
Epoch 140, training loss: 67.49454498291016 = 1.7883713245391846 + 10.0 * 6.57061767578125
Epoch 140, val loss: 1.787054419517517
Epoch 150, training loss: 67.21751403808594 = 1.779002070426941 + 10.0 * 6.543851375579834
Epoch 150, val loss: 1.7785332202911377
Epoch 160, training loss: 66.95098876953125 = 1.769360899925232 + 10.0 * 6.518162250518799
Epoch 160, val loss: 1.7697900533676147
Epoch 170, training loss: 66.75035095214844 = 1.7591888904571533 + 10.0 * 6.49911642074585
Epoch 170, val loss: 1.7608354091644287
Epoch 180, training loss: 66.54114532470703 = 1.7483460903167725 + 10.0 * 6.4792799949646
Epoch 180, val loss: 1.7513705492019653
Epoch 190, training loss: 66.37805938720703 = 1.7365869283676147 + 10.0 * 6.464147090911865
Epoch 190, val loss: 1.7412850856781006
Epoch 200, training loss: 66.31245422363281 = 1.7237672805786133 + 10.0 * 6.458868026733398
Epoch 200, val loss: 1.730401635169983
Epoch 210, training loss: 66.12326049804688 = 1.709760308265686 + 10.0 * 6.441349983215332
Epoch 210, val loss: 1.7185612916946411
Epoch 220, training loss: 66.00848388671875 = 1.6945801973342896 + 10.0 * 6.431390762329102
Epoch 220, val loss: 1.7058097124099731
Epoch 230, training loss: 65.9354476928711 = 1.6780147552490234 + 10.0 * 6.425743103027344
Epoch 230, val loss: 1.6919896602630615
Epoch 240, training loss: 65.80694580078125 = 1.6599178314208984 + 10.0 * 6.414702892303467
Epoch 240, val loss: 1.6769695281982422
Epoch 250, training loss: 65.70069122314453 = 1.640289306640625 + 10.0 * 6.406040191650391
Epoch 250, val loss: 1.660606026649475
Epoch 260, training loss: 65.62007904052734 = 1.6189714670181274 + 10.0 * 6.400110721588135
Epoch 260, val loss: 1.6429340839385986
Epoch 270, training loss: 65.57717895507812 = 1.5958970785140991 + 10.0 * 6.398128509521484
Epoch 270, val loss: 1.623806118965149
Epoch 280, training loss: 65.43926239013672 = 1.5711112022399902 + 10.0 * 6.386815071105957
Epoch 280, val loss: 1.603399634361267
Epoch 290, training loss: 65.35800170898438 = 1.544679045677185 + 10.0 * 6.3813323974609375
Epoch 290, val loss: 1.5816441774368286
Epoch 300, training loss: 65.27566528320312 = 1.5166414976119995 + 10.0 * 6.3759026527404785
Epoch 300, val loss: 1.5586189031600952
Epoch 310, training loss: 65.21419525146484 = 1.4869846105575562 + 10.0 * 6.372720718383789
Epoch 310, val loss: 1.5343852043151855
Epoch 320, training loss: 65.1222152709961 = 1.4560796022415161 + 10.0 * 6.366613388061523
Epoch 320, val loss: 1.5091300010681152
Epoch 330, training loss: 65.04817962646484 = 1.4240610599517822 + 10.0 * 6.3624114990234375
Epoch 330, val loss: 1.483082890510559
Epoch 340, training loss: 64.98917388916016 = 1.391061782836914 + 10.0 * 6.359811305999756
Epoch 340, val loss: 1.4563095569610596
Epoch 350, training loss: 64.89473724365234 = 1.3574779033660889 + 10.0 * 6.353725910186768
Epoch 350, val loss: 1.4291242361068726
Epoch 360, training loss: 64.82186889648438 = 1.3233412504196167 + 10.0 * 6.349853038787842
Epoch 360, val loss: 1.4017201662063599
Epoch 370, training loss: 64.80436706542969 = 1.288851022720337 + 10.0 * 6.351551532745361
Epoch 370, val loss: 1.3742244243621826
Epoch 380, training loss: 64.7042465209961 = 1.2544907331466675 + 10.0 * 6.344975471496582
Epoch 380, val loss: 1.3469364643096924
Epoch 390, training loss: 64.6225814819336 = 1.2202820777893066 + 10.0 * 6.3402299880981445
Epoch 390, val loss: 1.3201165199279785
Epoch 400, training loss: 64.58501434326172 = 1.186545491218567 + 10.0 * 6.339846611022949
Epoch 400, val loss: 1.293870210647583
Epoch 410, training loss: 64.51119995117188 = 1.1531167030334473 + 10.0 * 6.335808753967285
Epoch 410, val loss: 1.26835298538208
Epoch 420, training loss: 64.42329406738281 = 1.1204948425292969 + 10.0 * 6.330279350280762
Epoch 420, val loss: 1.2435979843139648
Epoch 430, training loss: 64.37296295166016 = 1.0886133909225464 + 10.0 * 6.328434944152832
Epoch 430, val loss: 1.2197502851486206
Epoch 440, training loss: 64.31399536132812 = 1.0573664903640747 + 10.0 * 6.325662612915039
Epoch 440, val loss: 1.1968332529067993
Epoch 450, training loss: 64.27645874023438 = 1.0271415710449219 + 10.0 * 6.3249311447143555
Epoch 450, val loss: 1.17502760887146
Epoch 460, training loss: 64.1998291015625 = 0.9977858066558838 + 10.0 * 6.320204257965088
Epoch 460, val loss: 1.1542235612869263
Epoch 470, training loss: 64.14450073242188 = 0.9693204164505005 + 10.0 * 6.31751823425293
Epoch 470, val loss: 1.1347203254699707
Epoch 480, training loss: 64.0972900390625 = 0.9418506026268005 + 10.0 * 6.3155436515808105
Epoch 480, val loss: 1.116321086883545
Epoch 490, training loss: 64.08114624023438 = 0.9151979684829712 + 10.0 * 6.316595077514648
Epoch 490, val loss: 1.0989668369293213
Epoch 500, training loss: 64.01326751708984 = 0.8894487023353577 + 10.0 * 6.312381744384766
Epoch 500, val loss: 1.0826780796051025
Epoch 510, training loss: 63.963226318359375 = 0.864625871181488 + 10.0 * 6.3098602294921875
Epoch 510, val loss: 1.067686915397644
Epoch 520, training loss: 63.927547454833984 = 0.8406798839569092 + 10.0 * 6.30868673324585
Epoch 520, val loss: 1.0537714958190918
Epoch 530, training loss: 63.886234283447266 = 0.8174422979354858 + 10.0 * 6.306879043579102
Epoch 530, val loss: 1.040628433227539
Epoch 540, training loss: 63.83060836791992 = 0.7950013875961304 + 10.0 * 6.303560733795166
Epoch 540, val loss: 1.0283640623092651
Epoch 550, training loss: 63.81736755371094 = 0.773205578327179 + 10.0 * 6.304416179656982
Epoch 550, val loss: 1.0171478986740112
Epoch 560, training loss: 63.759986877441406 = 0.7520889043807983 + 10.0 * 6.300789833068848
Epoch 560, val loss: 1.0064467191696167
Epoch 570, training loss: 63.7303466796875 = 0.7314631938934326 + 10.0 * 6.2998881340026855
Epoch 570, val loss: 0.9967741966247559
Epoch 580, training loss: 63.677467346191406 = 0.7114043235778809 + 10.0 * 6.296606540679932
Epoch 580, val loss: 0.9873929619789124
Epoch 590, training loss: 63.653411865234375 = 0.6917635202407837 + 10.0 * 6.296164512634277
Epoch 590, val loss: 0.9788673520088196
Epoch 600, training loss: 63.617774963378906 = 0.6725525856018066 + 10.0 * 6.294522285461426
Epoch 600, val loss: 0.9708002209663391
Epoch 610, training loss: 63.56731414794922 = 0.6537660956382751 + 10.0 * 6.291354656219482
Epoch 610, val loss: 0.9632403254508972
Epoch 620, training loss: 63.53478240966797 = 0.6353780627250671 + 10.0 * 6.289940357208252
Epoch 620, val loss: 0.9562405943870544
Epoch 630, training loss: 63.548885345458984 = 0.6173353791236877 + 10.0 * 6.293154716491699
Epoch 630, val loss: 0.9497130513191223
Epoch 640, training loss: 63.51165008544922 = 0.5995838046073914 + 10.0 * 6.2912068367004395
Epoch 640, val loss: 0.943264901638031
Epoch 650, training loss: 63.43952941894531 = 0.5820975303649902 + 10.0 * 6.285743236541748
Epoch 650, val loss: 0.9373260736465454
Epoch 660, training loss: 63.44142532348633 = 0.564968466758728 + 10.0 * 6.2876458168029785
Epoch 660, val loss: 0.9318670630455017
Epoch 670, training loss: 63.384681701660156 = 0.5482435822486877 + 10.0 * 6.28364372253418
Epoch 670, val loss: 0.9267594218254089
Epoch 680, training loss: 63.349952697753906 = 0.5316970944404602 + 10.0 * 6.281825542449951
Epoch 680, val loss: 0.9218711853027344
Epoch 690, training loss: 63.3364143371582 = 0.5155372619628906 + 10.0 * 6.282087802886963
Epoch 690, val loss: 0.9175728559494019
Epoch 700, training loss: 63.316158294677734 = 0.49957722425460815 + 10.0 * 6.281658172607422
Epoch 700, val loss: 0.9131550788879395
Epoch 710, training loss: 63.282466888427734 = 0.48392531275749207 + 10.0 * 6.2798542976379395
Epoch 710, val loss: 0.9093337655067444
Epoch 720, training loss: 63.244449615478516 = 0.46859049797058105 + 10.0 * 6.277585983276367
Epoch 720, val loss: 0.9057096242904663
Epoch 730, training loss: 63.24429702758789 = 0.45363718271255493 + 10.0 * 6.27906608581543
Epoch 730, val loss: 0.9024123549461365
Epoch 740, training loss: 63.206241607666016 = 0.4389265179634094 + 10.0 * 6.276731491088867
Epoch 740, val loss: 0.8996217846870422
Epoch 750, training loss: 63.16511535644531 = 0.4245460629463196 + 10.0 * 6.274056911468506
Epoch 750, val loss: 0.8968450427055359
Epoch 760, training loss: 63.17597961425781 = 0.4105069041252136 + 10.0 * 6.276547431945801
Epoch 760, val loss: 0.8945087790489197
Epoch 770, training loss: 63.124267578125 = 0.39675965905189514 + 10.0 * 6.2727508544921875
Epoch 770, val loss: 0.8923237919807434
Epoch 780, training loss: 63.097679138183594 = 0.38337600231170654 + 10.0 * 6.271430015563965
Epoch 780, val loss: 0.890531599521637
Epoch 790, training loss: 63.1251335144043 = 0.3703254163265228 + 10.0 * 6.2754807472229
Epoch 790, val loss: 0.8889298439025879
Epoch 800, training loss: 63.063148498535156 = 0.3576217293739319 + 10.0 * 6.270552635192871
Epoch 800, val loss: 0.8875705599784851
Epoch 810, training loss: 63.03460693359375 = 0.34525904059410095 + 10.0 * 6.268934726715088
Epoch 810, val loss: 0.886543333530426
Epoch 820, training loss: 63.07868957519531 = 0.3332670331001282 + 10.0 * 6.274542331695557
Epoch 820, val loss: 0.8856800198554993
Epoch 830, training loss: 63.00503158569336 = 0.3217051327228546 + 10.0 * 6.268332481384277
Epoch 830, val loss: 0.8852084875106812
Epoch 840, training loss: 62.9674072265625 = 0.3104117214679718 + 10.0 * 6.26569938659668
Epoch 840, val loss: 0.8849272727966309
Epoch 850, training loss: 62.94603729248047 = 0.29959017038345337 + 10.0 * 6.264644622802734
Epoch 850, val loss: 0.885109543800354
Epoch 860, training loss: 63.00717544555664 = 0.28905975818634033 + 10.0 * 6.271811485290527
Epoch 860, val loss: 0.8854142427444458
Epoch 870, training loss: 62.94464874267578 = 0.2788541913032532 + 10.0 * 6.266579627990723
Epoch 870, val loss: 0.8856589198112488
Epoch 880, training loss: 62.9006233215332 = 0.2689835727214813 + 10.0 * 6.263164043426514
Epoch 880, val loss: 0.8864409923553467
Epoch 890, training loss: 62.880401611328125 = 0.25952044129371643 + 10.0 * 6.262087821960449
Epoch 890, val loss: 0.887514591217041
Epoch 900, training loss: 62.8626823425293 = 0.25042933225631714 + 10.0 * 6.26122522354126
Epoch 900, val loss: 0.8888413906097412
Epoch 910, training loss: 62.87263107299805 = 0.24169015884399414 + 10.0 * 6.263093948364258
Epoch 910, val loss: 0.8903529644012451
Epoch 920, training loss: 62.84574508666992 = 0.2331967055797577 + 10.0 * 6.261254787445068
Epoch 920, val loss: 0.8923408389091492
Epoch 930, training loss: 62.8704948425293 = 0.22505776584148407 + 10.0 * 6.264543533325195
Epoch 930, val loss: 0.8940603137016296
Epoch 940, training loss: 62.81065368652344 = 0.2171972095966339 + 10.0 * 6.259345531463623
Epoch 940, val loss: 0.8962328433990479
Epoch 950, training loss: 62.78010559082031 = 0.20971520245075226 + 10.0 * 6.2570390701293945
Epoch 950, val loss: 0.89872807264328
Epoch 960, training loss: 62.769474029541016 = 0.20252959430217743 + 10.0 * 6.256694316864014
Epoch 960, val loss: 0.9014703035354614
Epoch 970, training loss: 62.85713195800781 = 0.19561344385147095 + 10.0 * 6.2661519050598145
Epoch 970, val loss: 0.9041301608085632
Epoch 980, training loss: 62.74699401855469 = 0.1888360232114792 + 10.0 * 6.2558159828186035
Epoch 980, val loss: 0.9069429636001587
Epoch 990, training loss: 62.73292922973633 = 0.18239890038967133 + 10.0 * 6.2550530433654785
Epoch 990, val loss: 0.9098628163337708
Epoch 1000, training loss: 62.70969772338867 = 0.17629826068878174 + 10.0 * 6.253339767456055
Epoch 1000, val loss: 0.9133445620536804
Epoch 1010, training loss: 62.69789505004883 = 0.17044608294963837 + 10.0 * 6.252745151519775
Epoch 1010, val loss: 0.9169114828109741
Epoch 1020, training loss: 62.785850524902344 = 0.16480325162410736 + 10.0 * 6.2621049880981445
Epoch 1020, val loss: 0.9204137325286865
Epoch 1030, training loss: 62.709571838378906 = 0.15930268168449402 + 10.0 * 6.255026817321777
Epoch 1030, val loss: 0.9241569638252258
Epoch 1040, training loss: 62.67779541015625 = 0.15405866503715515 + 10.0 * 6.252373695373535
Epoch 1040, val loss: 0.9280520677566528
Epoch 1050, training loss: 62.654808044433594 = 0.14902648329734802 + 10.0 * 6.2505784034729
Epoch 1050, val loss: 0.9319975972175598
Epoch 1060, training loss: 62.643707275390625 = 0.14421072602272034 + 10.0 * 6.2499494552612305
Epoch 1060, val loss: 0.9361843466758728
Epoch 1070, training loss: 62.70779800415039 = 0.13957460224628448 + 10.0 * 6.256822109222412
Epoch 1070, val loss: 0.9405348300933838
Epoch 1080, training loss: 62.668880462646484 = 0.13508324325084686 + 10.0 * 6.253379821777344
Epoch 1080, val loss: 0.9445586204528809
Epoch 1090, training loss: 62.620018005371094 = 0.13076837360858917 + 10.0 * 6.24892520904541
Epoch 1090, val loss: 0.9488697052001953
Epoch 1100, training loss: 62.621742248535156 = 0.1266649216413498 + 10.0 * 6.249507904052734
Epoch 1100, val loss: 0.9534665942192078
Epoch 1110, training loss: 62.596656799316406 = 0.12268045544624329 + 10.0 * 6.247397422790527
Epoch 1110, val loss: 0.9580053091049194
Epoch 1120, training loss: 62.59069061279297 = 0.11887392401695251 + 10.0 * 6.2471818923950195
Epoch 1120, val loss: 0.9628180861473083
Epoch 1130, training loss: 62.57315444946289 = 0.1152149885892868 + 10.0 * 6.24579381942749
Epoch 1130, val loss: 0.9675527811050415
Epoch 1140, training loss: 62.63204574584961 = 0.11169616878032684 + 10.0 * 6.252035140991211
Epoch 1140, val loss: 0.9724741578102112
Epoch 1150, training loss: 62.602054595947266 = 0.10825610905885696 + 10.0 * 6.249379634857178
Epoch 1150, val loss: 0.9770095348358154
Epoch 1160, training loss: 62.56794357299805 = 0.10497673600912094 + 10.0 * 6.2462968826293945
Epoch 1160, val loss: 0.9819410443305969
Epoch 1170, training loss: 62.54368209838867 = 0.10182012617588043 + 10.0 * 6.244185924530029
Epoch 1170, val loss: 0.98704594373703
Epoch 1180, training loss: 62.56654357910156 = 0.09880893677473068 + 10.0 * 6.246773719787598
Epoch 1180, val loss: 0.9921332597732544
Epoch 1190, training loss: 62.53288269042969 = 0.09589336067438126 + 10.0 * 6.243699073791504
Epoch 1190, val loss: 0.9970531463623047
Epoch 1200, training loss: 62.5423469543457 = 0.09308287501335144 + 10.0 * 6.244926452636719
Epoch 1200, val loss: 1.002261996269226
Epoch 1210, training loss: 62.5190544128418 = 0.09037117660045624 + 10.0 * 6.242868423461914
Epoch 1210, val loss: 1.007206916809082
Epoch 1220, training loss: 62.50940704345703 = 0.08775199949741364 + 10.0 * 6.242165565490723
Epoch 1220, val loss: 1.012219786643982
Epoch 1230, training loss: 62.525455474853516 = 0.0852537676692009 + 10.0 * 6.244019985198975
Epoch 1230, val loss: 1.0174387693405151
Epoch 1240, training loss: 62.50102996826172 = 0.08282066881656647 + 10.0 * 6.241820812225342
Epoch 1240, val loss: 1.0224584341049194
Epoch 1250, training loss: 62.51774215698242 = 0.08048535138368607 + 10.0 * 6.243725776672363
Epoch 1250, val loss: 1.027653694152832
Epoch 1260, training loss: 62.48507308959961 = 0.07821174710988998 + 10.0 * 6.240685939788818
Epoch 1260, val loss: 1.032731294631958
Epoch 1270, training loss: 62.47046661376953 = 0.0760532096028328 + 10.0 * 6.239441394805908
Epoch 1270, val loss: 1.038062572479248
Epoch 1280, training loss: 62.501224517822266 = 0.07397549599409103 + 10.0 * 6.242724895477295
Epoch 1280, val loss: 1.0433454513549805
Epoch 1290, training loss: 62.48317337036133 = 0.07194902002811432 + 10.0 * 6.241122245788574
Epoch 1290, val loss: 1.048427700996399
Epoch 1300, training loss: 62.46599578857422 = 0.06998468935489655 + 10.0 * 6.239601135253906
Epoch 1300, val loss: 1.05348539352417
Epoch 1310, training loss: 62.47582244873047 = 0.0681045800447464 + 10.0 * 6.240771770477295
Epoch 1310, val loss: 1.0586602687835693
Epoch 1320, training loss: 62.45540237426758 = 0.06629490107297897 + 10.0 * 6.238910675048828
Epoch 1320, val loss: 1.0640121698379517
Epoch 1330, training loss: 62.433631896972656 = 0.06455039978027344 + 10.0 * 6.236907958984375
Epoch 1330, val loss: 1.0689631700515747
Epoch 1340, training loss: 62.440330505371094 = 0.06287483870983124 + 10.0 * 6.237745761871338
Epoch 1340, val loss: 1.074134349822998
Epoch 1350, training loss: 62.46076965332031 = 0.061251986771821976 + 10.0 * 6.2399516105651855
Epoch 1350, val loss: 1.0793973207473755
Epoch 1360, training loss: 62.43951416015625 = 0.05967501550912857 + 10.0 * 6.2379841804504395
Epoch 1360, val loss: 1.0845391750335693
Epoch 1370, training loss: 62.442352294921875 = 0.058157067745923996 + 10.0 * 6.238419532775879
Epoch 1370, val loss: 1.0894560813903809
Epoch 1380, training loss: 62.42985153198242 = 0.0566851831972599 + 10.0 * 6.237316608428955
Epoch 1380, val loss: 1.0946824550628662
Epoch 1390, training loss: 62.415897369384766 = 0.05525635555386543 + 10.0 * 6.2360639572143555
Epoch 1390, val loss: 1.099664330482483
Epoch 1400, training loss: 62.397254943847656 = 0.053893573582172394 + 10.0 * 6.234335899353027
Epoch 1400, val loss: 1.1048965454101562
Epoch 1410, training loss: 62.41926956176758 = 0.05258488655090332 + 10.0 * 6.236668586730957
Epoch 1410, val loss: 1.1099274158477783
Epoch 1420, training loss: 62.387935638427734 = 0.05129845440387726 + 10.0 * 6.233663558959961
Epoch 1420, val loss: 1.1150192022323608
Epoch 1430, training loss: 62.39532470703125 = 0.05006573721766472 + 10.0 * 6.23452615737915
Epoch 1430, val loss: 1.1199578046798706
Epoch 1440, training loss: 62.39704513549805 = 0.048868328332901 + 10.0 * 6.2348175048828125
Epoch 1440, val loss: 1.1248770952224731
Epoch 1450, training loss: 62.392364501953125 = 0.04772377014160156 + 10.0 * 6.234464168548584
Epoch 1450, val loss: 1.1299479007720947
Epoch 1460, training loss: 62.45405578613281 = 0.04659172520041466 + 10.0 * 6.24074649810791
Epoch 1460, val loss: 1.134934425354004
Epoch 1470, training loss: 62.38526153564453 = 0.0455002598464489 + 10.0 * 6.233975887298584
Epoch 1470, val loss: 1.1396037340164185
Epoch 1480, training loss: 62.35785675048828 = 0.04444091394543648 + 10.0 * 6.231341361999512
Epoch 1480, val loss: 1.1445257663726807
Epoch 1490, training loss: 62.34823226928711 = 0.04344608634710312 + 10.0 * 6.230478763580322
Epoch 1490, val loss: 1.1495481729507446
Epoch 1500, training loss: 62.34357452392578 = 0.04247880354523659 + 10.0 * 6.230109214782715
Epoch 1500, val loss: 1.154474139213562
Epoch 1510, training loss: 62.380680084228516 = 0.04154197871685028 + 10.0 * 6.233913898468018
Epoch 1510, val loss: 1.1593514680862427
Epoch 1520, training loss: 62.342777252197266 = 0.0406157560646534 + 10.0 * 6.230216026306152
Epoch 1520, val loss: 1.1638548374176025
Epoch 1530, training loss: 62.354068756103516 = 0.039713989943265915 + 10.0 * 6.231435298919678
Epoch 1530, val loss: 1.1686019897460938
Epoch 1540, training loss: 62.331687927246094 = 0.038845185190439224 + 10.0 * 6.229284286499023
Epoch 1540, val loss: 1.1732563972473145
Epoch 1550, training loss: 62.336326599121094 = 0.03801531344652176 + 10.0 * 6.229831218719482
Epoch 1550, val loss: 1.1780035495758057
Epoch 1560, training loss: 62.3667106628418 = 0.03721141815185547 + 10.0 * 6.232949733734131
Epoch 1560, val loss: 1.182674527168274
Epoch 1570, training loss: 62.37551498413086 = 0.03641003370285034 + 10.0 * 6.23391056060791
Epoch 1570, val loss: 1.1872562170028687
Epoch 1580, training loss: 62.32311248779297 = 0.03562900424003601 + 10.0 * 6.228748321533203
Epoch 1580, val loss: 1.1914982795715332
Epoch 1590, training loss: 62.31322479248047 = 0.03488805890083313 + 10.0 * 6.2278337478637695
Epoch 1590, val loss: 1.1961697340011597
Epoch 1600, training loss: 62.30511474609375 = 0.034182678908109665 + 10.0 * 6.22709321975708
Epoch 1600, val loss: 1.2007981538772583
Epoch 1610, training loss: 62.306827545166016 = 0.03349945321679115 + 10.0 * 6.227332592010498
Epoch 1610, val loss: 1.205242395401001
Epoch 1620, training loss: 62.39921951293945 = 0.03282712399959564 + 10.0 * 6.236639499664307
Epoch 1620, val loss: 1.209497094154358
Epoch 1630, training loss: 62.317527770996094 = 0.032141782343387604 + 10.0 * 6.228538513183594
Epoch 1630, val loss: 1.213826298713684
Epoch 1640, training loss: 62.29144287109375 = 0.03150106593966484 + 10.0 * 6.225994110107422
Epoch 1640, val loss: 1.218347430229187
Epoch 1650, training loss: 62.28530502319336 = 0.03089103475213051 + 10.0 * 6.2254414558410645
Epoch 1650, val loss: 1.222643256187439
Epoch 1660, training loss: 62.29965591430664 = 0.030296601355075836 + 10.0 * 6.226935863494873
Epoch 1660, val loss: 1.2270694971084595
Epoch 1670, training loss: 62.30557632446289 = 0.02970420941710472 + 10.0 * 6.2275872230529785
Epoch 1670, val loss: 1.2310746908187866
Epoch 1680, training loss: 62.28129577636719 = 0.02913500741124153 + 10.0 * 6.225215911865234
Epoch 1680, val loss: 1.2352889776229858
Epoch 1690, training loss: 62.27682113647461 = 0.028583070263266563 + 10.0 * 6.224823951721191
Epoch 1690, val loss: 1.239586353302002
Epoch 1700, training loss: 62.288841247558594 = 0.02805403806269169 + 10.0 * 6.226078987121582
Epoch 1700, val loss: 1.2438160181045532
Epoch 1710, training loss: 62.2788200378418 = 0.027532346546649933 + 10.0 * 6.225129127502441
Epoch 1710, val loss: 1.247965931892395
Epoch 1720, training loss: 62.313804626464844 = 0.02702534757554531 + 10.0 * 6.228677749633789
Epoch 1720, val loss: 1.2521339654922485
Epoch 1730, training loss: 62.28793716430664 = 0.026530079543590546 + 10.0 * 6.22614049911499
Epoch 1730, val loss: 1.2559514045715332
Epoch 1740, training loss: 62.27497100830078 = 0.026046480983495712 + 10.0 * 6.224892616271973
Epoch 1740, val loss: 1.2600915431976318
Epoch 1750, training loss: 62.252593994140625 = 0.02558252029120922 + 10.0 * 6.222701072692871
Epoch 1750, val loss: 1.2640756368637085
Epoch 1760, training loss: 62.251258850097656 = 0.025133095681667328 + 10.0 * 6.2226128578186035
Epoch 1760, val loss: 1.2680563926696777
Epoch 1770, training loss: 62.276878356933594 = 0.024700118228793144 + 10.0 * 6.225217819213867
Epoch 1770, val loss: 1.2719963788986206
Epoch 1780, training loss: 62.2860221862793 = 0.024263838306069374 + 10.0 * 6.226175785064697
Epoch 1780, val loss: 1.2759089469909668
Epoch 1790, training loss: 62.25355529785156 = 0.02382192201912403 + 10.0 * 6.222973346710205
Epoch 1790, val loss: 1.2795968055725098
Epoch 1800, training loss: 62.252655029296875 = 0.02341541461646557 + 10.0 * 6.22292423248291
Epoch 1800, val loss: 1.2833529710769653
Epoch 1810, training loss: 62.273433685302734 = 0.023021018132567406 + 10.0 * 6.225041389465332
Epoch 1810, val loss: 1.2872965335845947
Epoch 1820, training loss: 62.25252914428711 = 0.022631999105215073 + 10.0 * 6.222989559173584
Epoch 1820, val loss: 1.2912487983703613
Epoch 1830, training loss: 62.23394012451172 = 0.022248296067118645 + 10.0 * 6.221169471740723
Epoch 1830, val loss: 1.2948687076568604
Epoch 1840, training loss: 62.23057556152344 = 0.021885672584176064 + 10.0 * 6.220869064331055
Epoch 1840, val loss: 1.2985713481903076
Epoch 1850, training loss: 62.26148986816406 = 0.021531157195568085 + 10.0 * 6.223996162414551
Epoch 1850, val loss: 1.3023452758789062
Epoch 1860, training loss: 62.242618560791016 = 0.02117381989955902 + 10.0 * 6.22214412689209
Epoch 1860, val loss: 1.3060013055801392
Epoch 1870, training loss: 62.22740936279297 = 0.02082853578031063 + 10.0 * 6.220658302307129
Epoch 1870, val loss: 1.3095803260803223
Epoch 1880, training loss: 62.218177795410156 = 0.020495746284723282 + 10.0 * 6.219768047332764
Epoch 1880, val loss: 1.313265085220337
Epoch 1890, training loss: 62.226707458496094 = 0.020177917554974556 + 10.0 * 6.2206525802612305
Epoch 1890, val loss: 1.3170260190963745
Epoch 1900, training loss: 62.241737365722656 = 0.01986175961792469 + 10.0 * 6.222187519073486
Epoch 1900, val loss: 1.3205311298370361
Epoch 1910, training loss: 62.2128791809082 = 0.01953984797000885 + 10.0 * 6.219334125518799
Epoch 1910, val loss: 1.323914647102356
Epoch 1920, training loss: 62.2380256652832 = 0.019241295754909515 + 10.0 * 6.221878528594971
Epoch 1920, val loss: 1.3275467157363892
Epoch 1930, training loss: 62.20748519897461 = 0.018939418718218803 + 10.0 * 6.2188544273376465
Epoch 1930, val loss: 1.331152319908142
Epoch 1940, training loss: 62.212955474853516 = 0.01864505559206009 + 10.0 * 6.219430923461914
Epoch 1940, val loss: 1.3346121311187744
Epoch 1950, training loss: 62.21923065185547 = 0.018363507464528084 + 10.0 * 6.220086574554443
Epoch 1950, val loss: 1.338065266609192
Epoch 1960, training loss: 62.2261848449707 = 0.018085986375808716 + 10.0 * 6.2208099365234375
Epoch 1960, val loss: 1.3413536548614502
Epoch 1970, training loss: 62.19361114501953 = 0.017814816907048225 + 10.0 * 6.2175798416137695
Epoch 1970, val loss: 1.3447871208190918
Epoch 1980, training loss: 62.19962692260742 = 0.017551904544234276 + 10.0 * 6.218207359313965
Epoch 1980, val loss: 1.3482422828674316
Epoch 1990, training loss: 62.20780563354492 = 0.017298055812716484 + 10.0 * 6.219050407409668
Epoch 1990, val loss: 1.351528525352478
Epoch 2000, training loss: 62.21103286743164 = 0.01704278588294983 + 10.0 * 6.2193989753723145
Epoch 2000, val loss: 1.3548418283462524
Epoch 2010, training loss: 62.186553955078125 = 0.016794616356492043 + 10.0 * 6.216975688934326
Epoch 2010, val loss: 1.3582426309585571
Epoch 2020, training loss: 62.198890686035156 = 0.016555529087781906 + 10.0 * 6.218233585357666
Epoch 2020, val loss: 1.3615938425064087
Epoch 2030, training loss: 62.22938919067383 = 0.016316061839461327 + 10.0 * 6.221307277679443
Epoch 2030, val loss: 1.3647586107254028
Epoch 2040, training loss: 62.2032356262207 = 0.01608552224934101 + 10.0 * 6.218714714050293
Epoch 2040, val loss: 1.3677488565444946
Epoch 2050, training loss: 62.18482208251953 = 0.01585487462580204 + 10.0 * 6.216897010803223
Epoch 2050, val loss: 1.3710252046585083
Epoch 2060, training loss: 62.21706771850586 = 0.015636349096894264 + 10.0 * 6.2201433181762695
Epoch 2060, val loss: 1.3738478422164917
Epoch 2070, training loss: 62.173255920410156 = 0.015415709465742111 + 10.0 * 6.215784072875977
Epoch 2070, val loss: 1.377265214920044
Epoch 2080, training loss: 62.16096878051758 = 0.015203255228698254 + 10.0 * 6.214576721191406
Epoch 2080, val loss: 1.3802204132080078
Epoch 2090, training loss: 62.161643981933594 = 0.015001305378973484 + 10.0 * 6.214663982391357
Epoch 2090, val loss: 1.3833736181259155
Epoch 2100, training loss: 62.186519622802734 = 0.014804585836827755 + 10.0 * 6.217171669006348
Epoch 2100, val loss: 1.3865574598312378
Epoch 2110, training loss: 62.17477798461914 = 0.014600016176700592 + 10.0 * 6.216017723083496
Epoch 2110, val loss: 1.389434814453125
Epoch 2120, training loss: 62.20766067504883 = 0.014403151348233223 + 10.0 * 6.219325542449951
Epoch 2120, val loss: 1.3924142122268677
Epoch 2130, training loss: 62.15715026855469 = 0.014209444634616375 + 10.0 * 6.214293956756592
Epoch 2130, val loss: 1.3952645063400269
Epoch 2140, training loss: 62.15292739868164 = 0.014026978984475136 + 10.0 * 6.213890075683594
Epoch 2140, val loss: 1.3984320163726807
Epoch 2150, training loss: 62.153411865234375 = 0.013847504742443562 + 10.0 * 6.213956356048584
Epoch 2150, val loss: 1.4014201164245605
Epoch 2160, training loss: 62.205875396728516 = 0.013674067333340645 + 10.0 * 6.219220161437988
Epoch 2160, val loss: 1.4044045209884644
Epoch 2170, training loss: 62.173343658447266 = 0.013491543009877205 + 10.0 * 6.215985298156738
Epoch 2170, val loss: 1.4070557355880737
Epoch 2180, training loss: 62.20348358154297 = 0.013317962177097797 + 10.0 * 6.2190165519714355
Epoch 2180, val loss: 1.409900426864624
Epoch 2190, training loss: 62.161643981933594 = 0.013146293349564075 + 10.0 * 6.214849948883057
Epoch 2190, val loss: 1.4123989343643188
Epoch 2200, training loss: 62.1405029296875 = 0.012983801774680614 + 10.0 * 6.212751865386963
Epoch 2200, val loss: 1.4154492616653442
Epoch 2210, training loss: 62.14051055908203 = 0.012826451100409031 + 10.0 * 6.2127685546875
Epoch 2210, val loss: 1.4183530807495117
Epoch 2220, training loss: 62.196250915527344 = 0.012674848549067974 + 10.0 * 6.218357563018799
Epoch 2220, val loss: 1.4211084842681885
Epoch 2230, training loss: 62.140899658203125 = 0.012508965097367764 + 10.0 * 6.212839126586914
Epoch 2230, val loss: 1.4234418869018555
Epoch 2240, training loss: 62.176124572753906 = 0.012356986291706562 + 10.0 * 6.216376781463623
Epoch 2240, val loss: 1.4263207912445068
Epoch 2250, training loss: 62.12557601928711 = 0.012204199098050594 + 10.0 * 6.211337089538574
Epoch 2250, val loss: 1.428932785987854
Epoch 2260, training loss: 62.12384033203125 = 0.01205939706414938 + 10.0 * 6.211178302764893
Epoch 2260, val loss: 1.4317325353622437
Epoch 2270, training loss: 62.12726593017578 = 0.011920328252017498 + 10.0 * 6.21153450012207
Epoch 2270, val loss: 1.4344748258590698
Epoch 2280, training loss: 62.18693161010742 = 0.011783256195485592 + 10.0 * 6.217514991760254
Epoch 2280, val loss: 1.4370778799057007
Epoch 2290, training loss: 62.14780807495117 = 0.011637195944786072 + 10.0 * 6.213616847991943
Epoch 2290, val loss: 1.4392797946929932
Epoch 2300, training loss: 62.129844665527344 = 0.011501314118504524 + 10.0 * 6.21183443069458
Epoch 2300, val loss: 1.4420017004013062
Epoch 2310, training loss: 62.14905548095703 = 0.011369267478585243 + 10.0 * 6.21376895904541
Epoch 2310, val loss: 1.444410800933838
Epoch 2320, training loss: 62.13832473754883 = 0.011235665529966354 + 10.0 * 6.212708950042725
Epoch 2320, val loss: 1.447153925895691
Epoch 2330, training loss: 62.14091873168945 = 0.011104780249297619 + 10.0 * 6.212981224060059
Epoch 2330, val loss: 1.4496865272521973
Epoch 2340, training loss: 62.13190460205078 = 0.010975496843457222 + 10.0 * 6.212092876434326
Epoch 2340, val loss: 1.4519271850585938
Epoch 2350, training loss: 62.13109588623047 = 0.010853537358343601 + 10.0 * 6.212024211883545
Epoch 2350, val loss: 1.4543954133987427
Epoch 2360, training loss: 62.10953903198242 = 0.010731831192970276 + 10.0 * 6.209880828857422
Epoch 2360, val loss: 1.4570553302764893
Epoch 2370, training loss: 62.10785675048828 = 0.010613756254315376 + 10.0 * 6.209724426269531
Epoch 2370, val loss: 1.459494709968567
Epoch 2380, training loss: 62.14107894897461 = 0.010499715805053711 + 10.0 * 6.213057994842529
Epoch 2380, val loss: 1.4619572162628174
Epoch 2390, training loss: 62.101322174072266 = 0.010381273925304413 + 10.0 * 6.209094047546387
Epoch 2390, val loss: 1.464020848274231
Epoch 2400, training loss: 62.1392822265625 = 0.010268465615808964 + 10.0 * 6.2129011154174805
Epoch 2400, val loss: 1.4662461280822754
Epoch 2410, training loss: 62.11502456665039 = 0.010150748305022717 + 10.0 * 6.210487365722656
Epoch 2410, val loss: 1.468732476234436
Epoch 2420, training loss: 62.11183166503906 = 0.01004418171942234 + 10.0 * 6.210178852081299
Epoch 2420, val loss: 1.4711438417434692
Epoch 2430, training loss: 62.11240768432617 = 0.009936719201505184 + 10.0 * 6.210247039794922
Epoch 2430, val loss: 1.4733338356018066
Epoch 2440, training loss: 62.11638259887695 = 0.009833386167883873 + 10.0 * 6.2106547355651855
Epoch 2440, val loss: 1.4756218194961548
Epoch 2450, training loss: 62.134483337402344 = 0.00972755253314972 + 10.0 * 6.212475776672363
Epoch 2450, val loss: 1.4778310060501099
Epoch 2460, training loss: 62.099456787109375 = 0.009621459990739822 + 10.0 * 6.208983421325684
Epoch 2460, val loss: 1.480064034461975
Epoch 2470, training loss: 62.09918975830078 = 0.009520916268229485 + 10.0 * 6.2089667320251465
Epoch 2470, val loss: 1.482474684715271
Epoch 2480, training loss: 62.10325241088867 = 0.009424211457371712 + 10.0 * 6.2093825340271
Epoch 2480, val loss: 1.4846669435501099
Epoch 2490, training loss: 62.105220794677734 = 0.009326964616775513 + 10.0 * 6.20958948135376
Epoch 2490, val loss: 1.4868685007095337
Epoch 2500, training loss: 62.08839797973633 = 0.009231409057974815 + 10.0 * 6.207916736602783
Epoch 2500, val loss: 1.4889016151428223
Epoch 2510, training loss: 62.123992919921875 = 0.009139862842857838 + 10.0 * 6.211485385894775
Epoch 2510, val loss: 1.491148591041565
Epoch 2520, training loss: 62.093570709228516 = 0.009045910090208054 + 10.0 * 6.2084527015686035
Epoch 2520, val loss: 1.4933744668960571
Epoch 2530, training loss: 62.08342361450195 = 0.008953161537647247 + 10.0 * 6.207447052001953
Epoch 2530, val loss: 1.495495319366455
Epoch 2540, training loss: 62.0933837890625 = 0.008864711970090866 + 10.0 * 6.208451747894287
Epoch 2540, val loss: 1.4977107048034668
Epoch 2550, training loss: 62.116390228271484 = 0.008775419555604458 + 10.0 * 6.210761547088623
Epoch 2550, val loss: 1.4997705221176147
Epoch 2560, training loss: 62.09029006958008 = 0.00868682749569416 + 10.0 * 6.208160400390625
Epoch 2560, val loss: 1.50132155418396
Epoch 2570, training loss: 62.080055236816406 = 0.008602353744208813 + 10.0 * 6.2071452140808105
Epoch 2570, val loss: 1.5038331747055054
Epoch 2580, training loss: 62.090431213378906 = 0.008519106544554234 + 10.0 * 6.208191394805908
Epoch 2580, val loss: 1.505942463874817
Epoch 2590, training loss: 62.09358215332031 = 0.00843642745167017 + 10.0 * 6.208514213562012
Epoch 2590, val loss: 1.5077182054519653
Epoch 2600, training loss: 62.09358596801758 = 0.00835790392011404 + 10.0 * 6.208522796630859
Epoch 2600, val loss: 1.51004958152771
Epoch 2610, training loss: 62.081295013427734 = 0.008275133557617664 + 10.0 * 6.207302093505859
Epoch 2610, val loss: 1.5117554664611816
Epoch 2620, training loss: 62.07510757446289 = 0.008195898495614529 + 10.0 * 6.206690788269043
Epoch 2620, val loss: 1.5138550996780396
Epoch 2630, training loss: 62.09831619262695 = 0.008121157065033913 + 10.0 * 6.209019660949707
Epoch 2630, val loss: 1.5159889459609985
Epoch 2640, training loss: 62.07463073730469 = 0.008041190914809704 + 10.0 * 6.206658840179443
Epoch 2640, val loss: 1.5175683498382568
Epoch 2650, training loss: 62.059608459472656 = 0.007964820601046085 + 10.0 * 6.205164432525635
Epoch 2650, val loss: 1.5194673538208008
Epoch 2660, training loss: 62.056888580322266 = 0.007892865687608719 + 10.0 * 6.204899787902832
Epoch 2660, val loss: 1.521670937538147
Epoch 2670, training loss: 62.05520248413086 = 0.007822326384484768 + 10.0 * 6.204737663269043
Epoch 2670, val loss: 1.5236480236053467
Epoch 2680, training loss: 62.09767532348633 = 0.007754066959023476 + 10.0 * 6.208992004394531
Epoch 2680, val loss: 1.5255897045135498
Epoch 2690, training loss: 62.08205032348633 = 0.007680710405111313 + 10.0 * 6.207437038421631
Epoch 2690, val loss: 1.5270473957061768
Epoch 2700, training loss: 62.089500427246094 = 0.007608312182128429 + 10.0 * 6.208189487457275
Epoch 2700, val loss: 1.5289076566696167
Epoch 2710, training loss: 62.06987380981445 = 0.00753604294732213 + 10.0 * 6.206233501434326
Epoch 2710, val loss: 1.5306206941604614
Epoch 2720, training loss: 62.053070068359375 = 0.007467578165233135 + 10.0 * 6.204560279846191
Epoch 2720, val loss: 1.532643437385559
Epoch 2730, training loss: 62.06324005126953 = 0.007404138799756765 + 10.0 * 6.205583572387695
Epoch 2730, val loss: 1.5346227884292603
Epoch 2740, training loss: 62.08742141723633 = 0.00733920419588685 + 10.0 * 6.208008289337158
Epoch 2740, val loss: 1.53641676902771
Epoch 2750, training loss: 62.07029724121094 = 0.007274253759533167 + 10.0 * 6.206302165985107
Epoch 2750, val loss: 1.5378057956695557
Epoch 2760, training loss: 62.04641342163086 = 0.007208474911749363 + 10.0 * 6.203920364379883
Epoch 2760, val loss: 1.539787769317627
Epoch 2770, training loss: 62.049800872802734 = 0.007148124743252993 + 10.0 * 6.204265117645264
Epoch 2770, val loss: 1.5415197610855103
Epoch 2780, training loss: 62.07181167602539 = 0.007086559664458036 + 10.0 * 6.206472396850586
Epoch 2780, val loss: 1.5430691242218018
Epoch 2790, training loss: 62.05565643310547 = 0.007026880979537964 + 10.0 * 6.20486307144165
Epoch 2790, val loss: 1.545083999633789
Epoch 2800, training loss: 62.06549072265625 = 0.006966247223317623 + 10.0 * 6.205852508544922
Epoch 2800, val loss: 1.5466145277023315
Epoch 2810, training loss: 62.07706832885742 = 0.006904914043843746 + 10.0 * 6.207016468048096
Epoch 2810, val loss: 1.5483980178833008
Epoch 2820, training loss: 62.03502655029297 = 0.006844848394393921 + 10.0 * 6.202818393707275
Epoch 2820, val loss: 1.5496031045913696
Epoch 2830, training loss: 62.0321044921875 = 0.006787719205021858 + 10.0 * 6.202531814575195
Epoch 2830, val loss: 1.5515211820602417
Epoch 2840, training loss: 62.03104782104492 = 0.006732077803462744 + 10.0 * 6.202431678771973
Epoch 2840, val loss: 1.5533243417739868
Epoch 2850, training loss: 62.032779693603516 = 0.006679175887256861 + 10.0 * 6.202610015869141
Epoch 2850, val loss: 1.554943323135376
Epoch 2860, training loss: 62.07025909423828 = 0.0066268895752727985 + 10.0 * 6.206363201141357
Epoch 2860, val loss: 1.55668044090271
Epoch 2870, training loss: 62.068660736083984 = 0.006570835597813129 + 10.0 * 6.2062087059021
Epoch 2870, val loss: 1.5579237937927246
Epoch 2880, training loss: 62.05732345581055 = 0.006512576714158058 + 10.0 * 6.205080986022949
Epoch 2880, val loss: 1.5591353178024292
Epoch 2890, training loss: 62.04267883300781 = 0.0064554824493825436 + 10.0 * 6.203622341156006
Epoch 2890, val loss: 1.5608190298080444
Epoch 2900, training loss: 62.02729797363281 = 0.006405623164027929 + 10.0 * 6.202089309692383
Epoch 2900, val loss: 1.5623652935028076
Epoch 2910, training loss: 62.027381896972656 = 0.006357201840728521 + 10.0 * 6.202102184295654
Epoch 2910, val loss: 1.5642365217208862
Epoch 2920, training loss: 62.0700569152832 = 0.006309268530458212 + 10.0 * 6.206374645233154
Epoch 2920, val loss: 1.5658937692642212
Epoch 2930, training loss: 62.049415588378906 = 0.006255536340177059 + 10.0 * 6.204316139221191
Epoch 2930, val loss: 1.5668995380401611
Epoch 2940, training loss: 62.0247688293457 = 0.006204939913004637 + 10.0 * 6.2018561363220215
Epoch 2940, val loss: 1.5684596300125122
Epoch 2950, training loss: 62.0167236328125 = 0.0061569055542349815 + 10.0 * 6.201056480407715
Epoch 2950, val loss: 1.5699288845062256
Epoch 2960, training loss: 62.01490783691406 = 0.006109714042395353 + 10.0 * 6.2008795738220215
Epoch 2960, val loss: 1.571418046951294
Epoch 2970, training loss: 62.076454162597656 = 0.006066648289561272 + 10.0 * 6.207038879394531
Epoch 2970, val loss: 1.5729894638061523
Epoch 2980, training loss: 62.02524948120117 = 0.006015334278345108 + 10.0 * 6.201923370361328
Epoch 2980, val loss: 1.573873519897461
Epoch 2990, training loss: 62.01723861694336 = 0.005968241486698389 + 10.0 * 6.201127052307129
Epoch 2990, val loss: 1.5753687620162964
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8023194517659463
=== training gcn model ===
Epoch 0, training loss: 87.89696502685547 = 1.9285802841186523 + 10.0 * 8.596837997436523
Epoch 0, val loss: 1.924027919769287
Epoch 10, training loss: 87.8811264038086 = 1.9195563793182373 + 10.0 * 8.59615707397461
Epoch 10, val loss: 1.9147180318832397
Epoch 20, training loss: 87.81913757324219 = 1.9083595275878906 + 10.0 * 8.59107780456543
Epoch 20, val loss: 1.903075933456421
Epoch 30, training loss: 87.46393585205078 = 1.8940373659133911 + 10.0 * 8.556989669799805
Epoch 30, val loss: 1.8882732391357422
Epoch 40, training loss: 85.52059173583984 = 1.8769084215164185 + 10.0 * 8.364368438720703
Epoch 40, val loss: 1.8713632822036743
Epoch 50, training loss: 80.02018737792969 = 1.858262538909912 + 10.0 * 7.816192626953125
Epoch 50, val loss: 1.853615641593933
Epoch 60, training loss: 76.14246368408203 = 1.8441548347473145 + 10.0 * 7.429831027984619
Epoch 60, val loss: 1.8411239385604858
Epoch 70, training loss: 73.4801254272461 = 1.8355869054794312 + 10.0 * 7.164453983306885
Epoch 70, val loss: 1.8333739042282104
Epoch 80, training loss: 72.02483367919922 = 1.825778841972351 + 10.0 * 7.019906044006348
Epoch 80, val loss: 1.8238887786865234
Epoch 90, training loss: 70.60784149169922 = 1.8166896104812622 + 10.0 * 6.879115104675293
Epoch 90, val loss: 1.815474510192871
Epoch 100, training loss: 69.73554992675781 = 1.808197021484375 + 10.0 * 6.792735576629639
Epoch 100, val loss: 1.8073621988296509
Epoch 110, training loss: 68.99633026123047 = 1.7999111413955688 + 10.0 * 6.719642162322998
Epoch 110, val loss: 1.7995483875274658
Epoch 120, training loss: 68.44835662841797 = 1.7915475368499756 + 10.0 * 6.665680885314941
Epoch 120, val loss: 1.7918227910995483
Epoch 130, training loss: 68.04273223876953 = 1.7825512886047363 + 10.0 * 6.6260175704956055
Epoch 130, val loss: 1.7834805250167847
Epoch 140, training loss: 67.75070190429688 = 1.7727251052856445 + 10.0 * 6.597797393798828
Epoch 140, val loss: 1.7745387554168701
Epoch 150, training loss: 67.53194427490234 = 1.7619190216064453 + 10.0 * 6.57700252532959
Epoch 150, val loss: 1.7648991346359253
Epoch 160, training loss: 67.33721923828125 = 1.750019907951355 + 10.0 * 6.558719635009766
Epoch 160, val loss: 1.7544434070587158
Epoch 170, training loss: 67.15589904785156 = 1.7369190454483032 + 10.0 * 6.541898250579834
Epoch 170, val loss: 1.7428994178771973
Epoch 180, training loss: 67.02039337158203 = 1.7225289344787598 + 10.0 * 6.529787063598633
Epoch 180, val loss: 1.730226755142212
Epoch 190, training loss: 66.8337631225586 = 1.706581473350525 + 10.0 * 6.512718200683594
Epoch 190, val loss: 1.716429591178894
Epoch 200, training loss: 66.66234588623047 = 1.6891316175460815 + 10.0 * 6.497321128845215
Epoch 200, val loss: 1.701430082321167
Epoch 210, training loss: 66.51004791259766 = 1.670023798942566 + 10.0 * 6.484002590179443
Epoch 210, val loss: 1.6851269006729126
Epoch 220, training loss: 66.41584777832031 = 1.6491374969482422 + 10.0 * 6.476670742034912
Epoch 220, val loss: 1.667140245437622
Epoch 230, training loss: 66.23477935791016 = 1.6263294219970703 + 10.0 * 6.460844993591309
Epoch 230, val loss: 1.6478101015090942
Epoch 240, training loss: 66.103271484375 = 1.6017836332321167 + 10.0 * 6.450149059295654
Epoch 240, val loss: 1.627103567123413
Epoch 250, training loss: 65.98841857910156 = 1.5751967430114746 + 10.0 * 6.441322326660156
Epoch 250, val loss: 1.604794979095459
Epoch 260, training loss: 65.85253143310547 = 1.5469924211502075 + 10.0 * 6.430554389953613
Epoch 260, val loss: 1.5811004638671875
Epoch 270, training loss: 65.74493408203125 = 1.5171245336532593 + 10.0 * 6.422780513763428
Epoch 270, val loss: 1.5562773942947388
Epoch 280, training loss: 65.63973236083984 = 1.4857672452926636 + 10.0 * 6.415396690368652
Epoch 280, val loss: 1.5303690433502197
Epoch 290, training loss: 65.53728485107422 = 1.4530538320541382 + 10.0 * 6.408422470092773
Epoch 290, val loss: 1.5036131143569946
Epoch 300, training loss: 65.43047332763672 = 1.4194551706314087 + 10.0 * 6.401101589202881
Epoch 300, val loss: 1.4762074947357178
Epoch 310, training loss: 65.33796691894531 = 1.3850107192993164 + 10.0 * 6.3952956199646
Epoch 310, val loss: 1.4484899044036865
Epoch 320, training loss: 65.24884033203125 = 1.349959373474121 + 10.0 * 6.389887809753418
Epoch 320, val loss: 1.4204157590866089
Epoch 330, training loss: 65.1543960571289 = 1.314761757850647 + 10.0 * 6.383963584899902
Epoch 330, val loss: 1.3926081657409668
Epoch 340, training loss: 65.06941223144531 = 1.279517650604248 + 10.0 * 6.3789896965026855
Epoch 340, val loss: 1.3649722337722778
Epoch 350, training loss: 64.98368835449219 = 1.244307041168213 + 10.0 * 6.373938083648682
Epoch 350, val loss: 1.3375699520111084
Epoch 360, training loss: 64.90877532958984 = 1.2093706130981445 + 10.0 * 6.369940280914307
Epoch 360, val loss: 1.3108482360839844
Epoch 370, training loss: 64.85772705078125 = 1.174860954284668 + 10.0 * 6.368286609649658
Epoch 370, val loss: 1.2847093343734741
Epoch 380, training loss: 64.77944946289062 = 1.1407647132873535 + 10.0 * 6.363868236541748
Epoch 380, val loss: 1.2592616081237793
Epoch 390, training loss: 64.6869125366211 = 1.107627511024475 + 10.0 * 6.357928276062012
Epoch 390, val loss: 1.234728455543518
Epoch 400, training loss: 64.61292266845703 = 1.0752294063568115 + 10.0 * 6.353769302368164
Epoch 400, val loss: 1.2113033533096313
Epoch 410, training loss: 64.57722473144531 = 1.0436921119689941 + 10.0 * 6.353353023529053
Epoch 410, val loss: 1.18894362449646
Epoch 420, training loss: 64.51806640625 = 1.0131369829177856 + 10.0 * 6.35049295425415
Epoch 420, val loss: 1.1677619218826294
Epoch 430, training loss: 64.43262481689453 = 0.9838825464248657 + 10.0 * 6.344874382019043
Epoch 430, val loss: 1.1475993394851685
Epoch 440, training loss: 64.35401916503906 = 0.9555783867835999 + 10.0 * 6.339844226837158
Epoch 440, val loss: 1.128893494606018
Epoch 450, training loss: 64.31519317626953 = 0.9284459352493286 + 10.0 * 6.338674545288086
Epoch 450, val loss: 1.1115046739578247
Epoch 460, training loss: 64.27577209472656 = 0.9025333523750305 + 10.0 * 6.337324142456055
Epoch 460, val loss: 1.0949816703796387
Epoch 470, training loss: 64.19864654541016 = 0.8775262236595154 + 10.0 * 6.332111835479736
Epoch 470, val loss: 1.0799840688705444
Epoch 480, training loss: 64.16112518310547 = 0.8537077903747559 + 10.0 * 6.3307414054870605
Epoch 480, val loss: 1.066071629524231
Epoch 490, training loss: 64.09386444091797 = 0.8309317827224731 + 10.0 * 6.326292991638184
Epoch 490, val loss: 1.0532149076461792
Epoch 500, training loss: 64.05210876464844 = 0.8090593814849854 + 10.0 * 6.324304580688477
Epoch 500, val loss: 1.041364073753357
Epoch 510, training loss: 64.02223205566406 = 0.7881014943122864 + 10.0 * 6.323412895202637
Epoch 510, val loss: 1.0304014682769775
Epoch 520, training loss: 63.97633743286133 = 0.7677798867225647 + 10.0 * 6.320855617523193
Epoch 520, val loss: 1.0205461978912354
Epoch 530, training loss: 63.931854248046875 = 0.7482755780220032 + 10.0 * 6.318357944488525
Epoch 530, val loss: 1.0111969709396362
Epoch 540, training loss: 63.882667541503906 = 0.7293246388435364 + 10.0 * 6.315334320068359
Epoch 540, val loss: 1.002822756767273
Epoch 550, training loss: 63.87575149536133 = 0.7110100388526917 + 10.0 * 6.316473960876465
Epoch 550, val loss: 0.9951645731925964
Epoch 560, training loss: 63.8099365234375 = 0.6931624412536621 + 10.0 * 6.3116774559021
Epoch 560, val loss: 0.9879395365715027
Epoch 570, training loss: 63.7717170715332 = 0.6758054494857788 + 10.0 * 6.309591293334961
Epoch 570, val loss: 0.981353223323822
Epoch 580, training loss: 63.78813934326172 = 0.6588490605354309 + 10.0 * 6.312929153442383
Epoch 580, val loss: 0.9754292964935303
Epoch 590, training loss: 63.72935485839844 = 0.6420404314994812 + 10.0 * 6.308731555938721
Epoch 590, val loss: 0.970055341720581
Epoch 600, training loss: 63.677589416503906 = 0.6256982684135437 + 10.0 * 6.30518913269043
Epoch 600, val loss: 0.9649432301521301
Epoch 610, training loss: 63.63517761230469 = 0.6095337271690369 + 10.0 * 6.3025641441345215
Epoch 610, val loss: 0.9604615569114685
Epoch 620, training loss: 63.6141471862793 = 0.5936126708984375 + 10.0 * 6.302053451538086
Epoch 620, val loss: 0.9564417004585266
Epoch 630, training loss: 63.59604263305664 = 0.5778476595878601 + 10.0 * 6.301819801330566
Epoch 630, val loss: 0.952771008014679
Epoch 640, training loss: 63.54744338989258 = 0.5622949004173279 + 10.0 * 6.2985148429870605
Epoch 640, val loss: 0.94902503490448
Epoch 650, training loss: 63.557708740234375 = 0.5469146966934204 + 10.0 * 6.301079750061035
Epoch 650, val loss: 0.9459174275398254
Epoch 660, training loss: 63.4892578125 = 0.5317528247833252 + 10.0 * 6.295750617980957
Epoch 660, val loss: 0.9430963397026062
Epoch 670, training loss: 63.456459045410156 = 0.5168204307556152 + 10.0 * 6.29396390914917
Epoch 670, val loss: 0.9405064582824707
Epoch 680, training loss: 63.4326286315918 = 0.5020642280578613 + 10.0 * 6.293056488037109
Epoch 680, val loss: 0.9383102655410767
Epoch 690, training loss: 63.44166564941406 = 0.4874315559864044 + 10.0 * 6.29542350769043
Epoch 690, val loss: 0.9364280700683594
Epoch 700, training loss: 63.38125228881836 = 0.4729710817337036 + 10.0 * 6.290827751159668
Epoch 700, val loss: 0.9347158074378967
Epoch 710, training loss: 63.35059356689453 = 0.4587430953979492 + 10.0 * 6.289185047149658
Epoch 710, val loss: 0.9335522651672363
Epoch 720, training loss: 63.37156295776367 = 0.44476044178009033 + 10.0 * 6.292680263519287
Epoch 720, val loss: 0.93269282579422
Epoch 730, training loss: 63.315860748291016 = 0.43110308051109314 + 10.0 * 6.28847599029541
Epoch 730, val loss: 0.9316469430923462
Epoch 740, training loss: 63.28483581542969 = 0.41752704977989197 + 10.0 * 6.286730766296387
Epoch 740, val loss: 0.9313503503799438
Epoch 750, training loss: 63.27017593383789 = 0.40433135628700256 + 10.0 * 6.286584377288818
Epoch 750, val loss: 0.9311189651489258
Epoch 760, training loss: 63.23896789550781 = 0.39137136936187744 + 10.0 * 6.284759521484375
Epoch 760, val loss: 0.9312117695808411
Epoch 770, training loss: 63.21220397949219 = 0.3786061406135559 + 10.0 * 6.283360004425049
Epoch 770, val loss: 0.9318704009056091
Epoch 780, training loss: 63.188072204589844 = 0.36628374457359314 + 10.0 * 6.28217887878418
Epoch 780, val loss: 0.9324337244033813
Epoch 790, training loss: 63.15460968017578 = 0.3542078733444214 + 10.0 * 6.280040264129639
Epoch 790, val loss: 0.933701753616333
Epoch 800, training loss: 63.148765563964844 = 0.34252113103866577 + 10.0 * 6.2806243896484375
Epoch 800, val loss: 0.935198962688446
Epoch 810, training loss: 63.118370056152344 = 0.3310448229312897 + 10.0 * 6.278732776641846
Epoch 810, val loss: 0.9367504715919495
Epoch 820, training loss: 63.10113525390625 = 0.3199244439601898 + 10.0 * 6.278120994567871
Epoch 820, val loss: 0.9387437701225281
Epoch 830, training loss: 63.092166900634766 = 0.3091246783733368 + 10.0 * 6.278304100036621
Epoch 830, val loss: 0.9410806894302368
Epoch 840, training loss: 63.05209732055664 = 0.298650860786438 + 10.0 * 6.275344371795654
Epoch 840, val loss: 0.9436099529266357
Epoch 850, training loss: 63.04266357421875 = 0.2884868383407593 + 10.0 * 6.275417804718018
Epoch 850, val loss: 0.9463909268379211
Epoch 860, training loss: 63.019527435302734 = 0.2786352038383484 + 10.0 * 6.2740888595581055
Epoch 860, val loss: 0.9492714405059814
Epoch 870, training loss: 63.033504486083984 = 0.2691428065299988 + 10.0 * 6.2764363288879395
Epoch 870, val loss: 0.9520721435546875
Epoch 880, training loss: 63.00849533081055 = 0.25982245802879333 + 10.0 * 6.274867057800293
Epoch 880, val loss: 0.9558457136154175
Epoch 890, training loss: 62.96415328979492 = 0.25092723965644836 + 10.0 * 6.271322727203369
Epoch 890, val loss: 0.9590137600898743
Epoch 900, training loss: 62.94021224975586 = 0.24232220649719238 + 10.0 * 6.269789218902588
Epoch 900, val loss: 0.9628135561943054
Epoch 910, training loss: 62.94484329223633 = 0.23403914272785187 + 10.0 * 6.271080493927002
Epoch 910, val loss: 0.9668323397636414
Epoch 920, training loss: 62.91088104248047 = 0.22601214051246643 + 10.0 * 6.268486976623535
Epoch 920, val loss: 0.9708343744277954
Epoch 930, training loss: 62.90073013305664 = 0.21827727556228638 + 10.0 * 6.268245220184326
Epoch 930, val loss: 0.9752377867698669
Epoch 940, training loss: 62.88234329223633 = 0.21083524823188782 + 10.0 * 6.26715087890625
Epoch 940, val loss: 0.9795471429824829
Epoch 950, training loss: 62.895442962646484 = 0.2036798745393753 + 10.0 * 6.269176483154297
Epoch 950, val loss: 0.9839268326759338
Epoch 960, training loss: 62.896263122558594 = 0.19669702649116516 + 10.0 * 6.269956588745117
Epoch 960, val loss: 0.9887760877609253
Epoch 970, training loss: 62.8549919128418 = 0.19003067910671234 + 10.0 * 6.266496181488037
Epoch 970, val loss: 0.9936820864677429
Epoch 980, training loss: 62.816585540771484 = 0.18363747000694275 + 10.0 * 6.2632951736450195
Epoch 980, val loss: 0.9986110925674438
Epoch 990, training loss: 62.81460952758789 = 0.17753301560878754 + 10.0 * 6.263707637786865
Epoch 990, val loss: 1.0036089420318604
Epoch 1000, training loss: 62.82695388793945 = 0.17162561416625977 + 10.0 * 6.265532493591309
Epoch 1000, val loss: 1.008758783340454
Epoch 1010, training loss: 62.78947067260742 = 0.16596654057502747 + 10.0 * 6.262350559234619
Epoch 1010, val loss: 1.0140808820724487
Epoch 1020, training loss: 62.77368927001953 = 0.16052782535552979 + 10.0 * 6.261316299438477
Epoch 1020, val loss: 1.0193792581558228
Epoch 1030, training loss: 62.83888244628906 = 0.15528512001037598 + 10.0 * 6.268359661102295
Epoch 1030, val loss: 1.0248875617980957
Epoch 1040, training loss: 62.75598907470703 = 0.15022596716880798 + 10.0 * 6.260576248168945
Epoch 1040, val loss: 1.0302445888519287
Epoch 1050, training loss: 62.72907638549805 = 0.14540031552314758 + 10.0 * 6.258367538452148
Epoch 1050, val loss: 1.0358854532241821
Epoch 1060, training loss: 62.72134780883789 = 0.14078356325626373 + 10.0 * 6.258056163787842
Epoch 1060, val loss: 1.0415351390838623
Epoch 1070, training loss: 62.759674072265625 = 0.13634419441223145 + 10.0 * 6.262332916259766
Epoch 1070, val loss: 1.047141671180725
Epoch 1080, training loss: 62.717811584472656 = 0.1320134997367859 + 10.0 * 6.258579730987549
Epoch 1080, val loss: 1.0527479648590088
Epoch 1090, training loss: 62.73530960083008 = 0.1278955340385437 + 10.0 * 6.260741233825684
Epoch 1090, val loss: 1.0585658550262451
Epoch 1100, training loss: 62.68912887573242 = 0.12387827038764954 + 10.0 * 6.256525039672852
Epoch 1100, val loss: 1.064271092414856
Epoch 1110, training loss: 62.669681549072266 = 0.12009219825267792 + 10.0 * 6.2549591064453125
Epoch 1110, val loss: 1.0701706409454346
Epoch 1120, training loss: 62.65420150756836 = 0.11644107103347778 + 10.0 * 6.2537760734558105
Epoch 1120, val loss: 1.075984239578247
Epoch 1130, training loss: 62.65602111816406 = 0.11293385177850723 + 10.0 * 6.254308700561523
Epoch 1130, val loss: 1.0820037126541138
Epoch 1140, training loss: 62.65058135986328 = 0.1095004603266716 + 10.0 * 6.25410795211792
Epoch 1140, val loss: 1.0876281261444092
Epoch 1150, training loss: 62.66050338745117 = 0.10621432214975357 + 10.0 * 6.255429267883301
Epoch 1150, val loss: 1.093488335609436
Epoch 1160, training loss: 62.619808197021484 = 0.10307785123586655 + 10.0 * 6.251673221588135
Epoch 1160, val loss: 1.0993175506591797
Epoch 1170, training loss: 62.61451721191406 = 0.10007767379283905 + 10.0 * 6.251443862915039
Epoch 1170, val loss: 1.1050777435302734
Epoch 1180, training loss: 62.63556671142578 = 0.09717670828104019 + 10.0 * 6.253839015960693
Epoch 1180, val loss: 1.1107945442199707
Epoch 1190, training loss: 62.59777069091797 = 0.09435462951660156 + 10.0 * 6.250341892242432
Epoch 1190, val loss: 1.116930603981018
Epoch 1200, training loss: 62.63675308227539 = 0.09164682030677795 + 10.0 * 6.254510402679443
Epoch 1200, val loss: 1.1227777004241943
Epoch 1210, training loss: 62.59101104736328 = 0.08901304751634598 + 10.0 * 6.250199794769287
Epoch 1210, val loss: 1.128458023071289
Epoch 1220, training loss: 62.58408737182617 = 0.0864962637424469 + 10.0 * 6.249759197235107
Epoch 1220, val loss: 1.134451150894165
Epoch 1230, training loss: 62.57978057861328 = 0.08407115191221237 + 10.0 * 6.249570846557617
Epoch 1230, val loss: 1.1401363611221313
Epoch 1240, training loss: 62.555641174316406 = 0.0817374438047409 + 10.0 * 6.247390270233154
Epoch 1240, val loss: 1.1459736824035645
Epoch 1250, training loss: 62.58852005004883 = 0.07949969172477722 + 10.0 * 6.25090217590332
Epoch 1250, val loss: 1.1514613628387451
Epoch 1260, training loss: 62.55106735229492 = 0.07729483395814896 + 10.0 * 6.247377395629883
Epoch 1260, val loss: 1.1575113534927368
Epoch 1270, training loss: 62.533573150634766 = 0.07519121468067169 + 10.0 * 6.245838165283203
Epoch 1270, val loss: 1.163025975227356
Epoch 1280, training loss: 62.54861831665039 = 0.07316166162490845 + 10.0 * 6.2475457191467285
Epoch 1280, val loss: 1.1688247919082642
Epoch 1290, training loss: 62.52723693847656 = 0.07118876278400421 + 10.0 * 6.245604515075684
Epoch 1290, val loss: 1.1744521856307983
Epoch 1300, training loss: 62.530662536621094 = 0.06928756833076477 + 10.0 * 6.246137619018555
Epoch 1300, val loss: 1.1800810098648071
Epoch 1310, training loss: 62.52295684814453 = 0.06745941936969757 + 10.0 * 6.24554967880249
Epoch 1310, val loss: 1.1854259967803955
Epoch 1320, training loss: 62.50010681152344 = 0.06569664180278778 + 10.0 * 6.243441104888916
Epoch 1320, val loss: 1.191217303276062
Epoch 1330, training loss: 62.4975700378418 = 0.0640006884932518 + 10.0 * 6.243357181549072
Epoch 1330, val loss: 1.1966763734817505
Epoch 1340, training loss: 62.525360107421875 = 0.06236114725470543 + 10.0 * 6.246299743652344
Epoch 1340, val loss: 1.2023085355758667
Epoch 1350, training loss: 62.52131271362305 = 0.060753319412469864 + 10.0 * 6.246056079864502
Epoch 1350, val loss: 1.2077192068099976
Epoch 1360, training loss: 62.49062728881836 = 0.05919620394706726 + 10.0 * 6.243143081665039
Epoch 1360, val loss: 1.2132911682128906
Epoch 1370, training loss: 62.4929313659668 = 0.05771210417151451 + 10.0 * 6.243521690368652
Epoch 1370, val loss: 1.218672513961792
Epoch 1380, training loss: 62.478118896484375 = 0.056259430944919586 + 10.0 * 6.242186069488525
Epoch 1380, val loss: 1.2242321968078613
Epoch 1390, training loss: 62.4793701171875 = 0.054859623312950134 + 10.0 * 6.242451190948486
Epoch 1390, val loss: 1.2295976877212524
Epoch 1400, training loss: 62.45573043823242 = 0.05350978672504425 + 10.0 * 6.240221977233887
Epoch 1400, val loss: 1.2349237203598022
Epoch 1410, training loss: 62.472957611083984 = 0.05220823734998703 + 10.0 * 6.242074966430664
Epoch 1410, val loss: 1.2402905225753784
Epoch 1420, training loss: 62.475337982177734 = 0.050928935408592224 + 10.0 * 6.242440700531006
Epoch 1420, val loss: 1.246090292930603
Epoch 1430, training loss: 62.47343826293945 = 0.049703557044267654 + 10.0 * 6.242373466491699
Epoch 1430, val loss: 1.2509337663650513
Epoch 1440, training loss: 62.4503059387207 = 0.04851309955120087 + 10.0 * 6.240179538726807
Epoch 1440, val loss: 1.2563425302505493
Epoch 1450, training loss: 62.42965316772461 = 0.04736959934234619 + 10.0 * 6.2382283210754395
Epoch 1450, val loss: 1.2613558769226074
Epoch 1460, training loss: 62.422000885009766 = 0.046261996030807495 + 10.0 * 6.237574100494385
Epoch 1460, val loss: 1.2666767835617065
Epoch 1470, training loss: 62.49604034423828 = 0.04520418494939804 + 10.0 * 6.245083808898926
Epoch 1470, val loss: 1.2718113660812378
Epoch 1480, training loss: 62.43610382080078 = 0.04412942752242088 + 10.0 * 6.239197731018066
Epoch 1480, val loss: 1.276947259902954
Epoch 1490, training loss: 62.40775680541992 = 0.043124228715896606 + 10.0 * 6.2364630699157715
Epoch 1490, val loss: 1.2821426391601562
Epoch 1500, training loss: 62.41769027709961 = 0.04215019941329956 + 10.0 * 6.23755407333374
Epoch 1500, val loss: 1.2873746156692505
Epoch 1510, training loss: 62.421146392822266 = 0.041201286017894745 + 10.0 * 6.237994194030762
Epoch 1510, val loss: 1.2924211025238037
Epoch 1520, training loss: 62.387516021728516 = 0.040273942053318024 + 10.0 * 6.234724044799805
Epoch 1520, val loss: 1.297396183013916
Epoch 1530, training loss: 62.40952682495117 = 0.039380304515361786 + 10.0 * 6.2370147705078125
Epoch 1530, val loss: 1.3023242950439453
Epoch 1540, training loss: 62.38481521606445 = 0.03851456195116043 + 10.0 * 6.234630107879639
Epoch 1540, val loss: 1.3073608875274658
Epoch 1550, training loss: 62.37417984008789 = 0.037672217935323715 + 10.0 * 6.2336506843566895
Epoch 1550, val loss: 1.3122389316558838
Epoch 1560, training loss: 62.373409271240234 = 0.036862630397081375 + 10.0 * 6.233654975891113
Epoch 1560, val loss: 1.3171128034591675
Epoch 1570, training loss: 62.413665771484375 = 0.036083146929740906 + 10.0 * 6.237758159637451
Epoch 1570, val loss: 1.3218814134597778
Epoch 1580, training loss: 62.37207794189453 = 0.03530663996934891 + 10.0 * 6.233677387237549
Epoch 1580, val loss: 1.3270442485809326
Epoch 1590, training loss: 62.398765563964844 = 0.03456312417984009 + 10.0 * 6.236420154571533
Epoch 1590, val loss: 1.3318003416061401
Epoch 1600, training loss: 62.35475158691406 = 0.033830709755420685 + 10.0 * 6.232091903686523
Epoch 1600, val loss: 1.3367091417312622
Epoch 1610, training loss: 62.34651565551758 = 0.03313079848885536 + 10.0 * 6.2313385009765625
Epoch 1610, val loss: 1.341392993927002
Epoch 1620, training loss: 62.343360900878906 = 0.03245224431157112 + 10.0 * 6.231091022491455
Epoch 1620, val loss: 1.3462560176849365
Epoch 1630, training loss: 62.38943862915039 = 0.031795501708984375 + 10.0 * 6.235764503479004
Epoch 1630, val loss: 1.3511062860488892
Epoch 1640, training loss: 62.35844421386719 = 0.031144369393587112 + 10.0 * 6.232729911804199
Epoch 1640, val loss: 1.3553335666656494
Epoch 1650, training loss: 62.343196868896484 = 0.030512578785419464 + 10.0 * 6.231268405914307
Epoch 1650, val loss: 1.3605635166168213
Epoch 1660, training loss: 62.35322570800781 = 0.029906507581472397 + 10.0 * 6.2323317527771
Epoch 1660, val loss: 1.364890217781067
Epoch 1670, training loss: 62.32867431640625 = 0.029311254620552063 + 10.0 * 6.229936122894287
Epoch 1670, val loss: 1.3696482181549072
Epoch 1680, training loss: 62.36206817626953 = 0.028741231188178062 + 10.0 * 6.233332633972168
Epoch 1680, val loss: 1.373939871788025
Epoch 1690, training loss: 62.32095718383789 = 0.0281723253428936 + 10.0 * 6.229278564453125
Epoch 1690, val loss: 1.3785204887390137
Epoch 1700, training loss: 62.31081771850586 = 0.02762875333428383 + 10.0 * 6.228318691253662
Epoch 1700, val loss: 1.3828529119491577
Epoch 1710, training loss: 62.33325958251953 = 0.027103973552584648 + 10.0 * 6.230615615844727
Epoch 1710, val loss: 1.3872432708740234
Epoch 1720, training loss: 62.32514953613281 = 0.026582136750221252 + 10.0 * 6.229856967926025
Epoch 1720, val loss: 1.391660213470459
Epoch 1730, training loss: 62.31245803833008 = 0.0260748490691185 + 10.0 * 6.228638648986816
Epoch 1730, val loss: 1.3962254524230957
Epoch 1740, training loss: 62.3248176574707 = 0.025585349649190903 + 10.0 * 6.229923248291016
Epoch 1740, val loss: 1.40064537525177
Epoch 1750, training loss: 62.3006591796875 = 0.025113550946116447 + 10.0 * 6.227554798126221
Epoch 1750, val loss: 1.4049185514450073
Epoch 1760, training loss: 62.3207893371582 = 0.024655139073729515 + 10.0 * 6.229613304138184
Epoch 1760, val loss: 1.4094780683517456
Epoch 1770, training loss: 62.316001892089844 = 0.02420124039053917 + 10.0 * 6.229180335998535
Epoch 1770, val loss: 1.4134702682495117
Epoch 1780, training loss: 62.30196762084961 = 0.023763509467244148 + 10.0 * 6.22782039642334
Epoch 1780, val loss: 1.4175868034362793
Epoch 1790, training loss: 62.290889739990234 = 0.02333536371588707 + 10.0 * 6.226755619049072
Epoch 1790, val loss: 1.4220459461212158
Epoch 1800, training loss: 62.28911590576172 = 0.022921301424503326 + 10.0 * 6.226619243621826
Epoch 1800, val loss: 1.4261316061019897
Epoch 1810, training loss: 62.344749450683594 = 0.0225228238850832 + 10.0 * 6.232222557067871
Epoch 1810, val loss: 1.430435061454773
Epoch 1820, training loss: 62.29398727416992 = 0.022113274782896042 + 10.0 * 6.227187156677246
Epoch 1820, val loss: 1.4343959093093872
Epoch 1830, training loss: 62.28657150268555 = 0.02173125557601452 + 10.0 * 6.2264838218688965
Epoch 1830, val loss: 1.4383478164672852
Epoch 1840, training loss: 62.2993278503418 = 0.021356653422117233 + 10.0 * 6.227797031402588
Epoch 1840, val loss: 1.4424172639846802
Epoch 1850, training loss: 62.26511764526367 = 0.02098662406206131 + 10.0 * 6.22441291809082
Epoch 1850, val loss: 1.4466089010238647
Epoch 1860, training loss: 62.27046203613281 = 0.02063494548201561 + 10.0 * 6.224982738494873
Epoch 1860, val loss: 1.4505553245544434
Epoch 1870, training loss: 62.286643981933594 = 0.020287632942199707 + 10.0 * 6.226635932922363
Epoch 1870, val loss: 1.4544044733047485
Epoch 1880, training loss: 62.28300094604492 = 0.019948411732912064 + 10.0 * 6.2263054847717285
Epoch 1880, val loss: 1.4582971334457397
Epoch 1890, training loss: 62.25732421875 = 0.01961374469101429 + 10.0 * 6.223771095275879
Epoch 1890, val loss: 1.462219476699829
Epoch 1900, training loss: 62.27691650390625 = 0.019298091530799866 + 10.0 * 6.225761890411377
Epoch 1900, val loss: 1.4662449359893799
Epoch 1910, training loss: 62.26554870605469 = 0.018977578729391098 + 10.0 * 6.22465705871582
Epoch 1910, val loss: 1.4699589014053345
Epoch 1920, training loss: 62.25767135620117 = 0.018665527924895287 + 10.0 * 6.22390079498291
Epoch 1920, val loss: 1.4737104177474976
Epoch 1930, training loss: 62.24958038330078 = 0.018361905589699745 + 10.0 * 6.2231221199035645
Epoch 1930, val loss: 1.4773753881454468
Epoch 1940, training loss: 62.2364501953125 = 0.018070679157972336 + 10.0 * 6.221837997436523
Epoch 1940, val loss: 1.481201171875
Epoch 1950, training loss: 62.23396682739258 = 0.01778682880103588 + 10.0 * 6.221617698669434
Epoch 1950, val loss: 1.4850103855133057
Epoch 1960, training loss: 62.27801513671875 = 0.0175095833837986 + 10.0 * 6.22605037689209
Epoch 1960, val loss: 1.4887555837631226
Epoch 1970, training loss: 62.24385452270508 = 0.017234954982995987 + 10.0 * 6.222661972045898
Epoch 1970, val loss: 1.4919835329055786
Epoch 1980, training loss: 62.23384475708008 = 0.01696324162185192 + 10.0 * 6.221688270568848
Epoch 1980, val loss: 1.4961367845535278
Epoch 1990, training loss: 62.240779876708984 = 0.0167035311460495 + 10.0 * 6.222407341003418
Epoch 1990, val loss: 1.4995880126953125
Epoch 2000, training loss: 62.23618698120117 = 0.016447586938738823 + 10.0 * 6.221973896026611
Epoch 2000, val loss: 1.5032817125320435
Epoch 2010, training loss: 62.22184371948242 = 0.016199683770537376 + 10.0 * 6.220564365386963
Epoch 2010, val loss: 1.506361484527588
Epoch 2020, training loss: 62.211971282958984 = 0.015957564115524292 + 10.0 * 6.219601631164551
Epoch 2020, val loss: 1.5099419355392456
Epoch 2030, training loss: 62.22830581665039 = 0.01572376862168312 + 10.0 * 6.221258163452148
Epoch 2030, val loss: 1.513387680053711
Epoch 2040, training loss: 62.24535369873047 = 0.015491141937673092 + 10.0 * 6.222986221313477
Epoch 2040, val loss: 1.516911268234253
Epoch 2050, training loss: 62.24858856201172 = 0.015258482657372952 + 10.0 * 6.22333288192749
Epoch 2050, val loss: 1.5204561948776245
Epoch 2060, training loss: 62.209495544433594 = 0.015029591508209705 + 10.0 * 6.219446659088135
Epoch 2060, val loss: 1.5236490964889526
Epoch 2070, training loss: 62.20378494262695 = 0.014816000126302242 + 10.0 * 6.218896865844727
Epoch 2070, val loss: 1.5269757509231567
Epoch 2080, training loss: 62.27000427246094 = 0.014606635086238384 + 10.0 * 6.225539684295654
Epoch 2080, val loss: 1.5304585695266724
Epoch 2090, training loss: 62.231536865234375 = 0.014388580806553364 + 10.0 * 6.221714973449707
Epoch 2090, val loss: 1.5336412191390991
Epoch 2100, training loss: 62.20652770996094 = 0.01418459415435791 + 10.0 * 6.219234466552734
Epoch 2100, val loss: 1.5368901491165161
Epoch 2110, training loss: 62.19561767578125 = 0.013985816389322281 + 10.0 * 6.21816349029541
Epoch 2110, val loss: 1.540123701095581
Epoch 2120, training loss: 62.18904495239258 = 0.01379473228007555 + 10.0 * 6.217525005340576
Epoch 2120, val loss: 1.5434778928756714
Epoch 2130, training loss: 62.243099212646484 = 0.01360838022083044 + 10.0 * 6.222949028015137
Epoch 2130, val loss: 1.546499490737915
Epoch 2140, training loss: 62.22071838378906 = 0.013417338952422142 + 10.0 * 6.220730304718018
Epoch 2140, val loss: 1.5498383045196533
Epoch 2150, training loss: 62.204681396484375 = 0.013228041119873524 + 10.0 * 6.21914529800415
Epoch 2150, val loss: 1.5527215003967285
Epoch 2160, training loss: 62.18736267089844 = 0.013046158477663994 + 10.0 * 6.217431545257568
Epoch 2160, val loss: 1.5560359954833984
Epoch 2170, training loss: 62.17510223388672 = 0.012872830033302307 + 10.0 * 6.216222763061523
Epoch 2170, val loss: 1.5591413974761963
Epoch 2180, training loss: 62.17272186279297 = 0.01270425133407116 + 10.0 * 6.216001987457275
Epoch 2180, val loss: 1.5620805025100708
Epoch 2190, training loss: 62.212608337402344 = 0.01254237163811922 + 10.0 * 6.220006465911865
Epoch 2190, val loss: 1.5650110244750977
Epoch 2200, training loss: 62.18885040283203 = 0.01236924808472395 + 10.0 * 6.217648029327393
Epoch 2200, val loss: 1.5679601430892944
Epoch 2210, training loss: 62.17685317993164 = 0.012203679420053959 + 10.0 * 6.216464996337891
Epoch 2210, val loss: 1.5708168745040894
Epoch 2220, training loss: 62.17680740356445 = 0.012046685442328453 + 10.0 * 6.216475963592529
Epoch 2220, val loss: 1.574047327041626
Epoch 2230, training loss: 62.20882797241211 = 0.01189769059419632 + 10.0 * 6.219693183898926
Epoch 2230, val loss: 1.5769124031066895
Epoch 2240, training loss: 62.17318344116211 = 0.011738941073417664 + 10.0 * 6.216144561767578
Epoch 2240, val loss: 1.5799447298049927
Epoch 2250, training loss: 62.171016693115234 = 0.011587981134653091 + 10.0 * 6.215942859649658
Epoch 2250, val loss: 1.5826016664505005
Epoch 2260, training loss: 62.17548751831055 = 0.01144444290548563 + 10.0 * 6.216404438018799
Epoch 2260, val loss: 1.585544466972351
Epoch 2270, training loss: 62.17716598510742 = 0.011301154270768166 + 10.0 * 6.216586112976074
Epoch 2270, val loss: 1.5884686708450317
Epoch 2280, training loss: 62.156494140625 = 0.011161671951413155 + 10.0 * 6.21453332901001
Epoch 2280, val loss: 1.5909416675567627
Epoch 2290, training loss: 62.168373107910156 = 0.011026869527995586 + 10.0 * 6.215734481811523
Epoch 2290, val loss: 1.5937957763671875
Epoch 2300, training loss: 62.17055892944336 = 0.010891005396842957 + 10.0 * 6.215966701507568
Epoch 2300, val loss: 1.596648931503296
Epoch 2310, training loss: 62.20479202270508 = 0.01076075155287981 + 10.0 * 6.219403266906738
Epoch 2310, val loss: 1.59904146194458
Epoch 2320, training loss: 62.169742584228516 = 0.010624940507113934 + 10.0 * 6.215911865234375
Epoch 2320, val loss: 1.601836085319519
Epoch 2330, training loss: 62.15422058105469 = 0.010496564209461212 + 10.0 * 6.214372158050537
Epoch 2330, val loss: 1.6043086051940918
Epoch 2340, training loss: 62.14424514770508 = 0.010372529737651348 + 10.0 * 6.213387489318848
Epoch 2340, val loss: 1.6072523593902588
Epoch 2350, training loss: 62.16870880126953 = 0.010253195650875568 + 10.0 * 6.215845584869385
Epoch 2350, val loss: 1.6095699071884155
Epoch 2360, training loss: 62.14776611328125 = 0.010130111128091812 + 10.0 * 6.213763236999512
Epoch 2360, val loss: 1.61251699924469
Epoch 2370, training loss: 62.168148040771484 = 0.01001313328742981 + 10.0 * 6.215813636779785
Epoch 2370, val loss: 1.614874005317688
Epoch 2380, training loss: 62.1612663269043 = 0.009898802265524864 + 10.0 * 6.215136528015137
Epoch 2380, val loss: 1.617342472076416
Epoch 2390, training loss: 62.14717483520508 = 0.00978285726159811 + 10.0 * 6.213738918304443
Epoch 2390, val loss: 1.620012879371643
Epoch 2400, training loss: 62.15532684326172 = 0.009670915082097054 + 10.0 * 6.214565753936768
Epoch 2400, val loss: 1.6222010850906372
Epoch 2410, training loss: 62.15439224243164 = 0.009561577811837196 + 10.0 * 6.21448278427124
Epoch 2410, val loss: 1.6246652603149414
Epoch 2420, training loss: 62.14580535888672 = 0.009452265687286854 + 10.0 * 6.213635444641113
Epoch 2420, val loss: 1.627137541770935
Epoch 2430, training loss: 62.15015411376953 = 0.009348428808152676 + 10.0 * 6.214080333709717
Epoch 2430, val loss: 1.6296617984771729
Epoch 2440, training loss: 62.13918685913086 = 0.009243595413863659 + 10.0 * 6.212994575500488
Epoch 2440, val loss: 1.632000207901001
Epoch 2450, training loss: 62.123809814453125 = 0.009139441885054111 + 10.0 * 6.2114667892456055
Epoch 2450, val loss: 1.6344622373580933
Epoch 2460, training loss: 62.12546920776367 = 0.009041333571076393 + 10.0 * 6.211642742156982
Epoch 2460, val loss: 1.6369428634643555
Epoch 2470, training loss: 62.158226013183594 = 0.008946467190980911 + 10.0 * 6.214928150177002
Epoch 2470, val loss: 1.6390877962112427
Epoch 2480, training loss: 62.12260437011719 = 0.008846073411405087 + 10.0 * 6.211375713348389
Epoch 2480, val loss: 1.6412690877914429
Epoch 2490, training loss: 62.12943649291992 = 0.008748889900743961 + 10.0 * 6.212069034576416
Epoch 2490, val loss: 1.6436434984207153
Epoch 2500, training loss: 62.16988754272461 = 0.008656229823827744 + 10.0 * 6.216123104095459
Epoch 2500, val loss: 1.6455612182617188
Epoch 2510, training loss: 62.117244720458984 = 0.008561544120311737 + 10.0 * 6.2108683586120605
Epoch 2510, val loss: 1.6482665538787842
Epoch 2520, training loss: 62.10837936401367 = 0.008471966721117496 + 10.0 * 6.209990501403809
Epoch 2520, val loss: 1.650412678718567
Epoch 2530, training loss: 62.112060546875 = 0.00838492251932621 + 10.0 * 6.210367679595947
Epoch 2530, val loss: 1.6527271270751953
Epoch 2540, training loss: 62.1849250793457 = 0.00830098707228899 + 10.0 * 6.217662334442139
Epoch 2540, val loss: 1.6548876762390137
Epoch 2550, training loss: 62.12345886230469 = 0.008209663443267345 + 10.0 * 6.211524963378906
Epoch 2550, val loss: 1.656982183456421
Epoch 2560, training loss: 62.10287094116211 = 0.008125296793878078 + 10.0 * 6.209474563598633
Epoch 2560, val loss: 1.6589990854263306
Epoch 2570, training loss: 62.099029541015625 = 0.00804419256746769 + 10.0 * 6.2090983390808105
Epoch 2570, val loss: 1.6610679626464844
Epoch 2580, training loss: 62.13142013549805 = 0.00796700082719326 + 10.0 * 6.212345123291016
Epoch 2580, val loss: 1.6628755331039429
Epoch 2590, training loss: 62.10199737548828 = 0.007883627898991108 + 10.0 * 6.209411144256592
Epoch 2590, val loss: 1.6652848720550537
Epoch 2600, training loss: 62.09675216674805 = 0.007803349290043116 + 10.0 * 6.208894729614258
Epoch 2600, val loss: 1.6672810316085815
Epoch 2610, training loss: 62.11896514892578 = 0.007727830205112696 + 10.0 * 6.211123466491699
Epoch 2610, val loss: 1.6691405773162842
Epoch 2620, training loss: 62.1064338684082 = 0.007650110870599747 + 10.0 * 6.209878444671631
Epoch 2620, val loss: 1.671245813369751
Epoch 2630, training loss: 62.10797882080078 = 0.007574528455734253 + 10.0 * 6.210040092468262
Epoch 2630, val loss: 1.6734083890914917
Epoch 2640, training loss: 62.13254165649414 = 0.007504270877689123 + 10.0 * 6.212503910064697
Epoch 2640, val loss: 1.6752914190292358
Epoch 2650, training loss: 62.090213775634766 = 0.0074269818142056465 + 10.0 * 6.208278656005859
Epoch 2650, val loss: 1.677342176437378
Epoch 2660, training loss: 62.1021842956543 = 0.0073572746478021145 + 10.0 * 6.209482669830322
Epoch 2660, val loss: 1.679184079170227
Epoch 2670, training loss: 62.098968505859375 = 0.007288538385182619 + 10.0 * 6.209167957305908
Epoch 2670, val loss: 1.6812363862991333
Epoch 2680, training loss: 62.091552734375 = 0.007219383958727121 + 10.0 * 6.208433151245117
Epoch 2680, val loss: 1.6829254627227783
Epoch 2690, training loss: 62.14502716064453 = 0.007152487523853779 + 10.0 * 6.21378755569458
Epoch 2690, val loss: 1.684504747390747
Epoch 2700, training loss: 62.08563232421875 = 0.007083048578351736 + 10.0 * 6.207854747772217
Epoch 2700, val loss: 1.6868054866790771
Epoch 2710, training loss: 62.083648681640625 = 0.007018107920885086 + 10.0 * 6.207663059234619
Epoch 2710, val loss: 1.6884605884552002
Epoch 2720, training loss: 62.103328704833984 = 0.006954221986234188 + 10.0 * 6.209637641906738
Epoch 2720, val loss: 1.6902257204055786
Epoch 2730, training loss: 62.090003967285156 = 0.006890713237226009 + 10.0 * 6.208311557769775
Epoch 2730, val loss: 1.6922192573547363
Epoch 2740, training loss: 62.08430862426758 = 0.006828267127275467 + 10.0 * 6.207747936248779
Epoch 2740, val loss: 1.6938862800598145
Epoch 2750, training loss: 62.085777282714844 = 0.006766944658011198 + 10.0 * 6.2079010009765625
Epoch 2750, val loss: 1.6959675550460815
Epoch 2760, training loss: 62.12055969238281 = 0.006707277614623308 + 10.0 * 6.211385250091553
Epoch 2760, val loss: 1.6974387168884277
Epoch 2770, training loss: 62.08873748779297 = 0.006645546294748783 + 10.0 * 6.208209037780762
Epoch 2770, val loss: 1.6988164186477661
Epoch 2780, training loss: 62.084312438964844 = 0.006585896480828524 + 10.0 * 6.207772731781006
Epoch 2780, val loss: 1.7008651494979858
Epoch 2790, training loss: 62.06646728515625 = 0.006527960300445557 + 10.0 * 6.205994129180908
Epoch 2790, val loss: 1.702340006828308
Epoch 2800, training loss: 62.077083587646484 = 0.006473419722169638 + 10.0 * 6.207060813903809
Epoch 2800, val loss: 1.703861951828003
Epoch 2810, training loss: 62.075340270996094 = 0.006417213473469019 + 10.0 * 6.206892490386963
Epoch 2810, val loss: 1.705442190170288
Epoch 2820, training loss: 62.08050537109375 = 0.006360645405948162 + 10.0 * 6.207414627075195
Epoch 2820, val loss: 1.7073508501052856
Epoch 2830, training loss: 62.06801986694336 = 0.006306639406830072 + 10.0 * 6.20617151260376
Epoch 2830, val loss: 1.7087451219558716
Epoch 2840, training loss: 62.09084701538086 = 0.006253910716623068 + 10.0 * 6.208459377288818
Epoch 2840, val loss: 1.7105439901351929
Epoch 2850, training loss: 62.076148986816406 = 0.0061989822424948215 + 10.0 * 6.206995010375977
Epoch 2850, val loss: 1.7121626138687134
Epoch 2860, training loss: 62.09177780151367 = 0.006148638669401407 + 10.0 * 6.208562850952148
Epoch 2860, val loss: 1.7132328748703003
Epoch 2870, training loss: 62.119964599609375 = 0.006095001474022865 + 10.0 * 6.211386680603027
Epoch 2870, val loss: 1.715146541595459
Epoch 2880, training loss: 62.06780242919922 = 0.006041104905307293 + 10.0 * 6.206175804138184
Epoch 2880, val loss: 1.716412901878357
Epoch 2890, training loss: 62.05156707763672 = 0.0059922561049461365 + 10.0 * 6.204557418823242
Epoch 2890, val loss: 1.7179179191589355
Epoch 2900, training loss: 62.04618835449219 = 0.005945074371993542 + 10.0 * 6.204024314880371
Epoch 2900, val loss: 1.7194656133651733
Epoch 2910, training loss: 62.05049133300781 = 0.005898545496165752 + 10.0 * 6.204459190368652
Epoch 2910, val loss: 1.7209439277648926
Epoch 2920, training loss: 62.1192512512207 = 0.0058529069647192955 + 10.0 * 6.211339950561523
Epoch 2920, val loss: 1.7220381498336792
Epoch 2930, training loss: 62.07297897338867 = 0.005801637656986713 + 10.0 * 6.206717491149902
Epoch 2930, val loss: 1.723592758178711
Epoch 2940, training loss: 62.069950103759766 = 0.00575492437928915 + 10.0 * 6.206419467926025
Epoch 2940, val loss: 1.725032091140747
Epoch 2950, training loss: 62.052833557128906 = 0.005708107724785805 + 10.0 * 6.204712867736816
Epoch 2950, val loss: 1.7265230417251587
Epoch 2960, training loss: 62.060977935791016 = 0.005663314368575811 + 10.0 * 6.205531120300293
Epoch 2960, val loss: 1.7281489372253418
Epoch 2970, training loss: 62.040992736816406 = 0.005619077477604151 + 10.0 * 6.203537464141846
Epoch 2970, val loss: 1.729153037071228
Epoch 2980, training loss: 62.092552185058594 = 0.005576631519943476 + 10.0 * 6.208697319030762
Epoch 2980, val loss: 1.730495810508728
Epoch 2990, training loss: 62.070045471191406 = 0.005529677961021662 + 10.0 * 6.206451416015625
Epoch 2990, val loss: 1.7318097352981567
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8081180811808119
The final CL Acc:0.71605, 0.01259, The final GNN Acc:0.80566, 0.00245
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13238])
remove edge: torch.Size([2, 7922])
updated graph: torch.Size([2, 10604])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.93128204345703 = 1.9627739191055298 + 10.0 * 8.596850395202637
Epoch 0, val loss: 1.9543489217758179
Epoch 10, training loss: 87.91569519042969 = 1.952301025390625 + 10.0 * 8.596339225769043
Epoch 10, val loss: 1.9443233013153076
Epoch 20, training loss: 87.86318969726562 = 1.939664602279663 + 10.0 * 8.592351913452148
Epoch 20, val loss: 1.9318108558654785
Epoch 30, training loss: 87.5594711303711 = 1.9235997200012207 + 10.0 * 8.563587188720703
Epoch 30, val loss: 1.9154655933380127
Epoch 40, training loss: 86.01212310791016 = 1.9038602113723755 + 10.0 * 8.410825729370117
Epoch 40, val loss: 1.8957104682922363
Epoch 50, training loss: 80.90450286865234 = 1.8812127113342285 + 10.0 * 7.902328968048096
Epoch 50, val loss: 1.8732173442840576
Epoch 60, training loss: 75.91520690917969 = 1.859089732170105 + 10.0 * 7.405611515045166
Epoch 60, val loss: 1.853046178817749
Epoch 70, training loss: 72.94661712646484 = 1.8467308282852173 + 10.0 * 7.109988689422607
Epoch 70, val loss: 1.842252254486084
Epoch 80, training loss: 71.67926788330078 = 1.8330868482589722 + 10.0 * 6.984618663787842
Epoch 80, val loss: 1.8296113014221191
Epoch 90, training loss: 70.68701934814453 = 1.8193895816802979 + 10.0 * 6.886763095855713
Epoch 90, val loss: 1.8177006244659424
Epoch 100, training loss: 69.90019989013672 = 1.8065489530563354 + 10.0 * 6.809365272521973
Epoch 100, val loss: 1.8068491220474243
Epoch 110, training loss: 69.21723937988281 = 1.7962754964828491 + 10.0 * 6.742096424102783
Epoch 110, val loss: 1.7982085943222046
Epoch 120, training loss: 68.72950744628906 = 1.7858927249908447 + 10.0 * 6.694361209869385
Epoch 120, val loss: 1.7889049053192139
Epoch 130, training loss: 68.2959213256836 = 1.7748498916625977 + 10.0 * 6.652107238769531
Epoch 130, val loss: 1.7788299322128296
Epoch 140, training loss: 67.93986511230469 = 1.7637903690338135 + 10.0 * 6.617607116699219
Epoch 140, val loss: 1.768889307975769
Epoch 150, training loss: 67.52647399902344 = 1.7527868747711182 + 10.0 * 6.577369213104248
Epoch 150, val loss: 1.7589762210845947
Epoch 160, training loss: 67.19864654541016 = 1.7412960529327393 + 10.0 * 6.5457353591918945
Epoch 160, val loss: 1.7487672567367554
Epoch 170, training loss: 66.95855712890625 = 1.7286510467529297 + 10.0 * 6.5229902267456055
Epoch 170, val loss: 1.7374863624572754
Epoch 180, training loss: 66.73918151855469 = 1.714552879333496 + 10.0 * 6.502462387084961
Epoch 180, val loss: 1.7251477241516113
Epoch 190, training loss: 66.56792449951172 = 1.6993937492370605 + 10.0 * 6.486853122711182
Epoch 190, val loss: 1.7118505239486694
Epoch 200, training loss: 66.40957641601562 = 1.6829229593276978 + 10.0 * 6.472665309906006
Epoch 200, val loss: 1.6974122524261475
Epoch 210, training loss: 66.28395080566406 = 1.6648104190826416 + 10.0 * 6.4619140625
Epoch 210, val loss: 1.6816505193710327
Epoch 220, training loss: 66.14311218261719 = 1.6452714204788208 + 10.0 * 6.449784278869629
Epoch 220, val loss: 1.6645519733428955
Epoch 230, training loss: 66.01850891113281 = 1.624219298362732 + 10.0 * 6.439428329467773
Epoch 230, val loss: 1.646178126335144
Epoch 240, training loss: 65.92222595214844 = 1.601603627204895 + 10.0 * 6.43206262588501
Epoch 240, val loss: 1.626513957977295
Epoch 250, training loss: 65.79686737060547 = 1.57742178440094 + 10.0 * 6.4219441413879395
Epoch 250, val loss: 1.6056783199310303
Epoch 260, training loss: 65.69340515136719 = 1.5518302917480469 + 10.0 * 6.414156913757324
Epoch 260, val loss: 1.5837262868881226
Epoch 270, training loss: 65.61331939697266 = 1.5249130725860596 + 10.0 * 6.408840656280518
Epoch 270, val loss: 1.5605348348617554
Epoch 280, training loss: 65.49073028564453 = 1.4968081712722778 + 10.0 * 6.399392127990723
Epoch 280, val loss: 1.5367920398712158
Epoch 290, training loss: 65.39533233642578 = 1.467800259590149 + 10.0 * 6.3927531242370605
Epoch 290, val loss: 1.5124342441558838
Epoch 300, training loss: 65.33167266845703 = 1.4381330013275146 + 10.0 * 6.389354228973389
Epoch 300, val loss: 1.4877488613128662
Epoch 310, training loss: 65.21714782714844 = 1.4081261157989502 + 10.0 * 6.380902290344238
Epoch 310, val loss: 1.4629218578338623
Epoch 320, training loss: 65.14085388183594 = 1.3779406547546387 + 10.0 * 6.376291751861572
Epoch 320, val loss: 1.438291311264038
Epoch 330, training loss: 65.08914947509766 = 1.3477011919021606 + 10.0 * 6.374145030975342
Epoch 330, val loss: 1.4139589071273804
Epoch 340, training loss: 64.98206329345703 = 1.3179246187210083 + 10.0 * 6.366413593292236
Epoch 340, val loss: 1.3900233507156372
Epoch 350, training loss: 64.90838623046875 = 1.288443922996521 + 10.0 * 6.36199426651001
Epoch 350, val loss: 1.3668127059936523
Epoch 360, training loss: 64.83642578125 = 1.2594529390335083 + 10.0 * 6.357697486877441
Epoch 360, val loss: 1.3441836833953857
Epoch 370, training loss: 64.81697082519531 = 1.230855107307434 + 10.0 * 6.358611106872559
Epoch 370, val loss: 1.3221673965454102
Epoch 380, training loss: 64.72647857666016 = 1.202965497970581 + 10.0 * 6.352351188659668
Epoch 380, val loss: 1.300533413887024
Epoch 390, training loss: 64.63997650146484 = 1.1754608154296875 + 10.0 * 6.346451759338379
Epoch 390, val loss: 1.2797516584396362
Epoch 400, training loss: 64.58943176269531 = 1.148611068725586 + 10.0 * 6.344081878662109
Epoch 400, val loss: 1.2595704793930054
Epoch 410, training loss: 64.54792785644531 = 1.1222854852676392 + 10.0 * 6.342564105987549
Epoch 410, val loss: 1.2399652004241943
Epoch 420, training loss: 64.50433349609375 = 1.0965843200683594 + 10.0 * 6.340775489807129
Epoch 420, val loss: 1.2210367918014526
Epoch 430, training loss: 64.4325180053711 = 1.0715086460113525 + 10.0 * 6.3361005783081055
Epoch 430, val loss: 1.202721357345581
Epoch 440, training loss: 64.36978912353516 = 1.0471420288085938 + 10.0 * 6.3322649002075195
Epoch 440, val loss: 1.1851507425308228
Epoch 450, training loss: 64.32926177978516 = 1.0233564376831055 + 10.0 * 6.330590724945068
Epoch 450, val loss: 1.1681846380233765
Epoch 460, training loss: 64.27163696289062 = 0.9999512434005737 + 10.0 * 6.3271684646606445
Epoch 460, val loss: 1.1517466306686401
Epoch 470, training loss: 64.23472595214844 = 0.977297842502594 + 10.0 * 6.325742721557617
Epoch 470, val loss: 1.136193871498108
Epoch 480, training loss: 64.17771911621094 = 0.9550924301147461 + 10.0 * 6.322262763977051
Epoch 480, val loss: 1.1212905645370483
Epoch 490, training loss: 64.13502502441406 = 0.9334316253662109 + 10.0 * 6.320158958435059
Epoch 490, val loss: 1.106779932975769
Epoch 500, training loss: 64.13651275634766 = 0.9121962189674377 + 10.0 * 6.322431564331055
Epoch 500, val loss: 1.0928925275802612
Epoch 510, training loss: 64.06116485595703 = 0.8913363218307495 + 10.0 * 6.316983222961426
Epoch 510, val loss: 1.0796077251434326
Epoch 520, training loss: 64.00730895996094 = 0.8710014224052429 + 10.0 * 6.3136305809021
Epoch 520, val loss: 1.0669174194335938
Epoch 530, training loss: 63.96883773803711 = 0.8510692715644836 + 10.0 * 6.311776638031006
Epoch 530, val loss: 1.054780125617981
Epoch 540, training loss: 64.03002166748047 = 0.8313683271408081 + 10.0 * 6.3198652267456055
Epoch 540, val loss: 1.043052315711975
Epoch 550, training loss: 63.90510559082031 = 0.812173068523407 + 10.0 * 6.309293270111084
Epoch 550, val loss: 1.031875729560852
Epoch 560, training loss: 63.857242584228516 = 0.7933211326599121 + 10.0 * 6.306392192840576
Epoch 560, val loss: 1.0212109088897705
Epoch 570, training loss: 63.82437515258789 = 0.7747782468795776 + 10.0 * 6.304959774017334
Epoch 570, val loss: 1.0110485553741455
Epoch 580, training loss: 63.797088623046875 = 0.7564728260040283 + 10.0 * 6.304061412811279
Epoch 580, val loss: 1.0013854503631592
Epoch 590, training loss: 63.7651481628418 = 0.7383673191070557 + 10.0 * 6.302678108215332
Epoch 590, val loss: 0.9919663071632385
Epoch 600, training loss: 63.72710037231445 = 0.7205402851104736 + 10.0 * 6.300656318664551
Epoch 600, val loss: 0.983169674873352
Epoch 610, training loss: 63.70259094238281 = 0.7029860615730286 + 10.0 * 6.299960136413574
Epoch 610, val loss: 0.9747658967971802
Epoch 620, training loss: 63.66083526611328 = 0.6855824589729309 + 10.0 * 6.297525405883789
Epoch 620, val loss: 0.9667328000068665
Epoch 630, training loss: 63.63663864135742 = 0.6684392094612122 + 10.0 * 6.296820163726807
Epoch 630, val loss: 0.9592121243476868
Epoch 640, training loss: 63.5935173034668 = 0.6514841318130493 + 10.0 * 6.294203281402588
Epoch 640, val loss: 0.9520779252052307
Epoch 650, training loss: 63.579193115234375 = 0.6346726417541504 + 10.0 * 6.294451713562012
Epoch 650, val loss: 0.9453784227371216
Epoch 660, training loss: 63.58488464355469 = 0.6180124878883362 + 10.0 * 6.296687126159668
Epoch 660, val loss: 0.9388564229011536
Epoch 670, training loss: 63.51491165161133 = 0.6015707850456238 + 10.0 * 6.29133415222168
Epoch 670, val loss: 0.9329098463058472
Epoch 680, training loss: 63.47541046142578 = 0.585245668888092 + 10.0 * 6.289016246795654
Epoch 680, val loss: 0.9273012280464172
Epoch 690, training loss: 63.44844055175781 = 0.5691444277763367 + 10.0 * 6.287929534912109
Epoch 690, val loss: 0.9221257567405701
Epoch 700, training loss: 63.46711730957031 = 0.5531275272369385 + 10.0 * 6.291399002075195
Epoch 700, val loss: 0.9172569513320923
Epoch 710, training loss: 63.402587890625 = 0.5373644232749939 + 10.0 * 6.286522388458252
Epoch 710, val loss: 0.9129025340080261
Epoch 720, training loss: 63.371070861816406 = 0.5216587781906128 + 10.0 * 6.28494119644165
Epoch 720, val loss: 0.9087949991226196
Epoch 730, training loss: 63.37260055541992 = 0.5062232613563538 + 10.0 * 6.286637783050537
Epoch 730, val loss: 0.905270516872406
Epoch 740, training loss: 63.34259796142578 = 0.4909219443798065 + 10.0 * 6.285167694091797
Epoch 740, val loss: 0.90181565284729
Epoch 750, training loss: 63.29359817504883 = 0.475775808095932 + 10.0 * 6.281782150268555
Epoch 750, val loss: 0.8989067673683167
Epoch 760, training loss: 63.27989196777344 = 0.46092987060546875 + 10.0 * 6.281896114349365
Epoch 760, val loss: 0.8963997960090637
Epoch 770, training loss: 63.24696350097656 = 0.4463253319263458 + 10.0 * 6.280063629150391
Epoch 770, val loss: 0.8943572044372559
Epoch 780, training loss: 63.21445083618164 = 0.4319864809513092 + 10.0 * 6.2782464027404785
Epoch 780, val loss: 0.8925489783287048
Epoch 790, training loss: 63.247989654541016 = 0.41793686151504517 + 10.0 * 6.283005237579346
Epoch 790, val loss: 0.8912945985794067
Epoch 800, training loss: 63.18120574951172 = 0.4040493965148926 + 10.0 * 6.277715682983398
Epoch 800, val loss: 0.8901613354682922
Epoch 810, training loss: 63.136863708496094 = 0.39055490493774414 + 10.0 * 6.274630546569824
Epoch 810, val loss: 0.8895726799964905
Epoch 820, training loss: 63.11595916748047 = 0.3774414658546448 + 10.0 * 6.2738518714904785
Epoch 820, val loss: 0.8893750905990601
Epoch 830, training loss: 63.122535705566406 = 0.3646303117275238 + 10.0 * 6.275790214538574
Epoch 830, val loss: 0.8894048929214478
Epoch 840, training loss: 63.11122512817383 = 0.352061927318573 + 10.0 * 6.27591609954834
Epoch 840, val loss: 0.8902375102043152
Epoch 850, training loss: 63.05973434448242 = 0.33996254205703735 + 10.0 * 6.271977424621582
Epoch 850, val loss: 0.8909416198730469
Epoch 860, training loss: 63.0282096862793 = 0.3282228410243988 + 10.0 * 6.269998550415039
Epoch 860, val loss: 0.8923594355583191
Epoch 870, training loss: 63.00663757324219 = 0.3168639540672302 + 10.0 * 6.268977165222168
Epoch 870, val loss: 0.8939401507377625
Epoch 880, training loss: 63.0982666015625 = 0.30590423941612244 + 10.0 * 6.279236316680908
Epoch 880, val loss: 0.8959476351737976
Epoch 890, training loss: 62.973533630371094 = 0.29517433047294617 + 10.0 * 6.267836093902588
Epoch 890, val loss: 0.8980408310890198
Epoch 900, training loss: 62.96266555786133 = 0.2849082350730896 + 10.0 * 6.267775535583496
Epoch 900, val loss: 0.9003681540489197
Epoch 910, training loss: 62.97120666503906 = 0.27504995465278625 + 10.0 * 6.269615650177002
Epoch 910, val loss: 0.9031246900558472
Epoch 920, training loss: 62.917259216308594 = 0.2654387950897217 + 10.0 * 6.265182018280029
Epoch 920, val loss: 0.9058844447135925
Epoch 930, training loss: 62.904170989990234 = 0.2562178373336792 + 10.0 * 6.264795303344727
Epoch 930, val loss: 0.9092465043067932
Epoch 940, training loss: 62.89733123779297 = 0.2473965585231781 + 10.0 * 6.264993190765381
Epoch 940, val loss: 0.9129061102867126
Epoch 950, training loss: 62.8878288269043 = 0.23881322145462036 + 10.0 * 6.264901638031006
Epoch 950, val loss: 0.9164568185806274
Epoch 960, training loss: 62.85363006591797 = 0.23055167496204376 + 10.0 * 6.262307643890381
Epoch 960, val loss: 0.9204617142677307
Epoch 970, training loss: 62.839088439941406 = 0.22264669835567474 + 10.0 * 6.26164436340332
Epoch 970, val loss: 0.9246412515640259
Epoch 980, training loss: 62.892112731933594 = 0.21503505110740662 + 10.0 * 6.267707824707031
Epoch 980, val loss: 0.9289661049842834
Epoch 990, training loss: 62.82352828979492 = 0.20759747922420502 + 10.0 * 6.261593341827393
Epoch 990, val loss: 0.93304044008255
Epoch 1000, training loss: 62.800193786621094 = 0.20055194199085236 + 10.0 * 6.259964466094971
Epoch 1000, val loss: 0.9378685355186462
Epoch 1010, training loss: 62.784149169921875 = 0.19376540184020996 + 10.0 * 6.25903844833374
Epoch 1010, val loss: 0.9425439834594727
Epoch 1020, training loss: 62.80133056640625 = 0.18724964559078217 + 10.0 * 6.26140832901001
Epoch 1020, val loss: 0.9474664926528931
Epoch 1030, training loss: 62.77743911743164 = 0.18092398345470428 + 10.0 * 6.2596516609191895
Epoch 1030, val loss: 0.9521686434745789
Epoch 1040, training loss: 62.74694061279297 = 0.17488139867782593 + 10.0 * 6.257205963134766
Epoch 1040, val loss: 0.9574588537216187
Epoch 1050, training loss: 62.73079299926758 = 0.16907542943954468 + 10.0 * 6.256171703338623
Epoch 1050, val loss: 0.962834358215332
Epoch 1060, training loss: 62.743064880371094 = 0.16348735988140106 + 10.0 * 6.257957935333252
Epoch 1060, val loss: 0.9682091474533081
Epoch 1070, training loss: 62.712852478027344 = 0.15811370313167572 + 10.0 * 6.255473613739014
Epoch 1070, val loss: 0.9733964800834656
Epoch 1080, training loss: 62.72278594970703 = 0.15293222665786743 + 10.0 * 6.256985664367676
Epoch 1080, val loss: 0.9790095686912537
Epoch 1090, training loss: 62.75700759887695 = 0.14792539179325104 + 10.0 * 6.260908126831055
Epoch 1090, val loss: 0.984369695186615
Epoch 1100, training loss: 62.689735412597656 = 0.14317439496517181 + 10.0 * 6.2546563148498535
Epoch 1100, val loss: 0.9902772307395935
Epoch 1110, training loss: 62.656822204589844 = 0.13855613768100739 + 10.0 * 6.251826286315918
Epoch 1110, val loss: 0.9960477352142334
Epoch 1120, training loss: 62.6441764831543 = 0.1341748982667923 + 10.0 * 6.25100040435791
Epoch 1120, val loss: 1.0018869638442993
Epoch 1130, training loss: 62.633506774902344 = 0.12995773553848267 + 10.0 * 6.250354766845703
Epoch 1130, val loss: 1.0078341960906982
Epoch 1140, training loss: 62.65087127685547 = 0.12590470910072327 + 10.0 * 6.252496719360352
Epoch 1140, val loss: 1.0137250423431396
Epoch 1150, training loss: 62.613277435302734 = 0.12193113565444946 + 10.0 * 6.249134540557861
Epoch 1150, val loss: 1.0197253227233887
Epoch 1160, training loss: 62.606685638427734 = 0.11815089732408524 + 10.0 * 6.2488532066345215
Epoch 1160, val loss: 1.0257086753845215
Epoch 1170, training loss: 62.66664505004883 = 0.11449896544218063 + 10.0 * 6.255214691162109
Epoch 1170, val loss: 1.0313493013381958
Epoch 1180, training loss: 62.62822723388672 = 0.11101731657981873 + 10.0 * 6.251720905303955
Epoch 1180, val loss: 1.0380398035049438
Epoch 1190, training loss: 62.59076690673828 = 0.10763606429100037 + 10.0 * 6.248312950134277
Epoch 1190, val loss: 1.0439965724945068
Epoch 1200, training loss: 62.572357177734375 = 0.10440805554389954 + 10.0 * 6.246794700622559
Epoch 1200, val loss: 1.0501664876937866
Epoch 1210, training loss: 62.56380081176758 = 0.10131626576185226 + 10.0 * 6.246248722076416
Epoch 1210, val loss: 1.0563822984695435
Epoch 1220, training loss: 62.59807586669922 = 0.09832887351512909 + 10.0 * 6.249974727630615
Epoch 1220, val loss: 1.0626055002212524
Epoch 1230, training loss: 62.55826950073242 = 0.09542727470397949 + 10.0 * 6.246284008026123
Epoch 1230, val loss: 1.0686759948730469
Epoch 1240, training loss: 62.55838394165039 = 0.09264351427555084 + 10.0 * 6.2465739250183105
Epoch 1240, val loss: 1.074823260307312
Epoch 1250, training loss: 62.549495697021484 = 0.08996354043483734 + 10.0 * 6.245953559875488
Epoch 1250, val loss: 1.0809084177017212
Epoch 1260, training loss: 62.52754211425781 = 0.08740339428186417 + 10.0 * 6.244013786315918
Epoch 1260, val loss: 1.0875121355056763
Epoch 1270, training loss: 62.530059814453125 = 0.08493689447641373 + 10.0 * 6.24451208114624
Epoch 1270, val loss: 1.0937955379486084
Epoch 1280, training loss: 62.53805160522461 = 0.08255427330732346 + 10.0 * 6.24554967880249
Epoch 1280, val loss: 1.0998865365982056
Epoch 1290, training loss: 62.52178192138672 = 0.08022967725992203 + 10.0 * 6.244154930114746
Epoch 1290, val loss: 1.1061216592788696
Epoch 1300, training loss: 62.52017593383789 = 0.07800576090812683 + 10.0 * 6.2442169189453125
Epoch 1300, val loss: 1.1127276420593262
Epoch 1310, training loss: 62.53665542602539 = 0.0758659690618515 + 10.0 * 6.246078968048096
Epoch 1310, val loss: 1.1188462972640991
Epoch 1320, training loss: 62.489078521728516 = 0.07375620305538177 + 10.0 * 6.241532325744629
Epoch 1320, val loss: 1.124699354171753
Epoch 1330, training loss: 62.47643280029297 = 0.07177603244781494 + 10.0 * 6.2404656410217285
Epoch 1330, val loss: 1.1312202215194702
Epoch 1340, training loss: 62.47121047973633 = 0.06986372172832489 + 10.0 * 6.2401347160339355
Epoch 1340, val loss: 1.13746178150177
Epoch 1350, training loss: 62.48256301879883 = 0.06802744418382645 + 10.0 * 6.241453647613525
Epoch 1350, val loss: 1.1439472436904907
Epoch 1360, training loss: 62.492149353027344 = 0.0662238821387291 + 10.0 * 6.242592811584473
Epoch 1360, val loss: 1.1501342058181763
Epoch 1370, training loss: 62.49586868286133 = 0.06446247547864914 + 10.0 * 6.243140697479248
Epoch 1370, val loss: 1.156154990196228
Epoch 1380, training loss: 62.45465087890625 = 0.06277453154325485 + 10.0 * 6.239187717437744
Epoch 1380, val loss: 1.1621637344360352
Epoch 1390, training loss: 62.44572067260742 = 0.06116150692105293 + 10.0 * 6.238455772399902
Epoch 1390, val loss: 1.168484091758728
Epoch 1400, training loss: 62.462459564208984 = 0.05959808826446533 + 10.0 * 6.240286350250244
Epoch 1400, val loss: 1.1745579242706299
Epoch 1410, training loss: 62.43242263793945 = 0.05809434503316879 + 10.0 * 6.237432956695557
Epoch 1410, val loss: 1.1806728839874268
Epoch 1420, training loss: 62.431007385253906 = 0.05663444474339485 + 10.0 * 6.2374372482299805
Epoch 1420, val loss: 1.186574935913086
Epoch 1430, training loss: 62.47895431518555 = 0.05523082986474037 + 10.0 * 6.242372512817383
Epoch 1430, val loss: 1.1921416521072388
Epoch 1440, training loss: 62.434635162353516 = 0.053861621767282486 + 10.0 * 6.238077163696289
Epoch 1440, val loss: 1.1989705562591553
Epoch 1450, training loss: 62.440650939941406 = 0.05253798887133598 + 10.0 * 6.238811016082764
Epoch 1450, val loss: 1.2045952081680298
Epoch 1460, training loss: 62.41456604003906 = 0.05125489458441734 + 10.0 * 6.236330986022949
Epoch 1460, val loss: 1.2108360528945923
Epoch 1470, training loss: 62.40547561645508 = 0.050014857202768326 + 10.0 * 6.235546112060547
Epoch 1470, val loss: 1.2163513898849487
Epoch 1480, training loss: 62.41365432739258 = 0.04883124679327011 + 10.0 * 6.2364821434021
Epoch 1480, val loss: 1.222417950630188
Epoch 1490, training loss: 62.40776824951172 = 0.047676317393779755 + 10.0 * 6.236009120941162
Epoch 1490, val loss: 1.2282391786575317
Epoch 1500, training loss: 62.41096115112305 = 0.046568065881729126 + 10.0 * 6.236439228057861
Epoch 1500, val loss: 1.2340103387832642
Epoch 1510, training loss: 62.391788482666016 = 0.045480482280254364 + 10.0 * 6.234631061553955
Epoch 1510, val loss: 1.2400152683258057
Epoch 1520, training loss: 62.387325286865234 = 0.04442867636680603 + 10.0 * 6.234289646148682
Epoch 1520, val loss: 1.245336651802063
Epoch 1530, training loss: 62.44509506225586 = 0.04341317340731621 + 10.0 * 6.24016809463501
Epoch 1530, val loss: 1.250982403755188
Epoch 1540, training loss: 62.380374908447266 = 0.04242924973368645 + 10.0 * 6.233794212341309
Epoch 1540, val loss: 1.2569829225540161
Epoch 1550, training loss: 62.37654113769531 = 0.04147396981716156 + 10.0 * 6.233506679534912
Epoch 1550, val loss: 1.2625656127929688
Epoch 1560, training loss: 62.37773895263672 = 0.040553923696279526 + 10.0 * 6.233718395233154
Epoch 1560, val loss: 1.2681481838226318
Epoch 1570, training loss: 62.36795425415039 = 0.03965136408805847 + 10.0 * 6.23283052444458
Epoch 1570, val loss: 1.2734456062316895
Epoch 1580, training loss: 62.375450134277344 = 0.03878984600305557 + 10.0 * 6.233665943145752
Epoch 1580, val loss: 1.279152512550354
Epoch 1590, training loss: 62.35099792480469 = 0.03794825077056885 + 10.0 * 6.231305122375488
Epoch 1590, val loss: 1.2845768928527832
Epoch 1600, training loss: 62.35466003417969 = 0.037137314677238464 + 10.0 * 6.231752395629883
Epoch 1600, val loss: 1.290326714515686
Epoch 1610, training loss: 62.420997619628906 = 0.0363377220928669 + 10.0 * 6.238465785980225
Epoch 1610, val loss: 1.2954766750335693
Epoch 1620, training loss: 62.356876373291016 = 0.035573069006204605 + 10.0 * 6.232130527496338
Epoch 1620, val loss: 1.3008978366851807
Epoch 1630, training loss: 62.337158203125 = 0.03481840342283249 + 10.0 * 6.230234146118164
Epoch 1630, val loss: 1.3064719438552856
Epoch 1640, training loss: 62.332515716552734 = 0.0341038815677166 + 10.0 * 6.229841232299805
Epoch 1640, val loss: 1.3117775917053223
Epoch 1650, training loss: 62.378719329833984 = 0.03341367095708847 + 10.0 * 6.234530448913574
Epoch 1650, val loss: 1.317228078842163
Epoch 1660, training loss: 62.3365364074707 = 0.032717444002628326 + 10.0 * 6.230381965637207
Epoch 1660, val loss: 1.3222088813781738
Epoch 1670, training loss: 62.33625030517578 = 0.03206218406558037 + 10.0 * 6.230418682098389
Epoch 1670, val loss: 1.3277320861816406
Epoch 1680, training loss: 62.34490203857422 = 0.03141944482922554 + 10.0 * 6.231348514556885
Epoch 1680, val loss: 1.3327620029449463
Epoch 1690, training loss: 62.32805633544922 = 0.03079349547624588 + 10.0 * 6.229726314544678
Epoch 1690, val loss: 1.337769627571106
Epoch 1700, training loss: 62.308685302734375 = 0.03018059767782688 + 10.0 * 6.227850437164307
Epoch 1700, val loss: 1.3428469896316528
Epoch 1710, training loss: 62.32615661621094 = 0.029596427455544472 + 10.0 * 6.229655742645264
Epoch 1710, val loss: 1.3479405641555786
Epoch 1720, training loss: 62.31431198120117 = 0.02902481146156788 + 10.0 * 6.2285284996032715
Epoch 1720, val loss: 1.3529644012451172
Epoch 1730, training loss: 62.33790969848633 = 0.028469819575548172 + 10.0 * 6.2309441566467285
Epoch 1730, val loss: 1.3582557439804077
Epoch 1740, training loss: 62.306190490722656 = 0.027916517108678818 + 10.0 * 6.227827548980713
Epoch 1740, val loss: 1.3625128269195557
Epoch 1750, training loss: 62.289886474609375 = 0.027385344728827477 + 10.0 * 6.226250171661377
Epoch 1750, val loss: 1.3677160739898682
Epoch 1760, training loss: 62.291568756103516 = 0.026876209303736687 + 10.0 * 6.22646951675415
Epoch 1760, val loss: 1.372722864151001
Epoch 1770, training loss: 62.30045700073242 = 0.026382407173514366 + 10.0 * 6.227407455444336
Epoch 1770, val loss: 1.377479910850525
Epoch 1780, training loss: 62.30875778198242 = 0.025894660502672195 + 10.0 * 6.228286266326904
Epoch 1780, val loss: 1.381994605064392
Epoch 1790, training loss: 62.28400421142578 = 0.025420499965548515 + 10.0 * 6.225858211517334
Epoch 1790, val loss: 1.3872679471969604
Epoch 1800, training loss: 62.301578521728516 = 0.024959556758403778 + 10.0 * 6.227662086486816
Epoch 1800, val loss: 1.3922146558761597
Epoch 1810, training loss: 62.304466247558594 = 0.024503452703356743 + 10.0 * 6.227996349334717
Epoch 1810, val loss: 1.3965907096862793
Epoch 1820, training loss: 62.27888870239258 = 0.024065999314188957 + 10.0 * 6.225481986999512
Epoch 1820, val loss: 1.401697039604187
Epoch 1830, training loss: 62.266178131103516 = 0.02363997884094715 + 10.0 * 6.2242536544799805
Epoch 1830, val loss: 1.406121850013733
Epoch 1840, training loss: 62.263099670410156 = 0.023230332881212234 + 10.0 * 6.223986625671387
Epoch 1840, val loss: 1.4109554290771484
Epoch 1850, training loss: 62.32843017578125 = 0.022834813222289085 + 10.0 * 6.230559349060059
Epoch 1850, val loss: 1.415765643119812
Epoch 1860, training loss: 62.286956787109375 = 0.022431669756770134 + 10.0 * 6.226452827453613
Epoch 1860, val loss: 1.4201492071151733
Epoch 1870, training loss: 62.27381896972656 = 0.022048940882086754 + 10.0 * 6.225176811218262
Epoch 1870, val loss: 1.4247708320617676
Epoch 1880, training loss: 62.262699127197266 = 0.021672988310456276 + 10.0 * 6.224102973937988
Epoch 1880, val loss: 1.429132103919983
Epoch 1890, training loss: 62.27047348022461 = 0.021309247240424156 + 10.0 * 6.224916458129883
Epoch 1890, val loss: 1.4337496757507324
Epoch 1900, training loss: 62.26801681518555 = 0.020947786048054695 + 10.0 * 6.224707126617432
Epoch 1900, val loss: 1.4376869201660156
Epoch 1910, training loss: 62.24277114868164 = 0.020601259544491768 + 10.0 * 6.222217082977295
Epoch 1910, val loss: 1.4422847032546997
Epoch 1920, training loss: 62.26139831542969 = 0.020267684012651443 + 10.0 * 6.2241129875183105
Epoch 1920, val loss: 1.4464190006256104
Epoch 1930, training loss: 62.264503479003906 = 0.019931945949792862 + 10.0 * 6.224457263946533
Epoch 1930, val loss: 1.4507076740264893
Epoch 1940, training loss: 62.23637771606445 = 0.019612696021795273 + 10.0 * 6.221676826477051
Epoch 1940, val loss: 1.455580472946167
Epoch 1950, training loss: 62.23933792114258 = 0.019298268482089043 + 10.0 * 6.222003936767578
Epoch 1950, val loss: 1.4597151279449463
Epoch 1960, training loss: 62.23007583618164 = 0.018992174416780472 + 10.0 * 6.221108436584473
Epoch 1960, val loss: 1.4636191129684448
Epoch 1970, training loss: 62.28094482421875 = 0.018697606399655342 + 10.0 * 6.226224899291992
Epoch 1970, val loss: 1.4677459001541138
Epoch 1980, training loss: 62.25798797607422 = 0.018403097987174988 + 10.0 * 6.223958492279053
Epoch 1980, val loss: 1.4724328517913818
Epoch 1990, training loss: 62.2597770690918 = 0.01811128295958042 + 10.0 * 6.224166393280029
Epoch 1990, val loss: 1.4763203859329224
Epoch 2000, training loss: 62.227237701416016 = 0.017823796719312668 + 10.0 * 6.220941543579102
Epoch 2000, val loss: 1.4803436994552612
Epoch 2010, training loss: 62.20946502685547 = 0.017555108293890953 + 10.0 * 6.219191074371338
Epoch 2010, val loss: 1.4844064712524414
Epoch 2020, training loss: 62.2092399597168 = 0.017293453216552734 + 10.0 * 6.2191948890686035
Epoch 2020, val loss: 1.4885954856872559
Epoch 2030, training loss: 62.24274826049805 = 0.017039211466908455 + 10.0 * 6.222570896148682
Epoch 2030, val loss: 1.4924302101135254
Epoch 2040, training loss: 62.24025344848633 = 0.016779249534010887 + 10.0 * 6.222347259521484
Epoch 2040, val loss: 1.4963980913162231
Epoch 2050, training loss: 62.204864501953125 = 0.01652761921286583 + 10.0 * 6.2188334465026855
Epoch 2050, val loss: 1.5007203817367554
Epoch 2060, training loss: 62.202693939208984 = 0.016284091398119926 + 10.0 * 6.2186408042907715
Epoch 2060, val loss: 1.5045450925827026
Epoch 2070, training loss: 62.224365234375 = 0.016051311045885086 + 10.0 * 6.220831394195557
Epoch 2070, val loss: 1.5083461999893188
Epoch 2080, training loss: 62.20116424560547 = 0.01581577956676483 + 10.0 * 6.21853494644165
Epoch 2080, val loss: 1.5124874114990234
Epoch 2090, training loss: 62.19983673095703 = 0.015588492155075073 + 10.0 * 6.2184247970581055
Epoch 2090, val loss: 1.5163171291351318
Epoch 2100, training loss: 62.20679473876953 = 0.015370707027614117 + 10.0 * 6.219142436981201
Epoch 2100, val loss: 1.5200546979904175
Epoch 2110, training loss: 62.19281005859375 = 0.015149599872529507 + 10.0 * 6.217766284942627
Epoch 2110, val loss: 1.523952841758728
Epoch 2120, training loss: 62.222442626953125 = 0.01493431068956852 + 10.0 * 6.22075080871582
Epoch 2120, val loss: 1.527172565460205
Epoch 2130, training loss: 62.20768737792969 = 0.014725033193826675 + 10.0 * 6.219296455383301
Epoch 2130, val loss: 1.5309933423995972
Epoch 2140, training loss: 62.190067291259766 = 0.014521142467856407 + 10.0 * 6.217554569244385
Epoch 2140, val loss: 1.5346989631652832
Epoch 2150, training loss: 62.203006744384766 = 0.014322257600724697 + 10.0 * 6.218868732452393
Epoch 2150, val loss: 1.5382827520370483
Epoch 2160, training loss: 62.18337631225586 = 0.014127101749181747 + 10.0 * 6.216925144195557
Epoch 2160, val loss: 1.5419883728027344
Epoch 2170, training loss: 62.17365264892578 = 0.013937252573668957 + 10.0 * 6.21597146987915
Epoch 2170, val loss: 1.54598069190979
Epoch 2180, training loss: 62.18394470214844 = 0.013754798099398613 + 10.0 * 6.217019081115723
Epoch 2180, val loss: 1.5496866703033447
Epoch 2190, training loss: 62.2348747253418 = 0.013575010001659393 + 10.0 * 6.222129821777344
Epoch 2190, val loss: 1.5530493259429932
Epoch 2200, training loss: 62.175209045410156 = 0.013387107290327549 + 10.0 * 6.216182231903076
Epoch 2200, val loss: 1.5562381744384766
Epoch 2210, training loss: 62.16532897949219 = 0.013212259858846664 + 10.0 * 6.215211391448975
Epoch 2210, val loss: 1.5599348545074463
Epoch 2220, training loss: 62.16135025024414 = 0.013044143095612526 + 10.0 * 6.2148308753967285
Epoch 2220, val loss: 1.5632925033569336
Epoch 2230, training loss: 62.23322677612305 = 0.012882526963949203 + 10.0 * 6.222034454345703
Epoch 2230, val loss: 1.5671417713165283
Epoch 2240, training loss: 62.19386291503906 = 0.0127114774659276 + 10.0 * 6.218115329742432
Epoch 2240, val loss: 1.5701898336410522
Epoch 2250, training loss: 62.16306686401367 = 0.012546203099191189 + 10.0 * 6.215052127838135
Epoch 2250, val loss: 1.5735286474227905
Epoch 2260, training loss: 62.15082550048828 = 0.012386973015964031 + 10.0 * 6.213843822479248
Epoch 2260, val loss: 1.5767003297805786
Epoch 2270, training loss: 62.15310287475586 = 0.012235280126333237 + 10.0 * 6.214087009429932
Epoch 2270, val loss: 1.5798863172531128
Epoch 2280, training loss: 62.19215393066406 = 0.012086281552910805 + 10.0 * 6.2180070877075195
Epoch 2280, val loss: 1.5831066370010376
Epoch 2290, training loss: 62.18793869018555 = 0.011936776340007782 + 10.0 * 6.217600345611572
Epoch 2290, val loss: 1.587254285812378
Epoch 2300, training loss: 62.1724739074707 = 0.011784846894443035 + 10.0 * 6.216069221496582
Epoch 2300, val loss: 1.5901739597320557
Epoch 2310, training loss: 62.15122604370117 = 0.01163623295724392 + 10.0 * 6.213959217071533
Epoch 2310, val loss: 1.5932142734527588
Epoch 2320, training loss: 62.13945770263672 = 0.011497090570628643 + 10.0 * 6.212796211242676
Epoch 2320, val loss: 1.5964744091033936
Epoch 2330, training loss: 62.13343811035156 = 0.011362450197339058 + 10.0 * 6.212207317352295
Epoch 2330, val loss: 1.5997754335403442
Epoch 2340, training loss: 62.175418853759766 = 0.011229654774069786 + 10.0 * 6.216418743133545
Epoch 2340, val loss: 1.6029841899871826
Epoch 2350, training loss: 62.161251068115234 = 0.011097322218120098 + 10.0 * 6.215015411376953
Epoch 2350, val loss: 1.6065503358840942
Epoch 2360, training loss: 62.147727966308594 = 0.01096363365650177 + 10.0 * 6.213676452636719
Epoch 2360, val loss: 1.6094154119491577
Epoch 2370, training loss: 62.136104583740234 = 0.010835994966328144 + 10.0 * 6.212526798248291
Epoch 2370, val loss: 1.6123086214065552
Epoch 2380, training loss: 62.127471923828125 = 0.01071123406291008 + 10.0 * 6.211676120758057
Epoch 2380, val loss: 1.6154693365097046
Epoch 2390, training loss: 62.148155212402344 = 0.010593551211059093 + 10.0 * 6.213756084442139
Epoch 2390, val loss: 1.6187410354614258
Epoch 2400, training loss: 62.154754638671875 = 0.010469572618603706 + 10.0 * 6.214428424835205
Epoch 2400, val loss: 1.622064471244812
Epoch 2410, training loss: 62.121795654296875 = 0.010346746072173119 + 10.0 * 6.211144924163818
Epoch 2410, val loss: 1.6246616840362549
Epoch 2420, training loss: 62.120365142822266 = 0.010228794068098068 + 10.0 * 6.2110137939453125
Epoch 2420, val loss: 1.6274760961532593
Epoch 2430, training loss: 62.116790771484375 = 0.010117548517882824 + 10.0 * 6.210667610168457
Epoch 2430, val loss: 1.6305580139160156
Epoch 2440, training loss: 62.17268371582031 = 0.010011388920247555 + 10.0 * 6.2162675857543945
Epoch 2440, val loss: 1.6335972547531128
Epoch 2450, training loss: 62.113311767578125 = 0.009894891642034054 + 10.0 * 6.210341453552246
Epoch 2450, val loss: 1.636505126953125
Epoch 2460, training loss: 62.15001678466797 = 0.00978793390095234 + 10.0 * 6.214022636413574
Epoch 2460, val loss: 1.6388980150222778
Epoch 2470, training loss: 62.10940170288086 = 0.009679324924945831 + 10.0 * 6.209972381591797
Epoch 2470, val loss: 1.642227053642273
Epoch 2480, training loss: 62.11103057861328 = 0.009574861265718937 + 10.0 * 6.210145473480225
Epoch 2480, val loss: 1.645321011543274
Epoch 2490, training loss: 62.11433410644531 = 0.009473809041082859 + 10.0 * 6.210485935211182
Epoch 2490, val loss: 1.6478217840194702
Epoch 2500, training loss: 62.15395736694336 = 0.0093738604336977 + 10.0 * 6.214458465576172
Epoch 2500, val loss: 1.65097177028656
Epoch 2510, training loss: 62.11668395996094 = 0.009274423122406006 + 10.0 * 6.21074104309082
Epoch 2510, val loss: 1.6535063982009888
Epoch 2520, training loss: 62.14607620239258 = 0.009175341576337814 + 10.0 * 6.213690280914307
Epoch 2520, val loss: 1.6559910774230957
Epoch 2530, training loss: 62.09695816040039 = 0.009078511968255043 + 10.0 * 6.20878791809082
Epoch 2530, val loss: 1.659259557723999
Epoch 2540, training loss: 62.10031509399414 = 0.008986447937786579 + 10.0 * 6.209132671356201
Epoch 2540, val loss: 1.662172794342041
Epoch 2550, training loss: 62.10612487792969 = 0.00889530498534441 + 10.0 * 6.209722995758057
Epoch 2550, val loss: 1.6647560596466064
Epoch 2560, training loss: 62.12489700317383 = 0.008804785087704659 + 10.0 * 6.211609363555908
Epoch 2560, val loss: 1.6676392555236816
Epoch 2570, training loss: 62.10361099243164 = 0.008713272400200367 + 10.0 * 6.209489822387695
Epoch 2570, val loss: 1.6700166463851929
Epoch 2580, training loss: 62.09103775024414 = 0.008626008406281471 + 10.0 * 6.2082414627075195
Epoch 2580, val loss: 1.6725858449935913
Epoch 2590, training loss: 62.09927749633789 = 0.008541394956409931 + 10.0 * 6.209073543548584
Epoch 2590, val loss: 1.6751396656036377
Epoch 2600, training loss: 62.12934494018555 = 0.00845611933618784 + 10.0 * 6.212088584899902
Epoch 2600, val loss: 1.6781445741653442
Epoch 2610, training loss: 62.11031723022461 = 0.008374258875846863 + 10.0 * 6.2101945877075195
Epoch 2610, val loss: 1.680916428565979
Epoch 2620, training loss: 62.14699172973633 = 0.008290097117424011 + 10.0 * 6.213870048522949
Epoch 2620, val loss: 1.6833226680755615
Epoch 2630, training loss: 62.08502197265625 = 0.008206028491258621 + 10.0 * 6.207681655883789
Epoch 2630, val loss: 1.685494065284729
Epoch 2640, training loss: 62.080039978027344 = 0.00812484696507454 + 10.0 * 6.207191467285156
Epoch 2640, val loss: 1.6880110502243042
Epoch 2650, training loss: 62.07914733886719 = 0.008050821721553802 + 10.0 * 6.2071099281311035
Epoch 2650, val loss: 1.690855860710144
Epoch 2660, training loss: 62.1060791015625 = 0.00797532219439745 + 10.0 * 6.209810256958008
Epoch 2660, val loss: 1.6933578252792358
Epoch 2670, training loss: 62.09859085083008 = 0.007898998446762562 + 10.0 * 6.20906925201416
Epoch 2670, val loss: 1.6952153444290161
Epoch 2680, training loss: 62.07838821411133 = 0.007824823260307312 + 10.0 * 6.207056522369385
Epoch 2680, val loss: 1.6980745792388916
Epoch 2690, training loss: 62.08601379394531 = 0.007750870659947395 + 10.0 * 6.207826137542725
Epoch 2690, val loss: 1.70051109790802
Epoch 2700, training loss: 62.10444641113281 = 0.007680848706513643 + 10.0 * 6.209676742553711
Epoch 2700, val loss: 1.7027759552001953
Epoch 2710, training loss: 62.08357238769531 = 0.007608314976096153 + 10.0 * 6.207596778869629
Epoch 2710, val loss: 1.7052054405212402
Epoch 2720, training loss: 62.078121185302734 = 0.0075369966216385365 + 10.0 * 6.207058429718018
Epoch 2720, val loss: 1.7075040340423584
Epoch 2730, training loss: 62.07331466674805 = 0.007470134645700455 + 10.0 * 6.206584453582764
Epoch 2730, val loss: 1.710227608680725
Epoch 2740, training loss: 62.092342376708984 = 0.007403693627566099 + 10.0 * 6.208493709564209
Epoch 2740, val loss: 1.7124342918395996
Epoch 2750, training loss: 62.0761604309082 = 0.007336484733968973 + 10.0 * 6.206882476806641
Epoch 2750, val loss: 1.7145261764526367
Epoch 2760, training loss: 62.081459045410156 = 0.007272204849869013 + 10.0 * 6.207418918609619
Epoch 2760, val loss: 1.7169575691223145
Epoch 2770, training loss: 62.06844711303711 = 0.00720577035099268 + 10.0 * 6.206124305725098
Epoch 2770, val loss: 1.7193022966384888
Epoch 2780, training loss: 62.117698669433594 = 0.007142828311771154 + 10.0 * 6.211055755615234
Epoch 2780, val loss: 1.7213865518569946
Epoch 2790, training loss: 62.06218338012695 = 0.007077389396727085 + 10.0 * 6.20551061630249
Epoch 2790, val loss: 1.7234013080596924
Epoch 2800, training loss: 62.056854248046875 = 0.007012919522821903 + 10.0 * 6.204984188079834
Epoch 2800, val loss: 1.7256299257278442
Epoch 2810, training loss: 62.05375289916992 = 0.006955234799534082 + 10.0 * 6.2046799659729
Epoch 2810, val loss: 1.7277308702468872
Epoch 2820, training loss: 62.07475280761719 = 0.00689748814329505 + 10.0 * 6.206785678863525
Epoch 2820, val loss: 1.7300487756729126
Epoch 2830, training loss: 62.075199127197266 = 0.006837320514023304 + 10.0 * 6.206836223602295
Epoch 2830, val loss: 1.7324912548065186
Epoch 2840, training loss: 62.067874908447266 = 0.006780028343200684 + 10.0 * 6.206109523773193
Epoch 2840, val loss: 1.7344183921813965
Epoch 2850, training loss: 62.05753707885742 = 0.006722189951688051 + 10.0 * 6.205081462860107
Epoch 2850, val loss: 1.7364529371261597
Epoch 2860, training loss: 62.09378433227539 = 0.006669528316706419 + 10.0 * 6.208711624145508
Epoch 2860, val loss: 1.738755226135254
Epoch 2870, training loss: 62.04724884033203 = 0.006609954871237278 + 10.0 * 6.204063892364502
Epoch 2870, val loss: 1.7405474185943604
Epoch 2880, training loss: 62.05411911010742 = 0.0065557644702494144 + 10.0 * 6.204756259918213
Epoch 2880, val loss: 1.7425494194030762
Epoch 2890, training loss: 62.055049896240234 = 0.0065041882917284966 + 10.0 * 6.204854488372803
Epoch 2890, val loss: 1.7448893785476685
Epoch 2900, training loss: 62.0663948059082 = 0.006452012341469526 + 10.0 * 6.205994606018066
Epoch 2900, val loss: 1.7465298175811768
Epoch 2910, training loss: 62.07852554321289 = 0.006397824734449387 + 10.0 * 6.207212924957275
Epoch 2910, val loss: 1.7482634782791138
Epoch 2920, training loss: 62.05308532714844 = 0.006344373803585768 + 10.0 * 6.204674243927002
Epoch 2920, val loss: 1.750600814819336
Epoch 2930, training loss: 62.05947494506836 = 0.006293606478720903 + 10.0 * 6.205317974090576
Epoch 2930, val loss: 1.7521382570266724
Epoch 2940, training loss: 62.050048828125 = 0.006243250332772732 + 10.0 * 6.204380512237549
Epoch 2940, val loss: 1.754265546798706
Epoch 2950, training loss: 62.041316986083984 = 0.006194825284183025 + 10.0 * 6.203512191772461
Epoch 2950, val loss: 1.7559864521026611
Epoch 2960, training loss: 62.08567810058594 = 0.006147740408778191 + 10.0 * 6.207952976226807
Epoch 2960, val loss: 1.757697343826294
Epoch 2970, training loss: 62.057411193847656 = 0.006097087636590004 + 10.0 * 6.205131530761719
Epoch 2970, val loss: 1.7603081464767456
Epoch 2980, training loss: 62.04985046386719 = 0.006048941519111395 + 10.0 * 6.204380035400391
Epoch 2980, val loss: 1.761603832244873
Epoch 2990, training loss: 62.046844482421875 = 0.006002563051879406 + 10.0 * 6.2040839195251465
Epoch 2990, val loss: 1.763748049736023
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 87.90966033935547 = 1.9410218000411987 + 10.0 * 8.596863746643066
Epoch 0, val loss: 1.932759404182434
Epoch 10, training loss: 87.89488220214844 = 1.9310959577560425 + 10.0 * 8.596378326416016
Epoch 10, val loss: 1.9224598407745361
Epoch 20, training loss: 87.84370422363281 = 1.9187244176864624 + 10.0 * 8.592497825622559
Epoch 20, val loss: 1.9095877408981323
Epoch 30, training loss: 87.55872344970703 = 1.9024369716644287 + 10.0 * 8.565629005432129
Epoch 30, val loss: 1.892969012260437
Epoch 40, training loss: 86.07306671142578 = 1.8813666105270386 + 10.0 * 8.419170379638672
Epoch 40, val loss: 1.872157335281372
Epoch 50, training loss: 80.64424133300781 = 1.8565233945846558 + 10.0 * 7.878771781921387
Epoch 50, val loss: 1.8482861518859863
Epoch 60, training loss: 76.95503997802734 = 1.833412528038025 + 10.0 * 7.512162685394287
Epoch 60, val loss: 1.8277888298034668
Epoch 70, training loss: 74.4233627319336 = 1.8188269138336182 + 10.0 * 7.260453701019287
Epoch 70, val loss: 1.8148202896118164
Epoch 80, training loss: 73.27012634277344 = 1.8058593273162842 + 10.0 * 7.146426677703857
Epoch 80, val loss: 1.8022375106811523
Epoch 90, training loss: 72.49280548095703 = 1.7897634506225586 + 10.0 * 7.0703043937683105
Epoch 90, val loss: 1.7871932983398438
Epoch 100, training loss: 71.51605224609375 = 1.774794340133667 + 10.0 * 6.974125862121582
Epoch 100, val loss: 1.7741786241531372
Epoch 110, training loss: 70.39845275878906 = 1.7642019987106323 + 10.0 * 6.863425254821777
Epoch 110, val loss: 1.7650671005249023
Epoch 120, training loss: 69.5670394897461 = 1.754960536956787 + 10.0 * 6.781208038330078
Epoch 120, val loss: 1.7562683820724487
Epoch 130, training loss: 68.8937759399414 = 1.7437337636947632 + 10.0 * 6.715003967285156
Epoch 130, val loss: 1.745753526687622
Epoch 140, training loss: 68.40098571777344 = 1.7308317422866821 + 10.0 * 6.667015075683594
Epoch 140, val loss: 1.7340539693832397
Epoch 150, training loss: 68.06394958496094 = 1.7159768342971802 + 10.0 * 6.634797096252441
Epoch 150, val loss: 1.720704197883606
Epoch 160, training loss: 67.77610778808594 = 1.699826717376709 + 10.0 * 6.607627868652344
Epoch 160, val loss: 1.7061851024627686
Epoch 170, training loss: 67.50328063964844 = 1.6827025413513184 + 10.0 * 6.582057952880859
Epoch 170, val loss: 1.6909137964248657
Epoch 180, training loss: 67.267333984375 = 1.6643744707107544 + 10.0 * 6.560295581817627
Epoch 180, val loss: 1.6748030185699463
Epoch 190, training loss: 67.083984375 = 1.6445196866989136 + 10.0 * 6.543946743011475
Epoch 190, val loss: 1.657454252243042
Epoch 200, training loss: 66.85454559326172 = 1.6231355667114258 + 10.0 * 6.5231404304504395
Epoch 200, val loss: 1.6389081478118896
Epoch 210, training loss: 66.6517333984375 = 1.6003576517105103 + 10.0 * 6.5051374435424805
Epoch 210, val loss: 1.6192255020141602
Epoch 220, training loss: 66.47317504882812 = 1.5760853290557861 + 10.0 * 6.489708423614502
Epoch 220, val loss: 1.5983747243881226
Epoch 230, training loss: 66.36415100097656 = 1.5503196716308594 + 10.0 * 6.481383800506592
Epoch 230, val loss: 1.5763866901397705
Epoch 240, training loss: 66.18368530273438 = 1.5231174230575562 + 10.0 * 6.466056823730469
Epoch 240, val loss: 1.5533738136291504
Epoch 250, training loss: 66.05400848388672 = 1.495021104812622 + 10.0 * 6.455898761749268
Epoch 250, val loss: 1.5298008918762207
Epoch 260, training loss: 65.96202087402344 = 1.4661176204681396 + 10.0 * 6.44959020614624
Epoch 260, val loss: 1.5059679746627808
Epoch 270, training loss: 65.82501220703125 = 1.4366012811660767 + 10.0 * 6.438841342926025
Epoch 270, val loss: 1.4817638397216797
Epoch 280, training loss: 65.72099304199219 = 1.4070966243743896 + 10.0 * 6.431389331817627
Epoch 280, val loss: 1.45806884765625
Epoch 290, training loss: 65.62429809570312 = 1.3774338960647583 + 10.0 * 6.424686431884766
Epoch 290, val loss: 1.4346790313720703
Epoch 300, training loss: 65.53111267089844 = 1.3477120399475098 + 10.0 * 6.41834020614624
Epoch 300, val loss: 1.4116696119308472
Epoch 310, training loss: 65.4342269897461 = 1.3181719779968262 + 10.0 * 6.4116058349609375
Epoch 310, val loss: 1.389241337776184
Epoch 320, training loss: 65.3434066772461 = 1.2889436483383179 + 10.0 * 6.405446529388428
Epoch 320, val loss: 1.3674206733703613
Epoch 330, training loss: 65.30007934570312 = 1.2600067853927612 + 10.0 * 6.4040069580078125
Epoch 330, val loss: 1.346171498298645
Epoch 340, training loss: 65.19111633300781 = 1.2311792373657227 + 10.0 * 6.395993709564209
Epoch 340, val loss: 1.325354814529419
Epoch 350, training loss: 65.09626770019531 = 1.2027570009231567 + 10.0 * 6.3893513679504395
Epoch 350, val loss: 1.3051269054412842
Epoch 360, training loss: 65.04008483886719 = 1.1744941473007202 + 10.0 * 6.386559009552002
Epoch 360, val loss: 1.2853233814239502
Epoch 370, training loss: 64.96389770507812 = 1.146543264389038 + 10.0 * 6.381735324859619
Epoch 370, val loss: 1.2660441398620605
Epoch 380, training loss: 64.87289428710938 = 1.1187924146652222 + 10.0 * 6.375410079956055
Epoch 380, val loss: 1.2470653057098389
Epoch 390, training loss: 64.8034896850586 = 1.0912981033325195 + 10.0 * 6.371219158172607
Epoch 390, val loss: 1.2284657955169678
Epoch 400, training loss: 64.7453384399414 = 1.0640945434570312 + 10.0 * 6.368124485015869
Epoch 400, val loss: 1.2101799249649048
Epoch 410, training loss: 64.68441772460938 = 1.03726065158844 + 10.0 * 6.364715576171875
Epoch 410, val loss: 1.1924428939819336
Epoch 420, training loss: 64.60832977294922 = 1.0108031034469604 + 10.0 * 6.359752655029297
Epoch 420, val loss: 1.1750500202178955
Epoch 430, training loss: 64.55696105957031 = 0.9848650693893433 + 10.0 * 6.3572096824646
Epoch 430, val loss: 1.1582216024398804
Epoch 440, training loss: 64.48873901367188 = 0.9594345092773438 + 10.0 * 6.352930545806885
Epoch 440, val loss: 1.1419930458068848
Epoch 450, training loss: 64.45025634765625 = 0.9344007968902588 + 10.0 * 6.351585388183594
Epoch 450, val loss: 1.1261202096939087
Epoch 460, training loss: 64.3915023803711 = 0.910065770149231 + 10.0 * 6.348143577575684
Epoch 460, val loss: 1.1108782291412354
Epoch 470, training loss: 64.32796478271484 = 0.8862273693084717 + 10.0 * 6.344173908233643
Epoch 470, val loss: 1.0961061716079712
Epoch 480, training loss: 64.26747131347656 = 0.863124668598175 + 10.0 * 6.340435028076172
Epoch 480, val loss: 1.0821149349212646
Epoch 490, training loss: 64.21866607666016 = 0.8405998349189758 + 10.0 * 6.337806701660156
Epoch 490, val loss: 1.0686525106430054
Epoch 500, training loss: 64.31099700927734 = 0.8187182545661926 + 10.0 * 6.3492279052734375
Epoch 500, val loss: 1.0557830333709717
Epoch 510, training loss: 64.14386749267578 = 0.797112762928009 + 10.0 * 6.334675312042236
Epoch 510, val loss: 1.043121576309204
Epoch 520, training loss: 64.07774353027344 = 0.7763986587524414 + 10.0 * 6.330134391784668
Epoch 520, val loss: 1.0311901569366455
Epoch 530, training loss: 64.03398132324219 = 0.7563294172286987 + 10.0 * 6.327764987945557
Epoch 530, val loss: 1.0199191570281982
Epoch 540, training loss: 63.98595428466797 = 0.7367696166038513 + 10.0 * 6.324918270111084
Epoch 540, val loss: 1.009193778038025
Epoch 550, training loss: 63.94132995605469 = 0.7176573872566223 + 10.0 * 6.322367191314697
Epoch 550, val loss: 0.9989116787910461
Epoch 560, training loss: 63.93476486206055 = 0.6989415884017944 + 10.0 * 6.323582649230957
Epoch 560, val loss: 0.9888788461685181
Epoch 570, training loss: 63.87424850463867 = 0.6805775761604309 + 10.0 * 6.319367408752441
Epoch 570, val loss: 0.9794610142707825
Epoch 580, training loss: 63.832252502441406 = 0.6626960635185242 + 10.0 * 6.31695556640625
Epoch 580, val loss: 0.9703347682952881
Epoch 590, training loss: 63.79051971435547 = 0.6453417539596558 + 10.0 * 6.314517974853516
Epoch 590, val loss: 0.9618769884109497
Epoch 600, training loss: 63.74964904785156 = 0.6283709406852722 + 10.0 * 6.312127590179443
Epoch 600, val loss: 0.9538662433624268
Epoch 610, training loss: 63.734867095947266 = 0.6117116212844849 + 10.0 * 6.312315464019775
Epoch 610, val loss: 0.9459965825080872
Epoch 620, training loss: 63.70185852050781 = 0.5953300595283508 + 10.0 * 6.310652732849121
Epoch 620, val loss: 0.9384323358535767
Epoch 630, training loss: 63.653076171875 = 0.5793625712394714 + 10.0 * 6.307371616363525
Epoch 630, val loss: 0.9312408566474915
Epoch 640, training loss: 63.613372802734375 = 0.5637451410293579 + 10.0 * 6.304963111877441
Epoch 640, val loss: 0.9245212078094482
Epoch 650, training loss: 63.58303451538086 = 0.5484102368354797 + 10.0 * 6.303462505340576
Epoch 650, val loss: 0.9181278944015503
Epoch 660, training loss: 63.54867172241211 = 0.53331458568573 + 10.0 * 6.301535606384277
Epoch 660, val loss: 0.911714494228363
Epoch 670, training loss: 63.53501510620117 = 0.5185202360153198 + 10.0 * 6.301649570465088
Epoch 670, val loss: 0.9056836366653442
Epoch 680, training loss: 63.49217224121094 = 0.5040509700775146 + 10.0 * 6.298811912536621
Epoch 680, val loss: 0.9000400304794312
Epoch 690, training loss: 63.4650764465332 = 0.4899088442325592 + 10.0 * 6.297516822814941
Epoch 690, val loss: 0.8947038054466248
Epoch 700, training loss: 63.44963455200195 = 0.4760339856147766 + 10.0 * 6.297360420227051
Epoch 700, val loss: 0.8896775841712952
Epoch 710, training loss: 63.41147994995117 = 0.4624081254005432 + 10.0 * 6.294907093048096
Epoch 710, val loss: 0.8847761154174805
Epoch 720, training loss: 63.40412521362305 = 0.4490469694137573 + 10.0 * 6.295507907867432
Epoch 720, val loss: 0.8801774382591248
Epoch 730, training loss: 63.36334228515625 = 0.43597519397735596 + 10.0 * 6.292737007141113
Epoch 730, val loss: 0.8758149743080139
Epoch 740, training loss: 63.37120056152344 = 0.4231815040111542 + 10.0 * 6.294802188873291
Epoch 740, val loss: 0.8716738820075989
Epoch 750, training loss: 63.314796447753906 = 0.4106326103210449 + 10.0 * 6.290416240692139
Epoch 750, val loss: 0.8676578402519226
Epoch 760, training loss: 63.28376007080078 = 0.39844411611557007 + 10.0 * 6.288531303405762
Epoch 760, val loss: 0.8639699220657349
Epoch 770, training loss: 63.25873947143555 = 0.38653722405433655 + 10.0 * 6.287220478057861
Epoch 770, val loss: 0.8606653213500977
Epoch 780, training loss: 63.3013801574707 = 0.3749558627605438 + 10.0 * 6.292642593383789
Epoch 780, val loss: 0.8577361106872559
Epoch 790, training loss: 63.23967361450195 = 0.3634594678878784 + 10.0 * 6.28762149810791
Epoch 790, val loss: 0.8545905947685242
Epoch 800, training loss: 63.19676208496094 = 0.35239163041114807 + 10.0 * 6.28443717956543
Epoch 800, val loss: 0.8520839810371399
Epoch 810, training loss: 63.1728401184082 = 0.34156617522239685 + 10.0 * 6.283127307891846
Epoch 810, val loss: 0.849712073802948
Epoch 820, training loss: 63.20549774169922 = 0.33101120591163635 + 10.0 * 6.287448406219482
Epoch 820, val loss: 0.8475689888000488
Epoch 830, training loss: 63.144386291503906 = 0.320801317691803 + 10.0 * 6.282358646392822
Epoch 830, val loss: 0.84598708152771
Epoch 840, training loss: 63.13682556152344 = 0.31081637740135193 + 10.0 * 6.2826008796691895
Epoch 840, val loss: 0.8443935513496399
Epoch 850, training loss: 63.093360900878906 = 0.3011946976184845 + 10.0 * 6.279216766357422
Epoch 850, val loss: 0.8430378437042236
Epoch 860, training loss: 63.07475280761719 = 0.29183053970336914 + 10.0 * 6.278292179107666
Epoch 860, val loss: 0.8420219421386719
Epoch 870, training loss: 63.111175537109375 = 0.28278470039367676 + 10.0 * 6.282839298248291
Epoch 870, val loss: 0.8413817286491394
Epoch 880, training loss: 63.05485916137695 = 0.2739134728908539 + 10.0 * 6.278094291687012
Epoch 880, val loss: 0.8407465219497681
Epoch 890, training loss: 63.02188491821289 = 0.265360563993454 + 10.0 * 6.2756524085998535
Epoch 890, val loss: 0.8403768539428711
Epoch 900, training loss: 63.00263977050781 = 0.25713205337524414 + 10.0 * 6.274550437927246
Epoch 900, val loss: 0.8403686881065369
Epoch 910, training loss: 63.01283264160156 = 0.24912624061107635 + 10.0 * 6.276370525360107
Epoch 910, val loss: 0.8406344652175903
Epoch 920, training loss: 63.01911163330078 = 0.24138112366199493 + 10.0 * 6.277772903442383
Epoch 920, val loss: 0.8410242199897766
Epoch 930, training loss: 62.961910247802734 = 0.233851820230484 + 10.0 * 6.272805690765381
Epoch 930, val loss: 0.8413169384002686
Epoch 940, training loss: 62.94340515136719 = 0.22662998735904694 + 10.0 * 6.271677494049072
Epoch 940, val loss: 0.8422883749008179
Epoch 950, training loss: 62.92124938964844 = 0.21969854831695557 + 10.0 * 6.27015495300293
Epoch 950, val loss: 0.8435265421867371
Epoch 960, training loss: 62.94683837890625 = 0.21302901208400726 + 10.0 * 6.273381233215332
Epoch 960, val loss: 0.8447995185852051
Epoch 970, training loss: 62.9546012878418 = 0.20643050968647003 + 10.0 * 6.274816989898682
Epoch 970, val loss: 0.8459087014198303
Epoch 980, training loss: 62.88871383666992 = 0.20009773969650269 + 10.0 * 6.268861770629883
Epoch 980, val loss: 0.8470813632011414
Epoch 990, training loss: 62.867183685302734 = 0.19401495158672333 + 10.0 * 6.267316818237305
Epoch 990, val loss: 0.8490228056907654
Epoch 1000, training loss: 62.84977340698242 = 0.18817628920078278 + 10.0 * 6.266160011291504
Epoch 1000, val loss: 0.8509089350700378
Epoch 1010, training loss: 62.86640548706055 = 0.1825462281703949 + 10.0 * 6.268385887145996
Epoch 1010, val loss: 0.8530547022819519
Epoch 1020, training loss: 62.82456970214844 = 0.17700329422950745 + 10.0 * 6.264756679534912
Epoch 1020, val loss: 0.8549056649208069
Epoch 1030, training loss: 62.82082748413086 = 0.17167873680591583 + 10.0 * 6.264914512634277
Epoch 1030, val loss: 0.8573511838912964
Epoch 1040, training loss: 62.80586242675781 = 0.1665336638689041 + 10.0 * 6.263932704925537
Epoch 1040, val loss: 0.8596649169921875
Epoch 1050, training loss: 62.872196197509766 = 0.1616014838218689 + 10.0 * 6.271059513092041
Epoch 1050, val loss: 0.8622602820396423
Epoch 1060, training loss: 62.8060302734375 = 0.15671679377555847 + 10.0 * 6.264931678771973
Epoch 1060, val loss: 0.865107536315918
Epoch 1070, training loss: 62.76648712158203 = 0.1520540416240692 + 10.0 * 6.261443138122559
Epoch 1070, val loss: 0.8678062558174133
Epoch 1080, training loss: 62.7675666809082 = 0.14757780730724335 + 10.0 * 6.261998653411865
Epoch 1080, val loss: 0.870823860168457
Epoch 1090, training loss: 62.75600051879883 = 0.1432172954082489 + 10.0 * 6.26127815246582
Epoch 1090, val loss: 0.8737559914588928
Epoch 1100, training loss: 62.74798583984375 = 0.13897983729839325 + 10.0 * 6.260900497436523
Epoch 1100, val loss: 0.8766850233078003
Epoch 1110, training loss: 62.741214752197266 = 0.13489177823066711 + 10.0 * 6.260632514953613
Epoch 1110, val loss: 0.8799225687980652
Epoch 1120, training loss: 62.735740661621094 = 0.13091593980789185 + 10.0 * 6.260482311248779
Epoch 1120, val loss: 0.8831836581230164
Epoch 1130, training loss: 62.710365295410156 = 0.12710581719875336 + 10.0 * 6.258326053619385
Epoch 1130, val loss: 0.8865018486976624
Epoch 1140, training loss: 62.69559860229492 = 0.12340724468231201 + 10.0 * 6.257219314575195
Epoch 1140, val loss: 0.8901389837265015
Epoch 1150, training loss: 62.70109939575195 = 0.11983051151037216 + 10.0 * 6.258126735687256
Epoch 1150, val loss: 0.893725574016571
Epoch 1160, training loss: 62.69524002075195 = 0.11631512641906738 + 10.0 * 6.257892608642578
Epoch 1160, val loss: 0.8973541259765625
Epoch 1170, training loss: 62.72593688964844 = 0.11288388818502426 + 10.0 * 6.261305332183838
Epoch 1170, val loss: 0.9006767868995667
Epoch 1180, training loss: 62.659934997558594 = 0.10959671437740326 + 10.0 * 6.25503396987915
Epoch 1180, val loss: 0.9045792818069458
Epoch 1190, training loss: 62.64223861694336 = 0.10642630606889725 + 10.0 * 6.2535810470581055
Epoch 1190, val loss: 0.9082798957824707
Epoch 1200, training loss: 62.633296966552734 = 0.10335849225521088 + 10.0 * 6.252993583679199
Epoch 1200, val loss: 0.9122905731201172
Epoch 1210, training loss: 62.711029052734375 = 0.10039647668600082 + 10.0 * 6.261063575744629
Epoch 1210, val loss: 0.9162329435348511
Epoch 1220, training loss: 62.673797607421875 = 0.09747447073459625 + 10.0 * 6.257632255554199
Epoch 1220, val loss: 0.919931948184967
Epoch 1230, training loss: 62.60898208618164 = 0.09467340260744095 + 10.0 * 6.251430988311768
Epoch 1230, val loss: 0.9239473938941956
Epoch 1240, training loss: 62.60436248779297 = 0.09198745340108871 + 10.0 * 6.251237392425537
Epoch 1240, val loss: 0.9278765916824341
Epoch 1250, training loss: 62.613101959228516 = 0.08939228951931 + 10.0 * 6.252370834350586
Epoch 1250, val loss: 0.9319736957550049
Epoch 1260, training loss: 62.6202392578125 = 0.086860790848732 + 10.0 * 6.253337860107422
Epoch 1260, val loss: 0.9360697865486145
Epoch 1270, training loss: 62.57819366455078 = 0.08437709510326385 + 10.0 * 6.2493815422058105
Epoch 1270, val loss: 0.9398458003997803
Epoch 1280, training loss: 62.56939697265625 = 0.08200331777334213 + 10.0 * 6.248739242553711
Epoch 1280, val loss: 0.9441478848457336
Epoch 1290, training loss: 62.56354904174805 = 0.07973222434520721 + 10.0 * 6.248381614685059
Epoch 1290, val loss: 0.9482560753822327
Epoch 1300, training loss: 62.609195709228516 = 0.07751964032649994 + 10.0 * 6.253167629241943
Epoch 1300, val loss: 0.9523962140083313
Epoch 1310, training loss: 62.57522964477539 = 0.07538831979036331 + 10.0 * 6.249983787536621
Epoch 1310, val loss: 0.9564412236213684
Epoch 1320, training loss: 62.54663848876953 = 0.07330232113599777 + 10.0 * 6.247333526611328
Epoch 1320, val loss: 0.9606715440750122
Epoch 1330, training loss: 62.539764404296875 = 0.07132840156555176 + 10.0 * 6.2468438148498535
Epoch 1330, val loss: 0.9650028944015503
Epoch 1340, training loss: 62.625186920166016 = 0.0693921446800232 + 10.0 * 6.255579471588135
Epoch 1340, val loss: 0.96909099817276
Epoch 1350, training loss: 62.52534484863281 = 0.06751634180545807 + 10.0 * 6.245782852172852
Epoch 1350, val loss: 0.9730770587921143
Epoch 1360, training loss: 62.519676208496094 = 0.06571859866380692 + 10.0 * 6.245395660400391
Epoch 1360, val loss: 0.9772544503211975
Epoch 1370, training loss: 62.50428009033203 = 0.06398423761129379 + 10.0 * 6.244029521942139
Epoch 1370, val loss: 0.9816367626190186
Epoch 1380, training loss: 62.49701690673828 = 0.062319397926330566 + 10.0 * 6.243469715118408
Epoch 1380, val loss: 0.9859480261802673
Epoch 1390, training loss: 62.569908142089844 = 0.06070100888609886 + 10.0 * 6.25092077255249
Epoch 1390, val loss: 0.9900445938110352
Epoch 1400, training loss: 62.547306060791016 = 0.059110093861818314 + 10.0 * 6.248819828033447
Epoch 1400, val loss: 0.9945504665374756
Epoch 1410, training loss: 62.483665466308594 = 0.057580843567848206 + 10.0 * 6.242608547210693
Epoch 1410, val loss: 0.998487114906311
Epoch 1420, training loss: 62.480411529541016 = 0.056114379316568375 + 10.0 * 6.242429733276367
Epoch 1420, val loss: 1.0026775598526
Epoch 1430, training loss: 62.47425842285156 = 0.05470847710967064 + 10.0 * 6.241955280303955
Epoch 1430, val loss: 1.0071282386779785
Epoch 1440, training loss: 62.53318405151367 = 0.05334409326314926 + 10.0 * 6.247983932495117
Epoch 1440, val loss: 1.0114572048187256
Epoch 1450, training loss: 62.49772644042969 = 0.051993366330862045 + 10.0 * 6.24457311630249
Epoch 1450, val loss: 1.0154461860656738
Epoch 1460, training loss: 62.473934173583984 = 0.050692975521087646 + 10.0 * 6.242323875427246
Epoch 1460, val loss: 1.0197744369506836
Epoch 1470, training loss: 62.44704818725586 = 0.04945632442831993 + 10.0 * 6.2397589683532715
Epoch 1470, val loss: 1.0238399505615234
Epoch 1480, training loss: 62.4569206237793 = 0.048260584473609924 + 10.0 * 6.240866184234619
Epoch 1480, val loss: 1.028272271156311
Epoch 1490, training loss: 62.46388626098633 = 0.04709022119641304 + 10.0 * 6.241679668426514
Epoch 1490, val loss: 1.0324876308441162
Epoch 1500, training loss: 62.47929382324219 = 0.04595135152339935 + 10.0 * 6.2433342933654785
Epoch 1500, val loss: 1.036651611328125
Epoch 1510, training loss: 62.43669509887695 = 0.04485524818301201 + 10.0 * 6.2391839027404785
Epoch 1510, val loss: 1.0406100749969482
Epoch 1520, training loss: 62.442264556884766 = 0.04379769787192345 + 10.0 * 6.239846706390381
Epoch 1520, val loss: 1.0446434020996094
Epoch 1530, training loss: 62.44194793701172 = 0.04276656731963158 + 10.0 * 6.239918231964111
Epoch 1530, val loss: 1.0490275621414185
Epoch 1540, training loss: 62.4338264465332 = 0.04178229719400406 + 10.0 * 6.239204406738281
Epoch 1540, val loss: 1.053248405456543
Epoch 1550, training loss: 62.46513366699219 = 0.040811508893966675 + 10.0 * 6.242432594299316
Epoch 1550, val loss: 1.0572855472564697
Epoch 1560, training loss: 62.41720199584961 = 0.03988794982433319 + 10.0 * 6.237731456756592
Epoch 1560, val loss: 1.061410665512085
Epoch 1570, training loss: 62.439937591552734 = 0.03898540139198303 + 10.0 * 6.240095138549805
Epoch 1570, val loss: 1.0653445720672607
Epoch 1580, training loss: 62.40268325805664 = 0.0381048321723938 + 10.0 * 6.236457824707031
Epoch 1580, val loss: 1.069667935371399
Epoch 1590, training loss: 62.39299774169922 = 0.03725529834628105 + 10.0 * 6.235574245452881
Epoch 1590, val loss: 1.0736236572265625
Epoch 1600, training loss: 62.38888168334961 = 0.036441437900066376 + 10.0 * 6.235243797302246
Epoch 1600, val loss: 1.0777627229690552
Epoch 1610, training loss: 62.445899963378906 = 0.03564610332250595 + 10.0 * 6.241025447845459
Epoch 1610, val loss: 1.0819071531295776
Epoch 1620, training loss: 62.39970016479492 = 0.034862618893384933 + 10.0 * 6.236483573913574
Epoch 1620, val loss: 1.085670828819275
Epoch 1630, training loss: 62.381649017333984 = 0.03410705551505089 + 10.0 * 6.2347540855407715
Epoch 1630, val loss: 1.089704990386963
Epoch 1640, training loss: 62.37251663208008 = 0.033381253480911255 + 10.0 * 6.233913421630859
Epoch 1640, val loss: 1.093749761581421
Epoch 1650, training loss: 62.408180236816406 = 0.03268466889858246 + 10.0 * 6.237549781799316
Epoch 1650, val loss: 1.097898244857788
Epoch 1660, training loss: 62.36764907836914 = 0.031987328082323074 + 10.0 * 6.2335662841796875
Epoch 1660, val loss: 1.1016062498092651
Epoch 1670, training loss: 62.39433288574219 = 0.03131771832704544 + 10.0 * 6.236301422119141
Epoch 1670, val loss: 1.1057029962539673
Epoch 1680, training loss: 62.35578918457031 = 0.030663013458251953 + 10.0 * 6.232512474060059
Epoch 1680, val loss: 1.109415054321289
Epoch 1690, training loss: 62.35956573486328 = 0.030036618933081627 + 10.0 * 6.232953071594238
Epoch 1690, val loss: 1.113466739654541
Epoch 1700, training loss: 62.390201568603516 = 0.029432497918605804 + 10.0 * 6.236076831817627
Epoch 1700, val loss: 1.1173559427261353
Epoch 1710, training loss: 62.36117935180664 = 0.028838975355029106 + 10.0 * 6.23323392868042
Epoch 1710, val loss: 1.1211742162704468
Epoch 1720, training loss: 62.34127426147461 = 0.028253093361854553 + 10.0 * 6.231302261352539
Epoch 1720, val loss: 1.1249442100524902
Epoch 1730, training loss: 62.33266830444336 = 0.027693763375282288 + 10.0 * 6.230497360229492
Epoch 1730, val loss: 1.1289160251617432
Epoch 1740, training loss: 62.33768844604492 = 0.027155591174960136 + 10.0 * 6.231053352355957
Epoch 1740, val loss: 1.13274085521698
Epoch 1750, training loss: 62.37759017944336 = 0.02662869356572628 + 10.0 * 6.235095977783203
Epoch 1750, val loss: 1.136441707611084
Epoch 1760, training loss: 62.37251281738281 = 0.026112502440810204 + 10.0 * 6.234640121459961
Epoch 1760, val loss: 1.1402719020843506
Epoch 1770, training loss: 62.34236526489258 = 0.02559979073703289 + 10.0 * 6.2316765785217285
Epoch 1770, val loss: 1.144020915031433
Epoch 1780, training loss: 62.322998046875 = 0.02510586567223072 + 10.0 * 6.2297892570495605
Epoch 1780, val loss: 1.1476882696151733
Epoch 1790, training loss: 62.31444549560547 = 0.024638494476675987 + 10.0 * 6.228980541229248
Epoch 1790, val loss: 1.1515182256698608
Epoch 1800, training loss: 62.32927322387695 = 0.02418382838368416 + 10.0 * 6.230508804321289
Epoch 1800, val loss: 1.155362606048584
Epoch 1810, training loss: 62.359405517578125 = 0.023733612149953842 + 10.0 * 6.233567237854004
Epoch 1810, val loss: 1.1591204404830933
Epoch 1820, training loss: 62.32695770263672 = 0.023297753185033798 + 10.0 * 6.230366230010986
Epoch 1820, val loss: 1.1625339984893799
Epoch 1830, training loss: 62.30366516113281 = 0.02286495640873909 + 10.0 * 6.228079795837402
Epoch 1830, val loss: 1.1662135124206543
Epoch 1840, training loss: 62.29524612426758 = 0.022454900667071342 + 10.0 * 6.227279186248779
Epoch 1840, val loss: 1.1699988842010498
Epoch 1850, training loss: 62.29512405395508 = 0.022056136280298233 + 10.0 * 6.227306842803955
Epoch 1850, val loss: 1.173672080039978
Epoch 1860, training loss: 62.370811462402344 = 0.021667078137397766 + 10.0 * 6.234914302825928
Epoch 1860, val loss: 1.1772897243499756
Epoch 1870, training loss: 62.33877944946289 = 0.021283486858010292 + 10.0 * 6.231749534606934
Epoch 1870, val loss: 1.1808499097824097
Epoch 1880, training loss: 62.29966735839844 = 0.0209009051322937 + 10.0 * 6.227876663208008
Epoch 1880, val loss: 1.184225082397461
Epoch 1890, training loss: 62.27743148803711 = 0.020540529862046242 + 10.0 * 6.225688934326172
Epoch 1890, val loss: 1.1878262758255005
Epoch 1900, training loss: 62.27824401855469 = 0.02018863521516323 + 10.0 * 6.225805759429932
Epoch 1900, val loss: 1.1914857625961304
Epoch 1910, training loss: 62.32730484008789 = 0.01984691619873047 + 10.0 * 6.230745792388916
Epoch 1910, val loss: 1.1950488090515137
Epoch 1920, training loss: 62.28833770751953 = 0.01951042003929615 + 10.0 * 6.226882457733154
Epoch 1920, val loss: 1.19818913936615
Epoch 1930, training loss: 62.28858947753906 = 0.019179681316018105 + 10.0 * 6.226941108703613
Epoch 1930, val loss: 1.201825737953186
Epoch 1940, training loss: 62.30632400512695 = 0.018859082832932472 + 10.0 * 6.22874641418457
Epoch 1940, val loss: 1.2052249908447266
Epoch 1950, training loss: 62.287906646728516 = 0.018537411466240883 + 10.0 * 6.226937294006348
Epoch 1950, val loss: 1.2086119651794434
Epoch 1960, training loss: 62.265663146972656 = 0.018228907138109207 + 10.0 * 6.224743366241455
Epoch 1960, val loss: 1.2117470502853394
Epoch 1970, training loss: 62.263309478759766 = 0.017934711650013924 + 10.0 * 6.2245378494262695
Epoch 1970, val loss: 1.2150802612304688
Epoch 1980, training loss: 62.25196075439453 = 0.017647549510002136 + 10.0 * 6.223431587219238
Epoch 1980, val loss: 1.218672275543213
Epoch 1990, training loss: 62.246612548828125 = 0.017371412366628647 + 10.0 * 6.22292423248291
Epoch 1990, val loss: 1.2221360206604004
Epoch 2000, training loss: 62.30046844482422 = 0.017101382836699486 + 10.0 * 6.228336811065674
Epoch 2000, val loss: 1.2257198095321655
Epoch 2010, training loss: 62.25778579711914 = 0.016822846606373787 + 10.0 * 6.224096298217773
Epoch 2010, val loss: 1.2284257411956787
Epoch 2020, training loss: 62.258567810058594 = 0.0165551844984293 + 10.0 * 6.224201202392578
Epoch 2020, val loss: 1.2318956851959229
Epoch 2030, training loss: 62.24250030517578 = 0.01629863679409027 + 10.0 * 6.222620010375977
Epoch 2030, val loss: 1.2350361347198486
Epoch 2040, training loss: 62.25864028930664 = 0.01605197973549366 + 10.0 * 6.224258899688721
Epoch 2040, val loss: 1.238252878189087
Epoch 2050, training loss: 62.23744583129883 = 0.01580399088561535 + 10.0 * 6.222164154052734
Epoch 2050, val loss: 1.2415473461151123
Epoch 2060, training loss: 62.242095947265625 = 0.015563788823783398 + 10.0 * 6.222653388977051
Epoch 2060, val loss: 1.2445896863937378
Epoch 2070, training loss: 62.27375030517578 = 0.015328525565564632 + 10.0 * 6.225842475891113
Epoch 2070, val loss: 1.247766137123108
Epoch 2080, training loss: 62.2454833984375 = 0.015101523138582706 + 10.0 * 6.223038196563721
Epoch 2080, val loss: 1.250933289527893
Epoch 2090, training loss: 62.228946685791016 = 0.014878833666443825 + 10.0 * 6.221406936645508
Epoch 2090, val loss: 1.2539979219436646
Epoch 2100, training loss: 62.22556686401367 = 0.01465850230306387 + 10.0 * 6.221090793609619
Epoch 2100, val loss: 1.257067322731018
Epoch 2110, training loss: 62.27286911010742 = 0.014450650662183762 + 10.0 * 6.225841999053955
Epoch 2110, val loss: 1.2601220607757568
Epoch 2120, training loss: 62.221900939941406 = 0.014235151000320911 + 10.0 * 6.220766544342041
Epoch 2120, val loss: 1.2634270191192627
Epoch 2130, training loss: 62.21515655517578 = 0.014031591825187206 + 10.0 * 6.2201128005981445
Epoch 2130, val loss: 1.266369104385376
Epoch 2140, training loss: 62.243106842041016 = 0.013835055753588676 + 10.0 * 6.222927093505859
Epoch 2140, val loss: 1.2695077657699585
Epoch 2150, training loss: 62.22267532348633 = 0.013640088029205799 + 10.0 * 6.220903396606445
Epoch 2150, val loss: 1.2725214958190918
Epoch 2160, training loss: 62.22599411010742 = 0.013446391560137272 + 10.0 * 6.221254825592041
Epoch 2160, val loss: 1.275307536125183
Epoch 2170, training loss: 62.20458984375 = 0.01325502060353756 + 10.0 * 6.219133377075195
Epoch 2170, val loss: 1.2782878875732422
Epoch 2180, training loss: 62.22195053100586 = 0.013073854148387909 + 10.0 * 6.220887660980225
Epoch 2180, val loss: 1.2812656164169312
Epoch 2190, training loss: 62.21065139770508 = 0.012895261868834496 + 10.0 * 6.219775676727295
Epoch 2190, val loss: 1.2841017246246338
Epoch 2200, training loss: 62.21702575683594 = 0.012722731567919254 + 10.0 * 6.220430374145508
Epoch 2200, val loss: 1.2871177196502686
Epoch 2210, training loss: 62.20916748046875 = 0.012549412436783314 + 10.0 * 6.219661712646484
Epoch 2210, val loss: 1.2898937463760376
Epoch 2220, training loss: 62.20700454711914 = 0.012377653270959854 + 10.0 * 6.2194623947143555
Epoch 2220, val loss: 1.2927696704864502
Epoch 2230, training loss: 62.23644256591797 = 0.012212792411446571 + 10.0 * 6.2224225997924805
Epoch 2230, val loss: 1.295549750328064
Epoch 2240, training loss: 62.20845031738281 = 0.012052977457642555 + 10.0 * 6.219639778137207
Epoch 2240, val loss: 1.2983444929122925
Epoch 2250, training loss: 62.187644958496094 = 0.011890827678143978 + 10.0 * 6.217575550079346
Epoch 2250, val loss: 1.3012527227401733
Epoch 2260, training loss: 62.184669494628906 = 0.011740999296307564 + 10.0 * 6.217292785644531
Epoch 2260, val loss: 1.304043173789978
Epoch 2270, training loss: 62.23686218261719 = 0.011592114344239235 + 10.0 * 6.222527027130127
Epoch 2270, val loss: 1.3068028688430786
Epoch 2280, training loss: 62.22836685180664 = 0.011441333219408989 + 10.0 * 6.2216925621032715
Epoch 2280, val loss: 1.3093751668930054
Epoch 2290, training loss: 62.19057846069336 = 0.011293597519397736 + 10.0 * 6.217928409576416
Epoch 2290, val loss: 1.3122891187667847
Epoch 2300, training loss: 62.18576431274414 = 0.01115002203732729 + 10.0 * 6.217461585998535
Epoch 2300, val loss: 1.3149192333221436
Epoch 2310, training loss: 62.20480728149414 = 0.011011558584868908 + 10.0 * 6.219379425048828
Epoch 2310, val loss: 1.3174915313720703
Epoch 2320, training loss: 62.17973327636719 = 0.010873387567698956 + 10.0 * 6.216886043548584
Epoch 2320, val loss: 1.3203604221343994
Epoch 2330, training loss: 62.17926025390625 = 0.010740893892943859 + 10.0 * 6.216851711273193
Epoch 2330, val loss: 1.3229111433029175
Epoch 2340, training loss: 62.189231872558594 = 0.010611758567392826 + 10.0 * 6.217862129211426
Epoch 2340, val loss: 1.3257899284362793
Epoch 2350, training loss: 62.176673889160156 = 0.01048086304217577 + 10.0 * 6.216619491577148
Epoch 2350, val loss: 1.3283673524856567
Epoch 2360, training loss: 62.16986846923828 = 0.010351774282753468 + 10.0 * 6.215951442718506
Epoch 2360, val loss: 1.3307570219039917
Epoch 2370, training loss: 62.15927505493164 = 0.010231468826532364 + 10.0 * 6.214904308319092
Epoch 2370, val loss: 1.333625316619873
Epoch 2380, training loss: 62.25635528564453 = 0.010116402991116047 + 10.0 * 6.224623680114746
Epoch 2380, val loss: 1.3360497951507568
Epoch 2390, training loss: 62.19178009033203 = 0.009987852536141872 + 10.0 * 6.218179225921631
Epoch 2390, val loss: 1.3383195400238037
Epoch 2400, training loss: 62.16066360473633 = 0.009869201108813286 + 10.0 * 6.215079307556152
Epoch 2400, val loss: 1.3410013914108276
Epoch 2410, training loss: 62.15047073364258 = 0.00975346565246582 + 10.0 * 6.214071750640869
Epoch 2410, val loss: 1.3435046672821045
Epoch 2420, training loss: 62.151634216308594 = 0.009644241072237492 + 10.0 * 6.214199066162109
Epoch 2420, val loss: 1.3463356494903564
Epoch 2430, training loss: 62.208499908447266 = 0.009534400887787342 + 10.0 * 6.2198967933654785
Epoch 2430, val loss: 1.3487696647644043
Epoch 2440, training loss: 62.165096282958984 = 0.009425627999007702 + 10.0 * 6.215567111968994
Epoch 2440, val loss: 1.3510339260101318
Epoch 2450, training loss: 62.187660217285156 = 0.009320552460849285 + 10.0 * 6.217833995819092
Epoch 2450, val loss: 1.3535929918289185
Epoch 2460, training loss: 62.15115737915039 = 0.009213320910930634 + 10.0 * 6.214194297790527
Epoch 2460, val loss: 1.355981469154358
Epoch 2470, training loss: 62.171607971191406 = 0.0091111334040761 + 10.0 * 6.216249942779541
Epoch 2470, val loss: 1.3583823442459106
Epoch 2480, training loss: 62.15031051635742 = 0.009007846005260944 + 10.0 * 6.214130401611328
Epoch 2480, val loss: 1.3607767820358276
Epoch 2490, training loss: 62.14986038208008 = 0.00890800915658474 + 10.0 * 6.214095115661621
Epoch 2490, val loss: 1.3633337020874023
Epoch 2500, training loss: 62.1309700012207 = 0.008812187239527702 + 10.0 * 6.212215900421143
Epoch 2500, val loss: 1.36576509475708
Epoch 2510, training loss: 62.144195556640625 = 0.008719410747289658 + 10.0 * 6.213547706604004
Epoch 2510, val loss: 1.368174433708191
Epoch 2520, training loss: 62.153587341308594 = 0.008625615388154984 + 10.0 * 6.21449613571167
Epoch 2520, val loss: 1.3706239461898804
Epoch 2530, training loss: 62.14480972290039 = 0.008533530868589878 + 10.0 * 6.213627815246582
Epoch 2530, val loss: 1.372759461402893
Epoch 2540, training loss: 62.21042251586914 = 0.008441836573183537 + 10.0 * 6.220198154449463
Epoch 2540, val loss: 1.3751779794692993
Epoch 2550, training loss: 62.14374542236328 = 0.00835185032337904 + 10.0 * 6.2135396003723145
Epoch 2550, val loss: 1.377462387084961
Epoch 2560, training loss: 62.12708282470703 = 0.008261607028543949 + 10.0 * 6.2118821144104
Epoch 2560, val loss: 1.3795067071914673
Epoch 2570, training loss: 62.11799240112305 = 0.008177344687283039 + 10.0 * 6.210981369018555
Epoch 2570, val loss: 1.3820782899856567
Epoch 2580, training loss: 62.12035369873047 = 0.00809610728174448 + 10.0 * 6.211225986480713
Epoch 2580, val loss: 1.3844795227050781
Epoch 2590, training loss: 62.21638488769531 = 0.008017918094992638 + 10.0 * 6.220836639404297
Epoch 2590, val loss: 1.3867011070251465
Epoch 2600, training loss: 62.1495246887207 = 0.00792502798140049 + 10.0 * 6.214159965515137
Epoch 2600, val loss: 1.3886940479278564
Epoch 2610, training loss: 62.131221771240234 = 0.007847514934837818 + 10.0 * 6.212337493896484
Epoch 2610, val loss: 1.39106285572052
Epoch 2620, training loss: 62.11604309082031 = 0.007767010945826769 + 10.0 * 6.210827827453613
Epoch 2620, val loss: 1.3931952714920044
Epoch 2630, training loss: 62.10478210449219 = 0.007691195700317621 + 10.0 * 6.209709167480469
Epoch 2630, val loss: 1.3955130577087402
Epoch 2640, training loss: 62.11787033081055 = 0.00761707779020071 + 10.0 * 6.211025238037109
Epoch 2640, val loss: 1.397836685180664
Epoch 2650, training loss: 62.15556716918945 = 0.007543288636952639 + 10.0 * 6.2148027420043945
Epoch 2650, val loss: 1.3999007940292358
Epoch 2660, training loss: 62.141117095947266 = 0.007467237301170826 + 10.0 * 6.213365077972412
Epoch 2660, val loss: 1.4018558263778687
Epoch 2670, training loss: 62.136173248291016 = 0.007394834887236357 + 10.0 * 6.2128777503967285
Epoch 2670, val loss: 1.4040429592132568
Epoch 2680, training loss: 62.123714447021484 = 0.007323644123971462 + 10.0 * 6.211638927459717
Epoch 2680, val loss: 1.4059972763061523
Epoch 2690, training loss: 62.1210823059082 = 0.007253426592797041 + 10.0 * 6.211382865905762
Epoch 2690, val loss: 1.4083001613616943
Epoch 2700, training loss: 62.11009216308594 = 0.007182714994996786 + 10.0 * 6.210290908813477
Epoch 2700, val loss: 1.4105207920074463
Epoch 2710, training loss: 62.095882415771484 = 0.007115548010915518 + 10.0 * 6.208876609802246
Epoch 2710, val loss: 1.4125794172286987
Epoch 2720, training loss: 62.119686126708984 = 0.007051633205264807 + 10.0 * 6.211263179779053
Epoch 2720, val loss: 1.414551019668579
Epoch 2730, training loss: 62.12849426269531 = 0.006986851803958416 + 10.0 * 6.212150573730469
Epoch 2730, val loss: 1.416805386543274
Epoch 2740, training loss: 62.10480499267578 = 0.006919975858181715 + 10.0 * 6.2097883224487305
Epoch 2740, val loss: 1.4187793731689453
Epoch 2750, training loss: 62.096221923828125 = 0.006856297608464956 + 10.0 * 6.20893669128418
Epoch 2750, val loss: 1.420706868171692
Epoch 2760, training loss: 62.09096908569336 = 0.00679442984983325 + 10.0 * 6.2084174156188965
Epoch 2760, val loss: 1.4229706525802612
Epoch 2770, training loss: 62.11780548095703 = 0.00673552043735981 + 10.0 * 6.211106777191162
Epoch 2770, val loss: 1.4249449968338013
Epoch 2780, training loss: 62.099178314208984 = 0.006673465482890606 + 10.0 * 6.209250450134277
Epoch 2780, val loss: 1.4269598722457886
Epoch 2790, training loss: 62.094276428222656 = 0.006612391676753759 + 10.0 * 6.208766460418701
Epoch 2790, val loss: 1.4290618896484375
Epoch 2800, training loss: 62.13509750366211 = 0.006553500425070524 + 10.0 * 6.212854385375977
Epoch 2800, val loss: 1.4308624267578125
Epoch 2810, training loss: 62.10096740722656 = 0.006493585649877787 + 10.0 * 6.209447383880615
Epoch 2810, val loss: 1.4329328536987305
Epoch 2820, training loss: 62.083168029785156 = 0.006438000593334436 + 10.0 * 6.207673072814941
Epoch 2820, val loss: 1.4347120523452759
Epoch 2830, training loss: 62.078453063964844 = 0.00638178363442421 + 10.0 * 6.207207202911377
Epoch 2830, val loss: 1.4368526935577393
Epoch 2840, training loss: 62.08530807495117 = 0.0063290903344750404 + 10.0 * 6.207898139953613
Epoch 2840, val loss: 1.4389090538024902
Epoch 2850, training loss: 62.1313591003418 = 0.00627504987642169 + 10.0 * 6.212508201599121
Epoch 2850, val loss: 1.4407888650894165
Epoch 2860, training loss: 62.08989715576172 = 0.006219514179974794 + 10.0 * 6.208367824554443
Epoch 2860, val loss: 1.4422444105148315
Epoch 2870, training loss: 62.088741302490234 = 0.006166635546833277 + 10.0 * 6.20825719833374
Epoch 2870, val loss: 1.4444445371627808
Epoch 2880, training loss: 62.122920989990234 = 0.006114272866398096 + 10.0 * 6.2116804122924805
Epoch 2880, val loss: 1.4460939168930054
Epoch 2890, training loss: 62.07244110107422 = 0.006062623579055071 + 10.0 * 6.206637859344482
Epoch 2890, val loss: 1.4480655193328857
Epoch 2900, training loss: 62.0928840637207 = 0.00601252680644393 + 10.0 * 6.2086873054504395
Epoch 2900, val loss: 1.4498733282089233
Epoch 2910, training loss: 62.08694839477539 = 0.005962030962109566 + 10.0 * 6.208098411560059
Epoch 2910, val loss: 1.4518157243728638
Epoch 2920, training loss: 62.0720329284668 = 0.005914322566241026 + 10.0 * 6.2066121101379395
Epoch 2920, val loss: 1.4535282850265503
Epoch 2930, training loss: 62.071563720703125 = 0.005865868646651506 + 10.0 * 6.206569671630859
Epoch 2930, val loss: 1.4555341005325317
Epoch 2940, training loss: 62.12259292602539 = 0.0058188545517623425 + 10.0 * 6.211677551269531
Epoch 2940, val loss: 1.457197904586792
Epoch 2950, training loss: 62.075923919677734 = 0.005773001816123724 + 10.0 * 6.207015037536621
Epoch 2950, val loss: 1.4589264392852783
Epoch 2960, training loss: 62.05694580078125 = 0.00572552066296339 + 10.0 * 6.205121994018555
Epoch 2960, val loss: 1.460697054862976
Epoch 2970, training loss: 62.066219329833984 = 0.005681815091520548 + 10.0 * 6.206053733825684
Epoch 2970, val loss: 1.4625495672225952
Epoch 2980, training loss: 62.12691116333008 = 0.005637691356241703 + 10.0 * 6.212127208709717
Epoch 2980, val loss: 1.4640142917633057
Epoch 2990, training loss: 62.0757942199707 = 0.005591922905296087 + 10.0 * 6.207020282745361
Epoch 2990, val loss: 1.4659497737884521
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 87.93574523925781 = 1.9672253131866455 + 10.0 * 8.59685230255127
Epoch 0, val loss: 1.965011715888977
Epoch 10, training loss: 87.91960144042969 = 1.9561125040054321 + 10.0 * 8.596348762512207
Epoch 10, val loss: 1.9544180631637573
Epoch 20, training loss: 87.86815643310547 = 1.942910075187683 + 10.0 * 8.592524528503418
Epoch 20, val loss: 1.9414604902267456
Epoch 30, training loss: 87.5825424194336 = 1.9262503385543823 + 10.0 * 8.565629959106445
Epoch 30, val loss: 1.9249674081802368
Epoch 40, training loss: 86.05973815917969 = 1.9054681062698364 + 10.0 * 8.415426254272461
Epoch 40, val loss: 1.9045839309692383
Epoch 50, training loss: 81.28872680664062 = 1.8799246549606323 + 10.0 * 7.940880298614502
Epoch 50, val loss: 1.879487156867981
Epoch 60, training loss: 77.9883041381836 = 1.8564308881759644 + 10.0 * 7.613187313079834
Epoch 60, val loss: 1.8581053018569946
Epoch 70, training loss: 73.88311004638672 = 1.8430441617965698 + 10.0 * 7.204007148742676
Epoch 70, val loss: 1.845876932144165
Epoch 80, training loss: 71.68081665039062 = 1.832571029663086 + 10.0 * 6.984824180603027
Epoch 80, val loss: 1.8353849649429321
Epoch 90, training loss: 70.48222351074219 = 1.8203325271606445 + 10.0 * 6.866189002990723
Epoch 90, val loss: 1.8233802318572998
Epoch 100, training loss: 69.55654907226562 = 1.8077616691589355 + 10.0 * 6.774878978729248
Epoch 100, val loss: 1.8116313219070435
Epoch 110, training loss: 68.91949462890625 = 1.7969343662261963 + 10.0 * 6.712255954742432
Epoch 110, val loss: 1.8015934228897095
Epoch 120, training loss: 68.46670532226562 = 1.787369728088379 + 10.0 * 6.667933940887451
Epoch 120, val loss: 1.792698621749878
Epoch 130, training loss: 68.11261749267578 = 1.7779796123504639 + 10.0 * 6.633464336395264
Epoch 130, val loss: 1.7841415405273438
Epoch 140, training loss: 67.81951141357422 = 1.7684502601623535 + 10.0 * 6.605106353759766
Epoch 140, val loss: 1.7755860090255737
Epoch 150, training loss: 67.55034637451172 = 1.7587058544158936 + 10.0 * 6.579164028167725
Epoch 150, val loss: 1.767037034034729
Epoch 160, training loss: 67.28050231933594 = 1.748552918434143 + 10.0 * 6.553194999694824
Epoch 160, val loss: 1.7583978176116943
Epoch 170, training loss: 67.0644302368164 = 1.7377569675445557 + 10.0 * 6.532667636871338
Epoch 170, val loss: 1.7493857145309448
Epoch 180, training loss: 66.88616943359375 = 1.7259645462036133 + 10.0 * 6.51602029800415
Epoch 180, val loss: 1.7396836280822754
Epoch 190, training loss: 66.69517517089844 = 1.7132136821746826 + 10.0 * 6.498196601867676
Epoch 190, val loss: 1.7292121648788452
Epoch 200, training loss: 66.56080627441406 = 1.6995031833648682 + 10.0 * 6.486130237579346
Epoch 200, val loss: 1.7179192304611206
Epoch 210, training loss: 66.38467407226562 = 1.6844942569732666 + 10.0 * 6.470017910003662
Epoch 210, val loss: 1.7056125402450562
Epoch 220, training loss: 66.23381042480469 = 1.668321967124939 + 10.0 * 6.456549167633057
Epoch 220, val loss: 1.6923696994781494
Epoch 230, training loss: 66.13611602783203 = 1.6508307456970215 + 10.0 * 6.448528289794922
Epoch 230, val loss: 1.6781052350997925
Epoch 240, training loss: 65.98329162597656 = 1.6319353580474854 + 10.0 * 6.435135841369629
Epoch 240, val loss: 1.6627107858657837
Epoch 250, training loss: 65.87096405029297 = 1.611711859703064 + 10.0 * 6.425925254821777
Epoch 250, val loss: 1.646252155303955
Epoch 260, training loss: 65.75769805908203 = 1.5901321172714233 + 10.0 * 6.4167561531066895
Epoch 260, val loss: 1.6287190914154053
Epoch 270, training loss: 65.72392272949219 = 1.5671422481536865 + 10.0 * 6.415678024291992
Epoch 270, val loss: 1.6099351644515991
Epoch 280, training loss: 65.56945037841797 = 1.5428818464279175 + 10.0 * 6.402656555175781
Epoch 280, val loss: 1.5902131795883179
Epoch 290, training loss: 65.45832824707031 = 1.5177522897720337 + 10.0 * 6.394057273864746
Epoch 290, val loss: 1.5696402788162231
Epoch 300, training loss: 65.36060333251953 = 1.491729497909546 + 10.0 * 6.386887550354004
Epoch 300, val loss: 1.5484338998794556
Epoch 310, training loss: 65.28927612304688 = 1.464816689491272 + 10.0 * 6.382445812225342
Epoch 310, val loss: 1.5265462398529053
Epoch 320, training loss: 65.19509887695312 = 1.4372400045394897 + 10.0 * 6.375785827636719
Epoch 320, val loss: 1.5040559768676758
Epoch 330, training loss: 65.11893463134766 = 1.4092777967453003 + 10.0 * 6.370965957641602
Epoch 330, val loss: 1.4814436435699463
Epoch 340, training loss: 65.04593658447266 = 1.3808962106704712 + 10.0 * 6.366503715515137
Epoch 340, val loss: 1.4586023092269897
Epoch 350, training loss: 64.98766326904297 = 1.3521865606307983 + 10.0 * 6.363547325134277
Epoch 350, val loss: 1.435746669769287
Epoch 360, training loss: 64.90147399902344 = 1.323256492614746 + 10.0 * 6.357821464538574
Epoch 360, val loss: 1.4127432107925415
Epoch 370, training loss: 64.8371810913086 = 1.2943084239959717 + 10.0 * 6.354287147521973
Epoch 370, val loss: 1.390093445777893
Epoch 380, training loss: 64.79300689697266 = 1.2654225826263428 + 10.0 * 6.352758884429932
Epoch 380, val loss: 1.3678950071334839
Epoch 390, training loss: 64.70342254638672 = 1.2365810871124268 + 10.0 * 6.346683979034424
Epoch 390, val loss: 1.346068263053894
Epoch 400, training loss: 64.64046478271484 = 1.208034873008728 + 10.0 * 6.343242645263672
Epoch 400, val loss: 1.3247430324554443
Epoch 410, training loss: 64.59959411621094 = 1.1796822547912598 + 10.0 * 6.341990947723389
Epoch 410, val loss: 1.303898572921753
Epoch 420, training loss: 64.54928588867188 = 1.1515800952911377 + 10.0 * 6.339770793914795
Epoch 420, val loss: 1.2836719751358032
Epoch 430, training loss: 64.46369171142578 = 1.1238415241241455 + 10.0 * 6.333984851837158
Epoch 430, val loss: 1.2640964984893799
Epoch 440, training loss: 64.41072082519531 = 1.096570611000061 + 10.0 * 6.331415176391602
Epoch 440, val loss: 1.2452021837234497
Epoch 450, training loss: 64.37969970703125 = 1.069717288017273 + 10.0 * 6.330998420715332
Epoch 450, val loss: 1.2269213199615479
Epoch 460, training loss: 64.30870056152344 = 1.0431936979293823 + 10.0 * 6.3265509605407715
Epoch 460, val loss: 1.2089920043945312
Epoch 470, training loss: 64.24861907958984 = 1.0172609090805054 + 10.0 * 6.323135852813721
Epoch 470, val loss: 1.1919026374816895
Epoch 480, training loss: 64.20024108886719 = 0.9919078350067139 + 10.0 * 6.320833206176758
Epoch 480, val loss: 1.1754645109176636
Epoch 490, training loss: 64.18183898925781 = 0.9670148491859436 + 10.0 * 6.321482181549072
Epoch 490, val loss: 1.159541130065918
Epoch 500, training loss: 64.1283950805664 = 0.9425888657569885 + 10.0 * 6.318580627441406
Epoch 500, val loss: 1.1440660953521729
Epoch 510, training loss: 64.07084655761719 = 0.9188697338104248 + 10.0 * 6.315197467803955
Epoch 510, val loss: 1.1293635368347168
Epoch 520, training loss: 64.01539611816406 = 0.8957781195640564 + 10.0 * 6.311961650848389
Epoch 520, val loss: 1.1152191162109375
Epoch 530, training loss: 63.97663116455078 = 0.873379647731781 + 10.0 * 6.3103251457214355
Epoch 530, val loss: 1.1018661260604858
Epoch 540, training loss: 63.98486328125 = 0.8515050411224365 + 10.0 * 6.31333589553833
Epoch 540, val loss: 1.0889955759048462
Epoch 550, training loss: 63.90528869628906 = 0.8301706910133362 + 10.0 * 6.307511806488037
Epoch 550, val loss: 1.076616883277893
Epoch 560, training loss: 63.85380554199219 = 0.809542179107666 + 10.0 * 6.304426193237305
Epoch 560, val loss: 1.0650136470794678
Epoch 570, training loss: 63.84107208251953 = 0.7894805073738098 + 10.0 * 6.305159091949463
Epoch 570, val loss: 1.0539977550506592
Epoch 580, training loss: 63.809226989746094 = 0.7699853181838989 + 10.0 * 6.303924083709717
Epoch 580, val loss: 1.0433465242385864
Epoch 590, training loss: 63.76475143432617 = 0.7510850429534912 + 10.0 * 6.301366806030273
Epoch 590, val loss: 1.0335367918014526
Epoch 600, training loss: 63.709510803222656 = 0.7327685952186584 + 10.0 * 6.297674179077148
Epoch 600, val loss: 1.0243016481399536
Epoch 610, training loss: 63.68119812011719 = 0.7150464057922363 + 10.0 * 6.296615123748779
Epoch 610, val loss: 1.0155549049377441
Epoch 620, training loss: 63.66700744628906 = 0.6978077292442322 + 10.0 * 6.296919822692871
Epoch 620, val loss: 1.0074071884155273
Epoch 630, training loss: 63.65583419799805 = 0.6809852719306946 + 10.0 * 6.297484874725342
Epoch 630, val loss: 0.999554455280304
Epoch 640, training loss: 63.591583251953125 = 0.6646655797958374 + 10.0 * 6.292691707611084
Epoch 640, val loss: 0.9921512007713318
Epoch 650, training loss: 63.55988693237305 = 0.6488529443740845 + 10.0 * 6.291103363037109
Epoch 650, val loss: 0.9854088425636292
Epoch 660, training loss: 63.54728698730469 = 0.6334720849990845 + 10.0 * 6.291381359100342
Epoch 660, val loss: 0.9789811968803406
Epoch 670, training loss: 63.50724792480469 = 0.6185715198516846 + 10.0 * 6.288867473602295
Epoch 670, val loss: 0.9730054140090942
Epoch 680, training loss: 63.47584533691406 = 0.6040222644805908 + 10.0 * 6.287182331085205
Epoch 680, val loss: 0.9674170613288879
Epoch 690, training loss: 63.43952941894531 = 0.5898661017417908 + 10.0 * 6.284966468811035
Epoch 690, val loss: 0.9621151685714722
Epoch 700, training loss: 63.43907165527344 = 0.5761008262634277 + 10.0 * 6.28629732131958
Epoch 700, val loss: 0.9571951627731323
Epoch 710, training loss: 63.40943908691406 = 0.5626096725463867 + 10.0 * 6.284682750701904
Epoch 710, val loss: 0.9526973962783813
Epoch 720, training loss: 63.377288818359375 = 0.5493703484535217 + 10.0 * 6.282792091369629
Epoch 720, val loss: 0.9482476115226746
Epoch 730, training loss: 63.34603500366211 = 0.5364879965782166 + 10.0 * 6.280954837799072
Epoch 730, val loss: 0.9443296790122986
Epoch 740, training loss: 63.31067657470703 = 0.5239399075508118 + 10.0 * 6.2786736488342285
Epoch 740, val loss: 0.940789520740509
Epoch 750, training loss: 63.2953987121582 = 0.5117330551147461 + 10.0 * 6.278366565704346
Epoch 750, val loss: 0.9375365376472473
Epoch 760, training loss: 63.269737243652344 = 0.4998047649860382 + 10.0 * 6.276993274688721
Epoch 760, val loss: 0.934510350227356
Epoch 770, training loss: 63.267333984375 = 0.4881047308444977 + 10.0 * 6.277922630310059
Epoch 770, val loss: 0.9316600561141968
Epoch 780, training loss: 63.22343444824219 = 0.47673138976097107 + 10.0 * 6.274670600891113
Epoch 780, val loss: 0.9291201233863831
Epoch 790, training loss: 63.217403411865234 = 0.46562695503234863 + 10.0 * 6.275177955627441
Epoch 790, val loss: 0.9268987774848938
Epoch 800, training loss: 63.19886016845703 = 0.4547266364097595 + 10.0 * 6.274413108825684
Epoch 800, val loss: 0.9247681498527527
Epoch 810, training loss: 63.15776062011719 = 0.4440237581729889 + 10.0 * 6.271373748779297
Epoch 810, val loss: 0.9229013323783875
Epoch 820, training loss: 63.14182662963867 = 0.4335906207561493 + 10.0 * 6.2708234786987305
Epoch 820, val loss: 0.921430766582489
Epoch 830, training loss: 63.20866775512695 = 0.42336559295654297 + 10.0 * 6.278530120849609
Epoch 830, val loss: 0.919996440410614
Epoch 840, training loss: 63.09596633911133 = 0.41323333978652954 + 10.0 * 6.26827335357666
Epoch 840, val loss: 0.9186980128288269
Epoch 850, training loss: 63.084510803222656 = 0.4034360647201538 + 10.0 * 6.2681074142456055
Epoch 850, val loss: 0.917782723903656
Epoch 860, training loss: 63.058345794677734 = 0.3938535153865814 + 10.0 * 6.266449451446533
Epoch 860, val loss: 0.9170942306518555
Epoch 870, training loss: 63.1092529296875 = 0.3844442367553711 + 10.0 * 6.2724809646606445
Epoch 870, val loss: 0.9165703654289246
Epoch 880, training loss: 63.05437088012695 = 0.3750656545162201 + 10.0 * 6.267930507659912
Epoch 880, val loss: 0.9159278869628906
Epoch 890, training loss: 63.014739990234375 = 0.3659708499908447 + 10.0 * 6.264876842498779
Epoch 890, val loss: 0.9157991409301758
Epoch 900, training loss: 63.014827728271484 = 0.3570373058319092 + 10.0 * 6.2657790184021
Epoch 900, val loss: 0.915775716304779
Epoch 910, training loss: 62.98747634887695 = 0.34824633598327637 + 10.0 * 6.263922691345215
Epoch 910, val loss: 0.9160112738609314
Epoch 920, training loss: 62.983821868896484 = 0.33961862325668335 + 10.0 * 6.264420509338379
Epoch 920, val loss: 0.9163889288902283
Epoch 930, training loss: 62.94749450683594 = 0.3311316967010498 + 10.0 * 6.261636257171631
Epoch 930, val loss: 0.9170586466789246
Epoch 940, training loss: 62.924129486083984 = 0.32279491424560547 + 10.0 * 6.260133266448975
Epoch 940, val loss: 0.9178668260574341
Epoch 950, training loss: 62.952980041503906 = 0.31458696722984314 + 10.0 * 6.263839244842529
Epoch 950, val loss: 0.9187943935394287
Epoch 960, training loss: 62.91831970214844 = 0.3064417839050293 + 10.0 * 6.261187553405762
Epoch 960, val loss: 0.9197555184364319
Epoch 970, training loss: 62.88362503051758 = 0.29843422770500183 + 10.0 * 6.258519172668457
Epoch 970, val loss: 0.9210299849510193
Epoch 980, training loss: 62.87241744995117 = 0.2905777096748352 + 10.0 * 6.25818395614624
Epoch 980, val loss: 0.9224807620048523
Epoch 990, training loss: 62.89380645751953 = 0.28280770778656006 + 10.0 * 6.261099815368652
Epoch 990, val loss: 0.9239940047264099
Epoch 1000, training loss: 62.83775329589844 = 0.2751624286174774 + 10.0 * 6.256258964538574
Epoch 1000, val loss: 0.9255785346031189
Epoch 1010, training loss: 62.822357177734375 = 0.26767414808273315 + 10.0 * 6.255468368530273
Epoch 1010, val loss: 0.9275513291358948
Epoch 1020, training loss: 62.805023193359375 = 0.26030686497688293 + 10.0 * 6.254471778869629
Epoch 1020, val loss: 0.929703950881958
Epoch 1030, training loss: 62.796287536621094 = 0.25306829810142517 + 10.0 * 6.254322052001953
Epoch 1030, val loss: 0.9320183396339417
Epoch 1040, training loss: 62.83095169067383 = 0.24591724574565887 + 10.0 * 6.258503437042236
Epoch 1040, val loss: 0.9344443678855896
Epoch 1050, training loss: 62.82267379760742 = 0.2388809621334076 + 10.0 * 6.2583794593811035
Epoch 1050, val loss: 0.9367355108261108
Epoch 1060, training loss: 62.75 = 0.23195742070674896 + 10.0 * 6.251804351806641
Epoch 1060, val loss: 0.9393801093101501
Epoch 1070, training loss: 62.744144439697266 = 0.2252461463212967 + 10.0 * 6.251889705657959
Epoch 1070, val loss: 0.9422850012779236
Epoch 1080, training loss: 62.726078033447266 = 0.21870875358581543 + 10.0 * 6.250737190246582
Epoch 1080, val loss: 0.9453672766685486
Epoch 1090, training loss: 62.7488899230957 = 0.21229927241802216 + 10.0 * 6.253659248352051
Epoch 1090, val loss: 0.948577880859375
Epoch 1100, training loss: 62.7033805847168 = 0.20602641999721527 + 10.0 * 6.249735355377197
Epoch 1100, val loss: 0.9518057107925415
Epoch 1110, training loss: 62.691558837890625 = 0.19989754259586334 + 10.0 * 6.249166011810303
Epoch 1110, val loss: 0.9554181098937988
Epoch 1120, training loss: 62.69023513793945 = 0.19396406412124634 + 10.0 * 6.249627113342285
Epoch 1120, val loss: 0.9591916799545288
Epoch 1130, training loss: 62.69496154785156 = 0.1881776601076126 + 10.0 * 6.250678062438965
Epoch 1130, val loss: 0.9630587100982666
Epoch 1140, training loss: 62.6827392578125 = 0.18256570398807526 + 10.0 * 6.250017166137695
Epoch 1140, val loss: 0.9669339656829834
Epoch 1150, training loss: 62.65915298461914 = 0.1771024912595749 + 10.0 * 6.248205184936523
Epoch 1150, val loss: 0.9710051417350769
Epoch 1160, training loss: 62.64065170288086 = 0.17180399596691132 + 10.0 * 6.246884822845459
Epoch 1160, val loss: 0.9754490852355957
Epoch 1170, training loss: 62.6254768371582 = 0.16669246554374695 + 10.0 * 6.24587869644165
Epoch 1170, val loss: 0.9800146222114563
Epoch 1180, training loss: 62.61357879638672 = 0.1617390662431717 + 10.0 * 6.245183944702148
Epoch 1180, val loss: 0.9847196340560913
Epoch 1190, training loss: 62.67333221435547 = 0.15693353116512299 + 10.0 * 6.2516398429870605
Epoch 1190, val loss: 0.9893808960914612
Epoch 1200, training loss: 62.62921905517578 = 0.1522546261548996 + 10.0 * 6.247696399688721
Epoch 1200, val loss: 0.9943417906761169
Epoch 1210, training loss: 62.62805938720703 = 0.14772117137908936 + 10.0 * 6.2480340003967285
Epoch 1210, val loss: 0.9992027878761292
Epoch 1220, training loss: 62.59162902832031 = 0.14335455000400543 + 10.0 * 6.2448272705078125
Epoch 1220, val loss: 1.0042716264724731
Epoch 1230, training loss: 62.57099533081055 = 0.13914264738559723 + 10.0 * 6.243185520172119
Epoch 1230, val loss: 1.0095438957214355
Epoch 1240, training loss: 62.564632415771484 = 0.13508833944797516 + 10.0 * 6.242954254150391
Epoch 1240, val loss: 1.0149468183517456
Epoch 1250, training loss: 62.581825256347656 = 0.13117176294326782 + 10.0 * 6.245065212249756
Epoch 1250, val loss: 1.0203571319580078
Epoch 1260, training loss: 62.56890869140625 = 0.12737339735031128 + 10.0 * 6.2441534996032715
Epoch 1260, val loss: 1.025820016860962
Epoch 1270, training loss: 62.56077575683594 = 0.1236991211771965 + 10.0 * 6.243707656860352
Epoch 1270, val loss: 1.0311957597732544
Epoch 1280, training loss: 62.535003662109375 = 0.12014094740152359 + 10.0 * 6.241486549377441
Epoch 1280, val loss: 1.0368489027023315
Epoch 1290, training loss: 62.51839828491211 = 0.11674226820468903 + 10.0 * 6.240165710449219
Epoch 1290, val loss: 1.042526364326477
Epoch 1300, training loss: 62.51780700683594 = 0.11346250772476196 + 10.0 * 6.240434169769287
Epoch 1300, val loss: 1.0482525825500488
Epoch 1310, training loss: 62.53202819824219 = 0.11028285324573517 + 10.0 * 6.2421746253967285
Epoch 1310, val loss: 1.0539411306381226
Epoch 1320, training loss: 62.50907516479492 = 0.10718227177858353 + 10.0 * 6.240189552307129
Epoch 1320, val loss: 1.059653878211975
Epoch 1330, training loss: 62.508182525634766 = 0.10420695692300797 + 10.0 * 6.2403974533081055
Epoch 1330, val loss: 1.0654559135437012
Epoch 1340, training loss: 62.49845504760742 = 0.10134478658437729 + 10.0 * 6.239710807800293
Epoch 1340, val loss: 1.0711711645126343
Epoch 1350, training loss: 62.49332046508789 = 0.09857688844203949 + 10.0 * 6.239474296569824
Epoch 1350, val loss: 1.077047348022461
Epoch 1360, training loss: 62.47380828857422 = 0.09588582068681717 + 10.0 * 6.237792015075684
Epoch 1360, val loss: 1.082924246788025
Epoch 1370, training loss: 62.47539520263672 = 0.09330898523330688 + 10.0 * 6.238208770751953
Epoch 1370, val loss: 1.0888400077819824
Epoch 1380, training loss: 62.49385452270508 = 0.09080682694911957 + 10.0 * 6.240304946899414
Epoch 1380, val loss: 1.094690203666687
Epoch 1390, training loss: 62.47715377807617 = 0.08834385126829147 + 10.0 * 6.2388811111450195
Epoch 1390, val loss: 1.100588083267212
Epoch 1400, training loss: 62.45499038696289 = 0.0860086977481842 + 10.0 * 6.236897945404053
Epoch 1400, val loss: 1.1064813137054443
Epoch 1410, training loss: 62.51778030395508 = 0.08373264223337173 + 10.0 * 6.243404865264893
Epoch 1410, val loss: 1.1123449802398682
Epoch 1420, training loss: 62.46650314331055 = 0.0815223827958107 + 10.0 * 6.238497734069824
Epoch 1420, val loss: 1.1184378862380981
Epoch 1430, training loss: 62.436180114746094 = 0.07938148081302643 + 10.0 * 6.235680103302002
Epoch 1430, val loss: 1.124277949333191
Epoch 1440, training loss: 62.42255401611328 = 0.07733509689569473 + 10.0 * 6.234521865844727
Epoch 1440, val loss: 1.130475640296936
Epoch 1450, training loss: 62.43366622924805 = 0.07535319775342941 + 10.0 * 6.235831260681152
Epoch 1450, val loss: 1.1365083456039429
Epoch 1460, training loss: 62.421539306640625 = 0.07341969758272171 + 10.0 * 6.234811782836914
Epoch 1460, val loss: 1.1424663066864014
Epoch 1470, training loss: 62.422000885009766 = 0.07153413444757462 + 10.0 * 6.235046863555908
Epoch 1470, val loss: 1.1484949588775635
Epoch 1480, training loss: 62.450233459472656 = 0.06972958147525787 + 10.0 * 6.23805046081543
Epoch 1480, val loss: 1.1543617248535156
Epoch 1490, training loss: 62.406410217285156 = 0.06793934106826782 + 10.0 * 6.233847141265869
Epoch 1490, val loss: 1.1603797674179077
Epoch 1500, training loss: 62.39226150512695 = 0.06624224781990051 + 10.0 * 6.232602119445801
Epoch 1500, val loss: 1.1663480997085571
Epoch 1510, training loss: 62.382930755615234 = 0.06460129469633102 + 10.0 * 6.231832981109619
Epoch 1510, val loss: 1.1724406480789185
Epoch 1520, training loss: 62.39613723754883 = 0.06301222741603851 + 10.0 * 6.233312606811523
Epoch 1520, val loss: 1.178442358970642
Epoch 1530, training loss: 62.39985275268555 = 0.061442334204912186 + 10.0 * 6.2338409423828125
Epoch 1530, val loss: 1.1841002702713013
Epoch 1540, training loss: 62.386314392089844 = 0.059927597641944885 + 10.0 * 6.232638359069824
Epoch 1540, val loss: 1.1899508237838745
Epoch 1550, training loss: 62.369407653808594 = 0.058460090309381485 + 10.0 * 6.231094837188721
Epoch 1550, val loss: 1.1958118677139282
Epoch 1560, training loss: 62.372982025146484 = 0.05705909803509712 + 10.0 * 6.231592178344727
Epoch 1560, val loss: 1.2017608880996704
Epoch 1570, training loss: 62.38112258911133 = 0.05568942800164223 + 10.0 * 6.232542991638184
Epoch 1570, val loss: 1.2075213193893433
Epoch 1580, training loss: 62.369014739990234 = 0.05434677004814148 + 10.0 * 6.231466770172119
Epoch 1580, val loss: 1.2133677005767822
Epoch 1590, training loss: 62.40215301513672 = 0.053065456449985504 + 10.0 * 6.234908580780029
Epoch 1590, val loss: 1.2191764116287231
Epoch 1600, training loss: 62.355316162109375 = 0.051795125007629395 + 10.0 * 6.23035192489624
Epoch 1600, val loss: 1.2246760129928589
Epoch 1610, training loss: 62.347740173339844 = 0.050583239644765854 + 10.0 * 6.229715824127197
Epoch 1610, val loss: 1.2304515838623047
Epoch 1620, training loss: 62.3724479675293 = 0.04941272363066673 + 10.0 * 6.232303619384766
Epoch 1620, val loss: 1.2359646558761597
Epoch 1630, training loss: 62.36056900024414 = 0.04828206077218056 + 10.0 * 6.231228828430176
Epoch 1630, val loss: 1.241450309753418
Epoch 1640, training loss: 62.33491897583008 = 0.04716780036687851 + 10.0 * 6.2287750244140625
Epoch 1640, val loss: 1.2469724416732788
Epoch 1650, training loss: 62.32326889038086 = 0.04610035568475723 + 10.0 * 6.22771692276001
Epoch 1650, val loss: 1.2524522542953491
Epoch 1660, training loss: 62.335872650146484 = 0.045072831213474274 + 10.0 * 6.229079723358154
Epoch 1660, val loss: 1.2578723430633545
Epoch 1670, training loss: 62.37417984008789 = 0.04406459257006645 + 10.0 * 6.233011722564697
Epoch 1670, val loss: 1.2630863189697266
Epoch 1680, training loss: 62.32742691040039 = 0.043093230575323105 + 10.0 * 6.228433132171631
Epoch 1680, val loss: 1.2682132720947266
Epoch 1690, training loss: 62.30939483642578 = 0.042137499898672104 + 10.0 * 6.2267255783081055
Epoch 1690, val loss: 1.2735099792480469
Epoch 1700, training loss: 62.30594253540039 = 0.041228849440813065 + 10.0 * 6.226471424102783
Epoch 1700, val loss: 1.2788622379302979
Epoch 1710, training loss: 62.325286865234375 = 0.04035218060016632 + 10.0 * 6.228493690490723
Epoch 1710, val loss: 1.2839609384536743
Epoch 1720, training loss: 62.3132209777832 = 0.039488933980464935 + 10.0 * 6.227373123168945
Epoch 1720, val loss: 1.289115309715271
Epoch 1730, training loss: 62.305877685546875 = 0.03865688666701317 + 10.0 * 6.22672176361084
Epoch 1730, val loss: 1.2941815853118896
Epoch 1740, training loss: 62.29182434082031 = 0.03784285485744476 + 10.0 * 6.225398063659668
Epoch 1740, val loss: 1.2994076013565063
Epoch 1750, training loss: 62.29209899902344 = 0.03707238659262657 + 10.0 * 6.225502967834473
Epoch 1750, val loss: 1.3045207262039185
Epoch 1760, training loss: 62.319976806640625 = 0.03632397949695587 + 10.0 * 6.228365421295166
Epoch 1760, val loss: 1.3094784021377563
Epoch 1770, training loss: 62.29014587402344 = 0.03557879477739334 + 10.0 * 6.225456714630127
Epoch 1770, val loss: 1.3143811225891113
Epoch 1780, training loss: 62.29190444946289 = 0.034862905740737915 + 10.0 * 6.225704193115234
Epoch 1780, val loss: 1.319216251373291
Epoch 1790, training loss: 62.308494567871094 = 0.034169986844062805 + 10.0 * 6.227432727813721
Epoch 1790, val loss: 1.3240798711776733
Epoch 1800, training loss: 62.30575180053711 = 0.03348377346992493 + 10.0 * 6.227226734161377
Epoch 1800, val loss: 1.328779697418213
Epoch 1810, training loss: 62.28463363647461 = 0.03282131999731064 + 10.0 * 6.2251811027526855
Epoch 1810, val loss: 1.333667278289795
Epoch 1820, training loss: 62.26857376098633 = 0.03218517825007439 + 10.0 * 6.223639011383057
Epoch 1820, val loss: 1.3383694887161255
Epoch 1830, training loss: 62.26161193847656 = 0.03157481178641319 + 10.0 * 6.22300386428833
Epoch 1830, val loss: 1.3432444334030151
Epoch 1840, training loss: 62.30473327636719 = 0.030986210331320763 + 10.0 * 6.22737455368042
Epoch 1840, val loss: 1.3478845357894897
Epoch 1850, training loss: 62.26759338378906 = 0.030391227453947067 + 10.0 * 6.223720073699951
Epoch 1850, val loss: 1.352373719215393
Epoch 1860, training loss: 62.25933837890625 = 0.02982070855796337 + 10.0 * 6.222951889038086
Epoch 1860, val loss: 1.3570590019226074
Epoch 1870, training loss: 62.257328033447266 = 0.02927202172577381 + 10.0 * 6.222805976867676
Epoch 1870, val loss: 1.3616784811019897
Epoch 1880, training loss: 62.30429458618164 = 0.02874763496220112 + 10.0 * 6.227554798126221
Epoch 1880, val loss: 1.3661742210388184
Epoch 1890, training loss: 62.26521682739258 = 0.028207926079630852 + 10.0 * 6.223700523376465
Epoch 1890, val loss: 1.3704801797866821
Epoch 1900, training loss: 62.25191116333008 = 0.02769932709634304 + 10.0 * 6.222421169281006
Epoch 1900, val loss: 1.3749655485153198
Epoch 1910, training loss: 62.24414825439453 = 0.02720571868121624 + 10.0 * 6.221693992614746
Epoch 1910, val loss: 1.3794958591461182
Epoch 1920, training loss: 62.26918411254883 = 0.026730909943580627 + 10.0 * 6.224245548248291
Epoch 1920, val loss: 1.3839157819747925
Epoch 1930, training loss: 62.23809814453125 = 0.02625245600938797 + 10.0 * 6.221184730529785
Epoch 1930, val loss: 1.3883442878723145
Epoch 1940, training loss: 62.256221771240234 = 0.02579795941710472 + 10.0 * 6.2230424880981445
Epoch 1940, val loss: 1.3926340341567993
Epoch 1950, training loss: 62.24789047241211 = 0.025350166484713554 + 10.0 * 6.222254276275635
Epoch 1950, val loss: 1.3968876600265503
Epoch 1960, training loss: 62.25885009765625 = 0.024916887283325195 + 10.0 * 6.223393440246582
Epoch 1960, val loss: 1.4011423587799072
Epoch 1970, training loss: 62.22858428955078 = 0.02448282577097416 + 10.0 * 6.220410346984863
Epoch 1970, val loss: 1.4054906368255615
Epoch 1980, training loss: 62.22252655029297 = 0.02407621592283249 + 10.0 * 6.219845294952393
Epoch 1980, val loss: 1.4098104238510132
Epoch 1990, training loss: 62.25953674316406 = 0.02368428371846676 + 10.0 * 6.22358512878418
Epoch 1990, val loss: 1.4141616821289062
Epoch 2000, training loss: 62.22890853881836 = 0.02328057773411274 + 10.0 * 6.220562934875488
Epoch 2000, val loss: 1.417860507965088
Epoch 2010, training loss: 62.2313346862793 = 0.022892503067851067 + 10.0 * 6.220844268798828
Epoch 2010, val loss: 1.4221173524856567
Epoch 2020, training loss: 62.21662139892578 = 0.02251746505498886 + 10.0 * 6.219410419464111
Epoch 2020, val loss: 1.4261713027954102
Epoch 2030, training loss: 62.213565826416016 = 0.022157644852995872 + 10.0 * 6.219141006469727
Epoch 2030, val loss: 1.4303375482559204
Epoch 2040, training loss: 62.24237823486328 = 0.021807465702295303 + 10.0 * 6.222056865692139
Epoch 2040, val loss: 1.4343926906585693
Epoch 2050, training loss: 62.21420669555664 = 0.021456869319081306 + 10.0 * 6.219274997711182
Epoch 2050, val loss: 1.4383474588394165
Epoch 2060, training loss: 62.244197845458984 = 0.02112145535647869 + 10.0 * 6.2223076820373535
Epoch 2060, val loss: 1.442213535308838
Epoch 2070, training loss: 62.20825958251953 = 0.02077387645840645 + 10.0 * 6.218748569488525
Epoch 2070, val loss: 1.446221947669983
Epoch 2080, training loss: 62.19523239135742 = 0.020454898476600647 + 10.0 * 6.217477798461914
Epoch 2080, val loss: 1.450251817703247
Epoch 2090, training loss: 62.20723342895508 = 0.020143810659646988 + 10.0 * 6.2187089920043945
Epoch 2090, val loss: 1.4542049169540405
Epoch 2100, training loss: 62.23506164550781 = 0.019834669306874275 + 10.0 * 6.221522331237793
Epoch 2100, val loss: 1.4579614400863647
Epoch 2110, training loss: 62.20578384399414 = 0.01952524483203888 + 10.0 * 6.218626022338867
Epoch 2110, val loss: 1.461814045906067
Epoch 2120, training loss: 62.19174575805664 = 0.01923312060534954 + 10.0 * 6.217251300811768
Epoch 2120, val loss: 1.4657707214355469
Epoch 2130, training loss: 62.271053314208984 = 0.018952801823616028 + 10.0 * 6.225210189819336
Epoch 2130, val loss: 1.4694551229476929
Epoch 2140, training loss: 62.21182632446289 = 0.018658148124814034 + 10.0 * 6.2193169593811035
Epoch 2140, val loss: 1.4730726480484009
Epoch 2150, training loss: 62.19187545776367 = 0.018382161855697632 + 10.0 * 6.217349052429199
Epoch 2150, val loss: 1.4769450426101685
Epoch 2160, training loss: 62.18729782104492 = 0.018115192651748657 + 10.0 * 6.216917991638184
Epoch 2160, val loss: 1.4806289672851562
Epoch 2170, training loss: 62.20711135864258 = 0.017854372039437294 + 10.0 * 6.218925952911377
Epoch 2170, val loss: 1.4843883514404297
Epoch 2180, training loss: 62.1892204284668 = 0.01759457029402256 + 10.0 * 6.217162609100342
Epoch 2180, val loss: 1.487793207168579
Epoch 2190, training loss: 62.17721939086914 = 0.017342954874038696 + 10.0 * 6.215987682342529
Epoch 2190, val loss: 1.491513967514038
Epoch 2200, training loss: 62.20186233520508 = 0.017100071534514427 + 10.0 * 6.218476295471191
Epoch 2200, val loss: 1.4952195882797241
Epoch 2210, training loss: 62.17849349975586 = 0.01685377210378647 + 10.0 * 6.2161641120910645
Epoch 2210, val loss: 1.4986613988876343
Epoch 2220, training loss: 62.173728942871094 = 0.016615433618426323 + 10.0 * 6.2157111167907715
Epoch 2220, val loss: 1.5022565126419067
Epoch 2230, training loss: 62.186527252197266 = 0.016387538984417915 + 10.0 * 6.217013835906982
Epoch 2230, val loss: 1.5058497190475464
Epoch 2240, training loss: 62.17454528808594 = 0.016156304627656937 + 10.0 * 6.21583890914917
Epoch 2240, val loss: 1.5091673135757446
Epoch 2250, training loss: 62.172733306884766 = 0.01593359000980854 + 10.0 * 6.215680122375488
Epoch 2250, val loss: 1.5125815868377686
Epoch 2260, training loss: 62.17355728149414 = 0.015715181827545166 + 10.0 * 6.215784072875977
Epoch 2260, val loss: 1.516110897064209
Epoch 2270, training loss: 62.1832389831543 = 0.015502013266086578 + 10.0 * 6.216773509979248
Epoch 2270, val loss: 1.5195280313491821
Epoch 2280, training loss: 62.19062042236328 = 0.015291806310415268 + 10.0 * 6.217532634735107
Epoch 2280, val loss: 1.5228676795959473
Epoch 2290, training loss: 62.15766906738281 = 0.015087515115737915 + 10.0 * 6.214258193969727
Epoch 2290, val loss: 1.526137113571167
Epoch 2300, training loss: 62.15084457397461 = 0.014885878190398216 + 10.0 * 6.213595867156982
Epoch 2300, val loss: 1.5295677185058594
Epoch 2310, training loss: 62.146305084228516 = 0.0146937295794487 + 10.0 * 6.213160991668701
Epoch 2310, val loss: 1.533023476600647
Epoch 2320, training loss: 62.17892837524414 = 0.014506937935948372 + 10.0 * 6.216442108154297
Epoch 2320, val loss: 1.5364515781402588
Epoch 2330, training loss: 62.16057586669922 = 0.014315237291157246 + 10.0 * 6.214625835418701
Epoch 2330, val loss: 1.5393755435943604
Epoch 2340, training loss: 62.148494720458984 = 0.01412553247064352 + 10.0 * 6.213437080383301
Epoch 2340, val loss: 1.5425598621368408
Epoch 2350, training loss: 62.154083251953125 = 0.013943607918918133 + 10.0 * 6.214014053344727
Epoch 2350, val loss: 1.5457324981689453
Epoch 2360, training loss: 62.17211151123047 = 0.013763630762696266 + 10.0 * 6.215834617614746
Epoch 2360, val loss: 1.5489941835403442
Epoch 2370, training loss: 62.13933181762695 = 0.013589142821729183 + 10.0 * 6.212574481964111
Epoch 2370, val loss: 1.5520099401474
Epoch 2380, training loss: 62.134220123291016 = 0.01341873500496149 + 10.0 * 6.212080001831055
Epoch 2380, val loss: 1.5552003383636475
Epoch 2390, training loss: 62.15724563598633 = 0.013253872282803059 + 10.0 * 6.214399337768555
Epoch 2390, val loss: 1.5583248138427734
Epoch 2400, training loss: 62.142642974853516 = 0.013088090345263481 + 10.0 * 6.212955474853516
Epoch 2400, val loss: 1.5612455606460571
Epoch 2410, training loss: 62.127159118652344 = 0.012924518436193466 + 10.0 * 6.211423397064209
Epoch 2410, val loss: 1.5642582178115845
Epoch 2420, training loss: 62.12671661376953 = 0.012766111642122269 + 10.0 * 6.211394786834717
Epoch 2420, val loss: 1.5673459768295288
Epoch 2430, training loss: 62.1348991394043 = 0.01261394564062357 + 10.0 * 6.212228298187256
Epoch 2430, val loss: 1.5703840255737305
Epoch 2440, training loss: 62.20199203491211 = 0.012461446225643158 + 10.0 * 6.2189531326293945
Epoch 2440, val loss: 1.573320984840393
Epoch 2450, training loss: 62.138282775878906 = 0.012305228039622307 + 10.0 * 6.212597846984863
Epoch 2450, val loss: 1.5760833024978638
Epoch 2460, training loss: 62.12018585205078 = 0.012155238538980484 + 10.0 * 6.210803031921387
Epoch 2460, val loss: 1.5790365934371948
Epoch 2470, training loss: 62.118621826171875 = 0.012011295184493065 + 10.0 * 6.210660934448242
Epoch 2470, val loss: 1.5821038484573364
Epoch 2480, training loss: 62.11870574951172 = 0.011873154900968075 + 10.0 * 6.210683345794678
Epoch 2480, val loss: 1.585081934928894
Epoch 2490, training loss: 62.19456481933594 = 0.011737285181879997 + 10.0 * 6.218282699584961
Epoch 2490, val loss: 1.5879584550857544
Epoch 2500, training loss: 62.15571975708008 = 0.011599761433899403 + 10.0 * 6.214411735534668
Epoch 2500, val loss: 1.5906344652175903
Epoch 2510, training loss: 62.12687683105469 = 0.011457500047981739 + 10.0 * 6.211541652679443
Epoch 2510, val loss: 1.5934224128723145
Epoch 2520, training loss: 62.116580963134766 = 0.011328800581395626 + 10.0 * 6.210525035858154
Epoch 2520, val loss: 1.596253752708435
Epoch 2530, training loss: 62.12458419799805 = 0.011201189830899239 + 10.0 * 6.211338520050049
Epoch 2530, val loss: 1.5991334915161133
Epoch 2540, training loss: 62.13271713256836 = 0.011075401678681374 + 10.0 * 6.212164402008057
Epoch 2540, val loss: 1.60188889503479
Epoch 2550, training loss: 62.13024139404297 = 0.0109501788392663 + 10.0 * 6.211928844451904
Epoch 2550, val loss: 1.6047110557556152
Epoch 2560, training loss: 62.11858367919922 = 0.010825812816619873 + 10.0 * 6.210775852203369
Epoch 2560, val loss: 1.6073240041732788
Epoch 2570, training loss: 62.10093307495117 = 0.010704622603952885 + 10.0 * 6.2090229988098145
Epoch 2570, val loss: 1.6099733114242554
Epoch 2580, training loss: 62.10800552368164 = 0.010589182376861572 + 10.0 * 6.209741592407227
Epoch 2580, val loss: 1.6127393245697021
Epoch 2590, training loss: 62.14998245239258 = 0.010474290698766708 + 10.0 * 6.2139506340026855
Epoch 2590, val loss: 1.6153435707092285
Epoch 2600, training loss: 62.1431770324707 = 0.010357488878071308 + 10.0 * 6.213282108306885
Epoch 2600, val loss: 1.6178667545318604
Epoch 2610, training loss: 62.11259460449219 = 0.010244054719805717 + 10.0 * 6.210235118865967
Epoch 2610, val loss: 1.6203593015670776
Epoch 2620, training loss: 62.111236572265625 = 0.010132666677236557 + 10.0 * 6.210110664367676
Epoch 2620, val loss: 1.6229318380355835
Epoch 2630, training loss: 62.11775588989258 = 0.01002658810466528 + 10.0 * 6.21077299118042
Epoch 2630, val loss: 1.625537395477295
Epoch 2640, training loss: 62.09416198730469 = 0.009918414987623692 + 10.0 * 6.2084245681762695
Epoch 2640, val loss: 1.6281157732009888
Epoch 2650, training loss: 62.10703659057617 = 0.009815327823162079 + 10.0 * 6.20972204208374
Epoch 2650, val loss: 1.6307392120361328
Epoch 2660, training loss: 62.11018371582031 = 0.009711513295769691 + 10.0 * 6.210047245025635
Epoch 2660, val loss: 1.633144736289978
Epoch 2670, training loss: 62.097103118896484 = 0.009609309956431389 + 10.0 * 6.208749294281006
Epoch 2670, val loss: 1.6356650590896606
Epoch 2680, training loss: 62.09853744506836 = 0.00951103214174509 + 10.0 * 6.208902835845947
Epoch 2680, val loss: 1.6382033824920654
Epoch 2690, training loss: 62.13832092285156 = 0.009411945939064026 + 10.0 * 6.212891101837158
Epoch 2690, val loss: 1.6407217979431152
Epoch 2700, training loss: 62.09346008300781 = 0.009314363822340965 + 10.0 * 6.208414554595947
Epoch 2700, val loss: 1.6427477598190308
Epoch 2710, training loss: 62.087738037109375 = 0.009218224324285984 + 10.0 * 6.207851886749268
Epoch 2710, val loss: 1.6453204154968262
Epoch 2720, training loss: 62.09078598022461 = 0.009126145392656326 + 10.0 * 6.208166122436523
Epoch 2720, val loss: 1.6476558446884155
Epoch 2730, training loss: 62.100826263427734 = 0.009035998024046421 + 10.0 * 6.209178924560547
Epoch 2730, val loss: 1.6500164270401
Epoch 2740, training loss: 62.10020065307617 = 0.008945374749600887 + 10.0 * 6.209125518798828
Epoch 2740, val loss: 1.6522011756896973
Epoch 2750, training loss: 62.087615966796875 = 0.008855723775923252 + 10.0 * 6.207876205444336
Epoch 2750, val loss: 1.6545138359069824
Epoch 2760, training loss: 62.097434997558594 = 0.008769087493419647 + 10.0 * 6.208866596221924
Epoch 2760, val loss: 1.6568604707717896
Epoch 2770, training loss: 62.095123291015625 = 0.008682897314429283 + 10.0 * 6.208643913269043
Epoch 2770, val loss: 1.6590548753738403
Epoch 2780, training loss: 62.09712219238281 = 0.008595315739512444 + 10.0 * 6.208852767944336
Epoch 2780, val loss: 1.6611284017562866
Epoch 2790, training loss: 62.077911376953125 = 0.008511985652148724 + 10.0 * 6.206940174102783
Epoch 2790, val loss: 1.663456678390503
Epoch 2800, training loss: 62.0765380859375 = 0.00843045674264431 + 10.0 * 6.20681095123291
Epoch 2800, val loss: 1.6656092405319214
Epoch 2810, training loss: 62.080650329589844 = 0.008351041935384274 + 10.0 * 6.207230091094971
Epoch 2810, val loss: 1.6677621603012085
Epoch 2820, training loss: 62.08203125 = 0.008271926082670689 + 10.0 * 6.207376003265381
Epoch 2820, val loss: 1.6698787212371826
Epoch 2830, training loss: 62.08226776123047 = 0.008194668218493462 + 10.0 * 6.207407474517822
Epoch 2830, val loss: 1.6719772815704346
Epoch 2840, training loss: 62.07890701293945 = 0.008117929100990295 + 10.0 * 6.20707893371582
Epoch 2840, val loss: 1.6740835905075073
Epoch 2850, training loss: 62.085205078125 = 0.008041953667998314 + 10.0 * 6.207716464996338
Epoch 2850, val loss: 1.6761529445648193
Epoch 2860, training loss: 62.124454498291016 = 0.007968329824507236 + 10.0 * 6.211648464202881
Epoch 2860, val loss: 1.678308129310608
Epoch 2870, training loss: 62.08156204223633 = 0.007890436798334122 + 10.0 * 6.207367420196533
Epoch 2870, val loss: 1.679844856262207
Epoch 2880, training loss: 62.06193542480469 = 0.007816554978489876 + 10.0 * 6.205411911010742
Epoch 2880, val loss: 1.682026743888855
Epoch 2890, training loss: 62.05754089355469 = 0.007746668998152018 + 10.0 * 6.204979419708252
Epoch 2890, val loss: 1.6840852499008179
Epoch 2900, training loss: 62.06816864013672 = 0.007678665686398745 + 10.0 * 6.206048965454102
Epoch 2900, val loss: 1.6860337257385254
Epoch 2910, training loss: 62.09695053100586 = 0.0076099298894405365 + 10.0 * 6.2089338302612305
Epoch 2910, val loss: 1.6879196166992188
Epoch 2920, training loss: 62.077392578125 = 0.007540085818618536 + 10.0 * 6.206984996795654
Epoch 2920, val loss: 1.6898479461669922
Epoch 2930, training loss: 62.077030181884766 = 0.007472531870007515 + 10.0 * 6.206955909729004
Epoch 2930, val loss: 1.6917364597320557
Epoch 2940, training loss: 62.065765380859375 = 0.0074068657122552395 + 10.0 * 6.205835819244385
Epoch 2940, val loss: 1.693668246269226
Epoch 2950, training loss: 62.07028579711914 = 0.007343380246311426 + 10.0 * 6.206294059753418
Epoch 2950, val loss: 1.6954607963562012
Epoch 2960, training loss: 62.0611457824707 = 0.0072794449515640736 + 10.0 * 6.205386638641357
Epoch 2960, val loss: 1.6973326206207275
Epoch 2970, training loss: 62.0809211730957 = 0.007217141799628735 + 10.0 * 6.207370281219482
Epoch 2970, val loss: 1.6992287635803223
Epoch 2980, training loss: 62.05209732055664 = 0.007153510581701994 + 10.0 * 6.204494476318359
Epoch 2980, val loss: 1.7009891271591187
Epoch 2990, training loss: 62.04610061645508 = 0.007089969236403704 + 10.0 * 6.2039008140563965
Epoch 2990, val loss: 1.702837347984314
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8397469688982605
The final CL Acc:0.73827, 0.03805, The final GNN Acc:0.83781, 0.00237
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9538])
updated graph: torch.Size([2, 10556])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.90174865722656 = 1.9332891702651978 + 10.0 * 8.596845626831055
Epoch 0, val loss: 1.9289706945419312
Epoch 10, training loss: 87.88668060302734 = 1.9240697622299194 + 10.0 * 8.596261024475098
Epoch 10, val loss: 1.9198873043060303
Epoch 20, training loss: 87.83313751220703 = 1.9125627279281616 + 10.0 * 8.592058181762695
Epoch 20, val loss: 1.9082460403442383
Epoch 30, training loss: 87.5262222290039 = 1.8978043794631958 + 10.0 * 8.562841415405273
Epoch 30, val loss: 1.8933881521224976
Epoch 40, training loss: 85.95866394042969 = 1.8807412385940552 + 10.0 * 8.407792091369629
Epoch 40, val loss: 1.876808524131775
Epoch 50, training loss: 80.26943969726562 = 1.862465739250183 + 10.0 * 7.840697765350342
Epoch 50, val loss: 1.8589140176773071
Epoch 60, training loss: 76.04802703857422 = 1.847656488418579 + 10.0 * 7.420037269592285
Epoch 60, val loss: 1.8460346460342407
Epoch 70, training loss: 73.16786193847656 = 1.838370442390442 + 10.0 * 7.132949352264404
Epoch 70, val loss: 1.8371323347091675
Epoch 80, training loss: 71.36136627197266 = 1.8296459913253784 + 10.0 * 6.95317268371582
Epoch 80, val loss: 1.8287174701690674
Epoch 90, training loss: 69.85029602050781 = 1.8215899467468262 + 10.0 * 6.802871227264404
Epoch 90, val loss: 1.8207980394363403
Epoch 100, training loss: 69.00818634033203 = 1.813333511352539 + 10.0 * 6.719485759735107
Epoch 100, val loss: 1.8128010034561157
Epoch 110, training loss: 68.42526245117188 = 1.8051737546920776 + 10.0 * 6.662008285522461
Epoch 110, val loss: 1.8047616481781006
Epoch 120, training loss: 67.94290161132812 = 1.797183871269226 + 10.0 * 6.614572048187256
Epoch 120, val loss: 1.7967946529388428
Epoch 130, training loss: 67.56039428710938 = 1.7895148992538452 + 10.0 * 6.577087879180908
Epoch 130, val loss: 1.788934588432312
Epoch 140, training loss: 67.22830963134766 = 1.7818503379821777 + 10.0 * 6.544645309448242
Epoch 140, val loss: 1.7809932231903076
Epoch 150, training loss: 66.95100402832031 = 1.7738637924194336 + 10.0 * 6.51771354675293
Epoch 150, val loss: 1.7728279829025269
Epoch 160, training loss: 66.73794555664062 = 1.7652711868286133 + 10.0 * 6.497267246246338
Epoch 160, val loss: 1.7643529176712036
Epoch 170, training loss: 66.5538330078125 = 1.755945086479187 + 10.0 * 6.4797892570495605
Epoch 170, val loss: 1.7553770542144775
Epoch 180, training loss: 66.39312744140625 = 1.7457963228225708 + 10.0 * 6.464733123779297
Epoch 180, val loss: 1.7457917928695679
Epoch 190, training loss: 66.25711059570312 = 1.7346307039260864 + 10.0 * 6.452247619628906
Epoch 190, val loss: 1.7354298830032349
Epoch 200, training loss: 66.12537384033203 = 1.7224063873291016 + 10.0 * 6.440296649932861
Epoch 200, val loss: 1.7241801023483276
Epoch 210, training loss: 66.02959442138672 = 1.7089686393737793 + 10.0 * 6.43206262588501
Epoch 210, val loss: 1.7119007110595703
Epoch 220, training loss: 65.93521881103516 = 1.6941066980361938 + 10.0 * 6.4241108894348145
Epoch 220, val loss: 1.6985111236572266
Epoch 230, training loss: 65.81230926513672 = 1.6778887510299683 + 10.0 * 6.413442134857178
Epoch 230, val loss: 1.6839731931686401
Epoch 240, training loss: 65.72100830078125 = 1.6601800918579102 + 10.0 * 6.406083106994629
Epoch 240, val loss: 1.668184518814087
Epoch 250, training loss: 65.67357635498047 = 1.6409257650375366 + 10.0 * 6.403265476226807
Epoch 250, val loss: 1.6510837078094482
Epoch 260, training loss: 65.55500030517578 = 1.6200478076934814 + 10.0 * 6.393495559692383
Epoch 260, val loss: 1.6326361894607544
Epoch 270, training loss: 65.4667739868164 = 1.597628116607666 + 10.0 * 6.3869147300720215
Epoch 270, val loss: 1.6130038499832153
Epoch 280, training loss: 65.44937133789062 = 1.5736031532287598 + 10.0 * 6.387577056884766
Epoch 280, val loss: 1.5921066999435425
Epoch 290, training loss: 65.3266372680664 = 1.5481644868850708 + 10.0 * 6.377847194671631
Epoch 290, val loss: 1.5701591968536377
Epoch 300, training loss: 65.2448501586914 = 1.5213948488235474 + 10.0 * 6.372345924377441
Epoch 300, val loss: 1.5472604036331177
Epoch 310, training loss: 65.15916442871094 = 1.493432879447937 + 10.0 * 6.366572856903076
Epoch 310, val loss: 1.5235047340393066
Epoch 320, training loss: 65.1061019897461 = 1.4643950462341309 + 10.0 * 6.364171028137207
Epoch 320, val loss: 1.4990843534469604
Epoch 330, training loss: 65.06906127929688 = 1.4343633651733398 + 10.0 * 6.363470077514648
Epoch 330, val loss: 1.4742051362991333
Epoch 340, training loss: 64.95780944824219 = 1.4037137031555176 + 10.0 * 6.355409622192383
Epoch 340, val loss: 1.4490653276443481
Epoch 350, training loss: 64.8835220336914 = 1.3727006912231445 + 10.0 * 6.351081848144531
Epoch 350, val loss: 1.4239569902420044
Epoch 360, training loss: 64.84111022949219 = 1.3414286375045776 + 10.0 * 6.349968433380127
Epoch 360, val loss: 1.3990168571472168
Epoch 370, training loss: 64.75022888183594 = 1.310178518295288 + 10.0 * 6.3440046310424805
Epoch 370, val loss: 1.3742547035217285
Epoch 380, training loss: 64.6899185180664 = 1.2790495157241821 + 10.0 * 6.341086387634277
Epoch 380, val loss: 1.3500115871429443
Epoch 390, training loss: 64.62564086914062 = 1.2481849193572998 + 10.0 * 6.337745666503906
Epoch 390, val loss: 1.3263673782348633
Epoch 400, training loss: 64.56636810302734 = 1.2177156209945679 + 10.0 * 6.334865093231201
Epoch 400, val loss: 1.303348422050476
Epoch 410, training loss: 64.52173614501953 = 1.1877132654190063 + 10.0 * 6.333402156829834
Epoch 410, val loss: 1.2809568643569946
Epoch 420, training loss: 64.44070434570312 = 1.1583585739135742 + 10.0 * 6.328234672546387
Epoch 420, val loss: 1.2594820261001587
Epoch 430, training loss: 64.38555908203125 = 1.1297317743301392 + 10.0 * 6.325582981109619
Epoch 430, val loss: 1.2390494346618652
Epoch 440, training loss: 64.40034484863281 = 1.1017869710922241 + 10.0 * 6.329855918884277
Epoch 440, val loss: 1.2193113565444946
Epoch 450, training loss: 64.29667663574219 = 1.0747013092041016 + 10.0 * 6.322197914123535
Epoch 450, val loss: 1.2007434368133545
Epoch 460, training loss: 64.23298645019531 = 1.0483840703964233 + 10.0 * 6.318459987640381
Epoch 460, val loss: 1.1830345392227173
Epoch 470, training loss: 64.17842102050781 = 1.023024559020996 + 10.0 * 6.315539360046387
Epoch 470, val loss: 1.166327953338623
Epoch 480, training loss: 64.14508056640625 = 0.998476505279541 + 10.0 * 6.314660549163818
Epoch 480, val loss: 1.150508999824524
Epoch 490, training loss: 64.1043930053711 = 0.97463059425354 + 10.0 * 6.312975883483887
Epoch 490, val loss: 1.1356302499771118
Epoch 500, training loss: 64.045654296875 = 0.9515461921691895 + 10.0 * 6.309410572052002
Epoch 500, val loss: 1.1213454008102417
Epoch 510, training loss: 64.00533294677734 = 0.9293824434280396 + 10.0 * 6.307595252990723
Epoch 510, val loss: 1.108257532119751
Epoch 520, training loss: 63.957069396972656 = 0.9079678654670715 + 10.0 * 6.304910182952881
Epoch 520, val loss: 1.09584641456604
Epoch 530, training loss: 63.995792388916016 = 0.8871972560882568 + 10.0 * 6.310859680175781
Epoch 530, val loss: 1.0842119455337524
Epoch 540, training loss: 63.886985778808594 = 0.8671432733535767 + 10.0 * 6.3019843101501465
Epoch 540, val loss: 1.0735230445861816
Epoch 550, training loss: 63.85047149658203 = 0.8477626442909241 + 10.0 * 6.300271034240723
Epoch 550, val loss: 1.0634980201721191
Epoch 560, training loss: 63.80550765991211 = 0.8289991617202759 + 10.0 * 6.2976508140563965
Epoch 560, val loss: 1.0541311502456665
Epoch 570, training loss: 63.827171325683594 = 0.8108073472976685 + 10.0 * 6.301636695861816
Epoch 570, val loss: 1.0454269647598267
Epoch 580, training loss: 63.74517822265625 = 0.7929440140724182 + 10.0 * 6.295223236083984
Epoch 580, val loss: 1.0371013879776
Epoch 590, training loss: 63.710487365722656 = 0.7756816148757935 + 10.0 * 6.29348087310791
Epoch 590, val loss: 1.0294992923736572
Epoch 600, training loss: 63.67401123046875 = 0.7589247822761536 + 10.0 * 6.291508674621582
Epoch 600, val loss: 1.0226205587387085
Epoch 610, training loss: 63.685550689697266 = 0.7425205707550049 + 10.0 * 6.294302940368652
Epoch 610, val loss: 1.0160709619522095
Epoch 620, training loss: 63.64363479614258 = 0.7264134287834167 + 10.0 * 6.291722297668457
Epoch 620, val loss: 1.0099425315856934
Epoch 630, training loss: 63.584903717041016 = 0.7106839418411255 + 10.0 * 6.287421703338623
Epoch 630, val loss: 1.0042448043823242
Epoch 640, training loss: 63.584991455078125 = 0.6952885985374451 + 10.0 * 6.288969993591309
Epoch 640, val loss: 0.9989268183708191
Epoch 650, training loss: 63.530670166015625 = 0.6802334189414978 + 10.0 * 6.285043716430664
Epoch 650, val loss: 0.9942870140075684
Epoch 660, training loss: 63.50050735473633 = 0.6654329895973206 + 10.0 * 6.283507347106934
Epoch 660, val loss: 0.9897554516792297
Epoch 670, training loss: 63.547401428222656 = 0.6508772969245911 + 10.0 * 6.289652347564697
Epoch 670, val loss: 0.9855479001998901
Epoch 680, training loss: 63.44877243041992 = 0.6364771723747253 + 10.0 * 6.281229496002197
Epoch 680, val loss: 0.9817841649055481
Epoch 690, training loss: 63.43744659423828 = 0.6223549246788025 + 10.0 * 6.281508922576904
Epoch 690, val loss: 0.9783863425254822
Epoch 700, training loss: 63.41800308227539 = 0.6084396243095398 + 10.0 * 6.280956268310547
Epoch 700, val loss: 0.975174069404602
Epoch 710, training loss: 63.37718963623047 = 0.5946189761161804 + 10.0 * 6.278256893157959
Epoch 710, val loss: 0.9720672369003296
Epoch 720, training loss: 63.35003662109375 = 0.5811306238174438 + 10.0 * 6.276890754699707
Epoch 720, val loss: 0.9695082902908325
Epoch 730, training loss: 63.3271598815918 = 0.5677211284637451 + 10.0 * 6.275943756103516
Epoch 730, val loss: 0.9670962691307068
Epoch 740, training loss: 63.3286018371582 = 0.5544737577438354 + 10.0 * 6.2774128913879395
Epoch 740, val loss: 0.9649185538291931
Epoch 750, training loss: 63.28653335571289 = 0.5413662791252136 + 10.0 * 6.274516582489014
Epoch 750, val loss: 0.9632809162139893
Epoch 760, training loss: 63.27888107299805 = 0.528391420841217 + 10.0 * 6.275048732757568
Epoch 760, val loss: 0.9615235328674316
Epoch 770, training loss: 63.23776626586914 = 0.5155981779098511 + 10.0 * 6.272216796875
Epoch 770, val loss: 0.9601123929023743
Epoch 780, training loss: 63.214473724365234 = 0.5029923915863037 + 10.0 * 6.271148204803467
Epoch 780, val loss: 0.9589048027992249
Epoch 790, training loss: 63.236881256103516 = 0.4906088709831238 + 10.0 * 6.274627208709717
Epoch 790, val loss: 0.9580852389335632
Epoch 800, training loss: 63.182945251464844 = 0.47824597358703613 + 10.0 * 6.270470142364502
Epoch 800, val loss: 0.9572404623031616
Epoch 810, training loss: 63.15303039550781 = 0.46615469455718994 + 10.0 * 6.2686872482299805
Epoch 810, val loss: 0.9566488862037659
Epoch 820, training loss: 63.12726593017578 = 0.4542384445667267 + 10.0 * 6.267302513122559
Epoch 820, val loss: 0.9563952684402466
Epoch 830, training loss: 63.16436004638672 = 0.44247812032699585 + 10.0 * 6.272188186645508
Epoch 830, val loss: 0.9561381936073303
Epoch 840, training loss: 63.106754302978516 = 0.43095332384109497 + 10.0 * 6.267580032348633
Epoch 840, val loss: 0.956436812877655
Epoch 850, training loss: 63.07152557373047 = 0.41956591606140137 + 10.0 * 6.265195846557617
Epoch 850, val loss: 0.9567264318466187
Epoch 860, training loss: 63.04600143432617 = 0.4084222912788391 + 10.0 * 6.263758182525635
Epoch 860, val loss: 0.957234263420105
Epoch 870, training loss: 63.03657913208008 = 0.3974802494049072 + 10.0 * 6.263909816741943
Epoch 870, val loss: 0.9580276012420654
Epoch 880, training loss: 63.02207946777344 = 0.3866936266422272 + 10.0 * 6.263538837432861
Epoch 880, val loss: 0.9588909149169922
Epoch 890, training loss: 62.999637603759766 = 0.37608644366264343 + 10.0 * 6.262354850769043
Epoch 890, val loss: 0.9600057005882263
Epoch 900, training loss: 63.02911376953125 = 0.36576932668685913 + 10.0 * 6.266334533691406
Epoch 900, val loss: 0.9613687992095947
Epoch 910, training loss: 62.97300720214844 = 0.35550615191459656 + 10.0 * 6.261750221252441
Epoch 910, val loss: 0.9625609517097473
Epoch 920, training loss: 62.94023132324219 = 0.34558358788490295 + 10.0 * 6.259464740753174
Epoch 920, val loss: 0.964241087436676
Epoch 930, training loss: 62.92278289794922 = 0.33584433794021606 + 10.0 * 6.258693695068359
Epoch 930, val loss: 0.9659837484359741
Epoch 940, training loss: 62.96092224121094 = 0.3262960910797119 + 10.0 * 6.263462543487549
Epoch 940, val loss: 0.9678918719291687
Epoch 950, training loss: 62.90798568725586 = 0.3169686496257782 + 10.0 * 6.259101867675781
Epoch 950, val loss: 0.9702180027961731
Epoch 960, training loss: 62.89300537109375 = 0.30778801441192627 + 10.0 * 6.258521556854248
Epoch 960, val loss: 0.9723705053329468
Epoch 970, training loss: 62.88005065917969 = 0.2988719642162323 + 10.0 * 6.258118152618408
Epoch 970, val loss: 0.9749908447265625
Epoch 980, training loss: 62.8498420715332 = 0.2901664078235626 + 10.0 * 6.255967617034912
Epoch 980, val loss: 0.9777558445930481
Epoch 990, training loss: 62.836788177490234 = 0.2816695272922516 + 10.0 * 6.25551176071167
Epoch 990, val loss: 0.9805009365081787
Epoch 1000, training loss: 62.84658432006836 = 0.2733824849128723 + 10.0 * 6.257319927215576
Epoch 1000, val loss: 0.9836415648460388
Epoch 1010, training loss: 62.800838470458984 = 0.2652488946914673 + 10.0 * 6.253559112548828
Epoch 1010, val loss: 0.9866915345191956
Epoch 1020, training loss: 62.79097366333008 = 0.25736525654792786 + 10.0 * 6.253360748291016
Epoch 1020, val loss: 0.9901134371757507
Epoch 1030, training loss: 62.77397155761719 = 0.24972477555274963 + 10.0 * 6.252424716949463
Epoch 1030, val loss: 0.9935605525970459
Epoch 1040, training loss: 62.83184051513672 = 0.2423042356967926 + 10.0 * 6.25895357131958
Epoch 1040, val loss: 0.9973361492156982
Epoch 1050, training loss: 62.77197265625 = 0.23494601249694824 + 10.0 * 6.253702640533447
Epoch 1050, val loss: 1.0007531642913818
Epoch 1060, training loss: 62.73564529418945 = 0.22788767516613007 + 10.0 * 6.2507758140563965
Epoch 1060, val loss: 1.0047707557678223
Epoch 1070, training loss: 62.732357025146484 = 0.22102385759353638 + 10.0 * 6.251133441925049
Epoch 1070, val loss: 1.0087449550628662
Epoch 1080, training loss: 62.723411560058594 = 0.21433576941490173 + 10.0 * 6.2509074211120605
Epoch 1080, val loss: 1.0127098560333252
Epoch 1090, training loss: 62.707252502441406 = 0.2078545093536377 + 10.0 * 6.249939918518066
Epoch 1090, val loss: 1.0171127319335938
Epoch 1100, training loss: 62.71220779418945 = 0.20160768926143646 + 10.0 * 6.2510600090026855
Epoch 1100, val loss: 1.0215401649475098
Epoch 1110, training loss: 62.67567825317383 = 0.19551889598369598 + 10.0 * 6.248015880584717
Epoch 1110, val loss: 1.0260859727859497
Epoch 1120, training loss: 62.66987991333008 = 0.18964657187461853 + 10.0 * 6.24802303314209
Epoch 1120, val loss: 1.030869483947754
Epoch 1130, training loss: 62.6618537902832 = 0.18395550549030304 + 10.0 * 6.2477898597717285
Epoch 1130, val loss: 1.0355920791625977
Epoch 1140, training loss: 62.676692962646484 = 0.17842449247837067 + 10.0 * 6.249826908111572
Epoch 1140, val loss: 1.0403374433517456
Epoch 1150, training loss: 62.64386749267578 = 0.1730174720287323 + 10.0 * 6.247085094451904
Epoch 1150, val loss: 1.0451098680496216
Epoch 1160, training loss: 62.66081619262695 = 0.16782712936401367 + 10.0 * 6.249299049377441
Epoch 1160, val loss: 1.050276517868042
Epoch 1170, training loss: 62.622188568115234 = 0.16281220316886902 + 10.0 * 6.245937824249268
Epoch 1170, val loss: 1.0555591583251953
Epoch 1180, training loss: 62.60734176635742 = 0.15794429183006287 + 10.0 * 6.244939804077148
Epoch 1180, val loss: 1.0608030557632446
Epoch 1190, training loss: 62.595947265625 = 0.15327519178390503 + 10.0 * 6.244267463684082
Epoch 1190, val loss: 1.066241979598999
Epoch 1200, training loss: 62.62153625488281 = 0.148741215467453 + 10.0 * 6.247279167175293
Epoch 1200, val loss: 1.071726679801941
Epoch 1210, training loss: 62.629642486572266 = 0.14434754848480225 + 10.0 * 6.248529434204102
Epoch 1210, val loss: 1.077325701713562
Epoch 1220, training loss: 62.591739654541016 = 0.1400631070137024 + 10.0 * 6.2451677322387695
Epoch 1220, val loss: 1.0826466083526611
Epoch 1230, training loss: 62.56005096435547 = 0.1359557807445526 + 10.0 * 6.242409706115723
Epoch 1230, val loss: 1.088463306427002
Epoch 1240, training loss: 62.5616455078125 = 0.13200996816158295 + 10.0 * 6.2429633140563965
Epoch 1240, val loss: 1.0943098068237305
Epoch 1250, training loss: 62.58102035522461 = 0.12818926572799683 + 10.0 * 6.245283126831055
Epoch 1250, val loss: 1.1000326871871948
Epoch 1260, training loss: 62.55162048339844 = 0.12443564832210541 + 10.0 * 6.242718696594238
Epoch 1260, val loss: 1.1059666872024536
Epoch 1270, training loss: 62.547271728515625 = 0.12089554220438004 + 10.0 * 6.242637634277344
Epoch 1270, val loss: 1.1121848821640015
Epoch 1280, training loss: 62.5419807434082 = 0.11742912977933884 + 10.0 * 6.24245548248291
Epoch 1280, val loss: 1.1181358098983765
Epoch 1290, training loss: 62.544918060302734 = 0.1141006276011467 + 10.0 * 6.243081569671631
Epoch 1290, val loss: 1.124189019203186
Epoch 1300, training loss: 62.517154693603516 = 0.11086638271808624 + 10.0 * 6.240628719329834
Epoch 1300, val loss: 1.130458116531372
Epoch 1310, training loss: 62.504241943359375 = 0.10775211453437805 + 10.0 * 6.239648818969727
Epoch 1310, val loss: 1.136643648147583
Epoch 1320, training loss: 62.502864837646484 = 0.10476399213075638 + 10.0 * 6.239809989929199
Epoch 1320, val loss: 1.143034815788269
Epoch 1330, training loss: 62.50465774536133 = 0.10186011344194412 + 10.0 * 6.240279674530029
Epoch 1330, val loss: 1.1495380401611328
Epoch 1340, training loss: 62.48433303833008 = 0.09905941784381866 + 10.0 * 6.238527297973633
Epoch 1340, val loss: 1.1561975479125977
Epoch 1350, training loss: 62.50374221801758 = 0.09634667634963989 + 10.0 * 6.240739345550537
Epoch 1350, val loss: 1.1626784801483154
Epoch 1360, training loss: 62.47691345214844 = 0.09371532499790192 + 10.0 * 6.2383198738098145
Epoch 1360, val loss: 1.169323205947876
Epoch 1370, training loss: 62.46324157714844 = 0.09118903428316116 + 10.0 * 6.2372050285339355
Epoch 1370, val loss: 1.1762900352478027
Epoch 1380, training loss: 62.454627990722656 = 0.08875410258769989 + 10.0 * 6.2365875244140625
Epoch 1380, val loss: 1.1830613613128662
Epoch 1390, training loss: 62.48743438720703 = 0.08640498667955399 + 10.0 * 6.240102767944336
Epoch 1390, val loss: 1.1899787187576294
Epoch 1400, training loss: 62.45783996582031 = 0.08412575721740723 + 10.0 * 6.237371444702148
Epoch 1400, val loss: 1.1968498229980469
Epoch 1410, training loss: 62.46599578857422 = 0.08190101385116577 + 10.0 * 6.238409519195557
Epoch 1410, val loss: 1.203654170036316
Epoch 1420, training loss: 62.44580078125 = 0.0797736793756485 + 10.0 * 6.236602783203125
Epoch 1420, val loss: 1.2107868194580078
Epoch 1430, training loss: 62.4541015625 = 0.07771552354097366 + 10.0 * 6.237638473510742
Epoch 1430, val loss: 1.2177180051803589
Epoch 1440, training loss: 62.42319107055664 = 0.07570727914571762 + 10.0 * 6.234748363494873
Epoch 1440, val loss: 1.2245724201202393
Epoch 1450, training loss: 62.41354751586914 = 0.07378925383090973 + 10.0 * 6.233975887298584
Epoch 1450, val loss: 1.2315800189971924
Epoch 1460, training loss: 62.43021011352539 = 0.07193512469530106 + 10.0 * 6.235827445983887
Epoch 1460, val loss: 1.238532543182373
Epoch 1470, training loss: 62.413177490234375 = 0.07013209164142609 + 10.0 * 6.234304428100586
Epoch 1470, val loss: 1.2454909086227417
Epoch 1480, training loss: 62.41339111328125 = 0.06838429719209671 + 10.0 * 6.234500408172607
Epoch 1480, val loss: 1.2526757717132568
Epoch 1490, training loss: 62.42283630371094 = 0.06669750809669495 + 10.0 * 6.235613822937012
Epoch 1490, val loss: 1.2595933675765991
Epoch 1500, training loss: 62.4078483581543 = 0.06504373997449875 + 10.0 * 6.234280586242676
Epoch 1500, val loss: 1.2662889957427979
Epoch 1510, training loss: 62.38663864135742 = 0.06347411125898361 + 10.0 * 6.232316493988037
Epoch 1510, val loss: 1.2734438180923462
Epoch 1520, training loss: 62.38399887084961 = 0.06195857375860214 + 10.0 * 6.232203960418701
Epoch 1520, val loss: 1.2803875207901
Epoch 1530, training loss: 62.3813591003418 = 0.060489606112241745 + 10.0 * 6.232087135314941
Epoch 1530, val loss: 1.287310004234314
Epoch 1540, training loss: 62.42704772949219 = 0.05906037986278534 + 10.0 * 6.2367987632751465
Epoch 1540, val loss: 1.2939283847808838
Epoch 1550, training loss: 62.39589309692383 = 0.05765458941459656 + 10.0 * 6.233823776245117
Epoch 1550, val loss: 1.3008791208267212
Epoch 1560, training loss: 62.366275787353516 = 0.05630739778280258 + 10.0 * 6.230996608734131
Epoch 1560, val loss: 1.307742953300476
Epoch 1570, training loss: 62.356719970703125 = 0.05500466749072075 + 10.0 * 6.2301716804504395
Epoch 1570, val loss: 1.3145086765289307
Epoch 1580, training loss: 62.362430572509766 = 0.053762100636959076 + 10.0 * 6.2308669090271
Epoch 1580, val loss: 1.3213772773742676
Epoch 1590, training loss: 62.38591384887695 = 0.05254779011011124 + 10.0 * 6.233336448669434
Epoch 1590, val loss: 1.3280597925186157
Epoch 1600, training loss: 62.36859893798828 = 0.05134544521570206 + 10.0 * 6.231725215911865
Epoch 1600, val loss: 1.3345640897750854
Epoch 1610, training loss: 62.358707427978516 = 0.050201836973428726 + 10.0 * 6.230850696563721
Epoch 1610, val loss: 1.3414206504821777
Epoch 1620, training loss: 62.38065719604492 = 0.04908108711242676 + 10.0 * 6.233157634735107
Epoch 1620, val loss: 1.3477773666381836
Epoch 1630, training loss: 62.3476448059082 = 0.04799339547753334 + 10.0 * 6.2299652099609375
Epoch 1630, val loss: 1.3545114994049072
Epoch 1640, training loss: 62.32871627807617 = 0.0469508171081543 + 10.0 * 6.228176593780518
Epoch 1640, val loss: 1.3610599040985107
Epoch 1650, training loss: 62.33952331542969 = 0.04594610631465912 + 10.0 * 6.229357719421387
Epoch 1650, val loss: 1.3675543069839478
Epoch 1660, training loss: 62.358436584472656 = 0.044951654970645905 + 10.0 * 6.231348514556885
Epoch 1660, val loss: 1.373764991760254
Epoch 1670, training loss: 62.37058639526367 = 0.04398641735315323 + 10.0 * 6.232659816741943
Epoch 1670, val loss: 1.3800705671310425
Epoch 1680, training loss: 62.32482147216797 = 0.04305119812488556 + 10.0 * 6.228177070617676
Epoch 1680, val loss: 1.386687159538269
Epoch 1690, training loss: 62.31172180175781 = 0.042145274579524994 + 10.0 * 6.22695779800415
Epoch 1690, val loss: 1.3930188417434692
Epoch 1700, training loss: 62.30988311767578 = 0.041280921548604965 + 10.0 * 6.226860046386719
Epoch 1700, val loss: 1.3993854522705078
Epoch 1710, training loss: 62.324092864990234 = 0.04043973982334137 + 10.0 * 6.228365421295166
Epoch 1710, val loss: 1.4056103229522705
Epoch 1720, training loss: 62.317813873291016 = 0.039618119597435 + 10.0 * 6.227819442749023
Epoch 1720, val loss: 1.4119104146957397
Epoch 1730, training loss: 62.320255279541016 = 0.03880852833390236 + 10.0 * 6.228144645690918
Epoch 1730, val loss: 1.4179991483688354
Epoch 1740, training loss: 62.31254196166992 = 0.038022108376026154 + 10.0 * 6.227452278137207
Epoch 1740, val loss: 1.4239914417266846
Epoch 1750, training loss: 62.310821533203125 = 0.037261057645082474 + 10.0 * 6.22735595703125
Epoch 1750, val loss: 1.4300669431686401
Epoch 1760, training loss: 62.32445526123047 = 0.03651472553610802 + 10.0 * 6.228794097900391
Epoch 1760, val loss: 1.4359360933303833
Epoch 1770, training loss: 62.30045700073242 = 0.03580160439014435 + 10.0 * 6.226465702056885
Epoch 1770, val loss: 1.4421769380569458
Epoch 1780, training loss: 62.286190032958984 = 0.03509773686528206 + 10.0 * 6.225109100341797
Epoch 1780, val loss: 1.448004961013794
Epoch 1790, training loss: 62.29286193847656 = 0.03442560136318207 + 10.0 * 6.225843906402588
Epoch 1790, val loss: 1.4539049863815308
Epoch 1800, training loss: 62.28837203979492 = 0.033768512308597565 + 10.0 * 6.225460529327393
Epoch 1800, val loss: 1.4598745107650757
Epoch 1810, training loss: 62.310646057128906 = 0.033124279230833054 + 10.0 * 6.227752208709717
Epoch 1810, val loss: 1.4657440185546875
Epoch 1820, training loss: 62.29478073120117 = 0.032488495111465454 + 10.0 * 6.226229190826416
Epoch 1820, val loss: 1.4712008237838745
Epoch 1830, training loss: 62.27666473388672 = 0.03187411278486252 + 10.0 * 6.224478721618652
Epoch 1830, val loss: 1.4771052598953247
Epoch 1840, training loss: 62.27450942993164 = 0.0312831848859787 + 10.0 * 6.224322319030762
Epoch 1840, val loss: 1.4828407764434814
Epoch 1850, training loss: 62.276123046875 = 0.03070795349776745 + 10.0 * 6.224541664123535
Epoch 1850, val loss: 1.4883877038955688
Epoch 1860, training loss: 62.2654914855957 = 0.030148275196552277 + 10.0 * 6.22353458404541
Epoch 1860, val loss: 1.4940519332885742
Epoch 1870, training loss: 62.27571487426758 = 0.029604138806462288 + 10.0 * 6.224610805511475
Epoch 1870, val loss: 1.499717354774475
Epoch 1880, training loss: 62.26502227783203 = 0.029074188321828842 + 10.0 * 6.223594665527344
Epoch 1880, val loss: 1.505192756652832
Epoch 1890, training loss: 62.28070831298828 = 0.02855706959962845 + 10.0 * 6.225214958190918
Epoch 1890, val loss: 1.510778784751892
Epoch 1900, training loss: 62.26639175415039 = 0.028044428676366806 + 10.0 * 6.22383451461792
Epoch 1900, val loss: 1.5160672664642334
Epoch 1910, training loss: 62.2504768371582 = 0.02754255011677742 + 10.0 * 6.222293376922607
Epoch 1910, val loss: 1.5213181972503662
Epoch 1920, training loss: 62.23932647705078 = 0.02706807665526867 + 10.0 * 6.221225738525391
Epoch 1920, val loss: 1.526871919631958
Epoch 1930, training loss: 62.2647819519043 = 0.026608221232891083 + 10.0 * 6.223817348480225
Epoch 1930, val loss: 1.5322505235671997
Epoch 1940, training loss: 62.265560150146484 = 0.026148542761802673 + 10.0 * 6.223940849304199
Epoch 1940, val loss: 1.5373109579086304
Epoch 1950, training loss: 62.248191833496094 = 0.025695111602544785 + 10.0 * 6.222249507904053
Epoch 1950, val loss: 1.5427157878875732
Epoch 1960, training loss: 62.27952194213867 = 0.025265419855713844 + 10.0 * 6.225425720214844
Epoch 1960, val loss: 1.5477737188339233
Epoch 1970, training loss: 62.2412223815918 = 0.02482379414141178 + 10.0 * 6.221640110015869
Epoch 1970, val loss: 1.5527266263961792
Epoch 1980, training loss: 62.225738525390625 = 0.024415500462055206 + 10.0 * 6.220132350921631
Epoch 1980, val loss: 1.5581413507461548
Epoch 1990, training loss: 62.245628356933594 = 0.02401774562895298 + 10.0 * 6.222161293029785
Epoch 1990, val loss: 1.5633333921432495
Epoch 2000, training loss: 62.218017578125 = 0.023620132356882095 + 10.0 * 6.219439506530762
Epoch 2000, val loss: 1.5681788921356201
Epoch 2010, training loss: 62.227813720703125 = 0.023241261020302773 + 10.0 * 6.220457077026367
Epoch 2010, val loss: 1.573315143585205
Epoch 2020, training loss: 62.27735900878906 = 0.022874735295772552 + 10.0 * 6.2254486083984375
Epoch 2020, val loss: 1.5783641338348389
Epoch 2030, training loss: 62.227500915527344 = 0.022490961477160454 + 10.0 * 6.220500946044922
Epoch 2030, val loss: 1.582999587059021
Epoch 2040, training loss: 62.214195251464844 = 0.022135009989142418 + 10.0 * 6.219205856323242
Epoch 2040, val loss: 1.5881412029266357
Epoch 2050, training loss: 62.28059387207031 = 0.021791212260723114 + 10.0 * 6.2258806228637695
Epoch 2050, val loss: 1.5930485725402832
Epoch 2060, training loss: 62.22526550292969 = 0.02144092693924904 + 10.0 * 6.220382213592529
Epoch 2060, val loss: 1.59774649143219
Epoch 2070, training loss: 62.208126068115234 = 0.021103812381625175 + 10.0 * 6.21870231628418
Epoch 2070, val loss: 1.6026990413665771
Epoch 2080, training loss: 62.209903717041016 = 0.020782699808478355 + 10.0 * 6.218912124633789
Epoch 2080, val loss: 1.6074914932250977
Epoch 2090, training loss: 62.26277542114258 = 0.020466886460781097 + 10.0 * 6.224230766296387
Epoch 2090, val loss: 1.6121628284454346
Epoch 2100, training loss: 62.22018051147461 = 0.02015150710940361 + 10.0 * 6.2200026512146
Epoch 2100, val loss: 1.6168098449707031
Epoch 2110, training loss: 62.21125411987305 = 0.019845036789774895 + 10.0 * 6.219141006469727
Epoch 2110, val loss: 1.6215242147445679
Epoch 2120, training loss: 62.23170471191406 = 0.01954626850783825 + 10.0 * 6.221215724945068
Epoch 2120, val loss: 1.6259822845458984
Epoch 2130, training loss: 62.20271301269531 = 0.019253207370638847 + 10.0 * 6.218346118927002
Epoch 2130, val loss: 1.6308048963546753
Epoch 2140, training loss: 62.192813873291016 = 0.01897178404033184 + 10.0 * 6.217384338378906
Epoch 2140, val loss: 1.6354269981384277
Epoch 2150, training loss: 62.22317886352539 = 0.018696196377277374 + 10.0 * 6.2204484939575195
Epoch 2150, val loss: 1.6398098468780518
Epoch 2160, training loss: 62.1862907409668 = 0.018421007320284843 + 10.0 * 6.216786861419678
Epoch 2160, val loss: 1.6446350812911987
Epoch 2170, training loss: 62.1843376159668 = 0.01815853826701641 + 10.0 * 6.216618061065674
Epoch 2170, val loss: 1.6491048336029053
Epoch 2180, training loss: 62.20856857299805 = 0.017901822924613953 + 10.0 * 6.219066619873047
Epoch 2180, val loss: 1.653549075126648
Epoch 2190, training loss: 62.193241119384766 = 0.017644548788666725 + 10.0 * 6.217559814453125
Epoch 2190, val loss: 1.6577210426330566
Epoch 2200, training loss: 62.21059036254883 = 0.017391329631209373 + 10.0 * 6.219319820404053
Epoch 2200, val loss: 1.6620036363601685
Epoch 2210, training loss: 62.188636779785156 = 0.017151132225990295 + 10.0 * 6.217148780822754
Epoch 2210, val loss: 1.666534423828125
Epoch 2220, training loss: 62.191585540771484 = 0.016910657286643982 + 10.0 * 6.217467308044434
Epoch 2220, val loss: 1.670889973640442
Epoch 2230, training loss: 62.17290115356445 = 0.016677511855959892 + 10.0 * 6.215622425079346
Epoch 2230, val loss: 1.6751165390014648
Epoch 2240, training loss: 62.17536163330078 = 0.016451004892587662 + 10.0 * 6.215890884399414
Epoch 2240, val loss: 1.6792659759521484
Epoch 2250, training loss: 62.20574951171875 = 0.016230303794145584 + 10.0 * 6.21895170211792
Epoch 2250, val loss: 1.683476209640503
Epoch 2260, training loss: 62.17955780029297 = 0.016011562198400497 + 10.0 * 6.216354846954346
Epoch 2260, val loss: 1.6878442764282227
Epoch 2270, training loss: 62.19291305541992 = 0.015793293714523315 + 10.0 * 6.217711925506592
Epoch 2270, val loss: 1.691792368888855
Epoch 2280, training loss: 62.16544723510742 = 0.015577261336147785 + 10.0 * 6.214987277984619
Epoch 2280, val loss: 1.6958391666412354
Epoch 2290, training loss: 62.16462326049805 = 0.015371063724160194 + 10.0 * 6.214925289154053
Epoch 2290, val loss: 1.6999541521072388
Epoch 2300, training loss: 62.1602897644043 = 0.015171433798968792 + 10.0 * 6.214511871337891
Epoch 2300, val loss: 1.7040081024169922
Epoch 2310, training loss: 62.18423080444336 = 0.014979559928178787 + 10.0 * 6.216925144195557
Epoch 2310, val loss: 1.7079790830612183
Epoch 2320, training loss: 62.16164779663086 = 0.01478300616145134 + 10.0 * 6.214686393737793
Epoch 2320, val loss: 1.7121531963348389
Epoch 2330, training loss: 62.16780090332031 = 0.014592970721423626 + 10.0 * 6.215321063995361
Epoch 2330, val loss: 1.7160571813583374
Epoch 2340, training loss: 62.17607498168945 = 0.014404169283807278 + 10.0 * 6.216166973114014
Epoch 2340, val loss: 1.7199182510375977
Epoch 2350, training loss: 62.192569732666016 = 0.014218132011592388 + 10.0 * 6.217835426330566
Epoch 2350, val loss: 1.7235187292099
Epoch 2360, training loss: 62.15559768676758 = 0.014040946029126644 + 10.0 * 6.214155673980713
Epoch 2360, val loss: 1.727548360824585
Epoch 2370, training loss: 62.14874267578125 = 0.013865089043974876 + 10.0 * 6.21348762512207
Epoch 2370, val loss: 1.7315895557403564
Epoch 2380, training loss: 62.17181396484375 = 0.013694672845304012 + 10.0 * 6.215811729431152
Epoch 2380, val loss: 1.7351515293121338
Epoch 2390, training loss: 62.13962936401367 = 0.013524736277759075 + 10.0 * 6.212610721588135
Epoch 2390, val loss: 1.7390077114105225
Epoch 2400, training loss: 62.149349212646484 = 0.013361716642975807 + 10.0 * 6.213598728179932
Epoch 2400, val loss: 1.7428706884384155
Epoch 2410, training loss: 62.147830963134766 = 0.013199119828641415 + 10.0 * 6.213463306427002
Epoch 2410, val loss: 1.7464784383773804
Epoch 2420, training loss: 62.146751403808594 = 0.013040810823440552 + 10.0 * 6.2133708000183105
Epoch 2420, val loss: 1.7502058744430542
Epoch 2430, training loss: 62.21941375732422 = 0.012886562384665012 + 10.0 * 6.2206525802612305
Epoch 2430, val loss: 1.7537659406661987
Epoch 2440, training loss: 62.15414047241211 = 0.012725086882710457 + 10.0 * 6.214141368865967
Epoch 2440, val loss: 1.757264256477356
Epoch 2450, training loss: 62.128395080566406 = 0.012573016807436943 + 10.0 * 6.211582183837891
Epoch 2450, val loss: 1.760994791984558
Epoch 2460, training loss: 62.12777328491211 = 0.012429061345756054 + 10.0 * 6.21153450012207
Epoch 2460, val loss: 1.7645254135131836
Epoch 2470, training loss: 62.187957763671875 = 0.012290584854781628 + 10.0 * 6.21756649017334
Epoch 2470, val loss: 1.7681046724319458
Epoch 2480, training loss: 62.14119338989258 = 0.012140684761106968 + 10.0 * 6.212904930114746
Epoch 2480, val loss: 1.7713861465454102
Epoch 2490, training loss: 62.14097213745117 = 0.012004802003502846 + 10.0 * 6.212896823883057
Epoch 2490, val loss: 1.7750407457351685
Epoch 2500, training loss: 62.1406135559082 = 0.011866243556141853 + 10.0 * 6.212874412536621
Epoch 2500, val loss: 1.7784100770950317
Epoch 2510, training loss: 62.151702880859375 = 0.011733490973711014 + 10.0 * 6.213996887207031
Epoch 2510, val loss: 1.7817660570144653
Epoch 2520, training loss: 62.113826751708984 = 0.01159349549561739 + 10.0 * 6.210223197937012
Epoch 2520, val loss: 1.7850064039230347
Epoch 2530, training loss: 62.11518859863281 = 0.011466801166534424 + 10.0 * 6.210371971130371
Epoch 2530, val loss: 1.788563847541809
Epoch 2540, training loss: 62.11674880981445 = 0.01134205050766468 + 10.0 * 6.210540771484375
Epoch 2540, val loss: 1.791887879371643
Epoch 2550, training loss: 62.169246673583984 = 0.011219656094908714 + 10.0 * 6.2158026695251465
Epoch 2550, val loss: 1.7952390909194946
Epoch 2560, training loss: 62.15669250488281 = 0.011094314977526665 + 10.0 * 6.214560031890869
Epoch 2560, val loss: 1.7985179424285889
Epoch 2570, training loss: 62.12477493286133 = 0.010967967100441456 + 10.0 * 6.211380958557129
Epoch 2570, val loss: 1.801326870918274
Epoch 2580, training loss: 62.14046096801758 = 0.010847784578800201 + 10.0 * 6.212961196899414
Epoch 2580, val loss: 1.8045018911361694
Epoch 2590, training loss: 62.11250686645508 = 0.01072961837053299 + 10.0 * 6.210177421569824
Epoch 2590, val loss: 1.8078651428222656
Epoch 2600, training loss: 62.11270523071289 = 0.010617253370583057 + 10.0 * 6.210208892822266
Epoch 2600, val loss: 1.8110647201538086
Epoch 2610, training loss: 62.11901092529297 = 0.010503947734832764 + 10.0 * 6.210850715637207
Epoch 2610, val loss: 1.8140207529067993
Epoch 2620, training loss: 62.11424255371094 = 0.010393872857093811 + 10.0 * 6.210384845733643
Epoch 2620, val loss: 1.8172160387039185
Epoch 2630, training loss: 62.11713409423828 = 0.010284366086125374 + 10.0 * 6.210684776306152
Epoch 2630, val loss: 1.8202669620513916
Epoch 2640, training loss: 62.11406707763672 = 0.010178208351135254 + 10.0 * 6.210389137268066
Epoch 2640, val loss: 1.8233740329742432
Epoch 2650, training loss: 62.13535690307617 = 0.010073862038552761 + 10.0 * 6.212528228759766
Epoch 2650, val loss: 1.8265939950942993
Epoch 2660, training loss: 62.10373306274414 = 0.009965083561837673 + 10.0 * 6.209376811981201
Epoch 2660, val loss: 1.82928466796875
Epoch 2670, training loss: 62.0948600769043 = 0.009864330291748047 + 10.0 * 6.208499431610107
Epoch 2670, val loss: 1.8323665857315063
Epoch 2680, training loss: 62.10256576538086 = 0.009766662493348122 + 10.0 * 6.209280014038086
Epoch 2680, val loss: 1.8354676961898804
Epoch 2690, training loss: 62.100547790527344 = 0.009668027982115746 + 10.0 * 6.209088325500488
Epoch 2690, val loss: 1.8383451700210571
Epoch 2700, training loss: 62.12930679321289 = 0.009572740644216537 + 10.0 * 6.211973667144775
Epoch 2700, val loss: 1.8413658142089844
Epoch 2710, training loss: 62.116729736328125 = 0.009474814869463444 + 10.0 * 6.2107253074646
Epoch 2710, val loss: 1.8438893556594849
Epoch 2720, training loss: 62.09138107299805 = 0.0093768872320652 + 10.0 * 6.208200454711914
Epoch 2720, val loss: 1.8469007015228271
Epoch 2730, training loss: 62.082340240478516 = 0.009285114705562592 + 10.0 * 6.207305431365967
Epoch 2730, val loss: 1.8497563600540161
Epoch 2740, training loss: 62.08641052246094 = 0.009196888655424118 + 10.0 * 6.20772123336792
Epoch 2740, val loss: 1.8527038097381592
Epoch 2750, training loss: 62.126407623291016 = 0.00911288894712925 + 10.0 * 6.211729526519775
Epoch 2750, val loss: 1.855621337890625
Epoch 2760, training loss: 62.10297393798828 = 0.009018119424581528 + 10.0 * 6.209395408630371
Epoch 2760, val loss: 1.8577971458435059
Epoch 2770, training loss: 62.08310317993164 = 0.008930346928536892 + 10.0 * 6.2074174880981445
Epoch 2770, val loss: 1.8608866930007935
Epoch 2780, training loss: 62.095333099365234 = 0.008845747448503971 + 10.0 * 6.208648681640625
Epoch 2780, val loss: 1.8635133504867554
Epoch 2790, training loss: 62.098575592041016 = 0.00876070186495781 + 10.0 * 6.208981513977051
Epoch 2790, val loss: 1.8663150072097778
Epoch 2800, training loss: 62.09169006347656 = 0.008676775731146336 + 10.0 * 6.208301067352295
Epoch 2800, val loss: 1.8687610626220703
Epoch 2810, training loss: 62.07925796508789 = 0.008594286628067493 + 10.0 * 6.207066535949707
Epoch 2810, val loss: 1.8714789152145386
Epoch 2820, training loss: 62.06761169433594 = 0.008515681140124798 + 10.0 * 6.205909729003906
Epoch 2820, val loss: 1.8742876052856445
Epoch 2830, training loss: 62.07257843017578 = 0.008440808393061161 + 10.0 * 6.206413745880127
Epoch 2830, val loss: 1.8769186735153198
Epoch 2840, training loss: 62.1469841003418 = 0.008369156159460545 + 10.0 * 6.213861465454102
Epoch 2840, val loss: 1.8794546127319336
Epoch 2850, training loss: 62.09186935424805 = 0.008282262831926346 + 10.0 * 6.2083587646484375
Epoch 2850, val loss: 1.8816511631011963
Epoch 2860, training loss: 62.077117919921875 = 0.008208904415369034 + 10.0 * 6.206891059875488
Epoch 2860, val loss: 1.884303331375122
Epoch 2870, training loss: 62.08440399169922 = 0.00813526101410389 + 10.0 * 6.207626819610596
Epoch 2870, val loss: 1.8868411779403687
Epoch 2880, training loss: 62.09702682495117 = 0.008063388988375664 + 10.0 * 6.208896160125732
Epoch 2880, val loss: 1.889267921447754
Epoch 2890, training loss: 62.084625244140625 = 0.00798885989934206 + 10.0 * 6.207663536071777
Epoch 2890, val loss: 1.8919622898101807
Epoch 2900, training loss: 62.06593704223633 = 0.007916615344583988 + 10.0 * 6.205801963806152
Epoch 2900, val loss: 1.894202470779419
Epoch 2910, training loss: 62.05369186401367 = 0.007848597131669521 + 10.0 * 6.20458459854126
Epoch 2910, val loss: 1.8968125581741333
Epoch 2920, training loss: 62.07265853881836 = 0.007784748449921608 + 10.0 * 6.20648717880249
Epoch 2920, val loss: 1.8993747234344482
Epoch 2930, training loss: 62.10379409790039 = 0.007715434767305851 + 10.0 * 6.2096076011657715
Epoch 2930, val loss: 1.9015003442764282
Epoch 2940, training loss: 62.06471633911133 = 0.00764433853328228 + 10.0 * 6.20570707321167
Epoch 2940, val loss: 1.903593897819519
Epoch 2950, training loss: 62.055763244628906 = 0.007578754797577858 + 10.0 * 6.204818248748779
Epoch 2950, val loss: 1.9060348272323608
Epoch 2960, training loss: 62.08055877685547 = 0.007514222990721464 + 10.0 * 6.20730447769165
Epoch 2960, val loss: 1.9083017110824585
Epoch 2970, training loss: 62.062347412109375 = 0.007449577562510967 + 10.0 * 6.205489635467529
Epoch 2970, val loss: 1.910528302192688
Epoch 2980, training loss: 62.08234405517578 = 0.007386178243905306 + 10.0 * 6.20749568939209
Epoch 2980, val loss: 1.9127784967422485
Epoch 2990, training loss: 62.05695343017578 = 0.007323313504457474 + 10.0 * 6.204962730407715
Epoch 2990, val loss: 1.91497802734375
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 87.92449951171875 = 1.9563453197479248 + 10.0 * 8.59681510925293
Epoch 0, val loss: 1.9560092687606812
Epoch 10, training loss: 87.90644836425781 = 1.945663571357727 + 10.0 * 8.596078872680664
Epoch 10, val loss: 1.9459327459335327
Epoch 20, training loss: 87.83858489990234 = 1.93258535861969 + 10.0 * 8.59060001373291
Epoch 20, val loss: 1.9333161115646362
Epoch 30, training loss: 87.44825744628906 = 1.916014313697815 + 10.0 * 8.553224563598633
Epoch 30, val loss: 1.9175057411193848
Epoch 40, training loss: 85.20919799804688 = 1.8974905014038086 + 10.0 * 8.331171035766602
Epoch 40, val loss: 1.9006985425949097
Epoch 50, training loss: 78.41352844238281 = 1.876660704612732 + 10.0 * 7.6536865234375
Epoch 50, val loss: 1.8814653158187866
Epoch 60, training loss: 76.80923461914062 = 1.8576027154922485 + 10.0 * 7.4951629638671875
Epoch 60, val loss: 1.8642834424972534
Epoch 70, training loss: 75.18551635742188 = 1.841691017150879 + 10.0 * 7.3343825340271
Epoch 70, val loss: 1.8500349521636963
Epoch 80, training loss: 73.26385498046875 = 1.8293678760528564 + 10.0 * 7.143448352813721
Epoch 80, val loss: 1.8392493724822998
Epoch 90, training loss: 71.49600982666016 = 1.8213695287704468 + 10.0 * 6.967463970184326
Epoch 90, val loss: 1.8313902616500854
Epoch 100, training loss: 70.23271179199219 = 1.8143744468688965 + 10.0 * 6.841833591461182
Epoch 100, val loss: 1.8237038850784302
Epoch 110, training loss: 69.36984252929688 = 1.8065634965896606 + 10.0 * 6.756328105926514
Epoch 110, val loss: 1.815605878829956
Epoch 120, training loss: 68.71611022949219 = 1.7989088296890259 + 10.0 * 6.691720008850098
Epoch 120, val loss: 1.8077189922332764
Epoch 130, training loss: 68.2271728515625 = 1.7914447784423828 + 10.0 * 6.643572807312012
Epoch 130, val loss: 1.8001171350479126
Epoch 140, training loss: 67.84036254882812 = 1.7836599349975586 + 10.0 * 6.605669975280762
Epoch 140, val loss: 1.7924294471740723
Epoch 150, training loss: 67.5423583984375 = 1.7754898071289062 + 10.0 * 6.576686859130859
Epoch 150, val loss: 1.7845699787139893
Epoch 160, training loss: 67.28984069824219 = 1.7668817043304443 + 10.0 * 6.552295684814453
Epoch 160, val loss: 1.7765344381332397
Epoch 170, training loss: 67.06991577148438 = 1.7579193115234375 + 10.0 * 6.5311994552612305
Epoch 170, val loss: 1.768295168876648
Epoch 180, training loss: 66.85221862792969 = 1.7484729290008545 + 10.0 * 6.510374069213867
Epoch 180, val loss: 1.7597838640213013
Epoch 190, training loss: 66.6661148071289 = 1.7381703853607178 + 10.0 * 6.492794513702393
Epoch 190, val loss: 1.7508883476257324
Epoch 200, training loss: 66.5244369506836 = 1.7269071340560913 + 10.0 * 6.479753017425537
Epoch 200, val loss: 1.7411556243896484
Epoch 210, training loss: 66.37413024902344 = 1.7143634557724 + 10.0 * 6.465976715087891
Epoch 210, val loss: 1.7305198907852173
Epoch 220, training loss: 66.2335433959961 = 1.70073664188385 + 10.0 * 6.453280448913574
Epoch 220, val loss: 1.7191033363342285
Epoch 230, training loss: 66.1128921508789 = 1.6860055923461914 + 10.0 * 6.442688465118408
Epoch 230, val loss: 1.7066855430603027
Epoch 240, training loss: 65.99423217773438 = 1.6700003147125244 + 10.0 * 6.4324235916137695
Epoch 240, val loss: 1.693320870399475
Epoch 250, training loss: 65.89820861816406 = 1.6528301239013672 + 10.0 * 6.424537658691406
Epoch 250, val loss: 1.6790289878845215
Epoch 260, training loss: 65.8005599975586 = 1.6344373226165771 + 10.0 * 6.41661262512207
Epoch 260, val loss: 1.6638025045394897
Epoch 270, training loss: 65.69337463378906 = 1.614820122718811 + 10.0 * 6.40785551071167
Epoch 270, val loss: 1.6477646827697754
Epoch 280, training loss: 65.62342071533203 = 1.5941035747528076 + 10.0 * 6.402932167053223
Epoch 280, val loss: 1.6309670209884644
Epoch 290, training loss: 65.51709747314453 = 1.5725221633911133 + 10.0 * 6.3944573402404785
Epoch 290, val loss: 1.6134262084960938
Epoch 300, training loss: 65.42703247070312 = 1.5501341819763184 + 10.0 * 6.387689590454102
Epoch 300, val loss: 1.5955477952957153
Epoch 310, training loss: 65.33878326416016 = 1.5272350311279297 + 10.0 * 6.381155014038086
Epoch 310, val loss: 1.5774461030960083
Epoch 320, training loss: 65.31652069091797 = 1.5038179159164429 + 10.0 * 6.381270408630371
Epoch 320, val loss: 1.5592434406280518
Epoch 330, training loss: 65.20267486572266 = 1.4801771640777588 + 10.0 * 6.372249603271484
Epoch 330, val loss: 1.5409876108169556
Epoch 340, training loss: 65.10807037353516 = 1.4565696716308594 + 10.0 * 6.365149974822998
Epoch 340, val loss: 1.5231635570526123
Epoch 350, training loss: 65.03450012207031 = 1.4330201148986816 + 10.0 * 6.360147953033447
Epoch 350, val loss: 1.505674958229065
Epoch 360, training loss: 65.00276947021484 = 1.4094822406768799 + 10.0 * 6.359328746795654
Epoch 360, val loss: 1.488584041595459
Epoch 370, training loss: 64.91268920898438 = 1.3860318660736084 + 10.0 * 6.352665901184082
Epoch 370, val loss: 1.4716140031814575
Epoch 380, training loss: 64.8373031616211 = 1.3628791570663452 + 10.0 * 6.347442626953125
Epoch 380, val loss: 1.4552040100097656
Epoch 390, training loss: 64.77286529541016 = 1.3399511575698853 + 10.0 * 6.343291282653809
Epoch 390, val loss: 1.4392507076263428
Epoch 400, training loss: 64.73873901367188 = 1.3172495365142822 + 10.0 * 6.342148780822754
Epoch 400, val loss: 1.4235494136810303
Epoch 410, training loss: 64.66254425048828 = 1.2948601245880127 + 10.0 * 6.33676815032959
Epoch 410, val loss: 1.4082145690917969
Epoch 420, training loss: 64.62139892578125 = 1.272482991218567 + 10.0 * 6.3348917961120605
Epoch 420, val loss: 1.393156886100769
Epoch 430, training loss: 64.5486831665039 = 1.2502663135528564 + 10.0 * 6.329841613769531
Epoch 430, val loss: 1.3782868385314941
Epoch 440, training loss: 64.49040985107422 = 1.2282664775848389 + 10.0 * 6.32621431350708
Epoch 440, val loss: 1.3636014461517334
Epoch 450, training loss: 64.48464965820312 = 1.2062987089157104 + 10.0 * 6.3278350830078125
Epoch 450, val loss: 1.3489999771118164
Epoch 460, training loss: 64.39752960205078 = 1.1842749118804932 + 10.0 * 6.321325778961182
Epoch 460, val loss: 1.3343724012374878
Epoch 470, training loss: 64.35723114013672 = 1.162374496459961 + 10.0 * 6.319485664367676
Epoch 470, val loss: 1.3199130296707153
Epoch 480, training loss: 64.30342864990234 = 1.1407017707824707 + 10.0 * 6.316272735595703
Epoch 480, val loss: 1.3055740594863892
Epoch 490, training loss: 64.26912689208984 = 1.1189998388290405 + 10.0 * 6.315012454986572
Epoch 490, val loss: 1.2913507223129272
Epoch 500, training loss: 64.22523498535156 = 1.097308874130249 + 10.0 * 6.312792778015137
Epoch 500, val loss: 1.277303695678711
Epoch 510, training loss: 64.17533874511719 = 1.0756783485412598 + 10.0 * 6.309966087341309
Epoch 510, val loss: 1.2633546590805054
Epoch 520, training loss: 64.12181854248047 = 1.054207444190979 + 10.0 * 6.306760787963867
Epoch 520, val loss: 1.2495503425598145
Epoch 530, training loss: 64.12740325927734 = 1.032785177230835 + 10.0 * 6.309462070465088
Epoch 530, val loss: 1.2358591556549072
Epoch 540, training loss: 64.05317687988281 = 1.011613130569458 + 10.0 * 6.304156303405762
Epoch 540, val loss: 1.222620964050293
Epoch 550, training loss: 63.99986267089844 = 0.990447461605072 + 10.0 * 6.300941467285156
Epoch 550, val loss: 1.2095344066619873
Epoch 560, training loss: 64.00392150878906 = 0.9695677161216736 + 10.0 * 6.303435325622559
Epoch 560, val loss: 1.1967339515686035
Epoch 570, training loss: 63.93921661376953 = 0.948624312877655 + 10.0 * 6.2990593910217285
Epoch 570, val loss: 1.1844295263290405
Epoch 580, training loss: 63.91140365600586 = 0.9281195402145386 + 10.0 * 6.298328399658203
Epoch 580, val loss: 1.1724398136138916
Epoch 590, training loss: 63.84423828125 = 0.9077408909797668 + 10.0 * 6.293649673461914
Epoch 590, val loss: 1.1608716249465942
Epoch 600, training loss: 63.80913162231445 = 0.8876066207885742 + 10.0 * 6.292152404785156
Epoch 600, val loss: 1.1497547626495361
Epoch 610, training loss: 63.80448532104492 = 0.8677036762237549 + 10.0 * 6.293678283691406
Epoch 610, val loss: 1.1390764713287354
Epoch 620, training loss: 63.77732849121094 = 0.8480609655380249 + 10.0 * 6.292926788330078
Epoch 620, val loss: 1.128920078277588
Epoch 630, training loss: 63.7092399597168 = 0.8286308646202087 + 10.0 * 6.288060665130615
Epoch 630, val loss: 1.119179129600525
Epoch 640, training loss: 63.66871643066406 = 0.8094863891601562 + 10.0 * 6.285923004150391
Epoch 640, val loss: 1.1098675727844238
Epoch 650, training loss: 63.638423919677734 = 0.7905564308166504 + 10.0 * 6.284786701202393
Epoch 650, val loss: 1.1010608673095703
Epoch 660, training loss: 63.6912841796875 = 0.7717760801315308 + 10.0 * 6.2919511795043945
Epoch 660, val loss: 1.0925657749176025
Epoch 670, training loss: 63.5953483581543 = 0.7532234191894531 + 10.0 * 6.284212589263916
Epoch 670, val loss: 1.0846219062805176
Epoch 680, training loss: 63.543922424316406 = 0.7348694205284119 + 10.0 * 6.280905246734619
Epoch 680, val loss: 1.0771188735961914
Epoch 690, training loss: 63.51338577270508 = 0.7167472243309021 + 10.0 * 6.279664039611816
Epoch 690, val loss: 1.070006251335144
Epoch 700, training loss: 63.540191650390625 = 0.6987320184707642 + 10.0 * 6.284146308898926
Epoch 700, val loss: 1.0632474422454834
Epoch 710, training loss: 63.4887809753418 = 0.6810635924339294 + 10.0 * 6.280771732330322
Epoch 710, val loss: 1.0569664239883423
Epoch 720, training loss: 63.425907135009766 = 0.663360595703125 + 10.0 * 6.276254653930664
Epoch 720, val loss: 1.0509517192840576
Epoch 730, training loss: 63.398216247558594 = 0.6459545493125916 + 10.0 * 6.27522611618042
Epoch 730, val loss: 1.0453555583953857
Epoch 740, training loss: 63.3715705871582 = 0.6287863850593567 + 10.0 * 6.274278163909912
Epoch 740, val loss: 1.0401830673217773
Epoch 750, training loss: 63.36426544189453 = 0.6117335557937622 + 10.0 * 6.2752532958984375
Epoch 750, val loss: 1.0353924036026
Epoch 760, training loss: 63.33815383911133 = 0.594757616519928 + 10.0 * 6.27433967590332
Epoch 760, val loss: 1.0306386947631836
Epoch 770, training loss: 63.287132263183594 = 0.5780417919158936 + 10.0 * 6.270909309387207
Epoch 770, val loss: 1.0264673233032227
Epoch 780, training loss: 63.26160430908203 = 0.5615603923797607 + 10.0 * 6.2700042724609375
Epoch 780, val loss: 1.0226638317108154
Epoch 790, training loss: 63.261810302734375 = 0.5452480912208557 + 10.0 * 6.271656036376953
Epoch 790, val loss: 1.019202709197998
Epoch 800, training loss: 63.21113967895508 = 0.5291554927825928 + 10.0 * 6.268198490142822
Epoch 800, val loss: 1.0161467790603638
Epoch 810, training loss: 63.18669891357422 = 0.5133124589920044 + 10.0 * 6.267338752746582
Epoch 810, val loss: 1.0134292840957642
Epoch 820, training loss: 63.178585052490234 = 0.49775034189224243 + 10.0 * 6.268083572387695
Epoch 820, val loss: 1.0111918449401855
Epoch 830, training loss: 63.14347839355469 = 0.4824005663394928 + 10.0 * 6.26610803604126
Epoch 830, val loss: 1.0092248916625977
Epoch 840, training loss: 63.1241340637207 = 0.46731922030448914 + 10.0 * 6.265681266784668
Epoch 840, val loss: 1.007594108581543
Epoch 850, training loss: 63.105499267578125 = 0.4525821805000305 + 10.0 * 6.265291690826416
Epoch 850, val loss: 1.0063141584396362
Epoch 860, training loss: 63.0641975402832 = 0.4382566213607788 + 10.0 * 6.262594223022461
Epoch 860, val loss: 1.0057308673858643
Epoch 870, training loss: 63.04935836791992 = 0.42420294880867004 + 10.0 * 6.262515544891357
Epoch 870, val loss: 1.005266785621643
Epoch 880, training loss: 63.052249908447266 = 0.4104119539260864 + 10.0 * 6.26418399810791
Epoch 880, val loss: 1.0049865245819092
Epoch 890, training loss: 63.00510787963867 = 0.3970811367034912 + 10.0 * 6.260802745819092
Epoch 890, val loss: 1.0052169561386108
Epoch 900, training loss: 62.98222732543945 = 0.3840523958206177 + 10.0 * 6.259817600250244
Epoch 900, val loss: 1.0055164098739624
Epoch 910, training loss: 62.957923889160156 = 0.3714502155780792 + 10.0 * 6.258647441864014
Epoch 910, val loss: 1.0063101053237915
Epoch 920, training loss: 62.957942962646484 = 0.3591776490211487 + 10.0 * 6.259876728057861
Epoch 920, val loss: 1.0072877407073975
Epoch 930, training loss: 62.914329528808594 = 0.34722381830215454 + 10.0 * 6.256710529327393
Epoch 930, val loss: 1.0083482265472412
Epoch 940, training loss: 62.9256477355957 = 0.3356650769710541 + 10.0 * 6.258997917175293
Epoch 940, val loss: 1.0096060037612915
Epoch 950, training loss: 62.88726043701172 = 0.3245700001716614 + 10.0 * 6.2562689781188965
Epoch 950, val loss: 1.0116153955459595
Epoch 960, training loss: 62.859432220458984 = 0.3137647211551666 + 10.0 * 6.254566669464111
Epoch 960, val loss: 1.0133064985275269
Epoch 970, training loss: 62.84162521362305 = 0.3034175932407379 + 10.0 * 6.253820896148682
Epoch 970, val loss: 1.0155802965164185
Epoch 980, training loss: 62.87150192260742 = 0.2934090793132782 + 10.0 * 6.257809638977051
Epoch 980, val loss: 1.0178300142288208
Epoch 990, training loss: 62.81991195678711 = 0.2837555408477783 + 10.0 * 6.253615379333496
Epoch 990, val loss: 1.0203989744186401
Epoch 1000, training loss: 62.79513931274414 = 0.274465948343277 + 10.0 * 6.2520670890808105
Epoch 1000, val loss: 1.0231348276138306
Epoch 1010, training loss: 62.79237365722656 = 0.2655521333217621 + 10.0 * 6.252682209014893
Epoch 1010, val loss: 1.0260909795761108
Epoch 1020, training loss: 62.810970306396484 = 0.2568723261356354 + 10.0 * 6.2554097175598145
Epoch 1020, val loss: 1.0288619995117188
Epoch 1030, training loss: 62.76738357543945 = 0.2486215978860855 + 10.0 * 6.251875877380371
Epoch 1030, val loss: 1.0322071313858032
Epoch 1040, training loss: 62.736576080322266 = 0.24064278602600098 + 10.0 * 6.249593257904053
Epoch 1040, val loss: 1.0354838371276855
Epoch 1050, training loss: 62.714725494384766 = 0.23301617801189423 + 10.0 * 6.248170852661133
Epoch 1050, val loss: 1.039064645767212
Epoch 1060, training loss: 62.726036071777344 = 0.22570675611495972 + 10.0 * 6.250032901763916
Epoch 1060, val loss: 1.0427976846694946
Epoch 1070, training loss: 62.70664978027344 = 0.2185935229063034 + 10.0 * 6.248805522918701
Epoch 1070, val loss: 1.0461467504501343
Epoch 1080, training loss: 62.69844055175781 = 0.211777463555336 + 10.0 * 6.248666286468506
Epoch 1080, val loss: 1.050083875656128
Epoch 1090, training loss: 62.66364288330078 = 0.20523911714553833 + 10.0 * 6.245840549468994
Epoch 1090, val loss: 1.0539215803146362
Epoch 1100, training loss: 62.65610122680664 = 0.1989583671092987 + 10.0 * 6.24571418762207
Epoch 1100, val loss: 1.0578107833862305
Epoch 1110, training loss: 62.69718551635742 = 0.19291627407073975 + 10.0 * 6.250426769256592
Epoch 1110, val loss: 1.0617997646331787
Epoch 1120, training loss: 62.6535530090332 = 0.18706756830215454 + 10.0 * 6.24664831161499
Epoch 1120, val loss: 1.066163182258606
Epoch 1130, training loss: 62.635704040527344 = 0.18141835927963257 + 10.0 * 6.245428562164307
Epoch 1130, val loss: 1.0702229738235474
Epoch 1140, training loss: 62.66567611694336 = 0.17601493000984192 + 10.0 * 6.248966217041016
Epoch 1140, val loss: 1.0746151208877563
Epoch 1150, training loss: 62.61225891113281 = 0.17081236839294434 + 10.0 * 6.244144916534424
Epoch 1150, val loss: 1.0792890787124634
Epoch 1160, training loss: 62.59087371826172 = 0.16577030718326569 + 10.0 * 6.2425103187561035
Epoch 1160, val loss: 1.0836126804351807
Epoch 1170, training loss: 62.58478546142578 = 0.16095568239688873 + 10.0 * 6.242383003234863
Epoch 1170, val loss: 1.0884325504302979
Epoch 1180, training loss: 62.62804412841797 = 0.15631042420864105 + 10.0 * 6.247173309326172
Epoch 1180, val loss: 1.0930813550949097
Epoch 1190, training loss: 62.57262420654297 = 0.15174348652362823 + 10.0 * 6.2420878410339355
Epoch 1190, val loss: 1.0975494384765625
Epoch 1200, training loss: 62.55791091918945 = 0.14741136133670807 + 10.0 * 6.241049766540527
Epoch 1200, val loss: 1.1023741960525513
Epoch 1210, training loss: 62.59328842163086 = 0.14321434497833252 + 10.0 * 6.245007514953613
Epoch 1210, val loss: 1.1071019172668457
Epoch 1220, training loss: 62.548587799072266 = 0.13916468620300293 + 10.0 * 6.240942478179932
Epoch 1220, val loss: 1.1120357513427734
Epoch 1230, training loss: 62.530513763427734 = 0.13525597751140594 + 10.0 * 6.23952579498291
Epoch 1230, val loss: 1.1169853210449219
Epoch 1240, training loss: 62.52337646484375 = 0.13148823380470276 + 10.0 * 6.2391886711120605
Epoch 1240, val loss: 1.1219035387039185
Epoch 1250, training loss: 62.5853157043457 = 0.12784788012504578 + 10.0 * 6.245746612548828
Epoch 1250, val loss: 1.1268370151519775
Epoch 1260, training loss: 62.53289031982422 = 0.12429579347372055 + 10.0 * 6.240859508514404
Epoch 1260, val loss: 1.1318953037261963
Epoch 1270, training loss: 62.501808166503906 = 0.12087094783782959 + 10.0 * 6.238093376159668
Epoch 1270, val loss: 1.136841058731079
Epoch 1280, training loss: 62.488990783691406 = 0.11758960783481598 + 10.0 * 6.23714017868042
Epoch 1280, val loss: 1.1421253681182861
Epoch 1290, training loss: 62.494197845458984 = 0.11440370231866837 + 10.0 * 6.237979412078857
Epoch 1290, val loss: 1.1471736431121826
Epoch 1300, training loss: 62.49944305419922 = 0.11130749434232712 + 10.0 * 6.238813400268555
Epoch 1300, val loss: 1.1522245407104492
Epoch 1310, training loss: 62.487396240234375 = 0.10833566635847092 + 10.0 * 6.237905979156494
Epoch 1310, val loss: 1.1577264070510864
Epoch 1320, training loss: 62.482486724853516 = 0.10541501641273499 + 10.0 * 6.237707138061523
Epoch 1320, val loss: 1.1626466512680054
Epoch 1330, training loss: 62.49047088623047 = 0.102628692984581 + 10.0 * 6.238784313201904
Epoch 1330, val loss: 1.1681183576583862
Epoch 1340, training loss: 62.4557991027832 = 0.09988952428102493 + 10.0 * 6.235590934753418
Epoch 1340, val loss: 1.1731125116348267
Epoch 1350, training loss: 62.44441223144531 = 0.09727106243371964 + 10.0 * 6.234714031219482
Epoch 1350, val loss: 1.178593397140503
Epoch 1360, training loss: 62.446006774902344 = 0.09472420811653137 + 10.0 * 6.235128402709961
Epoch 1360, val loss: 1.1836531162261963
Epoch 1370, training loss: 62.457279205322266 = 0.09227575361728668 + 10.0 * 6.236500263214111
Epoch 1370, val loss: 1.1889872550964355
Epoch 1380, training loss: 62.43275833129883 = 0.08988024294376373 + 10.0 * 6.234287738800049
Epoch 1380, val loss: 1.1945195198059082
Epoch 1390, training loss: 62.42597579956055 = 0.08756529539823532 + 10.0 * 6.2338409423828125
Epoch 1390, val loss: 1.1997169256210327
Epoch 1400, training loss: 62.45591735839844 = 0.08533003181219101 + 10.0 * 6.237058639526367
Epoch 1400, val loss: 1.205139398574829
Epoch 1410, training loss: 62.42304611206055 = 0.08315444737672806 + 10.0 * 6.233989238739014
Epoch 1410, val loss: 1.2105398178100586
Epoch 1420, training loss: 62.41585159301758 = 0.08104852586984634 + 10.0 * 6.233480453491211
Epoch 1420, val loss: 1.2160190343856812
Epoch 1430, training loss: 62.405574798583984 = 0.07900718599557877 + 10.0 * 6.232656955718994
Epoch 1430, val loss: 1.2214683294296265
Epoch 1440, training loss: 62.4332160949707 = 0.07703892886638641 + 10.0 * 6.235617637634277
Epoch 1440, val loss: 1.227090835571289
Epoch 1450, training loss: 62.40241622924805 = 0.075101837515831 + 10.0 * 6.232731819152832
Epoch 1450, val loss: 1.2323226928710938
Epoch 1460, training loss: 62.39223098754883 = 0.0732329711318016 + 10.0 * 6.231899738311768
Epoch 1460, val loss: 1.2377402782440186
Epoch 1470, training loss: 62.405128479003906 = 0.07143571972846985 + 10.0 * 6.23336935043335
Epoch 1470, val loss: 1.2431925535202026
Epoch 1480, training loss: 62.3775749206543 = 0.06967691332101822 + 10.0 * 6.230790138244629
Epoch 1480, val loss: 1.2486319541931152
Epoch 1490, training loss: 62.38157272338867 = 0.06797487288713455 + 10.0 * 6.231359958648682
Epoch 1490, val loss: 1.2539901733398438
Epoch 1500, training loss: 62.38140106201172 = 0.06633007526397705 + 10.0 * 6.231507301330566
Epoch 1500, val loss: 1.2593295574188232
Epoch 1510, training loss: 62.37725830078125 = 0.06472768634557724 + 10.0 * 6.231253147125244
Epoch 1510, val loss: 1.2644731998443604
Epoch 1520, training loss: 62.384151458740234 = 0.06317274272441864 + 10.0 * 6.23209810256958
Epoch 1520, val loss: 1.2701082229614258
Epoch 1530, training loss: 62.36839294433594 = 0.06167440488934517 + 10.0 * 6.2306718826293945
Epoch 1530, val loss: 1.2755529880523682
Epoch 1540, training loss: 62.346012115478516 = 0.06020568311214447 + 10.0 * 6.228580474853516
Epoch 1540, val loss: 1.2810416221618652
Epoch 1550, training loss: 62.341102600097656 = 0.058804359287023544 + 10.0 * 6.228229999542236
Epoch 1550, val loss: 1.2866575717926025
Epoch 1560, training loss: 62.3571662902832 = 0.05744137242436409 + 10.0 * 6.2299723625183105
Epoch 1560, val loss: 1.2920873165130615
Epoch 1570, training loss: 62.33407211303711 = 0.05610111728310585 + 10.0 * 6.227797031402588
Epoch 1570, val loss: 1.2974334955215454
Epoch 1580, training loss: 62.350337982177734 = 0.0548076331615448 + 10.0 * 6.22955322265625
Epoch 1580, val loss: 1.3026957511901855
Epoch 1590, training loss: 62.3466682434082 = 0.05354652926325798 + 10.0 * 6.229311943054199
Epoch 1590, val loss: 1.307964563369751
Epoch 1600, training loss: 62.31962203979492 = 0.05231010168790817 + 10.0 * 6.226731300354004
Epoch 1600, val loss: 1.3131409883499146
Epoch 1610, training loss: 62.318912506103516 = 0.05113183334469795 + 10.0 * 6.226778030395508
Epoch 1610, val loss: 1.318583369255066
Epoch 1620, training loss: 62.37216567993164 = 0.04998960345983505 + 10.0 * 6.232217788696289
Epoch 1620, val loss: 1.3237816095352173
Epoch 1630, training loss: 62.32689666748047 = 0.048868462443351746 + 10.0 * 6.227802753448486
Epoch 1630, val loss: 1.3293310403823853
Epoch 1640, training loss: 62.30751419067383 = 0.04778299480676651 + 10.0 * 6.225973129272461
Epoch 1640, val loss: 1.3345057964324951
Epoch 1650, training loss: 62.30234909057617 = 0.04673690348863602 + 10.0 * 6.225561141967773
Epoch 1650, val loss: 1.3398956060409546
Epoch 1660, training loss: 62.33289337158203 = 0.045720044523477554 + 10.0 * 6.22871732711792
Epoch 1660, val loss: 1.3449636697769165
Epoch 1670, training loss: 62.34987258911133 = 0.0447220616042614 + 10.0 * 6.230515003204346
Epoch 1670, val loss: 1.3497917652130127
Epoch 1680, training loss: 62.30421447753906 = 0.043755702674388885 + 10.0 * 6.226046085357666
Epoch 1680, val loss: 1.3556349277496338
Epoch 1690, training loss: 62.28528594970703 = 0.04281606525182724 + 10.0 * 6.224246978759766
Epoch 1690, val loss: 1.3606117963790894
Epoch 1700, training loss: 62.27933883666992 = 0.041913360357284546 + 10.0 * 6.223742485046387
Epoch 1700, val loss: 1.3659310340881348
Epoch 1710, training loss: 62.299537658691406 = 0.04104183614253998 + 10.0 * 6.225849628448486
Epoch 1710, val loss: 1.3711298704147339
Epoch 1720, training loss: 62.286888122558594 = 0.040175288915634155 + 10.0 * 6.224671363830566
Epoch 1720, val loss: 1.3759697675704956
Epoch 1730, training loss: 62.305355072021484 = 0.03932904824614525 + 10.0 * 6.226602554321289
Epoch 1730, val loss: 1.3808670043945312
Epoch 1740, training loss: 62.2768440246582 = 0.03851138800382614 + 10.0 * 6.223833084106445
Epoch 1740, val loss: 1.385953426361084
Epoch 1750, training loss: 62.265201568603516 = 0.03772401809692383 + 10.0 * 6.222747802734375
Epoch 1750, val loss: 1.391046166419983
Epoch 1760, training loss: 62.267398834228516 = 0.03695996478199959 + 10.0 * 6.223043918609619
Epoch 1760, val loss: 1.3960508108139038
Epoch 1770, training loss: 62.3060188293457 = 0.036214765161275864 + 10.0 * 6.226980686187744
Epoch 1770, val loss: 1.4007920026779175
Epoch 1780, training loss: 62.28469467163086 = 0.03548787534236908 + 10.0 * 6.224920749664307
Epoch 1780, val loss: 1.4059255123138428
Epoch 1790, training loss: 62.28438186645508 = 0.03478047624230385 + 10.0 * 6.2249603271484375
Epoch 1790, val loss: 1.410995364189148
Epoch 1800, training loss: 62.267330169677734 = 0.03409082069993019 + 10.0 * 6.223323822021484
Epoch 1800, val loss: 1.4156877994537354
Epoch 1810, training loss: 62.252620697021484 = 0.03341704607009888 + 10.0 * 6.221920490264893
Epoch 1810, val loss: 1.4204246997833252
Epoch 1820, training loss: 62.23991775512695 = 0.03277278319001198 + 10.0 * 6.220714569091797
Epoch 1820, val loss: 1.4254494905471802
Epoch 1830, training loss: 62.2402229309082 = 0.03214827924966812 + 10.0 * 6.2208075523376465
Epoch 1830, val loss: 1.4303843975067139
Epoch 1840, training loss: 62.290122985839844 = 0.03154166415333748 + 10.0 * 6.225858211517334
Epoch 1840, val loss: 1.4352139234542847
Epoch 1850, training loss: 62.29645919799805 = 0.030934050679206848 + 10.0 * 6.226552486419678
Epoch 1850, val loss: 1.4392755031585693
Epoch 1860, training loss: 62.246585845947266 = 0.030337339267134666 + 10.0 * 6.221624851226807
Epoch 1860, val loss: 1.4443151950836182
Epoch 1870, training loss: 62.224971771240234 = 0.0297698937356472 + 10.0 * 6.219520092010498
Epoch 1870, val loss: 1.4490140676498413
Epoch 1880, training loss: 62.22100067138672 = 0.02922113612294197 + 10.0 * 6.219178199768066
Epoch 1880, val loss: 1.4537559747695923
Epoch 1890, training loss: 62.225608825683594 = 0.028689295053482056 + 10.0 * 6.219691753387451
Epoch 1890, val loss: 1.4584535360336304
Epoch 1900, training loss: 62.300540924072266 = 0.028167571872472763 + 10.0 * 6.227237224578857
Epoch 1900, val loss: 1.462761402130127
Epoch 1910, training loss: 62.22925567626953 = 0.027642158791422844 + 10.0 * 6.220161437988281
Epoch 1910, val loss: 1.4672026634216309
Epoch 1920, training loss: 62.217308044433594 = 0.027142878621816635 + 10.0 * 6.2190165519714355
Epoch 1920, val loss: 1.471923589706421
Epoch 1930, training loss: 62.21444320678711 = 0.026661673560738564 + 10.0 * 6.218778133392334
Epoch 1930, val loss: 1.4765417575836182
Epoch 1940, training loss: 62.2465705871582 = 0.0261938888579607 + 10.0 * 6.222037315368652
Epoch 1940, val loss: 1.4808720350265503
Epoch 1950, training loss: 62.20907974243164 = 0.025732101872563362 + 10.0 * 6.218334674835205
Epoch 1950, val loss: 1.4856483936309814
Epoch 1960, training loss: 62.20870590209961 = 0.025283686816692352 + 10.0 * 6.218342304229736
Epoch 1960, val loss: 1.4902050495147705
Epoch 1970, training loss: 62.22513198852539 = 0.024849358946084976 + 10.0 * 6.220028400421143
Epoch 1970, val loss: 1.4947433471679688
Epoch 1980, training loss: 62.19868087768555 = 0.02441156841814518 + 10.0 * 6.2174272537231445
Epoch 1980, val loss: 1.4986586570739746
Epoch 1990, training loss: 62.19578552246094 = 0.023996934294700623 + 10.0 * 6.217178821563721
Epoch 1990, val loss: 1.5031230449676514
Epoch 2000, training loss: 62.235206604003906 = 0.02359692193567753 + 10.0 * 6.221160888671875
Epoch 2000, val loss: 1.5074974298477173
Epoch 2010, training loss: 62.21928405761719 = 0.023196741938591003 + 10.0 * 6.219608783721924
Epoch 2010, val loss: 1.5117427110671997
Epoch 2020, training loss: 62.20624923706055 = 0.022801410406827927 + 10.0 * 6.218344688415527
Epoch 2020, val loss: 1.5157212018966675
Epoch 2030, training loss: 62.191383361816406 = 0.02242310717701912 + 10.0 * 6.216896057128906
Epoch 2030, val loss: 1.5201261043548584
Epoch 2040, training loss: 62.18208694458008 = 0.022059226408600807 + 10.0 * 6.216002464294434
Epoch 2040, val loss: 1.5245041847229004
Epoch 2050, training loss: 62.186744689941406 = 0.02170504629611969 + 10.0 * 6.216504096984863
Epoch 2050, val loss: 1.5287448167800903
Epoch 2060, training loss: 62.25192642211914 = 0.021353423595428467 + 10.0 * 6.223057270050049
Epoch 2060, val loss: 1.532538890838623
Epoch 2070, training loss: 62.187255859375 = 0.02100476622581482 + 10.0 * 6.216625213623047
Epoch 2070, val loss: 1.5365270376205444
Epoch 2080, training loss: 62.174922943115234 = 0.020667916163802147 + 10.0 * 6.215425491333008
Epoch 2080, val loss: 1.5407203435897827
Epoch 2090, training loss: 62.17629623413086 = 0.020342661067843437 + 10.0 * 6.215595245361328
Epoch 2090, val loss: 1.5448284149169922
Epoch 2100, training loss: 62.208595275878906 = 0.020029054954648018 + 10.0 * 6.2188568115234375
Epoch 2100, val loss: 1.5486444234848022
Epoch 2110, training loss: 62.1898193359375 = 0.019710958003997803 + 10.0 * 6.217010974884033
Epoch 2110, val loss: 1.5524410009384155
Epoch 2120, training loss: 62.17195510864258 = 0.019403671845793724 + 10.0 * 6.215254783630371
Epoch 2120, val loss: 1.5565036535263062
Epoch 2130, training loss: 62.163578033447266 = 0.01910785771906376 + 10.0 * 6.214447021484375
Epoch 2130, val loss: 1.5605798959732056
Epoch 2140, training loss: 62.189353942871094 = 0.018823545426130295 + 10.0 * 6.217053413391113
Epoch 2140, val loss: 1.5644936561584473
Epoch 2150, training loss: 62.16368103027344 = 0.018531860783696175 + 10.0 * 6.21451473236084
Epoch 2150, val loss: 1.5682436227798462
Epoch 2160, training loss: 62.171810150146484 = 0.018252570182085037 + 10.0 * 6.21535587310791
Epoch 2160, val loss: 1.5717875957489014
Epoch 2170, training loss: 62.162620544433594 = 0.01797894388437271 + 10.0 * 6.21446418762207
Epoch 2170, val loss: 1.575670599937439
Epoch 2180, training loss: 62.182979583740234 = 0.017713487148284912 + 10.0 * 6.216526985168457
Epoch 2180, val loss: 1.5792999267578125
Epoch 2190, training loss: 62.16303253173828 = 0.017451738938689232 + 10.0 * 6.214558124542236
Epoch 2190, val loss: 1.5830003023147583
Epoch 2200, training loss: 62.16411590576172 = 0.017197370529174805 + 10.0 * 6.214692115783691
Epoch 2200, val loss: 1.586738109588623
Epoch 2210, training loss: 62.15742111206055 = 0.01694907620549202 + 10.0 * 6.214047431945801
Epoch 2210, val loss: 1.5906177759170532
Epoch 2220, training loss: 62.159393310546875 = 0.016707008704543114 + 10.0 * 6.214268684387207
Epoch 2220, val loss: 1.594246745109558
Epoch 2230, training loss: 62.157230377197266 = 0.016468005254864693 + 10.0 * 6.214076042175293
Epoch 2230, val loss: 1.5975744724273682
Epoch 2240, training loss: 62.1660041809082 = 0.016232464462518692 + 10.0 * 6.214977264404297
Epoch 2240, val loss: 1.6009809970855713
Epoch 2250, training loss: 62.16139602661133 = 0.01600526086986065 + 10.0 * 6.214539051055908
Epoch 2250, val loss: 1.6048415899276733
Epoch 2260, training loss: 62.176692962646484 = 0.015782002359628677 + 10.0 * 6.216091156005859
Epoch 2260, val loss: 1.6079952716827393
Epoch 2270, training loss: 62.14970779418945 = 0.015559757128357887 + 10.0 * 6.213414669036865
Epoch 2270, val loss: 1.6117148399353027
Epoch 2280, training loss: 62.1503791809082 = 0.015344862826168537 + 10.0 * 6.213503360748291
Epoch 2280, val loss: 1.6150178909301758
Epoch 2290, training loss: 62.13201141357422 = 0.01513582281768322 + 10.0 * 6.2116875648498535
Epoch 2290, val loss: 1.6185271739959717
Epoch 2300, training loss: 62.13865661621094 = 0.014932375401258469 + 10.0 * 6.212372779846191
Epoch 2300, val loss: 1.621924877166748
Epoch 2310, training loss: 62.16582107543945 = 0.014733904041349888 + 10.0 * 6.215108871459961
Epoch 2310, val loss: 1.6250839233398438
Epoch 2320, training loss: 62.14700698852539 = 0.014537142589688301 + 10.0 * 6.213246822357178
Epoch 2320, val loss: 1.6287920475006104
Epoch 2330, training loss: 62.13600540161133 = 0.014343393966555595 + 10.0 * 6.2121663093566895
Epoch 2330, val loss: 1.6320360898971558
Epoch 2340, training loss: 62.17110824584961 = 0.014158437959849834 + 10.0 * 6.215694904327393
Epoch 2340, val loss: 1.6355186700820923
Epoch 2350, training loss: 62.13359451293945 = 0.013961326330900192 + 10.0 * 6.211963176727295
Epoch 2350, val loss: 1.6378839015960693
Epoch 2360, training loss: 62.12980270385742 = 0.013781425543129444 + 10.0 * 6.211602210998535
Epoch 2360, val loss: 1.6416062116622925
Epoch 2370, training loss: 62.148590087890625 = 0.01360451802611351 + 10.0 * 6.213498592376709
Epoch 2370, val loss: 1.6444919109344482
Epoch 2380, training loss: 62.11201095581055 = 0.013428515754640102 + 10.0 * 6.209858417510986
Epoch 2380, val loss: 1.6477794647216797
Epoch 2390, training loss: 62.11830520629883 = 0.013260090723633766 + 10.0 * 6.210504531860352
Epoch 2390, val loss: 1.651023030281067
Epoch 2400, training loss: 62.13265609741211 = 0.013093234039843082 + 10.0 * 6.21195650100708
Epoch 2400, val loss: 1.654033899307251
Epoch 2410, training loss: 62.14366912841797 = 0.01292821392416954 + 10.0 * 6.213074207305908
Epoch 2410, val loss: 1.6571195125579834
Epoch 2420, training loss: 62.116783142089844 = 0.012762363068759441 + 10.0 * 6.210402011871338
Epoch 2420, val loss: 1.6597237586975098
Epoch 2430, training loss: 62.111473083496094 = 0.012603824958205223 + 10.0 * 6.2098870277404785
Epoch 2430, val loss: 1.6630655527114868
Epoch 2440, training loss: 62.12377166748047 = 0.012449611909687519 + 10.0 * 6.211132049560547
Epoch 2440, val loss: 1.666100263595581
Epoch 2450, training loss: 62.11450958251953 = 0.012298058718442917 + 10.0 * 6.210221290588379
Epoch 2450, val loss: 1.6690179109573364
Epoch 2460, training loss: 62.11503982543945 = 0.012147786095738411 + 10.0 * 6.210289478302002
Epoch 2460, val loss: 1.671937108039856
Epoch 2470, training loss: 62.12028503417969 = 0.012002645991742611 + 10.0 * 6.2108283042907715
Epoch 2470, val loss: 1.6748307943344116
Epoch 2480, training loss: 62.113399505615234 = 0.011860080994665623 + 10.0 * 6.210154056549072
Epoch 2480, val loss: 1.6776899099349976
Epoch 2490, training loss: 62.1260871887207 = 0.011721918359398842 + 10.0 * 6.2114362716674805
Epoch 2490, val loss: 1.6809055805206299
Epoch 2500, training loss: 62.125335693359375 = 0.011579183861613274 + 10.0 * 6.211375713348389
Epoch 2500, val loss: 1.6835440397262573
Epoch 2510, training loss: 62.115970611572266 = 0.011442887596786022 + 10.0 * 6.210452556610107
Epoch 2510, val loss: 1.6865402460098267
Epoch 2520, training loss: 62.09587097167969 = 0.011307244189083576 + 10.0 * 6.208456516265869
Epoch 2520, val loss: 1.6891520023345947
Epoch 2530, training loss: 62.108665466308594 = 0.01117775123566389 + 10.0 * 6.2097487449646
Epoch 2530, val loss: 1.6920340061187744
Epoch 2540, training loss: 62.099796295166016 = 0.011050986126065254 + 10.0 * 6.208874702453613
Epoch 2540, val loss: 1.6948965787887573
Epoch 2550, training loss: 62.090599060058594 = 0.01092377956956625 + 10.0 * 6.207967281341553
Epoch 2550, val loss: 1.697588324546814
Epoch 2560, training loss: 62.11009216308594 = 0.010805432684719563 + 10.0 * 6.209928512573242
Epoch 2560, val loss: 1.7006884813308716
Epoch 2570, training loss: 62.11344909667969 = 0.010681909509003162 + 10.0 * 6.2102766036987305
Epoch 2570, val loss: 1.7032254934310913
Epoch 2580, training loss: 62.09846496582031 = 0.010558849200606346 + 10.0 * 6.2087907791137695
Epoch 2580, val loss: 1.7054640054702759
Epoch 2590, training loss: 62.127647399902344 = 0.010443317703902721 + 10.0 * 6.2117204666137695
Epoch 2590, val loss: 1.7080957889556885
Epoch 2600, training loss: 62.08454895019531 = 0.010323434136807919 + 10.0 * 6.207422733306885
Epoch 2600, val loss: 1.7110002040863037
Epoch 2610, training loss: 62.08069610595703 = 0.010211861692368984 + 10.0 * 6.207048416137695
Epoch 2610, val loss: 1.7136270999908447
Epoch 2620, training loss: 62.083282470703125 = 0.010101846419274807 + 10.0 * 6.207318305969238
Epoch 2620, val loss: 1.716122031211853
Epoch 2630, training loss: 62.10722351074219 = 0.009995244443416595 + 10.0 * 6.209722995758057
Epoch 2630, val loss: 1.718827247619629
Epoch 2640, training loss: 62.08924102783203 = 0.009887943975627422 + 10.0 * 6.207935333251953
Epoch 2640, val loss: 1.7215583324432373
Epoch 2650, training loss: 62.07832336425781 = 0.009780309163033962 + 10.0 * 6.206854343414307
Epoch 2650, val loss: 1.7239582538604736
Epoch 2660, training loss: 62.07796859741211 = 0.00967634841799736 + 10.0 * 6.206829071044922
Epoch 2660, val loss: 1.7263076305389404
Epoch 2670, training loss: 62.14024353027344 = 0.009576971642673016 + 10.0 * 6.213066577911377
Epoch 2670, val loss: 1.7285668849945068
Epoch 2680, training loss: 62.11176300048828 = 0.009474135003983974 + 10.0 * 6.21022891998291
Epoch 2680, val loss: 1.7313640117645264
Epoch 2690, training loss: 62.095306396484375 = 0.009372699074447155 + 10.0 * 6.208593368530273
Epoch 2690, val loss: 1.7335131168365479
Epoch 2700, training loss: 62.06620407104492 = 0.00927423033863306 + 10.0 * 6.205693244934082
Epoch 2700, val loss: 1.7363874912261963
Epoch 2710, training loss: 62.06697463989258 = 0.00918132346123457 + 10.0 * 6.205779075622559
Epoch 2710, val loss: 1.738642930984497
Epoch 2720, training loss: 62.083251953125 = 0.009090813808143139 + 10.0 * 6.20741605758667
Epoch 2720, val loss: 1.7412587404251099
Epoch 2730, training loss: 62.110050201416016 = 0.008998528122901917 + 10.0 * 6.210104942321777
Epoch 2730, val loss: 1.7435146570205688
Epoch 2740, training loss: 62.06674575805664 = 0.0089033804833889 + 10.0 * 6.205784320831299
Epoch 2740, val loss: 1.7453664541244507
Epoch 2750, training loss: 62.057186126708984 = 0.008813290856778622 + 10.0 * 6.204837322235107
Epoch 2750, val loss: 1.7479681968688965
Epoch 2760, training loss: 62.06072235107422 = 0.00872680451720953 + 10.0 * 6.205199241638184
Epoch 2760, val loss: 1.7503560781478882
Epoch 2770, training loss: 62.13352584838867 = 0.008643299341201782 + 10.0 * 6.212488174438477
Epoch 2770, val loss: 1.7519197463989258
Epoch 2780, training loss: 62.07347106933594 = 0.008556006476283073 + 10.0 * 6.206491470336914
Epoch 2780, val loss: 1.7550970315933228
Epoch 2790, training loss: 62.06928634643555 = 0.008471774868667126 + 10.0 * 6.206081390380859
Epoch 2790, val loss: 1.7569242715835571
Epoch 2800, training loss: 62.08268737792969 = 0.00839060079306364 + 10.0 * 6.2074294090271
Epoch 2800, val loss: 1.7593148946762085
Epoch 2810, training loss: 62.051124572753906 = 0.008308589458465576 + 10.0 * 6.204281806945801
Epoch 2810, val loss: 1.7616124153137207
Epoch 2820, training loss: 62.073394775390625 = 0.008231469430029392 + 10.0 * 6.206516265869141
Epoch 2820, val loss: 1.7640130519866943
Epoch 2830, training loss: 62.061851501464844 = 0.00815178919583559 + 10.0 * 6.20536994934082
Epoch 2830, val loss: 1.7658883333206177
Epoch 2840, training loss: 62.05180740356445 = 0.008074427023530006 + 10.0 * 6.204373359680176
Epoch 2840, val loss: 1.7682206630706787
Epoch 2850, training loss: 62.06524658203125 = 0.008001509122550488 + 10.0 * 6.205724239349365
Epoch 2850, val loss: 1.7707715034484863
Epoch 2860, training loss: 62.07419967651367 = 0.007927251048386097 + 10.0 * 6.206627368927002
Epoch 2860, val loss: 1.7724908590316772
Epoch 2870, training loss: 62.05431365966797 = 0.007849149405956268 + 10.0 * 6.204646110534668
Epoch 2870, val loss: 1.7741873264312744
Epoch 2880, training loss: 62.05331039428711 = 0.007778514176607132 + 10.0 * 6.204553127288818
Epoch 2880, val loss: 1.7765650749206543
Epoch 2890, training loss: 62.06401062011719 = 0.007706772536039352 + 10.0 * 6.205630302429199
Epoch 2890, val loss: 1.7784115076065063
Epoch 2900, training loss: 62.04922866821289 = 0.007636206690222025 + 10.0 * 6.204159259796143
Epoch 2900, val loss: 1.7808531522750854
Epoch 2910, training loss: 62.081459045410156 = 0.007567023392766714 + 10.0 * 6.207388877868652
Epoch 2910, val loss: 1.7824739217758179
Epoch 2920, training loss: 62.060604095458984 = 0.007499918807297945 + 10.0 * 6.205310344696045
Epoch 2920, val loss: 1.7847506999969482
Epoch 2930, training loss: 62.04522705078125 = 0.007430546451359987 + 10.0 * 6.203779697418213
Epoch 2930, val loss: 1.7867976427078247
Epoch 2940, training loss: 62.03727340698242 = 0.007365566678345203 + 10.0 * 6.202990531921387
Epoch 2940, val loss: 1.7889156341552734
Epoch 2950, training loss: 62.03547668457031 = 0.007301701698452234 + 10.0 * 6.202817440032959
Epoch 2950, val loss: 1.7908313274383545
Epoch 2960, training loss: 62.06454086303711 = 0.007238451857119799 + 10.0 * 6.205729961395264
Epoch 2960, val loss: 1.7926580905914307
Epoch 2970, training loss: 62.043460845947266 = 0.007173548918217421 + 10.0 * 6.2036285400390625
Epoch 2970, val loss: 1.7943106889724731
Epoch 2980, training loss: 62.05615997314453 = 0.0071105873212218285 + 10.0 * 6.204905033111572
Epoch 2980, val loss: 1.796262502670288
Epoch 2990, training loss: 62.04737854003906 = 0.007049225736409426 + 10.0 * 6.204032897949219
Epoch 2990, val loss: 1.7983030080795288
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 87.90890502929688 = 1.9406474828720093 + 10.0 * 8.59682559967041
Epoch 0, val loss: 1.9426000118255615
Epoch 10, training loss: 87.89237976074219 = 1.9307019710540771 + 10.0 * 8.59616756439209
Epoch 10, val loss: 1.9320303201675415
Epoch 20, training loss: 87.8372802734375 = 1.9185017347335815 + 10.0 * 8.591877937316895
Epoch 20, val loss: 1.9189319610595703
Epoch 30, training loss: 87.54569244384766 = 1.9032667875289917 + 10.0 * 8.564242362976074
Epoch 30, val loss: 1.9027644395828247
Epoch 40, training loss: 85.95382690429688 = 1.8871430158615112 + 10.0 * 8.406667709350586
Epoch 40, val loss: 1.886544108390808
Epoch 50, training loss: 78.60478210449219 = 1.871289849281311 + 10.0 * 7.673348903656006
Epoch 50, val loss: 1.870482087135315
Epoch 60, training loss: 74.27903747558594 = 1.8571332693099976 + 10.0 * 7.242190361022949
Epoch 60, val loss: 1.8569456338882446
Epoch 70, training loss: 72.01395416259766 = 1.8459558486938477 + 10.0 * 7.0167999267578125
Epoch 70, val loss: 1.846445083618164
Epoch 80, training loss: 71.15959167480469 = 1.835305094718933 + 10.0 * 6.93242883682251
Epoch 80, val loss: 1.8359777927398682
Epoch 90, training loss: 70.5244140625 = 1.8253319263458252 + 10.0 * 6.869907855987549
Epoch 90, val loss: 1.8262110948562622
Epoch 100, training loss: 69.90313720703125 = 1.8163973093032837 + 10.0 * 6.808673858642578
Epoch 100, val loss: 1.8172129392623901
Epoch 110, training loss: 69.28575897216797 = 1.8087455034255981 + 10.0 * 6.747701168060303
Epoch 110, val loss: 1.8094087839126587
Epoch 120, training loss: 68.7809829711914 = 1.8022661209106445 + 10.0 * 6.697871208190918
Epoch 120, val loss: 1.8027759790420532
Epoch 130, training loss: 68.36504364013672 = 1.7956058979034424 + 10.0 * 6.6569437980651855
Epoch 130, val loss: 1.7962614297866821
Epoch 140, training loss: 67.96670532226562 = 1.7889333963394165 + 10.0 * 6.617777347564697
Epoch 140, val loss: 1.789825439453125
Epoch 150, training loss: 67.57939910888672 = 1.7826460599899292 + 10.0 * 6.579675674438477
Epoch 150, val loss: 1.7837327718734741
Epoch 160, training loss: 67.26878356933594 = 1.776220440864563 + 10.0 * 6.549255847930908
Epoch 160, val loss: 1.7776037454605103
Epoch 170, training loss: 67.0328369140625 = 1.7691198587417603 + 10.0 * 6.526371479034424
Epoch 170, val loss: 1.7709611654281616
Epoch 180, training loss: 66.80992126464844 = 1.7610113620758057 + 10.0 * 6.504891395568848
Epoch 180, val loss: 1.7636412382125854
Epoch 190, training loss: 66.63428497314453 = 1.7520467042922974 + 10.0 * 6.488224029541016
Epoch 190, val loss: 1.7555851936340332
Epoch 200, training loss: 66.50740051269531 = 1.7422904968261719 + 10.0 * 6.476511001586914
Epoch 200, val loss: 1.7468996047973633
Epoch 210, training loss: 66.34992980957031 = 1.7314108610153198 + 10.0 * 6.461852073669434
Epoch 210, val loss: 1.7372888326644897
Epoch 220, training loss: 66.227294921875 = 1.7195000648498535 + 10.0 * 6.450779438018799
Epoch 220, val loss: 1.7268048524856567
Epoch 230, training loss: 66.14945983886719 = 1.7064342498779297 + 10.0 * 6.444302558898926
Epoch 230, val loss: 1.715362548828125
Epoch 240, training loss: 66.01638793945312 = 1.6920462846755981 + 10.0 * 6.43243408203125
Epoch 240, val loss: 1.702860713005066
Epoch 250, training loss: 65.9005355834961 = 1.6763216257095337 + 10.0 * 6.422420978546143
Epoch 250, val loss: 1.6893632411956787
Epoch 260, training loss: 65.79972076416016 = 1.6591198444366455 + 10.0 * 6.414060115814209
Epoch 260, val loss: 1.6746901273727417
Epoch 270, training loss: 65.76345825195312 = 1.6404531002044678 + 10.0 * 6.412301063537598
Epoch 270, val loss: 1.6585747003555298
Epoch 280, training loss: 65.63811492919922 = 1.619928002357483 + 10.0 * 6.401819229125977
Epoch 280, val loss: 1.6412303447723389
Epoch 290, training loss: 65.54125213623047 = 1.5979769229888916 + 10.0 * 6.394327640533447
Epoch 290, val loss: 1.6228394508361816
Epoch 300, training loss: 65.45343017578125 = 1.574607014656067 + 10.0 * 6.387882709503174
Epoch 300, val loss: 1.6031678915023804
Epoch 310, training loss: 65.37113952636719 = 1.549655556678772 + 10.0 * 6.382148742675781
Epoch 310, val loss: 1.5824167728424072
Epoch 320, training loss: 65.37093353271484 = 1.5232596397399902 + 10.0 * 6.384767055511475
Epoch 320, val loss: 1.560659408569336
Epoch 330, training loss: 65.23818969726562 = 1.495167851448059 + 10.0 * 6.374301910400391
Epoch 330, val loss: 1.5375624895095825
Epoch 340, training loss: 65.14312744140625 = 1.4660844802856445 + 10.0 * 6.367703914642334
Epoch 340, val loss: 1.5136969089508057
Epoch 350, training loss: 65.06910705566406 = 1.435957908630371 + 10.0 * 6.363315105438232
Epoch 350, val loss: 1.4892696142196655
Epoch 360, training loss: 65.01326751708984 = 1.4049360752105713 + 10.0 * 6.360833168029785
Epoch 360, val loss: 1.4643168449401855
Epoch 370, training loss: 64.93844604492188 = 1.3727394342422485 + 10.0 * 6.356570720672607
Epoch 370, val loss: 1.438734531402588
Epoch 380, training loss: 64.8555679321289 = 1.340320110321045 + 10.0 * 6.351524829864502
Epoch 380, val loss: 1.4130629301071167
Epoch 390, training loss: 64.78800964355469 = 1.3074767589569092 + 10.0 * 6.348053455352783
Epoch 390, val loss: 1.387381911277771
Epoch 400, training loss: 64.7234878540039 = 1.2743691205978394 + 10.0 * 6.344911575317383
Epoch 400, val loss: 1.3617267608642578
Epoch 410, training loss: 64.66134643554688 = 1.2413884401321411 + 10.0 * 6.341995716094971
Epoch 410, val loss: 1.3363865613937378
Epoch 420, training loss: 64.5876693725586 = 1.2085950374603271 + 10.0 * 6.337907314300537
Epoch 420, val loss: 1.311458706855774
Epoch 430, training loss: 64.58879089355469 = 1.176055908203125 + 10.0 * 6.341273307800293
Epoch 430, val loss: 1.2871562242507935
Epoch 440, training loss: 64.47016906738281 = 1.1439052820205688 + 10.0 * 6.332626819610596
Epoch 440, val loss: 1.2630720138549805
Epoch 450, training loss: 64.40604400634766 = 1.1126214265823364 + 10.0 * 6.329342365264893
Epoch 450, val loss: 1.2399829626083374
Epoch 460, training loss: 64.35002136230469 = 1.081979513168335 + 10.0 * 6.326804161071777
Epoch 460, val loss: 1.2177696228027344
Epoch 470, training loss: 64.29847717285156 = 1.0520700216293335 + 10.0 * 6.32464075088501
Epoch 470, val loss: 1.1964530944824219
Epoch 480, training loss: 64.24327850341797 = 1.0227882862091064 + 10.0 * 6.322049140930176
Epoch 480, val loss: 1.1758133172988892
Epoch 490, training loss: 64.2459716796875 = 0.9943669438362122 + 10.0 * 6.325160026550293
Epoch 490, val loss: 1.15610933303833
Epoch 500, training loss: 64.1691665649414 = 0.9671223759651184 + 10.0 * 6.320204257965088
Epoch 500, val loss: 1.1376492977142334
Epoch 510, training loss: 64.09476470947266 = 0.9405981302261353 + 10.0 * 6.3154168128967285
Epoch 510, val loss: 1.1199438571929932
Epoch 520, training loss: 64.04344940185547 = 0.9150421619415283 + 10.0 * 6.312840938568115
Epoch 520, val loss: 1.1032583713531494
Epoch 530, training loss: 64.00763702392578 = 0.8902336359024048 + 10.0 * 6.311740398406982
Epoch 530, val loss: 1.0874600410461426
Epoch 540, training loss: 64.00496673583984 = 0.8662005662918091 + 10.0 * 6.313876152038574
Epoch 540, val loss: 1.0721577405929565
Epoch 550, training loss: 63.93229675292969 = 0.8430044054985046 + 10.0 * 6.308928966522217
Epoch 550, val loss: 1.057923674583435
Epoch 560, training loss: 63.877323150634766 = 0.820683479309082 + 10.0 * 6.3056640625
Epoch 560, val loss: 1.0447274446487427
Epoch 570, training loss: 63.83415222167969 = 0.7990943789482117 + 10.0 * 6.303505897521973
Epoch 570, val loss: 1.032265067100525
Epoch 580, training loss: 63.907310485839844 = 0.7782345414161682 + 10.0 * 6.3129072189331055
Epoch 580, val loss: 1.0204696655273438
Epoch 590, training loss: 63.76935958862305 = 0.7576607465744019 + 10.0 * 6.3011698722839355
Epoch 590, val loss: 1.008968710899353
Epoch 600, training loss: 63.7249870300293 = 0.7379597425460815 + 10.0 * 6.298702716827393
Epoch 600, val loss: 0.9984928965568542
Epoch 610, training loss: 63.707496643066406 = 0.7189148664474487 + 10.0 * 6.298858165740967
Epoch 610, val loss: 0.9884677529335022
Epoch 620, training loss: 63.65652084350586 = 0.7003762722015381 + 10.0 * 6.295614719390869
Epoch 620, val loss: 0.9792956113815308
Epoch 630, training loss: 63.62379837036133 = 0.6823287606239319 + 10.0 * 6.29414701461792
Epoch 630, val loss: 0.9703565835952759
Epoch 640, training loss: 63.58987045288086 = 0.664766252040863 + 10.0 * 6.292510509490967
Epoch 640, val loss: 0.9620058536529541
Epoch 650, training loss: 63.605587005615234 = 0.6475762128829956 + 10.0 * 6.295801162719727
Epoch 650, val loss: 0.9541379809379578
Epoch 660, training loss: 63.55821990966797 = 0.630852997303009 + 10.0 * 6.292737007141113
Epoch 660, val loss: 0.9464853405952454
Epoch 670, training loss: 63.50169372558594 = 0.614429235458374 + 10.0 * 6.288726329803467
Epoch 670, val loss: 0.9393892288208008
Epoch 680, training loss: 63.47047424316406 = 0.5984728932380676 + 10.0 * 6.287199974060059
Epoch 680, val loss: 0.9327012896537781
Epoch 690, training loss: 63.44288635253906 = 0.5828909277915955 + 10.0 * 6.285999774932861
Epoch 690, val loss: 0.9264383912086487
Epoch 700, training loss: 63.502113342285156 = 0.5676115155220032 + 10.0 * 6.293450355529785
Epoch 700, val loss: 0.920369565486908
Epoch 710, training loss: 63.42232131958008 = 0.5525447726249695 + 10.0 * 6.286977767944336
Epoch 710, val loss: 0.9145044684410095
Epoch 720, training loss: 63.368385314941406 = 0.5380117893218994 + 10.0 * 6.283037185668945
Epoch 720, val loss: 0.909115195274353
Epoch 730, training loss: 63.336360931396484 = 0.5238305330276489 + 10.0 * 6.281252861022949
Epoch 730, val loss: 0.9040707349777222
Epoch 740, training loss: 63.31005096435547 = 0.5099297761917114 + 10.0 * 6.280012130737305
Epoch 740, val loss: 0.8993633985519409
Epoch 750, training loss: 63.29241180419922 = 0.49624279141426086 + 10.0 * 6.279616832733154
Epoch 750, val loss: 0.8949841260910034
Epoch 760, training loss: 63.27397918701172 = 0.4827120006084442 + 10.0 * 6.2791266441345215
Epoch 760, val loss: 0.8905463218688965
Epoch 770, training loss: 63.25519943237305 = 0.4695172607898712 + 10.0 * 6.278568267822266
Epoch 770, val loss: 0.8866396546363831
Epoch 780, training loss: 63.22386932373047 = 0.4566744565963745 + 10.0 * 6.276719570159912
Epoch 780, val loss: 0.8829967975616455
Epoch 790, training loss: 63.19472885131836 = 0.4440854489803314 + 10.0 * 6.275064468383789
Epoch 790, val loss: 0.8795920014381409
Epoch 800, training loss: 63.22129440307617 = 0.4317675828933716 + 10.0 * 6.278952598571777
Epoch 800, val loss: 0.8764676451683044
Epoch 810, training loss: 63.17104721069336 = 0.4196797311306 + 10.0 * 6.275136470794678
Epoch 810, val loss: 0.8735266327857971
Epoch 820, training loss: 63.1330451965332 = 0.40779611468315125 + 10.0 * 6.272524833679199
Epoch 820, val loss: 0.870731770992279
Epoch 830, training loss: 63.1168212890625 = 0.3962951898574829 + 10.0 * 6.272052764892578
Epoch 830, val loss: 0.8683829307556152
Epoch 840, training loss: 63.12040710449219 = 0.3849879205226898 + 10.0 * 6.2735419273376465
Epoch 840, val loss: 0.8660939335823059
Epoch 850, training loss: 63.0760498046875 = 0.3738557696342468 + 10.0 * 6.270219326019287
Epoch 850, val loss: 0.8641557097434998
Epoch 860, training loss: 63.05801010131836 = 0.36310264468193054 + 10.0 * 6.269490718841553
Epoch 860, val loss: 0.8623946309089661
Epoch 870, training loss: 63.07552719116211 = 0.3525341749191284 + 10.0 * 6.272299289703369
Epoch 870, val loss: 0.8608403205871582
Epoch 880, training loss: 63.019081115722656 = 0.34227338433265686 + 10.0 * 6.267680644989014
Epoch 880, val loss: 0.8596802949905396
Epoch 890, training loss: 62.99198532104492 = 0.3322722017765045 + 10.0 * 6.2659711837768555
Epoch 890, val loss: 0.8586334586143494
Epoch 900, training loss: 62.976844787597656 = 0.3225594758987427 + 10.0 * 6.26542854309082
Epoch 900, val loss: 0.857957661151886
Epoch 910, training loss: 62.9976921081543 = 0.313068687915802 + 10.0 * 6.268462181091309
Epoch 910, val loss: 0.8573102355003357
Epoch 920, training loss: 62.975242614746094 = 0.3037479519844055 + 10.0 * 6.267149448394775
Epoch 920, val loss: 0.8567926287651062
Epoch 930, training loss: 62.92499923706055 = 0.2947717607021332 + 10.0 * 6.263022422790527
Epoch 930, val loss: 0.8566962480545044
Epoch 940, training loss: 62.91412353515625 = 0.28603115677833557 + 10.0 * 6.2628092765808105
Epoch 940, val loss: 0.8568438291549683
Epoch 950, training loss: 62.933589935302734 = 0.27750635147094727 + 10.0 * 6.265608310699463
Epoch 950, val loss: 0.856941819190979
Epoch 960, training loss: 62.88700485229492 = 0.26926833391189575 + 10.0 * 6.261773586273193
Epoch 960, val loss: 0.8574971556663513
Epoch 970, training loss: 62.862789154052734 = 0.2612072229385376 + 10.0 * 6.260158061981201
Epoch 970, val loss: 0.858129620552063
Epoch 980, training loss: 62.88863754272461 = 0.2534361183643341 + 10.0 * 6.263520240783691
Epoch 980, val loss: 0.8589096069335938
Epoch 990, training loss: 62.845458984375 = 0.24580518901348114 + 10.0 * 6.259965419769287
Epoch 990, val loss: 0.8601483106613159
Epoch 1000, training loss: 62.8281364440918 = 0.23845532536506653 + 10.0 * 6.258967876434326
Epoch 1000, val loss: 0.8612333536148071
Epoch 1010, training loss: 62.818748474121094 = 0.23132024705410004 + 10.0 * 6.258742809295654
Epoch 1010, val loss: 0.8627594113349915
Epoch 1020, training loss: 62.8045539855957 = 0.22441525757312775 + 10.0 * 6.258013725280762
Epoch 1020, val loss: 0.8643243312835693
Epoch 1030, training loss: 62.78697204589844 = 0.21771860122680664 + 10.0 * 6.256925106048584
Epoch 1030, val loss: 0.8661139607429504
Epoch 1040, training loss: 62.76446533203125 = 0.21124084293842316 + 10.0 * 6.255322456359863
Epoch 1040, val loss: 0.8679385781288147
Epoch 1050, training loss: 62.749855041503906 = 0.20493736863136292 + 10.0 * 6.254491806030273
Epoch 1050, val loss: 0.869928777217865
Epoch 1060, training loss: 62.84334182739258 = 0.19880856573581696 + 10.0 * 6.264453411102295
Epoch 1060, val loss: 0.8718698024749756
Epoch 1070, training loss: 62.73197937011719 = 0.19288668036460876 + 10.0 * 6.253909111022949
Epoch 1070, val loss: 0.87408447265625
Epoch 1080, training loss: 62.722999572753906 = 0.1871623545885086 + 10.0 * 6.253583908081055
Epoch 1080, val loss: 0.8765620589256287
Epoch 1090, training loss: 62.7080078125 = 0.18166756629943848 + 10.0 * 6.252634048461914
Epoch 1090, val loss: 0.8791393041610718
Epoch 1100, training loss: 62.71979904174805 = 0.17632831633090973 + 10.0 * 6.254347324371338
Epoch 1100, val loss: 0.8815985321998596
Epoch 1110, training loss: 62.681087493896484 = 0.1711079180240631 + 10.0 * 6.250998020172119
Epoch 1110, val loss: 0.8841856122016907
Epoch 1120, training loss: 62.66489028930664 = 0.16612546145915985 + 10.0 * 6.249876499176025
Epoch 1120, val loss: 0.887000560760498
Epoch 1130, training loss: 62.667964935302734 = 0.1613064557313919 + 10.0 * 6.250665664672852
Epoch 1130, val loss: 0.8898265361785889
Epoch 1140, training loss: 62.65433883666992 = 0.1565854400396347 + 10.0 * 6.249775409698486
Epoch 1140, val loss: 0.8925409913063049
Epoch 1150, training loss: 62.63925552368164 = 0.15203675627708435 + 10.0 * 6.248721599578857
Epoch 1150, val loss: 0.8954452872276306
Epoch 1160, training loss: 62.629268646240234 = 0.1476476639509201 + 10.0 * 6.248162269592285
Epoch 1160, val loss: 0.8984419107437134
Epoch 1170, training loss: 62.62449264526367 = 0.14343327283859253 + 10.0 * 6.248106002807617
Epoch 1170, val loss: 0.9014949202537537
Epoch 1180, training loss: 62.64461898803711 = 0.1393236219882965 + 10.0 * 6.2505292892456055
Epoch 1180, val loss: 0.9043922424316406
Epoch 1190, training loss: 62.60432434082031 = 0.13530845940113068 + 10.0 * 6.246901512145996
Epoch 1190, val loss: 0.9075904488563538
Epoch 1200, training loss: 62.6178092956543 = 0.13148470222949982 + 10.0 * 6.248632431030273
Epoch 1200, val loss: 0.9107922315597534
Epoch 1210, training loss: 62.63473892211914 = 0.12773750722408295 + 10.0 * 6.250699996948242
Epoch 1210, val loss: 0.9135882258415222
Epoch 1220, training loss: 62.58336639404297 = 0.12414601445198059 + 10.0 * 6.245922088623047
Epoch 1220, val loss: 0.917216956615448
Epoch 1230, training loss: 62.57255172729492 = 0.12066023051738739 + 10.0 * 6.245189189910889
Epoch 1230, val loss: 0.9203541874885559
Epoch 1240, training loss: 62.55704879760742 = 0.1173277497291565 + 10.0 * 6.243971824645996
Epoch 1240, val loss: 0.9238214492797852
Epoch 1250, training loss: 62.54671859741211 = 0.11408866941928864 + 10.0 * 6.243262767791748
Epoch 1250, val loss: 0.9271855354309082
Epoch 1260, training loss: 62.5712776184082 = 0.11097447574138641 + 10.0 * 6.246030330657959
Epoch 1260, val loss: 0.9306243062019348
Epoch 1270, training loss: 62.55247116088867 = 0.10789664834737778 + 10.0 * 6.244457721710205
Epoch 1270, val loss: 0.9339775443077087
Epoch 1280, training loss: 62.54331970214844 = 0.1049463301897049 + 10.0 * 6.243837356567383
Epoch 1280, val loss: 0.9373649954795837
Epoch 1290, training loss: 62.51714324951172 = 0.10210444778203964 + 10.0 * 6.241503715515137
Epoch 1290, val loss: 0.9410054087638855
Epoch 1300, training loss: 62.52457046508789 = 0.09937678277492523 + 10.0 * 6.242519378662109
Epoch 1300, val loss: 0.9445974230766296
Epoch 1310, training loss: 62.52482604980469 = 0.09671943634748459 + 10.0 * 6.2428107261657715
Epoch 1310, val loss: 0.9480770826339722
Epoch 1320, training loss: 62.53803253173828 = 0.09413179755210876 + 10.0 * 6.24439001083374
Epoch 1320, val loss: 0.951545774936676
Epoch 1330, training loss: 62.508934020996094 = 0.09163375943899155 + 10.0 * 6.241730213165283
Epoch 1330, val loss: 0.9551912546157837
Epoch 1340, training loss: 62.49122619628906 = 0.08922050148248672 + 10.0 * 6.240200519561768
Epoch 1340, val loss: 0.9586847424507141
Epoch 1350, training loss: 62.480140686035156 = 0.08691146969795227 + 10.0 * 6.239323139190674
Epoch 1350, val loss: 0.9623680710792542
Epoch 1360, training loss: 62.51662063598633 = 0.08468049019575119 + 10.0 * 6.243194103240967
Epoch 1360, val loss: 0.9658355712890625
Epoch 1370, training loss: 62.48955535888672 = 0.08247974514961243 + 10.0 * 6.2407073974609375
Epoch 1370, val loss: 0.9697487354278564
Epoch 1380, training loss: 62.4781379699707 = 0.08037800341844559 + 10.0 * 6.239775657653809
Epoch 1380, val loss: 0.973258912563324
Epoch 1390, training loss: 62.506134033203125 = 0.07833721488714218 + 10.0 * 6.242779731750488
Epoch 1390, val loss: 0.976872980594635
Epoch 1400, training loss: 62.462318420410156 = 0.07633403688669205 + 10.0 * 6.238598346710205
Epoch 1400, val loss: 0.9804585576057434
Epoch 1410, training loss: 62.44679260253906 = 0.07443585246801376 + 10.0 * 6.2372355461120605
Epoch 1410, val loss: 0.9841534495353699
Epoch 1420, training loss: 62.43369674682617 = 0.07259362936019897 + 10.0 * 6.236110210418701
Epoch 1420, val loss: 0.9879116415977478
Epoch 1430, training loss: 62.43090057373047 = 0.07081761211156845 + 10.0 * 6.236008644104004
Epoch 1430, val loss: 0.9916500449180603
Epoch 1440, training loss: 62.48573303222656 = 0.0691070705652237 + 10.0 * 6.241662502288818
Epoch 1440, val loss: 0.995353102684021
Epoch 1450, training loss: 62.45000076293945 = 0.06739471852779388 + 10.0 * 6.238260746002197
Epoch 1450, val loss: 0.9987621307373047
Epoch 1460, training loss: 62.449562072753906 = 0.06575772911310196 + 10.0 * 6.238380432128906
Epoch 1460, val loss: 1.002402663230896
Epoch 1470, training loss: 62.43198776245117 = 0.06417347490787506 + 10.0 * 6.236781120300293
Epoch 1470, val loss: 1.0059884786605835
Epoch 1480, training loss: 62.411441802978516 = 0.06266097724437714 + 10.0 * 6.234878063201904
Epoch 1480, val loss: 1.0098049640655518
Epoch 1490, training loss: 62.39444351196289 = 0.06118294969201088 + 10.0 * 6.233325958251953
Epoch 1490, val loss: 1.0133525133132935
Epoch 1500, training loss: 62.39805603027344 = 0.05976080149412155 + 10.0 * 6.233829498291016
Epoch 1500, val loss: 1.0170546770095825
Epoch 1510, training loss: 62.46327590942383 = 0.05838017538189888 + 10.0 * 6.240489482879639
Epoch 1510, val loss: 1.0207558870315552
Epoch 1520, training loss: 62.425437927246094 = 0.057037778198719025 + 10.0 * 6.23684024810791
Epoch 1520, val loss: 1.0240510702133179
Epoch 1530, training loss: 62.38126754760742 = 0.0557103306055069 + 10.0 * 6.232555866241455
Epoch 1530, val loss: 1.0274806022644043
Epoch 1540, training loss: 62.382713317871094 = 0.05445631593465805 + 10.0 * 6.232825756072998
Epoch 1540, val loss: 1.0312409400939941
Epoch 1550, training loss: 62.42570877075195 = 0.053250137716531754 + 10.0 * 6.237246036529541
Epoch 1550, val loss: 1.034708023071289
Epoch 1560, training loss: 62.3819580078125 = 0.05203859508037567 + 10.0 * 6.232991695404053
Epoch 1560, val loss: 1.0382081270217896
Epoch 1570, training loss: 62.370723724365234 = 0.05089222639799118 + 10.0 * 6.231983184814453
Epoch 1570, val loss: 1.0417709350585938
Epoch 1580, training loss: 62.39082336425781 = 0.049785226583480835 + 10.0 * 6.234103679656982
Epoch 1580, val loss: 1.0453730821609497
Epoch 1590, training loss: 62.3568115234375 = 0.04869232699275017 + 10.0 * 6.230812072753906
Epoch 1590, val loss: 1.0488778352737427
Epoch 1600, training loss: 62.35573196411133 = 0.04764023423194885 + 10.0 * 6.230809211730957
Epoch 1600, val loss: 1.0524629354476929
Epoch 1610, training loss: 62.34703826904297 = 0.046624694019556046 + 10.0 * 6.23004150390625
Epoch 1610, val loss: 1.0560890436172485
Epoch 1620, training loss: 62.405181884765625 = 0.04565846174955368 + 10.0 * 6.235952377319336
Epoch 1620, val loss: 1.059639573097229
Epoch 1630, training loss: 62.352535247802734 = 0.04466169327497482 + 10.0 * 6.23078727722168
Epoch 1630, val loss: 1.0626364946365356
Epoch 1640, training loss: 62.34790802001953 = 0.04373760148882866 + 10.0 * 6.230416774749756
Epoch 1640, val loss: 1.0663801431655884
Epoch 1650, training loss: 62.35812759399414 = 0.04282834380865097 + 10.0 * 6.23153018951416
Epoch 1650, val loss: 1.0694501399993896
Epoch 1660, training loss: 62.34762954711914 = 0.041949570178985596 + 10.0 * 6.230567932128906
Epoch 1660, val loss: 1.0729317665100098
Epoch 1670, training loss: 62.32748794555664 = 0.04108989238739014 + 10.0 * 6.228640079498291
Epoch 1670, val loss: 1.076277256011963
Epoch 1680, training loss: 62.318626403808594 = 0.04026515781879425 + 10.0 * 6.2278361320495605
Epoch 1680, val loss: 1.0796658992767334
Epoch 1690, training loss: 62.32701110839844 = 0.03946101292967796 + 10.0 * 6.228754997253418
Epoch 1690, val loss: 1.082924723625183
Epoch 1700, training loss: 62.38152313232422 = 0.03868351876735687 + 10.0 * 6.234283924102783
Epoch 1700, val loss: 1.0858548879623413
Epoch 1710, training loss: 62.32453918457031 = 0.03789759427309036 + 10.0 * 6.228663921356201
Epoch 1710, val loss: 1.0891050100326538
Epoch 1720, training loss: 62.32062530517578 = 0.03715493157505989 + 10.0 * 6.228346824645996
Epoch 1720, val loss: 1.092302680015564
Epoch 1730, training loss: 62.296756744384766 = 0.0364358015358448 + 10.0 * 6.226032257080078
Epoch 1730, val loss: 1.0956416130065918
Epoch 1740, training loss: 62.29311752319336 = 0.03574845567345619 + 10.0 * 6.22573709487915
Epoch 1740, val loss: 1.098915934562683
Epoch 1750, training loss: 62.293853759765625 = 0.035081300884485245 + 10.0 * 6.225877285003662
Epoch 1750, val loss: 1.102097988128662
Epoch 1760, training loss: 62.36155319213867 = 0.03443378582596779 + 10.0 * 6.2327117919921875
Epoch 1760, val loss: 1.1051995754241943
Epoch 1770, training loss: 62.31600570678711 = 0.0337650366127491 + 10.0 * 6.228224277496338
Epoch 1770, val loss: 1.108015775680542
Epoch 1780, training loss: 62.29159164428711 = 0.033141035586595535 + 10.0 * 6.225844860076904
Epoch 1780, val loss: 1.1112017631530762
Epoch 1790, training loss: 62.28128433227539 = 0.032529640942811966 + 10.0 * 6.224875450134277
Epoch 1790, val loss: 1.1144248247146606
Epoch 1800, training loss: 62.331668853759766 = 0.031943123787641525 + 10.0 * 6.2299723625183105
Epoch 1800, val loss: 1.1174421310424805
Epoch 1810, training loss: 62.29610824584961 = 0.031364597380161285 + 10.0 * 6.226474285125732
Epoch 1810, val loss: 1.1203653812408447
Epoch 1820, training loss: 62.2983283996582 = 0.030798081308603287 + 10.0 * 6.226752758026123
Epoch 1820, val loss: 1.1235030889511108
Epoch 1830, training loss: 62.266448974609375 = 0.03024887479841709 + 10.0 * 6.2236199378967285
Epoch 1830, val loss: 1.1263759136199951
Epoch 1840, training loss: 62.266841888427734 = 0.02971913293004036 + 10.0 * 6.223711967468262
Epoch 1840, val loss: 1.1294327974319458
Epoch 1850, training loss: 62.37141799926758 = 0.029216716066002846 + 10.0 * 6.234220027923584
Epoch 1850, val loss: 1.1324107646942139
Epoch 1860, training loss: 62.29618453979492 = 0.02868768386542797 + 10.0 * 6.226749897003174
Epoch 1860, val loss: 1.1349977254867554
Epoch 1870, training loss: 62.251285552978516 = 0.02818591520190239 + 10.0 * 6.2223100662231445
Epoch 1870, val loss: 1.1379153728485107
Epoch 1880, training loss: 62.251747131347656 = 0.02770872786641121 + 10.0 * 6.222403526306152
Epoch 1880, val loss: 1.14082670211792
Epoch 1890, training loss: 62.253963470458984 = 0.027245216071605682 + 10.0 * 6.222671985626221
Epoch 1890, val loss: 1.1436080932617188
Epoch 1900, training loss: 62.3052978515625 = 0.02679271064698696 + 10.0 * 6.227850437164307
Epoch 1900, val loss: 1.1461551189422607
Epoch 1910, training loss: 62.261539459228516 = 0.026335647329688072 + 10.0 * 6.223520278930664
Epoch 1910, val loss: 1.1493732929229736
Epoch 1920, training loss: 62.28157424926758 = 0.025904018431901932 + 10.0 * 6.225566864013672
Epoch 1920, val loss: 1.1519707441329956
Epoch 1930, training loss: 62.2431640625 = 0.025467529892921448 + 10.0 * 6.2217698097229
Epoch 1930, val loss: 1.1546365022659302
Epoch 1940, training loss: 62.24017333984375 = 0.02504969760775566 + 10.0 * 6.221512317657471
Epoch 1940, val loss: 1.1573747396469116
Epoch 1950, training loss: 62.23337936401367 = 0.024652699008584023 + 10.0 * 6.220872402191162
Epoch 1950, val loss: 1.1602603197097778
Epoch 1960, training loss: 62.281715393066406 = 0.02427150309085846 + 10.0 * 6.225744247436523
Epoch 1960, val loss: 1.1629672050476074
Epoch 1970, training loss: 62.238712310791016 = 0.02386331558227539 + 10.0 * 6.221484661102295
Epoch 1970, val loss: 1.1654367446899414
Epoch 1980, training loss: 62.230186462402344 = 0.023489387705922127 + 10.0 * 6.220669746398926
Epoch 1980, val loss: 1.1681568622589111
Epoch 1990, training loss: 62.22200393676758 = 0.023120375350117683 + 10.0 * 6.219888210296631
Epoch 1990, val loss: 1.1708647012710571
Epoch 2000, training loss: 62.222442626953125 = 0.022769669070839882 + 10.0 * 6.219967365264893
Epoch 2000, val loss: 1.1735461950302124
Epoch 2010, training loss: 62.30495071411133 = 0.022425806149840355 + 10.0 * 6.228252410888672
Epoch 2010, val loss: 1.1761829853057861
Epoch 2020, training loss: 62.271732330322266 = 0.02206195704638958 + 10.0 * 6.224967002868652
Epoch 2020, val loss: 1.1785132884979248
Epoch 2030, training loss: 62.223182678222656 = 0.021724119782447815 + 10.0 * 6.2201457023620605
Epoch 2030, val loss: 1.1810009479522705
Epoch 2040, training loss: 62.21257400512695 = 0.02139480598270893 + 10.0 * 6.219117641448975
Epoch 2040, val loss: 1.183664083480835
Epoch 2050, training loss: 62.23454666137695 = 0.021082337945699692 + 10.0 * 6.221346378326416
Epoch 2050, val loss: 1.1861023902893066
Epoch 2060, training loss: 62.21107864379883 = 0.020764371380209923 + 10.0 * 6.21903133392334
Epoch 2060, val loss: 1.1887247562408447
Epoch 2070, training loss: 62.21077346801758 = 0.020457690581679344 + 10.0 * 6.21903133392334
Epoch 2070, val loss: 1.191251277923584
Epoch 2080, training loss: 62.20426940917969 = 0.020159950479865074 + 10.0 * 6.218410968780518
Epoch 2080, val loss: 1.1938313245773315
Epoch 2090, training loss: 62.226585388183594 = 0.019873710349202156 + 10.0 * 6.2206711769104
Epoch 2090, val loss: 1.1961652040481567
Epoch 2100, training loss: 62.21918869018555 = 0.019580120220780373 + 10.0 * 6.219960689544678
Epoch 2100, val loss: 1.1984065771102905
Epoch 2110, training loss: 62.20456314086914 = 0.019296830520033836 + 10.0 * 6.218526363372803
Epoch 2110, val loss: 1.2009105682373047
Epoch 2120, training loss: 62.19475173950195 = 0.019023045897483826 + 10.0 * 6.2175726890563965
Epoch 2120, val loss: 1.2032968997955322
Epoch 2130, training loss: 62.21007537841797 = 0.018756553530693054 + 10.0 * 6.219131946563721
Epoch 2130, val loss: 1.2056641578674316
Epoch 2140, training loss: 62.199974060058594 = 0.018494712188839912 + 10.0 * 6.218148231506348
Epoch 2140, val loss: 1.2081348896026611
Epoch 2150, training loss: 62.20185852050781 = 0.018234247341752052 + 10.0 * 6.218362331390381
Epoch 2150, val loss: 1.210389256477356
Epoch 2160, training loss: 62.18545150756836 = 0.01798132434487343 + 10.0 * 6.216746807098389
Epoch 2160, val loss: 1.2127904891967773
Epoch 2170, training loss: 62.188575744628906 = 0.017737148329615593 + 10.0 * 6.217083930969238
Epoch 2170, val loss: 1.2151132822036743
Epoch 2180, training loss: 62.19986343383789 = 0.01749677211046219 + 10.0 * 6.218236446380615
Epoch 2180, val loss: 1.2173904180526733
Epoch 2190, training loss: 62.18036651611328 = 0.017262572422623634 + 10.0 * 6.216310501098633
Epoch 2190, val loss: 1.2197301387786865
Epoch 2200, training loss: 62.20756530761719 = 0.01703771948814392 + 10.0 * 6.219052791595459
Epoch 2200, val loss: 1.2220875024795532
Epoch 2210, training loss: 62.19365692138672 = 0.01680375076830387 + 10.0 * 6.217685222625732
Epoch 2210, val loss: 1.2240300178527832
Epoch 2220, training loss: 62.17934036254883 = 0.016578681766986847 + 10.0 * 6.216276168823242
Epoch 2220, val loss: 1.2263833284378052
Epoch 2230, training loss: 62.16512680053711 = 0.01636134460568428 + 10.0 * 6.214876651763916
Epoch 2230, val loss: 1.228712558746338
Epoch 2240, training loss: 62.160621643066406 = 0.01615079864859581 + 10.0 * 6.214447021484375
Epoch 2240, val loss: 1.2310125827789307
Epoch 2250, training loss: 62.17245101928711 = 0.015948519110679626 + 10.0 * 6.2156500816345215
Epoch 2250, val loss: 1.2331904172897339
Epoch 2260, training loss: 62.20237350463867 = 0.015746043995022774 + 10.0 * 6.218662738800049
Epoch 2260, val loss: 1.235277533531189
Epoch 2270, training loss: 62.19760513305664 = 0.015545540489256382 + 10.0 * 6.21820592880249
Epoch 2270, val loss: 1.2376205921173096
Epoch 2280, training loss: 62.16685104370117 = 0.015341369435191154 + 10.0 * 6.215150833129883
Epoch 2280, val loss: 1.2395356893539429
Epoch 2290, training loss: 62.15742111206055 = 0.015148569829761982 + 10.0 * 6.214227199554443
Epoch 2290, val loss: 1.241739273071289
Epoch 2300, training loss: 62.18510055541992 = 0.01496610976755619 + 10.0 * 6.217013359069824
Epoch 2300, val loss: 1.2436915636062622
Epoch 2310, training loss: 62.14916229248047 = 0.014776859432458878 + 10.0 * 6.213438510894775
Epoch 2310, val loss: 1.2459676265716553
Epoch 2320, training loss: 62.148075103759766 = 0.014596820808947086 + 10.0 * 6.213347911834717
Epoch 2320, val loss: 1.2479954957962036
Epoch 2330, training loss: 62.147911071777344 = 0.014421077445149422 + 10.0 * 6.213349342346191
Epoch 2330, val loss: 1.250054955482483
Epoch 2340, training loss: 62.194862365722656 = 0.014252074994146824 + 10.0 * 6.2180609703063965
Epoch 2340, val loss: 1.2520954608917236
Epoch 2350, training loss: 62.15525436401367 = 0.014072176069021225 + 10.0 * 6.214118003845215
Epoch 2350, val loss: 1.25408136844635
Epoch 2360, training loss: 62.16244888305664 = 0.013905851170420647 + 10.0 * 6.2148542404174805
Epoch 2360, val loss: 1.255956768989563
Epoch 2370, training loss: 62.158573150634766 = 0.013737930916249752 + 10.0 * 6.214483737945557
Epoch 2370, val loss: 1.257892370223999
Epoch 2380, training loss: 62.14662170410156 = 0.013574576005339622 + 10.0 * 6.2133049964904785
Epoch 2380, val loss: 1.259721040725708
Epoch 2390, training loss: 62.141239166259766 = 0.013417743146419525 + 10.0 * 6.212782382965088
Epoch 2390, val loss: 1.2618204355239868
Epoch 2400, training loss: 62.1560173034668 = 0.013264888897538185 + 10.0 * 6.214275360107422
Epoch 2400, val loss: 1.2637325525283813
Epoch 2410, training loss: 62.13784408569336 = 0.013110150583088398 + 10.0 * 6.212473392486572
Epoch 2410, val loss: 1.2656631469726562
Epoch 2420, training loss: 62.140506744384766 = 0.012958372011780739 + 10.0 * 6.212754726409912
Epoch 2420, val loss: 1.2676048278808594
Epoch 2430, training loss: 62.12761688232422 = 0.012810646556317806 + 10.0 * 6.211480617523193
Epoch 2430, val loss: 1.2697219848632812
Epoch 2440, training loss: 62.17915344238281 = 0.012672919780015945 + 10.0 * 6.216648101806641
Epoch 2440, val loss: 1.2715643644332886
Epoch 2450, training loss: 62.14307403564453 = 0.012523324228823185 + 10.0 * 6.21305513381958
Epoch 2450, val loss: 1.2730377912521362
Epoch 2460, training loss: 62.1363525390625 = 0.012381656095385551 + 10.0 * 6.21239709854126
Epoch 2460, val loss: 1.2752189636230469
Epoch 2470, training loss: 62.14023971557617 = 0.012245015241205692 + 10.0 * 6.212799549102783
Epoch 2470, val loss: 1.277132272720337
Epoch 2480, training loss: 62.120033264160156 = 0.012106814421713352 + 10.0 * 6.210792541503906
Epoch 2480, val loss: 1.2789576053619385
Epoch 2490, training loss: 62.134849548339844 = 0.011976019479334354 + 10.0 * 6.212287425994873
Epoch 2490, val loss: 1.280771017074585
Epoch 2500, training loss: 62.118221282958984 = 0.01184505969285965 + 10.0 * 6.21063756942749
Epoch 2500, val loss: 1.28254234790802
Epoch 2510, training loss: 62.13009262084961 = 0.011719867587089539 + 10.0 * 6.211837291717529
Epoch 2510, val loss: 1.2843527793884277
Epoch 2520, training loss: 62.18264389038086 = 0.011589745990931988 + 10.0 * 6.217105388641357
Epoch 2520, val loss: 1.2860407829284668
Epoch 2530, training loss: 62.12049102783203 = 0.011464002542197704 + 10.0 * 6.210902690887451
Epoch 2530, val loss: 1.287596344947815
Epoch 2540, training loss: 62.108726501464844 = 0.011337540112435818 + 10.0 * 6.209738731384277
Epoch 2540, val loss: 1.2895392179489136
Epoch 2550, training loss: 62.102516174316406 = 0.011222871020436287 + 10.0 * 6.209129333496094
Epoch 2550, val loss: 1.2913702726364136
Epoch 2560, training loss: 62.10365676879883 = 0.011107735335826874 + 10.0 * 6.209254741668701
Epoch 2560, val loss: 1.2931902408599854
Epoch 2570, training loss: 62.17851638793945 = 0.010997182689607143 + 10.0 * 6.216752052307129
Epoch 2570, val loss: 1.294729232788086
Epoch 2580, training loss: 62.13315963745117 = 0.010876040905714035 + 10.0 * 6.212228298187256
Epoch 2580, val loss: 1.2964856624603271
Epoch 2590, training loss: 62.11015319824219 = 0.010763744823634624 + 10.0 * 6.209939002990723
Epoch 2590, val loss: 1.2980070114135742
Epoch 2600, training loss: 62.11957931518555 = 0.010653183795511723 + 10.0 * 6.210892677307129
Epoch 2600, val loss: 1.2996982336044312
Epoch 2610, training loss: 62.096824645996094 = 0.010545165278017521 + 10.0 * 6.208628177642822
Epoch 2610, val loss: 1.301506757736206
Epoch 2620, training loss: 62.09654998779297 = 0.01043887808918953 + 10.0 * 6.208611488342285
Epoch 2620, val loss: 1.303397297859192
Epoch 2630, training loss: 62.14316940307617 = 0.010337168350815773 + 10.0 * 6.213283061981201
Epoch 2630, val loss: 1.304841160774231
Epoch 2640, training loss: 62.10997009277344 = 0.010229405015707016 + 10.0 * 6.2099738121032715
Epoch 2640, val loss: 1.3064041137695312
Epoch 2650, training loss: 62.103729248046875 = 0.010128779336810112 + 10.0 * 6.209360122680664
Epoch 2650, val loss: 1.308242917060852
Epoch 2660, training loss: 62.10403823852539 = 0.010027479380369186 + 10.0 * 6.2094011306762695
Epoch 2660, val loss: 1.3098382949829102
Epoch 2670, training loss: 62.09055709838867 = 0.009929705411195755 + 10.0 * 6.208062648773193
Epoch 2670, val loss: 1.3115020990371704
Epoch 2680, training loss: 62.082218170166016 = 0.009832075797021389 + 10.0 * 6.207238674163818
Epoch 2680, val loss: 1.313180923461914
Epoch 2690, training loss: 62.10441207885742 = 0.009739013388752937 + 10.0 * 6.20946741104126
Epoch 2690, val loss: 1.3148070573806763
Epoch 2700, training loss: 62.088497161865234 = 0.009642635472118855 + 10.0 * 6.207885265350342
Epoch 2700, val loss: 1.3162641525268555
Epoch 2710, training loss: 62.104087829589844 = 0.009550157934427261 + 10.0 * 6.209453582763672
Epoch 2710, val loss: 1.3176363706588745
Epoch 2720, training loss: 62.108795166015625 = 0.009460068307816982 + 10.0 * 6.209933280944824
Epoch 2720, val loss: 1.3191664218902588
Epoch 2730, training loss: 62.086463928222656 = 0.009368469938635826 + 10.0 * 6.207709312438965
Epoch 2730, val loss: 1.3206485509872437
Epoch 2740, training loss: 62.07837677001953 = 0.009279029443860054 + 10.0 * 6.206910133361816
Epoch 2740, val loss: 1.3223817348480225
Epoch 2750, training loss: 62.074371337890625 = 0.009194272570312023 + 10.0 * 6.206517696380615
Epoch 2750, val loss: 1.3238739967346191
Epoch 2760, training loss: 62.0908203125 = 0.00911118183284998 + 10.0 * 6.2081708908081055
Epoch 2760, val loss: 1.325264811515808
Epoch 2770, training loss: 62.10704803466797 = 0.009023887105286121 + 10.0 * 6.209802150726318
Epoch 2770, val loss: 1.3264330625534058
Epoch 2780, training loss: 62.10483932495117 = 0.008936040103435516 + 10.0 * 6.209589958190918
Epoch 2780, val loss: 1.327918291091919
Epoch 2790, training loss: 62.06736373901367 = 0.008854555897414684 + 10.0 * 6.205851078033447
Epoch 2790, val loss: 1.3294224739074707
Epoch 2800, training loss: 62.06184768676758 = 0.008773751556873322 + 10.0 * 6.205307483673096
Epoch 2800, val loss: 1.3309242725372314
Epoch 2810, training loss: 62.06315612792969 = 0.008696089498698711 + 10.0 * 6.205445766448975
Epoch 2810, val loss: 1.3324021100997925
Epoch 2820, training loss: 62.14772033691406 = 0.00862324982881546 + 10.0 * 6.21390962600708
Epoch 2820, val loss: 1.3335641622543335
Epoch 2830, training loss: 62.09406280517578 = 0.008538417518138885 + 10.0 * 6.208552360534668
Epoch 2830, val loss: 1.3352274894714355
Epoch 2840, training loss: 62.06809616088867 = 0.008460155688226223 + 10.0 * 6.205963611602783
Epoch 2840, val loss: 1.336388349533081
Epoch 2850, training loss: 62.057071685791016 = 0.008386123925447464 + 10.0 * 6.204868793487549
Epoch 2850, val loss: 1.3380727767944336
Epoch 2860, training loss: 62.05634307861328 = 0.008315877988934517 + 10.0 * 6.204802513122559
Epoch 2860, val loss: 1.3394246101379395
Epoch 2870, training loss: 62.096839904785156 = 0.008246761746704578 + 10.0 * 6.208859443664551
Epoch 2870, val loss: 1.3407725095748901
Epoch 2880, training loss: 62.05961608886719 = 0.008171305991709232 + 10.0 * 6.20514440536499
Epoch 2880, val loss: 1.3421164751052856
Epoch 2890, training loss: 62.10105895996094 = 0.008102546446025372 + 10.0 * 6.209295749664307
Epoch 2890, val loss: 1.3434531688690186
Epoch 2900, training loss: 62.06512451171875 = 0.008026792667806149 + 10.0 * 6.205709934234619
Epoch 2900, val loss: 1.3446046113967896
Epoch 2910, training loss: 62.05143356323242 = 0.007958987727761269 + 10.0 * 6.204347610473633
Epoch 2910, val loss: 1.3459150791168213
Epoch 2920, training loss: 62.046451568603516 = 0.007892873138189316 + 10.0 * 6.203855991363525
Epoch 2920, val loss: 1.3472950458526611
Epoch 2930, training loss: 62.042755126953125 = 0.007827327586710453 + 10.0 * 6.203492641448975
Epoch 2930, val loss: 1.3486815690994263
Epoch 2940, training loss: 62.075923919677734 = 0.0077643729746341705 + 10.0 * 6.20681619644165
Epoch 2940, val loss: 1.3499222993850708
Epoch 2950, training loss: 62.05691909790039 = 0.007698430214077234 + 10.0 * 6.204922199249268
Epoch 2950, val loss: 1.350895881652832
Epoch 2960, training loss: 62.04183578491211 = 0.007631171494722366 + 10.0 * 6.203420639038086
Epoch 2960, val loss: 1.3522452116012573
Epoch 2970, training loss: 62.04423904418945 = 0.007567903026938438 + 10.0 * 6.203667163848877
Epoch 2970, val loss: 1.3535743951797485
Epoch 2980, training loss: 62.06642150878906 = 0.0075078257359564304 + 10.0 * 6.2058916091918945
Epoch 2980, val loss: 1.3549073934555054
Epoch 2990, training loss: 62.05959701538086 = 0.007447108626365662 + 10.0 * 6.205214977264404
Epoch 2990, val loss: 1.356086254119873
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8070637849235636
The final CL Acc:0.73827, 0.01522, The final GNN Acc:0.81005, 0.00287
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13140])
remove edge: torch.Size([2, 7774])
updated graph: torch.Size([2, 10358])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.93295288085938 = 1.9646650552749634 + 10.0 * 8.59682846069336
Epoch 0, val loss: 1.971482515335083
Epoch 10, training loss: 87.9152603149414 = 1.953926920890808 + 10.0 * 8.5961332321167
Epoch 10, val loss: 1.9609228372573853
Epoch 20, training loss: 87.85369873046875 = 1.9403225183486938 + 10.0 * 8.591337203979492
Epoch 20, val loss: 1.9470093250274658
Epoch 30, training loss: 87.53357696533203 = 1.9224812984466553 + 10.0 * 8.56110954284668
Epoch 30, val loss: 1.9287066459655762
Epoch 40, training loss: 85.89118957519531 = 1.901304006576538 + 10.0 * 8.398988723754883
Epoch 40, val loss: 1.9074745178222656
Epoch 50, training loss: 78.37216186523438 = 1.8781280517578125 + 10.0 * 7.649403095245361
Epoch 50, val loss: 1.8837515115737915
Epoch 60, training loss: 74.09172821044922 = 1.8598393201828003 + 10.0 * 7.223188400268555
Epoch 60, val loss: 1.8661526441574097
Epoch 70, training loss: 71.79109954833984 = 1.8446238040924072 + 10.0 * 6.994647026062012
Epoch 70, val loss: 1.8509753942489624
Epoch 80, training loss: 70.63700103759766 = 1.8292624950408936 + 10.0 * 6.880774021148682
Epoch 80, val loss: 1.8354082107543945
Epoch 90, training loss: 70.0268325805664 = 1.8159154653549194 + 10.0 * 6.821091175079346
Epoch 90, val loss: 1.8217971324920654
Epoch 100, training loss: 69.69693756103516 = 1.8040177822113037 + 10.0 * 6.789292335510254
Epoch 100, val loss: 1.809501051902771
Epoch 110, training loss: 69.40894317626953 = 1.7929089069366455 + 10.0 * 6.761603355407715
Epoch 110, val loss: 1.798020362854004
Epoch 120, training loss: 69.1124267578125 = 1.7826262712478638 + 10.0 * 6.732980728149414
Epoch 120, val loss: 1.787366509437561
Epoch 130, training loss: 68.77294158935547 = 1.7736135721206665 + 10.0 * 6.69993257522583
Epoch 130, val loss: 1.7780240774154663
Epoch 140, training loss: 68.39225769042969 = 1.7659120559692383 + 10.0 * 6.662634372711182
Epoch 140, val loss: 1.7700231075286865
Epoch 150, training loss: 68.05303955078125 = 1.758629560470581 + 10.0 * 6.629441261291504
Epoch 150, val loss: 1.7624579668045044
Epoch 160, training loss: 67.7359848022461 = 1.7501271963119507 + 10.0 * 6.598585605621338
Epoch 160, val loss: 1.7540757656097412
Epoch 170, training loss: 67.47643280029297 = 1.7412129640579224 + 10.0 * 6.573522090911865
Epoch 170, val loss: 1.7453898191452026
Epoch 180, training loss: 67.20668029785156 = 1.7315925359725952 + 10.0 * 6.547508716583252
Epoch 180, val loss: 1.73629891872406
Epoch 190, training loss: 66.92868041992188 = 1.7215708494186401 + 10.0 * 6.5207109451293945
Epoch 190, val loss: 1.7269723415374756
Epoch 200, training loss: 66.66842651367188 = 1.7112257480621338 + 10.0 * 6.495719909667969
Epoch 200, val loss: 1.717456340789795
Epoch 210, training loss: 66.48538970947266 = 1.6998093128204346 + 10.0 * 6.478558540344238
Epoch 210, val loss: 1.7069998979568481
Epoch 220, training loss: 66.28352355957031 = 1.686911940574646 + 10.0 * 6.45966100692749
Epoch 220, val loss: 1.695464015007019
Epoch 230, training loss: 66.1234130859375 = 1.6727955341339111 + 10.0 * 6.445061206817627
Epoch 230, val loss: 1.6827375888824463
Epoch 240, training loss: 65.98209381103516 = 1.6573141813278198 + 10.0 * 6.432478427886963
Epoch 240, val loss: 1.6688599586486816
Epoch 250, training loss: 65.90108489990234 = 1.6403275728225708 + 10.0 * 6.426075458526611
Epoch 250, val loss: 1.653829574584961
Epoch 260, training loss: 65.74391174316406 = 1.6219884157180786 + 10.0 * 6.4121928215026855
Epoch 260, val loss: 1.637527585029602
Epoch 270, training loss: 65.6250991821289 = 1.6022859811782837 + 10.0 * 6.402280807495117
Epoch 270, val loss: 1.6202163696289062
Epoch 280, training loss: 65.5215072631836 = 1.5810999870300293 + 10.0 * 6.394040584564209
Epoch 280, val loss: 1.6017612218856812
Epoch 290, training loss: 65.44644927978516 = 1.5584620237350464 + 10.0 * 6.388798713684082
Epoch 290, val loss: 1.5820789337158203
Epoch 300, training loss: 65.33397674560547 = 1.5343599319458008 + 10.0 * 6.3799614906311035
Epoch 300, val loss: 1.5613869428634644
Epoch 310, training loss: 65.24388122558594 = 1.5090301036834717 + 10.0 * 6.373485088348389
Epoch 310, val loss: 1.5397648811340332
Epoch 320, training loss: 65.174072265625 = 1.4825494289398193 + 10.0 * 6.369152069091797
Epoch 320, val loss: 1.5171366930007935
Epoch 330, training loss: 65.09429168701172 = 1.45464026927948 + 10.0 * 6.363965034484863
Epoch 330, val loss: 1.4937703609466553
Epoch 340, training loss: 65.0023422241211 = 1.426148533821106 + 10.0 * 6.357619285583496
Epoch 340, val loss: 1.4699090719223022
Epoch 350, training loss: 64.92391204833984 = 1.3967926502227783 + 10.0 * 6.352712154388428
Epoch 350, val loss: 1.4457030296325684
Epoch 360, training loss: 64.88797760009766 = 1.3667620420455933 + 10.0 * 6.352121829986572
Epoch 360, val loss: 1.4212291240692139
Epoch 370, training loss: 64.8033447265625 = 1.3362598419189453 + 10.0 * 6.346708297729492
Epoch 370, val loss: 1.3965978622436523
Epoch 380, training loss: 64.71931457519531 = 1.305719256401062 + 10.0 * 6.341359615325928
Epoch 380, val loss: 1.3720744848251343
Epoch 390, training loss: 64.64962768554688 = 1.275047779083252 + 10.0 * 6.337457656860352
Epoch 390, val loss: 1.3479596376419067
Epoch 400, training loss: 64.59477233886719 = 1.2443243265151978 + 10.0 * 6.3350443840026855
Epoch 400, val loss: 1.3242648839950562
Epoch 410, training loss: 64.5726318359375 = 1.213862419128418 + 10.0 * 6.335876941680908
Epoch 410, val loss: 1.3004038333892822
Epoch 420, training loss: 64.47034454345703 = 1.1836330890655518 + 10.0 * 6.328670978546143
Epoch 420, val loss: 1.277775764465332
Epoch 430, training loss: 64.4017105102539 = 1.1538602113723755 + 10.0 * 6.324784755706787
Epoch 430, val loss: 1.2556889057159424
Epoch 440, training loss: 64.3440933227539 = 1.1245429515838623 + 10.0 * 6.321954727172852
Epoch 440, val loss: 1.2341694831848145
Epoch 450, training loss: 64.28850555419922 = 1.0956193208694458 + 10.0 * 6.31928825378418
Epoch 450, val loss: 1.2132139205932617
Epoch 460, training loss: 64.2598648071289 = 1.0669097900390625 + 10.0 * 6.319295406341553
Epoch 460, val loss: 1.1926753520965576
Epoch 470, training loss: 64.20418548583984 = 1.039018154144287 + 10.0 * 6.316516399383545
Epoch 470, val loss: 1.1728872060775757
Epoch 480, training loss: 64.1302261352539 = 1.0116301774978638 + 10.0 * 6.311859607696533
Epoch 480, val loss: 1.1539535522460938
Epoch 490, training loss: 64.08198547363281 = 0.9848167896270752 + 10.0 * 6.309716701507568
Epoch 490, val loss: 1.1356556415557861
Epoch 500, training loss: 64.03125 = 0.9584892392158508 + 10.0 * 6.307275772094727
Epoch 500, val loss: 1.1178853511810303
Epoch 510, training loss: 64.06952667236328 = 0.9326626658439636 + 10.0 * 6.313686847686768
Epoch 510, val loss: 1.1005429029464722
Epoch 520, training loss: 63.96232986450195 = 0.9070965051651001 + 10.0 * 6.30552339553833
Epoch 520, val loss: 1.083949327468872
Epoch 530, training loss: 63.90181350708008 = 0.8824587464332581 + 10.0 * 6.30193567276001
Epoch 530, val loss: 1.068176507949829
Epoch 540, training loss: 63.85233688354492 = 0.8584406971931458 + 10.0 * 6.299389839172363
Epoch 540, val loss: 1.0529985427856445
Epoch 550, training loss: 63.80955123901367 = 0.8349587917327881 + 10.0 * 6.297459602355957
Epoch 550, val loss: 1.038422703742981
Epoch 560, training loss: 63.80659484863281 = 0.8119350075721741 + 10.0 * 6.299466133117676
Epoch 560, val loss: 1.0243715047836304
Epoch 570, training loss: 63.73130798339844 = 0.7897255420684814 + 10.0 * 6.294157981872559
Epoch 570, val loss: 1.0108388662338257
Epoch 580, training loss: 63.692691802978516 = 0.7681103348731995 + 10.0 * 6.2924580574035645
Epoch 580, val loss: 0.997998833656311
Epoch 590, training loss: 63.684669494628906 = 0.7471414804458618 + 10.0 * 6.293752670288086
Epoch 590, val loss: 0.985791802406311
Epoch 600, training loss: 63.62977600097656 = 0.72679603099823 + 10.0 * 6.290297985076904
Epoch 600, val loss: 0.9741520881652832
Epoch 610, training loss: 63.58793640136719 = 0.7069795727729797 + 10.0 * 6.288095951080322
Epoch 610, val loss: 0.9632622599601746
Epoch 620, training loss: 63.550445556640625 = 0.6878206729888916 + 10.0 * 6.286262512207031
Epoch 620, val loss: 0.9528014063835144
Epoch 630, training loss: 63.51106643676758 = 0.6693503856658936 + 10.0 * 6.2841715812683105
Epoch 630, val loss: 0.9430240392684937
Epoch 640, training loss: 63.48311233520508 = 0.651418924331665 + 10.0 * 6.283169269561768
Epoch 640, val loss: 0.9337789416313171
Epoch 650, training loss: 63.506080627441406 = 0.6340354681015015 + 10.0 * 6.287204265594482
Epoch 650, val loss: 0.9248945713043213
Epoch 660, training loss: 63.44818878173828 = 0.6170271635055542 + 10.0 * 6.283116340637207
Epoch 660, val loss: 0.916633129119873
Epoch 670, training loss: 63.393062591552734 = 0.60074383020401 + 10.0 * 6.279232025146484
Epoch 670, val loss: 0.9090297818183899
Epoch 680, training loss: 63.35978698730469 = 0.5849414467811584 + 10.0 * 6.27748441696167
Epoch 680, val loss: 0.9019487500190735
Epoch 690, training loss: 63.332481384277344 = 0.5696331262588501 + 10.0 * 6.276284694671631
Epoch 690, val loss: 0.8953177332878113
Epoch 700, training loss: 63.38056945800781 = 0.5547142624855042 + 10.0 * 6.282585620880127
Epoch 700, val loss: 0.8890269994735718
Epoch 710, training loss: 63.28568649291992 = 0.5401795506477356 + 10.0 * 6.274550437927246
Epoch 710, val loss: 0.8830030560493469
Epoch 720, training loss: 63.260162353515625 = 0.5261112451553345 + 10.0 * 6.273405075073242
Epoch 720, val loss: 0.8776426315307617
Epoch 730, training loss: 63.230316162109375 = 0.5124735832214355 + 10.0 * 6.27178430557251
Epoch 730, val loss: 0.8726964592933655
Epoch 740, training loss: 63.204185485839844 = 0.4991800785064697 + 10.0 * 6.270500659942627
Epoch 740, val loss: 0.8681349754333496
Epoch 750, training loss: 63.20142364501953 = 0.48619896173477173 + 10.0 * 6.271522521972656
Epoch 750, val loss: 0.8638260364532471
Epoch 760, training loss: 63.182918548583984 = 0.4732230007648468 + 10.0 * 6.270969390869141
Epoch 760, val loss: 0.8599523901939392
Epoch 770, training loss: 63.13747024536133 = 0.4607711136341095 + 10.0 * 6.267670154571533
Epoch 770, val loss: 0.8561899662017822
Epoch 780, training loss: 63.119022369384766 = 0.4485216736793518 + 10.0 * 6.267050266265869
Epoch 780, val loss: 0.8528323769569397
Epoch 790, training loss: 63.09050369262695 = 0.43650445342063904 + 10.0 * 6.265399932861328
Epoch 790, val loss: 0.8497965931892395
Epoch 800, training loss: 63.101715087890625 = 0.42466139793395996 + 10.0 * 6.26770544052124
Epoch 800, val loss: 0.8470739722251892
Epoch 810, training loss: 63.06829833984375 = 0.41295915842056274 + 10.0 * 6.265533924102783
Epoch 810, val loss: 0.844275951385498
Epoch 820, training loss: 63.031944274902344 = 0.40146055817604065 + 10.0 * 6.2630486488342285
Epoch 820, val loss: 0.8419216871261597
Epoch 830, training loss: 63.012939453125 = 0.3901238441467285 + 10.0 * 6.26228141784668
Epoch 830, val loss: 0.8397363424301147
Epoch 840, training loss: 63.02550506591797 = 0.3789355456829071 + 10.0 * 6.264657020568848
Epoch 840, val loss: 0.8376286029815674
Epoch 850, training loss: 62.978206634521484 = 0.36792609095573425 + 10.0 * 6.261027812957764
Epoch 850, val loss: 0.8357623219490051
Epoch 860, training loss: 62.9565315246582 = 0.35708510875701904 + 10.0 * 6.259944438934326
Epoch 860, val loss: 0.834022581577301
Epoch 870, training loss: 62.942012786865234 = 0.3464340269565582 + 10.0 * 6.259557723999023
Epoch 870, val loss: 0.8324564099311829
Epoch 880, training loss: 62.92449951171875 = 0.33594268560409546 + 10.0 * 6.258855819702148
Epoch 880, val loss: 0.8310564756393433
Epoch 890, training loss: 62.897010803222656 = 0.3256542682647705 + 10.0 * 6.25713586807251
Epoch 890, val loss: 0.8297836184501648
Epoch 900, training loss: 62.91286087036133 = 0.3155083954334259 + 10.0 * 6.259735107421875
Epoch 900, val loss: 0.8286994695663452
Epoch 910, training loss: 62.879302978515625 = 0.30552253127098083 + 10.0 * 6.257378101348877
Epoch 910, val loss: 0.8275888562202454
Epoch 920, training loss: 62.845149993896484 = 0.2957550585269928 + 10.0 * 6.254939556121826
Epoch 920, val loss: 0.8269059062004089
Epoch 930, training loss: 62.8231201171875 = 0.2862510681152344 + 10.0 * 6.253686904907227
Epoch 930, val loss: 0.8263320326805115
Epoch 940, training loss: 62.82295227050781 = 0.27698269486427307 + 10.0 * 6.254597187042236
Epoch 940, val loss: 0.8258988261222839
Epoch 950, training loss: 62.7912483215332 = 0.2677640914916992 + 10.0 * 6.25234842300415
Epoch 950, val loss: 0.8257079720497131
Epoch 960, training loss: 62.79443359375 = 0.25892770290374756 + 10.0 * 6.2535505294799805
Epoch 960, val loss: 0.8255434632301331
Epoch 970, training loss: 62.75640106201172 = 0.25026044249534607 + 10.0 * 6.250614166259766
Epoch 970, val loss: 0.8257021307945251
Epoch 980, training loss: 62.74260711669922 = 0.24185939133167267 + 10.0 * 6.250074863433838
Epoch 980, val loss: 0.8260957598686218
Epoch 990, training loss: 62.72835922241211 = 0.2337135672569275 + 10.0 * 6.249464511871338
Epoch 990, val loss: 0.826541543006897
Epoch 1000, training loss: 62.75239944458008 = 0.22572922706604004 + 10.0 * 6.25266695022583
Epoch 1000, val loss: 0.827107310295105
Epoch 1010, training loss: 62.70850372314453 = 0.21795615553855896 + 10.0 * 6.249054908752441
Epoch 1010, val loss: 0.8278858065605164
Epoch 1020, training loss: 62.690025329589844 = 0.21046851575374603 + 10.0 * 6.247955799102783
Epoch 1020, val loss: 0.8288448452949524
Epoch 1030, training loss: 62.6891975402832 = 0.20322221517562866 + 10.0 * 6.248597621917725
Epoch 1030, val loss: 0.8299301266670227
Epoch 1040, training loss: 62.661624908447266 = 0.19620105624198914 + 10.0 * 6.246542453765869
Epoch 1040, val loss: 0.8312858939170837
Epoch 1050, training loss: 62.67420959472656 = 0.18942834436893463 + 10.0 * 6.248477935791016
Epoch 1050, val loss: 0.8327674269676208
Epoch 1060, training loss: 62.662044525146484 = 0.18282939493656158 + 10.0 * 6.247921466827393
Epoch 1060, val loss: 0.8342980146408081
Epoch 1070, training loss: 62.62553405761719 = 0.17649675905704498 + 10.0 * 6.244903564453125
Epoch 1070, val loss: 0.8361817002296448
Epoch 1080, training loss: 62.61133575439453 = 0.17044655978679657 + 10.0 * 6.244088649749756
Epoch 1080, val loss: 0.8381780385971069
Epoch 1090, training loss: 62.599754333496094 = 0.1646220088005066 + 10.0 * 6.243513107299805
Epoch 1090, val loss: 0.840363085269928
Epoch 1100, training loss: 62.59843444824219 = 0.15899339318275452 + 10.0 * 6.24394416809082
Epoch 1100, val loss: 0.8427361249923706
Epoch 1110, training loss: 62.63883590698242 = 0.1535208523273468 + 10.0 * 6.248531341552734
Epoch 1110, val loss: 0.8453675508499146
Epoch 1120, training loss: 62.58575439453125 = 0.14834775030612946 + 10.0 * 6.243741035461426
Epoch 1120, val loss: 0.8477031588554382
Epoch 1130, training loss: 62.55775451660156 = 0.14329646527767181 + 10.0 * 6.241446018218994
Epoch 1130, val loss: 0.8506431579589844
Epoch 1140, training loss: 62.542301177978516 = 0.13852958381175995 + 10.0 * 6.240376949310303
Epoch 1140, val loss: 0.853529691696167
Epoch 1150, training loss: 62.58261489868164 = 0.13390208780765533 + 10.0 * 6.244871139526367
Epoch 1150, val loss: 0.8566644787788391
Epoch 1160, training loss: 62.556880950927734 = 0.1294708400964737 + 10.0 * 6.242741107940674
Epoch 1160, val loss: 0.8595104813575745
Epoch 1170, training loss: 62.52086639404297 = 0.1251654028892517 + 10.0 * 6.239570140838623
Epoch 1170, val loss: 0.8629533648490906
Epoch 1180, training loss: 62.518768310546875 = 0.12108395993709564 + 10.0 * 6.2397685050964355
Epoch 1180, val loss: 0.8662368655204773
Epoch 1190, training loss: 62.49660110473633 = 0.11717940866947174 + 10.0 * 6.237942218780518
Epoch 1190, val loss: 0.8696259260177612
Epoch 1200, training loss: 62.48563003540039 = 0.11342154443264008 + 10.0 * 6.237220764160156
Epoch 1200, val loss: 0.8732262849807739
Epoch 1210, training loss: 62.500267028808594 = 0.10981140285730362 + 10.0 * 6.2390456199646
Epoch 1210, val loss: 0.8768208026885986
Epoch 1220, training loss: 62.4773063659668 = 0.10629774630069733 + 10.0 * 6.237101078033447
Epoch 1220, val loss: 0.8805505037307739
Epoch 1230, training loss: 62.4735107421875 = 0.10293609648942947 + 10.0 * 6.237057685852051
Epoch 1230, val loss: 0.8842870593070984
Epoch 1240, training loss: 62.480995178222656 = 0.09971264004707336 + 10.0 * 6.238128185272217
Epoch 1240, val loss: 0.8882107138633728
Epoch 1250, training loss: 62.463497161865234 = 0.09664848446846008 + 10.0 * 6.236684799194336
Epoch 1250, val loss: 0.8918843865394592
Epoch 1260, training loss: 62.43637466430664 = 0.09369219839572906 + 10.0 * 6.2342681884765625
Epoch 1260, val loss: 0.8959224820137024
Epoch 1270, training loss: 62.43191146850586 = 0.09086334705352783 + 10.0 * 6.234105110168457
Epoch 1270, val loss: 0.8999788165092468
Epoch 1280, training loss: 62.43998718261719 = 0.08815868943929672 + 10.0 * 6.235182762145996
Epoch 1280, val loss: 0.9039313793182373
Epoch 1290, training loss: 62.4473991394043 = 0.08553215861320496 + 10.0 * 6.236186504364014
Epoch 1290, val loss: 0.9079136848449707
Epoch 1300, training loss: 62.4133415222168 = 0.08296617865562439 + 10.0 * 6.23303747177124
Epoch 1300, val loss: 0.9120676517486572
Epoch 1310, training loss: 62.40044403076172 = 0.08054135739803314 + 10.0 * 6.231990337371826
Epoch 1310, val loss: 0.9163087010383606
Epoch 1320, training loss: 62.392120361328125 = 0.07822415232658386 + 10.0 * 6.23138952255249
Epoch 1320, val loss: 0.9204762578010559
Epoch 1330, training loss: 62.40732192993164 = 0.07599131762981415 + 10.0 * 6.233132839202881
Epoch 1330, val loss: 0.9246745109558105
Epoch 1340, training loss: 62.394126892089844 = 0.07381759583950043 + 10.0 * 6.232030868530273
Epoch 1340, val loss: 0.9288734793663025
Epoch 1350, training loss: 62.39086151123047 = 0.07173195481300354 + 10.0 * 6.231913089752197
Epoch 1350, val loss: 0.9330048561096191
Epoch 1360, training loss: 62.382896423339844 = 0.06974560767412186 + 10.0 * 6.2313151359558105
Epoch 1360, val loss: 0.9371774196624756
Epoch 1370, training loss: 62.395294189453125 = 0.06783679127693176 + 10.0 * 6.23274564743042
Epoch 1370, val loss: 0.9413720369338989
Epoch 1380, training loss: 62.35939407348633 = 0.06596066802740097 + 10.0 * 6.229343414306641
Epoch 1380, val loss: 0.9457621574401855
Epoch 1390, training loss: 62.355159759521484 = 0.0641845315694809 + 10.0 * 6.229097366333008
Epoch 1390, val loss: 0.9500548243522644
Epoch 1400, training loss: 62.35068893432617 = 0.0624806247651577 + 10.0 * 6.22882080078125
Epoch 1400, val loss: 0.9543059468269348
Epoch 1410, training loss: 62.38003158569336 = 0.06083369627594948 + 10.0 * 6.231919765472412
Epoch 1410, val loss: 0.9584693312644958
Epoch 1420, training loss: 62.36697006225586 = 0.05922531709074974 + 10.0 * 6.230774402618408
Epoch 1420, val loss: 0.9626946449279785
Epoch 1430, training loss: 62.337589263916016 = 0.05768093839287758 + 10.0 * 6.227990627288818
Epoch 1430, val loss: 0.9668671488761902
Epoch 1440, training loss: 62.33359146118164 = 0.056197989732027054 + 10.0 * 6.227739334106445
Epoch 1440, val loss: 0.9711611270904541
Epoch 1450, training loss: 62.36693572998047 = 0.05477554351091385 + 10.0 * 6.231215953826904
Epoch 1450, val loss: 0.9754359722137451
Epoch 1460, training loss: 62.33156967163086 = 0.0533902682363987 + 10.0 * 6.227818012237549
Epoch 1460, val loss: 0.9794505834579468
Epoch 1470, training loss: 62.32738494873047 = 0.05204932391643524 + 10.0 * 6.22753381729126
Epoch 1470, val loss: 0.9836674332618713
Epoch 1480, training loss: 62.311309814453125 = 0.05077207088470459 + 10.0 * 6.226053714752197
Epoch 1480, val loss: 0.9878420233726501
Epoch 1490, training loss: 62.30202865600586 = 0.04954121261835098 + 10.0 * 6.22524881362915
Epoch 1490, val loss: 0.9920789003372192
Epoch 1500, training loss: 62.32229232788086 = 0.048346493393182755 + 10.0 * 6.2273945808410645
Epoch 1500, val loss: 0.9963968992233276
Epoch 1510, training loss: 62.29471969604492 = 0.047182537615299225 + 10.0 * 6.224753379821777
Epoch 1510, val loss: 1.0001013278961182
Epoch 1520, training loss: 62.293365478515625 = 0.04605631157755852 + 10.0 * 6.224730968475342
Epoch 1520, val loss: 1.004320502281189
Epoch 1530, training loss: 62.32025909423828 = 0.04498151317238808 + 10.0 * 6.227527618408203
Epoch 1530, val loss: 1.0083163976669312
Epoch 1540, training loss: 62.28372573852539 = 0.043931588530540466 + 10.0 * 6.223979473114014
Epoch 1540, val loss: 1.0126266479492188
Epoch 1550, training loss: 62.27686309814453 = 0.042931314557790756 + 10.0 * 6.223393440246582
Epoch 1550, val loss: 1.0165958404541016
Epoch 1560, training loss: 62.2825927734375 = 0.041959647089242935 + 10.0 * 6.224063396453857
Epoch 1560, val loss: 1.0207017660140991
Epoch 1570, training loss: 62.295745849609375 = 0.04102414846420288 + 10.0 * 6.225472450256348
Epoch 1570, val loss: 1.0246634483337402
Epoch 1580, training loss: 62.28497314453125 = 0.04010508581995964 + 10.0 * 6.224486827850342
Epoch 1580, val loss: 1.0286818742752075
Epoch 1590, training loss: 62.27827072143555 = 0.03921892121434212 + 10.0 * 6.223905086517334
Epoch 1590, val loss: 1.0326114892959595
Epoch 1600, training loss: 62.259090423583984 = 0.03835497051477432 + 10.0 * 6.222073554992676
Epoch 1600, val loss: 1.0367136001586914
Epoch 1610, training loss: 62.25236511230469 = 0.037533145397901535 + 10.0 * 6.22148323059082
Epoch 1610, val loss: 1.0406566858291626
Epoch 1620, training loss: 62.282161712646484 = 0.036732424050569534 + 10.0 * 6.22454309463501
Epoch 1620, val loss: 1.0446865558624268
Epoch 1630, training loss: 62.26496505737305 = 0.035953134298324585 + 10.0 * 6.222901344299316
Epoch 1630, val loss: 1.048288345336914
Epoch 1640, training loss: 62.2631721496582 = 0.03518403694033623 + 10.0 * 6.222798824310303
Epoch 1640, val loss: 1.0521876811981201
Epoch 1650, training loss: 62.251869201660156 = 0.03446725383400917 + 10.0 * 6.221740245819092
Epoch 1650, val loss: 1.0560147762298584
Epoch 1660, training loss: 62.250247955322266 = 0.033761490136384964 + 10.0 * 6.221648693084717
Epoch 1660, val loss: 1.0597374439239502
Epoch 1670, training loss: 62.23887252807617 = 0.033067379146814346 + 10.0 * 6.220580577850342
Epoch 1670, val loss: 1.0636845827102661
Epoch 1680, training loss: 62.25181579589844 = 0.032410141080617905 + 10.0 * 6.221940517425537
Epoch 1680, val loss: 1.0674375295639038
Epoch 1690, training loss: 62.22895050048828 = 0.03176916763186455 + 10.0 * 6.219717979431152
Epoch 1690, val loss: 1.0709816217422485
Epoch 1700, training loss: 62.23393249511719 = 0.031142039224505424 + 10.0 * 6.220278739929199
Epoch 1700, val loss: 1.074698567390442
Epoch 1710, training loss: 62.224952697753906 = 0.030527492985129356 + 10.0 * 6.219442367553711
Epoch 1710, val loss: 1.0784848928451538
Epoch 1720, training loss: 62.268131256103516 = 0.029938340187072754 + 10.0 * 6.223819255828857
Epoch 1720, val loss: 1.0820906162261963
Epoch 1730, training loss: 62.23078918457031 = 0.029367364943027496 + 10.0 * 6.220142364501953
Epoch 1730, val loss: 1.085658073425293
Epoch 1740, training loss: 62.21419143676758 = 0.02880333550274372 + 10.0 * 6.218538761138916
Epoch 1740, val loss: 1.0892972946166992
Epoch 1750, training loss: 62.206634521484375 = 0.028266731649637222 + 10.0 * 6.217836856842041
Epoch 1750, val loss: 1.0929992198944092
Epoch 1760, training loss: 62.20355987548828 = 0.02774684503674507 + 10.0 * 6.217581272125244
Epoch 1760, val loss: 1.0965592861175537
Epoch 1770, training loss: 62.277442932128906 = 0.027238260954618454 + 10.0 * 6.225020408630371
Epoch 1770, val loss: 1.1001918315887451
Epoch 1780, training loss: 62.233089447021484 = 0.026736153289675713 + 10.0 * 6.220635414123535
Epoch 1780, val loss: 1.1033676862716675
Epoch 1790, training loss: 62.197792053222656 = 0.02624589204788208 + 10.0 * 6.217154502868652
Epoch 1790, val loss: 1.1070170402526855
Epoch 1800, training loss: 62.20040512084961 = 0.025781309232115746 + 10.0 * 6.217462539672852
Epoch 1800, val loss: 1.1105461120605469
Epoch 1810, training loss: 62.20710754394531 = 0.025328945368528366 + 10.0 * 6.218177795410156
Epoch 1810, val loss: 1.1139332056045532
Epoch 1820, training loss: 62.19579315185547 = 0.024883009493350983 + 10.0 * 6.217091083526611
Epoch 1820, val loss: 1.1174677610397339
Epoch 1830, training loss: 62.19071960449219 = 0.02445385232567787 + 10.0 * 6.2166266441345215
Epoch 1830, val loss: 1.1208380460739136
Epoch 1840, training loss: 62.195587158203125 = 0.024036820977926254 + 10.0 * 6.2171549797058105
Epoch 1840, val loss: 1.1242845058441162
Epoch 1850, training loss: 62.19761657714844 = 0.02362549677491188 + 10.0 * 6.2173991203308105
Epoch 1850, val loss: 1.127592921257019
Epoch 1860, training loss: 62.21501159667969 = 0.023219024762511253 + 10.0 * 6.219179153442383
Epoch 1860, val loss: 1.1308238506317139
Epoch 1870, training loss: 62.19017791748047 = 0.022827308624982834 + 10.0 * 6.216734886169434
Epoch 1870, val loss: 1.1342878341674805
Epoch 1880, training loss: 62.172325134277344 = 0.022444922477006912 + 10.0 * 6.214987754821777
Epoch 1880, val loss: 1.137709617614746
Epoch 1890, training loss: 62.1671028137207 = 0.02207901142537594 + 10.0 * 6.214502334594727
Epoch 1890, val loss: 1.1410657167434692
Epoch 1900, training loss: 62.19921875 = 0.021720437332987785 + 10.0 * 6.21774959564209
Epoch 1900, val loss: 1.1443387269973755
Epoch 1910, training loss: 62.17394256591797 = 0.021367324516177177 + 10.0 * 6.21525764465332
Epoch 1910, val loss: 1.1476235389709473
Epoch 1920, training loss: 62.16379165649414 = 0.021021168678998947 + 10.0 * 6.2142767906188965
Epoch 1920, val loss: 1.1507524251937866
Epoch 1930, training loss: 62.17499542236328 = 0.02068575657904148 + 10.0 * 6.215430736541748
Epoch 1930, val loss: 1.1540621519088745
Epoch 1940, training loss: 62.16673278808594 = 0.020357714965939522 + 10.0 * 6.214637279510498
Epoch 1940, val loss: 1.1571916341781616
Epoch 1950, training loss: 62.17993927001953 = 0.02003336325287819 + 10.0 * 6.2159905433654785
Epoch 1950, val loss: 1.1602786779403687
Epoch 1960, training loss: 62.15219497680664 = 0.019727393984794617 + 10.0 * 6.213246822357178
Epoch 1960, val loss: 1.1633495092391968
Epoch 1970, training loss: 62.144813537597656 = 0.019425619393587112 + 10.0 * 6.212538719177246
Epoch 1970, val loss: 1.1665306091308594
Epoch 1980, training loss: 62.142818450927734 = 0.019135667011141777 + 10.0 * 6.212368488311768
Epoch 1980, val loss: 1.1696369647979736
Epoch 1990, training loss: 62.163204193115234 = 0.01885492540895939 + 10.0 * 6.214434623718262
Epoch 1990, val loss: 1.1726962327957153
Epoch 2000, training loss: 62.14536666870117 = 0.018567128106951714 + 10.0 * 6.212679862976074
Epoch 2000, val loss: 1.1757431030273438
Epoch 2010, training loss: 62.157997131347656 = 0.018288511782884598 + 10.0 * 6.213971138000488
Epoch 2010, val loss: 1.1787292957305908
Epoch 2020, training loss: 62.137203216552734 = 0.01801539957523346 + 10.0 * 6.211918830871582
Epoch 2020, val loss: 1.181777000427246
Epoch 2030, training loss: 62.20317077636719 = 0.01775449328124523 + 10.0 * 6.218541622161865
Epoch 2030, val loss: 1.1848742961883545
Epoch 2040, training loss: 62.154964447021484 = 0.017490696161985397 + 10.0 * 6.213747501373291
Epoch 2040, val loss: 1.1877069473266602
Epoch 2050, training loss: 62.130592346191406 = 0.017233505845069885 + 10.0 * 6.2113356590271
Epoch 2050, val loss: 1.190704345703125
Epoch 2060, training loss: 62.12024688720703 = 0.016991565003991127 + 10.0 * 6.210325717926025
Epoch 2060, val loss: 1.1937392950057983
Epoch 2070, training loss: 62.1175651550293 = 0.01675678789615631 + 10.0 * 6.210080623626709
Epoch 2070, val loss: 1.196648120880127
Epoch 2080, training loss: 62.14447784423828 = 0.016524754464626312 + 10.0 * 6.212795257568359
Epoch 2080, val loss: 1.1995354890823364
Epoch 2090, training loss: 62.142356872558594 = 0.016289275139570236 + 10.0 * 6.212606906890869
Epoch 2090, val loss: 1.2024636268615723
Epoch 2100, training loss: 62.11443328857422 = 0.01606181636452675 + 10.0 * 6.209836959838867
Epoch 2100, val loss: 1.2051670551300049
Epoch 2110, training loss: 62.11566925048828 = 0.01584458537399769 + 10.0 * 6.209982395172119
Epoch 2110, val loss: 1.208161473274231
Epoch 2120, training loss: 62.10992431640625 = 0.01563173718750477 + 10.0 * 6.2094292640686035
Epoch 2120, val loss: 1.2109929323196411
Epoch 2130, training loss: 62.137752532958984 = 0.015427866019308567 + 10.0 * 6.21223258972168
Epoch 2130, val loss: 1.2137640714645386
Epoch 2140, training loss: 62.12497329711914 = 0.015215598978102207 + 10.0 * 6.210975646972656
Epoch 2140, val loss: 1.2165719270706177
Epoch 2150, training loss: 62.107784271240234 = 0.015007749199867249 + 10.0 * 6.209277629852295
Epoch 2150, val loss: 1.219389796257019
Epoch 2160, training loss: 62.1065559387207 = 0.014809726737439632 + 10.0 * 6.209174633026123
Epoch 2160, val loss: 1.2222223281860352
Epoch 2170, training loss: 62.10066223144531 = 0.014619511552155018 + 10.0 * 6.208604335784912
Epoch 2170, val loss: 1.224948763847351
Epoch 2180, training loss: 62.116600036621094 = 0.014432594180107117 + 10.0 * 6.210216999053955
Epoch 2180, val loss: 1.2277154922485352
Epoch 2190, training loss: 62.10877990722656 = 0.01424678135663271 + 10.0 * 6.209453105926514
Epoch 2190, val loss: 1.230220079421997
Epoch 2200, training loss: 62.13401794433594 = 0.014071984216570854 + 10.0 * 6.211994647979736
Epoch 2200, val loss: 1.2328509092330933
Epoch 2210, training loss: 62.10628128051758 = 0.013884634710848331 + 10.0 * 6.209239482879639
Epoch 2210, val loss: 1.2355129718780518
Epoch 2220, training loss: 62.091304779052734 = 0.013709275983273983 + 10.0 * 6.207759380340576
Epoch 2220, val loss: 1.2381919622421265
Epoch 2230, training loss: 62.08399963378906 = 0.01354061346501112 + 10.0 * 6.207045555114746
Epoch 2230, val loss: 1.2409886121749878
Epoch 2240, training loss: 62.08395767211914 = 0.013376721180975437 + 10.0 * 6.207057952880859
Epoch 2240, val loss: 1.2437119483947754
Epoch 2250, training loss: 62.127742767333984 = 0.01321898028254509 + 10.0 * 6.211452484130859
Epoch 2250, val loss: 1.2464205026626587
Epoch 2260, training loss: 62.108882904052734 = 0.013053152710199356 + 10.0 * 6.209582805633545
Epoch 2260, val loss: 1.2485063076019287
Epoch 2270, training loss: 62.10647964477539 = 0.012890860438346863 + 10.0 * 6.209359169006348
Epoch 2270, val loss: 1.251374363899231
Epoch 2280, training loss: 62.08335494995117 = 0.012734576128423214 + 10.0 * 6.207062244415283
Epoch 2280, val loss: 1.2538659572601318
Epoch 2290, training loss: 62.077667236328125 = 0.012582860887050629 + 10.0 * 6.206508159637451
Epoch 2290, val loss: 1.2564562559127808
Epoch 2300, training loss: 62.08723449707031 = 0.012434164062142372 + 10.0 * 6.207479953765869
Epoch 2300, val loss: 1.2591466903686523
Epoch 2310, training loss: 62.10397720336914 = 0.012285413220524788 + 10.0 * 6.209168910980225
Epoch 2310, val loss: 1.2615288496017456
Epoch 2320, training loss: 62.083229064941406 = 0.012145615182816982 + 10.0 * 6.207108497619629
Epoch 2320, val loss: 1.2639001607894897
Epoch 2330, training loss: 62.07586669921875 = 0.012002168223261833 + 10.0 * 6.206386566162109
Epoch 2330, val loss: 1.2665742635726929
Epoch 2340, training loss: 62.104331970214844 = 0.011870158836245537 + 10.0 * 6.2092461585998535
Epoch 2340, val loss: 1.2689857482910156
Epoch 2350, training loss: 62.08576202392578 = 0.011728636920452118 + 10.0 * 6.207403182983398
Epoch 2350, val loss: 1.2713772058486938
Epoch 2360, training loss: 62.0714111328125 = 0.01159458328038454 + 10.0 * 6.205981731414795
Epoch 2360, val loss: 1.27383291721344
Epoch 2370, training loss: 62.06718826293945 = 0.011463252827525139 + 10.0 * 6.205572605133057
Epoch 2370, val loss: 1.276265025138855
Epoch 2380, training loss: 62.06482696533203 = 0.011338410899043083 + 10.0 * 6.205348968505859
Epoch 2380, val loss: 1.2787044048309326
Epoch 2390, training loss: 62.06733322143555 = 0.011214617639780045 + 10.0 * 6.205611705780029
Epoch 2390, val loss: 1.280993938446045
Epoch 2400, training loss: 62.075164794921875 = 0.011092441156506538 + 10.0 * 6.206407070159912
Epoch 2400, val loss: 1.2834057807922363
Epoch 2410, training loss: 62.1124267578125 = 0.010972324758768082 + 10.0 * 6.210145473480225
Epoch 2410, val loss: 1.2856903076171875
Epoch 2420, training loss: 62.06204605102539 = 0.01084276381880045 + 10.0 * 6.20512056350708
Epoch 2420, val loss: 1.2880585193634033
Epoch 2430, training loss: 62.05305480957031 = 0.010726815089583397 + 10.0 * 6.204232692718506
Epoch 2430, val loss: 1.2905291318893433
Epoch 2440, training loss: 62.05534362792969 = 0.010613490827381611 + 10.0 * 6.20447301864624
Epoch 2440, val loss: 1.2928411960601807
Epoch 2450, training loss: 62.08320999145508 = 0.010501555167138577 + 10.0 * 6.207270622253418
Epoch 2450, val loss: 1.295029640197754
Epoch 2460, training loss: 62.04829788208008 = 0.010390629060566425 + 10.0 * 6.203790664672852
Epoch 2460, val loss: 1.2973767518997192
Epoch 2470, training loss: 62.06680679321289 = 0.010282563976943493 + 10.0 * 6.205652236938477
Epoch 2470, val loss: 1.299634337425232
Epoch 2480, training loss: 62.05758285522461 = 0.010172270238399506 + 10.0 * 6.20474100112915
Epoch 2480, val loss: 1.3018819093704224
Epoch 2490, training loss: 62.048301696777344 = 0.010065890848636627 + 10.0 * 6.203823566436768
Epoch 2490, val loss: 1.3040801286697388
Epoch 2500, training loss: 62.04732131958008 = 0.00996421743184328 + 10.0 * 6.203735828399658
Epoch 2500, val loss: 1.3063056468963623
Epoch 2510, training loss: 62.10763168334961 = 0.009867803193628788 + 10.0 * 6.209776401519775
Epoch 2510, val loss: 1.3082308769226074
Epoch 2520, training loss: 62.05104064941406 = 0.009755822829902172 + 10.0 * 6.204128742218018
Epoch 2520, val loss: 1.3107130527496338
Epoch 2530, training loss: 62.03647994995117 = 0.00965950172394514 + 10.0 * 6.202682018280029
Epoch 2530, val loss: 1.312712550163269
Epoch 2540, training loss: 62.03696060180664 = 0.00956436526030302 + 10.0 * 6.202739715576172
Epoch 2540, val loss: 1.3149999380111694
Epoch 2550, training loss: 62.08572006225586 = 0.009470432065427303 + 10.0 * 6.207624912261963
Epoch 2550, val loss: 1.316995620727539
Epoch 2560, training loss: 62.04291534423828 = 0.00937342457473278 + 10.0 * 6.203354358673096
Epoch 2560, val loss: 1.3192970752716064
Epoch 2570, training loss: 62.035003662109375 = 0.009281644597649574 + 10.0 * 6.202572345733643
Epoch 2570, val loss: 1.3213611841201782
Epoch 2580, training loss: 62.046630859375 = 0.00919187068939209 + 10.0 * 6.203743934631348
Epoch 2580, val loss: 1.3235312700271606
Epoch 2590, training loss: 62.038021087646484 = 0.009101391769945621 + 10.0 * 6.202891826629639
Epoch 2590, val loss: 1.3255212306976318
Epoch 2600, training loss: 62.05892562866211 = 0.00901456642895937 + 10.0 * 6.204991340637207
Epoch 2600, val loss: 1.3276357650756836
Epoch 2610, training loss: 62.03069305419922 = 0.008924584835767746 + 10.0 * 6.202176570892334
Epoch 2610, val loss: 1.329419732093811
Epoch 2620, training loss: 62.026710510253906 = 0.008836669847369194 + 10.0 * 6.20178747177124
Epoch 2620, val loss: 1.331811547279358
Epoch 2630, training loss: 62.056678771972656 = 0.008751792833209038 + 10.0 * 6.2047929763793945
Epoch 2630, val loss: 1.3337085247039795
Epoch 2640, training loss: 62.02340316772461 = 0.008669566363096237 + 10.0 * 6.201473236083984
Epoch 2640, val loss: 1.3356528282165527
Epoch 2650, training loss: 62.02465057373047 = 0.008591195568442345 + 10.0 * 6.201605796813965
Epoch 2650, val loss: 1.3376344442367554
Epoch 2660, training loss: 62.013675689697266 = 0.0085111940279603 + 10.0 * 6.200516700744629
Epoch 2660, val loss: 1.3397632837295532
Epoch 2670, training loss: 62.01984405517578 = 0.008434881456196308 + 10.0 * 6.201140880584717
Epoch 2670, val loss: 1.3417569398880005
Epoch 2680, training loss: 62.069766998291016 = 0.008359323255717754 + 10.0 * 6.206140995025635
Epoch 2680, val loss: 1.3438236713409424
Epoch 2690, training loss: 62.051029205322266 = 0.008280922658741474 + 10.0 * 6.204274654388428
Epoch 2690, val loss: 1.3454777002334595
Epoch 2700, training loss: 62.042320251464844 = 0.008199812844395638 + 10.0 * 6.203412055969238
Epoch 2700, val loss: 1.3476399183273315
Epoch 2710, training loss: 62.020687103271484 = 0.008123382925987244 + 10.0 * 6.20125675201416
Epoch 2710, val loss: 1.3494644165039062
Epoch 2720, training loss: 62.01327896118164 = 0.00805242732167244 + 10.0 * 6.200522422790527
Epoch 2720, val loss: 1.3510957956314087
Epoch 2730, training loss: 62.00816345214844 = 0.007980258204042912 + 10.0 * 6.200018405914307
Epoch 2730, val loss: 1.3532819747924805
Epoch 2740, training loss: 62.00229263305664 = 0.007912911474704742 + 10.0 * 6.199438095092773
Epoch 2740, val loss: 1.355165719985962
Epoch 2750, training loss: 62.012813568115234 = 0.007845445536077023 + 10.0 * 6.200496673583984
Epoch 2750, val loss: 1.3572027683258057
Epoch 2760, training loss: 62.07475280761719 = 0.007773644290864468 + 10.0 * 6.206697940826416
Epoch 2760, val loss: 1.3589692115783691
Epoch 2770, training loss: 62.01325607299805 = 0.007702285423874855 + 10.0 * 6.200555324554443
Epoch 2770, val loss: 1.3603962659835815
Epoch 2780, training loss: 62.00983810424805 = 0.00763242831453681 + 10.0 * 6.200220584869385
Epoch 2780, val loss: 1.3625680208206177
Epoch 2790, training loss: 61.997314453125 = 0.0075687868520617485 + 10.0 * 6.198974609375
Epoch 2790, val loss: 1.3644654750823975
Epoch 2800, training loss: 61.996238708496094 = 0.007507276721298695 + 10.0 * 6.198873043060303
Epoch 2800, val loss: 1.3662759065628052
Epoch 2810, training loss: 62.02816390991211 = 0.0074477954767644405 + 10.0 * 6.202071666717529
Epoch 2810, val loss: 1.3679929971694946
Epoch 2820, training loss: 62.00617218017578 = 0.0073805805295705795 + 10.0 * 6.199879169464111
Epoch 2820, val loss: 1.3695802688598633
Epoch 2830, training loss: 62.00006866455078 = 0.00731512950733304 + 10.0 * 6.199275016784668
Epoch 2830, val loss: 1.3715053796768188
Epoch 2840, training loss: 61.99517059326172 = 0.007254679221659899 + 10.0 * 6.19879150390625
Epoch 2840, val loss: 1.3733689785003662
Epoch 2850, training loss: 62.00042724609375 = 0.00719617772847414 + 10.0 * 6.1993231773376465
Epoch 2850, val loss: 1.3751142024993896
Epoch 2860, training loss: 62.006591796875 = 0.007137192413210869 + 10.0 * 6.199945449829102
Epoch 2860, val loss: 1.3768984079360962
Epoch 2870, training loss: 61.99415969848633 = 0.0070780604146420956 + 10.0 * 6.198708534240723
Epoch 2870, val loss: 1.3786044120788574
Epoch 2880, training loss: 62.02108383178711 = 0.007021734490990639 + 10.0 * 6.201406002044678
Epoch 2880, val loss: 1.3802392482757568
Epoch 2890, training loss: 62.06218719482422 = 0.006965773645788431 + 10.0 * 6.205522060394287
Epoch 2890, val loss: 1.3816990852355957
Epoch 2900, training loss: 62.01780319213867 = 0.006900839973241091 + 10.0 * 6.201090335845947
Epoch 2900, val loss: 1.3835760354995728
Epoch 2910, training loss: 61.989898681640625 = 0.006845051422715187 + 10.0 * 6.198305606842041
Epoch 2910, val loss: 1.3851412534713745
Epoch 2920, training loss: 61.981781005859375 = 0.006791497580707073 + 10.0 * 6.1974992752075195
Epoch 2920, val loss: 1.3870478868484497
Epoch 2930, training loss: 61.9810905456543 = 0.006739770993590355 + 10.0 * 6.197434902191162
Epoch 2930, val loss: 1.3888201713562012
Epoch 2940, training loss: 61.99027633666992 = 0.006688748486340046 + 10.0 * 6.19835901260376
Epoch 2940, val loss: 1.3904000520706177
Epoch 2950, training loss: 62.041473388671875 = 0.006634908262640238 + 10.0 * 6.203484058380127
Epoch 2950, val loss: 1.391855001449585
Epoch 2960, training loss: 62.00185775756836 = 0.0065836552530527115 + 10.0 * 6.199527263641357
Epoch 2960, val loss: 1.3935306072235107
Epoch 2970, training loss: 61.98710632324219 = 0.006530105136334896 + 10.0 * 6.198057651519775
Epoch 2970, val loss: 1.395215630531311
Epoch 2980, training loss: 61.989288330078125 = 0.006482146680355072 + 10.0 * 6.1982808113098145
Epoch 2980, val loss: 1.396797776222229
Epoch 2990, training loss: 62.005470275878906 = 0.006432513240724802 + 10.0 * 6.199903964996338
Epoch 2990, val loss: 1.3982707262039185
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 87.89591979980469 = 1.927717685699463 + 10.0 * 8.596819877624512
Epoch 0, val loss: 1.9249862432479858
Epoch 10, training loss: 87.87914276123047 = 1.9194310903549194 + 10.0 * 8.59597110748291
Epoch 10, val loss: 1.9170902967453003
Epoch 20, training loss: 87.80529022216797 = 1.9090927839279175 + 10.0 * 8.589619636535645
Epoch 20, val loss: 1.9068427085876465
Epoch 30, training loss: 87.3438491821289 = 1.8954840898513794 + 10.0 * 8.544836044311523
Epoch 30, val loss: 1.893101692199707
Epoch 40, training loss: 84.44351196289062 = 1.8787306547164917 + 10.0 * 8.256478309631348
Epoch 40, val loss: 1.8763397932052612
Epoch 50, training loss: 77.0592041015625 = 1.8595255613327026 + 10.0 * 7.519968032836914
Epoch 50, val loss: 1.8578667640686035
Epoch 60, training loss: 73.91075134277344 = 1.8465487957000732 + 10.0 * 7.206420421600342
Epoch 60, val loss: 1.8466839790344238
Epoch 70, training loss: 71.84982299804688 = 1.8361681699752808 + 10.0 * 7.001365661621094
Epoch 70, val loss: 1.837289810180664
Epoch 80, training loss: 70.59996032714844 = 1.8268847465515137 + 10.0 * 6.877307891845703
Epoch 80, val loss: 1.8284071683883667
Epoch 90, training loss: 69.83232879638672 = 1.8175466060638428 + 10.0 * 6.801478385925293
Epoch 90, val loss: 1.8194087743759155
Epoch 100, training loss: 69.30160522460938 = 1.8083723783493042 + 10.0 * 6.74932336807251
Epoch 100, val loss: 1.8106226921081543
Epoch 110, training loss: 68.79459381103516 = 1.8002504110336304 + 10.0 * 6.699434757232666
Epoch 110, val loss: 1.8029534816741943
Epoch 120, training loss: 68.23908233642578 = 1.7932029962539673 + 10.0 * 6.644587993621826
Epoch 120, val loss: 1.7962377071380615
Epoch 130, training loss: 67.78912353515625 = 1.7865464687347412 + 10.0 * 6.600257873535156
Epoch 130, val loss: 1.7897306680679321
Epoch 140, training loss: 67.44898223876953 = 1.7793304920196533 + 10.0 * 6.566965579986572
Epoch 140, val loss: 1.7827800512313843
Epoch 150, training loss: 67.13668060302734 = 1.7714009284973145 + 10.0 * 6.53652811050415
Epoch 150, val loss: 1.7752134799957275
Epoch 160, training loss: 66.87895202636719 = 1.7630287408828735 + 10.0 * 6.511591911315918
Epoch 160, val loss: 1.7671267986297607
Epoch 170, training loss: 66.66053771972656 = 1.7539142370224 + 10.0 * 6.490662574768066
Epoch 170, val loss: 1.7585525512695312
Epoch 180, training loss: 66.46296691894531 = 1.7437891960144043 + 10.0 * 6.471918106079102
Epoch 180, val loss: 1.7489787340164185
Epoch 190, training loss: 66.2863998413086 = 1.7325811386108398 + 10.0 * 6.455381870269775
Epoch 190, val loss: 1.7386425733566284
Epoch 200, training loss: 66.1221923828125 = 1.7203494310379028 + 10.0 * 6.440184116363525
Epoch 200, val loss: 1.727524757385254
Epoch 210, training loss: 66.02012634277344 = 1.706958532333374 + 10.0 * 6.431317329406738
Epoch 210, val loss: 1.7152379751205444
Epoch 220, training loss: 65.859375 = 1.6920034885406494 + 10.0 * 6.4167375564575195
Epoch 220, val loss: 1.7019904851913452
Epoch 230, training loss: 65.73987579345703 = 1.6757402420043945 + 10.0 * 6.4064130783081055
Epoch 230, val loss: 1.6873974800109863
Epoch 240, training loss: 65.64468383789062 = 1.6580172777175903 + 10.0 * 6.398666858673096
Epoch 240, val loss: 1.6715208292007446
Epoch 250, training loss: 65.55977630615234 = 1.6385836601257324 + 10.0 * 6.392119407653809
Epoch 250, val loss: 1.6543914079666138
Epoch 260, training loss: 65.4537582397461 = 1.6177680492401123 + 10.0 * 6.383599281311035
Epoch 260, val loss: 1.6358592510223389
Epoch 270, training loss: 65.36238098144531 = 1.5954395532608032 + 10.0 * 6.376694202423096
Epoch 270, val loss: 1.6161904335021973
Epoch 280, training loss: 65.30126953125 = 1.5716395378112793 + 10.0 * 6.372962951660156
Epoch 280, val loss: 1.5954196453094482
Epoch 290, training loss: 65.20307922363281 = 1.5462321043014526 + 10.0 * 6.365684509277344
Epoch 290, val loss: 1.5732879638671875
Epoch 300, training loss: 65.13267517089844 = 1.519744873046875 + 10.0 * 6.361293315887451
Epoch 300, val loss: 1.5503140687942505
Epoch 310, training loss: 65.0464096069336 = 1.4921821355819702 + 10.0 * 6.355422496795654
Epoch 310, val loss: 1.5267447233200073
Epoch 320, training loss: 64.97470092773438 = 1.4636200666427612 + 10.0 * 6.351108074188232
Epoch 320, val loss: 1.5025262832641602
Epoch 330, training loss: 64.9476547241211 = 1.4341511726379395 + 10.0 * 6.3513503074646
Epoch 330, val loss: 1.477907419204712
Epoch 340, training loss: 64.83763885498047 = 1.4039255380630493 + 10.0 * 6.343371391296387
Epoch 340, val loss: 1.452984094619751
Epoch 350, training loss: 64.75593566894531 = 1.3732722997665405 + 10.0 * 6.338266372680664
Epoch 350, val loss: 1.4281758069992065
Epoch 360, training loss: 64.68964385986328 = 1.3424437046051025 + 10.0 * 6.334720134735107
Epoch 360, val loss: 1.403386116027832
Epoch 370, training loss: 64.65220642089844 = 1.3115234375 + 10.0 * 6.334068775177002
Epoch 370, val loss: 1.378716230392456
Epoch 380, training loss: 64.5674819946289 = 1.280781865119934 + 10.0 * 6.328670024871826
Epoch 380, val loss: 1.3543792963027954
Epoch 390, training loss: 64.50533294677734 = 1.2504009008407593 + 10.0 * 6.325492858886719
Epoch 390, val loss: 1.3305881023406982
Epoch 400, training loss: 64.43463134765625 = 1.2202579975128174 + 10.0 * 6.321436882019043
Epoch 400, val loss: 1.3074603080749512
Epoch 410, training loss: 64.39996337890625 = 1.1903798580169678 + 10.0 * 6.320958614349365
Epoch 410, val loss: 1.2849937677383423
Epoch 420, training loss: 64.36945343017578 = 1.1611195802688599 + 10.0 * 6.320833206176758
Epoch 420, val loss: 1.2624986171722412
Epoch 430, training loss: 64.28221130371094 = 1.1321028470993042 + 10.0 * 6.315011024475098
Epoch 430, val loss: 1.2410310506820679
Epoch 440, training loss: 64.21813201904297 = 1.1037052869796753 + 10.0 * 6.3114423751831055
Epoch 440, val loss: 1.2202609777450562
Epoch 450, training loss: 64.1614761352539 = 1.0757845640182495 + 10.0 * 6.308568954467773
Epoch 450, val loss: 1.1999574899673462
Epoch 460, training loss: 64.16088104248047 = 1.0482354164123535 + 10.0 * 6.311264991760254
Epoch 460, val loss: 1.1800355911254883
Epoch 470, training loss: 64.08274841308594 = 1.0210415124893188 + 10.0 * 6.30617094039917
Epoch 470, val loss: 1.1605479717254639
Epoch 480, training loss: 64.01802062988281 = 0.9944474697113037 + 10.0 * 6.3023576736450195
Epoch 480, val loss: 1.1416646242141724
Epoch 490, training loss: 63.96724319458008 = 0.9684168696403503 + 10.0 * 6.299882411956787
Epoch 490, val loss: 1.123247504234314
Epoch 500, training loss: 63.93457794189453 = 0.942871630191803 + 10.0 * 6.29917049407959
Epoch 500, val loss: 1.1052687168121338
Epoch 510, training loss: 63.89408493041992 = 0.9176090955734253 + 10.0 * 6.297647476196289
Epoch 510, val loss: 1.0878241062164307
Epoch 520, training loss: 63.85981750488281 = 0.8929004073143005 + 10.0 * 6.29669189453125
Epoch 520, val loss: 1.070464849472046
Epoch 530, training loss: 63.79939270019531 = 0.8685643076896667 + 10.0 * 6.2930827140808105
Epoch 530, val loss: 1.0537171363830566
Epoch 540, training loss: 63.75606155395508 = 0.844759464263916 + 10.0 * 6.291130065917969
Epoch 540, val loss: 1.037409782409668
Epoch 550, training loss: 63.72774124145508 = 0.8214603066444397 + 10.0 * 6.290627956390381
Epoch 550, val loss: 1.021432638168335
Epoch 560, training loss: 63.689334869384766 = 0.798542320728302 + 10.0 * 6.289079189300537
Epoch 560, val loss: 1.005632996559143
Epoch 570, training loss: 63.64141082763672 = 0.7760728001594543 + 10.0 * 6.286533832550049
Epoch 570, val loss: 0.9903685450553894
Epoch 580, training loss: 63.62708282470703 = 0.7541162967681885 + 10.0 * 6.287296772003174
Epoch 580, val loss: 0.9753937721252441
Epoch 590, training loss: 63.5713996887207 = 0.732465922832489 + 10.0 * 6.28389310836792
Epoch 590, val loss: 0.9608068466186523
Epoch 600, training loss: 63.528194427490234 = 0.7114272117614746 + 10.0 * 6.281676769256592
Epoch 600, val loss: 0.9465264678001404
Epoch 610, training loss: 63.52706527709961 = 0.6907532215118408 + 10.0 * 6.283631324768066
Epoch 610, val loss: 0.9327957630157471
Epoch 620, training loss: 63.47235870361328 = 0.6707220077514648 + 10.0 * 6.280163764953613
Epoch 620, val loss: 0.9190990924835205
Epoch 630, training loss: 63.43192672729492 = 0.650989294052124 + 10.0 * 6.2780938148498535
Epoch 630, val loss: 0.9061657190322876
Epoch 640, training loss: 63.4271125793457 = 0.6318918466567993 + 10.0 * 6.279521942138672
Epoch 640, val loss: 0.8934491872787476
Epoch 650, training loss: 63.386390686035156 = 0.6131599545478821 + 10.0 * 6.277323246002197
Epoch 650, val loss: 0.881502091884613
Epoch 660, training loss: 63.333045959472656 = 0.5950769782066345 + 10.0 * 6.273797035217285
Epoch 660, val loss: 0.8697689175605774
Epoch 670, training loss: 63.301265716552734 = 0.5774951577186584 + 10.0 * 6.272377014160156
Epoch 670, val loss: 0.8586610555648804
Epoch 680, training loss: 63.323265075683594 = 0.5604734420776367 + 10.0 * 6.276278972625732
Epoch 680, val loss: 0.8480470776557922
Epoch 690, training loss: 63.285682678222656 = 0.5436382293701172 + 10.0 * 6.274204730987549
Epoch 690, val loss: 0.837906002998352
Epoch 700, training loss: 63.21883010864258 = 0.527484655380249 + 10.0 * 6.269134521484375
Epoch 700, val loss: 0.8281401991844177
Epoch 710, training loss: 63.193946838378906 = 0.5118724703788757 + 10.0 * 6.268207550048828
Epoch 710, val loss: 0.8190586566925049
Epoch 720, training loss: 63.1815071105957 = 0.49673062562942505 + 10.0 * 6.268477439880371
Epoch 720, val loss: 0.8104170560836792
Epoch 730, training loss: 63.15573501586914 = 0.4818921387195587 + 10.0 * 6.2673845291137695
Epoch 730, val loss: 0.802347719669342
Epoch 740, training loss: 63.12612533569336 = 0.4675394892692566 + 10.0 * 6.2658586502075195
Epoch 740, val loss: 0.7944245338439941
Epoch 750, training loss: 63.0958137512207 = 0.4536472260951996 + 10.0 * 6.264216423034668
Epoch 750, val loss: 0.787147581577301
Epoch 760, training loss: 63.10981750488281 = 0.44017261266708374 + 10.0 * 6.266964435577393
Epoch 760, val loss: 0.7804506421089172
Epoch 770, training loss: 63.072349548339844 = 0.42687273025512695 + 10.0 * 6.264547824859619
Epoch 770, val loss: 0.7739647030830383
Epoch 780, training loss: 63.03328323364258 = 0.4141090512275696 + 10.0 * 6.261917591094971
Epoch 780, val loss: 0.7678672075271606
Epoch 790, training loss: 63.01511764526367 = 0.4016880393028259 + 10.0 * 6.261343002319336
Epoch 790, val loss: 0.7623167037963867
Epoch 800, training loss: 62.9966926574707 = 0.3896234333515167 + 10.0 * 6.260706901550293
Epoch 800, val loss: 0.7571407556533813
Epoch 810, training loss: 62.97758102416992 = 0.37792065739631653 + 10.0 * 6.259965896606445
Epoch 810, val loss: 0.7522020936012268
Epoch 820, training loss: 62.96260452270508 = 0.36645182967185974 + 10.0 * 6.259615421295166
Epoch 820, val loss: 0.7476130723953247
Epoch 830, training loss: 62.92580032348633 = 0.3553089201450348 + 10.0 * 6.257049083709717
Epoch 830, val loss: 0.7434247136116028
Epoch 840, training loss: 62.91181564331055 = 0.3444955348968506 + 10.0 * 6.256731986999512
Epoch 840, val loss: 0.7395238876342773
Epoch 850, training loss: 62.92425537109375 = 0.3339995741844177 + 10.0 * 6.259025573730469
Epoch 850, val loss: 0.7359267473220825
Epoch 860, training loss: 62.878570556640625 = 0.32379093766212463 + 10.0 * 6.2554779052734375
Epoch 860, val loss: 0.7327353358268738
Epoch 870, training loss: 62.85673141479492 = 0.3138509690761566 + 10.0 * 6.254288196563721
Epoch 870, val loss: 0.7297925353050232
Epoch 880, training loss: 62.90190505981445 = 0.30416446924209595 + 10.0 * 6.259774208068848
Epoch 880, val loss: 0.7273070812225342
Epoch 890, training loss: 62.82334899902344 = 0.2948603332042694 + 10.0 * 6.2528486251831055
Epoch 890, val loss: 0.7247469425201416
Epoch 900, training loss: 62.80195236206055 = 0.28577637672424316 + 10.0 * 6.251617431640625
Epoch 900, val loss: 0.7227745652198792
Epoch 910, training loss: 62.787559509277344 = 0.2769874036312103 + 10.0 * 6.251057147979736
Epoch 910, val loss: 0.7211718559265137
Epoch 920, training loss: 62.84049987792969 = 0.2684803307056427 + 10.0 * 6.257201671600342
Epoch 920, val loss: 0.719673216342926
Epoch 930, training loss: 62.76929473876953 = 0.26014864444732666 + 10.0 * 6.250914573669434
Epoch 930, val loss: 0.718440592288971
Epoch 940, training loss: 62.747562408447266 = 0.2520940899848938 + 10.0 * 6.249547004699707
Epoch 940, val loss: 0.7176257967948914
Epoch 950, training loss: 62.72714614868164 = 0.24435512721538544 + 10.0 * 6.248279094696045
Epoch 950, val loss: 0.716991662979126
Epoch 960, training loss: 62.82331085205078 = 0.23683640360832214 + 10.0 * 6.258647441864014
Epoch 960, val loss: 0.7166359424591064
Epoch 970, training loss: 62.709564208984375 = 0.22959867119789124 + 10.0 * 6.2479963302612305
Epoch 970, val loss: 0.7163661122322083
Epoch 980, training loss: 62.68960952758789 = 0.22256428003311157 + 10.0 * 6.246704578399658
Epoch 980, val loss: 0.7165811061859131
Epoch 990, training loss: 62.67266845703125 = 0.21579787135124207 + 10.0 * 6.245687007904053
Epoch 990, val loss: 0.7170578837394714
Epoch 1000, training loss: 62.663795471191406 = 0.20927467942237854 + 10.0 * 6.245451927185059
Epoch 1000, val loss: 0.7176616787910461
Epoch 1010, training loss: 62.708011627197266 = 0.2029004991054535 + 10.0 * 6.250511169433594
Epoch 1010, val loss: 0.7184781432151794
Epoch 1020, training loss: 62.642417907714844 = 0.19675984978675842 + 10.0 * 6.244565963745117
Epoch 1020, val loss: 0.719364583492279
Epoch 1030, training loss: 62.62843322753906 = 0.19084949791431427 + 10.0 * 6.243758201599121
Epoch 1030, val loss: 0.7204850912094116
Epoch 1040, training loss: 62.61394500732422 = 0.18514809012413025 + 10.0 * 6.242879867553711
Epoch 1040, val loss: 0.7219190001487732
Epoch 1050, training loss: 62.63176727294922 = 0.17964403331279755 + 10.0 * 6.245212078094482
Epoch 1050, val loss: 0.7235305905342102
Epoch 1060, training loss: 62.593143463134766 = 0.17430046200752258 + 10.0 * 6.241884231567383
Epoch 1060, val loss: 0.7254496216773987
Epoch 1070, training loss: 62.62022399902344 = 0.1691635102033615 + 10.0 * 6.245106220245361
Epoch 1070, val loss: 0.7272651791572571
Epoch 1080, training loss: 62.57960891723633 = 0.16412343084812164 + 10.0 * 6.241548538208008
Epoch 1080, val loss: 0.7294586896896362
Epoch 1090, training loss: 62.57173156738281 = 0.15930981934070587 + 10.0 * 6.241242408752441
Epoch 1090, val loss: 0.7316989302635193
Epoch 1100, training loss: 62.5637321472168 = 0.15465444326400757 + 10.0 * 6.240907669067383
Epoch 1100, val loss: 0.7341126799583435
Epoch 1110, training loss: 62.541786193847656 = 0.15014547109603882 + 10.0 * 6.239163875579834
Epoch 1110, val loss: 0.7366726994514465
Epoch 1120, training loss: 62.555564880371094 = 0.145835742354393 + 10.0 * 6.240972995758057
Epoch 1120, val loss: 0.7393375039100647
Epoch 1130, training loss: 62.52547836303711 = 0.1416195034980774 + 10.0 * 6.2383856773376465
Epoch 1130, val loss: 0.7421048283576965
Epoch 1140, training loss: 62.5159797668457 = 0.13754703104496002 + 10.0 * 6.2378435134887695
Epoch 1140, val loss: 0.7449961304664612
Epoch 1150, training loss: 62.52099609375 = 0.1336515247821808 + 10.0 * 6.238734245300293
Epoch 1150, val loss: 0.7481112480163574
Epoch 1160, training loss: 62.51150894165039 = 0.12984924018383026 + 10.0 * 6.238165855407715
Epoch 1160, val loss: 0.7511497139930725
Epoch 1170, training loss: 62.49245834350586 = 0.12620612978935242 + 10.0 * 6.2366251945495605
Epoch 1170, val loss: 0.7543840408325195
Epoch 1180, training loss: 62.487579345703125 = 0.12268218398094177 + 10.0 * 6.236489772796631
Epoch 1180, val loss: 0.7576260566711426
Epoch 1190, training loss: 62.483497619628906 = 0.11928188800811768 + 10.0 * 6.236421585083008
Epoch 1190, val loss: 0.7610496282577515
Epoch 1200, training loss: 62.49525833129883 = 0.11600827425718307 + 10.0 * 6.237925052642822
Epoch 1200, val loss: 0.7644907236099243
Epoch 1210, training loss: 62.498470306396484 = 0.11281511932611465 + 10.0 * 6.238565444946289
Epoch 1210, val loss: 0.7680482268333435
Epoch 1220, training loss: 62.456844329833984 = 0.10970387607812881 + 10.0 * 6.234714031219482
Epoch 1220, val loss: 0.7715800404548645
Epoch 1230, training loss: 62.4459114074707 = 0.1067361906170845 + 10.0 * 6.233917713165283
Epoch 1230, val loss: 0.775300920009613
Epoch 1240, training loss: 62.43767166137695 = 0.10389190167188644 + 10.0 * 6.233377933502197
Epoch 1240, val loss: 0.7790226936340332
Epoch 1250, training loss: 62.500633239746094 = 0.10111407190561295 + 10.0 * 6.239952087402344
Epoch 1250, val loss: 0.7828549146652222
Epoch 1260, training loss: 62.424415588378906 = 0.09842301905155182 + 10.0 * 6.232599258422852
Epoch 1260, val loss: 0.7865048050880432
Epoch 1270, training loss: 62.41167068481445 = 0.09583794325590134 + 10.0 * 6.231583595275879
Epoch 1270, val loss: 0.7904276251792908
Epoch 1280, training loss: 62.408111572265625 = 0.09334561973810196 + 10.0 * 6.231476783752441
Epoch 1280, val loss: 0.7942883372306824
Epoch 1290, training loss: 62.44970703125 = 0.09094421565532684 + 10.0 * 6.235876560211182
Epoch 1290, val loss: 0.7981061339378357
Epoch 1300, training loss: 62.4149169921875 = 0.08858046680688858 + 10.0 * 6.232633590698242
Epoch 1300, val loss: 0.8021616339683533
Epoch 1310, training loss: 62.447425842285156 = 0.08629487454891205 + 10.0 * 6.23611307144165
Epoch 1310, val loss: 0.8060538172721863
Epoch 1320, training loss: 62.3983154296875 = 0.08409593254327774 + 10.0 * 6.231421947479248
Epoch 1320, val loss: 0.8101567625999451
Epoch 1330, training loss: 62.38359832763672 = 0.08198367059230804 + 10.0 * 6.230161190032959
Epoch 1330, val loss: 0.8141933679580688
Epoch 1340, training loss: 62.36825942993164 = 0.07993610203266144 + 10.0 * 6.228832244873047
Epoch 1340, val loss: 0.8182878494262695
Epoch 1350, training loss: 62.36162567138672 = 0.07795807719230652 + 10.0 * 6.228366851806641
Epoch 1350, val loss: 0.822429358959198
Epoch 1360, training loss: 62.39952850341797 = 0.07603392750024796 + 10.0 * 6.232349395751953
Epoch 1360, val loss: 0.8265969157218933
Epoch 1370, training loss: 62.38252639770508 = 0.07417786121368408 + 10.0 * 6.2308349609375
Epoch 1370, val loss: 0.830452561378479
Epoch 1380, training loss: 62.37784957885742 = 0.07235724478960037 + 10.0 * 6.230549335479736
Epoch 1380, val loss: 0.8346214294433594
Epoch 1390, training loss: 62.35685729980469 = 0.07061609625816345 + 10.0 * 6.228623867034912
Epoch 1390, val loss: 0.8386971950531006
Epoch 1400, training loss: 62.335453033447266 = 0.06893475353717804 + 10.0 * 6.226651668548584
Epoch 1400, val loss: 0.8427808880805969
Epoch 1410, training loss: 62.33902359008789 = 0.06731227040290833 + 10.0 * 6.227170944213867
Epoch 1410, val loss: 0.8469506502151489
Epoch 1420, training loss: 62.34978485107422 = 0.06573714315891266 + 10.0 * 6.228404521942139
Epoch 1420, val loss: 0.8510947823524475
Epoch 1430, training loss: 62.35005187988281 = 0.06420201808214188 + 10.0 * 6.228585243225098
Epoch 1430, val loss: 0.8553319573402405
Epoch 1440, training loss: 62.3407096862793 = 0.06270962953567505 + 10.0 * 6.227799892425537
Epoch 1440, val loss: 0.8594255447387695
Epoch 1450, training loss: 62.32912063598633 = 0.061261557042598724 + 10.0 * 6.226786136627197
Epoch 1450, val loss: 0.8636467456817627
Epoch 1460, training loss: 62.36936950683594 = 0.05985122546553612 + 10.0 * 6.23095178604126
Epoch 1460, val loss: 0.8677718043327332
Epoch 1470, training loss: 62.30652618408203 = 0.05850296840071678 + 10.0 * 6.224802494049072
Epoch 1470, val loss: 0.871957004070282
Epoch 1480, training loss: 62.29954147338867 = 0.057191528379917145 + 10.0 * 6.2242350578308105
Epoch 1480, val loss: 0.8761438131332397
Epoch 1490, training loss: 62.29217529296875 = 0.05592948943376541 + 10.0 * 6.223624229431152
Epoch 1490, val loss: 0.8803077936172485
Epoch 1500, training loss: 62.28718566894531 = 0.05470911040902138 + 10.0 * 6.223247528076172
Epoch 1500, val loss: 0.884528398513794
Epoch 1510, training loss: 62.31681442260742 = 0.05351429432630539 + 10.0 * 6.226330280303955
Epoch 1510, val loss: 0.8887131810188293
Epoch 1520, training loss: 62.28242492675781 = 0.05235512927174568 + 10.0 * 6.223006725311279
Epoch 1520, val loss: 0.8927417397499084
Epoch 1530, training loss: 62.28227233886719 = 0.05121744051575661 + 10.0 * 6.223105430603027
Epoch 1530, val loss: 0.8969436287879944
Epoch 1540, training loss: 62.27616500854492 = 0.05013364180922508 + 10.0 * 6.2226033210754395
Epoch 1540, val loss: 0.9010676741600037
Epoch 1550, training loss: 62.35405349731445 = 0.049080390483140945 + 10.0 * 6.230497360229492
Epoch 1550, val loss: 0.9051980972290039
Epoch 1560, training loss: 62.304012298583984 = 0.04805208742618561 + 10.0 * 6.225595951080322
Epoch 1560, val loss: 0.9092500805854797
Epoch 1570, training loss: 62.26537322998047 = 0.04704178869724274 + 10.0 * 6.221833229064941
Epoch 1570, val loss: 0.9133153557777405
Epoch 1580, training loss: 62.25459289550781 = 0.0460783913731575 + 10.0 * 6.220851421356201
Epoch 1580, val loss: 0.9175367951393127
Epoch 1590, training loss: 62.254600524902344 = 0.045148033648729324 + 10.0 * 6.220945358276367
Epoch 1590, val loss: 0.9216223359107971
Epoch 1600, training loss: 62.31873321533203 = 0.044237617403268814 + 10.0 * 6.227449417114258
Epoch 1600, val loss: 0.9257473945617676
Epoch 1610, training loss: 62.27752685546875 = 0.043349601328372955 + 10.0 * 6.22341775894165
Epoch 1610, val loss: 0.9298210144042969
Epoch 1620, training loss: 62.2479362487793 = 0.04247761517763138 + 10.0 * 6.220545768737793
Epoch 1620, val loss: 0.9338505864143372
Epoch 1630, training loss: 62.256126403808594 = 0.04164864122867584 + 10.0 * 6.221447944641113
Epoch 1630, val loss: 0.9379602074623108
Epoch 1640, training loss: 62.24172592163086 = 0.04083007201552391 + 10.0 * 6.220089912414551
Epoch 1640, val loss: 0.9419187903404236
Epoch 1650, training loss: 62.236289978027344 = 0.040032438933849335 + 10.0 * 6.219625949859619
Epoch 1650, val loss: 0.9459625482559204
Epoch 1660, training loss: 62.232078552246094 = 0.03926872834563255 + 10.0 * 6.219281196594238
Epoch 1660, val loss: 0.9499868750572205
Epoch 1670, training loss: 62.25725173950195 = 0.038523510098457336 + 10.0 * 6.221872806549072
Epoch 1670, val loss: 0.9539571404457092
Epoch 1680, training loss: 62.23662185668945 = 0.0378015898168087 + 10.0 * 6.219882011413574
Epoch 1680, val loss: 0.9579157829284668
Epoch 1690, training loss: 62.255149841308594 = 0.037087779492139816 + 10.0 * 6.221806526184082
Epoch 1690, val loss: 0.9618037343025208
Epoch 1700, training loss: 62.22941970825195 = 0.03638583421707153 + 10.0 * 6.219303607940674
Epoch 1700, val loss: 0.9658035635948181
Epoch 1710, training loss: 62.213619232177734 = 0.03571327030658722 + 10.0 * 6.217790603637695
Epoch 1710, val loss: 0.9696671962738037
Epoch 1720, training loss: 62.21642303466797 = 0.03506696969270706 + 10.0 * 6.218135356903076
Epoch 1720, val loss: 0.9736272692680359
Epoch 1730, training loss: 62.24806594848633 = 0.0344342477619648 + 10.0 * 6.221363067626953
Epoch 1730, val loss: 0.9774734973907471
Epoch 1740, training loss: 62.23662185668945 = 0.03380492702126503 + 10.0 * 6.220281600952148
Epoch 1740, val loss: 0.9813079833984375
Epoch 1750, training loss: 62.20795822143555 = 0.03319578990340233 + 10.0 * 6.2174763679504395
Epoch 1750, val loss: 0.9851477742195129
Epoch 1760, training loss: 62.19542694091797 = 0.03260614722967148 + 10.0 * 6.216281890869141
Epoch 1760, val loss: 0.9890207648277283
Epoch 1770, training loss: 62.19670486450195 = 0.03203492984175682 + 10.0 * 6.216466903686523
Epoch 1770, val loss: 0.9929006099700928
Epoch 1780, training loss: 62.22368240356445 = 0.03148379549384117 + 10.0 * 6.219220161437988
Epoch 1780, val loss: 0.9966870546340942
Epoch 1790, training loss: 62.210350036621094 = 0.030941996723413467 + 10.0 * 6.217940807342529
Epoch 1790, val loss: 1.0005234479904175
Epoch 1800, training loss: 62.255794525146484 = 0.030410226434469223 + 10.0 * 6.222538471221924
Epoch 1800, val loss: 1.0042132139205933
Epoch 1810, training loss: 62.2000846862793 = 0.02986607886850834 + 10.0 * 6.217021942138672
Epoch 1810, val loss: 1.008010745048523
Epoch 1820, training loss: 62.181846618652344 = 0.029368897899985313 + 10.0 * 6.215247631072998
Epoch 1820, val loss: 1.0117700099945068
Epoch 1830, training loss: 62.17347717285156 = 0.02887713722884655 + 10.0 * 6.2144598960876465
Epoch 1830, val loss: 1.015499234199524
Epoch 1840, training loss: 62.1734619140625 = 0.028401251882314682 + 10.0 * 6.214506149291992
Epoch 1840, val loss: 1.0192493200302124
Epoch 1850, training loss: 62.287986755371094 = 0.02794474922120571 + 10.0 * 6.226004123687744
Epoch 1850, val loss: 1.022889494895935
Epoch 1860, training loss: 62.19993591308594 = 0.027473920956254005 + 10.0 * 6.217246055603027
Epoch 1860, val loss: 1.0264769792556763
Epoch 1870, training loss: 62.16551971435547 = 0.027021491900086403 + 10.0 * 6.2138495445251465
Epoch 1870, val loss: 1.0301567316055298
Epoch 1880, training loss: 62.16617965698242 = 0.026589328423142433 + 10.0 * 6.213959217071533
Epoch 1880, val loss: 1.0338388681411743
Epoch 1890, training loss: 62.20853805541992 = 0.02617437019944191 + 10.0 * 6.218236446380615
Epoch 1890, val loss: 1.0374951362609863
Epoch 1900, training loss: 62.160064697265625 = 0.02575031854212284 + 10.0 * 6.213431358337402
Epoch 1900, val loss: 1.0411127805709839
Epoch 1910, training loss: 62.158897399902344 = 0.025345927104353905 + 10.0 * 6.21335506439209
Epoch 1910, val loss: 1.0447036027908325
Epoch 1920, training loss: 62.17266845703125 = 0.024954264983534813 + 10.0 * 6.214771270751953
Epoch 1920, val loss: 1.048281192779541
Epoch 1930, training loss: 62.161903381347656 = 0.024565264582633972 + 10.0 * 6.213733673095703
Epoch 1930, val loss: 1.0517983436584473
Epoch 1940, training loss: 62.15459442138672 = 0.024182336404919624 + 10.0 * 6.213041305541992
Epoch 1940, val loss: 1.0553085803985596
Epoch 1950, training loss: 62.180938720703125 = 0.02381921000778675 + 10.0 * 6.215712070465088
Epoch 1950, val loss: 1.0588123798370361
Epoch 1960, training loss: 62.17354965209961 = 0.02345290593802929 + 10.0 * 6.215009689331055
Epoch 1960, val loss: 1.0623202323913574
Epoch 1970, training loss: 62.14656448364258 = 0.023090263828635216 + 10.0 * 6.212347507476807
Epoch 1970, val loss: 1.0657482147216797
Epoch 1980, training loss: 62.138916015625 = 0.022746017202734947 + 10.0 * 6.2116169929504395
Epoch 1980, val loss: 1.0693445205688477
Epoch 1990, training loss: 62.17683029174805 = 0.022408850491046906 + 10.0 * 6.215442180633545
Epoch 1990, val loss: 1.0727509260177612
Epoch 2000, training loss: 62.13440704345703 = 0.022075975313782692 + 10.0 * 6.211233139038086
Epoch 2000, val loss: 1.076172947883606
Epoch 2010, training loss: 62.13105392456055 = 0.021752145141363144 + 10.0 * 6.210930347442627
Epoch 2010, val loss: 1.0795296430587769
Epoch 2020, training loss: 62.1280403137207 = 0.02143530361354351 + 10.0 * 6.210660457611084
Epoch 2020, val loss: 1.082994818687439
Epoch 2030, training loss: 62.17082595825195 = 0.021132467314600945 + 10.0 * 6.214969158172607
Epoch 2030, val loss: 1.0863990783691406
Epoch 2040, training loss: 62.13887405395508 = 0.020823074504733086 + 10.0 * 6.2118048667907715
Epoch 2040, val loss: 1.0896483659744263
Epoch 2050, training loss: 62.136634826660156 = 0.0205267034471035 + 10.0 * 6.211610794067383
Epoch 2050, val loss: 1.093033790588379
Epoch 2060, training loss: 62.127376556396484 = 0.020236294716596603 + 10.0 * 6.210713863372803
Epoch 2060, val loss: 1.0962754487991333
Epoch 2070, training loss: 62.14409255981445 = 0.019961588084697723 + 10.0 * 6.2124128341674805
Epoch 2070, val loss: 1.0995843410491943
Epoch 2080, training loss: 62.131378173828125 = 0.019679106771945953 + 10.0 * 6.211169719696045
Epoch 2080, val loss: 1.1028831005096436
Epoch 2090, training loss: 62.12651824951172 = 0.019400911405682564 + 10.0 * 6.210711479187012
Epoch 2090, val loss: 1.1061166524887085
Epoch 2100, training loss: 62.13894271850586 = 0.01913798600435257 + 10.0 * 6.21198034286499
Epoch 2100, val loss: 1.1094205379486084
Epoch 2110, training loss: 62.13325881958008 = 0.018870826810598373 + 10.0 * 6.2114386558532715
Epoch 2110, val loss: 1.1125682592391968
Epoch 2120, training loss: 62.11867904663086 = 0.018618788570165634 + 10.0 * 6.210005760192871
Epoch 2120, val loss: 1.1156713962554932
Epoch 2130, training loss: 62.11452865600586 = 0.018368631601333618 + 10.0 * 6.209616184234619
Epoch 2130, val loss: 1.11893892288208
Epoch 2140, training loss: 62.142250061035156 = 0.01812770776450634 + 10.0 * 6.212412357330322
Epoch 2140, val loss: 1.1220983266830444
Epoch 2150, training loss: 62.114437103271484 = 0.017881369218230247 + 10.0 * 6.20965576171875
Epoch 2150, val loss: 1.125252366065979
Epoch 2160, training loss: 62.098106384277344 = 0.01764349639415741 + 10.0 * 6.2080464363098145
Epoch 2160, val loss: 1.1283397674560547
Epoch 2170, training loss: 62.099891662597656 = 0.017417212948203087 + 10.0 * 6.208247184753418
Epoch 2170, val loss: 1.131518840789795
Epoch 2180, training loss: 62.11003494262695 = 0.01719316467642784 + 10.0 * 6.20928430557251
Epoch 2180, val loss: 1.1346242427825928
Epoch 2190, training loss: 62.14236831665039 = 0.01697617582976818 + 10.0 * 6.212539196014404
Epoch 2190, val loss: 1.1376522779464722
Epoch 2200, training loss: 62.1002197265625 = 0.016748126596212387 + 10.0 * 6.208347320556641
Epoch 2200, val loss: 1.14067804813385
Epoch 2210, training loss: 62.08732604980469 = 0.016532927751541138 + 10.0 * 6.2070794105529785
Epoch 2210, val loss: 1.1436339616775513
Epoch 2220, training loss: 62.09400939941406 = 0.016328027471899986 + 10.0 * 6.207768440246582
Epoch 2220, val loss: 1.1467736959457397
Epoch 2230, training loss: 62.11166763305664 = 0.01612476259469986 + 10.0 * 6.209554195404053
Epoch 2230, val loss: 1.149817943572998
Epoch 2240, training loss: 62.107906341552734 = 0.015923306345939636 + 10.0 * 6.209198474884033
Epoch 2240, val loss: 1.1527464389801025
Epoch 2250, training loss: 62.10251998901367 = 0.015722399577498436 + 10.0 * 6.208679676055908
Epoch 2250, val loss: 1.1557384729385376
Epoch 2260, training loss: 62.08205795288086 = 0.015525110997259617 + 10.0 * 6.206653118133545
Epoch 2260, val loss: 1.1586554050445557
Epoch 2270, training loss: 62.09355163574219 = 0.015337140299379826 + 10.0 * 6.207821369171143
Epoch 2270, val loss: 1.1616703271865845
Epoch 2280, training loss: 62.112274169921875 = 0.015150986611843109 + 10.0 * 6.209712028503418
Epoch 2280, val loss: 1.1644538640975952
Epoch 2290, training loss: 62.07971954345703 = 0.014967073686420918 + 10.0 * 6.206475257873535
Epoch 2290, val loss: 1.1673312187194824
Epoch 2300, training loss: 62.067989349365234 = 0.014786307699978352 + 10.0 * 6.205320358276367
Epoch 2300, val loss: 1.170261263847351
Epoch 2310, training loss: 62.0726432800293 = 0.014611180871725082 + 10.0 * 6.205803394317627
Epoch 2310, val loss: 1.1731475591659546
Epoch 2320, training loss: 62.13278579711914 = 0.014442987740039825 + 10.0 * 6.21183443069458
Epoch 2320, val loss: 1.1759707927703857
Epoch 2330, training loss: 62.08744430541992 = 0.014271353371441364 + 10.0 * 6.207317352294922
Epoch 2330, val loss: 1.1787097454071045
Epoch 2340, training loss: 62.0710563659668 = 0.014101807028055191 + 10.0 * 6.205695152282715
Epoch 2340, val loss: 1.1815426349639893
Epoch 2350, training loss: 62.0819091796875 = 0.013941511511802673 + 10.0 * 6.206796646118164
Epoch 2350, val loss: 1.1844381093978882
Epoch 2360, training loss: 62.11073303222656 = 0.013779361732304096 + 10.0 * 6.209695339202881
Epoch 2360, val loss: 1.1871192455291748
Epoch 2370, training loss: 62.07081985473633 = 0.01361391507089138 + 10.0 * 6.2057204246521
Epoch 2370, val loss: 1.189911961555481
Epoch 2380, training loss: 62.05390930175781 = 0.013457358814775944 + 10.0 * 6.204045295715332
Epoch 2380, val loss: 1.1925880908966064
Epoch 2390, training loss: 62.0513916015625 = 0.013306776061654091 + 10.0 * 6.203808784484863
Epoch 2390, val loss: 1.1953061819076538
Epoch 2400, training loss: 62.05280303955078 = 0.013159766793251038 + 10.0 * 6.2039642333984375
Epoch 2400, val loss: 1.1981168985366821
Epoch 2410, training loss: 62.09567642211914 = 0.01301780715584755 + 10.0 * 6.208265781402588
Epoch 2410, val loss: 1.2007702589035034
Epoch 2420, training loss: 62.066341400146484 = 0.012870736420154572 + 10.0 * 6.205347061157227
Epoch 2420, val loss: 1.2034121751785278
Epoch 2430, training loss: 62.09551239013672 = 0.012727424502372742 + 10.0 * 6.208278656005859
Epoch 2430, val loss: 1.2059481143951416
Epoch 2440, training loss: 62.056880950927734 = 0.012580987066030502 + 10.0 * 6.204430103302002
Epoch 2440, val loss: 1.208608627319336
Epoch 2450, training loss: 62.044647216796875 = 0.012445692904293537 + 10.0 * 6.203219890594482
Epoch 2450, val loss: 1.211208462715149
Epoch 2460, training loss: 62.06641387939453 = 0.012315717525780201 + 10.0 * 6.205410003662109
Epoch 2460, val loss: 1.213897943496704
Epoch 2470, training loss: 62.05795669555664 = 0.01217980869114399 + 10.0 * 6.204577445983887
Epoch 2470, val loss: 1.2163784503936768
Epoch 2480, training loss: 62.03762435913086 = 0.012041745707392693 + 10.0 * 6.2025580406188965
Epoch 2480, val loss: 1.21877920627594
Epoch 2490, training loss: 62.03799057006836 = 0.01191315334290266 + 10.0 * 6.20260763168335
Epoch 2490, val loss: 1.2214224338531494
Epoch 2500, training loss: 62.03575134277344 = 0.011789854615926743 + 10.0 * 6.202395915985107
Epoch 2500, val loss: 1.224057674407959
Epoch 2510, training loss: 62.06817626953125 = 0.011667139828205109 + 10.0 * 6.20565128326416
Epoch 2510, val loss: 1.2265576124191284
Epoch 2520, training loss: 62.04185485839844 = 0.011541859246790409 + 10.0 * 6.203031063079834
Epoch 2520, val loss: 1.2288763523101807
Epoch 2530, training loss: 62.03397750854492 = 0.011419427581131458 + 10.0 * 6.202255725860596
Epoch 2530, val loss: 1.2314155101776123
Epoch 2540, training loss: 62.035179138183594 = 0.011299186386168003 + 10.0 * 6.202387809753418
Epoch 2540, val loss: 1.2339197397232056
Epoch 2550, training loss: 62.042694091796875 = 0.01118583232164383 + 10.0 * 6.203150749206543
Epoch 2550, val loss: 1.2364475727081299
Epoch 2560, training loss: 62.05778884887695 = 0.011073545552790165 + 10.0 * 6.204671382904053
Epoch 2560, val loss: 1.2388554811477661
Epoch 2570, training loss: 62.033592224121094 = 0.010957383550703526 + 10.0 * 6.202263832092285
Epoch 2570, val loss: 1.2412718534469604
Epoch 2580, training loss: 62.03589630126953 = 0.010846559889614582 + 10.0 * 6.202505111694336
Epoch 2580, val loss: 1.2437374591827393
Epoch 2590, training loss: 62.056671142578125 = 0.010737704113125801 + 10.0 * 6.204593181610107
Epoch 2590, val loss: 1.2461508512496948
Epoch 2600, training loss: 62.03692626953125 = 0.010631409473717213 + 10.0 * 6.202629566192627
Epoch 2600, val loss: 1.2485648393630981
Epoch 2610, training loss: 62.045352935791016 = 0.010526416823267937 + 10.0 * 6.203482627868652
Epoch 2610, val loss: 1.2509509325027466
Epoch 2620, training loss: 62.02630615234375 = 0.010420534759759903 + 10.0 * 6.2015886306762695
Epoch 2620, val loss: 1.2531449794769287
Epoch 2630, training loss: 62.03498840332031 = 0.01031925156712532 + 10.0 * 6.20246696472168
Epoch 2630, val loss: 1.2555636167526245
Epoch 2640, training loss: 62.036376953125 = 0.010216951370239258 + 10.0 * 6.202616214752197
Epoch 2640, val loss: 1.2578996419906616
Epoch 2650, training loss: 62.02125930786133 = 0.010116446763277054 + 10.0 * 6.201114177703857
Epoch 2650, val loss: 1.2601429224014282
Epoch 2660, training loss: 62.02099609375 = 0.010018894448876381 + 10.0 * 6.2010979652404785
Epoch 2660, val loss: 1.2624672651290894
Epoch 2670, training loss: 62.048770904541016 = 0.00992173608392477 + 10.0 * 6.203885078430176
Epoch 2670, val loss: 1.2647082805633545
Epoch 2680, training loss: 62.04914093017578 = 0.00982682779431343 + 10.0 * 6.2039313316345215
Epoch 2680, val loss: 1.266858696937561
Epoch 2690, training loss: 62.02757263183594 = 0.00973229669034481 + 10.0 * 6.201784133911133
Epoch 2690, val loss: 1.2691640853881836
Epoch 2700, training loss: 62.01225662231445 = 0.009640096686780453 + 10.0 * 6.20026159286499
Epoch 2700, val loss: 1.2713652849197388
Epoch 2710, training loss: 62.016841888427734 = 0.00955126155167818 + 10.0 * 6.200728893280029
Epoch 2710, val loss: 1.2736704349517822
Epoch 2720, training loss: 62.0474967956543 = 0.00946291908621788 + 10.0 * 6.203803062438965
Epoch 2720, val loss: 1.2758315801620483
Epoch 2730, training loss: 62.0254020690918 = 0.009371201507747173 + 10.0 * 6.201602935791016
Epoch 2730, val loss: 1.2779945135116577
Epoch 2740, training loss: 62.01387023925781 = 0.009285751730203629 + 10.0 * 6.200458526611328
Epoch 2740, val loss: 1.2799972295761108
Epoch 2750, training loss: 62.02161407470703 = 0.009200640954077244 + 10.0 * 6.201241493225098
Epoch 2750, val loss: 1.282313585281372
Epoch 2760, training loss: 62.014183044433594 = 0.009116736240684986 + 10.0 * 6.200506687164307
Epoch 2760, val loss: 1.2844346761703491
Epoch 2770, training loss: 62.01826858520508 = 0.009034046903252602 + 10.0 * 6.200923442840576
Epoch 2770, val loss: 1.2865151166915894
Epoch 2780, training loss: 62.016109466552734 = 0.00895089004188776 + 10.0 * 6.200716018676758
Epoch 2780, val loss: 1.2885119915008545
Epoch 2790, training loss: 62.01606369018555 = 0.008868598379194736 + 10.0 * 6.200719356536865
Epoch 2790, val loss: 1.290621280670166
Epoch 2800, training loss: 62.008079528808594 = 0.008788409642875195 + 10.0 * 6.199929237365723
Epoch 2800, val loss: 1.2926174402236938
Epoch 2810, training loss: 62.03602981567383 = 0.00871176179498434 + 10.0 * 6.202731609344482
Epoch 2810, val loss: 1.2946295738220215
Epoch 2820, training loss: 62.000545501708984 = 0.00863130483776331 + 10.0 * 6.199191093444824
Epoch 2820, val loss: 1.2966235876083374
Epoch 2830, training loss: 61.997684478759766 = 0.0085580013692379 + 10.0 * 6.198912620544434
Epoch 2830, val loss: 1.2985880374908447
Epoch 2840, training loss: 61.98892593383789 = 0.008483171463012695 + 10.0 * 6.198044300079346
Epoch 2840, val loss: 1.3006926774978638
Epoch 2850, training loss: 61.9952278137207 = 0.008411613292992115 + 10.0 * 6.198681831359863
Epoch 2850, val loss: 1.3026224374771118
Epoch 2860, training loss: 62.06272888183594 = 0.008341033942997456 + 10.0 * 6.205438613891602
Epoch 2860, val loss: 1.3044630289077759
Epoch 2870, training loss: 62.005985260009766 = 0.008262594230473042 + 10.0 * 6.199772357940674
Epoch 2870, val loss: 1.306442379951477
Epoch 2880, training loss: 61.98549270629883 = 0.008191857486963272 + 10.0 * 6.19773006439209
Epoch 2880, val loss: 1.3083921670913696
Epoch 2890, training loss: 61.99324035644531 = 0.008124787360429764 + 10.0 * 6.198511600494385
Epoch 2890, val loss: 1.3103808164596558
Epoch 2900, training loss: 62.023860931396484 = 0.008058188483119011 + 10.0 * 6.20158052444458
Epoch 2900, val loss: 1.3122315406799316
Epoch 2910, training loss: 61.994606018066406 = 0.007987059652805328 + 10.0 * 6.198661804199219
Epoch 2910, val loss: 1.3139103651046753
Epoch 2920, training loss: 61.98074722290039 = 0.00791860744357109 + 10.0 * 6.197282791137695
Epoch 2920, val loss: 1.315872073173523
Epoch 2930, training loss: 61.99930191040039 = 0.007856064476072788 + 10.0 * 6.1991448402404785
Epoch 2930, val loss: 1.3177305459976196
Epoch 2940, training loss: 62.008846282958984 = 0.00779191218316555 + 10.0 * 6.2001051902771
Epoch 2940, val loss: 1.3195277452468872
Epoch 2950, training loss: 61.989349365234375 = 0.007725389674305916 + 10.0 * 6.19816255569458
Epoch 2950, val loss: 1.3212530612945557
Epoch 2960, training loss: 61.98884582519531 = 0.007662770338356495 + 10.0 * 6.198118209838867
Epoch 2960, val loss: 1.3230763673782349
Epoch 2970, training loss: 62.00229263305664 = 0.007601126097142696 + 10.0 * 6.199469089508057
Epoch 2970, val loss: 1.3247424364089966
Epoch 2980, training loss: 62.000038146972656 = 0.007537571247667074 + 10.0 * 6.199250221252441
Epoch 2980, val loss: 1.3265599012374878
Epoch 2990, training loss: 61.99679946899414 = 0.007475722581148148 + 10.0 * 6.19893217086792
Epoch 2990, val loss: 1.328062891960144
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8429098576700054
=== training gcn model ===
Epoch 0, training loss: 87.9264144897461 = 1.9582931995391846 + 10.0 * 8.59681224822998
Epoch 0, val loss: 1.9588367938995361
Epoch 10, training loss: 87.90855407714844 = 1.9485118389129639 + 10.0 * 8.596004486083984
Epoch 10, val loss: 1.9493286609649658
Epoch 20, training loss: 87.8338623046875 = 1.9368749856948853 + 10.0 * 8.589698791503906
Epoch 20, val loss: 1.937330961227417
Epoch 30, training loss: 87.3687744140625 = 1.9220918416976929 + 10.0 * 8.544668197631836
Epoch 30, val loss: 1.9217400550842285
Epoch 40, training loss: 84.5436019897461 = 1.9054248332977295 + 10.0 * 8.26381778717041
Epoch 40, val loss: 1.904708981513977
Epoch 50, training loss: 76.69383239746094 = 1.8861736059188843 + 10.0 * 7.4807658195495605
Epoch 50, val loss: 1.8847754001617432
Epoch 60, training loss: 74.43075561523438 = 1.8686579465866089 + 10.0 * 7.256209850311279
Epoch 60, val loss: 1.8681836128234863
Epoch 70, training loss: 72.04110717773438 = 1.855334758758545 + 10.0 * 7.018577575683594
Epoch 70, val loss: 1.8557124137878418
Epoch 80, training loss: 70.81072235107422 = 1.8427940607070923 + 10.0 * 6.896792888641357
Epoch 80, val loss: 1.8427910804748535
Epoch 90, training loss: 70.08565521240234 = 1.8311238288879395 + 10.0 * 6.825453281402588
Epoch 90, val loss: 1.8312959671020508
Epoch 100, training loss: 69.35679626464844 = 1.8200316429138184 + 10.0 * 6.753676891326904
Epoch 100, val loss: 1.820565938949585
Epoch 110, training loss: 68.8298568725586 = 1.8110761642456055 + 10.0 * 6.701878547668457
Epoch 110, val loss: 1.8118900060653687
Epoch 120, training loss: 68.42279815673828 = 1.8025546073913574 + 10.0 * 6.662024974822998
Epoch 120, val loss: 1.8032994270324707
Epoch 130, training loss: 67.99774932861328 = 1.7944577932357788 + 10.0 * 6.620328903198242
Epoch 130, val loss: 1.7955402135849
Epoch 140, training loss: 67.5823974609375 = 1.7875707149505615 + 10.0 * 6.5794830322265625
Epoch 140, val loss: 1.7887698411941528
Epoch 150, training loss: 67.26802825927734 = 1.7810755968093872 + 10.0 * 6.548695087432861
Epoch 150, val loss: 1.7826505899429321
Epoch 160, training loss: 66.9679946899414 = 1.7739719152450562 + 10.0 * 6.519402027130127
Epoch 160, val loss: 1.7763452529907227
Epoch 170, training loss: 66.74235534667969 = 1.766421914100647 + 10.0 * 6.497593879699707
Epoch 170, val loss: 1.7697941064834595
Epoch 180, training loss: 66.5482406616211 = 1.7584874629974365 + 10.0 * 6.478975772857666
Epoch 180, val loss: 1.7628469467163086
Epoch 190, training loss: 66.37942504882812 = 1.7499102354049683 + 10.0 * 6.46295166015625
Epoch 190, val loss: 1.7555691003799438
Epoch 200, training loss: 66.2558364868164 = 1.7405391931533813 + 10.0 * 6.451529502868652
Epoch 200, val loss: 1.747832179069519
Epoch 210, training loss: 66.13306427001953 = 1.7302277088165283 + 10.0 * 6.44028377532959
Epoch 210, val loss: 1.7393083572387695
Epoch 220, training loss: 66.01445770263672 = 1.7191214561462402 + 10.0 * 6.429533004760742
Epoch 220, val loss: 1.7301474809646606
Epoch 230, training loss: 65.90715789794922 = 1.707133173942566 + 10.0 * 6.420002460479736
Epoch 230, val loss: 1.7205421924591064
Epoch 240, training loss: 65.85731506347656 = 1.6940464973449707 + 10.0 * 6.416326522827148
Epoch 240, val loss: 1.7101014852523804
Epoch 250, training loss: 65.72067260742188 = 1.6798738241195679 + 10.0 * 6.404080390930176
Epoch 250, val loss: 1.69869863986969
Epoch 260, training loss: 65.62194061279297 = 1.6645716428756714 + 10.0 * 6.395737171173096
Epoch 260, val loss: 1.6865942478179932
Epoch 270, training loss: 65.5317611694336 = 1.6479644775390625 + 10.0 * 6.3883795738220215
Epoch 270, val loss: 1.6734539270401
Epoch 280, training loss: 65.46265411376953 = 1.6298925876617432 + 10.0 * 6.383275985717773
Epoch 280, val loss: 1.659067988395691
Epoch 290, training loss: 65.36197662353516 = 1.610434651374817 + 10.0 * 6.3751540184021
Epoch 290, val loss: 1.6436026096343994
Epoch 300, training loss: 65.27586364746094 = 1.5895519256591797 + 10.0 * 6.368631362915039
Epoch 300, val loss: 1.6271594762802124
Epoch 310, training loss: 65.21163940429688 = 1.5671764612197876 + 10.0 * 6.364446640014648
Epoch 310, val loss: 1.6096222400665283
Epoch 320, training loss: 65.15060424804688 = 1.5433979034423828 + 10.0 * 6.360720634460449
Epoch 320, val loss: 1.5905861854553223
Epoch 330, training loss: 65.04757690429688 = 1.5181617736816406 + 10.0 * 6.352941513061523
Epoch 330, val loss: 1.5708814859390259
Epoch 340, training loss: 64.97052764892578 = 1.4916306734085083 + 10.0 * 6.3478899002075195
Epoch 340, val loss: 1.5502440929412842
Epoch 350, training loss: 64.92049407958984 = 1.4636411666870117 + 10.0 * 6.3456854820251465
Epoch 350, val loss: 1.5285218954086304
Epoch 360, training loss: 64.8370590209961 = 1.4346929788589478 + 10.0 * 6.340236663818359
Epoch 360, val loss: 1.5059959888458252
Epoch 370, training loss: 64.75788116455078 = 1.4048250913619995 + 10.0 * 6.335305690765381
Epoch 370, val loss: 1.482878565788269
Epoch 380, training loss: 64.71712493896484 = 1.3742496967315674 + 10.0 * 6.334287166595459
Epoch 380, val loss: 1.459120273590088
Epoch 390, training loss: 64.65423583984375 = 1.3430583477020264 + 10.0 * 6.331118106842041
Epoch 390, val loss: 1.4353777170181274
Epoch 400, training loss: 64.57109832763672 = 1.3117773532867432 + 10.0 * 6.325932502746582
Epoch 400, val loss: 1.4116218090057373
Epoch 410, training loss: 64.50132751464844 = 1.2804150581359863 + 10.0 * 6.322091102600098
Epoch 410, val loss: 1.3880889415740967
Epoch 420, training loss: 64.50697326660156 = 1.2489415407180786 + 10.0 * 6.325803279876709
Epoch 420, val loss: 1.3648087978363037
Epoch 430, training loss: 64.38235473632812 = 1.2178356647491455 + 10.0 * 6.316451549530029
Epoch 430, val loss: 1.3414572477340698
Epoch 440, training loss: 64.32689666748047 = 1.1872025728225708 + 10.0 * 6.313969612121582
Epoch 440, val loss: 1.3189970254898071
Epoch 450, training loss: 64.27339172363281 = 1.1569421291351318 + 10.0 * 6.311645030975342
Epoch 450, val loss: 1.2970566749572754
Epoch 460, training loss: 64.2397689819336 = 1.1271233558654785 + 10.0 * 6.311264991760254
Epoch 460, val loss: 1.2754896879196167
Epoch 470, training loss: 64.17025756835938 = 1.0977420806884766 + 10.0 * 6.307251453399658
Epoch 470, val loss: 1.2547138929367065
Epoch 480, training loss: 64.10774993896484 = 1.0690360069274902 + 10.0 * 6.303871154785156
Epoch 480, val loss: 1.2346360683441162
Epoch 490, training loss: 64.0682601928711 = 1.0408765077590942 + 10.0 * 6.302738666534424
Epoch 490, val loss: 1.215049147605896
Epoch 500, training loss: 64.019287109375 = 1.0132627487182617 + 10.0 * 6.300602436065674
Epoch 500, val loss: 1.195801019668579
Epoch 510, training loss: 63.99571990966797 = 0.9861971139907837 + 10.0 * 6.300951957702637
Epoch 510, val loss: 1.1774088144302368
Epoch 520, training loss: 63.92165756225586 = 0.9596749544143677 + 10.0 * 6.29619836807251
Epoch 520, val loss: 1.1599769592285156
Epoch 530, training loss: 63.87116622924805 = 0.9338924288749695 + 10.0 * 6.293727397918701
Epoch 530, val loss: 1.1427464485168457
Epoch 540, training loss: 63.86213684082031 = 0.9085251688957214 + 10.0 * 6.295361518859863
Epoch 540, val loss: 1.1263197660446167
Epoch 550, training loss: 63.802223205566406 = 0.8837885856628418 + 10.0 * 6.291843414306641
Epoch 550, val loss: 1.1102946996688843
Epoch 560, training loss: 63.75041198730469 = 0.8595715165138245 + 10.0 * 6.289083957672119
Epoch 560, val loss: 1.0947288274765015
Epoch 570, training loss: 63.7099494934082 = 0.8358998894691467 + 10.0 * 6.287405014038086
Epoch 570, val loss: 1.0798618793487549
Epoch 580, training loss: 63.666847229003906 = 0.812921404838562 + 10.0 * 6.285392761230469
Epoch 580, val loss: 1.065394639968872
Epoch 590, training loss: 63.65620040893555 = 0.7902724146842957 + 10.0 * 6.286592960357666
Epoch 590, val loss: 1.0517793893814087
Epoch 600, training loss: 63.588951110839844 = 0.7683010101318359 + 10.0 * 6.282064914703369
Epoch 600, val loss: 1.038238286972046
Epoch 610, training loss: 63.54936981201172 = 0.7468406558036804 + 10.0 * 6.280252933502197
Epoch 610, val loss: 1.0258458852767944
Epoch 620, training loss: 63.595909118652344 = 0.7257469892501831 + 10.0 * 6.28701639175415
Epoch 620, val loss: 1.0140790939331055
Epoch 630, training loss: 63.479061126708984 = 0.7053784728050232 + 10.0 * 6.277368068695068
Epoch 630, val loss: 1.0023138523101807
Epoch 640, training loss: 63.44791030883789 = 0.6855058670043945 + 10.0 * 6.276240348815918
Epoch 640, val loss: 0.9914597272872925
Epoch 650, training loss: 63.4098014831543 = 0.6660666465759277 + 10.0 * 6.274373531341553
Epoch 650, val loss: 0.9812666177749634
Epoch 660, training loss: 63.39985656738281 = 0.6470049619674683 + 10.0 * 6.275285243988037
Epoch 660, val loss: 0.9716204404830933
Epoch 670, training loss: 63.36998748779297 = 0.628362238407135 + 10.0 * 6.274162769317627
Epoch 670, val loss: 0.9620842933654785
Epoch 680, training loss: 63.349735260009766 = 0.6100804805755615 + 10.0 * 6.273965358734131
Epoch 680, val loss: 0.9532380104064941
Epoch 690, training loss: 63.29724884033203 = 0.5923003554344177 + 10.0 * 6.2704949378967285
Epoch 690, val loss: 0.9450168609619141
Epoch 700, training loss: 63.259429931640625 = 0.5749650001525879 + 10.0 * 6.268446445465088
Epoch 700, val loss: 0.9372411370277405
Epoch 710, training loss: 63.23336410522461 = 0.5579860806465149 + 10.0 * 6.267537593841553
Epoch 710, val loss: 0.9298995137214661
Epoch 720, training loss: 63.23506164550781 = 0.5413104891777039 + 10.0 * 6.269375324249268
Epoch 720, val loss: 0.9229687452316284
Epoch 730, training loss: 63.212921142578125 = 0.5248134732246399 + 10.0 * 6.268810749053955
Epoch 730, val loss: 0.9165319800376892
Epoch 740, training loss: 63.1583137512207 = 0.5088880658149719 + 10.0 * 6.264942646026611
Epoch 740, val loss: 0.9104253649711609
Epoch 750, training loss: 63.13161087036133 = 0.4932158589363098 + 10.0 * 6.263839244842529
Epoch 750, val loss: 0.9050198793411255
Epoch 760, training loss: 63.14484405517578 = 0.47794994711875916 + 10.0 * 6.266689300537109
Epoch 760, val loss: 0.8997299075126648
Epoch 770, training loss: 63.0869026184082 = 0.4630104601383209 + 10.0 * 6.262389183044434
Epoch 770, val loss: 0.8947905898094177
Epoch 780, training loss: 63.05299758911133 = 0.44847458600997925 + 10.0 * 6.2604522705078125
Epoch 780, val loss: 0.8903877139091492
Epoch 790, training loss: 63.03202438354492 = 0.4343102276325226 + 10.0 * 6.259771347045898
Epoch 790, val loss: 0.8862392902374268
Epoch 800, training loss: 63.0482292175293 = 0.4204460382461548 + 10.0 * 6.262778282165527
Epoch 800, val loss: 0.882451057434082
Epoch 810, training loss: 63.02360534667969 = 0.40690869092941284 + 10.0 * 6.261669635772705
Epoch 810, val loss: 0.8788937330245972
Epoch 820, training loss: 62.97528839111328 = 0.39364609122276306 + 10.0 * 6.258164405822754
Epoch 820, val loss: 0.8760408163070679
Epoch 830, training loss: 62.959957122802734 = 0.3808560371398926 + 10.0 * 6.257910251617432
Epoch 830, val loss: 0.8733047842979431
Epoch 840, training loss: 62.92237854003906 = 0.36844345927238464 + 10.0 * 6.2553935050964355
Epoch 840, val loss: 0.8708821535110474
Epoch 850, training loss: 62.90456008911133 = 0.35638004541397095 + 10.0 * 6.254817962646484
Epoch 850, val loss: 0.868867039680481
Epoch 860, training loss: 62.89202880859375 = 0.3446589410305023 + 10.0 * 6.25473690032959
Epoch 860, val loss: 0.8670481443405151
Epoch 870, training loss: 62.87661361694336 = 0.33323171734809875 + 10.0 * 6.254338264465332
Epoch 870, val loss: 0.8655291795730591
Epoch 880, training loss: 62.87702560424805 = 0.3220992088317871 + 10.0 * 6.255492687225342
Epoch 880, val loss: 0.8645839691162109
Epoch 890, training loss: 62.853885650634766 = 0.31141966581344604 + 10.0 * 6.254246711730957
Epoch 890, val loss: 0.8631647229194641
Epoch 900, training loss: 62.81912612915039 = 0.30109599232673645 + 10.0 * 6.251802921295166
Epoch 900, val loss: 0.8623958230018616
Epoch 910, training loss: 62.7935791015625 = 0.29117903113365173 + 10.0 * 6.250239849090576
Epoch 910, val loss: 0.8621637225151062
Epoch 920, training loss: 62.77463150024414 = 0.2815917730331421 + 10.0 * 6.249303817749023
Epoch 920, val loss: 0.8621014356613159
Epoch 930, training loss: 62.79158401489258 = 0.27235907316207886 + 10.0 * 6.251922607421875
Epoch 930, val loss: 0.8621052503585815
Epoch 940, training loss: 62.77403259277344 = 0.26339036226272583 + 10.0 * 6.251064300537109
Epoch 940, val loss: 0.8624444603919983
Epoch 950, training loss: 62.73856735229492 = 0.25474753975868225 + 10.0 * 6.248381614685059
Epoch 950, val loss: 0.862919270992279
Epoch 960, training loss: 62.71648025512695 = 0.2464679479598999 + 10.0 * 6.2470011711120605
Epoch 960, val loss: 0.8637621402740479
Epoch 970, training loss: 62.70697784423828 = 0.23851130902767181 + 10.0 * 6.246846675872803
Epoch 970, val loss: 0.8649336695671082
Epoch 980, training loss: 62.71916580200195 = 0.2308247834444046 + 10.0 * 6.248834133148193
Epoch 980, val loss: 0.866151750087738
Epoch 990, training loss: 62.7212028503418 = 0.22339484095573425 + 10.0 * 6.249780654907227
Epoch 990, val loss: 0.867388129234314
Epoch 1000, training loss: 62.678001403808594 = 0.21624308824539185 + 10.0 * 6.246175765991211
Epoch 1000, val loss: 0.8689971566200256
Epoch 1010, training loss: 62.64912796020508 = 0.20939190685749054 + 10.0 * 6.243973731994629
Epoch 1010, val loss: 0.8706790804862976
Epoch 1020, training loss: 62.657020568847656 = 0.2027914673089981 + 10.0 * 6.245422840118408
Epoch 1020, val loss: 0.8726916313171387
Epoch 1030, training loss: 62.637184143066406 = 0.19642247259616852 + 10.0 * 6.244076251983643
Epoch 1030, val loss: 0.8748056888580322
Epoch 1040, training loss: 62.64048767089844 = 0.19028286635875702 + 10.0 * 6.245020389556885
Epoch 1040, val loss: 0.8769903779029846
Epoch 1050, training loss: 62.60220718383789 = 0.18441401422023773 + 10.0 * 6.241779327392578
Epoch 1050, val loss: 0.879325807094574
Epoch 1060, training loss: 62.5950813293457 = 0.17876076698303223 + 10.0 * 6.241631984710693
Epoch 1060, val loss: 0.8818365931510925
Epoch 1070, training loss: 62.612449645996094 = 0.17332357168197632 + 10.0 * 6.243912696838379
Epoch 1070, val loss: 0.8844355940818787
Epoch 1080, training loss: 62.5739631652832 = 0.16802825033664703 + 10.0 * 6.240593433380127
Epoch 1080, val loss: 0.8872926831245422
Epoch 1090, training loss: 62.57773208618164 = 0.1629708856344223 + 10.0 * 6.241476058959961
Epoch 1090, val loss: 0.890129029750824
Epoch 1100, training loss: 62.574371337890625 = 0.15810684859752655 + 10.0 * 6.241626262664795
Epoch 1100, val loss: 0.8931002616882324
Epoch 1110, training loss: 62.543601989746094 = 0.15339849889278412 + 10.0 * 6.239020347595215
Epoch 1110, val loss: 0.8962950706481934
Epoch 1120, training loss: 62.53712844848633 = 0.14887987077236176 + 10.0 * 6.238824844360352
Epoch 1120, val loss: 0.8995659351348877
Epoch 1130, training loss: 62.56776809692383 = 0.1445387303829193 + 10.0 * 6.24232292175293
Epoch 1130, val loss: 0.902901291847229
Epoch 1140, training loss: 62.541812896728516 = 0.14032438397407532 + 10.0 * 6.240149021148682
Epoch 1140, val loss: 0.9064049124717712
Epoch 1150, training loss: 62.51725769042969 = 0.136274516582489 + 10.0 * 6.23809814453125
Epoch 1150, val loss: 0.9098125696182251
Epoch 1160, training loss: 62.4967041015625 = 0.13237664103507996 + 10.0 * 6.2364325523376465
Epoch 1160, val loss: 0.9134770035743713
Epoch 1170, training loss: 62.49795913696289 = 0.12863598763942719 + 10.0 * 6.236932277679443
Epoch 1170, val loss: 0.9172430038452148
Epoch 1180, training loss: 62.517356872558594 = 0.12501110136508942 + 10.0 * 6.239234447479248
Epoch 1180, val loss: 0.9208345413208008
Epoch 1190, training loss: 62.493038177490234 = 0.12149425595998764 + 10.0 * 6.237154483795166
Epoch 1190, val loss: 0.9246139526367188
Epoch 1200, training loss: 62.487022399902344 = 0.11808617413043976 + 10.0 * 6.236893653869629
Epoch 1200, val loss: 0.9284912943840027
Epoch 1210, training loss: 62.47244644165039 = 0.1148163452744484 + 10.0 * 6.235763072967529
Epoch 1210, val loss: 0.9324738383293152
Epoch 1220, training loss: 62.45728302001953 = 0.11166603118181229 + 10.0 * 6.234561443328857
Epoch 1220, val loss: 0.9364413619041443
Epoch 1230, training loss: 62.45722961425781 = 0.10863486677408218 + 10.0 * 6.234859466552734
Epoch 1230, val loss: 0.9403884410858154
Epoch 1240, training loss: 62.470096588134766 = 0.1057169958949089 + 10.0 * 6.236437797546387
Epoch 1240, val loss: 0.9444097876548767
Epoch 1250, training loss: 62.437889099121094 = 0.1028536930680275 + 10.0 * 6.233503818511963
Epoch 1250, val loss: 0.9484883546829224
Epoch 1260, training loss: 62.434791564941406 = 0.10011663287878036 + 10.0 * 6.2334675788879395
Epoch 1260, val loss: 0.9524214267730713
Epoch 1270, training loss: 62.43427658081055 = 0.0974787026643753 + 10.0 * 6.23367977142334
Epoch 1270, val loss: 0.9566506743431091
Epoch 1280, training loss: 62.4177131652832 = 0.09488329291343689 + 10.0 * 6.232283115386963
Epoch 1280, val loss: 0.9608434438705444
Epoch 1290, training loss: 62.42198181152344 = 0.09242787212133408 + 10.0 * 6.232955455780029
Epoch 1290, val loss: 0.9648707509040833
Epoch 1300, training loss: 62.40424346923828 = 0.09001675993204117 + 10.0 * 6.2314229011535645
Epoch 1300, val loss: 0.9691553115844727
Epoch 1310, training loss: 62.427703857421875 = 0.08769284933805466 + 10.0 * 6.234001159667969
Epoch 1310, val loss: 0.9732485413551331
Epoch 1320, training loss: 62.3876953125 = 0.08544110506772995 + 10.0 * 6.230225563049316
Epoch 1320, val loss: 0.9776763916015625
Epoch 1330, training loss: 62.38081741333008 = 0.08329374343156815 + 10.0 * 6.229752540588379
Epoch 1330, val loss: 0.9818124175071716
Epoch 1340, training loss: 62.38026809692383 = 0.08120869845151901 + 10.0 * 6.22990608215332
Epoch 1340, val loss: 0.986180305480957
Epoch 1350, training loss: 62.41328811645508 = 0.07918689399957657 + 10.0 * 6.233410358428955
Epoch 1350, val loss: 0.9905551671981812
Epoch 1360, training loss: 62.39118576049805 = 0.07719405740499496 + 10.0 * 6.231399059295654
Epoch 1360, val loss: 0.9947888255119324
Epoch 1370, training loss: 62.378929138183594 = 0.0752895176410675 + 10.0 * 6.230363845825195
Epoch 1370, val loss: 0.9990997314453125
Epoch 1380, training loss: 62.357723236083984 = 0.07344809174537659 + 10.0 * 6.228427410125732
Epoch 1380, val loss: 1.0034887790679932
Epoch 1390, training loss: 62.346923828125 = 0.0716700628399849 + 10.0 * 6.227525234222412
Epoch 1390, val loss: 1.0079245567321777
Epoch 1400, training loss: 62.38228988647461 = 0.06996725499629974 + 10.0 * 6.231232643127441
Epoch 1400, val loss: 1.0122371912002563
Epoch 1410, training loss: 62.35912322998047 = 0.06826483458280563 + 10.0 * 6.229085922241211
Epoch 1410, val loss: 1.0167579650878906
Epoch 1420, training loss: 62.38037872314453 = 0.06665177643299103 + 10.0 * 6.231372833251953
Epoch 1420, val loss: 1.021031141281128
Epoch 1430, training loss: 62.344722747802734 = 0.06507819145917892 + 10.0 * 6.227964401245117
Epoch 1430, val loss: 1.0252456665039062
Epoch 1440, training loss: 62.3199348449707 = 0.06355740875005722 + 10.0 * 6.225637912750244
Epoch 1440, val loss: 1.0297563076019287
Epoch 1450, training loss: 62.312255859375 = 0.06209567189216614 + 10.0 * 6.225016117095947
Epoch 1450, val loss: 1.0340032577514648
Epoch 1460, training loss: 62.341209411621094 = 0.06068578362464905 + 10.0 * 6.228052616119385
Epoch 1460, val loss: 1.0383199453353882
Epoch 1470, training loss: 62.32648849487305 = 0.059280380606651306 + 10.0 * 6.226720809936523
Epoch 1470, val loss: 1.0425511598587036
Epoch 1480, training loss: 62.32124328613281 = 0.05792703852057457 + 10.0 * 6.22633171081543
Epoch 1480, val loss: 1.0468389987945557
Epoch 1490, training loss: 62.29704666137695 = 0.056619979441165924 + 10.0 * 6.2240424156188965
Epoch 1490, val loss: 1.0511401891708374
Epoch 1500, training loss: 62.28763198852539 = 0.05536576732993126 + 10.0 * 6.223226547241211
Epoch 1500, val loss: 1.0554587841033936
Epoch 1510, training loss: 62.30784606933594 = 0.0541461780667305 + 10.0 * 6.225369930267334
Epoch 1510, val loss: 1.0599867105484009
Epoch 1520, training loss: 62.28256607055664 = 0.0529525987803936 + 10.0 * 6.22296142578125
Epoch 1520, val loss: 1.063931941986084
Epoch 1530, training loss: 62.3045768737793 = 0.05178515613079071 + 10.0 * 6.225279331207275
Epoch 1530, val loss: 1.0684977769851685
Epoch 1540, training loss: 62.281925201416016 = 0.05066412687301636 + 10.0 * 6.223126411437988
Epoch 1540, val loss: 1.07253897190094
Epoch 1550, training loss: 62.27174758911133 = 0.04957597702741623 + 10.0 * 6.222217082977295
Epoch 1550, val loss: 1.0768849849700928
Epoch 1560, training loss: 62.28672409057617 = 0.04852394759654999 + 10.0 * 6.223820209503174
Epoch 1560, val loss: 1.0812568664550781
Epoch 1570, training loss: 62.29671096801758 = 0.04750416427850723 + 10.0 * 6.224920749664307
Epoch 1570, val loss: 1.0855268239974976
Epoch 1580, training loss: 62.263038635253906 = 0.046503979712724686 + 10.0 * 6.221653461456299
Epoch 1580, val loss: 1.0895980596542358
Epoch 1590, training loss: 62.25487518310547 = 0.04553769528865814 + 10.0 * 6.22093391418457
Epoch 1590, val loss: 1.0938997268676758
Epoch 1600, training loss: 62.309993743896484 = 0.044604212045669556 + 10.0 * 6.22653865814209
Epoch 1600, val loss: 1.098114013671875
Epoch 1610, training loss: 62.283599853515625 = 0.04366638883948326 + 10.0 * 6.223993301391602
Epoch 1610, val loss: 1.1020523309707642
Epoch 1620, training loss: 62.253318786621094 = 0.04277878999710083 + 10.0 * 6.2210540771484375
Epoch 1620, val loss: 1.1062144041061401
Epoch 1630, training loss: 62.234928131103516 = 0.04191393405199051 + 10.0 * 6.219301700592041
Epoch 1630, val loss: 1.1104103326797485
Epoch 1640, training loss: 62.230228424072266 = 0.04108329489827156 + 10.0 * 6.21891450881958
Epoch 1640, val loss: 1.1146059036254883
Epoch 1650, training loss: 62.23180389404297 = 0.04027869924902916 + 10.0 * 6.219152450561523
Epoch 1650, val loss: 1.1187036037445068
Epoch 1660, training loss: 62.30662155151367 = 0.039499565958976746 + 10.0 * 6.226712226867676
Epoch 1660, val loss: 1.1225100755691528
Epoch 1670, training loss: 62.23714828491211 = 0.03869488090276718 + 10.0 * 6.219845294952393
Epoch 1670, val loss: 1.126753568649292
Epoch 1680, training loss: 62.22916030883789 = 0.03794050216674805 + 10.0 * 6.219121932983398
Epoch 1680, val loss: 1.1306452751159668
Epoch 1690, training loss: 62.25986099243164 = 0.03720986843109131 + 10.0 * 6.222265243530273
Epoch 1690, val loss: 1.1347486972808838
Epoch 1700, training loss: 62.2236328125 = 0.0364953875541687 + 10.0 * 6.218713760375977
Epoch 1700, val loss: 1.138534665107727
Epoch 1710, training loss: 62.20947265625 = 0.03580491989850998 + 10.0 * 6.217366695404053
Epoch 1710, val loss: 1.1425846815109253
Epoch 1720, training loss: 62.204036712646484 = 0.035134702920913696 + 10.0 * 6.216890335083008
Epoch 1720, val loss: 1.1466271877288818
Epoch 1730, training loss: 62.2376823425293 = 0.034487031400203705 + 10.0 * 6.2203192710876465
Epoch 1730, val loss: 1.1504915952682495
Epoch 1740, training loss: 62.231842041015625 = 0.03384476527571678 + 10.0 * 6.219799995422363
Epoch 1740, val loss: 1.153923511505127
Epoch 1750, training loss: 62.22591781616211 = 0.03321119770407677 + 10.0 * 6.219270706176758
Epoch 1750, val loss: 1.1581969261169434
Epoch 1760, training loss: 62.19440460205078 = 0.032595664262771606 + 10.0 * 6.216180801391602
Epoch 1760, val loss: 1.1618750095367432
Epoch 1770, training loss: 62.18963623046875 = 0.03200836479663849 + 10.0 * 6.215762615203857
Epoch 1770, val loss: 1.1657589673995972
Epoch 1780, training loss: 62.213653564453125 = 0.03144010901451111 + 10.0 * 6.218221187591553
Epoch 1780, val loss: 1.1696069240570068
Epoch 1790, training loss: 62.23015594482422 = 0.030873801559209824 + 10.0 * 6.21992826461792
Epoch 1790, val loss: 1.1733205318450928
Epoch 1800, training loss: 62.185516357421875 = 0.030322061851620674 + 10.0 * 6.215519428253174
Epoch 1800, val loss: 1.176857829093933
Epoch 1810, training loss: 62.176517486572266 = 0.029783161357045174 + 10.0 * 6.2146735191345215
Epoch 1810, val loss: 1.180623173713684
Epoch 1820, training loss: 62.17112731933594 = 0.02926972694694996 + 10.0 * 6.21418571472168
Epoch 1820, val loss: 1.184386134147644
Epoch 1830, training loss: 62.17243957519531 = 0.02876817248761654 + 10.0 * 6.214367389678955
Epoch 1830, val loss: 1.188157081604004
Epoch 1840, training loss: 62.23777389526367 = 0.02827661857008934 + 10.0 * 6.220949649810791
Epoch 1840, val loss: 1.1917325258255005
Epoch 1850, training loss: 62.18541717529297 = 0.02778446115553379 + 10.0 * 6.215763092041016
Epoch 1850, val loss: 1.195173978805542
Epoch 1860, training loss: 62.18437194824219 = 0.027316834777593613 + 10.0 * 6.215705394744873
Epoch 1860, val loss: 1.1986634731292725
Epoch 1870, training loss: 62.18088150024414 = 0.02685190550982952 + 10.0 * 6.215403079986572
Epoch 1870, val loss: 1.2024427652359009
Epoch 1880, training loss: 62.163002014160156 = 0.026397543027997017 + 10.0 * 6.21366024017334
Epoch 1880, val loss: 1.205952763557434
Epoch 1890, training loss: 62.15271759033203 = 0.025961799547076225 + 10.0 * 6.21267557144165
Epoch 1890, val loss: 1.2095898389816284
Epoch 1900, training loss: 62.15837478637695 = 0.025537552312016487 + 10.0 * 6.213283538818359
Epoch 1900, val loss: 1.2130051851272583
Epoch 1910, training loss: 62.21786880493164 = 0.02512829191982746 + 10.0 * 6.219274044036865
Epoch 1910, val loss: 1.2162936925888062
Epoch 1920, training loss: 62.17354965209961 = 0.02471191994845867 + 10.0 * 6.214883804321289
Epoch 1920, val loss: 1.2199232578277588
Epoch 1930, training loss: 62.14979553222656 = 0.024308692663908005 + 10.0 * 6.212548732757568
Epoch 1930, val loss: 1.2232413291931152
Epoch 1940, training loss: 62.14171600341797 = 0.023924576118588448 + 10.0 * 6.2117791175842285
Epoch 1940, val loss: 1.2267628908157349
Epoch 1950, training loss: 62.14580154418945 = 0.023548856377601624 + 10.0 * 6.212225437164307
Epoch 1950, val loss: 1.2301944494247437
Epoch 1960, training loss: 62.1868782043457 = 0.023180240765213966 + 10.0 * 6.21636962890625
Epoch 1960, val loss: 1.2335041761398315
Epoch 1970, training loss: 62.168399810791016 = 0.022817768156528473 + 10.0 * 6.214558124542236
Epoch 1970, val loss: 1.2366869449615479
Epoch 1980, training loss: 62.16637420654297 = 0.022454220801591873 + 10.0 * 6.214392185211182
Epoch 1980, val loss: 1.240127682685852
Epoch 1990, training loss: 62.154850006103516 = 0.022109368816018105 + 10.0 * 6.213274002075195
Epoch 1990, val loss: 1.2432554960250854
Epoch 2000, training loss: 62.13298797607422 = 0.021766461431980133 + 10.0 * 6.211122035980225
Epoch 2000, val loss: 1.2464065551757812
Epoch 2010, training loss: 62.13841247558594 = 0.021438932046294212 + 10.0 * 6.211697578430176
Epoch 2010, val loss: 1.2497515678405762
Epoch 2020, training loss: 62.12612533569336 = 0.021115196868777275 + 10.0 * 6.210501194000244
Epoch 2020, val loss: 1.252793312072754
Epoch 2030, training loss: 62.12879943847656 = 0.020800989121198654 + 10.0 * 6.210799694061279
Epoch 2030, val loss: 1.2559576034545898
Epoch 2040, training loss: 62.12990951538086 = 0.020493339747190475 + 10.0 * 6.210941791534424
Epoch 2040, val loss: 1.2591345310211182
Epoch 2050, training loss: 62.14503479003906 = 0.020190220326185226 + 10.0 * 6.212484359741211
Epoch 2050, val loss: 1.2623834609985352
Epoch 2060, training loss: 62.11316680908203 = 0.01989080011844635 + 10.0 * 6.209327697753906
Epoch 2060, val loss: 1.265182614326477
Epoch 2070, training loss: 62.11548614501953 = 0.019598638638854027 + 10.0 * 6.209588527679443
Epoch 2070, val loss: 1.2682900428771973
Epoch 2080, training loss: 62.1190071105957 = 0.019315702840685844 + 10.0 * 6.209969520568848
Epoch 2080, val loss: 1.2714154720306396
Epoch 2090, training loss: 62.11262512207031 = 0.019039133563637733 + 10.0 * 6.2093586921691895
Epoch 2090, val loss: 1.2742656469345093
Epoch 2100, training loss: 62.13175964355469 = 0.018769947811961174 + 10.0 * 6.211298942565918
Epoch 2100, val loss: 1.2771964073181152
Epoch 2110, training loss: 62.139183044433594 = 0.018500180914998055 + 10.0 * 6.2120680809021
Epoch 2110, val loss: 1.2805501222610474
Epoch 2120, training loss: 62.10590362548828 = 0.018236789852380753 + 10.0 * 6.208766460418701
Epoch 2120, val loss: 1.2830876111984253
Epoch 2130, training loss: 62.09527587890625 = 0.017982298508286476 + 10.0 * 6.207729339599609
Epoch 2130, val loss: 1.2861576080322266
Epoch 2140, training loss: 62.09473419189453 = 0.017737001180648804 + 10.0 * 6.207699775695801
Epoch 2140, val loss: 1.2890838384628296
Epoch 2150, training loss: 62.111751556396484 = 0.0174968671053648 + 10.0 * 6.209425449371338
Epoch 2150, val loss: 1.2919431924819946
Epoch 2160, training loss: 62.15141296386719 = 0.01725713536143303 + 10.0 * 6.213415622711182
Epoch 2160, val loss: 1.294819951057434
Epoch 2170, training loss: 62.09941482543945 = 0.01701461337506771 + 10.0 * 6.208240032196045
Epoch 2170, val loss: 1.2971899509429932
Epoch 2180, training loss: 62.09818649291992 = 0.01678197644650936 + 10.0 * 6.2081403732299805
Epoch 2180, val loss: 1.3003088235855103
Epoch 2190, training loss: 62.12179946899414 = 0.01656320132315159 + 10.0 * 6.21052360534668
Epoch 2190, val loss: 1.302890419960022
Epoch 2200, training loss: 62.102176666259766 = 0.01634056493639946 + 10.0 * 6.208583354949951
Epoch 2200, val loss: 1.3058428764343262
Epoch 2210, training loss: 62.07493591308594 = 0.016120193526148796 + 10.0 * 6.205881595611572
Epoch 2210, val loss: 1.3084383010864258
Epoch 2220, training loss: 62.07683181762695 = 0.015909751877188683 + 10.0 * 6.20609188079834
Epoch 2220, val loss: 1.3113456964492798
Epoch 2230, training loss: 62.08588790893555 = 0.015706241130828857 + 10.0 * 6.2070183753967285
Epoch 2230, val loss: 1.3140989542007446
Epoch 2240, training loss: 62.10806655883789 = 0.015506446361541748 + 10.0 * 6.209256172180176
Epoch 2240, val loss: 1.3164697885513306
Epoch 2250, training loss: 62.082069396972656 = 0.015300791710615158 + 10.0 * 6.206676959991455
Epoch 2250, val loss: 1.319307804107666
Epoch 2260, training loss: 62.08708190917969 = 0.015109281055629253 + 10.0 * 6.207197189331055
Epoch 2260, val loss: 1.3219590187072754
Epoch 2270, training loss: 62.088226318359375 = 0.014914694242179394 + 10.0 * 6.20733118057251
Epoch 2270, val loss: 1.3245089054107666
Epoch 2280, training loss: 62.07272720336914 = 0.014720096252858639 + 10.0 * 6.205800533294678
Epoch 2280, val loss: 1.3270950317382812
Epoch 2290, training loss: 62.07115936279297 = 0.014536987990140915 + 10.0 * 6.205662250518799
Epoch 2290, val loss: 1.329926609992981
Epoch 2300, training loss: 62.10956573486328 = 0.014356941916048527 + 10.0 * 6.2095208168029785
Epoch 2300, val loss: 1.3323860168457031
Epoch 2310, training loss: 62.08032989501953 = 0.014178385026752949 + 10.0 * 6.206614971160889
Epoch 2310, val loss: 1.334709882736206
Epoch 2320, training loss: 62.06217956542969 = 0.014000498689711094 + 10.0 * 6.204817771911621
Epoch 2320, val loss: 1.33736252784729
Epoch 2330, training loss: 62.05418395996094 = 0.013831965625286102 + 10.0 * 6.20403528213501
Epoch 2330, val loss: 1.3398833274841309
Epoch 2340, training loss: 62.052894592285156 = 0.013665897771716118 + 10.0 * 6.203923225402832
Epoch 2340, val loss: 1.3424347639083862
Epoch 2350, training loss: 62.12383270263672 = 0.013508295640349388 + 10.0 * 6.211032390594482
Epoch 2350, val loss: 1.3445866107940674
Epoch 2360, training loss: 62.104244232177734 = 0.013338391669094563 + 10.0 * 6.209090232849121
Epoch 2360, val loss: 1.3472410440444946
Epoch 2370, training loss: 62.065616607666016 = 0.013174940831959248 + 10.0 * 6.205244064331055
Epoch 2370, val loss: 1.3494184017181396
Epoch 2380, training loss: 62.04690170288086 = 0.013017022982239723 + 10.0 * 6.203388690948486
Epoch 2380, val loss: 1.3518832921981812
Epoch 2390, training loss: 62.04236602783203 = 0.012865048833191395 + 10.0 * 6.202950477600098
Epoch 2390, val loss: 1.3543143272399902
Epoch 2400, training loss: 62.07058334350586 = 0.012720457278192043 + 10.0 * 6.205786228179932
Epoch 2400, val loss: 1.356547236442566
Epoch 2410, training loss: 62.062652587890625 = 0.01257155742496252 + 10.0 * 6.205008029937744
Epoch 2410, val loss: 1.359024167060852
Epoch 2420, training loss: 62.037200927734375 = 0.012419511564075947 + 10.0 * 6.202477931976318
Epoch 2420, val loss: 1.361134648323059
Epoch 2430, training loss: 62.035037994384766 = 0.012277994304895401 + 10.0 * 6.20227575302124
Epoch 2430, val loss: 1.363547444343567
Epoch 2440, training loss: 62.03266906738281 = 0.0121398214250803 + 10.0 * 6.202053070068359
Epoch 2440, val loss: 1.3659645318984985
Epoch 2450, training loss: 62.04547882080078 = 0.012007114477455616 + 10.0 * 6.203347206115723
Epoch 2450, val loss: 1.3681472539901733
Epoch 2460, training loss: 62.077484130859375 = 0.011871215887367725 + 10.0 * 6.206561088562012
Epoch 2460, val loss: 1.3703644275665283
Epoch 2470, training loss: 62.04209899902344 = 0.01173313520848751 + 10.0 * 6.203036308288574
Epoch 2470, val loss: 1.3724579811096191
Epoch 2480, training loss: 62.03031539916992 = 0.011600234545767307 + 10.0 * 6.201871395111084
Epoch 2480, val loss: 1.3747416734695435
Epoch 2490, training loss: 62.03490447998047 = 0.011474753729999065 + 10.0 * 6.202342987060547
Epoch 2490, val loss: 1.37684166431427
Epoch 2500, training loss: 62.06608581542969 = 0.01135152205824852 + 10.0 * 6.20547342300415
Epoch 2500, val loss: 1.378836750984192
Epoch 2510, training loss: 62.03677749633789 = 0.011226250790059566 + 10.0 * 6.202555179595947
Epoch 2510, val loss: 1.3813000917434692
Epoch 2520, training loss: 62.052066802978516 = 0.011104658246040344 + 10.0 * 6.20409631729126
Epoch 2520, val loss: 1.3831171989440918
Epoch 2530, training loss: 62.031280517578125 = 0.010980350896716118 + 10.0 * 6.202030181884766
Epoch 2530, val loss: 1.3854421377182007
Epoch 2540, training loss: 62.02157211303711 = 0.010863550938665867 + 10.0 * 6.201070785522461
Epoch 2540, val loss: 1.3872102499008179
Epoch 2550, training loss: 62.01823806762695 = 0.010749264620244503 + 10.0 * 6.200748920440674
Epoch 2550, val loss: 1.3893922567367554
Epoch 2560, training loss: 62.04138946533203 = 0.010639439336955547 + 10.0 * 6.203074932098389
Epoch 2560, val loss: 1.391528844833374
Epoch 2570, training loss: 62.02897644042969 = 0.010526434518396854 + 10.0 * 6.201845169067383
Epoch 2570, val loss: 1.3932442665100098
Epoch 2580, training loss: 62.02327346801758 = 0.010412883944809437 + 10.0 * 6.2012858390808105
Epoch 2580, val loss: 1.3953913450241089
Epoch 2590, training loss: 62.03823471069336 = 0.010304993949830532 + 10.0 * 6.202793121337891
Epoch 2590, val loss: 1.3975560665130615
Epoch 2600, training loss: 62.023582458496094 = 0.010198865085840225 + 10.0 * 6.201338291168213
Epoch 2600, val loss: 1.3992308378219604
Epoch 2610, training loss: 62.02419662475586 = 0.010095136240124702 + 10.0 * 6.201410293579102
Epoch 2610, val loss: 1.4011305570602417
Epoch 2620, training loss: 62.0139045715332 = 0.009991166181862354 + 10.0 * 6.2003912925720215
Epoch 2620, val loss: 1.4031094312667847
Epoch 2630, training loss: 62.030364990234375 = 0.009893047623336315 + 10.0 * 6.202047348022461
Epoch 2630, val loss: 1.4048978090286255
Epoch 2640, training loss: 62.01726150512695 = 0.00979106966406107 + 10.0 * 6.200747013092041
Epoch 2640, val loss: 1.4065414667129517
Epoch 2650, training loss: 62.076412200927734 = 0.009693371132016182 + 10.0 * 6.206671714782715
Epoch 2650, val loss: 1.408346176147461
Epoch 2660, training loss: 62.02472686767578 = 0.009588954038918018 + 10.0 * 6.201513767242432
Epoch 2660, val loss: 1.4100992679595947
Epoch 2670, training loss: 62.00553512573242 = 0.009489853866398335 + 10.0 * 6.199604511260986
Epoch 2670, val loss: 1.4120302200317383
Epoch 2680, training loss: 62.00189971923828 = 0.009398511610925198 + 10.0 * 6.199250221252441
Epoch 2680, val loss: 1.4136220216751099
Epoch 2690, training loss: 61.99500274658203 = 0.009308707900345325 + 10.0 * 6.198569297790527
Epoch 2690, val loss: 1.4157395362854004
Epoch 2700, training loss: 61.9927864074707 = 0.009221169166266918 + 10.0 * 6.198356628417969
Epoch 2700, val loss: 1.4175033569335938
Epoch 2710, training loss: 62.01948928833008 = 0.009135838598012924 + 10.0 * 6.201035499572754
Epoch 2710, val loss: 1.4192321300506592
Epoch 2720, training loss: 61.9981575012207 = 0.009045207872986794 + 10.0 * 6.198911190032959
Epoch 2720, val loss: 1.4209238290786743
Epoch 2730, training loss: 62.00468826293945 = 0.008954968303442001 + 10.0 * 6.199573516845703
Epoch 2730, val loss: 1.422606348991394
Epoch 2740, training loss: 62.021549224853516 = 0.008870300836861134 + 10.0 * 6.201268196105957
Epoch 2740, val loss: 1.4242404699325562
Epoch 2750, training loss: 61.997379302978516 = 0.008784881792962551 + 10.0 * 6.198859214782715
Epoch 2750, val loss: 1.425944447517395
Epoch 2760, training loss: 62.0097770690918 = 0.008703259751200676 + 10.0 * 6.200107097625732
Epoch 2760, val loss: 1.427497386932373
Epoch 2770, training loss: 61.997100830078125 = 0.008620944805443287 + 10.0 * 6.198847770690918
Epoch 2770, val loss: 1.4292364120483398
Epoch 2780, training loss: 61.99422836303711 = 0.00854004081338644 + 10.0 * 6.198568820953369
Epoch 2780, val loss: 1.430898666381836
Epoch 2790, training loss: 61.9931640625 = 0.008461271412670612 + 10.0 * 6.198470115661621
Epoch 2790, val loss: 1.4325681924819946
Epoch 2800, training loss: 62.01433563232422 = 0.008385197259485722 + 10.0 * 6.200594902038574
Epoch 2800, val loss: 1.4340630769729614
Epoch 2810, training loss: 61.98640060424805 = 0.008306761272251606 + 10.0 * 6.197809219360352
Epoch 2810, val loss: 1.4357342720031738
Epoch 2820, training loss: 61.98548889160156 = 0.008231197483837605 + 10.0 * 6.197725772857666
Epoch 2820, val loss: 1.4373040199279785
Epoch 2830, training loss: 62.01308822631836 = 0.008158978074789047 + 10.0 * 6.200492858886719
Epoch 2830, val loss: 1.4389129877090454
Epoch 2840, training loss: 62.00296401977539 = 0.00808486994355917 + 10.0 * 6.199488162994385
Epoch 2840, val loss: 1.4401603937149048
Epoch 2850, training loss: 61.990291595458984 = 0.008007366210222244 + 10.0 * 6.198228359222412
Epoch 2850, val loss: 1.4418164491653442
Epoch 2860, training loss: 61.976036071777344 = 0.007937072776257992 + 10.0 * 6.196809768676758
Epoch 2860, val loss: 1.4433778524398804
Epoch 2870, training loss: 61.97541427612305 = 0.007867102511227131 + 10.0 * 6.1967549324035645
Epoch 2870, val loss: 1.4448884725570679
Epoch 2880, training loss: 62.00757598876953 = 0.007802051026374102 + 10.0 * 6.199977397918701
Epoch 2880, val loss: 1.446303367614746
Epoch 2890, training loss: 61.98350143432617 = 0.007730862125754356 + 10.0 * 6.197576999664307
Epoch 2890, val loss: 1.447853446006775
Epoch 2900, training loss: 61.97814178466797 = 0.007662766147404909 + 10.0 * 6.197047710418701
Epoch 2900, val loss: 1.4493898153305054
Epoch 2910, training loss: 61.99539566040039 = 0.007596402894705534 + 10.0 * 6.198780059814453
Epoch 2910, val loss: 1.4509197473526
Epoch 2920, training loss: 61.98689270019531 = 0.007530027534812689 + 10.0 * 6.197936058044434
Epoch 2920, val loss: 1.4522221088409424
Epoch 2930, training loss: 61.98130798339844 = 0.007463897578418255 + 10.0 * 6.197384357452393
Epoch 2930, val loss: 1.4534443616867065
Epoch 2940, training loss: 61.97492218017578 = 0.0074015730060637 + 10.0 * 6.196752071380615
Epoch 2940, val loss: 1.4550641775131226
Epoch 2950, training loss: 61.96482849121094 = 0.007339136675000191 + 10.0 * 6.195748805999756
Epoch 2950, val loss: 1.4565284252166748
Epoch 2960, training loss: 61.98275375366211 = 0.007278044708073139 + 10.0 * 6.197547435760498
Epoch 2960, val loss: 1.4578849077224731
Epoch 2970, training loss: 61.9835205078125 = 0.007216996513307095 + 10.0 * 6.197630405426025
Epoch 2970, val loss: 1.4590915441513062
Epoch 2980, training loss: 61.988059997558594 = 0.007157241925597191 + 10.0 * 6.198090553283691
Epoch 2980, val loss: 1.4602762460708618
Epoch 2990, training loss: 61.97172164916992 = 0.007096487563103437 + 10.0 * 6.196462631225586
Epoch 2990, val loss: 1.4617946147918701
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8397469688982605
The final CL Acc:0.75926, 0.01512, The final GNN Acc:0.84027, 0.00197
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10562])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.92051696777344 = 1.952285885810852 + 10.0 * 8.596822738647461
Epoch 0, val loss: 1.9498366117477417
Epoch 10, training loss: 87.90361785888672 = 1.9426512718200684 + 10.0 * 8.596096992492676
Epoch 10, val loss: 1.9409512281417847
Epoch 20, training loss: 87.84104919433594 = 1.9307239055633545 + 10.0 * 8.591032028198242
Epoch 20, val loss: 1.9295114278793335
Epoch 30, training loss: 87.49004364013672 = 1.914953589439392 + 10.0 * 8.55750846862793
Epoch 30, val loss: 1.9141268730163574
Epoch 40, training loss: 85.64342498779297 = 1.89517343044281 + 10.0 * 8.374825477600098
Epoch 40, val loss: 1.8953042030334473
Epoch 50, training loss: 81.52820587158203 = 1.87123703956604 + 10.0 * 7.965696334838867
Epoch 50, val loss: 1.872471570968628
Epoch 60, training loss: 79.48795318603516 = 1.8488242626190186 + 10.0 * 7.763913154602051
Epoch 60, val loss: 1.8517812490463257
Epoch 70, training loss: 76.64369201660156 = 1.8331997394561768 + 10.0 * 7.481049060821533
Epoch 70, val loss: 1.8379698991775513
Epoch 80, training loss: 74.14442443847656 = 1.8255454301834106 + 10.0 * 7.2318878173828125
Epoch 80, val loss: 1.8306750059127808
Epoch 90, training loss: 72.44397735595703 = 1.8160109519958496 + 10.0 * 7.0627970695495605
Epoch 90, val loss: 1.821324110031128
Epoch 100, training loss: 71.16426086425781 = 1.8057408332824707 + 10.0 * 6.93585205078125
Epoch 100, val loss: 1.8115113973617554
Epoch 110, training loss: 70.26901245117188 = 1.796919345855713 + 10.0 * 6.8472089767456055
Epoch 110, val loss: 1.802611231803894
Epoch 120, training loss: 69.62353515625 = 1.7879260778427124 + 10.0 * 6.7835612297058105
Epoch 120, val loss: 1.7935274839401245
Epoch 130, training loss: 69.06535339355469 = 1.7784277200698853 + 10.0 * 6.728693008422852
Epoch 130, val loss: 1.7843304872512817
Epoch 140, training loss: 68.6075668334961 = 1.7693309783935547 + 10.0 * 6.683823585510254
Epoch 140, val loss: 1.7756989002227783
Epoch 150, training loss: 68.23352813720703 = 1.7598216533660889 + 10.0 * 6.647371292114258
Epoch 150, val loss: 1.766822099685669
Epoch 160, training loss: 67.9030990600586 = 1.7493716478347778 + 10.0 * 6.615372657775879
Epoch 160, val loss: 1.757319450378418
Epoch 170, training loss: 67.63487243652344 = 1.7380483150482178 + 10.0 * 6.589682579040527
Epoch 170, val loss: 1.7472363710403442
Epoch 180, training loss: 67.3750991821289 = 1.725614309310913 + 10.0 * 6.564948558807373
Epoch 180, val loss: 1.736336588859558
Epoch 190, training loss: 67.14801788330078 = 1.7121148109436035 + 10.0 * 6.543590545654297
Epoch 190, val loss: 1.724487066268921
Epoch 200, training loss: 66.96040344238281 = 1.6973659992218018 + 10.0 * 6.526303768157959
Epoch 200, val loss: 1.7117111682891846
Epoch 210, training loss: 66.79803466796875 = 1.6813633441925049 + 10.0 * 6.511666774749756
Epoch 210, val loss: 1.6979221105575562
Epoch 220, training loss: 66.62686157226562 = 1.6641544103622437 + 10.0 * 6.496271133422852
Epoch 220, val loss: 1.6833722591400146
Epoch 230, training loss: 66.50324249267578 = 1.6456724405288696 + 10.0 * 6.485756874084473
Epoch 230, val loss: 1.6677234172821045
Epoch 240, training loss: 66.34374237060547 = 1.625895380973816 + 10.0 * 6.471785068511963
Epoch 240, val loss: 1.6510804891586304
Epoch 250, training loss: 66.21604919433594 = 1.6050381660461426 + 10.0 * 6.4611005783081055
Epoch 250, val loss: 1.6337913274765015
Epoch 260, training loss: 66.09675598144531 = 1.5830942392349243 + 10.0 * 6.451366424560547
Epoch 260, val loss: 1.6157821416854858
Epoch 270, training loss: 65.99480438232422 = 1.5600436925888062 + 10.0 * 6.44347620010376
Epoch 270, val loss: 1.5970903635025024
Epoch 280, training loss: 65.88327026367188 = 1.5363296270370483 + 10.0 * 6.434694290161133
Epoch 280, val loss: 1.5781170129776
Epoch 290, training loss: 65.78824615478516 = 1.5119675397872925 + 10.0 * 6.4276275634765625
Epoch 290, val loss: 1.5589885711669922
Epoch 300, training loss: 65.71269226074219 = 1.4870744943618774 + 10.0 * 6.4225616455078125
Epoch 300, val loss: 1.5399484634399414
Epoch 310, training loss: 65.6070785522461 = 1.4621070623397827 + 10.0 * 6.414497375488281
Epoch 310, val loss: 1.5209907293319702
Epoch 320, training loss: 65.52425384521484 = 1.4370449781417847 + 10.0 * 6.408720970153809
Epoch 320, val loss: 1.5024219751358032
Epoch 330, training loss: 65.44615173339844 = 1.4119869470596313 + 10.0 * 6.403416633605957
Epoch 330, val loss: 1.484200358390808
Epoch 340, training loss: 65.37049865722656 = 1.3869425058364868 + 10.0 * 6.398355960845947
Epoch 340, val loss: 1.4664572477340698
Epoch 350, training loss: 65.30868530273438 = 1.3618301153182983 + 10.0 * 6.3946852684021
Epoch 350, val loss: 1.4489071369171143
Epoch 360, training loss: 65.21775817871094 = 1.3368797302246094 + 10.0 * 6.388088226318359
Epoch 360, val loss: 1.431716799736023
Epoch 370, training loss: 65.14176940917969 = 1.3119316101074219 + 10.0 * 6.382984161376953
Epoch 370, val loss: 1.4148505926132202
Epoch 380, training loss: 65.0865249633789 = 1.2868378162384033 + 10.0 * 6.379968643188477
Epoch 380, val loss: 1.3980679512023926
Epoch 390, training loss: 65.01628112792969 = 1.2617805004119873 + 10.0 * 6.375450134277344
Epoch 390, val loss: 1.3814213275909424
Epoch 400, training loss: 64.95014953613281 = 1.236750841140747 + 10.0 * 6.371340274810791
Epoch 400, val loss: 1.3649489879608154
Epoch 410, training loss: 64.88253784179688 = 1.2116197347640991 + 10.0 * 6.367091655731201
Epoch 410, val loss: 1.3485294580459595
Epoch 420, training loss: 64.940185546875 = 1.1863662004470825 + 10.0 * 6.375381946563721
Epoch 420, val loss: 1.332094430923462
Epoch 430, training loss: 64.79499053955078 = 1.1608792543411255 + 10.0 * 6.363410949707031
Epoch 430, val loss: 1.3154529333114624
Epoch 440, training loss: 64.70724487304688 = 1.1354646682739258 + 10.0 * 6.357177734375
Epoch 440, val loss: 1.2989798784255981
Epoch 450, training loss: 64.65142822265625 = 1.1100502014160156 + 10.0 * 6.354137897491455
Epoch 450, val loss: 1.2825559377670288
Epoch 460, training loss: 64.59637451171875 = 1.0846716165542603 + 10.0 * 6.351170063018799
Epoch 460, val loss: 1.2661433219909668
Epoch 470, training loss: 64.53910827636719 = 1.059418797492981 + 10.0 * 6.347969055175781
Epoch 470, val loss: 1.249847650527954
Epoch 480, training loss: 64.48747253417969 = 1.0343725681304932 + 10.0 * 6.345309734344482
Epoch 480, val loss: 1.2337535619735718
Epoch 490, training loss: 64.46221160888672 = 1.0095330476760864 + 10.0 * 6.345267295837402
Epoch 490, val loss: 1.2179266214370728
Epoch 500, training loss: 64.41731262207031 = 0.9849604368209839 + 10.0 * 6.343235015869141
Epoch 500, val loss: 1.2022044658660889
Epoch 510, training loss: 64.3526840209961 = 0.9608016610145569 + 10.0 * 6.339188575744629
Epoch 510, val loss: 1.1868977546691895
Epoch 520, training loss: 64.2994613647461 = 0.9370155930519104 + 10.0 * 6.336244583129883
Epoch 520, val loss: 1.172149658203125
Epoch 530, training loss: 64.24479675292969 = 0.9137150645256042 + 10.0 * 6.333108425140381
Epoch 530, val loss: 1.1577777862548828
Epoch 540, training loss: 64.2117691040039 = 0.89089035987854 + 10.0 * 6.332087516784668
Epoch 540, val loss: 1.143865704536438
Epoch 550, training loss: 64.15995788574219 = 0.8683345913887024 + 10.0 * 6.329162120819092
Epoch 550, val loss: 1.1306020021438599
Epoch 560, training loss: 64.1216049194336 = 0.846477210521698 + 10.0 * 6.327512741088867
Epoch 560, val loss: 1.117857813835144
Epoch 570, training loss: 64.07781219482422 = 0.825052797794342 + 10.0 * 6.325275897979736
Epoch 570, val loss: 1.1058831214904785
Epoch 580, training loss: 64.03697967529297 = 0.8040622472763062 + 10.0 * 6.323291301727295
Epoch 580, val loss: 1.0945138931274414
Epoch 590, training loss: 63.995452880859375 = 0.7836530208587646 + 10.0 * 6.3211798667907715
Epoch 590, val loss: 1.0837113857269287
Epoch 600, training loss: 63.9651985168457 = 0.7637460827827454 + 10.0 * 6.320145606994629
Epoch 600, val loss: 1.0736042261123657
Epoch 610, training loss: 63.91684341430664 = 0.7442073225975037 + 10.0 * 6.317263603210449
Epoch 610, val loss: 1.0644304752349854
Epoch 620, training loss: 63.88334274291992 = 0.7251468896865845 + 10.0 * 6.31581974029541
Epoch 620, val loss: 1.0557109117507935
Epoch 630, training loss: 63.845314025878906 = 0.7065403461456299 + 10.0 * 6.313877582550049
Epoch 630, val loss: 1.047791600227356
Epoch 640, training loss: 63.8367805480957 = 0.688324511051178 + 10.0 * 6.314845561981201
Epoch 640, val loss: 1.0403752326965332
Epoch 650, training loss: 63.77798080444336 = 0.6704810857772827 + 10.0 * 6.3107500076293945
Epoch 650, val loss: 1.0336371660232544
Epoch 660, training loss: 63.80881118774414 = 0.6531858444213867 + 10.0 * 6.3155622482299805
Epoch 660, val loss: 1.0276119709014893
Epoch 670, training loss: 63.718711853027344 = 0.6361005306243896 + 10.0 * 6.308260917663574
Epoch 670, val loss: 1.0221929550170898
Epoch 680, training loss: 63.6773681640625 = 0.6195697784423828 + 10.0 * 6.305779933929443
Epoch 680, val loss: 1.017511248588562
Epoch 690, training loss: 63.654598236083984 = 0.603391170501709 + 10.0 * 6.305120944976807
Epoch 690, val loss: 1.0133981704711914
Epoch 700, training loss: 63.6330451965332 = 0.5875118970870972 + 10.0 * 6.304553031921387
Epoch 700, val loss: 1.0096181631088257
Epoch 710, training loss: 63.60200500488281 = 0.572023868560791 + 10.0 * 6.302998065948486
Epoch 710, val loss: 1.0066195726394653
Epoch 720, training loss: 63.5837287902832 = 0.5568621754646301 + 10.0 * 6.30268669128418
Epoch 720, val loss: 1.0041539669036865
Epoch 730, training loss: 63.53665542602539 = 0.5420588254928589 + 10.0 * 6.299459934234619
Epoch 730, val loss: 1.002340316772461
Epoch 740, training loss: 63.50833511352539 = 0.5276404023170471 + 10.0 * 6.298069477081299
Epoch 740, val loss: 1.0008291006088257
Epoch 750, training loss: 63.53559494018555 = 0.5135762095451355 + 10.0 * 6.302201747894287
Epoch 750, val loss: 0.9996505379676819
Epoch 760, training loss: 63.460689544677734 = 0.499855637550354 + 10.0 * 6.296083450317383
Epoch 760, val loss: 0.999056875705719
Epoch 770, training loss: 63.430503845214844 = 0.48654913902282715 + 10.0 * 6.294395446777344
Epoch 770, val loss: 0.9990264773368835
Epoch 780, training loss: 63.39981460571289 = 0.4735904037952423 + 10.0 * 6.2926225662231445
Epoch 780, val loss: 0.9994475245475769
Epoch 790, training loss: 63.46470260620117 = 0.4609142243862152 + 10.0 * 6.300378799438477
Epoch 790, val loss: 1.0002473592758179
Epoch 800, training loss: 63.38016891479492 = 0.4485693871974945 + 10.0 * 6.2931599617004395
Epoch 800, val loss: 1.0011907815933228
Epoch 810, training loss: 63.33469009399414 = 0.436558336019516 + 10.0 * 6.289813041687012
Epoch 810, val loss: 1.0024746656417847
Epoch 820, training loss: 63.30781936645508 = 0.4248849153518677 + 10.0 * 6.288293361663818
Epoch 820, val loss: 1.0042766332626343
Epoch 830, training loss: 63.363685607910156 = 0.4135163426399231 + 10.0 * 6.295016765594482
Epoch 830, val loss: 1.0064395666122437
Epoch 840, training loss: 63.27936935424805 = 0.40242040157318115 + 10.0 * 6.287694931030273
Epoch 840, val loss: 1.008841633796692
Epoch 850, training loss: 63.258338928222656 = 0.391673743724823 + 10.0 * 6.286666393280029
Epoch 850, val loss: 1.0115903615951538
Epoch 860, training loss: 63.241729736328125 = 0.38121843338012695 + 10.0 * 6.286051273345947
Epoch 860, val loss: 1.014514446258545
Epoch 870, training loss: 63.208274841308594 = 0.3710525929927826 + 10.0 * 6.283722400665283
Epoch 870, val loss: 1.0177420377731323
Epoch 880, training loss: 63.21193313598633 = 0.36117973923683167 + 10.0 * 6.2850751876831055
Epoch 880, val loss: 1.0210480690002441
Epoch 890, training loss: 63.20170211791992 = 0.3514806032180786 + 10.0 * 6.285021781921387
Epoch 890, val loss: 1.0248370170593262
Epoch 900, training loss: 63.1572151184082 = 0.3420235514640808 + 10.0 * 6.281519412994385
Epoch 900, val loss: 1.0283046960830688
Epoch 910, training loss: 63.12821960449219 = 0.33285072445869446 + 10.0 * 6.279536724090576
Epoch 910, val loss: 1.0324381589889526
Epoch 920, training loss: 63.1207389831543 = 0.32390841841697693 + 10.0 * 6.2796831130981445
Epoch 920, val loss: 1.0367358922958374
Epoch 930, training loss: 63.1196174621582 = 0.31514954566955566 + 10.0 * 6.280446529388428
Epoch 930, val loss: 1.04109525680542
Epoch 940, training loss: 63.11166763305664 = 0.3065683841705322 + 10.0 * 6.280509948730469
Epoch 940, val loss: 1.0455451011657715
Epoch 950, training loss: 63.082942962646484 = 0.2982005476951599 + 10.0 * 6.2784743309021
Epoch 950, val loss: 1.0502246618270874
Epoch 960, training loss: 63.05036544799805 = 0.29002153873443604 + 10.0 * 6.276034355163574
Epoch 960, val loss: 1.054905891418457
Epoch 970, training loss: 63.0921745300293 = 0.2819948196411133 + 10.0 * 6.281018257141113
Epoch 970, val loss: 1.059861421585083
Epoch 980, training loss: 63.03962326049805 = 0.274135947227478 + 10.0 * 6.276548862457275
Epoch 980, val loss: 1.064421534538269
Epoch 990, training loss: 62.990779876708984 = 0.2664085328578949 + 10.0 * 6.27243709564209
Epoch 990, val loss: 1.0695886611938477
Epoch 1000, training loss: 62.97536849975586 = 0.25886696577072144 + 10.0 * 6.271650314331055
Epoch 1000, val loss: 1.0747278928756714
Epoch 1010, training loss: 63.004364013671875 = 0.2514462471008301 + 10.0 * 6.275291919708252
Epoch 1010, val loss: 1.0799531936645508
Epoch 1020, training loss: 62.98625183105469 = 0.2440916895866394 + 10.0 * 6.274216175079346
Epoch 1020, val loss: 1.0851037502288818
Epoch 1030, training loss: 62.950504302978516 = 0.23688066005706787 + 10.0 * 6.2713623046875
Epoch 1030, val loss: 1.0904443264007568
Epoch 1040, training loss: 62.91606903076172 = 0.2298300564289093 + 10.0 * 6.2686238288879395
Epoch 1040, val loss: 1.0958802700042725
Epoch 1050, training loss: 62.90439224243164 = 0.22293587028980255 + 10.0 * 6.268145561218262
Epoch 1050, val loss: 1.1014071702957153
Epoch 1060, training loss: 62.88587188720703 = 0.2161705642938614 + 10.0 * 6.266970157623291
Epoch 1060, val loss: 1.1070719957351685
Epoch 1070, training loss: 62.893280029296875 = 0.20953339338302612 + 10.0 * 6.268374443054199
Epoch 1070, val loss: 1.1127960681915283
Epoch 1080, training loss: 62.86918258666992 = 0.20296776294708252 + 10.0 * 6.2666215896606445
Epoch 1080, val loss: 1.1184821128845215
Epoch 1090, training loss: 62.9366455078125 = 0.19654583930969238 + 10.0 * 6.274010181427002
Epoch 1090, val loss: 1.1242324113845825
Epoch 1100, training loss: 62.86505889892578 = 0.1902845948934555 + 10.0 * 6.267477512359619
Epoch 1100, val loss: 1.1298835277557373
Epoch 1110, training loss: 62.82672119140625 = 0.18419001996517181 + 10.0 * 6.26425313949585
Epoch 1110, val loss: 1.1358486413955688
Epoch 1120, training loss: 62.80970764160156 = 0.1782667636871338 + 10.0 * 6.263144016265869
Epoch 1120, val loss: 1.1419035196304321
Epoch 1130, training loss: 62.79847717285156 = 0.172506645321846 + 10.0 * 6.26259708404541
Epoch 1130, val loss: 1.1481128931045532
Epoch 1140, training loss: 62.91226577758789 = 0.16686661541461945 + 10.0 * 6.274539947509766
Epoch 1140, val loss: 1.1544526815414429
Epoch 1150, training loss: 62.778045654296875 = 0.1613600105047226 + 10.0 * 6.2616682052612305
Epoch 1150, val loss: 1.160200595855713
Epoch 1160, training loss: 62.77402877807617 = 0.15603482723236084 + 10.0 * 6.261799335479736
Epoch 1160, val loss: 1.16652250289917
Epoch 1170, training loss: 62.745418548583984 = 0.15088284015655518 + 10.0 * 6.259453773498535
Epoch 1170, val loss: 1.1728354692459106
Epoch 1180, training loss: 62.736454010009766 = 0.14589805901050568 + 10.0 * 6.2590556144714355
Epoch 1180, val loss: 1.1792653799057007
Epoch 1190, training loss: 62.77334213256836 = 0.14108112454414368 + 10.0 * 6.26322603225708
Epoch 1190, val loss: 1.1857377290725708
Epoch 1200, training loss: 62.76173400878906 = 0.13632826507091522 + 10.0 * 6.262540340423584
Epoch 1200, val loss: 1.1921415328979492
Epoch 1210, training loss: 62.71491622924805 = 0.13177549839019775 + 10.0 * 6.25831413269043
Epoch 1210, val loss: 1.1983752250671387
Epoch 1220, training loss: 62.70134353637695 = 0.12739060819149017 + 10.0 * 6.257395267486572
Epoch 1220, val loss: 1.2048324346542358
Epoch 1230, training loss: 62.755802154541016 = 0.12316080927848816 + 10.0 * 6.263264179229736
Epoch 1230, val loss: 1.211199164390564
Epoch 1240, training loss: 62.69463348388672 = 0.11905042827129364 + 10.0 * 6.257558345794678
Epoch 1240, val loss: 1.2176764011383057
Epoch 1250, training loss: 62.670135498046875 = 0.11509518325328827 + 10.0 * 6.2555036544799805
Epoch 1250, val loss: 1.2240302562713623
Epoch 1260, training loss: 62.654197692871094 = 0.11130077391862869 + 10.0 * 6.254289627075195
Epoch 1260, val loss: 1.2304097414016724
Epoch 1270, training loss: 62.67063903808594 = 0.10763946920633316 + 10.0 * 6.25629997253418
Epoch 1270, val loss: 1.2368899583816528
Epoch 1280, training loss: 62.647796630859375 = 0.10408251732587814 + 10.0 * 6.254371166229248
Epoch 1280, val loss: 1.242906928062439
Epoch 1290, training loss: 62.65178680419922 = 0.10065222531557083 + 10.0 * 6.25511360168457
Epoch 1290, val loss: 1.2493571043014526
Epoch 1300, training loss: 62.6223258972168 = 0.09737212210893631 + 10.0 * 6.252495288848877
Epoch 1300, val loss: 1.2555278539657593
Epoch 1310, training loss: 62.64813232421875 = 0.09422790259122849 + 10.0 * 6.255390644073486
Epoch 1310, val loss: 1.2617692947387695
Epoch 1320, training loss: 62.620208740234375 = 0.09114541858434677 + 10.0 * 6.252906322479248
Epoch 1320, val loss: 1.2679105997085571
Epoch 1330, training loss: 62.60847091674805 = 0.08820576220750809 + 10.0 * 6.252026557922363
Epoch 1330, val loss: 1.2740397453308105
Epoch 1340, training loss: 62.59059524536133 = 0.08537312597036362 + 10.0 * 6.250522136688232
Epoch 1340, val loss: 1.2800800800323486
Epoch 1350, training loss: 62.59584426879883 = 0.08266130834817886 + 10.0 * 6.251318454742432
Epoch 1350, val loss: 1.286222219467163
Epoch 1360, training loss: 62.60722351074219 = 0.0800340473651886 + 10.0 * 6.252718925476074
Epoch 1360, val loss: 1.292060375213623
Epoch 1370, training loss: 62.56687545776367 = 0.07749616354703903 + 10.0 * 6.248938083648682
Epoch 1370, val loss: 1.297937035560608
Epoch 1380, training loss: 62.56624221801758 = 0.07507447153329849 + 10.0 * 6.249116897583008
Epoch 1380, val loss: 1.3038208484649658
Epoch 1390, training loss: 62.62782669067383 = 0.07275012880563736 + 10.0 * 6.255507469177246
Epoch 1390, val loss: 1.3095357418060303
Epoch 1400, training loss: 62.572052001953125 = 0.0704847201704979 + 10.0 * 6.250156879425049
Epoch 1400, val loss: 1.3153598308563232
Epoch 1410, training loss: 62.54505920410156 = 0.06832990050315857 + 10.0 * 6.247673034667969
Epoch 1410, val loss: 1.3208731412887573
Epoch 1420, training loss: 62.53148651123047 = 0.06625719368457794 + 10.0 * 6.246522903442383
Epoch 1420, val loss: 1.326607584953308
Epoch 1430, training loss: 62.546051025390625 = 0.06427007913589478 + 10.0 * 6.248178005218506
Epoch 1430, val loss: 1.332095980644226
Epoch 1440, training loss: 62.52335739135742 = 0.06233464926481247 + 10.0 * 6.246102333068848
Epoch 1440, val loss: 1.3375877141952515
Epoch 1450, training loss: 62.53069305419922 = 0.06046539917588234 + 10.0 * 6.24702262878418
Epoch 1450, val loss: 1.3427449464797974
Epoch 1460, training loss: 62.531185150146484 = 0.05867268517613411 + 10.0 * 6.247251033782959
Epoch 1460, val loss: 1.3481800556182861
Epoch 1470, training loss: 62.51005172729492 = 0.05696357786655426 + 10.0 * 6.245308876037598
Epoch 1470, val loss: 1.3534952402114868
Epoch 1480, training loss: 62.49372863769531 = 0.05531644821166992 + 10.0 * 6.243841171264648
Epoch 1480, val loss: 1.3586162328720093
Epoch 1490, training loss: 62.506404876708984 = 0.053736478090286255 + 10.0 * 6.245266914367676
Epoch 1490, val loss: 1.3636945486068726
Epoch 1500, training loss: 62.52960205078125 = 0.05220716819167137 + 10.0 * 6.247739315032959
Epoch 1500, val loss: 1.368740439414978
Epoch 1510, training loss: 62.530879974365234 = 0.05071522295475006 + 10.0 * 6.248016357421875
Epoch 1510, val loss: 1.3735586404800415
Epoch 1520, training loss: 62.47652053833008 = 0.04928043857216835 + 10.0 * 6.2427239418029785
Epoch 1520, val loss: 1.3783835172653198
Epoch 1530, training loss: 62.4691047668457 = 0.047909729182720184 + 10.0 * 6.242119789123535
Epoch 1530, val loss: 1.383121132850647
Epoch 1540, training loss: 62.45978927612305 = 0.04659699648618698 + 10.0 * 6.241319179534912
Epoch 1540, val loss: 1.387995958328247
Epoch 1550, training loss: 62.455421447753906 = 0.04533341899514198 + 10.0 * 6.241008758544922
Epoch 1550, val loss: 1.3928593397140503
Epoch 1560, training loss: 62.563812255859375 = 0.044123031198978424 + 10.0 * 6.251968860626221
Epoch 1560, val loss: 1.3975476026535034
Epoch 1570, training loss: 62.50589370727539 = 0.04291302710771561 + 10.0 * 6.246298313140869
Epoch 1570, val loss: 1.401756763458252
Epoch 1580, training loss: 62.45254135131836 = 0.041762951761484146 + 10.0 * 6.241077899932861
Epoch 1580, val loss: 1.406258225440979
Epoch 1590, training loss: 62.44417953491211 = 0.04066843166947365 + 10.0 * 6.24035120010376
Epoch 1590, val loss: 1.4108586311340332
Epoch 1600, training loss: 62.46186447143555 = 0.039614420384168625 + 10.0 * 6.24222469329834
Epoch 1600, val loss: 1.415276288986206
Epoch 1610, training loss: 62.42779541015625 = 0.038590870797634125 + 10.0 * 6.23892068862915
Epoch 1610, val loss: 1.419533133506775
Epoch 1620, training loss: 62.46176528930664 = 0.03760887309908867 + 10.0 * 6.242415428161621
Epoch 1620, val loss: 1.4236565828323364
Epoch 1630, training loss: 62.441810607910156 = 0.03665005788207054 + 10.0 * 6.24051570892334
Epoch 1630, val loss: 1.428014874458313
Epoch 1640, training loss: 62.4195442199707 = 0.035731807351112366 + 10.0 * 6.238381385803223
Epoch 1640, val loss: 1.4320355653762817
Epoch 1650, training loss: 62.40662384033203 = 0.034842949360609055 + 10.0 * 6.237177848815918
Epoch 1650, val loss: 1.4362106323242188
Epoch 1660, training loss: 62.406105041503906 = 0.03399558737874031 + 10.0 * 6.237210750579834
Epoch 1660, val loss: 1.4403349161148071
Epoch 1670, training loss: 62.460140228271484 = 0.033182185143232346 + 10.0 * 6.2426958084106445
Epoch 1670, val loss: 1.4442025423049927
Epoch 1680, training loss: 62.42845153808594 = 0.03236887976527214 + 10.0 * 6.239608287811279
Epoch 1680, val loss: 1.448264479637146
Epoch 1690, training loss: 62.43470764160156 = 0.03158723562955856 + 10.0 * 6.240312099456787
Epoch 1690, val loss: 1.4519269466400146
Epoch 1700, training loss: 62.401920318603516 = 0.030838118866086006 + 10.0 * 6.23710823059082
Epoch 1700, val loss: 1.455932855606079
Epoch 1710, training loss: 62.387054443359375 = 0.030119167640805244 + 10.0 * 6.235693454742432
Epoch 1710, val loss: 1.4596904516220093
Epoch 1720, training loss: 62.39134216308594 = 0.02942574769258499 + 10.0 * 6.236191749572754
Epoch 1720, val loss: 1.46345853805542
Epoch 1730, training loss: 62.395259857177734 = 0.02875388041138649 + 10.0 * 6.236650466918945
Epoch 1730, val loss: 1.467187762260437
Epoch 1740, training loss: 62.42399215698242 = 0.028096282854676247 + 10.0 * 6.239589691162109
Epoch 1740, val loss: 1.4708038568496704
Epoch 1750, training loss: 62.396705627441406 = 0.027455933392047882 + 10.0 * 6.23692512512207
Epoch 1750, val loss: 1.4742076396942139
Epoch 1760, training loss: 62.37749481201172 = 0.026844259351491928 + 10.0 * 6.23506498336792
Epoch 1760, val loss: 1.4776611328125
Epoch 1770, training loss: 62.39776611328125 = 0.026250256225466728 + 10.0 * 6.237151622772217
Epoch 1770, val loss: 1.4810773134231567
Epoch 1780, training loss: 62.376277923583984 = 0.025675758719444275 + 10.0 * 6.235060214996338
Epoch 1780, val loss: 1.484401822090149
Epoch 1790, training loss: 62.369781494140625 = 0.02512410841882229 + 10.0 * 6.234465599060059
Epoch 1790, val loss: 1.4877270460128784
Epoch 1800, training loss: 62.36627960205078 = 0.02458464540541172 + 10.0 * 6.2341694831848145
Epoch 1800, val loss: 1.4910143613815308
Epoch 1810, training loss: 62.38534164428711 = 0.02405957505106926 + 10.0 * 6.236128330230713
Epoch 1810, val loss: 1.494201421737671
Epoch 1820, training loss: 62.36652374267578 = 0.02355540357530117 + 10.0 * 6.234296798706055
Epoch 1820, val loss: 1.497522234916687
Epoch 1830, training loss: 62.38902282714844 = 0.023062562569975853 + 10.0 * 6.23659610748291
Epoch 1830, val loss: 1.5006585121154785
Epoch 1840, training loss: 62.350013732910156 = 0.022581594064831734 + 10.0 * 6.232743263244629
Epoch 1840, val loss: 1.5036711692810059
Epoch 1850, training loss: 62.35232162475586 = 0.02212030626833439 + 10.0 * 6.233019828796387
Epoch 1850, val loss: 1.5068414211273193
Epoch 1860, training loss: 62.364593505859375 = 0.02167387679219246 + 10.0 * 6.234292030334473
Epoch 1860, val loss: 1.509792447090149
Epoch 1870, training loss: 62.34727478027344 = 0.02123982273042202 + 10.0 * 6.232603549957275
Epoch 1870, val loss: 1.5127464532852173
Epoch 1880, training loss: 62.338417053222656 = 0.02081422135233879 + 10.0 * 6.231760501861572
Epoch 1880, val loss: 1.5157963037490845
Epoch 1890, training loss: 62.38686752319336 = 0.02040601707994938 + 10.0 * 6.2366461753845215
Epoch 1890, val loss: 1.5184986591339111
Epoch 1900, training loss: 62.357704162597656 = 0.020000828430056572 + 10.0 * 6.233770370483398
Epoch 1900, val loss: 1.5212092399597168
Epoch 1910, training loss: 62.32195281982422 = 0.01961139403283596 + 10.0 * 6.230234146118164
Epoch 1910, val loss: 1.524107813835144
Epoch 1920, training loss: 62.313167572021484 = 0.019240958616137505 + 10.0 * 6.2293925285339355
Epoch 1920, val loss: 1.526987075805664
Epoch 1930, training loss: 62.32192611694336 = 0.018880847841501236 + 10.0 * 6.23030424118042
Epoch 1930, val loss: 1.5298606157302856
Epoch 1940, training loss: 62.35625457763672 = 0.018535859882831573 + 10.0 * 6.233771800994873
Epoch 1940, val loss: 1.5325326919555664
Epoch 1950, training loss: 62.330970764160156 = 0.018177950754761696 + 10.0 * 6.231279373168945
Epoch 1950, val loss: 1.5350744724273682
Epoch 1960, training loss: 62.308265686035156 = 0.01784425973892212 + 10.0 * 6.229042053222656
Epoch 1960, val loss: 1.5377370119094849
Epoch 1970, training loss: 62.3419075012207 = 0.017520995810627937 + 10.0 * 6.232438564300537
Epoch 1970, val loss: 1.5402756929397583
Epoch 1980, training loss: 62.31015396118164 = 0.017198026180267334 + 10.0 * 6.22929573059082
Epoch 1980, val loss: 1.5426818132400513
Epoch 1990, training loss: 62.2944450378418 = 0.016886817291378975 + 10.0 * 6.227755546569824
Epoch 1990, val loss: 1.54537034034729
Epoch 2000, training loss: 62.28871154785156 = 0.016588184982538223 + 10.0 * 6.227212429046631
Epoch 2000, val loss: 1.5478631258010864
Epoch 2010, training loss: 62.30377960205078 = 0.01629979908466339 + 10.0 * 6.228747844696045
Epoch 2010, val loss: 1.5503208637237549
Epoch 2020, training loss: 62.33793258666992 = 0.016016602516174316 + 10.0 * 6.232191562652588
Epoch 2020, val loss: 1.552765965461731
Epoch 2030, training loss: 62.31121063232422 = 0.015739858150482178 + 10.0 * 6.229547023773193
Epoch 2030, val loss: 1.5552213191986084
Epoch 2040, training loss: 62.29347610473633 = 0.015469166450202465 + 10.0 * 6.2278008460998535
Epoch 2040, val loss: 1.5575149059295654
Epoch 2050, training loss: 62.3112907409668 = 0.015210363082587719 + 10.0 * 6.229608058929443
Epoch 2050, val loss: 1.5598727464675903
Epoch 2060, training loss: 62.289825439453125 = 0.014948136173188686 + 10.0 * 6.227487564086914
Epoch 2060, val loss: 1.5621787309646606
Epoch 2070, training loss: 62.27299880981445 = 0.014698646031320095 + 10.0 * 6.225830078125
Epoch 2070, val loss: 1.5645478963851929
Epoch 2080, training loss: 62.26826477050781 = 0.014458964578807354 + 10.0 * 6.225380897521973
Epoch 2080, val loss: 1.5669223070144653
Epoch 2090, training loss: 62.27983856201172 = 0.014228183776140213 + 10.0 * 6.226561069488525
Epoch 2090, val loss: 1.5691871643066406
Epoch 2100, training loss: 62.30817794799805 = 0.014000331982970238 + 10.0 * 6.22941780090332
Epoch 2100, val loss: 1.5713173151016235
Epoch 2110, training loss: 62.286922454833984 = 0.013765751384198666 + 10.0 * 6.227315425872803
Epoch 2110, val loss: 1.5733681917190552
Epoch 2120, training loss: 62.28316879272461 = 0.013546719215810299 + 10.0 * 6.226962089538574
Epoch 2120, val loss: 1.5754879713058472
Epoch 2130, training loss: 62.2623176574707 = 0.013332574628293514 + 10.0 * 6.224898338317871
Epoch 2130, val loss: 1.5777571201324463
Epoch 2140, training loss: 62.30270767211914 = 0.013131825253367424 + 10.0 * 6.228957653045654
Epoch 2140, val loss: 1.579830527305603
Epoch 2150, training loss: 62.259033203125 = 0.01291926484555006 + 10.0 * 6.224611282348633
Epoch 2150, val loss: 1.5817818641662598
Epoch 2160, training loss: 62.24604797363281 = 0.01272001676261425 + 10.0 * 6.22333288192749
Epoch 2160, val loss: 1.5838812589645386
Epoch 2170, training loss: 62.24024963378906 = 0.012526653707027435 + 10.0 * 6.222772121429443
Epoch 2170, val loss: 1.5859323740005493
Epoch 2180, training loss: 62.23881912231445 = 0.012341011315584183 + 10.0 * 6.222647666931152
Epoch 2180, val loss: 1.588025450706482
Epoch 2190, training loss: 62.309295654296875 = 0.012159841135144234 + 10.0 * 6.229713439941406
Epoch 2190, val loss: 1.5899423360824585
Epoch 2200, training loss: 62.260047912597656 = 0.011978788301348686 + 10.0 * 6.224806785583496
Epoch 2200, val loss: 1.5918495655059814
Epoch 2210, training loss: 62.26344299316406 = 0.011800037696957588 + 10.0 * 6.225164413452148
Epoch 2210, val loss: 1.5938103199005127
Epoch 2220, training loss: 62.245975494384766 = 0.011628401465713978 + 10.0 * 6.223434925079346
Epoch 2220, val loss: 1.5956087112426758
Epoch 2230, training loss: 62.23438262939453 = 0.011460577137768269 + 10.0 * 6.222292423248291
Epoch 2230, val loss: 1.5975701808929443
Epoch 2240, training loss: 62.30778884887695 = 0.011298088356852531 + 10.0 * 6.229649066925049
Epoch 2240, val loss: 1.5992969274520874
Epoch 2250, training loss: 62.24385452270508 = 0.011135849170386791 + 10.0 * 6.22327184677124
Epoch 2250, val loss: 1.6011476516723633
Epoch 2260, training loss: 62.225467681884766 = 0.010977983474731445 + 10.0 * 6.22144889831543
Epoch 2260, val loss: 1.6029266119003296
Epoch 2270, training loss: 62.22150802612305 = 0.010828099213540554 + 10.0 * 6.221067905426025
Epoch 2270, val loss: 1.6047440767288208
Epoch 2280, training loss: 62.28730392456055 = 0.010687220841646194 + 10.0 * 6.227661609649658
Epoch 2280, val loss: 1.6064366102218628
Epoch 2290, training loss: 62.23237991333008 = 0.010530435480177402 + 10.0 * 6.222185134887695
Epoch 2290, val loss: 1.6080751419067383
Epoch 2300, training loss: 62.24280548095703 = 0.010389033704996109 + 10.0 * 6.223241806030273
Epoch 2300, val loss: 1.609861969947815
Epoch 2310, training loss: 62.24251174926758 = 0.01024662796407938 + 10.0 * 6.223226547241211
Epoch 2310, val loss: 1.6113916635513306
Epoch 2320, training loss: 62.22917175292969 = 0.010108495131134987 + 10.0 * 6.2219061851501465
Epoch 2320, val loss: 1.612975835800171
Epoch 2330, training loss: 62.21216583251953 = 0.009974651969969273 + 10.0 * 6.220219135284424
Epoch 2330, val loss: 1.6146860122680664
Epoch 2340, training loss: 62.241539001464844 = 0.00984887219965458 + 10.0 * 6.223168849945068
Epoch 2340, val loss: 1.6162796020507812
Epoch 2350, training loss: 62.21317672729492 = 0.009716037660837173 + 10.0 * 6.220345973968506
Epoch 2350, val loss: 1.6178648471832275
Epoch 2360, training loss: 62.222721099853516 = 0.00959064532071352 + 10.0 * 6.221312999725342
Epoch 2360, val loss: 1.6194114685058594
Epoch 2370, training loss: 62.20969009399414 = 0.009468009695410728 + 10.0 * 6.220022201538086
Epoch 2370, val loss: 1.6210271120071411
Epoch 2380, training loss: 62.235103607177734 = 0.009348724037408829 + 10.0 * 6.2225751876831055
Epoch 2380, val loss: 1.6225128173828125
Epoch 2390, training loss: 62.22556686401367 = 0.0092276306822896 + 10.0 * 6.2216339111328125
Epoch 2390, val loss: 1.6238503456115723
Epoch 2400, training loss: 62.19649124145508 = 0.009106665849685669 + 10.0 * 6.218738555908203
Epoch 2400, val loss: 1.625400185585022
Epoch 2410, training loss: 62.18939208984375 = 0.00899509247392416 + 10.0 * 6.218039512634277
Epoch 2410, val loss: 1.6269572973251343
Epoch 2420, training loss: 62.18465042114258 = 0.008885638788342476 + 10.0 * 6.217576503753662
Epoch 2420, val loss: 1.6284197568893433
Epoch 2430, training loss: 62.18976593017578 = 0.008780186995863914 + 10.0 * 6.2180986404418945
Epoch 2430, val loss: 1.6298420429229736
Epoch 2440, training loss: 62.2976188659668 = 0.008679775521159172 + 10.0 * 6.228894233703613
Epoch 2440, val loss: 1.631230354309082
Epoch 2450, training loss: 62.22378921508789 = 0.008565121330320835 + 10.0 * 6.221522331237793
Epoch 2450, val loss: 1.6325773000717163
Epoch 2460, training loss: 62.18474578857422 = 0.008459924720227718 + 10.0 * 6.217628479003906
Epoch 2460, val loss: 1.633948802947998
Epoch 2470, training loss: 62.17557144165039 = 0.008359911851584911 + 10.0 * 6.216721057891846
Epoch 2470, val loss: 1.635535478591919
Epoch 2480, training loss: 62.18219757080078 = 0.008263887837529182 + 10.0 * 6.217393398284912
Epoch 2480, val loss: 1.6370207071304321
Epoch 2490, training loss: 62.26465606689453 = 0.00817179586738348 + 10.0 * 6.225648403167725
Epoch 2490, val loss: 1.6383252143859863
Epoch 2500, training loss: 62.197975158691406 = 0.008068270981311798 + 10.0 * 6.218990802764893
Epoch 2500, val loss: 1.6396421194076538
Epoch 2510, training loss: 62.17599105834961 = 0.007976836524903774 + 10.0 * 6.216801643371582
Epoch 2510, val loss: 1.6411325931549072
Epoch 2520, training loss: 62.174713134765625 = 0.007886048406362534 + 10.0 * 6.2166829109191895
Epoch 2520, val loss: 1.642424464225769
Epoch 2530, training loss: 62.28460693359375 = 0.007797955069690943 + 10.0 * 6.2276811599731445
Epoch 2530, val loss: 1.6435950994491577
Epoch 2540, training loss: 62.20411682128906 = 0.007707248441874981 + 10.0 * 6.219641208648682
Epoch 2540, val loss: 1.6449809074401855
Epoch 2550, training loss: 62.17593002319336 = 0.0076194885186851025 + 10.0 * 6.216831207275391
Epoch 2550, val loss: 1.6462675333023071
Epoch 2560, training loss: 62.16468811035156 = 0.007537263445556164 + 10.0 * 6.215714931488037
Epoch 2560, val loss: 1.6476166248321533
Epoch 2570, training loss: 62.17876052856445 = 0.007456561084836721 + 10.0 * 6.217130184173584
Epoch 2570, val loss: 1.6487934589385986
Epoch 2580, training loss: 62.19050216674805 = 0.0073743234388530254 + 10.0 * 6.218312740325928
Epoch 2580, val loss: 1.6500076055526733
Epoch 2590, training loss: 62.18209457397461 = 0.007291282992810011 + 10.0 * 6.217480659484863
Epoch 2590, val loss: 1.65105140209198
Epoch 2600, training loss: 62.16939926147461 = 0.007212580181658268 + 10.0 * 6.2162184715271
Epoch 2600, val loss: 1.6524003744125366
Epoch 2610, training loss: 62.15275955200195 = 0.007135996129363775 + 10.0 * 6.21456241607666
Epoch 2610, val loss: 1.6536000967025757
Epoch 2620, training loss: 62.155784606933594 = 0.007062101736664772 + 10.0 * 6.214872360229492
Epoch 2620, val loss: 1.654983639717102
Epoch 2630, training loss: 62.182228088378906 = 0.006990802474319935 + 10.0 * 6.217523574829102
Epoch 2630, val loss: 1.656063437461853
Epoch 2640, training loss: 62.164642333984375 = 0.006916507612913847 + 10.0 * 6.21577262878418
Epoch 2640, val loss: 1.6571255922317505
Epoch 2650, training loss: 62.16749572753906 = 0.006843117065727711 + 10.0 * 6.216065406799316
Epoch 2650, val loss: 1.6581854820251465
Epoch 2660, training loss: 62.15754699707031 = 0.006773001980036497 + 10.0 * 6.2150774002075195
Epoch 2660, val loss: 1.6593966484069824
Epoch 2670, training loss: 62.19167709350586 = 0.006704268977046013 + 10.0 * 6.218497276306152
Epoch 2670, val loss: 1.6602771282196045
Epoch 2680, training loss: 62.161170959472656 = 0.006636312231421471 + 10.0 * 6.215453147888184
Epoch 2680, val loss: 1.6615407466888428
Epoch 2690, training loss: 62.195213317871094 = 0.006571477744728327 + 10.0 * 6.2188639640808105
Epoch 2690, val loss: 1.6625614166259766
Epoch 2700, training loss: 62.14740753173828 = 0.006501726806163788 + 10.0 * 6.214090824127197
Epoch 2700, val loss: 1.6635438203811646
Epoch 2710, training loss: 62.146278381347656 = 0.006437396164983511 + 10.0 * 6.21398401260376
Epoch 2710, val loss: 1.6646430492401123
Epoch 2720, training loss: 62.16029357910156 = 0.006377945188432932 + 10.0 * 6.215391635894775
Epoch 2720, val loss: 1.6658118963241577
Epoch 2730, training loss: 62.1530647277832 = 0.006313852034509182 + 10.0 * 6.214674949645996
Epoch 2730, val loss: 1.666670560836792
Epoch 2740, training loss: 62.141944885253906 = 0.006251503713428974 + 10.0 * 6.213569164276123
Epoch 2740, val loss: 1.6679060459136963
Epoch 2750, training loss: 62.15296173095703 = 0.006193190347403288 + 10.0 * 6.214676856994629
Epoch 2750, val loss: 1.668922781944275
Epoch 2760, training loss: 62.159934997558594 = 0.006134177092462778 + 10.0 * 6.2153801918029785
Epoch 2760, val loss: 1.669977068901062
Epoch 2770, training loss: 62.15052795410156 = 0.006074399687349796 + 10.0 * 6.2144455909729
Epoch 2770, val loss: 1.6709436178207397
Epoch 2780, training loss: 62.13663864135742 = 0.00601658970117569 + 10.0 * 6.213062286376953
Epoch 2780, val loss: 1.6719025373458862
Epoch 2790, training loss: 62.13564682006836 = 0.005961337126791477 + 10.0 * 6.212968349456787
Epoch 2790, val loss: 1.6728541851043701
Epoch 2800, training loss: 62.22100067138672 = 0.005906680598855019 + 10.0 * 6.2215094566345215
Epoch 2800, val loss: 1.6737481355667114
Epoch 2810, training loss: 62.17527389526367 = 0.005853302776813507 + 10.0 * 6.216942310333252
Epoch 2810, val loss: 1.6746312379837036
Epoch 2820, training loss: 62.13321304321289 = 0.005794984754174948 + 10.0 * 6.212741851806641
Epoch 2820, val loss: 1.6755695343017578
Epoch 2830, training loss: 62.12030029296875 = 0.005744537804275751 + 10.0 * 6.211455345153809
Epoch 2830, val loss: 1.6766178607940674
Epoch 2840, training loss: 62.12264633178711 = 0.005693905055522919 + 10.0 * 6.211695194244385
Epoch 2840, val loss: 1.6775658130645752
Epoch 2850, training loss: 62.17721939086914 = 0.005646112374961376 + 10.0 * 6.217157363891602
Epoch 2850, val loss: 1.678545355796814
Epoch 2860, training loss: 62.13479995727539 = 0.005593355279415846 + 10.0 * 6.212920665740967
Epoch 2860, val loss: 1.6792799234390259
Epoch 2870, training loss: 62.132930755615234 = 0.005543274339288473 + 10.0 * 6.212738990783691
Epoch 2870, val loss: 1.680085301399231
Epoch 2880, training loss: 62.13600158691406 = 0.005494680255651474 + 10.0 * 6.213050842285156
Epoch 2880, val loss: 1.6809272766113281
Epoch 2890, training loss: 62.11376953125 = 0.005446767900139093 + 10.0 * 6.210832118988037
Epoch 2890, val loss: 1.681917428970337
Epoch 2900, training loss: 62.13846206665039 = 0.005401329603046179 + 10.0 * 6.213305950164795
Epoch 2900, val loss: 1.6827932596206665
Epoch 2910, training loss: 62.157508850097656 = 0.005354408640414476 + 10.0 * 6.21521520614624
Epoch 2910, val loss: 1.6833375692367554
Epoch 2920, training loss: 62.12479019165039 = 0.005307333078235388 + 10.0 * 6.211948394775391
Epoch 2920, val loss: 1.6842775344848633
Epoch 2930, training loss: 62.1134033203125 = 0.005263784434646368 + 10.0 * 6.210813999176025
Epoch 2930, val loss: 1.6850864887237549
Epoch 2940, training loss: 62.103355407714844 = 0.0052192299626767635 + 10.0 * 6.209813594818115
Epoch 2940, val loss: 1.6859480142593384
Epoch 2950, training loss: 62.143558502197266 = 0.005177206359803677 + 10.0 * 6.21383810043335
Epoch 2950, val loss: 1.6867196559906006
Epoch 2960, training loss: 62.11325454711914 = 0.005133341532200575 + 10.0 * 6.210812091827393
Epoch 2960, val loss: 1.6873921155929565
Epoch 2970, training loss: 62.112003326416016 = 0.005090050864964724 + 10.0 * 6.210691452026367
Epoch 2970, val loss: 1.6880935430526733
Epoch 2980, training loss: 62.14396667480469 = 0.005049158353358507 + 10.0 * 6.213891506195068
Epoch 2980, val loss: 1.6888667345046997
Epoch 2990, training loss: 62.09799575805664 = 0.005007782019674778 + 10.0 * 6.209298610687256
Epoch 2990, val loss: 1.689554214477539
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 87.91476440429688 = 1.9463026523590088 + 10.0 * 8.596845626831055
Epoch 0, val loss: 1.955297827720642
Epoch 10, training loss: 87.89857482910156 = 1.9359911680221558 + 10.0 * 8.596258163452148
Epoch 10, val loss: 1.944743037223816
Epoch 20, training loss: 87.83572387695312 = 1.9234278202056885 + 10.0 * 8.591229438781738
Epoch 20, val loss: 1.9312045574188232
Epoch 30, training loss: 87.41637420654297 = 1.9074838161468506 + 10.0 * 8.550889015197754
Epoch 30, val loss: 1.9137426614761353
Epoch 40, training loss: 84.57052612304688 = 1.889249563217163 + 10.0 * 8.26812744140625
Epoch 40, val loss: 1.8942923545837402
Epoch 50, training loss: 79.07478332519531 = 1.8716017007827759 + 10.0 * 7.720317840576172
Epoch 50, val loss: 1.876619815826416
Epoch 60, training loss: 74.90667724609375 = 1.8592699766159058 + 10.0 * 7.304740905761719
Epoch 60, val loss: 1.8649518489837646
Epoch 70, training loss: 71.91804504394531 = 1.8492729663848877 + 10.0 * 7.0068769454956055
Epoch 70, val loss: 1.8550703525543213
Epoch 80, training loss: 70.25254821777344 = 1.8397561311721802 + 10.0 * 6.841279029846191
Epoch 80, val loss: 1.8457447290420532
Epoch 90, training loss: 69.21232604980469 = 1.8294243812561035 + 10.0 * 6.738290309906006
Epoch 90, val loss: 1.8357816934585571
Epoch 100, training loss: 68.40472412109375 = 1.8186824321746826 + 10.0 * 6.658604621887207
Epoch 100, val loss: 1.8255620002746582
Epoch 110, training loss: 67.88018798828125 = 1.8080379962921143 + 10.0 * 6.607215404510498
Epoch 110, val loss: 1.8154027462005615
Epoch 120, training loss: 67.51923370361328 = 1.7971922159194946 + 10.0 * 6.572204113006592
Epoch 120, val loss: 1.8051692247390747
Epoch 130, training loss: 67.20680236816406 = 1.7857990264892578 + 10.0 * 6.542100429534912
Epoch 130, val loss: 1.7944709062576294
Epoch 140, training loss: 67.06243896484375 = 1.7736616134643555 + 10.0 * 6.5288777351379395
Epoch 140, val loss: 1.7833871841430664
Epoch 150, training loss: 66.78990936279297 = 1.760789394378662 + 10.0 * 6.5029120445251465
Epoch 150, val loss: 1.7716021537780762
Epoch 160, training loss: 66.60913848876953 = 1.7468808889389038 + 10.0 * 6.486225605010986
Epoch 160, val loss: 1.7592941522598267
Epoch 170, training loss: 66.4509506225586 = 1.7317547798156738 + 10.0 * 6.471919059753418
Epoch 170, val loss: 1.7460511922836304
Epoch 180, training loss: 66.31214904785156 = 1.7151747941970825 + 10.0 * 6.459697723388672
Epoch 180, val loss: 1.731721043586731
Epoch 190, training loss: 66.18795776367188 = 1.6969513893127441 + 10.0 * 6.449100494384766
Epoch 190, val loss: 1.716191053390503
Epoch 200, training loss: 66.08110809326172 = 1.6768922805786133 + 10.0 * 6.440421104431152
Epoch 200, val loss: 1.699240803718567
Epoch 210, training loss: 65.97259521484375 = 1.654937982559204 + 10.0 * 6.431766033172607
Epoch 210, val loss: 1.6807622909545898
Epoch 220, training loss: 65.8599853515625 = 1.6309248208999634 + 10.0 * 6.422905445098877
Epoch 220, val loss: 1.6606802940368652
Epoch 230, training loss: 65.7662124633789 = 1.6048264503479004 + 10.0 * 6.416138648986816
Epoch 230, val loss: 1.6389459371566772
Epoch 240, training loss: 65.69024658203125 = 1.5764890909194946 + 10.0 * 6.411375045776367
Epoch 240, val loss: 1.615628957748413
Epoch 250, training loss: 65.55863952636719 = 1.5461331605911255 + 10.0 * 6.40125036239624
Epoch 250, val loss: 1.5907065868377686
Epoch 260, training loss: 65.4683609008789 = 1.5138307809829712 + 10.0 * 6.395452976226807
Epoch 260, val loss: 1.5644376277923584
Epoch 270, training loss: 65.38236236572266 = 1.4797284603118896 + 10.0 * 6.390263557434082
Epoch 270, val loss: 1.5369069576263428
Epoch 280, training loss: 65.34336853027344 = 1.4440441131591797 + 10.0 * 6.389932632446289
Epoch 280, val loss: 1.5084619522094727
Epoch 290, training loss: 65.2003173828125 = 1.4072462320327759 + 10.0 * 6.379307270050049
Epoch 290, val loss: 1.4792324304580688
Epoch 300, training loss: 65.1170654296875 = 1.3696372509002686 + 10.0 * 6.3747429847717285
Epoch 300, val loss: 1.4496437311172485
Epoch 310, training loss: 65.03126525878906 = 1.3315836191177368 + 10.0 * 6.369967937469482
Epoch 310, val loss: 1.4200066328048706
Epoch 320, training loss: 64.95092010498047 = 1.2934837341308594 + 10.0 * 6.365743637084961
Epoch 320, val loss: 1.3906399011611938
Epoch 330, training loss: 64.8736572265625 = 1.2555937767028809 + 10.0 * 6.361806392669678
Epoch 330, val loss: 1.3617918491363525
Epoch 340, training loss: 64.80062103271484 = 1.218322515487671 + 10.0 * 6.358229637145996
Epoch 340, val loss: 1.3335981369018555
Epoch 350, training loss: 64.73478698730469 = 1.1818327903747559 + 10.0 * 6.355295658111572
Epoch 350, val loss: 1.3064366579055786
Epoch 360, training loss: 64.6520004272461 = 1.1464264392852783 + 10.0 * 6.350557804107666
Epoch 360, val loss: 1.2804129123687744
Epoch 370, training loss: 64.58382415771484 = 1.1121816635131836 + 10.0 * 6.347164154052734
Epoch 370, val loss: 1.2553834915161133
Epoch 380, training loss: 64.53516387939453 = 1.0790553092956543 + 10.0 * 6.345610618591309
Epoch 380, val loss: 1.2315576076507568
Epoch 390, training loss: 64.46758270263672 = 1.0471515655517578 + 10.0 * 6.342043399810791
Epoch 390, val loss: 1.2089972496032715
Epoch 400, training loss: 64.40768432617188 = 1.0165497064590454 + 10.0 * 6.339113712310791
Epoch 400, val loss: 1.1876651048660278
Epoch 410, training loss: 64.37034606933594 = 0.9872400164604187 + 10.0 * 6.338310241699219
Epoch 410, val loss: 1.1674526929855347
Epoch 420, training loss: 64.29226684570312 = 0.9591875672340393 + 10.0 * 6.33330774307251
Epoch 420, val loss: 1.148566484451294
Epoch 430, training loss: 64.23182678222656 = 0.9323359727859497 + 10.0 * 6.329949378967285
Epoch 430, val loss: 1.1307997703552246
Epoch 440, training loss: 64.23448944091797 = 0.9066755175590515 + 10.0 * 6.3327813148498535
Epoch 440, val loss: 1.114366054534912
Epoch 450, training loss: 64.14746856689453 = 0.8819409012794495 + 10.0 * 6.326552391052246
Epoch 450, val loss: 1.0987234115600586
Epoch 460, training loss: 64.10538482666016 = 0.8584131598472595 + 10.0 * 6.324697494506836
Epoch 460, val loss: 1.0843549966812134
Epoch 470, training loss: 64.06104278564453 = 0.8356818556785583 + 10.0 * 6.322536468505859
Epoch 470, val loss: 1.0708181858062744
Epoch 480, training loss: 63.99277114868164 = 0.8138757944107056 + 10.0 * 6.317889213562012
Epoch 480, val loss: 1.058281421661377
Epoch 490, training loss: 63.95279312133789 = 0.7928432822227478 + 10.0 * 6.315995216369629
Epoch 490, val loss: 1.0466465950012207
Epoch 500, training loss: 63.93695068359375 = 0.7724725604057312 + 10.0 * 6.316447734832764
Epoch 500, val loss: 1.0357823371887207
Epoch 510, training loss: 63.92679977416992 = 0.7525432705879211 + 10.0 * 6.317425727844238
Epoch 510, val loss: 1.0254441499710083
Epoch 520, training loss: 63.845359802246094 = 0.7331727743148804 + 10.0 * 6.311218738555908
Epoch 520, val loss: 1.0158507823944092
Epoch 530, training loss: 63.79621887207031 = 0.7143170833587646 + 10.0 * 6.30819034576416
Epoch 530, val loss: 1.0069297552108765
Epoch 540, training loss: 63.7978515625 = 0.6958903074264526 + 10.0 * 6.310196399688721
Epoch 540, val loss: 0.9985723495483398
Epoch 550, training loss: 63.73402786254883 = 0.677660346031189 + 10.0 * 6.305636405944824
Epoch 550, val loss: 0.9905948042869568
Epoch 560, training loss: 63.691650390625 = 0.6598207354545593 + 10.0 * 6.303183078765869
Epoch 560, val loss: 0.983248233795166
Epoch 570, training loss: 63.670345306396484 = 0.642278790473938 + 10.0 * 6.302806377410889
Epoch 570, val loss: 0.976468563079834
Epoch 580, training loss: 63.63775634765625 = 0.6249834299087524 + 10.0 * 6.301277160644531
Epoch 580, val loss: 0.9700544476509094
Epoch 590, training loss: 63.60626983642578 = 0.6079626083374023 + 10.0 * 6.299830436706543
Epoch 590, val loss: 0.9641346335411072
Epoch 600, training loss: 63.563655853271484 = 0.5913270115852356 + 10.0 * 6.297232627868652
Epoch 600, val loss: 0.9587888121604919
Epoch 610, training loss: 63.549049377441406 = 0.5750348567962646 + 10.0 * 6.297401428222656
Epoch 610, val loss: 0.9540029764175415
Epoch 620, training loss: 63.51066207885742 = 0.5590629577636719 + 10.0 * 6.295159816741943
Epoch 620, val loss: 0.9495832920074463
Epoch 630, training loss: 63.49107360839844 = 0.5434564352035522 + 10.0 * 6.294761657714844
Epoch 630, val loss: 0.9458333253860474
Epoch 640, training loss: 63.466182708740234 = 0.5281627774238586 + 10.0 * 6.293801784515381
Epoch 640, val loss: 0.9423580169677734
Epoch 650, training loss: 63.421142578125 = 0.5132524967193604 + 10.0 * 6.2907891273498535
Epoch 650, val loss: 0.939699113368988
Epoch 660, training loss: 63.38666534423828 = 0.4987953305244446 + 10.0 * 6.288786888122559
Epoch 660, val loss: 0.9374950528144836
Epoch 670, training loss: 63.36717224121094 = 0.48474353551864624 + 10.0 * 6.288242816925049
Epoch 670, val loss: 0.9358730316162109
Epoch 680, training loss: 63.35343551635742 = 0.47105172276496887 + 10.0 * 6.288238525390625
Epoch 680, val loss: 0.9347931146621704
Epoch 690, training loss: 63.35451889038086 = 0.4577428996562958 + 10.0 * 6.289677619934082
Epoch 690, val loss: 0.9341021776199341
Epoch 700, training loss: 63.300331115722656 = 0.4448743462562561 + 10.0 * 6.285545825958252
Epoch 700, val loss: 0.9339694380760193
Epoch 710, training loss: 63.256561279296875 = 0.43239498138427734 + 10.0 * 6.282416343688965
Epoch 710, val loss: 0.9343419671058655
Epoch 720, training loss: 63.23952102661133 = 0.42032837867736816 + 10.0 * 6.281919002532959
Epoch 720, val loss: 0.9352498054504395
Epoch 730, training loss: 63.293148040771484 = 0.40862494707107544 + 10.0 * 6.2884521484375
Epoch 730, val loss: 0.9366664886474609
Epoch 740, training loss: 63.24528121948242 = 0.39729705452919006 + 10.0 * 6.284798622131348
Epoch 740, val loss: 0.9381734132766724
Epoch 750, training loss: 63.178016662597656 = 0.3862459361553192 + 10.0 * 6.279177188873291
Epoch 750, val loss: 0.9403189420700073
Epoch 760, training loss: 63.15325927734375 = 0.37562933564186096 + 10.0 * 6.2777628898620605
Epoch 760, val loss: 0.9429547190666199
Epoch 770, training loss: 63.165443420410156 = 0.3653357923030853 + 10.0 * 6.28001070022583
Epoch 770, val loss: 0.9458138942718506
Epoch 780, training loss: 63.12620544433594 = 0.3553153872489929 + 10.0 * 6.2770891189575195
Epoch 780, val loss: 0.9490930438041687
Epoch 790, training loss: 63.124874114990234 = 0.3455800712108612 + 10.0 * 6.277929306030273
Epoch 790, val loss: 0.9524576663970947
Epoch 800, training loss: 63.09143829345703 = 0.33613601326942444 + 10.0 * 6.2755303382873535
Epoch 800, val loss: 0.9563182592391968
Epoch 810, training loss: 63.06219482421875 = 0.3269260823726654 + 10.0 * 6.273526668548584
Epoch 810, val loss: 0.9602668881416321
Epoch 820, training loss: 63.07295608520508 = 0.31799599528312683 + 10.0 * 6.275496006011963
Epoch 820, val loss: 0.9646782875061035
Epoch 830, training loss: 63.026790618896484 = 0.3092738091945648 + 10.0 * 6.271751880645752
Epoch 830, val loss: 0.9690281748771667
Epoch 840, training loss: 63.00715255737305 = 0.3007834553718567 + 10.0 * 6.270636558532715
Epoch 840, val loss: 0.9738154411315918
Epoch 850, training loss: 63.04581832885742 = 0.2925134599208832 + 10.0 * 6.275330543518066
Epoch 850, val loss: 0.9785195589065552
Epoch 860, training loss: 62.98153305053711 = 0.2843596339225769 + 10.0 * 6.269717216491699
Epoch 860, val loss: 0.9836152195930481
Epoch 870, training loss: 62.98889923095703 = 0.27645280957221985 + 10.0 * 6.271245002746582
Epoch 870, val loss: 0.9885931015014648
Epoch 880, training loss: 62.98588180541992 = 0.26870429515838623 + 10.0 * 6.2717180252075195
Epoch 880, val loss: 0.9940267205238342
Epoch 890, training loss: 62.93679428100586 = 0.2610717713832855 + 10.0 * 6.267572402954102
Epoch 890, val loss: 0.9993064403533936
Epoch 900, training loss: 62.91170120239258 = 0.2536611258983612 + 10.0 * 6.265803813934326
Epoch 900, val loss: 1.0050688982009888
Epoch 910, training loss: 62.89468002319336 = 0.2464333027601242 + 10.0 * 6.264824867248535
Epoch 910, val loss: 1.010768175125122
Epoch 920, training loss: 62.92018127441406 = 0.2393600046634674 + 10.0 * 6.268082141876221
Epoch 920, val loss: 1.0165694952011108
Epoch 930, training loss: 62.87086868286133 = 0.2323349267244339 + 10.0 * 6.263853549957275
Epoch 930, val loss: 1.0223333835601807
Epoch 940, training loss: 62.874046325683594 = 0.225502148270607 + 10.0 * 6.264854431152344
Epoch 940, val loss: 1.0283331871032715
Epoch 950, training loss: 62.85868453979492 = 0.21880784630775452 + 10.0 * 6.2639875411987305
Epoch 950, val loss: 1.0343081951141357
Epoch 960, training loss: 62.84200668334961 = 0.21225519478321075 + 10.0 * 6.262975215911865
Epoch 960, val loss: 1.040719747543335
Epoch 970, training loss: 62.83448791503906 = 0.20583222806453705 + 10.0 * 6.2628655433654785
Epoch 970, val loss: 1.0469615459442139
Epoch 980, training loss: 62.82155227661133 = 0.1995762437582016 + 10.0 * 6.262197494506836
Epoch 980, val loss: 1.0531061887741089
Epoch 990, training loss: 62.80754470825195 = 0.19346213340759277 + 10.0 * 6.26140832901001
Epoch 990, val loss: 1.0594741106033325
Epoch 1000, training loss: 62.795249938964844 = 0.18748529255390167 + 10.0 * 6.260776519775391
Epoch 1000, val loss: 1.0661598443984985
Epoch 1010, training loss: 62.76714324951172 = 0.18166983127593994 + 10.0 * 6.258547306060791
Epoch 1010, val loss: 1.0726667642593384
Epoch 1020, training loss: 62.78324890136719 = 0.17601755261421204 + 10.0 * 6.260723114013672
Epoch 1020, val loss: 1.0793123245239258
Epoch 1030, training loss: 62.823272705078125 = 0.17048028111457825 + 10.0 * 6.265279293060303
Epoch 1030, val loss: 1.085463285446167
Epoch 1040, training loss: 62.75462341308594 = 0.1650453358888626 + 10.0 * 6.258957862854004
Epoch 1040, val loss: 1.0926108360290527
Epoch 1050, training loss: 62.719234466552734 = 0.159822016954422 + 10.0 * 6.255941390991211
Epoch 1050, val loss: 1.0991963148117065
Epoch 1060, training loss: 62.71607208251953 = 0.15478357672691345 + 10.0 * 6.256128787994385
Epoch 1060, val loss: 1.106011152267456
Epoch 1070, training loss: 62.75530242919922 = 0.14987018704414368 + 10.0 * 6.260542869567871
Epoch 1070, val loss: 1.1127378940582275
Epoch 1080, training loss: 62.69214630126953 = 0.1450584977865219 + 10.0 * 6.254708766937256
Epoch 1080, val loss: 1.1196820735931396
Epoch 1090, training loss: 62.67933654785156 = 0.14043286442756653 + 10.0 * 6.253890037536621
Epoch 1090, val loss: 1.1267633438110352
Epoch 1100, training loss: 62.67450714111328 = 0.1359873265028 + 10.0 * 6.253851890563965
Epoch 1100, val loss: 1.1338181495666504
Epoch 1110, training loss: 62.7156867980957 = 0.13167913258075714 + 10.0 * 6.258400917053223
Epoch 1110, val loss: 1.1408103704452515
Epoch 1120, training loss: 62.75099563598633 = 0.12748973071575165 + 10.0 * 6.262350559234619
Epoch 1120, val loss: 1.1473639011383057
Epoch 1130, training loss: 62.67048263549805 = 0.1233702152967453 + 10.0 * 6.254711151123047
Epoch 1130, val loss: 1.1548978090286255
Epoch 1140, training loss: 62.6311149597168 = 0.11947562545537949 + 10.0 * 6.251163959503174
Epoch 1140, val loss: 1.1619445085525513
Epoch 1150, training loss: 62.62147903442383 = 0.11571982502937317 + 10.0 * 6.250576019287109
Epoch 1150, val loss: 1.1691995859146118
Epoch 1160, training loss: 62.67518997192383 = 0.11209416389465332 + 10.0 * 6.256309509277344
Epoch 1160, val loss: 1.1766809225082397
Epoch 1170, training loss: 62.64366912841797 = 0.10854172706604004 + 10.0 * 6.253512382507324
Epoch 1170, val loss: 1.1828703880310059
Epoch 1180, training loss: 62.61325454711914 = 0.10514792799949646 + 10.0 * 6.250810623168945
Epoch 1180, val loss: 1.19047212600708
Epoch 1190, training loss: 62.590797424316406 = 0.10188018530607224 + 10.0 * 6.248891830444336
Epoch 1190, val loss: 1.1973671913146973
Epoch 1200, training loss: 62.622459411621094 = 0.09874555468559265 + 10.0 * 6.252371311187744
Epoch 1200, val loss: 1.2045923471450806
Epoch 1210, training loss: 62.583831787109375 = 0.09567619860172272 + 10.0 * 6.248815536499023
Epoch 1210, val loss: 1.2114956378936768
Epoch 1220, training loss: 62.61138153076172 = 0.09272980690002441 + 10.0 * 6.251864910125732
Epoch 1220, val loss: 1.2187238931655884
Epoch 1230, training loss: 62.57175827026367 = 0.08988254517316818 + 10.0 * 6.24818754196167
Epoch 1230, val loss: 1.2257269620895386
Epoch 1240, training loss: 62.56106185913086 = 0.08717231452465057 + 10.0 * 6.24738883972168
Epoch 1240, val loss: 1.2325862646102905
Epoch 1250, training loss: 62.5428352355957 = 0.08454430848360062 + 10.0 * 6.245829105377197
Epoch 1250, val loss: 1.2399311065673828
Epoch 1260, training loss: 62.57832717895508 = 0.08202847838401794 + 10.0 * 6.249629974365234
Epoch 1260, val loss: 1.2469782829284668
Epoch 1270, training loss: 62.552825927734375 = 0.07957301288843155 + 10.0 * 6.2473249435424805
Epoch 1270, val loss: 1.253467082977295
Epoch 1280, training loss: 62.5244026184082 = 0.07719732075929642 + 10.0 * 6.244720458984375
Epoch 1280, val loss: 1.261051893234253
Epoch 1290, training loss: 62.541202545166016 = 0.07492659240961075 + 10.0 * 6.246627330780029
Epoch 1290, val loss: 1.2680606842041016
Epoch 1300, training loss: 62.52246856689453 = 0.07274414598941803 + 10.0 * 6.2449727058410645
Epoch 1300, val loss: 1.2750041484832764
Epoch 1310, training loss: 62.54094696044922 = 0.07062187790870667 + 10.0 * 6.247032642364502
Epoch 1310, val loss: 1.2818377017974854
Epoch 1320, training loss: 62.51250076293945 = 0.06856071203947067 + 10.0 * 6.244393825531006
Epoch 1320, val loss: 1.2889026403427124
Epoch 1330, training loss: 62.49510955810547 = 0.06659107655286789 + 10.0 * 6.242852210998535
Epoch 1330, val loss: 1.2960915565490723
Epoch 1340, training loss: 62.48518753051758 = 0.06471718847751617 + 10.0 * 6.242047309875488
Epoch 1340, val loss: 1.30301833152771
Epoch 1350, training loss: 62.5148811340332 = 0.06290785223245621 + 10.0 * 6.245197296142578
Epoch 1350, val loss: 1.3102333545684814
Epoch 1360, training loss: 62.476016998291016 = 0.061139993369579315 + 10.0 * 6.241487979888916
Epoch 1360, val loss: 1.3167519569396973
Epoch 1370, training loss: 62.5270881652832 = 0.05945321172475815 + 10.0 * 6.246763706207275
Epoch 1370, val loss: 1.3234772682189941
Epoch 1380, training loss: 62.494537353515625 = 0.0577884204685688 + 10.0 * 6.2436747550964355
Epoch 1380, val loss: 1.3300731182098389
Epoch 1390, training loss: 62.46683120727539 = 0.05619483441114426 + 10.0 * 6.241063594818115
Epoch 1390, val loss: 1.3373759984970093
Epoch 1400, training loss: 62.454490661621094 = 0.05468154698610306 + 10.0 * 6.239981174468994
Epoch 1400, val loss: 1.343995451927185
Epoch 1410, training loss: 62.44588851928711 = 0.05322737991809845 + 10.0 * 6.239266395568848
Epoch 1410, val loss: 1.3509329557418823
Epoch 1420, training loss: 62.450653076171875 = 0.05182722583413124 + 10.0 * 6.239882469177246
Epoch 1420, val loss: 1.357564091682434
Epoch 1430, training loss: 62.49052047729492 = 0.05047062039375305 + 10.0 * 6.244004726409912
Epoch 1430, val loss: 1.3639880418777466
Epoch 1440, training loss: 62.505821228027344 = 0.049142464995384216 + 10.0 * 6.245667934417725
Epoch 1440, val loss: 1.3709213733673096
Epoch 1450, training loss: 62.47938537597656 = 0.04784490913152695 + 10.0 * 6.243154048919678
Epoch 1450, val loss: 1.3775639533996582
Epoch 1460, training loss: 62.432464599609375 = 0.046614911407232285 + 10.0 * 6.238584995269775
Epoch 1460, val loss: 1.3838684558868408
Epoch 1470, training loss: 62.422000885009766 = 0.04543549194931984 + 10.0 * 6.237656593322754
Epoch 1470, val loss: 1.3908623456954956
Epoch 1480, training loss: 62.41227722167969 = 0.04430244117975235 + 10.0 * 6.236797332763672
Epoch 1480, val loss: 1.3972108364105225
Epoch 1490, training loss: 62.43198013305664 = 0.04321252927184105 + 10.0 * 6.238876819610596
Epoch 1490, val loss: 1.4035468101501465
Epoch 1500, training loss: 62.418701171875 = 0.042146068066358566 + 10.0 * 6.2376556396484375
Epoch 1500, val loss: 1.409937858581543
Epoch 1510, training loss: 62.45741271972656 = 0.041123662143945694 + 10.0 * 6.241629123687744
Epoch 1510, val loss: 1.416163444519043
Epoch 1520, training loss: 62.427555084228516 = 0.04010600969195366 + 10.0 * 6.238744735717773
Epoch 1520, val loss: 1.4230073690414429
Epoch 1530, training loss: 62.39826965332031 = 0.039138615131378174 + 10.0 * 6.235913276672363
Epoch 1530, val loss: 1.4290647506713867
Epoch 1540, training loss: 62.385108947753906 = 0.03821444511413574 + 10.0 * 6.234689235687256
Epoch 1540, val loss: 1.4354767799377441
Epoch 1550, training loss: 62.385677337646484 = 0.037327855825424194 + 10.0 * 6.234835147857666
Epoch 1550, val loss: 1.441916823387146
Epoch 1560, training loss: 62.46788024902344 = 0.03647151589393616 + 10.0 * 6.243140697479248
Epoch 1560, val loss: 1.447959065437317
Epoch 1570, training loss: 62.42299270629883 = 0.035616252571344376 + 10.0 * 6.2387375831604
Epoch 1570, val loss: 1.4535704851150513
Epoch 1580, training loss: 62.40390396118164 = 0.03479389101266861 + 10.0 * 6.236910820007324
Epoch 1580, val loss: 1.4598451852798462
Epoch 1590, training loss: 62.36847686767578 = 0.03400133550167084 + 10.0 * 6.233447551727295
Epoch 1590, val loss: 1.4660724401474
Epoch 1600, training loss: 62.368038177490234 = 0.03324732556939125 + 10.0 * 6.233479022979736
Epoch 1600, val loss: 1.472203254699707
Epoch 1610, training loss: 62.37500762939453 = 0.03252457082271576 + 10.0 * 6.234248161315918
Epoch 1610, val loss: 1.4781320095062256
Epoch 1620, training loss: 62.444278717041016 = 0.031824640929698944 + 10.0 * 6.241245269775391
Epoch 1620, val loss: 1.4834094047546387
Epoch 1630, training loss: 62.3623046875 = 0.031104043126106262 + 10.0 * 6.233119964599609
Epoch 1630, val loss: 1.4898452758789062
Epoch 1640, training loss: 62.34895324707031 = 0.03043547458946705 + 10.0 * 6.231852054595947
Epoch 1640, val loss: 1.4955016374588013
Epoch 1650, training loss: 62.34505844116211 = 0.02979854680597782 + 10.0 * 6.231525897979736
Epoch 1650, val loss: 1.501216173171997
Epoch 1660, training loss: 62.364341735839844 = 0.029182069003582 + 10.0 * 6.233515739440918
Epoch 1660, val loss: 1.5070284605026245
Epoch 1670, training loss: 62.35145950317383 = 0.028572270646691322 + 10.0 * 6.232288837432861
Epoch 1670, val loss: 1.5127636194229126
Epoch 1680, training loss: 62.35739517211914 = 0.027984926477074623 + 10.0 * 6.232941150665283
Epoch 1680, val loss: 1.518466830253601
Epoch 1690, training loss: 62.364498138427734 = 0.02741522528231144 + 10.0 * 6.233708381652832
Epoch 1690, val loss: 1.5238076448440552
Epoch 1700, training loss: 62.361759185791016 = 0.02685960754752159 + 10.0 * 6.233489990234375
Epoch 1700, val loss: 1.5290577411651611
Epoch 1710, training loss: 62.333988189697266 = 0.0263189896941185 + 10.0 * 6.230767250061035
Epoch 1710, val loss: 1.5349200963974
Epoch 1720, training loss: 62.32941818237305 = 0.025799036026000977 + 10.0 * 6.2303619384765625
Epoch 1720, val loss: 1.5404473543167114
Epoch 1730, training loss: 62.39424133300781 = 0.025298530235886574 + 10.0 * 6.236894130706787
Epoch 1730, val loss: 1.5461734533309937
Epoch 1740, training loss: 62.35270309448242 = 0.02480657957494259 + 10.0 * 6.232789516448975
Epoch 1740, val loss: 1.55111563205719
Epoch 1750, training loss: 62.32562255859375 = 0.024325331673026085 + 10.0 * 6.230129718780518
Epoch 1750, val loss: 1.5562161207199097
Epoch 1760, training loss: 62.32113265991211 = 0.02386930026113987 + 10.0 * 6.229726314544678
Epoch 1760, val loss: 1.5615012645721436
Epoch 1770, training loss: 62.38271713256836 = 0.023427478969097137 + 10.0 * 6.235929012298584
Epoch 1770, val loss: 1.566712498664856
Epoch 1780, training loss: 62.33140563964844 = 0.022973274812102318 + 10.0 * 6.230843544006348
Epoch 1780, val loss: 1.571832299232483
Epoch 1790, training loss: 62.30902099609375 = 0.022553090006113052 + 10.0 * 6.228646755218506
Epoch 1790, val loss: 1.5769965648651123
Epoch 1800, training loss: 62.30162811279297 = 0.02214225009083748 + 10.0 * 6.2279486656188965
Epoch 1800, val loss: 1.5821479558944702
Epoch 1810, training loss: 62.352020263671875 = 0.021751370280981064 + 10.0 * 6.23302698135376
Epoch 1810, val loss: 1.5868809223175049
Epoch 1820, training loss: 62.36561584472656 = 0.02136163040995598 + 10.0 * 6.2344255447387695
Epoch 1820, val loss: 1.5917916297912598
Epoch 1830, training loss: 62.31682205200195 = 0.020964832976460457 + 10.0 * 6.229585647583008
Epoch 1830, val loss: 1.5969449281692505
Epoch 1840, training loss: 62.2969856262207 = 0.020595384761691093 + 10.0 * 6.227639198303223
Epoch 1840, val loss: 1.602098822593689
Epoch 1850, training loss: 62.288414001464844 = 0.020241035148501396 + 10.0 * 6.2268171310424805
Epoch 1850, val loss: 1.6068415641784668
Epoch 1860, training loss: 62.29620361328125 = 0.01989932172000408 + 10.0 * 6.227630615234375
Epoch 1860, val loss: 1.6118286848068237
Epoch 1870, training loss: 62.34416580200195 = 0.01956191658973694 + 10.0 * 6.2324604988098145
Epoch 1870, val loss: 1.6163291931152344
Epoch 1880, training loss: 62.32697677612305 = 0.019217688590288162 + 10.0 * 6.230775833129883
Epoch 1880, val loss: 1.62091064453125
Epoch 1890, training loss: 62.28003692626953 = 0.018893342465162277 + 10.0 * 6.226114273071289
Epoch 1890, val loss: 1.6258900165557861
Epoch 1900, training loss: 62.28232955932617 = 0.018581179901957512 + 10.0 * 6.226374626159668
Epoch 1900, val loss: 1.6306272745132446
Epoch 1910, training loss: 62.31230163574219 = 0.018277056515216827 + 10.0 * 6.229402542114258
Epoch 1910, val loss: 1.6353259086608887
Epoch 1920, training loss: 62.27918243408203 = 0.017975768074393272 + 10.0 * 6.226120948791504
Epoch 1920, val loss: 1.6400302648544312
Epoch 1930, training loss: 62.2878532409668 = 0.01768597960472107 + 10.0 * 6.227016925811768
Epoch 1930, val loss: 1.6447843313217163
Epoch 1940, training loss: 62.27635955810547 = 0.017404617741703987 + 10.0 * 6.225895404815674
Epoch 1940, val loss: 1.6488850116729736
Epoch 1950, training loss: 62.286346435546875 = 0.01713307946920395 + 10.0 * 6.226921558380127
Epoch 1950, val loss: 1.6535753011703491
Epoch 1960, training loss: 62.27478790283203 = 0.01686262898147106 + 10.0 * 6.22579288482666
Epoch 1960, val loss: 1.657551884651184
Epoch 1970, training loss: 62.26864242553711 = 0.01659516617655754 + 10.0 * 6.225204944610596
Epoch 1970, val loss: 1.6619327068328857
Epoch 1980, training loss: 62.26422119140625 = 0.016338752582669258 + 10.0 * 6.224788188934326
Epoch 1980, val loss: 1.6665138006210327
Epoch 1990, training loss: 62.270023345947266 = 0.01608988642692566 + 10.0 * 6.225393295288086
Epoch 1990, val loss: 1.6708837747573853
Epoch 2000, training loss: 62.32653045654297 = 0.015846796333789825 + 10.0 * 6.2310686111450195
Epoch 2000, val loss: 1.6757023334503174
Epoch 2010, training loss: 62.2706413269043 = 0.015605184249579906 + 10.0 * 6.225503444671631
Epoch 2010, val loss: 1.6791154146194458
Epoch 2020, training loss: 62.25143051147461 = 0.015368848107755184 + 10.0 * 6.223606109619141
Epoch 2020, val loss: 1.6837457418441772
Epoch 2030, training loss: 62.265872955322266 = 0.01514528039842844 + 10.0 * 6.225072860717773
Epoch 2030, val loss: 1.6880515813827515
Epoch 2040, training loss: 62.2762336730957 = 0.014922413043677807 + 10.0 * 6.226130962371826
Epoch 2040, val loss: 1.6922155618667603
Epoch 2050, training loss: 62.273216247558594 = 0.01470805425196886 + 10.0 * 6.225850582122803
Epoch 2050, val loss: 1.69625985622406
Epoch 2060, training loss: 62.26223373413086 = 0.014491125009953976 + 10.0 * 6.224774360656738
Epoch 2060, val loss: 1.7004880905151367
Epoch 2070, training loss: 62.25319290161133 = 0.014285936951637268 + 10.0 * 6.223890781402588
Epoch 2070, val loss: 1.7040914297103882
Epoch 2080, training loss: 62.25566101074219 = 0.014082939364016056 + 10.0 * 6.224157810211182
Epoch 2080, val loss: 1.708238124847412
Epoch 2090, training loss: 62.237648010253906 = 0.013886414468288422 + 10.0 * 6.222376346588135
Epoch 2090, val loss: 1.7122979164123535
Epoch 2100, training loss: 62.248531341552734 = 0.01369632501155138 + 10.0 * 6.223483562469482
Epoch 2100, val loss: 1.7159923315048218
Epoch 2110, training loss: 62.243343353271484 = 0.013508213683962822 + 10.0 * 6.222983360290527
Epoch 2110, val loss: 1.7199856042861938
Epoch 2120, training loss: 62.23616027832031 = 0.013321686536073685 + 10.0 * 6.222283840179443
Epoch 2120, val loss: 1.724145770072937
Epoch 2130, training loss: 62.258331298828125 = 0.013147672638297081 + 10.0 * 6.224518299102783
Epoch 2130, val loss: 1.727799415588379
Epoch 2140, training loss: 62.23706817626953 = 0.012967441231012344 + 10.0 * 6.222410202026367
Epoch 2140, val loss: 1.7318440675735474
Epoch 2150, training loss: 62.29487609863281 = 0.012794221751391888 + 10.0 * 6.228208065032959
Epoch 2150, val loss: 1.73519766330719
Epoch 2160, training loss: 62.2311897277832 = 0.012620511464774609 + 10.0 * 6.221857070922852
Epoch 2160, val loss: 1.7395530939102173
Epoch 2170, training loss: 62.2166862487793 = 0.01245503406971693 + 10.0 * 6.220423221588135
Epoch 2170, val loss: 1.7431401014328003
Epoch 2180, training loss: 62.233924865722656 = 0.012297013774514198 + 10.0 * 6.22216272354126
Epoch 2180, val loss: 1.7470194101333618
Epoch 2190, training loss: 62.25977325439453 = 0.012138324789702892 + 10.0 * 6.2247633934021
Epoch 2190, val loss: 1.7502690553665161
Epoch 2200, training loss: 62.22164535522461 = 0.01197828073054552 + 10.0 * 6.220966815948486
Epoch 2200, val loss: 1.7538126707077026
Epoch 2210, training loss: 62.204505920410156 = 0.011823763139545918 + 10.0 * 6.219267845153809
Epoch 2210, val loss: 1.757718801498413
Epoch 2220, training loss: 62.20090866088867 = 0.011677861213684082 + 10.0 * 6.218923091888428
Epoch 2220, val loss: 1.7615514993667603
Epoch 2230, training loss: 62.20424270629883 = 0.011535747908055782 + 10.0 * 6.219270706176758
Epoch 2230, val loss: 1.765329122543335
Epoch 2240, training loss: 62.24873733520508 = 0.011396045796573162 + 10.0 * 6.223734378814697
Epoch 2240, val loss: 1.7691367864608765
Epoch 2250, training loss: 62.21267318725586 = 0.011254321783781052 + 10.0 * 6.220141887664795
Epoch 2250, val loss: 1.7719202041625977
Epoch 2260, training loss: 62.27381134033203 = 0.011114893481135368 + 10.0 * 6.226269721984863
Epoch 2260, val loss: 1.7755707502365112
Epoch 2270, training loss: 62.20769500732422 = 0.010978063568472862 + 10.0 * 6.219671726226807
Epoch 2270, val loss: 1.778536319732666
Epoch 2280, training loss: 62.192848205566406 = 0.010844615288078785 + 10.0 * 6.218200206756592
Epoch 2280, val loss: 1.7823519706726074
Epoch 2290, training loss: 62.20744323730469 = 0.010718164034187794 + 10.0 * 6.219672203063965
Epoch 2290, val loss: 1.7857259511947632
Epoch 2300, training loss: 62.21459197998047 = 0.010593952611088753 + 10.0 * 6.220399856567383
Epoch 2300, val loss: 1.7890112400054932
Epoch 2310, training loss: 62.191654205322266 = 0.01047030184417963 + 10.0 * 6.218118190765381
Epoch 2310, val loss: 1.7925564050674438
Epoch 2320, training loss: 62.183982849121094 = 0.010351378470659256 + 10.0 * 6.217362880706787
Epoch 2320, val loss: 1.7958210706710815
Epoch 2330, training loss: 62.213985443115234 = 0.010234933346509933 + 10.0 * 6.220375061035156
Epoch 2330, val loss: 1.7995147705078125
Epoch 2340, training loss: 62.220115661621094 = 0.010114427655935287 + 10.0 * 6.2210001945495605
Epoch 2340, val loss: 1.8024215698242188
Epoch 2350, training loss: 62.18521499633789 = 0.009994774125516415 + 10.0 * 6.217522144317627
Epoch 2350, val loss: 1.8053851127624512
Epoch 2360, training loss: 62.184242248535156 = 0.009882853366434574 + 10.0 * 6.217435836791992
Epoch 2360, val loss: 1.808555245399475
Epoch 2370, training loss: 62.17504119873047 = 0.009774977341294289 + 10.0 * 6.216526985168457
Epoch 2370, val loss: 1.8120988607406616
Epoch 2380, training loss: 62.197853088378906 = 0.009672446176409721 + 10.0 * 6.218817710876465
Epoch 2380, val loss: 1.8151572942733765
Epoch 2390, training loss: 62.220001220703125 = 0.009566756896674633 + 10.0 * 6.221043586730957
Epoch 2390, val loss: 1.8178129196166992
Epoch 2400, training loss: 62.17981719970703 = 0.009453725069761276 + 10.0 * 6.217036247253418
Epoch 2400, val loss: 1.8213658332824707
Epoch 2410, training loss: 62.17506408691406 = 0.009351923130452633 + 10.0 * 6.216570854187012
Epoch 2410, val loss: 1.8244624137878418
Epoch 2420, training loss: 62.19326400756836 = 0.009253699332475662 + 10.0 * 6.218400955200195
Epoch 2420, val loss: 1.827528715133667
Epoch 2430, training loss: 62.1873664855957 = 0.009155355393886566 + 10.0 * 6.21782112121582
Epoch 2430, val loss: 1.8303927183151245
Epoch 2440, training loss: 62.19881057739258 = 0.009060229174792767 + 10.0 * 6.218975067138672
Epoch 2440, val loss: 1.8334453105926514
Epoch 2450, training loss: 62.17788314819336 = 0.00896323099732399 + 10.0 * 6.216891765594482
Epoch 2450, val loss: 1.8365780115127563
Epoch 2460, training loss: 62.18391799926758 = 0.008870578370988369 + 10.0 * 6.217504978179932
Epoch 2460, val loss: 1.8394393920898438
Epoch 2470, training loss: 62.20964050292969 = 0.008783412165939808 + 10.0 * 6.220085620880127
Epoch 2470, val loss: 1.8420466184616089
Epoch 2480, training loss: 62.177825927734375 = 0.008689864538609982 + 10.0 * 6.21691370010376
Epoch 2480, val loss: 1.8455294370651245
Epoch 2490, training loss: 62.15226364135742 = 0.008600297383964062 + 10.0 * 6.2143659591674805
Epoch 2490, val loss: 1.8484437465667725
Epoch 2500, training loss: 62.16309356689453 = 0.008516195230185986 + 10.0 * 6.215457916259766
Epoch 2500, val loss: 1.851418137550354
Epoch 2510, training loss: 62.196720123291016 = 0.008432742208242416 + 10.0 * 6.2188286781311035
Epoch 2510, val loss: 1.8542487621307373
Epoch 2520, training loss: 62.17924880981445 = 0.008350089192390442 + 10.0 * 6.217089653015137
Epoch 2520, val loss: 1.8568037748336792
Epoch 2530, training loss: 62.17833709716797 = 0.008266771212220192 + 10.0 * 6.217007160186768
Epoch 2530, val loss: 1.8593194484710693
Epoch 2540, training loss: 62.15884017944336 = 0.008184281177818775 + 10.0 * 6.215065956115723
Epoch 2540, val loss: 1.8624205589294434
Epoch 2550, training loss: 62.14850997924805 = 0.00810289941728115 + 10.0 * 6.214040756225586
Epoch 2550, val loss: 1.8655717372894287
Epoch 2560, training loss: 62.16115188598633 = 0.00802688766270876 + 10.0 * 6.215312480926514
Epoch 2560, val loss: 1.8682969808578491
Epoch 2570, training loss: 62.185054779052734 = 0.007952624000608921 + 10.0 * 6.217710018157959
Epoch 2570, val loss: 1.870689868927002
Epoch 2580, training loss: 62.171878814697266 = 0.007876893505454063 + 10.0 * 6.216400146484375
Epoch 2580, val loss: 1.8733845949172974
Epoch 2590, training loss: 62.1484260559082 = 0.0078001259826123714 + 10.0 * 6.214062690734863
Epoch 2590, val loss: 1.876504898071289
Epoch 2600, training loss: 62.16073989868164 = 0.007728605065494776 + 10.0 * 6.215301036834717
Epoch 2600, val loss: 1.8788484334945679
Epoch 2610, training loss: 62.185054779052734 = 0.007656867615878582 + 10.0 * 6.217740058898926
Epoch 2610, val loss: 1.8814551830291748
Epoch 2620, training loss: 62.16376495361328 = 0.0075838384218513966 + 10.0 * 6.215618133544922
Epoch 2620, val loss: 1.8841279745101929
Epoch 2630, training loss: 62.14118194580078 = 0.0075137559324502945 + 10.0 * 6.213366508483887
Epoch 2630, val loss: 1.8866745233535767
Epoch 2640, training loss: 62.139400482177734 = 0.007448270451277494 + 10.0 * 6.213194847106934
Epoch 2640, val loss: 1.8892278671264648
Epoch 2650, training loss: 62.18989181518555 = 0.007384605705738068 + 10.0 * 6.218250751495361
Epoch 2650, val loss: 1.8916865587234497
Epoch 2660, training loss: 62.12961196899414 = 0.007313895039260387 + 10.0 * 6.2122297286987305
Epoch 2660, val loss: 1.8947304487228394
Epoch 2670, training loss: 62.12934875488281 = 0.007250498048961163 + 10.0 * 6.212209701538086
Epoch 2670, val loss: 1.8974483013153076
Epoch 2680, training loss: 62.14847183227539 = 0.007190254516899586 + 10.0 * 6.214128017425537
Epoch 2680, val loss: 1.899984359741211
Epoch 2690, training loss: 62.17716979980469 = 0.007126456592231989 + 10.0 * 6.217004299163818
Epoch 2690, val loss: 1.9022027254104614
Epoch 2700, training loss: 62.163047790527344 = 0.007061016745865345 + 10.0 * 6.2155985832214355
Epoch 2700, val loss: 1.904298186302185
Epoch 2710, training loss: 62.134056091308594 = 0.006997405551373959 + 10.0 * 6.212706089019775
Epoch 2710, val loss: 1.9071300029754639
Epoch 2720, training loss: 62.119651794433594 = 0.006938791833817959 + 10.0 * 6.211271286010742
Epoch 2720, val loss: 1.9096087217330933
Epoch 2730, training loss: 62.11707305908203 = 0.006883000023663044 + 10.0 * 6.211019039154053
Epoch 2730, val loss: 1.9118597507476807
Epoch 2740, training loss: 62.15681838989258 = 0.006830711383372545 + 10.0 * 6.214998722076416
Epoch 2740, val loss: 1.9140620231628418
Epoch 2750, training loss: 62.127994537353516 = 0.006769842468202114 + 10.0 * 6.212122440338135
Epoch 2750, val loss: 1.916534662246704
Epoch 2760, training loss: 62.15420913696289 = 0.006710950285196304 + 10.0 * 6.214749813079834
Epoch 2760, val loss: 1.9191523790359497
Epoch 2770, training loss: 62.142601013183594 = 0.0066582257859408855 + 10.0 * 6.213594436645508
Epoch 2770, val loss: 1.92112135887146
Epoch 2780, training loss: 62.14838790893555 = 0.006601993925869465 + 10.0 * 6.214178562164307
Epoch 2780, val loss: 1.9238039255142212
Epoch 2790, training loss: 62.12752151489258 = 0.006549025420099497 + 10.0 * 6.21209716796875
Epoch 2790, val loss: 1.9258430004119873
Epoch 2800, training loss: 62.149417877197266 = 0.006495742127299309 + 10.0 * 6.214292049407959
Epoch 2800, val loss: 1.928411841392517
Epoch 2810, training loss: 62.15056610107422 = 0.00644476804882288 + 10.0 * 6.214412212371826
Epoch 2810, val loss: 1.9310264587402344
Epoch 2820, training loss: 62.11443328857422 = 0.006391076371073723 + 10.0 * 6.210804462432861
Epoch 2820, val loss: 1.932745337486267
Epoch 2830, training loss: 62.12540054321289 = 0.006342535372823477 + 10.0 * 6.211905479431152
Epoch 2830, val loss: 1.9350619316101074
Epoch 2840, training loss: 62.15110778808594 = 0.006293673533946276 + 10.0 * 6.214481353759766
Epoch 2840, val loss: 1.9374667406082153
Epoch 2850, training loss: 62.122989654541016 = 0.006243168842047453 + 10.0 * 6.211674690246582
Epoch 2850, val loss: 1.939305067062378
Epoch 2860, training loss: 62.109195709228516 = 0.006195995956659317 + 10.0 * 6.210299968719482
Epoch 2860, val loss: 1.9415547847747803
Epoch 2870, training loss: 62.13739776611328 = 0.006149741355329752 + 10.0 * 6.213124752044678
Epoch 2870, val loss: 1.9435914754867554
Epoch 2880, training loss: 62.109622955322266 = 0.006102107465267181 + 10.0 * 6.210351943969727
Epoch 2880, val loss: 1.9459152221679688
Epoch 2890, training loss: 62.108089447021484 = 0.006056871265172958 + 10.0 * 6.210203170776367
Epoch 2890, val loss: 1.9485446214675903
Epoch 2900, training loss: 62.159812927246094 = 0.006012304220348597 + 10.0 * 6.2153801918029785
Epoch 2900, val loss: 1.9507769346237183
Epoch 2910, training loss: 62.16633605957031 = 0.0059656561352312565 + 10.0 * 6.216036796569824
Epoch 2910, val loss: 1.9523383378982544
Epoch 2920, training loss: 62.11692810058594 = 0.005920072551816702 + 10.0 * 6.2111005783081055
Epoch 2920, val loss: 1.9544743299484253
Epoch 2930, training loss: 62.09943771362305 = 0.005876312498003244 + 10.0 * 6.209356307983398
Epoch 2930, val loss: 1.9563707113265991
Epoch 2940, training loss: 62.09798812866211 = 0.005836123134940863 + 10.0 * 6.20921516418457
Epoch 2940, val loss: 1.9586061239242554
Epoch 2950, training loss: 62.12648010253906 = 0.0057966867461800575 + 10.0 * 6.2120680809021
Epoch 2950, val loss: 1.9604613780975342
Epoch 2960, training loss: 62.10521697998047 = 0.005754516460001469 + 10.0 * 6.209946155548096
Epoch 2960, val loss: 1.9626281261444092
Epoch 2970, training loss: 62.13663864135742 = 0.005712790880352259 + 10.0 * 6.21309232711792
Epoch 2970, val loss: 1.964910626411438
Epoch 2980, training loss: 62.15985107421875 = 0.005671336781233549 + 10.0 * 6.215417861938477
Epoch 2980, val loss: 1.9664143323898315
Epoch 2990, training loss: 62.112911224365234 = 0.005629909224808216 + 10.0 * 6.210728168487549
Epoch 2990, val loss: 1.9685451984405518
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 87.92433166503906 = 1.9556257724761963 + 10.0 * 8.596870422363281
Epoch 0, val loss: 1.9472898244857788
Epoch 10, training loss: 87.91010284423828 = 1.9452791213989258 + 10.0 * 8.596482276916504
Epoch 10, val loss: 1.9374206066131592
Epoch 20, training loss: 87.86687469482422 = 1.9322658777236938 + 10.0 * 8.593461036682129
Epoch 20, val loss: 1.9246505498886108
Epoch 30, training loss: 87.61922454833984 = 1.915075421333313 + 10.0 * 8.570414543151855
Epoch 30, val loss: 1.9076424837112427
Epoch 40, training loss: 86.10562896728516 = 1.8934093713760376 + 10.0 * 8.421221733093262
Epoch 40, val loss: 1.8868283033370972
Epoch 50, training loss: 81.03805541992188 = 1.869438886642456 + 10.0 * 7.9168620109558105
Epoch 50, val loss: 1.8642901182174683
Epoch 60, training loss: 76.50260162353516 = 1.85420823097229 + 10.0 * 7.464839458465576
Epoch 60, val loss: 1.8515900373458862
Epoch 70, training loss: 72.6407241821289 = 1.843764066696167 + 10.0 * 7.079695701599121
Epoch 70, val loss: 1.8420851230621338
Epoch 80, training loss: 70.94343566894531 = 1.8346939086914062 + 10.0 * 6.910873889923096
Epoch 80, val loss: 1.8333237171173096
Epoch 90, training loss: 70.0277328491211 = 1.823360800743103 + 10.0 * 6.820436954498291
Epoch 90, val loss: 1.8230987787246704
Epoch 100, training loss: 69.37015533447266 = 1.8112726211547852 + 10.0 * 6.755887985229492
Epoch 100, val loss: 1.8128116130828857
Epoch 110, training loss: 68.7947769165039 = 1.7998913526535034 + 10.0 * 6.699489116668701
Epoch 110, val loss: 1.803403377532959
Epoch 120, training loss: 68.381103515625 = 1.7892605066299438 + 10.0 * 6.659184455871582
Epoch 120, val loss: 1.794566035270691
Epoch 130, training loss: 68.04188537597656 = 1.7785593271255493 + 10.0 * 6.626332759857178
Epoch 130, val loss: 1.785689353942871
Epoch 140, training loss: 67.7378921508789 = 1.7673468589782715 + 10.0 * 6.5970540046691895
Epoch 140, val loss: 1.7766386270523071
Epoch 150, training loss: 67.4896011352539 = 1.7554833889007568 + 10.0 * 6.573411464691162
Epoch 150, val loss: 1.7672663927078247
Epoch 160, training loss: 67.22666931152344 = 1.7429035902023315 + 10.0 * 6.548376560211182
Epoch 160, val loss: 1.7573943138122559
Epoch 170, training loss: 67.01931762695312 = 1.7293524742126465 + 10.0 * 6.528996467590332
Epoch 170, val loss: 1.7468876838684082
Epoch 180, training loss: 66.82839965820312 = 1.7145856618881226 + 10.0 * 6.511381149291992
Epoch 180, val loss: 1.7355321645736694
Epoch 190, training loss: 66.64907836914062 = 1.69855535030365 + 10.0 * 6.495052337646484
Epoch 190, val loss: 1.7232255935668945
Epoch 200, training loss: 66.51531219482422 = 1.6810104846954346 + 10.0 * 6.483430862426758
Epoch 200, val loss: 1.7099014520645142
Epoch 210, training loss: 66.36138916015625 = 1.661919355392456 + 10.0 * 6.469947338104248
Epoch 210, val loss: 1.6954931020736694
Epoch 220, training loss: 66.22866821289062 = 1.6412248611450195 + 10.0 * 6.458744049072266
Epoch 220, val loss: 1.6798999309539795
Epoch 230, training loss: 66.10972595214844 = 1.618908405303955 + 10.0 * 6.4490814208984375
Epoch 230, val loss: 1.6632120609283447
Epoch 240, training loss: 66.01126861572266 = 1.5950099229812622 + 10.0 * 6.441625595092773
Epoch 240, val loss: 1.6454249620437622
Epoch 250, training loss: 65.9066390991211 = 1.5694687366485596 + 10.0 * 6.433717727661133
Epoch 250, val loss: 1.6265555620193481
Epoch 260, training loss: 65.81017303466797 = 1.5426121950149536 + 10.0 * 6.426756381988525
Epoch 260, val loss: 1.6069576740264893
Epoch 270, training loss: 65.7301025390625 = 1.5144497156143188 + 10.0 * 6.421565532684326
Epoch 270, val loss: 1.586638331413269
Epoch 280, training loss: 65.64913177490234 = 1.4853813648223877 + 10.0 * 6.416375160217285
Epoch 280, val loss: 1.5658857822418213
Epoch 290, training loss: 65.56021881103516 = 1.4554259777069092 + 10.0 * 6.4104790687561035
Epoch 290, val loss: 1.544756293296814
Epoch 300, training loss: 65.47264862060547 = 1.4249945878982544 + 10.0 * 6.404765605926514
Epoch 300, val loss: 1.523710012435913
Epoch 310, training loss: 65.39177703857422 = 1.394200325012207 + 10.0 * 6.399757385253906
Epoch 310, val loss: 1.5026930570602417
Epoch 320, training loss: 65.31810760498047 = 1.363188624382019 + 10.0 * 6.395491600036621
Epoch 320, val loss: 1.4818623065948486
Epoch 330, training loss: 65.26625061035156 = 1.3320766687393188 + 10.0 * 6.3934173583984375
Epoch 330, val loss: 1.4614111185073853
Epoch 340, training loss: 65.18419647216797 = 1.3009806871414185 + 10.0 * 6.388321399688721
Epoch 340, val loss: 1.4411134719848633
Epoch 350, training loss: 65.09684753417969 = 1.2701973915100098 + 10.0 * 6.382665157318115
Epoch 350, val loss: 1.4214658737182617
Epoch 360, training loss: 65.10611724853516 = 1.2396103143692017 + 10.0 * 6.386651039123535
Epoch 360, val loss: 1.4022198915481567
Epoch 370, training loss: 64.979736328125 = 1.2091771364212036 + 10.0 * 6.377055644989014
Epoch 370, val loss: 1.3831108808517456
Epoch 380, training loss: 64.90753173828125 = 1.1791272163391113 + 10.0 * 6.372840404510498
Epoch 380, val loss: 1.3645352125167847
Epoch 390, training loss: 64.84921264648438 = 1.1493691205978394 + 10.0 * 6.369984149932861
Epoch 390, val loss: 1.346284031867981
Epoch 400, training loss: 64.77816772460938 = 1.1199872493743896 + 10.0 * 6.365818500518799
Epoch 400, val loss: 1.328526496887207
Epoch 410, training loss: 64.72018432617188 = 1.09099280834198 + 10.0 * 6.362918853759766
Epoch 410, val loss: 1.3110426664352417
Epoch 420, training loss: 64.66384887695312 = 1.06236732006073 + 10.0 * 6.360147953033447
Epoch 420, val loss: 1.2938822507858276
Epoch 430, training loss: 64.67219543457031 = 1.0341570377349854 + 10.0 * 6.363803386688232
Epoch 430, val loss: 1.2772794961929321
Epoch 440, training loss: 64.54561614990234 = 1.0063802003860474 + 10.0 * 6.353923797607422
Epoch 440, val loss: 1.2608041763305664
Epoch 450, training loss: 64.47644805908203 = 0.9793183207511902 + 10.0 * 6.349713325500488
Epoch 450, val loss: 1.2449215650558472
Epoch 460, training loss: 64.42131042480469 = 0.9527969360351562 + 10.0 * 6.346851348876953
Epoch 460, val loss: 1.2295891046524048
Epoch 470, training loss: 64.40186309814453 = 0.9267916083335876 + 10.0 * 6.347506999969482
Epoch 470, val loss: 1.2147058248519897
Epoch 480, training loss: 64.3735122680664 = 0.9014182090759277 + 10.0 * 6.3472089767456055
Epoch 480, val loss: 1.20040762424469
Epoch 490, training loss: 64.28292083740234 = 0.8766971230506897 + 10.0 * 6.340622425079346
Epoch 490, val loss: 1.1864959001541138
Epoch 500, training loss: 64.25272369384766 = 0.8527590036392212 + 10.0 * 6.339996337890625
Epoch 500, val loss: 1.173250436782837
Epoch 510, training loss: 64.17774200439453 = 0.8294881582260132 + 10.0 * 6.334825038909912
Epoch 510, val loss: 1.1608763933181763
Epoch 520, training loss: 64.13256072998047 = 0.8069702982902527 + 10.0 * 6.332559108734131
Epoch 520, val loss: 1.1488913297653198
Epoch 530, training loss: 64.08735656738281 = 0.7850770354270935 + 10.0 * 6.330227851867676
Epoch 530, val loss: 1.13741135597229
Epoch 540, training loss: 64.03170013427734 = 0.763791561126709 + 10.0 * 6.326790809631348
Epoch 540, val loss: 1.1266837120056152
Epoch 550, training loss: 64.0083999633789 = 0.7432161569595337 + 10.0 * 6.3265180587768555
Epoch 550, val loss: 1.1165014505386353
Epoch 560, training loss: 63.958274841308594 = 0.7231893539428711 + 10.0 * 6.323508262634277
Epoch 560, val loss: 1.1066454648971558
Epoch 570, training loss: 63.914371490478516 = 0.7038026452064514 + 10.0 * 6.321056842803955
Epoch 570, val loss: 1.0976769924163818
Epoch 580, training loss: 63.89505386352539 = 0.684927761554718 + 10.0 * 6.321012496948242
Epoch 580, val loss: 1.0888938903808594
Epoch 590, training loss: 63.84270477294922 = 0.6665119528770447 + 10.0 * 6.317619323730469
Epoch 590, val loss: 1.0806139707565308
Epoch 600, training loss: 63.80240249633789 = 0.6486058235168457 + 10.0 * 6.315379619598389
Epoch 600, val loss: 1.0727444887161255
Epoch 610, training loss: 63.76801300048828 = 0.6311482787132263 + 10.0 * 6.313686370849609
Epoch 610, val loss: 1.06545889377594
Epoch 620, training loss: 63.73622512817383 = 0.6141224503517151 + 10.0 * 6.3122100830078125
Epoch 620, val loss: 1.0582275390625
Epoch 630, training loss: 63.688560485839844 = 0.5975462198257446 + 10.0 * 6.309101581573486
Epoch 630, val loss: 1.0515716075897217
Epoch 640, training loss: 63.72737121582031 = 0.5813159346580505 + 10.0 * 6.314605712890625
Epoch 640, val loss: 1.045151948928833
Epoch 650, training loss: 63.63959503173828 = 0.5653161406517029 + 10.0 * 6.307427883148193
Epoch 650, val loss: 1.0388935804367065
Epoch 660, training loss: 63.591896057128906 = 0.5497357845306396 + 10.0 * 6.304215908050537
Epoch 660, val loss: 1.0329781770706177
Epoch 670, training loss: 63.557979583740234 = 0.5345035791397095 + 10.0 * 6.302347660064697
Epoch 670, val loss: 1.0274513959884644
Epoch 680, training loss: 63.64128112792969 = 0.5194894671440125 + 10.0 * 6.312179088592529
Epoch 680, val loss: 1.0222539901733398
Epoch 690, training loss: 63.50453567504883 = 0.504666805267334 + 10.0 * 6.299986839294434
Epoch 690, val loss: 1.0169053077697754
Epoch 700, training loss: 63.49224853515625 = 0.4901181757450104 + 10.0 * 6.300212860107422
Epoch 700, val loss: 1.011956810951233
Epoch 710, training loss: 63.451412200927734 = 0.47579556703567505 + 10.0 * 6.2975616455078125
Epoch 710, val loss: 1.0072648525238037
Epoch 720, training loss: 63.450748443603516 = 0.46169623732566833 + 10.0 * 6.298905372619629
Epoch 720, val loss: 1.002810001373291
Epoch 730, training loss: 63.400699615478516 = 0.44778886437416077 + 10.0 * 6.295290946960449
Epoch 730, val loss: 0.9983730912208557
Epoch 740, training loss: 63.36000061035156 = 0.43409624695777893 + 10.0 * 6.292590141296387
Epoch 740, val loss: 0.9942788481712341
Epoch 750, training loss: 63.38526153564453 = 0.4205787777900696 + 10.0 * 6.296468257904053
Epoch 750, val loss: 0.9901561737060547
Epoch 760, training loss: 63.334983825683594 = 0.40729373693466187 + 10.0 * 6.292768955230713
Epoch 760, val loss: 0.986526608467102
Epoch 770, training loss: 63.30231857299805 = 0.3940984010696411 + 10.0 * 6.2908220291137695
Epoch 770, val loss: 0.9826592206954956
Epoch 780, training loss: 63.296356201171875 = 0.38122373819351196 + 10.0 * 6.291512966156006
Epoch 780, val loss: 0.9790962338447571
Epoch 790, training loss: 63.24493408203125 = 0.3684951961040497 + 10.0 * 6.287643909454346
Epoch 790, val loss: 0.9759172201156616
Epoch 800, training loss: 63.21470260620117 = 0.3560202717781067 + 10.0 * 6.285868167877197
Epoch 800, val loss: 0.9727997779846191
Epoch 810, training loss: 63.227378845214844 = 0.3437507152557373 + 10.0 * 6.288362979888916
Epoch 810, val loss: 0.9698507785797119
Epoch 820, training loss: 63.22863006591797 = 0.33166828751564026 + 10.0 * 6.289696216583252
Epoch 820, val loss: 0.9674352407455444
Epoch 830, training loss: 63.15422821044922 = 0.3198719620704651 + 10.0 * 6.283435344696045
Epoch 830, val loss: 0.9645262360572815
Epoch 840, training loss: 63.11708068847656 = 0.3083898723125458 + 10.0 * 6.280869007110596
Epoch 840, val loss: 0.9623921513557434
Epoch 850, training loss: 63.10205078125 = 0.29721277952194214 + 10.0 * 6.280483722686768
Epoch 850, val loss: 0.960594892501831
Epoch 860, training loss: 63.1373405456543 = 0.2863257825374603 + 10.0 * 6.285101413726807
Epoch 860, val loss: 0.9587761759757996
Epoch 870, training loss: 63.0812873840332 = 0.2756703794002533 + 10.0 * 6.280561923980713
Epoch 870, val loss: 0.9574944376945496
Epoch 880, training loss: 63.081180572509766 = 0.26541367173194885 + 10.0 * 6.281576633453369
Epoch 880, val loss: 0.9561829566955566
Epoch 890, training loss: 63.02108383178711 = 0.25539523363113403 + 10.0 * 6.27656888961792
Epoch 890, val loss: 0.9552584290504456
Epoch 900, training loss: 63.0088005065918 = 0.24576134979724884 + 10.0 * 6.276303768157959
Epoch 900, val loss: 0.9546872973442078
Epoch 910, training loss: 63.05345153808594 = 0.23650480806827545 + 10.0 * 6.2816948890686035
Epoch 910, val loss: 0.9543174505233765
Epoch 920, training loss: 63.010929107666016 = 0.2274966537952423 + 10.0 * 6.278343200683594
Epoch 920, val loss: 0.9540207386016846
Epoch 930, training loss: 62.96982955932617 = 0.21886160969734192 + 10.0 * 6.275096893310547
Epoch 930, val loss: 0.9543113708496094
Epoch 940, training loss: 62.948333740234375 = 0.21056921780109406 + 10.0 * 6.273776531219482
Epoch 940, val loss: 0.9546358585357666
Epoch 950, training loss: 62.94126510620117 = 0.20262065529823303 + 10.0 * 6.273864269256592
Epoch 950, val loss: 0.9552672505378723
Epoch 960, training loss: 62.90669250488281 = 0.19500111043453217 + 10.0 * 6.271169185638428
Epoch 960, val loss: 0.9562473893165588
Epoch 970, training loss: 62.925140380859375 = 0.1876990795135498 + 10.0 * 6.273744106292725
Epoch 970, val loss: 0.9573939442634583
Epoch 980, training loss: 62.90042495727539 = 0.1807066947221756 + 10.0 * 6.271971702575684
Epoch 980, val loss: 0.9585782885551453
Epoch 990, training loss: 62.892974853515625 = 0.1740175187587738 + 10.0 * 6.271895408630371
Epoch 990, val loss: 0.9604669809341431
Epoch 1000, training loss: 62.85858917236328 = 0.1676321178674698 + 10.0 * 6.269095420837402
Epoch 1000, val loss: 0.9620362520217896
Epoch 1010, training loss: 62.855350494384766 = 0.1615423858165741 + 10.0 * 6.269381046295166
Epoch 1010, val loss: 0.9641925692558289
Epoch 1020, training loss: 62.87147521972656 = 0.1557120978832245 + 10.0 * 6.271576404571533
Epoch 1020, val loss: 0.9665348529815674
Epoch 1030, training loss: 62.82959747314453 = 0.15015572309494019 + 10.0 * 6.2679443359375
Epoch 1030, val loss: 0.9685009717941284
Epoch 1040, training loss: 62.814476013183594 = 0.14485549926757812 + 10.0 * 6.266962051391602
Epoch 1040, val loss: 0.9714548587799072
Epoch 1050, training loss: 62.818031311035156 = 0.13984327018260956 + 10.0 * 6.267818927764893
Epoch 1050, val loss: 0.9739522337913513
Epoch 1060, training loss: 62.770851135253906 = 0.1350071132183075 + 10.0 * 6.263584613800049
Epoch 1060, val loss: 0.9770236611366272
Epoch 1070, training loss: 62.7716064453125 = 0.13043931126594543 + 10.0 * 6.2641167640686035
Epoch 1070, val loss: 0.9800183176994324
Epoch 1080, training loss: 62.78195571899414 = 0.12606847286224365 + 10.0 * 6.265588760375977
Epoch 1080, val loss: 0.9833255410194397
Epoch 1090, training loss: 62.780452728271484 = 0.12189654260873795 + 10.0 * 6.26585578918457
Epoch 1090, val loss: 0.9865725040435791
Epoch 1100, training loss: 62.74878692626953 = 0.11787386238574982 + 10.0 * 6.263091087341309
Epoch 1100, val loss: 0.9897809028625488
Epoch 1110, training loss: 62.75516128540039 = 0.11407581716775894 + 10.0 * 6.264108657836914
Epoch 1110, val loss: 0.9936041831970215
Epoch 1120, training loss: 62.73816680908203 = 0.11041006445884705 + 10.0 * 6.262775897979736
Epoch 1120, val loss: 0.9968774914741516
Epoch 1130, training loss: 62.707733154296875 = 0.10694196820259094 + 10.0 * 6.260079383850098
Epoch 1130, val loss: 1.0007104873657227
Epoch 1140, training loss: 62.69344711303711 = 0.10362731665372849 + 10.0 * 6.258982181549072
Epoch 1140, val loss: 1.0045443773269653
Epoch 1150, training loss: 62.69042205810547 = 0.10047151893377304 + 10.0 * 6.258995056152344
Epoch 1150, val loss: 1.0084021091461182
Epoch 1160, training loss: 62.73298263549805 = 0.09742848575115204 + 10.0 * 6.263555526733398
Epoch 1160, val loss: 1.0125550031661987
Epoch 1170, training loss: 62.70835494995117 = 0.0944875031709671 + 10.0 * 6.261386871337891
Epoch 1170, val loss: 1.0160648822784424
Epoch 1180, training loss: 62.66899871826172 = 0.0916806161403656 + 10.0 * 6.2577314376831055
Epoch 1180, val loss: 1.0200353860855103
Epoch 1190, training loss: 62.67264938354492 = 0.08899211883544922 + 10.0 * 6.258365631103516
Epoch 1190, val loss: 1.0241867303848267
Epoch 1200, training loss: 62.67232894897461 = 0.08641897141933441 + 10.0 * 6.258591175079346
Epoch 1200, val loss: 1.0281970500946045
Epoch 1210, training loss: 62.66322708129883 = 0.0839482769370079 + 10.0 * 6.257927894592285
Epoch 1210, val loss: 1.0325261354446411
Epoch 1220, training loss: 62.627994537353516 = 0.08157242834568024 + 10.0 * 6.254642009735107
Epoch 1220, val loss: 1.0364959239959717
Epoch 1230, training loss: 62.644657135009766 = 0.07931656390428543 + 10.0 * 6.256534099578857
Epoch 1230, val loss: 1.0406125783920288
Epoch 1240, training loss: 62.68471145629883 = 0.07710099220275879 + 10.0 * 6.26076078414917
Epoch 1240, val loss: 1.0448756217956543
Epoch 1250, training loss: 62.621280670166016 = 0.07497890293598175 + 10.0 * 6.254630088806152
Epoch 1250, val loss: 1.048876166343689
Epoch 1260, training loss: 62.609615325927734 = 0.0729406327009201 + 10.0 * 6.25366735458374
Epoch 1260, val loss: 1.0533366203308105
Epoch 1270, training loss: 62.594539642333984 = 0.07101073861122131 + 10.0 * 6.252352714538574
Epoch 1270, val loss: 1.0575406551361084
Epoch 1280, training loss: 62.593143463134766 = 0.06914196163415909 + 10.0 * 6.2524003982543945
Epoch 1280, val loss: 1.0618953704833984
Epoch 1290, training loss: 62.6802864074707 = 0.06733956187963486 + 10.0 * 6.261294364929199
Epoch 1290, val loss: 1.065919041633606
Epoch 1300, training loss: 62.57426834106445 = 0.06554082781076431 + 10.0 * 6.250872611999512
Epoch 1300, val loss: 1.0699042081832886
Epoch 1310, training loss: 62.57862091064453 = 0.0638536885380745 + 10.0 * 6.251476764678955
Epoch 1310, val loss: 1.074483871459961
Epoch 1320, training loss: 62.56044387817383 = 0.06224244832992554 + 10.0 * 6.249820232391357
Epoch 1320, val loss: 1.078757405281067
Epoch 1330, training loss: 62.61034393310547 = 0.06067938730120659 + 10.0 * 6.2549662590026855
Epoch 1330, val loss: 1.0829739570617676
Epoch 1340, training loss: 62.5851936340332 = 0.05916309729218483 + 10.0 * 6.252603054046631
Epoch 1340, val loss: 1.0872730016708374
Epoch 1350, training loss: 62.562843322753906 = 0.057674113661050797 + 10.0 * 6.250516891479492
Epoch 1350, val loss: 1.0914956331253052
Epoch 1360, training loss: 62.53990936279297 = 0.0562719963490963 + 10.0 * 6.248363971710205
Epoch 1360, val loss: 1.0959211587905884
Epoch 1370, training loss: 62.55277633666992 = 0.05491593852639198 + 10.0 * 6.249785900115967
Epoch 1370, val loss: 1.1003090143203735
Epoch 1380, training loss: 62.5772705078125 = 0.053586263209581375 + 10.0 * 6.252368450164795
Epoch 1380, val loss: 1.1043870449066162
Epoch 1390, training loss: 62.52256774902344 = 0.05227736756205559 + 10.0 * 6.2470293045043945
Epoch 1390, val loss: 1.108739972114563
Epoch 1400, training loss: 62.51325225830078 = 0.05103342607617378 + 10.0 * 6.246222019195557
Epoch 1400, val loss: 1.1128530502319336
Epoch 1410, training loss: 62.50958251953125 = 0.0498444139957428 + 10.0 * 6.245974063873291
Epoch 1410, val loss: 1.117302417755127
Epoch 1420, training loss: 62.50950241088867 = 0.04869457334280014 + 10.0 * 6.2460808753967285
Epoch 1420, val loss: 1.1214475631713867
Epoch 1430, training loss: 62.59197998046875 = 0.04758189618587494 + 10.0 * 6.254439830780029
Epoch 1430, val loss: 1.125376582145691
Epoch 1440, training loss: 62.53070068359375 = 0.046456899493932724 + 10.0 * 6.248424530029297
Epoch 1440, val loss: 1.1297848224639893
Epoch 1450, training loss: 62.50546646118164 = 0.045388419181108475 + 10.0 * 6.246007919311523
Epoch 1450, val loss: 1.1342206001281738
Epoch 1460, training loss: 62.4907341003418 = 0.044372040778398514 + 10.0 * 6.244636058807373
Epoch 1460, val loss: 1.1381815671920776
Epoch 1470, training loss: 62.48774719238281 = 0.04338378459215164 + 10.0 * 6.244436264038086
Epoch 1470, val loss: 1.1425668001174927
Epoch 1480, training loss: 62.522727966308594 = 0.04242725670337677 + 10.0 * 6.248030185699463
Epoch 1480, val loss: 1.1465169191360474
Epoch 1490, training loss: 62.51424026489258 = 0.04147695004940033 + 10.0 * 6.247276306152344
Epoch 1490, val loss: 1.1506847143173218
Epoch 1500, training loss: 62.484642028808594 = 0.0405627004802227 + 10.0 * 6.244408130645752
Epoch 1500, val loss: 1.1548036336898804
Epoch 1510, training loss: 62.502220153808594 = 0.03968144580721855 + 10.0 * 6.246253967285156
Epoch 1510, val loss: 1.1588811874389648
Epoch 1520, training loss: 62.52070617675781 = 0.03882421553134918 + 10.0 * 6.248188018798828
Epoch 1520, val loss: 1.162760853767395
Epoch 1530, training loss: 62.464508056640625 = 0.03797657787799835 + 10.0 * 6.2426533699035645
Epoch 1530, val loss: 1.1669210195541382
Epoch 1540, training loss: 62.45184326171875 = 0.03716950863599777 + 10.0 * 6.241467475891113
Epoch 1540, val loss: 1.170932412147522
Epoch 1550, training loss: 62.44266891479492 = 0.03639507293701172 + 10.0 * 6.240627288818359
Epoch 1550, val loss: 1.1751126050949097
Epoch 1560, training loss: 62.44294738769531 = 0.03564289212226868 + 10.0 * 6.240730285644531
Epoch 1560, val loss: 1.1792188882827759
Epoch 1570, training loss: 62.509483337402344 = 0.03492075204849243 + 10.0 * 6.2474565505981445
Epoch 1570, val loss: 1.183264970779419
Epoch 1580, training loss: 62.46090316772461 = 0.03417515009641647 + 10.0 * 6.242672920227051
Epoch 1580, val loss: 1.1868234872817993
Epoch 1590, training loss: 62.45353698730469 = 0.033476877957582474 + 10.0 * 6.242005825042725
Epoch 1590, val loss: 1.1909587383270264
Epoch 1600, training loss: 62.483333587646484 = 0.03279587998986244 + 10.0 * 6.245053768157959
Epoch 1600, val loss: 1.1946454048156738
Epoch 1610, training loss: 62.43825149536133 = 0.03212936222553253 + 10.0 * 6.240612030029297
Epoch 1610, val loss: 1.1985725164413452
Epoch 1620, training loss: 62.41688537597656 = 0.031482163816690445 + 10.0 * 6.238540172576904
Epoch 1620, val loss: 1.2024723291397095
Epoch 1630, training loss: 62.42798614501953 = 0.03086564689874649 + 10.0 * 6.239712238311768
Epoch 1630, val loss: 1.206529140472412
Epoch 1640, training loss: 62.49507522583008 = 0.030261605978012085 + 10.0 * 6.246481418609619
Epoch 1640, val loss: 1.2102696895599365
Epoch 1650, training loss: 62.477569580078125 = 0.02965988777577877 + 10.0 * 6.244791030883789
Epoch 1650, val loss: 1.2138473987579346
Epoch 1660, training loss: 62.418212890625 = 0.02906554378569126 + 10.0 * 6.238914966583252
Epoch 1660, val loss: 1.2175588607788086
Epoch 1670, training loss: 62.40081787109375 = 0.028513560071587563 + 10.0 * 6.23723030090332
Epoch 1670, val loss: 1.2213414907455444
Epoch 1680, training loss: 62.39402770996094 = 0.02797696366906166 + 10.0 * 6.236605167388916
Epoch 1680, val loss: 1.2252367734909058
Epoch 1690, training loss: 62.390445709228516 = 0.02745462767779827 + 10.0 * 6.23629903793335
Epoch 1690, val loss: 1.2289936542510986
Epoch 1700, training loss: 62.457786560058594 = 0.026947058737277985 + 10.0 * 6.243083953857422
Epoch 1700, val loss: 1.232977032661438
Epoch 1710, training loss: 62.400367736816406 = 0.02644304372370243 + 10.0 * 6.237392425537109
Epoch 1710, val loss: 1.2359272241592407
Epoch 1720, training loss: 62.409183502197266 = 0.02595582976937294 + 10.0 * 6.238322734832764
Epoch 1720, val loss: 1.2398561239242554
Epoch 1730, training loss: 62.397499084472656 = 0.025478366762399673 + 10.0 * 6.237202167510986
Epoch 1730, val loss: 1.2432774305343628
Epoch 1740, training loss: 62.38370132446289 = 0.02500455267727375 + 10.0 * 6.235869407653809
Epoch 1740, val loss: 1.2468777894973755
Epoch 1750, training loss: 62.44172668457031 = 0.024567358195781708 + 10.0 * 6.241715908050537
Epoch 1750, val loss: 1.2504854202270508
Epoch 1760, training loss: 62.37621307373047 = 0.024113032966852188 + 10.0 * 6.235209941864014
Epoch 1760, val loss: 1.2540154457092285
Epoch 1770, training loss: 62.36608123779297 = 0.023683514446020126 + 10.0 * 6.2342400550842285
Epoch 1770, val loss: 1.2575788497924805
Epoch 1780, training loss: 62.35880661010742 = 0.02327439934015274 + 10.0 * 6.233553409576416
Epoch 1780, val loss: 1.2611217498779297
Epoch 1790, training loss: 62.3697395324707 = 0.022876441478729248 + 10.0 * 6.234686374664307
Epoch 1790, val loss: 1.2648143768310547
Epoch 1800, training loss: 62.418724060058594 = 0.02248455584049225 + 10.0 * 6.2396240234375
Epoch 1800, val loss: 1.2680824995040894
Epoch 1810, training loss: 62.35747528076172 = 0.02208016626536846 + 10.0 * 6.233539581298828
Epoch 1810, val loss: 1.2713489532470703
Epoch 1820, training loss: 62.34873962402344 = 0.021707134321331978 + 10.0 * 6.23270320892334
Epoch 1820, val loss: 1.2749764919281006
Epoch 1830, training loss: 62.404640197753906 = 0.021349580958485603 + 10.0 * 6.23832893371582
Epoch 1830, val loss: 1.2784513235092163
Epoch 1840, training loss: 62.35050582885742 = 0.0209824126213789 + 10.0 * 6.23295259475708
Epoch 1840, val loss: 1.281617522239685
Epoch 1850, training loss: 62.34084701538086 = 0.02063567191362381 + 10.0 * 6.232020854949951
Epoch 1850, val loss: 1.284955382347107
Epoch 1860, training loss: 62.340118408203125 = 0.020301422104239464 + 10.0 * 6.2319817543029785
Epoch 1860, val loss: 1.288581371307373
Epoch 1870, training loss: 62.47010803222656 = 0.019987333565950394 + 10.0 * 6.245011806488037
Epoch 1870, val loss: 1.2917073965072632
Epoch 1880, training loss: 62.37005615234375 = 0.01962994411587715 + 10.0 * 6.235042572021484
Epoch 1880, val loss: 1.2947847843170166
Epoch 1890, training loss: 62.33213806152344 = 0.019317559897899628 + 10.0 * 6.2312822341918945
Epoch 1890, val loss: 1.298079490661621
Epoch 1900, training loss: 62.340389251708984 = 0.019012026488780975 + 10.0 * 6.232137680053711
Epoch 1900, val loss: 1.3014233112335205
Epoch 1910, training loss: 62.34659194946289 = 0.018710823729634285 + 10.0 * 6.2327880859375
Epoch 1910, val loss: 1.3046926259994507
Epoch 1920, training loss: 62.32499313354492 = 0.01841668225824833 + 10.0 * 6.230657577514648
Epoch 1920, val loss: 1.308025598526001
Epoch 1930, training loss: 62.342308044433594 = 0.018130531534552574 + 10.0 * 6.232417583465576
Epoch 1930, val loss: 1.31123948097229
Epoch 1940, training loss: 62.326934814453125 = 0.017852040007710457 + 10.0 * 6.230908393859863
Epoch 1940, val loss: 1.3141974210739136
Epoch 1950, training loss: 62.32579040527344 = 0.017577694728970528 + 10.0 * 6.230821132659912
Epoch 1950, val loss: 1.3173627853393555
Epoch 1960, training loss: 62.39085006713867 = 0.017309479415416718 + 10.0 * 6.237353801727295
Epoch 1960, val loss: 1.320430874824524
Epoch 1970, training loss: 62.35736083984375 = 0.01704319193959236 + 10.0 * 6.234031677246094
Epoch 1970, val loss: 1.3233191967010498
Epoch 1980, training loss: 62.32337188720703 = 0.016782602295279503 + 10.0 * 6.230659008026123
Epoch 1980, val loss: 1.3264061212539673
Epoch 1990, training loss: 62.30900192260742 = 0.016536453738808632 + 10.0 * 6.229246616363525
Epoch 1990, val loss: 1.3295412063598633
Epoch 2000, training loss: 62.33662414550781 = 0.01629576086997986 + 10.0 * 6.232032775878906
Epoch 2000, val loss: 1.3324542045593262
Epoch 2010, training loss: 62.31482696533203 = 0.01605565845966339 + 10.0 * 6.22987699508667
Epoch 2010, val loss: 1.3355334997177124
Epoch 2020, training loss: 62.31180953979492 = 0.015817638486623764 + 10.0 * 6.229599475860596
Epoch 2020, val loss: 1.3386203050613403
Epoch 2030, training loss: 62.31692123413086 = 0.0155905457213521 + 10.0 * 6.230133056640625
Epoch 2030, val loss: 1.3414018154144287
Epoch 2040, training loss: 62.30527877807617 = 0.015368986874818802 + 10.0 * 6.2289910316467285
Epoch 2040, val loss: 1.3445854187011719
Epoch 2050, training loss: 62.29206848144531 = 0.01515121292322874 + 10.0 * 6.227691650390625
Epoch 2050, val loss: 1.3476014137268066
Epoch 2060, training loss: 62.3415412902832 = 0.014941412955522537 + 10.0 * 6.232659816741943
Epoch 2060, val loss: 1.3506317138671875
Epoch 2070, training loss: 62.31984329223633 = 0.01472880132496357 + 10.0 * 6.230511665344238
Epoch 2070, val loss: 1.3532003164291382
Epoch 2080, training loss: 62.28912353515625 = 0.014517109841108322 + 10.0 * 6.2274603843688965
Epoch 2080, val loss: 1.356155514717102
Epoch 2090, training loss: 62.28204345703125 = 0.014318113215267658 + 10.0 * 6.226772785186768
Epoch 2090, val loss: 1.3589650392532349
Epoch 2100, training loss: 62.3375358581543 = 0.01412439439445734 + 10.0 * 6.2323408126831055
Epoch 2100, val loss: 1.361902117729187
Epoch 2110, training loss: 62.29241180419922 = 0.013933691196143627 + 10.0 * 6.227847576141357
Epoch 2110, val loss: 1.3643696308135986
Epoch 2120, training loss: 62.27293395996094 = 0.013741469010710716 + 10.0 * 6.225919246673584
Epoch 2120, val loss: 1.3672641515731812
Epoch 2130, training loss: 62.272125244140625 = 0.013558778911828995 + 10.0 * 6.225856781005859
Epoch 2130, val loss: 1.3699768781661987
Epoch 2140, training loss: 62.31879425048828 = 0.013385005295276642 + 10.0 * 6.230540752410889
Epoch 2140, val loss: 1.3725522756576538
Epoch 2150, training loss: 62.29069519042969 = 0.013203345239162445 + 10.0 * 6.227749347686768
Epoch 2150, val loss: 1.3759117126464844
Epoch 2160, training loss: 62.291534423828125 = 0.013030176982283592 + 10.0 * 6.227850437164307
Epoch 2160, val loss: 1.3780649900436401
Epoch 2170, training loss: 62.28529357910156 = 0.0128551060333848 + 10.0 * 6.227243900299072
Epoch 2170, val loss: 1.3808395862579346
Epoch 2180, training loss: 62.25828170776367 = 0.012692702002823353 + 10.0 * 6.2245588302612305
Epoch 2180, val loss: 1.3836414813995361
Epoch 2190, training loss: 62.265987396240234 = 0.012534123845398426 + 10.0 * 6.225345134735107
Epoch 2190, val loss: 1.3863847255706787
Epoch 2200, training loss: 62.3004150390625 = 0.012376283295452595 + 10.0 * 6.228804111480713
Epoch 2200, val loss: 1.3887170553207397
Epoch 2210, training loss: 62.262691497802734 = 0.012212926521897316 + 10.0 * 6.225047588348389
Epoch 2210, val loss: 1.3912947177886963
Epoch 2220, training loss: 62.25521469116211 = 0.012062333524227142 + 10.0 * 6.224315166473389
Epoch 2220, val loss: 1.3940144777297974
Epoch 2230, training loss: 62.28314208984375 = 0.011915038339793682 + 10.0 * 6.227122783660889
Epoch 2230, val loss: 1.3965967893600464
Epoch 2240, training loss: 62.24095916748047 = 0.011762747541069984 + 10.0 * 6.222919464111328
Epoch 2240, val loss: 1.3992022275924683
Epoch 2250, training loss: 62.267974853515625 = 0.011620732955634594 + 10.0 * 6.225635528564453
Epoch 2250, val loss: 1.4017308950424194
Epoch 2260, training loss: 62.28303146362305 = 0.011478960514068604 + 10.0 * 6.2271552085876465
Epoch 2260, val loss: 1.4041998386383057
Epoch 2270, training loss: 62.330116271972656 = 0.011338070966303349 + 10.0 * 6.23187780380249
Epoch 2270, val loss: 1.4066522121429443
Epoch 2280, training loss: 62.25526809692383 = 0.011198082007467747 + 10.0 * 6.224406719207764
Epoch 2280, val loss: 1.4087088108062744
Epoch 2290, training loss: 62.23775863647461 = 0.011064904741942883 + 10.0 * 6.2226691246032715
Epoch 2290, val loss: 1.411635398864746
Epoch 2300, training loss: 62.22836685180664 = 0.010936941020190716 + 10.0 * 6.221743106842041
Epoch 2300, val loss: 1.4140214920043945
Epoch 2310, training loss: 62.22855758666992 = 0.01081131212413311 + 10.0 * 6.221774578094482
Epoch 2310, val loss: 1.4164636135101318
Epoch 2320, training loss: 62.292930603027344 = 0.010688835754990578 + 10.0 * 6.228224277496338
Epoch 2320, val loss: 1.4185152053833008
Epoch 2330, training loss: 62.237831115722656 = 0.010563494637608528 + 10.0 * 6.222726821899414
Epoch 2330, val loss: 1.4211193323135376
Epoch 2340, training loss: 62.22444534301758 = 0.010439172387123108 + 10.0 * 6.221400260925293
Epoch 2340, val loss: 1.4233286380767822
Epoch 2350, training loss: 62.22703552246094 = 0.010321633890271187 + 10.0 * 6.221671104431152
Epoch 2350, val loss: 1.4258354902267456
Epoch 2360, training loss: 62.26958465576172 = 0.010208336636424065 + 10.0 * 6.225937843322754
Epoch 2360, val loss: 1.427987813949585
Epoch 2370, training loss: 62.22182083129883 = 0.010090368799865246 + 10.0 * 6.221173286437988
Epoch 2370, val loss: 1.4305589199066162
Epoch 2380, training loss: 62.25556182861328 = 0.009981327690184116 + 10.0 * 6.224557876586914
Epoch 2380, val loss: 1.432830810546875
Epoch 2390, training loss: 62.24394607543945 = 0.009868193417787552 + 10.0 * 6.223407745361328
Epoch 2390, val loss: 1.4348268508911133
Epoch 2400, training loss: 62.23580551147461 = 0.00975803378969431 + 10.0 * 6.222604751586914
Epoch 2400, val loss: 1.4371707439422607
Epoch 2410, training loss: 62.263797760009766 = 0.009652895852923393 + 10.0 * 6.225414752960205
Epoch 2410, val loss: 1.4394015073776245
Epoch 2420, training loss: 62.22051239013672 = 0.009544135071337223 + 10.0 * 6.221096992492676
Epoch 2420, val loss: 1.4419370889663696
Epoch 2430, training loss: 62.20485305786133 = 0.009441573172807693 + 10.0 * 6.219541072845459
Epoch 2430, val loss: 1.443911075592041
Epoch 2440, training loss: 62.20610427856445 = 0.009342706762254238 + 10.0 * 6.2196760177612305
Epoch 2440, val loss: 1.446109414100647
Epoch 2450, training loss: 62.228450775146484 = 0.0092467050999403 + 10.0 * 6.221920490264893
Epoch 2450, val loss: 1.448553204536438
Epoch 2460, training loss: 62.22745132446289 = 0.009147187694907188 + 10.0 * 6.221830368041992
Epoch 2460, val loss: 1.4504432678222656
Epoch 2470, training loss: 62.219661712646484 = 0.009048921056091785 + 10.0 * 6.2210612297058105
Epoch 2470, val loss: 1.4526375532150269
Epoch 2480, training loss: 62.21693420410156 = 0.0089556984603405 + 10.0 * 6.220797538757324
Epoch 2480, val loss: 1.4549068212509155
Epoch 2490, training loss: 62.248756408691406 = 0.008863282389938831 + 10.0 * 6.223989486694336
Epoch 2490, val loss: 1.4569751024246216
Epoch 2500, training loss: 62.266693115234375 = 0.008774885907769203 + 10.0 * 6.225791931152344
Epoch 2500, val loss: 1.4590808153152466
Epoch 2510, training loss: 62.21060562133789 = 0.008681103587150574 + 10.0 * 6.2201924324035645
Epoch 2510, val loss: 1.4610204696655273
Epoch 2520, training loss: 62.23512268066406 = 0.008597251959145069 + 10.0 * 6.222652435302734
Epoch 2520, val loss: 1.463218092918396
Epoch 2530, training loss: 62.192195892333984 = 0.008505532518029213 + 10.0 * 6.218369007110596
Epoch 2530, val loss: 1.4653728008270264
Epoch 2540, training loss: 62.19856643676758 = 0.008420709520578384 + 10.0 * 6.219014644622803
Epoch 2540, val loss: 1.4674063920974731
Epoch 2550, training loss: 62.194969177246094 = 0.008340432308614254 + 10.0 * 6.218663215637207
Epoch 2550, val loss: 1.4695923328399658
Epoch 2560, training loss: 62.203765869140625 = 0.008259841240942478 + 10.0 * 6.219550609588623
Epoch 2560, val loss: 1.4716299772262573
Epoch 2570, training loss: 62.214691162109375 = 0.008179044350981712 + 10.0 * 6.220651149749756
Epoch 2570, val loss: 1.4733712673187256
Epoch 2580, training loss: 62.2387580871582 = 0.008100041188299656 + 10.0 * 6.2230658531188965
Epoch 2580, val loss: 1.4750992059707642
Epoch 2590, training loss: 62.203651428222656 = 0.008017349056899548 + 10.0 * 6.2195634841918945
Epoch 2590, val loss: 1.4773786067962646
Epoch 2600, training loss: 62.216976165771484 = 0.007941486313939095 + 10.0 * 6.220903396606445
Epoch 2600, val loss: 1.478934407234192
Epoch 2610, training loss: 62.18275833129883 = 0.00786542147397995 + 10.0 * 6.217489242553711
Epoch 2610, val loss: 1.4815349578857422
Epoch 2620, training loss: 62.227901458740234 = 0.00779314711689949 + 10.0 * 6.222010612487793
Epoch 2620, val loss: 1.4832653999328613
Epoch 2630, training loss: 62.183841705322266 = 0.007717934437096119 + 10.0 * 6.217612266540527
Epoch 2630, val loss: 1.484877347946167
Epoch 2640, training loss: 62.17056655883789 = 0.007646720856428146 + 10.0 * 6.216291904449463
Epoch 2640, val loss: 1.486960530281067
Epoch 2650, training loss: 62.19233703613281 = 0.00757778063416481 + 10.0 * 6.218476295471191
Epoch 2650, val loss: 1.4889978170394897
Epoch 2660, training loss: 62.204044342041016 = 0.00750727578997612 + 10.0 * 6.219653606414795
Epoch 2660, val loss: 1.4906386137008667
Epoch 2670, training loss: 62.20985794067383 = 0.007440755609422922 + 10.0 * 6.220241546630859
Epoch 2670, val loss: 1.4923946857452393
Epoch 2680, training loss: 62.204959869384766 = 0.0073707215487957 + 10.0 * 6.219758987426758
Epoch 2680, val loss: 1.4944225549697876
Epoch 2690, training loss: 62.18671798706055 = 0.007304931990802288 + 10.0 * 6.2179412841796875
Epoch 2690, val loss: 1.4958709478378296
Epoch 2700, training loss: 62.174922943115234 = 0.0072382353246212006 + 10.0 * 6.216768741607666
Epoch 2700, val loss: 1.4981366395950317
Epoch 2710, training loss: 62.17927169799805 = 0.0071754553355276585 + 10.0 * 6.217209815979004
Epoch 2710, val loss: 1.4997570514678955
Epoch 2720, training loss: 62.19681930541992 = 0.007112054154276848 + 10.0 * 6.218970775604248
Epoch 2720, val loss: 1.5015063285827637
Epoch 2730, training loss: 62.1838264465332 = 0.00704667204990983 + 10.0 * 6.217678070068359
Epoch 2730, val loss: 1.5032386779785156
Epoch 2740, training loss: 62.18721389770508 = 0.006984720937907696 + 10.0 * 6.21802282333374
Epoch 2740, val loss: 1.5051661729812622
Epoch 2750, training loss: 62.15578842163086 = 0.006925168912857771 + 10.0 * 6.214886665344238
Epoch 2750, val loss: 1.5067553520202637
Epoch 2760, training loss: 62.16162109375 = 0.0068671428598463535 + 10.0 * 6.215475559234619
Epoch 2760, val loss: 1.5086448192596436
Epoch 2770, training loss: 62.1777229309082 = 0.006809362210333347 + 10.0 * 6.2170915603637695
Epoch 2770, val loss: 1.5105842351913452
Epoch 2780, training loss: 62.171600341796875 = 0.0067501855082809925 + 10.0 * 6.216485023498535
Epoch 2780, val loss: 1.5120640993118286
Epoch 2790, training loss: 62.20173645019531 = 0.006691850256174803 + 10.0 * 6.219504356384277
Epoch 2790, val loss: 1.513764500617981
Epoch 2800, training loss: 62.1695442199707 = 0.006634431425482035 + 10.0 * 6.2162909507751465
Epoch 2800, val loss: 1.5149897336959839
Epoch 2810, training loss: 62.152809143066406 = 0.006576974876224995 + 10.0 * 6.21462345123291
Epoch 2810, val loss: 1.5172984600067139
Epoch 2820, training loss: 62.15839385986328 = 0.006524624302983284 + 10.0 * 6.215187072753906
Epoch 2820, val loss: 1.5189112424850464
Epoch 2830, training loss: 62.183868408203125 = 0.006472114007920027 + 10.0 * 6.217739582061768
Epoch 2830, val loss: 1.5206806659698486
Epoch 2840, training loss: 62.150184631347656 = 0.0064177424646914005 + 10.0 * 6.214376926422119
Epoch 2840, val loss: 1.521935224533081
Epoch 2850, training loss: 62.18021011352539 = 0.0063667600043118 + 10.0 * 6.217384338378906
Epoch 2850, val loss: 1.523828387260437
Epoch 2860, training loss: 62.14803695678711 = 0.00631289929151535 + 10.0 * 6.21417236328125
Epoch 2860, val loss: 1.5253009796142578
Epoch 2870, training loss: 62.172969818115234 = 0.006264247931540012 + 10.0 * 6.216670513153076
Epoch 2870, val loss: 1.5267583131790161
Epoch 2880, training loss: 62.18606185913086 = 0.006213858257979155 + 10.0 * 6.217984676361084
Epoch 2880, val loss: 1.5284054279327393
Epoch 2890, training loss: 62.149112701416016 = 0.006160349119454622 + 10.0 * 6.214295387268066
Epoch 2890, val loss: 1.529987096786499
Epoch 2900, training loss: 62.12975311279297 = 0.006112837232649326 + 10.0 * 6.212364196777344
Epoch 2900, val loss: 1.5317703485488892
Epoch 2910, training loss: 62.15169143676758 = 0.006067962385714054 + 10.0 * 6.21456241607666
Epoch 2910, val loss: 1.5331394672393799
Epoch 2920, training loss: 62.17365264892578 = 0.00601998483762145 + 10.0 * 6.216763496398926
Epoch 2920, val loss: 1.5344651937484741
Epoch 2930, training loss: 62.15003967285156 = 0.005972833372652531 + 10.0 * 6.214406490325928
Epoch 2930, val loss: 1.5363894701004028
Epoch 2940, training loss: 62.17387008666992 = 0.005930210929363966 + 10.0 * 6.216794013977051
Epoch 2940, val loss: 1.5378848314285278
Epoch 2950, training loss: 62.1426887512207 = 0.005880990065634251 + 10.0 * 6.213680744171143
Epoch 2950, val loss: 1.5393091440200806
Epoch 2960, training loss: 62.149600982666016 = 0.005835595540702343 + 10.0 * 6.214376449584961
Epoch 2960, val loss: 1.540812373161316
Epoch 2970, training loss: 62.15692138671875 = 0.005792350508272648 + 10.0 * 6.215113162994385
Epoch 2970, val loss: 1.5424656867980957
Epoch 2980, training loss: 62.13081741333008 = 0.005746862851083279 + 10.0 * 6.2125067710876465
Epoch 2980, val loss: 1.543617606163025
Epoch 2990, training loss: 62.126014709472656 = 0.005705798044800758 + 10.0 * 6.21203088760376
Epoch 2990, val loss: 1.5454283952713013
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.816025303110174
The final CL Acc:0.72222, 0.02636, The final GNN Acc:0.81708, 0.00075
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13232])
remove edge: torch.Size([2, 7992])
updated graph: torch.Size([2, 10668])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.9134521484375 = 1.9450534582138062 + 10.0 * 8.596839904785156
Epoch 0, val loss: 1.9365112781524658
Epoch 10, training loss: 87.89552307128906 = 1.9347431659698486 + 10.0 * 8.596077919006348
Epoch 10, val loss: 1.9258034229278564
Epoch 20, training loss: 87.82193756103516 = 1.9224947690963745 + 10.0 * 8.589944839477539
Epoch 20, val loss: 1.91266930103302
Epoch 30, training loss: 87.37613677978516 = 1.9073044061660767 + 10.0 * 8.546883583068848
Epoch 30, val loss: 1.8962881565093994
Epoch 40, training loss: 84.68516540527344 = 1.8892110586166382 + 10.0 * 8.279595375061035
Epoch 40, val loss: 1.8772366046905518
Epoch 50, training loss: 78.25188446044922 = 1.8683868646621704 + 10.0 * 7.638350009918213
Epoch 50, val loss: 1.8557095527648926
Epoch 60, training loss: 75.68204498291016 = 1.8529466390609741 + 10.0 * 7.382910251617432
Epoch 60, val loss: 1.8410735130310059
Epoch 70, training loss: 73.48560333251953 = 1.8397328853607178 + 10.0 * 7.164587497711182
Epoch 70, val loss: 1.8282203674316406
Epoch 80, training loss: 71.73191833496094 = 1.8261924982070923 + 10.0 * 6.990572452545166
Epoch 80, val loss: 1.8152092695236206
Epoch 90, training loss: 70.98152160644531 = 1.814803957939148 + 10.0 * 6.9166717529296875
Epoch 90, val loss: 1.8043071031570435
Epoch 100, training loss: 70.22286224365234 = 1.8024320602416992 + 10.0 * 6.842043399810791
Epoch 100, val loss: 1.7928540706634521
Epoch 110, training loss: 69.40293884277344 = 1.7914879322052002 + 10.0 * 6.761144638061523
Epoch 110, val loss: 1.783227562904358
Epoch 120, training loss: 68.70346069335938 = 1.7814581394195557 + 10.0 * 6.692200183868408
Epoch 120, val loss: 1.7738845348358154
Epoch 130, training loss: 68.25240325927734 = 1.770452857017517 + 10.0 * 6.648194789886475
Epoch 130, val loss: 1.7637759447097778
Epoch 140, training loss: 67.89688110351562 = 1.758358120918274 + 10.0 * 6.613851547241211
Epoch 140, val loss: 1.7526353597640991
Epoch 150, training loss: 67.62564086914062 = 1.7457915544509888 + 10.0 * 6.587985515594482
Epoch 150, val loss: 1.7414016723632812
Epoch 160, training loss: 67.42686462402344 = 1.7324334383010864 + 10.0 * 6.5694427490234375
Epoch 160, val loss: 1.7293550968170166
Epoch 170, training loss: 67.21394348144531 = 1.717976689338684 + 10.0 * 6.549596309661865
Epoch 170, val loss: 1.7163169384002686
Epoch 180, training loss: 67.03167724609375 = 1.7023005485534668 + 10.0 * 6.532937526702881
Epoch 180, val loss: 1.7023414373397827
Epoch 190, training loss: 66.9131088256836 = 1.685531497001648 + 10.0 * 6.5227580070495605
Epoch 190, val loss: 1.687248706817627
Epoch 200, training loss: 66.71766662597656 = 1.6672393083572388 + 10.0 * 6.505043029785156
Epoch 200, val loss: 1.671143889427185
Epoch 210, training loss: 66.57122039794922 = 1.647904634475708 + 10.0 * 6.492331504821777
Epoch 210, val loss: 1.654068946838379
Epoch 220, training loss: 66.4550552368164 = 1.627345085144043 + 10.0 * 6.482771396636963
Epoch 220, val loss: 1.6359702348709106
Epoch 230, training loss: 66.28756713867188 = 1.6053272485733032 + 10.0 * 6.468224048614502
Epoch 230, val loss: 1.616729497909546
Epoch 240, training loss: 66.13529968261719 = 1.5821373462677002 + 10.0 * 6.455316066741943
Epoch 240, val loss: 1.5965774059295654
Epoch 250, training loss: 66.06429290771484 = 1.5576897859573364 + 10.0 * 6.450660228729248
Epoch 250, val loss: 1.5754375457763672
Epoch 260, training loss: 65.8970947265625 = 1.5319476127624512 + 10.0 * 6.4365153312683105
Epoch 260, val loss: 1.5533764362335205
Epoch 270, training loss: 65.77278900146484 = 1.5053844451904297 + 10.0 * 6.4267401695251465
Epoch 270, val loss: 1.5307234525680542
Epoch 280, training loss: 65.6767578125 = 1.4778733253479004 + 10.0 * 6.419888496398926
Epoch 280, val loss: 1.5074725151062012
Epoch 290, training loss: 65.56244659423828 = 1.4493460655212402 + 10.0 * 6.411310195922852
Epoch 290, val loss: 1.4837441444396973
Epoch 300, training loss: 65.47176361083984 = 1.4202449321746826 + 10.0 * 6.405152320861816
Epoch 300, val loss: 1.4596670866012573
Epoch 310, training loss: 65.37494659423828 = 1.3902872800827026 + 10.0 * 6.398466110229492
Epoch 310, val loss: 1.4353564977645874
Epoch 320, training loss: 65.29379272460938 = 1.3599166870117188 + 10.0 * 6.393387794494629
Epoch 320, val loss: 1.4108999967575073
Epoch 330, training loss: 65.19585418701172 = 1.3288604021072388 + 10.0 * 6.386699676513672
Epoch 330, val loss: 1.3863818645477295
Epoch 340, training loss: 65.11307525634766 = 1.2975579500198364 + 10.0 * 6.381551742553711
Epoch 340, val loss: 1.3619768619537354
Epoch 350, training loss: 65.03129577636719 = 1.2658299207687378 + 10.0 * 6.376546382904053
Epoch 350, val loss: 1.3374831676483154
Epoch 360, training loss: 64.93305969238281 = 1.2339844703674316 + 10.0 * 6.369907855987549
Epoch 360, val loss: 1.3133795261383057
Epoch 370, training loss: 64.88341522216797 = 1.202022910118103 + 10.0 * 6.368139266967773
Epoch 370, val loss: 1.2895550727844238
Epoch 380, training loss: 64.82872772216797 = 1.1699681282043457 + 10.0 * 6.365876197814941
Epoch 380, val loss: 1.2659080028533936
Epoch 390, training loss: 64.72369384765625 = 1.138286828994751 + 10.0 * 6.3585405349731445
Epoch 390, val loss: 1.242965817451477
Epoch 400, training loss: 64.64437866210938 = 1.1069341897964478 + 10.0 * 6.353744029998779
Epoch 400, val loss: 1.2206686735153198
Epoch 410, training loss: 64.63899230957031 = 1.0761845111846924 + 10.0 * 6.35628080368042
Epoch 410, val loss: 1.199177622795105
Epoch 420, training loss: 64.52100372314453 = 1.0456936359405518 + 10.0 * 6.347531318664551
Epoch 420, val loss: 1.1782306432724
Epoch 430, training loss: 64.45613098144531 = 1.0162744522094727 + 10.0 * 6.343985557556152
Epoch 430, val loss: 1.1584455966949463
Epoch 440, training loss: 64.38958740234375 = 0.9877572059631348 + 10.0 * 6.340182781219482
Epoch 440, val loss: 1.1397836208343506
Epoch 450, training loss: 64.33472442626953 = 0.9600659608840942 + 10.0 * 6.337465763092041
Epoch 450, val loss: 1.1220979690551758
Epoch 460, training loss: 64.27507781982422 = 0.9334387183189392 + 10.0 * 6.334163665771484
Epoch 460, val loss: 1.105552077293396
Epoch 470, training loss: 64.23564910888672 = 0.9077332615852356 + 10.0 * 6.332791805267334
Epoch 470, val loss: 1.0901083946228027
Epoch 480, training loss: 64.16529846191406 = 0.88292396068573 + 10.0 * 6.328237533569336
Epoch 480, val loss: 1.0754766464233398
Epoch 490, training loss: 64.1128158569336 = 0.8590861558914185 + 10.0 * 6.325372695922852
Epoch 490, val loss: 1.061762809753418
Epoch 500, training loss: 64.0852279663086 = 0.8361012935638428 + 10.0 * 6.324913024902344
Epoch 500, val loss: 1.0489981174468994
Epoch 510, training loss: 64.04744720458984 = 0.8140382766723633 + 10.0 * 6.323340892791748
Epoch 510, val loss: 1.037158489227295
Epoch 520, training loss: 63.9822883605957 = 0.7928376793861389 + 10.0 * 6.318944931030273
Epoch 520, val loss: 1.0263806581497192
Epoch 530, training loss: 63.97280502319336 = 0.7726497054100037 + 10.0 * 6.320015907287598
Epoch 530, val loss: 1.0164885520935059
Epoch 540, training loss: 63.917354583740234 = 0.7530959844589233 + 10.0 * 6.316425800323486
Epoch 540, val loss: 1.0072826147079468
Epoch 550, training loss: 63.84606170654297 = 0.7344364523887634 + 10.0 * 6.31116247177124
Epoch 550, val loss: 0.9990198016166687
Epoch 560, training loss: 63.80615234375 = 0.716453492641449 + 10.0 * 6.308969974517822
Epoch 560, val loss: 0.9915660619735718
Epoch 570, training loss: 63.824615478515625 = 0.6991220712661743 + 10.0 * 6.312549114227295
Epoch 570, val loss: 0.9847336411476135
Epoch 580, training loss: 63.775569915771484 = 0.6821892261505127 + 10.0 * 6.309338092803955
Epoch 580, val loss: 0.9783515334129333
Epoch 590, training loss: 63.71373748779297 = 0.6660580039024353 + 10.0 * 6.304768085479736
Epoch 590, val loss: 0.97275310754776
Epoch 600, training loss: 63.67730712890625 = 0.6504362225532532 + 10.0 * 6.302687168121338
Epoch 600, val loss: 0.9676464200019836
Epoch 610, training loss: 63.62569046020508 = 0.6352652311325073 + 10.0 * 6.299042701721191
Epoch 610, val loss: 0.9630175828933716
Epoch 620, training loss: 63.64208221435547 = 0.6205204725265503 + 10.0 * 6.3021559715271
Epoch 620, val loss: 0.958931565284729
Epoch 630, training loss: 63.614768981933594 = 0.6060776710510254 + 10.0 * 6.300868988037109
Epoch 630, val loss: 0.954917311668396
Epoch 640, training loss: 63.54995346069336 = 0.5921174883842468 + 10.0 * 6.295783519744873
Epoch 640, val loss: 0.9515233039855957
Epoch 650, training loss: 63.504947662353516 = 0.5784979462623596 + 10.0 * 6.29264497756958
Epoch 650, val loss: 0.9485391974449158
Epoch 660, training loss: 63.48989486694336 = 0.5652005076408386 + 10.0 * 6.292469501495361
Epoch 660, val loss: 0.9457690119743347
Epoch 670, training loss: 63.45250701904297 = 0.5521326065063477 + 10.0 * 6.290037631988525
Epoch 670, val loss: 0.943209707736969
Epoch 680, training loss: 63.43663024902344 = 0.5393698215484619 + 10.0 * 6.2897257804870605
Epoch 680, val loss: 0.9409348964691162
Epoch 690, training loss: 63.402557373046875 = 0.5268616080284119 + 10.0 * 6.287569522857666
Epoch 690, val loss: 0.9388498663902283
Epoch 700, training loss: 63.42218017578125 = 0.5145654678344727 + 10.0 * 6.290761470794678
Epoch 700, val loss: 0.9370009899139404
Epoch 710, training loss: 63.36078643798828 = 0.5024431347846985 + 10.0 * 6.285834312438965
Epoch 710, val loss: 0.935243546962738
Epoch 720, training loss: 63.35130310058594 = 0.49050700664520264 + 10.0 * 6.2860798835754395
Epoch 720, val loss: 0.9336938261985779
Epoch 730, training loss: 63.30152130126953 = 0.4787370264530182 + 10.0 * 6.282278537750244
Epoch 730, val loss: 0.9321793913841248
Epoch 740, training loss: 63.285457611083984 = 0.46706679463386536 + 10.0 * 6.281838893890381
Epoch 740, val loss: 0.9307788610458374
Epoch 750, training loss: 63.291648864746094 = 0.45550423860549927 + 10.0 * 6.283614158630371
Epoch 750, val loss: 0.929602861404419
Epoch 760, training loss: 63.23013687133789 = 0.44399330019950867 + 10.0 * 6.278614521026611
Epoch 760, val loss: 0.9283984303474426
Epoch 770, training loss: 63.20431137084961 = 0.43252986669540405 + 10.0 * 6.2771782875061035
Epoch 770, val loss: 0.927395224571228
Epoch 780, training loss: 63.25034713745117 = 0.4210840165615082 + 10.0 * 6.282926082611084
Epoch 780, val loss: 0.9263747334480286
Epoch 790, training loss: 63.18936538696289 = 0.4095490574836731 + 10.0 * 6.277981758117676
Epoch 790, val loss: 0.9254009127616882
Epoch 800, training loss: 63.1689338684082 = 0.3980446457862854 + 10.0 * 6.2770891189575195
Epoch 800, val loss: 0.9244334697723389
Epoch 810, training loss: 63.125701904296875 = 0.38654136657714844 + 10.0 * 6.273916244506836
Epoch 810, val loss: 0.9235454201698303
Epoch 820, training loss: 63.15040969848633 = 0.37497198581695557 + 10.0 * 6.277543544769287
Epoch 820, val loss: 0.922666072845459
Epoch 830, training loss: 63.09028625488281 = 0.36333537101745605 + 10.0 * 6.272695064544678
Epoch 830, val loss: 0.9216983318328857
Epoch 840, training loss: 63.07591247558594 = 0.3516824245452881 + 10.0 * 6.272423267364502
Epoch 840, val loss: 0.9208469986915588
Epoch 850, training loss: 63.05213928222656 = 0.3399697542190552 + 10.0 * 6.271216869354248
Epoch 850, val loss: 0.9200088381767273
Epoch 860, training loss: 63.042823791503906 = 0.32826197147369385 + 10.0 * 6.271456241607666
Epoch 860, val loss: 0.9193378686904907
Epoch 870, training loss: 63.00210189819336 = 0.31660977005958557 + 10.0 * 6.26854944229126
Epoch 870, val loss: 0.9187657833099365
Epoch 880, training loss: 62.984256744384766 = 0.3050571382045746 + 10.0 * 6.267920017242432
Epoch 880, val loss: 0.918385922908783
Epoch 890, training loss: 62.99388885498047 = 0.2936190068721771 + 10.0 * 6.270027160644531
Epoch 890, val loss: 0.918192446231842
Epoch 900, training loss: 62.96413040161133 = 0.2823229432106018 + 10.0 * 6.268180847167969
Epoch 900, val loss: 0.9181811809539795
Epoch 910, training loss: 62.93059158325195 = 0.2713109254837036 + 10.0 * 6.265927791595459
Epoch 910, val loss: 0.9184462428092957
Epoch 920, training loss: 62.90423583984375 = 0.2605671286582947 + 10.0 * 6.26436710357666
Epoch 920, val loss: 0.9190685153007507
Epoch 930, training loss: 62.89838790893555 = 0.25014638900756836 + 10.0 * 6.264824390411377
Epoch 930, val loss: 0.9201093316078186
Epoch 940, training loss: 62.878238677978516 = 0.24004969000816345 + 10.0 * 6.263818740844727
Epoch 940, val loss: 0.9214359521865845
Epoch 950, training loss: 62.883995056152344 = 0.2303144782781601 + 10.0 * 6.2653679847717285
Epoch 950, val loss: 0.9231956005096436
Epoch 960, training loss: 62.857662200927734 = 0.22093906998634338 + 10.0 * 6.263672351837158
Epoch 960, val loss: 0.9252673983573914
Epoch 970, training loss: 62.8446159362793 = 0.2120353877544403 + 10.0 * 6.26325798034668
Epoch 970, val loss: 0.9278801083564758
Epoch 980, training loss: 62.81242370605469 = 0.20355042815208435 + 10.0 * 6.260887145996094
Epoch 980, val loss: 0.9308280348777771
Epoch 990, training loss: 62.8123893737793 = 0.19544930756092072 + 10.0 * 6.261693954467773
Epoch 990, val loss: 0.9342343211174011
Epoch 1000, training loss: 62.780391693115234 = 0.18774037063121796 + 10.0 * 6.259264945983887
Epoch 1000, val loss: 0.9378183484077454
Epoch 1010, training loss: 62.75788497924805 = 0.18040823936462402 + 10.0 * 6.257747650146484
Epoch 1010, val loss: 0.9418105483055115
Epoch 1020, training loss: 62.75048828125 = 0.17344844341278076 + 10.0 * 6.257704257965088
Epoch 1020, val loss: 0.9461416602134705
Epoch 1030, training loss: 62.802772521972656 = 0.16683745384216309 + 10.0 * 6.263593673706055
Epoch 1030, val loss: 0.9507212042808533
Epoch 1040, training loss: 62.74372863769531 = 0.16055399179458618 + 10.0 * 6.258317470550537
Epoch 1040, val loss: 0.9556387662887573
Epoch 1050, training loss: 62.724159240722656 = 0.15457190573215485 + 10.0 * 6.256958961486816
Epoch 1050, val loss: 0.9608672261238098
Epoch 1060, training loss: 62.74127197265625 = 0.148879736661911 + 10.0 * 6.259239196777344
Epoch 1060, val loss: 0.9663336873054504
Epoch 1070, training loss: 62.69952392578125 = 0.1434570699930191 + 10.0 * 6.255606651306152
Epoch 1070, val loss: 0.9718514680862427
Epoch 1080, training loss: 62.69606399536133 = 0.13829338550567627 + 10.0 * 6.255776882171631
Epoch 1080, val loss: 0.9776264429092407
Epoch 1090, training loss: 62.66225051879883 = 0.13338841497898102 + 10.0 * 6.2528862953186035
Epoch 1090, val loss: 0.983460009098053
Epoch 1100, training loss: 62.673004150390625 = 0.12871752679347992 + 10.0 * 6.254428386688232
Epoch 1100, val loss: 0.9894960522651672
Epoch 1110, training loss: 62.67544174194336 = 0.12423333525657654 + 10.0 * 6.255120754241943
Epoch 1110, val loss: 0.9956524968147278
Epoch 1120, training loss: 62.67326736450195 = 0.11993229389190674 + 10.0 * 6.255333423614502
Epoch 1120, val loss: 1.0016696453094482
Epoch 1130, training loss: 62.66455841064453 = 0.11585860699415207 + 10.0 * 6.2548699378967285
Epoch 1130, val loss: 1.007954478263855
Epoch 1140, training loss: 62.630096435546875 = 0.11192493885755539 + 10.0 * 6.251817226409912
Epoch 1140, val loss: 1.0141860246658325
Epoch 1150, training loss: 62.61030960083008 = 0.10820608586072922 + 10.0 * 6.250210285186768
Epoch 1150, val loss: 1.0205979347229004
Epoch 1160, training loss: 62.59850311279297 = 0.10464002937078476 + 10.0 * 6.249386310577393
Epoch 1160, val loss: 1.0270541906356812
Epoch 1170, training loss: 62.63453674316406 = 0.10121439397335052 + 10.0 * 6.253332138061523
Epoch 1170, val loss: 1.0336240530014038
Epoch 1180, training loss: 62.691795349121094 = 0.09791967272758484 + 10.0 * 6.259387493133545
Epoch 1180, val loss: 1.0400540828704834
Epoch 1190, training loss: 62.59991455078125 = 0.09476583451032639 + 10.0 * 6.250514984130859
Epoch 1190, val loss: 1.0465410947799683
Epoch 1200, training loss: 62.56847381591797 = 0.09172636270523071 + 10.0 * 6.247674465179443
Epoch 1200, val loss: 1.0531443357467651
Epoch 1210, training loss: 62.55746841430664 = 0.08884425461292267 + 10.0 * 6.246862411499023
Epoch 1210, val loss: 1.059739351272583
Epoch 1220, training loss: 62.549503326416016 = 0.08607783913612366 + 10.0 * 6.246342658996582
Epoch 1220, val loss: 1.0663799047470093
Epoch 1230, training loss: 62.659446716308594 = 0.08341461420059204 + 10.0 * 6.257603168487549
Epoch 1230, val loss: 1.0729275941848755
Epoch 1240, training loss: 62.554710388183594 = 0.08082238584756851 + 10.0 * 6.24738883972168
Epoch 1240, val loss: 1.0794320106506348
Epoch 1250, training loss: 62.531551361083984 = 0.07835178822278976 + 10.0 * 6.2453203201293945
Epoch 1250, val loss: 1.0860995054244995
Epoch 1260, training loss: 62.52162551879883 = 0.07599304616451263 + 10.0 * 6.244563102722168
Epoch 1260, val loss: 1.0926580429077148
Epoch 1270, training loss: 62.601009368896484 = 0.07372790575027466 + 10.0 * 6.252728462219238
Epoch 1270, val loss: 1.099194884300232
Epoch 1280, training loss: 62.53773498535156 = 0.07153508812189102 + 10.0 * 6.246620178222656
Epoch 1280, val loss: 1.1055920124053955
Epoch 1290, training loss: 62.510196685791016 = 0.06940716505050659 + 10.0 * 6.244078636169434
Epoch 1290, val loss: 1.1121231317520142
Epoch 1300, training loss: 62.49479293823242 = 0.06739264726638794 + 10.0 * 6.242739677429199
Epoch 1300, val loss: 1.1185637712478638
Epoch 1310, training loss: 62.4930419921875 = 0.06544023752212524 + 10.0 * 6.242760181427002
Epoch 1310, val loss: 1.125006079673767
Epoch 1320, training loss: 62.548885345458984 = 0.06354264914989471 + 10.0 * 6.248534202575684
Epoch 1320, val loss: 1.131360411643982
Epoch 1330, training loss: 62.50723648071289 = 0.06171620264649391 + 10.0 * 6.244551658630371
Epoch 1330, val loss: 1.137650489807129
Epoch 1340, training loss: 62.476783752441406 = 0.059957943856716156 + 10.0 * 6.241682529449463
Epoch 1340, val loss: 1.1439356803894043
Epoch 1350, training loss: 62.4603271484375 = 0.05826408788561821 + 10.0 * 6.240206241607666
Epoch 1350, val loss: 1.1503350734710693
Epoch 1360, training loss: 62.45547103881836 = 0.05664004385471344 + 10.0 * 6.239882946014404
Epoch 1360, val loss: 1.1567319631576538
Epoch 1370, training loss: 62.5217399597168 = 0.05507636070251465 + 10.0 * 6.246666431427002
Epoch 1370, val loss: 1.1629438400268555
Epoch 1380, training loss: 62.49443054199219 = 0.0535416305065155 + 10.0 * 6.244088649749756
Epoch 1380, val loss: 1.1688412427902222
Epoch 1390, training loss: 62.45595932006836 = 0.05205242708325386 + 10.0 * 6.240390777587891
Epoch 1390, val loss: 1.1751155853271484
Epoch 1400, training loss: 62.44489288330078 = 0.05062690004706383 + 10.0 * 6.239426612854004
Epoch 1400, val loss: 1.1811338663101196
Epoch 1410, training loss: 62.458160400390625 = 0.04925531521439552 + 10.0 * 6.2408905029296875
Epoch 1410, val loss: 1.187135100364685
Epoch 1420, training loss: 62.43822479248047 = 0.047935426235198975 + 10.0 * 6.2390289306640625
Epoch 1420, val loss: 1.1930062770843506
Epoch 1430, training loss: 62.449127197265625 = 0.04664020985364914 + 10.0 * 6.240248680114746
Epoch 1430, val loss: 1.198937177658081
Epoch 1440, training loss: 62.43315124511719 = 0.045394428074359894 + 10.0 * 6.238775730133057
Epoch 1440, val loss: 1.20465886592865
Epoch 1450, training loss: 62.407196044921875 = 0.04418795928359032 + 10.0 * 6.236300468444824
Epoch 1450, val loss: 1.210439682006836
Epoch 1460, training loss: 62.405174255371094 = 0.04303137585520744 + 10.0 * 6.236214637756348
Epoch 1460, val loss: 1.2161997556686401
Epoch 1470, training loss: 62.432437896728516 = 0.0419105626642704 + 10.0 * 6.239052772521973
Epoch 1470, val loss: 1.2219042778015137
Epoch 1480, training loss: 62.393157958984375 = 0.0408063568174839 + 10.0 * 6.235235214233398
Epoch 1480, val loss: 1.227431297302246
Epoch 1490, training loss: 62.42045211791992 = 0.039746515452861786 + 10.0 * 6.238070487976074
Epoch 1490, val loss: 1.2330266237258911
Epoch 1500, training loss: 62.38071823120117 = 0.038716308772563934 + 10.0 * 6.234200477600098
Epoch 1500, val loss: 1.238516092300415
Epoch 1510, training loss: 62.38166809082031 = 0.03772986680269241 + 10.0 * 6.23439359664917
Epoch 1510, val loss: 1.2440189123153687
Epoch 1520, training loss: 62.392269134521484 = 0.03677762299776077 + 10.0 * 6.235548973083496
Epoch 1520, val loss: 1.2494797706604004
Epoch 1530, training loss: 62.4276123046875 = 0.03584197536110878 + 10.0 * 6.2391767501831055
Epoch 1530, val loss: 1.254891276359558
Epoch 1540, training loss: 62.39373016357422 = 0.034945640712976456 + 10.0 * 6.2358784675598145
Epoch 1540, val loss: 1.2600961923599243
Epoch 1550, training loss: 62.36748123168945 = 0.03406089171767235 + 10.0 * 6.233342170715332
Epoch 1550, val loss: 1.2654794454574585
Epoch 1560, training loss: 62.35432815551758 = 0.03321665897965431 + 10.0 * 6.232110977172852
Epoch 1560, val loss: 1.2708595991134644
Epoch 1570, training loss: 62.40363693237305 = 0.032405246049165726 + 10.0 * 6.237123012542725
Epoch 1570, val loss: 1.276154637336731
Epoch 1580, training loss: 62.366233825683594 = 0.031607531011104584 + 10.0 * 6.233462333679199
Epoch 1580, val loss: 1.2811877727508545
Epoch 1590, training loss: 62.3580322265625 = 0.030841810628771782 + 10.0 * 6.2327189445495605
Epoch 1590, val loss: 1.2864216566085815
Epoch 1600, training loss: 62.337520599365234 = 0.030109811574220657 + 10.0 * 6.230741024017334
Epoch 1600, val loss: 1.2914350032806396
Epoch 1610, training loss: 62.334232330322266 = 0.02939893677830696 + 10.0 * 6.230483055114746
Epoch 1610, val loss: 1.2965563535690308
Epoch 1620, training loss: 62.34630584716797 = 0.02871105447411537 + 10.0 * 6.231759548187256
Epoch 1620, val loss: 1.3016165494918823
Epoch 1630, training loss: 62.37868118286133 = 0.028034880757331848 + 10.0 * 6.235064506530762
Epoch 1630, val loss: 1.3065670728683472
Epoch 1640, training loss: 62.35456466674805 = 0.027387991547584534 + 10.0 * 6.232717514038086
Epoch 1640, val loss: 1.3114526271820068
Epoch 1650, training loss: 62.3422966003418 = 0.026760948821902275 + 10.0 * 6.231553554534912
Epoch 1650, val loss: 1.316398024559021
Epoch 1660, training loss: 62.322288513183594 = 0.026149684563279152 + 10.0 * 6.229613780975342
Epoch 1660, val loss: 1.3213071823120117
Epoch 1670, training loss: 62.329994201660156 = 0.02556847780942917 + 10.0 * 6.230442523956299
Epoch 1670, val loss: 1.326251745223999
Epoch 1680, training loss: 62.32167053222656 = 0.024997036904096603 + 10.0 * 6.2296671867370605
Epoch 1680, val loss: 1.3309341669082642
Epoch 1690, training loss: 62.3239860534668 = 0.024448322132229805 + 10.0 * 6.229953765869141
Epoch 1690, val loss: 1.3356722593307495
Epoch 1700, training loss: 62.31902313232422 = 0.02391274832189083 + 10.0 * 6.22951078414917
Epoch 1700, val loss: 1.3404138088226318
Epoch 1710, training loss: 62.29609680175781 = 0.023390542715787888 + 10.0 * 6.227270603179932
Epoch 1710, val loss: 1.3450772762298584
Epoch 1720, training loss: 62.37667465209961 = 0.02289075218141079 + 10.0 * 6.235378265380859
Epoch 1720, val loss: 1.3497669696807861
Epoch 1730, training loss: 62.327388763427734 = 0.022398293018341064 + 10.0 * 6.230498790740967
Epoch 1730, val loss: 1.354145884513855
Epoch 1740, training loss: 62.29724884033203 = 0.021916959434747696 + 10.0 * 6.227533340454102
Epoch 1740, val loss: 1.358742117881775
Epoch 1750, training loss: 62.28157043457031 = 0.02146032825112343 + 10.0 * 6.226010799407959
Epoch 1750, val loss: 1.3632310628890991
Epoch 1760, training loss: 62.30916976928711 = 0.021016424521803856 + 10.0 * 6.22881555557251
Epoch 1760, val loss: 1.3678274154663086
Epoch 1770, training loss: 62.276268005371094 = 0.020584536716341972 + 10.0 * 6.2255682945251465
Epoch 1770, val loss: 1.3720791339874268
Epoch 1780, training loss: 62.2810173034668 = 0.02016877941787243 + 10.0 * 6.2260847091674805
Epoch 1780, val loss: 1.37639319896698
Epoch 1790, training loss: 62.315025329589844 = 0.01976427249610424 + 10.0 * 6.229526042938232
Epoch 1790, val loss: 1.3808680772781372
Epoch 1800, training loss: 62.307838439941406 = 0.019365234300494194 + 10.0 * 6.228847503662109
Epoch 1800, val loss: 1.384934425354004
Epoch 1810, training loss: 62.26604461669922 = 0.018974941223859787 + 10.0 * 6.224707126617432
Epoch 1810, val loss: 1.389344573020935
Epoch 1820, training loss: 62.26051330566406 = 0.01860921084880829 + 10.0 * 6.224190711975098
Epoch 1820, val loss: 1.3935414552688599
Epoch 1830, training loss: 62.254974365234375 = 0.018246684223413467 + 10.0 * 6.223672866821289
Epoch 1830, val loss: 1.3978430032730103
Epoch 1840, training loss: 62.318260192871094 = 0.01790199801325798 + 10.0 * 6.230035781860352
Epoch 1840, val loss: 1.4020025730133057
Epoch 1850, training loss: 62.27192687988281 = 0.017556725069880486 + 10.0 * 6.225437164306641
Epoch 1850, val loss: 1.4059593677520752
Epoch 1860, training loss: 62.24845886230469 = 0.017220601439476013 + 10.0 * 6.223124027252197
Epoch 1860, val loss: 1.4101957082748413
Epoch 1870, training loss: 62.24273681640625 = 0.016899580135941505 + 10.0 * 6.222583770751953
Epoch 1870, val loss: 1.4142487049102783
Epoch 1880, training loss: 62.31060028076172 = 0.01659073494374752 + 10.0 * 6.229401111602783
Epoch 1880, val loss: 1.4183145761489868
Epoch 1890, training loss: 62.27762222290039 = 0.016285182908177376 + 10.0 * 6.226133823394775
Epoch 1890, val loss: 1.422193169593811
Epoch 1900, training loss: 62.25648880004883 = 0.015978366136550903 + 10.0 * 6.224050998687744
Epoch 1900, val loss: 1.4262205362319946
Epoch 1910, training loss: 62.240623474121094 = 0.015692928805947304 + 10.0 * 6.2224931716918945
Epoch 1910, val loss: 1.4302183389663696
Epoch 1920, training loss: 62.28828811645508 = 0.015411315485835075 + 10.0 * 6.227287769317627
Epoch 1920, val loss: 1.4342385530471802
Epoch 1930, training loss: 62.2364616394043 = 0.015132574364542961 + 10.0 * 6.222132682800293
Epoch 1930, val loss: 1.437896728515625
Epoch 1940, training loss: 62.23896789550781 = 0.014864370226860046 + 10.0 * 6.222410202026367
Epoch 1940, val loss: 1.4418799877166748
Epoch 1950, training loss: 62.22764587402344 = 0.014602825045585632 + 10.0 * 6.221304416656494
Epoch 1950, val loss: 1.4457131624221802
Epoch 1960, training loss: 62.22119903564453 = 0.014351082034409046 + 10.0 * 6.220685005187988
Epoch 1960, val loss: 1.449513554573059
Epoch 1970, training loss: 62.24189376831055 = 0.014106660149991512 + 10.0 * 6.222778797149658
Epoch 1970, val loss: 1.4532421827316284
Epoch 1980, training loss: 62.23163604736328 = 0.013866043649613857 + 10.0 * 6.221776962280273
Epoch 1980, val loss: 1.4568957090377808
Epoch 1990, training loss: 62.26784896850586 = 0.013634435832500458 + 10.0 * 6.22542142868042
Epoch 1990, val loss: 1.4603947401046753
Epoch 2000, training loss: 62.22243118286133 = 0.013401184231042862 + 10.0 * 6.220902919769287
Epoch 2000, val loss: 1.4641584157943726
Epoch 2010, training loss: 62.2123908996582 = 0.013180171139538288 + 10.0 * 6.219921112060547
Epoch 2010, val loss: 1.4678699970245361
Epoch 2020, training loss: 62.24277114868164 = 0.012965625151991844 + 10.0 * 6.222980499267578
Epoch 2020, val loss: 1.4713490009307861
Epoch 2030, training loss: 62.21810531616211 = 0.012750569730997086 + 10.0 * 6.220535755157471
Epoch 2030, val loss: 1.4750151634216309
Epoch 2040, training loss: 62.212711334228516 = 0.012541646137833595 + 10.0 * 6.220016956329346
Epoch 2040, val loss: 1.4783148765563965
Epoch 2050, training loss: 62.21570587158203 = 0.01234330516308546 + 10.0 * 6.220335960388184
Epoch 2050, val loss: 1.4819210767745972
Epoch 2060, training loss: 62.236141204833984 = 0.012148777022957802 + 10.0 * 6.2223992347717285
Epoch 2060, val loss: 1.4852899312973022
Epoch 2070, training loss: 62.19609069824219 = 0.011953338049352169 + 10.0 * 6.218413829803467
Epoch 2070, val loss: 1.488850712776184
Epoch 2080, training loss: 62.21473693847656 = 0.011769598349928856 + 10.0 * 6.220296859741211
Epoch 2080, val loss: 1.4922147989273071
Epoch 2090, training loss: 62.234989166259766 = 0.011593419127166271 + 10.0 * 6.222339630126953
Epoch 2090, val loss: 1.4954849481582642
Epoch 2100, training loss: 62.21255111694336 = 0.011408031918108463 + 10.0 * 6.220114231109619
Epoch 2100, val loss: 1.499005913734436
Epoch 2110, training loss: 62.21157455444336 = 0.01123829372227192 + 10.0 * 6.220033645629883
Epoch 2110, val loss: 1.5022635459899902
Epoch 2120, training loss: 62.18636703491211 = 0.011068230494856834 + 10.0 * 6.217530250549316
Epoch 2120, val loss: 1.505548357963562
Epoch 2130, training loss: 62.197410583496094 = 0.010905558243393898 + 10.0 * 6.2186503410339355
Epoch 2130, val loss: 1.5087544918060303
Epoch 2140, training loss: 62.22172546386719 = 0.010744055733084679 + 10.0 * 6.22109842300415
Epoch 2140, val loss: 1.511987328529358
Epoch 2150, training loss: 62.1934814453125 = 0.010583440773189068 + 10.0 * 6.218289852142334
Epoch 2150, val loss: 1.5151424407958984
Epoch 2160, training loss: 62.1827278137207 = 0.010432874783873558 + 10.0 * 6.21722936630249
Epoch 2160, val loss: 1.5183056592941284
Epoch 2170, training loss: 62.21498489379883 = 0.010284589603543282 + 10.0 * 6.220469951629639
Epoch 2170, val loss: 1.5215271711349487
Epoch 2180, training loss: 62.179134368896484 = 0.010135382413864136 + 10.0 * 6.216899871826172
Epoch 2180, val loss: 1.5246679782867432
Epoch 2190, training loss: 62.19767761230469 = 0.00999095756560564 + 10.0 * 6.21876859664917
Epoch 2190, val loss: 1.527696967124939
Epoch 2200, training loss: 62.18363571166992 = 0.009852847084403038 + 10.0 * 6.21737813949585
Epoch 2200, val loss: 1.5308116674423218
Epoch 2210, training loss: 62.17524337768555 = 0.009715215303003788 + 10.0 * 6.216552734375
Epoch 2210, val loss: 1.5338832139968872
Epoch 2220, training loss: 62.16651916503906 = 0.009580923244357109 + 10.0 * 6.215693473815918
Epoch 2220, val loss: 1.5370033979415894
Epoch 2230, training loss: 62.19486999511719 = 0.009450491517782211 + 10.0 * 6.218542098999023
Epoch 2230, val loss: 1.5400171279907227
Epoch 2240, training loss: 62.174835205078125 = 0.009321856312453747 + 10.0 * 6.216551303863525
Epoch 2240, val loss: 1.542824149131775
Epoch 2250, training loss: 62.17459487915039 = 0.00919647328555584 + 10.0 * 6.2165398597717285
Epoch 2250, val loss: 1.545714020729065
Epoch 2260, training loss: 62.1744384765625 = 0.009072738699615002 + 10.0 * 6.216536521911621
Epoch 2260, val loss: 1.5487545728683472
Epoch 2270, training loss: 62.175174713134766 = 0.008952182717621326 + 10.0 * 6.216622352600098
Epoch 2270, val loss: 1.5515719652175903
Epoch 2280, training loss: 62.15580749511719 = 0.00883586797863245 + 10.0 * 6.214697360992432
Epoch 2280, val loss: 1.5544594526290894
Epoch 2290, training loss: 62.16330337524414 = 0.008722666651010513 + 10.0 * 6.215457916259766
Epoch 2290, val loss: 1.557327151298523
Epoch 2300, training loss: 62.17856216430664 = 0.008610769174993038 + 10.0 * 6.2169952392578125
Epoch 2300, val loss: 1.5601916313171387
Epoch 2310, training loss: 62.1685676574707 = 0.008500514551997185 + 10.0 * 6.216006755828857
Epoch 2310, val loss: 1.5630319118499756
Epoch 2320, training loss: 62.1640625 = 0.00839207787066698 + 10.0 * 6.215567111968994
Epoch 2320, val loss: 1.5657521486282349
Epoch 2330, training loss: 62.161537170410156 = 0.008285450749099255 + 10.0 * 6.215325355529785
Epoch 2330, val loss: 1.5685720443725586
Epoch 2340, training loss: 62.15773391723633 = 0.008182506076991558 + 10.0 * 6.2149553298950195
Epoch 2340, val loss: 1.571342945098877
Epoch 2350, training loss: 62.17930221557617 = 0.008084445260465145 + 10.0 * 6.2171220779418945
Epoch 2350, val loss: 1.5739189386367798
Epoch 2360, training loss: 62.18377685546875 = 0.007981033995747566 + 10.0 * 6.2175798416137695
Epoch 2360, val loss: 1.576615810394287
Epoch 2370, training loss: 62.13459396362305 = 0.007881811819970608 + 10.0 * 6.212671279907227
Epoch 2370, val loss: 1.579288363456726
Epoch 2380, training loss: 62.12895584106445 = 0.007786197122186422 + 10.0 * 6.2121171951293945
Epoch 2380, val loss: 1.5821009874343872
Epoch 2390, training loss: 62.13393020629883 = 0.007694486062973738 + 10.0 * 6.212623596191406
Epoch 2390, val loss: 1.584738850593567
Epoch 2400, training loss: 62.16178512573242 = 0.0076048122718930244 + 10.0 * 6.215417861938477
Epoch 2400, val loss: 1.5873262882232666
Epoch 2410, training loss: 62.13953399658203 = 0.007514484692364931 + 10.0 * 6.213201999664307
Epoch 2410, val loss: 1.589858889579773
Epoch 2420, training loss: 62.191383361816406 = 0.007429672870784998 + 10.0 * 6.218395233154297
Epoch 2420, val loss: 1.5923463106155396
Epoch 2430, training loss: 62.165367126464844 = 0.0073382798582315445 + 10.0 * 6.2158026695251465
Epoch 2430, val loss: 1.5947569608688354
Epoch 2440, training loss: 62.128562927246094 = 0.007250766735523939 + 10.0 * 6.212131023406982
Epoch 2440, val loss: 1.597479224205017
Epoch 2450, training loss: 62.134525299072266 = 0.007168839685618877 + 10.0 * 6.212735652923584
Epoch 2450, val loss: 1.6000089645385742
Epoch 2460, training loss: 62.13365936279297 = 0.0070884195156395435 + 10.0 * 6.2126569747924805
Epoch 2460, val loss: 1.6023743152618408
Epoch 2470, training loss: 62.11857986450195 = 0.007009247317910194 + 10.0 * 6.211157321929932
Epoch 2470, val loss: 1.6048800945281982
Epoch 2480, training loss: 62.160369873046875 = 0.006932936608791351 + 10.0 * 6.215343952178955
Epoch 2480, val loss: 1.6072958707809448
Epoch 2490, training loss: 62.13479232788086 = 0.006853656843304634 + 10.0 * 6.212793827056885
Epoch 2490, val loss: 1.6097283363342285
Epoch 2500, training loss: 62.118003845214844 = 0.006778230890631676 + 10.0 * 6.211122512817383
Epoch 2500, val loss: 1.6121060848236084
Epoch 2510, training loss: 62.141178131103516 = 0.0067052049562335014 + 10.0 * 6.213447093963623
Epoch 2510, val loss: 1.6145275831222534
Epoch 2520, training loss: 62.13419723510742 = 0.006631943862885237 + 10.0 * 6.212756156921387
Epoch 2520, val loss: 1.6167786121368408
Epoch 2530, training loss: 62.137718200683594 = 0.00656173937022686 + 10.0 * 6.213115692138672
Epoch 2530, val loss: 1.6189261674880981
Epoch 2540, training loss: 62.11250686645508 = 0.006490795407444239 + 10.0 * 6.210601329803467
Epoch 2540, val loss: 1.621498942375183
Epoch 2550, training loss: 62.14152526855469 = 0.006423832383006811 + 10.0 * 6.213510036468506
Epoch 2550, val loss: 1.6237465143203735
Epoch 2560, training loss: 62.11497497558594 = 0.006355967838317156 + 10.0 * 6.210862159729004
Epoch 2560, val loss: 1.625750184059143
Epoch 2570, training loss: 62.10679244995117 = 0.006289924494922161 + 10.0 * 6.210050106048584
Epoch 2570, val loss: 1.6282366514205933
Epoch 2580, training loss: 62.1889762878418 = 0.0062274811789393425 + 10.0 * 6.21827507019043
Epoch 2580, val loss: 1.6303656101226807
Epoch 2590, training loss: 62.13225173950195 = 0.006159354466944933 + 10.0 * 6.21260929107666
Epoch 2590, val loss: 1.6324862241744995
Epoch 2600, training loss: 62.11066436767578 = 0.0060958946123719215 + 10.0 * 6.210456848144531
Epoch 2600, val loss: 1.6347196102142334
Epoch 2610, training loss: 62.105255126953125 = 0.006034512538462877 + 10.0 * 6.209921836853027
Epoch 2610, val loss: 1.6369214057922363
Epoch 2620, training loss: 62.14362335205078 = 0.005975192878395319 + 10.0 * 6.2137651443481445
Epoch 2620, val loss: 1.6391743421554565
Epoch 2630, training loss: 62.10904312133789 = 0.0059147425927221775 + 10.0 * 6.210312843322754
Epoch 2630, val loss: 1.6411535739898682
Epoch 2640, training loss: 62.11936950683594 = 0.005856993608176708 + 10.0 * 6.21135139465332
Epoch 2640, val loss: 1.6432685852050781
Epoch 2650, training loss: 62.10953140258789 = 0.005798972211778164 + 10.0 * 6.210373401641846
Epoch 2650, val loss: 1.6452609300613403
Epoch 2660, training loss: 62.09844207763672 = 0.005742764566093683 + 10.0 * 6.209270000457764
Epoch 2660, val loss: 1.6474272012710571
Epoch 2670, training loss: 62.09239959716797 = 0.005687918048352003 + 10.0 * 6.2086710929870605
Epoch 2670, val loss: 1.649595856666565
Epoch 2680, training loss: 62.14299011230469 = 0.005634808912873268 + 10.0 * 6.213735580444336
Epoch 2680, val loss: 1.6517324447631836
Epoch 2690, training loss: 62.111446380615234 = 0.005578960292041302 + 10.0 * 6.210587024688721
Epoch 2690, val loss: 1.6534779071807861
Epoch 2700, training loss: 62.101051330566406 = 0.005525196902453899 + 10.0 * 6.209552764892578
Epoch 2700, val loss: 1.6556620597839355
Epoch 2710, training loss: 62.08736038208008 = 0.005474555306136608 + 10.0 * 6.208188533782959
Epoch 2710, val loss: 1.6575489044189453
Epoch 2720, training loss: 62.11859130859375 = 0.005425992887467146 + 10.0 * 6.2113165855407715
Epoch 2720, val loss: 1.6595439910888672
Epoch 2730, training loss: 62.08677291870117 = 0.005373835563659668 + 10.0 * 6.208139896392822
Epoch 2730, val loss: 1.6616395711898804
Epoch 2740, training loss: 62.085533142089844 = 0.0053243981674313545 + 10.0 * 6.2080206871032715
Epoch 2740, val loss: 1.6635769605636597
Epoch 2750, training loss: 62.07841491699219 = 0.00527684623375535 + 10.0 * 6.2073140144348145
Epoch 2750, val loss: 1.665513277053833
Epoch 2760, training loss: 62.15532684326172 = 0.005231582093983889 + 10.0 * 6.215009689331055
Epoch 2760, val loss: 1.667401909828186
Epoch 2770, training loss: 62.105369567871094 = 0.005181722808629274 + 10.0 * 6.210019111633301
Epoch 2770, val loss: 1.6694090366363525
Epoch 2780, training loss: 62.092674255371094 = 0.005135247949510813 + 10.0 * 6.208754062652588
Epoch 2780, val loss: 1.6711448431015015
Epoch 2790, training loss: 62.113121032714844 = 0.00508896354585886 + 10.0 * 6.210803031921387
Epoch 2790, val loss: 1.6731661558151245
Epoch 2800, training loss: 62.07752227783203 = 0.005044805351644754 + 10.0 * 6.207247734069824
Epoch 2800, val loss: 1.6749823093414307
Epoch 2810, training loss: 62.085670471191406 = 0.005002694204449654 + 10.0 * 6.208066940307617
Epoch 2810, val loss: 1.6768760681152344
Epoch 2820, training loss: 62.07867431640625 = 0.004959281999617815 + 10.0 * 6.207371711730957
Epoch 2820, val loss: 1.678671956062317
Epoch 2830, training loss: 62.10297775268555 = 0.004916451405733824 + 10.0 * 6.209805965423584
Epoch 2830, val loss: 1.6805189847946167
Epoch 2840, training loss: 62.094947814941406 = 0.004874438047409058 + 10.0 * 6.209007263183594
Epoch 2840, val loss: 1.6824628114700317
Epoch 2850, training loss: 62.08713150024414 = 0.004831430967897177 + 10.0 * 6.208230018615723
Epoch 2850, val loss: 1.6840578317642212
Epoch 2860, training loss: 62.06338882446289 = 0.004790033213794231 + 10.0 * 6.205859661102295
Epoch 2860, val loss: 1.6858980655670166
Epoch 2870, training loss: 62.060302734375 = 0.004751735832542181 + 10.0 * 6.205554962158203
Epoch 2870, val loss: 1.6877490282058716
Epoch 2880, training loss: 62.08094024658203 = 0.004713304806500673 + 10.0 * 6.207622528076172
Epoch 2880, val loss: 1.6894842386245728
Epoch 2890, training loss: 62.08120346069336 = 0.004674404393881559 + 10.0 * 6.207653045654297
Epoch 2890, val loss: 1.691249966621399
Epoch 2900, training loss: 62.07538604736328 = 0.00463636452332139 + 10.0 * 6.207075119018555
Epoch 2900, val loss: 1.6929547786712646
Epoch 2910, training loss: 62.08576583862305 = 0.004598775412887335 + 10.0 * 6.20811653137207
Epoch 2910, val loss: 1.6947381496429443
Epoch 2920, training loss: 62.093017578125 = 0.004560607951134443 + 10.0 * 6.208845615386963
Epoch 2920, val loss: 1.696353793144226
Epoch 2930, training loss: 62.07176971435547 = 0.00452459929510951 + 10.0 * 6.206724643707275
Epoch 2930, val loss: 1.6978967189788818
Epoch 2940, training loss: 62.05843734741211 = 0.004488714970648289 + 10.0 * 6.205394744873047
Epoch 2940, val loss: 1.699541687965393
Epoch 2950, training loss: 62.07108688354492 = 0.004454599227756262 + 10.0 * 6.206663131713867
Epoch 2950, val loss: 1.7012417316436768
Epoch 2960, training loss: 62.09031295776367 = 0.004421057645231485 + 10.0 * 6.20858907699585
Epoch 2960, val loss: 1.7027571201324463
Epoch 2970, training loss: 62.07967758178711 = 0.0043828776106238365 + 10.0 * 6.207529544830322
Epoch 2970, val loss: 1.7044703960418701
Epoch 2980, training loss: 62.05794906616211 = 0.004350207280367613 + 10.0 * 6.205359935760498
Epoch 2980, val loss: 1.7059861421585083
Epoch 2990, training loss: 62.05492401123047 = 0.004317308310419321 + 10.0 * 6.2050604820251465
Epoch 2990, val loss: 1.7076447010040283
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 87.91244506835938 = 1.9438931941986084 + 10.0 * 8.596855163574219
Epoch 0, val loss: 1.947300672531128
Epoch 10, training loss: 87.89696502685547 = 1.933858871459961 + 10.0 * 8.59631061553955
Epoch 10, val loss: 1.9370818138122559
Epoch 20, training loss: 87.84122467041016 = 1.921531319618225 + 10.0 * 8.59196949005127
Epoch 20, val loss: 1.9244722127914429
Epoch 30, training loss: 87.51210021972656 = 1.9060531854629517 + 10.0 * 8.5606050491333
Epoch 30, val loss: 1.9088026285171509
Epoch 40, training loss: 85.65739440917969 = 1.888902187347412 + 10.0 * 8.376849174499512
Epoch 40, val loss: 1.8920855522155762
Epoch 50, training loss: 79.93286895751953 = 1.8709043264389038 + 10.0 * 7.806196212768555
Epoch 50, val loss: 1.8746641874313354
Epoch 60, training loss: 75.78085327148438 = 1.8555251359939575 + 10.0 * 7.392533302307129
Epoch 60, val loss: 1.859484314918518
Epoch 70, training loss: 72.73755645751953 = 1.8414407968521118 + 10.0 * 7.089611530303955
Epoch 70, val loss: 1.8451650142669678
Epoch 80, training loss: 70.57150268554688 = 1.830456256866455 + 10.0 * 6.874104022979736
Epoch 80, val loss: 1.8341084718704224
Epoch 90, training loss: 69.43962097167969 = 1.819934606552124 + 10.0 * 6.761969089508057
Epoch 90, val loss: 1.8235008716583252
Epoch 100, training loss: 68.77457427978516 = 1.808303952217102 + 10.0 * 6.696627140045166
Epoch 100, val loss: 1.8122977018356323
Epoch 110, training loss: 68.28375244140625 = 1.7963095903396606 + 10.0 * 6.648744583129883
Epoch 110, val loss: 1.8007185459136963
Epoch 120, training loss: 67.88262176513672 = 1.7847354412078857 + 10.0 * 6.60978889465332
Epoch 120, val loss: 1.7895746231079102
Epoch 130, training loss: 67.55094146728516 = 1.773558259010315 + 10.0 * 6.577738285064697
Epoch 130, val loss: 1.7785011529922485
Epoch 140, training loss: 67.3360824584961 = 1.7618619203567505 + 10.0 * 6.557422161102295
Epoch 140, val loss: 1.7667509317398071
Epoch 150, training loss: 67.06041717529297 = 1.7491023540496826 + 10.0 * 6.531131744384766
Epoch 150, val loss: 1.7543432712554932
Epoch 160, training loss: 66.85236358642578 = 1.7357289791107178 + 10.0 * 6.511663913726807
Epoch 160, val loss: 1.7413021326065063
Epoch 170, training loss: 66.67225646972656 = 1.7211718559265137 + 10.0 * 6.495108604431152
Epoch 170, val loss: 1.7273272275924683
Epoch 180, training loss: 66.50169372558594 = 1.7053076028823853 + 10.0 * 6.479639053344727
Epoch 180, val loss: 1.7122400999069214
Epoch 190, training loss: 66.34123992919922 = 1.6879907846450806 + 10.0 * 6.465324878692627
Epoch 190, val loss: 1.6959172487258911
Epoch 200, training loss: 66.24703216552734 = 1.6690495014190674 + 10.0 * 6.457798004150391
Epoch 200, val loss: 1.678224802017212
Epoch 210, training loss: 66.0836181640625 = 1.6482576131820679 + 10.0 * 6.443536758422852
Epoch 210, val loss: 1.6589159965515137
Epoch 220, training loss: 65.95155334472656 = 1.6256850957870483 + 10.0 * 6.432586669921875
Epoch 220, val loss: 1.63805091381073
Epoch 230, training loss: 65.83502197265625 = 1.601102590560913 + 10.0 * 6.423391819000244
Epoch 230, val loss: 1.6155054569244385
Epoch 240, training loss: 65.7284927368164 = 1.5744693279266357 + 10.0 * 6.415401935577393
Epoch 240, val loss: 1.5913053750991821
Epoch 250, training loss: 65.64464569091797 = 1.545707106590271 + 10.0 * 6.40989351272583
Epoch 250, val loss: 1.565397024154663
Epoch 260, training loss: 65.54700469970703 = 1.515223503112793 + 10.0 * 6.4031782150268555
Epoch 260, val loss: 1.5382418632507324
Epoch 270, training loss: 65.44033813476562 = 1.4829024076461792 + 10.0 * 6.395743370056152
Epoch 270, val loss: 1.5098540782928467
Epoch 280, training loss: 65.39586639404297 = 1.4489848613739014 + 10.0 * 6.394688129425049
Epoch 280, val loss: 1.4804214239120483
Epoch 290, training loss: 65.26107788085938 = 1.4140796661376953 + 10.0 * 6.384699821472168
Epoch 290, val loss: 1.450212836265564
Epoch 300, training loss: 65.17145538330078 = 1.378040075302124 + 10.0 * 6.3793416023254395
Epoch 300, val loss: 1.4196287393569946
Epoch 310, training loss: 65.08391571044922 = 1.3412806987762451 + 10.0 * 6.374263286590576
Epoch 310, val loss: 1.3887314796447754
Epoch 320, training loss: 65.01156616210938 = 1.3041632175445557 + 10.0 * 6.3707404136657715
Epoch 320, val loss: 1.3580034971237183
Epoch 330, training loss: 64.95291137695312 = 1.2668333053588867 + 10.0 * 6.368607997894287
Epoch 330, val loss: 1.3276352882385254
Epoch 340, training loss: 64.89093780517578 = 1.2300844192504883 + 10.0 * 6.366085529327393
Epoch 340, val loss: 1.2978547811508179
Epoch 350, training loss: 64.77081298828125 = 1.193823218345642 + 10.0 * 6.357698917388916
Epoch 350, val loss: 1.2691911458969116
Epoch 360, training loss: 64.698974609375 = 1.1583850383758545 + 10.0 * 6.354058742523193
Epoch 360, val loss: 1.241504430770874
Epoch 370, training loss: 64.62432861328125 = 1.1239210367202759 + 10.0 * 6.350040435791016
Epoch 370, val loss: 1.2149837017059326
Epoch 380, training loss: 64.55998229980469 = 1.0903863906860352 + 10.0 * 6.346959590911865
Epoch 380, val loss: 1.1896864175796509
Epoch 390, training loss: 64.53229522705078 = 1.0578349828720093 + 10.0 * 6.347445487976074
Epoch 390, val loss: 1.1656203269958496
Epoch 400, training loss: 64.45023345947266 = 1.0266225337982178 + 10.0 * 6.342360973358154
Epoch 400, val loss: 1.142832636833191
Epoch 410, training loss: 64.3745346069336 = 0.9966180324554443 + 10.0 * 6.337791919708252
Epoch 410, val loss: 1.1214861869812012
Epoch 420, training loss: 64.31561279296875 = 0.9678699970245361 + 10.0 * 6.334774494171143
Epoch 420, val loss: 1.1015018224716187
Epoch 430, training loss: 64.27873992919922 = 0.9402178525924683 + 10.0 * 6.333852291107178
Epoch 430, val loss: 1.082759976387024
Epoch 440, training loss: 64.2164077758789 = 0.9137900471687317 + 10.0 * 6.330261707305908
Epoch 440, val loss: 1.065223217010498
Epoch 450, training loss: 64.16493225097656 = 0.888282299041748 + 10.0 * 6.327664852142334
Epoch 450, val loss: 1.0488369464874268
Epoch 460, training loss: 64.10348510742188 = 0.863970935344696 + 10.0 * 6.323951721191406
Epoch 460, val loss: 1.0336358547210693
Epoch 470, training loss: 64.07867431640625 = 0.8405539989471436 + 10.0 * 6.323812007904053
Epoch 470, val loss: 1.0194826126098633
Epoch 480, training loss: 64.0447006225586 = 0.8179168701171875 + 10.0 * 6.322678565979004
Epoch 480, val loss: 1.0062273740768433
Epoch 490, training loss: 63.97534942626953 = 0.7961829900741577 + 10.0 * 6.317916393280029
Epoch 490, val loss: 0.9938815832138062
Epoch 500, training loss: 63.9444694519043 = 0.7751280665397644 + 10.0 * 6.316934108734131
Epoch 500, val loss: 0.9823533892631531
Epoch 510, training loss: 63.895545959472656 = 0.7546020150184631 + 10.0 * 6.314094543457031
Epoch 510, val loss: 0.9713934063911438
Epoch 520, training loss: 63.882625579833984 = 0.7346773147583008 + 10.0 * 6.314795017242432
Epoch 520, val loss: 0.9613285660743713
Epoch 530, training loss: 63.804237365722656 = 0.7153278589248657 + 10.0 * 6.3088908195495605
Epoch 530, val loss: 0.951743483543396
Epoch 540, training loss: 63.77566909790039 = 0.6964406371116638 + 10.0 * 6.307922840118408
Epoch 540, val loss: 0.9427969455718994
Epoch 550, training loss: 63.7363395690918 = 0.6779404282569885 + 10.0 * 6.305840015411377
Epoch 550, val loss: 0.9344298839569092
Epoch 560, training loss: 63.72922897338867 = 0.6597797870635986 + 10.0 * 6.306944847106934
Epoch 560, val loss: 0.9264212250709534
Epoch 570, training loss: 63.67623519897461 = 0.6420809030532837 + 10.0 * 6.303415298461914
Epoch 570, val loss: 0.9190418124198914
Epoch 580, training loss: 63.69215393066406 = 0.6246358156204224 + 10.0 * 6.306751728057861
Epoch 580, val loss: 0.9119198322296143
Epoch 590, training loss: 63.61312484741211 = 0.6073120832443237 + 10.0 * 6.300581455230713
Epoch 590, val loss: 0.9051422476768494
Epoch 600, training loss: 63.568016052246094 = 0.5905394554138184 + 10.0 * 6.297747611999512
Epoch 600, val loss: 0.8989455103874207
Epoch 610, training loss: 63.52897644042969 = 0.5739588141441345 + 10.0 * 6.295501708984375
Epoch 610, val loss: 0.8930469751358032
Epoch 620, training loss: 63.498374938964844 = 0.5576669573783875 + 10.0 * 6.294070720672607
Epoch 620, val loss: 0.8876102566719055
Epoch 630, training loss: 63.53718185424805 = 0.5414973497390747 + 10.0 * 6.2995686531066895
Epoch 630, val loss: 0.8824627995491028
Epoch 640, training loss: 63.45119857788086 = 0.5257003307342529 + 10.0 * 6.292550086975098
Epoch 640, val loss: 0.8777424693107605
Epoch 650, training loss: 63.42632293701172 = 0.5100679993629456 + 10.0 * 6.291625499725342
Epoch 650, val loss: 0.8734548091888428
Epoch 660, training loss: 63.38312911987305 = 0.49469614028930664 + 10.0 * 6.288843154907227
Epoch 660, val loss: 0.8694344162940979
Epoch 670, training loss: 63.351715087890625 = 0.4796607494354248 + 10.0 * 6.287205696105957
Epoch 670, val loss: 0.865957498550415
Epoch 680, training loss: 63.40828323364258 = 0.4647298753261566 + 10.0 * 6.294355392456055
Epoch 680, val loss: 0.8627100586891174
Epoch 690, training loss: 63.30290985107422 = 0.4502635896205902 + 10.0 * 6.285264492034912
Epoch 690, val loss: 0.8600320219993591
Epoch 700, training loss: 63.27470397949219 = 0.4359244406223297 + 10.0 * 6.283877849578857
Epoch 700, val loss: 0.8576565980911255
Epoch 710, training loss: 63.23551559448242 = 0.4219893515110016 + 10.0 * 6.281352519989014
Epoch 710, val loss: 0.8558362126350403
Epoch 720, training loss: 63.2728271484375 = 0.4083738923072815 + 10.0 * 6.286445140838623
Epoch 720, val loss: 0.8544167876243591
Epoch 730, training loss: 63.24466323852539 = 0.39483240246772766 + 10.0 * 6.284983158111572
Epoch 730, val loss: 0.8531379699707031
Epoch 740, training loss: 63.18361282348633 = 0.38183122873306274 + 10.0 * 6.280178070068359
Epoch 740, val loss: 0.8524447083473206
Epoch 750, training loss: 63.144325256347656 = 0.3691280782222748 + 10.0 * 6.277519702911377
Epoch 750, val loss: 0.8521389365196228
Epoch 760, training loss: 63.11569595336914 = 0.35679033398628235 + 10.0 * 6.275890827178955
Epoch 760, val loss: 0.8522207736968994
Epoch 770, training loss: 63.09818649291992 = 0.34475305676460266 + 10.0 * 6.275343418121338
Epoch 770, val loss: 0.8526514172554016
Epoch 780, training loss: 63.124088287353516 = 0.33295050263404846 + 10.0 * 6.27911376953125
Epoch 780, val loss: 0.853298008441925
Epoch 790, training loss: 63.0565299987793 = 0.3215474784374237 + 10.0 * 6.273498058319092
Epoch 790, val loss: 0.8542703986167908
Epoch 800, training loss: 63.0353889465332 = 0.31046637892723083 + 10.0 * 6.272492408752441
Epoch 800, val loss: 0.8555856943130493
Epoch 810, training loss: 63.00739288330078 = 0.2997385859489441 + 10.0 * 6.27076530456543
Epoch 810, val loss: 0.8572270274162292
Epoch 820, training loss: 63.02169418334961 = 0.28938159346580505 + 10.0 * 6.273231029510498
Epoch 820, val loss: 0.8591296076774597
Epoch 830, training loss: 62.98194122314453 = 0.27913764119148254 + 10.0 * 6.270280361175537
Epoch 830, val loss: 0.860991895198822
Epoch 840, training loss: 62.967525482177734 = 0.26939892768859863 + 10.0 * 6.26981258392334
Epoch 840, val loss: 0.8634076118469238
Epoch 850, training loss: 62.93195343017578 = 0.2599124610424042 + 10.0 * 6.267204284667969
Epoch 850, val loss: 0.866009533405304
Epoch 860, training loss: 62.918888092041016 = 0.25079628825187683 + 10.0 * 6.266808986663818
Epoch 860, val loss: 0.8688744902610779
Epoch 870, training loss: 62.934078216552734 = 0.24196980893611908 + 10.0 * 6.2692108154296875
Epoch 870, val loss: 0.8718905448913574
Epoch 880, training loss: 62.88052749633789 = 0.23344671726226807 + 10.0 * 6.264708042144775
Epoch 880, val loss: 0.875199019908905
Epoch 890, training loss: 62.878414154052734 = 0.22521387040615082 + 10.0 * 6.265320301055908
Epoch 890, val loss: 0.8786910176277161
Epoch 900, training loss: 62.861087799072266 = 0.21725226938724518 + 10.0 * 6.264383792877197
Epoch 900, val loss: 0.8824054002761841
Epoch 910, training loss: 62.854026794433594 = 0.20956242084503174 + 10.0 * 6.264446258544922
Epoch 910, val loss: 0.8862508535385132
Epoch 920, training loss: 62.83197784423828 = 0.2021673172712326 + 10.0 * 6.262980937957764
Epoch 920, val loss: 0.8902407884597778
Epoch 930, training loss: 62.809104919433594 = 0.19504782557487488 + 10.0 * 6.2614054679870605
Epoch 930, val loss: 0.8943812847137451
Epoch 940, training loss: 62.788299560546875 = 0.1882208287715912 + 10.0 * 6.260007858276367
Epoch 940, val loss: 0.8987932205200195
Epoch 950, training loss: 62.7859992980957 = 0.1816694587469101 + 10.0 * 6.260432720184326
Epoch 950, val loss: 0.9033113718032837
Epoch 960, training loss: 62.80311584472656 = 0.17534257471561432 + 10.0 * 6.262777328491211
Epoch 960, val loss: 0.9079116582870483
Epoch 970, training loss: 62.765159606933594 = 0.16916373372077942 + 10.0 * 6.259599685668945
Epoch 970, val loss: 0.9125639796257019
Epoch 980, training loss: 62.7620735168457 = 0.1633647382259369 + 10.0 * 6.259871006011963
Epoch 980, val loss: 0.9175544381141663
Epoch 990, training loss: 62.72618103027344 = 0.1576913446187973 + 10.0 * 6.2568488121032715
Epoch 990, val loss: 0.9224889874458313
Epoch 1000, training loss: 62.7086296081543 = 0.1523035615682602 + 10.0 * 6.2556328773498535
Epoch 1000, val loss: 0.9276443719863892
Epoch 1010, training loss: 62.712677001953125 = 0.1471429467201233 + 10.0 * 6.2565531730651855
Epoch 1010, val loss: 0.9329307079315186
Epoch 1020, training loss: 62.69523620605469 = 0.14213815331459045 + 10.0 * 6.255309581756592
Epoch 1020, val loss: 0.9382016062736511
Epoch 1030, training loss: 62.69008255004883 = 0.13732844591140747 + 10.0 * 6.255275249481201
Epoch 1030, val loss: 0.9436615705490112
Epoch 1040, training loss: 62.679386138916016 = 0.1327165812253952 + 10.0 * 6.254666805267334
Epoch 1040, val loss: 0.9491856098175049
Epoch 1050, training loss: 62.66941833496094 = 0.12831181287765503 + 10.0 * 6.254110813140869
Epoch 1050, val loss: 0.9548677802085876
Epoch 1060, training loss: 62.648799896240234 = 0.12407507747411728 + 10.0 * 6.252472400665283
Epoch 1060, val loss: 0.9606007933616638
Epoch 1070, training loss: 62.6756591796875 = 0.12000332772731781 + 10.0 * 6.255565643310547
Epoch 1070, val loss: 0.966451108455658
Epoch 1080, training loss: 62.66943359375 = 0.11608511954545975 + 10.0 * 6.255334854125977
Epoch 1080, val loss: 0.9723830819129944
Epoch 1090, training loss: 62.61870193481445 = 0.11232608556747437 + 10.0 * 6.250637531280518
Epoch 1090, val loss: 0.9783546924591064
Epoch 1100, training loss: 62.60663986206055 = 0.10874561220407486 + 10.0 * 6.249789237976074
Epoch 1100, val loss: 0.984467089176178
Epoch 1110, training loss: 62.59043502807617 = 0.10529453307390213 + 10.0 * 6.248514175415039
Epoch 1110, val loss: 0.9906695485115051
Epoch 1120, training loss: 62.58611297607422 = 0.10199999064207077 + 10.0 * 6.248411178588867
Epoch 1120, val loss: 0.996989369392395
Epoch 1130, training loss: 62.650413513183594 = 0.09882961958646774 + 10.0 * 6.255158424377441
Epoch 1130, val loss: 1.0033012628555298
Epoch 1140, training loss: 62.586822509765625 = 0.09570908546447754 + 10.0 * 6.249111175537109
Epoch 1140, val loss: 1.009552001953125
Epoch 1150, training loss: 62.582740783691406 = 0.09273795038461685 + 10.0 * 6.249000072479248
Epoch 1150, val loss: 1.0158113241195679
Epoch 1160, training loss: 62.54777908325195 = 0.08990111202001572 + 10.0 * 6.245787620544434
Epoch 1160, val loss: 1.022215723991394
Epoch 1170, training loss: 62.55976867675781 = 0.08719170093536377 + 10.0 * 6.247257709503174
Epoch 1170, val loss: 1.028620958328247
Epoch 1180, training loss: 62.558353424072266 = 0.08456659317016602 + 10.0 * 6.247378349304199
Epoch 1180, val loss: 1.0349820852279663
Epoch 1190, training loss: 62.540287017822266 = 0.0820770263671875 + 10.0 * 6.245820999145508
Epoch 1190, val loss: 1.0414648056030273
Epoch 1200, training loss: 62.5745964050293 = 0.07965419441461563 + 10.0 * 6.2494940757751465
Epoch 1200, val loss: 1.0478404760360718
Epoch 1210, training loss: 62.535308837890625 = 0.07729917019605637 + 10.0 * 6.245800971984863
Epoch 1210, val loss: 1.0541937351226807
Epoch 1220, training loss: 62.51905822753906 = 0.07507617771625519 + 10.0 * 6.24439811706543
Epoch 1220, val loss: 1.0607666969299316
Epoch 1230, training loss: 62.552791595458984 = 0.07291325181722641 + 10.0 * 6.247987747192383
Epoch 1230, val loss: 1.0670751333236694
Epoch 1240, training loss: 62.50605773925781 = 0.07083816081285477 + 10.0 * 6.243521690368652
Epoch 1240, val loss: 1.0735098123550415
Epoch 1250, training loss: 62.50100326538086 = 0.06882842630147934 + 10.0 * 6.243217468261719
Epoch 1250, val loss: 1.0799663066864014
Epoch 1260, training loss: 62.51399230957031 = 0.06690265238285065 + 10.0 * 6.244709014892578
Epoch 1260, val loss: 1.0863473415374756
Epoch 1270, training loss: 62.483116149902344 = 0.06506188958883286 + 10.0 * 6.241805076599121
Epoch 1270, val loss: 1.0928407907485962
Epoch 1280, training loss: 62.46568298339844 = 0.06327706575393677 + 10.0 * 6.240240573883057
Epoch 1280, val loss: 1.0993155241012573
Epoch 1290, training loss: 62.486602783203125 = 0.06157509982585907 + 10.0 * 6.242502689361572
Epoch 1290, val loss: 1.1057684421539307
Epoch 1300, training loss: 62.51158905029297 = 0.0599142424762249 + 10.0 * 6.2451677322387695
Epoch 1300, val loss: 1.1120576858520508
Epoch 1310, training loss: 62.468528747558594 = 0.05827315151691437 + 10.0 * 6.241025447845459
Epoch 1310, val loss: 1.1182570457458496
Epoch 1320, training loss: 62.44677734375 = 0.05674019083380699 + 10.0 * 6.239003658294678
Epoch 1320, val loss: 1.1245349645614624
Epoch 1330, training loss: 62.436729431152344 = 0.055259719491004944 + 10.0 * 6.238146781921387
Epoch 1330, val loss: 1.1308319568634033
Epoch 1340, training loss: 62.476318359375 = 0.05383562669157982 + 10.0 * 6.242248058319092
Epoch 1340, val loss: 1.1370000839233398
Epoch 1350, training loss: 62.44139862060547 = 0.05244453251361847 + 10.0 * 6.238895416259766
Epoch 1350, val loss: 1.1431444883346558
Epoch 1360, training loss: 62.43787384033203 = 0.05109351873397827 + 10.0 * 6.238677978515625
Epoch 1360, val loss: 1.149260401725769
Epoch 1370, training loss: 62.484107971191406 = 0.04980091378092766 + 10.0 * 6.2434306144714355
Epoch 1370, val loss: 1.1552735567092896
Epoch 1380, training loss: 62.420654296875 = 0.04855986312031746 + 10.0 * 6.237209320068359
Epoch 1380, val loss: 1.161421775817871
Epoch 1390, training loss: 62.40998840332031 = 0.047350287437438965 + 10.0 * 6.236263751983643
Epoch 1390, val loss: 1.1674273014068604
Epoch 1400, training loss: 62.39964294433594 = 0.046198852360248566 + 10.0 * 6.235344409942627
Epoch 1400, val loss: 1.1734570264816284
Epoch 1410, training loss: 62.42172622680664 = 0.04508945345878601 + 10.0 * 6.237663745880127
Epoch 1410, val loss: 1.179384469985962
Epoch 1420, training loss: 62.420318603515625 = 0.04400019720196724 + 10.0 * 6.237631797790527
Epoch 1420, val loss: 1.1853057146072388
Epoch 1430, training loss: 62.40306854248047 = 0.04292260482907295 + 10.0 * 6.236014366149902
Epoch 1430, val loss: 1.1908930540084839
Epoch 1440, training loss: 62.387699127197266 = 0.041907794773578644 + 10.0 * 6.234579086303711
Epoch 1440, val loss: 1.1968411207199097
Epoch 1450, training loss: 62.37858200073242 = 0.040937114506959915 + 10.0 * 6.2337646484375
Epoch 1450, val loss: 1.2026695013046265
Epoch 1460, training loss: 62.38810729980469 = 0.04000411927700043 + 10.0 * 6.2348103523254395
Epoch 1460, val loss: 1.2085307836532593
Epoch 1470, training loss: 62.40458297729492 = 0.03908909857273102 + 10.0 * 6.236549377441406
Epoch 1470, val loss: 1.2140847444534302
Epoch 1480, training loss: 62.399932861328125 = 0.038170091807842255 + 10.0 * 6.236176490783691
Epoch 1480, val loss: 1.2196279764175415
Epoch 1490, training loss: 62.36813735961914 = 0.0372999869287014 + 10.0 * 6.233083724975586
Epoch 1490, val loss: 1.2251685857772827
Epoch 1500, training loss: 62.35378646850586 = 0.036471690982580185 + 10.0 * 6.231731414794922
Epoch 1500, val loss: 1.2308650016784668
Epoch 1510, training loss: 62.34938430786133 = 0.03567051514983177 + 10.0 * 6.2313714027404785
Epoch 1510, val loss: 1.2364226579666138
Epoch 1520, training loss: 62.38208770751953 = 0.03490407392382622 + 10.0 * 6.234718322753906
Epoch 1520, val loss: 1.2420084476470947
Epoch 1530, training loss: 62.39921188354492 = 0.03414275497198105 + 10.0 * 6.236506938934326
Epoch 1530, val loss: 1.2472723722457886
Epoch 1540, training loss: 62.35853576660156 = 0.033381637185811996 + 10.0 * 6.232515335083008
Epoch 1540, val loss: 1.2525142431259155
Epoch 1550, training loss: 62.34025573730469 = 0.03266482055187225 + 10.0 * 6.230759143829346
Epoch 1550, val loss: 1.2578896284103394
Epoch 1560, training loss: 62.33689880371094 = 0.031984247267246246 + 10.0 * 6.230491638183594
Epoch 1560, val loss: 1.263216257095337
Epoch 1570, training loss: 62.384918212890625 = 0.03131232038140297 + 10.0 * 6.235360622406006
Epoch 1570, val loss: 1.268334150314331
Epoch 1580, training loss: 62.354007720947266 = 0.030658496543765068 + 10.0 * 6.232335090637207
Epoch 1580, val loss: 1.273550271987915
Epoch 1590, training loss: 62.33124923706055 = 0.030026614665985107 + 10.0 * 6.2301225662231445
Epoch 1590, val loss: 1.2787889242172241
Epoch 1600, training loss: 62.33091354370117 = 0.029414810240268707 + 10.0 * 6.230149745941162
Epoch 1600, val loss: 1.2838677167892456
Epoch 1610, training loss: 62.352413177490234 = 0.02882372960448265 + 10.0 * 6.232358932495117
Epoch 1610, val loss: 1.2889158725738525
Epoch 1620, training loss: 62.32027053833008 = 0.02823394536972046 + 10.0 * 6.229203701019287
Epoch 1620, val loss: 1.2937411069869995
Epoch 1630, training loss: 62.316497802734375 = 0.02767845243215561 + 10.0 * 6.2288818359375
Epoch 1630, val loss: 1.2987831830978394
Epoch 1640, training loss: 62.3729248046875 = 0.02712918631732464 + 10.0 * 6.234579563140869
Epoch 1640, val loss: 1.3035364151000977
Epoch 1650, training loss: 62.333065032958984 = 0.026602422818541527 + 10.0 * 6.230646133422852
Epoch 1650, val loss: 1.308611512184143
Epoch 1660, training loss: 62.30646896362305 = 0.026076151058077812 + 10.0 * 6.228039264678955
Epoch 1660, val loss: 1.3132659196853638
Epoch 1670, training loss: 62.295921325683594 = 0.025582458823919296 + 10.0 * 6.227034091949463
Epoch 1670, val loss: 1.3182315826416016
Epoch 1680, training loss: 62.31816101074219 = 0.025094974786043167 + 10.0 * 6.229306697845459
Epoch 1680, val loss: 1.3228330612182617
Epoch 1690, training loss: 62.30781555175781 = 0.02462277002632618 + 10.0 * 6.22831916809082
Epoch 1690, val loss: 1.327752947807312
Epoch 1700, training loss: 62.30552291870117 = 0.024160895496606827 + 10.0 * 6.22813606262207
Epoch 1700, val loss: 1.3323453664779663
Epoch 1710, training loss: 62.32624053955078 = 0.023704053834080696 + 10.0 * 6.23025369644165
Epoch 1710, val loss: 1.336941123008728
Epoch 1720, training loss: 62.28435134887695 = 0.023269690573215485 + 10.0 * 6.226108074188232
Epoch 1720, val loss: 1.34163236618042
Epoch 1730, training loss: 62.273353576660156 = 0.0228487066924572 + 10.0 * 6.225050449371338
Epoch 1730, val loss: 1.3462554216384888
Epoch 1740, training loss: 62.278411865234375 = 0.022444235160946846 + 10.0 * 6.2255964279174805
Epoch 1740, val loss: 1.3508323431015015
Epoch 1750, training loss: 62.33433151245117 = 0.02205117605626583 + 10.0 * 6.231227874755859
Epoch 1750, val loss: 1.35533607006073
Epoch 1760, training loss: 62.293827056884766 = 0.021640369668602943 + 10.0 * 6.2272186279296875
Epoch 1760, val loss: 1.3595181703567505
Epoch 1770, training loss: 62.26985168457031 = 0.02125656232237816 + 10.0 * 6.224859714508057
Epoch 1770, val loss: 1.3639200925827026
Epoch 1780, training loss: 62.2633171081543 = 0.02088780142366886 + 10.0 * 6.224242687225342
Epoch 1780, val loss: 1.368410348892212
Epoch 1790, training loss: 62.32157897949219 = 0.020528806373476982 + 10.0 * 6.230104923248291
Epoch 1790, val loss: 1.3726191520690918
Epoch 1800, training loss: 62.272911071777344 = 0.02017105743288994 + 10.0 * 6.225274085998535
Epoch 1800, val loss: 1.3768677711486816
Epoch 1810, training loss: 62.25335693359375 = 0.019829479977488518 + 10.0 * 6.223352909088135
Epoch 1810, val loss: 1.381165623664856
Epoch 1820, training loss: 62.24978256225586 = 0.01949901506304741 + 10.0 * 6.223028182983398
Epoch 1820, val loss: 1.3854421377182007
Epoch 1830, training loss: 62.32832717895508 = 0.019183918833732605 + 10.0 * 6.230914115905762
Epoch 1830, val loss: 1.389661192893982
Epoch 1840, training loss: 62.294281005859375 = 0.018847905099391937 + 10.0 * 6.227543354034424
Epoch 1840, val loss: 1.3936047554016113
Epoch 1850, training loss: 62.24678039550781 = 0.018533678725361824 + 10.0 * 6.222825050354004
Epoch 1850, val loss: 1.3977208137512207
Epoch 1860, training loss: 62.23304748535156 = 0.018229911103844643 + 10.0 * 6.221481800079346
Epoch 1860, val loss: 1.401820421218872
Epoch 1870, training loss: 62.25908279418945 = 0.017941130325198174 + 10.0 * 6.224114418029785
Epoch 1870, val loss: 1.4058020114898682
Epoch 1880, training loss: 62.24856185913086 = 0.01765768975019455 + 10.0 * 6.223090171813965
Epoch 1880, val loss: 1.409893274307251
Epoch 1890, training loss: 62.236183166503906 = 0.017368365079164505 + 10.0 * 6.22188138961792
Epoch 1890, val loss: 1.4136666059494019
Epoch 1900, training loss: 62.22230911254883 = 0.017091142013669014 + 10.0 * 6.220521926879883
Epoch 1900, val loss: 1.417710781097412
Epoch 1910, training loss: 62.22439193725586 = 0.016825178638100624 + 10.0 * 6.220756530761719
Epoch 1910, val loss: 1.4214848279953003
Epoch 1920, training loss: 62.28234100341797 = 0.016565969213843346 + 10.0 * 6.226577281951904
Epoch 1920, val loss: 1.4252456426620483
Epoch 1930, training loss: 62.21435546875 = 0.016310766339302063 + 10.0 * 6.219804286956787
Epoch 1930, val loss: 1.429128646850586
Epoch 1940, training loss: 62.216426849365234 = 0.0160631462931633 + 10.0 * 6.220036506652832
Epoch 1940, val loss: 1.4329760074615479
Epoch 1950, training loss: 62.23258590698242 = 0.01581941731274128 + 10.0 * 6.221676826477051
Epoch 1950, val loss: 1.4365609884262085
Epoch 1960, training loss: 62.22907638549805 = 0.015581265091896057 + 10.0 * 6.221349239349365
Epoch 1960, val loss: 1.4402717351913452
Epoch 1970, training loss: 62.20353698730469 = 0.015342400409281254 + 10.0 * 6.218819618225098
Epoch 1970, val loss: 1.4438170194625854
Epoch 1980, training loss: 62.20076370239258 = 0.015117405913770199 + 10.0 * 6.218564510345459
Epoch 1980, val loss: 1.4476544857025146
Epoch 1990, training loss: 62.225372314453125 = 0.014902987517416477 + 10.0 * 6.2210469245910645
Epoch 1990, val loss: 1.4512532949447632
Epoch 2000, training loss: 62.2158203125 = 0.014684234745800495 + 10.0 * 6.220113754272461
Epoch 2000, val loss: 1.4547702074050903
Epoch 2010, training loss: 62.201812744140625 = 0.014462003484368324 + 10.0 * 6.218735218048096
Epoch 2010, val loss: 1.4581451416015625
Epoch 2020, training loss: 62.20130920410156 = 0.014258947223424911 + 10.0 * 6.218705177307129
Epoch 2020, val loss: 1.4618836641311646
Epoch 2030, training loss: 62.214969635009766 = 0.014058994129300117 + 10.0 * 6.220091342926025
Epoch 2030, val loss: 1.4652639627456665
Epoch 2040, training loss: 62.217010498046875 = 0.01386337447911501 + 10.0 * 6.220314979553223
Epoch 2040, val loss: 1.4687532186508179
Epoch 2050, training loss: 62.20674133300781 = 0.013662967830896378 + 10.0 * 6.219307899475098
Epoch 2050, val loss: 1.471988558769226
Epoch 2060, training loss: 62.20901870727539 = 0.01347150094807148 + 10.0 * 6.219554901123047
Epoch 2060, val loss: 1.4752949476242065
Epoch 2070, training loss: 62.21940612792969 = 0.013287472538650036 + 10.0 * 6.220612049102783
Epoch 2070, val loss: 1.4787778854370117
Epoch 2080, training loss: 62.192527770996094 = 0.013100845739245415 + 10.0 * 6.217942714691162
Epoch 2080, val loss: 1.48186194896698
Epoch 2090, training loss: 62.20612335205078 = 0.01292418036609888 + 10.0 * 6.219319820404053
Epoch 2090, val loss: 1.4851725101470947
Epoch 2100, training loss: 62.18123245239258 = 0.012751995585858822 + 10.0 * 6.216847896575928
Epoch 2100, val loss: 1.488661527633667
Epoch 2110, training loss: 62.17841339111328 = 0.01258429978042841 + 10.0 * 6.216582775115967
Epoch 2110, val loss: 1.4919800758361816
Epoch 2120, training loss: 62.24968719482422 = 0.012424523942172527 + 10.0 * 6.223726272583008
Epoch 2120, val loss: 1.495134711265564
Epoch 2130, training loss: 62.18145751953125 = 0.01224926020950079 + 10.0 * 6.216920852661133
Epoch 2130, val loss: 1.4981474876403809
Epoch 2140, training loss: 62.16579055786133 = 0.012088378891348839 + 10.0 * 6.215370178222656
Epoch 2140, val loss: 1.5013939142227173
Epoch 2150, training loss: 62.16648483276367 = 0.011934694834053516 + 10.0 * 6.215455055236816
Epoch 2150, val loss: 1.5045784711837769
Epoch 2160, training loss: 62.202701568603516 = 0.011784028261899948 + 10.0 * 6.219091892242432
Epoch 2160, val loss: 1.5075926780700684
Epoch 2170, training loss: 62.16529083251953 = 0.011631917208433151 + 10.0 * 6.215365886688232
Epoch 2170, val loss: 1.510712742805481
Epoch 2180, training loss: 62.17078399658203 = 0.011480236425995827 + 10.0 * 6.215930461883545
Epoch 2180, val loss: 1.5137642621994019
Epoch 2190, training loss: 62.17620086669922 = 0.011338292621076107 + 10.0 * 6.21648645401001
Epoch 2190, val loss: 1.5169671773910522
Epoch 2200, training loss: 62.1779670715332 = 0.011195948347449303 + 10.0 * 6.216677188873291
Epoch 2200, val loss: 1.5198763608932495
Epoch 2210, training loss: 62.155941009521484 = 0.011057380586862564 + 10.0 * 6.2144880294799805
Epoch 2210, val loss: 1.5229425430297852
Epoch 2220, training loss: 62.2348747253418 = 0.010926761664450169 + 10.0 * 6.222394943237305
Epoch 2220, val loss: 1.5259851217269897
Epoch 2230, training loss: 62.189762115478516 = 0.010785847902297974 + 10.0 * 6.217897891998291
Epoch 2230, val loss: 1.528678297996521
Epoch 2240, training loss: 62.16965103149414 = 0.010653424076735973 + 10.0 * 6.215899467468262
Epoch 2240, val loss: 1.5316953659057617
Epoch 2250, training loss: 62.14558792114258 = 0.010522888042032719 + 10.0 * 6.21350622177124
Epoch 2250, val loss: 1.5345927476882935
Epoch 2260, training loss: 62.1406364440918 = 0.010401043109595776 + 10.0 * 6.2130231857299805
Epoch 2260, val loss: 1.5375224351882935
Epoch 2270, training loss: 62.1533317565918 = 0.010279812850058079 + 10.0 * 6.2143049240112305
Epoch 2270, val loss: 1.54025137424469
Epoch 2280, training loss: 62.174591064453125 = 0.010156474076211452 + 10.0 * 6.2164435386657715
Epoch 2280, val loss: 1.5428330898284912
Epoch 2290, training loss: 62.14603805541992 = 0.010035700164735317 + 10.0 * 6.213600158691406
Epoch 2290, val loss: 1.5456607341766357
Epoch 2300, training loss: 62.21196746826172 = 0.009917416609823704 + 10.0 * 6.220204830169678
Epoch 2300, val loss: 1.5484368801116943
Epoch 2310, training loss: 62.14250564575195 = 0.009801669977605343 + 10.0 * 6.213270664215088
Epoch 2310, val loss: 1.5510956048965454
Epoch 2320, training loss: 62.128074645996094 = 0.009687675163149834 + 10.0 * 6.211838722229004
Epoch 2320, val loss: 1.5538386106491089
Epoch 2330, training loss: 62.12778091430664 = 0.009579779580235481 + 10.0 * 6.211820125579834
Epoch 2330, val loss: 1.5565450191497803
Epoch 2340, training loss: 62.12722396850586 = 0.009476328268647194 + 10.0 * 6.211774826049805
Epoch 2340, val loss: 1.559218168258667
Epoch 2350, training loss: 62.184059143066406 = 0.009374753572046757 + 10.0 * 6.21746826171875
Epoch 2350, val loss: 1.561748743057251
Epoch 2360, training loss: 62.16117858886719 = 0.009265896864235401 + 10.0 * 6.21519136428833
Epoch 2360, val loss: 1.5640759468078613
Epoch 2370, training loss: 62.15286636352539 = 0.009161540307104588 + 10.0 * 6.214370250701904
Epoch 2370, val loss: 1.5667600631713867
Epoch 2380, training loss: 62.14314651489258 = 0.00905965082347393 + 10.0 * 6.213408470153809
Epoch 2380, val loss: 1.5692663192749023
Epoch 2390, training loss: 62.12925720214844 = 0.008963360451161861 + 10.0 * 6.212029457092285
Epoch 2390, val loss: 1.5720691680908203
Epoch 2400, training loss: 62.13077163696289 = 0.008869190700352192 + 10.0 * 6.2121901512146
Epoch 2400, val loss: 1.574644923210144
Epoch 2410, training loss: 62.150150299072266 = 0.008775333873927593 + 10.0 * 6.214137554168701
Epoch 2410, val loss: 1.5770363807678223
Epoch 2420, training loss: 62.13443374633789 = 0.008676809258759022 + 10.0 * 6.212575435638428
Epoch 2420, val loss: 1.5794062614440918
Epoch 2430, training loss: 62.1265983581543 = 0.008584735915064812 + 10.0 * 6.211801528930664
Epoch 2430, val loss: 1.5819464921951294
Epoch 2440, training loss: 62.149879455566406 = 0.0084977513179183 + 10.0 * 6.214138031005859
Epoch 2440, val loss: 1.5843967199325562
Epoch 2450, training loss: 62.11985778808594 = 0.008407091721892357 + 10.0 * 6.211144924163818
Epoch 2450, val loss: 1.586798906326294
Epoch 2460, training loss: 62.1078987121582 = 0.008319101296365261 + 10.0 * 6.209958076477051
Epoch 2460, val loss: 1.5892006158828735
Epoch 2470, training loss: 62.12351989746094 = 0.008236597292125225 + 10.0 * 6.211528301239014
Epoch 2470, val loss: 1.5917818546295166
Epoch 2480, training loss: 62.13902282714844 = 0.008154249750077724 + 10.0 * 6.2130866050720215
Epoch 2480, val loss: 1.5939176082611084
Epoch 2490, training loss: 62.111900329589844 = 0.008066066540777683 + 10.0 * 6.210383415222168
Epoch 2490, val loss: 1.5962425470352173
Epoch 2500, training loss: 62.098392486572266 = 0.007985357195138931 + 10.0 * 6.209040641784668
Epoch 2500, val loss: 1.5985586643218994
Epoch 2510, training loss: 62.0970344543457 = 0.007907276973128319 + 10.0 * 6.2089128494262695
Epoch 2510, val loss: 1.6008806228637695
Epoch 2520, training loss: 62.18519973754883 = 0.007833297364413738 + 10.0 * 6.217736721038818
Epoch 2520, val loss: 1.6032655239105225
Epoch 2530, training loss: 62.1579475402832 = 0.007748000789433718 + 10.0 * 6.215020179748535
Epoch 2530, val loss: 1.6049436330795288
Epoch 2540, training loss: 62.108787536621094 = 0.007669610902667046 + 10.0 * 6.210111618041992
Epoch 2540, val loss: 1.6074559688568115
Epoch 2550, training loss: 62.0936279296875 = 0.007596650626510382 + 10.0 * 6.208603382110596
Epoch 2550, val loss: 1.6098666191101074
Epoch 2560, training loss: 62.09379959106445 = 0.007526082918047905 + 10.0 * 6.208627223968506
Epoch 2560, val loss: 1.6120710372924805
Epoch 2570, training loss: 62.11136245727539 = 0.007457428146153688 + 10.0 * 6.210390567779541
Epoch 2570, val loss: 1.6143717765808105
Epoch 2580, training loss: 62.11088943481445 = 0.007385726552456617 + 10.0 * 6.210350513458252
Epoch 2580, val loss: 1.6164218187332153
Epoch 2590, training loss: 62.147945404052734 = 0.007314303889870644 + 10.0 * 6.2140631675720215
Epoch 2590, val loss: 1.618619441986084
Epoch 2600, training loss: 62.1045036315918 = 0.007242891006171703 + 10.0 * 6.209725856781006
Epoch 2600, val loss: 1.6205565929412842
Epoch 2610, training loss: 62.09636688232422 = 0.007172712590545416 + 10.0 * 6.208919525146484
Epoch 2610, val loss: 1.622686743736267
Epoch 2620, training loss: 62.100440979003906 = 0.007107370533049107 + 10.0 * 6.209333419799805
Epoch 2620, val loss: 1.6248441934585571
Epoch 2630, training loss: 62.086021423339844 = 0.0070419940166175365 + 10.0 * 6.207898139953613
Epoch 2630, val loss: 1.626939296722412
Epoch 2640, training loss: 62.07915115356445 = 0.006978477817028761 + 10.0 * 6.207217216491699
Epoch 2640, val loss: 1.6290191411972046
Epoch 2650, training loss: 62.11604309082031 = 0.006916245445609093 + 10.0 * 6.210912704467773
Epoch 2650, val loss: 1.6309009790420532
Epoch 2660, training loss: 62.11354064941406 = 0.006851948797702789 + 10.0 * 6.210669040679932
Epoch 2660, val loss: 1.6328431367874146
Epoch 2670, training loss: 62.081298828125 = 0.006790568120777607 + 10.0 * 6.207450866699219
Epoch 2670, val loss: 1.635049819946289
Epoch 2680, training loss: 62.08121871948242 = 0.006728829815983772 + 10.0 * 6.207448959350586
Epoch 2680, val loss: 1.6369030475616455
Epoch 2690, training loss: 62.08604431152344 = 0.006670628674328327 + 10.0 * 6.207937240600586
Epoch 2690, val loss: 1.6389130353927612
Epoch 2700, training loss: 62.079803466796875 = 0.006613526493310928 + 10.0 * 6.2073187828063965
Epoch 2700, val loss: 1.64093816280365
Epoch 2710, training loss: 62.0992431640625 = 0.006557156797498465 + 10.0 * 6.209268569946289
Epoch 2710, val loss: 1.6428669691085815
Epoch 2720, training loss: 62.095821380615234 = 0.006497654132544994 + 10.0 * 6.208932399749756
Epoch 2720, val loss: 1.6447309255599976
Epoch 2730, training loss: 62.086421966552734 = 0.006440779194235802 + 10.0 * 6.207998275756836
Epoch 2730, val loss: 1.6466082334518433
Epoch 2740, training loss: 62.093257904052734 = 0.00638654874637723 + 10.0 * 6.2086873054504395
Epoch 2740, val loss: 1.6485041379928589
Epoch 2750, training loss: 62.07197189331055 = 0.0063301860354840755 + 10.0 * 6.206564426422119
Epoch 2750, val loss: 1.6503660678863525
Epoch 2760, training loss: 62.061492919921875 = 0.006278254557400942 + 10.0 * 6.205521583557129
Epoch 2760, val loss: 1.6522433757781982
Epoch 2770, training loss: 62.076133728027344 = 0.006227185018360615 + 10.0 * 6.206990718841553
Epoch 2770, val loss: 1.653965711593628
Epoch 2780, training loss: 62.10359191894531 = 0.006173531524837017 + 10.0 * 6.209742069244385
Epoch 2780, val loss: 1.65570867061615
Epoch 2790, training loss: 62.070655822753906 = 0.006123053375631571 + 10.0 * 6.206453323364258
Epoch 2790, val loss: 1.6576168537139893
Epoch 2800, training loss: 62.06028366088867 = 0.006070602685213089 + 10.0 * 6.205421447753906
Epoch 2800, val loss: 1.6593945026397705
Epoch 2810, training loss: 62.0756721496582 = 0.0060236575081944466 + 10.0 * 6.20696496963501
Epoch 2810, val loss: 1.6612063646316528
Epoch 2820, training loss: 62.0740966796875 = 0.0059735518880188465 + 10.0 * 6.206812381744385
Epoch 2820, val loss: 1.6629281044006348
Epoch 2830, training loss: 62.12515640258789 = 0.0059249564073979855 + 10.0 * 6.211923122406006
Epoch 2830, val loss: 1.6647007465362549
Epoch 2840, training loss: 62.07261657714844 = 0.005876351147890091 + 10.0 * 6.206674098968506
Epoch 2840, val loss: 1.666284441947937
Epoch 2850, training loss: 62.054012298583984 = 0.005830127280205488 + 10.0 * 6.204818248748779
Epoch 2850, val loss: 1.6682164669036865
Epoch 2860, training loss: 62.048892974853516 = 0.00578615628182888 + 10.0 * 6.204310417175293
Epoch 2860, val loss: 1.6699261665344238
Epoch 2870, training loss: 62.06034469604492 = 0.0057442267425358295 + 10.0 * 6.205460071563721
Epoch 2870, val loss: 1.6717300415039062
Epoch 2880, training loss: 62.09894561767578 = 0.005700941663235426 + 10.0 * 6.209324836730957
Epoch 2880, val loss: 1.6732120513916016
Epoch 2890, training loss: 62.083045959472656 = 0.005651514511555433 + 10.0 * 6.207739353179932
Epoch 2890, val loss: 1.674557089805603
Epoch 2900, training loss: 62.085052490234375 = 0.0056076450273394585 + 10.0 * 6.207944393157959
Epoch 2900, val loss: 1.6763371229171753
Epoch 2910, training loss: 62.05203628540039 = 0.005562508013099432 + 10.0 * 6.204647541046143
Epoch 2910, val loss: 1.6779581308364868
Epoch 2920, training loss: 62.04458999633789 = 0.005521921906620264 + 10.0 * 6.203906536102295
Epoch 2920, val loss: 1.6796027421951294
Epoch 2930, training loss: 62.037132263183594 = 0.005481424741446972 + 10.0 * 6.203165054321289
Epoch 2930, val loss: 1.6812113523483276
Epoch 2940, training loss: 62.04751205444336 = 0.005442316178232431 + 10.0 * 6.204206943511963
Epoch 2940, val loss: 1.682790756225586
Epoch 2950, training loss: 62.12066650390625 = 0.005400916561484337 + 10.0 * 6.211526393890381
Epoch 2950, val loss: 1.6841970682144165
Epoch 2960, training loss: 62.07148361206055 = 0.005360282026231289 + 10.0 * 6.206612586975098
Epoch 2960, val loss: 1.6856286525726318
Epoch 2970, training loss: 62.04884338378906 = 0.005319070536643267 + 10.0 * 6.204352378845215
Epoch 2970, val loss: 1.6872403621673584
Epoch 2980, training loss: 62.057735443115234 = 0.005281286314129829 + 10.0 * 6.205245494842529
Epoch 2980, val loss: 1.6887184381484985
Epoch 2990, training loss: 62.04510498046875 = 0.005242237355560064 + 10.0 * 6.203986167907715
Epoch 2990, val loss: 1.690091848373413
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 87.91119384765625 = 1.9424970149993896 + 10.0 * 8.596869468688965
Epoch 0, val loss: 1.943359613418579
Epoch 10, training loss: 87.89715576171875 = 1.932838797569275 + 10.0 * 8.596431732177734
Epoch 10, val loss: 1.9331047534942627
Epoch 20, training loss: 87.84800720214844 = 1.921567440032959 + 10.0 * 8.592643737792969
Epoch 20, val loss: 1.9208449125289917
Epoch 30, training loss: 87.55268096923828 = 1.907278060913086 + 10.0 * 8.564539909362793
Epoch 30, val loss: 1.9053279161453247
Epoch 40, training loss: 86.03762817382812 = 1.889260172843933 + 10.0 * 8.414836883544922
Epoch 40, val loss: 1.886751651763916
Epoch 50, training loss: 82.4376220703125 = 1.868865728378296 + 10.0 * 8.056875228881836
Epoch 50, val loss: 1.8670237064361572
Epoch 60, training loss: 79.59041595458984 = 1.8504184484481812 + 10.0 * 7.7739996910095215
Epoch 60, val loss: 1.8504852056503296
Epoch 70, training loss: 75.14338684082031 = 1.8371742963790894 + 10.0 * 7.330621242523193
Epoch 70, val loss: 1.8382216691970825
Epoch 80, training loss: 72.44585418701172 = 1.8278008699417114 + 10.0 * 7.06180477142334
Epoch 80, val loss: 1.8288819789886475
Epoch 90, training loss: 71.12108612060547 = 1.8174258470535278 + 10.0 * 6.930366039276123
Epoch 90, val loss: 1.818725824356079
Epoch 100, training loss: 70.24778747558594 = 1.805092215538025 + 10.0 * 6.844269275665283
Epoch 100, val loss: 1.8074398040771484
Epoch 110, training loss: 69.64042663574219 = 1.7932946681976318 + 10.0 * 6.784713268280029
Epoch 110, val loss: 1.7966992855072021
Epoch 120, training loss: 69.09941864013672 = 1.7820234298706055 + 10.0 * 6.731739521026611
Epoch 120, val loss: 1.786305546760559
Epoch 130, training loss: 68.61083221435547 = 1.7709400653839111 + 10.0 * 6.68398904800415
Epoch 130, val loss: 1.775819182395935
Epoch 140, training loss: 68.15677642822266 = 1.759063482284546 + 10.0 * 6.639771461486816
Epoch 140, val loss: 1.764787197113037
Epoch 150, training loss: 67.78705596923828 = 1.7460193634033203 + 10.0 * 6.604104042053223
Epoch 150, val loss: 1.7527893781661987
Epoch 160, training loss: 67.470458984375 = 1.7315913438796997 + 10.0 * 6.573886871337891
Epoch 160, val loss: 1.739708423614502
Epoch 170, training loss: 67.21114349365234 = 1.7159806489944458 + 10.0 * 6.549516201019287
Epoch 170, val loss: 1.7255584001541138
Epoch 180, training loss: 66.98062896728516 = 1.6989476680755615 + 10.0 * 6.528168201446533
Epoch 180, val loss: 1.7102020978927612
Epoch 190, training loss: 66.78739929199219 = 1.6801795959472656 + 10.0 * 6.510721683502197
Epoch 190, val loss: 1.6934219598770142
Epoch 200, training loss: 66.60733795166016 = 1.6596806049346924 + 10.0 * 6.494765758514404
Epoch 200, val loss: 1.6752184629440308
Epoch 210, training loss: 66.43012237548828 = 1.6373984813690186 + 10.0 * 6.479272842407227
Epoch 210, val loss: 1.6554797887802124
Epoch 220, training loss: 66.29070281982422 = 1.6131891012191772 + 10.0 * 6.467751502990723
Epoch 220, val loss: 1.6340457201004028
Epoch 230, training loss: 66.15769958496094 = 1.5871286392211914 + 10.0 * 6.457056999206543
Epoch 230, val loss: 1.6110467910766602
Epoch 240, training loss: 66.01263427734375 = 1.559236764907837 + 10.0 * 6.445339679718018
Epoch 240, val loss: 1.5865050554275513
Epoch 250, training loss: 65.89179992675781 = 1.529704213142395 + 10.0 * 6.436209678649902
Epoch 250, val loss: 1.5605441331863403
Epoch 260, training loss: 65.77770233154297 = 1.4984585046768188 + 10.0 * 6.427924633026123
Epoch 260, val loss: 1.5332605838775635
Epoch 270, training loss: 65.70380401611328 = 1.4657686948776245 + 10.0 * 6.423803329467773
Epoch 270, val loss: 1.5047578811645508
Epoch 280, training loss: 65.56356811523438 = 1.4319957494735718 + 10.0 * 6.413156986236572
Epoch 280, val loss: 1.475674033164978
Epoch 290, training loss: 65.45545959472656 = 1.397247552871704 + 10.0 * 6.405820846557617
Epoch 290, val loss: 1.4459959268569946
Epoch 300, training loss: 65.35688018798828 = 1.362133264541626 + 10.0 * 6.399474620819092
Epoch 300, val loss: 1.4161103963851929
Epoch 310, training loss: 65.27397918701172 = 1.3267239332199097 + 10.0 * 6.394725322723389
Epoch 310, val loss: 1.3863420486450195
Epoch 320, training loss: 65.18075561523438 = 1.2915380001068115 + 10.0 * 6.388922214508057
Epoch 320, val loss: 1.357027530670166
Epoch 330, training loss: 65.09551239013672 = 1.256800651550293 + 10.0 * 6.383871078491211
Epoch 330, val loss: 1.3284666538238525
Epoch 340, training loss: 65.02095794677734 = 1.2226661443710327 + 10.0 * 6.379829406738281
Epoch 340, val loss: 1.3007482290267944
Epoch 350, training loss: 64.99500274658203 = 1.1890674829483032 + 10.0 * 6.380593776702881
Epoch 350, val loss: 1.273742437362671
Epoch 360, training loss: 64.87812805175781 = 1.1565402746200562 + 10.0 * 6.372159004211426
Epoch 360, val loss: 1.2480385303497314
Epoch 370, training loss: 64.78845977783203 = 1.1249724626541138 + 10.0 * 6.366349220275879
Epoch 370, val loss: 1.2235147953033447
Epoch 380, training loss: 64.7152328491211 = 1.094180703163147 + 10.0 * 6.362105369567871
Epoch 380, val loss: 1.1999669075012207
Epoch 390, training loss: 64.68328857421875 = 1.0641335248947144 + 10.0 * 6.361915588378906
Epoch 390, val loss: 1.1773563623428345
Epoch 400, training loss: 64.57543182373047 = 1.0347555875778198 + 10.0 * 6.354067802429199
Epoch 400, val loss: 1.1555895805358887
Epoch 410, training loss: 64.51692199707031 = 1.0060327053070068 + 10.0 * 6.351089000701904
Epoch 410, val loss: 1.134660005569458
Epoch 420, training loss: 64.4964599609375 = 0.9777809381484985 + 10.0 * 6.351868152618408
Epoch 420, val loss: 1.1144962310791016
Epoch 430, training loss: 64.4219970703125 = 0.9500687122344971 + 10.0 * 6.347193241119385
Epoch 430, val loss: 1.0947799682617188
Epoch 440, training loss: 64.3410415649414 = 0.9228351712226868 + 10.0 * 6.34182071685791
Epoch 440, val loss: 1.0758198499679565
Epoch 450, training loss: 64.28276062011719 = 0.8960512280464172 + 10.0 * 6.33867073059082
Epoch 450, val loss: 1.0573678016662598
Epoch 460, training loss: 64.24056243896484 = 0.8695937395095825 + 10.0 * 6.33709716796875
Epoch 460, val loss: 1.039309024810791
Epoch 470, training loss: 64.22221374511719 = 0.8433888554573059 + 10.0 * 6.3378825187683105
Epoch 470, val loss: 1.021857500076294
Epoch 480, training loss: 64.13086700439453 = 0.817670464515686 + 10.0 * 6.331319808959961
Epoch 480, val loss: 1.0045884847640991
Epoch 490, training loss: 64.07577514648438 = 0.7924569249153137 + 10.0 * 6.32833194732666
Epoch 490, val loss: 0.9880720376968384
Epoch 500, training loss: 64.04280090332031 = 0.7676544189453125 + 10.0 * 6.327514171600342
Epoch 500, val loss: 0.9720188975334167
Epoch 510, training loss: 64.01564025878906 = 0.7432447671890259 + 10.0 * 6.327239036560059
Epoch 510, val loss: 0.9566481709480286
Epoch 520, training loss: 63.93796157836914 = 0.7193467020988464 + 10.0 * 6.321861743927002
Epoch 520, val loss: 0.9417662024497986
Epoch 530, training loss: 63.89179992675781 = 0.6960163116455078 + 10.0 * 6.319578647613525
Epoch 530, val loss: 0.9275690317153931
Epoch 540, training loss: 63.96621322631836 = 0.6732369661331177 + 10.0 * 6.3292975425720215
Epoch 540, val loss: 0.9141976833343506
Epoch 550, training loss: 63.8050422668457 = 0.6509061455726624 + 10.0 * 6.315413475036621
Epoch 550, val loss: 0.9012698531150818
Epoch 560, training loss: 63.76368713378906 = 0.6292580366134644 + 10.0 * 6.313443183898926
Epoch 560, val loss: 0.8892882466316223
Epoch 570, training loss: 63.727088928222656 = 0.6082258224487305 + 10.0 * 6.311886310577393
Epoch 570, val loss: 0.8780444264411926
Epoch 580, training loss: 63.74259948730469 = 0.5878005623817444 + 10.0 * 6.3154802322387695
Epoch 580, val loss: 0.8676456809043884
Epoch 590, training loss: 63.71620178222656 = 0.5677841901779175 + 10.0 * 6.3148417472839355
Epoch 590, val loss: 0.8576592206954956
Epoch 600, training loss: 63.62386703491211 = 0.548401415348053 + 10.0 * 6.307546615600586
Epoch 600, val loss: 0.8486031889915466
Epoch 610, training loss: 63.57814407348633 = 0.5297447443008423 + 10.0 * 6.304840087890625
Epoch 610, val loss: 0.8405067920684814
Epoch 620, training loss: 63.5443229675293 = 0.5116563439369202 + 10.0 * 6.303266525268555
Epoch 620, val loss: 0.8330584168434143
Epoch 630, training loss: 63.536865234375 = 0.4941011369228363 + 10.0 * 6.304276466369629
Epoch 630, val loss: 0.8264143466949463
Epoch 640, training loss: 63.47639846801758 = 0.4770180583000183 + 10.0 * 6.299938201904297
Epoch 640, val loss: 0.8204101920127869
Epoch 650, training loss: 63.50294876098633 = 0.4605013430118561 + 10.0 * 6.304244518280029
Epoch 650, val loss: 0.8151443600654602
Epoch 660, training loss: 63.43395233154297 = 0.4444168508052826 + 10.0 * 6.298953533172607
Epoch 660, val loss: 0.810481071472168
Epoch 670, training loss: 63.41094970703125 = 0.42895665764808655 + 10.0 * 6.298199653625488
Epoch 670, val loss: 0.806622326374054
Epoch 680, training loss: 63.36536407470703 = 0.4139343202114105 + 10.0 * 6.295143127441406
Epoch 680, val loss: 0.8033788800239563
Epoch 690, training loss: 63.34568405151367 = 0.3994225263595581 + 10.0 * 6.294626235961914
Epoch 690, val loss: 0.8007370829582214
Epoch 700, training loss: 63.30869674682617 = 0.38533636927604675 + 10.0 * 6.2923359870910645
Epoch 700, val loss: 0.7987294793128967
Epoch 710, training loss: 63.30253982543945 = 0.37167003750801086 + 10.0 * 6.293087005615234
Epoch 710, val loss: 0.7972331047058105
Epoch 720, training loss: 63.265968322753906 = 0.35838478803634644 + 10.0 * 6.2907586097717285
Epoch 720, val loss: 0.7962274551391602
Epoch 730, training loss: 63.26272201538086 = 0.34551241993904114 + 10.0 * 6.291720867156982
Epoch 730, val loss: 0.7958354353904724
Epoch 740, training loss: 63.22421646118164 = 0.3329867422580719 + 10.0 * 6.289123058319092
Epoch 740, val loss: 0.7958150506019592
Epoch 750, training loss: 63.24861145019531 = 0.3208450376987457 + 10.0 * 6.292776584625244
Epoch 750, val loss: 0.7963383793830872
Epoch 760, training loss: 63.17284393310547 = 0.3091100752353668 + 10.0 * 6.286373615264893
Epoch 760, val loss: 0.7971964478492737
Epoch 770, training loss: 63.139041900634766 = 0.29767948389053345 + 10.0 * 6.2841362953186035
Epoch 770, val loss: 0.7987151145935059
Epoch 780, training loss: 63.11930847167969 = 0.2866579294204712 + 10.0 * 6.283265113830566
Epoch 780, val loss: 0.800621747970581
Epoch 790, training loss: 63.159908294677734 = 0.27596449851989746 + 10.0 * 6.288394451141357
Epoch 790, val loss: 0.8028696179389954
Epoch 800, training loss: 63.08196258544922 = 0.26556405425071716 + 10.0 * 6.28164005279541
Epoch 800, val loss: 0.8053988218307495
Epoch 810, training loss: 63.05281066894531 = 0.2555454969406128 + 10.0 * 6.279726505279541
Epoch 810, val loss: 0.8084020018577576
Epoch 820, training loss: 63.0325927734375 = 0.24589845538139343 + 10.0 * 6.278669357299805
Epoch 820, val loss: 0.8118084669113159
Epoch 830, training loss: 63.098812103271484 = 0.2365489900112152 + 10.0 * 6.286226272583008
Epoch 830, val loss: 0.8154586553573608
Epoch 840, training loss: 63.01066589355469 = 0.22758662700653076 + 10.0 * 6.278307914733887
Epoch 840, val loss: 0.8191909193992615
Epoch 850, training loss: 62.976097106933594 = 0.21893377602100372 + 10.0 * 6.275716304779053
Epoch 850, val loss: 0.8233657479286194
Epoch 860, training loss: 62.98865509033203 = 0.21065878868103027 + 10.0 * 6.277799606323242
Epoch 860, val loss: 0.8278139233589172
Epoch 870, training loss: 62.9586181640625 = 0.20265398919582367 + 10.0 * 6.2755961418151855
Epoch 870, val loss: 0.8323887586593628
Epoch 880, training loss: 62.94242477416992 = 0.19500020146369934 + 10.0 * 6.274742603302002
Epoch 880, val loss: 0.8372229933738708
Epoch 890, training loss: 62.910118103027344 = 0.18766652047634125 + 10.0 * 6.272244930267334
Epoch 890, val loss: 0.8423184156417847
Epoch 900, training loss: 62.91197204589844 = 0.18064314126968384 + 10.0 * 6.273132801055908
Epoch 900, val loss: 0.8476080298423767
Epoch 910, training loss: 62.90529251098633 = 0.1739177107810974 + 10.0 * 6.27313756942749
Epoch 910, val loss: 0.8529635667800903
Epoch 920, training loss: 62.88444519042969 = 0.16743573546409607 + 10.0 * 6.271700859069824
Epoch 920, val loss: 0.8586063385009766
Epoch 930, training loss: 62.86382293701172 = 0.16124796867370605 + 10.0 * 6.270257472991943
Epoch 930, val loss: 0.8642833232879639
Epoch 940, training loss: 62.84804153442383 = 0.15534456074237823 + 10.0 * 6.2692694664001465
Epoch 940, val loss: 0.8701891899108887
Epoch 950, training loss: 62.82963562011719 = 0.14967462420463562 + 10.0 * 6.267996311187744
Epoch 950, val loss: 0.8762667775154114
Epoch 960, training loss: 62.803855895996094 = 0.144273042678833 + 10.0 * 6.265958309173584
Epoch 960, val loss: 0.8824539184570312
Epoch 970, training loss: 62.81547164916992 = 0.1391253024339676 + 10.0 * 6.267634391784668
Epoch 970, val loss: 0.8886956572532654
Epoch 980, training loss: 62.8206901550293 = 0.1341501921415329 + 10.0 * 6.268653869628906
Epoch 980, val loss: 0.8950516581535339
Epoch 990, training loss: 62.804019927978516 = 0.1293882578611374 + 10.0 * 6.267463207244873
Epoch 990, val loss: 0.9013294577598572
Epoch 1000, training loss: 62.76255798339844 = 0.12485215067863464 + 10.0 * 6.263770580291748
Epoch 1000, val loss: 0.9079979658126831
Epoch 1010, training loss: 62.746917724609375 = 0.12053816020488739 + 10.0 * 6.262638092041016
Epoch 1010, val loss: 0.9148164391517639
Epoch 1020, training loss: 62.83005905151367 = 0.11641467362642288 + 10.0 * 6.271364688873291
Epoch 1020, val loss: 0.9215139746665955
Epoch 1030, training loss: 62.77146911621094 = 0.11243374645709991 + 10.0 * 6.265903472900391
Epoch 1030, val loss: 0.9280866980552673
Epoch 1040, training loss: 62.76333236694336 = 0.10862694680690765 + 10.0 * 6.265470504760742
Epoch 1040, val loss: 0.9349082112312317
Epoch 1050, training loss: 62.725921630859375 = 0.10499229282140732 + 10.0 * 6.2620930671691895
Epoch 1050, val loss: 0.941784679889679
Epoch 1060, training loss: 62.695411682128906 = 0.10152895748615265 + 10.0 * 6.259388446807861
Epoch 1060, val loss: 0.9487246870994568
Epoch 1070, training loss: 62.68938064575195 = 0.0982157289981842 + 10.0 * 6.259116172790527
Epoch 1070, val loss: 0.9557378888130188
Epoch 1080, training loss: 62.712066650390625 = 0.09503263980150223 + 10.0 * 6.2617034912109375
Epoch 1080, val loss: 0.9628182649612427
Epoch 1090, training loss: 62.72792434692383 = 0.09196144342422485 + 10.0 * 6.263596534729004
Epoch 1090, val loss: 0.9697234034538269
Epoch 1100, training loss: 62.68600082397461 = 0.0890343189239502 + 10.0 * 6.2596964836120605
Epoch 1100, val loss: 0.9765632748603821
Epoch 1110, training loss: 62.65900802612305 = 0.0862220972776413 + 10.0 * 6.2572784423828125
Epoch 1110, val loss: 0.9837520122528076
Epoch 1120, training loss: 62.6567268371582 = 0.08355192095041275 + 10.0 * 6.257317543029785
Epoch 1120, val loss: 0.9908008575439453
Epoch 1130, training loss: 62.667911529541016 = 0.08096656948328018 + 10.0 * 6.258694648742676
Epoch 1130, val loss: 0.9978487491607666
Epoch 1140, training loss: 62.64626693725586 = 0.07847391068935394 + 10.0 * 6.256779670715332
Epoch 1140, val loss: 1.0047805309295654
Epoch 1150, training loss: 62.6210823059082 = 0.07609138637781143 + 10.0 * 6.2544989585876465
Epoch 1150, val loss: 1.0119116306304932
Epoch 1160, training loss: 62.61793899536133 = 0.07381218671798706 + 10.0 * 6.254412651062012
Epoch 1160, val loss: 1.0190579891204834
Epoch 1170, training loss: 62.65168762207031 = 0.07163761556148529 + 10.0 * 6.258005142211914
Epoch 1170, val loss: 1.0258991718292236
Epoch 1180, training loss: 62.62300491333008 = 0.06952565908432007 + 10.0 * 6.255347728729248
Epoch 1180, val loss: 1.0330520868301392
Epoch 1190, training loss: 62.61213684082031 = 0.06749832630157471 + 10.0 * 6.254464149475098
Epoch 1190, val loss: 1.040006160736084
Epoch 1200, training loss: 62.585453033447266 = 0.06556389480829239 + 10.0 * 6.251988887786865
Epoch 1200, val loss: 1.0469894409179688
Epoch 1210, training loss: 62.596153259277344 = 0.06370437145233154 + 10.0 * 6.253244876861572
Epoch 1210, val loss: 1.0539849996566772
Epoch 1220, training loss: 62.59708023071289 = 0.06191389635205269 + 10.0 * 6.253516674041748
Epoch 1220, val loss: 1.0608525276184082
Epoch 1230, training loss: 62.59857177734375 = 0.0601654127240181 + 10.0 * 6.253840446472168
Epoch 1230, val loss: 1.0678645372390747
Epoch 1240, training loss: 62.55657196044922 = 0.058511700481176376 + 10.0 * 6.2498064041137695
Epoch 1240, val loss: 1.074555516242981
Epoch 1250, training loss: 62.55018615722656 = 0.05692221224308014 + 10.0 * 6.249326229095459
Epoch 1250, val loss: 1.0814155340194702
Epoch 1260, training loss: 62.54022979736328 = 0.055394452065229416 + 10.0 * 6.248483657836914
Epoch 1260, val loss: 1.0883971452713013
Epoch 1270, training loss: 62.64841842651367 = 0.053929343819618225 + 10.0 * 6.259449005126953
Epoch 1270, val loss: 1.0950875282287598
Epoch 1280, training loss: 62.56846237182617 = 0.05247888341546059 + 10.0 * 6.251598358154297
Epoch 1280, val loss: 1.10172700881958
Epoch 1290, training loss: 62.53644943237305 = 0.0511055551469326 + 10.0 * 6.248534202575684
Epoch 1290, val loss: 1.1083378791809082
Epoch 1300, training loss: 62.56698989868164 = 0.04979448765516281 + 10.0 * 6.2517194747924805
Epoch 1300, val loss: 1.1149652004241943
Epoch 1310, training loss: 62.510032653808594 = 0.048500727862119675 + 10.0 * 6.246153354644775
Epoch 1310, val loss: 1.1218022108078003
Epoch 1320, training loss: 62.53768539428711 = 0.04727824404835701 + 10.0 * 6.249040603637695
Epoch 1320, val loss: 1.128282070159912
Epoch 1330, training loss: 62.511634826660156 = 0.04608249291777611 + 10.0 * 6.246555328369141
Epoch 1330, val loss: 1.134782075881958
Epoch 1340, training loss: 62.4981803894043 = 0.04492722824215889 + 10.0 * 6.245325565338135
Epoch 1340, val loss: 1.1415375471115112
Epoch 1350, training loss: 62.49187469482422 = 0.04382864013314247 + 10.0 * 6.244804859161377
Epoch 1350, val loss: 1.1479154825210571
Epoch 1360, training loss: 62.53685760498047 = 0.042764417827129364 + 10.0 * 6.2494096755981445
Epoch 1360, val loss: 1.154622197151184
Epoch 1370, training loss: 62.49149703979492 = 0.041728436946868896 + 10.0 * 6.244976997375488
Epoch 1370, val loss: 1.1608068943023682
Epoch 1380, training loss: 62.46980667114258 = 0.04072459414601326 + 10.0 * 6.242908000946045
Epoch 1380, val loss: 1.1673064231872559
Epoch 1390, training loss: 62.48229217529297 = 0.039770856499671936 + 10.0 * 6.2442522048950195
Epoch 1390, val loss: 1.1735306978225708
Epoch 1400, training loss: 62.48442459106445 = 0.03883681073784828 + 10.0 * 6.244558811187744
Epoch 1400, val loss: 1.1797713041305542
Epoch 1410, training loss: 62.45909118652344 = 0.03792170435190201 + 10.0 * 6.242116928100586
Epoch 1410, val loss: 1.1860893964767456
Epoch 1420, training loss: 62.49927520751953 = 0.03704715147614479 + 10.0 * 6.246222496032715
Epoch 1420, val loss: 1.1924290657043457
Epoch 1430, training loss: 62.45186233520508 = 0.03620629012584686 + 10.0 * 6.241565704345703
Epoch 1430, val loss: 1.198378562927246
Epoch 1440, training loss: 62.445064544677734 = 0.035390716046094894 + 10.0 * 6.240967750549316
Epoch 1440, val loss: 1.204519271850586
Epoch 1450, training loss: 62.449798583984375 = 0.03460123762488365 + 10.0 * 6.241519451141357
Epoch 1450, val loss: 1.2108367681503296
Epoch 1460, training loss: 62.479801177978516 = 0.0338381752371788 + 10.0 * 6.244596481323242
Epoch 1460, val loss: 1.2167521715164185
Epoch 1470, training loss: 62.43819808959961 = 0.033085573464632034 + 10.0 * 6.240511417388916
Epoch 1470, val loss: 1.2227619886398315
Epoch 1480, training loss: 62.43772506713867 = 0.03236677870154381 + 10.0 * 6.240535736083984
Epoch 1480, val loss: 1.2288572788238525
Epoch 1490, training loss: 62.445579528808594 = 0.03166951984167099 + 10.0 * 6.241391181945801
Epoch 1490, val loss: 1.234723687171936
Epoch 1500, training loss: 62.447235107421875 = 0.031002739444375038 + 10.0 * 6.241623401641846
Epoch 1500, val loss: 1.2401981353759766
Epoch 1510, training loss: 62.41665267944336 = 0.030334342271089554 + 10.0 * 6.238631725311279
Epoch 1510, val loss: 1.2464468479156494
Epoch 1520, training loss: 62.405948638916016 = 0.02970035932958126 + 10.0 * 6.237624645233154
Epoch 1520, val loss: 1.2523103952407837
Epoch 1530, training loss: 62.402828216552734 = 0.029089972376823425 + 10.0 * 6.2373738288879395
Epoch 1530, val loss: 1.2581546306610107
Epoch 1540, training loss: 62.45933532714844 = 0.028491932898759842 + 10.0 * 6.24308443069458
Epoch 1540, val loss: 1.2639691829681396
Epoch 1550, training loss: 62.42755889892578 = 0.02790258266031742 + 10.0 * 6.239965915679932
Epoch 1550, val loss: 1.2696672677993774
Epoch 1560, training loss: 62.42609786987305 = 0.027340853586792946 + 10.0 * 6.239875793457031
Epoch 1560, val loss: 1.2751643657684326
Epoch 1570, training loss: 62.406185150146484 = 0.026786955073475838 + 10.0 * 6.237939834594727
Epoch 1570, val loss: 1.2808068990707397
Epoch 1580, training loss: 62.382476806640625 = 0.026252228766679764 + 10.0 * 6.235622406005859
Epoch 1580, val loss: 1.2865335941314697
Epoch 1590, training loss: 62.38764572143555 = 0.025737855583429337 + 10.0 * 6.2361907958984375
Epoch 1590, val loss: 1.2921617031097412
Epoch 1600, training loss: 62.42478942871094 = 0.025238944217562675 + 10.0 * 6.239954948425293
Epoch 1600, val loss: 1.2975202798843384
Epoch 1610, training loss: 62.38562774658203 = 0.02474258467555046 + 10.0 * 6.236088752746582
Epoch 1610, val loss: 1.3031692504882812
Epoch 1620, training loss: 62.36528396606445 = 0.02426297217607498 + 10.0 * 6.234102249145508
Epoch 1620, val loss: 1.3085858821868896
Epoch 1630, training loss: 62.388572692871094 = 0.02380659617483616 + 10.0 * 6.236476421356201
Epoch 1630, val loss: 1.313942313194275
Epoch 1640, training loss: 62.36627960205078 = 0.023353077471256256 + 10.0 * 6.234292507171631
Epoch 1640, val loss: 1.319218397140503
Epoch 1650, training loss: 62.350154876708984 = 0.02290840819478035 + 10.0 * 6.232724666595459
Epoch 1650, val loss: 1.324752688407898
Epoch 1660, training loss: 62.346439361572266 = 0.022485295310616493 + 10.0 * 6.232395648956299
Epoch 1660, val loss: 1.3301016092300415
Epoch 1670, training loss: 62.36223220825195 = 0.022074447944760323 + 10.0 * 6.234015464782715
Epoch 1670, val loss: 1.3356366157531738
Epoch 1680, training loss: 62.38019561767578 = 0.02166914939880371 + 10.0 * 6.2358527183532715
Epoch 1680, val loss: 1.3407247066497803
Epoch 1690, training loss: 62.39032745361328 = 0.02127731777727604 + 10.0 * 6.236905097961426
Epoch 1690, val loss: 1.3456499576568604
Epoch 1700, training loss: 62.346954345703125 = 0.020886672660708427 + 10.0 * 6.232606887817383
Epoch 1700, val loss: 1.3508965969085693
Epoch 1710, training loss: 62.35259246826172 = 0.020520225167274475 + 10.0 * 6.2332072257995605
Epoch 1710, val loss: 1.356024980545044
Epoch 1720, training loss: 62.335845947265625 = 0.020154276862740517 + 10.0 * 6.231569290161133
Epoch 1720, val loss: 1.3612372875213623
Epoch 1730, training loss: 62.34012985229492 = 0.019805071875452995 + 10.0 * 6.232032299041748
Epoch 1730, val loss: 1.3663229942321777
Epoch 1740, training loss: 62.335914611816406 = 0.01945914328098297 + 10.0 * 6.231645584106445
Epoch 1740, val loss: 1.3713502883911133
Epoch 1750, training loss: 62.394893646240234 = 0.01913125440478325 + 10.0 * 6.237576484680176
Epoch 1750, val loss: 1.3759920597076416
Epoch 1760, training loss: 62.364906311035156 = 0.018792157992720604 + 10.0 * 6.234611511230469
Epoch 1760, val loss: 1.3813074827194214
Epoch 1770, training loss: 62.31918716430664 = 0.01846734620630741 + 10.0 * 6.230072021484375
Epoch 1770, val loss: 1.385978102684021
Epoch 1780, training loss: 62.304988861083984 = 0.018158895894885063 + 10.0 * 6.228682994842529
Epoch 1780, val loss: 1.3910259008407593
Epoch 1790, training loss: 62.29985427856445 = 0.017861079424619675 + 10.0 * 6.228199481964111
Epoch 1790, val loss: 1.3960154056549072
Epoch 1800, training loss: 62.324256896972656 = 0.017571821808815002 + 10.0 * 6.230668544769287
Epoch 1800, val loss: 1.4008634090423584
Epoch 1810, training loss: 62.305912017822266 = 0.01727968454360962 + 10.0 * 6.22886323928833
Epoch 1810, val loss: 1.4055209159851074
Epoch 1820, training loss: 62.304832458496094 = 0.016997728496789932 + 10.0 * 6.22878360748291
Epoch 1820, val loss: 1.4102914333343506
Epoch 1830, training loss: 62.33258819580078 = 0.016723057255148888 + 10.0 * 6.231586456298828
Epoch 1830, val loss: 1.4151536226272583
Epoch 1840, training loss: 62.2944221496582 = 0.01645384356379509 + 10.0 * 6.227797031402588
Epoch 1840, val loss: 1.419790267944336
Epoch 1850, training loss: 62.29664993286133 = 0.01619407907128334 + 10.0 * 6.228045463562012
Epoch 1850, val loss: 1.4245175123214722
Epoch 1860, training loss: 62.35906219482422 = 0.01593981310725212 + 10.0 * 6.234312057495117
Epoch 1860, val loss: 1.4294347763061523
Epoch 1870, training loss: 62.301761627197266 = 0.015689793974161148 + 10.0 * 6.228607177734375
Epoch 1870, val loss: 1.433179259300232
Epoch 1880, training loss: 62.28562927246094 = 0.015441256575286388 + 10.0 * 6.2270188331604
Epoch 1880, val loss: 1.438214898109436
Epoch 1890, training loss: 62.274417877197266 = 0.015206769108772278 + 10.0 * 6.225921154022217
Epoch 1890, val loss: 1.442694067955017
Epoch 1900, training loss: 62.28288269042969 = 0.014977255836129189 + 10.0 * 6.226790428161621
Epoch 1900, val loss: 1.4474042654037476
Epoch 1910, training loss: 62.324424743652344 = 0.014748958870768547 + 10.0 * 6.2309675216674805
Epoch 1910, val loss: 1.4517920017242432
Epoch 1920, training loss: 62.28862380981445 = 0.014527203515172005 + 10.0 * 6.227409839630127
Epoch 1920, val loss: 1.4558396339416504
Epoch 1930, training loss: 62.272151947021484 = 0.01430972758680582 + 10.0 * 6.2257843017578125
Epoch 1930, val loss: 1.460619568824768
Epoch 1940, training loss: 62.32273483276367 = 0.01410179864615202 + 10.0 * 6.230863094329834
Epoch 1940, val loss: 1.4648160934448242
Epoch 1950, training loss: 62.27744674682617 = 0.013891002163290977 + 10.0 * 6.22635555267334
Epoch 1950, val loss: 1.469191551208496
Epoch 1960, training loss: 62.273250579833984 = 0.013688061386346817 + 10.0 * 6.225956439971924
Epoch 1960, val loss: 1.473600149154663
Epoch 1970, training loss: 62.2543830871582 = 0.013491506688296795 + 10.0 * 6.2240891456604
Epoch 1970, val loss: 1.4778441190719604
Epoch 1980, training loss: 62.2451057434082 = 0.013300715014338493 + 10.0 * 6.223180294036865
Epoch 1980, val loss: 1.4821243286132812
Epoch 1990, training loss: 62.28089904785156 = 0.013117446564137936 + 10.0 * 6.226778030395508
Epoch 1990, val loss: 1.4863284826278687
Epoch 2000, training loss: 62.26710510253906 = 0.012928278185427189 + 10.0 * 6.225417613983154
Epoch 2000, val loss: 1.4905356168746948
Epoch 2010, training loss: 62.281063079833984 = 0.01274305023252964 + 10.0 * 6.226831912994385
Epoch 2010, val loss: 1.4947481155395508
Epoch 2020, training loss: 62.248046875 = 0.01256301999092102 + 10.0 * 6.223548412322998
Epoch 2020, val loss: 1.4987573623657227
Epoch 2030, training loss: 62.243064880371094 = 0.012391716241836548 + 10.0 * 6.223067283630371
Epoch 2030, val loss: 1.50296151638031
Epoch 2040, training loss: 62.25612258911133 = 0.012223472818732262 + 10.0 * 6.224390029907227
Epoch 2040, val loss: 1.507185459136963
Epoch 2050, training loss: 62.26059341430664 = 0.012057063169777393 + 10.0 * 6.224853515625
Epoch 2050, val loss: 1.511206865310669
Epoch 2060, training loss: 62.25743865966797 = 0.011895246803760529 + 10.0 * 6.224554538726807
Epoch 2060, val loss: 1.5150858163833618
Epoch 2070, training loss: 62.229530334472656 = 0.011734704487025738 + 10.0 * 6.221779823303223
Epoch 2070, val loss: 1.5192065238952637
Epoch 2080, training loss: 62.22997283935547 = 0.011580632999539375 + 10.0 * 6.22183895111084
Epoch 2080, val loss: 1.52321457862854
Epoch 2090, training loss: 62.284767150878906 = 0.011429432779550552 + 10.0 * 6.227334022521973
Epoch 2090, val loss: 1.5272889137268066
Epoch 2100, training loss: 62.227027893066406 = 0.011278844438493252 + 10.0 * 6.221574783325195
Epoch 2100, val loss: 1.5307034254074097
Epoch 2110, training loss: 62.2403450012207 = 0.0111316479742527 + 10.0 * 6.222921371459961
Epoch 2110, val loss: 1.5346509218215942
Epoch 2120, training loss: 62.22055435180664 = 0.01098553091287613 + 10.0 * 6.220956802368164
Epoch 2120, val loss: 1.538595199584961
Epoch 2130, training loss: 62.236610412597656 = 0.01084593404084444 + 10.0 * 6.22257661819458
Epoch 2130, val loss: 1.5425825119018555
Epoch 2140, training loss: 62.2266960144043 = 0.010708171874284744 + 10.0 * 6.2215986251831055
Epoch 2140, val loss: 1.546234369277954
Epoch 2150, training loss: 62.21781921386719 = 0.010575124993920326 + 10.0 * 6.220724582672119
Epoch 2150, val loss: 1.5499300956726074
Epoch 2160, training loss: 62.21495056152344 = 0.01044431421905756 + 10.0 * 6.220450401306152
Epoch 2160, val loss: 1.553719401359558
Epoch 2170, training loss: 62.2308464050293 = 0.010316171683371067 + 10.0 * 6.222053050994873
Epoch 2170, val loss: 1.5574511289596558
Epoch 2180, training loss: 62.243995666503906 = 0.01018550992012024 + 10.0 * 6.223381042480469
Epoch 2180, val loss: 1.561356544494629
Epoch 2190, training loss: 62.21782302856445 = 0.010059451684355736 + 10.0 * 6.220776557922363
Epoch 2190, val loss: 1.5649261474609375
Epoch 2200, training loss: 62.2114143371582 = 0.009938426315784454 + 10.0 * 6.220147609710693
Epoch 2200, val loss: 1.5684354305267334
Epoch 2210, training loss: 62.21758270263672 = 0.009818053804337978 + 10.0 * 6.220776557922363
Epoch 2210, val loss: 1.572173833847046
Epoch 2220, training loss: 62.2259635925293 = 0.009701035916805267 + 10.0 * 6.221626281738281
Epoch 2220, val loss: 1.5755138397216797
Epoch 2230, training loss: 62.20070266723633 = 0.009584026411175728 + 10.0 * 6.219111919403076
Epoch 2230, val loss: 1.5791734457015991
Epoch 2240, training loss: 62.22520065307617 = 0.009470777586102486 + 10.0 * 6.2215728759765625
Epoch 2240, val loss: 1.5828173160552979
Epoch 2250, training loss: 62.225181579589844 = 0.009358463808894157 + 10.0 * 6.221582412719727
Epoch 2250, val loss: 1.5862526893615723
Epoch 2260, training loss: 62.207862854003906 = 0.009246768429875374 + 10.0 * 6.2198615074157715
Epoch 2260, val loss: 1.5898319482803345
Epoch 2270, training loss: 62.179012298583984 = 0.009140017442405224 + 10.0 * 6.216987133026123
Epoch 2270, val loss: 1.5931123495101929
Epoch 2280, training loss: 62.17853927612305 = 0.00903782807290554 + 10.0 * 6.216950416564941
Epoch 2280, val loss: 1.5966229438781738
Epoch 2290, training loss: 62.17633056640625 = 0.008936361409723759 + 10.0 * 6.216739177703857
Epoch 2290, val loss: 1.6001571416854858
Epoch 2300, training loss: 62.20493698120117 = 0.008838520385324955 + 10.0 * 6.21960973739624
Epoch 2300, val loss: 1.6035099029541016
Epoch 2310, training loss: 62.18879699707031 = 0.008738817647099495 + 10.0 * 6.218005657196045
Epoch 2310, val loss: 1.6066172122955322
Epoch 2320, training loss: 62.275394439697266 = 0.00864326674491167 + 10.0 * 6.226675033569336
Epoch 2320, val loss: 1.6093966960906982
Epoch 2330, training loss: 62.19577407836914 = 0.008538011461496353 + 10.0 * 6.218723773956299
Epoch 2330, val loss: 1.613283395767212
Epoch 2340, training loss: 62.176151275634766 = 0.008445377461612225 + 10.0 * 6.216770648956299
Epoch 2340, val loss: 1.6163303852081299
Epoch 2350, training loss: 62.164886474609375 = 0.008354146033525467 + 10.0 * 6.215653419494629
Epoch 2350, val loss: 1.6198655366897583
Epoch 2360, training loss: 62.20005798339844 = 0.008267032913863659 + 10.0 * 6.219179153442383
Epoch 2360, val loss: 1.6230788230895996
Epoch 2370, training loss: 62.168212890625 = 0.008177337236702442 + 10.0 * 6.21600341796875
Epoch 2370, val loss: 1.626120924949646
Epoch 2380, training loss: 62.182403564453125 = 0.008088269270956516 + 10.0 * 6.217431545257568
Epoch 2380, val loss: 1.6294397115707397
Epoch 2390, training loss: 62.165462493896484 = 0.008002188056707382 + 10.0 * 6.21574592590332
Epoch 2390, val loss: 1.6323635578155518
Epoch 2400, training loss: 62.15618896484375 = 0.007918220944702625 + 10.0 * 6.214827060699463
Epoch 2400, val loss: 1.6358238458633423
Epoch 2410, training loss: 62.17171096801758 = 0.007838075049221516 + 10.0 * 6.2163872718811035
Epoch 2410, val loss: 1.639003872871399
Epoch 2420, training loss: 62.19005584716797 = 0.007757487241178751 + 10.0 * 6.2182297706604
Epoch 2420, val loss: 1.6420283317565918
Epoch 2430, training loss: 62.184444427490234 = 0.0076776002533733845 + 10.0 * 6.217676639556885
Epoch 2430, val loss: 1.644818663597107
Epoch 2440, training loss: 62.18894958496094 = 0.007600090466439724 + 10.0 * 6.218134880065918
Epoch 2440, val loss: 1.6478546857833862
Epoch 2450, training loss: 62.14509201049805 = 0.007520229555666447 + 10.0 * 6.213757514953613
Epoch 2450, val loss: 1.6511143445968628
Epoch 2460, training loss: 62.17071533203125 = 0.007445022463798523 + 10.0 * 6.216326713562012
Epoch 2460, val loss: 1.654264211654663
Epoch 2470, training loss: 62.17645263671875 = 0.007370459847152233 + 10.0 * 6.2169084548950195
Epoch 2470, val loss: 1.657029628753662
Epoch 2480, training loss: 62.15400314331055 = 0.007297968491911888 + 10.0 * 6.214670658111572
Epoch 2480, val loss: 1.6597638130187988
Epoch 2490, training loss: 62.14131164550781 = 0.007225606590509415 + 10.0 * 6.213408470153809
Epoch 2490, val loss: 1.6630644798278809
Epoch 2500, training loss: 62.16693878173828 = 0.0071572656743228436 + 10.0 * 6.215978145599365
Epoch 2500, val loss: 1.6658157110214233
Epoch 2510, training loss: 62.17207336425781 = 0.0070881969295442104 + 10.0 * 6.216498374938965
Epoch 2510, val loss: 1.668478012084961
Epoch 2520, training loss: 62.15989685058594 = 0.007017824333161116 + 10.0 * 6.215287685394287
Epoch 2520, val loss: 1.6712563037872314
Epoch 2530, training loss: 62.138206481933594 = 0.0069490643218159676 + 10.0 * 6.213125705718994
Epoch 2530, val loss: 1.6744084358215332
Epoch 2540, training loss: 62.134952545166016 = 0.0068848528899252415 + 10.0 * 6.212806701660156
Epoch 2540, val loss: 1.677324652671814
Epoch 2550, training loss: 62.182132720947266 = 0.006821583956480026 + 10.0 * 6.217531204223633
Epoch 2550, val loss: 1.6801066398620605
Epoch 2560, training loss: 62.14486312866211 = 0.006756152026355267 + 10.0 * 6.213810920715332
Epoch 2560, val loss: 1.6825768947601318
Epoch 2570, training loss: 62.19318389892578 = 0.006691263057291508 + 10.0 * 6.218649387359619
Epoch 2570, val loss: 1.6853724718093872
Epoch 2580, training loss: 62.14897537231445 = 0.0066266669891774654 + 10.0 * 6.214234828948975
Epoch 2580, val loss: 1.688187837600708
Epoch 2590, training loss: 62.14443588256836 = 0.006565489806234837 + 10.0 * 6.213787078857422
Epoch 2590, val loss: 1.6910862922668457
Epoch 2600, training loss: 62.151126861572266 = 0.006505501456558704 + 10.0 * 6.2144622802734375
Epoch 2600, val loss: 1.693806767463684
Epoch 2610, training loss: 62.1204833984375 = 0.006447401363402605 + 10.0 * 6.211403846740723
Epoch 2610, val loss: 1.6963930130004883
Epoch 2620, training loss: 62.113773345947266 = 0.006390562281012535 + 10.0 * 6.210738182067871
Epoch 2620, val loss: 1.6991157531738281
Epoch 2630, training loss: 62.15920639038086 = 0.006336780730634928 + 10.0 * 6.215287208557129
Epoch 2630, val loss: 1.7016724348068237
Epoch 2640, training loss: 62.12605285644531 = 0.006277627777308226 + 10.0 * 6.211977481842041
Epoch 2640, val loss: 1.7043609619140625
Epoch 2650, training loss: 62.15415573120117 = 0.0062205432914197445 + 10.0 * 6.2147932052612305
Epoch 2650, val loss: 1.7066985368728638
Epoch 2660, training loss: 62.131103515625 = 0.0061646513640880585 + 10.0 * 6.212493896484375
Epoch 2660, val loss: 1.7094320058822632
Epoch 2670, training loss: 62.1304931640625 = 0.0061109960079193115 + 10.0 * 6.212438106536865
Epoch 2670, val loss: 1.7120786905288696
Epoch 2680, training loss: 62.12907028198242 = 0.006058065220713615 + 10.0 * 6.212301254272461
Epoch 2680, val loss: 1.7147585153579712
Epoch 2690, training loss: 62.110164642333984 = 0.0060069565661251545 + 10.0 * 6.210415840148926
Epoch 2690, val loss: 1.717147707939148
Epoch 2700, training loss: 62.103763580322266 = 0.005956496577709913 + 10.0 * 6.209780693054199
Epoch 2700, val loss: 1.7198253870010376
Epoch 2710, training loss: 62.15955352783203 = 0.00590819027274847 + 10.0 * 6.215364456176758
Epoch 2710, val loss: 1.722285509109497
Epoch 2720, training loss: 62.11357116699219 = 0.005856926087290049 + 10.0 * 6.210771560668945
Epoch 2720, val loss: 1.7244373559951782
Epoch 2730, training loss: 62.12162780761719 = 0.005807520356029272 + 10.0 * 6.211582183837891
Epoch 2730, val loss: 1.726892352104187
Epoch 2740, training loss: 62.111331939697266 = 0.005757858976721764 + 10.0 * 6.210557460784912
Epoch 2740, val loss: 1.729120135307312
Epoch 2750, training loss: 62.111881256103516 = 0.005711241625249386 + 10.0 * 6.2106170654296875
Epoch 2750, val loss: 1.7316360473632812
Epoch 2760, training loss: 62.14570617675781 = 0.005665793549269438 + 10.0 * 6.214004039764404
Epoch 2760, val loss: 1.7339081764221191
Epoch 2770, training loss: 62.10790252685547 = 0.005616350099444389 + 10.0 * 6.21022891998291
Epoch 2770, val loss: 1.7365611791610718
Epoch 2780, training loss: 62.11406707763672 = 0.005572301335632801 + 10.0 * 6.210849285125732
Epoch 2780, val loss: 1.7388962507247925
Epoch 2790, training loss: 62.129188537597656 = 0.0055281491950154305 + 10.0 * 6.212366104125977
Epoch 2790, val loss: 1.7410374879837036
Epoch 2800, training loss: 62.09480667114258 = 0.005482463166117668 + 10.0 * 6.208932399749756
Epoch 2800, val loss: 1.7435269355773926
Epoch 2810, training loss: 62.09739685058594 = 0.005439573898911476 + 10.0 * 6.209195613861084
Epoch 2810, val loss: 1.7460238933563232
Epoch 2820, training loss: 62.10920333862305 = 0.005397867877036333 + 10.0 * 6.210380554199219
Epoch 2820, val loss: 1.7482855319976807
Epoch 2830, training loss: 62.13066864013672 = 0.0053564514964818954 + 10.0 * 6.212531089782715
Epoch 2830, val loss: 1.7501733303070068
Epoch 2840, training loss: 62.11249542236328 = 0.0053130327723920345 + 10.0 * 6.210718154907227
Epoch 2840, val loss: 1.7525074481964111
Epoch 2850, training loss: 62.09279251098633 = 0.0052711511962115765 + 10.0 * 6.208752155303955
Epoch 2850, val loss: 1.755057454109192
Epoch 2860, training loss: 62.09648513793945 = 0.0052318088710308075 + 10.0 * 6.209125518798828
Epoch 2860, val loss: 1.757401704788208
Epoch 2870, training loss: 62.11665725708008 = 0.005193049553781748 + 10.0 * 6.211146354675293
Epoch 2870, val loss: 1.7593921422958374
Epoch 2880, training loss: 62.118106842041016 = 0.005154211074113846 + 10.0 * 6.211295127868652
Epoch 2880, val loss: 1.761345386505127
Epoch 2890, training loss: 62.090003967285156 = 0.005113700404763222 + 10.0 * 6.208488941192627
Epoch 2890, val loss: 1.7636969089508057
Epoch 2900, training loss: 62.10191345214844 = 0.005074990447610617 + 10.0 * 6.209683895111084
Epoch 2900, val loss: 1.766083002090454
Epoch 2910, training loss: 62.1048469543457 = 0.005038056056946516 + 10.0 * 6.2099809646606445
Epoch 2910, val loss: 1.7681443691253662
Epoch 2920, training loss: 62.09196090698242 = 0.004999835975468159 + 10.0 * 6.208695888519287
Epoch 2920, val loss: 1.7703931331634521
Epoch 2930, training loss: 62.08759307861328 = 0.0049635679461061954 + 10.0 * 6.208262920379639
Epoch 2930, val loss: 1.7725391387939453
Epoch 2940, training loss: 62.080902099609375 = 0.004927459172904491 + 10.0 * 6.207597255706787
Epoch 2940, val loss: 1.7746682167053223
Epoch 2950, training loss: 62.094120025634766 = 0.004891871940344572 + 10.0 * 6.208922863006592
Epoch 2950, val loss: 1.7771239280700684
Epoch 2960, training loss: 62.122772216796875 = 0.004856117535382509 + 10.0 * 6.211791515350342
Epoch 2960, val loss: 1.7789220809936523
Epoch 2970, training loss: 62.0804557800293 = 0.004821148235350847 + 10.0 * 6.207563400268555
Epoch 2970, val loss: 1.7806955575942993
Epoch 2980, training loss: 62.06794357299805 = 0.004786213859915733 + 10.0 * 6.206315517425537
Epoch 2980, val loss: 1.7829912900924683
Epoch 2990, training loss: 62.066619873046875 = 0.004754466004669666 + 10.0 * 6.206186771392822
Epoch 2990, val loss: 1.7850443124771118
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8376383763837639
The final CL Acc:0.76420, 0.01145, The final GNN Acc:0.83834, 0.00050
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11626])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10546])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.9150619506836 = 1.9465677738189697 + 10.0 * 8.59684944152832
Epoch 0, val loss: 1.9489974975585938
Epoch 10, training loss: 87.90116882324219 = 1.9373804330825806 + 10.0 * 8.596379280090332
Epoch 10, val loss: 1.9394006729125977
Epoch 20, training loss: 87.85494232177734 = 1.9261037111282349 + 10.0 * 8.592884063720703
Epoch 20, val loss: 1.927418828010559
Epoch 30, training loss: 87.60072326660156 = 1.9110599756240845 + 10.0 * 8.56896686553955
Epoch 30, val loss: 1.9115071296691895
Epoch 40, training loss: 86.2949447631836 = 1.8924708366394043 + 10.0 * 8.440247535705566
Epoch 40, val loss: 1.8925200700759888
Epoch 50, training loss: 82.42562103271484 = 1.87147057056427 + 10.0 * 8.055415153503418
Epoch 50, val loss: 1.8712438344955444
Epoch 60, training loss: 79.26962280273438 = 1.8545280694961548 + 10.0 * 7.741509437561035
Epoch 60, val loss: 1.8553383350372314
Epoch 70, training loss: 75.50279235839844 = 1.8444536924362183 + 10.0 * 7.3658342361450195
Epoch 70, val loss: 1.846083402633667
Epoch 80, training loss: 73.19335174560547 = 1.838413953781128 + 10.0 * 7.135493755340576
Epoch 80, val loss: 1.8401485681533813
Epoch 90, training loss: 71.80137634277344 = 1.829796552658081 + 10.0 * 6.997158050537109
Epoch 90, val loss: 1.8317935466766357
Epoch 100, training loss: 70.68936157226562 = 1.8208175897598267 + 10.0 * 6.88685417175293
Epoch 100, val loss: 1.8236287832260132
Epoch 110, training loss: 69.95013427734375 = 1.8123310804367065 + 10.0 * 6.813779830932617
Epoch 110, val loss: 1.8156614303588867
Epoch 120, training loss: 69.33972930908203 = 1.8033934831619263 + 10.0 * 6.753633975982666
Epoch 120, val loss: 1.807548999786377
Epoch 130, training loss: 68.83405303955078 = 1.795202374458313 + 10.0 * 6.703885078430176
Epoch 130, val loss: 1.8001383543014526
Epoch 140, training loss: 68.35279846191406 = 1.7870349884033203 + 10.0 * 6.656576633453369
Epoch 140, val loss: 1.7927098274230957
Epoch 150, training loss: 67.95256042480469 = 1.7783373594284058 + 10.0 * 6.617422580718994
Epoch 150, val loss: 1.7851347923278809
Epoch 160, training loss: 67.576416015625 = 1.7690335512161255 + 10.0 * 6.580738067626953
Epoch 160, val loss: 1.776981234550476
Epoch 170, training loss: 67.26447296142578 = 1.7588465213775635 + 10.0 * 6.550562381744385
Epoch 170, val loss: 1.7682896852493286
Epoch 180, training loss: 67.02420806884766 = 1.7475566864013672 + 10.0 * 6.527665138244629
Epoch 180, val loss: 1.7587250471115112
Epoch 190, training loss: 66.83345031738281 = 1.7350786924362183 + 10.0 * 6.509837627410889
Epoch 190, val loss: 1.7481913566589355
Epoch 200, training loss: 66.67037200927734 = 1.721320629119873 + 10.0 * 6.4949049949646
Epoch 200, val loss: 1.7366169691085815
Epoch 210, training loss: 66.52826690673828 = 1.7062177658081055 + 10.0 * 6.482205390930176
Epoch 210, val loss: 1.723939061164856
Epoch 220, training loss: 66.38911437988281 = 1.6896653175354004 + 10.0 * 6.469944477081299
Epoch 220, val loss: 1.7100675106048584
Epoch 230, training loss: 66.25946044921875 = 1.6716045141220093 + 10.0 * 6.458785057067871
Epoch 230, val loss: 1.695001482963562
Epoch 240, training loss: 66.15025329589844 = 1.6520406007766724 + 10.0 * 6.449821472167969
Epoch 240, val loss: 1.6787381172180176
Epoch 250, training loss: 66.06111145019531 = 1.6309937238693237 + 10.0 * 6.44301176071167
Epoch 250, val loss: 1.6612874269485474
Epoch 260, training loss: 65.9424819946289 = 1.6085442304611206 + 10.0 * 6.433393955230713
Epoch 260, val loss: 1.6428101062774658
Epoch 270, training loss: 65.83411407470703 = 1.5848966836929321 + 10.0 * 6.42492151260376
Epoch 270, val loss: 1.6234287023544312
Epoch 280, training loss: 65.77825927734375 = 1.5601028203964233 + 10.0 * 6.421815872192383
Epoch 280, val loss: 1.6033533811569214
Epoch 290, training loss: 65.6581802368164 = 1.5345381498336792 + 10.0 * 6.412364482879639
Epoch 290, val loss: 1.582646369934082
Epoch 300, training loss: 65.5578842163086 = 1.5085246562957764 + 10.0 * 6.404935836791992
Epoch 300, val loss: 1.5618900060653687
Epoch 310, training loss: 65.47122955322266 = 1.4822604656219482 + 10.0 * 6.39889669418335
Epoch 310, val loss: 1.5412147045135498
Epoch 320, training loss: 65.39141082763672 = 1.4558913707733154 + 10.0 * 6.393551826477051
Epoch 320, val loss: 1.5206934213638306
Epoch 330, training loss: 65.32340240478516 = 1.4298748970031738 + 10.0 * 6.389352798461914
Epoch 330, val loss: 1.5006822347640991
Epoch 340, training loss: 65.23037719726562 = 1.4041571617126465 + 10.0 * 6.382621765136719
Epoch 340, val loss: 1.4811655282974243
Epoch 350, training loss: 65.18418884277344 = 1.3788281679153442 + 10.0 * 6.380536079406738
Epoch 350, val loss: 1.4621813297271729
Epoch 360, training loss: 65.10667419433594 = 1.3536787033081055 + 10.0 * 6.37529993057251
Epoch 360, val loss: 1.443537950515747
Epoch 370, training loss: 65.0189437866211 = 1.3288851976394653 + 10.0 * 6.3690056800842285
Epoch 370, val loss: 1.4252675771713257
Epoch 380, training loss: 64.95635986328125 = 1.3042044639587402 + 10.0 * 6.365215301513672
Epoch 380, val loss: 1.4071989059448242
Epoch 390, training loss: 64.89236450195312 = 1.2794442176818848 + 10.0 * 6.361291885375977
Epoch 390, val loss: 1.3890081644058228
Epoch 400, training loss: 64.82264709472656 = 1.2545186281204224 + 10.0 * 6.356812953948975
Epoch 400, val loss: 1.370955228805542
Epoch 410, training loss: 64.77254486083984 = 1.2293769121170044 + 10.0 * 6.354316711425781
Epoch 410, val loss: 1.3527086973190308
Epoch 420, training loss: 64.71802520751953 = 1.2038341760635376 + 10.0 * 6.351419448852539
Epoch 420, val loss: 1.3341830968856812
Epoch 430, training loss: 64.65823364257812 = 1.177881121635437 + 10.0 * 6.3480353355407715
Epoch 430, val loss: 1.315312147140503
Epoch 440, training loss: 64.6226577758789 = 1.15151047706604 + 10.0 * 6.3471150398254395
Epoch 440, val loss: 1.2964271306991577
Epoch 450, training loss: 64.54905700683594 = 1.124671220779419 + 10.0 * 6.342438697814941
Epoch 450, val loss: 1.276889681816101
Epoch 460, training loss: 64.4783706665039 = 1.097479224205017 + 10.0 * 6.338089466094971
Epoch 460, val loss: 1.2576439380645752
Epoch 470, training loss: 64.42606353759766 = 1.0700633525848389 + 10.0 * 6.335599899291992
Epoch 470, val loss: 1.2382210493087769
Epoch 480, training loss: 64.42828369140625 = 1.0424549579620361 + 10.0 * 6.338582992553711
Epoch 480, val loss: 1.2189456224441528
Epoch 490, training loss: 64.3309555053711 = 1.014845609664917 + 10.0 * 6.331610679626465
Epoch 490, val loss: 1.1997146606445312
Epoch 500, training loss: 64.2710952758789 = 0.9873690009117126 + 10.0 * 6.328372955322266
Epoch 500, val loss: 1.180910348892212
Epoch 510, training loss: 64.20822143554688 = 0.9602503180503845 + 10.0 * 6.3247971534729
Epoch 510, val loss: 1.1626265048980713
Epoch 520, training loss: 64.20135498046875 = 0.9334756731987 + 10.0 * 6.32678747177124
Epoch 520, val loss: 1.1446527242660522
Epoch 530, training loss: 64.17855072021484 = 0.9072715640068054 + 10.0 * 6.327127933502197
Epoch 530, val loss: 1.127927303314209
Epoch 540, training loss: 64.08903503417969 = 0.8815710544586182 + 10.0 * 6.320746421813965
Epoch 540, val loss: 1.1117445230484009
Epoch 550, training loss: 64.02862548828125 = 0.8567208647727966 + 10.0 * 6.317190647125244
Epoch 550, val loss: 1.096488118171692
Epoch 560, training loss: 63.97550582885742 = 0.8326895236968994 + 10.0 * 6.314281463623047
Epoch 560, val loss: 1.0820927619934082
Epoch 570, training loss: 63.961029052734375 = 0.809401273727417 + 10.0 * 6.315162658691406
Epoch 570, val loss: 1.0687060356140137
Epoch 580, training loss: 63.94805145263672 = 0.7867027521133423 + 10.0 * 6.316134929656982
Epoch 580, val loss: 1.0564440488815308
Epoch 590, training loss: 63.86085510253906 = 0.7649234533309937 + 10.0 * 6.309593200683594
Epoch 590, val loss: 1.0448684692382812
Epoch 600, training loss: 63.824954986572266 = 0.743973433971405 + 10.0 * 6.308098316192627
Epoch 600, val loss: 1.0344221591949463
Epoch 610, training loss: 63.78087615966797 = 0.723760187625885 + 10.0 * 6.30571174621582
Epoch 610, val loss: 1.0249325037002563
Epoch 620, training loss: 63.809722900390625 = 0.7042610049247742 + 10.0 * 6.310545921325684
Epoch 620, val loss: 1.0163326263427734
Epoch 630, training loss: 63.7292594909668 = 0.6853364706039429 + 10.0 * 6.304392337799072
Epoch 630, val loss: 1.0084291696548462
Epoch 640, training loss: 63.68000793457031 = 0.6671264171600342 + 10.0 * 6.30128812789917
Epoch 640, val loss: 1.0012872219085693
Epoch 650, training loss: 63.683616638183594 = 0.649581253528595 + 10.0 * 6.303403377532959
Epoch 650, val loss: 0.995076596736908
Epoch 660, training loss: 63.635501861572266 = 0.6325408220291138 + 10.0 * 6.300295829772949
Epoch 660, val loss: 0.9889159202575684
Epoch 670, training loss: 63.586055755615234 = 0.6160388588905334 + 10.0 * 6.297001838684082
Epoch 670, val loss: 0.9839100241661072
Epoch 680, training loss: 63.55515670776367 = 0.6000970005989075 + 10.0 * 6.295506000518799
Epoch 680, val loss: 0.9792805910110474
Epoch 690, training loss: 63.579742431640625 = 0.5844963192939758 + 10.0 * 6.299524784088135
Epoch 690, val loss: 0.9746835231781006
Epoch 700, training loss: 63.51869201660156 = 0.5693591237068176 + 10.0 * 6.294933319091797
Epoch 700, val loss: 0.9712622761726379
Epoch 710, training loss: 63.47422790527344 = 0.5546466708183289 + 10.0 * 6.291958332061768
Epoch 710, val loss: 0.9681107401847839
Epoch 720, training loss: 63.439823150634766 = 0.5403701663017273 + 10.0 * 6.289945125579834
Epoch 720, val loss: 0.9654020071029663
Epoch 730, training loss: 63.411834716796875 = 0.5263910889625549 + 10.0 * 6.288544654846191
Epoch 730, val loss: 0.9630230069160461
Epoch 740, training loss: 63.46656036376953 = 0.5126413106918335 + 10.0 * 6.295392036437988
Epoch 740, val loss: 0.9608598351478577
Epoch 750, training loss: 63.37042236328125 = 0.49924394488334656 + 10.0 * 6.287117958068848
Epoch 750, val loss: 0.959627628326416
Epoch 760, training loss: 63.3420295715332 = 0.48611506819725037 + 10.0 * 6.2855916023254395
Epoch 760, val loss: 0.9585495591163635
Epoch 770, training loss: 63.3557243347168 = 0.4732455611228943 + 10.0 * 6.288248062133789
Epoch 770, val loss: 0.957351803779602
Epoch 780, training loss: 63.30287170410156 = 0.4606624245643616 + 10.0 * 6.2842206954956055
Epoch 780, val loss: 0.9564415812492371
Epoch 790, training loss: 63.30010986328125 = 0.4483895003795624 + 10.0 * 6.285171985626221
Epoch 790, val loss: 0.9563397169113159
Epoch 800, training loss: 63.24956130981445 = 0.43628185987472534 + 10.0 * 6.281327724456787
Epoch 800, val loss: 0.9559643864631653
Epoch 810, training loss: 63.24276351928711 = 0.4245029389858246 + 10.0 * 6.281826019287109
Epoch 810, val loss: 0.9565072655677795
Epoch 820, training loss: 63.211673736572266 = 0.4128912091255188 + 10.0 * 6.27987813949585
Epoch 820, val loss: 0.9564844369888306
Epoch 830, training loss: 63.176692962646484 = 0.4015522599220276 + 10.0 * 6.2775139808654785
Epoch 830, val loss: 0.9573692083358765
Epoch 840, training loss: 63.175628662109375 = 0.3904409408569336 + 10.0 * 6.2785186767578125
Epoch 840, val loss: 0.9581366777420044
Epoch 850, training loss: 63.15155029296875 = 0.37954476475715637 + 10.0 * 6.277200698852539
Epoch 850, val loss: 0.9591756463050842
Epoch 860, training loss: 63.117652893066406 = 0.36883413791656494 + 10.0 * 6.274881839752197
Epoch 860, val loss: 0.9604178071022034
Epoch 870, training loss: 63.10224151611328 = 0.3584262728691101 + 10.0 * 6.274381637573242
Epoch 870, val loss: 0.961868405342102
Epoch 880, training loss: 63.08245086669922 = 0.348236083984375 + 10.0 * 6.273421287536621
Epoch 880, val loss: 0.963702380657196
Epoch 890, training loss: 63.083152770996094 = 0.33826524019241333 + 10.0 * 6.274488925933838
Epoch 890, val loss: 0.9658941030502319
Epoch 900, training loss: 63.05474853515625 = 0.32839372754096985 + 10.0 * 6.272635459899902
Epoch 900, val loss: 0.9675500988960266
Epoch 910, training loss: 63.03988265991211 = 0.3188595473766327 + 10.0 * 6.272102355957031
Epoch 910, val loss: 0.9700816869735718
Epoch 920, training loss: 63.006248474121094 = 0.30948659777641296 + 10.0 * 6.269676208496094
Epoch 920, val loss: 0.9728776812553406
Epoch 930, training loss: 63.00260543823242 = 0.3003692328929901 + 10.0 * 6.270223617553711
Epoch 930, val loss: 0.9755461812019348
Epoch 940, training loss: 62.956871032714844 = 0.2914738655090332 + 10.0 * 6.266539573669434
Epoch 940, val loss: 0.9785786867141724
Epoch 950, training loss: 62.94732666015625 = 0.2827989161014557 + 10.0 * 6.266452789306641
Epoch 950, val loss: 0.9818041920661926
Epoch 960, training loss: 62.96954345703125 = 0.27432891726493835 + 10.0 * 6.269521236419678
Epoch 960, val loss: 0.9852190017700195
Epoch 970, training loss: 62.93107986450195 = 0.26597681641578674 + 10.0 * 6.266510486602783
Epoch 970, val loss: 0.988955020904541
Epoch 980, training loss: 62.9075927734375 = 0.25789812207221985 + 10.0 * 6.264969825744629
Epoch 980, val loss: 0.9927041530609131
Epoch 990, training loss: 62.89760208129883 = 0.25002866983413696 + 10.0 * 6.26475715637207
Epoch 990, val loss: 0.9971306920051575
Epoch 1000, training loss: 62.886497497558594 = 0.24234826862812042 + 10.0 * 6.2644147872924805
Epoch 1000, val loss: 1.0010868310928345
Epoch 1010, training loss: 62.86613464355469 = 0.23483973741531372 + 10.0 * 6.263129234313965
Epoch 1010, val loss: 1.0053861141204834
Epoch 1020, training loss: 62.84022521972656 = 0.22753645479679108 + 10.0 * 6.2612690925598145
Epoch 1020, val loss: 1.0100486278533936
Epoch 1030, training loss: 62.87969970703125 = 0.22045180201530457 + 10.0 * 6.26592493057251
Epoch 1030, val loss: 1.0147818326950073
Epoch 1040, training loss: 62.821495056152344 = 0.21353411674499512 + 10.0 * 6.260796070098877
Epoch 1040, val loss: 1.0195738077163696
Epoch 1050, training loss: 62.79735565185547 = 0.20683902502059937 + 10.0 * 6.259051322937012
Epoch 1050, val loss: 1.0249173641204834
Epoch 1060, training loss: 62.80887222290039 = 0.2003820836544037 + 10.0 * 6.2608489990234375
Epoch 1060, val loss: 1.030737042427063
Epoch 1070, training loss: 62.7628288269043 = 0.19404181838035583 + 10.0 * 6.256878852844238
Epoch 1070, val loss: 1.0355125665664673
Epoch 1080, training loss: 62.752498626708984 = 0.18791073560714722 + 10.0 * 6.256458759307861
Epoch 1080, val loss: 1.0415005683898926
Epoch 1090, training loss: 62.76274871826172 = 0.18199749290943146 + 10.0 * 6.258074760437012
Epoch 1090, val loss: 1.0470150709152222
Epoch 1100, training loss: 62.75737762451172 = 0.17625391483306885 + 10.0 * 6.25811243057251
Epoch 1100, val loss: 1.0526632070541382
Epoch 1110, training loss: 62.71379089355469 = 0.1707158237695694 + 10.0 * 6.254307746887207
Epoch 1110, val loss: 1.0588608980178833
Epoch 1120, training loss: 62.7043571472168 = 0.16538020968437195 + 10.0 * 6.253897666931152
Epoch 1120, val loss: 1.0649547576904297
Epoch 1130, training loss: 62.733558654785156 = 0.1601990908384323 + 10.0 * 6.257336139678955
Epoch 1130, val loss: 1.0707157850265503
Epoch 1140, training loss: 62.6932258605957 = 0.1551707684993744 + 10.0 * 6.253805637359619
Epoch 1140, val loss: 1.0776418447494507
Epoch 1150, training loss: 62.710548400878906 = 0.15029147267341614 + 10.0 * 6.256025791168213
Epoch 1150, val loss: 1.0835795402526855
Epoch 1160, training loss: 62.68172073364258 = 0.14562512934207916 + 10.0 * 6.253609657287598
Epoch 1160, val loss: 1.090725064277649
Epoch 1170, training loss: 62.65007019042969 = 0.1410876214504242 + 10.0 * 6.250898361206055
Epoch 1170, val loss: 1.0966954231262207
Epoch 1180, training loss: 62.63882827758789 = 0.1367376446723938 + 10.0 * 6.250208854675293
Epoch 1180, val loss: 1.1033813953399658
Epoch 1190, training loss: 62.67799758911133 = 0.13252955675125122 + 10.0 * 6.254546642303467
Epoch 1190, val loss: 1.109498143196106
Epoch 1200, training loss: 62.63527297973633 = 0.12848035991191864 + 10.0 * 6.2506794929504395
Epoch 1200, val loss: 1.1171845197677612
Epoch 1210, training loss: 62.62537384033203 = 0.1245141327381134 + 10.0 * 6.250085830688477
Epoch 1210, val loss: 1.1232877969741821
Epoch 1220, training loss: 62.639732360839844 = 0.12075718492269516 + 10.0 * 6.25189733505249
Epoch 1220, val loss: 1.130756139755249
Epoch 1230, training loss: 62.59471130371094 = 0.11708535999059677 + 10.0 * 6.247762680053711
Epoch 1230, val loss: 1.1369761228561401
Epoch 1240, training loss: 62.58989715576172 = 0.11358088999986649 + 10.0 * 6.247631549835205
Epoch 1240, val loss: 1.1440645456314087
Epoch 1250, training loss: 62.60636520385742 = 0.11017642915248871 + 10.0 * 6.249619007110596
Epoch 1250, val loss: 1.1508917808532715
Epoch 1260, training loss: 62.591678619384766 = 0.10688328742980957 + 10.0 * 6.24847936630249
Epoch 1260, val loss: 1.1578516960144043
Epoch 1270, training loss: 62.584869384765625 = 0.10371147841215134 + 10.0 * 6.2481160163879395
Epoch 1270, val loss: 1.1648108959197998
Epoch 1280, training loss: 62.55112838745117 = 0.10064703971147537 + 10.0 * 6.2450480461120605
Epoch 1280, val loss: 1.1714059114456177
Epoch 1290, training loss: 62.53866958618164 = 0.09771309047937393 + 10.0 * 6.244095802307129
Epoch 1290, val loss: 1.1786510944366455
Epoch 1300, training loss: 62.52900695800781 = 0.09489001333713531 + 10.0 * 6.243411540985107
Epoch 1300, val loss: 1.1855571269989014
Epoch 1310, training loss: 62.56785202026367 = 0.09216722100973129 + 10.0 * 6.247568607330322
Epoch 1310, val loss: 1.1921212673187256
Epoch 1320, training loss: 62.55970764160156 = 0.08948074281215668 + 10.0 * 6.24702262878418
Epoch 1320, val loss: 1.1990574598312378
Epoch 1330, training loss: 62.53749465942383 = 0.08691579848527908 + 10.0 * 6.245058059692383
Epoch 1330, val loss: 1.2061697244644165
Epoch 1340, training loss: 62.50398254394531 = 0.08444139361381531 + 10.0 * 6.2419538497924805
Epoch 1340, val loss: 1.2130321264266968
Epoch 1350, training loss: 62.4971809387207 = 0.0820724219083786 + 10.0 * 6.24151086807251
Epoch 1350, val loss: 1.219852089881897
Epoch 1360, training loss: 62.55013656616211 = 0.07978126406669617 + 10.0 * 6.247035503387451
Epoch 1360, val loss: 1.22678804397583
Epoch 1370, training loss: 62.51504135131836 = 0.0775611475110054 + 10.0 * 6.243748188018799
Epoch 1370, val loss: 1.2338085174560547
Epoch 1380, training loss: 62.49034118652344 = 0.07539097219705582 + 10.0 * 6.241495132446289
Epoch 1380, val loss: 1.2401835918426514
Epoch 1390, training loss: 62.49081039428711 = 0.0733211562037468 + 10.0 * 6.241748809814453
Epoch 1390, val loss: 1.2469581365585327
Epoch 1400, training loss: 62.51854705810547 = 0.0713115930557251 + 10.0 * 6.244723320007324
Epoch 1400, val loss: 1.2539973258972168
Epoch 1410, training loss: 62.47782897949219 = 0.06937511265277863 + 10.0 * 6.240845680236816
Epoch 1410, val loss: 1.261012077331543
Epoch 1420, training loss: 62.45420455932617 = 0.06749275326728821 + 10.0 * 6.23867130279541
Epoch 1420, val loss: 1.267364740371704
Epoch 1430, training loss: 62.443397521972656 = 0.06570395827293396 + 10.0 * 6.23776912689209
Epoch 1430, val loss: 1.2745509147644043
Epoch 1440, training loss: 62.48120880126953 = 0.06397188454866409 + 10.0 * 6.241723537445068
Epoch 1440, val loss: 1.281099796295166
Epoch 1450, training loss: 62.456695556640625 = 0.06225569546222687 + 10.0 * 6.239443778991699
Epoch 1450, val loss: 1.2871805429458618
Epoch 1460, training loss: 62.435218811035156 = 0.06061441823840141 + 10.0 * 6.237460136413574
Epoch 1460, val loss: 1.29451584815979
Epoch 1470, training loss: 62.41904830932617 = 0.059028755873441696 + 10.0 * 6.236001968383789
Epoch 1470, val loss: 1.3007482290267944
Epoch 1480, training loss: 62.437103271484375 = 0.057511404156684875 + 10.0 * 6.237959384918213
Epoch 1480, val loss: 1.3075546026229858
Epoch 1490, training loss: 62.42988204956055 = 0.05601588636636734 + 10.0 * 6.237386703491211
Epoch 1490, val loss: 1.313839077949524
Epoch 1500, training loss: 62.4209098815918 = 0.054572172462940216 + 10.0 * 6.236633777618408
Epoch 1500, val loss: 1.3202300071716309
Epoch 1510, training loss: 62.431724548339844 = 0.05318262800574303 + 10.0 * 6.23785400390625
Epoch 1510, val loss: 1.3264403343200684
Epoch 1520, training loss: 62.397037506103516 = 0.051845017820596695 + 10.0 * 6.234519004821777
Epoch 1520, val loss: 1.3332347869873047
Epoch 1530, training loss: 62.4002571105957 = 0.050554655492305756 + 10.0 * 6.2349700927734375
Epoch 1530, val loss: 1.3396046161651611
Epoch 1540, training loss: 62.449256896972656 = 0.049308981746435165 + 10.0 * 6.239995002746582
Epoch 1540, val loss: 1.3458091020584106
Epoch 1550, training loss: 62.40419387817383 = 0.04806778207421303 + 10.0 * 6.235612392425537
Epoch 1550, val loss: 1.3513866662979126
Epoch 1560, training loss: 62.38671875 = 0.046892791986465454 + 10.0 * 6.233982563018799
Epoch 1560, val loss: 1.357986330986023
Epoch 1570, training loss: 62.4018669128418 = 0.045759182423353195 + 10.0 * 6.2356109619140625
Epoch 1570, val loss: 1.3642014265060425
Epoch 1580, training loss: 62.38771057128906 = 0.04464804381132126 + 10.0 * 6.234306335449219
Epoch 1580, val loss: 1.369988203048706
Epoch 1590, training loss: 62.37865447998047 = 0.043561507016420364 + 10.0 * 6.233509540557861
Epoch 1590, val loss: 1.37535560131073
Epoch 1600, training loss: 62.40489196777344 = 0.04253248870372772 + 10.0 * 6.236235618591309
Epoch 1600, val loss: 1.381957769393921
Epoch 1610, training loss: 62.36070251464844 = 0.041520167142152786 + 10.0 * 6.2319183349609375
Epoch 1610, val loss: 1.3872307538986206
Epoch 1620, training loss: 62.37724685668945 = 0.04055335000157356 + 10.0 * 6.233669281005859
Epoch 1620, val loss: 1.3936967849731445
Epoch 1630, training loss: 62.35194778442383 = 0.03960203751921654 + 10.0 * 6.231234550476074
Epoch 1630, val loss: 1.3987561464309692
Epoch 1640, training loss: 62.34695816040039 = 0.038691382855176926 + 10.0 * 6.230826377868652
Epoch 1640, val loss: 1.4045778512954712
Epoch 1650, training loss: 62.33921432495117 = 0.03781390190124512 + 10.0 * 6.23013973236084
Epoch 1650, val loss: 1.410452961921692
Epoch 1660, training loss: 62.401397705078125 = 0.036963384598493576 + 10.0 * 6.236443519592285
Epoch 1660, val loss: 1.4161609411239624
Epoch 1670, training loss: 62.361751556396484 = 0.03611273691058159 + 10.0 * 6.2325639724731445
Epoch 1670, val loss: 1.4210479259490967
Epoch 1680, training loss: 62.35173034667969 = 0.035302530974149704 + 10.0 * 6.231642723083496
Epoch 1680, val loss: 1.4270166158676147
Epoch 1690, training loss: 62.341697692871094 = 0.03451474756002426 + 10.0 * 6.23071813583374
Epoch 1690, val loss: 1.4320839643478394
Epoch 1700, training loss: 62.328025817871094 = 0.03375977650284767 + 10.0 * 6.229426383972168
Epoch 1700, val loss: 1.4376529455184937
Epoch 1710, training loss: 62.32538986206055 = 0.03302446007728577 + 10.0 * 6.229236602783203
Epoch 1710, val loss: 1.442811131477356
Epoch 1720, training loss: 62.33951950073242 = 0.03231048956513405 + 10.0 * 6.2307209968566895
Epoch 1720, val loss: 1.447997808456421
Epoch 1730, training loss: 62.335994720458984 = 0.0316137969493866 + 10.0 * 6.230438232421875
Epoch 1730, val loss: 1.4532963037490845
Epoch 1740, training loss: 62.34523010253906 = 0.030943643301725388 + 10.0 * 6.231428623199463
Epoch 1740, val loss: 1.4583909511566162
Epoch 1750, training loss: 62.319332122802734 = 0.03027675487101078 + 10.0 * 6.22890567779541
Epoch 1750, val loss: 1.46342134475708
Epoch 1760, training loss: 62.305931091308594 = 0.02963830530643463 + 10.0 * 6.2276291847229
Epoch 1760, val loss: 1.4687671661376953
Epoch 1770, training loss: 62.29497528076172 = 0.02902192249894142 + 10.0 * 6.226595401763916
Epoch 1770, val loss: 1.4737215042114258
Epoch 1780, training loss: 62.29361343383789 = 0.028432387858629227 + 10.0 * 6.226518154144287
Epoch 1780, val loss: 1.4788823127746582
Epoch 1790, training loss: 62.37293243408203 = 0.02786143496632576 + 10.0 * 6.234507083892822
Epoch 1790, val loss: 1.4841370582580566
Epoch 1800, training loss: 62.29582595825195 = 0.02726994827389717 + 10.0 * 6.226855278015137
Epoch 1800, val loss: 1.4881649017333984
Epoch 1810, training loss: 62.2771110534668 = 0.026722682639956474 + 10.0 * 6.225039005279541
Epoch 1810, val loss: 1.4933468103408813
Epoch 1820, training loss: 62.300689697265625 = 0.026194067671895027 + 10.0 * 6.227449417114258
Epoch 1820, val loss: 1.498561143875122
Epoch 1830, training loss: 62.2809944152832 = 0.025669991970062256 + 10.0 * 6.225532531738281
Epoch 1830, val loss: 1.502407193183899
Epoch 1840, training loss: 62.281349182128906 = 0.025162935256958008 + 10.0 * 6.225618839263916
Epoch 1840, val loss: 1.5071008205413818
Epoch 1850, training loss: 62.27227020263672 = 0.0246712826192379 + 10.0 * 6.224760055541992
Epoch 1850, val loss: 1.5119422674179077
Epoch 1860, training loss: 62.276161193847656 = 0.0241972915828228 + 10.0 * 6.225196361541748
Epoch 1860, val loss: 1.5161762237548828
Epoch 1870, training loss: 62.30146789550781 = 0.023733556270599365 + 10.0 * 6.227773189544678
Epoch 1870, val loss: 1.5207276344299316
Epoch 1880, training loss: 62.278377532958984 = 0.023275965824723244 + 10.0 * 6.225510120391846
Epoch 1880, val loss: 1.5258257389068604
Epoch 1890, training loss: 62.266021728515625 = 0.02283661440014839 + 10.0 * 6.224318504333496
Epoch 1890, val loss: 1.5299957990646362
Epoch 1900, training loss: 62.341129302978516 = 0.022406643256545067 + 10.0 * 6.231872081756592
Epoch 1900, val loss: 1.534394383430481
Epoch 1910, training loss: 62.28181076049805 = 0.021984469145536423 + 10.0 * 6.225982666015625
Epoch 1910, val loss: 1.5385328531265259
Epoch 1920, training loss: 62.25440216064453 = 0.021576836705207825 + 10.0 * 6.223282814025879
Epoch 1920, val loss: 1.5430505275726318
Epoch 1930, training loss: 62.23839569091797 = 0.02118590846657753 + 10.0 * 6.2217206954956055
Epoch 1930, val loss: 1.5474321842193604
Epoch 1940, training loss: 62.23596954345703 = 0.02080424875020981 + 10.0 * 6.2215166091918945
Epoch 1940, val loss: 1.5517123937606812
Epoch 1950, training loss: 62.3186149597168 = 0.02043364755809307 + 10.0 * 6.229817867279053
Epoch 1950, val loss: 1.5553066730499268
Epoch 1960, training loss: 62.300621032714844 = 0.020063823089003563 + 10.0 * 6.228055477142334
Epoch 1960, val loss: 1.5594488382339478
Epoch 1970, training loss: 62.242000579833984 = 0.019695183262228966 + 10.0 * 6.222230434417725
Epoch 1970, val loss: 1.5637450218200684
Epoch 1980, training loss: 62.222476959228516 = 0.01935095526278019 + 10.0 * 6.220312595367432
Epoch 1980, val loss: 1.5678602457046509
Epoch 1990, training loss: 62.219661712646484 = 0.019017549231648445 + 10.0 * 6.220064640045166
Epoch 1990, val loss: 1.5718332529067993
Epoch 2000, training loss: 62.236534118652344 = 0.018693068996071815 + 10.0 * 6.2217841148376465
Epoch 2000, val loss: 1.5754410028457642
Epoch 2010, training loss: 62.24033737182617 = 0.01837058924138546 + 10.0 * 6.222196578979492
Epoch 2010, val loss: 1.579342246055603
Epoch 2020, training loss: 62.254310607910156 = 0.018053216859698296 + 10.0 * 6.223625659942627
Epoch 2020, val loss: 1.5831955671310425
Epoch 2030, training loss: 62.27018356323242 = 0.017742175608873367 + 10.0 * 6.225244045257568
Epoch 2030, val loss: 1.5868338346481323
Epoch 2040, training loss: 62.23442459106445 = 0.0174441859126091 + 10.0 * 6.221697807312012
Epoch 2040, val loss: 1.5909355878829956
Epoch 2050, training loss: 62.21624755859375 = 0.017149798572063446 + 10.0 * 6.21990966796875
Epoch 2050, val loss: 1.5947587490081787
Epoch 2060, training loss: 62.21381378173828 = 0.016870245337486267 + 10.0 * 6.2196946144104
Epoch 2060, val loss: 1.5983930826187134
Epoch 2070, training loss: 62.2439079284668 = 0.016597220674157143 + 10.0 * 6.222731113433838
Epoch 2070, val loss: 1.6018904447555542
Epoch 2080, training loss: 62.1954231262207 = 0.016321737319231033 + 10.0 * 6.217909812927246
Epoch 2080, val loss: 1.6059057712554932
Epoch 2090, training loss: 62.22736358642578 = 0.016063010320067406 + 10.0 * 6.221129894256592
Epoch 2090, val loss: 1.6095633506774902
Epoch 2100, training loss: 62.22600555419922 = 0.015803707763552666 + 10.0 * 6.221020221710205
Epoch 2100, val loss: 1.6127103567123413
Epoch 2110, training loss: 62.20018005371094 = 0.015547029674053192 + 10.0 * 6.218462944030762
Epoch 2110, val loss: 1.61653470993042
Epoch 2120, training loss: 62.200408935546875 = 0.015304227359592915 + 10.0 * 6.218510627746582
Epoch 2120, val loss: 1.62015962600708
Epoch 2130, training loss: 62.221675872802734 = 0.015065568499267101 + 10.0 * 6.220661163330078
Epoch 2130, val loss: 1.623095154762268
Epoch 2140, training loss: 62.1908073425293 = 0.014830224215984344 + 10.0 * 6.217597484588623
Epoch 2140, val loss: 1.6269490718841553
Epoch 2150, training loss: 62.19805145263672 = 0.014603644609451294 + 10.0 * 6.218344688415527
Epoch 2150, val loss: 1.6304651498794556
Epoch 2160, training loss: 62.22772979736328 = 0.014379536733031273 + 10.0 * 6.221334934234619
Epoch 2160, val loss: 1.6342021226882935
Epoch 2170, training loss: 62.19334030151367 = 0.014162172563374043 + 10.0 * 6.217917442321777
Epoch 2170, val loss: 1.6372323036193848
Epoch 2180, training loss: 62.178382873535156 = 0.013946471735835075 + 10.0 * 6.2164435386657715
Epoch 2180, val loss: 1.6403355598449707
Epoch 2190, training loss: 62.17916488647461 = 0.013744467869400978 + 10.0 * 6.2165422439575195
Epoch 2190, val loss: 1.643750786781311
Epoch 2200, training loss: 62.271018981933594 = 0.013543994165956974 + 10.0 * 6.225747585296631
Epoch 2200, val loss: 1.6468209028244019
Epoch 2210, training loss: 62.20506286621094 = 0.013334371149539948 + 10.0 * 6.219172477722168
Epoch 2210, val loss: 1.650355339050293
Epoch 2220, training loss: 62.173004150390625 = 0.013139440678060055 + 10.0 * 6.215986728668213
Epoch 2220, val loss: 1.6534769535064697
Epoch 2230, training loss: 62.16524887084961 = 0.012950890697538853 + 10.0 * 6.2152299880981445
Epoch 2230, val loss: 1.6569459438323975
Epoch 2240, training loss: 62.200660705566406 = 0.0127743910998106 + 10.0 * 6.2187886238098145
Epoch 2240, val loss: 1.6605249643325806
Epoch 2250, training loss: 62.193870544433594 = 0.012584217824041843 + 10.0 * 6.218128681182861
Epoch 2250, val loss: 1.6630396842956543
Epoch 2260, training loss: 62.162166595458984 = 0.012402425520122051 + 10.0 * 6.2149763107299805
Epoch 2260, val loss: 1.6658028364181519
Epoch 2270, training loss: 62.159725189208984 = 0.012225834652781487 + 10.0 * 6.214749813079834
Epoch 2270, val loss: 1.66881263256073
Epoch 2280, training loss: 62.15511703491211 = 0.0120605593547225 + 10.0 * 6.214305400848389
Epoch 2280, val loss: 1.6722655296325684
Epoch 2290, training loss: 62.18273162841797 = 0.011896125040948391 + 10.0 * 6.217083930969238
Epoch 2290, val loss: 1.675019383430481
Epoch 2300, training loss: 62.14647674560547 = 0.01173155102878809 + 10.0 * 6.213474750518799
Epoch 2300, val loss: 1.6780071258544922
Epoch 2310, training loss: 62.20439529418945 = 0.0115743987262249 + 10.0 * 6.219282150268555
Epoch 2310, val loss: 1.6805404424667358
Epoch 2320, training loss: 62.162574768066406 = 0.0114144841209054 + 10.0 * 6.215116024017334
Epoch 2320, val loss: 1.684043526649475
Epoch 2330, training loss: 62.1534538269043 = 0.011257266625761986 + 10.0 * 6.214219570159912
Epoch 2330, val loss: 1.6866317987442017
Epoch 2340, training loss: 62.14692306518555 = 0.011109823361039162 + 10.0 * 6.213581562042236
Epoch 2340, val loss: 1.6899594068527222
Epoch 2350, training loss: 62.19816970825195 = 0.010966571047902107 + 10.0 * 6.218720436096191
Epoch 2350, val loss: 1.6928399801254272
Epoch 2360, training loss: 62.1432991027832 = 0.010814688168466091 + 10.0 * 6.213248252868652
Epoch 2360, val loss: 1.6950225830078125
Epoch 2370, training loss: 62.14549255371094 = 0.010673856362700462 + 10.0 * 6.213481903076172
Epoch 2370, val loss: 1.6976882219314575
Epoch 2380, training loss: 62.146610260009766 = 0.010537796653807163 + 10.0 * 6.213607311248779
Epoch 2380, val loss: 1.700652003288269
Epoch 2390, training loss: 62.14470291137695 = 0.010401071980595589 + 10.0 * 6.213429927825928
Epoch 2390, val loss: 1.7032568454742432
Epoch 2400, training loss: 62.15821838378906 = 0.01027000043541193 + 10.0 * 6.214795112609863
Epoch 2400, val loss: 1.7061172723770142
Epoch 2410, training loss: 62.12652587890625 = 0.010136798955500126 + 10.0 * 6.211638927459717
Epoch 2410, val loss: 1.7088937759399414
Epoch 2420, training loss: 62.142974853515625 = 0.010011015459895134 + 10.0 * 6.213296413421631
Epoch 2420, val loss: 1.711604356765747
Epoch 2430, training loss: 62.163719177246094 = 0.009885872714221478 + 10.0 * 6.215383052825928
Epoch 2430, val loss: 1.714282751083374
Epoch 2440, training loss: 62.14841842651367 = 0.009762044996023178 + 10.0 * 6.213865756988525
Epoch 2440, val loss: 1.7162117958068848
Epoch 2450, training loss: 62.14973068237305 = 0.009639434516429901 + 10.0 * 6.2140092849731445
Epoch 2450, val loss: 1.718997836112976
Epoch 2460, training loss: 62.130680084228516 = 0.009522979147732258 + 10.0 * 6.21211576461792
Epoch 2460, val loss: 1.72130286693573
Epoch 2470, training loss: 62.12696075439453 = 0.009408220648765564 + 10.0 * 6.211755275726318
Epoch 2470, val loss: 1.7242519855499268
Epoch 2480, training loss: 62.13167953491211 = 0.009295709431171417 + 10.0 * 6.212238311767578
Epoch 2480, val loss: 1.7269790172576904
Epoch 2490, training loss: 62.14006805419922 = 0.009186463430523872 + 10.0 * 6.213088035583496
Epoch 2490, val loss: 1.729802131652832
Epoch 2500, training loss: 62.131744384765625 = 0.009072951972484589 + 10.0 * 6.2122673988342285
Epoch 2500, val loss: 1.731540560722351
Epoch 2510, training loss: 62.12937545776367 = 0.008966684341430664 + 10.0 * 6.212040901184082
Epoch 2510, val loss: 1.7338426113128662
Epoch 2520, training loss: 62.1246452331543 = 0.008862745016813278 + 10.0 * 6.211578369140625
Epoch 2520, val loss: 1.7364150285720825
Epoch 2530, training loss: 62.12704086303711 = 0.008757754229009151 + 10.0 * 6.211828231811523
Epoch 2530, val loss: 1.738323450088501
Epoch 2540, training loss: 62.10507583618164 = 0.008655060082674026 + 10.0 * 6.209641933441162
Epoch 2540, val loss: 1.7410168647766113
Epoch 2550, training loss: 62.11886215209961 = 0.008558187633752823 + 10.0 * 6.21103048324585
Epoch 2550, val loss: 1.743472933769226
Epoch 2560, training loss: 62.11909103393555 = 0.00846020132303238 + 10.0 * 6.211062908172607
Epoch 2560, val loss: 1.745552897453308
Epoch 2570, training loss: 62.1299934387207 = 0.008365723304450512 + 10.0 * 6.212162971496582
Epoch 2570, val loss: 1.7481263875961304
Epoch 2580, training loss: 62.12865447998047 = 0.008272325620055199 + 10.0 * 6.212038040161133
Epoch 2580, val loss: 1.7507349252700806
Epoch 2590, training loss: 62.11125183105469 = 0.008175689727067947 + 10.0 * 6.210307598114014
Epoch 2590, val loss: 1.7522114515304565
Epoch 2600, training loss: 62.10453414916992 = 0.008086473681032658 + 10.0 * 6.209644794464111
Epoch 2600, val loss: 1.7550914287567139
Epoch 2610, training loss: 62.107513427734375 = 0.00799896102398634 + 10.0 * 6.209951400756836
Epoch 2610, val loss: 1.757167935371399
Epoch 2620, training loss: 62.11890411376953 = 0.00791197456419468 + 10.0 * 6.211099147796631
Epoch 2620, val loss: 1.7591912746429443
Epoch 2630, training loss: 62.09383010864258 = 0.007825521752238274 + 10.0 * 6.2086005210876465
Epoch 2630, val loss: 1.7610478401184082
Epoch 2640, training loss: 62.13079833984375 = 0.007742537185549736 + 10.0 * 6.212305545806885
Epoch 2640, val loss: 1.7629282474517822
Epoch 2650, training loss: 62.09891128540039 = 0.007656665053218603 + 10.0 * 6.209125518798828
Epoch 2650, val loss: 1.7655737400054932
Epoch 2660, training loss: 62.091712951660156 = 0.00757672218605876 + 10.0 * 6.208413600921631
Epoch 2660, val loss: 1.7673918008804321
Epoch 2670, training loss: 62.09720993041992 = 0.007496585138142109 + 10.0 * 6.2089715003967285
Epoch 2670, val loss: 1.7691985368728638
Epoch 2680, training loss: 62.10325622558594 = 0.007417657878249884 + 10.0 * 6.2095842361450195
Epoch 2680, val loss: 1.771160364151001
Epoch 2690, training loss: 62.1201286315918 = 0.007341571152210236 + 10.0 * 6.211278438568115
Epoch 2690, val loss: 1.77383553981781
Epoch 2700, training loss: 62.07666015625 = 0.0072648776695132256 + 10.0 * 6.206939697265625
Epoch 2700, val loss: 1.7755298614501953
Epoch 2710, training loss: 62.07729721069336 = 0.007191102020442486 + 10.0 * 6.207010746002197
Epoch 2710, val loss: 1.777750015258789
Epoch 2720, training loss: 62.08366394042969 = 0.007119327783584595 + 10.0 * 6.2076544761657715
Epoch 2720, val loss: 1.7798378467559814
Epoch 2730, training loss: 62.1002082824707 = 0.0070465258322656155 + 10.0 * 6.209316253662109
Epoch 2730, val loss: 1.78167724609375
Epoch 2740, training loss: 62.09941101074219 = 0.006975233554840088 + 10.0 * 6.209243297576904
Epoch 2740, val loss: 1.7834168672561646
Epoch 2750, training loss: 62.07912063598633 = 0.00690433569252491 + 10.0 * 6.207221508026123
Epoch 2750, val loss: 1.7854185104370117
Epoch 2760, training loss: 62.08584976196289 = 0.006836245767772198 + 10.0 * 6.207901477813721
Epoch 2760, val loss: 1.7871649265289307
Epoch 2770, training loss: 62.0887336730957 = 0.006768678314983845 + 10.0 * 6.208196640014648
Epoch 2770, val loss: 1.7890706062316895
Epoch 2780, training loss: 62.08616256713867 = 0.006702790968120098 + 10.0 * 6.207945823669434
Epoch 2780, val loss: 1.7904318571090698
Epoch 2790, training loss: 62.08733367919922 = 0.0066360956989228725 + 10.0 * 6.208069801330566
Epoch 2790, val loss: 1.7924691438674927
Epoch 2800, training loss: 62.105648040771484 = 0.00657016783952713 + 10.0 * 6.2099080085754395
Epoch 2800, val loss: 1.7936396598815918
Epoch 2810, training loss: 62.05948257446289 = 0.00650780601426959 + 10.0 * 6.205297470092773
Epoch 2810, val loss: 1.7958563566207886
Epoch 2820, training loss: 62.053531646728516 = 0.006446576211601496 + 10.0 * 6.204708576202393
Epoch 2820, val loss: 1.7980220317840576
Epoch 2830, training loss: 62.06366729736328 = 0.0063872020691633224 + 10.0 * 6.205728054046631
Epoch 2830, val loss: 1.7997233867645264
Epoch 2840, training loss: 62.08096694946289 = 0.006328039336949587 + 10.0 * 6.20746374130249
Epoch 2840, val loss: 1.8013252019882202
Epoch 2850, training loss: 62.07027816772461 = 0.006268883589655161 + 10.0 * 6.2064008712768555
Epoch 2850, val loss: 1.8021858930587769
Epoch 2860, training loss: 62.065425872802734 = 0.006209459155797958 + 10.0 * 6.205921649932861
Epoch 2860, val loss: 1.8039230108261108
Epoch 2870, training loss: 62.1031608581543 = 0.006150951609015465 + 10.0 * 6.209701061248779
Epoch 2870, val loss: 1.8055615425109863
Epoch 2880, training loss: 62.0688362121582 = 0.006094958167523146 + 10.0 * 6.206274032592773
Epoch 2880, val loss: 1.8075307607650757
Epoch 2890, training loss: 62.04441452026367 = 0.006037499755620956 + 10.0 * 6.2038373947143555
Epoch 2890, val loss: 1.8089871406555176
Epoch 2900, training loss: 62.048526763916016 = 0.005984881427139044 + 10.0 * 6.204254150390625
Epoch 2900, val loss: 1.8106939792633057
Epoch 2910, training loss: 62.11040115356445 = 0.005932350177317858 + 10.0 * 6.210446834564209
Epoch 2910, val loss: 1.8121424913406372
Epoch 2920, training loss: 62.05759048461914 = 0.0058790394105017185 + 10.0 * 6.20517110824585
Epoch 2920, val loss: 1.8136787414550781
Epoch 2930, training loss: 62.04329299926758 = 0.005825376138091087 + 10.0 * 6.203746795654297
Epoch 2930, val loss: 1.8149945735931396
Epoch 2940, training loss: 62.0448112487793 = 0.00577560905367136 + 10.0 * 6.203903675079346
Epoch 2940, val loss: 1.8166264295578003
Epoch 2950, training loss: 62.062591552734375 = 0.005727038253098726 + 10.0 * 6.205686569213867
Epoch 2950, val loss: 1.8183541297912598
Epoch 2960, training loss: 62.04078674316406 = 0.005675901658833027 + 10.0 * 6.2035112380981445
Epoch 2960, val loss: 1.8198984861373901
Epoch 2970, training loss: 62.070884704589844 = 0.005628894083201885 + 10.0 * 6.2065253257751465
Epoch 2970, val loss: 1.8218083381652832
Epoch 2980, training loss: 62.05491256713867 = 0.005579511169344187 + 10.0 * 6.204933166503906
Epoch 2980, val loss: 1.8226916790008545
Epoch 2990, training loss: 62.05168914794922 = 0.00553002068772912 + 10.0 * 6.204615592956543
Epoch 2990, val loss: 1.823754072189331
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 87.91849517822266 = 1.9498076438903809 + 10.0 * 8.596868515014648
Epoch 0, val loss: 1.9530117511749268
Epoch 10, training loss: 87.90499877929688 = 1.9398865699768066 + 10.0 * 8.596510887145996
Epoch 10, val loss: 1.9423351287841797
Epoch 20, training loss: 87.86760711669922 = 1.9277901649475098 + 10.0 * 8.593981742858887
Epoch 20, val loss: 1.9289835691452026
Epoch 30, training loss: 87.67604064941406 = 1.9120322465896606 + 10.0 * 8.576400756835938
Epoch 30, val loss: 1.911311149597168
Epoch 40, training loss: 86.55213928222656 = 1.8926682472229004 + 10.0 * 8.465947151184082
Epoch 40, val loss: 1.8901231288909912
Epoch 50, training loss: 80.1695785522461 = 1.8730052709579468 + 10.0 * 7.829656600952148
Epoch 50, val loss: 1.869381308555603
Epoch 60, training loss: 76.4321060180664 = 1.8579622507095337 + 10.0 * 7.457414150238037
Epoch 60, val loss: 1.85435950756073
Epoch 70, training loss: 73.70122528076172 = 1.8476585149765015 + 10.0 * 7.185357093811035
Epoch 70, val loss: 1.8444510698318481
Epoch 80, training loss: 72.06810760498047 = 1.8360990285873413 + 10.0 * 7.023200988769531
Epoch 80, val loss: 1.8332377672195435
Epoch 90, training loss: 71.02464294433594 = 1.824431300163269 + 10.0 * 6.920021057128906
Epoch 90, val loss: 1.8219727277755737
Epoch 100, training loss: 70.255126953125 = 1.813594102859497 + 10.0 * 6.844153881072998
Epoch 100, val loss: 1.8112950325012207
Epoch 110, training loss: 69.7042465209961 = 1.8050708770751953 + 10.0 * 6.789917945861816
Epoch 110, val loss: 1.80252206325531
Epoch 120, training loss: 69.17814636230469 = 1.7974722385406494 + 10.0 * 6.738067626953125
Epoch 120, val loss: 1.795153021812439
Epoch 130, training loss: 68.7214584350586 = 1.7903850078582764 + 10.0 * 6.693107604980469
Epoch 130, val loss: 1.788534164428711
Epoch 140, training loss: 68.31867218017578 = 1.7832280397415161 + 10.0 * 6.6535444259643555
Epoch 140, val loss: 1.781807541847229
Epoch 150, training loss: 67.92230987548828 = 1.775720477104187 + 10.0 * 6.614658832550049
Epoch 150, val loss: 1.7747881412506104
Epoch 160, training loss: 67.62208557128906 = 1.7678272724151611 + 10.0 * 6.585425853729248
Epoch 160, val loss: 1.7672499418258667
Epoch 170, training loss: 67.33492279052734 = 1.7588902711868286 + 10.0 * 6.55760383605957
Epoch 170, val loss: 1.7591675519943237
Epoch 180, training loss: 67.07386016845703 = 1.749114751815796 + 10.0 * 6.532474517822266
Epoch 180, val loss: 1.7503732442855835
Epoch 190, training loss: 66.85760498046875 = 1.7385342121124268 + 10.0 * 6.51190710067749
Epoch 190, val loss: 1.7410430908203125
Epoch 200, training loss: 66.66724395751953 = 1.7269989252090454 + 10.0 * 6.494024276733398
Epoch 200, val loss: 1.7309536933898926
Epoch 210, training loss: 66.49981689453125 = 1.7143847942352295 + 10.0 * 6.478542804718018
Epoch 210, val loss: 1.720054030418396
Epoch 220, training loss: 66.44517517089844 = 1.700567364692688 + 10.0 * 6.474460601806641
Epoch 220, val loss: 1.7080644369125366
Epoch 230, training loss: 66.26068115234375 = 1.6851731538772583 + 10.0 * 6.457550525665283
Epoch 230, val loss: 1.6950793266296387
Epoch 240, training loss: 66.11415100097656 = 1.66852605342865 + 10.0 * 6.4445624351501465
Epoch 240, val loss: 1.6810567378997803
Epoch 250, training loss: 65.99244689941406 = 1.6504478454589844 + 10.0 * 6.434199333190918
Epoch 250, val loss: 1.6658592224121094
Epoch 260, training loss: 65.90777587890625 = 1.6308647394180298 + 10.0 * 6.4276909828186035
Epoch 260, val loss: 1.649490475654602
Epoch 270, training loss: 65.7833023071289 = 1.609619140625 + 10.0 * 6.417368412017822
Epoch 270, val loss: 1.63210928440094
Epoch 280, training loss: 65.68792724609375 = 1.5868855714797974 + 10.0 * 6.410104274749756
Epoch 280, val loss: 1.6134454011917114
Epoch 290, training loss: 65.60713195800781 = 1.5625755786895752 + 10.0 * 6.404455184936523
Epoch 290, val loss: 1.5938338041305542
Epoch 300, training loss: 65.51675415039062 = 1.5368002653121948 + 10.0 * 6.397995948791504
Epoch 300, val loss: 1.5727711915969849
Epoch 310, training loss: 65.42781829833984 = 1.5097119808197021 + 10.0 * 6.391810417175293
Epoch 310, val loss: 1.5512371063232422
Epoch 320, training loss: 65.37024688720703 = 1.4815008640289307 + 10.0 * 6.3888750076293945
Epoch 320, val loss: 1.528854489326477
Epoch 330, training loss: 65.25592041015625 = 1.4522087574005127 + 10.0 * 6.380371570587158
Epoch 330, val loss: 1.5060899257659912
Epoch 340, training loss: 65.17890930175781 = 1.4222041368484497 + 10.0 * 6.375670433044434
Epoch 340, val loss: 1.4829463958740234
Epoch 350, training loss: 65.12103271484375 = 1.3914644718170166 + 10.0 * 6.3729567527771
Epoch 350, val loss: 1.4597179889678955
Epoch 360, training loss: 65.06159973144531 = 1.360425591468811 + 10.0 * 6.3701171875
Epoch 360, val loss: 1.4357396364212036
Epoch 370, training loss: 64.96221923828125 = 1.3290938138961792 + 10.0 * 6.363312721252441
Epoch 370, val loss: 1.4124935865402222
Epoch 380, training loss: 64.89244842529297 = 1.2977559566497803 + 10.0 * 6.359469413757324
Epoch 380, val loss: 1.3894784450531006
Epoch 390, training loss: 64.8384780883789 = 1.266446590423584 + 10.0 * 6.357203006744385
Epoch 390, val loss: 1.3665941953659058
Epoch 400, training loss: 64.7703857421875 = 1.2353779077529907 + 10.0 * 6.353500843048096
Epoch 400, val loss: 1.3442165851593018
Epoch 410, training loss: 64.69731903076172 = 1.2046887874603271 + 10.0 * 6.3492631912231445
Epoch 410, val loss: 1.3224546909332275
Epoch 420, training loss: 64.64091491699219 = 1.174502968788147 + 10.0 * 6.346641540527344
Epoch 420, val loss: 1.301275372505188
Epoch 430, training loss: 64.57440185546875 = 1.1447824239730835 + 10.0 * 6.342962265014648
Epoch 430, val loss: 1.2803888320922852
Epoch 440, training loss: 64.54574584960938 = 1.1155482530593872 + 10.0 * 6.343019962310791
Epoch 440, val loss: 1.2602144479751587
Epoch 450, training loss: 64.47578430175781 = 1.086823582649231 + 10.0 * 6.338895797729492
Epoch 450, val loss: 1.2408342361450195
Epoch 460, training loss: 64.40843200683594 = 1.0587421655654907 + 10.0 * 6.334969520568848
Epoch 460, val loss: 1.2218915224075317
Epoch 470, training loss: 64.3795166015625 = 1.031240463256836 + 10.0 * 6.334827899932861
Epoch 470, val loss: 1.203633427619934
Epoch 480, training loss: 64.336181640625 = 1.0042158365249634 + 10.0 * 6.333196640014648
Epoch 480, val loss: 1.185943841934204
Epoch 490, training loss: 64.26598358154297 = 0.9779050350189209 + 10.0 * 6.328807830810547
Epoch 490, val loss: 1.1689527034759521
Epoch 500, training loss: 64.2015609741211 = 0.9522979259490967 + 10.0 * 6.324926376342773
Epoch 500, val loss: 1.1528230905532837
Epoch 510, training loss: 64.1513442993164 = 0.9273305535316467 + 10.0 * 6.32240104675293
Epoch 510, val loss: 1.137336015701294
Epoch 520, training loss: 64.15415954589844 = 0.9028273224830627 + 10.0 * 6.325133323669434
Epoch 520, val loss: 1.1224703788757324
Epoch 530, training loss: 64.10540008544922 = 0.8788467645645142 + 10.0 * 6.32265567779541
Epoch 530, val loss: 1.108353853225708
Epoch 540, training loss: 64.03853607177734 = 0.8555196523666382 + 10.0 * 6.318301677703857
Epoch 540, val loss: 1.094380259513855
Epoch 550, training loss: 63.97873306274414 = 0.8326947689056396 + 10.0 * 6.314603805541992
Epoch 550, val loss: 1.0815656185150146
Epoch 560, training loss: 63.93451690673828 = 0.810487687587738 + 10.0 * 6.312403202056885
Epoch 560, val loss: 1.0691792964935303
Epoch 570, training loss: 63.98162078857422 = 0.7885987162590027 + 10.0 * 6.319302558898926
Epoch 570, val loss: 1.0574675798416138
Epoch 580, training loss: 63.865333557128906 = 0.7673335075378418 + 10.0 * 6.309800148010254
Epoch 580, val loss: 1.0460007190704346
Epoch 590, training loss: 63.81538772583008 = 0.746557891368866 + 10.0 * 6.306882858276367
Epoch 590, val loss: 1.035253643989563
Epoch 600, training loss: 63.77926254272461 = 0.7262771129608154 + 10.0 * 6.305298805236816
Epoch 600, val loss: 1.025247573852539
Epoch 610, training loss: 63.78401184082031 = 0.7063568234443665 + 10.0 * 6.307765483856201
Epoch 610, val loss: 1.0158791542053223
Epoch 620, training loss: 63.752777099609375 = 0.6868306994438171 + 10.0 * 6.306594371795654
Epoch 620, val loss: 1.0066596269607544
Epoch 630, training loss: 63.69103240966797 = 0.6677250862121582 + 10.0 * 6.30233097076416
Epoch 630, val loss: 0.9973004460334778
Epoch 640, training loss: 63.650474548339844 = 0.648995041847229 + 10.0 * 6.300148010253906
Epoch 640, val loss: 0.9894550442695618
Epoch 650, training loss: 63.610050201416016 = 0.6307417154312134 + 10.0 * 6.297930717468262
Epoch 650, val loss: 0.9818063378334045
Epoch 660, training loss: 63.61334228515625 = 0.6127440333366394 + 10.0 * 6.300059795379639
Epoch 660, val loss: 0.9748594164848328
Epoch 670, training loss: 63.57128143310547 = 0.5950854420661926 + 10.0 * 6.297619819641113
Epoch 670, val loss: 0.9674562811851501
Epoch 680, training loss: 63.515689849853516 = 0.5776519775390625 + 10.0 * 6.293803691864014
Epoch 680, val loss: 0.9611927270889282
Epoch 690, training loss: 63.479026794433594 = 0.5606399774551392 + 10.0 * 6.291838645935059
Epoch 690, val loss: 0.9553132057189941
Epoch 700, training loss: 63.45606231689453 = 0.5439028739929199 + 10.0 * 6.291215896606445
Epoch 700, val loss: 0.9497892260551453
Epoch 710, training loss: 63.45161819458008 = 0.5273054838180542 + 10.0 * 6.292431354522705
Epoch 710, val loss: 0.9444716572761536
Epoch 720, training loss: 63.41281509399414 = 0.5110864043235779 + 10.0 * 6.290173053741455
Epoch 720, val loss: 0.9395725727081299
Epoch 730, training loss: 63.37504196166992 = 0.4952346384525299 + 10.0 * 6.287980556488037
Epoch 730, val loss: 0.9352394938468933
Epoch 740, training loss: 63.36189270019531 = 0.4796355664730072 + 10.0 * 6.2882256507873535
Epoch 740, val loss: 0.9312299489974976
Epoch 750, training loss: 63.31980895996094 = 0.4643535017967224 + 10.0 * 6.285545825958252
Epoch 750, val loss: 0.9275129437446594
Epoch 760, training loss: 63.28777313232422 = 0.4493945837020874 + 10.0 * 6.283837795257568
Epoch 760, val loss: 0.924652636051178
Epoch 770, training loss: 63.34804916381836 = 0.43470627069473267 + 10.0 * 6.29133415222168
Epoch 770, val loss: 0.9219956398010254
Epoch 780, training loss: 63.23820877075195 = 0.42027416825294495 + 10.0 * 6.281793594360352
Epoch 780, val loss: 0.9193552136421204
Epoch 790, training loss: 63.212303161621094 = 0.4062950611114502 + 10.0 * 6.280600547790527
Epoch 790, val loss: 0.9174965023994446
Epoch 800, training loss: 63.18596267700195 = 0.3926906883716583 + 10.0 * 6.279326915740967
Epoch 800, val loss: 0.9162483215332031
Epoch 810, training loss: 63.17380905151367 = 0.3794439733028412 + 10.0 * 6.2794365882873535
Epoch 810, val loss: 0.915215790271759
Epoch 820, training loss: 63.144832611083984 = 0.3664495348930359 + 10.0 * 6.277838230133057
Epoch 820, val loss: 0.914423942565918
Epoch 830, training loss: 63.14051055908203 = 0.35387229919433594 + 10.0 * 6.278663635253906
Epoch 830, val loss: 0.9139943718910217
Epoch 840, training loss: 63.09463119506836 = 0.34170809388160706 + 10.0 * 6.27529239654541
Epoch 840, val loss: 0.9142798185348511
Epoch 850, training loss: 63.12706756591797 = 0.330002099275589 + 10.0 * 6.2797064781188965
Epoch 850, val loss: 0.9145414233207703
Epoch 860, training loss: 63.0682258605957 = 0.31849440932273865 + 10.0 * 6.274973392486572
Epoch 860, val loss: 0.9161347150802612
Epoch 870, training loss: 63.06890106201172 = 0.30752116441726685 + 10.0 * 6.276137828826904
Epoch 870, val loss: 0.9173694849014282
Epoch 880, training loss: 63.02424240112305 = 0.2968495190143585 + 10.0 * 6.272739410400391
Epoch 880, val loss: 0.9188876152038574
Epoch 890, training loss: 63.005760192871094 = 0.28664863109588623 + 10.0 * 6.271911144256592
Epoch 890, val loss: 0.9206658005714417
Epoch 900, training loss: 63.00102233886719 = 0.2767755091190338 + 10.0 * 6.272424697875977
Epoch 900, val loss: 0.9231687188148499
Epoch 910, training loss: 62.9617919921875 = 0.26730263233184814 + 10.0 * 6.269448757171631
Epoch 910, val loss: 0.9254181385040283
Epoch 920, training loss: 62.95609664916992 = 0.25815266370773315 + 10.0 * 6.269794464111328
Epoch 920, val loss: 0.9283255338668823
Epoch 930, training loss: 62.951717376708984 = 0.2493385672569275 + 10.0 * 6.270237922668457
Epoch 930, val loss: 0.9316971898078918
Epoch 940, training loss: 62.90583419799805 = 0.24084317684173584 + 10.0 * 6.266499042510986
Epoch 940, val loss: 0.9348013997077942
Epoch 950, training loss: 62.89633560180664 = 0.23272296786308289 + 10.0 * 6.266361236572266
Epoch 950, val loss: 0.9385278820991516
Epoch 960, training loss: 62.914306640625 = 0.22492961585521698 + 10.0 * 6.268937587738037
Epoch 960, val loss: 0.942338228225708
Epoch 970, training loss: 62.89527893066406 = 0.21734116971492767 + 10.0 * 6.267793655395508
Epoch 970, val loss: 0.9472754597663879
Epoch 980, training loss: 62.87337875366211 = 0.2101331204175949 + 10.0 * 6.266324520111084
Epoch 980, val loss: 0.9509667158126831
Epoch 990, training loss: 62.836280822753906 = 0.20320157706737518 + 10.0 * 6.263308048248291
Epoch 990, val loss: 0.9554479718208313
Epoch 1000, training loss: 62.815799713134766 = 0.1965693086385727 + 10.0 * 6.261923313140869
Epoch 1000, val loss: 0.9601821303367615
Epoch 1010, training loss: 62.8170051574707 = 0.1902112066745758 + 10.0 * 6.262679100036621
Epoch 1010, val loss: 0.9650055766105652
Epoch 1020, training loss: 62.81630325317383 = 0.18404386937618256 + 10.0 * 6.26322603225708
Epoch 1020, val loss: 0.9703874588012695
Epoch 1030, training loss: 62.814449310302734 = 0.17809395492076874 + 10.0 * 6.263635635375977
Epoch 1030, val loss: 0.9753651022911072
Epoch 1040, training loss: 62.767723083496094 = 0.1724022626876831 + 10.0 * 6.2595319747924805
Epoch 1040, val loss: 0.9807440638542175
Epoch 1050, training loss: 62.771820068359375 = 0.16698288917541504 + 10.0 * 6.260483741760254
Epoch 1050, val loss: 0.9860898852348328
Epoch 1060, training loss: 62.758934020996094 = 0.16173461079597473 + 10.0 * 6.2597198486328125
Epoch 1060, val loss: 0.9921483993530273
Epoch 1070, training loss: 62.734779357910156 = 0.1566934883594513 + 10.0 * 6.257808685302734
Epoch 1070, val loss: 0.9985089302062988
Epoch 1080, training loss: 62.721256256103516 = 0.15186837315559387 + 10.0 * 6.256938934326172
Epoch 1080, val loss: 1.0046765804290771
Epoch 1090, training loss: 62.765010833740234 = 0.14721836149692535 + 10.0 * 6.261779308319092
Epoch 1090, val loss: 1.011038064956665
Epoch 1100, training loss: 62.7380485534668 = 0.14275841414928436 + 10.0 * 6.259529113769531
Epoch 1100, val loss: 1.0163792371749878
Epoch 1110, training loss: 62.715267181396484 = 0.13843417167663574 + 10.0 * 6.257683277130127
Epoch 1110, val loss: 1.0231541395187378
Epoch 1120, training loss: 62.67512130737305 = 0.1343005746603012 + 10.0 * 6.254082202911377
Epoch 1120, val loss: 1.0297297239303589
Epoch 1130, training loss: 62.67323303222656 = 0.13035309314727783 + 10.0 * 6.254288196563721
Epoch 1130, val loss: 1.0364080667495728
Epoch 1140, training loss: 62.6811408996582 = 0.12653569877147675 + 10.0 * 6.255460262298584
Epoch 1140, val loss: 1.0434167385101318
Epoch 1150, training loss: 62.6666145324707 = 0.12286032736301422 + 10.0 * 6.254375457763672
Epoch 1150, val loss: 1.0497212409973145
Epoch 1160, training loss: 62.663543701171875 = 0.11929964274168015 + 10.0 * 6.254424095153809
Epoch 1160, val loss: 1.0567559003829956
Epoch 1170, training loss: 62.638336181640625 = 0.11587636917829514 + 10.0 * 6.252245903015137
Epoch 1170, val loss: 1.0632071495056152
Epoch 1180, training loss: 62.62336730957031 = 0.11261168867349625 + 10.0 * 6.251075744628906
Epoch 1180, val loss: 1.0703151226043701
Epoch 1190, training loss: 62.61288833618164 = 0.10948596894741058 + 10.0 * 6.250340461730957
Epoch 1190, val loss: 1.0769692659378052
Epoch 1200, training loss: 62.668174743652344 = 0.10648167878389359 + 10.0 * 6.256169319152832
Epoch 1200, val loss: 1.0834996700286865
Epoch 1210, training loss: 62.623966217041016 = 0.10350476205348969 + 10.0 * 6.25204610824585
Epoch 1210, val loss: 1.0914040803909302
Epoch 1220, training loss: 62.60148239135742 = 0.1006871834397316 + 10.0 * 6.250079154968262
Epoch 1220, val loss: 1.0976479053497314
Epoch 1230, training loss: 62.61384201049805 = 0.09796454012393951 + 10.0 * 6.251587867736816
Epoch 1230, val loss: 1.105031967163086
Epoch 1240, training loss: 62.57931137084961 = 0.09530922770500183 + 10.0 * 6.2484002113342285
Epoch 1240, val loss: 1.1126282215118408
Epoch 1250, training loss: 62.56770706176758 = 0.09277704358100891 + 10.0 * 6.247492790222168
Epoch 1250, val loss: 1.1196415424346924
Epoch 1260, training loss: 62.56819152832031 = 0.09033903479576111 + 10.0 * 6.2477850914001465
Epoch 1260, val loss: 1.1265740394592285
Epoch 1270, training loss: 62.56986999511719 = 0.08797343820333481 + 10.0 * 6.248189449310303
Epoch 1270, val loss: 1.1339918375015259
Epoch 1280, training loss: 62.55535888671875 = 0.08569446206092834 + 10.0 * 6.246966361999512
Epoch 1280, val loss: 1.1407573223114014
Epoch 1290, training loss: 62.558021545410156 = 0.08347491919994354 + 10.0 * 6.247454643249512
Epoch 1290, val loss: 1.1477704048156738
Epoch 1300, training loss: 62.5585823059082 = 0.08133231848478317 + 10.0 * 6.247725009918213
Epoch 1300, val loss: 1.1546595096588135
Epoch 1310, training loss: 62.53261947631836 = 0.07926437258720398 + 10.0 * 6.245335578918457
Epoch 1310, val loss: 1.1616895198822021
Epoch 1320, training loss: 62.515804290771484 = 0.07727905362844467 + 10.0 * 6.243852615356445
Epoch 1320, val loss: 1.168944239616394
Epoch 1330, training loss: 62.50816345214844 = 0.07536216080188751 + 10.0 * 6.243279933929443
Epoch 1330, val loss: 1.1761528253555298
Epoch 1340, training loss: 62.573543548583984 = 0.0735001340508461 + 10.0 * 6.250004291534424
Epoch 1340, val loss: 1.1839654445648193
Epoch 1350, training loss: 62.54553985595703 = 0.07169325649738312 + 10.0 * 6.247384548187256
Epoch 1350, val loss: 1.189649224281311
Epoch 1360, training loss: 62.49211883544922 = 0.06991560012102127 + 10.0 * 6.242220401763916
Epoch 1360, val loss: 1.1969082355499268
Epoch 1370, training loss: 62.481014251708984 = 0.06823980063199997 + 10.0 * 6.24127721786499
Epoch 1370, val loss: 1.204035997390747
Epoch 1380, training loss: 62.5008544921875 = 0.06662248820066452 + 10.0 * 6.243422985076904
Epoch 1380, val loss: 1.2107690572738647
Epoch 1390, training loss: 62.48256301879883 = 0.06501707434654236 + 10.0 * 6.241754531860352
Epoch 1390, val loss: 1.2179163694381714
Epoch 1400, training loss: 62.51192855834961 = 0.0634671002626419 + 10.0 * 6.244845867156982
Epoch 1400, val loss: 1.2250056266784668
Epoch 1410, training loss: 62.46152877807617 = 0.06197591871023178 + 10.0 * 6.239954948425293
Epoch 1410, val loss: 1.231162667274475
Epoch 1420, training loss: 62.45426559448242 = 0.060536403208971024 + 10.0 * 6.239373207092285
Epoch 1420, val loss: 1.2382910251617432
Epoch 1430, training loss: 62.44849395751953 = 0.0591493584215641 + 10.0 * 6.238934516906738
Epoch 1430, val loss: 1.2451540231704712
Epoch 1440, training loss: 62.451812744140625 = 0.05780940130352974 + 10.0 * 6.239400386810303
Epoch 1440, val loss: 1.2515109777450562
Epoch 1450, training loss: 62.477962493896484 = 0.05649830773472786 + 10.0 * 6.2421464920043945
Epoch 1450, val loss: 1.2578985691070557
Epoch 1460, training loss: 62.43640899658203 = 0.055200062692165375 + 10.0 * 6.238121032714844
Epoch 1460, val loss: 1.2647640705108643
Epoch 1470, training loss: 62.46517562866211 = 0.053963854908943176 + 10.0 * 6.241121292114258
Epoch 1470, val loss: 1.2720379829406738
Epoch 1480, training loss: 62.44886779785156 = 0.052755456417798996 + 10.0 * 6.2396111488342285
Epoch 1480, val loss: 1.2785693407058716
Epoch 1490, training loss: 62.414588928222656 = 0.05158945173025131 + 10.0 * 6.236299991607666
Epoch 1490, val loss: 1.2852181196212769
Epoch 1500, training loss: 62.41393280029297 = 0.050463393330574036 + 10.0 * 6.23634672164917
Epoch 1500, val loss: 1.2920564413070679
Epoch 1510, training loss: 62.41862106323242 = 0.04937014356255531 + 10.0 * 6.23692512512207
Epoch 1510, val loss: 1.2985835075378418
Epoch 1520, training loss: 62.465301513671875 = 0.04830361530184746 + 10.0 * 6.241699695587158
Epoch 1520, val loss: 1.3050411939620972
Epoch 1530, training loss: 62.440032958984375 = 0.047259412705898285 + 10.0 * 6.239277362823486
Epoch 1530, val loss: 1.3116202354431152
Epoch 1540, training loss: 62.410831451416016 = 0.04623698070645332 + 10.0 * 6.236459255218506
Epoch 1540, val loss: 1.3182038068771362
Epoch 1550, training loss: 62.47477340698242 = 0.04525446519255638 + 10.0 * 6.2429518699646
Epoch 1550, val loss: 1.325145959854126
Epoch 1560, training loss: 62.40720748901367 = 0.04430544003844261 + 10.0 * 6.236290454864502
Epoch 1560, val loss: 1.3303523063659668
Epoch 1570, training loss: 62.38435745239258 = 0.0433754101395607 + 10.0 * 6.234097957611084
Epoch 1570, val loss: 1.3371716737747192
Epoch 1580, training loss: 62.37617874145508 = 0.04249177500605583 + 10.0 * 6.233368873596191
Epoch 1580, val loss: 1.343205451965332
Epoch 1590, training loss: 62.38444900512695 = 0.04163030534982681 + 10.0 * 6.23428201675415
Epoch 1590, val loss: 1.3494559526443481
Epoch 1600, training loss: 62.40299606323242 = 0.04078119248151779 + 10.0 * 6.2362213134765625
Epoch 1600, val loss: 1.3554813861846924
Epoch 1610, training loss: 62.40574645996094 = 0.03993179276585579 + 10.0 * 6.236581325531006
Epoch 1610, val loss: 1.3622043132781982
Epoch 1620, training loss: 62.360042572021484 = 0.039124615490436554 + 10.0 * 6.232091903686523
Epoch 1620, val loss: 1.3677765130996704
Epoch 1630, training loss: 62.359107971191406 = 0.03834505379199982 + 10.0 * 6.232076168060303
Epoch 1630, val loss: 1.374205231666565
Epoch 1640, training loss: 62.35307312011719 = 0.03758890554308891 + 10.0 * 6.231548309326172
Epoch 1640, val loss: 1.3801875114440918
Epoch 1650, training loss: 62.387107849121094 = 0.03685540333390236 + 10.0 * 6.235025405883789
Epoch 1650, val loss: 1.386523723602295
Epoch 1660, training loss: 62.360809326171875 = 0.0361308716237545 + 10.0 * 6.232468128204346
Epoch 1660, val loss: 1.391972541809082
Epoch 1670, training loss: 62.3653678894043 = 0.035420116037130356 + 10.0 * 6.23299503326416
Epoch 1670, val loss: 1.3984814882278442
Epoch 1680, training loss: 62.380516052246094 = 0.03473765030503273 + 10.0 * 6.2345781326293945
Epoch 1680, val loss: 1.4041407108306885
Epoch 1690, training loss: 62.33937454223633 = 0.034070659428834915 + 10.0 * 6.230530738830566
Epoch 1690, val loss: 1.4093313217163086
Epoch 1700, training loss: 62.33452224731445 = 0.03342914208769798 + 10.0 * 6.230109214782715
Epoch 1700, val loss: 1.4153671264648438
Epoch 1710, training loss: 62.4168815612793 = 0.032801613211631775 + 10.0 * 6.238408088684082
Epoch 1710, val loss: 1.4214200973510742
Epoch 1720, training loss: 62.3502082824707 = 0.03217131644487381 + 10.0 * 6.2318034172058105
Epoch 1720, val loss: 1.4267354011535645
Epoch 1730, training loss: 62.32815933227539 = 0.03157313913106918 + 10.0 * 6.229658603668213
Epoch 1730, val loss: 1.4325450658798218
Epoch 1740, training loss: 62.32072448730469 = 0.030994366854429245 + 10.0 * 6.228972911834717
Epoch 1740, val loss: 1.4382485151290894
Epoch 1750, training loss: 62.38794708251953 = 0.030428098514676094 + 10.0 * 6.235751628875732
Epoch 1750, val loss: 1.4443625211715698
Epoch 1760, training loss: 62.3481559753418 = 0.029870476573705673 + 10.0 * 6.231828689575195
Epoch 1760, val loss: 1.449129581451416
Epoch 1770, training loss: 62.324134826660156 = 0.029322467744350433 + 10.0 * 6.229481220245361
Epoch 1770, val loss: 1.4550050497055054
Epoch 1780, training loss: 62.304481506347656 = 0.028805019333958626 + 10.0 * 6.227567672729492
Epoch 1780, val loss: 1.4599231481552124
Epoch 1790, training loss: 62.30585479736328 = 0.028298791497945786 + 10.0 * 6.227755546569824
Epoch 1790, val loss: 1.46541428565979
Epoch 1800, training loss: 62.34912109375 = 0.027808669954538345 + 10.0 * 6.232131004333496
Epoch 1800, val loss: 1.470516562461853
Epoch 1810, training loss: 62.31193923950195 = 0.027303557842969894 + 10.0 * 6.228463649749756
Epoch 1810, val loss: 1.4766544103622437
Epoch 1820, training loss: 62.30983352661133 = 0.026826990768313408 + 10.0 * 6.22830057144165
Epoch 1820, val loss: 1.4818447828292847
Epoch 1830, training loss: 62.31635665893555 = 0.02636120468378067 + 10.0 * 6.228999614715576
Epoch 1830, val loss: 1.4869624376296997
Epoch 1840, training loss: 62.299530029296875 = 0.02591167576611042 + 10.0 * 6.227361679077148
Epoch 1840, val loss: 1.491650104522705
Epoch 1850, training loss: 62.2856330871582 = 0.02546733245253563 + 10.0 * 6.226016521453857
Epoch 1850, val loss: 1.4972357749938965
Epoch 1860, training loss: 62.29387664794922 = 0.02504141442477703 + 10.0 * 6.226883888244629
Epoch 1860, val loss: 1.5026068687438965
Epoch 1870, training loss: 62.29207992553711 = 0.024622485041618347 + 10.0 * 6.22674560546875
Epoch 1870, val loss: 1.5071778297424316
Epoch 1880, training loss: 62.311119079589844 = 0.024217452853918076 + 10.0 * 6.228690147399902
Epoch 1880, val loss: 1.5119930505752563
Epoch 1890, training loss: 62.312103271484375 = 0.02380402572453022 + 10.0 * 6.228829860687256
Epoch 1890, val loss: 1.517545223236084
Epoch 1900, training loss: 62.278526306152344 = 0.02340923249721527 + 10.0 * 6.22551155090332
Epoch 1900, val loss: 1.5220322608947754
Epoch 1910, training loss: 62.265769958496094 = 0.023027224466204643 + 10.0 * 6.224274635314941
Epoch 1910, val loss: 1.5269876718521118
Epoch 1920, training loss: 62.264705657958984 = 0.022659962996840477 + 10.0 * 6.2242045402526855
Epoch 1920, val loss: 1.5321563482284546
Epoch 1930, training loss: 62.334373474121094 = 0.022300705313682556 + 10.0 * 6.231207370758057
Epoch 1930, val loss: 1.53678560256958
Epoch 1940, training loss: 62.27724838256836 = 0.02193893864750862 + 10.0 * 6.225531101226807
Epoch 1940, val loss: 1.5415960550308228
Epoch 1950, training loss: 62.262691497802734 = 0.021589813753962517 + 10.0 * 6.224110126495361
Epoch 1950, val loss: 1.5464268922805786
Epoch 1960, training loss: 62.2717170715332 = 0.02125445194542408 + 10.0 * 6.225046157836914
Epoch 1960, val loss: 1.5512619018554688
Epoch 1970, training loss: 62.259925842285156 = 0.020918473601341248 + 10.0 * 6.22390079498291
Epoch 1970, val loss: 1.5559887886047363
Epoch 1980, training loss: 62.334228515625 = 0.02059771865606308 + 10.0 * 6.231362819671631
Epoch 1980, val loss: 1.560338020324707
Epoch 1990, training loss: 62.260528564453125 = 0.020263636484742165 + 10.0 * 6.224026679992676
Epoch 1990, val loss: 1.566053032875061
Epoch 2000, training loss: 62.24216079711914 = 0.019955221563577652 + 10.0 * 6.222220420837402
Epoch 2000, val loss: 1.5699729919433594
Epoch 2010, training loss: 62.23680877685547 = 0.01965411752462387 + 10.0 * 6.221715450286865
Epoch 2010, val loss: 1.5753215551376343
Epoch 2020, training loss: 62.2932243347168 = 0.019362807273864746 + 10.0 * 6.227385997772217
Epoch 2020, val loss: 1.5801148414611816
Epoch 2030, training loss: 62.243125915527344 = 0.01906774193048477 + 10.0 * 6.222405910491943
Epoch 2030, val loss: 1.5832862854003906
Epoch 2040, training loss: 62.24146270751953 = 0.01878516934812069 + 10.0 * 6.222268104553223
Epoch 2040, val loss: 1.58832585811615
Epoch 2050, training loss: 62.26704406738281 = 0.018512489274144173 + 10.0 * 6.224853038787842
Epoch 2050, val loss: 1.5922819375991821
Epoch 2060, training loss: 62.259803771972656 = 0.0182341318577528 + 10.0 * 6.224156856536865
Epoch 2060, val loss: 1.5970863103866577
Epoch 2070, training loss: 62.23109817504883 = 0.017965838313102722 + 10.0 * 6.221312999725342
Epoch 2070, val loss: 1.6012651920318604
Epoch 2080, training loss: 62.22233581542969 = 0.01770627312362194 + 10.0 * 6.220462799072266
Epoch 2080, val loss: 1.6057153940200806
Epoch 2090, training loss: 62.293731689453125 = 0.01745731011033058 + 10.0 * 6.227627754211426
Epoch 2090, val loss: 1.6097930669784546
Epoch 2100, training loss: 62.24156951904297 = 0.01720111072063446 + 10.0 * 6.222436904907227
Epoch 2100, val loss: 1.6145633459091187
Epoch 2110, training loss: 62.2175178527832 = 0.0169542096555233 + 10.0 * 6.220056533813477
Epoch 2110, val loss: 1.6185219287872314
Epoch 2120, training loss: 62.20622253417969 = 0.016718771308660507 + 10.0 * 6.218950271606445
Epoch 2120, val loss: 1.622969388961792
Epoch 2130, training loss: 62.22046661376953 = 0.016494298353791237 + 10.0 * 6.220396995544434
Epoch 2130, val loss: 1.6266366243362427
Epoch 2140, training loss: 62.26392364501953 = 0.016265520825982094 + 10.0 * 6.224765777587891
Epoch 2140, val loss: 1.631010890007019
Epoch 2150, training loss: 62.2028923034668 = 0.016022689640522003 + 10.0 * 6.218687057495117
Epoch 2150, val loss: 1.6349725723266602
Epoch 2160, training loss: 62.202308654785156 = 0.015803515911102295 + 10.0 * 6.2186503410339355
Epoch 2160, val loss: 1.6396124362945557
Epoch 2170, training loss: 62.2226676940918 = 0.01559428870677948 + 10.0 * 6.220707416534424
Epoch 2170, val loss: 1.6431971788406372
Epoch 2180, training loss: 62.20384216308594 = 0.015382844023406506 + 10.0 * 6.218845844268799
Epoch 2180, val loss: 1.6475067138671875
Epoch 2190, training loss: 62.21046829223633 = 0.015180599875748158 + 10.0 * 6.219528675079346
Epoch 2190, val loss: 1.6511999368667603
Epoch 2200, training loss: 62.19175720214844 = 0.014977743849158287 + 10.0 * 6.217678070068359
Epoch 2200, val loss: 1.6551425457000732
Epoch 2210, training loss: 62.217159271240234 = 0.014786588959395885 + 10.0 * 6.2202372550964355
Epoch 2210, val loss: 1.6587631702423096
Epoch 2220, training loss: 62.19636535644531 = 0.014588120393455029 + 10.0 * 6.218177795410156
Epoch 2220, val loss: 1.662758469581604
Epoch 2230, training loss: 62.206783294677734 = 0.014397541992366314 + 10.0 * 6.219238758087158
Epoch 2230, val loss: 1.6664923429489136
Epoch 2240, training loss: 62.19720458984375 = 0.014207713305950165 + 10.0 * 6.218299865722656
Epoch 2240, val loss: 1.6702141761779785
Epoch 2250, training loss: 62.21059799194336 = 0.014024435542523861 + 10.0 * 6.2196574211120605
Epoch 2250, val loss: 1.674025535583496
Epoch 2260, training loss: 62.185707092285156 = 0.01384438294917345 + 10.0 * 6.217186450958252
Epoch 2260, val loss: 1.6777515411376953
Epoch 2270, training loss: 62.1860466003418 = 0.013673082925379276 + 10.0 * 6.21723747253418
Epoch 2270, val loss: 1.6813536882400513
Epoch 2280, training loss: 62.20664978027344 = 0.013498106971383095 + 10.0 * 6.219315528869629
Epoch 2280, val loss: 1.6854068040847778
Epoch 2290, training loss: 62.17741394042969 = 0.01332792267203331 + 10.0 * 6.216408729553223
Epoch 2290, val loss: 1.6891999244689941
Epoch 2300, training loss: 62.180908203125 = 0.013164570555090904 + 10.0 * 6.2167744636535645
Epoch 2300, val loss: 1.6927160024642944
Epoch 2310, training loss: 62.190032958984375 = 0.0130024254322052 + 10.0 * 6.217702865600586
Epoch 2310, val loss: 1.6962027549743652
Epoch 2320, training loss: 62.17460632324219 = 0.012841618619859219 + 10.0 * 6.216176509857178
Epoch 2320, val loss: 1.6998779773712158
Epoch 2330, training loss: 62.25251770019531 = 0.012684611603617668 + 10.0 * 6.223983287811279
Epoch 2330, val loss: 1.704033613204956
Epoch 2340, training loss: 62.19680404663086 = 0.012523905374109745 + 10.0 * 6.218428134918213
Epoch 2340, val loss: 1.7066960334777832
Epoch 2350, training loss: 62.16374206542969 = 0.012372343800961971 + 10.0 * 6.215137004852295
Epoch 2350, val loss: 1.7102867364883423
Epoch 2360, training loss: 62.15304183959961 = 0.01222744770348072 + 10.0 * 6.214081764221191
Epoch 2360, val loss: 1.7136310338974
Epoch 2370, training loss: 62.15247344970703 = 0.01208633091300726 + 10.0 * 6.214038848876953
Epoch 2370, val loss: 1.7169302701950073
Epoch 2380, training loss: 62.284759521484375 = 0.011956848204135895 + 10.0 * 6.227280616760254
Epoch 2380, val loss: 1.7194322347640991
Epoch 2390, training loss: 62.203392028808594 = 0.011795728467404842 + 10.0 * 6.2191596031188965
Epoch 2390, val loss: 1.72439444065094
Epoch 2400, training loss: 62.1575927734375 = 0.011658964678645134 + 10.0 * 6.214593410491943
Epoch 2400, val loss: 1.7265808582305908
Epoch 2410, training loss: 62.143333435058594 = 0.011523967608809471 + 10.0 * 6.213181018829346
Epoch 2410, val loss: 1.7304428815841675
Epoch 2420, training loss: 62.17666244506836 = 0.01139477826654911 + 10.0 * 6.216526985168457
Epoch 2420, val loss: 1.7342854738235474
Epoch 2430, training loss: 62.15715789794922 = 0.011262208223342896 + 10.0 * 6.214589595794678
Epoch 2430, val loss: 1.736734390258789
Epoch 2440, training loss: 62.13909912109375 = 0.011132113635540009 + 10.0 * 6.212796688079834
Epoch 2440, val loss: 1.7402253150939941
Epoch 2450, training loss: 62.134361267089844 = 0.011007561348378658 + 10.0 * 6.212335109710693
Epoch 2450, val loss: 1.743011474609375
Epoch 2460, training loss: 62.138492584228516 = 0.010888716205954552 + 10.0 * 6.2127604484558105
Epoch 2460, val loss: 1.7463395595550537
Epoch 2470, training loss: 62.20346450805664 = 0.010772192850708961 + 10.0 * 6.219269275665283
Epoch 2470, val loss: 1.7492773532867432
Epoch 2480, training loss: 62.197669982910156 = 0.01064536813646555 + 10.0 * 6.21870231628418
Epoch 2480, val loss: 1.7532416582107544
Epoch 2490, training loss: 62.14751434326172 = 0.010526252910494804 + 10.0 * 6.213698863983154
Epoch 2490, val loss: 1.7553699016571045
Epoch 2500, training loss: 62.132869720458984 = 0.010410959832370281 + 10.0 * 6.212245941162109
Epoch 2500, val loss: 1.7588926553726196
Epoch 2510, training loss: 62.12905502319336 = 0.01030210591852665 + 10.0 * 6.211874961853027
Epoch 2510, val loss: 1.7616970539093018
Epoch 2520, training loss: 62.15413284301758 = 0.010195033624768257 + 10.0 * 6.214393615722656
Epoch 2520, val loss: 1.7646757364273071
Epoch 2530, training loss: 62.15861892700195 = 0.010082150809466839 + 10.0 * 6.214853763580322
Epoch 2530, val loss: 1.767993450164795
Epoch 2540, training loss: 62.14533996582031 = 0.009972608648240566 + 10.0 * 6.213536739349365
Epoch 2540, val loss: 1.7708885669708252
Epoch 2550, training loss: 62.12382507324219 = 0.009866058826446533 + 10.0 * 6.211396217346191
Epoch 2550, val loss: 1.7734777927398682
Epoch 2560, training loss: 62.11943817138672 = 0.009762068279087543 + 10.0 * 6.210967540740967
Epoch 2560, val loss: 1.7767131328582764
Epoch 2570, training loss: 62.12776565551758 = 0.009664050303399563 + 10.0 * 6.211810111999512
Epoch 2570, val loss: 1.7795945405960083
Epoch 2580, training loss: 62.165687561035156 = 0.00956510379910469 + 10.0 * 6.215612411499023
Epoch 2580, val loss: 1.7825305461883545
Epoch 2590, training loss: 62.134727478027344 = 0.009463696740567684 + 10.0 * 6.212526321411133
Epoch 2590, val loss: 1.7856196165084839
Epoch 2600, training loss: 62.14490509033203 = 0.009366905316710472 + 10.0 * 6.2135539054870605
Epoch 2600, val loss: 1.7879676818847656
Epoch 2610, training loss: 62.12661361694336 = 0.009269589558243752 + 10.0 * 6.211734294891357
Epoch 2610, val loss: 1.790753722190857
Epoch 2620, training loss: 62.12851333618164 = 0.009174052625894547 + 10.0 * 6.2119340896606445
Epoch 2620, val loss: 1.7940526008605957
Epoch 2630, training loss: 62.177955627441406 = 0.009080047719180584 + 10.0 * 6.216887474060059
Epoch 2630, val loss: 1.7969846725463867
Epoch 2640, training loss: 62.12578201293945 = 0.008991686627268791 + 10.0 * 6.211678981781006
Epoch 2640, val loss: 1.7984284162521362
Epoch 2650, training loss: 62.107933044433594 = 0.008900155313313007 + 10.0 * 6.209903240203857
Epoch 2650, val loss: 1.8018312454223633
Epoch 2660, training loss: 62.10708236694336 = 0.008815016597509384 + 10.0 * 6.209826469421387
Epoch 2660, val loss: 1.8047360181808472
Epoch 2670, training loss: 62.176109313964844 = 0.008732040412724018 + 10.0 * 6.216737747192383
Epoch 2670, val loss: 1.807076096534729
Epoch 2680, training loss: 62.12879943847656 = 0.008643782697618008 + 10.0 * 6.212015628814697
Epoch 2680, val loss: 1.8098517656326294
Epoch 2690, training loss: 62.1066780090332 = 0.008558651432394981 + 10.0 * 6.209811687469482
Epoch 2690, val loss: 1.8119717836380005
Epoch 2700, training loss: 62.101722717285156 = 0.008476674556732178 + 10.0 * 6.209324836730957
Epoch 2700, val loss: 1.8153516054153442
Epoch 2710, training loss: 62.154605865478516 = 0.008398459292948246 + 10.0 * 6.214620590209961
Epoch 2710, val loss: 1.8177660703659058
Epoch 2720, training loss: 62.14058303833008 = 0.008316042833030224 + 10.0 * 6.213226795196533
Epoch 2720, val loss: 1.8206908702850342
Epoch 2730, training loss: 62.132896423339844 = 0.008233361877501011 + 10.0 * 6.212466239929199
Epoch 2730, val loss: 1.8229299783706665
Epoch 2740, training loss: 62.10248565673828 = 0.008156328462064266 + 10.0 * 6.209433078765869
Epoch 2740, val loss: 1.8251559734344482
Epoch 2750, training loss: 62.090023040771484 = 0.008081793785095215 + 10.0 * 6.208193778991699
Epoch 2750, val loss: 1.8278329372406006
Epoch 2760, training loss: 62.091033935546875 = 0.00800925400108099 + 10.0 * 6.2083024978637695
Epoch 2760, val loss: 1.8305373191833496
Epoch 2770, training loss: 62.12891387939453 = 0.007940061390399933 + 10.0 * 6.212097644805908
Epoch 2770, val loss: 1.8325620889663696
Epoch 2780, training loss: 62.119266510009766 = 0.007861308753490448 + 10.0 * 6.2111406326293945
Epoch 2780, val loss: 1.8349792957305908
Epoch 2790, training loss: 62.08897399902344 = 0.0077848066575825214 + 10.0 * 6.208118915557861
Epoch 2790, val loss: 1.8375190496444702
Epoch 2800, training loss: 62.08109664916992 = 0.007715144194662571 + 10.0 * 6.207338333129883
Epoch 2800, val loss: 1.8400437831878662
Epoch 2810, training loss: 62.0772705078125 = 0.007647715508937836 + 10.0 * 6.2069621086120605
Epoch 2810, val loss: 1.842641830444336
Epoch 2820, training loss: 62.10425567626953 = 0.007581935729831457 + 10.0 * 6.209667205810547
Epoch 2820, val loss: 1.8454217910766602
Epoch 2830, training loss: 62.0967903137207 = 0.007511895149946213 + 10.0 * 6.208928108215332
Epoch 2830, val loss: 1.8474568128585815
Epoch 2840, training loss: 62.08511734008789 = 0.007443743757903576 + 10.0 * 6.207767486572266
Epoch 2840, val loss: 1.8498889207839966
Epoch 2850, training loss: 62.084381103515625 = 0.007378469221293926 + 10.0 * 6.207700252532959
Epoch 2850, val loss: 1.8521995544433594
Epoch 2860, training loss: 62.09465789794922 = 0.007315508555620909 + 10.0 * 6.208734035491943
Epoch 2860, val loss: 1.8552144765853882
Epoch 2870, training loss: 62.13726043701172 = 0.007252814248204231 + 10.0 * 6.213000774383545
Epoch 2870, val loss: 1.8564045429229736
Epoch 2880, training loss: 62.085018157958984 = 0.007189795840531588 + 10.0 * 6.207782745361328
Epoch 2880, val loss: 1.8583881855010986
Epoch 2890, training loss: 62.073570251464844 = 0.007125760428607464 + 10.0 * 6.206644535064697
Epoch 2890, val loss: 1.861380934715271
Epoch 2900, training loss: 62.068206787109375 = 0.007067065220326185 + 10.0 * 6.206113815307617
Epoch 2900, val loss: 1.8631300926208496
Epoch 2910, training loss: 62.07769775390625 = 0.007009529042989016 + 10.0 * 6.20706844329834
Epoch 2910, val loss: 1.8655180931091309
Epoch 2920, training loss: 62.12635040283203 = 0.006949025671929121 + 10.0 * 6.211939811706543
Epoch 2920, val loss: 1.8679994344711304
Epoch 2930, training loss: 62.10896301269531 = 0.00689201895147562 + 10.0 * 6.210206985473633
Epoch 2930, val loss: 1.8690847158432007
Epoch 2940, training loss: 62.077823638916016 = 0.006830912083387375 + 10.0 * 6.207098960876465
Epoch 2940, val loss: 1.8716707229614258
Epoch 2950, training loss: 62.067073822021484 = 0.006774604320526123 + 10.0 * 6.206029891967773
Epoch 2950, val loss: 1.8744466304779053
Epoch 2960, training loss: 62.08460998535156 = 0.006720676552504301 + 10.0 * 6.207788944244385
Epoch 2960, val loss: 1.876562237739563
Epoch 2970, training loss: 62.1271858215332 = 0.006668237037956715 + 10.0 * 6.212051868438721
Epoch 2970, val loss: 1.8780267238616943
Epoch 2980, training loss: 62.06928634643555 = 0.006606526207178831 + 10.0 * 6.206267833709717
Epoch 2980, val loss: 1.8808047771453857
Epoch 2990, training loss: 62.05672073364258 = 0.006554018706083298 + 10.0 * 6.205016613006592
Epoch 2990, val loss: 1.8824505805969238
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 87.9235610961914 = 1.9551739692687988 + 10.0 * 8.596837997436523
Epoch 0, val loss: 1.9644588232040405
Epoch 10, training loss: 87.90889739990234 = 1.9460361003875732 + 10.0 * 8.596285820007324
Epoch 10, val loss: 1.9550964832305908
Epoch 20, training loss: 87.85533142089844 = 1.9344207048416138 + 10.0 * 8.59209156036377
Epoch 20, val loss: 1.9428834915161133
Epoch 30, training loss: 87.54618072509766 = 1.918670654296875 + 10.0 * 8.562750816345215
Epoch 30, val loss: 1.9261484146118164
Epoch 40, training loss: 85.94303894042969 = 1.898808479309082 + 10.0 * 8.404422760009766
Epoch 40, val loss: 1.905696153640747
Epoch 50, training loss: 81.92781829833984 = 1.8752413988113403 + 10.0 * 8.005257606506348
Epoch 50, val loss: 1.8815449476242065
Epoch 60, training loss: 79.23213958740234 = 1.8540929555892944 + 10.0 * 7.737804412841797
Epoch 60, val loss: 1.8613380193710327
Epoch 70, training loss: 75.69223022460938 = 1.8412115573883057 + 10.0 * 7.385101795196533
Epoch 70, val loss: 1.8492120504379272
Epoch 80, training loss: 73.7691879272461 = 1.8323639631271362 + 10.0 * 7.1936821937561035
Epoch 80, val loss: 1.839310884475708
Epoch 90, training loss: 72.11009216308594 = 1.8203157186508179 + 10.0 * 7.028977870941162
Epoch 90, val loss: 1.8270692825317383
Epoch 100, training loss: 70.91572570800781 = 1.8112150430679321 + 10.0 * 6.910450458526611
Epoch 100, val loss: 1.817451000213623
Epoch 110, training loss: 70.00263214111328 = 1.8025732040405273 + 10.0 * 6.820005893707275
Epoch 110, val loss: 1.8079428672790527
Epoch 120, training loss: 69.31159210205078 = 1.7943965196609497 + 10.0 * 6.7517194747924805
Epoch 120, val loss: 1.798655390739441
Epoch 130, training loss: 68.7051773071289 = 1.7870169878005981 + 10.0 * 6.6918158531188965
Epoch 130, val loss: 1.7901273965835571
Epoch 140, training loss: 68.2176513671875 = 1.7797691822052002 + 10.0 * 6.6437883377075195
Epoch 140, val loss: 1.782280683517456
Epoch 150, training loss: 67.89644622802734 = 1.7716999053955078 + 10.0 * 6.6124749183654785
Epoch 150, val loss: 1.7740602493286133
Epoch 160, training loss: 67.61456298828125 = 1.762701153755188 + 10.0 * 6.585186004638672
Epoch 160, val loss: 1.7652620077133179
Epoch 170, training loss: 67.37849426269531 = 1.7531614303588867 + 10.0 * 6.562533855438232
Epoch 170, val loss: 1.7561818361282349
Epoch 180, training loss: 67.17007446289062 = 1.7432010173797607 + 10.0 * 6.54268741607666
Epoch 180, val loss: 1.7468243837356567
Epoch 190, training loss: 67.00131225585938 = 1.732483148574829 + 10.0 * 6.526882648468018
Epoch 190, val loss: 1.7368687391281128
Epoch 200, training loss: 66.84358215332031 = 1.720740556716919 + 10.0 * 6.512284278869629
Epoch 200, val loss: 1.7262314558029175
Epoch 210, training loss: 66.6834487915039 = 1.708142638206482 + 10.0 * 6.497530460357666
Epoch 210, val loss: 1.7148488759994507
Epoch 220, training loss: 66.54304504394531 = 1.6943941116333008 + 10.0 * 6.484864711761475
Epoch 220, val loss: 1.7026671171188354
Epoch 230, training loss: 66.41483306884766 = 1.6794739961624146 + 10.0 * 6.473536014556885
Epoch 230, val loss: 1.6894484758377075
Epoch 240, training loss: 66.28870391845703 = 1.6633844375610352 + 10.0 * 6.462532043457031
Epoch 240, val loss: 1.6753191947937012
Epoch 250, training loss: 66.15765380859375 = 1.646072506904602 + 10.0 * 6.451158046722412
Epoch 250, val loss: 1.6601448059082031
Epoch 260, training loss: 66.09143829345703 = 1.6275259256362915 + 10.0 * 6.446391582489014
Epoch 260, val loss: 1.643965482711792
Epoch 270, training loss: 65.93603515625 = 1.6076606512069702 + 10.0 * 6.432837963104248
Epoch 270, val loss: 1.6267822980880737
Epoch 280, training loss: 65.83784484863281 = 1.5869061946868896 + 10.0 * 6.425093650817871
Epoch 280, val loss: 1.6090223789215088
Epoch 290, training loss: 65.72447967529297 = 1.5651603937149048 + 10.0 * 6.415931701660156
Epoch 290, val loss: 1.5906188488006592
Epoch 300, training loss: 65.64945220947266 = 1.5424925088882446 + 10.0 * 6.410695552825928
Epoch 300, val loss: 1.5715296268463135
Epoch 310, training loss: 65.54827880859375 = 1.51883065700531 + 10.0 * 6.402945041656494
Epoch 310, val loss: 1.5518046617507935
Epoch 320, training loss: 65.47006225585938 = 1.4945935010910034 + 10.0 * 6.397547245025635
Epoch 320, val loss: 1.5318634510040283
Epoch 330, training loss: 65.36172485351562 = 1.4699279069900513 + 10.0 * 6.389179706573486
Epoch 330, val loss: 1.511802077293396
Epoch 340, training loss: 65.2815170288086 = 1.4449585676193237 + 10.0 * 6.383656024932861
Epoch 340, val loss: 1.4918214082717896
Epoch 350, training loss: 65.2953872680664 = 1.4198352098464966 + 10.0 * 6.387555122375488
Epoch 350, val loss: 1.4719457626342773
Epoch 360, training loss: 65.14920806884766 = 1.3944199085235596 + 10.0 * 6.375478744506836
Epoch 360, val loss: 1.4521167278289795
Epoch 370, training loss: 65.06743621826172 = 1.3690781593322754 + 10.0 * 6.36983585357666
Epoch 370, val loss: 1.432559847831726
Epoch 380, training loss: 65.02468872070312 = 1.3437730073928833 + 10.0 * 6.368091583251953
Epoch 380, val loss: 1.4133437871932983
Epoch 390, training loss: 64.96344757080078 = 1.318544864654541 + 10.0 * 6.364490032196045
Epoch 390, val loss: 1.3943530321121216
Epoch 400, training loss: 64.85797882080078 = 1.293341040611267 + 10.0 * 6.356463432312012
Epoch 400, val loss: 1.3756431341171265
Epoch 410, training loss: 64.80217742919922 = 1.2682421207427979 + 10.0 * 6.353393077850342
Epoch 410, val loss: 1.3571281433105469
Epoch 420, training loss: 64.75513458251953 = 1.2431941032409668 + 10.0 * 6.351193904876709
Epoch 420, val loss: 1.3386719226837158
Epoch 430, training loss: 64.724853515625 = 1.2180044651031494 + 10.0 * 6.350685119628906
Epoch 430, val loss: 1.3203392028808594
Epoch 440, training loss: 64.6353759765625 = 1.193167805671692 + 10.0 * 6.3442206382751465
Epoch 440, val loss: 1.3022115230560303
Epoch 450, training loss: 64.564453125 = 1.1683049201965332 + 10.0 * 6.3396148681640625
Epoch 450, val loss: 1.2844069004058838
Epoch 460, training loss: 64.51087951660156 = 1.1436747312545776 + 10.0 * 6.3367204666137695
Epoch 460, val loss: 1.266784429550171
Epoch 470, training loss: 64.48853302001953 = 1.1191189289093018 + 10.0 * 6.336941242218018
Epoch 470, val loss: 1.249314308166504
Epoch 480, training loss: 64.46884155273438 = 1.0945090055465698 + 10.0 * 6.337433815002441
Epoch 480, val loss: 1.2317428588867188
Epoch 490, training loss: 64.37100219726562 = 1.070515751838684 + 10.0 * 6.330048561096191
Epoch 490, val loss: 1.2148596048355103
Epoch 500, training loss: 64.30622863769531 = 1.04695725440979 + 10.0 * 6.325927257537842
Epoch 500, val loss: 1.1986216306686401
Epoch 510, training loss: 64.2588882446289 = 1.0238032341003418 + 10.0 * 6.323508262634277
Epoch 510, val loss: 1.1827393770217896
Epoch 520, training loss: 64.21691131591797 = 1.0009229183197021 + 10.0 * 6.321599006652832
Epoch 520, val loss: 1.1675996780395508
Epoch 530, training loss: 64.1668930053711 = 0.9786449074745178 + 10.0 * 6.3188252449035645
Epoch 530, val loss: 1.1525832414627075
Epoch 540, training loss: 64.1218032836914 = 0.9570361971855164 + 10.0 * 6.316476821899414
Epoch 540, val loss: 1.1385464668273926
Epoch 550, training loss: 64.08061981201172 = 0.9359886646270752 + 10.0 * 6.314463138580322
Epoch 550, val loss: 1.125180959701538
Epoch 560, training loss: 64.1838607788086 = 0.9153333902359009 + 10.0 * 6.326852798461914
Epoch 560, val loss: 1.1125606298446655
Epoch 570, training loss: 64.03506469726562 = 0.8952357172966003 + 10.0 * 6.313982963562012
Epoch 570, val loss: 1.1001465320587158
Epoch 580, training loss: 63.972537994384766 = 0.8757086992263794 + 10.0 * 6.309682846069336
Epoch 580, val loss: 1.0888549089431763
Epoch 590, training loss: 63.93272018432617 = 0.8567376732826233 + 10.0 * 6.307598114013672
Epoch 590, val loss: 1.0781663656234741
Epoch 600, training loss: 63.927677154541016 = 0.8381476402282715 + 10.0 * 6.308953285217285
Epoch 600, val loss: 1.0680776834487915
Epoch 610, training loss: 63.855106353759766 = 0.8200036287307739 + 10.0 * 6.3035101890563965
Epoch 610, val loss: 1.0584146976470947
Epoch 620, training loss: 63.8209228515625 = 0.8023061752319336 + 10.0 * 6.301861763000488
Epoch 620, val loss: 1.0496466159820557
Epoch 630, training loss: 63.78596496582031 = 0.7849631905555725 + 10.0 * 6.300100326538086
Epoch 630, val loss: 1.041229009628296
Epoch 640, training loss: 63.83401107788086 = 0.7680100202560425 + 10.0 * 6.306600093841553
Epoch 640, val loss: 1.0337321758270264
Epoch 650, training loss: 63.75590896606445 = 0.7511295080184937 + 10.0 * 6.300477981567383
Epoch 650, val loss: 1.0259549617767334
Epoch 660, training loss: 63.737762451171875 = 0.734741747379303 + 10.0 * 6.300302028656006
Epoch 660, val loss: 1.019446849822998
Epoch 670, training loss: 63.66983413696289 = 0.7185956239700317 + 10.0 * 6.29512357711792
Epoch 670, val loss: 1.0130798816680908
Epoch 680, training loss: 63.63395309448242 = 0.7028374671936035 + 10.0 * 6.293111324310303
Epoch 680, val loss: 1.007434606552124
Epoch 690, training loss: 63.60155487060547 = 0.687284529209137 + 10.0 * 6.291426658630371
Epoch 690, val loss: 1.002242088317871
Epoch 700, training loss: 63.65705490112305 = 0.6719456315040588 + 10.0 * 6.298510551452637
Epoch 700, val loss: 0.9971271753311157
Epoch 710, training loss: 63.59922409057617 = 0.6566579341888428 + 10.0 * 6.294256687164307
Epoch 710, val loss: 0.9932093024253845
Epoch 720, training loss: 63.52329635620117 = 0.6416756510734558 + 10.0 * 6.2881622314453125
Epoch 720, val loss: 0.9893444776535034
Epoch 730, training loss: 63.5089225769043 = 0.6269938945770264 + 10.0 * 6.2881927490234375
Epoch 730, val loss: 0.9856643080711365
Epoch 740, training loss: 63.494239807128906 = 0.6125604510307312 + 10.0 * 6.288167953491211
Epoch 740, val loss: 0.9829510450363159
Epoch 750, training loss: 63.46599197387695 = 0.5982310771942139 + 10.0 * 6.286776065826416
Epoch 750, val loss: 0.9799257516860962
Epoch 760, training loss: 63.413970947265625 = 0.5843462944030762 + 10.0 * 6.282962322235107
Epoch 760, val loss: 0.978085458278656
Epoch 770, training loss: 63.390281677246094 = 0.5706725120544434 + 10.0 * 6.281960964202881
Epoch 770, val loss: 0.9764397740364075
Epoch 780, training loss: 63.386192321777344 = 0.5572095513343811 + 10.0 * 6.282898426055908
Epoch 780, val loss: 0.9750849008560181
Epoch 790, training loss: 63.35954666137695 = 0.5439184904098511 + 10.0 * 6.281562805175781
Epoch 790, val loss: 0.9737856388092041
Epoch 800, training loss: 63.339900970458984 = 0.530904233455658 + 10.0 * 6.280900001525879
Epoch 800, val loss: 0.9732196927070618
Epoch 810, training loss: 63.29773712158203 = 0.5182932019233704 + 10.0 * 6.277944564819336
Epoch 810, val loss: 0.9728884100914001
Epoch 820, training loss: 63.29340744018555 = 0.5059934854507446 + 10.0 * 6.278741359710693
Epoch 820, val loss: 0.9728498458862305
Epoch 830, training loss: 63.260902404785156 = 0.4938831031322479 + 10.0 * 6.276701927185059
Epoch 830, val loss: 0.973107099533081
Epoch 840, training loss: 63.2394905090332 = 0.4821188449859619 + 10.0 * 6.2757368087768555
Epoch 840, val loss: 0.9736388325691223
Epoch 850, training loss: 63.237239837646484 = 0.47065356373786926 + 10.0 * 6.276658535003662
Epoch 850, val loss: 0.9744415283203125
Epoch 860, training loss: 63.195003509521484 = 0.45952728390693665 + 10.0 * 6.273547649383545
Epoch 860, val loss: 0.9761130809783936
Epoch 870, training loss: 63.17441177368164 = 0.4486914873123169 + 10.0 * 6.272572040557861
Epoch 870, val loss: 0.9776573181152344
Epoch 880, training loss: 63.231258392333984 = 0.4381228983402252 + 10.0 * 6.279313564300537
Epoch 880, val loss: 0.9788683652877808
Epoch 890, training loss: 63.16835403442383 = 0.42788034677505493 + 10.0 * 6.274047374725342
Epoch 890, val loss: 0.9815097451210022
Epoch 900, training loss: 63.11891174316406 = 0.41788598895072937 + 10.0 * 6.270102500915527
Epoch 900, val loss: 0.9834661483764648
Epoch 910, training loss: 63.10401153564453 = 0.4082193672657013 + 10.0 * 6.2695794105529785
Epoch 910, val loss: 0.9863126277923584
Epoch 920, training loss: 63.152557373046875 = 0.3988191783428192 + 10.0 * 6.275373935699463
Epoch 920, val loss: 0.9887840151786804
Epoch 930, training loss: 63.083099365234375 = 0.3895624577999115 + 10.0 * 6.269353866577148
Epoch 930, val loss: 0.99199378490448
Epoch 940, training loss: 63.048675537109375 = 0.3806198537349701 + 10.0 * 6.266805648803711
Epoch 940, val loss: 0.9951891899108887
Epoch 950, training loss: 63.040340423583984 = 0.3719293475151062 + 10.0 * 6.266840934753418
Epoch 950, val loss: 0.9984768629074097
Epoch 960, training loss: 63.056671142578125 = 0.3634091913700104 + 10.0 * 6.269326210021973
Epoch 960, val loss: 1.002010703086853
Epoch 970, training loss: 63.031803131103516 = 0.35506218671798706 + 10.0 * 6.267674446105957
Epoch 970, val loss: 1.0059261322021484
Epoch 980, training loss: 63.00404357910156 = 0.34694698452949524 + 10.0 * 6.26570987701416
Epoch 980, val loss: 1.0100643634796143
Epoch 990, training loss: 62.97356414794922 = 0.33897268772125244 + 10.0 * 6.263459205627441
Epoch 990, val loss: 1.0140036344528198
Epoch 1000, training loss: 62.957881927490234 = 0.3311941623687744 + 10.0 * 6.262668609619141
Epoch 1000, val loss: 1.0180598497390747
Epoch 1010, training loss: 62.96885681152344 = 0.32352495193481445 + 10.0 * 6.264533042907715
Epoch 1010, val loss: 1.0221588611602783
Epoch 1020, training loss: 62.95375061035156 = 0.3159341812133789 + 10.0 * 6.263781547546387
Epoch 1020, val loss: 1.027209758758545
Epoch 1030, training loss: 62.90953063964844 = 0.3084065020084381 + 10.0 * 6.260112285614014
Epoch 1030, val loss: 1.0308741331100464
Epoch 1040, training loss: 62.91468048095703 = 0.3010093867778778 + 10.0 * 6.261366844177246
Epoch 1040, val loss: 1.0355417728424072
Epoch 1050, training loss: 62.898826599121094 = 0.2936631739139557 + 10.0 * 6.260516166687012
Epoch 1050, val loss: 1.0400748252868652
Epoch 1060, training loss: 62.876487731933594 = 0.28641802072525024 + 10.0 * 6.259006977081299
Epoch 1060, val loss: 1.045167326927185
Epoch 1070, training loss: 62.856082916259766 = 0.2791968882083893 + 10.0 * 6.257688522338867
Epoch 1070, val loss: 1.0498502254486084
Epoch 1080, training loss: 62.84614562988281 = 0.27203086018562317 + 10.0 * 6.257411479949951
Epoch 1080, val loss: 1.0551997423171997
Epoch 1090, training loss: 62.9241828918457 = 0.2648469805717468 + 10.0 * 6.265933513641357
Epoch 1090, val loss: 1.0597474575042725
Epoch 1100, training loss: 62.879764556884766 = 0.2577034533023834 + 10.0 * 6.262206077575684
Epoch 1100, val loss: 1.065672755241394
Epoch 1110, training loss: 62.812156677246094 = 0.2505631148815155 + 10.0 * 6.25615930557251
Epoch 1110, val loss: 1.0706063508987427
Epoch 1120, training loss: 62.79685974121094 = 0.24354679882526398 + 10.0 * 6.255331516265869
Epoch 1120, val loss: 1.0765604972839355
Epoch 1130, training loss: 62.81330490112305 = 0.23659685254096985 + 10.0 * 6.257670879364014
Epoch 1130, val loss: 1.0820411443710327
Epoch 1140, training loss: 62.77373123168945 = 0.22966602444648743 + 10.0 * 6.254406452178955
Epoch 1140, val loss: 1.0877373218536377
Epoch 1150, training loss: 62.792137145996094 = 0.22287079691886902 + 10.0 * 6.256926536560059
Epoch 1150, val loss: 1.0933669805526733
Epoch 1160, training loss: 62.753448486328125 = 0.21611961722373962 + 10.0 * 6.253733158111572
Epoch 1160, val loss: 1.0999990701675415
Epoch 1170, training loss: 62.732608795166016 = 0.20952974259853363 + 10.0 * 6.252307891845703
Epoch 1170, val loss: 1.1056236028671265
Epoch 1180, training loss: 62.74382781982422 = 0.20308755338191986 + 10.0 * 6.2540740966796875
Epoch 1180, val loss: 1.111764907836914
Epoch 1190, training loss: 62.73770523071289 = 0.19679778814315796 + 10.0 * 6.254090785980225
Epoch 1190, val loss: 1.118730068206787
Epoch 1200, training loss: 62.71542739868164 = 0.1906486451625824 + 10.0 * 6.252478122711182
Epoch 1200, val loss: 1.1252282857894897
Epoch 1210, training loss: 62.6922607421875 = 0.18468989431858063 + 10.0 * 6.250757217407227
Epoch 1210, val loss: 1.1315075159072876
Epoch 1220, training loss: 62.68183517456055 = 0.17893609404563904 + 10.0 * 6.2502899169921875
Epoch 1220, val loss: 1.1386868953704834
Epoch 1230, training loss: 62.715145111083984 = 0.17337441444396973 + 10.0 * 6.254177093505859
Epoch 1230, val loss: 1.1453207731246948
Epoch 1240, training loss: 62.67124557495117 = 0.16797129809856415 + 10.0 * 6.250327110290527
Epoch 1240, val loss: 1.1515699625015259
Epoch 1250, training loss: 62.65074920654297 = 0.16278164088726044 + 10.0 * 6.2487969398498535
Epoch 1250, val loss: 1.1587140560150146
Epoch 1260, training loss: 62.634952545166016 = 0.1578015387058258 + 10.0 * 6.247714996337891
Epoch 1260, val loss: 1.1659703254699707
Epoch 1270, training loss: 62.655521392822266 = 0.15299902856349945 + 10.0 * 6.2502522468566895
Epoch 1270, val loss: 1.1732864379882812
Epoch 1280, training loss: 62.64234924316406 = 0.14832375943660736 + 10.0 * 6.2494025230407715
Epoch 1280, val loss: 1.1804059743881226
Epoch 1290, training loss: 62.6041259765625 = 0.14382168650627136 + 10.0 * 6.246030330657959
Epoch 1290, val loss: 1.187415599822998
Epoch 1300, training loss: 62.59096145629883 = 0.1395183503627777 + 10.0 * 6.245144367218018
Epoch 1300, val loss: 1.194633960723877
Epoch 1310, training loss: 62.61359405517578 = 0.13537758588790894 + 10.0 * 6.247821807861328
Epoch 1310, val loss: 1.2025622129440308
Epoch 1320, training loss: 62.57706832885742 = 0.131352499127388 + 10.0 * 6.244571685791016
Epoch 1320, val loss: 1.2089675664901733
Epoch 1330, training loss: 62.57532501220703 = 0.12747198343276978 + 10.0 * 6.244785308837891
Epoch 1330, val loss: 1.2164218425750732
Epoch 1340, training loss: 62.618995666503906 = 0.12376569956541061 + 10.0 * 6.249523162841797
Epoch 1340, val loss: 1.223551869392395
Epoch 1350, training loss: 62.57207489013672 = 0.12012388557195663 + 10.0 * 6.245194911956787
Epoch 1350, val loss: 1.2310153245925903
Epoch 1360, training loss: 62.550559997558594 = 0.11666911095380783 + 10.0 * 6.243389129638672
Epoch 1360, val loss: 1.2386460304260254
Epoch 1370, training loss: 62.55744552612305 = 0.11333023011684418 + 10.0 * 6.244411468505859
Epoch 1370, val loss: 1.2461678981781006
Epoch 1380, training loss: 62.5404052734375 = 0.11010701954364777 + 10.0 * 6.243029594421387
Epoch 1380, val loss: 1.2533998489379883
Epoch 1390, training loss: 62.52449417114258 = 0.1070043295621872 + 10.0 * 6.241748809814453
Epoch 1390, val loss: 1.2611289024353027
Epoch 1400, training loss: 62.51234817504883 = 0.10400627553462982 + 10.0 * 6.2408342361450195
Epoch 1400, val loss: 1.2685760259628296
Epoch 1410, training loss: 62.50990676879883 = 0.10110194981098175 + 10.0 * 6.240880489349365
Epoch 1410, val loss: 1.2762138843536377
Epoch 1420, training loss: 62.571746826171875 = 0.09829957038164139 + 10.0 * 6.247344493865967
Epoch 1420, val loss: 1.284087061882019
Epoch 1430, training loss: 62.515323638916016 = 0.09555479884147644 + 10.0 * 6.241976737976074
Epoch 1430, val loss: 1.291387677192688
Epoch 1440, training loss: 62.4900016784668 = 0.09290361404418945 + 10.0 * 6.239709854125977
Epoch 1440, val loss: 1.2990193367004395
Epoch 1450, training loss: 62.538639068603516 = 0.09038335829973221 + 10.0 * 6.244825839996338
Epoch 1450, val loss: 1.3070440292358398
Epoch 1460, training loss: 62.48649978637695 = 0.08792871981859207 + 10.0 * 6.239857196807861
Epoch 1460, val loss: 1.3137552738189697
Epoch 1470, training loss: 62.46009826660156 = 0.08556842058897018 + 10.0 * 6.237452983856201
Epoch 1470, val loss: 1.3220372200012207
Epoch 1480, training loss: 62.457244873046875 = 0.08329082280397415 + 10.0 * 6.237395286560059
Epoch 1480, val loss: 1.32941472530365
Epoch 1490, training loss: 62.55900955200195 = 0.08108200132846832 + 10.0 * 6.247792720794678
Epoch 1490, val loss: 1.336699366569519
Epoch 1500, training loss: 62.47262191772461 = 0.0789409652352333 + 10.0 * 6.239367961883545
Epoch 1500, val loss: 1.3449201583862305
Epoch 1510, training loss: 62.43812561035156 = 0.07685842365026474 + 10.0 * 6.236126899719238
Epoch 1510, val loss: 1.3523708581924438
Epoch 1520, training loss: 62.42979431152344 = 0.07488573342561722 + 10.0 * 6.235490798950195
Epoch 1520, val loss: 1.3602094650268555
Epoch 1530, training loss: 62.514312744140625 = 0.07297718524932861 + 10.0 * 6.244133472442627
Epoch 1530, val loss: 1.3677892684936523
Epoch 1540, training loss: 62.466163635253906 = 0.0710684061050415 + 10.0 * 6.239509582519531
Epoch 1540, val loss: 1.3759995698928833
Epoch 1550, training loss: 62.4271240234375 = 0.06925641745328903 + 10.0 * 6.2357869148254395
Epoch 1550, val loss: 1.383335828781128
Epoch 1560, training loss: 62.40904998779297 = 0.06750747561454773 + 10.0 * 6.234154224395752
Epoch 1560, val loss: 1.391196370124817
Epoch 1570, training loss: 62.442874908447266 = 0.06581858545541763 + 10.0 * 6.237705707550049
Epoch 1570, val loss: 1.3990449905395508
Epoch 1580, training loss: 62.41135787963867 = 0.06417447328567505 + 10.0 * 6.234718322753906
Epoch 1580, val loss: 1.4063106775283813
Epoch 1590, training loss: 62.416805267333984 = 0.0625792145729065 + 10.0 * 6.235422611236572
Epoch 1590, val loss: 1.4144176244735718
Epoch 1600, training loss: 62.41240692138672 = 0.06103269383311272 + 10.0 * 6.235137462615967
Epoch 1600, val loss: 1.4216973781585693
Epoch 1610, training loss: 62.390743255615234 = 0.059528153389692307 + 10.0 * 6.233121395111084
Epoch 1610, val loss: 1.4286977052688599
Epoch 1620, training loss: 62.37858963012695 = 0.05809217318892479 + 10.0 * 6.232049465179443
Epoch 1620, val loss: 1.436753749847412
Epoch 1630, training loss: 62.375465393066406 = 0.05670318379998207 + 10.0 * 6.231876373291016
Epoch 1630, val loss: 1.44423246383667
Epoch 1640, training loss: 62.448944091796875 = 0.05535903573036194 + 10.0 * 6.239358425140381
Epoch 1640, val loss: 1.451198935508728
Epoch 1650, training loss: 62.39466094970703 = 0.05403383448719978 + 10.0 * 6.234062671661377
Epoch 1650, val loss: 1.4595954418182373
Epoch 1660, training loss: 62.36336898803711 = 0.05275217071175575 + 10.0 * 6.2310614585876465
Epoch 1660, val loss: 1.466752529144287
Epoch 1670, training loss: 62.35741424560547 = 0.0515277162194252 + 10.0 * 6.230588436126709
Epoch 1670, val loss: 1.4745111465454102
Epoch 1680, training loss: 62.44706726074219 = 0.05033489689230919 + 10.0 * 6.239673137664795
Epoch 1680, val loss: 1.4813032150268555
Epoch 1690, training loss: 62.397315979003906 = 0.0491594560444355 + 10.0 * 6.23481559753418
Epoch 1690, val loss: 1.4891834259033203
Epoch 1700, training loss: 62.35382843017578 = 0.04801653325557709 + 10.0 * 6.230581283569336
Epoch 1700, val loss: 1.496695637702942
Epoch 1710, training loss: 62.33501052856445 = 0.04692601040005684 + 10.0 * 6.228808403015137
Epoch 1710, val loss: 1.5041056871414185
Epoch 1720, training loss: 62.331764221191406 = 0.045876264572143555 + 10.0 * 6.228589057922363
Epoch 1720, val loss: 1.511561632156372
Epoch 1730, training loss: 62.38238525390625 = 0.04485965892672539 + 10.0 * 6.233752250671387
Epoch 1730, val loss: 1.5193175077438354
Epoch 1740, training loss: 62.407772064208984 = 0.043858397752046585 + 10.0 * 6.236391544342041
Epoch 1740, val loss: 1.526007890701294
Epoch 1750, training loss: 62.343727111816406 = 0.042863354086875916 + 10.0 * 6.230086326599121
Epoch 1750, val loss: 1.532845377922058
Epoch 1760, training loss: 62.3134765625 = 0.04192422330379486 + 10.0 * 6.2271552085876465
Epoch 1760, val loss: 1.5405452251434326
Epoch 1770, training loss: 62.31047058105469 = 0.04102665185928345 + 10.0 * 6.226944446563721
Epoch 1770, val loss: 1.5475780963897705
Epoch 1780, training loss: 62.337135314941406 = 0.04015811160206795 + 10.0 * 6.2296977043151855
Epoch 1780, val loss: 1.5548053979873657
Epoch 1790, training loss: 62.31912612915039 = 0.03928748890757561 + 10.0 * 6.2279839515686035
Epoch 1790, val loss: 1.5613240003585815
Epoch 1800, training loss: 62.298675537109375 = 0.03844822570681572 + 10.0 * 6.226022720336914
Epoch 1800, val loss: 1.5684338808059692
Epoch 1810, training loss: 62.294490814208984 = 0.03763860836625099 + 10.0 * 6.225685119628906
Epoch 1810, val loss: 1.5753464698791504
Epoch 1820, training loss: 62.3095817565918 = 0.036862220615148544 + 10.0 * 6.227272033691406
Epoch 1820, val loss: 1.5818673372268677
Epoch 1830, training loss: 62.313926696777344 = 0.03610023111104965 + 10.0 * 6.227782726287842
Epoch 1830, val loss: 1.588678240776062
Epoch 1840, training loss: 62.310401916503906 = 0.0353592187166214 + 10.0 * 6.227504253387451
Epoch 1840, val loss: 1.5957794189453125
Epoch 1850, training loss: 62.310577392578125 = 0.03463797643780708 + 10.0 * 6.227593898773193
Epoch 1850, val loss: 1.6021974086761475
Epoch 1860, training loss: 62.292503356933594 = 0.03393647447228432 + 10.0 * 6.225856781005859
Epoch 1860, val loss: 1.609389066696167
Epoch 1870, training loss: 62.2752685546875 = 0.03325563296675682 + 10.0 * 6.224201202392578
Epoch 1870, val loss: 1.6158937215805054
Epoch 1880, training loss: 62.299251556396484 = 0.03260069340467453 + 10.0 * 6.226665019989014
Epoch 1880, val loss: 1.622104287147522
Epoch 1890, training loss: 62.30253601074219 = 0.03195686638355255 + 10.0 * 6.227057933807373
Epoch 1890, val loss: 1.6287126541137695
Epoch 1900, training loss: 62.26835632324219 = 0.03132234513759613 + 10.0 * 6.223703384399414
Epoch 1900, val loss: 1.635015606880188
Epoch 1910, training loss: 62.264678955078125 = 0.030715618282556534 + 10.0 * 6.223396301269531
Epoch 1910, val loss: 1.6416443586349487
Epoch 1920, training loss: 62.271053314208984 = 0.030131222680211067 + 10.0 * 6.22409200668335
Epoch 1920, val loss: 1.647926926612854
Epoch 1930, training loss: 62.28318405151367 = 0.02955768071115017 + 10.0 * 6.225362777709961
Epoch 1930, val loss: 1.6539902687072754
Epoch 1940, training loss: 62.281707763671875 = 0.028996463865041733 + 10.0 * 6.225271224975586
Epoch 1940, val loss: 1.6601134538650513
Epoch 1950, training loss: 62.262760162353516 = 0.02845475822687149 + 10.0 * 6.223430633544922
Epoch 1950, val loss: 1.6663548946380615
Epoch 1960, training loss: 62.27030944824219 = 0.02792530134320259 + 10.0 * 6.224238395690918
Epoch 1960, val loss: 1.6727923154830933
Epoch 1970, training loss: 62.28432083129883 = 0.027409514412283897 + 10.0 * 6.225691318511963
Epoch 1970, val loss: 1.6785320043563843
Epoch 1980, training loss: 62.246971130371094 = 0.02690141834318638 + 10.0 * 6.222006797790527
Epoch 1980, val loss: 1.6849819421768188
Epoch 1990, training loss: 62.243202209472656 = 0.02641899138689041 + 10.0 * 6.221678256988525
Epoch 1990, val loss: 1.6912912130355835
Epoch 2000, training loss: 62.237117767333984 = 0.025947686284780502 + 10.0 * 6.22111701965332
Epoch 2000, val loss: 1.6972236633300781
Epoch 2010, training loss: 62.33049011230469 = 0.025502681732177734 + 10.0 * 6.230498790740967
Epoch 2010, val loss: 1.7037054300308228
Epoch 2020, training loss: 62.26854705810547 = 0.025022640824317932 + 10.0 * 6.2243523597717285
Epoch 2020, val loss: 1.7084252834320068
Epoch 2030, training loss: 62.24429702758789 = 0.024586297571659088 + 10.0 * 6.221971035003662
Epoch 2030, val loss: 1.7145955562591553
Epoch 2040, training loss: 62.230045318603516 = 0.02415541559457779 + 10.0 * 6.2205891609191895
Epoch 2040, val loss: 1.720259189605713
Epoch 2050, training loss: 62.2921028137207 = 0.02375372126698494 + 10.0 * 6.226834774017334
Epoch 2050, val loss: 1.7258106470108032
Epoch 2060, training loss: 62.23900604248047 = 0.023333001881837845 + 10.0 * 6.221567153930664
Epoch 2060, val loss: 1.7311393022537231
Epoch 2070, training loss: 62.22252655029297 = 0.022936472669243813 + 10.0 * 6.219958782196045
Epoch 2070, val loss: 1.7370549440383911
Epoch 2080, training loss: 62.213706970214844 = 0.022549021989107132 + 10.0 * 6.219115734100342
Epoch 2080, val loss: 1.7425236701965332
Epoch 2090, training loss: 62.22328186035156 = 0.022182254120707512 + 10.0 * 6.220109939575195
Epoch 2090, val loss: 1.7479537725448608
Epoch 2100, training loss: 62.2496337890625 = 0.02181575819849968 + 10.0 * 6.222781658172607
Epoch 2100, val loss: 1.7530514001846313
Epoch 2110, training loss: 62.2447509765625 = 0.021448496729135513 + 10.0 * 6.222330093383789
Epoch 2110, val loss: 1.7589271068572998
Epoch 2120, training loss: 62.21567153930664 = 0.02109634503722191 + 10.0 * 6.219457626342773
Epoch 2120, val loss: 1.7637525796890259
Epoch 2130, training loss: 62.22669982910156 = 0.020752454176545143 + 10.0 * 6.220594882965088
Epoch 2130, val loss: 1.7688082456588745
Epoch 2140, training loss: 62.20875549316406 = 0.020422326400876045 + 10.0 * 6.218832969665527
Epoch 2140, val loss: 1.7743204832077026
Epoch 2150, training loss: 62.207496643066406 = 0.02009420655667782 + 10.0 * 6.218739986419678
Epoch 2150, val loss: 1.779852032661438
Epoch 2160, training loss: 62.231597900390625 = 0.019778888672590256 + 10.0 * 6.221181869506836
Epoch 2160, val loss: 1.7848186492919922
Epoch 2170, training loss: 62.20624542236328 = 0.019462058320641518 + 10.0 * 6.2186784744262695
Epoch 2170, val loss: 1.790035605430603
Epoch 2180, training loss: 62.24101638793945 = 0.019165756180882454 + 10.0 * 6.222185134887695
Epoch 2180, val loss: 1.795530915260315
Epoch 2190, training loss: 62.19645690917969 = 0.018858574330806732 + 10.0 * 6.217759609222412
Epoch 2190, val loss: 1.7997117042541504
Epoch 2200, training loss: 62.1866340637207 = 0.018565136939287186 + 10.0 * 6.216806888580322
Epoch 2200, val loss: 1.8049029111862183
Epoch 2210, training loss: 62.18022918701172 = 0.018285010010004044 + 10.0 * 6.2161946296691895
Epoch 2210, val loss: 1.809996247291565
Epoch 2220, training loss: 62.180267333984375 = 0.018013086169958115 + 10.0 * 6.216225624084473
Epoch 2220, val loss: 1.8144091367721558
Epoch 2230, training loss: 62.28858947753906 = 0.01775294356048107 + 10.0 * 6.227083683013916
Epoch 2230, val loss: 1.818259596824646
Epoch 2240, training loss: 62.23693084716797 = 0.017471853643655777 + 10.0 * 6.221945762634277
Epoch 2240, val loss: 1.8239798545837402
Epoch 2250, training loss: 62.18742752075195 = 0.01720973290503025 + 10.0 * 6.217021942138672
Epoch 2250, val loss: 1.8287239074707031
Epoch 2260, training loss: 62.170310974121094 = 0.016956772655248642 + 10.0 * 6.215335369110107
Epoch 2260, val loss: 1.8336573839187622
Epoch 2270, training loss: 62.17156982421875 = 0.016714775934815407 + 10.0 * 6.215485572814941
Epoch 2270, val loss: 1.838273286819458
Epoch 2280, training loss: 62.23339080810547 = 0.016480037942528725 + 10.0 * 6.221691131591797
Epoch 2280, val loss: 1.8425254821777344
Epoch 2290, training loss: 62.18434143066406 = 0.016237737610936165 + 10.0 * 6.21681022644043
Epoch 2290, val loss: 1.846971035003662
Epoch 2300, training loss: 62.1890869140625 = 0.01600477658212185 + 10.0 * 6.217308044433594
Epoch 2300, val loss: 1.8510234355926514
Epoch 2310, training loss: 62.17241668701172 = 0.015776775777339935 + 10.0 * 6.215663909912109
Epoch 2310, val loss: 1.8559273481369019
Epoch 2320, training loss: 62.170875549316406 = 0.015556734055280685 + 10.0 * 6.215531826019287
Epoch 2320, val loss: 1.8603615760803223
Epoch 2330, training loss: 62.17009735107422 = 0.015344582498073578 + 10.0 * 6.215475559234619
Epoch 2330, val loss: 1.8647141456604004
Epoch 2340, training loss: 62.17808532714844 = 0.01513618789613247 + 10.0 * 6.216294765472412
Epoch 2340, val loss: 1.868990421295166
Epoch 2350, training loss: 62.1507568359375 = 0.014923402108252048 + 10.0 * 6.213583469390869
Epoch 2350, val loss: 1.8737757205963135
Epoch 2360, training loss: 62.166629791259766 = 0.014725860208272934 + 10.0 * 6.215190410614014
Epoch 2360, val loss: 1.8782787322998047
Epoch 2370, training loss: 62.221736907958984 = 0.014535916037857533 + 10.0 * 6.220719814300537
Epoch 2370, val loss: 1.8823796510696411
Epoch 2380, training loss: 62.20787811279297 = 0.014326893724501133 + 10.0 * 6.21935510635376
Epoch 2380, val loss: 1.8867037296295166
Epoch 2390, training loss: 62.16160202026367 = 0.01413528062403202 + 10.0 * 6.214746952056885
Epoch 2390, val loss: 1.8903077840805054
Epoch 2400, training loss: 62.146263122558594 = 0.013945065438747406 + 10.0 * 6.213231563568115
Epoch 2400, val loss: 1.8942973613739014
Epoch 2410, training loss: 62.1526985168457 = 0.013767519034445286 + 10.0 * 6.213892936706543
Epoch 2410, val loss: 1.8985519409179688
Epoch 2420, training loss: 62.19521713256836 = 0.013589481823146343 + 10.0 * 6.218163013458252
Epoch 2420, val loss: 1.9028569459915161
Epoch 2430, training loss: 62.169403076171875 = 0.013415363617241383 + 10.0 * 6.2155985832214355
Epoch 2430, val loss: 1.9069198369979858
Epoch 2440, training loss: 62.138328552246094 = 0.013238861225545406 + 10.0 * 6.2125091552734375
Epoch 2440, val loss: 1.9103273153305054
Epoch 2450, training loss: 62.130496978759766 = 0.013073200359940529 + 10.0 * 6.211742401123047
Epoch 2450, val loss: 1.914368748664856
Epoch 2460, training loss: 62.15238571166992 = 0.012912891805171967 + 10.0 * 6.213947296142578
Epoch 2460, val loss: 1.9180681705474854
Epoch 2470, training loss: 62.1887321472168 = 0.012756296433508396 + 10.0 * 6.217597484588623
Epoch 2470, val loss: 1.9213652610778809
Epoch 2480, training loss: 62.1440544128418 = 0.012585509568452835 + 10.0 * 6.213147163391113
Epoch 2480, val loss: 1.9258366823196411
Epoch 2490, training loss: 62.13251876831055 = 0.01243147999048233 + 10.0 * 6.212008476257324
Epoch 2490, val loss: 1.9294662475585938
Epoch 2500, training loss: 62.13911819458008 = 0.012278699316084385 + 10.0 * 6.21268367767334
Epoch 2500, val loss: 1.9331755638122559
Epoch 2510, training loss: 62.15581512451172 = 0.012131749652326107 + 10.0 * 6.2143683433532715
Epoch 2510, val loss: 1.9365105628967285
Epoch 2520, training loss: 62.12831497192383 = 0.01198309101164341 + 10.0 * 6.211633205413818
Epoch 2520, val loss: 1.9408761262893677
Epoch 2530, training loss: 62.15237808227539 = 0.011844323016703129 + 10.0 * 6.214053153991699
Epoch 2530, val loss: 1.9441487789154053
Epoch 2540, training loss: 62.12775802612305 = 0.011697926558554173 + 10.0 * 6.211606025695801
Epoch 2540, val loss: 1.9477423429489136
Epoch 2550, training loss: 62.11249542236328 = 0.011560042388737202 + 10.0 * 6.2100934982299805
Epoch 2550, val loss: 1.9517091512680054
Epoch 2560, training loss: 62.140113830566406 = 0.01142950914800167 + 10.0 * 6.212868690490723
Epoch 2560, val loss: 1.9555522203445435
Epoch 2570, training loss: 62.14601135253906 = 0.011292573995888233 + 10.0 * 6.21347188949585
Epoch 2570, val loss: 1.958166480064392
Epoch 2580, training loss: 62.11684036254883 = 0.011161583475768566 + 10.0 * 6.210567951202393
Epoch 2580, val loss: 1.962030291557312
Epoch 2590, training loss: 62.10912322998047 = 0.011033334769308567 + 10.0 * 6.209809303283691
Epoch 2590, val loss: 1.9660425186157227
Epoch 2600, training loss: 62.1324577331543 = 0.010909117758274078 + 10.0 * 6.212154865264893
Epoch 2600, val loss: 1.969573736190796
Epoch 2610, training loss: 62.13887405395508 = 0.01078319177031517 + 10.0 * 6.212809085845947
Epoch 2610, val loss: 1.9721627235412598
Epoch 2620, training loss: 62.116600036621094 = 0.010659736581146717 + 10.0 * 6.210594177246094
Epoch 2620, val loss: 1.975538730621338
Epoch 2630, training loss: 62.1138916015625 = 0.010539878159761429 + 10.0 * 6.2103352546691895
Epoch 2630, val loss: 1.9793128967285156
Epoch 2640, training loss: 62.16812515258789 = 0.010424336418509483 + 10.0 * 6.2157697677612305
Epoch 2640, val loss: 1.9821709394454956
Epoch 2650, training loss: 62.1099739074707 = 0.010306615382432938 + 10.0 * 6.209966659545898
Epoch 2650, val loss: 1.9857946634292603
Epoch 2660, training loss: 62.0922737121582 = 0.010193083435297012 + 10.0 * 6.208208084106445
Epoch 2660, val loss: 1.989396572113037
Epoch 2670, training loss: 62.098663330078125 = 0.010084986686706543 + 10.0 * 6.208857536315918
Epoch 2670, val loss: 1.992692470550537
Epoch 2680, training loss: 62.14215087890625 = 0.009979802183806896 + 10.0 * 6.213217258453369
Epoch 2680, val loss: 1.9956170320510864
Epoch 2690, training loss: 62.109737396240234 = 0.009867612272500992 + 10.0 * 6.209986686706543
Epoch 2690, val loss: 1.9983235597610474
Epoch 2700, training loss: 62.13202667236328 = 0.009759417735040188 + 10.0 * 6.212226867675781
Epoch 2700, val loss: 2.0013232231140137
Epoch 2710, training loss: 62.08762741088867 = 0.009654170833528042 + 10.0 * 6.207797050476074
Epoch 2710, val loss: 2.0052907466888428
Epoch 2720, training loss: 62.1085319519043 = 0.009554908610880375 + 10.0 * 6.209897518157959
Epoch 2720, val loss: 2.0082457065582275
Epoch 2730, training loss: 62.12348556518555 = 0.009452838450670242 + 10.0 * 6.2114033699035645
Epoch 2730, val loss: 2.0108041763305664
Epoch 2740, training loss: 62.093223571777344 = 0.009354566223919392 + 10.0 * 6.2083868980407715
Epoch 2740, val loss: 2.014401912689209
Epoch 2750, training loss: 62.086387634277344 = 0.009260592982172966 + 10.0 * 6.207712650299072
Epoch 2750, val loss: 2.0173754692077637
Epoch 2760, training loss: 62.12800598144531 = 0.009171036072075367 + 10.0 * 6.211883544921875
Epoch 2760, val loss: 2.0196895599365234
Epoch 2770, training loss: 62.08140182495117 = 0.009072307497262955 + 10.0 * 6.20723295211792
Epoch 2770, val loss: 2.02343487739563
Epoch 2780, training loss: 62.073387145996094 = 0.00898019876331091 + 10.0 * 6.2064409255981445
Epoch 2780, val loss: 2.0260090827941895
Epoch 2790, training loss: 62.078067779541016 = 0.008891221135854721 + 10.0 * 6.206917762756348
Epoch 2790, val loss: 2.029215097427368
Epoch 2800, training loss: 62.12676239013672 = 0.008805815130472183 + 10.0 * 6.211795806884766
Epoch 2800, val loss: 2.032010793685913
Epoch 2810, training loss: 62.10041427612305 = 0.008718661963939667 + 10.0 * 6.209169387817383
Epoch 2810, val loss: 2.034867286682129
Epoch 2820, training loss: 62.10395050048828 = 0.008631730452179909 + 10.0 * 6.209531784057617
Epoch 2820, val loss: 2.0379083156585693
Epoch 2830, training loss: 62.096981048583984 = 0.008542872965335846 + 10.0 * 6.208844184875488
Epoch 2830, val loss: 2.040529251098633
Epoch 2840, training loss: 62.07438278198242 = 0.008458259515464306 + 10.0 * 6.206592559814453
Epoch 2840, val loss: 2.042888879776001
Epoch 2850, training loss: 62.08238983154297 = 0.00837874598801136 + 10.0 * 6.207401275634766
Epoch 2850, val loss: 2.045768976211548
Epoch 2860, training loss: 62.090126037597656 = 0.00830049254000187 + 10.0 * 6.208182334899902
Epoch 2860, val loss: 2.048729419708252
Epoch 2870, training loss: 62.07101821899414 = 0.008221684955060482 + 10.0 * 6.206279754638672
Epoch 2870, val loss: 2.0513272285461426
Epoch 2880, training loss: 62.06486129760742 = 0.008144655264914036 + 10.0 * 6.205671787261963
Epoch 2880, val loss: 2.0538697242736816
Epoch 2890, training loss: 62.07783508300781 = 0.008071236312389374 + 10.0 * 6.206976413726807
Epoch 2890, val loss: 2.0565216541290283
Epoch 2900, training loss: 62.112640380859375 = 0.007998056709766388 + 10.0 * 6.210464000701904
Epoch 2900, val loss: 2.0587897300720215
Epoch 2910, training loss: 62.09541702270508 = 0.00791906751692295 + 10.0 * 6.208749771118164
Epoch 2910, val loss: 2.061420440673828
Epoch 2920, training loss: 62.08403396606445 = 0.007846232503652573 + 10.0 * 6.207618713378906
Epoch 2920, val loss: 2.0644752979278564
Epoch 2930, training loss: 62.076148986816406 = 0.00777462450787425 + 10.0 * 6.2068376541137695
Epoch 2930, val loss: 2.0663540363311768
Epoch 2940, training loss: 62.06511688232422 = 0.007703612092882395 + 10.0 * 6.2057414054870605
Epoch 2940, val loss: 2.0691287517547607
Epoch 2950, training loss: 62.05874252319336 = 0.007635023910552263 + 10.0 * 6.205111026763916
Epoch 2950, val loss: 2.0710339546203613
Epoch 2960, training loss: 62.160343170166016 = 0.007571886759251356 + 10.0 * 6.215277194976807
Epoch 2960, val loss: 2.0725467205047607
Epoch 2970, training loss: 62.09497833251953 = 0.007498631719499826 + 10.0 * 6.208747863769531
Epoch 2970, val loss: 2.076378107070923
Epoch 2980, training loss: 62.0611686706543 = 0.007429471239447594 + 10.0 * 6.205373764038086
Epoch 2980, val loss: 2.0781872272491455
Epoch 2990, training loss: 62.05984878540039 = 0.0073673417791724205 + 10.0 * 6.2052483558654785
Epoch 2990, val loss: 2.081284761428833
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8065366367949395
The final CL Acc:0.72840, 0.00873, The final GNN Acc:0.80900, 0.00194
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13122])
remove edge: torch.Size([2, 7944])
updated graph: torch.Size([2, 10510])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.91207122802734 = 1.9438740015029907 + 10.0 * 8.596819877624512
Epoch 0, val loss: 1.9486007690429688
Epoch 10, training loss: 87.89429473876953 = 1.9342185258865356 + 10.0 * 8.596007347106934
Epoch 10, val loss: 1.9395288228988647
Epoch 20, training loss: 87.8204345703125 = 1.9221001863479614 + 10.0 * 8.58983325958252
Epoch 20, val loss: 1.927575707435608
Epoch 30, training loss: 87.38331604003906 = 1.9060243368148804 + 10.0 * 8.5477294921875
Epoch 30, val loss: 1.9113473892211914
Epoch 40, training loss: 85.00550079345703 = 1.8863446712493896 + 10.0 * 8.311915397644043
Epoch 40, val loss: 1.8915539979934692
Epoch 50, training loss: 79.94871520996094 = 1.8627183437347412 + 10.0 * 7.808599472045898
Epoch 50, val loss: 1.8682756423950195
Epoch 60, training loss: 76.22566986083984 = 1.8454174995422363 + 10.0 * 7.438024997711182
Epoch 60, val loss: 1.852631688117981
Epoch 70, training loss: 72.6862564086914 = 1.8405096530914307 + 10.0 * 7.0845746994018555
Epoch 70, val loss: 1.848185658454895
Epoch 80, training loss: 71.50739288330078 = 1.8335130214691162 + 10.0 * 6.967388153076172
Epoch 80, val loss: 1.8405749797821045
Epoch 90, training loss: 70.68083953857422 = 1.821216344833374 + 10.0 * 6.885962963104248
Epoch 90, val loss: 1.8287100791931152
Epoch 100, training loss: 69.83223724365234 = 1.810294508934021 + 10.0 * 6.802194595336914
Epoch 100, val loss: 1.8187319040298462
Epoch 110, training loss: 69.15919494628906 = 1.8024653196334839 + 10.0 * 6.735672950744629
Epoch 110, val loss: 1.8108892440795898
Epoch 120, training loss: 68.60115814208984 = 1.7944574356079102 + 10.0 * 6.680670261383057
Epoch 120, val loss: 1.8025600910186768
Epoch 130, training loss: 68.1286849975586 = 1.7859739065170288 + 10.0 * 6.634271621704102
Epoch 130, val loss: 1.793897271156311
Epoch 140, training loss: 67.76396179199219 = 1.7770824432373047 + 10.0 * 6.598687648773193
Epoch 140, val loss: 1.7848031520843506
Epoch 150, training loss: 67.44964599609375 = 1.767964243888855 + 10.0 * 6.5681681632995605
Epoch 150, val loss: 1.775707721710205
Epoch 160, training loss: 67.20362854003906 = 1.758867859840393 + 10.0 * 6.54447603225708
Epoch 160, val loss: 1.7667057514190674
Epoch 170, training loss: 66.95768737792969 = 1.7494007349014282 + 10.0 * 6.520829200744629
Epoch 170, val loss: 1.7575258016586304
Epoch 180, training loss: 66.80862426757812 = 1.7393345832824707 + 10.0 * 6.50692892074585
Epoch 180, val loss: 1.748024821281433
Epoch 190, training loss: 66.58172607421875 = 1.728167176246643 + 10.0 * 6.485355854034424
Epoch 190, val loss: 1.7378449440002441
Epoch 200, training loss: 66.4173812866211 = 1.7163575887680054 + 10.0 * 6.470102787017822
Epoch 200, val loss: 1.727144479751587
Epoch 210, training loss: 66.2717514038086 = 1.7035571336746216 + 10.0 * 6.4568190574646
Epoch 210, val loss: 1.7156826257705688
Epoch 220, training loss: 66.14364624023438 = 1.689640760421753 + 10.0 * 6.445400238037109
Epoch 220, val loss: 1.7033849954605103
Epoch 230, training loss: 66.02638244628906 = 1.6742875576019287 + 10.0 * 6.435209274291992
Epoch 230, val loss: 1.6898174285888672
Epoch 240, training loss: 65.91374206542969 = 1.65789794921875 + 10.0 * 6.42558479309082
Epoch 240, val loss: 1.6753637790679932
Epoch 250, training loss: 65.8117446899414 = 1.6402738094329834 + 10.0 * 6.417147159576416
Epoch 250, val loss: 1.6599334478378296
Epoch 260, training loss: 65.7225341796875 = 1.6213470697402954 + 10.0 * 6.410118579864502
Epoch 260, val loss: 1.6435091495513916
Epoch 270, training loss: 65.6485366821289 = 1.6010862588882446 + 10.0 * 6.404744625091553
Epoch 270, val loss: 1.6259005069732666
Epoch 280, training loss: 65.54434967041016 = 1.5797096490859985 + 10.0 * 6.396463871002197
Epoch 280, val loss: 1.6074929237365723
Epoch 290, training loss: 65.46016693115234 = 1.5572881698608398 + 10.0 * 6.39028787612915
Epoch 290, val loss: 1.5883004665374756
Epoch 300, training loss: 65.40934753417969 = 1.5337165594100952 + 10.0 * 6.387563228607178
Epoch 300, val loss: 1.5682066679000854
Epoch 310, training loss: 65.29462432861328 = 1.5096452236175537 + 10.0 * 6.378498077392578
Epoch 310, val loss: 1.5477914810180664
Epoch 320, training loss: 65.21571350097656 = 1.4848730564117432 + 10.0 * 6.37308406829834
Epoch 320, val loss: 1.5269137620925903
Epoch 330, training loss: 65.14093017578125 = 1.459611177444458 + 10.0 * 6.3681321144104
Epoch 330, val loss: 1.5057040452957153
Epoch 340, training loss: 65.14373779296875 = 1.433678150177002 + 10.0 * 6.371005535125732
Epoch 340, val loss: 1.4842305183410645
Epoch 350, training loss: 65.01039123535156 = 1.407720923423767 + 10.0 * 6.360267162322998
Epoch 350, val loss: 1.462790608406067
Epoch 360, training loss: 64.93788146972656 = 1.3816274404525757 + 10.0 * 6.355625629425049
Epoch 360, val loss: 1.441433310508728
Epoch 370, training loss: 64.86266326904297 = 1.3554906845092773 + 10.0 * 6.350717067718506
Epoch 370, val loss: 1.4202120304107666
Epoch 380, training loss: 64.80738830566406 = 1.3292388916015625 + 10.0 * 6.347814559936523
Epoch 380, val loss: 1.3990247249603271
Epoch 390, training loss: 64.74203491210938 = 1.3026328086853027 + 10.0 * 6.343940258026123
Epoch 390, val loss: 1.3778963088989258
Epoch 400, training loss: 64.72154235839844 = 1.276211142539978 + 10.0 * 6.3445329666137695
Epoch 400, val loss: 1.356813669204712
Epoch 410, training loss: 64.65176391601562 = 1.249538540840149 + 10.0 * 6.340222358703613
Epoch 410, val loss: 1.3357399702072144
Epoch 420, training loss: 64.57089233398438 = 1.2229398488998413 + 10.0 * 6.334794998168945
Epoch 420, val loss: 1.3149269819259644
Epoch 430, training loss: 64.50917053222656 = 1.1962627172470093 + 10.0 * 6.331290245056152
Epoch 430, val loss: 1.2942211627960205
Epoch 440, training loss: 64.50641632080078 = 1.1693953275680542 + 10.0 * 6.333702087402344
Epoch 440, val loss: 1.273578405380249
Epoch 450, training loss: 64.4211196899414 = 1.1423283815383911 + 10.0 * 6.327879428863525
Epoch 450, val loss: 1.2526870965957642
Epoch 460, training loss: 64.34329986572266 = 1.1153512001037598 + 10.0 * 6.3227949142456055
Epoch 460, val loss: 1.232090950012207
Epoch 470, training loss: 64.294921875 = 1.0883938074111938 + 10.0 * 6.320652961730957
Epoch 470, val loss: 1.2117202281951904
Epoch 480, training loss: 64.2555160522461 = 1.0614347457885742 + 10.0 * 6.319408416748047
Epoch 480, val loss: 1.1914514303207397
Epoch 490, training loss: 64.22257995605469 = 1.0344102382659912 + 10.0 * 6.318817138671875
Epoch 490, val loss: 1.171152949333191
Epoch 500, training loss: 64.1443099975586 = 1.007602334022522 + 10.0 * 6.313671112060547
Epoch 500, val loss: 1.1513049602508545
Epoch 510, training loss: 64.13811492919922 = 0.981106162071228 + 10.0 * 6.315700531005859
Epoch 510, val loss: 1.1318190097808838
Epoch 520, training loss: 64.0833969116211 = 0.9549189209938049 + 10.0 * 6.312848091125488
Epoch 520, val loss: 1.1127521991729736
Epoch 530, training loss: 64.009765625 = 0.929107129573822 + 10.0 * 6.308065891265869
Epoch 530, val loss: 1.0941147804260254
Epoch 540, training loss: 63.95518112182617 = 0.9038349986076355 + 10.0 * 6.3051347732543945
Epoch 540, val loss: 1.07602858543396
Epoch 550, training loss: 63.94440460205078 = 0.8790349960327148 + 10.0 * 6.306536674499512
Epoch 550, val loss: 1.0585604906082153
Epoch 560, training loss: 63.89521408081055 = 0.8547144532203674 + 10.0 * 6.304049968719482
Epoch 560, val loss: 1.0415433645248413
Epoch 570, training loss: 63.84531784057617 = 0.8310559988021851 + 10.0 * 6.301426410675049
Epoch 570, val loss: 1.025184988975525
Epoch 580, training loss: 63.79409408569336 = 0.8080726265907288 + 10.0 * 6.298602104187012
Epoch 580, val loss: 1.009521484375
Epoch 590, training loss: 63.760276794433594 = 0.7857233285903931 + 10.0 * 6.297455310821533
Epoch 590, val loss: 0.994641900062561
Epoch 600, training loss: 63.722869873046875 = 0.7639843225479126 + 10.0 * 6.295888423919678
Epoch 600, val loss: 0.9802460670471191
Epoch 610, training loss: 63.68384552001953 = 0.7429070472717285 + 10.0 * 6.294093608856201
Epoch 610, val loss: 0.9665535092353821
Epoch 620, training loss: 63.672943115234375 = 0.7225415110588074 + 10.0 * 6.295040130615234
Epoch 620, val loss: 0.9535679817199707
Epoch 630, training loss: 63.61781311035156 = 0.7027116417884827 + 10.0 * 6.291510105133057
Epoch 630, val loss: 0.9411388039588928
Epoch 640, training loss: 63.58635330200195 = 0.6835981011390686 + 10.0 * 6.290275573730469
Epoch 640, val loss: 0.9295340180397034
Epoch 650, training loss: 63.55261993408203 = 0.6650548577308655 + 10.0 * 6.288756370544434
Epoch 650, val loss: 0.9184783101081848
Epoch 660, training loss: 63.52760314941406 = 0.6470499038696289 + 10.0 * 6.288055419921875
Epoch 660, val loss: 0.9079799056053162
Epoch 670, training loss: 63.489925384521484 = 0.6296669840812683 + 10.0 * 6.2860260009765625
Epoch 670, val loss: 0.8980871438980103
Epoch 680, training loss: 63.456600189208984 = 0.6128039956092834 + 10.0 * 6.284379482269287
Epoch 680, val loss: 0.888576090335846
Epoch 690, training loss: 63.42289352416992 = 0.596490740776062 + 10.0 * 6.28264045715332
Epoch 690, val loss: 0.879807710647583
Epoch 700, training loss: 63.38449478149414 = 0.5806498527526855 + 10.0 * 6.280384540557861
Epoch 700, val loss: 0.8715640306472778
Epoch 710, training loss: 63.44734573364258 = 0.5652186274528503 + 10.0 * 6.288212776184082
Epoch 710, val loss: 0.8638519048690796
Epoch 720, training loss: 63.357757568359375 = 0.5500238537788391 + 10.0 * 6.280773639678955
Epoch 720, val loss: 0.8561100959777832
Epoch 730, training loss: 63.343624114990234 = 0.5352771878242493 + 10.0 * 6.280834674835205
Epoch 730, val loss: 0.8490294218063354
Epoch 740, training loss: 63.28770446777344 = 0.52094566822052 + 10.0 * 6.276675701141357
Epoch 740, val loss: 0.8424011468887329
Epoch 750, training loss: 63.25490188598633 = 0.5069044232368469 + 10.0 * 6.274799823760986
Epoch 750, val loss: 0.8361155390739441
Epoch 760, training loss: 63.253074645996094 = 0.49318617582321167 + 10.0 * 6.275988578796387
Epoch 760, val loss: 0.8302198052406311
Epoch 770, training loss: 63.26321029663086 = 0.4797258973121643 + 10.0 * 6.278348445892334
Epoch 770, val loss: 0.8244988918304443
Epoch 780, training loss: 63.21040725708008 = 0.46641790866851807 + 10.0 * 6.2743988037109375
Epoch 780, val loss: 0.8190174698829651
Epoch 790, training loss: 63.166019439697266 = 0.4534979462623596 + 10.0 * 6.271252155303955
Epoch 790, val loss: 0.8139939308166504
Epoch 800, training loss: 63.15869903564453 = 0.44078415632247925 + 10.0 * 6.271791458129883
Epoch 800, val loss: 0.8092794418334961
Epoch 810, training loss: 63.11668395996094 = 0.4282217025756836 + 10.0 * 6.268846035003662
Epoch 810, val loss: 0.8047075867652893
Epoch 820, training loss: 63.098106384277344 = 0.41589224338531494 + 10.0 * 6.268221378326416
Epoch 820, val loss: 0.8005191087722778
Epoch 830, training loss: 63.086124420166016 = 0.4037727117538452 + 10.0 * 6.268235206604004
Epoch 830, val loss: 0.7964984178543091
Epoch 840, training loss: 63.071990966796875 = 0.39188525080680847 + 10.0 * 6.26801061630249
Epoch 840, val loss: 0.7927342057228088
Epoch 850, training loss: 63.04824447631836 = 0.3801988661289215 + 10.0 * 6.2668046951293945
Epoch 850, val loss: 0.7892234921455383
Epoch 860, training loss: 63.023250579833984 = 0.36874815821647644 + 10.0 * 6.265450477600098
Epoch 860, val loss: 0.7858866453170776
Epoch 870, training loss: 63.03665542602539 = 0.3575544059276581 + 10.0 * 6.267910003662109
Epoch 870, val loss: 0.7828764915466309
Epoch 880, training loss: 62.99094009399414 = 0.3465522229671478 + 10.0 * 6.264438629150391
Epoch 880, val loss: 0.7801087498664856
Epoch 890, training loss: 62.96539306640625 = 0.33584725856781006 + 10.0 * 6.2629547119140625
Epoch 890, val loss: 0.7775154113769531
Epoch 900, training loss: 62.952972412109375 = 0.325402170419693 + 10.0 * 6.262757301330566
Epoch 900, val loss: 0.7751986980438232
Epoch 910, training loss: 62.92417907714844 = 0.3152170479297638 + 10.0 * 6.2608962059021
Epoch 910, val loss: 0.7730587720870972
Epoch 920, training loss: 62.92934036254883 = 0.3053097426891327 + 10.0 * 6.2624030113220215
Epoch 920, val loss: 0.7710567116737366
Epoch 930, training loss: 62.92497253417969 = 0.29565027356147766 + 10.0 * 6.262932300567627
Epoch 930, val loss: 0.7693116068840027
Epoch 940, training loss: 62.878456115722656 = 0.2862327992916107 + 10.0 * 6.259222507476807
Epoch 940, val loss: 0.7676598429679871
Epoch 950, training loss: 62.84218215942383 = 0.2771831452846527 + 10.0 * 6.256499767303467
Epoch 950, val loss: 0.7663443684577942
Epoch 960, training loss: 62.828125 = 0.2684476673603058 + 10.0 * 6.255967617034912
Epoch 960, val loss: 0.765286386013031
Epoch 970, training loss: 62.91755294799805 = 0.25998231768608093 + 10.0 * 6.265757083892822
Epoch 970, val loss: 0.7643958330154419
Epoch 980, training loss: 62.821109771728516 = 0.2516820728778839 + 10.0 * 6.2569427490234375
Epoch 980, val loss: 0.7634802460670471
Epoch 990, training loss: 62.79262924194336 = 0.24374271929264069 + 10.0 * 6.254888534545898
Epoch 990, val loss: 0.7629305124282837
Epoch 1000, training loss: 62.77712631225586 = 0.2360798865556717 + 10.0 * 6.2541046142578125
Epoch 1000, val loss: 0.7626351714134216
Epoch 1010, training loss: 62.78391647338867 = 0.22869344055652618 + 10.0 * 6.25552225112915
Epoch 1010, val loss: 0.7625292539596558
Epoch 1020, training loss: 62.779518127441406 = 0.22153881192207336 + 10.0 * 6.255797863006592
Epoch 1020, val loss: 0.7626654505729675
Epoch 1030, training loss: 62.73579025268555 = 0.21462813019752502 + 10.0 * 6.2521162033081055
Epoch 1030, val loss: 0.7627847194671631
Epoch 1040, training loss: 62.736446380615234 = 0.20797289907932281 + 10.0 * 6.252847194671631
Epoch 1040, val loss: 0.7632842063903809
Epoch 1050, training loss: 62.72624969482422 = 0.2015354037284851 + 10.0 * 6.252471446990967
Epoch 1050, val loss: 0.7636685967445374
Epoch 1060, training loss: 62.70745086669922 = 0.19530659914016724 + 10.0 * 6.251214504241943
Epoch 1060, val loss: 0.7644022107124329
Epoch 1070, training loss: 62.675865173339844 = 0.18929623067378998 + 10.0 * 6.248656749725342
Epoch 1070, val loss: 0.7652796506881714
Epoch 1080, training loss: 62.66477584838867 = 0.18351547420024872 + 10.0 * 6.248126029968262
Epoch 1080, val loss: 0.7662293314933777
Epoch 1090, training loss: 62.7088737487793 = 0.177927628159523 + 10.0 * 6.253094673156738
Epoch 1090, val loss: 0.767304003238678
Epoch 1100, training loss: 62.66243362426758 = 0.17249035835266113 + 10.0 * 6.24899435043335
Epoch 1100, val loss: 0.7686583399772644
Epoch 1110, training loss: 62.647132873535156 = 0.16725388169288635 + 10.0 * 6.247987747192383
Epoch 1110, val loss: 0.7698491215705872
Epoch 1120, training loss: 62.650718688964844 = 0.16221684217453003 + 10.0 * 6.248850345611572
Epoch 1120, val loss: 0.7713366150856018
Epoch 1130, training loss: 62.61738967895508 = 0.15733148157596588 + 10.0 * 6.246005535125732
Epoch 1130, val loss: 0.7726723551750183
Epoch 1140, training loss: 62.60384750366211 = 0.15261821448802948 + 10.0 * 6.245122909545898
Epoch 1140, val loss: 0.7742693424224854
Epoch 1150, training loss: 62.61713790893555 = 0.14809422194957733 + 10.0 * 6.246904373168945
Epoch 1150, val loss: 0.7761149406433105
Epoch 1160, training loss: 62.57200622558594 = 0.14366763830184937 + 10.0 * 6.242833614349365
Epoch 1160, val loss: 0.7778273224830627
Epoch 1170, training loss: 62.57188415527344 = 0.13940837979316711 + 10.0 * 6.2432475090026855
Epoch 1170, val loss: 0.7796724438667297
Epoch 1180, training loss: 62.58078384399414 = 0.13530243933200836 + 10.0 * 6.2445478439331055
Epoch 1180, val loss: 0.7817071676254272
Epoch 1190, training loss: 62.5870361328125 = 0.1312793493270874 + 10.0 * 6.245575904846191
Epoch 1190, val loss: 0.7834281921386719
Epoch 1200, training loss: 62.572139739990234 = 0.12741106748580933 + 10.0 * 6.244472980499268
Epoch 1200, val loss: 0.7856596112251282
Epoch 1210, training loss: 62.53868865966797 = 0.1236804649233818 + 10.0 * 6.2415008544921875
Epoch 1210, val loss: 0.7879833579063416
Epoch 1220, training loss: 62.54685592651367 = 0.12009602040052414 + 10.0 * 6.242676258087158
Epoch 1220, val loss: 0.7901957631111145
Epoch 1230, training loss: 62.512149810791016 = 0.11664384603500366 + 10.0 * 6.239550590515137
Epoch 1230, val loss: 0.7925840616226196
Epoch 1240, training loss: 62.52931213378906 = 0.1133088618516922 + 10.0 * 6.241600513458252
Epoch 1240, val loss: 0.7949880361557007
Epoch 1250, training loss: 62.54652786254883 = 0.11005817353725433 + 10.0 * 6.24364709854126
Epoch 1250, val loss: 0.797374963760376
Epoch 1260, training loss: 62.498680114746094 = 0.10692489147186279 + 10.0 * 6.239175319671631
Epoch 1260, val loss: 0.7998644709587097
Epoch 1270, training loss: 62.479286193847656 = 0.10389088094234467 + 10.0 * 6.237539768218994
Epoch 1270, val loss: 0.8023073673248291
Epoch 1280, training loss: 62.49052429199219 = 0.10099408775568008 + 10.0 * 6.238953113555908
Epoch 1280, val loss: 0.8048328161239624
Epoch 1290, training loss: 62.49689483642578 = 0.09816844016313553 + 10.0 * 6.239872932434082
Epoch 1290, val loss: 0.8074104189872742
Epoch 1300, training loss: 62.46549606323242 = 0.09541823714971542 + 10.0 * 6.237008094787598
Epoch 1300, val loss: 0.8100547790527344
Epoch 1310, training loss: 62.44785690307617 = 0.09275688230991364 + 10.0 * 6.235509872436523
Epoch 1310, val loss: 0.8125888705253601
Epoch 1320, training loss: 62.44221115112305 = 0.09021222591400146 + 10.0 * 6.235199928283691
Epoch 1320, val loss: 0.8152850866317749
Epoch 1330, training loss: 62.50593185424805 = 0.08775179833173752 + 10.0 * 6.241817951202393
Epoch 1330, val loss: 0.8181163668632507
Epoch 1340, training loss: 62.44216537475586 = 0.08531717211008072 + 10.0 * 6.235684871673584
Epoch 1340, val loss: 0.8206122517585754
Epoch 1350, training loss: 62.43134307861328 = 0.08297888189554214 + 10.0 * 6.234836578369141
Epoch 1350, val loss: 0.8235273957252502
Epoch 1360, training loss: 62.4682731628418 = 0.08072537928819656 + 10.0 * 6.238754749298096
Epoch 1360, val loss: 0.8259509801864624
Epoch 1370, training loss: 62.418914794921875 = 0.07854166626930237 + 10.0 * 6.234037399291992
Epoch 1370, val loss: 0.8288067579269409
Epoch 1380, training loss: 62.41047286987305 = 0.07642717659473419 + 10.0 * 6.233404636383057
Epoch 1380, val loss: 0.8314549922943115
Epoch 1390, training loss: 62.442562103271484 = 0.07439547777175903 + 10.0 * 6.236816883087158
Epoch 1390, val loss: 0.8342574834823608
Epoch 1400, training loss: 62.41985321044922 = 0.07241181284189224 + 10.0 * 6.234744071960449
Epoch 1400, val loss: 0.8369214534759521
Epoch 1410, training loss: 62.39607620239258 = 0.07048651576042175 + 10.0 * 6.232558727264404
Epoch 1410, val loss: 0.8394818305969238
Epoch 1420, training loss: 62.386940002441406 = 0.06863710284233093 + 10.0 * 6.23183012008667
Epoch 1420, val loss: 0.8421134948730469
Epoch 1430, training loss: 62.403236389160156 = 0.06685435026884079 + 10.0 * 6.233638286590576
Epoch 1430, val loss: 0.844723105430603
Epoch 1440, training loss: 62.37797927856445 = 0.06512811779975891 + 10.0 * 6.231285095214844
Epoch 1440, val loss: 0.8475946187973022
Epoch 1450, training loss: 62.39897537231445 = 0.06346691399812698 + 10.0 * 6.233551025390625
Epoch 1450, val loss: 0.8502277731895447
Epoch 1460, training loss: 62.378509521484375 = 0.06183614954352379 + 10.0 * 6.231667518615723
Epoch 1460, val loss: 0.8527507781982422
Epoch 1470, training loss: 62.36673355102539 = 0.060272928327322006 + 10.0 * 6.230646133422852
Epoch 1470, val loss: 0.8554587960243225
Epoch 1480, training loss: 62.371864318847656 = 0.058768823742866516 + 10.0 * 6.231309413909912
Epoch 1480, val loss: 0.8581171631813049
Epoch 1490, training loss: 62.35917282104492 = 0.0573003925383091 + 10.0 * 6.23018741607666
Epoch 1490, val loss: 0.8606740236282349
Epoch 1500, training loss: 62.345184326171875 = 0.055889666080474854 + 10.0 * 6.22892951965332
Epoch 1500, val loss: 0.863472044467926
Epoch 1510, training loss: 62.36085510253906 = 0.054525118321180344 + 10.0 * 6.23063325881958
Epoch 1510, val loss: 0.8661949038505554
Epoch 1520, training loss: 62.35904312133789 = 0.05320184677839279 + 10.0 * 6.230584144592285
Epoch 1520, val loss: 0.8689897060394287
Epoch 1530, training loss: 62.34557342529297 = 0.05190681293606758 + 10.0 * 6.229366779327393
Epoch 1530, val loss: 0.8715589642524719
Epoch 1540, training loss: 62.318885803222656 = 0.050658322870731354 + 10.0 * 6.226822853088379
Epoch 1540, val loss: 0.8745538592338562
Epoch 1550, training loss: 62.325477600097656 = 0.04945466294884682 + 10.0 * 6.227602481842041
Epoch 1550, val loss: 0.8771684169769287
Epoch 1560, training loss: 62.344173431396484 = 0.04829268157482147 + 10.0 * 6.229588031768799
Epoch 1560, val loss: 0.8799596428871155
Epoch 1570, training loss: 62.31791687011719 = 0.04715725779533386 + 10.0 * 6.227076053619385
Epoch 1570, val loss: 0.8828524947166443
Epoch 1580, training loss: 62.326744079589844 = 0.046060189604759216 + 10.0 * 6.2280683517456055
Epoch 1580, val loss: 0.88556307554245
Epoch 1590, training loss: 62.313720703125 = 0.044981956481933594 + 10.0 * 6.226873874664307
Epoch 1590, val loss: 0.8882544636726379
Epoch 1600, training loss: 62.31766891479492 = 0.043947383761405945 + 10.0 * 6.227372169494629
Epoch 1600, val loss: 0.8910514712333679
Epoch 1610, training loss: 62.29865646362305 = 0.0429464615881443 + 10.0 * 6.225571155548096
Epoch 1610, val loss: 0.8938202261924744
Epoch 1620, training loss: 62.28266906738281 = 0.0419747419655323 + 10.0 * 6.224069595336914
Epoch 1620, val loss: 0.8966841101646423
Epoch 1630, training loss: 62.330562591552734 = 0.04103941470384598 + 10.0 * 6.228952407836914
Epoch 1630, val loss: 0.8994208574295044
Epoch 1640, training loss: 62.310176849365234 = 0.040120821446180344 + 10.0 * 6.227005958557129
Epoch 1640, val loss: 0.9021978974342346
Epoch 1650, training loss: 62.29063034057617 = 0.03922538831830025 + 10.0 * 6.225140571594238
Epoch 1650, val loss: 0.9049393534660339
Epoch 1660, training loss: 62.27827072143555 = 0.03836460039019585 + 10.0 * 6.223990440368652
Epoch 1660, val loss: 0.90777987241745
Epoch 1670, training loss: 62.28097152709961 = 0.037539247423410416 + 10.0 * 6.224343299865723
Epoch 1670, val loss: 0.9105281233787537
Epoch 1680, training loss: 62.2800407409668 = 0.03673049807548523 + 10.0 * 6.224330902099609
Epoch 1680, val loss: 0.9133040308952332
Epoch 1690, training loss: 62.26776885986328 = 0.03594127297401428 + 10.0 * 6.223182678222656
Epoch 1690, val loss: 0.9159023761749268
Epoch 1700, training loss: 62.27444076538086 = 0.0351836159825325 + 10.0 * 6.223925590515137
Epoch 1700, val loss: 0.9186504483222961
Epoch 1710, training loss: 62.280555725097656 = 0.034445714205503464 + 10.0 * 6.224610805511475
Epoch 1710, val loss: 0.9212823510169983
Epoch 1720, training loss: 62.25387954711914 = 0.03372208774089813 + 10.0 * 6.222015857696533
Epoch 1720, val loss: 0.9241745471954346
Epoch 1730, training loss: 62.23418045043945 = 0.03302893415093422 + 10.0 * 6.2201151847839355
Epoch 1730, val loss: 0.9268109202384949
Epoch 1740, training loss: 62.24083709716797 = 0.03236363083124161 + 10.0 * 6.220847129821777
Epoch 1740, val loss: 0.9294562935829163
Epoch 1750, training loss: 62.344337463378906 = 0.031715597957372665 + 10.0 * 6.23126220703125
Epoch 1750, val loss: 0.9321249127388
Epoch 1760, training loss: 62.28406524658203 = 0.031063245609402657 + 10.0 * 6.225300312042236
Epoch 1760, val loss: 0.9346678853034973
Epoch 1770, training loss: 62.239009857177734 = 0.030438369140028954 + 10.0 * 6.2208571434021
Epoch 1770, val loss: 0.9373369216918945
Epoch 1780, training loss: 62.21919631958008 = 0.029834434390068054 + 10.0 * 6.218935966491699
Epoch 1780, val loss: 0.9400699138641357
Epoch 1790, training loss: 62.21528625488281 = 0.02925826795399189 + 10.0 * 6.218602657318115
Epoch 1790, val loss: 0.9427347779273987
Epoch 1800, training loss: 62.32344436645508 = 0.02869926579296589 + 10.0 * 6.2294745445251465
Epoch 1800, val loss: 0.9453839659690857
Epoch 1810, training loss: 62.263526916503906 = 0.028135977685451508 + 10.0 * 6.223538875579834
Epoch 1810, val loss: 0.9476751685142517
Epoch 1820, training loss: 62.24201965332031 = 0.027588820084929466 + 10.0 * 6.221443176269531
Epoch 1820, val loss: 0.9506138563156128
Epoch 1830, training loss: 62.21120834350586 = 0.027064567431807518 + 10.0 * 6.218414306640625
Epoch 1830, val loss: 0.9531947374343872
Epoch 1840, training loss: 62.21379470825195 = 0.026559554040431976 + 10.0 * 6.218723773956299
Epoch 1840, val loss: 0.9558165669441223
Epoch 1850, training loss: 62.21439743041992 = 0.026063350960612297 + 10.0 * 6.2188334465026855
Epoch 1850, val loss: 0.958389937877655
Epoch 1860, training loss: 62.22203826904297 = 0.025580141693353653 + 10.0 * 6.2196455001831055
Epoch 1860, val loss: 0.9607940912246704
Epoch 1870, training loss: 62.21870803833008 = 0.025109373033046722 + 10.0 * 6.219359874725342
Epoch 1870, val loss: 0.9634276032447815
Epoch 1880, training loss: 62.20320510864258 = 0.02464968152344227 + 10.0 * 6.217855453491211
Epoch 1880, val loss: 0.9661363363265991
Epoch 1890, training loss: 62.25918960571289 = 0.024202927947044373 + 10.0 * 6.223498344421387
Epoch 1890, val loss: 0.9685342311859131
Epoch 1900, training loss: 62.198951721191406 = 0.02375967986881733 + 10.0 * 6.217519283294678
Epoch 1900, val loss: 0.971023440361023
Epoch 1910, training loss: 62.18465805053711 = 0.023330477997660637 + 10.0 * 6.216132640838623
Epoch 1910, val loss: 0.9736509919166565
Epoch 1920, training loss: 62.178958892822266 = 0.022921524941921234 + 10.0 * 6.215603828430176
Epoch 1920, val loss: 0.9761550426483154
Epoch 1930, training loss: 62.207603454589844 = 0.02252642996609211 + 10.0 * 6.218507766723633
Epoch 1930, val loss: 0.9786915183067322
Epoch 1940, training loss: 62.19485855102539 = 0.022128835320472717 + 10.0 * 6.217272758483887
Epoch 1940, val loss: 0.9809775948524475
Epoch 1950, training loss: 62.17950439453125 = 0.021743224933743477 + 10.0 * 6.215775966644287
Epoch 1950, val loss: 0.9836626052856445
Epoch 1960, training loss: 62.17888259887695 = 0.02136891894042492 + 10.0 * 6.2157511711120605
Epoch 1960, val loss: 0.9861834645271301
Epoch 1970, training loss: 62.27167892456055 = 0.021011754870414734 + 10.0 * 6.225066661834717
Epoch 1970, val loss: 0.9887165427207947
Epoch 1980, training loss: 62.19136428833008 = 0.020640483126044273 + 10.0 * 6.217072486877441
Epoch 1980, val loss: 0.9908777475357056
Epoch 1990, training loss: 62.16179275512695 = 0.02029118314385414 + 10.0 * 6.214150428771973
Epoch 1990, val loss: 0.9935095906257629
Epoch 2000, training loss: 62.154239654541016 = 0.019953511655330658 + 10.0 * 6.213428497314453
Epoch 2000, val loss: 0.9959390163421631
Epoch 2010, training loss: 62.17914581298828 = 0.019630959257483482 + 10.0 * 6.215951442718506
Epoch 2010, val loss: 0.998156726360321
Epoch 2020, training loss: 62.17185974121094 = 0.01930464804172516 + 10.0 * 6.215255260467529
Epoch 2020, val loss: 1.0006461143493652
Epoch 2030, training loss: 62.161949157714844 = 0.018981561064720154 + 10.0 * 6.214296817779541
Epoch 2030, val loss: 1.0033117532730103
Epoch 2040, training loss: 62.175079345703125 = 0.018675560131669044 + 10.0 * 6.215640068054199
Epoch 2040, val loss: 1.0055642127990723
Epoch 2050, training loss: 62.16719436645508 = 0.01837047003209591 + 10.0 * 6.2148823738098145
Epoch 2050, val loss: 1.0080121755599976
Epoch 2060, training loss: 62.15056228637695 = 0.018077079206705093 + 10.0 * 6.213248252868652
Epoch 2060, val loss: 1.0103702545166016
Epoch 2070, training loss: 62.13625717163086 = 0.01778963766992092 + 10.0 * 6.211846828460693
Epoch 2070, val loss: 1.012884259223938
Epoch 2080, training loss: 62.14224624633789 = 0.017515044659376144 + 10.0 * 6.212473392486572
Epoch 2080, val loss: 1.0152236223220825
Epoch 2090, training loss: 62.21540451049805 = 0.017246967181563377 + 10.0 * 6.219815731048584
Epoch 2090, val loss: 1.0174448490142822
Epoch 2100, training loss: 62.1433219909668 = 0.01697136089205742 + 10.0 * 6.212635040283203
Epoch 2100, val loss: 1.0199594497680664
Epoch 2110, training loss: 62.125858306884766 = 0.01670837588608265 + 10.0 * 6.2109150886535645
Epoch 2110, val loss: 1.0223064422607422
Epoch 2120, training loss: 62.15810012817383 = 0.0164569690823555 + 10.0 * 6.2141642570495605
Epoch 2120, val loss: 1.0244641304016113
Epoch 2130, training loss: 62.12682342529297 = 0.01620551198720932 + 10.0 * 6.211061954498291
Epoch 2130, val loss: 1.0267137289047241
Epoch 2140, training loss: 62.13246154785156 = 0.015961455181241035 + 10.0 * 6.2116498947143555
Epoch 2140, val loss: 1.0290881395339966
Epoch 2150, training loss: 62.17499923706055 = 0.015724115073680878 + 10.0 * 6.215927600860596
Epoch 2150, val loss: 1.0312327146530151
Epoch 2160, training loss: 62.14788818359375 = 0.015491610392928123 + 10.0 * 6.213239669799805
Epoch 2160, val loss: 1.0337448120117188
Epoch 2170, training loss: 62.130157470703125 = 0.015259914100170135 + 10.0 * 6.211489677429199
Epoch 2170, val loss: 1.0360609292984009
Epoch 2180, training loss: 62.11444091796875 = 0.015037327073514462 + 10.0 * 6.209940433502197
Epoch 2180, val loss: 1.0382622480392456
Epoch 2190, training loss: 62.11067581176758 = 0.01482216827571392 + 10.0 * 6.209585189819336
Epoch 2190, val loss: 1.0404188632965088
Epoch 2200, training loss: 62.16347885131836 = 0.01461381372064352 + 10.0 * 6.214886665344238
Epoch 2200, val loss: 1.0424014329910278
Epoch 2210, training loss: 62.12694549560547 = 0.014402841217815876 + 10.0 * 6.211254119873047
Epoch 2210, val loss: 1.0450438261032104
Epoch 2220, training loss: 62.1478157043457 = 0.01419526431709528 + 10.0 * 6.213362216949463
Epoch 2220, val loss: 1.0469639301300049
Epoch 2230, training loss: 62.11256408691406 = 0.013991301879286766 + 10.0 * 6.209856986999512
Epoch 2230, val loss: 1.0492777824401855
Epoch 2240, training loss: 62.10575485229492 = 0.013793560676276684 + 10.0 * 6.209196090698242
Epoch 2240, val loss: 1.0513348579406738
Epoch 2250, training loss: 62.10813903808594 = 0.01360628753900528 + 10.0 * 6.209453105926514
Epoch 2250, val loss: 1.053573489189148
Epoch 2260, training loss: 62.12835693359375 = 0.013421613723039627 + 10.0 * 6.211493492126465
Epoch 2260, val loss: 1.0557327270507812
Epoch 2270, training loss: 62.09566116333008 = 0.013234097510576248 + 10.0 * 6.208242893218994
Epoch 2270, val loss: 1.058014988899231
Epoch 2280, training loss: 62.120086669921875 = 0.013055809773504734 + 10.0 * 6.210702896118164
Epoch 2280, val loss: 1.060113549232483
Epoch 2290, training loss: 62.1175422668457 = 0.012880595400929451 + 10.0 * 6.210465908050537
Epoch 2290, val loss: 1.0620355606079102
Epoch 2300, training loss: 62.11652755737305 = 0.012705088593065739 + 10.0 * 6.210381984710693
Epoch 2300, val loss: 1.0643835067749023
Epoch 2310, training loss: 62.10074996948242 = 0.012533015571534634 + 10.0 * 6.208821773529053
Epoch 2310, val loss: 1.066461443901062
Epoch 2320, training loss: 62.09845733642578 = 0.012368384748697281 + 10.0 * 6.208609104156494
Epoch 2320, val loss: 1.0684343576431274
Epoch 2330, training loss: 62.12879943847656 = 0.012205174192786217 + 10.0 * 6.2116594314575195
Epoch 2330, val loss: 1.0704680681228638
Epoch 2340, training loss: 62.09880447387695 = 0.012042475864291191 + 10.0 * 6.208676338195801
Epoch 2340, val loss: 1.0725940465927124
Epoch 2350, training loss: 62.08453369140625 = 0.011886918917298317 + 10.0 * 6.2072649002075195
Epoch 2350, val loss: 1.0746891498565674
Epoch 2360, training loss: 62.07453155517578 = 0.01173475943505764 + 10.0 * 6.206279754638672
Epoch 2360, val loss: 1.0767121315002441
Epoch 2370, training loss: 62.103702545166016 = 0.011587338522076607 + 10.0 * 6.209211349487305
Epoch 2370, val loss: 1.0785833597183228
Epoch 2380, training loss: 62.10706329345703 = 0.011438034474849701 + 10.0 * 6.2095627784729
Epoch 2380, val loss: 1.0808428525924683
Epoch 2390, training loss: 62.107215881347656 = 0.011292536742985249 + 10.0 * 6.209592342376709
Epoch 2390, val loss: 1.0827566385269165
Epoch 2400, training loss: 62.08124923706055 = 0.011148305609822273 + 10.0 * 6.207010269165039
Epoch 2400, val loss: 1.0848742723464966
Epoch 2410, training loss: 62.087181091308594 = 0.011011514812707901 + 10.0 * 6.207616806030273
Epoch 2410, val loss: 1.0867472887039185
Epoch 2420, training loss: 62.07439041137695 = 0.010873249731957912 + 10.0 * 6.2063517570495605
Epoch 2420, val loss: 1.088661551475525
Epoch 2430, training loss: 62.085025787353516 = 0.010742198675870895 + 10.0 * 6.207428455352783
Epoch 2430, val loss: 1.0906455516815186
Epoch 2440, training loss: 62.076393127441406 = 0.010610588826239109 + 10.0 * 6.206578254699707
Epoch 2440, val loss: 1.0926553010940552
Epoch 2450, training loss: 62.073394775390625 = 0.010483832098543644 + 10.0 * 6.206291198730469
Epoch 2450, val loss: 1.0947620868682861
Epoch 2460, training loss: 62.064632415771484 = 0.010358113795518875 + 10.0 * 6.205427646636963
Epoch 2460, val loss: 1.0965570211410522
Epoch 2470, training loss: 62.14692687988281 = 0.010238316841423512 + 10.0 * 6.2136688232421875
Epoch 2470, val loss: 1.0983535051345825
Epoch 2480, training loss: 62.07583999633789 = 0.010111230425536633 + 10.0 * 6.206572532653809
Epoch 2480, val loss: 1.1003392934799194
Epoch 2490, training loss: 62.05805969238281 = 0.009989745914936066 + 10.0 * 6.204806804656982
Epoch 2490, val loss: 1.1021673679351807
Epoch 2500, training loss: 62.07682800292969 = 0.009874464944005013 + 10.0 * 6.206695079803467
Epoch 2500, val loss: 1.104154109954834
Epoch 2510, training loss: 62.0670280456543 = 0.009757558815181255 + 10.0 * 6.2057271003723145
Epoch 2510, val loss: 1.105762004852295
Epoch 2520, training loss: 62.07408905029297 = 0.009644925594329834 + 10.0 * 6.20644474029541
Epoch 2520, val loss: 1.1076396703720093
Epoch 2530, training loss: 62.07730484008789 = 0.009533234871923923 + 10.0 * 6.206777095794678
Epoch 2530, val loss: 1.109485149383545
Epoch 2540, training loss: 62.04403305053711 = 0.009421355091035366 + 10.0 * 6.203461170196533
Epoch 2540, val loss: 1.11150062084198
Epoch 2550, training loss: 62.05030059814453 = 0.009316755458712578 + 10.0 * 6.204098701477051
Epoch 2550, val loss: 1.1133322715759277
Epoch 2560, training loss: 62.11648178100586 = 0.009211597964167595 + 10.0 * 6.210726737976074
Epoch 2560, val loss: 1.1150524616241455
Epoch 2570, training loss: 62.05009841918945 = 0.009104112163186073 + 10.0 * 6.204099178314209
Epoch 2570, val loss: 1.1167672872543335
Epoch 2580, training loss: 62.0307731628418 = 0.009002075530588627 + 10.0 * 6.202177047729492
Epoch 2580, val loss: 1.1187186241149902
Epoch 2590, training loss: 62.02667999267578 = 0.008904118090867996 + 10.0 * 6.201777458190918
Epoch 2590, val loss: 1.1205132007598877
Epoch 2600, training loss: 62.03163528442383 = 0.008810246363282204 + 10.0 * 6.202282428741455
Epoch 2600, val loss: 1.1223903894424438
Epoch 2610, training loss: 62.14323043823242 = 0.008718851022422314 + 10.0 * 6.213450908660889
Epoch 2610, val loss: 1.124269962310791
Epoch 2620, training loss: 62.1057014465332 = 0.00861663930118084 + 10.0 * 6.209708213806152
Epoch 2620, val loss: 1.125489592552185
Epoch 2630, training loss: 62.036720275878906 = 0.008520897477865219 + 10.0 * 6.20281982421875
Epoch 2630, val loss: 1.127751350402832
Epoch 2640, training loss: 62.02352523803711 = 0.00842870119959116 + 10.0 * 6.201509475708008
Epoch 2640, val loss: 1.1293354034423828
Epoch 2650, training loss: 62.022193908691406 = 0.008342359215021133 + 10.0 * 6.201385021209717
Epoch 2650, val loss: 1.1310840845108032
Epoch 2660, training loss: 62.061439514160156 = 0.008259361609816551 + 10.0 * 6.205317974090576
Epoch 2660, val loss: 1.1327037811279297
Epoch 2670, training loss: 62.060569763183594 = 0.008169637061655521 + 10.0 * 6.205239772796631
Epoch 2670, val loss: 1.1344094276428223
Epoch 2680, training loss: 62.01617431640625 = 0.008079808205366135 + 10.0 * 6.200809478759766
Epoch 2680, val loss: 1.1360468864440918
Epoch 2690, training loss: 62.01270294189453 = 0.007994327694177628 + 10.0 * 6.200470924377441
Epoch 2690, val loss: 1.1378616094589233
Epoch 2700, training loss: 62.01496124267578 = 0.007913793437182903 + 10.0 * 6.200704574584961
Epoch 2700, val loss: 1.1395853757858276
Epoch 2710, training loss: 62.07514953613281 = 0.007836082018911839 + 10.0 * 6.20673131942749
Epoch 2710, val loss: 1.1412153244018555
Epoch 2720, training loss: 62.016136169433594 = 0.00775271188467741 + 10.0 * 6.200838565826416
Epoch 2720, val loss: 1.1427974700927734
Epoch 2730, training loss: 62.0110969543457 = 0.007674002088606358 + 10.0 * 6.200342178344727
Epoch 2730, val loss: 1.1445685625076294
Epoch 2740, training loss: 62.03850173950195 = 0.0075995740480721 + 10.0 * 6.203090190887451
Epoch 2740, val loss: 1.1461737155914307
Epoch 2750, training loss: 62.02030944824219 = 0.007522645406424999 + 10.0 * 6.2012786865234375
Epoch 2750, val loss: 1.1477487087249756
Epoch 2760, training loss: 62.02635955810547 = 0.007447816431522369 + 10.0 * 6.2018914222717285
Epoch 2760, val loss: 1.1494617462158203
Epoch 2770, training loss: 62.03360366821289 = 0.007375393528491259 + 10.0 * 6.202622890472412
Epoch 2770, val loss: 1.151192545890808
Epoch 2780, training loss: 62.03042984008789 = 0.007301825564354658 + 10.0 * 6.20231294631958
Epoch 2780, val loss: 1.152659296989441
Epoch 2790, training loss: 62.012481689453125 = 0.007229298818856478 + 10.0 * 6.200525283813477
Epoch 2790, val loss: 1.1542067527770996
Epoch 2800, training loss: 62.004512786865234 = 0.0071598924696445465 + 10.0 * 6.199735164642334
Epoch 2800, val loss: 1.1558201313018799
Epoch 2810, training loss: 61.995792388916016 = 0.0070919436402618885 + 10.0 * 6.1988701820373535
Epoch 2810, val loss: 1.1573736667633057
Epoch 2820, training loss: 62.026878356933594 = 0.0070275720208883286 + 10.0 * 6.2019853591918945
Epoch 2820, val loss: 1.1591380834579468
Epoch 2830, training loss: 62.0155143737793 = 0.006958840414881706 + 10.0 * 6.200855731964111
Epoch 2830, val loss: 1.1604524850845337
Epoch 2840, training loss: 62.0113639831543 = 0.0068910736590623856 + 10.0 * 6.2004475593566895
Epoch 2840, val loss: 1.161842703819275
Epoch 2850, training loss: 62.01880645751953 = 0.006826876662671566 + 10.0 * 6.201197624206543
Epoch 2850, val loss: 1.163386583328247
Epoch 2860, training loss: 61.99505615234375 = 0.006762208882719278 + 10.0 * 6.198829174041748
Epoch 2860, val loss: 1.1650162935256958
Epoch 2870, training loss: 62.01964569091797 = 0.006702505983412266 + 10.0 * 6.201294422149658
Epoch 2870, val loss: 1.1665011644363403
Epoch 2880, training loss: 62.01583480834961 = 0.00664124172180891 + 10.0 * 6.200919151306152
Epoch 2880, val loss: 1.167900562286377
Epoch 2890, training loss: 62.01320266723633 = 0.006578385829925537 + 10.0 * 6.200662612915039
Epoch 2890, val loss: 1.169465184211731
Epoch 2900, training loss: 61.99080276489258 = 0.006516937632113695 + 10.0 * 6.198428630828857
Epoch 2900, val loss: 1.1710941791534424
Epoch 2910, training loss: 61.9878044128418 = 0.006459859199821949 + 10.0 * 6.198134422302246
Epoch 2910, val loss: 1.1726278066635132
Epoch 2920, training loss: 62.0112419128418 = 0.006403597071766853 + 10.0 * 6.200483798980713
Epoch 2920, val loss: 1.1740319728851318
Epoch 2930, training loss: 62.00629425048828 = 0.006345794070512056 + 10.0 * 6.1999945640563965
Epoch 2930, val loss: 1.175431251525879
Epoch 2940, training loss: 62.00347900390625 = 0.0062888916581869125 + 10.0 * 6.199718952178955
Epoch 2940, val loss: 1.17684805393219
Epoch 2950, training loss: 62.00420379638672 = 0.006233589258044958 + 10.0 * 6.1997971534729
Epoch 2950, val loss: 1.1785072088241577
Epoch 2960, training loss: 62.01531219482422 = 0.0061795953661203384 + 10.0 * 6.200913429260254
Epoch 2960, val loss: 1.179988980293274
Epoch 2970, training loss: 61.980045318603516 = 0.006124190520495176 + 10.0 * 6.197392463684082
Epoch 2970, val loss: 1.181247591972351
Epoch 2980, training loss: 61.97298812866211 = 0.006072462536394596 + 10.0 * 6.196691513061523
Epoch 2980, val loss: 1.1827805042266846
Epoch 2990, training loss: 62.0009651184082 = 0.006023772992193699 + 10.0 * 6.199494361877441
Epoch 2990, val loss: 1.1842290163040161
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 87.91484069824219 = 1.9465783834457397 + 10.0 * 8.596826553344727
Epoch 0, val loss: 1.9513734579086304
Epoch 10, training loss: 87.89842224121094 = 1.9370442628860474 + 10.0 * 8.596138000488281
Epoch 10, val loss: 1.941961646080017
Epoch 20, training loss: 87.8353042602539 = 1.925531029701233 + 10.0 * 8.590977668762207
Epoch 20, val loss: 1.9300804138183594
Epoch 30, training loss: 87.4456787109375 = 1.9108474254608154 + 10.0 * 8.553483009338379
Epoch 30, val loss: 1.9148821830749512
Epoch 40, training loss: 84.93450164794922 = 1.8934412002563477 + 10.0 * 8.304105758666992
Epoch 40, val loss: 1.8974816799163818
Epoch 50, training loss: 77.59989929199219 = 1.8744405508041382 + 10.0 * 7.572545528411865
Epoch 50, val loss: 1.879281997680664
Epoch 60, training loss: 74.54859161376953 = 1.8599475622177124 + 10.0 * 7.268864631652832
Epoch 60, val loss: 1.8656985759735107
Epoch 70, training loss: 72.29576110839844 = 1.8472858667373657 + 10.0 * 7.0448479652404785
Epoch 70, val loss: 1.8533881902694702
Epoch 80, training loss: 70.90280151367188 = 1.835526466369629 + 10.0 * 6.906727313995361
Epoch 80, val loss: 1.8423714637756348
Epoch 90, training loss: 69.81938934326172 = 1.8239269256591797 + 10.0 * 6.799546241760254
Epoch 90, val loss: 1.8313531875610352
Epoch 100, training loss: 69.19384765625 = 1.8124797344207764 + 10.0 * 6.7381367683410645
Epoch 100, val loss: 1.820473551750183
Epoch 110, training loss: 68.68311309814453 = 1.8012285232543945 + 10.0 * 6.688188552856445
Epoch 110, val loss: 1.809492588043213
Epoch 120, training loss: 68.29136657714844 = 1.7903478145599365 + 10.0 * 6.650102138519287
Epoch 120, val loss: 1.7993745803833008
Epoch 130, training loss: 67.95986938476562 = 1.7800447940826416 + 10.0 * 6.617982387542725
Epoch 130, val loss: 1.7898741960525513
Epoch 140, training loss: 67.67786407470703 = 1.7700942754745483 + 10.0 * 6.5907769203186035
Epoch 140, val loss: 1.7808115482330322
Epoch 150, training loss: 67.34330749511719 = 1.7599270343780518 + 10.0 * 6.558338165283203
Epoch 150, val loss: 1.7718021869659424
Epoch 160, training loss: 67.06817626953125 = 1.7492854595184326 + 10.0 * 6.531888961791992
Epoch 160, val loss: 1.76260244846344
Epoch 170, training loss: 66.88091278076172 = 1.7377772331237793 + 10.0 * 6.514313697814941
Epoch 170, val loss: 1.7528144121170044
Epoch 180, training loss: 66.67646789550781 = 1.7251007556915283 + 10.0 * 6.495136737823486
Epoch 180, val loss: 1.7419891357421875
Epoch 190, training loss: 66.50759887695312 = 1.7112172842025757 + 10.0 * 6.479637622833252
Epoch 190, val loss: 1.7303344011306763
Epoch 200, training loss: 66.36835479736328 = 1.696142315864563 + 10.0 * 6.467220783233643
Epoch 200, val loss: 1.7177773714065552
Epoch 210, training loss: 66.2349624633789 = 1.6797837018966675 + 10.0 * 6.455517768859863
Epoch 210, val loss: 1.7042702436447144
Epoch 220, training loss: 66.11209106445312 = 1.662044644355774 + 10.0 * 6.445004463195801
Epoch 220, val loss: 1.6898037195205688
Epoch 230, training loss: 65.9950180053711 = 1.642771601676941 + 10.0 * 6.435225009918213
Epoch 230, val loss: 1.6741888523101807
Epoch 240, training loss: 65.86390686035156 = 1.6221115589141846 + 10.0 * 6.424180030822754
Epoch 240, val loss: 1.6573888063430786
Epoch 250, training loss: 65.76239776611328 = 1.5999177694320679 + 10.0 * 6.416248321533203
Epoch 250, val loss: 1.639400839805603
Epoch 260, training loss: 65.67329406738281 = 1.576050043106079 + 10.0 * 6.409724235534668
Epoch 260, val loss: 1.6201006174087524
Epoch 270, training loss: 65.57264709472656 = 1.5507758855819702 + 10.0 * 6.402187347412109
Epoch 270, val loss: 1.5997580289840698
Epoch 280, training loss: 65.4679946899414 = 1.5241038799285889 + 10.0 * 6.3943891525268555
Epoch 280, val loss: 1.5785106420516968
Epoch 290, training loss: 65.39244842529297 = 1.496208667755127 + 10.0 * 6.389623641967773
Epoch 290, val loss: 1.5563884973526
Epoch 300, training loss: 65.2950668334961 = 1.467172384262085 + 10.0 * 6.382789134979248
Epoch 300, val loss: 1.5334713459014893
Epoch 310, training loss: 65.21726989746094 = 1.437196135520935 + 10.0 * 6.378007411956787
Epoch 310, val loss: 1.510066032409668
Epoch 320, training loss: 65.15789794921875 = 1.406504511833191 + 10.0 * 6.375139236450195
Epoch 320, val loss: 1.4862016439437866
Epoch 330, training loss: 65.04994201660156 = 1.3751863241195679 + 10.0 * 6.367475986480713
Epoch 330, val loss: 1.4621953964233398
Epoch 340, training loss: 64.9620361328125 = 1.3435616493225098 + 10.0 * 6.361847400665283
Epoch 340, val loss: 1.438192367553711
Epoch 350, training loss: 64.97207641601562 = 1.3116910457611084 + 10.0 * 6.3660383224487305
Epoch 350, val loss: 1.4142019748687744
Epoch 360, training loss: 64.83385467529297 = 1.279589056968689 + 10.0 * 6.355426788330078
Epoch 360, val loss: 1.3903433084487915
Epoch 370, training loss: 64.7489013671875 = 1.2477076053619385 + 10.0 * 6.350119113922119
Epoch 370, val loss: 1.3668615818023682
Epoch 380, training loss: 64.67359924316406 = 1.216052532196045 + 10.0 * 6.345755100250244
Epoch 380, val loss: 1.3438737392425537
Epoch 390, training loss: 64.67913818359375 = 1.1845448017120361 + 10.0 * 6.349459171295166
Epoch 390, val loss: 1.3212658166885376
Epoch 400, training loss: 64.5704345703125 = 1.153440237045288 + 10.0 * 6.341699600219727
Epoch 400, val loss: 1.2990796566009521
Epoch 410, training loss: 64.48196411132812 = 1.122823715209961 + 10.0 * 6.335914134979248
Epoch 410, val loss: 1.2776587009429932
Epoch 420, training loss: 64.41876220703125 = 1.0928462743759155 + 10.0 * 6.332591533660889
Epoch 420, val loss: 1.256945252418518
Epoch 430, training loss: 64.37757873535156 = 1.0633769035339355 + 10.0 * 6.331420421600342
Epoch 430, val loss: 1.2369097471237183
Epoch 440, training loss: 64.38034057617188 = 1.034340739250183 + 10.0 * 6.33459997177124
Epoch 440, val loss: 1.217642068862915
Epoch 450, training loss: 64.25426483154297 = 1.0061308145523071 + 10.0 * 6.324812889099121
Epoch 450, val loss: 1.1989578008651733
Epoch 460, training loss: 64.19844055175781 = 0.9786490797996521 + 10.0 * 6.32197904586792
Epoch 460, val loss: 1.1811527013778687
Epoch 470, training loss: 64.14824676513672 = 0.9519273042678833 + 10.0 * 6.319631576538086
Epoch 470, val loss: 1.1641393899917603
Epoch 480, training loss: 64.10830688476562 = 0.9259122610092163 + 10.0 * 6.318239688873291
Epoch 480, val loss: 1.1479742527008057
Epoch 490, training loss: 64.05845642089844 = 0.9006223082542419 + 10.0 * 6.315783500671387
Epoch 490, val loss: 1.1325738430023193
Epoch 500, training loss: 64.03163146972656 = 0.8760480880737305 + 10.0 * 6.315558433532715
Epoch 500, val loss: 1.1177082061767578
Epoch 510, training loss: 63.96840286254883 = 0.8522926568984985 + 10.0 * 6.311611175537109
Epoch 510, val loss: 1.1039276123046875
Epoch 520, training loss: 63.91679763793945 = 0.8294404745101929 + 10.0 * 6.3087358474731445
Epoch 520, val loss: 1.0909957885742188
Epoch 530, training loss: 63.93280029296875 = 0.8072803020477295 + 10.0 * 6.312551975250244
Epoch 530, val loss: 1.0789008140563965
Epoch 540, training loss: 63.85503005981445 = 0.7858186960220337 + 10.0 * 6.306921005249023
Epoch 540, val loss: 1.0673295259475708
Epoch 550, training loss: 63.80499267578125 = 0.7650628685951233 + 10.0 * 6.303992748260498
Epoch 550, val loss: 1.056542158126831
Epoch 560, training loss: 63.77975082397461 = 0.7449476718902588 + 10.0 * 6.30348014831543
Epoch 560, val loss: 1.046550989151001
Epoch 570, training loss: 63.72392272949219 = 0.7254383563995361 + 10.0 * 6.299848556518555
Epoch 570, val loss: 1.0370023250579834
Epoch 580, training loss: 63.693206787109375 = 0.7064972519874573 + 10.0 * 6.298670768737793
Epoch 580, val loss: 1.0281177759170532
Epoch 590, training loss: 63.65084457397461 = 0.6880006194114685 + 10.0 * 6.2962846755981445
Epoch 590, val loss: 1.0197681188583374
Epoch 600, training loss: 63.65180587768555 = 0.6699419021606445 + 10.0 * 6.298186302185059
Epoch 600, val loss: 1.0118119716644287
Epoch 610, training loss: 63.62958526611328 = 0.6522359848022461 + 10.0 * 6.29773473739624
Epoch 610, val loss: 1.0042805671691895
Epoch 620, training loss: 63.55598449707031 = 0.6349356174468994 + 10.0 * 6.292104721069336
Epoch 620, val loss: 0.9970188736915588
Epoch 630, training loss: 63.523502349853516 = 0.6181239485740662 + 10.0 * 6.2905378341674805
Epoch 630, val loss: 0.9904894828796387
Epoch 640, training loss: 63.521018981933594 = 0.6016632318496704 + 10.0 * 6.291935920715332
Epoch 640, val loss: 0.9843355417251587
Epoch 650, training loss: 63.51350021362305 = 0.5854728817939758 + 10.0 * 6.292802810668945
Epoch 650, val loss: 0.9781108498573303
Epoch 660, training loss: 63.43845748901367 = 0.5696219205856323 + 10.0 * 6.286883354187012
Epoch 660, val loss: 0.972659170627594
Epoch 670, training loss: 63.4006233215332 = 0.5542260408401489 + 10.0 * 6.284639835357666
Epoch 670, val loss: 0.9676406383514404
Epoch 680, training loss: 63.374481201171875 = 0.5391770005226135 + 10.0 * 6.283530235290527
Epoch 680, val loss: 0.9629473686218262
Epoch 690, training loss: 63.35737609863281 = 0.5244241952896118 + 10.0 * 6.283295154571533
Epoch 690, val loss: 0.9586230516433716
Epoch 700, training loss: 63.32200241088867 = 0.5098784565925598 + 10.0 * 6.281212329864502
Epoch 700, val loss: 0.9544814229011536
Epoch 710, training loss: 63.311790466308594 = 0.4956320822238922 + 10.0 * 6.281615734100342
Epoch 710, val loss: 0.9506585001945496
Epoch 720, training loss: 63.27435302734375 = 0.4817322790622711 + 10.0 * 6.279262065887451
Epoch 720, val loss: 0.9472764730453491
Epoch 730, training loss: 63.278377532958984 = 0.4681127071380615 + 10.0 * 6.281026363372803
Epoch 730, val loss: 0.9442261457443237
Epoch 740, training loss: 63.2279167175293 = 0.45475372672080994 + 10.0 * 6.277316093444824
Epoch 740, val loss: 0.9414597153663635
Epoch 750, training loss: 63.22665786743164 = 0.4417194426059723 + 10.0 * 6.278493881225586
Epoch 750, val loss: 0.9390475749969482
Epoch 760, training loss: 63.17634201049805 = 0.42894265055656433 + 10.0 * 6.274739742279053
Epoch 760, val loss: 0.9369540214538574
Epoch 770, training loss: 63.1706428527832 = 0.41647791862487793 + 10.0 * 6.275416374206543
Epoch 770, val loss: 0.9352650046348572
Epoch 780, training loss: 63.14379119873047 = 0.4042833745479584 + 10.0 * 6.273951053619385
Epoch 780, val loss: 0.9337580800056458
Epoch 790, training loss: 63.120094299316406 = 0.39240118861198425 + 10.0 * 6.272768974304199
Epoch 790, val loss: 0.9327020645141602
Epoch 800, training loss: 63.1195182800293 = 0.3807012438774109 + 10.0 * 6.273881435394287
Epoch 800, val loss: 0.9316574931144714
Epoch 810, training loss: 63.07250213623047 = 0.36930686235427856 + 10.0 * 6.27031946182251
Epoch 810, val loss: 0.9309475421905518
Epoch 820, training loss: 63.054847717285156 = 0.3581961691379547 + 10.0 * 6.269665241241455
Epoch 820, val loss: 0.9304272532463074
Epoch 830, training loss: 63.0339469909668 = 0.34742510318756104 + 10.0 * 6.268652439117432
Epoch 830, val loss: 0.9302909970283508
Epoch 840, training loss: 63.055389404296875 = 0.33688515424728394 + 10.0 * 6.2718505859375
Epoch 840, val loss: 0.9302124381065369
Epoch 850, training loss: 63.07741928100586 = 0.32649773359298706 + 10.0 * 6.275092124938965
Epoch 850, val loss: 0.9303826689720154
Epoch 860, training loss: 62.98088455200195 = 0.31651440262794495 + 10.0 * 6.26643705368042
Epoch 860, val loss: 0.9307840466499329
Epoch 870, training loss: 62.957767486572266 = 0.3067769408226013 + 10.0 * 6.265099048614502
Epoch 870, val loss: 0.9312984347343445
Epoch 880, training loss: 62.93603515625 = 0.29736974835395813 + 10.0 * 6.263866424560547
Epoch 880, val loss: 0.9323193430900574
Epoch 890, training loss: 62.94485855102539 = 0.28822630643844604 + 10.0 * 6.265663146972656
Epoch 890, val loss: 0.9333147406578064
Epoch 900, training loss: 62.93312454223633 = 0.2792348861694336 + 10.0 * 6.2653889656066895
Epoch 900, val loss: 0.9342697262763977
Epoch 910, training loss: 62.89069747924805 = 0.27053675055503845 + 10.0 * 6.2620158195495605
Epoch 910, val loss: 0.9354389309883118
Epoch 920, training loss: 62.87275695800781 = 0.2621539831161499 + 10.0 * 6.2610602378845215
Epoch 920, val loss: 0.9369354248046875
Epoch 930, training loss: 62.85097122192383 = 0.2540602684020996 + 10.0 * 6.25969123840332
Epoch 930, val loss: 0.9386810660362244
Epoch 940, training loss: 62.86963653564453 = 0.24621717631816864 + 10.0 * 6.2623419761657715
Epoch 940, val loss: 0.9405664205551147
Epoch 950, training loss: 62.85076141357422 = 0.23846419155597687 + 10.0 * 6.261229515075684
Epoch 950, val loss: 0.9419748187065125
Epoch 960, training loss: 62.82034683227539 = 0.23100495338439941 + 10.0 * 6.258934020996094
Epoch 960, val loss: 0.9441834688186646
Epoch 970, training loss: 62.79609298706055 = 0.22383341193199158 + 10.0 * 6.25722599029541
Epoch 970, val loss: 0.946490466594696
Epoch 980, training loss: 62.78033447265625 = 0.2168973982334137 + 10.0 * 6.256343841552734
Epoch 980, val loss: 0.9488333463668823
Epoch 990, training loss: 62.82417297363281 = 0.2102043181657791 + 10.0 * 6.261396884918213
Epoch 990, val loss: 0.9513012766838074
Epoch 1000, training loss: 62.804500579833984 = 0.203635111451149 + 10.0 * 6.260086536407471
Epoch 1000, val loss: 0.9538937211036682
Epoch 1010, training loss: 62.74300765991211 = 0.1973021924495697 + 10.0 * 6.254570484161377
Epoch 1010, val loss: 0.9565075039863586
Epoch 1020, training loss: 62.73908615112305 = 0.1912274807691574 + 10.0 * 6.254786014556885
Epoch 1020, val loss: 0.9593836069107056
Epoch 1030, training loss: 62.770538330078125 = 0.18535955250263214 + 10.0 * 6.258517742156982
Epoch 1030, val loss: 0.9624046087265015
Epoch 1040, training loss: 62.7249755859375 = 0.17962074279785156 + 10.0 * 6.254535675048828
Epoch 1040, val loss: 0.9657025337219238
Epoch 1050, training loss: 62.70114517211914 = 0.1740996539592743 + 10.0 * 6.252704620361328
Epoch 1050, val loss: 0.9688361287117004
Epoch 1060, training loss: 62.686283111572266 = 0.1687876284122467 + 10.0 * 6.251749515533447
Epoch 1060, val loss: 0.9723527431488037
Epoch 1070, training loss: 62.715145111083984 = 0.16364003717899323 + 10.0 * 6.25515079498291
Epoch 1070, val loss: 0.9757825136184692
Epoch 1080, training loss: 62.68049621582031 = 0.15864305198192596 + 10.0 * 6.252185344696045
Epoch 1080, val loss: 0.9791963696479797
Epoch 1090, training loss: 62.680946350097656 = 0.15378710627555847 + 10.0 * 6.252716064453125
Epoch 1090, val loss: 0.9827834367752075
Epoch 1100, training loss: 62.65970230102539 = 0.14914341270923615 + 10.0 * 6.251055717468262
Epoch 1100, val loss: 0.9865077137947083
Epoch 1110, training loss: 62.630592346191406 = 0.14466626942157745 + 10.0 * 6.248592853546143
Epoch 1110, val loss: 0.9902531504631042
Epoch 1120, training loss: 62.6264533996582 = 0.14035961031913757 + 10.0 * 6.24860954284668
Epoch 1120, val loss: 0.9943368434906006
Epoch 1130, training loss: 62.66697692871094 = 0.1361970752477646 + 10.0 * 6.253077983856201
Epoch 1130, val loss: 0.9982087016105652
Epoch 1140, training loss: 62.61127853393555 = 0.13208888471126556 + 10.0 * 6.247919082641602
Epoch 1140, val loss: 1.0023653507232666
Epoch 1150, training loss: 62.599830627441406 = 0.12819211184978485 + 10.0 * 6.247163772583008
Epoch 1150, val loss: 1.006541132926941
Epoch 1160, training loss: 62.59963607788086 = 0.12441381812095642 + 10.0 * 6.247522354125977
Epoch 1160, val loss: 1.0107228755950928
Epoch 1170, training loss: 62.58287048339844 = 0.12074873596429825 + 10.0 * 6.246212005615234
Epoch 1170, val loss: 1.014877438545227
Epoch 1180, training loss: 62.57735824584961 = 0.1172366738319397 + 10.0 * 6.246012210845947
Epoch 1180, val loss: 1.0191878080368042
Epoch 1190, training loss: 62.59709167480469 = 0.11382424831390381 + 10.0 * 6.248326778411865
Epoch 1190, val loss: 1.0237096548080444
Epoch 1200, training loss: 62.58125686645508 = 0.11050611734390259 + 10.0 * 6.247075080871582
Epoch 1200, val loss: 1.0280437469482422
Epoch 1210, training loss: 62.54557800292969 = 0.10731734335422516 + 10.0 * 6.243825912475586
Epoch 1210, val loss: 1.0324467420578003
Epoch 1220, training loss: 62.53895950317383 = 0.10426390171051025 + 10.0 * 6.243469715118408
Epoch 1220, val loss: 1.0370500087738037
Epoch 1230, training loss: 62.53631591796875 = 0.10131030529737473 + 10.0 * 6.243500709533691
Epoch 1230, val loss: 1.0417659282684326
Epoch 1240, training loss: 62.56411361694336 = 0.09844641387462616 + 10.0 * 6.2465667724609375
Epoch 1240, val loss: 1.046427845954895
Epoch 1250, training loss: 62.52105712890625 = 0.09567132592201233 + 10.0 * 6.2425384521484375
Epoch 1250, val loss: 1.0510592460632324
Epoch 1260, training loss: 62.50678253173828 = 0.09299904108047485 + 10.0 * 6.241378307342529
Epoch 1260, val loss: 1.0556732416152954
Epoch 1270, training loss: 62.49656677246094 = 0.09043125063180923 + 10.0 * 6.2406134605407715
Epoch 1270, val loss: 1.0606344938278198
Epoch 1280, training loss: 62.50592041015625 = 0.0879717767238617 + 10.0 * 6.241795063018799
Epoch 1280, val loss: 1.0654470920562744
Epoch 1290, training loss: 62.49036407470703 = 0.0855582132935524 + 10.0 * 6.240480422973633
Epoch 1290, val loss: 1.070353388786316
Epoch 1300, training loss: 62.48758316040039 = 0.08323610574007034 + 10.0 * 6.240434646606445
Epoch 1300, val loss: 1.0752040147781372
Epoch 1310, training loss: 62.48044967651367 = 0.08100482821464539 + 10.0 * 6.2399444580078125
Epoch 1310, val loss: 1.0801403522491455
Epoch 1320, training loss: 62.49258041381836 = 0.07883434742689133 + 10.0 * 6.241374492645264
Epoch 1320, val loss: 1.084796667098999
Epoch 1330, training loss: 62.48090362548828 = 0.07669927924871445 + 10.0 * 6.240420341491699
Epoch 1330, val loss: 1.0898593664169312
Epoch 1340, training loss: 62.44540023803711 = 0.0746748298406601 + 10.0 * 6.237072467803955
Epoch 1340, val loss: 1.0946552753448486
Epoch 1350, training loss: 62.4368782043457 = 0.07273388653993607 + 10.0 * 6.236414432525635
Epoch 1350, val loss: 1.0999120473861694
Epoch 1360, training loss: 62.430023193359375 = 0.07086452841758728 + 10.0 * 6.235915660858154
Epoch 1360, val loss: 1.1048613786697388
Epoch 1370, training loss: 62.47678756713867 = 0.06905816495418549 + 10.0 * 6.2407732009887695
Epoch 1370, val loss: 1.1099371910095215
Epoch 1380, training loss: 62.454647064208984 = 0.06727651506662369 + 10.0 * 6.238737106323242
Epoch 1380, val loss: 1.1145129203796387
Epoch 1390, training loss: 62.447078704833984 = 0.06554786115884781 + 10.0 * 6.238152980804443
Epoch 1390, val loss: 1.1195733547210693
Epoch 1400, training loss: 62.419368743896484 = 0.06389316916465759 + 10.0 * 6.2355475425720215
Epoch 1400, val loss: 1.1245622634887695
Epoch 1410, training loss: 62.41313934326172 = 0.06230814382433891 + 10.0 * 6.235083103179932
Epoch 1410, val loss: 1.1296008825302124
Epoch 1420, training loss: 62.4242057800293 = 0.06076793000102043 + 10.0 * 6.236343860626221
Epoch 1420, val loss: 1.134640097618103
Epoch 1430, training loss: 62.40439987182617 = 0.05927363410592079 + 10.0 * 6.234512805938721
Epoch 1430, val loss: 1.1394720077514648
Epoch 1440, training loss: 62.403629302978516 = 0.05783599987626076 + 10.0 * 6.234579563140869
Epoch 1440, val loss: 1.1445671319961548
Epoch 1450, training loss: 62.41053771972656 = 0.05644378066062927 + 10.0 * 6.235409736633301
Epoch 1450, val loss: 1.1494998931884766
Epoch 1460, training loss: 62.44116973876953 = 0.05507843196392059 + 10.0 * 6.238609313964844
Epoch 1460, val loss: 1.1543240547180176
Epoch 1470, training loss: 62.38925552368164 = 0.053738534450531006 + 10.0 * 6.233551979064941
Epoch 1470, val loss: 1.1594007015228271
Epoch 1480, training loss: 62.3696174621582 = 0.05247177556157112 + 10.0 * 6.231714725494385
Epoch 1480, val loss: 1.1643222570419312
Epoch 1490, training loss: 62.36490249633789 = 0.051249776035547256 + 10.0 * 6.231365203857422
Epoch 1490, val loss: 1.1694527864456177
Epoch 1500, training loss: 62.42537307739258 = 0.05006152018904686 + 10.0 * 6.2375311851501465
Epoch 1500, val loss: 1.1743425130844116
Epoch 1510, training loss: 62.36238098144531 = 0.048900503665208817 + 10.0 * 6.231348037719727
Epoch 1510, val loss: 1.1789841651916504
Epoch 1520, training loss: 62.34679412841797 = 0.047775935381650925 + 10.0 * 6.2299017906188965
Epoch 1520, val loss: 1.1840617656707764
Epoch 1530, training loss: 62.35718536376953 = 0.04669856280088425 + 10.0 * 6.231048583984375
Epoch 1530, val loss: 1.1889898777008057
Epoch 1540, training loss: 62.3765869140625 = 0.04564068838953972 + 10.0 * 6.233094692230225
Epoch 1540, val loss: 1.193868637084961
Epoch 1550, training loss: 62.34391784667969 = 0.044617123901844025 + 10.0 * 6.2299299240112305
Epoch 1550, val loss: 1.1987882852554321
Epoch 1560, training loss: 62.34479522705078 = 0.043625228106975555 + 10.0 * 6.230116844177246
Epoch 1560, val loss: 1.2036584615707397
Epoch 1570, training loss: 62.35017776489258 = 0.04266449064016342 + 10.0 * 6.2307515144348145
Epoch 1570, val loss: 1.2085483074188232
Epoch 1580, training loss: 62.37104034423828 = 0.04172937572002411 + 10.0 * 6.232931137084961
Epoch 1580, val loss: 1.2135274410247803
Epoch 1590, training loss: 62.32860565185547 = 0.0408177524805069 + 10.0 * 6.228778839111328
Epoch 1590, val loss: 1.2181215286254883
Epoch 1600, training loss: 62.319393157958984 = 0.039950743317604065 + 10.0 * 6.227944374084473
Epoch 1600, val loss: 1.2231297492980957
Epoch 1610, training loss: 62.317710876464844 = 0.03910868614912033 + 10.0 * 6.227860450744629
Epoch 1610, val loss: 1.2278289794921875
Epoch 1620, training loss: 62.33999252319336 = 0.038288962095975876 + 10.0 * 6.230170249938965
Epoch 1620, val loss: 1.2327673435211182
Epoch 1630, training loss: 62.35818862915039 = 0.037471748888492584 + 10.0 * 6.232071876525879
Epoch 1630, val loss: 1.237383246421814
Epoch 1640, training loss: 62.314212799072266 = 0.03669138252735138 + 10.0 * 6.227752208709717
Epoch 1640, val loss: 1.241896390914917
Epoch 1650, training loss: 62.2971305847168 = 0.0359361469745636 + 10.0 * 6.226119518280029
Epoch 1650, val loss: 1.246753454208374
Epoch 1660, training loss: 62.29355239868164 = 0.035216838121414185 + 10.0 * 6.225833415985107
Epoch 1660, val loss: 1.2515382766723633
Epoch 1670, training loss: 62.355037689208984 = 0.03451642021536827 + 10.0 * 6.232052326202393
Epoch 1670, val loss: 1.2561780214309692
Epoch 1680, training loss: 62.303585052490234 = 0.03380528464913368 + 10.0 * 6.226977825164795
Epoch 1680, val loss: 1.2608451843261719
Epoch 1690, training loss: 62.287025451660156 = 0.03313027694821358 + 10.0 * 6.22538948059082
Epoch 1690, val loss: 1.2654297351837158
Epoch 1700, training loss: 62.297332763671875 = 0.03247799351811409 + 10.0 * 6.226485252380371
Epoch 1700, val loss: 1.2700116634368896
Epoch 1710, training loss: 62.27922058105469 = 0.03184473142027855 + 10.0 * 6.224737644195557
Epoch 1710, val loss: 1.2747557163238525
Epoch 1720, training loss: 62.275569915771484 = 0.031230386346578598 + 10.0 * 6.224433898925781
Epoch 1720, val loss: 1.2792413234710693
Epoch 1730, training loss: 62.313228607177734 = 0.030632654204964638 + 10.0 * 6.228259563446045
Epoch 1730, val loss: 1.2836958169937134
Epoch 1740, training loss: 62.28972244262695 = 0.030043184757232666 + 10.0 * 6.225967884063721
Epoch 1740, val loss: 1.2881191968917847
Epoch 1750, training loss: 62.268211364746094 = 0.029474005103111267 + 10.0 * 6.223874092102051
Epoch 1750, val loss: 1.2925678491592407
Epoch 1760, training loss: 62.266090393066406 = 0.028928404673933983 + 10.0 * 6.2237162590026855
Epoch 1760, val loss: 1.2972227334976196
Epoch 1770, training loss: 62.26626205444336 = 0.02839287929236889 + 10.0 * 6.2237868309021
Epoch 1770, val loss: 1.3015937805175781
Epoch 1780, training loss: 62.27455520629883 = 0.0278711449354887 + 10.0 * 6.224668502807617
Epoch 1780, val loss: 1.3059933185577393
Epoch 1790, training loss: 62.290435791015625 = 0.02735179290175438 + 10.0 * 6.226308345794678
Epoch 1790, val loss: 1.3103097677230835
Epoch 1800, training loss: 62.253578186035156 = 0.026847418397665024 + 10.0 * 6.222672939300537
Epoch 1800, val loss: 1.3146699666976929
Epoch 1810, training loss: 62.25989532470703 = 0.026366006582975388 + 10.0 * 6.223352909088135
Epoch 1810, val loss: 1.3189443349838257
Epoch 1820, training loss: 62.262760162353516 = 0.025898050516843796 + 10.0 * 6.223686218261719
Epoch 1820, val loss: 1.3234342336654663
Epoch 1830, training loss: 62.23835372924805 = 0.02543989010155201 + 10.0 * 6.221291542053223
Epoch 1830, val loss: 1.3277640342712402
Epoch 1840, training loss: 62.252689361572266 = 0.025002988055348396 + 10.0 * 6.222768783569336
Epoch 1840, val loss: 1.332047462463379
Epoch 1850, training loss: 62.275604248046875 = 0.024569692090153694 + 10.0 * 6.225103378295898
Epoch 1850, val loss: 1.3360792398452759
Epoch 1860, training loss: 62.225013732910156 = 0.02413347363471985 + 10.0 * 6.220088005065918
Epoch 1860, val loss: 1.3402965068817139
Epoch 1870, training loss: 62.22486114501953 = 0.023723283782601357 + 10.0 * 6.220113754272461
Epoch 1870, val loss: 1.3445943593978882
Epoch 1880, training loss: 62.220088958740234 = 0.023326389491558075 + 10.0 * 6.2196760177612305
Epoch 1880, val loss: 1.3488504886627197
Epoch 1890, training loss: 62.24924087524414 = 0.022940628230571747 + 10.0 * 6.222630023956299
Epoch 1890, val loss: 1.3529994487762451
Epoch 1900, training loss: 62.24545669555664 = 0.022547155618667603 + 10.0 * 6.222290992736816
Epoch 1900, val loss: 1.3566275835037231
Epoch 1910, training loss: 62.22100830078125 = 0.022165242582559586 + 10.0 * 6.219884395599365
Epoch 1910, val loss: 1.360823631286621
Epoch 1920, training loss: 62.213417053222656 = 0.021802932024002075 + 10.0 * 6.219161033630371
Epoch 1920, val loss: 1.3648149967193604
Epoch 1930, training loss: 62.235538482666016 = 0.021452419459819794 + 10.0 * 6.221408367156982
Epoch 1930, val loss: 1.3690319061279297
Epoch 1940, training loss: 62.21653747558594 = 0.021100936457514763 + 10.0 * 6.219543933868408
Epoch 1940, val loss: 1.3727762699127197
Epoch 1950, training loss: 62.22045135498047 = 0.020759418606758118 + 10.0 * 6.219969272613525
Epoch 1950, val loss: 1.376953125
Epoch 1960, training loss: 62.22211456298828 = 0.020427025854587555 + 10.0 * 6.220168590545654
Epoch 1960, val loss: 1.380865454673767
Epoch 1970, training loss: 62.194984436035156 = 0.020104169845581055 + 10.0 * 6.2174882888793945
Epoch 1970, val loss: 1.3845865726470947
Epoch 1980, training loss: 62.19806671142578 = 0.019795682281255722 + 10.0 * 6.217827320098877
Epoch 1980, val loss: 1.38857102394104
Epoch 1990, training loss: 62.23065185546875 = 0.01949615776538849 + 10.0 * 6.221115589141846
Epoch 1990, val loss: 1.392366647720337
Epoch 2000, training loss: 62.215553283691406 = 0.019184816628694534 + 10.0 * 6.219636917114258
Epoch 2000, val loss: 1.3961386680603027
Epoch 2010, training loss: 62.19670486450195 = 0.018888287246227264 + 10.0 * 6.217782020568848
Epoch 2010, val loss: 1.399888277053833
Epoch 2020, training loss: 62.198238372802734 = 0.01860191859304905 + 10.0 * 6.217963695526123
Epoch 2020, val loss: 1.4036717414855957
Epoch 2030, training loss: 62.19112014770508 = 0.018321089446544647 + 10.0 * 6.21727991104126
Epoch 2030, val loss: 1.4074525833129883
Epoch 2040, training loss: 62.17646408081055 = 0.018047748133540154 + 10.0 * 6.215841770172119
Epoch 2040, val loss: 1.4114229679107666
Epoch 2050, training loss: 62.170982360839844 = 0.01778438128530979 + 10.0 * 6.215319633483887
Epoch 2050, val loss: 1.415136694908142
Epoch 2060, training loss: 62.19450759887695 = 0.01752995140850544 + 10.0 * 6.217698097229004
Epoch 2060, val loss: 1.4188255071640015
Epoch 2070, training loss: 62.20451736450195 = 0.017265664413571358 + 10.0 * 6.218725204467773
Epoch 2070, val loss: 1.4225385189056396
Epoch 2080, training loss: 62.184288024902344 = 0.017004169523715973 + 10.0 * 6.216728210449219
Epoch 2080, val loss: 1.4257839918136597
Epoch 2090, training loss: 62.172752380371094 = 0.016757266595959663 + 10.0 * 6.215599536895752
Epoch 2090, val loss: 1.4295125007629395
Epoch 2100, training loss: 62.16299057006836 = 0.01652216538786888 + 10.0 * 6.214646816253662
Epoch 2100, val loss: 1.43319571018219
Epoch 2110, training loss: 62.17002487182617 = 0.016291016712784767 + 10.0 * 6.2153730392456055
Epoch 2110, val loss: 1.4369292259216309
Epoch 2120, training loss: 62.229557037353516 = 0.016062473878264427 + 10.0 * 6.221349239349365
Epoch 2120, val loss: 1.4404189586639404
Epoch 2130, training loss: 62.179317474365234 = 0.015834033489227295 + 10.0 * 6.216348171234131
Epoch 2130, val loss: 1.4437921047210693
Epoch 2140, training loss: 62.157508850097656 = 0.015611366368830204 + 10.0 * 6.2141900062561035
Epoch 2140, val loss: 1.4472274780273438
Epoch 2150, training loss: 62.14925003051758 = 0.015401838347315788 + 10.0 * 6.213385105133057
Epoch 2150, val loss: 1.450961947441101
Epoch 2160, training loss: 62.196983337402344 = 0.015195975080132484 + 10.0 * 6.218178749084473
Epoch 2160, val loss: 1.454572081565857
Epoch 2170, training loss: 62.17192077636719 = 0.014984873123466969 + 10.0 * 6.215693473815918
Epoch 2170, val loss: 1.4580312967300415
Epoch 2180, training loss: 62.15473937988281 = 0.014777627773582935 + 10.0 * 6.213995933532715
Epoch 2180, val loss: 1.4611793756484985
Epoch 2190, training loss: 62.14533615112305 = 0.01458015013486147 + 10.0 * 6.213075637817383
Epoch 2190, val loss: 1.4645320177078247
Epoch 2200, training loss: 62.13528823852539 = 0.014389622025191784 + 10.0 * 6.212090015411377
Epoch 2200, val loss: 1.4681323766708374
Epoch 2210, training loss: 62.167694091796875 = 0.014205368235707283 + 10.0 * 6.215348720550537
Epoch 2210, val loss: 1.4716953039169312
Epoch 2220, training loss: 62.14034652709961 = 0.014014427550137043 + 10.0 * 6.21263313293457
Epoch 2220, val loss: 1.4747214317321777
Epoch 2230, training loss: 62.13656234741211 = 0.013830902054905891 + 10.0 * 6.212273120880127
Epoch 2230, val loss: 1.478074312210083
Epoch 2240, training loss: 62.14471435546875 = 0.013654250651597977 + 10.0 * 6.213106155395508
Epoch 2240, val loss: 1.4815795421600342
Epoch 2250, training loss: 62.19012451171875 = 0.013479327782988548 + 10.0 * 6.2176642417907715
Epoch 2250, val loss: 1.4849809408187866
Epoch 2260, training loss: 62.145870208740234 = 0.013305658474564552 + 10.0 * 6.213256359100342
Epoch 2260, val loss: 1.4877417087554932
Epoch 2270, training loss: 62.12614440917969 = 0.013138112612068653 + 10.0 * 6.211300849914551
Epoch 2270, val loss: 1.491288423538208
Epoch 2280, training loss: 62.13627243041992 = 0.01297585479915142 + 10.0 * 6.212329387664795
Epoch 2280, val loss: 1.4946306943893433
Epoch 2290, training loss: 62.1799430847168 = 0.012815208174288273 + 10.0 * 6.216712951660156
Epoch 2290, val loss: 1.497560977935791
Epoch 2300, training loss: 62.151424407958984 = 0.012654965743422508 + 10.0 * 6.213877201080322
Epoch 2300, val loss: 1.5007743835449219
Epoch 2310, training loss: 62.129276275634766 = 0.012496689334511757 + 10.0 * 6.2116780281066895
Epoch 2310, val loss: 1.5039300918579102
Epoch 2320, training loss: 62.112430572509766 = 0.012346223928034306 + 10.0 * 6.21000862121582
Epoch 2320, val loss: 1.5071353912353516
Epoch 2330, training loss: 62.121036529541016 = 0.012201538309454918 + 10.0 * 6.210883140563965
Epoch 2330, val loss: 1.5103429555892944
Epoch 2340, training loss: 62.15047836303711 = 0.012058177962899208 + 10.0 * 6.213841915130615
Epoch 2340, val loss: 1.5134069919586182
Epoch 2350, training loss: 62.14457702636719 = 0.011910309083759785 + 10.0 * 6.213266849517822
Epoch 2350, val loss: 1.5162731409072876
Epoch 2360, training loss: 62.11251449584961 = 0.011764336377382278 + 10.0 * 6.2100749015808105
Epoch 2360, val loss: 1.519347071647644
Epoch 2370, training loss: 62.10282516479492 = 0.011625770479440689 + 10.0 * 6.20911979675293
Epoch 2370, val loss: 1.5225305557250977
Epoch 2380, training loss: 62.121097564697266 = 0.01149442046880722 + 10.0 * 6.210960388183594
Epoch 2380, val loss: 1.5258092880249023
Epoch 2390, training loss: 62.13581466674805 = 0.011362546123564243 + 10.0 * 6.212445259094238
Epoch 2390, val loss: 1.5286885499954224
Epoch 2400, training loss: 62.114288330078125 = 0.011228942312300205 + 10.0 * 6.210305690765381
Epoch 2400, val loss: 1.531396508216858
Epoch 2410, training loss: 62.11127853393555 = 0.011103366501629353 + 10.0 * 6.210017204284668
Epoch 2410, val loss: 1.5346416234970093
Epoch 2420, training loss: 62.163330078125 = 0.010980520397424698 + 10.0 * 6.215235233306885
Epoch 2420, val loss: 1.5373752117156982
Epoch 2430, training loss: 62.12391662597656 = 0.010847126133739948 + 10.0 * 6.211306571960449
Epoch 2430, val loss: 1.5403711795806885
Epoch 2440, training loss: 62.09589385986328 = 0.01072362158447504 + 10.0 * 6.208517074584961
Epoch 2440, val loss: 1.54315185546875
Epoch 2450, training loss: 62.08998107910156 = 0.010608085431158543 + 10.0 * 6.207937240600586
Epoch 2450, val loss: 1.5464470386505127
Epoch 2460, training loss: 62.087772369384766 = 0.010493583045899868 + 10.0 * 6.207727909088135
Epoch 2460, val loss: 1.549432396888733
Epoch 2470, training loss: 62.142906188964844 = 0.010383703745901585 + 10.0 * 6.213252067565918
Epoch 2470, val loss: 1.5524747371673584
Epoch 2480, training loss: 62.103485107421875 = 0.010266727767884731 + 10.0 * 6.209321975708008
Epoch 2480, val loss: 1.554940938949585
Epoch 2490, training loss: 62.08915328979492 = 0.010152353905141354 + 10.0 * 6.207900047302246
Epoch 2490, val loss: 1.5577423572540283
Epoch 2500, training loss: 62.09263229370117 = 0.010045278817415237 + 10.0 * 6.208258628845215
Epoch 2500, val loss: 1.5607829093933105
Epoch 2510, training loss: 62.105106353759766 = 0.009940095245838165 + 10.0 * 6.209516525268555
Epoch 2510, val loss: 1.563481330871582
Epoch 2520, training loss: 62.08314514160156 = 0.009833870455622673 + 10.0 * 6.20733118057251
Epoch 2520, val loss: 1.5663299560546875
Epoch 2530, training loss: 62.140811920166016 = 0.009733717888593674 + 10.0 * 6.213107585906982
Epoch 2530, val loss: 1.569148302078247
Epoch 2540, training loss: 62.10897445678711 = 0.00962716806679964 + 10.0 * 6.209934711456299
Epoch 2540, val loss: 1.5716713666915894
Epoch 2550, training loss: 62.09817123413086 = 0.009526479057967663 + 10.0 * 6.208864688873291
Epoch 2550, val loss: 1.5743403434753418
Epoch 2560, training loss: 62.082820892333984 = 0.009427226148545742 + 10.0 * 6.207339286804199
Epoch 2560, val loss: 1.5772521495819092
Epoch 2570, training loss: 62.082618713378906 = 0.009334070608019829 + 10.0 * 6.2073283195495605
Epoch 2570, val loss: 1.5799435377120972
Epoch 2580, training loss: 62.08262252807617 = 0.0092402882874012 + 10.0 * 6.207338333129883
Epoch 2580, val loss: 1.5826444625854492
Epoch 2590, training loss: 62.11204147338867 = 0.009149332530796528 + 10.0 * 6.210289478302002
Epoch 2590, val loss: 1.5854343175888062
Epoch 2600, training loss: 62.07192611694336 = 0.009052876383066177 + 10.0 * 6.206287384033203
Epoch 2600, val loss: 1.5878770351409912
Epoch 2610, training loss: 62.07984924316406 = 0.00896252878010273 + 10.0 * 6.207088947296143
Epoch 2610, val loss: 1.590549349784851
Epoch 2620, training loss: 62.0977668762207 = 0.008876362815499306 + 10.0 * 6.208889007568359
Epoch 2620, val loss: 1.593216896057129
Epoch 2630, training loss: 62.081966400146484 = 0.008786773309111595 + 10.0 * 6.207318305969238
Epoch 2630, val loss: 1.596004605293274
Epoch 2640, training loss: 62.07085037231445 = 0.008700081147253513 + 10.0 * 6.206214904785156
Epoch 2640, val loss: 1.598479151725769
Epoch 2650, training loss: 62.05727005004883 = 0.008617497980594635 + 10.0 * 6.204865455627441
Epoch 2650, val loss: 1.601007342338562
Epoch 2660, training loss: 62.09136962890625 = 0.008537287823855877 + 10.0 * 6.208283424377441
Epoch 2660, val loss: 1.603652000427246
Epoch 2670, training loss: 62.11671447753906 = 0.008451074361801147 + 10.0 * 6.210826396942139
Epoch 2670, val loss: 1.60623037815094
Epoch 2680, training loss: 62.06795883178711 = 0.008367028087377548 + 10.0 * 6.205959320068359
Epoch 2680, val loss: 1.6081485748291016
Epoch 2690, training loss: 62.05373001098633 = 0.008288160897791386 + 10.0 * 6.2045440673828125
Epoch 2690, val loss: 1.610939383506775
Epoch 2700, training loss: 62.04765701293945 = 0.00821309071034193 + 10.0 * 6.203944206237793
Epoch 2700, val loss: 1.6136269569396973
Epoch 2710, training loss: 62.074684143066406 = 0.008140753023326397 + 10.0 * 6.2066545486450195
Epoch 2710, val loss: 1.6160805225372314
Epoch 2720, training loss: 62.07430648803711 = 0.008061832748353481 + 10.0 * 6.206624507904053
Epoch 2720, val loss: 1.618349552154541
Epoch 2730, training loss: 62.06122970581055 = 0.007983262650668621 + 10.0 * 6.205324649810791
Epoch 2730, val loss: 1.6207276582717896
Epoch 2740, training loss: 62.05119323730469 = 0.007911019958555698 + 10.0 * 6.2043280601501465
Epoch 2740, val loss: 1.6232701539993286
Epoch 2750, training loss: 62.052574157714844 = 0.00784070510417223 + 10.0 * 6.204473495483398
Epoch 2750, val loss: 1.6256986856460571
Epoch 2760, training loss: 62.08333206176758 = 0.007771349977701902 + 10.0 * 6.207556247711182
Epoch 2760, val loss: 1.6280806064605713
Epoch 2770, training loss: 62.098976135253906 = 0.007699368055909872 + 10.0 * 6.209127902984619
Epoch 2770, val loss: 1.6302154064178467
Epoch 2780, training loss: 62.04920959472656 = 0.007627490907907486 + 10.0 * 6.204157829284668
Epoch 2780, val loss: 1.6325440406799316
Epoch 2790, training loss: 62.04006576538086 = 0.007559994701296091 + 10.0 * 6.203250408172607
Epoch 2790, val loss: 1.6350014209747314
Epoch 2800, training loss: 62.04157257080078 = 0.00749488128349185 + 10.0 * 6.2034077644348145
Epoch 2800, val loss: 1.6374733448028564
Epoch 2810, training loss: 62.09662628173828 = 0.00743121188133955 + 10.0 * 6.208919525146484
Epoch 2810, val loss: 1.6398444175720215
Epoch 2820, training loss: 62.02973175048828 = 0.007363452110439539 + 10.0 * 6.202237129211426
Epoch 2820, val loss: 1.6415305137634277
Epoch 2830, training loss: 62.03372573852539 = 0.007300701458007097 + 10.0 * 6.202642440795898
Epoch 2830, val loss: 1.6439557075500488
Epoch 2840, training loss: 62.0542106628418 = 0.0072400979697704315 + 10.0 * 6.204697132110596
Epoch 2840, val loss: 1.6463918685913086
Epoch 2850, training loss: 62.061790466308594 = 0.007177052088081837 + 10.0 * 6.205461502075195
Epoch 2850, val loss: 1.6484627723693848
Epoch 2860, training loss: 62.043277740478516 = 0.007113661617040634 + 10.0 * 6.203616142272949
Epoch 2860, val loss: 1.650730848312378
Epoch 2870, training loss: 62.03854751586914 = 0.0070540108717978 + 10.0 * 6.203149318695068
Epoch 2870, val loss: 1.6528916358947754
Epoch 2880, training loss: 62.038700103759766 = 0.0069954534992575645 + 10.0 * 6.203170299530029
Epoch 2880, val loss: 1.6551629304885864
Epoch 2890, training loss: 62.078792572021484 = 0.006940022110939026 + 10.0 * 6.2071852684021
Epoch 2890, val loss: 1.6573187112808228
Epoch 2900, training loss: 62.03490447998047 = 0.006878766231238842 + 10.0 * 6.202802658081055
Epoch 2900, val loss: 1.6588609218597412
Epoch 2910, training loss: 62.03017807006836 = 0.006822321563959122 + 10.0 * 6.202335834503174
Epoch 2910, val loss: 1.6614223718643188
Epoch 2920, training loss: 62.05600357055664 = 0.006767674814909697 + 10.0 * 6.204923629760742
Epoch 2920, val loss: 1.6635286808013916
Epoch 2930, training loss: 62.03226852416992 = 0.006711798720061779 + 10.0 * 6.2025556564331055
Epoch 2930, val loss: 1.6652487516403198
Epoch 2940, training loss: 62.047298431396484 = 0.0066585722379386425 + 10.0 * 6.204063892364502
Epoch 2940, val loss: 1.667589545249939
Epoch 2950, training loss: 62.03471755981445 = 0.006602981127798557 + 10.0 * 6.202811241149902
Epoch 2950, val loss: 1.6694496870040894
Epoch 2960, training loss: 62.028419494628906 = 0.006550285499542952 + 10.0 * 6.2021870613098145
Epoch 2960, val loss: 1.6716208457946777
Epoch 2970, training loss: 62.069889068603516 = 0.006498630624264479 + 10.0 * 6.206338882446289
Epoch 2970, val loss: 1.6737861633300781
Epoch 2980, training loss: 62.01450729370117 = 0.006445947103202343 + 10.0 * 6.200806140899658
Epoch 2980, val loss: 1.6751153469085693
Epoch 2990, training loss: 62.02117156982422 = 0.006395754404366016 + 10.0 * 6.201477527618408
Epoch 2990, val loss: 1.677266240119934
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 87.910400390625 = 1.9421141147613525 + 10.0 * 8.59682846069336
Epoch 0, val loss: 1.9469581842422485
Epoch 10, training loss: 87.89466857910156 = 1.9325237274169922 + 10.0 * 8.596214294433594
Epoch 10, val loss: 1.9370777606964111
Epoch 20, training loss: 87.83757019042969 = 1.920806884765625 + 10.0 * 8.591676712036133
Epoch 20, val loss: 1.9247900247573853
Epoch 30, training loss: 87.48432922363281 = 1.9055767059326172 + 10.0 * 8.55787467956543
Epoch 30, val loss: 1.9088762998580933
Epoch 40, training loss: 85.10551452636719 = 1.8865230083465576 + 10.0 * 8.3218994140625
Epoch 40, val loss: 1.8892680406570435
Epoch 50, training loss: 78.97166442871094 = 1.8642586469650269 + 10.0 * 7.71074104309082
Epoch 50, val loss: 1.86702561378479
Epoch 60, training loss: 76.25838470458984 = 1.848187804222107 + 10.0 * 7.441019535064697
Epoch 60, val loss: 1.8521175384521484
Epoch 70, training loss: 73.33172607421875 = 1.8371403217315674 + 10.0 * 7.149458408355713
Epoch 70, val loss: 1.8409041166305542
Epoch 80, training loss: 70.92285919189453 = 1.8283212184906006 + 10.0 * 6.909453392028809
Epoch 80, val loss: 1.8318301439285278
Epoch 90, training loss: 69.86511993408203 = 1.8193097114562988 + 10.0 * 6.8045806884765625
Epoch 90, val loss: 1.822521448135376
Epoch 100, training loss: 69.13489532470703 = 1.809442162513733 + 10.0 * 6.732545852661133
Epoch 100, val loss: 1.8123234510421753
Epoch 110, training loss: 68.45966339111328 = 1.8001478910446167 + 10.0 * 6.665951251983643
Epoch 110, val loss: 1.8026975393295288
Epoch 120, training loss: 68.0331802368164 = 1.7914804220199585 + 10.0 * 6.624170303344727
Epoch 120, val loss: 1.7936183214187622
Epoch 130, training loss: 67.68722534179688 = 1.7822914123535156 + 10.0 * 6.5904927253723145
Epoch 130, val loss: 1.7844741344451904
Epoch 140, training loss: 67.40754699707031 = 1.7728209495544434 + 10.0 * 6.563472747802734
Epoch 140, val loss: 1.7754408121109009
Epoch 150, training loss: 67.17697143554688 = 1.7630685567855835 + 10.0 * 6.5413899421691895
Epoch 150, val loss: 1.7664002180099487
Epoch 160, training loss: 66.992431640625 = 1.7527613639831543 + 10.0 * 6.523967266082764
Epoch 160, val loss: 1.7570912837982178
Epoch 170, training loss: 66.78691864013672 = 1.7417596578598022 + 10.0 * 6.504516124725342
Epoch 170, val loss: 1.7472888231277466
Epoch 180, training loss: 66.61434173583984 = 1.7299765348434448 + 10.0 * 6.488436698913574
Epoch 180, val loss: 1.7370116710662842
Epoch 190, training loss: 66.47575378417969 = 1.717139720916748 + 10.0 * 6.475861549377441
Epoch 190, val loss: 1.7260795831680298
Epoch 200, training loss: 66.33285522460938 = 1.7030843496322632 + 10.0 * 6.462976932525635
Epoch 200, val loss: 1.714090347290039
Epoch 210, training loss: 66.19265747070312 = 1.6876450777053833 + 10.0 * 6.450500965118408
Epoch 210, val loss: 1.7010749578475952
Epoch 220, training loss: 66.07769775390625 = 1.670702576637268 + 10.0 * 6.440700054168701
Epoch 220, val loss: 1.6868692636489868
Epoch 230, training loss: 65.96435546875 = 1.6522184610366821 + 10.0 * 6.43121337890625
Epoch 230, val loss: 1.671448826789856
Epoch 240, training loss: 65.85454559326172 = 1.6319987773895264 + 10.0 * 6.422255039215088
Epoch 240, val loss: 1.6546515226364136
Epoch 250, training loss: 65.74662780761719 = 1.6100362539291382 + 10.0 * 6.413658618927002
Epoch 250, val loss: 1.6365156173706055
Epoch 260, training loss: 65.65621185302734 = 1.5864512920379639 + 10.0 * 6.406976222991943
Epoch 260, val loss: 1.617142915725708
Epoch 270, training loss: 65.55814361572266 = 1.561213493347168 + 10.0 * 6.399693012237549
Epoch 270, val loss: 1.5964983701705933
Epoch 280, training loss: 65.4530029296875 = 1.5344395637512207 + 10.0 * 6.3918561935424805
Epoch 280, val loss: 1.5746831893920898
Epoch 290, training loss: 65.39714050292969 = 1.5061697959899902 + 10.0 * 6.389096736907959
Epoch 290, val loss: 1.5517818927764893
Epoch 300, training loss: 65.29750061035156 = 1.4765079021453857 + 10.0 * 6.382099628448486
Epoch 300, val loss: 1.527901530265808
Epoch 310, training loss: 65.20423889160156 = 1.4457144737243652 + 10.0 * 6.375852584838867
Epoch 310, val loss: 1.503225564956665
Epoch 320, training loss: 65.11869812011719 = 1.4141182899475098 + 10.0 * 6.370458126068115
Epoch 320, val loss: 1.4781798124313354
Epoch 330, training loss: 65.03460693359375 = 1.3819586038589478 + 10.0 * 6.365264415740967
Epoch 330, val loss: 1.452850341796875
Epoch 340, training loss: 64.99998474121094 = 1.349386215209961 + 10.0 * 6.365059852600098
Epoch 340, val loss: 1.4274343252182007
Epoch 350, training loss: 64.89955139160156 = 1.316521167755127 + 10.0 * 6.358303070068359
Epoch 350, val loss: 1.4021246433258057
Epoch 360, training loss: 64.81626892089844 = 1.2838166952133179 + 10.0 * 6.353245735168457
Epoch 360, val loss: 1.3771347999572754
Epoch 370, training loss: 64.76095581054688 = 1.2514163255691528 + 10.0 * 6.350953578948975
Epoch 370, val loss: 1.3527356386184692
Epoch 380, training loss: 64.70811462402344 = 1.21927011013031 + 10.0 * 6.348884582519531
Epoch 380, val loss: 1.3288682699203491
Epoch 390, training loss: 64.6185531616211 = 1.1878615617752075 + 10.0 * 6.343069076538086
Epoch 390, val loss: 1.3057653903961182
Epoch 400, training loss: 64.54734802246094 = 1.157135248184204 + 10.0 * 6.3390212059021
Epoch 400, val loss: 1.2835112810134888
Epoch 410, training loss: 64.5429916381836 = 1.127028465270996 + 10.0 * 6.3415961265563965
Epoch 410, val loss: 1.2620031833648682
Epoch 420, training loss: 64.43746185302734 = 1.0975853204727173 + 10.0 * 6.333987236022949
Epoch 420, val loss: 1.241132378578186
Epoch 430, training loss: 64.37115478515625 = 1.0689741373062134 + 10.0 * 6.3302178382873535
Epoch 430, val loss: 1.2211774587631226
Epoch 440, training loss: 64.32672119140625 = 1.0411789417266846 + 10.0 * 6.328554153442383
Epoch 440, val loss: 1.202185869216919
Epoch 450, training loss: 64.26370239257812 = 1.0140416622161865 + 10.0 * 6.324965953826904
Epoch 450, val loss: 1.1838459968566895
Epoch 460, training loss: 64.2123031616211 = 0.9876918792724609 + 10.0 * 6.322461128234863
Epoch 460, val loss: 1.1663001775741577
Epoch 470, training loss: 64.16094970703125 = 0.9620471596717834 + 10.0 * 6.31989049911499
Epoch 470, val loss: 1.1494941711425781
Epoch 480, training loss: 64.17559814453125 = 0.9370277523994446 + 10.0 * 6.323857307434082
Epoch 480, val loss: 1.1333962678909302
Epoch 490, training loss: 64.09345245361328 = 0.9126555919647217 + 10.0 * 6.318079471588135
Epoch 490, val loss: 1.1179195642471313
Epoch 500, training loss: 64.01951599121094 = 0.8890396356582642 + 10.0 * 6.313047885894775
Epoch 500, val loss: 1.1032929420471191
Epoch 510, training loss: 63.97529983520508 = 0.8660873174667358 + 10.0 * 6.3109211921691895
Epoch 510, val loss: 1.0894145965576172
Epoch 520, training loss: 63.93596267700195 = 0.8436520099639893 + 10.0 * 6.309231281280518
Epoch 520, val loss: 1.076196551322937
Epoch 530, training loss: 63.935096740722656 = 0.8216583132743835 + 10.0 * 6.311343669891357
Epoch 530, val loss: 1.0636518001556396
Epoch 540, training loss: 63.86175537109375 = 0.8002282381057739 + 10.0 * 6.306152820587158
Epoch 540, val loss: 1.051226019859314
Epoch 550, training loss: 63.8028678894043 = 0.7793086767196655 + 10.0 * 6.302355766296387
Epoch 550, val loss: 1.0396605730056763
Epoch 560, training loss: 63.7652587890625 = 0.7589190006256104 + 10.0 * 6.300633907318115
Epoch 560, val loss: 1.0287604331970215
Epoch 570, training loss: 63.74573516845703 = 0.738955020904541 + 10.0 * 6.30067777633667
Epoch 570, val loss: 1.0184053182601929
Epoch 580, training loss: 63.70812225341797 = 0.7193048000335693 + 10.0 * 6.298882007598877
Epoch 580, val loss: 1.008463740348816
Epoch 590, training loss: 63.65985107421875 = 0.7000889778137207 + 10.0 * 6.295976161956787
Epoch 590, val loss: 0.9990379214286804
Epoch 600, training loss: 63.64670181274414 = 0.6813492774963379 + 10.0 * 6.296535015106201
Epoch 600, val loss: 0.9901057481765747
Epoch 610, training loss: 63.587013244628906 = 0.662952184677124 + 10.0 * 6.29240608215332
Epoch 610, val loss: 0.981704831123352
Epoch 620, training loss: 63.55747985839844 = 0.6449885964393616 + 10.0 * 6.2912492752075195
Epoch 620, val loss: 0.9738442897796631
Epoch 630, training loss: 63.53691101074219 = 0.6273864507675171 + 10.0 * 6.290952205657959
Epoch 630, val loss: 0.9662653207778931
Epoch 640, training loss: 63.49968719482422 = 0.6100875735282898 + 10.0 * 6.288959980010986
Epoch 640, val loss: 0.959030032157898
Epoch 650, training loss: 63.4586067199707 = 0.5932138562202454 + 10.0 * 6.286539554595947
Epoch 650, val loss: 0.9521623253822327
Epoch 660, training loss: 63.420345306396484 = 0.5767185688018799 + 10.0 * 6.28436279296875
Epoch 660, val loss: 0.9458558559417725
Epoch 670, training loss: 63.39004135131836 = 0.5605854988098145 + 10.0 * 6.28294563293457
Epoch 670, val loss: 0.9399312138557434
Epoch 680, training loss: 63.49298858642578 = 0.544754683971405 + 10.0 * 6.29482364654541
Epoch 680, val loss: 0.9343068599700928
Epoch 690, training loss: 63.37590789794922 = 0.5290970802307129 + 10.0 * 6.2846808433532715
Epoch 690, val loss: 0.9287782311439514
Epoch 700, training loss: 63.31863784790039 = 0.51390141248703 + 10.0 * 6.280473709106445
Epoch 700, val loss: 0.9239374399185181
Epoch 710, training loss: 63.28068923950195 = 0.49914732575416565 + 10.0 * 6.278154373168945
Epoch 710, val loss: 0.9195818305015564
Epoch 720, training loss: 63.25201416015625 = 0.48472169041633606 + 10.0 * 6.276729106903076
Epoch 720, val loss: 0.9155680537223816
Epoch 730, training loss: 63.29664993286133 = 0.4705829918384552 + 10.0 * 6.282606601715088
Epoch 730, val loss: 0.9118254780769348
Epoch 740, training loss: 63.20596694946289 = 0.4566571116447449 + 10.0 * 6.274930953979492
Epoch 740, val loss: 0.9082636833190918
Epoch 750, training loss: 63.17927932739258 = 0.4431442320346832 + 10.0 * 6.273613452911377
Epoch 750, val loss: 0.905158281326294
Epoch 760, training loss: 63.15583038330078 = 0.43001970648765564 + 10.0 * 6.272581100463867
Epoch 760, val loss: 0.9024221301078796
Epoch 770, training loss: 63.19090270996094 = 0.4171803593635559 + 10.0 * 6.277372360229492
Epoch 770, val loss: 0.8999287486076355
Epoch 780, training loss: 63.12586975097656 = 0.40456512570381165 + 10.0 * 6.272130489349365
Epoch 780, val loss: 0.8977788686752319
Epoch 790, training loss: 63.08798599243164 = 0.3922978937625885 + 10.0 * 6.26956844329834
Epoch 790, val loss: 0.8960189819335938
Epoch 800, training loss: 63.06865692138672 = 0.38041844964027405 + 10.0 * 6.268824100494385
Epoch 800, val loss: 0.8945518136024475
Epoch 810, training loss: 63.07286834716797 = 0.36885833740234375 + 10.0 * 6.2704010009765625
Epoch 810, val loss: 0.8933944702148438
Epoch 820, training loss: 63.05372619628906 = 0.35749661922454834 + 10.0 * 6.269622802734375
Epoch 820, val loss: 0.8926300406455994
Epoch 830, training loss: 63.01490783691406 = 0.34649601578712463 + 10.0 * 6.266840934753418
Epoch 830, val loss: 0.8920764327049255
Epoch 840, training loss: 62.985294342041016 = 0.33583539724349976 + 10.0 * 6.264945983886719
Epoch 840, val loss: 0.8919588923454285
Epoch 850, training loss: 62.99717712402344 = 0.32544782757759094 + 10.0 * 6.267172813415527
Epoch 850, val loss: 0.8920940160751343
Epoch 860, training loss: 62.95552062988281 = 0.3153691291809082 + 10.0 * 6.264015197753906
Epoch 860, val loss: 0.8925058245658875
Epoch 870, training loss: 62.92858123779297 = 0.3055656850337982 + 10.0 * 6.262301445007324
Epoch 870, val loss: 0.8932886719703674
Epoch 880, training loss: 62.94216537475586 = 0.2961091101169586 + 10.0 * 6.264605522155762
Epoch 880, val loss: 0.8943405747413635
Epoch 890, training loss: 62.90148162841797 = 0.2868497967720032 + 10.0 * 6.261463165283203
Epoch 890, val loss: 0.8956462740898132
Epoch 900, training loss: 62.9024658203125 = 0.2779468297958374 + 10.0 * 6.262452125549316
Epoch 900, val loss: 0.8972237706184387
Epoch 910, training loss: 62.874298095703125 = 0.2692973017692566 + 10.0 * 6.260499954223633
Epoch 910, val loss: 0.8991156220436096
Epoch 920, training loss: 62.85329818725586 = 0.26099705696105957 + 10.0 * 6.259230136871338
Epoch 920, val loss: 0.9012079238891602
Epoch 930, training loss: 62.842063903808594 = 0.2529473900794983 + 10.0 * 6.258911609649658
Epoch 930, val loss: 0.9035860896110535
Epoch 940, training loss: 62.81361770629883 = 0.2451845407485962 + 10.0 * 6.256843090057373
Epoch 940, val loss: 0.9062532782554626
Epoch 950, training loss: 62.79819869995117 = 0.23770204186439514 + 10.0 * 6.256049633026123
Epoch 950, val loss: 0.9091923832893372
Epoch 960, training loss: 62.8189697265625 = 0.23046095669269562 + 10.0 * 6.258851051330566
Epoch 960, val loss: 0.9122995734214783
Epoch 970, training loss: 62.7739372253418 = 0.22337757050991058 + 10.0 * 6.255055904388428
Epoch 970, val loss: 0.9155985713005066
Epoch 980, training loss: 62.77787780761719 = 0.21660543978214264 + 10.0 * 6.25612735748291
Epoch 980, val loss: 0.9191897511482239
Epoch 990, training loss: 62.73991775512695 = 0.21005511283874512 + 10.0 * 6.252985954284668
Epoch 990, val loss: 0.9228324890136719
Epoch 1000, training loss: 62.726905822753906 = 0.20372608304023743 + 10.0 * 6.252317905426025
Epoch 1000, val loss: 0.9267023801803589
Epoch 1010, training loss: 62.72557830810547 = 0.19763928651809692 + 10.0 * 6.252793788909912
Epoch 1010, val loss: 0.9307931661605835
Epoch 1020, training loss: 62.744197845458984 = 0.19174005091190338 + 10.0 * 6.255245685577393
Epoch 1020, val loss: 0.934872567653656
Epoch 1030, training loss: 62.6986083984375 = 0.1860005408525467 + 10.0 * 6.251260757446289
Epoch 1030, val loss: 0.9391646385192871
Epoch 1040, training loss: 62.68815612792969 = 0.18049030005931854 + 10.0 * 6.250766754150391
Epoch 1040, val loss: 0.9435948729515076
Epoch 1050, training loss: 62.66400909423828 = 0.17518018186092377 + 10.0 * 6.248883247375488
Epoch 1050, val loss: 0.9482791423797607
Epoch 1060, training loss: 62.70806884765625 = 0.17005951702594757 + 10.0 * 6.253800868988037
Epoch 1060, val loss: 0.9530689120292664
Epoch 1070, training loss: 62.6575927734375 = 0.1650647073984146 + 10.0 * 6.249252796173096
Epoch 1070, val loss: 0.9576639533042908
Epoch 1080, training loss: 62.63356399536133 = 0.1602519452571869 + 10.0 * 6.247331142425537
Epoch 1080, val loss: 0.9626682996749878
Epoch 1090, training loss: 62.634742736816406 = 0.1556178629398346 + 10.0 * 6.247912406921387
Epoch 1090, val loss: 0.9675697684288025
Epoch 1100, training loss: 62.630958557128906 = 0.15109674632549286 + 10.0 * 6.247986316680908
Epoch 1100, val loss: 0.9725813269615173
Epoch 1110, training loss: 62.62158966064453 = 0.14671970903873444 + 10.0 * 6.2474870681762695
Epoch 1110, val loss: 0.9775635004043579
Epoch 1120, training loss: 62.58880615234375 = 0.14252397418022156 + 10.0 * 6.244627952575684
Epoch 1120, val loss: 0.9828249216079712
Epoch 1130, training loss: 62.5809211730957 = 0.13849003612995148 + 10.0 * 6.244243144989014
Epoch 1130, val loss: 0.988179087638855
Epoch 1140, training loss: 62.612396240234375 = 0.13458935916423798 + 10.0 * 6.247780799865723
Epoch 1140, val loss: 0.9935047030448914
Epoch 1150, training loss: 62.583717346191406 = 0.1307733952999115 + 10.0 * 6.245294570922852
Epoch 1150, val loss: 0.9987586140632629
Epoch 1160, training loss: 62.583316802978516 = 0.12706929445266724 + 10.0 * 6.245625019073486
Epoch 1160, val loss: 1.004086971282959
Epoch 1170, training loss: 62.55500411987305 = 0.123511902987957 + 10.0 * 6.243149280548096
Epoch 1170, val loss: 1.0094506740570068
Epoch 1180, training loss: 62.539634704589844 = 0.1200818195939064 + 10.0 * 6.241955280303955
Epoch 1180, val loss: 1.0149438381195068
Epoch 1190, training loss: 62.52837371826172 = 0.11676938831806183 + 10.0 * 6.2411603927612305
Epoch 1190, val loss: 1.0203534364700317
Epoch 1200, training loss: 62.52979278564453 = 0.11356551200151443 + 10.0 * 6.241622447967529
Epoch 1200, val loss: 1.0258455276489258
Epoch 1210, training loss: 62.52775573730469 = 0.11045528203248978 + 10.0 * 6.241730213165283
Epoch 1210, val loss: 1.031320571899414
Epoch 1220, training loss: 62.562625885009766 = 0.10742834210395813 + 10.0 * 6.245519638061523
Epoch 1220, val loss: 1.0368176698684692
Epoch 1230, training loss: 62.49896240234375 = 0.10445670783519745 + 10.0 * 6.239450454711914
Epoch 1230, val loss: 1.0423113107681274
Epoch 1240, training loss: 62.49874496459961 = 0.10162296891212463 + 10.0 * 6.239712238311768
Epoch 1240, val loss: 1.0479164123535156
Epoch 1250, training loss: 62.502071380615234 = 0.09890588372945786 + 10.0 * 6.240316390991211
Epoch 1250, val loss: 1.0534330606460571
Epoch 1260, training loss: 62.47737503051758 = 0.09624765813350677 + 10.0 * 6.238112449645996
Epoch 1260, val loss: 1.059004783630371
Epoch 1270, training loss: 62.492218017578125 = 0.09368343651294708 + 10.0 * 6.239853382110596
Epoch 1270, val loss: 1.0645678043365479
Epoch 1280, training loss: 62.47983932495117 = 0.09119746088981628 + 10.0 * 6.238863945007324
Epoch 1280, val loss: 1.0700819492340088
Epoch 1290, training loss: 62.455169677734375 = 0.08877656608819962 + 10.0 * 6.236639499664307
Epoch 1290, val loss: 1.0757187604904175
Epoch 1300, training loss: 62.44404983520508 = 0.08645268529653549 + 10.0 * 6.235759735107422
Epoch 1300, val loss: 1.0812630653381348
Epoch 1310, training loss: 62.45085144042969 = 0.08421628177165985 + 10.0 * 6.236663341522217
Epoch 1310, val loss: 1.0869430303573608
Epoch 1320, training loss: 62.476505279541016 = 0.08202403783798218 + 10.0 * 6.239448070526123
Epoch 1320, val loss: 1.0924075841903687
Epoch 1330, training loss: 62.44612503051758 = 0.07988645881414413 + 10.0 * 6.236623764038086
Epoch 1330, val loss: 1.0979254245758057
Epoch 1340, training loss: 62.4494514465332 = 0.07782121747732162 + 10.0 * 6.237163066864014
Epoch 1340, val loss: 1.1033363342285156
Epoch 1350, training loss: 62.43997573852539 = 0.07583678513765335 + 10.0 * 6.236413955688477
Epoch 1350, val loss: 1.1091054677963257
Epoch 1360, training loss: 62.413265228271484 = 0.07391895353794098 + 10.0 * 6.2339348793029785
Epoch 1360, val loss: 1.1146976947784424
Epoch 1370, training loss: 62.40423583984375 = 0.07206891477108002 + 10.0 * 6.233216762542725
Epoch 1370, val loss: 1.1203739643096924
Epoch 1380, training loss: 62.4041633605957 = 0.07027892023324966 + 10.0 * 6.233388423919678
Epoch 1380, val loss: 1.126105785369873
Epoch 1390, training loss: 62.43125915527344 = 0.06854060292243958 + 10.0 * 6.236271858215332
Epoch 1390, val loss: 1.1316121816635132
Epoch 1400, training loss: 62.44993591308594 = 0.0668262392282486 + 10.0 * 6.238310813903809
Epoch 1400, val loss: 1.137067198753357
Epoch 1410, training loss: 62.41332244873047 = 0.06516548246145248 + 10.0 * 6.23481559753418
Epoch 1410, val loss: 1.1428512334823608
Epoch 1420, training loss: 62.3792724609375 = 0.06356175988912582 + 10.0 * 6.231571197509766
Epoch 1420, val loss: 1.1484280824661255
Epoch 1430, training loss: 62.369789123535156 = 0.062032125890254974 + 10.0 * 6.230775833129883
Epoch 1430, val loss: 1.1540590524673462
Epoch 1440, training loss: 62.37875747680664 = 0.06055884808301926 + 10.0 * 6.231820106506348
Epoch 1440, val loss: 1.1596250534057617
Epoch 1450, training loss: 62.40217590332031 = 0.05911530926823616 + 10.0 * 6.2343058586120605
Epoch 1450, val loss: 1.1650928258895874
Epoch 1460, training loss: 62.37166213989258 = 0.057681627571582794 + 10.0 * 6.231398105621338
Epoch 1460, val loss: 1.1706395149230957
Epoch 1470, training loss: 62.3619270324707 = 0.056320950388908386 + 10.0 * 6.230560779571533
Epoch 1470, val loss: 1.1760731935501099
Epoch 1480, training loss: 62.357208251953125 = 0.055005788803100586 + 10.0 * 6.230220317840576
Epoch 1480, val loss: 1.1816236972808838
Epoch 1490, training loss: 62.368011474609375 = 0.05373228341341019 + 10.0 * 6.2314276695251465
Epoch 1490, val loss: 1.1869027614593506
Epoch 1500, training loss: 62.34646987915039 = 0.05248813331127167 + 10.0 * 6.229398250579834
Epoch 1500, val loss: 1.192422866821289
Epoch 1510, training loss: 62.35356521606445 = 0.05128438025712967 + 10.0 * 6.230227947235107
Epoch 1510, val loss: 1.197763442993164
Epoch 1520, training loss: 62.33753204345703 = 0.05012466013431549 + 10.0 * 6.228740692138672
Epoch 1520, val loss: 1.2031865119934082
Epoch 1530, training loss: 62.35335159301758 = 0.04899849370121956 + 10.0 * 6.230435371398926
Epoch 1530, val loss: 1.208483099937439
Epoch 1540, training loss: 62.33241271972656 = 0.04789130762219429 + 10.0 * 6.228452205657959
Epoch 1540, val loss: 1.2138959169387817
Epoch 1550, training loss: 62.32442855834961 = 0.0468161478638649 + 10.0 * 6.227761268615723
Epoch 1550, val loss: 1.219147801399231
Epoch 1560, training loss: 62.32803726196289 = 0.04578188806772232 + 10.0 * 6.2282257080078125
Epoch 1560, val loss: 1.2244433164596558
Epoch 1570, training loss: 62.33597183227539 = 0.04477224126458168 + 10.0 * 6.229119777679443
Epoch 1570, val loss: 1.229712724685669
Epoch 1580, training loss: 62.320709228515625 = 0.04378649964928627 + 10.0 * 6.227692604064941
Epoch 1580, val loss: 1.2348077297210693
Epoch 1590, training loss: 62.31204605102539 = 0.04284409061074257 + 10.0 * 6.226920127868652
Epoch 1590, val loss: 1.2401602268218994
Epoch 1600, training loss: 62.3070068359375 = 0.041925203055143356 + 10.0 * 6.226508140563965
Epoch 1600, val loss: 1.2453385591506958
Epoch 1610, training loss: 62.29567337036133 = 0.04103678837418556 + 10.0 * 6.225463390350342
Epoch 1610, val loss: 1.2506132125854492
Epoch 1620, training loss: 62.33649826049805 = 0.04018156975507736 + 10.0 * 6.2296319007873535
Epoch 1620, val loss: 1.255761742591858
Epoch 1630, training loss: 62.29925537109375 = 0.0393158383667469 + 10.0 * 6.225994110107422
Epoch 1630, val loss: 1.2607922554016113
Epoch 1640, training loss: 62.305267333984375 = 0.03848907724022865 + 10.0 * 6.226677894592285
Epoch 1640, val loss: 1.2656946182250977
Epoch 1650, training loss: 62.29034423828125 = 0.037688013166189194 + 10.0 * 6.2252655029296875
Epoch 1650, val loss: 1.270816683769226
Epoch 1660, training loss: 62.28153610229492 = 0.03692356124520302 + 10.0 * 6.224461555480957
Epoch 1660, val loss: 1.275854468345642
Epoch 1670, training loss: 62.28260040283203 = 0.03618040308356285 + 10.0 * 6.224642276763916
Epoch 1670, val loss: 1.2809169292449951
Epoch 1680, training loss: 62.33251953125 = 0.03545548766851425 + 10.0 * 6.229706764221191
Epoch 1680, val loss: 1.2859514951705933
Epoch 1690, training loss: 62.29581069946289 = 0.034729890525341034 + 10.0 * 6.226108074188232
Epoch 1690, val loss: 1.2906988859176636
Epoch 1700, training loss: 62.27410888671875 = 0.03403357043862343 + 10.0 * 6.224007606506348
Epoch 1700, val loss: 1.2956897020339966
Epoch 1710, training loss: 62.26046371459961 = 0.03336310759186745 + 10.0 * 6.222710132598877
Epoch 1710, val loss: 1.3006231784820557
Epoch 1720, training loss: 62.27375411987305 = 0.03272514045238495 + 10.0 * 6.224102973937988
Epoch 1720, val loss: 1.3054866790771484
Epoch 1730, training loss: 62.26509475708008 = 0.0320841521024704 + 10.0 * 6.223300933837891
Epoch 1730, val loss: 1.3101965188980103
Epoch 1740, training loss: 62.25752258300781 = 0.0314614400267601 + 10.0 * 6.222606182098389
Epoch 1740, val loss: 1.3148772716522217
Epoch 1750, training loss: 62.26005554199219 = 0.03086674213409424 + 10.0 * 6.222918510437012
Epoch 1750, val loss: 1.3197277784347534
Epoch 1760, training loss: 62.306793212890625 = 0.030278630554676056 + 10.0 * 6.227651596069336
Epoch 1760, val loss: 1.3244459629058838
Epoch 1770, training loss: 62.24687576293945 = 0.02969275787472725 + 10.0 * 6.2217183113098145
Epoch 1770, val loss: 1.328656554222107
Epoch 1780, training loss: 62.24633026123047 = 0.02913307212293148 + 10.0 * 6.221719741821289
Epoch 1780, val loss: 1.3335672616958618
Epoch 1790, training loss: 62.232948303222656 = 0.02860136888921261 + 10.0 * 6.220434665679932
Epoch 1790, val loss: 1.3381130695343018
Epoch 1800, training loss: 62.22984313964844 = 0.028083436191082 + 10.0 * 6.220175743103027
Epoch 1800, val loss: 1.342706561088562
Epoch 1810, training loss: 62.29706573486328 = 0.02758152037858963 + 10.0 * 6.2269487380981445
Epoch 1810, val loss: 1.3471086025238037
Epoch 1820, training loss: 62.25984191894531 = 0.02706959657371044 + 10.0 * 6.2232770919799805
Epoch 1820, val loss: 1.3516488075256348
Epoch 1830, training loss: 62.24462890625 = 0.026575729250907898 + 10.0 * 6.221805095672607
Epoch 1830, val loss: 1.3560547828674316
Epoch 1840, training loss: 62.2227668762207 = 0.02610265463590622 + 10.0 * 6.219666481018066
Epoch 1840, val loss: 1.360658049583435
Epoch 1850, training loss: 62.23016357421875 = 0.02564678154885769 + 10.0 * 6.220451831817627
Epoch 1850, val loss: 1.3650504350662231
Epoch 1860, training loss: 62.228694915771484 = 0.025196634232997894 + 10.0 * 6.2203497886657715
Epoch 1860, val loss: 1.3694273233413696
Epoch 1870, training loss: 62.22816467285156 = 0.02475813589990139 + 10.0 * 6.220340728759766
Epoch 1870, val loss: 1.3737937211990356
Epoch 1880, training loss: 62.21337127685547 = 0.02432856522500515 + 10.0 * 6.2189040184021
Epoch 1880, val loss: 1.3780635595321655
Epoch 1890, training loss: 62.2669563293457 = 0.023920932784676552 + 10.0 * 6.224303245544434
Epoch 1890, val loss: 1.3824189901351929
Epoch 1900, training loss: 62.23073959350586 = 0.023496083915233612 + 10.0 * 6.220724582672119
Epoch 1900, val loss: 1.386412262916565
Epoch 1910, training loss: 62.214515686035156 = 0.023089464753866196 + 10.0 * 6.219142436981201
Epoch 1910, val loss: 1.3907394409179688
Epoch 1920, training loss: 62.21308135986328 = 0.022704308852553368 + 10.0 * 6.2190375328063965
Epoch 1920, val loss: 1.3947831392288208
Epoch 1930, training loss: 62.22138595581055 = 0.02233160473406315 + 10.0 * 6.219905376434326
Epoch 1930, val loss: 1.3989580869674683
Epoch 1940, training loss: 62.19980239868164 = 0.02196078561246395 + 10.0 * 6.2177839279174805
Epoch 1940, val loss: 1.4033080339431763
Epoch 1950, training loss: 62.23273468017578 = 0.021604662761092186 + 10.0 * 6.2211127281188965
Epoch 1950, val loss: 1.407364010810852
Epoch 1960, training loss: 62.20043182373047 = 0.02124466374516487 + 10.0 * 6.217918872833252
Epoch 1960, val loss: 1.411105751991272
Epoch 1970, training loss: 62.193790435791016 = 0.020903483033180237 + 10.0 * 6.217288494110107
Epoch 1970, val loss: 1.4152206182479858
Epoch 1980, training loss: 62.20836639404297 = 0.020570019260048866 + 10.0 * 6.218779563903809
Epoch 1980, val loss: 1.4191604852676392
Epoch 1990, training loss: 62.18735122680664 = 0.020241405814886093 + 10.0 * 6.216711044311523
Epoch 1990, val loss: 1.4231069087982178
Epoch 2000, training loss: 62.1972541809082 = 0.019924072548747063 + 10.0 * 6.217732906341553
Epoch 2000, val loss: 1.4271445274353027
Epoch 2010, training loss: 62.21638107299805 = 0.019608277827501297 + 10.0 * 6.219677448272705
Epoch 2010, val loss: 1.4307681322097778
Epoch 2020, training loss: 62.18878173828125 = 0.019301733002066612 + 10.0 * 6.21694803237915
Epoch 2020, val loss: 1.4347479343414307
Epoch 2030, training loss: 62.17660140991211 = 0.01900319568812847 + 10.0 * 6.215759754180908
Epoch 2030, val loss: 1.4383738040924072
Epoch 2040, training loss: 62.189666748046875 = 0.018716316670179367 + 10.0 * 6.217095375061035
Epoch 2040, val loss: 1.4421722888946533
Epoch 2050, training loss: 62.18461227416992 = 0.01843123883008957 + 10.0 * 6.216618061065674
Epoch 2050, val loss: 1.4459044933319092
Epoch 2060, training loss: 62.20404052734375 = 0.018150614574551582 + 10.0 * 6.218588829040527
Epoch 2060, val loss: 1.4495905637741089
Epoch 2070, training loss: 62.19236373901367 = 0.017874451354146004 + 10.0 * 6.217448711395264
Epoch 2070, val loss: 1.4535144567489624
Epoch 2080, training loss: 62.19028854370117 = 0.01760459877550602 + 10.0 * 6.217268466949463
Epoch 2080, val loss: 1.4570196866989136
Epoch 2090, training loss: 62.16892623901367 = 0.017344456166028976 + 10.0 * 6.215157985687256
Epoch 2090, val loss: 1.4606246948242188
Epoch 2100, training loss: 62.16506576538086 = 0.017089219763875008 + 10.0 * 6.214797496795654
Epoch 2100, val loss: 1.46428382396698
Epoch 2110, training loss: 62.196449279785156 = 0.016843270510435104 + 10.0 * 6.217960834503174
Epoch 2110, val loss: 1.4679263830184937
Epoch 2120, training loss: 62.161659240722656 = 0.01659831777215004 + 10.0 * 6.214506149291992
Epoch 2120, val loss: 1.4714056253433228
Epoch 2130, training loss: 62.15556716918945 = 0.016361583024263382 + 10.0 * 6.213920593261719
Epoch 2130, val loss: 1.4750527143478394
Epoch 2140, training loss: 62.228599548339844 = 0.01613466814160347 + 10.0 * 6.221246242523193
Epoch 2140, val loss: 1.478497862815857
Epoch 2150, training loss: 62.17082214355469 = 0.015894128009676933 + 10.0 * 6.2154927253723145
Epoch 2150, val loss: 1.4818683862686157
Epoch 2160, training loss: 62.150882720947266 = 0.015668392181396484 + 10.0 * 6.213521480560303
Epoch 2160, val loss: 1.4854261875152588
Epoch 2170, training loss: 62.15667724609375 = 0.015453050844371319 + 10.0 * 6.214122295379639
Epoch 2170, val loss: 1.488713264465332
Epoch 2180, training loss: 62.167152404785156 = 0.01523987203836441 + 10.0 * 6.21519136428833
Epoch 2180, val loss: 1.492065191268921
Epoch 2190, training loss: 62.16126251220703 = 0.015033984556794167 + 10.0 * 6.214622974395752
Epoch 2190, val loss: 1.495400071144104
Epoch 2200, training loss: 62.147796630859375 = 0.014825161546468735 + 10.0 * 6.213297367095947
Epoch 2200, val loss: 1.4987651109695435
Epoch 2210, training loss: 62.155696868896484 = 0.014624404720962048 + 10.0 * 6.214107036590576
Epoch 2210, val loss: 1.5020383596420288
Epoch 2220, training loss: 62.14297103881836 = 0.014425581321120262 + 10.0 * 6.212854385375977
Epoch 2220, val loss: 1.5054974555969238
Epoch 2230, training loss: 62.15061569213867 = 0.014231826178729534 + 10.0 * 6.2136383056640625
Epoch 2230, val loss: 1.508611798286438
Epoch 2240, training loss: 62.14589309692383 = 0.014044058509171009 + 10.0 * 6.213184833526611
Epoch 2240, val loss: 1.5117803812026978
Epoch 2250, training loss: 62.14251708984375 = 0.013858158141374588 + 10.0 * 6.212865829467773
Epoch 2250, val loss: 1.5150134563446045
Epoch 2260, training loss: 62.14004135131836 = 0.013676675036549568 + 10.0 * 6.212636470794678
Epoch 2260, val loss: 1.5184733867645264
Epoch 2270, training loss: 62.13519287109375 = 0.013498983345925808 + 10.0 * 6.212169170379639
Epoch 2270, val loss: 1.5215574502944946
Epoch 2280, training loss: 62.135955810546875 = 0.013324687257409096 + 10.0 * 6.212263107299805
Epoch 2280, val loss: 1.5246524810791016
Epoch 2290, training loss: 62.154876708984375 = 0.013153338804841042 + 10.0 * 6.21417236328125
Epoch 2290, val loss: 1.5278593301773071
Epoch 2300, training loss: 62.14057159423828 = 0.01298116147518158 + 10.0 * 6.212759017944336
Epoch 2300, val loss: 1.5306984186172485
Epoch 2310, training loss: 62.16019058227539 = 0.012816241942346096 + 10.0 * 6.214737415313721
Epoch 2310, val loss: 1.5337070226669312
Epoch 2320, training loss: 62.1324348449707 = 0.01265293825417757 + 10.0 * 6.211977958679199
Epoch 2320, val loss: 1.5366990566253662
Epoch 2330, training loss: 62.11450958251953 = 0.01249222457408905 + 10.0 * 6.210201740264893
Epoch 2330, val loss: 1.5397634506225586
Epoch 2340, training loss: 62.11563491821289 = 0.012340549379587173 + 10.0 * 6.210329532623291
Epoch 2340, val loss: 1.5427582263946533
Epoch 2350, training loss: 62.12438201904297 = 0.01219173800200224 + 10.0 * 6.21121883392334
Epoch 2350, val loss: 1.5457528829574585
Epoch 2360, training loss: 62.144073486328125 = 0.01204360369592905 + 10.0 * 6.213202953338623
Epoch 2360, val loss: 1.5485023260116577
Epoch 2370, training loss: 62.129512786865234 = 0.011892811395227909 + 10.0 * 6.211761951446533
Epoch 2370, val loss: 1.5514814853668213
Epoch 2380, training loss: 62.11746597290039 = 0.011746939271688461 + 10.0 * 6.210572242736816
Epoch 2380, val loss: 1.5543491840362549
Epoch 2390, training loss: 62.114501953125 = 0.011608835309743881 + 10.0 * 6.210289478302002
Epoch 2390, val loss: 1.557236909866333
Epoch 2400, training loss: 62.128173828125 = 0.011472800746560097 + 10.0 * 6.211669921875
Epoch 2400, val loss: 1.560043454170227
Epoch 2410, training loss: 62.11687469482422 = 0.011337735690176487 + 10.0 * 6.2105536460876465
Epoch 2410, val loss: 1.5631247758865356
Epoch 2420, training loss: 62.12071990966797 = 0.011201147921383381 + 10.0 * 6.210951805114746
Epoch 2420, val loss: 1.5656980276107788
Epoch 2430, training loss: 62.114410400390625 = 0.011066464707255363 + 10.0 * 6.210334300994873
Epoch 2430, val loss: 1.5683516263961792
Epoch 2440, training loss: 62.103782653808594 = 0.01093871146440506 + 10.0 * 6.20928430557251
Epoch 2440, val loss: 1.5709969997406006
Epoch 2450, training loss: 62.09013366699219 = 0.010813914239406586 + 10.0 * 6.207931995391846
Epoch 2450, val loss: 1.5738078355789185
Epoch 2460, training loss: 62.0954704284668 = 0.010694148950278759 + 10.0 * 6.208477973937988
Epoch 2460, val loss: 1.576384425163269
Epoch 2470, training loss: 62.147769927978516 = 0.010577446781098843 + 10.0 * 6.213719367980957
Epoch 2470, val loss: 1.5789315700531006
Epoch 2480, training loss: 62.12153625488281 = 0.010451224632561207 + 10.0 * 6.211108207702637
Epoch 2480, val loss: 1.5817416906356812
Epoch 2490, training loss: 62.121307373046875 = 0.010330982506275177 + 10.0 * 6.211097717285156
Epoch 2490, val loss: 1.5840479135513306
Epoch 2500, training loss: 62.10384750366211 = 0.010213671252131462 + 10.0 * 6.2093634605407715
Epoch 2500, val loss: 1.587065577507019
Epoch 2510, training loss: 62.08466720581055 = 0.010099473409354687 + 10.0 * 6.207456588745117
Epoch 2510, val loss: 1.5894153118133545
Epoch 2520, training loss: 62.09514617919922 = 0.009991398081183434 + 10.0 * 6.208515644073486
Epoch 2520, val loss: 1.5920287370681763
Epoch 2530, training loss: 62.10411071777344 = 0.009882176294922829 + 10.0 * 6.209422588348389
Epoch 2530, val loss: 1.5945621728897095
Epoch 2540, training loss: 62.10204315185547 = 0.009775643236935139 + 10.0 * 6.209226608276367
Epoch 2540, val loss: 1.597160816192627
Epoch 2550, training loss: 62.10387420654297 = 0.009667839854955673 + 10.0 * 6.209420680999756
Epoch 2550, val loss: 1.5994784832000732
Epoch 2560, training loss: 62.08076858520508 = 0.00956242997199297 + 10.0 * 6.207120418548584
Epoch 2560, val loss: 1.602171778678894
Epoch 2570, training loss: 62.09539794921875 = 0.009462038055062294 + 10.0 * 6.208593845367432
Epoch 2570, val loss: 1.6045024394989014
Epoch 2580, training loss: 62.08592224121094 = 0.009360099211335182 + 10.0 * 6.207655906677246
Epoch 2580, val loss: 1.6068466901779175
Epoch 2590, training loss: 62.10359191894531 = 0.00926138460636139 + 10.0 * 6.209433078765869
Epoch 2590, val loss: 1.6091488599777222
Epoch 2600, training loss: 62.07891845703125 = 0.00916204508394003 + 10.0 * 6.20697546005249
Epoch 2600, val loss: 1.6117349863052368
Epoch 2610, training loss: 62.07787322998047 = 0.009067565202713013 + 10.0 * 6.206880569458008
Epoch 2610, val loss: 1.613925814628601
Epoch 2620, training loss: 62.079978942871094 = 0.008976534940302372 + 10.0 * 6.2071003913879395
Epoch 2620, val loss: 1.6163884401321411
Epoch 2630, training loss: 62.11941146850586 = 0.008885227143764496 + 10.0 * 6.211052894592285
Epoch 2630, val loss: 1.618682622909546
Epoch 2640, training loss: 62.07197570800781 = 0.008791348896920681 + 10.0 * 6.206318378448486
Epoch 2640, val loss: 1.6210271120071411
Epoch 2650, training loss: 62.06010055541992 = 0.008702005259692669 + 10.0 * 6.205140113830566
Epoch 2650, val loss: 1.6232006549835205
Epoch 2660, training loss: 62.0872802734375 = 0.008618537336587906 + 10.0 * 6.207866191864014
Epoch 2660, val loss: 1.6254314184188843
Epoch 2670, training loss: 62.065208435058594 = 0.008529762737452984 + 10.0 * 6.205667972564697
Epoch 2670, val loss: 1.6277446746826172
Epoch 2680, training loss: 62.08124542236328 = 0.0084467101842165 + 10.0 * 6.207280158996582
Epoch 2680, val loss: 1.6299995183944702
Epoch 2690, training loss: 62.07630157470703 = 0.008363522589206696 + 10.0 * 6.206793785095215
Epoch 2690, val loss: 1.632079839706421
Epoch 2700, training loss: 62.082889556884766 = 0.008280602283775806 + 10.0 * 6.207460880279541
Epoch 2700, val loss: 1.6346360445022583
Epoch 2710, training loss: 62.0609245300293 = 0.008198457770049572 + 10.0 * 6.205272674560547
Epoch 2710, val loss: 1.6365095376968384
Epoch 2720, training loss: 62.05777359008789 = 0.008119035512208939 + 10.0 * 6.204965591430664
Epoch 2720, val loss: 1.6386650800704956
Epoch 2730, training loss: 62.06080627441406 = 0.008043132722377777 + 10.0 * 6.2052764892578125
Epoch 2730, val loss: 1.6408181190490723
Epoch 2740, training loss: 62.0810661315918 = 0.007967431098222733 + 10.0 * 6.207309722900391
Epoch 2740, val loss: 1.6428072452545166
Epoch 2750, training loss: 62.08451843261719 = 0.007889499887824059 + 10.0 * 6.207663059234619
Epoch 2750, val loss: 1.6448429822921753
Epoch 2760, training loss: 62.05680465698242 = 0.007813871838152409 + 10.0 * 6.204899311065674
Epoch 2760, val loss: 1.6470133066177368
Epoch 2770, training loss: 62.06025314331055 = 0.007741531357169151 + 10.0 * 6.205251216888428
Epoch 2770, val loss: 1.6490864753723145
Epoch 2780, training loss: 62.04790115356445 = 0.007669260259717703 + 10.0 * 6.204023361206055
Epoch 2780, val loss: 1.6510424613952637
Epoch 2790, training loss: 62.04719543457031 = 0.007600767537951469 + 10.0 * 6.2039594650268555
Epoch 2790, val loss: 1.6532056331634521
Epoch 2800, training loss: 62.083335876464844 = 0.0075340005569159985 + 10.0 * 6.207580089569092
Epoch 2800, val loss: 1.6554213762283325
Epoch 2810, training loss: 62.04669189453125 = 0.0074642193503677845 + 10.0 * 6.203922748565674
Epoch 2810, val loss: 1.6568776369094849
Epoch 2820, training loss: 62.03948211669922 = 0.0073952507227659225 + 10.0 * 6.2032084465026855
Epoch 2820, val loss: 1.6589620113372803
Epoch 2830, training loss: 62.06486129760742 = 0.007330016233026981 + 10.0 * 6.205752849578857
Epoch 2830, val loss: 1.6610597372055054
Epoch 2840, training loss: 62.05287170410156 = 0.007262192200869322 + 10.0 * 6.20456075668335
Epoch 2840, val loss: 1.662724256515503
Epoch 2850, training loss: 62.04783630371094 = 0.007197014521807432 + 10.0 * 6.204063892364502
Epoch 2850, val loss: 1.6647735834121704
Epoch 2860, training loss: 62.065982818603516 = 0.007135446649044752 + 10.0 * 6.2058844566345215
Epoch 2860, val loss: 1.6666998863220215
Epoch 2870, training loss: 62.047142028808594 = 0.007070728112012148 + 10.0 * 6.204007148742676
Epoch 2870, val loss: 1.6685765981674194
Epoch 2880, training loss: 62.050106048583984 = 0.007008241955190897 + 10.0 * 6.204309940338135
Epoch 2880, val loss: 1.6702550649642944
Epoch 2890, training loss: 62.07545852661133 = 0.006947225425392389 + 10.0 * 6.206851005554199
Epoch 2890, val loss: 1.6720528602600098
Epoch 2900, training loss: 62.038021087646484 = 0.0068854172714054585 + 10.0 * 6.203113555908203
Epoch 2900, val loss: 1.6739031076431274
Epoch 2910, training loss: 62.02519226074219 = 0.0068270256742835045 + 10.0 * 6.201836585998535
Epoch 2910, val loss: 1.6757041215896606
Epoch 2920, training loss: 62.029991149902344 = 0.006772051565349102 + 10.0 * 6.202322006225586
Epoch 2920, val loss: 1.6774259805679321
Epoch 2930, training loss: 62.09627914428711 = 0.006717335898429155 + 10.0 * 6.208956241607666
Epoch 2930, val loss: 1.6790902614593506
Epoch 2940, training loss: 62.05656814575195 = 0.006655352655798197 + 10.0 * 6.204991340637207
Epoch 2940, val loss: 1.6811915636062622
Epoch 2950, training loss: 62.04127883911133 = 0.006597313564270735 + 10.0 * 6.203468322753906
Epoch 2950, val loss: 1.6824305057525635
Epoch 2960, training loss: 62.03874206542969 = 0.006542686838656664 + 10.0 * 6.203219890594482
Epoch 2960, val loss: 1.6844236850738525
Epoch 2970, training loss: 62.03519821166992 = 0.006489724852144718 + 10.0 * 6.202870845794678
Epoch 2970, val loss: 1.68600332736969
Epoch 2980, training loss: 62.04179000854492 = 0.006437983829528093 + 10.0 * 6.203535079956055
Epoch 2980, val loss: 1.6877539157867432
Epoch 2990, training loss: 62.02781295776367 = 0.0063844360411167145 + 10.0 * 6.202142715454102
Epoch 2990, val loss: 1.689136028289795
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8386926726410122
The final CL Acc:0.77901, 0.01522, The final GNN Acc:0.83887, 0.00025
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10510])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.92961120605469 = 1.9617308378219604 + 10.0 * 8.59678840637207
Epoch 0, val loss: 1.9705986976623535
Epoch 10, training loss: 87.91046142578125 = 1.9511752128601074 + 10.0 * 8.595929145812988
Epoch 10, val loss: 1.9603748321533203
Epoch 20, training loss: 87.8409423828125 = 1.938321828842163 + 10.0 * 8.590261459350586
Epoch 20, val loss: 1.9474295377731323
Epoch 30, training loss: 87.46794891357422 = 1.921778678894043 + 10.0 * 8.554616928100586
Epoch 30, val loss: 1.9303165674209595
Epoch 40, training loss: 85.29862213134766 = 1.9033769369125366 + 10.0 * 8.33952522277832
Epoch 40, val loss: 1.9114829301834106
Epoch 50, training loss: 77.10381317138672 = 1.883408546447754 + 10.0 * 7.522040843963623
Epoch 50, val loss: 1.8905092477798462
Epoch 60, training loss: 73.82852172851562 = 1.868065595626831 + 10.0 * 7.196045875549316
Epoch 60, val loss: 1.8756026029586792
Epoch 70, training loss: 72.23697662353516 = 1.8570711612701416 + 10.0 * 7.037990570068359
Epoch 70, val loss: 1.864076852798462
Epoch 80, training loss: 71.24068450927734 = 1.845632553100586 + 10.0 * 6.939505100250244
Epoch 80, val loss: 1.8520339727401733
Epoch 90, training loss: 70.34830474853516 = 1.8359869718551636 + 10.0 * 6.851232051849365
Epoch 90, val loss: 1.8416383266448975
Epoch 100, training loss: 69.51860809326172 = 1.8272639513015747 + 10.0 * 6.769134521484375
Epoch 100, val loss: 1.832230567932129
Epoch 110, training loss: 68.94119262695312 = 1.8202128410339355 + 10.0 * 6.712097644805908
Epoch 110, val loss: 1.8244738578796387
Epoch 120, training loss: 68.50431060791016 = 1.8129923343658447 + 10.0 * 6.669131278991699
Epoch 120, val loss: 1.816473364830017
Epoch 130, training loss: 68.13246154785156 = 1.8053555488586426 + 10.0 * 6.632709980010986
Epoch 130, val loss: 1.8079168796539307
Epoch 140, training loss: 67.82726287841797 = 1.7978910207748413 + 10.0 * 6.6029372215271
Epoch 140, val loss: 1.7997266054153442
Epoch 150, training loss: 67.56336975097656 = 1.790601372718811 + 10.0 * 6.577276706695557
Epoch 150, val loss: 1.7920472621917725
Epoch 160, training loss: 67.33092498779297 = 1.7832539081573486 + 10.0 * 6.554766654968262
Epoch 160, val loss: 1.7845369577407837
Epoch 170, training loss: 67.10366821289062 = 1.7756611108779907 + 10.0 * 6.532801151275635
Epoch 170, val loss: 1.7772752046585083
Epoch 180, training loss: 66.91262817382812 = 1.7677485942840576 + 10.0 * 6.514488220214844
Epoch 180, val loss: 1.7698838710784912
Epoch 190, training loss: 66.78739166259766 = 1.7592341899871826 + 10.0 * 6.5028157234191895
Epoch 190, val loss: 1.762216329574585
Epoch 200, training loss: 66.59967041015625 = 1.7501546144485474 + 10.0 * 6.484951972961426
Epoch 200, val loss: 1.754051923751831
Epoch 210, training loss: 66.47337341308594 = 1.7402825355529785 + 10.0 * 6.473309516906738
Epoch 210, val loss: 1.7455589771270752
Epoch 220, training loss: 66.33718872070312 = 1.729527235031128 + 10.0 * 6.460765838623047
Epoch 220, val loss: 1.7365647554397583
Epoch 230, training loss: 66.2257080078125 = 1.7179515361785889 + 10.0 * 6.450775623321533
Epoch 230, val loss: 1.72687828540802
Epoch 240, training loss: 66.1122055053711 = 1.705402135848999 + 10.0 * 6.440680503845215
Epoch 240, val loss: 1.7165734767913818
Epoch 250, training loss: 65.99437713623047 = 1.6918333768844604 + 10.0 * 6.4302544593811035
Epoch 250, val loss: 1.7053266763687134
Epoch 260, training loss: 65.94718933105469 = 1.6771591901779175 + 10.0 * 6.427002906799316
Epoch 260, val loss: 1.6933517456054688
Epoch 270, training loss: 65.79716491699219 = 1.6611231565475464 + 10.0 * 6.413604259490967
Epoch 270, val loss: 1.6805403232574463
Epoch 280, training loss: 65.68279266357422 = 1.643977403640747 + 10.0 * 6.403882026672363
Epoch 280, val loss: 1.6667718887329102
Epoch 290, training loss: 65.59019470214844 = 1.6256158351898193 + 10.0 * 6.396458148956299
Epoch 290, val loss: 1.6521190404891968
Epoch 300, training loss: 65.59114074707031 = 1.6060357093811035 + 10.0 * 6.398510932922363
Epoch 300, val loss: 1.6364299058914185
Epoch 310, training loss: 65.4378433227539 = 1.5849661827087402 + 10.0 * 6.385287761688232
Epoch 310, val loss: 1.6198079586029053
Epoch 320, training loss: 65.33772277832031 = 1.5628857612609863 + 10.0 * 6.37748384475708
Epoch 320, val loss: 1.6024787425994873
Epoch 330, training loss: 65.26069641113281 = 1.539745569229126 + 10.0 * 6.372094631195068
Epoch 330, val loss: 1.5843359231948853
Epoch 340, training loss: 65.23008728027344 = 1.5154595375061035 + 10.0 * 6.371462821960449
Epoch 340, val loss: 1.565295934677124
Epoch 350, training loss: 65.113037109375 = 1.49025559425354 + 10.0 * 6.362278461456299
Epoch 350, val loss: 1.5456697940826416
Epoch 360, training loss: 65.040283203125 = 1.4642452001571655 + 10.0 * 6.357603549957275
Epoch 360, val loss: 1.5255722999572754
Epoch 370, training loss: 65.02227783203125 = 1.4376145601272583 + 10.0 * 6.358466148376465
Epoch 370, val loss: 1.5048524141311646
Epoch 380, training loss: 64.90260314941406 = 1.4100420475006104 + 10.0 * 6.3492560386657715
Epoch 380, val loss: 1.484102487564087
Epoch 390, training loss: 64.8371810913086 = 1.3820984363555908 + 10.0 * 6.345508098602295
Epoch 390, val loss: 1.4631701707839966
Epoch 400, training loss: 64.82537078857422 = 1.3536725044250488 + 10.0 * 6.347169399261475
Epoch 400, val loss: 1.442016839981079
Epoch 410, training loss: 64.7069091796875 = 1.3250006437301636 + 10.0 * 6.338191032409668
Epoch 410, val loss: 1.4208184480667114
Epoch 420, training loss: 64.64848327636719 = 1.2961231470108032 + 10.0 * 6.335236549377441
Epoch 420, val loss: 1.3998082876205444
Epoch 430, training loss: 64.62613677978516 = 1.2669833898544312 + 10.0 * 6.3359150886535645
Epoch 430, val loss: 1.3790792226791382
Epoch 440, training loss: 64.53124237060547 = 1.237827181816101 + 10.0 * 6.329341411590576
Epoch 440, val loss: 1.3584686517715454
Epoch 450, training loss: 64.4697036743164 = 1.2087453603744507 + 10.0 * 6.326096057891846
Epoch 450, val loss: 1.3384431600570679
Epoch 460, training loss: 64.41475677490234 = 1.1797971725463867 + 10.0 * 6.323495864868164
Epoch 460, val loss: 1.31891930103302
Epoch 470, training loss: 64.42465209960938 = 1.1509836912155151 + 10.0 * 6.327366828918457
Epoch 470, val loss: 1.299830436706543
Epoch 480, training loss: 64.32239532470703 = 1.122442603111267 + 10.0 * 6.319994926452637
Epoch 480, val loss: 1.2813632488250732
Epoch 490, training loss: 64.25993347167969 = 1.0944182872772217 + 10.0 * 6.316551208496094
Epoch 490, val loss: 1.2637920379638672
Epoch 500, training loss: 64.20716094970703 = 1.0669766664505005 + 10.0 * 6.314018249511719
Epoch 500, val loss: 1.2470816373825073
Epoch 510, training loss: 64.16357421875 = 1.040018081665039 + 10.0 * 6.3123555183410645
Epoch 510, val loss: 1.2311842441558838
Epoch 520, training loss: 64.14086151123047 = 1.0135799646377563 + 10.0 * 6.312728404998779
Epoch 520, val loss: 1.215938687324524
Epoch 530, training loss: 64.08026123046875 = 0.9876819252967834 + 10.0 * 6.309257984161377
Epoch 530, val loss: 1.2017446756362915
Epoch 540, training loss: 64.02436828613281 = 0.9626937508583069 + 10.0 * 6.306167125701904
Epoch 540, val loss: 1.1883082389831543
Epoch 550, training loss: 63.981021881103516 = 0.9384402632713318 + 10.0 * 6.304258346557617
Epoch 550, val loss: 1.1758906841278076
Epoch 560, training loss: 63.97182846069336 = 0.9147728681564331 + 10.0 * 6.305705547332764
Epoch 560, val loss: 1.1642988920211792
Epoch 570, training loss: 63.93682098388672 = 0.8919442296028137 + 10.0 * 6.304487705230713
Epoch 570, val loss: 1.1532727479934692
Epoch 580, training loss: 63.866939544677734 = 0.8697623610496521 + 10.0 * 6.299717903137207
Epoch 580, val loss: 1.1430315971374512
Epoch 590, training loss: 63.82233810424805 = 0.848365306854248 + 10.0 * 6.297397136688232
Epoch 590, val loss: 1.133690595626831
Epoch 600, training loss: 63.81281661987305 = 0.8276990652084351 + 10.0 * 6.298511981964111
Epoch 600, val loss: 1.124842643737793
Epoch 610, training loss: 63.771217346191406 = 0.8075011968612671 + 10.0 * 6.2963714599609375
Epoch 610, val loss: 1.1169291734695435
Epoch 620, training loss: 63.722320556640625 = 0.7880581021308899 + 10.0 * 6.293426036834717
Epoch 620, val loss: 1.1090664863586426
Epoch 630, training loss: 63.71281814575195 = 0.7691830396652222 + 10.0 * 6.294363498687744
Epoch 630, val loss: 1.1018476486206055
Epoch 640, training loss: 63.6590690612793 = 0.750849187374115 + 10.0 * 6.2908220291137695
Epoch 640, val loss: 1.095263957977295
Epoch 650, training loss: 63.61841583251953 = 0.7330874800682068 + 10.0 * 6.2885332107543945
Epoch 650, val loss: 1.0890411138534546
Epoch 660, training loss: 63.593074798583984 = 0.7157767415046692 + 10.0 * 6.287729740142822
Epoch 660, val loss: 1.083195686340332
Epoch 670, training loss: 63.57620620727539 = 0.6988470554351807 + 10.0 * 6.287735939025879
Epoch 670, val loss: 1.077571988105774
Epoch 680, training loss: 63.53676986694336 = 0.6822313666343689 + 10.0 * 6.285453796386719
Epoch 680, val loss: 1.0721944570541382
Epoch 690, training loss: 63.5101203918457 = 0.6661113500595093 + 10.0 * 6.284400939941406
Epoch 690, val loss: 1.0672036409378052
Epoch 700, training loss: 63.49257278442383 = 0.6502822041511536 + 10.0 * 6.284228801727295
Epoch 700, val loss: 1.0623340606689453
Epoch 710, training loss: 63.452510833740234 = 0.6347134113311768 + 10.0 * 6.281779766082764
Epoch 710, val loss: 1.057690143585205
Epoch 720, training loss: 63.424190521240234 = 0.6194963455200195 + 10.0 * 6.2804694175720215
Epoch 720, val loss: 1.0533599853515625
Epoch 730, training loss: 63.416011810302734 = 0.604499101638794 + 10.0 * 6.281151294708252
Epoch 730, val loss: 1.0492280721664429
Epoch 740, training loss: 63.37928009033203 = 0.5896664261817932 + 10.0 * 6.278961181640625
Epoch 740, val loss: 1.045180082321167
Epoch 750, training loss: 63.35370635986328 = 0.5751006007194519 + 10.0 * 6.277860641479492
Epoch 750, val loss: 1.041284203529358
Epoch 760, training loss: 63.32732391357422 = 0.5607833862304688 + 10.0 * 6.276654243469238
Epoch 760, val loss: 1.0373685359954834
Epoch 770, training loss: 63.30677795410156 = 0.5465972423553467 + 10.0 * 6.276018142700195
Epoch 770, val loss: 1.033858299255371
Epoch 780, training loss: 63.28078079223633 = 0.5326000452041626 + 10.0 * 6.274817943572998
Epoch 780, val loss: 1.0303623676300049
Epoch 790, training loss: 63.27485275268555 = 0.5188857316970825 + 10.0 * 6.275596618652344
Epoch 790, val loss: 1.0271434783935547
Epoch 800, training loss: 63.227420806884766 = 0.5052679777145386 + 10.0 * 6.272215366363525
Epoch 800, val loss: 1.023803472518921
Epoch 810, training loss: 63.195011138916016 = 0.49194544553756714 + 10.0 * 6.270306587219238
Epoch 810, val loss: 1.0208700895309448
Epoch 820, training loss: 63.17241668701172 = 0.4788217544555664 + 10.0 * 6.269359588623047
Epoch 820, val loss: 1.0180848836898804
Epoch 830, training loss: 63.21173858642578 = 0.4658522307872772 + 10.0 * 6.274588584899902
Epoch 830, val loss: 1.0153894424438477
Epoch 840, training loss: 63.14919662475586 = 0.4530363976955414 + 10.0 * 6.26961612701416
Epoch 840, val loss: 1.0129092931747437
Epoch 850, training loss: 63.113311767578125 = 0.4404589533805847 + 10.0 * 6.267285346984863
Epoch 850, val loss: 1.010590672492981
Epoch 860, training loss: 63.201778411865234 = 0.42808249592781067 + 10.0 * 6.277369499206543
Epoch 860, val loss: 1.0083242654800415
Epoch 870, training loss: 63.073097229003906 = 0.41593995690345764 + 10.0 * 6.265715599060059
Epoch 870, val loss: 1.0063378810882568
Epoch 880, training loss: 63.052452087402344 = 0.40415042638778687 + 10.0 * 6.264830112457275
Epoch 880, val loss: 1.0044825077056885
Epoch 890, training loss: 63.025856018066406 = 0.3926071524620056 + 10.0 * 6.263324737548828
Epoch 890, val loss: 1.0030916929244995
Epoch 900, training loss: 63.00448989868164 = 0.3812898099422455 + 10.0 * 6.262320041656494
Epoch 900, val loss: 1.001699447631836
Epoch 910, training loss: 62.986141204833984 = 0.370179682970047 + 10.0 * 6.261596202850342
Epoch 910, val loss: 1.000524640083313
Epoch 920, training loss: 63.12229537963867 = 0.3592516779899597 + 10.0 * 6.276304244995117
Epoch 920, val loss: 0.99944007396698
Epoch 930, training loss: 62.993141174316406 = 0.34847086668014526 + 10.0 * 6.264466762542725
Epoch 930, val loss: 0.9980866312980652
Epoch 940, training loss: 62.94725036621094 = 0.33809709548950195 + 10.0 * 6.260915279388428
Epoch 940, val loss: 0.9972950220108032
Epoch 950, training loss: 62.91963195800781 = 0.32802265882492065 + 10.0 * 6.259160995483398
Epoch 950, val loss: 0.9966921210289001
Epoch 960, training loss: 62.89856719970703 = 0.3182072639465332 + 10.0 * 6.258036136627197
Epoch 960, val loss: 0.9962680339813232
Epoch 970, training loss: 62.91853713989258 = 0.3086065649986267 + 10.0 * 6.260993003845215
Epoch 970, val loss: 0.9959399104118347
Epoch 980, training loss: 62.88100814819336 = 0.29925304651260376 + 10.0 * 6.258175849914551
Epoch 980, val loss: 0.9958193302154541
Epoch 990, training loss: 62.8739013671875 = 0.2900931239128113 + 10.0 * 6.258380889892578
Epoch 990, val loss: 0.9955968856811523
Epoch 1000, training loss: 62.839515686035156 = 0.2812569737434387 + 10.0 * 6.255825996398926
Epoch 1000, val loss: 0.9958030581474304
Epoch 1010, training loss: 62.82534408569336 = 0.2726992964744568 + 10.0 * 6.255264759063721
Epoch 1010, val loss: 0.9961570501327515
Epoch 1020, training loss: 62.81169891357422 = 0.2643596827983856 + 10.0 * 6.254734039306641
Epoch 1020, val loss: 0.996428906917572
Epoch 1030, training loss: 62.8285026550293 = 0.2562723457813263 + 10.0 * 6.257223129272461
Epoch 1030, val loss: 0.9970111846923828
Epoch 1040, training loss: 62.779361724853516 = 0.24835745990276337 + 10.0 * 6.253100395202637
Epoch 1040, val loss: 0.9974862337112427
Epoch 1050, training loss: 62.7794189453125 = 0.24074722826480865 + 10.0 * 6.253867149353027
Epoch 1050, val loss: 0.9983763694763184
Epoch 1060, training loss: 62.75090026855469 = 0.23338934779167175 + 10.0 * 6.251750946044922
Epoch 1060, val loss: 0.9990746378898621
Epoch 1070, training loss: 62.744407653808594 = 0.22624853253364563 + 10.0 * 6.2518157958984375
Epoch 1070, val loss: 1.0001405477523804
Epoch 1080, training loss: 62.74908447265625 = 0.21937356889247894 + 10.0 * 6.252971172332764
Epoch 1080, val loss: 1.0011796951293945
Epoch 1090, training loss: 62.71684265136719 = 0.21269835531711578 + 10.0 * 6.2504143714904785
Epoch 1090, val loss: 1.0024800300598145
Epoch 1100, training loss: 62.71632385253906 = 0.20626242458820343 + 10.0 * 6.251006126403809
Epoch 1100, val loss: 1.003714680671692
Epoch 1110, training loss: 62.70362091064453 = 0.2000146210193634 + 10.0 * 6.250360488891602
Epoch 1110, val loss: 1.005116581916809
Epoch 1120, training loss: 62.68010711669922 = 0.1939978003501892 + 10.0 * 6.248610973358154
Epoch 1120, val loss: 1.0068224668502808
Epoch 1130, training loss: 62.682518005371094 = 0.188182532787323 + 10.0 * 6.249433517456055
Epoch 1130, val loss: 1.008448839187622
Epoch 1140, training loss: 62.673404693603516 = 0.18255342543125153 + 10.0 * 6.249085426330566
Epoch 1140, val loss: 1.0101134777069092
Epoch 1150, training loss: 62.656368255615234 = 0.1770908683538437 + 10.0 * 6.247927665710449
Epoch 1150, val loss: 1.0119708776474
Epoch 1160, training loss: 62.638465881347656 = 0.1718585044145584 + 10.0 * 6.2466607093811035
Epoch 1160, val loss: 1.0139250755310059
Epoch 1170, training loss: 62.6265754699707 = 0.16680431365966797 + 10.0 * 6.24597692489624
Epoch 1170, val loss: 1.016115665435791
Epoch 1180, training loss: 62.640869140625 = 0.1619463860988617 + 10.0 * 6.247892379760742
Epoch 1180, val loss: 1.0180951356887817
Epoch 1190, training loss: 62.616432189941406 = 0.1572103649377823 + 10.0 * 6.245922088623047
Epoch 1190, val loss: 1.0203872919082642
Epoch 1200, training loss: 62.61854934692383 = 0.15263628959655762 + 10.0 * 6.246591091156006
Epoch 1200, val loss: 1.0226038694381714
Epoch 1210, training loss: 62.59553909301758 = 0.14825038611888885 + 10.0 * 6.244729042053223
Epoch 1210, val loss: 1.0247474908828735
Epoch 1220, training loss: 62.58586502075195 = 0.14401867985725403 + 10.0 * 6.244184494018555
Epoch 1220, val loss: 1.0271484851837158
Epoch 1230, training loss: 62.565948486328125 = 0.13995058834552765 + 10.0 * 6.242599964141846
Epoch 1230, val loss: 1.0296728610992432
Epoch 1240, training loss: 62.574249267578125 = 0.13602586090564728 + 10.0 * 6.2438225746154785
Epoch 1240, val loss: 1.0323970317840576
Epoch 1250, training loss: 62.552799224853516 = 0.13221319019794464 + 10.0 * 6.242058753967285
Epoch 1250, val loss: 1.0346927642822266
Epoch 1260, training loss: 62.547462463378906 = 0.12854208052158356 + 10.0 * 6.241891860961914
Epoch 1260, val loss: 1.0373848676681519
Epoch 1270, training loss: 62.550506591796875 = 0.12500180304050446 + 10.0 * 6.242550849914551
Epoch 1270, val loss: 1.0402710437774658
Epoch 1280, training loss: 62.55641174316406 = 0.12156904488801956 + 10.0 * 6.243484020233154
Epoch 1280, val loss: 1.0428749322891235
Epoch 1290, training loss: 62.516883850097656 = 0.1182354986667633 + 10.0 * 6.239864826202393
Epoch 1290, val loss: 1.0456290245056152
Epoch 1300, training loss: 62.50337219238281 = 0.11505726724863052 + 10.0 * 6.238831520080566
Epoch 1300, val loss: 1.0486170053482056
Epoch 1310, training loss: 62.50802230834961 = 0.11199422925710678 + 10.0 * 6.239602565765381
Epoch 1310, val loss: 1.0514425039291382
Epoch 1320, training loss: 62.49892807006836 = 0.109006866812706 + 10.0 * 6.238992214202881
Epoch 1320, val loss: 1.0543326139450073
Epoch 1330, training loss: 62.501853942871094 = 0.10613327473402023 + 10.0 * 6.239572048187256
Epoch 1330, val loss: 1.0572389364242554
Epoch 1340, training loss: 62.496612548828125 = 0.1033439114689827 + 10.0 * 6.2393269538879395
Epoch 1340, val loss: 1.0602787733078003
Epoch 1350, training loss: 62.474998474121094 = 0.10065355896949768 + 10.0 * 6.237434387207031
Epoch 1350, val loss: 1.0632882118225098
Epoch 1360, training loss: 62.48169708251953 = 0.09806329011917114 + 10.0 * 6.238363265991211
Epoch 1360, val loss: 1.0663810968399048
Epoch 1370, training loss: 62.45883560180664 = 0.09552351385354996 + 10.0 * 6.236330986022949
Epoch 1370, val loss: 1.0693223476409912
Epoch 1380, training loss: 62.4660530090332 = 0.09310410171747208 + 10.0 * 6.237294673919678
Epoch 1380, val loss: 1.0724109411239624
Epoch 1390, training loss: 62.48271560668945 = 0.09074544906616211 + 10.0 * 6.239197254180908
Epoch 1390, val loss: 1.0755404233932495
Epoch 1400, training loss: 62.43403625488281 = 0.08843793720006943 + 10.0 * 6.234560012817383
Epoch 1400, val loss: 1.0786242485046387
Epoch 1410, training loss: 62.427066802978516 = 0.08624551445245743 + 10.0 * 6.234082221984863
Epoch 1410, val loss: 1.0819238424301147
Epoch 1420, training loss: 62.4240608215332 = 0.08412190526723862 + 10.0 * 6.233994007110596
Epoch 1420, val loss: 1.0851961374282837
Epoch 1430, training loss: 62.48197555541992 = 0.0820733979344368 + 10.0 * 6.239990234375
Epoch 1430, val loss: 1.0883111953735352
Epoch 1440, training loss: 62.42970657348633 = 0.08003786206245422 + 10.0 * 6.234967231750488
Epoch 1440, val loss: 1.091772437095642
Epoch 1450, training loss: 62.42750930786133 = 0.0780901163816452 + 10.0 * 6.2349419593811035
Epoch 1450, val loss: 1.094903588294983
Epoch 1460, training loss: 62.42122268676758 = 0.07620517164468765 + 10.0 * 6.234501838684082
Epoch 1460, val loss: 1.098322868347168
Epoch 1470, training loss: 62.392024993896484 = 0.07437398284673691 + 10.0 * 6.231764793395996
Epoch 1470, val loss: 1.1016688346862793
Epoch 1480, training loss: 62.38692092895508 = 0.07261690497398376 + 10.0 * 6.231430530548096
Epoch 1480, val loss: 1.1050175428390503
Epoch 1490, training loss: 62.415870666503906 = 0.07092348486185074 + 10.0 * 6.234494686126709
Epoch 1490, val loss: 1.1083087921142578
Epoch 1500, training loss: 62.378570556640625 = 0.06925150752067566 + 10.0 * 6.230931758880615
Epoch 1500, val loss: 1.1116437911987305
Epoch 1510, training loss: 62.37345504760742 = 0.06764375418424606 + 10.0 * 6.230581283569336
Epoch 1510, val loss: 1.1150907278060913
Epoch 1520, training loss: 62.37662124633789 = 0.06609222292900085 + 10.0 * 6.231052875518799
Epoch 1520, val loss: 1.1184931993484497
Epoch 1530, training loss: 62.389892578125 = 0.06458710134029388 + 10.0 * 6.23253059387207
Epoch 1530, val loss: 1.1220004558563232
Epoch 1540, training loss: 62.373966217041016 = 0.06312137097120285 + 10.0 * 6.23108434677124
Epoch 1540, val loss: 1.1253604888916016
Epoch 1550, training loss: 62.35182189941406 = 0.061705414205789566 + 10.0 * 6.229011535644531
Epoch 1550, val loss: 1.1288542747497559
Epoch 1560, training loss: 62.35179138183594 = 0.06033967435359955 + 10.0 * 6.229145050048828
Epoch 1560, val loss: 1.1323498487472534
Epoch 1570, training loss: 62.405860900878906 = 0.05902290344238281 + 10.0 * 6.234683513641357
Epoch 1570, val loss: 1.1359833478927612
Epoch 1580, training loss: 62.35054397583008 = 0.057690076529979706 + 10.0 * 6.22928524017334
Epoch 1580, val loss: 1.1391351222991943
Epoch 1590, training loss: 62.33211898803711 = 0.05644097924232483 + 10.0 * 6.227567672729492
Epoch 1590, val loss: 1.1426230669021606
Epoch 1600, training loss: 62.335426330566406 = 0.05523218959569931 + 10.0 * 6.2280192375183105
Epoch 1600, val loss: 1.1461671590805054
Epoch 1610, training loss: 62.37324523925781 = 0.05406146124005318 + 10.0 * 6.2319183349609375
Epoch 1610, val loss: 1.1494933366775513
Epoch 1620, training loss: 62.336490631103516 = 0.05289450287818909 + 10.0 * 6.228359699249268
Epoch 1620, val loss: 1.1530760526657104
Epoch 1630, training loss: 62.32150650024414 = 0.05178428813815117 + 10.0 * 6.2269721031188965
Epoch 1630, val loss: 1.156528353691101
Epoch 1640, training loss: 62.37714767456055 = 0.05070926249027252 + 10.0 * 6.232644081115723
Epoch 1640, val loss: 1.160126805305481
Epoch 1650, training loss: 62.326683044433594 = 0.04964325204491615 + 10.0 * 6.227704048156738
Epoch 1650, val loss: 1.1630927324295044
Epoch 1660, training loss: 62.31475067138672 = 0.04862236604094505 + 10.0 * 6.2266130447387695
Epoch 1660, val loss: 1.1667790412902832
Epoch 1670, training loss: 62.323402404785156 = 0.04763869568705559 + 10.0 * 6.22757625579834
Epoch 1670, val loss: 1.170082449913025
Epoch 1680, training loss: 62.32978439331055 = 0.0466652512550354 + 10.0 * 6.228312015533447
Epoch 1680, val loss: 1.1738446950912476
Epoch 1690, training loss: 62.30055236816406 = 0.04572732374072075 + 10.0 * 6.22548246383667
Epoch 1690, val loss: 1.177198886871338
Epoch 1700, training loss: 62.292152404785156 = 0.04481734335422516 + 10.0 * 6.224733352661133
Epoch 1700, val loss: 1.1806026697158813
Epoch 1710, training loss: 62.308109283447266 = 0.043935131281614304 + 10.0 * 6.226417541503906
Epoch 1710, val loss: 1.184091329574585
Epoch 1720, training loss: 62.28434753417969 = 0.04307786747813225 + 10.0 * 6.224126815795898
Epoch 1720, val loss: 1.1875715255737305
Epoch 1730, training loss: 62.283348083496094 = 0.04223717004060745 + 10.0 * 6.224111080169678
Epoch 1730, val loss: 1.191164493560791
Epoch 1740, training loss: 62.30538558959961 = 0.04143105819821358 + 10.0 * 6.226395606994629
Epoch 1740, val loss: 1.1944423913955688
Epoch 1750, training loss: 62.28775405883789 = 0.04063395783305168 + 10.0 * 6.224711894989014
Epoch 1750, val loss: 1.197808861732483
Epoch 1760, training loss: 62.2760124206543 = 0.03985268250107765 + 10.0 * 6.223616123199463
Epoch 1760, val loss: 1.20126473903656
Epoch 1770, training loss: 62.268959045410156 = 0.03909783065319061 + 10.0 * 6.222986221313477
Epoch 1770, val loss: 1.2045806646347046
Epoch 1780, training loss: 62.28156661987305 = 0.038373176008462906 + 10.0 * 6.2243194580078125
Epoch 1780, val loss: 1.2080377340316772
Epoch 1790, training loss: 62.29186248779297 = 0.03766123950481415 + 10.0 * 6.225419998168945
Epoch 1790, val loss: 1.2116445302963257
Epoch 1800, training loss: 62.26085662841797 = 0.036964550614356995 + 10.0 * 6.222389221191406
Epoch 1800, val loss: 1.2147014141082764
Epoch 1810, training loss: 62.24984359741211 = 0.03628259897232056 + 10.0 * 6.221356391906738
Epoch 1810, val loss: 1.2180578708648682
Epoch 1820, training loss: 62.244712829589844 = 0.035636529326438904 + 10.0 * 6.220907688140869
Epoch 1820, val loss: 1.2215027809143066
Epoch 1830, training loss: 62.24323654174805 = 0.03500301390886307 + 10.0 * 6.220823287963867
Epoch 1830, val loss: 1.224800944328308
Epoch 1840, training loss: 62.31065368652344 = 0.034388281404972076 + 10.0 * 6.227626323699951
Epoch 1840, val loss: 1.2281767129898071
Epoch 1850, training loss: 62.284645080566406 = 0.03376701846718788 + 10.0 * 6.225087642669678
Epoch 1850, val loss: 1.2310388088226318
Epoch 1860, training loss: 62.2414665222168 = 0.033160243183374405 + 10.0 * 6.22083044052124
Epoch 1860, val loss: 1.234428882598877
Epoch 1870, training loss: 62.235374450683594 = 0.0325859896838665 + 10.0 * 6.220278739929199
Epoch 1870, val loss: 1.237717866897583
Epoch 1880, training loss: 62.229759216308594 = 0.032032500952482224 + 10.0 * 6.219772815704346
Epoch 1880, val loss: 1.240938663482666
Epoch 1890, training loss: 62.27256774902344 = 0.03149285912513733 + 10.0 * 6.224107265472412
Epoch 1890, val loss: 1.2441591024398804
Epoch 1900, training loss: 62.243412017822266 = 0.03094596602022648 + 10.0 * 6.221246719360352
Epoch 1900, val loss: 1.2472057342529297
Epoch 1910, training loss: 62.23029708862305 = 0.030422601848840714 + 10.0 * 6.219987392425537
Epoch 1910, val loss: 1.2503072023391724
Epoch 1920, training loss: 62.237361907958984 = 0.02991449646651745 + 10.0 * 6.220744609832764
Epoch 1920, val loss: 1.2535066604614258
Epoch 1930, training loss: 62.22727584838867 = 0.02941887639462948 + 10.0 * 6.219785690307617
Epoch 1930, val loss: 1.2566726207733154
Epoch 1940, training loss: 62.245582580566406 = 0.02893327921628952 + 10.0 * 6.221664905548096
Epoch 1940, val loss: 1.2602037191390991
Epoch 1950, training loss: 62.212974548339844 = 0.0284593403339386 + 10.0 * 6.218451499938965
Epoch 1950, val loss: 1.2628074884414673
Epoch 1960, training loss: 62.23426055908203 = 0.027997633442282677 + 10.0 * 6.220626354217529
Epoch 1960, val loss: 1.2660466432571411
Epoch 1970, training loss: 62.217864990234375 = 0.027539163827896118 + 10.0 * 6.2190327644348145
Epoch 1970, val loss: 1.269087791442871
Epoch 1980, training loss: 62.207637786865234 = 0.027101488783955574 + 10.0 * 6.218053817749023
Epoch 1980, val loss: 1.2720248699188232
Epoch 1990, training loss: 62.198692321777344 = 0.026674140244722366 + 10.0 * 6.217202186584473
Epoch 1990, val loss: 1.2752619981765747
Epoch 2000, training loss: 62.20524978637695 = 0.02626039832830429 + 10.0 * 6.217898845672607
Epoch 2000, val loss: 1.2782419919967651
Epoch 2010, training loss: 62.233421325683594 = 0.025850437581539154 + 10.0 * 6.220757007598877
Epoch 2010, val loss: 1.2811708450317383
Epoch 2020, training loss: 62.20262145996094 = 0.02543693408370018 + 10.0 * 6.217718601226807
Epoch 2020, val loss: 1.284040093421936
Epoch 2030, training loss: 62.21204376220703 = 0.025048164650797844 + 10.0 * 6.2186994552612305
Epoch 2030, val loss: 1.2869915962219238
Epoch 2040, training loss: 62.19431686401367 = 0.024661704897880554 + 10.0 * 6.216965675354004
Epoch 2040, val loss: 1.289940357208252
Epoch 2050, training loss: 62.199066162109375 = 0.024289486929774284 + 10.0 * 6.217477798461914
Epoch 2050, val loss: 1.2932201623916626
Epoch 2060, training loss: 62.19307327270508 = 0.023921387270092964 + 10.0 * 6.216915130615234
Epoch 2060, val loss: 1.2959908246994019
Epoch 2070, training loss: 62.176170349121094 = 0.0235653817653656 + 10.0 * 6.2152605056762695
Epoch 2070, val loss: 1.2989355325698853
Epoch 2080, training loss: 62.20035171508789 = 0.023221071809530258 + 10.0 * 6.217713356018066
Epoch 2080, val loss: 1.3017404079437256
Epoch 2090, training loss: 62.18086624145508 = 0.02287357486784458 + 10.0 * 6.215799331665039
Epoch 2090, val loss: 1.3045086860656738
Epoch 2100, training loss: 62.18925476074219 = 0.022539198398590088 + 10.0 * 6.216671466827393
Epoch 2100, val loss: 1.3073434829711914
Epoch 2110, training loss: 62.17451858520508 = 0.02220713533461094 + 10.0 * 6.215231418609619
Epoch 2110, val loss: 1.3104101419448853
Epoch 2120, training loss: 62.19601058959961 = 0.021884465590119362 + 10.0 * 6.21741247177124
Epoch 2120, val loss: 1.3133618831634521
Epoch 2130, training loss: 62.16714096069336 = 0.021570641547441483 + 10.0 * 6.214556694030762
Epoch 2130, val loss: 1.316011905670166
Epoch 2140, training loss: 62.170738220214844 = 0.021264573559165 + 10.0 * 6.214947700500488
Epoch 2140, val loss: 1.3189189434051514
Epoch 2150, training loss: 62.22714614868164 = 0.020968163385987282 + 10.0 * 6.220617771148682
Epoch 2150, val loss: 1.3218681812286377
Epoch 2160, training loss: 62.18111801147461 = 0.020665667951107025 + 10.0 * 6.216045379638672
Epoch 2160, val loss: 1.3243567943572998
Epoch 2170, training loss: 62.157920837402344 = 0.020376069471240044 + 10.0 * 6.213754653930664
Epoch 2170, val loss: 1.327246904373169
Epoch 2180, training loss: 62.14885711669922 = 0.0200984925031662 + 10.0 * 6.212875843048096
Epoch 2180, val loss: 1.3301271200180054
Epoch 2190, training loss: 62.15498352050781 = 0.019827628508210182 + 10.0 * 6.213515281677246
Epoch 2190, val loss: 1.3330446481704712
Epoch 2200, training loss: 62.21384048461914 = 0.019558915868401527 + 10.0 * 6.219428062438965
Epoch 2200, val loss: 1.335908055305481
Epoch 2210, training loss: 62.193241119384766 = 0.019289212301373482 + 10.0 * 6.217394828796387
Epoch 2210, val loss: 1.3381712436676025
Epoch 2220, training loss: 62.15732955932617 = 0.019019970670342445 + 10.0 * 6.213830947875977
Epoch 2220, val loss: 1.3410087823867798
Epoch 2230, training loss: 62.176368713378906 = 0.01876768283545971 + 10.0 * 6.215760231018066
Epoch 2230, val loss: 1.3439102172851562
Epoch 2240, training loss: 62.13722229003906 = 0.018517345190048218 + 10.0 * 6.2118706703186035
Epoch 2240, val loss: 1.346117377281189
Epoch 2250, training loss: 62.13796615600586 = 0.018275655806064606 + 10.0 * 6.211968898773193
Epoch 2250, val loss: 1.3489782810211182
Epoch 2260, training loss: 62.14731979370117 = 0.018040889874100685 + 10.0 * 6.21292781829834
Epoch 2260, val loss: 1.351719856262207
Epoch 2270, training loss: 62.165897369384766 = 0.01780623570084572 + 10.0 * 6.214808940887451
Epoch 2270, val loss: 1.3542412519454956
Epoch 2280, training loss: 62.13997268676758 = 0.01757538691163063 + 10.0 * 6.212239742279053
Epoch 2280, val loss: 1.3570563793182373
Epoch 2290, training loss: 62.130943298339844 = 0.01734926737844944 + 10.0 * 6.21135950088501
Epoch 2290, val loss: 1.3598030805587769
Epoch 2300, training loss: 62.209346771240234 = 0.017132539302110672 + 10.0 * 6.219221591949463
Epoch 2300, val loss: 1.3624377250671387
Epoch 2310, training loss: 62.163055419921875 = 0.01690886728465557 + 10.0 * 6.214614391326904
Epoch 2310, val loss: 1.3647539615631104
Epoch 2320, training loss: 62.12764358520508 = 0.016690494492650032 + 10.0 * 6.211095333099365
Epoch 2320, val loss: 1.3674273490905762
Epoch 2330, training loss: 62.11863327026367 = 0.016485383734107018 + 10.0 * 6.210214614868164
Epoch 2330, val loss: 1.3700542449951172
Epoch 2340, training loss: 62.14455795288086 = 0.016284866258502007 + 10.0 * 6.212827205657959
Epoch 2340, val loss: 1.3728001117706299
Epoch 2350, training loss: 62.130821228027344 = 0.01608249731361866 + 10.0 * 6.2114739418029785
Epoch 2350, val loss: 1.375238299369812
Epoch 2360, training loss: 62.12295150756836 = 0.01588563807308674 + 10.0 * 6.21070671081543
Epoch 2360, val loss: 1.3775396347045898
Epoch 2370, training loss: 62.11854934692383 = 0.01569032110273838 + 10.0 * 6.2102861404418945
Epoch 2370, val loss: 1.3800979852676392
Epoch 2380, training loss: 62.12691116333008 = 0.01550623495131731 + 10.0 * 6.2111406326293945
Epoch 2380, val loss: 1.3825584650039673
Epoch 2390, training loss: 62.13220977783203 = 0.015319024212658405 + 10.0 * 6.211688995361328
Epoch 2390, val loss: 1.3851051330566406
Epoch 2400, training loss: 62.13877868652344 = 0.015136399306356907 + 10.0 * 6.212364196777344
Epoch 2400, val loss: 1.3878066539764404
Epoch 2410, training loss: 62.104488372802734 = 0.014952371828258038 + 10.0 * 6.208953380584717
Epoch 2410, val loss: 1.3900518417358398
Epoch 2420, training loss: 62.13270568847656 = 0.01477763894945383 + 10.0 * 6.211792945861816
Epoch 2420, val loss: 1.3926390409469604
Epoch 2430, training loss: 62.11286926269531 = 0.014602997340261936 + 10.0 * 6.209826469421387
Epoch 2430, val loss: 1.3948644399642944
Epoch 2440, training loss: 62.10089111328125 = 0.014433213509619236 + 10.0 * 6.208645820617676
Epoch 2440, val loss: 1.3971328735351562
Epoch 2450, training loss: 62.101497650146484 = 0.014267886988818645 + 10.0 * 6.208723068237305
Epoch 2450, val loss: 1.3996847867965698
Epoch 2460, training loss: 62.1572380065918 = 0.014109420590102673 + 10.0 * 6.214312553405762
Epoch 2460, val loss: 1.4019763469696045
Epoch 2470, training loss: 62.10435104370117 = 0.013942002318799496 + 10.0 * 6.209040641784668
Epoch 2470, val loss: 1.4044383764266968
Epoch 2480, training loss: 62.09507369995117 = 0.013782578520476818 + 10.0 * 6.208128929138184
Epoch 2480, val loss: 1.406777262687683
Epoch 2490, training loss: 62.114593505859375 = 0.01363207958638668 + 10.0 * 6.2100958824157715
Epoch 2490, val loss: 1.4091556072235107
Epoch 2500, training loss: 62.10459518432617 = 0.01347569189965725 + 10.0 * 6.20911169052124
Epoch 2500, val loss: 1.4114185571670532
Epoch 2510, training loss: 62.12227249145508 = 0.013325594365596771 + 10.0 * 6.210894584655762
Epoch 2510, val loss: 1.4137336015701294
Epoch 2520, training loss: 62.10158920288086 = 0.013177708722651005 + 10.0 * 6.208841323852539
Epoch 2520, val loss: 1.4161103963851929
Epoch 2530, training loss: 62.13157272338867 = 0.01303519494831562 + 10.0 * 6.211853981018066
Epoch 2530, val loss: 1.4183367490768433
Epoch 2540, training loss: 62.08587646484375 = 0.012887789867818356 + 10.0 * 6.20729923248291
Epoch 2540, val loss: 1.4205440282821655
Epoch 2550, training loss: 62.082801818847656 = 0.012751956470310688 + 10.0 * 6.207005023956299
Epoch 2550, val loss: 1.422830581665039
Epoch 2560, training loss: 62.11149978637695 = 0.012618312612175941 + 10.0 * 6.209887981414795
Epoch 2560, val loss: 1.424972653388977
Epoch 2570, training loss: 62.08395767211914 = 0.012479192577302456 + 10.0 * 6.20714807510376
Epoch 2570, val loss: 1.4273074865341187
Epoch 2580, training loss: 62.08689498901367 = 0.01234515756368637 + 10.0 * 6.207455158233643
Epoch 2580, val loss: 1.4297322034835815
Epoch 2590, training loss: 62.115753173828125 = 0.012217114679515362 + 10.0 * 6.210353374481201
Epoch 2590, val loss: 1.4320536851882935
Epoch 2600, training loss: 62.08005142211914 = 0.012083499692380428 + 10.0 * 6.206796646118164
Epoch 2600, val loss: 1.433982253074646
Epoch 2610, training loss: 62.076759338378906 = 0.01195905078202486 + 10.0 * 6.206480026245117
Epoch 2610, val loss: 1.4362748861312866
Epoch 2620, training loss: 62.074729919433594 = 0.011834883131086826 + 10.0 * 6.206289768218994
Epoch 2620, val loss: 1.4387143850326538
Epoch 2630, training loss: 62.12181091308594 = 0.011714468710124493 + 10.0 * 6.211009502410889
Epoch 2630, val loss: 1.4410017728805542
Epoch 2640, training loss: 62.09782409667969 = 0.011589249595999718 + 10.0 * 6.20862340927124
Epoch 2640, val loss: 1.4424772262573242
Epoch 2650, training loss: 62.08406448364258 = 0.011468213051557541 + 10.0 * 6.207259654998779
Epoch 2650, val loss: 1.4450106620788574
Epoch 2660, training loss: 62.080135345458984 = 0.011352972127497196 + 10.0 * 6.206878185272217
Epoch 2660, val loss: 1.446962594985962
Epoch 2670, training loss: 62.0925407409668 = 0.011238583363592625 + 10.0 * 6.208130359649658
Epoch 2670, val loss: 1.449103832244873
Epoch 2680, training loss: 62.07429504394531 = 0.011123556643724442 + 10.0 * 6.206316947937012
Epoch 2680, val loss: 1.4511444568634033
Epoch 2690, training loss: 62.05925369262695 = 0.011013636365532875 + 10.0 * 6.204823970794678
Epoch 2690, val loss: 1.4534640312194824
Epoch 2700, training loss: 62.08584976196289 = 0.010907511226832867 + 10.0 * 6.207494258880615
Epoch 2700, val loss: 1.4555604457855225
Epoch 2710, training loss: 62.10536193847656 = 0.010797492228448391 + 10.0 * 6.209456443786621
Epoch 2710, val loss: 1.4574388265609741
Epoch 2720, training loss: 62.07369613647461 = 0.010685411281883717 + 10.0 * 6.206301212310791
Epoch 2720, val loss: 1.4592506885528564
Epoch 2730, training loss: 62.056060791015625 = 0.010580925270915031 + 10.0 * 6.204547882080078
Epoch 2730, val loss: 1.461376428604126
Epoch 2740, training loss: 62.05257797241211 = 0.010480059310793877 + 10.0 * 6.204209804534912
Epoch 2740, val loss: 1.4634082317352295
Epoch 2750, training loss: 62.0736083984375 = 0.01038288976997137 + 10.0 * 6.20632266998291
Epoch 2750, val loss: 1.4654760360717773
Epoch 2760, training loss: 62.075992584228516 = 0.010281921364367008 + 10.0 * 6.206571102142334
Epoch 2760, val loss: 1.4672971963882446
Epoch 2770, training loss: 62.05885696411133 = 0.010180697776377201 + 10.0 * 6.204867362976074
Epoch 2770, val loss: 1.4691921472549438
Epoch 2780, training loss: 62.05323028564453 = 0.010083407163619995 + 10.0 * 6.204314708709717
Epoch 2780, val loss: 1.4712530374526978
Epoch 2790, training loss: 62.07382583618164 = 0.00999309029430151 + 10.0 * 6.206383228302002
Epoch 2790, val loss: 1.4729759693145752
Epoch 2800, training loss: 62.05284118652344 = 0.00989550445228815 + 10.0 * 6.204294681549072
Epoch 2800, val loss: 1.474981665611267
Epoch 2810, training loss: 62.04399490356445 = 0.009800763800740242 + 10.0 * 6.2034196853637695
Epoch 2810, val loss: 1.4769922494888306
Epoch 2820, training loss: 62.038536071777344 = 0.009709351696074009 + 10.0 * 6.202882766723633
Epoch 2820, val loss: 1.4790008068084717
Epoch 2830, training loss: 62.050018310546875 = 0.00962239969521761 + 10.0 * 6.204039573669434
Epoch 2830, val loss: 1.4810025691986084
Epoch 2840, training loss: 62.08441925048828 = 0.009532660245895386 + 10.0 * 6.207488536834717
Epoch 2840, val loss: 1.4827611446380615
Epoch 2850, training loss: 62.05630111694336 = 0.009444484487175941 + 10.0 * 6.204685688018799
Epoch 2850, val loss: 1.4845786094665527
Epoch 2860, training loss: 62.05781936645508 = 0.009357613511383533 + 10.0 * 6.204846382141113
Epoch 2860, val loss: 1.4866142272949219
Epoch 2870, training loss: 62.0496711730957 = 0.009275088086724281 + 10.0 * 6.204039573669434
Epoch 2870, val loss: 1.488447904586792
Epoch 2880, training loss: 62.034912109375 = 0.009190347045660019 + 10.0 * 6.202572345733643
Epoch 2880, val loss: 1.4902329444885254
Epoch 2890, training loss: 62.07912063598633 = 0.009112539701163769 + 10.0 * 6.207000732421875
Epoch 2890, val loss: 1.4921095371246338
Epoch 2900, training loss: 62.068214416503906 = 0.00902547501027584 + 10.0 * 6.205918788909912
Epoch 2900, val loss: 1.4937920570373535
Epoch 2910, training loss: 62.04150390625 = 0.008943367749452591 + 10.0 * 6.203256130218506
Epoch 2910, val loss: 1.4955161809921265
Epoch 2920, training loss: 62.03090286254883 = 0.008865216746926308 + 10.0 * 6.202203750610352
Epoch 2920, val loss: 1.497534990310669
Epoch 2930, training loss: 62.033897399902344 = 0.008791614323854446 + 10.0 * 6.202510356903076
Epoch 2930, val loss: 1.4994386434555054
Epoch 2940, training loss: 62.07929992675781 = 0.008716767653822899 + 10.0 * 6.207058429718018
Epoch 2940, val loss: 1.5011603832244873
Epoch 2950, training loss: 62.0324592590332 = 0.008639466017484665 + 10.0 * 6.2023820877075195
Epoch 2950, val loss: 1.5027650594711304
Epoch 2960, training loss: 62.02186584472656 = 0.008564849384129047 + 10.0 * 6.201330184936523
Epoch 2960, val loss: 1.5047358274459839
Epoch 2970, training loss: 62.03911590576172 = 0.008494563400745392 + 10.0 * 6.203062057495117
Epoch 2970, val loss: 1.506757140159607
Epoch 2980, training loss: 62.064002990722656 = 0.008422117680311203 + 10.0 * 6.205557823181152
Epoch 2980, val loss: 1.5082597732543945
Epoch 2990, training loss: 62.05073165893555 = 0.008347159251570702 + 10.0 * 6.204238414764404
Epoch 2990, val loss: 1.5098174810409546
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 87.9385757446289 = 1.9703186750411987 + 10.0 * 8.59682559967041
Epoch 0, val loss: 1.970729112625122
Epoch 10, training loss: 87.9219741821289 = 1.9600507020950317 + 10.0 * 8.596192359924316
Epoch 10, val loss: 1.9612910747528076
Epoch 20, training loss: 87.8664779663086 = 1.947630763053894 + 10.0 * 8.59188461303711
Epoch 20, val loss: 1.9493896961212158
Epoch 30, training loss: 87.5757827758789 = 1.9315203428268433 + 10.0 * 8.56442642211914
Epoch 30, val loss: 1.933745265007019
Epoch 40, training loss: 86.06845092773438 = 1.9122799634933472 + 10.0 * 8.415616989135742
Epoch 40, val loss: 1.9152441024780273
Epoch 50, training loss: 80.71125030517578 = 1.8895047903060913 + 10.0 * 7.882174968719482
Epoch 50, val loss: 1.8934701681137085
Epoch 60, training loss: 76.76594543457031 = 1.8710929155349731 + 10.0 * 7.489484786987305
Epoch 60, val loss: 1.877661943435669
Epoch 70, training loss: 73.8769760131836 = 1.8598953485488892 + 10.0 * 7.2017083168029785
Epoch 70, val loss: 1.8675569295883179
Epoch 80, training loss: 72.7579574584961 = 1.8483890295028687 + 10.0 * 7.090957164764404
Epoch 80, val loss: 1.8564436435699463
Epoch 90, training loss: 71.65036010742188 = 1.835342526435852 + 10.0 * 6.981501579284668
Epoch 90, val loss: 1.8445614576339722
Epoch 100, training loss: 70.64751434326172 = 1.8241465091705322 + 10.0 * 6.882336139678955
Epoch 100, val loss: 1.8346682786941528
Epoch 110, training loss: 69.73641967773438 = 1.8157836198806763 + 10.0 * 6.792064189910889
Epoch 110, val loss: 1.8267536163330078
Epoch 120, training loss: 69.01378631591797 = 1.8080822229385376 + 10.0 * 6.720570087432861
Epoch 120, val loss: 1.8190832138061523
Epoch 130, training loss: 68.38336181640625 = 1.8004686832427979 + 10.0 * 6.658289432525635
Epoch 130, val loss: 1.8115577697753906
Epoch 140, training loss: 67.9283676147461 = 1.7935396432876587 + 10.0 * 6.613482475280762
Epoch 140, val loss: 1.8045812845230103
Epoch 150, training loss: 67.53726196289062 = 1.7864099740982056 + 10.0 * 6.575085639953613
Epoch 150, val loss: 1.7976588010787964
Epoch 160, training loss: 67.25589752197266 = 1.7790113687515259 + 10.0 * 6.5476884841918945
Epoch 160, val loss: 1.7905664443969727
Epoch 170, training loss: 67.05191802978516 = 1.7713255882263184 + 10.0 * 6.528059005737305
Epoch 170, val loss: 1.7834430932998657
Epoch 180, training loss: 66.87024688720703 = 1.7631925344467163 + 10.0 * 6.510705947875977
Epoch 180, val loss: 1.776120901107788
Epoch 190, training loss: 66.69271087646484 = 1.7546566724777222 + 10.0 * 6.493805885314941
Epoch 190, val loss: 1.7686671018600464
Epoch 200, training loss: 66.5466079711914 = 1.7456337213516235 + 10.0 * 6.48009729385376
Epoch 200, val loss: 1.760980486869812
Epoch 210, training loss: 66.42412567138672 = 1.7359912395477295 + 10.0 * 6.468813419342041
Epoch 210, val loss: 1.7529475688934326
Epoch 220, training loss: 66.28398895263672 = 1.725610375404358 + 10.0 * 6.455838203430176
Epoch 220, val loss: 1.7445018291473389
Epoch 230, training loss: 66.15589904785156 = 1.7145092487335205 + 10.0 * 6.44413948059082
Epoch 230, val loss: 1.735593557357788
Epoch 240, training loss: 66.06450653076172 = 1.7025744915008545 + 10.0 * 6.436192989349365
Epoch 240, val loss: 1.7261003255844116
Epoch 250, training loss: 65.94378662109375 = 1.689662218093872 + 10.0 * 6.425412654876709
Epoch 250, val loss: 1.7158993482589722
Epoch 260, training loss: 65.83367156982422 = 1.675748586654663 + 10.0 * 6.415791988372803
Epoch 260, val loss: 1.70501708984375
Epoch 270, training loss: 65.73844909667969 = 1.660822868347168 + 10.0 * 6.4077630043029785
Epoch 270, val loss: 1.693421721458435
Epoch 280, training loss: 65.6514663696289 = 1.6447616815567017 + 10.0 * 6.400670528411865
Epoch 280, val loss: 1.6810139417648315
Epoch 290, training loss: 65.57538604736328 = 1.6276098489761353 + 10.0 * 6.394777297973633
Epoch 290, val loss: 1.6677734851837158
Epoch 300, training loss: 65.47484588623047 = 1.6093628406524658 + 10.0 * 6.3865485191345215
Epoch 300, val loss: 1.6537641286849976
Epoch 310, training loss: 65.39662170410156 = 1.5901575088500977 + 10.0 * 6.380646705627441
Epoch 310, val loss: 1.6391133069992065
Epoch 320, training loss: 65.34487915039062 = 1.569940209388733 + 10.0 * 6.377493858337402
Epoch 320, val loss: 1.6237868070602417
Epoch 330, training loss: 65.25084686279297 = 1.5489072799682617 + 10.0 * 6.370194435119629
Epoch 330, val loss: 1.6079350709915161
Epoch 340, training loss: 65.18244171142578 = 1.5271257162094116 + 10.0 * 6.365531921386719
Epoch 340, val loss: 1.5916550159454346
Epoch 350, training loss: 65.12767791748047 = 1.504632830619812 + 10.0 * 6.3623046875
Epoch 350, val loss: 1.575165033340454
Epoch 360, training loss: 65.03813171386719 = 1.481801152229309 + 10.0 * 6.35563325881958
Epoch 360, val loss: 1.558469533920288
Epoch 370, training loss: 64.97706604003906 = 1.4586427211761475 + 10.0 * 6.351842403411865
Epoch 370, val loss: 1.541772723197937
Epoch 380, training loss: 64.92365264892578 = 1.435238003730774 + 10.0 * 6.348841190338135
Epoch 380, val loss: 1.5253175497055054
Epoch 390, training loss: 64.87889862060547 = 1.4115478992462158 + 10.0 * 6.346735000610352
Epoch 390, val loss: 1.5088850259780884
Epoch 400, training loss: 64.8043441772461 = 1.388002872467041 + 10.0 * 6.341634273529053
Epoch 400, val loss: 1.4927282333374023
Epoch 410, training loss: 64.73762512207031 = 1.364484429359436 + 10.0 * 6.337313652038574
Epoch 410, val loss: 1.477138638496399
Epoch 420, training loss: 64.68415832519531 = 1.3410923480987549 + 10.0 * 6.334306716918945
Epoch 420, val loss: 1.4618924856185913
Epoch 430, training loss: 64.63996124267578 = 1.3178167343139648 + 10.0 * 6.33221435546875
Epoch 430, val loss: 1.4470469951629639
Epoch 440, training loss: 64.57911682128906 = 1.2946797609329224 + 10.0 * 6.32844352722168
Epoch 440, val loss: 1.4326400756835938
Epoch 450, training loss: 64.52436065673828 = 1.271759033203125 + 10.0 * 6.325260162353516
Epoch 450, val loss: 1.4186846017837524
Epoch 460, training loss: 64.5093765258789 = 1.2489465475082397 + 10.0 * 6.326043128967285
Epoch 460, val loss: 1.405103325843811
Epoch 470, training loss: 64.45516204833984 = 1.226253628730774 + 10.0 * 6.322890758514404
Epoch 470, val loss: 1.3919496536254883
Epoch 480, training loss: 64.40360260009766 = 1.2036746740341187 + 10.0 * 6.319993019104004
Epoch 480, val loss: 1.3791576623916626
Epoch 490, training loss: 64.33819580078125 = 1.181206226348877 + 10.0 * 6.315699100494385
Epoch 490, val loss: 1.366681694984436
Epoch 500, training loss: 64.28892517089844 = 1.15888249874115 + 10.0 * 6.313004016876221
Epoch 500, val loss: 1.3545292615890503
Epoch 510, training loss: 64.24169158935547 = 1.1365972757339478 + 10.0 * 6.310509204864502
Epoch 510, val loss: 1.3426876068115234
Epoch 520, training loss: 64.21928405761719 = 1.1143019199371338 + 10.0 * 6.310498237609863
Epoch 520, val loss: 1.3312348127365112
Epoch 530, training loss: 64.20999908447266 = 1.0919408798217773 + 10.0 * 6.3118062019348145
Epoch 530, val loss: 1.319377064704895
Epoch 540, training loss: 64.12950134277344 = 1.069587230682373 + 10.0 * 6.305991172790527
Epoch 540, val loss: 1.3081307411193848
Epoch 550, training loss: 64.07479095458984 = 1.047303557395935 + 10.0 * 6.302748680114746
Epoch 550, val loss: 1.2971057891845703
Epoch 560, training loss: 64.03052520751953 = 1.025022268295288 + 10.0 * 6.30055046081543
Epoch 560, val loss: 1.2862493991851807
Epoch 570, training loss: 63.99755859375 = 1.0027481317520142 + 10.0 * 6.299481391906738
Epoch 570, val loss: 1.2755835056304932
Epoch 580, training loss: 63.98719024658203 = 0.9802485108375549 + 10.0 * 6.300694465637207
Epoch 580, val loss: 1.264745831489563
Epoch 590, training loss: 63.91993713378906 = 0.9578822255134583 + 10.0 * 6.296205520629883
Epoch 590, val loss: 1.2541261911392212
Epoch 600, training loss: 63.88246536254883 = 0.9357171654701233 + 10.0 * 6.294674873352051
Epoch 600, val loss: 1.2438760995864868
Epoch 610, training loss: 63.841392517089844 = 0.9137557744979858 + 10.0 * 6.292763710021973
Epoch 610, val loss: 1.233802080154419
Epoch 620, training loss: 63.81230545043945 = 0.8918782472610474 + 10.0 * 6.2920427322387695
Epoch 620, val loss: 1.22406005859375
Epoch 630, training loss: 63.77322769165039 = 0.8699543476104736 + 10.0 * 6.290327548980713
Epoch 630, val loss: 1.214264154434204
Epoch 640, training loss: 63.74537658691406 = 0.8482324481010437 + 10.0 * 6.289714336395264
Epoch 640, val loss: 1.204912543296814
Epoch 650, training loss: 63.69617462158203 = 0.8267416954040527 + 10.0 * 6.286943435668945
Epoch 650, val loss: 1.1957135200500488
Epoch 660, training loss: 63.686622619628906 = 0.8054634928703308 + 10.0 * 6.288115978240967
Epoch 660, val loss: 1.1868752241134644
Epoch 670, training loss: 63.64793395996094 = 0.7843567728996277 + 10.0 * 6.286357879638672
Epoch 670, val loss: 1.1781772375106812
Epoch 680, training loss: 63.61341857910156 = 0.763481616973877 + 10.0 * 6.284993648529053
Epoch 680, val loss: 1.1699954271316528
Epoch 690, training loss: 63.56576156616211 = 0.7429856061935425 + 10.0 * 6.282277584075928
Epoch 690, val loss: 1.162209153175354
Epoch 700, training loss: 63.53505325317383 = 0.7228494882583618 + 10.0 * 6.281220436096191
Epoch 700, val loss: 1.1548007726669312
Epoch 710, training loss: 63.547550201416016 = 0.7029768824577332 + 10.0 * 6.284457206726074
Epoch 710, val loss: 1.1476984024047852
Epoch 720, training loss: 63.482704162597656 = 0.6833297610282898 + 10.0 * 6.279937267303467
Epoch 720, val loss: 1.1409270763397217
Epoch 730, training loss: 63.461387634277344 = 0.6641422510147095 + 10.0 * 6.279724597930908
Epoch 730, val loss: 1.1345099210739136
Epoch 740, training loss: 63.41652297973633 = 0.6453201174736023 + 10.0 * 6.277120113372803
Epoch 740, val loss: 1.128623127937317
Epoch 750, training loss: 63.38714599609375 = 0.6268622875213623 + 10.0 * 6.276028633117676
Epoch 750, val loss: 1.1232811212539673
Epoch 760, training loss: 63.375301361083984 = 0.6087721586227417 + 10.0 * 6.276652812957764
Epoch 760, val loss: 1.1182265281677246
Epoch 770, training loss: 63.370113372802734 = 0.5909628868103027 + 10.0 * 6.277915000915527
Epoch 770, val loss: 1.1130640506744385
Epoch 780, training loss: 63.31822204589844 = 0.5735536217689514 + 10.0 * 6.274466514587402
Epoch 780, val loss: 1.1091288328170776
Epoch 790, training loss: 63.2720947265625 = 0.5565573573112488 + 10.0 * 6.271553993225098
Epoch 790, val loss: 1.1051206588745117
Epoch 800, training loss: 63.2472038269043 = 0.5399722456932068 + 10.0 * 6.270723342895508
Epoch 800, val loss: 1.1015515327453613
Epoch 810, training loss: 63.29964065551758 = 0.523708701133728 + 10.0 * 6.27759313583374
Epoch 810, val loss: 1.0983848571777344
Epoch 820, training loss: 63.216217041015625 = 0.5076643824577332 + 10.0 * 6.27085542678833
Epoch 820, val loss: 1.095597267150879
Epoch 830, training loss: 63.18555450439453 = 0.4920397698879242 + 10.0 * 6.269351482391357
Epoch 830, val loss: 1.093002438545227
Epoch 840, training loss: 63.15377426147461 = 0.47686779499053955 + 10.0 * 6.267690658569336
Epoch 840, val loss: 1.0908472537994385
Epoch 850, training loss: 63.15262985229492 = 0.46200695633888245 + 10.0 * 6.269062519073486
Epoch 850, val loss: 1.088965654373169
Epoch 860, training loss: 63.114707946777344 = 0.447466641664505 + 10.0 * 6.266724109649658
Epoch 860, val loss: 1.0873013734817505
Epoch 870, training loss: 63.1004638671875 = 0.4332239031791687 + 10.0 * 6.266724109649658
Epoch 870, val loss: 1.0860974788665771
Epoch 880, training loss: 63.077510833740234 = 0.419388085603714 + 10.0 * 6.265812397003174
Epoch 880, val loss: 1.0850096940994263
Epoch 890, training loss: 63.04380798339844 = 0.4059027433395386 + 10.0 * 6.263790607452393
Epoch 890, val loss: 1.0844606161117554
Epoch 900, training loss: 63.022727966308594 = 0.3927758038043976 + 10.0 * 6.26299524307251
Epoch 900, val loss: 1.0841633081436157
Epoch 910, training loss: 63.025657653808594 = 0.3799591660499573 + 10.0 * 6.2645697593688965
Epoch 910, val loss: 1.0839693546295166
Epoch 920, training loss: 63.00136947631836 = 0.36740362644195557 + 10.0 * 6.263396263122559
Epoch 920, val loss: 1.0845268964767456
Epoch 930, training loss: 62.99395751953125 = 0.3552515506744385 + 10.0 * 6.263870716094971
Epoch 930, val loss: 1.084918737411499
Epoch 940, training loss: 62.953041076660156 = 0.3434275686740875 + 10.0 * 6.260961532592773
Epoch 940, val loss: 1.0857455730438232
Epoch 950, training loss: 62.93156814575195 = 0.3320448398590088 + 10.0 * 6.259952068328857
Epoch 950, val loss: 1.086658239364624
Epoch 960, training loss: 62.91448211669922 = 0.3210395872592926 + 10.0 * 6.259344100952148
Epoch 960, val loss: 1.088228702545166
Epoch 970, training loss: 62.89971923828125 = 0.3103867471218109 + 10.0 * 6.258933067321777
Epoch 970, val loss: 1.0899842977523804
Epoch 980, training loss: 62.87885665893555 = 0.3000364303588867 + 10.0 * 6.257882118225098
Epoch 980, val loss: 1.0918999910354614
Epoch 990, training loss: 62.85480880737305 = 0.29000648856163025 + 10.0 * 6.2564802169799805
Epoch 990, val loss: 1.0940994024276733
Epoch 1000, training loss: 62.861045837402344 = 0.28032776713371277 + 10.0 * 6.2580718994140625
Epoch 1000, val loss: 1.0964515209197998
Epoch 1010, training loss: 62.83677673339844 = 0.27090007066726685 + 10.0 * 6.256587505340576
Epoch 1010, val loss: 1.0989869832992554
Epoch 1020, training loss: 62.83573532104492 = 0.26184022426605225 + 10.0 * 6.257389545440674
Epoch 1020, val loss: 1.1017827987670898
Epoch 1030, training loss: 62.793373107910156 = 0.253136545419693 + 10.0 * 6.254023551940918
Epoch 1030, val loss: 1.104726791381836
Epoch 1040, training loss: 62.77830123901367 = 0.2447659820318222 + 10.0 * 6.253353595733643
Epoch 1040, val loss: 1.1079370975494385
Epoch 1050, training loss: 62.77095413208008 = 0.23668330907821655 + 10.0 * 6.253427028656006
Epoch 1050, val loss: 1.1113320589065552
Epoch 1060, training loss: 62.77899932861328 = 0.22884519398212433 + 10.0 * 6.2550153732299805
Epoch 1060, val loss: 1.114645004272461
Epoch 1070, training loss: 62.73767852783203 = 0.2213091105222702 + 10.0 * 6.251636981964111
Epoch 1070, val loss: 1.1186716556549072
Epoch 1080, training loss: 62.72955322265625 = 0.21408677101135254 + 10.0 * 6.251546382904053
Epoch 1080, val loss: 1.1224479675292969
Epoch 1090, training loss: 62.71707534790039 = 0.2071109265089035 + 10.0 * 6.2509965896606445
Epoch 1090, val loss: 1.1264255046844482
Epoch 1100, training loss: 62.731266021728516 = 0.20040178298950195 + 10.0 * 6.253086566925049
Epoch 1100, val loss: 1.1307024955749512
Epoch 1110, training loss: 62.70848846435547 = 0.19392512738704681 + 10.0 * 6.251456260681152
Epoch 1110, val loss: 1.1351324319839478
Epoch 1120, training loss: 62.674110412597656 = 0.1876874417066574 + 10.0 * 6.248642444610596
Epoch 1120, val loss: 1.139578938484192
Epoch 1130, training loss: 62.66078567504883 = 0.18172374367713928 + 10.0 * 6.24790620803833
Epoch 1130, val loss: 1.1443142890930176
Epoch 1140, training loss: 62.68714904785156 = 0.17600341141223907 + 10.0 * 6.251114845275879
Epoch 1140, val loss: 1.1491678953170776
Epoch 1150, training loss: 62.65196228027344 = 0.17040851712226868 + 10.0 * 6.248155117034912
Epoch 1150, val loss: 1.1536846160888672
Epoch 1160, training loss: 62.65764236450195 = 0.16507433354854584 + 10.0 * 6.2492570877075195
Epoch 1160, val loss: 1.1586194038391113
Epoch 1170, training loss: 62.62105941772461 = 0.15992219746112823 + 10.0 * 6.2461137771606445
Epoch 1170, val loss: 1.1635855436325073
Epoch 1180, training loss: 62.62247848510742 = 0.15499353408813477 + 10.0 * 6.246748447418213
Epoch 1180, val loss: 1.1686174869537354
Epoch 1190, training loss: 62.62930679321289 = 0.15025576949119568 + 10.0 * 6.2479047775268555
Epoch 1190, val loss: 1.1736035346984863
Epoch 1200, training loss: 62.587181091308594 = 0.145659402012825 + 10.0 * 6.244152069091797
Epoch 1200, val loss: 1.1789876222610474
Epoch 1210, training loss: 62.575748443603516 = 0.14126715064048767 + 10.0 * 6.243448257446289
Epoch 1210, val loss: 1.1840636730194092
Epoch 1220, training loss: 62.58407211303711 = 0.1370609551668167 + 10.0 * 6.244700908660889
Epoch 1220, val loss: 1.1894687414169312
Epoch 1230, training loss: 62.5777473449707 = 0.13298165798187256 + 10.0 * 6.244476795196533
Epoch 1230, val loss: 1.1947017908096313
Epoch 1240, training loss: 62.56156539916992 = 0.12902408838272095 + 10.0 * 6.2432541847229
Epoch 1240, val loss: 1.2000945806503296
Epoch 1250, training loss: 62.55610275268555 = 0.12524570524692535 + 10.0 * 6.243085861206055
Epoch 1250, val loss: 1.2055332660675049
Epoch 1260, training loss: 62.54088592529297 = 0.12162624299526215 + 10.0 * 6.2419257164001465
Epoch 1260, val loss: 1.2111778259277344
Epoch 1270, training loss: 62.542999267578125 = 0.11814761906862259 + 10.0 * 6.242485046386719
Epoch 1270, val loss: 1.2167432308197021
Epoch 1280, training loss: 62.525821685791016 = 0.11475011706352234 + 10.0 * 6.241106986999512
Epoch 1280, val loss: 1.2220662832260132
Epoch 1290, training loss: 62.51520538330078 = 0.11148413270711899 + 10.0 * 6.240372180938721
Epoch 1290, val loss: 1.227593183517456
Epoch 1300, training loss: 62.52239227294922 = 0.10836856067180634 + 10.0 * 6.241402626037598
Epoch 1300, val loss: 1.2330753803253174
Epoch 1310, training loss: 62.51224136352539 = 0.10533720254898071 + 10.0 * 6.240690231323242
Epoch 1310, val loss: 1.2389909029006958
Epoch 1320, training loss: 62.49312210083008 = 0.10242009162902832 + 10.0 * 6.239069938659668
Epoch 1320, val loss: 1.2443139553070068
Epoch 1330, training loss: 62.4902229309082 = 0.09962362796068192 + 10.0 * 6.239059925079346
Epoch 1330, val loss: 1.2499955892562866
Epoch 1340, training loss: 62.48659133911133 = 0.09691229462623596 + 10.0 * 6.2389678955078125
Epoch 1340, val loss: 1.255646824836731
Epoch 1350, training loss: 62.490386962890625 = 0.09430450946092606 + 10.0 * 6.239608287811279
Epoch 1350, val loss: 1.2611674070358276
Epoch 1360, training loss: 62.4671745300293 = 0.09177853167057037 + 10.0 * 6.237539768218994
Epoch 1360, val loss: 1.2671254873275757
Epoch 1370, training loss: 62.52936935424805 = 0.08937722444534302 + 10.0 * 6.243999004364014
Epoch 1370, val loss: 1.2724072933197021
Epoch 1380, training loss: 62.47463607788086 = 0.08695636689662933 + 10.0 * 6.238768100738525
Epoch 1380, val loss: 1.2779130935668945
Epoch 1390, training loss: 62.44673156738281 = 0.08468957990407944 + 10.0 * 6.236204147338867
Epoch 1390, val loss: 1.2837039232254028
Epoch 1400, training loss: 62.433109283447266 = 0.08250781148672104 + 10.0 * 6.235060214996338
Epoch 1400, val loss: 1.2892837524414062
Epoch 1410, training loss: 62.42657470703125 = 0.08040008693933487 + 10.0 * 6.234617710113525
Epoch 1410, val loss: 1.294882893562317
Epoch 1420, training loss: 62.457130432128906 = 0.078367680311203 + 10.0 * 6.237875938415527
Epoch 1420, val loss: 1.3004125356674194
Epoch 1430, training loss: 62.44746398925781 = 0.07634010165929794 + 10.0 * 6.237112522125244
Epoch 1430, val loss: 1.3060829639434814
Epoch 1440, training loss: 62.43046188354492 = 0.0744086280465126 + 10.0 * 6.235605239868164
Epoch 1440, val loss: 1.3114278316497803
Epoch 1450, training loss: 62.403358459472656 = 0.07254912704229355 + 10.0 * 6.233080863952637
Epoch 1450, val loss: 1.3167985677719116
Epoch 1460, training loss: 62.39947509765625 = 0.0707700103521347 + 10.0 * 6.232870578765869
Epoch 1460, val loss: 1.3224738836288452
Epoch 1470, training loss: 62.39179992675781 = 0.06904454529285431 + 10.0 * 6.232275485992432
Epoch 1470, val loss: 1.3280342817306519
Epoch 1480, training loss: 62.443145751953125 = 0.06738198548555374 + 10.0 * 6.237576484680176
Epoch 1480, val loss: 1.333379864692688
Epoch 1490, training loss: 62.43999099731445 = 0.06572049111127853 + 10.0 * 6.237427234649658
Epoch 1490, val loss: 1.3385409116744995
Epoch 1500, training loss: 62.37651824951172 = 0.06410680711269379 + 10.0 * 6.231241226196289
Epoch 1500, val loss: 1.344254493713379
Epoch 1510, training loss: 62.379154205322266 = 0.06258464604616165 + 10.0 * 6.231657028198242
Epoch 1510, val loss: 1.3495415449142456
Epoch 1520, training loss: 62.376617431640625 = 0.06111927703022957 + 10.0 * 6.2315497398376465
Epoch 1520, val loss: 1.3548314571380615
Epoch 1530, training loss: 62.389339447021484 = 0.05968719348311424 + 10.0 * 6.232964992523193
Epoch 1530, val loss: 1.3602756261825562
Epoch 1540, training loss: 62.35822677612305 = 0.058292362838983536 + 10.0 * 6.2299933433532715
Epoch 1540, val loss: 1.3658849000930786
Epoch 1550, training loss: 62.35087203979492 = 0.05694660171866417 + 10.0 * 6.2293925285339355
Epoch 1550, val loss: 1.3712778091430664
Epoch 1560, training loss: 62.35593795776367 = 0.05565665289759636 + 10.0 * 6.23002815246582
Epoch 1560, val loss: 1.376613736152649
Epoch 1570, training loss: 62.3745231628418 = 0.054402898997068405 + 10.0 * 6.2320122718811035
Epoch 1570, val loss: 1.3818660974502563
Epoch 1580, training loss: 62.342803955078125 = 0.05315583199262619 + 10.0 * 6.228964805603027
Epoch 1580, val loss: 1.3872376680374146
Epoch 1590, training loss: 62.35830307006836 = 0.051968637853860855 + 10.0 * 6.230633735656738
Epoch 1590, val loss: 1.3924564123153687
Epoch 1600, training loss: 62.34088897705078 = 0.050814442336559296 + 10.0 * 6.229007244110107
Epoch 1600, val loss: 1.3978513479232788
Epoch 1610, training loss: 62.33233642578125 = 0.049692556262016296 + 10.0 * 6.228264331817627
Epoch 1610, val loss: 1.4032011032104492
Epoch 1620, training loss: 62.387943267822266 = 0.0486222542822361 + 10.0 * 6.233932018280029
Epoch 1620, val loss: 1.4084993600845337
Epoch 1630, training loss: 62.33797073364258 = 0.04755140095949173 + 10.0 * 6.229042053222656
Epoch 1630, val loss: 1.4134324789047241
Epoch 1640, training loss: 62.323238372802734 = 0.04652867466211319 + 10.0 * 6.227671146392822
Epoch 1640, val loss: 1.4188711643218994
Epoch 1650, training loss: 62.337955474853516 = 0.045548953115940094 + 10.0 * 6.229240894317627
Epoch 1650, val loss: 1.4239708185195923
Epoch 1660, training loss: 62.3160400390625 = 0.04457493871450424 + 10.0 * 6.227146625518799
Epoch 1660, val loss: 1.4289326667785645
Epoch 1670, training loss: 62.307125091552734 = 0.04363683611154556 + 10.0 * 6.226348876953125
Epoch 1670, val loss: 1.4341076612472534
Epoch 1680, training loss: 62.30583572387695 = 0.04273632541298866 + 10.0 * 6.226309776306152
Epoch 1680, val loss: 1.439132571220398
Epoch 1690, training loss: 62.32942199707031 = 0.04185725376009941 + 10.0 * 6.228756427764893
Epoch 1690, val loss: 1.4443186521530151
Epoch 1700, training loss: 62.29883575439453 = 0.04097721725702286 + 10.0 * 6.225785732269287
Epoch 1700, val loss: 1.4492789506912231
Epoch 1710, training loss: 62.2917594909668 = 0.04014084115624428 + 10.0 * 6.225161552429199
Epoch 1710, val loss: 1.4541420936584473
Epoch 1720, training loss: 62.331729888916016 = 0.03933409973978996 + 10.0 * 6.229239463806152
Epoch 1720, val loss: 1.4589581489562988
Epoch 1730, training loss: 62.29045486450195 = 0.03853171691298485 + 10.0 * 6.225192070007324
Epoch 1730, val loss: 1.463523268699646
Epoch 1740, training loss: 62.27552032470703 = 0.0377650186419487 + 10.0 * 6.223775386810303
Epoch 1740, val loss: 1.468902826309204
Epoch 1750, training loss: 62.280330657958984 = 0.03702157363295555 + 10.0 * 6.224330902099609
Epoch 1750, val loss: 1.4735257625579834
Epoch 1760, training loss: 62.335655212402344 = 0.03630128502845764 + 10.0 * 6.229935646057129
Epoch 1760, val loss: 1.4779338836669922
Epoch 1770, training loss: 62.28951644897461 = 0.03558284044265747 + 10.0 * 6.225393295288086
Epoch 1770, val loss: 1.4830292463302612
Epoch 1780, training loss: 62.26433563232422 = 0.034890010952949524 + 10.0 * 6.222944736480713
Epoch 1780, val loss: 1.4878040552139282
Epoch 1790, training loss: 62.25632095336914 = 0.03423088788986206 + 10.0 * 6.2222089767456055
Epoch 1790, val loss: 1.4924955368041992
Epoch 1800, training loss: 62.272132873535156 = 0.03359131142497063 + 10.0 * 6.223854064941406
Epoch 1800, val loss: 1.497215986251831
Epoch 1810, training loss: 62.26184844970703 = 0.032952431589365005 + 10.0 * 6.2228899002075195
Epoch 1810, val loss: 1.5017528533935547
Epoch 1820, training loss: 62.25324249267578 = 0.032320763915777206 + 10.0 * 6.222092151641846
Epoch 1820, val loss: 1.506220817565918
Epoch 1830, training loss: 62.336181640625 = 0.031728241592645645 + 10.0 * 6.230445384979248
Epoch 1830, val loss: 1.5110092163085938
Epoch 1840, training loss: 62.273555755615234 = 0.031128278002142906 + 10.0 * 6.224242687225342
Epoch 1840, val loss: 1.515030860900879
Epoch 1850, training loss: 62.2440071105957 = 0.030552862212061882 + 10.0 * 6.2213454246521
Epoch 1850, val loss: 1.5196689367294312
Epoch 1860, training loss: 62.23455810546875 = 0.030003676190972328 + 10.0 * 6.220455646514893
Epoch 1860, val loss: 1.524185061454773
Epoch 1870, training loss: 62.23931884765625 = 0.029469942674040794 + 10.0 * 6.220984935760498
Epoch 1870, val loss: 1.5283782482147217
Epoch 1880, training loss: 62.277042388916016 = 0.028946910053491592 + 10.0 * 6.224809646606445
Epoch 1880, val loss: 1.5326002836227417
Epoch 1890, training loss: 62.23168182373047 = 0.02841857075691223 + 10.0 * 6.2203264236450195
Epoch 1890, val loss: 1.5373166799545288
Epoch 1900, training loss: 62.219696044921875 = 0.027918759733438492 + 10.0 * 6.219177722930908
Epoch 1900, val loss: 1.5416349172592163
Epoch 1910, training loss: 62.21975326538086 = 0.027438601478934288 + 10.0 * 6.219231605529785
Epoch 1910, val loss: 1.545930027961731
Epoch 1920, training loss: 62.27765655517578 = 0.026977643370628357 + 10.0 * 6.225068092346191
Epoch 1920, val loss: 1.550036907196045
Epoch 1930, training loss: 62.248016357421875 = 0.02648788131773472 + 10.0 * 6.2221527099609375
Epoch 1930, val loss: 1.5540932416915894
Epoch 1940, training loss: 62.21603775024414 = 0.026034992188215256 + 10.0 * 6.219000339508057
Epoch 1940, val loss: 1.558822512626648
Epoch 1950, training loss: 62.21849060058594 = 0.025594748556613922 + 10.0 * 6.219289302825928
Epoch 1950, val loss: 1.5626870393753052
Epoch 1960, training loss: 62.248252868652344 = 0.025165952742099762 + 10.0 * 6.22230863571167
Epoch 1960, val loss: 1.5665924549102783
Epoch 1970, training loss: 62.207786560058594 = 0.024729721248149872 + 10.0 * 6.218305587768555
Epoch 1970, val loss: 1.5708720684051514
Epoch 1980, training loss: 62.2011833190918 = 0.024318866431713104 + 10.0 * 6.217686653137207
Epoch 1980, val loss: 1.5749131441116333
Epoch 1990, training loss: 62.201168060302734 = 0.023924129083752632 + 10.0 * 6.217724323272705
Epoch 1990, val loss: 1.5788261890411377
Epoch 2000, training loss: 62.22077178955078 = 0.023538338020443916 + 10.0 * 6.219723701477051
Epoch 2000, val loss: 1.5826165676116943
Epoch 2010, training loss: 62.1899528503418 = 0.023150349035859108 + 10.0 * 6.21668004989624
Epoch 2010, val loss: 1.5873465538024902
Epoch 2020, training loss: 62.21277618408203 = 0.022780392318964005 + 10.0 * 6.21899938583374
Epoch 2020, val loss: 1.591065764427185
Epoch 2030, training loss: 62.20985794067383 = 0.02241227775812149 + 10.0 * 6.21874475479126
Epoch 2030, val loss: 1.5947538614273071
Epoch 2040, training loss: 62.19000244140625 = 0.02205486223101616 + 10.0 * 6.216794490814209
Epoch 2040, val loss: 1.5988471508026123
Epoch 2050, training loss: 62.200870513916016 = 0.02171129174530506 + 10.0 * 6.217916011810303
Epoch 2050, val loss: 1.6026902198791504
Epoch 2060, training loss: 62.18309020996094 = 0.021364184096455574 + 10.0 * 6.216172695159912
Epoch 2060, val loss: 1.6065243482589722
Epoch 2070, training loss: 62.17693328857422 = 0.02103440836071968 + 10.0 * 6.215590000152588
Epoch 2070, val loss: 1.6107202768325806
Epoch 2080, training loss: 62.186283111572266 = 0.020716246217489243 + 10.0 * 6.216556549072266
Epoch 2080, val loss: 1.6144720315933228
Epoch 2090, training loss: 62.25940704345703 = 0.02040226012468338 + 10.0 * 6.22390079498291
Epoch 2090, val loss: 1.6181882619857788
Epoch 2100, training loss: 62.19289779663086 = 0.020075814798474312 + 10.0 * 6.217282295227051
Epoch 2100, val loss: 1.6216802597045898
Epoch 2110, training loss: 62.16705322265625 = 0.01976839452981949 + 10.0 * 6.214728355407715
Epoch 2110, val loss: 1.6256117820739746
Epoch 2120, training loss: 62.1622314453125 = 0.019474899396300316 + 10.0 * 6.21427583694458
Epoch 2120, val loss: 1.6294591426849365
Epoch 2130, training loss: 62.166019439697266 = 0.019190605729818344 + 10.0 * 6.214682579040527
Epoch 2130, val loss: 1.6329941749572754
Epoch 2140, training loss: 62.21543502807617 = 0.018912024796009064 + 10.0 * 6.21965217590332
Epoch 2140, val loss: 1.6365770101547241
Epoch 2150, training loss: 62.190521240234375 = 0.01862836256623268 + 10.0 * 6.217189311981201
Epoch 2150, val loss: 1.6401313543319702
Epoch 2160, training loss: 62.174930572509766 = 0.01835288293659687 + 10.0 * 6.215657711029053
Epoch 2160, val loss: 1.6436748504638672
Epoch 2170, training loss: 62.159908294677734 = 0.01809363253414631 + 10.0 * 6.214181423187256
Epoch 2170, val loss: 1.6470762491226196
Epoch 2180, training loss: 62.171810150146484 = 0.01783723756670952 + 10.0 * 6.215397357940674
Epoch 2180, val loss: 1.6504369974136353
Epoch 2190, training loss: 62.158447265625 = 0.017582153901457787 + 10.0 * 6.214086532592773
Epoch 2190, val loss: 1.6544504165649414
Epoch 2200, training loss: 62.15283203125 = 0.017334526404738426 + 10.0 * 6.213549613952637
Epoch 2200, val loss: 1.6578302383422852
Epoch 2210, training loss: 62.17152786254883 = 0.0170962605625391 + 10.0 * 6.215443134307861
Epoch 2210, val loss: 1.6614222526550293
Epoch 2220, training loss: 62.15425491333008 = 0.01684970036149025 + 10.0 * 6.213740348815918
Epoch 2220, val loss: 1.6644971370697021
Epoch 2230, training loss: 62.14645004272461 = 0.016613774001598358 + 10.0 * 6.21298360824585
Epoch 2230, val loss: 1.6679788827896118
Epoch 2240, training loss: 62.15189743041992 = 0.016389556229114532 + 10.0 * 6.213551044464111
Epoch 2240, val loss: 1.671470284461975
Epoch 2250, training loss: 62.142269134521484 = 0.01616363599896431 + 10.0 * 6.212610721588135
Epoch 2250, val loss: 1.6746249198913574
Epoch 2260, training loss: 62.14140319824219 = 0.01594698615372181 + 10.0 * 6.212545871734619
Epoch 2260, val loss: 1.677946925163269
Epoch 2270, training loss: 62.19411849975586 = 0.01573641411960125 + 10.0 * 6.217838287353516
Epoch 2270, val loss: 1.6811189651489258
Epoch 2280, training loss: 62.156455993652344 = 0.015519801527261734 + 10.0 * 6.2140936851501465
Epoch 2280, val loss: 1.6842693090438843
Epoch 2290, training loss: 62.134483337402344 = 0.01531064510345459 + 10.0 * 6.211916923522949
Epoch 2290, val loss: 1.687943935394287
Epoch 2300, training loss: 62.1342887878418 = 0.015112180262804031 + 10.0 * 6.211917400360107
Epoch 2300, val loss: 1.6907384395599365
Epoch 2310, training loss: 62.16345977783203 = 0.014915409497916698 + 10.0 * 6.2148542404174805
Epoch 2310, val loss: 1.6936326026916504
Epoch 2320, training loss: 62.13520050048828 = 0.014720743522047997 + 10.0 * 6.212048053741455
Epoch 2320, val loss: 1.6974979639053345
Epoch 2330, training loss: 62.121726989746094 = 0.01452888548374176 + 10.0 * 6.210719585418701
Epoch 2330, val loss: 1.7003388404846191
Epoch 2340, training loss: 62.15913391113281 = 0.014350700192153454 + 10.0 * 6.214478492736816
Epoch 2340, val loss: 1.7030693292617798
Epoch 2350, training loss: 62.14432907104492 = 0.014157303608953953 + 10.0 * 6.213017463684082
Epoch 2350, val loss: 1.706543207168579
Epoch 2360, training loss: 62.12555694580078 = 0.013974746689200401 + 10.0 * 6.211158275604248
Epoch 2360, val loss: 1.7094660997390747
Epoch 2370, training loss: 62.112518310546875 = 0.013795684091746807 + 10.0 * 6.209872245788574
Epoch 2370, val loss: 1.712824821472168
Epoch 2380, training loss: 62.10575485229492 = 0.013628901913762093 + 10.0 * 6.209212303161621
Epoch 2380, val loss: 1.7155879735946655
Epoch 2390, training loss: 62.10624694824219 = 0.013462583534419537 + 10.0 * 6.209278583526611
Epoch 2390, val loss: 1.7188318967819214
Epoch 2400, training loss: 62.19530487060547 = 0.013300108723342419 + 10.0 * 6.218200206756592
Epoch 2400, val loss: 1.7216689586639404
Epoch 2410, training loss: 62.13633728027344 = 0.013131041079759598 + 10.0 * 6.212320804595947
Epoch 2410, val loss: 1.7244069576263428
Epoch 2420, training loss: 62.11856460571289 = 0.012968135066330433 + 10.0 * 6.210559368133545
Epoch 2420, val loss: 1.7277764081954956
Epoch 2430, training loss: 62.155006408691406 = 0.012817459180951118 + 10.0 * 6.214219093322754
Epoch 2430, val loss: 1.7303529977798462
Epoch 2440, training loss: 62.11110305786133 = 0.012657077983021736 + 10.0 * 6.209844589233398
Epoch 2440, val loss: 1.7334433794021606
Epoch 2450, training loss: 62.09563446044922 = 0.012504796497523785 + 10.0 * 6.20831298828125
Epoch 2450, val loss: 1.7364752292633057
Epoch 2460, training loss: 62.09262466430664 = 0.012359722517430782 + 10.0 * 6.20802640914917
Epoch 2460, val loss: 1.7389651536941528
Epoch 2470, training loss: 62.109493255615234 = 0.01221796590834856 + 10.0 * 6.2097272872924805
Epoch 2470, val loss: 1.7416902780532837
Epoch 2480, training loss: 62.112144470214844 = 0.012074938975274563 + 10.0 * 6.210007190704346
Epoch 2480, val loss: 1.7443755865097046
Epoch 2490, training loss: 62.145748138427734 = 0.011938883922994137 + 10.0 * 6.213380813598633
Epoch 2490, val loss: 1.7480038404464722
Epoch 2500, training loss: 62.093013763427734 = 0.011789374053478241 + 10.0 * 6.208122253417969
Epoch 2500, val loss: 1.7499308586120605
Epoch 2510, training loss: 62.08365249633789 = 0.011653855443000793 + 10.0 * 6.207200050354004
Epoch 2510, val loss: 1.7531896829605103
Epoch 2520, training loss: 62.08101272583008 = 0.01152313593775034 + 10.0 * 6.206948757171631
Epoch 2520, val loss: 1.7557644844055176
Epoch 2530, training loss: 62.0806999206543 = 0.011396150104701519 + 10.0 * 6.206930637359619
Epoch 2530, val loss: 1.7584184408187866
Epoch 2540, training loss: 62.107139587402344 = 0.01127181388437748 + 10.0 * 6.2095866203308105
Epoch 2540, val loss: 1.7609636783599854
Epoch 2550, training loss: 62.09471130371094 = 0.011144294403493404 + 10.0 * 6.208356857299805
Epoch 2550, val loss: 1.7639073133468628
Epoch 2560, training loss: 62.081275939941406 = 0.011015564203262329 + 10.0 * 6.20702600479126
Epoch 2560, val loss: 1.7663131952285767
Epoch 2570, training loss: 62.081321716308594 = 0.01089571788907051 + 10.0 * 6.207042694091797
Epoch 2570, val loss: 1.769379734992981
Epoch 2580, training loss: 62.10416793823242 = 0.010777756571769714 + 10.0 * 6.209339141845703
Epoch 2580, val loss: 1.7715026140213013
Epoch 2590, training loss: 62.08531188964844 = 0.010657847858965397 + 10.0 * 6.207465171813965
Epoch 2590, val loss: 1.7743264436721802
Epoch 2600, training loss: 62.090579986572266 = 0.010542722418904305 + 10.0 * 6.208003520965576
Epoch 2600, val loss: 1.7766636610031128
Epoch 2610, training loss: 62.07196807861328 = 0.010428556241095066 + 10.0 * 6.206153869628906
Epoch 2610, val loss: 1.7792906761169434
Epoch 2620, training loss: 62.07630157470703 = 0.010320213623344898 + 10.0 * 6.206598281860352
Epoch 2620, val loss: 1.7818206548690796
Epoch 2630, training loss: 62.11824417114258 = 0.010213629342615604 + 10.0 * 6.210803031921387
Epoch 2630, val loss: 1.784455418586731
Epoch 2640, training loss: 62.09870910644531 = 0.010102302767336369 + 10.0 * 6.208860874176025
Epoch 2640, val loss: 1.786340594291687
Epoch 2650, training loss: 62.07684326171875 = 0.00999362301081419 + 10.0 * 6.2066850662231445
Epoch 2650, val loss: 1.7891651391983032
Epoch 2660, training loss: 62.067386627197266 = 0.009889280423521996 + 10.0 * 6.205749988555908
Epoch 2660, val loss: 1.791723370552063
Epoch 2670, training loss: 62.075469970703125 = 0.009788603521883488 + 10.0 * 6.206568241119385
Epoch 2670, val loss: 1.7938289642333984
Epoch 2680, training loss: 62.102317810058594 = 0.00968932919204235 + 10.0 * 6.209262847900391
Epoch 2680, val loss: 1.7965651750564575
Epoch 2690, training loss: 62.08330154418945 = 0.009584714658558369 + 10.0 * 6.207371711730957
Epoch 2690, val loss: 1.7990312576293945
Epoch 2700, training loss: 62.058837890625 = 0.009486319497227669 + 10.0 * 6.204935073852539
Epoch 2700, val loss: 1.801376461982727
Epoch 2710, training loss: 62.0543212890625 = 0.009390506893396378 + 10.0 * 6.204493045806885
Epoch 2710, val loss: 1.8036292791366577
Epoch 2720, training loss: 62.08079528808594 = 0.009299565106630325 + 10.0 * 6.207149505615234
Epoch 2720, val loss: 1.805699348449707
Epoch 2730, training loss: 62.057437896728516 = 0.00920555368065834 + 10.0 * 6.2048234939575195
Epoch 2730, val loss: 1.8082354068756104
Epoch 2740, training loss: 62.05239486694336 = 0.009114007465541363 + 10.0 * 6.2043280601501465
Epoch 2740, val loss: 1.810723066329956
Epoch 2750, training loss: 62.09512710571289 = 0.009024031460285187 + 10.0 * 6.2086100578308105
Epoch 2750, val loss: 1.8130104541778564
Epoch 2760, training loss: 62.05011749267578 = 0.008931751362979412 + 10.0 * 6.204118728637695
Epoch 2760, val loss: 1.8149168491363525
Epoch 2770, training loss: 62.047264099121094 = 0.008845176547765732 + 10.0 * 6.203841686248779
Epoch 2770, val loss: 1.8177385330200195
Epoch 2780, training loss: 62.05009078979492 = 0.00876210443675518 + 10.0 * 6.204133033752441
Epoch 2780, val loss: 1.819947361946106
Epoch 2790, training loss: 62.1115608215332 = 0.008680051192641258 + 10.0 * 6.210288047790527
Epoch 2790, val loss: 1.8221561908721924
Epoch 2800, training loss: 62.069297790527344 = 0.008593900129199028 + 10.0 * 6.206070423126221
Epoch 2800, val loss: 1.8239398002624512
Epoch 2810, training loss: 62.05121612548828 = 0.008508059196174145 + 10.0 * 6.204270839691162
Epoch 2810, val loss: 1.8265173435211182
Epoch 2820, training loss: 62.04609680175781 = 0.008429964073002338 + 10.0 * 6.203766822814941
Epoch 2820, val loss: 1.8282502889633179
Epoch 2830, training loss: 62.07167053222656 = 0.00835287943482399 + 10.0 * 6.206331729888916
Epoch 2830, val loss: 1.8304091691970825
Epoch 2840, training loss: 62.054710388183594 = 0.008276063948869705 + 10.0 * 6.204643249511719
Epoch 2840, val loss: 1.8329110145568848
Epoch 2850, training loss: 62.05175018310547 = 0.008196412585675716 + 10.0 * 6.204355239868164
Epoch 2850, val loss: 1.834886908531189
Epoch 2860, training loss: 62.05067443847656 = 0.008118961937725544 + 10.0 * 6.2042555809021
Epoch 2860, val loss: 1.8369709253311157
Epoch 2870, training loss: 62.060882568359375 = 0.008044923655688763 + 10.0 * 6.2052836418151855
Epoch 2870, val loss: 1.839240312576294
Epoch 2880, training loss: 62.03798294067383 = 0.007969497703015804 + 10.0 * 6.203001499176025
Epoch 2880, val loss: 1.84104585647583
Epoch 2890, training loss: 62.042903900146484 = 0.007897370494902134 + 10.0 * 6.203500747680664
Epoch 2890, val loss: 1.843430995941162
Epoch 2900, training loss: 62.06342315673828 = 0.00782603770494461 + 10.0 * 6.205559730529785
Epoch 2900, val loss: 1.844900369644165
Epoch 2910, training loss: 62.03526306152344 = 0.007754247635602951 + 10.0 * 6.2027506828308105
Epoch 2910, val loss: 1.8470474481582642
Epoch 2920, training loss: 62.04216003417969 = 0.00768785597756505 + 10.0 * 6.203447341918945
Epoch 2920, val loss: 1.8494139909744263
Epoch 2930, training loss: 62.05187225341797 = 0.0076179513707757 + 10.0 * 6.20442533493042
Epoch 2930, val loss: 1.8509531021118164
Epoch 2940, training loss: 62.03110885620117 = 0.007549459580332041 + 10.0 * 6.202355861663818
Epoch 2940, val loss: 1.853378176689148
Epoch 2950, training loss: 62.04086685180664 = 0.007484375964850187 + 10.0 * 6.203338146209717
Epoch 2950, val loss: 1.855285406112671
Epoch 2960, training loss: 62.03857421875 = 0.0074187698774039745 + 10.0 * 6.203115463256836
Epoch 2960, val loss: 1.8572871685028076
Epoch 2970, training loss: 62.034881591796875 = 0.007355692330747843 + 10.0 * 6.202752590179443
Epoch 2970, val loss: 1.8589600324630737
Epoch 2980, training loss: 62.074554443359375 = 0.007290611043572426 + 10.0 * 6.206726551055908
Epoch 2980, val loss: 1.8609474897384644
Epoch 2990, training loss: 62.03196716308594 = 0.007224886678159237 + 10.0 * 6.202474117279053
Epoch 2990, val loss: 1.8626192808151245
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 87.91934204101562 = 1.951019525527954 + 10.0 * 8.596832275390625
Epoch 0, val loss: 1.9429649114608765
Epoch 10, training loss: 87.90313720703125 = 1.9407234191894531 + 10.0 * 8.596240997314453
Epoch 10, val loss: 1.9332722425460815
Epoch 20, training loss: 87.84790802001953 = 1.9281253814697266 + 10.0 * 8.591978073120117
Epoch 20, val loss: 1.9209036827087402
Epoch 30, training loss: 87.54039001464844 = 1.9119415283203125 + 10.0 * 8.562845230102539
Epoch 30, val loss: 1.9048328399658203
Epoch 40, training loss: 85.85305786132812 = 1.8938301801681519 + 10.0 * 8.395922660827637
Epoch 40, val loss: 1.8873653411865234
Epoch 50, training loss: 81.09558868408203 = 1.8755091428756714 + 10.0 * 7.922008037567139
Epoch 50, val loss: 1.8698012828826904
Epoch 60, training loss: 76.6517105102539 = 1.8610928058624268 + 10.0 * 7.479061603546143
Epoch 60, val loss: 1.8571417331695557
Epoch 70, training loss: 73.30062103271484 = 1.849582552909851 + 10.0 * 7.145103931427002
Epoch 70, val loss: 1.8463501930236816
Epoch 80, training loss: 71.49059295654297 = 1.8383313417434692 + 10.0 * 6.965225696563721
Epoch 80, val loss: 1.8361907005310059
Epoch 90, training loss: 70.27057647705078 = 1.8279722929000854 + 10.0 * 6.8442606925964355
Epoch 90, val loss: 1.8272356986999512
Epoch 100, training loss: 69.47119903564453 = 1.819223403930664 + 10.0 * 6.76519775390625
Epoch 100, val loss: 1.8198208808898926
Epoch 110, training loss: 68.91847229003906 = 1.8108466863632202 + 10.0 * 6.710762977600098
Epoch 110, val loss: 1.8124935626983643
Epoch 120, training loss: 68.46239471435547 = 1.8026565313339233 + 10.0 * 6.665973663330078
Epoch 120, val loss: 1.8051767349243164
Epoch 130, training loss: 68.09028625488281 = 1.7949215173721313 + 10.0 * 6.6295366287231445
Epoch 130, val loss: 1.7982501983642578
Epoch 140, training loss: 67.79043579101562 = 1.787051796913147 + 10.0 * 6.600338935852051
Epoch 140, val loss: 1.7910799980163574
Epoch 150, training loss: 67.53356170654297 = 1.7786015272140503 + 10.0 * 6.575495719909668
Epoch 150, val loss: 1.7834970951080322
Epoch 160, training loss: 67.30439758300781 = 1.7695635557174683 + 10.0 * 6.553483486175537
Epoch 160, val loss: 1.7754614353179932
Epoch 170, training loss: 67.10555267333984 = 1.7598505020141602 + 10.0 * 6.534570693969727
Epoch 170, val loss: 1.7669289112091064
Epoch 180, training loss: 66.90963745117188 = 1.7493748664855957 + 10.0 * 6.516026496887207
Epoch 180, val loss: 1.757815957069397
Epoch 190, training loss: 66.73545837402344 = 1.7380239963531494 + 10.0 * 6.499743938446045
Epoch 190, val loss: 1.7480394840240479
Epoch 200, training loss: 66.60000610351562 = 1.725562334060669 + 10.0 * 6.4874444007873535
Epoch 200, val loss: 1.737349271774292
Epoch 210, training loss: 66.43391418457031 = 1.7119113206863403 + 10.0 * 6.472200393676758
Epoch 210, val loss: 1.7256722450256348
Epoch 220, training loss: 66.29010009765625 = 1.6969592571258545 + 10.0 * 6.459314346313477
Epoch 220, val loss: 1.7128715515136719
Epoch 230, training loss: 66.20250701904297 = 1.6805530786514282 + 10.0 * 6.45219612121582
Epoch 230, val loss: 1.698876142501831
Epoch 240, training loss: 66.0460205078125 = 1.6626774072647095 + 10.0 * 6.438334941864014
Epoch 240, val loss: 1.683626413345337
Epoch 250, training loss: 65.95099639892578 = 1.6432206630706787 + 10.0 * 6.4307780265808105
Epoch 250, val loss: 1.6670713424682617
Epoch 260, training loss: 65.83218383789062 = 1.6221954822540283 + 10.0 * 6.420999050140381
Epoch 260, val loss: 1.6491689682006836
Epoch 270, training loss: 65.76409149169922 = 1.599605917930603 + 10.0 * 6.41644811630249
Epoch 270, val loss: 1.6301002502441406
Epoch 280, training loss: 65.64149475097656 = 1.5755702257156372 + 10.0 * 6.40659236907959
Epoch 280, val loss: 1.609910488128662
Epoch 290, training loss: 65.54317474365234 = 1.5502233505249023 + 10.0 * 6.399294853210449
Epoch 290, val loss: 1.5887768268585205
Epoch 300, training loss: 65.49620056152344 = 1.523658037185669 + 10.0 * 6.397254467010498
Epoch 300, val loss: 1.5669076442718506
Epoch 310, training loss: 65.3926010131836 = 1.4962002038955688 + 10.0 * 6.389639854431152
Epoch 310, val loss: 1.544614553451538
Epoch 320, training loss: 65.29217529296875 = 1.4679487943649292 + 10.0 * 6.382422924041748
Epoch 320, val loss: 1.522085428237915
Epoch 330, training loss: 65.20728302001953 = 1.4392226934432983 + 10.0 * 6.376805782318115
Epoch 330, val loss: 1.4995912313461304
Epoch 340, training loss: 65.16317749023438 = 1.4101948738098145 + 10.0 * 6.375298500061035
Epoch 340, val loss: 1.477433681488037
Epoch 350, training loss: 65.06422424316406 = 1.3810428380966187 + 10.0 * 6.3683180809021
Epoch 350, val loss: 1.4556787014007568
Epoch 360, training loss: 64.97869110107422 = 1.3519972562789917 + 10.0 * 6.362669467926025
Epoch 360, val loss: 1.4345518350601196
Epoch 370, training loss: 64.908935546875 = 1.323081135749817 + 10.0 * 6.358585834503174
Epoch 370, val loss: 1.4140559434890747
Epoch 380, training loss: 64.88643646240234 = 1.2943726778030396 + 10.0 * 6.359206199645996
Epoch 380, val loss: 1.394343614578247
Epoch 390, training loss: 64.80862426757812 = 1.2658717632293701 + 10.0 * 6.354275703430176
Epoch 390, val loss: 1.374836802482605
Epoch 400, training loss: 64.71784973144531 = 1.2378380298614502 + 10.0 * 6.348001003265381
Epoch 400, val loss: 1.3563451766967773
Epoch 410, training loss: 64.6546401977539 = 1.2102175951004028 + 10.0 * 6.344442367553711
Epoch 410, val loss: 1.3384603261947632
Epoch 420, training loss: 64.60468292236328 = 1.1830248832702637 + 10.0 * 6.342165946960449
Epoch 420, val loss: 1.3211884498596191
Epoch 430, training loss: 64.57271575927734 = 1.1561366319656372 + 10.0 * 6.341658115386963
Epoch 430, val loss: 1.3045543432235718
Epoch 440, training loss: 64.4933853149414 = 1.1296062469482422 + 10.0 * 6.33637809753418
Epoch 440, val loss: 1.2881885766983032
Epoch 450, training loss: 64.4202651977539 = 1.103607416152954 + 10.0 * 6.331665992736816
Epoch 450, val loss: 1.2727015018463135
Epoch 460, training loss: 64.36703491210938 = 1.0780372619628906 + 10.0 * 6.328899383544922
Epoch 460, val loss: 1.2576192617416382
Epoch 470, training loss: 64.40772247314453 = 1.0528895854949951 + 10.0 * 6.335483074188232
Epoch 470, val loss: 1.2431074380874634
Epoch 480, training loss: 64.30245971679688 = 1.0279325246810913 + 10.0 * 6.327452659606934
Epoch 480, val loss: 1.2285776138305664
Epoch 490, training loss: 64.2306900024414 = 1.0035775899887085 + 10.0 * 6.322710990905762
Epoch 490, val loss: 1.2146499156951904
Epoch 500, training loss: 64.18079376220703 = 0.9797206521034241 + 10.0 * 6.320107460021973
Epoch 500, val loss: 1.201277494430542
Epoch 510, training loss: 64.14688873291016 = 0.9563122391700745 + 10.0 * 6.319057941436768
Epoch 510, val loss: 1.1883167028427124
Epoch 520, training loss: 64.09078979492188 = 0.9332534670829773 + 10.0 * 6.315753936767578
Epoch 520, val loss: 1.17569899559021
Epoch 530, training loss: 64.05307006835938 = 0.9106327891349792 + 10.0 * 6.314243793487549
Epoch 530, val loss: 1.1635006666183472
Epoch 540, training loss: 64.03257751464844 = 0.8884093761444092 + 10.0 * 6.314416885375977
Epoch 540, val loss: 1.151249885559082
Epoch 550, training loss: 63.971435546875 = 0.8667469620704651 + 10.0 * 6.310468673706055
Epoch 550, val loss: 1.1397976875305176
Epoch 560, training loss: 63.9267692565918 = 0.8455805778503418 + 10.0 * 6.30811882019043
Epoch 560, val loss: 1.1286252737045288
Epoch 570, training loss: 63.88493728637695 = 0.8249196410179138 + 10.0 * 6.306001663208008
Epoch 570, val loss: 1.118049144744873
Epoch 580, training loss: 63.902732849121094 = 0.8046791553497314 + 10.0 * 6.309805393218994
Epoch 580, val loss: 1.10764479637146
Epoch 590, training loss: 63.83342361450195 = 0.7848322987556458 + 10.0 * 6.304859161376953
Epoch 590, val loss: 1.0975333452224731
Epoch 600, training loss: 63.7767333984375 = 0.76545649766922 + 10.0 * 6.3011274337768555
Epoch 600, val loss: 1.0877727270126343
Epoch 610, training loss: 63.74477005004883 = 0.7465777397155762 + 10.0 * 6.299818992614746
Epoch 610, val loss: 1.0786393880844116
Epoch 620, training loss: 63.765838623046875 = 0.728066623210907 + 10.0 * 6.30377721786499
Epoch 620, val loss: 1.0693410634994507
Epoch 630, training loss: 63.68164825439453 = 0.7099556922912598 + 10.0 * 6.297169208526611
Epoch 630, val loss: 1.0612764358520508
Epoch 640, training loss: 63.648887634277344 = 0.6922644376754761 + 10.0 * 6.2956624031066895
Epoch 640, val loss: 1.053514003753662
Epoch 650, training loss: 63.64918899536133 = 0.6749299764633179 + 10.0 * 6.297425746917725
Epoch 650, val loss: 1.045548677444458
Epoch 660, training loss: 63.58449172973633 = 0.6579530239105225 + 10.0 * 6.292654037475586
Epoch 660, val loss: 1.0379352569580078
Epoch 670, training loss: 63.554073333740234 = 0.6413663625717163 + 10.0 * 6.291270732879639
Epoch 670, val loss: 1.0309574604034424
Epoch 680, training loss: 63.582374572753906 = 0.6251282691955566 + 10.0 * 6.295724391937256
Epoch 680, val loss: 1.024625539779663
Epoch 690, training loss: 63.50383758544922 = 0.6090595722198486 + 10.0 * 6.289477825164795
Epoch 690, val loss: 1.0180684328079224
Epoch 700, training loss: 63.46448516845703 = 0.5934234857559204 + 10.0 * 6.287106513977051
Epoch 700, val loss: 1.0121212005615234
Epoch 710, training loss: 63.451725006103516 = 0.5781455039978027 + 10.0 * 6.287358283996582
Epoch 710, val loss: 1.0069807767868042
Epoch 720, training loss: 63.413238525390625 = 0.5630650520324707 + 10.0 * 6.285017490386963
Epoch 720, val loss: 1.0015161037445068
Epoch 730, training loss: 63.38386917114258 = 0.5483194589614868 + 10.0 * 6.283555030822754
Epoch 730, val loss: 0.9968969225883484
Epoch 740, training loss: 63.41379165649414 = 0.5338560342788696 + 10.0 * 6.287993431091309
Epoch 740, val loss: 0.992344856262207
Epoch 750, training loss: 63.362606048583984 = 0.5196807980537415 + 10.0 * 6.284292697906494
Epoch 750, val loss: 0.98801189661026
Epoch 760, training loss: 63.31060028076172 = 0.5057657361030579 + 10.0 * 6.280483722686768
Epoch 760, val loss: 0.9843783974647522
Epoch 770, training loss: 63.28594970703125 = 0.4922091066837311 + 10.0 * 6.279374122619629
Epoch 770, val loss: 0.9810367226600647
Epoch 780, training loss: 63.32141876220703 = 0.4789278209209442 + 10.0 * 6.284249305725098
Epoch 780, val loss: 0.9776198863983154
Epoch 790, training loss: 63.26012420654297 = 0.46582841873168945 + 10.0 * 6.2794294357299805
Epoch 790, val loss: 0.975089430809021
Epoch 800, training loss: 63.21213912963867 = 0.4530543088912964 + 10.0 * 6.275908470153809
Epoch 800, val loss: 0.9724969863891602
Epoch 810, training loss: 63.20671081542969 = 0.44057631492614746 + 10.0 * 6.276613712310791
Epoch 810, val loss: 0.9702953100204468
Epoch 820, training loss: 63.19118118286133 = 0.4283190667629242 + 10.0 * 6.2762861251831055
Epoch 820, val loss: 0.9689098000526428
Epoch 830, training loss: 63.16393280029297 = 0.41628554463386536 + 10.0 * 6.274764537811279
Epoch 830, val loss: 0.9666319489479065
Epoch 840, training loss: 63.12694549560547 = 0.4046177268028259 + 10.0 * 6.272233009338379
Epoch 840, val loss: 0.9653884172439575
Epoch 850, training loss: 63.110740661621094 = 0.3932594656944275 + 10.0 * 6.271748065948486
Epoch 850, val loss: 0.964816153049469
Epoch 860, training loss: 63.146148681640625 = 0.3821166455745697 + 10.0 * 6.276402950286865
Epoch 860, val loss: 0.9640611410140991
Epoch 870, training loss: 63.08039474487305 = 0.37118908762931824 + 10.0 * 6.270920753479004
Epoch 870, val loss: 0.9637179970741272
Epoch 880, training loss: 63.055198669433594 = 0.3605651557445526 + 10.0 * 6.269463539123535
Epoch 880, val loss: 0.9633073806762695
Epoch 890, training loss: 63.03208541870117 = 0.3502632975578308 + 10.0 * 6.268182277679443
Epoch 890, val loss: 0.963633120059967
Epoch 900, training loss: 63.062171936035156 = 0.34023651480674744 + 10.0 * 6.272193431854248
Epoch 900, val loss: 0.9645162224769592
Epoch 910, training loss: 63.025142669677734 = 0.3303084075450897 + 10.0 * 6.26948356628418
Epoch 910, val loss: 0.9645881652832031
Epoch 920, training loss: 62.985713958740234 = 0.3207521438598633 + 10.0 * 6.266496181488037
Epoch 920, val loss: 0.9654874205589294
Epoch 930, training loss: 62.960567474365234 = 0.3114577531814575 + 10.0 * 6.264910697937012
Epoch 930, val loss: 0.9665302634239197
Epoch 940, training loss: 62.974979400634766 = 0.30244266986846924 + 10.0 * 6.267253398895264
Epoch 940, val loss: 0.9680149555206299
Epoch 950, training loss: 62.93755340576172 = 0.29359251260757446 + 10.0 * 6.264395713806152
Epoch 950, val loss: 0.9693715572357178
Epoch 960, training loss: 62.92543411254883 = 0.28502342104911804 + 10.0 * 6.264040946960449
Epoch 960, val loss: 0.9714726209640503
Epoch 970, training loss: 62.92747497558594 = 0.27672648429870605 + 10.0 * 6.265074729919434
Epoch 970, val loss: 0.9734185338020325
Epoch 980, training loss: 62.882808685302734 = 0.2686692476272583 + 10.0 * 6.261414051055908
Epoch 980, val loss: 0.9755714535713196
Epoch 990, training loss: 62.87097930908203 = 0.2608400583267212 + 10.0 * 6.261013984680176
Epoch 990, val loss: 0.9781821966171265
Epoch 1000, training loss: 62.90795135498047 = 0.2532476782798767 + 10.0 * 6.265470504760742
Epoch 1000, val loss: 0.9807473421096802
Epoch 1010, training loss: 62.852420806884766 = 0.2458081692457199 + 10.0 * 6.2606611251831055
Epoch 1010, val loss: 0.9829161763191223
Epoch 1020, training loss: 62.826324462890625 = 0.23865823447704315 + 10.0 * 6.2587666511535645
Epoch 1020, val loss: 0.986030638217926
Epoch 1030, training loss: 62.82439422607422 = 0.23173657059669495 + 10.0 * 6.259265899658203
Epoch 1030, val loss: 0.9889765977859497
Epoch 1040, training loss: 62.81132125854492 = 0.2249957174062729 + 10.0 * 6.258632659912109
Epoch 1040, val loss: 0.9923449754714966
Epoch 1050, training loss: 62.789031982421875 = 0.21848009526729584 + 10.0 * 6.257055282592773
Epoch 1050, val loss: 0.9960444569587708
Epoch 1060, training loss: 62.77685546875 = 0.21216385066509247 + 10.0 * 6.256469249725342
Epoch 1060, val loss: 0.9994346499443054
Epoch 1070, training loss: 62.815574645996094 = 0.20603443682193756 + 10.0 * 6.260953903198242
Epoch 1070, val loss: 1.0029388666152954
Epoch 1080, training loss: 62.767940521240234 = 0.20005446672439575 + 10.0 * 6.256788730621338
Epoch 1080, val loss: 1.0069200992584229
Epoch 1090, training loss: 62.744773864746094 = 0.19428500533103943 + 10.0 * 6.255048751831055
Epoch 1090, val loss: 1.010611653327942
Epoch 1100, training loss: 62.780601501464844 = 0.1887284368276596 + 10.0 * 6.2591872215271
Epoch 1100, val loss: 1.0146105289459229
Epoch 1110, training loss: 62.733131408691406 = 0.1833164244890213 + 10.0 * 6.254981517791748
Epoch 1110, val loss: 1.0190606117248535
Epoch 1120, training loss: 62.71454620361328 = 0.17809662222862244 + 10.0 * 6.253644943237305
Epoch 1120, val loss: 1.0231467485427856
Epoch 1130, training loss: 62.72615051269531 = 0.17305520176887512 + 10.0 * 6.255309581756592
Epoch 1130, val loss: 1.0272886753082275
Epoch 1140, training loss: 62.7088508605957 = 0.16815702617168427 + 10.0 * 6.2540693283081055
Epoch 1140, val loss: 1.0316410064697266
Epoch 1150, training loss: 62.6904182434082 = 0.16341477632522583 + 10.0 * 6.252700328826904
Epoch 1150, val loss: 1.0364165306091309
Epoch 1160, training loss: 62.67816162109375 = 0.15885290503501892 + 10.0 * 6.251931190490723
Epoch 1160, val loss: 1.0412309169769287
Epoch 1170, training loss: 62.65547180175781 = 0.15441802144050598 + 10.0 * 6.250105381011963
Epoch 1170, val loss: 1.0455862283706665
Epoch 1180, training loss: 62.66462326049805 = 0.15014877915382385 + 10.0 * 6.2514472007751465
Epoch 1180, val loss: 1.0505021810531616
Epoch 1190, training loss: 62.646549224853516 = 0.14595767855644226 + 10.0 * 6.250059127807617
Epoch 1190, val loss: 1.0544326305389404
Epoch 1200, training loss: 62.636898040771484 = 0.14193479716777802 + 10.0 * 6.2494964599609375
Epoch 1200, val loss: 1.0598983764648438
Epoch 1210, training loss: 62.62384796142578 = 0.1380431205034256 + 10.0 * 6.248580455780029
Epoch 1210, val loss: 1.0643082857131958
Epoch 1220, training loss: 62.67629623413086 = 0.13428962230682373 + 10.0 * 6.2542009353637695
Epoch 1220, val loss: 1.0693467855453491
Epoch 1230, training loss: 62.6229133605957 = 0.1306384950876236 + 10.0 * 6.249227523803711
Epoch 1230, val loss: 1.0743459463119507
Epoch 1240, training loss: 62.602386474609375 = 0.1271112859249115 + 10.0 * 6.247527599334717
Epoch 1240, val loss: 1.0793758630752563
Epoch 1250, training loss: 62.65424728393555 = 0.12372744083404541 + 10.0 * 6.253052234649658
Epoch 1250, val loss: 1.0844213962554932
Epoch 1260, training loss: 62.59310531616211 = 0.12038438767194748 + 10.0 * 6.24727201461792
Epoch 1260, val loss: 1.0889415740966797
Epoch 1270, training loss: 62.569454193115234 = 0.11719943583011627 + 10.0 * 6.245225429534912
Epoch 1270, val loss: 1.094298243522644
Epoch 1280, training loss: 62.56120681762695 = 0.1141301691532135 + 10.0 * 6.2447075843811035
Epoch 1280, val loss: 1.0994997024536133
Epoch 1290, training loss: 62.59150314331055 = 0.11114569753408432 + 10.0 * 6.248035907745361
Epoch 1290, val loss: 1.104506015777588
Epoch 1300, training loss: 62.59675216674805 = 0.10823261737823486 + 10.0 * 6.248851776123047
Epoch 1300, val loss: 1.109153389930725
Epoch 1310, training loss: 62.55458450317383 = 0.10540413111448288 + 10.0 * 6.244917869567871
Epoch 1310, val loss: 1.1149015426635742
Epoch 1320, training loss: 62.52954864501953 = 0.10269299894571304 + 10.0 * 6.242685794830322
Epoch 1320, val loss: 1.1199809312820435
Epoch 1330, training loss: 62.52020263671875 = 0.10007496178150177 + 10.0 * 6.242012977600098
Epoch 1330, val loss: 1.1253478527069092
Epoch 1340, training loss: 62.55194854736328 = 0.09753934293985367 + 10.0 * 6.24544095993042
Epoch 1340, val loss: 1.1304166316986084
Epoch 1350, training loss: 62.539520263671875 = 0.09504153579473495 + 10.0 * 6.244447708129883
Epoch 1350, val loss: 1.1362618207931519
Epoch 1360, training loss: 62.522274017333984 = 0.0926240012049675 + 10.0 * 6.242964744567871
Epoch 1360, val loss: 1.1414366960525513
Epoch 1370, training loss: 62.51018524169922 = 0.09030815958976746 + 10.0 * 6.241987705230713
Epoch 1370, val loss: 1.1469007730484009
Epoch 1380, training loss: 62.496253967285156 = 0.0880669504404068 + 10.0 * 6.240818977355957
Epoch 1380, val loss: 1.152446985244751
Epoch 1390, training loss: 62.49525451660156 = 0.08589871972799301 + 10.0 * 6.240935325622559
Epoch 1390, val loss: 1.1578048467636108
Epoch 1400, training loss: 62.510520935058594 = 0.08378621190786362 + 10.0 * 6.242673397064209
Epoch 1400, val loss: 1.1632616519927979
Epoch 1410, training loss: 62.47550964355469 = 0.08171842992305756 + 10.0 * 6.239378929138184
Epoch 1410, val loss: 1.1685221195220947
Epoch 1420, training loss: 62.4703369140625 = 0.07973837852478027 + 10.0 * 6.239059925079346
Epoch 1420, val loss: 1.174103021621704
Epoch 1430, training loss: 62.52597427368164 = 0.07782045006752014 + 10.0 * 6.244815349578857
Epoch 1430, val loss: 1.1797785758972168
Epoch 1440, training loss: 62.47365188598633 = 0.07593833655118942 + 10.0 * 6.239771366119385
Epoch 1440, val loss: 1.1853175163269043
Epoch 1450, training loss: 62.45430374145508 = 0.07412948459386826 + 10.0 * 6.2380170822143555
Epoch 1450, val loss: 1.1906729936599731
Epoch 1460, training loss: 62.447349548339844 = 0.07238929718732834 + 10.0 * 6.237496376037598
Epoch 1460, val loss: 1.1962997913360596
Epoch 1470, training loss: 62.52054214477539 = 0.07070018351078033 + 10.0 * 6.244984149932861
Epoch 1470, val loss: 1.201591968536377
Epoch 1480, training loss: 62.490196228027344 = 0.0690222755074501 + 10.0 * 6.242117404937744
Epoch 1480, val loss: 1.2070753574371338
Epoch 1490, training loss: 62.431541442871094 = 0.06741564720869064 + 10.0 * 6.236412525177002
Epoch 1490, val loss: 1.2129591703414917
Epoch 1500, training loss: 62.42326736450195 = 0.06587732583284378 + 10.0 * 6.235739231109619
Epoch 1500, val loss: 1.2184869050979614
Epoch 1510, training loss: 62.417388916015625 = 0.06439279019832611 + 10.0 * 6.235299587249756
Epoch 1510, val loss: 1.22419273853302
Epoch 1520, training loss: 62.44547653198242 = 0.06295560300350189 + 10.0 * 6.23825216293335
Epoch 1520, val loss: 1.2299294471740723
Epoch 1530, training loss: 62.42900466918945 = 0.061521075665950775 + 10.0 * 6.236748695373535
Epoch 1530, val loss: 1.2349635362625122
Epoch 1540, training loss: 62.40624237060547 = 0.060142967849969864 + 10.0 * 6.234610080718994
Epoch 1540, val loss: 1.2407195568084717
Epoch 1550, training loss: 62.418182373046875 = 0.0588109977543354 + 10.0 * 6.235937118530273
Epoch 1550, val loss: 1.2460365295410156
Epoch 1560, training loss: 62.408931732177734 = 0.057523153722286224 + 10.0 * 6.235140800476074
Epoch 1560, val loss: 1.251906394958496
Epoch 1570, training loss: 62.41412353515625 = 0.05626051500439644 + 10.0 * 6.235786437988281
Epoch 1570, val loss: 1.2572379112243652
Epoch 1580, training loss: 62.40776824951172 = 0.055035848170518875 + 10.0 * 6.235273361206055
Epoch 1580, val loss: 1.2625393867492676
Epoch 1590, training loss: 62.39152145385742 = 0.053852543234825134 + 10.0 * 6.233767032623291
Epoch 1590, val loss: 1.2676362991333008
Epoch 1600, training loss: 62.40989685058594 = 0.05270984396338463 + 10.0 * 6.235718727111816
Epoch 1600, val loss: 1.273580551147461
Epoch 1610, training loss: 62.368892669677734 = 0.051582444459199905 + 10.0 * 6.231730937957764
Epoch 1610, val loss: 1.2785959243774414
Epoch 1620, training loss: 62.36486053466797 = 0.050505343824625015 + 10.0 * 6.231435298919678
Epoch 1620, val loss: 1.284094214439392
Epoch 1630, training loss: 62.36399841308594 = 0.04946266859769821 + 10.0 * 6.231453895568848
Epoch 1630, val loss: 1.289656162261963
Epoch 1640, training loss: 62.4476203918457 = 0.04845072329044342 + 10.0 * 6.239916801452637
Epoch 1640, val loss: 1.2950544357299805
Epoch 1650, training loss: 62.359344482421875 = 0.047416459769010544 + 10.0 * 6.231192588806152
Epoch 1650, val loss: 1.299975037574768
Epoch 1660, training loss: 62.34609603881836 = 0.04645046964287758 + 10.0 * 6.229964256286621
Epoch 1660, val loss: 1.305227518081665
Epoch 1670, training loss: 62.3463249206543 = 0.04552244767546654 + 10.0 * 6.230080604553223
Epoch 1670, val loss: 1.3106015920639038
Epoch 1680, training loss: 62.416988372802734 = 0.04461967945098877 + 10.0 * 6.237236976623535
Epoch 1680, val loss: 1.3160293102264404
Epoch 1690, training loss: 62.360347747802734 = 0.043716803193092346 + 10.0 * 6.231663227081299
Epoch 1690, val loss: 1.3209415674209595
Epoch 1700, training loss: 62.3494987487793 = 0.04285049065947533 + 10.0 * 6.2306647300720215
Epoch 1700, val loss: 1.3263869285583496
Epoch 1710, training loss: 62.35783767700195 = 0.04201523959636688 + 10.0 * 6.231582164764404
Epoch 1710, val loss: 1.3315068483352661
Epoch 1720, training loss: 62.35560607910156 = 0.041192930191755295 + 10.0 * 6.231441497802734
Epoch 1720, val loss: 1.336144208908081
Epoch 1730, training loss: 62.328983306884766 = 0.040392544120550156 + 10.0 * 6.228858947753906
Epoch 1730, val loss: 1.3413336277008057
Epoch 1740, training loss: 62.32861328125 = 0.03962334990501404 + 10.0 * 6.228899002075195
Epoch 1740, val loss: 1.3467849493026733
Epoch 1750, training loss: 62.35108947753906 = 0.03886997327208519 + 10.0 * 6.231221675872803
Epoch 1750, val loss: 1.3514444828033447
Epoch 1760, training loss: 62.3214111328125 = 0.038131531327962875 + 10.0 * 6.228327751159668
Epoch 1760, val loss: 1.356663703918457
Epoch 1770, training loss: 62.32937240600586 = 0.03742262348532677 + 10.0 * 6.2291951179504395
Epoch 1770, val loss: 1.362014651298523
Epoch 1780, training loss: 62.307865142822266 = 0.03672278672456741 + 10.0 * 6.227114200592041
Epoch 1780, val loss: 1.3666859865188599
Epoch 1790, training loss: 62.30539321899414 = 0.03604980185627937 + 10.0 * 6.226934432983398
Epoch 1790, val loss: 1.3714103698730469
Epoch 1800, training loss: 62.33901596069336 = 0.03539760783314705 + 10.0 * 6.2303619384765625
Epoch 1800, val loss: 1.3764441013336182
Epoch 1810, training loss: 62.30743408203125 = 0.034746021032333374 + 10.0 * 6.227269172668457
Epoch 1810, val loss: 1.381872296333313
Epoch 1820, training loss: 62.301151275634766 = 0.034113816916942596 + 10.0 * 6.226703643798828
Epoch 1820, val loss: 1.386314034461975
Epoch 1830, training loss: 62.32248306274414 = 0.033512674272060394 + 10.0 * 6.2288970947265625
Epoch 1830, val loss: 1.3916677236557007
Epoch 1840, training loss: 62.28426742553711 = 0.032906971871852875 + 10.0 * 6.2251362800598145
Epoch 1840, val loss: 1.396080493927002
Epoch 1850, training loss: 62.2984733581543 = 0.03232972323894501 + 10.0 * 6.226614475250244
Epoch 1850, val loss: 1.4011114835739136
Epoch 1860, training loss: 62.303157806396484 = 0.03176416829228401 + 10.0 * 6.227139472961426
Epoch 1860, val loss: 1.4061435461044312
Epoch 1870, training loss: 62.28281021118164 = 0.031209807842969894 + 10.0 * 6.225160121917725
Epoch 1870, val loss: 1.4104385375976562
Epoch 1880, training loss: 62.28621292114258 = 0.030675141140818596 + 10.0 * 6.2255539894104
Epoch 1880, val loss: 1.4151527881622314
Epoch 1890, training loss: 62.294097900390625 = 0.03014943189918995 + 10.0 * 6.226395130157471
Epoch 1890, val loss: 1.4200431108474731
Epoch 1900, training loss: 62.27780532836914 = 0.029635636135935783 + 10.0 * 6.224816799163818
Epoch 1900, val loss: 1.424707055091858
Epoch 1910, training loss: 62.26251220703125 = 0.029136015102267265 + 10.0 * 6.223337650299072
Epoch 1910, val loss: 1.4292323589324951
Epoch 1920, training loss: 62.27399444580078 = 0.028654461726546288 + 10.0 * 6.224534034729004
Epoch 1920, val loss: 1.4337037801742554
Epoch 1930, training loss: 62.291690826416016 = 0.028180399909615517 + 10.0 * 6.226351261138916
Epoch 1930, val loss: 1.4385677576065063
Epoch 1940, training loss: 62.297515869140625 = 0.027708319947123528 + 10.0 * 6.226980686187744
Epoch 1940, val loss: 1.4430348873138428
Epoch 1950, training loss: 62.260963439941406 = 0.027246573939919472 + 10.0 * 6.223371505737305
Epoch 1950, val loss: 1.4472275972366333
Epoch 1960, training loss: 62.242706298828125 = 0.026806961745023727 + 10.0 * 6.221590042114258
Epoch 1960, val loss: 1.4519035816192627
Epoch 1970, training loss: 62.24089813232422 = 0.026384105905890465 + 10.0 * 6.221451759338379
Epoch 1970, val loss: 1.4567036628723145
Epoch 1980, training loss: 62.284183502197266 = 0.025972813367843628 + 10.0 * 6.225821018218994
Epoch 1980, val loss: 1.4614940881729126
Epoch 1990, training loss: 62.23390579223633 = 0.02554778754711151 + 10.0 * 6.2208356857299805
Epoch 1990, val loss: 1.4649370908737183
Epoch 2000, training loss: 62.23420333862305 = 0.025145532563328743 + 10.0 * 6.220905780792236
Epoch 2000, val loss: 1.4695806503295898
Epoch 2010, training loss: 62.293888092041016 = 0.02475646510720253 + 10.0 * 6.226912975311279
Epoch 2010, val loss: 1.4738942384719849
Epoch 2020, training loss: 62.28764724731445 = 0.024366997182369232 + 10.0 * 6.226327896118164
Epoch 2020, val loss: 1.4778000116348267
Epoch 2030, training loss: 62.24187469482422 = 0.023979276418685913 + 10.0 * 6.221789360046387
Epoch 2030, val loss: 1.481985330581665
Epoch 2040, training loss: 62.22113037109375 = 0.023620309308171272 + 10.0 * 6.219750881195068
Epoch 2040, val loss: 1.486464262008667
Epoch 2050, training loss: 62.214866638183594 = 0.02326735109090805 + 10.0 * 6.219160079956055
Epoch 2050, val loss: 1.4907537698745728
Epoch 2060, training loss: 62.21530532836914 = 0.022923408076167107 + 10.0 * 6.21923828125
Epoch 2060, val loss: 1.4948925971984863
Epoch 2070, training loss: 62.29658889770508 = 0.02258768491446972 + 10.0 * 6.227400302886963
Epoch 2070, val loss: 1.4987211227416992
Epoch 2080, training loss: 62.25680160522461 = 0.02224287949502468 + 10.0 * 6.223455905914307
Epoch 2080, val loss: 1.5032358169555664
Epoch 2090, training loss: 62.22775650024414 = 0.021907001733779907 + 10.0 * 6.220584869384766
Epoch 2090, val loss: 1.506780743598938
Epoch 2100, training loss: 62.23170852661133 = 0.021590115502476692 + 10.0 * 6.221011638641357
Epoch 2100, val loss: 1.5110772848129272
Epoch 2110, training loss: 62.20636749267578 = 0.02127622440457344 + 10.0 * 6.218509197235107
Epoch 2110, val loss: 1.5151424407958984
Epoch 2120, training loss: 62.21430206298828 = 0.02097376435995102 + 10.0 * 6.219332695007324
Epoch 2120, val loss: 1.5189317464828491
Epoch 2130, training loss: 62.24073791503906 = 0.020676152780652046 + 10.0 * 6.222006320953369
Epoch 2130, val loss: 1.5227876901626587
Epoch 2140, training loss: 62.221920013427734 = 0.02037794515490532 + 10.0 * 6.220154285430908
Epoch 2140, val loss: 1.5268120765686035
Epoch 2150, training loss: 62.20331573486328 = 0.020090481266379356 + 10.0 * 6.218322277069092
Epoch 2150, val loss: 1.5307409763336182
Epoch 2160, training loss: 62.211517333984375 = 0.01981295272707939 + 10.0 * 6.219170570373535
Epoch 2160, val loss: 1.534326195716858
Epoch 2170, training loss: 62.2031135559082 = 0.019535280764102936 + 10.0 * 6.218358039855957
Epoch 2170, val loss: 1.5382133722305298
Epoch 2180, training loss: 62.23228454589844 = 0.01926729641854763 + 10.0 * 6.221301555633545
Epoch 2180, val loss: 1.542223572731018
Epoch 2190, training loss: 62.20027542114258 = 0.01900097168982029 + 10.0 * 6.218127250671387
Epoch 2190, val loss: 1.5461158752441406
Epoch 2200, training loss: 62.19941711425781 = 0.018740389496088028 + 10.0 * 6.218067646026611
Epoch 2200, val loss: 1.549696683883667
Epoch 2210, training loss: 62.21388626098633 = 0.01848825439810753 + 10.0 * 6.219539642333984
Epoch 2210, val loss: 1.552855372428894
Epoch 2220, training loss: 62.18080520629883 = 0.0182392168790102 + 10.0 * 6.216256618499756
Epoch 2220, val loss: 1.5571978092193604
Epoch 2230, training loss: 62.17593002319336 = 0.017999760806560516 + 10.0 * 6.215792655944824
Epoch 2230, val loss: 1.5609110593795776
Epoch 2240, training loss: 62.18461227416992 = 0.017766591161489487 + 10.0 * 6.216684818267822
Epoch 2240, val loss: 1.5644694566726685
Epoch 2250, training loss: 62.22621154785156 = 0.017531871795654297 + 10.0 * 6.220868110656738
Epoch 2250, val loss: 1.5678958892822266
Epoch 2260, training loss: 62.18611145019531 = 0.01729651167988777 + 10.0 * 6.21688175201416
Epoch 2260, val loss: 1.5714237689971924
Epoch 2270, training loss: 62.1708869934082 = 0.017070211470127106 + 10.0 * 6.215381622314453
Epoch 2270, val loss: 1.574878215789795
Epoch 2280, training loss: 62.170135498046875 = 0.016853565350174904 + 10.0 * 6.215328216552734
Epoch 2280, val loss: 1.5785502195358276
Epoch 2290, training loss: 62.20950698852539 = 0.01664232276380062 + 10.0 * 6.2192864418029785
Epoch 2290, val loss: 1.582291603088379
Epoch 2300, training loss: 62.218414306640625 = 0.01642749458551407 + 10.0 * 6.220198631286621
Epoch 2300, val loss: 1.5851408243179321
Epoch 2310, training loss: 62.18373107910156 = 0.016214260831475258 + 10.0 * 6.216752052307129
Epoch 2310, val loss: 1.588805913925171
Epoch 2320, training loss: 62.17814254760742 = 0.016014620661735535 + 10.0 * 6.216212749481201
Epoch 2320, val loss: 1.5924521684646606
Epoch 2330, training loss: 62.182945251464844 = 0.01581568643450737 + 10.0 * 6.216712951660156
Epoch 2330, val loss: 1.595895528793335
Epoch 2340, training loss: 62.171234130859375 = 0.015617641620337963 + 10.0 * 6.215561866760254
Epoch 2340, val loss: 1.5989480018615723
Epoch 2350, training loss: 62.15007400512695 = 0.015426921658217907 + 10.0 * 6.213464736938477
Epoch 2350, val loss: 1.6025317907333374
Epoch 2360, training loss: 62.15645217895508 = 0.015244048088788986 + 10.0 * 6.214120864868164
Epoch 2360, val loss: 1.6060457229614258
Epoch 2370, training loss: 62.19868087768555 = 0.015064123086631298 + 10.0 * 6.218361854553223
Epoch 2370, val loss: 1.6095492839813232
Epoch 2380, training loss: 62.178836822509766 = 0.014874073676764965 + 10.0 * 6.216396331787109
Epoch 2380, val loss: 1.6118366718292236
Epoch 2390, training loss: 62.16783142089844 = 0.014695159159600735 + 10.0 * 6.215313911437988
Epoch 2390, val loss: 1.6154040098190308
Epoch 2400, training loss: 62.1828727722168 = 0.014518413692712784 + 10.0 * 6.2168354988098145
Epoch 2400, val loss: 1.618056297302246
Epoch 2410, training loss: 62.15443801879883 = 0.014346601441502571 + 10.0 * 6.2140092849731445
Epoch 2410, val loss: 1.6217604875564575
Epoch 2420, training loss: 62.14719772338867 = 0.014177772216498852 + 10.0 * 6.213301658630371
Epoch 2420, val loss: 1.6248772144317627
Epoch 2430, training loss: 62.158790588378906 = 0.014017508365213871 + 10.0 * 6.214477062225342
Epoch 2430, val loss: 1.628595232963562
Epoch 2440, training loss: 62.14840316772461 = 0.013851138763129711 + 10.0 * 6.2134552001953125
Epoch 2440, val loss: 1.6310069561004639
Epoch 2450, training loss: 62.14125442504883 = 0.013690328225493431 + 10.0 * 6.212756156921387
Epoch 2450, val loss: 1.6338894367218018
Epoch 2460, training loss: 62.14499282836914 = 0.013538210652768612 + 10.0 * 6.2131452560424805
Epoch 2460, val loss: 1.6371862888336182
Epoch 2470, training loss: 62.173404693603516 = 0.013385449536144733 + 10.0 * 6.216001987457275
Epoch 2470, val loss: 1.6398732662200928
Epoch 2480, training loss: 62.1497802734375 = 0.013230639509856701 + 10.0 * 6.2136549949646
Epoch 2480, val loss: 1.6431629657745361
Epoch 2490, training loss: 62.160797119140625 = 0.013083730824291706 + 10.0 * 6.214771270751953
Epoch 2490, val loss: 1.6461611986160278
Epoch 2500, training loss: 62.153438568115234 = 0.012935122475028038 + 10.0 * 6.21405029296875
Epoch 2500, val loss: 1.6486784219741821
Epoch 2510, training loss: 62.129150390625 = 0.012790066190063953 + 10.0 * 6.211636066436768
Epoch 2510, val loss: 1.6517671346664429
Epoch 2520, training loss: 62.14719772338867 = 0.012651091441512108 + 10.0 * 6.213454723358154
Epoch 2520, val loss: 1.6545097827911377
Epoch 2530, training loss: 62.1415901184082 = 0.012511685490608215 + 10.0 * 6.212907791137695
Epoch 2530, val loss: 1.6572916507720947
Epoch 2540, training loss: 62.125274658203125 = 0.01237531378865242 + 10.0 * 6.211289882659912
Epoch 2540, val loss: 1.6606515645980835
Epoch 2550, training loss: 62.14573669433594 = 0.012244409881532192 + 10.0 * 6.213349342346191
Epoch 2550, val loss: 1.6634377241134644
Epoch 2560, training loss: 62.13523483276367 = 0.012112559750676155 + 10.0 * 6.2123122215271
Epoch 2560, val loss: 1.666029453277588
Epoch 2570, training loss: 62.117942810058594 = 0.011980844661593437 + 10.0 * 6.210596084594727
Epoch 2570, val loss: 1.6684277057647705
Epoch 2580, training loss: 62.10849380493164 = 0.011856152676045895 + 10.0 * 6.2096638679504395
Epoch 2580, val loss: 1.6714868545532227
Epoch 2590, training loss: 62.118309020996094 = 0.011735227890312672 + 10.0 * 6.210657596588135
Epoch 2590, val loss: 1.6744097471237183
Epoch 2600, training loss: 62.189598083496094 = 0.011612383648753166 + 10.0 * 6.217798709869385
Epoch 2600, val loss: 1.6764194965362549
Epoch 2610, training loss: 62.127323150634766 = 0.011485685594379902 + 10.0 * 6.211583614349365
Epoch 2610, val loss: 1.679181456565857
Epoch 2620, training loss: 62.10822296142578 = 0.01136747281998396 + 10.0 * 6.209685325622559
Epoch 2620, val loss: 1.6820262670516968
Epoch 2630, training loss: 62.11496353149414 = 0.011254413053393364 + 10.0 * 6.210371017456055
Epoch 2630, val loss: 1.6846320629119873
Epoch 2640, training loss: 62.132720947265625 = 0.011141600087285042 + 10.0 * 6.212157726287842
Epoch 2640, val loss: 1.6869968175888062
Epoch 2650, training loss: 62.11674499511719 = 0.011027367785573006 + 10.0 * 6.210571765899658
Epoch 2650, val loss: 1.6896699666976929
Epoch 2660, training loss: 62.12885284423828 = 0.010916471481323242 + 10.0 * 6.211793422698975
Epoch 2660, val loss: 1.691706657409668
Epoch 2670, training loss: 62.11452865600586 = 0.010805387981235981 + 10.0 * 6.210371971130371
Epoch 2670, val loss: 1.6948069334030151
Epoch 2680, training loss: 62.112308502197266 = 0.010698421858251095 + 10.0 * 6.210160732269287
Epoch 2680, val loss: 1.697295069694519
Epoch 2690, training loss: 62.10966873168945 = 0.010592841543257236 + 10.0 * 6.209907531738281
Epoch 2690, val loss: 1.6999289989471436
Epoch 2700, training loss: 62.11197280883789 = 0.010489250533282757 + 10.0 * 6.210148334503174
Epoch 2700, val loss: 1.7021878957748413
Epoch 2710, training loss: 62.111080169677734 = 0.010387483052909374 + 10.0 * 6.210069179534912
Epoch 2710, val loss: 1.7049070596694946
Epoch 2720, training loss: 62.111026763916016 = 0.010287895798683167 + 10.0 * 6.210073947906494
Epoch 2720, val loss: 1.7076576948165894
Epoch 2730, training loss: 62.09667205810547 = 0.010187748819589615 + 10.0 * 6.208648204803467
Epoch 2730, val loss: 1.7096278667449951
Epoch 2740, training loss: 62.096839904785156 = 0.0100908437743783 + 10.0 * 6.208674907684326
Epoch 2740, val loss: 1.7118650674819946
Epoch 2750, training loss: 62.120147705078125 = 0.009995986707508564 + 10.0 * 6.211015224456787
Epoch 2750, val loss: 1.71416175365448
Epoch 2760, training loss: 62.08414077758789 = 0.009899575263261795 + 10.0 * 6.207424163818359
Epoch 2760, val loss: 1.716665267944336
Epoch 2770, training loss: 62.09804916381836 = 0.009808531031012535 + 10.0 * 6.208824157714844
Epoch 2770, val loss: 1.7193249464035034
Epoch 2780, training loss: 62.12778854370117 = 0.00971511285752058 + 10.0 * 6.2118072509765625
Epoch 2780, val loss: 1.7210651636123657
Epoch 2790, training loss: 62.0973014831543 = 0.009622696787118912 + 10.0 * 6.208767890930176
Epoch 2790, val loss: 1.7229094505310059
Epoch 2800, training loss: 62.11673355102539 = 0.009534883312880993 + 10.0 * 6.210719585418701
Epoch 2800, val loss: 1.7253892421722412
Epoch 2810, training loss: 62.08755874633789 = 0.009446288459002972 + 10.0 * 6.20781135559082
Epoch 2810, val loss: 1.7277792692184448
Epoch 2820, training loss: 62.100162506103516 = 0.009360520169138908 + 10.0 * 6.209080219268799
Epoch 2820, val loss: 1.730507254600525
Epoch 2830, training loss: 62.10113525390625 = 0.009272925555706024 + 10.0 * 6.20918607711792
Epoch 2830, val loss: 1.7319190502166748
Epoch 2840, training loss: 62.07799530029297 = 0.009189129807054996 + 10.0 * 6.206880569458008
Epoch 2840, val loss: 1.734175682067871
Epoch 2850, training loss: 62.07359313964844 = 0.009109300561249256 + 10.0 * 6.206448554992676
Epoch 2850, val loss: 1.7367501258850098
Epoch 2860, training loss: 62.069313049316406 = 0.009030596353113651 + 10.0 * 6.206028461456299
Epoch 2860, val loss: 1.7388144731521606
Epoch 2870, training loss: 62.09012985229492 = 0.008953195996582508 + 10.0 * 6.208117485046387
Epoch 2870, val loss: 1.74077308177948
Epoch 2880, training loss: 62.11660385131836 = 0.008871672675013542 + 10.0 * 6.21077299118042
Epoch 2880, val loss: 1.7421880960464478
Epoch 2890, training loss: 62.08396911621094 = 0.0087890038266778 + 10.0 * 6.207518100738525
Epoch 2890, val loss: 1.7448186874389648
Epoch 2900, training loss: 62.07234191894531 = 0.008711312897503376 + 10.0 * 6.206362724304199
Epoch 2900, val loss: 1.7464417219161987
Epoch 2910, training loss: 62.0737419128418 = 0.008637591265141964 + 10.0 * 6.206510543823242
Epoch 2910, val loss: 1.7487151622772217
Epoch 2920, training loss: 62.07666778564453 = 0.008565547876060009 + 10.0 * 6.206810474395752
Epoch 2920, val loss: 1.7506933212280273
Epoch 2930, training loss: 62.071067810058594 = 0.008492794819176197 + 10.0 * 6.2062578201293945
Epoch 2930, val loss: 1.7529618740081787
Epoch 2940, training loss: 62.06909942626953 = 0.008422109298408031 + 10.0 * 6.2060675621032715
Epoch 2940, val loss: 1.7550371885299683
Epoch 2950, training loss: 62.10166931152344 = 0.00835075881332159 + 10.0 * 6.20933198928833
Epoch 2950, val loss: 1.7567442655563354
Epoch 2960, training loss: 62.08422088623047 = 0.008276795968413353 + 10.0 * 6.207594394683838
Epoch 2960, val loss: 1.7581754922866821
Epoch 2970, training loss: 62.09471130371094 = 0.008209142833948135 + 10.0 * 6.2086501121521
Epoch 2970, val loss: 1.760899543762207
Epoch 2980, training loss: 62.075809478759766 = 0.008138343691825867 + 10.0 * 6.2067670822143555
Epoch 2980, val loss: 1.7622756958007812
Epoch 2990, training loss: 62.06450653076172 = 0.008071417920291424 + 10.0 * 6.205643653869629
Epoch 2990, val loss: 1.7640405893325806
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8154981549815499
The final CL Acc:0.69877, 0.01522, The final GNN Acc:0.81427, 0.00334
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13252])
remove edge: torch.Size([2, 7940])
updated graph: torch.Size([2, 10636])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.89119720458984 = 1.9232470989227295 + 10.0 * 8.596795082092285
Epoch 0, val loss: 1.924438714981079
Epoch 10, training loss: 87.87336730957031 = 1.9142075777053833 + 10.0 * 8.595915794372559
Epoch 10, val loss: 1.9156986474990845
Epoch 20, training loss: 87.79859924316406 = 1.902917504310608 + 10.0 * 8.589568138122559
Epoch 20, val loss: 1.9043002128601074
Epoch 30, training loss: 87.39407348632812 = 1.888093113899231 + 10.0 * 8.55059814453125
Epoch 30, val loss: 1.8893029689788818
Epoch 40, training loss: 85.3366470336914 = 1.8700740337371826 + 10.0 * 8.346657752990723
Epoch 40, val loss: 1.871584415435791
Epoch 50, training loss: 80.51627349853516 = 1.848227620124817 + 10.0 * 7.866804122924805
Epoch 50, val loss: 1.850768804550171
Epoch 60, training loss: 76.74922943115234 = 1.8329143524169922 + 10.0 * 7.491631507873535
Epoch 60, val loss: 1.8375557661056519
Epoch 70, training loss: 73.40711975097656 = 1.8243850469589233 + 10.0 * 7.158273696899414
Epoch 70, val loss: 1.8292051553726196
Epoch 80, training loss: 71.5947036743164 = 1.8176132440567017 + 10.0 * 6.9777092933654785
Epoch 80, val loss: 1.821508765220642
Epoch 90, training loss: 70.46736145019531 = 1.8093245029449463 + 10.0 * 6.865803241729736
Epoch 90, val loss: 1.8121824264526367
Epoch 100, training loss: 69.50724029541016 = 1.8015477657318115 + 10.0 * 6.770569801330566
Epoch 100, val loss: 1.803489327430725
Epoch 110, training loss: 68.76470947265625 = 1.794613242149353 + 10.0 * 6.697009563446045
Epoch 110, val loss: 1.7956080436706543
Epoch 120, training loss: 68.14151763916016 = 1.787597894668579 + 10.0 * 6.635392189025879
Epoch 120, val loss: 1.7878466844558716
Epoch 130, training loss: 67.68194580078125 = 1.780348300933838 + 10.0 * 6.5901594161987305
Epoch 130, val loss: 1.7800984382629395
Epoch 140, training loss: 67.3492660522461 = 1.77226722240448 + 10.0 * 6.557699203491211
Epoch 140, val loss: 1.7715998888015747
Epoch 150, training loss: 67.0634765625 = 1.7633436918258667 + 10.0 * 6.530013561248779
Epoch 150, val loss: 1.7623859643936157
Epoch 160, training loss: 66.83776092529297 = 1.7537736892700195 + 10.0 * 6.508398056030273
Epoch 160, val loss: 1.7526829242706299
Epoch 170, training loss: 66.6387939453125 = 1.7433446645736694 + 10.0 * 6.489544868469238
Epoch 170, val loss: 1.7423629760742188
Epoch 180, training loss: 66.47361755371094 = 1.7320431470870972 + 10.0 * 6.474157810211182
Epoch 180, val loss: 1.73135507106781
Epoch 190, training loss: 66.32378387451172 = 1.7197043895721436 + 10.0 * 6.4604082107543945
Epoch 190, val loss: 1.7195312976837158
Epoch 200, training loss: 66.20693969726562 = 1.7061350345611572 + 10.0 * 6.450079917907715
Epoch 200, val loss: 1.706666350364685
Epoch 210, training loss: 66.0738296508789 = 1.6912237405776978 + 10.0 * 6.438260555267334
Epoch 210, val loss: 1.6927324533462524
Epoch 220, training loss: 65.96393585205078 = 1.6749944686889648 + 10.0 * 6.42889404296875
Epoch 220, val loss: 1.6776624917984009
Epoch 230, training loss: 65.85945892333984 = 1.6572037935256958 + 10.0 * 6.420225620269775
Epoch 230, val loss: 1.6613471508026123
Epoch 240, training loss: 65.75275421142578 = 1.6379836797714233 + 10.0 * 6.4114766120910645
Epoch 240, val loss: 1.6438218355178833
Epoch 250, training loss: 65.6502456665039 = 1.6172541379928589 + 10.0 * 6.403299331665039
Epoch 250, val loss: 1.6250635385513306
Epoch 260, training loss: 65.59294128417969 = 1.5949897766113281 + 10.0 * 6.3997955322265625
Epoch 260, val loss: 1.6050721406936646
Epoch 270, training loss: 65.4814453125 = 1.5710551738739014 + 10.0 * 6.39103889465332
Epoch 270, val loss: 1.5839191675186157
Epoch 280, training loss: 65.3917007446289 = 1.545802354812622 + 10.0 * 6.384590148925781
Epoch 280, val loss: 1.561689019203186
Epoch 290, training loss: 65.30377960205078 = 1.5192166566848755 + 10.0 * 6.378456115722656
Epoch 290, val loss: 1.5387285947799683
Epoch 300, training loss: 65.22476959228516 = 1.4914432764053345 + 10.0 * 6.373332500457764
Epoch 300, val loss: 1.5149623155593872
Epoch 310, training loss: 65.1404800415039 = 1.4625786542892456 + 10.0 * 6.367790222167969
Epoch 310, val loss: 1.4906504154205322
Epoch 320, training loss: 65.06279754638672 = 1.4328248500823975 + 10.0 * 6.362997055053711
Epoch 320, val loss: 1.465968370437622
Epoch 330, training loss: 65.0399169921875 = 1.4023038148880005 + 10.0 * 6.363760948181152
Epoch 330, val loss: 1.4410020112991333
Epoch 340, training loss: 64.90299224853516 = 1.371596336364746 + 10.0 * 6.353139877319336
Epoch 340, val loss: 1.4163564443588257
Epoch 350, training loss: 64.83897399902344 = 1.3407193422317505 + 10.0 * 6.349825382232666
Epoch 350, val loss: 1.3919085264205933
Epoch 360, training loss: 64.7608871459961 = 1.3097352981567383 + 10.0 * 6.3451151847839355
Epoch 360, val loss: 1.367772102355957
Epoch 370, training loss: 64.70089721679688 = 1.278772234916687 + 10.0 * 6.342212200164795
Epoch 370, val loss: 1.344003677368164
Epoch 380, training loss: 64.65843200683594 = 1.247831106185913 + 10.0 * 6.341059684753418
Epoch 380, val loss: 1.320724368095398
Epoch 390, training loss: 64.5757064819336 = 1.2173861265182495 + 10.0 * 6.335832118988037
Epoch 390, val loss: 1.2980138063430786
Epoch 400, training loss: 64.50675201416016 = 1.187451720237732 + 10.0 * 6.331930160522461
Epoch 400, val loss: 1.2760660648345947
Epoch 410, training loss: 64.470947265625 = 1.1579577922821045 + 10.0 * 6.331298828125
Epoch 410, val loss: 1.2546947002410889
Epoch 420, training loss: 64.40013885498047 = 1.1290768384933472 + 10.0 * 6.327106475830078
Epoch 420, val loss: 1.2340445518493652
Epoch 430, training loss: 64.33464813232422 = 1.100870132446289 + 10.0 * 6.32337760925293
Epoch 430, val loss: 1.2141510248184204
Epoch 440, training loss: 64.28805541992188 = 1.0732612609863281 + 10.0 * 6.321479320526123
Epoch 440, val loss: 1.194947600364685
Epoch 450, training loss: 64.2327880859375 = 1.0462545156478882 + 10.0 * 6.318653583526611
Epoch 450, val loss: 1.1764999628067017
Epoch 460, training loss: 64.18903350830078 = 1.0199342966079712 + 10.0 * 6.316910266876221
Epoch 460, val loss: 1.1588243246078491
Epoch 470, training loss: 64.14350128173828 = 0.9943103194236755 + 10.0 * 6.3149189949035645
Epoch 470, val loss: 1.1417990922927856
Epoch 480, training loss: 64.08307647705078 = 0.9692875742912292 + 10.0 * 6.3113789558410645
Epoch 480, val loss: 1.1255916357040405
Epoch 490, training loss: 64.03484344482422 = 0.9449531435966492 + 10.0 * 6.30898904800415
Epoch 490, val loss: 1.1100902557373047
Epoch 500, training loss: 64.01639556884766 = 0.9212835431098938 + 10.0 * 6.309511184692383
Epoch 500, val loss: 1.0952860116958618
Epoch 510, training loss: 63.95555877685547 = 0.8981389403343201 + 10.0 * 6.305741786956787
Epoch 510, val loss: 1.0811512470245361
Epoch 520, training loss: 63.90865707397461 = 0.87567138671875 + 10.0 * 6.303298473358154
Epoch 520, val loss: 1.0678389072418213
Epoch 530, training loss: 63.86648941040039 = 0.8539220094680786 + 10.0 * 6.3012566566467285
Epoch 530, val loss: 1.0551773309707642
Epoch 540, training loss: 63.86071014404297 = 0.8327001929283142 + 10.0 * 6.302801132202148
Epoch 540, val loss: 1.0432368516921997
Epoch 550, training loss: 63.8071174621582 = 0.8120877742767334 + 10.0 * 6.299502849578857
Epoch 550, val loss: 1.031887173652649
Epoch 560, training loss: 63.76282501220703 = 0.7920169830322266 + 10.0 * 6.297080993652344
Epoch 560, val loss: 1.021101713180542
Epoch 570, training loss: 63.73139190673828 = 0.7724788784980774 + 10.0 * 6.295891284942627
Epoch 570, val loss: 1.0110129117965698
Epoch 580, training loss: 63.68730545043945 = 0.7534637451171875 + 10.0 * 6.293384075164795
Epoch 580, val loss: 1.0014804601669312
Epoch 590, training loss: 63.6452751159668 = 0.7350436449050903 + 10.0 * 6.291023254394531
Epoch 590, val loss: 0.9925218224525452
Epoch 600, training loss: 63.63087844848633 = 0.7170463800430298 + 10.0 * 6.291383266448975
Epoch 600, val loss: 0.984027087688446
Epoch 610, training loss: 63.5954704284668 = 0.6993511915206909 + 10.0 * 6.28961181640625
Epoch 610, val loss: 0.9758715629577637
Epoch 620, training loss: 63.559326171875 = 0.682166576385498 + 10.0 * 6.287715911865234
Epoch 620, val loss: 0.9682938456535339
Epoch 630, training loss: 63.53147506713867 = 0.6653879284858704 + 10.0 * 6.286608695983887
Epoch 630, val loss: 0.9611595869064331
Epoch 640, training loss: 63.52193832397461 = 0.6489458084106445 + 10.0 * 6.287299156188965
Epoch 640, val loss: 0.9541343450546265
Epoch 650, training loss: 63.46989059448242 = 0.6329458355903625 + 10.0 * 6.283694267272949
Epoch 650, val loss: 0.9476848244667053
Epoch 660, training loss: 63.457637786865234 = 0.6173121333122253 + 10.0 * 6.284032344818115
Epoch 660, val loss: 0.9415521025657654
Epoch 670, training loss: 63.404788970947266 = 0.60201096534729 + 10.0 * 6.280277729034424
Epoch 670, val loss: 0.9355230331420898
Epoch 680, training loss: 63.37757873535156 = 0.5870558619499207 + 10.0 * 6.279052257537842
Epoch 680, val loss: 0.9299668073654175
Epoch 690, training loss: 63.35361862182617 = 0.5724689960479736 + 10.0 * 6.278115272521973
Epoch 690, val loss: 0.9246413111686707
Epoch 700, training loss: 63.353981018066406 = 0.5581440925598145 + 10.0 * 6.279583930969238
Epoch 700, val loss: 0.9193884134292603
Epoch 710, training loss: 63.31644821166992 = 0.5440428853034973 + 10.0 * 6.27724027633667
Epoch 710, val loss: 0.91440349817276
Epoch 720, training loss: 63.272464752197266 = 0.5303443074226379 + 10.0 * 6.274211883544922
Epoch 720, val loss: 0.9096329212188721
Epoch 730, training loss: 63.25732421875 = 0.516979455947876 + 10.0 * 6.27403450012207
Epoch 730, val loss: 0.9050249457359314
Epoch 740, training loss: 63.25138473510742 = 0.5037973523139954 + 10.0 * 6.274758815765381
Epoch 740, val loss: 0.9005295634269714
Epoch 750, training loss: 63.20156478881836 = 0.49085667729377747 + 10.0 * 6.271070957183838
Epoch 750, val loss: 0.8961611986160278
Epoch 760, training loss: 63.2027587890625 = 0.4782082438468933 + 10.0 * 6.272455215454102
Epoch 760, val loss: 0.8919597864151001
Epoch 770, training loss: 63.1693115234375 = 0.46583595871925354 + 10.0 * 6.270347595214844
Epoch 770, val loss: 0.8878436088562012
Epoch 780, training loss: 63.14500045776367 = 0.45361459255218506 + 10.0 * 6.269138813018799
Epoch 780, val loss: 0.8838201761245728
Epoch 790, training loss: 63.115394592285156 = 0.44173529744148254 + 10.0 * 6.2673659324646
Epoch 790, val loss: 0.8800333142280579
Epoch 800, training loss: 63.12709426879883 = 0.43004927039146423 + 10.0 * 6.269704341888428
Epoch 800, val loss: 0.8763212561607361
Epoch 810, training loss: 63.0816535949707 = 0.4185226559638977 + 10.0 * 6.266313076019287
Epoch 810, val loss: 0.872742772102356
Epoch 820, training loss: 63.067935943603516 = 0.40725183486938477 + 10.0 * 6.266068458557129
Epoch 820, val loss: 0.8692505359649658
Epoch 830, training loss: 63.03693771362305 = 0.39610645174980164 + 10.0 * 6.264082908630371
Epoch 830, val loss: 0.8658215403556824
Epoch 840, training loss: 63.022254943847656 = 0.3851945996284485 + 10.0 * 6.263706207275391
Epoch 840, val loss: 0.8625338673591614
Epoch 850, training loss: 62.99399185180664 = 0.37455514073371887 + 10.0 * 6.261943817138672
Epoch 850, val loss: 0.8594397306442261
Epoch 860, training loss: 63.00096130371094 = 0.36408472061157227 + 10.0 * 6.263687610626221
Epoch 860, val loss: 0.8564647436141968
Epoch 870, training loss: 62.974464416503906 = 0.3538142144680023 + 10.0 * 6.2620649337768555
Epoch 870, val loss: 0.8535248637199402
Epoch 880, training loss: 62.946739196777344 = 0.343723326921463 + 10.0 * 6.26030158996582
Epoch 880, val loss: 0.8507584929466248
Epoch 890, training loss: 62.928321838378906 = 0.33390000462532043 + 10.0 * 6.259442329406738
Epoch 890, val loss: 0.8481360077857971
Epoch 900, training loss: 62.907630920410156 = 0.32423949241638184 + 10.0 * 6.2583394050598145
Epoch 900, val loss: 0.8456776738166809
Epoch 910, training loss: 62.89458465576172 = 0.3148110806941986 + 10.0 * 6.257977485656738
Epoch 910, val loss: 0.8433671593666077
Epoch 920, training loss: 62.89834213256836 = 0.30556488037109375 + 10.0 * 6.259277820587158
Epoch 920, val loss: 0.8411837816238403
Epoch 930, training loss: 62.915260314941406 = 0.29649055004119873 + 10.0 * 6.261877059936523
Epoch 930, val loss: 0.8390759825706482
Epoch 940, training loss: 62.8509635925293 = 0.2876823842525482 + 10.0 * 6.256328105926514
Epoch 940, val loss: 0.8371649384498596
Epoch 950, training loss: 62.82958984375 = 0.2791192829608917 + 10.0 * 6.25504732131958
Epoch 950, val loss: 0.8355855345726013
Epoch 960, training loss: 62.80775833129883 = 0.27084556221961975 + 10.0 * 6.25369119644165
Epoch 960, val loss: 0.8342001438140869
Epoch 970, training loss: 62.80464553833008 = 0.26278045773506165 + 10.0 * 6.254186630249023
Epoch 970, val loss: 0.8330119848251343
Epoch 980, training loss: 62.77775192260742 = 0.2549367845058441 + 10.0 * 6.252281665802002
Epoch 980, val loss: 0.8320124745368958
Epoch 990, training loss: 62.78648376464844 = 0.24733605980873108 + 10.0 * 6.253914833068848
Epoch 990, val loss: 0.8312650322914124
Epoch 1000, training loss: 62.7909049987793 = 0.23985767364501953 + 10.0 * 6.255105018615723
Epoch 1000, val loss: 0.8305135369300842
Epoch 1010, training loss: 62.74600601196289 = 0.23268984258174896 + 10.0 * 6.251331806182861
Epoch 1010, val loss: 0.8301828503608704
Epoch 1020, training loss: 62.71730422973633 = 0.22572951018810272 + 10.0 * 6.249157428741455
Epoch 1020, val loss: 0.8300679922103882
Epoch 1030, training loss: 62.70351028442383 = 0.21904483437538147 + 10.0 * 6.248446464538574
Epoch 1030, val loss: 0.83021080493927
Epoch 1040, training loss: 62.76336669921875 = 0.21253502368927002 + 10.0 * 6.255083084106445
Epoch 1040, val loss: 0.8305537104606628
Epoch 1050, training loss: 62.714698791503906 = 0.20625439286231995 + 10.0 * 6.250844478607178
Epoch 1050, val loss: 0.8309871554374695
Epoch 1060, training loss: 62.67205047607422 = 0.20012342929840088 + 10.0 * 6.247192859649658
Epoch 1060, val loss: 0.8317158818244934
Epoch 1070, training loss: 62.6622314453125 = 0.19425660371780396 + 10.0 * 6.246797561645508
Epoch 1070, val loss: 0.8326815366744995
Epoch 1080, training loss: 62.678993225097656 = 0.18860270082950592 + 10.0 * 6.249039173126221
Epoch 1080, val loss: 0.8338214159011841
Epoch 1090, training loss: 62.645660400390625 = 0.18309202790260315 + 10.0 * 6.2462568283081055
Epoch 1090, val loss: 0.8350133895874023
Epoch 1100, training loss: 62.62754821777344 = 0.17778001725673676 + 10.0 * 6.244976997375488
Epoch 1100, val loss: 0.8364884853363037
Epoch 1110, training loss: 62.61281967163086 = 0.17265155911445618 + 10.0 * 6.244016647338867
Epoch 1110, val loss: 0.8381274342536926
Epoch 1120, training loss: 62.623817443847656 = 0.16769985854625702 + 10.0 * 6.245611667633057
Epoch 1120, val loss: 0.839974582195282
Epoch 1130, training loss: 62.625328063964844 = 0.1628965586423874 + 10.0 * 6.246243476867676
Epoch 1130, val loss: 0.8419665098190308
Epoch 1140, training loss: 62.594451904296875 = 0.15823428332805634 + 10.0 * 6.243621826171875
Epoch 1140, val loss: 0.84401535987854
Epoch 1150, training loss: 62.57911682128906 = 0.15376631915569305 + 10.0 * 6.24253511428833
Epoch 1150, val loss: 0.8464686274528503
Epoch 1160, training loss: 62.63214111328125 = 0.14945076406002045 + 10.0 * 6.248269081115723
Epoch 1160, val loss: 0.848883330821991
Epoch 1170, training loss: 62.56726837158203 = 0.14520828425884247 + 10.0 * 6.24220609664917
Epoch 1170, val loss: 0.8514932990074158
Epoch 1180, training loss: 62.54332733154297 = 0.14117783308029175 + 10.0 * 6.240214824676514
Epoch 1180, val loss: 0.8543399572372437
Epoch 1190, training loss: 62.57707214355469 = 0.13730919361114502 + 10.0 * 6.24397611618042
Epoch 1190, val loss: 0.8573755621910095
Epoch 1200, training loss: 62.53574752807617 = 0.13347448408603668 + 10.0 * 6.240227222442627
Epoch 1200, val loss: 0.8602889180183411
Epoch 1210, training loss: 62.52164840698242 = 0.12982842326164246 + 10.0 * 6.239181995391846
Epoch 1210, val loss: 0.863554060459137
Epoch 1220, training loss: 62.52358627319336 = 0.12629282474517822 + 10.0 * 6.239729404449463
Epoch 1220, val loss: 0.8668763637542725
Epoch 1230, training loss: 62.51786422729492 = 0.12287850677967072 + 10.0 * 6.239498615264893
Epoch 1230, val loss: 0.8702720999717712
Epoch 1240, training loss: 62.50428009033203 = 0.11956540495157242 + 10.0 * 6.238471508026123
Epoch 1240, val loss: 0.8737513422966003
Epoch 1250, training loss: 62.50032043457031 = 0.11635666340589523 + 10.0 * 6.238396644592285
Epoch 1250, val loss: 0.8773513436317444
Epoch 1260, training loss: 62.49625015258789 = 0.11325404793024063 + 10.0 * 6.238299369812012
Epoch 1260, val loss: 0.8810457587242126
Epoch 1270, training loss: 62.51691818237305 = 0.1102486401796341 + 10.0 * 6.24066686630249
Epoch 1270, val loss: 0.8848211765289307
Epoch 1280, training loss: 62.48261642456055 = 0.1073344349861145 + 10.0 * 6.237528324127197
Epoch 1280, val loss: 0.8885589241981506
Epoch 1290, training loss: 62.464420318603516 = 0.10451529175043106 + 10.0 * 6.235990524291992
Epoch 1290, val loss: 0.8925155997276306
Epoch 1300, training loss: 62.452606201171875 = 0.10181757062673569 + 10.0 * 6.235078811645508
Epoch 1300, val loss: 0.8965764045715332
Epoch 1310, training loss: 62.4598274230957 = 0.09920243918895721 + 10.0 * 6.236062526702881
Epoch 1310, val loss: 0.9006187915802002
Epoch 1320, training loss: 62.45831298828125 = 0.09664158523082733 + 10.0 * 6.236166954040527
Epoch 1320, val loss: 0.9045754671096802
Epoch 1330, training loss: 62.45954895019531 = 0.09415384382009506 + 10.0 * 6.236539363861084
Epoch 1330, val loss: 0.9086658358573914
Epoch 1340, training loss: 62.435977935791016 = 0.09173769503831863 + 10.0 * 6.234424114227295
Epoch 1340, val loss: 0.9127522706985474
Epoch 1350, training loss: 62.42655944824219 = 0.08943147212266922 + 10.0 * 6.233712673187256
Epoch 1350, val loss: 0.9170399308204651
Epoch 1360, training loss: 62.4456787109375 = 0.08720351755619049 + 10.0 * 6.235847473144531
Epoch 1360, val loss: 0.9213302135467529
Epoch 1370, training loss: 62.411869049072266 = 0.08501297980546951 + 10.0 * 6.232685565948486
Epoch 1370, val loss: 0.9253643155097961
Epoch 1380, training loss: 62.406707763671875 = 0.08290759474039078 + 10.0 * 6.232379913330078
Epoch 1380, val loss: 0.9297361969947815
Epoch 1390, training loss: 62.42875289916992 = 0.08089138567447662 + 10.0 * 6.234786033630371
Epoch 1390, val loss: 0.934021532535553
Epoch 1400, training loss: 62.418880462646484 = 0.07887079566717148 + 10.0 * 6.2340006828308105
Epoch 1400, val loss: 0.9381833076477051
Epoch 1410, training loss: 62.392459869384766 = 0.07694768160581589 + 10.0 * 6.231551170349121
Epoch 1410, val loss: 0.9425216317176819
Epoch 1420, training loss: 62.39994430541992 = 0.0750780701637268 + 10.0 * 6.232486724853516
Epoch 1420, val loss: 0.9468432664871216
Epoch 1430, training loss: 62.3907356262207 = 0.07327635586261749 + 10.0 * 6.231745719909668
Epoch 1430, val loss: 0.9512437582015991
Epoch 1440, training loss: 62.384883880615234 = 0.07151224464178085 + 10.0 * 6.231337070465088
Epoch 1440, val loss: 0.9555086493492126
Epoch 1450, training loss: 62.3813591003418 = 0.06981944292783737 + 10.0 * 6.231153964996338
Epoch 1450, val loss: 0.9598965048789978
Epoch 1460, training loss: 62.361087799072266 = 0.06816359609365463 + 10.0 * 6.229292392730713
Epoch 1460, val loss: 0.9642374515533447
Epoch 1470, training loss: 62.36652374267578 = 0.0665692687034607 + 10.0 * 6.229995250701904
Epoch 1470, val loss: 0.9685813784599304
Epoch 1480, training loss: 62.376182556152344 = 0.06501065939664841 + 10.0 * 6.231117248535156
Epoch 1480, val loss: 0.9729476571083069
Epoch 1490, training loss: 62.35447692871094 = 0.06350451707839966 + 10.0 * 6.229097366333008
Epoch 1490, val loss: 0.9772917628288269
Epoch 1500, training loss: 62.33983612060547 = 0.06202365458011627 + 10.0 * 6.227781295776367
Epoch 1500, val loss: 0.9816173315048218
Epoch 1510, training loss: 62.3343620300293 = 0.060613472014665604 + 10.0 * 6.227375030517578
Epoch 1510, val loss: 0.9860038757324219
Epoch 1520, training loss: 62.36300277709961 = 0.05925211310386658 + 10.0 * 6.230374813079834
Epoch 1520, val loss: 0.9903530478477478
Epoch 1530, training loss: 62.3267707824707 = 0.05789478123188019 + 10.0 * 6.2268877029418945
Epoch 1530, val loss: 0.9945443868637085
Epoch 1540, training loss: 62.33274459838867 = 0.05659225583076477 + 10.0 * 6.2276153564453125
Epoch 1540, val loss: 0.9989422559738159
Epoch 1550, training loss: 62.34287643432617 = 0.05533456802368164 + 10.0 * 6.228754043579102
Epoch 1550, val loss: 1.0032037496566772
Epoch 1560, training loss: 62.336082458496094 = 0.05409885570406914 + 10.0 * 6.228198051452637
Epoch 1560, val loss: 1.0074228048324585
Epoch 1570, training loss: 62.31584167480469 = 0.05292080342769623 + 10.0 * 6.226292133331299
Epoch 1570, val loss: 1.0116961002349854
Epoch 1580, training loss: 62.32438278198242 = 0.051761988550424576 + 10.0 * 6.227262020111084
Epoch 1580, val loss: 1.0159807205200195
Epoch 1590, training loss: 62.314762115478516 = 0.05064248666167259 + 10.0 * 6.226411819458008
Epoch 1590, val loss: 1.0201970338821411
Epoch 1600, training loss: 62.3089485168457 = 0.04955076426267624 + 10.0 * 6.225939750671387
Epoch 1600, val loss: 1.0243772268295288
Epoch 1610, training loss: 62.3181037902832 = 0.0484917126595974 + 10.0 * 6.226961135864258
Epoch 1610, val loss: 1.0284754037857056
Epoch 1620, training loss: 62.29433822631836 = 0.047465696930885315 + 10.0 * 6.224687099456787
Epoch 1620, val loss: 1.0328401327133179
Epoch 1630, training loss: 62.326942443847656 = 0.04647359997034073 + 10.0 * 6.228046894073486
Epoch 1630, val loss: 1.036953330039978
Epoch 1640, training loss: 62.28951644897461 = 0.04548533633351326 + 10.0 * 6.224402904510498
Epoch 1640, val loss: 1.0409010648727417
Epoch 1650, training loss: 62.288761138916016 = 0.0445503368973732 + 10.0 * 6.22442102432251
Epoch 1650, val loss: 1.0451619625091553
Epoch 1660, training loss: 62.313568115234375 = 0.04363499954342842 + 10.0 * 6.226993083953857
Epoch 1660, val loss: 1.0492037534713745
Epoch 1670, training loss: 62.29029083251953 = 0.0427328459918499 + 10.0 * 6.224755764007568
Epoch 1670, val loss: 1.053348422050476
Epoch 1680, training loss: 62.269142150878906 = 0.041858792304992676 + 10.0 * 6.222728252410889
Epoch 1680, val loss: 1.0572509765625
Epoch 1690, training loss: 62.266597747802734 = 0.04101862385869026 + 10.0 * 6.22255802154541
Epoch 1690, val loss: 1.0613391399383545
Epoch 1700, training loss: 62.31608200073242 = 0.040194932371377945 + 10.0 * 6.227588653564453
Epoch 1700, val loss: 1.0652371644973755
Epoch 1710, training loss: 62.27741241455078 = 0.0393982008099556 + 10.0 * 6.223801612854004
Epoch 1710, val loss: 1.069143295288086
Epoch 1720, training loss: 62.267433166503906 = 0.03860216960310936 + 10.0 * 6.222883224487305
Epoch 1720, val loss: 1.0731706619262695
Epoch 1730, training loss: 62.253135681152344 = 0.03785576671361923 + 10.0 * 6.221528053283691
Epoch 1730, val loss: 1.0771187543869019
Epoch 1740, training loss: 62.291847229003906 = 0.037128742784261703 + 10.0 * 6.2254719734191895
Epoch 1740, val loss: 1.0812262296676636
Epoch 1750, training loss: 62.24029541015625 = 0.036399077624082565 + 10.0 * 6.220389366149902
Epoch 1750, val loss: 1.0848904848098755
Epoch 1760, training loss: 62.24066925048828 = 0.03570156916975975 + 10.0 * 6.220496654510498
Epoch 1760, val loss: 1.0889217853546143
Epoch 1770, training loss: 62.26559829711914 = 0.03503761067986488 + 10.0 * 6.223055839538574
Epoch 1770, val loss: 1.092866063117981
Epoch 1780, training loss: 62.236297607421875 = 0.034360285848379135 + 10.0 * 6.220193862915039
Epoch 1780, val loss: 1.096455454826355
Epoch 1790, training loss: 62.25406265258789 = 0.03371729701757431 + 10.0 * 6.222034454345703
Epoch 1790, val loss: 1.1004023551940918
Epoch 1800, training loss: 62.22587966918945 = 0.03308110311627388 + 10.0 * 6.219279766082764
Epoch 1800, val loss: 1.1040890216827393
Epoch 1810, training loss: 62.22856521606445 = 0.03247777000069618 + 10.0 * 6.219608783721924
Epoch 1810, val loss: 1.1079788208007812
Epoch 1820, training loss: 62.281070709228516 = 0.03188995271921158 + 10.0 * 6.224917888641357
Epoch 1820, val loss: 1.1117044687271118
Epoch 1830, training loss: 62.234867095947266 = 0.03130023181438446 + 10.0 * 6.2203569412231445
Epoch 1830, val loss: 1.1151769161224365
Epoch 1840, training loss: 62.2239990234375 = 0.030729562044143677 + 10.0 * 6.219326972961426
Epoch 1840, val loss: 1.1190438270568848
Epoch 1850, training loss: 62.2128791809082 = 0.030185064300894737 + 10.0 * 6.218269348144531
Epoch 1850, val loss: 1.122709035873413
Epoch 1860, training loss: 62.22074890136719 = 0.0296573955565691 + 10.0 * 6.219109058380127
Epoch 1860, val loss: 1.126509189605713
Epoch 1870, training loss: 62.244300842285156 = 0.02913721837103367 + 10.0 * 6.2215166091918945
Epoch 1870, val loss: 1.129976511001587
Epoch 1880, training loss: 62.244815826416016 = 0.028610991314053535 + 10.0 * 6.221620559692383
Epoch 1880, val loss: 1.133499264717102
Epoch 1890, training loss: 62.21068572998047 = 0.02809884026646614 + 10.0 * 6.218258857727051
Epoch 1890, val loss: 1.1369379758834839
Epoch 1900, training loss: 62.19913864135742 = 0.027619527652859688 + 10.0 * 6.217152118682861
Epoch 1900, val loss: 1.140694260597229
Epoch 1910, training loss: 62.19562530517578 = 0.02715374156832695 + 10.0 * 6.2168474197387695
Epoch 1910, val loss: 1.1442776918411255
Epoch 1920, training loss: 62.19742965698242 = 0.026700129732489586 + 10.0 * 6.2170729637146
Epoch 1920, val loss: 1.1478544473648071
Epoch 1930, training loss: 62.24504852294922 = 0.02625259943306446 + 10.0 * 6.221879482269287
Epoch 1930, val loss: 1.1512610912322998
Epoch 1940, training loss: 62.225643157958984 = 0.025799166411161423 + 10.0 * 6.219984531402588
Epoch 1940, val loss: 1.1545076370239258
Epoch 1950, training loss: 62.251708984375 = 0.02536669932305813 + 10.0 * 6.222634315490723
Epoch 1950, val loss: 1.1580106019973755
Epoch 1960, training loss: 62.190303802490234 = 0.02493825927376747 + 10.0 * 6.216536521911621
Epoch 1960, val loss: 1.161451816558838
Epoch 1970, training loss: 62.180362701416016 = 0.024531139060854912 + 10.0 * 6.215582847595215
Epoch 1970, val loss: 1.1648736000061035
Epoch 1980, training loss: 62.17605209350586 = 0.024134520441293716 + 10.0 * 6.215191841125488
Epoch 1980, val loss: 1.1683135032653809
Epoch 1990, training loss: 62.1821403503418 = 0.023750299587845802 + 10.0 * 6.21583890914917
Epoch 1990, val loss: 1.1716824769973755
Epoch 2000, training loss: 62.2255973815918 = 0.02337256446480751 + 10.0 * 6.220222473144531
Epoch 2000, val loss: 1.1748297214508057
Epoch 2010, training loss: 62.19404220581055 = 0.02300078235566616 + 10.0 * 6.217103958129883
Epoch 2010, val loss: 1.1783366203308105
Epoch 2020, training loss: 62.175262451171875 = 0.022628650069236755 + 10.0 * 6.215263366699219
Epoch 2020, val loss: 1.1815500259399414
Epoch 2030, training loss: 62.174537658691406 = 0.022280137985944748 + 10.0 * 6.215225696563721
Epoch 2030, val loss: 1.1850512027740479
Epoch 2040, training loss: 62.22883224487305 = 0.02193235419690609 + 10.0 * 6.2206902503967285
Epoch 2040, val loss: 1.1881844997406006
Epoch 2050, training loss: 62.19388198852539 = 0.021587632596492767 + 10.0 * 6.21722936630249
Epoch 2050, val loss: 1.191017985343933
Epoch 2060, training loss: 62.17534255981445 = 0.021252334117889404 + 10.0 * 6.215409278869629
Epoch 2060, val loss: 1.1944538354873657
Epoch 2070, training loss: 62.17998123168945 = 0.020932670682668686 + 10.0 * 6.21590518951416
Epoch 2070, val loss: 1.1975902318954468
Epoch 2080, training loss: 62.16395950317383 = 0.020614037290215492 + 10.0 * 6.214334487915039
Epoch 2080, val loss: 1.20066499710083
Epoch 2090, training loss: 62.15903091430664 = 0.020300259813666344 + 10.0 * 6.213872909545898
Epoch 2090, val loss: 1.2038493156433105
Epoch 2100, training loss: 62.16565704345703 = 0.020002074539661407 + 10.0 * 6.214565753936768
Epoch 2100, val loss: 1.2069873809814453
Epoch 2110, training loss: 62.18085861206055 = 0.019711969420313835 + 10.0 * 6.2161149978637695
Epoch 2110, val loss: 1.2100650072097778
Epoch 2120, training loss: 62.15503692626953 = 0.019414717331528664 + 10.0 * 6.213562488555908
Epoch 2120, val loss: 1.2130807638168335
Epoch 2130, training loss: 62.1461296081543 = 0.019131189212203026 + 10.0 * 6.212699890136719
Epoch 2130, val loss: 1.2161920070648193
Epoch 2140, training loss: 62.198726654052734 = 0.018858956173062325 + 10.0 * 6.217986583709717
Epoch 2140, val loss: 1.2191015481948853
Epoch 2150, training loss: 62.15913772583008 = 0.018578941002488136 + 10.0 * 6.214056015014648
Epoch 2150, val loss: 1.2221804857254028
Epoch 2160, training loss: 62.1475944519043 = 0.018313037231564522 + 10.0 * 6.21292781829834
Epoch 2160, val loss: 1.2250319719314575
Epoch 2170, training loss: 62.1522216796875 = 0.018050724640488625 + 10.0 * 6.213417053222656
Epoch 2170, val loss: 1.2280999422073364
Epoch 2180, training loss: 62.14741134643555 = 0.017799170687794685 + 10.0 * 6.212961196899414
Epoch 2180, val loss: 1.23091721534729
Epoch 2190, training loss: 62.13934326171875 = 0.017551537603139877 + 10.0 * 6.212179183959961
Epoch 2190, val loss: 1.2339833974838257
Epoch 2200, training loss: 62.15715026855469 = 0.017309892922639847 + 10.0 * 6.21398401260376
Epoch 2200, val loss: 1.2368019819259644
Epoch 2210, training loss: 62.1591911315918 = 0.017067724838852882 + 10.0 * 6.214212417602539
Epoch 2210, val loss: 1.2396490573883057
Epoch 2220, training loss: 62.15081787109375 = 0.016833927482366562 + 10.0 * 6.213398456573486
Epoch 2220, val loss: 1.242520809173584
Epoch 2230, training loss: 62.13213348388672 = 0.01660284213721752 + 10.0 * 6.21155309677124
Epoch 2230, val loss: 1.2454330921173096
Epoch 2240, training loss: 62.13275146484375 = 0.0163789801299572 + 10.0 * 6.211637020111084
Epoch 2240, val loss: 1.2482677698135376
Epoch 2250, training loss: 62.14629364013672 = 0.01615995354950428 + 10.0 * 6.213013648986816
Epoch 2250, val loss: 1.2509433031082153
Epoch 2260, training loss: 62.143211364746094 = 0.01593800075352192 + 10.0 * 6.2127275466918945
Epoch 2260, val loss: 1.2535682916641235
Epoch 2270, training loss: 62.122196197509766 = 0.015728384256362915 + 10.0 * 6.210646629333496
Epoch 2270, val loss: 1.2563921213150024
Epoch 2280, training loss: 62.1185417175293 = 0.015521063469350338 + 10.0 * 6.210301876068115
Epoch 2280, val loss: 1.2591577768325806
Epoch 2290, training loss: 62.187198638916016 = 0.015318715013563633 + 10.0 * 6.217187881469727
Epoch 2290, val loss: 1.261757731437683
Epoch 2300, training loss: 62.145294189453125 = 0.015120597556233406 + 10.0 * 6.213017463684082
Epoch 2300, val loss: 1.2644520998001099
Epoch 2310, training loss: 62.114810943603516 = 0.01491868682205677 + 10.0 * 6.209989070892334
Epoch 2310, val loss: 1.267133116722107
Epoch 2320, training loss: 62.10627365112305 = 0.014734349213540554 + 10.0 * 6.209153652191162
Epoch 2320, val loss: 1.2698237895965576
Epoch 2330, training loss: 62.11175537109375 = 0.014549126848578453 + 10.0 * 6.209720611572266
Epoch 2330, val loss: 1.2723666429519653
Epoch 2340, training loss: 62.15208053588867 = 0.014367428608238697 + 10.0 * 6.213771343231201
Epoch 2340, val loss: 1.274869441986084
Epoch 2350, training loss: 62.13628005981445 = 0.014182180166244507 + 10.0 * 6.212209701538086
Epoch 2350, val loss: 1.2777099609375
Epoch 2360, training loss: 62.10846710205078 = 0.014002195559442043 + 10.0 * 6.209446430206299
Epoch 2360, val loss: 1.2801045179367065
Epoch 2370, training loss: 62.10286331176758 = 0.013829815201461315 + 10.0 * 6.2089033126831055
Epoch 2370, val loss: 1.2827914953231812
Epoch 2380, training loss: 62.117794036865234 = 0.013662954792380333 + 10.0 * 6.210412979125977
Epoch 2380, val loss: 1.2853705883026123
Epoch 2390, training loss: 62.097320556640625 = 0.013494794256985188 + 10.0 * 6.208382606506348
Epoch 2390, val loss: 1.2878634929656982
Epoch 2400, training loss: 62.145179748535156 = 0.013333513401448727 + 10.0 * 6.213184833526611
Epoch 2400, val loss: 1.2903227806091309
Epoch 2410, training loss: 62.10173034667969 = 0.013170125894248486 + 10.0 * 6.208856105804443
Epoch 2410, val loss: 1.2928193807601929
Epoch 2420, training loss: 62.11673355102539 = 0.013010657392442226 + 10.0 * 6.210371971130371
Epoch 2420, val loss: 1.295210361480713
Epoch 2430, training loss: 62.095977783203125 = 0.012854674831032753 + 10.0 * 6.208312034606934
Epoch 2430, val loss: 1.297674536705017
Epoch 2440, training loss: 62.089447021484375 = 0.012703406624495983 + 10.0 * 6.207674503326416
Epoch 2440, val loss: 1.300220012664795
Epoch 2450, training loss: 62.080142974853516 = 0.012556277215480804 + 10.0 * 6.206758499145508
Epoch 2450, val loss: 1.3026940822601318
Epoch 2460, training loss: 62.09135055541992 = 0.01241332944482565 + 10.0 * 6.2078938484191895
Epoch 2460, val loss: 1.3051899671554565
Epoch 2470, training loss: 62.13557815551758 = 0.012271350249648094 + 10.0 * 6.2123308181762695
Epoch 2470, val loss: 1.3073674440383911
Epoch 2480, training loss: 62.104984283447266 = 0.012126347981393337 + 10.0 * 6.209285736083984
Epoch 2480, val loss: 1.3096520900726318
Epoch 2490, training loss: 62.08490753173828 = 0.011984109878540039 + 10.0 * 6.207292079925537
Epoch 2490, val loss: 1.3120712041854858
Epoch 2500, training loss: 62.089088439941406 = 0.011851942166686058 + 10.0 * 6.207723617553711
Epoch 2500, val loss: 1.3144361972808838
Epoch 2510, training loss: 62.10193634033203 = 0.011720642447471619 + 10.0 * 6.20902156829834
Epoch 2510, val loss: 1.3168529272079468
Epoch 2520, training loss: 62.0963134765625 = 0.011589065194129944 + 10.0 * 6.20847225189209
Epoch 2520, val loss: 1.3190727233886719
Epoch 2530, training loss: 62.08268737792969 = 0.011456125415861607 + 10.0 * 6.207123279571533
Epoch 2530, val loss: 1.3212862014770508
Epoch 2540, training loss: 62.072105407714844 = 0.011329524219036102 + 10.0 * 6.206077575683594
Epoch 2540, val loss: 1.3235344886779785
Epoch 2550, training loss: 62.07272720336914 = 0.011206933297216892 + 10.0 * 6.206151962280273
Epoch 2550, val loss: 1.3258695602416992
Epoch 2560, training loss: 62.106021881103516 = 0.011084736324846745 + 10.0 * 6.209493637084961
Epoch 2560, val loss: 1.327978253364563
Epoch 2570, training loss: 62.1213264465332 = 0.010964170098304749 + 10.0 * 6.211036205291748
Epoch 2570, val loss: 1.3301218748092651
Epoch 2580, training loss: 62.074615478515625 = 0.010839615948498249 + 10.0 * 6.2063775062561035
Epoch 2580, val loss: 1.3322577476501465
Epoch 2590, training loss: 62.06111526489258 = 0.010724605061113834 + 10.0 * 6.205039024353027
Epoch 2590, val loss: 1.3345478773117065
Epoch 2600, training loss: 62.05758285522461 = 0.010612317360937595 + 10.0 * 6.204697132110596
Epoch 2600, val loss: 1.3367877006530762
Epoch 2610, training loss: 62.07742691040039 = 0.010504534468054771 + 10.0 * 6.206692218780518
Epoch 2610, val loss: 1.338881254196167
Epoch 2620, training loss: 62.07072830200195 = 0.010393084026873112 + 10.0 * 6.206033706665039
Epoch 2620, val loss: 1.3409159183502197
Epoch 2630, training loss: 62.06377029418945 = 0.010283461771905422 + 10.0 * 6.205348491668701
Epoch 2630, val loss: 1.3431720733642578
Epoch 2640, training loss: 62.06040954589844 = 0.010177571326494217 + 10.0 * 6.205023288726807
Epoch 2640, val loss: 1.3452398777008057
Epoch 2650, training loss: 62.10811996459961 = 0.010072297416627407 + 10.0 * 6.209805011749268
Epoch 2650, val loss: 1.347247838973999
Epoch 2660, training loss: 62.094547271728516 = 0.009969104081392288 + 10.0 * 6.208457946777344
Epoch 2660, val loss: 1.3493183851242065
Epoch 2670, training loss: 62.05747985839844 = 0.009861142374575138 + 10.0 * 6.204761981964111
Epoch 2670, val loss: 1.3514052629470825
Epoch 2680, training loss: 62.05405807495117 = 0.009763197042047977 + 10.0 * 6.204429626464844
Epoch 2680, val loss: 1.353581190109253
Epoch 2690, training loss: 62.097877502441406 = 0.00966564565896988 + 10.0 * 6.2088212966918945
Epoch 2690, val loss: 1.3554483652114868
Epoch 2700, training loss: 62.0474967956543 = 0.009569580666720867 + 10.0 * 6.203792572021484
Epoch 2700, val loss: 1.3575780391693115
Epoch 2710, training loss: 62.04991912841797 = 0.009475386701524258 + 10.0 * 6.204044342041016
Epoch 2710, val loss: 1.3596287965774536
Epoch 2720, training loss: 62.090110778808594 = 0.009383397176861763 + 10.0 * 6.208072662353516
Epoch 2720, val loss: 1.3614829778671265
Epoch 2730, training loss: 62.0472526550293 = 0.009286690503358841 + 10.0 * 6.203796863555908
Epoch 2730, val loss: 1.3635426759719849
Epoch 2740, training loss: 62.04012680053711 = 0.009195749647915363 + 10.0 * 6.2030930519104
Epoch 2740, val loss: 1.3655139207839966
Epoch 2750, training loss: 62.03932189941406 = 0.009110298939049244 + 10.0 * 6.203021049499512
Epoch 2750, val loss: 1.3675199747085571
Epoch 2760, training loss: 62.10588073730469 = 0.009026715531945229 + 10.0 * 6.209685325622559
Epoch 2760, val loss: 1.3693499565124512
Epoch 2770, training loss: 62.06494903564453 = 0.008932974189519882 + 10.0 * 6.205601692199707
Epoch 2770, val loss: 1.371373176574707
Epoch 2780, training loss: 62.056495666503906 = 0.008848791942000389 + 10.0 * 6.204764366149902
Epoch 2780, val loss: 1.3732049465179443
Epoch 2790, training loss: 62.05139923095703 = 0.008762022480368614 + 10.0 * 6.204263687133789
Epoch 2790, val loss: 1.3750935792922974
Epoch 2800, training loss: 62.04827880859375 = 0.008679516613483429 + 10.0 * 6.203959941864014
Epoch 2800, val loss: 1.3770073652267456
Epoch 2810, training loss: 62.0521240234375 = 0.008597652427852154 + 10.0 * 6.204352378845215
Epoch 2810, val loss: 1.378847360610962
Epoch 2820, training loss: 62.05666732788086 = 0.008519358932971954 + 10.0 * 6.204814910888672
Epoch 2820, val loss: 1.380746603012085
Epoch 2830, training loss: 62.04193115234375 = 0.00843785610049963 + 10.0 * 6.2033491134643555
Epoch 2830, val loss: 1.3826022148132324
Epoch 2840, training loss: 62.02438735961914 = 0.008359524421393871 + 10.0 * 6.201602935791016
Epoch 2840, val loss: 1.3843413591384888
Epoch 2850, training loss: 62.02438735961914 = 0.008284960873425007 + 10.0 * 6.201610088348389
Epoch 2850, val loss: 1.3862065076828003
Epoch 2860, training loss: 62.0260124206543 = 0.008211620151996613 + 10.0 * 6.201779842376709
Epoch 2860, val loss: 1.3880486488342285
Epoch 2870, training loss: 62.0711784362793 = 0.008142788894474506 + 10.0 * 6.206303596496582
Epoch 2870, val loss: 1.389945387840271
Epoch 2880, training loss: 62.03817367553711 = 0.008064140565693378 + 10.0 * 6.2030110359191895
Epoch 2880, val loss: 1.3914928436279297
Epoch 2890, training loss: 62.02495574951172 = 0.00798961240798235 + 10.0 * 6.201696872711182
Epoch 2890, val loss: 1.3933416604995728
Epoch 2900, training loss: 62.028045654296875 = 0.007918464951217175 + 10.0 * 6.202012538909912
Epoch 2900, val loss: 1.3950140476226807
Epoch 2910, training loss: 62.074195861816406 = 0.007849306799471378 + 10.0 * 6.206634521484375
Epoch 2910, val loss: 1.3967921733856201
Epoch 2920, training loss: 62.039920806884766 = 0.007779423613101244 + 10.0 * 6.203214168548584
Epoch 2920, val loss: 1.3983508348464966
Epoch 2930, training loss: 62.02092361450195 = 0.007708709686994553 + 10.0 * 6.201321601867676
Epoch 2930, val loss: 1.4001058340072632
Epoch 2940, training loss: 62.01396560668945 = 0.007644927594810724 + 10.0 * 6.200632095336914
Epoch 2940, val loss: 1.4019533395767212
Epoch 2950, training loss: 62.01541519165039 = 0.007581036537885666 + 10.0 * 6.200783729553223
Epoch 2950, val loss: 1.4037137031555176
Epoch 2960, training loss: 62.07300567626953 = 0.007517740596085787 + 10.0 * 6.206548690795898
Epoch 2960, val loss: 1.4052993059158325
Epoch 2970, training loss: 62.04877853393555 = 0.007451650686562061 + 10.0 * 6.204133033752441
Epoch 2970, val loss: 1.4066303968429565
Epoch 2980, training loss: 62.021583557128906 = 0.007385826203972101 + 10.0 * 6.201419830322266
Epoch 2980, val loss: 1.4085216522216797
Epoch 2990, training loss: 62.0240592956543 = 0.007324362639337778 + 10.0 * 6.20167350769043
Epoch 2990, val loss: 1.4100362062454224
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8444913020558777
=== training gcn model ===
Epoch 0, training loss: 87.91732788085938 = 1.949129343032837 + 10.0 * 8.596819877624512
Epoch 0, val loss: 1.9498525857925415
Epoch 10, training loss: 87.90013885498047 = 1.939102053642273 + 10.0 * 8.59610366821289
Epoch 10, val loss: 1.9398084878921509
Epoch 20, training loss: 87.83634948730469 = 1.927160382270813 + 10.0 * 8.59091854095459
Epoch 20, val loss: 1.9278360605239868
Epoch 30, training loss: 87.43439483642578 = 1.9119759798049927 + 10.0 * 8.552241325378418
Epoch 30, val loss: 1.9128612279891968
Epoch 40, training loss: 84.5442123413086 = 1.89426589012146 + 10.0 * 8.264994621276855
Epoch 40, val loss: 1.8958522081375122
Epoch 50, training loss: 77.1866683959961 = 1.874028205871582 + 10.0 * 7.53126335144043
Epoch 50, val loss: 1.8763278722763062
Epoch 60, training loss: 75.77445983886719 = 1.856636881828308 + 10.0 * 7.391782283782959
Epoch 60, val loss: 1.8603570461273193
Epoch 70, training loss: 73.32945251464844 = 1.8451595306396484 + 10.0 * 7.148428916931152
Epoch 70, val loss: 1.8490288257598877
Epoch 80, training loss: 71.4972915649414 = 1.8332267999649048 + 10.0 * 6.966405868530273
Epoch 80, val loss: 1.8372536897659302
Epoch 90, training loss: 70.56600952148438 = 1.8231761455535889 + 10.0 * 6.874283790588379
Epoch 90, val loss: 1.8269470930099487
Epoch 100, training loss: 69.61487579345703 = 1.8125213384628296 + 10.0 * 6.780235290527344
Epoch 100, val loss: 1.8166552782058716
Epoch 110, training loss: 68.73082733154297 = 1.8050585985183716 + 10.0 * 6.692576885223389
Epoch 110, val loss: 1.8091771602630615
Epoch 120, training loss: 68.13821411132812 = 1.79787278175354 + 10.0 * 6.634034156799316
Epoch 120, val loss: 1.801571011543274
Epoch 130, training loss: 67.69644165039062 = 1.7891401052474976 + 10.0 * 6.590730667114258
Epoch 130, val loss: 1.7925556898117065
Epoch 140, training loss: 67.38084411621094 = 1.7800782918930054 + 10.0 * 6.560076713562012
Epoch 140, val loss: 1.7834436893463135
Epoch 150, training loss: 67.06082153320312 = 1.7710239887237549 + 10.0 * 6.528979301452637
Epoch 150, val loss: 1.7744057178497314
Epoch 160, training loss: 66.81669616699219 = 1.7617135047912598 + 10.0 * 6.50549840927124
Epoch 160, val loss: 1.7651041746139526
Epoch 170, training loss: 66.64532470703125 = 1.7519181966781616 + 10.0 * 6.4893412590026855
Epoch 170, val loss: 1.7554922103881836
Epoch 180, training loss: 66.4427261352539 = 1.7410476207733154 + 10.0 * 6.47016716003418
Epoch 180, val loss: 1.7452079057693481
Epoch 190, training loss: 66.2901840209961 = 1.7290571928024292 + 10.0 * 6.456112384796143
Epoch 190, val loss: 1.7340734004974365
Epoch 200, training loss: 66.17912292480469 = 1.7158350944519043 + 10.0 * 6.446329116821289
Epoch 200, val loss: 1.7219802141189575
Epoch 210, training loss: 66.0470199584961 = 1.7012591361999512 + 10.0 * 6.434576034545898
Epoch 210, val loss: 1.7087810039520264
Epoch 220, training loss: 65.93557739257812 = 1.6854424476623535 + 10.0 * 6.425013542175293
Epoch 220, val loss: 1.6945605278015137
Epoch 230, training loss: 65.87190246582031 = 1.668097972869873 + 10.0 * 6.420380592346191
Epoch 230, val loss: 1.679170846939087
Epoch 240, training loss: 65.75096130371094 = 1.6491827964782715 + 10.0 * 6.410177707672119
Epoch 240, val loss: 1.6624010801315308
Epoch 250, training loss: 65.6393051147461 = 1.6288200616836548 + 10.0 * 6.401048183441162
Epoch 250, val loss: 1.6445814371109009
Epoch 260, training loss: 65.56048583984375 = 1.607022762298584 + 10.0 * 6.395346164703369
Epoch 260, val loss: 1.625643014907837
Epoch 270, training loss: 65.47722625732422 = 1.58369779586792 + 10.0 * 6.389353275299072
Epoch 270, val loss: 1.6055251359939575
Epoch 280, training loss: 65.37602996826172 = 1.559200406074524 + 10.0 * 6.381682872772217
Epoch 280, val loss: 1.584629774093628
Epoch 290, training loss: 65.29461669921875 = 1.5335493087768555 + 10.0 * 6.376107215881348
Epoch 290, val loss: 1.5630440711975098
Epoch 300, training loss: 65.22212982177734 = 1.5068700313568115 + 10.0 * 6.37152624130249
Epoch 300, val loss: 1.5409010648727417
Epoch 310, training loss: 65.15983581542969 = 1.4794377088546753 + 10.0 * 6.368040084838867
Epoch 310, val loss: 1.51842200756073
Epoch 320, training loss: 65.06820678710938 = 1.4513617753982544 + 10.0 * 6.361684799194336
Epoch 320, val loss: 1.495761513710022
Epoch 330, training loss: 65.00105285644531 = 1.4229826927185059 + 10.0 * 6.35780668258667
Epoch 330, val loss: 1.4732069969177246
Epoch 340, training loss: 64.9200439453125 = 1.3942668437957764 + 10.0 * 6.3525776863098145
Epoch 340, val loss: 1.4506841897964478
Epoch 350, training loss: 64.85206604003906 = 1.3656469583511353 + 10.0 * 6.348641872406006
Epoch 350, val loss: 1.4286271333694458
Epoch 360, training loss: 64.78475189208984 = 1.3369088172912598 + 10.0 * 6.344784259796143
Epoch 360, val loss: 1.4066308736801147
Epoch 370, training loss: 64.75113677978516 = 1.3081560134887695 + 10.0 * 6.344297885894775
Epoch 370, val loss: 1.3849114179611206
Epoch 380, training loss: 64.66241455078125 = 1.2794195413589478 + 10.0 * 6.33829927444458
Epoch 380, val loss: 1.3635140657424927
Epoch 390, training loss: 64.59403228759766 = 1.2509804964065552 + 10.0 * 6.334305286407471
Epoch 390, val loss: 1.3424913883209229
Epoch 400, training loss: 64.53765869140625 = 1.2226307392120361 + 10.0 * 6.331502437591553
Epoch 400, val loss: 1.3217958211898804
Epoch 410, training loss: 64.49552917480469 = 1.1941841840744019 + 10.0 * 6.330134391784668
Epoch 410, val loss: 1.3012912273406982
Epoch 420, training loss: 64.4338150024414 = 1.1660817861557007 + 10.0 * 6.326773166656494
Epoch 420, val loss: 1.2810752391815186
Epoch 430, training loss: 64.36601257324219 = 1.1379468441009521 + 10.0 * 6.322806358337402
Epoch 430, val loss: 1.2610727548599243
Epoch 440, training loss: 64.312744140625 = 1.1099815368652344 + 10.0 * 6.320276260375977
Epoch 440, val loss: 1.24134361743927
Epoch 450, training loss: 64.3306655883789 = 1.0820728540420532 + 10.0 * 6.324859142303467
Epoch 450, val loss: 1.221683382987976
Epoch 460, training loss: 64.25057983398438 = 1.0544191598892212 + 10.0 * 6.319615840911865
Epoch 460, val loss: 1.2024677991867065
Epoch 470, training loss: 64.16859436035156 = 1.0271087884902954 + 10.0 * 6.314148426055908
Epoch 470, val loss: 1.1837267875671387
Epoch 480, training loss: 64.11064147949219 = 1.0002728700637817 + 10.0 * 6.311037063598633
Epoch 480, val loss: 1.1655305624008179
Epoch 490, training loss: 64.06657409667969 = 0.9737710356712341 + 10.0 * 6.3092803955078125
Epoch 490, val loss: 1.147855520248413
Epoch 500, training loss: 64.03306579589844 = 0.947629451751709 + 10.0 * 6.308543682098389
Epoch 500, val loss: 1.130476951599121
Epoch 510, training loss: 63.976585388183594 = 0.9218523502349854 + 10.0 * 6.305473327636719
Epoch 510, val loss: 1.113623857498169
Epoch 520, training loss: 63.93179702758789 = 0.8966789245605469 + 10.0 * 6.303511619567871
Epoch 520, val loss: 1.097400188446045
Epoch 530, training loss: 63.89059066772461 = 0.8720776438713074 + 10.0 * 6.301851272583008
Epoch 530, val loss: 1.0818326473236084
Epoch 540, training loss: 63.84872817993164 = 0.8480199575424194 + 10.0 * 6.300070762634277
Epoch 540, val loss: 1.066709280014038
Epoch 550, training loss: 63.82574462890625 = 0.8245710730552673 + 10.0 * 6.300117492675781
Epoch 550, val loss: 1.0522258281707764
Epoch 560, training loss: 63.77724838256836 = 0.801628828048706 + 10.0 * 6.297562122344971
Epoch 560, val loss: 1.038630723953247
Epoch 570, training loss: 63.738243103027344 = 0.7794058322906494 + 10.0 * 6.295883655548096
Epoch 570, val loss: 1.0253851413726807
Epoch 580, training loss: 63.71026611328125 = 0.7576707601547241 + 10.0 * 6.295259475708008
Epoch 580, val loss: 1.0128400325775146
Epoch 590, training loss: 63.66197967529297 = 0.736622154712677 + 10.0 * 6.292535781860352
Epoch 590, val loss: 1.0009715557098389
Epoch 600, training loss: 63.61640548706055 = 0.7162210941314697 + 10.0 * 6.290018558502197
Epoch 600, val loss: 0.9897553324699402
Epoch 610, training loss: 63.57655715942383 = 0.6964226365089417 + 10.0 * 6.288013458251953
Epoch 610, val loss: 0.9792306423187256
Epoch 620, training loss: 63.6336669921875 = 0.6771847605705261 + 10.0 * 6.295648097991943
Epoch 620, val loss: 0.9691905975341797
Epoch 630, training loss: 63.51998519897461 = 0.6582843065261841 + 10.0 * 6.28617000579834
Epoch 630, val loss: 0.9597024321556091
Epoch 640, training loss: 63.4772834777832 = 0.6400713324546814 + 10.0 * 6.283720970153809
Epoch 640, val loss: 0.9508706331253052
Epoch 650, training loss: 63.4495964050293 = 0.6223628520965576 + 10.0 * 6.282723426818848
Epoch 650, val loss: 0.9426880478858948
Epoch 660, training loss: 63.476200103759766 = 0.6051214337348938 + 10.0 * 6.287107944488525
Epoch 660, val loss: 0.9349377751350403
Epoch 670, training loss: 63.414669036865234 = 0.5881862044334412 + 10.0 * 6.28264856338501
Epoch 670, val loss: 0.9276899099349976
Epoch 680, training loss: 63.368797302246094 = 0.5718030333518982 + 10.0 * 6.279699325561523
Epoch 680, val loss: 0.9209051132202148
Epoch 690, training loss: 63.3299674987793 = 0.5558831691741943 + 10.0 * 6.277408599853516
Epoch 690, val loss: 0.9146602749824524
Epoch 700, training loss: 63.314971923828125 = 0.5403491258621216 + 10.0 * 6.277462482452393
Epoch 700, val loss: 0.9089659452438354
Epoch 710, training loss: 63.308650970458984 = 0.5250816941261292 + 10.0 * 6.278357028961182
Epoch 710, val loss: 0.9034574031829834
Epoch 720, training loss: 63.2593994140625 = 0.5103180408477783 + 10.0 * 6.274908065795898
Epoch 720, val loss: 0.8983820676803589
Epoch 730, training loss: 63.23006057739258 = 0.49599355459213257 + 10.0 * 6.273406505584717
Epoch 730, val loss: 0.893911600112915
Epoch 740, training loss: 63.223793029785156 = 0.4820495843887329 + 10.0 * 6.274174690246582
Epoch 740, val loss: 0.8899000883102417
Epoch 750, training loss: 63.17899703979492 = 0.46846845746040344 + 10.0 * 6.271052837371826
Epoch 750, val loss: 0.8862020969390869
Epoch 760, training loss: 63.15623474121094 = 0.45519620180130005 + 10.0 * 6.270103931427002
Epoch 760, val loss: 0.8829907178878784
Epoch 770, training loss: 63.13633728027344 = 0.4423144459724426 + 10.0 * 6.269402503967285
Epoch 770, val loss: 0.8801343441009521
Epoch 780, training loss: 63.15637969970703 = 0.4297129213809967 + 10.0 * 6.2726664543151855
Epoch 780, val loss: 0.877659022808075
Epoch 790, training loss: 63.091712951660156 = 0.41741976141929626 + 10.0 * 6.267429351806641
Epoch 790, val loss: 0.8756712675094604
Epoch 800, training loss: 63.06785583496094 = 0.40550947189331055 + 10.0 * 6.266234397888184
Epoch 800, val loss: 0.8740500211715698
Epoch 810, training loss: 63.06047821044922 = 0.3938872516155243 + 10.0 * 6.266659259796143
Epoch 810, val loss: 0.8728410005569458
Epoch 820, training loss: 63.051395416259766 = 0.38251209259033203 + 10.0 * 6.266888618469238
Epoch 820, val loss: 0.8718064427375793
Epoch 830, training loss: 63.011600494384766 = 0.3715011477470398 + 10.0 * 6.264009952545166
Epoch 830, val loss: 0.8711799383163452
Epoch 840, training loss: 62.99518966674805 = 0.36085107922554016 + 10.0 * 6.263433933258057
Epoch 840, val loss: 0.8710163235664368
Epoch 850, training loss: 62.97262191772461 = 0.3505465090274811 + 10.0 * 6.262207508087158
Epoch 850, val loss: 0.8711725473403931
Epoch 860, training loss: 63.019657135009766 = 0.34055760502815247 + 10.0 * 6.267910003662109
Epoch 860, val loss: 0.8715718388557434
Epoch 870, training loss: 62.961875915527344 = 0.3306550979614258 + 10.0 * 6.263122081756592
Epoch 870, val loss: 0.87230384349823
Epoch 880, training loss: 62.924922943115234 = 0.32120245695114136 + 10.0 * 6.260372161865234
Epoch 880, val loss: 0.8732813000679016
Epoch 890, training loss: 62.91805648803711 = 0.31199318170547485 + 10.0 * 6.260606288909912
Epoch 890, val loss: 0.8746257424354553
Epoch 900, training loss: 62.88983154296875 = 0.3030661940574646 + 10.0 * 6.258676528930664
Epoch 900, val loss: 0.8761628866195679
Epoch 910, training loss: 62.90887451171875 = 0.29437434673309326 + 10.0 * 6.261450290679932
Epoch 910, val loss: 0.8780011534690857
Epoch 920, training loss: 62.8671875 = 0.28595054149627686 + 10.0 * 6.258123874664307
Epoch 920, val loss: 0.8799786567687988
Epoch 930, training loss: 62.848690032958984 = 0.2778145968914032 + 10.0 * 6.257087707519531
Epoch 930, val loss: 0.8822565674781799
Epoch 940, training loss: 62.83893966674805 = 0.269931823015213 + 10.0 * 6.256900787353516
Epoch 940, val loss: 0.8848724365234375
Epoch 950, training loss: 62.81747817993164 = 0.26230403780937195 + 10.0 * 6.255517482757568
Epoch 950, val loss: 0.8876360654830933
Epoch 960, training loss: 62.80753707885742 = 0.25493311882019043 + 10.0 * 6.255260467529297
Epoch 960, val loss: 0.8905673027038574
Epoch 970, training loss: 62.78931427001953 = 0.24775683879852295 + 10.0 * 6.25415563583374
Epoch 970, val loss: 0.893746018409729
Epoch 980, training loss: 62.797176361083984 = 0.24081942439079285 + 10.0 * 6.255635738372803
Epoch 980, val loss: 0.8970218896865845
Epoch 990, training loss: 62.77291488647461 = 0.23405896127223969 + 10.0 * 6.253885746002197
Epoch 990, val loss: 0.9005327820777893
Epoch 1000, training loss: 62.753849029541016 = 0.22750864923000336 + 10.0 * 6.252634048461914
Epoch 1000, val loss: 0.9041755795478821
Epoch 1010, training loss: 62.737911224365234 = 0.22120694816112518 + 10.0 * 6.2516703605651855
Epoch 1010, val loss: 0.9080810546875
Epoch 1020, training loss: 62.77320861816406 = 0.21511271595954895 + 10.0 * 6.255809307098389
Epoch 1020, val loss: 0.9120532870292664
Epoch 1030, training loss: 62.724002838134766 = 0.20911680161952972 + 10.0 * 6.25148868560791
Epoch 1030, val loss: 0.9161648750305176
Epoch 1040, training loss: 62.69835662841797 = 0.2033933848142624 + 10.0 * 6.2494964599609375
Epoch 1040, val loss: 0.9204741716384888
Epoch 1050, training loss: 62.687294006347656 = 0.19782769680023193 + 10.0 * 6.248946666717529
Epoch 1050, val loss: 0.9250645041465759
Epoch 1060, training loss: 62.724143981933594 = 0.19242635369300842 + 10.0 * 6.253171920776367
Epoch 1060, val loss: 0.929638683795929
Epoch 1070, training loss: 62.67323303222656 = 0.1871979981660843 + 10.0 * 6.248603343963623
Epoch 1070, val loss: 0.9342367053031921
Epoch 1080, training loss: 62.650150299072266 = 0.18211810290813446 + 10.0 * 6.246803283691406
Epoch 1080, val loss: 0.9391731023788452
Epoch 1090, training loss: 62.662841796875 = 0.1772228479385376 + 10.0 * 6.248561859130859
Epoch 1090, val loss: 0.9442188143730164
Epoch 1100, training loss: 62.6471061706543 = 0.1724199801683426 + 10.0 * 6.2474684715271
Epoch 1100, val loss: 0.9492908716201782
Epoch 1110, training loss: 62.62247848510742 = 0.16780076920986176 + 10.0 * 6.245467662811279
Epoch 1110, val loss: 0.9544290900230408
Epoch 1120, training loss: 62.609188079833984 = 0.163334459066391 + 10.0 * 6.2445855140686035
Epoch 1120, val loss: 0.9599069356918335
Epoch 1130, training loss: 62.60795593261719 = 0.15900327265262604 + 10.0 * 6.244894981384277
Epoch 1130, val loss: 0.9654527306556702
Epoch 1140, training loss: 62.62812042236328 = 0.15478311479091644 + 10.0 * 6.247334003448486
Epoch 1140, val loss: 0.9709994792938232
Epoch 1150, training loss: 62.59877014160156 = 0.15062016248703003 + 10.0 * 6.244814872741699
Epoch 1150, val loss: 0.9766737222671509
Epoch 1160, training loss: 62.57157897949219 = 0.14663267135620117 + 10.0 * 6.242494583129883
Epoch 1160, val loss: 0.982500433921814
Epoch 1170, training loss: 62.56364440917969 = 0.14276418089866638 + 10.0 * 6.2420878410339355
Epoch 1170, val loss: 0.9884859323501587
Epoch 1180, training loss: 62.600833892822266 = 0.1390053778886795 + 10.0 * 6.246182918548584
Epoch 1180, val loss: 0.99455326795578
Epoch 1190, training loss: 62.57364273071289 = 0.13535740971565247 + 10.0 * 6.243828773498535
Epoch 1190, val loss: 1.0005701780319214
Epoch 1200, training loss: 62.543983459472656 = 0.13180124759674072 + 10.0 * 6.241218090057373
Epoch 1200, val loss: 1.006726861000061
Epoch 1210, training loss: 62.52793502807617 = 0.12837596237659454 + 10.0 * 6.239955902099609
Epoch 1210, val loss: 1.0130460262298584
Epoch 1220, training loss: 62.52783966064453 = 0.1250554919242859 + 10.0 * 6.240278244018555
Epoch 1220, val loss: 1.0193928480148315
Epoch 1230, training loss: 62.543914794921875 = 0.12180999666452408 + 10.0 * 6.242210388183594
Epoch 1230, val loss: 1.025673508644104
Epoch 1240, training loss: 62.53902053833008 = 0.11860255897045135 + 10.0 * 6.24204158782959
Epoch 1240, val loss: 1.032102346420288
Epoch 1250, training loss: 62.52062225341797 = 0.11555557698011398 + 10.0 * 6.240506649017334
Epoch 1250, val loss: 1.038405179977417
Epoch 1260, training loss: 62.49187088012695 = 0.11256073415279388 + 10.0 * 6.237931251525879
Epoch 1260, val loss: 1.0449339151382446
Epoch 1270, training loss: 62.477420806884766 = 0.10969860851764679 + 10.0 * 6.236772060394287
Epoch 1270, val loss: 1.0515806674957275
Epoch 1280, training loss: 62.47128677368164 = 0.10690801590681076 + 10.0 * 6.236437797546387
Epoch 1280, val loss: 1.058241844177246
Epoch 1290, training loss: 62.523292541503906 = 0.10420149564743042 + 10.0 * 6.241909027099609
Epoch 1290, val loss: 1.064762830734253
Epoch 1300, training loss: 62.5332145690918 = 0.10154020041227341 + 10.0 * 6.243167400360107
Epoch 1300, val loss: 1.0712101459503174
Epoch 1310, training loss: 62.471580505371094 = 0.09893812984228134 + 10.0 * 6.237264156341553
Epoch 1310, val loss: 1.0777711868286133
Epoch 1320, training loss: 62.45029067993164 = 0.09644829481840134 + 10.0 * 6.235384464263916
Epoch 1320, val loss: 1.0845308303833008
Epoch 1330, training loss: 62.43647766113281 = 0.0940571278333664 + 10.0 * 6.234241962432861
Epoch 1330, val loss: 1.0913139581680298
Epoch 1340, training loss: 62.42881393432617 = 0.09173734486103058 + 10.0 * 6.233707904815674
Epoch 1340, val loss: 1.0981366634368896
Epoch 1350, training loss: 62.534671783447266 = 0.0894889160990715 + 10.0 * 6.244518280029297
Epoch 1350, val loss: 1.1047691106796265
Epoch 1360, training loss: 62.45109558105469 = 0.08726031333208084 + 10.0 * 6.236383438110352
Epoch 1360, val loss: 1.1114617586135864
Epoch 1370, training loss: 62.4090461730957 = 0.08509241044521332 + 10.0 * 6.232395648956299
Epoch 1370, val loss: 1.1182206869125366
Epoch 1380, training loss: 62.414825439453125 = 0.0830332338809967 + 10.0 * 6.233179092407227
Epoch 1380, val loss: 1.1250321865081787
Epoch 1390, training loss: 62.45353698730469 = 0.08104097843170166 + 10.0 * 6.237249851226807
Epoch 1390, val loss: 1.1317365169525146
Epoch 1400, training loss: 62.41347885131836 = 0.0790567696094513 + 10.0 * 6.233442306518555
Epoch 1400, val loss: 1.1385161876678467
Epoch 1410, training loss: 62.401512145996094 = 0.07717857509851456 + 10.0 * 6.232433319091797
Epoch 1410, val loss: 1.145352840423584
Epoch 1420, training loss: 62.42011642456055 = 0.07533913850784302 + 10.0 * 6.234477519989014
Epoch 1420, val loss: 1.152134656906128
Epoch 1430, training loss: 62.379661560058594 = 0.073561891913414 + 10.0 * 6.230609893798828
Epoch 1430, val loss: 1.1587368249893188
Epoch 1440, training loss: 62.38398361206055 = 0.07183723896741867 + 10.0 * 6.23121452331543
Epoch 1440, val loss: 1.1656659841537476
Epoch 1450, training loss: 62.404541015625 = 0.07015932351350784 + 10.0 * 6.233438014984131
Epoch 1450, val loss: 1.172261118888855
Epoch 1460, training loss: 62.373661041259766 = 0.06853435933589935 + 10.0 * 6.230512619018555
Epoch 1460, val loss: 1.1789708137512207
Epoch 1470, training loss: 62.36296081542969 = 0.06695732474327087 + 10.0 * 6.229600429534912
Epoch 1470, val loss: 1.185806155204773
Epoch 1480, training loss: 62.357444763183594 = 0.06543619930744171 + 10.0 * 6.229200839996338
Epoch 1480, val loss: 1.1925946474075317
Epoch 1490, training loss: 62.36648178100586 = 0.06396014243364334 + 10.0 * 6.230252265930176
Epoch 1490, val loss: 1.1992796659469604
Epoch 1500, training loss: 62.35296630859375 = 0.06251680850982666 + 10.0 * 6.2290449142456055
Epoch 1500, val loss: 1.2060235738754272
Epoch 1510, training loss: 62.36194610595703 = 0.06112806126475334 + 10.0 * 6.230082035064697
Epoch 1510, val loss: 1.2126566171646118
Epoch 1520, training loss: 62.331321716308594 = 0.05976700037717819 + 10.0 * 6.2271552085876465
Epoch 1520, val loss: 1.2193727493286133
Epoch 1530, training loss: 62.3870735168457 = 0.05846099928021431 + 10.0 * 6.232861518859863
Epoch 1530, val loss: 1.2260466814041138
Epoch 1540, training loss: 62.33342742919922 = 0.05715812370181084 + 10.0 * 6.227626800537109
Epoch 1540, val loss: 1.2324941158294678
Epoch 1550, training loss: 62.31923294067383 = 0.05591569468379021 + 10.0 * 6.22633171081543
Epoch 1550, val loss: 1.2391804456710815
Epoch 1560, training loss: 62.311195373535156 = 0.054717034101486206 + 10.0 * 6.225647926330566
Epoch 1560, val loss: 1.2458239793777466
Epoch 1570, training loss: 62.32707214355469 = 0.0535578727722168 + 10.0 * 6.227351188659668
Epoch 1570, val loss: 1.2525213956832886
Epoch 1580, training loss: 62.314674377441406 = 0.052408408373594284 + 10.0 * 6.226226329803467
Epoch 1580, val loss: 1.2587298154830933
Epoch 1590, training loss: 62.30479049682617 = 0.05129383131861687 + 10.0 * 6.2253499031066895
Epoch 1590, val loss: 1.2653014659881592
Epoch 1600, training loss: 62.30213165283203 = 0.05022306367754936 + 10.0 * 6.22519063949585
Epoch 1600, val loss: 1.2717161178588867
Epoch 1610, training loss: 62.359405517578125 = 0.04918163642287254 + 10.0 * 6.231022357940674
Epoch 1610, val loss: 1.278174877166748
Epoch 1620, training loss: 62.30839538574219 = 0.04816992208361626 + 10.0 * 6.226022720336914
Epoch 1620, val loss: 1.2845323085784912
Epoch 1630, training loss: 62.300880432128906 = 0.04717426747083664 + 10.0 * 6.22537088394165
Epoch 1630, val loss: 1.2909038066864014
Epoch 1640, training loss: 62.288719177246094 = 0.046218350529670715 + 10.0 * 6.224249839782715
Epoch 1640, val loss: 1.2973688840866089
Epoch 1650, training loss: 62.283992767333984 = 0.045284684747457504 + 10.0 * 6.223870754241943
Epoch 1650, val loss: 1.3036823272705078
Epoch 1660, training loss: 62.3341064453125 = 0.044377345591783524 + 10.0 * 6.228972911834717
Epoch 1660, val loss: 1.3099223375320435
Epoch 1670, training loss: 62.29249572753906 = 0.04348823428153992 + 10.0 * 6.224900722503662
Epoch 1670, val loss: 1.3160622119903564
Epoch 1680, training loss: 62.26728820800781 = 0.042623985558748245 + 10.0 * 6.222466468811035
Epoch 1680, val loss: 1.3222278356552124
Epoch 1690, training loss: 62.26200485229492 = 0.04179714620113373 + 10.0 * 6.222020626068115
Epoch 1690, val loss: 1.3285287618637085
Epoch 1700, training loss: 62.29029846191406 = 0.04099279269576073 + 10.0 * 6.224930763244629
Epoch 1700, val loss: 1.3347156047821045
Epoch 1710, training loss: 62.265716552734375 = 0.040192920714616776 + 10.0 * 6.222552299499512
Epoch 1710, val loss: 1.340652346611023
Epoch 1720, training loss: 62.26463317871094 = 0.039414405822753906 + 10.0 * 6.222521781921387
Epoch 1720, val loss: 1.3466228246688843
Epoch 1730, training loss: 62.24909591674805 = 0.03866169974207878 + 10.0 * 6.221043586730957
Epoch 1730, val loss: 1.3527638912200928
Epoch 1740, training loss: 62.23957443237305 = 0.037939343601465225 + 10.0 * 6.220163345336914
Epoch 1740, val loss: 1.3589130640029907
Epoch 1750, training loss: 62.28460693359375 = 0.03724810481071472 + 10.0 * 6.224736213684082
Epoch 1750, val loss: 1.3647537231445312
Epoch 1760, training loss: 62.252784729003906 = 0.036537956446409225 + 10.0 * 6.221624851226807
Epoch 1760, val loss: 1.3709231615066528
Epoch 1770, training loss: 62.24388885498047 = 0.03586449474096298 + 10.0 * 6.220802307128906
Epoch 1770, val loss: 1.376786231994629
Epoch 1780, training loss: 62.24783706665039 = 0.03520208224654198 + 10.0 * 6.221263408660889
Epoch 1780, val loss: 1.3828315734863281
Epoch 1790, training loss: 62.22529220581055 = 0.03456582874059677 + 10.0 * 6.2190728187561035
Epoch 1790, val loss: 1.3885821104049683
Epoch 1800, training loss: 62.286033630371094 = 0.033943187445402145 + 10.0 * 6.2252092361450195
Epoch 1800, val loss: 1.3943674564361572
Epoch 1810, training loss: 62.24174118041992 = 0.033332519233226776 + 10.0 * 6.220840930938721
Epoch 1810, val loss: 1.400101900100708
Epoch 1820, training loss: 62.2248649597168 = 0.03273400664329529 + 10.0 * 6.219213008880615
Epoch 1820, val loss: 1.405879020690918
Epoch 1830, training loss: 62.21846389770508 = 0.0321611650288105 + 10.0 * 6.218630313873291
Epoch 1830, val loss: 1.4117687940597534
Epoch 1840, training loss: 62.29704284667969 = 0.03160679340362549 + 10.0 * 6.226543426513672
Epoch 1840, val loss: 1.4172484874725342
Epoch 1850, training loss: 62.24104690551758 = 0.031032854691147804 + 10.0 * 6.221001625061035
Epoch 1850, val loss: 1.4228596687316895
Epoch 1860, training loss: 62.21115493774414 = 0.030489986762404442 + 10.0 * 6.218066215515137
Epoch 1860, val loss: 1.4282547235488892
Epoch 1870, training loss: 62.22705841064453 = 0.029968269169330597 + 10.0 * 6.2197089195251465
Epoch 1870, val loss: 1.4341166019439697
Epoch 1880, training loss: 62.20042037963867 = 0.02945507876574993 + 10.0 * 6.21709680557251
Epoch 1880, val loss: 1.4395112991333008
Epoch 1890, training loss: 62.19891357421875 = 0.02895607426762581 + 10.0 * 6.216995716094971
Epoch 1890, val loss: 1.4450774192810059
Epoch 1900, training loss: 62.19404602050781 = 0.02847454510629177 + 10.0 * 6.216557502746582
Epoch 1900, val loss: 1.4506323337554932
Epoch 1910, training loss: 62.2379264831543 = 0.02800440788269043 + 10.0 * 6.220992088317871
Epoch 1910, val loss: 1.4560879468917847
Epoch 1920, training loss: 62.19291687011719 = 0.027533838525414467 + 10.0 * 6.216538429260254
Epoch 1920, val loss: 1.4612464904785156
Epoch 1930, training loss: 62.17717742919922 = 0.027077794075012207 + 10.0 * 6.215010166168213
Epoch 1930, val loss: 1.4667937755584717
Epoch 1940, training loss: 62.18302917480469 = 0.026640325784683228 + 10.0 * 6.215638637542725
Epoch 1940, val loss: 1.4722241163253784
Epoch 1950, training loss: 62.23535919189453 = 0.026210548356175423 + 10.0 * 6.220914840698242
Epoch 1950, val loss: 1.4772762060165405
Epoch 1960, training loss: 62.20139694213867 = 0.025784337893128395 + 10.0 * 6.2175612449646
Epoch 1960, val loss: 1.4826043844223022
Epoch 1970, training loss: 62.1819953918457 = 0.025362849235534668 + 10.0 * 6.215662956237793
Epoch 1970, val loss: 1.4877195358276367
Epoch 1980, training loss: 62.20771026611328 = 0.02496068738400936 + 10.0 * 6.21827507019043
Epoch 1980, val loss: 1.4930022954940796
Epoch 1990, training loss: 62.17844772338867 = 0.02456185407936573 + 10.0 * 6.215388298034668
Epoch 1990, val loss: 1.4979941844940186
Epoch 2000, training loss: 62.162532806396484 = 0.024179507046937943 + 10.0 * 6.2138352394104
Epoch 2000, val loss: 1.503224492073059
Epoch 2010, training loss: 62.16014862060547 = 0.023806143552064896 + 10.0 * 6.213634014129639
Epoch 2010, val loss: 1.5083847045898438
Epoch 2020, training loss: 62.18871307373047 = 0.023444460704922676 + 10.0 * 6.216526985168457
Epoch 2020, val loss: 1.5133851766586304
Epoch 2030, training loss: 62.183799743652344 = 0.023074369877576828 + 10.0 * 6.2160725593566895
Epoch 2030, val loss: 1.5184987783432007
Epoch 2040, training loss: 62.16686248779297 = 0.022714294493198395 + 10.0 * 6.214415073394775
Epoch 2040, val loss: 1.5232130289077759
Epoch 2050, training loss: 62.15349578857422 = 0.02236924320459366 + 10.0 * 6.213112831115723
Epoch 2050, val loss: 1.5284814834594727
Epoch 2060, training loss: 62.177425384521484 = 0.022035669535398483 + 10.0 * 6.21553897857666
Epoch 2060, val loss: 1.5333960056304932
Epoch 2070, training loss: 62.14877700805664 = 0.021701686084270477 + 10.0 * 6.21270751953125
Epoch 2070, val loss: 1.5382790565490723
Epoch 2080, training loss: 62.141868591308594 = 0.02137579396367073 + 10.0 * 6.2120490074157715
Epoch 2080, val loss: 1.5432058572769165
Epoch 2090, training loss: 62.139095306396484 = 0.02106235735118389 + 10.0 * 6.211803436279297
Epoch 2090, val loss: 1.5481770038604736
Epoch 2100, training loss: 62.18787384033203 = 0.0207600649446249 + 10.0 * 6.216711521148682
Epoch 2100, val loss: 1.5531437397003174
Epoch 2110, training loss: 62.14230728149414 = 0.020451335236430168 + 10.0 * 6.212185859680176
Epoch 2110, val loss: 1.5577073097229004
Epoch 2120, training loss: 62.1363410949707 = 0.020154956728219986 + 10.0 * 6.211618423461914
Epoch 2120, val loss: 1.562546968460083
Epoch 2130, training loss: 62.131141662597656 = 0.019867654889822006 + 10.0 * 6.211127281188965
Epoch 2130, val loss: 1.5674172639846802
Epoch 2140, training loss: 62.17806625366211 = 0.019591953605413437 + 10.0 * 6.215847492218018
Epoch 2140, val loss: 1.5720359086990356
Epoch 2150, training loss: 62.14658737182617 = 0.019305163994431496 + 10.0 * 6.212728023529053
Epoch 2150, val loss: 1.5764896869659424
Epoch 2160, training loss: 62.128089904785156 = 0.01902473345398903 + 10.0 * 6.210906505584717
Epoch 2160, val loss: 1.5811508893966675
Epoch 2170, training loss: 62.116554260253906 = 0.018761837854981422 + 10.0 * 6.209779262542725
Epoch 2170, val loss: 1.5858469009399414
Epoch 2180, training loss: 62.11421203613281 = 0.018507275730371475 + 10.0 * 6.209570407867432
Epoch 2180, val loss: 1.5905793905258179
Epoch 2190, training loss: 62.13230895996094 = 0.018259620293974876 + 10.0 * 6.211404800415039
Epoch 2190, val loss: 1.5951106548309326
Epoch 2200, training loss: 62.151336669921875 = 0.018004171550273895 + 10.0 * 6.2133331298828125
Epoch 2200, val loss: 1.599429965019226
Epoch 2210, training loss: 62.110565185546875 = 0.017748834565281868 + 10.0 * 6.2092814445495605
Epoch 2210, val loss: 1.60373055934906
Epoch 2220, training loss: 62.109962463378906 = 0.017508983612060547 + 10.0 * 6.209245204925537
Epoch 2220, val loss: 1.608142614364624
Epoch 2230, training loss: 62.11394500732422 = 0.017276672646403313 + 10.0 * 6.209666728973389
Epoch 2230, val loss: 1.6127703189849854
Epoch 2240, training loss: 62.16154861450195 = 0.017046941444277763 + 10.0 * 6.214449882507324
Epoch 2240, val loss: 1.6170668601989746
Epoch 2250, training loss: 62.11821746826172 = 0.01681484654545784 + 10.0 * 6.210140228271484
Epoch 2250, val loss: 1.621208906173706
Epoch 2260, training loss: 62.09754943847656 = 0.016594037413597107 + 10.0 * 6.208095550537109
Epoch 2260, val loss: 1.6257128715515137
Epoch 2270, training loss: 62.09599685668945 = 0.016380833461880684 + 10.0 * 6.207961559295654
Epoch 2270, val loss: 1.630066990852356
Epoch 2280, training loss: 62.1265983581543 = 0.016170889139175415 + 10.0 * 6.211042881011963
Epoch 2280, val loss: 1.6342867612838745
Epoch 2290, training loss: 62.118953704833984 = 0.015954751521348953 + 10.0 * 6.210299968719482
Epoch 2290, val loss: 1.6382391452789307
Epoch 2300, training loss: 62.09695053100586 = 0.015747033059597015 + 10.0 * 6.208120346069336
Epoch 2300, val loss: 1.6422479152679443
Epoch 2310, training loss: 62.10820388793945 = 0.015545491129159927 + 10.0 * 6.20926570892334
Epoch 2310, val loss: 1.6466084718704224
Epoch 2320, training loss: 62.106571197509766 = 0.01534545049071312 + 10.0 * 6.209122657775879
Epoch 2320, val loss: 1.650511622428894
Epoch 2330, training loss: 62.08637237548828 = 0.015152263455092907 + 10.0 * 6.207121849060059
Epoch 2330, val loss: 1.6546541452407837
Epoch 2340, training loss: 62.080665588378906 = 0.014964648522436619 + 10.0 * 6.206570148468018
Epoch 2340, val loss: 1.6588528156280518
Epoch 2350, training loss: 62.09466552734375 = 0.014782977290451527 + 10.0 * 6.207988262176514
Epoch 2350, val loss: 1.6629847288131714
Epoch 2360, training loss: 62.12445068359375 = 0.01460040733218193 + 10.0 * 6.21098518371582
Epoch 2360, val loss: 1.6667251586914062
Epoch 2370, training loss: 62.08949661254883 = 0.014412249438464642 + 10.0 * 6.207508563995361
Epoch 2370, val loss: 1.6704936027526855
Epoch 2380, training loss: 62.081905364990234 = 0.014232313260436058 + 10.0 * 6.2067670822143555
Epoch 2380, val loss: 1.6744368076324463
Epoch 2390, training loss: 62.07487487792969 = 0.014061973430216312 + 10.0 * 6.206081390380859
Epoch 2390, val loss: 1.678313136100769
Epoch 2400, training loss: 62.08555603027344 = 0.01389473956078291 + 10.0 * 6.2071661949157715
Epoch 2400, val loss: 1.682229995727539
Epoch 2410, training loss: 62.07743453979492 = 0.013727569952607155 + 10.0 * 6.2063703536987305
Epoch 2410, val loss: 1.6861063241958618
Epoch 2420, training loss: 62.11301040649414 = 0.013563090935349464 + 10.0 * 6.209944725036621
Epoch 2420, val loss: 1.6898577213287354
Epoch 2430, training loss: 62.07971954345703 = 0.013397712260484695 + 10.0 * 6.206632137298584
Epoch 2430, val loss: 1.6935055255889893
Epoch 2440, training loss: 62.081092834472656 = 0.013235731981694698 + 10.0 * 6.206785678863525
Epoch 2440, val loss: 1.6970793008804321
Epoch 2450, training loss: 62.06371307373047 = 0.013078809715807438 + 10.0 * 6.205063343048096
Epoch 2450, val loss: 1.7009822130203247
Epoch 2460, training loss: 62.06740951538086 = 0.01292768307030201 + 10.0 * 6.205448150634766
Epoch 2460, val loss: 1.7046654224395752
Epoch 2470, training loss: 62.07829284667969 = 0.012778461910784245 + 10.0 * 6.206551551818848
Epoch 2470, val loss: 1.7083721160888672
Epoch 2480, training loss: 62.06690216064453 = 0.012631935998797417 + 10.0 * 6.205427169799805
Epoch 2480, val loss: 1.7118743658065796
Epoch 2490, training loss: 62.09064483642578 = 0.012488546781241894 + 10.0 * 6.207815647125244
Epoch 2490, val loss: 1.7155029773712158
Epoch 2500, training loss: 62.09459686279297 = 0.01234053261578083 + 10.0 * 6.208225727081299
Epoch 2500, val loss: 1.719037413597107
Epoch 2510, training loss: 62.05062484741211 = 0.012197737582027912 + 10.0 * 6.203842639923096
Epoch 2510, val loss: 1.7224200963974
Epoch 2520, training loss: 62.04472732543945 = 0.012060157023370266 + 10.0 * 6.2032670974731445
Epoch 2520, val loss: 1.7261205911636353
Epoch 2530, training loss: 62.04104232788086 = 0.011927633546292782 + 10.0 * 6.202911376953125
Epoch 2530, val loss: 1.7297618389129639
Epoch 2540, training loss: 62.05324172973633 = 0.011799976229667664 + 10.0 * 6.204144477844238
Epoch 2540, val loss: 1.7332566976547241
Epoch 2550, training loss: 62.11228561401367 = 0.011668602004647255 + 10.0 * 6.210061550140381
Epoch 2550, val loss: 1.7364610433578491
Epoch 2560, training loss: 62.05778503417969 = 0.011534174904227257 + 10.0 * 6.204625129699707
Epoch 2560, val loss: 1.739782452583313
Epoch 2570, training loss: 62.03912353515625 = 0.011404462158679962 + 10.0 * 6.2027716636657715
Epoch 2570, val loss: 1.7431278228759766
Epoch 2580, training loss: 62.03748321533203 = 0.011283068917691708 + 10.0 * 6.202620029449463
Epoch 2580, val loss: 1.7466398477554321
Epoch 2590, training loss: 62.09492111206055 = 0.01116598304361105 + 10.0 * 6.208375453948975
Epoch 2590, val loss: 1.7499552965164185
Epoch 2600, training loss: 62.043399810791016 = 0.011038525030016899 + 10.0 * 6.203236103057861
Epoch 2600, val loss: 1.7531611919403076
Epoch 2610, training loss: 62.04750442504883 = 0.010919458232820034 + 10.0 * 6.203658580780029
Epoch 2610, val loss: 1.7563899755477905
Epoch 2620, training loss: 62.050804138183594 = 0.010800514370203018 + 10.0 * 6.204000473022461
Epoch 2620, val loss: 1.759607195854187
Epoch 2630, training loss: 62.04288864135742 = 0.010688213631510735 + 10.0 * 6.203219890594482
Epoch 2630, val loss: 1.7627004384994507
Epoch 2640, training loss: 62.04065704345703 = 0.01057620719075203 + 10.0 * 6.20300817489624
Epoch 2640, val loss: 1.7660107612609863
Epoch 2650, training loss: 62.03169631958008 = 0.01046530157327652 + 10.0 * 6.202123165130615
Epoch 2650, val loss: 1.769214391708374
Epoch 2660, training loss: 62.02350997924805 = 0.010357645340263844 + 10.0 * 6.201315402984619
Epoch 2660, val loss: 1.772459864616394
Epoch 2670, training loss: 62.05394744873047 = 0.010255151428282261 + 10.0 * 6.20436954498291
Epoch 2670, val loss: 1.775439739227295
Epoch 2680, training loss: 62.02617645263672 = 0.01014531496912241 + 10.0 * 6.201602935791016
Epoch 2680, val loss: 1.7781811952590942
Epoch 2690, training loss: 62.0278434753418 = 0.010038869455456734 + 10.0 * 6.201780319213867
Epoch 2690, val loss: 1.7813639640808105
Epoch 2700, training loss: 62.0269660949707 = 0.009936817921698093 + 10.0 * 6.201703071594238
Epoch 2700, val loss: 1.7842990159988403
Epoch 2710, training loss: 62.01445770263672 = 0.009836209937930107 + 10.0 * 6.2004618644714355
Epoch 2710, val loss: 1.7873185873031616
Epoch 2720, training loss: 62.04215621948242 = 0.009739164263010025 + 10.0 * 6.20324182510376
Epoch 2720, val loss: 1.7904030084609985
Epoch 2730, training loss: 62.041465759277344 = 0.009639015421271324 + 10.0 * 6.203182697296143
Epoch 2730, val loss: 1.792720913887024
Epoch 2740, training loss: 62.013919830322266 = 0.009540020488202572 + 10.0 * 6.200438022613525
Epoch 2740, val loss: 1.7956403493881226
Epoch 2750, training loss: 62.00782775878906 = 0.00944497436285019 + 10.0 * 6.199838161468506
Epoch 2750, val loss: 1.798652172088623
Epoch 2760, training loss: 62.00642776489258 = 0.00935430359095335 + 10.0 * 6.199707508087158
Epoch 2760, val loss: 1.801783800125122
Epoch 2770, training loss: 62.0216064453125 = 0.009268135763704777 + 10.0 * 6.201233863830566
Epoch 2770, val loss: 1.8046241998672485
Epoch 2780, training loss: 62.05097198486328 = 0.009176489897072315 + 10.0 * 6.204179286956787
Epoch 2780, val loss: 1.8070365190505981
Epoch 2790, training loss: 62.035240173339844 = 0.009082316420972347 + 10.0 * 6.202615737915039
Epoch 2790, val loss: 1.809433937072754
Epoch 2800, training loss: 62.01277542114258 = 0.008992879651486874 + 10.0 * 6.20037841796875
Epoch 2800, val loss: 1.8125485181808472
Epoch 2810, training loss: 62.01249313354492 = 0.00890867505222559 + 10.0 * 6.2003583908081055
Epoch 2810, val loss: 1.8153271675109863
Epoch 2820, training loss: 62.06380844116211 = 0.008824101649224758 + 10.0 * 6.205498695373535
Epoch 2820, val loss: 1.8179805278778076
Epoch 2830, training loss: 62.019649505615234 = 0.008738025091588497 + 10.0 * 6.2010908126831055
Epoch 2830, val loss: 1.8205944299697876
Epoch 2840, training loss: 62.001644134521484 = 0.00865577906370163 + 10.0 * 6.199298858642578
Epoch 2840, val loss: 1.8233659267425537
Epoch 2850, training loss: 61.996551513671875 = 0.008577733300626278 + 10.0 * 6.198797225952148
Epoch 2850, val loss: 1.8261876106262207
Epoch 2860, training loss: 62.043025970458984 = 0.008502024225890636 + 10.0 * 6.203452110290527
Epoch 2860, val loss: 1.8287575244903564
Epoch 2870, training loss: 62.00326919555664 = 0.008419118821620941 + 10.0 * 6.199484825134277
Epoch 2870, val loss: 1.8310911655426025
Epoch 2880, training loss: 62.00737762451172 = 0.008339392952620983 + 10.0 * 6.199903964996338
Epoch 2880, val loss: 1.8336628675460815
Epoch 2890, training loss: 62.00790786743164 = 0.008263664320111275 + 10.0 * 6.19996452331543
Epoch 2890, val loss: 1.8361278772354126
Epoch 2900, training loss: 61.99638366699219 = 0.008186918683350086 + 10.0 * 6.198819637298584
Epoch 2900, val loss: 1.8388922214508057
Epoch 2910, training loss: 61.987518310546875 = 0.008114174008369446 + 10.0 * 6.197940349578857
Epoch 2910, val loss: 1.841497540473938
Epoch 2920, training loss: 62.011634826660156 = 0.008043942973017693 + 10.0 * 6.200358867645264
Epoch 2920, val loss: 1.8439345359802246
Epoch 2930, training loss: 61.99756622314453 = 0.007970757782459259 + 10.0 * 6.198959827423096
Epoch 2930, val loss: 1.8463737964630127
Epoch 2940, training loss: 62.01251983642578 = 0.00789895560592413 + 10.0 * 6.2004618644714355
Epoch 2940, val loss: 1.8487942218780518
Epoch 2950, training loss: 62.01557922363281 = 0.007828845642507076 + 10.0 * 6.200775146484375
Epoch 2950, val loss: 1.8511276245117188
Epoch 2960, training loss: 62.02423858642578 = 0.007758724968880415 + 10.0 * 6.201647758483887
Epoch 2960, val loss: 1.8535457849502563
Epoch 2970, training loss: 61.97903060913086 = 0.0076889111660420895 + 10.0 * 6.197134017944336
Epoch 2970, val loss: 1.8557263612747192
Epoch 2980, training loss: 61.979087829589844 = 0.007622550241649151 + 10.0 * 6.197146415710449
Epoch 2980, val loss: 1.8581925630569458
Epoch 2990, training loss: 61.97486114501953 = 0.007559214718639851 + 10.0 * 6.196730136871338
Epoch 2990, val loss: 1.86065673828125
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 87.91959381103516 = 1.9514390230178833 + 10.0 * 8.59681510925293
Epoch 0, val loss: 1.9504797458648682
Epoch 10, training loss: 87.90213775634766 = 1.9415515661239624 + 10.0 * 8.59605884552002
Epoch 10, val loss: 1.9408501386642456
Epoch 20, training loss: 87.83805084228516 = 1.9294248819351196 + 10.0 * 8.590862274169922
Epoch 20, val loss: 1.9284842014312744
Epoch 30, training loss: 87.47273254394531 = 1.9138082265853882 + 10.0 * 8.555891990661621
Epoch 30, val loss: 1.9122587442398071
Epoch 40, training loss: 85.16029357910156 = 1.8953065872192383 + 10.0 * 8.326498031616211
Epoch 40, val loss: 1.8934800624847412
Epoch 50, training loss: 75.35646057128906 = 1.8742581605911255 + 10.0 * 7.348220348358154
Epoch 50, val loss: 1.8718550205230713
Epoch 60, training loss: 72.5247802734375 = 1.859555721282959 + 10.0 * 7.066522121429443
Epoch 60, val loss: 1.857791543006897
Epoch 70, training loss: 71.09339904785156 = 1.8483896255493164 + 10.0 * 6.924501419067383
Epoch 70, val loss: 1.8460997343063354
Epoch 80, training loss: 70.25508117675781 = 1.835585355758667 + 10.0 * 6.841949462890625
Epoch 80, val loss: 1.8335307836532593
Epoch 90, training loss: 69.64310455322266 = 1.8244017362594604 + 10.0 * 6.781870365142822
Epoch 90, val loss: 1.822633147239685
Epoch 100, training loss: 69.14822387695312 = 1.8130671977996826 + 10.0 * 6.733515739440918
Epoch 100, val loss: 1.8119370937347412
Epoch 110, training loss: 68.6601333618164 = 1.8034236431121826 + 10.0 * 6.685671329498291
Epoch 110, val loss: 1.8029108047485352
Epoch 120, training loss: 68.23540496826172 = 1.7950043678283691 + 10.0 * 6.644040107727051
Epoch 120, val loss: 1.7949882745742798
Epoch 130, training loss: 67.8640365600586 = 1.7873927354812622 + 10.0 * 6.607664108276367
Epoch 130, val loss: 1.7877074480056763
Epoch 140, training loss: 67.51597595214844 = 1.7797788381576538 + 10.0 * 6.573619842529297
Epoch 140, val loss: 1.7806867361068726
Epoch 150, training loss: 67.2676773071289 = 1.7718766927719116 + 10.0 * 6.549580097198486
Epoch 150, val loss: 1.773490071296692
Epoch 160, training loss: 67.02024841308594 = 1.763486385345459 + 10.0 * 6.525676250457764
Epoch 160, val loss: 1.7658615112304688
Epoch 170, training loss: 66.79954528808594 = 1.7547144889831543 + 10.0 * 6.504483222961426
Epoch 170, val loss: 1.7580302953720093
Epoch 180, training loss: 66.59579467773438 = 1.745501160621643 + 10.0 * 6.485029697418213
Epoch 180, val loss: 1.7499767541885376
Epoch 190, training loss: 66.43312072753906 = 1.7355406284332275 + 10.0 * 6.469757556915283
Epoch 190, val loss: 1.7414464950561523
Epoch 200, training loss: 66.25038146972656 = 1.7246952056884766 + 10.0 * 6.452569007873535
Epoch 200, val loss: 1.7321505546569824
Epoch 210, training loss: 66.0934829711914 = 1.7128163576126099 + 10.0 * 6.4380669593811035
Epoch 210, val loss: 1.7220748662948608
Epoch 220, training loss: 65.95854949951172 = 1.6998608112335205 + 10.0 * 6.425868988037109
Epoch 220, val loss: 1.7111718654632568
Epoch 230, training loss: 65.8548812866211 = 1.6855953931808472 + 10.0 * 6.416928768157959
Epoch 230, val loss: 1.6992954015731812
Epoch 240, training loss: 65.73442840576172 = 1.6698850393295288 + 10.0 * 6.406454563140869
Epoch 240, val loss: 1.686221718788147
Epoch 250, training loss: 65.62287139892578 = 1.6527873277664185 + 10.0 * 6.397008419036865
Epoch 250, val loss: 1.6720900535583496
Epoch 260, training loss: 65.5440902709961 = 1.634290337562561 + 10.0 * 6.390979766845703
Epoch 260, val loss: 1.656943678855896
Epoch 270, training loss: 65.44692993164062 = 1.6142007112503052 + 10.0 * 6.383272647857666
Epoch 270, val loss: 1.6405516862869263
Epoch 280, training loss: 65.35094451904297 = 1.592766284942627 + 10.0 * 6.375817775726318
Epoch 280, val loss: 1.6232043504714966
Epoch 290, training loss: 65.26243591308594 = 1.569956660270691 + 10.0 * 6.369247913360596
Epoch 290, val loss: 1.6049299240112305
Epoch 300, training loss: 65.21595764160156 = 1.5457521677017212 + 10.0 * 6.367020606994629
Epoch 300, val loss: 1.585716724395752
Epoch 310, training loss: 65.13391876220703 = 1.5201538801193237 + 10.0 * 6.3613762855529785
Epoch 310, val loss: 1.5656205415725708
Epoch 320, training loss: 65.02943420410156 = 1.4935177564620972 + 10.0 * 6.3535919189453125
Epoch 320, val loss: 1.545043706893921
Epoch 330, training loss: 64.95853424072266 = 1.4659273624420166 + 10.0 * 6.3492608070373535
Epoch 330, val loss: 1.5239180326461792
Epoch 340, training loss: 64.90768432617188 = 1.4374700784683228 + 10.0 * 6.347021579742432
Epoch 340, val loss: 1.502342939376831
Epoch 350, training loss: 64.83911895751953 = 1.4082176685333252 + 10.0 * 6.343090534210205
Epoch 350, val loss: 1.4804846048355103
Epoch 360, training loss: 64.75361633300781 = 1.3785673379898071 + 10.0 * 6.337504863739014
Epoch 360, val loss: 1.458499550819397
Epoch 370, training loss: 64.67411804199219 = 1.3484752178192139 + 10.0 * 6.332564353942871
Epoch 370, val loss: 1.4364573955535889
Epoch 380, training loss: 64.63418579101562 = 1.318163514137268 + 10.0 * 6.331602573394775
Epoch 380, val loss: 1.4144471883773804
Epoch 390, training loss: 64.58309936523438 = 1.2874221801757812 + 10.0 * 6.3295674324035645
Epoch 390, val loss: 1.3922892808914185
Epoch 400, training loss: 64.49272155761719 = 1.2568976879119873 + 10.0 * 6.323582649230957
Epoch 400, val loss: 1.370532512664795
Epoch 410, training loss: 64.41970825195312 = 1.2265533208847046 + 10.0 * 6.319315433502197
Epoch 410, val loss: 1.349100947380066
Epoch 420, training loss: 64.37516784667969 = 1.196361780166626 + 10.0 * 6.317880153656006
Epoch 420, val loss: 1.3279756307601929
Epoch 430, training loss: 64.33111572265625 = 1.1664587259292603 + 10.0 * 6.316465377807617
Epoch 430, val loss: 1.3070839643478394
Epoch 440, training loss: 64.25060272216797 = 1.1368017196655273 + 10.0 * 6.311380386352539
Epoch 440, val loss: 1.2866733074188232
Epoch 450, training loss: 64.19322204589844 = 1.1076874732971191 + 10.0 * 6.308553218841553
Epoch 450, val loss: 1.2668170928955078
Epoch 460, training loss: 64.14226531982422 = 1.0791083574295044 + 10.0 * 6.306315898895264
Epoch 460, val loss: 1.2475793361663818
Epoch 470, training loss: 64.11175537109375 = 1.0510514974594116 + 10.0 * 6.306070804595947
Epoch 470, val loss: 1.228754997253418
Epoch 480, training loss: 64.05144500732422 = 1.023512363433838 + 10.0 * 6.302793502807617
Epoch 480, val loss: 1.210553765296936
Epoch 490, training loss: 63.99488067626953 = 0.9967331886291504 + 10.0 * 6.299814701080322
Epoch 490, val loss: 1.193016529083252
Epoch 500, training loss: 63.9497184753418 = 0.9706078171730042 + 10.0 * 6.297911167144775
Epoch 500, val loss: 1.1762645244598389
Epoch 510, training loss: 63.905887603759766 = 0.9450729489326477 + 10.0 * 6.29608154296875
Epoch 510, val loss: 1.1599960327148438
Epoch 520, training loss: 63.86548614501953 = 0.9202155470848083 + 10.0 * 6.294527053833008
Epoch 520, val loss: 1.1444189548492432
Epoch 530, training loss: 63.81612777709961 = 0.8960625529289246 + 10.0 * 6.292006492614746
Epoch 530, val loss: 1.129368782043457
Epoch 540, training loss: 63.812095642089844 = 0.8724850416183472 + 10.0 * 6.293961048126221
Epoch 540, val loss: 1.1149752140045166
Epoch 550, training loss: 63.750118255615234 = 0.8492092490196228 + 10.0 * 6.290091037750244
Epoch 550, val loss: 1.1010124683380127
Epoch 560, training loss: 63.71903610229492 = 0.8266898989677429 + 10.0 * 6.289234638214111
Epoch 560, val loss: 1.0874691009521484
Epoch 570, training loss: 63.66429138183594 = 0.8046345710754395 + 10.0 * 6.285965919494629
Epoch 570, val loss: 1.0747178792953491
Epoch 580, training loss: 63.62483596801758 = 0.7830817699432373 + 10.0 * 6.284175395965576
Epoch 580, val loss: 1.0621787309646606
Epoch 590, training loss: 63.60013961791992 = 0.7620072960853577 + 10.0 * 6.283812999725342
Epoch 590, val loss: 1.0502156019210815
Epoch 600, training loss: 63.56412124633789 = 0.7412069439888 + 10.0 * 6.282291412353516
Epoch 600, val loss: 1.0385173559188843
Epoch 610, training loss: 63.5225830078125 = 0.720947265625 + 10.0 * 6.280163764953613
Epoch 610, val loss: 1.0272860527038574
Epoch 620, training loss: 63.48581314086914 = 0.7011130452156067 + 10.0 * 6.278470039367676
Epoch 620, val loss: 1.0164604187011719
Epoch 630, training loss: 63.47087860107422 = 0.6817212104797363 + 10.0 * 6.278915882110596
Epoch 630, val loss: 1.0062764883041382
Epoch 640, training loss: 63.422725677490234 = 0.6625396609306335 + 10.0 * 6.2760186195373535
Epoch 640, val loss: 0.9961416721343994
Epoch 650, training loss: 63.3939094543457 = 0.6438122391700745 + 10.0 * 6.275009632110596
Epoch 650, val loss: 0.9866530895233154
Epoch 660, training loss: 63.42005157470703 = 0.6255550980567932 + 10.0 * 6.279449462890625
Epoch 660, val loss: 0.9774066209793091
Epoch 670, training loss: 63.34190368652344 = 0.6076077222824097 + 10.0 * 6.2734293937683105
Epoch 670, val loss: 0.9686594605445862
Epoch 680, training loss: 63.30535125732422 = 0.5901639461517334 + 10.0 * 6.271518707275391
Epoch 680, val loss: 0.9602947235107422
Epoch 690, training loss: 63.27634048461914 = 0.5732123851776123 + 10.0 * 6.270312786102295
Epoch 690, val loss: 0.9524936079978943
Epoch 700, training loss: 63.2809944152832 = 0.5566380023956299 + 10.0 * 6.272435665130615
Epoch 700, val loss: 0.9450184106826782
Epoch 710, training loss: 63.247093200683594 = 0.5404654145240784 + 10.0 * 6.270662784576416
Epoch 710, val loss: 0.9379122257232666
Epoch 720, training loss: 63.20999526977539 = 0.5246978402137756 + 10.0 * 6.268529891967773
Epoch 720, val loss: 0.9312353730201721
Epoch 730, training loss: 63.1767692565918 = 0.5094138383865356 + 10.0 * 6.266735553741455
Epoch 730, val loss: 0.9250456690788269
Epoch 740, training loss: 63.14701461791992 = 0.4946012794971466 + 10.0 * 6.2652411460876465
Epoch 740, val loss: 0.9193141460418701
Epoch 750, training loss: 63.15373229980469 = 0.4802009165287018 + 10.0 * 6.267353057861328
Epoch 750, val loss: 0.9139420390129089
Epoch 760, training loss: 63.11323928833008 = 0.4663037955760956 + 10.0 * 6.264693260192871
Epoch 760, val loss: 0.9088332056999207
Epoch 770, training loss: 63.0822868347168 = 0.4527316987514496 + 10.0 * 6.262955665588379
Epoch 770, val loss: 0.9041327834129333
Epoch 780, training loss: 63.05630111694336 = 0.4397197961807251 + 10.0 * 6.261658191680908
Epoch 780, val loss: 0.8999494314193726
Epoch 790, training loss: 63.0546760559082 = 0.427074670791626 + 10.0 * 6.262760162353516
Epoch 790, val loss: 0.896117627620697
Epoch 800, training loss: 63.04038619995117 = 0.4147508442401886 + 10.0 * 6.262563705444336
Epoch 800, val loss: 0.8924664855003357
Epoch 810, training loss: 63.00712966918945 = 0.40281200408935547 + 10.0 * 6.26043176651001
Epoch 810, val loss: 0.8892806172370911
Epoch 820, training loss: 62.98127746582031 = 0.3913438618183136 + 10.0 * 6.258993625640869
Epoch 820, val loss: 0.8864456415176392
Epoch 830, training loss: 62.958621978759766 = 0.3802119493484497 + 10.0 * 6.257841110229492
Epoch 830, val loss: 0.8839912414550781
Epoch 840, training loss: 62.955596923828125 = 0.3694376051425934 + 10.0 * 6.258615970611572
Epoch 840, val loss: 0.8818034529685974
Epoch 850, training loss: 62.954132080078125 = 0.35895827412605286 + 10.0 * 6.259517192840576
Epoch 850, val loss: 0.8799669146537781
Epoch 860, training loss: 62.93031311035156 = 0.3487216830253601 + 10.0 * 6.258159160614014
Epoch 860, val loss: 0.8780158758163452
Epoch 870, training loss: 62.888553619384766 = 0.3389224410057068 + 10.0 * 6.254963397979736
Epoch 870, val loss: 0.8766248822212219
Epoch 880, training loss: 62.87318801879883 = 0.329393595457077 + 10.0 * 6.2543792724609375
Epoch 880, val loss: 0.8754923343658447
Epoch 890, training loss: 62.85353469848633 = 0.3201695680618286 + 10.0 * 6.253336429595947
Epoch 890, val loss: 0.8745459914207458
Epoch 900, training loss: 62.84391784667969 = 0.31118711829185486 + 10.0 * 6.253273010253906
Epoch 900, val loss: 0.8737804293632507
Epoch 910, training loss: 62.85042953491211 = 0.3023994565010071 + 10.0 * 6.25480318069458
Epoch 910, val loss: 0.8731695413589478
Epoch 920, training loss: 62.825042724609375 = 0.29389703273773193 + 10.0 * 6.253114700317383
Epoch 920, val loss: 0.872390627861023
Epoch 930, training loss: 62.80198287963867 = 0.28563910722732544 + 10.0 * 6.251634120941162
Epoch 930, val loss: 0.872194766998291
Epoch 940, training loss: 62.795135498046875 = 0.27764207124710083 + 10.0 * 6.251749515533447
Epoch 940, val loss: 0.8720210790634155
Epoch 950, training loss: 62.769405364990234 = 0.26984134316444397 + 10.0 * 6.2499566078186035
Epoch 950, val loss: 0.8719664216041565
Epoch 960, training loss: 62.76953887939453 = 0.26225507259368896 + 10.0 * 6.250728130340576
Epoch 960, val loss: 0.8721269965171814
Epoch 970, training loss: 62.73854446411133 = 0.25483834743499756 + 10.0 * 6.24837064743042
Epoch 970, val loss: 0.8723371624946594
Epoch 980, training loss: 62.756004333496094 = 0.24768291413784027 + 10.0 * 6.2508320808410645
Epoch 980, val loss: 0.8726881146430969
Epoch 990, training loss: 62.7307014465332 = 0.24062608182430267 + 10.0 * 6.249007225036621
Epoch 990, val loss: 0.8730939626693726
Epoch 1000, training loss: 62.701026916503906 = 0.23377135396003723 + 10.0 * 6.246725559234619
Epoch 1000, val loss: 0.8737944960594177
Epoch 1010, training loss: 62.68716049194336 = 0.22719120979309082 + 10.0 * 6.245996952056885
Epoch 1010, val loss: 0.8746275901794434
Epoch 1020, training loss: 62.687644958496094 = 0.2207631766796112 + 10.0 * 6.246687889099121
Epoch 1020, val loss: 0.8755837678909302
Epoch 1030, training loss: 62.6849479675293 = 0.21450792253017426 + 10.0 * 6.247044086456299
Epoch 1030, val loss: 0.8764577507972717
Epoch 1040, training loss: 62.66849899291992 = 0.20842117071151733 + 10.0 * 6.246007919311523
Epoch 1040, val loss: 0.8774659037590027
Epoch 1050, training loss: 62.646785736083984 = 0.20248813927173615 + 10.0 * 6.244429588317871
Epoch 1050, val loss: 0.8787581324577332
Epoch 1060, training loss: 62.63551330566406 = 0.19674603641033173 + 10.0 * 6.2438764572143555
Epoch 1060, val loss: 0.8801825046539307
Epoch 1070, training loss: 62.62277603149414 = 0.19119121134281158 + 10.0 * 6.243158340454102
Epoch 1070, val loss: 0.8816105723381042
Epoch 1080, training loss: 62.6524772644043 = 0.1858140379190445 + 10.0 * 6.246666431427002
Epoch 1080, val loss: 0.8831424713134766
Epoch 1090, training loss: 62.6292839050293 = 0.1805189549922943 + 10.0 * 6.244876384735107
Epoch 1090, val loss: 0.8845723867416382
Epoch 1100, training loss: 62.614341735839844 = 0.1753927767276764 + 10.0 * 6.243895053863525
Epoch 1100, val loss: 0.886272132396698
Epoch 1110, training loss: 62.58403396606445 = 0.17039819061756134 + 10.0 * 6.241363525390625
Epoch 1110, val loss: 0.8881065249443054
Epoch 1120, training loss: 62.57596969604492 = 0.16559135913848877 + 10.0 * 6.241037845611572
Epoch 1120, val loss: 0.8899851441383362
Epoch 1130, training loss: 62.5796012878418 = 0.16093586385250092 + 10.0 * 6.241866588592529
Epoch 1130, val loss: 0.8920276165008545
Epoch 1140, training loss: 62.56793212890625 = 0.15641534328460693 + 10.0 * 6.241151809692383
Epoch 1140, val loss: 0.8939899206161499
Epoch 1150, training loss: 62.54605484008789 = 0.15199846029281616 + 10.0 * 6.239405632019043
Epoch 1150, val loss: 0.8961765766143799
Epoch 1160, training loss: 62.57589340209961 = 0.14770719408988953 + 10.0 * 6.242818355560303
Epoch 1160, val loss: 0.8984090089797974
Epoch 1170, training loss: 62.55472946166992 = 0.14356765151023865 + 10.0 * 6.241116523742676
Epoch 1170, val loss: 0.9006876945495605
Epoch 1180, training loss: 62.521636962890625 = 0.1395641267299652 + 10.0 * 6.2382073402404785
Epoch 1180, val loss: 0.9030483365058899
Epoch 1190, training loss: 62.50662612915039 = 0.1356789767742157 + 10.0 * 6.237094879150391
Epoch 1190, val loss: 0.9056150913238525
Epoch 1200, training loss: 62.50125503540039 = 0.1319240927696228 + 10.0 * 6.23693323135376
Epoch 1200, val loss: 0.9082111716270447
Epoch 1210, training loss: 62.52312469482422 = 0.1282651275396347 + 10.0 * 6.239485740661621
Epoch 1210, val loss: 0.9108186960220337
Epoch 1220, training loss: 62.49524688720703 = 0.12471447139978409 + 10.0 * 6.237053394317627
Epoch 1220, val loss: 0.913429856300354
Epoch 1230, training loss: 62.5054931640625 = 0.12127213180065155 + 10.0 * 6.23842191696167
Epoch 1230, val loss: 0.9160815477371216
Epoch 1240, training loss: 62.47822189331055 = 0.11792050302028656 + 10.0 * 6.236030101776123
Epoch 1240, val loss: 0.9188539385795593
Epoch 1250, training loss: 62.46828079223633 = 0.11468294262886047 + 10.0 * 6.235360145568848
Epoch 1250, val loss: 0.9216794967651367
Epoch 1260, training loss: 62.47744369506836 = 0.11158430576324463 + 10.0 * 6.236586093902588
Epoch 1260, val loss: 0.9246909022331238
Epoch 1270, training loss: 62.45960235595703 = 0.10854774713516235 + 10.0 * 6.235105514526367
Epoch 1270, val loss: 0.9275748133659363
Epoch 1280, training loss: 62.44344711303711 = 0.10560528188943863 + 10.0 * 6.233784198760986
Epoch 1280, val loss: 0.9306667447090149
Epoch 1290, training loss: 62.437522888183594 = 0.10276200622320175 + 10.0 * 6.233476161956787
Epoch 1290, val loss: 0.9337663650512695
Epoch 1300, training loss: 62.456207275390625 = 0.10002459585666656 + 10.0 * 6.2356181144714355
Epoch 1300, val loss: 0.9368476867675781
Epoch 1310, training loss: 62.446632385253906 = 0.09735966473817825 + 10.0 * 6.234927177429199
Epoch 1310, val loss: 0.9401585459709167
Epoch 1320, training loss: 62.429481506347656 = 0.09474945068359375 + 10.0 * 6.233473300933838
Epoch 1320, val loss: 0.9433324933052063
Epoch 1330, training loss: 62.432369232177734 = 0.0922636017203331 + 10.0 * 6.234010696411133
Epoch 1330, val loss: 0.9467865824699402
Epoch 1340, training loss: 62.402679443359375 = 0.08982866257429123 + 10.0 * 6.231285095214844
Epoch 1340, val loss: 0.95006263256073
Epoch 1350, training loss: 62.39895248413086 = 0.08749737590551376 + 10.0 * 6.23114538192749
Epoch 1350, val loss: 0.9535984992980957
Epoch 1360, training loss: 62.407894134521484 = 0.08523555845022202 + 10.0 * 6.232265949249268
Epoch 1360, val loss: 0.9570900201797485
Epoch 1370, training loss: 62.403076171875 = 0.08302970230579376 + 10.0 * 6.232004642486572
Epoch 1370, val loss: 0.960654616355896
Epoch 1380, training loss: 62.39390563964844 = 0.08088868856430054 + 10.0 * 6.231301784515381
Epoch 1380, val loss: 0.9640073180198669
Epoch 1390, training loss: 62.3798713684082 = 0.07885268330574036 + 10.0 * 6.230101585388184
Epoch 1390, val loss: 0.9677135348320007
Epoch 1400, training loss: 62.36614990234375 = 0.07687065750360489 + 10.0 * 6.228928089141846
Epoch 1400, val loss: 0.9713098406791687
Epoch 1410, training loss: 62.360877990722656 = 0.07496009021997452 + 10.0 * 6.2285919189453125
Epoch 1410, val loss: 0.9750242829322815
Epoch 1420, training loss: 62.409976959228516 = 0.07311537116765976 + 10.0 * 6.2336859703063965
Epoch 1420, val loss: 0.978576123714447
Epoch 1430, training loss: 62.40937423706055 = 0.0712762176990509 + 10.0 * 6.233809471130371
Epoch 1430, val loss: 0.982256293296814
Epoch 1440, training loss: 62.34675598144531 = 0.06951434910297394 + 10.0 * 6.227724075317383
Epoch 1440, val loss: 0.9858865141868591
Epoch 1450, training loss: 62.33987045288086 = 0.06782730668783188 + 10.0 * 6.227204322814941
Epoch 1450, val loss: 0.9896643757820129
Epoch 1460, training loss: 62.374359130859375 = 0.06619394570589066 + 10.0 * 6.230816841125488
Epoch 1460, val loss: 0.9935294985771179
Epoch 1470, training loss: 62.332862854003906 = 0.06460177153348923 + 10.0 * 6.226826190948486
Epoch 1470, val loss: 0.9969905018806458
Epoch 1480, training loss: 62.330047607421875 = 0.06305264681577682 + 10.0 * 6.226699352264404
Epoch 1480, val loss: 1.0009474754333496
Epoch 1490, training loss: 62.320106506347656 = 0.061582207679748535 + 10.0 * 6.2258524894714355
Epoch 1490, val loss: 1.0046682357788086
Epoch 1500, training loss: 62.317298889160156 = 0.06014377996325493 + 10.0 * 6.225715637207031
Epoch 1500, val loss: 1.0085474252700806
Epoch 1510, training loss: 62.362205505371094 = 0.058757226914167404 + 10.0 * 6.230344772338867
Epoch 1510, val loss: 1.0121829509735107
Epoch 1520, training loss: 62.32398223876953 = 0.05739457160234451 + 10.0 * 6.226658821105957
Epoch 1520, val loss: 1.0160664319992065
Epoch 1530, training loss: 62.31157302856445 = 0.05607561022043228 + 10.0 * 6.225549697875977
Epoch 1530, val loss: 1.0198785066604614
Epoch 1540, training loss: 62.31233215332031 = 0.05481110140681267 + 10.0 * 6.225752353668213
Epoch 1540, val loss: 1.0237380266189575
Epoch 1550, training loss: 62.29506301879883 = 0.053570330142974854 + 10.0 * 6.224149227142334
Epoch 1550, val loss: 1.0275838375091553
Epoch 1560, training loss: 62.294063568115234 = 0.05237278714776039 + 10.0 * 6.2241692543029785
Epoch 1560, val loss: 1.0315104722976685
Epoch 1570, training loss: 62.34134292602539 = 0.051213931292295456 + 10.0 * 6.229012966156006
Epoch 1570, val loss: 1.0353419780731201
Epoch 1580, training loss: 62.29420471191406 = 0.05006425827741623 + 10.0 * 6.224413871765137
Epoch 1580, val loss: 1.039085030555725
Epoch 1590, training loss: 62.277103424072266 = 0.04896223917603493 + 10.0 * 6.222814083099365
Epoch 1590, val loss: 1.042990803718567
Epoch 1600, training loss: 62.2801399230957 = 0.04790925979614258 + 10.0 * 6.2232232093811035
Epoch 1600, val loss: 1.0469460487365723
Epoch 1610, training loss: 62.3190803527832 = 0.04687764495611191 + 10.0 * 6.227220058441162
Epoch 1610, val loss: 1.0508040189743042
Epoch 1620, training loss: 62.27809524536133 = 0.04587884247303009 + 10.0 * 6.223221778869629
Epoch 1620, val loss: 1.054477572441101
Epoch 1630, training loss: 62.26713943481445 = 0.044904936105012894 + 10.0 * 6.222223281860352
Epoch 1630, val loss: 1.0584094524383545
Epoch 1640, training loss: 62.27445983886719 = 0.043961938470602036 + 10.0 * 6.223050117492676
Epoch 1640, val loss: 1.0622807741165161
Epoch 1650, training loss: 62.26287078857422 = 0.04304315894842148 + 10.0 * 6.221982479095459
Epoch 1650, val loss: 1.0660744905471802
Epoch 1660, training loss: 62.25700759887695 = 0.04215909540653229 + 10.0 * 6.221484661102295
Epoch 1660, val loss: 1.0699331760406494
Epoch 1670, training loss: 62.250980377197266 = 0.04129672050476074 + 10.0 * 6.220968246459961
Epoch 1670, val loss: 1.0738009214401245
Epoch 1680, training loss: 62.29906463623047 = 0.04046061262488365 + 10.0 * 6.225860118865967
Epoch 1680, val loss: 1.0775104761123657
Epoch 1690, training loss: 62.26492691040039 = 0.039629627019166946 + 10.0 * 6.222529411315918
Epoch 1690, val loss: 1.0812487602233887
Epoch 1700, training loss: 62.26374816894531 = 0.03882922604680061 + 10.0 * 6.22249174118042
Epoch 1700, val loss: 1.085017204284668
Epoch 1710, training loss: 62.238441467285156 = 0.03806165233254433 + 10.0 * 6.220037937164307
Epoch 1710, val loss: 1.0887486934661865
Epoch 1720, training loss: 62.267250061035156 = 0.037322212010622025 + 10.0 * 6.222992897033691
Epoch 1720, val loss: 1.092668056488037
Epoch 1730, training loss: 62.240840911865234 = 0.03658096492290497 + 10.0 * 6.220426082611084
Epoch 1730, val loss: 1.0961459875106812
Epoch 1740, training loss: 62.236228942871094 = 0.035879313945770264 + 10.0 * 6.220034599304199
Epoch 1740, val loss: 1.100111961364746
Epoch 1750, training loss: 62.237083435058594 = 0.035188447684049606 + 10.0 * 6.220189571380615
Epoch 1750, val loss: 1.103640079498291
Epoch 1760, training loss: 62.220699310302734 = 0.034517254680395126 + 10.0 * 6.218618392944336
Epoch 1760, val loss: 1.1072771549224854
Epoch 1770, training loss: 62.24906539916992 = 0.033870629966259 + 10.0 * 6.221519470214844
Epoch 1770, val loss: 1.1109613180160522
Epoch 1780, training loss: 62.259132385253906 = 0.033240847289562225 + 10.0 * 6.222589015960693
Epoch 1780, val loss: 1.1145933866500854
Epoch 1790, training loss: 62.22246551513672 = 0.032601986080408096 + 10.0 * 6.218986511230469
Epoch 1790, val loss: 1.1181256771087646
Epoch 1800, training loss: 62.20708465576172 = 0.031997211277484894 + 10.0 * 6.217508792877197
Epoch 1800, val loss: 1.1219494342803955
Epoch 1810, training loss: 62.200218200683594 = 0.031417299062013626 + 10.0 * 6.216879844665527
Epoch 1810, val loss: 1.1255728006362915
Epoch 1820, training loss: 62.2132453918457 = 0.03085310570895672 + 10.0 * 6.2182393074035645
Epoch 1820, val loss: 1.1292349100112915
Epoch 1830, training loss: 62.22480392456055 = 0.03028782829642296 + 10.0 * 6.219451427459717
Epoch 1830, val loss: 1.1328279972076416
Epoch 1840, training loss: 62.19923782348633 = 0.029742594808340073 + 10.0 * 6.216949462890625
Epoch 1840, val loss: 1.1361736059188843
Epoch 1850, training loss: 62.18946838378906 = 0.029219765216112137 + 10.0 * 6.216024875640869
Epoch 1850, val loss: 1.1398450136184692
Epoch 1860, training loss: 62.187862396240234 = 0.02871130220592022 + 10.0 * 6.215915203094482
Epoch 1860, val loss: 1.1434235572814941
Epoch 1870, training loss: 62.202938079833984 = 0.028220394626259804 + 10.0 * 6.217471599578857
Epoch 1870, val loss: 1.1468778848648071
Epoch 1880, training loss: 62.19864273071289 = 0.02772805467247963 + 10.0 * 6.2170915603637695
Epoch 1880, val loss: 1.1502387523651123
Epoch 1890, training loss: 62.2368278503418 = 0.027245383709669113 + 10.0 * 6.220958232879639
Epoch 1890, val loss: 1.1535725593566895
Epoch 1900, training loss: 62.18206787109375 = 0.026764795184135437 + 10.0 * 6.2155303955078125
Epoch 1900, val loss: 1.1571794748306274
Epoch 1910, training loss: 62.170169830322266 = 0.026316387578845024 + 10.0 * 6.214385032653809
Epoch 1910, val loss: 1.1606444120407104
Epoch 1920, training loss: 62.170005798339844 = 0.025881055742502213 + 10.0 * 6.214412212371826
Epoch 1920, val loss: 1.1640912294387817
Epoch 1930, training loss: 62.1931266784668 = 0.025455905124545097 + 10.0 * 6.216767311096191
Epoch 1930, val loss: 1.167497158050537
Epoch 1940, training loss: 62.17557907104492 = 0.025025995448231697 + 10.0 * 6.215055465698242
Epoch 1940, val loss: 1.1709537506103516
Epoch 1950, training loss: 62.17179489135742 = 0.024610916152596474 + 10.0 * 6.214718341827393
Epoch 1950, val loss: 1.1741660833358765
Epoch 1960, training loss: 62.17034149169922 = 0.024210933595895767 + 10.0 * 6.21461296081543
Epoch 1960, val loss: 1.177679181098938
Epoch 1970, training loss: 62.198482513427734 = 0.02382415346801281 + 10.0 * 6.217465877532959
Epoch 1970, val loss: 1.1809717416763306
Epoch 1980, training loss: 62.1674690246582 = 0.023438239470124245 + 10.0 * 6.21440315246582
Epoch 1980, val loss: 1.1840888261795044
Epoch 1990, training loss: 62.15633773803711 = 0.023066436871886253 + 10.0 * 6.213326930999756
Epoch 1990, val loss: 1.1875542402267456
Epoch 2000, training loss: 62.16387939453125 = 0.0227048397064209 + 10.0 * 6.214117527008057
Epoch 2000, val loss: 1.1908246278762817
Epoch 2010, training loss: 62.203819274902344 = 0.022348543629050255 + 10.0 * 6.218146800994873
Epoch 2010, val loss: 1.194231629371643
Epoch 2020, training loss: 62.16897201538086 = 0.022005146369338036 + 10.0 * 6.214696407318115
Epoch 2020, val loss: 1.1970493793487549
Epoch 2030, training loss: 62.15667724609375 = 0.021658144891262054 + 10.0 * 6.213501930236816
Epoch 2030, val loss: 1.20037841796875
Epoch 2040, training loss: 62.161766052246094 = 0.021335290744900703 + 10.0 * 6.214043140411377
Epoch 2040, val loss: 1.2035338878631592
Epoch 2050, training loss: 62.14336013793945 = 0.021010572090744972 + 10.0 * 6.212234973907471
Epoch 2050, val loss: 1.2068192958831787
Epoch 2060, training loss: 62.145965576171875 = 0.020699169486761093 + 10.0 * 6.212526798248291
Epoch 2060, val loss: 1.2099971771240234
Epoch 2070, training loss: 62.16203689575195 = 0.020391590893268585 + 10.0 * 6.2141642570495605
Epoch 2070, val loss: 1.2129881381988525
Epoch 2080, training loss: 62.144676208496094 = 0.020082589238882065 + 10.0 * 6.212459087371826
Epoch 2080, val loss: 1.2159091234207153
Epoch 2090, training loss: 62.13072204589844 = 0.0197908952832222 + 10.0 * 6.211092948913574
Epoch 2090, val loss: 1.2191969156265259
Epoch 2100, training loss: 62.14938735961914 = 0.019505517557263374 + 10.0 * 6.212988376617432
Epoch 2100, val loss: 1.22224760055542
Epoch 2110, training loss: 62.16802978515625 = 0.0192177202552557 + 10.0 * 6.21488094329834
Epoch 2110, val loss: 1.2251369953155518
Epoch 2120, training loss: 62.157196044921875 = 0.01894202269613743 + 10.0 * 6.213825225830078
Epoch 2120, val loss: 1.2281265258789062
Epoch 2130, training loss: 62.15214157104492 = 0.018661167472600937 + 10.0 * 6.213347911834717
Epoch 2130, val loss: 1.2312136888504028
Epoch 2140, training loss: 62.11900329589844 = 0.018400654196739197 + 10.0 * 6.210060119628906
Epoch 2140, val loss: 1.234048843383789
Epoch 2150, training loss: 62.11625289916992 = 0.018146952614188194 + 10.0 * 6.209810733795166
Epoch 2150, val loss: 1.2370463609695435
Epoch 2160, training loss: 62.12508010864258 = 0.017899779602885246 + 10.0 * 6.210718154907227
Epoch 2160, val loss: 1.239899754524231
Epoch 2170, training loss: 62.143470764160156 = 0.017655810341238976 + 10.0 * 6.212581634521484
Epoch 2170, val loss: 1.2427167892456055
Epoch 2180, training loss: 62.13031005859375 = 0.017403079196810722 + 10.0 * 6.2112908363342285
Epoch 2180, val loss: 1.245653748512268
Epoch 2190, training loss: 62.141815185546875 = 0.017166126519441605 + 10.0 * 6.212464809417725
Epoch 2190, val loss: 1.248276710510254
Epoch 2200, training loss: 62.11040496826172 = 0.016930893063545227 + 10.0 * 6.209347724914551
Epoch 2200, val loss: 1.251247525215149
Epoch 2210, training loss: 62.1151237487793 = 0.016704633831977844 + 10.0 * 6.209841728210449
Epoch 2210, val loss: 1.253919005393982
Epoch 2220, training loss: 62.133506774902344 = 0.016485918313264847 + 10.0 * 6.2117018699646
Epoch 2220, val loss: 1.2567890882492065
Epoch 2230, training loss: 62.11117172241211 = 0.016262365505099297 + 10.0 * 6.209490776062012
Epoch 2230, val loss: 1.259642481803894
Epoch 2240, training loss: 62.100032806396484 = 0.01604640856385231 + 10.0 * 6.208398818969727
Epoch 2240, val loss: 1.2622696161270142
Epoch 2250, training loss: 62.097721099853516 = 0.015841776505112648 + 10.0 * 6.208188056945801
Epoch 2250, val loss: 1.2651348114013672
Epoch 2260, training loss: 62.13286209106445 = 0.015643903985619545 + 10.0 * 6.211721897125244
Epoch 2260, val loss: 1.2676399946212769
Epoch 2270, training loss: 62.13448715209961 = 0.01543206162750721 + 10.0 * 6.211905479431152
Epoch 2270, val loss: 1.270186424255371
Epoch 2280, training loss: 62.105159759521484 = 0.015230637043714523 + 10.0 * 6.208992958068848
Epoch 2280, val loss: 1.2731934785842896
Epoch 2290, training loss: 62.09312057495117 = 0.01503551285713911 + 10.0 * 6.207808494567871
Epoch 2290, val loss: 1.2757055759429932
Epoch 2300, training loss: 62.0914421081543 = 0.0148519491776824 + 10.0 * 6.2076592445373535
Epoch 2300, val loss: 1.2785682678222656
Epoch 2310, training loss: 62.136390686035156 = 0.014670658856630325 + 10.0 * 6.212172031402588
Epoch 2310, val loss: 1.2810512781143188
Epoch 2320, training loss: 62.11949920654297 = 0.014481920748949051 + 10.0 * 6.210501670837402
Epoch 2320, val loss: 1.2835979461669922
Epoch 2330, training loss: 62.10343933105469 = 0.014302805997431278 + 10.0 * 6.208913803100586
Epoch 2330, val loss: 1.2862741947174072
Epoch 2340, training loss: 62.09286880493164 = 0.014121823944151402 + 10.0 * 6.207874774932861
Epoch 2340, val loss: 1.2889174222946167
Epoch 2350, training loss: 62.08733367919922 = 0.013952367007732391 + 10.0 * 6.207337856292725
Epoch 2350, val loss: 1.291540265083313
Epoch 2360, training loss: 62.09872817993164 = 0.013783445581793785 + 10.0 * 6.208494663238525
Epoch 2360, val loss: 1.293930172920227
Epoch 2370, training loss: 62.08650207519531 = 0.013616395182907581 + 10.0 * 6.20728874206543
Epoch 2370, val loss: 1.2963875532150269
Epoch 2380, training loss: 62.0787239074707 = 0.013454168103635311 + 10.0 * 6.206526756286621
Epoch 2380, val loss: 1.2989845275878906
Epoch 2390, training loss: 62.099300384521484 = 0.013295028358697891 + 10.0 * 6.2086005210876465
Epoch 2390, val loss: 1.301642656326294
Epoch 2400, training loss: 62.09621047973633 = 0.013133611530065536 + 10.0 * 6.20830774307251
Epoch 2400, val loss: 1.3038517236709595
Epoch 2410, training loss: 62.07282257080078 = 0.012981681153178215 + 10.0 * 6.205984115600586
Epoch 2410, val loss: 1.3062251806259155
Epoch 2420, training loss: 62.062442779541016 = 0.012829110957682133 + 10.0 * 6.20496129989624
Epoch 2420, val loss: 1.3087042570114136
Epoch 2430, training loss: 62.06039810180664 = 0.012684405781328678 + 10.0 * 6.204771518707275
Epoch 2430, val loss: 1.3112070560455322
Epoch 2440, training loss: 62.07557678222656 = 0.012543529272079468 + 10.0 * 6.206303596496582
Epoch 2440, val loss: 1.3133983612060547
Epoch 2450, training loss: 62.08205032348633 = 0.012399718165397644 + 10.0 * 6.20696496963501
Epoch 2450, val loss: 1.3156920671463013
Epoch 2460, training loss: 62.060951232910156 = 0.012253010645508766 + 10.0 * 6.204869747161865
Epoch 2460, val loss: 1.3181754350662231
Epoch 2470, training loss: 62.06553649902344 = 0.012117736041545868 + 10.0 * 6.205341815948486
Epoch 2470, val loss: 1.3205972909927368
Epoch 2480, training loss: 62.09530258178711 = 0.011983592063188553 + 10.0 * 6.208332061767578
Epoch 2480, val loss: 1.3228720426559448
Epoch 2490, training loss: 62.079139709472656 = 0.011844921857118607 + 10.0 * 6.206729412078857
Epoch 2490, val loss: 1.3252038955688477
Epoch 2500, training loss: 62.079097747802734 = 0.011714459396898746 + 10.0 * 6.206738471984863
Epoch 2500, val loss: 1.3275178670883179
Epoch 2510, training loss: 62.06179428100586 = 0.011585338972508907 + 10.0 * 6.205020904541016
Epoch 2510, val loss: 1.3297511339187622
Epoch 2520, training loss: 62.08080291748047 = 0.011459785513579845 + 10.0 * 6.206934452056885
Epoch 2520, val loss: 1.3322021961212158
Epoch 2530, training loss: 62.05397415161133 = 0.011335703544318676 + 10.0 * 6.204263687133789
Epoch 2530, val loss: 1.3342151641845703
Epoch 2540, training loss: 62.047332763671875 = 0.011216433718800545 + 10.0 * 6.203611850738525
Epoch 2540, val loss: 1.3365010023117065
Epoch 2550, training loss: 62.05516052246094 = 0.01110039371997118 + 10.0 * 6.204405784606934
Epoch 2550, val loss: 1.338840126991272
Epoch 2560, training loss: 62.078277587890625 = 0.010984339751303196 + 10.0 * 6.206729412078857
Epoch 2560, val loss: 1.3411263227462769
Epoch 2570, training loss: 62.058738708496094 = 0.010862397029995918 + 10.0 * 6.204787254333496
Epoch 2570, val loss: 1.3431079387664795
Epoch 2580, training loss: 62.09908676147461 = 0.010747866705060005 + 10.0 * 6.208834171295166
Epoch 2580, val loss: 1.34544038772583
Epoch 2590, training loss: 62.04823303222656 = 0.010635503567755222 + 10.0 * 6.203759670257568
Epoch 2590, val loss: 1.3473880290985107
Epoch 2600, training loss: 62.04110336303711 = 0.010526061989367008 + 10.0 * 6.203057765960693
Epoch 2600, val loss: 1.3496432304382324
Epoch 2610, training loss: 62.036014556884766 = 0.010417970828711987 + 10.0 * 6.202559471130371
Epoch 2610, val loss: 1.3518177270889282
Epoch 2620, training loss: 62.04997253417969 = 0.01031458843499422 + 10.0 * 6.203965663909912
Epoch 2620, val loss: 1.3539717197418213
Epoch 2630, training loss: 62.06169891357422 = 0.010210344567894936 + 10.0 * 6.205148696899414
Epoch 2630, val loss: 1.3560562133789062
Epoch 2640, training loss: 62.03889083862305 = 0.01010251697152853 + 10.0 * 6.202878952026367
Epoch 2640, val loss: 1.3580958843231201
Epoch 2650, training loss: 62.03739929199219 = 0.010000975802540779 + 10.0 * 6.202739715576172
Epoch 2650, val loss: 1.3603630065917969
Epoch 2660, training loss: 62.02899169921875 = 0.009903675876557827 + 10.0 * 6.201909065246582
Epoch 2660, val loss: 1.3623976707458496
Epoch 2670, training loss: 62.043418884277344 = 0.009810810908675194 + 10.0 * 6.203360557556152
Epoch 2670, val loss: 1.36432945728302
Epoch 2680, training loss: 62.062477111816406 = 0.009713411331176758 + 10.0 * 6.2052764892578125
Epoch 2680, val loss: 1.3663301467895508
Epoch 2690, training loss: 62.032527923583984 = 0.009609951637685299 + 10.0 * 6.202291965484619
Epoch 2690, val loss: 1.3685225248336792
Epoch 2700, training loss: 62.02498245239258 = 0.009518716484308243 + 10.0 * 6.201546669006348
Epoch 2700, val loss: 1.3705381155014038
Epoch 2710, training loss: 62.029083251953125 = 0.009428758174180984 + 10.0 * 6.20196533203125
Epoch 2710, val loss: 1.3727957010269165
Epoch 2720, training loss: 62.06645584106445 = 0.009341230615973473 + 10.0 * 6.205711364746094
Epoch 2720, val loss: 1.3746626377105713
Epoch 2730, training loss: 62.04191207885742 = 0.009249821305274963 + 10.0 * 6.203266143798828
Epoch 2730, val loss: 1.3764514923095703
Epoch 2740, training loss: 62.04621124267578 = 0.009162233211100101 + 10.0 * 6.203704833984375
Epoch 2740, val loss: 1.3786206245422363
Epoch 2750, training loss: 62.02912902832031 = 0.009073411114513874 + 10.0 * 6.202005386352539
Epoch 2750, val loss: 1.3803815841674805
Epoch 2760, training loss: 62.044097900390625 = 0.008988930843770504 + 10.0 * 6.2035112380981445
Epoch 2760, val loss: 1.3823051452636719
Epoch 2770, training loss: 62.022682189941406 = 0.0089053800329566 + 10.0 * 6.201377868652344
Epoch 2770, val loss: 1.3843863010406494
Epoch 2780, training loss: 62.02229309082031 = 0.008822411298751831 + 10.0 * 6.2013468742370605
Epoch 2780, val loss: 1.3864307403564453
Epoch 2790, training loss: 62.04185104370117 = 0.008740602061152458 + 10.0 * 6.203310966491699
Epoch 2790, val loss: 1.3882675170898438
Epoch 2800, training loss: 62.028892517089844 = 0.008661039173603058 + 10.0 * 6.202023506164551
Epoch 2800, val loss: 1.3901041746139526
Epoch 2810, training loss: 62.014183044433594 = 0.008583426475524902 + 10.0 * 6.200560092926025
Epoch 2810, val loss: 1.392037034034729
Epoch 2820, training loss: 62.00884246826172 = 0.00850757583975792 + 10.0 * 6.200033664703369
Epoch 2820, val loss: 1.3938629627227783
Epoch 2830, training loss: 62.0429801940918 = 0.008434982039034367 + 10.0 * 6.203454494476318
Epoch 2830, val loss: 1.3956067562103271
Epoch 2840, training loss: 62.02080535888672 = 0.008355322293937206 + 10.0 * 6.201245307922363
Epoch 2840, val loss: 1.3975814580917358
Epoch 2850, training loss: 62.018436431884766 = 0.008278519846498966 + 10.0 * 6.201015949249268
Epoch 2850, val loss: 1.3994020223617554
Epoch 2860, training loss: 62.021305084228516 = 0.008206145837903023 + 10.0 * 6.201310157775879
Epoch 2860, val loss: 1.4013400077819824
Epoch 2870, training loss: 62.01443099975586 = 0.0081357192248106 + 10.0 * 6.200629234313965
Epoch 2870, val loss: 1.4028557538986206
Epoch 2880, training loss: 62.00909423828125 = 0.008064663968980312 + 10.0 * 6.200102806091309
Epoch 2880, val loss: 1.4046772718429565
Epoch 2890, training loss: 61.99930191040039 = 0.007994056679308414 + 10.0 * 6.199130535125732
Epoch 2890, val loss: 1.4066121578216553
Epoch 2900, training loss: 62.01575469970703 = 0.007928330451250076 + 10.0 * 6.200782775878906
Epoch 2900, val loss: 1.4085702896118164
Epoch 2910, training loss: 62.01588439941406 = 0.007860471494495869 + 10.0 * 6.200802326202393
Epoch 2910, val loss: 1.4103114604949951
Epoch 2920, training loss: 62.02006530761719 = 0.007792994845658541 + 10.0 * 6.201227188110352
Epoch 2920, val loss: 1.4118293523788452
Epoch 2930, training loss: 62.00904846191406 = 0.007725102361291647 + 10.0 * 6.200132369995117
Epoch 2930, val loss: 1.4136987924575806
Epoch 2940, training loss: 61.99614715576172 = 0.007657711394131184 + 10.0 * 6.198849201202393
Epoch 2940, val loss: 1.4153757095336914
Epoch 2950, training loss: 62.00156021118164 = 0.007597105111926794 + 10.0 * 6.199396133422852
Epoch 2950, val loss: 1.4171907901763916
Epoch 2960, training loss: 62.0549430847168 = 0.007538046222180128 + 10.0 * 6.204740524291992
Epoch 2960, val loss: 1.4186291694641113
Epoch 2970, training loss: 62.00969314575195 = 0.0074674515053629875 + 10.0 * 6.200222492218018
Epoch 2970, val loss: 1.420666217803955
Epoch 2980, training loss: 61.998233795166016 = 0.007405730430036783 + 10.0 * 6.199082851409912
Epoch 2980, val loss: 1.422239899635315
Epoch 2990, training loss: 61.98701477050781 = 0.0073463707230985165 + 10.0 * 6.197966575622559
Epoch 2990, val loss: 1.4240975379943848
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8323668950975225
The final CL Acc:0.76543, 0.02310, The final GNN Acc:0.83852, 0.00495
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11652])
remove edge: torch.Size([2, 9574])
updated graph: torch.Size([2, 10670])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.92988586425781 = 1.9614434242248535 + 10.0 * 8.596844673156738
Epoch 0, val loss: 1.9693734645843506
Epoch 10, training loss: 87.91242980957031 = 1.9512327909469604 + 10.0 * 8.59611988067627
Epoch 10, val loss: 1.9586842060089111
Epoch 20, training loss: 87.84074401855469 = 1.9389605522155762 + 10.0 * 8.590178489685059
Epoch 20, val loss: 1.9457381963729858
Epoch 30, training loss: 87.38011169433594 = 1.923722743988037 + 10.0 * 8.545639038085938
Epoch 30, val loss: 1.9297561645507812
Epoch 40, training loss: 84.12667083740234 = 1.906245231628418 + 10.0 * 8.22204303741455
Epoch 40, val loss: 1.9118478298187256
Epoch 50, training loss: 78.51319122314453 = 1.8872829675674438 + 10.0 * 7.662590503692627
Epoch 50, val loss: 1.8928101062774658
Epoch 60, training loss: 77.13842010498047 = 1.872241497039795 + 10.0 * 7.526618003845215
Epoch 60, val loss: 1.8784219026565552
Epoch 70, training loss: 74.8180160522461 = 1.8591333627700806 + 10.0 * 7.295888423919678
Epoch 70, val loss: 1.8660262823104858
Epoch 80, training loss: 72.45088195800781 = 1.8479037284851074 + 10.0 * 7.060297966003418
Epoch 80, val loss: 1.8551701307296753
Epoch 90, training loss: 71.1500473022461 = 1.8371291160583496 + 10.0 * 6.9312920570373535
Epoch 90, val loss: 1.844014048576355
Epoch 100, training loss: 70.1559829711914 = 1.8251880407333374 + 10.0 * 6.833079814910889
Epoch 100, val loss: 1.8323988914489746
Epoch 110, training loss: 69.45662689208984 = 1.814225673675537 + 10.0 * 6.764240264892578
Epoch 110, val loss: 1.821728229522705
Epoch 120, training loss: 68.94476318359375 = 1.8042114973068237 + 10.0 * 6.714055061340332
Epoch 120, val loss: 1.8119159936904907
Epoch 130, training loss: 68.53656005859375 = 1.7944504022598267 + 10.0 * 6.674211502075195
Epoch 130, val loss: 1.8022425174713135
Epoch 140, training loss: 68.20974731445312 = 1.7844537496566772 + 10.0 * 6.642529010772705
Epoch 140, val loss: 1.7924292087554932
Epoch 150, training loss: 67.91329193115234 = 1.7742093801498413 + 10.0 * 6.613908767700195
Epoch 150, val loss: 1.782688021659851
Epoch 160, training loss: 67.65343475341797 = 1.7635794878005981 + 10.0 * 6.588985443115234
Epoch 160, val loss: 1.7727410793304443
Epoch 170, training loss: 67.43251037597656 = 1.7522214651107788 + 10.0 * 6.568028926849365
Epoch 170, val loss: 1.7623200416564941
Epoch 180, training loss: 67.23934936523438 = 1.7399331331253052 + 10.0 * 6.549941062927246
Epoch 180, val loss: 1.751311182975769
Epoch 190, training loss: 67.02699279785156 = 1.726622223854065 + 10.0 * 6.530036926269531
Epoch 190, val loss: 1.7395904064178467
Epoch 200, training loss: 66.8586196899414 = 1.7122753858566284 + 10.0 * 6.51463508605957
Epoch 200, val loss: 1.727016806602478
Epoch 210, training loss: 66.73574829101562 = 1.696574330329895 + 10.0 * 6.503917694091797
Epoch 210, val loss: 1.7134186029434204
Epoch 220, training loss: 66.53567504882812 = 1.6794356107711792 + 10.0 * 6.485623836517334
Epoch 220, val loss: 1.6988228559494019
Epoch 230, training loss: 66.39383697509766 = 1.6609904766082764 + 10.0 * 6.47328519821167
Epoch 230, val loss: 1.6830374002456665
Epoch 240, training loss: 66.30826568603516 = 1.6410443782806396 + 10.0 * 6.46672248840332
Epoch 240, val loss: 1.6660526990890503
Epoch 250, training loss: 66.13058471679688 = 1.6193530559539795 + 10.0 * 6.451122760772705
Epoch 250, val loss: 1.6479010581970215
Epoch 260, training loss: 66.02134704589844 = 1.596366047859192 + 10.0 * 6.442498207092285
Epoch 260, val loss: 1.6287572383880615
Epoch 270, training loss: 65.92777252197266 = 1.5719752311706543 + 10.0 * 6.435579776763916
Epoch 270, val loss: 1.6086218357086182
Epoch 280, training loss: 65.80866241455078 = 1.5464081764221191 + 10.0 * 6.426225185394287
Epoch 280, val loss: 1.5876866579055786
Epoch 290, training loss: 65.7375259399414 = 1.5197551250457764 + 10.0 * 6.421777725219727
Epoch 290, val loss: 1.5660908222198486
Epoch 300, training loss: 65.61661529541016 = 1.4921905994415283 + 10.0 * 6.412442684173584
Epoch 300, val loss: 1.5441412925720215
Epoch 310, training loss: 65.55592346191406 = 1.4639122486114502 + 10.0 * 6.409201145172119
Epoch 310, val loss: 1.5220565795898438
Epoch 320, training loss: 65.44503021240234 = 1.4351305961608887 + 10.0 * 6.400990009307861
Epoch 320, val loss: 1.4998245239257812
Epoch 330, training loss: 65.36798858642578 = 1.406221628189087 + 10.0 * 6.396176815032959
Epoch 330, val loss: 1.4779309034347534
Epoch 340, training loss: 65.27217864990234 = 1.377381443977356 + 10.0 * 6.389479637145996
Epoch 340, val loss: 1.4564818143844604
Epoch 350, training loss: 65.19610595703125 = 1.3486738204956055 + 10.0 * 6.3847432136535645
Epoch 350, val loss: 1.435522198677063
Epoch 360, training loss: 65.12482452392578 = 1.320199728012085 + 10.0 * 6.380462646484375
Epoch 360, val loss: 1.4150737524032593
Epoch 370, training loss: 65.0558853149414 = 1.2921797037124634 + 10.0 * 6.376370429992676
Epoch 370, val loss: 1.3953261375427246
Epoch 380, training loss: 64.98480987548828 = 1.264832854270935 + 10.0 * 6.371997833251953
Epoch 380, val loss: 1.3764500617980957
Epoch 390, training loss: 64.92920684814453 = 1.238066554069519 + 10.0 * 6.369114398956299
Epoch 390, val loss: 1.3583797216415405
Epoch 400, training loss: 64.88801574707031 = 1.2118785381317139 + 10.0 * 6.367613792419434
Epoch 400, val loss: 1.3410907983779907
Epoch 410, training loss: 64.8179702758789 = 1.1863259077072144 + 10.0 * 6.36316442489624
Epoch 410, val loss: 1.3247730731964111
Epoch 420, training loss: 64.73505401611328 = 1.1616069078445435 + 10.0 * 6.357344627380371
Epoch 420, val loss: 1.3091652393341064
Epoch 430, training loss: 64.67903900146484 = 1.1375371217727661 + 10.0 * 6.354150295257568
Epoch 430, val loss: 1.2944468259811401
Epoch 440, training loss: 64.62401580810547 = 1.113989233970642 + 10.0 * 6.3510026931762695
Epoch 440, val loss: 1.2804583311080933
Epoch 450, training loss: 64.58060455322266 = 1.091080665588379 + 10.0 * 6.348952293395996
Epoch 450, val loss: 1.2670320272445679
Epoch 460, training loss: 64.58366394042969 = 1.0686845779418945 + 10.0 * 6.351497650146484
Epoch 460, val loss: 1.2543203830718994
Epoch 470, training loss: 64.49588012695312 = 1.0470342636108398 + 10.0 * 6.344884395599365
Epoch 470, val loss: 1.2425013780593872
Epoch 480, training loss: 64.42739868164062 = 1.0256596803665161 + 10.0 * 6.340173721313477
Epoch 480, val loss: 1.2311080694198608
Epoch 490, training loss: 64.3887710571289 = 1.0048656463623047 + 10.0 * 6.338390827178955
Epoch 490, val loss: 1.2204492092132568
Epoch 500, training loss: 64.36299133300781 = 0.984186053276062 + 10.0 * 6.337880611419678
Epoch 500, val loss: 1.2099812030792236
Epoch 510, training loss: 64.29481506347656 = 0.9640679955482483 + 10.0 * 6.333074569702148
Epoch 510, val loss: 1.2000211477279663
Epoch 520, training loss: 64.2505111694336 = 0.9441971778869629 + 10.0 * 6.330631732940674
Epoch 520, val loss: 1.1905235052108765
Epoch 530, training loss: 64.27592468261719 = 0.9246309399604797 + 10.0 * 6.335129737854004
Epoch 530, val loss: 1.1812692880630493
Epoch 540, training loss: 64.19619750976562 = 0.9053576588630676 + 10.0 * 6.3290839195251465
Epoch 540, val loss: 1.172768473625183
Epoch 550, training loss: 64.14124298095703 = 0.8862770795822144 + 10.0 * 6.325496673583984
Epoch 550, val loss: 1.1643232107162476
Epoch 560, training loss: 64.0923080444336 = 0.8674958348274231 + 10.0 * 6.322481155395508
Epoch 560, val loss: 1.156143307685852
Epoch 570, training loss: 64.06423950195312 = 0.8489236831665039 + 10.0 * 6.321531772613525
Epoch 570, val loss: 1.1482481956481934
Epoch 580, training loss: 64.05225372314453 = 0.8305489420890808 + 10.0 * 6.322170257568359
Epoch 580, val loss: 1.1410086154937744
Epoch 590, training loss: 64.00833892822266 = 0.8123689889907837 + 10.0 * 6.319596767425537
Epoch 590, val loss: 1.133525013923645
Epoch 600, training loss: 63.94780731201172 = 0.7946150302886963 + 10.0 * 6.315319061279297
Epoch 600, val loss: 1.1265053749084473
Epoch 610, training loss: 63.91926574707031 = 0.7770847082138062 + 10.0 * 6.314218044281006
Epoch 610, val loss: 1.120041847229004
Epoch 620, training loss: 63.94470977783203 = 0.7598028779029846 + 10.0 * 6.318490505218506
Epoch 620, val loss: 1.113518476486206
Epoch 630, training loss: 63.8796272277832 = 0.7425950169563293 + 10.0 * 6.3137030601501465
Epoch 630, val loss: 1.1074036359786987
Epoch 640, training loss: 63.858795166015625 = 0.7257303595542908 + 10.0 * 6.3133063316345215
Epoch 640, val loss: 1.1015554666519165
Epoch 650, training loss: 63.79161071777344 = 0.7091894745826721 + 10.0 * 6.308241844177246
Epoch 650, val loss: 1.0959583520889282
Epoch 660, training loss: 63.759979248046875 = 0.6928087472915649 + 10.0 * 6.3067169189453125
Epoch 660, val loss: 1.0905132293701172
Epoch 670, training loss: 63.72966003417969 = 0.6767157316207886 + 10.0 * 6.305294513702393
Epoch 670, val loss: 1.0856101512908936
Epoch 680, training loss: 63.74293518066406 = 0.6607123017311096 + 10.0 * 6.30822229385376
Epoch 680, val loss: 1.0807386636734009
Epoch 690, training loss: 63.715492248535156 = 0.6448104977607727 + 10.0 * 6.307068347930908
Epoch 690, val loss: 1.0761243104934692
Epoch 700, training loss: 63.653831481933594 = 0.6289767622947693 + 10.0 * 6.302485466003418
Epoch 700, val loss: 1.0717254877090454
Epoch 710, training loss: 63.61929702758789 = 0.6133162379264832 + 10.0 * 6.30059814453125
Epoch 710, val loss: 1.0672062635421753
Epoch 720, training loss: 63.58211898803711 = 0.5978214144706726 + 10.0 * 6.2984299659729
Epoch 720, val loss: 1.0632734298706055
Epoch 730, training loss: 63.63039016723633 = 0.5823632478713989 + 10.0 * 6.304802894592285
Epoch 730, val loss: 1.0594961643218994
Epoch 740, training loss: 63.5810661315918 = 0.566885232925415 + 10.0 * 6.301417827606201
Epoch 740, val loss: 1.0553321838378906
Epoch 750, training loss: 63.5008544921875 = 0.5515011548995972 + 10.0 * 6.29493522644043
Epoch 750, val loss: 1.0515880584716797
Epoch 760, training loss: 63.48021697998047 = 0.536191463470459 + 10.0 * 6.294402599334717
Epoch 760, val loss: 1.0480730533599854
Epoch 770, training loss: 63.44802474975586 = 0.520949125289917 + 10.0 * 6.292707443237305
Epoch 770, val loss: 1.0446668863296509
Epoch 780, training loss: 63.47165298461914 = 0.5058037042617798 + 10.0 * 6.2965850830078125
Epoch 780, val loss: 1.0414445400238037
Epoch 790, training loss: 63.47692108154297 = 0.4904712438583374 + 10.0 * 6.29864501953125
Epoch 790, val loss: 1.0383508205413818
Epoch 800, training loss: 63.38092803955078 = 0.4752051830291748 + 10.0 * 6.290572166442871
Epoch 800, val loss: 1.0348289012908936
Epoch 810, training loss: 63.34953308105469 = 0.4601619839668274 + 10.0 * 6.288937091827393
Epoch 810, val loss: 1.032129168510437
Epoch 820, training loss: 63.324581146240234 = 0.44519248604774475 + 10.0 * 6.287938594818115
Epoch 820, val loss: 1.0294032096862793
Epoch 830, training loss: 63.371803283691406 = 0.4302908480167389 + 10.0 * 6.294151306152344
Epoch 830, val loss: 1.0265992879867554
Epoch 840, training loss: 63.29169464111328 = 0.41543275117874146 + 10.0 * 6.287626266479492
Epoch 840, val loss: 1.024552822113037
Epoch 850, training loss: 63.24774169921875 = 0.4008638858795166 + 10.0 * 6.2846879959106445
Epoch 850, val loss: 1.0226649045944214
Epoch 860, training loss: 63.22542190551758 = 0.3864942193031311 + 10.0 * 6.283892631530762
Epoch 860, val loss: 1.0207629203796387
Epoch 870, training loss: 63.30702209472656 = 0.37241965532302856 + 10.0 * 6.293459892272949
Epoch 870, val loss: 1.0194005966186523
Epoch 880, training loss: 63.20968246459961 = 0.3583313822746277 + 10.0 * 6.285135269165039
Epoch 880, val loss: 1.0179017782211304
Epoch 890, training loss: 63.15472412109375 = 0.3447573482990265 + 10.0 * 6.280996799468994
Epoch 890, val loss: 1.0169203281402588
Epoch 900, training loss: 63.13822937011719 = 0.3315231204032898 + 10.0 * 6.280670642852783
Epoch 900, val loss: 1.0162980556488037
Epoch 910, training loss: 63.1981086730957 = 0.31856951117515564 + 10.0 * 6.287953853607178
Epoch 910, val loss: 1.0159999132156372
Epoch 920, training loss: 63.107486724853516 = 0.3060418367385864 + 10.0 * 6.280144691467285
Epoch 920, val loss: 1.0162700414657593
Epoch 930, training loss: 63.075931549072266 = 0.2938496172428131 + 10.0 * 6.278208255767822
Epoch 930, val loss: 1.0168102979660034
Epoch 940, training loss: 63.05036163330078 = 0.2821461856365204 + 10.0 * 6.276821613311768
Epoch 940, val loss: 1.017706274986267
Epoch 950, training loss: 63.07310485839844 = 0.2708703875541687 + 10.0 * 6.280223369598389
Epoch 950, val loss: 1.0194357633590698
Epoch 960, training loss: 63.062232971191406 = 0.2598627209663391 + 10.0 * 6.280237197875977
Epoch 960, val loss: 1.0203050374984741
Epoch 970, training loss: 63.00752639770508 = 0.24936750531196594 + 10.0 * 6.275815963745117
Epoch 970, val loss: 1.0225721597671509
Epoch 980, training loss: 62.98414611816406 = 0.2393004298210144 + 10.0 * 6.274484634399414
Epoch 980, val loss: 1.024992823600769
Epoch 990, training loss: 62.98422622680664 = 0.22968560457229614 + 10.0 * 6.275454044342041
Epoch 990, val loss: 1.0273652076721191
Epoch 1000, training loss: 62.98280334472656 = 0.2204466015100479 + 10.0 * 6.276235580444336
Epoch 1000, val loss: 1.0305505990982056
Epoch 1010, training loss: 62.93092346191406 = 0.21156305074691772 + 10.0 * 6.271935939788818
Epoch 1010, val loss: 1.0334547758102417
Epoch 1020, training loss: 62.92313766479492 = 0.20314307510852814 + 10.0 * 6.271999359130859
Epoch 1020, val loss: 1.0371736288070679
Epoch 1030, training loss: 62.93904113769531 = 0.1951342672109604 + 10.0 * 6.274390697479248
Epoch 1030, val loss: 1.040717363357544
Epoch 1040, training loss: 62.89293670654297 = 0.187381312251091 + 10.0 * 6.27055549621582
Epoch 1040, val loss: 1.0446046590805054
Epoch 1050, training loss: 62.888065338134766 = 0.18007177114486694 + 10.0 * 6.270799160003662
Epoch 1050, val loss: 1.0486682653427124
Epoch 1060, training loss: 62.87565231323242 = 0.1730746328830719 + 10.0 * 6.270257949829102
Epoch 1060, val loss: 1.0530577898025513
Epoch 1070, training loss: 62.84840774536133 = 0.1664080023765564 + 10.0 * 6.268199920654297
Epoch 1070, val loss: 1.0572277307510376
Epoch 1080, training loss: 62.83439636230469 = 0.1600622534751892 + 10.0 * 6.2674336433410645
Epoch 1080, val loss: 1.06182861328125
Epoch 1090, training loss: 62.836524963378906 = 0.1540391594171524 + 10.0 * 6.268248558044434
Epoch 1090, val loss: 1.0666264295578003
Epoch 1100, training loss: 62.819793701171875 = 0.14825358986854553 + 10.0 * 6.267153739929199
Epoch 1100, val loss: 1.071543574333191
Epoch 1110, training loss: 62.84633255004883 = 0.14275839924812317 + 10.0 * 6.270357608795166
Epoch 1110, val loss: 1.0769652128219604
Epoch 1120, training loss: 62.794124603271484 = 0.13744811713695526 + 10.0 * 6.265667915344238
Epoch 1120, val loss: 1.081554651260376
Epoch 1130, training loss: 62.7729377746582 = 0.13244807720184326 + 10.0 * 6.264049053192139
Epoch 1130, val loss: 1.0869296789169312
Epoch 1140, training loss: 62.76613998413086 = 0.1276898831129074 + 10.0 * 6.263844966888428
Epoch 1140, val loss: 1.0927085876464844
Epoch 1150, training loss: 62.86117935180664 = 0.12310425937175751 + 10.0 * 6.273807525634766
Epoch 1150, val loss: 1.0976773500442505
Epoch 1160, training loss: 62.77729415893555 = 0.1187620460987091 + 10.0 * 6.265852928161621
Epoch 1160, val loss: 1.1033236980438232
Epoch 1170, training loss: 62.73474884033203 = 0.11455490440130234 + 10.0 * 6.262019157409668
Epoch 1170, val loss: 1.1089675426483154
Epoch 1180, training loss: 62.729042053222656 = 0.11059889197349548 + 10.0 * 6.261844158172607
Epoch 1180, val loss: 1.1145312786102295
Epoch 1190, training loss: 62.73378372192383 = 0.10678780823945999 + 10.0 * 6.262699604034424
Epoch 1190, val loss: 1.1203746795654297
Epoch 1200, training loss: 62.70974349975586 = 0.10311880707740784 + 10.0 * 6.26066255569458
Epoch 1200, val loss: 1.1263319253921509
Epoch 1210, training loss: 62.77479934692383 = 0.09966515004634857 + 10.0 * 6.267513275146484
Epoch 1210, val loss: 1.1323750019073486
Epoch 1220, training loss: 62.753700256347656 = 0.09623879939317703 + 10.0 * 6.265746116638184
Epoch 1220, val loss: 1.1368745565414429
Epoch 1230, training loss: 62.679283142089844 = 0.09304656833410263 + 10.0 * 6.2586236000061035
Epoch 1230, val loss: 1.1430433988571167
Epoch 1240, training loss: 62.66505813598633 = 0.08998499810695648 + 10.0 * 6.25750732421875
Epoch 1240, val loss: 1.1491190195083618
Epoch 1250, training loss: 62.675113677978516 = 0.08705125004053116 + 10.0 * 6.258806228637695
Epoch 1250, val loss: 1.1546401977539062
Epoch 1260, training loss: 62.67057800292969 = 0.08423616737127304 + 10.0 * 6.258634090423584
Epoch 1260, val loss: 1.1606618165969849
Epoch 1270, training loss: 62.65586853027344 = 0.0815291628241539 + 10.0 * 6.257433891296387
Epoch 1270, val loss: 1.1661487817764282
Epoch 1280, training loss: 62.63747787475586 = 0.07892602682113647 + 10.0 * 6.255855083465576
Epoch 1280, val loss: 1.171568512916565
Epoch 1290, training loss: 62.74148178100586 = 0.07645167410373688 + 10.0 * 6.26650333404541
Epoch 1290, val loss: 1.1769577264785767
Epoch 1300, training loss: 62.6572151184082 = 0.07406126707792282 + 10.0 * 6.258315086364746
Epoch 1300, val loss: 1.1830799579620361
Epoch 1310, training loss: 62.6151008605957 = 0.07175932079553604 + 10.0 * 6.254334449768066
Epoch 1310, val loss: 1.1885476112365723
Epoch 1320, training loss: 62.638282775878906 = 0.06957841664552689 + 10.0 * 6.256870269775391
Epoch 1320, val loss: 1.1941800117492676
Epoch 1330, training loss: 62.618865966796875 = 0.0674591213464737 + 10.0 * 6.255140781402588
Epoch 1330, val loss: 1.1999375820159912
Epoch 1340, training loss: 62.596656799316406 = 0.06542842835187912 + 10.0 * 6.253122806549072
Epoch 1340, val loss: 1.205798864364624
Epoch 1350, training loss: 62.59246063232422 = 0.06348758935928345 + 10.0 * 6.252897262573242
Epoch 1350, val loss: 1.2116186618804932
Epoch 1360, training loss: 62.602420806884766 = 0.06162266805768013 + 10.0 * 6.254079818725586
Epoch 1360, val loss: 1.217015027999878
Epoch 1370, training loss: 62.598751068115234 = 0.05983675643801689 + 10.0 * 6.253891468048096
Epoch 1370, val loss: 1.2225559949874878
Epoch 1380, training loss: 62.58080291748047 = 0.05811314657330513 + 10.0 * 6.2522687911987305
Epoch 1380, val loss: 1.2282980680465698
Epoch 1390, training loss: 62.59287643432617 = 0.05645414814352989 + 10.0 * 6.2536420822143555
Epoch 1390, val loss: 1.2342561483383179
Epoch 1400, training loss: 62.56863021850586 = 0.05485476553440094 + 10.0 * 6.251377582550049
Epoch 1400, val loss: 1.2396224737167358
Epoch 1410, training loss: 62.554161071777344 = 0.05329907312989235 + 10.0 * 6.250086307525635
Epoch 1410, val loss: 1.2453336715698242
Epoch 1420, training loss: 62.545597076416016 = 0.05183154717087746 + 10.0 * 6.2493767738342285
Epoch 1420, val loss: 1.2508009672164917
Epoch 1430, training loss: 62.58612060546875 = 0.05041838809847832 + 10.0 * 6.253570079803467
Epoch 1430, val loss: 1.2558602094650269
Epoch 1440, training loss: 62.5509033203125 = 0.0490429550409317 + 10.0 * 6.250185966491699
Epoch 1440, val loss: 1.262346625328064
Epoch 1450, training loss: 62.5608024597168 = 0.04771901294589043 + 10.0 * 6.251308441162109
Epoch 1450, val loss: 1.267616868019104
Epoch 1460, training loss: 62.53544235229492 = 0.046426933258771896 + 10.0 * 6.2489013671875
Epoch 1460, val loss: 1.2732689380645752
Epoch 1470, training loss: 62.51329040527344 = 0.045204032212495804 + 10.0 * 6.2468085289001465
Epoch 1470, val loss: 1.2785145044326782
Epoch 1480, training loss: 62.52164077758789 = 0.04403923079371452 + 10.0 * 6.247759819030762
Epoch 1480, val loss: 1.2836815118789673
Epoch 1490, training loss: 62.55302047729492 = 0.04290366545319557 + 10.0 * 6.251011848449707
Epoch 1490, val loss: 1.2894254922866821
Epoch 1500, training loss: 62.53779220581055 = 0.041789717972278595 + 10.0 * 6.249600410461426
Epoch 1500, val loss: 1.2946858406066895
Epoch 1510, training loss: 62.508941650390625 = 0.04073699563741684 + 10.0 * 6.246820449829102
Epoch 1510, val loss: 1.3000675439834595
Epoch 1520, training loss: 62.52246856689453 = 0.03971768170595169 + 10.0 * 6.248274803161621
Epoch 1520, val loss: 1.3052711486816406
Epoch 1530, training loss: 62.49622344970703 = 0.038726963102817535 + 10.0 * 6.245749473571777
Epoch 1530, val loss: 1.3106153011322021
Epoch 1540, training loss: 62.5342903137207 = 0.03776728734374046 + 10.0 * 6.24965238571167
Epoch 1540, val loss: 1.315486192703247
Epoch 1550, training loss: 62.50634002685547 = 0.03685060143470764 + 10.0 * 6.246949195861816
Epoch 1550, val loss: 1.321029782295227
Epoch 1560, training loss: 62.48281478881836 = 0.03595900163054466 + 10.0 * 6.244685649871826
Epoch 1560, val loss: 1.326024055480957
Epoch 1570, training loss: 62.46921920776367 = 0.03510328009724617 + 10.0 * 6.243411540985107
Epoch 1570, val loss: 1.3311718702316284
Epoch 1580, training loss: 62.45861053466797 = 0.03428482264280319 + 10.0 * 6.242432594299316
Epoch 1580, val loss: 1.3362916707992554
Epoch 1590, training loss: 62.50849914550781 = 0.03350204974412918 + 10.0 * 6.247499942779541
Epoch 1590, val loss: 1.341681957244873
Epoch 1600, training loss: 62.51033401489258 = 0.03270583227276802 + 10.0 * 6.247762680053711
Epoch 1600, val loss: 1.346274733543396
Epoch 1610, training loss: 62.47804260253906 = 0.03194962441921234 + 10.0 * 6.244609355926514
Epoch 1610, val loss: 1.3511849641799927
Epoch 1620, training loss: 62.446083068847656 = 0.03122030384838581 + 10.0 * 6.241486549377441
Epoch 1620, val loss: 1.3564611673355103
Epoch 1630, training loss: 62.4439697265625 = 0.030525295063853264 + 10.0 * 6.241344451904297
Epoch 1630, val loss: 1.361193299293518
Epoch 1640, training loss: 62.50509262084961 = 0.029854681342840195 + 10.0 * 6.247523784637451
Epoch 1640, val loss: 1.3666871786117554
Epoch 1650, training loss: 62.45744323730469 = 0.029202233999967575 + 10.0 * 6.242824077606201
Epoch 1650, val loss: 1.3708702325820923
Epoch 1660, training loss: 62.459049224853516 = 0.028561191633343697 + 10.0 * 6.243048667907715
Epoch 1660, val loss: 1.3759324550628662
Epoch 1670, training loss: 62.4519157409668 = 0.027941856533288956 + 10.0 * 6.242397308349609
Epoch 1670, val loss: 1.3802064657211304
Epoch 1680, training loss: 62.426292419433594 = 0.02734038420021534 + 10.0 * 6.239895343780518
Epoch 1680, val loss: 1.3850507736206055
Epoch 1690, training loss: 62.422786712646484 = 0.02677035890519619 + 10.0 * 6.2396016120910645
Epoch 1690, val loss: 1.3896511793136597
Epoch 1700, training loss: 62.46645736694336 = 0.026216018944978714 + 10.0 * 6.244024276733398
Epoch 1700, val loss: 1.3946325778961182
Epoch 1710, training loss: 62.431365966796875 = 0.025668837130069733 + 10.0 * 6.240569591522217
Epoch 1710, val loss: 1.3991209268569946
Epoch 1720, training loss: 62.42104721069336 = 0.02513338252902031 + 10.0 * 6.239591121673584
Epoch 1720, val loss: 1.4037169218063354
Epoch 1730, training loss: 62.41712188720703 = 0.024626076221466064 + 10.0 * 6.239249229431152
Epoch 1730, val loss: 1.4082589149475098
Epoch 1740, training loss: 62.42002868652344 = 0.024132786318659782 + 10.0 * 6.239589691162109
Epoch 1740, val loss: 1.4130041599273682
Epoch 1750, training loss: 62.41237258911133 = 0.023652896285057068 + 10.0 * 6.238872051239014
Epoch 1750, val loss: 1.4174596071243286
Epoch 1760, training loss: 62.453250885009766 = 0.023180337622761726 + 10.0 * 6.243006706237793
Epoch 1760, val loss: 1.4213160276412964
Epoch 1770, training loss: 62.42615509033203 = 0.022729193791747093 + 10.0 * 6.240342617034912
Epoch 1770, val loss: 1.4260210990905762
Epoch 1780, training loss: 62.40937805175781 = 0.022271791473031044 + 10.0 * 6.238710880279541
Epoch 1780, val loss: 1.4300910234451294
Epoch 1790, training loss: 62.3853645324707 = 0.021853767335414886 + 10.0 * 6.236351013183594
Epoch 1790, val loss: 1.4350322484970093
Epoch 1800, training loss: 62.38325119018555 = 0.021444283425807953 + 10.0 * 6.236180782318115
Epoch 1800, val loss: 1.4394831657409668
Epoch 1810, training loss: 62.44561767578125 = 0.021050604060292244 + 10.0 * 6.242456912994385
Epoch 1810, val loss: 1.444035291671753
Epoch 1820, training loss: 62.44090270996094 = 0.020646564662456512 + 10.0 * 6.242025852203369
Epoch 1820, val loss: 1.4472293853759766
Epoch 1830, training loss: 62.38373947143555 = 0.020258605480194092 + 10.0 * 6.2363481521606445
Epoch 1830, val loss: 1.4519822597503662
Epoch 1840, training loss: 62.375091552734375 = 0.01988816075026989 + 10.0 * 6.235520362854004
Epoch 1840, val loss: 1.4557840824127197
Epoch 1850, training loss: 62.361263275146484 = 0.019533244892954826 + 10.0 * 6.234172821044922
Epoch 1850, val loss: 1.4601731300354004
Epoch 1860, training loss: 62.3585205078125 = 0.019189320504665375 + 10.0 * 6.233933448791504
Epoch 1860, val loss: 1.4642795324325562
Epoch 1870, training loss: 62.47090148925781 = 0.018867161124944687 + 10.0 * 6.245203495025635
Epoch 1870, val loss: 1.468050479888916
Epoch 1880, training loss: 62.429290771484375 = 0.018513187766075134 + 10.0 * 6.241077899932861
Epoch 1880, val loss: 1.4720463752746582
Epoch 1890, training loss: 62.36431121826172 = 0.018182015046477318 + 10.0 * 6.234612941741943
Epoch 1890, val loss: 1.476261854171753
Epoch 1900, training loss: 62.37663650512695 = 0.017871951684355736 + 10.0 * 6.235876560211182
Epoch 1900, val loss: 1.4803495407104492
Epoch 1910, training loss: 62.368194580078125 = 0.01756114512681961 + 10.0 * 6.235063076019287
Epoch 1910, val loss: 1.4842954874038696
Epoch 1920, training loss: 62.35502243041992 = 0.017264321446418762 + 10.0 * 6.233775615692139
Epoch 1920, val loss: 1.4880173206329346
Epoch 1930, training loss: 62.34531021118164 = 0.01697915978729725 + 10.0 * 6.232832908630371
Epoch 1930, val loss: 1.4920566082000732
Epoch 1940, training loss: 62.45444107055664 = 0.016698962077498436 + 10.0 * 6.243773937225342
Epoch 1940, val loss: 1.4959447383880615
Epoch 1950, training loss: 62.401004791259766 = 0.016417276114225388 + 10.0 * 6.238458633422852
Epoch 1950, val loss: 1.499181866645813
Epoch 1960, training loss: 62.34428787231445 = 0.016142336651682854 + 10.0 * 6.232814311981201
Epoch 1960, val loss: 1.5034866333007812
Epoch 1970, training loss: 62.330238342285156 = 0.015883196145296097 + 10.0 * 6.231435298919678
Epoch 1970, val loss: 1.5073524713516235
Epoch 1980, training loss: 62.34241485595703 = 0.015634838491678238 + 10.0 * 6.232677936553955
Epoch 1980, val loss: 1.511379599571228
Epoch 1990, training loss: 62.363529205322266 = 0.015386676415801048 + 10.0 * 6.234814167022705
Epoch 1990, val loss: 1.5148414373397827
Epoch 2000, training loss: 62.33755874633789 = 0.015139522962272167 + 10.0 * 6.232241630554199
Epoch 2000, val loss: 1.5177974700927734
Epoch 2010, training loss: 62.387657165527344 = 0.014900949783623219 + 10.0 * 6.23727560043335
Epoch 2010, val loss: 1.5212663412094116
Epoch 2020, training loss: 62.35637664794922 = 0.014667765237390995 + 10.0 * 6.234170913696289
Epoch 2020, val loss: 1.5260250568389893
Epoch 2030, training loss: 62.32448959350586 = 0.014438984915614128 + 10.0 * 6.2310051918029785
Epoch 2030, val loss: 1.5287576913833618
Epoch 2040, training loss: 62.32815933227539 = 0.014223605394363403 + 10.0 * 6.231393337249756
Epoch 2040, val loss: 1.5330040454864502
Epoch 2050, training loss: 62.347843170166016 = 0.014008632861077785 + 10.0 * 6.233383655548096
Epoch 2050, val loss: 1.5361343622207642
Epoch 2060, training loss: 62.343467712402344 = 0.0137972766533494 + 10.0 * 6.232966899871826
Epoch 2060, val loss: 1.5394282341003418
Epoch 2070, training loss: 62.30674362182617 = 0.013593153096735477 + 10.0 * 6.229315280914307
Epoch 2070, val loss: 1.5431756973266602
Epoch 2080, training loss: 62.324893951416016 = 0.013395518995821476 + 10.0 * 6.231149673461914
Epoch 2080, val loss: 1.5465830564498901
Epoch 2090, training loss: 62.32914733886719 = 0.01319956686347723 + 10.0 * 6.231595039367676
Epoch 2090, val loss: 1.5497889518737793
Epoch 2100, training loss: 62.32122802734375 = 0.013007503934204578 + 10.0 * 6.2308220863342285
Epoch 2100, val loss: 1.5532793998718262
Epoch 2110, training loss: 62.31224822998047 = 0.012823013588786125 + 10.0 * 6.229942798614502
Epoch 2110, val loss: 1.5564600229263306
Epoch 2120, training loss: 62.307186126708984 = 0.01264180988073349 + 10.0 * 6.229454517364502
Epoch 2120, val loss: 1.5604884624481201
Epoch 2130, training loss: 62.33637237548828 = 0.0124617675319314 + 10.0 * 6.232390880584717
Epoch 2130, val loss: 1.5631418228149414
Epoch 2140, training loss: 62.29488754272461 = 0.012286250479519367 + 10.0 * 6.228260040283203
Epoch 2140, val loss: 1.5663162469863892
Epoch 2150, training loss: 62.290138244628906 = 0.01211670134216547 + 10.0 * 6.227802276611328
Epoch 2150, val loss: 1.5695375204086304
Epoch 2160, training loss: 62.35210418701172 = 0.011954290792346 + 10.0 * 6.234014987945557
Epoch 2160, val loss: 1.572674036026001
Epoch 2170, training loss: 62.30531311035156 = 0.011781364679336548 + 10.0 * 6.229353427886963
Epoch 2170, val loss: 1.5755366086959839
Epoch 2180, training loss: 62.27961349487305 = 0.011623389087617397 + 10.0 * 6.226799011230469
Epoch 2180, val loss: 1.579355239868164
Epoch 2190, training loss: 62.27532196044922 = 0.011470292694866657 + 10.0 * 6.226385116577148
Epoch 2190, val loss: 1.5822340250015259
Epoch 2200, training loss: 62.28865432739258 = 0.011320403777062893 + 10.0 * 6.227733135223389
Epoch 2200, val loss: 1.58547842502594
Epoch 2210, training loss: 62.31349563598633 = 0.011170432902872562 + 10.0 * 6.2302327156066895
Epoch 2210, val loss: 1.5880634784698486
Epoch 2220, training loss: 62.329410552978516 = 0.011021967977285385 + 10.0 * 6.231839179992676
Epoch 2220, val loss: 1.591503381729126
Epoch 2230, training loss: 62.28214645385742 = 0.010878113098442554 + 10.0 * 6.227126598358154
Epoch 2230, val loss: 1.594347596168518
Epoch 2240, training loss: 62.30909729003906 = 0.010741307400166988 + 10.0 * 6.229835510253906
Epoch 2240, val loss: 1.597826600074768
Epoch 2250, training loss: 62.27396774291992 = 0.010599144734442234 + 10.0 * 6.226336479187012
Epoch 2250, val loss: 1.600659728050232
Epoch 2260, training loss: 62.2734489440918 = 0.01046246662735939 + 10.0 * 6.2262983322143555
Epoch 2260, val loss: 1.6030241250991821
Epoch 2270, training loss: 62.26155471801758 = 0.010334080085158348 + 10.0 * 6.225121974945068
Epoch 2270, val loss: 1.6064833402633667
Epoch 2280, training loss: 62.34187316894531 = 0.010207906365394592 + 10.0 * 6.233166694641113
Epoch 2280, val loss: 1.6094435453414917
Epoch 2290, training loss: 62.27328109741211 = 0.01007935218513012 + 10.0 * 6.226320266723633
Epoch 2290, val loss: 1.6115108728408813
Epoch 2300, training loss: 62.262752532958984 = 0.009953977540135384 + 10.0 * 6.225279808044434
Epoch 2300, val loss: 1.6146308183670044
Epoch 2310, training loss: 62.27242660522461 = 0.00983395241200924 + 10.0 * 6.226259231567383
Epoch 2310, val loss: 1.6173336505889893
Epoch 2320, training loss: 62.276214599609375 = 0.009713022969663143 + 10.0 * 6.226650238037109
Epoch 2320, val loss: 1.6203852891921997
Epoch 2330, training loss: 62.27090835571289 = 0.009597302414476871 + 10.0 * 6.226130962371826
Epoch 2330, val loss: 1.6232718229293823
Epoch 2340, training loss: 62.265445709228516 = 0.009485716000199318 + 10.0 * 6.225595951080322
Epoch 2340, val loss: 1.6255491971969604
Epoch 2350, training loss: 62.27252197265625 = 0.009370946325361729 + 10.0 * 6.226315021514893
Epoch 2350, val loss: 1.6283290386199951
Epoch 2360, training loss: 62.250152587890625 = 0.00926135666668415 + 10.0 * 6.2240891456604
Epoch 2360, val loss: 1.6313682794570923
Epoch 2370, training loss: 62.286312103271484 = 0.00915901456028223 + 10.0 * 6.227715492248535
Epoch 2370, val loss: 1.6341116428375244
Epoch 2380, training loss: 62.24700927734375 = 0.00904967449605465 + 10.0 * 6.2237958908081055
Epoch 2380, val loss: 1.6367201805114746
Epoch 2390, training loss: 62.23600387573242 = 0.00894657801836729 + 10.0 * 6.222705841064453
Epoch 2390, val loss: 1.6393941640853882
Epoch 2400, training loss: 62.24949645996094 = 0.008849645033478737 + 10.0 * 6.224064826965332
Epoch 2400, val loss: 1.6423330307006836
Epoch 2410, training loss: 62.29323959350586 = 0.00875381100922823 + 10.0 * 6.228448390960693
Epoch 2410, val loss: 1.6450402736663818
Epoch 2420, training loss: 62.26886749267578 = 0.008645783178508282 + 10.0 * 6.226022243499756
Epoch 2420, val loss: 1.646891713142395
Epoch 2430, training loss: 62.23528289794922 = 0.008552104234695435 + 10.0 * 6.222672939300537
Epoch 2430, val loss: 1.6497676372528076
Epoch 2440, training loss: 62.2257080078125 = 0.00845783855766058 + 10.0 * 6.221724987030029
Epoch 2440, val loss: 1.6522901058197021
Epoch 2450, training loss: 62.232547760009766 = 0.008367051370441914 + 10.0 * 6.222418308258057
Epoch 2450, val loss: 1.6549721956253052
Epoch 2460, training loss: 62.293113708496094 = 0.00827835313975811 + 10.0 * 6.2284836769104
Epoch 2460, val loss: 1.6573967933654785
Epoch 2470, training loss: 62.26587677001953 = 0.008192086592316628 + 10.0 * 6.225768089294434
Epoch 2470, val loss: 1.6593068838119507
Epoch 2480, training loss: 62.25350570678711 = 0.008097865618765354 + 10.0 * 6.224540710449219
Epoch 2480, val loss: 1.6617944240570068
Epoch 2490, training loss: 62.260986328125 = 0.00801222026348114 + 10.0 * 6.225297451019287
Epoch 2490, val loss: 1.664064645767212
Epoch 2500, training loss: 62.24238967895508 = 0.00792734045535326 + 10.0 * 6.223446369171143
Epoch 2500, val loss: 1.6673495769500732
Epoch 2510, training loss: 62.21669387817383 = 0.007845125161111355 + 10.0 * 6.220884799957275
Epoch 2510, val loss: 1.6688669919967651
Epoch 2520, training loss: 62.212249755859375 = 0.007765366230159998 + 10.0 * 6.2204484939575195
Epoch 2520, val loss: 1.671640396118164
Epoch 2530, training loss: 62.21185302734375 = 0.0076889339834451675 + 10.0 * 6.220416069030762
Epoch 2530, val loss: 1.6739006042480469
Epoch 2540, training loss: 62.2775764465332 = 0.007616197224706411 + 10.0 * 6.226995944976807
Epoch 2540, val loss: 1.6758246421813965
Epoch 2550, training loss: 62.232627868652344 = 0.00753204757347703 + 10.0 * 6.222509860992432
Epoch 2550, val loss: 1.6786373853683472
Epoch 2560, training loss: 62.252708435058594 = 0.007455534767359495 + 10.0 * 6.224525451660156
Epoch 2560, val loss: 1.6802939176559448
Epoch 2570, training loss: 62.231407165527344 = 0.0073781926184892654 + 10.0 * 6.222403049468994
Epoch 2570, val loss: 1.682613492012024
Epoch 2580, training loss: 62.21862030029297 = 0.007308737374842167 + 10.0 * 6.221131324768066
Epoch 2580, val loss: 1.6852171421051025
Epoch 2590, training loss: 62.202308654785156 = 0.00723522063344717 + 10.0 * 6.219507217407227
Epoch 2590, val loss: 1.6874216794967651
Epoch 2600, training loss: 62.19945526123047 = 0.007168802432715893 + 10.0 * 6.219228744506836
Epoch 2600, val loss: 1.6898306608200073
Epoch 2610, training loss: 62.29905319213867 = 0.0071024587377905846 + 10.0 * 6.2291951179504395
Epoch 2610, val loss: 1.6923569440841675
Epoch 2620, training loss: 62.22439193725586 = 0.007028370164334774 + 10.0 * 6.221736431121826
Epoch 2620, val loss: 1.6940491199493408
Epoch 2630, training loss: 62.19987106323242 = 0.006961034145206213 + 10.0 * 6.219290733337402
Epoch 2630, val loss: 1.6964198350906372
Epoch 2640, training loss: 62.20600891113281 = 0.0068965996615588665 + 10.0 * 6.219911098480225
Epoch 2640, val loss: 1.6981548070907593
Epoch 2650, training loss: 62.22340774536133 = 0.006831551436334848 + 10.0 * 6.221657752990723
Epoch 2650, val loss: 1.7003804445266724
Epoch 2660, training loss: 62.259178161621094 = 0.006766973994672298 + 10.0 * 6.225241184234619
Epoch 2660, val loss: 1.7018249034881592
Epoch 2670, training loss: 62.19443893432617 = 0.006702505052089691 + 10.0 * 6.21877384185791
Epoch 2670, val loss: 1.7046492099761963
Epoch 2680, training loss: 62.181888580322266 = 0.00664098747074604 + 10.0 * 6.217524528503418
Epoch 2680, val loss: 1.7064961194992065
Epoch 2690, training loss: 62.181785583496094 = 0.006583900190889835 + 10.0 * 6.217520236968994
Epoch 2690, val loss: 1.7089266777038574
Epoch 2700, training loss: 62.24293518066406 = 0.006530617363750935 + 10.0 * 6.223640441894531
Epoch 2700, val loss: 1.7108312845230103
Epoch 2710, training loss: 62.1786994934082 = 0.006464247591793537 + 10.0 * 6.217223167419434
Epoch 2710, val loss: 1.7128437757492065
Epoch 2720, training loss: 62.188995361328125 = 0.0064081051386892796 + 10.0 * 6.218258857727051
Epoch 2720, val loss: 1.714870572090149
Epoch 2730, training loss: 62.250526428222656 = 0.006353769917041063 + 10.0 * 6.224417209625244
Epoch 2730, val loss: 1.7166022062301636
Epoch 2740, training loss: 62.20729064941406 = 0.006292139180004597 + 10.0 * 6.220099925994873
Epoch 2740, val loss: 1.718061089515686
Epoch 2750, training loss: 62.1766471862793 = 0.0062384712509810925 + 10.0 * 6.217041015625
Epoch 2750, val loss: 1.7204135656356812
Epoch 2760, training loss: 62.17148208618164 = 0.006184170953929424 + 10.0 * 6.216529846191406
Epoch 2760, val loss: 1.7224866151809692
Epoch 2770, training loss: 62.21183395385742 = 0.006133439484983683 + 10.0 * 6.220570087432861
Epoch 2770, val loss: 1.7241538763046265
Epoch 2780, training loss: 62.171627044677734 = 0.006079700775444508 + 10.0 * 6.216554641723633
Epoch 2780, val loss: 1.7261643409729004
Epoch 2790, training loss: 62.177223205566406 = 0.006028858013451099 + 10.0 * 6.2171196937561035
Epoch 2790, val loss: 1.7280778884887695
Epoch 2800, training loss: 62.2293815612793 = 0.005979630630463362 + 10.0 * 6.222340106964111
Epoch 2800, val loss: 1.7293974161148071
Epoch 2810, training loss: 62.167762756347656 = 0.005927323829382658 + 10.0 * 6.216183662414551
Epoch 2810, val loss: 1.7317203283309937
Epoch 2820, training loss: 62.172237396240234 = 0.005878886673599482 + 10.0 * 6.216635704040527
Epoch 2820, val loss: 1.7337517738342285
Epoch 2830, training loss: 62.17241287231445 = 0.005831962451338768 + 10.0 * 6.216658115386963
Epoch 2830, val loss: 1.7350666522979736
Epoch 2840, training loss: 62.22442626953125 = 0.005784930661320686 + 10.0 * 6.221864223480225
Epoch 2840, val loss: 1.736588954925537
Epoch 2850, training loss: 62.18779754638672 = 0.005735490471124649 + 10.0 * 6.21820592880249
Epoch 2850, val loss: 1.7393163442611694
Epoch 2860, training loss: 62.18008804321289 = 0.005688437260687351 + 10.0 * 6.217440128326416
Epoch 2860, val loss: 1.740053653717041
Epoch 2870, training loss: 62.15948486328125 = 0.005644568707793951 + 10.0 * 6.215384006500244
Epoch 2870, val loss: 1.7426753044128418
Epoch 2880, training loss: 62.21186828613281 = 0.005602295510470867 + 10.0 * 6.220626354217529
Epoch 2880, val loss: 1.7441025972366333
Epoch 2890, training loss: 62.16072463989258 = 0.005554958712309599 + 10.0 * 6.215517044067383
Epoch 2890, val loss: 1.7459739446640015
Epoch 2900, training loss: 62.1540412902832 = 0.005510949995368719 + 10.0 * 6.214852809906006
Epoch 2900, val loss: 1.747675895690918
Epoch 2910, training loss: 62.15545654296875 = 0.005471199285238981 + 10.0 * 6.214998722076416
Epoch 2910, val loss: 1.7495745420455933
Epoch 2920, training loss: 62.212074279785156 = 0.005432802252471447 + 10.0 * 6.220664024353027
Epoch 2920, val loss: 1.751142978668213
Epoch 2930, training loss: 62.14397048950195 = 0.0053841001354157925 + 10.0 * 6.213858604431152
Epoch 2930, val loss: 1.752588152885437
Epoch 2940, training loss: 62.15102767944336 = 0.0053438772447407246 + 10.0 * 6.214568138122559
Epoch 2940, val loss: 1.7542963027954102
Epoch 2950, training loss: 62.19052505493164 = 0.005306560546159744 + 10.0 * 6.218522071838379
Epoch 2950, val loss: 1.7558597326278687
Epoch 2960, training loss: 62.16619110107422 = 0.005264009814709425 + 10.0 * 6.216092586517334
Epoch 2960, val loss: 1.7575148344039917
Epoch 2970, training loss: 62.21017074584961 = 0.0052229249849915504 + 10.0 * 6.220494747161865
Epoch 2970, val loss: 1.758537769317627
Epoch 2980, training loss: 62.15177536010742 = 0.005184709560126066 + 10.0 * 6.214659214019775
Epoch 2980, val loss: 1.7606316804885864
Epoch 2990, training loss: 62.14265060424805 = 0.00514752883464098 + 10.0 * 6.21375036239624
Epoch 2990, val loss: 1.7622309923171997
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 87.9046401977539 = 1.9361159801483154 + 10.0 * 8.59685230255127
Epoch 0, val loss: 1.9346169233322144
Epoch 10, training loss: 87.88860321044922 = 1.9261937141418457 + 10.0 * 8.596240997314453
Epoch 10, val loss: 1.9250311851501465
Epoch 20, training loss: 87.82762145996094 = 1.9145290851593018 + 10.0 * 8.591309547424316
Epoch 20, val loss: 1.9132622480392456
Epoch 30, training loss: 87.44276428222656 = 1.9005264043807983 + 10.0 * 8.554224014282227
Epoch 30, val loss: 1.8988516330718994
Epoch 40, training loss: 85.19087982177734 = 1.8841434717178345 + 10.0 * 8.330674171447754
Epoch 40, val loss: 1.8822097778320312
Epoch 50, training loss: 81.2075424194336 = 1.865466594696045 + 10.0 * 7.934207916259766
Epoch 50, val loss: 1.8639041185379028
Epoch 60, training loss: 77.51399230957031 = 1.8513327836990356 + 10.0 * 7.566266059875488
Epoch 60, val loss: 1.8511348962783813
Epoch 70, training loss: 73.84430694580078 = 1.8445520401000977 + 10.0 * 7.199975967407227
Epoch 70, val loss: 1.8454021215438843
Epoch 80, training loss: 72.05294799804688 = 1.8375660181045532 + 10.0 * 7.021538257598877
Epoch 80, val loss: 1.8391575813293457
Epoch 90, training loss: 71.00399780273438 = 1.8278151750564575 + 10.0 * 6.917618751525879
Epoch 90, val loss: 1.8297501802444458
Epoch 100, training loss: 70.08822631835938 = 1.818676233291626 + 10.0 * 6.826954364776611
Epoch 100, val loss: 1.8211274147033691
Epoch 110, training loss: 69.42127990722656 = 1.8105915784835815 + 10.0 * 6.761068820953369
Epoch 110, val loss: 1.8134393692016602
Epoch 120, training loss: 68.80999755859375 = 1.8022454977035522 + 10.0 * 6.700775146484375
Epoch 120, val loss: 1.8056302070617676
Epoch 130, training loss: 68.34437561035156 = 1.7935398817062378 + 10.0 * 6.655083656311035
Epoch 130, val loss: 1.7976332902908325
Epoch 140, training loss: 67.97920989990234 = 1.7845590114593506 + 10.0 * 6.619464874267578
Epoch 140, val loss: 1.7892953157424927
Epoch 150, training loss: 67.67180633544922 = 1.7749273777008057 + 10.0 * 6.589687824249268
Epoch 150, val loss: 1.7806557416915894
Epoch 160, training loss: 67.41531372070312 = 1.7645487785339355 + 10.0 * 6.5650763511657715
Epoch 160, val loss: 1.771561622619629
Epoch 170, training loss: 67.1952133178711 = 1.753316044807434 + 10.0 * 6.544189453125
Epoch 170, val loss: 1.761831521987915
Epoch 180, training loss: 67.00312042236328 = 1.7409210205078125 + 10.0 * 6.526219844818115
Epoch 180, val loss: 1.7511999607086182
Epoch 190, training loss: 66.84724426269531 = 1.7272037267684937 + 10.0 * 6.512004375457764
Epoch 190, val loss: 1.739513635635376
Epoch 200, training loss: 66.66506958007812 = 1.7120957374572754 + 10.0 * 6.495297431945801
Epoch 200, val loss: 1.7266963720321655
Epoch 210, training loss: 66.5165786743164 = 1.695544719696045 + 10.0 * 6.4821038246154785
Epoch 210, val loss: 1.7127501964569092
Epoch 220, training loss: 66.39972686767578 = 1.6772412061691284 + 10.0 * 6.472248554229736
Epoch 220, val loss: 1.6974952220916748
Epoch 230, training loss: 66.25469970703125 = 1.6573660373687744 + 10.0 * 6.459733486175537
Epoch 230, val loss: 1.6809062957763672
Epoch 240, training loss: 66.13096618652344 = 1.6358028650283813 + 10.0 * 6.449516296386719
Epoch 240, val loss: 1.6630353927612305
Epoch 250, training loss: 66.02198791503906 = 1.6124587059020996 + 10.0 * 6.440953254699707
Epoch 250, val loss: 1.6437766551971436
Epoch 260, training loss: 65.90617370605469 = 1.5875440835952759 + 10.0 * 6.431862831115723
Epoch 260, val loss: 1.623301386833191
Epoch 270, training loss: 65.8149185180664 = 1.5610158443450928 + 10.0 * 6.425390243530273
Epoch 270, val loss: 1.601648211479187
Epoch 280, training loss: 65.71046447753906 = 1.5330793857574463 + 10.0 * 6.41773796081543
Epoch 280, val loss: 1.5790233612060547
Epoch 290, training loss: 65.649658203125 = 1.5039273500442505 + 10.0 * 6.4145731925964355
Epoch 290, val loss: 1.55561101436615
Epoch 300, training loss: 65.5290298461914 = 1.4736396074295044 + 10.0 * 6.405538558959961
Epoch 300, val loss: 1.5316779613494873
Epoch 310, training loss: 65.4274673461914 = 1.442641019821167 + 10.0 * 6.398482322692871
Epoch 310, val loss: 1.5073468685150146
Epoch 320, training loss: 65.33944702148438 = 1.4109598398208618 + 10.0 * 6.392848968505859
Epoch 320, val loss: 1.482852816581726
Epoch 330, training loss: 65.2879409790039 = 1.378799319267273 + 10.0 * 6.390913963317871
Epoch 330, val loss: 1.4585087299346924
Epoch 340, training loss: 65.21416473388672 = 1.3464068174362183 + 10.0 * 6.386775493621826
Epoch 340, val loss: 1.4343501329421997
Epoch 350, training loss: 65.11261749267578 = 1.314064621925354 + 10.0 * 6.379855155944824
Epoch 350, val loss: 1.4108726978302002
Epoch 360, training loss: 65.03114318847656 = 1.2818074226379395 + 10.0 * 6.37493371963501
Epoch 360, val loss: 1.388010859489441
Epoch 370, training loss: 64.96302032470703 = 1.2498480081558228 + 10.0 * 6.371317386627197
Epoch 370, val loss: 1.365845799446106
Epoch 380, training loss: 64.9111557006836 = 1.2181673049926758 + 10.0 * 6.369298458099365
Epoch 380, val loss: 1.3442977666854858
Epoch 390, training loss: 64.8405532836914 = 1.1870670318603516 + 10.0 * 6.365348815917969
Epoch 390, val loss: 1.3237636089324951
Epoch 400, training loss: 64.76084899902344 = 1.156400442123413 + 10.0 * 6.360445022583008
Epoch 400, val loss: 1.3040282726287842
Epoch 410, training loss: 64.72166442871094 = 1.1264280080795288 + 10.0 * 6.359523296356201
Epoch 410, val loss: 1.2853466272354126
Epoch 420, training loss: 64.64752960205078 = 1.0969797372817993 + 10.0 * 6.355055332183838
Epoch 420, val loss: 1.2675830125808716
Epoch 430, training loss: 64.58900451660156 = 1.0683510303497314 + 10.0 * 6.352065563201904
Epoch 430, val loss: 1.2507189512252808
Epoch 440, training loss: 64.60054779052734 = 1.0402971506118774 + 10.0 * 6.356025218963623
Epoch 440, val loss: 1.2347923517227173
Epoch 450, training loss: 64.48389434814453 = 1.0129319429397583 + 10.0 * 6.3470964431762695
Epoch 450, val loss: 1.2196893692016602
Epoch 460, training loss: 64.45043182373047 = 0.986349880695343 + 10.0 * 6.346408367156982
Epoch 460, val loss: 1.2055931091308594
Epoch 470, training loss: 64.37342071533203 = 0.9604136943817139 + 10.0 * 6.3413004875183105
Epoch 470, val loss: 1.1924511194229126
Epoch 480, training loss: 64.33290100097656 = 0.9352227449417114 + 10.0 * 6.339767932891846
Epoch 480, val loss: 1.180159091949463
Epoch 490, training loss: 64.29208374023438 = 0.9107710123062134 + 10.0 * 6.338131427764893
Epoch 490, val loss: 1.1686210632324219
Epoch 500, training loss: 64.25664520263672 = 0.8868140578269958 + 10.0 * 6.3369832038879395
Epoch 500, val loss: 1.1576263904571533
Epoch 510, training loss: 64.1923828125 = 0.8636742830276489 + 10.0 * 6.3328704833984375
Epoch 510, val loss: 1.1476414203643799
Epoch 520, training loss: 64.15354919433594 = 0.8411362767219543 + 10.0 * 6.331241130828857
Epoch 520, val loss: 1.1384259462356567
Epoch 530, training loss: 64.1904525756836 = 0.8191775679588318 + 10.0 * 6.337127208709717
Epoch 530, val loss: 1.1298216581344604
Epoch 540, training loss: 64.06320190429688 = 0.7978276014328003 + 10.0 * 6.326537609100342
Epoch 540, val loss: 1.1218841075897217
Epoch 550, training loss: 64.03833770751953 = 0.7770550847053528 + 10.0 * 6.3261284828186035
Epoch 550, val loss: 1.1146702766418457
Epoch 560, training loss: 63.99114227294922 = 0.7567658424377441 + 10.0 * 6.323437690734863
Epoch 560, val loss: 1.1081223487854004
Epoch 570, training loss: 63.965023040771484 = 0.7370663285255432 + 10.0 * 6.322795391082764
Epoch 570, val loss: 1.101944923400879
Epoch 580, training loss: 63.919315338134766 = 0.7178878784179688 + 10.0 * 6.32014274597168
Epoch 580, val loss: 1.0965811014175415
Epoch 590, training loss: 63.877559661865234 = 0.6991826891899109 + 10.0 * 6.317837715148926
Epoch 590, val loss: 1.0917909145355225
Epoch 600, training loss: 63.845298767089844 = 0.6808914542198181 + 10.0 * 6.316440582275391
Epoch 600, val loss: 1.0876134634017944
Epoch 610, training loss: 63.84730529785156 = 0.6629068851470947 + 10.0 * 6.318439960479736
Epoch 610, val loss: 1.0836968421936035
Epoch 620, training loss: 63.81296920776367 = 0.6454024910926819 + 10.0 * 6.316756725311279
Epoch 620, val loss: 1.0802335739135742
Epoch 630, training loss: 63.75146484375 = 0.6282538175582886 + 10.0 * 6.312321186065674
Epoch 630, val loss: 1.0774085521697998
Epoch 640, training loss: 63.709354400634766 = 0.6115203499794006 + 10.0 * 6.309783458709717
Epoch 640, val loss: 1.0750164985656738
Epoch 650, training loss: 63.81297302246094 = 0.5950794816017151 + 10.0 * 6.321789264678955
Epoch 650, val loss: 1.0729568004608154
Epoch 660, training loss: 63.659515380859375 = 0.5789607167243958 + 10.0 * 6.308055400848389
Epoch 660, val loss: 1.0710821151733398
Epoch 670, training loss: 63.62480926513672 = 0.5632241368293762 + 10.0 * 6.306158542633057
Epoch 670, val loss: 1.069914698600769
Epoch 680, training loss: 63.59233856201172 = 0.5478596091270447 + 10.0 * 6.304448127746582
Epoch 680, val loss: 1.069130778312683
Epoch 690, training loss: 63.58258819580078 = 0.5328106880187988 + 10.0 * 6.304977893829346
Epoch 690, val loss: 1.06878662109375
Epoch 700, training loss: 63.546382904052734 = 0.5180032849311829 + 10.0 * 6.302838325500488
Epoch 700, val loss: 1.0686310529708862
Epoch 710, training loss: 63.52061462402344 = 0.5034551024436951 + 10.0 * 6.301715850830078
Epoch 710, val loss: 1.0687623023986816
Epoch 720, training loss: 63.49753952026367 = 0.48923158645629883 + 10.0 * 6.300830841064453
Epoch 720, val loss: 1.0691778659820557
Epoch 730, training loss: 63.45622253417969 = 0.4752998948097229 + 10.0 * 6.298092365264893
Epoch 730, val loss: 1.0701217651367188
Epoch 740, training loss: 63.43505859375 = 0.4616585969924927 + 10.0 * 6.297339916229248
Epoch 740, val loss: 1.0713655948638916
Epoch 750, training loss: 63.43873977661133 = 0.44821402430534363 + 10.0 * 6.2990522384643555
Epoch 750, val loss: 1.0727992057800293
Epoch 760, training loss: 63.43114471435547 = 0.4350036680698395 + 10.0 * 6.299613952636719
Epoch 760, val loss: 1.0742385387420654
Epoch 770, training loss: 63.38721466064453 = 0.4220415949821472 + 10.0 * 6.296517372131348
Epoch 770, val loss: 1.076249361038208
Epoch 780, training loss: 63.33604431152344 = 0.40939438343048096 + 10.0 * 6.292665004730225
Epoch 780, val loss: 1.0786927938461304
Epoch 790, training loss: 63.31504821777344 = 0.3970259130001068 + 10.0 * 6.291802406311035
Epoch 790, val loss: 1.081446886062622
Epoch 800, training loss: 63.33664321899414 = 0.3848612606525421 + 10.0 * 6.295178413391113
Epoch 800, val loss: 1.0841718912124634
Epoch 810, training loss: 63.31169509887695 = 0.37292245030403137 + 10.0 * 6.293877601623535
Epoch 810, val loss: 1.0870110988616943
Epoch 820, training loss: 63.25126266479492 = 0.36125460267066956 + 10.0 * 6.289000511169434
Epoch 820, val loss: 1.0904600620269775
Epoch 830, training loss: 63.221160888671875 = 0.3498687148094177 + 10.0 * 6.2871294021606445
Epoch 830, val loss: 1.093929409980774
Epoch 840, training loss: 63.254852294921875 = 0.3387535810470581 + 10.0 * 6.291609764099121
Epoch 840, val loss: 1.0976687669754028
Epoch 850, training loss: 63.22021484375 = 0.3277754783630371 + 10.0 * 6.289244174957275
Epoch 850, val loss: 1.101474642753601
Epoch 860, training loss: 63.20258331298828 = 0.3171390891075134 + 10.0 * 6.288544654846191
Epoch 860, val loss: 1.1057779788970947
Epoch 870, training loss: 63.14352035522461 = 0.30672770738601685 + 10.0 * 6.283679008483887
Epoch 870, val loss: 1.1099728345870972
Epoch 880, training loss: 63.1279411315918 = 0.2966509461402893 + 10.0 * 6.2831292152404785
Epoch 880, val loss: 1.114547610282898
Epoch 890, training loss: 63.1347541809082 = 0.28681638836860657 + 10.0 * 6.284793853759766
Epoch 890, val loss: 1.119173288345337
Epoch 900, training loss: 63.109161376953125 = 0.27725860476493835 + 10.0 * 6.2831902503967285
Epoch 900, val loss: 1.1246403455734253
Epoch 910, training loss: 63.09944152832031 = 0.26796069741249084 + 10.0 * 6.283148288726807
Epoch 910, val loss: 1.129630208015442
Epoch 920, training loss: 63.049991607666016 = 0.25897130370140076 + 10.0 * 6.279101848602295
Epoch 920, val loss: 1.1350651979446411
Epoch 930, training loss: 63.051021575927734 = 0.2502524256706238 + 10.0 * 6.28007698059082
Epoch 930, val loss: 1.1405102014541626
Epoch 940, training loss: 63.03479766845703 = 0.2418239414691925 + 10.0 * 6.279297351837158
Epoch 940, val loss: 1.1465126276016235
Epoch 950, training loss: 63.02544021606445 = 0.23363319039344788 + 10.0 * 6.279180526733398
Epoch 950, val loss: 1.1522221565246582
Epoch 960, training loss: 62.98368453979492 = 0.22574962675571442 + 10.0 * 6.275793552398682
Epoch 960, val loss: 1.1582967042922974
Epoch 970, training loss: 62.98417663574219 = 0.2181343287229538 + 10.0 * 6.276604175567627
Epoch 970, val loss: 1.1645103693008423
Epoch 980, training loss: 62.97261047363281 = 0.21075625717639923 + 10.0 * 6.276185512542725
Epoch 980, val loss: 1.1707713603973389
Epoch 990, training loss: 62.974037170410156 = 0.2036360651254654 + 10.0 * 6.277040004730225
Epoch 990, val loss: 1.1774343252182007
Epoch 1000, training loss: 62.94542694091797 = 0.19678303599357605 + 10.0 * 6.274864673614502
Epoch 1000, val loss: 1.1840908527374268
Epoch 1010, training loss: 62.917381286621094 = 0.1901835948228836 + 10.0 * 6.272719860076904
Epoch 1010, val loss: 1.1909239292144775
Epoch 1020, training loss: 63.00735855102539 = 0.1838635355234146 + 10.0 * 6.282349586486816
Epoch 1020, val loss: 1.197779893875122
Epoch 1030, training loss: 62.903751373291016 = 0.17769214510917664 + 10.0 * 6.272605895996094
Epoch 1030, val loss: 1.2050200700759888
Epoch 1040, training loss: 62.87065887451172 = 0.17179933190345764 + 10.0 * 6.269886016845703
Epoch 1040, val loss: 1.2121810913085938
Epoch 1050, training loss: 62.860008239746094 = 0.16615121066570282 + 10.0 * 6.26938533782959
Epoch 1050, val loss: 1.2195253372192383
Epoch 1060, training loss: 62.99721908569336 = 0.16070130467414856 + 10.0 * 6.283651828765869
Epoch 1060, val loss: 1.2267147302627563
Epoch 1070, training loss: 62.83461380004883 = 0.1553535908460617 + 10.0 * 6.267926216125488
Epoch 1070, val loss: 1.23406982421875
Epoch 1080, training loss: 62.833251953125 = 0.15029318630695343 + 10.0 * 6.268296241760254
Epoch 1080, val loss: 1.2416894435882568
Epoch 1090, training loss: 62.80488586425781 = 0.14545336365699768 + 10.0 * 6.2659430503845215
Epoch 1090, val loss: 1.2493650913238525
Epoch 1100, training loss: 62.8028450012207 = 0.14078305661678314 + 10.0 * 6.26620626449585
Epoch 1100, val loss: 1.2569421529769897
Epoch 1110, training loss: 62.82957077026367 = 0.13625803589820862 + 10.0 * 6.269331455230713
Epoch 1110, val loss: 1.2647446393966675
Epoch 1120, training loss: 62.801578521728516 = 0.13187912106513977 + 10.0 * 6.266970157623291
Epoch 1120, val loss: 1.272172212600708
Epoch 1130, training loss: 62.76442337036133 = 0.12771815061569214 + 10.0 * 6.263670444488525
Epoch 1130, val loss: 1.2804590463638306
Epoch 1140, training loss: 62.75678253173828 = 0.12372054904699326 + 10.0 * 6.263306140899658
Epoch 1140, val loss: 1.2885856628417969
Epoch 1150, training loss: 62.860774993896484 = 0.11988088488578796 + 10.0 * 6.274089336395264
Epoch 1150, val loss: 1.2962963581085205
Epoch 1160, training loss: 62.78053665161133 = 0.11612161993980408 + 10.0 * 6.266441345214844
Epoch 1160, val loss: 1.3042562007904053
Epoch 1170, training loss: 62.72718811035156 = 0.11251874268054962 + 10.0 * 6.261466979980469
Epoch 1170, val loss: 1.312360405921936
Epoch 1180, training loss: 62.71406555175781 = 0.10908480733633041 + 10.0 * 6.260498046875
Epoch 1180, val loss: 1.3204797506332397
Epoch 1190, training loss: 62.72841262817383 = 0.1057879775762558 + 10.0 * 6.262262344360352
Epoch 1190, val loss: 1.3287267684936523
Epoch 1200, training loss: 62.70921325683594 = 0.10255790501832962 + 10.0 * 6.260665416717529
Epoch 1200, val loss: 1.3363500833511353
Epoch 1210, training loss: 62.720375061035156 = 0.09946791082620621 + 10.0 * 6.262090682983398
Epoch 1210, val loss: 1.3446317911148071
Epoch 1220, training loss: 62.68421173095703 = 0.09648024290800095 + 10.0 * 6.258772850036621
Epoch 1220, val loss: 1.3522828817367554
Epoch 1230, training loss: 62.6700439453125 = 0.0936322808265686 + 10.0 * 6.257641315460205
Epoch 1230, val loss: 1.3601642847061157
Epoch 1240, training loss: 62.67258071899414 = 0.09088817238807678 + 10.0 * 6.258169174194336
Epoch 1240, val loss: 1.368112325668335
Epoch 1250, training loss: 62.710811614990234 = 0.08822427690029144 + 10.0 * 6.262259006500244
Epoch 1250, val loss: 1.3757890462875366
Epoch 1260, training loss: 62.685462951660156 = 0.08561486750841141 + 10.0 * 6.259984970092773
Epoch 1260, val loss: 1.3833725452423096
Epoch 1270, training loss: 62.64346694946289 = 0.08311591297388077 + 10.0 * 6.256035327911377
Epoch 1270, val loss: 1.3915271759033203
Epoch 1280, training loss: 62.634891510009766 = 0.08074049651622772 + 10.0 * 6.255414962768555
Epoch 1280, val loss: 1.399263858795166
Epoch 1290, training loss: 62.62596893310547 = 0.07845805585384369 + 10.0 * 6.254751205444336
Epoch 1290, val loss: 1.4071236848831177
Epoch 1300, training loss: 62.637393951416016 = 0.07626176625490189 + 10.0 * 6.256113052368164
Epoch 1300, val loss: 1.415036678314209
Epoch 1310, training loss: 62.63608169555664 = 0.0740942731499672 + 10.0 * 6.256198883056641
Epoch 1310, val loss: 1.4221810102462769
Epoch 1320, training loss: 62.64628219604492 = 0.07199639827013016 + 10.0 * 6.2574286460876465
Epoch 1320, val loss: 1.4301033020019531
Epoch 1330, training loss: 62.60005187988281 = 0.06999509781599045 + 10.0 * 6.253005504608154
Epoch 1330, val loss: 1.4377731084823608
Epoch 1340, training loss: 62.59444046020508 = 0.06808318942785263 + 10.0 * 6.252635478973389
Epoch 1340, val loss: 1.4454742670059204
Epoch 1350, training loss: 62.59490966796875 = 0.06624434888362885 + 10.0 * 6.252866268157959
Epoch 1350, val loss: 1.4531331062316895
Epoch 1360, training loss: 62.65830612182617 = 0.06446034461259842 + 10.0 * 6.259384632110596
Epoch 1360, val loss: 1.4605340957641602
Epoch 1370, training loss: 62.6032829284668 = 0.06269824504852295 + 10.0 * 6.254058361053467
Epoch 1370, val loss: 1.468266487121582
Epoch 1380, training loss: 62.57794952392578 = 0.06103375926613808 + 10.0 * 6.2516913414001465
Epoch 1380, val loss: 1.475587248802185
Epoch 1390, training loss: 62.620338439941406 = 0.05943330004811287 + 10.0 * 6.2560906410217285
Epoch 1390, val loss: 1.4831362962722778
Epoch 1400, training loss: 62.56976318359375 = 0.05785509571433067 + 10.0 * 6.251191139221191
Epoch 1400, val loss: 1.4904017448425293
Epoch 1410, training loss: 62.57087326049805 = 0.05634810030460358 + 10.0 * 6.251452445983887
Epoch 1410, val loss: 1.4976682662963867
Epoch 1420, training loss: 62.57856369018555 = 0.05489104241132736 + 10.0 * 6.2523674964904785
Epoch 1420, val loss: 1.5047377347946167
Epoch 1430, training loss: 62.54130554199219 = 0.05348813161253929 + 10.0 * 6.248781681060791
Epoch 1430, val loss: 1.512477159500122
Epoch 1440, training loss: 62.55536651611328 = 0.052143532782793045 + 10.0 * 6.250322341918945
Epoch 1440, val loss: 1.5195882320404053
Epoch 1450, training loss: 62.58286666870117 = 0.05082866549491882 + 10.0 * 6.253203868865967
Epoch 1450, val loss: 1.526657223701477
Epoch 1460, training loss: 62.54539108276367 = 0.04954029619693756 + 10.0 * 6.249585151672363
Epoch 1460, val loss: 1.5334120988845825
Epoch 1470, training loss: 62.55097579956055 = 0.048322997987270355 + 10.0 * 6.250265121459961
Epoch 1470, val loss: 1.540528655052185
Epoch 1480, training loss: 62.53311538696289 = 0.047131139785051346 + 10.0 * 6.248598575592041
Epoch 1480, val loss: 1.5474809408187866
Epoch 1490, training loss: 62.50687026977539 = 0.04598812013864517 + 10.0 * 6.246088027954102
Epoch 1490, val loss: 1.5547789335250854
Epoch 1500, training loss: 62.5218391418457 = 0.04488792270421982 + 10.0 * 6.247694969177246
Epoch 1500, val loss: 1.5616719722747803
Epoch 1510, training loss: 62.53276824951172 = 0.04382384195923805 + 10.0 * 6.248894691467285
Epoch 1510, val loss: 1.5684020519256592
Epoch 1520, training loss: 62.525630950927734 = 0.04278143495321274 + 10.0 * 6.248284816741943
Epoch 1520, val loss: 1.5751031637191772
Epoch 1530, training loss: 62.53099060058594 = 0.04176519811153412 + 10.0 * 6.248922824859619
Epoch 1530, val loss: 1.5816905498504639
Epoch 1540, training loss: 62.489105224609375 = 0.040788911283016205 + 10.0 * 6.244831562042236
Epoch 1540, val loss: 1.5880961418151855
Epoch 1550, training loss: 62.48603057861328 = 0.03985285758972168 + 10.0 * 6.244617938995361
Epoch 1550, val loss: 1.5948588848114014
Epoch 1560, training loss: 62.478004455566406 = 0.03894897550344467 + 10.0 * 6.243905544281006
Epoch 1560, val loss: 1.6013543605804443
Epoch 1570, training loss: 62.56058120727539 = 0.038076598197221756 + 10.0 * 6.2522501945495605
Epoch 1570, val loss: 1.6077022552490234
Epoch 1580, training loss: 62.50946807861328 = 0.037210240960121155 + 10.0 * 6.247225761413574
Epoch 1580, val loss: 1.614091157913208
Epoch 1590, training loss: 62.48221206665039 = 0.03638092800974846 + 10.0 * 6.2445831298828125
Epoch 1590, val loss: 1.6203579902648926
Epoch 1600, training loss: 62.5294075012207 = 0.03558941185474396 + 10.0 * 6.2493815422058105
Epoch 1600, val loss: 1.6263874769210815
Epoch 1610, training loss: 62.46330642700195 = 0.034797366708517075 + 10.0 * 6.2428507804870605
Epoch 1610, val loss: 1.6328948736190796
Epoch 1620, training loss: 62.45085525512695 = 0.03404555842280388 + 10.0 * 6.241681098937988
Epoch 1620, val loss: 1.6389962434768677
Epoch 1630, training loss: 62.48037338256836 = 0.033321231603622437 + 10.0 * 6.2447052001953125
Epoch 1630, val loss: 1.644867181777954
Epoch 1640, training loss: 62.48283386230469 = 0.03261560574173927 + 10.0 * 6.245021820068359
Epoch 1640, val loss: 1.6511088609695435
Epoch 1650, training loss: 62.46186447143555 = 0.03192318230867386 + 10.0 * 6.24299430847168
Epoch 1650, val loss: 1.6569876670837402
Epoch 1660, training loss: 62.4351921081543 = 0.03125423938035965 + 10.0 * 6.24039363861084
Epoch 1660, val loss: 1.6629081964492798
Epoch 1670, training loss: 62.50166702270508 = 0.030620867386460304 + 10.0 * 6.247104644775391
Epoch 1670, val loss: 1.6690196990966797
Epoch 1680, training loss: 62.4434928894043 = 0.02997705526649952 + 10.0 * 6.24135160446167
Epoch 1680, val loss: 1.6742751598358154
Epoch 1690, training loss: 62.424522399902344 = 0.02937052585184574 + 10.0 * 6.23951530456543
Epoch 1690, val loss: 1.6801776885986328
Epoch 1700, training loss: 62.45705032348633 = 0.02878621779382229 + 10.0 * 6.242826461791992
Epoch 1700, val loss: 1.68562912940979
Epoch 1710, training loss: 62.43108367919922 = 0.028204774484038353 + 10.0 * 6.240287780761719
Epoch 1710, val loss: 1.6912977695465088
Epoch 1720, training loss: 62.41642379760742 = 0.027642780914902687 + 10.0 * 6.23887825012207
Epoch 1720, val loss: 1.696825385093689
Epoch 1730, training loss: 62.41219711303711 = 0.027103513479232788 + 10.0 * 6.238509178161621
Epoch 1730, val loss: 1.7023135423660278
Epoch 1740, training loss: 62.46814727783203 = 0.026583854109048843 + 10.0 * 6.244156360626221
Epoch 1740, val loss: 1.7078710794448853
Epoch 1750, training loss: 62.41184616088867 = 0.026061255484819412 + 10.0 * 6.2385783195495605
Epoch 1750, val loss: 1.712937593460083
Epoch 1760, training loss: 62.404136657714844 = 0.02556574158370495 + 10.0 * 6.237856864929199
Epoch 1760, val loss: 1.7184487581253052
Epoch 1770, training loss: 62.42692947387695 = 0.025090573355555534 + 10.0 * 6.2401838302612305
Epoch 1770, val loss: 1.7237250804901123
Epoch 1780, training loss: 62.40966033935547 = 0.024612411856651306 + 10.0 * 6.238504886627197
Epoch 1780, val loss: 1.728599190711975
Epoch 1790, training loss: 62.40793991088867 = 0.024149874225258827 + 10.0 * 6.238379001617432
Epoch 1790, val loss: 1.7337853908538818
Epoch 1800, training loss: 62.40553283691406 = 0.02370886132121086 + 10.0 * 6.238182544708252
Epoch 1800, val loss: 1.738852858543396
Epoch 1810, training loss: 62.39894485473633 = 0.023274024948477745 + 10.0 * 6.237566947937012
Epoch 1810, val loss: 1.7438089847564697
Epoch 1820, training loss: 62.408935546875 = 0.022857408970594406 + 10.0 * 6.238607883453369
Epoch 1820, val loss: 1.7489004135131836
Epoch 1830, training loss: 62.417938232421875 = 0.022443179041147232 + 10.0 * 6.23954963684082
Epoch 1830, val loss: 1.7535476684570312
Epoch 1840, training loss: 62.39622497558594 = 0.02203894592821598 + 10.0 * 6.2374186515808105
Epoch 1840, val loss: 1.7582818269729614
Epoch 1850, training loss: 62.3889045715332 = 0.02164262905716896 + 10.0 * 6.2367262840271
Epoch 1850, val loss: 1.7627977132797241
Epoch 1860, training loss: 62.37165832519531 = 0.021266097202897072 + 10.0 * 6.235039234161377
Epoch 1860, val loss: 1.7678862810134888
Epoch 1870, training loss: 62.375457763671875 = 0.020898407325148582 + 10.0 * 6.2354559898376465
Epoch 1870, val loss: 1.7723901271820068
Epoch 1880, training loss: 62.393367767333984 = 0.020541459321975708 + 10.0 * 6.237282752990723
Epoch 1880, val loss: 1.7768092155456543
Epoch 1890, training loss: 62.38765335083008 = 0.02019260823726654 + 10.0 * 6.236746311187744
Epoch 1890, val loss: 1.7816251516342163
Epoch 1900, training loss: 62.39105987548828 = 0.019844496622681618 + 10.0 * 6.23712158203125
Epoch 1900, val loss: 1.7860167026519775
Epoch 1910, training loss: 62.37374496459961 = 0.019507046788930893 + 10.0 * 6.235423564910889
Epoch 1910, val loss: 1.7903543710708618
Epoch 1920, training loss: 62.35056686401367 = 0.019181478768587112 + 10.0 * 6.233138561248779
Epoch 1920, val loss: 1.7947413921356201
Epoch 1930, training loss: 62.344078063964844 = 0.018867256119847298 + 10.0 * 6.232521057128906
Epoch 1930, val loss: 1.799302577972412
Epoch 1940, training loss: 62.34831619262695 = 0.01856330595910549 + 10.0 * 6.232975482940674
Epoch 1940, val loss: 1.8037668466567993
Epoch 1950, training loss: 62.434974670410156 = 0.018263211473822594 + 10.0 * 6.241671085357666
Epoch 1950, val loss: 1.8077329397201538
Epoch 1960, training loss: 62.39442443847656 = 0.01796755939722061 + 10.0 * 6.237645626068115
Epoch 1960, val loss: 1.812151312828064
Epoch 1970, training loss: 62.36402893066406 = 0.01767021417617798 + 10.0 * 6.234635829925537
Epoch 1970, val loss: 1.816184163093567
Epoch 1980, training loss: 62.363914489746094 = 0.01739540696144104 + 10.0 * 6.234652042388916
Epoch 1980, val loss: 1.8203248977661133
Epoch 1990, training loss: 62.363807678222656 = 0.017117347568273544 + 10.0 * 6.234669208526611
Epoch 1990, val loss: 1.8242113590240479
Epoch 2000, training loss: 62.33396530151367 = 0.016846254467964172 + 10.0 * 6.2317118644714355
Epoch 2000, val loss: 1.8283193111419678
Epoch 2010, training loss: 62.32863998413086 = 0.016590481624007225 + 10.0 * 6.231204986572266
Epoch 2010, val loss: 1.832769751548767
Epoch 2020, training loss: 62.32212829589844 = 0.016340989619493484 + 10.0 * 6.230578422546387
Epoch 2020, val loss: 1.8368475437164307
Epoch 2030, training loss: 62.39500045776367 = 0.016098495572805405 + 10.0 * 6.237890243530273
Epoch 2030, val loss: 1.8407835960388184
Epoch 2040, training loss: 62.33396530151367 = 0.015843430534005165 + 10.0 * 6.231812477111816
Epoch 2040, val loss: 1.8438665866851807
Epoch 2050, training loss: 62.324832916259766 = 0.015609038062393665 + 10.0 * 6.230922222137451
Epoch 2050, val loss: 1.8481743335723877
Epoch 2060, training loss: 62.37339401245117 = 0.015375285409390926 + 10.0 * 6.235801696777344
Epoch 2060, val loss: 1.8516170978546143
Epoch 2070, training loss: 62.323822021484375 = 0.015150196850299835 + 10.0 * 6.2308669090271
Epoch 2070, val loss: 1.8555392026901245
Epoch 2080, training loss: 62.31342315673828 = 0.01492393296211958 + 10.0 * 6.229849815368652
Epoch 2080, val loss: 1.8594108819961548
Epoch 2090, training loss: 62.34996795654297 = 0.014715717174112797 + 10.0 * 6.233525276184082
Epoch 2090, val loss: 1.8630752563476562
Epoch 2100, training loss: 62.30662155151367 = 0.014499159529805183 + 10.0 * 6.229212284088135
Epoch 2100, val loss: 1.8664432764053345
Epoch 2110, training loss: 62.30167007446289 = 0.014293935149908066 + 10.0 * 6.228737831115723
Epoch 2110, val loss: 1.870102882385254
Epoch 2120, training loss: 62.2949333190918 = 0.01409546472132206 + 10.0 * 6.228083610534668
Epoch 2120, val loss: 1.8737030029296875
Epoch 2130, training loss: 62.309688568115234 = 0.013902940787374973 + 10.0 * 6.229578495025635
Epoch 2130, val loss: 1.8772339820861816
Epoch 2140, training loss: 62.317325592041016 = 0.013707170262932777 + 10.0 * 6.2303619384765625
Epoch 2140, val loss: 1.8803538084030151
Epoch 2150, training loss: 62.2978401184082 = 0.013509580865502357 + 10.0 * 6.228433132171631
Epoch 2150, val loss: 1.8836724758148193
Epoch 2160, training loss: 62.343833923339844 = 0.013326043263077736 + 10.0 * 6.23305082321167
Epoch 2160, val loss: 1.8873060941696167
Epoch 2170, training loss: 62.29496383666992 = 0.013141005299985409 + 10.0 * 6.228182315826416
Epoch 2170, val loss: 1.8903241157531738
Epoch 2180, training loss: 62.287208557128906 = 0.012965315021574497 + 10.0 * 6.227424144744873
Epoch 2180, val loss: 1.893699049949646
Epoch 2190, training loss: 62.28130340576172 = 0.012792900204658508 + 10.0 * 6.226850986480713
Epoch 2190, val loss: 1.8971774578094482
Epoch 2200, training loss: 62.28520965576172 = 0.012627488933503628 + 10.0 * 6.227258205413818
Epoch 2200, val loss: 1.9004136323928833
Epoch 2210, training loss: 62.33577346801758 = 0.012466548942029476 + 10.0 * 6.232330799102783
Epoch 2210, val loss: 1.903533697128296
Epoch 2220, training loss: 62.31284713745117 = 0.012297915294766426 + 10.0 * 6.23005485534668
Epoch 2220, val loss: 1.906498670578003
Epoch 2230, training loss: 62.29126739501953 = 0.012133943848311901 + 10.0 * 6.2279133796691895
Epoch 2230, val loss: 1.909515142440796
Epoch 2240, training loss: 62.27870178222656 = 0.011979078873991966 + 10.0 * 6.226672172546387
Epoch 2240, val loss: 1.912927508354187
Epoch 2250, training loss: 62.29286575317383 = 0.011829162947833538 + 10.0 * 6.2281036376953125
Epoch 2250, val loss: 1.915816068649292
Epoch 2260, training loss: 62.29292678833008 = 0.011679887771606445 + 10.0 * 6.228124618530273
Epoch 2260, val loss: 1.9187326431274414
Epoch 2270, training loss: 62.279109954833984 = 0.011531898751854897 + 10.0 * 6.226758003234863
Epoch 2270, val loss: 1.9215410947799683
Epoch 2280, training loss: 62.32332992553711 = 0.011390440165996552 + 10.0 * 6.231194019317627
Epoch 2280, val loss: 1.924586534500122
Epoch 2290, training loss: 62.26492691040039 = 0.011246416717767715 + 10.0 * 6.225368022918701
Epoch 2290, val loss: 1.9275031089782715
Epoch 2300, training loss: 62.25474548339844 = 0.011108025908470154 + 10.0 * 6.224363803863525
Epoch 2300, val loss: 1.9304777383804321
Epoch 2310, training loss: 62.2553825378418 = 0.01097593642771244 + 10.0 * 6.224440574645996
Epoch 2310, val loss: 1.9333597421646118
Epoch 2320, training loss: 62.31367111206055 = 0.01085150521248579 + 10.0 * 6.230281829833984
Epoch 2320, val loss: 1.9362967014312744
Epoch 2330, training loss: 62.2684440612793 = 0.010713266208767891 + 10.0 * 6.225773334503174
Epoch 2330, val loss: 1.938592791557312
Epoch 2340, training loss: 62.271018981933594 = 0.010586552321910858 + 10.0 * 6.226043224334717
Epoch 2340, val loss: 1.9416069984436035
Epoch 2350, training loss: 62.25060272216797 = 0.01045962329953909 + 10.0 * 6.2240142822265625
Epoch 2350, val loss: 1.9441460371017456
Epoch 2360, training loss: 62.26536178588867 = 0.010340350680053234 + 10.0 * 6.225502014160156
Epoch 2360, val loss: 1.9468754529953003
Epoch 2370, training loss: 62.263389587402344 = 0.010217959992587566 + 10.0 * 6.225317001342773
Epoch 2370, val loss: 1.9493319988250732
Epoch 2380, training loss: 62.263648986816406 = 0.010098671540617943 + 10.0 * 6.22535514831543
Epoch 2380, val loss: 1.9518661499023438
Epoch 2390, training loss: 62.261409759521484 = 0.009984140284359455 + 10.0 * 6.225142478942871
Epoch 2390, val loss: 1.954331874847412
Epoch 2400, training loss: 62.30162048339844 = 0.009872015565633774 + 10.0 * 6.229174613952637
Epoch 2400, val loss: 1.9566395282745361
Epoch 2410, training loss: 62.260013580322266 = 0.009755229577422142 + 10.0 * 6.2250261306762695
Epoch 2410, val loss: 1.9592314958572388
Epoch 2420, training loss: 62.24348831176758 = 0.009643731638789177 + 10.0 * 6.223384380340576
Epoch 2420, val loss: 1.9617239236831665
Epoch 2430, training loss: 62.22933578491211 = 0.009539336897432804 + 10.0 * 6.22197961807251
Epoch 2430, val loss: 1.9645050764083862
Epoch 2440, training loss: 62.228515625 = 0.009438266046345234 + 10.0 * 6.221907615661621
Epoch 2440, val loss: 1.9669767618179321
Epoch 2450, training loss: 62.289215087890625 = 0.009339788928627968 + 10.0 * 6.227987766265869
Epoch 2450, val loss: 1.9694145917892456
Epoch 2460, training loss: 62.28727340698242 = 0.00923184398561716 + 10.0 * 6.227804183959961
Epoch 2460, val loss: 1.9714279174804688
Epoch 2470, training loss: 62.263572692871094 = 0.009129487909376621 + 10.0 * 6.225444316864014
Epoch 2470, val loss: 1.9735357761383057
Epoch 2480, training loss: 62.237762451171875 = 0.009028042666614056 + 10.0 * 6.222873210906982
Epoch 2480, val loss: 1.9758033752441406
Epoch 2490, training loss: 62.21736145019531 = 0.00893495324999094 + 10.0 * 6.2208428382873535
Epoch 2490, val loss: 1.9783971309661865
Epoch 2500, training loss: 62.22418212890625 = 0.008844098076224327 + 10.0 * 6.22153377532959
Epoch 2500, val loss: 1.980723261833191
Epoch 2510, training loss: 62.28620529174805 = 0.00875419657677412 + 10.0 * 6.227745056152344
Epoch 2510, val loss: 1.9828310012817383
Epoch 2520, training loss: 62.23044204711914 = 0.008657271973788738 + 10.0 * 6.2221784591674805
Epoch 2520, val loss: 1.98489248752594
Epoch 2530, training loss: 62.20726776123047 = 0.00856874044984579 + 10.0 * 6.219870090484619
Epoch 2530, val loss: 1.9871826171875
Epoch 2540, training loss: 62.275482177734375 = 0.008486773818731308 + 10.0 * 6.226699352264404
Epoch 2540, val loss: 1.9893875122070312
Epoch 2550, training loss: 62.22495651245117 = 0.008393433876335621 + 10.0 * 6.221656322479248
Epoch 2550, val loss: 1.9913052320480347
Epoch 2560, training loss: 62.211875915527344 = 0.008308649063110352 + 10.0 * 6.2203569412231445
Epoch 2560, val loss: 1.9935641288757324
Epoch 2570, training loss: 62.208404541015625 = 0.008225013501942158 + 10.0 * 6.220017910003662
Epoch 2570, val loss: 1.9956730604171753
Epoch 2580, training loss: 62.227569580078125 = 0.008147680200636387 + 10.0 * 6.221941947937012
Epoch 2580, val loss: 1.9976928234100342
Epoch 2590, training loss: 62.21678161621094 = 0.008065964095294476 + 10.0 * 6.220871925354004
Epoch 2590, val loss: 1.9997755289077759
Epoch 2600, training loss: 62.25939178466797 = 0.007989130914211273 + 10.0 * 6.225140571594238
Epoch 2600, val loss: 2.0020811557769775
Epoch 2610, training loss: 62.21322250366211 = 0.007907077670097351 + 10.0 * 6.220531463623047
Epoch 2610, val loss: 2.003535032272339
Epoch 2620, training loss: 62.202003479003906 = 0.00783100351691246 + 10.0 * 6.219417095184326
Epoch 2620, val loss: 2.005730390548706
Epoch 2630, training loss: 62.19915771484375 = 0.007758268155157566 + 10.0 * 6.21914005279541
Epoch 2630, val loss: 2.007812023162842
Epoch 2640, training loss: 62.267513275146484 = 0.007687550038099289 + 10.0 * 6.225982666015625
Epoch 2640, val loss: 2.0097334384918213
Epoch 2650, training loss: 62.2214241027832 = 0.00761022511869669 + 10.0 * 6.221381187438965
Epoch 2650, val loss: 2.0113654136657715
Epoch 2660, training loss: 62.21758270263672 = 0.007538381963968277 + 10.0 * 6.221004486083984
Epoch 2660, val loss: 2.0131547451019287
Epoch 2670, training loss: 62.20748519897461 = 0.007466230075806379 + 10.0 * 6.220002174377441
Epoch 2670, val loss: 2.014975070953369
Epoch 2680, training loss: 62.18507766723633 = 0.00739730428904295 + 10.0 * 6.21776819229126
Epoch 2680, val loss: 2.016897201538086
Epoch 2690, training loss: 62.20966339111328 = 0.007331507746130228 + 10.0 * 6.220232963562012
Epoch 2690, val loss: 2.0187277793884277
Epoch 2700, training loss: 62.21002197265625 = 0.007264598272740841 + 10.0 * 6.22027587890625
Epoch 2700, val loss: 2.0201432704925537
Epoch 2710, training loss: 62.209381103515625 = 0.007196970749646425 + 10.0 * 6.220218181610107
Epoch 2710, val loss: 2.0218496322631836
Epoch 2720, training loss: 62.20654296875 = 0.0071328263729810715 + 10.0 * 6.219941139221191
Epoch 2720, val loss: 2.0237624645233154
Epoch 2730, training loss: 62.19707107543945 = 0.007069592829793692 + 10.0 * 6.219000339508057
Epoch 2730, val loss: 2.025411367416382
Epoch 2740, training loss: 62.20035171508789 = 0.007007270120084286 + 10.0 * 6.219334602355957
Epoch 2740, val loss: 2.027106523513794
Epoch 2750, training loss: 62.19903564453125 = 0.006944166496396065 + 10.0 * 6.21920919418335
Epoch 2750, val loss: 2.028489351272583
Epoch 2760, training loss: 62.1879768371582 = 0.006882260553538799 + 10.0 * 6.218109607696533
Epoch 2760, val loss: 2.0301764011383057
Epoch 2770, training loss: 62.250858306884766 = 0.006826488766819239 + 10.0 * 6.224402904510498
Epoch 2770, val loss: 2.0317635536193848
Epoch 2780, training loss: 62.18729782104492 = 0.006762977689504623 + 10.0 * 6.218053340911865
Epoch 2780, val loss: 2.033083200454712
Epoch 2790, training loss: 62.17242431640625 = 0.006704344879835844 + 10.0 * 6.216571807861328
Epoch 2790, val loss: 2.0346620082855225
Epoch 2800, training loss: 62.178123474121094 = 0.006647884380072355 + 10.0 * 6.217147350311279
Epoch 2800, val loss: 2.0361826419830322
Epoch 2810, training loss: 62.24061584472656 = 0.006593572441488504 + 10.0 * 6.223402500152588
Epoch 2810, val loss: 2.037703275680542
Epoch 2820, training loss: 62.191192626953125 = 0.006536030676215887 + 10.0 * 6.218465805053711
Epoch 2820, val loss: 2.0390961170196533
Epoch 2830, training loss: 62.186710357666016 = 0.006480502430349588 + 10.0 * 6.21802282333374
Epoch 2830, val loss: 2.040355682373047
Epoch 2840, training loss: 62.18795394897461 = 0.006427027750760317 + 10.0 * 6.2181525230407715
Epoch 2840, val loss: 2.0419464111328125
Epoch 2850, training loss: 62.175411224365234 = 0.006375168915838003 + 10.0 * 6.2169036865234375
Epoch 2850, val loss: 2.043316602706909
Epoch 2860, training loss: 62.192771911621094 = 0.006323070731014013 + 10.0 * 6.218644618988037
Epoch 2860, val loss: 2.044734477996826
Epoch 2870, training loss: 62.16080856323242 = 0.006271917838603258 + 10.0 * 6.215453624725342
Epoch 2870, val loss: 2.046107530593872
Epoch 2880, training loss: 62.18464660644531 = 0.006223093252629042 + 10.0 * 6.2178425788879395
Epoch 2880, val loss: 2.047464370727539
Epoch 2890, training loss: 62.16865158081055 = 0.00617368845269084 + 10.0 * 6.216248035430908
Epoch 2890, val loss: 2.0487990379333496
Epoch 2900, training loss: 62.197959899902344 = 0.006124584469944239 + 10.0 * 6.219183444976807
Epoch 2900, val loss: 2.050161361694336
Epoch 2910, training loss: 62.18431091308594 = 0.006073485594242811 + 10.0 * 6.2178239822387695
Epoch 2910, val loss: 2.0511040687561035
Epoch 2920, training loss: 62.18074417114258 = 0.006026395130902529 + 10.0 * 6.217471599578857
Epoch 2920, val loss: 2.0523457527160645
Epoch 2930, training loss: 62.166168212890625 = 0.005977905821055174 + 10.0 * 6.216019153594971
Epoch 2930, val loss: 2.053524971008301
Epoch 2940, training loss: 62.16899871826172 = 0.005933952517807484 + 10.0 * 6.216306209564209
Epoch 2940, val loss: 2.0547847747802734
Epoch 2950, training loss: 62.1772346496582 = 0.005887811072170734 + 10.0 * 6.217134952545166
Epoch 2950, val loss: 2.0560834407806396
Epoch 2960, training loss: 62.15996170043945 = 0.005841908045113087 + 10.0 * 6.215412139892578
Epoch 2960, val loss: 2.057204246520996
Epoch 2970, training loss: 62.16351318359375 = 0.005798037629574537 + 10.0 * 6.215771675109863
Epoch 2970, val loss: 2.05841064453125
Epoch 2980, training loss: 62.16203689575195 = 0.005755338352173567 + 10.0 * 6.215628147125244
Epoch 2980, val loss: 2.0596718788146973
Epoch 2990, training loss: 62.15435028076172 = 0.005712550599128008 + 10.0 * 6.2148637771606445
Epoch 2990, val loss: 2.060716390609741
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6925925925925926
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 87.91627502441406 = 1.9476280212402344 + 10.0 * 8.596864700317383
Epoch 0, val loss: 1.9472870826721191
Epoch 10, training loss: 87.90095520019531 = 1.9368582963943481 + 10.0 * 8.596409797668457
Epoch 10, val loss: 1.9366205930709839
Epoch 20, training loss: 87.85041046142578 = 1.9232854843139648 + 10.0 * 8.59271240234375
Epoch 20, val loss: 1.9225540161132812
Epoch 30, training loss: 87.5736312866211 = 1.905449628829956 + 10.0 * 8.566818237304688
Epoch 30, val loss: 1.9039011001586914
Epoch 40, training loss: 86.1739273071289 = 1.8851511478424072 + 10.0 * 8.428876876831055
Epoch 40, val loss: 1.8836655616760254
Epoch 50, training loss: 82.40123748779297 = 1.86454439163208 + 10.0 * 8.053668975830078
Epoch 50, val loss: 1.8637440204620361
Epoch 60, training loss: 78.3091812133789 = 1.850847840309143 + 10.0 * 7.645833492279053
Epoch 60, val loss: 1.8520889282226562
Epoch 70, training loss: 74.26643371582031 = 1.8417015075683594 + 10.0 * 7.242473125457764
Epoch 70, val loss: 1.8443084955215454
Epoch 80, training loss: 71.472412109375 = 1.833222508430481 + 10.0 * 6.963919162750244
Epoch 80, val loss: 1.836639404296875
Epoch 90, training loss: 69.94270324707031 = 1.8241708278656006 + 10.0 * 6.811852931976318
Epoch 90, val loss: 1.828223466873169
Epoch 100, training loss: 69.12139892578125 = 1.8138463497161865 + 10.0 * 6.730755805969238
Epoch 100, val loss: 1.81901216506958
Epoch 110, training loss: 68.56619262695312 = 1.8032135963439941 + 10.0 * 6.676297664642334
Epoch 110, val loss: 1.8097219467163086
Epoch 120, training loss: 68.14450073242188 = 1.7927614450454712 + 10.0 * 6.635173797607422
Epoch 120, val loss: 1.8006387948989868
Epoch 130, training loss: 67.82373046875 = 1.782347321510315 + 10.0 * 6.604137897491455
Epoch 130, val loss: 1.7915568351745605
Epoch 140, training loss: 67.54790496826172 = 1.7714896202087402 + 10.0 * 6.577641010284424
Epoch 140, val loss: 1.7822203636169434
Epoch 150, training loss: 67.30754852294922 = 1.760029673576355 + 10.0 * 6.554751396179199
Epoch 150, val loss: 1.7724664211273193
Epoch 160, training loss: 67.12106323242188 = 1.7476303577423096 + 10.0 * 6.537343502044678
Epoch 160, val loss: 1.7620304822921753
Epoch 170, training loss: 66.929931640625 = 1.7340686321258545 + 10.0 * 6.519586086273193
Epoch 170, val loss: 1.750746726989746
Epoch 180, training loss: 66.75056457519531 = 1.7192997932434082 + 10.0 * 6.503126621246338
Epoch 180, val loss: 1.7384980916976929
Epoch 190, training loss: 66.60261535644531 = 1.7030850648880005 + 10.0 * 6.48995304107666
Epoch 190, val loss: 1.7251031398773193
Epoch 200, training loss: 66.46282196044922 = 1.6853681802749634 + 10.0 * 6.477745056152344
Epoch 200, val loss: 1.710552453994751
Epoch 210, training loss: 66.35726165771484 = 1.6660187244415283 + 10.0 * 6.4691243171691895
Epoch 210, val loss: 1.6947317123413086
Epoch 220, training loss: 66.22772979736328 = 1.6450717449188232 + 10.0 * 6.458265781402588
Epoch 220, val loss: 1.6776444911956787
Epoch 230, training loss: 66.10717010498047 = 1.622493863105774 + 10.0 * 6.448467254638672
Epoch 230, val loss: 1.6593618392944336
Epoch 240, training loss: 66.0169906616211 = 1.5983796119689941 + 10.0 * 6.441860675811768
Epoch 240, val loss: 1.6398711204528809
Epoch 250, training loss: 65.91275024414062 = 1.5728641748428345 + 10.0 * 6.43398904800415
Epoch 250, val loss: 1.6194932460784912
Epoch 260, training loss: 65.80270385742188 = 1.546172022819519 + 10.0 * 6.425653457641602
Epoch 260, val loss: 1.598211407661438
Epoch 270, training loss: 65.70014190673828 = 1.5183932781219482 + 10.0 * 6.418174743652344
Epoch 270, val loss: 1.5763601064682007
Epoch 280, training loss: 65.61003112792969 = 1.4897353649139404 + 10.0 * 6.412029266357422
Epoch 280, val loss: 1.5539978742599487
Epoch 290, training loss: 65.51244354248047 = 1.4603592157363892 + 10.0 * 6.405208587646484
Epoch 290, val loss: 1.5313714742660522
Epoch 300, training loss: 65.43568420410156 = 1.430505394935608 + 10.0 * 6.40051794052124
Epoch 300, val loss: 1.5084642171859741
Epoch 310, training loss: 65.34280395507812 = 1.4002310037612915 + 10.0 * 6.394257068634033
Epoch 310, val loss: 1.4856288433074951
Epoch 320, training loss: 65.25530242919922 = 1.3698383569717407 + 10.0 * 6.388546943664551
Epoch 320, val loss: 1.4628679752349854
Epoch 330, training loss: 65.17936706542969 = 1.3392661809921265 + 10.0 * 6.384009838104248
Epoch 330, val loss: 1.4402137994766235
Epoch 340, training loss: 65.18611145019531 = 1.3087712526321411 + 10.0 * 6.3877339363098145
Epoch 340, val loss: 1.4179309606552124
Epoch 350, training loss: 65.06468200683594 = 1.2782173156738281 + 10.0 * 6.378646373748779
Epoch 350, val loss: 1.3957237005233765
Epoch 360, training loss: 64.96934509277344 = 1.2478408813476562 + 10.0 * 6.372150897979736
Epoch 360, val loss: 1.373942494392395
Epoch 370, training loss: 64.8951416015625 = 1.217647671699524 + 10.0 * 6.367749214172363
Epoch 370, val loss: 1.3524492979049683
Epoch 380, training loss: 64.84558868408203 = 1.1876503229141235 + 10.0 * 6.365793704986572
Epoch 380, val loss: 1.3313653469085693
Epoch 390, training loss: 64.79259490966797 = 1.1578826904296875 + 10.0 * 6.363471031188965
Epoch 390, val loss: 1.3105006217956543
Epoch 400, training loss: 64.70655822753906 = 1.1284786462783813 + 10.0 * 6.3578081130981445
Epoch 400, val loss: 1.2900205850601196
Epoch 410, training loss: 64.64969635009766 = 1.0995228290557861 + 10.0 * 6.355017185211182
Epoch 410, val loss: 1.2700172662734985
Epoch 420, training loss: 64.58584594726562 = 1.0710219144821167 + 10.0 * 6.351482391357422
Epoch 420, val loss: 1.2506500482559204
Epoch 430, training loss: 64.53366088867188 = 1.0431259870529175 + 10.0 * 6.349053382873535
Epoch 430, val loss: 1.2316839694976807
Epoch 440, training loss: 64.48675537109375 = 1.0158584117889404 + 10.0 * 6.3470892906188965
Epoch 440, val loss: 1.2135263681411743
Epoch 450, training loss: 64.42800903320312 = 0.9892889261245728 + 10.0 * 6.343871593475342
Epoch 450, val loss: 1.1958718299865723
Epoch 460, training loss: 64.3849868774414 = 0.9634767174720764 + 10.0 * 6.342150688171387
Epoch 460, val loss: 1.1791709661483765
Epoch 470, training loss: 64.32128143310547 = 0.9385022521018982 + 10.0 * 6.338278293609619
Epoch 470, val loss: 1.163227915763855
Epoch 480, training loss: 64.30647277832031 = 0.9143291115760803 + 10.0 * 6.33921480178833
Epoch 480, val loss: 1.1481095552444458
Epoch 490, training loss: 64.24923706054688 = 0.890960156917572 + 10.0 * 6.335827827453613
Epoch 490, val loss: 1.1337165832519531
Epoch 500, training loss: 64.19156646728516 = 0.868349552154541 + 10.0 * 6.332321643829346
Epoch 500, val loss: 1.120248556137085
Epoch 510, training loss: 64.19392395019531 = 0.8465224504470825 + 10.0 * 6.334740161895752
Epoch 510, val loss: 1.10759437084198
Epoch 520, training loss: 64.14649200439453 = 0.8254913687705994 + 10.0 * 6.3321003913879395
Epoch 520, val loss: 1.0955637693405151
Epoch 530, training loss: 64.07588195800781 = 0.8051568865776062 + 10.0 * 6.327072620391846
Epoch 530, val loss: 1.0841768980026245
Epoch 540, training loss: 64.03430938720703 = 0.785483181476593 + 10.0 * 6.324882984161377
Epoch 540, val loss: 1.0737650394439697
Epoch 550, training loss: 64.00863647460938 = 0.7663949728012085 + 10.0 * 6.324224472045898
Epoch 550, val loss: 1.0638667345046997
Epoch 560, training loss: 63.95688247680664 = 0.7478308081626892 + 10.0 * 6.3209052085876465
Epoch 560, val loss: 1.054648756980896
Epoch 570, training loss: 63.92844009399414 = 0.7297606468200684 + 10.0 * 6.319868087768555
Epoch 570, val loss: 1.0460131168365479
Epoch 580, training loss: 63.93511199951172 = 0.7121307253837585 + 10.0 * 6.322298049926758
Epoch 580, val loss: 1.0379135608673096
Epoch 590, training loss: 63.86444091796875 = 0.6949447989463806 + 10.0 * 6.316949367523193
Epoch 590, val loss: 1.0300792455673218
Epoch 600, training loss: 63.82291030883789 = 0.6780371069908142 + 10.0 * 6.314487457275391
Epoch 600, val loss: 1.02290678024292
Epoch 610, training loss: 63.81023025512695 = 0.6615233421325684 + 10.0 * 6.314870834350586
Epoch 610, val loss: 1.016079068183899
Epoch 620, training loss: 63.76897430419922 = 0.6451520323753357 + 10.0 * 6.312382221221924
Epoch 620, val loss: 1.009603500366211
Epoch 630, training loss: 63.75387191772461 = 0.6291040182113647 + 10.0 * 6.312476634979248
Epoch 630, val loss: 1.0033456087112427
Epoch 640, training loss: 63.694732666015625 = 0.613240659236908 + 10.0 * 6.308149337768555
Epoch 640, val loss: 0.9976146817207336
Epoch 650, training loss: 63.67377853393555 = 0.5976062417030334 + 10.0 * 6.3076171875
Epoch 650, val loss: 0.9921182990074158
Epoch 660, training loss: 63.6630859375 = 0.5821276307106018 + 10.0 * 6.308095932006836
Epoch 660, val loss: 0.9867711067199707
Epoch 670, training loss: 63.63688278198242 = 0.5668171644210815 + 10.0 * 6.307006359100342
Epoch 670, val loss: 0.9816191792488098
Epoch 680, training loss: 63.615779876708984 = 0.5515506267547607 + 10.0 * 6.306422710418701
Epoch 680, val loss: 0.9769220352172852
Epoch 690, training loss: 63.552894592285156 = 0.5364702939987183 + 10.0 * 6.301642417907715
Epoch 690, val loss: 0.9722875356674194
Epoch 700, training loss: 63.5279541015625 = 0.5215446352958679 + 10.0 * 6.300641059875488
Epoch 700, val loss: 0.9678668975830078
Epoch 710, training loss: 63.50292205810547 = 0.5067122578620911 + 10.0 * 6.299620628356934
Epoch 710, val loss: 0.9637080430984497
Epoch 720, training loss: 63.50565719604492 = 0.4920448958873749 + 10.0 * 6.301361083984375
Epoch 720, val loss: 0.9596472978591919
Epoch 730, training loss: 63.49663543701172 = 0.47735634446144104 + 10.0 * 6.3019280433654785
Epoch 730, val loss: 0.9558504223823547
Epoch 740, training loss: 63.42076873779297 = 0.4628530740737915 + 10.0 * 6.2957916259765625
Epoch 740, val loss: 0.9520353078842163
Epoch 750, training loss: 63.39653015136719 = 0.4485374391078949 + 10.0 * 6.294799327850342
Epoch 750, val loss: 0.9484543204307556
Epoch 760, training loss: 63.42306137084961 = 0.4344409108161926 + 10.0 * 6.298861980438232
Epoch 760, val loss: 0.9450452327728271
Epoch 770, training loss: 63.37735366821289 = 0.42040714621543884 + 10.0 * 6.295694828033447
Epoch 770, val loss: 0.9420219659805298
Epoch 780, training loss: 63.323219299316406 = 0.40662312507629395 + 10.0 * 6.291659355163574
Epoch 780, val loss: 0.9390971064567566
Epoch 790, training loss: 63.2931022644043 = 0.3930875360965729 + 10.0 * 6.290001392364502
Epoch 790, val loss: 0.9364830255508423
Epoch 800, training loss: 63.26933670043945 = 0.3798103630542755 + 10.0 * 6.288952827453613
Epoch 800, val loss: 0.9341979622840881
Epoch 810, training loss: 63.45072555541992 = 0.36672714352607727 + 10.0 * 6.3084001541137695
Epoch 810, val loss: 0.9321059584617615
Epoch 820, training loss: 63.2329216003418 = 0.35392728447914124 + 10.0 * 6.287899494171143
Epoch 820, val loss: 0.9301213026046753
Epoch 830, training loss: 63.20014190673828 = 0.34144434332847595 + 10.0 * 6.285869598388672
Epoch 830, val loss: 0.9286295771598816
Epoch 840, training loss: 63.1795768737793 = 0.32935765385627747 + 10.0 * 6.285021781921387
Epoch 840, val loss: 0.9275220632553101
Epoch 850, training loss: 63.16946792602539 = 0.3175935745239258 + 10.0 * 6.285187721252441
Epoch 850, val loss: 0.9268738627433777
Epoch 860, training loss: 63.15702438354492 = 0.30615341663360596 + 10.0 * 6.2850871086120605
Epoch 860, val loss: 0.9263470768928528
Epoch 870, training loss: 63.11948013305664 = 0.29503509402275085 + 10.0 * 6.282444477081299
Epoch 870, val loss: 0.9262473583221436
Epoch 880, training loss: 63.11149978637695 = 0.284331351518631 + 10.0 * 6.282716751098633
Epoch 880, val loss: 0.9263850450515747
Epoch 890, training loss: 63.13946533203125 = 0.27395865321159363 + 10.0 * 6.286550521850586
Epoch 890, val loss: 0.9271326065063477
Epoch 900, training loss: 63.072566986083984 = 0.26397544145584106 + 10.0 * 6.280858993530273
Epoch 900, val loss: 0.9280217289924622
Epoch 910, training loss: 63.048152923583984 = 0.25433874130249023 + 10.0 * 6.27938175201416
Epoch 910, val loss: 0.9292986392974854
Epoch 920, training loss: 63.097900390625 = 0.24509936571121216 + 10.0 * 6.285280227661133
Epoch 920, val loss: 0.9310511350631714
Epoch 930, training loss: 63.04926300048828 = 0.23611490428447723 + 10.0 * 6.281314849853516
Epoch 930, val loss: 0.9329794049263
Epoch 940, training loss: 63.00448989868164 = 0.22754845023155212 + 10.0 * 6.277694225311279
Epoch 940, val loss: 0.9351240396499634
Epoch 950, training loss: 62.98244094848633 = 0.21930140256881714 + 10.0 * 6.276313781738281
Epoch 950, val loss: 0.9377803802490234
Epoch 960, training loss: 62.96466064453125 = 0.21141941845417023 + 10.0 * 6.27532434463501
Epoch 960, val loss: 0.9407387375831604
Epoch 970, training loss: 63.026466369628906 = 0.20384328067302704 + 10.0 * 6.282262325286865
Epoch 970, val loss: 0.9439216256141663
Epoch 980, training loss: 62.978370666503906 = 0.19649983942508698 + 10.0 * 6.278187274932861
Epoch 980, val loss: 0.94724440574646
Epoch 990, training loss: 62.927154541015625 = 0.1894795149564743 + 10.0 * 6.273767471313477
Epoch 990, val loss: 0.9508807063102722
Epoch 1000, training loss: 62.908836364746094 = 0.18277502059936523 + 10.0 * 6.272606372833252
Epoch 1000, val loss: 0.9547694325447083
Epoch 1010, training loss: 62.95409393310547 = 0.17637667059898376 + 10.0 * 6.277771949768066
Epoch 1010, val loss: 0.958694338798523
Epoch 1020, training loss: 62.89767837524414 = 0.1701398491859436 + 10.0 * 6.272753715515137
Epoch 1020, val loss: 0.9633461236953735
Epoch 1030, training loss: 62.901187896728516 = 0.16422082483768463 + 10.0 * 6.273696422576904
Epoch 1030, val loss: 0.9675998091697693
Epoch 1040, training loss: 62.86572265625 = 0.1585412323474884 + 10.0 * 6.270718097686768
Epoch 1040, val loss: 0.9726269841194153
Epoch 1050, training loss: 62.859596252441406 = 0.15312699973583221 + 10.0 * 6.270647048950195
Epoch 1050, val loss: 0.9773062467575073
Epoch 1060, training loss: 62.84207534790039 = 0.14788833260536194 + 10.0 * 6.269418716430664
Epoch 1060, val loss: 0.9824572801589966
Epoch 1070, training loss: 62.86626434326172 = 0.14287830889225006 + 10.0 * 6.272338390350342
Epoch 1070, val loss: 0.9875456690788269
Epoch 1080, training loss: 62.828941345214844 = 0.1380986124277115 + 10.0 * 6.2690839767456055
Epoch 1080, val loss: 0.9929315447807312
Epoch 1090, training loss: 62.8320426940918 = 0.1334879994392395 + 10.0 * 6.269855499267578
Epoch 1090, val loss: 0.9982647895812988
Epoch 1100, training loss: 62.834930419921875 = 0.12905952334403992 + 10.0 * 6.270586967468262
Epoch 1100, val loss: 1.0039361715316772
Epoch 1110, training loss: 62.7840576171875 = 0.12480929493904114 + 10.0 * 6.26592493057251
Epoch 1110, val loss: 1.0092294216156006
Epoch 1120, training loss: 62.76807403564453 = 0.12075284123420715 + 10.0 * 6.2647318840026855
Epoch 1120, val loss: 1.0150820016860962
Epoch 1130, training loss: 62.761653900146484 = 0.1168718934059143 + 10.0 * 6.2644782066345215
Epoch 1130, val loss: 1.0209132432937622
Epoch 1140, training loss: 62.79798889160156 = 0.113169364631176 + 10.0 * 6.268481731414795
Epoch 1140, val loss: 1.026656985282898
Epoch 1150, training loss: 62.75897979736328 = 0.10955340415239334 + 10.0 * 6.264942646026611
Epoch 1150, val loss: 1.0325956344604492
Epoch 1160, training loss: 62.77699279785156 = 0.10611623525619507 + 10.0 * 6.267087459564209
Epoch 1160, val loss: 1.0385807752609253
Epoch 1170, training loss: 62.75239562988281 = 0.1027691438794136 + 10.0 * 6.264962673187256
Epoch 1170, val loss: 1.0445847511291504
Epoch 1180, training loss: 62.715755462646484 = 0.0995786041021347 + 10.0 * 6.261617660522461
Epoch 1180, val loss: 1.050549864768982
Epoch 1190, training loss: 62.71419143676758 = 0.09654383361339569 + 10.0 * 6.261765003204346
Epoch 1190, val loss: 1.0565767288208008
Epoch 1200, training loss: 62.71772003173828 = 0.09361851960420609 + 10.0 * 6.2624101638793945
Epoch 1200, val loss: 1.062697172164917
Epoch 1210, training loss: 62.70656967163086 = 0.09078464657068253 + 10.0 * 6.261578559875488
Epoch 1210, val loss: 1.0689494609832764
Epoch 1220, training loss: 62.72944641113281 = 0.0880652442574501 + 10.0 * 6.264138221740723
Epoch 1220, val loss: 1.0752803087234497
Epoch 1230, training loss: 62.706153869628906 = 0.08542052656412125 + 10.0 * 6.262073516845703
Epoch 1230, val loss: 1.081390380859375
Epoch 1240, training loss: 62.69118881225586 = 0.08288251608610153 + 10.0 * 6.260830879211426
Epoch 1240, val loss: 1.0878124237060547
Epoch 1250, training loss: 62.68194580078125 = 0.08045797795057297 + 10.0 * 6.260148525238037
Epoch 1250, val loss: 1.0939998626708984
Epoch 1260, training loss: 62.664859771728516 = 0.07813413441181183 + 10.0 * 6.258672714233398
Epoch 1260, val loss: 1.1002132892608643
Epoch 1270, training loss: 62.64515686035156 = 0.07590416818857193 + 10.0 * 6.256925106048584
Epoch 1270, val loss: 1.106840968132019
Epoch 1280, training loss: 62.656002044677734 = 0.07376538962125778 + 10.0 * 6.258223533630371
Epoch 1280, val loss: 1.1132102012634277
Epoch 1290, training loss: 62.664833068847656 = 0.07168487459421158 + 10.0 * 6.25931453704834
Epoch 1290, val loss: 1.1194387674331665
Epoch 1300, training loss: 62.66431427001953 = 0.06967486441135406 + 10.0 * 6.259463787078857
Epoch 1300, val loss: 1.1253571510314941
Epoch 1310, training loss: 62.65139389038086 = 0.06773252040147781 + 10.0 * 6.258366107940674
Epoch 1310, val loss: 1.1314773559570312
Epoch 1320, training loss: 62.62257385253906 = 0.06584402918815613 + 10.0 * 6.255672931671143
Epoch 1320, val loss: 1.1381521224975586
Epoch 1330, training loss: 62.61356735229492 = 0.06405715644359589 + 10.0 * 6.254951000213623
Epoch 1330, val loss: 1.1443394422531128
Epoch 1340, training loss: 62.653690338134766 = 0.062349505722522736 + 10.0 * 6.259133815765381
Epoch 1340, val loss: 1.1508690118789673
Epoch 1350, training loss: 62.59855270385742 = 0.0606529600918293 + 10.0 * 6.253789901733398
Epoch 1350, val loss: 1.1567283868789673
Epoch 1360, training loss: 62.58772277832031 = 0.0590418316423893 + 10.0 * 6.252868175506592
Epoch 1360, val loss: 1.1630804538726807
Epoch 1370, training loss: 62.58995056152344 = 0.057494353502988815 + 10.0 * 6.2532453536987305
Epoch 1370, val loss: 1.169274926185608
Epoch 1380, training loss: 62.64259719848633 = 0.05599808320403099 + 10.0 * 6.258659839630127
Epoch 1380, val loss: 1.1753966808319092
Epoch 1390, training loss: 62.58753967285156 = 0.05454827845096588 + 10.0 * 6.253298759460449
Epoch 1390, val loss: 1.1813093423843384
Epoch 1400, training loss: 62.5702018737793 = 0.05314135178923607 + 10.0 * 6.251706123352051
Epoch 1400, val loss: 1.187574863433838
Epoch 1410, training loss: 62.60127639770508 = 0.05179804190993309 + 10.0 * 6.254947662353516
Epoch 1410, val loss: 1.1934362649917603
Epoch 1420, training loss: 62.57298278808594 = 0.05049688369035721 + 10.0 * 6.252248764038086
Epoch 1420, val loss: 1.1995233297348022
Epoch 1430, training loss: 62.617286682128906 = 0.04923202469944954 + 10.0 * 6.256805419921875
Epoch 1430, val loss: 1.2052702903747559
Epoch 1440, training loss: 62.56856918334961 = 0.04799959808588028 + 10.0 * 6.252057075500488
Epoch 1440, val loss: 1.2118804454803467
Epoch 1450, training loss: 62.54430389404297 = 0.04681827872991562 + 10.0 * 6.249748706817627
Epoch 1450, val loss: 1.2175593376159668
Epoch 1460, training loss: 62.5341911315918 = 0.04568969085812569 + 10.0 * 6.248850345611572
Epoch 1460, val loss: 1.223694920539856
Epoch 1470, training loss: 62.5679817199707 = 0.0445927157998085 + 10.0 * 6.252338886260986
Epoch 1470, val loss: 1.2296651601791382
Epoch 1480, training loss: 62.5523567199707 = 0.04352179542183876 + 10.0 * 6.25088357925415
Epoch 1480, val loss: 1.2351062297821045
Epoch 1490, training loss: 62.534996032714844 = 0.042475827038288116 + 10.0 * 6.249251842498779
Epoch 1490, val loss: 1.2413307428359985
Epoch 1500, training loss: 62.53318405151367 = 0.04148152098059654 + 10.0 * 6.249170303344727
Epoch 1500, val loss: 1.2470788955688477
Epoch 1510, training loss: 62.51108932495117 = 0.040514908730983734 + 10.0 * 6.2470574378967285
Epoch 1510, val loss: 1.2529159784317017
Epoch 1520, training loss: 62.516422271728516 = 0.039592862129211426 + 10.0 * 6.247683048248291
Epoch 1520, val loss: 1.2586090564727783
Epoch 1530, training loss: 62.53834533691406 = 0.038691677153110504 + 10.0 * 6.249965190887451
Epoch 1530, val loss: 1.2643669843673706
Epoch 1540, training loss: 62.54508972167969 = 0.037823788821697235 + 10.0 * 6.250726699829102
Epoch 1540, val loss: 1.2704085111618042
Epoch 1550, training loss: 62.50759506225586 = 0.036964476108551025 + 10.0 * 6.247063159942627
Epoch 1550, val loss: 1.2757846117019653
Epoch 1560, training loss: 62.52876663208008 = 0.036147985607385635 + 10.0 * 6.249261856079102
Epoch 1560, val loss: 1.2811520099639893
Epoch 1570, training loss: 62.48983383178711 = 0.03534359112381935 + 10.0 * 6.245449066162109
Epoch 1570, val loss: 1.2874329090118408
Epoch 1580, training loss: 62.489139556884766 = 0.03457595407962799 + 10.0 * 6.245456218719482
Epoch 1580, val loss: 1.2928881645202637
Epoch 1590, training loss: 62.50144958496094 = 0.03383839502930641 + 10.0 * 6.246760845184326
Epoch 1590, val loss: 1.298527717590332
Epoch 1600, training loss: 62.48308563232422 = 0.03311172127723694 + 10.0 * 6.244997501373291
Epoch 1600, val loss: 1.3041050434112549
Epoch 1610, training loss: 62.50031661987305 = 0.03241207078099251 + 10.0 * 6.246790409088135
Epoch 1610, val loss: 1.309360146522522
Epoch 1620, training loss: 62.47358322143555 = 0.03172583505511284 + 10.0 * 6.244185447692871
Epoch 1620, val loss: 1.314751386642456
Epoch 1630, training loss: 62.47080993652344 = 0.031063688918948174 + 10.0 * 6.243974685668945
Epoch 1630, val loss: 1.3201454877853394
Epoch 1640, training loss: 62.471370697021484 = 0.030424272641539574 + 10.0 * 6.244094371795654
Epoch 1640, val loss: 1.3254899978637695
Epoch 1650, training loss: 62.50742721557617 = 0.02980002388358116 + 10.0 * 6.247762680053711
Epoch 1650, val loss: 1.3309564590454102
Epoch 1660, training loss: 62.49278259277344 = 0.029194490984082222 + 10.0 * 6.246358871459961
Epoch 1660, val loss: 1.3365527391433716
Epoch 1670, training loss: 62.45716857910156 = 0.02860257588326931 + 10.0 * 6.242856502532959
Epoch 1670, val loss: 1.3413151502609253
Epoch 1680, training loss: 62.43965148925781 = 0.028034908697009087 + 10.0 * 6.241161823272705
Epoch 1680, val loss: 1.3468842506408691
Epoch 1690, training loss: 62.43854522705078 = 0.02748742327094078 + 10.0 * 6.241105556488037
Epoch 1690, val loss: 1.352196216583252
Epoch 1700, training loss: 62.48025894165039 = 0.026957649737596512 + 10.0 * 6.245329856872559
Epoch 1700, val loss: 1.357343077659607
Epoch 1710, training loss: 62.45808792114258 = 0.02642875723540783 + 10.0 * 6.243165969848633
Epoch 1710, val loss: 1.3621865510940552
Epoch 1720, training loss: 62.44857406616211 = 0.02592095546424389 + 10.0 * 6.242265224456787
Epoch 1720, val loss: 1.3671472072601318
Epoch 1730, training loss: 62.43402862548828 = 0.025424625724554062 + 10.0 * 6.240860462188721
Epoch 1730, val loss: 1.3724405765533447
Epoch 1740, training loss: 62.437965393066406 = 0.024947557598352432 + 10.0 * 6.241301536560059
Epoch 1740, val loss: 1.3777228593826294
Epoch 1750, training loss: 62.45606994628906 = 0.024487227201461792 + 10.0 * 6.243158340454102
Epoch 1750, val loss: 1.3826555013656616
Epoch 1760, training loss: 62.42095184326172 = 0.024022508412599564 + 10.0 * 6.2396931648254395
Epoch 1760, val loss: 1.387298345565796
Epoch 1770, training loss: 62.41343688964844 = 0.023586275056004524 + 10.0 * 6.238985061645508
Epoch 1770, val loss: 1.392429232597351
Epoch 1780, training loss: 62.45037078857422 = 0.023172762244939804 + 10.0 * 6.242719650268555
Epoch 1780, val loss: 1.3970253467559814
Epoch 1790, training loss: 62.404930114746094 = 0.022745687514543533 + 10.0 * 6.238218307495117
Epoch 1790, val loss: 1.4021120071411133
Epoch 1800, training loss: 62.39921569824219 = 0.022335316985845566 + 10.0 * 6.237688064575195
Epoch 1800, val loss: 1.4071084260940552
Epoch 1810, training loss: 62.40487289428711 = 0.021947374567389488 + 10.0 * 6.238292694091797
Epoch 1810, val loss: 1.41169273853302
Epoch 1820, training loss: 62.469024658203125 = 0.021567275747656822 + 10.0 * 6.24474573135376
Epoch 1820, val loss: 1.4162397384643555
Epoch 1830, training loss: 62.432098388671875 = 0.02118072658777237 + 10.0 * 6.241091728210449
Epoch 1830, val loss: 1.4213653802871704
Epoch 1840, training loss: 62.414241790771484 = 0.020815733820199966 + 10.0 * 6.23934268951416
Epoch 1840, val loss: 1.4256830215454102
Epoch 1850, training loss: 62.39789962768555 = 0.020462486892938614 + 10.0 * 6.237743854522705
Epoch 1850, val loss: 1.4306557178497314
Epoch 1860, training loss: 62.39175796508789 = 0.02011481113731861 + 10.0 * 6.237164497375488
Epoch 1860, val loss: 1.4351309537887573
Epoch 1870, training loss: 62.41381072998047 = 0.019780660048127174 + 10.0 * 6.239403247833252
Epoch 1870, val loss: 1.439523696899414
Epoch 1880, training loss: 62.37266159057617 = 0.01945260539650917 + 10.0 * 6.235321044921875
Epoch 1880, val loss: 1.4445545673370361
Epoch 1890, training loss: 62.38875198364258 = 0.019138043746352196 + 10.0 * 6.236961364746094
Epoch 1890, val loss: 1.449190378189087
Epoch 1900, training loss: 62.413475036621094 = 0.01882333867251873 + 10.0 * 6.239465236663818
Epoch 1900, val loss: 1.4533995389938354
Epoch 1910, training loss: 62.38739013671875 = 0.01851707696914673 + 10.0 * 6.236887454986572
Epoch 1910, val loss: 1.4577722549438477
Epoch 1920, training loss: 62.39155578613281 = 0.018222693353891373 + 10.0 * 6.237333297729492
Epoch 1920, val loss: 1.4624850749969482
Epoch 1930, training loss: 62.386077880859375 = 0.01792844571173191 + 10.0 * 6.236814975738525
Epoch 1930, val loss: 1.4666334390640259
Epoch 1940, training loss: 62.3631706237793 = 0.017647314816713333 + 10.0 * 6.234552383422852
Epoch 1940, val loss: 1.4709010124206543
Epoch 1950, training loss: 62.36204147338867 = 0.017375152558088303 + 10.0 * 6.234466552734375
Epoch 1950, val loss: 1.475155234336853
Epoch 1960, training loss: 62.36965560913086 = 0.017107317224144936 + 10.0 * 6.235254764556885
Epoch 1960, val loss: 1.4796981811523438
Epoch 1970, training loss: 62.41230773925781 = 0.016846690326929092 + 10.0 * 6.239546298980713
Epoch 1970, val loss: 1.4843369722366333
Epoch 1980, training loss: 62.36259841918945 = 0.01658124476671219 + 10.0 * 6.2346014976501465
Epoch 1980, val loss: 1.4878287315368652
Epoch 1990, training loss: 62.35060119628906 = 0.016331903636455536 + 10.0 * 6.233427047729492
Epoch 1990, val loss: 1.4920408725738525
Epoch 2000, training loss: 62.36376953125 = 0.016090290620923042 + 10.0 * 6.234767913818359
Epoch 2000, val loss: 1.4964733123779297
Epoch 2010, training loss: 62.36479187011719 = 0.015851957723498344 + 10.0 * 6.234894275665283
Epoch 2010, val loss: 1.500325322151184
Epoch 2020, training loss: 62.39608383178711 = 0.015616379678249359 + 10.0 * 6.238046646118164
Epoch 2020, val loss: 1.5040500164031982
Epoch 2030, training loss: 62.351463317871094 = 0.015386983752250671 + 10.0 * 6.233607292175293
Epoch 2030, val loss: 1.5082732439041138
Epoch 2040, training loss: 62.33634948730469 = 0.015167336910963058 + 10.0 * 6.232118129730225
Epoch 2040, val loss: 1.5123958587646484
Epoch 2050, training loss: 62.33267593383789 = 0.014950511045753956 + 10.0 * 6.231772422790527
Epoch 2050, val loss: 1.5163192749023438
Epoch 2060, training loss: 62.37114334106445 = 0.014744602143764496 + 10.0 * 6.235640048980713
Epoch 2060, val loss: 1.5198332071304321
Epoch 2070, training loss: 62.33662796020508 = 0.014529203996062279 + 10.0 * 6.2322096824646
Epoch 2070, val loss: 1.5243191719055176
Epoch 2080, training loss: 62.34502029418945 = 0.014325241558253765 + 10.0 * 6.23306941986084
Epoch 2080, val loss: 1.528463363647461
Epoch 2090, training loss: 62.339141845703125 = 0.014129105024039745 + 10.0 * 6.232501029968262
Epoch 2090, val loss: 1.5321192741394043
Epoch 2100, training loss: 62.33018112182617 = 0.013935952447354794 + 10.0 * 6.231624603271484
Epoch 2100, val loss: 1.5361428260803223
Epoch 2110, training loss: 62.35408401489258 = 0.013749422505497932 + 10.0 * 6.234033584594727
Epoch 2110, val loss: 1.5400675535202026
Epoch 2120, training loss: 62.315650939941406 = 0.013554517179727554 + 10.0 * 6.230209827423096
Epoch 2120, val loss: 1.5434993505477905
Epoch 2130, training loss: 62.31422424316406 = 0.013373301364481449 + 10.0 * 6.2300848960876465
Epoch 2130, val loss: 1.5475329160690308
Epoch 2140, training loss: 62.32707595825195 = 0.013200280256569386 + 10.0 * 6.231387615203857
Epoch 2140, val loss: 1.5515443086624146
Epoch 2150, training loss: 62.34601974487305 = 0.013030034489929676 + 10.0 * 6.2332987785339355
Epoch 2150, val loss: 1.5550323724746704
Epoch 2160, training loss: 62.336219787597656 = 0.01285473257303238 + 10.0 * 6.232336521148682
Epoch 2160, val loss: 1.5578160285949707
Epoch 2170, training loss: 62.3371467590332 = 0.012685425579547882 + 10.0 * 6.232446193695068
Epoch 2170, val loss: 1.5621294975280762
Epoch 2180, training loss: 62.3073844909668 = 0.012519069015979767 + 10.0 * 6.229486465454102
Epoch 2180, val loss: 1.5654025077819824
Epoch 2190, training loss: 62.31436538696289 = 0.012362373992800713 + 10.0 * 6.230200290679932
Epoch 2190, val loss: 1.569461464881897
Epoch 2200, training loss: 62.2920036315918 = 0.012205822393298149 + 10.0 * 6.22797966003418
Epoch 2200, val loss: 1.572955846786499
Epoch 2210, training loss: 62.324161529541016 = 0.012059153988957405 + 10.0 * 6.231210231781006
Epoch 2210, val loss: 1.5765866041183472
Epoch 2220, training loss: 62.31499481201172 = 0.011908569373190403 + 10.0 * 6.230308532714844
Epoch 2220, val loss: 1.5798410177230835
Epoch 2230, training loss: 62.28489685058594 = 0.011751416139304638 + 10.0 * 6.227314472198486
Epoch 2230, val loss: 1.5833274126052856
Epoch 2240, training loss: 62.28217315673828 = 0.011606979183852673 + 10.0 * 6.227056503295898
Epoch 2240, val loss: 1.5865896940231323
Epoch 2250, training loss: 62.32346725463867 = 0.011471961624920368 + 10.0 * 6.231199741363525
Epoch 2250, val loss: 1.589766025543213
Epoch 2260, training loss: 62.28622817993164 = 0.011330410838127136 + 10.0 * 6.227489948272705
Epoch 2260, val loss: 1.5939642190933228
Epoch 2270, training loss: 62.2867546081543 = 0.011195007711648941 + 10.0 * 6.227555751800537
Epoch 2270, val loss: 1.5972998142242432
Epoch 2280, training loss: 62.299346923828125 = 0.011061987839639187 + 10.0 * 6.228828430175781
Epoch 2280, val loss: 1.600453495979309
Epoch 2290, training loss: 62.295101165771484 = 0.01093143131583929 + 10.0 * 6.228416919708252
Epoch 2290, val loss: 1.6041795015335083
Epoch 2300, training loss: 62.29427719116211 = 0.010804273188114166 + 10.0 * 6.228347301483154
Epoch 2300, val loss: 1.606731653213501
Epoch 2310, training loss: 62.27007293701172 = 0.01067957654595375 + 10.0 * 6.2259392738342285
Epoch 2310, val loss: 1.610352635383606
Epoch 2320, training loss: 62.30192565917969 = 0.010559366084635258 + 10.0 * 6.2291364669799805
Epoch 2320, val loss: 1.6134998798370361
Epoch 2330, training loss: 62.27813720703125 = 0.01043558306992054 + 10.0 * 6.226769924163818
Epoch 2330, val loss: 1.6164491176605225
Epoch 2340, training loss: 62.271846771240234 = 0.010317737236618996 + 10.0 * 6.2261528968811035
Epoch 2340, val loss: 1.6198861598968506
Epoch 2350, training loss: 62.28192901611328 = 0.010203003883361816 + 10.0 * 6.227172374725342
Epoch 2350, val loss: 1.62288498878479
Epoch 2360, training loss: 62.26310729980469 = 0.010088336654007435 + 10.0 * 6.225301742553711
Epoch 2360, val loss: 1.6261259317398071
Epoch 2370, training loss: 62.25773620605469 = 0.009977745823562145 + 10.0 * 6.224775791168213
Epoch 2370, val loss: 1.6293433904647827
Epoch 2380, training loss: 62.32782745361328 = 0.00986966397613287 + 10.0 * 6.231795787811279
Epoch 2380, val loss: 1.63211190700531
Epoch 2390, training loss: 62.30954360961914 = 0.009761578403413296 + 10.0 * 6.229978084564209
Epoch 2390, val loss: 1.6361024379730225
Epoch 2400, training loss: 62.277137756347656 = 0.009650747291743755 + 10.0 * 6.226748466491699
Epoch 2400, val loss: 1.6384724378585815
Epoch 2410, training loss: 62.25381088256836 = 0.009548467583954334 + 10.0 * 6.22442626953125
Epoch 2410, val loss: 1.6411620378494263
Epoch 2420, training loss: 62.24257278442383 = 0.009448106400668621 + 10.0 * 6.2233123779296875
Epoch 2420, val loss: 1.6444202661514282
Epoch 2430, training loss: 62.251644134521484 = 0.009351428598165512 + 10.0 * 6.224229335784912
Epoch 2430, val loss: 1.6473382711410522
Epoch 2440, training loss: 62.30691909790039 = 0.00925509724766016 + 10.0 * 6.229766368865967
Epoch 2440, val loss: 1.6501106023788452
Epoch 2450, training loss: 62.25920104980469 = 0.00915483944118023 + 10.0 * 6.22500467300415
Epoch 2450, val loss: 1.6534925699234009
Epoch 2460, training loss: 62.24675369262695 = 0.00905904546380043 + 10.0 * 6.223769187927246
Epoch 2460, val loss: 1.6563490629196167
Epoch 2470, training loss: 62.2711296081543 = 0.008967557922005653 + 10.0 * 6.2262163162231445
Epoch 2470, val loss: 1.659102201461792
Epoch 2480, training loss: 62.237125396728516 = 0.008876181207597256 + 10.0 * 6.222825050354004
Epoch 2480, val loss: 1.6617467403411865
Epoch 2490, training loss: 62.2421760559082 = 0.00878884643316269 + 10.0 * 6.223338603973389
Epoch 2490, val loss: 1.6646007299423218
Epoch 2500, training loss: 62.24856185913086 = 0.008701682090759277 + 10.0 * 6.2239861488342285
Epoch 2500, val loss: 1.6675366163253784
Epoch 2510, training loss: 62.2906494140625 = 0.008614659309387207 + 10.0 * 6.228203773498535
Epoch 2510, val loss: 1.6699697971343994
Epoch 2520, training loss: 62.271236419677734 = 0.008531719446182251 + 10.0 * 6.2262701988220215
Epoch 2520, val loss: 1.6725778579711914
Epoch 2530, training loss: 62.241798400878906 = 0.00844298955053091 + 10.0 * 6.2233357429504395
Epoch 2530, val loss: 1.6749898195266724
Epoch 2540, training loss: 62.231258392333984 = 0.00836247205734253 + 10.0 * 6.222289562225342
Epoch 2540, val loss: 1.6781573295593262
Epoch 2550, training loss: 62.21672439575195 = 0.008281966671347618 + 10.0 * 6.220844268798828
Epoch 2550, val loss: 1.6808733940124512
Epoch 2560, training loss: 62.21953201293945 = 0.008205178193747997 + 10.0 * 6.221132755279541
Epoch 2560, val loss: 1.683544397354126
Epoch 2570, training loss: 62.30960464477539 = 0.008136207237839699 + 10.0 * 6.230146884918213
Epoch 2570, val loss: 1.685423731803894
Epoch 2580, training loss: 62.25906753540039 = 0.008049705997109413 + 10.0 * 6.225101947784424
Epoch 2580, val loss: 1.6894004344940186
Epoch 2590, training loss: 62.23455047607422 = 0.007974403910338879 + 10.0 * 6.222657680511475
Epoch 2590, val loss: 1.691332459449768
Epoch 2600, training loss: 62.250892639160156 = 0.007902242243289948 + 10.0 * 6.22429895401001
Epoch 2600, val loss: 1.694578766822815
Epoch 2610, training loss: 62.23063659667969 = 0.00782739371061325 + 10.0 * 6.222280979156494
Epoch 2610, val loss: 1.6967430114746094
Epoch 2620, training loss: 62.233131408691406 = 0.00775301456451416 + 10.0 * 6.222537994384766
Epoch 2620, val loss: 1.6990233659744263
Epoch 2630, training loss: 62.21149444580078 = 0.007682756055146456 + 10.0 * 6.220381259918213
Epoch 2630, val loss: 1.701951026916504
Epoch 2640, training loss: 62.20524597167969 = 0.007615169510245323 + 10.0 * 6.219763278961182
Epoch 2640, val loss: 1.7042977809906006
Epoch 2650, training loss: 62.228729248046875 = 0.00754951685667038 + 10.0 * 6.222117900848389
Epoch 2650, val loss: 1.7066110372543335
Epoch 2660, training loss: 62.24240493774414 = 0.0074819838628172874 + 10.0 * 6.223492622375488
Epoch 2660, val loss: 1.7090486288070679
Epoch 2670, training loss: 62.231136322021484 = 0.007414840627461672 + 10.0 * 6.222372055053711
Epoch 2670, val loss: 1.7116403579711914
Epoch 2680, training loss: 62.21452713012695 = 0.007349833380430937 + 10.0 * 6.220717430114746
Epoch 2680, val loss: 1.713908314704895
Epoch 2690, training loss: 62.23020935058594 = 0.007286574225872755 + 10.0 * 6.222292423248291
Epoch 2690, val loss: 1.7164822816848755
Epoch 2700, training loss: 62.195831298828125 = 0.00722088199108839 + 10.0 * 6.218861103057861
Epoch 2700, val loss: 1.718534231185913
Epoch 2710, training loss: 62.2137565612793 = 0.007159998174756765 + 10.0 * 6.2206597328186035
Epoch 2710, val loss: 1.7209604978561401
Epoch 2720, training loss: 62.224952697753906 = 0.0071012359112501144 + 10.0 * 6.221785068511963
Epoch 2720, val loss: 1.7232155799865723
Epoch 2730, training loss: 62.21804428100586 = 0.007041461765766144 + 10.0 * 6.221100330352783
Epoch 2730, val loss: 1.726121187210083
Epoch 2740, training loss: 62.19512176513672 = 0.006981353275477886 + 10.0 * 6.218813896179199
Epoch 2740, val loss: 1.7282878160476685
Epoch 2750, training loss: 62.21601867675781 = 0.006925760302692652 + 10.0 * 6.220909118652344
Epoch 2750, val loss: 1.7304972410202026
Epoch 2760, training loss: 62.19642639160156 = 0.0068658930249512196 + 10.0 * 6.218955993652344
Epoch 2760, val loss: 1.7326406240463257
Epoch 2770, training loss: 62.21197509765625 = 0.0068100434727966785 + 10.0 * 6.220516681671143
Epoch 2770, val loss: 1.7344504594802856
Epoch 2780, training loss: 62.24691390991211 = 0.00675529008731246 + 10.0 * 6.224015712738037
Epoch 2780, val loss: 1.7371948957443237
Epoch 2790, training loss: 62.198081970214844 = 0.006694766227155924 + 10.0 * 6.2191386222839355
Epoch 2790, val loss: 1.739785075187683
Epoch 2800, training loss: 62.18779373168945 = 0.006640741601586342 + 10.0 * 6.218115329742432
Epoch 2800, val loss: 1.7412651777267456
Epoch 2810, training loss: 62.1815299987793 = 0.0065886760130524635 + 10.0 * 6.217494010925293
Epoch 2810, val loss: 1.7441710233688354
Epoch 2820, training loss: 62.25187683105469 = 0.00653819041326642 + 10.0 * 6.224534034729004
Epoch 2820, val loss: 1.745652675628662
Epoch 2830, training loss: 62.202415466308594 = 0.006484337616711855 + 10.0 * 6.219593048095703
Epoch 2830, val loss: 1.7482776641845703
Epoch 2840, training loss: 62.178627014160156 = 0.006432345137000084 + 10.0 * 6.217219352722168
Epoch 2840, val loss: 1.7502944469451904
Epoch 2850, training loss: 62.169189453125 = 0.006384079344570637 + 10.0 * 6.216280460357666
Epoch 2850, val loss: 1.7523772716522217
Epoch 2860, training loss: 62.172786712646484 = 0.006335912272334099 + 10.0 * 6.216645240783691
Epoch 2860, val loss: 1.7544152736663818
Epoch 2870, training loss: 62.25837707519531 = 0.006289131473749876 + 10.0 * 6.225208759307861
Epoch 2870, val loss: 1.7560731172561646
Epoch 2880, training loss: 62.227264404296875 = 0.006239642389118671 + 10.0 * 6.222102165222168
Epoch 2880, val loss: 1.759354829788208
Epoch 2890, training loss: 62.1870231628418 = 0.0061893584206700325 + 10.0 * 6.218083381652832
Epoch 2890, val loss: 1.7604353427886963
Epoch 2900, training loss: 62.173683166503906 = 0.006143500562757254 + 10.0 * 6.216753959655762
Epoch 2900, val loss: 1.7626187801361084
Epoch 2910, training loss: 62.220455169677734 = 0.006100556347519159 + 10.0 * 6.221435546875
Epoch 2910, val loss: 1.7642818689346313
Epoch 2920, training loss: 62.164424896240234 = 0.006051149684935808 + 10.0 * 6.215837478637695
Epoch 2920, val loss: 1.767135739326477
Epoch 2930, training loss: 62.1724739074707 = 0.006006503012031317 + 10.0 * 6.216646671295166
Epoch 2930, val loss: 1.7687954902648926
Epoch 2940, training loss: 62.18864059448242 = 0.005963495932519436 + 10.0 * 6.218267917633057
Epoch 2940, val loss: 1.770555853843689
Epoch 2950, training loss: 62.17766571044922 = 0.005919283721596003 + 10.0 * 6.217174530029297
Epoch 2950, val loss: 1.7724374532699585
Epoch 2960, training loss: 62.20500564575195 = 0.005877419374883175 + 10.0 * 6.219912528991699
Epoch 2960, val loss: 1.774222731590271
Epoch 2970, training loss: 62.15583801269531 = 0.005833464674651623 + 10.0 * 6.215000629425049
Epoch 2970, val loss: 1.7764310836791992
Epoch 2980, training loss: 62.15229034423828 = 0.0057923332788050175 + 10.0 * 6.2146501541137695
Epoch 2980, val loss: 1.7784501314163208
Epoch 2990, training loss: 62.18069839477539 = 0.005754735786467791 + 10.0 * 6.217494010925293
Epoch 2990, val loss: 1.7800472974777222
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8223510806536637
The final CL Acc:0.70370, 0.01318, The final GNN Acc:0.81936, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13312])
remove edge: torch.Size([2, 7974])
updated graph: torch.Size([2, 10730])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.92089080810547 = 1.9524575471878052 + 10.0 * 8.596842765808105
Epoch 0, val loss: 1.9572912454605103
Epoch 10, training loss: 87.90451049804688 = 1.9426416158676147 + 10.0 * 8.596186637878418
Epoch 10, val loss: 1.9464802742004395
Epoch 20, training loss: 87.8428726196289 = 1.9307446479797363 + 10.0 * 8.591212272644043
Epoch 20, val loss: 1.932937502861023
Epoch 30, training loss: 87.50444030761719 = 1.9158978462219238 + 10.0 * 8.558854103088379
Epoch 30, val loss: 1.9157735109329224
Epoch 40, training loss: 85.71638488769531 = 1.8998985290527344 + 10.0 * 8.381649017333984
Epoch 40, val loss: 1.897706389427185
Epoch 50, training loss: 80.1776351928711 = 1.8839598894119263 + 10.0 * 7.8293681144714355
Epoch 50, val loss: 1.879705786705017
Epoch 60, training loss: 76.05083465576172 = 1.8697662353515625 + 10.0 * 7.418106555938721
Epoch 60, val loss: 1.8653004169464111
Epoch 70, training loss: 73.49276733398438 = 1.8578726053237915 + 10.0 * 7.163489818572998
Epoch 70, val loss: 1.8536810874938965
Epoch 80, training loss: 72.09330749511719 = 1.8442375659942627 + 10.0 * 7.024907112121582
Epoch 80, val loss: 1.8395416736602783
Epoch 90, training loss: 71.18804931640625 = 1.8306010961532593 + 10.0 * 6.935744285583496
Epoch 90, val loss: 1.826032280921936
Epoch 100, training loss: 70.39796447753906 = 1.8176263570785522 + 10.0 * 6.858034133911133
Epoch 100, val loss: 1.8134839534759521
Epoch 110, training loss: 69.67210388183594 = 1.8061330318450928 + 10.0 * 6.786597728729248
Epoch 110, val loss: 1.8020609617233276
Epoch 120, training loss: 69.10185241699219 = 1.7948490381240845 + 10.0 * 6.730700969696045
Epoch 120, val loss: 1.7909636497497559
Epoch 130, training loss: 68.63996124267578 = 1.7831642627716064 + 10.0 * 6.6856794357299805
Epoch 130, val loss: 1.7797298431396484
Epoch 140, training loss: 68.22106170654297 = 1.770758032798767 + 10.0 * 6.6450300216674805
Epoch 140, val loss: 1.7678415775299072
Epoch 150, training loss: 67.87418365478516 = 1.7577834129333496 + 10.0 * 6.611640453338623
Epoch 150, val loss: 1.7557244300842285
Epoch 160, training loss: 67.60587310791016 = 1.743729829788208 + 10.0 * 6.586214542388916
Epoch 160, val loss: 1.7426681518554688
Epoch 170, training loss: 67.35537719726562 = 1.7283036708831787 + 10.0 * 6.562707424163818
Epoch 170, val loss: 1.7282533645629883
Epoch 180, training loss: 67.16010284423828 = 1.711232304573059 + 10.0 * 6.544887065887451
Epoch 180, val loss: 1.71257483959198
Epoch 190, training loss: 66.94686889648438 = 1.6924247741699219 + 10.0 * 6.525444030761719
Epoch 190, val loss: 1.6951587200164795
Epoch 200, training loss: 66.76548767089844 = 1.6716814041137695 + 10.0 * 6.509380340576172
Epoch 200, val loss: 1.676113247871399
Epoch 210, training loss: 66.62770080566406 = 1.6486763954162598 + 10.0 * 6.4979023933410645
Epoch 210, val loss: 1.6552764177322388
Epoch 220, training loss: 66.45712280273438 = 1.6236449480056763 + 10.0 * 6.4833478927612305
Epoch 220, val loss: 1.6324713230133057
Epoch 230, training loss: 66.3416976928711 = 1.5962196588516235 + 10.0 * 6.474547386169434
Epoch 230, val loss: 1.6077853441238403
Epoch 240, training loss: 66.1860122680664 = 1.5665156841278076 + 10.0 * 6.461949825286865
Epoch 240, val loss: 1.5812588930130005
Epoch 250, training loss: 66.06031036376953 = 1.5346332788467407 + 10.0 * 6.452568054199219
Epoch 250, val loss: 1.55312979221344
Epoch 260, training loss: 65.94339752197266 = 1.5006859302520752 + 10.0 * 6.444271087646484
Epoch 260, val loss: 1.5235214233398438
Epoch 270, training loss: 65.83860778808594 = 1.4650399684906006 + 10.0 * 6.437356472015381
Epoch 270, val loss: 1.4924160242080688
Epoch 280, training loss: 65.708984375 = 1.4278935194015503 + 10.0 * 6.4281086921691895
Epoch 280, val loss: 1.4606937170028687
Epoch 290, training loss: 65.60321807861328 = 1.3896281719207764 + 10.0 * 6.421359062194824
Epoch 290, val loss: 1.428257703781128
Epoch 300, training loss: 65.51171112060547 = 1.3501547574996948 + 10.0 * 6.41615629196167
Epoch 300, val loss: 1.3950878381729126
Epoch 310, training loss: 65.38786315917969 = 1.3103828430175781 + 10.0 * 6.407748699188232
Epoch 310, val loss: 1.3622410297393799
Epoch 320, training loss: 65.28482818603516 = 1.2706499099731445 + 10.0 * 6.4014177322387695
Epoch 320, val loss: 1.3297096490859985
Epoch 330, training loss: 65.2042007446289 = 1.2310980558395386 + 10.0 * 6.397310733795166
Epoch 330, val loss: 1.2978241443634033
Epoch 340, training loss: 65.10749053955078 = 1.1922305822372437 + 10.0 * 6.391525745391846
Epoch 340, val loss: 1.2665828466415405
Epoch 350, training loss: 65.00338745117188 = 1.1542617082595825 + 10.0 * 6.384912490844727
Epoch 350, val loss: 1.2368414402008057
Epoch 360, training loss: 64.91499328613281 = 1.117188811302185 + 10.0 * 6.3797807693481445
Epoch 360, val loss: 1.207904577255249
Epoch 370, training loss: 64.84443664550781 = 1.0810002088546753 + 10.0 * 6.376343727111816
Epoch 370, val loss: 1.1800199747085571
Epoch 380, training loss: 64.76449584960938 = 1.0458332300186157 + 10.0 * 6.371866703033447
Epoch 380, val loss: 1.1532564163208008
Epoch 390, training loss: 64.6818618774414 = 1.0115761756896973 + 10.0 * 6.367028713226318
Epoch 390, val loss: 1.127717137336731
Epoch 400, training loss: 64.60496520996094 = 0.9782703518867493 + 10.0 * 6.362669467926025
Epoch 400, val loss: 1.1029661893844604
Epoch 410, training loss: 64.56452941894531 = 0.9460231065750122 + 10.0 * 6.361850738525391
Epoch 410, val loss: 1.0794798135757446
Epoch 420, training loss: 64.47132873535156 = 0.914697527885437 + 10.0 * 6.355662822723389
Epoch 420, val loss: 1.0569978952407837
Epoch 430, training loss: 64.39926147460938 = 0.8843836784362793 + 10.0 * 6.35148811340332
Epoch 430, val loss: 1.035557508468628
Epoch 440, training loss: 64.35179901123047 = 0.8550974130630493 + 10.0 * 6.34967041015625
Epoch 440, val loss: 1.015110969543457
Epoch 450, training loss: 64.2887191772461 = 0.8265833258628845 + 10.0 * 6.3462138175964355
Epoch 450, val loss: 0.9958701729774475
Epoch 460, training loss: 64.23615264892578 = 0.7992562651634216 + 10.0 * 6.343689918518066
Epoch 460, val loss: 0.9775137305259705
Epoch 470, training loss: 64.16978454589844 = 0.772853672504425 + 10.0 * 6.339693546295166
Epoch 470, val loss: 0.9604291915893555
Epoch 480, training loss: 64.12080383300781 = 0.7473666667938232 + 10.0 * 6.337343692779541
Epoch 480, val loss: 0.9445148706436157
Epoch 490, training loss: 64.10579681396484 = 0.7227522134780884 + 10.0 * 6.33830451965332
Epoch 490, val loss: 0.9291253089904785
Epoch 500, training loss: 64.02039337158203 = 0.6990165114402771 + 10.0 * 6.3321380615234375
Epoch 500, val loss: 0.9151363372802734
Epoch 510, training loss: 63.96377944946289 = 0.6761385798454285 + 10.0 * 6.328763961791992
Epoch 510, val loss: 0.9021214246749878
Epoch 520, training loss: 63.919456481933594 = 0.654022216796875 + 10.0 * 6.32654333114624
Epoch 520, val loss: 0.8900147080421448
Epoch 530, training loss: 63.888648986816406 = 0.6324844360351562 + 10.0 * 6.325616359710693
Epoch 530, val loss: 0.8784588575363159
Epoch 540, training loss: 63.83282470703125 = 0.611729621887207 + 10.0 * 6.322109699249268
Epoch 540, val loss: 0.867887020111084
Epoch 550, training loss: 63.792415618896484 = 0.5915849804878235 + 10.0 * 6.320083141326904
Epoch 550, val loss: 0.858153223991394
Epoch 560, training loss: 63.753211975097656 = 0.5720740556716919 + 10.0 * 6.318113803863525
Epoch 560, val loss: 0.8491331338882446
Epoch 570, training loss: 63.78359603881836 = 0.5530120730400085 + 10.0 * 6.323058128356934
Epoch 570, val loss: 0.840721845626831
Epoch 580, training loss: 63.677642822265625 = 0.5345986485481262 + 10.0 * 6.314304351806641
Epoch 580, val loss: 0.832690954208374
Epoch 590, training loss: 63.66009521484375 = 0.5166481733322144 + 10.0 * 6.314344882965088
Epoch 590, val loss: 0.8252745270729065
Epoch 600, training loss: 63.605018615722656 = 0.49907177686691284 + 10.0 * 6.31059455871582
Epoch 600, val loss: 0.8185361623764038
Epoch 610, training loss: 63.57611083984375 = 0.4820103645324707 + 10.0 * 6.309410095214844
Epoch 610, val loss: 0.8121015429496765
Epoch 620, training loss: 63.62929916381836 = 0.46539533138275146 + 10.0 * 6.316390037536621
Epoch 620, val loss: 0.8059982657432556
Epoch 630, training loss: 63.51402282714844 = 0.4490737318992615 + 10.0 * 6.30649471282959
Epoch 630, val loss: 0.8004937767982483
Epoch 640, training loss: 63.48019027709961 = 0.43323811888694763 + 10.0 * 6.304695129394531
Epoch 640, val loss: 0.7953150868415833
Epoch 650, training loss: 63.44480514526367 = 0.4178279638290405 + 10.0 * 6.302697658538818
Epoch 650, val loss: 0.79047691822052
Epoch 660, training loss: 63.452701568603516 = 0.40279093384742737 + 10.0 * 6.304991245269775
Epoch 660, val loss: 0.7858793139457703
Epoch 670, training loss: 63.420555114746094 = 0.3880428671836853 + 10.0 * 6.303251266479492
Epoch 670, val loss: 0.7815534472465515
Epoch 680, training loss: 63.36168670654297 = 0.37372496724128723 + 10.0 * 6.2987961769104
Epoch 680, val loss: 0.7775675058364868
Epoch 690, training loss: 63.33395004272461 = 0.3598211705684662 + 10.0 * 6.297412872314453
Epoch 690, val loss: 0.7739428281784058
Epoch 700, training loss: 63.31662368774414 = 0.3463168144226074 + 10.0 * 6.297030448913574
Epoch 700, val loss: 0.7705284357070923
Epoch 710, training loss: 63.30083084106445 = 0.33309152722358704 + 10.0 * 6.296773910522461
Epoch 710, val loss: 0.7675814032554626
Epoch 720, training loss: 63.27574157714844 = 0.3203037679195404 + 10.0 * 6.295543670654297
Epoch 720, val loss: 0.7646623849868774
Epoch 730, training loss: 63.239044189453125 = 0.3079383969306946 + 10.0 * 6.2931108474731445
Epoch 730, val loss: 0.7623133659362793
Epoch 740, training loss: 63.21009063720703 = 0.2959524095058441 + 10.0 * 6.2914137840271
Epoch 740, val loss: 0.7602767944335938
Epoch 750, training loss: 63.243961334228516 = 0.2843528687953949 + 10.0 * 6.295960903167725
Epoch 750, val loss: 0.7584707140922546
Epoch 760, training loss: 63.19563674926758 = 0.27308008074760437 + 10.0 * 6.292255878448486
Epoch 760, val loss: 0.7568811774253845
Epoch 770, training loss: 63.142608642578125 = 0.26218435168266296 + 10.0 * 6.2880425453186035
Epoch 770, val loss: 0.7557623982429504
Epoch 780, training loss: 63.13422393798828 = 0.2517155706882477 + 10.0 * 6.288250923156738
Epoch 780, val loss: 0.7548873424530029
Epoch 790, training loss: 63.143516540527344 = 0.24160176515579224 + 10.0 * 6.290191650390625
Epoch 790, val loss: 0.7542375922203064
Epoch 800, training loss: 63.09013748168945 = 0.23193030059337616 + 10.0 * 6.285820960998535
Epoch 800, val loss: 0.7537367939949036
Epoch 810, training loss: 63.061885833740234 = 0.22256627678871155 + 10.0 * 6.283932209014893
Epoch 810, val loss: 0.7538835406303406
Epoch 820, training loss: 63.050376892089844 = 0.21362991631031036 + 10.0 * 6.283674716949463
Epoch 820, val loss: 0.7540243268013
Epoch 830, training loss: 63.05815505981445 = 0.20498570799827576 + 10.0 * 6.2853169441223145
Epoch 830, val loss: 0.7546059489250183
Epoch 840, training loss: 63.028167724609375 = 0.1967269629240036 + 10.0 * 6.283143997192383
Epoch 840, val loss: 0.7552394270896912
Epoch 850, training loss: 62.986305236816406 = 0.18883326649665833 + 10.0 * 6.279747009277344
Epoch 850, val loss: 0.7564977407455444
Epoch 860, training loss: 62.97217559814453 = 0.18131133913993835 + 10.0 * 6.279086112976074
Epoch 860, val loss: 0.7580021619796753
Epoch 870, training loss: 63.020751953125 = 0.17412754893302917 + 10.0 * 6.28466272354126
Epoch 870, val loss: 0.759639322757721
Epoch 880, training loss: 62.994319915771484 = 0.16717226803302765 + 10.0 * 6.28271484375
Epoch 880, val loss: 0.761353611946106
Epoch 890, training loss: 62.92301940917969 = 0.160544291138649 + 10.0 * 6.276247501373291
Epoch 890, val loss: 0.7636541724205017
Epoch 900, training loss: 62.908878326416016 = 0.15425293147563934 + 10.0 * 6.275462627410889
Epoch 900, val loss: 0.7661392688751221
Epoch 910, training loss: 62.92686080932617 = 0.14823222160339355 + 10.0 * 6.277863025665283
Epoch 910, val loss: 0.7689405083656311
Epoch 920, training loss: 62.892181396484375 = 0.14249014854431152 + 10.0 * 6.274969100952148
Epoch 920, val loss: 0.7712059617042542
Epoch 930, training loss: 62.882686614990234 = 0.13694460690021515 + 10.0 * 6.274574279785156
Epoch 930, val loss: 0.7744875550270081
Epoch 940, training loss: 62.864540100097656 = 0.13172593712806702 + 10.0 * 6.273281574249268
Epoch 940, val loss: 0.7775409817695618
Epoch 950, training loss: 62.8524284362793 = 0.12670359015464783 + 10.0 * 6.2725725173950195
Epoch 950, val loss: 0.7808181643486023
Epoch 960, training loss: 62.83766555786133 = 0.12187526375055313 + 10.0 * 6.271578788757324
Epoch 960, val loss: 0.7846496105194092
Epoch 970, training loss: 62.8169059753418 = 0.11732878535985947 + 10.0 * 6.269957542419434
Epoch 970, val loss: 0.7881430983543396
Epoch 980, training loss: 62.85367965698242 = 0.11295536160469055 + 10.0 * 6.274072170257568
Epoch 980, val loss: 0.7919759154319763
Epoch 990, training loss: 62.79143524169922 = 0.10873017460107803 + 10.0 * 6.268270492553711
Epoch 990, val loss: 0.7961427569389343
Epoch 1000, training loss: 62.77800369262695 = 0.1047479435801506 + 10.0 * 6.267325401306152
Epoch 1000, val loss: 0.8002197742462158
Epoch 1010, training loss: 62.856319427490234 = 0.10092002898454666 + 10.0 * 6.275539875030518
Epoch 1010, val loss: 0.8046525120735168
Epoch 1020, training loss: 62.769405364990234 = 0.097261443734169 + 10.0 * 6.267214298248291
Epoch 1020, val loss: 0.8086022138595581
Epoch 1030, training loss: 62.74724578857422 = 0.09378017485141754 + 10.0 * 6.265346527099609
Epoch 1030, val loss: 0.8130614757537842
Epoch 1040, training loss: 62.76111602783203 = 0.09044443815946579 + 10.0 * 6.2670674324035645
Epoch 1040, val loss: 0.8177112936973572
Epoch 1050, training loss: 62.748390197753906 = 0.08725760132074356 + 10.0 * 6.26611328125
Epoch 1050, val loss: 0.8224028944969177
Epoch 1060, training loss: 62.745445251464844 = 0.08422115445137024 + 10.0 * 6.266122341156006
Epoch 1060, val loss: 0.8267768621444702
Epoch 1070, training loss: 62.71261215209961 = 0.08131073415279388 + 10.0 * 6.263130187988281
Epoch 1070, val loss: 0.8316289186477661
Epoch 1080, training loss: 62.69719314575195 = 0.07851999998092651 + 10.0 * 6.261867046356201
Epoch 1080, val loss: 0.8364496231079102
Epoch 1090, training loss: 62.72509765625 = 0.07585814595222473 + 10.0 * 6.264924049377441
Epoch 1090, val loss: 0.8414197564125061
Epoch 1100, training loss: 62.679866790771484 = 0.07330004870891571 + 10.0 * 6.260656833648682
Epoch 1100, val loss: 0.8462508916854858
Epoch 1110, training loss: 62.661128997802734 = 0.07085778564214706 + 10.0 * 6.259027004241943
Epoch 1110, val loss: 0.8512481451034546
Epoch 1120, training loss: 62.72762680053711 = 0.06851810961961746 + 10.0 * 6.265910625457764
Epoch 1120, val loss: 0.8564764261245728
Epoch 1130, training loss: 62.695987701416016 = 0.06628064066171646 + 10.0 * 6.262970924377441
Epoch 1130, val loss: 0.8608515858650208
Epoch 1140, training loss: 62.645511627197266 = 0.06410378217697144 + 10.0 * 6.258141040802002
Epoch 1140, val loss: 0.8660292029380798
Epoch 1150, training loss: 62.625953674316406 = 0.06206485629081726 + 10.0 * 6.2563886642456055
Epoch 1150, val loss: 0.8710845112800598
Epoch 1160, training loss: 62.64849853515625 = 0.060115326195955276 + 10.0 * 6.258838176727295
Epoch 1160, val loss: 0.8759855031967163
Epoch 1170, training loss: 62.612247467041016 = 0.05821087583899498 + 10.0 * 6.255403518676758
Epoch 1170, val loss: 0.8812220692634583
Epoch 1180, training loss: 62.63789749145508 = 0.05641256272792816 + 10.0 * 6.258148670196533
Epoch 1180, val loss: 0.8858805298805237
Epoch 1190, training loss: 62.603309631347656 = 0.054655689746141434 + 10.0 * 6.2548651695251465
Epoch 1190, val loss: 0.8913633823394775
Epoch 1200, training loss: 62.59621810913086 = 0.052990470081567764 + 10.0 * 6.2543230056762695
Epoch 1200, val loss: 0.8963115811347961
Epoch 1210, training loss: 62.59978103637695 = 0.05139588937163353 + 10.0 * 6.254838466644287
Epoch 1210, val loss: 0.9015004634857178
Epoch 1220, training loss: 62.60310745239258 = 0.04985426738858223 + 10.0 * 6.2553253173828125
Epoch 1220, val loss: 0.9064532518386841
Epoch 1230, training loss: 62.56576919555664 = 0.048377372324466705 + 10.0 * 6.251739025115967
Epoch 1230, val loss: 0.911482036113739
Epoch 1240, training loss: 62.55774688720703 = 0.04696498066186905 + 10.0 * 6.251078128814697
Epoch 1240, val loss: 0.9166083931922913
Epoch 1250, training loss: 62.56222152709961 = 0.04561127722263336 + 10.0 * 6.2516608238220215
Epoch 1250, val loss: 0.9216906428337097
Epoch 1260, training loss: 62.57705307006836 = 0.04430932179093361 + 10.0 * 6.253274440765381
Epoch 1260, val loss: 0.9265040755271912
Epoch 1270, training loss: 62.56950759887695 = 0.043046966195106506 + 10.0 * 6.252645969390869
Epoch 1270, val loss: 0.9313358664512634
Epoch 1280, training loss: 62.559688568115234 = 0.04183141887187958 + 10.0 * 6.251785755157471
Epoch 1280, val loss: 0.936259388923645
Epoch 1290, training loss: 62.527915954589844 = 0.040661104023456573 + 10.0 * 6.248725414276123
Epoch 1290, val loss: 0.941644549369812
Epoch 1300, training loss: 62.53459167480469 = 0.03954726830124855 + 10.0 * 6.249504566192627
Epoch 1300, val loss: 0.9466403722763062
Epoch 1310, training loss: 62.579986572265625 = 0.03846430033445358 + 10.0 * 6.254152297973633
Epoch 1310, val loss: 0.9513341784477234
Epoch 1320, training loss: 62.529537200927734 = 0.03744226694107056 + 10.0 * 6.249209403991699
Epoch 1320, val loss: 0.9557892680168152
Epoch 1330, training loss: 62.5098762512207 = 0.036429714411497116 + 10.0 * 6.247344493865967
Epoch 1330, val loss: 0.9606971740722656
Epoch 1340, training loss: 62.51248550415039 = 0.03548016399145126 + 10.0 * 6.2477006912231445
Epoch 1340, val loss: 0.9654649496078491
Epoch 1350, training loss: 62.5058479309082 = 0.03455287590622902 + 10.0 * 6.247129440307617
Epoch 1350, val loss: 0.9702854752540588
Epoch 1360, training loss: 62.50371170043945 = 0.03365645557641983 + 10.0 * 6.247005462646484
Epoch 1360, val loss: 0.9752919673919678
Epoch 1370, training loss: 62.500579833984375 = 0.03279765322804451 + 10.0 * 6.2467780113220215
Epoch 1370, val loss: 0.9798566102981567
Epoch 1380, training loss: 62.486236572265625 = 0.031967949122190475 + 10.0 * 6.245427131652832
Epoch 1380, val loss: 0.9842243194580078
Epoch 1390, training loss: 62.48092269897461 = 0.03116600587964058 + 10.0 * 6.244975566864014
Epoch 1390, val loss: 0.9890339970588684
Epoch 1400, training loss: 62.467132568359375 = 0.030400479212403297 + 10.0 * 6.243673324584961
Epoch 1400, val loss: 0.9935106039047241
Epoch 1410, training loss: 62.4595832824707 = 0.02965831197798252 + 10.0 * 6.242992401123047
Epoch 1410, val loss: 0.9981163740158081
Epoch 1420, training loss: 62.51752471923828 = 0.028950057923793793 + 10.0 * 6.248857498168945
Epoch 1420, val loss: 1.0025326013565063
Epoch 1430, training loss: 62.48627853393555 = 0.028239533305168152 + 10.0 * 6.2458038330078125
Epoch 1430, val loss: 1.006978988647461
Epoch 1440, training loss: 62.45662307739258 = 0.02756870537996292 + 10.0 * 6.242905616760254
Epoch 1440, val loss: 1.0112674236297607
Epoch 1450, training loss: 62.44725799560547 = 0.0269161406904459 + 10.0 * 6.242033958435059
Epoch 1450, val loss: 1.016034483909607
Epoch 1460, training loss: 62.431880950927734 = 0.026295678690075874 + 10.0 * 6.240558624267578
Epoch 1460, val loss: 1.020370364189148
Epoch 1470, training loss: 62.448490142822266 = 0.025696421042084694 + 10.0 * 6.242279529571533
Epoch 1470, val loss: 1.0247530937194824
Epoch 1480, training loss: 62.43352508544922 = 0.025116316974163055 + 10.0 * 6.240840911865234
Epoch 1480, val loss: 1.028777003288269
Epoch 1490, training loss: 62.45619201660156 = 0.02454853430390358 + 10.0 * 6.243164539337158
Epoch 1490, val loss: 1.0329762697219849
Epoch 1500, training loss: 62.420188903808594 = 0.023993678390979767 + 10.0 * 6.239619255065918
Epoch 1500, val loss: 1.0374367237091064
Epoch 1510, training loss: 62.40309524536133 = 0.02346418984234333 + 10.0 * 6.2379631996154785
Epoch 1510, val loss: 1.04153311252594
Epoch 1520, training loss: 62.415138244628906 = 0.022963881492614746 + 10.0 * 6.239217281341553
Epoch 1520, val loss: 1.0455100536346436
Epoch 1530, training loss: 62.4262809753418 = 0.02246771566569805 + 10.0 * 6.240381240844727
Epoch 1530, val loss: 1.0497323274612427
Epoch 1540, training loss: 62.425472259521484 = 0.02198437601327896 + 10.0 * 6.240348815917969
Epoch 1540, val loss: 1.0538324117660522
Epoch 1550, training loss: 62.394447326660156 = 0.021516021341085434 + 10.0 * 6.237293243408203
Epoch 1550, val loss: 1.05795419216156
Epoch 1560, training loss: 62.38474655151367 = 0.0210652444511652 + 10.0 * 6.236368179321289
Epoch 1560, val loss: 1.0621600151062012
Epoch 1570, training loss: 62.44370651245117 = 0.020634425804018974 + 10.0 * 6.242307186126709
Epoch 1570, val loss: 1.0662342309951782
Epoch 1580, training loss: 62.407344818115234 = 0.020208144560456276 + 10.0 * 6.23871374130249
Epoch 1580, val loss: 1.0697952508926392
Epoch 1590, training loss: 62.42058181762695 = 0.019799338653683662 + 10.0 * 6.240078449249268
Epoch 1590, val loss: 1.0734775066375732
Epoch 1600, training loss: 62.38665771484375 = 0.019394557923078537 + 10.0 * 6.2367262840271
Epoch 1600, val loss: 1.0776923894882202
Epoch 1610, training loss: 62.3641357421875 = 0.019004950299859047 + 10.0 * 6.234513282775879
Epoch 1610, val loss: 1.0816504955291748
Epoch 1620, training loss: 62.35771179199219 = 0.0186354611068964 + 10.0 * 6.233907699584961
Epoch 1620, val loss: 1.085373044013977
Epoch 1630, training loss: 62.350040435791016 = 0.018274370580911636 + 10.0 * 6.233176231384277
Epoch 1630, val loss: 1.0892860889434814
Epoch 1640, training loss: 62.42153549194336 = 0.017926041036844254 + 10.0 * 6.240361213684082
Epoch 1640, val loss: 1.0931813716888428
Epoch 1650, training loss: 62.38207244873047 = 0.017577476799488068 + 10.0 * 6.236449241638184
Epoch 1650, val loss: 1.0965404510498047
Epoch 1660, training loss: 62.38154602050781 = 0.017236197367310524 + 10.0 * 6.236431121826172
Epoch 1660, val loss: 1.1006306409835815
Epoch 1670, training loss: 62.35445785522461 = 0.01691189594566822 + 10.0 * 6.233754634857178
Epoch 1670, val loss: 1.1043034791946411
Epoch 1680, training loss: 62.3443603515625 = 0.016601474955677986 + 10.0 * 6.232775688171387
Epoch 1680, val loss: 1.1074177026748657
Epoch 1690, training loss: 62.33258056640625 = 0.016296714544296265 + 10.0 * 6.23162841796875
Epoch 1690, val loss: 1.1112949848175049
Epoch 1700, training loss: 62.383968353271484 = 0.01600014977157116 + 10.0 * 6.236796855926514
Epoch 1700, val loss: 1.1149177551269531
Epoch 1710, training loss: 62.37729263305664 = 0.015708018094301224 + 10.0 * 6.23615837097168
Epoch 1710, val loss: 1.1182118654251099
Epoch 1720, training loss: 62.342201232910156 = 0.015420663170516491 + 10.0 * 6.232678413391113
Epoch 1720, val loss: 1.1217772960662842
Epoch 1730, training loss: 62.315303802490234 = 0.015147297643125057 + 10.0 * 6.230015754699707
Epoch 1730, val loss: 1.1252583265304565
Epoch 1740, training loss: 62.3162727355957 = 0.014883637428283691 + 10.0 * 6.230138778686523
Epoch 1740, val loss: 1.1289671659469604
Epoch 1750, training loss: 62.3599967956543 = 0.014625493437051773 + 10.0 * 6.234537124633789
Epoch 1750, val loss: 1.1327193975448608
Epoch 1760, training loss: 62.331478118896484 = 0.01437490712851286 + 10.0 * 6.231710433959961
Epoch 1760, val loss: 1.1353237628936768
Epoch 1770, training loss: 62.32469940185547 = 0.014123770408332348 + 10.0 * 6.231057643890381
Epoch 1770, val loss: 1.138986349105835
Epoch 1780, training loss: 62.306793212890625 = 0.013884971849620342 + 10.0 * 6.229290962219238
Epoch 1780, val loss: 1.1420050859451294
Epoch 1790, training loss: 62.298221588134766 = 0.01365216076374054 + 10.0 * 6.228456974029541
Epoch 1790, val loss: 1.1454637050628662
Epoch 1800, training loss: 62.295814514160156 = 0.01342711877077818 + 10.0 * 6.228238582611084
Epoch 1800, val loss: 1.1488374471664429
Epoch 1810, training loss: 62.36372375488281 = 0.013207104988396168 + 10.0 * 6.23505163192749
Epoch 1810, val loss: 1.1521785259246826
Epoch 1820, training loss: 62.322532653808594 = 0.012992327101528645 + 10.0 * 6.230954170227051
Epoch 1820, val loss: 1.1545028686523438
Epoch 1830, training loss: 62.299198150634766 = 0.01277594082057476 + 10.0 * 6.228642463684082
Epoch 1830, val loss: 1.1579017639160156
Epoch 1840, training loss: 62.29352951049805 = 0.01257252600044012 + 10.0 * 6.228095531463623
Epoch 1840, val loss: 1.1609567403793335
Epoch 1850, training loss: 62.30978775024414 = 0.012372461147606373 + 10.0 * 6.22974157333374
Epoch 1850, val loss: 1.1644082069396973
Epoch 1860, training loss: 62.330684661865234 = 0.012172888964414597 + 10.0 * 6.231851100921631
Epoch 1860, val loss: 1.1677720546722412
Epoch 1870, training loss: 62.28569793701172 = 0.011982971802353859 + 10.0 * 6.227371692657471
Epoch 1870, val loss: 1.1701408624649048
Epoch 1880, training loss: 62.269081115722656 = 0.01179666630923748 + 10.0 * 6.225728511810303
Epoch 1880, val loss: 1.1735328435897827
Epoch 1890, training loss: 62.26307678222656 = 0.01161819975823164 + 10.0 * 6.2251458168029785
Epoch 1890, val loss: 1.1764391660690308
Epoch 1900, training loss: 62.3648796081543 = 0.011444698087871075 + 10.0 * 6.2353434562683105
Epoch 1900, val loss: 1.1795099973678589
Epoch 1910, training loss: 62.332542419433594 = 0.011262413114309311 + 10.0 * 6.232128143310547
Epoch 1910, val loss: 1.1824620962142944
Epoch 1920, training loss: 62.26060485839844 = 0.01109189074486494 + 10.0 * 6.224951267242432
Epoch 1920, val loss: 1.1849286556243896
Epoch 1930, training loss: 62.25520324707031 = 0.010927146300673485 + 10.0 * 6.224427700042725
Epoch 1930, val loss: 1.1878385543823242
Epoch 1940, training loss: 62.2512092590332 = 0.010767477564513683 + 10.0 * 6.224043846130371
Epoch 1940, val loss: 1.1908328533172607
Epoch 1950, training loss: 62.26414108276367 = 0.010615481995046139 + 10.0 * 6.2253522872924805
Epoch 1950, val loss: 1.1934114694595337
Epoch 1960, training loss: 62.290382385253906 = 0.010461205616593361 + 10.0 * 6.227992057800293
Epoch 1960, val loss: 1.1962406635284424
Epoch 1970, training loss: 62.29533004760742 = 0.01030243095010519 + 10.0 * 6.2285027503967285
Epoch 1970, val loss: 1.1994966268539429
Epoch 1980, training loss: 62.273258209228516 = 0.010156824253499508 + 10.0 * 6.226309776306152
Epoch 1980, val loss: 1.2016043663024902
Epoch 1990, training loss: 62.266815185546875 = 0.01000850647687912 + 10.0 * 6.225680351257324
Epoch 1990, val loss: 1.2046258449554443
Epoch 2000, training loss: 62.25717544555664 = 0.009869685396552086 + 10.0 * 6.224730491638184
Epoch 2000, val loss: 1.2074060440063477
Epoch 2010, training loss: 62.25361633300781 = 0.009731154888868332 + 10.0 * 6.224388599395752
Epoch 2010, val loss: 1.2102360725402832
Epoch 2020, training loss: 62.246055603027344 = 0.00959738902747631 + 10.0 * 6.2236456871032715
Epoch 2020, val loss: 1.2126644849777222
Epoch 2030, training loss: 62.2435417175293 = 0.00946543924510479 + 10.0 * 6.223407745361328
Epoch 2030, val loss: 1.2154730558395386
Epoch 2040, training loss: 62.25045394897461 = 0.00933860708028078 + 10.0 * 6.224111557006836
Epoch 2040, val loss: 1.217933177947998
Epoch 2050, training loss: 62.27128982543945 = 0.0092105558142066 + 10.0 * 6.226208209991455
Epoch 2050, val loss: 1.22042977809906
Epoch 2060, training loss: 62.244590759277344 = 0.009085721336305141 + 10.0 * 6.223550319671631
Epoch 2060, val loss: 1.2229009866714478
Epoch 2070, training loss: 62.231327056884766 = 0.008965106680989265 + 10.0 * 6.222236156463623
Epoch 2070, val loss: 1.2253087759017944
Epoch 2080, training loss: 62.223655700683594 = 0.008848262950778008 + 10.0 * 6.221480369567871
Epoch 2080, val loss: 1.228040337562561
Epoch 2090, training loss: 62.23157501220703 = 0.00873588863760233 + 10.0 * 6.222283840179443
Epoch 2090, val loss: 1.2304062843322754
Epoch 2100, training loss: 62.24565505981445 = 0.008622684516012669 + 10.0 * 6.223703384399414
Epoch 2100, val loss: 1.2329803705215454
Epoch 2110, training loss: 62.24959945678711 = 0.008509756065905094 + 10.0 * 6.224108695983887
Epoch 2110, val loss: 1.2353090047836304
Epoch 2120, training loss: 62.232391357421875 = 0.0084005082026124 + 10.0 * 6.2223992347717285
Epoch 2120, val loss: 1.237877607345581
Epoch 2130, training loss: 62.21003723144531 = 0.008291300386190414 + 10.0 * 6.220174312591553
Epoch 2130, val loss: 1.240288257598877
Epoch 2140, training loss: 62.21025848388672 = 0.008188970386981964 + 10.0 * 6.2202067375183105
Epoch 2140, val loss: 1.2430037260055542
Epoch 2150, training loss: 62.2563591003418 = 0.008087130263447762 + 10.0 * 6.224827289581299
Epoch 2150, val loss: 1.2455518245697021
Epoch 2160, training loss: 62.21710968017578 = 0.007986409589648247 + 10.0 * 6.220911979675293
Epoch 2160, val loss: 1.2474478483200073
Epoch 2170, training loss: 62.20597457885742 = 0.007887820713222027 + 10.0 * 6.219808578491211
Epoch 2170, val loss: 1.2496708631515503
Epoch 2180, training loss: 62.22734451293945 = 0.007792568765580654 + 10.0 * 6.221955299377441
Epoch 2180, val loss: 1.252248764038086
Epoch 2190, training loss: 62.207096099853516 = 0.007698293309658766 + 10.0 * 6.219939708709717
Epoch 2190, val loss: 1.2543035745620728
Epoch 2200, training loss: 62.2226448059082 = 0.0076072802767157555 + 10.0 * 6.221503734588623
Epoch 2200, val loss: 1.2565447092056274
Epoch 2210, training loss: 62.20981216430664 = 0.007514664437621832 + 10.0 * 6.220229625701904
Epoch 2210, val loss: 1.259069800376892
Epoch 2220, training loss: 62.20492935180664 = 0.0074273282662034035 + 10.0 * 6.21975040435791
Epoch 2220, val loss: 1.2610327005386353
Epoch 2230, training loss: 62.20809555053711 = 0.007339848671108484 + 10.0 * 6.220075607299805
Epoch 2230, val loss: 1.2630761861801147
Epoch 2240, training loss: 62.214805603027344 = 0.007254944648593664 + 10.0 * 6.220755100250244
Epoch 2240, val loss: 1.2653645277023315
Epoch 2250, training loss: 62.193992614746094 = 0.007170920260250568 + 10.0 * 6.218682289123535
Epoch 2250, val loss: 1.2674245834350586
Epoch 2260, training loss: 62.1876220703125 = 0.007089573889970779 + 10.0 * 6.218053340911865
Epoch 2260, val loss: 1.2696703672409058
Epoch 2270, training loss: 62.205570220947266 = 0.007009116467088461 + 10.0 * 6.219856262207031
Epoch 2270, val loss: 1.2719368934631348
Epoch 2280, training loss: 62.215877532958984 = 0.006930530071258545 + 10.0 * 6.220894813537598
Epoch 2280, val loss: 1.2735966444015503
Epoch 2290, training loss: 62.194114685058594 = 0.006851725745946169 + 10.0 * 6.21872615814209
Epoch 2290, val loss: 1.2755405902862549
Epoch 2300, training loss: 62.17828369140625 = 0.00677501829341054 + 10.0 * 6.217150688171387
Epoch 2300, val loss: 1.2774626016616821
Epoch 2310, training loss: 62.17233657836914 = 0.0067018549889326096 + 10.0 * 6.2165632247924805
Epoch 2310, val loss: 1.279503345489502
Epoch 2320, training loss: 62.19712448120117 = 0.006632168311625719 + 10.0 * 6.219048976898193
Epoch 2320, val loss: 1.2812564373016357
Epoch 2330, training loss: 62.1754035949707 = 0.006558055989444256 + 10.0 * 6.216884613037109
Epoch 2330, val loss: 1.28341543674469
Epoch 2340, training loss: 62.1826171875 = 0.006485534831881523 + 10.0 * 6.217613220214844
Epoch 2340, val loss: 1.2856647968292236
Epoch 2350, training loss: 62.201148986816406 = 0.006417274009436369 + 10.0 * 6.219473361968994
Epoch 2350, val loss: 1.2871934175491333
Epoch 2360, training loss: 62.17584228515625 = 0.006347307004034519 + 10.0 * 6.216949462890625
Epoch 2360, val loss: 1.2892248630523682
Epoch 2370, training loss: 62.2025146484375 = 0.006283732131123543 + 10.0 * 6.21962308883667
Epoch 2370, val loss: 1.290774941444397
Epoch 2380, training loss: 62.18075180053711 = 0.006213742308318615 + 10.0 * 6.217453956604004
Epoch 2380, val loss: 1.293095588684082
Epoch 2390, training loss: 62.17741775512695 = 0.006148067303001881 + 10.0 * 6.217126846313477
Epoch 2390, val loss: 1.2948487997055054
Epoch 2400, training loss: 62.15999984741211 = 0.006086023990064859 + 10.0 * 6.215391635894775
Epoch 2400, val loss: 1.2964857816696167
Epoch 2410, training loss: 62.14792251586914 = 0.006025040987879038 + 10.0 * 6.2141900062561035
Epoch 2410, val loss: 1.2983444929122925
Epoch 2420, training loss: 62.16764831542969 = 0.005965414457023144 + 10.0 * 6.216168403625488
Epoch 2420, val loss: 1.3002952337265015
Epoch 2430, training loss: 62.17570495605469 = 0.005904678255319595 + 10.0 * 6.21697998046875
Epoch 2430, val loss: 1.3020228147506714
Epoch 2440, training loss: 62.15851593017578 = 0.005844373255968094 + 10.0 * 6.215267181396484
Epoch 2440, val loss: 1.3035314083099365
Epoch 2450, training loss: 62.16706085205078 = 0.0057867225259542465 + 10.0 * 6.216127395629883
Epoch 2450, val loss: 1.305190086364746
Epoch 2460, training loss: 62.144500732421875 = 0.005729941185563803 + 10.0 * 6.213877201080322
Epoch 2460, val loss: 1.306931495666504
Epoch 2470, training loss: 62.16170120239258 = 0.005675850436091423 + 10.0 * 6.215602397918701
Epoch 2470, val loss: 1.3084888458251953
Epoch 2480, training loss: 62.1900634765625 = 0.005621658638119698 + 10.0 * 6.218443870544434
Epoch 2480, val loss: 1.3101329803466797
Epoch 2490, training loss: 62.16278076171875 = 0.005561213940382004 + 10.0 * 6.21572208404541
Epoch 2490, val loss: 1.3123399019241333
Epoch 2500, training loss: 62.17394256591797 = 0.005508543457835913 + 10.0 * 6.216843605041504
Epoch 2500, val loss: 1.3136513233184814
Epoch 2510, training loss: 62.14109420776367 = 0.0054559665732085705 + 10.0 * 6.213563919067383
Epoch 2510, val loss: 1.3152045011520386
Epoch 2520, training loss: 62.1554069519043 = 0.0054060323163867 + 10.0 * 6.215000152587891
Epoch 2520, val loss: 1.316644549369812
Epoch 2530, training loss: 62.14841079711914 = 0.00535506522282958 + 10.0 * 6.214305400848389
Epoch 2530, val loss: 1.3185094594955444
Epoch 2540, training loss: 62.135948181152344 = 0.005305429454892874 + 10.0 * 6.213064193725586
Epoch 2540, val loss: 1.3201344013214111
Epoch 2550, training loss: 62.13270568847656 = 0.0052568926475942135 + 10.0 * 6.21274471282959
Epoch 2550, val loss: 1.3215936422348022
Epoch 2560, training loss: 62.18351364135742 = 0.0052098473533988 + 10.0 * 6.217830181121826
Epoch 2560, val loss: 1.3232076168060303
Epoch 2570, training loss: 62.16794204711914 = 0.005162157583981752 + 10.0 * 6.216278076171875
Epoch 2570, val loss: 1.3241028785705566
Epoch 2580, training loss: 62.140541076660156 = 0.005113612394779921 + 10.0 * 6.213542461395264
Epoch 2580, val loss: 1.3259196281433105
Epoch 2590, training loss: 62.13556671142578 = 0.0050672017969191074 + 10.0 * 6.21304988861084
Epoch 2590, val loss: 1.3274900913238525
Epoch 2600, training loss: 62.13441848754883 = 0.005021920893341303 + 10.0 * 6.212939739227295
Epoch 2600, val loss: 1.3292686939239502
Epoch 2610, training loss: 62.14630126953125 = 0.004978013690561056 + 10.0 * 6.214132308959961
Epoch 2610, val loss: 1.3307512998580933
Epoch 2620, training loss: 62.143524169921875 = 0.004934217780828476 + 10.0 * 6.2138590812683105
Epoch 2620, val loss: 1.3319019079208374
Epoch 2630, training loss: 62.122291564941406 = 0.004891245625913143 + 10.0 * 6.211740016937256
Epoch 2630, val loss: 1.3330135345458984
Epoch 2640, training loss: 62.14653396606445 = 0.004849524237215519 + 10.0 * 6.214168548583984
Epoch 2640, val loss: 1.3346410989761353
Epoch 2650, training loss: 62.12134552001953 = 0.004806815646588802 + 10.0 * 6.211653709411621
Epoch 2650, val loss: 1.3358012437820435
Epoch 2660, training loss: 62.11667251586914 = 0.004766165278851986 + 10.0 * 6.211190700531006
Epoch 2660, val loss: 1.3371220827102661
Epoch 2670, training loss: 62.12310028076172 = 0.004726273939013481 + 10.0 * 6.211837291717529
Epoch 2670, val loss: 1.3385688066482544
Epoch 2680, training loss: 62.14856719970703 = 0.004689430817961693 + 10.0 * 6.214387893676758
Epoch 2680, val loss: 1.3394372463226318
Epoch 2690, training loss: 62.15814971923828 = 0.004646161571145058 + 10.0 * 6.215350151062012
Epoch 2690, val loss: 1.3409957885742188
Epoch 2700, training loss: 62.11391067504883 = 0.004605460911989212 + 10.0 * 6.210930824279785
Epoch 2700, val loss: 1.3428553342819214
Epoch 2710, training loss: 62.10146713256836 = 0.004568695090711117 + 10.0 * 6.209689617156982
Epoch 2710, val loss: 1.343907356262207
Epoch 2720, training loss: 62.10460662841797 = 0.00453239306807518 + 10.0 * 6.210007667541504
Epoch 2720, val loss: 1.3453168869018555
Epoch 2730, training loss: 62.22879409790039 = 0.0044961958192288876 + 10.0 * 6.2224297523498535
Epoch 2730, val loss: 1.3472813367843628
Epoch 2740, training loss: 62.14927291870117 = 0.004458439536392689 + 10.0 * 6.214481353759766
Epoch 2740, val loss: 1.3475899696350098
Epoch 2750, training loss: 62.10030746459961 = 0.004420889541506767 + 10.0 * 6.209588527679443
Epoch 2750, val loss: 1.348771095275879
Epoch 2760, training loss: 62.0902214050293 = 0.0043862019665539265 + 10.0 * 6.208583354949951
Epoch 2760, val loss: 1.350406527519226
Epoch 2770, training loss: 62.09310531616211 = 0.0043523916974663734 + 10.0 * 6.2088751792907715
Epoch 2770, val loss: 1.3517903089523315
Epoch 2780, training loss: 62.13482666015625 = 0.00431879423558712 + 10.0 * 6.213050842285156
Epoch 2780, val loss: 1.3532308340072632
Epoch 2790, training loss: 62.09571075439453 = 0.004285146482288837 + 10.0 * 6.209142684936523
Epoch 2790, val loss: 1.3537366390228271
Epoch 2800, training loss: 62.11997985839844 = 0.004252927843481302 + 10.0 * 6.211572647094727
Epoch 2800, val loss: 1.354935884475708
Epoch 2810, training loss: 62.11811065673828 = 0.004217283800244331 + 10.0 * 6.211389064788818
Epoch 2810, val loss: 1.3565902709960938
Epoch 2820, training loss: 62.146854400634766 = 0.004184472840279341 + 10.0 * 6.214266777038574
Epoch 2820, val loss: 1.3574490547180176
Epoch 2830, training loss: 62.100425720214844 = 0.004152178764343262 + 10.0 * 6.209627628326416
Epoch 2830, val loss: 1.3582353591918945
Epoch 2840, training loss: 62.08470916748047 = 0.0041208271868526936 + 10.0 * 6.208058834075928
Epoch 2840, val loss: 1.359923243522644
Epoch 2850, training loss: 62.083553314208984 = 0.0040906816720962524 + 10.0 * 6.207946300506592
Epoch 2850, val loss: 1.361129641532898
Epoch 2860, training loss: 62.11369705200195 = 0.0040614912286400795 + 10.0 * 6.210963249206543
Epoch 2860, val loss: 1.3624149560928345
Epoch 2870, training loss: 62.09136199951172 = 0.004030420910567045 + 10.0 * 6.208733081817627
Epoch 2870, val loss: 1.363354206085205
Epoch 2880, training loss: 62.099857330322266 = 0.00400165468454361 + 10.0 * 6.209585666656494
Epoch 2880, val loss: 1.3639411926269531
Epoch 2890, training loss: 62.128143310546875 = 0.00397251034155488 + 10.0 * 6.212417125701904
Epoch 2890, val loss: 1.3650864362716675
Epoch 2900, training loss: 62.10615921020508 = 0.00394146703183651 + 10.0 * 6.210221767425537
Epoch 2900, val loss: 1.3665907382965088
Epoch 2910, training loss: 62.08134460449219 = 0.0039128875359892845 + 10.0 * 6.207743167877197
Epoch 2910, val loss: 1.367509126663208
Epoch 2920, training loss: 62.11096954345703 = 0.003885524347424507 + 10.0 * 6.210708141326904
Epoch 2920, val loss: 1.3686045408248901
Epoch 2930, training loss: 62.09231185913086 = 0.003857545554637909 + 10.0 * 6.208845615386963
Epoch 2930, val loss: 1.3698337078094482
Epoch 2940, training loss: 62.07344055175781 = 0.003831062698736787 + 10.0 * 6.206961154937744
Epoch 2940, val loss: 1.3704609870910645
Epoch 2950, training loss: 62.071842193603516 = 0.003804845968261361 + 10.0 * 6.206803798675537
Epoch 2950, val loss: 1.3715559244155884
Epoch 2960, training loss: 62.09223556518555 = 0.0037791626527905464 + 10.0 * 6.208845615386963
Epoch 2960, val loss: 1.3727277517318726
Epoch 2970, training loss: 62.10444259643555 = 0.003751586889848113 + 10.0 * 6.210069179534912
Epoch 2970, val loss: 1.3742218017578125
Epoch 2980, training loss: 62.11768341064453 = 0.003724464448168874 + 10.0 * 6.211396217346191
Epoch 2980, val loss: 1.3748984336853027
Epoch 2990, training loss: 62.078125 = 0.0036999492440372705 + 10.0 * 6.207442283630371
Epoch 2990, val loss: 1.3751403093338013
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 87.91202545166016 = 1.9435203075408936 + 10.0 * 8.596850395202637
Epoch 0, val loss: 1.9318031072616577
Epoch 10, training loss: 87.8961410522461 = 1.9337979555130005 + 10.0 * 8.596234321594238
Epoch 10, val loss: 1.9223746061325073
Epoch 20, training loss: 87.83807373046875 = 1.9219022989273071 + 10.0 * 8.5916166305542
Epoch 20, val loss: 1.9103883504867554
Epoch 30, training loss: 87.50241088867188 = 1.9071297645568848 + 10.0 * 8.559528350830078
Epoch 30, val loss: 1.8953943252563477
Epoch 40, training loss: 85.5677261352539 = 1.8903954029083252 + 10.0 * 8.367733001708984
Epoch 40, val loss: 1.8791841268539429
Epoch 50, training loss: 79.72948455810547 = 1.8725926876068115 + 10.0 * 7.785689830780029
Epoch 50, val loss: 1.862502098083496
Epoch 60, training loss: 75.54057312011719 = 1.8562864065170288 + 10.0 * 7.368428707122803
Epoch 60, val loss: 1.8474136590957642
Epoch 70, training loss: 72.7721939086914 = 1.841317892074585 + 10.0 * 7.093087673187256
Epoch 70, val loss: 1.8334978818893433
Epoch 80, training loss: 71.28416442871094 = 1.8270008563995361 + 10.0 * 6.945716381072998
Epoch 80, val loss: 1.8206034898757935
Epoch 90, training loss: 70.3436050415039 = 1.8143000602722168 + 10.0 * 6.852930545806885
Epoch 90, val loss: 1.809451937675476
Epoch 100, training loss: 69.681396484375 = 1.8020820617675781 + 10.0 * 6.787931442260742
Epoch 100, val loss: 1.7992509603500366
Epoch 110, training loss: 69.06647491455078 = 1.790639042854309 + 10.0 * 6.727583408355713
Epoch 110, val loss: 1.7900023460388184
Epoch 120, training loss: 68.60645294189453 = 1.7798150777816772 + 10.0 * 6.682663917541504
Epoch 120, val loss: 1.7812373638153076
Epoch 130, training loss: 68.20803833007812 = 1.7686444520950317 + 10.0 * 6.64393949508667
Epoch 130, val loss: 1.77207612991333
Epoch 140, training loss: 67.86962890625 = 1.7566391229629517 + 10.0 * 6.61129903793335
Epoch 140, val loss: 1.7622727155685425
Epoch 150, training loss: 67.59562683105469 = 1.7438358068466187 + 10.0 * 6.585178852081299
Epoch 150, val loss: 1.7518702745437622
Epoch 160, training loss: 67.35784912109375 = 1.7299938201904297 + 10.0 * 6.562785625457764
Epoch 160, val loss: 1.7407406568527222
Epoch 170, training loss: 67.13713836669922 = 1.7149940729141235 + 10.0 * 6.542214393615723
Epoch 170, val loss: 1.7287055253982544
Epoch 180, training loss: 66.93067932128906 = 1.6986805200576782 + 10.0 * 6.523200035095215
Epoch 180, val loss: 1.7156593799591064
Epoch 190, training loss: 66.73665618896484 = 1.6809346675872803 + 10.0 * 6.50557279586792
Epoch 190, val loss: 1.7016522884368896
Epoch 200, training loss: 66.58427429199219 = 1.6616684198379517 + 10.0 * 6.492260932922363
Epoch 200, val loss: 1.6865462064743042
Epoch 210, training loss: 66.40402221679688 = 1.6406511068344116 + 10.0 * 6.476337432861328
Epoch 210, val loss: 1.6701221466064453
Epoch 220, training loss: 66.26475524902344 = 1.6177492141723633 + 10.0 * 6.464700698852539
Epoch 220, val loss: 1.6523226499557495
Epoch 230, training loss: 66.18782806396484 = 1.5929192304611206 + 10.0 * 6.459490776062012
Epoch 230, val loss: 1.6330751180648804
Epoch 240, training loss: 66.0242919921875 = 1.5661826133728027 + 10.0 * 6.445810794830322
Epoch 240, val loss: 1.6122913360595703
Epoch 250, training loss: 65.89946746826172 = 1.537710189819336 + 10.0 * 6.43617582321167
Epoch 250, val loss: 1.5902304649353027
Epoch 260, training loss: 65.82247161865234 = 1.5075089931488037 + 10.0 * 6.4314961433410645
Epoch 260, val loss: 1.5670205354690552
Epoch 270, training loss: 65.68892669677734 = 1.4759469032287598 + 10.0 * 6.421298027038574
Epoch 270, val loss: 1.542614221572876
Epoch 280, training loss: 65.57921600341797 = 1.4431620836257935 + 10.0 * 6.413605213165283
Epoch 280, val loss: 1.5176235437393188
Epoch 290, training loss: 65.47386932373047 = 1.4094862937927246 + 10.0 * 6.40643835067749
Epoch 290, val loss: 1.4920599460601807
Epoch 300, training loss: 65.38433837890625 = 1.3751343488693237 + 10.0 * 6.400920391082764
Epoch 300, val loss: 1.4661208391189575
Epoch 310, training loss: 65.30487823486328 = 1.3402531147003174 + 10.0 * 6.396462440490723
Epoch 310, val loss: 1.440098524093628
Epoch 320, training loss: 65.20147705078125 = 1.3055427074432373 + 10.0 * 6.389593601226807
Epoch 320, val loss: 1.4142463207244873
Epoch 330, training loss: 65.09156799316406 = 1.2710793018341064 + 10.0 * 6.382049083709717
Epoch 330, val loss: 1.3891466856002808
Epoch 340, training loss: 65.00839233398438 = 1.2370991706848145 + 10.0 * 6.377129554748535
Epoch 340, val loss: 1.3644999265670776
Epoch 350, training loss: 64.99190521240234 = 1.2036244869232178 + 10.0 * 6.378828048706055
Epoch 350, val loss: 1.3405673503875732
Epoch 360, training loss: 64.85572814941406 = 1.1709716320037842 + 10.0 * 6.368475437164307
Epoch 360, val loss: 1.3173333406448364
Epoch 370, training loss: 64.77497100830078 = 1.1391469240188599 + 10.0 * 6.363582611083984
Epoch 370, val loss: 1.294967770576477
Epoch 380, training loss: 64.70386505126953 = 1.108187198638916 + 10.0 * 6.359567642211914
Epoch 380, val loss: 1.2736084461212158
Epoch 390, training loss: 64.64812469482422 = 1.0779904127120972 + 10.0 * 6.357013702392578
Epoch 390, val loss: 1.2529966831207275
Epoch 400, training loss: 64.59929656982422 = 1.0485950708389282 + 10.0 * 6.3550705909729
Epoch 400, val loss: 1.2332494258880615
Epoch 410, training loss: 64.51563262939453 = 1.0200570821762085 + 10.0 * 6.349557399749756
Epoch 410, val loss: 1.2140028476715088
Epoch 420, training loss: 64.44779968261719 = 0.9920196533203125 + 10.0 * 6.345577716827393
Epoch 420, val loss: 1.1954199075698853
Epoch 430, training loss: 64.39933776855469 = 0.9645843505859375 + 10.0 * 6.343475818634033
Epoch 430, val loss: 1.177400827407837
Epoch 440, training loss: 64.34725952148438 = 0.9376635551452637 + 10.0 * 6.3409600257873535
Epoch 440, val loss: 1.1596810817718506
Epoch 450, training loss: 64.2944564819336 = 0.9111664295196533 + 10.0 * 6.338328838348389
Epoch 450, val loss: 1.1422595977783203
Epoch 460, training loss: 64.21955108642578 = 0.8851220607757568 + 10.0 * 6.333442687988281
Epoch 460, val loss: 1.1253223419189453
Epoch 470, training loss: 64.17478942871094 = 0.8595243692398071 + 10.0 * 6.331526279449463
Epoch 470, val loss: 1.1088752746582031
Epoch 480, training loss: 64.14177703857422 = 0.8343039751052856 + 10.0 * 6.330747127532959
Epoch 480, val loss: 1.0926258563995361
Epoch 490, training loss: 64.0765380859375 = 0.8095954060554504 + 10.0 * 6.326694488525391
Epoch 490, val loss: 1.076766014099121
Epoch 500, training loss: 64.02803802490234 = 0.7852405905723572 + 10.0 * 6.324279308319092
Epoch 500, val loss: 1.0611575841903687
Epoch 510, training loss: 63.974365234375 = 0.7614425420761108 + 10.0 * 6.321291923522949
Epoch 510, val loss: 1.0462772846221924
Epoch 520, training loss: 63.95608139038086 = 0.7381211519241333 + 10.0 * 6.32179594039917
Epoch 520, val loss: 1.0316890478134155
Epoch 530, training loss: 63.89949417114258 = 0.7152363061904907 + 10.0 * 6.31842565536499
Epoch 530, val loss: 1.017439842224121
Epoch 540, training loss: 63.85341262817383 = 0.6929121017456055 + 10.0 * 6.316050052642822
Epoch 540, val loss: 1.0037904977798462
Epoch 550, training loss: 63.801456451416016 = 0.6711355447769165 + 10.0 * 6.313032150268555
Epoch 550, val loss: 0.990831732749939
Epoch 560, training loss: 63.76694869995117 = 0.6499678492546082 + 10.0 * 6.311697959899902
Epoch 560, val loss: 0.9783864617347717
Epoch 570, training loss: 63.72660827636719 = 0.6292668581008911 + 10.0 * 6.309733867645264
Epoch 570, val loss: 0.9665095806121826
Epoch 580, training loss: 63.728675842285156 = 0.6091456413269043 + 10.0 * 6.311953067779541
Epoch 580, val loss: 0.955045223236084
Epoch 590, training loss: 63.652374267578125 = 0.5894747376441956 + 10.0 * 6.306290149688721
Epoch 590, val loss: 0.9442490935325623
Epoch 600, training loss: 63.61048126220703 = 0.5704710483551025 + 10.0 * 6.3040008544921875
Epoch 600, val loss: 0.9342166185379028
Epoch 610, training loss: 63.586456298828125 = 0.5519862771034241 + 10.0 * 6.3034467697143555
Epoch 610, val loss: 0.9247952103614807
Epoch 620, training loss: 63.55996322631836 = 0.5338845252990723 + 10.0 * 6.302607536315918
Epoch 620, val loss: 0.9154087901115417
Epoch 630, training loss: 63.51431655883789 = 0.5163785815238953 + 10.0 * 6.299793720245361
Epoch 630, val loss: 0.9069594740867615
Epoch 640, training loss: 63.47551345825195 = 0.4993480145931244 + 10.0 * 6.297616481781006
Epoch 640, val loss: 0.8990261554718018
Epoch 650, training loss: 63.463661193847656 = 0.4828319847583771 + 10.0 * 6.2980828285217285
Epoch 650, val loss: 0.8915085196495056
Epoch 660, training loss: 63.44776916503906 = 0.46659010648727417 + 10.0 * 6.298117637634277
Epoch 660, val loss: 0.884425163269043
Epoch 670, training loss: 63.390113830566406 = 0.45089274644851685 + 10.0 * 6.293921947479248
Epoch 670, val loss: 0.8777189254760742
Epoch 680, training loss: 63.35897445678711 = 0.43561992049217224 + 10.0 * 6.292335510253906
Epoch 680, val loss: 0.8716046810150146
Epoch 690, training loss: 63.324974060058594 = 0.4208063781261444 + 10.0 * 6.290416717529297
Epoch 690, val loss: 0.8659451603889465
Epoch 700, training loss: 63.33489227294922 = 0.4064165949821472 + 10.0 * 6.292847633361816
Epoch 700, val loss: 0.8606067895889282
Epoch 710, training loss: 63.343326568603516 = 0.3922567367553711 + 10.0 * 6.295106887817383
Epoch 710, val loss: 0.8555663228034973
Epoch 720, training loss: 63.25666427612305 = 0.37863242626190186 + 10.0 * 6.2878031730651855
Epoch 720, val loss: 0.8507751822471619
Epoch 730, training loss: 63.22231674194336 = 0.3653934597969055 + 10.0 * 6.28569221496582
Epoch 730, val loss: 0.8465961813926697
Epoch 740, training loss: 63.1994514465332 = 0.3526023328304291 + 10.0 * 6.284684658050537
Epoch 740, val loss: 0.8428877592086792
Epoch 750, training loss: 63.18505859375 = 0.340194433927536 + 10.0 * 6.284486293792725
Epoch 750, val loss: 0.8394830226898193
Epoch 760, training loss: 63.16379928588867 = 0.3281210958957672 + 10.0 * 6.283567905426025
Epoch 760, val loss: 0.8362810611724854
Epoch 770, training loss: 63.139793395996094 = 0.31644198298454285 + 10.0 * 6.28233528137207
Epoch 770, val loss: 0.8334764242172241
Epoch 780, training loss: 63.13643264770508 = 0.3051854968070984 + 10.0 * 6.2831244468688965
Epoch 780, val loss: 0.8311517238616943
Epoch 790, training loss: 63.09944534301758 = 0.2942312955856323 + 10.0 * 6.280521392822266
Epoch 790, val loss: 0.8289140462875366
Epoch 800, training loss: 63.07026290893555 = 0.28370678424835205 + 10.0 * 6.278655529022217
Epoch 800, val loss: 0.8272879123687744
Epoch 810, training loss: 63.04463577270508 = 0.2735641896724701 + 10.0 * 6.277107238769531
Epoch 810, val loss: 0.8260241746902466
Epoch 820, training loss: 63.04527282714844 = 0.26379427313804626 + 10.0 * 6.2781476974487305
Epoch 820, val loss: 0.8251809477806091
Epoch 830, training loss: 63.02321243286133 = 0.2543289363384247 + 10.0 * 6.276888370513916
Epoch 830, val loss: 0.82428377866745
Epoch 840, training loss: 62.997798919677734 = 0.24520684778690338 + 10.0 * 6.275259017944336
Epoch 840, val loss: 0.8239734172821045
Epoch 850, training loss: 62.973323822021484 = 0.23647287487983704 + 10.0 * 6.273684978485107
Epoch 850, val loss: 0.824138879776001
Epoch 860, training loss: 62.95067596435547 = 0.22807501256465912 + 10.0 * 6.2722601890563965
Epoch 860, val loss: 0.8246656060218811
Epoch 870, training loss: 62.98207092285156 = 0.21996915340423584 + 10.0 * 6.276209831237793
Epoch 870, val loss: 0.8255278468132019
Epoch 880, training loss: 62.972511291503906 = 0.21217478811740875 + 10.0 * 6.276033878326416
Epoch 880, val loss: 0.8265630006790161
Epoch 890, training loss: 62.913177490234375 = 0.20465250313282013 + 10.0 * 6.270852565765381
Epoch 890, val loss: 0.8278818726539612
Epoch 900, training loss: 62.90465545654297 = 0.19746562838554382 + 10.0 * 6.270719051361084
Epoch 900, val loss: 0.8296661972999573
Epoch 910, training loss: 62.88054656982422 = 0.1905439794063568 + 10.0 * 6.269000053405762
Epoch 910, val loss: 0.8316980600357056
Epoch 920, training loss: 62.86603546142578 = 0.18390782177448273 + 10.0 * 6.268212795257568
Epoch 920, val loss: 0.8340100049972534
Epoch 930, training loss: 62.8779411315918 = 0.17751573026180267 + 10.0 * 6.270042419433594
Epoch 930, val loss: 0.8366404175758362
Epoch 940, training loss: 62.845550537109375 = 0.1713305413722992 + 10.0 * 6.267422199249268
Epoch 940, val loss: 0.8392865657806396
Epoch 950, training loss: 62.8262825012207 = 0.16541406512260437 + 10.0 * 6.266087055206299
Epoch 950, val loss: 0.8423368334770203
Epoch 960, training loss: 62.81102752685547 = 0.15974146127700806 + 10.0 * 6.2651286125183105
Epoch 960, val loss: 0.8456907868385315
Epoch 970, training loss: 62.796669006347656 = 0.1542956382036209 + 10.0 * 6.264237403869629
Epoch 970, val loss: 0.849323034286499
Epoch 980, training loss: 62.82323455810547 = 0.14907129108905792 + 10.0 * 6.267416477203369
Epoch 980, val loss: 0.8530705571174622
Epoch 990, training loss: 62.80559158325195 = 0.1440075933933258 + 10.0 * 6.266158103942871
Epoch 990, val loss: 0.8570826053619385
Epoch 1000, training loss: 62.76507568359375 = 0.1391172707080841 + 10.0 * 6.2625956535339355
Epoch 1000, val loss: 0.8610183000564575
Epoch 1010, training loss: 62.74352264404297 = 0.13447785377502441 + 10.0 * 6.260904312133789
Epoch 1010, val loss: 0.8653683662414551
Epoch 1020, training loss: 62.762245178222656 = 0.13002565503120422 + 10.0 * 6.2632222175598145
Epoch 1020, val loss: 0.8698614239692688
Epoch 1030, training loss: 62.722808837890625 = 0.12570072710514069 + 10.0 * 6.259710788726807
Epoch 1030, val loss: 0.8741567730903625
Epoch 1040, training loss: 62.71816635131836 = 0.12155728042125702 + 10.0 * 6.259660720825195
Epoch 1040, val loss: 0.8788253664970398
Epoch 1050, training loss: 62.71215057373047 = 0.11758441478013992 + 10.0 * 6.259456634521484
Epoch 1050, val loss: 0.8835247755050659
Epoch 1060, training loss: 62.68871307373047 = 0.1137491762638092 + 10.0 * 6.257496356964111
Epoch 1060, val loss: 0.8884396553039551
Epoch 1070, training loss: 62.70848083496094 = 0.11008097231388092 + 10.0 * 6.25984001159668
Epoch 1070, val loss: 0.8934237360954285
Epoch 1080, training loss: 62.6763916015625 = 0.1065073162317276 + 10.0 * 6.256988525390625
Epoch 1080, val loss: 0.8984214067459106
Epoch 1090, training loss: 62.6666374206543 = 0.10309877246618271 + 10.0 * 6.256353855133057
Epoch 1090, val loss: 0.903519332408905
Epoch 1100, training loss: 62.66027069091797 = 0.09981725364923477 + 10.0 * 6.256045341491699
Epoch 1100, val loss: 0.9087619185447693
Epoch 1110, training loss: 62.684791564941406 = 0.0966755822300911 + 10.0 * 6.2588114738464355
Epoch 1110, val loss: 0.9139718413352966
Epoch 1120, training loss: 62.661041259765625 = 0.09362044930458069 + 10.0 * 6.256742000579834
Epoch 1120, val loss: 0.9192469716072083
Epoch 1130, training loss: 62.62506866455078 = 0.09072070568799973 + 10.0 * 6.253434658050537
Epoch 1130, val loss: 0.9246750473976135
Epoch 1140, training loss: 62.60850524902344 = 0.08792049437761307 + 10.0 * 6.252058506011963
Epoch 1140, val loss: 0.9301880598068237
Epoch 1150, training loss: 62.6051025390625 = 0.08524434268474579 + 10.0 * 6.251986026763916
Epoch 1150, val loss: 0.9357642531394958
Epoch 1160, training loss: 62.66459655761719 = 0.08265800774097443 + 10.0 * 6.2581939697265625
Epoch 1160, val loss: 0.9413861036300659
Epoch 1170, training loss: 62.63951110839844 = 0.08016896992921829 + 10.0 * 6.255934238433838
Epoch 1170, val loss: 0.9469061493873596
Epoch 1180, training loss: 62.591556549072266 = 0.07773814350366592 + 10.0 * 6.251381874084473
Epoch 1180, val loss: 0.9523599147796631
Epoch 1190, training loss: 62.57589340209961 = 0.07542935013771057 + 10.0 * 6.250046730041504
Epoch 1190, val loss: 0.9579923152923584
Epoch 1200, training loss: 62.60065841674805 = 0.07322611659765244 + 10.0 * 6.252743244171143
Epoch 1200, val loss: 0.9637947082519531
Epoch 1210, training loss: 62.568382263183594 = 0.07108306139707565 + 10.0 * 6.249730110168457
Epoch 1210, val loss: 0.9694867134094238
Epoch 1220, training loss: 62.54785919189453 = 0.06900956481695175 + 10.0 * 6.247885227203369
Epoch 1220, val loss: 0.9751701951026917
Epoch 1230, training loss: 62.543190002441406 = 0.0670371949672699 + 10.0 * 6.247615337371826
Epoch 1230, val loss: 0.9810648560523987
Epoch 1240, training loss: 62.568172454833984 = 0.06513503938913345 + 10.0 * 6.250303745269775
Epoch 1240, val loss: 0.9868417382240295
Epoch 1250, training loss: 62.55878448486328 = 0.0633000060915947 + 10.0 * 6.249548435211182
Epoch 1250, val loss: 0.9927258491516113
Epoch 1260, training loss: 62.529415130615234 = 0.06149589642882347 + 10.0 * 6.246791839599609
Epoch 1260, val loss: 0.998293936252594
Epoch 1270, training loss: 62.52623748779297 = 0.05980575829744339 + 10.0 * 6.24664306640625
Epoch 1270, val loss: 1.004382610321045
Epoch 1280, training loss: 62.556312561035156 = 0.058166638016700745 + 10.0 * 6.249814510345459
Epoch 1280, val loss: 1.0101481676101685
Epoch 1290, training loss: 62.51029586791992 = 0.0565706230700016 + 10.0 * 6.245372295379639
Epoch 1290, val loss: 1.0159412622451782
Epoch 1300, training loss: 62.49049377441406 = 0.05504583567380905 + 10.0 * 6.243544578552246
Epoch 1300, val loss: 1.0218641757965088
Epoch 1310, training loss: 62.49490737915039 = 0.05358459800481796 + 10.0 * 6.244132041931152
Epoch 1310, val loss: 1.0278189182281494
Epoch 1320, training loss: 62.528438568115234 = 0.052170298993587494 + 10.0 * 6.247626781463623
Epoch 1320, val loss: 1.0336114168167114
Epoch 1330, training loss: 62.524513244628906 = 0.050781723111867905 + 10.0 * 6.247373104095459
Epoch 1330, val loss: 1.0391563177108765
Epoch 1340, training loss: 62.4777717590332 = 0.049436427652835846 + 10.0 * 6.242833614349365
Epoch 1340, val loss: 1.0449715852737427
Epoch 1350, training loss: 62.462249755859375 = 0.04815260320901871 + 10.0 * 6.241409778594971
Epoch 1350, val loss: 1.050848126411438
Epoch 1360, training loss: 62.463809967041016 = 0.046931929886341095 + 10.0 * 6.241687774658203
Epoch 1360, val loss: 1.0567373037338257
Epoch 1370, training loss: 62.50929260253906 = 0.04574418440461159 + 10.0 * 6.246354579925537
Epoch 1370, val loss: 1.0625032186508179
Epoch 1380, training loss: 62.462730407714844 = 0.04459686949849129 + 10.0 * 6.2418131828308105
Epoch 1380, val loss: 1.0682445764541626
Epoch 1390, training loss: 62.455810546875 = 0.04348818212747574 + 10.0 * 6.241232395172119
Epoch 1390, val loss: 1.0741139650344849
Epoch 1400, training loss: 62.4737548828125 = 0.0424233116209507 + 10.0 * 6.243133068084717
Epoch 1400, val loss: 1.0797439813613892
Epoch 1410, training loss: 62.44184112548828 = 0.041367195546627045 + 10.0 * 6.240047454833984
Epoch 1410, val loss: 1.0853360891342163
Epoch 1420, training loss: 62.42575454711914 = 0.0403771698474884 + 10.0 * 6.238537788391113
Epoch 1420, val loss: 1.0911364555358887
Epoch 1430, training loss: 62.4576301574707 = 0.0394195094704628 + 10.0 * 6.241820812225342
Epoch 1430, val loss: 1.096828818321228
Epoch 1440, training loss: 62.47190856933594 = 0.03846915811300278 + 10.0 * 6.243344306945801
Epoch 1440, val loss: 1.1023014783859253
Epoch 1450, training loss: 62.43684768676758 = 0.03755366802215576 + 10.0 * 6.239929676055908
Epoch 1450, val loss: 1.1076796054840088
Epoch 1460, training loss: 62.40739440917969 = 0.03667936846613884 + 10.0 * 6.237071514129639
Epoch 1460, val loss: 1.1133887767791748
Epoch 1470, training loss: 62.403564453125 = 0.03584417700767517 + 10.0 * 6.236772060394287
Epoch 1470, val loss: 1.1190606355667114
Epoch 1480, training loss: 62.399513244628906 = 0.03503340482711792 + 10.0 * 6.236447811126709
Epoch 1480, val loss: 1.1246918439865112
Epoch 1490, training loss: 62.44822311401367 = 0.03424787148833275 + 10.0 * 6.241397380828857
Epoch 1490, val loss: 1.1301977634429932
Epoch 1500, training loss: 62.410457611083984 = 0.03346814587712288 + 10.0 * 6.237699031829834
Epoch 1500, val loss: 1.1353495121002197
Epoch 1510, training loss: 62.42155456542969 = 0.0327175073325634 + 10.0 * 6.2388834953308105
Epoch 1510, val loss: 1.1408580541610718
Epoch 1520, training loss: 62.39809799194336 = 0.03199993073940277 + 10.0 * 6.23660945892334
Epoch 1520, val loss: 1.1462476253509521
Epoch 1530, training loss: 62.38038635253906 = 0.03129993751645088 + 10.0 * 6.234908580780029
Epoch 1530, val loss: 1.1516518592834473
Epoch 1540, training loss: 62.378883361816406 = 0.03063276968896389 + 10.0 * 6.234825134277344
Epoch 1540, val loss: 1.1571359634399414
Epoch 1550, training loss: 62.43684768676758 = 0.029984792694449425 + 10.0 * 6.240686416625977
Epoch 1550, val loss: 1.162438154220581
Epoch 1560, training loss: 62.39423370361328 = 0.029334746301174164 + 10.0 * 6.236489772796631
Epoch 1560, val loss: 1.1677359342575073
Epoch 1570, training loss: 62.36759567260742 = 0.028714748099446297 + 10.0 * 6.233888149261475
Epoch 1570, val loss: 1.1729774475097656
Epoch 1580, training loss: 62.3590087890625 = 0.028119033202528954 + 10.0 * 6.233088970184326
Epoch 1580, val loss: 1.1783256530761719
Epoch 1590, training loss: 62.37525177001953 = 0.027542278170585632 + 10.0 * 6.234770774841309
Epoch 1590, val loss: 1.1835719347000122
Epoch 1600, training loss: 62.358489990234375 = 0.026976926252245903 + 10.0 * 6.233151435852051
Epoch 1600, val loss: 1.1885823011398315
Epoch 1610, training loss: 62.38652801513672 = 0.026436520740389824 + 10.0 * 6.236009120941162
Epoch 1610, val loss: 1.1936712265014648
Epoch 1620, training loss: 62.38801193237305 = 0.025898288935422897 + 10.0 * 6.23621129989624
Epoch 1620, val loss: 1.198968768119812
Epoch 1630, training loss: 62.35300064086914 = 0.02537095546722412 + 10.0 * 6.232762813568115
Epoch 1630, val loss: 1.203646183013916
Epoch 1640, training loss: 62.33572006225586 = 0.024869902059435844 + 10.0 * 6.231085300445557
Epoch 1640, val loss: 1.2088710069656372
Epoch 1650, training loss: 62.33692932128906 = 0.024388881400227547 + 10.0 * 6.2312541007995605
Epoch 1650, val loss: 1.213865876197815
Epoch 1660, training loss: 62.3646125793457 = 0.023921029642224312 + 10.0 * 6.234068870544434
Epoch 1660, val loss: 1.2188398838043213
Epoch 1670, training loss: 62.329227447509766 = 0.023455549031496048 + 10.0 * 6.230576992034912
Epoch 1670, val loss: 1.22366201877594
Epoch 1680, training loss: 62.33879089355469 = 0.02301078662276268 + 10.0 * 6.2315778732299805
Epoch 1680, val loss: 1.2286779880523682
Epoch 1690, training loss: 62.35808181762695 = 0.022565750405192375 + 10.0 * 6.233551979064941
Epoch 1690, val loss: 1.2331626415252686
Epoch 1700, training loss: 62.32966613769531 = 0.0221355352550745 + 10.0 * 6.230752944946289
Epoch 1700, val loss: 1.237955093383789
Epoch 1710, training loss: 62.31268310546875 = 0.02172662876546383 + 10.0 * 6.229095458984375
Epoch 1710, val loss: 1.2428237199783325
Epoch 1720, training loss: 62.305301666259766 = 0.021329818293452263 + 10.0 * 6.228397369384766
Epoch 1720, val loss: 1.2476606369018555
Epoch 1730, training loss: 62.30206298828125 = 0.020945271477103233 + 10.0 * 6.228111743927002
Epoch 1730, val loss: 1.2524900436401367
Epoch 1740, training loss: 62.3798713684082 = 0.020571574568748474 + 10.0 * 6.2359299659729
Epoch 1740, val loss: 1.2571927309036255
Epoch 1750, training loss: 62.317630767822266 = 0.020198659971356392 + 10.0 * 6.229743003845215
Epoch 1750, val loss: 1.2614688873291016
Epoch 1760, training loss: 62.31088638305664 = 0.019840184599161148 + 10.0 * 6.229104518890381
Epoch 1760, val loss: 1.2663193941116333
Epoch 1770, training loss: 62.346073150634766 = 0.019495615735650063 + 10.0 * 6.232657432556152
Epoch 1770, val loss: 1.270792007446289
Epoch 1780, training loss: 62.299583435058594 = 0.019143402576446533 + 10.0 * 6.228044033050537
Epoch 1780, val loss: 1.275123119354248
Epoch 1790, training loss: 62.28257369995117 = 0.01881525292992592 + 10.0 * 6.226376056671143
Epoch 1790, val loss: 1.2797406911849976
Epoch 1800, training loss: 62.30270004272461 = 0.01849985122680664 + 10.0 * 6.228419780731201
Epoch 1800, val loss: 1.28431236743927
Epoch 1810, training loss: 62.29753875732422 = 0.01818050816655159 + 10.0 * 6.227935791015625
Epoch 1810, val loss: 1.288425326347351
Epoch 1820, training loss: 62.27709197998047 = 0.017868852242827415 + 10.0 * 6.225922584533691
Epoch 1820, val loss: 1.2928506135940552
Epoch 1830, training loss: 62.273624420166016 = 0.017572399228811264 + 10.0 * 6.225605487823486
Epoch 1830, val loss: 1.2973283529281616
Epoch 1840, training loss: 62.34104919433594 = 0.017290230840444565 + 10.0 * 6.232375621795654
Epoch 1840, val loss: 1.301723599433899
Epoch 1850, training loss: 62.285762786865234 = 0.016994325444102287 + 10.0 * 6.226876735687256
Epoch 1850, val loss: 1.3056668043136597
Epoch 1860, training loss: 62.28913879394531 = 0.01671820506453514 + 10.0 * 6.2272419929504395
Epoch 1860, val loss: 1.3098750114440918
Epoch 1870, training loss: 62.297061920166016 = 0.016447346657514572 + 10.0 * 6.228061199188232
Epoch 1870, val loss: 1.3141671419143677
Epoch 1880, training loss: 62.27585983276367 = 0.0161855760961771 + 10.0 * 6.2259674072265625
Epoch 1880, val loss: 1.3184034824371338
Epoch 1890, training loss: 62.27082824707031 = 0.015923555940389633 + 10.0 * 6.225490570068359
Epoch 1890, val loss: 1.3225359916687012
Epoch 1900, training loss: 62.27998352050781 = 0.01567550003528595 + 10.0 * 6.226430892944336
Epoch 1900, val loss: 1.3268522024154663
Epoch 1910, training loss: 62.25980758666992 = 0.01543116569519043 + 10.0 * 6.224437713623047
Epoch 1910, val loss: 1.3307850360870361
Epoch 1920, training loss: 62.261474609375 = 0.015193619765341282 + 10.0 * 6.22462797164917
Epoch 1920, val loss: 1.334858775138855
Epoch 1930, training loss: 62.27011489868164 = 0.014961070381104946 + 10.0 * 6.225515365600586
Epoch 1930, val loss: 1.338982105255127
Epoch 1940, training loss: 62.25001907348633 = 0.014730226248502731 + 10.0 * 6.223528861999512
Epoch 1940, val loss: 1.3429303169250488
Epoch 1950, training loss: 62.28423309326172 = 0.01450791023671627 + 10.0 * 6.226972579956055
Epoch 1950, val loss: 1.3468632698059082
Epoch 1960, training loss: 62.24703598022461 = 0.014285662211477757 + 10.0 * 6.223275184631348
Epoch 1960, val loss: 1.3508394956588745
Epoch 1970, training loss: 62.24646759033203 = 0.014072268269956112 + 10.0 * 6.223239421844482
Epoch 1970, val loss: 1.3546807765960693
Epoch 1980, training loss: 62.24895477294922 = 0.013865316286683083 + 10.0 * 6.223508834838867
Epoch 1980, val loss: 1.3585842847824097
Epoch 1990, training loss: 62.25227355957031 = 0.013662532903254032 + 10.0 * 6.223860740661621
Epoch 1990, val loss: 1.3627036809921265
Epoch 2000, training loss: 62.24867248535156 = 0.013465863652527332 + 10.0 * 6.223520755767822
Epoch 2000, val loss: 1.3664652109146118
Epoch 2010, training loss: 62.26085662841797 = 0.01326908078044653 + 10.0 * 6.224759101867676
Epoch 2010, val loss: 1.3701149225234985
Epoch 2020, training loss: 62.22659683227539 = 0.013076347298920155 + 10.0 * 6.2213521003723145
Epoch 2020, val loss: 1.3738515377044678
Epoch 2030, training loss: 62.22230529785156 = 0.012890051119029522 + 10.0 * 6.220941543579102
Epoch 2030, val loss: 1.3778177499771118
Epoch 2040, training loss: 62.234352111816406 = 0.012710999697446823 + 10.0 * 6.222164154052734
Epoch 2040, val loss: 1.3815786838531494
Epoch 2050, training loss: 62.24714660644531 = 0.012531755492091179 + 10.0 * 6.223461627960205
Epoch 2050, val loss: 1.3851172924041748
Epoch 2060, training loss: 62.233238220214844 = 0.012357029132544994 + 10.0 * 6.22208833694458
Epoch 2060, val loss: 1.3886892795562744
Epoch 2070, training loss: 62.21929168701172 = 0.01218536775559187 + 10.0 * 6.220710754394531
Epoch 2070, val loss: 1.3924931287765503
Epoch 2080, training loss: 62.26333236694336 = 0.012018122710287571 + 10.0 * 6.225131511688232
Epoch 2080, val loss: 1.395905613899231
Epoch 2090, training loss: 62.20988464355469 = 0.011854173615574837 + 10.0 * 6.2198028564453125
Epoch 2090, val loss: 1.3997524976730347
Epoch 2100, training loss: 62.204654693603516 = 0.011695096269249916 + 10.0 * 6.219295978546143
Epoch 2100, val loss: 1.4033982753753662
Epoch 2110, training loss: 62.20243453979492 = 0.011540229432284832 + 10.0 * 6.219089508056641
Epoch 2110, val loss: 1.4069029092788696
Epoch 2120, training loss: 62.228294372558594 = 0.011392386630177498 + 10.0 * 6.2216901779174805
Epoch 2120, val loss: 1.4104894399642944
Epoch 2130, training loss: 62.22189712524414 = 0.01123801525682211 + 10.0 * 6.221065998077393
Epoch 2130, val loss: 1.4140645265579224
Epoch 2140, training loss: 62.211509704589844 = 0.011086143553256989 + 10.0 * 6.2200422286987305
Epoch 2140, val loss: 1.4171907901763916
Epoch 2150, training loss: 62.21733856201172 = 0.010941204614937305 + 10.0 * 6.220639705657959
Epoch 2150, val loss: 1.4207935333251953
Epoch 2160, training loss: 62.205631256103516 = 0.010800250805914402 + 10.0 * 6.219483375549316
Epoch 2160, val loss: 1.4242992401123047
Epoch 2170, training loss: 62.190860748291016 = 0.010663675144314766 + 10.0 * 6.218019962310791
Epoch 2170, val loss: 1.4279640913009644
Epoch 2180, training loss: 62.18685531616211 = 0.010530466213822365 + 10.0 * 6.21763277053833
Epoch 2180, val loss: 1.4314932823181152
Epoch 2190, training loss: 62.214847564697266 = 0.010402994230389595 + 10.0 * 6.220444679260254
Epoch 2190, val loss: 1.4348852634429932
Epoch 2200, training loss: 62.198768615722656 = 0.01026949379593134 + 10.0 * 6.218850135803223
Epoch 2200, val loss: 1.4378880262374878
Epoch 2210, training loss: 62.21308898925781 = 0.010139964520931244 + 10.0 * 6.220294952392578
Epoch 2210, val loss: 1.4414196014404297
Epoch 2220, training loss: 62.19526290893555 = 0.010011812672019005 + 10.0 * 6.218524932861328
Epoch 2220, val loss: 1.444627046585083
Epoch 2230, training loss: 62.185062408447266 = 0.009887331165373325 + 10.0 * 6.217517375946045
Epoch 2230, val loss: 1.4478548765182495
Epoch 2240, training loss: 62.17072677612305 = 0.009768523275852203 + 10.0 * 6.216095924377441
Epoch 2240, val loss: 1.4513015747070312
Epoch 2250, training loss: 62.17830276489258 = 0.009654485620558262 + 10.0 * 6.216864585876465
Epoch 2250, val loss: 1.4546281099319458
Epoch 2260, training loss: 62.26570129394531 = 0.009539573453366756 + 10.0 * 6.225615978240967
Epoch 2260, val loss: 1.457589864730835
Epoch 2270, training loss: 62.193965911865234 = 0.009421712718904018 + 10.0 * 6.218454360961914
Epoch 2270, val loss: 1.4608014822006226
Epoch 2280, training loss: 62.17123031616211 = 0.009308566339313984 + 10.0 * 6.216192245483398
Epoch 2280, val loss: 1.4637795686721802
Epoch 2290, training loss: 62.162384033203125 = 0.009201578795909882 + 10.0 * 6.215318202972412
Epoch 2290, val loss: 1.4672538042068481
Epoch 2300, training loss: 62.16903305053711 = 0.009097717702388763 + 10.0 * 6.215993404388428
Epoch 2300, val loss: 1.4704047441482544
Epoch 2310, training loss: 62.23634338378906 = 0.008995597250759602 + 10.0 * 6.2227349281311035
Epoch 2310, val loss: 1.4734054803848267
Epoch 2320, training loss: 62.18111038208008 = 0.0088870944455266 + 10.0 * 6.217222213745117
Epoch 2320, val loss: 1.4762718677520752
Epoch 2330, training loss: 62.16243362426758 = 0.008787157945334911 + 10.0 * 6.215364456176758
Epoch 2330, val loss: 1.479514241218567
Epoch 2340, training loss: 62.16254425048828 = 0.00868969690054655 + 10.0 * 6.215385437011719
Epoch 2340, val loss: 1.482630729675293
Epoch 2350, training loss: 62.25321960449219 = 0.008598828688263893 + 10.0 * 6.224462032318115
Epoch 2350, val loss: 1.4856619834899902
Epoch 2360, training loss: 62.18659973144531 = 0.008492113091051579 + 10.0 * 6.21781063079834
Epoch 2360, val loss: 1.4880940914154053
Epoch 2370, training loss: 62.15453338623047 = 0.0083988131955266 + 10.0 * 6.214613437652588
Epoch 2370, val loss: 1.4912729263305664
Epoch 2380, training loss: 62.14663314819336 = 0.008307951502501965 + 10.0 * 6.213832378387451
Epoch 2380, val loss: 1.4941966533660889
Epoch 2390, training loss: 62.149662017822266 = 0.008220219984650612 + 10.0 * 6.214144229888916
Epoch 2390, val loss: 1.497243881225586
Epoch 2400, training loss: 62.20783615112305 = 0.008133772760629654 + 10.0 * 6.219970226287842
Epoch 2400, val loss: 1.499987244606018
Epoch 2410, training loss: 62.155372619628906 = 0.008045186288654804 + 10.0 * 6.214732646942139
Epoch 2410, val loss: 1.502708077430725
Epoch 2420, training loss: 62.158809661865234 = 0.007960733026266098 + 10.0 * 6.215085029602051
Epoch 2420, val loss: 1.505519151687622
Epoch 2430, training loss: 62.16214370727539 = 0.007876995950937271 + 10.0 * 6.215426445007324
Epoch 2430, val loss: 1.5085270404815674
Epoch 2440, training loss: 62.14729690551758 = 0.0077934046275913715 + 10.0 * 6.213950157165527
Epoch 2440, val loss: 1.5112413167953491
Epoch 2450, training loss: 62.14033889770508 = 0.00771363778039813 + 10.0 * 6.213262557983398
Epoch 2450, val loss: 1.5142093896865845
Epoch 2460, training loss: 62.1944465637207 = 0.0076364376582205296 + 10.0 * 6.2186808586120605
Epoch 2460, val loss: 1.517065167427063
Epoch 2470, training loss: 62.156639099121094 = 0.0075531951151788235 + 10.0 * 6.214908599853516
Epoch 2470, val loss: 1.5194453001022339
Epoch 2480, training loss: 62.15147018432617 = 0.007476076949387789 + 10.0 * 6.214399337768555
Epoch 2480, val loss: 1.5224149227142334
Epoch 2490, training loss: 62.129638671875 = 0.00739963399246335 + 10.0 * 6.212224006652832
Epoch 2490, val loss: 1.5249476432800293
Epoch 2500, training loss: 62.12700271606445 = 0.0073267072439193726 + 10.0 * 6.211967468261719
Epoch 2500, val loss: 1.5278629064559937
Epoch 2510, training loss: 62.16139602661133 = 0.0072580245323479176 + 10.0 * 6.215413570404053
Epoch 2510, val loss: 1.530599594116211
Epoch 2520, training loss: 62.1617431640625 = 0.007183697074651718 + 10.0 * 6.215456008911133
Epoch 2520, val loss: 1.5327141284942627
Epoch 2530, training loss: 62.13768768310547 = 0.007106775417923927 + 10.0 * 6.213057994842529
Epoch 2530, val loss: 1.535306692123413
Epoch 2540, training loss: 62.126827239990234 = 0.007037322036921978 + 10.0 * 6.211978912353516
Epoch 2540, val loss: 1.538025975227356
Epoch 2550, training loss: 62.117454528808594 = 0.006970235612243414 + 10.0 * 6.211048603057861
Epoch 2550, val loss: 1.5407922267913818
Epoch 2560, training loss: 62.12852478027344 = 0.006905789487063885 + 10.0 * 6.212162017822266
Epoch 2560, val loss: 1.5433160066604614
Epoch 2570, training loss: 62.156925201416016 = 0.006839951500296593 + 10.0 * 6.215008735656738
Epoch 2570, val loss: 1.5457968711853027
Epoch 2580, training loss: 62.13886260986328 = 0.006774100475013256 + 10.0 * 6.2132086753845215
Epoch 2580, val loss: 1.5484155416488647
Epoch 2590, training loss: 62.12898635864258 = 0.006709380075335503 + 10.0 * 6.212227821350098
Epoch 2590, val loss: 1.5507501363754272
Epoch 2600, training loss: 62.129520416259766 = 0.0066470792517066 + 10.0 * 6.212287425994873
Epoch 2600, val loss: 1.5532232522964478
Epoch 2610, training loss: 62.14262390136719 = 0.006585720460861921 + 10.0 * 6.213603973388672
Epoch 2610, val loss: 1.5556548833847046
Epoch 2620, training loss: 62.125553131103516 = 0.006525406613945961 + 10.0 * 6.211902618408203
Epoch 2620, val loss: 1.55820894241333
Epoch 2630, training loss: 62.11360168457031 = 0.0064637428149580956 + 10.0 * 6.210713863372803
Epoch 2630, val loss: 1.5606623888015747
Epoch 2640, training loss: 62.114620208740234 = 0.006406950764358044 + 10.0 * 6.210821151733398
Epoch 2640, val loss: 1.5632461309432983
Epoch 2650, training loss: 62.13033676147461 = 0.006350796669721603 + 10.0 * 6.212398529052734
Epoch 2650, val loss: 1.5656754970550537
Epoch 2660, training loss: 62.12791442871094 = 0.006293280981481075 + 10.0 * 6.212162017822266
Epoch 2660, val loss: 1.5678266286849976
Epoch 2670, training loss: 62.110504150390625 = 0.006236365530639887 + 10.0 * 6.2104268074035645
Epoch 2670, val loss: 1.5700255632400513
Epoch 2680, training loss: 62.13621139526367 = 0.006180515978485346 + 10.0 * 6.213003158569336
Epoch 2680, val loss: 1.572405219078064
Epoch 2690, training loss: 62.11819076538086 = 0.006124758627265692 + 10.0 * 6.211206436157227
Epoch 2690, val loss: 1.5745295286178589
Epoch 2700, training loss: 62.1074333190918 = 0.006071876734495163 + 10.0 * 6.2101359367370605
Epoch 2700, val loss: 1.5770339965820312
Epoch 2710, training loss: 62.096656799316406 = 0.006018324289470911 + 10.0 * 6.209063529968262
Epoch 2710, val loss: 1.5793794393539429
Epoch 2720, training loss: 62.091556549072266 = 0.005968685727566481 + 10.0 * 6.208558559417725
Epoch 2720, val loss: 1.5818252563476562
Epoch 2730, training loss: 62.122737884521484 = 0.005919756833463907 + 10.0 * 6.211681842803955
Epoch 2730, val loss: 1.5838139057159424
Epoch 2740, training loss: 62.117637634277344 = 0.00586777925491333 + 10.0 * 6.211176872253418
Epoch 2740, val loss: 1.5859811305999756
Epoch 2750, training loss: 62.120399475097656 = 0.005816855002194643 + 10.0 * 6.211458206176758
Epoch 2750, val loss: 1.5880942344665527
Epoch 2760, training loss: 62.100284576416016 = 0.0057664369232952595 + 10.0 * 6.209451675415039
Epoch 2760, val loss: 1.5905605554580688
Epoch 2770, training loss: 62.11737823486328 = 0.00571935810148716 + 10.0 * 6.211165904998779
Epoch 2770, val loss: 1.5926458835601807
Epoch 2780, training loss: 62.09305953979492 = 0.005671242251992226 + 10.0 * 6.208738803863525
Epoch 2780, val loss: 1.594937801361084
Epoch 2790, training loss: 62.090087890625 = 0.005625927355140448 + 10.0 * 6.208446025848389
Epoch 2790, val loss: 1.59718918800354
Epoch 2800, training loss: 62.08114242553711 = 0.005581171717494726 + 10.0 * 6.207556247711182
Epoch 2800, val loss: 1.59940767288208
Epoch 2810, training loss: 62.109344482421875 = 0.005537933669984341 + 10.0 * 6.210380554199219
Epoch 2810, val loss: 1.6014119386672974
Epoch 2820, training loss: 62.1048698425293 = 0.005492610856890678 + 10.0 * 6.209937572479248
Epoch 2820, val loss: 1.6034988164901733
Epoch 2830, training loss: 62.101806640625 = 0.005447134375572205 + 10.0 * 6.2096357345581055
Epoch 2830, val loss: 1.605407953262329
Epoch 2840, training loss: 62.091644287109375 = 0.005403229501098394 + 10.0 * 6.208624362945557
Epoch 2840, val loss: 1.6074343919754028
Epoch 2850, training loss: 62.08926773071289 = 0.0053613027557730675 + 10.0 * 6.208390712738037
Epoch 2850, val loss: 1.6096101999282837
Epoch 2860, training loss: 62.11289978027344 = 0.005319799762219191 + 10.0 * 6.210757732391357
Epoch 2860, val loss: 1.6115432977676392
Epoch 2870, training loss: 62.08216094970703 = 0.005278317723423243 + 10.0 * 6.207688331604004
Epoch 2870, val loss: 1.613702654838562
Epoch 2880, training loss: 62.077083587646484 = 0.005237913690507412 + 10.0 * 6.207184791564941
Epoch 2880, val loss: 1.6157381534576416
Epoch 2890, training loss: 62.078975677490234 = 0.0051988717168569565 + 10.0 * 6.2073774337768555
Epoch 2890, val loss: 1.6177704334259033
Epoch 2900, training loss: 62.09527587890625 = 0.00516005652025342 + 10.0 * 6.209011554718018
Epoch 2900, val loss: 1.6198129653930664
Epoch 2910, training loss: 62.11262130737305 = 0.0051215835846960545 + 10.0 * 6.210749626159668
Epoch 2910, val loss: 1.6218725442886353
Epoch 2920, training loss: 62.09885025024414 = 0.005083059426397085 + 10.0 * 6.209376811981201
Epoch 2920, val loss: 1.6234701871871948
Epoch 2930, training loss: 62.07772445678711 = 0.005042837001383305 + 10.0 * 6.207268238067627
Epoch 2930, val loss: 1.6253939867019653
Epoch 2940, training loss: 62.07251739501953 = 0.00500664534047246 + 10.0 * 6.206751346588135
Epoch 2940, val loss: 1.6272697448730469
Epoch 2950, training loss: 62.068851470947266 = 0.004970825742930174 + 10.0 * 6.206387996673584
Epoch 2950, val loss: 1.6294127702713013
Epoch 2960, training loss: 62.06724166870117 = 0.004934623371809721 + 10.0 * 6.206230640411377
Epoch 2960, val loss: 1.631300926208496
Epoch 2970, training loss: 62.10491180419922 = 0.004900831263512373 + 10.0 * 6.210000991821289
Epoch 2970, val loss: 1.6330969333648682
Epoch 2980, training loss: 62.06451416015625 = 0.004864112474024296 + 10.0 * 6.205965042114258
Epoch 2980, val loss: 1.6349550485610962
Epoch 2990, training loss: 62.067771911621094 = 0.004829574842005968 + 10.0 * 6.206294059753418
Epoch 2990, val loss: 1.6368765830993652
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 87.90924072265625 = 1.940948486328125 + 10.0 * 8.596829414367676
Epoch 0, val loss: 1.9448078870773315
Epoch 10, training loss: 87.8911361694336 = 1.9305113554000854 + 10.0 * 8.596062660217285
Epoch 10, val loss: 1.9335392713546753
Epoch 20, training loss: 87.81481170654297 = 1.9172413349151611 + 10.0 * 8.589756965637207
Epoch 20, val loss: 1.9190431833267212
Epoch 30, training loss: 87.35810089111328 = 1.8999238014221191 + 10.0 * 8.545817375183105
Epoch 30, val loss: 1.9003175497055054
Epoch 40, training loss: 84.59707641601562 = 1.8800628185272217 + 10.0 * 8.271700859069824
Epoch 40, val loss: 1.879581332206726
Epoch 50, training loss: 79.0846176147461 = 1.8585902452468872 + 10.0 * 7.722602844238281
Epoch 50, val loss: 1.8584065437316895
Epoch 60, training loss: 74.9477767944336 = 1.8446881771087646 + 10.0 * 7.310308933258057
Epoch 60, val loss: 1.845353603363037
Epoch 70, training loss: 72.82965087890625 = 1.8334906101226807 + 10.0 * 7.099616527557373
Epoch 70, val loss: 1.8349955081939697
Epoch 80, training loss: 71.48680877685547 = 1.8222428560256958 + 10.0 * 6.966456413269043
Epoch 80, val loss: 1.8244532346725464
Epoch 90, training loss: 70.17041778564453 = 1.8112430572509766 + 10.0 * 6.835917949676514
Epoch 90, val loss: 1.8142142295837402
Epoch 100, training loss: 69.2738265991211 = 1.8016283512115479 + 10.0 * 6.747220039367676
Epoch 100, val loss: 1.8054386377334595
Epoch 110, training loss: 68.66265869140625 = 1.791467308998108 + 10.0 * 6.687119007110596
Epoch 110, val loss: 1.7961167097091675
Epoch 120, training loss: 68.22932434082031 = 1.7803165912628174 + 10.0 * 6.644900321960449
Epoch 120, val loss: 1.7857180833816528
Epoch 130, training loss: 67.86979675292969 = 1.7684978246688843 + 10.0 * 6.6101298332214355
Epoch 130, val loss: 1.7749520540237427
Epoch 140, training loss: 67.57711791992188 = 1.7566198110580444 + 10.0 * 6.58204984664917
Epoch 140, val loss: 1.7641671895980835
Epoch 150, training loss: 67.35154724121094 = 1.7441846132278442 + 10.0 * 6.560735702514648
Epoch 150, val loss: 1.753040075302124
Epoch 160, training loss: 67.0850601196289 = 1.7308200597763062 + 10.0 * 6.53542423248291
Epoch 160, val loss: 1.7412256002426147
Epoch 170, training loss: 66.8634262084961 = 1.716363787651062 + 10.0 * 6.514706134796143
Epoch 170, val loss: 1.7287309169769287
Epoch 180, training loss: 66.74464416503906 = 1.700703501701355 + 10.0 * 6.504393577575684
Epoch 180, val loss: 1.7152583599090576
Epoch 190, training loss: 66.5003433227539 = 1.6832247972488403 + 10.0 * 6.481711387634277
Epoch 190, val loss: 1.700635313987732
Epoch 200, training loss: 66.330810546875 = 1.66429603099823 + 10.0 * 6.46665096282959
Epoch 200, val loss: 1.6848130226135254
Epoch 210, training loss: 66.17845916748047 = 1.6436420679092407 + 10.0 * 6.453481674194336
Epoch 210, val loss: 1.6675945520401
Epoch 220, training loss: 66.04033660888672 = 1.6211905479431152 + 10.0 * 6.441914081573486
Epoch 220, val loss: 1.648913860321045
Epoch 230, training loss: 65.94178771972656 = 1.5967903137207031 + 10.0 * 6.434499263763428
Epoch 230, val loss: 1.6286953687667847
Epoch 240, training loss: 65.81268310546875 = 1.570557713508606 + 10.0 * 6.42421293258667
Epoch 240, val loss: 1.6070406436920166
Epoch 250, training loss: 65.71614074707031 = 1.542646884918213 + 10.0 * 6.417348861694336
Epoch 250, val loss: 1.584014654159546
Epoch 260, training loss: 65.59603881835938 = 1.5132856369018555 + 10.0 * 6.408275604248047
Epoch 260, val loss: 1.5598361492156982
Epoch 270, training loss: 65.49527740478516 = 1.4825068712234497 + 10.0 * 6.401277542114258
Epoch 270, val loss: 1.5346863269805908
Epoch 280, training loss: 65.40572357177734 = 1.450562834739685 + 10.0 * 6.395516395568848
Epoch 280, val loss: 1.5087156295776367
Epoch 290, training loss: 65.33451843261719 = 1.4173935651779175 + 10.0 * 6.391712665557861
Epoch 290, val loss: 1.4822131395339966
Epoch 300, training loss: 65.23297119140625 = 1.3838210105895996 + 10.0 * 6.384915351867676
Epoch 300, val loss: 1.4552949666976929
Epoch 310, training loss: 65.1384048461914 = 1.3499114513397217 + 10.0 * 6.378849029541016
Epoch 310, val loss: 1.428338646888733
Epoch 320, training loss: 65.04732513427734 = 1.3157902956008911 + 10.0 * 6.3731536865234375
Epoch 320, val loss: 1.401563048362732
Epoch 330, training loss: 65.02754974365234 = 1.2816178798675537 + 10.0 * 6.374593257904053
Epoch 330, val loss: 1.3749629259109497
Epoch 340, training loss: 64.90384674072266 = 1.2476074695587158 + 10.0 * 6.365623950958252
Epoch 340, val loss: 1.3487142324447632
Epoch 350, training loss: 64.81192779541016 = 1.2140395641326904 + 10.0 * 6.35978889465332
Epoch 350, val loss: 1.323154091835022
Epoch 360, training loss: 64.75840759277344 = 1.1810333728790283 + 10.0 * 6.3577375411987305
Epoch 360, val loss: 1.2983454465866089
Epoch 370, training loss: 64.69269561767578 = 1.1484848260879517 + 10.0 * 6.354421138763428
Epoch 370, val loss: 1.2742103338241577
Epoch 380, training loss: 64.61124420166016 = 1.1168279647827148 + 10.0 * 6.349442005157471
Epoch 380, val loss: 1.2510778903961182
Epoch 390, training loss: 64.5376968383789 = 1.0860066413879395 + 10.0 * 6.3451690673828125
Epoch 390, val loss: 1.228960633277893
Epoch 400, training loss: 64.49002838134766 = 1.0560795068740845 + 10.0 * 6.343395233154297
Epoch 400, val loss: 1.2077745199203491
Epoch 410, training loss: 64.47721862792969 = 1.0267493724822998 + 10.0 * 6.345046520233154
Epoch 410, val loss: 1.1875519752502441
Epoch 420, training loss: 64.35459899902344 = 0.9987022280693054 + 10.0 * 6.33558988571167
Epoch 420, val loss: 1.1684925556182861
Epoch 430, training loss: 64.30384063720703 = 0.9716843962669373 + 10.0 * 6.333215236663818
Epoch 430, val loss: 1.1506950855255127
Epoch 440, training loss: 64.24736785888672 = 0.9456888437271118 + 10.0 * 6.3301682472229
Epoch 440, val loss: 1.1341208219528198
Epoch 450, training loss: 64.19686126708984 = 0.9206842184066772 + 10.0 * 6.327617645263672
Epoch 450, val loss: 1.1186647415161133
Epoch 460, training loss: 64.1678466796875 = 0.8965398073196411 + 10.0 * 6.3271307945251465
Epoch 460, val loss: 1.104045033454895
Epoch 470, training loss: 64.11339569091797 = 0.8733572959899902 + 10.0 * 6.32400369644165
Epoch 470, val loss: 1.0906622409820557
Epoch 480, training loss: 64.06419372558594 = 0.8513027429580688 + 10.0 * 6.3212890625
Epoch 480, val loss: 1.0784367322921753
Epoch 490, training loss: 64.02556610107422 = 0.8301914930343628 + 10.0 * 6.319537162780762
Epoch 490, val loss: 1.0672221183776855
Epoch 500, training loss: 63.98406982421875 = 0.8099958896636963 + 10.0 * 6.317407131195068
Epoch 500, val loss: 1.0568264722824097
Epoch 510, training loss: 63.93230056762695 = 0.7905858159065247 + 10.0 * 6.31417179107666
Epoch 510, val loss: 1.0473511219024658
Epoch 520, training loss: 63.89268112182617 = 0.7719961404800415 + 10.0 * 6.312068462371826
Epoch 520, val loss: 1.0387018918991089
Epoch 530, training loss: 63.91630935668945 = 0.7542550563812256 + 10.0 * 6.3162055015563965
Epoch 530, val loss: 1.0308737754821777
Epoch 540, training loss: 63.879730224609375 = 0.7368588447570801 + 10.0 * 6.314287185668945
Epoch 540, val loss: 1.0235978364944458
Epoch 550, training loss: 63.789485931396484 = 0.7204012274742126 + 10.0 * 6.30690860748291
Epoch 550, val loss: 1.0171195268630981
Epoch 560, training loss: 63.75657653808594 = 0.7046219110488892 + 10.0 * 6.3051958084106445
Epoch 560, val loss: 1.0114293098449707
Epoch 570, training loss: 63.71947479248047 = 0.6893613338470459 + 10.0 * 6.303011417388916
Epoch 570, val loss: 1.006380319595337
Epoch 580, training loss: 63.79249572753906 = 0.6745470762252808 + 10.0 * 6.311795234680176
Epoch 580, val loss: 1.0017874240875244
Epoch 590, training loss: 63.68321228027344 = 0.660212516784668 + 10.0 * 6.302299976348877
Epoch 590, val loss: 0.9976012110710144
Epoch 600, training loss: 63.63323974609375 = 0.6464091539382935 + 10.0 * 6.298683166503906
Epoch 600, val loss: 0.9940242171287537
Epoch 610, training loss: 63.61012649536133 = 0.6330689787864685 + 10.0 * 6.29770565032959
Epoch 610, val loss: 0.9909515380859375
Epoch 620, training loss: 63.5894775390625 = 0.6200481057167053 + 10.0 * 6.296942710876465
Epoch 620, val loss: 0.9882532358169556
Epoch 630, training loss: 63.56251525878906 = 0.6073700189590454 + 10.0 * 6.2955145835876465
Epoch 630, val loss: 0.9859327673912048
Epoch 640, training loss: 63.57476806640625 = 0.5949550867080688 + 10.0 * 6.297981262207031
Epoch 640, val loss: 0.983904242515564
Epoch 650, training loss: 63.4925651550293 = 0.5829746723175049 + 10.0 * 6.290959358215332
Epoch 650, val loss: 0.982228696346283
Epoch 660, training loss: 63.476593017578125 = 0.5713275074958801 + 10.0 * 6.290526390075684
Epoch 660, val loss: 0.9810225963592529
Epoch 670, training loss: 63.442081451416016 = 0.5599005818367004 + 10.0 * 6.288218021392822
Epoch 670, val loss: 0.9801362156867981
Epoch 680, training loss: 63.44124984741211 = 0.5487507581710815 + 10.0 * 6.289249897003174
Epoch 680, val loss: 0.9795793294906616
Epoch 690, training loss: 63.41408157348633 = 0.5377197265625 + 10.0 * 6.2876362800598145
Epoch 690, val loss: 0.9790095686912537
Epoch 700, training loss: 63.378517150878906 = 0.5268961787223816 + 10.0 * 6.285161972045898
Epoch 700, val loss: 0.9788252115249634
Epoch 710, training loss: 63.35472869873047 = 0.516324520111084 + 10.0 * 6.283840656280518
Epoch 710, val loss: 0.9789391160011292
Epoch 720, training loss: 63.37290954589844 = 0.5058932304382324 + 10.0 * 6.286701679229736
Epoch 720, val loss: 0.9792658090591431
Epoch 730, training loss: 63.30952453613281 = 0.495633065700531 + 10.0 * 6.281389236450195
Epoch 730, val loss: 0.9797468185424805
Epoch 740, training loss: 63.291831970214844 = 0.48548170924186707 + 10.0 * 6.280634880065918
Epoch 740, val loss: 0.9805670976638794
Epoch 750, training loss: 63.33574676513672 = 0.4754313826560974 + 10.0 * 6.286031723022461
Epoch 750, val loss: 0.9814292788505554
Epoch 760, training loss: 63.25265884399414 = 0.4655618369579315 + 10.0 * 6.278709888458252
Epoch 760, val loss: 0.9826086163520813
Epoch 770, training loss: 63.222862243652344 = 0.45571351051330566 + 10.0 * 6.27671480178833
Epoch 770, val loss: 0.9838676452636719
Epoch 780, training loss: 63.20833206176758 = 0.44602498412132263 + 10.0 * 6.276230812072754
Epoch 780, val loss: 0.9854916930198669
Epoch 790, training loss: 63.219390869140625 = 0.4363461136817932 + 10.0 * 6.278304100036621
Epoch 790, val loss: 0.9870929718017578
Epoch 800, training loss: 63.171024322509766 = 0.4267265200614929 + 10.0 * 6.274429798126221
Epoch 800, val loss: 0.9888039827346802
Epoch 810, training loss: 63.144142150878906 = 0.41721951961517334 + 10.0 * 6.2726922035217285
Epoch 810, val loss: 0.9908478856086731
Epoch 820, training loss: 63.149200439453125 = 0.4077603816986084 + 10.0 * 6.274144172668457
Epoch 820, val loss: 0.9930080771446228
Epoch 830, training loss: 63.11224365234375 = 0.3983725905418396 + 10.0 * 6.271387100219727
Epoch 830, val loss: 0.9954005479812622
Epoch 840, training loss: 63.13588333129883 = 0.38898766040802 + 10.0 * 6.274689674377441
Epoch 840, val loss: 0.9976322650909424
Epoch 850, training loss: 63.06816101074219 = 0.37973156571388245 + 10.0 * 6.268843173980713
Epoch 850, val loss: 1.00052011013031
Epoch 860, training loss: 63.05028533935547 = 0.37050777673721313 + 10.0 * 6.267977714538574
Epoch 860, val loss: 1.0033644437789917
Epoch 870, training loss: 63.06216049194336 = 0.36138588190078735 + 10.0 * 6.270077705383301
Epoch 870, val loss: 1.0064078569412231
Epoch 880, training loss: 63.021629333496094 = 0.3522968292236328 + 10.0 * 6.266933441162109
Epoch 880, val loss: 1.0096522569656372
Epoch 890, training loss: 63.02791976928711 = 0.3432959020137787 + 10.0 * 6.268462181091309
Epoch 890, val loss: 1.012740969657898
Epoch 900, training loss: 63.00621032714844 = 0.3343653678894043 + 10.0 * 6.267184257507324
Epoch 900, val loss: 1.0163464546203613
Epoch 910, training loss: 62.97359085083008 = 0.3255247473716736 + 10.0 * 6.264806747436523
Epoch 910, val loss: 1.0197322368621826
Epoch 920, training loss: 62.945823669433594 = 0.3167569935321808 + 10.0 * 6.262906551361084
Epoch 920, val loss: 1.023615837097168
Epoch 930, training loss: 62.93708419799805 = 0.3080875277519226 + 10.0 * 6.262899875640869
Epoch 930, val loss: 1.0274348258972168
Epoch 940, training loss: 62.98112869262695 = 0.2994750440120697 + 10.0 * 6.268165111541748
Epoch 940, val loss: 1.0315334796905518
Epoch 950, training loss: 62.90683364868164 = 0.2907995581626892 + 10.0 * 6.261603355407715
Epoch 950, val loss: 1.0348299741744995
Epoch 960, training loss: 62.88338851928711 = 0.2823125422000885 + 10.0 * 6.260107517242432
Epoch 960, val loss: 1.0390524864196777
Epoch 970, training loss: 62.85844421386719 = 0.27392780780792236 + 10.0 * 6.258451461791992
Epoch 970, val loss: 1.0432101488113403
Epoch 980, training loss: 62.866058349609375 = 0.2656315267086029 + 10.0 * 6.260042667388916
Epoch 980, val loss: 1.047424554824829
Epoch 990, training loss: 62.84327697753906 = 0.25737106800079346 + 10.0 * 6.2585906982421875
Epoch 990, val loss: 1.051780343055725
Epoch 1000, training loss: 62.82360076904297 = 0.24920019507408142 + 10.0 * 6.257440090179443
Epoch 1000, val loss: 1.056136965751648
Epoch 1010, training loss: 62.821311950683594 = 0.24118749797344208 + 10.0 * 6.258012294769287
Epoch 1010, val loss: 1.0606735944747925
Epoch 1020, training loss: 62.802677154541016 = 0.23327308893203735 + 10.0 * 6.2569403648376465
Epoch 1020, val loss: 1.065405011177063
Epoch 1030, training loss: 62.77512741088867 = 0.22555100917816162 + 10.0 * 6.254957675933838
Epoch 1030, val loss: 1.0702767372131348
Epoch 1040, training loss: 62.835567474365234 = 0.21796241402626038 + 10.0 * 6.261760234832764
Epoch 1040, val loss: 1.0753355026245117
Epoch 1050, training loss: 62.76953125 = 0.2104181945323944 + 10.0 * 6.255911350250244
Epoch 1050, val loss: 1.0798417329788208
Epoch 1060, training loss: 62.73604202270508 = 0.20316839218139648 + 10.0 * 6.253287315368652
Epoch 1060, val loss: 1.08516526222229
Epoch 1070, training loss: 62.715450286865234 = 0.19607844948768616 + 10.0 * 6.251936912536621
Epoch 1070, val loss: 1.090410590171814
Epoch 1080, training loss: 62.735137939453125 = 0.1891949325799942 + 10.0 * 6.254594326019287
Epoch 1080, val loss: 1.0957201719284058
Epoch 1090, training loss: 62.721107482910156 = 0.18248842656612396 + 10.0 * 6.253861904144287
Epoch 1090, val loss: 1.1014512777328491
Epoch 1100, training loss: 62.688682556152344 = 0.17595688998699188 + 10.0 * 6.251272678375244
Epoch 1100, val loss: 1.1070287227630615
Epoch 1110, training loss: 62.66803741455078 = 0.1696791797876358 + 10.0 * 6.249835968017578
Epoch 1110, val loss: 1.1129937171936035
Epoch 1120, training loss: 62.717227935791016 = 0.16363933682441711 + 10.0 * 6.255358695983887
Epoch 1120, val loss: 1.1191394329071045
Epoch 1130, training loss: 62.661842346191406 = 0.15776991844177246 + 10.0 * 6.2504072189331055
Epoch 1130, val loss: 1.125138759613037
Epoch 1140, training loss: 62.64404296875 = 0.15213295817375183 + 10.0 * 6.249190807342529
Epoch 1140, val loss: 1.1314098834991455
Epoch 1150, training loss: 62.686851501464844 = 0.1467217057943344 + 10.0 * 6.2540130615234375
Epoch 1150, val loss: 1.1377705335617065
Epoch 1160, training loss: 62.6214714050293 = 0.14152300357818604 + 10.0 * 6.247994899749756
Epoch 1160, val loss: 1.1445657014846802
Epoch 1170, training loss: 62.60182189941406 = 0.1365332454442978 + 10.0 * 6.2465291023254395
Epoch 1170, val loss: 1.1512651443481445
Epoch 1180, training loss: 62.62420654296875 = 0.13176320493221283 + 10.0 * 6.249244213104248
Epoch 1180, val loss: 1.1581509113311768
Epoch 1190, training loss: 62.57764434814453 = 0.12715338170528412 + 10.0 * 6.245049476623535
Epoch 1190, val loss: 1.1649028062820435
Epoch 1200, training loss: 62.57600784301758 = 0.12273220717906952 + 10.0 * 6.245327472686768
Epoch 1200, val loss: 1.171803593635559
Epoch 1210, training loss: 62.5980224609375 = 0.11851147562265396 + 10.0 * 6.247951030731201
Epoch 1210, val loss: 1.1788901090621948
Epoch 1220, training loss: 62.58119201660156 = 0.11447227001190186 + 10.0 * 6.2466721534729
Epoch 1220, val loss: 1.1859266757965088
Epoch 1230, training loss: 62.542510986328125 = 0.11058581620454788 + 10.0 * 6.243192672729492
Epoch 1230, val loss: 1.1933300495147705
Epoch 1240, training loss: 62.53423309326172 = 0.10688565671443939 + 10.0 * 6.242734909057617
Epoch 1240, val loss: 1.2008812427520752
Epoch 1250, training loss: 62.55552291870117 = 0.10337530076503754 + 10.0 * 6.245214939117432
Epoch 1250, val loss: 1.2085562944412231
Epoch 1260, training loss: 62.540557861328125 = 0.09993303567171097 + 10.0 * 6.244062423706055
Epoch 1260, val loss: 1.2156257629394531
Epoch 1270, training loss: 62.524330139160156 = 0.09664095938205719 + 10.0 * 6.24276876449585
Epoch 1270, val loss: 1.2229318618774414
Epoch 1280, training loss: 62.52255630493164 = 0.09352726489305496 + 10.0 * 6.242902755737305
Epoch 1280, val loss: 1.2307974100112915
Epoch 1290, training loss: 62.51765441894531 = 0.09051557630300522 + 10.0 * 6.242713928222656
Epoch 1290, val loss: 1.2380518913269043
Epoch 1300, training loss: 62.508689880371094 = 0.08763157576322556 + 10.0 * 6.242105960845947
Epoch 1300, val loss: 1.245713710784912
Epoch 1310, training loss: 62.50965881347656 = 0.0848698541522026 + 10.0 * 6.242478847503662
Epoch 1310, val loss: 1.2534037828445435
Epoch 1320, training loss: 62.48765182495117 = 0.08222218602895737 + 10.0 * 6.240542888641357
Epoch 1320, val loss: 1.2608994245529175
Epoch 1330, training loss: 62.52393341064453 = 0.0796695128083229 + 10.0 * 6.244426250457764
Epoch 1330, val loss: 1.2684109210968018
Epoch 1340, training loss: 62.49274444580078 = 0.0772012248635292 + 10.0 * 6.241554260253906
Epoch 1340, val loss: 1.275842308998108
Epoch 1350, training loss: 62.459999084472656 = 0.0748530849814415 + 10.0 * 6.2385149002075195
Epoch 1350, val loss: 1.2836799621582031
Epoch 1360, training loss: 62.451995849609375 = 0.07261545956134796 + 10.0 * 6.237937927246094
Epoch 1360, val loss: 1.2914072275161743
Epoch 1370, training loss: 62.48823928833008 = 0.07047513872385025 + 10.0 * 6.241776466369629
Epoch 1370, val loss: 1.299238920211792
Epoch 1380, training loss: 62.44438934326172 = 0.0683593899011612 + 10.0 * 6.237603187561035
Epoch 1380, val loss: 1.3061704635620117
Epoch 1390, training loss: 62.43632507324219 = 0.0663372352719307 + 10.0 * 6.236998558044434
Epoch 1390, val loss: 1.3137307167053223
Epoch 1400, training loss: 62.44673156738281 = 0.06442295759916306 + 10.0 * 6.2382307052612305
Epoch 1400, val loss: 1.3213403224945068
Epoch 1410, training loss: 62.422340393066406 = 0.0625772476196289 + 10.0 * 6.235976219177246
Epoch 1410, val loss: 1.3288216590881348
Epoch 1420, training loss: 62.43150329589844 = 0.06080770492553711 + 10.0 * 6.237069606781006
Epoch 1420, val loss: 1.3362971544265747
Epoch 1430, training loss: 62.44473648071289 = 0.059109993278980255 + 10.0 * 6.23856258392334
Epoch 1430, val loss: 1.3437808752059937
Epoch 1440, training loss: 62.41587829589844 = 0.057438187301158905 + 10.0 * 6.235844135284424
Epoch 1440, val loss: 1.3507994413375854
Epoch 1450, training loss: 62.4155387878418 = 0.05585041642189026 + 10.0 * 6.235968589782715
Epoch 1450, val loss: 1.3579838275909424
Epoch 1460, training loss: 62.394466400146484 = 0.0543263703584671 + 10.0 * 6.23401403427124
Epoch 1460, val loss: 1.3651655912399292
Epoch 1470, training loss: 62.408329010009766 = 0.05286894738674164 + 10.0 * 6.235546112060547
Epoch 1470, val loss: 1.372431755065918
Epoch 1480, training loss: 62.396732330322266 = 0.05145714059472084 + 10.0 * 6.234527587890625
Epoch 1480, val loss: 1.3794581890106201
Epoch 1490, training loss: 62.39081954956055 = 0.05008663982152939 + 10.0 * 6.234073162078857
Epoch 1490, val loss: 1.386542558670044
Epoch 1500, training loss: 62.390602111816406 = 0.04877408593893051 + 10.0 * 6.234182834625244
Epoch 1500, val loss: 1.3934918642044067
Epoch 1510, training loss: 62.38555145263672 = 0.04750877246260643 + 10.0 * 6.233804225921631
Epoch 1510, val loss: 1.4002723693847656
Epoch 1520, training loss: 62.388126373291016 = 0.04628327861428261 + 10.0 * 6.234184265136719
Epoch 1520, val loss: 1.4071122407913208
Epoch 1530, training loss: 62.370357513427734 = 0.04511437192559242 + 10.0 * 6.232524394989014
Epoch 1530, val loss: 1.4141387939453125
Epoch 1540, training loss: 62.36577606201172 = 0.04398094117641449 + 10.0 * 6.232179641723633
Epoch 1540, val loss: 1.4207944869995117
Epoch 1550, training loss: 62.433570861816406 = 0.04288458079099655 + 10.0 * 6.239068508148193
Epoch 1550, val loss: 1.4271917343139648
Epoch 1560, training loss: 62.36255645751953 = 0.04181332513689995 + 10.0 * 6.23207426071167
Epoch 1560, val loss: 1.4341809749603271
Epoch 1570, training loss: 62.337684631347656 = 0.04078684747219086 + 10.0 * 6.229689598083496
Epoch 1570, val loss: 1.4406683444976807
Epoch 1580, training loss: 62.331092834472656 = 0.03980926051735878 + 10.0 * 6.229128360748291
Epoch 1580, val loss: 1.4474400281906128
Epoch 1590, training loss: 62.335113525390625 = 0.038879651576280594 + 10.0 * 6.229623317718506
Epoch 1590, val loss: 1.4541577100753784
Epoch 1600, training loss: 62.400753021240234 = 0.03797585144639015 + 10.0 * 6.2362775802612305
Epoch 1600, val loss: 1.4605026245117188
Epoch 1610, training loss: 62.345211029052734 = 0.03705872595310211 + 10.0 * 6.2308149337768555
Epoch 1610, val loss: 1.466949224472046
Epoch 1620, training loss: 62.33822250366211 = 0.03621089085936546 + 10.0 * 6.230201244354248
Epoch 1620, val loss: 1.4734309911727905
Epoch 1630, training loss: 62.35310745239258 = 0.035381391644477844 + 10.0 * 6.231772422790527
Epoch 1630, val loss: 1.479722261428833
Epoch 1640, training loss: 62.34954833984375 = 0.03457656875252724 + 10.0 * 6.231497287750244
Epoch 1640, val loss: 1.4860550165176392
Epoch 1650, training loss: 62.324432373046875 = 0.03379670903086662 + 10.0 * 6.229063510894775
Epoch 1650, val loss: 1.492161512374878
Epoch 1660, training loss: 62.31916427612305 = 0.03305679187178612 + 10.0 * 6.228610515594482
Epoch 1660, val loss: 1.4985145330429077
Epoch 1670, training loss: 62.324424743652344 = 0.03232945129275322 + 10.0 * 6.2292094230651855
Epoch 1670, val loss: 1.504530429840088
Epoch 1680, training loss: 62.31028747558594 = 0.031623490154743195 + 10.0 * 6.227866172790527
Epoch 1680, val loss: 1.5105186700820923
Epoch 1690, training loss: 62.33357620239258 = 0.030944915488362312 + 10.0 * 6.2302632331848145
Epoch 1690, val loss: 1.5164722204208374
Epoch 1700, training loss: 62.30375289916992 = 0.030284497886896133 + 10.0 * 6.227346897125244
Epoch 1700, val loss: 1.5225499868392944
Epoch 1710, training loss: 62.29600143432617 = 0.029648179188370705 + 10.0 * 6.226635456085205
Epoch 1710, val loss: 1.528356909751892
Epoch 1720, training loss: 62.310733795166016 = 0.029039693996310234 + 10.0 * 6.2281694412231445
Epoch 1720, val loss: 1.534538984298706
Epoch 1730, training loss: 62.30864715576172 = 0.028431836515665054 + 10.0 * 6.228021621704102
Epoch 1730, val loss: 1.5399938821792603
Epoch 1740, training loss: 62.27885437011719 = 0.027832036837935448 + 10.0 * 6.225102424621582
Epoch 1740, val loss: 1.545547366142273
Epoch 1750, training loss: 62.27326202392578 = 0.02727733738720417 + 10.0 * 6.224598407745361
Epoch 1750, val loss: 1.5515365600585938
Epoch 1760, training loss: 62.27619171142578 = 0.02673843502998352 + 10.0 * 6.224945545196533
Epoch 1760, val loss: 1.557083249092102
Epoch 1770, training loss: 62.31558609008789 = 0.02621898613870144 + 10.0 * 6.228936672210693
Epoch 1770, val loss: 1.5627964735031128
Epoch 1780, training loss: 62.34480285644531 = 0.02568761631846428 + 10.0 * 6.231911659240723
Epoch 1780, val loss: 1.5683932304382324
Epoch 1790, training loss: 62.290428161621094 = 0.025179320946335793 + 10.0 * 6.226524829864502
Epoch 1790, val loss: 1.573585867881775
Epoch 1800, training loss: 62.29771423339844 = 0.024687577039003372 + 10.0 * 6.227302551269531
Epoch 1800, val loss: 1.5791417360305786
Epoch 1810, training loss: 62.25804138183594 = 0.02421013079583645 + 10.0 * 6.223382949829102
Epoch 1810, val loss: 1.5845363140106201
Epoch 1820, training loss: 62.25379943847656 = 0.023760471493005753 + 10.0 * 6.22300386428833
Epoch 1820, val loss: 1.5902893543243408
Epoch 1830, training loss: 62.25653076171875 = 0.02332225814461708 + 10.0 * 6.223320960998535
Epoch 1830, val loss: 1.5955841541290283
Epoch 1840, training loss: 62.282405853271484 = 0.022899366915225983 + 10.0 * 6.225950717926025
Epoch 1840, val loss: 1.6010180711746216
Epoch 1850, training loss: 62.29944610595703 = 0.022471170872449875 + 10.0 * 6.227697372436523
Epoch 1850, val loss: 1.6059619188308716
Epoch 1860, training loss: 62.25208282470703 = 0.02204306051135063 + 10.0 * 6.223004341125488
Epoch 1860, val loss: 1.6108022928237915
Epoch 1870, training loss: 62.237579345703125 = 0.02165340632200241 + 10.0 * 6.221592903137207
Epoch 1870, val loss: 1.616419792175293
Epoch 1880, training loss: 62.2312126159668 = 0.021268678829073906 + 10.0 * 6.220994472503662
Epoch 1880, val loss: 1.6214936971664429
Epoch 1890, training loss: 62.26725769042969 = 0.020902764052152634 + 10.0 * 6.224635124206543
Epoch 1890, val loss: 1.6267551183700562
Epoch 1900, training loss: 62.2454948425293 = 0.02053152397274971 + 10.0 * 6.222496509552002
Epoch 1900, val loss: 1.6312265396118164
Epoch 1910, training loss: 62.249359130859375 = 0.02016373537480831 + 10.0 * 6.222919464111328
Epoch 1910, val loss: 1.6364537477493286
Epoch 1920, training loss: 62.221317291259766 = 0.01981622539460659 + 10.0 * 6.220149993896484
Epoch 1920, val loss: 1.641430139541626
Epoch 1930, training loss: 62.21437072753906 = 0.019486399367451668 + 10.0 * 6.219488620758057
Epoch 1930, val loss: 1.6465741395950317
Epoch 1940, training loss: 62.26194763183594 = 0.019171882420778275 + 10.0 * 6.224277496337891
Epoch 1940, val loss: 1.651672124862671
Epoch 1950, training loss: 62.22319793701172 = 0.01883779838681221 + 10.0 * 6.220436096191406
Epoch 1950, val loss: 1.6557965278625488
Epoch 1960, training loss: 62.22050476074219 = 0.0185183584690094 + 10.0 * 6.220198631286621
Epoch 1960, val loss: 1.6608327627182007
Epoch 1970, training loss: 62.21856689453125 = 0.018214749172329903 + 10.0 * 6.220035076141357
Epoch 1970, val loss: 1.6656211614608765
Epoch 1980, training loss: 62.21945571899414 = 0.017921248450875282 + 10.0 * 6.220153331756592
Epoch 1980, val loss: 1.6702525615692139
Epoch 1990, training loss: 62.23021697998047 = 0.017636552453041077 + 10.0 * 6.221258163452148
Epoch 1990, val loss: 1.675005555152893
Epoch 2000, training loss: 62.214630126953125 = 0.017346162348985672 + 10.0 * 6.219728469848633
Epoch 2000, val loss: 1.679491400718689
Epoch 2010, training loss: 62.23556900024414 = 0.01708013378083706 + 10.0 * 6.221848964691162
Epoch 2010, val loss: 1.6842676401138306
Epoch 2020, training loss: 62.204444885253906 = 0.016801588237285614 + 10.0 * 6.218764305114746
Epoch 2020, val loss: 1.6886074542999268
Epoch 2030, training loss: 62.20990753173828 = 0.016538303345441818 + 10.0 * 6.219336986541748
Epoch 2030, val loss: 1.6930444240570068
Epoch 2040, training loss: 62.199302673339844 = 0.016285177320241928 + 10.0 * 6.218301773071289
Epoch 2040, val loss: 1.6975268125534058
Epoch 2050, training loss: 62.198822021484375 = 0.016038816422224045 + 10.0 * 6.218278408050537
Epoch 2050, val loss: 1.7022029161453247
Epoch 2060, training loss: 62.21555709838867 = 0.015797914937138557 + 10.0 * 6.21997594833374
Epoch 2060, val loss: 1.7064656019210815
Epoch 2070, training loss: 62.22357940673828 = 0.01556028425693512 + 10.0 * 6.220801830291748
Epoch 2070, val loss: 1.7106447219848633
Epoch 2080, training loss: 62.21533203125 = 0.015321465209126472 + 10.0 * 6.220001220703125
Epoch 2080, val loss: 1.7151906490325928
Epoch 2090, training loss: 62.20640182495117 = 0.015093388967216015 + 10.0 * 6.219130516052246
Epoch 2090, val loss: 1.7191194295883179
Epoch 2100, training loss: 62.18294143676758 = 0.014872276224195957 + 10.0 * 6.216806888580322
Epoch 2100, val loss: 1.7236343622207642
Epoch 2110, training loss: 62.17778015136719 = 0.014660174027085304 + 10.0 * 6.216311931610107
Epoch 2110, val loss: 1.727917194366455
Epoch 2120, training loss: 62.23088836669922 = 0.014455243945121765 + 10.0 * 6.221643447875977
Epoch 2120, val loss: 1.7322814464569092
Epoch 2130, training loss: 62.17820358276367 = 0.014243091456592083 + 10.0 * 6.216395854949951
Epoch 2130, val loss: 1.7363669872283936
Epoch 2140, training loss: 62.1755256652832 = 0.014039751142263412 + 10.0 * 6.216148853302002
Epoch 2140, val loss: 1.7404961585998535
Epoch 2150, training loss: 62.24903106689453 = 0.013840412721037865 + 10.0 * 6.223519325256348
Epoch 2150, val loss: 1.7441743612289429
Epoch 2160, training loss: 62.186649322509766 = 0.0136471102014184 + 10.0 * 6.217299938201904
Epoch 2160, val loss: 1.7485278844833374
Epoch 2170, training loss: 62.16428756713867 = 0.01345408707857132 + 10.0 * 6.215083122253418
Epoch 2170, val loss: 1.752518892288208
Epoch 2180, training loss: 62.158870697021484 = 0.013273714110255241 + 10.0 * 6.214559555053711
Epoch 2180, val loss: 1.7567585706710815
Epoch 2190, training loss: 62.2022590637207 = 0.013099275529384613 + 10.0 * 6.218915939331055
Epoch 2190, val loss: 1.7606405019760132
Epoch 2200, training loss: 62.1546745300293 = 0.012918193824589252 + 10.0 * 6.214175701141357
Epoch 2200, val loss: 1.7646067142486572
Epoch 2210, training loss: 62.166378021240234 = 0.01274591963738203 + 10.0 * 6.215363502502441
Epoch 2210, val loss: 1.7685047388076782
Epoch 2220, training loss: 62.15890884399414 = 0.01257568784058094 + 10.0 * 6.214632987976074
Epoch 2220, val loss: 1.7724308967590332
Epoch 2230, training loss: 62.175838470458984 = 0.012413376942276955 + 10.0 * 6.216342449188232
Epoch 2230, val loss: 1.7764734029769897
Epoch 2240, training loss: 62.19540023803711 = 0.012248787097632885 + 10.0 * 6.218315124511719
Epoch 2240, val loss: 1.7800343036651611
Epoch 2250, training loss: 62.14740753173828 = 0.012084349989891052 + 10.0 * 6.213532447814941
Epoch 2250, val loss: 1.7838830947875977
Epoch 2260, training loss: 62.13991928100586 = 0.011926357634365559 + 10.0 * 6.212799549102783
Epoch 2260, val loss: 1.7876665592193604
Epoch 2270, training loss: 62.1588020324707 = 0.011778241023421288 + 10.0 * 6.214702129364014
Epoch 2270, val loss: 1.7914958000183105
Epoch 2280, training loss: 62.17327880859375 = 0.011628729291260242 + 10.0 * 6.216165065765381
Epoch 2280, val loss: 1.7952039241790771
Epoch 2290, training loss: 62.15122604370117 = 0.011485214345157146 + 10.0 * 6.2139739990234375
Epoch 2290, val loss: 1.7989559173583984
Epoch 2300, training loss: 62.13578796386719 = 0.011341412551701069 + 10.0 * 6.21244478225708
Epoch 2300, val loss: 1.802700161933899
Epoch 2310, training loss: 62.15871047973633 = 0.011203116737306118 + 10.0 * 6.21475076675415
Epoch 2310, val loss: 1.8062459230422974
Epoch 2320, training loss: 62.156620025634766 = 0.011064684018492699 + 10.0 * 6.214555263519287
Epoch 2320, val loss: 1.8098697662353516
Epoch 2330, training loss: 62.15690994262695 = 0.010929630137979984 + 10.0 * 6.214598178863525
Epoch 2330, val loss: 1.8135199546813965
Epoch 2340, training loss: 62.15642547607422 = 0.010797173716127872 + 10.0 * 6.214562892913818
Epoch 2340, val loss: 1.816936731338501
Epoch 2350, training loss: 62.136253356933594 = 0.010667300783097744 + 10.0 * 6.212558746337891
Epoch 2350, val loss: 1.820742130279541
Epoch 2360, training loss: 62.13783264160156 = 0.010543749667704105 + 10.0 * 6.212728977203369
Epoch 2360, val loss: 1.8242650032043457
Epoch 2370, training loss: 62.154808044433594 = 0.010422022081911564 + 10.0 * 6.214438438415527
Epoch 2370, val loss: 1.8276666402816772
Epoch 2380, training loss: 62.1202392578125 = 0.01029464416205883 + 10.0 * 6.210994243621826
Epoch 2380, val loss: 1.8310363292694092
Epoch 2390, training loss: 62.140567779541016 = 0.010176008567214012 + 10.0 * 6.213038921356201
Epoch 2390, val loss: 1.8343291282653809
Epoch 2400, training loss: 62.1581916809082 = 0.010057819075882435 + 10.0 * 6.214813232421875
Epoch 2400, val loss: 1.8378403186798096
Epoch 2410, training loss: 62.15373611450195 = 0.00994392205029726 + 10.0 * 6.21437931060791
Epoch 2410, val loss: 1.8413887023925781
Epoch 2420, training loss: 62.13787841796875 = 0.009825961664319038 + 10.0 * 6.212805271148682
Epoch 2420, val loss: 1.8444464206695557
Epoch 2430, training loss: 62.1249885559082 = 0.009717014618217945 + 10.0 * 6.211527347564697
Epoch 2430, val loss: 1.8481098413467407
Epoch 2440, training loss: 62.11305236816406 = 0.009607626125216484 + 10.0 * 6.210344314575195
Epoch 2440, val loss: 1.8511793613433838
Epoch 2450, training loss: 62.11381530761719 = 0.009504351764917374 + 10.0 * 6.210431098937988
Epoch 2450, val loss: 1.854493260383606
Epoch 2460, training loss: 62.13793182373047 = 0.009403684176504612 + 10.0 * 6.212852954864502
Epoch 2460, val loss: 1.8577207326889038
Epoch 2470, training loss: 62.154579162597656 = 0.009303673170506954 + 10.0 * 6.214527606964111
Epoch 2470, val loss: 1.8609988689422607
Epoch 2480, training loss: 62.13330841064453 = 0.009192237630486488 + 10.0 * 6.212411403656006
Epoch 2480, val loss: 1.863805890083313
Epoch 2490, training loss: 62.1130485534668 = 0.009087866172194481 + 10.0 * 6.2103962898254395
Epoch 2490, val loss: 1.866872787475586
Epoch 2500, training loss: 62.099796295166016 = 0.008992341347038746 + 10.0 * 6.209080696105957
Epoch 2500, val loss: 1.870163083076477
Epoch 2510, training loss: 62.09817886352539 = 0.00890133623033762 + 10.0 * 6.208928108215332
Epoch 2510, val loss: 1.8733301162719727
Epoch 2520, training loss: 62.183876037597656 = 0.008811756037175655 + 10.0 * 6.217506408691406
Epoch 2520, val loss: 1.8762822151184082
Epoch 2530, training loss: 62.107540130615234 = 0.008711643517017365 + 10.0 * 6.209882736206055
Epoch 2530, val loss: 1.8791940212249756
Epoch 2540, training loss: 62.09540557861328 = 0.008621486835181713 + 10.0 * 6.208678245544434
Epoch 2540, val loss: 1.882355809211731
Epoch 2550, training loss: 62.13074493408203 = 0.008534244261682034 + 10.0 * 6.212221145629883
Epoch 2550, val loss: 1.885474681854248
Epoch 2560, training loss: 62.08917236328125 = 0.008444411680102348 + 10.0 * 6.208072662353516
Epoch 2560, val loss: 1.8880844116210938
Epoch 2570, training loss: 62.093360900878906 = 0.008358459919691086 + 10.0 * 6.208500385284424
Epoch 2570, val loss: 1.8910300731658936
Epoch 2580, training loss: 62.104244232177734 = 0.008275006897747517 + 10.0 * 6.209597110748291
Epoch 2580, val loss: 1.8939118385314941
Epoch 2590, training loss: 62.15633773803711 = 0.00819163117557764 + 10.0 * 6.21481466293335
Epoch 2590, val loss: 1.8964309692382812
Epoch 2600, training loss: 62.116661071777344 = 0.008108836598694324 + 10.0 * 6.210855007171631
Epoch 2600, val loss: 1.8996726274490356
Epoch 2610, training loss: 62.120079040527344 = 0.00802637916058302 + 10.0 * 6.21120548248291
Epoch 2610, val loss: 1.9022393226623535
Epoch 2620, training loss: 62.095909118652344 = 0.007948337122797966 + 10.0 * 6.20879602432251
Epoch 2620, val loss: 1.905180811882019
Epoch 2630, training loss: 62.09637451171875 = 0.007872505113482475 + 10.0 * 6.208849906921387
Epoch 2630, val loss: 1.9077324867248535
Epoch 2640, training loss: 62.086734771728516 = 0.007797201629728079 + 10.0 * 6.2078938484191895
Epoch 2640, val loss: 1.91078782081604
Epoch 2650, training loss: 62.10039520263672 = 0.007724454160779715 + 10.0 * 6.2092671394348145
Epoch 2650, val loss: 1.9134876728057861
Epoch 2660, training loss: 62.12129592895508 = 0.007650601211935282 + 10.0 * 6.211364269256592
Epoch 2660, val loss: 1.9163024425506592
Epoch 2670, training loss: 62.08157730102539 = 0.007572967559099197 + 10.0 * 6.207400321960449
Epoch 2670, val loss: 1.9187874794006348
Epoch 2680, training loss: 62.096683502197266 = 0.007502544205635786 + 10.0 * 6.20891809463501
Epoch 2680, val loss: 1.9214951992034912
Epoch 2690, training loss: 62.09599304199219 = 0.0074325138702988625 + 10.0 * 6.208856105804443
Epoch 2690, val loss: 1.9242347478866577
Epoch 2700, training loss: 62.07981872558594 = 0.007363053038716316 + 10.0 * 6.207245826721191
Epoch 2700, val loss: 1.9266269207000732
Epoch 2710, training loss: 62.0760383605957 = 0.007297149859368801 + 10.0 * 6.206873893737793
Epoch 2710, val loss: 1.9293828010559082
Epoch 2720, training loss: 62.12208938598633 = 0.007235717494040728 + 10.0 * 6.211485385894775
Epoch 2720, val loss: 1.9319883584976196
Epoch 2730, training loss: 62.09397506713867 = 0.007162454538047314 + 10.0 * 6.208681106567383
Epoch 2730, val loss: 1.934021234512329
Epoch 2740, training loss: 62.075538635253906 = 0.0070934290997684 + 10.0 * 6.206844806671143
Epoch 2740, val loss: 1.936686635017395
Epoch 2750, training loss: 62.08512496948242 = 0.0070314365439116955 + 10.0 * 6.2078094482421875
Epoch 2750, val loss: 1.9393810033798218
Epoch 2760, training loss: 62.09189224243164 = 0.006968307308852673 + 10.0 * 6.208492279052734
Epoch 2760, val loss: 1.9418221712112427
Epoch 2770, training loss: 62.08700180053711 = 0.006907986011356115 + 10.0 * 6.208009243011475
Epoch 2770, val loss: 1.9442920684814453
Epoch 2780, training loss: 62.06821060180664 = 0.006844294257462025 + 10.0 * 6.206136703491211
Epoch 2780, val loss: 1.9465808868408203
Epoch 2790, training loss: 62.06666946411133 = 0.006786019075661898 + 10.0 * 6.20598840713501
Epoch 2790, val loss: 1.949213981628418
Epoch 2800, training loss: 62.08354187011719 = 0.0067280917428433895 + 10.0 * 6.207681179046631
Epoch 2800, val loss: 1.9514293670654297
Epoch 2810, training loss: 62.09331512451172 = 0.006669512949883938 + 10.0 * 6.208664894104004
Epoch 2810, val loss: 1.953644871711731
Epoch 2820, training loss: 62.08913803100586 = 0.0066094533540308475 + 10.0 * 6.208252906799316
Epoch 2820, val loss: 1.9561575651168823
Epoch 2830, training loss: 62.071861267089844 = 0.0065521495416760445 + 10.0 * 6.206530570983887
Epoch 2830, val loss: 1.9584847688674927
Epoch 2840, training loss: 62.06313705444336 = 0.006498583126813173 + 10.0 * 6.205663681030273
Epoch 2840, val loss: 1.9609359502792358
Epoch 2850, training loss: 62.08375549316406 = 0.006445207633078098 + 10.0 * 6.207730770111084
Epoch 2850, val loss: 1.9631799459457397
Epoch 2860, training loss: 62.06796646118164 = 0.006391851231455803 + 10.0 * 6.206157207489014
Epoch 2860, val loss: 1.965358018875122
Epoch 2870, training loss: 62.07337188720703 = 0.006338721141219139 + 10.0 * 6.206703186035156
Epoch 2870, val loss: 1.9674004316329956
Epoch 2880, training loss: 62.092124938964844 = 0.006286978721618652 + 10.0 * 6.208583831787109
Epoch 2880, val loss: 1.9699914455413818
Epoch 2890, training loss: 62.05121994018555 = 0.00623298529535532 + 10.0 * 6.204498767852783
Epoch 2890, val loss: 1.971831202507019
Epoch 2900, training loss: 62.05019760131836 = 0.0061829243786633015 + 10.0 * 6.20440149307251
Epoch 2900, val loss: 1.974191427230835
Epoch 2910, training loss: 62.076717376708984 = 0.006135615985840559 + 10.0 * 6.207058429718018
Epoch 2910, val loss: 1.9762797355651855
Epoch 2920, training loss: 62.066619873046875 = 0.006086975336074829 + 10.0 * 6.206053256988525
Epoch 2920, val loss: 1.978318452835083
Epoch 2930, training loss: 62.061580657958984 = 0.006037264131009579 + 10.0 * 6.205554008483887
Epoch 2930, val loss: 1.9805352687835693
Epoch 2940, training loss: 62.087955474853516 = 0.005991065409034491 + 10.0 * 6.20819616317749
Epoch 2940, val loss: 1.9826743602752686
Epoch 2950, training loss: 62.04545211791992 = 0.00594062777236104 + 10.0 * 6.203951358795166
Epoch 2950, val loss: 1.98466956615448
Epoch 2960, training loss: 62.044586181640625 = 0.0058944690972566605 + 10.0 * 6.203869342803955
Epoch 2960, val loss: 1.9868218898773193
Epoch 2970, training loss: 62.06931686401367 = 0.005850812420248985 + 10.0 * 6.20634651184082
Epoch 2970, val loss: 1.9891269207000732
Epoch 2980, training loss: 62.0643310546875 = 0.005806209985166788 + 10.0 * 6.205852508544922
Epoch 2980, val loss: 1.991007685661316
Epoch 2990, training loss: 62.05575942993164 = 0.005761537700891495 + 10.0 * 6.204999923706055
Epoch 2990, val loss: 1.9929249286651611
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8418555614127571
The final CL Acc:0.75185, 0.04157, The final GNN Acc:0.84063, 0.00108
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11658])
remove edge: torch.Size([2, 9598])
updated graph: torch.Size([2, 10700])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.88968658447266 = 1.9215561151504517 + 10.0 * 8.596813201904297
Epoch 0, val loss: 1.9241749048233032
Epoch 10, training loss: 87.87354278564453 = 1.9132393598556519 + 10.0 * 8.596030235290527
Epoch 10, val loss: 1.9158004522323608
Epoch 20, training loss: 87.80755615234375 = 1.9024958610534668 + 10.0 * 8.590505599975586
Epoch 20, val loss: 1.904473900794983
Epoch 30, training loss: 87.43746185302734 = 1.8880791664123535 + 10.0 * 8.554938316345215
Epoch 30, val loss: 1.8889814615249634
Epoch 40, training loss: 85.25991821289062 = 1.871625304222107 + 10.0 * 8.338829040527344
Epoch 40, val loss: 1.8719263076782227
Epoch 50, training loss: 79.12293243408203 = 1.8529441356658936 + 10.0 * 7.726999282836914
Epoch 50, val loss: 1.8533326387405396
Epoch 60, training loss: 74.55533599853516 = 1.8407106399536133 + 10.0 * 7.271462440490723
Epoch 60, val loss: 1.842595100402832
Epoch 70, training loss: 72.15601348876953 = 1.8326770067214966 + 10.0 * 7.0323333740234375
Epoch 70, val loss: 1.8349509239196777
Epoch 80, training loss: 70.93512725830078 = 1.8249175548553467 + 10.0 * 6.911020755767822
Epoch 80, val loss: 1.8269264698028564
Epoch 90, training loss: 69.91856384277344 = 1.8162720203399658 + 10.0 * 6.810229301452637
Epoch 90, val loss: 1.8183503150939941
Epoch 100, training loss: 69.23106384277344 = 1.8083144426345825 + 10.0 * 6.742275238037109
Epoch 100, val loss: 1.810714840888977
Epoch 110, training loss: 68.76022338867188 = 1.8003904819488525 + 10.0 * 6.695982933044434
Epoch 110, val loss: 1.8029768466949463
Epoch 120, training loss: 68.37225341796875 = 1.792189359664917 + 10.0 * 6.658006191253662
Epoch 120, val loss: 1.7950619459152222
Epoch 130, training loss: 68.02167510986328 = 1.7841404676437378 + 10.0 * 6.623753547668457
Epoch 130, val loss: 1.7873117923736572
Epoch 140, training loss: 67.70196533203125 = 1.775922179222107 + 10.0 * 6.59260368347168
Epoch 140, val loss: 1.779659390449524
Epoch 150, training loss: 67.4277114868164 = 1.7669448852539062 + 10.0 * 6.566076278686523
Epoch 150, val loss: 1.7716327905654907
Epoch 160, training loss: 67.21669006347656 = 1.7571722269058228 + 10.0 * 6.545951843261719
Epoch 160, val loss: 1.7629958391189575
Epoch 170, training loss: 66.99671173095703 = 1.7462992668151855 + 10.0 * 6.525041580200195
Epoch 170, val loss: 1.7535587549209595
Epoch 180, training loss: 66.813720703125 = 1.7344112396240234 + 10.0 * 6.507930755615234
Epoch 180, val loss: 1.7432430982589722
Epoch 190, training loss: 66.65101623535156 = 1.7212516069412231 + 10.0 * 6.492976188659668
Epoch 190, val loss: 1.7319964170455933
Epoch 200, training loss: 66.5438232421875 = 1.7067813873291016 + 10.0 * 6.483704090118408
Epoch 200, val loss: 1.7196006774902344
Epoch 210, training loss: 66.38944244384766 = 1.6908668279647827 + 10.0 * 6.469857692718506
Epoch 210, val loss: 1.7060173749923706
Epoch 220, training loss: 66.26386260986328 = 1.6734870672225952 + 10.0 * 6.459037780761719
Epoch 220, val loss: 1.6913800239562988
Epoch 230, training loss: 66.14998626708984 = 1.6545251607894897 + 10.0 * 6.449545860290527
Epoch 230, val loss: 1.6755247116088867
Epoch 240, training loss: 66.06837463378906 = 1.6339001655578613 + 10.0 * 6.443447113037109
Epoch 240, val loss: 1.6583647727966309
Epoch 250, training loss: 65.94214630126953 = 1.6117346286773682 + 10.0 * 6.433041095733643
Epoch 250, val loss: 1.639998197555542
Epoch 260, training loss: 65.84113311767578 = 1.5879877805709839 + 10.0 * 6.425314426422119
Epoch 260, val loss: 1.620522141456604
Epoch 270, training loss: 65.76256561279297 = 1.5626994371414185 + 10.0 * 6.419986724853516
Epoch 270, val loss: 1.599940538406372
Epoch 280, training loss: 65.66267395019531 = 1.5360599756240845 + 10.0 * 6.412661552429199
Epoch 280, val loss: 1.5784428119659424
Epoch 290, training loss: 65.5708236694336 = 1.5082401037216187 + 10.0 * 6.406258583068848
Epoch 290, val loss: 1.5561599731445312
Epoch 300, training loss: 65.48645782470703 = 1.4793732166290283 + 10.0 * 6.4007086753845215
Epoch 300, val loss: 1.533295750617981
Epoch 310, training loss: 65.40563201904297 = 1.4494551420211792 + 10.0 * 6.395617961883545
Epoch 310, val loss: 1.50991952419281
Epoch 320, training loss: 65.3254623413086 = 1.4189614057540894 + 10.0 * 6.390650272369385
Epoch 320, val loss: 1.4862805604934692
Epoch 330, training loss: 65.25244903564453 = 1.3879636526107788 + 10.0 * 6.386448860168457
Epoch 330, val loss: 1.4626156091690063
Epoch 340, training loss: 65.16709899902344 = 1.3566182851791382 + 10.0 * 6.381048202514648
Epoch 340, val loss: 1.4390383958816528
Epoch 350, training loss: 65.09119415283203 = 1.3251277208328247 + 10.0 * 6.3766069412231445
Epoch 350, val loss: 1.4156534671783447
Epoch 360, training loss: 65.04019165039062 = 1.2936493158340454 + 10.0 * 6.374654293060303
Epoch 360, val loss: 1.3925694227218628
Epoch 370, training loss: 64.98978424072266 = 1.2622102499008179 + 10.0 * 6.372757434844971
Epoch 370, val loss: 1.3698569536209106
Epoch 380, training loss: 64.8900375366211 = 1.2311906814575195 + 10.0 * 6.365884780883789
Epoch 380, val loss: 1.3476855754852295
Epoch 390, training loss: 64.81739044189453 = 1.200561285018921 + 10.0 * 6.361682891845703
Epoch 390, val loss: 1.3261630535125732
Epoch 400, training loss: 64.74830627441406 = 1.1704435348510742 + 10.0 * 6.357786178588867
Epoch 400, val loss: 1.3053878545761108
Epoch 410, training loss: 64.7000503540039 = 1.1408036947250366 + 10.0 * 6.355924606323242
Epoch 410, val loss: 1.2852833271026611
Epoch 420, training loss: 64.67758178710938 = 1.1115859746932983 + 10.0 * 6.3565993309021
Epoch 420, val loss: 1.2656426429748535
Epoch 430, training loss: 64.58551788330078 = 1.0831255912780762 + 10.0 * 6.3502397537231445
Epoch 430, val loss: 1.246943712234497
Epoch 440, training loss: 64.52727508544922 = 1.0554734468460083 + 10.0 * 6.347179889678955
Epoch 440, val loss: 1.2291908264160156
Epoch 450, training loss: 64.46981811523438 = 1.0284881591796875 + 10.0 * 6.344133377075195
Epoch 450, val loss: 1.212212324142456
Epoch 460, training loss: 64.41275787353516 = 1.0022764205932617 + 10.0 * 6.341048240661621
Epoch 460, val loss: 1.1960532665252686
Epoch 470, training loss: 64.3783950805664 = 0.9768924117088318 + 10.0 * 6.340150356292725
Epoch 470, val loss: 1.1806690692901611
Epoch 480, training loss: 64.3154525756836 = 0.952179491519928 + 10.0 * 6.336327075958252
Epoch 480, val loss: 1.1661947965621948
Epoch 490, training loss: 64.26202392578125 = 0.9283133745193481 + 10.0 * 6.333371162414551
Epoch 490, val loss: 1.1524314880371094
Epoch 500, training loss: 64.27391815185547 = 0.9051108956336975 + 10.0 * 6.336880683898926
Epoch 500, val loss: 1.139623761177063
Epoch 510, training loss: 64.1769790649414 = 0.8828521966934204 + 10.0 * 6.329412460327148
Epoch 510, val loss: 1.1274136304855347
Epoch 520, training loss: 64.126220703125 = 0.8612260818481445 + 10.0 * 6.326498985290527
Epoch 520, val loss: 1.1161316633224487
Epoch 530, training loss: 64.1081771850586 = 0.8402575850486755 + 10.0 * 6.326792240142822
Epoch 530, val loss: 1.1055799722671509
Epoch 540, training loss: 64.04988861083984 = 0.819991946220398 + 10.0 * 6.322989463806152
Epoch 540, val loss: 1.095751404762268
Epoch 550, training loss: 64.00904083251953 = 0.8004108667373657 + 10.0 * 6.320863246917725
Epoch 550, val loss: 1.0866390466690063
Epoch 560, training loss: 64.00169372558594 = 0.7813582420349121 + 10.0 * 6.322033882141113
Epoch 560, val loss: 1.0780678987503052
Epoch 570, training loss: 63.93634033203125 = 0.7627862095832825 + 10.0 * 6.317355155944824
Epoch 570, val loss: 1.0701767206192017
Epoch 580, training loss: 63.89606475830078 = 0.7449150681495667 + 10.0 * 6.315114974975586
Epoch 580, val loss: 1.0628836154937744
Epoch 590, training loss: 63.85849380493164 = 0.7274801731109619 + 10.0 * 6.313101291656494
Epoch 590, val loss: 1.0562165975570679
Epoch 600, training loss: 63.90448760986328 = 0.7105888724327087 + 10.0 * 6.319389820098877
Epoch 600, val loss: 1.0501515865325928
Epoch 610, training loss: 63.80338668823242 = 0.6938797831535339 + 10.0 * 6.310950756072998
Epoch 610, val loss: 1.0442733764648438
Epoch 620, training loss: 63.76696014404297 = 0.677780270576477 + 10.0 * 6.308917999267578
Epoch 620, val loss: 1.039074420928955
Epoch 630, training loss: 63.730960845947266 = 0.6621179580688477 + 10.0 * 6.306884288787842
Epoch 630, val loss: 1.0344114303588867
Epoch 640, training loss: 63.70155715942383 = 0.6468448042869568 + 10.0 * 6.305471420288086
Epoch 640, val loss: 1.0301713943481445
Epoch 650, training loss: 63.72013473510742 = 0.6318753957748413 + 10.0 * 6.308825969696045
Epoch 650, val loss: 1.026183009147644
Epoch 660, training loss: 63.66963195800781 = 0.6171456575393677 + 10.0 * 6.305248737335205
Epoch 660, val loss: 1.0225913524627686
Epoch 670, training loss: 63.62108612060547 = 0.6029182076454163 + 10.0 * 6.301816940307617
Epoch 670, val loss: 1.0195099115371704
Epoch 680, training loss: 63.58720397949219 = 0.5890498757362366 + 10.0 * 6.2998151779174805
Epoch 680, val loss: 1.016802430152893
Epoch 690, training loss: 63.58671188354492 = 0.5754956603050232 + 10.0 * 6.301121711730957
Epoch 690, val loss: 1.0143978595733643
Epoch 700, training loss: 63.58245086669922 = 0.5621781349182129 + 10.0 * 6.302027225494385
Epoch 700, val loss: 1.0121406316757202
Epoch 710, training loss: 63.51082229614258 = 0.5490840077400208 + 10.0 * 6.296174049377441
Epoch 710, val loss: 1.0099477767944336
Epoch 720, training loss: 63.4870491027832 = 0.536434531211853 + 10.0 * 6.2950615882873535
Epoch 720, val loss: 1.0082781314849854
Epoch 730, training loss: 63.46074295043945 = 0.524058997631073 + 10.0 * 6.293668270111084
Epoch 730, val loss: 1.006874918937683
Epoch 740, training loss: 63.45818328857422 = 0.5119587182998657 + 10.0 * 6.294622421264648
Epoch 740, val loss: 1.0057796239852905
Epoch 750, training loss: 63.422786712646484 = 0.5000059008598328 + 10.0 * 6.292277812957764
Epoch 750, val loss: 1.0045394897460938
Epoch 760, training loss: 63.40022277832031 = 0.48834797739982605 + 10.0 * 6.291187763214111
Epoch 760, val loss: 1.0038038492202759
Epoch 770, training loss: 63.374755859375 = 0.4769251048564911 + 10.0 * 6.289783000946045
Epoch 770, val loss: 1.0031225681304932
Epoch 780, training loss: 63.39805603027344 = 0.46574217081069946 + 10.0 * 6.293231010437012
Epoch 780, val loss: 1.002700924873352
Epoch 790, training loss: 63.341793060302734 = 0.45480653643608093 + 10.0 * 6.288698673248291
Epoch 790, val loss: 1.00242280960083
Epoch 800, training loss: 63.306217193603516 = 0.4440670609474182 + 10.0 * 6.286214828491211
Epoch 800, val loss: 1.0023865699768066
Epoch 810, training loss: 63.28110885620117 = 0.4335942268371582 + 10.0 * 6.2847514152526855
Epoch 810, val loss: 1.0025027990341187
Epoch 820, training loss: 63.270042419433594 = 0.4233219027519226 + 10.0 * 6.284672260284424
Epoch 820, val loss: 1.002794623374939
Epoch 830, training loss: 63.27912139892578 = 0.4131923317909241 + 10.0 * 6.286592960357666
Epoch 830, val loss: 1.003123164176941
Epoch 840, training loss: 63.256874084472656 = 0.40313056111335754 + 10.0 * 6.285374641418457
Epoch 840, val loss: 1.0033866167068481
Epoch 850, training loss: 63.215675354003906 = 0.3934061825275421 + 10.0 * 6.282227039337158
Epoch 850, val loss: 1.0040382146835327
Epoch 860, training loss: 63.184356689453125 = 0.38385680317878723 + 10.0 * 6.280049800872803
Epoch 860, val loss: 1.004776120185852
Epoch 870, training loss: 63.18193054199219 = 0.3744603991508484 + 10.0 * 6.280746936798096
Epoch 870, val loss: 1.0055816173553467
Epoch 880, training loss: 63.17171859741211 = 0.36512330174446106 + 10.0 * 6.2806596755981445
Epoch 880, val loss: 1.0064903497695923
Epoch 890, training loss: 63.16171646118164 = 0.3559378385543823 + 10.0 * 6.280577659606934
Epoch 890, val loss: 1.0073679685592651
Epoch 900, training loss: 63.115684509277344 = 0.34690624475479126 + 10.0 * 6.2768778800964355
Epoch 900, val loss: 1.0086544752120972
Epoch 910, training loss: 63.1076545715332 = 0.33804014325141907 + 10.0 * 6.276961326599121
Epoch 910, val loss: 1.010061264038086
Epoch 920, training loss: 63.13310623168945 = 0.32929882407188416 + 10.0 * 6.280380725860596
Epoch 920, val loss: 1.011552095413208
Epoch 930, training loss: 63.11659622192383 = 0.32053059339523315 + 10.0 * 6.279606819152832
Epoch 930, val loss: 1.012515902519226
Epoch 940, training loss: 63.06007385253906 = 0.3118937611579895 + 10.0 * 6.274817943572998
Epoch 940, val loss: 1.014190912246704
Epoch 950, training loss: 63.032081604003906 = 0.30343303084373474 + 10.0 * 6.272864818572998
Epoch 950, val loss: 1.015945315361023
Epoch 960, training loss: 63.021080017089844 = 0.29506051540374756 + 10.0 * 6.272602081298828
Epoch 960, val loss: 1.0177634954452515
Epoch 970, training loss: 63.05523681640625 = 0.2867409586906433 + 10.0 * 6.276849746704102
Epoch 970, val loss: 1.0194963216781616
Epoch 980, training loss: 63.00408172607422 = 0.278523713350296 + 10.0 * 6.272555828094482
Epoch 980, val loss: 1.0215003490447998
Epoch 990, training loss: 62.98095703125 = 0.2704164683818817 + 10.0 * 6.271054267883301
Epoch 990, val loss: 1.0236984491348267
Epoch 1000, training loss: 62.963077545166016 = 0.26242873072624207 + 10.0 * 6.270064830780029
Epoch 1000, val loss: 1.025979995727539
Epoch 1010, training loss: 62.96918869018555 = 0.254611998796463 + 10.0 * 6.271457672119141
Epoch 1010, val loss: 1.0284652709960938
Epoch 1020, training loss: 62.96177291870117 = 0.2469097375869751 + 10.0 * 6.271486282348633
Epoch 1020, val loss: 1.0309923887252808
Epoch 1030, training loss: 62.91867446899414 = 0.2392750382423401 + 10.0 * 6.267939567565918
Epoch 1030, val loss: 1.0335272550582886
Epoch 1040, training loss: 62.900882720947266 = 0.2318829745054245 + 10.0 * 6.266900062561035
Epoch 1040, val loss: 1.0362578630447388
Epoch 1050, training loss: 62.959110260009766 = 0.22463448345661163 + 10.0 * 6.273447513580322
Epoch 1050, val loss: 1.0391939878463745
Epoch 1060, training loss: 62.909549713134766 = 0.21757304668426514 + 10.0 * 6.269197940826416
Epoch 1060, val loss: 1.0421963930130005
Epoch 1070, training loss: 62.851741790771484 = 0.21064393222332 + 10.0 * 6.2641096115112305
Epoch 1070, val loss: 1.045322060585022
Epoch 1080, training loss: 62.84355545043945 = 0.2039937525987625 + 10.0 * 6.263956069946289
Epoch 1080, val loss: 1.0485819578170776
Epoch 1090, training loss: 62.82627487182617 = 0.19757460057735443 + 10.0 * 6.262869834899902
Epoch 1090, val loss: 1.0521583557128906
Epoch 1100, training loss: 62.87446975708008 = 0.19134113192558289 + 10.0 * 6.268312931060791
Epoch 1100, val loss: 1.0557537078857422
Epoch 1110, training loss: 62.86416244506836 = 0.18518052995204926 + 10.0 * 6.267898082733154
Epoch 1110, val loss: 1.0594500303268433
Epoch 1120, training loss: 62.787662506103516 = 0.17928990721702576 + 10.0 * 6.260837078094482
Epoch 1120, val loss: 1.0632493495941162
Epoch 1130, training loss: 62.783416748046875 = 0.17364384233951569 + 10.0 * 6.260977268218994
Epoch 1130, val loss: 1.0672205686569214
Epoch 1140, training loss: 62.771324157714844 = 0.16819040477275848 + 10.0 * 6.260313510894775
Epoch 1140, val loss: 1.0713328123092651
Epoch 1150, training loss: 62.823341369628906 = 0.1628972887992859 + 10.0 * 6.2660441398620605
Epoch 1150, val loss: 1.0753411054611206
Epoch 1160, training loss: 62.74390411376953 = 0.15776164829730988 + 10.0 * 6.258614540100098
Epoch 1160, val loss: 1.0796340703964233
Epoch 1170, training loss: 62.76032638549805 = 0.15286175906658173 + 10.0 * 6.260746479034424
Epoch 1170, val loss: 1.0841375589370728
Epoch 1180, training loss: 62.74238967895508 = 0.148109570145607 + 10.0 * 6.259428024291992
Epoch 1180, val loss: 1.0884252786636353
Epoch 1190, training loss: 62.72367858886719 = 0.14354975521564484 + 10.0 * 6.258012771606445
Epoch 1190, val loss: 1.0930074453353882
Epoch 1200, training loss: 62.72322082519531 = 0.13918212056159973 + 10.0 * 6.258403778076172
Epoch 1200, val loss: 1.0976898670196533
Epoch 1210, training loss: 62.718597412109375 = 0.13493843376636505 + 10.0 * 6.258366107940674
Epoch 1210, val loss: 1.102231740951538
Epoch 1220, training loss: 62.701332092285156 = 0.13083089888095856 + 10.0 * 6.257050037384033
Epoch 1220, val loss: 1.1066604852676392
Epoch 1230, training loss: 62.72306442260742 = 0.12689457833766937 + 10.0 * 6.259616851806641
Epoch 1230, val loss: 1.1113715171813965
Epoch 1240, training loss: 62.6928825378418 = 0.12307998538017273 + 10.0 * 6.256979942321777
Epoch 1240, val loss: 1.1161222457885742
Epoch 1250, training loss: 62.668357849121094 = 0.1194157600402832 + 10.0 * 6.254894256591797
Epoch 1250, val loss: 1.1211321353912354
Epoch 1260, training loss: 62.65336227416992 = 0.11589806526899338 + 10.0 * 6.253746509552002
Epoch 1260, val loss: 1.1258933544158936
Epoch 1270, training loss: 62.712825775146484 = 0.11249080300331116 + 10.0 * 6.26003360748291
Epoch 1270, val loss: 1.1307384967803955
Epoch 1280, training loss: 62.66282272338867 = 0.10920798778533936 + 10.0 * 6.255361557006836
Epoch 1280, val loss: 1.1360251903533936
Epoch 1290, training loss: 62.637237548828125 = 0.10602428764104843 + 10.0 * 6.253121376037598
Epoch 1290, val loss: 1.1409178972244263
Epoch 1300, training loss: 62.64790725708008 = 0.10298895090818405 + 10.0 * 6.254491806030273
Epoch 1300, val loss: 1.1460391283035278
Epoch 1310, training loss: 62.61079025268555 = 0.10004773736000061 + 10.0 * 6.251074314117432
Epoch 1310, val loss: 1.151334285736084
Epoch 1320, training loss: 62.610782623291016 = 0.09722626209259033 + 10.0 * 6.2513556480407715
Epoch 1320, val loss: 1.1565858125686646
Epoch 1330, training loss: 62.60840606689453 = 0.09449983388185501 + 10.0 * 6.25139045715332
Epoch 1330, val loss: 1.1618343591690063
Epoch 1340, training loss: 62.62123107910156 = 0.09185625612735748 + 10.0 * 6.252937316894531
Epoch 1340, val loss: 1.167027235031128
Epoch 1350, training loss: 62.594139099121094 = 0.0893113911151886 + 10.0 * 6.25048303604126
Epoch 1350, val loss: 1.1723681688308716
Epoch 1360, training loss: 62.5775032043457 = 0.08684110641479492 + 10.0 * 6.249066352844238
Epoch 1360, val loss: 1.1775320768356323
Epoch 1370, training loss: 62.58606719970703 = 0.08448419719934464 + 10.0 * 6.250158309936523
Epoch 1370, val loss: 1.1829125881195068
Epoch 1380, training loss: 62.57307052612305 = 0.08218508958816528 + 10.0 * 6.249088764190674
Epoch 1380, val loss: 1.1878384351730347
Epoch 1390, training loss: 62.55236053466797 = 0.07997286319732666 + 10.0 * 6.247239112854004
Epoch 1390, val loss: 1.1932317018508911
Epoch 1400, training loss: 62.55073165893555 = 0.0778488963842392 + 10.0 * 6.247288227081299
Epoch 1400, val loss: 1.1984705924987793
Epoch 1410, training loss: 62.59506607055664 = 0.07579862326383591 + 10.0 * 6.251926898956299
Epoch 1410, val loss: 1.203560709953308
Epoch 1420, training loss: 62.551761627197266 = 0.0738171860575676 + 10.0 * 6.247794151306152
Epoch 1420, val loss: 1.2091370820999146
Epoch 1430, training loss: 62.52998352050781 = 0.0718892440199852 + 10.0 * 6.245809555053711
Epoch 1430, val loss: 1.214280605316162
Epoch 1440, training loss: 62.58340072631836 = 0.07005573064088821 + 10.0 * 6.251334190368652
Epoch 1440, val loss: 1.2194041013717651
Epoch 1450, training loss: 62.52752685546875 = 0.06824907660484314 + 10.0 * 6.245927810668945
Epoch 1450, val loss: 1.2251996994018555
Epoch 1460, training loss: 62.507469177246094 = 0.06652314960956573 + 10.0 * 6.244094371795654
Epoch 1460, val loss: 1.2303264141082764
Epoch 1470, training loss: 62.54719924926758 = 0.06486602127552032 + 10.0 * 6.248233318328857
Epoch 1470, val loss: 1.235853672027588
Epoch 1480, training loss: 62.52566146850586 = 0.06323419511318207 + 10.0 * 6.246243000030518
Epoch 1480, val loss: 1.2408558130264282
Epoch 1490, training loss: 62.515995025634766 = 0.06166382506489754 + 10.0 * 6.2454328536987305
Epoch 1490, val loss: 1.246048927307129
Epoch 1500, training loss: 62.50696563720703 = 0.06016560271382332 + 10.0 * 6.244679927825928
Epoch 1500, val loss: 1.2514195442199707
Epoch 1510, training loss: 62.47951889038086 = 0.05869510397315025 + 10.0 * 6.242082118988037
Epoch 1510, val loss: 1.2565217018127441
Epoch 1520, training loss: 62.47687911987305 = 0.05728376656770706 + 10.0 * 6.241959571838379
Epoch 1520, val loss: 1.2617566585540771
Epoch 1530, training loss: 62.46501541137695 = 0.05592632666230202 + 10.0 * 6.240908622741699
Epoch 1530, val loss: 1.2670490741729736
Epoch 1540, training loss: 62.54328155517578 = 0.05461232364177704 + 10.0 * 6.248867034912109
Epoch 1540, val loss: 1.2718932628631592
Epoch 1550, training loss: 62.48352813720703 = 0.053318385034799576 + 10.0 * 6.243021011352539
Epoch 1550, val loss: 1.2771943807601929
Epoch 1560, training loss: 62.46355438232422 = 0.05206403508782387 + 10.0 * 6.241148948669434
Epoch 1560, val loss: 1.2822794914245605
Epoch 1570, training loss: 62.47771072387695 = 0.05087703838944435 + 10.0 * 6.242683410644531
Epoch 1570, val loss: 1.2874305248260498
Epoch 1580, training loss: 62.45285415649414 = 0.04970499128103256 + 10.0 * 6.240314960479736
Epoch 1580, val loss: 1.2925262451171875
Epoch 1590, training loss: 62.44638442993164 = 0.048573557287454605 + 10.0 * 6.239781379699707
Epoch 1590, val loss: 1.2975084781646729
Epoch 1600, training loss: 62.45439910888672 = 0.04747966304421425 + 10.0 * 6.240691661834717
Epoch 1600, val loss: 1.302666425704956
Epoch 1610, training loss: 62.43023681640625 = 0.04641763120889664 + 10.0 * 6.238381862640381
Epoch 1610, val loss: 1.3075511455535889
Epoch 1620, training loss: 62.423057556152344 = 0.04539269581437111 + 10.0 * 6.237766742706299
Epoch 1620, val loss: 1.3124885559082031
Epoch 1630, training loss: 62.51642608642578 = 0.04441125690937042 + 10.0 * 6.247201442718506
Epoch 1630, val loss: 1.3175443410873413
Epoch 1640, training loss: 62.424766540527344 = 0.043409381061792374 + 10.0 * 6.23813533782959
Epoch 1640, val loss: 1.3221368789672852
Epoch 1650, training loss: 62.40095520019531 = 0.04247434064745903 + 10.0 * 6.235848426818848
Epoch 1650, val loss: 1.3270373344421387
Epoch 1660, training loss: 62.429962158203125 = 0.04157450050115585 + 10.0 * 6.2388386726379395
Epoch 1660, val loss: 1.3318045139312744
Epoch 1670, training loss: 62.402645111083984 = 0.04067915678024292 + 10.0 * 6.236196517944336
Epoch 1670, val loss: 1.3365585803985596
Epoch 1680, training loss: 62.38703155517578 = 0.039818815886974335 + 10.0 * 6.2347211837768555
Epoch 1680, val loss: 1.3413649797439575
Epoch 1690, training loss: 62.382179260253906 = 0.03899334371089935 + 10.0 * 6.234318733215332
Epoch 1690, val loss: 1.3462246656417847
Epoch 1700, training loss: 62.38742446899414 = 0.0381978414952755 + 10.0 * 6.234922885894775
Epoch 1700, val loss: 1.3507981300354004
Epoch 1710, training loss: 62.42820739746094 = 0.03741799294948578 + 10.0 * 6.239078998565674
Epoch 1710, val loss: 1.355397343635559
Epoch 1720, training loss: 62.40977478027344 = 0.03664523735642433 + 10.0 * 6.237313270568848
Epoch 1720, val loss: 1.3601219654083252
Epoch 1730, training loss: 62.39004135131836 = 0.03590164706110954 + 10.0 * 6.235414028167725
Epoch 1730, val loss: 1.3644843101501465
Epoch 1740, training loss: 62.4129524230957 = 0.035184718668460846 + 10.0 * 6.237776756286621
Epoch 1740, val loss: 1.369030237197876
Epoch 1750, training loss: 62.37065505981445 = 0.03447475656867027 + 10.0 * 6.233618259429932
Epoch 1750, val loss: 1.373630404472351
Epoch 1760, training loss: 62.36138916015625 = 0.03379315137863159 + 10.0 * 6.232759475708008
Epoch 1760, val loss: 1.378105878829956
Epoch 1770, training loss: 62.352046966552734 = 0.033137500286102295 + 10.0 * 6.231890678405762
Epoch 1770, val loss: 1.382578730583191
Epoch 1780, training loss: 62.35926818847656 = 0.032502640038728714 + 10.0 * 6.2326765060424805
Epoch 1780, val loss: 1.3869339227676392
Epoch 1790, training loss: 62.39210891723633 = 0.031874656677246094 + 10.0 * 6.236023426055908
Epoch 1790, val loss: 1.3912632465362549
Epoch 1800, training loss: 62.3805046081543 = 0.03126215562224388 + 10.0 * 6.23492431640625
Epoch 1800, val loss: 1.3956562280654907
Epoch 1810, training loss: 62.33605194091797 = 0.030657051131129265 + 10.0 * 6.230539321899414
Epoch 1810, val loss: 1.3998390436172485
Epoch 1820, training loss: 62.34471130371094 = 0.030085720121860504 + 10.0 * 6.231462478637695
Epoch 1820, val loss: 1.4041624069213867
Epoch 1830, training loss: 62.367366790771484 = 0.02953304909169674 + 10.0 * 6.23378324508667
Epoch 1830, val loss: 1.4085530042648315
Epoch 1840, training loss: 62.33507537841797 = 0.028976257890462875 + 10.0 * 6.230609893798828
Epoch 1840, val loss: 1.4125425815582275
Epoch 1850, training loss: 62.33901596069336 = 0.02844490297138691 + 10.0 * 6.231057167053223
Epoch 1850, val loss: 1.4167147874832153
Epoch 1860, training loss: 62.383094787597656 = 0.027931205928325653 + 10.0 * 6.235516548156738
Epoch 1860, val loss: 1.4208256006240845
Epoch 1870, training loss: 62.34086227416992 = 0.027416281402111053 + 10.0 * 6.231344699859619
Epoch 1870, val loss: 1.4246931076049805
Epoch 1880, training loss: 62.33345031738281 = 0.026929818093776703 + 10.0 * 6.230652332305908
Epoch 1880, val loss: 1.429063320159912
Epoch 1890, training loss: 62.341365814208984 = 0.026450660079717636 + 10.0 * 6.231491565704346
Epoch 1890, val loss: 1.4328874349594116
Epoch 1900, training loss: 62.318214416503906 = 0.025979747995734215 + 10.0 * 6.229223728179932
Epoch 1900, val loss: 1.436921238899231
Epoch 1910, training loss: 62.310447692871094 = 0.02552691288292408 + 10.0 * 6.22849178314209
Epoch 1910, val loss: 1.4408327341079712
Epoch 1920, training loss: 62.32249069213867 = 0.025090187788009644 + 10.0 * 6.229740142822266
Epoch 1920, val loss: 1.444821834564209
Epoch 1930, training loss: 62.30590057373047 = 0.024655720219016075 + 10.0 * 6.228124618530273
Epoch 1930, val loss: 1.448740005493164
Epoch 1940, training loss: 62.34418869018555 = 0.024234693497419357 + 10.0 * 6.231995582580566
Epoch 1940, val loss: 1.4526292085647583
Epoch 1950, training loss: 62.31473922729492 = 0.023814545944333076 + 10.0 * 6.229092597961426
Epoch 1950, val loss: 1.4560765027999878
Epoch 1960, training loss: 62.29013442993164 = 0.023407423868775368 + 10.0 * 6.226672649383545
Epoch 1960, val loss: 1.4598219394683838
Epoch 1970, training loss: 62.30039978027344 = 0.02302304096519947 + 10.0 * 6.227737903594971
Epoch 1970, val loss: 1.4636913537979126
Epoch 1980, training loss: 62.334842681884766 = 0.022641967982053757 + 10.0 * 6.231220245361328
Epoch 1980, val loss: 1.4672034978866577
Epoch 1990, training loss: 62.29731750488281 = 0.022261302918195724 + 10.0 * 6.227505683898926
Epoch 1990, val loss: 1.4711523056030273
Epoch 2000, training loss: 62.28525161743164 = 0.02190205082297325 + 10.0 * 6.226335048675537
Epoch 2000, val loss: 1.4748674631118774
Epoch 2010, training loss: 62.32550048828125 = 0.02155238762497902 + 10.0 * 6.2303948402404785
Epoch 2010, val loss: 1.4785090684890747
Epoch 2020, training loss: 62.27217483520508 = 0.021195564419031143 + 10.0 * 6.225098133087158
Epoch 2020, val loss: 1.4817256927490234
Epoch 2030, training loss: 62.28011703491211 = 0.020856400951743126 + 10.0 * 6.225926399230957
Epoch 2030, val loss: 1.4854005575180054
Epoch 2040, training loss: 62.30062484741211 = 0.020527919754385948 + 10.0 * 6.2280097007751465
Epoch 2040, val loss: 1.4888298511505127
Epoch 2050, training loss: 62.26556396484375 = 0.020199013873934746 + 10.0 * 6.224536418914795
Epoch 2050, val loss: 1.492440104484558
Epoch 2060, training loss: 62.31909942626953 = 0.019883623346686363 + 10.0 * 6.229921817779541
Epoch 2060, val loss: 1.495545506477356
Epoch 2070, training loss: 62.272396087646484 = 0.019570983946323395 + 10.0 * 6.225282669067383
Epoch 2070, val loss: 1.4992148876190186
Epoch 2080, training loss: 62.2673225402832 = 0.019266609102487564 + 10.0 * 6.2248053550720215
Epoch 2080, val loss: 1.5024211406707764
Epoch 2090, training loss: 62.25358963012695 = 0.018973199650645256 + 10.0 * 6.223461627960205
Epoch 2090, val loss: 1.5059363842010498
Epoch 2100, training loss: 62.247493743896484 = 0.018688712269067764 + 10.0 * 6.2228803634643555
Epoch 2100, val loss: 1.5093064308166504
Epoch 2110, training loss: 62.27775192260742 = 0.018411733210086823 + 10.0 * 6.225934028625488
Epoch 2110, val loss: 1.5125263929367065
Epoch 2120, training loss: 62.24644470214844 = 0.018131041899323463 + 10.0 * 6.2228312492370605
Epoch 2120, val loss: 1.5158767700195312
Epoch 2130, training loss: 62.2897834777832 = 0.01786346733570099 + 10.0 * 6.227191925048828
Epoch 2130, val loss: 1.5190932750701904
Epoch 2140, training loss: 62.25621795654297 = 0.017595898360013962 + 10.0 * 6.223862171173096
Epoch 2140, val loss: 1.5221940279006958
Epoch 2150, training loss: 62.25072479248047 = 0.01733335852622986 + 10.0 * 6.223339080810547
Epoch 2150, val loss: 1.5252702236175537
Epoch 2160, training loss: 62.242210388183594 = 0.017082497477531433 + 10.0 * 6.222512722015381
Epoch 2160, val loss: 1.5284491777420044
Epoch 2170, training loss: 62.241065979003906 = 0.01683858409523964 + 10.0 * 6.2224225997924805
Epoch 2170, val loss: 1.5315632820129395
Epoch 2180, training loss: 62.22459030151367 = 0.016597850248217583 + 10.0 * 6.220799446105957
Epoch 2180, val loss: 1.5347880125045776
Epoch 2190, training loss: 62.244380950927734 = 0.01636650040745735 + 10.0 * 6.222801685333252
Epoch 2190, val loss: 1.5377682447433472
Epoch 2200, training loss: 62.25783920288086 = 0.01613270491361618 + 10.0 * 6.224170684814453
Epoch 2200, val loss: 1.54055655002594
Epoch 2210, training loss: 62.23172378540039 = 0.01589740440249443 + 10.0 * 6.221582889556885
Epoch 2210, val loss: 1.5435456037521362
Epoch 2220, training loss: 62.21931457519531 = 0.01567636802792549 + 10.0 * 6.220364093780518
Epoch 2220, val loss: 1.5467209815979004
Epoch 2230, training loss: 62.237064361572266 = 0.015464805997908115 + 10.0 * 6.2221598625183105
Epoch 2230, val loss: 1.549522876739502
Epoch 2240, training loss: 62.23276901245117 = 0.01525141391903162 + 10.0 * 6.221751689910889
Epoch 2240, val loss: 1.5525072813034058
Epoch 2250, training loss: 62.23303985595703 = 0.015043907798826694 + 10.0 * 6.221799373626709
Epoch 2250, val loss: 1.555440068244934
Epoch 2260, training loss: 62.20694351196289 = 0.014842167496681213 + 10.0 * 6.219210147857666
Epoch 2260, val loss: 1.5585323572158813
Epoch 2270, training loss: 62.24083709716797 = 0.014650841243565083 + 10.0 * 6.222618579864502
Epoch 2270, val loss: 1.5614649057388306
Epoch 2280, training loss: 62.24144744873047 = 0.014447801746428013 + 10.0 * 6.222700119018555
Epoch 2280, val loss: 1.5639667510986328
Epoch 2290, training loss: 62.209800720214844 = 0.014251240529119968 + 10.0 * 6.219554901123047
Epoch 2290, val loss: 1.566633939743042
Epoch 2300, training loss: 62.1981315612793 = 0.014065532945096493 + 10.0 * 6.218406677246094
Epoch 2300, val loss: 1.5697301626205444
Epoch 2310, training loss: 62.202545166015625 = 0.01388559490442276 + 10.0 * 6.218865871429443
Epoch 2310, val loss: 1.5723809003829956
Epoch 2320, training loss: 62.286251068115234 = 0.013712599873542786 + 10.0 * 6.2272539138793945
Epoch 2320, val loss: 1.5749611854553223
Epoch 2330, training loss: 62.225101470947266 = 0.013525315560400486 + 10.0 * 6.221157550811768
Epoch 2330, val loss: 1.5777273178100586
Epoch 2340, training loss: 62.19830322265625 = 0.013352313078939915 + 10.0 * 6.2184953689575195
Epoch 2340, val loss: 1.5805387496948242
Epoch 2350, training loss: 62.200870513916016 = 0.013185105286538601 + 10.0 * 6.21876859664917
Epoch 2350, val loss: 1.5832788944244385
Epoch 2360, training loss: 62.24607467651367 = 0.013023113831877708 + 10.0 * 6.2233052253723145
Epoch 2360, val loss: 1.5858149528503418
Epoch 2370, training loss: 62.20344924926758 = 0.01285561453551054 + 10.0 * 6.219059467315674
Epoch 2370, val loss: 1.5884953737258911
Epoch 2380, training loss: 62.192264556884766 = 0.012698316015303135 + 10.0 * 6.21795654296875
Epoch 2380, val loss: 1.591304898262024
Epoch 2390, training loss: 62.246707916259766 = 0.012544813565909863 + 10.0 * 6.223416328430176
Epoch 2390, val loss: 1.5935724973678589
Epoch 2400, training loss: 62.200843811035156 = 0.012386849150061607 + 10.0 * 6.218845844268799
Epoch 2400, val loss: 1.596147060394287
Epoch 2410, training loss: 62.17954635620117 = 0.012234805151820183 + 10.0 * 6.216731071472168
Epoch 2410, val loss: 1.5987591743469238
Epoch 2420, training loss: 62.178646087646484 = 0.01209118403494358 + 10.0 * 6.216655254364014
Epoch 2420, val loss: 1.6014056205749512
Epoch 2430, training loss: 62.207672119140625 = 0.011950057931244373 + 10.0 * 6.219572067260742
Epoch 2430, val loss: 1.60397469997406
Epoch 2440, training loss: 62.18949890136719 = 0.011804701760411263 + 10.0 * 6.217769145965576
Epoch 2440, val loss: 1.606318712234497
Epoch 2450, training loss: 62.191402435302734 = 0.011664975434541702 + 10.0 * 6.217973709106445
Epoch 2450, val loss: 1.6087099313735962
Epoch 2460, training loss: 62.19669723510742 = 0.011527481488883495 + 10.0 * 6.218516826629639
Epoch 2460, val loss: 1.6112257242202759
Epoch 2470, training loss: 62.178462982177734 = 0.01139168068766594 + 10.0 * 6.216707229614258
Epoch 2470, val loss: 1.6134880781173706
Epoch 2480, training loss: 62.165809631347656 = 0.011260892264544964 + 10.0 * 6.215455055236816
Epoch 2480, val loss: 1.6159168481826782
Epoch 2490, training loss: 62.158241271972656 = 0.011134760454297066 + 10.0 * 6.214710712432861
Epoch 2490, val loss: 1.6183383464813232
Epoch 2500, training loss: 62.19961166381836 = 0.011012127622961998 + 10.0 * 6.218859672546387
Epoch 2500, val loss: 1.620418906211853
Epoch 2510, training loss: 62.17018508911133 = 0.01088232547044754 + 10.0 * 6.215929985046387
Epoch 2510, val loss: 1.6227846145629883
Epoch 2520, training loss: 62.16847229003906 = 0.010759425349533558 + 10.0 * 6.215771198272705
Epoch 2520, val loss: 1.6251258850097656
Epoch 2530, training loss: 62.17293930053711 = 0.010639782063663006 + 10.0 * 6.2162299156188965
Epoch 2530, val loss: 1.6275099515914917
Epoch 2540, training loss: 62.191200256347656 = 0.010522935539484024 + 10.0 * 6.218067646026611
Epoch 2540, val loss: 1.6295677423477173
Epoch 2550, training loss: 62.160614013671875 = 0.010403594933450222 + 10.0 * 6.215021133422852
Epoch 2550, val loss: 1.631925106048584
Epoch 2560, training loss: 62.145530700683594 = 0.010292607359588146 + 10.0 * 6.213523864746094
Epoch 2560, val loss: 1.6342873573303223
Epoch 2570, training loss: 62.15862274169922 = 0.010185678489506245 + 10.0 * 6.21484375
Epoch 2570, val loss: 1.6364809274673462
Epoch 2580, training loss: 62.19737243652344 = 0.01007741317152977 + 10.0 * 6.218729496002197
Epoch 2580, val loss: 1.6385645866394043
Epoch 2590, training loss: 62.182430267333984 = 0.009961788542568684 + 10.0 * 6.217247009277344
Epoch 2590, val loss: 1.6404938697814941
Epoch 2600, training loss: 62.1522216796875 = 0.009855381213128567 + 10.0 * 6.214236259460449
Epoch 2600, val loss: 1.64266836643219
Epoch 2610, training loss: 62.142189025878906 = 0.00975185539573431 + 10.0 * 6.2132439613342285
Epoch 2610, val loss: 1.6447830200195312
Epoch 2620, training loss: 62.16716766357422 = 0.00965256430208683 + 10.0 * 6.215751647949219
Epoch 2620, val loss: 1.6468499898910522
Epoch 2630, training loss: 62.15959167480469 = 0.009550390765070915 + 10.0 * 6.215003967285156
Epoch 2630, val loss: 1.6487598419189453
Epoch 2640, training loss: 62.14767074584961 = 0.009449910372495651 + 10.0 * 6.213822364807129
Epoch 2640, val loss: 1.650903582572937
Epoch 2650, training loss: 62.15751647949219 = 0.009354211390018463 + 10.0 * 6.214816093444824
Epoch 2650, val loss: 1.6529171466827393
Epoch 2660, training loss: 62.15130615234375 = 0.009258045814931393 + 10.0 * 6.214204788208008
Epoch 2660, val loss: 1.6549538373947144
Epoch 2670, training loss: 62.138526916503906 = 0.009165053255856037 + 10.0 * 6.212935924530029
Epoch 2670, val loss: 1.6571769714355469
Epoch 2680, training loss: 62.14320373535156 = 0.009074251167476177 + 10.0 * 6.213412761688232
Epoch 2680, val loss: 1.6591423749923706
Epoch 2690, training loss: 62.14387512207031 = 0.008983777835965157 + 10.0 * 6.213489055633545
Epoch 2690, val loss: 1.6609923839569092
Epoch 2700, training loss: 62.21062469482422 = 0.008894484490156174 + 10.0 * 6.220172882080078
Epoch 2700, val loss: 1.662992238998413
Epoch 2710, training loss: 62.16194152832031 = 0.00880312267690897 + 10.0 * 6.215313911437988
Epoch 2710, val loss: 1.6646456718444824
Epoch 2720, training loss: 62.13988494873047 = 0.00871290173381567 + 10.0 * 6.2131171226501465
Epoch 2720, val loss: 1.6667920351028442
Epoch 2730, training loss: 62.134674072265625 = 0.00863157119601965 + 10.0 * 6.21260404586792
Epoch 2730, val loss: 1.6687676906585693
Epoch 2740, training loss: 62.15580368041992 = 0.008550368249416351 + 10.0 * 6.214725494384766
Epoch 2740, val loss: 1.6708240509033203
Epoch 2750, training loss: 62.130374908447266 = 0.00846567191183567 + 10.0 * 6.212191104888916
Epoch 2750, val loss: 1.67242431640625
Epoch 2760, training loss: 62.13556671142578 = 0.008384502492845058 + 10.0 * 6.2127180099487305
Epoch 2760, val loss: 1.6742422580718994
Epoch 2770, training loss: 62.1320915222168 = 0.00830572284758091 + 10.0 * 6.21237850189209
Epoch 2770, val loss: 1.676236867904663
Epoch 2780, training loss: 62.1303596496582 = 0.008227718062698841 + 10.0 * 6.212213039398193
Epoch 2780, val loss: 1.6780647039413452
Epoch 2790, training loss: 62.128936767578125 = 0.008150875568389893 + 10.0 * 6.21207857131958
Epoch 2790, val loss: 1.679949402809143
Epoch 2800, training loss: 62.1292839050293 = 0.008076231926679611 + 10.0 * 6.21212100982666
Epoch 2800, val loss: 1.6818397045135498
Epoch 2810, training loss: 62.11397933959961 = 0.00800136849284172 + 10.0 * 6.210597515106201
Epoch 2810, val loss: 1.6835384368896484
Epoch 2820, training loss: 62.18395233154297 = 0.007930652238428593 + 10.0 * 6.217602252960205
Epoch 2820, val loss: 1.6854056119918823
Epoch 2830, training loss: 62.126869201660156 = 0.007851896807551384 + 10.0 * 6.211901664733887
Epoch 2830, val loss: 1.6866307258605957
Epoch 2840, training loss: 62.11079788208008 = 0.007779465056955814 + 10.0 * 6.210301876068115
Epoch 2840, val loss: 1.6885687112808228
Epoch 2850, training loss: 62.10366439819336 = 0.007711650338023901 + 10.0 * 6.209595203399658
Epoch 2850, val loss: 1.690381646156311
Epoch 2860, training loss: 62.126338958740234 = 0.007646604906767607 + 10.0 * 6.211869239807129
Epoch 2860, val loss: 1.6922144889831543
Epoch 2870, training loss: 62.113155364990234 = 0.007576134987175465 + 10.0 * 6.21055793762207
Epoch 2870, val loss: 1.6935285329818726
Epoch 2880, training loss: 62.113014221191406 = 0.007507642265409231 + 10.0 * 6.210550785064697
Epoch 2880, val loss: 1.6952452659606934
Epoch 2890, training loss: 62.129207611083984 = 0.007444096263498068 + 10.0 * 6.212176322937012
Epoch 2890, val loss: 1.6970832347869873
Epoch 2900, training loss: 62.136314392089844 = 0.0073784878477454185 + 10.0 * 6.212893486022949
Epoch 2900, val loss: 1.6986336708068848
Epoch 2910, training loss: 62.11683654785156 = 0.007312120869755745 + 10.0 * 6.210952281951904
Epoch 2910, val loss: 1.7002205848693848
Epoch 2920, training loss: 62.10659408569336 = 0.007248601410537958 + 10.0 * 6.209934711456299
Epoch 2920, val loss: 1.7017593383789062
Epoch 2930, training loss: 62.104732513427734 = 0.007189064286649227 + 10.0 * 6.20975399017334
Epoch 2930, val loss: 1.7035318613052368
Epoch 2940, training loss: 62.12683868408203 = 0.007128966972231865 + 10.0 * 6.211970806121826
Epoch 2940, val loss: 1.7049617767333984
Epoch 2950, training loss: 62.09614562988281 = 0.007067451719194651 + 10.0 * 6.208907604217529
Epoch 2950, val loss: 1.7066271305084229
Epoch 2960, training loss: 62.13970184326172 = 0.007011426147073507 + 10.0 * 6.213269233703613
Epoch 2960, val loss: 1.7082788944244385
Epoch 2970, training loss: 62.086483001708984 = 0.006948007270693779 + 10.0 * 6.207953453063965
Epoch 2970, val loss: 1.7095718383789062
Epoch 2980, training loss: 62.08372116088867 = 0.006891078781336546 + 10.0 * 6.207683086395264
Epoch 2980, val loss: 1.7110553979873657
Epoch 2990, training loss: 62.08679962158203 = 0.006837703287601471 + 10.0 * 6.207996368408203
Epoch 2990, val loss: 1.7127360105514526
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8239325250395362
=== training gcn model ===
Epoch 0, training loss: 87.90178680419922 = 1.9332083463668823 + 10.0 * 8.596858024597168
Epoch 0, val loss: 1.9368919134140015
Epoch 10, training loss: 87.88723754882812 = 1.9232999086380005 + 10.0 * 8.596393585205078
Epoch 10, val loss: 1.9262200593948364
Epoch 20, training loss: 87.83952331542969 = 1.9113006591796875 + 10.0 * 8.592822074890137
Epoch 20, val loss: 1.9130949974060059
Epoch 30, training loss: 87.546142578125 = 1.8962535858154297 + 10.0 * 8.56498908996582
Epoch 30, val loss: 1.896632194519043
Epoch 40, training loss: 85.42292785644531 = 1.8803848028182983 + 10.0 * 8.354253768920898
Epoch 40, val loss: 1.880865454673767
Epoch 50, training loss: 79.30670928955078 = 1.8666307926177979 + 10.0 * 7.744007587432861
Epoch 50, val loss: 1.867837905883789
Epoch 60, training loss: 74.29142761230469 = 1.856614589691162 + 10.0 * 7.243481159210205
Epoch 60, val loss: 1.8580514192581177
Epoch 70, training loss: 71.75569152832031 = 1.84781014919281 + 10.0 * 6.990787982940674
Epoch 70, val loss: 1.8492980003356934
Epoch 80, training loss: 70.30599975585938 = 1.838756799697876 + 10.0 * 6.846724033355713
Epoch 80, val loss: 1.8403968811035156
Epoch 90, training loss: 69.42160034179688 = 1.8288971185684204 + 10.0 * 6.759270668029785
Epoch 90, val loss: 1.830698013305664
Epoch 100, training loss: 68.71832275390625 = 1.8185280561447144 + 10.0 * 6.689979553222656
Epoch 100, val loss: 1.8206321001052856
Epoch 110, training loss: 68.24423217773438 = 1.808316946029663 + 10.0 * 6.643591403961182
Epoch 110, val loss: 1.810667872428894
Epoch 120, training loss: 67.87361145019531 = 1.798230767250061 + 10.0 * 6.607537746429443
Epoch 120, val loss: 1.8006298542022705
Epoch 130, training loss: 67.59098052978516 = 1.7881388664245605 + 10.0 * 6.580284118652344
Epoch 130, val loss: 1.7905030250549316
Epoch 140, training loss: 67.34461212158203 = 1.777776837348938 + 10.0 * 6.556683540344238
Epoch 140, val loss: 1.7801439762115479
Epoch 150, training loss: 67.12954711914062 = 1.766839623451233 + 10.0 * 6.536270618438721
Epoch 150, val loss: 1.7693955898284912
Epoch 160, training loss: 66.9842529296875 = 1.7551860809326172 + 10.0 * 6.522906303405762
Epoch 160, val loss: 1.75809907913208
Epoch 170, training loss: 66.7745590209961 = 1.742611289024353 + 10.0 * 6.503194332122803
Epoch 170, val loss: 1.746109962463379
Epoch 180, training loss: 66.61866760253906 = 1.7289767265319824 + 10.0 * 6.488969326019287
Epoch 180, val loss: 1.733269214630127
Epoch 190, training loss: 66.52840423583984 = 1.7141495943069458 + 10.0 * 6.481425762176514
Epoch 190, val loss: 1.7194708585739136
Epoch 200, training loss: 66.37521362304688 = 1.6979396343231201 + 10.0 * 6.4677276611328125
Epoch 200, val loss: 1.704514503479004
Epoch 210, training loss: 66.23037719726562 = 1.6803746223449707 + 10.0 * 6.454999923706055
Epoch 210, val loss: 1.6884732246398926
Epoch 220, training loss: 66.11133575439453 = 1.6613638401031494 + 10.0 * 6.444997310638428
Epoch 220, val loss: 1.6713097095489502
Epoch 230, training loss: 66.0311279296875 = 1.6408405303955078 + 10.0 * 6.439028739929199
Epoch 230, val loss: 1.6528929471969604
Epoch 240, training loss: 65.90226745605469 = 1.6188147068023682 + 10.0 * 6.428345680236816
Epoch 240, val loss: 1.6332547664642334
Epoch 250, training loss: 65.84127044677734 = 1.5953419208526611 + 10.0 * 6.4245924949646
Epoch 250, val loss: 1.612519383430481
Epoch 260, training loss: 65.71461486816406 = 1.5704132318496704 + 10.0 * 6.414420127868652
Epoch 260, val loss: 1.59074068069458
Epoch 270, training loss: 65.61759185791016 = 1.544260025024414 + 10.0 * 6.4073333740234375
Epoch 270, val loss: 1.5681225061416626
Epoch 280, training loss: 65.58132934570312 = 1.5169659852981567 + 10.0 * 6.406435966491699
Epoch 280, val loss: 1.5446689128875732
Epoch 290, training loss: 65.4554672241211 = 1.488776683807373 + 10.0 * 6.396669387817383
Epoch 290, val loss: 1.5209509134292603
Epoch 300, training loss: 65.36569213867188 = 1.4599740505218506 + 10.0 * 6.3905720710754395
Epoch 300, val loss: 1.4970409870147705
Epoch 310, training loss: 65.28500366210938 = 1.4307680130004883 + 10.0 * 6.385423183441162
Epoch 310, val loss: 1.4731025695800781
Epoch 320, training loss: 65.25534057617188 = 1.4012792110443115 + 10.0 * 6.385406017303467
Epoch 320, val loss: 1.4493001699447632
Epoch 330, training loss: 65.14596557617188 = 1.371701955795288 + 10.0 * 6.3774261474609375
Epoch 330, val loss: 1.4258525371551514
Epoch 340, training loss: 65.07150268554688 = 1.3419488668441772 + 10.0 * 6.372955322265625
Epoch 340, val loss: 1.4025753736495972
Epoch 350, training loss: 65.00418853759766 = 1.31228768825531 + 10.0 * 6.369189739227295
Epoch 350, val loss: 1.3795658349990845
Epoch 360, training loss: 64.96967315673828 = 1.2825567722320557 + 10.0 * 6.368711471557617
Epoch 360, val loss: 1.3568484783172607
Epoch 370, training loss: 64.86763763427734 = 1.2528762817382812 + 10.0 * 6.361475944519043
Epoch 370, val loss: 1.3343342542648315
Epoch 380, training loss: 64.79743194580078 = 1.2233308553695679 + 10.0 * 6.357410430908203
Epoch 380, val loss: 1.3119980096817017
Epoch 390, training loss: 64.76518249511719 = 1.1938915252685547 + 10.0 * 6.357129096984863
Epoch 390, val loss: 1.2900341749191284
Epoch 400, training loss: 64.72566223144531 = 1.1643807888031006 + 10.0 * 6.356127738952637
Epoch 400, val loss: 1.268036961555481
Epoch 410, training loss: 64.61554718017578 = 1.1352890729904175 + 10.0 * 6.348025321960449
Epoch 410, val loss: 1.2465345859527588
Epoch 420, training loss: 64.5572280883789 = 1.1064859628677368 + 10.0 * 6.34507417678833
Epoch 420, val loss: 1.2253559827804565
Epoch 430, training loss: 64.53028106689453 = 1.0779508352279663 + 10.0 * 6.345232963562012
Epoch 430, val loss: 1.2044957876205444
Epoch 440, training loss: 64.48137664794922 = 1.0497037172317505 + 10.0 * 6.343167304992676
Epoch 440, val loss: 1.1838449239730835
Epoch 450, training loss: 64.4010238647461 = 1.0220484733581543 + 10.0 * 6.337897777557373
Epoch 450, val loss: 1.1641191244125366
Epoch 460, training loss: 64.34601593017578 = 0.9949506521224976 + 10.0 * 6.335106372833252
Epoch 460, val loss: 1.1448887586593628
Epoch 470, training loss: 64.30690002441406 = 0.9685068726539612 + 10.0 * 6.333839416503906
Epoch 470, val loss: 1.1264327764511108
Epoch 480, training loss: 64.24108123779297 = 0.9428179264068604 + 10.0 * 6.329826354980469
Epoch 480, val loss: 1.1086153984069824
Epoch 490, training loss: 64.22299194335938 = 0.917942464351654 + 10.0 * 6.330504894256592
Epoch 490, val loss: 1.0920002460479736
Epoch 500, training loss: 64.18065643310547 = 0.8937763571739197 + 10.0 * 6.328688144683838
Epoch 500, val loss: 1.075816035270691
Epoch 510, training loss: 64.11280059814453 = 0.8705300092697144 + 10.0 * 6.324227333068848
Epoch 510, val loss: 1.0608210563659668
Epoch 520, training loss: 64.06787109375 = 0.8481130599975586 + 10.0 * 6.3219757080078125
Epoch 520, val loss: 1.0469003915786743
Epoch 530, training loss: 64.08541107177734 = 0.8264334797859192 + 10.0 * 6.325897693634033
Epoch 530, val loss: 1.0336334705352783
Epoch 540, training loss: 64.00364685058594 = 0.8056533932685852 + 10.0 * 6.319798946380615
Epoch 540, val loss: 1.0214911699295044
Epoch 550, training loss: 63.98151397705078 = 0.7854018807411194 + 10.0 * 6.319611549377441
Epoch 550, val loss: 1.0098474025726318
Epoch 560, training loss: 63.917640686035156 = 0.7659284472465515 + 10.0 * 6.315171241760254
Epoch 560, val loss: 0.9991117119789124
Epoch 570, training loss: 63.880794525146484 = 0.7470123171806335 + 10.0 * 6.31337833404541
Epoch 570, val loss: 0.9891160726547241
Epoch 580, training loss: 63.84258270263672 = 0.7287213802337646 + 10.0 * 6.3113861083984375
Epoch 580, val loss: 0.9798195362091064
Epoch 590, training loss: 63.88386917114258 = 0.7108796238899231 + 10.0 * 6.317298889160156
Epoch 590, val loss: 0.9710580110549927
Epoch 600, training loss: 63.80168533325195 = 0.6934618949890137 + 10.0 * 6.310822486877441
Epoch 600, val loss: 0.9628891348838806
Epoch 610, training loss: 63.747344970703125 = 0.6765865683555603 + 10.0 * 6.3070759773254395
Epoch 610, val loss: 0.9552757143974304
Epoch 620, training loss: 63.71084976196289 = 0.6601427793502808 + 10.0 * 6.305070877075195
Epoch 620, val loss: 0.9482566714286804
Epoch 630, training loss: 63.768035888671875 = 0.6441447138786316 + 10.0 * 6.312388896942139
Epoch 630, val loss: 0.9418724179267883
Epoch 640, training loss: 63.69567108154297 = 0.628272294998169 + 10.0 * 6.306739807128906
Epoch 640, val loss: 0.9354610443115234
Epoch 650, training loss: 63.638946533203125 = 0.612814724445343 + 10.0 * 6.302613258361816
Epoch 650, val loss: 0.9295545220375061
Epoch 660, training loss: 63.59408187866211 = 0.5977829098701477 + 10.0 * 6.299630165100098
Epoch 660, val loss: 0.9242371320724487
Epoch 670, training loss: 63.56161880493164 = 0.5830866694450378 + 10.0 * 6.297852993011475
Epoch 670, val loss: 0.9192640781402588
Epoch 680, training loss: 63.5600700378418 = 0.56866055727005 + 10.0 * 6.299140930175781
Epoch 680, val loss: 0.9146924614906311
Epoch 690, training loss: 63.586097717285156 = 0.5545171499252319 + 10.0 * 6.303158283233643
Epoch 690, val loss: 0.9097555875778198
Epoch 700, training loss: 63.49903869628906 = 0.540673553943634 + 10.0 * 6.295836448669434
Epoch 700, val loss: 0.9057430028915405
Epoch 710, training loss: 63.46269226074219 = 0.5272582769393921 + 10.0 * 6.293543338775635
Epoch 710, val loss: 0.9021228551864624
Epoch 720, training loss: 63.45044708251953 = 0.5142180919647217 + 10.0 * 6.293622970581055
Epoch 720, val loss: 0.8988116979598999
Epoch 730, training loss: 63.43278503417969 = 0.5014527440071106 + 10.0 * 6.29313325881958
Epoch 730, val loss: 0.8956897258758545
Epoch 740, training loss: 63.39704132080078 = 0.48893898725509644 + 10.0 * 6.290810585021973
Epoch 740, val loss: 0.892723023891449
Epoch 750, training loss: 63.391239166259766 = 0.47684404253959656 + 10.0 * 6.291439533233643
Epoch 750, val loss: 0.8900963664054871
Epoch 760, training loss: 63.38041687011719 = 0.46508172154426575 + 10.0 * 6.291533470153809
Epoch 760, val loss: 0.8877305388450623
Epoch 770, training loss: 63.3280143737793 = 0.45356643199920654 + 10.0 * 6.287444591522217
Epoch 770, val loss: 0.8855603933334351
Epoch 780, training loss: 63.30117416381836 = 0.44243383407592773 + 10.0 * 6.285874366760254
Epoch 780, val loss: 0.8835967183113098
Epoch 790, training loss: 63.287811279296875 = 0.43164047598838806 + 10.0 * 6.285616874694824
Epoch 790, val loss: 0.8820106983184814
Epoch 800, training loss: 63.30310821533203 = 0.4211069345474243 + 10.0 * 6.2881999015808105
Epoch 800, val loss: 0.880556046962738
Epoch 810, training loss: 63.244476318359375 = 0.41093310713768005 + 10.0 * 6.28335428237915
Epoch 810, val loss: 0.8794434070587158
Epoch 820, training loss: 63.2330436706543 = 0.4010551869869232 + 10.0 * 6.283198833465576
Epoch 820, val loss: 0.8785468339920044
Epoch 830, training loss: 63.2365608215332 = 0.39143022894859314 + 10.0 * 6.284512996673584
Epoch 830, val loss: 0.8777948617935181
Epoch 840, training loss: 63.19133758544922 = 0.3820461332798004 + 10.0 * 6.280929088592529
Epoch 840, val loss: 0.8771822452545166
Epoch 850, training loss: 63.18769836425781 = 0.3729170858860016 + 10.0 * 6.281477928161621
Epoch 850, val loss: 0.8767899870872498
Epoch 860, training loss: 63.17961120605469 = 0.36404016613960266 + 10.0 * 6.281557083129883
Epoch 860, val loss: 0.8765519857406616
Epoch 870, training loss: 63.158424377441406 = 0.3554200232028961 + 10.0 * 6.280300617218018
Epoch 870, val loss: 0.8766826391220093
Epoch 880, training loss: 63.12647247314453 = 0.34693601727485657 + 10.0 * 6.277953624725342
Epoch 880, val loss: 0.8768270611763
Epoch 890, training loss: 63.1242790222168 = 0.3386937975883484 + 10.0 * 6.278558254241943
Epoch 890, val loss: 0.87712562084198
Epoch 900, training loss: 63.11772918701172 = 0.3306145966053009 + 10.0 * 6.278711318969727
Epoch 900, val loss: 0.8776057958602905
Epoch 910, training loss: 63.097225189208984 = 0.3226282596588135 + 10.0 * 6.277459621429443
Epoch 910, val loss: 0.8780160546302795
Epoch 920, training loss: 63.06349182128906 = 0.31490206718444824 + 10.0 * 6.2748589515686035
Epoch 920, val loss: 0.8789108991622925
Epoch 930, training loss: 63.06147766113281 = 0.3073022961616516 + 10.0 * 6.275417804718018
Epoch 930, val loss: 0.8797120451927185
Epoch 940, training loss: 63.034122467041016 = 0.29984626173973083 + 10.0 * 6.273427486419678
Epoch 940, val loss: 0.8808814287185669
Epoch 950, training loss: 63.03023147583008 = 0.29255789518356323 + 10.0 * 6.273767471313477
Epoch 950, val loss: 0.8822252750396729
Epoch 960, training loss: 63.045108795166016 = 0.2853288948535919 + 10.0 * 6.275978088378906
Epoch 960, val loss: 0.8833192586898804
Epoch 970, training loss: 63.023414611816406 = 0.27832335233688354 + 10.0 * 6.274508953094482
Epoch 970, val loss: 0.8850711584091187
Epoch 980, training loss: 62.9797477722168 = 0.27139759063720703 + 10.0 * 6.270834922790527
Epoch 980, val loss: 0.8865909576416016
Epoch 990, training loss: 62.955711364746094 = 0.26469215750694275 + 10.0 * 6.269102096557617
Epoch 990, val loss: 0.8886443972587585
Epoch 1000, training loss: 62.97102737426758 = 0.2581053078174591 + 10.0 * 6.271292209625244
Epoch 1000, val loss: 0.8906981348991394
Epoch 1010, training loss: 62.93217468261719 = 0.25164666771888733 + 10.0 * 6.268052577972412
Epoch 1010, val loss: 0.8929471373558044
Epoch 1020, training loss: 62.95024108886719 = 0.24533845484256744 + 10.0 * 6.2704901695251465
Epoch 1020, val loss: 0.8952718377113342
Epoch 1030, training loss: 62.90879821777344 = 0.23912127315998077 + 10.0 * 6.2669677734375
Epoch 1030, val loss: 0.8977372646331787
Epoch 1040, training loss: 62.896484375 = 0.23308275640010834 + 10.0 * 6.266340255737305
Epoch 1040, val loss: 0.9003657698631287
Epoch 1050, training loss: 62.919124603271484 = 0.22718669474124908 + 10.0 * 6.269193649291992
Epoch 1050, val loss: 0.9031667709350586
Epoch 1060, training loss: 62.88867950439453 = 0.22145019471645355 + 10.0 * 6.266722679138184
Epoch 1060, val loss: 0.9060381650924683
Epoch 1070, training loss: 62.88228988647461 = 0.21582336723804474 + 10.0 * 6.266646385192871
Epoch 1070, val loss: 0.9090222120285034
Epoch 1080, training loss: 62.89387512207031 = 0.21039116382598877 + 10.0 * 6.268348217010498
Epoch 1080, val loss: 0.9124724864959717
Epoch 1090, training loss: 62.84080505371094 = 0.20501135289669037 + 10.0 * 6.263579368591309
Epoch 1090, val loss: 0.915756344795227
Epoch 1100, training loss: 62.81901550292969 = 0.1998271644115448 + 10.0 * 6.261919021606445
Epoch 1100, val loss: 0.9191635847091675
Epoch 1110, training loss: 62.80810546875 = 0.19480229914188385 + 10.0 * 6.261330604553223
Epoch 1110, val loss: 0.9228809475898743
Epoch 1120, training loss: 62.879722595214844 = 0.18989501893520355 + 10.0 * 6.268982887268066
Epoch 1120, val loss: 0.9264106750488281
Epoch 1130, training loss: 62.84510040283203 = 0.1851142942905426 + 10.0 * 6.265998363494873
Epoch 1130, val loss: 0.9305323958396912
Epoch 1140, training loss: 62.78529357910156 = 0.18042930960655212 + 10.0 * 6.260486602783203
Epoch 1140, val loss: 0.9344466328620911
Epoch 1150, training loss: 62.762603759765625 = 0.17592720687389374 + 10.0 * 6.258667945861816
Epoch 1150, val loss: 0.9385530352592468
Epoch 1160, training loss: 62.757530212402344 = 0.17155154049396515 + 10.0 * 6.2585978507995605
Epoch 1160, val loss: 0.9428285360336304
Epoch 1170, training loss: 62.86463165283203 = 0.1672886162996292 + 10.0 * 6.2697343826293945
Epoch 1170, val loss: 0.9469203352928162
Epoch 1180, training loss: 62.73983383178711 = 0.1631234586238861 + 10.0 * 6.257670879364014
Epoch 1180, val loss: 0.9514505863189697
Epoch 1190, training loss: 62.72573471069336 = 0.15907755494117737 + 10.0 * 6.2566657066345215
Epoch 1190, val loss: 0.955963671207428
Epoch 1200, training loss: 62.7213020324707 = 0.15516333281993866 + 10.0 * 6.256613731384277
Epoch 1200, val loss: 0.9605746865272522
Epoch 1210, training loss: 62.77012252807617 = 0.15137095749378204 + 10.0 * 6.261875152587891
Epoch 1210, val loss: 0.9653658270835876
Epoch 1220, training loss: 62.72216796875 = 0.14765281975269318 + 10.0 * 6.25745153427124
Epoch 1220, val loss: 0.97014319896698
Epoch 1230, training loss: 62.71088790893555 = 0.1440325230360031 + 10.0 * 6.256685733795166
Epoch 1230, val loss: 0.9749441742897034
Epoch 1240, training loss: 62.709022521972656 = 0.140515998005867 + 10.0 * 6.256850719451904
Epoch 1240, val loss: 0.9796893000602722
Epoch 1250, training loss: 62.67570877075195 = 0.13709762692451477 + 10.0 * 6.253861427307129
Epoch 1250, val loss: 0.9848713278770447
Epoch 1260, training loss: 62.66549301147461 = 0.1337885707616806 + 10.0 * 6.253170490264893
Epoch 1260, val loss: 0.9899461269378662
Epoch 1270, training loss: 62.699859619140625 = 0.13056959211826324 + 10.0 * 6.25692892074585
Epoch 1270, val loss: 0.995050847530365
Epoch 1280, training loss: 62.68208312988281 = 0.12740996479988098 + 10.0 * 6.255467414855957
Epoch 1280, val loss: 1.0003662109375
Epoch 1290, training loss: 62.67010498046875 = 0.12434221059083939 + 10.0 * 6.254576206207275
Epoch 1290, val loss: 1.0052742958068848
Epoch 1300, training loss: 62.65155792236328 = 0.1213681548833847 + 10.0 * 6.253018856048584
Epoch 1300, val loss: 1.0108518600463867
Epoch 1310, training loss: 62.630393981933594 = 0.11848293244838715 + 10.0 * 6.251191139221191
Epoch 1310, val loss: 1.0161349773406982
Epoch 1320, training loss: 62.65080642700195 = 0.11567895114421844 + 10.0 * 6.253512382507324
Epoch 1320, val loss: 1.021456241607666
Epoch 1330, training loss: 62.64792251586914 = 0.11295515298843384 + 10.0 * 6.2534966468811035
Epoch 1330, val loss: 1.0269432067871094
Epoch 1340, training loss: 62.62654113769531 = 0.11029939353466034 + 10.0 * 6.25162410736084
Epoch 1340, val loss: 1.0325219631195068
Epoch 1350, training loss: 62.62204360961914 = 0.10770066827535629 + 10.0 * 6.251434326171875
Epoch 1350, val loss: 1.0379010438919067
Epoch 1360, training loss: 62.5939826965332 = 0.10519778728485107 + 10.0 * 6.248878479003906
Epoch 1360, val loss: 1.043648362159729
Epoch 1370, training loss: 62.60346603393555 = 0.10275974124670029 + 10.0 * 6.250070571899414
Epoch 1370, val loss: 1.0492826700210571
Epoch 1380, training loss: 62.62329864501953 = 0.10038664937019348 + 10.0 * 6.252291202545166
Epoch 1380, val loss: 1.0549564361572266
Epoch 1390, training loss: 62.597557067871094 = 0.09805581718683243 + 10.0 * 6.249949932098389
Epoch 1390, val loss: 1.06055748462677
Epoch 1400, training loss: 62.63264083862305 = 0.09581135958433151 + 10.0 * 6.253683090209961
Epoch 1400, val loss: 1.0663865804672241
Epoch 1410, training loss: 62.567298889160156 = 0.09361240267753601 + 10.0 * 6.247368812561035
Epoch 1410, val loss: 1.072152853012085
Epoch 1420, training loss: 62.55329513549805 = 0.09147943556308746 + 10.0 * 6.246181488037109
Epoch 1420, val loss: 1.0780645608901978
Epoch 1430, training loss: 62.5633659362793 = 0.08942367881536484 + 10.0 * 6.24739408493042
Epoch 1430, val loss: 1.0839967727661133
Epoch 1440, training loss: 62.593109130859375 = 0.0874045118689537 + 10.0 * 6.250570297241211
Epoch 1440, val loss: 1.089603304862976
Epoch 1450, training loss: 62.559593200683594 = 0.08541489392518997 + 10.0 * 6.24741792678833
Epoch 1450, val loss: 1.0954341888427734
Epoch 1460, training loss: 62.546836853027344 = 0.08349308371543884 + 10.0 * 6.246334552764893
Epoch 1460, val loss: 1.1013356447219849
Epoch 1470, training loss: 62.535240173339844 = 0.08164525032043457 + 10.0 * 6.245359420776367
Epoch 1470, val loss: 1.1075767278671265
Epoch 1480, training loss: 62.520408630371094 = 0.07984089106321335 + 10.0 * 6.244056701660156
Epoch 1480, val loss: 1.1134711503982544
Epoch 1490, training loss: 62.52915573120117 = 0.07809875905513763 + 10.0 * 6.245105743408203
Epoch 1490, val loss: 1.11954927444458
Epoch 1500, training loss: 62.54349899291992 = 0.07638274878263474 + 10.0 * 6.246711730957031
Epoch 1500, val loss: 1.1253665685653687
Epoch 1510, training loss: 62.54226303100586 = 0.07469277083873749 + 10.0 * 6.2467570304870605
Epoch 1510, val loss: 1.1313283443450928
Epoch 1520, training loss: 62.51948165893555 = 0.07306333631277084 + 10.0 * 6.2446417808532715
Epoch 1520, val loss: 1.1372531652450562
Epoch 1530, training loss: 62.499210357666016 = 0.07147572189569473 + 10.0 * 6.242773532867432
Epoch 1530, val loss: 1.143195629119873
Epoch 1540, training loss: 62.48684310913086 = 0.0699353739619255 + 10.0 * 6.241690635681152
Epoch 1540, val loss: 1.149215817451477
Epoch 1550, training loss: 62.515586853027344 = 0.06845233589410782 + 10.0 * 6.24471378326416
Epoch 1550, val loss: 1.1551337242126465
Epoch 1560, training loss: 62.484771728515625 = 0.06699438393115997 + 10.0 * 6.2417778968811035
Epoch 1560, val loss: 1.1609816551208496
Epoch 1570, training loss: 62.499149322509766 = 0.06557770818471909 + 10.0 * 6.243357181549072
Epoch 1570, val loss: 1.166937232017517
Epoch 1580, training loss: 62.484954833984375 = 0.06418822705745697 + 10.0 * 6.242076396942139
Epoch 1580, val loss: 1.172832727432251
Epoch 1590, training loss: 62.468711853027344 = 0.06285266578197479 + 10.0 * 6.240586280822754
Epoch 1590, val loss: 1.1789894104003906
Epoch 1600, training loss: 62.57255935668945 = 0.0615546815097332 + 10.0 * 6.251100540161133
Epoch 1600, val loss: 1.1848642826080322
Epoch 1610, training loss: 62.48331069946289 = 0.06024998426437378 + 10.0 * 6.242306232452393
Epoch 1610, val loss: 1.1906554698944092
Epoch 1620, training loss: 62.453712463378906 = 0.05900271236896515 + 10.0 * 6.239470958709717
Epoch 1620, val loss: 1.196583867073059
Epoch 1630, training loss: 62.4429817199707 = 0.05779937654733658 + 10.0 * 6.238518238067627
Epoch 1630, val loss: 1.2026475667953491
Epoch 1640, training loss: 62.44084167480469 = 0.05663270130753517 + 10.0 * 6.2384209632873535
Epoch 1640, val loss: 1.2085479497909546
Epoch 1650, training loss: 62.54855728149414 = 0.05550684779882431 + 10.0 * 6.24930477142334
Epoch 1650, val loss: 1.214417576789856
Epoch 1660, training loss: 62.477474212646484 = 0.05435427650809288 + 10.0 * 6.242311954498291
Epoch 1660, val loss: 1.2199782133102417
Epoch 1670, training loss: 62.44533920288086 = 0.05326302722096443 + 10.0 * 6.2392072677612305
Epoch 1670, val loss: 1.2258400917053223
Epoch 1680, training loss: 62.42533493041992 = 0.05219768360257149 + 10.0 * 6.237313747406006
Epoch 1680, val loss: 1.2317670583724976
Epoch 1690, training loss: 62.447898864746094 = 0.05117833986878395 + 10.0 * 6.2396721839904785
Epoch 1690, val loss: 1.23760187625885
Epoch 1700, training loss: 62.46022033691406 = 0.050167858600616455 + 10.0 * 6.2410054206848145
Epoch 1700, val loss: 1.243315577507019
Epoch 1710, training loss: 62.41014862060547 = 0.04916100576519966 + 10.0 * 6.236098766326904
Epoch 1710, val loss: 1.2488338947296143
Epoch 1720, training loss: 62.405452728271484 = 0.048202481120824814 + 10.0 * 6.235724925994873
Epoch 1720, val loss: 1.2546567916870117
Epoch 1730, training loss: 62.399192810058594 = 0.047278713434934616 + 10.0 * 6.235191345214844
Epoch 1730, val loss: 1.2604838609695435
Epoch 1740, training loss: 62.43889236450195 = 0.04638185352087021 + 10.0 * 6.239251136779785
Epoch 1740, val loss: 1.2661024332046509
Epoch 1750, training loss: 62.4359245300293 = 0.04548163339495659 + 10.0 * 6.239044189453125
Epoch 1750, val loss: 1.2713078260421753
Epoch 1760, training loss: 62.39825439453125 = 0.04460720717906952 + 10.0 * 6.2353644371032715
Epoch 1760, val loss: 1.2771389484405518
Epoch 1770, training loss: 62.392398834228516 = 0.043760888278484344 + 10.0 * 6.234863758087158
Epoch 1770, val loss: 1.282673716545105
Epoch 1780, training loss: 62.48482894897461 = 0.04296032339334488 + 10.0 * 6.244186878204346
Epoch 1780, val loss: 1.2883377075195312
Epoch 1790, training loss: 62.396976470947266 = 0.04213317483663559 + 10.0 * 6.2354841232299805
Epoch 1790, val loss: 1.2934787273406982
Epoch 1800, training loss: 62.38011932373047 = 0.04134894162416458 + 10.0 * 6.233877182006836
Epoch 1800, val loss: 1.2990641593933105
Epoch 1810, training loss: 62.367549896240234 = 0.04058900475502014 + 10.0 * 6.232696056365967
Epoch 1810, val loss: 1.3046833276748657
Epoch 1820, training loss: 62.364288330078125 = 0.03985459730029106 + 10.0 * 6.232443332672119
Epoch 1820, val loss: 1.3101882934570312
Epoch 1830, training loss: 62.465152740478516 = 0.03913792967796326 + 10.0 * 6.24260139465332
Epoch 1830, val loss: 1.3154070377349854
Epoch 1840, training loss: 62.442989349365234 = 0.03842491656541824 + 10.0 * 6.240456581115723
Epoch 1840, val loss: 1.3207916021347046
Epoch 1850, training loss: 62.36567306518555 = 0.03771818056702614 + 10.0 * 6.232795238494873
Epoch 1850, val loss: 1.3261555433273315
Epoch 1860, training loss: 62.35554122924805 = 0.0370466448366642 + 10.0 * 6.231849193572998
Epoch 1860, val loss: 1.3316130638122559
Epoch 1870, training loss: 62.38475036621094 = 0.03640589118003845 + 10.0 * 6.23483419418335
Epoch 1870, val loss: 1.3370364904403687
Epoch 1880, training loss: 62.35451889038086 = 0.03575954586267471 + 10.0 * 6.231875896453857
Epoch 1880, val loss: 1.3421200513839722
Epoch 1890, training loss: 62.36035919189453 = 0.03513401746749878 + 10.0 * 6.232522487640381
Epoch 1890, val loss: 1.3472950458526611
Epoch 1900, training loss: 62.380001068115234 = 0.03453516215085983 + 10.0 * 6.234546661376953
Epoch 1900, val loss: 1.352652907371521
Epoch 1910, training loss: 62.342830657958984 = 0.033930305391550064 + 10.0 * 6.230889797210693
Epoch 1910, val loss: 1.3576059341430664
Epoch 1920, training loss: 62.39339828491211 = 0.03335529565811157 + 10.0 * 6.23600435256958
Epoch 1920, val loss: 1.3627463579177856
Epoch 1930, training loss: 62.34299850463867 = 0.032777369022369385 + 10.0 * 6.231022357940674
Epoch 1930, val loss: 1.367658257484436
Epoch 1940, training loss: 62.32429885864258 = 0.03221718594431877 + 10.0 * 6.229207992553711
Epoch 1940, val loss: 1.3727729320526123
Epoch 1950, training loss: 62.32444381713867 = 0.03168126940727234 + 10.0 * 6.229276180267334
Epoch 1950, val loss: 1.3778594732284546
Epoch 1960, training loss: 62.3537712097168 = 0.031156476587057114 + 10.0 * 6.232261657714844
Epoch 1960, val loss: 1.3827223777770996
Epoch 1970, training loss: 62.33811569213867 = 0.030641643330454826 + 10.0 * 6.230747222900391
Epoch 1970, val loss: 1.3876229524612427
Epoch 1980, training loss: 62.35413360595703 = 0.03013654239475727 + 10.0 * 6.232399940490723
Epoch 1980, val loss: 1.3924239873886108
Epoch 1990, training loss: 62.33085250854492 = 0.029639119282364845 + 10.0 * 6.23012113571167
Epoch 1990, val loss: 1.3973956108093262
Epoch 2000, training loss: 62.34406661987305 = 0.029164498671889305 + 10.0 * 6.231490135192871
Epoch 2000, val loss: 1.4023762941360474
Epoch 2010, training loss: 62.32288360595703 = 0.028686407953500748 + 10.0 * 6.229419708251953
Epoch 2010, val loss: 1.4070992469787598
Epoch 2020, training loss: 62.31206512451172 = 0.02822590060532093 + 10.0 * 6.228384017944336
Epoch 2020, val loss: 1.4118915796279907
Epoch 2030, training loss: 62.36287307739258 = 0.02777901105582714 + 10.0 * 6.233509540557861
Epoch 2030, val loss: 1.4165849685668945
Epoch 2040, training loss: 62.311607360839844 = 0.027336813509464264 + 10.0 * 6.228426933288574
Epoch 2040, val loss: 1.4213594198226929
Epoch 2050, training loss: 62.306251525878906 = 0.026909345760941505 + 10.0 * 6.22793436050415
Epoch 2050, val loss: 1.4262531995773315
Epoch 2060, training loss: 62.34452819824219 = 0.026492636650800705 + 10.0 * 6.2318034172058105
Epoch 2060, val loss: 1.4307689666748047
Epoch 2070, training loss: 62.301246643066406 = 0.02607729658484459 + 10.0 * 6.227517127990723
Epoch 2070, val loss: 1.4354504346847534
Epoch 2080, training loss: 62.310035705566406 = 0.025675645098090172 + 10.0 * 6.22843599319458
Epoch 2080, val loss: 1.4401061534881592
Epoch 2090, training loss: 62.3113899230957 = 0.025282086804509163 + 10.0 * 6.228610515594482
Epoch 2090, val loss: 1.4446985721588135
Epoch 2100, training loss: 62.29034423828125 = 0.02489626035094261 + 10.0 * 6.2265448570251465
Epoch 2100, val loss: 1.4491416215896606
Epoch 2110, training loss: 62.32621383666992 = 0.024523114785552025 + 10.0 * 6.23016881942749
Epoch 2110, val loss: 1.453582763671875
Epoch 2120, training loss: 62.283390045166016 = 0.024149740114808083 + 10.0 * 6.225924015045166
Epoch 2120, val loss: 1.4579309225082397
Epoch 2130, training loss: 62.278297424316406 = 0.023786967620253563 + 10.0 * 6.2254509925842285
Epoch 2130, val loss: 1.462336540222168
Epoch 2140, training loss: 62.28871536254883 = 0.02343626692891121 + 10.0 * 6.226527690887451
Epoch 2140, val loss: 1.4666938781738281
Epoch 2150, training loss: 62.293460845947266 = 0.023091541603207588 + 10.0 * 6.227036952972412
Epoch 2150, val loss: 1.4709664583206177
Epoch 2160, training loss: 62.32958221435547 = 0.02276141382753849 + 10.0 * 6.230681896209717
Epoch 2160, val loss: 1.4753271341323853
Epoch 2170, training loss: 62.308414459228516 = 0.022421015426516533 + 10.0 * 6.2285990715026855
Epoch 2170, val loss: 1.4794896841049194
Epoch 2180, training loss: 62.2691650390625 = 0.022100256755948067 + 10.0 * 6.224706649780273
Epoch 2180, val loss: 1.483764410018921
Epoch 2190, training loss: 62.26247787475586 = 0.021782590076327324 + 10.0 * 6.224069595336914
Epoch 2190, val loss: 1.4881069660186768
Epoch 2200, training loss: 62.26108932495117 = 0.02147703431546688 + 10.0 * 6.223961353302002
Epoch 2200, val loss: 1.4922901391983032
Epoch 2210, training loss: 62.33378219604492 = 0.021181141957640648 + 10.0 * 6.231259822845459
Epoch 2210, val loss: 1.4962618350982666
Epoch 2220, training loss: 62.28757095336914 = 0.02087913081049919 + 10.0 * 6.2266693115234375
Epoch 2220, val loss: 1.5004364252090454
Epoch 2230, training loss: 62.261627197265625 = 0.020583124831318855 + 10.0 * 6.224104404449463
Epoch 2230, val loss: 1.5045276880264282
Epoch 2240, training loss: 62.274845123291016 = 0.02030152454972267 + 10.0 * 6.225454330444336
Epoch 2240, val loss: 1.5087833404541016
Epoch 2250, training loss: 62.282257080078125 = 0.020024022087454796 + 10.0 * 6.226223468780518
Epoch 2250, val loss: 1.5127367973327637
Epoch 2260, training loss: 62.27690124511719 = 0.019745541736483574 + 10.0 * 6.225715637207031
Epoch 2260, val loss: 1.516530156135559
Epoch 2270, training loss: 62.250274658203125 = 0.019476762041449547 + 10.0 * 6.223079681396484
Epoch 2270, val loss: 1.520633339881897
Epoch 2280, training loss: 62.24391174316406 = 0.019214550033211708 + 10.0 * 6.222469806671143
Epoch 2280, val loss: 1.5246145725250244
Epoch 2290, training loss: 62.28693771362305 = 0.018963290378451347 + 10.0 * 6.226797580718994
Epoch 2290, val loss: 1.528471827507019
Epoch 2300, training loss: 62.260807037353516 = 0.01870960369706154 + 10.0 * 6.224209785461426
Epoch 2300, val loss: 1.5323607921600342
Epoch 2310, training loss: 62.26520538330078 = 0.018458915874361992 + 10.0 * 6.224674701690674
Epoch 2310, val loss: 1.5360724925994873
Epoch 2320, training loss: 62.23619079589844 = 0.018209200352430344 + 10.0 * 6.221798419952393
Epoch 2320, val loss: 1.5398995876312256
Epoch 2330, training loss: 62.233280181884766 = 0.017972780391573906 + 10.0 * 6.221530914306641
Epoch 2330, val loss: 1.5437607765197754
Epoch 2340, training loss: 62.26227951049805 = 0.017745517194271088 + 10.0 * 6.224453449249268
Epoch 2340, val loss: 1.547493815422058
Epoch 2350, training loss: 62.253196716308594 = 0.017516834661364555 + 10.0 * 6.223567962646484
Epoch 2350, val loss: 1.5512090921401978
Epoch 2360, training loss: 62.22421646118164 = 0.017286259680986404 + 10.0 * 6.220693111419678
Epoch 2360, val loss: 1.554856538772583
Epoch 2370, training loss: 62.21782684326172 = 0.01706623286008835 + 10.0 * 6.220076084136963
Epoch 2370, val loss: 1.5587420463562012
Epoch 2380, training loss: 62.254676818847656 = 0.016857603564858437 + 10.0 * 6.223782062530518
Epoch 2380, val loss: 1.5624219179153442
Epoch 2390, training loss: 62.241085052490234 = 0.016641834750771523 + 10.0 * 6.2224440574646
Epoch 2390, val loss: 1.5657670497894287
Epoch 2400, training loss: 62.2357063293457 = 0.016429858282208443 + 10.0 * 6.221927642822266
Epoch 2400, val loss: 1.5693632364273071
Epoch 2410, training loss: 62.24512481689453 = 0.016226591542363167 + 10.0 * 6.2228899002075195
Epoch 2410, val loss: 1.573073387145996
Epoch 2420, training loss: 62.208885192871094 = 0.016026712954044342 + 10.0 * 6.21928596496582
Epoch 2420, val loss: 1.5767621994018555
Epoch 2430, training loss: 62.21109390258789 = 0.01583639532327652 + 10.0 * 6.2195258140563965
Epoch 2430, val loss: 1.580399751663208
Epoch 2440, training loss: 62.254878997802734 = 0.01565159112215042 + 10.0 * 6.2239227294921875
Epoch 2440, val loss: 1.5837732553482056
Epoch 2450, training loss: 62.23054122924805 = 0.015455552376806736 + 10.0 * 6.221508502960205
Epoch 2450, val loss: 1.5872167348861694
Epoch 2460, training loss: 62.20731735229492 = 0.015268789604306221 + 10.0 * 6.219204902648926
Epoch 2460, val loss: 1.5906319618225098
Epoch 2470, training loss: 62.21192169189453 = 0.015089264139533043 + 10.0 * 6.2196831703186035
Epoch 2470, val loss: 1.5941656827926636
Epoch 2480, training loss: 62.27166748046875 = 0.014916326850652695 + 10.0 * 6.225675106048584
Epoch 2480, val loss: 1.5977649688720703
Epoch 2490, training loss: 62.207393646240234 = 0.014729847200214863 + 10.0 * 6.219266414642334
Epoch 2490, val loss: 1.60075843334198
Epoch 2500, training loss: 62.19462585449219 = 0.014556555077433586 + 10.0 * 6.2180070877075195
Epoch 2500, val loss: 1.6043107509613037
Epoch 2510, training loss: 62.198280334472656 = 0.014390282332897186 + 10.0 * 6.21838903427124
Epoch 2510, val loss: 1.607750415802002
Epoch 2520, training loss: 62.28761291503906 = 0.014228611253201962 + 10.0 * 6.2273383140563965
Epoch 2520, val loss: 1.6108380556106567
Epoch 2530, training loss: 62.23019790649414 = 0.014064951799809933 + 10.0 * 6.22161340713501
Epoch 2530, val loss: 1.6142692565917969
Epoch 2540, training loss: 62.19759750366211 = 0.013898120261728764 + 10.0 * 6.218369960784912
Epoch 2540, val loss: 1.6174683570861816
Epoch 2550, training loss: 62.18970489501953 = 0.013744751922786236 + 10.0 * 6.217596054077148
Epoch 2550, val loss: 1.6207953691482544
Epoch 2560, training loss: 62.263790130615234 = 0.013593834824860096 + 10.0 * 6.225019454956055
Epoch 2560, val loss: 1.6238104104995728
Epoch 2570, training loss: 62.18889236450195 = 0.013438616879284382 + 10.0 * 6.217545509338379
Epoch 2570, val loss: 1.6270489692687988
Epoch 2580, training loss: 62.180267333984375 = 0.013287108391523361 + 10.0 * 6.216698169708252
Epoch 2580, val loss: 1.6301896572113037
Epoch 2590, training loss: 62.237205505371094 = 0.013142758049070835 + 10.0 * 6.222406387329102
Epoch 2590, val loss: 1.6332001686096191
Epoch 2600, training loss: 62.18014144897461 = 0.012996519915759563 + 10.0 * 6.216714382171631
Epoch 2600, val loss: 1.6364238262176514
Epoch 2610, training loss: 62.21802520751953 = 0.01285917591303587 + 10.0 * 6.220516681671143
Epoch 2610, val loss: 1.639612078666687
Epoch 2620, training loss: 62.178489685058594 = 0.012712829746305943 + 10.0 * 6.216577529907227
Epoch 2620, val loss: 1.6425241231918335
Epoch 2630, training loss: 62.17640686035156 = 0.012576224282383919 + 10.0 * 6.21638298034668
Epoch 2630, val loss: 1.6457016468048096
Epoch 2640, training loss: 62.16758728027344 = 0.01244300790131092 + 10.0 * 6.215514183044434
Epoch 2640, val loss: 1.648803472518921
Epoch 2650, training loss: 62.175533294677734 = 0.012314370833337307 + 10.0 * 6.21632194519043
Epoch 2650, val loss: 1.6518206596374512
Epoch 2660, training loss: 62.25267028808594 = 0.012189215049147606 + 10.0 * 6.224048137664795
Epoch 2660, val loss: 1.6546931266784668
Epoch 2670, training loss: 62.1916618347168 = 0.012055297382175922 + 10.0 * 6.217960834503174
Epoch 2670, val loss: 1.6574667692184448
Epoch 2680, training loss: 62.20731735229492 = 0.01192898117005825 + 10.0 * 6.219538688659668
Epoch 2680, val loss: 1.6605064868927002
Epoch 2690, training loss: 62.18309020996094 = 0.01180351059883833 + 10.0 * 6.217128753662109
Epoch 2690, val loss: 1.6632888317108154
Epoch 2700, training loss: 62.19048309326172 = 0.011680454015731812 + 10.0 * 6.2178802490234375
Epoch 2700, val loss: 1.666121244430542
Epoch 2710, training loss: 62.163429260253906 = 0.011559468694031239 + 10.0 * 6.215187072753906
Epoch 2710, val loss: 1.6692118644714355
Epoch 2720, training loss: 62.158714294433594 = 0.011445610783994198 + 10.0 * 6.21472692489624
Epoch 2720, val loss: 1.6722179651260376
Epoch 2730, training loss: 62.15641403198242 = 0.011332964524626732 + 10.0 * 6.214508056640625
Epoch 2730, val loss: 1.6750532388687134
Epoch 2740, training loss: 62.19453048706055 = 0.011223995126783848 + 10.0 * 6.2183308601379395
Epoch 2740, val loss: 1.6778398752212524
Epoch 2750, training loss: 62.160614013671875 = 0.01110752485692501 + 10.0 * 6.2149505615234375
Epoch 2750, val loss: 1.6802430152893066
Epoch 2760, training loss: 62.217220306396484 = 0.010997925885021687 + 10.0 * 6.2206220626831055
Epoch 2760, val loss: 1.682799220085144
Epoch 2770, training loss: 62.17749786376953 = 0.01088675856590271 + 10.0 * 6.216660976409912
Epoch 2770, val loss: 1.6856900453567505
Epoch 2780, training loss: 62.15730667114258 = 0.010777448303997517 + 10.0 * 6.214653015136719
Epoch 2780, val loss: 1.6883291006088257
Epoch 2790, training loss: 62.16826248168945 = 0.010674567893147469 + 10.0 * 6.215758800506592
Epoch 2790, val loss: 1.6912263631820679
Epoch 2800, training loss: 62.18730926513672 = 0.010570939630270004 + 10.0 * 6.2176737785339355
Epoch 2800, val loss: 1.6937733888626099
Epoch 2810, training loss: 62.150272369384766 = 0.010470951907336712 + 10.0 * 6.213980197906494
Epoch 2810, val loss: 1.6964884996414185
Epoch 2820, training loss: 62.153236389160156 = 0.010371897369623184 + 10.0 * 6.2142863273620605
Epoch 2820, val loss: 1.699173092842102
Epoch 2830, training loss: 62.20488739013672 = 0.01027736160904169 + 10.0 * 6.219460964202881
Epoch 2830, val loss: 1.7018183469772339
Epoch 2840, training loss: 62.153446197509766 = 0.010174191556870937 + 10.0 * 6.214327335357666
Epoch 2840, val loss: 1.704119324684143
Epoch 2850, training loss: 62.14243698120117 = 0.010078775696456432 + 10.0 * 6.213235855102539
Epoch 2850, val loss: 1.7069240808486938
Epoch 2860, training loss: 62.13229751586914 = 0.009985106065869331 + 10.0 * 6.212231159210205
Epoch 2860, val loss: 1.7095850706100464
Epoch 2870, training loss: 62.160362243652344 = 0.009896468371152878 + 10.0 * 6.2150468826293945
Epoch 2870, val loss: 1.7121491432189941
Epoch 2880, training loss: 62.15786361694336 = 0.009806167334318161 + 10.0 * 6.214805603027344
Epoch 2880, val loss: 1.7145473957061768
Epoch 2890, training loss: 62.15519332885742 = 0.009712892584502697 + 10.0 * 6.214548110961914
Epoch 2890, val loss: 1.7169508934020996
Epoch 2900, training loss: 62.184303283691406 = 0.009627155028283596 + 10.0 * 6.217467308044434
Epoch 2900, val loss: 1.7194255590438843
Epoch 2910, training loss: 62.166629791259766 = 0.009537660516798496 + 10.0 * 6.215709209442139
Epoch 2910, val loss: 1.7217177152633667
Epoch 2920, training loss: 62.128299713134766 = 0.00945082027465105 + 10.0 * 6.21188497543335
Epoch 2920, val loss: 1.7244579792022705
Epoch 2930, training loss: 62.140377044677734 = 0.00936721172183752 + 10.0 * 6.213100910186768
Epoch 2930, val loss: 1.7268807888031006
Epoch 2940, training loss: 62.19472885131836 = 0.009285583160817623 + 10.0 * 6.2185444831848145
Epoch 2940, val loss: 1.7290916442871094
Epoch 2950, training loss: 62.144840240478516 = 0.009202108718454838 + 10.0 * 6.213563919067383
Epoch 2950, val loss: 1.7314525842666626
Epoch 2960, training loss: 62.125728607177734 = 0.009120744653046131 + 10.0 * 6.211660861968994
Epoch 2960, val loss: 1.7340781688690186
Epoch 2970, training loss: 62.11782455444336 = 0.00904239621013403 + 10.0 * 6.210878372192383
Epoch 2970, val loss: 1.736330270767212
Epoch 2980, training loss: 62.16200637817383 = 0.008968093432486057 + 10.0 * 6.215303897857666
Epoch 2980, val loss: 1.738593578338623
Epoch 2990, training loss: 62.1507682800293 = 0.00888913031667471 + 10.0 * 6.214188098907471
Epoch 2990, val loss: 1.7408473491668701
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6925925925925926
0.8218239325250396
=== training gcn model ===
Epoch 0, training loss: 87.91446685791016 = 1.9462355375289917 + 10.0 * 8.596822738647461
Epoch 0, val loss: 1.935745358467102
Epoch 10, training loss: 87.89619445800781 = 1.9358079433441162 + 10.0 * 8.596038818359375
Epoch 10, val loss: 1.9257328510284424
Epoch 20, training loss: 87.83000183105469 = 1.923143982887268 + 10.0 * 8.590685844421387
Epoch 20, val loss: 1.9133986234664917
Epoch 30, training loss: 87.42849731445312 = 1.907561182975769 + 10.0 * 8.552093505859375
Epoch 30, val loss: 1.898192286491394
Epoch 40, training loss: 84.58244323730469 = 1.8903177976608276 + 10.0 * 8.26921272277832
Epoch 40, val loss: 1.8816739320755005
Epoch 50, training loss: 76.02537536621094 = 1.8728705644607544 + 10.0 * 7.415250301361084
Epoch 50, val loss: 1.8656156063079834
Epoch 60, training loss: 73.68891906738281 = 1.8605529069900513 + 10.0 * 7.182836532592773
Epoch 60, val loss: 1.8540853261947632
Epoch 70, training loss: 72.36266326904297 = 1.849704623222351 + 10.0 * 7.051296234130859
Epoch 70, val loss: 1.8439147472381592
Epoch 80, training loss: 71.34136962890625 = 1.8391526937484741 + 10.0 * 6.950222015380859
Epoch 80, val loss: 1.8344602584838867
Epoch 90, training loss: 70.51558685302734 = 1.830861210823059 + 10.0 * 6.868472576141357
Epoch 90, val loss: 1.827093243598938
Epoch 100, training loss: 69.68675994873047 = 1.8236188888549805 + 10.0 * 6.786314487457275
Epoch 100, val loss: 1.820780634880066
Epoch 110, training loss: 69.08861541748047 = 1.8172637224197388 + 10.0 * 6.72713565826416
Epoch 110, val loss: 1.8149440288543701
Epoch 120, training loss: 68.61064147949219 = 1.8105899095535278 + 10.0 * 6.680004596710205
Epoch 120, val loss: 1.8087396621704102
Epoch 130, training loss: 68.21752166748047 = 1.803758144378662 + 10.0 * 6.641376495361328
Epoch 130, val loss: 1.8024876117706299
Epoch 140, training loss: 67.90373229980469 = 1.7970256805419922 + 10.0 * 6.61067008972168
Epoch 140, val loss: 1.796267032623291
Epoch 150, training loss: 67.61575317382812 = 1.789941668510437 + 10.0 * 6.582581520080566
Epoch 150, val loss: 1.7900203466415405
Epoch 160, training loss: 67.38190460205078 = 1.78257155418396 + 10.0 * 6.559933662414551
Epoch 160, val loss: 1.7836077213287354
Epoch 170, training loss: 67.19886016845703 = 1.7747492790222168 + 10.0 * 6.5424113273620605
Epoch 170, val loss: 1.7768590450286865
Epoch 180, training loss: 67.00370788574219 = 1.7662756443023682 + 10.0 * 6.523743152618408
Epoch 180, val loss: 1.7697858810424805
Epoch 190, training loss: 66.84201049804688 = 1.7570850849151611 + 10.0 * 6.5084919929504395
Epoch 190, val loss: 1.7622231245040894
Epoch 200, training loss: 66.6903305053711 = 1.747059941291809 + 10.0 * 6.494327068328857
Epoch 200, val loss: 1.754014492034912
Epoch 210, training loss: 66.59512329101562 = 1.736133098602295 + 10.0 * 6.485899448394775
Epoch 210, val loss: 1.7451575994491577
Epoch 220, training loss: 66.44571685791016 = 1.7240201234817505 + 10.0 * 6.472169876098633
Epoch 220, val loss: 1.7355197668075562
Epoch 230, training loss: 66.32456970214844 = 1.710787057876587 + 10.0 * 6.46137809753418
Epoch 230, val loss: 1.7249572277069092
Epoch 240, training loss: 66.23068237304688 = 1.6962236166000366 + 10.0 * 6.453446388244629
Epoch 240, val loss: 1.7135614156723022
Epoch 250, training loss: 66.10846710205078 = 1.6804587841033936 + 10.0 * 6.442800998687744
Epoch 250, val loss: 1.7010314464569092
Epoch 260, training loss: 66.01342010498047 = 1.6633410453796387 + 10.0 * 6.4350080490112305
Epoch 260, val loss: 1.6874839067459106
Epoch 270, training loss: 65.93585205078125 = 1.6447043418884277 + 10.0 * 6.429114818572998
Epoch 270, val loss: 1.6729227304458618
Epoch 280, training loss: 65.8222427368164 = 1.624708890914917 + 10.0 * 6.419753074645996
Epoch 280, val loss: 1.6572597026824951
Epoch 290, training loss: 65.7285385131836 = 1.60335111618042 + 10.0 * 6.4125189781188965
Epoch 290, val loss: 1.6405775547027588
Epoch 300, training loss: 65.71569061279297 = 1.58063805103302 + 10.0 * 6.413505554199219
Epoch 300, val loss: 1.6227729320526123
Epoch 310, training loss: 65.58615112304688 = 1.5564912557601929 + 10.0 * 6.402966022491455
Epoch 310, val loss: 1.6042132377624512
Epoch 320, training loss: 65.4908218383789 = 1.5312447547912598 + 10.0 * 6.395957946777344
Epoch 320, val loss: 1.584817886352539
Epoch 330, training loss: 65.39735412597656 = 1.505108118057251 + 10.0 * 6.389224052429199
Epoch 330, val loss: 1.5647072792053223
Epoch 340, training loss: 65.36128997802734 = 1.4780875444412231 + 10.0 * 6.388319969177246
Epoch 340, val loss: 1.5440559387207031
Epoch 350, training loss: 65.24864196777344 = 1.450063705444336 + 10.0 * 6.379858016967773
Epoch 350, val loss: 1.5232025384902954
Epoch 360, training loss: 65.16264343261719 = 1.421592354774475 + 10.0 * 6.374105453491211
Epoch 360, val loss: 1.5020802021026611
Epoch 370, training loss: 65.1521224975586 = 1.3925367593765259 + 10.0 * 6.375958442687988
Epoch 370, val loss: 1.4808281660079956
Epoch 380, training loss: 65.03269958496094 = 1.3631902933120728 + 10.0 * 6.366950988769531
Epoch 380, val loss: 1.4596885442733765
Epoch 390, training loss: 64.95254516601562 = 1.3337668180465698 + 10.0 * 6.361878395080566
Epoch 390, val loss: 1.4387648105621338
Epoch 400, training loss: 64.89793395996094 = 1.304255723953247 + 10.0 * 6.359367847442627
Epoch 400, val loss: 1.4182851314544678
Epoch 410, training loss: 64.8259506225586 = 1.2749054431915283 + 10.0 * 6.355104446411133
Epoch 410, val loss: 1.3983081579208374
Epoch 420, training loss: 64.7724380493164 = 1.245834231376648 + 10.0 * 6.352660179138184
Epoch 420, val loss: 1.3787989616394043
Epoch 430, training loss: 64.73697662353516 = 1.2168288230895996 + 10.0 * 6.352014541625977
Epoch 430, val loss: 1.3601566553115845
Epoch 440, training loss: 64.640625 = 1.1885943412780762 + 10.0 * 6.345203399658203
Epoch 440, val loss: 1.3419721126556396
Epoch 450, training loss: 64.57267761230469 = 1.160860300064087 + 10.0 * 6.341181755065918
Epoch 450, val loss: 1.324731469154358
Epoch 460, training loss: 64.5147476196289 = 1.1336814165115356 + 10.0 * 6.338106632232666
Epoch 460, val loss: 1.3083224296569824
Epoch 470, training loss: 64.50241088867188 = 1.1071257591247559 + 10.0 * 6.339528560638428
Epoch 470, val loss: 1.2926784753799438
Epoch 480, training loss: 64.4367446899414 = 1.0809595584869385 + 10.0 * 6.335577964782715
Epoch 480, val loss: 1.277793526649475
Epoch 490, training loss: 64.36489868164062 = 1.0556447505950928 + 10.0 * 6.330925941467285
Epoch 490, val loss: 1.263660192489624
Epoch 500, training loss: 64.31061553955078 = 1.031103253364563 + 10.0 * 6.327950954437256
Epoch 500, val loss: 1.2504998445510864
Epoch 510, training loss: 64.3035888671875 = 1.0072643756866455 + 10.0 * 6.32963228225708
Epoch 510, val loss: 1.2379711866378784
Epoch 520, training loss: 64.22095489501953 = 0.9839200973510742 + 10.0 * 6.323703765869141
Epoch 520, val loss: 1.226244330406189
Epoch 530, training loss: 64.1731948852539 = 0.9614284038543701 + 10.0 * 6.321177005767822
Epoch 530, val loss: 1.2151013612747192
Epoch 540, training loss: 64.15961456298828 = 0.9395028948783875 + 10.0 * 6.3220109939575195
Epoch 540, val loss: 1.2045248746871948
Epoch 550, training loss: 64.09785461425781 = 0.9180595874786377 + 10.0 * 6.31797981262207
Epoch 550, val loss: 1.194654941558838
Epoch 560, training loss: 64.07992553710938 = 0.8973159790039062 + 10.0 * 6.31826114654541
Epoch 560, val loss: 1.185096025466919
Epoch 570, training loss: 64.0302963256836 = 0.8770220875740051 + 10.0 * 6.315327167510986
Epoch 570, val loss: 1.1764823198318481
Epoch 580, training loss: 63.98225784301758 = 0.8572573661804199 + 10.0 * 6.3125
Epoch 580, val loss: 1.1679667234420776
Epoch 590, training loss: 63.941070556640625 = 0.8378952145576477 + 10.0 * 6.310317516326904
Epoch 590, val loss: 1.1601226329803467
Epoch 600, training loss: 63.90321731567383 = 0.8190378546714783 + 10.0 * 6.308417797088623
Epoch 600, val loss: 1.1527788639068604
Epoch 610, training loss: 63.86677169799805 = 0.8006554841995239 + 10.0 * 6.30661153793335
Epoch 610, val loss: 1.1457768678665161
Epoch 620, training loss: 63.84196090698242 = 0.7826050519943237 + 10.0 * 6.305935859680176
Epoch 620, val loss: 1.1390777826309204
Epoch 630, training loss: 63.80964660644531 = 0.764824390411377 + 10.0 * 6.304482460021973
Epoch 630, val loss: 1.1328312158584595
Epoch 640, training loss: 63.779300689697266 = 0.7474555969238281 + 10.0 * 6.303184509277344
Epoch 640, val loss: 1.1267738342285156
Epoch 650, training loss: 63.74481201171875 = 0.7302918434143066 + 10.0 * 6.301451683044434
Epoch 650, val loss: 1.1211671829223633
Epoch 660, training loss: 63.74162673950195 = 0.7136020064353943 + 10.0 * 6.302802562713623
Epoch 660, val loss: 1.1158791780471802
Epoch 670, training loss: 63.68000411987305 = 0.6971551179885864 + 10.0 * 6.298285007476807
Epoch 670, val loss: 1.1105703115463257
Epoch 680, training loss: 63.64179992675781 = 0.6808844208717346 + 10.0 * 6.296091556549072
Epoch 680, val loss: 1.1060400009155273
Epoch 690, training loss: 63.605960845947266 = 0.6650246977806091 + 10.0 * 6.294093608856201
Epoch 690, val loss: 1.1015859842300415
Epoch 700, training loss: 63.57911682128906 = 0.6492835879325867 + 10.0 * 6.292983055114746
Epoch 700, val loss: 1.097435712814331
Epoch 710, training loss: 63.60504913330078 = 0.6336670517921448 + 10.0 * 6.297138214111328
Epoch 710, val loss: 1.0932142734527588
Epoch 720, training loss: 63.53023910522461 = 0.618137538433075 + 10.0 * 6.291210174560547
Epoch 720, val loss: 1.0891404151916504
Epoch 730, training loss: 63.49882888793945 = 0.6028962731361389 + 10.0 * 6.28959321975708
Epoch 730, val loss: 1.085423469543457
Epoch 740, training loss: 63.46543502807617 = 0.5879335999488831 + 10.0 * 6.287750244140625
Epoch 740, val loss: 1.0820879936218262
Epoch 750, training loss: 63.45028305053711 = 0.5731391310691833 + 10.0 * 6.28771448135376
Epoch 750, val loss: 1.0787789821624756
Epoch 760, training loss: 63.42874526977539 = 0.5584208965301514 + 10.0 * 6.287032127380371
Epoch 760, val loss: 1.0752754211425781
Epoch 770, training loss: 63.402984619140625 = 0.5438337326049805 + 10.0 * 6.285914897918701
Epoch 770, val loss: 1.0721467733383179
Epoch 780, training loss: 63.370582580566406 = 0.5295109748840332 + 10.0 * 6.284107208251953
Epoch 780, val loss: 1.0691709518432617
Epoch 790, training loss: 63.415802001953125 = 0.5153514742851257 + 10.0 * 6.290045261383057
Epoch 790, val loss: 1.06634521484375
Epoch 800, training loss: 63.34925079345703 = 0.5012309551239014 + 10.0 * 6.284801959991455
Epoch 800, val loss: 1.0634530782699585
Epoch 810, training loss: 63.29163360595703 = 0.48740559816360474 + 10.0 * 6.280423164367676
Epoch 810, val loss: 1.060809850692749
Epoch 820, training loss: 63.26871871948242 = 0.47380954027175903 + 10.0 * 6.279490947723389
Epoch 820, val loss: 1.0583930015563965
Epoch 830, training loss: 63.24742126464844 = 0.46036845445632935 + 10.0 * 6.27870512008667
Epoch 830, val loss: 1.055984616279602
Epoch 840, training loss: 63.267005920410156 = 0.4470311403274536 + 10.0 * 6.281997203826904
Epoch 840, val loss: 1.0538264513015747
Epoch 850, training loss: 63.204010009765625 = 0.43389055132865906 + 10.0 * 6.277011871337891
Epoch 850, val loss: 1.0515345335006714
Epoch 860, training loss: 63.181339263916016 = 0.42104271054267883 + 10.0 * 6.276029586791992
Epoch 860, val loss: 1.0497310161590576
Epoch 870, training loss: 63.15494918823242 = 0.40841275453567505 + 10.0 * 6.274653434753418
Epoch 870, val loss: 1.0478662252426147
Epoch 880, training loss: 63.200950622558594 = 0.3960709571838379 + 10.0 * 6.280488014221191
Epoch 880, val loss: 1.0460692644119263
Epoch 890, training loss: 63.13023376464844 = 0.3837282955646515 + 10.0 * 6.274650573730469
Epoch 890, val loss: 1.0444947481155396
Epoch 900, training loss: 63.09316635131836 = 0.3717796206474304 + 10.0 * 6.272138595581055
Epoch 900, val loss: 1.0430482625961304
Epoch 910, training loss: 63.08964920043945 = 0.36015138030052185 + 10.0 * 6.272949695587158
Epoch 910, val loss: 1.0419970750808716
Epoch 920, training loss: 63.06636428833008 = 0.34870389103889465 + 10.0 * 6.27176570892334
Epoch 920, val loss: 1.0407562255859375
Epoch 930, training loss: 63.040931701660156 = 0.3376104235649109 + 10.0 * 6.270331859588623
Epoch 930, val loss: 1.0398577451705933
Epoch 940, training loss: 63.02201843261719 = 0.3268364369869232 + 10.0 * 6.2695183753967285
Epoch 940, val loss: 1.0391682386398315
Epoch 950, training loss: 63.03350830078125 = 0.31639784574508667 + 10.0 * 6.2717108726501465
Epoch 950, val loss: 1.0389443635940552
Epoch 960, training loss: 63.00422286987305 = 0.30618780851364136 + 10.0 * 6.269803524017334
Epoch 960, val loss: 1.0384339094161987
Epoch 970, training loss: 62.96857833862305 = 0.2963615357875824 + 10.0 * 6.267221927642822
Epoch 970, val loss: 1.0384340286254883
Epoch 980, training loss: 62.95894241333008 = 0.286883145570755 + 10.0 * 6.267205715179443
Epoch 980, val loss: 1.0388545989990234
Epoch 990, training loss: 62.942527770996094 = 0.27768054604530334 + 10.0 * 6.26648473739624
Epoch 990, val loss: 1.03913414478302
Epoch 1000, training loss: 62.919883728027344 = 0.2688271701335907 + 10.0 * 6.265105724334717
Epoch 1000, val loss: 1.0399280786514282
Epoch 1010, training loss: 62.900672912597656 = 0.26029303669929504 + 10.0 * 6.2640380859375
Epoch 1010, val loss: 1.0408614873886108
Epoch 1020, training loss: 62.88307571411133 = 0.2520929276943207 + 10.0 * 6.263098239898682
Epoch 1020, val loss: 1.0421398878097534
Epoch 1030, training loss: 62.96205139160156 = 0.24423784017562866 + 10.0 * 6.2717814445495605
Epoch 1030, val loss: 1.043530821800232
Epoch 1040, training loss: 62.907936096191406 = 0.23638616502285004 + 10.0 * 6.267155170440674
Epoch 1040, val loss: 1.044818639755249
Epoch 1050, training loss: 62.84292221069336 = 0.22899632155895233 + 10.0 * 6.261392593383789
Epoch 1050, val loss: 1.0466598272323608
Epoch 1060, training loss: 62.83369445800781 = 0.2219466120004654 + 10.0 * 6.26117467880249
Epoch 1060, val loss: 1.0489161014556885
Epoch 1070, training loss: 62.85140609741211 = 0.21514646708965302 + 10.0 * 6.2636260986328125
Epoch 1070, val loss: 1.0511523485183716
Epoch 1080, training loss: 62.80477523803711 = 0.20856304466724396 + 10.0 * 6.2596211433410645
Epoch 1080, val loss: 1.053644061088562
Epoch 1090, training loss: 62.78444290161133 = 0.20224960148334503 + 10.0 * 6.258219242095947
Epoch 1090, val loss: 1.056315302848816
Epoch 1100, training loss: 62.89948272705078 = 0.19620126485824585 + 10.0 * 6.270328044891357
Epoch 1100, val loss: 1.0595945119857788
Epoch 1110, training loss: 62.79275894165039 = 0.19021780788898468 + 10.0 * 6.260254383087158
Epoch 1110, val loss: 1.0614981651306152
Epoch 1120, training loss: 62.75962448120117 = 0.18454955518245697 + 10.0 * 6.25750732421875
Epoch 1120, val loss: 1.0643470287322998
Epoch 1130, training loss: 62.737342834472656 = 0.17915451526641846 + 10.0 * 6.255818843841553
Epoch 1130, val loss: 1.067708134651184
Epoch 1140, training loss: 62.73568344116211 = 0.17396199703216553 + 10.0 * 6.256172180175781
Epoch 1140, val loss: 1.0709689855575562
Epoch 1150, training loss: 62.743350982666016 = 0.16890671849250793 + 10.0 * 6.257444381713867
Epoch 1150, val loss: 1.0741102695465088
Epoch 1160, training loss: 62.71033477783203 = 0.1640060544013977 + 10.0 * 6.254632949829102
Epoch 1160, val loss: 1.0774697065353394
Epoch 1170, training loss: 62.743900299072266 = 0.15931491553783417 + 10.0 * 6.258458614349365
Epoch 1170, val loss: 1.0812585353851318
Epoch 1180, training loss: 62.70390701293945 = 0.1548076570034027 + 10.0 * 6.254909992218018
Epoch 1180, val loss: 1.0845381021499634
Epoch 1190, training loss: 62.681114196777344 = 0.1504492163658142 + 10.0 * 6.253066539764404
Epoch 1190, val loss: 1.0880722999572754
Epoch 1200, training loss: 62.665767669677734 = 0.14626574516296387 + 10.0 * 6.251950263977051
Epoch 1200, val loss: 1.0919861793518066
Epoch 1210, training loss: 62.65781021118164 = 0.14224585890769958 + 10.0 * 6.251556396484375
Epoch 1210, val loss: 1.0958964824676514
Epoch 1220, training loss: 62.73131561279297 = 0.13838550448417664 + 10.0 * 6.259293079376221
Epoch 1220, val loss: 1.100008249282837
Epoch 1230, training loss: 62.697940826416016 = 0.13452273607254028 + 10.0 * 6.256341934204102
Epoch 1230, val loss: 1.1029995679855347
Epoch 1240, training loss: 62.628662109375 = 0.1308528482913971 + 10.0 * 6.249781131744385
Epoch 1240, val loss: 1.1071534156799316
Epoch 1250, training loss: 62.62721633911133 = 0.1273699849843979 + 10.0 * 6.2499847412109375
Epoch 1250, val loss: 1.1114999055862427
Epoch 1260, training loss: 62.61839294433594 = 0.12400288879871368 + 10.0 * 6.249438762664795
Epoch 1260, val loss: 1.1154402494430542
Epoch 1270, training loss: 62.660491943359375 = 0.12075097858905792 + 10.0 * 6.253973960876465
Epoch 1270, val loss: 1.1193956136703491
Epoch 1280, training loss: 62.6580696105957 = 0.1175663024187088 + 10.0 * 6.254050254821777
Epoch 1280, val loss: 1.1234490871429443
Epoch 1290, training loss: 62.599361419677734 = 0.11445894092321396 + 10.0 * 6.248490333557129
Epoch 1290, val loss: 1.1277494430541992
Epoch 1300, training loss: 62.582244873046875 = 0.11149564385414124 + 10.0 * 6.247075080871582
Epoch 1300, val loss: 1.131988525390625
Epoch 1310, training loss: 62.5745849609375 = 0.10866455733776093 + 10.0 * 6.246592044830322
Epoch 1310, val loss: 1.1362359523773193
Epoch 1320, training loss: 62.575382232666016 = 0.10591837018728256 + 10.0 * 6.246946334838867
Epoch 1320, val loss: 1.1404932737350464
Epoch 1330, training loss: 62.605857849121094 = 0.10323317348957062 + 10.0 * 6.250262260437012
Epoch 1330, val loss: 1.1447607278823853
Epoch 1340, training loss: 62.562965393066406 = 0.10064619034528732 + 10.0 * 6.246232032775879
Epoch 1340, val loss: 1.1492304801940918
Epoch 1350, training loss: 62.560638427734375 = 0.09814337641000748 + 10.0 * 6.246249198913574
Epoch 1350, val loss: 1.1533606052398682
Epoch 1360, training loss: 62.572750091552734 = 0.09572220593690872 + 10.0 * 6.247702598571777
Epoch 1360, val loss: 1.1579656600952148
Epoch 1370, training loss: 62.544288635253906 = 0.09334777295589447 + 10.0 * 6.245093822479248
Epoch 1370, val loss: 1.1622302532196045
Epoch 1380, training loss: 62.559059143066406 = 0.09110042452812195 + 10.0 * 6.246796131134033
Epoch 1380, val loss: 1.1667957305908203
Epoch 1390, training loss: 62.52246856689453 = 0.08887653052806854 + 10.0 * 6.243359565734863
Epoch 1390, val loss: 1.1706737279891968
Epoch 1400, training loss: 62.516754150390625 = 0.08674812316894531 + 10.0 * 6.2430009841918945
Epoch 1400, val loss: 1.1753413677215576
Epoch 1410, training loss: 62.51081848144531 = 0.08469168841838837 + 10.0 * 6.242612838745117
Epoch 1410, val loss: 1.1797131299972534
Epoch 1420, training loss: 62.556644439697266 = 0.08271422237157822 + 10.0 * 6.2473931312561035
Epoch 1420, val loss: 1.1842560768127441
Epoch 1430, training loss: 62.50510787963867 = 0.08073690533638 + 10.0 * 6.24243688583374
Epoch 1430, val loss: 1.1882270574569702
Epoch 1440, training loss: 62.51047897338867 = 0.07885323464870453 + 10.0 * 6.243162631988525
Epoch 1440, val loss: 1.1928911209106445
Epoch 1450, training loss: 62.48957443237305 = 0.07703632861375809 + 10.0 * 6.241253852844238
Epoch 1450, val loss: 1.1971768140792847
Epoch 1460, training loss: 62.480186462402344 = 0.07527933269739151 + 10.0 * 6.240490913391113
Epoch 1460, val loss: 1.20125150680542
Epoch 1470, training loss: 62.480918884277344 = 0.07358426600694656 + 10.0 * 6.2407331466674805
Epoch 1470, val loss: 1.2056998014450073
Epoch 1480, training loss: 62.50669479370117 = 0.07191837579011917 + 10.0 * 6.243477821350098
Epoch 1480, val loss: 1.209952473640442
Epoch 1490, training loss: 62.49726867675781 = 0.07027298957109451 + 10.0 * 6.24269962310791
Epoch 1490, val loss: 1.2142161130905151
Epoch 1500, training loss: 62.464378356933594 = 0.06870757043361664 + 10.0 * 6.239567279815674
Epoch 1500, val loss: 1.2184480428695679
Epoch 1510, training loss: 62.46549987792969 = 0.06719110161066055 + 10.0 * 6.23983097076416
Epoch 1510, val loss: 1.222946286201477
Epoch 1520, training loss: 62.46916198730469 = 0.06571711599826813 + 10.0 * 6.240344524383545
Epoch 1520, val loss: 1.227070927619934
Epoch 1530, training loss: 62.45075225830078 = 0.0642712339758873 + 10.0 * 6.238648414611816
Epoch 1530, val loss: 1.231823444366455
Epoch 1540, training loss: 62.458160400390625 = 0.06289133429527283 + 10.0 * 6.239526748657227
Epoch 1540, val loss: 1.2358654737472534
Epoch 1550, training loss: 62.436031341552734 = 0.06153256818652153 + 10.0 * 6.237450122833252
Epoch 1550, val loss: 1.2402377128601074
Epoch 1560, training loss: 62.44032287597656 = 0.060211215168237686 + 10.0 * 6.238011360168457
Epoch 1560, val loss: 1.2445629835128784
Epoch 1570, training loss: 62.478572845458984 = 0.058949075639247894 + 10.0 * 6.241962432861328
Epoch 1570, val loss: 1.248461365699768
Epoch 1580, training loss: 62.439945220947266 = 0.057676080614328384 + 10.0 * 6.238226890563965
Epoch 1580, val loss: 1.253259539604187
Epoch 1590, training loss: 62.42116928100586 = 0.056476108729839325 + 10.0 * 6.236469268798828
Epoch 1590, val loss: 1.2571284770965576
Epoch 1600, training loss: 62.410247802734375 = 0.055305808782577515 + 10.0 * 6.235494136810303
Epoch 1600, val loss: 1.2616239786148071
Epoch 1610, training loss: 62.4143180847168 = 0.0541725791990757 + 10.0 * 6.236014366149902
Epoch 1610, val loss: 1.2658100128173828
Epoch 1620, training loss: 62.428462982177734 = 0.05306439474225044 + 10.0 * 6.237539768218994
Epoch 1620, val loss: 1.2698583602905273
Epoch 1630, training loss: 62.42695999145508 = 0.05198429524898529 + 10.0 * 6.237497806549072
Epoch 1630, val loss: 1.2741782665252686
Epoch 1640, training loss: 62.40340042114258 = 0.05092126503586769 + 10.0 * 6.235247611999512
Epoch 1640, val loss: 1.2782071828842163
Epoch 1650, training loss: 62.42025375366211 = 0.04990757256746292 + 10.0 * 6.237034797668457
Epoch 1650, val loss: 1.2823967933654785
Epoch 1660, training loss: 62.38832473754883 = 0.04890468716621399 + 10.0 * 6.233942031860352
Epoch 1660, val loss: 1.2865163087844849
Epoch 1670, training loss: 62.38893127441406 = 0.04794840142130852 + 10.0 * 6.234098434448242
Epoch 1670, val loss: 1.2909440994262695
Epoch 1680, training loss: 62.4083137512207 = 0.04701371118426323 + 10.0 * 6.236130237579346
Epoch 1680, val loss: 1.2948265075683594
Epoch 1690, training loss: 62.38140106201172 = 0.046090640127658844 + 10.0 * 6.2335309982299805
Epoch 1690, val loss: 1.2987558841705322
Epoch 1700, training loss: 62.37406921386719 = 0.04520271345973015 + 10.0 * 6.23288631439209
Epoch 1700, val loss: 1.3028514385223389
Epoch 1710, training loss: 62.399356842041016 = 0.044350698590278625 + 10.0 * 6.235500812530518
Epoch 1710, val loss: 1.3069149255752563
Epoch 1720, training loss: 62.40700912475586 = 0.043491266667842865 + 10.0 * 6.23635196685791
Epoch 1720, val loss: 1.3110798597335815
Epoch 1730, training loss: 62.35966873168945 = 0.04265240579843521 + 10.0 * 6.231701850891113
Epoch 1730, val loss: 1.3147250413894653
Epoch 1740, training loss: 62.348167419433594 = 0.041849009692668915 + 10.0 * 6.2306318283081055
Epoch 1740, val loss: 1.3188499212265015
Epoch 1750, training loss: 62.3431510925293 = 0.04108113795518875 + 10.0 * 6.2302069664001465
Epoch 1750, val loss: 1.3226686716079712
Epoch 1760, training loss: 62.38925552368164 = 0.04033762589097023 + 10.0 * 6.234891891479492
Epoch 1760, val loss: 1.3262934684753418
Epoch 1770, training loss: 62.37488555908203 = 0.03958063945174217 + 10.0 * 6.233530521392822
Epoch 1770, val loss: 1.3306437730789185
Epoch 1780, training loss: 62.35093307495117 = 0.038843780755996704 + 10.0 * 6.231208801269531
Epoch 1780, val loss: 1.3340377807617188
Epoch 1790, training loss: 62.336158752441406 = 0.038138654083013535 + 10.0 * 6.229802131652832
Epoch 1790, val loss: 1.3384134769439697
Epoch 1800, training loss: 62.328514099121094 = 0.03746223449707031 + 10.0 * 6.229105472564697
Epoch 1800, val loss: 1.3420737981796265
Epoch 1810, training loss: 62.383480072021484 = 0.03681587055325508 + 10.0 * 6.234666347503662
Epoch 1810, val loss: 1.3459421396255493
Epoch 1820, training loss: 62.32592010498047 = 0.03613212704658508 + 10.0 * 6.228978633880615
Epoch 1820, val loss: 1.3497278690338135
Epoch 1830, training loss: 62.313236236572266 = 0.03549681603908539 + 10.0 * 6.227774143218994
Epoch 1830, val loss: 1.353405475616455
Epoch 1840, training loss: 62.311092376708984 = 0.03488221764564514 + 10.0 * 6.227621078491211
Epoch 1840, val loss: 1.3573163747787476
Epoch 1850, training loss: 62.361305236816406 = 0.03428411856293678 + 10.0 * 6.232702255249023
Epoch 1850, val loss: 1.3612691164016724
Epoch 1860, training loss: 62.34855270385742 = 0.03368433564901352 + 10.0 * 6.231486797332764
Epoch 1860, val loss: 1.3643043041229248
Epoch 1870, training loss: 62.32331466674805 = 0.033090509474277496 + 10.0 * 6.22902250289917
Epoch 1870, val loss: 1.368269681930542
Epoch 1880, training loss: 62.30809783935547 = 0.03253424912691116 + 10.0 * 6.227556228637695
Epoch 1880, val loss: 1.3717122077941895
Epoch 1890, training loss: 62.312965393066406 = 0.03198208659887314 + 10.0 * 6.228098392486572
Epoch 1890, val loss: 1.3754823207855225
Epoch 1900, training loss: 62.3090705871582 = 0.03145160153508186 + 10.0 * 6.227761745452881
Epoch 1900, val loss: 1.3789368867874146
Epoch 1910, training loss: 62.3067741394043 = 0.030929921194911003 + 10.0 * 6.227584362030029
Epoch 1910, val loss: 1.3828405141830444
Epoch 1920, training loss: 62.29641342163086 = 0.030414944514632225 + 10.0 * 6.22659969329834
Epoch 1920, val loss: 1.3864710330963135
Epoch 1930, training loss: 62.3348274230957 = 0.02992614544928074 + 10.0 * 6.230490207672119
Epoch 1930, val loss: 1.3903331756591797
Epoch 1940, training loss: 62.30108642578125 = 0.02942636050283909 + 10.0 * 6.227166175842285
Epoch 1940, val loss: 1.3934768438339233
Epoch 1950, training loss: 62.28364944458008 = 0.0289424080401659 + 10.0 * 6.225470542907715
Epoch 1950, val loss: 1.3970320224761963
Epoch 1960, training loss: 62.27840805053711 = 0.028480829671025276 + 10.0 * 6.224992752075195
Epoch 1960, val loss: 1.400730013847351
Epoch 1970, training loss: 62.309730529785156 = 0.028037581592798233 + 10.0 * 6.2281694412231445
Epoch 1970, val loss: 1.4040671586990356
Epoch 1980, training loss: 62.27267074584961 = 0.027585627511143684 + 10.0 * 6.224508762359619
Epoch 1980, val loss: 1.4076423645019531
Epoch 1990, training loss: 62.280174255371094 = 0.027155065909028053 + 10.0 * 6.225301742553711
Epoch 1990, val loss: 1.4114571809768677
Epoch 2000, training loss: 62.296058654785156 = 0.026730429381132126 + 10.0 * 6.226933002471924
Epoch 2000, val loss: 1.4148221015930176
Epoch 2010, training loss: 62.26626205444336 = 0.026305289939045906 + 10.0 * 6.223995685577393
Epoch 2010, val loss: 1.4180712699890137
Epoch 2020, training loss: 62.28626251220703 = 0.025899652391672134 + 10.0 * 6.226036548614502
Epoch 2020, val loss: 1.4215353727340698
Epoch 2030, training loss: 62.26168441772461 = 0.025503909215331078 + 10.0 * 6.223618030548096
Epoch 2030, val loss: 1.4247715473175049
Epoch 2040, training loss: 62.262115478515625 = 0.025123219937086105 + 10.0 * 6.22369909286499
Epoch 2040, val loss: 1.428178071975708
Epoch 2050, training loss: 62.30672836303711 = 0.02474495768547058 + 10.0 * 6.228198051452637
Epoch 2050, val loss: 1.4313256740570068
Epoch 2060, training loss: 62.2600212097168 = 0.024356147274374962 + 10.0 * 6.22356653213501
Epoch 2060, val loss: 1.4347140789031982
Epoch 2070, training loss: 62.2450065612793 = 0.023996606469154358 + 10.0 * 6.222100734710693
Epoch 2070, val loss: 1.4380614757537842
Epoch 2080, training loss: 62.24696350097656 = 0.023646444082260132 + 10.0 * 6.222331523895264
Epoch 2080, val loss: 1.4414316415786743
Epoch 2090, training loss: 62.296112060546875 = 0.023311804980039597 + 10.0 * 6.227280139923096
Epoch 2090, val loss: 1.4443447589874268
Epoch 2100, training loss: 62.25414276123047 = 0.02294815331697464 + 10.0 * 6.223119258880615
Epoch 2100, val loss: 1.4476252794265747
Epoch 2110, training loss: 62.265201568603516 = 0.02262110449373722 + 10.0 * 6.224257946014404
Epoch 2110, val loss: 1.450785517692566
Epoch 2120, training loss: 62.232513427734375 = 0.022287145256996155 + 10.0 * 6.221022605895996
Epoch 2120, val loss: 1.4543702602386475
Epoch 2130, training loss: 62.23933792114258 = 0.021976498886942863 + 10.0 * 6.221735954284668
Epoch 2130, val loss: 1.4577744007110596
Epoch 2140, training loss: 62.34065628051758 = 0.021673034876585007 + 10.0 * 6.231898307800293
Epoch 2140, val loss: 1.4608936309814453
Epoch 2150, training loss: 62.263031005859375 = 0.02134004607796669 + 10.0 * 6.2241692543029785
Epoch 2150, val loss: 1.4634703397750854
Epoch 2160, training loss: 62.22358703613281 = 0.02103905752301216 + 10.0 * 6.220254898071289
Epoch 2160, val loss: 1.466813325881958
Epoch 2170, training loss: 62.21437454223633 = 0.02074800245463848 + 10.0 * 6.219362735748291
Epoch 2170, val loss: 1.4701004028320312
Epoch 2180, training loss: 62.214393615722656 = 0.020469626411795616 + 10.0 * 6.2193922996521
Epoch 2180, val loss: 1.4732141494750977
Epoch 2190, training loss: 62.29627990722656 = 0.020196788012981415 + 10.0 * 6.227608680725098
Epoch 2190, val loss: 1.475859522819519
Epoch 2200, training loss: 62.254634857177734 = 0.01990746334195137 + 10.0 * 6.223472595214844
Epoch 2200, val loss: 1.4790925979614258
Epoch 2210, training loss: 62.25017547607422 = 0.019631024450063705 + 10.0 * 6.2230544090271
Epoch 2210, val loss: 1.4819856882095337
Epoch 2220, training loss: 62.215843200683594 = 0.01936039701104164 + 10.0 * 6.219648361206055
Epoch 2220, val loss: 1.4852256774902344
Epoch 2230, training loss: 62.2105827331543 = 0.019100327044725418 + 10.0 * 6.2191481590271
Epoch 2230, val loss: 1.488174557685852
Epoch 2240, training loss: 62.20049285888672 = 0.018850194290280342 + 10.0 * 6.218164443969727
Epoch 2240, val loss: 1.4913743734359741
Epoch 2250, training loss: 62.20881271362305 = 0.01860458217561245 + 10.0 * 6.219020843505859
Epoch 2250, val loss: 1.4941805601119995
Epoch 2260, training loss: 62.242061614990234 = 0.018356800079345703 + 10.0 * 6.222370624542236
Epoch 2260, val loss: 1.4967694282531738
Epoch 2270, training loss: 62.247215270996094 = 0.018115723505616188 + 10.0 * 6.222909927368164
Epoch 2270, val loss: 1.5004167556762695
Epoch 2280, training loss: 62.21111297607422 = 0.01787078194320202 + 10.0 * 6.219324111938477
Epoch 2280, val loss: 1.5028761625289917
Epoch 2290, training loss: 62.20030212402344 = 0.017640911042690277 + 10.0 * 6.218266487121582
Epoch 2290, val loss: 1.5054733753204346
Epoch 2300, training loss: 62.19443130493164 = 0.017420083284378052 + 10.0 * 6.217700958251953
Epoch 2300, val loss: 1.5086015462875366
Epoch 2310, training loss: 62.211334228515625 = 0.017202721908688545 + 10.0 * 6.219412803649902
Epoch 2310, val loss: 1.5115675926208496
Epoch 2320, training loss: 62.19524002075195 = 0.016980458050966263 + 10.0 * 6.217825889587402
Epoch 2320, val loss: 1.5143743753433228
Epoch 2330, training loss: 62.19927215576172 = 0.01676464080810547 + 10.0 * 6.218250751495361
Epoch 2330, val loss: 1.5174611806869507
Epoch 2340, training loss: 62.2293815612793 = 0.016553258523344994 + 10.0 * 6.221282958984375
Epoch 2340, val loss: 1.5198391675949097
Epoch 2350, training loss: 62.20246887207031 = 0.016347547993063927 + 10.0 * 6.218612194061279
Epoch 2350, val loss: 1.5222527980804443
Epoch 2360, training loss: 62.191566467285156 = 0.016140857711434364 + 10.0 * 6.21754264831543
Epoch 2360, val loss: 1.5254827737808228
Epoch 2370, training loss: 62.17873001098633 = 0.015943286940455437 + 10.0 * 6.216279029846191
Epoch 2370, val loss: 1.528259038925171
Epoch 2380, training loss: 62.17939376831055 = 0.015755195170640945 + 10.0 * 6.216363906860352
Epoch 2380, val loss: 1.5312384366989136
Epoch 2390, training loss: 62.209632873535156 = 0.015570257790386677 + 10.0 * 6.2194061279296875
Epoch 2390, val loss: 1.534055233001709
Epoch 2400, training loss: 62.18933868408203 = 0.015371741726994514 + 10.0 * 6.2173967361450195
Epoch 2400, val loss: 1.5358248949050903
Epoch 2410, training loss: 62.18561935424805 = 0.01518645416945219 + 10.0 * 6.217043399810791
Epoch 2410, val loss: 1.5391407012939453
Epoch 2420, training loss: 62.188106536865234 = 0.015004875138401985 + 10.0 * 6.217310428619385
Epoch 2420, val loss: 1.5414369106292725
Epoch 2430, training loss: 62.16653060913086 = 0.014827013947069645 + 10.0 * 6.215170383453369
Epoch 2430, val loss: 1.5440759658813477
Epoch 2440, training loss: 62.1655387878418 = 0.014659362845122814 + 10.0 * 6.215087890625
Epoch 2440, val loss: 1.5468817949295044
Epoch 2450, training loss: 62.187252044677734 = 0.014495106413960457 + 10.0 * 6.217275619506836
Epoch 2450, val loss: 1.549128532409668
Epoch 2460, training loss: 62.17023468017578 = 0.014320489950478077 + 10.0 * 6.2155914306640625
Epoch 2460, val loss: 1.551735281944275
Epoch 2470, training loss: 62.222412109375 = 0.01415451429784298 + 10.0 * 6.220825672149658
Epoch 2470, val loss: 1.5543878078460693
Epoch 2480, training loss: 62.16185760498047 = 0.013978502713143826 + 10.0 * 6.21478796005249
Epoch 2480, val loss: 1.5568122863769531
Epoch 2490, training loss: 62.152435302734375 = 0.013821420259773731 + 10.0 * 6.213861465454102
Epoch 2490, val loss: 1.5590453147888184
Epoch 2500, training loss: 62.14869689941406 = 0.013667862862348557 + 10.0 * 6.213502883911133
Epoch 2500, val loss: 1.5620540380477905
Epoch 2510, training loss: 62.15630340576172 = 0.013519961386919022 + 10.0 * 6.214278221130371
Epoch 2510, val loss: 1.5643640756607056
Epoch 2520, training loss: 62.19038772583008 = 0.01337171345949173 + 10.0 * 6.2177019119262695
Epoch 2520, val loss: 1.5667338371276855
Epoch 2530, training loss: 62.19175720214844 = 0.013218975625932217 + 10.0 * 6.217854022979736
Epoch 2530, val loss: 1.5695490837097168
Epoch 2540, training loss: 62.157752990722656 = 0.013068082742393017 + 10.0 * 6.214468479156494
Epoch 2540, val loss: 1.571670651435852
Epoch 2550, training loss: 62.15778350830078 = 0.0129251629114151 + 10.0 * 6.214486122131348
Epoch 2550, val loss: 1.5741143226623535
Epoch 2560, training loss: 62.16364669799805 = 0.012787643820047379 + 10.0 * 6.215085983276367
Epoch 2560, val loss: 1.5762871503829956
Epoch 2570, training loss: 62.158016204833984 = 0.012646572664380074 + 10.0 * 6.214537143707275
Epoch 2570, val loss: 1.5784393548965454
Epoch 2580, training loss: 62.160160064697266 = 0.012506907805800438 + 10.0 * 6.2147650718688965
Epoch 2580, val loss: 1.5809012651443481
Epoch 2590, training loss: 62.15946578979492 = 0.012372610159218311 + 10.0 * 6.214709281921387
Epoch 2590, val loss: 1.5833698511123657
Epoch 2600, training loss: 62.135929107666016 = 0.01224084384739399 + 10.0 * 6.212368965148926
Epoch 2600, val loss: 1.585760235786438
Epoch 2610, training loss: 62.13605499267578 = 0.012115897610783577 + 10.0 * 6.212393760681152
Epoch 2610, val loss: 1.5884182453155518
Epoch 2620, training loss: 62.166343688964844 = 0.011992252431809902 + 10.0 * 6.215435028076172
Epoch 2620, val loss: 1.5908606052398682
Epoch 2630, training loss: 62.150516510009766 = 0.011859570629894733 + 10.0 * 6.213865756988525
Epoch 2630, val loss: 1.5926449298858643
Epoch 2640, training loss: 62.15022277832031 = 0.011734206229448318 + 10.0 * 6.213849067687988
Epoch 2640, val loss: 1.5947399139404297
Epoch 2650, training loss: 62.150394439697266 = 0.011616608127951622 + 10.0 * 6.2138776779174805
Epoch 2650, val loss: 1.5970778465270996
Epoch 2660, training loss: 62.14682388305664 = 0.011494500562548637 + 10.0 * 6.2135329246521
Epoch 2660, val loss: 1.5990301370620728
Epoch 2670, training loss: 62.131263732910156 = 0.011374682188034058 + 10.0 * 6.211988925933838
Epoch 2670, val loss: 1.6018695831298828
Epoch 2680, training loss: 62.12580108642578 = 0.01125990878790617 + 10.0 * 6.211453914642334
Epoch 2680, val loss: 1.603929042816162
Epoch 2690, training loss: 62.12415313720703 = 0.011147525161504745 + 10.0 * 6.211300849914551
Epoch 2690, val loss: 1.6062283515930176
Epoch 2700, training loss: 62.14986801147461 = 0.011040684767067432 + 10.0 * 6.213882923126221
Epoch 2700, val loss: 1.6084539890289307
Epoch 2710, training loss: 62.13043975830078 = 0.010926468297839165 + 10.0 * 6.21195125579834
Epoch 2710, val loss: 1.61012601852417
Epoch 2720, training loss: 62.141357421875 = 0.010819202288985252 + 10.0 * 6.2130537033081055
Epoch 2720, val loss: 1.6122509241104126
Epoch 2730, training loss: 62.11470031738281 = 0.01070780772715807 + 10.0 * 6.210399150848389
Epoch 2730, val loss: 1.6146055459976196
Epoch 2740, training loss: 62.1266975402832 = 0.010605406947433949 + 10.0 * 6.211609363555908
Epoch 2740, val loss: 1.6167235374450684
Epoch 2750, training loss: 62.157840728759766 = 0.010502157732844353 + 10.0 * 6.214734077453613
Epoch 2750, val loss: 1.6182007789611816
Epoch 2760, training loss: 62.119651794433594 = 0.010398470796644688 + 10.0 * 6.210925102233887
Epoch 2760, val loss: 1.6206601858139038
Epoch 2770, training loss: 62.14463806152344 = 0.010297705419361591 + 10.0 * 6.213434219360352
Epoch 2770, val loss: 1.6223784685134888
Epoch 2780, training loss: 62.11301803588867 = 0.01019641850143671 + 10.0 * 6.210282325744629
Epoch 2780, val loss: 1.6248403787612915
Epoch 2790, training loss: 62.11281967163086 = 0.010098762810230255 + 10.0 * 6.210272312164307
Epoch 2790, val loss: 1.6270724534988403
Epoch 2800, training loss: 62.10940933227539 = 0.010006357915699482 + 10.0 * 6.209940433502197
Epoch 2800, val loss: 1.6289827823638916
Epoch 2810, training loss: 62.13888168334961 = 0.009915449656546116 + 10.0 * 6.212896823883057
Epoch 2810, val loss: 1.631176471710205
Epoch 2820, training loss: 62.099300384521484 = 0.009819026105105877 + 10.0 * 6.208948135375977
Epoch 2820, val loss: 1.6330496072769165
Epoch 2830, training loss: 62.10197067260742 = 0.009729172103106976 + 10.0 * 6.209224224090576
Epoch 2830, val loss: 1.6349118947982788
Epoch 2840, training loss: 62.11874008178711 = 0.009639750234782696 + 10.0 * 6.210909843444824
Epoch 2840, val loss: 1.6368404626846313
Epoch 2850, training loss: 62.11974334716797 = 0.009550487622618675 + 10.0 * 6.211019039154053
Epoch 2850, val loss: 1.6385698318481445
Epoch 2860, training loss: 62.10724639892578 = 0.009464525617659092 + 10.0 * 6.209778308868408
Epoch 2860, val loss: 1.6410167217254639
Epoch 2870, training loss: 62.20606231689453 = 0.009382947348058224 + 10.0 * 6.219667911529541
Epoch 2870, val loss: 1.642516851425171
Epoch 2880, training loss: 62.1359748840332 = 0.009287964552640915 + 10.0 * 6.212668418884277
Epoch 2880, val loss: 1.6441694498062134
Epoch 2890, training loss: 62.09843063354492 = 0.009202909655869007 + 10.0 * 6.208922863006592
Epoch 2890, val loss: 1.646485447883606
Epoch 2900, training loss: 62.08366775512695 = 0.009122206829488277 + 10.0 * 6.207454681396484
Epoch 2900, val loss: 1.648446798324585
Epoch 2910, training loss: 62.08164978027344 = 0.009044398553669453 + 10.0 * 6.207260608673096
Epoch 2910, val loss: 1.650399923324585
Epoch 2920, training loss: 62.094886779785156 = 0.008967896923422813 + 10.0 * 6.208591938018799
Epoch 2920, val loss: 1.652181625366211
Epoch 2930, training loss: 62.13693618774414 = 0.008890016935765743 + 10.0 * 6.212804794311523
Epoch 2930, val loss: 1.6538387537002563
Epoch 2940, training loss: 62.12889862060547 = 0.008809310384094715 + 10.0 * 6.212008953094482
Epoch 2940, val loss: 1.6558012962341309
Epoch 2950, training loss: 62.089874267578125 = 0.008727037347853184 + 10.0 * 6.2081146240234375
Epoch 2950, val loss: 1.657596230506897
Epoch 2960, training loss: 62.08506393432617 = 0.008652457036077976 + 10.0 * 6.207641124725342
Epoch 2960, val loss: 1.6595382690429688
Epoch 2970, training loss: 62.094364166259766 = 0.008583255112171173 + 10.0 * 6.208578109741211
Epoch 2970, val loss: 1.6618136167526245
Epoch 2980, training loss: 62.10941696166992 = 0.008508961647748947 + 10.0 * 6.210090637207031
Epoch 2980, val loss: 1.663469672203064
Epoch 2990, training loss: 62.08272933959961 = 0.008436000905930996 + 10.0 * 6.2074294090271
Epoch 2990, val loss: 1.6644076108932495
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.823405376910912
The final CL Acc:0.71111, 0.02117, The final GNN Acc:0.82305, 0.00090
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13160])
remove edge: torch.Size([2, 7842])
updated graph: torch.Size([2, 10446])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.89981079101562 = 1.9314030408859253 + 10.0 * 8.596840858459473
Epoch 0, val loss: 1.9329605102539062
Epoch 10, training loss: 87.88434600830078 = 1.9219566583633423 + 10.0 * 8.59623908996582
Epoch 10, val loss: 1.9236828088760376
Epoch 20, training loss: 87.8301010131836 = 1.9102715253829956 + 10.0 * 8.5919828414917
Epoch 20, val loss: 1.9121522903442383
Epoch 30, training loss: 87.50997924804688 = 1.8950942754745483 + 10.0 * 8.561488151550293
Epoch 30, val loss: 1.896983027458191
Epoch 40, training loss: 85.39347076416016 = 1.8774747848510742 + 10.0 * 8.35159969329834
Epoch 40, val loss: 1.879630208015442
Epoch 50, training loss: 78.51966857910156 = 1.8583029508590698 + 10.0 * 7.666136741638184
Epoch 50, val loss: 1.860554575920105
Epoch 60, training loss: 75.85075378417969 = 1.841853380203247 + 10.0 * 7.400890350341797
Epoch 60, val loss: 1.8446720838546753
Epoch 70, training loss: 73.6159439086914 = 1.8311184644699097 + 10.0 * 7.178483009338379
Epoch 70, val loss: 1.8340398073196411
Epoch 80, training loss: 72.39228057861328 = 1.81893789768219 + 10.0 * 7.0573344230651855
Epoch 80, val loss: 1.8220428228378296
Epoch 90, training loss: 71.04447174072266 = 1.8073562383651733 + 10.0 * 6.92371129989624
Epoch 90, val loss: 1.811538815498352
Epoch 100, training loss: 70.08392333984375 = 1.7981234788894653 + 10.0 * 6.828579425811768
Epoch 100, val loss: 1.8027011156082153
Epoch 110, training loss: 69.42394256591797 = 1.7884749174118042 + 10.0 * 6.763546943664551
Epoch 110, val loss: 1.793492317199707
Epoch 120, training loss: 68.8829345703125 = 1.7774314880371094 + 10.0 * 6.710549831390381
Epoch 120, val loss: 1.783241868019104
Epoch 130, training loss: 68.35606384277344 = 1.7670339345932007 + 10.0 * 6.658902645111084
Epoch 130, val loss: 1.7737162113189697
Epoch 140, training loss: 67.88627624511719 = 1.757041573524475 + 10.0 * 6.6129231452941895
Epoch 140, val loss: 1.7645649909973145
Epoch 150, training loss: 67.54423522949219 = 1.7460148334503174 + 10.0 * 6.579822063446045
Epoch 150, val loss: 1.7545220851898193
Epoch 160, training loss: 67.24748992919922 = 1.7337636947631836 + 10.0 * 6.551372528076172
Epoch 160, val loss: 1.7435901165008545
Epoch 170, training loss: 66.99838256835938 = 1.7203034162521362 + 10.0 * 6.527807235717773
Epoch 170, val loss: 1.7316631078720093
Epoch 180, training loss: 66.78520202636719 = 1.7056093215942383 + 10.0 * 6.507959365844727
Epoch 180, val loss: 1.7187150716781616
Epoch 190, training loss: 66.61842346191406 = 1.6895689964294434 + 10.0 * 6.492885589599609
Epoch 190, val loss: 1.7047232389450073
Epoch 200, training loss: 66.44024658203125 = 1.6720128059387207 + 10.0 * 6.476823329925537
Epoch 200, val loss: 1.6895639896392822
Epoch 210, training loss: 66.27142333984375 = 1.6529563665390015 + 10.0 * 6.461846828460693
Epoch 210, val loss: 1.6732511520385742
Epoch 220, training loss: 66.15824127197266 = 1.6321924924850464 + 10.0 * 6.452604293823242
Epoch 220, val loss: 1.6555818319320679
Epoch 230, training loss: 66.00953674316406 = 1.6097650527954102 + 10.0 * 6.439977169036865
Epoch 230, val loss: 1.6366164684295654
Epoch 240, training loss: 65.97066497802734 = 1.5857187509536743 + 10.0 * 6.43849515914917
Epoch 240, val loss: 1.616422176361084
Epoch 250, training loss: 65.78173828125 = 1.560153603553772 + 10.0 * 6.422159194946289
Epoch 250, val loss: 1.594935655593872
Epoch 260, training loss: 65.67498779296875 = 1.5333409309387207 + 10.0 * 6.414165019989014
Epoch 260, val loss: 1.5726336240768433
Epoch 270, training loss: 65.56660461425781 = 1.505367398262024 + 10.0 * 6.406123161315918
Epoch 270, val loss: 1.5496034622192383
Epoch 280, training loss: 65.49011993408203 = 1.4764026403427124 + 10.0 * 6.401371479034424
Epoch 280, val loss: 1.5258982181549072
Epoch 290, training loss: 65.38552856445312 = 1.446434736251831 + 10.0 * 6.393909454345703
Epoch 290, val loss: 1.5017399787902832
Epoch 300, training loss: 65.29930877685547 = 1.4160010814666748 + 10.0 * 6.388330459594727
Epoch 300, val loss: 1.477463960647583
Epoch 310, training loss: 65.20809173583984 = 1.3851746320724487 + 10.0 * 6.382291793823242
Epoch 310, val loss: 1.45327889919281
Epoch 320, training loss: 65.17365264892578 = 1.3540804386138916 + 10.0 * 6.381957054138184
Epoch 320, val loss: 1.4292237758636475
Epoch 330, training loss: 65.05415344238281 = 1.323038101196289 + 10.0 * 6.373111724853516
Epoch 330, val loss: 1.4055860042572021
Epoch 340, training loss: 64.97740936279297 = 1.2920809984207153 + 10.0 * 6.368532657623291
Epoch 340, val loss: 1.3822407722473145
Epoch 350, training loss: 64.89730072021484 = 1.2613426446914673 + 10.0 * 6.363595485687256
Epoch 350, val loss: 1.3592997789382935
Epoch 360, training loss: 64.8697280883789 = 1.230806589126587 + 10.0 * 6.363892555236816
Epoch 360, val loss: 1.3367339372634888
Epoch 370, training loss: 64.76698303222656 = 1.200637936592102 + 10.0 * 6.356634140014648
Epoch 370, val loss: 1.3145887851715088
Epoch 380, training loss: 64.69080352783203 = 1.1709082126617432 + 10.0 * 6.351989269256592
Epoch 380, val loss: 1.2929710149765015
Epoch 390, training loss: 64.6219482421875 = 1.1416124105453491 + 10.0 * 6.348033905029297
Epoch 390, val loss: 1.271777629852295
Epoch 400, training loss: 64.57644653320312 = 1.1126673221588135 + 10.0 * 6.346377849578857
Epoch 400, val loss: 1.2508647441864014
Epoch 410, training loss: 64.50102233886719 = 1.084214448928833 + 10.0 * 6.341680526733398
Epoch 410, val loss: 1.230331301689148
Epoch 420, training loss: 64.43709564208984 = 1.0562633275985718 + 10.0 * 6.338083267211914
Epoch 420, val loss: 1.210296869277954
Epoch 430, training loss: 64.37647247314453 = 1.028828740119934 + 10.0 * 6.33476448059082
Epoch 430, val loss: 1.1907345056533813
Epoch 440, training loss: 64.32759857177734 = 1.0018527507781982 + 10.0 * 6.332574844360352
Epoch 440, val loss: 1.1716282367706299
Epoch 450, training loss: 64.30223846435547 = 0.9752829074859619 + 10.0 * 6.332695484161377
Epoch 450, val loss: 1.1526997089385986
Epoch 460, training loss: 64.22565460205078 = 0.9493012428283691 + 10.0 * 6.327635288238525
Epoch 460, val loss: 1.134408950805664
Epoch 470, training loss: 64.1653823852539 = 0.9239473938941956 + 10.0 * 6.324143409729004
Epoch 470, val loss: 1.1168208122253418
Epoch 480, training loss: 64.21845245361328 = 0.8992591500282288 + 10.0 * 6.331919193267822
Epoch 480, val loss: 1.0998846292495728
Epoch 490, training loss: 64.07984161376953 = 0.8749085664749146 + 10.0 * 6.320493221282959
Epoch 490, val loss: 1.083301067352295
Epoch 500, training loss: 64.02069091796875 = 0.8513249158859253 + 10.0 * 6.31693696975708
Epoch 500, val loss: 1.0674453973770142
Epoch 510, training loss: 64.00183868408203 = 0.8283889293670654 + 10.0 * 6.317344665527344
Epoch 510, val loss: 1.0523098707199097
Epoch 520, training loss: 63.96096420288086 = 0.8060591220855713 + 10.0 * 6.315490245819092
Epoch 520, val loss: 1.037692904472351
Epoch 530, training loss: 63.887271881103516 = 0.7844303250312805 + 10.0 * 6.31028413772583
Epoch 530, val loss: 1.0239554643630981
Epoch 540, training loss: 63.84794616699219 = 0.7634264826774597 + 10.0 * 6.3084516525268555
Epoch 540, val loss: 1.0109871625900269
Epoch 550, training loss: 63.82245635986328 = 0.7429987192153931 + 10.0 * 6.307945728302002
Epoch 550, val loss: 0.9987422227859497
Epoch 560, training loss: 63.771507263183594 = 0.7230836749076843 + 10.0 * 6.304842472076416
Epoch 560, val loss: 0.986994743347168
Epoch 570, training loss: 63.75725555419922 = 0.7036613821983337 + 10.0 * 6.305359363555908
Epoch 570, val loss: 0.9760034680366516
Epoch 580, training loss: 63.69752502441406 = 0.6849099397659302 + 10.0 * 6.3012614250183105
Epoch 580, val loss: 0.9657302498817444
Epoch 590, training loss: 63.6612434387207 = 0.6665574312210083 + 10.0 * 6.299468517303467
Epoch 590, val loss: 0.9560249447822571
Epoch 600, training loss: 63.646427154541016 = 0.6486796736717224 + 10.0 * 6.299775123596191
Epoch 600, val loss: 0.9468448758125305
Epoch 610, training loss: 63.63903045654297 = 0.6310886740684509 + 10.0 * 6.3007941246032715
Epoch 610, val loss: 0.9380869269371033
Epoch 620, training loss: 63.556461334228516 = 0.6140096783638 + 10.0 * 6.29424524307251
Epoch 620, val loss: 0.9299646615982056
Epoch 630, training loss: 63.52861404418945 = 0.5973582863807678 + 10.0 * 6.293125629425049
Epoch 630, val loss: 0.9223911762237549
Epoch 640, training loss: 63.5003776550293 = 0.5810042023658752 + 10.0 * 6.291937351226807
Epoch 640, val loss: 0.9152161478996277
Epoch 650, training loss: 63.4672966003418 = 0.5649619102478027 + 10.0 * 6.290233612060547
Epoch 650, val loss: 0.9083907604217529
Epoch 660, training loss: 63.44208908081055 = 0.5491834878921509 + 10.0 * 6.289290428161621
Epoch 660, val loss: 0.9019750356674194
Epoch 670, training loss: 63.410518646240234 = 0.5338467955589294 + 10.0 * 6.287667274475098
Epoch 670, val loss: 0.8960446119308472
Epoch 680, training loss: 63.396690368652344 = 0.5188031196594238 + 10.0 * 6.2877888679504395
Epoch 680, val loss: 0.8904788494110107
Epoch 690, training loss: 63.34986114501953 = 0.5039661526679993 + 10.0 * 6.2845892906188965
Epoch 690, val loss: 0.8849502801895142
Epoch 700, training loss: 63.379127502441406 = 0.4894145727157593 + 10.0 * 6.288971424102783
Epoch 700, val loss: 0.8798112273216248
Epoch 710, training loss: 63.31065368652344 = 0.4750889241695404 + 10.0 * 6.2835564613342285
Epoch 710, val loss: 0.8751750588417053
Epoch 720, training loss: 63.27214431762695 = 0.4611510634422302 + 10.0 * 6.281099319458008
Epoch 720, val loss: 0.8706445693969727
Epoch 730, training loss: 63.248958587646484 = 0.4474448263645172 + 10.0 * 6.2801513671875
Epoch 730, val loss: 0.8665681481361389
Epoch 740, training loss: 63.26469421386719 = 0.4339660108089447 + 10.0 * 6.283072471618652
Epoch 740, val loss: 0.8626701831817627
Epoch 750, training loss: 63.19554901123047 = 0.42077329754829407 + 10.0 * 6.277477741241455
Epoch 750, val loss: 0.8590429425239563
Epoch 760, training loss: 63.17498016357422 = 0.40786057710647583 + 10.0 * 6.276711940765381
Epoch 760, val loss: 0.8558014035224915
Epoch 770, training loss: 63.18320083618164 = 0.3952091634273529 + 10.0 * 6.278799057006836
Epoch 770, val loss: 0.85298091173172
Epoch 780, training loss: 63.14760208129883 = 0.38282009959220886 + 10.0 * 6.276478290557861
Epoch 780, val loss: 0.8500165939331055
Epoch 790, training loss: 63.12279510498047 = 0.37064269185066223 + 10.0 * 6.275215148925781
Epoch 790, val loss: 0.8477799296379089
Epoch 800, training loss: 63.08869171142578 = 0.35887694358825684 + 10.0 * 6.272981643676758
Epoch 800, val loss: 0.8457574248313904
Epoch 810, training loss: 63.096012115478516 = 0.34740856289863586 + 10.0 * 6.274860382080078
Epoch 810, val loss: 0.844117283821106
Epoch 820, training loss: 63.0450439453125 = 0.33613675832748413 + 10.0 * 6.270890712738037
Epoch 820, val loss: 0.842612624168396
Epoch 830, training loss: 63.03172302246094 = 0.32521936297416687 + 10.0 * 6.270650386810303
Epoch 830, val loss: 0.8414170145988464
Epoch 840, training loss: 63.03867721557617 = 0.3145849406719208 + 10.0 * 6.272408962249756
Epoch 840, val loss: 0.8405570983886719
Epoch 850, training loss: 62.99380874633789 = 0.30423346161842346 + 10.0 * 6.268957614898682
Epoch 850, val loss: 0.8398628830909729
Epoch 860, training loss: 62.976585388183594 = 0.29420024156570435 + 10.0 * 6.268238544464111
Epoch 860, val loss: 0.8395261764526367
Epoch 870, training loss: 62.955055236816406 = 0.2845208942890167 + 10.0 * 6.267053604125977
Epoch 870, val loss: 0.8393641114234924
Epoch 880, training loss: 62.94736862182617 = 0.27507612109184265 + 10.0 * 6.267229080200195
Epoch 880, val loss: 0.8395959138870239
Epoch 890, training loss: 62.940181732177734 = 0.2659038007259369 + 10.0 * 6.267427921295166
Epoch 890, val loss: 0.8398343324661255
Epoch 900, training loss: 62.92137145996094 = 0.2569867670536041 + 10.0 * 6.2664384841918945
Epoch 900, val loss: 0.8402819037437439
Epoch 910, training loss: 62.89314270019531 = 0.24836060404777527 + 10.0 * 6.2644782066345215
Epoch 910, val loss: 0.8409605026245117
Epoch 920, training loss: 62.87087631225586 = 0.24006125330924988 + 10.0 * 6.2630815505981445
Epoch 920, val loss: 0.8419663906097412
Epoch 930, training loss: 62.85273361206055 = 0.2320542186498642 + 10.0 * 6.262067794799805
Epoch 930, val loss: 0.8430685997009277
Epoch 940, training loss: 62.88153076171875 = 0.22432011365890503 + 10.0 * 6.265721321105957
Epoch 940, val loss: 0.8444086313247681
Epoch 950, training loss: 62.83407974243164 = 0.21676146984100342 + 10.0 * 6.2617316246032715
Epoch 950, val loss: 0.8456964492797852
Epoch 960, training loss: 62.833900451660156 = 0.2095194011926651 + 10.0 * 6.2624382972717285
Epoch 960, val loss: 0.8473691344261169
Epoch 970, training loss: 62.84381866455078 = 0.20253808796405792 + 10.0 * 6.2641282081604
Epoch 970, val loss: 0.8489950299263
Epoch 980, training loss: 62.7943000793457 = 0.19572652876377106 + 10.0 * 6.259857177734375
Epoch 980, val loss: 0.8508468270301819
Epoch 990, training loss: 62.766639709472656 = 0.18928268551826477 + 10.0 * 6.257735729217529
Epoch 990, val loss: 0.8528082966804504
Epoch 1000, training loss: 62.75230026245117 = 0.1830938458442688 + 10.0 * 6.25692081451416
Epoch 1000, val loss: 0.8549500703811646
Epoch 1010, training loss: 62.74159622192383 = 0.17713119089603424 + 10.0 * 6.256446361541748
Epoch 1010, val loss: 0.857231080532074
Epoch 1020, training loss: 62.781837463378906 = 0.17140184342861176 + 10.0 * 6.261043548583984
Epoch 1020, val loss: 0.8594127297401428
Epoch 1030, training loss: 62.7501335144043 = 0.16575968265533447 + 10.0 * 6.258437156677246
Epoch 1030, val loss: 0.8619258999824524
Epoch 1040, training loss: 62.70906448364258 = 0.16039685904979706 + 10.0 * 6.254866600036621
Epoch 1040, val loss: 0.8644757270812988
Epoch 1050, training loss: 62.719261169433594 = 0.15524853765964508 + 10.0 * 6.256401538848877
Epoch 1050, val loss: 0.8670792579650879
Epoch 1060, training loss: 62.687156677246094 = 0.1502613127231598 + 10.0 * 6.253689765930176
Epoch 1060, val loss: 0.8695493340492249
Epoch 1070, training loss: 62.671024322509766 = 0.14549671113491058 + 10.0 * 6.2525529861450195
Epoch 1070, val loss: 0.8722963929176331
Epoch 1080, training loss: 62.657310485839844 = 0.14090071618556976 + 10.0 * 6.251641273498535
Epoch 1080, val loss: 0.8751354813575745
Epoch 1090, training loss: 62.67079544067383 = 0.13649724423885345 + 10.0 * 6.253429889678955
Epoch 1090, val loss: 0.8781449794769287
Epoch 1100, training loss: 62.65268325805664 = 0.13221576809883118 + 10.0 * 6.252046585083008
Epoch 1100, val loss: 0.8807891011238098
Epoch 1110, training loss: 62.672088623046875 = 0.1280858963727951 + 10.0 * 6.254400253295898
Epoch 1110, val loss: 0.8839909434318542
Epoch 1120, training loss: 62.61885452270508 = 0.12410532683134079 + 10.0 * 6.249475002288818
Epoch 1120, val loss: 0.886772632598877
Epoch 1130, training loss: 62.615142822265625 = 0.12029355019330978 + 10.0 * 6.249485015869141
Epoch 1130, val loss: 0.8897472620010376
Epoch 1140, training loss: 62.59893035888672 = 0.11665420979261398 + 10.0 * 6.248227596282959
Epoch 1140, val loss: 0.8929360508918762
Epoch 1150, training loss: 62.60254669189453 = 0.11315315216779709 + 10.0 * 6.248939514160156
Epoch 1150, val loss: 0.8962433934211731
Epoch 1160, training loss: 62.58826446533203 = 0.10972316563129425 + 10.0 * 6.247854232788086
Epoch 1160, val loss: 0.8993245959281921
Epoch 1170, training loss: 62.59965515136719 = 0.10643060505390167 + 10.0 * 6.249322414398193
Epoch 1170, val loss: 0.9023799896240234
Epoch 1180, training loss: 62.56107711791992 = 0.10328961908817291 + 10.0 * 6.245778560638428
Epoch 1180, val loss: 0.9057961702346802
Epoch 1190, training loss: 62.55689239501953 = 0.10028301924467087 + 10.0 * 6.245660781860352
Epoch 1190, val loss: 0.9091963768005371
Epoch 1200, training loss: 62.55659103393555 = 0.09736863523721695 + 10.0 * 6.245922088623047
Epoch 1200, val loss: 0.9127671718597412
Epoch 1210, training loss: 62.58521270751953 = 0.09453002363443375 + 10.0 * 6.249068260192871
Epoch 1210, val loss: 0.9160326719284058
Epoch 1220, training loss: 62.5518798828125 = 0.09176726639270782 + 10.0 * 6.246011257171631
Epoch 1220, val loss: 0.9194150567054749
Epoch 1230, training loss: 62.5323600769043 = 0.08914542943239212 + 10.0 * 6.244321346282959
Epoch 1230, val loss: 0.9229453206062317
Epoch 1240, training loss: 62.5162467956543 = 0.08662910014390945 + 10.0 * 6.242961883544922
Epoch 1240, val loss: 0.9266541004180908
Epoch 1250, training loss: 62.525428771972656 = 0.08420703560113907 + 10.0 * 6.244122505187988
Epoch 1250, val loss: 0.9302935600280762
Epoch 1260, training loss: 62.514915466308594 = 0.08183307945728302 + 10.0 * 6.243308067321777
Epoch 1260, val loss: 0.9338242411613464
Epoch 1270, training loss: 62.499603271484375 = 0.07954274117946625 + 10.0 * 6.242005825042725
Epoch 1270, val loss: 0.9373196363449097
Epoch 1280, training loss: 62.494808197021484 = 0.07736127078533173 + 10.0 * 6.241744518280029
Epoch 1280, val loss: 0.9410697221755981
Epoch 1290, training loss: 62.51155090332031 = 0.0752539187669754 + 10.0 * 6.2436299324035645
Epoch 1290, val loss: 0.9449217915534973
Epoch 1300, training loss: 62.47712707519531 = 0.07320120930671692 + 10.0 * 6.240392684936523
Epoch 1300, val loss: 0.9485871195793152
Epoch 1310, training loss: 62.46770477294922 = 0.07121099531650543 + 10.0 * 6.239649295806885
Epoch 1310, val loss: 0.9523372054100037
Epoch 1320, training loss: 62.46149444580078 = 0.06930722296237946 + 10.0 * 6.239218711853027
Epoch 1320, val loss: 0.9561444520950317
Epoch 1330, training loss: 62.47419738769531 = 0.06747189909219742 + 10.0 * 6.240672588348389
Epoch 1330, val loss: 0.9600129127502441
Epoch 1340, training loss: 62.45781326293945 = 0.06565608084201813 + 10.0 * 6.239215850830078
Epoch 1340, val loss: 0.963717520236969
Epoch 1350, training loss: 62.44367980957031 = 0.06390377879142761 + 10.0 * 6.237977504730225
Epoch 1350, val loss: 0.9674674868583679
Epoch 1360, training loss: 62.450435638427734 = 0.06223038211464882 + 10.0 * 6.238820552825928
Epoch 1360, val loss: 0.971280038356781
Epoch 1370, training loss: 62.4532585144043 = 0.0606175996363163 + 10.0 * 6.239264011383057
Epoch 1370, val loss: 0.9752103686332703
Epoch 1380, training loss: 62.427303314208984 = 0.05903841182589531 + 10.0 * 6.236826419830322
Epoch 1380, val loss: 0.9788541197776794
Epoch 1390, training loss: 62.42469024658203 = 0.057534679770469666 + 10.0 * 6.236715793609619
Epoch 1390, val loss: 0.9828845262527466
Epoch 1400, training loss: 62.4544792175293 = 0.05609862878918648 + 10.0 * 6.239838123321533
Epoch 1400, val loss: 0.986668586730957
Epoch 1410, training loss: 62.41740798950195 = 0.05465983599424362 + 10.0 * 6.236274719238281
Epoch 1410, val loss: 0.9907321333885193
Epoch 1420, training loss: 62.42273712158203 = 0.053284790366888046 + 10.0 * 6.236945152282715
Epoch 1420, val loss: 0.9943910837173462
Epoch 1430, training loss: 62.4063835144043 = 0.051959387958049774 + 10.0 * 6.235442161560059
Epoch 1430, val loss: 0.9984270930290222
Epoch 1440, training loss: 62.406551361083984 = 0.05070275440812111 + 10.0 * 6.2355852127075195
Epoch 1440, val loss: 1.002297282218933
Epoch 1450, training loss: 62.42095184326172 = 0.04946332424879074 + 10.0 * 6.237148761749268
Epoch 1450, val loss: 1.0062484741210938
Epoch 1460, training loss: 62.412330627441406 = 0.04827296361327171 + 10.0 * 6.236405849456787
Epoch 1460, val loss: 1.010006308555603
Epoch 1470, training loss: 62.404544830322266 = 0.04710500314831734 + 10.0 * 6.235743999481201
Epoch 1470, val loss: 1.0139888525009155
Epoch 1480, training loss: 62.398956298828125 = 0.04598419740796089 + 10.0 * 6.235297203063965
Epoch 1480, val loss: 1.0178614854812622
Epoch 1490, training loss: 62.38831329345703 = 0.0448891781270504 + 10.0 * 6.234342575073242
Epoch 1490, val loss: 1.0215439796447754
Epoch 1500, training loss: 62.372039794921875 = 0.043837837874889374 + 10.0 * 6.2328200340271
Epoch 1500, val loss: 1.0255167484283447
Epoch 1510, training loss: 62.363677978515625 = 0.04282037168741226 + 10.0 * 6.232085704803467
Epoch 1510, val loss: 1.0294164419174194
Epoch 1520, training loss: 62.37302017211914 = 0.04185187444090843 + 10.0 * 6.23311710357666
Epoch 1520, val loss: 1.0333086252212524
Epoch 1530, training loss: 62.37570571899414 = 0.04088597744703293 + 10.0 * 6.2334818840026855
Epoch 1530, val loss: 1.037086844444275
Epoch 1540, training loss: 62.36084747314453 = 0.03994723781943321 + 10.0 * 6.232089996337891
Epoch 1540, val loss: 1.0407336950302124
Epoch 1550, training loss: 62.34050369262695 = 0.03903913497924805 + 10.0 * 6.230146408081055
Epoch 1550, val loss: 1.04464590549469
Epoch 1560, training loss: 62.33773422241211 = 0.03818036615848541 + 10.0 * 6.229955196380615
Epoch 1560, val loss: 1.0485388040542603
Epoch 1570, training loss: 62.37107467651367 = 0.03734012320637703 + 10.0 * 6.233373641967773
Epoch 1570, val loss: 1.0524513721466064
Epoch 1580, training loss: 62.342384338378906 = 0.036513883620500565 + 10.0 * 6.230587005615234
Epoch 1580, val loss: 1.0560981035232544
Epoch 1590, training loss: 62.3486328125 = 0.03571077063679695 + 10.0 * 6.231292247772217
Epoch 1590, val loss: 1.0598080158233643
Epoch 1600, training loss: 62.33247375488281 = 0.03494160249829292 + 10.0 * 6.229753017425537
Epoch 1600, val loss: 1.063578724861145
Epoch 1610, training loss: 62.31998825073242 = 0.03419050574302673 + 10.0 * 6.228579521179199
Epoch 1610, val loss: 1.067352294921875
Epoch 1620, training loss: 62.32017135620117 = 0.0334748812019825 + 10.0 * 6.2286696434021
Epoch 1620, val loss: 1.0711640119552612
Epoch 1630, training loss: 62.325645446777344 = 0.03277678042650223 + 10.0 * 6.229287147521973
Epoch 1630, val loss: 1.0747969150543213
Epoch 1640, training loss: 62.33003234863281 = 0.032094717025756836 + 10.0 * 6.229794025421143
Epoch 1640, val loss: 1.0786199569702148
Epoch 1650, training loss: 62.319759368896484 = 0.031417474150657654 + 10.0 * 6.22883415222168
Epoch 1650, val loss: 1.0823642015457153
Epoch 1660, training loss: 62.30801010131836 = 0.030771326273679733 + 10.0 * 6.227723598480225
Epoch 1660, val loss: 1.0859344005584717
Epoch 1670, training loss: 62.308204650878906 = 0.030145684257149696 + 10.0 * 6.2278056144714355
Epoch 1670, val loss: 1.0894948244094849
Epoch 1680, training loss: 62.29080581665039 = 0.02953629568219185 + 10.0 * 6.226126670837402
Epoch 1680, val loss: 1.0933226346969604
Epoch 1690, training loss: 62.290931701660156 = 0.028950948268175125 + 10.0 * 6.226198196411133
Epoch 1690, val loss: 1.0969107151031494
Epoch 1700, training loss: 62.320960998535156 = 0.028387462720274925 + 10.0 * 6.229257106781006
Epoch 1700, val loss: 1.1005980968475342
Epoch 1710, training loss: 62.2963752746582 = 0.027819816023111343 + 10.0 * 6.226855278015137
Epoch 1710, val loss: 1.103835940361023
Epoch 1720, training loss: 62.309730529785156 = 0.02727646939456463 + 10.0 * 6.228245735168457
Epoch 1720, val loss: 1.107611060142517
Epoch 1730, training loss: 62.280372619628906 = 0.026745997369289398 + 10.0 * 6.225362777709961
Epoch 1730, val loss: 1.110991358757019
Epoch 1740, training loss: 62.27731704711914 = 0.02623506262898445 + 10.0 * 6.2251081466674805
Epoch 1740, val loss: 1.1147501468658447
Epoch 1750, training loss: 62.28208923339844 = 0.02573821321129799 + 10.0 * 6.225635051727295
Epoch 1750, val loss: 1.1181505918502808
Epoch 1760, training loss: 62.290042877197266 = 0.02525540255010128 + 10.0 * 6.226478576660156
Epoch 1760, val loss: 1.121764063835144
Epoch 1770, training loss: 62.28223419189453 = 0.024774275720119476 + 10.0 * 6.225746154785156
Epoch 1770, val loss: 1.1250501871109009
Epoch 1780, training loss: 62.282676696777344 = 0.024315331131219864 + 10.0 * 6.225836277008057
Epoch 1780, val loss: 1.1287050247192383
Epoch 1790, training loss: 62.260196685791016 = 0.02386331558227539 + 10.0 * 6.223633289337158
Epoch 1790, val loss: 1.132041096687317
Epoch 1800, training loss: 62.247188568115234 = 0.023435013368725777 + 10.0 * 6.222375392913818
Epoch 1800, val loss: 1.135380744934082
Epoch 1810, training loss: 62.24946594238281 = 0.023018140345811844 + 10.0 * 6.222644805908203
Epoch 1810, val loss: 1.1388567686080933
Epoch 1820, training loss: 62.3241081237793 = 0.022611254826188087 + 10.0 * 6.230149745941162
Epoch 1820, val loss: 1.142128825187683
Epoch 1830, training loss: 62.287200927734375 = 0.022191176190972328 + 10.0 * 6.226500988006592
Epoch 1830, val loss: 1.1454336643218994
Epoch 1840, training loss: 62.25776290893555 = 0.02179616130888462 + 10.0 * 6.223596572875977
Epoch 1840, val loss: 1.1487740278244019
Epoch 1850, training loss: 62.25896453857422 = 0.021410878747701645 + 10.0 * 6.223755359649658
Epoch 1850, val loss: 1.1522130966186523
Epoch 1860, training loss: 62.23381423950195 = 0.02104194462299347 + 10.0 * 6.221277236938477
Epoch 1860, val loss: 1.1555678844451904
Epoch 1870, training loss: 62.23173904418945 = 0.020684612914919853 + 10.0 * 6.221105575561523
Epoch 1870, val loss: 1.158542275428772
Epoch 1880, training loss: 62.23125076293945 = 0.02033393457531929 + 10.0 * 6.2210917472839355
Epoch 1880, val loss: 1.1619319915771484
Epoch 1890, training loss: 62.27980041503906 = 0.019989514723420143 + 10.0 * 6.22598123550415
Epoch 1890, val loss: 1.1652711629867554
Epoch 1900, training loss: 62.2435188293457 = 0.0196425449103117 + 10.0 * 6.222387790679932
Epoch 1900, val loss: 1.1680933237075806
Epoch 1910, training loss: 62.22837448120117 = 0.019312122836709023 + 10.0 * 6.2209062576293945
Epoch 1910, val loss: 1.171511173248291
Epoch 1920, training loss: 62.22173309326172 = 0.01899123191833496 + 10.0 * 6.220274448394775
Epoch 1920, val loss: 1.1743437051773071
Epoch 1930, training loss: 62.23054504394531 = 0.018685488030314445 + 10.0 * 6.22118616104126
Epoch 1930, val loss: 1.1777675151824951
Epoch 1940, training loss: 62.21381759643555 = 0.018380802124738693 + 10.0 * 6.219543933868408
Epoch 1940, val loss: 1.1807236671447754
Epoch 1950, training loss: 62.22893524169922 = 0.0180838443338871 + 10.0 * 6.221085071563721
Epoch 1950, val loss: 1.183862566947937
Epoch 1960, training loss: 62.20271301269531 = 0.017789172008633614 + 10.0 * 6.21849250793457
Epoch 1960, val loss: 1.1869460344314575
Epoch 1970, training loss: 62.217559814453125 = 0.01750933565199375 + 10.0 * 6.220005035400391
Epoch 1970, val loss: 1.1898894309997559
Epoch 1980, training loss: 62.27957534790039 = 0.017236776649951935 + 10.0 * 6.22623348236084
Epoch 1980, val loss: 1.192855715751648
Epoch 1990, training loss: 62.199005126953125 = 0.01694663241505623 + 10.0 * 6.21820592880249
Epoch 1990, val loss: 1.1960457563400269
Epoch 2000, training loss: 62.196781158447266 = 0.01668311469256878 + 10.0 * 6.218009948730469
Epoch 2000, val loss: 1.1986427307128906
Epoch 2010, training loss: 62.18514633178711 = 0.01643134467303753 + 10.0 * 6.216871738433838
Epoch 2010, val loss: 1.2018500566482544
Epoch 2020, training loss: 62.18118667602539 = 0.016184911131858826 + 10.0 * 6.216500282287598
Epoch 2020, val loss: 1.2048462629318237
Epoch 2030, training loss: 62.20903778076172 = 0.015945401042699814 + 10.0 * 6.219309329986572
Epoch 2030, val loss: 1.2077267169952393
Epoch 2040, training loss: 62.19447708129883 = 0.015697432681918144 + 10.0 * 6.2178778648376465
Epoch 2040, val loss: 1.2105789184570312
Epoch 2050, training loss: 62.19570541381836 = 0.015453299507498741 + 10.0 * 6.218025207519531
Epoch 2050, val loss: 1.2133020162582397
Epoch 2060, training loss: 62.1901741027832 = 0.015223830938339233 + 10.0 * 6.217494964599609
Epoch 2060, val loss: 1.2161352634429932
Epoch 2070, training loss: 62.19270706176758 = 0.01500268466770649 + 10.0 * 6.217770576477051
Epoch 2070, val loss: 1.2190539836883545
Epoch 2080, training loss: 62.17103958129883 = 0.014783983118832111 + 10.0 * 6.215625286102295
Epoch 2080, val loss: 1.2219220399856567
Epoch 2090, training loss: 62.193241119384766 = 0.014575010165572166 + 10.0 * 6.21786642074585
Epoch 2090, val loss: 1.2246544361114502
Epoch 2100, training loss: 62.17061233520508 = 0.014360055327415466 + 10.0 * 6.215625286102295
Epoch 2100, val loss: 1.2272981405258179
Epoch 2110, training loss: 62.167076110839844 = 0.014152823016047478 + 10.0 * 6.215292453765869
Epoch 2110, val loss: 1.2298871278762817
Epoch 2120, training loss: 62.1762809753418 = 0.013953578658401966 + 10.0 * 6.216232776641846
Epoch 2120, val loss: 1.2326226234436035
Epoch 2130, training loss: 62.21480941772461 = 0.013759802095592022 + 10.0 * 6.220105171203613
Epoch 2130, val loss: 1.2350636720657349
Epoch 2140, training loss: 62.17586135864258 = 0.013561701402068138 + 10.0 * 6.2162299156188965
Epoch 2140, val loss: 1.2375954389572144
Epoch 2150, training loss: 62.160709381103516 = 0.013369831256568432 + 10.0 * 6.214734077453613
Epoch 2150, val loss: 1.2403596639633179
Epoch 2160, training loss: 62.15052032470703 = 0.013191591948270798 + 10.0 * 6.213732719421387
Epoch 2160, val loss: 1.2430410385131836
Epoch 2170, training loss: 62.15748977661133 = 0.013017622753977776 + 10.0 * 6.214447498321533
Epoch 2170, val loss: 1.2457493543624878
Epoch 2180, training loss: 62.19076156616211 = 0.01284133642911911 + 10.0 * 6.21779203414917
Epoch 2180, val loss: 1.248562216758728
Epoch 2190, training loss: 62.18013000488281 = 0.012660875916481018 + 10.0 * 6.216746807098389
Epoch 2190, val loss: 1.250523567199707
Epoch 2200, training loss: 62.1536750793457 = 0.012492801062762737 + 10.0 * 6.214118003845215
Epoch 2200, val loss: 1.2532023191452026
Epoch 2210, training loss: 62.147987365722656 = 0.012326978147029877 + 10.0 * 6.213566303253174
Epoch 2210, val loss: 1.2555785179138184
Epoch 2220, training loss: 62.194358825683594 = 0.012169165536761284 + 10.0 * 6.218218803405762
Epoch 2220, val loss: 1.2583671808242798
Epoch 2230, training loss: 62.1455078125 = 0.012001226656138897 + 10.0 * 6.213350772857666
Epoch 2230, val loss: 1.2603360414505005
Epoch 2240, training loss: 62.146610260009766 = 0.011848014779388905 + 10.0 * 6.213476181030273
Epoch 2240, val loss: 1.2629430294036865
Epoch 2250, training loss: 62.164459228515625 = 0.011694916523993015 + 10.0 * 6.21527624130249
Epoch 2250, val loss: 1.2653440237045288
Epoch 2260, training loss: 62.15082550048828 = 0.01154361106455326 + 10.0 * 6.21392822265625
Epoch 2260, val loss: 1.267770767211914
Epoch 2270, training loss: 62.1298713684082 = 0.011394881643354893 + 10.0 * 6.21184778213501
Epoch 2270, val loss: 1.269877314567566
Epoch 2280, training loss: 62.137203216552734 = 0.011253365315496922 + 10.0 * 6.212594985961914
Epoch 2280, val loss: 1.2723960876464844
Epoch 2290, training loss: 62.143707275390625 = 0.011114721186459064 + 10.0 * 6.213259220123291
Epoch 2290, val loss: 1.2744706869125366
Epoch 2300, training loss: 62.1505241394043 = 0.010975797660648823 + 10.0 * 6.213954925537109
Epoch 2300, val loss: 1.2771943807601929
Epoch 2310, training loss: 62.127403259277344 = 0.010836141183972359 + 10.0 * 6.21165657043457
Epoch 2310, val loss: 1.2793360948562622
Epoch 2320, training loss: 62.124088287353516 = 0.010700339451432228 + 10.0 * 6.211338996887207
Epoch 2320, val loss: 1.281641960144043
Epoch 2330, training loss: 62.12815475463867 = 0.010573201812803745 + 10.0 * 6.211758136749268
Epoch 2330, val loss: 1.2840330600738525
Epoch 2340, training loss: 62.16330337524414 = 0.010445748455822468 + 10.0 * 6.215285778045654
Epoch 2340, val loss: 1.286271333694458
Epoch 2350, training loss: 62.14047622680664 = 0.010314133949577808 + 10.0 * 6.213016033172607
Epoch 2350, val loss: 1.2882214784622192
Epoch 2360, training loss: 62.11387634277344 = 0.010186492465436459 + 10.0 * 6.210369110107422
Epoch 2360, val loss: 1.2906854152679443
Epoch 2370, training loss: 62.105926513671875 = 0.010069668292999268 + 10.0 * 6.209585666656494
Epoch 2370, val loss: 1.2927097082138062
Epoch 2380, training loss: 62.10476303100586 = 0.009952053427696228 + 10.0 * 6.209481239318848
Epoch 2380, val loss: 1.2950973510742188
Epoch 2390, training loss: 62.171531677246094 = 0.009837780147790909 + 10.0 * 6.216169357299805
Epoch 2390, val loss: 1.2974814176559448
Epoch 2400, training loss: 62.148277282714844 = 0.00972000788897276 + 10.0 * 6.213855743408203
Epoch 2400, val loss: 1.299088716506958
Epoch 2410, training loss: 62.117610931396484 = 0.00960039533674717 + 10.0 * 6.210801124572754
Epoch 2410, val loss: 1.3013317584991455
Epoch 2420, training loss: 62.14110565185547 = 0.0094903614372015 + 10.0 * 6.213161468505859
Epoch 2420, val loss: 1.3038129806518555
Epoch 2430, training loss: 62.10610580444336 = 0.009379502385854721 + 10.0 * 6.209672451019287
Epoch 2430, val loss: 1.3053538799285889
Epoch 2440, training loss: 62.104270935058594 = 0.009273755364120007 + 10.0 * 6.209499835968018
Epoch 2440, val loss: 1.3078293800354004
Epoch 2450, training loss: 62.095699310302734 = 0.00916864164173603 + 10.0 * 6.208652973175049
Epoch 2450, val loss: 1.3098225593566895
Epoch 2460, training loss: 62.10910415649414 = 0.009068998508155346 + 10.0 * 6.210003852844238
Epoch 2460, val loss: 1.3120267391204834
Epoch 2470, training loss: 62.120967864990234 = 0.008967502042651176 + 10.0 * 6.211199760437012
Epoch 2470, val loss: 1.3138830661773682
Epoch 2480, training loss: 62.11030578613281 = 0.00886558834463358 + 10.0 * 6.21014404296875
Epoch 2480, val loss: 1.3157700300216675
Epoch 2490, training loss: 62.09637451171875 = 0.008767569437623024 + 10.0 * 6.208760738372803
Epoch 2490, val loss: 1.3179643154144287
Epoch 2500, training loss: 62.10499954223633 = 0.008672659285366535 + 10.0 * 6.209632873535156
Epoch 2500, val loss: 1.3197965621948242
Epoch 2510, training loss: 62.082862854003906 = 0.008577663451433182 + 10.0 * 6.207428455352783
Epoch 2510, val loss: 1.3217097520828247
Epoch 2520, training loss: 62.093143463134766 = 0.008486353792250156 + 10.0 * 6.208465576171875
Epoch 2520, val loss: 1.3237680196762085
Epoch 2530, training loss: 62.10624694824219 = 0.008396098390221596 + 10.0 * 6.209784984588623
Epoch 2530, val loss: 1.3257015943527222
Epoch 2540, training loss: 62.11703109741211 = 0.008302958682179451 + 10.0 * 6.210872650146484
Epoch 2540, val loss: 1.3279166221618652
Epoch 2550, training loss: 62.08844757080078 = 0.008212278597056866 + 10.0 * 6.208023548126221
Epoch 2550, val loss: 1.3294602632522583
Epoch 2560, training loss: 62.080326080322266 = 0.00812561996281147 + 10.0 * 6.207220077514648
Epoch 2560, val loss: 1.331284761428833
Epoch 2570, training loss: 62.070953369140625 = 0.008041546680033207 + 10.0 * 6.206291198730469
Epoch 2570, val loss: 1.3333039283752441
Epoch 2580, training loss: 62.079612731933594 = 0.007961125113070011 + 10.0 * 6.207165241241455
Epoch 2580, val loss: 1.3352309465408325
Epoch 2590, training loss: 62.116878509521484 = 0.00788046419620514 + 10.0 * 6.210899829864502
Epoch 2590, val loss: 1.337175726890564
Epoch 2600, training loss: 62.11274337768555 = 0.007797684520483017 + 10.0 * 6.210494518280029
Epoch 2600, val loss: 1.338599443435669
Epoch 2610, training loss: 62.08784866333008 = 0.007715081330388784 + 10.0 * 6.208013534545898
Epoch 2610, val loss: 1.3405201435089111
Epoch 2620, training loss: 62.079288482666016 = 0.0076361652463674545 + 10.0 * 6.207165241241455
Epoch 2620, val loss: 1.3422163724899292
Epoch 2630, training loss: 62.064327239990234 = 0.0075598848052322865 + 10.0 * 6.205676555633545
Epoch 2630, val loss: 1.3442038297653198
Epoch 2640, training loss: 62.07504653930664 = 0.007487037219107151 + 10.0 * 6.206755638122559
Epoch 2640, val loss: 1.3459105491638184
Epoch 2650, training loss: 62.075531005859375 = 0.007412767503410578 + 10.0 * 6.206811904907227
Epoch 2650, val loss: 1.3477474451065063
Epoch 2660, training loss: 62.07972717285156 = 0.007337578106671572 + 10.0 * 6.207238674163818
Epoch 2660, val loss: 1.3492754697799683
Epoch 2670, training loss: 62.08177947998047 = 0.007263903506100178 + 10.0 * 6.207451820373535
Epoch 2670, val loss: 1.3510550260543823
Epoch 2680, training loss: 62.08518981933594 = 0.007194728124886751 + 10.0 * 6.207799434661865
Epoch 2680, val loss: 1.3526561260223389
Epoch 2690, training loss: 62.08495330810547 = 0.007122659124433994 + 10.0 * 6.207783222198486
Epoch 2690, val loss: 1.3545185327529907
Epoch 2700, training loss: 62.05325698852539 = 0.0070502860471606255 + 10.0 * 6.204620838165283
Epoch 2700, val loss: 1.3559809923171997
Epoch 2710, training loss: 62.052886962890625 = 0.00698453513905406 + 10.0 * 6.204590320587158
Epoch 2710, val loss: 1.3578826189041138
Epoch 2720, training loss: 62.04465866088867 = 0.006920037791132927 + 10.0 * 6.2037739753723145
Epoch 2720, val loss: 1.359544277191162
Epoch 2730, training loss: 62.06088638305664 = 0.006856580264866352 + 10.0 * 6.2054033279418945
Epoch 2730, val loss: 1.3612918853759766
Epoch 2740, training loss: 62.071319580078125 = 0.006791135296225548 + 10.0 * 6.2064528465271
Epoch 2740, val loss: 1.3630107641220093
Epoch 2750, training loss: 62.07468795776367 = 0.006726982071995735 + 10.0 * 6.206796169281006
Epoch 2750, val loss: 1.364147424697876
Epoch 2760, training loss: 62.05815124511719 = 0.0066633643582463264 + 10.0 * 6.205148696899414
Epoch 2760, val loss: 1.3660060167312622
Epoch 2770, training loss: 62.0514030456543 = 0.00660285260528326 + 10.0 * 6.204480171203613
Epoch 2770, val loss: 1.3673628568649292
Epoch 2780, training loss: 62.101497650146484 = 0.006542300805449486 + 10.0 * 6.209495544433594
Epoch 2780, val loss: 1.3691749572753906
Epoch 2790, training loss: 62.081153869628906 = 0.006476562470197678 + 10.0 * 6.207467555999756
Epoch 2790, val loss: 1.3704620599746704
Epoch 2800, training loss: 62.043087005615234 = 0.0064198048785328865 + 10.0 * 6.203666687011719
Epoch 2800, val loss: 1.3720334768295288
Epoch 2810, training loss: 62.03954315185547 = 0.006363480351865292 + 10.0 * 6.203318119049072
Epoch 2810, val loss: 1.3734968900680542
Epoch 2820, training loss: 62.05686950683594 = 0.006309031508862972 + 10.0 * 6.205056190490723
Epoch 2820, val loss: 1.3754348754882812
Epoch 2830, training loss: 62.04670715332031 = 0.006250374019145966 + 10.0 * 6.20404577255249
Epoch 2830, val loss: 1.3765783309936523
Epoch 2840, training loss: 62.04145431518555 = 0.006195485591888428 + 10.0 * 6.203526020050049
Epoch 2840, val loss: 1.3779480457305908
Epoch 2850, training loss: 62.026615142822266 = 0.006141043733805418 + 10.0 * 6.202047348022461
Epoch 2850, val loss: 1.3795504570007324
Epoch 2860, training loss: 62.05887222290039 = 0.0060896677896380424 + 10.0 * 6.205278396606445
Epoch 2860, val loss: 1.3811243772506714
Epoch 2870, training loss: 62.03856658935547 = 0.006034791003912687 + 10.0 * 6.203253269195557
Epoch 2870, val loss: 1.3823885917663574
Epoch 2880, training loss: 62.058101654052734 = 0.005982679780572653 + 10.0 * 6.205212116241455
Epoch 2880, val loss: 1.3836640119552612
Epoch 2890, training loss: 62.03753662109375 = 0.005929286126047373 + 10.0 * 6.203160762786865
Epoch 2890, val loss: 1.385123610496521
Epoch 2900, training loss: 62.0313606262207 = 0.005880100652575493 + 10.0 * 6.202548027038574
Epoch 2900, val loss: 1.3863362073898315
Epoch 2910, training loss: 62.05057907104492 = 0.005832371301949024 + 10.0 * 6.204474449157715
Epoch 2910, val loss: 1.3878000974655151
Epoch 2920, training loss: 62.02033996582031 = 0.005780844949185848 + 10.0 * 6.201456069946289
Epoch 2920, val loss: 1.3890783786773682
Epoch 2930, training loss: 62.02300262451172 = 0.005733012221753597 + 10.0 * 6.201726913452148
Epoch 2930, val loss: 1.3905919790267944
Epoch 2940, training loss: 62.024391174316406 = 0.005686004180461168 + 10.0 * 6.201870441436768
Epoch 2940, val loss: 1.3919575214385986
Epoch 2950, training loss: 62.067237854003906 = 0.005639613606035709 + 10.0 * 6.206160068511963
Epoch 2950, val loss: 1.392936110496521
Epoch 2960, training loss: 62.028343200683594 = 0.005592558532953262 + 10.0 * 6.202275276184082
Epoch 2960, val loss: 1.3945280313491821
Epoch 2970, training loss: 62.01606750488281 = 0.005546262953430414 + 10.0 * 6.201052188873291
Epoch 2970, val loss: 1.3955469131469727
Epoch 2980, training loss: 62.02029037475586 = 0.005502711050212383 + 10.0 * 6.201478481292725
Epoch 2980, val loss: 1.397060751914978
Epoch 2990, training loss: 62.02939224243164 = 0.005459608510136604 + 10.0 * 6.202393531799316
Epoch 2990, val loss: 1.398319959640503
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 87.91595458984375 = 1.9478795528411865 + 10.0 * 8.596807479858398
Epoch 0, val loss: 1.948797583580017
Epoch 10, training loss: 87.89747619628906 = 1.9372897148132324 + 10.0 * 8.59601879119873
Epoch 10, val loss: 1.938100814819336
Epoch 20, training loss: 87.828125 = 1.9239057302474976 + 10.0 * 8.590421676635742
Epoch 20, val loss: 1.9240245819091797
Epoch 30, training loss: 87.43790435791016 = 1.9064135551452637 + 10.0 * 8.553149223327637
Epoch 30, val loss: 1.9053369760513306
Epoch 40, training loss: 84.79619598388672 = 1.887168526649475 + 10.0 * 8.290903091430664
Epoch 40, val loss: 1.8856414556503296
Epoch 50, training loss: 78.02793884277344 = 1.866408109664917 + 10.0 * 7.616153240203857
Epoch 50, val loss: 1.8646464347839355
Epoch 60, training loss: 76.50483703613281 = 1.849503993988037 + 10.0 * 7.465533256530762
Epoch 60, val loss: 1.8485782146453857
Epoch 70, training loss: 74.17700958251953 = 1.8356068134307861 + 10.0 * 7.234139919281006
Epoch 70, val loss: 1.8348374366760254
Epoch 80, training loss: 72.43842315673828 = 1.8228998184204102 + 10.0 * 7.061552047729492
Epoch 80, val loss: 1.8226804733276367
Epoch 90, training loss: 71.14920806884766 = 1.813585638999939 + 10.0 * 6.933562755584717
Epoch 90, val loss: 1.814022421836853
Epoch 100, training loss: 70.1673812866211 = 1.8040224313735962 + 10.0 * 6.836336135864258
Epoch 100, val loss: 1.8049509525299072
Epoch 110, training loss: 69.32624816894531 = 1.7936387062072754 + 10.0 * 6.753261089324951
Epoch 110, val loss: 1.7950537204742432
Epoch 120, training loss: 68.69842529296875 = 1.783639669418335 + 10.0 * 6.691478729248047
Epoch 120, val loss: 1.7855701446533203
Epoch 130, training loss: 68.24696350097656 = 1.7735908031463623 + 10.0 * 6.647336959838867
Epoch 130, val loss: 1.7760478258132935
Epoch 140, training loss: 67.92366790771484 = 1.762900948524475 + 10.0 * 6.616076469421387
Epoch 140, val loss: 1.765946865081787
Epoch 150, training loss: 67.65612030029297 = 1.7515932321548462 + 10.0 * 6.590452194213867
Epoch 150, val loss: 1.7553131580352783
Epoch 160, training loss: 67.3952865600586 = 1.7396924495697021 + 10.0 * 6.565559387207031
Epoch 160, val loss: 1.744299292564392
Epoch 170, training loss: 67.17003631591797 = 1.7271063327789307 + 10.0 * 6.54429292678833
Epoch 170, val loss: 1.7328530550003052
Epoch 180, training loss: 66.98674011230469 = 1.7136396169662476 + 10.0 * 6.527310371398926
Epoch 180, val loss: 1.7206929922103882
Epoch 190, training loss: 66.78942108154297 = 1.6991819143295288 + 10.0 * 6.509024143218994
Epoch 190, val loss: 1.7078042030334473
Epoch 200, training loss: 66.61355590820312 = 1.6837037801742554 + 10.0 * 6.492985725402832
Epoch 200, val loss: 1.6941378116607666
Epoch 210, training loss: 66.45218658447266 = 1.6669702529907227 + 10.0 * 6.478521347045898
Epoch 210, val loss: 1.6793981790542603
Epoch 220, training loss: 66.31298828125 = 1.6487559080123901 + 10.0 * 6.466423034667969
Epoch 220, val loss: 1.663455605506897
Epoch 230, training loss: 66.1879653930664 = 1.6290520429611206 + 10.0 * 6.4558916091918945
Epoch 230, val loss: 1.646209955215454
Epoch 240, training loss: 66.0573501586914 = 1.6078259944915771 + 10.0 * 6.444952487945557
Epoch 240, val loss: 1.6277028322219849
Epoch 250, training loss: 66.01374053955078 = 1.5850121974945068 + 10.0 * 6.442873001098633
Epoch 250, val loss: 1.6079200506210327
Epoch 260, training loss: 65.85350036621094 = 1.5606050491333008 + 10.0 * 6.4292893409729
Epoch 260, val loss: 1.5864888429641724
Epoch 270, training loss: 65.73219299316406 = 1.5348979234695435 + 10.0 * 6.419729709625244
Epoch 270, val loss: 1.564086675643921
Epoch 280, training loss: 65.61988830566406 = 1.507739543914795 + 10.0 * 6.411214828491211
Epoch 280, val loss: 1.5405877828598022
Epoch 290, training loss: 65.5448226928711 = 1.4791263341903687 + 10.0 * 6.406569957733154
Epoch 290, val loss: 1.5158600807189941
Epoch 300, training loss: 65.42579650878906 = 1.4492149353027344 + 10.0 * 6.397658348083496
Epoch 300, val loss: 1.490146517753601
Epoch 310, training loss: 65.33285522460938 = 1.4182125329971313 + 10.0 * 6.3914642333984375
Epoch 310, val loss: 1.4637091159820557
Epoch 320, training loss: 65.31987762451172 = 1.3862546682357788 + 10.0 * 6.393362045288086
Epoch 320, val loss: 1.436657190322876
Epoch 330, training loss: 65.18096923828125 = 1.3534209728240967 + 10.0 * 6.382754802703857
Epoch 330, val loss: 1.4088882207870483
Epoch 340, training loss: 65.07931518554688 = 1.3200232982635498 + 10.0 * 6.375929355621338
Epoch 340, val loss: 1.3808512687683105
Epoch 350, training loss: 65.01390075683594 = 1.2861807346343994 + 10.0 * 6.372771739959717
Epoch 350, val loss: 1.3526138067245483
Epoch 360, training loss: 64.93836212158203 = 1.2517776489257812 + 10.0 * 6.368658542633057
Epoch 360, val loss: 1.3241835832595825
Epoch 370, training loss: 64.84239959716797 = 1.2175369262695312 + 10.0 * 6.362486362457275
Epoch 370, val loss: 1.2960095405578613
Epoch 380, training loss: 64.7618179321289 = 1.1833178997039795 + 10.0 * 6.357850074768066
Epoch 380, val loss: 1.2680377960205078
Epoch 390, training loss: 64.73127746582031 = 1.149288296699524 + 10.0 * 6.358198642730713
Epoch 390, val loss: 1.2405321598052979
Epoch 400, training loss: 64.63565826416016 = 1.1155585050582886 + 10.0 * 6.352010250091553
Epoch 400, val loss: 1.213185429573059
Epoch 410, training loss: 64.55715942382812 = 1.0823848247528076 + 10.0 * 6.347477436065674
Epoch 410, val loss: 1.1866589784622192
Epoch 420, training loss: 64.50121307373047 = 1.0499941110610962 + 10.0 * 6.34512186050415
Epoch 420, val loss: 1.1610208749771118
Epoch 430, training loss: 64.43022918701172 = 1.0184205770492554 + 10.0 * 6.341180801391602
Epoch 430, val loss: 1.136147379875183
Epoch 440, training loss: 64.36756896972656 = 0.9877992868423462 + 10.0 * 6.337976932525635
Epoch 440, val loss: 1.11240816116333
Epoch 450, training loss: 64.3202896118164 = 0.9583029747009277 + 10.0 * 6.336198329925537
Epoch 450, val loss: 1.0898302793502808
Epoch 460, training loss: 64.28543090820312 = 0.9296501278877258 + 10.0 * 6.335577964782715
Epoch 460, val loss: 1.0679632425308228
Epoch 470, training loss: 64.2027359008789 = 0.9022642970085144 + 10.0 * 6.330047130584717
Epoch 470, val loss: 1.047662615776062
Epoch 480, training loss: 64.151611328125 = 0.8759877681732178 + 10.0 * 6.32756233215332
Epoch 480, val loss: 1.0285054445266724
Epoch 490, training loss: 64.0917739868164 = 0.8506838083267212 + 10.0 * 6.324109077453613
Epoch 490, val loss: 1.0103795528411865
Epoch 500, training loss: 64.06327819824219 = 0.8263324499130249 + 10.0 * 6.323694229125977
Epoch 500, val loss: 0.9933125972747803
Epoch 510, training loss: 64.0009765625 = 0.8029564023017883 + 10.0 * 6.3198018074035645
Epoch 510, val loss: 0.9772166013717651
Epoch 520, training loss: 63.957881927490234 = 0.7803929448127747 + 10.0 * 6.3177490234375
Epoch 520, val loss: 0.9619070291519165
Epoch 530, training loss: 63.91272735595703 = 0.7585806250572205 + 10.0 * 6.315414905548096
Epoch 530, val loss: 0.9475621581077576
Epoch 540, training loss: 63.9555549621582 = 0.7374394536018372 + 10.0 * 6.321811676025391
Epoch 540, val loss: 0.9338006973266602
Epoch 550, training loss: 63.83613967895508 = 0.7166950106620789 + 10.0 * 6.311944484710693
Epoch 550, val loss: 0.9207213521003723
Epoch 560, training loss: 63.791351318359375 = 0.6966021656990051 + 10.0 * 6.309474945068359
Epoch 560, val loss: 0.9083331227302551
Epoch 570, training loss: 63.75882339477539 = 0.6770585179328918 + 10.0 * 6.308176517486572
Epoch 570, val loss: 0.8965268731117249
Epoch 580, training loss: 63.744590759277344 = 0.6579307913780212 + 10.0 * 6.308665752410889
Epoch 580, val loss: 0.8852436542510986
Epoch 590, training loss: 63.680728912353516 = 0.6390029788017273 + 10.0 * 6.304172515869141
Epoch 590, val loss: 0.8740465641021729
Epoch 600, training loss: 63.65108871459961 = 0.620586097240448 + 10.0 * 6.3030500411987305
Epoch 600, val loss: 0.8637324571609497
Epoch 610, training loss: 63.60402297973633 = 0.602540910243988 + 10.0 * 6.3001484870910645
Epoch 610, val loss: 0.8537046909332275
Epoch 620, training loss: 63.62281036376953 = 0.5848225355148315 + 10.0 * 6.303798675537109
Epoch 620, val loss: 0.8441198468208313
Epoch 630, training loss: 63.5708122253418 = 0.5674285292625427 + 10.0 * 6.300338268280029
Epoch 630, val loss: 0.8349711298942566
Epoch 640, training loss: 63.516117095947266 = 0.5503477454185486 + 10.0 * 6.296576976776123
Epoch 640, val loss: 0.8261829018592834
Epoch 650, training loss: 63.51306915283203 = 0.5337181091308594 + 10.0 * 6.2979350090026855
Epoch 650, val loss: 0.8179759979248047
Epoch 660, training loss: 63.46367645263672 = 0.5173160433769226 + 10.0 * 6.294636249542236
Epoch 660, val loss: 0.8099793791770935
Epoch 670, training loss: 63.42001724243164 = 0.5014108419418335 + 10.0 * 6.291860580444336
Epoch 670, val loss: 0.8025275468826294
Epoch 680, training loss: 63.4271125793457 = 0.485843688249588 + 10.0 * 6.294126987457275
Epoch 680, val loss: 0.7953599691390991
Epoch 690, training loss: 63.38726043701172 = 0.4706581234931946 + 10.0 * 6.291660308837891
Epoch 690, val loss: 0.788766086101532
Epoch 700, training loss: 63.342044830322266 = 0.4558492600917816 + 10.0 * 6.288619518280029
Epoch 700, val loss: 0.782516598701477
Epoch 710, training loss: 63.31310272216797 = 0.4413917660713196 + 10.0 * 6.287171363830566
Epoch 710, val loss: 0.7766244411468506
Epoch 720, training loss: 63.282772064208984 = 0.4273156225681305 + 10.0 * 6.285545825958252
Epoch 720, val loss: 0.7712123394012451
Epoch 730, training loss: 63.27627944946289 = 0.41365399956703186 + 10.0 * 6.286262512207031
Epoch 730, val loss: 0.7661954760551453
Epoch 740, training loss: 63.23332595825195 = 0.4002106189727783 + 10.0 * 6.283311367034912
Epoch 740, val loss: 0.7612258791923523
Epoch 750, training loss: 63.206138610839844 = 0.38720035552978516 + 10.0 * 6.281893730163574
Epoch 750, val loss: 0.756842851638794
Epoch 760, training loss: 63.22425079345703 = 0.3745187222957611 + 10.0 * 6.28497314453125
Epoch 760, val loss: 0.7526437044143677
Epoch 770, training loss: 63.19890213012695 = 0.36212605237960815 + 10.0 * 6.283677577972412
Epoch 770, val loss: 0.7488700151443481
Epoch 780, training loss: 63.133487701416016 = 0.3500610291957855 + 10.0 * 6.2783427238464355
Epoch 780, val loss: 0.7452887892723083
Epoch 790, training loss: 63.11315155029297 = 0.338349848985672 + 10.0 * 6.277480125427246
Epoch 790, val loss: 0.7420625686645508
Epoch 800, training loss: 63.09661102294922 = 0.32695409655570984 + 10.0 * 6.276965618133545
Epoch 800, val loss: 0.7392040491104126
Epoch 810, training loss: 63.1138916015625 = 0.31585589051246643 + 10.0 * 6.279803276062012
Epoch 810, val loss: 0.7364076375961304
Epoch 820, training loss: 63.054893493652344 = 0.3049786686897278 + 10.0 * 6.274991512298584
Epoch 820, val loss: 0.7338725924491882
Epoch 830, training loss: 63.036521911621094 = 0.2944614589214325 + 10.0 * 6.274206161499023
Epoch 830, val loss: 0.7316561937332153
Epoch 840, training loss: 63.01228332519531 = 0.2842707931995392 + 10.0 * 6.272801399230957
Epoch 840, val loss: 0.729781448841095
Epoch 850, training loss: 63.05573654174805 = 0.274379700422287 + 10.0 * 6.278135776519775
Epoch 850, val loss: 0.728014349937439
Epoch 860, training loss: 63.01625442504883 = 0.2648186981678009 + 10.0 * 6.275143623352051
Epoch 860, val loss: 0.726578950881958
Epoch 870, training loss: 62.958518981933594 = 0.25547635555267334 + 10.0 * 6.270304203033447
Epoch 870, val loss: 0.7254282236099243
Epoch 880, training loss: 62.936126708984375 = 0.24653871357440948 + 10.0 * 6.268958568572998
Epoch 880, val loss: 0.7245883345603943
Epoch 890, training loss: 62.9490852355957 = 0.23790960013866425 + 10.0 * 6.271117210388184
Epoch 890, val loss: 0.7239949107170105
Epoch 900, training loss: 62.924278259277344 = 0.22951601445674896 + 10.0 * 6.269476413726807
Epoch 900, val loss: 0.7235594391822815
Epoch 910, training loss: 62.89614486694336 = 0.2214294821023941 + 10.0 * 6.267471790313721
Epoch 910, val loss: 0.7235153913497925
Epoch 920, training loss: 62.87704849243164 = 0.21363219618797302 + 10.0 * 6.266341686248779
Epoch 920, val loss: 0.7236517667770386
Epoch 930, training loss: 62.86508560180664 = 0.2061680108308792 + 10.0 * 6.2658915519714355
Epoch 930, val loss: 0.7241621017456055
Epoch 940, training loss: 62.922183990478516 = 0.19896233081817627 + 10.0 * 6.272322177886963
Epoch 940, val loss: 0.7248440980911255
Epoch 950, training loss: 62.83356857299805 = 0.19199156761169434 + 10.0 * 6.264157772064209
Epoch 950, val loss: 0.7256636619567871
Epoch 960, training loss: 62.81461715698242 = 0.1853279024362564 + 10.0 * 6.2629289627075195
Epoch 960, val loss: 0.7268738746643066
Epoch 970, training loss: 62.800453186035156 = 0.17893987894058228 + 10.0 * 6.26215124130249
Epoch 970, val loss: 0.7283772826194763
Epoch 980, training loss: 62.80659103393555 = 0.17282862961292267 + 10.0 * 6.263376235961914
Epoch 980, val loss: 0.7301123142242432
Epoch 990, training loss: 62.789955139160156 = 0.16685524582862854 + 10.0 * 6.262310028076172
Epoch 990, val loss: 0.7318271994590759
Epoch 1000, training loss: 62.790462493896484 = 0.16116660833358765 + 10.0 * 6.262929439544678
Epoch 1000, val loss: 0.7338191866874695
Epoch 1010, training loss: 62.74900436401367 = 0.15571269392967224 + 10.0 * 6.259329319000244
Epoch 1010, val loss: 0.7362069487571716
Epoch 1020, training loss: 62.74818801879883 = 0.1504983752965927 + 10.0 * 6.259768962860107
Epoch 1020, val loss: 0.7386071085929871
Epoch 1030, training loss: 62.755340576171875 = 0.1454796940088272 + 10.0 * 6.260985851287842
Epoch 1030, val loss: 0.7411661744117737
Epoch 1040, training loss: 62.757572174072266 = 0.14067313075065613 + 10.0 * 6.26168966293335
Epoch 1040, val loss: 0.7438625693321228
Epoch 1050, training loss: 62.7075080871582 = 0.1360188126564026 + 10.0 * 6.257148742675781
Epoch 1050, val loss: 0.7467186450958252
Epoch 1060, training loss: 62.69076919555664 = 0.1316007375717163 + 10.0 * 6.255917072296143
Epoch 1060, val loss: 0.7497177720069885
Epoch 1070, training loss: 62.6806526184082 = 0.12736192345619202 + 10.0 * 6.255329132080078
Epoch 1070, val loss: 0.7529154419898987
Epoch 1080, training loss: 62.701988220214844 = 0.1232956275343895 + 10.0 * 6.257869243621826
Epoch 1080, val loss: 0.7562367916107178
Epoch 1090, training loss: 62.672119140625 = 0.11932429671287537 + 10.0 * 6.255279541015625
Epoch 1090, val loss: 0.759401798248291
Epoch 1100, training loss: 62.672630310058594 = 0.11552833765745163 + 10.0 * 6.255710124969482
Epoch 1100, val loss: 0.7628582715988159
Epoch 1110, training loss: 62.656436920166016 = 0.11188240349292755 + 10.0 * 6.25445556640625
Epoch 1110, val loss: 0.766250491142273
Epoch 1120, training loss: 62.638084411621094 = 0.10840560495853424 + 10.0 * 6.252967834472656
Epoch 1120, val loss: 0.7700245380401611
Epoch 1130, training loss: 62.62424850463867 = 0.10506423562765121 + 10.0 * 6.251918315887451
Epoch 1130, val loss: 0.7736877202987671
Epoch 1140, training loss: 62.64745330810547 = 0.10185030847787857 + 10.0 * 6.254560470581055
Epoch 1140, val loss: 0.7774416208267212
Epoch 1150, training loss: 62.60798645019531 = 0.09873764961957932 + 10.0 * 6.250925064086914
Epoch 1150, val loss: 0.7812929749488831
Epoch 1160, training loss: 62.647621154785156 = 0.09574186056852341 + 10.0 * 6.25518798828125
Epoch 1160, val loss: 0.7850977182388306
Epoch 1170, training loss: 62.61178970336914 = 0.0928514301776886 + 10.0 * 6.251893997192383
Epoch 1170, val loss: 0.7889987230300903
Epoch 1180, training loss: 62.590065002441406 = 0.09007232636213303 + 10.0 * 6.249999046325684
Epoch 1180, val loss: 0.7929785251617432
Epoch 1190, training loss: 62.58359146118164 = 0.08743028342723846 + 10.0 * 6.2496161460876465
Epoch 1190, val loss: 0.7971014976501465
Epoch 1200, training loss: 62.582237243652344 = 0.08487122505903244 + 10.0 * 6.249736785888672
Epoch 1200, val loss: 0.8011654615402222
Epoch 1210, training loss: 62.57063293457031 = 0.08240536600351334 + 10.0 * 6.2488226890563965
Epoch 1210, val loss: 0.8052282333374023
Epoch 1220, training loss: 62.568782806396484 = 0.08003941178321838 + 10.0 * 6.248874187469482
Epoch 1220, val loss: 0.8093889951705933
Epoch 1230, training loss: 62.56616973876953 = 0.07775221765041351 + 10.0 * 6.248841762542725
Epoch 1230, val loss: 0.8135968446731567
Epoch 1240, training loss: 62.56059646606445 = 0.07553449273109436 + 10.0 * 6.24850606918335
Epoch 1240, val loss: 0.8176358342170715
Epoch 1250, training loss: 62.535400390625 = 0.07339662313461304 + 10.0 * 6.2462005615234375
Epoch 1250, val loss: 0.82172691822052
Epoch 1260, training loss: 62.52250671386719 = 0.07136426120996475 + 10.0 * 6.245114326477051
Epoch 1260, val loss: 0.8260675072669983
Epoch 1270, training loss: 62.553016662597656 = 0.06940648704767227 + 10.0 * 6.248361110687256
Epoch 1270, val loss: 0.8302567601203918
Epoch 1280, training loss: 62.51005554199219 = 0.06749038398265839 + 10.0 * 6.244256496429443
Epoch 1280, val loss: 0.8344221711158752
Epoch 1290, training loss: 62.5036506652832 = 0.06564901769161224 + 10.0 * 6.243800163269043
Epoch 1290, val loss: 0.838555097579956
Epoch 1300, training loss: 62.54640197753906 = 0.06388852745294571 + 10.0 * 6.248251438140869
Epoch 1300, val loss: 0.842862069606781
Epoch 1310, training loss: 62.49573516845703 = 0.06217815354466438 + 10.0 * 6.243355751037598
Epoch 1310, val loss: 0.847028911113739
Epoch 1320, training loss: 62.491737365722656 = 0.06052311137318611 + 10.0 * 6.243121147155762
Epoch 1320, val loss: 0.8512868881225586
Epoch 1330, training loss: 62.510887145996094 = 0.05895794928073883 + 10.0 * 6.245193004608154
Epoch 1330, val loss: 0.8556426167488098
Epoch 1340, training loss: 62.46890640258789 = 0.0574101097881794 + 10.0 * 6.241149425506592
Epoch 1340, val loss: 0.8597759008407593
Epoch 1350, training loss: 62.490997314453125 = 0.05594664439558983 + 10.0 * 6.243505001068115
Epoch 1350, val loss: 0.8641586303710938
Epoch 1360, training loss: 62.47465133666992 = 0.05452347174286842 + 10.0 * 6.242012977600098
Epoch 1360, val loss: 0.8683418035507202
Epoch 1370, training loss: 62.452903747558594 = 0.053132254630327225 + 10.0 * 6.2399773597717285
Epoch 1370, val loss: 0.8724073171615601
Epoch 1380, training loss: 62.4466552734375 = 0.05181252583861351 + 10.0 * 6.2394843101501465
Epoch 1380, val loss: 0.8767614960670471
Epoch 1390, training loss: 62.49721908569336 = 0.050544373691082 + 10.0 * 6.2446675300598145
Epoch 1390, val loss: 0.8810291290283203
Epoch 1400, training loss: 62.45039749145508 = 0.0492759607732296 + 10.0 * 6.2401123046875
Epoch 1400, val loss: 0.8849571347236633
Epoch 1410, training loss: 62.441226959228516 = 0.048074010759592056 + 10.0 * 6.239315509796143
Epoch 1410, val loss: 0.889189600944519
Epoch 1420, training loss: 62.44647979736328 = 0.046918924897909164 + 10.0 * 6.239955902099609
Epoch 1420, val loss: 0.893349289894104
Epoch 1430, training loss: 62.424007415771484 = 0.04579510539770126 + 10.0 * 6.237821102142334
Epoch 1430, val loss: 0.8974677920341492
Epoch 1440, training loss: 62.41567611694336 = 0.04470961540937424 + 10.0 * 6.237096786499023
Epoch 1440, val loss: 0.9015260338783264
Epoch 1450, training loss: 62.42277526855469 = 0.04366448521614075 + 10.0 * 6.237911224365234
Epoch 1450, val loss: 0.9057278633117676
Epoch 1460, training loss: 62.44705581665039 = 0.0426517128944397 + 10.0 * 6.240440368652344
Epoch 1460, val loss: 0.9097297787666321
Epoch 1470, training loss: 62.41874694824219 = 0.041661255061626434 + 10.0 * 6.237708568572998
Epoch 1470, val loss: 0.9137892723083496
Epoch 1480, training loss: 62.4385986328125 = 0.040720053017139435 + 10.0 * 6.239787578582764
Epoch 1480, val loss: 0.9179615378379822
Epoch 1490, training loss: 62.40648651123047 = 0.03978782892227173 + 10.0 * 6.236670017242432
Epoch 1490, val loss: 0.9217526912689209
Epoch 1500, training loss: 62.39322280883789 = 0.03888813033699989 + 10.0 * 6.235433578491211
Epoch 1500, val loss: 0.9258576035499573
Epoch 1510, training loss: 62.390167236328125 = 0.03802654147148132 + 10.0 * 6.2352142333984375
Epoch 1510, val loss: 0.9297791123390198
Epoch 1520, training loss: 62.43497848510742 = 0.0371890552341938 + 10.0 * 6.239778995513916
Epoch 1520, val loss: 0.9336680173873901
Epoch 1530, training loss: 62.38581848144531 = 0.03637983277440071 + 10.0 * 6.234943866729736
Epoch 1530, val loss: 0.9378849864006042
Epoch 1540, training loss: 62.37456512451172 = 0.03559056669473648 + 10.0 * 6.2338972091674805
Epoch 1540, val loss: 0.941727340221405
Epoch 1550, training loss: 62.42110824584961 = 0.034845899790525436 + 10.0 * 6.238626003265381
Epoch 1550, val loss: 0.9458261728286743
Epoch 1560, training loss: 62.3656120300293 = 0.034083109349012375 + 10.0 * 6.233152866363525
Epoch 1560, val loss: 0.9494844675064087
Epoch 1570, training loss: 62.35354995727539 = 0.033371586352586746 + 10.0 * 6.232017993927002
Epoch 1570, val loss: 0.9535046815872192
Epoch 1580, training loss: 62.35716247558594 = 0.03268350288271904 + 10.0 * 6.232447624206543
Epoch 1580, val loss: 0.9573751091957092
Epoch 1590, training loss: 62.394779205322266 = 0.032011378556489944 + 10.0 * 6.236276626586914
Epoch 1590, val loss: 0.9610915780067444
Epoch 1600, training loss: 62.355552673339844 = 0.031350210309028625 + 10.0 * 6.232420444488525
Epoch 1600, val loss: 0.9649881720542908
Epoch 1610, training loss: 62.350406646728516 = 0.03071349486708641 + 10.0 * 6.231969356536865
Epoch 1610, val loss: 0.9687711596488953
Epoch 1620, training loss: 62.36421585083008 = 0.03009815141558647 + 10.0 * 6.23341178894043
Epoch 1620, val loss: 0.9725141525268555
Epoch 1630, training loss: 62.389068603515625 = 0.0294942669570446 + 10.0 * 6.235957145690918
Epoch 1630, val loss: 0.9762550592422485
Epoch 1640, training loss: 62.351600646972656 = 0.028904996812343597 + 10.0 * 6.232269763946533
Epoch 1640, val loss: 0.9799516201019287
Epoch 1650, training loss: 62.32421875 = 0.02833859808743 + 10.0 * 6.229588031768799
Epoch 1650, val loss: 0.9838246703147888
Epoch 1660, training loss: 62.32054138183594 = 0.02779628150165081 + 10.0 * 6.229274272918701
Epoch 1660, val loss: 0.9876435399055481
Epoch 1670, training loss: 62.349735260009766 = 0.027275629341602325 + 10.0 * 6.232245922088623
Epoch 1670, val loss: 0.9914966821670532
Epoch 1680, training loss: 62.316226959228516 = 0.02674662508070469 + 10.0 * 6.22894811630249
Epoch 1680, val loss: 0.9949527382850647
Epoch 1690, training loss: 62.33474349975586 = 0.026238424703478813 + 10.0 * 6.230850696563721
Epoch 1690, val loss: 0.9985918998718262
Epoch 1700, training loss: 62.330909729003906 = 0.025746788829565048 + 10.0 * 6.23051643371582
Epoch 1700, val loss: 1.002328634262085
Epoch 1710, training loss: 62.33656311035156 = 0.025260355323553085 + 10.0 * 6.231130123138428
Epoch 1710, val loss: 1.0057072639465332
Epoch 1720, training loss: 62.32251739501953 = 0.02479676343500614 + 10.0 * 6.229772090911865
Epoch 1720, val loss: 1.0094339847564697
Epoch 1730, training loss: 62.32774353027344 = 0.024339651688933372 + 10.0 * 6.230340480804443
Epoch 1730, val loss: 1.0129432678222656
Epoch 1740, training loss: 62.3051643371582 = 0.023901430889964104 + 10.0 * 6.228126049041748
Epoch 1740, val loss: 1.0166099071502686
Epoch 1750, training loss: 62.29985046386719 = 0.023469293490052223 + 10.0 * 6.227638244628906
Epoch 1750, val loss: 1.0200657844543457
Epoch 1760, training loss: 62.29436111450195 = 0.023050932213664055 + 10.0 * 6.227130889892578
Epoch 1760, val loss: 1.0235871076583862
Epoch 1770, training loss: 62.31515121459961 = 0.022647161036729813 + 10.0 * 6.229250431060791
Epoch 1770, val loss: 1.027112603187561
Epoch 1780, training loss: 62.325035095214844 = 0.022249191999435425 + 10.0 * 6.230278968811035
Epoch 1780, val loss: 1.0304491519927979
Epoch 1790, training loss: 62.2890625 = 0.021856853738427162 + 10.0 * 6.226720333099365
Epoch 1790, val loss: 1.033889651298523
Epoch 1800, training loss: 62.303794860839844 = 0.0214824341237545 + 10.0 * 6.228231430053711
Epoch 1800, val loss: 1.037314772605896
Epoch 1810, training loss: 62.285282135009766 = 0.02111285924911499 + 10.0 * 6.22641658782959
Epoch 1810, val loss: 1.0406746864318848
Epoch 1820, training loss: 62.282657623291016 = 0.020755235105752945 + 10.0 * 6.226190090179443
Epoch 1820, val loss: 1.0440424680709839
Epoch 1830, training loss: 62.269710540771484 = 0.020406344905495644 + 10.0 * 6.224930763244629
Epoch 1830, val loss: 1.0474164485931396
Epoch 1840, training loss: 62.294734954833984 = 0.020073216408491135 + 10.0 * 6.227466106414795
Epoch 1840, val loss: 1.0508147478103638
Epoch 1850, training loss: 62.28670120239258 = 0.019736742600798607 + 10.0 * 6.226696491241455
Epoch 1850, val loss: 1.0538864135742188
Epoch 1860, training loss: 62.27653121948242 = 0.01940435916185379 + 10.0 * 6.225712776184082
Epoch 1860, val loss: 1.0569939613342285
Epoch 1870, training loss: 62.265785217285156 = 0.019085485488176346 + 10.0 * 6.224669933319092
Epoch 1870, val loss: 1.0602003335952759
Epoch 1880, training loss: 62.25926208496094 = 0.018780076876282692 + 10.0 * 6.224048137664795
Epoch 1880, val loss: 1.0635054111480713
Epoch 1890, training loss: 62.260074615478516 = 0.018481438979506493 + 10.0 * 6.224159240722656
Epoch 1890, val loss: 1.0666574239730835
Epoch 1900, training loss: 62.27720642089844 = 0.018188897520303726 + 10.0 * 6.2259016036987305
Epoch 1900, val loss: 1.0697894096374512
Epoch 1910, training loss: 62.24060821533203 = 0.017901437357068062 + 10.0 * 6.222270488739014
Epoch 1910, val loss: 1.073009967803955
Epoch 1920, training loss: 62.31901550292969 = 0.0176283847540617 + 10.0 * 6.230138778686523
Epoch 1920, val loss: 1.076253056526184
Epoch 1930, training loss: 62.27261734008789 = 0.017344623804092407 + 10.0 * 6.225527286529541
Epoch 1930, val loss: 1.0789599418640137
Epoch 1940, training loss: 62.246620178222656 = 0.017073897644877434 + 10.0 * 6.222954750061035
Epoch 1940, val loss: 1.0821493864059448
Epoch 1950, training loss: 62.230995178222656 = 0.01681439019739628 + 10.0 * 6.2214179039001465
Epoch 1950, val loss: 1.085223913192749
Epoch 1960, training loss: 62.24376678466797 = 0.016564441844820976 + 10.0 * 6.222720146179199
Epoch 1960, val loss: 1.0883201360702515
Epoch 1970, training loss: 62.246952056884766 = 0.016314050182700157 + 10.0 * 6.2230634689331055
Epoch 1970, val loss: 1.091207504272461
Epoch 1980, training loss: 62.22526550292969 = 0.01606646738946438 + 10.0 * 6.220919609069824
Epoch 1980, val loss: 1.09415864944458
Epoch 1990, training loss: 62.232669830322266 = 0.01583252102136612 + 10.0 * 6.221683979034424
Epoch 1990, val loss: 1.0973501205444336
Epoch 2000, training loss: 62.274559020996094 = 0.015601594932377338 + 10.0 * 6.225895881652832
Epoch 2000, val loss: 1.1001111268997192
Epoch 2010, training loss: 62.231300354003906 = 0.01536246482282877 + 10.0 * 6.221593856811523
Epoch 2010, val loss: 1.1029266119003296
Epoch 2020, training loss: 62.216575622558594 = 0.015143530443310738 + 10.0 * 6.2201433181762695
Epoch 2020, val loss: 1.105962872505188
Epoch 2030, training loss: 62.226802825927734 = 0.01492787804454565 + 10.0 * 6.221187591552734
Epoch 2030, val loss: 1.1088080406188965
Epoch 2040, training loss: 62.237613677978516 = 0.014716746285557747 + 10.0 * 6.222289562225342
Epoch 2040, val loss: 1.111690878868103
Epoch 2050, training loss: 62.21755599975586 = 0.014505989849567413 + 10.0 * 6.2203049659729
Epoch 2050, val loss: 1.1144123077392578
Epoch 2060, training loss: 62.19993591308594 = 0.01430370844900608 + 10.0 * 6.218563079833984
Epoch 2060, val loss: 1.1173450946807861
Epoch 2070, training loss: 62.29511642456055 = 0.014108714647591114 + 10.0 * 6.228100776672363
Epoch 2070, val loss: 1.1198441982269287
Epoch 2080, training loss: 62.25204849243164 = 0.013912157155573368 + 10.0 * 6.223813533782959
Epoch 2080, val loss: 1.1229249238967896
Epoch 2090, training loss: 62.213924407958984 = 0.013712388463318348 + 10.0 * 6.2200212478637695
Epoch 2090, val loss: 1.1254931688308716
Epoch 2100, training loss: 62.19358444213867 = 0.01352599635720253 + 10.0 * 6.218005657196045
Epoch 2100, val loss: 1.1283667087554932
Epoch 2110, training loss: 62.19314193725586 = 0.013347680680453777 + 10.0 * 6.217979431152344
Epoch 2110, val loss: 1.1312744617462158
Epoch 2120, training loss: 62.22721862792969 = 0.013174687512218952 + 10.0 * 6.221404075622559
Epoch 2120, val loss: 1.134001612663269
Epoch 2130, training loss: 62.1973876953125 = 0.012994013726711273 + 10.0 * 6.21843957901001
Epoch 2130, val loss: 1.1365692615509033
Epoch 2140, training loss: 62.201812744140625 = 0.012818804942071438 + 10.0 * 6.218899726867676
Epoch 2140, val loss: 1.1391016244888306
Epoch 2150, training loss: 62.2121467590332 = 0.012652301229536533 + 10.0 * 6.219949245452881
Epoch 2150, val loss: 1.141866683959961
Epoch 2160, training loss: 62.201011657714844 = 0.012483229860663414 + 10.0 * 6.218852996826172
Epoch 2160, val loss: 1.1443078517913818
Epoch 2170, training loss: 62.18809127807617 = 0.012322629801928997 + 10.0 * 6.21757698059082
Epoch 2170, val loss: 1.1470832824707031
Epoch 2180, training loss: 62.177059173583984 = 0.012165294960141182 + 10.0 * 6.216489315032959
Epoch 2180, val loss: 1.149794578552246
Epoch 2190, training loss: 62.17780685424805 = 0.012015125714242458 + 10.0 * 6.216578960418701
Epoch 2190, val loss: 1.1524605751037598
Epoch 2200, training loss: 62.23697280883789 = 0.011867977678775787 + 10.0 * 6.22251033782959
Epoch 2200, val loss: 1.1550747156143188
Epoch 2210, training loss: 62.207244873046875 = 0.01170966774225235 + 10.0 * 6.219553470611572
Epoch 2210, val loss: 1.1571731567382812
Epoch 2220, training loss: 62.189064025878906 = 0.011563332751393318 + 10.0 * 6.217750072479248
Epoch 2220, val loss: 1.1598800420761108
Epoch 2230, training loss: 62.18050003051758 = 0.01141954492777586 + 10.0 * 6.216907978057861
Epoch 2230, val loss: 1.1622779369354248
Epoch 2240, training loss: 62.211517333984375 = 0.011280824430286884 + 10.0 * 6.2200236320495605
Epoch 2240, val loss: 1.1648964881896973
Epoch 2250, training loss: 62.17967224121094 = 0.011139328591525555 + 10.0 * 6.216853141784668
Epoch 2250, val loss: 1.16707444190979
Epoch 2260, training loss: 62.16740798950195 = 0.011004404164850712 + 10.0 * 6.215640068054199
Epoch 2260, val loss: 1.1697688102722168
Epoch 2270, training loss: 62.17924118041992 = 0.010874739848077297 + 10.0 * 6.216836452484131
Epoch 2270, val loss: 1.1720770597457886
Epoch 2280, training loss: 62.17863845825195 = 0.010743755847215652 + 10.0 * 6.216789722442627
Epoch 2280, val loss: 1.1744270324707031
Epoch 2290, training loss: 62.163360595703125 = 0.010615297593176365 + 10.0 * 6.215274333953857
Epoch 2290, val loss: 1.1769013404846191
Epoch 2300, training loss: 62.16096878051758 = 0.010492231696844101 + 10.0 * 6.215047836303711
Epoch 2300, val loss: 1.179315209388733
Epoch 2310, training loss: 62.170650482177734 = 0.010370819829404354 + 10.0 * 6.216027736663818
Epoch 2310, val loss: 1.1816208362579346
Epoch 2320, training loss: 62.18437576293945 = 0.010250893421471119 + 10.0 * 6.21741247177124
Epoch 2320, val loss: 1.1840133666992188
Epoch 2330, training loss: 62.186283111572266 = 0.010132407769560814 + 10.0 * 6.217615127563477
Epoch 2330, val loss: 1.186400294303894
Epoch 2340, training loss: 62.15779495239258 = 0.010012321174144745 + 10.0 * 6.214777946472168
Epoch 2340, val loss: 1.1884950399398804
Epoch 2350, training loss: 62.151397705078125 = 0.009897594340145588 + 10.0 * 6.2141499519348145
Epoch 2350, val loss: 1.1908546686172485
Epoch 2360, training loss: 62.15513229370117 = 0.009789446368813515 + 10.0 * 6.214534282684326
Epoch 2360, val loss: 1.1933225393295288
Epoch 2370, training loss: 62.20160675048828 = 0.009681783616542816 + 10.0 * 6.2191925048828125
Epoch 2370, val loss: 1.1955140829086304
Epoch 2380, training loss: 62.156036376953125 = 0.00956633035093546 + 10.0 * 6.214646816253662
Epoch 2380, val loss: 1.1975592374801636
Epoch 2390, training loss: 62.136268615722656 = 0.009462233632802963 + 10.0 * 6.212680816650391
Epoch 2390, val loss: 1.200038194656372
Epoch 2400, training loss: 62.15413284301758 = 0.00936113204807043 + 10.0 * 6.214477062225342
Epoch 2400, val loss: 1.202206015586853
Epoch 2410, training loss: 62.162803649902344 = 0.00925721786916256 + 10.0 * 6.2153544425964355
Epoch 2410, val loss: 1.204440712928772
Epoch 2420, training loss: 62.15037536621094 = 0.00915679894387722 + 10.0 * 6.2141218185424805
Epoch 2420, val loss: 1.2064813375473022
Epoch 2430, training loss: 62.165489196777344 = 0.00905977189540863 + 10.0 * 6.215642929077148
Epoch 2430, val loss: 1.2088863849639893
Epoch 2440, training loss: 62.132659912109375 = 0.008960460312664509 + 10.0 * 6.212369918823242
Epoch 2440, val loss: 1.210775375366211
Epoch 2450, training loss: 62.20659255981445 = 0.008864255622029305 + 10.0 * 6.219772815704346
Epoch 2450, val loss: 1.2127535343170166
Epoch 2460, training loss: 62.13869857788086 = 0.008767972700297832 + 10.0 * 6.212993144989014
Epoch 2460, val loss: 1.2149689197540283
Epoch 2470, training loss: 62.12311553955078 = 0.008674440905451775 + 10.0 * 6.211443901062012
Epoch 2470, val loss: 1.2170902490615845
Epoch 2480, training loss: 62.11640167236328 = 0.008587426505982876 + 10.0 * 6.210781574249268
Epoch 2480, val loss: 1.2193918228149414
Epoch 2490, training loss: 62.11803436279297 = 0.008501715958118439 + 10.0 * 6.210953235626221
Epoch 2490, val loss: 1.2215521335601807
Epoch 2500, training loss: 62.24120330810547 = 0.008421763777732849 + 10.0 * 6.223278045654297
Epoch 2500, val loss: 1.223623514175415
Epoch 2510, training loss: 62.15454864501953 = 0.008325599133968353 + 10.0 * 6.2146220207214355
Epoch 2510, val loss: 1.2253080606460571
Epoch 2520, training loss: 62.13672637939453 = 0.00824069045484066 + 10.0 * 6.212848663330078
Epoch 2520, val loss: 1.2275172472000122
Epoch 2530, training loss: 62.1203727722168 = 0.00815658364444971 + 10.0 * 6.211221694946289
Epoch 2530, val loss: 1.2294752597808838
Epoch 2540, training loss: 62.11319351196289 = 0.008075470104813576 + 10.0 * 6.210511684417725
Epoch 2540, val loss: 1.2314523458480835
Epoch 2550, training loss: 62.1929817199707 = 0.007996425963938236 + 10.0 * 6.218498706817627
Epoch 2550, val loss: 1.2334825992584229
Epoch 2560, training loss: 62.13620376586914 = 0.007915757596492767 + 10.0 * 6.212828636169434
Epoch 2560, val loss: 1.2355183362960815
Epoch 2570, training loss: 62.111785888671875 = 0.007835556752979755 + 10.0 * 6.210394859313965
Epoch 2570, val loss: 1.2373933792114258
Epoch 2580, training loss: 62.104618072509766 = 0.00776258111000061 + 10.0 * 6.209685325622559
Epoch 2580, val loss: 1.2395299673080444
Epoch 2590, training loss: 62.13093185424805 = 0.0076896981336176395 + 10.0 * 6.212324142456055
Epoch 2590, val loss: 1.2413663864135742
Epoch 2600, training loss: 62.11493682861328 = 0.007614346221089363 + 10.0 * 6.210732460021973
Epoch 2600, val loss: 1.2433475255966187
Epoch 2610, training loss: 62.12153625488281 = 0.007540300954133272 + 10.0 * 6.211399555206299
Epoch 2610, val loss: 1.2453269958496094
Epoch 2620, training loss: 62.12538528442383 = 0.007467812858521938 + 10.0 * 6.211791515350342
Epoch 2620, val loss: 1.247130274772644
Epoch 2630, training loss: 62.120304107666016 = 0.007395684253424406 + 10.0 * 6.2112908363342285
Epoch 2630, val loss: 1.249029278755188
Epoch 2640, training loss: 62.10704040527344 = 0.007326040416955948 + 10.0 * 6.2099714279174805
Epoch 2640, val loss: 1.2508858442306519
Epoch 2650, training loss: 62.13809585571289 = 0.007258228491991758 + 10.0 * 6.213083744049072
Epoch 2650, val loss: 1.2528504133224487
Epoch 2660, training loss: 62.10200881958008 = 0.007190045900642872 + 10.0 * 6.209481716156006
Epoch 2660, val loss: 1.2546868324279785
Epoch 2670, training loss: 62.093936920166016 = 0.007126053795218468 + 10.0 * 6.208681106567383
Epoch 2670, val loss: 1.256801962852478
Epoch 2680, training loss: 62.12220764160156 = 0.007063749711960554 + 10.0 * 6.211514472961426
Epoch 2680, val loss: 1.2585511207580566
Epoch 2690, training loss: 62.09037399291992 = 0.006996293552219868 + 10.0 * 6.208337783813477
Epoch 2690, val loss: 1.2603423595428467
Epoch 2700, training loss: 62.09952926635742 = 0.006932368967682123 + 10.0 * 6.209259986877441
Epoch 2700, val loss: 1.2620205879211426
Epoch 2710, training loss: 62.09994125366211 = 0.006871482357382774 + 10.0 * 6.2093071937561035
Epoch 2710, val loss: 1.2639029026031494
Epoch 2720, training loss: 62.12408447265625 = 0.006811751052737236 + 10.0 * 6.211727142333984
Epoch 2720, val loss: 1.265785813331604
Epoch 2730, training loss: 62.11845779418945 = 0.0067480700090527534 + 10.0 * 6.2111711502075195
Epoch 2730, val loss: 1.2673801183700562
Epoch 2740, training loss: 62.09910583496094 = 0.006686022505164146 + 10.0 * 6.20924186706543
Epoch 2740, val loss: 1.269028663635254
Epoch 2750, training loss: 62.110740661621094 = 0.00662892684340477 + 10.0 * 6.210411071777344
Epoch 2750, val loss: 1.2708479166030884
Epoch 2760, training loss: 62.077667236328125 = 0.006570018827915192 + 10.0 * 6.2071099281311035
Epoch 2760, val loss: 1.2726598978042603
Epoch 2770, training loss: 62.07878112792969 = 0.006515317130833864 + 10.0 * 6.207226753234863
Epoch 2770, val loss: 1.2744605541229248
Epoch 2780, training loss: 62.11840057373047 = 0.006461441982537508 + 10.0 * 6.211194038391113
Epoch 2780, val loss: 1.276182770729065
Epoch 2790, training loss: 62.107086181640625 = 0.006404357496649027 + 10.0 * 6.210068225860596
Epoch 2790, val loss: 1.277639389038086
Epoch 2800, training loss: 62.07550811767578 = 0.0063486057333648205 + 10.0 * 6.206915855407715
Epoch 2800, val loss: 1.2795079946517944
Epoch 2810, training loss: 62.07605743408203 = 0.00629577599465847 + 10.0 * 6.206976413726807
Epoch 2810, val loss: 1.2811757326126099
Epoch 2820, training loss: 62.09773254394531 = 0.006243662443011999 + 10.0 * 6.20914888381958
Epoch 2820, val loss: 1.2827271223068237
Epoch 2830, training loss: 62.069637298583984 = 0.006191409192979336 + 10.0 * 6.2063446044921875
Epoch 2830, val loss: 1.2845264673233032
Epoch 2840, training loss: 62.085533142089844 = 0.0061418102122843266 + 10.0 * 6.207939147949219
Epoch 2840, val loss: 1.2863366603851318
Epoch 2850, training loss: 62.15757369995117 = 0.006091890390962362 + 10.0 * 6.215147972106934
Epoch 2850, val loss: 1.2878175973892212
Epoch 2860, training loss: 62.09716796875 = 0.006036046426743269 + 10.0 * 6.209113121032715
Epoch 2860, val loss: 1.2892118692398071
Epoch 2870, training loss: 62.0671272277832 = 0.005987443495541811 + 10.0 * 6.206113815307617
Epoch 2870, val loss: 1.290945053100586
Epoch 2880, training loss: 62.05948257446289 = 0.00594016257673502 + 10.0 * 6.2053542137146
Epoch 2880, val loss: 1.292617917060852
Epoch 2890, training loss: 62.07888412475586 = 0.005895246751606464 + 10.0 * 6.20729923248291
Epoch 2890, val loss: 1.2942163944244385
Epoch 2900, training loss: 62.08999252319336 = 0.005847977474331856 + 10.0 * 6.208414554595947
Epoch 2900, val loss: 1.2957357168197632
Epoch 2910, training loss: 62.099063873291016 = 0.005801165476441383 + 10.0 * 6.209326267242432
Epoch 2910, val loss: 1.2973215579986572
Epoch 2920, training loss: 62.06139373779297 = 0.005753019824624062 + 10.0 * 6.205564022064209
Epoch 2920, val loss: 1.2987874746322632
Epoch 2930, training loss: 62.06174850463867 = 0.005709697958081961 + 10.0 * 6.20560359954834
Epoch 2930, val loss: 1.3004064559936523
Epoch 2940, training loss: 62.12239074707031 = 0.005667570047080517 + 10.0 * 6.211672306060791
Epoch 2940, val loss: 1.3018985986709595
Epoch 2950, training loss: 62.07099914550781 = 0.005621438845992088 + 10.0 * 6.20653772354126
Epoch 2950, val loss: 1.3034979104995728
Epoch 2960, training loss: 62.04823684692383 = 0.005577405449002981 + 10.0 * 6.20426607131958
Epoch 2960, val loss: 1.3050634860992432
Epoch 2970, training loss: 62.046966552734375 = 0.005536775570362806 + 10.0 * 6.204143047332764
Epoch 2970, val loss: 1.306591510772705
Epoch 2980, training loss: 62.10202407836914 = 0.005496820900589228 + 10.0 * 6.209652900695801
Epoch 2980, val loss: 1.3080281019210815
Epoch 2990, training loss: 62.05805206298828 = 0.005453619174659252 + 10.0 * 6.205259799957275
Epoch 2990, val loss: 1.3094377517700195
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 87.91963958740234 = 1.9513558149337769 + 10.0 * 8.59682846069336
Epoch 0, val loss: 1.9587135314941406
Epoch 10, training loss: 87.9029769897461 = 1.941554307937622 + 10.0 * 8.596142768859863
Epoch 10, val loss: 1.9490126371383667
Epoch 20, training loss: 87.84410095214844 = 1.9294395446777344 + 10.0 * 8.591465950012207
Epoch 20, val loss: 1.9364924430847168
Epoch 30, training loss: 87.49551391601562 = 1.9138562679290771 + 10.0 * 8.558165550231934
Epoch 30, val loss: 1.9201034307479858
Epoch 40, training loss: 85.0162124633789 = 1.8948622941970825 + 10.0 * 8.312135696411133
Epoch 40, val loss: 1.9001762866973877
Epoch 50, training loss: 76.07896423339844 = 1.873245120048523 + 10.0 * 7.420572280883789
Epoch 50, val loss: 1.8780722618103027
Epoch 60, training loss: 73.86109161376953 = 1.8580983877182007 + 10.0 * 7.200299263000488
Epoch 60, val loss: 1.8639053106307983
Epoch 70, training loss: 72.6501235961914 = 1.843816876411438 + 10.0 * 7.080630302429199
Epoch 70, val loss: 1.8497228622436523
Epoch 80, training loss: 71.43173217773438 = 1.8294636011123657 + 10.0 * 6.9602274894714355
Epoch 80, val loss: 1.8356870412826538
Epoch 90, training loss: 70.449951171875 = 1.8161547183990479 + 10.0 * 6.863379955291748
Epoch 90, val loss: 1.822698712348938
Epoch 100, training loss: 69.53992462158203 = 1.804168462753296 + 10.0 * 6.773575782775879
Epoch 100, val loss: 1.8112560510635376
Epoch 110, training loss: 68.770751953125 = 1.7929812669754028 + 10.0 * 6.697776794433594
Epoch 110, val loss: 1.8005338907241821
Epoch 120, training loss: 68.20738983154297 = 1.7813408374786377 + 10.0 * 6.642604827880859
Epoch 120, val loss: 1.7890357971191406
Epoch 130, training loss: 67.81119537353516 = 1.7688580751419067 + 10.0 * 6.604233741760254
Epoch 130, val loss: 1.7768272161483765
Epoch 140, training loss: 67.47078704833984 = 1.755580186843872 + 10.0 * 6.571521282196045
Epoch 140, val loss: 1.763903260231018
Epoch 150, training loss: 67.19097900390625 = 1.7416621446609497 + 10.0 * 6.544931888580322
Epoch 150, val loss: 1.750677227973938
Epoch 160, training loss: 66.97743225097656 = 1.7268476486206055 + 10.0 * 6.525058746337891
Epoch 160, val loss: 1.736774206161499
Epoch 170, training loss: 66.77276611328125 = 1.7105709314346313 + 10.0 * 6.506219863891602
Epoch 170, val loss: 1.721832275390625
Epoch 180, training loss: 66.58781433105469 = 1.6928889751434326 + 10.0 * 6.489492893218994
Epoch 180, val loss: 1.7057353258132935
Epoch 190, training loss: 66.42121124267578 = 1.6737301349639893 + 10.0 * 6.474748611450195
Epoch 190, val loss: 1.6882750988006592
Epoch 200, training loss: 66.27571105957031 = 1.6527951955795288 + 10.0 * 6.462291717529297
Epoch 200, val loss: 1.6693443059921265
Epoch 210, training loss: 66.13668060302734 = 1.630178689956665 + 10.0 * 6.450649738311768
Epoch 210, val loss: 1.6490135192871094
Epoch 220, training loss: 66.00714111328125 = 1.6058855056762695 + 10.0 * 6.440124988555908
Epoch 220, val loss: 1.6272538900375366
Epoch 230, training loss: 65.9417495727539 = 1.5798907279968262 + 10.0 * 6.436185836791992
Epoch 230, val loss: 1.6039906740188599
Epoch 240, training loss: 65.78243255615234 = 1.5521695613861084 + 10.0 * 6.423026084899902
Epoch 240, val loss: 1.5793673992156982
Epoch 250, training loss: 65.67306518554688 = 1.5231342315673828 + 10.0 * 6.4149932861328125
Epoch 250, val loss: 1.553817868232727
Epoch 260, training loss: 65.57418060302734 = 1.4929282665252686 + 10.0 * 6.408124923706055
Epoch 260, val loss: 1.527207374572754
Epoch 270, training loss: 65.51631164550781 = 1.4616007804870605 + 10.0 * 6.405470848083496
Epoch 270, val loss: 1.49984872341156
Epoch 280, training loss: 65.40043640136719 = 1.4296637773513794 + 10.0 * 6.3970770835876465
Epoch 280, val loss: 1.4720160961151123
Epoch 290, training loss: 65.3046646118164 = 1.3973345756530762 + 10.0 * 6.390733242034912
Epoch 290, val loss: 1.4440228939056396
Epoch 300, training loss: 65.259521484375 = 1.3647940158843994 + 10.0 * 6.389472961425781
Epoch 300, val loss: 1.4163000583648682
Epoch 310, training loss: 65.13668823242188 = 1.3321150541305542 + 10.0 * 6.380457401275635
Epoch 310, val loss: 1.3882818222045898
Epoch 320, training loss: 65.06094360351562 = 1.299786925315857 + 10.0 * 6.376115798950195
Epoch 320, val loss: 1.3609124422073364
Epoch 330, training loss: 64.977783203125 = 1.2677702903747559 + 10.0 * 6.371001243591309
Epoch 330, val loss: 1.3340387344360352
Epoch 340, training loss: 64.99176025390625 = 1.2361974716186523 + 10.0 * 6.375556468963623
Epoch 340, val loss: 1.3076083660125732
Epoch 350, training loss: 64.85201263427734 = 1.204856038093567 + 10.0 * 6.364715576171875
Epoch 350, val loss: 1.2818938493728638
Epoch 360, training loss: 64.76542663574219 = 1.1743062734603882 + 10.0 * 6.359111785888672
Epoch 360, val loss: 1.2569276094436646
Epoch 370, training loss: 64.68990325927734 = 1.1443496942520142 + 10.0 * 6.354555606842041
Epoch 370, val loss: 1.2328134775161743
Epoch 380, training loss: 64.6403579711914 = 1.1150128841400146 + 10.0 * 6.352534770965576
Epoch 380, val loss: 1.209447979927063
Epoch 390, training loss: 64.58589935302734 = 1.0862138271331787 + 10.0 * 6.349968433380127
Epoch 390, val loss: 1.186592936515808
Epoch 400, training loss: 64.50043487548828 = 1.058051347732544 + 10.0 * 6.344238758087158
Epoch 400, val loss: 1.1646181344985962
Epoch 410, training loss: 64.47114562988281 = 1.03053617477417 + 10.0 * 6.344060897827148
Epoch 410, val loss: 1.1435012817382812
Epoch 420, training loss: 64.39897918701172 = 1.0036712884902954 + 10.0 * 6.3395304679870605
Epoch 420, val loss: 1.1230278015136719
Epoch 430, training loss: 64.33004760742188 = 0.9774653911590576 + 10.0 * 6.3352580070495605
Epoch 430, val loss: 1.1033203601837158
Epoch 440, training loss: 64.28373718261719 = 0.9519869685173035 + 10.0 * 6.333175182342529
Epoch 440, val loss: 1.0844168663024902
Epoch 450, training loss: 64.2648696899414 = 0.9271071553230286 + 10.0 * 6.333776473999023
Epoch 450, val loss: 1.0662389993667603
Epoch 460, training loss: 64.18495178222656 = 0.9028977751731873 + 10.0 * 6.328205585479736
Epoch 460, val loss: 1.0491929054260254
Epoch 470, training loss: 64.12971496582031 = 0.879511833190918 + 10.0 * 6.325020790100098
Epoch 470, val loss: 1.0327738523483276
Epoch 480, training loss: 64.0798568725586 = 0.8568723797798157 + 10.0 * 6.322298526763916
Epoch 480, val loss: 1.0173606872558594
Epoch 490, training loss: 64.03248596191406 = 0.834904134273529 + 10.0 * 6.31975793838501
Epoch 490, val loss: 1.002813696861267
Epoch 500, training loss: 64.0730972290039 = 0.8135052919387817 + 10.0 * 6.325959205627441
Epoch 500, val loss: 0.9891223907470703
Epoch 510, training loss: 63.949798583984375 = 0.7926584482192993 + 10.0 * 6.315713882446289
Epoch 510, val loss: 0.9756585955619812
Epoch 520, training loss: 63.91257858276367 = 0.7725462317466736 + 10.0 * 6.3140034675598145
Epoch 520, val loss: 0.9632655382156372
Epoch 530, training loss: 63.869903564453125 = 0.7530863285064697 + 10.0 * 6.311681747436523
Epoch 530, val loss: 0.951799750328064
Epoch 540, training loss: 63.86921310424805 = 0.7342169880867004 + 10.0 * 6.313499450683594
Epoch 540, val loss: 0.9411866068840027
Epoch 550, training loss: 63.838504791259766 = 0.7156991362571716 + 10.0 * 6.312280654907227
Epoch 550, val loss: 0.9308955073356628
Epoch 560, training loss: 63.7631950378418 = 0.6977260112762451 + 10.0 * 6.306546688079834
Epoch 560, val loss: 0.9214080572128296
Epoch 570, training loss: 63.722412109375 = 0.6803375482559204 + 10.0 * 6.304207801818848
Epoch 570, val loss: 0.9127570986747742
Epoch 580, training loss: 63.68647766113281 = 0.6634230017662048 + 10.0 * 6.302305698394775
Epoch 580, val loss: 0.9046070575714111
Epoch 590, training loss: 63.69608688354492 = 0.6468786597251892 + 10.0 * 6.3049211502075195
Epoch 590, val loss: 0.8968461751937866
Epoch 600, training loss: 63.64467239379883 = 0.6305568218231201 + 10.0 * 6.3014116287231445
Epoch 600, val loss: 0.8894924521446228
Epoch 610, training loss: 63.625511169433594 = 0.6146330833435059 + 10.0 * 6.301087856292725
Epoch 610, val loss: 0.8826866745948792
Epoch 620, training loss: 63.58469772338867 = 0.5989927053451538 + 10.0 * 6.29857063293457
Epoch 620, val loss: 0.8761687874794006
Epoch 630, training loss: 63.52870559692383 = 0.5836113095283508 + 10.0 * 6.294509410858154
Epoch 630, val loss: 0.8698921203613281
Epoch 640, training loss: 63.49982452392578 = 0.5685356855392456 + 10.0 * 6.293128967285156
Epoch 640, val loss: 0.8640353083610535
Epoch 650, training loss: 63.510093688964844 = 0.553683340549469 + 10.0 * 6.29564094543457
Epoch 650, val loss: 0.8584023714065552
Epoch 660, training loss: 63.44569396972656 = 0.5388856530189514 + 10.0 * 6.290680885314941
Epoch 660, val loss: 0.8532883524894714
Epoch 670, training loss: 63.407928466796875 = 0.5243593454360962 + 10.0 * 6.288356781005859
Epoch 670, val loss: 0.8480945229530334
Epoch 680, training loss: 63.40166473388672 = 0.5100127458572388 + 10.0 * 6.289165019989014
Epoch 680, val loss: 0.843158483505249
Epoch 690, training loss: 63.390869140625 = 0.4957379102706909 + 10.0 * 6.289513111114502
Epoch 690, val loss: 0.8385655879974365
Epoch 700, training loss: 63.33173751831055 = 0.48169222474098206 + 10.0 * 6.285004615783691
Epoch 700, val loss: 0.8340279459953308
Epoch 710, training loss: 63.304317474365234 = 0.4679240882396698 + 10.0 * 6.283639430999756
Epoch 710, val loss: 0.8298394083976746
Epoch 720, training loss: 63.27931213378906 = 0.45438140630722046 + 10.0 * 6.2824931144714355
Epoch 720, val loss: 0.825894832611084
Epoch 730, training loss: 63.34156799316406 = 0.44102802872657776 + 10.0 * 6.290053844451904
Epoch 730, val loss: 0.8221284747123718
Epoch 740, training loss: 63.228172302246094 = 0.4277573823928833 + 10.0 * 6.280041694641113
Epoch 740, val loss: 0.8186987042427063
Epoch 750, training loss: 63.21002197265625 = 0.41481655836105347 + 10.0 * 6.279520511627197
Epoch 750, val loss: 0.8156328797340393
Epoch 760, training loss: 63.227073669433594 = 0.4021719992160797 + 10.0 * 6.282490253448486
Epoch 760, val loss: 0.8127357363700867
Epoch 770, training loss: 63.203094482421875 = 0.3897032141685486 + 10.0 * 6.281339168548584
Epoch 770, val loss: 0.8100542426109314
Epoch 780, training loss: 63.13349533081055 = 0.37748831510543823 + 10.0 * 6.275600910186768
Epoch 780, val loss: 0.8077614307403564
Epoch 790, training loss: 63.11350631713867 = 0.36561834812164307 + 10.0 * 6.274788856506348
Epoch 790, val loss: 0.80588299036026
Epoch 800, training loss: 63.092411041259766 = 0.35401278734207153 + 10.0 * 6.273839950561523
Epoch 800, val loss: 0.8043626546859741
Epoch 810, training loss: 63.10782241821289 = 0.3426268994808197 + 10.0 * 6.276519298553467
Epoch 810, val loss: 0.802923321723938
Epoch 820, training loss: 63.06913757324219 = 0.3314409852027893 + 10.0 * 6.273769855499268
Epoch 820, val loss: 0.8018020987510681
Epoch 830, training loss: 63.03230667114258 = 0.3205520212650299 + 10.0 * 6.271175384521484
Epoch 830, val loss: 0.8007978200912476
Epoch 840, training loss: 63.025150299072266 = 0.3099959194660187 + 10.0 * 6.271515369415283
Epoch 840, val loss: 0.8002893924713135
Epoch 850, training loss: 63.02298355102539 = 0.29966846108436584 + 10.0 * 6.272331714630127
Epoch 850, val loss: 0.8000662922859192
Epoch 860, training loss: 62.978458404541016 = 0.2895369231700897 + 10.0 * 6.268892288208008
Epoch 860, val loss: 0.7999678254127502
Epoch 870, training loss: 62.95332336425781 = 0.2797495424747467 + 10.0 * 6.267357349395752
Epoch 870, val loss: 0.8001475930213928
Epoch 880, training loss: 62.936317443847656 = 0.2702174186706543 + 10.0 * 6.266610145568848
Epoch 880, val loss: 0.8006696701049805
Epoch 890, training loss: 62.973388671875 = 0.2609400749206543 + 10.0 * 6.271245002746582
Epoch 890, val loss: 0.8013843894004822
Epoch 900, training loss: 62.9194450378418 = 0.25182586908340454 + 10.0 * 6.266761779785156
Epoch 900, val loss: 0.802196204662323
Epoch 910, training loss: 62.88941192626953 = 0.24305115640163422 + 10.0 * 6.264636039733887
Epoch 910, val loss: 0.8032861351966858
Epoch 920, training loss: 62.868534088134766 = 0.23458586633205414 + 10.0 * 6.263394832611084
Epoch 920, val loss: 0.8047266602516174
Epoch 930, training loss: 62.8890266418457 = 0.22637204825878143 + 10.0 * 6.266265392303467
Epoch 930, val loss: 0.8064177632331848
Epoch 940, training loss: 62.87275314331055 = 0.21833282709121704 + 10.0 * 6.26544189453125
Epoch 940, val loss: 0.8079243898391724
Epoch 950, training loss: 62.85725784301758 = 0.2105766236782074 + 10.0 * 6.2646684646606445
Epoch 950, val loss: 0.8099932670593262
Epoch 960, training loss: 62.80677032470703 = 0.20310115814208984 + 10.0 * 6.260366916656494
Epoch 960, val loss: 0.8118787407875061
Epoch 970, training loss: 62.7896842956543 = 0.1959180384874344 + 10.0 * 6.259376525878906
Epoch 970, val loss: 0.8143151998519897
Epoch 980, training loss: 62.77655029296875 = 0.18901216983795166 + 10.0 * 6.258753776550293
Epoch 980, val loss: 0.8169723153114319
Epoch 990, training loss: 62.78788757324219 = 0.18233512341976166 + 10.0 * 6.260555267333984
Epoch 990, val loss: 0.8198217153549194
Epoch 1000, training loss: 62.768836975097656 = 0.17585183680057526 + 10.0 * 6.259298324584961
Epoch 1000, val loss: 0.8224711418151855
Epoch 1010, training loss: 62.75183868408203 = 0.16961665451526642 + 10.0 * 6.2582221031188965
Epoch 1010, val loss: 0.8255607485771179
Epoch 1020, training loss: 62.72115707397461 = 0.16367241740226746 + 10.0 * 6.255748271942139
Epoch 1020, val loss: 0.8287946581840515
Epoch 1030, training loss: 62.71285629272461 = 0.15797168016433716 + 10.0 * 6.255488395690918
Epoch 1030, val loss: 0.832284688949585
Epoch 1040, training loss: 62.77824020385742 = 0.15248429775238037 + 10.0 * 6.262575626373291
Epoch 1040, val loss: 0.8358182311058044
Epoch 1050, training loss: 62.71110534667969 = 0.14715267717838287 + 10.0 * 6.25639533996582
Epoch 1050, val loss: 0.8394603133201599
Epoch 1060, training loss: 62.68870162963867 = 0.1420678049325943 + 10.0 * 6.254663467407227
Epoch 1060, val loss: 0.843224287033081
Epoch 1070, training loss: 62.697532653808594 = 0.13722316920757294 + 10.0 * 6.256031036376953
Epoch 1070, val loss: 0.8471383452415466
Epoch 1080, training loss: 62.669437408447266 = 0.132566899061203 + 10.0 * 6.253686904907227
Epoch 1080, val loss: 0.8513238430023193
Epoch 1090, training loss: 62.6467399597168 = 0.1281057447195053 + 10.0 * 6.251863479614258
Epoch 1090, val loss: 0.8555086255073547
Epoch 1100, training loss: 62.63725280761719 = 0.12384254485368729 + 10.0 * 6.251340866088867
Epoch 1100, val loss: 0.8598638772964478
Epoch 1110, training loss: 62.665889739990234 = 0.11976732313632965 + 10.0 * 6.254612445831299
Epoch 1110, val loss: 0.8642915487289429
Epoch 1120, training loss: 62.633567810058594 = 0.11581094563007355 + 10.0 * 6.251775741577148
Epoch 1120, val loss: 0.8687477707862854
Epoch 1130, training loss: 62.61209487915039 = 0.11203654855489731 + 10.0 * 6.250005722045898
Epoch 1130, val loss: 0.8732669353485107
Epoch 1140, training loss: 62.621238708496094 = 0.10844298452138901 + 10.0 * 6.251279354095459
Epoch 1140, val loss: 0.8779789805412292
Epoch 1150, training loss: 62.61084747314453 = 0.10496541857719421 + 10.0 * 6.250588417053223
Epoch 1150, val loss: 0.8826615214347839
Epoch 1160, training loss: 62.582984924316406 = 0.10161454975605011 + 10.0 * 6.2481369972229
Epoch 1160, val loss: 0.8874083161354065
Epoch 1170, training loss: 62.56828689575195 = 0.09843664616346359 + 10.0 * 6.246984958648682
Epoch 1170, val loss: 0.8923147916793823
Epoch 1180, training loss: 62.56262969970703 = 0.09539909660816193 + 10.0 * 6.246723175048828
Epoch 1180, val loss: 0.8972597718238831
Epoch 1190, training loss: 62.647796630859375 = 0.09246394783258438 + 10.0 * 6.255533218383789
Epoch 1190, val loss: 0.9020947217941284
Epoch 1200, training loss: 62.5705451965332 = 0.08964802324771881 + 10.0 * 6.248089790344238
Epoch 1200, val loss: 0.9072836637496948
Epoch 1210, training loss: 62.540164947509766 = 0.08693298697471619 + 10.0 * 6.245323181152344
Epoch 1210, val loss: 0.9122965931892395
Epoch 1220, training loss: 62.56138610839844 = 0.08435851335525513 + 10.0 * 6.247702598571777
Epoch 1220, val loss: 0.917366623878479
Epoch 1230, training loss: 62.5346794128418 = 0.0818626880645752 + 10.0 * 6.24528169631958
Epoch 1230, val loss: 0.9224482178688049
Epoch 1240, training loss: 62.52632141113281 = 0.07946539670228958 + 10.0 * 6.244685649871826
Epoch 1240, val loss: 0.9275957942008972
Epoch 1250, training loss: 62.5186767578125 = 0.0771721825003624 + 10.0 * 6.244150638580322
Epoch 1250, val loss: 0.9328023791313171
Epoch 1260, training loss: 62.545936584472656 = 0.07497453689575195 + 10.0 * 6.247096061706543
Epoch 1260, val loss: 0.9379785656929016
Epoch 1270, training loss: 62.52537536621094 = 0.07282815128564835 + 10.0 * 6.245254993438721
Epoch 1270, val loss: 0.943172812461853
Epoch 1280, training loss: 62.50627517700195 = 0.0707741305232048 + 10.0 * 6.2435503005981445
Epoch 1280, val loss: 0.9482940435409546
Epoch 1290, training loss: 62.479736328125 = 0.06880827248096466 + 10.0 * 6.241092681884766
Epoch 1290, val loss: 0.9535351991653442
Epoch 1300, training loss: 62.47148513793945 = 0.06693127006292343 + 10.0 * 6.240455150604248
Epoch 1300, val loss: 0.9588977694511414
Epoch 1310, training loss: 62.50363540649414 = 0.06512921303510666 + 10.0 * 6.2438507080078125
Epoch 1310, val loss: 0.9641301035881042
Epoch 1320, training loss: 62.49859619140625 = 0.06333604454994202 + 10.0 * 6.243525981903076
Epoch 1320, val loss: 0.9692177176475525
Epoch 1330, training loss: 62.480873107910156 = 0.06162559613585472 + 10.0 * 6.24192476272583
Epoch 1330, val loss: 0.9742705225944519
Epoch 1340, training loss: 62.45180130004883 = 0.0599902905523777 + 10.0 * 6.239181041717529
Epoch 1340, val loss: 0.9795565009117126
Epoch 1350, training loss: 62.44552993774414 = 0.058438029140233994 + 10.0 * 6.238709449768066
Epoch 1350, val loss: 0.9848790764808655
Epoch 1360, training loss: 62.44091796875 = 0.05693861097097397 + 10.0 * 6.23839807510376
Epoch 1360, val loss: 0.9900726079940796
Epoch 1370, training loss: 62.534202575683594 = 0.055480554699897766 + 10.0 * 6.247872352600098
Epoch 1370, val loss: 0.9952085018157959
Epoch 1380, training loss: 62.446903228759766 = 0.05404137820005417 + 10.0 * 6.239285945892334
Epoch 1380, val loss: 1.0002176761627197
Epoch 1390, training loss: 62.42586898803711 = 0.05267643555998802 + 10.0 * 6.237318992614746
Epoch 1390, val loss: 1.0053305625915527
Epoch 1400, training loss: 62.448116302490234 = 0.05137282609939575 + 10.0 * 6.2396745681762695
Epoch 1400, val loss: 1.0105340480804443
Epoch 1410, training loss: 62.42488098144531 = 0.05009704455733299 + 10.0 * 6.237478256225586
Epoch 1410, val loss: 1.0154547691345215
Epoch 1420, training loss: 62.41872024536133 = 0.04887661710381508 + 10.0 * 6.2369842529296875
Epoch 1420, val loss: 1.0205553770065308
Epoch 1430, training loss: 62.40922546386719 = 0.047699399292469025 + 10.0 * 6.236152648925781
Epoch 1430, val loss: 1.0258326530456543
Epoch 1440, training loss: 62.44209671020508 = 0.04656071960926056 + 10.0 * 6.239553451538086
Epoch 1440, val loss: 1.0307507514953613
Epoch 1450, training loss: 62.40324783325195 = 0.04544235020875931 + 10.0 * 6.235780715942383
Epoch 1450, val loss: 1.0356080532073975
Epoch 1460, training loss: 62.41100311279297 = 0.04437253624200821 + 10.0 * 6.236662864685059
Epoch 1460, val loss: 1.0406546592712402
Epoch 1470, training loss: 62.39994430541992 = 0.04333394020795822 + 10.0 * 6.235661029815674
Epoch 1470, val loss: 1.0454998016357422
Epoch 1480, training loss: 62.40043258666992 = 0.042342882603406906 + 10.0 * 6.235808849334717
Epoch 1480, val loss: 1.0506564378738403
Epoch 1490, training loss: 62.378807067871094 = 0.041369032114744186 + 10.0 * 6.233743667602539
Epoch 1490, val loss: 1.0554542541503906
Epoch 1500, training loss: 62.37819290161133 = 0.040432583540678024 + 10.0 * 6.233776092529297
Epoch 1500, val loss: 1.0603758096694946
Epoch 1510, training loss: 62.38743591308594 = 0.03953149914741516 + 10.0 * 6.234790325164795
Epoch 1510, val loss: 1.0652337074279785
Epoch 1520, training loss: 62.41618347167969 = 0.038650330156087875 + 10.0 * 6.237753391265869
Epoch 1520, val loss: 1.0701535940170288
Epoch 1530, training loss: 62.398067474365234 = 0.03778322786092758 + 10.0 * 6.23602819442749
Epoch 1530, val loss: 1.0746653079986572
Epoch 1540, training loss: 62.35430908203125 = 0.03694208711385727 + 10.0 * 6.231736660003662
Epoch 1540, val loss: 1.0794943571090698
Epoch 1550, training loss: 62.350547790527344 = 0.036148957908153534 + 10.0 * 6.23144006729126
Epoch 1550, val loss: 1.0843133926391602
Epoch 1560, training loss: 62.34299087524414 = 0.035387225449085236 + 10.0 * 6.230760097503662
Epoch 1560, val loss: 1.0891362428665161
Epoch 1570, training loss: 62.34245681762695 = 0.03464550897479057 + 10.0 * 6.230781078338623
Epoch 1570, val loss: 1.0938665866851807
Epoch 1580, training loss: 62.45043182373047 = 0.03391454368829727 + 10.0 * 6.241652011871338
Epoch 1580, val loss: 1.0985289812088013
Epoch 1590, training loss: 62.35047149658203 = 0.03320149704813957 + 10.0 * 6.23172664642334
Epoch 1590, val loss: 1.1029880046844482
Epoch 1600, training loss: 62.32325744628906 = 0.03250858187675476 + 10.0 * 6.229074954986572
Epoch 1600, val loss: 1.1076887845993042
Epoch 1610, training loss: 62.31983947753906 = 0.031853724271059036 + 10.0 * 6.228798866271973
Epoch 1610, val loss: 1.1123582124710083
Epoch 1620, training loss: 62.343650817871094 = 0.03121819905936718 + 10.0 * 6.231243133544922
Epoch 1620, val loss: 1.116944670677185
Epoch 1630, training loss: 62.34517288208008 = 0.03058566525578499 + 10.0 * 6.23145866394043
Epoch 1630, val loss: 1.1214256286621094
Epoch 1640, training loss: 62.353797912597656 = 0.02996772527694702 + 10.0 * 6.232382774353027
Epoch 1640, val loss: 1.1257104873657227
Epoch 1650, training loss: 62.314571380615234 = 0.029373014345765114 + 10.0 * 6.228519916534424
Epoch 1650, val loss: 1.1302151679992676
Epoch 1660, training loss: 62.303470611572266 = 0.028805451467633247 + 10.0 * 6.227466583251953
Epoch 1660, val loss: 1.1347209215164185
Epoch 1670, training loss: 62.31719207763672 = 0.02825954183936119 + 10.0 * 6.228893280029297
Epoch 1670, val loss: 1.1392252445220947
Epoch 1680, training loss: 62.32189178466797 = 0.027711346745491028 + 10.0 * 6.2294182777404785
Epoch 1680, val loss: 1.143424153327942
Epoch 1690, training loss: 62.29684066772461 = 0.027175968512892723 + 10.0 * 6.226966381072998
Epoch 1690, val loss: 1.1476622819900513
Epoch 1700, training loss: 62.289817810058594 = 0.026666441932320595 + 10.0 * 6.226315498352051
Epoch 1700, val loss: 1.1519235372543335
Epoch 1710, training loss: 62.28703689575195 = 0.026176301762461662 + 10.0 * 6.226086139678955
Epoch 1710, val loss: 1.1563537120819092
Epoch 1720, training loss: 62.354042053222656 = 0.025701027363538742 + 10.0 * 6.232834339141846
Epoch 1720, val loss: 1.160677433013916
Epoch 1730, training loss: 62.30656433105469 = 0.02521967701613903 + 10.0 * 6.228134632110596
Epoch 1730, val loss: 1.1642905473709106
Epoch 1740, training loss: 62.310791015625 = 0.02475680783390999 + 10.0 * 6.228603363037109
Epoch 1740, val loss: 1.1686328649520874
Epoch 1750, training loss: 62.287872314453125 = 0.024312885478138924 + 10.0 * 6.226356029510498
Epoch 1750, val loss: 1.1728776693344116
Epoch 1760, training loss: 62.3049430847168 = 0.023884233087301254 + 10.0 * 6.2281060218811035
Epoch 1760, val loss: 1.1770493984222412
Epoch 1770, training loss: 62.295772552490234 = 0.02345682494342327 + 10.0 * 6.227231502532959
Epoch 1770, val loss: 1.181016206741333
Epoch 1780, training loss: 62.27678298950195 = 0.023043647408485413 + 10.0 * 6.2253737449646
Epoch 1780, val loss: 1.1849844455718994
Epoch 1790, training loss: 62.264564514160156 = 0.02264449931681156 + 10.0 * 6.224192142486572
Epoch 1790, val loss: 1.189120888710022
Epoch 1800, training loss: 62.27922058105469 = 0.022259226068854332 + 10.0 * 6.225696086883545
Epoch 1800, val loss: 1.1931895017623901
Epoch 1810, training loss: 62.315040588378906 = 0.021875927224755287 + 10.0 * 6.229316234588623
Epoch 1810, val loss: 1.1967558860778809
Epoch 1820, training loss: 62.275352478027344 = 0.021501149982213974 + 10.0 * 6.2253851890563965
Epoch 1820, val loss: 1.200914740562439
Epoch 1830, training loss: 62.27620315551758 = 0.021140482276678085 + 10.0 * 6.22550630569458
Epoch 1830, val loss: 1.2047568559646606
Epoch 1840, training loss: 62.279048919677734 = 0.020788656547665596 + 10.0 * 6.225825786590576
Epoch 1840, val loss: 1.2087368965148926
Epoch 1850, training loss: 62.26839828491211 = 0.02044062316417694 + 10.0 * 6.224795818328857
Epoch 1850, val loss: 1.2122405767440796
Epoch 1860, training loss: 62.24263000488281 = 0.020105767995119095 + 10.0 * 6.222252368927002
Epoch 1860, val loss: 1.2162126302719116
Epoch 1870, training loss: 62.2426643371582 = 0.01978282444179058 + 10.0 * 6.222288131713867
Epoch 1870, val loss: 1.2199147939682007
Epoch 1880, training loss: 62.24443435668945 = 0.01946915313601494 + 10.0 * 6.222496509552002
Epoch 1880, val loss: 1.2236436605453491
Epoch 1890, training loss: 62.276084899902344 = 0.019159309566020966 + 10.0 * 6.2256927490234375
Epoch 1890, val loss: 1.227265477180481
Epoch 1900, training loss: 62.295040130615234 = 0.01884598284959793 + 10.0 * 6.227619647979736
Epoch 1900, val loss: 1.2308142185211182
Epoch 1910, training loss: 62.241371154785156 = 0.01854638382792473 + 10.0 * 6.222282409667969
Epoch 1910, val loss: 1.2343695163726807
Epoch 1920, training loss: 62.221927642822266 = 0.01825716905295849 + 10.0 * 6.220366954803467
Epoch 1920, val loss: 1.2381786108016968
Epoch 1930, training loss: 62.21953582763672 = 0.017979001626372337 + 10.0 * 6.220155715942383
Epoch 1930, val loss: 1.2417641878128052
Epoch 1940, training loss: 62.25271987915039 = 0.017707839608192444 + 10.0 * 6.223501205444336
Epoch 1940, val loss: 1.2452274560928345
Epoch 1950, training loss: 62.23076629638672 = 0.017432333901524544 + 10.0 * 6.2213335037231445
Epoch 1950, val loss: 1.248824954032898
Epoch 1960, training loss: 62.23078536987305 = 0.017160536721348763 + 10.0 * 6.221362113952637
Epoch 1960, val loss: 1.2519514560699463
Epoch 1970, training loss: 62.22336196899414 = 0.016904445365071297 + 10.0 * 6.220645904541016
Epoch 1970, val loss: 1.255622386932373
Epoch 1980, training loss: 62.230682373046875 = 0.016656139865517616 + 10.0 * 6.221402645111084
Epoch 1980, val loss: 1.2591139078140259
Epoch 1990, training loss: 62.21333694458008 = 0.016408512368798256 + 10.0 * 6.219693183898926
Epoch 1990, val loss: 1.2625726461410522
Epoch 2000, training loss: 62.206485748291016 = 0.016169369220733643 + 10.0 * 6.21903133392334
Epoch 2000, val loss: 1.2659085988998413
Epoch 2010, training loss: 62.23058319091797 = 0.015938816592097282 + 10.0 * 6.22146463394165
Epoch 2010, val loss: 1.2691552639007568
Epoch 2020, training loss: 62.2539176940918 = 0.015703625977039337 + 10.0 * 6.22382116317749
Epoch 2020, val loss: 1.2724368572235107
Epoch 2030, training loss: 62.21040344238281 = 0.015471858903765678 + 10.0 * 6.2194929122924805
Epoch 2030, val loss: 1.2753790616989136
Epoch 2040, training loss: 62.199092864990234 = 0.01525053195655346 + 10.0 * 6.218384265899658
Epoch 2040, val loss: 1.2788671255111694
Epoch 2050, training loss: 62.191368103027344 = 0.015041639097034931 + 10.0 * 6.21763277053833
Epoch 2050, val loss: 1.282227635383606
Epoch 2060, training loss: 62.20186233520508 = 0.014837097376585007 + 10.0 * 6.218702793121338
Epoch 2060, val loss: 1.2854743003845215
Epoch 2070, training loss: 62.21604919433594 = 0.014629529789090157 + 10.0 * 6.220141887664795
Epoch 2070, val loss: 1.288643479347229
Epoch 2080, training loss: 62.19261932373047 = 0.014422954991459846 + 10.0 * 6.217819690704346
Epoch 2080, val loss: 1.2916837930679321
Epoch 2090, training loss: 62.19833755493164 = 0.014228700660169125 + 10.0 * 6.218410968780518
Epoch 2090, val loss: 1.2950434684753418
Epoch 2100, training loss: 62.21381759643555 = 0.014035443775355816 + 10.0 * 6.219978332519531
Epoch 2100, val loss: 1.2981324195861816
Epoch 2110, training loss: 62.225257873535156 = 0.013847063295543194 + 10.0 * 6.2211408615112305
Epoch 2110, val loss: 1.300970435142517
Epoch 2120, training loss: 62.19800567626953 = 0.01365557499229908 + 10.0 * 6.218434810638428
Epoch 2120, val loss: 1.3038798570632935
Epoch 2130, training loss: 62.17815399169922 = 0.013474301435053349 + 10.0 * 6.21646785736084
Epoch 2130, val loss: 1.3069387674331665
Epoch 2140, training loss: 62.16929626464844 = 0.013299813494086266 + 10.0 * 6.21560001373291
Epoch 2140, val loss: 1.3101853132247925
Epoch 2150, training loss: 62.17369079589844 = 0.01313004083931446 + 10.0 * 6.216055870056152
Epoch 2150, val loss: 1.3131555318832397
Epoch 2160, training loss: 62.229496002197266 = 0.01296183094382286 + 10.0 * 6.221653461456299
Epoch 2160, val loss: 1.3159234523773193
Epoch 2170, training loss: 62.234214782714844 = 0.012786849401891232 + 10.0 * 6.222142696380615
Epoch 2170, val loss: 1.3186062574386597
Epoch 2180, training loss: 62.17860794067383 = 0.012618277221918106 + 10.0 * 6.216598987579346
Epoch 2180, val loss: 1.321539282798767
Epoch 2190, training loss: 62.16033172607422 = 0.012457738630473614 + 10.0 * 6.214787483215332
Epoch 2190, val loss: 1.324506402015686
Epoch 2200, training loss: 62.15578079223633 = 0.012305596843361855 + 10.0 * 6.2143473625183105
Epoch 2200, val loss: 1.3275173902511597
Epoch 2210, training loss: 62.17363357543945 = 0.01215621642768383 + 10.0 * 6.216147422790527
Epoch 2210, val loss: 1.3304438591003418
Epoch 2220, training loss: 62.172386169433594 = 0.012003317475318909 + 10.0 * 6.216038227081299
Epoch 2220, val loss: 1.3331059217453003
Epoch 2230, training loss: 62.17594528198242 = 0.011852256022393703 + 10.0 * 6.216409206390381
Epoch 2230, val loss: 1.3356596231460571
Epoch 2240, training loss: 62.21495819091797 = 0.011708991602063179 + 10.0 * 6.220324993133545
Epoch 2240, val loss: 1.3386688232421875
Epoch 2250, training loss: 62.158119201660156 = 0.011559531092643738 + 10.0 * 6.214655876159668
Epoch 2250, val loss: 1.3408539295196533
Epoch 2260, training loss: 62.149234771728516 = 0.01141940988600254 + 10.0 * 6.213781356811523
Epoch 2260, val loss: 1.3438993692398071
Epoch 2270, training loss: 62.17771911621094 = 0.011285074055194855 + 10.0 * 6.216643333435059
Epoch 2270, val loss: 1.3466562032699585
Epoch 2280, training loss: 62.156898498535156 = 0.011149771511554718 + 10.0 * 6.214574813842773
Epoch 2280, val loss: 1.3490626811981201
Epoch 2290, training loss: 62.14939498901367 = 0.011014865711331367 + 10.0 * 6.21383810043335
Epoch 2290, val loss: 1.351624846458435
Epoch 2300, training loss: 62.14591598510742 = 0.010885861702263355 + 10.0 * 6.213502883911133
Epoch 2300, val loss: 1.3544412851333618
Epoch 2310, training loss: 62.151493072509766 = 0.010761009529232979 + 10.0 * 6.214073181152344
Epoch 2310, val loss: 1.3568631410598755
Epoch 2320, training loss: 62.16448974609375 = 0.010635978542268276 + 10.0 * 6.215385437011719
Epoch 2320, val loss: 1.3594093322753906
Epoch 2330, training loss: 62.14702224731445 = 0.01051002275198698 + 10.0 * 6.213651180267334
Epoch 2330, val loss: 1.3618202209472656
Epoch 2340, training loss: 62.15182876586914 = 0.010388732887804508 + 10.0 * 6.214144229888916
Epoch 2340, val loss: 1.3641071319580078
Epoch 2350, training loss: 62.16278076171875 = 0.010269575752317905 + 10.0 * 6.2152509689331055
Epoch 2350, val loss: 1.3669731616973877
Epoch 2360, training loss: 62.15108871459961 = 0.010150210931897163 + 10.0 * 6.2140936851501465
Epoch 2360, val loss: 1.3693755865097046
Epoch 2370, training loss: 62.136661529541016 = 0.01003557164222002 + 10.0 * 6.212662696838379
Epoch 2370, val loss: 1.3717957735061646
Epoch 2380, training loss: 62.13798904418945 = 0.00992367323487997 + 10.0 * 6.212806701660156
Epoch 2380, val loss: 1.3741952180862427
Epoch 2390, training loss: 62.140071868896484 = 0.009813877753913403 + 10.0 * 6.2130255699157715
Epoch 2390, val loss: 1.3766679763793945
Epoch 2400, training loss: 62.135108947753906 = 0.009704945608973503 + 10.0 * 6.212540626525879
Epoch 2400, val loss: 1.3791054487228394
Epoch 2410, training loss: 62.134849548339844 = 0.00959926936775446 + 10.0 * 6.212525367736816
Epoch 2410, val loss: 1.3813438415527344
Epoch 2420, training loss: 62.11545181274414 = 0.009494852274656296 + 10.0 * 6.210595607757568
Epoch 2420, val loss: 1.3837976455688477
Epoch 2430, training loss: 62.184749603271484 = 0.009396538138389587 + 10.0 * 6.217535495758057
Epoch 2430, val loss: 1.386262059211731
Epoch 2440, training loss: 62.14393997192383 = 0.00929083488881588 + 10.0 * 6.213464736938477
Epoch 2440, val loss: 1.38828706741333
Epoch 2450, training loss: 62.122562408447266 = 0.009188555181026459 + 10.0 * 6.211337089538574
Epoch 2450, val loss: 1.3905692100524902
Epoch 2460, training loss: 62.14485168457031 = 0.009093128144741058 + 10.0 * 6.213575839996338
Epoch 2460, val loss: 1.3930062055587769
Epoch 2470, training loss: 62.110897064208984 = 0.008996469900012016 + 10.0 * 6.210190296173096
Epoch 2470, val loss: 1.395216464996338
Epoch 2480, training loss: 62.116241455078125 = 0.008903758600354195 + 10.0 * 6.210733890533447
Epoch 2480, val loss: 1.3975762128829956
Epoch 2490, training loss: 62.12091064453125 = 0.008811760693788528 + 10.0 * 6.211209774017334
Epoch 2490, val loss: 1.3998366594314575
Epoch 2500, training loss: 62.151710510253906 = 0.008720763958990574 + 10.0 * 6.214299201965332
Epoch 2500, val loss: 1.4018771648406982
Epoch 2510, training loss: 62.14491653442383 = 0.008626098744571209 + 10.0 * 6.213629245758057
Epoch 2510, val loss: 1.4038954973220825
Epoch 2520, training loss: 62.111270904541016 = 0.008538360707461834 + 10.0 * 6.210273265838623
Epoch 2520, val loss: 1.4062250852584839
Epoch 2530, training loss: 62.1209716796875 = 0.008452773094177246 + 10.0 * 6.211251735687256
Epoch 2530, val loss: 1.4082512855529785
Epoch 2540, training loss: 62.10676193237305 = 0.008367469534277916 + 10.0 * 6.209839820861816
Epoch 2540, val loss: 1.4106444120407104
Epoch 2550, training loss: 62.094825744628906 = 0.008283799514174461 + 10.0 * 6.208653926849365
Epoch 2550, val loss: 1.412736177444458
Epoch 2560, training loss: 62.10263442993164 = 0.00820412952452898 + 10.0 * 6.209443092346191
Epoch 2560, val loss: 1.4147655963897705
Epoch 2570, training loss: 62.119632720947266 = 0.008125554770231247 + 10.0 * 6.211150646209717
Epoch 2570, val loss: 1.4170111417770386
Epoch 2580, training loss: 62.141990661621094 = 0.008045009337365627 + 10.0 * 6.213394641876221
Epoch 2580, val loss: 1.419049620628357
Epoch 2590, training loss: 62.13873291015625 = 0.007961335591971874 + 10.0 * 6.213077068328857
Epoch 2590, val loss: 1.4207923412322998
Epoch 2600, training loss: 62.097991943359375 = 0.007881205528974533 + 10.0 * 6.209011077880859
Epoch 2600, val loss: 1.4225714206695557
Epoch 2610, training loss: 62.08711624145508 = 0.007806532084941864 + 10.0 * 6.207931041717529
Epoch 2610, val loss: 1.424896240234375
Epoch 2620, training loss: 62.0972900390625 = 0.007734848652034998 + 10.0 * 6.20895528793335
Epoch 2620, val loss: 1.427016258239746
Epoch 2630, training loss: 62.123626708984375 = 0.007660858798772097 + 10.0 * 6.211596488952637
Epoch 2630, val loss: 1.4287279844284058
Epoch 2640, training loss: 62.09552001953125 = 0.007586131803691387 + 10.0 * 6.2087931632995605
Epoch 2640, val loss: 1.4308741092681885
Epoch 2650, training loss: 62.108787536621094 = 0.007517632097005844 + 10.0 * 6.210126876831055
Epoch 2650, val loss: 1.4326810836791992
Epoch 2660, training loss: 62.10562515258789 = 0.007445849943906069 + 10.0 * 6.209817886352539
Epoch 2660, val loss: 1.4346368312835693
Epoch 2670, training loss: 62.0845947265625 = 0.007374925073236227 + 10.0 * 6.207722187042236
Epoch 2670, val loss: 1.4365214109420776
Epoch 2680, training loss: 62.07582092285156 = 0.007308799773454666 + 10.0 * 6.206851005554199
Epoch 2680, val loss: 1.4383083581924438
Epoch 2690, training loss: 62.07820510864258 = 0.007244383916258812 + 10.0 * 6.207096099853516
Epoch 2690, val loss: 1.4402493238449097
Epoch 2700, training loss: 62.130096435546875 = 0.007180650718510151 + 10.0 * 6.212291717529297
Epoch 2700, val loss: 1.4420945644378662
Epoch 2710, training loss: 62.12384796142578 = 0.007113597355782986 + 10.0 * 6.211673259735107
Epoch 2710, val loss: 1.443760633468628
Epoch 2720, training loss: 62.083251953125 = 0.00704367458820343 + 10.0 * 6.207621097564697
Epoch 2720, val loss: 1.4455492496490479
Epoch 2730, training loss: 62.068939208984375 = 0.006981962360441685 + 10.0 * 6.206195831298828
Epoch 2730, val loss: 1.447593331336975
Epoch 2740, training loss: 62.08740234375 = 0.006922277621924877 + 10.0 * 6.208047866821289
Epoch 2740, val loss: 1.4491400718688965
Epoch 2750, training loss: 62.096717834472656 = 0.006858512759208679 + 10.0 * 6.208985805511475
Epoch 2750, val loss: 1.4509583711624146
Epoch 2760, training loss: 62.06547546386719 = 0.006796772591769695 + 10.0 * 6.205867767333984
Epoch 2760, val loss: 1.4525643587112427
Epoch 2770, training loss: 62.054664611816406 = 0.006737848743796349 + 10.0 * 6.2047929763793945
Epoch 2770, val loss: 1.4544713497161865
Epoch 2780, training loss: 62.05500793457031 = 0.006682538893073797 + 10.0 * 6.204832553863525
Epoch 2780, val loss: 1.4563404321670532
Epoch 2790, training loss: 62.06648635864258 = 0.006628233008086681 + 10.0 * 6.2059855461120605
Epoch 2790, val loss: 1.4581328630447388
Epoch 2800, training loss: 62.096866607666016 = 0.0065718176774680614 + 10.0 * 6.209029197692871
Epoch 2800, val loss: 1.4598133563995361
Epoch 2810, training loss: 62.095821380615234 = 0.006513252854347229 + 10.0 * 6.208930969238281
Epoch 2810, val loss: 1.461221694946289
Epoch 2820, training loss: 62.070526123046875 = 0.00645572692155838 + 10.0 * 6.206407070159912
Epoch 2820, val loss: 1.4628100395202637
Epoch 2830, training loss: 62.067474365234375 = 0.006400512997061014 + 10.0 * 6.206107139587402
Epoch 2830, val loss: 1.4645817279815674
Epoch 2840, training loss: 62.05483627319336 = 0.006347940303385258 + 10.0 * 6.204848766326904
Epoch 2840, val loss: 1.4660344123840332
Epoch 2850, training loss: 62.04893112182617 = 0.006297433748841286 + 10.0 * 6.204263210296631
Epoch 2850, val loss: 1.467929720878601
Epoch 2860, training loss: 62.07431411743164 = 0.006247645244002342 + 10.0 * 6.206806659698486
Epoch 2860, val loss: 1.469280481338501
Epoch 2870, training loss: 62.07189178466797 = 0.006194374058395624 + 10.0 * 6.206569671630859
Epoch 2870, val loss: 1.4707661867141724
Epoch 2880, training loss: 62.06957244873047 = 0.006142590660601854 + 10.0 * 6.206343173980713
Epoch 2880, val loss: 1.4722964763641357
Epoch 2890, training loss: 62.05247116088867 = 0.006093272473663092 + 10.0 * 6.2046380043029785
Epoch 2890, val loss: 1.4740982055664062
Epoch 2900, training loss: 62.05011749267578 = 0.006046184338629246 + 10.0 * 6.204407215118408
Epoch 2900, val loss: 1.4758058786392212
Epoch 2910, training loss: 62.099178314208984 = 0.005999095272272825 + 10.0 * 6.209317684173584
Epoch 2910, val loss: 1.4772300720214844
Epoch 2920, training loss: 62.060874938964844 = 0.005949621554464102 + 10.0 * 6.2054924964904785
Epoch 2920, val loss: 1.478545069694519
Epoch 2930, training loss: 62.04386901855469 = 0.005901617463678122 + 10.0 * 6.203796863555908
Epoch 2930, val loss: 1.4801068305969238
Epoch 2940, training loss: 62.04481887817383 = 0.0058563775382936 + 10.0 * 6.203896522521973
Epoch 2940, val loss: 1.48164963722229
Epoch 2950, training loss: 62.06451416015625 = 0.005812404677271843 + 10.0 * 6.205870151519775
Epoch 2950, val loss: 1.483058214187622
Epoch 2960, training loss: 62.064510345458984 = 0.005765076261013746 + 10.0 * 6.205874443054199
Epoch 2960, val loss: 1.4845460653305054
Epoch 2970, training loss: 62.039249420166016 = 0.00571970222517848 + 10.0 * 6.203352928161621
Epoch 2970, val loss: 1.4858266115188599
Epoch 2980, training loss: 62.033931732177734 = 0.005677458364516497 + 10.0 * 6.202825546264648
Epoch 2980, val loss: 1.4874264001846313
Epoch 2990, training loss: 62.0396728515625 = 0.005636065267026424 + 10.0 * 6.203403949737549
Epoch 2990, val loss: 1.4889755249023438
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8481813389562468
The final CL Acc:0.76790, 0.00972, The final GNN Acc:0.84221, 0.00456
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9526])
updated graph: torch.Size([2, 10610])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.9129409790039 = 1.9445087909698486 + 10.0 * 8.596842765808105
Epoch 0, val loss: 1.9451301097869873
Epoch 10, training loss: 87.89833068847656 = 1.934844970703125 + 10.0 * 8.596348762512207
Epoch 10, val loss: 1.9358984231948853
Epoch 20, training loss: 87.85365295410156 = 1.9231606721878052 + 10.0 * 8.593049049377441
Epoch 20, val loss: 1.9242745637893677
Epoch 30, training loss: 87.62109375 = 1.9076719284057617 + 10.0 * 8.571342468261719
Epoch 30, val loss: 1.9085509777069092
Epoch 40, training loss: 86.32938385009766 = 1.889980673789978 + 10.0 * 8.443940162658691
Epoch 40, val loss: 1.8910489082336426
Epoch 50, training loss: 80.50922393798828 = 1.8724309206008911 + 10.0 * 7.8636794090271
Epoch 50, val loss: 1.8733291625976562
Epoch 60, training loss: 76.70069122314453 = 1.857598066329956 + 10.0 * 7.484309196472168
Epoch 60, val loss: 1.8592849969863892
Epoch 70, training loss: 73.42361450195312 = 1.847140908241272 + 10.0 * 7.157647609710693
Epoch 70, val loss: 1.849007487297058
Epoch 80, training loss: 71.1423110961914 = 1.8374552726745605 + 10.0 * 6.930485248565674
Epoch 80, val loss: 1.8397613763809204
Epoch 90, training loss: 70.0594711303711 = 1.8293863534927368 + 10.0 * 6.8230085372924805
Epoch 90, val loss: 1.83149254322052
Epoch 100, training loss: 69.31649780273438 = 1.8208527565002441 + 10.0 * 6.749564170837402
Epoch 100, val loss: 1.822986364364624
Epoch 110, training loss: 68.73143005371094 = 1.8125691413879395 + 10.0 * 6.6918864250183105
Epoch 110, val loss: 1.8147276639938354
Epoch 120, training loss: 68.36650848388672 = 1.8048650026321411 + 10.0 * 6.656164169311523
Epoch 120, val loss: 1.806923747062683
Epoch 130, training loss: 68.06348419189453 = 1.7972264289855957 + 10.0 * 6.6266255378723145
Epoch 130, val loss: 1.7990087270736694
Epoch 140, training loss: 67.77692413330078 = 1.7893484830856323 + 10.0 * 6.598758220672607
Epoch 140, val loss: 1.7909737825393677
Epoch 150, training loss: 67.51322937011719 = 1.7812364101409912 + 10.0 * 6.573199272155762
Epoch 150, val loss: 1.7828953266143799
Epoch 160, training loss: 67.27823638916016 = 1.7726010084152222 + 10.0 * 6.550563812255859
Epoch 160, val loss: 1.7744646072387695
Epoch 170, training loss: 67.04621887207031 = 1.763166069984436 + 10.0 * 6.5283050537109375
Epoch 170, val loss: 1.7655051946640015
Epoch 180, training loss: 66.88483428955078 = 1.752827763557434 + 10.0 * 6.513200759887695
Epoch 180, val loss: 1.7558495998382568
Epoch 190, training loss: 66.69193267822266 = 1.7414859533309937 + 10.0 * 6.495044708251953
Epoch 190, val loss: 1.7451447248458862
Epoch 200, training loss: 66.53401184082031 = 1.7288992404937744 + 10.0 * 6.480511665344238
Epoch 200, val loss: 1.7335188388824463
Epoch 210, training loss: 66.39268493652344 = 1.7149990797042847 + 10.0 * 6.467769145965576
Epoch 210, val loss: 1.7207283973693848
Epoch 220, training loss: 66.29006958007812 = 1.699594497680664 + 10.0 * 6.459047794342041
Epoch 220, val loss: 1.706673264503479
Epoch 230, training loss: 66.15039825439453 = 1.6827318668365479 + 10.0 * 6.446766376495361
Epoch 230, val loss: 1.691354751586914
Epoch 240, training loss: 66.0409164428711 = 1.6643143892288208 + 10.0 * 6.437660217285156
Epoch 240, val loss: 1.6748415231704712
Epoch 250, training loss: 65.94059753417969 = 1.6443426609039307 + 10.0 * 6.429625511169434
Epoch 250, val loss: 1.6570693254470825
Epoch 260, training loss: 65.84549713134766 = 1.622839331626892 + 10.0 * 6.422266006469727
Epoch 260, val loss: 1.6381316184997559
Epoch 270, training loss: 65.73595428466797 = 1.599937915802002 + 10.0 * 6.413601398468018
Epoch 270, val loss: 1.618189811706543
Epoch 280, training loss: 65.6447982788086 = 1.5757181644439697 + 10.0 * 6.406907558441162
Epoch 280, val loss: 1.5973433256149292
Epoch 290, training loss: 65.55913543701172 = 1.5504076480865479 + 10.0 * 6.400873184204102
Epoch 290, val loss: 1.575762152671814
Epoch 300, training loss: 65.48912811279297 = 1.5241106748580933 + 10.0 * 6.396501541137695
Epoch 300, val loss: 1.5536978244781494
Epoch 310, training loss: 65.38981628417969 = 1.4971673488616943 + 10.0 * 6.3892645835876465
Epoch 310, val loss: 1.5313302278518677
Epoch 320, training loss: 65.30564880371094 = 1.4697221517562866 + 10.0 * 6.38359260559082
Epoch 320, val loss: 1.5088390111923218
Epoch 330, training loss: 65.2278823852539 = 1.4419819116592407 + 10.0 * 6.378590106964111
Epoch 330, val loss: 1.4864062070846558
Epoch 340, training loss: 65.18557739257812 = 1.414097785949707 + 10.0 * 6.377147674560547
Epoch 340, val loss: 1.4641211032867432
Epoch 350, training loss: 65.09809875488281 = 1.386244773864746 + 10.0 * 6.371185779571533
Epoch 350, val loss: 1.4421730041503906
Epoch 360, training loss: 65.02011108398438 = 1.3585292100906372 + 10.0 * 6.3661580085754395
Epoch 360, val loss: 1.4207602739334106
Epoch 370, training loss: 64.95775604248047 = 1.3310511112213135 + 10.0 * 6.362670421600342
Epoch 370, val loss: 1.399806022644043
Epoch 380, training loss: 64.91331481933594 = 1.303732991218567 + 10.0 * 6.360958576202393
Epoch 380, val loss: 1.379273533821106
Epoch 390, training loss: 64.84491729736328 = 1.2767060995101929 + 10.0 * 6.356821537017822
Epoch 390, val loss: 1.3592039346694946
Epoch 400, training loss: 64.7718505859375 = 1.250030755996704 + 10.0 * 6.352181911468506
Epoch 400, val loss: 1.3395814895629883
Epoch 410, training loss: 64.7636489868164 = 1.2235758304595947 + 10.0 * 6.354007244110107
Epoch 410, val loss: 1.3203513622283936
Epoch 420, training loss: 64.66992950439453 = 1.1973680257797241 + 10.0 * 6.347256660461426
Epoch 420, val loss: 1.3015435934066772
Epoch 430, training loss: 64.60624694824219 = 1.1714749336242676 + 10.0 * 6.343477249145508
Epoch 430, val loss: 1.2832077741622925
Epoch 440, training loss: 64.56754302978516 = 1.1459513902664185 + 10.0 * 6.342158794403076
Epoch 440, val loss: 1.265297293663025
Epoch 450, training loss: 64.50025939941406 = 1.1207358837127686 + 10.0 * 6.337952613830566
Epoch 450, val loss: 1.2478773593902588
Epoch 460, training loss: 64.45406341552734 = 1.095922589302063 + 10.0 * 6.335813999176025
Epoch 460, val loss: 1.2307828664779663
Epoch 470, training loss: 64.41315460205078 = 1.0714527368545532 + 10.0 * 6.334170341491699
Epoch 470, val loss: 1.2143393754959106
Epoch 480, training loss: 64.35649871826172 = 1.0474711656570435 + 10.0 * 6.330903053283691
Epoch 480, val loss: 1.198231816291809
Epoch 490, training loss: 64.30339050292969 = 1.023939609527588 + 10.0 * 6.327944755554199
Epoch 490, val loss: 1.1827775239944458
Epoch 500, training loss: 64.3165054321289 = 1.0008906126022339 + 10.0 * 6.331561088562012
Epoch 500, val loss: 1.1678860187530518
Epoch 510, training loss: 64.22039031982422 = 0.9781453609466553 + 10.0 * 6.324224472045898
Epoch 510, val loss: 1.1533905267715454
Epoch 520, training loss: 64.1695327758789 = 0.9560656547546387 + 10.0 * 6.321346759796143
Epoch 520, val loss: 1.1394649744033813
Epoch 530, training loss: 64.16919708251953 = 0.9344320297241211 + 10.0 * 6.323476314544678
Epoch 530, val loss: 1.1260234117507935
Epoch 540, training loss: 64.11537170410156 = 0.9131854176521301 + 10.0 * 6.320218563079834
Epoch 540, val loss: 1.1132363080978394
Epoch 550, training loss: 64.04657745361328 = 0.8924117088317871 + 10.0 * 6.3154168128967285
Epoch 550, val loss: 1.1009924411773682
Epoch 560, training loss: 64.02015686035156 = 0.8721093535423279 + 10.0 * 6.314805030822754
Epoch 560, val loss: 1.0893595218658447
Epoch 570, training loss: 63.96916961669922 = 0.8521336317062378 + 10.0 * 6.311703681945801
Epoch 570, val loss: 1.0780231952667236
Epoch 580, training loss: 63.96011734008789 = 0.8325574994087219 + 10.0 * 6.312756061553955
Epoch 580, val loss: 1.0673521757125854
Epoch 590, training loss: 63.905391693115234 = 0.8133133053779602 + 10.0 * 6.309207916259766
Epoch 590, val loss: 1.0569261312484741
Epoch 600, training loss: 63.857566833496094 = 0.7944208383560181 + 10.0 * 6.306314468383789
Epoch 600, val loss: 1.047101616859436
Epoch 610, training loss: 63.83586120605469 = 0.7758617401123047 + 10.0 * 6.306000232696533
Epoch 610, val loss: 1.0376216173171997
Epoch 620, training loss: 63.808753967285156 = 0.7574689388275146 + 10.0 * 6.305128574371338
Epoch 620, val loss: 1.028505802154541
Epoch 630, training loss: 63.762664794921875 = 0.7393319606781006 + 10.0 * 6.302333354949951
Epoch 630, val loss: 1.0197440385818481
Epoch 640, training loss: 63.72909927368164 = 0.7215122580528259 + 10.0 * 6.3007588386535645
Epoch 640, val loss: 1.011448860168457
Epoch 650, training loss: 63.70915603637695 = 0.703941285610199 + 10.0 * 6.300521373748779
Epoch 650, val loss: 1.0035061836242676
Epoch 660, training loss: 63.65969467163086 = 0.6864289045333862 + 10.0 * 6.297326564788818
Epoch 660, val loss: 0.9959324598312378
Epoch 670, training loss: 63.63966751098633 = 0.6691857576370239 + 10.0 * 6.297048091888428
Epoch 670, val loss: 0.9886271357536316
Epoch 680, training loss: 63.602874755859375 = 0.6520876884460449 + 10.0 * 6.295078754425049
Epoch 680, val loss: 0.9816939830780029
Epoch 690, training loss: 63.5716667175293 = 0.635156512260437 + 10.0 * 6.293651103973389
Epoch 690, val loss: 0.9749884605407715
Epoch 700, training loss: 63.536773681640625 = 0.6184346079826355 + 10.0 * 6.291833877563477
Epoch 700, val loss: 0.9686397910118103
Epoch 710, training loss: 63.51980209350586 = 0.601921558380127 + 10.0 * 6.291788101196289
Epoch 710, val loss: 0.9625587463378906
Epoch 720, training loss: 63.48192596435547 = 0.5855345129966736 + 10.0 * 6.289639472961426
Epoch 720, val loss: 0.9568431973457336
Epoch 730, training loss: 63.482994079589844 = 0.5693591237068176 + 10.0 * 6.291363716125488
Epoch 730, val loss: 0.9512363076210022
Epoch 740, training loss: 63.431053161621094 = 0.5533252358436584 + 10.0 * 6.2877726554870605
Epoch 740, val loss: 0.9462045431137085
Epoch 750, training loss: 63.397037506103516 = 0.5375949740409851 + 10.0 * 6.28594446182251
Epoch 750, val loss: 0.9413201212882996
Epoch 760, training loss: 63.36847686767578 = 0.5220960974693298 + 10.0 * 6.284638404846191
Epoch 760, val loss: 0.9368529915809631
Epoch 770, training loss: 63.39780807495117 = 0.5067861676216125 + 10.0 * 6.289102077484131
Epoch 770, val loss: 0.9325771927833557
Epoch 780, training loss: 63.34745407104492 = 0.4917696714401245 + 10.0 * 6.285568714141846
Epoch 780, val loss: 0.928717851638794
Epoch 790, training loss: 63.28901290893555 = 0.47697311639785767 + 10.0 * 6.281203746795654
Epoch 790, val loss: 0.9250926971435547
Epoch 800, training loss: 63.266422271728516 = 0.46256324648857117 + 10.0 * 6.280385971069336
Epoch 800, val loss: 0.9218693375587463
Epoch 810, training loss: 63.25276184082031 = 0.448494017124176 + 10.0 * 6.280426979064941
Epoch 810, val loss: 0.9190184473991394
Epoch 820, training loss: 63.234275817871094 = 0.4346328377723694 + 10.0 * 6.279964447021484
Epoch 820, val loss: 0.9163447618484497
Epoch 830, training loss: 63.20497512817383 = 0.4211558997631073 + 10.0 * 6.278382301330566
Epoch 830, val loss: 0.913920521736145
Epoch 840, training loss: 63.19153594970703 = 0.4080037474632263 + 10.0 * 6.278353214263916
Epoch 840, val loss: 0.9118273258209229
Epoch 850, training loss: 63.15686798095703 = 0.39527708292007446 + 10.0 * 6.276158809661865
Epoch 850, val loss: 0.9100580215454102
Epoch 860, training loss: 63.12800598144531 = 0.3829144239425659 + 10.0 * 6.274508953094482
Epoch 860, val loss: 0.9086099863052368
Epoch 870, training loss: 63.12340545654297 = 0.37097349762916565 + 10.0 * 6.275243282318115
Epoch 870, val loss: 0.9075077176094055
Epoch 880, training loss: 63.120033264160156 = 0.3592749536037445 + 10.0 * 6.276075839996338
Epoch 880, val loss: 0.9065803289413452
Epoch 890, training loss: 63.099761962890625 = 0.34796035289764404 + 10.0 * 6.275179862976074
Epoch 890, val loss: 0.9059368968009949
Epoch 900, training loss: 63.05865478515625 = 0.3370167315006256 + 10.0 * 6.2721638679504395
Epoch 900, val loss: 0.9054165482521057
Epoch 910, training loss: 63.031768798828125 = 0.3265218138694763 + 10.0 * 6.270524501800537
Epoch 910, val loss: 0.9052823781967163
Epoch 920, training loss: 63.03013610839844 = 0.3163745105266571 + 10.0 * 6.271376132965088
Epoch 920, val loss: 0.9054222106933594
Epoch 930, training loss: 62.99091720581055 = 0.3065119683742523 + 10.0 * 6.2684407234191895
Epoch 930, val loss: 0.9055740237236023
Epoch 940, training loss: 62.972991943359375 = 0.2969987094402313 + 10.0 * 6.267599582672119
Epoch 940, val loss: 0.9059766530990601
Epoch 950, training loss: 62.959007263183594 = 0.2878411114215851 + 10.0 * 6.267116546630859
Epoch 950, val loss: 0.9065961241722107
Epoch 960, training loss: 63.022125244140625 = 0.279021292924881 + 10.0 * 6.274310111999512
Epoch 960, val loss: 0.9072574377059937
Epoch 970, training loss: 62.976436614990234 = 0.27029019594192505 + 10.0 * 6.2706146240234375
Epoch 970, val loss: 0.9080767035484314
Epoch 980, training loss: 62.92732620239258 = 0.2619904577732086 + 10.0 * 6.266533851623535
Epoch 980, val loss: 0.9090234041213989
Epoch 990, training loss: 62.895668029785156 = 0.2540516257286072 + 10.0 * 6.264161586761475
Epoch 990, val loss: 0.9102362990379333
Epoch 1000, training loss: 62.884742736816406 = 0.24641838669776917 + 10.0 * 6.2638325691223145
Epoch 1000, val loss: 0.9116354584693909
Epoch 1010, training loss: 62.922359466552734 = 0.23902766406536102 + 10.0 * 6.2683329582214355
Epoch 1010, val loss: 0.9131361842155457
Epoch 1020, training loss: 62.8741455078125 = 0.2317579835653305 + 10.0 * 6.2642388343811035
Epoch 1020, val loss: 0.9146216511726379
Epoch 1030, training loss: 62.84737014770508 = 0.22482429444789886 + 10.0 * 6.26225471496582
Epoch 1030, val loss: 0.9163438081741333
Epoch 1040, training loss: 62.829261779785156 = 0.21810603141784668 + 10.0 * 6.261115550994873
Epoch 1040, val loss: 0.9181696176528931
Epoch 1050, training loss: 62.84898376464844 = 0.2116454690694809 + 10.0 * 6.263733863830566
Epoch 1050, val loss: 0.920147180557251
Epoch 1060, training loss: 62.83555221557617 = 0.2052628993988037 + 10.0 * 6.263029098510742
Epoch 1060, val loss: 0.9219836592674255
Epoch 1070, training loss: 62.80754089355469 = 0.19914335012435913 + 10.0 * 6.260839939117432
Epoch 1070, val loss: 0.9241048097610474
Epoch 1080, training loss: 62.775814056396484 = 0.1932576596736908 + 10.0 * 6.258255958557129
Epoch 1080, val loss: 0.9263375997543335
Epoch 1090, training loss: 62.77320098876953 = 0.1876036673784256 + 10.0 * 6.258559703826904
Epoch 1090, val loss: 0.9286929368972778
Epoch 1100, training loss: 62.77293395996094 = 0.18208637833595276 + 10.0 * 6.259084701538086
Epoch 1100, val loss: 0.9309784770011902
Epoch 1110, training loss: 62.747337341308594 = 0.17672553658485413 + 10.0 * 6.257061004638672
Epoch 1110, val loss: 0.9333309531211853
Epoch 1120, training loss: 62.73563003540039 = 0.17155712842941284 + 10.0 * 6.256407260894775
Epoch 1120, val loss: 0.9357786774635315
Epoch 1130, training loss: 62.726043701171875 = 0.1666024625301361 + 10.0 * 6.25594425201416
Epoch 1130, val loss: 0.9383684992790222
Epoch 1140, training loss: 62.77640151977539 = 0.16175507009029388 + 10.0 * 6.261464595794678
Epoch 1140, val loss: 0.9408915042877197
Epoch 1150, training loss: 62.707176208496094 = 0.1570090502500534 + 10.0 * 6.255016803741455
Epoch 1150, val loss: 0.9433706998825073
Epoch 1160, training loss: 62.69084930419922 = 0.15246616303920746 + 10.0 * 6.253838539123535
Epoch 1160, val loss: 0.9460684061050415
Epoch 1170, training loss: 62.70127868652344 = 0.14808188378810883 + 10.0 * 6.255319595336914
Epoch 1170, val loss: 0.9488467574119568
Epoch 1180, training loss: 62.67656707763672 = 0.1438223123550415 + 10.0 * 6.253274440765381
Epoch 1180, val loss: 0.9514791965484619
Epoch 1190, training loss: 62.670494079589844 = 0.1396866738796234 + 10.0 * 6.25308084487915
Epoch 1190, val loss: 0.9542245268821716
Epoch 1200, training loss: 62.6702995300293 = 0.13570748269557953 + 10.0 * 6.2534589767456055
Epoch 1200, val loss: 0.957169234752655
Epoch 1210, training loss: 62.669288635253906 = 0.13183660805225372 + 10.0 * 6.253745079040527
Epoch 1210, val loss: 0.9600278735160828
Epoch 1220, training loss: 62.64606857299805 = 0.1281099021434784 + 10.0 * 6.251795768737793
Epoch 1220, val loss: 0.9628998637199402
Epoch 1230, training loss: 62.63545227050781 = 0.12452016770839691 + 10.0 * 6.25109338760376
Epoch 1230, val loss: 0.9659080505371094
Epoch 1240, training loss: 62.642127990722656 = 0.12104176729917526 + 10.0 * 6.252108573913574
Epoch 1240, val loss: 0.9688857197761536
Epoch 1250, training loss: 62.61564254760742 = 0.1176561489701271 + 10.0 * 6.249798774719238
Epoch 1250, val loss: 0.9719107151031494
Epoch 1260, training loss: 62.61502456665039 = 0.11439122259616852 + 10.0 * 6.250063419342041
Epoch 1260, val loss: 0.9748876690864563
Epoch 1270, training loss: 62.6177864074707 = 0.11124002933502197 + 10.0 * 6.250654697418213
Epoch 1270, val loss: 0.9779123067855835
Epoch 1280, training loss: 62.5985221862793 = 0.10818851739168167 + 10.0 * 6.249033451080322
Epoch 1280, val loss: 0.9809194803237915
Epoch 1290, training loss: 62.59285354614258 = 0.1052330881357193 + 10.0 * 6.248762130737305
Epoch 1290, val loss: 0.9839312434196472
Epoch 1300, training loss: 62.587982177734375 = 0.10237034410238266 + 10.0 * 6.248560905456543
Epoch 1300, val loss: 0.9871179461479187
Epoch 1310, training loss: 62.57539749145508 = 0.09960104525089264 + 10.0 * 6.247579574584961
Epoch 1310, val loss: 0.9901119470596313
Epoch 1320, training loss: 62.557220458984375 = 0.09693901985883713 + 10.0 * 6.246027946472168
Epoch 1320, val loss: 0.9932035207748413
Epoch 1330, training loss: 62.5502815246582 = 0.0943751335144043 + 10.0 * 6.245590686798096
Epoch 1330, val loss: 0.9963439702987671
Epoch 1340, training loss: 62.56652069091797 = 0.09190420806407928 + 10.0 * 6.247461795806885
Epoch 1340, val loss: 0.9994565844535828
Epoch 1350, training loss: 62.5605354309082 = 0.089457668364048 + 10.0 * 6.24710750579834
Epoch 1350, val loss: 1.002514123916626
Epoch 1360, training loss: 62.54478454589844 = 0.08711433410644531 + 10.0 * 6.245767116546631
Epoch 1360, val loss: 1.005537986755371
Epoch 1370, training loss: 62.53828430175781 = 0.0848565474152565 + 10.0 * 6.24534273147583
Epoch 1370, val loss: 1.0088329315185547
Epoch 1380, training loss: 62.538665771484375 = 0.08268047124147415 + 10.0 * 6.245598793029785
Epoch 1380, val loss: 1.0119684934616089
Epoch 1390, training loss: 62.5126953125 = 0.08056391030550003 + 10.0 * 6.243213176727295
Epoch 1390, val loss: 1.015214204788208
Epoch 1400, training loss: 62.53662109375 = 0.07852472364902496 + 10.0 * 6.245809555053711
Epoch 1400, val loss: 1.0184046030044556
Epoch 1410, training loss: 62.51390838623047 = 0.07654183357954025 + 10.0 * 6.243736743927002
Epoch 1410, val loss: 1.0215438604354858
Epoch 1420, training loss: 62.49956130981445 = 0.07460197061300278 + 10.0 * 6.242496013641357
Epoch 1420, val loss: 1.0246379375457764
Epoch 1430, training loss: 62.49235153198242 = 0.07276526838541031 + 10.0 * 6.2419586181640625
Epoch 1430, val loss: 1.0279213190078735
Epoch 1440, training loss: 62.5160026550293 = 0.07098989188671112 + 10.0 * 6.244501113891602
Epoch 1440, val loss: 1.031144618988037
Epoch 1450, training loss: 62.48131561279297 = 0.06923842430114746 + 10.0 * 6.241208076477051
Epoch 1450, val loss: 1.0341720581054688
Epoch 1460, training loss: 62.506919860839844 = 0.06756588071584702 + 10.0 * 6.243935585021973
Epoch 1460, val loss: 1.0373432636260986
Epoch 1470, training loss: 62.47450637817383 = 0.06592686474323273 + 10.0 * 6.24085807800293
Epoch 1470, val loss: 1.0405337810516357
Epoch 1480, training loss: 62.467750549316406 = 0.06434004753828049 + 10.0 * 6.2403411865234375
Epoch 1480, val loss: 1.0436134338378906
Epoch 1490, training loss: 62.473331451416016 = 0.06283299624919891 + 10.0 * 6.241049766540527
Epoch 1490, val loss: 1.0467709302902222
Epoch 1500, training loss: 62.46809768676758 = 0.06134536489844322 + 10.0 * 6.240675449371338
Epoch 1500, val loss: 1.0499565601348877
Epoch 1510, training loss: 62.445125579833984 = 0.059917811304330826 + 10.0 * 6.238520622253418
Epoch 1510, val loss: 1.053199291229248
Epoch 1520, training loss: 62.48215103149414 = 0.05854412913322449 + 10.0 * 6.242360591888428
Epoch 1520, val loss: 1.0564360618591309
Epoch 1530, training loss: 62.462100982666016 = 0.05717873200774193 + 10.0 * 6.240492343902588
Epoch 1530, val loss: 1.0592992305755615
Epoch 1540, training loss: 62.4453239440918 = 0.055870622396469116 + 10.0 * 6.238945484161377
Epoch 1540, val loss: 1.0624480247497559
Epoch 1550, training loss: 62.438968658447266 = 0.05459802597761154 + 10.0 * 6.238436698913574
Epoch 1550, val loss: 1.0655580759048462
Epoch 1560, training loss: 62.428794860839844 = 0.05337943881750107 + 10.0 * 6.237541675567627
Epoch 1560, val loss: 1.0687235593795776
Epoch 1570, training loss: 62.443538665771484 = 0.052201952785253525 + 10.0 * 6.239133834838867
Epoch 1570, val loss: 1.071804404258728
Epoch 1580, training loss: 62.4466552734375 = 0.051041435450315475 + 10.0 * 6.239561557769775
Epoch 1580, val loss: 1.074816107749939
Epoch 1590, training loss: 62.42026138305664 = 0.04990383982658386 + 10.0 * 6.237035751342773
Epoch 1590, val loss: 1.0776848793029785
Epoch 1600, training loss: 62.40923309326172 = 0.04881931096315384 + 10.0 * 6.236041069030762
Epoch 1600, val loss: 1.0808542966842651
Epoch 1610, training loss: 62.411354064941406 = 0.04777500405907631 + 10.0 * 6.236357688903809
Epoch 1610, val loss: 1.0838890075683594
Epoch 1620, training loss: 62.43239974975586 = 0.046761300414800644 + 10.0 * 6.2385640144348145
Epoch 1620, val loss: 1.086873173713684
Epoch 1630, training loss: 62.411895751953125 = 0.04575788602232933 + 10.0 * 6.236613750457764
Epoch 1630, val loss: 1.0897878408432007
Epoch 1640, training loss: 62.39825439453125 = 0.04479677230119705 + 10.0 * 6.235345840454102
Epoch 1640, val loss: 1.092862606048584
Epoch 1650, training loss: 62.441219329833984 = 0.04386802762746811 + 10.0 * 6.239735126495361
Epoch 1650, val loss: 1.0957913398742676
Epoch 1660, training loss: 62.409488677978516 = 0.04294300079345703 + 10.0 * 6.236654758453369
Epoch 1660, val loss: 1.098650574684143
Epoch 1670, training loss: 62.38276290893555 = 0.04206155613064766 + 10.0 * 6.234070301055908
Epoch 1670, val loss: 1.1016426086425781
Epoch 1680, training loss: 62.37726593017578 = 0.041215017437934875 + 10.0 * 6.23360538482666
Epoch 1680, val loss: 1.1045472621917725
Epoch 1690, training loss: 62.393375396728516 = 0.040393296629190445 + 10.0 * 6.235298156738281
Epoch 1690, val loss: 1.1074010133743286
Epoch 1700, training loss: 62.37618637084961 = 0.03957785665988922 + 10.0 * 6.233660697937012
Epoch 1700, val loss: 1.1103816032409668
Epoch 1710, training loss: 62.39524459838867 = 0.03878600895404816 + 10.0 * 6.235645771026611
Epoch 1710, val loss: 1.113148808479309
Epoch 1720, training loss: 62.371707916259766 = 0.03801979124546051 + 10.0 * 6.233368873596191
Epoch 1720, val loss: 1.116095781326294
Epoch 1730, training loss: 62.36869430541992 = 0.037279706448316574 + 10.0 * 6.2331414222717285
Epoch 1730, val loss: 1.1189672946929932
Epoch 1740, training loss: 62.37411117553711 = 0.03656540438532829 + 10.0 * 6.233754634857178
Epoch 1740, val loss: 1.1216827630996704
Epoch 1750, training loss: 62.35136413574219 = 0.03585069999098778 + 10.0 * 6.231551170349121
Epoch 1750, val loss: 1.124456524848938
Epoch 1760, training loss: 62.353553771972656 = 0.03517277538776398 + 10.0 * 6.231838226318359
Epoch 1760, val loss: 1.1272631883621216
Epoch 1770, training loss: 62.38631820678711 = 0.034518275409936905 + 10.0 * 6.235179901123047
Epoch 1770, val loss: 1.1300297975540161
Epoch 1780, training loss: 62.3629264831543 = 0.03384970501065254 + 10.0 * 6.232907772064209
Epoch 1780, val loss: 1.1326767206192017
Epoch 1790, training loss: 62.340599060058594 = 0.03322131931781769 + 10.0 * 6.230737686157227
Epoch 1790, val loss: 1.135396122932434
Epoch 1800, training loss: 62.337371826171875 = 0.03261314332485199 + 10.0 * 6.230475902557373
Epoch 1800, val loss: 1.1382648944854736
Epoch 1810, training loss: 62.36916732788086 = 0.032025471329689026 + 10.0 * 6.2337141036987305
Epoch 1810, val loss: 1.1408207416534424
Epoch 1820, training loss: 62.33842468261719 = 0.03142762929201126 + 10.0 * 6.23069953918457
Epoch 1820, val loss: 1.1433836221694946
Epoch 1830, training loss: 62.3508186340332 = 0.030872253701090813 + 10.0 * 6.23199462890625
Epoch 1830, val loss: 1.146065354347229
Epoch 1840, training loss: 62.327674865722656 = 0.03031250461935997 + 10.0 * 6.229736328125
Epoch 1840, val loss: 1.1487908363342285
Epoch 1850, training loss: 62.34404754638672 = 0.02977578528225422 + 10.0 * 6.231427192687988
Epoch 1850, val loss: 1.1514527797698975
Epoch 1860, training loss: 62.315223693847656 = 0.02924930676817894 + 10.0 * 6.228597164154053
Epoch 1860, val loss: 1.1538326740264893
Epoch 1870, training loss: 62.31277847290039 = 0.028744548559188843 + 10.0 * 6.228403568267822
Epoch 1870, val loss: 1.1564686298370361
Epoch 1880, training loss: 62.316566467285156 = 0.02825155295431614 + 10.0 * 6.2288312911987305
Epoch 1880, val loss: 1.15909743309021
Epoch 1890, training loss: 62.31246566772461 = 0.027766268700361252 + 10.0 * 6.2284698486328125
Epoch 1890, val loss: 1.1616318225860596
Epoch 1900, training loss: 62.29987716674805 = 0.027295459061861038 + 10.0 * 6.227258205413818
Epoch 1900, val loss: 1.164138913154602
Epoch 1910, training loss: 62.31124496459961 = 0.02684326283633709 + 10.0 * 6.228440284729004
Epoch 1910, val loss: 1.1667076349258423
Epoch 1920, training loss: 62.32079315185547 = 0.026389384642243385 + 10.0 * 6.229440212249756
Epoch 1920, val loss: 1.169211745262146
Epoch 1930, training loss: 62.29940414428711 = 0.0259358212351799 + 10.0 * 6.227346897125244
Epoch 1930, val loss: 1.1714236736297607
Epoch 1940, training loss: 62.28517150878906 = 0.0255048219114542 + 10.0 * 6.225966453552246
Epoch 1940, val loss: 1.17399001121521
Epoch 1950, training loss: 62.278621673583984 = 0.025094402953982353 + 10.0 * 6.225352764129639
Epoch 1950, val loss: 1.1765247583389282
Epoch 1960, training loss: 62.29063034057617 = 0.024693051353096962 + 10.0 * 6.226593971252441
Epoch 1960, val loss: 1.1790504455566406
Epoch 1970, training loss: 62.298561096191406 = 0.024293357506394386 + 10.0 * 6.227427005767822
Epoch 1970, val loss: 1.1812890768051147
Epoch 1980, training loss: 62.28424072265625 = 0.02389366365969181 + 10.0 * 6.226034641265869
Epoch 1980, val loss: 1.1835216283798218
Epoch 1990, training loss: 62.27503967285156 = 0.023511813953518867 + 10.0 * 6.225152969360352
Epoch 1990, val loss: 1.1859378814697266
Epoch 2000, training loss: 62.26853561401367 = 0.02314491756260395 + 10.0 * 6.224539279937744
Epoch 2000, val loss: 1.1884779930114746
Epoch 2010, training loss: 62.32024002075195 = 0.022795775905251503 + 10.0 * 6.2297444343566895
Epoch 2010, val loss: 1.1907354593276978
Epoch 2020, training loss: 62.27296447753906 = 0.022429343312978745 + 10.0 * 6.225053310394287
Epoch 2020, val loss: 1.1930567026138306
Epoch 2030, training loss: 62.2695426940918 = 0.022084876894950867 + 10.0 * 6.224745750427246
Epoch 2030, val loss: 1.1952513456344604
Epoch 2040, training loss: 62.27347183227539 = 0.02174750715494156 + 10.0 * 6.225172519683838
Epoch 2040, val loss: 1.1976675987243652
Epoch 2050, training loss: 62.271522521972656 = 0.021419603377580643 + 10.0 * 6.225010395050049
Epoch 2050, val loss: 1.199804425239563
Epoch 2060, training loss: 62.28960418701172 = 0.02109338715672493 + 10.0 * 6.226850986480713
Epoch 2060, val loss: 1.2021286487579346
Epoch 2070, training loss: 62.25453186035156 = 0.02077498845756054 + 10.0 * 6.2233757972717285
Epoch 2070, val loss: 1.2043232917785645
Epoch 2080, training loss: 62.24714279174805 = 0.020465925335884094 + 10.0 * 6.222667694091797
Epoch 2080, val loss: 1.2065706253051758
Epoch 2090, training loss: 62.24113082885742 = 0.02016972377896309 + 10.0 * 6.2220964431762695
Epoch 2090, val loss: 1.2087798118591309
Epoch 2100, training loss: 62.24706268310547 = 0.01988365687429905 + 10.0 * 6.222718238830566
Epoch 2100, val loss: 1.2109653949737549
Epoch 2110, training loss: 62.3187141418457 = 0.019600823521614075 + 10.0 * 6.2299113273620605
Epoch 2110, val loss: 1.2131296396255493
Epoch 2120, training loss: 62.262508392333984 = 0.01929859258234501 + 10.0 * 6.224320888519287
Epoch 2120, val loss: 1.215071678161621
Epoch 2130, training loss: 62.23259735107422 = 0.01902044378221035 + 10.0 * 6.221357822418213
Epoch 2130, val loss: 1.2170754671096802
Epoch 2140, training loss: 62.23162841796875 = 0.018755845725536346 + 10.0 * 6.221287250518799
Epoch 2140, val loss: 1.2193347215652466
Epoch 2150, training loss: 62.244354248046875 = 0.01849614456295967 + 10.0 * 6.222585678100586
Epoch 2150, val loss: 1.2214049100875854
Epoch 2160, training loss: 62.2525520324707 = 0.018237093463540077 + 10.0 * 6.223431587219238
Epoch 2160, val loss: 1.2234688997268677
Epoch 2170, training loss: 62.290992736816406 = 0.01797674596309662 + 10.0 * 6.227301597595215
Epoch 2170, val loss: 1.2253507375717163
Epoch 2180, training loss: 62.2322998046875 = 0.017721612006425858 + 10.0 * 6.221457481384277
Epoch 2180, val loss: 1.2273054122924805
Epoch 2190, training loss: 62.22264099121094 = 0.0174788236618042 + 10.0 * 6.220516204833984
Epoch 2190, val loss: 1.229323148727417
Epoch 2200, training loss: 62.21969985961914 = 0.01725030317902565 + 10.0 * 6.220244884490967
Epoch 2200, val loss: 1.231531023979187
Epoch 2210, training loss: 62.25605773925781 = 0.017025569453835487 + 10.0 * 6.223903179168701
Epoch 2210, val loss: 1.2335349321365356
Epoch 2220, training loss: 62.21502685546875 = 0.016789305955171585 + 10.0 * 6.219823837280273
Epoch 2220, val loss: 1.2352900505065918
Epoch 2230, training loss: 62.211143493652344 = 0.01656435802578926 + 10.0 * 6.219458103179932
Epoch 2230, val loss: 1.2372220754623413
Epoch 2240, training loss: 62.21535110473633 = 0.016348576173186302 + 10.0 * 6.219900131225586
Epoch 2240, val loss: 1.2393487691879272
Epoch 2250, training loss: 62.2822380065918 = 0.016136769205331802 + 10.0 * 6.22661018371582
Epoch 2250, val loss: 1.2412469387054443
Epoch 2260, training loss: 62.21736526489258 = 0.015916835516691208 + 10.0 * 6.220144748687744
Epoch 2260, val loss: 1.242926836013794
Epoch 2270, training loss: 62.20244598388672 = 0.0157120693475008 + 10.0 * 6.218673229217529
Epoch 2270, val loss: 1.2449284791946411
Epoch 2280, training loss: 62.20283889770508 = 0.015514620579779148 + 10.0 * 6.2187323570251465
Epoch 2280, val loss: 1.2468360662460327
Epoch 2290, training loss: 62.27511215209961 = 0.015323469415307045 + 10.0 * 6.225978851318359
Epoch 2290, val loss: 1.2485744953155518
Epoch 2300, training loss: 62.2647590637207 = 0.015117325820028782 + 10.0 * 6.224964141845703
Epoch 2300, val loss: 1.2506040334701538
Epoch 2310, training loss: 62.207611083984375 = 0.014925242401659489 + 10.0 * 6.219268321990967
Epoch 2310, val loss: 1.2521048784255981
Epoch 2320, training loss: 62.19190979003906 = 0.014741051010787487 + 10.0 * 6.217717170715332
Epoch 2320, val loss: 1.2541004419326782
Epoch 2330, training loss: 62.1911735534668 = 0.014563340693712234 + 10.0 * 6.217660903930664
Epoch 2330, val loss: 1.2559552192687988
Epoch 2340, training loss: 62.200286865234375 = 0.01439040619879961 + 10.0 * 6.218589782714844
Epoch 2340, val loss: 1.257761836051941
Epoch 2350, training loss: 62.236106872558594 = 0.014217576012015343 + 10.0 * 6.222188949584961
Epoch 2350, val loss: 1.2593168020248413
Epoch 2360, training loss: 62.23314666748047 = 0.014040478505194187 + 10.0 * 6.22191047668457
Epoch 2360, val loss: 1.2609546184539795
Epoch 2370, training loss: 62.19651412963867 = 0.01386246643960476 + 10.0 * 6.218265056610107
Epoch 2370, val loss: 1.2627766132354736
Epoch 2380, training loss: 62.18779373168945 = 0.013699973933398724 + 10.0 * 6.217409610748291
Epoch 2380, val loss: 1.2644695043563843
Epoch 2390, training loss: 62.203102111816406 = 0.013541270978748798 + 10.0 * 6.218955993652344
Epoch 2390, val loss: 1.2662941217422485
Epoch 2400, training loss: 62.21529006958008 = 0.013378452509641647 + 10.0 * 6.22019100189209
Epoch 2400, val loss: 1.267734408378601
Epoch 2410, training loss: 62.18267822265625 = 0.013213367201387882 + 10.0 * 6.216946601867676
Epoch 2410, val loss: 1.2692736387252808
Epoch 2420, training loss: 62.17603302001953 = 0.013058648444712162 + 10.0 * 6.216297626495361
Epoch 2420, val loss: 1.2710405588150024
Epoch 2430, training loss: 62.17380905151367 = 0.012913353741168976 + 10.0 * 6.216089725494385
Epoch 2430, val loss: 1.2727673053741455
Epoch 2440, training loss: 62.192604064941406 = 0.012771512381732464 + 10.0 * 6.217983245849609
Epoch 2440, val loss: 1.2745521068572998
Epoch 2450, training loss: 62.183677673339844 = 0.012622569687664509 + 10.0 * 6.217105388641357
Epoch 2450, val loss: 1.2759418487548828
Epoch 2460, training loss: 62.202423095703125 = 0.012474630028009415 + 10.0 * 6.218995094299316
Epoch 2460, val loss: 1.277451753616333
Epoch 2470, training loss: 62.181514739990234 = 0.01233325619250536 + 10.0 * 6.216917991638184
Epoch 2470, val loss: 1.2789987325668335
Epoch 2480, training loss: 62.176124572753906 = 0.012195917777717113 + 10.0 * 6.216392993927002
Epoch 2480, val loss: 1.2804135084152222
Epoch 2490, training loss: 62.180301666259766 = 0.012063705362379551 + 10.0 * 6.216824054718018
Epoch 2490, val loss: 1.282084345817566
Epoch 2500, training loss: 62.191986083984375 = 0.01193012110888958 + 10.0 * 6.218005657196045
Epoch 2500, val loss: 1.2835605144500732
Epoch 2510, training loss: 62.174137115478516 = 0.011796760372817516 + 10.0 * 6.21623420715332
Epoch 2510, val loss: 1.2850704193115234
Epoch 2520, training loss: 62.19398880004883 = 0.011670239269733429 + 10.0 * 6.218232154846191
Epoch 2520, val loss: 1.2865720987319946
Epoch 2530, training loss: 62.16318893432617 = 0.011540130712091923 + 10.0 * 6.215165138244629
Epoch 2530, val loss: 1.2879157066345215
Epoch 2540, training loss: 62.163108825683594 = 0.011415167711675167 + 10.0 * 6.215169429779053
Epoch 2540, val loss: 1.2893905639648438
Epoch 2550, training loss: 62.1679801940918 = 0.01129454467445612 + 10.0 * 6.215668678283691
Epoch 2550, val loss: 1.2908556461334229
Epoch 2560, training loss: 62.1854248046875 = 0.011175219900906086 + 10.0 * 6.2174248695373535
Epoch 2560, val loss: 1.292310357093811
Epoch 2570, training loss: 62.16410827636719 = 0.011056897230446339 + 10.0 * 6.215305328369141
Epoch 2570, val loss: 1.2937694787979126
Epoch 2580, training loss: 62.165653228759766 = 0.010940303094685078 + 10.0 * 6.215471267700195
Epoch 2580, val loss: 1.2952561378479004
Epoch 2590, training loss: 62.158363342285156 = 0.010827942751348019 + 10.0 * 6.2147536277771
Epoch 2590, val loss: 1.2964876890182495
Epoch 2600, training loss: 62.15282440185547 = 0.010717180557549 + 10.0 * 6.2142109870910645
Epoch 2600, val loss: 1.2980085611343384
Epoch 2610, training loss: 62.167762756347656 = 0.010608986020088196 + 10.0 * 6.215715408325195
Epoch 2610, val loss: 1.2994574308395386
Epoch 2620, training loss: 62.16237258911133 = 0.010500469245016575 + 10.0 * 6.215187072753906
Epoch 2620, val loss: 1.3005956411361694
Epoch 2630, training loss: 62.1725959777832 = 0.01039118506014347 + 10.0 * 6.216220378875732
Epoch 2630, val loss: 1.3016389608383179
Epoch 2640, training loss: 62.15690612792969 = 0.010279438458383083 + 10.0 * 6.214662551879883
Epoch 2640, val loss: 1.3030434846878052
Epoch 2650, training loss: 62.16033172607422 = 0.010177534073591232 + 10.0 * 6.215015411376953
Epoch 2650, val loss: 1.304338812828064
Epoch 2660, training loss: 62.1416130065918 = 0.010076509788632393 + 10.0 * 6.213153839111328
Epoch 2660, val loss: 1.3057819604873657
Epoch 2670, training loss: 62.157379150390625 = 0.009980499744415283 + 10.0 * 6.214739799499512
Epoch 2670, val loss: 1.3070595264434814
Epoch 2680, training loss: 62.163368225097656 = 0.00988064706325531 + 10.0 * 6.215348720550537
Epoch 2680, val loss: 1.3084129095077515
Epoch 2690, training loss: 62.15730667114258 = 0.009781604632735252 + 10.0 * 6.214752674102783
Epoch 2690, val loss: 1.3093403577804565
Epoch 2700, training loss: 62.14329528808594 = 0.00968776922672987 + 10.0 * 6.213360786437988
Epoch 2700, val loss: 1.3108066320419312
Epoch 2710, training loss: 62.140525817871094 = 0.009595442563295364 + 10.0 * 6.213093280792236
Epoch 2710, val loss: 1.3118951320648193
Epoch 2720, training loss: 62.16417694091797 = 0.00950450450181961 + 10.0 * 6.2154669761657715
Epoch 2720, val loss: 1.3132704496383667
Epoch 2730, training loss: 62.142181396484375 = 0.009412901476025581 + 10.0 * 6.2132768630981445
Epoch 2730, val loss: 1.314366340637207
Epoch 2740, training loss: 62.14543914794922 = 0.009325426071882248 + 10.0 * 6.213611125946045
Epoch 2740, val loss: 1.3155601024627686
Epoch 2750, training loss: 62.15106201171875 = 0.009237773716449738 + 10.0 * 6.214182376861572
Epoch 2750, val loss: 1.3167227506637573
Epoch 2760, training loss: 62.142818450927734 = 0.009151263162493706 + 10.0 * 6.213366508483887
Epoch 2760, val loss: 1.3179221153259277
Epoch 2770, training loss: 62.146629333496094 = 0.009063327684998512 + 10.0 * 6.213756561279297
Epoch 2770, val loss: 1.3190548419952393
Epoch 2780, training loss: 62.12392807006836 = 0.008978484198451042 + 10.0 * 6.2114949226379395
Epoch 2780, val loss: 1.3199862241744995
Epoch 2790, training loss: 62.12114334106445 = 0.008897876366972923 + 10.0 * 6.211224555969238
Epoch 2790, val loss: 1.3212924003601074
Epoch 2800, training loss: 62.121437072753906 = 0.008820489048957825 + 10.0 * 6.211261749267578
Epoch 2800, val loss: 1.3225432634353638
Epoch 2810, training loss: 62.174198150634766 = 0.008746115490794182 + 10.0 * 6.216545104980469
Epoch 2810, val loss: 1.3236629962921143
Epoch 2820, training loss: 62.12962341308594 = 0.008659422397613525 + 10.0 * 6.212096214294434
Epoch 2820, val loss: 1.324478268623352
Epoch 2830, training loss: 62.15110778808594 = 0.008580129593610764 + 10.0 * 6.214252948760986
Epoch 2830, val loss: 1.3256886005401611
Epoch 2840, training loss: 62.125083923339844 = 0.008500113151967525 + 10.0 * 6.211658477783203
Epoch 2840, val loss: 1.3263354301452637
Epoch 2850, training loss: 62.117733001708984 = 0.008427903987467289 + 10.0 * 6.210930824279785
Epoch 2850, val loss: 1.3276394605636597
Epoch 2860, training loss: 62.11116027832031 = 0.008355417288839817 + 10.0 * 6.210280418395996
Epoch 2860, val loss: 1.3287800550460815
Epoch 2870, training loss: 62.12126159667969 = 0.008287191390991211 + 10.0 * 6.211297512054443
Epoch 2870, val loss: 1.3299068212509155
Epoch 2880, training loss: 62.16310501098633 = 0.008216512389481068 + 10.0 * 6.215488910675049
Epoch 2880, val loss: 1.3308193683624268
Epoch 2890, training loss: 62.13541793823242 = 0.008137170225381851 + 10.0 * 6.212728023529053
Epoch 2890, val loss: 1.3317567110061646
Epoch 2900, training loss: 62.12263870239258 = 0.008066937327384949 + 10.0 * 6.211457252502441
Epoch 2900, val loss: 1.3326784372329712
Epoch 2910, training loss: 62.129180908203125 = 0.0079977260902524 + 10.0 * 6.212118148803711
Epoch 2910, val loss: 1.3336942195892334
Epoch 2920, training loss: 62.10667037963867 = 0.007929845713078976 + 10.0 * 6.209874153137207
Epoch 2920, val loss: 1.3345823287963867
Epoch 2930, training loss: 62.11487579345703 = 0.007865956984460354 + 10.0 * 6.210700988769531
Epoch 2930, val loss: 1.3355056047439575
Epoch 2940, training loss: 62.11456298828125 = 0.00780068151652813 + 10.0 * 6.210676193237305
Epoch 2940, val loss: 1.336512804031372
Epoch 2950, training loss: 62.121150970458984 = 0.007735046092420816 + 10.0 * 6.211341381072998
Epoch 2950, val loss: 1.33749258518219
Epoch 2960, training loss: 62.10432815551758 = 0.007670282851904631 + 10.0 * 6.209665775299072
Epoch 2960, val loss: 1.3383525609970093
Epoch 2970, training loss: 62.12097930908203 = 0.007607749197632074 + 10.0 * 6.211337089538574
Epoch 2970, val loss: 1.3392117023468018
Epoch 2980, training loss: 62.15425109863281 = 0.0075445352122187614 + 10.0 * 6.214670658111572
Epoch 2980, val loss: 1.3399237394332886
Epoch 2990, training loss: 62.11258316040039 = 0.007481129840016365 + 10.0 * 6.21051025390625
Epoch 2990, val loss: 1.3408312797546387
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.7991565629942015
=== training gcn model ===
Epoch 0, training loss: 87.90478515625 = 1.9365062713623047 + 10.0 * 8.596827507019043
Epoch 0, val loss: 1.931406021118164
Epoch 10, training loss: 87.88910675048828 = 1.9267929792404175 + 10.0 * 8.596231460571289
Epoch 10, val loss: 1.9217522144317627
Epoch 20, training loss: 87.8410873413086 = 1.9143316745758057 + 10.0 * 8.592676162719727
Epoch 20, val loss: 1.9089233875274658
Epoch 30, training loss: 87.61002349853516 = 1.8978198766708374 + 10.0 * 8.571220397949219
Epoch 30, val loss: 1.891693115234375
Epoch 40, training loss: 86.34893798828125 = 1.8788878917694092 + 10.0 * 8.447004318237305
Epoch 40, val loss: 1.8726263046264648
Epoch 50, training loss: 79.31399536132812 = 1.8603652715682983 + 10.0 * 7.745362758636475
Epoch 50, val loss: 1.854315161705017
Epoch 60, training loss: 74.66056823730469 = 1.8459442853927612 + 10.0 * 7.281461715698242
Epoch 60, val loss: 1.8410449028015137
Epoch 70, training loss: 72.01150512695312 = 1.834956169128418 + 10.0 * 7.017655372619629
Epoch 70, val loss: 1.8308939933776855
Epoch 80, training loss: 70.6977310180664 = 1.8250771760940552 + 10.0 * 6.887265205383301
Epoch 80, val loss: 1.8213026523590088
Epoch 90, training loss: 69.86087799072266 = 1.8157356977462769 + 10.0 * 6.804514408111572
Epoch 90, val loss: 1.8121784925460815
Epoch 100, training loss: 69.23782348632812 = 1.8063163757324219 + 10.0 * 6.74315071105957
Epoch 100, val loss: 1.8031182289123535
Epoch 110, training loss: 68.75774383544922 = 1.7976949214935303 + 10.0 * 6.696004867553711
Epoch 110, val loss: 1.7950903177261353
Epoch 120, training loss: 68.34620666503906 = 1.7895790338516235 + 10.0 * 6.655662536621094
Epoch 120, val loss: 1.7877224683761597
Epoch 130, training loss: 67.98616027832031 = 1.781453251838684 + 10.0 * 6.6204705238342285
Epoch 130, val loss: 1.780349612236023
Epoch 140, training loss: 67.6563720703125 = 1.7730132341384888 + 10.0 * 6.588335990905762
Epoch 140, val loss: 1.7728065252304077
Epoch 150, training loss: 67.38182830810547 = 1.7640273571014404 + 10.0 * 6.561779975891113
Epoch 150, val loss: 1.7649009227752686
Epoch 160, training loss: 67.1528091430664 = 1.754090428352356 + 10.0 * 6.539872169494629
Epoch 160, val loss: 1.7562421560287476
Epoch 170, training loss: 66.94898223876953 = 1.7430713176727295 + 10.0 * 6.5205912590026855
Epoch 170, val loss: 1.7467137575149536
Epoch 180, training loss: 66.77279663085938 = 1.7308168411254883 + 10.0 * 6.504197597503662
Epoch 180, val loss: 1.7362825870513916
Epoch 190, training loss: 66.58841705322266 = 1.71726393699646 + 10.0 * 6.487115383148193
Epoch 190, val loss: 1.724658489227295
Epoch 200, training loss: 66.43390655517578 = 1.7023143768310547 + 10.0 * 6.473159313201904
Epoch 200, val loss: 1.7119742631912231
Epoch 210, training loss: 66.29107666015625 = 1.6857647895812988 + 10.0 * 6.460530757904053
Epoch 210, val loss: 1.6980388164520264
Epoch 220, training loss: 66.16053771972656 = 1.6675803661346436 + 10.0 * 6.449295997619629
Epoch 220, val loss: 1.6827133893966675
Epoch 230, training loss: 66.0608901977539 = 1.6475414037704468 + 10.0 * 6.4413347244262695
Epoch 230, val loss: 1.6660387516021729
Epoch 240, training loss: 65.94007110595703 = 1.6256780624389648 + 10.0 * 6.431439399719238
Epoch 240, val loss: 1.647737741470337
Epoch 250, training loss: 65.82222747802734 = 1.6021143198013306 + 10.0 * 6.422011852264404
Epoch 250, val loss: 1.6281121969223022
Epoch 260, training loss: 65.72229766845703 = 1.5768741369247437 + 10.0 * 6.414542198181152
Epoch 260, val loss: 1.6073254346847534
Epoch 270, training loss: 65.6279067993164 = 1.5500457286834717 + 10.0 * 6.407785892486572
Epoch 270, val loss: 1.58539879322052
Epoch 280, training loss: 65.5434799194336 = 1.521918773651123 + 10.0 * 6.402155876159668
Epoch 280, val loss: 1.562512755393982
Epoch 290, training loss: 65.45428466796875 = 1.492645025253296 + 10.0 * 6.3961639404296875
Epoch 290, val loss: 1.539325475692749
Epoch 300, training loss: 65.35398864746094 = 1.4627275466918945 + 10.0 * 6.389126300811768
Epoch 300, val loss: 1.5157326459884644
Epoch 310, training loss: 65.26644897460938 = 1.4321973323822021 + 10.0 * 6.383425235748291
Epoch 310, val loss: 1.491959810256958
Epoch 320, training loss: 65.23934173583984 = 1.4012045860290527 + 10.0 * 6.383813381195068
Epoch 320, val loss: 1.4685630798339844
Epoch 330, training loss: 65.11869049072266 = 1.3701239824295044 + 10.0 * 6.374856948852539
Epoch 330, val loss: 1.4449834823608398
Epoch 340, training loss: 65.03089141845703 = 1.3391122817993164 + 10.0 * 6.36917781829834
Epoch 340, val loss: 1.4220623970031738
Epoch 350, training loss: 64.98739624023438 = 1.3083148002624512 + 10.0 * 6.367908477783203
Epoch 350, val loss: 1.399609923362732
Epoch 360, training loss: 64.87770080566406 = 1.2776153087615967 + 10.0 * 6.360008239746094
Epoch 360, val loss: 1.3776136636734009
Epoch 370, training loss: 64.81531524658203 = 1.2471837997436523 + 10.0 * 6.356812953948975
Epoch 370, val loss: 1.356274962425232
Epoch 380, training loss: 64.7410659790039 = 1.2171069383621216 + 10.0 * 6.352396011352539
Epoch 380, val loss: 1.33526611328125
Epoch 390, training loss: 64.6747817993164 = 1.1873561143875122 + 10.0 * 6.348742485046387
Epoch 390, val loss: 1.314953088760376
Epoch 400, training loss: 64.60758209228516 = 1.157982587814331 + 10.0 * 6.3449602127075195
Epoch 400, val loss: 1.295238733291626
Epoch 410, training loss: 64.55908966064453 = 1.128974437713623 + 10.0 * 6.343011379241943
Epoch 410, val loss: 1.2756104469299316
Epoch 420, training loss: 64.5015640258789 = 1.100516438484192 + 10.0 * 6.340104579925537
Epoch 420, val loss: 1.257021188735962
Epoch 430, training loss: 64.43260955810547 = 1.072671890258789 + 10.0 * 6.335993766784668
Epoch 430, val loss: 1.2391892671585083
Epoch 440, training loss: 64.37024688720703 = 1.0454280376434326 + 10.0 * 6.332481861114502
Epoch 440, val loss: 1.2220032215118408
Epoch 450, training loss: 64.35952758789062 = 1.018819808959961 + 10.0 * 6.334070682525635
Epoch 450, val loss: 1.205435872077942
Epoch 460, training loss: 64.27278900146484 = 0.9925644397735596 + 10.0 * 6.328022480010986
Epoch 460, val loss: 1.1896631717681885
Epoch 470, training loss: 64.21560668945312 = 0.9673073887825012 + 10.0 * 6.324830055236816
Epoch 470, val loss: 1.1747097969055176
Epoch 480, training loss: 64.16875457763672 = 0.9428260922431946 + 10.0 * 6.322592735290527
Epoch 480, val loss: 1.1606388092041016
Epoch 490, training loss: 64.12300109863281 = 0.9190437197685242 + 10.0 * 6.320395469665527
Epoch 490, val loss: 1.1474353075027466
Epoch 500, training loss: 64.07782745361328 = 0.8959322571754456 + 10.0 * 6.3181891441345215
Epoch 500, val loss: 1.1349101066589355
Epoch 510, training loss: 64.04586029052734 = 0.873616099357605 + 10.0 * 6.317224502563477
Epoch 510, val loss: 1.1232517957687378
Epoch 520, training loss: 64.00517272949219 = 0.8521319031715393 + 10.0 * 6.315304279327393
Epoch 520, val loss: 1.1124060153961182
Epoch 530, training loss: 63.96030044555664 = 0.831376850605011 + 10.0 * 6.312892436981201
Epoch 530, val loss: 1.102425217628479
Epoch 540, training loss: 63.91574478149414 = 0.8112713098526001 + 10.0 * 6.3104472160339355
Epoch 540, val loss: 1.0931497812271118
Epoch 550, training loss: 63.880523681640625 = 0.7919330596923828 + 10.0 * 6.308858871459961
Epoch 550, val loss: 1.0846014022827148
Epoch 560, training loss: 63.860958099365234 = 0.7732211351394653 + 10.0 * 6.308773994445801
Epoch 560, val loss: 1.0769829750061035
Epoch 570, training loss: 63.808815002441406 = 0.7551285028457642 + 10.0 * 6.305368900299072
Epoch 570, val loss: 1.0696879625320435
Epoch 580, training loss: 63.77596664428711 = 0.7376285195350647 + 10.0 * 6.303833961486816
Epoch 580, val loss: 1.0633398294448853
Epoch 590, training loss: 63.76639175415039 = 0.7206545472145081 + 10.0 * 6.304574012756348
Epoch 590, val loss: 1.0574196577072144
Epoch 600, training loss: 63.711124420166016 = 0.7041816115379333 + 10.0 * 6.300694465637207
Epoch 600, val loss: 1.052087664604187
Epoch 610, training loss: 63.700164794921875 = 0.6881960034370422 + 10.0 * 6.301197052001953
Epoch 610, val loss: 1.0473004579544067
Epoch 620, training loss: 63.65763473510742 = 0.6726678013801575 + 10.0 * 6.298496723175049
Epoch 620, val loss: 1.0429953336715698
Epoch 630, training loss: 63.62326431274414 = 0.6575486660003662 + 10.0 * 6.296571731567383
Epoch 630, val loss: 1.0392662286758423
Epoch 640, training loss: 63.59193420410156 = 0.6428314447402954 + 10.0 * 6.294910430908203
Epoch 640, val loss: 1.0360612869262695
Epoch 650, training loss: 63.604061126708984 = 0.6283302307128906 + 10.0 * 6.297573089599609
Epoch 650, val loss: 1.0332871675491333
Epoch 660, training loss: 63.57709503173828 = 0.614159882068634 + 10.0 * 6.29629373550415
Epoch 660, val loss: 1.0305010080337524
Epoch 670, training loss: 63.518680572509766 = 0.6002153158187866 + 10.0 * 6.29184627532959
Epoch 670, val loss: 1.0283509492874146
Epoch 680, training loss: 63.48259353637695 = 0.5866231918334961 + 10.0 * 6.289597034454346
Epoch 680, val loss: 1.026638388633728
Epoch 690, training loss: 63.47314453125 = 0.5732581615447998 + 10.0 * 6.2899885177612305
Epoch 690, val loss: 1.0251922607421875
Epoch 700, training loss: 63.457881927490234 = 0.5600168108940125 + 10.0 * 6.289786338806152
Epoch 700, val loss: 1.024070382118225
Epoch 710, training loss: 63.423274993896484 = 0.5469252467155457 + 10.0 * 6.28763484954834
Epoch 710, val loss: 1.0228967666625977
Epoch 720, training loss: 63.391273498535156 = 0.5341079831123352 + 10.0 * 6.285716533660889
Epoch 720, val loss: 1.0222132205963135
Epoch 730, training loss: 63.3615608215332 = 0.5215045213699341 + 10.0 * 6.284005641937256
Epoch 730, val loss: 1.021767020225525
Epoch 740, training loss: 63.34306335449219 = 0.5090159773826599 + 10.0 * 6.28340482711792
Epoch 740, val loss: 1.0215290784835815
Epoch 750, training loss: 63.342193603515625 = 0.4966506361961365 + 10.0 * 6.284554481506348
Epoch 750, val loss: 1.021437168121338
Epoch 760, training loss: 63.30405807495117 = 0.4844110906124115 + 10.0 * 6.2819647789001465
Epoch 760, val loss: 1.0214496850967407
Epoch 770, training loss: 63.28678894042969 = 0.4723623991012573 + 10.0 * 6.281442642211914
Epoch 770, val loss: 1.021749496459961
Epoch 780, training loss: 63.25989532470703 = 0.46046167612075806 + 10.0 * 6.279943466186523
Epoch 780, val loss: 1.0221883058547974
Epoch 790, training loss: 63.234519958496094 = 0.44872811436653137 + 10.0 * 6.278579235076904
Epoch 790, val loss: 1.0227880477905273
Epoch 800, training loss: 63.22184371948242 = 0.43714457750320435 + 10.0 * 6.278470039367676
Epoch 800, val loss: 1.0235753059387207
Epoch 810, training loss: 63.20175552368164 = 0.42572295665740967 + 10.0 * 6.2776031494140625
Epoch 810, val loss: 1.0244969129562378
Epoch 820, training loss: 63.19224548339844 = 0.41444113850593567 + 10.0 * 6.277780532836914
Epoch 820, val loss: 1.025465488433838
Epoch 830, training loss: 63.20315933227539 = 0.4033527672290802 + 10.0 * 6.279980659484863
Epoch 830, val loss: 1.026558518409729
Epoch 840, training loss: 63.1445198059082 = 0.3923296630382538 + 10.0 * 6.275218963623047
Epoch 840, val loss: 1.0278117656707764
Epoch 850, training loss: 63.11326599121094 = 0.38160350918769836 + 10.0 * 6.273166179656982
Epoch 850, val loss: 1.0292068719863892
Epoch 860, training loss: 63.091880798339844 = 0.3711097836494446 + 10.0 * 6.2720770835876465
Epoch 860, val loss: 1.0308259725570679
Epoch 870, training loss: 63.07455062866211 = 0.36080631613731384 + 10.0 * 6.271374702453613
Epoch 870, val loss: 1.0325628519058228
Epoch 880, training loss: 63.12499237060547 = 0.3506954312324524 + 10.0 * 6.277429580688477
Epoch 880, val loss: 1.034367561340332
Epoch 890, training loss: 63.05423355102539 = 0.3406131863594055 + 10.0 * 6.271361827850342
Epoch 890, val loss: 1.0361363887786865
Epoch 900, training loss: 63.01880645751953 = 0.3308512270450592 + 10.0 * 6.268795490264893
Epoch 900, val loss: 1.0382424592971802
Epoch 910, training loss: 63.00633239746094 = 0.3213348686695099 + 10.0 * 6.268499851226807
Epoch 910, val loss: 1.0405725240707397
Epoch 920, training loss: 63.03575897216797 = 0.311989963054657 + 10.0 * 6.272377014160156
Epoch 920, val loss: 1.0429496765136719
Epoch 930, training loss: 62.987369537353516 = 0.3028922975063324 + 10.0 * 6.2684478759765625
Epoch 930, val loss: 1.0454087257385254
Epoch 940, training loss: 62.992733001708984 = 0.29396718740463257 + 10.0 * 6.269876480102539
Epoch 940, val loss: 1.0481561422348022
Epoch 950, training loss: 62.94498825073242 = 0.2853055000305176 + 10.0 * 6.265968322753906
Epoch 950, val loss: 1.0509932041168213
Epoch 960, training loss: 62.93098449707031 = 0.2768564522266388 + 10.0 * 6.2654128074646
Epoch 960, val loss: 1.054083228111267
Epoch 970, training loss: 62.91374588012695 = 0.26862719655036926 + 10.0 * 6.264512062072754
Epoch 970, val loss: 1.0572421550750732
Epoch 980, training loss: 62.90964889526367 = 0.26060131192207336 + 10.0 * 6.264904975891113
Epoch 980, val loss: 1.0606210231781006
Epoch 990, training loss: 62.89237976074219 = 0.2527710497379303 + 10.0 * 6.263960838317871
Epoch 990, val loss: 1.0639567375183105
Epoch 1000, training loss: 62.8640022277832 = 0.24518121778964996 + 10.0 * 6.2618818283081055
Epoch 1000, val loss: 1.0676534175872803
Epoch 1010, training loss: 62.84551239013672 = 0.23782263696193695 + 10.0 * 6.260768890380859
Epoch 1010, val loss: 1.071426272392273
Epoch 1020, training loss: 62.864402770996094 = 0.2306838035583496 + 10.0 * 6.26337194442749
Epoch 1020, val loss: 1.0753639936447144
Epoch 1030, training loss: 62.86193084716797 = 0.22373002767562866 + 10.0 * 6.263820171356201
Epoch 1030, val loss: 1.0793203115463257
Epoch 1040, training loss: 62.825172424316406 = 0.2169448882341385 + 10.0 * 6.260822772979736
Epoch 1040, val loss: 1.0834659337997437
Epoch 1050, training loss: 62.79145812988281 = 0.2104334533214569 + 10.0 * 6.2581024169921875
Epoch 1050, val loss: 1.0878639221191406
Epoch 1060, training loss: 62.7781867980957 = 0.20416779816150665 + 10.0 * 6.257401943206787
Epoch 1060, val loss: 1.09256911277771
Epoch 1070, training loss: 62.815608978271484 = 0.1980615109205246 + 10.0 * 6.261754512786865
Epoch 1070, val loss: 1.0973566770553589
Epoch 1080, training loss: 62.8010139465332 = 0.1921217441558838 + 10.0 * 6.260889053344727
Epoch 1080, val loss: 1.1019539833068848
Epoch 1090, training loss: 62.745365142822266 = 0.18637137115001678 + 10.0 * 6.255899429321289
Epoch 1090, val loss: 1.106951355934143
Epoch 1100, training loss: 62.742919921875 = 0.18084703385829926 + 10.0 * 6.256207466125488
Epoch 1100, val loss: 1.1121405363082886
Epoch 1110, training loss: 62.73346710205078 = 0.17551685869693756 + 10.0 * 6.255795001983643
Epoch 1110, val loss: 1.1174089908599854
Epoch 1120, training loss: 62.726253509521484 = 0.17034700512886047 + 10.0 * 6.255590915679932
Epoch 1120, val loss: 1.1228013038635254
Epoch 1130, training loss: 62.7214469909668 = 0.1653548628091812 + 10.0 * 6.255609035491943
Epoch 1130, val loss: 1.1282856464385986
Epoch 1140, training loss: 62.68452453613281 = 0.16050706803798676 + 10.0 * 6.252401828765869
Epoch 1140, val loss: 1.1337429285049438
Epoch 1150, training loss: 62.67491912841797 = 0.15584513545036316 + 10.0 * 6.2519073486328125
Epoch 1150, val loss: 1.1395713090896606
Epoch 1160, training loss: 62.66606140136719 = 0.15135274827480316 + 10.0 * 6.251471042633057
Epoch 1160, val loss: 1.1454012393951416
Epoch 1170, training loss: 62.71869659423828 = 0.14701871573925018 + 10.0 * 6.257167816162109
Epoch 1170, val loss: 1.1512774229049683
Epoch 1180, training loss: 62.67388916015625 = 0.14277678728103638 + 10.0 * 6.253111362457275
Epoch 1180, val loss: 1.1571080684661865
Epoch 1190, training loss: 62.6492919921875 = 0.1387079954147339 + 10.0 * 6.251058578491211
Epoch 1190, val loss: 1.1631550788879395
Epoch 1200, training loss: 62.65612030029297 = 0.13479986786842346 + 10.0 * 6.252131938934326
Epoch 1200, val loss: 1.169267177581787
Epoch 1210, training loss: 62.64737319946289 = 0.13097801804542542 + 10.0 * 6.251639366149902
Epoch 1210, val loss: 1.1754218339920044
Epoch 1220, training loss: 62.616607666015625 = 0.12732811272144318 + 10.0 * 6.248928070068359
Epoch 1220, val loss: 1.1815561056137085
Epoch 1230, training loss: 62.61056137084961 = 0.12381290644407272 + 10.0 * 6.2486748695373535
Epoch 1230, val loss: 1.1879558563232422
Epoch 1240, training loss: 62.6083984375 = 0.12041418254375458 + 10.0 * 6.248798370361328
Epoch 1240, val loss: 1.1942299604415894
Epoch 1250, training loss: 62.58438491821289 = 0.11713185161352158 + 10.0 * 6.246725559234619
Epoch 1250, val loss: 1.2007330656051636
Epoch 1260, training loss: 62.584800720214844 = 0.11396929621696472 + 10.0 * 6.2470831871032715
Epoch 1260, val loss: 1.207276463508606
Epoch 1270, training loss: 62.6002311706543 = 0.11089332401752472 + 10.0 * 6.248933792114258
Epoch 1270, val loss: 1.2136597633361816
Epoch 1280, training loss: 62.59708786010742 = 0.10790247470140457 + 10.0 * 6.248918533325195
Epoch 1280, val loss: 1.22006356716156
Epoch 1290, training loss: 62.56468200683594 = 0.10504486411809921 + 10.0 * 6.2459635734558105
Epoch 1290, val loss: 1.2268744707107544
Epoch 1300, training loss: 62.56174850463867 = 0.10228246450424194 + 10.0 * 6.245946407318115
Epoch 1300, val loss: 1.233341097831726
Epoch 1310, training loss: 62.55951690673828 = 0.0996146872639656 + 10.0 * 6.24599027633667
Epoch 1310, val loss: 1.2400890588760376
Epoch 1320, training loss: 62.53736877441406 = 0.09702663868665695 + 10.0 * 6.244034290313721
Epoch 1320, val loss: 1.2466624975204468
Epoch 1330, training loss: 62.52326965332031 = 0.09454317390918732 + 10.0 * 6.242872714996338
Epoch 1330, val loss: 1.2534314393997192
Epoch 1340, training loss: 62.59146499633789 = 0.09214566648006439 + 10.0 * 6.249932289123535
Epoch 1340, val loss: 1.2600206136703491
Epoch 1350, training loss: 62.57429504394531 = 0.08978185802698135 + 10.0 * 6.248451232910156
Epoch 1350, val loss: 1.2666635513305664
Epoch 1360, training loss: 62.51375198364258 = 0.08748143166303635 + 10.0 * 6.242627143859863
Epoch 1360, val loss: 1.2732934951782227
Epoch 1370, training loss: 62.49568176269531 = 0.08531883358955383 + 10.0 * 6.241036415100098
Epoch 1370, val loss: 1.2801400423049927
Epoch 1380, training loss: 62.49141311645508 = 0.08323031663894653 + 10.0 * 6.240818500518799
Epoch 1380, val loss: 1.2869540452957153
Epoch 1390, training loss: 62.53874588012695 = 0.08121193945407867 + 10.0 * 6.245753288269043
Epoch 1390, val loss: 1.293814778327942
Epoch 1400, training loss: 62.5128173828125 = 0.07919353246688843 + 10.0 * 6.2433624267578125
Epoch 1400, val loss: 1.2999805212020874
Epoch 1410, training loss: 62.488773345947266 = 0.07728251069784164 + 10.0 * 6.241148948669434
Epoch 1410, val loss: 1.3070226907730103
Epoch 1420, training loss: 62.50303268432617 = 0.07544262707233429 + 10.0 * 6.242758750915527
Epoch 1420, val loss: 1.3134617805480957
Epoch 1430, training loss: 62.46226119995117 = 0.07364621013402939 + 10.0 * 6.238861560821533
Epoch 1430, val loss: 1.3201096057891846
Epoch 1440, training loss: 62.45510482788086 = 0.0719170942902565 + 10.0 * 6.23831844329834
Epoch 1440, val loss: 1.3268611431121826
Epoch 1450, training loss: 62.454708099365234 = 0.07025172561407089 + 10.0 * 6.23844575881958
Epoch 1450, val loss: 1.333351492881775
Epoch 1460, training loss: 62.51490783691406 = 0.06864827871322632 + 10.0 * 6.244626045227051
Epoch 1460, val loss: 1.3399138450622559
Epoch 1470, training loss: 62.4681396484375 = 0.067033551633358 + 10.0 * 6.240110874176025
Epoch 1470, val loss: 1.3462750911712646
Epoch 1480, training loss: 62.435794830322266 = 0.06550073623657227 + 10.0 * 6.237029075622559
Epoch 1480, val loss: 1.352782130241394
Epoch 1490, training loss: 62.42876434326172 = 0.06403117626905441 + 10.0 * 6.236473560333252
Epoch 1490, val loss: 1.3593500852584839
Epoch 1500, training loss: 62.477054595947266 = 0.06261102855205536 + 10.0 * 6.2414445877075195
Epoch 1500, val loss: 1.365731120109558
Epoch 1510, training loss: 62.45368576049805 = 0.06120508164167404 + 10.0 * 6.239247798919678
Epoch 1510, val loss: 1.371817946434021
Epoch 1520, training loss: 62.4228515625 = 0.05984489247202873 + 10.0 * 6.236300468444824
Epoch 1520, val loss: 1.378340244293213
Epoch 1530, training loss: 62.417293548583984 = 0.058544617146253586 + 10.0 * 6.235875129699707
Epoch 1530, val loss: 1.384679913520813
Epoch 1540, training loss: 62.43142318725586 = 0.05727764591574669 + 10.0 * 6.237414360046387
Epoch 1540, val loss: 1.3909432888031006
Epoch 1550, training loss: 62.4148063659668 = 0.05604188144207001 + 10.0 * 6.235876560211182
Epoch 1550, val loss: 1.3970001935958862
Epoch 1560, training loss: 62.39250183105469 = 0.054840948432683945 + 10.0 * 6.233766078948975
Epoch 1560, val loss: 1.4032337665557861
Epoch 1570, training loss: 62.392879486083984 = 0.053682807832956314 + 10.0 * 6.233919620513916
Epoch 1570, val loss: 1.4095808267593384
Epoch 1580, training loss: 62.39759826660156 = 0.05256500467658043 + 10.0 * 6.234503269195557
Epoch 1580, val loss: 1.4157136678695679
Epoch 1590, training loss: 62.39567184448242 = 0.051458582282066345 + 10.0 * 6.234421253204346
Epoch 1590, val loss: 1.421662449836731
Epoch 1600, training loss: 62.382720947265625 = 0.05036894977092743 + 10.0 * 6.2332353591918945
Epoch 1600, val loss: 1.4275977611541748
Epoch 1610, training loss: 62.370872497558594 = 0.04933901131153107 + 10.0 * 6.232153415679932
Epoch 1610, val loss: 1.4337713718414307
Epoch 1620, training loss: 62.3770751953125 = 0.048341214656829834 + 10.0 * 6.232873439788818
Epoch 1620, val loss: 1.4396758079528809
Epoch 1630, training loss: 62.4166374206543 = 0.04736103117465973 + 10.0 * 6.2369279861450195
Epoch 1630, val loss: 1.4454562664031982
Epoch 1640, training loss: 62.36993408203125 = 0.046395208686590195 + 10.0 * 6.232354164123535
Epoch 1640, val loss: 1.4512304067611694
Epoch 1650, training loss: 62.35350036621094 = 0.04546496644616127 + 10.0 * 6.230803489685059
Epoch 1650, val loss: 1.457136869430542
Epoch 1660, training loss: 62.348838806152344 = 0.04457787424325943 + 10.0 * 6.230425834655762
Epoch 1660, val loss: 1.4629888534545898
Epoch 1670, training loss: 62.401283264160156 = 0.04371565952897072 + 10.0 * 6.235756874084473
Epoch 1670, val loss: 1.4685646295547485
Epoch 1680, training loss: 62.37848663330078 = 0.04284249618649483 + 10.0 * 6.233564376831055
Epoch 1680, val loss: 1.4741902351379395
Epoch 1690, training loss: 62.353111267089844 = 0.0419987253844738 + 10.0 * 6.2311110496521
Epoch 1690, val loss: 1.4796960353851318
Epoch 1700, training loss: 62.3369140625 = 0.041192974895238876 + 10.0 * 6.229572296142578
Epoch 1700, val loss: 1.4853582382202148
Epoch 1710, training loss: 62.35519027709961 = 0.040410690009593964 + 10.0 * 6.231478214263916
Epoch 1710, val loss: 1.4907920360565186
Epoch 1720, training loss: 62.32505798339844 = 0.03964300826191902 + 10.0 * 6.228541374206543
Epoch 1720, val loss: 1.4965003728866577
Epoch 1730, training loss: 62.338558197021484 = 0.03890156373381615 + 10.0 * 6.229965686798096
Epoch 1730, val loss: 1.5021848678588867
Epoch 1740, training loss: 62.36170196533203 = 0.03816927596926689 + 10.0 * 6.232353210449219
Epoch 1740, val loss: 1.507341980934143
Epoch 1750, training loss: 62.32749557495117 = 0.03745299205183983 + 10.0 * 6.229004383087158
Epoch 1750, val loss: 1.5127723217010498
Epoch 1760, training loss: 62.31074905395508 = 0.03676280379295349 + 10.0 * 6.227398872375488
Epoch 1760, val loss: 1.518218994140625
Epoch 1770, training loss: 62.306640625 = 0.036098577082157135 + 10.0 * 6.227054119110107
Epoch 1770, val loss: 1.523543119430542
Epoch 1780, training loss: 62.31916427612305 = 0.035452570766210556 + 10.0 * 6.2283711433410645
Epoch 1780, val loss: 1.5288728475570679
Epoch 1790, training loss: 62.31988525390625 = 0.03481373190879822 + 10.0 * 6.228507041931152
Epoch 1790, val loss: 1.5340865850448608
Epoch 1800, training loss: 62.32939147949219 = 0.034184765070676804 + 10.0 * 6.229520797729492
Epoch 1800, val loss: 1.5391913652420044
Epoch 1810, training loss: 62.299110412597656 = 0.033568598330020905 + 10.0 * 6.226553916931152
Epoch 1810, val loss: 1.5444586277008057
Epoch 1820, training loss: 62.307735443115234 = 0.03297802805900574 + 10.0 * 6.227475643157959
Epoch 1820, val loss: 1.5496608018875122
Epoch 1830, training loss: 62.31581115722656 = 0.03240286931395531 + 10.0 * 6.228341102600098
Epoch 1830, val loss: 1.5545927286148071
Epoch 1840, training loss: 62.290321350097656 = 0.031826864928007126 + 10.0 * 6.225849628448486
Epoch 1840, val loss: 1.5595794916152954
Epoch 1850, training loss: 62.28470230102539 = 0.03127805143594742 + 10.0 * 6.225342750549316
Epoch 1850, val loss: 1.5646685361862183
Epoch 1860, training loss: 62.27848815917969 = 0.030749859288334846 + 10.0 * 6.22477388381958
Epoch 1860, val loss: 1.5697296857833862
Epoch 1870, training loss: 62.321407318115234 = 0.030238378793001175 + 10.0 * 6.229116916656494
Epoch 1870, val loss: 1.57486093044281
Epoch 1880, training loss: 62.29402160644531 = 0.029713643714785576 + 10.0 * 6.226430892944336
Epoch 1880, val loss: 1.5792580842971802
Epoch 1890, training loss: 62.28799819946289 = 0.029206184670329094 + 10.0 * 6.225879192352295
Epoch 1890, val loss: 1.5842235088348389
Epoch 1900, training loss: 62.298160552978516 = 0.028722327202558517 + 10.0 * 6.2269439697265625
Epoch 1900, val loss: 1.5891014337539673
Epoch 1910, training loss: 62.26874542236328 = 0.02823902852833271 + 10.0 * 6.224050521850586
Epoch 1910, val loss: 1.5937288999557495
Epoch 1920, training loss: 62.274051666259766 = 0.027775511145591736 + 10.0 * 6.224627494812012
Epoch 1920, val loss: 1.5986082553863525
Epoch 1930, training loss: 62.267356872558594 = 0.027324514463543892 + 10.0 * 6.224003314971924
Epoch 1930, val loss: 1.6032207012176514
Epoch 1940, training loss: 62.29315185546875 = 0.026884980499744415 + 10.0 * 6.226626396179199
Epoch 1940, val loss: 1.6077791452407837
Epoch 1950, training loss: 62.264705657958984 = 0.02644767425954342 + 10.0 * 6.223825931549072
Epoch 1950, val loss: 1.6123297214508057
Epoch 1960, training loss: 62.253108978271484 = 0.026018571108579636 + 10.0 * 6.222708702087402
Epoch 1960, val loss: 1.6168248653411865
Epoch 1970, training loss: 62.251338958740234 = 0.025607777759432793 + 10.0 * 6.222573280334473
Epoch 1970, val loss: 1.6214807033538818
Epoch 1980, training loss: 62.27065658569336 = 0.025209246203303337 + 10.0 * 6.224545001983643
Epoch 1980, val loss: 1.6259177923202515
Epoch 1990, training loss: 62.26441955566406 = 0.024806423112750053 + 10.0 * 6.223961353302002
Epoch 1990, val loss: 1.630303144454956
Epoch 2000, training loss: 62.25419616699219 = 0.024422161281108856 + 10.0 * 6.222977638244629
Epoch 2000, val loss: 1.634779691696167
Epoch 2010, training loss: 62.25112533569336 = 0.024045566096901894 + 10.0 * 6.222708225250244
Epoch 2010, val loss: 1.639036774635315
Epoch 2020, training loss: 62.25182342529297 = 0.023677047342061996 + 10.0 * 6.222814559936523
Epoch 2020, val loss: 1.6434441804885864
Epoch 2030, training loss: 62.24755859375 = 0.023318590596318245 + 10.0 * 6.222424030303955
Epoch 2030, val loss: 1.647695779800415
Epoch 2040, training loss: 62.24241256713867 = 0.022964410483837128 + 10.0 * 6.221944808959961
Epoch 2040, val loss: 1.6517671346664429
Epoch 2050, training loss: 62.23569107055664 = 0.022618239745497704 + 10.0 * 6.221307277679443
Epoch 2050, val loss: 1.6560461521148682
Epoch 2060, training loss: 62.24441909790039 = 0.022287338972091675 + 10.0 * 6.222213268280029
Epoch 2060, val loss: 1.6604310274124146
Epoch 2070, training loss: 62.22795867919922 = 0.02195369452238083 + 10.0 * 6.220600605010986
Epoch 2070, val loss: 1.6645036935806274
Epoch 2080, training loss: 62.2568473815918 = 0.02162977121770382 + 10.0 * 6.223521709442139
Epoch 2080, val loss: 1.66871178150177
Epoch 2090, training loss: 62.22544479370117 = 0.021312166005373 + 10.0 * 6.2204132080078125
Epoch 2090, val loss: 1.6726675033569336
Epoch 2100, training loss: 62.217079162597656 = 0.02099786140024662 + 10.0 * 6.219608306884766
Epoch 2100, val loss: 1.6766992807388306
Epoch 2110, training loss: 62.221710205078125 = 0.020698463544249535 + 10.0 * 6.220101356506348
Epoch 2110, val loss: 1.6808454990386963
Epoch 2120, training loss: 62.22794723510742 = 0.02040337398648262 + 10.0 * 6.220754146575928
Epoch 2120, val loss: 1.6847370862960815
Epoch 2130, training loss: 62.21642303466797 = 0.020110955461859703 + 10.0 * 6.219631195068359
Epoch 2130, val loss: 1.688668966293335
Epoch 2140, training loss: 62.23502731323242 = 0.019830334931612015 + 10.0 * 6.221519947052002
Epoch 2140, val loss: 1.6924967765808105
Epoch 2150, training loss: 62.22016525268555 = 0.01954859122633934 + 10.0 * 6.220061302185059
Epoch 2150, val loss: 1.6962945461273193
Epoch 2160, training loss: 62.20709991455078 = 0.019273806363344193 + 10.0 * 6.218782424926758
Epoch 2160, val loss: 1.7000477313995361
Epoch 2170, training loss: 62.200599670410156 = 0.01900920271873474 + 10.0 * 6.218159198760986
Epoch 2170, val loss: 1.7039817571640015
Epoch 2180, training loss: 62.21834182739258 = 0.018753690645098686 + 10.0 * 6.219958782196045
Epoch 2180, val loss: 1.707580327987671
Epoch 2190, training loss: 62.226600646972656 = 0.018497956916689873 + 10.0 * 6.220810413360596
Epoch 2190, val loss: 1.7112797498703003
Epoch 2200, training loss: 62.227909088134766 = 0.01824202761054039 + 10.0 * 6.220966815948486
Epoch 2200, val loss: 1.71510910987854
Epoch 2210, training loss: 62.19720458984375 = 0.01798805594444275 + 10.0 * 6.217921733856201
Epoch 2210, val loss: 1.7186247110366821
Epoch 2220, training loss: 62.18756866455078 = 0.017749687656760216 + 10.0 * 6.216981887817383
Epoch 2220, val loss: 1.7223687171936035
Epoch 2230, training loss: 62.188377380371094 = 0.017517350614070892 + 10.0 * 6.217085838317871
Epoch 2230, val loss: 1.725995659828186
Epoch 2240, training loss: 62.228179931640625 = 0.017289219424128532 + 10.0 * 6.2210893630981445
Epoch 2240, val loss: 1.7293604612350464
Epoch 2250, training loss: 62.20088577270508 = 0.01705697365105152 + 10.0 * 6.218382835388184
Epoch 2250, val loss: 1.7330776453018188
Epoch 2260, training loss: 62.19039535522461 = 0.016833068802952766 + 10.0 * 6.217356204986572
Epoch 2260, val loss: 1.7364553213119507
Epoch 2270, training loss: 62.19207763671875 = 0.016617145389318466 + 10.0 * 6.217545986175537
Epoch 2270, val loss: 1.7398818731307983
Epoch 2280, training loss: 62.20589065551758 = 0.016406139358878136 + 10.0 * 6.2189483642578125
Epoch 2280, val loss: 1.743353009223938
Epoch 2290, training loss: 62.190673828125 = 0.01618756540119648 + 10.0 * 6.217448711395264
Epoch 2290, val loss: 1.7466115951538086
Epoch 2300, training loss: 62.182552337646484 = 0.015986425802111626 + 10.0 * 6.216656684875488
Epoch 2300, val loss: 1.7500452995300293
Epoch 2310, training loss: 62.17753982543945 = 0.015784431248903275 + 10.0 * 6.216175556182861
Epoch 2310, val loss: 1.753365397453308
Epoch 2320, training loss: 62.182212829589844 = 0.015586904250085354 + 10.0 * 6.216662406921387
Epoch 2320, val loss: 1.75663423538208
Epoch 2330, training loss: 62.18616485595703 = 0.01539327297359705 + 10.0 * 6.217077255249023
Epoch 2330, val loss: 1.7598345279693604
Epoch 2340, training loss: 62.18391799926758 = 0.015199987217783928 + 10.0 * 6.216871738433838
Epoch 2340, val loss: 1.7629773616790771
Epoch 2350, training loss: 62.18648910522461 = 0.015013271942734718 + 10.0 * 6.217147350311279
Epoch 2350, val loss: 1.7662473917007446
Epoch 2360, training loss: 62.181427001953125 = 0.014830605126917362 + 10.0 * 6.2166595458984375
Epoch 2360, val loss: 1.7692567110061646
Epoch 2370, training loss: 62.16796112060547 = 0.014647656120359898 + 10.0 * 6.215331077575684
Epoch 2370, val loss: 1.772502064704895
Epoch 2380, training loss: 62.15963363647461 = 0.014471917413175106 + 10.0 * 6.2145161628723145
Epoch 2380, val loss: 1.7756108045578003
Epoch 2390, training loss: 62.16603469848633 = 0.014301336370408535 + 10.0 * 6.215173244476318
Epoch 2390, val loss: 1.7787401676177979
Epoch 2400, training loss: 62.18953323364258 = 0.014133092015981674 + 10.0 * 6.2175397872924805
Epoch 2400, val loss: 1.7818864583969116
Epoch 2410, training loss: 62.18669891357422 = 0.013958552852272987 + 10.0 * 6.217274188995361
Epoch 2410, val loss: 1.7846969366073608
Epoch 2420, training loss: 62.18620300292969 = 0.013792227022349834 + 10.0 * 6.217240810394287
Epoch 2420, val loss: 1.7873948812484741
Epoch 2430, training loss: 62.15351486206055 = 0.01362802367657423 + 10.0 * 6.213988304138184
Epoch 2430, val loss: 1.7906347513198853
Epoch 2440, training loss: 62.15037536621094 = 0.013472572900354862 + 10.0 * 6.213690280914307
Epoch 2440, val loss: 1.793613314628601
Epoch 2450, training loss: 62.197025299072266 = 0.013323461636900902 + 10.0 * 6.218369960784912
Epoch 2450, val loss: 1.796202540397644
Epoch 2460, training loss: 62.16848373413086 = 0.013162803836166859 + 10.0 * 6.215531826019287
Epoch 2460, val loss: 1.7994780540466309
Epoch 2470, training loss: 62.15548324584961 = 0.013004123233258724 + 10.0 * 6.214247703552246
Epoch 2470, val loss: 1.8017370700836182
Epoch 2480, training loss: 62.141544342041016 = 0.012858104892075062 + 10.0 * 6.212868690490723
Epoch 2480, val loss: 1.8051226139068604
Epoch 2490, training loss: 62.136070251464844 = 0.01271753665059805 + 10.0 * 6.212335109710693
Epoch 2490, val loss: 1.8079489469528198
Epoch 2500, training loss: 62.13835144042969 = 0.012579281814396381 + 10.0 * 6.212576866149902
Epoch 2500, val loss: 1.8107428550720215
Epoch 2510, training loss: 62.20611572265625 = 0.012444714084267616 + 10.0 * 6.219367027282715
Epoch 2510, val loss: 1.8132745027542114
Epoch 2520, training loss: 62.18937301635742 = 0.012295177206397057 + 10.0 * 6.217707633972168
Epoch 2520, val loss: 1.815830945968628
Epoch 2530, training loss: 62.144466400146484 = 0.012156474404036999 + 10.0 * 6.213231086730957
Epoch 2530, val loss: 1.818549633026123
Epoch 2540, training loss: 62.12937545776367 = 0.012024282477796078 + 10.0 * 6.211735248565674
Epoch 2540, val loss: 1.8213787078857422
Epoch 2550, training loss: 62.12533950805664 = 0.01189719419926405 + 10.0 * 6.211344242095947
Epoch 2550, val loss: 1.8241463899612427
Epoch 2560, training loss: 62.145755767822266 = 0.011772275902330875 + 10.0 * 6.213398456573486
Epoch 2560, val loss: 1.8266066312789917
Epoch 2570, training loss: 62.15178298950195 = 0.011643061414361 + 10.0 * 6.214014053344727
Epoch 2570, val loss: 1.829054594039917
Epoch 2580, training loss: 62.13062286376953 = 0.011512158438563347 + 10.0 * 6.211911201477051
Epoch 2580, val loss: 1.8314021825790405
Epoch 2590, training loss: 62.129940032958984 = 0.011390645056962967 + 10.0 * 6.211854934692383
Epoch 2590, val loss: 1.834362268447876
Epoch 2600, training loss: 62.12277603149414 = 0.011273441836237907 + 10.0 * 6.211150169372559
Epoch 2600, val loss: 1.8369003534317017
Epoch 2610, training loss: 62.1221809387207 = 0.011158634908497334 + 10.0 * 6.211102485656738
Epoch 2610, val loss: 1.8394907712936401
Epoch 2620, training loss: 62.15631103515625 = 0.01104667317122221 + 10.0 * 6.214526176452637
Epoch 2620, val loss: 1.8418233394622803
Epoch 2630, training loss: 62.13304138183594 = 0.010929529555141926 + 10.0 * 6.2122111320495605
Epoch 2630, val loss: 1.8442903757095337
Epoch 2640, training loss: 62.1324462890625 = 0.010812330059707165 + 10.0 * 6.21216344833374
Epoch 2640, val loss: 1.8465427160263062
Epoch 2650, training loss: 62.12786102294922 = 0.010699345730245113 + 10.0 * 6.211716175079346
Epoch 2650, val loss: 1.8490526676177979
Epoch 2660, training loss: 62.108577728271484 = 0.01059169601649046 + 10.0 * 6.209798336029053
Epoch 2660, val loss: 1.8514504432678223
Epoch 2670, training loss: 62.1130256652832 = 0.010488384403288364 + 10.0 * 6.210253715515137
Epoch 2670, val loss: 1.8539444208145142
Epoch 2680, training loss: 62.134605407714844 = 0.010385960340499878 + 10.0 * 6.212421894073486
Epoch 2680, val loss: 1.8561360836029053
Epoch 2690, training loss: 62.13356018066406 = 0.01027968991547823 + 10.0 * 6.21232795715332
Epoch 2690, val loss: 1.8581457138061523
Epoch 2700, training loss: 62.12936019897461 = 0.010174164548516273 + 10.0 * 6.211918830871582
Epoch 2700, val loss: 1.8605798482894897
Epoch 2710, training loss: 62.12633514404297 = 0.010075804777443409 + 10.0 * 6.211626052856445
Epoch 2710, val loss: 1.8629348278045654
Epoch 2720, training loss: 62.1120719909668 = 0.009975132532417774 + 10.0 * 6.210209846496582
Epoch 2720, val loss: 1.8649978637695312
Epoch 2730, training loss: 62.11469650268555 = 0.009879104793071747 + 10.0 * 6.210481643676758
Epoch 2730, val loss: 1.8673285245895386
Epoch 2740, training loss: 62.1351432800293 = 0.009784048423171043 + 10.0 * 6.212535858154297
Epoch 2740, val loss: 1.8693877458572388
Epoch 2750, training loss: 62.13597869873047 = 0.00968899019062519 + 10.0 * 6.2126288414001465
Epoch 2750, val loss: 1.8717124462127686
Epoch 2760, training loss: 62.11847686767578 = 0.009594196453690529 + 10.0 * 6.210888385772705
Epoch 2760, val loss: 1.8737080097198486
Epoch 2770, training loss: 62.10070037841797 = 0.00950293242931366 + 10.0 * 6.20911979675293
Epoch 2770, val loss: 1.8760300874710083
Epoch 2780, training loss: 62.09788513183594 = 0.00941549427807331 + 10.0 * 6.2088470458984375
Epoch 2780, val loss: 1.8783011436462402
Epoch 2790, training loss: 62.09840393066406 = 0.009328317828476429 + 10.0 * 6.208907604217529
Epoch 2790, val loss: 1.8803809881210327
Epoch 2800, training loss: 62.124168395996094 = 0.009243648499250412 + 10.0 * 6.211492538452148
Epoch 2800, val loss: 1.8823981285095215
Epoch 2810, training loss: 62.094844818115234 = 0.009154895320534706 + 10.0 * 6.208569049835205
Epoch 2810, val loss: 1.8841290473937988
Epoch 2820, training loss: 62.151309967041016 = 0.00907177571207285 + 10.0 * 6.214223861694336
Epoch 2820, val loss: 1.885966181755066
Epoch 2830, training loss: 62.10443878173828 = 0.008982425555586815 + 10.0 * 6.209545612335205
Epoch 2830, val loss: 1.888095498085022
Epoch 2840, training loss: 62.09061050415039 = 0.008899230509996414 + 10.0 * 6.2081708908081055
Epoch 2840, val loss: 1.8901026248931885
Epoch 2850, training loss: 62.11975860595703 = 0.00882031675428152 + 10.0 * 6.211093902587891
Epoch 2850, val loss: 1.8923524618148804
Epoch 2860, training loss: 62.101985931396484 = 0.00873995665460825 + 10.0 * 6.209324836730957
Epoch 2860, val loss: 1.8940361738204956
Epoch 2870, training loss: 62.10285568237305 = 0.008660217747092247 + 10.0 * 6.2094197273254395
Epoch 2870, val loss: 1.8958768844604492
Epoch 2880, training loss: 62.08887481689453 = 0.00858320388942957 + 10.0 * 6.208029270172119
Epoch 2880, val loss: 1.898039698600769
Epoch 2890, training loss: 62.124446868896484 = 0.008511161431670189 + 10.0 * 6.2115936279296875
Epoch 2890, val loss: 1.8999263048171997
Epoch 2900, training loss: 62.08634948730469 = 0.0084316311404109 + 10.0 * 6.207791805267334
Epoch 2900, val loss: 1.901667594909668
Epoch 2910, training loss: 62.07763671875 = 0.008356745354831219 + 10.0 * 6.20692777633667
Epoch 2910, val loss: 1.9035170078277588
Epoch 2920, training loss: 62.08897399902344 = 0.008285615593194962 + 10.0 * 6.20806884765625
Epoch 2920, val loss: 1.9053937196731567
Epoch 2930, training loss: 62.122989654541016 = 0.008214672096073627 + 10.0 * 6.211477756500244
Epoch 2930, val loss: 1.9071677923202515
Epoch 2940, training loss: 62.0902214050293 = 0.008140935562551022 + 10.0 * 6.208208084106445
Epoch 2940, val loss: 1.9086750745773315
Epoch 2950, training loss: 62.08515167236328 = 0.008070770651102066 + 10.0 * 6.20770788192749
Epoch 2950, val loss: 1.9105758666992188
Epoch 2960, training loss: 62.096214294433594 = 0.008001850917935371 + 10.0 * 6.2088212966918945
Epoch 2960, val loss: 1.91221022605896
Epoch 2970, training loss: 62.10887145996094 = 0.007934069260954857 + 10.0 * 6.2100934982299805
Epoch 2970, val loss: 1.914122462272644
Epoch 2980, training loss: 62.081512451171875 = 0.007866432890295982 + 10.0 * 6.207364559173584
Epoch 2980, val loss: 1.9157029390335083
Epoch 2990, training loss: 62.07890701293945 = 0.00780049292370677 + 10.0 * 6.207110404968262
Epoch 2990, val loss: 1.917409062385559
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6925925925925926
0.8186610437532947
=== training gcn model ===
Epoch 0, training loss: 87.90522003173828 = 1.937164306640625 + 10.0 * 8.596805572509766
Epoch 0, val loss: 1.9353002309799194
Epoch 10, training loss: 87.88712310791016 = 1.9279577732086182 + 10.0 * 8.595916748046875
Epoch 10, val loss: 1.9259806871414185
Epoch 20, training loss: 87.80968475341797 = 1.916654348373413 + 10.0 * 8.589303016662598
Epoch 20, val loss: 1.9144370555877686
Epoch 30, training loss: 87.3571548461914 = 1.90215265750885 + 10.0 * 8.545499801635742
Epoch 30, val loss: 1.899744987487793
Epoch 40, training loss: 84.67617797851562 = 1.885099172592163 + 10.0 * 8.279108047485352
Epoch 40, val loss: 1.8827245235443115
Epoch 50, training loss: 80.02387237548828 = 1.8669179677963257 + 10.0 * 7.815694808959961
Epoch 50, val loss: 1.8658382892608643
Epoch 60, training loss: 75.05721282958984 = 1.8561404943466187 + 10.0 * 7.320107460021973
Epoch 60, val loss: 1.8565025329589844
Epoch 70, training loss: 71.78495025634766 = 1.8484959602355957 + 10.0 * 6.993645191192627
Epoch 70, val loss: 1.8491944074630737
Epoch 80, training loss: 70.29161834716797 = 1.8409898281097412 + 10.0 * 6.845062732696533
Epoch 80, val loss: 1.8416450023651123
Epoch 90, training loss: 69.35775756835938 = 1.8320194482803345 + 10.0 * 6.7525739669799805
Epoch 90, val loss: 1.833069920539856
Epoch 100, training loss: 68.7013931274414 = 1.8230971097946167 + 10.0 * 6.687829494476318
Epoch 100, val loss: 1.8246843814849854
Epoch 110, training loss: 68.19499969482422 = 1.8151456117630005 + 10.0 * 6.6379852294921875
Epoch 110, val loss: 1.8172508478164673
Epoch 120, training loss: 67.80320739746094 = 1.8078866004943848 + 10.0 * 6.599532127380371
Epoch 120, val loss: 1.8103861808776855
Epoch 130, training loss: 67.48664093017578 = 1.8005849123001099 + 10.0 * 6.568605899810791
Epoch 130, val loss: 1.8034762144088745
Epoch 140, training loss: 67.22762298583984 = 1.7930822372436523 + 10.0 * 6.543454170227051
Epoch 140, val loss: 1.7964054346084595
Epoch 150, training loss: 67.02096557617188 = 1.7852095365524292 + 10.0 * 6.523575782775879
Epoch 150, val loss: 1.7891165018081665
Epoch 160, training loss: 66.82372283935547 = 1.7768445014953613 + 10.0 * 6.504687786102295
Epoch 160, val loss: 1.7815377712249756
Epoch 170, training loss: 66.65522003173828 = 1.7677727937698364 + 10.0 * 6.488744258880615
Epoch 170, val loss: 1.7735189199447632
Epoch 180, training loss: 66.4867935180664 = 1.757951021194458 + 10.0 * 6.472884178161621
Epoch 180, val loss: 1.764947533607483
Epoch 190, training loss: 66.35810089111328 = 1.7471576929092407 + 10.0 * 6.461094379425049
Epoch 190, val loss: 1.7557607889175415
Epoch 200, training loss: 66.2215576171875 = 1.7352395057678223 + 10.0 * 6.448631763458252
Epoch 200, val loss: 1.7457786798477173
Epoch 210, training loss: 66.0947265625 = 1.7221386432647705 + 10.0 * 6.437259197235107
Epoch 210, val loss: 1.734921932220459
Epoch 220, training loss: 65.98287200927734 = 1.7077248096466064 + 10.0 * 6.427515029907227
Epoch 220, val loss: 1.7230981588363647
Epoch 230, training loss: 65.90396881103516 = 1.6919151544570923 + 10.0 * 6.421205520629883
Epoch 230, val loss: 1.7101788520812988
Epoch 240, training loss: 65.79072570800781 = 1.6745054721832275 + 10.0 * 6.411621570587158
Epoch 240, val loss: 1.6961073875427246
Epoch 250, training loss: 65.68782806396484 = 1.6555920839309692 + 10.0 * 6.403223514556885
Epoch 250, val loss: 1.6808477640151978
Epoch 260, training loss: 65.60041046142578 = 1.635164499282837 + 10.0 * 6.396524429321289
Epoch 260, val loss: 1.6644806861877441
Epoch 270, training loss: 65.53055572509766 = 1.613168716430664 + 10.0 * 6.391738414764404
Epoch 270, val loss: 1.647020936012268
Epoch 280, training loss: 65.44317626953125 = 1.589795708656311 + 10.0 * 6.385338306427002
Epoch 280, val loss: 1.6284576654434204
Epoch 290, training loss: 65.35257720947266 = 1.5650745630264282 + 10.0 * 6.378750324249268
Epoch 290, val loss: 1.6090950965881348
Epoch 300, training loss: 65.27869415283203 = 1.5392265319824219 + 10.0 * 6.373946666717529
Epoch 300, val loss: 1.5889369249343872
Epoch 310, training loss: 65.20381927490234 = 1.5123393535614014 + 10.0 * 6.369147777557373
Epoch 310, val loss: 1.5680148601531982
Epoch 320, training loss: 65.16439819335938 = 1.4845731258392334 + 10.0 * 6.367982387542725
Epoch 320, val loss: 1.5466585159301758
Epoch 330, training loss: 65.060791015625 = 1.456272006034851 + 10.0 * 6.360452175140381
Epoch 330, val loss: 1.5249168872833252
Epoch 340, training loss: 64.9879379272461 = 1.4276227951049805 + 10.0 * 6.356031894683838
Epoch 340, val loss: 1.5031177997589111
Epoch 350, training loss: 64.92070007324219 = 1.3987852334976196 + 10.0 * 6.35219144821167
Epoch 350, val loss: 1.4812711477279663
Epoch 360, training loss: 64.93013000488281 = 1.3697246313095093 + 10.0 * 6.356040000915527
Epoch 360, val loss: 1.4595786333084106
Epoch 370, training loss: 64.80604553222656 = 1.3407955169677734 + 10.0 * 6.346525192260742
Epoch 370, val loss: 1.4379985332489014
Epoch 380, training loss: 64.7361831665039 = 1.312111735343933 + 10.0 * 6.342406749725342
Epoch 380, val loss: 1.4167472124099731
Epoch 390, training loss: 64.68006896972656 = 1.2837268114089966 + 10.0 * 6.339634418487549
Epoch 390, val loss: 1.395927906036377
Epoch 400, training loss: 64.62229919433594 = 1.2556122541427612 + 10.0 * 6.336668491363525
Epoch 400, val loss: 1.375529408454895
Epoch 410, training loss: 64.58902740478516 = 1.227921485900879 + 10.0 * 6.336110591888428
Epoch 410, val loss: 1.3556084632873535
Epoch 420, training loss: 64.51319885253906 = 1.2006279230117798 + 10.0 * 6.331257343292236
Epoch 420, val loss: 1.3362383842468262
Epoch 430, training loss: 64.45173645019531 = 1.1738861799240112 + 10.0 * 6.327784538269043
Epoch 430, val loss: 1.3174189329147339
Epoch 440, training loss: 64.41179656982422 = 1.1475692987442017 + 10.0 * 6.326422691345215
Epoch 440, val loss: 1.2991594076156616
Epoch 450, training loss: 64.3831558227539 = 1.1216577291488647 + 10.0 * 6.326149940490723
Epoch 450, val loss: 1.281366229057312
Epoch 460, training loss: 64.31564331054688 = 1.0961965322494507 + 10.0 * 6.321944713592529
Epoch 460, val loss: 1.2642613649368286
Epoch 470, training loss: 64.26824951171875 = 1.0712674856185913 + 10.0 * 6.319697856903076
Epoch 470, val loss: 1.2477848529815674
Epoch 480, training loss: 64.2086181640625 = 1.0468823909759521 + 10.0 * 6.316174030303955
Epoch 480, val loss: 1.2320197820663452
Epoch 490, training loss: 64.16015625 = 1.0229929685592651 + 10.0 * 6.313715934753418
Epoch 490, val loss: 1.2168861627578735
Epoch 500, training loss: 64.17703247070312 = 0.9995744228363037 + 10.0 * 6.317746162414551
Epoch 500, val loss: 1.202369213104248
Epoch 510, training loss: 64.08058166503906 = 0.9766280651092529 + 10.0 * 6.310395240783691
Epoch 510, val loss: 1.188301682472229
Epoch 520, training loss: 64.03007507324219 = 0.9542430639266968 + 10.0 * 6.307583332061768
Epoch 520, val loss: 1.1751435995101929
Epoch 530, training loss: 63.985347747802734 = 0.932402491569519 + 10.0 * 6.305294513702393
Epoch 530, val loss: 1.1627123355865479
Epoch 540, training loss: 63.98505783081055 = 0.9109917283058167 + 10.0 * 6.307406425476074
Epoch 540, val loss: 1.1509283781051636
Epoch 550, training loss: 63.942928314208984 = 0.8900695443153381 + 10.0 * 6.305285930633545
Epoch 550, val loss: 1.1395797729492188
Epoch 560, training loss: 63.870506286621094 = 0.8695029020309448 + 10.0 * 6.300100326538086
Epoch 560, val loss: 1.1288926601409912
Epoch 570, training loss: 63.86499786376953 = 0.8494327068328857 + 10.0 * 6.301556587219238
Epoch 570, val loss: 1.1187365055084229
Epoch 580, training loss: 63.80089569091797 = 0.8297648429870605 + 10.0 * 6.297112941741943
Epoch 580, val loss: 1.1091539859771729
Epoch 590, training loss: 63.76137161254883 = 0.8105493187904358 + 10.0 * 6.295082092285156
Epoch 590, val loss: 1.1001861095428467
Epoch 600, training loss: 63.79954147338867 = 0.7916976809501648 + 10.0 * 6.300784111022949
Epoch 600, val loss: 1.0916738510131836
Epoch 610, training loss: 63.70661544799805 = 0.7732415795326233 + 10.0 * 6.293337345123291
Epoch 610, val loss: 1.083475112915039
Epoch 620, training loss: 63.660133361816406 = 0.7552563548088074 + 10.0 * 6.290487766265869
Epoch 620, val loss: 1.0759601593017578
Epoch 630, training loss: 63.62688446044922 = 0.7377051115036011 + 10.0 * 6.2889180183410645
Epoch 630, val loss: 1.0688683986663818
Epoch 640, training loss: 63.59230041503906 = 0.7204874753952026 + 10.0 * 6.287181377410889
Epoch 640, val loss: 1.062168836593628
Epoch 650, training loss: 63.66740417480469 = 0.7036082148551941 + 10.0 * 6.296379566192627
Epoch 650, val loss: 1.0557689666748047
Epoch 660, training loss: 63.568519592285156 = 0.6869823932647705 + 10.0 * 6.288153648376465
Epoch 660, val loss: 1.0496684312820435
Epoch 670, training loss: 63.5175895690918 = 0.6708014011383057 + 10.0 * 6.2846784591674805
Epoch 670, val loss: 1.0441163778305054
Epoch 680, training loss: 63.47721862792969 = 0.6550635099411011 + 10.0 * 6.282215595245361
Epoch 680, val loss: 1.0389931201934814
Epoch 690, training loss: 63.45087814331055 = 0.6396663784980774 + 10.0 * 6.281121253967285
Epoch 690, val loss: 1.0341731309890747
Epoch 700, training loss: 63.467010498046875 = 0.6245247721672058 + 10.0 * 6.2842488288879395
Epoch 700, val loss: 1.0295014381408691
Epoch 710, training loss: 63.41960525512695 = 0.609662652015686 + 10.0 * 6.280994415283203
Epoch 710, val loss: 1.0251734256744385
Epoch 720, training loss: 63.37087631225586 = 0.5952039361000061 + 10.0 * 6.277567386627197
Epoch 720, val loss: 1.0212807655334473
Epoch 730, training loss: 63.37926483154297 = 0.5810741782188416 + 10.0 * 6.279819011688232
Epoch 730, val loss: 1.0176640748977661
Epoch 740, training loss: 63.33163070678711 = 0.5671887993812561 + 10.0 * 6.276444435119629
Epoch 740, val loss: 1.0143524408340454
Epoch 750, training loss: 63.3022346496582 = 0.5536868572235107 + 10.0 * 6.27485466003418
Epoch 750, val loss: 1.0113164186477661
Epoch 760, training loss: 63.29895782470703 = 0.5404338836669922 + 10.0 * 6.275852203369141
Epoch 760, val loss: 1.0085976123809814
Epoch 770, training loss: 63.25202178955078 = 0.5274831652641296 + 10.0 * 6.272453784942627
Epoch 770, val loss: 1.0060597658157349
Epoch 780, training loss: 63.242774963378906 = 0.5147839188575745 + 10.0 * 6.272799015045166
Epoch 780, val loss: 1.0039620399475098
Epoch 790, training loss: 63.240135192871094 = 0.5023833513259888 + 10.0 * 6.273775100708008
Epoch 790, val loss: 1.001956820487976
Epoch 800, training loss: 63.20265197753906 = 0.49021390080451965 + 10.0 * 6.271243572235107
Epoch 800, val loss: 1.0002729892730713
Epoch 810, training loss: 63.15963363647461 = 0.4783429503440857 + 10.0 * 6.268128871917725
Epoch 810, val loss: 0.9988190531730652
Epoch 820, training loss: 63.13701629638672 = 0.46676790714263916 + 10.0 * 6.267024993896484
Epoch 820, val loss: 0.9976915121078491
Epoch 830, training loss: 63.16242599487305 = 0.45541635155677795 + 10.0 * 6.270700931549072
Epoch 830, val loss: 0.9967329502105713
Epoch 840, training loss: 63.1495246887207 = 0.4442165195941925 + 10.0 * 6.270530700683594
Epoch 840, val loss: 0.9958343505859375
Epoch 850, training loss: 63.07970428466797 = 0.43330419063568115 + 10.0 * 6.264639854431152
Epoch 850, val loss: 0.9952609539031982
Epoch 860, training loss: 63.062538146972656 = 0.4226621985435486 + 10.0 * 6.2639875411987305
Epoch 860, val loss: 0.9949368834495544
Epoch 870, training loss: 63.04166793823242 = 0.4122132956981659 + 10.0 * 6.262945652008057
Epoch 870, val loss: 0.9948387145996094
Epoch 880, training loss: 63.05946731567383 = 0.4020191729068756 + 10.0 * 6.265744686126709
Epoch 880, val loss: 0.9948638081550598
Epoch 890, training loss: 63.046810150146484 = 0.39196330308914185 + 10.0 * 6.265484809875488
Epoch 890, val loss: 0.9948500990867615
Epoch 900, training loss: 63.000022888183594 = 0.3821136951446533 + 10.0 * 6.261790752410889
Epoch 900, val loss: 0.9951719045639038
Epoch 910, training loss: 62.964115142822266 = 0.3725460171699524 + 10.0 * 6.259156703948975
Epoch 910, val loss: 0.9958076477050781
Epoch 920, training loss: 62.951751708984375 = 0.3632128834724426 + 10.0 * 6.258853912353516
Epoch 920, val loss: 0.9966283440589905
Epoch 930, training loss: 63.02263641357422 = 0.3540843427181244 + 10.0 * 6.266855239868164
Epoch 930, val loss: 0.9975177645683289
Epoch 940, training loss: 62.92828369140625 = 0.3450087308883667 + 10.0 * 6.258327484130859
Epoch 940, val loss: 0.9984442591667175
Epoch 950, training loss: 62.91365051269531 = 0.3362521231174469 + 10.0 * 6.257740020751953
Epoch 950, val loss: 0.9995964765548706
Epoch 960, training loss: 62.88253402709961 = 0.32773613929748535 + 10.0 * 6.25547981262207
Epoch 960, val loss: 1.001082420349121
Epoch 970, training loss: 62.87885284423828 = 0.3194131851196289 + 10.0 * 6.25594425201416
Epoch 970, val loss: 1.0026828050613403
Epoch 980, training loss: 62.887760162353516 = 0.311237096786499 + 10.0 * 6.257652282714844
Epoch 980, val loss: 1.0042364597320557
Epoch 990, training loss: 62.854888916015625 = 0.3032180964946747 + 10.0 * 6.255167007446289
Epoch 990, val loss: 1.006007432937622
Epoch 1000, training loss: 62.83298110961914 = 0.29540756344795227 + 10.0 * 6.253757476806641
Epoch 1000, val loss: 1.0081144571304321
Epoch 1010, training loss: 62.81121826171875 = 0.28781017661094666 + 10.0 * 6.252340793609619
Epoch 1010, val loss: 1.0102825164794922
Epoch 1020, training loss: 62.81749725341797 = 0.2803988456726074 + 10.0 * 6.25370979309082
Epoch 1020, val loss: 1.012616515159607
Epoch 1030, training loss: 62.79698944091797 = 0.27310997247695923 + 10.0 * 6.252388000488281
Epoch 1030, val loss: 1.0149177312850952
Epoch 1040, training loss: 62.790279388427734 = 0.2660030722618103 + 10.0 * 6.252427577972412
Epoch 1040, val loss: 1.0174188613891602
Epoch 1050, training loss: 62.76882553100586 = 0.2590661346912384 + 10.0 * 6.250975608825684
Epoch 1050, val loss: 1.020318627357483
Epoch 1060, training loss: 62.78628158569336 = 0.25233933329582214 + 10.0 * 6.25339412689209
Epoch 1060, val loss: 1.023095726966858
Epoch 1070, training loss: 62.75016784667969 = 0.24574102461338043 + 10.0 * 6.2504425048828125
Epoch 1070, val loss: 1.0261541604995728
Epoch 1080, training loss: 62.730812072753906 = 0.2393360733985901 + 10.0 * 6.249147415161133
Epoch 1080, val loss: 1.0292092561721802
Epoch 1090, training loss: 62.72788619995117 = 0.23308813571929932 + 10.0 * 6.2494797706604
Epoch 1090, val loss: 1.0325003862380981
Epoch 1100, training loss: 62.71989059448242 = 0.226981982588768 + 10.0 * 6.249290943145752
Epoch 1100, val loss: 1.0356240272521973
Epoch 1110, training loss: 62.69948196411133 = 0.22103428840637207 + 10.0 * 6.247844696044922
Epoch 1110, val loss: 1.0390211343765259
Epoch 1120, training loss: 62.686946868896484 = 0.21523641049861908 + 10.0 * 6.247170925140381
Epoch 1120, val loss: 1.0424611568450928
Epoch 1130, training loss: 62.6735725402832 = 0.2096225768327713 + 10.0 * 6.246395111083984
Epoch 1130, val loss: 1.0462292432785034
Epoch 1140, training loss: 62.681190490722656 = 0.2041371613740921 + 10.0 * 6.247705459594727
Epoch 1140, val loss: 1.0498387813568115
Epoch 1150, training loss: 62.65623092651367 = 0.19875869154930115 + 10.0 * 6.2457475662231445
Epoch 1150, val loss: 1.053613543510437
Epoch 1160, training loss: 62.64506149291992 = 0.1935446560382843 + 10.0 * 6.245151519775391
Epoch 1160, val loss: 1.0573099851608276
Epoch 1170, training loss: 62.6652717590332 = 0.1884850710630417 + 10.0 * 6.247678756713867
Epoch 1170, val loss: 1.0612645149230957
Epoch 1180, training loss: 62.622928619384766 = 0.18351224064826965 + 10.0 * 6.243941307067871
Epoch 1180, val loss: 1.0651408433914185
Epoch 1190, training loss: 62.601806640625 = 0.1787041574716568 + 10.0 * 6.242310523986816
Epoch 1190, val loss: 1.0693256855010986
Epoch 1200, training loss: 62.59699249267578 = 0.17404679954051971 + 10.0 * 6.242294788360596
Epoch 1200, val loss: 1.0733662843704224
Epoch 1210, training loss: 62.638919830322266 = 0.16951721906661987 + 10.0 * 6.2469401359558105
Epoch 1210, val loss: 1.0775336027145386
Epoch 1220, training loss: 62.59235763549805 = 0.1650353968143463 + 10.0 * 6.242732048034668
Epoch 1220, val loss: 1.0816084146499634
Epoch 1230, training loss: 62.5701904296875 = 0.16073690354824066 + 10.0 * 6.240945339202881
Epoch 1230, val loss: 1.0859613418579102
Epoch 1240, training loss: 62.582393646240234 = 0.15655240416526794 + 10.0 * 6.242584228515625
Epoch 1240, val loss: 1.0903462171554565
Epoch 1250, training loss: 62.55598449707031 = 0.1524728685617447 + 10.0 * 6.24035120010376
Epoch 1250, val loss: 1.094520926475525
Epoch 1260, training loss: 62.607177734375 = 0.1484895646572113 + 10.0 * 6.245868682861328
Epoch 1260, val loss: 1.0990244150161743
Epoch 1270, training loss: 62.548583984375 = 0.14461982250213623 + 10.0 * 6.240396499633789
Epoch 1270, val loss: 1.103309988975525
Epoch 1280, training loss: 62.52198791503906 = 0.14088912308216095 + 10.0 * 6.238110065460205
Epoch 1280, val loss: 1.1080913543701172
Epoch 1290, training loss: 62.507991790771484 = 0.1372688263654709 + 10.0 * 6.237072467803955
Epoch 1290, val loss: 1.1127266883850098
Epoch 1300, training loss: 62.51728820800781 = 0.13374635577201843 + 10.0 * 6.238354206085205
Epoch 1300, val loss: 1.117448091506958
Epoch 1310, training loss: 62.511634826660156 = 0.13029563426971436 + 10.0 * 6.238133907318115
Epoch 1310, val loss: 1.1219148635864258
Epoch 1320, training loss: 62.50448989868164 = 0.12694112956523895 + 10.0 * 6.237754821777344
Epoch 1320, val loss: 1.1266555786132812
Epoch 1330, training loss: 62.482784271240234 = 0.12371862679719925 + 10.0 * 6.235906600952148
Epoch 1330, val loss: 1.1315877437591553
Epoch 1340, training loss: 62.48095703125 = 0.12059150636196136 + 10.0 * 6.236036777496338
Epoch 1340, val loss: 1.136467456817627
Epoch 1350, training loss: 62.50621032714844 = 0.1175411269068718 + 10.0 * 6.238866806030273
Epoch 1350, val loss: 1.1412349939346313
Epoch 1360, training loss: 62.47135543823242 = 0.11456671357154846 + 10.0 * 6.235678672790527
Epoch 1360, val loss: 1.146242380142212
Epoch 1370, training loss: 62.4722900390625 = 0.11169586330652237 + 10.0 * 6.236059665679932
Epoch 1370, val loss: 1.1510541439056396
Epoch 1380, training loss: 62.46926498413086 = 0.10889793187379837 + 10.0 * 6.236036777496338
Epoch 1380, val loss: 1.156058430671692
Epoch 1390, training loss: 62.475341796875 = 0.1061909943819046 + 10.0 * 6.236915111541748
Epoch 1390, val loss: 1.1610665321350098
Epoch 1400, training loss: 62.441383361816406 = 0.1035376787185669 + 10.0 * 6.2337846755981445
Epoch 1400, val loss: 1.1657919883728027
Epoch 1410, training loss: 62.44411849975586 = 0.10098473727703094 + 10.0 * 6.234313011169434
Epoch 1410, val loss: 1.1708741188049316
Epoch 1420, training loss: 62.44012451171875 = 0.09851180016994476 + 10.0 * 6.234161376953125
Epoch 1420, val loss: 1.1759716272354126
Epoch 1430, training loss: 62.422306060791016 = 0.09610231965780258 + 10.0 * 6.2326202392578125
Epoch 1430, val loss: 1.180899739265442
Epoch 1440, training loss: 62.41462326049805 = 0.09376710653305054 + 10.0 * 6.232085704803467
Epoch 1440, val loss: 1.185940146446228
Epoch 1450, training loss: 62.433414459228516 = 0.0915018767118454 + 10.0 * 6.234190940856934
Epoch 1450, val loss: 1.191062569618225
Epoch 1460, training loss: 62.410499572753906 = 0.08927829563617706 + 10.0 * 6.23212194442749
Epoch 1460, val loss: 1.1958023309707642
Epoch 1470, training loss: 62.40479278564453 = 0.08712916821241379 + 10.0 * 6.231766700744629
Epoch 1470, val loss: 1.2008206844329834
Epoch 1480, training loss: 62.39377975463867 = 0.08504090458154678 + 10.0 * 6.230874061584473
Epoch 1480, val loss: 1.2058900594711304
Epoch 1490, training loss: 62.426124572753906 = 0.08302252739667892 + 10.0 * 6.234310150146484
Epoch 1490, val loss: 1.210740327835083
Epoch 1500, training loss: 62.39613342285156 = 0.08106144517660141 + 10.0 * 6.231507301330566
Epoch 1500, val loss: 1.21584951877594
Epoch 1510, training loss: 62.390724182128906 = 0.07915108650922775 + 10.0 * 6.231157302856445
Epoch 1510, val loss: 1.220805287361145
Epoch 1520, training loss: 62.36940002441406 = 0.07730358839035034 + 10.0 * 6.2292094230651855
Epoch 1520, val loss: 1.225864291191101
Epoch 1530, training loss: 62.359920501708984 = 0.07551831007003784 + 10.0 * 6.228440284729004
Epoch 1530, val loss: 1.2310289144515991
Epoch 1540, training loss: 62.37657165527344 = 0.0737944096326828 + 10.0 * 6.2302775382995605
Epoch 1540, val loss: 1.2359577417373657
Epoch 1550, training loss: 62.364681243896484 = 0.07209490984678268 + 10.0 * 6.2292585372924805
Epoch 1550, val loss: 1.2406957149505615
Epoch 1560, training loss: 62.36443328857422 = 0.07043550908565521 + 10.0 * 6.229399681091309
Epoch 1560, val loss: 1.2453995943069458
Epoch 1570, training loss: 62.35747146606445 = 0.0688365176320076 + 10.0 * 6.228863716125488
Epoch 1570, val loss: 1.2506519556045532
Epoch 1580, training loss: 62.33954620361328 = 0.06727747619152069 + 10.0 * 6.227227210998535
Epoch 1580, val loss: 1.2554337978363037
Epoch 1590, training loss: 62.339473724365234 = 0.06578372418880463 + 10.0 * 6.2273688316345215
Epoch 1590, val loss: 1.2606068849563599
Epoch 1600, training loss: 62.36551284790039 = 0.06432559341192245 + 10.0 * 6.230118751525879
Epoch 1600, val loss: 1.2651559114456177
Epoch 1610, training loss: 62.33894348144531 = 0.06288869678974152 + 10.0 * 6.22760534286499
Epoch 1610, val loss: 1.2699204683303833
Epoch 1620, training loss: 62.32362365722656 = 0.06150493398308754 + 10.0 * 6.226212024688721
Epoch 1620, val loss: 1.2750087976455688
Epoch 1630, training loss: 62.32610321044922 = 0.06016358733177185 + 10.0 * 6.226593971252441
Epoch 1630, val loss: 1.2797415256500244
Epoch 1640, training loss: 62.3198127746582 = 0.05886126682162285 + 10.0 * 6.226095199584961
Epoch 1640, val loss: 1.2847726345062256
Epoch 1650, training loss: 62.33031463623047 = 0.057594239711761475 + 10.0 * 6.227272033691406
Epoch 1650, val loss: 1.2894999980926514
Epoch 1660, training loss: 62.3128662109375 = 0.05635657534003258 + 10.0 * 6.225650787353516
Epoch 1660, val loss: 1.294240951538086
Epoch 1670, training loss: 62.33083724975586 = 0.05516330152750015 + 10.0 * 6.227567195892334
Epoch 1670, val loss: 1.2992244958877563
Epoch 1680, training loss: 62.30893325805664 = 0.0539863221347332 + 10.0 * 6.225494861602783
Epoch 1680, val loss: 1.3036807775497437
Epoch 1690, training loss: 62.28763198852539 = 0.05284779146313667 + 10.0 * 6.223478317260742
Epoch 1690, val loss: 1.3084852695465088
Epoch 1700, training loss: 62.28517532348633 = 0.05174699425697327 + 10.0 * 6.2233428955078125
Epoch 1700, val loss: 1.3131284713745117
Epoch 1710, training loss: 62.30241394042969 = 0.05067925900220871 + 10.0 * 6.225173473358154
Epoch 1710, val loss: 1.3176946640014648
Epoch 1720, training loss: 62.28675079345703 = 0.049638018012046814 + 10.0 * 6.2237114906311035
Epoch 1720, val loss: 1.3223965167999268
Epoch 1730, training loss: 62.27572250366211 = 0.04862440004944801 + 10.0 * 6.222709655761719
Epoch 1730, val loss: 1.3268272876739502
Epoch 1740, training loss: 62.28721237182617 = 0.047633878886699677 + 10.0 * 6.2239580154418945
Epoch 1740, val loss: 1.3313868045806885
Epoch 1750, training loss: 62.292823791503906 = 0.0466734878718853 + 10.0 * 6.224615097045898
Epoch 1750, val loss: 1.3356586694717407
Epoch 1760, training loss: 62.301700592041016 = 0.04573328047990799 + 10.0 * 6.2255964279174805
Epoch 1760, val loss: 1.3398213386535645
Epoch 1770, training loss: 62.263877868652344 = 0.04482792317867279 + 10.0 * 6.22190523147583
Epoch 1770, val loss: 1.3449376821517944
Epoch 1780, training loss: 62.25547790527344 = 0.04393976926803589 + 10.0 * 6.221153736114502
Epoch 1780, val loss: 1.3489775657653809
Epoch 1790, training loss: 62.266544342041016 = 0.04308600351214409 + 10.0 * 6.22234582901001
Epoch 1790, val loss: 1.3534722328186035
Epoch 1800, training loss: 62.258705139160156 = 0.04224856570363045 + 10.0 * 6.221645832061768
Epoch 1800, val loss: 1.357786774635315
Epoch 1810, training loss: 62.246578216552734 = 0.0414310023188591 + 10.0 * 6.22051477432251
Epoch 1810, val loss: 1.3622307777404785
Epoch 1820, training loss: 62.24713134765625 = 0.040635738521814346 + 10.0 * 6.220649719238281
Epoch 1820, val loss: 1.366435170173645
Epoch 1830, training loss: 62.30487823486328 = 0.0398709662258625 + 10.0 * 6.226500511169434
Epoch 1830, val loss: 1.3702939748764038
Epoch 1840, training loss: 62.27960968017578 = 0.03910314291715622 + 10.0 * 6.224050521850586
Epoch 1840, val loss: 1.3748486042022705
Epoch 1850, training loss: 62.23122787475586 = 0.03836319223046303 + 10.0 * 6.2192864418029785
Epoch 1850, val loss: 1.3789345026016235
Epoch 1860, training loss: 62.22639083862305 = 0.03764570876955986 + 10.0 * 6.218874454498291
Epoch 1860, val loss: 1.383227825164795
Epoch 1870, training loss: 62.2195930480957 = 0.036956265568733215 + 10.0 * 6.218263626098633
Epoch 1870, val loss: 1.3874412775039673
Epoch 1880, training loss: 62.24062728881836 = 0.036282364279031754 + 10.0 * 6.220434665679932
Epoch 1880, val loss: 1.3914893865585327
Epoch 1890, training loss: 62.25478744506836 = 0.03562437370419502 + 10.0 * 6.221916198730469
Epoch 1890, val loss: 1.3958077430725098
Epoch 1900, training loss: 62.22276306152344 = 0.034953732043504715 + 10.0 * 6.218780994415283
Epoch 1900, val loss: 1.3991868495941162
Epoch 1910, training loss: 62.210784912109375 = 0.03433049097657204 + 10.0 * 6.217645168304443
Epoch 1910, val loss: 1.4036134481430054
Epoch 1920, training loss: 62.20330810546875 = 0.0337214320898056 + 10.0 * 6.216958522796631
Epoch 1920, val loss: 1.4077314138412476
Epoch 1930, training loss: 62.21364974975586 = 0.03313066437840462 + 10.0 * 6.218051910400391
Epoch 1930, val loss: 1.4117166996002197
Epoch 1940, training loss: 62.24551773071289 = 0.03254450857639313 + 10.0 * 6.221297264099121
Epoch 1940, val loss: 1.4150563478469849
Epoch 1950, training loss: 62.210487365722656 = 0.03196076303720474 + 10.0 * 6.217852592468262
Epoch 1950, val loss: 1.4192469120025635
Epoch 1960, training loss: 62.192413330078125 = 0.031406790018081665 + 10.0 * 6.216100692749023
Epoch 1960, val loss: 1.4235548973083496
Epoch 1970, training loss: 62.19036865234375 = 0.03086909092962742 + 10.0 * 6.215950012207031
Epoch 1970, val loss: 1.4273149967193604
Epoch 1980, training loss: 62.21881103515625 = 0.030349936336278915 + 10.0 * 6.218846321105957
Epoch 1980, val loss: 1.431447148323059
Epoch 1990, training loss: 62.207515716552734 = 0.029823480173945427 + 10.0 * 6.217769145965576
Epoch 1990, val loss: 1.4344559907913208
Epoch 2000, training loss: 62.18295669555664 = 0.029313569888472557 + 10.0 * 6.215364456176758
Epoch 2000, val loss: 1.4384177923202515
Epoch 2010, training loss: 62.18159866333008 = 0.028819547966122627 + 10.0 * 6.215277671813965
Epoch 2010, val loss: 1.4423748254776
Epoch 2020, training loss: 62.17841339111328 = 0.028344089165329933 + 10.0 * 6.2150068283081055
Epoch 2020, val loss: 1.4460315704345703
Epoch 2030, training loss: 62.273868560791016 = 0.02788649871945381 + 10.0 * 6.224598407745361
Epoch 2030, val loss: 1.4497771263122559
Epoch 2040, training loss: 62.213199615478516 = 0.027416298165917397 + 10.0 * 6.218578338623047
Epoch 2040, val loss: 1.4528861045837402
Epoch 2050, training loss: 62.181644439697266 = 0.026960084214806557 + 10.0 * 6.215468406677246
Epoch 2050, val loss: 1.4568169116973877
Epoch 2060, training loss: 62.173973083496094 = 0.026530249044299126 + 10.0 * 6.2147440910339355
Epoch 2060, val loss: 1.4603484869003296
Epoch 2070, training loss: 62.20176315307617 = 0.026106512174010277 + 10.0 * 6.217565536499023
Epoch 2070, val loss: 1.4637967348098755
Epoch 2080, training loss: 62.165103912353516 = 0.025682756677269936 + 10.0 * 6.213942050933838
Epoch 2080, val loss: 1.4674639701843262
Epoch 2090, training loss: 62.17059326171875 = 0.025277946144342422 + 10.0 * 6.214531898498535
Epoch 2090, val loss: 1.4709293842315674
Epoch 2100, training loss: 62.22278594970703 = 0.024887414649128914 + 10.0 * 6.219789981842041
Epoch 2100, val loss: 1.4742605686187744
Epoch 2110, training loss: 62.176422119140625 = 0.02448042295873165 + 10.0 * 6.215194225311279
Epoch 2110, val loss: 1.4776109457015991
Epoch 2120, training loss: 62.15034103393555 = 0.024097222834825516 + 10.0 * 6.212624549865723
Epoch 2120, val loss: 1.4812681674957275
Epoch 2130, training loss: 62.147377014160156 = 0.023729357868433 + 10.0 * 6.212364673614502
Epoch 2130, val loss: 1.4844660758972168
Epoch 2140, training loss: 62.150146484375 = 0.02336967922747135 + 10.0 * 6.212677955627441
Epoch 2140, val loss: 1.4877873659133911
Epoch 2150, training loss: 62.20880126953125 = 0.02301768772304058 + 10.0 * 6.218578338623047
Epoch 2150, val loss: 1.4905482530593872
Epoch 2160, training loss: 62.17308807373047 = 0.02266828902065754 + 10.0 * 6.2150421142578125
Epoch 2160, val loss: 1.4947525262832642
Epoch 2170, training loss: 62.158939361572266 = 0.022320279851555824 + 10.0 * 6.213662147521973
Epoch 2170, val loss: 1.4974334239959717
Epoch 2180, training loss: 62.17825698852539 = 0.02199205383658409 + 10.0 * 6.2156267166137695
Epoch 2180, val loss: 1.500964641571045
Epoch 2190, training loss: 62.14104461669922 = 0.021658355370163918 + 10.0 * 6.211938381195068
Epoch 2190, val loss: 1.5041062831878662
Epoch 2200, training loss: 62.13689041137695 = 0.021342415362596512 + 10.0 * 6.211554527282715
Epoch 2200, val loss: 1.5072799921035767
Epoch 2210, training loss: 62.14997100830078 = 0.021033549681305885 + 10.0 * 6.212893486022949
Epoch 2210, val loss: 1.510437250137329
Epoch 2220, training loss: 62.17353439331055 = 0.02073107659816742 + 10.0 * 6.215280055999756
Epoch 2220, val loss: 1.5134303569793701
Epoch 2230, training loss: 62.15546417236328 = 0.020420923829078674 + 10.0 * 6.213504314422607
Epoch 2230, val loss: 1.5161935091018677
Epoch 2240, training loss: 62.14021682739258 = 0.02012495882809162 + 10.0 * 6.212008953094482
Epoch 2240, val loss: 1.519372582435608
Epoch 2250, training loss: 62.134552001953125 = 0.019838528707623482 + 10.0 * 6.211471080780029
Epoch 2250, val loss: 1.522389531135559
Epoch 2260, training loss: 62.13298797607422 = 0.019563356414437294 + 10.0 * 6.211342811584473
Epoch 2260, val loss: 1.5254279375076294
Epoch 2270, training loss: 62.137481689453125 = 0.01928877830505371 + 10.0 * 6.211819648742676
Epoch 2270, val loss: 1.5283387899398804
Epoch 2280, training loss: 62.154693603515625 = 0.019019825384020805 + 10.0 * 6.21356725692749
Epoch 2280, val loss: 1.5314428806304932
Epoch 2290, training loss: 62.13069152832031 = 0.0187542662024498 + 10.0 * 6.211194038391113
Epoch 2290, val loss: 1.5343575477600098
Epoch 2300, training loss: 62.12392044067383 = 0.01849781908094883 + 10.0 * 6.21054220199585
Epoch 2300, val loss: 1.5374712944030762
Epoch 2310, training loss: 62.15666961669922 = 0.01824636571109295 + 10.0 * 6.213842391967773
Epoch 2310, val loss: 1.540053367614746
Epoch 2320, training loss: 62.121559143066406 = 0.017989275977015495 + 10.0 * 6.210356712341309
Epoch 2320, val loss: 1.5428564548492432
Epoch 2330, training loss: 62.11438751220703 = 0.017746491357684135 + 10.0 * 6.209664344787598
Epoch 2330, val loss: 1.5458160638809204
Epoch 2340, training loss: 62.13953399658203 = 0.017510175704956055 + 10.0 * 6.212202548980713
Epoch 2340, val loss: 1.5485703945159912
Epoch 2350, training loss: 62.11286163330078 = 0.01727374643087387 + 10.0 * 6.209558963775635
Epoch 2350, val loss: 1.551439881324768
Epoch 2360, training loss: 62.123443603515625 = 0.01705138199031353 + 10.0 * 6.210638999938965
Epoch 2360, val loss: 1.554366946220398
Epoch 2370, training loss: 62.12314987182617 = 0.01682211458683014 + 10.0 * 6.210632801055908
Epoch 2370, val loss: 1.5569514036178589
Epoch 2380, training loss: 62.104095458984375 = 0.016599785536527634 + 10.0 * 6.208749294281006
Epoch 2380, val loss: 1.5593340396881104
Epoch 2390, training loss: 62.13045120239258 = 0.016387850046157837 + 10.0 * 6.211406230926514
Epoch 2390, val loss: 1.562379002571106
Epoch 2400, training loss: 62.12669372558594 = 0.01617494784295559 + 10.0 * 6.211051940917969
Epoch 2400, val loss: 1.564945936203003
Epoch 2410, training loss: 62.10347366333008 = 0.015959840267896652 + 10.0 * 6.208751201629639
Epoch 2410, val loss: 1.5672199726104736
Epoch 2420, training loss: 62.09978103637695 = 0.0157571192830801 + 10.0 * 6.208402156829834
Epoch 2420, val loss: 1.5700215101242065
Epoch 2430, training loss: 62.097984313964844 = 0.0155573645606637 + 10.0 * 6.208242893218994
Epoch 2430, val loss: 1.5724924802780151
Epoch 2440, training loss: 62.101837158203125 = 0.015363754704594612 + 10.0 * 6.20864725112915
Epoch 2440, val loss: 1.574977993965149
Epoch 2450, training loss: 62.11723327636719 = 0.01517493836581707 + 10.0 * 6.210206031799316
Epoch 2450, val loss: 1.5777556896209717
Epoch 2460, training loss: 62.10112762451172 = 0.014981969259679317 + 10.0 * 6.208614826202393
Epoch 2460, val loss: 1.580198049545288
Epoch 2470, training loss: 62.11864471435547 = 0.014797777868807316 + 10.0 * 6.210384845733643
Epoch 2470, val loss: 1.5825999975204468
Epoch 2480, training loss: 62.08705520629883 = 0.014611980877816677 + 10.0 * 6.207244396209717
Epoch 2480, val loss: 1.5850516557693481
Epoch 2490, training loss: 62.09248733520508 = 0.014435826800763607 + 10.0 * 6.207805156707764
Epoch 2490, val loss: 1.5873550176620483
Epoch 2500, training loss: 62.11589431762695 = 0.014260747469961643 + 10.0 * 6.210163593292236
Epoch 2500, val loss: 1.5898704528808594
Epoch 2510, training loss: 62.0923957824707 = 0.014086544513702393 + 10.0 * 6.207830905914307
Epoch 2510, val loss: 1.5921428203582764
Epoch 2520, training loss: 62.09555435180664 = 0.013918472453951836 + 10.0 * 6.208163261413574
Epoch 2520, val loss: 1.5945580005645752
Epoch 2530, training loss: 62.088783264160156 = 0.013754446990787983 + 10.0 * 6.207502841949463
Epoch 2530, val loss: 1.5972871780395508
Epoch 2540, training loss: 62.08345413208008 = 0.0135917067527771 + 10.0 * 6.206986427307129
Epoch 2540, val loss: 1.5996148586273193
Epoch 2550, training loss: 62.077754974365234 = 0.013430311344563961 + 10.0 * 6.206432342529297
Epoch 2550, val loss: 1.6016807556152344
Epoch 2560, training loss: 62.08194351196289 = 0.013275236822664738 + 10.0 * 6.20686674118042
Epoch 2560, val loss: 1.6036779880523682
Epoch 2570, training loss: 62.111778259277344 = 0.013124024495482445 + 10.0 * 6.209865570068359
Epoch 2570, val loss: 1.6058988571166992
Epoch 2580, training loss: 62.087547302246094 = 0.012966955080628395 + 10.0 * 6.207458019256592
Epoch 2580, val loss: 1.6077557802200317
Epoch 2590, training loss: 62.069698333740234 = 0.012815571390092373 + 10.0 * 6.205687999725342
Epoch 2590, val loss: 1.6100423336029053
Epoch 2600, training loss: 62.07521057128906 = 0.01267549954354763 + 10.0 * 6.206253528594971
Epoch 2600, val loss: 1.6124520301818848
Epoch 2610, training loss: 62.126060485839844 = 0.012530675157904625 + 10.0 * 6.211352825164795
Epoch 2610, val loss: 1.614119529724121
Epoch 2620, training loss: 62.08195877075195 = 0.012385196052491665 + 10.0 * 6.2069573402404785
Epoch 2620, val loss: 1.616350531578064
Epoch 2630, training loss: 62.06929397583008 = 0.012248996645212173 + 10.0 * 6.205704689025879
Epoch 2630, val loss: 1.6186227798461914
Epoch 2640, training loss: 62.08478927612305 = 0.0121134202927351 + 10.0 * 6.207267761230469
Epoch 2640, val loss: 1.620133876800537
Epoch 2650, training loss: 62.05873489379883 = 0.01197904534637928 + 10.0 * 6.204675674438477
Epoch 2650, val loss: 1.622388482093811
Epoch 2660, training loss: 62.06898498535156 = 0.01185131911188364 + 10.0 * 6.205713272094727
Epoch 2660, val loss: 1.6246609687805176
Epoch 2670, training loss: 62.0633659362793 = 0.011720763519406319 + 10.0 * 6.205164432525635
Epoch 2670, val loss: 1.6262723207473755
Epoch 2680, training loss: 62.09337615966797 = 0.01159724686294794 + 10.0 * 6.2081780433654785
Epoch 2680, val loss: 1.6282249689102173
Epoch 2690, training loss: 62.08351516723633 = 0.01147127989679575 + 10.0 * 6.207204341888428
Epoch 2690, val loss: 1.6309053897857666
Epoch 2700, training loss: 62.05125427246094 = 0.011345536448061466 + 10.0 * 6.203990936279297
Epoch 2700, val loss: 1.6321486234664917
Epoch 2710, training loss: 62.053035736083984 = 0.011227874085307121 + 10.0 * 6.204180717468262
Epoch 2710, val loss: 1.6343412399291992
Epoch 2720, training loss: 62.10103225708008 = 0.011114146560430527 + 10.0 * 6.208991527557373
Epoch 2720, val loss: 1.6361405849456787
Epoch 2730, training loss: 62.070194244384766 = 0.010988441295921803 + 10.0 * 6.205920219421387
Epoch 2730, val loss: 1.6373594999313354
Epoch 2740, training loss: 62.04712677001953 = 0.010874159634113312 + 10.0 * 6.203625202178955
Epoch 2740, val loss: 1.6394835710525513
Epoch 2750, training loss: 62.03995895385742 = 0.010762223042547703 + 10.0 * 6.202919960021973
Epoch 2750, val loss: 1.6416186094284058
Epoch 2760, training loss: 62.03850173950195 = 0.010654416866600513 + 10.0 * 6.202784538269043
Epoch 2760, val loss: 1.643524169921875
Epoch 2770, training loss: 62.1081657409668 = 0.010549151338636875 + 10.0 * 6.209761619567871
Epoch 2770, val loss: 1.6450672149658203
Epoch 2780, training loss: 62.0533561706543 = 0.01043800637125969 + 10.0 * 6.204291820526123
Epoch 2780, val loss: 1.6463282108306885
Epoch 2790, training loss: 62.06147766113281 = 0.010328470729291439 + 10.0 * 6.205114841461182
Epoch 2790, val loss: 1.6483477354049683
Epoch 2800, training loss: 62.0529899597168 = 0.010226601734757423 + 10.0 * 6.204276084899902
Epoch 2800, val loss: 1.6501728296279907
Epoch 2810, training loss: 62.04362487792969 = 0.01012492273002863 + 10.0 * 6.203350067138672
Epoch 2810, val loss: 1.652003526687622
Epoch 2820, training loss: 62.04765319824219 = 0.010025366209447384 + 10.0 * 6.203763008117676
Epoch 2820, val loss: 1.653657078742981
Epoch 2830, training loss: 62.05402755737305 = 0.009923935867846012 + 10.0 * 6.204410076141357
Epoch 2830, val loss: 1.6550241708755493
Epoch 2840, training loss: 62.033843994140625 = 0.009826396591961384 + 10.0 * 6.202401638031006
Epoch 2840, val loss: 1.6567589044570923
Epoch 2850, training loss: 62.039222717285156 = 0.009731465019285679 + 10.0 * 6.202949047088623
Epoch 2850, val loss: 1.6583750247955322
Epoch 2860, training loss: 62.08893585205078 = 0.009639434516429901 + 10.0 * 6.207929611206055
Epoch 2860, val loss: 1.6596851348876953
Epoch 2870, training loss: 62.04795455932617 = 0.009541508741676807 + 10.0 * 6.203841209411621
Epoch 2870, val loss: 1.6615017652511597
Epoch 2880, training loss: 62.0324592590332 = 0.009449388831853867 + 10.0 * 6.202301025390625
Epoch 2880, val loss: 1.6631159782409668
Epoch 2890, training loss: 62.028343200683594 = 0.009359580464661121 + 10.0 * 6.201898097991943
Epoch 2890, val loss: 1.6646201610565186
Epoch 2900, training loss: 62.05789566040039 = 0.009272059425711632 + 10.0 * 6.204862117767334
Epoch 2900, val loss: 1.665991187095642
Epoch 2910, training loss: 62.018611907958984 = 0.009183945134282112 + 10.0 * 6.200942516326904
Epoch 2910, val loss: 1.667677640914917
Epoch 2920, training loss: 62.03384017944336 = 0.009098791517317295 + 10.0 * 6.202474117279053
Epoch 2920, val loss: 1.669113278388977
Epoch 2930, training loss: 62.053890228271484 = 0.009013727307319641 + 10.0 * 6.2044878005981445
Epoch 2930, val loss: 1.6702324151992798
Epoch 2940, training loss: 62.02690887451172 = 0.008930012583732605 + 10.0 * 6.201797962188721
Epoch 2940, val loss: 1.6722402572631836
Epoch 2950, training loss: 62.05656814575195 = 0.008846919052302837 + 10.0 * 6.204771995544434
Epoch 2950, val loss: 1.6732233762741089
Epoch 2960, training loss: 62.03330612182617 = 0.008764585480093956 + 10.0 * 6.202454090118408
Epoch 2960, val loss: 1.6745882034301758
Epoch 2970, training loss: 62.01286697387695 = 0.008684203028678894 + 10.0 * 6.200417995452881
Epoch 2970, val loss: 1.6766587495803833
Epoch 2980, training loss: 62.012237548828125 = 0.008607365190982819 + 10.0 * 6.2003631591796875
Epoch 2980, val loss: 1.6780177354812622
Epoch 2990, training loss: 62.0640983581543 = 0.008533268235623837 + 10.0 * 6.205556392669678
Epoch 2990, val loss: 1.679544448852539
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8118081180811808
The final CL Acc:0.70370, 0.00800, The final GNN Acc:0.80988, 0.00808
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 8030])
updated graph: torch.Size([2, 10626])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.90921020507812 = 1.9411441087722778 + 10.0 * 8.596806526184082
Epoch 0, val loss: 1.9492465257644653
Epoch 10, training loss: 87.8922119140625 = 1.9310613870620728 + 10.0 * 8.596115112304688
Epoch 10, val loss: 1.9387229681015015
Epoch 20, training loss: 87.8323974609375 = 1.9189784526824951 + 10.0 * 8.591341972351074
Epoch 20, val loss: 1.9258016347885132
Epoch 30, training loss: 87.53213500976562 = 1.9033983945846558 + 10.0 * 8.562873840332031
Epoch 30, val loss: 1.909077763557434
Epoch 40, training loss: 86.1288070678711 = 1.8850772380828857 + 10.0 * 8.424372673034668
Epoch 40, val loss: 1.8902314901351929
Epoch 50, training loss: 82.23580169677734 = 1.8635613918304443 + 10.0 * 8.037223815917969
Epoch 50, val loss: 1.8679049015045166
Epoch 60, training loss: 78.7318344116211 = 1.8446203470230103 + 10.0 * 7.688721656799316
Epoch 60, val loss: 1.8496718406677246
Epoch 70, training loss: 74.39259338378906 = 1.832916259765625 + 10.0 * 7.255967617034912
Epoch 70, val loss: 1.8381496667861938
Epoch 80, training loss: 72.35194396972656 = 1.823290467262268 + 10.0 * 7.052865982055664
Epoch 80, val loss: 1.8276207447052002
Epoch 90, training loss: 71.33229064941406 = 1.8103214502334595 + 10.0 * 6.952197551727295
Epoch 90, val loss: 1.8146820068359375
Epoch 100, training loss: 70.42686462402344 = 1.7983945608139038 + 10.0 * 6.862846851348877
Epoch 100, val loss: 1.8032338619232178
Epoch 110, training loss: 69.5578842163086 = 1.7889306545257568 + 10.0 * 6.776895046234131
Epoch 110, val loss: 1.7938257455825806
Epoch 120, training loss: 68.89462280273438 = 1.7801679372787476 + 10.0 * 6.7114458084106445
Epoch 120, val loss: 1.7847353219985962
Epoch 130, training loss: 68.3626480102539 = 1.7713701725006104 + 10.0 * 6.659127712249756
Epoch 130, val loss: 1.7756009101867676
Epoch 140, training loss: 67.97702026367188 = 1.7622219324111938 + 10.0 * 6.6214799880981445
Epoch 140, val loss: 1.7662168741226196
Epoch 150, training loss: 67.68267059326172 = 1.752225637435913 + 10.0 * 6.593044281005859
Epoch 150, val loss: 1.7561973333358765
Epoch 160, training loss: 67.4486083984375 = 1.7413898706436157 + 10.0 * 6.5707221031188965
Epoch 160, val loss: 1.7456700801849365
Epoch 170, training loss: 67.24288940429688 = 1.7298791408538818 + 10.0 * 6.551300525665283
Epoch 170, val loss: 1.7347230911254883
Epoch 180, training loss: 67.05486297607422 = 1.717621088027954 + 10.0 * 6.533724308013916
Epoch 180, val loss: 1.723249912261963
Epoch 190, training loss: 66.8629150390625 = 1.7046836614608765 + 10.0 * 6.5158233642578125
Epoch 190, val loss: 1.7112904787063599
Epoch 200, training loss: 66.68595886230469 = 1.6908674240112305 + 10.0 * 6.499509334564209
Epoch 200, val loss: 1.6987594366073608
Epoch 210, training loss: 66.52599334716797 = 1.6759693622589111 + 10.0 * 6.485002517700195
Epoch 210, val loss: 1.6854264736175537
Epoch 220, training loss: 66.36792755126953 = 1.659852147102356 + 10.0 * 6.4708075523376465
Epoch 220, val loss: 1.671122431755066
Epoch 230, training loss: 66.24190521240234 = 1.6422674655914307 + 10.0 * 6.459963798522949
Epoch 230, val loss: 1.6555671691894531
Epoch 240, training loss: 66.10347747802734 = 1.6234208345413208 + 10.0 * 6.448005676269531
Epoch 240, val loss: 1.6388776302337646
Epoch 250, training loss: 65.97053527832031 = 1.6032832860946655 + 10.0 * 6.43672513961792
Epoch 250, val loss: 1.6211031675338745
Epoch 260, training loss: 65.85371398925781 = 1.581754207611084 + 10.0 * 6.427196025848389
Epoch 260, val loss: 1.6021963357925415
Epoch 270, training loss: 65.73571014404297 = 1.5589253902435303 + 10.0 * 6.4176788330078125
Epoch 270, val loss: 1.5822144746780396
Epoch 280, training loss: 65.6331787109375 = 1.534834384918213 + 10.0 * 6.409834384918213
Epoch 280, val loss: 1.5612138509750366
Epoch 290, training loss: 65.54747009277344 = 1.5093194246292114 + 10.0 * 6.403814792633057
Epoch 290, val loss: 1.5389472246170044
Epoch 300, training loss: 65.43965911865234 = 1.482747197151184 + 10.0 * 6.39569091796875
Epoch 300, val loss: 1.515974760055542
Epoch 310, training loss: 65.3432388305664 = 1.4551634788513184 + 10.0 * 6.38880729675293
Epoch 310, val loss: 1.4922229051589966
Epoch 320, training loss: 65.29457092285156 = 1.4266213178634644 + 10.0 * 6.3867950439453125
Epoch 320, val loss: 1.4677315950393677
Epoch 330, training loss: 65.17729949951172 = 1.3971543312072754 + 10.0 * 6.37801456451416
Epoch 330, val loss: 1.4427975416183472
Epoch 340, training loss: 65.09700775146484 = 1.3672198057174683 + 10.0 * 6.372979164123535
Epoch 340, val loss: 1.417622447013855
Epoch 350, training loss: 65.0284423828125 = 1.3368098735809326 + 10.0 * 6.3691630363464355
Epoch 350, val loss: 1.3922665119171143
Epoch 360, training loss: 64.94511413574219 = 1.3060141801834106 + 10.0 * 6.36391019821167
Epoch 360, val loss: 1.3668954372406006
Epoch 370, training loss: 64.89456939697266 = 1.2751661539077759 + 10.0 * 6.361940383911133
Epoch 370, val loss: 1.3415429592132568
Epoch 380, training loss: 64.8012466430664 = 1.2441524267196655 + 10.0 * 6.355709552764893
Epoch 380, val loss: 1.3163981437683105
Epoch 390, training loss: 64.7315902709961 = 1.2133737802505493 + 10.0 * 6.351821422576904
Epoch 390, val loss: 1.2917033433914185
Epoch 400, training loss: 64.6708755493164 = 1.1826987266540527 + 10.0 * 6.348817825317383
Epoch 400, val loss: 1.2671303749084473
Epoch 410, training loss: 64.59276580810547 = 1.1523311138153076 + 10.0 * 6.344043254852295
Epoch 410, val loss: 1.243276834487915
Epoch 420, training loss: 64.59639739990234 = 1.1222397089004517 + 10.0 * 6.347415447235107
Epoch 420, val loss: 1.2198059558868408
Epoch 430, training loss: 64.4749526977539 = 1.092722773551941 + 10.0 * 6.338222980499268
Epoch 430, val loss: 1.1970725059509277
Epoch 440, training loss: 64.41121673583984 = 1.0638545751571655 + 10.0 * 6.334735870361328
Epoch 440, val loss: 1.1750860214233398
Epoch 450, training loss: 64.34683990478516 = 1.0356389284133911 + 10.0 * 6.331120014190674
Epoch 450, val loss: 1.1538541316986084
Epoch 460, training loss: 64.31715393066406 = 1.0080111026763916 + 10.0 * 6.330914497375488
Epoch 460, val loss: 1.1333215236663818
Epoch 470, training loss: 64.24546813964844 = 0.9810954332351685 + 10.0 * 6.326436996459961
Epoch 470, val loss: 1.1134828329086304
Epoch 480, training loss: 64.19135284423828 = 0.9549610018730164 + 10.0 * 6.323639392852783
Epoch 480, val loss: 1.0944418907165527
Epoch 490, training loss: 64.13289642333984 = 0.9294711947441101 + 10.0 * 6.320342063903809
Epoch 490, val loss: 1.0761666297912598
Epoch 500, training loss: 64.09058380126953 = 0.9047255516052246 + 10.0 * 6.3185858726501465
Epoch 500, val loss: 1.0586274862289429
Epoch 510, training loss: 64.0475082397461 = 0.8806279301643372 + 10.0 * 6.31668758392334
Epoch 510, val loss: 1.0418617725372314
Epoch 520, training loss: 64.01558685302734 = 0.8572430610656738 + 10.0 * 6.315834045410156
Epoch 520, val loss: 1.0257078409194946
Epoch 530, training loss: 63.95191192626953 = 0.8343822360038757 + 10.0 * 6.311753273010254
Epoch 530, val loss: 1.0102753639221191
Epoch 540, training loss: 63.90375518798828 = 0.8121581077575684 + 10.0 * 6.309159755706787
Epoch 540, val loss: 0.9952117800712585
Epoch 550, training loss: 63.87861251831055 = 0.7904940247535706 + 10.0 * 6.308812141418457
Epoch 550, val loss: 0.9809169173240662
Epoch 560, training loss: 63.84392166137695 = 0.7690642476081848 + 10.0 * 6.307485580444336
Epoch 560, val loss: 0.9667300581932068
Epoch 570, training loss: 63.777408599853516 = 0.7483049035072327 + 10.0 * 6.302910327911377
Epoch 570, val loss: 0.9532252550125122
Epoch 580, training loss: 63.73886489868164 = 0.7280541658401489 + 10.0 * 6.30108118057251
Epoch 580, val loss: 0.9404653906822205
Epoch 590, training loss: 63.69902420043945 = 0.7081891298294067 + 10.0 * 6.299083232879639
Epoch 590, val loss: 0.9280996322631836
Epoch 600, training loss: 63.666664123535156 = 0.6886254549026489 + 10.0 * 6.29780387878418
Epoch 600, val loss: 0.9160768985748291
Epoch 610, training loss: 63.666683197021484 = 0.6693164706230164 + 10.0 * 6.299736976623535
Epoch 610, val loss: 0.9042736291885376
Epoch 620, training loss: 63.6014404296875 = 0.6503732800483704 + 10.0 * 6.295106887817383
Epoch 620, val loss: 0.8930563926696777
Epoch 630, training loss: 63.565101623535156 = 0.6319511532783508 + 10.0 * 6.2933149337768555
Epoch 630, val loss: 0.882326602935791
Epoch 640, training loss: 63.5259895324707 = 0.6139175891876221 + 10.0 * 6.291207313537598
Epoch 640, val loss: 0.8722010254859924
Epoch 650, training loss: 63.49054718017578 = 0.5961885452270508 + 10.0 * 6.289435863494873
Epoch 650, val loss: 0.8624850511550903
Epoch 660, training loss: 63.49983215332031 = 0.5787598490715027 + 10.0 * 6.292107582092285
Epoch 660, val loss: 0.853085994720459
Epoch 670, training loss: 63.4595832824707 = 0.5613992214202881 + 10.0 * 6.28981876373291
Epoch 670, val loss: 0.8440890312194824
Epoch 680, training loss: 63.405433654785156 = 0.5445656776428223 + 10.0 * 6.286086559295654
Epoch 680, val loss: 0.8357378244400024
Epoch 690, training loss: 63.368690490722656 = 0.5281280279159546 + 10.0 * 6.284056186676025
Epoch 690, val loss: 0.8279228210449219
Epoch 700, training loss: 63.33842468261719 = 0.512030839920044 + 10.0 * 6.282639503479004
Epoch 700, val loss: 0.8206806778907776
Epoch 710, training loss: 63.4431037902832 = 0.4962463974952698 + 10.0 * 6.2946858406066895
Epoch 710, val loss: 0.8137648701667786
Epoch 720, training loss: 63.287071228027344 = 0.4806608259677887 + 10.0 * 6.280641078948975
Epoch 720, val loss: 0.8071812391281128
Epoch 730, training loss: 63.26106262207031 = 0.46558380126953125 + 10.0 * 6.279547691345215
Epoch 730, val loss: 0.8013488054275513
Epoch 740, training loss: 63.2318115234375 = 0.45095589756965637 + 10.0 * 6.278085708618164
Epoch 740, val loss: 0.796067476272583
Epoch 750, training loss: 63.20524215698242 = 0.43670451641082764 + 10.0 * 6.276854038238525
Epoch 750, val loss: 0.7912960648536682
Epoch 760, training loss: 63.26173782348633 = 0.4227672815322876 + 10.0 * 6.2838969230651855
Epoch 760, val loss: 0.7870324850082397
Epoch 770, training loss: 63.1636848449707 = 0.40921521186828613 + 10.0 * 6.275446891784668
Epoch 770, val loss: 0.7829836010932922
Epoch 780, training loss: 63.13188934326172 = 0.3961205780506134 + 10.0 * 6.273576736450195
Epoch 780, val loss: 0.7795947194099426
Epoch 790, training loss: 63.109230041503906 = 0.38345396518707275 + 10.0 * 6.27257776260376
Epoch 790, val loss: 0.7767826914787292
Epoch 800, training loss: 63.14093017578125 = 0.3711526393890381 + 10.0 * 6.276978015899658
Epoch 800, val loss: 0.7743691205978394
Epoch 810, training loss: 63.108482360839844 = 0.3590846657752991 + 10.0 * 6.27493953704834
Epoch 810, val loss: 0.7721281051635742
Epoch 820, training loss: 63.05128479003906 = 0.3474947214126587 + 10.0 * 6.270379066467285
Epoch 820, val loss: 0.7705016136169434
Epoch 830, training loss: 63.0225830078125 = 0.3363308310508728 + 10.0 * 6.268625259399414
Epoch 830, val loss: 0.7693567276000977
Epoch 840, training loss: 63.00288009643555 = 0.32553431391716003 + 10.0 * 6.267734527587891
Epoch 840, val loss: 0.7685455083847046
Epoch 850, training loss: 63.06745529174805 = 0.3150373697280884 + 10.0 * 6.275241851806641
Epoch 850, val loss: 0.7680527567863464
Epoch 860, training loss: 62.984832763671875 = 0.30482450127601624 + 10.0 * 6.268000602722168
Epoch 860, val loss: 0.7674578428268433
Epoch 870, training loss: 62.95278549194336 = 0.2950514256954193 + 10.0 * 6.265773296356201
Epoch 870, val loss: 0.7675053477287292
Epoch 880, training loss: 62.92988586425781 = 0.2856932282447815 + 10.0 * 6.264419078826904
Epoch 880, val loss: 0.7679082751274109
Epoch 890, training loss: 62.908634185791016 = 0.27666375041007996 + 10.0 * 6.26319694519043
Epoch 890, val loss: 0.7686628699302673
Epoch 900, training loss: 62.89223861694336 = 0.26791512966156006 + 10.0 * 6.26243257522583
Epoch 900, val loss: 0.7696253061294556
Epoch 910, training loss: 62.90822982788086 = 0.25943684577941895 + 10.0 * 6.26487922668457
Epoch 910, val loss: 0.7708142995834351
Epoch 920, training loss: 62.86018753051758 = 0.2512228190898895 + 10.0 * 6.2608962059021
Epoch 920, val loss: 0.7719219923019409
Epoch 930, training loss: 62.84256362915039 = 0.24330291152000427 + 10.0 * 6.2599263191223145
Epoch 930, val loss: 0.7735579609870911
Epoch 940, training loss: 62.83269119262695 = 0.2356983870267868 + 10.0 * 6.25969934463501
Epoch 940, val loss: 0.7752566337585449
Epoch 950, training loss: 62.839805603027344 = 0.22835606336593628 + 10.0 * 6.261145114898682
Epoch 950, val loss: 0.7772127985954285
Epoch 960, training loss: 62.81110763549805 = 0.2212286740541458 + 10.0 * 6.258987903594971
Epoch 960, val loss: 0.7792565226554871
Epoch 970, training loss: 62.79572677612305 = 0.21440254151821136 + 10.0 * 6.258132457733154
Epoch 970, val loss: 0.7815942168235779
Epoch 980, training loss: 62.778587341308594 = 0.2078334540128708 + 10.0 * 6.257075309753418
Epoch 980, val loss: 0.7839075922966003
Epoch 990, training loss: 62.76954650878906 = 0.20148466527462006 + 10.0 * 6.256806373596191
Epoch 990, val loss: 0.7865626811981201
Epoch 1000, training loss: 62.758636474609375 = 0.19538404047489166 + 10.0 * 6.2563252449035645
Epoch 1000, val loss: 0.789148211479187
Epoch 1010, training loss: 62.81520080566406 = 0.18948765099048615 + 10.0 * 6.262571334838867
Epoch 1010, val loss: 0.7918766140937805
Epoch 1020, training loss: 62.755821228027344 = 0.18376773595809937 + 10.0 * 6.257205009460449
Epoch 1020, val loss: 0.7946034073829651
Epoch 1030, training loss: 62.70820999145508 = 0.1783008873462677 + 10.0 * 6.25299072265625
Epoch 1030, val loss: 0.7976123094558716
Epoch 1040, training loss: 62.6967887878418 = 0.17306075990200043 + 10.0 * 6.252372741699219
Epoch 1040, val loss: 0.8008903861045837
Epoch 1050, training loss: 62.712467193603516 = 0.1680118292570114 + 10.0 * 6.254445552825928
Epoch 1050, val loss: 0.8042365908622742
Epoch 1060, training loss: 62.69231414794922 = 0.1630915105342865 + 10.0 * 6.252922534942627
Epoch 1060, val loss: 0.8071871399879456
Epoch 1070, training loss: 62.670982360839844 = 0.15833452343940735 + 10.0 * 6.251265048980713
Epoch 1070, val loss: 0.8107765913009644
Epoch 1080, training loss: 62.65011215209961 = 0.15379498898983002 + 10.0 * 6.249631881713867
Epoch 1080, val loss: 0.8143864274024963
Epoch 1090, training loss: 62.65312576293945 = 0.14940690994262695 + 10.0 * 6.250371932983398
Epoch 1090, val loss: 0.8180615901947021
Epoch 1100, training loss: 62.65848159790039 = 0.1451377123594284 + 10.0 * 6.251334190368652
Epoch 1100, val loss: 0.8216514587402344
Epoch 1110, training loss: 62.63134765625 = 0.14099867641925812 + 10.0 * 6.249034881591797
Epoch 1110, val loss: 0.8253812193870544
Epoch 1120, training loss: 62.621551513671875 = 0.13704229891300201 + 10.0 * 6.248450756072998
Epoch 1120, val loss: 0.8291313052177429
Epoch 1130, training loss: 62.60077667236328 = 0.13321909308433533 + 10.0 * 6.246755599975586
Epoch 1130, val loss: 0.8331666588783264
Epoch 1140, training loss: 62.59587860107422 = 0.12953244149684906 + 10.0 * 6.246634483337402
Epoch 1140, val loss: 0.8372285962104797
Epoch 1150, training loss: 62.63072204589844 = 0.1259503811597824 + 10.0 * 6.250477313995361
Epoch 1150, val loss: 0.8412299752235413
Epoch 1160, training loss: 62.590023040771484 = 0.1224568709731102 + 10.0 * 6.246756553649902
Epoch 1160, val loss: 0.8452629446983337
Epoch 1170, training loss: 62.5636100769043 = 0.11909187585115433 + 10.0 * 6.244451999664307
Epoch 1170, val loss: 0.8494630455970764
Epoch 1180, training loss: 62.55973815917969 = 0.11585231870412827 + 10.0 * 6.244388580322266
Epoch 1180, val loss: 0.8537737131118774
Epoch 1190, training loss: 62.59370803833008 = 0.11272141337394714 + 10.0 * 6.248098850250244
Epoch 1190, val loss: 0.8580572605133057
Epoch 1200, training loss: 62.55925369262695 = 0.109645776450634 + 10.0 * 6.244960784912109
Epoch 1200, val loss: 0.8623787760734558
Epoch 1210, training loss: 62.55255889892578 = 0.10670355707406998 + 10.0 * 6.2445855140686035
Epoch 1210, val loss: 0.8667680621147156
Epoch 1220, training loss: 62.52314376831055 = 0.10385330766439438 + 10.0 * 6.241929054260254
Epoch 1220, val loss: 0.8712520599365234
Epoch 1230, training loss: 62.52546310424805 = 0.10110795497894287 + 10.0 * 6.242435455322266
Epoch 1230, val loss: 0.8757547736167908
Epoch 1240, training loss: 62.584049224853516 = 0.09846024215221405 + 10.0 * 6.24855899810791
Epoch 1240, val loss: 0.8803035020828247
Epoch 1250, training loss: 62.51857376098633 = 0.09584035724401474 + 10.0 * 6.242273330688477
Epoch 1250, val loss: 0.8846403360366821
Epoch 1260, training loss: 62.4946174621582 = 0.09335091710090637 + 10.0 * 6.240126609802246
Epoch 1260, val loss: 0.8893717527389526
Epoch 1270, training loss: 62.483760833740234 = 0.09094783663749695 + 10.0 * 6.23928165435791
Epoch 1270, val loss: 0.8940167427062988
Epoch 1280, training loss: 62.485565185546875 = 0.08861996233463287 + 10.0 * 6.239694595336914
Epoch 1280, val loss: 0.8986620903015137
Epoch 1290, training loss: 62.51344680786133 = 0.08633851259946823 + 10.0 * 6.242711067199707
Epoch 1290, val loss: 0.9033349752426147
Epoch 1300, training loss: 62.48781967163086 = 0.08410727977752686 + 10.0 * 6.240371227264404
Epoch 1300, val loss: 0.907921314239502
Epoch 1310, training loss: 62.457725524902344 = 0.08197580277919769 + 10.0 * 6.237575054168701
Epoch 1310, val loss: 0.9127576351165771
Epoch 1320, training loss: 62.45185089111328 = 0.07992571592330933 + 10.0 * 6.237192630767822
Epoch 1320, val loss: 0.9176997542381287
Epoch 1330, training loss: 62.47430419921875 = 0.07794761657714844 + 10.0 * 6.239635467529297
Epoch 1330, val loss: 0.922586977481842
Epoch 1340, training loss: 62.44273376464844 = 0.07598576694726944 + 10.0 * 6.236674785614014
Epoch 1340, val loss: 0.9273545742034912
Epoch 1350, training loss: 62.44113540649414 = 0.07409609109163284 + 10.0 * 6.236703872680664
Epoch 1350, val loss: 0.932318389415741
Epoch 1360, training loss: 62.47040939331055 = 0.07228073477745056 + 10.0 * 6.239812850952148
Epoch 1360, val loss: 0.9372294545173645
Epoch 1370, training loss: 62.41693878173828 = 0.07050025463104248 + 10.0 * 6.234643936157227
Epoch 1370, val loss: 0.9421194195747375
Epoch 1380, training loss: 62.411346435546875 = 0.0687798485159874 + 10.0 * 6.234256744384766
Epoch 1380, val loss: 0.9471096396446228
Epoch 1390, training loss: 62.40874481201172 = 0.06712176650762558 + 10.0 * 6.234162330627441
Epoch 1390, val loss: 0.9521853923797607
Epoch 1400, training loss: 62.404510498046875 = 0.06550950556993484 + 10.0 * 6.23390007019043
Epoch 1400, val loss: 0.9573062658309937
Epoch 1410, training loss: 62.43803787231445 = 0.06393732130527496 + 10.0 * 6.237410068511963
Epoch 1410, val loss: 0.9621255397796631
Epoch 1420, training loss: 62.41063690185547 = 0.062395013868808746 + 10.0 * 6.234824180603027
Epoch 1420, val loss: 0.9670865535736084
Epoch 1430, training loss: 62.39278030395508 = 0.06090879067778587 + 10.0 * 6.233187198638916
Epoch 1430, val loss: 0.972071647644043
Epoch 1440, training loss: 62.386146545410156 = 0.05948057025671005 + 10.0 * 6.232666969299316
Epoch 1440, val loss: 0.9772225618362427
Epoch 1450, training loss: 62.42082214355469 = 0.05809759348630905 + 10.0 * 6.23627233505249
Epoch 1450, val loss: 0.9821977615356445
Epoch 1460, training loss: 62.38124465942383 = 0.056724663823843 + 10.0 * 6.232451915740967
Epoch 1460, val loss: 0.9870964884757996
Epoch 1470, training loss: 62.365360260009766 = 0.05540835112333298 + 10.0 * 6.230995178222656
Epoch 1470, val loss: 0.9922383427619934
Epoch 1480, training loss: 62.35675048828125 = 0.054140355437994 + 10.0 * 6.230260848999023
Epoch 1480, val loss: 0.997428834438324
Epoch 1490, training loss: 62.37205505371094 = 0.05290837213397026 + 10.0 * 6.231914520263672
Epoch 1490, val loss: 1.0026236772537231
Epoch 1500, training loss: 62.37825012207031 = 0.051697634160518646 + 10.0 * 6.2326555252075195
Epoch 1500, val loss: 1.0074875354766846
Epoch 1510, training loss: 62.358428955078125 = 0.05051760748028755 + 10.0 * 6.230791091918945
Epoch 1510, val loss: 1.0124348402023315
Epoch 1520, training loss: 62.3542366027832 = 0.049382489174604416 + 10.0 * 6.230485439300537
Epoch 1520, val loss: 1.017383098602295
Epoch 1530, training loss: 62.35535430908203 = 0.04828459769487381 + 10.0 * 6.230706691741943
Epoch 1530, val loss: 1.0225257873535156
Epoch 1540, training loss: 62.337799072265625 = 0.047211531549692154 + 10.0 * 6.229058742523193
Epoch 1540, val loss: 1.0275990962982178
Epoch 1550, training loss: 62.36845397949219 = 0.046174682676792145 + 10.0 * 6.2322282791137695
Epoch 1550, val loss: 1.0326286554336548
Epoch 1560, training loss: 62.333740234375 = 0.04515908658504486 + 10.0 * 6.22885799407959
Epoch 1560, val loss: 1.0374033451080322
Epoch 1570, training loss: 62.3154296875 = 0.04417841508984566 + 10.0 * 6.22712516784668
Epoch 1570, val loss: 1.042564868927002
Epoch 1580, training loss: 62.326717376708984 = 0.043233755975961685 + 10.0 * 6.228348731994629
Epoch 1580, val loss: 1.0475623607635498
Epoch 1590, training loss: 62.31710433959961 = 0.042303942143917084 + 10.0 * 6.227479934692383
Epoch 1590, val loss: 1.052577257156372
Epoch 1600, training loss: 62.33087158203125 = 0.041403595358133316 + 10.0 * 6.228946685791016
Epoch 1600, val loss: 1.0574036836624146
Epoch 1610, training loss: 62.322330474853516 = 0.04052341356873512 + 10.0 * 6.228180885314941
Epoch 1610, val loss: 1.0622661113739014
Epoch 1620, training loss: 62.30193328857422 = 0.03966854512691498 + 10.0 * 6.226226329803467
Epoch 1620, val loss: 1.0672436952590942
Epoch 1630, training loss: 62.29212951660156 = 0.038843363523483276 + 10.0 * 6.22532844543457
Epoch 1630, val loss: 1.0723693370819092
Epoch 1640, training loss: 62.328121185302734 = 0.03805232048034668 + 10.0 * 6.229006767272949
Epoch 1640, val loss: 1.0772149562835693
Epoch 1650, training loss: 62.29187774658203 = 0.03725574538111687 + 10.0 * 6.225462436676025
Epoch 1650, val loss: 1.0820612907409668
Epoch 1660, training loss: 62.27821350097656 = 0.036493271589279175 + 10.0 * 6.224172115325928
Epoch 1660, val loss: 1.0871398448944092
Epoch 1670, training loss: 62.290523529052734 = 0.03576196730136871 + 10.0 * 6.225476264953613
Epoch 1670, val loss: 1.0919511318206787
Epoch 1680, training loss: 62.31496047973633 = 0.03503793105483055 + 10.0 * 6.227992057800293
Epoch 1680, val loss: 1.096738338470459
Epoch 1690, training loss: 62.28388595581055 = 0.034327175468206406 + 10.0 * 6.2249555587768555
Epoch 1690, val loss: 1.1017459630966187
Epoch 1700, training loss: 62.266597747802734 = 0.03364584967494011 + 10.0 * 6.223295211791992
Epoch 1700, val loss: 1.1064529418945312
Epoch 1710, training loss: 62.263004302978516 = 0.032988931983709335 + 10.0 * 6.223001480102539
Epoch 1710, val loss: 1.1114953756332397
Epoch 1720, training loss: 62.279998779296875 = 0.032350338995456696 + 10.0 * 6.224764823913574
Epoch 1720, val loss: 1.116223692893982
Epoch 1730, training loss: 62.26860046386719 = 0.031721487641334534 + 10.0 * 6.223687648773193
Epoch 1730, val loss: 1.1209837198257446
Epoch 1740, training loss: 62.25872802734375 = 0.031106021255254745 + 10.0 * 6.222762107849121
Epoch 1740, val loss: 1.1256568431854248
Epoch 1750, training loss: 62.264041900634766 = 0.03050718456506729 + 10.0 * 6.223353385925293
Epoch 1750, val loss: 1.1306935548782349
Epoch 1760, training loss: 62.25227355957031 = 0.029930928722023964 + 10.0 * 6.22223424911499
Epoch 1760, val loss: 1.1351984739303589
Epoch 1770, training loss: 62.245296478271484 = 0.02936803176999092 + 10.0 * 6.221592903137207
Epoch 1770, val loss: 1.140085220336914
Epoch 1780, training loss: 62.249107360839844 = 0.028821920976042747 + 10.0 * 6.222028732299805
Epoch 1780, val loss: 1.1447291374206543
Epoch 1790, training loss: 62.27571487426758 = 0.028283191844820976 + 10.0 * 6.224743366241455
Epoch 1790, val loss: 1.1494783163070679
Epoch 1800, training loss: 62.24289321899414 = 0.027754003182053566 + 10.0 * 6.221513748168945
Epoch 1800, val loss: 1.1538301706314087
Epoch 1810, training loss: 62.230384826660156 = 0.027243319898843765 + 10.0 * 6.220314025878906
Epoch 1810, val loss: 1.158522605895996
Epoch 1820, training loss: 62.22019577026367 = 0.026753125712275505 + 10.0 * 6.219344139099121
Epoch 1820, val loss: 1.1633445024490356
Epoch 1830, training loss: 62.227996826171875 = 0.026276282966136932 + 10.0 * 6.220171928405762
Epoch 1830, val loss: 1.1678494215011597
Epoch 1840, training loss: 62.25636672973633 = 0.025807827711105347 + 10.0 * 6.223055839538574
Epoch 1840, val loss: 1.1724351644515991
Epoch 1850, training loss: 62.23363494873047 = 0.025338884443044662 + 10.0 * 6.220829963684082
Epoch 1850, val loss: 1.1766493320465088
Epoch 1860, training loss: 62.211700439453125 = 0.024886835366487503 + 10.0 * 6.218681335449219
Epoch 1860, val loss: 1.1810752153396606
Epoch 1870, training loss: 62.20461654663086 = 0.024457251653075218 + 10.0 * 6.218016147613525
Epoch 1870, val loss: 1.185775637626648
Epoch 1880, training loss: 62.23193359375 = 0.024038324132561684 + 10.0 * 6.220789432525635
Epoch 1880, val loss: 1.1900160312652588
Epoch 1890, training loss: 62.20213317871094 = 0.023618804290890694 + 10.0 * 6.217851161956787
Epoch 1890, val loss: 1.194459080696106
Epoch 1900, training loss: 62.19514846801758 = 0.023213058710098267 + 10.0 * 6.217193603515625
Epoch 1900, val loss: 1.1986749172210693
Epoch 1910, training loss: 62.20724868774414 = 0.022823473438620567 + 10.0 * 6.218442440032959
Epoch 1910, val loss: 1.2030445337295532
Epoch 1920, training loss: 62.198341369628906 = 0.02243834361433983 + 10.0 * 6.21759033203125
Epoch 1920, val loss: 1.2072529792785645
Epoch 1930, training loss: 62.20140075683594 = 0.022062860429286957 + 10.0 * 6.217933654785156
Epoch 1930, val loss: 1.2117589712142944
Epoch 1940, training loss: 62.22843551635742 = 0.021695371717214584 + 10.0 * 6.22067403793335
Epoch 1940, val loss: 1.2159994840621948
Epoch 1950, training loss: 62.18819808959961 = 0.021329045295715332 + 10.0 * 6.216687202453613
Epoch 1950, val loss: 1.219929575920105
Epoch 1960, training loss: 62.1750373840332 = 0.02098325453698635 + 10.0 * 6.215405464172363
Epoch 1960, val loss: 1.2243257761001587
Epoch 1970, training loss: 62.17280197143555 = 0.020646749064326286 + 10.0 * 6.215215682983398
Epoch 1970, val loss: 1.2285791635513306
Epoch 1980, training loss: 62.197486877441406 = 0.02031790092587471 + 10.0 * 6.217717170715332
Epoch 1980, val loss: 1.2329471111297607
Epoch 1990, training loss: 62.19658279418945 = 0.01998848095536232 + 10.0 * 6.2176594734191895
Epoch 1990, val loss: 1.2366911172866821
Epoch 2000, training loss: 62.20535659790039 = 0.019666850566864014 + 10.0 * 6.218568801879883
Epoch 2000, val loss: 1.2408535480499268
Epoch 2010, training loss: 62.16879653930664 = 0.019352002069354057 + 10.0 * 6.214944362640381
Epoch 2010, val loss: 1.2445855140686035
Epoch 2020, training loss: 62.16233825683594 = 0.019050395116209984 + 10.0 * 6.214328765869141
Epoch 2020, val loss: 1.2487053871154785
Epoch 2030, training loss: 62.1549072265625 = 0.018759779632091522 + 10.0 * 6.213614463806152
Epoch 2030, val loss: 1.2527775764465332
Epoch 2040, training loss: 62.15616226196289 = 0.018474172800779343 + 10.0 * 6.21376895904541
Epoch 2040, val loss: 1.2567461729049683
Epoch 2050, training loss: 62.21480941772461 = 0.018193168565630913 + 10.0 * 6.219661712646484
Epoch 2050, val loss: 1.2606987953186035
Epoch 2060, training loss: 62.1695442199707 = 0.01791083998978138 + 10.0 * 6.215163230895996
Epoch 2060, val loss: 1.2645598649978638
Epoch 2070, training loss: 62.16580581665039 = 0.01764114946126938 + 10.0 * 6.214816093444824
Epoch 2070, val loss: 1.2684170007705688
Epoch 2080, training loss: 62.191261291503906 = 0.017378438264131546 + 10.0 * 6.217388153076172
Epoch 2080, val loss: 1.2722758054733276
Epoch 2090, training loss: 62.183998107910156 = 0.017112933099269867 + 10.0 * 6.216688632965088
Epoch 2090, val loss: 1.2759701013565063
Epoch 2100, training loss: 62.14678955078125 = 0.016857197508215904 + 10.0 * 6.212993144989014
Epoch 2100, val loss: 1.2797688245773315
Epoch 2110, training loss: 62.13804626464844 = 0.016613397747278214 + 10.0 * 6.212143421173096
Epoch 2110, val loss: 1.2835822105407715
Epoch 2120, training loss: 62.13064956665039 = 0.016374515369534492 + 10.0 * 6.211427688598633
Epoch 2120, val loss: 1.2875518798828125
Epoch 2130, training loss: 62.135284423828125 = 0.016141893342137337 + 10.0 * 6.2119140625
Epoch 2130, val loss: 1.2913779020309448
Epoch 2140, training loss: 62.221588134765625 = 0.015913156792521477 + 10.0 * 6.220567226409912
Epoch 2140, val loss: 1.2950303554534912
Epoch 2150, training loss: 62.17218017578125 = 0.01567748561501503 + 10.0 * 6.2156500816345215
Epoch 2150, val loss: 1.2984721660614014
Epoch 2160, training loss: 62.140052795410156 = 0.015448402613401413 + 10.0 * 6.212460517883301
Epoch 2160, val loss: 1.3021942377090454
Epoch 2170, training loss: 62.121158599853516 = 0.01523330993950367 + 10.0 * 6.210592746734619
Epoch 2170, val loss: 1.3059285879135132
Epoch 2180, training loss: 62.11910629272461 = 0.015025397762656212 + 10.0 * 6.2104082107543945
Epoch 2180, val loss: 1.3095555305480957
Epoch 2190, training loss: 62.12197494506836 = 0.014820153824985027 + 10.0 * 6.210715293884277
Epoch 2190, val loss: 1.3131065368652344
Epoch 2200, training loss: 62.15306854248047 = 0.01461842842400074 + 10.0 * 6.213845252990723
Epoch 2200, val loss: 1.3165342807769775
Epoch 2210, training loss: 62.12970733642578 = 0.014412960968911648 + 10.0 * 6.211529731750488
Epoch 2210, val loss: 1.320582389831543
Epoch 2220, training loss: 62.15190505981445 = 0.014216135255992413 + 10.0 * 6.21376895904541
Epoch 2220, val loss: 1.3236995935440063
Epoch 2230, training loss: 62.12527084350586 = 0.0140226436778903 + 10.0 * 6.211124897003174
Epoch 2230, val loss: 1.3272905349731445
Epoch 2240, training loss: 62.12547302246094 = 0.013832858763635159 + 10.0 * 6.2111639976501465
Epoch 2240, val loss: 1.3307054042816162
Epoch 2250, training loss: 62.111915588378906 = 0.013648688793182373 + 10.0 * 6.209826469421387
Epoch 2250, val loss: 1.3342102766036987
Epoch 2260, training loss: 62.140628814697266 = 0.013470664620399475 + 10.0 * 6.212716102600098
Epoch 2260, val loss: 1.3376294374465942
Epoch 2270, training loss: 62.1038818359375 = 0.013289280235767365 + 10.0 * 6.209059238433838
Epoch 2270, val loss: 1.3410148620605469
Epoch 2280, training loss: 62.10480880737305 = 0.013117761351168156 + 10.0 * 6.209168910980225
Epoch 2280, val loss: 1.3442533016204834
Epoch 2290, training loss: 62.122684478759766 = 0.012950693257153034 + 10.0 * 6.210973262786865
Epoch 2290, val loss: 1.3477970361709595
Epoch 2300, training loss: 62.126861572265625 = 0.01277944166213274 + 10.0 * 6.2114081382751465
Epoch 2300, val loss: 1.351002812385559
Epoch 2310, training loss: 62.11016845703125 = 0.012611232697963715 + 10.0 * 6.209755897521973
Epoch 2310, val loss: 1.3540737628936768
Epoch 2320, training loss: 62.09378433227539 = 0.012451747432351112 + 10.0 * 6.208133220672607
Epoch 2320, val loss: 1.3574628829956055
Epoch 2330, training loss: 62.09092712402344 = 0.012296270579099655 + 10.0 * 6.2078633308410645
Epoch 2330, val loss: 1.3608009815216064
Epoch 2340, training loss: 62.117149353027344 = 0.012146009132266045 + 10.0 * 6.210500240325928
Epoch 2340, val loss: 1.3641314506530762
Epoch 2350, training loss: 62.09640884399414 = 0.011989282444119453 + 10.0 * 6.208441734313965
Epoch 2350, val loss: 1.3672113418579102
Epoch 2360, training loss: 62.08974075317383 = 0.01183987408876419 + 10.0 * 6.207789897918701
Epoch 2360, val loss: 1.3702536821365356
Epoch 2370, training loss: 62.08841323852539 = 0.01169477216899395 + 10.0 * 6.207671642303467
Epoch 2370, val loss: 1.373624563217163
Epoch 2380, training loss: 62.11323928833008 = 0.011554987169802189 + 10.0 * 6.210168361663818
Epoch 2380, val loss: 1.3768402338027954
Epoch 2390, training loss: 62.09962844848633 = 0.011411041021347046 + 10.0 * 6.208821773529053
Epoch 2390, val loss: 1.3796579837799072
Epoch 2400, training loss: 62.08355712890625 = 0.011270418763160706 + 10.0 * 6.207228660583496
Epoch 2400, val loss: 1.3826693296432495
Epoch 2410, training loss: 62.07889175415039 = 0.011135749518871307 + 10.0 * 6.206775665283203
Epoch 2410, val loss: 1.385875940322876
Epoch 2420, training loss: 62.08948516845703 = 0.011006134562194347 + 10.0 * 6.207848072052002
Epoch 2420, val loss: 1.3890631198883057
Epoch 2430, training loss: 62.10332107543945 = 0.010875578969717026 + 10.0 * 6.209244728088379
Epoch 2430, val loss: 1.392112374305725
Epoch 2440, training loss: 62.09597396850586 = 0.010746430605649948 + 10.0 * 6.208522796630859
Epoch 2440, val loss: 1.395317792892456
Epoch 2450, training loss: 62.06916809082031 = 0.01061874721199274 + 10.0 * 6.205854892730713
Epoch 2450, val loss: 1.398079752922058
Epoch 2460, training loss: 62.066463470458984 = 0.01049690693616867 + 10.0 * 6.205596446990967
Epoch 2460, val loss: 1.4011383056640625
Epoch 2470, training loss: 62.08798599243164 = 0.0103787612169981 + 10.0 * 6.207760810852051
Epoch 2470, val loss: 1.4042173624038696
Epoch 2480, training loss: 62.081783294677734 = 0.010257137008011341 + 10.0 * 6.207152366638184
Epoch 2480, val loss: 1.4071208238601685
Epoch 2490, training loss: 62.08610534667969 = 0.010139357298612595 + 10.0 * 6.207596778869629
Epoch 2490, val loss: 1.4098877906799316
Epoch 2500, training loss: 62.0667724609375 = 0.010022010654211044 + 10.0 * 6.20567512512207
Epoch 2500, val loss: 1.4127033948898315
Epoch 2510, training loss: 62.05637741088867 = 0.009910923428833485 + 10.0 * 6.204646587371826
Epoch 2510, val loss: 1.4157931804656982
Epoch 2520, training loss: 62.06372833251953 = 0.009802310727536678 + 10.0 * 6.205392360687256
Epoch 2520, val loss: 1.4188851118087769
Epoch 2530, training loss: 62.087982177734375 = 0.009694854728877544 + 10.0 * 6.207828998565674
Epoch 2530, val loss: 1.421818494796753
Epoch 2540, training loss: 62.11335372924805 = 0.009585470892488956 + 10.0 * 6.210376739501953
Epoch 2540, val loss: 1.4242594242095947
Epoch 2550, training loss: 62.06674575805664 = 0.009476035833358765 + 10.0 * 6.2057271003723145
Epoch 2550, val loss: 1.4271247386932373
Epoch 2560, training loss: 62.05276870727539 = 0.00937272422015667 + 10.0 * 6.204339504241943
Epoch 2560, val loss: 1.4298967123031616
Epoch 2570, training loss: 62.04914093017578 = 0.009273258969187737 + 10.0 * 6.203986644744873
Epoch 2570, val loss: 1.4328269958496094
Epoch 2580, training loss: 62.098873138427734 = 0.009176713414490223 + 10.0 * 6.208969593048096
Epoch 2580, val loss: 1.4354368448257446
Epoch 2590, training loss: 62.068302154541016 = 0.009072410874068737 + 10.0 * 6.205923080444336
Epoch 2590, val loss: 1.4384225606918335
Epoch 2600, training loss: 62.05052185058594 = 0.00897285621613264 + 10.0 * 6.204154968261719
Epoch 2600, val loss: 1.4409250020980835
Epoch 2610, training loss: 62.048095703125 = 0.00887905340641737 + 10.0 * 6.203921318054199
Epoch 2610, val loss: 1.4437531232833862
Epoch 2620, training loss: 62.051326751708984 = 0.008787507191300392 + 10.0 * 6.204253673553467
Epoch 2620, val loss: 1.4467285871505737
Epoch 2630, training loss: 62.062530517578125 = 0.008695906959474087 + 10.0 * 6.20538330078125
Epoch 2630, val loss: 1.4491922855377197
Epoch 2640, training loss: 62.044403076171875 = 0.008605223149061203 + 10.0 * 6.203579902648926
Epoch 2640, val loss: 1.4516761302947998
Epoch 2650, training loss: 62.05231857299805 = 0.00851724948734045 + 10.0 * 6.204380035400391
Epoch 2650, val loss: 1.454499363899231
Epoch 2660, training loss: 62.04869079589844 = 0.008429278619587421 + 10.0 * 6.204026222229004
Epoch 2660, val loss: 1.4571477174758911
Epoch 2670, training loss: 62.0482177734375 = 0.008343784138560295 + 10.0 * 6.2039875984191895
Epoch 2670, val loss: 1.45964515209198
Epoch 2680, training loss: 62.0616569519043 = 0.008257715031504631 + 10.0 * 6.2053399085998535
Epoch 2680, val loss: 1.4622834920883179
Epoch 2690, training loss: 62.05314254760742 = 0.008173132315278053 + 10.0 * 6.20449686050415
Epoch 2690, val loss: 1.464672565460205
Epoch 2700, training loss: 62.04311752319336 = 0.00808996893465519 + 10.0 * 6.203502655029297
Epoch 2700, val loss: 1.4672822952270508
Epoch 2710, training loss: 62.025638580322266 = 0.008008806966245174 + 10.0 * 6.201763153076172
Epoch 2710, val loss: 1.4696476459503174
Epoch 2720, training loss: 62.02678680419922 = 0.007932063192129135 + 10.0 * 6.20188570022583
Epoch 2720, val loss: 1.4722410440444946
Epoch 2730, training loss: 62.052879333496094 = 0.007855916395783424 + 10.0 * 6.204502582550049
Epoch 2730, val loss: 1.4746928215026855
Epoch 2740, training loss: 62.03862762451172 = 0.007775739300996065 + 10.0 * 6.203085422515869
Epoch 2740, val loss: 1.4770169258117676
Epoch 2750, training loss: 62.04261016845703 = 0.007698846980929375 + 10.0 * 6.2034912109375
Epoch 2750, val loss: 1.4795641899108887
Epoch 2760, training loss: 62.03482437133789 = 0.007624738849699497 + 10.0 * 6.202719688415527
Epoch 2760, val loss: 1.4819527864456177
Epoch 2770, training loss: 62.0274543762207 = 0.0075514549389481544 + 10.0 * 6.201990127563477
Epoch 2770, val loss: 1.4845504760742188
Epoch 2780, training loss: 62.02698516845703 = 0.007480639033019543 + 10.0 * 6.201950550079346
Epoch 2780, val loss: 1.4870859384536743
Epoch 2790, training loss: 62.05591583251953 = 0.007409870624542236 + 10.0 * 6.204850673675537
Epoch 2790, val loss: 1.4894921779632568
Epoch 2800, training loss: 62.03563690185547 = 0.007338011637330055 + 10.0 * 6.202829837799072
Epoch 2800, val loss: 1.4915766716003418
Epoch 2810, training loss: 62.02534866333008 = 0.007268671412020922 + 10.0 * 6.201807975769043
Epoch 2810, val loss: 1.4940565824508667
Epoch 2820, training loss: 62.040809631347656 = 0.007200407329946756 + 10.0 * 6.203360557556152
Epoch 2820, val loss: 1.4965827465057373
Epoch 2830, training loss: 62.01527404785156 = 0.007133177947252989 + 10.0 * 6.200814247131348
Epoch 2830, val loss: 1.4986482858657837
Epoch 2840, training loss: 62.01800537109375 = 0.007067782338708639 + 10.0 * 6.201093673706055
Epoch 2840, val loss: 1.5009044408798218
Epoch 2850, training loss: 62.07405471801758 = 0.0070035639218986034 + 10.0 * 6.206705093383789
Epoch 2850, val loss: 1.5031988620758057
Epoch 2860, training loss: 62.01480484008789 = 0.006937076803296804 + 10.0 * 6.200786590576172
Epoch 2860, val loss: 1.5054435729980469
Epoch 2870, training loss: 62.005645751953125 = 0.006873061414808035 + 10.0 * 6.1998772621154785
Epoch 2870, val loss: 1.5078256130218506
Epoch 2880, training loss: 62.00593948364258 = 0.006813309621065855 + 10.0 * 6.1999125480651855
Epoch 2880, val loss: 1.5101666450500488
Epoch 2890, training loss: 62.0369987487793 = 0.006754650734364986 + 10.0 * 6.203024387359619
Epoch 2890, val loss: 1.5123684406280518
Epoch 2900, training loss: 62.000030517578125 = 0.006691530346870422 + 10.0 * 6.199334144592285
Epoch 2900, val loss: 1.5145492553710938
Epoch 2910, training loss: 61.99971008300781 = 0.006630742456763983 + 10.0 * 6.199307918548584
Epoch 2910, val loss: 1.5167829990386963
Epoch 2920, training loss: 62.00923538208008 = 0.006574665196239948 + 10.0 * 6.200265884399414
Epoch 2920, val loss: 1.519026279449463
Epoch 2930, training loss: 62.03797912597656 = 0.006517227739095688 + 10.0 * 6.203146457672119
Epoch 2930, val loss: 1.5211759805679321
Epoch 2940, training loss: 62.01511764526367 = 0.006458710879087448 + 10.0 * 6.200865745544434
Epoch 2940, val loss: 1.5230705738067627
Epoch 2950, training loss: 62.010215759277344 = 0.006402263417840004 + 10.0 * 6.200381278991699
Epoch 2950, val loss: 1.5252225399017334
Epoch 2960, training loss: 62.00699996948242 = 0.006347232963889837 + 10.0 * 6.2000651359558105
Epoch 2960, val loss: 1.5274016857147217
Epoch 2970, training loss: 62.010162353515625 = 0.006293918937444687 + 10.0 * 6.200387001037598
Epoch 2970, val loss: 1.5294831991195679
Epoch 2980, training loss: 61.998329162597656 = 0.006238653790205717 + 10.0 * 6.199209213256836
Epoch 2980, val loss: 1.5316346883773804
Epoch 2990, training loss: 62.00580596923828 = 0.0061860838904976845 + 10.0 * 6.1999616622924805
Epoch 2990, val loss: 1.5337419509887695
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 87.9149169921875 = 1.94701087474823 + 10.0 * 8.596790313720703
Epoch 0, val loss: 1.9460370540618896
Epoch 10, training loss: 87.89653015136719 = 1.9378308057785034 + 10.0 * 8.595870018005371
Epoch 10, val loss: 1.9372321367263794
Epoch 20, training loss: 87.82733917236328 = 1.9264558553695679 + 10.0 * 8.590088844299316
Epoch 20, val loss: 1.9259220361709595
Epoch 30, training loss: 87.45458984375 = 1.9116519689559937 + 10.0 * 8.554293632507324
Epoch 30, val loss: 1.9109867811203003
Epoch 40, training loss: 85.29443359375 = 1.893545389175415 + 10.0 * 8.340088844299316
Epoch 40, val loss: 1.8929823637008667
Epoch 50, training loss: 77.27899169921875 = 1.8714230060577393 + 10.0 * 7.540756702423096
Epoch 50, val loss: 1.8712965250015259
Epoch 60, training loss: 73.80181884765625 = 1.853949785232544 + 10.0 * 7.194786548614502
Epoch 60, val loss: 1.8558740615844727
Epoch 70, training loss: 71.53380584716797 = 1.8407233953475952 + 10.0 * 6.969308853149414
Epoch 70, val loss: 1.84360933303833
Epoch 80, training loss: 70.3532943725586 = 1.8281992673873901 + 10.0 * 6.852509498596191
Epoch 80, val loss: 1.8318897485733032
Epoch 90, training loss: 69.70059204101562 = 1.8173491954803467 + 10.0 * 6.788324356079102
Epoch 90, val loss: 1.8213512897491455
Epoch 100, training loss: 69.06952667236328 = 1.8066788911819458 + 10.0 * 6.726284503936768
Epoch 100, val loss: 1.8110147714614868
Epoch 110, training loss: 68.56437683105469 = 1.7975257635116577 + 10.0 * 6.676685333251953
Epoch 110, val loss: 1.8018769025802612
Epoch 120, training loss: 68.20198059082031 = 1.788603663444519 + 10.0 * 6.6413373947143555
Epoch 120, val loss: 1.7927957773208618
Epoch 130, training loss: 67.89686584472656 = 1.7791576385498047 + 10.0 * 6.6117706298828125
Epoch 130, val loss: 1.7832528352737427
Epoch 140, training loss: 67.57816314697266 = 1.7695221900939941 + 10.0 * 6.580863952636719
Epoch 140, val loss: 1.7737455368041992
Epoch 150, training loss: 67.30038452148438 = 1.7597436904907227 + 10.0 * 6.5540642738342285
Epoch 150, val loss: 1.7642269134521484
Epoch 160, training loss: 67.07682800292969 = 1.7492539882659912 + 10.0 * 6.532757759094238
Epoch 160, val loss: 1.7540861368179321
Epoch 170, training loss: 66.88382720947266 = 1.7375818490982056 + 10.0 * 6.514625072479248
Epoch 170, val loss: 1.7428873777389526
Epoch 180, training loss: 66.71141815185547 = 1.7246745824813843 + 10.0 * 6.498674392700195
Epoch 180, val loss: 1.7306894063949585
Epoch 190, training loss: 66.56046295166016 = 1.7104629278182983 + 10.0 * 6.484999656677246
Epoch 190, val loss: 1.7174668312072754
Epoch 200, training loss: 66.42290496826172 = 1.69487464427948 + 10.0 * 6.472802639007568
Epoch 200, val loss: 1.7030220031738281
Epoch 210, training loss: 66.28799438476562 = 1.677788257598877 + 10.0 * 6.4610209465026855
Epoch 210, val loss: 1.6872718334197998
Epoch 220, training loss: 66.16901397705078 = 1.6592357158660889 + 10.0 * 6.450977802276611
Epoch 220, val loss: 1.6702406406402588
Epoch 230, training loss: 66.03934478759766 = 1.6391427516937256 + 10.0 * 6.4400200843811035
Epoch 230, val loss: 1.6519343852996826
Epoch 240, training loss: 65.91934204101562 = 1.617478609085083 + 10.0 * 6.4301862716674805
Epoch 240, val loss: 1.6323119401931763
Epoch 250, training loss: 65.80916595458984 = 1.5943124294281006 + 10.0 * 6.421485424041748
Epoch 250, val loss: 1.6113617420196533
Epoch 260, training loss: 65.70158386230469 = 1.5698890686035156 + 10.0 * 6.413168907165527
Epoch 260, val loss: 1.5894066095352173
Epoch 270, training loss: 65.58440399169922 = 1.5443675518035889 + 10.0 * 6.404004096984863
Epoch 270, val loss: 1.5665247440338135
Epoch 280, training loss: 65.51319885253906 = 1.5179343223571777 + 10.0 * 6.399526596069336
Epoch 280, val loss: 1.5429331064224243
Epoch 290, training loss: 65.39203643798828 = 1.490788459777832 + 10.0 * 6.390124320983887
Epoch 290, val loss: 1.5190869569778442
Epoch 300, training loss: 65.3015365600586 = 1.4633848667144775 + 10.0 * 6.383814811706543
Epoch 300, val loss: 1.495115041732788
Epoch 310, training loss: 65.21766662597656 = 1.4358199834823608 + 10.0 * 6.378184795379639
Epoch 310, val loss: 1.4712488651275635
Epoch 320, training loss: 65.16356658935547 = 1.4081168174743652 + 10.0 * 6.375544548034668
Epoch 320, val loss: 1.4476255178451538
Epoch 330, training loss: 65.06229400634766 = 1.3808057308197021 + 10.0 * 6.368149280548096
Epoch 330, val loss: 1.424540400505066
Epoch 340, training loss: 64.97950744628906 = 1.3538134098052979 + 10.0 * 6.362569332122803
Epoch 340, val loss: 1.4019862413406372
Epoch 350, training loss: 64.91986083984375 = 1.3272210359573364 + 10.0 * 6.359263896942139
Epoch 350, val loss: 1.3799558877944946
Epoch 360, training loss: 64.85781860351562 = 1.3008347749710083 + 10.0 * 6.355698108673096
Epoch 360, val loss: 1.3588175773620605
Epoch 370, training loss: 64.80420684814453 = 1.2751097679138184 + 10.0 * 6.352909564971924
Epoch 370, val loss: 1.338119387626648
Epoch 380, training loss: 64.72127532958984 = 1.249634861946106 + 10.0 * 6.347164154052734
Epoch 380, val loss: 1.3181427717208862
Epoch 390, training loss: 64.64893341064453 = 1.224724531173706 + 10.0 * 6.34242057800293
Epoch 390, val loss: 1.2987306118011475
Epoch 400, training loss: 64.59449768066406 = 1.2001240253448486 + 10.0 * 6.339437007904053
Epoch 400, val loss: 1.2798421382904053
Epoch 410, training loss: 64.54388427734375 = 1.175750970840454 + 10.0 * 6.336812973022461
Epoch 410, val loss: 1.2615267038345337
Epoch 420, training loss: 64.47107696533203 = 1.1517364978790283 + 10.0 * 6.331934452056885
Epoch 420, val loss: 1.243600606918335
Epoch 430, training loss: 64.43163299560547 = 1.1279548406600952 + 10.0 * 6.3303680419921875
Epoch 430, val loss: 1.226253628730774
Epoch 440, training loss: 64.3884048461914 = 1.1045732498168945 + 10.0 * 6.328382968902588
Epoch 440, val loss: 1.209186315536499
Epoch 450, training loss: 64.31977844238281 = 1.0813010931015015 + 10.0 * 6.323847770690918
Epoch 450, val loss: 1.1927229166030884
Epoch 460, training loss: 64.26453399658203 = 1.0583258867263794 + 10.0 * 6.320620536804199
Epoch 460, val loss: 1.1765295267105103
Epoch 470, training loss: 64.2544937133789 = 1.0353929996490479 + 10.0 * 6.3219099044799805
Epoch 470, val loss: 1.1606438159942627
Epoch 480, training loss: 64.17636108398438 = 1.0128039121627808 + 10.0 * 6.3163557052612305
Epoch 480, val loss: 1.1450332403182983
Epoch 490, training loss: 64.12310028076172 = 0.9903966188430786 + 10.0 * 6.313270092010498
Epoch 490, val loss: 1.1298162937164307
Epoch 500, training loss: 64.07719421386719 = 0.9681797027587891 + 10.0 * 6.310901641845703
Epoch 500, val loss: 1.1148313283920288
Epoch 510, training loss: 64.07020568847656 = 0.9459903240203857 + 10.0 * 6.3124213218688965
Epoch 510, val loss: 1.1001036167144775
Epoch 520, training loss: 64.00480651855469 = 0.9240185618400574 + 10.0 * 6.308078765869141
Epoch 520, val loss: 1.0855462551116943
Epoch 530, training loss: 63.951744079589844 = 0.9022326469421387 + 10.0 * 6.304951190948486
Epoch 530, val loss: 1.0712122917175293
Epoch 540, training loss: 63.93131637573242 = 0.880608320236206 + 10.0 * 6.305070877075195
Epoch 540, val loss: 1.0570685863494873
Epoch 550, training loss: 63.91046905517578 = 0.8589750528335571 + 10.0 * 6.305149555206299
Epoch 550, val loss: 1.043146014213562
Epoch 560, training loss: 63.82789611816406 = 0.8375113606452942 + 10.0 * 6.299038410186768
Epoch 560, val loss: 1.029489517211914
Epoch 570, training loss: 63.78821563720703 = 0.8164606690406799 + 10.0 * 6.297175407409668
Epoch 570, val loss: 1.0159807205200195
Epoch 580, training loss: 63.76131057739258 = 0.7955806255340576 + 10.0 * 6.296572685241699
Epoch 580, val loss: 1.0028196573257446
Epoch 590, training loss: 63.717498779296875 = 0.7748600244522095 + 10.0 * 6.29426383972168
Epoch 590, val loss: 0.9900122880935669
Epoch 600, training loss: 63.680179595947266 = 0.7545742988586426 + 10.0 * 6.292560577392578
Epoch 600, val loss: 0.9773157238960266
Epoch 610, training loss: 63.6386833190918 = 0.7345520853996277 + 10.0 * 6.2904133796691895
Epoch 610, val loss: 0.9651143550872803
Epoch 620, training loss: 63.62955856323242 = 0.7147852182388306 + 10.0 * 6.291477203369141
Epoch 620, val loss: 0.9532892107963562
Epoch 630, training loss: 63.5767707824707 = 0.6952947378158569 + 10.0 * 6.288147926330566
Epoch 630, val loss: 0.941615104675293
Epoch 640, training loss: 63.53069305419922 = 0.6761133074760437 + 10.0 * 6.285458087921143
Epoch 640, val loss: 0.9304039478302002
Epoch 650, training loss: 63.50735092163086 = 0.6573498249053955 + 10.0 * 6.285000324249268
Epoch 650, val loss: 0.9195387363433838
Epoch 660, training loss: 63.47922134399414 = 0.6388707756996155 + 10.0 * 6.2840352058410645
Epoch 660, val loss: 0.908972442150116
Epoch 670, training loss: 63.443580627441406 = 0.6206828951835632 + 10.0 * 6.282289981842041
Epoch 670, val loss: 0.8987927436828613
Epoch 680, training loss: 63.40488052368164 = 0.6029835343360901 + 10.0 * 6.280189514160156
Epoch 680, val loss: 0.8890743851661682
Epoch 690, training loss: 63.37020492553711 = 0.5856627821922302 + 10.0 * 6.278454303741455
Epoch 690, val loss: 0.8798192739486694
Epoch 700, training loss: 63.38037872314453 = 0.5686657428741455 + 10.0 * 6.2811713218688965
Epoch 700, val loss: 0.8709882497787476
Epoch 710, training loss: 63.347171783447266 = 0.5518391132354736 + 10.0 * 6.279533386230469
Epoch 710, val loss: 0.862436056137085
Epoch 720, training loss: 63.28770446777344 = 0.5355584025382996 + 10.0 * 6.275214672088623
Epoch 720, val loss: 0.8541987538337708
Epoch 730, training loss: 63.2581672668457 = 0.5196797251701355 + 10.0 * 6.273848533630371
Epoch 730, val loss: 0.8465219736099243
Epoch 740, training loss: 63.24806594848633 = 0.5041771531105042 + 10.0 * 6.274388790130615
Epoch 740, val loss: 0.839267373085022
Epoch 750, training loss: 63.22101593017578 = 0.48885634541511536 + 10.0 * 6.2732157707214355
Epoch 750, val loss: 0.8326200246810913
Epoch 760, training loss: 63.18325424194336 = 0.47409456968307495 + 10.0 * 6.270915985107422
Epoch 760, val loss: 0.8258944153785706
Epoch 770, training loss: 63.15430450439453 = 0.4596812427043915 + 10.0 * 6.2694621086120605
Epoch 770, val loss: 0.8199176788330078
Epoch 780, training loss: 63.14102554321289 = 0.44569647312164307 + 10.0 * 6.269532680511475
Epoch 780, val loss: 0.8143850564956665
Epoch 790, training loss: 63.102989196777344 = 0.4320167005062103 + 10.0 * 6.267096996307373
Epoch 790, val loss: 0.8092463612556458
Epoch 800, training loss: 63.08680725097656 = 0.4187508523464203 + 10.0 * 6.266805648803711
Epoch 800, val loss: 0.8044137358665466
Epoch 810, training loss: 63.07918167114258 = 0.405872106552124 + 10.0 * 6.267331123352051
Epoch 810, val loss: 0.8000809550285339
Epoch 820, training loss: 63.04734802246094 = 0.39340656995773315 + 10.0 * 6.26539421081543
Epoch 820, val loss: 0.7962746024131775
Epoch 830, training loss: 63.035179138183594 = 0.38129568099975586 + 10.0 * 6.265388488769531
Epoch 830, val loss: 0.7925356030464172
Epoch 840, training loss: 63.00074005126953 = 0.36953288316726685 + 10.0 * 6.263120651245117
Epoch 840, val loss: 0.789375901222229
Epoch 850, training loss: 62.97819137573242 = 0.3581996560096741 + 10.0 * 6.261999130249023
Epoch 850, val loss: 0.7863495349884033
Epoch 860, training loss: 62.96771240234375 = 0.3471335470676422 + 10.0 * 6.262057781219482
Epoch 860, val loss: 0.7838293313980103
Epoch 870, training loss: 62.9414176940918 = 0.33645331859588623 + 10.0 * 6.260496616363525
Epoch 870, val loss: 0.7813092470169067
Epoch 880, training loss: 62.926170349121094 = 0.3260386288166046 + 10.0 * 6.260013103485107
Epoch 880, val loss: 0.779166579246521
Epoch 890, training loss: 62.90446853637695 = 0.31594380736351013 + 10.0 * 6.258852481842041
Epoch 890, val loss: 0.7772408723831177
Epoch 900, training loss: 62.929080963134766 = 0.3061683773994446 + 10.0 * 6.262291431427002
Epoch 900, val loss: 0.7755900025367737
Epoch 910, training loss: 62.888153076171875 = 0.29656949639320374 + 10.0 * 6.259158134460449
Epoch 910, val loss: 0.7740418314933777
Epoch 920, training loss: 62.851661682128906 = 0.28732791543006897 + 10.0 * 6.256433486938477
Epoch 920, val loss: 0.7726783156394958
Epoch 930, training loss: 62.83259963989258 = 0.27835044264793396 + 10.0 * 6.255424976348877
Epoch 930, val loss: 0.7715377807617188
Epoch 940, training loss: 62.866817474365234 = 0.2696240544319153 + 10.0 * 6.259719371795654
Epoch 940, val loss: 0.7706360816955566
Epoch 950, training loss: 62.81376647949219 = 0.26107698678970337 + 10.0 * 6.2552690505981445
Epoch 950, val loss: 0.7696100473403931
Epoch 960, training loss: 62.78609085083008 = 0.25282737612724304 + 10.0 * 6.253326416015625
Epoch 960, val loss: 0.7689366936683655
Epoch 970, training loss: 62.78504943847656 = 0.24481776356697083 + 10.0 * 6.25402307510376
Epoch 970, val loss: 0.7684319615364075
Epoch 980, training loss: 62.76662063598633 = 0.2370547652244568 + 10.0 * 6.252956867218018
Epoch 980, val loss: 0.7680180668830872
Epoch 990, training loss: 62.756710052490234 = 0.22951073944568634 + 10.0 * 6.252719879150391
Epoch 990, val loss: 0.7676440477371216
Epoch 1000, training loss: 62.728065490722656 = 0.2222375124692917 + 10.0 * 6.250582695007324
Epoch 1000, val loss: 0.7674906253814697
Epoch 1010, training loss: 62.720672607421875 = 0.21520981192588806 + 10.0 * 6.250546455383301
Epoch 1010, val loss: 0.7675330638885498
Epoch 1020, training loss: 62.723079681396484 = 0.20839734375476837 + 10.0 * 6.251468181610107
Epoch 1020, val loss: 0.7677778005599976
Epoch 1030, training loss: 62.69638442993164 = 0.20185652375221252 + 10.0 * 6.249453067779541
Epoch 1030, val loss: 0.7679919600486755
Epoch 1040, training loss: 62.68568420410156 = 0.19552171230316162 + 10.0 * 6.249016284942627
Epoch 1040, val loss: 0.7685039639472961
Epoch 1050, training loss: 62.67312240600586 = 0.1894100308418274 + 10.0 * 6.248371124267578
Epoch 1050, val loss: 0.7693034410476685
Epoch 1060, training loss: 62.66202926635742 = 0.18352405726909637 + 10.0 * 6.24785041809082
Epoch 1060, val loss: 0.7701420783996582
Epoch 1070, training loss: 62.66875076293945 = 0.17786918580532074 + 10.0 * 6.249088287353516
Epoch 1070, val loss: 0.7710987329483032
Epoch 1080, training loss: 62.64238739013672 = 0.17240126430988312 + 10.0 * 6.2469987869262695
Epoch 1080, val loss: 0.7721030712127686
Epoch 1090, training loss: 62.61622619628906 = 0.16711053252220154 + 10.0 * 6.2449116706848145
Epoch 1090, val loss: 0.7733451724052429
Epoch 1100, training loss: 62.60513687133789 = 0.16207073628902435 + 10.0 * 6.244306564331055
Epoch 1100, val loss: 0.7746952772140503
Epoch 1110, training loss: 62.59553909301758 = 0.1572280079126358 + 10.0 * 6.243831157684326
Epoch 1110, val loss: 0.7762914299964905
Epoch 1120, training loss: 62.638179779052734 = 0.15256091952323914 + 10.0 * 6.248561859130859
Epoch 1120, val loss: 0.777793288230896
Epoch 1130, training loss: 62.618003845214844 = 0.14798957109451294 + 10.0 * 6.2470011711120605
Epoch 1130, val loss: 0.7798771858215332
Epoch 1140, training loss: 62.57093811035156 = 0.14364080131053925 + 10.0 * 6.242729663848877
Epoch 1140, val loss: 0.7817230820655823
Epoch 1150, training loss: 62.56866455078125 = 0.1394660323858261 + 10.0 * 6.242919921875
Epoch 1150, val loss: 0.7835752964019775
Epoch 1160, training loss: 62.5553092956543 = 0.1354173719882965 + 10.0 * 6.2419891357421875
Epoch 1160, val loss: 0.7858489751815796
Epoch 1170, training loss: 62.5519905090332 = 0.13152065873146057 + 10.0 * 6.242047309875488
Epoch 1170, val loss: 0.7882030010223389
Epoch 1180, training loss: 62.54510498046875 = 0.1277538686990738 + 10.0 * 6.241734981536865
Epoch 1180, val loss: 0.7904177904129028
Epoch 1190, training loss: 62.52863311767578 = 0.12411243468523026 + 10.0 * 6.240452289581299
Epoch 1190, val loss: 0.7928277254104614
Epoch 1200, training loss: 62.52009963989258 = 0.1206255629658699 + 10.0 * 6.239947319030762
Epoch 1200, val loss: 0.7953464984893799
Epoch 1210, training loss: 62.55582046508789 = 0.1172715350985527 + 10.0 * 6.243854999542236
Epoch 1210, val loss: 0.7978658676147461
Epoch 1220, training loss: 62.50696563720703 = 0.11394758522510529 + 10.0 * 6.239301681518555
Epoch 1220, val loss: 0.8004549741744995
Epoch 1230, training loss: 62.4898796081543 = 0.1108069196343422 + 10.0 * 6.237907409667969
Epoch 1230, val loss: 0.8031375408172607
Epoch 1240, training loss: 62.48120880126953 = 0.10778157413005829 + 10.0 * 6.237342834472656
Epoch 1240, val loss: 0.8059095740318298
Epoch 1250, training loss: 62.49728012084961 = 0.10485143959522247 + 10.0 * 6.239243030548096
Epoch 1250, val loss: 0.8088186979293823
Epoch 1260, training loss: 62.47031784057617 = 0.10199626535177231 + 10.0 * 6.236832141876221
Epoch 1260, val loss: 0.8117058873176575
Epoch 1270, training loss: 62.499267578125 = 0.0992315337061882 + 10.0 * 6.24000358581543
Epoch 1270, val loss: 0.8147225975990295
Epoch 1280, training loss: 62.45778274536133 = 0.09658609330654144 + 10.0 * 6.236119747161865
Epoch 1280, val loss: 0.8174628615379333
Epoch 1290, training loss: 62.443111419677734 = 0.09402526170015335 + 10.0 * 6.234908580780029
Epoch 1290, val loss: 0.820597231388092
Epoch 1300, training loss: 62.44082260131836 = 0.09155651181936264 + 10.0 * 6.234926700592041
Epoch 1300, val loss: 0.82379150390625
Epoch 1310, training loss: 62.51215744018555 = 0.08917951583862305 + 10.0 * 6.242297649383545
Epoch 1310, val loss: 0.8268372416496277
Epoch 1320, training loss: 62.45131301879883 = 0.08680590987205505 + 10.0 * 6.236450672149658
Epoch 1320, val loss: 0.8299424052238464
Epoch 1330, training loss: 62.424434661865234 = 0.08453710377216339 + 10.0 * 6.233989715576172
Epoch 1330, val loss: 0.8331225514411926
Epoch 1340, training loss: 62.40995788574219 = 0.08237597346305847 + 10.0 * 6.232758522033691
Epoch 1340, val loss: 0.8364222049713135
Epoch 1350, training loss: 62.404422760009766 = 0.08028428256511688 + 10.0 * 6.2324137687683105
Epoch 1350, val loss: 0.8397679328918457
Epoch 1360, training loss: 62.429832458496094 = 0.0782598927617073 + 10.0 * 6.235157489776611
Epoch 1360, val loss: 0.8431685566902161
Epoch 1370, training loss: 62.39188766479492 = 0.07626713812351227 + 10.0 * 6.23156213760376
Epoch 1370, val loss: 0.8464155197143555
Epoch 1380, training loss: 62.41497802734375 = 0.07436089217662811 + 10.0 * 6.2340617179870605
Epoch 1380, val loss: 0.8497632741928101
Epoch 1390, training loss: 62.38656997680664 = 0.07248219847679138 + 10.0 * 6.231408596038818
Epoch 1390, val loss: 0.8530459403991699
Epoch 1400, training loss: 62.38419723510742 = 0.07069042325019836 + 10.0 * 6.231350898742676
Epoch 1400, val loss: 0.8565303087234497
Epoch 1410, training loss: 62.380088806152344 = 0.06895296275615692 + 10.0 * 6.231113433837891
Epoch 1410, val loss: 0.8599172830581665
Epoch 1420, training loss: 62.37755584716797 = 0.06727831065654755 + 10.0 * 6.231027603149414
Epoch 1420, val loss: 0.8633856177330017
Epoch 1430, training loss: 62.35954666137695 = 0.0656503215432167 + 10.0 * 6.229389667510986
Epoch 1430, val loss: 0.8667542338371277
Epoch 1440, training loss: 62.35971450805664 = 0.06408340483903885 + 10.0 * 6.229563236236572
Epoch 1440, val loss: 0.8701337575912476
Epoch 1450, training loss: 62.40346145629883 = 0.0625663623213768 + 10.0 * 6.2340898513793945
Epoch 1450, val loss: 0.8735687732696533
Epoch 1460, training loss: 62.35214614868164 = 0.06103631854057312 + 10.0 * 6.229111194610596
Epoch 1460, val loss: 0.8769353628158569
Epoch 1470, training loss: 62.357975006103516 = 0.05960134416818619 + 10.0 * 6.229837417602539
Epoch 1470, val loss: 0.8804377913475037
Epoch 1480, training loss: 62.34471130371094 = 0.058205753564834595 + 10.0 * 6.2286505699157715
Epoch 1480, val loss: 0.8838538527488708
Epoch 1490, training loss: 62.33639907836914 = 0.05684400349855423 + 10.0 * 6.2279558181762695
Epoch 1490, val loss: 0.8875938653945923
Epoch 1500, training loss: 62.343101501464844 = 0.055547747761011124 + 10.0 * 6.228754997253418
Epoch 1500, val loss: 0.8910712599754333
Epoch 1510, training loss: 62.326297760009766 = 0.05426798760890961 + 10.0 * 6.227202892303467
Epoch 1510, val loss: 0.8945899605751038
Epoch 1520, training loss: 62.322837829589844 = 0.053034599870443344 + 10.0 * 6.226980209350586
Epoch 1520, val loss: 0.8981791734695435
Epoch 1530, training loss: 62.36075210571289 = 0.05185473710298538 + 10.0 * 6.230889797210693
Epoch 1530, val loss: 0.9018160700798035
Epoch 1540, training loss: 62.32942199707031 = 0.05066671594977379 + 10.0 * 6.227875709533691
Epoch 1540, val loss: 0.9051748514175415
Epoch 1550, training loss: 62.311309814453125 = 0.04953717440366745 + 10.0 * 6.226177215576172
Epoch 1550, val loss: 0.9086983799934387
Epoch 1560, training loss: 62.30215072631836 = 0.048456642776727676 + 10.0 * 6.225369453430176
Epoch 1560, val loss: 0.91200852394104
Epoch 1570, training loss: 62.29697799682617 = 0.04740964248776436 + 10.0 * 6.22495698928833
Epoch 1570, val loss: 0.9157312512397766
Epoch 1580, training loss: 62.33628845214844 = 0.04639984294772148 + 10.0 * 6.228989124298096
Epoch 1580, val loss: 0.9190101623535156
Epoch 1590, training loss: 62.323482513427734 = 0.045385345816612244 + 10.0 * 6.227809906005859
Epoch 1590, val loss: 0.9230210185050964
Epoch 1600, training loss: 62.30934524536133 = 0.04442662000656128 + 10.0 * 6.226491928100586
Epoch 1600, val loss: 0.9261031150817871
Epoch 1610, training loss: 62.280696868896484 = 0.04347671568393707 + 10.0 * 6.223721981048584
Epoch 1610, val loss: 0.9297203421592712
Epoch 1620, training loss: 62.27479934692383 = 0.04257742688059807 + 10.0 * 6.223222255706787
Epoch 1620, val loss: 0.933199405670166
Epoch 1630, training loss: 62.276832580566406 = 0.041701674461364746 + 10.0 * 6.223513126373291
Epoch 1630, val loss: 0.9366949200630188
Epoch 1640, training loss: 62.30015563964844 = 0.04084697738289833 + 10.0 * 6.225930690765381
Epoch 1640, val loss: 0.9400803446769714
Epoch 1650, training loss: 62.2799072265625 = 0.04001666605472565 + 10.0 * 6.223989009857178
Epoch 1650, val loss: 0.9436838030815125
Epoch 1660, training loss: 62.29624938964844 = 0.039207134395837784 + 10.0 * 6.225704193115234
Epoch 1660, val loss: 0.947139322757721
Epoch 1670, training loss: 62.267578125 = 0.03842148929834366 + 10.0 * 6.2229156494140625
Epoch 1670, val loss: 0.9503742456436157
Epoch 1680, training loss: 62.2642936706543 = 0.03765849769115448 + 10.0 * 6.222663402557373
Epoch 1680, val loss: 0.9538306593894958
Epoch 1690, training loss: 62.29497528076172 = 0.0369243323802948 + 10.0 * 6.225805282592773
Epoch 1690, val loss: 0.957145094871521
Epoch 1700, training loss: 62.259849548339844 = 0.03618747368454933 + 10.0 * 6.2223663330078125
Epoch 1700, val loss: 0.9606437683105469
Epoch 1710, training loss: 62.28227233886719 = 0.035489343106746674 + 10.0 * 6.2246785163879395
Epoch 1710, val loss: 0.9635458588600159
Epoch 1720, training loss: 62.24515151977539 = 0.03480321541428566 + 10.0 * 6.221035003662109
Epoch 1720, val loss: 0.96724933385849
Epoch 1730, training loss: 62.24009323120117 = 0.03414716571569443 + 10.0 * 6.220594882965088
Epoch 1730, val loss: 0.9704884886741638
Epoch 1740, training loss: 62.234519958496094 = 0.03350725769996643 + 10.0 * 6.220101356506348
Epoch 1740, val loss: 0.9738996624946594
Epoch 1750, training loss: 62.25053787231445 = 0.03289235755801201 + 10.0 * 6.22176456451416
Epoch 1750, val loss: 0.9772082567214966
Epoch 1760, training loss: 62.257423400878906 = 0.032277289777994156 + 10.0 * 6.222514629364014
Epoch 1760, val loss: 0.9805639386177063
Epoch 1770, training loss: 62.22486877441406 = 0.031664907932281494 + 10.0 * 6.219320297241211
Epoch 1770, val loss: 0.9836442470550537
Epoch 1780, training loss: 62.22553253173828 = 0.031083716079592705 + 10.0 * 6.21944522857666
Epoch 1780, val loss: 0.9868931770324707
Epoch 1790, training loss: 62.23323059082031 = 0.030527502298355103 + 10.0 * 6.220270156860352
Epoch 1790, val loss: 0.9902251958847046
Epoch 1800, training loss: 62.24099349975586 = 0.029976677149534225 + 10.0 * 6.221101760864258
Epoch 1800, val loss: 0.9933983683586121
Epoch 1810, training loss: 62.23152542114258 = 0.029443824663758278 + 10.0 * 6.220208168029785
Epoch 1810, val loss: 0.9965334534645081
Epoch 1820, training loss: 62.22562789916992 = 0.02891935035586357 + 10.0 * 6.21967077255249
Epoch 1820, val loss: 0.9999603033065796
Epoch 1830, training loss: 62.21226119995117 = 0.028414184227585793 + 10.0 * 6.218384742736816
Epoch 1830, val loss: 1.0030814409255981
Epoch 1840, training loss: 62.21135711669922 = 0.027920836582779884 + 10.0 * 6.218343734741211
Epoch 1840, val loss: 1.0063046216964722
Epoch 1850, training loss: 62.239471435546875 = 0.027440369129180908 + 10.0 * 6.221203327178955
Epoch 1850, val loss: 1.0095202922821045
Epoch 1860, training loss: 62.23247146606445 = 0.026961084455251694 + 10.0 * 6.220551013946533
Epoch 1860, val loss: 1.0128389596939087
Epoch 1870, training loss: 62.206031799316406 = 0.026504063978791237 + 10.0 * 6.217952728271484
Epoch 1870, val loss: 1.0156636238098145
Epoch 1880, training loss: 62.234413146972656 = 0.02606036514043808 + 10.0 * 6.220835208892822
Epoch 1880, val loss: 1.0191181898117065
Epoch 1890, training loss: 62.19416046142578 = 0.025616800412535667 + 10.0 * 6.216854572296143
Epoch 1890, val loss: 1.0219178199768066
Epoch 1900, training loss: 62.18739318847656 = 0.025191806256771088 + 10.0 * 6.216219902038574
Epoch 1900, val loss: 1.0250146389007568
Epoch 1910, training loss: 62.18926239013672 = 0.02478325180709362 + 10.0 * 6.216447830200195
Epoch 1910, val loss: 1.0282453298568726
Epoch 1920, training loss: 62.194942474365234 = 0.02438202127814293 + 10.0 * 6.217055797576904
Epoch 1920, val loss: 1.03122878074646
Epoch 1930, training loss: 62.21329879760742 = 0.023983247578144073 + 10.0 * 6.218931674957275
Epoch 1930, val loss: 1.034265160560608
Epoch 1940, training loss: 62.21689224243164 = 0.023589447140693665 + 10.0 * 6.219330310821533
Epoch 1940, val loss: 1.0372567176818848
Epoch 1950, training loss: 62.177791595458984 = 0.023210827261209488 + 10.0 * 6.215457916259766
Epoch 1950, val loss: 1.040134072303772
Epoch 1960, training loss: 62.173038482666016 = 0.022842591628432274 + 10.0 * 6.215019702911377
Epoch 1960, val loss: 1.0431420803070068
Epoch 1970, training loss: 62.17252731323242 = 0.022488221526145935 + 10.0 * 6.215003967285156
Epoch 1970, val loss: 1.0462554693222046
Epoch 1980, training loss: 62.20195388793945 = 0.02214459329843521 + 10.0 * 6.217980861663818
Epoch 1980, val loss: 1.049185037612915
Epoch 1990, training loss: 62.16999435424805 = 0.02179373800754547 + 10.0 * 6.21481990814209
Epoch 1990, val loss: 1.0520074367523193
Epoch 2000, training loss: 62.18241500854492 = 0.02146471105515957 + 10.0 * 6.216094970703125
Epoch 2000, val loss: 1.0548121929168701
Epoch 2010, training loss: 62.18118667602539 = 0.02113207057118416 + 10.0 * 6.216005325317383
Epoch 2010, val loss: 1.0577596426010132
Epoch 2020, training loss: 62.17193603515625 = 0.020808730274438858 + 10.0 * 6.215112686157227
Epoch 2020, val loss: 1.0605859756469727
Epoch 2030, training loss: 62.16938400268555 = 0.020496321842074394 + 10.0 * 6.214888572692871
Epoch 2030, val loss: 1.0632901191711426
Epoch 2040, training loss: 62.17949295043945 = 0.020189639180898666 + 10.0 * 6.215929985046387
Epoch 2040, val loss: 1.0661771297454834
Epoch 2050, training loss: 62.15883255004883 = 0.019890200346708298 + 10.0 * 6.213894367218018
Epoch 2050, val loss: 1.0692250728607178
Epoch 2060, training loss: 62.15282440185547 = 0.01959824189543724 + 10.0 * 6.213322639465332
Epoch 2060, val loss: 1.0720186233520508
Epoch 2070, training loss: 62.16812515258789 = 0.019313190132379532 + 10.0 * 6.21488094329834
Epoch 2070, val loss: 1.0747919082641602
Epoch 2080, training loss: 62.16421127319336 = 0.019035661593079567 + 10.0 * 6.214517593383789
Epoch 2080, val loss: 1.077244758605957
Epoch 2090, training loss: 62.15828323364258 = 0.01876307837665081 + 10.0 * 6.21395206451416
Epoch 2090, val loss: 1.0799915790557861
Epoch 2100, training loss: 62.183414459228516 = 0.018492843955755234 + 10.0 * 6.216492176055908
Epoch 2100, val loss: 1.0828295946121216
Epoch 2110, training loss: 62.14209747314453 = 0.018226196989417076 + 10.0 * 6.2123870849609375
Epoch 2110, val loss: 1.085435390472412
Epoch 2120, training loss: 62.14042663574219 = 0.017968956381082535 + 10.0 * 6.212245941162109
Epoch 2120, val loss: 1.0882717370986938
Epoch 2130, training loss: 62.133460998535156 = 0.017722753807902336 + 10.0 * 6.211573600769043
Epoch 2130, val loss: 1.0908321142196655
Epoch 2140, training loss: 62.139312744140625 = 0.017481239512562752 + 10.0 * 6.212182998657227
Epoch 2140, val loss: 1.0935816764831543
Epoch 2150, training loss: 62.205055236816406 = 0.017246631905436516 + 10.0 * 6.218780994415283
Epoch 2150, val loss: 1.0964385271072388
Epoch 2160, training loss: 62.165863037109375 = 0.01699703000485897 + 10.0 * 6.214886665344238
Epoch 2160, val loss: 1.0987402200698853
Epoch 2170, training loss: 62.12871551513672 = 0.01676083169877529 + 10.0 * 6.211195468902588
Epoch 2170, val loss: 1.1010812520980835
Epoch 2180, training loss: 62.122127532958984 = 0.016537146642804146 + 10.0 * 6.210558891296387
Epoch 2180, val loss: 1.103695034980774
Epoch 2190, training loss: 62.12161636352539 = 0.01631900668144226 + 10.0 * 6.210529804229736
Epoch 2190, val loss: 1.106412410736084
Epoch 2200, training loss: 62.14432144165039 = 0.01610507443547249 + 10.0 * 6.2128214836120605
Epoch 2200, val loss: 1.1089951992034912
Epoch 2210, training loss: 62.14759826660156 = 0.01589009165763855 + 10.0 * 6.213171005249023
Epoch 2210, val loss: 1.111586332321167
Epoch 2220, training loss: 62.122554779052734 = 0.015677398070693016 + 10.0 * 6.210687637329102
Epoch 2220, val loss: 1.113824486732483
Epoch 2230, training loss: 62.122291564941406 = 0.015473464503884315 + 10.0 * 6.210681915283203
Epoch 2230, val loss: 1.1163325309753418
Epoch 2240, training loss: 62.15343475341797 = 0.0152761684730649 + 10.0 * 6.213815689086914
Epoch 2240, val loss: 1.1187165975570679
Epoch 2250, training loss: 62.11001205444336 = 0.015076848678290844 + 10.0 * 6.209493637084961
Epoch 2250, val loss: 1.1214560270309448
Epoch 2260, training loss: 62.108768463134766 = 0.01488587073981762 + 10.0 * 6.209388256072998
Epoch 2260, val loss: 1.123856782913208
Epoch 2270, training loss: 62.10732650756836 = 0.014699828810989857 + 10.0 * 6.209262847900391
Epoch 2270, val loss: 1.1262667179107666
Epoch 2280, training loss: 62.1171760559082 = 0.01451665535569191 + 10.0 * 6.21026611328125
Epoch 2280, val loss: 1.1288199424743652
Epoch 2290, training loss: 62.15788269042969 = 0.014335264451801777 + 10.0 * 6.214354515075684
Epoch 2290, val loss: 1.1310431957244873
Epoch 2300, training loss: 62.1086540222168 = 0.01415332406759262 + 10.0 * 6.2094502449035645
Epoch 2300, val loss: 1.1333255767822266
Epoch 2310, training loss: 62.10405731201172 = 0.013977295719087124 + 10.0 * 6.20900821685791
Epoch 2310, val loss: 1.1356080770492554
Epoch 2320, training loss: 62.10041427612305 = 0.013806979171931744 + 10.0 * 6.208661079406738
Epoch 2320, val loss: 1.1382033824920654
Epoch 2330, training loss: 62.10331344604492 = 0.013645323924720287 + 10.0 * 6.2089667320251465
Epoch 2330, val loss: 1.1405729055404663
Epoch 2340, training loss: 62.14059066772461 = 0.013483718037605286 + 10.0 * 6.212710380554199
Epoch 2340, val loss: 1.1430964469909668
Epoch 2350, training loss: 62.11696243286133 = 0.013314846903085709 + 10.0 * 6.210364818572998
Epoch 2350, val loss: 1.145325779914856
Epoch 2360, training loss: 62.13424301147461 = 0.013159049674868584 + 10.0 * 6.212108135223389
Epoch 2360, val loss: 1.147708773612976
Epoch 2370, training loss: 62.10240936279297 = 0.012998813763260841 + 10.0 * 6.2089409828186035
Epoch 2370, val loss: 1.1493432521820068
Epoch 2380, training loss: 62.09197998046875 = 0.012847105041146278 + 10.0 * 6.207913398742676
Epoch 2380, val loss: 1.1519336700439453
Epoch 2390, training loss: 62.116939544677734 = 0.012700527906417847 + 10.0 * 6.210423946380615
Epoch 2390, val loss: 1.1540800333023071
Epoch 2400, training loss: 62.08783721923828 = 0.012550434097647667 + 10.0 * 6.207528591156006
Epoch 2400, val loss: 1.156412959098816
Epoch 2410, training loss: 62.12114715576172 = 0.012405550107359886 + 10.0 * 6.210874080657959
Epoch 2410, val loss: 1.1587436199188232
Epoch 2420, training loss: 62.08561706542969 = 0.01226210780441761 + 10.0 * 6.207335472106934
Epoch 2420, val loss: 1.1605302095413208
Epoch 2430, training loss: 62.08290100097656 = 0.012122816406190395 + 10.0 * 6.207077980041504
Epoch 2430, val loss: 1.1628010272979736
Epoch 2440, training loss: 62.083316802978516 = 0.011987737379968166 + 10.0 * 6.207132816314697
Epoch 2440, val loss: 1.16507089138031
Epoch 2450, training loss: 62.12600326538086 = 0.011855313554406166 + 10.0 * 6.211414813995361
Epoch 2450, val loss: 1.1671611070632935
Epoch 2460, training loss: 62.08629608154297 = 0.011719702742993832 + 10.0 * 6.207457542419434
Epoch 2460, val loss: 1.1690597534179688
Epoch 2470, training loss: 62.077903747558594 = 0.011587953194975853 + 10.0 * 6.206631660461426
Epoch 2470, val loss: 1.1712043285369873
Epoch 2480, training loss: 62.08051300048828 = 0.011462047696113586 + 10.0 * 6.206904888153076
Epoch 2480, val loss: 1.173462986946106
Epoch 2490, training loss: 62.107872009277344 = 0.011338378302752972 + 10.0 * 6.209653377532959
Epoch 2490, val loss: 1.1752973794937134
Epoch 2500, training loss: 62.094329833984375 = 0.01121506467461586 + 10.0 * 6.208311557769775
Epoch 2500, val loss: 1.1773828268051147
Epoch 2510, training loss: 62.089149475097656 = 0.011091751977801323 + 10.0 * 6.207805633544922
Epoch 2510, val loss: 1.1798765659332275
Epoch 2520, training loss: 62.07816696166992 = 0.01097352709621191 + 10.0 * 6.206719398498535
Epoch 2520, val loss: 1.181626558303833
Epoch 2530, training loss: 62.06483840942383 = 0.010854765772819519 + 10.0 * 6.205398082733154
Epoch 2530, val loss: 1.183574914932251
Epoch 2540, training loss: 62.09565353393555 = 0.010744121856987476 + 10.0 * 6.20849084854126
Epoch 2540, val loss: 1.185624122619629
Epoch 2550, training loss: 62.08906936645508 = 0.010630432516336441 + 10.0 * 6.207843780517578
Epoch 2550, val loss: 1.1878135204315186
Epoch 2560, training loss: 62.06517028808594 = 0.0105125792324543 + 10.0 * 6.205465793609619
Epoch 2560, val loss: 1.189412236213684
Epoch 2570, training loss: 62.058021545410156 = 0.01040413323789835 + 10.0 * 6.204761981964111
Epoch 2570, val loss: 1.1914803981781006
Epoch 2580, training loss: 62.057960510253906 = 0.010299335233867168 + 10.0 * 6.204766273498535
Epoch 2580, val loss: 1.1935418844223022
Epoch 2590, training loss: 62.10390090942383 = 0.010195515118539333 + 10.0 * 6.2093706130981445
Epoch 2590, val loss: 1.1957813501358032
Epoch 2600, training loss: 62.07901382446289 = 0.010088483802974224 + 10.0 * 6.206892490386963
Epoch 2600, val loss: 1.1972891092300415
Epoch 2610, training loss: 62.06922912597656 = 0.009984864853322506 + 10.0 * 6.2059245109558105
Epoch 2610, val loss: 1.1993541717529297
Epoch 2620, training loss: 62.052181243896484 = 0.009883657097816467 + 10.0 * 6.204229831695557
Epoch 2620, val loss: 1.2010822296142578
Epoch 2630, training loss: 62.05168533325195 = 0.009785939939320087 + 10.0 * 6.204190254211426
Epoch 2630, val loss: 1.2031257152557373
Epoch 2640, training loss: 62.12192153930664 = 0.009691078215837479 + 10.0 * 6.211223125457764
Epoch 2640, val loss: 1.2047804594039917
Epoch 2650, training loss: 62.06864929199219 = 0.009593992494046688 + 10.0 * 6.205905437469482
Epoch 2650, val loss: 1.2067509889602661
Epoch 2660, training loss: 62.051795959472656 = 0.009496781975030899 + 10.0 * 6.204229831695557
Epoch 2660, val loss: 1.2085741758346558
Epoch 2670, training loss: 62.05885696411133 = 0.009406091645359993 + 10.0 * 6.204945087432861
Epoch 2670, val loss: 1.2105598449707031
Epoch 2680, training loss: 62.07228469848633 = 0.009315384551882744 + 10.0 * 6.206296920776367
Epoch 2680, val loss: 1.2124767303466797
Epoch 2690, training loss: 62.05500793457031 = 0.009224185720086098 + 10.0 * 6.204578399658203
Epoch 2690, val loss: 1.2139698266983032
Epoch 2700, training loss: 62.064476013183594 = 0.009136900305747986 + 10.0 * 6.205533981323242
Epoch 2700, val loss: 1.2161266803741455
Epoch 2710, training loss: 62.04140090942383 = 0.009047812782227993 + 10.0 * 6.203235149383545
Epoch 2710, val loss: 1.217537760734558
Epoch 2720, training loss: 62.04199981689453 = 0.00896220002323389 + 10.0 * 6.203303813934326
Epoch 2720, val loss: 1.2190507650375366
Epoch 2730, training loss: 62.080501556396484 = 0.008877366781234741 + 10.0 * 6.207162380218506
Epoch 2730, val loss: 1.2210969924926758
Epoch 2740, training loss: 62.03617477416992 = 0.008794448338449001 + 10.0 * 6.202738285064697
Epoch 2740, val loss: 1.222489356994629
Epoch 2750, training loss: 62.0308723449707 = 0.00871056504547596 + 10.0 * 6.202216148376465
Epoch 2750, val loss: 1.224345088005066
Epoch 2760, training loss: 62.02790069580078 = 0.008631419390439987 + 10.0 * 6.2019267082214355
Epoch 2760, val loss: 1.225882887840271
Epoch 2770, training loss: 62.03752517700195 = 0.008555145934224129 + 10.0 * 6.202897071838379
Epoch 2770, val loss: 1.227600336074829
Epoch 2780, training loss: 62.08220672607422 = 0.008476736955344677 + 10.0 * 6.207373142242432
Epoch 2780, val loss: 1.2292753458023071
Epoch 2790, training loss: 62.05112075805664 = 0.00839558057487011 + 10.0 * 6.204272270202637
Epoch 2790, val loss: 1.2308741807937622
Epoch 2800, training loss: 62.027347564697266 = 0.008319679647684097 + 10.0 * 6.201902866363525
Epoch 2800, val loss: 1.232293963432312
Epoch 2810, training loss: 62.02354431152344 = 0.00824612658470869 + 10.0 * 6.201529502868652
Epoch 2810, val loss: 1.233910083770752
Epoch 2820, training loss: 62.02307891845703 = 0.008173703216016293 + 10.0 * 6.20149040222168
Epoch 2820, val loss: 1.2357282638549805
Epoch 2830, training loss: 62.093387603759766 = 0.008103045634925365 + 10.0 * 6.208528518676758
Epoch 2830, val loss: 1.23737633228302
Epoch 2840, training loss: 62.04682540893555 = 0.00802933145314455 + 10.0 * 6.203879356384277
Epoch 2840, val loss: 1.2382732629776
Epoch 2850, training loss: 62.03596115112305 = 0.007955852895975113 + 10.0 * 6.202800273895264
Epoch 2850, val loss: 1.2400041818618774
Epoch 2860, training loss: 62.03810501098633 = 0.007886919192969799 + 10.0 * 6.20302152633667
Epoch 2860, val loss: 1.2413777112960815
Epoch 2870, training loss: 62.026153564453125 = 0.007818086072802544 + 10.0 * 6.201833724975586
Epoch 2870, val loss: 1.243146300315857
Epoch 2880, training loss: 62.024864196777344 = 0.0077521116472780704 + 10.0 * 6.201711177825928
Epoch 2880, val loss: 1.244908332824707
Epoch 2890, training loss: 62.04228591918945 = 0.007687143981456757 + 10.0 * 6.203459739685059
Epoch 2890, val loss: 1.246587872505188
Epoch 2900, training loss: 62.02096176147461 = 0.00762099027633667 + 10.0 * 6.201333999633789
Epoch 2900, val loss: 1.2474963665008545
Epoch 2910, training loss: 62.01744842529297 = 0.007556275464594364 + 10.0 * 6.200989246368408
Epoch 2910, val loss: 1.249192237854004
Epoch 2920, training loss: 62.066139221191406 = 0.007493790239095688 + 10.0 * 6.205864906311035
Epoch 2920, val loss: 1.2507258653640747
Epoch 2930, training loss: 62.034706115722656 = 0.007431219797581434 + 10.0 * 6.202727317810059
Epoch 2930, val loss: 1.2517505884170532
Epoch 2940, training loss: 62.031959533691406 = 0.007367366459220648 + 10.0 * 6.202459335327148
Epoch 2940, val loss: 1.2534688711166382
Epoch 2950, training loss: 62.03498077392578 = 0.0073072477243840694 + 10.0 * 6.202767372131348
Epoch 2950, val loss: 1.2544230222702026
Epoch 2960, training loss: 62.013423919677734 = 0.007244050968438387 + 10.0 * 6.200617790222168
Epoch 2960, val loss: 1.255897879600525
Epoch 2970, training loss: 62.009254455566406 = 0.007185020018368959 + 10.0 * 6.200206756591797
Epoch 2970, val loss: 1.25722336769104
Epoch 2980, training loss: 62.00939178466797 = 0.007127594202756882 + 10.0 * 6.200226783752441
Epoch 2980, val loss: 1.2588844299316406
Epoch 2990, training loss: 62.045780181884766 = 0.007072536274790764 + 10.0 * 6.20387077331543
Epoch 2990, val loss: 1.2601783275604248
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 87.91350555419922 = 1.9453368186950684 + 10.0 * 8.596817016601562
Epoch 0, val loss: 1.9474595785140991
Epoch 10, training loss: 87.89554595947266 = 1.935802698135376 + 10.0 * 8.59597396850586
Epoch 10, val loss: 1.938258171081543
Epoch 20, training loss: 87.82508850097656 = 1.9245837926864624 + 10.0 * 8.59005069732666
Epoch 20, val loss: 1.926864504814148
Epoch 30, training loss: 87.37237548828125 = 1.9109584093093872 + 10.0 * 8.546141624450684
Epoch 30, val loss: 1.9125843048095703
Epoch 40, training loss: 83.84365844726562 = 1.895290493965149 + 10.0 * 8.194836616516113
Epoch 40, val loss: 1.8961522579193115
Epoch 50, training loss: 76.04974365234375 = 1.8779280185699463 + 10.0 * 7.417181015014648
Epoch 50, val loss: 1.8783963918685913
Epoch 60, training loss: 73.42806243896484 = 1.866014838218689 + 10.0 * 7.156205177307129
Epoch 60, val loss: 1.8670685291290283
Epoch 70, training loss: 71.88166046142578 = 1.8563646078109741 + 10.0 * 7.002530097961426
Epoch 70, val loss: 1.8574762344360352
Epoch 80, training loss: 70.6534652709961 = 1.8465205430984497 + 10.0 * 6.88069486618042
Epoch 80, val loss: 1.8480172157287598
Epoch 90, training loss: 69.6593246459961 = 1.8379292488098145 + 10.0 * 6.782139778137207
Epoch 90, val loss: 1.8394694328308105
Epoch 100, training loss: 68.95426940917969 = 1.8297231197357178 + 10.0 * 6.712454795837402
Epoch 100, val loss: 1.8311192989349365
Epoch 110, training loss: 68.4319076538086 = 1.8217368125915527 + 10.0 * 6.661016464233398
Epoch 110, val loss: 1.8226863145828247
Epoch 120, training loss: 68.03041076660156 = 1.8139961957931519 + 10.0 * 6.621641635894775
Epoch 120, val loss: 1.8146475553512573
Epoch 130, training loss: 67.68263244628906 = 1.80648934841156 + 10.0 * 6.587614059448242
Epoch 130, val loss: 1.8070037364959717
Epoch 140, training loss: 67.41030883789062 = 1.7993179559707642 + 10.0 * 6.561099052429199
Epoch 140, val loss: 1.7998273372650146
Epoch 150, training loss: 67.14606475830078 = 1.792136788368225 + 10.0 * 6.535392761230469
Epoch 150, val loss: 1.7925726175308228
Epoch 160, training loss: 66.91268157958984 = 1.784770131111145 + 10.0 * 6.512791633605957
Epoch 160, val loss: 1.7852880954742432
Epoch 170, training loss: 66.732421875 = 1.7772499322891235 + 10.0 * 6.495516777038574
Epoch 170, val loss: 1.7781591415405273
Epoch 180, training loss: 66.56974029541016 = 1.769263744354248 + 10.0 * 6.480047225952148
Epoch 180, val loss: 1.7708803415298462
Epoch 190, training loss: 66.406005859375 = 1.7607728242874146 + 10.0 * 6.4645233154296875
Epoch 190, val loss: 1.7631515264511108
Epoch 200, training loss: 66.28296661376953 = 1.7516316175460815 + 10.0 * 6.453133583068848
Epoch 200, val loss: 1.7551178932189941
Epoch 210, training loss: 66.16263580322266 = 1.741591453552246 + 10.0 * 6.442104339599609
Epoch 210, val loss: 1.7465001344680786
Epoch 220, training loss: 66.03655242919922 = 1.730797529220581 + 10.0 * 6.430575847625732
Epoch 220, val loss: 1.737236499786377
Epoch 230, training loss: 65.93671417236328 = 1.7190226316452026 + 10.0 * 6.421768665313721
Epoch 230, val loss: 1.7272952795028687
Epoch 240, training loss: 65.84087371826172 = 1.7060470581054688 + 10.0 * 6.413482666015625
Epoch 240, val loss: 1.7165409326553345
Epoch 250, training loss: 65.73444366455078 = 1.6919763088226318 + 10.0 * 6.4042463302612305
Epoch 250, val loss: 1.7049118280410767
Epoch 260, training loss: 65.63192749023438 = 1.6767228841781616 + 10.0 * 6.3955206871032715
Epoch 260, val loss: 1.6923482418060303
Epoch 270, training loss: 65.54452514648438 = 1.6599994897842407 + 10.0 * 6.388452529907227
Epoch 270, val loss: 1.6787033081054688
Epoch 280, training loss: 65.45950317382812 = 1.6418509483337402 + 10.0 * 6.381764888763428
Epoch 280, val loss: 1.6640615463256836
Epoch 290, training loss: 65.37718200683594 = 1.6222538948059082 + 10.0 * 6.375492572784424
Epoch 290, val loss: 1.6483651399612427
Epoch 300, training loss: 65.33702850341797 = 1.6011701822280884 + 10.0 * 6.3735857009887695
Epoch 300, val loss: 1.6315510272979736
Epoch 310, training loss: 65.22724151611328 = 1.57852041721344 + 10.0 * 6.364871978759766
Epoch 310, val loss: 1.6136056184768677
Epoch 320, training loss: 65.1682357788086 = 1.5544888973236084 + 10.0 * 6.361374855041504
Epoch 320, val loss: 1.5946228504180908
Epoch 330, training loss: 65.07965850830078 = 1.5289963483810425 + 10.0 * 6.355066299438477
Epoch 330, val loss: 1.5748825073242188
Epoch 340, training loss: 65.00808715820312 = 1.50225830078125 + 10.0 * 6.350583076477051
Epoch 340, val loss: 1.5543932914733887
Epoch 350, training loss: 64.97476196289062 = 1.4741491079330444 + 10.0 * 6.350060939788818
Epoch 350, val loss: 1.5331213474273682
Epoch 360, training loss: 64.8876953125 = 1.4452248811721802 + 10.0 * 6.344246864318848
Epoch 360, val loss: 1.511292576789856
Epoch 370, training loss: 64.80644989013672 = 1.4154736995697021 + 10.0 * 6.33909797668457
Epoch 370, val loss: 1.4892024993896484
Epoch 380, training loss: 64.75141143798828 = 1.385072112083435 + 10.0 * 6.336634159088135
Epoch 380, val loss: 1.46689772605896
Epoch 390, training loss: 64.69293975830078 = 1.354034662246704 + 10.0 * 6.333890438079834
Epoch 390, val loss: 1.4443175792694092
Epoch 400, training loss: 64.62340545654297 = 1.322618842124939 + 10.0 * 6.330078601837158
Epoch 400, val loss: 1.4217815399169922
Epoch 410, training loss: 64.59176635742188 = 1.290940523147583 + 10.0 * 6.330082416534424
Epoch 410, val loss: 1.399524211883545
Epoch 420, training loss: 64.49855041503906 = 1.2593309879302979 + 10.0 * 6.323922157287598
Epoch 420, val loss: 1.3773013353347778
Epoch 430, training loss: 64.43033599853516 = 1.2279021739959717 + 10.0 * 6.3202433586120605
Epoch 430, val loss: 1.3556358814239502
Epoch 440, training loss: 64.37818145751953 = 1.1967110633850098 + 10.0 * 6.3181471824646
Epoch 440, val loss: 1.3345069885253906
Epoch 450, training loss: 64.3394546508789 = 1.165816068649292 + 10.0 * 6.317363739013672
Epoch 450, val loss: 1.3135919570922852
Epoch 460, training loss: 64.26080322265625 = 1.1352521181106567 + 10.0 * 6.312555313110352
Epoch 460, val loss: 1.293532371520996
Epoch 470, training loss: 64.20486450195312 = 1.1056002378463745 + 10.0 * 6.309926509857178
Epoch 470, val loss: 1.2744829654693604
Epoch 480, training loss: 64.1587142944336 = 1.0766303539276123 + 10.0 * 6.308208465576172
Epoch 480, val loss: 1.256247878074646
Epoch 490, training loss: 64.14141845703125 = 1.048304796218872 + 10.0 * 6.309311866760254
Epoch 490, val loss: 1.2385804653167725
Epoch 500, training loss: 64.07012939453125 = 1.0204838514328003 + 10.0 * 6.304964542388916
Epoch 500, val loss: 1.2217637300491333
Epoch 510, training loss: 64.01146697998047 = 0.9935572147369385 + 10.0 * 6.301790714263916
Epoch 510, val loss: 1.2061299085617065
Epoch 520, training loss: 63.988338470458984 = 0.9675912261009216 + 10.0 * 6.302074909210205
Epoch 520, val loss: 1.191344976425171
Epoch 530, training loss: 63.9372444152832 = 0.9424600005149841 + 10.0 * 6.299478530883789
Epoch 530, val loss: 1.1773037910461426
Epoch 540, training loss: 63.8908805847168 = 0.9179880619049072 + 10.0 * 6.2972893714904785
Epoch 540, val loss: 1.164194107055664
Epoch 550, training loss: 63.84117889404297 = 0.8945083022117615 + 10.0 * 6.2946672439575195
Epoch 550, val loss: 1.1519709825515747
Epoch 560, training loss: 63.805824279785156 = 0.871728777885437 + 10.0 * 6.293409824371338
Epoch 560, val loss: 1.1406595706939697
Epoch 570, training loss: 63.772682189941406 = 0.8497040271759033 + 10.0 * 6.292297840118408
Epoch 570, val loss: 1.1300678253173828
Epoch 580, training loss: 63.739349365234375 = 0.8282834887504578 + 10.0 * 6.291106224060059
Epoch 580, val loss: 1.120245099067688
Epoch 590, training loss: 63.69139862060547 = 0.8076724410057068 + 10.0 * 6.288372993469238
Epoch 590, val loss: 1.1112817525863647
Epoch 600, training loss: 63.682533264160156 = 0.7876803874969482 + 10.0 * 6.289484977722168
Epoch 600, val loss: 1.1030499935150146
Epoch 610, training loss: 63.617740631103516 = 0.768388032913208 + 10.0 * 6.284935474395752
Epoch 610, val loss: 1.0954684019088745
Epoch 620, training loss: 63.58353805541992 = 0.7497178316116333 + 10.0 * 6.283381938934326
Epoch 620, val loss: 1.0883846282958984
Epoch 630, training loss: 63.59455871582031 = 0.7315974235534668 + 10.0 * 6.2862958908081055
Epoch 630, val loss: 1.082052230834961
Epoch 640, training loss: 63.571556091308594 = 0.7138356566429138 + 10.0 * 6.28577184677124
Epoch 640, val loss: 1.0757837295532227
Epoch 650, training loss: 63.50564956665039 = 0.6967874765396118 + 10.0 * 6.280886173248291
Epoch 650, val loss: 1.0700386762619019
Epoch 660, training loss: 63.48405838012695 = 0.6802229285240173 + 10.0 * 6.280383586883545
Epoch 660, val loss: 1.0651167631149292
Epoch 670, training loss: 63.43914031982422 = 0.6643058657646179 + 10.0 * 6.2774834632873535
Epoch 670, val loss: 1.060423731803894
Epoch 680, training loss: 63.407264709472656 = 0.6487409472465515 + 10.0 * 6.275852203369141
Epoch 680, val loss: 1.0563300848007202
Epoch 690, training loss: 63.382469177246094 = 0.6336365342140198 + 10.0 * 6.274883270263672
Epoch 690, val loss: 1.0526219606399536
Epoch 700, training loss: 63.37877655029297 = 0.6188596487045288 + 10.0 * 6.275991916656494
Epoch 700, val loss: 1.049110770225525
Epoch 710, training loss: 63.373191833496094 = 0.604453444480896 + 10.0 * 6.276873588562012
Epoch 710, val loss: 1.0454752445220947
Epoch 720, training loss: 63.33544158935547 = 0.5904945135116577 + 10.0 * 6.274494647979736
Epoch 720, val loss: 1.0424844026565552
Epoch 730, training loss: 63.286285400390625 = 0.5768298506736755 + 10.0 * 6.2709455490112305
Epoch 730, val loss: 1.0399378538131714
Epoch 740, training loss: 63.25829315185547 = 0.5636706948280334 + 10.0 * 6.2694621086120605
Epoch 740, val loss: 1.0375306606292725
Epoch 750, training loss: 63.232112884521484 = 0.5508796572685242 + 10.0 * 6.268123149871826
Epoch 750, val loss: 1.035446047782898
Epoch 760, training loss: 63.23282241821289 = 0.5383805632591248 + 10.0 * 6.269444465637207
Epoch 760, val loss: 1.0335599184036255
Epoch 770, training loss: 63.210838317871094 = 0.5259614586830139 + 10.0 * 6.268487453460693
Epoch 770, val loss: 1.0318810939788818
Epoch 780, training loss: 63.19779586791992 = 0.5140216946601868 + 10.0 * 6.268377304077148
Epoch 780, val loss: 1.0299874544143677
Epoch 790, training loss: 63.148101806640625 = 0.5022973418235779 + 10.0 * 6.264580726623535
Epoch 790, val loss: 1.0286723375320435
Epoch 800, training loss: 63.12813949584961 = 0.4909256100654602 + 10.0 * 6.263721466064453
Epoch 800, val loss: 1.0274041891098022
Epoch 810, training loss: 63.11567687988281 = 0.47981593012809753 + 10.0 * 6.263586044311523
Epoch 810, val loss: 1.0263055562973022
Epoch 820, training loss: 63.0996208190918 = 0.46881723403930664 + 10.0 * 6.26308012008667
Epoch 820, val loss: 1.0251545906066895
Epoch 830, training loss: 63.09349060058594 = 0.4579955041408539 + 10.0 * 6.263549327850342
Epoch 830, val loss: 1.023970365524292
Epoch 840, training loss: 63.05902099609375 = 0.4474327564239502 + 10.0 * 6.2611589431762695
Epoch 840, val loss: 1.023250699043274
Epoch 850, training loss: 63.033302307128906 = 0.4371040165424347 + 10.0 * 6.25961971282959
Epoch 850, val loss: 1.0224272012710571
Epoch 860, training loss: 63.0172004699707 = 0.42694470286369324 + 10.0 * 6.259025573730469
Epoch 860, val loss: 1.0216314792633057
Epoch 870, training loss: 63.0341796875 = 0.41697123646736145 + 10.0 * 6.261720657348633
Epoch 870, val loss: 1.0209405422210693
Epoch 880, training loss: 63.011810302734375 = 0.40702906250953674 + 10.0 * 6.2604780197143555
Epoch 880, val loss: 1.0203791856765747
Epoch 890, training loss: 62.99531936645508 = 0.3972470462322235 + 10.0 * 6.259807109832764
Epoch 890, val loss: 1.019553303718567
Epoch 900, training loss: 62.9523811340332 = 0.3876321017742157 + 10.0 * 6.25647497177124
Epoch 900, val loss: 1.0191442966461182
Epoch 910, training loss: 62.929710388183594 = 0.3781904876232147 + 10.0 * 6.255152225494385
Epoch 910, val loss: 1.0189006328582764
Epoch 920, training loss: 62.92935562133789 = 0.3689143657684326 + 10.0 * 6.256043910980225
Epoch 920, val loss: 1.0184234380722046
Epoch 930, training loss: 62.90781784057617 = 0.3596343696117401 + 10.0 * 6.254818439483643
Epoch 930, val loss: 1.018100619316101
Epoch 940, training loss: 62.88484573364258 = 0.35048243403434753 + 10.0 * 6.253436088562012
Epoch 940, val loss: 1.017483115196228
Epoch 950, training loss: 62.86916732788086 = 0.34151071310043335 + 10.0 * 6.252765655517578
Epoch 950, val loss: 1.0172709226608276
Epoch 960, training loss: 62.861270904541016 = 0.3326796591281891 + 10.0 * 6.252859115600586
Epoch 960, val loss: 1.0168719291687012
Epoch 970, training loss: 62.84513854980469 = 0.32391276955604553 + 10.0 * 6.252122402191162
Epoch 970, val loss: 1.0167028903961182
Epoch 980, training loss: 62.850555419921875 = 0.3153145909309387 + 10.0 * 6.253523826599121
Epoch 980, val loss: 1.0164384841918945
Epoch 990, training loss: 62.811119079589844 = 0.30677667260169983 + 10.0 * 6.250433921813965
Epoch 990, val loss: 1.0159984827041626
Epoch 1000, training loss: 62.807395935058594 = 0.2984115481376648 + 10.0 * 6.250898361206055
Epoch 1000, val loss: 1.0158724784851074
Epoch 1010, training loss: 62.78293228149414 = 0.29019612073898315 + 10.0 * 6.249273777008057
Epoch 1010, val loss: 1.0155949592590332
Epoch 1020, training loss: 62.77253723144531 = 0.2821483612060547 + 10.0 * 6.249039173126221
Epoch 1020, val loss: 1.0154153108596802
Epoch 1030, training loss: 62.7735481262207 = 0.27420249581336975 + 10.0 * 6.249934196472168
Epoch 1030, val loss: 1.0152424573898315
Epoch 1040, training loss: 62.747406005859375 = 0.2664587199687958 + 10.0 * 6.24809455871582
Epoch 1040, val loss: 1.0151230096817017
Epoch 1050, training loss: 62.751041412353516 = 0.2588312327861786 + 10.0 * 6.249220848083496
Epoch 1050, val loss: 1.015106439590454
Epoch 1060, training loss: 62.70747375488281 = 0.2514016628265381 + 10.0 * 6.245607376098633
Epoch 1060, val loss: 1.015032172203064
Epoch 1070, training loss: 62.69935607910156 = 0.24412663280963898 + 10.0 * 6.245522975921631
Epoch 1070, val loss: 1.015098214149475
Epoch 1080, training loss: 62.70499801635742 = 0.23704828321933746 + 10.0 * 6.246794700622559
Epoch 1080, val loss: 1.015264630317688
Epoch 1090, training loss: 62.69416427612305 = 0.2301185429096222 + 10.0 * 6.246404647827148
Epoch 1090, val loss: 1.0152219533920288
Epoch 1100, training loss: 62.66691589355469 = 0.2233760803937912 + 10.0 * 6.244353771209717
Epoch 1100, val loss: 1.0155304670333862
Epoch 1110, training loss: 62.6472282409668 = 0.21683892607688904 + 10.0 * 6.243039131164551
Epoch 1110, val loss: 1.0161161422729492
Epoch 1120, training loss: 62.63402557373047 = 0.21051639318466187 + 10.0 * 6.2423505783081055
Epoch 1120, val loss: 1.0167585611343384
Epoch 1130, training loss: 62.65803909301758 = 0.20435719192028046 + 10.0 * 6.245368003845215
Epoch 1130, val loss: 1.0176459550857544
Epoch 1140, training loss: 62.61573791503906 = 0.19837145507335663 + 10.0 * 6.24173641204834
Epoch 1140, val loss: 1.0181306600570679
Epoch 1150, training loss: 62.60950469970703 = 0.19253401458263397 + 10.0 * 6.241696834564209
Epoch 1150, val loss: 1.0192228555679321
Epoch 1160, training loss: 62.610801696777344 = 0.1869409680366516 + 10.0 * 6.242386341094971
Epoch 1160, val loss: 1.0202425718307495
Epoch 1170, training loss: 62.59071350097656 = 0.18145941197872162 + 10.0 * 6.2409257888793945
Epoch 1170, val loss: 1.0215036869049072
Epoch 1180, training loss: 62.58106994628906 = 0.1761644035577774 + 10.0 * 6.240490913391113
Epoch 1180, val loss: 1.0226290225982666
Epoch 1190, training loss: 62.583412170410156 = 0.17106451094150543 + 10.0 * 6.24123477935791
Epoch 1190, val loss: 1.024128794670105
Epoch 1200, training loss: 62.56161880493164 = 0.1661243885755539 + 10.0 * 6.239549160003662
Epoch 1200, val loss: 1.0255029201507568
Epoch 1210, training loss: 62.53679275512695 = 0.16133742034435272 + 10.0 * 6.237545490264893
Epoch 1210, val loss: 1.0271276235580444
Epoch 1220, training loss: 62.532073974609375 = 0.1567324697971344 + 10.0 * 6.237534523010254
Epoch 1220, val loss: 1.0289099216461182
Epoch 1230, training loss: 62.56509017944336 = 0.15230339765548706 + 10.0 * 6.241278648376465
Epoch 1230, val loss: 1.030910849571228
Epoch 1240, training loss: 62.564048767089844 = 0.147953599691391 + 10.0 * 6.241609573364258
Epoch 1240, val loss: 1.0326225757598877
Epoch 1250, training loss: 62.51729965209961 = 0.1437586545944214 + 10.0 * 6.237354278564453
Epoch 1250, val loss: 1.0345531702041626
Epoch 1260, training loss: 62.49750900268555 = 0.13974323868751526 + 10.0 * 6.235776424407959
Epoch 1260, val loss: 1.0369188785552979
Epoch 1270, training loss: 62.5343017578125 = 0.13590818643569946 + 10.0 * 6.23983907699585
Epoch 1270, val loss: 1.0391899347305298
Epoch 1280, training loss: 62.49140548706055 = 0.13212047517299652 + 10.0 * 6.235928535461426
Epoch 1280, val loss: 1.041323184967041
Epoch 1290, training loss: 62.47543716430664 = 0.128520205616951 + 10.0 * 6.234691619873047
Epoch 1290, val loss: 1.0438385009765625
Epoch 1300, training loss: 62.531246185302734 = 0.12505269050598145 + 10.0 * 6.24061918258667
Epoch 1300, val loss: 1.0462218523025513
Epoch 1310, training loss: 62.472145080566406 = 0.12162903696298599 + 10.0 * 6.23505163192749
Epoch 1310, val loss: 1.049131155014038
Epoch 1320, training loss: 62.449501037597656 = 0.11838570982217789 + 10.0 * 6.233111381530762
Epoch 1320, val loss: 1.0515276193618774
Epoch 1330, training loss: 62.43735885620117 = 0.11525218188762665 + 10.0 * 6.232210636138916
Epoch 1330, val loss: 1.054473876953125
Epoch 1340, training loss: 62.494850158691406 = 0.11224180459976196 + 10.0 * 6.238260746002197
Epoch 1340, val loss: 1.0574249029159546
Epoch 1350, training loss: 62.467063903808594 = 0.10927996039390564 + 10.0 * 6.235778331756592
Epoch 1350, val loss: 1.0597522258758545
Epoch 1360, training loss: 62.45018768310547 = 0.10639568418264389 + 10.0 * 6.234379291534424
Epoch 1360, val loss: 1.0626060962677002
Epoch 1370, training loss: 62.41877746582031 = 0.10366179049015045 + 10.0 * 6.23151159286499
Epoch 1370, val loss: 1.0657598972320557
Epoch 1380, training loss: 62.41242599487305 = 0.1010320857167244 + 10.0 * 6.231139183044434
Epoch 1380, val loss: 1.068880319595337
Epoch 1390, training loss: 62.398014068603516 = 0.09847182035446167 + 10.0 * 6.229954242706299
Epoch 1390, val loss: 1.0719313621520996
Epoch 1400, training loss: 62.43293380737305 = 0.09600810706615448 + 10.0 * 6.233692646026611
Epoch 1400, val loss: 1.0750459432601929
Epoch 1410, training loss: 62.402244567871094 = 0.09360373020172119 + 10.0 * 6.23086404800415
Epoch 1410, val loss: 1.078408122062683
Epoch 1420, training loss: 62.397544860839844 = 0.09125251322984695 + 10.0 * 6.2306294441223145
Epoch 1420, val loss: 1.0812647342681885
Epoch 1430, training loss: 62.38677215576172 = 0.08900724351406097 + 10.0 * 6.229776382446289
Epoch 1430, val loss: 1.0848288536071777
Epoch 1440, training loss: 62.38936233520508 = 0.08685321360826492 + 10.0 * 6.230250835418701
Epoch 1440, val loss: 1.0881298780441284
Epoch 1450, training loss: 62.39370346069336 = 0.08476508408784866 + 10.0 * 6.230893611907959
Epoch 1450, val loss: 1.0916168689727783
Epoch 1460, training loss: 62.38716506958008 = 0.08271244913339615 + 10.0 * 6.23044490814209
Epoch 1460, val loss: 1.0952565670013428
Epoch 1470, training loss: 62.39503860473633 = 0.08073444664478302 + 10.0 * 6.231430530548096
Epoch 1470, val loss: 1.098288655281067
Epoch 1480, training loss: 62.35612869262695 = 0.07884815335273743 + 10.0 * 6.227727890014648
Epoch 1480, val loss: 1.102019190788269
Epoch 1490, training loss: 62.34891891479492 = 0.07701826840639114 + 10.0 * 6.227190017700195
Epoch 1490, val loss: 1.1053847074508667
Epoch 1500, training loss: 62.344112396240234 = 0.07524415105581284 + 10.0 * 6.226886749267578
Epoch 1500, val loss: 1.1089364290237427
Epoch 1510, training loss: 62.38435745239258 = 0.0735292136669159 + 10.0 * 6.231082916259766
Epoch 1510, val loss: 1.1124517917633057
Epoch 1520, training loss: 62.34876251220703 = 0.07184504717588425 + 10.0 * 6.227691650390625
Epoch 1520, val loss: 1.1158902645111084
Epoch 1530, training loss: 62.35392761230469 = 0.07022130489349365 + 10.0 * 6.228370666503906
Epoch 1530, val loss: 1.1194218397140503
Epoch 1540, training loss: 62.35951232910156 = 0.06865638494491577 + 10.0 * 6.229085445404053
Epoch 1540, val loss: 1.1229040622711182
Epoch 1550, training loss: 62.326900482177734 = 0.06711024045944214 + 10.0 * 6.225978851318359
Epoch 1550, val loss: 1.1263558864593506
Epoch 1560, training loss: 62.315162658691406 = 0.0656343474984169 + 10.0 * 6.224952697753906
Epoch 1560, val loss: 1.1300785541534424
Epoch 1570, training loss: 62.31097412109375 = 0.06421647220849991 + 10.0 * 6.22467565536499
Epoch 1570, val loss: 1.1336637735366821
Epoch 1580, training loss: 62.32484436035156 = 0.06284168362617493 + 10.0 * 6.226200103759766
Epoch 1580, val loss: 1.137254238128662
Epoch 1590, training loss: 62.315589904785156 = 0.06148988753557205 + 10.0 * 6.225409984588623
Epoch 1590, val loss: 1.140791654586792
Epoch 1600, training loss: 62.319087982177734 = 0.06018519774079323 + 10.0 * 6.225890159606934
Epoch 1600, val loss: 1.1444944143295288
Epoch 1610, training loss: 62.30595397949219 = 0.0589090958237648 + 10.0 * 6.224704265594482
Epoch 1610, val loss: 1.1479108333587646
Epoch 1620, training loss: 62.32228469848633 = 0.057681769132614136 + 10.0 * 6.2264604568481445
Epoch 1620, val loss: 1.1517964601516724
Epoch 1630, training loss: 62.30324172973633 = 0.056484706699848175 + 10.0 * 6.22467565536499
Epoch 1630, val loss: 1.1551148891448975
Epoch 1640, training loss: 62.29585647583008 = 0.05531654879450798 + 10.0 * 6.224053859710693
Epoch 1640, val loss: 1.1588373184204102
Epoch 1650, training loss: 62.294376373291016 = 0.05418771877884865 + 10.0 * 6.2240190505981445
Epoch 1650, val loss: 1.1625053882598877
Epoch 1660, training loss: 62.31100845336914 = 0.053093694150447845 + 10.0 * 6.2257914543151855
Epoch 1660, val loss: 1.1655398607254028
Epoch 1670, training loss: 62.27389144897461 = 0.05202712118625641 + 10.0 * 6.222186088562012
Epoch 1670, val loss: 1.169527292251587
Epoch 1680, training loss: 62.265960693359375 = 0.050993166863918304 + 10.0 * 6.22149658203125
Epoch 1680, val loss: 1.173099160194397
Epoch 1690, training loss: 62.2788200378418 = 0.049992844462394714 + 10.0 * 6.2228827476501465
Epoch 1690, val loss: 1.1766573190689087
Epoch 1700, training loss: 62.305908203125 = 0.049010343849658966 + 10.0 * 6.225689888000488
Epoch 1700, val loss: 1.1804252862930298
Epoch 1710, training loss: 62.26503372192383 = 0.048040721565485 + 10.0 * 6.221699237823486
Epoch 1710, val loss: 1.1836546659469604
Epoch 1720, training loss: 62.25621795654297 = 0.04711271822452545 + 10.0 * 6.220910549163818
Epoch 1720, val loss: 1.1874970197677612
Epoch 1730, training loss: 62.265052795410156 = 0.04622495174407959 + 10.0 * 6.2218828201293945
Epoch 1730, val loss: 1.1911633014678955
Epoch 1740, training loss: 62.279701232910156 = 0.04535144194960594 + 10.0 * 6.223434925079346
Epoch 1740, val loss: 1.1945854425430298
Epoch 1750, training loss: 62.27756881713867 = 0.04448068141937256 + 10.0 * 6.22330904006958
Epoch 1750, val loss: 1.1977839469909668
Epoch 1760, training loss: 62.24100112915039 = 0.04363477975130081 + 10.0 * 6.219736576080322
Epoch 1760, val loss: 1.2013146877288818
Epoch 1770, training loss: 62.23454284667969 = 0.042832788079977036 + 10.0 * 6.219171047210693
Epoch 1770, val loss: 1.204971194267273
Epoch 1780, training loss: 62.25376510620117 = 0.04206205904483795 + 10.0 * 6.221170425415039
Epoch 1780, val loss: 1.2084484100341797
Epoch 1790, training loss: 62.239295959472656 = 0.04128745198249817 + 10.0 * 6.21980094909668
Epoch 1790, val loss: 1.2119462490081787
Epoch 1800, training loss: 62.22821044921875 = 0.040527042001485825 + 10.0 * 6.218768119812012
Epoch 1800, val loss: 1.2155689001083374
Epoch 1810, training loss: 62.243595123291016 = 0.039810504764318466 + 10.0 * 6.220378398895264
Epoch 1810, val loss: 1.2190768718719482
Epoch 1820, training loss: 62.24802017211914 = 0.039101265370845795 + 10.0 * 6.220891952514648
Epoch 1820, val loss: 1.2224491834640503
Epoch 1830, training loss: 62.22126007080078 = 0.038390956819057465 + 10.0 * 6.218286991119385
Epoch 1830, val loss: 1.2258046865463257
Epoch 1840, training loss: 62.21381759643555 = 0.03771607577800751 + 10.0 * 6.2176103591918945
Epoch 1840, val loss: 1.2291853427886963
Epoch 1850, training loss: 62.21390151977539 = 0.03706439957022667 + 10.0 * 6.217683792114258
Epoch 1850, val loss: 1.2326761484146118
Epoch 1860, training loss: 62.23835754394531 = 0.03642910718917847 + 10.0 * 6.220192909240723
Epoch 1860, val loss: 1.236214280128479
Epoch 1870, training loss: 62.22071075439453 = 0.03580717369914055 + 10.0 * 6.218490123748779
Epoch 1870, val loss: 1.2394341230392456
Epoch 1880, training loss: 62.24076461791992 = 0.03519133850932121 + 10.0 * 6.22055721282959
Epoch 1880, val loss: 1.2427287101745605
Epoch 1890, training loss: 62.20887756347656 = 0.03457815945148468 + 10.0 * 6.217430114746094
Epoch 1890, val loss: 1.2458244562149048
Epoch 1900, training loss: 62.19685745239258 = 0.03400692343711853 + 10.0 * 6.21628475189209
Epoch 1900, val loss: 1.2495145797729492
Epoch 1910, training loss: 62.19880294799805 = 0.033445775508880615 + 10.0 * 6.216535568237305
Epoch 1910, val loss: 1.2529314756393433
Epoch 1920, training loss: 62.25672912597656 = 0.03290480747818947 + 10.0 * 6.222382545471191
Epoch 1920, val loss: 1.2561792135238647
Epoch 1930, training loss: 62.19733428955078 = 0.03233807533979416 + 10.0 * 6.2164998054504395
Epoch 1930, val loss: 1.2590303421020508
Epoch 1940, training loss: 62.18513488769531 = 0.0318104587495327 + 10.0 * 6.215332508087158
Epoch 1940, val loss: 1.2625631093978882
Epoch 1950, training loss: 62.22914505004883 = 0.03130507096648216 + 10.0 * 6.219784259796143
Epoch 1950, val loss: 1.2658898830413818
Epoch 1960, training loss: 62.1905517578125 = 0.03079329803586006 + 10.0 * 6.215975761413574
Epoch 1960, val loss: 1.2686967849731445
Epoch 1970, training loss: 62.1909065246582 = 0.03030337765812874 + 10.0 * 6.216060161590576
Epoch 1970, val loss: 1.2721819877624512
Epoch 1980, training loss: 62.19218826293945 = 0.029821114614605904 + 10.0 * 6.2162370681762695
Epoch 1980, val loss: 1.27531898021698
Epoch 1990, training loss: 62.18022537231445 = 0.029350996017456055 + 10.0 * 6.215087413787842
Epoch 1990, val loss: 1.2783911228179932
Epoch 2000, training loss: 62.201507568359375 = 0.028891930356621742 + 10.0 * 6.21726131439209
Epoch 2000, val loss: 1.2818307876586914
Epoch 2010, training loss: 62.18981170654297 = 0.028442945331335068 + 10.0 * 6.216136932373047
Epoch 2010, val loss: 1.2847956418991089
Epoch 2020, training loss: 62.168846130371094 = 0.028003623709082603 + 10.0 * 6.214084148406982
Epoch 2020, val loss: 1.2876735925674438
Epoch 2030, training loss: 62.19593048095703 = 0.02758168615400791 + 10.0 * 6.216835021972656
Epoch 2030, val loss: 1.2906897068023682
Epoch 2040, training loss: 62.16773223876953 = 0.02715601772069931 + 10.0 * 6.214057445526123
Epoch 2040, val loss: 1.2941045761108398
Epoch 2050, training loss: 62.16542434692383 = 0.02674756571650505 + 10.0 * 6.213867664337158
Epoch 2050, val loss: 1.297209620475769
Epoch 2060, training loss: 62.16791915893555 = 0.026347482576966286 + 10.0 * 6.2141571044921875
Epoch 2060, val loss: 1.3001292943954468
Epoch 2070, training loss: 62.18754959106445 = 0.025955719873309135 + 10.0 * 6.216159343719482
Epoch 2070, val loss: 1.3030461072921753
Epoch 2080, training loss: 62.19723892211914 = 0.02557830698788166 + 10.0 * 6.217165946960449
Epoch 2080, val loss: 1.3062056303024292
Epoch 2090, training loss: 62.168094635009766 = 0.02518952637910843 + 10.0 * 6.214290618896484
Epoch 2090, val loss: 1.3092341423034668
Epoch 2100, training loss: 62.155033111572266 = 0.024820737540721893 + 10.0 * 6.213021278381348
Epoch 2100, val loss: 1.3118951320648193
Epoch 2110, training loss: 62.154422760009766 = 0.024465305730700493 + 10.0 * 6.212996006011963
Epoch 2110, val loss: 1.3151471614837646
Epoch 2120, training loss: 62.17732238769531 = 0.02411673776805401 + 10.0 * 6.215320587158203
Epoch 2120, val loss: 1.3181155920028687
Epoch 2130, training loss: 62.15578842163086 = 0.023768361657857895 + 10.0 * 6.213201999664307
Epoch 2130, val loss: 1.3208205699920654
Epoch 2140, training loss: 62.13878631591797 = 0.023431582376360893 + 10.0 * 6.211535453796387
Epoch 2140, val loss: 1.323805332183838
Epoch 2150, training loss: 62.14814758300781 = 0.02310975454747677 + 10.0 * 6.212503910064697
Epoch 2150, val loss: 1.3268617391586304
Epoch 2160, training loss: 62.18053436279297 = 0.02279147133231163 + 10.0 * 6.215774059295654
Epoch 2160, val loss: 1.3296020030975342
Epoch 2170, training loss: 62.159149169921875 = 0.022456757724285126 + 10.0 * 6.213669300079346
Epoch 2170, val loss: 1.3322442770004272
Epoch 2180, training loss: 62.16282653808594 = 0.022152690216898918 + 10.0 * 6.214067459106445
Epoch 2180, val loss: 1.3350999355316162
Epoch 2190, training loss: 62.17180252075195 = 0.021842295303940773 + 10.0 * 6.214995861053467
Epoch 2190, val loss: 1.3373627662658691
Epoch 2200, training loss: 62.14175033569336 = 0.021541329100728035 + 10.0 * 6.2120208740234375
Epoch 2200, val loss: 1.3405940532684326
Epoch 2210, training loss: 62.12590789794922 = 0.021246900781989098 + 10.0 * 6.210465908050537
Epoch 2210, val loss: 1.3432888984680176
Epoch 2220, training loss: 62.119384765625 = 0.020967375487089157 + 10.0 * 6.209841728210449
Epoch 2220, val loss: 1.3462032079696655
Epoch 2230, training loss: 62.12831497192383 = 0.02069307491183281 + 10.0 * 6.210762023925781
Epoch 2230, val loss: 1.3488473892211914
Epoch 2240, training loss: 62.14936828613281 = 0.02041887864470482 + 10.0 * 6.212894916534424
Epoch 2240, val loss: 1.3515827655792236
Epoch 2250, training loss: 62.130794525146484 = 0.020138856023550034 + 10.0 * 6.211065769195557
Epoch 2250, val loss: 1.3541244268417358
Epoch 2260, training loss: 62.11564254760742 = 0.019872581586241722 + 10.0 * 6.2095770835876465
Epoch 2260, val loss: 1.3567668199539185
Epoch 2270, training loss: 62.11586380004883 = 0.019616998732089996 + 10.0 * 6.209624767303467
Epoch 2270, val loss: 1.3593274354934692
Epoch 2280, training loss: 62.1518669128418 = 0.01937033236026764 + 10.0 * 6.213249683380127
Epoch 2280, val loss: 1.3619505167007446
Epoch 2290, training loss: 62.136749267578125 = 0.019112419337034225 + 10.0 * 6.211763858795166
Epoch 2290, val loss: 1.3645710945129395
Epoch 2300, training loss: 62.11472702026367 = 0.018860897049307823 + 10.0 * 6.2095866203308105
Epoch 2300, val loss: 1.366806149482727
Epoch 2310, training loss: 62.10860061645508 = 0.01862281560897827 + 10.0 * 6.20899772644043
Epoch 2310, val loss: 1.369699239730835
Epoch 2320, training loss: 62.11895751953125 = 0.01839214749634266 + 10.0 * 6.210056781768799
Epoch 2320, val loss: 1.3723394870758057
Epoch 2330, training loss: 62.10572052001953 = 0.018159465864300728 + 10.0 * 6.208756446838379
Epoch 2330, val loss: 1.3746330738067627
Epoch 2340, training loss: 62.14279556274414 = 0.017930487170815468 + 10.0 * 6.212486743927002
Epoch 2340, val loss: 1.377192497253418
Epoch 2350, training loss: 62.10190963745117 = 0.017705736681818962 + 10.0 * 6.208420753479004
Epoch 2350, val loss: 1.3795887231826782
Epoch 2360, training loss: 62.0966682434082 = 0.017486214637756348 + 10.0 * 6.207918167114258
Epoch 2360, val loss: 1.3821364641189575
Epoch 2370, training loss: 62.11536407470703 = 0.017274370416998863 + 10.0 * 6.209809303283691
Epoch 2370, val loss: 1.3846935033798218
Epoch 2380, training loss: 62.09633255004883 = 0.017063379287719727 + 10.0 * 6.2079267501831055
Epoch 2380, val loss: 1.386979103088379
Epoch 2390, training loss: 62.09391784667969 = 0.016856256872415543 + 10.0 * 6.207705974578857
Epoch 2390, val loss: 1.3891609907150269
Epoch 2400, training loss: 62.10897445678711 = 0.016656585037708282 + 10.0 * 6.209231853485107
Epoch 2400, val loss: 1.3915393352508545
Epoch 2410, training loss: 62.12586212158203 = 0.016455315053462982 + 10.0 * 6.210940361022949
Epoch 2410, val loss: 1.3939359188079834
Epoch 2420, training loss: 62.0986442565918 = 0.01625777781009674 + 10.0 * 6.20823860168457
Epoch 2420, val loss: 1.396180272102356
Epoch 2430, training loss: 62.08734893798828 = 0.016063068062067032 + 10.0 * 6.207128524780273
Epoch 2430, val loss: 1.398614764213562
Epoch 2440, training loss: 62.119266510009766 = 0.01587888039648533 + 10.0 * 6.210338592529297
Epoch 2440, val loss: 1.401226282119751
Epoch 2450, training loss: 62.081573486328125 = 0.015688633546233177 + 10.0 * 6.206588268280029
Epoch 2450, val loss: 1.4031139612197876
Epoch 2460, training loss: 62.080074310302734 = 0.015503865666687489 + 10.0 * 6.206457138061523
Epoch 2460, val loss: 1.405422329902649
Epoch 2470, training loss: 62.07720184326172 = 0.015328468754887581 + 10.0 * 6.2061872482299805
Epoch 2470, val loss: 1.4077690839767456
Epoch 2480, training loss: 62.15367889404297 = 0.015156622976064682 + 10.0 * 6.213852405548096
Epoch 2480, val loss: 1.4098246097564697
Epoch 2490, training loss: 62.09612274169922 = 0.014977315440773964 + 10.0 * 6.2081146240234375
Epoch 2490, val loss: 1.4122428894042969
Epoch 2500, training loss: 62.08225631713867 = 0.014807094819843769 + 10.0 * 6.20674467086792
Epoch 2500, val loss: 1.414366364479065
Epoch 2510, training loss: 62.09670639038086 = 0.014640663750469685 + 10.0 * 6.208206653594971
Epoch 2510, val loss: 1.4169076681137085
Epoch 2520, training loss: 62.093414306640625 = 0.0144733227789402 + 10.0 * 6.207894325256348
Epoch 2520, val loss: 1.4183520078659058
Epoch 2530, training loss: 62.07568359375 = 0.014314945787191391 + 10.0 * 6.206136703491211
Epoch 2530, val loss: 1.4205535650253296
Epoch 2540, training loss: 62.06879806518555 = 0.01415702048689127 + 10.0 * 6.2054643630981445
Epoch 2540, val loss: 1.4229868650436401
Epoch 2550, training loss: 62.064083099365234 = 0.014005214907228947 + 10.0 * 6.205008029937744
Epoch 2550, val loss: 1.4249964952468872
Epoch 2560, training loss: 62.092498779296875 = 0.0138564333319664 + 10.0 * 6.207864284515381
Epoch 2560, val loss: 1.4271479845046997
Epoch 2570, training loss: 62.088584899902344 = 0.01369940210133791 + 10.0 * 6.207488536834717
Epoch 2570, val loss: 1.4290865659713745
Epoch 2580, training loss: 62.06585693359375 = 0.013549300841987133 + 10.0 * 6.205230712890625
Epoch 2580, val loss: 1.4310969114303589
Epoch 2590, training loss: 62.07623291015625 = 0.013403860852122307 + 10.0 * 6.206282615661621
Epoch 2590, val loss: 1.4333261251449585
Epoch 2600, training loss: 62.06644058227539 = 0.013258678838610649 + 10.0 * 6.205317974090576
Epoch 2600, val loss: 1.434958815574646
Epoch 2610, training loss: 62.0555419921875 = 0.013119140639901161 + 10.0 * 6.20424222946167
Epoch 2610, val loss: 1.4370319843292236
Epoch 2620, training loss: 62.05501174926758 = 0.012982944957911968 + 10.0 * 6.204203128814697
Epoch 2620, val loss: 1.439077377319336
Epoch 2630, training loss: 62.09900665283203 = 0.012848137877881527 + 10.0 * 6.208615779876709
Epoch 2630, val loss: 1.4409279823303223
Epoch 2640, training loss: 62.092933654785156 = 0.012709681876003742 + 10.0 * 6.208022117614746
Epoch 2640, val loss: 1.4433985948562622
Epoch 2650, training loss: 62.05696487426758 = 0.012573890388011932 + 10.0 * 6.204439163208008
Epoch 2650, val loss: 1.4447077512741089
Epoch 2660, training loss: 62.047821044921875 = 0.012446106411516666 + 10.0 * 6.203537464141846
Epoch 2660, val loss: 1.446838140487671
Epoch 2670, training loss: 62.044166564941406 = 0.012322327122092247 + 10.0 * 6.203184604644775
Epoch 2670, val loss: 1.4488905668258667
Epoch 2680, training loss: 62.090545654296875 = 0.012199926190078259 + 10.0 * 6.207834720611572
Epoch 2680, val loss: 1.450641393661499
Epoch 2690, training loss: 62.05002975463867 = 0.012072360143065453 + 10.0 * 6.203795433044434
Epoch 2690, val loss: 1.45255446434021
Epoch 2700, training loss: 62.03720474243164 = 0.011948328465223312 + 10.0 * 6.202525615692139
Epoch 2700, val loss: 1.4544754028320312
Epoch 2710, training loss: 62.03297805786133 = 0.011829339899122715 + 10.0 * 6.202115058898926
Epoch 2710, val loss: 1.456413984298706
Epoch 2720, training loss: 62.04029846191406 = 0.01171775534749031 + 10.0 * 6.202857971191406
Epoch 2720, val loss: 1.4583830833435059
Epoch 2730, training loss: 62.11507797241211 = 0.011607288382947445 + 10.0 * 6.2103471755981445
Epoch 2730, val loss: 1.4600294828414917
Epoch 2740, training loss: 62.07380294799805 = 0.01148147787898779 + 10.0 * 6.206232070922852
Epoch 2740, val loss: 1.4621458053588867
Epoch 2750, training loss: 62.044593811035156 = 0.011367986910045147 + 10.0 * 6.203322410583496
Epoch 2750, val loss: 1.4634037017822266
Epoch 2760, training loss: 62.03295135498047 = 0.011257311329245567 + 10.0 * 6.202169418334961
Epoch 2760, val loss: 1.4653339385986328
Epoch 2770, training loss: 62.06422805786133 = 0.011153187602758408 + 10.0 * 6.205307483673096
Epoch 2770, val loss: 1.4668819904327393
Epoch 2780, training loss: 62.04647445678711 = 0.011042780242860317 + 10.0 * 6.203543186187744
Epoch 2780, val loss: 1.4688655138015747
Epoch 2790, training loss: 62.02292251586914 = 0.010932305827736855 + 10.0 * 6.201199054718018
Epoch 2790, val loss: 1.4703240394592285
Epoch 2800, training loss: 62.023746490478516 = 0.010828978382050991 + 10.0 * 6.201291561126709
Epoch 2800, val loss: 1.472507357597351
Epoch 2810, training loss: 62.0238151550293 = 0.01072964258491993 + 10.0 * 6.201308250427246
Epoch 2810, val loss: 1.4743841886520386
Epoch 2820, training loss: 62.06835174560547 = 0.010632353834807873 + 10.0 * 6.2057719230651855
Epoch 2820, val loss: 1.4761381149291992
Epoch 2830, training loss: 62.035179138183594 = 0.010529971681535244 + 10.0 * 6.202465057373047
Epoch 2830, val loss: 1.4774680137634277
Epoch 2840, training loss: 62.02729797363281 = 0.010428555309772491 + 10.0 * 6.201686859130859
Epoch 2840, val loss: 1.4791182279586792
Epoch 2850, training loss: 62.021976470947266 = 0.01033199392259121 + 10.0 * 6.201164722442627
Epoch 2850, val loss: 1.4810580015182495
Epoch 2860, training loss: 62.0538330078125 = 0.010241104289889336 + 10.0 * 6.20435905456543
Epoch 2860, val loss: 1.4828864336013794
Epoch 2870, training loss: 62.02774429321289 = 0.010142363607883453 + 10.0 * 6.201760292053223
Epoch 2870, val loss: 1.484290361404419
Epoch 2880, training loss: 62.03219985961914 = 0.010048598051071167 + 10.0 * 6.202215194702148
Epoch 2880, val loss: 1.4859522581100464
Epoch 2890, training loss: 62.016048431396484 = 0.009956832975149155 + 10.0 * 6.20060920715332
Epoch 2890, val loss: 1.4875433444976807
Epoch 2900, training loss: 62.01500701904297 = 0.009868462570011616 + 10.0 * 6.20051383972168
Epoch 2900, val loss: 1.489099383354187
Epoch 2910, training loss: 62.07107925415039 = 0.009783327579498291 + 10.0 * 6.206129550933838
Epoch 2910, val loss: 1.4908263683319092
Epoch 2920, training loss: 62.02543258666992 = 0.009691054001450539 + 10.0 * 6.201574325561523
Epoch 2920, val loss: 1.4923685789108276
Epoch 2930, training loss: 62.011409759521484 = 0.009604820981621742 + 10.0 * 6.200180530548096
Epoch 2930, val loss: 1.493686556816101
Epoch 2940, training loss: 62.00286102294922 = 0.009520565159618855 + 10.0 * 6.199334144592285
Epoch 2940, val loss: 1.4954512119293213
Epoch 2950, training loss: 62.00712966918945 = 0.009440798312425613 + 10.0 * 6.199769020080566
Epoch 2950, val loss: 1.497112512588501
Epoch 2960, training loss: 62.06729507446289 = 0.009359607473015785 + 10.0 * 6.205793380737305
Epoch 2960, val loss: 1.4985326528549194
Epoch 2970, training loss: 62.05318832397461 = 0.009274464100599289 + 10.0 * 6.2043914794921875
Epoch 2970, val loss: 1.4996665716171265
Epoch 2980, training loss: 62.016883850097656 = 0.009190824814140797 + 10.0 * 6.200769424438477
Epoch 2980, val loss: 1.5012015104293823
Epoch 2990, training loss: 62.012969970703125 = 0.009112841449677944 + 10.0 * 6.200385570526123
Epoch 2990, val loss: 1.5028603076934814
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8397469688982605
The final CL Acc:0.75556, 0.01090, The final GNN Acc:0.83992, 0.00151
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11568])
remove edge: torch.Size([2, 9564])
updated graph: torch.Size([2, 10576])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.90298461914062 = 1.934289813041687 + 10.0 * 8.596869468688965
Epoch 0, val loss: 1.9207793474197388
Epoch 10, training loss: 87.88983154296875 = 1.9247909784317017 + 10.0 * 8.596504211425781
Epoch 10, val loss: 1.9120361804962158
Epoch 20, training loss: 87.8512954711914 = 1.9133695363998413 + 10.0 * 8.593792915344238
Epoch 20, val loss: 1.9012876749038696
Epoch 30, training loss: 87.64866638183594 = 1.8985453844070435 + 10.0 * 8.57501220703125
Epoch 30, val loss: 1.8871742486953735
Epoch 40, training loss: 86.66303253173828 = 1.880986213684082 + 10.0 * 8.478204727172852
Epoch 40, val loss: 1.8710203170776367
Epoch 50, training loss: 82.57460021972656 = 1.8641622066497803 + 10.0 * 8.071043968200684
Epoch 50, val loss: 1.855547547340393
Epoch 60, training loss: 78.67086791992188 = 1.849609375 + 10.0 * 7.682126045227051
Epoch 60, val loss: 1.8423386812210083
Epoch 70, training loss: 76.459228515625 = 1.839774489402771 + 10.0 * 7.461945533752441
Epoch 70, val loss: 1.8324899673461914
Epoch 80, training loss: 74.12901306152344 = 1.8297717571258545 + 10.0 * 7.229924201965332
Epoch 80, val loss: 1.8221548795700073
Epoch 90, training loss: 72.31733703613281 = 1.8178819417953491 + 10.0 * 7.049945831298828
Epoch 90, val loss: 1.811362385749817
Epoch 100, training loss: 71.49651336669922 = 1.806181788444519 + 10.0 * 6.9690327644348145
Epoch 100, val loss: 1.801089882850647
Epoch 110, training loss: 70.6613998413086 = 1.795110821723938 + 10.0 * 6.886629104614258
Epoch 110, val loss: 1.7915409803390503
Epoch 120, training loss: 69.96029663085938 = 1.784847378730774 + 10.0 * 6.817544460296631
Epoch 120, val loss: 1.7827306985855103
Epoch 130, training loss: 69.474365234375 = 1.7742183208465576 + 10.0 * 6.770015239715576
Epoch 130, val loss: 1.7738200426101685
Epoch 140, training loss: 69.01847839355469 = 1.7623716592788696 + 10.0 * 6.725610256195068
Epoch 140, val loss: 1.7640150785446167
Epoch 150, training loss: 68.62114715576172 = 1.749737024307251 + 10.0 * 6.687140464782715
Epoch 150, val loss: 1.7535094022750854
Epoch 160, training loss: 68.2981185913086 = 1.736270785331726 + 10.0 * 6.656185150146484
Epoch 160, val loss: 1.7419321537017822
Epoch 170, training loss: 67.9673080444336 = 1.721259593963623 + 10.0 * 6.624605178833008
Epoch 170, val loss: 1.7290838956832886
Epoch 180, training loss: 67.66511535644531 = 1.704816222190857 + 10.0 * 6.596029758453369
Epoch 180, val loss: 1.7148605585098267
Epoch 190, training loss: 67.46136474609375 = 1.6866652965545654 + 10.0 * 6.577469348907471
Epoch 190, val loss: 1.6993041038513184
Epoch 200, training loss: 67.20050811767578 = 1.6666829586029053 + 10.0 * 6.553382873535156
Epoch 200, val loss: 1.6821954250335693
Epoch 210, training loss: 66.98635864257812 = 1.6448949575424194 + 10.0 * 6.534145832061768
Epoch 210, val loss: 1.6636165380477905
Epoch 220, training loss: 66.82655334472656 = 1.621132254600525 + 10.0 * 6.520542144775391
Epoch 220, val loss: 1.6434962749481201
Epoch 230, training loss: 66.65296936035156 = 1.5956790447235107 + 10.0 * 6.5057291984558105
Epoch 230, val loss: 1.6219629049301147
Epoch 240, training loss: 66.51130676269531 = 1.5683867931365967 + 10.0 * 6.49429178237915
Epoch 240, val loss: 1.5990434885025024
Epoch 250, training loss: 66.38630676269531 = 1.539357304573059 + 10.0 * 6.484694957733154
Epoch 250, val loss: 1.57485032081604
Epoch 260, training loss: 66.256591796875 = 1.5088001489639282 + 10.0 * 6.4747796058654785
Epoch 260, val loss: 1.549738883972168
Epoch 270, training loss: 66.12525177001953 = 1.4768974781036377 + 10.0 * 6.464835166931152
Epoch 270, val loss: 1.5238358974456787
Epoch 280, training loss: 66.05570220947266 = 1.4438962936401367 + 10.0 * 6.461181163787842
Epoch 280, val loss: 1.4972851276397705
Epoch 290, training loss: 65.90994262695312 = 1.4099422693252563 + 10.0 * 6.450000286102295
Epoch 290, val loss: 1.470219373703003
Epoch 300, training loss: 65.80398559570312 = 1.3755292892456055 + 10.0 * 6.442845821380615
Epoch 300, val loss: 1.442948341369629
Epoch 310, training loss: 65.7004165649414 = 1.3406643867492676 + 10.0 * 6.435974597930908
Epoch 310, val loss: 1.4156404733657837
Epoch 320, training loss: 65.6341323852539 = 1.3053767681121826 + 10.0 * 6.432876110076904
Epoch 320, val loss: 1.3884207010269165
Epoch 330, training loss: 65.5101547241211 = 1.2699973583221436 + 10.0 * 6.424015522003174
Epoch 330, val loss: 1.3612120151519775
Epoch 340, training loss: 65.41161346435547 = 1.2344443798065186 + 10.0 * 6.417716979980469
Epoch 340, val loss: 1.3341703414916992
Epoch 350, training loss: 65.3326187133789 = 1.1989413499832153 + 10.0 * 6.41336727142334
Epoch 350, val loss: 1.307517647743225
Epoch 360, training loss: 65.27958679199219 = 1.16344153881073 + 10.0 * 6.411614418029785
Epoch 360, val loss: 1.2811896800994873
Epoch 370, training loss: 65.1624984741211 = 1.128419041633606 + 10.0 * 6.403408050537109
Epoch 370, val loss: 1.2555043697357178
Epoch 380, training loss: 65.07227325439453 = 1.0937368869781494 + 10.0 * 6.397853374481201
Epoch 380, val loss: 1.2303208112716675
Epoch 390, training loss: 65.00529479980469 = 1.059502124786377 + 10.0 * 6.3945794105529785
Epoch 390, val loss: 1.2056269645690918
Epoch 400, training loss: 64.95824432373047 = 1.025593876838684 + 10.0 * 6.3932647705078125
Epoch 400, val loss: 1.1814759969711304
Epoch 410, training loss: 64.8552474975586 = 0.9923868775367737 + 10.0 * 6.386285781860352
Epoch 410, val loss: 1.1579798460006714
Epoch 420, training loss: 64.77339172363281 = 0.9598686695098877 + 10.0 * 6.381352424621582
Epoch 420, val loss: 1.135193943977356
Epoch 430, training loss: 64.72708129882812 = 0.9280783534049988 + 10.0 * 6.379899978637695
Epoch 430, val loss: 1.1130352020263672
Epoch 440, training loss: 64.67896270751953 = 0.8968647718429565 + 10.0 * 6.378210067749023
Epoch 440, val loss: 1.0917500257492065
Epoch 450, training loss: 64.59271240234375 = 0.8665459752082825 + 10.0 * 6.372616767883301
Epoch 450, val loss: 1.0710734128952026
Epoch 460, training loss: 64.51530456542969 = 0.8370817303657532 + 10.0 * 6.367822647094727
Epoch 460, val loss: 1.0512291193008423
Epoch 470, training loss: 64.47309112548828 = 0.8083498477935791 + 10.0 * 6.366474151611328
Epoch 470, val loss: 1.0321446657180786
Epoch 480, training loss: 64.41829681396484 = 0.7804126143455505 + 10.0 * 6.36378812789917
Epoch 480, val loss: 1.0138901472091675
Epoch 490, training loss: 64.35498046875 = 0.7530835270881653 + 10.0 * 6.360189914703369
Epoch 490, val loss: 0.9962992072105408
Epoch 500, training loss: 64.2920913696289 = 0.7265797257423401 + 10.0 * 6.356551170349121
Epoch 500, val loss: 0.9795961976051331
Epoch 510, training loss: 64.23680114746094 = 0.7007535696029663 + 10.0 * 6.353604793548584
Epoch 510, val loss: 0.9636770486831665
Epoch 520, training loss: 64.26439666748047 = 0.6754500865936279 + 10.0 * 6.358894348144531
Epoch 520, val loss: 0.9483311176300049
Epoch 530, training loss: 64.16276550292969 = 0.6506548523902893 + 10.0 * 6.351210594177246
Epoch 530, val loss: 0.9336026906967163
Epoch 540, training loss: 64.08931732177734 = 0.6265167593955994 + 10.0 * 6.346280097961426
Epoch 540, val loss: 0.9197388291358948
Epoch 550, training loss: 64.04700469970703 = 0.6029292941093445 + 10.0 * 6.344407558441162
Epoch 550, val loss: 0.9065624475479126
Epoch 560, training loss: 63.99759292602539 = 0.5798316597938538 + 10.0 * 6.341776371002197
Epoch 560, val loss: 0.8939957618713379
Epoch 570, training loss: 64.03874206542969 = 0.5572683811187744 + 10.0 * 6.348147392272949
Epoch 570, val loss: 0.8819311261177063
Epoch 580, training loss: 63.93007278442383 = 0.5349863171577454 + 10.0 * 6.339509010314941
Epoch 580, val loss: 0.8704937696456909
Epoch 590, training loss: 63.878238677978516 = 0.5133711695671082 + 10.0 * 6.33648681640625
Epoch 590, val loss: 0.859870195388794
Epoch 600, training loss: 63.84039306640625 = 0.4923132359981537 + 10.0 * 6.334807872772217
Epoch 600, val loss: 0.8499230742454529
Epoch 610, training loss: 63.80367660522461 = 0.4718668758869171 + 10.0 * 6.333180904388428
Epoch 610, val loss: 0.8405728340148926
Epoch 620, training loss: 63.7829704284668 = 0.45193150639533997 + 10.0 * 6.333104133605957
Epoch 620, val loss: 0.8317626118659973
Epoch 630, training loss: 63.731632232666016 = 0.4326249361038208 + 10.0 * 6.329900741577148
Epoch 630, val loss: 0.8237386345863342
Epoch 640, training loss: 63.684669494628906 = 0.4139837324619293 + 10.0 * 6.32706880569458
Epoch 640, val loss: 0.816318929195404
Epoch 650, training loss: 63.649654388427734 = 0.3960407078266144 + 10.0 * 6.325361251831055
Epoch 650, val loss: 0.8095768690109253
Epoch 660, training loss: 63.69120788574219 = 0.3787161707878113 + 10.0 * 6.331249237060547
Epoch 660, val loss: 0.8032718896865845
Epoch 670, training loss: 63.590370178222656 = 0.36207321286201477 + 10.0 * 6.322829723358154
Epoch 670, val loss: 0.797774612903595
Epoch 680, training loss: 63.5617790222168 = 0.3462127149105072 + 10.0 * 6.321556568145752
Epoch 680, val loss: 0.7928587198257446
Epoch 690, training loss: 63.573280334472656 = 0.3311142027378082 + 10.0 * 6.324216365814209
Epoch 690, val loss: 0.7885797619819641
Epoch 700, training loss: 63.50408935546875 = 0.31654390692710876 + 10.0 * 6.31875467300415
Epoch 700, val loss: 0.7848995923995972
Epoch 710, training loss: 63.47041320800781 = 0.3028193414211273 + 10.0 * 6.3167595863342285
Epoch 710, val loss: 0.7817369103431702
Epoch 720, training loss: 63.48667526245117 = 0.2896968424320221 + 10.0 * 6.319697856903076
Epoch 720, val loss: 0.7792065143585205
Epoch 730, training loss: 63.42485809326172 = 0.27721571922302246 + 10.0 * 6.314764499664307
Epoch 730, val loss: 0.7771766185760498
Epoch 740, training loss: 63.38628387451172 = 0.26536738872528076 + 10.0 * 6.312091827392578
Epoch 740, val loss: 0.775647759437561
Epoch 750, training loss: 63.40333557128906 = 0.2541889250278473 + 10.0 * 6.314914703369141
Epoch 750, val loss: 0.7745932340621948
Epoch 760, training loss: 63.3677978515625 = 0.24339903891086578 + 10.0 * 6.312439918518066
Epoch 760, val loss: 0.7739810943603516
Epoch 770, training loss: 63.329010009765625 = 0.2332904189825058 + 10.0 * 6.309571743011475
Epoch 770, val loss: 0.773749589920044
Epoch 780, training loss: 63.33396530151367 = 0.22368066012859344 + 10.0 * 6.311028480529785
Epoch 780, val loss: 0.7740665674209595
Epoch 790, training loss: 63.29127502441406 = 0.21452893316745758 + 10.0 * 6.307674884796143
Epoch 790, val loss: 0.7748771905899048
Epoch 800, training loss: 63.26169204711914 = 0.2059067189693451 + 10.0 * 6.305578708648682
Epoch 800, val loss: 0.7759456634521484
Epoch 810, training loss: 63.26931381225586 = 0.19767330586910248 + 10.0 * 6.307164192199707
Epoch 810, val loss: 0.7773942947387695
Epoch 820, training loss: 63.23236846923828 = 0.18983778357505798 + 10.0 * 6.304253101348877
Epoch 820, val loss: 0.7790605425834656
Epoch 830, training loss: 63.22390365600586 = 0.18239891529083252 + 10.0 * 6.304150581359863
Epoch 830, val loss: 0.7812411785125732
Epoch 840, training loss: 63.18832015991211 = 0.17535386979579926 + 10.0 * 6.301296710968018
Epoch 840, val loss: 0.7837281823158264
Epoch 850, training loss: 63.17643356323242 = 0.16865210235118866 + 10.0 * 6.300778388977051
Epoch 850, val loss: 0.7863718867301941
Epoch 860, training loss: 63.15699768066406 = 0.1622546762228012 + 10.0 * 6.299474239349365
Epoch 860, val loss: 0.7892580032348633
Epoch 870, training loss: 63.133365631103516 = 0.15619321167469025 + 10.0 * 6.297717094421387
Epoch 870, val loss: 0.7924593687057495
Epoch 880, training loss: 63.167667388916016 = 0.15041062235832214 + 10.0 * 6.3017258644104
Epoch 880, val loss: 0.7958765625953674
Epoch 890, training loss: 63.12405776977539 = 0.1447954773902893 + 10.0 * 6.297926425933838
Epoch 890, val loss: 0.7994339466094971
Epoch 900, training loss: 63.117767333984375 = 0.13954992592334747 + 10.0 * 6.297821998596191
Epoch 900, val loss: 0.8031709790229797
Epoch 910, training loss: 63.073936462402344 = 0.13446791470050812 + 10.0 * 6.293946743011475
Epoch 910, val loss: 0.8071893453598022
Epoch 920, training loss: 63.063201904296875 = 0.12966620922088623 + 10.0 * 6.29335355758667
Epoch 920, val loss: 0.811482310295105
Epoch 930, training loss: 63.098453521728516 = 0.12508776783943176 + 10.0 * 6.297336578369141
Epoch 930, val loss: 0.8157082200050354
Epoch 940, training loss: 63.06446075439453 = 0.12066762149333954 + 10.0 * 6.294379234313965
Epoch 940, val loss: 0.8201276659965515
Epoch 950, training loss: 63.02545166015625 = 0.1164608746767044 + 10.0 * 6.29089879989624
Epoch 950, val loss: 0.8249024152755737
Epoch 960, training loss: 63.006202697753906 = 0.1124541386961937 + 10.0 * 6.289374828338623
Epoch 960, val loss: 0.8296975493431091
Epoch 970, training loss: 63.01308059692383 = 0.10862225294113159 + 10.0 * 6.290445804595947
Epoch 970, val loss: 0.8346825838088989
Epoch 980, training loss: 63.028343200683594 = 0.10493204742670059 + 10.0 * 6.292341232299805
Epoch 980, val loss: 0.8398447632789612
Epoch 990, training loss: 63.02716827392578 = 0.10138721019029617 + 10.0 * 6.292578220367432
Epoch 990, val loss: 0.8444156050682068
Epoch 1000, training loss: 62.95384979248047 = 0.09797799587249756 + 10.0 * 6.285587310791016
Epoch 1000, val loss: 0.8497508764266968
Epoch 1010, training loss: 62.944026947021484 = 0.09476731717586517 + 10.0 * 6.284925937652588
Epoch 1010, val loss: 0.8551107048988342
Epoch 1020, training loss: 62.92885971069336 = 0.091693215072155 + 10.0 * 6.283716678619385
Epoch 1020, val loss: 0.8604519963264465
Epoch 1030, training loss: 63.02614212036133 = 0.08872567862272263 + 10.0 * 6.293741703033447
Epoch 1030, val loss: 0.865606427192688
Epoch 1040, training loss: 62.919124603271484 = 0.0858253762125969 + 10.0 * 6.283329963684082
Epoch 1040, val loss: 0.8711692690849304
Epoch 1050, training loss: 62.90793228149414 = 0.08308910578489304 + 10.0 * 6.282484531402588
Epoch 1050, val loss: 0.8765885829925537
Epoch 1060, training loss: 62.88371658325195 = 0.08048630505800247 + 10.0 * 6.280323028564453
Epoch 1060, val loss: 0.8821220397949219
Epoch 1070, training loss: 62.88215255737305 = 0.07799100875854492 + 10.0 * 6.280416011810303
Epoch 1070, val loss: 0.8876846432685852
Epoch 1080, training loss: 62.92249298095703 = 0.07554753869771957 + 10.0 * 6.284694671630859
Epoch 1080, val loss: 0.8931244611740112
Epoch 1090, training loss: 62.889949798583984 = 0.07319788634777069 + 10.0 * 6.281675338745117
Epoch 1090, val loss: 0.8986513018608093
Epoch 1100, training loss: 62.84361267089844 = 0.0709729939699173 + 10.0 * 6.27726411819458
Epoch 1100, val loss: 0.9042063355445862
Epoch 1110, training loss: 62.83829116821289 = 0.06885198503732681 + 10.0 * 6.276944160461426
Epoch 1110, val loss: 0.9099656939506531
Epoch 1120, training loss: 62.869606018066406 = 0.06682824343442917 + 10.0 * 6.280277729034424
Epoch 1120, val loss: 0.915496289730072
Epoch 1130, training loss: 62.846275329589844 = 0.06481005996465683 + 10.0 * 6.278146266937256
Epoch 1130, val loss: 0.921027421951294
Epoch 1140, training loss: 62.8370475769043 = 0.06291871517896652 + 10.0 * 6.2774128913879395
Epoch 1140, val loss: 0.9266980886459351
Epoch 1150, training loss: 62.814178466796875 = 0.061087850481271744 + 10.0 * 6.275309085845947
Epoch 1150, val loss: 0.9322475790977478
Epoch 1160, training loss: 62.798561096191406 = 0.05932610109448433 + 10.0 * 6.273923397064209
Epoch 1160, val loss: 0.9377514123916626
Epoch 1170, training loss: 62.80833053588867 = 0.05764763429760933 + 10.0 * 6.275068283081055
Epoch 1170, val loss: 0.9434553384780884
Epoch 1180, training loss: 62.812870025634766 = 0.05600887909531593 + 10.0 * 6.275686264038086
Epoch 1180, val loss: 0.9490548372268677
Epoch 1190, training loss: 62.77976608276367 = 0.054422665387392044 + 10.0 * 6.272534370422363
Epoch 1190, val loss: 0.9544097781181335
Epoch 1200, training loss: 62.766841888427734 = 0.052927590906620026 + 10.0 * 6.27139139175415
Epoch 1200, val loss: 0.9599477052688599
Epoch 1210, training loss: 62.76091384887695 = 0.05150136724114418 + 10.0 * 6.270941257476807
Epoch 1210, val loss: 0.965522825717926
Epoch 1220, training loss: 62.80618667602539 = 0.05011508986353874 + 10.0 * 6.275607109069824
Epoch 1220, val loss: 0.9709782600402832
Epoch 1230, training loss: 62.753902435302734 = 0.0487399697303772 + 10.0 * 6.270516395568848
Epoch 1230, val loss: 0.976450502872467
Epoch 1240, training loss: 62.73670196533203 = 0.047460731118917465 + 10.0 * 6.268923759460449
Epoch 1240, val loss: 0.9819424152374268
Epoch 1250, training loss: 62.78455352783203 = 0.04621127247810364 + 10.0 * 6.273834228515625
Epoch 1250, val loss: 0.9873747825622559
Epoch 1260, training loss: 62.73997116088867 = 0.04500779137015343 + 10.0 * 6.269495964050293
Epoch 1260, val loss: 0.9927700757980347
Epoch 1270, training loss: 62.72570037841797 = 0.043839409947395325 + 10.0 * 6.268186092376709
Epoch 1270, val loss: 0.9980984330177307
Epoch 1280, training loss: 62.739524841308594 = 0.04273109510540962 + 10.0 * 6.269679069519043
Epoch 1280, val loss: 1.0035159587860107
Epoch 1290, training loss: 62.71969223022461 = 0.041639480739831924 + 10.0 * 6.267805099487305
Epoch 1290, val loss: 1.0085524320602417
Epoch 1300, training loss: 62.69959259033203 = 0.0405895933508873 + 10.0 * 6.265900611877441
Epoch 1300, val loss: 1.0139844417572021
Epoch 1310, training loss: 62.68588638305664 = 0.03957873582839966 + 10.0 * 6.2646307945251465
Epoch 1310, val loss: 1.0193819999694824
Epoch 1320, training loss: 62.68186950683594 = 0.038620881736278534 + 10.0 * 6.264325141906738
Epoch 1320, val loss: 1.0246831178665161
Epoch 1330, training loss: 62.746788024902344 = 0.03770217299461365 + 10.0 * 6.270908832550049
Epoch 1330, val loss: 1.0298062562942505
Epoch 1340, training loss: 62.7113151550293 = 0.03675903007388115 + 10.0 * 6.267455577850342
Epoch 1340, val loss: 1.034712791442871
Epoch 1350, training loss: 62.67131423950195 = 0.03588739037513733 + 10.0 * 6.263542652130127
Epoch 1350, val loss: 1.0400344133377075
Epoch 1360, training loss: 62.6596794128418 = 0.03503408282995224 + 10.0 * 6.26246452331543
Epoch 1360, val loss: 1.0451979637145996
Epoch 1370, training loss: 62.71754455566406 = 0.03422955051064491 + 10.0 * 6.268331527709961
Epoch 1370, val loss: 1.0503449440002441
Epoch 1380, training loss: 62.694190979003906 = 0.03342607989907265 + 10.0 * 6.266076564788818
Epoch 1380, val loss: 1.0551339387893677
Epoch 1390, training loss: 62.648860931396484 = 0.03264173865318298 + 10.0 * 6.261621952056885
Epoch 1390, val loss: 1.0602482557296753
Epoch 1400, training loss: 62.62969970703125 = 0.03191246837377548 + 10.0 * 6.2597784996032715
Epoch 1400, val loss: 1.0653607845306396
Epoch 1410, training loss: 62.632667541503906 = 0.031197529286146164 + 10.0 * 6.2601470947265625
Epoch 1410, val loss: 1.0704426765441895
Epoch 1420, training loss: 62.69130325317383 = 0.030504966154694557 + 10.0 * 6.266079902648926
Epoch 1420, val loss: 1.075495719909668
Epoch 1430, training loss: 62.635738372802734 = 0.029821250587701797 + 10.0 * 6.260591983795166
Epoch 1430, val loss: 1.080020785331726
Epoch 1440, training loss: 62.6084098815918 = 0.029161173850297928 + 10.0 * 6.257925033569336
Epoch 1440, val loss: 1.084717869758606
Epoch 1450, training loss: 62.605133056640625 = 0.02853858843445778 + 10.0 * 6.257659435272217
Epoch 1450, val loss: 1.089720368385315
Epoch 1460, training loss: 62.62655258178711 = 0.027936886996030807 + 10.0 * 6.259861946105957
Epoch 1460, val loss: 1.0943015813827515
Epoch 1470, training loss: 62.59672546386719 = 0.027338311076164246 + 10.0 * 6.256938457489014
Epoch 1470, val loss: 1.0992289781570435
Epoch 1480, training loss: 62.62447738647461 = 0.02676429972052574 + 10.0 * 6.259771347045898
Epoch 1480, val loss: 1.103725552558899
Epoch 1490, training loss: 62.60417556762695 = 0.026198141276836395 + 10.0 * 6.257797718048096
Epoch 1490, val loss: 1.108547329902649
Epoch 1500, training loss: 62.61250305175781 = 0.025660337880253792 + 10.0 * 6.258684158325195
Epoch 1500, val loss: 1.1133668422698975
Epoch 1510, training loss: 62.603126525878906 = 0.025132646784186363 + 10.0 * 6.2577996253967285
Epoch 1510, val loss: 1.1176362037658691
Epoch 1520, training loss: 62.619056701660156 = 0.024620218202471733 + 10.0 * 6.259443759918213
Epoch 1520, val loss: 1.1224279403686523
Epoch 1530, training loss: 62.57387924194336 = 0.024114640429615974 + 10.0 * 6.254976272583008
Epoch 1530, val loss: 1.1268997192382812
Epoch 1540, training loss: 62.56128692626953 = 0.023639095947146416 + 10.0 * 6.253764629364014
Epoch 1540, val loss: 1.131401538848877
Epoch 1550, training loss: 62.55339431762695 = 0.023182008415460587 + 10.0 * 6.253021240234375
Epoch 1550, val loss: 1.1361316442489624
Epoch 1560, training loss: 62.55116271972656 = 0.022735556587576866 + 10.0 * 6.252842903137207
Epoch 1560, val loss: 1.140528678894043
Epoch 1570, training loss: 62.67472457885742 = 0.022313077002763748 + 10.0 * 6.2652411460876465
Epoch 1570, val loss: 1.1448458433151245
Epoch 1580, training loss: 62.60166549682617 = 0.021854834631085396 + 10.0 * 6.257981300354004
Epoch 1580, val loss: 1.1490126848220825
Epoch 1590, training loss: 62.53646469116211 = 0.021434389054775238 + 10.0 * 6.251502990722656
Epoch 1590, val loss: 1.1535826921463013
Epoch 1600, training loss: 62.53752136230469 = 0.02103748917579651 + 10.0 * 6.251648426055908
Epoch 1600, val loss: 1.157967209815979
Epoch 1610, training loss: 62.543758392333984 = 0.020650021731853485 + 10.0 * 6.252310752868652
Epoch 1610, val loss: 1.162415623664856
Epoch 1620, training loss: 62.5834846496582 = 0.02027234435081482 + 10.0 * 6.256321430206299
Epoch 1620, val loss: 1.1665397882461548
Epoch 1630, training loss: 62.529354095458984 = 0.019885046407580376 + 10.0 * 6.250946998596191
Epoch 1630, val loss: 1.170572280883789
Epoch 1640, training loss: 62.55293273925781 = 0.019523538649082184 + 10.0 * 6.253340721130371
Epoch 1640, val loss: 1.174658179283142
Epoch 1650, training loss: 62.5238037109375 = 0.0191733967512846 + 10.0 * 6.250463008880615
Epoch 1650, val loss: 1.1791250705718994
Epoch 1660, training loss: 62.51469039916992 = 0.01882963813841343 + 10.0 * 6.24958610534668
Epoch 1660, val loss: 1.183306336402893
Epoch 1670, training loss: 62.5312385559082 = 0.018501419574022293 + 10.0 * 6.2512736320495605
Epoch 1670, val loss: 1.1876041889190674
Epoch 1680, training loss: 62.535160064697266 = 0.018177922815084457 + 10.0 * 6.2516984939575195
Epoch 1680, val loss: 1.1915968656539917
Epoch 1690, training loss: 62.514076232910156 = 0.017858410254120827 + 10.0 * 6.249621868133545
Epoch 1690, val loss: 1.1953479051589966
Epoch 1700, training loss: 62.50339889526367 = 0.017550019547343254 + 10.0 * 6.248584747314453
Epoch 1700, val loss: 1.1994436979293823
Epoch 1710, training loss: 62.53411102294922 = 0.017254265025258064 + 10.0 * 6.251685619354248
Epoch 1710, val loss: 1.2033863067626953
Epoch 1720, training loss: 62.496498107910156 = 0.01695910096168518 + 10.0 * 6.24795389175415
Epoch 1720, val loss: 1.2073053121566772
Epoch 1730, training loss: 62.50122833251953 = 0.016673618927598 + 10.0 * 6.24845552444458
Epoch 1730, val loss: 1.2114977836608887
Epoch 1740, training loss: 62.49928665161133 = 0.016394643113017082 + 10.0 * 6.248289108276367
Epoch 1740, val loss: 1.2152762413024902
Epoch 1750, training loss: 62.47584915161133 = 0.016126049682497978 + 10.0 * 6.245972633361816
Epoch 1750, val loss: 1.2189974784851074
Epoch 1760, training loss: 62.52751922607422 = 0.01586594618856907 + 10.0 * 6.251165390014648
Epoch 1760, val loss: 1.2225278615951538
Epoch 1770, training loss: 62.49259948730469 = 0.015604459680616856 + 10.0 * 6.24769926071167
Epoch 1770, val loss: 1.2270231246948242
Epoch 1780, training loss: 62.476097106933594 = 0.015352076850831509 + 10.0 * 6.246074676513672
Epoch 1780, val loss: 1.2305352687835693
Epoch 1790, training loss: 62.468990325927734 = 0.015110100619494915 + 10.0 * 6.245388031005859
Epoch 1790, val loss: 1.2344207763671875
Epoch 1800, training loss: 62.50041961669922 = 0.01487337239086628 + 10.0 * 6.248554706573486
Epoch 1800, val loss: 1.2381073236465454
Epoch 1810, training loss: 62.4681510925293 = 0.014636325649917126 + 10.0 * 6.245351314544678
Epoch 1810, val loss: 1.2415794134140015
Epoch 1820, training loss: 62.46105194091797 = 0.014411560259759426 + 10.0 * 6.244664192199707
Epoch 1820, val loss: 1.2456910610198975
Epoch 1830, training loss: 62.528194427490234 = 0.014190653339028358 + 10.0 * 6.251400470733643
Epoch 1830, val loss: 1.2489712238311768
Epoch 1840, training loss: 62.48043441772461 = 0.013971352949738503 + 10.0 * 6.246645927429199
Epoch 1840, val loss: 1.2524524927139282
Epoch 1850, training loss: 62.52423095703125 = 0.013759447261691093 + 10.0 * 6.251047134399414
Epoch 1850, val loss: 1.2559995651245117
Epoch 1860, training loss: 62.45280838012695 = 0.013539285399019718 + 10.0 * 6.243927001953125
Epoch 1860, val loss: 1.2594772577285767
Epoch 1870, training loss: 62.43929672241211 = 0.013339958153665066 + 10.0 * 6.242595672607422
Epoch 1870, val loss: 1.26296067237854
Epoch 1880, training loss: 62.43023681640625 = 0.013144910335540771 + 10.0 * 6.241709232330322
Epoch 1880, val loss: 1.266658067703247
Epoch 1890, training loss: 62.4472770690918 = 0.012957342900335789 + 10.0 * 6.24343204498291
Epoch 1890, val loss: 1.2701730728149414
Epoch 1900, training loss: 62.48603439331055 = 0.012770694680511951 + 10.0 * 6.247326374053955
Epoch 1900, val loss: 1.2736883163452148
Epoch 1910, training loss: 62.44008255004883 = 0.012575576081871986 + 10.0 * 6.242750644683838
Epoch 1910, val loss: 1.2766748666763306
Epoch 1920, training loss: 62.42136001586914 = 0.012397412210702896 + 10.0 * 6.240896224975586
Epoch 1920, val loss: 1.2801916599273682
Epoch 1930, training loss: 62.41878128051758 = 0.012224781326949596 + 10.0 * 6.240655422210693
Epoch 1930, val loss: 1.2836308479309082
Epoch 1940, training loss: 62.48479080200195 = 0.012058517895638943 + 10.0 * 6.2472734451293945
Epoch 1940, val loss: 1.287341594696045
Epoch 1950, training loss: 62.43196487426758 = 0.011880275793373585 + 10.0 * 6.242008686065674
Epoch 1950, val loss: 1.2895780801773071
Epoch 1960, training loss: 62.47496032714844 = 0.011716412380337715 + 10.0 * 6.24632453918457
Epoch 1960, val loss: 1.2933841943740845
Epoch 1970, training loss: 62.40888595581055 = 0.01154839713126421 + 10.0 * 6.239733695983887
Epoch 1970, val loss: 1.296468734741211
Epoch 1980, training loss: 62.416656494140625 = 0.011393224820494652 + 10.0 * 6.24052619934082
Epoch 1980, val loss: 1.2997123003005981
Epoch 1990, training loss: 62.45762252807617 = 0.011243592947721481 + 10.0 * 6.244637966156006
Epoch 1990, val loss: 1.3027427196502686
Epoch 2000, training loss: 62.412879943847656 = 0.011084254831075668 + 10.0 * 6.240179538726807
Epoch 2000, val loss: 1.3059834241867065
Epoch 2010, training loss: 62.39723205566406 = 0.010939514264464378 + 10.0 * 6.238629341125488
Epoch 2010, val loss: 1.3092154264450073
Epoch 2020, training loss: 62.42524719238281 = 0.010795393958687782 + 10.0 * 6.241445064544678
Epoch 2020, val loss: 1.3122320175170898
Epoch 2030, training loss: 62.388275146484375 = 0.010654129087924957 + 10.0 * 6.237761974334717
Epoch 2030, val loss: 1.3154160976409912
Epoch 2040, training loss: 62.41184997558594 = 0.010516349226236343 + 10.0 * 6.240133285522461
Epoch 2040, val loss: 1.3185242414474487
Epoch 2050, training loss: 62.39479446411133 = 0.010378159582614899 + 10.0 * 6.238441467285156
Epoch 2050, val loss: 1.3213779926300049
Epoch 2060, training loss: 62.434654235839844 = 0.01024471316486597 + 10.0 * 6.242440700531006
Epoch 2060, val loss: 1.3246489763259888
Epoch 2070, training loss: 62.416961669921875 = 0.010114354081451893 + 10.0 * 6.240684986114502
Epoch 2070, val loss: 1.3275575637817383
Epoch 2080, training loss: 62.376129150390625 = 0.009981021285057068 + 10.0 * 6.236615180969238
Epoch 2080, val loss: 1.3302350044250488
Epoch 2090, training loss: 62.3734130859375 = 0.009859590791165829 + 10.0 * 6.236355304718018
Epoch 2090, val loss: 1.3332300186157227
Epoch 2100, training loss: 62.37510681152344 = 0.009740895591676235 + 10.0 * 6.236536502838135
Epoch 2100, val loss: 1.3360837697982788
Epoch 2110, training loss: 62.417694091796875 = 0.009622999466955662 + 10.0 * 6.240807056427002
Epoch 2110, val loss: 1.3387069702148438
Epoch 2120, training loss: 62.3931999206543 = 0.009500921703875065 + 10.0 * 6.238369941711426
Epoch 2120, val loss: 1.3418339490890503
Epoch 2130, training loss: 62.38748550415039 = 0.009384211152791977 + 10.0 * 6.237810134887695
Epoch 2130, val loss: 1.3447850942611694
Epoch 2140, training loss: 62.379859924316406 = 0.009270396083593369 + 10.0 * 6.237059116363525
Epoch 2140, val loss: 1.3473150730133057
Epoch 2150, training loss: 62.359397888183594 = 0.00915868952870369 + 10.0 * 6.2350239753723145
Epoch 2150, val loss: 1.35049307346344
Epoch 2160, training loss: 62.36290740966797 = 0.009051998145878315 + 10.0 * 6.235385417938232
Epoch 2160, val loss: 1.3532686233520508
Epoch 2170, training loss: 62.39640808105469 = 0.00894829723984003 + 10.0 * 6.23874568939209
Epoch 2170, val loss: 1.3559156656265259
Epoch 2180, training loss: 62.38819885253906 = 0.008841276168823242 + 10.0 * 6.237935543060303
Epoch 2180, val loss: 1.3586759567260742
Epoch 2190, training loss: 62.37623596191406 = 0.008736535906791687 + 10.0 * 6.23675012588501
Epoch 2190, val loss: 1.3613100051879883
Epoch 2200, training loss: 62.35139846801758 = 0.008632069453597069 + 10.0 * 6.23427677154541
Epoch 2200, val loss: 1.3639065027236938
Epoch 2210, training loss: 62.369483947753906 = 0.008537979796528816 + 10.0 * 6.2360944747924805
Epoch 2210, val loss: 1.3664493560791016
Epoch 2220, training loss: 62.373687744140625 = 0.008439776487648487 + 10.0 * 6.236525058746338
Epoch 2220, val loss: 1.3695178031921387
Epoch 2230, training loss: 62.35273742675781 = 0.00833951961249113 + 10.0 * 6.234439849853516
Epoch 2230, val loss: 1.371893286705017
Epoch 2240, training loss: 62.358360290527344 = 0.008247027173638344 + 10.0 * 6.235011100769043
Epoch 2240, val loss: 1.374660611152649
Epoch 2250, training loss: 62.34375762939453 = 0.008156134746968746 + 10.0 * 6.233560085296631
Epoch 2250, val loss: 1.3772567510604858
Epoch 2260, training loss: 62.32850646972656 = 0.008067241869866848 + 10.0 * 6.232043743133545
Epoch 2260, val loss: 1.3796789646148682
Epoch 2270, training loss: 62.35140609741211 = 0.007981933653354645 + 10.0 * 6.234342575073242
Epoch 2270, val loss: 1.3823736906051636
Epoch 2280, training loss: 62.35223388671875 = 0.007891503162682056 + 10.0 * 6.234434127807617
Epoch 2280, val loss: 1.3845903873443604
Epoch 2290, training loss: 62.34649658203125 = 0.007804872468113899 + 10.0 * 6.2338690757751465
Epoch 2290, val loss: 1.386963129043579
Epoch 2300, training loss: 62.405975341796875 = 0.007721587084233761 + 10.0 * 6.239825248718262
Epoch 2300, val loss: 1.389501929283142
Epoch 2310, training loss: 62.33330535888672 = 0.007636731490492821 + 10.0 * 6.232566833496094
Epoch 2310, val loss: 1.392129898071289
Epoch 2320, training loss: 62.32256317138672 = 0.007555683143436909 + 10.0 * 6.231500625610352
Epoch 2320, val loss: 1.3943099975585938
Epoch 2330, training loss: 62.38535690307617 = 0.007480050902813673 + 10.0 * 6.23778772354126
Epoch 2330, val loss: 1.3967219591140747
Epoch 2340, training loss: 62.31547546386719 = 0.007397748529911041 + 10.0 * 6.230807781219482
Epoch 2340, val loss: 1.399437665939331
Epoch 2350, training loss: 62.309452056884766 = 0.007321398239582777 + 10.0 * 6.230213165283203
Epoch 2350, val loss: 1.4018000364303589
Epoch 2360, training loss: 62.309871673583984 = 0.007248337380588055 + 10.0 * 6.230262279510498
Epoch 2360, val loss: 1.4040974378585815
Epoch 2370, training loss: 62.317073822021484 = 0.007175664883106947 + 10.0 * 6.230989933013916
Epoch 2370, val loss: 1.4063901901245117
Epoch 2380, training loss: 62.36001205444336 = 0.007102890405803919 + 10.0 * 6.235291004180908
Epoch 2380, val loss: 1.4086840152740479
Epoch 2390, training loss: 62.346466064453125 = 0.007032278925180435 + 10.0 * 6.233943462371826
Epoch 2390, val loss: 1.4110840559005737
Epoch 2400, training loss: 62.325679779052734 = 0.006958179175853729 + 10.0 * 6.231872081756592
Epoch 2400, val loss: 1.4130603075027466
Epoch 2410, training loss: 62.315643310546875 = 0.006890146527439356 + 10.0 * 6.230875492095947
Epoch 2410, val loss: 1.4154413938522339
Epoch 2420, training loss: 62.31732177734375 = 0.00682159373536706 + 10.0 * 6.23105001449585
Epoch 2420, val loss: 1.4175716638565063
Epoch 2430, training loss: 62.28813934326172 = 0.0067550926469266415 + 10.0 * 6.228138446807861
Epoch 2430, val loss: 1.4201692342758179
Epoch 2440, training loss: 62.295654296875 = 0.006692214868962765 + 10.0 * 6.228896141052246
Epoch 2440, val loss: 1.4225844144821167
Epoch 2450, training loss: 62.345855712890625 = 0.00663051288574934 + 10.0 * 6.233922481536865
Epoch 2450, val loss: 1.4248223304748535
Epoch 2460, training loss: 62.341468811035156 = 0.00656453613191843 + 10.0 * 6.233490467071533
Epoch 2460, val loss: 1.426557183265686
Epoch 2470, training loss: 62.31031799316406 = 0.0064987498335540295 + 10.0 * 6.230381965637207
Epoch 2470, val loss: 1.4288666248321533
Epoch 2480, training loss: 62.29751968383789 = 0.00643384363502264 + 10.0 * 6.2291083335876465
Epoch 2480, val loss: 1.4306601285934448
Epoch 2490, training loss: 62.293739318847656 = 0.00637581804767251 + 10.0 * 6.228736400604248
Epoch 2490, val loss: 1.4330466985702515
Epoch 2500, training loss: 62.32317352294922 = 0.006319232750684023 + 10.0 * 6.231685161590576
Epoch 2500, val loss: 1.4348385334014893
Epoch 2510, training loss: 62.29141616821289 = 0.0062592304311692715 + 10.0 * 6.228515625
Epoch 2510, val loss: 1.4374362230300903
Epoch 2520, training loss: 62.2982177734375 = 0.006200834643095732 + 10.0 * 6.229201316833496
Epoch 2520, val loss: 1.4396014213562012
Epoch 2530, training loss: 62.31929397583008 = 0.006142903119325638 + 10.0 * 6.2313151359558105
Epoch 2530, val loss: 1.4414247274398804
Epoch 2540, training loss: 62.28519821166992 = 0.006086753681302071 + 10.0 * 6.227910995483398
Epoch 2540, val loss: 1.4433737993240356
Epoch 2550, training loss: 62.268699645996094 = 0.006032697856426239 + 10.0 * 6.226266860961914
Epoch 2550, val loss: 1.445494532585144
Epoch 2560, training loss: 62.274505615234375 = 0.005980710964649916 + 10.0 * 6.2268524169921875
Epoch 2560, val loss: 1.4476935863494873
Epoch 2570, training loss: 62.372657775878906 = 0.005928620230406523 + 10.0 * 6.236672878265381
Epoch 2570, val loss: 1.4495744705200195
Epoch 2580, training loss: 62.30365753173828 = 0.005873889662325382 + 10.0 * 6.229778289794922
Epoch 2580, val loss: 1.4511438608169556
Epoch 2590, training loss: 62.28165054321289 = 0.005821166560053825 + 10.0 * 6.227582931518555
Epoch 2590, val loss: 1.4534660577774048
Epoch 2600, training loss: 62.28936767578125 = 0.005772335454821587 + 10.0 * 6.228359699249268
Epoch 2600, val loss: 1.4553571939468384
Epoch 2610, training loss: 62.266632080078125 = 0.00572183495387435 + 10.0 * 6.226090908050537
Epoch 2610, val loss: 1.4575855731964111
Epoch 2620, training loss: 62.27519226074219 = 0.005673946347087622 + 10.0 * 6.226952075958252
Epoch 2620, val loss: 1.4596446752548218
Epoch 2630, training loss: 62.31468200683594 = 0.005627352744340897 + 10.0 * 6.230905532836914
Epoch 2630, val loss: 1.461521863937378
Epoch 2640, training loss: 62.292823791503906 = 0.005575780291110277 + 10.0 * 6.228724479675293
Epoch 2640, val loss: 1.4628337621688843
Epoch 2650, training loss: 62.25680160522461 = 0.005526680499315262 + 10.0 * 6.225127220153809
Epoch 2650, val loss: 1.4649072885513306
Epoch 2660, training loss: 62.251834869384766 = 0.005481148138642311 + 10.0 * 6.224635124206543
Epoch 2660, val loss: 1.466727375984192
Epoch 2670, training loss: 62.26102066040039 = 0.005437353625893593 + 10.0 * 6.225558280944824
Epoch 2670, val loss: 1.4688719511032104
Epoch 2680, training loss: 62.296974182128906 = 0.005393147934228182 + 10.0 * 6.2291579246521
Epoch 2680, val loss: 1.4705610275268555
Epoch 2690, training loss: 62.30213928222656 = 0.005350658670067787 + 10.0 * 6.229678630828857
Epoch 2690, val loss: 1.4723526239395142
Epoch 2700, training loss: 62.25194549560547 = 0.00530280452221632 + 10.0 * 6.224664211273193
Epoch 2700, val loss: 1.4740667343139648
Epoch 2710, training loss: 62.24992370605469 = 0.005262626800686121 + 10.0 * 6.224465847015381
Epoch 2710, val loss: 1.4760596752166748
Epoch 2720, training loss: 62.28911209106445 = 0.005221128463745117 + 10.0 * 6.228388786315918
Epoch 2720, val loss: 1.4778047800064087
Epoch 2730, training loss: 62.2424201965332 = 0.005176063627004623 + 10.0 * 6.223724365234375
Epoch 2730, val loss: 1.479391098022461
Epoch 2740, training loss: 62.26504135131836 = 0.005136601626873016 + 10.0 * 6.225990295410156
Epoch 2740, val loss: 1.4810758829116821
Epoch 2750, training loss: 62.247344970703125 = 0.0050956024788320065 + 10.0 * 6.224225044250488
Epoch 2750, val loss: 1.4829758405685425
Epoch 2760, training loss: 62.24406433105469 = 0.005057059694081545 + 10.0 * 6.22390079498291
Epoch 2760, val loss: 1.484718680381775
Epoch 2770, training loss: 62.26887512207031 = 0.0050190226174890995 + 10.0 * 6.226385593414307
Epoch 2770, val loss: 1.4862865209579468
Epoch 2780, training loss: 62.26784896850586 = 0.00497914245352149 + 10.0 * 6.226286888122559
Epoch 2780, val loss: 1.4883990287780762
Epoch 2790, training loss: 62.282989501953125 = 0.004937832243740559 + 10.0 * 6.227805137634277
Epoch 2790, val loss: 1.4894579648971558
Epoch 2800, training loss: 62.247337341308594 = 0.00489912461489439 + 10.0 * 6.224244117736816
Epoch 2800, val loss: 1.4911741018295288
Epoch 2810, training loss: 62.226375579833984 = 0.004862647969275713 + 10.0 * 6.222151279449463
Epoch 2810, val loss: 1.4929448366165161
Epoch 2820, training loss: 62.222633361816406 = 0.004829008132219315 + 10.0 * 6.221780300140381
Epoch 2820, val loss: 1.4947588443756104
Epoch 2830, training loss: 62.22688674926758 = 0.004794352687895298 + 10.0 * 6.2222089767456055
Epoch 2830, val loss: 1.496433973312378
Epoch 2840, training loss: 62.30586242675781 = 0.004762039985507727 + 10.0 * 6.230110168457031
Epoch 2840, val loss: 1.4982722997665405
Epoch 2850, training loss: 62.27647018432617 = 0.0047208527103066444 + 10.0 * 6.227174758911133
Epoch 2850, val loss: 1.498892068862915
Epoch 2860, training loss: 62.235443115234375 = 0.004685237072408199 + 10.0 * 6.223075866699219
Epoch 2860, val loss: 1.50090491771698
Epoch 2870, training loss: 62.22489929199219 = 0.004651823081076145 + 10.0 * 6.222024917602539
Epoch 2870, val loss: 1.5022530555725098
Epoch 2880, training loss: 62.26831817626953 = 0.004620142746716738 + 10.0 * 6.226369857788086
Epoch 2880, val loss: 1.504098892211914
Epoch 2890, training loss: 62.225746154785156 = 0.004584700334817171 + 10.0 * 6.222115993499756
Epoch 2890, val loss: 1.5053507089614868
Epoch 2900, training loss: 62.22166061401367 = 0.004552590660750866 + 10.0 * 6.221711158752441
Epoch 2900, val loss: 1.5070070028305054
Epoch 2910, training loss: 62.25225830078125 = 0.004521464928984642 + 10.0 * 6.22477388381958
Epoch 2910, val loss: 1.508070707321167
Epoch 2920, training loss: 62.2700309753418 = 0.0044891489669680595 + 10.0 * 6.226553916931152
Epoch 2920, val loss: 1.509844422340393
Epoch 2930, training loss: 62.216487884521484 = 0.004453984554857016 + 10.0 * 6.221203327178955
Epoch 2930, val loss: 1.511582374572754
Epoch 2940, training loss: 62.203857421875 = 0.004423837643116713 + 10.0 * 6.219943046569824
Epoch 2940, val loss: 1.5128905773162842
Epoch 2950, training loss: 62.20429229736328 = 0.00439482694491744 + 10.0 * 6.219989776611328
Epoch 2950, val loss: 1.514452338218689
Epoch 2960, training loss: 62.24216079711914 = 0.004367550369352102 + 10.0 * 6.223779201507568
Epoch 2960, val loss: 1.516018271446228
Epoch 2970, training loss: 62.21099853515625 = 0.004333954304456711 + 10.0 * 6.220666408538818
Epoch 2970, val loss: 1.5173392295837402
Epoch 2980, training loss: 62.205169677734375 = 0.004303591325879097 + 10.0 * 6.220086574554443
Epoch 2980, val loss: 1.518713116645813
Epoch 2990, training loss: 62.210628509521484 = 0.00427526468411088 + 10.0 * 6.220635414123535
Epoch 2990, val loss: 1.5201071500778198
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8202424881391671
=== training gcn model ===
Epoch 0, training loss: 87.9366683959961 = 1.9683828353881836 + 10.0 * 8.59682846069336
Epoch 0, val loss: 1.963616967201233
Epoch 10, training loss: 87.91915893554688 = 1.9572539329528809 + 10.0 * 8.596190452575684
Epoch 10, val loss: 1.953176498413086
Epoch 20, training loss: 87.85851287841797 = 1.9438120126724243 + 10.0 * 8.591470718383789
Epoch 20, val loss: 1.9401323795318604
Epoch 30, training loss: 87.53160095214844 = 1.9271109104156494 + 10.0 * 8.560449600219727
Epoch 30, val loss: 1.9237313270568848
Epoch 40, training loss: 85.81830596923828 = 1.9093044996261597 + 10.0 * 8.390900611877441
Epoch 40, val loss: 1.9067658185958862
Epoch 50, training loss: 81.7940444946289 = 1.890193223953247 + 10.0 * 7.99038553237915
Epoch 50, val loss: 1.8881722688674927
Epoch 60, training loss: 76.88597869873047 = 1.8745331764221191 + 10.0 * 7.5011444091796875
Epoch 60, val loss: 1.8730500936508179
Epoch 70, training loss: 73.65553283691406 = 1.8606696128845215 + 10.0 * 7.179486274719238
Epoch 70, val loss: 1.8598147630691528
Epoch 80, training loss: 72.13489532470703 = 1.8484760522842407 + 10.0 * 7.028642654418945
Epoch 80, val loss: 1.8481498956680298
Epoch 90, training loss: 71.03022003173828 = 1.8363419771194458 + 10.0 * 6.9193878173828125
Epoch 90, val loss: 1.836775541305542
Epoch 100, training loss: 70.12909698486328 = 1.82461678981781 + 10.0 * 6.830448150634766
Epoch 100, val loss: 1.8261475563049316
Epoch 110, training loss: 69.45970916748047 = 1.813661813735962 + 10.0 * 6.7646050453186035
Epoch 110, val loss: 1.8165762424468994
Epoch 120, training loss: 68.9439926147461 = 1.803525686264038 + 10.0 * 6.714046478271484
Epoch 120, val loss: 1.8075156211853027
Epoch 130, training loss: 68.53083801269531 = 1.7935563325881958 + 10.0 * 6.6737284660339355
Epoch 130, val loss: 1.7986032962799072
Epoch 140, training loss: 68.20142364501953 = 1.7835590839385986 + 10.0 * 6.641786575317383
Epoch 140, val loss: 1.789689302444458
Epoch 150, training loss: 67.92340087890625 = 1.7733536958694458 + 10.0 * 6.615005016326904
Epoch 150, val loss: 1.780540943145752
Epoch 160, training loss: 67.63219451904297 = 1.7627394199371338 + 10.0 * 6.586945533752441
Epoch 160, val loss: 1.7711683511734009
Epoch 170, training loss: 67.38581848144531 = 1.7514537572860718 + 10.0 * 6.563436031341553
Epoch 170, val loss: 1.761322021484375
Epoch 180, training loss: 67.19718170166016 = 1.7391314506530762 + 10.0 * 6.54580545425415
Epoch 180, val loss: 1.750745415687561
Epoch 190, training loss: 66.97099304199219 = 1.7257400751113892 + 10.0 * 6.524525165557861
Epoch 190, val loss: 1.7393065690994263
Epoch 200, training loss: 66.81694030761719 = 1.711033821105957 + 10.0 * 6.510590553283691
Epoch 200, val loss: 1.7268654108047485
Epoch 210, training loss: 66.65281677246094 = 1.6948212385177612 + 10.0 * 6.495799541473389
Epoch 210, val loss: 1.7133595943450928
Epoch 220, training loss: 66.52293395996094 = 1.6771860122680664 + 10.0 * 6.484575271606445
Epoch 220, val loss: 1.6985887289047241
Epoch 230, training loss: 66.39253234863281 = 1.657938838005066 + 10.0 * 6.473459720611572
Epoch 230, val loss: 1.6826785802841187
Epoch 240, training loss: 66.25404357910156 = 1.6371599435806274 + 10.0 * 6.461688041687012
Epoch 240, val loss: 1.66545569896698
Epoch 250, training loss: 66.14795684814453 = 1.614879846572876 + 10.0 * 6.453307151794434
Epoch 250, val loss: 1.6471967697143555
Epoch 260, training loss: 66.05851745605469 = 1.5911468267440796 + 10.0 * 6.446736812591553
Epoch 260, val loss: 1.627915859222412
Epoch 270, training loss: 65.9533920288086 = 1.5662120580673218 + 10.0 * 6.438717365264893
Epoch 270, val loss: 1.607718825340271
Epoch 280, training loss: 65.84813690185547 = 1.5400506258010864 + 10.0 * 6.430808067321777
Epoch 280, val loss: 1.5870622396469116
Epoch 290, training loss: 65.74845886230469 = 1.5132217407226562 + 10.0 * 6.423523426055908
Epoch 290, val loss: 1.5660113096237183
Epoch 300, training loss: 65.66323852539062 = 1.485797643661499 + 10.0 * 6.417744159698486
Epoch 300, val loss: 1.5450278520584106
Epoch 310, training loss: 65.59764862060547 = 1.4581387042999268 + 10.0 * 6.413951396942139
Epoch 310, val loss: 1.5241957902908325
Epoch 320, training loss: 65.48822784423828 = 1.430550217628479 + 10.0 * 6.405767440795898
Epoch 320, val loss: 1.5040256977081299
Epoch 330, training loss: 65.41534423828125 = 1.4032570123672485 + 10.0 * 6.401208400726318
Epoch 330, val loss: 1.4846640825271606
Epoch 340, training loss: 65.34009552001953 = 1.376339077949524 + 10.0 * 6.3963751792907715
Epoch 340, val loss: 1.4662340879440308
Epoch 350, training loss: 65.26070404052734 = 1.3500264883041382 + 10.0 * 6.3910675048828125
Epoch 350, val loss: 1.4486087560653687
Epoch 360, training loss: 65.18964385986328 = 1.3243622779846191 + 10.0 * 6.386528015136719
Epoch 360, val loss: 1.431902527809143
Epoch 370, training loss: 65.1258544921875 = 1.2992359399795532 + 10.0 * 6.382662296295166
Epoch 370, val loss: 1.4160109758377075
Epoch 380, training loss: 65.05101013183594 = 1.2745524644851685 + 10.0 * 6.377645969390869
Epoch 380, val loss: 1.4007201194763184
Epoch 390, training loss: 64.99005889892578 = 1.2502835988998413 + 10.0 * 6.373977184295654
Epoch 390, val loss: 1.3859405517578125
Epoch 400, training loss: 64.95265197753906 = 1.2262834310531616 + 10.0 * 6.3726372718811035
Epoch 400, val loss: 1.3714734315872192
Epoch 410, training loss: 64.88291931152344 = 1.202349066734314 + 10.0 * 6.3680572509765625
Epoch 410, val loss: 1.3572241067886353
Epoch 420, training loss: 64.86605072021484 = 1.1784952878952026 + 10.0 * 6.368755340576172
Epoch 420, val loss: 1.3429991006851196
Epoch 430, training loss: 64.75199890136719 = 1.1546720266342163 + 10.0 * 6.359732627868652
Epoch 430, val loss: 1.3286792039871216
Epoch 440, training loss: 64.69405364990234 = 1.1308274269104004 + 10.0 * 6.356322765350342
Epoch 440, val loss: 1.3143352270126343
Epoch 450, training loss: 64.66827392578125 = 1.1069916486740112 + 10.0 * 6.356127738952637
Epoch 450, val loss: 1.2999134063720703
Epoch 460, training loss: 64.6291732788086 = 1.0828810930252075 + 10.0 * 6.354629039764404
Epoch 460, val loss: 1.28551185131073
Epoch 470, training loss: 64.54125213623047 = 1.0589885711669922 + 10.0 * 6.348226547241211
Epoch 470, val loss: 1.2709202766418457
Epoch 480, training loss: 64.50238037109375 = 1.0351855754852295 + 10.0 * 6.346719741821289
Epoch 480, val loss: 1.2564359903335571
Epoch 490, training loss: 64.43626403808594 = 1.0115498304367065 + 10.0 * 6.342471599578857
Epoch 490, val loss: 1.2421945333480835
Epoch 500, training loss: 64.43816375732422 = 0.9880859851837158 + 10.0 * 6.34500789642334
Epoch 500, val loss: 1.2281005382537842
Epoch 510, training loss: 64.35269165039062 = 0.9649054408073425 + 10.0 * 6.338778495788574
Epoch 510, val loss: 1.2142547369003296
Epoch 520, training loss: 64.29230499267578 = 0.942115843296051 + 10.0 * 6.335018634796143
Epoch 520, val loss: 1.2008259296417236
Epoch 530, training loss: 64.24598693847656 = 0.9197360277175903 + 10.0 * 6.332624912261963
Epoch 530, val loss: 1.1878808736801147
Epoch 540, training loss: 64.33009338378906 = 0.8976892828941345 + 10.0 * 6.343240737915039
Epoch 540, val loss: 1.1751242876052856
Epoch 550, training loss: 64.20348358154297 = 0.8760454058647156 + 10.0 * 6.3327436447143555
Epoch 550, val loss: 1.1630173921585083
Epoch 560, training loss: 64.13286590576172 = 0.8549649119377136 + 10.0 * 6.327790260314941
Epoch 560, val loss: 1.1515698432922363
Epoch 570, training loss: 64.08219146728516 = 0.8344282507896423 + 10.0 * 6.3247761726379395
Epoch 570, val loss: 1.140751838684082
Epoch 580, training loss: 64.0796890258789 = 0.8143622279167175 + 10.0 * 6.326532363891602
Epoch 580, val loss: 1.1304059028625488
Epoch 590, training loss: 64.07412719726562 = 0.7945076823234558 + 10.0 * 6.3279619216918945
Epoch 590, val loss: 1.1205270290374756
Epoch 600, training loss: 63.99214553833008 = 0.7752160429954529 + 10.0 * 6.321692943572998
Epoch 600, val loss: 1.1112494468688965
Epoch 610, training loss: 63.93747329711914 = 0.7564282417297363 + 10.0 * 6.3181047439575195
Epoch 610, val loss: 1.1025311946868896
Epoch 620, training loss: 63.90079116821289 = 0.738070011138916 + 10.0 * 6.316271781921387
Epoch 620, val loss: 1.0944660902023315
Epoch 630, training loss: 63.8975830078125 = 0.7199944853782654 + 10.0 * 6.317759037017822
Epoch 630, val loss: 1.0867241621017456
Epoch 640, training loss: 63.84001922607422 = 0.7022397518157959 + 10.0 * 6.313777923583984
Epoch 640, val loss: 1.0794975757598877
Epoch 650, training loss: 63.86528396606445 = 0.6848251223564148 + 10.0 * 6.318045616149902
Epoch 650, val loss: 1.072744607925415
Epoch 660, training loss: 63.77519226074219 = 0.6675918102264404 + 10.0 * 6.310760021209717
Epoch 660, val loss: 1.066546082496643
Epoch 670, training loss: 63.730613708496094 = 0.6507852673530579 + 10.0 * 6.307982921600342
Epoch 670, val loss: 1.0609246492385864
Epoch 680, training loss: 63.69919967651367 = 0.6343215703964233 + 10.0 * 6.306487560272217
Epoch 680, val loss: 1.0558125972747803
Epoch 690, training loss: 63.73668670654297 = 0.6180728077888489 + 10.0 * 6.311861515045166
Epoch 690, val loss: 1.0510599613189697
Epoch 700, training loss: 63.66865158081055 = 0.6020458936691284 + 10.0 * 6.3066606521606445
Epoch 700, val loss: 1.0468556880950928
Epoch 710, training loss: 63.612060546875 = 0.5863045454025269 + 10.0 * 6.302575588226318
Epoch 710, val loss: 1.043094277381897
Epoch 720, training loss: 63.6078987121582 = 0.5709006786346436 + 10.0 * 6.303699970245361
Epoch 720, val loss: 1.0400274991989136
Epoch 730, training loss: 63.59299850463867 = 0.5555921792984009 + 10.0 * 6.303740501403809
Epoch 730, val loss: 1.0370815992355347
Epoch 740, training loss: 63.534698486328125 = 0.5406494736671448 + 10.0 * 6.299405097961426
Epoch 740, val loss: 1.0348354578018188
Epoch 750, training loss: 63.52745819091797 = 0.5260076522827148 + 10.0 * 6.300145149230957
Epoch 750, val loss: 1.0330352783203125
Epoch 760, training loss: 63.47498321533203 = 0.5115765333175659 + 10.0 * 6.296340465545654
Epoch 760, val loss: 1.0317461490631104
Epoch 770, training loss: 63.51017379760742 = 0.49740365147590637 + 10.0 * 6.301277160644531
Epoch 770, val loss: 1.0308787822723389
Epoch 780, training loss: 63.45086669921875 = 0.48349252343177795 + 10.0 * 6.296737194061279
Epoch 780, val loss: 1.0301597118377686
Epoch 790, training loss: 63.41862487792969 = 0.46978628635406494 + 10.0 * 6.294883728027344
Epoch 790, val loss: 1.0302472114562988
Epoch 800, training loss: 63.41584014892578 = 0.4564061462879181 + 10.0 * 6.295943260192871
Epoch 800, val loss: 1.0306122303009033
Epoch 810, training loss: 63.360618591308594 = 0.44319358468055725 + 10.0 * 6.291742324829102
Epoch 810, val loss: 1.0312873125076294
Epoch 820, training loss: 63.340553283691406 = 0.43030494451522827 + 10.0 * 6.291024684906006
Epoch 820, val loss: 1.032454252243042
Epoch 830, training loss: 63.3721923828125 = 0.41760966181755066 + 10.0 * 6.2954583168029785
Epoch 830, val loss: 1.0340700149536133
Epoch 840, training loss: 63.33605194091797 = 0.4051230847835541 + 10.0 * 6.293092727661133
Epoch 840, val loss: 1.0356647968292236
Epoch 850, training loss: 63.26137161254883 = 0.392894983291626 + 10.0 * 6.2868475914001465
Epoch 850, val loss: 1.0379564762115479
Epoch 860, training loss: 63.24442672729492 = 0.38097020983695984 + 10.0 * 6.286345481872559
Epoch 860, val loss: 1.0406140089035034
Epoch 870, training loss: 63.24324417114258 = 0.3693060874938965 + 10.0 * 6.287394046783447
Epoch 870, val loss: 1.0435924530029297
Epoch 880, training loss: 63.202579498291016 = 0.35780569911003113 + 10.0 * 6.284477233886719
Epoch 880, val loss: 1.0466699600219727
Epoch 890, training loss: 63.18811798095703 = 0.34655457735061646 + 10.0 * 6.284156322479248
Epoch 890, val loss: 1.0501928329467773
Epoch 900, training loss: 63.18239212036133 = 0.33557969331741333 + 10.0 * 6.28468132019043
Epoch 900, val loss: 1.0541611909866333
Epoch 910, training loss: 63.1492919921875 = 0.3248320519924164 + 10.0 * 6.282445907592773
Epoch 910, val loss: 1.0581167936325073
Epoch 920, training loss: 63.17540740966797 = 0.31435340642929077 + 10.0 * 6.286105155944824
Epoch 920, val loss: 1.0625123977661133
Epoch 930, training loss: 63.10955047607422 = 0.304141104221344 + 10.0 * 6.280540943145752
Epoch 930, val loss: 1.0670981407165527
Epoch 940, training loss: 63.0802116394043 = 0.29419368505477905 + 10.0 * 6.27860164642334
Epoch 940, val loss: 1.0722488164901733
Epoch 950, training loss: 63.07033920288086 = 0.28454676270484924 + 10.0 * 6.278579235076904
Epoch 950, val loss: 1.0775316953659058
Epoch 960, training loss: 63.081260681152344 = 0.27509361505508423 + 10.0 * 6.280616760253906
Epoch 960, val loss: 1.0828094482421875
Epoch 970, training loss: 63.04244613647461 = 0.2658582627773285 + 10.0 * 6.277658939361572
Epoch 970, val loss: 1.088613510131836
Epoch 980, training loss: 63.04863739013672 = 0.2569345235824585 + 10.0 * 6.279170036315918
Epoch 980, val loss: 1.0947628021240234
Epoch 990, training loss: 62.99991989135742 = 0.2482953816652298 + 10.0 * 6.275162696838379
Epoch 990, val loss: 1.1009058952331543
Epoch 1000, training loss: 62.988651275634766 = 0.23994877934455872 + 10.0 * 6.2748703956604
Epoch 1000, val loss: 1.107475757598877
Epoch 1010, training loss: 63.0285530090332 = 0.23185274004936218 + 10.0 * 6.279669761657715
Epoch 1010, val loss: 1.1143879890441895
Epoch 1020, training loss: 62.985015869140625 = 0.22392942011356354 + 10.0 * 6.276108741760254
Epoch 1020, val loss: 1.1208513975143433
Epoch 1030, training loss: 62.952728271484375 = 0.21635177731513977 + 10.0 * 6.273637771606445
Epoch 1030, val loss: 1.127881407737732
Epoch 1040, training loss: 62.97183609008789 = 0.20900870859622955 + 10.0 * 6.276282787322998
Epoch 1040, val loss: 1.135075330734253
Epoch 1050, training loss: 62.9226188659668 = 0.2018997073173523 + 10.0 * 6.272071838378906
Epoch 1050, val loss: 1.1424684524536133
Epoch 1060, training loss: 62.9008903503418 = 0.19505135715007782 + 10.0 * 6.270583629608154
Epoch 1060, val loss: 1.1499426364898682
Epoch 1070, training loss: 62.90094757080078 = 0.18846505880355835 + 10.0 * 6.2712483406066895
Epoch 1070, val loss: 1.1575751304626465
Epoch 1080, training loss: 62.88275146484375 = 0.1820758879184723 + 10.0 * 6.2700676918029785
Epoch 1080, val loss: 1.1655503511428833
Epoch 1090, training loss: 62.89250564575195 = 0.1759149730205536 + 10.0 * 6.271658897399902
Epoch 1090, val loss: 1.17375648021698
Epoch 1100, training loss: 62.84869384765625 = 0.16995756328105927 + 10.0 * 6.267873764038086
Epoch 1100, val loss: 1.1814769506454468
Epoch 1110, training loss: 62.835960388183594 = 0.16424135863780975 + 10.0 * 6.267171859741211
Epoch 1110, val loss: 1.1900283098220825
Epoch 1120, training loss: 62.88467788696289 = 0.15871594846248627 + 10.0 * 6.27259635925293
Epoch 1120, val loss: 1.1983829736709595
Epoch 1130, training loss: 62.831138610839844 = 0.15338236093521118 + 10.0 * 6.267775535583496
Epoch 1130, val loss: 1.2066389322280884
Epoch 1140, training loss: 62.8152961730957 = 0.14824138581752777 + 10.0 * 6.266705513000488
Epoch 1140, val loss: 1.2153757810592651
Epoch 1150, training loss: 62.82106399536133 = 0.1432936042547226 + 10.0 * 6.267776966094971
Epoch 1150, val loss: 1.223812460899353
Epoch 1160, training loss: 62.790042877197266 = 0.1385284960269928 + 10.0 * 6.265151500701904
Epoch 1160, val loss: 1.2327607870101929
Epoch 1170, training loss: 62.77328872680664 = 0.13394658267498016 + 10.0 * 6.263934135437012
Epoch 1170, val loss: 1.241478681564331
Epoch 1180, training loss: 62.79783630371094 = 0.12952657043933868 + 10.0 * 6.266830921173096
Epoch 1180, val loss: 1.2503585815429688
Epoch 1190, training loss: 62.78612518310547 = 0.1252550184726715 + 10.0 * 6.266087055206299
Epoch 1190, val loss: 1.2591551542282104
Epoch 1200, training loss: 62.73102569580078 = 0.12112477421760559 + 10.0 * 6.260990142822266
Epoch 1200, val loss: 1.2681738138198853
Epoch 1210, training loss: 62.72771072387695 = 0.11719086021184921 + 10.0 * 6.261052131652832
Epoch 1210, val loss: 1.277130126953125
Epoch 1220, training loss: 62.75865173339844 = 0.11341895163059235 + 10.0 * 6.264523506164551
Epoch 1220, val loss: 1.2864476442337036
Epoch 1230, training loss: 62.718055725097656 = 0.10973405092954636 + 10.0 * 6.2608323097229
Epoch 1230, val loss: 1.2950425148010254
Epoch 1240, training loss: 62.70767593383789 = 0.10622032731771469 + 10.0 * 6.260145664215088
Epoch 1240, val loss: 1.304122805595398
Epoch 1250, training loss: 62.71479034423828 = 0.10285007953643799 + 10.0 * 6.261193752288818
Epoch 1250, val loss: 1.3133803606033325
Epoch 1260, training loss: 62.689735412597656 = 0.09957735985517502 + 10.0 * 6.2590155601501465
Epoch 1260, val loss: 1.3220770359039307
Epoch 1270, training loss: 62.6740608215332 = 0.09643928706645966 + 10.0 * 6.2577619552612305
Epoch 1270, val loss: 1.3310908079147339
Epoch 1280, training loss: 62.66556167602539 = 0.09344136714935303 + 10.0 * 6.257212162017822
Epoch 1280, val loss: 1.3403578996658325
Epoch 1290, training loss: 62.68311309814453 = 0.09054899960756302 + 10.0 * 6.259256362915039
Epoch 1290, val loss: 1.3490879535675049
Epoch 1300, training loss: 62.649574279785156 = 0.0877426341176033 + 10.0 * 6.25618314743042
Epoch 1300, val loss: 1.3581093549728394
Epoch 1310, training loss: 62.65082550048828 = 0.085035540163517 + 10.0 * 6.2565789222717285
Epoch 1310, val loss: 1.3664274215698242
Epoch 1320, training loss: 62.7015495300293 = 0.0824727788567543 + 10.0 * 6.261907577514648
Epoch 1320, val loss: 1.375334620475769
Epoch 1330, training loss: 62.63588333129883 = 0.079959936439991 + 10.0 * 6.255592346191406
Epoch 1330, val loss: 1.3844220638275146
Epoch 1340, training loss: 62.62328338623047 = 0.07757484167814255 + 10.0 * 6.254570960998535
Epoch 1340, val loss: 1.392856478691101
Epoch 1350, training loss: 62.6576042175293 = 0.07528353482484818 + 10.0 * 6.258232116699219
Epoch 1350, val loss: 1.4016367197036743
Epoch 1360, training loss: 62.61802291870117 = 0.07304617017507553 + 10.0 * 6.254497528076172
Epoch 1360, val loss: 1.409990906715393
Epoch 1370, training loss: 62.61161804199219 = 0.07092065364122391 + 10.0 * 6.254069805145264
Epoch 1370, val loss: 1.4186182022094727
Epoch 1380, training loss: 62.6186408996582 = 0.06886395812034607 + 10.0 * 6.254977703094482
Epoch 1380, val loss: 1.426971673965454
Epoch 1390, training loss: 62.68330383300781 = 0.0668814405798912 + 10.0 * 6.261641979217529
Epoch 1390, val loss: 1.4350899457931519
Epoch 1400, training loss: 62.612464904785156 = 0.06494319438934326 + 10.0 * 6.254752159118652
Epoch 1400, val loss: 1.4439643621444702
Epoch 1410, training loss: 62.5774040222168 = 0.06310813874006271 + 10.0 * 6.251429557800293
Epoch 1410, val loss: 1.4519202709197998
Epoch 1420, training loss: 62.56118392944336 = 0.06135805323719978 + 10.0 * 6.2499823570251465
Epoch 1420, val loss: 1.4605945348739624
Epoch 1430, training loss: 62.56747055053711 = 0.059672124683856964 + 10.0 * 6.250779628753662
Epoch 1430, val loss: 1.4687950611114502
Epoch 1440, training loss: 62.5853271484375 = 0.05802006646990776 + 10.0 * 6.252730369567871
Epoch 1440, val loss: 1.4767158031463623
Epoch 1450, training loss: 62.5788688659668 = 0.05642358586192131 + 10.0 * 6.252244472503662
Epoch 1450, val loss: 1.484583854675293
Epoch 1460, training loss: 62.551536560058594 = 0.05489185079932213 + 10.0 * 6.249664306640625
Epoch 1460, val loss: 1.4929319620132446
Epoch 1470, training loss: 62.53751754760742 = 0.05342881754040718 + 10.0 * 6.248408794403076
Epoch 1470, val loss: 1.501037836074829
Epoch 1480, training loss: 62.55571746826172 = 0.05202297493815422 + 10.0 * 6.250369071960449
Epoch 1480, val loss: 1.5089759826660156
Epoch 1490, training loss: 62.5481071472168 = 0.05064816400408745 + 10.0 * 6.249745845794678
Epoch 1490, val loss: 1.516669511795044
Epoch 1500, training loss: 62.554603576660156 = 0.04932223632931709 + 10.0 * 6.250527858734131
Epoch 1500, val loss: 1.52436363697052
Epoch 1510, training loss: 62.55150604248047 = 0.04804228991270065 + 10.0 * 6.2503461837768555
Epoch 1510, val loss: 1.5321178436279297
Epoch 1520, training loss: 62.51327133178711 = 0.046809859573841095 + 10.0 * 6.246645927429199
Epoch 1520, val loss: 1.539992332458496
Epoch 1530, training loss: 62.508296966552734 = 0.045628614723682404 + 10.0 * 6.246266841888428
Epoch 1530, val loss: 1.5478055477142334
Epoch 1540, training loss: 62.529117584228516 = 0.04449640214443207 + 10.0 * 6.248462200164795
Epoch 1540, val loss: 1.5554308891296387
Epoch 1550, training loss: 62.49665832519531 = 0.043385718017816544 + 10.0 * 6.245327472686768
Epoch 1550, val loss: 1.5627294778823853
Epoch 1560, training loss: 62.55172348022461 = 0.04232412204146385 + 10.0 * 6.250939846038818
Epoch 1560, val loss: 1.5701192617416382
Epoch 1570, training loss: 62.4979133605957 = 0.04127909615635872 + 10.0 * 6.245663642883301
Epoch 1570, val loss: 1.5775604248046875
Epoch 1580, training loss: 62.48588943481445 = 0.040286075323820114 + 10.0 * 6.244560241699219
Epoch 1580, val loss: 1.5849307775497437
Epoch 1590, training loss: 62.49336242675781 = 0.03933176025748253 + 10.0 * 6.245402812957764
Epoch 1590, val loss: 1.5925086736679077
Epoch 1600, training loss: 62.48536682128906 = 0.03839781880378723 + 10.0 * 6.244696617126465
Epoch 1600, val loss: 1.5995880365371704
Epoch 1610, training loss: 62.54798889160156 = 0.03748980909585953 + 10.0 * 6.251049995422363
Epoch 1610, val loss: 1.6067397594451904
Epoch 1620, training loss: 62.48882293701172 = 0.03661158308386803 + 10.0 * 6.245221138000488
Epoch 1620, val loss: 1.6133145093917847
Epoch 1630, training loss: 62.475547790527344 = 0.035765159875154495 + 10.0 * 6.243978023529053
Epoch 1630, val loss: 1.62063729763031
Epoch 1640, training loss: 62.4733772277832 = 0.034953467547893524 + 10.0 * 6.243842124938965
Epoch 1640, val loss: 1.6275367736816406
Epoch 1650, training loss: 62.476871490478516 = 0.03416898101568222 + 10.0 * 6.244270324707031
Epoch 1650, val loss: 1.6344810724258423
Epoch 1660, training loss: 62.4580192565918 = 0.03340180218219757 + 10.0 * 6.242461681365967
Epoch 1660, val loss: 1.641465663909912
Epoch 1670, training loss: 62.465946197509766 = 0.03266812488436699 + 10.0 * 6.243327617645264
Epoch 1670, val loss: 1.6481245756149292
Epoch 1680, training loss: 62.44205093383789 = 0.03194821998476982 + 10.0 * 6.2410101890563965
Epoch 1680, val loss: 1.654895544052124
Epoch 1690, training loss: 62.44934844970703 = 0.03125806152820587 + 10.0 * 6.241808891296387
Epoch 1690, val loss: 1.6618561744689941
Epoch 1700, training loss: 62.45780944824219 = 0.03058554418385029 + 10.0 * 6.242722511291504
Epoch 1700, val loss: 1.6683207750320435
Epoch 1710, training loss: 62.47487258911133 = 0.02993275597691536 + 10.0 * 6.2444939613342285
Epoch 1710, val loss: 1.6744699478149414
Epoch 1720, training loss: 62.45222473144531 = 0.02929060533642769 + 10.0 * 6.242293357849121
Epoch 1720, val loss: 1.6811338663101196
Epoch 1730, training loss: 62.42369079589844 = 0.028673093765974045 + 10.0 * 6.239501953125
Epoch 1730, val loss: 1.6877626180648804
Epoch 1740, training loss: 62.42402648925781 = 0.028087755665183067 + 10.0 * 6.239593982696533
Epoch 1740, val loss: 1.6942570209503174
Epoch 1750, training loss: 62.447654724121094 = 0.027513055130839348 + 10.0 * 6.242014408111572
Epoch 1750, val loss: 1.7001615762710571
Epoch 1760, training loss: 62.43784713745117 = 0.026952337473630905 + 10.0 * 6.241089820861816
Epoch 1760, val loss: 1.7068519592285156
Epoch 1770, training loss: 62.41238784790039 = 0.02640364319086075 + 10.0 * 6.238598346710205
Epoch 1770, val loss: 1.7130765914916992
Epoch 1780, training loss: 62.43523406982422 = 0.025879260152578354 + 10.0 * 6.240935325622559
Epoch 1780, val loss: 1.7194514274597168
Epoch 1790, training loss: 62.423709869384766 = 0.025363877415657043 + 10.0 * 6.239834785461426
Epoch 1790, val loss: 1.7254266738891602
Epoch 1800, training loss: 62.399925231933594 = 0.02486574649810791 + 10.0 * 6.237505912780762
Epoch 1800, val loss: 1.7318542003631592
Epoch 1810, training loss: 62.40641403198242 = 0.024385426193475723 + 10.0 * 6.238203048706055
Epoch 1810, val loss: 1.7378655672073364
Epoch 1820, training loss: 62.412078857421875 = 0.023918114602565765 + 10.0 * 6.238816261291504
Epoch 1820, val loss: 1.744052529335022
Epoch 1830, training loss: 62.46289825439453 = 0.023455508053302765 + 10.0 * 6.24394416809082
Epoch 1830, val loss: 1.7493927478790283
Epoch 1840, training loss: 62.40105056762695 = 0.02300715446472168 + 10.0 * 6.237804412841797
Epoch 1840, val loss: 1.7553367614746094
Epoch 1850, training loss: 62.3807258605957 = 0.022575922310352325 + 10.0 * 6.235815048217773
Epoch 1850, val loss: 1.7613567113876343
Epoch 1860, training loss: 62.37188720703125 = 0.022165382280945778 + 10.0 * 6.23497200012207
Epoch 1860, val loss: 1.7671914100646973
Epoch 1870, training loss: 62.488277435302734 = 0.021765748038887978 + 10.0 * 6.2466511726379395
Epoch 1870, val loss: 1.7724034786224365
Epoch 1880, training loss: 62.41691589355469 = 0.021346932277083397 + 10.0 * 6.239556789398193
Epoch 1880, val loss: 1.7785255908966064
Epoch 1890, training loss: 62.36464309692383 = 0.020960360765457153 + 10.0 * 6.234368324279785
Epoch 1890, val loss: 1.7840824127197266
Epoch 1900, training loss: 62.35198974609375 = 0.02059117704629898 + 10.0 * 6.233139991760254
Epoch 1900, val loss: 1.7897164821624756
Epoch 1910, training loss: 62.367435455322266 = 0.02023320272564888 + 10.0 * 6.234720230102539
Epoch 1910, val loss: 1.7953805923461914
Epoch 1920, training loss: 62.38145065307617 = 0.019870705902576447 + 10.0 * 6.2361578941345215
Epoch 1920, val loss: 1.8004602193832397
Epoch 1930, training loss: 62.36421585083008 = 0.01951451785862446 + 10.0 * 6.234469890594482
Epoch 1930, val loss: 1.8059203624725342
Epoch 1940, training loss: 62.35347366333008 = 0.01917812041938305 + 10.0 * 6.233429908752441
Epoch 1940, val loss: 1.8113923072814941
Epoch 1950, training loss: 62.339813232421875 = 0.018855059519410133 + 10.0 * 6.232095718383789
Epoch 1950, val loss: 1.8169689178466797
Epoch 1960, training loss: 62.35310363769531 = 0.018542634323239326 + 10.0 * 6.233456134796143
Epoch 1960, val loss: 1.8222132921218872
Epoch 1970, training loss: 62.37311935424805 = 0.018228325992822647 + 10.0 * 6.235489368438721
Epoch 1970, val loss: 1.8271082639694214
Epoch 1980, training loss: 62.40753936767578 = 0.017916183918714523 + 10.0 * 6.238962173461914
Epoch 1980, val loss: 1.832480549812317
Epoch 1990, training loss: 62.33770751953125 = 0.017612338066101074 + 10.0 * 6.232009410858154
Epoch 1990, val loss: 1.8373615741729736
Epoch 2000, training loss: 62.330078125 = 0.01732366718351841 + 10.0 * 6.23127555847168
Epoch 2000, val loss: 1.8426990509033203
Epoch 2010, training loss: 62.34567642211914 = 0.01704614795744419 + 10.0 * 6.232862949371338
Epoch 2010, val loss: 1.847978115081787
Epoch 2020, training loss: 62.340354919433594 = 0.016768941655755043 + 10.0 * 6.232358455657959
Epoch 2020, val loss: 1.8526688814163208
Epoch 2030, training loss: 62.33879470825195 = 0.016500461846590042 + 10.0 * 6.232229709625244
Epoch 2030, val loss: 1.85757577419281
Epoch 2040, training loss: 62.35781478881836 = 0.016240213066339493 + 10.0 * 6.234157562255859
Epoch 2040, val loss: 1.8623733520507812
Epoch 2050, training loss: 62.313961029052734 = 0.01598307117819786 + 10.0 * 6.229797840118408
Epoch 2050, val loss: 1.8677233457565308
Epoch 2060, training loss: 62.32466506958008 = 0.01573812961578369 + 10.0 * 6.230892658233643
Epoch 2060, val loss: 1.8727035522460938
Epoch 2070, training loss: 62.36628723144531 = 0.015497706830501556 + 10.0 * 6.235078811645508
Epoch 2070, val loss: 1.8769924640655518
Epoch 2080, training loss: 62.33964157104492 = 0.01525132730603218 + 10.0 * 6.232439041137695
Epoch 2080, val loss: 1.8818796873092651
Epoch 2090, training loss: 62.30617141723633 = 0.015017705038189888 + 10.0 * 6.2291154861450195
Epoch 2090, val loss: 1.8867928981781006
Epoch 2100, training loss: 62.30167007446289 = 0.01479587983340025 + 10.0 * 6.228687286376953
Epoch 2100, val loss: 1.8913426399230957
Epoch 2110, training loss: 62.309696197509766 = 0.014580128714442253 + 10.0 * 6.229511737823486
Epoch 2110, val loss: 1.8960485458374023
Epoch 2120, training loss: 62.34377670288086 = 0.014365187846124172 + 10.0 * 6.232941150665283
Epoch 2120, val loss: 1.9002740383148193
Epoch 2130, training loss: 62.32325744628906 = 0.014149445109069347 + 10.0 * 6.230910778045654
Epoch 2130, val loss: 1.9051707983016968
Epoch 2140, training loss: 62.316898345947266 = 0.013942812569439411 + 10.0 * 6.230295658111572
Epoch 2140, val loss: 1.9097956418991089
Epoch 2150, training loss: 62.303977966308594 = 0.013740596361458302 + 10.0 * 6.2290239334106445
Epoch 2150, val loss: 1.914245843887329
Epoch 2160, training loss: 62.33677291870117 = 0.013547900132834911 + 10.0 * 6.2323222160339355
Epoch 2160, val loss: 1.9186128377914429
Epoch 2170, training loss: 62.28218078613281 = 0.013347913511097431 + 10.0 * 6.226883411407471
Epoch 2170, val loss: 1.9224742650985718
Epoch 2180, training loss: 62.28475570678711 = 0.013161744922399521 + 10.0 * 6.22715950012207
Epoch 2180, val loss: 1.926936388015747
Epoch 2190, training loss: 62.342811584472656 = 0.012979595921933651 + 10.0 * 6.232983112335205
Epoch 2190, val loss: 1.9311528205871582
Epoch 2200, training loss: 62.30272674560547 = 0.012799912132322788 + 10.0 * 6.228992938995361
Epoch 2200, val loss: 1.9354500770568848
Epoch 2210, training loss: 62.304378509521484 = 0.012619574554264545 + 10.0 * 6.229176044464111
Epoch 2210, val loss: 1.939520001411438
Epoch 2220, training loss: 62.27278518676758 = 0.012448828667402267 + 10.0 * 6.226033687591553
Epoch 2220, val loss: 1.9435847997665405
Epoch 2230, training loss: 62.27851486206055 = 0.012284144759178162 + 10.0 * 6.226623058319092
Epoch 2230, val loss: 1.9476280212402344
Epoch 2240, training loss: 62.29564666748047 = 0.012121767736971378 + 10.0 * 6.2283525466918945
Epoch 2240, val loss: 1.9514440298080444
Epoch 2250, training loss: 62.31154251098633 = 0.011960246600210667 + 10.0 * 6.229958534240723
Epoch 2250, val loss: 1.9556875228881836
Epoch 2260, training loss: 62.27553176879883 = 0.011797352693974972 + 10.0 * 6.226373195648193
Epoch 2260, val loss: 1.959884762763977
Epoch 2270, training loss: 62.269351959228516 = 0.011645429767668247 + 10.0 * 6.225770473480225
Epoch 2270, val loss: 1.9635781049728394
Epoch 2280, training loss: 62.272972106933594 = 0.01149696670472622 + 10.0 * 6.226147651672363
Epoch 2280, val loss: 1.9675413370132446
Epoch 2290, training loss: 62.301273345947266 = 0.011349794454872608 + 10.0 * 6.228992462158203
Epoch 2290, val loss: 1.97129225730896
Epoch 2300, training loss: 62.29023742675781 = 0.011201350018382072 + 10.0 * 6.227903842926025
Epoch 2300, val loss: 1.9748175144195557
Epoch 2310, training loss: 62.2693977355957 = 0.011058262549340725 + 10.0 * 6.225833892822266
Epoch 2310, val loss: 1.9789093732833862
Epoch 2320, training loss: 62.25188446044922 = 0.010921433568000793 + 10.0 * 6.224096298217773
Epoch 2320, val loss: 1.982713222503662
Epoch 2330, training loss: 62.2524528503418 = 0.010789032094180584 + 10.0 * 6.224166393280029
Epoch 2330, val loss: 1.9864476919174194
Epoch 2340, training loss: 62.28475570678711 = 0.010659174993634224 + 10.0 * 6.227409839630127
Epoch 2340, val loss: 1.9899972677230835
Epoch 2350, training loss: 62.26695251464844 = 0.0105257174000144 + 10.0 * 6.225642681121826
Epoch 2350, val loss: 1.9935492277145386
Epoch 2360, training loss: 62.257293701171875 = 0.010395466350018978 + 10.0 * 6.224689960479736
Epoch 2360, val loss: 1.9972859621047974
Epoch 2370, training loss: 62.25636672973633 = 0.010270901024341583 + 10.0 * 6.224609851837158
Epoch 2370, val loss: 2.001143455505371
Epoch 2380, training loss: 62.30827713012695 = 0.010147025808691978 + 10.0 * 6.229813098907471
Epoch 2380, val loss: 2.0047671794891357
Epoch 2390, training loss: 62.26811218261719 = 0.010023484006524086 + 10.0 * 6.225808620452881
Epoch 2390, val loss: 2.0072028636932373
Epoch 2400, training loss: 62.24675369262695 = 0.009907814674079418 + 10.0 * 6.223684787750244
Epoch 2400, val loss: 2.0114948749542236
Epoch 2410, training loss: 62.26614761352539 = 0.009794654324650764 + 10.0 * 6.225635051727295
Epoch 2410, val loss: 2.0147764682769775
Epoch 2420, training loss: 62.233787536621094 = 0.0096810944378376 + 10.0 * 6.222410678863525
Epoch 2420, val loss: 2.018134593963623
Epoch 2430, training loss: 62.24093246459961 = 0.009571430273354053 + 10.0 * 6.223135948181152
Epoch 2430, val loss: 2.0214171409606934
Epoch 2440, training loss: 62.2503776550293 = 0.009464084170758724 + 10.0 * 6.224091529846191
Epoch 2440, val loss: 2.0246641635894775
Epoch 2450, training loss: 62.24521255493164 = 0.009357770904898643 + 10.0 * 6.223585605621338
Epoch 2450, val loss: 2.028383255004883
Epoch 2460, training loss: 62.230194091796875 = 0.009254236705601215 + 10.0 * 6.2220940589904785
Epoch 2460, val loss: 2.0316874980926514
Epoch 2470, training loss: 62.254119873046875 = 0.009152735583484173 + 10.0 * 6.224496841430664
Epoch 2470, val loss: 2.0346970558166504
Epoch 2480, training loss: 62.273719787597656 = 0.009050393477082253 + 10.0 * 6.226466655731201
Epoch 2480, val loss: 2.0379300117492676
Epoch 2490, training loss: 62.247108459472656 = 0.008949769660830498 + 10.0 * 6.22381591796875
Epoch 2490, val loss: 2.040771007537842
Epoch 2500, training loss: 62.22864532470703 = 0.00884829182177782 + 10.0 * 6.22197961807251
Epoch 2500, val loss: 2.043940305709839
Epoch 2510, training loss: 62.21384811401367 = 0.00875485222786665 + 10.0 * 6.2205095291137695
Epoch 2510, val loss: 2.047459602355957
Epoch 2520, training loss: 62.21482467651367 = 0.008664768189191818 + 10.0 * 6.220616340637207
Epoch 2520, val loss: 2.050675392150879
Epoch 2530, training loss: 62.258087158203125 = 0.008575394749641418 + 10.0 * 6.224951267242432
Epoch 2530, val loss: 2.0535600185394287
Epoch 2540, training loss: 62.2132568359375 = 0.008482556790113449 + 10.0 * 6.220477104187012
Epoch 2540, val loss: 2.05639910697937
Epoch 2550, training loss: 62.263343811035156 = 0.008395527489483356 + 10.0 * 6.225494861602783
Epoch 2550, val loss: 2.0594677925109863
Epoch 2560, training loss: 62.2138786315918 = 0.008303739130496979 + 10.0 * 6.22055721282959
Epoch 2560, val loss: 2.062450885772705
Epoch 2570, training loss: 62.21330261230469 = 0.008222108706831932 + 10.0 * 6.220508098602295
Epoch 2570, val loss: 2.0655291080474854
Epoch 2580, training loss: 62.26536560058594 = 0.008140229620039463 + 10.0 * 6.225722312927246
Epoch 2580, val loss: 2.0682804584503174
Epoch 2590, training loss: 62.20631408691406 = 0.008055304177105427 + 10.0 * 6.219825744628906
Epoch 2590, val loss: 2.0711426734924316
Epoch 2600, training loss: 62.21219253540039 = 0.007974633947014809 + 10.0 * 6.22042179107666
Epoch 2600, val loss: 2.0739200115203857
Epoch 2610, training loss: 62.21166229248047 = 0.007896026596426964 + 10.0 * 6.220376491546631
Epoch 2610, val loss: 2.0767741203308105
Epoch 2620, training loss: 62.18944549560547 = 0.007817734032869339 + 10.0 * 6.218163013458252
Epoch 2620, val loss: 2.079847574234009
Epoch 2630, training loss: 62.21171569824219 = 0.007743630092591047 + 10.0 * 6.220396995544434
Epoch 2630, val loss: 2.0826990604400635
Epoch 2640, training loss: 62.21891784667969 = 0.007668307516723871 + 10.0 * 6.22112512588501
Epoch 2640, val loss: 2.085123300552368
Epoch 2650, training loss: 62.21519088745117 = 0.007594182621687651 + 10.0 * 6.220759391784668
Epoch 2650, val loss: 2.0877716541290283
Epoch 2660, training loss: 62.22748947143555 = 0.0075210388749837875 + 10.0 * 6.221996784210205
Epoch 2660, val loss: 2.0905263423919678
Epoch 2670, training loss: 62.26943588256836 = 0.0074492329731583595 + 10.0 * 6.226198673248291
Epoch 2670, val loss: 2.093799114227295
Epoch 2680, training loss: 62.20978927612305 = 0.007372449152171612 + 10.0 * 6.220241546630859
Epoch 2680, val loss: 2.095388174057007
Epoch 2690, training loss: 62.18490982055664 = 0.007303648628294468 + 10.0 * 6.2177605628967285
Epoch 2690, val loss: 2.0989010334014893
Epoch 2700, training loss: 62.17984390258789 = 0.007238887716084719 + 10.0 * 6.217260360717773
Epoch 2700, val loss: 2.1014413833618164
Epoch 2710, training loss: 62.19985580444336 = 0.007175290025770664 + 10.0 * 6.219267845153809
Epoch 2710, val loss: 2.1039113998413086
Epoch 2720, training loss: 62.196861267089844 = 0.007106563076376915 + 10.0 * 6.21897554397583
Epoch 2720, val loss: 2.106257915496826
Epoch 2730, training loss: 62.22301483154297 = 0.007040278520435095 + 10.0 * 6.221597194671631
Epoch 2730, val loss: 2.108736515045166
Epoch 2740, training loss: 62.191917419433594 = 0.006973311770707369 + 10.0 * 6.218494415283203
Epoch 2740, val loss: 2.110802412033081
Epoch 2750, training loss: 62.175106048583984 = 0.00691074738278985 + 10.0 * 6.2168192863464355
Epoch 2750, val loss: 2.113668203353882
Epoch 2760, training loss: 62.17278289794922 = 0.006851072423160076 + 10.0 * 6.216593265533447
Epoch 2760, val loss: 2.1164770126342773
Epoch 2770, training loss: 62.19933319091797 = 0.006794004701077938 + 10.0 * 6.219254016876221
Epoch 2770, val loss: 2.1189022064208984
Epoch 2780, training loss: 62.188663482666016 = 0.006732274778187275 + 10.0 * 6.218193054199219
Epoch 2780, val loss: 2.120784044265747
Epoch 2790, training loss: 62.204288482666016 = 0.006670850329101086 + 10.0 * 6.219761848449707
Epoch 2790, val loss: 2.1230015754699707
Epoch 2800, training loss: 62.165897369384766 = 0.006611471995711327 + 10.0 * 6.215928554534912
Epoch 2800, val loss: 2.1255574226379395
Epoch 2810, training loss: 62.16228485107422 = 0.006557167507708073 + 10.0 * 6.215572834014893
Epoch 2810, val loss: 2.128084182739258
Epoch 2820, training loss: 62.1790885925293 = 0.006503995507955551 + 10.0 * 6.217258453369141
Epoch 2820, val loss: 2.1304373741149902
Epoch 2830, training loss: 62.21147155761719 = 0.006448593456298113 + 10.0 * 6.2205023765563965
Epoch 2830, val loss: 2.1324822902679443
Epoch 2840, training loss: 62.19587707519531 = 0.00639091432094574 + 10.0 * 6.218948841094971
Epoch 2840, val loss: 2.1347134113311768
Epoch 2850, training loss: 62.169429779052734 = 0.006338237319141626 + 10.0 * 6.216309070587158
Epoch 2850, val loss: 2.137031078338623
Epoch 2860, training loss: 62.19091796875 = 0.006288304924964905 + 10.0 * 6.218462944030762
Epoch 2860, val loss: 2.139338731765747
Epoch 2870, training loss: 62.21436309814453 = 0.006234630011022091 + 10.0 * 6.220812797546387
Epoch 2870, val loss: 2.1408722400665283
Epoch 2880, training loss: 62.16905975341797 = 0.006180143915116787 + 10.0 * 6.216288089752197
Epoch 2880, val loss: 2.14322566986084
Epoch 2890, training loss: 62.14909744262695 = 0.00613029208034277 + 10.0 * 6.214296817779541
Epoch 2890, val loss: 2.145543098449707
Epoch 2900, training loss: 62.14737319946289 = 0.006083224900066853 + 10.0 * 6.2141289710998535
Epoch 2900, val loss: 2.1478307247161865
Epoch 2910, training loss: 62.16608810424805 = 0.006037783343344927 + 10.0 * 6.216004848480225
Epoch 2910, val loss: 2.150247573852539
Epoch 2920, training loss: 62.172386169433594 = 0.005988432560116053 + 10.0 * 6.216639518737793
Epoch 2920, val loss: 2.151776075363159
Epoch 2930, training loss: 62.16618728637695 = 0.0059392740949988365 + 10.0 * 6.216024875640869
Epoch 2930, val loss: 2.153390884399414
Epoch 2940, training loss: 62.21133804321289 = 0.005892877001315355 + 10.0 * 6.220544338226318
Epoch 2940, val loss: 2.1554250717163086
Epoch 2950, training loss: 62.15919876098633 = 0.005843559745699167 + 10.0 * 6.215335369110107
Epoch 2950, val loss: 2.1576602458953857
Epoch 2960, training loss: 62.13991165161133 = 0.0058002411387860775 + 10.0 * 6.213411331176758
Epoch 2960, val loss: 2.1597137451171875
Epoch 2970, training loss: 62.14198303222656 = 0.005758247803896666 + 10.0 * 6.213622570037842
Epoch 2970, val loss: 2.1618151664733887
Epoch 2980, training loss: 62.24760818481445 = 0.005715085193514824 + 10.0 * 6.224189281463623
Epoch 2980, val loss: 2.163733720779419
Epoch 2990, training loss: 62.17816925048828 = 0.005668277852237225 + 10.0 * 6.217249870300293
Epoch 2990, val loss: 2.1647682189941406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.8228782287822879
=== training gcn model ===
Epoch 0, training loss: 87.92353820800781 = 1.9554774761199951 + 10.0 * 8.596806526184082
Epoch 0, val loss: 1.9498101472854614
Epoch 10, training loss: 87.90486145019531 = 1.9450972080230713 + 10.0 * 8.595975875854492
Epoch 10, val loss: 1.9392054080963135
Epoch 20, training loss: 87.83690643310547 = 1.9329917430877686 + 10.0 * 8.590391159057617
Epoch 20, val loss: 1.9268549680709839
Epoch 30, training loss: 87.443359375 = 1.9186749458312988 + 10.0 * 8.552468299865723
Epoch 30, val loss: 1.912214994430542
Epoch 40, training loss: 84.98204803466797 = 1.9026753902435303 + 10.0 * 8.307937622070312
Epoch 40, val loss: 1.8958483934402466
Epoch 50, training loss: 77.69705200195312 = 1.8850246667861938 + 10.0 * 7.581202983856201
Epoch 50, val loss: 1.8779330253601074
Epoch 60, training loss: 74.36744689941406 = 1.870603322982788 + 10.0 * 7.249683856964111
Epoch 60, val loss: 1.864294171333313
Epoch 70, training loss: 72.7131118774414 = 1.858309030532837 + 10.0 * 7.085480213165283
Epoch 70, val loss: 1.8520227670669556
Epoch 80, training loss: 71.63572692871094 = 1.8441025018692017 + 10.0 * 6.979162216186523
Epoch 80, val loss: 1.8384981155395508
Epoch 90, training loss: 70.79894256591797 = 1.8300180435180664 + 10.0 * 6.896892547607422
Epoch 90, val loss: 1.8256912231445312
Epoch 100, training loss: 70.2646255493164 = 1.817076563835144 + 10.0 * 6.844754695892334
Epoch 100, val loss: 1.8141705989837646
Epoch 110, training loss: 69.70408630371094 = 1.8051663637161255 + 10.0 * 6.789891719818115
Epoch 110, val loss: 1.8033215999603271
Epoch 120, training loss: 69.18951416015625 = 1.7939237356185913 + 10.0 * 6.739559173583984
Epoch 120, val loss: 1.7933272123336792
Epoch 130, training loss: 68.70006561279297 = 1.7827945947647095 + 10.0 * 6.691727638244629
Epoch 130, val loss: 1.783458948135376
Epoch 140, training loss: 68.32408905029297 = 1.7710301876068115 + 10.0 * 6.655306339263916
Epoch 140, val loss: 1.7733339071273804
Epoch 150, training loss: 68.0468978881836 = 1.758482813835144 + 10.0 * 6.628841400146484
Epoch 150, val loss: 1.7625788450241089
Epoch 160, training loss: 67.73909759521484 = 1.745223879814148 + 10.0 * 6.599387168884277
Epoch 160, val loss: 1.7509644031524658
Epoch 170, training loss: 67.48321533203125 = 1.7308900356292725 + 10.0 * 6.57523250579834
Epoch 170, val loss: 1.7385138273239136
Epoch 180, training loss: 67.2615737915039 = 1.7154779434204102 + 10.0 * 6.554609298706055
Epoch 180, val loss: 1.7252055406570435
Epoch 190, training loss: 67.07030487060547 = 1.6986502408981323 + 10.0 * 6.537166118621826
Epoch 190, val loss: 1.710720419883728
Epoch 200, training loss: 66.89334869384766 = 1.6802736520767212 + 10.0 * 6.521307468414307
Epoch 200, val loss: 1.6950258016586304
Epoch 210, training loss: 66.72978973388672 = 1.6602455377578735 + 10.0 * 6.506954193115234
Epoch 210, val loss: 1.677932858467102
Epoch 220, training loss: 66.55946350097656 = 1.6385271549224854 + 10.0 * 6.492093563079834
Epoch 220, val loss: 1.659611463546753
Epoch 230, training loss: 66.41316223144531 = 1.6152238845825195 + 10.0 * 6.479793548583984
Epoch 230, val loss: 1.6399827003479004
Epoch 240, training loss: 66.29972839355469 = 1.5901998281478882 + 10.0 * 6.47095251083374
Epoch 240, val loss: 1.618906855583191
Epoch 250, training loss: 66.13805389404297 = 1.5637264251708984 + 10.0 * 6.457432270050049
Epoch 250, val loss: 1.5968284606933594
Epoch 260, training loss: 66.00162506103516 = 1.5357731580734253 + 10.0 * 6.446584701538086
Epoch 260, val loss: 1.5736291408538818
Epoch 270, training loss: 65.88623046875 = 1.5064692497253418 + 10.0 * 6.437975883483887
Epoch 270, val loss: 1.5493277311325073
Epoch 280, training loss: 65.77222442626953 = 1.4760167598724365 + 10.0 * 6.42962121963501
Epoch 280, val loss: 1.524338722229004
Epoch 290, training loss: 65.66062927246094 = 1.4447211027145386 + 10.0 * 6.421590805053711
Epoch 290, val loss: 1.498979926109314
Epoch 300, training loss: 65.68983459472656 = 1.412798285484314 + 10.0 * 6.427703857421875
Epoch 300, val loss: 1.4732680320739746
Epoch 310, training loss: 65.49649810791016 = 1.380225658416748 + 10.0 * 6.411627292633057
Epoch 310, val loss: 1.4473029375076294
Epoch 320, training loss: 65.38117218017578 = 1.3474763631820679 + 10.0 * 6.403369903564453
Epoch 320, val loss: 1.4215563535690308
Epoch 330, training loss: 65.28936004638672 = 1.3147906064987183 + 10.0 * 6.397456645965576
Epoch 330, val loss: 1.3961846828460693
Epoch 340, training loss: 65.2021713256836 = 1.2821615934371948 + 10.0 * 6.392001152038574
Epoch 340, val loss: 1.371032953262329
Epoch 350, training loss: 65.25048828125 = 1.2496176958084106 + 10.0 * 6.400087356567383
Epoch 350, val loss: 1.3462553024291992
Epoch 360, training loss: 65.10308837890625 = 1.2174080610275269 + 10.0 * 6.388567924499512
Epoch 360, val loss: 1.3217583894729614
Epoch 370, training loss: 64.9900131225586 = 1.1856473684310913 + 10.0 * 6.380436420440674
Epoch 370, val loss: 1.2979215383529663
Epoch 380, training loss: 64.90316772460938 = 1.1543474197387695 + 10.0 * 6.374881744384766
Epoch 380, val loss: 1.2747036218643188
Epoch 390, training loss: 64.85948944091797 = 1.1235419511795044 + 10.0 * 6.373594760894775
Epoch 390, val loss: 1.2519415616989136
Epoch 400, training loss: 64.7950210571289 = 1.0931049585342407 + 10.0 * 6.370192050933838
Epoch 400, val loss: 1.229488730430603
Epoch 410, training loss: 64.70819091796875 = 1.063302993774414 + 10.0 * 6.36448860168457
Epoch 410, val loss: 1.2078816890716553
Epoch 420, training loss: 64.64579772949219 = 1.0341196060180664 + 10.0 * 6.361167907714844
Epoch 420, val loss: 1.1867375373840332
Epoch 430, training loss: 64.65733337402344 = 1.0055521726608276 + 10.0 * 6.365178108215332
Epoch 430, val loss: 1.1660939455032349
Epoch 440, training loss: 64.53031921386719 = 0.9774473309516907 + 10.0 * 6.355287075042725
Epoch 440, val loss: 1.1461066007614136
Epoch 450, training loss: 64.47293090820312 = 0.9501877427101135 + 10.0 * 6.352274417877197
Epoch 450, val loss: 1.126833438873291
Epoch 460, training loss: 64.41586303710938 = 0.9236660003662109 + 10.0 * 6.349219799041748
Epoch 460, val loss: 1.1082165241241455
Epoch 470, training loss: 64.46402740478516 = 0.8977749943733215 + 10.0 * 6.356625556945801
Epoch 470, val loss: 1.0900238752365112
Epoch 480, training loss: 64.33549499511719 = 0.8726664781570435 + 10.0 * 6.346282958984375
Epoch 480, val loss: 1.0726343393325806
Epoch 490, training loss: 64.27013397216797 = 0.8483060002326965 + 10.0 * 6.3421831130981445
Epoch 490, val loss: 1.0561422109603882
Epoch 500, training loss: 64.20980072021484 = 0.8247522115707397 + 10.0 * 6.338504791259766
Epoch 500, val loss: 1.0402477979660034
Epoch 510, training loss: 64.16067504882812 = 0.8018757700920105 + 10.0 * 6.335879802703857
Epoch 510, val loss: 1.0250024795532227
Epoch 520, training loss: 64.12983703613281 = 0.7796383500099182 + 10.0 * 6.335020065307617
Epoch 520, val loss: 1.010392665863037
Epoch 530, training loss: 64.10861206054688 = 0.7578709125518799 + 10.0 * 6.335073947906494
Epoch 530, val loss: 0.9960132837295532
Epoch 540, training loss: 64.02742004394531 = 0.7367711663246155 + 10.0 * 6.329064846038818
Epoch 540, val loss: 0.9824784994125366
Epoch 550, training loss: 64.0282974243164 = 0.7162543535232544 + 10.0 * 6.331204414367676
Epoch 550, val loss: 0.9694586992263794
Epoch 560, training loss: 63.991920471191406 = 0.6961981654167175 + 10.0 * 6.3295722007751465
Epoch 560, val loss: 0.9569016098976135
Epoch 570, training loss: 63.91763687133789 = 0.6765995621681213 + 10.0 * 6.324103355407715
Epoch 570, val loss: 0.944842517375946
Epoch 580, training loss: 63.87185287475586 = 0.6574656963348389 + 10.0 * 6.321438789367676
Epoch 580, val loss: 0.9333251714706421
Epoch 590, training loss: 63.845802307128906 = 0.6386943459510803 + 10.0 * 6.3207106590271
Epoch 590, val loss: 0.9221896529197693
Epoch 600, training loss: 63.813602447509766 = 0.6202597618103027 + 10.0 * 6.319334506988525
Epoch 600, val loss: 0.9112722277641296
Epoch 610, training loss: 63.76316452026367 = 0.6021512746810913 + 10.0 * 6.316101551055908
Epoch 610, val loss: 0.9009664058685303
Epoch 620, training loss: 63.732791900634766 = 0.5844773650169373 + 10.0 * 6.314831733703613
Epoch 620, val loss: 0.8911340236663818
Epoch 630, training loss: 63.7352294921875 = 0.5671970844268799 + 10.0 * 6.316803455352783
Epoch 630, val loss: 0.8816561698913574
Epoch 640, training loss: 63.701595306396484 = 0.5500414371490479 + 10.0 * 6.315155506134033
Epoch 640, val loss: 0.8725307583808899
Epoch 650, training loss: 63.64132308959961 = 0.5333951115608215 + 10.0 * 6.310792922973633
Epoch 650, val loss: 0.8639847636222839
Epoch 660, training loss: 63.6016845703125 = 0.5171502828598022 + 10.0 * 6.308453559875488
Epoch 660, val loss: 0.8559736013412476
Epoch 670, training loss: 63.57060623168945 = 0.5012019276618958 + 10.0 * 6.30694055557251
Epoch 670, val loss: 0.8483632206916809
Epoch 680, training loss: 63.562294006347656 = 0.48562002182006836 + 10.0 * 6.3076677322387695
Epoch 680, val loss: 0.8412200212478638
Epoch 690, training loss: 63.53578567504883 = 0.47041717171669006 + 10.0 * 6.306536674499512
Epoch 690, val loss: 0.8345614671707153
Epoch 700, training loss: 63.49978256225586 = 0.45545694231987 + 10.0 * 6.3044328689575195
Epoch 700, val loss: 0.8282296657562256
Epoch 710, training loss: 63.45620346069336 = 0.44097432494163513 + 10.0 * 6.301522731781006
Epoch 710, val loss: 0.822675883769989
Epoch 720, training loss: 63.42390823364258 = 0.4268299341201782 + 10.0 * 6.299707889556885
Epoch 720, val loss: 0.8174785375595093
Epoch 730, training loss: 63.46670150756836 = 0.41306012868881226 + 10.0 * 6.30536413192749
Epoch 730, val loss: 0.8126945495605469
Epoch 740, training loss: 63.3986701965332 = 0.3995235562324524 + 10.0 * 6.299914360046387
Epoch 740, val loss: 0.8083483576774597
Epoch 750, training loss: 63.35879135131836 = 0.3863939046859741 + 10.0 * 6.297239780426025
Epoch 750, val loss: 0.8046397566795349
Epoch 760, training loss: 63.37694549560547 = 0.3736860156059265 + 10.0 * 6.300325870513916
Epoch 760, val loss: 0.8012188673019409
Epoch 770, training loss: 63.32142639160156 = 0.36116185784339905 + 10.0 * 6.296026706695557
Epoch 770, val loss: 0.79833984375
Epoch 780, training loss: 63.27645492553711 = 0.3490988612174988 + 10.0 * 6.292735576629639
Epoch 780, val loss: 0.7958869934082031
Epoch 790, training loss: 63.27658462524414 = 0.3373273015022278 + 10.0 * 6.293925762176514
Epoch 790, val loss: 0.7939624190330505
Epoch 800, training loss: 63.24281311035156 = 0.3258647620677948 + 10.0 * 6.2916951179504395
Epoch 800, val loss: 0.7921518683433533
Epoch 810, training loss: 63.23725128173828 = 0.31467965245246887 + 10.0 * 6.292257308959961
Epoch 810, val loss: 0.7910739183425903
Epoch 820, training loss: 63.21578598022461 = 0.303853839635849 + 10.0 * 6.29119348526001
Epoch 820, val loss: 0.790359377861023
Epoch 830, training loss: 63.17427444458008 = 0.2933535873889923 + 10.0 * 6.288092136383057
Epoch 830, val loss: 0.7899369597434998
Epoch 840, training loss: 63.14790344238281 = 0.2832098603248596 + 10.0 * 6.286469459533691
Epoch 840, val loss: 0.7900972366333008
Epoch 850, training loss: 63.1397590637207 = 0.2733635902404785 + 10.0 * 6.286639213562012
Epoch 850, val loss: 0.7905700206756592
Epoch 860, training loss: 63.10141372680664 = 0.26377204060554504 + 10.0 * 6.283764362335205
Epoch 860, val loss: 0.7913484573364258
Epoch 870, training loss: 63.092803955078125 = 0.2545124590396881 + 10.0 * 6.283829212188721
Epoch 870, val loss: 0.7925736308097839
Epoch 880, training loss: 63.160194396972656 = 0.24558115005493164 + 10.0 * 6.291460990905762
Epoch 880, val loss: 0.7940025329589844
Epoch 890, training loss: 63.05687713623047 = 0.236891970038414 + 10.0 * 6.281998634338379
Epoch 890, val loss: 0.7959867715835571
Epoch 900, training loss: 63.03532409667969 = 0.22857783734798431 + 10.0 * 6.280674934387207
Epoch 900, val loss: 0.7983064651489258
Epoch 910, training loss: 63.009620666503906 = 0.22054587304592133 + 10.0 * 6.278907299041748
Epoch 910, val loss: 0.8008950352668762
Epoch 920, training loss: 62.99959182739258 = 0.2128271907567978 + 10.0 * 6.278676509857178
Epoch 920, val loss: 0.803853452205658
Epoch 930, training loss: 63.012542724609375 = 0.2053539752960205 + 10.0 * 6.280718803405762
Epoch 930, val loss: 0.8070571422576904
Epoch 940, training loss: 62.985145568847656 = 0.19809317588806152 + 10.0 * 6.27870512008667
Epoch 940, val loss: 0.8104276657104492
Epoch 950, training loss: 62.95608139038086 = 0.19111263751983643 + 10.0 * 6.276496887207031
Epoch 950, val loss: 0.8142029643058777
Epoch 960, training loss: 62.97087478637695 = 0.1844296157360077 + 10.0 * 6.278644561767578
Epoch 960, val loss: 0.8183527588844299
Epoch 970, training loss: 62.928428649902344 = 0.1779860556125641 + 10.0 * 6.2750444412231445
Epoch 970, val loss: 0.8224731087684631
Epoch 980, training loss: 62.90882873535156 = 0.17176152765750885 + 10.0 * 6.273706912994385
Epoch 980, val loss: 0.8270674347877502
Epoch 990, training loss: 62.90822982788086 = 0.1658124029636383 + 10.0 * 6.2742414474487305
Epoch 990, val loss: 0.8318613171577454
Epoch 1000, training loss: 62.90882873535156 = 0.16007164120674133 + 10.0 * 6.274875640869141
Epoch 1000, val loss: 0.8368232250213623
Epoch 1010, training loss: 62.879852294921875 = 0.15454690158367157 + 10.0 * 6.272530555725098
Epoch 1010, val loss: 0.8418001532554626
Epoch 1020, training loss: 62.86582946777344 = 0.14922304451465607 + 10.0 * 6.271660804748535
Epoch 1020, val loss: 0.8473398089408875
Epoch 1030, training loss: 62.86388397216797 = 0.14414159953594208 + 10.0 * 6.271974086761475
Epoch 1030, val loss: 0.8526483178138733
Epoch 1040, training loss: 62.88664245605469 = 0.13924279808998108 + 10.0 * 6.274739742279053
Epoch 1040, val loss: 0.8584129214286804
Epoch 1050, training loss: 62.83306884765625 = 0.13448162376880646 + 10.0 * 6.269858360290527
Epoch 1050, val loss: 0.8642300963401794
Epoch 1060, training loss: 62.811771392822266 = 0.12997695803642273 + 10.0 * 6.268179416656494
Epoch 1060, val loss: 0.8701319098472595
Epoch 1070, training loss: 62.8166389465332 = 0.12563057243824005 + 10.0 * 6.269101142883301
Epoch 1070, val loss: 0.8762637376785278
Epoch 1080, training loss: 62.80015182495117 = 0.12145503610372543 + 10.0 * 6.267869472503662
Epoch 1080, val loss: 0.8824261426925659
Epoch 1090, training loss: 62.79955291748047 = 0.11743736267089844 + 10.0 * 6.268211841583252
Epoch 1090, val loss: 0.8889813423156738
Epoch 1100, training loss: 62.76426696777344 = 0.11358490586280823 + 10.0 * 6.265068054199219
Epoch 1100, val loss: 0.8951913118362427
Epoch 1110, training loss: 62.783714294433594 = 0.10991272330284119 + 10.0 * 6.267380237579346
Epoch 1110, val loss: 0.9017612338066101
Epoch 1120, training loss: 62.76346206665039 = 0.10633310675621033 + 10.0 * 6.265712738037109
Epoch 1120, val loss: 0.9082964658737183
Epoch 1130, training loss: 62.73505401611328 = 0.10290933400392532 + 10.0 * 6.263214588165283
Epoch 1130, val loss: 0.9149203896522522
Epoch 1140, training loss: 62.72477722167969 = 0.09962352365255356 + 10.0 * 6.262515068054199
Epoch 1140, val loss: 0.921749472618103
Epoch 1150, training loss: 62.75628662109375 = 0.09647385030984879 + 10.0 * 6.265981197357178
Epoch 1150, val loss: 0.9284700751304626
Epoch 1160, training loss: 62.76112365722656 = 0.09343206882476807 + 10.0 * 6.266768932342529
Epoch 1160, val loss: 0.9354161620140076
Epoch 1170, training loss: 62.722076416015625 = 0.09049376100301743 + 10.0 * 6.263158321380615
Epoch 1170, val loss: 0.9419659972190857
Epoch 1180, training loss: 62.698307037353516 = 0.08768975734710693 + 10.0 * 6.261061668395996
Epoch 1180, val loss: 0.9488520622253418
Epoch 1190, training loss: 62.69208526611328 = 0.0850144550204277 + 10.0 * 6.260706901550293
Epoch 1190, val loss: 0.9558241367340088
Epoch 1200, training loss: 62.74732208251953 = 0.08243061602115631 + 10.0 * 6.266489028930664
Epoch 1200, val loss: 0.9625654220581055
Epoch 1210, training loss: 62.69193649291992 = 0.07992184907197952 + 10.0 * 6.26120138168335
Epoch 1210, val loss: 0.969609260559082
Epoch 1220, training loss: 62.66914749145508 = 0.0775391161441803 + 10.0 * 6.259160995483398
Epoch 1220, val loss: 0.9765151143074036
Epoch 1230, training loss: 62.69595718383789 = 0.07524628192186356 + 10.0 * 6.262071132659912
Epoch 1230, val loss: 0.9833259582519531
Epoch 1240, training loss: 62.65932083129883 = 0.07303355634212494 + 10.0 * 6.258628845214844
Epoch 1240, val loss: 0.9902941584587097
Epoch 1250, training loss: 62.6605110168457 = 0.07090260088443756 + 10.0 * 6.258960723876953
Epoch 1250, val loss: 0.99724280834198
Epoch 1260, training loss: 62.68364334106445 = 0.06885956227779388 + 10.0 * 6.261478424072266
Epoch 1260, val loss: 1.0040446519851685
Epoch 1270, training loss: 62.633426666259766 = 0.06685712933540344 + 10.0 * 6.256657123565674
Epoch 1270, val loss: 1.0109708309173584
Epoch 1280, training loss: 62.61774444580078 = 0.06497864425182343 + 10.0 * 6.255276679992676
Epoch 1280, val loss: 1.0179129838943481
Epoch 1290, training loss: 62.62491226196289 = 0.06315987557172775 + 10.0 * 6.2561750411987305
Epoch 1290, val loss: 1.0247551202774048
Epoch 1300, training loss: 62.631935119628906 = 0.061405640095472336 + 10.0 * 6.257052898406982
Epoch 1300, val loss: 1.031480073928833
Epoch 1310, training loss: 62.60890579223633 = 0.05970356613397598 + 10.0 * 6.25492000579834
Epoch 1310, val loss: 1.0381081104278564
Epoch 1320, training loss: 62.60512924194336 = 0.05807550996541977 + 10.0 * 6.254705429077148
Epoch 1320, val loss: 1.0448426008224487
Epoch 1330, training loss: 62.60482406616211 = 0.05651400238275528 + 10.0 * 6.254830837249756
Epoch 1330, val loss: 1.0515278577804565
Epoch 1340, training loss: 62.612205505371094 = 0.05500008538365364 + 10.0 * 6.255720615386963
Epoch 1340, val loss: 1.0582306385040283
Epoch 1350, training loss: 62.60432434082031 = 0.05355019122362137 + 10.0 * 6.255077362060547
Epoch 1350, val loss: 1.0646170377731323
Epoch 1360, training loss: 62.59462356567383 = 0.05214237794280052 + 10.0 * 6.254248142242432
Epoch 1360, val loss: 1.071203589439392
Epoch 1370, training loss: 62.56353759765625 = 0.05078042298555374 + 10.0 * 6.251275539398193
Epoch 1370, val loss: 1.0779500007629395
Epoch 1380, training loss: 62.557193756103516 = 0.049479421228170395 + 10.0 * 6.250771522521973
Epoch 1380, val loss: 1.0845450162887573
Epoch 1390, training loss: 62.59209060668945 = 0.048229802399873734 + 10.0 * 6.254385948181152
Epoch 1390, val loss: 1.0910468101501465
Epoch 1400, training loss: 62.551265716552734 = 0.047012969851493835 + 10.0 * 6.250425338745117
Epoch 1400, val loss: 1.097305178642273
Epoch 1410, training loss: 62.54571533203125 = 0.045837655663490295 + 10.0 * 6.249987602233887
Epoch 1410, val loss: 1.103556513786316
Epoch 1420, training loss: 62.60017395019531 = 0.044700346887111664 + 10.0 * 6.255547523498535
Epoch 1420, val loss: 1.1099615097045898
Epoch 1430, training loss: 62.55579376220703 = 0.043601926416158676 + 10.0 * 6.251219272613525
Epoch 1430, val loss: 1.1161437034606934
Epoch 1440, training loss: 62.550594329833984 = 0.042545467615127563 + 10.0 * 6.250804901123047
Epoch 1440, val loss: 1.1224344968795776
Epoch 1450, training loss: 62.528018951416016 = 0.04152136668562889 + 10.0 * 6.248649597167969
Epoch 1450, val loss: 1.128552794456482
Epoch 1460, training loss: 62.516815185546875 = 0.04053693264722824 + 10.0 * 6.2476277351379395
Epoch 1460, val loss: 1.1349916458129883
Epoch 1470, training loss: 62.51729202270508 = 0.03959235921502113 + 10.0 * 6.247769832611084
Epoch 1470, val loss: 1.141050934791565
Epoch 1480, training loss: 62.568397521972656 = 0.038675058633089066 + 10.0 * 6.252972602844238
Epoch 1480, val loss: 1.1470236778259277
Epoch 1490, training loss: 62.53962707519531 = 0.03777588531374931 + 10.0 * 6.250185012817383
Epoch 1490, val loss: 1.1526960134506226
Epoch 1500, training loss: 62.508827209472656 = 0.03690606728196144 + 10.0 * 6.247191905975342
Epoch 1500, val loss: 1.1590313911437988
Epoch 1510, training loss: 62.53445816040039 = 0.036077480763196945 + 10.0 * 6.249838352203369
Epoch 1510, val loss: 1.1648741960525513
Epoch 1520, training loss: 62.48427963256836 = 0.03525981679558754 + 10.0 * 6.24490213394165
Epoch 1520, val loss: 1.1706123352050781
Epoch 1530, training loss: 62.4865608215332 = 0.034482672810554504 + 10.0 * 6.245207786560059
Epoch 1530, val loss: 1.1762561798095703
Epoch 1540, training loss: 62.48295593261719 = 0.033734988421201706 + 10.0 * 6.244922161102295
Epoch 1540, val loss: 1.1820358037948608
Epoch 1550, training loss: 62.547035217285156 = 0.03301798179745674 + 10.0 * 6.251401901245117
Epoch 1550, val loss: 1.1876577138900757
Epoch 1560, training loss: 62.49290466308594 = 0.03227772191166878 + 10.0 * 6.246062755584717
Epoch 1560, val loss: 1.1934092044830322
Epoch 1570, training loss: 62.465797424316406 = 0.03158339112997055 + 10.0 * 6.24342155456543
Epoch 1570, val loss: 1.1990224123001099
Epoch 1580, training loss: 62.46046829223633 = 0.030923835933208466 + 10.0 * 6.242954730987549
Epoch 1580, val loss: 1.2046362161636353
Epoch 1590, training loss: 62.51521682739258 = 0.03028394654393196 + 10.0 * 6.248493194580078
Epoch 1590, val loss: 1.2098361253738403
Epoch 1600, training loss: 62.46620178222656 = 0.029647231101989746 + 10.0 * 6.243655204772949
Epoch 1600, val loss: 1.2157413959503174
Epoch 1610, training loss: 62.48116683959961 = 0.029033958911895752 + 10.0 * 6.245213508605957
Epoch 1610, val loss: 1.2210026979446411
Epoch 1620, training loss: 62.463958740234375 = 0.02844264544546604 + 10.0 * 6.243551731109619
Epoch 1620, val loss: 1.2262076139450073
Epoch 1630, training loss: 62.454742431640625 = 0.02786931023001671 + 10.0 * 6.242687225341797
Epoch 1630, val loss: 1.231440544128418
Epoch 1640, training loss: 62.4400634765625 = 0.027311859652400017 + 10.0 * 6.241274833679199
Epoch 1640, val loss: 1.2371834516525269
Epoch 1650, training loss: 62.43360137939453 = 0.026778817176818848 + 10.0 * 6.240682125091553
Epoch 1650, val loss: 1.2422696352005005
Epoch 1660, training loss: 62.46622085571289 = 0.026262441650032997 + 10.0 * 6.243995666503906
Epoch 1660, val loss: 1.247484803199768
Epoch 1670, training loss: 62.46428680419922 = 0.025749387219548225 + 10.0 * 6.243853569030762
Epoch 1670, val loss: 1.2526787519454956
Epoch 1680, training loss: 62.45261764526367 = 0.025250274688005447 + 10.0 * 6.24273681640625
Epoch 1680, val loss: 1.2573390007019043
Epoch 1690, training loss: 62.4331169128418 = 0.024761008098721504 + 10.0 * 6.240835666656494
Epoch 1690, val loss: 1.2626502513885498
Epoch 1700, training loss: 62.43191909790039 = 0.024295387789607048 + 10.0 * 6.240762233734131
Epoch 1700, val loss: 1.2676118612289429
Epoch 1710, training loss: 62.41161346435547 = 0.023841917514801025 + 10.0 * 6.238777160644531
Epoch 1710, val loss: 1.272616982460022
Epoch 1720, training loss: 62.43385696411133 = 0.02340712770819664 + 10.0 * 6.241044998168945
Epoch 1720, val loss: 1.2775685787200928
Epoch 1730, training loss: 62.42412567138672 = 0.02297195792198181 + 10.0 * 6.240115165710449
Epoch 1730, val loss: 1.2825264930725098
Epoch 1740, training loss: 62.47712707519531 = 0.02254394441843033 + 10.0 * 6.245458126068115
Epoch 1740, val loss: 1.2873616218566895
Epoch 1750, training loss: 62.41217803955078 = 0.022132283076643944 + 10.0 * 6.239004611968994
Epoch 1750, val loss: 1.291818380355835
Epoch 1760, training loss: 62.395198822021484 = 0.02173764631152153 + 10.0 * 6.237346172332764
Epoch 1760, val loss: 1.2968578338623047
Epoch 1770, training loss: 62.38383102416992 = 0.021357527002692223 + 10.0 * 6.2362470626831055
Epoch 1770, val loss: 1.3014850616455078
Epoch 1780, training loss: 62.408382415771484 = 0.020992480218410492 + 10.0 * 6.238739013671875
Epoch 1780, val loss: 1.3061116933822632
Epoch 1790, training loss: 62.3878173828125 = 0.020621659234166145 + 10.0 * 6.236719608306885
Epoch 1790, val loss: 1.3107612133026123
Epoch 1800, training loss: 62.41147994995117 = 0.020261231809854507 + 10.0 * 6.239121913909912
Epoch 1800, val loss: 1.3156323432922363
Epoch 1810, training loss: 62.38677215576172 = 0.01991144008934498 + 10.0 * 6.236685752868652
Epoch 1810, val loss: 1.3200541734695435
Epoch 1820, training loss: 62.38138198852539 = 0.019572144374251366 + 10.0 * 6.236180782318115
Epoch 1820, val loss: 1.3245114088058472
Epoch 1830, training loss: 62.39348602294922 = 0.019241871312260628 + 10.0 * 6.237424373626709
Epoch 1830, val loss: 1.3291919231414795
Epoch 1840, training loss: 62.37797164916992 = 0.018922410905361176 + 10.0 * 6.235905170440674
Epoch 1840, val loss: 1.3333375453948975
Epoch 1850, training loss: 62.41681671142578 = 0.018613561987876892 + 10.0 * 6.23982048034668
Epoch 1850, val loss: 1.337451696395874
Epoch 1860, training loss: 62.361183166503906 = 0.018302664160728455 + 10.0 * 6.234288215637207
Epoch 1860, val loss: 1.3422789573669434
Epoch 1870, training loss: 62.35610580444336 = 0.01800573617219925 + 10.0 * 6.233809947967529
Epoch 1870, val loss: 1.3465721607208252
Epoch 1880, training loss: 62.39506149291992 = 0.017720019444823265 + 10.0 * 6.237734317779541
Epoch 1880, val loss: 1.3509598970413208
Epoch 1890, training loss: 62.35715866088867 = 0.017432138323783875 + 10.0 * 6.233972549438477
Epoch 1890, val loss: 1.3551732301712036
Epoch 1900, training loss: 62.35352325439453 = 0.01715715229511261 + 10.0 * 6.233636379241943
Epoch 1900, val loss: 1.3591670989990234
Epoch 1910, training loss: 62.35965347290039 = 0.016890352591872215 + 10.0 * 6.234276294708252
Epoch 1910, val loss: 1.363771677017212
Epoch 1920, training loss: 62.36922836303711 = 0.016625020653009415 + 10.0 * 6.235260486602783
Epoch 1920, val loss: 1.3675320148468018
Epoch 1930, training loss: 62.34938049316406 = 0.016366921365261078 + 10.0 * 6.233301639556885
Epoch 1930, val loss: 1.371909499168396
Epoch 1940, training loss: 62.34522247314453 = 0.016116516664624214 + 10.0 * 6.232910633087158
Epoch 1940, val loss: 1.375754714012146
Epoch 1950, training loss: 62.36836624145508 = 0.015875134617090225 + 10.0 * 6.235249042510986
Epoch 1950, val loss: 1.3796565532684326
Epoch 1960, training loss: 62.33450698852539 = 0.015631303191184998 + 10.0 * 6.231887340545654
Epoch 1960, val loss: 1.3841581344604492
Epoch 1970, training loss: 62.330196380615234 = 0.015399936586618423 + 10.0 * 6.231479644775391
Epoch 1970, val loss: 1.3882209062576294
Epoch 1980, training loss: 62.33551025390625 = 0.015175648033618927 + 10.0 * 6.232033729553223
Epoch 1980, val loss: 1.3919885158538818
Epoch 1990, training loss: 62.3597297668457 = 0.01495369989424944 + 10.0 * 6.234477519989014
Epoch 1990, val loss: 1.396027684211731
Epoch 2000, training loss: 62.367923736572266 = 0.014731509611010551 + 10.0 * 6.235319137573242
Epoch 2000, val loss: 1.3997998237609863
Epoch 2010, training loss: 62.33905029296875 = 0.01451842114329338 + 10.0 * 6.232453346252441
Epoch 2010, val loss: 1.4038249254226685
Epoch 2020, training loss: 62.328330993652344 = 0.01430665422230959 + 10.0 * 6.231402397155762
Epoch 2020, val loss: 1.4078483581542969
Epoch 2030, training loss: 62.32258605957031 = 0.014105337671935558 + 10.0 * 6.2308478355407715
Epoch 2030, val loss: 1.4113529920578003
Epoch 2040, training loss: 62.33646774291992 = 0.013908530585467815 + 10.0 * 6.232255935668945
Epoch 2040, val loss: 1.415363073348999
Epoch 2050, training loss: 62.30138397216797 = 0.013711181469261646 + 10.0 * 6.228767395019531
Epoch 2050, val loss: 1.4193607568740845
Epoch 2060, training loss: 62.31687545776367 = 0.013523148372769356 + 10.0 * 6.230335235595703
Epoch 2060, val loss: 1.423249363899231
Epoch 2070, training loss: 62.36793518066406 = 0.013336054049432278 + 10.0 * 6.235459804534912
Epoch 2070, val loss: 1.4266304969787598
Epoch 2080, training loss: 62.31515121459961 = 0.013150081038475037 + 10.0 * 6.230200290679932
Epoch 2080, val loss: 1.4302479028701782
Epoch 2090, training loss: 62.2959098815918 = 0.012970348820090294 + 10.0 * 6.2282938957214355
Epoch 2090, val loss: 1.4342365264892578
Epoch 2100, training loss: 62.28814697265625 = 0.012799215503036976 + 10.0 * 6.227534770965576
Epoch 2100, val loss: 1.4376170635223389
Epoch 2110, training loss: 62.34217071533203 = 0.012631004676222801 + 10.0 * 6.232954025268555
Epoch 2110, val loss: 1.441359281539917
Epoch 2120, training loss: 62.29057693481445 = 0.012463166378438473 + 10.0 * 6.227811336517334
Epoch 2120, val loss: 1.4449995756149292
Epoch 2130, training loss: 62.3111457824707 = 0.012297152541577816 + 10.0 * 6.229884624481201
Epoch 2130, val loss: 1.4486266374588013
Epoch 2140, training loss: 62.27781677246094 = 0.012135699391365051 + 10.0 * 6.226568222045898
Epoch 2140, val loss: 1.4519411325454712
Epoch 2150, training loss: 62.2818717956543 = 0.011979235336184502 + 10.0 * 6.226989269256592
Epoch 2150, val loss: 1.4555835723876953
Epoch 2160, training loss: 62.356651306152344 = 0.011831171810626984 + 10.0 * 6.234482288360596
Epoch 2160, val loss: 1.4592151641845703
Epoch 2170, training loss: 62.28585433959961 = 0.011676379479467869 + 10.0 * 6.227417945861816
Epoch 2170, val loss: 1.4622973203659058
Epoch 2180, training loss: 62.268699645996094 = 0.011528881266713142 + 10.0 * 6.225717067718506
Epoch 2180, val loss: 1.4657456874847412
Epoch 2190, training loss: 62.274044036865234 = 0.01138710044324398 + 10.0 * 6.226265907287598
Epoch 2190, val loss: 1.4691907167434692
Epoch 2200, training loss: 62.340450286865234 = 0.011249418370425701 + 10.0 * 6.232920169830322
Epoch 2200, val loss: 1.4722148180007935
Epoch 2210, training loss: 62.293670654296875 = 0.011105671525001526 + 10.0 * 6.228256702423096
Epoch 2210, val loss: 1.4758750200271606
Epoch 2220, training loss: 62.289390563964844 = 0.010969876311719418 + 10.0 * 6.227841854095459
Epoch 2220, val loss: 1.4793071746826172
Epoch 2230, training loss: 62.2918701171875 = 0.010836987756192684 + 10.0 * 6.228103160858154
Epoch 2230, val loss: 1.4824178218841553
Epoch 2240, training loss: 62.27035140991211 = 0.010707253590226173 + 10.0 * 6.225964546203613
Epoch 2240, val loss: 1.4854590892791748
Epoch 2250, training loss: 62.277137756347656 = 0.0105813629925251 + 10.0 * 6.22665548324585
Epoch 2250, val loss: 1.4885698556900024
Epoch 2260, training loss: 62.282264709472656 = 0.010453488677740097 + 10.0 * 6.227181434631348
Epoch 2260, val loss: 1.4919310808181763
Epoch 2270, training loss: 62.263648986816406 = 0.010329602286219597 + 10.0 * 6.225331783294678
Epoch 2270, val loss: 1.4954406023025513
Epoch 2280, training loss: 62.2506217956543 = 0.010211294516921043 + 10.0 * 6.224040985107422
Epoch 2280, val loss: 1.4983162879943848
Epoch 2290, training loss: 62.264198303222656 = 0.010096060112118721 + 10.0 * 6.225409984588623
Epoch 2290, val loss: 1.501722812652588
Epoch 2300, training loss: 62.2642936706543 = 0.009978542104363441 + 10.0 * 6.225431442260742
Epoch 2300, val loss: 1.504814863204956
Epoch 2310, training loss: 62.297340393066406 = 0.009862535633146763 + 10.0 * 6.228747844696045
Epoch 2310, val loss: 1.5077670812606812
Epoch 2320, training loss: 62.2838249206543 = 0.00975214596837759 + 10.0 * 6.227407455444336
Epoch 2320, val loss: 1.510753870010376
Epoch 2330, training loss: 62.25947952270508 = 0.009638167917728424 + 10.0 * 6.224984169006348
Epoch 2330, val loss: 1.5134656429290771
Epoch 2340, training loss: 62.24856948852539 = 0.00953134149312973 + 10.0 * 6.223903656005859
Epoch 2340, val loss: 1.5168650150299072
Epoch 2350, training loss: 62.26234817504883 = 0.009427248500287533 + 10.0 * 6.225292205810547
Epoch 2350, val loss: 1.5195759534835815
Epoch 2360, training loss: 62.23750305175781 = 0.00932331383228302 + 10.0 * 6.222817897796631
Epoch 2360, val loss: 1.5227290391921997
Epoch 2370, training loss: 62.246246337890625 = 0.009222570806741714 + 10.0 * 6.223702430725098
Epoch 2370, val loss: 1.5257302522659302
Epoch 2380, training loss: 62.23516845703125 = 0.009123398922383785 + 10.0 * 6.222604274749756
Epoch 2380, val loss: 1.5286539793014526
Epoch 2390, training loss: 62.30693817138672 = 0.009027308784425259 + 10.0 * 6.229791164398193
Epoch 2390, val loss: 1.531382441520691
Epoch 2400, training loss: 62.25932693481445 = 0.008925024420022964 + 10.0 * 6.225039958953857
Epoch 2400, val loss: 1.534000039100647
Epoch 2410, training loss: 62.23968505859375 = 0.008831449784338474 + 10.0 * 6.223085403442383
Epoch 2410, val loss: 1.5369318723678589
Epoch 2420, training loss: 62.2214469909668 = 0.008738048374652863 + 10.0 * 6.221270561218262
Epoch 2420, val loss: 1.540008306503296
Epoch 2430, training loss: 62.22488021850586 = 0.008649731054902077 + 10.0 * 6.221623420715332
Epoch 2430, val loss: 1.5427863597869873
Epoch 2440, training loss: 62.25862503051758 = 0.008561236783862114 + 10.0 * 6.225006580352783
Epoch 2440, val loss: 1.5455445051193237
Epoch 2450, training loss: 62.21977996826172 = 0.008471483364701271 + 10.0 * 6.221130847930908
Epoch 2450, val loss: 1.548143744468689
Epoch 2460, training loss: 62.22773361206055 = 0.008385848253965378 + 10.0 * 6.221934795379639
Epoch 2460, val loss: 1.550885796546936
Epoch 2470, training loss: 62.22446823120117 = 0.008301477879285812 + 10.0 * 6.221616744995117
Epoch 2470, val loss: 1.553640604019165
Epoch 2480, training loss: 62.29509735107422 = 0.008223112672567368 + 10.0 * 6.228687286376953
Epoch 2480, val loss: 1.555835485458374
Epoch 2490, training loss: 62.25172805786133 = 0.008132673799991608 + 10.0 * 6.224359512329102
Epoch 2490, val loss: 1.558695912361145
Epoch 2500, training loss: 62.2159309387207 = 0.00805105920881033 + 10.0 * 6.22078800201416
Epoch 2500, val loss: 1.5614523887634277
Epoch 2510, training loss: 62.19851303100586 = 0.007969759404659271 + 10.0 * 6.219054222106934
Epoch 2510, val loss: 1.5642880201339722
Epoch 2520, training loss: 62.19813537597656 = 0.00789420586079359 + 10.0 * 6.219024181365967
Epoch 2520, val loss: 1.5670626163482666
Epoch 2530, training loss: 62.216373443603516 = 0.007820984348654747 + 10.0 * 6.220855236053467
Epoch 2530, val loss: 1.5695799589157104
Epoch 2540, training loss: 62.229862213134766 = 0.007745401468127966 + 10.0 * 6.222211837768555
Epoch 2540, val loss: 1.5719542503356934
Epoch 2550, training loss: 62.216800689697266 = 0.007668852806091309 + 10.0 * 6.220913410186768
Epoch 2550, val loss: 1.574270486831665
Epoch 2560, training loss: 62.19716262817383 = 0.007595603354275227 + 10.0 * 6.21895694732666
Epoch 2560, val loss: 1.5771011114120483
Epoch 2570, training loss: 62.20315933227539 = 0.007525090593844652 + 10.0 * 6.2195634841918945
Epoch 2570, val loss: 1.579471468925476
Epoch 2580, training loss: 62.23332595825195 = 0.007455458398908377 + 10.0 * 6.2225871086120605
Epoch 2580, val loss: 1.582121729850769
Epoch 2590, training loss: 62.266357421875 = 0.007385792210698128 + 10.0 * 6.225897312164307
Epoch 2590, val loss: 1.5849438905715942
Epoch 2600, training loss: 62.21184539794922 = 0.0073130009695887566 + 10.0 * 6.220453262329102
Epoch 2600, val loss: 1.5865715742111206
Epoch 2610, training loss: 62.184776306152344 = 0.007246886380016804 + 10.0 * 6.217752933502197
Epoch 2610, val loss: 1.5894478559494019
Epoch 2620, training loss: 62.176578521728516 = 0.007182348519563675 + 10.0 * 6.216939449310303
Epoch 2620, val loss: 1.5918574333190918
Epoch 2630, training loss: 62.18400192260742 = 0.007121063768863678 + 10.0 * 6.217688083648682
Epoch 2630, val loss: 1.5942282676696777
Epoch 2640, training loss: 62.23891830444336 = 0.007059035357087851 + 10.0 * 6.223186016082764
Epoch 2640, val loss: 1.596300482749939
Epoch 2650, training loss: 62.215091705322266 = 0.006992553360760212 + 10.0 * 6.2208099365234375
Epoch 2650, val loss: 1.5992218255996704
Epoch 2660, training loss: 62.220237731933594 = 0.006930181290954351 + 10.0 * 6.221330642700195
Epoch 2660, val loss: 1.600988745689392
Epoch 2670, training loss: 62.18450164794922 = 0.006867168005555868 + 10.0 * 6.217763423919678
Epoch 2670, val loss: 1.6036596298217773
Epoch 2680, training loss: 62.17062759399414 = 0.006808001082390547 + 10.0 * 6.216382026672363
Epoch 2680, val loss: 1.6062158346176147
Epoch 2690, training loss: 62.26419448852539 = 0.0067512271925807 + 10.0 * 6.225744247436523
Epoch 2690, val loss: 1.6087669134140015
Epoch 2700, training loss: 62.18994903564453 = 0.00669153593480587 + 10.0 * 6.218325614929199
Epoch 2700, val loss: 1.6101399660110474
Epoch 2710, training loss: 62.17042922973633 = 0.006632338277995586 + 10.0 * 6.216379642486572
Epoch 2710, val loss: 1.6130738258361816
Epoch 2720, training loss: 62.157073974609375 = 0.006578035186976194 + 10.0 * 6.215049743652344
Epoch 2720, val loss: 1.6150623559951782
Epoch 2730, training loss: 62.16511535644531 = 0.006525744218379259 + 10.0 * 6.2158589363098145
Epoch 2730, val loss: 1.617367148399353
Epoch 2740, training loss: 62.22734451293945 = 0.00647382577881217 + 10.0 * 6.2220869064331055
Epoch 2740, val loss: 1.6196420192718506
Epoch 2750, training loss: 62.20399475097656 = 0.006414942443370819 + 10.0 * 6.219758033752441
Epoch 2750, val loss: 1.621914267539978
Epoch 2760, training loss: 62.163021087646484 = 0.0063628326170146465 + 10.0 * 6.215665817260742
Epoch 2760, val loss: 1.6240161657333374
Epoch 2770, training loss: 62.175376892089844 = 0.0063108718022704124 + 10.0 * 6.216906547546387
Epoch 2770, val loss: 1.626092553138733
Epoch 2780, training loss: 62.19856262207031 = 0.006260688416659832 + 10.0 * 6.2192301750183105
Epoch 2780, val loss: 1.6282727718353271
Epoch 2790, training loss: 62.2039794921875 = 0.006208518519997597 + 10.0 * 6.2197771072387695
Epoch 2790, val loss: 1.6300734281539917
Epoch 2800, training loss: 62.16506576538086 = 0.006157328374683857 + 10.0 * 6.215890884399414
Epoch 2800, val loss: 1.6326959133148193
Epoch 2810, training loss: 62.149932861328125 = 0.00610884977504611 + 10.0 * 6.214382648468018
Epoch 2810, val loss: 1.6347476243972778
Epoch 2820, training loss: 62.17381286621094 = 0.006063549779355526 + 10.0 * 6.216774940490723
Epoch 2820, val loss: 1.6364878416061401
Epoch 2830, training loss: 62.168243408203125 = 0.0060149370692670345 + 10.0 * 6.216222763061523
Epoch 2830, val loss: 1.638672113418579
Epoch 2840, training loss: 62.16145706176758 = 0.005965474992990494 + 10.0 * 6.215548992156982
Epoch 2840, val loss: 1.6410162448883057
Epoch 2850, training loss: 62.15367126464844 = 0.005919715389609337 + 10.0 * 6.214775085449219
Epoch 2850, val loss: 1.6430072784423828
Epoch 2860, training loss: 62.1625862121582 = 0.005875084549188614 + 10.0 * 6.215671062469482
Epoch 2860, val loss: 1.645387887954712
Epoch 2870, training loss: 62.16011047363281 = 0.005831003654748201 + 10.0 * 6.215427875518799
Epoch 2870, val loss: 1.6471436023712158
Epoch 2880, training loss: 62.15510177612305 = 0.005787391681224108 + 10.0 * 6.214931488037109
Epoch 2880, val loss: 1.649072527885437
Epoch 2890, training loss: 62.16360092163086 = 0.005744572263211012 + 10.0 * 6.215785503387451
Epoch 2890, val loss: 1.6509394645690918
Epoch 2900, training loss: 62.179779052734375 = 0.005699829198420048 + 10.0 * 6.217408180236816
Epoch 2900, val loss: 1.6527462005615234
Epoch 2910, training loss: 62.13619613647461 = 0.005655956454575062 + 10.0 * 6.2130537033081055
Epoch 2910, val loss: 1.6550183296203613
Epoch 2920, training loss: 62.13387680053711 = 0.005615240894258022 + 10.0 * 6.212826251983643
Epoch 2920, val loss: 1.6572366952896118
Epoch 2930, training loss: 62.13945770263672 = 0.005575214512646198 + 10.0 * 6.213387966156006
Epoch 2930, val loss: 1.6591734886169434
Epoch 2940, training loss: 62.16297912597656 = 0.005535484291613102 + 10.0 * 6.215744495391846
Epoch 2940, val loss: 1.6612176895141602
Epoch 2950, training loss: 62.17512130737305 = 0.005494493991136551 + 10.0 * 6.216962814331055
Epoch 2950, val loss: 1.6625248193740845
Epoch 2960, training loss: 62.14234924316406 = 0.0054529872722923756 + 10.0 * 6.213689804077148
Epoch 2960, val loss: 1.6648131608963013
Epoch 2970, training loss: 62.133052825927734 = 0.005414441227912903 + 10.0 * 6.212763786315918
Epoch 2970, val loss: 1.666797161102295
Epoch 2980, training loss: 62.15267562866211 = 0.005377967841923237 + 10.0 * 6.2147297859191895
Epoch 2980, val loss: 1.6683286428451538
Epoch 2990, training loss: 62.155033111572266 = 0.005339921452105045 + 10.0 * 6.214969158172607
Epoch 2990, val loss: 1.6701422929763794
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7037037037037037
0.8144438587243016
The final CL Acc:0.71605, 0.01062, The final GNN Acc:0.81919, 0.00352
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13206])
remove edge: torch.Size([2, 7864])
updated graph: torch.Size([2, 10514])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.9115219116211 = 1.9430800676345825 + 10.0 * 8.596844673156738
Epoch 0, val loss: 1.9357995986938477
Epoch 10, training loss: 87.89612579345703 = 1.933486819267273 + 10.0 * 8.596263885498047
Epoch 10, val loss: 1.926771640777588
Epoch 20, training loss: 87.84326934814453 = 1.9219799041748047 + 10.0 * 8.59212875366211
Epoch 20, val loss: 1.9155112504959106
Epoch 30, training loss: 87.49200439453125 = 1.9077082872390747 + 10.0 * 8.558429718017578
Epoch 30, val loss: 1.9013195037841797
Epoch 40, training loss: 84.380615234375 = 1.8916966915130615 + 10.0 * 8.248891830444336
Epoch 40, val loss: 1.8858510255813599
Epoch 50, training loss: 75.61881256103516 = 1.8748525381088257 + 10.0 * 7.374395370483398
Epoch 50, val loss: 1.8697737455368042
Epoch 60, training loss: 73.02703094482422 = 1.86176598072052 + 10.0 * 7.1165266036987305
Epoch 60, val loss: 1.8574868440628052
Epoch 70, training loss: 71.44934844970703 = 1.84929358959198 + 10.0 * 6.960005283355713
Epoch 70, val loss: 1.8457741737365723
Epoch 80, training loss: 70.2058334350586 = 1.8376328945159912 + 10.0 * 6.836820125579834
Epoch 80, val loss: 1.8348956108093262
Epoch 90, training loss: 69.46747589111328 = 1.8256930112838745 + 10.0 * 6.76417875289917
Epoch 90, val loss: 1.8235657215118408
Epoch 100, training loss: 68.90959930419922 = 1.8138089179992676 + 10.0 * 6.709578514099121
Epoch 100, val loss: 1.8123633861541748
Epoch 110, training loss: 68.43806457519531 = 1.8024635314941406 + 10.0 * 6.663560390472412
Epoch 110, val loss: 1.8018007278442383
Epoch 120, training loss: 68.04093933105469 = 1.791521430015564 + 10.0 * 6.624942302703857
Epoch 120, val loss: 1.7916538715362549
Epoch 130, training loss: 67.72412872314453 = 1.780553936958313 + 10.0 * 6.594357013702393
Epoch 130, val loss: 1.7813465595245361
Epoch 140, training loss: 67.512939453125 = 1.7691044807434082 + 10.0 * 6.57438325881958
Epoch 140, val loss: 1.7705491781234741
Epoch 150, training loss: 67.23445129394531 = 1.7569082975387573 + 10.0 * 6.547754764556885
Epoch 150, val loss: 1.7590458393096924
Epoch 160, training loss: 67.01998138427734 = 1.7438304424285889 + 10.0 * 6.527615547180176
Epoch 160, val loss: 1.7469420433044434
Epoch 170, training loss: 66.8679428100586 = 1.729592204093933 + 10.0 * 6.513835430145264
Epoch 170, val loss: 1.7339805364608765
Epoch 180, training loss: 66.68004608154297 = 1.71397066116333 + 10.0 * 6.496607303619385
Epoch 180, val loss: 1.7197037935256958
Epoch 190, training loss: 66.50753784179688 = 1.696813941001892 + 10.0 * 6.481072425842285
Epoch 190, val loss: 1.7042427062988281
Epoch 200, training loss: 66.39396667480469 = 1.6778815984725952 + 10.0 * 6.471608638763428
Epoch 200, val loss: 1.6874200105667114
Epoch 210, training loss: 66.2466049194336 = 1.6570377349853516 + 10.0 * 6.458956718444824
Epoch 210, val loss: 1.6689246892929077
Epoch 220, training loss: 66.11373901367188 = 1.6341466903686523 + 10.0 * 6.4479594230651855
Epoch 220, val loss: 1.6489355564117432
Epoch 230, training loss: 65.99967193603516 = 1.6093451976776123 + 10.0 * 6.439032554626465
Epoch 230, val loss: 1.627416968345642
Epoch 240, training loss: 65.90042877197266 = 1.5825141668319702 + 10.0 * 6.43179178237915
Epoch 240, val loss: 1.6044824123382568
Epoch 250, training loss: 65.76754760742188 = 1.5540030002593994 + 10.0 * 6.421354293823242
Epoch 250, val loss: 1.5804076194763184
Epoch 260, training loss: 65.68877410888672 = 1.5238574743270874 + 10.0 * 6.416491508483887
Epoch 260, val loss: 1.5553319454193115
Epoch 270, training loss: 65.58057403564453 = 1.4921947717666626 + 10.0 * 6.408837795257568
Epoch 270, val loss: 1.529616117477417
Epoch 280, training loss: 65.48171997070312 = 1.4594693183898926 + 10.0 * 6.402224540710449
Epoch 280, val loss: 1.5033155679702759
Epoch 290, training loss: 65.38692474365234 = 1.4259511232376099 + 10.0 * 6.396097660064697
Epoch 290, val loss: 1.4769442081451416
Epoch 300, training loss: 65.29571533203125 = 1.3919204473495483 + 10.0 * 6.390379428863525
Epoch 300, val loss: 1.4506683349609375
Epoch 310, training loss: 65.25171661376953 = 1.3575809001922607 + 10.0 * 6.389413356781006
Epoch 310, val loss: 1.4245965480804443
Epoch 320, training loss: 65.16832733154297 = 1.323217749595642 + 10.0 * 6.3845109939575195
Epoch 320, val loss: 1.3989555835723877
Epoch 330, training loss: 65.05972290039062 = 1.2888633012771606 + 10.0 * 6.3770856857299805
Epoch 330, val loss: 1.374105453491211
Epoch 340, training loss: 64.99868774414062 = 1.254903793334961 + 10.0 * 6.374378681182861
Epoch 340, val loss: 1.3496993780136108
Epoch 350, training loss: 64.9086685180664 = 1.2212727069854736 + 10.0 * 6.368739604949951
Epoch 350, val loss: 1.3257375955581665
Epoch 360, training loss: 64.82823944091797 = 1.1881979703903198 + 10.0 * 6.364004611968994
Epoch 360, val loss: 1.3028911352157593
Epoch 370, training loss: 64.78014373779297 = 1.1559126377105713 + 10.0 * 6.362422943115234
Epoch 370, val loss: 1.280748963356018
Epoch 380, training loss: 64.7378158569336 = 1.124251365661621 + 10.0 * 6.361356258392334
Epoch 380, val loss: 1.2598106861114502
Epoch 390, training loss: 64.63198852539062 = 1.0938457250595093 + 10.0 * 6.353814125061035
Epoch 390, val loss: 1.239798665046692
Epoch 400, training loss: 64.56375885009766 = 1.0644288063049316 + 10.0 * 6.34993314743042
Epoch 400, val loss: 1.2209397554397583
Epoch 410, training loss: 64.50125885009766 = 1.035913348197937 + 10.0 * 6.346534729003906
Epoch 410, val loss: 1.2029882669448853
Epoch 420, training loss: 64.4856948852539 = 1.0082818269729614 + 10.0 * 6.34774112701416
Epoch 420, val loss: 1.1860406398773193
Epoch 430, training loss: 64.4144287109375 = 0.9814586043357849 + 10.0 * 6.343297004699707
Epoch 430, val loss: 1.169638991355896
Epoch 440, training loss: 64.34237670898438 = 0.9556102752685547 + 10.0 * 6.338676452636719
Epoch 440, val loss: 1.1542723178863525
Epoch 450, training loss: 64.32070922851562 = 0.9306849241256714 + 10.0 * 6.3390021324157715
Epoch 450, val loss: 1.1398814916610718
Epoch 460, training loss: 64.26720428466797 = 0.9064508676528931 + 10.0 * 6.336075782775879
Epoch 460, val loss: 1.1261924505233765
Epoch 470, training loss: 64.18950653076172 = 0.8831163048744202 + 10.0 * 6.330638885498047
Epoch 470, val loss: 1.1133253574371338
Epoch 480, training loss: 64.17755126953125 = 0.8606248497962952 + 10.0 * 6.331692695617676
Epoch 480, val loss: 1.101074457168579
Epoch 490, training loss: 64.13278198242188 = 0.838654100894928 + 10.0 * 6.329412937164307
Epoch 490, val loss: 1.0896942615509033
Epoch 500, training loss: 64.056640625 = 0.8174684643745422 + 10.0 * 6.323916912078857
Epoch 500, val loss: 1.078840970993042
Epoch 510, training loss: 64.01040649414062 = 0.7968681454658508 + 10.0 * 6.321353912353516
Epoch 510, val loss: 1.0685834884643555
Epoch 520, training loss: 64.02679443359375 = 0.7766972780227661 + 10.0 * 6.325009822845459
Epoch 520, val loss: 1.0588250160217285
Epoch 530, training loss: 63.9341926574707 = 0.7570276260375977 + 10.0 * 6.317716598510742
Epoch 530, val loss: 1.049538493156433
Epoch 540, training loss: 63.91376495361328 = 0.737708330154419 + 10.0 * 6.317605495452881
Epoch 540, val loss: 1.040710687637329
Epoch 550, training loss: 63.86200714111328 = 0.7186991572380066 + 10.0 * 6.314330577850342
Epoch 550, val loss: 1.032358169555664
Epoch 560, training loss: 63.81797790527344 = 0.7000651955604553 + 10.0 * 6.31179141998291
Epoch 560, val loss: 1.0244169235229492
Epoch 570, training loss: 63.8226432800293 = 0.6816267967224121 + 10.0 * 6.314101696014404
Epoch 570, val loss: 1.0168479681015015
Epoch 580, training loss: 63.7792854309082 = 0.6633313894271851 + 10.0 * 6.311595439910889
Epoch 580, val loss: 1.0095347166061401
Epoch 590, training loss: 63.7120475769043 = 0.6453692317008972 + 10.0 * 6.306667804718018
Epoch 590, val loss: 1.0026735067367554
Epoch 600, training loss: 63.716583251953125 = 0.6276166439056396 + 10.0 * 6.308896541595459
Epoch 600, val loss: 0.996088981628418
Epoch 610, training loss: 63.664852142333984 = 0.6099145412445068 + 10.0 * 6.3054938316345215
Epoch 610, val loss: 0.9898709058761597
Epoch 620, training loss: 63.61676788330078 = 0.5925448536872864 + 10.0 * 6.302422523498535
Epoch 620, val loss: 0.983916699886322
Epoch 630, training loss: 63.57929229736328 = 0.5753551125526428 + 10.0 * 6.300393581390381
Epoch 630, val loss: 0.9784723520278931
Epoch 640, training loss: 63.64503479003906 = 0.5582942962646484 + 10.0 * 6.308674335479736
Epoch 640, val loss: 0.973350465297699
Epoch 650, training loss: 63.52183532714844 = 0.5414053797721863 + 10.0 * 6.298043251037598
Epoch 650, val loss: 0.9683398008346558
Epoch 660, training loss: 63.491844177246094 = 0.5248098969459534 + 10.0 * 6.296703338623047
Epoch 660, val loss: 0.9638910293579102
Epoch 670, training loss: 63.45337677001953 = 0.5084764957427979 + 10.0 * 6.294489860534668
Epoch 670, val loss: 0.9599379897117615
Epoch 680, training loss: 63.51996994018555 = 0.49242764711380005 + 10.0 * 6.3027544021606445
Epoch 680, val loss: 0.9563257098197937
Epoch 690, training loss: 63.40395736694336 = 0.4765295088291168 + 10.0 * 6.292742729187012
Epoch 690, val loss: 0.9527691602706909
Epoch 700, training loss: 63.38711166381836 = 0.4610462784767151 + 10.0 * 6.292606353759766
Epoch 700, val loss: 0.9497658014297485
Epoch 710, training loss: 63.36064529418945 = 0.44591599702835083 + 10.0 * 6.291472911834717
Epoch 710, val loss: 0.9471912384033203
Epoch 720, training loss: 63.3432502746582 = 0.4311622679233551 + 10.0 * 6.291208744049072
Epoch 720, val loss: 0.9450623989105225
Epoch 730, training loss: 63.30254364013672 = 0.4166553318500519 + 10.0 * 6.288588523864746
Epoch 730, val loss: 0.9430868625640869
Epoch 740, training loss: 63.267391204833984 = 0.40259861946105957 + 10.0 * 6.2864789962768555
Epoch 740, val loss: 0.9416356086730957
Epoch 750, training loss: 63.2628173828125 = 0.38892099261283875 + 10.0 * 6.287389755249023
Epoch 750, val loss: 0.9405012726783752
Epoch 760, training loss: 63.22844314575195 = 0.3755611479282379 + 10.0 * 6.285288333892822
Epoch 760, val loss: 0.9397915005683899
Epoch 770, training loss: 63.197174072265625 = 0.36258119344711304 + 10.0 * 6.283459663391113
Epoch 770, val loss: 0.9393118619918823
Epoch 780, training loss: 63.17178726196289 = 0.3500704765319824 + 10.0 * 6.282171726226807
Epoch 780, val loss: 0.9392844438552856
Epoch 790, training loss: 63.18886947631836 = 0.3378940522670746 + 10.0 * 6.285097599029541
Epoch 790, val loss: 0.9395017623901367
Epoch 800, training loss: 63.152591705322266 = 0.32605811953544617 + 10.0 * 6.282653331756592
Epoch 800, val loss: 0.9399033784866333
Epoch 810, training loss: 63.12907791137695 = 0.314632385969162 + 10.0 * 6.281444549560547
Epoch 810, val loss: 0.9405708312988281
Epoch 820, training loss: 63.116207122802734 = 0.30360689759254456 + 10.0 * 6.281260013580322
Epoch 820, val loss: 0.9415721893310547
Epoch 830, training loss: 63.066287994384766 = 0.2929339110851288 + 10.0 * 6.277335166931152
Epoch 830, val loss: 0.9428327679634094
Epoch 840, training loss: 63.04292678833008 = 0.2826967239379883 + 10.0 * 6.276022911071777
Epoch 840, val loss: 0.9443674683570862
Epoch 850, training loss: 63.06977844238281 = 0.2728016674518585 + 10.0 * 6.279697895050049
Epoch 850, val loss: 0.9460539221763611
Epoch 860, training loss: 63.03077697753906 = 0.2632112503051758 + 10.0 * 6.276756763458252
Epoch 860, val loss: 0.9479937553405762
Epoch 870, training loss: 63.00223159790039 = 0.253980427980423 + 10.0 * 6.274825096130371
Epoch 870, val loss: 0.9499901533126831
Epoch 880, training loss: 63.02423858642578 = 0.24513235688209534 + 10.0 * 6.2779107093811035
Epoch 880, val loss: 0.952286422252655
Epoch 890, training loss: 62.9784049987793 = 0.23651064932346344 + 10.0 * 6.274189472198486
Epoch 890, val loss: 0.954505980014801
Epoch 900, training loss: 62.942466735839844 = 0.22832423448562622 + 10.0 * 6.271414279937744
Epoch 900, val loss: 0.9571099281311035
Epoch 910, training loss: 62.924766540527344 = 0.2204148769378662 + 10.0 * 6.270435333251953
Epoch 910, val loss: 0.9598449468612671
Epoch 920, training loss: 62.97404861450195 = 0.21281671524047852 + 10.0 * 6.276123046875
Epoch 920, val loss: 0.9626888632774353
Epoch 930, training loss: 62.905738830566406 = 0.20543642342090607 + 10.0 * 6.2700300216674805
Epoch 930, val loss: 0.9654844999313354
Epoch 940, training loss: 62.900596618652344 = 0.19837306439876556 + 10.0 * 6.2702226638793945
Epoch 940, val loss: 0.968726396560669
Epoch 950, training loss: 62.86628723144531 = 0.19158944487571716 + 10.0 * 6.267469882965088
Epoch 950, val loss: 0.971979558467865
Epoch 960, training loss: 62.88209915161133 = 0.18507596850395203 + 10.0 * 6.269701957702637
Epoch 960, val loss: 0.9753444194793701
Epoch 970, training loss: 62.85926818847656 = 0.1787872314453125 + 10.0 * 6.268048286437988
Epoch 970, val loss: 0.9788922071456909
Epoch 980, training loss: 62.84245681762695 = 0.17274974286556244 + 10.0 * 6.266970634460449
Epoch 980, val loss: 0.9824725985527039
Epoch 990, training loss: 62.82119369506836 = 0.1669512540102005 + 10.0 * 6.2654242515563965
Epoch 990, val loss: 0.9861176609992981
Epoch 1000, training loss: 62.82795715332031 = 0.16136127710342407 + 10.0 * 6.266659736633301
Epoch 1000, val loss: 0.9898921251296997
Epoch 1010, training loss: 62.826236724853516 = 0.15598128736019135 + 10.0 * 6.267025470733643
Epoch 1010, val loss: 0.9937455654144287
Epoch 1020, training loss: 62.78447723388672 = 0.1508161574602127 + 10.0 * 6.263365745544434
Epoch 1020, val loss: 0.9977825284004211
Epoch 1030, training loss: 62.765892028808594 = 0.14587508141994476 + 10.0 * 6.262001991271973
Epoch 1030, val loss: 1.001981496810913
Epoch 1040, training loss: 62.75099563598633 = 0.14112623035907745 + 10.0 * 6.260987281799316
Epoch 1040, val loss: 1.006240725517273
Epoch 1050, training loss: 62.8071174621582 = 0.13656896352767944 + 10.0 * 6.267054557800293
Epoch 1050, val loss: 1.0106935501098633
Epoch 1060, training loss: 62.76884460449219 = 0.1321081668138504 + 10.0 * 6.263673782348633
Epoch 1060, val loss: 1.0148875713348389
Epoch 1070, training loss: 62.727779388427734 = 0.12787362933158875 + 10.0 * 6.259990692138672
Epoch 1070, val loss: 1.0193963050842285
Epoch 1080, training loss: 62.733978271484375 = 0.12380287796258926 + 10.0 * 6.261017799377441
Epoch 1080, val loss: 1.0240085124969482
Epoch 1090, training loss: 62.722747802734375 = 0.11986788362264633 + 10.0 * 6.260287761688232
Epoch 1090, val loss: 1.028698444366455
Epoch 1100, training loss: 62.7030029296875 = 0.1160833016037941 + 10.0 * 6.258691787719727
Epoch 1100, val loss: 1.033373475074768
Epoch 1110, training loss: 62.68846130371094 = 0.11242949217557907 + 10.0 * 6.257603168487549
Epoch 1110, val loss: 1.0381605625152588
Epoch 1120, training loss: 62.69709777832031 = 0.10891991853713989 + 10.0 * 6.258817672729492
Epoch 1120, val loss: 1.0430582761764526
Epoch 1130, training loss: 62.663089752197266 = 0.10552923381328583 + 10.0 * 6.25575590133667
Epoch 1130, val loss: 1.048118233680725
Epoch 1140, training loss: 62.662113189697266 = 0.10227823257446289 + 10.0 * 6.255983352661133
Epoch 1140, val loss: 1.0531275272369385
Epoch 1150, training loss: 62.66664505004883 = 0.09915783256292343 + 10.0 * 6.256748676300049
Epoch 1150, val loss: 1.0582983493804932
Epoch 1160, training loss: 62.66587829589844 = 0.09614170342683792 + 10.0 * 6.256973743438721
Epoch 1160, val loss: 1.0632833242416382
Epoch 1170, training loss: 62.629852294921875 = 0.09323298931121826 + 10.0 * 6.253662109375
Epoch 1170, val loss: 1.068552017211914
Epoch 1180, training loss: 62.644508361816406 = 0.09045494347810745 + 10.0 * 6.255405426025391
Epoch 1180, val loss: 1.0737957954406738
Epoch 1190, training loss: 62.621944427490234 = 0.08777321130037308 + 10.0 * 6.253417015075684
Epoch 1190, val loss: 1.079217791557312
Epoch 1200, training loss: 62.63255310058594 = 0.08519817143678665 + 10.0 * 6.254735469818115
Epoch 1200, val loss: 1.0845117568969727
Epoch 1210, training loss: 62.635643005371094 = 0.08270375430583954 + 10.0 * 6.255293846130371
Epoch 1210, val loss: 1.0897854566574097
Epoch 1220, training loss: 62.62640380859375 = 0.08030333369970322 + 10.0 * 6.254610061645508
Epoch 1220, val loss: 1.095305323600769
Epoch 1230, training loss: 62.588233947753906 = 0.07800152152776718 + 10.0 * 6.251023292541504
Epoch 1230, val loss: 1.1008450984954834
Epoch 1240, training loss: 62.587642669677734 = 0.07578533887863159 + 10.0 * 6.251185417175293
Epoch 1240, val loss: 1.106367588043213
Epoch 1250, training loss: 62.6105842590332 = 0.07362892478704453 + 10.0 * 6.253695487976074
Epoch 1250, val loss: 1.1117236614227295
Epoch 1260, training loss: 62.56986999511719 = 0.07154238224029541 + 10.0 * 6.249833106994629
Epoch 1260, val loss: 1.1171377897262573
Epoch 1270, training loss: 62.56089782714844 = 0.06955423951148987 + 10.0 * 6.249134540557861
Epoch 1270, val loss: 1.1227202415466309
Epoch 1280, training loss: 62.56867599487305 = 0.0676388218998909 + 10.0 * 6.250103950500488
Epoch 1280, val loss: 1.1281373500823975
Epoch 1290, training loss: 62.550601959228516 = 0.06579235196113586 + 10.0 * 6.248480796813965
Epoch 1290, val loss: 1.1336981058120728
Epoch 1300, training loss: 62.555606842041016 = 0.06401689350605011 + 10.0 * 6.24915885925293
Epoch 1300, val loss: 1.139373540878296
Epoch 1310, training loss: 62.590797424316406 = 0.06230028718709946 + 10.0 * 6.252849578857422
Epoch 1310, val loss: 1.1447275876998901
Epoch 1320, training loss: 62.55754089355469 = 0.06061367690563202 + 10.0 * 6.249692440032959
Epoch 1320, val loss: 1.1500998735427856
Epoch 1330, training loss: 62.52297592163086 = 0.059008486568927765 + 10.0 * 6.246396541595459
Epoch 1330, val loss: 1.1554566621780396
Epoch 1340, training loss: 62.511817932128906 = 0.057474784553050995 + 10.0 * 6.245434284210205
Epoch 1340, val loss: 1.161033034324646
Epoch 1350, training loss: 62.541351318359375 = 0.05599761754274368 + 10.0 * 6.248535633087158
Epoch 1350, val loss: 1.1664330959320068
Epoch 1360, training loss: 62.5070686340332 = 0.05455769971013069 + 10.0 * 6.245251178741455
Epoch 1360, val loss: 1.171729326248169
Epoch 1370, training loss: 62.50929641723633 = 0.053164854645729065 + 10.0 * 6.245613098144531
Epoch 1370, val loss: 1.1772170066833496
Epoch 1380, training loss: 62.503570556640625 = 0.05183016136288643 + 10.0 * 6.245173931121826
Epoch 1380, val loss: 1.182617425918579
Epoch 1390, training loss: 62.57484436035156 = 0.05054253339767456 + 10.0 * 6.252430438995361
Epoch 1390, val loss: 1.1881474256515503
Epoch 1400, training loss: 62.504459381103516 = 0.049275461584329605 + 10.0 * 6.245518684387207
Epoch 1400, val loss: 1.1930137872695923
Epoch 1410, training loss: 62.48123550415039 = 0.04807436093688011 + 10.0 * 6.243316173553467
Epoch 1410, val loss: 1.1985766887664795
Epoch 1420, training loss: 62.4741325378418 = 0.04691902920603752 + 10.0 * 6.242721080780029
Epoch 1420, val loss: 1.2038781642913818
Epoch 1430, training loss: 62.50267791748047 = 0.045808736234903336 + 10.0 * 6.245687007904053
Epoch 1430, val loss: 1.2091395854949951
Epoch 1440, training loss: 62.47172927856445 = 0.044703301042318344 + 10.0 * 6.242702484130859
Epoch 1440, val loss: 1.2142016887664795
Epoch 1450, training loss: 62.46637725830078 = 0.04364151135087013 + 10.0 * 6.242273807525635
Epoch 1450, val loss: 1.2192268371582031
Epoch 1460, training loss: 62.467926025390625 = 0.04262447729706764 + 10.0 * 6.24252986907959
Epoch 1460, val loss: 1.2244681119918823
Epoch 1470, training loss: 62.48005676269531 = 0.041645292192697525 + 10.0 * 6.243841171264648
Epoch 1470, val loss: 1.2296037673950195
Epoch 1480, training loss: 62.465084075927734 = 0.040689412504434586 + 10.0 * 6.2424397468566895
Epoch 1480, val loss: 1.2346675395965576
Epoch 1490, training loss: 62.44340896606445 = 0.03976709395647049 + 10.0 * 6.240364074707031
Epoch 1490, val loss: 1.2394882440567017
Epoch 1500, training loss: 62.446441650390625 = 0.03887401148676872 + 10.0 * 6.240756511688232
Epoch 1500, val loss: 1.2445684671401978
Epoch 1510, training loss: 62.49340057373047 = 0.03801583871245384 + 10.0 * 6.245538234710693
Epoch 1510, val loss: 1.2496119737625122
Epoch 1520, training loss: 62.46592712402344 = 0.03716794401407242 + 10.0 * 6.242876052856445
Epoch 1520, val loss: 1.254536747932434
Epoch 1530, training loss: 62.44965744018555 = 0.03634936362504959 + 10.0 * 6.241330623626709
Epoch 1530, val loss: 1.2591934204101562
Epoch 1540, training loss: 62.45656967163086 = 0.03556676581501961 + 10.0 * 6.242100238800049
Epoch 1540, val loss: 1.2641162872314453
Epoch 1550, training loss: 62.419944763183594 = 0.034795794636011124 + 10.0 * 6.2385149002075195
Epoch 1550, val loss: 1.2688089609146118
Epoch 1560, training loss: 62.422183990478516 = 0.03406402841210365 + 10.0 * 6.23881196975708
Epoch 1560, val loss: 1.2737466096878052
Epoch 1570, training loss: 62.450416564941406 = 0.033348772674798965 + 10.0 * 6.241706848144531
Epoch 1570, val loss: 1.278396487236023
Epoch 1580, training loss: 62.43339157104492 = 0.032650694251060486 + 10.0 * 6.240074157714844
Epoch 1580, val loss: 1.2832177877426147
Epoch 1590, training loss: 62.411312103271484 = 0.031974900513887405 + 10.0 * 6.23793363571167
Epoch 1590, val loss: 1.288043737411499
Epoch 1600, training loss: 62.41329574584961 = 0.03132219985127449 + 10.0 * 6.238197326660156
Epoch 1600, val loss: 1.292589783668518
Epoch 1610, training loss: 62.396751403808594 = 0.03068488836288452 + 10.0 * 6.236606597900391
Epoch 1610, val loss: 1.2973040342330933
Epoch 1620, training loss: 62.491241455078125 = 0.03007495030760765 + 10.0 * 6.246116638183594
Epoch 1620, val loss: 1.3022632598876953
Epoch 1630, training loss: 62.41596984863281 = 0.029463177546858788 + 10.0 * 6.238650321960449
Epoch 1630, val loss: 1.3062297105789185
Epoch 1640, training loss: 62.39011764526367 = 0.028877636417746544 + 10.0 * 6.236124038696289
Epoch 1640, val loss: 1.3110443353652954
Epoch 1650, training loss: 62.38420486450195 = 0.028322696685791016 + 10.0 * 6.235588073730469
Epoch 1650, val loss: 1.3156819343566895
Epoch 1660, training loss: 62.46554183959961 = 0.02777669206261635 + 10.0 * 6.243776798248291
Epoch 1660, val loss: 1.3201688528060913
Epoch 1670, training loss: 62.389244079589844 = 0.027230747044086456 + 10.0 * 6.236201286315918
Epoch 1670, val loss: 1.3245998620986938
Epoch 1680, training loss: 62.368324279785156 = 0.026715073734521866 + 10.0 * 6.234160900115967
Epoch 1680, val loss: 1.3291471004486084
Epoch 1690, training loss: 62.39087677001953 = 0.026213230565190315 + 10.0 * 6.236466407775879
Epoch 1690, val loss: 1.3336176872253418
Epoch 1700, training loss: 62.36389923095703 = 0.025722561404109 + 10.0 * 6.2338175773620605
Epoch 1700, val loss: 1.3379710912704468
Epoch 1710, training loss: 62.371707916259766 = 0.025249062106013298 + 10.0 * 6.234645843505859
Epoch 1710, val loss: 1.3425397872924805
Epoch 1720, training loss: 62.38330078125 = 0.024784190580248833 + 10.0 * 6.235851764678955
Epoch 1720, val loss: 1.346663236618042
Epoch 1730, training loss: 62.386253356933594 = 0.024336613714694977 + 10.0 * 6.236191749572754
Epoch 1730, val loss: 1.351021409034729
Epoch 1740, training loss: 62.35934066772461 = 0.023888787254691124 + 10.0 * 6.233545303344727
Epoch 1740, val loss: 1.355300784111023
Epoch 1750, training loss: 62.351802825927734 = 0.023461680859327316 + 10.0 * 6.232834339141846
Epoch 1750, val loss: 1.359616994857788
Epoch 1760, training loss: 62.34001922607422 = 0.02304847165942192 + 10.0 * 6.231697082519531
Epoch 1760, val loss: 1.3639789819717407
Epoch 1770, training loss: 62.38167190551758 = 0.022647028788924217 + 10.0 * 6.235902309417725
Epoch 1770, val loss: 1.3679929971694946
Epoch 1780, training loss: 62.34217834472656 = 0.02224775217473507 + 10.0 * 6.231993198394775
Epoch 1780, val loss: 1.3723405599594116
Epoch 1790, training loss: 62.33708572387695 = 0.021862441673874855 + 10.0 * 6.231522560119629
Epoch 1790, val loss: 1.3765268325805664
Epoch 1800, training loss: 62.35795593261719 = 0.02149142511188984 + 10.0 * 6.233646392822266
Epoch 1800, val loss: 1.3805092573165894
Epoch 1810, training loss: 62.35620880126953 = 0.021127328276634216 + 10.0 * 6.233508110046387
Epoch 1810, val loss: 1.3848475217819214
Epoch 1820, training loss: 62.352020263671875 = 0.020763998851180077 + 10.0 * 6.233125686645508
Epoch 1820, val loss: 1.3887516260147095
Epoch 1830, training loss: 62.34311294555664 = 0.020416460931301117 + 10.0 * 6.232269763946533
Epoch 1830, val loss: 1.3929307460784912
Epoch 1840, training loss: 62.33766555786133 = 0.020078150555491447 + 10.0 * 6.231759071350098
Epoch 1840, val loss: 1.3971067667007446
Epoch 1850, training loss: 62.31886672973633 = 0.019748976454138756 + 10.0 * 6.229911804199219
Epoch 1850, val loss: 1.4009257555007935
Epoch 1860, training loss: 62.32963943481445 = 0.01942894235253334 + 10.0 * 6.231020927429199
Epoch 1860, val loss: 1.4049859046936035
Epoch 1870, training loss: 62.34616470336914 = 0.01911500096321106 + 10.0 * 6.232705116271973
Epoch 1870, val loss: 1.4089276790618896
Epoch 1880, training loss: 62.33488845825195 = 0.018798943608999252 + 10.0 * 6.231608867645264
Epoch 1880, val loss: 1.4128367900848389
Epoch 1890, training loss: 62.3103141784668 = 0.018499499186873436 + 10.0 * 6.229181289672852
Epoch 1890, val loss: 1.4166030883789062
Epoch 1900, training loss: 62.3083381652832 = 0.018208608031272888 + 10.0 * 6.229012966156006
Epoch 1900, val loss: 1.4207839965820312
Epoch 1910, training loss: 62.34785079956055 = 0.01793232187628746 + 10.0 * 6.232991695404053
Epoch 1910, val loss: 1.4246169328689575
Epoch 1920, training loss: 62.29633331298828 = 0.017644420266151428 + 10.0 * 6.227869033813477
Epoch 1920, val loss: 1.4282556772232056
Epoch 1930, training loss: 62.299190521240234 = 0.017375243827700615 + 10.0 * 6.2281813621521
Epoch 1930, val loss: 1.432215929031372
Epoch 1940, training loss: 62.32183074951172 = 0.017111776396632195 + 10.0 * 6.230471611022949
Epoch 1940, val loss: 1.4360370635986328
Epoch 1950, training loss: 62.311119079589844 = 0.016847210004925728 + 10.0 * 6.229427337646484
Epoch 1950, val loss: 1.4394562244415283
Epoch 1960, training loss: 62.28926086425781 = 0.016597725450992584 + 10.0 * 6.227266311645508
Epoch 1960, val loss: 1.4433817863464355
Epoch 1970, training loss: 62.292022705078125 = 0.01635095477104187 + 10.0 * 6.227567195892334
Epoch 1970, val loss: 1.4469397068023682
Epoch 1980, training loss: 62.294368743896484 = 0.016110463067889214 + 10.0 * 6.227826118469238
Epoch 1980, val loss: 1.4506078958511353
Epoch 1990, training loss: 62.32615661621094 = 0.015877000987529755 + 10.0 * 6.231028079986572
Epoch 1990, val loss: 1.4544196128845215
Epoch 2000, training loss: 62.304134368896484 = 0.01563836820423603 + 10.0 * 6.228849411010742
Epoch 2000, val loss: 1.457948088645935
Epoch 2010, training loss: 62.294586181640625 = 0.01540960930287838 + 10.0 * 6.227917671203613
Epoch 2010, val loss: 1.4614702463150024
Epoch 2020, training loss: 62.2817497253418 = 0.015189701691269875 + 10.0 * 6.226655960083008
Epoch 2020, val loss: 1.465182900428772
Epoch 2030, training loss: 62.274879455566406 = 0.014977443031966686 + 10.0 * 6.225990295410156
Epoch 2030, val loss: 1.4688819646835327
Epoch 2040, training loss: 62.29248046875 = 0.01476858276873827 + 10.0 * 6.227771282196045
Epoch 2040, val loss: 1.4725027084350586
Epoch 2050, training loss: 62.30791091918945 = 0.014561179094016552 + 10.0 * 6.229334831237793
Epoch 2050, val loss: 1.4759564399719238
Epoch 2060, training loss: 62.26283264160156 = 0.014355081133544445 + 10.0 * 6.224847793579102
Epoch 2060, val loss: 1.4792174100875854
Epoch 2070, training loss: 62.26285934448242 = 0.01415687520056963 + 10.0 * 6.224870204925537
Epoch 2070, val loss: 1.4828153848648071
Epoch 2080, training loss: 62.32736587524414 = 0.013966646045446396 + 10.0 * 6.231339931488037
Epoch 2080, val loss: 1.4861115217208862
Epoch 2090, training loss: 62.26757049560547 = 0.013774205930531025 + 10.0 * 6.225379467010498
Epoch 2090, val loss: 1.489822268486023
Epoch 2100, training loss: 62.24983596801758 = 0.013589329086244106 + 10.0 * 6.2236247062683105
Epoch 2100, val loss: 1.4931128025054932
Epoch 2110, training loss: 62.247802734375 = 0.01341269165277481 + 10.0 * 6.2234392166137695
Epoch 2110, val loss: 1.496687412261963
Epoch 2120, training loss: 62.32024002075195 = 0.013241291977465153 + 10.0 * 6.2307000160217285
Epoch 2120, val loss: 1.5001579523086548
Epoch 2130, training loss: 62.283599853515625 = 0.013057155534625053 + 10.0 * 6.227054119110107
Epoch 2130, val loss: 1.5031174421310425
Epoch 2140, training loss: 62.25748825073242 = 0.012885324656963348 + 10.0 * 6.224460124969482
Epoch 2140, val loss: 1.5064167976379395
Epoch 2150, training loss: 62.23877716064453 = 0.012718540616333485 + 10.0 * 6.2226057052612305
Epoch 2150, val loss: 1.509613037109375
Epoch 2160, training loss: 62.241939544677734 = 0.012560410425066948 + 10.0 * 6.22293758392334
Epoch 2160, val loss: 1.5130553245544434
Epoch 2170, training loss: 62.294776916503906 = 0.0124041847884655 + 10.0 * 6.228237152099609
Epoch 2170, val loss: 1.5161000490188599
Epoch 2180, training loss: 62.28887176513672 = 0.012241728603839874 + 10.0 * 6.227663040161133
Epoch 2180, val loss: 1.519456148147583
Epoch 2190, training loss: 62.238861083984375 = 0.012083365581929684 + 10.0 * 6.222677707672119
Epoch 2190, val loss: 1.5225220918655396
Epoch 2200, training loss: 62.2357292175293 = 0.0119362473487854 + 10.0 * 6.222379207611084
Epoch 2200, val loss: 1.5258605480194092
Epoch 2210, training loss: 62.25672149658203 = 0.011791779659688473 + 10.0 * 6.224493026733398
Epoch 2210, val loss: 1.5291650295257568
Epoch 2220, training loss: 62.24223327636719 = 0.011645261198282242 + 10.0 * 6.223058700561523
Epoch 2220, val loss: 1.5319492816925049
Epoch 2230, training loss: 62.255516052246094 = 0.011502495035529137 + 10.0 * 6.224401473999023
Epoch 2230, val loss: 1.5350000858306885
Epoch 2240, training loss: 62.238250732421875 = 0.011360741220414639 + 10.0 * 6.222689151763916
Epoch 2240, val loss: 1.5380761623382568
Epoch 2250, training loss: 62.247825622558594 = 0.011225064285099506 + 10.0 * 6.223659992218018
Epoch 2250, val loss: 1.5410151481628418
Epoch 2260, training loss: 62.21865463256836 = 0.011089563369750977 + 10.0 * 6.220756530761719
Epoch 2260, val loss: 1.5442909002304077
Epoch 2270, training loss: 62.214237213134766 = 0.010960947722196579 + 10.0 * 6.220327854156494
Epoch 2270, val loss: 1.5472909212112427
Epoch 2280, training loss: 62.21211242675781 = 0.010835809633135796 + 10.0 * 6.220127582550049
Epoch 2280, val loss: 1.5504039525985718
Epoch 2290, training loss: 62.2765998840332 = 0.010713805444538593 + 10.0 * 6.226588249206543
Epoch 2290, val loss: 1.553414225578308
Epoch 2300, training loss: 62.22214889526367 = 0.010581827722489834 + 10.0 * 6.221156597137451
Epoch 2300, val loss: 1.5563281774520874
Epoch 2310, training loss: 62.23101806640625 = 0.010462815873324871 + 10.0 * 6.222055435180664
Epoch 2310, val loss: 1.5592093467712402
Epoch 2320, training loss: 62.23041534423828 = 0.010339521802961826 + 10.0 * 6.222007751464844
Epoch 2320, val loss: 1.5619374513626099
Epoch 2330, training loss: 62.214298248291016 = 0.010222116485238075 + 10.0 * 6.220407485961914
Epoch 2330, val loss: 1.5648826360702515
Epoch 2340, training loss: 62.21576690673828 = 0.010108526796102524 + 10.0 * 6.2205657958984375
Epoch 2340, val loss: 1.5678231716156006
Epoch 2350, training loss: 62.23274612426758 = 0.009996546432375908 + 10.0 * 6.2222747802734375
Epoch 2350, val loss: 1.5706034898757935
Epoch 2360, training loss: 62.19413757324219 = 0.00988547969609499 + 10.0 * 6.218425273895264
Epoch 2360, val loss: 1.5735807418823242
Epoch 2370, training loss: 62.211666107177734 = 0.009780362248420715 + 10.0 * 6.220188617706299
Epoch 2370, val loss: 1.5762581825256348
Epoch 2380, training loss: 62.222103118896484 = 0.00967120099812746 + 10.0 * 6.221243381500244
Epoch 2380, val loss: 1.5790717601776123
Epoch 2390, training loss: 62.208160400390625 = 0.009567554108798504 + 10.0 * 6.2198591232299805
Epoch 2390, val loss: 1.5820592641830444
Epoch 2400, training loss: 62.23044204711914 = 0.009465359151363373 + 10.0 * 6.222097873687744
Epoch 2400, val loss: 1.5845648050308228
Epoch 2410, training loss: 62.23226547241211 = 0.009362432174384594 + 10.0 * 6.222290515899658
Epoch 2410, val loss: 1.5871269702911377
Epoch 2420, training loss: 62.187686920166016 = 0.00926024280488491 + 10.0 * 6.2178425788879395
Epoch 2420, val loss: 1.5900887250900269
Epoch 2430, training loss: 62.18113708496094 = 0.009164858609437943 + 10.0 * 6.217196941375732
Epoch 2430, val loss: 1.5929508209228516
Epoch 2440, training loss: 62.18299102783203 = 0.009072288870811462 + 10.0 * 6.2173919677734375
Epoch 2440, val loss: 1.5955921411514282
Epoch 2450, training loss: 62.247772216796875 = 0.008979576639831066 + 10.0 * 6.223879337310791
Epoch 2450, val loss: 1.5982437133789062
Epoch 2460, training loss: 62.20083236694336 = 0.008884428068995476 + 10.0 * 6.2191948890686035
Epoch 2460, val loss: 1.6011112928390503
Epoch 2470, training loss: 62.18808364868164 = 0.008793352171778679 + 10.0 * 6.217928886413574
Epoch 2470, val loss: 1.6035358905792236
Epoch 2480, training loss: 62.22908401489258 = 0.008705398999154568 + 10.0 * 6.2220377922058105
Epoch 2480, val loss: 1.6062424182891846
Epoch 2490, training loss: 62.18655014038086 = 0.008617007173597813 + 10.0 * 6.2177934646606445
Epoch 2490, val loss: 1.608757495880127
Epoch 2500, training loss: 62.1824836730957 = 0.008531498722732067 + 10.0 * 6.217395305633545
Epoch 2500, val loss: 1.6116061210632324
Epoch 2510, training loss: 62.21656036376953 = 0.008446785621345043 + 10.0 * 6.220811367034912
Epoch 2510, val loss: 1.6141339540481567
Epoch 2520, training loss: 62.18229293823242 = 0.008358921855688095 + 10.0 * 6.217393398284912
Epoch 2520, val loss: 1.6166911125183105
Epoch 2530, training loss: 62.16935348510742 = 0.008279905654489994 + 10.0 * 6.216107368469238
Epoch 2530, val loss: 1.6192961931228638
Epoch 2540, training loss: 62.19768524169922 = 0.00820300541818142 + 10.0 * 6.2189483642578125
Epoch 2540, val loss: 1.6219710111618042
Epoch 2550, training loss: 62.16743469238281 = 0.008121499791741371 + 10.0 * 6.215931415557861
Epoch 2550, val loss: 1.6241487264633179
Epoch 2560, training loss: 62.17829895019531 = 0.008043370209634304 + 10.0 * 6.217025279998779
Epoch 2560, val loss: 1.6266266107559204
Epoch 2570, training loss: 62.17357635498047 = 0.007967272773385048 + 10.0 * 6.2165608406066895
Epoch 2570, val loss: 1.6293113231658936
Epoch 2580, training loss: 62.217838287353516 = 0.007892027497291565 + 10.0 * 6.220994472503662
Epoch 2580, val loss: 1.6317049264907837
Epoch 2590, training loss: 62.1710205078125 = 0.007817437872290611 + 10.0 * 6.216320514678955
Epoch 2590, val loss: 1.6343497037887573
Epoch 2600, training loss: 62.19063186645508 = 0.007745928596705198 + 10.0 * 6.218288421630859
Epoch 2600, val loss: 1.6368135213851929
Epoch 2610, training loss: 62.15860366821289 = 0.007671713829040527 + 10.0 * 6.21509313583374
Epoch 2610, val loss: 1.6390509605407715
Epoch 2620, training loss: 62.1600227355957 = 0.007601331453770399 + 10.0 * 6.2152419090271
Epoch 2620, val loss: 1.6413754224777222
Epoch 2630, training loss: 62.15394973754883 = 0.007533538155257702 + 10.0 * 6.214641571044922
Epoch 2630, val loss: 1.6439250707626343
Epoch 2640, training loss: 62.2342414855957 = 0.007465178146958351 + 10.0 * 6.222677707672119
Epoch 2640, val loss: 1.6460627317428589
Epoch 2650, training loss: 62.17609786987305 = 0.007397257722914219 + 10.0 * 6.216870307922363
Epoch 2650, val loss: 1.6485557556152344
Epoch 2660, training loss: 62.156341552734375 = 0.007329639047384262 + 10.0 * 6.214901447296143
Epoch 2660, val loss: 1.6509435176849365
Epoch 2670, training loss: 62.1679573059082 = 0.007267633453011513 + 10.0 * 6.216069221496582
Epoch 2670, val loss: 1.6534128189086914
Epoch 2680, training loss: 62.15669250488281 = 0.007201963569968939 + 10.0 * 6.214949131011963
Epoch 2680, val loss: 1.6555677652359009
Epoch 2690, training loss: 62.209205627441406 = 0.007140145171433687 + 10.0 * 6.220206260681152
Epoch 2690, val loss: 1.657702088356018
Epoch 2700, training loss: 62.154850006103516 = 0.007076339330524206 + 10.0 * 6.21477746963501
Epoch 2700, val loss: 1.6600335836410522
Epoch 2710, training loss: 62.143245697021484 = 0.007015131413936615 + 10.0 * 6.213623046875
Epoch 2710, val loss: 1.662291407585144
Epoch 2720, training loss: 62.14203643798828 = 0.006957107689231634 + 10.0 * 6.213507652282715
Epoch 2720, val loss: 1.6647069454193115
Epoch 2730, training loss: 62.18470764160156 = 0.006898922845721245 + 10.0 * 6.217780590057373
Epoch 2730, val loss: 1.6668338775634766
Epoch 2740, training loss: 62.14876937866211 = 0.006838689558207989 + 10.0 * 6.214192867279053
Epoch 2740, val loss: 1.6688714027404785
Epoch 2750, training loss: 62.154685974121094 = 0.0067822691053152084 + 10.0 * 6.214790344238281
Epoch 2750, val loss: 1.6713234186172485
Epoch 2760, training loss: 62.15365982055664 = 0.006725446321070194 + 10.0 * 6.214693546295166
Epoch 2760, val loss: 1.673279047012329
Epoch 2770, training loss: 62.15382385253906 = 0.006669644266366959 + 10.0 * 6.214715480804443
Epoch 2770, val loss: 1.6754536628723145
Epoch 2780, training loss: 62.136268615722656 = 0.006614122074097395 + 10.0 * 6.212965488433838
Epoch 2780, val loss: 1.6776633262634277
Epoch 2790, training loss: 62.169830322265625 = 0.006561545189470053 + 10.0 * 6.216326713562012
Epoch 2790, val loss: 1.679523229598999
Epoch 2800, training loss: 62.132137298583984 = 0.00650754664093256 + 10.0 * 6.2125630378723145
Epoch 2800, val loss: 1.68161141872406
Epoch 2810, training loss: 62.13724899291992 = 0.006455579306930304 + 10.0 * 6.213079452514648
Epoch 2810, val loss: 1.6839053630828857
Epoch 2820, training loss: 62.16323471069336 = 0.006404723972082138 + 10.0 * 6.2156829833984375
Epoch 2820, val loss: 1.6857693195343018
Epoch 2830, training loss: 62.12797164916992 = 0.006351904012262821 + 10.0 * 6.212162017822266
Epoch 2830, val loss: 1.688157081604004
Epoch 2840, training loss: 62.13213348388672 = 0.006302409805357456 + 10.0 * 6.212583065032959
Epoch 2840, val loss: 1.6900355815887451
Epoch 2850, training loss: 62.127323150634766 = 0.006253562867641449 + 10.0 * 6.212107181549072
Epoch 2850, val loss: 1.6919993162155151
Epoch 2860, training loss: 62.15995407104492 = 0.006205254700034857 + 10.0 * 6.215374946594238
Epoch 2860, val loss: 1.6939942836761475
Epoch 2870, training loss: 62.119476318359375 = 0.006157903466373682 + 10.0 * 6.211331844329834
Epoch 2870, val loss: 1.6959431171417236
Epoch 2880, training loss: 62.15788269042969 = 0.006112137343734503 + 10.0 * 6.215177059173584
Epoch 2880, val loss: 1.6979939937591553
Epoch 2890, training loss: 62.1246452331543 = 0.0060615758411586285 + 10.0 * 6.21185827255249
Epoch 2890, val loss: 1.6998167037963867
Epoch 2900, training loss: 62.119686126708984 = 0.0060157813131809235 + 10.0 * 6.211367130279541
Epoch 2900, val loss: 1.7019098997116089
Epoch 2910, training loss: 62.132476806640625 = 0.005971941165626049 + 10.0 * 6.212650299072266
Epoch 2910, val loss: 1.7038514614105225
Epoch 2920, training loss: 62.126651763916016 = 0.005927151534706354 + 10.0 * 6.212072372436523
Epoch 2920, val loss: 1.7056721448898315
Epoch 2930, training loss: 62.137454986572266 = 0.005883433856070042 + 10.0 * 6.2131571769714355
Epoch 2930, val loss: 1.7077386379241943
Epoch 2940, training loss: 62.132606506347656 = 0.005839468911290169 + 10.0 * 6.212676525115967
Epoch 2940, val loss: 1.7095407247543335
Epoch 2950, training loss: 62.12093734741211 = 0.005795516539365053 + 10.0 * 6.211514472961426
Epoch 2950, val loss: 1.711477518081665
Epoch 2960, training loss: 62.13752365112305 = 0.0057549006305634975 + 10.0 * 6.213176727294922
Epoch 2960, val loss: 1.7131022214889526
Epoch 2970, training loss: 62.122474670410156 = 0.005713069345802069 + 10.0 * 6.211676120758057
Epoch 2970, val loss: 1.7150684595108032
Epoch 2980, training loss: 62.1007080078125 = 0.005672281142324209 + 10.0 * 6.209503650665283
Epoch 2980, val loss: 1.717100739479065
Epoch 2990, training loss: 62.11970901489258 = 0.005633912514895201 + 10.0 * 6.211407661437988
Epoch 2990, val loss: 1.7190841436386108
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 87.92462921142578 = 1.9559826850891113 + 10.0 * 8.596864700317383
Epoch 0, val loss: 1.9612396955490112
Epoch 10, training loss: 87.9095458984375 = 1.9450469017028809 + 10.0 * 8.596449851989746
Epoch 10, val loss: 1.9494041204452515
Epoch 20, training loss: 87.86376953125 = 1.9316537380218506 + 10.0 * 8.59321117401123
Epoch 20, val loss: 1.9344475269317627
Epoch 30, training loss: 87.59276580810547 = 1.9147019386291504 + 10.0 * 8.567806243896484
Epoch 30, val loss: 1.9154751300811768
Epoch 40, training loss: 85.87733459472656 = 1.8953187465667725 + 10.0 * 8.398201942443848
Epoch 40, val loss: 1.8948118686676025
Epoch 50, training loss: 79.50527954101562 = 1.8756353855133057 + 10.0 * 7.762964725494385
Epoch 50, val loss: 1.874348759651184
Epoch 60, training loss: 76.9375991821289 = 1.8588778972625732 + 10.0 * 7.507872104644775
Epoch 60, val loss: 1.8584398031234741
Epoch 70, training loss: 74.543212890625 = 1.8463900089263916 + 10.0 * 7.26968240737915
Epoch 70, val loss: 1.8471766710281372
Epoch 80, training loss: 71.89454650878906 = 1.8355810642242432 + 10.0 * 7.00589656829834
Epoch 80, val loss: 1.8372764587402344
Epoch 90, training loss: 70.31951904296875 = 1.8261796236038208 + 10.0 * 6.849333763122559
Epoch 90, val loss: 1.8286758661270142
Epoch 100, training loss: 69.51262664794922 = 1.8153996467590332 + 10.0 * 6.769722938537598
Epoch 100, val loss: 1.818453073501587
Epoch 110, training loss: 68.95418548583984 = 1.8039027452468872 + 10.0 * 6.715028285980225
Epoch 110, val loss: 1.8076685667037964
Epoch 120, training loss: 68.51586151123047 = 1.7927879095077515 + 10.0 * 6.67230749130249
Epoch 120, val loss: 1.7972787618637085
Epoch 130, training loss: 68.167236328125 = 1.7819182872772217 + 10.0 * 6.63853120803833
Epoch 130, val loss: 1.7873914241790771
Epoch 140, training loss: 67.87371063232422 = 1.7709319591522217 + 10.0 * 6.6102776527404785
Epoch 140, val loss: 1.777515172958374
Epoch 150, training loss: 67.59345245361328 = 1.7593224048614502 + 10.0 * 6.583413124084473
Epoch 150, val loss: 1.7674477100372314
Epoch 160, training loss: 67.34913635253906 = 1.747036099433899 + 10.0 * 6.560209274291992
Epoch 160, val loss: 1.7568241357803345
Epoch 170, training loss: 67.18457794189453 = 1.7338006496429443 + 10.0 * 6.545077323913574
Epoch 170, val loss: 1.7453311681747437
Epoch 180, training loss: 66.96415710449219 = 1.7193008661270142 + 10.0 * 6.524486064910889
Epoch 180, val loss: 1.7331303358078003
Epoch 190, training loss: 66.79068756103516 = 1.7037525177001953 + 10.0 * 6.508693695068359
Epoch 190, val loss: 1.719874382019043
Epoch 200, training loss: 66.62796020507812 = 1.686876654624939 + 10.0 * 6.4941086769104
Epoch 200, val loss: 1.705711007118225
Epoch 210, training loss: 66.4819107055664 = 1.6685141324996948 + 10.0 * 6.481339931488037
Epoch 210, val loss: 1.6903114318847656
Epoch 220, training loss: 66.41986846923828 = 1.6485164165496826 + 10.0 * 6.477135181427002
Epoch 220, val loss: 1.6734846830368042
Epoch 230, training loss: 66.26703643798828 = 1.6266934871673584 + 10.0 * 6.464034557342529
Epoch 230, val loss: 1.655352234840393
Epoch 240, training loss: 66.12967681884766 = 1.6033979654312134 + 10.0 * 6.452627182006836
Epoch 240, val loss: 1.6359962224960327
Epoch 250, training loss: 66.02433776855469 = 1.5784577131271362 + 10.0 * 6.444587707519531
Epoch 250, val loss: 1.6153396368026733
Epoch 260, training loss: 65.9234390258789 = 1.5518418550491333 + 10.0 * 6.437159538269043
Epoch 260, val loss: 1.5934462547302246
Epoch 270, training loss: 65.82957458496094 = 1.5237159729003906 + 10.0 * 6.430585861206055
Epoch 270, val loss: 1.570356845855713
Epoch 280, training loss: 65.76148986816406 = 1.4940840005874634 + 10.0 * 6.4267401695251465
Epoch 280, val loss: 1.5462067127227783
Epoch 290, training loss: 65.662353515625 = 1.463388204574585 + 10.0 * 6.419896602630615
Epoch 290, val loss: 1.5212599039077759
Epoch 300, training loss: 65.56079864501953 = 1.4318041801452637 + 10.0 * 6.412899494171143
Epoch 300, val loss: 1.4957475662231445
Epoch 310, training loss: 65.47291564941406 = 1.3995003700256348 + 10.0 * 6.407341957092285
Epoch 310, val loss: 1.4698342084884644
Epoch 320, training loss: 65.4067611694336 = 1.3665488958358765 + 10.0 * 6.404021263122559
Epoch 320, val loss: 1.4434934854507446
Epoch 330, training loss: 65.31116485595703 = 1.332962155342102 + 10.0 * 6.397819995880127
Epoch 330, val loss: 1.4168404340744019
Epoch 340, training loss: 65.22534942626953 = 1.2990764379501343 + 10.0 * 6.392627239227295
Epoch 340, val loss: 1.3902045488357544
Epoch 350, training loss: 65.17488098144531 = 1.2656744718551636 + 10.0 * 6.390920639038086
Epoch 350, val loss: 1.364081621170044
Epoch 360, training loss: 65.1028060913086 = 1.232219934463501 + 10.0 * 6.387058258056641
Epoch 360, val loss: 1.3383861780166626
Epoch 370, training loss: 65.00108337402344 = 1.199273943901062 + 10.0 * 6.380181312561035
Epoch 370, val loss: 1.3131431341171265
Epoch 380, training loss: 64.92896270751953 = 1.1667371988296509 + 10.0 * 6.376222610473633
Epoch 380, val loss: 1.2884399890899658
Epoch 390, training loss: 64.90467071533203 = 1.1345475912094116 + 10.0 * 6.377012252807617
Epoch 390, val loss: 1.2643451690673828
Epoch 400, training loss: 64.7945556640625 = 1.1029760837554932 + 10.0 * 6.369157791137695
Epoch 400, val loss: 1.2408709526062012
Epoch 410, training loss: 64.75830841064453 = 1.072066307067871 + 10.0 * 6.368624210357666
Epoch 410, val loss: 1.2180601358413696
Epoch 420, training loss: 64.68993377685547 = 1.0417168140411377 + 10.0 * 6.364821910858154
Epoch 420, val loss: 1.1960461139678955
Epoch 430, training loss: 64.60587310791016 = 1.012087345123291 + 10.0 * 6.359378337860107
Epoch 430, val loss: 1.174673318862915
Epoch 440, training loss: 64.53914642333984 = 0.98319411277771 + 10.0 * 6.355595588684082
Epoch 440, val loss: 1.154166579246521
Epoch 450, training loss: 64.56725311279297 = 0.9551356434822083 + 10.0 * 6.36121129989624
Epoch 450, val loss: 1.134289026260376
Epoch 460, training loss: 64.44200134277344 = 0.9274997711181641 + 10.0 * 6.351449966430664
Epoch 460, val loss: 1.115296483039856
Epoch 470, training loss: 64.37297821044922 = 0.9008880853652954 + 10.0 * 6.3472089767456055
Epoch 470, val loss: 1.0970433950424194
Epoch 480, training loss: 64.32350158691406 = 0.8751667737960815 + 10.0 * 6.3448333740234375
Epoch 480, val loss: 1.0796958208084106
Epoch 490, training loss: 64.29795837402344 = 0.85024493932724 + 10.0 * 6.344771385192871
Epoch 490, val loss: 1.063171625137329
Epoch 500, training loss: 64.23222351074219 = 0.8259885311126709 + 10.0 * 6.34062385559082
Epoch 500, val loss: 1.0476418733596802
Epoch 510, training loss: 64.2432632446289 = 0.8026720881462097 + 10.0 * 6.344059467315674
Epoch 510, val loss: 1.0325056314468384
Epoch 520, training loss: 64.13031768798828 = 0.7799760103225708 + 10.0 * 6.335034370422363
Epoch 520, val loss: 1.0183401107788086
Epoch 530, training loss: 64.08291625976562 = 0.7580475211143494 + 10.0 * 6.332487106323242
Epoch 530, val loss: 1.0049442052841187
Epoch 540, training loss: 64.06200408935547 = 0.7367910742759705 + 10.0 * 6.332521438598633
Epoch 540, val loss: 0.9922454953193665
Epoch 550, training loss: 64.02063751220703 = 0.7160802483558655 + 10.0 * 6.330455780029297
Epoch 550, val loss: 0.9800034165382385
Epoch 560, training loss: 63.972190856933594 = 0.6960560083389282 + 10.0 * 6.327613353729248
Epoch 560, val loss: 0.9683305621147156
Epoch 570, training loss: 63.93366241455078 = 0.6763612031936646 + 10.0 * 6.325730323791504
Epoch 570, val loss: 0.9573419690132141
Epoch 580, training loss: 63.87991714477539 = 0.6574109196662903 + 10.0 * 6.322250843048096
Epoch 580, val loss: 0.9468542337417603
Epoch 590, training loss: 63.83763122558594 = 0.6389235258102417 + 10.0 * 6.319870948791504
Epoch 590, val loss: 0.93698650598526
Epoch 600, training loss: 63.8163948059082 = 0.6208140850067139 + 10.0 * 6.319558143615723
Epoch 600, val loss: 0.927695095539093
Epoch 610, training loss: 63.75825119018555 = 0.6030200719833374 + 10.0 * 6.315523147583008
Epoch 610, val loss: 0.9184368252754211
Epoch 620, training loss: 63.723663330078125 = 0.5856173038482666 + 10.0 * 6.313804626464844
Epoch 620, val loss: 0.9098916053771973
Epoch 630, training loss: 63.68661880493164 = 0.5685569047927856 + 10.0 * 6.3118062019348145
Epoch 630, val loss: 0.9015962481498718
Epoch 640, training loss: 63.73329544067383 = 0.5518538951873779 + 10.0 * 6.31814432144165
Epoch 640, val loss: 0.8935927748680115
Epoch 650, training loss: 63.637447357177734 = 0.5353359580039978 + 10.0 * 6.310211181640625
Epoch 650, val loss: 0.8861647844314575
Epoch 660, training loss: 63.602149963378906 = 0.5191953182220459 + 10.0 * 6.308295249938965
Epoch 660, val loss: 0.8789256811141968
Epoch 670, training loss: 63.57333755493164 = 0.5034124255180359 + 10.0 * 6.306992530822754
Epoch 670, val loss: 0.872138500213623
Epoch 680, training loss: 63.54301452636719 = 0.4878329038619995 + 10.0 * 6.30551815032959
Epoch 680, val loss: 0.8657008409500122
Epoch 690, training loss: 63.505104064941406 = 0.47261205315589905 + 10.0 * 6.303249359130859
Epoch 690, val loss: 0.8595907092094421
Epoch 700, training loss: 63.55759048461914 = 0.4575975835323334 + 10.0 * 6.309999465942383
Epoch 700, val loss: 0.8539922833442688
Epoch 710, training loss: 63.450416564941406 = 0.44296935200691223 + 10.0 * 6.300744533538818
Epoch 710, val loss: 0.8482595682144165
Epoch 720, training loss: 63.42008972167969 = 0.4286668598651886 + 10.0 * 6.299142360687256
Epoch 720, val loss: 0.8430957794189453
Epoch 730, training loss: 63.38534927368164 = 0.41472458839416504 + 10.0 * 6.297062397003174
Epoch 730, val loss: 0.8384346961975098
Epoch 740, training loss: 63.3659553527832 = 0.4010883569717407 + 10.0 * 6.296486854553223
Epoch 740, val loss: 0.8341227173805237
Epoch 750, training loss: 63.390079498291016 = 0.3876546323299408 + 10.0 * 6.3002424240112305
Epoch 750, val loss: 0.8301704525947571
Epoch 760, training loss: 63.31608200073242 = 0.3745688498020172 + 10.0 * 6.294151306152344
Epoch 760, val loss: 0.8261473774909973
Epoch 770, training loss: 63.289554595947266 = 0.3618169128894806 + 10.0 * 6.292773723602295
Epoch 770, val loss: 0.8229173421859741
Epoch 780, training loss: 63.263427734375 = 0.34948113560676575 + 10.0 * 6.2913947105407715
Epoch 780, val loss: 0.8200399875640869
Epoch 790, training loss: 63.291114807128906 = 0.33747008442878723 + 10.0 * 6.2953643798828125
Epoch 790, val loss: 0.8174060583114624
Epoch 800, training loss: 63.27471923828125 = 0.3257208466529846 + 10.0 * 6.294899940490723
Epoch 800, val loss: 0.8148015141487122
Epoch 810, training loss: 63.19862365722656 = 0.3142879903316498 + 10.0 * 6.28843355178833
Epoch 810, val loss: 0.81321120262146
Epoch 820, training loss: 63.17420959472656 = 0.3032471239566803 + 10.0 * 6.2870965003967285
Epoch 820, val loss: 0.8117941617965698
Epoch 830, training loss: 63.15504837036133 = 0.2925376892089844 + 10.0 * 6.286251068115234
Epoch 830, val loss: 0.8106106519699097
Epoch 840, training loss: 63.17279815673828 = 0.2821308374404907 + 10.0 * 6.289066791534424
Epoch 840, val loss: 0.8098345994949341
Epoch 850, training loss: 63.19308853149414 = 0.2720376253128052 + 10.0 * 6.292105197906494
Epoch 850, val loss: 0.8088622093200684
Epoch 860, training loss: 63.10246658325195 = 0.26221632957458496 + 10.0 * 6.284025192260742
Epoch 860, val loss: 0.8087175488471985
Epoch 870, training loss: 63.0877799987793 = 0.2528200149536133 + 10.0 * 6.283495903015137
Epoch 870, val loss: 0.8086968660354614
Epoch 880, training loss: 63.10355758666992 = 0.24369341135025024 + 10.0 * 6.285986423492432
Epoch 880, val loss: 0.8090529441833496
Epoch 890, training loss: 63.04972839355469 = 0.234833225607872 + 10.0 * 6.281489372253418
Epoch 890, val loss: 0.809842050075531
Epoch 900, training loss: 63.02136993408203 = 0.22637134790420532 + 10.0 * 6.2795000076293945
Epoch 900, val loss: 0.8106818795204163
Epoch 910, training loss: 63.01542663574219 = 0.21817836165428162 + 10.0 * 6.279725074768066
Epoch 910, val loss: 0.8119735717773438
Epoch 920, training loss: 63.021766662597656 = 0.21023567020893097 + 10.0 * 6.281153202056885
Epoch 920, val loss: 0.8135823607444763
Epoch 930, training loss: 63.02186965942383 = 0.20250952243804932 + 10.0 * 6.281935691833496
Epoch 930, val loss: 0.8156272172927856
Epoch 940, training loss: 62.96205520629883 = 0.19514602422714233 + 10.0 * 6.27669095993042
Epoch 940, val loss: 0.8172904849052429
Epoch 950, training loss: 62.94388961791992 = 0.18805401027202606 + 10.0 * 6.275583744049072
Epoch 950, val loss: 0.8196924924850464
Epoch 960, training loss: 62.97605514526367 = 0.18124058842658997 + 10.0 * 6.279481410980225
Epoch 960, val loss: 0.8221768140792847
Epoch 970, training loss: 62.933780670166016 = 0.17459329962730408 + 10.0 * 6.275918483734131
Epoch 970, val loss: 0.8253730535507202
Epoch 980, training loss: 62.91457748413086 = 0.16827154159545898 + 10.0 * 6.274630546569824
Epoch 980, val loss: 0.828328549861908
Epoch 990, training loss: 62.928409576416016 = 0.16218732297420502 + 10.0 * 6.276622295379639
Epoch 990, val loss: 0.83188396692276
Epoch 1000, training loss: 62.87285614013672 = 0.1563185751438141 + 10.0 * 6.271653652191162
Epoch 1000, val loss: 0.8350793123245239
Epoch 1010, training loss: 62.87098693847656 = 0.15068505704402924 + 10.0 * 6.272030353546143
Epoch 1010, val loss: 0.8387848734855652
Epoch 1020, training loss: 62.87241744995117 = 0.14527256786823273 + 10.0 * 6.272714614868164
Epoch 1020, val loss: 0.8425788283348083
Epoch 1030, training loss: 62.8596305847168 = 0.14005331695079803 + 10.0 * 6.271957874298096
Epoch 1030, val loss: 0.8466251492500305
Epoch 1040, training loss: 62.87440872192383 = 0.13503387570381165 + 10.0 * 6.273937702178955
Epoch 1040, val loss: 0.8506544232368469
Epoch 1050, training loss: 62.82002258300781 = 0.1301809698343277 + 10.0 * 6.268984317779541
Epoch 1050, val loss: 0.8552855849266052
Epoch 1060, training loss: 62.805335998535156 = 0.12557072937488556 + 10.0 * 6.2679762840271
Epoch 1060, val loss: 0.8598945736885071
Epoch 1070, training loss: 62.81425094604492 = 0.12115849554538727 + 10.0 * 6.269309043884277
Epoch 1070, val loss: 0.8644564747810364
Epoch 1080, training loss: 62.79395294189453 = 0.11691844463348389 + 10.0 * 6.267703533172607
Epoch 1080, val loss: 0.8691462874412537
Epoch 1090, training loss: 62.798866271972656 = 0.1128145381808281 + 10.0 * 6.2686052322387695
Epoch 1090, val loss: 0.8741376996040344
Epoch 1100, training loss: 62.76852035522461 = 0.1088678389787674 + 10.0 * 6.265965461730957
Epoch 1100, val loss: 0.8794062733650208
Epoch 1110, training loss: 62.74795913696289 = 0.10513046383857727 + 10.0 * 6.264283180236816
Epoch 1110, val loss: 0.8846543431282043
Epoch 1120, training loss: 62.7495002746582 = 0.10154879093170166 + 10.0 * 6.264795303344727
Epoch 1120, val loss: 0.8901253938674927
Epoch 1130, training loss: 62.803035736083984 = 0.09808718413114548 + 10.0 * 6.2704949378967285
Epoch 1130, val loss: 0.8958308100700378
Epoch 1140, training loss: 62.7476921081543 = 0.09475568681955338 + 10.0 * 6.265293598175049
Epoch 1140, val loss: 0.9006763696670532
Epoch 1150, training loss: 62.715362548828125 = 0.09158335626125336 + 10.0 * 6.262377738952637
Epoch 1150, val loss: 0.9065353870391846
Epoch 1160, training loss: 62.70431137084961 = 0.08855686336755753 + 10.0 * 6.261575222015381
Epoch 1160, val loss: 0.9121755957603455
Epoch 1170, training loss: 62.7607421875 = 0.08566474914550781 + 10.0 * 6.267507553100586
Epoch 1170, val loss: 0.9179040193557739
Epoch 1180, training loss: 62.718894958496094 = 0.08284086734056473 + 10.0 * 6.26360559463501
Epoch 1180, val loss: 0.9235959053039551
Epoch 1190, training loss: 62.68858337402344 = 0.0801718607544899 + 10.0 * 6.260840892791748
Epoch 1190, val loss: 0.9294410943984985
Epoch 1200, training loss: 62.66563415527344 = 0.07761537283658981 + 10.0 * 6.2588019371032715
Epoch 1200, val loss: 0.9353381395339966
Epoch 1210, training loss: 62.663909912109375 = 0.0751764178276062 + 10.0 * 6.258873462677002
Epoch 1210, val loss: 0.9413884282112122
Epoch 1220, training loss: 62.6964225769043 = 0.07282540202140808 + 10.0 * 6.262359619140625
Epoch 1220, val loss: 0.9472280740737915
Epoch 1230, training loss: 62.70381164550781 = 0.07058002054691315 + 10.0 * 6.2633233070373535
Epoch 1230, val loss: 0.9526068568229675
Epoch 1240, training loss: 62.66283416748047 = 0.06833848357200623 + 10.0 * 6.259449481964111
Epoch 1240, val loss: 0.9594535827636719
Epoch 1250, training loss: 62.638545989990234 = 0.06626259535551071 + 10.0 * 6.257228374481201
Epoch 1250, val loss: 0.9648700952529907
Epoch 1260, training loss: 62.617950439453125 = 0.06426346302032471 + 10.0 * 6.255368709564209
Epoch 1260, val loss: 0.9712420701980591
Epoch 1270, training loss: 62.6103515625 = 0.06236235424876213 + 10.0 * 6.254798889160156
Epoch 1270, val loss: 0.9774183630943298
Epoch 1280, training loss: 62.640869140625 = 0.060534317046403885 + 10.0 * 6.258033275604248
Epoch 1280, val loss: 0.9834995865821838
Epoch 1290, training loss: 62.60551452636719 = 0.05874260142445564 + 10.0 * 6.2546772956848145
Epoch 1290, val loss: 0.9898355007171631
Epoch 1300, training loss: 62.628780364990234 = 0.05703999474644661 + 10.0 * 6.257174015045166
Epoch 1300, val loss: 0.9957978129386902
Epoch 1310, training loss: 62.60923385620117 = 0.05538833886384964 + 10.0 * 6.25538444519043
Epoch 1310, val loss: 1.0020357370376587
Epoch 1320, training loss: 62.58575439453125 = 0.053819689899683 + 10.0 * 6.253193378448486
Epoch 1320, val loss: 1.007952094078064
Epoch 1330, training loss: 62.57853317260742 = 0.05232851952314377 + 10.0 * 6.252620220184326
Epoch 1330, val loss: 1.0140783786773682
Epoch 1340, training loss: 62.6112174987793 = 0.05089183524250984 + 10.0 * 6.256032466888428
Epoch 1340, val loss: 1.020134449005127
Epoch 1350, training loss: 62.57447814941406 = 0.049468930810689926 + 10.0 * 6.252501010894775
Epoch 1350, val loss: 1.0263375043869019
Epoch 1360, training loss: 62.56795120239258 = 0.048117853701114655 + 10.0 * 6.251983165740967
Epoch 1360, val loss: 1.0325007438659668
Epoch 1370, training loss: 62.634498596191406 = 0.046830665320158005 + 10.0 * 6.258767127990723
Epoch 1370, val loss: 1.0386189222335815
Epoch 1380, training loss: 62.569759368896484 = 0.0455518513917923 + 10.0 * 6.252420902252197
Epoch 1380, val loss: 1.0448312759399414
Epoch 1390, training loss: 62.54126739501953 = 0.044358789920806885 + 10.0 * 6.249691009521484
Epoch 1390, val loss: 1.0507586002349854
Epoch 1400, training loss: 62.534236907958984 = 0.043202728033065796 + 10.0 * 6.249103546142578
Epoch 1400, val loss: 1.0568867921829224
Epoch 1410, training loss: 62.577518463134766 = 0.04211081191897392 + 10.0 * 6.253540992736816
Epoch 1410, val loss: 1.062499761581421
Epoch 1420, training loss: 62.53675842285156 = 0.0409889817237854 + 10.0 * 6.249577045440674
Epoch 1420, val loss: 1.0692198276519775
Epoch 1430, training loss: 62.5317497253418 = 0.039948709309101105 + 10.0 * 6.249180316925049
Epoch 1430, val loss: 1.0745455026626587
Epoch 1440, training loss: 62.517642974853516 = 0.038942500948905945 + 10.0 * 6.247869968414307
Epoch 1440, val loss: 1.0809582471847534
Epoch 1450, training loss: 62.518123626708984 = 0.037991177290678024 + 10.0 * 6.248013496398926
Epoch 1450, val loss: 1.0866222381591797
Epoch 1460, training loss: 62.54701614379883 = 0.03705587983131409 + 10.0 * 6.250996112823486
Epoch 1460, val loss: 1.092676043510437
Epoch 1470, training loss: 62.52610778808594 = 0.03613893315196037 + 10.0 * 6.248996734619141
Epoch 1470, val loss: 1.098503589630127
Epoch 1480, training loss: 62.51707077026367 = 0.03526489809155464 + 10.0 * 6.248180866241455
Epoch 1480, val loss: 1.104428768157959
Epoch 1490, training loss: 62.51701354980469 = 0.0344148650765419 + 10.0 * 6.248259544372559
Epoch 1490, val loss: 1.110258936882019
Epoch 1500, training loss: 62.51448440551758 = 0.03360166773200035 + 10.0 * 6.248088359832764
Epoch 1500, val loss: 1.1157184839248657
Epoch 1510, training loss: 62.488895416259766 = 0.0328071154654026 + 10.0 * 6.245608806610107
Epoch 1510, val loss: 1.1216024160385132
Epoch 1520, training loss: 62.479820251464844 = 0.03204582259058952 + 10.0 * 6.244777202606201
Epoch 1520, val loss: 1.1274176836013794
Epoch 1530, training loss: 62.47958755493164 = 0.031314026564359665 + 10.0 * 6.2448272705078125
Epoch 1530, val loss: 1.1332887411117554
Epoch 1540, training loss: 62.54374313354492 = 0.03060094825923443 + 10.0 * 6.251314163208008
Epoch 1540, val loss: 1.1391215324401855
Epoch 1550, training loss: 62.50794219970703 = 0.029890151694417 + 10.0 * 6.247805118560791
Epoch 1550, val loss: 1.1445156335830688
Epoch 1560, training loss: 62.475433349609375 = 0.02922707423567772 + 10.0 * 6.244620323181152
Epoch 1560, val loss: 1.1498653888702393
Epoch 1570, training loss: 62.47432327270508 = 0.02857493795454502 + 10.0 * 6.244574546813965
Epoch 1570, val loss: 1.1556460857391357
Epoch 1580, training loss: 62.4642219543457 = 0.027947507798671722 + 10.0 * 6.243627548217773
Epoch 1580, val loss: 1.1613012552261353
Epoch 1590, training loss: 62.447593688964844 = 0.02734316699206829 + 10.0 * 6.242024898529053
Epoch 1590, val loss: 1.1665568351745605
Epoch 1600, training loss: 62.55262756347656 = 0.026764821261167526 + 10.0 * 6.252586364746094
Epoch 1600, val loss: 1.1719850301742554
Epoch 1610, training loss: 62.47837829589844 = 0.026179464533925056 + 10.0 * 6.245219707489014
Epoch 1610, val loss: 1.177065134048462
Epoch 1620, training loss: 62.43856430053711 = 0.02562054619193077 + 10.0 * 6.2412943840026855
Epoch 1620, val loss: 1.1825026273727417
Epoch 1630, training loss: 62.42911148071289 = 0.025089042261242867 + 10.0 * 6.2404022216796875
Epoch 1630, val loss: 1.187940001487732
Epoch 1640, training loss: 62.43645477294922 = 0.024577857926487923 + 10.0 * 6.241187572479248
Epoch 1640, val loss: 1.1932591199874878
Epoch 1650, training loss: 62.4656982421875 = 0.024068346247076988 + 10.0 * 6.2441630363464355
Epoch 1650, val loss: 1.1987221240997314
Epoch 1660, training loss: 62.48604202270508 = 0.02357076294720173 + 10.0 * 6.246247291564941
Epoch 1660, val loss: 1.2037516832351685
Epoch 1670, training loss: 62.42875671386719 = 0.023092614486813545 + 10.0 * 6.240566253662109
Epoch 1670, val loss: 1.2085233926773071
Epoch 1680, training loss: 62.41230773925781 = 0.022629184648394585 + 10.0 * 6.2389678955078125
Epoch 1680, val loss: 1.2140171527862549
Epoch 1690, training loss: 62.404510498046875 = 0.022188952192664146 + 10.0 * 6.238232135772705
Epoch 1690, val loss: 1.2189805507659912
Epoch 1700, training loss: 62.41603469848633 = 0.02176431193947792 + 10.0 * 6.239427089691162
Epoch 1700, val loss: 1.2237958908081055
Epoch 1710, training loss: 62.46341323852539 = 0.021342670544981956 + 10.0 * 6.24420690536499
Epoch 1710, val loss: 1.2286975383758545
Epoch 1720, training loss: 62.411766052246094 = 0.020919522270560265 + 10.0 * 6.239084720611572
Epoch 1720, val loss: 1.2337865829467773
Epoch 1730, training loss: 62.389854431152344 = 0.020517008379101753 + 10.0 * 6.236933708190918
Epoch 1730, val loss: 1.2389551401138306
Epoch 1740, training loss: 62.386009216308594 = 0.02013503573834896 + 10.0 * 6.2365875244140625
Epoch 1740, val loss: 1.243775725364685
Epoch 1750, training loss: 62.421775817871094 = 0.019767947494983673 + 10.0 * 6.240200996398926
Epoch 1750, val loss: 1.2484468221664429
Epoch 1760, training loss: 62.391502380371094 = 0.019392795860767365 + 10.0 * 6.237210750579834
Epoch 1760, val loss: 1.2534586191177368
Epoch 1770, training loss: 62.38621139526367 = 0.019029667600989342 + 10.0 * 6.23671817779541
Epoch 1770, val loss: 1.2582767009735107
Epoch 1780, training loss: 62.376060485839844 = 0.018686486408114433 + 10.0 * 6.235737323760986
Epoch 1780, val loss: 1.2630637884140015
Epoch 1790, training loss: 62.38251876831055 = 0.018352976068854332 + 10.0 * 6.236416816711426
Epoch 1790, val loss: 1.267787218093872
Epoch 1800, training loss: 62.40812683105469 = 0.018031945452094078 + 10.0 * 6.239009380340576
Epoch 1800, val loss: 1.272299885749817
Epoch 1810, training loss: 62.367576599121094 = 0.017701735720038414 + 10.0 * 6.234987735748291
Epoch 1810, val loss: 1.2774535417556763
Epoch 1820, training loss: 62.41975402832031 = 0.017396720126271248 + 10.0 * 6.240235805511475
Epoch 1820, val loss: 1.2820812463760376
Epoch 1830, training loss: 62.393558502197266 = 0.01709342561662197 + 10.0 * 6.237646579742432
Epoch 1830, val loss: 1.2860640287399292
Epoch 1840, training loss: 62.423213958740234 = 0.016790054738521576 + 10.0 * 6.240642547607422
Epoch 1840, val loss: 1.2910528182983398
Epoch 1850, training loss: 62.371185302734375 = 0.01650196872651577 + 10.0 * 6.23546838760376
Epoch 1850, val loss: 1.295367956161499
Epoch 1860, training loss: 62.35706329345703 = 0.016221139580011368 + 10.0 * 6.234084129333496
Epoch 1860, val loss: 1.3001118898391724
Epoch 1870, training loss: 62.345149993896484 = 0.015955252572894096 + 10.0 * 6.232919216156006
Epoch 1870, val loss: 1.304459810256958
Epoch 1880, training loss: 62.357574462890625 = 0.015693815425038338 + 10.0 * 6.234188079833984
Epoch 1880, val loss: 1.309141993522644
Epoch 1890, training loss: 62.39250564575195 = 0.015433494932949543 + 10.0 * 6.237707138061523
Epoch 1890, val loss: 1.3130449056625366
Epoch 1900, training loss: 62.34711837768555 = 0.015175104141235352 + 10.0 * 6.233194351196289
Epoch 1900, val loss: 1.3174678087234497
Epoch 1910, training loss: 62.34967041015625 = 0.014927842654287815 + 10.0 * 6.233474254608154
Epoch 1910, val loss: 1.3214768171310425
Epoch 1920, training loss: 62.375518798828125 = 0.014695732854306698 + 10.0 * 6.236082553863525
Epoch 1920, val loss: 1.3257585763931274
Epoch 1930, training loss: 62.33583068847656 = 0.014453674666583538 + 10.0 * 6.232137680053711
Epoch 1930, val loss: 1.3303606510162354
Epoch 1940, training loss: 62.3458137512207 = 0.014224204234778881 + 10.0 * 6.233159065246582
Epoch 1940, val loss: 1.3347903490066528
Epoch 1950, training loss: 62.368370056152344 = 0.01400814950466156 + 10.0 * 6.23543643951416
Epoch 1950, val loss: 1.338708758354187
Epoch 1960, training loss: 62.325439453125 = 0.013785402290523052 + 10.0 * 6.231165409088135
Epoch 1960, val loss: 1.3429704904556274
Epoch 1970, training loss: 62.321502685546875 = 0.013573074713349342 + 10.0 * 6.230792999267578
Epoch 1970, val loss: 1.3469927310943604
Epoch 1980, training loss: 62.330875396728516 = 0.013371072709560394 + 10.0 * 6.23175048828125
Epoch 1980, val loss: 1.3512837886810303
Epoch 1990, training loss: 62.3712272644043 = 0.013168229721486568 + 10.0 * 6.235805988311768
Epoch 1990, val loss: 1.3556629419326782
Epoch 2000, training loss: 62.329444885253906 = 0.012967713177204132 + 10.0 * 6.231647968292236
Epoch 2000, val loss: 1.359387755393982
Epoch 2010, training loss: 62.31620788574219 = 0.012771987356245518 + 10.0 * 6.230343818664551
Epoch 2010, val loss: 1.3635051250457764
Epoch 2020, training loss: 62.3266487121582 = 0.012584976851940155 + 10.0 * 6.231406211853027
Epoch 2020, val loss: 1.367212176322937
Epoch 2030, training loss: 62.30703353881836 = 0.012400507926940918 + 10.0 * 6.22946310043335
Epoch 2030, val loss: 1.3714057207107544
Epoch 2040, training loss: 62.335716247558594 = 0.012223389931023121 + 10.0 * 6.232349395751953
Epoch 2040, val loss: 1.37533700466156
Epoch 2050, training loss: 62.304710388183594 = 0.012047478929162025 + 10.0 * 6.229266166687012
Epoch 2050, val loss: 1.3789089918136597
Epoch 2060, training loss: 62.32419967651367 = 0.011876918375492096 + 10.0 * 6.231232643127441
Epoch 2060, val loss: 1.3828462362289429
Epoch 2070, training loss: 62.330379486083984 = 0.01170682068914175 + 10.0 * 6.23186731338501
Epoch 2070, val loss: 1.386757493019104
Epoch 2080, training loss: 62.38313674926758 = 0.01153828389942646 + 10.0 * 6.237159729003906
Epoch 2080, val loss: 1.3913302421569824
Epoch 2090, training loss: 62.3093147277832 = 0.011378207243978977 + 10.0 * 6.229793548583984
Epoch 2090, val loss: 1.3938199281692505
Epoch 2100, training loss: 62.28744888305664 = 0.01121496967971325 + 10.0 * 6.227623462677002
Epoch 2100, val loss: 1.3979780673980713
Epoch 2110, training loss: 62.28208541870117 = 0.01106724888086319 + 10.0 * 6.227101802825928
Epoch 2110, val loss: 1.4016659259796143
Epoch 2120, training loss: 62.29140853881836 = 0.01092077698558569 + 10.0 * 6.228048801422119
Epoch 2120, val loss: 1.4052847623825073
Epoch 2130, training loss: 62.32220458984375 = 0.010774405673146248 + 10.0 * 6.231142997741699
Epoch 2130, val loss: 1.4088854789733887
Epoch 2140, training loss: 62.29885482788086 = 0.01062868908047676 + 10.0 * 6.228822708129883
Epoch 2140, val loss: 1.4124199151992798
Epoch 2150, training loss: 62.31751251220703 = 0.010490935295820236 + 10.0 * 6.2307024002075195
Epoch 2150, val loss: 1.4155874252319336
Epoch 2160, training loss: 62.29935836791992 = 0.010351420380175114 + 10.0 * 6.22890043258667
Epoch 2160, val loss: 1.4194036722183228
Epoch 2170, training loss: 62.27854537963867 = 0.010213302448391914 + 10.0 * 6.226833343505859
Epoch 2170, val loss: 1.423069715499878
Epoch 2180, training loss: 62.268402099609375 = 0.010083204135298729 + 10.0 * 6.225831985473633
Epoch 2180, val loss: 1.4265793561935425
Epoch 2190, training loss: 62.279457092285156 = 0.009956800378859043 + 10.0 * 6.226950168609619
Epoch 2190, val loss: 1.4302701950073242
Epoch 2200, training loss: 62.325984954833984 = 0.009829364717006683 + 10.0 * 6.2316155433654785
Epoch 2200, val loss: 1.4339452981948853
Epoch 2210, training loss: 62.279903411865234 = 0.009704705327749252 + 10.0 * 6.227019786834717
Epoch 2210, val loss: 1.4363747835159302
Epoch 2220, training loss: 62.2672004699707 = 0.00958267878741026 + 10.0 * 6.225761890411377
Epoch 2220, val loss: 1.4400521516799927
Epoch 2230, training loss: 62.30248260498047 = 0.009468172676861286 + 10.0 * 6.229301452636719
Epoch 2230, val loss: 1.443325400352478
Epoch 2240, training loss: 62.26668930053711 = 0.009345308877527714 + 10.0 * 6.225734233856201
Epoch 2240, val loss: 1.4466655254364014
Epoch 2250, training loss: 62.25601577758789 = 0.009231211617588997 + 10.0 * 6.2246785163879395
Epoch 2250, val loss: 1.449999213218689
Epoch 2260, training loss: 62.2595329284668 = 0.009121691808104515 + 10.0 * 6.225041389465332
Epoch 2260, val loss: 1.4532625675201416
Epoch 2270, training loss: 62.298828125 = 0.009012124501168728 + 10.0 * 6.228981971740723
Epoch 2270, val loss: 1.4564307928085327
Epoch 2280, training loss: 62.26429748535156 = 0.008901474997401237 + 10.0 * 6.225539684295654
Epoch 2280, val loss: 1.4598290920257568
Epoch 2290, training loss: 62.26213073730469 = 0.008796434849500656 + 10.0 * 6.225333213806152
Epoch 2290, val loss: 1.4631645679473877
Epoch 2300, training loss: 62.263065338134766 = 0.008692542091012001 + 10.0 * 6.225437164306641
Epoch 2300, val loss: 1.4664602279663086
Epoch 2310, training loss: 62.241981506347656 = 0.008593245409429073 + 10.0 * 6.223338603973389
Epoch 2310, val loss: 1.469020128250122
Epoch 2320, training loss: 62.25034713745117 = 0.008496765978634357 + 10.0 * 6.224184989929199
Epoch 2320, val loss: 1.4720492362976074
Epoch 2330, training loss: 62.24933624267578 = 0.0083987507969141 + 10.0 * 6.224093437194824
Epoch 2330, val loss: 1.4752110242843628
Epoch 2340, training loss: 62.294151306152344 = 0.008304140530526638 + 10.0 * 6.2285847663879395
Epoch 2340, val loss: 1.4783635139465332
Epoch 2350, training loss: 62.244747161865234 = 0.008206709288060665 + 10.0 * 6.223654270172119
Epoch 2350, val loss: 1.4813802242279053
Epoch 2360, training loss: 62.248291015625 = 0.008114275522530079 + 10.0 * 6.22401762008667
Epoch 2360, val loss: 1.4846915006637573
Epoch 2370, training loss: 62.29322052001953 = 0.008022185415029526 + 10.0 * 6.228519916534424
Epoch 2370, val loss: 1.4879164695739746
Epoch 2380, training loss: 62.247344970703125 = 0.007932538166642189 + 10.0 * 6.223941326141357
Epoch 2380, val loss: 1.4901373386383057
Epoch 2390, training loss: 62.22078323364258 = 0.007844598032534122 + 10.0 * 6.221293926239014
Epoch 2390, val loss: 1.4933173656463623
Epoch 2400, training loss: 62.22127914428711 = 0.007762290071696043 + 10.0 * 6.221351623535156
Epoch 2400, val loss: 1.4963054656982422
Epoch 2410, training loss: 62.247947692871094 = 0.007681147661060095 + 10.0 * 6.224026679992676
Epoch 2410, val loss: 1.4992034435272217
Epoch 2420, training loss: 62.246421813964844 = 0.0075983949936926365 + 10.0 * 6.22388219833374
Epoch 2420, val loss: 1.5015469789505005
Epoch 2430, training loss: 62.23652648925781 = 0.0075159212574362755 + 10.0 * 6.222901344299316
Epoch 2430, val loss: 1.5043851137161255
Epoch 2440, training loss: 62.25033187866211 = 0.007435779087245464 + 10.0 * 6.224289894104004
Epoch 2440, val loss: 1.5075247287750244
Epoch 2450, training loss: 62.23931121826172 = 0.007355131674557924 + 10.0 * 6.223195552825928
Epoch 2450, val loss: 1.5104655027389526
Epoch 2460, training loss: 62.217002868652344 = 0.007277410477399826 + 10.0 * 6.220972537994385
Epoch 2460, val loss: 1.513260006904602
Epoch 2470, training loss: 62.20769119262695 = 0.007203501649200916 + 10.0 * 6.220048904418945
Epoch 2470, val loss: 1.5157935619354248
Epoch 2480, training loss: 62.21067428588867 = 0.007131267338991165 + 10.0 * 6.2203545570373535
Epoch 2480, val loss: 1.5187442302703857
Epoch 2490, training loss: 62.258216857910156 = 0.007058188784867525 + 10.0 * 6.225115776062012
Epoch 2490, val loss: 1.5217173099517822
Epoch 2500, training loss: 62.26622009277344 = 0.006985872983932495 + 10.0 * 6.225923538208008
Epoch 2500, val loss: 1.5239067077636719
Epoch 2510, training loss: 62.2304573059082 = 0.006912811193615198 + 10.0 * 6.222354412078857
Epoch 2510, val loss: 1.5266005992889404
Epoch 2520, training loss: 62.20404052734375 = 0.006844979710876942 + 10.0 * 6.219719886779785
Epoch 2520, val loss: 1.5288763046264648
Epoch 2530, training loss: 62.19854736328125 = 0.006778127513825893 + 10.0 * 6.219176769256592
Epoch 2530, val loss: 1.531690239906311
Epoch 2540, training loss: 62.21119689941406 = 0.006714880932122469 + 10.0 * 6.2204484939575195
Epoch 2540, val loss: 1.53394615650177
Epoch 2550, training loss: 62.21725845336914 = 0.006649852730333805 + 10.0 * 6.221060752868652
Epoch 2550, val loss: 1.5364912748336792
Epoch 2560, training loss: 62.22514343261719 = 0.006582488305866718 + 10.0 * 6.221856117248535
Epoch 2560, val loss: 1.539460301399231
Epoch 2570, training loss: 62.22563934326172 = 0.006518254522234201 + 10.0 * 6.221911907196045
Epoch 2570, val loss: 1.541895866394043
Epoch 2580, training loss: 62.20051193237305 = 0.006454455200582743 + 10.0 * 6.219405651092529
Epoch 2580, val loss: 1.5443191528320312
Epoch 2590, training loss: 62.221893310546875 = 0.0063958726823329926 + 10.0 * 6.2215495109558105
Epoch 2590, val loss: 1.54639732837677
Epoch 2600, training loss: 62.18660354614258 = 0.006330905482172966 + 10.0 * 6.218027114868164
Epoch 2600, val loss: 1.5493667125701904
Epoch 2610, training loss: 62.19684600830078 = 0.006273373495787382 + 10.0 * 6.219057083129883
Epoch 2610, val loss: 1.551787257194519
Epoch 2620, training loss: 62.194190979003906 = 0.00621600029990077 + 10.0 * 6.21879768371582
Epoch 2620, val loss: 1.5542384386062622
Epoch 2630, training loss: 62.197021484375 = 0.006161506287753582 + 10.0 * 6.219086170196533
Epoch 2630, val loss: 1.5562533140182495
Epoch 2640, training loss: 62.220428466796875 = 0.0061042155139148235 + 10.0 * 6.221432685852051
Epoch 2640, val loss: 1.558642864227295
Epoch 2650, training loss: 62.189659118652344 = 0.006043887697160244 + 10.0 * 6.218361854553223
Epoch 2650, val loss: 1.561495304107666
Epoch 2660, training loss: 62.18332290649414 = 0.005991702899336815 + 10.0 * 6.217732906341553
Epoch 2660, val loss: 1.5635008811950684
Epoch 2670, training loss: 62.202571868896484 = 0.005939226597547531 + 10.0 * 6.219663143157959
Epoch 2670, val loss: 1.5657490491867065
Epoch 2680, training loss: 62.19268035888672 = 0.00588514469563961 + 10.0 * 6.218679428100586
Epoch 2680, val loss: 1.567893385887146
Epoch 2690, training loss: 62.18938446044922 = 0.005833735689520836 + 10.0 * 6.218355178833008
Epoch 2690, val loss: 1.57033109664917
Epoch 2700, training loss: 62.17449951171875 = 0.005782654974609613 + 10.0 * 6.216871738433838
Epoch 2700, val loss: 1.5726017951965332
Epoch 2710, training loss: 62.18431091308594 = 0.005733054131269455 + 10.0 * 6.217857837677002
Epoch 2710, val loss: 1.5748077630996704
Epoch 2720, training loss: 62.19283676147461 = 0.005682643502950668 + 10.0 * 6.218715190887451
Epoch 2720, val loss: 1.5771124362945557
Epoch 2730, training loss: 62.20402526855469 = 0.005633672699332237 + 10.0 * 6.219839096069336
Epoch 2730, val loss: 1.579052448272705
Epoch 2740, training loss: 62.2039680480957 = 0.005586655810475349 + 10.0 * 6.2198381423950195
Epoch 2740, val loss: 1.580906629562378
Epoch 2750, training loss: 62.17282485961914 = 0.005535314790904522 + 10.0 * 6.216729164123535
Epoch 2750, val loss: 1.5834670066833496
Epoch 2760, training loss: 62.156517028808594 = 0.005489299073815346 + 10.0 * 6.215102672576904
Epoch 2760, val loss: 1.5858534574508667
Epoch 2770, training loss: 62.154151916503906 = 0.005445207469165325 + 10.0 * 6.214870929718018
Epoch 2770, val loss: 1.587988018989563
Epoch 2780, training loss: 62.207313537597656 = 0.0054007931612432 + 10.0 * 6.22019100189209
Epoch 2780, val loss: 1.5904432535171509
Epoch 2790, training loss: 62.18234634399414 = 0.005355128552764654 + 10.0 * 6.21769905090332
Epoch 2790, val loss: 1.59225332736969
Epoch 2800, training loss: 62.18302917480469 = 0.005309484899044037 + 10.0 * 6.217772006988525
Epoch 2800, val loss: 1.594247579574585
Epoch 2810, training loss: 62.15400695800781 = 0.005266282241791487 + 10.0 * 6.214873790740967
Epoch 2810, val loss: 1.5959734916687012
Epoch 2820, training loss: 62.14900588989258 = 0.005224511492997408 + 10.0 * 6.2143778800964355
Epoch 2820, val loss: 1.5980645418167114
Epoch 2830, training loss: 62.14474868774414 = 0.00518477987498045 + 10.0 * 6.213956356048584
Epoch 2830, val loss: 1.600148320198059
Epoch 2840, training loss: 62.19363784790039 = 0.005145664792507887 + 10.0 * 6.218849182128906
Epoch 2840, val loss: 1.6022381782531738
Epoch 2850, training loss: 62.17097091674805 = 0.005101914517581463 + 10.0 * 6.216587066650391
Epoch 2850, val loss: 1.6046518087387085
Epoch 2860, training loss: 62.15110397338867 = 0.005060757044702768 + 10.0 * 6.214604377746582
Epoch 2860, val loss: 1.6059650182724
Epoch 2870, training loss: 62.13947677612305 = 0.00502164987847209 + 10.0 * 6.213445663452148
Epoch 2870, val loss: 1.60788893699646
Epoch 2880, training loss: 62.15788650512695 = 0.004984322004020214 + 10.0 * 6.215290069580078
Epoch 2880, val loss: 1.6099189519882202
Epoch 2890, training loss: 62.15754699707031 = 0.004945674445480108 + 10.0 * 6.2152605056762695
Epoch 2890, val loss: 1.6117929220199585
Epoch 2900, training loss: 62.146785736083984 = 0.004906372632831335 + 10.0 * 6.214188098907471
Epoch 2900, val loss: 1.6135718822479248
Epoch 2910, training loss: 62.14114761352539 = 0.0048698666505515575 + 10.0 * 6.213627815246582
Epoch 2910, val loss: 1.6155661344528198
Epoch 2920, training loss: 62.16825866699219 = 0.004834615159779787 + 10.0 * 6.216342449188232
Epoch 2920, val loss: 1.617394208908081
Epoch 2930, training loss: 62.13335418701172 = 0.004798536188900471 + 10.0 * 6.212855339050293
Epoch 2930, val loss: 1.6191632747650146
Epoch 2940, training loss: 62.14303970336914 = 0.004763373639434576 + 10.0 * 6.213827610015869
Epoch 2940, val loss: 1.6211199760437012
Epoch 2950, training loss: 62.18315505981445 = 0.004730462096631527 + 10.0 * 6.2178425788879395
Epoch 2950, val loss: 1.6223254203796387
Epoch 2960, training loss: 62.20138168334961 = 0.004691006150096655 + 10.0 * 6.219668865203857
Epoch 2960, val loss: 1.6246012449264526
Epoch 2970, training loss: 62.14006423950195 = 0.0046537406742572784 + 10.0 * 6.213541030883789
Epoch 2970, val loss: 1.626358151435852
Epoch 2980, training loss: 62.120670318603516 = 0.004620669409632683 + 10.0 * 6.211605072021484
Epoch 2980, val loss: 1.6282495260238647
Epoch 2990, training loss: 62.11739730834961 = 0.004590396303683519 + 10.0 * 6.211280822753906
Epoch 2990, val loss: 1.6299220323562622
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 87.89838409423828 = 1.9298052787780762 + 10.0 * 8.596858024597168
Epoch 0, val loss: 1.92120361328125
Epoch 10, training loss: 87.8844985961914 = 1.9208420515060425 + 10.0 * 8.596364974975586
Epoch 10, val loss: 1.9125711917877197
Epoch 20, training loss: 87.83863067626953 = 1.9098711013793945 + 10.0 * 8.592875480651855
Epoch 20, val loss: 1.9014463424682617
Epoch 30, training loss: 87.59243774414062 = 1.8955353498458862 + 10.0 * 8.569689750671387
Epoch 30, val loss: 1.8866502046585083
Epoch 40, training loss: 86.19705963134766 = 1.8778332471847534 + 10.0 * 8.431922912597656
Epoch 40, val loss: 1.8691093921661377
Epoch 50, training loss: 79.916748046875 = 1.8585208654403687 + 10.0 * 7.805822372436523
Epoch 50, val loss: 1.850316047668457
Epoch 60, training loss: 76.03340911865234 = 1.8416743278503418 + 10.0 * 7.419173240661621
Epoch 60, val loss: 1.8346340656280518
Epoch 70, training loss: 73.9332046508789 = 1.8282198905944824 + 10.0 * 7.210498809814453
Epoch 70, val loss: 1.821820616722107
Epoch 80, training loss: 72.87354278564453 = 1.8147894144058228 + 10.0 * 7.105875492095947
Epoch 80, val loss: 1.8090765476226807
Epoch 90, training loss: 72.00529479980469 = 1.8010280132293701 + 10.0 * 7.0204267501831055
Epoch 90, val loss: 1.796760082244873
Epoch 100, training loss: 71.15760040283203 = 1.789972186088562 + 10.0 * 6.936763286590576
Epoch 100, val loss: 1.7877956628799438
Epoch 110, training loss: 70.42172241210938 = 1.780917763710022 + 10.0 * 6.864080905914307
Epoch 110, val loss: 1.780496597290039
Epoch 120, training loss: 69.86235046386719 = 1.7702523469924927 + 10.0 * 6.80920934677124
Epoch 120, val loss: 1.7716262340545654
Epoch 130, training loss: 69.34551239013672 = 1.7584906816482544 + 10.0 * 6.758702278137207
Epoch 130, val loss: 1.7622475624084473
Epoch 140, training loss: 68.83930969238281 = 1.7472296953201294 + 10.0 * 6.709208011627197
Epoch 140, val loss: 1.7533851861953735
Epoch 150, training loss: 68.38286590576172 = 1.7356075048446655 + 10.0 * 6.664725303649902
Epoch 150, val loss: 1.7440534830093384
Epoch 160, training loss: 68.02204132080078 = 1.722785234451294 + 10.0 * 6.629925727844238
Epoch 160, val loss: 1.733440637588501
Epoch 170, training loss: 67.7334976196289 = 1.7080782651901245 + 10.0 * 6.602542400360107
Epoch 170, val loss: 1.7213184833526611
Epoch 180, training loss: 67.50072479248047 = 1.6910353899002075 + 10.0 * 6.580969333648682
Epoch 180, val loss: 1.7073866128921509
Epoch 190, training loss: 67.31159210205078 = 1.672213077545166 + 10.0 * 6.563938140869141
Epoch 190, val loss: 1.691994309425354
Epoch 200, training loss: 67.16035461425781 = 1.6518290042877197 + 10.0 * 6.550852298736572
Epoch 200, val loss: 1.6754285097122192
Epoch 210, training loss: 66.98880767822266 = 1.629851222038269 + 10.0 * 6.535895347595215
Epoch 210, val loss: 1.6577277183532715
Epoch 220, training loss: 66.82392883300781 = 1.6065572500228882 + 10.0 * 6.5217366218566895
Epoch 220, val loss: 1.6390093564987183
Epoch 230, training loss: 66.67351531982422 = 1.5817927122116089 + 10.0 * 6.509172439575195
Epoch 230, val loss: 1.6192092895507812
Epoch 240, training loss: 66.5271987915039 = 1.5555822849273682 + 10.0 * 6.497161865234375
Epoch 240, val loss: 1.5984089374542236
Epoch 250, training loss: 66.37013244628906 = 1.528347134590149 + 10.0 * 6.48417854309082
Epoch 250, val loss: 1.5767276287078857
Epoch 260, training loss: 66.23515319824219 = 1.5002610683441162 + 10.0 * 6.473489284515381
Epoch 260, val loss: 1.55450439453125
Epoch 270, training loss: 66.103271484375 = 1.471374750137329 + 10.0 * 6.463189601898193
Epoch 270, val loss: 1.531842589378357
Epoch 280, training loss: 66.05722045898438 = 1.4419623613357544 + 10.0 * 6.461525917053223
Epoch 280, val loss: 1.5086669921875
Epoch 290, training loss: 65.90005493164062 = 1.4120534658432007 + 10.0 * 6.448800086975098
Epoch 290, val loss: 1.485277533531189
Epoch 300, training loss: 65.77818298339844 = 1.3824025392532349 + 10.0 * 6.439578533172607
Epoch 300, val loss: 1.4622169733047485
Epoch 310, training loss: 65.67267608642578 = 1.352639079093933 + 10.0 * 6.432003498077393
Epoch 310, val loss: 1.4393140077590942
Epoch 320, training loss: 65.63412475585938 = 1.323029637336731 + 10.0 * 6.431109428405762
Epoch 320, val loss: 1.4165973663330078
Epoch 330, training loss: 65.51280975341797 = 1.2932415008544922 + 10.0 * 6.421956539154053
Epoch 330, val loss: 1.3938534259796143
Epoch 340, training loss: 65.39323425292969 = 1.2637931108474731 + 10.0 * 6.4129438400268555
Epoch 340, val loss: 1.371583342552185
Epoch 350, training loss: 65.33200073242188 = 1.2346522808074951 + 10.0 * 6.409734725952148
Epoch 350, val loss: 1.3495798110961914
Epoch 360, training loss: 65.24877166748047 = 1.2054762840270996 + 10.0 * 6.404329776763916
Epoch 360, val loss: 1.3276933431625366
Epoch 370, training loss: 65.15066528320312 = 1.1767828464508057 + 10.0 * 6.397388458251953
Epoch 370, val loss: 1.306237816810608
Epoch 380, training loss: 65.07047271728516 = 1.1482611894607544 + 10.0 * 6.392220973968506
Epoch 380, val loss: 1.2850092649459839
Epoch 390, training loss: 65.0074691772461 = 1.1199275255203247 + 10.0 * 6.388753890991211
Epoch 390, val loss: 1.264089584350586
Epoch 400, training loss: 64.96456146240234 = 1.091512680053711 + 10.0 * 6.387304782867432
Epoch 400, val loss: 1.2429088354110718
Epoch 410, training loss: 64.86479949951172 = 1.0635883808135986 + 10.0 * 6.380121231079102
Epoch 410, val loss: 1.2223951816558838
Epoch 420, training loss: 64.79204559326172 = 1.035915493965149 + 10.0 * 6.375612735748291
Epoch 420, val loss: 1.2022958993911743
Epoch 430, training loss: 64.7557144165039 = 1.0084545612335205 + 10.0 * 6.374726295471191
Epoch 430, val loss: 1.1823514699935913
Epoch 440, training loss: 64.68390655517578 = 0.9810750484466553 + 10.0 * 6.370283126831055
Epoch 440, val loss: 1.1625185012817383
Epoch 450, training loss: 64.61040496826172 = 0.9542006254196167 + 10.0 * 6.3656206130981445
Epoch 450, val loss: 1.143414855003357
Epoch 460, training loss: 64.54457092285156 = 0.92769455909729 + 10.0 * 6.361687660217285
Epoch 460, val loss: 1.1246020793914795
Epoch 470, training loss: 64.4975814819336 = 0.9015501141548157 + 10.0 * 6.359602928161621
Epoch 470, val loss: 1.1061350107192993
Epoch 480, training loss: 64.51294708251953 = 0.8757835626602173 + 10.0 * 6.3637166023254395
Epoch 480, val loss: 1.0884813070297241
Epoch 490, training loss: 64.40009307861328 = 0.8504700064659119 + 10.0 * 6.354962348937988
Epoch 490, val loss: 1.0708770751953125
Epoch 500, training loss: 64.33435821533203 = 0.8258747458457947 + 10.0 * 6.350848197937012
Epoch 500, val loss: 1.0543098449707031
Epoch 510, training loss: 64.28276824951172 = 0.8018379807472229 + 10.0 * 6.348093032836914
Epoch 510, val loss: 1.0384243726730347
Epoch 520, training loss: 64.2705078125 = 0.7784202098846436 + 10.0 * 6.349208831787109
Epoch 520, val loss: 1.0232504606246948
Epoch 530, training loss: 64.2175064086914 = 0.7552443146705627 + 10.0 * 6.346226692199707
Epoch 530, val loss: 1.0085408687591553
Epoch 540, training loss: 64.1399917602539 = 0.7328137159347534 + 10.0 * 6.3407182693481445
Epoch 540, val loss: 0.9945473074913025
Epoch 550, training loss: 64.10682678222656 = 0.7109603881835938 + 10.0 * 6.33958625793457
Epoch 550, val loss: 0.9812915921211243
Epoch 560, training loss: 64.056640625 = 0.6895662546157837 + 10.0 * 6.336707592010498
Epoch 560, val loss: 0.9687486290931702
Epoch 570, training loss: 64.00611877441406 = 0.6686736345291138 + 10.0 * 6.333745002746582
Epoch 570, val loss: 0.9567193984985352
Epoch 580, training loss: 63.96504211425781 = 0.6483520865440369 + 10.0 * 6.331668853759766
Epoch 580, val loss: 0.9455135464668274
Epoch 590, training loss: 63.94040298461914 = 0.6284283399581909 + 10.0 * 6.331197261810303
Epoch 590, val loss: 0.934791088104248
Epoch 600, training loss: 63.913822174072266 = 0.6089825630187988 + 10.0 * 6.330483913421631
Epoch 600, val loss: 0.9248008728027344
Epoch 610, training loss: 63.84107971191406 = 0.5900657176971436 + 10.0 * 6.325101375579834
Epoch 610, val loss: 0.9153005480766296
Epoch 620, training loss: 63.804019927978516 = 0.5716812610626221 + 10.0 * 6.323233604431152
Epoch 620, val loss: 0.9064568281173706
Epoch 630, training loss: 63.801761627197266 = 0.5538046360015869 + 10.0 * 6.324795722961426
Epoch 630, val loss: 0.8983187079429626
Epoch 640, training loss: 63.756710052490234 = 0.5361368060112 + 10.0 * 6.322057247161865
Epoch 640, val loss: 0.8902810215950012
Epoch 650, training loss: 63.69489288330078 = 0.519160807132721 + 10.0 * 6.317573070526123
Epoch 650, val loss: 0.8831496238708496
Epoch 660, training loss: 63.654823303222656 = 0.5026249885559082 + 10.0 * 6.315219879150391
Epoch 660, val loss: 0.8765068054199219
Epoch 670, training loss: 63.61931610107422 = 0.4866291284561157 + 10.0 * 6.313268661499023
Epoch 670, val loss: 0.870583176612854
Epoch 680, training loss: 63.659358978271484 = 0.47105854749679565 + 10.0 * 6.3188300132751465
Epoch 680, val loss: 0.8653113842010498
Epoch 690, training loss: 63.586891174316406 = 0.45574942231178284 + 10.0 * 6.313114166259766
Epoch 690, val loss: 0.8599105477333069
Epoch 700, training loss: 63.52751159667969 = 0.44105619192123413 + 10.0 * 6.308645725250244
Epoch 700, val loss: 0.8554792404174805
Epoch 710, training loss: 63.48981857299805 = 0.42690232396125793 + 10.0 * 6.306291580200195
Epoch 710, val loss: 0.8516719341278076
Epoch 720, training loss: 63.46326446533203 = 0.41318804025650024 + 10.0 * 6.305007457733154
Epoch 720, val loss: 0.8483229875564575
Epoch 730, training loss: 63.5047721862793 = 0.3999105393886566 + 10.0 * 6.310486316680908
Epoch 730, val loss: 0.8455257415771484
Epoch 740, training loss: 63.411800384521484 = 0.38686493039131165 + 10.0 * 6.302493572235107
Epoch 740, val loss: 0.8426778316497803
Epoch 750, training loss: 63.37923812866211 = 0.3744083344936371 + 10.0 * 6.300482749938965
Epoch 750, val loss: 0.8407608866691589
Epoch 760, training loss: 63.35445785522461 = 0.3623850643634796 + 10.0 * 6.2992072105407715
Epoch 760, val loss: 0.8391113877296448
Epoch 770, training loss: 63.3443489074707 = 0.3506467938423157 + 10.0 * 6.299370288848877
Epoch 770, val loss: 0.8378658890724182
Epoch 780, training loss: 63.31295394897461 = 0.33930349349975586 + 10.0 * 6.297365188598633
Epoch 780, val loss: 0.8369832038879395
Epoch 790, training loss: 63.27476119995117 = 0.3283989131450653 + 10.0 * 6.294636249542236
Epoch 790, val loss: 0.8366189002990723
Epoch 800, training loss: 63.27289581298828 = 0.31787756085395813 + 10.0 * 6.295501708984375
Epoch 800, val loss: 0.8367056250572205
Epoch 810, training loss: 63.23042297363281 = 0.30751827359199524 + 10.0 * 6.292290687561035
Epoch 810, val loss: 0.8366140127182007
Epoch 820, training loss: 63.22261428833008 = 0.2976168692111969 + 10.0 * 6.292500019073486
Epoch 820, val loss: 0.8373674154281616
Epoch 830, training loss: 63.206607818603516 = 0.2879851758480072 + 10.0 * 6.2918620109558105
Epoch 830, val loss: 0.838057279586792
Epoch 840, training loss: 63.17717361450195 = 0.2787071764469147 + 10.0 * 6.289846897125244
Epoch 840, val loss: 0.8390854597091675
Epoch 850, training loss: 63.16074752807617 = 0.2696714699268341 + 10.0 * 6.289107322692871
Epoch 850, val loss: 0.8403964638710022
Epoch 860, training loss: 63.12367630004883 = 0.26093772053718567 + 10.0 * 6.286273956298828
Epoch 860, val loss: 0.8421175479888916
Epoch 870, training loss: 63.109039306640625 = 0.2525162994861603 + 10.0 * 6.285652160644531
Epoch 870, val loss: 0.8439748883247375
Epoch 880, training loss: 63.177040100097656 = 0.24432511627674103 + 10.0 * 6.293271541595459
Epoch 880, val loss: 0.8460118770599365
Epoch 890, training loss: 63.106327056884766 = 0.23644661903381348 + 10.0 * 6.286988258361816
Epoch 890, val loss: 0.8483474254608154
Epoch 900, training loss: 63.05164337158203 = 0.22878509759902954 + 10.0 * 6.282285690307617
Epoch 900, val loss: 0.8510738015174866
Epoch 910, training loss: 63.035579681396484 = 0.22142037749290466 + 10.0 * 6.281415939331055
Epoch 910, val loss: 0.8538452386856079
Epoch 920, training loss: 63.08711242675781 = 0.21432366967201233 + 10.0 * 6.287278652191162
Epoch 920, val loss: 0.8570265173912048
Epoch 930, training loss: 63.0473518371582 = 0.2074640691280365 + 10.0 * 6.283988952636719
Epoch 930, val loss: 0.8601358532905579
Epoch 940, training loss: 62.985450744628906 = 0.20075081288814545 + 10.0 * 6.278470039367676
Epoch 940, val loss: 0.8636491298675537
Epoch 950, training loss: 62.97206497192383 = 0.1943601369857788 + 10.0 * 6.277770519256592
Epoch 950, val loss: 0.8674007058143616
Epoch 960, training loss: 62.976871490478516 = 0.18817684054374695 + 10.0 * 6.27886962890625
Epoch 960, val loss: 0.8710269331932068
Epoch 970, training loss: 62.96577072143555 = 0.182182177901268 + 10.0 * 6.2783589363098145
Epoch 970, val loss: 0.8754560947418213
Epoch 980, training loss: 62.9428596496582 = 0.1763700544834137 + 10.0 * 6.276648998260498
Epoch 980, val loss: 0.8794214129447937
Epoch 990, training loss: 62.914703369140625 = 0.17081089317798615 + 10.0 * 6.274389266967773
Epoch 990, val loss: 0.8842262625694275
Epoch 1000, training loss: 62.89620590209961 = 0.1654561311006546 + 10.0 * 6.273075103759766
Epoch 1000, val loss: 0.8887913823127747
Epoch 1010, training loss: 62.93735885620117 = 0.16030146181583405 + 10.0 * 6.277705669403076
Epoch 1010, val loss: 0.893562376499176
Epoch 1020, training loss: 62.88905715942383 = 0.1552749127149582 + 10.0 * 6.273378372192383
Epoch 1020, val loss: 0.8984555602073669
Epoch 1030, training loss: 62.86451721191406 = 0.15044523775577545 + 10.0 * 6.271407127380371
Epoch 1030, val loss: 0.9035170674324036
Epoch 1040, training loss: 62.88270568847656 = 0.14583633840084076 + 10.0 * 6.27368688583374
Epoch 1040, val loss: 0.9088294506072998
Epoch 1050, training loss: 62.842262268066406 = 0.1412804126739502 + 10.0 * 6.2700982093811035
Epoch 1050, val loss: 0.9137287139892578
Epoch 1060, training loss: 62.82975387573242 = 0.13696524500846863 + 10.0 * 6.269278526306152
Epoch 1060, val loss: 0.9193617105484009
Epoch 1070, training loss: 62.81040573120117 = 0.1327999234199524 + 10.0 * 6.267760276794434
Epoch 1070, val loss: 0.9247071146965027
Epoch 1080, training loss: 62.855350494384766 = 0.12878787517547607 + 10.0 * 6.272656440734863
Epoch 1080, val loss: 0.9299694299697876
Epoch 1090, training loss: 62.819000244140625 = 0.1248997300863266 + 10.0 * 6.269410133361816
Epoch 1090, val loss: 0.9362858533859253
Epoch 1100, training loss: 62.784996032714844 = 0.12113271653652191 + 10.0 * 6.26638650894165
Epoch 1100, val loss: 0.9415873885154724
Epoch 1110, training loss: 62.76139450073242 = 0.11754245311021805 + 10.0 * 6.264385223388672
Epoch 1110, val loss: 0.9475666880607605
Epoch 1120, training loss: 62.76955032348633 = 0.11408410966396332 + 10.0 * 6.265546798706055
Epoch 1120, val loss: 0.9535643458366394
Epoch 1130, training loss: 62.781368255615234 = 0.1107020154595375 + 10.0 * 6.267066478729248
Epoch 1130, val loss: 0.9591612815856934
Epoch 1140, training loss: 62.756324768066406 = 0.10743246972560883 + 10.0 * 6.264889240264893
Epoch 1140, val loss: 0.9654728770256042
Epoch 1150, training loss: 62.72343826293945 = 0.10430512577295303 + 10.0 * 6.261913299560547
Epoch 1150, val loss: 0.9715461134910583
Epoch 1160, training loss: 62.72240447998047 = 0.10130836814641953 + 10.0 * 6.262109756469727
Epoch 1160, val loss: 0.977561891078949
Epoch 1170, training loss: 62.75798416137695 = 0.09838496148586273 + 10.0 * 6.265959739685059
Epoch 1170, val loss: 0.9835596084594727
Epoch 1180, training loss: 62.74226760864258 = 0.09559627622365952 + 10.0 * 6.26466703414917
Epoch 1180, val loss: 0.9901967644691467
Epoch 1190, training loss: 62.69196319580078 = 0.09286320209503174 + 10.0 * 6.259909629821777
Epoch 1190, val loss: 0.996230959892273
Epoch 1200, training loss: 62.680870056152344 = 0.09025675058364868 + 10.0 * 6.259061336517334
Epoch 1200, val loss: 1.0023900270462036
Epoch 1210, training loss: 62.66791915893555 = 0.08774986863136292 + 10.0 * 6.258017063140869
Epoch 1210, val loss: 1.0087953805923462
Epoch 1220, training loss: 62.683712005615234 = 0.08532346785068512 + 10.0 * 6.259839057922363
Epoch 1220, val loss: 1.015049934387207
Epoch 1230, training loss: 62.6523323059082 = 0.08296605944633484 + 10.0 * 6.256936550140381
Epoch 1230, val loss: 1.0214200019836426
Epoch 1240, training loss: 62.65548324584961 = 0.0807156041264534 + 10.0 * 6.257476806640625
Epoch 1240, val loss: 1.027894139289856
Epoch 1250, training loss: 62.770484924316406 = 0.0785200223326683 + 10.0 * 6.269196510314941
Epoch 1250, val loss: 1.0339499711990356
Epoch 1260, training loss: 62.639591217041016 = 0.07634108513593674 + 10.0 * 6.2563252449035645
Epoch 1260, val loss: 1.040238857269287
Epoch 1270, training loss: 62.62554168701172 = 0.07430838793516159 + 10.0 * 6.255123615264893
Epoch 1270, val loss: 1.0466421842575073
Epoch 1280, training loss: 62.612770080566406 = 0.07233799993991852 + 10.0 * 6.254043102264404
Epoch 1280, val loss: 1.0529775619506836
Epoch 1290, training loss: 62.60444259643555 = 0.07044743001461029 + 10.0 * 6.253399848937988
Epoch 1290, val loss: 1.0591471195220947
Epoch 1300, training loss: 62.67283248901367 = 0.0686073899269104 + 10.0 * 6.260422706604004
Epoch 1300, val loss: 1.0651406049728394
Epoch 1310, training loss: 62.616703033447266 = 0.06682313233613968 + 10.0 * 6.254988193511963
Epoch 1310, val loss: 1.0722464323043823
Epoch 1320, training loss: 62.622432708740234 = 0.06510269641876221 + 10.0 * 6.255733013153076
Epoch 1320, val loss: 1.0781065225601196
Epoch 1330, training loss: 62.593955993652344 = 0.0634174570441246 + 10.0 * 6.253053665161133
Epoch 1330, val loss: 1.084608793258667
Epoch 1340, training loss: 62.57803726196289 = 0.06179609149694443 + 10.0 * 6.25162410736084
Epoch 1340, val loss: 1.0905702114105225
Epoch 1350, training loss: 62.56589126586914 = 0.060236670076847076 + 10.0 * 6.250565528869629
Epoch 1350, val loss: 1.0970579385757446
Epoch 1360, training loss: 62.5611457824707 = 0.05874579772353172 + 10.0 * 6.250239849090576
Epoch 1360, val loss: 1.103211760520935
Epoch 1370, training loss: 62.5919189453125 = 0.05728908255696297 + 10.0 * 6.253462791442871
Epoch 1370, val loss: 1.109426498413086
Epoch 1380, training loss: 62.554996490478516 = 0.0558653399348259 + 10.0 * 6.249913215637207
Epoch 1380, val loss: 1.1156479120254517
Epoch 1390, training loss: 62.54499435424805 = 0.05450146645307541 + 10.0 * 6.249049186706543
Epoch 1390, val loss: 1.1216586828231812
Epoch 1400, training loss: 62.64485168457031 = 0.05316825583577156 + 10.0 * 6.2591681480407715
Epoch 1400, val loss: 1.127813696861267
Epoch 1410, training loss: 62.556129455566406 = 0.05189107358455658 + 10.0 * 6.250423908233643
Epoch 1410, val loss: 1.1336992979049683
Epoch 1420, training loss: 62.51824951171875 = 0.050630275160074234 + 10.0 * 6.246762275695801
Epoch 1420, val loss: 1.1400917768478394
Epoch 1430, training loss: 62.515045166015625 = 0.04943643882870674 + 10.0 * 6.246561050415039
Epoch 1430, val loss: 1.1461694240570068
Epoch 1440, training loss: 62.521766662597656 = 0.04829339310526848 + 10.0 * 6.247347354888916
Epoch 1440, val loss: 1.1521728038787842
Epoch 1450, training loss: 62.53116226196289 = 0.047161366790533066 + 10.0 * 6.2484002113342285
Epoch 1450, val loss: 1.158051609992981
Epoch 1460, training loss: 62.50553894042969 = 0.04606033116579056 + 10.0 * 6.24594783782959
Epoch 1460, val loss: 1.1640150547027588
Epoch 1470, training loss: 62.50017547607422 = 0.04500534012913704 + 10.0 * 6.245516777038574
Epoch 1470, val loss: 1.1701284646987915
Epoch 1480, training loss: 62.505287170410156 = 0.043977171182632446 + 10.0 * 6.24613094329834
Epoch 1480, val loss: 1.1759432554244995
Epoch 1490, training loss: 62.48953628540039 = 0.042979106307029724 + 10.0 * 6.244655609130859
Epoch 1490, val loss: 1.1814393997192383
Epoch 1500, training loss: 62.486167907714844 = 0.04201861098408699 + 10.0 * 6.244414806365967
Epoch 1500, val loss: 1.18741774559021
Epoch 1510, training loss: 62.52596664428711 = 0.04109180346131325 + 10.0 * 6.24848747253418
Epoch 1510, val loss: 1.1931620836257935
Epoch 1520, training loss: 62.493892669677734 = 0.04015849903225899 + 10.0 * 6.245373725891113
Epoch 1520, val loss: 1.198636531829834
Epoch 1530, training loss: 62.466548919677734 = 0.03927827253937721 + 10.0 * 6.242726802825928
Epoch 1530, val loss: 1.2044358253479004
Epoch 1540, training loss: 62.45808029174805 = 0.03842439129948616 + 10.0 * 6.241965293884277
Epoch 1540, val loss: 1.210222601890564
Epoch 1550, training loss: 62.50038146972656 = 0.03761060908436775 + 10.0 * 6.246277332305908
Epoch 1550, val loss: 1.2159382104873657
Epoch 1560, training loss: 62.46071243286133 = 0.03679186850786209 + 10.0 * 6.242392063140869
Epoch 1560, val loss: 1.2211213111877441
Epoch 1570, training loss: 62.48187255859375 = 0.036016665399074554 + 10.0 * 6.2445855140686035
Epoch 1570, val loss: 1.227123498916626
Epoch 1580, training loss: 62.43973159790039 = 0.035242628306150436 + 10.0 * 6.240448951721191
Epoch 1580, val loss: 1.231958031654358
Epoch 1590, training loss: 62.43892288208008 = 0.03451015427708626 + 10.0 * 6.24044132232666
Epoch 1590, val loss: 1.237709879875183
Epoch 1600, training loss: 62.464229583740234 = 0.033801257610321045 + 10.0 * 6.243042945861816
Epoch 1600, val loss: 1.2428765296936035
Epoch 1610, training loss: 62.456241607666016 = 0.03311213478446007 + 10.0 * 6.242312908172607
Epoch 1610, val loss: 1.248680830001831
Epoch 1620, training loss: 62.423641204833984 = 0.03241981193423271 + 10.0 * 6.239121913909912
Epoch 1620, val loss: 1.2536667585372925
Epoch 1630, training loss: 62.42497634887695 = 0.031764645129442215 + 10.0 * 6.239321231842041
Epoch 1630, val loss: 1.2591911554336548
Epoch 1640, training loss: 62.4619140625 = 0.03114321641623974 + 10.0 * 6.243077278137207
Epoch 1640, val loss: 1.2642145156860352
Epoch 1650, training loss: 62.42338562011719 = 0.030510181561112404 + 10.0 * 6.239287376403809
Epoch 1650, val loss: 1.2697763442993164
Epoch 1660, training loss: 62.414337158203125 = 0.029917074367403984 + 10.0 * 6.2384419441223145
Epoch 1660, val loss: 1.2750110626220703
Epoch 1670, training loss: 62.4122428894043 = 0.02933053858578205 + 10.0 * 6.238291263580322
Epoch 1670, val loss: 1.2799646854400635
Epoch 1680, training loss: 62.41667556762695 = 0.028770694509148598 + 10.0 * 6.238790512084961
Epoch 1680, val loss: 1.285199761390686
Epoch 1690, training loss: 62.435264587402344 = 0.02821335382759571 + 10.0 * 6.2407050132751465
Epoch 1690, val loss: 1.2904374599456787
Epoch 1700, training loss: 62.40507888793945 = 0.027662284672260284 + 10.0 * 6.237741470336914
Epoch 1700, val loss: 1.2951453924179077
Epoch 1710, training loss: 62.39095687866211 = 0.027140192687511444 + 10.0 * 6.236381530761719
Epoch 1710, val loss: 1.3003675937652588
Epoch 1720, training loss: 62.38423156738281 = 0.026632405817508698 + 10.0 * 6.235759735107422
Epoch 1720, val loss: 1.3052709102630615
Epoch 1730, training loss: 62.400569915771484 = 0.026142677292227745 + 10.0 * 6.237442970275879
Epoch 1730, val loss: 1.3102747201919556
Epoch 1740, training loss: 62.4126091003418 = 0.02565797232091427 + 10.0 * 6.23869514465332
Epoch 1740, val loss: 1.3152145147323608
Epoch 1750, training loss: 62.38376235961914 = 0.025168543681502342 + 10.0 * 6.235859394073486
Epoch 1750, val loss: 1.3198095560073853
Epoch 1760, training loss: 62.36981964111328 = 0.024715900421142578 + 10.0 * 6.23451042175293
Epoch 1760, val loss: 1.3249861001968384
Epoch 1770, training loss: 62.368167877197266 = 0.024271776899695396 + 10.0 * 6.234389305114746
Epoch 1770, val loss: 1.3295912742614746
Epoch 1780, training loss: 62.38928985595703 = 0.02384159341454506 + 10.0 * 6.236544609069824
Epoch 1780, val loss: 1.3344937562942505
Epoch 1790, training loss: 62.36405944824219 = 0.02341848984360695 + 10.0 * 6.234064102172852
Epoch 1790, val loss: 1.3386379480361938
Epoch 1800, training loss: 62.39529800415039 = 0.023008260875940323 + 10.0 * 6.237229347229004
Epoch 1800, val loss: 1.3432457447052002
Epoch 1810, training loss: 62.36073303222656 = 0.02259371243417263 + 10.0 * 6.233813762664795
Epoch 1810, val loss: 1.3477953672409058
Epoch 1820, training loss: 62.345428466796875 = 0.022205863147974014 + 10.0 * 6.2323222160339355
Epoch 1820, val loss: 1.352719783782959
Epoch 1830, training loss: 62.35074996948242 = 0.021831395104527473 + 10.0 * 6.232892036437988
Epoch 1830, val loss: 1.3573001623153687
Epoch 1840, training loss: 62.3874397277832 = 0.021465960890054703 + 10.0 * 6.236597537994385
Epoch 1840, val loss: 1.36154305934906
Epoch 1850, training loss: 62.3614616394043 = 0.0210866779088974 + 10.0 * 6.234037399291992
Epoch 1850, val loss: 1.3655049800872803
Epoch 1860, training loss: 62.336639404296875 = 0.02073514461517334 + 10.0 * 6.231590270996094
Epoch 1860, val loss: 1.3704869747161865
Epoch 1870, training loss: 62.337074279785156 = 0.020393777638673782 + 10.0 * 6.231667995452881
Epoch 1870, val loss: 1.3745696544647217
Epoch 1880, training loss: 62.39741134643555 = 0.020058779045939445 + 10.0 * 6.237735271453857
Epoch 1880, val loss: 1.3787925243377686
Epoch 1890, training loss: 62.347816467285156 = 0.019721774384379387 + 10.0 * 6.232809543609619
Epoch 1890, val loss: 1.3833657503128052
Epoch 1900, training loss: 62.32752227783203 = 0.01939917914569378 + 10.0 * 6.2308125495910645
Epoch 1900, val loss: 1.3872878551483154
Epoch 1910, training loss: 62.31601333618164 = 0.01909027434885502 + 10.0 * 6.229692459106445
Epoch 1910, val loss: 1.39210844039917
Epoch 1920, training loss: 62.31428527832031 = 0.018789205700159073 + 10.0 * 6.229549884796143
Epoch 1920, val loss: 1.396163821220398
Epoch 1930, training loss: 62.3780403137207 = 0.018497977405786514 + 10.0 * 6.235954284667969
Epoch 1930, val loss: 1.4008440971374512
Epoch 1940, training loss: 62.42134475708008 = 0.01819513738155365 + 10.0 * 6.240314960479736
Epoch 1940, val loss: 1.4038217067718506
Epoch 1950, training loss: 62.31515121459961 = 0.01790086179971695 + 10.0 * 6.229724884033203
Epoch 1950, val loss: 1.4081007242202759
Epoch 1960, training loss: 62.313846588134766 = 0.017618464305996895 + 10.0 * 6.229622840881348
Epoch 1960, val loss: 1.4122986793518066
Epoch 1970, training loss: 62.295902252197266 = 0.017351659014821053 + 10.0 * 6.2278547286987305
Epoch 1970, val loss: 1.4162956476211548
Epoch 1980, training loss: 62.29392623901367 = 0.017092516645789146 + 10.0 * 6.227683067321777
Epoch 1980, val loss: 1.4202542304992676
Epoch 1990, training loss: 62.31085968017578 = 0.016838710755109787 + 10.0 * 6.2294020652771
Epoch 1990, val loss: 1.4242106676101685
Epoch 2000, training loss: 62.3296012878418 = 0.01658165082335472 + 10.0 * 6.231301784515381
Epoch 2000, val loss: 1.4279320240020752
Epoch 2010, training loss: 62.32566452026367 = 0.01632797345519066 + 10.0 * 6.230933666229248
Epoch 2010, val loss: 1.4315189123153687
Epoch 2020, training loss: 62.296451568603516 = 0.01608489453792572 + 10.0 * 6.228036403656006
Epoch 2020, val loss: 1.435717225074768
Epoch 2030, training loss: 62.28215026855469 = 0.015847573056817055 + 10.0 * 6.226630210876465
Epoch 2030, val loss: 1.4395917654037476
Epoch 2040, training loss: 62.28181076049805 = 0.015620885416865349 + 10.0 * 6.226618766784668
Epoch 2040, val loss: 1.4435768127441406
Epoch 2050, training loss: 62.33409118652344 = 0.015401219949126244 + 10.0 * 6.231869220733643
Epoch 2050, val loss: 1.4475574493408203
Epoch 2060, training loss: 62.28765106201172 = 0.015175973065197468 + 10.0 * 6.227247714996338
Epoch 2060, val loss: 1.450467824935913
Epoch 2070, training loss: 62.27302932739258 = 0.014958187937736511 + 10.0 * 6.225807189941406
Epoch 2070, val loss: 1.4546540975570679
Epoch 2080, training loss: 62.29822540283203 = 0.014753757044672966 + 10.0 * 6.228346824645996
Epoch 2080, val loss: 1.4583529233932495
Epoch 2090, training loss: 62.284881591796875 = 0.01454334706068039 + 10.0 * 6.227034091949463
Epoch 2090, val loss: 1.4617799520492554
Epoch 2100, training loss: 62.277862548828125 = 0.014334187842905521 + 10.0 * 6.226352691650391
Epoch 2100, val loss: 1.4653767347335815
Epoch 2110, training loss: 62.27924346923828 = 0.01413753256201744 + 10.0 * 6.226510524749756
Epoch 2110, val loss: 1.4689583778381348
Epoch 2120, training loss: 62.29151916503906 = 0.013947442173957825 + 10.0 * 6.227757453918457
Epoch 2120, val loss: 1.4727767705917358
Epoch 2130, training loss: 62.27231979370117 = 0.013756885193288326 + 10.0 * 6.225856304168701
Epoch 2130, val loss: 1.4761450290679932
Epoch 2140, training loss: 62.25849533081055 = 0.013567817397415638 + 10.0 * 6.22449254989624
Epoch 2140, val loss: 1.479483962059021
Epoch 2150, training loss: 62.273155212402344 = 0.013385254889726639 + 10.0 * 6.225976943969727
Epoch 2150, val loss: 1.4830193519592285
Epoch 2160, training loss: 62.283302307128906 = 0.013206998817622662 + 10.0 * 6.2270097732543945
Epoch 2160, val loss: 1.485985279083252
Epoch 2170, training loss: 62.267311096191406 = 0.013033275492489338 + 10.0 * 6.225427627563477
Epoch 2170, val loss: 1.4903377294540405
Epoch 2180, training loss: 62.25931167602539 = 0.01285899244248867 + 10.0 * 6.224645137786865
Epoch 2180, val loss: 1.4933838844299316
Epoch 2190, training loss: 62.285621643066406 = 0.012692735530436039 + 10.0 * 6.227293014526367
Epoch 2190, val loss: 1.4964358806610107
Epoch 2200, training loss: 62.24837875366211 = 0.012529326602816582 + 10.0 * 6.22358512878418
Epoch 2200, val loss: 1.49994957447052
Epoch 2210, training loss: 62.23714065551758 = 0.01236671581864357 + 10.0 * 6.222477436065674
Epoch 2210, val loss: 1.5032823085784912
Epoch 2220, training loss: 62.257179260253906 = 0.01220997329801321 + 10.0 * 6.224496841430664
Epoch 2220, val loss: 1.5063183307647705
Epoch 2230, training loss: 62.2528190612793 = 0.012057226151227951 + 10.0 * 6.224076271057129
Epoch 2230, val loss: 1.5100418329238892
Epoch 2240, training loss: 62.25068283081055 = 0.011907062493264675 + 10.0 * 6.223877906799316
Epoch 2240, val loss: 1.513183355331421
Epoch 2250, training loss: 62.236907958984375 = 0.011754847131669521 + 10.0 * 6.22251558303833
Epoch 2250, val loss: 1.516062617301941
Epoch 2260, training loss: 62.23851776123047 = 0.011613459326326847 + 10.0 * 6.222690582275391
Epoch 2260, val loss: 1.5194759368896484
Epoch 2270, training loss: 62.252052307128906 = 0.011473461054265499 + 10.0 * 6.224057674407959
Epoch 2270, val loss: 1.5221431255340576
Epoch 2280, training loss: 62.3070068359375 = 0.011336857452988625 + 10.0 * 6.229567050933838
Epoch 2280, val loss: 1.5255827903747559
Epoch 2290, training loss: 62.24668884277344 = 0.011185770854353905 + 10.0 * 6.223550319671631
Epoch 2290, val loss: 1.5290566682815552
Epoch 2300, training loss: 62.226036071777344 = 0.011055379174649715 + 10.0 * 6.221498012542725
Epoch 2300, val loss: 1.53203547000885
Epoch 2310, training loss: 62.25357437133789 = 0.010927636176347733 + 10.0 * 6.224264621734619
Epoch 2310, val loss: 1.5355584621429443
Epoch 2320, training loss: 62.22421646118164 = 0.01079468335956335 + 10.0 * 6.221342086791992
Epoch 2320, val loss: 1.5378592014312744
Epoch 2330, training loss: 62.21984100341797 = 0.010664807632565498 + 10.0 * 6.220917701721191
Epoch 2330, val loss: 1.5411756038665771
Epoch 2340, training loss: 62.21440124511719 = 0.010542466305196285 + 10.0 * 6.220385551452637
Epoch 2340, val loss: 1.544225811958313
Epoch 2350, training loss: 62.215057373046875 = 0.01042135152965784 + 10.0 * 6.220463752746582
Epoch 2350, val loss: 1.547137975692749
Epoch 2360, training loss: 62.22903060913086 = 0.010301968082785606 + 10.0 * 6.221872806549072
Epoch 2360, val loss: 1.5500353574752808
Epoch 2370, training loss: 62.21287155151367 = 0.01018517930060625 + 10.0 * 6.220268726348877
Epoch 2370, val loss: 1.5527633428573608
Epoch 2380, training loss: 62.23741149902344 = 0.010071921162307262 + 10.0 * 6.222733974456787
Epoch 2380, val loss: 1.5556390285491943
Epoch 2390, training loss: 62.262840270996094 = 0.009951820597052574 + 10.0 * 6.2252888679504395
Epoch 2390, val loss: 1.5584431886672974
Epoch 2400, training loss: 62.205726623535156 = 0.009839099831879139 + 10.0 * 6.219588756561279
Epoch 2400, val loss: 1.5608851909637451
Epoch 2410, training loss: 62.194515228271484 = 0.00972877349704504 + 10.0 * 6.218478679656982
Epoch 2410, val loss: 1.5641905069351196
Epoch 2420, training loss: 62.19105911254883 = 0.009624429047107697 + 10.0 * 6.218143463134766
Epoch 2420, val loss: 1.5670021772384644
Epoch 2430, training loss: 62.202964782714844 = 0.009522250853478909 + 10.0 * 6.219344139099121
Epoch 2430, val loss: 1.5702003240585327
Epoch 2440, training loss: 62.222816467285156 = 0.009419729933142662 + 10.0 * 6.221339702606201
Epoch 2440, val loss: 1.5726268291473389
Epoch 2450, training loss: 62.222721099853516 = 0.009315750561654568 + 10.0 * 6.221340656280518
Epoch 2450, val loss: 1.5745816230773926
Epoch 2460, training loss: 62.189762115478516 = 0.009211305528879166 + 10.0 * 6.21805477142334
Epoch 2460, val loss: 1.5777688026428223
Epoch 2470, training loss: 62.19639205932617 = 0.00911578256636858 + 10.0 * 6.2187275886535645
Epoch 2470, val loss: 1.5801973342895508
Epoch 2480, training loss: 62.212257385253906 = 0.009021364152431488 + 10.0 * 6.22032356262207
Epoch 2480, val loss: 1.5830414295196533
Epoch 2490, training loss: 62.20984649658203 = 0.008924419991672039 + 10.0 * 6.220092296600342
Epoch 2490, val loss: 1.5858807563781738
Epoch 2500, training loss: 62.19081497192383 = 0.008829980157315731 + 10.0 * 6.218198299407959
Epoch 2500, val loss: 1.5879837274551392
Epoch 2510, training loss: 62.173580169677734 = 0.008733382448554039 + 10.0 * 6.216485023498535
Epoch 2510, val loss: 1.5906572341918945
Epoch 2520, training loss: 62.16986083984375 = 0.008645313791930676 + 10.0 * 6.216121673583984
Epoch 2520, val loss: 1.5931048393249512
Epoch 2530, training loss: 62.179786682128906 = 0.008560795336961746 + 10.0 * 6.217122554779053
Epoch 2530, val loss: 1.595728874206543
Epoch 2540, training loss: 62.21698760986328 = 0.008472316898405552 + 10.0 * 6.220851421356201
Epoch 2540, val loss: 1.5979543924331665
Epoch 2550, training loss: 62.18551254272461 = 0.008385678753256798 + 10.0 * 6.217712879180908
Epoch 2550, val loss: 1.6008856296539307
Epoch 2560, training loss: 62.177528381347656 = 0.008304639719426632 + 10.0 * 6.216922283172607
Epoch 2560, val loss: 1.6029713153839111
Epoch 2570, training loss: 62.2235107421875 = 0.008220447227358818 + 10.0 * 6.221529006958008
Epoch 2570, val loss: 1.6052052974700928
Epoch 2580, training loss: 62.17728805541992 = 0.008136280812323093 + 10.0 * 6.216915130615234
Epoch 2580, val loss: 1.6078249216079712
Epoch 2590, training loss: 62.162811279296875 = 0.00805517565459013 + 10.0 * 6.215475559234619
Epoch 2590, val loss: 1.6104832887649536
Epoch 2600, training loss: 62.16753387451172 = 0.007980464026331902 + 10.0 * 6.2159552574157715
Epoch 2600, val loss: 1.6128805875778198
Epoch 2610, training loss: 62.19367980957031 = 0.00790351815521717 + 10.0 * 6.2185773849487305
Epoch 2610, val loss: 1.615246295928955
Epoch 2620, training loss: 62.163414001464844 = 0.007825197651982307 + 10.0 * 6.215559005737305
Epoch 2620, val loss: 1.617283821105957
Epoch 2630, training loss: 62.20718765258789 = 0.007754635997116566 + 10.0 * 6.219943046569824
Epoch 2630, val loss: 1.6199451684951782
Epoch 2640, training loss: 62.160858154296875 = 0.007675946690142155 + 10.0 * 6.215318202972412
Epoch 2640, val loss: 1.621756672859192
Epoch 2650, training loss: 62.15580749511719 = 0.0076034190133214 + 10.0 * 6.214820384979248
Epoch 2650, val loss: 1.6240761280059814
Epoch 2660, training loss: 62.14943313598633 = 0.007532256189733744 + 10.0 * 6.2141900062561035
Epoch 2660, val loss: 1.626745343208313
Epoch 2670, training loss: 62.15371322631836 = 0.007464660797268152 + 10.0 * 6.214624881744385
Epoch 2670, val loss: 1.6290688514709473
Epoch 2680, training loss: 62.16615295410156 = 0.007396790664643049 + 10.0 * 6.215875625610352
Epoch 2680, val loss: 1.6311595439910889
Epoch 2690, training loss: 62.152496337890625 = 0.007324653677642345 + 10.0 * 6.214517116546631
Epoch 2690, val loss: 1.6330088376998901
Epoch 2700, training loss: 62.25594711303711 = 0.007257631979882717 + 10.0 * 6.2248687744140625
Epoch 2700, val loss: 1.635585069656372
Epoch 2710, training loss: 62.17853546142578 = 0.00719185546040535 + 10.0 * 6.217134475708008
Epoch 2710, val loss: 1.6371821165084839
Epoch 2720, training loss: 62.136409759521484 = 0.007120970170944929 + 10.0 * 6.212928771972656
Epoch 2720, val loss: 1.6394577026367188
Epoch 2730, training loss: 62.12869644165039 = 0.007060183212161064 + 10.0 * 6.21216344833374
Epoch 2730, val loss: 1.6415740251541138
Epoch 2740, training loss: 62.12611770629883 = 0.006999708712100983 + 10.0 * 6.211911678314209
Epoch 2740, val loss: 1.6436715126037598
Epoch 2750, training loss: 62.14796447753906 = 0.006941348314285278 + 10.0 * 6.214102268218994
Epoch 2750, val loss: 1.6455838680267334
Epoch 2760, training loss: 62.14623260498047 = 0.0068794372491538525 + 10.0 * 6.213935375213623
Epoch 2760, val loss: 1.6475450992584229
Epoch 2770, training loss: 62.14204025268555 = 0.006817171350121498 + 10.0 * 6.213522434234619
Epoch 2770, val loss: 1.6499124765396118
Epoch 2780, training loss: 62.144798278808594 = 0.006758261937648058 + 10.0 * 6.213803768157959
Epoch 2780, val loss: 1.6517709493637085
Epoch 2790, training loss: 62.153076171875 = 0.0067012012004852295 + 10.0 * 6.214637279510498
Epoch 2790, val loss: 1.6542000770568848
Epoch 2800, training loss: 62.12899398803711 = 0.006641922984272242 + 10.0 * 6.212235450744629
Epoch 2800, val loss: 1.6561684608459473
Epoch 2810, training loss: 62.124698638916016 = 0.006585469003766775 + 10.0 * 6.211811542510986
Epoch 2810, val loss: 1.6580870151519775
Epoch 2820, training loss: 62.142337799072266 = 0.006530991289764643 + 10.0 * 6.21358060836792
Epoch 2820, val loss: 1.6598327159881592
Epoch 2830, training loss: 62.13998031616211 = 0.006474432069808245 + 10.0 * 6.213350772857666
Epoch 2830, val loss: 1.6622631549835205
Epoch 2840, training loss: 62.135623931884766 = 0.006420685909688473 + 10.0 * 6.212920188903809
Epoch 2840, val loss: 1.6641448736190796
Epoch 2850, training loss: 62.133785247802734 = 0.006365817040205002 + 10.0 * 6.212741851806641
Epoch 2850, val loss: 1.6658207178115845
Epoch 2860, training loss: 62.11719512939453 = 0.00631427438929677 + 10.0 * 6.211088180541992
Epoch 2860, val loss: 1.6678898334503174
Epoch 2870, training loss: 62.111351013183594 = 0.0062652332708239555 + 10.0 * 6.210508823394775
Epoch 2870, val loss: 1.6694285869598389
Epoch 2880, training loss: 62.14288330078125 = 0.006217672023922205 + 10.0 * 6.2136664390563965
Epoch 2880, val loss: 1.6710697412490845
Epoch 2890, training loss: 62.11874008178711 = 0.006165101192891598 + 10.0 * 6.211257457733154
Epoch 2890, val loss: 1.6734304428100586
Epoch 2900, training loss: 62.163612365722656 = 0.006116214673966169 + 10.0 * 6.215749740600586
Epoch 2900, val loss: 1.6741119623184204
Epoch 2910, training loss: 62.13248825073242 = 0.006064833607524633 + 10.0 * 6.212642192840576
Epoch 2910, val loss: 1.6779165267944336
Epoch 2920, training loss: 62.10834884643555 = 0.006015399936586618 + 10.0 * 6.210233211517334
Epoch 2920, val loss: 1.6782422065734863
Epoch 2930, training loss: 62.107540130615234 = 0.00596995186060667 + 10.0 * 6.2101569175720215
Epoch 2930, val loss: 1.6810011863708496
Epoch 2940, training loss: 62.16220474243164 = 0.00592429656535387 + 10.0 * 6.215628147125244
Epoch 2940, val loss: 1.682149052619934
Epoch 2950, training loss: 62.11155319213867 = 0.005877993535250425 + 10.0 * 6.210567474365234
Epoch 2950, val loss: 1.6842821836471558
Epoch 2960, training loss: 62.09465026855469 = 0.005831991322338581 + 10.0 * 6.208881855010986
Epoch 2960, val loss: 1.685891032218933
Epoch 2970, training loss: 62.095481872558594 = 0.005788025911897421 + 10.0 * 6.208969593048096
Epoch 2970, val loss: 1.6876723766326904
Epoch 2980, training loss: 62.100223541259766 = 0.005747286137193441 + 10.0 * 6.209447383880615
Epoch 2980, val loss: 1.6895785331726074
Epoch 2990, training loss: 62.13032531738281 = 0.005705484189093113 + 10.0 * 6.212461948394775
Epoch 2990, val loss: 1.6910128593444824
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8376383763837639
The final CL Acc:0.76420, 0.01145, The final GNN Acc:0.83869, 0.00269
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9450])
updated graph: torch.Size([2, 10510])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.92134094238281 = 1.9528169631958008 + 10.0 * 8.59685230255127
Epoch 0, val loss: 1.9478275775909424
Epoch 10, training loss: 87.9059066772461 = 1.9421778917312622 + 10.0 * 8.596372604370117
Epoch 10, val loss: 1.9376013278961182
Epoch 20, training loss: 87.85742950439453 = 1.9290857315063477 + 10.0 * 8.59283447265625
Epoch 20, val loss: 1.924682855606079
Epoch 30, training loss: 87.59107208251953 = 1.9121980667114258 + 10.0 * 8.567887306213379
Epoch 30, val loss: 1.907922625541687
Epoch 40, training loss: 86.23150634765625 = 1.8920680284500122 + 10.0 * 8.433943748474121
Epoch 40, val loss: 1.8883684873580933
Epoch 50, training loss: 81.72947692871094 = 1.8708248138427734 + 10.0 * 7.985865116119385
Epoch 50, val loss: 1.8674931526184082
Epoch 60, training loss: 77.38516235351562 = 1.8539881706237793 + 10.0 * 7.553117752075195
Epoch 60, val loss: 1.8529423475265503
Epoch 70, training loss: 73.16509246826172 = 1.8445550203323364 + 10.0 * 7.132053375244141
Epoch 70, val loss: 1.8442769050598145
Epoch 80, training loss: 71.00240325927734 = 1.8352471590042114 + 10.0 * 6.916715145111084
Epoch 80, val loss: 1.835709571838379
Epoch 90, training loss: 69.86400604248047 = 1.8253141641616821 + 10.0 * 6.803868770599365
Epoch 90, val loss: 1.8264679908752441
Epoch 100, training loss: 69.14443969726562 = 1.814968466758728 + 10.0 * 6.732946872711182
Epoch 100, val loss: 1.8171579837799072
Epoch 110, training loss: 68.68051147460938 = 1.8055148124694824 + 10.0 * 6.6875
Epoch 110, val loss: 1.8085049390792847
Epoch 120, training loss: 68.27980041503906 = 1.796732783317566 + 10.0 * 6.6483073234558105
Epoch 120, val loss: 1.8004403114318848
Epoch 130, training loss: 67.96003723144531 = 1.7884076833724976 + 10.0 * 6.617162704467773
Epoch 130, val loss: 1.7927433252334595
Epoch 140, training loss: 67.6635971069336 = 1.7803343534469604 + 10.0 * 6.5883259773254395
Epoch 140, val loss: 1.7852534055709839
Epoch 150, training loss: 67.38880157470703 = 1.7720835208892822 + 10.0 * 6.561671733856201
Epoch 150, val loss: 1.7777928113937378
Epoch 160, training loss: 67.16010284423828 = 1.7633676528930664 + 10.0 * 6.539673805236816
Epoch 160, val loss: 1.7700774669647217
Epoch 170, training loss: 66.95613861083984 = 1.7540276050567627 + 10.0 * 6.5202107429504395
Epoch 170, val loss: 1.7619738578796387
Epoch 180, training loss: 66.77547454833984 = 1.74379563331604 + 10.0 * 6.503168106079102
Epoch 180, val loss: 1.7532578706741333
Epoch 190, training loss: 66.60989379882812 = 1.7326258420944214 + 10.0 * 6.48772668838501
Epoch 190, val loss: 1.7438290119171143
Epoch 200, training loss: 66.48014068603516 = 1.7203965187072754 + 10.0 * 6.4759745597839355
Epoch 200, val loss: 1.7336392402648926
Epoch 210, training loss: 66.30890655517578 = 1.707013726234436 + 10.0 * 6.460188865661621
Epoch 210, val loss: 1.7224382162094116
Epoch 220, training loss: 66.1741714477539 = 1.6924161911010742 + 10.0 * 6.44817590713501
Epoch 220, val loss: 1.7102922201156616
Epoch 230, training loss: 66.06197357177734 = 1.6765170097351074 + 10.0 * 6.4385457038879395
Epoch 230, val loss: 1.6970561742782593
Epoch 240, training loss: 65.94708251953125 = 1.6590924263000488 + 10.0 * 6.428798675537109
Epoch 240, val loss: 1.6826664209365845
Epoch 250, training loss: 65.85386657714844 = 1.6402394771575928 + 10.0 * 6.42136287689209
Epoch 250, val loss: 1.6670598983764648
Epoch 260, training loss: 65.73818969726562 = 1.6199170351028442 + 10.0 * 6.411827087402344
Epoch 260, val loss: 1.65022611618042
Epoch 270, training loss: 65.64244079589844 = 1.5981040000915527 + 10.0 * 6.404433727264404
Epoch 270, val loss: 1.6322120428085327
Epoch 280, training loss: 65.61326599121094 = 1.5747853517532349 + 10.0 * 6.403848171234131
Epoch 280, val loss: 1.6129889488220215
Epoch 290, training loss: 65.47154998779297 = 1.5501136779785156 + 10.0 * 6.392143726348877
Epoch 290, val loss: 1.59266996383667
Epoch 300, training loss: 65.38157653808594 = 1.5243021249771118 + 10.0 * 6.385727405548096
Epoch 300, val loss: 1.5716677904129028
Epoch 310, training loss: 65.2943115234375 = 1.4973770380020142 + 10.0 * 6.379693508148193
Epoch 310, val loss: 1.5498433113098145
Epoch 320, training loss: 65.27587127685547 = 1.4693703651428223 + 10.0 * 6.380650043487549
Epoch 320, val loss: 1.5274218320846558
Epoch 330, training loss: 65.13941955566406 = 1.4406381845474243 + 10.0 * 6.36987829208374
Epoch 330, val loss: 1.5042450428009033
Epoch 340, training loss: 65.0670394897461 = 1.4113167524337769 + 10.0 * 6.365572452545166
Epoch 340, val loss: 1.4809232950210571
Epoch 350, training loss: 64.98866271972656 = 1.3816561698913574 + 10.0 * 6.360700607299805
Epoch 350, val loss: 1.4575518369674683
Epoch 360, training loss: 64.98572540283203 = 1.3517847061157227 + 10.0 * 6.363394260406494
Epoch 360, val loss: 1.434295892715454
Epoch 370, training loss: 64.86455535888672 = 1.3218443393707275 + 10.0 * 6.354271411895752
Epoch 370, val loss: 1.4111615419387817
Epoch 380, training loss: 64.81549835205078 = 1.2921454906463623 + 10.0 * 6.352334976196289
Epoch 380, val loss: 1.3884868621826172
Epoch 390, training loss: 64.72567749023438 = 1.2627919912338257 + 10.0 * 6.346288681030273
Epoch 390, val loss: 1.3662002086639404
Epoch 400, training loss: 64.65660858154297 = 1.233870506286621 + 10.0 * 6.342274188995361
Epoch 400, val loss: 1.3447675704956055
Epoch 410, training loss: 64.61119842529297 = 1.2054529190063477 + 10.0 * 6.340574741363525
Epoch 410, val loss: 1.3239221572875977
Epoch 420, training loss: 64.58442687988281 = 1.177545189857483 + 10.0 * 6.340688228607178
Epoch 420, val loss: 1.3038253784179688
Epoch 430, training loss: 64.48725128173828 = 1.15024733543396 + 10.0 * 6.333700180053711
Epoch 430, val loss: 1.2844747304916382
Epoch 440, training loss: 64.430419921875 = 1.1237263679504395 + 10.0 * 6.330668926239014
Epoch 440, val loss: 1.266148328781128
Epoch 450, training loss: 64.37886047363281 = 1.097959041595459 + 10.0 * 6.328090190887451
Epoch 450, val loss: 1.2486965656280518
Epoch 460, training loss: 64.34716796875 = 1.072895884513855 + 10.0 * 6.327426910400391
Epoch 460, val loss: 1.2319992780685425
Epoch 470, training loss: 64.30755615234375 = 1.0483816862106323 + 10.0 * 6.325917720794678
Epoch 470, val loss: 1.2166343927383423
Epoch 480, training loss: 64.2357177734375 = 1.0247255563735962 + 10.0 * 6.321099281311035
Epoch 480, val loss: 1.2015750408172607
Epoch 490, training loss: 64.19022369384766 = 1.0018055438995361 + 10.0 * 6.318841934204102
Epoch 490, val loss: 1.1876124143600464
Epoch 500, training loss: 64.19847106933594 = 0.9795657396316528 + 10.0 * 6.321890354156494
Epoch 500, val loss: 1.1746292114257812
Epoch 510, training loss: 64.14186096191406 = 0.9578914046287537 + 10.0 * 6.31839656829834
Epoch 510, val loss: 1.1624891757965088
Epoch 520, training loss: 64.07027435302734 = 0.9369415044784546 + 10.0 * 6.313333511352539
Epoch 520, val loss: 1.1509158611297607
Epoch 530, training loss: 64.02849578857422 = 0.9166927933692932 + 10.0 * 6.311180114746094
Epoch 530, val loss: 1.140289545059204
Epoch 540, training loss: 63.98808288574219 = 0.8970749974250793 + 10.0 * 6.30910062789917
Epoch 540, val loss: 1.1304616928100586
Epoch 550, training loss: 63.97776412963867 = 0.8779370784759521 + 10.0 * 6.309982776641846
Epoch 550, val loss: 1.1214498281478882
Epoch 560, training loss: 63.95759963989258 = 0.85921710729599 + 10.0 * 6.30983829498291
Epoch 560, val loss: 1.1127914190292358
Epoch 570, training loss: 63.903079986572266 = 0.8411445021629333 + 10.0 * 6.3061933517456055
Epoch 570, val loss: 1.1046971082687378
Epoch 580, training loss: 63.843746185302734 = 0.8234794735908508 + 10.0 * 6.302026748657227
Epoch 580, val loss: 1.0975221395492554
Epoch 590, training loss: 63.81462860107422 = 0.8063399791717529 + 10.0 * 6.30082893371582
Epoch 590, val loss: 1.09084951877594
Epoch 600, training loss: 63.83174514770508 = 0.7895861268043518 + 10.0 * 6.304215908050537
Epoch 600, val loss: 1.0848077535629272
Epoch 610, training loss: 63.76374053955078 = 0.7731465101242065 + 10.0 * 6.2990593910217285
Epoch 610, val loss: 1.0786335468292236
Epoch 620, training loss: 63.734100341796875 = 0.7571291923522949 + 10.0 * 6.297697067260742
Epoch 620, val loss: 1.07331383228302
Epoch 630, training loss: 63.6887321472168 = 0.7414973378181458 + 10.0 * 6.2947235107421875
Epoch 630, val loss: 1.068712592124939
Epoch 640, training loss: 63.69108200073242 = 0.7261982560157776 + 10.0 * 6.296488285064697
Epoch 640, val loss: 1.064331293106079
Epoch 650, training loss: 63.63112258911133 = 0.7111295461654663 + 10.0 * 6.291999340057373
Epoch 650, val loss: 1.060572862625122
Epoch 660, training loss: 63.630393981933594 = 0.6963987946510315 + 10.0 * 6.293399333953857
Epoch 660, val loss: 1.057157039642334
Epoch 670, training loss: 63.57555389404297 = 0.6819893717765808 + 10.0 * 6.289356708526611
Epoch 670, val loss: 1.053686261177063
Epoch 680, training loss: 63.54566955566406 = 0.6678287386894226 + 10.0 * 6.287784099578857
Epoch 680, val loss: 1.0510133504867554
Epoch 690, training loss: 63.56517028808594 = 0.6538851261138916 + 10.0 * 6.291128635406494
Epoch 690, val loss: 1.0486204624176025
Epoch 700, training loss: 63.514076232910156 = 0.6400116682052612 + 10.0 * 6.2874064445495605
Epoch 700, val loss: 1.0466450452804565
Epoch 710, training loss: 63.463558197021484 = 0.6264959573745728 + 10.0 * 6.283706188201904
Epoch 710, val loss: 1.044785737991333
Epoch 720, training loss: 63.440162658691406 = 0.6131349205970764 + 10.0 * 6.282702445983887
Epoch 720, val loss: 1.0435329675674438
Epoch 730, training loss: 63.44316864013672 = 0.5999060273170471 + 10.0 * 6.284326076507568
Epoch 730, val loss: 1.0421496629714966
Epoch 740, training loss: 63.42046356201172 = 0.5867999792098999 + 10.0 * 6.2833662033081055
Epoch 740, val loss: 1.0412997007369995
Epoch 750, training loss: 63.364356994628906 = 0.5738872289657593 + 10.0 * 6.279047012329102
Epoch 750, val loss: 1.0404586791992188
Epoch 760, training loss: 63.336734771728516 = 0.561184823513031 + 10.0 * 6.277554988861084
Epoch 760, val loss: 1.0399105548858643
Epoch 770, training loss: 63.36747360229492 = 0.5485979318618774 + 10.0 * 6.281887531280518
Epoch 770, val loss: 1.0399221181869507
Epoch 780, training loss: 63.3154411315918 = 0.5360769629478455 + 10.0 * 6.2779364585876465
Epoch 780, val loss: 1.039052128791809
Epoch 790, training loss: 63.27664566040039 = 0.5236685872077942 + 10.0 * 6.27529764175415
Epoch 790, val loss: 1.0395535230636597
Epoch 800, training loss: 63.25208282470703 = 0.5114746689796448 + 10.0 * 6.2740607261657715
Epoch 800, val loss: 1.0396175384521484
Epoch 810, training loss: 63.256500244140625 = 0.4993375241756439 + 10.0 * 6.275716304779053
Epoch 810, val loss: 1.0397764444351196
Epoch 820, training loss: 63.20222473144531 = 0.48727333545684814 + 10.0 * 6.2714948654174805
Epoch 820, val loss: 1.040444016456604
Epoch 830, training loss: 63.18717575073242 = 0.4754083454608917 + 10.0 * 6.271176815032959
Epoch 830, val loss: 1.0408761501312256
Epoch 840, training loss: 63.15626525878906 = 0.4637186825275421 + 10.0 * 6.269254684448242
Epoch 840, val loss: 1.0418442487716675
Epoch 850, training loss: 63.21025085449219 = 0.4521416425704956 + 10.0 * 6.275811195373535
Epoch 850, val loss: 1.0429162979125977
Epoch 860, training loss: 63.14717102050781 = 0.4407106041908264 + 10.0 * 6.270646095275879
Epoch 860, val loss: 1.043518304824829
Epoch 870, training loss: 63.09528732299805 = 0.4294205605983734 + 10.0 * 6.266586780548096
Epoch 870, val loss: 1.0446642637252808
Epoch 880, training loss: 63.080562591552734 = 0.4183460474014282 + 10.0 * 6.266221523284912
Epoch 880, val loss: 1.0462381839752197
Epoch 890, training loss: 63.09394073486328 = 0.40740275382995605 + 10.0 * 6.268653869628906
Epoch 890, val loss: 1.0476278066635132
Epoch 900, training loss: 63.04346466064453 = 0.39661291241645813 + 10.0 * 6.264685153961182
Epoch 900, val loss: 1.0486273765563965
Epoch 910, training loss: 63.02210998535156 = 0.3860400915145874 + 10.0 * 6.263607025146484
Epoch 910, val loss: 1.0503634214401245
Epoch 920, training loss: 63.01167297363281 = 0.37566110491752625 + 10.0 * 6.263601303100586
Epoch 920, val loss: 1.051966905593872
Epoch 930, training loss: 63.01424026489258 = 0.3654065728187561 + 10.0 * 6.264883518218994
Epoch 930, val loss: 1.054039716720581
Epoch 940, training loss: 62.96521759033203 = 0.35531553626060486 + 10.0 * 6.260990142822266
Epoch 940, val loss: 1.0558513402938843
Epoch 950, training loss: 62.951969146728516 = 0.34548699855804443 + 10.0 * 6.260648250579834
Epoch 950, val loss: 1.058031439781189
Epoch 960, training loss: 62.94981384277344 = 0.3358323574066162 + 10.0 * 6.2613983154296875
Epoch 960, val loss: 1.0600404739379883
Epoch 970, training loss: 62.91496276855469 = 0.3263663649559021 + 10.0 * 6.258859634399414
Epoch 970, val loss: 1.0621775388717651
Epoch 980, training loss: 62.90336227416992 = 0.31711307168006897 + 10.0 * 6.258625030517578
Epoch 980, val loss: 1.0643348693847656
Epoch 990, training loss: 62.92088317871094 = 0.3080271780490875 + 10.0 * 6.261285781860352
Epoch 990, val loss: 1.0669348239898682
Epoch 1000, training loss: 62.88496398925781 = 0.2991369366645813 + 10.0 * 6.258582592010498
Epoch 1000, val loss: 1.0688872337341309
Epoch 1010, training loss: 62.849510192871094 = 0.29042625427246094 + 10.0 * 6.255908489227295
Epoch 1010, val loss: 1.0713719129562378
Epoch 1020, training loss: 62.83809280395508 = 0.2819826006889343 + 10.0 * 6.255610942840576
Epoch 1020, val loss: 1.0737090110778809
Epoch 1030, training loss: 62.87709426879883 = 0.27369460463523865 + 10.0 * 6.260340213775635
Epoch 1030, val loss: 1.0764752626419067
Epoch 1040, training loss: 62.808372497558594 = 0.26556262373924255 + 10.0 * 6.254281044006348
Epoch 1040, val loss: 1.078883171081543
Epoch 1050, training loss: 62.791725158691406 = 0.25769680738449097 + 10.0 * 6.2534027099609375
Epoch 1050, val loss: 1.0816988945007324
Epoch 1060, training loss: 62.77790832519531 = 0.2500474750995636 + 10.0 * 6.252786159515381
Epoch 1060, val loss: 1.0844089984893799
Epoch 1070, training loss: 62.837432861328125 = 0.24256877601146698 + 10.0 * 6.259486198425293
Epoch 1070, val loss: 1.087003231048584
Epoch 1080, training loss: 62.779449462890625 = 0.23523801565170288 + 10.0 * 6.254421234130859
Epoch 1080, val loss: 1.0906323194503784
Epoch 1090, training loss: 62.80239486694336 = 0.22812533378601074 + 10.0 * 6.257426738739014
Epoch 1090, val loss: 1.0935876369476318
Epoch 1100, training loss: 62.73760223388672 = 0.2212541401386261 + 10.0 * 6.2516350746154785
Epoch 1100, val loss: 1.0960487127304077
Epoch 1110, training loss: 62.71602249145508 = 0.21455317735671997 + 10.0 * 6.250146865844727
Epoch 1110, val loss: 1.0998529195785522
Epoch 1120, training loss: 62.69918441772461 = 0.20808708667755127 + 10.0 * 6.249109745025635
Epoch 1120, val loss: 1.1029778718948364
Epoch 1130, training loss: 62.7515869140625 = 0.20178170502185822 + 10.0 * 6.254980564117432
Epoch 1130, val loss: 1.1063998937606812
Epoch 1140, training loss: 62.7004508972168 = 0.1956382840871811 + 10.0 * 6.250481605529785
Epoch 1140, val loss: 1.1099681854248047
Epoch 1150, training loss: 62.6757698059082 = 0.1896734982728958 + 10.0 * 6.24860954284668
Epoch 1150, val loss: 1.113203763961792
Epoch 1160, training loss: 62.67898941040039 = 0.18394172191619873 + 10.0 * 6.249505043029785
Epoch 1160, val loss: 1.1171764135360718
Epoch 1170, training loss: 62.644100189208984 = 0.1783471405506134 + 10.0 * 6.246575355529785
Epoch 1170, val loss: 1.120551347732544
Epoch 1180, training loss: 62.63274383544922 = 0.17294548451900482 + 10.0 * 6.2459797859191895
Epoch 1180, val loss: 1.1242666244506836
Epoch 1190, training loss: 62.665489196777344 = 0.1677364856004715 + 10.0 * 6.249775409698486
Epoch 1190, val loss: 1.12814462184906
Epoch 1200, training loss: 62.61689758300781 = 0.16263003647327423 + 10.0 * 6.245427131652832
Epoch 1200, val loss: 1.1318427324295044
Epoch 1210, training loss: 62.60361862182617 = 0.15772800147533417 + 10.0 * 6.244589328765869
Epoch 1210, val loss: 1.1357938051223755
Epoch 1220, training loss: 62.60150909423828 = 0.15299265086650848 + 10.0 * 6.244851589202881
Epoch 1220, val loss: 1.1397734880447388
Epoch 1230, training loss: 62.62050247192383 = 0.14840568602085114 + 10.0 * 6.247209548950195
Epoch 1230, val loss: 1.143837809562683
Epoch 1240, training loss: 62.581668853759766 = 0.1439298838376999 + 10.0 * 6.243773937225342
Epoch 1240, val loss: 1.148269772529602
Epoch 1250, training loss: 62.56822204589844 = 0.13965463638305664 + 10.0 * 6.242856502532959
Epoch 1250, val loss: 1.1524138450622559
Epoch 1260, training loss: 62.602622985839844 = 0.13552947342395782 + 10.0 * 6.24670934677124
Epoch 1260, val loss: 1.1570249795913696
Epoch 1270, training loss: 62.56083297729492 = 0.13148511946201324 + 10.0 * 6.242934703826904
Epoch 1270, val loss: 1.1609874963760376
Epoch 1280, training loss: 62.56993865966797 = 0.12761367857456207 + 10.0 * 6.244232654571533
Epoch 1280, val loss: 1.1657483577728271
Epoch 1290, training loss: 62.54257583618164 = 0.1238643229007721 + 10.0 * 6.241871356964111
Epoch 1290, val loss: 1.169610857963562
Epoch 1300, training loss: 62.52861404418945 = 0.1202588602900505 + 10.0 * 6.240835666656494
Epoch 1300, val loss: 1.1743552684783936
Epoch 1310, training loss: 62.51899337768555 = 0.11678062379360199 + 10.0 * 6.2402215003967285
Epoch 1310, val loss: 1.1787333488464355
Epoch 1320, training loss: 62.54487228393555 = 0.11342208832502365 + 10.0 * 6.243144989013672
Epoch 1320, val loss: 1.183308720588684
Epoch 1330, training loss: 62.51292419433594 = 0.11015378683805466 + 10.0 * 6.240277290344238
Epoch 1330, val loss: 1.188202142715454
Epoch 1340, training loss: 62.526676177978516 = 0.10699521005153656 + 10.0 * 6.241968154907227
Epoch 1340, val loss: 1.192780613899231
Epoch 1350, training loss: 62.503841400146484 = 0.10393811017274857 + 10.0 * 6.239990234375
Epoch 1350, val loss: 1.1975740194320679
Epoch 1360, training loss: 62.50004577636719 = 0.10098957270383835 + 10.0 * 6.23990535736084
Epoch 1360, val loss: 1.2017265558242798
Epoch 1370, training loss: 62.48017120361328 = 0.0981810986995697 + 10.0 * 6.238198757171631
Epoch 1370, val loss: 1.2069439888000488
Epoch 1380, training loss: 62.47025680541992 = 0.09546130895614624 + 10.0 * 6.237479209899902
Epoch 1380, val loss: 1.2115970849990845
Epoch 1390, training loss: 62.480934143066406 = 0.09284002333879471 + 10.0 * 6.238809585571289
Epoch 1390, val loss: 1.2166017293930054
Epoch 1400, training loss: 62.473079681396484 = 0.09028329700231552 + 10.0 * 6.238279819488525
Epoch 1400, val loss: 1.2213935852050781
Epoch 1410, training loss: 62.48250198364258 = 0.08780952543020248 + 10.0 * 6.239469051361084
Epoch 1410, val loss: 1.2257452011108398
Epoch 1420, training loss: 62.456443786621094 = 0.08541827648878098 + 10.0 * 6.237102508544922
Epoch 1420, val loss: 1.2305071353912354
Epoch 1430, training loss: 62.441131591796875 = 0.0831206887960434 + 10.0 * 6.235800743103027
Epoch 1430, val loss: 1.2356700897216797
Epoch 1440, training loss: 62.43708419799805 = 0.08091730624437332 + 10.0 * 6.235616683959961
Epoch 1440, val loss: 1.240341067314148
Epoch 1450, training loss: 62.47028732299805 = 0.0787786990404129 + 10.0 * 6.2391510009765625
Epoch 1450, val loss: 1.2451567649841309
Epoch 1460, training loss: 62.46171188354492 = 0.07669214904308319 + 10.0 * 6.238502025604248
Epoch 1460, val loss: 1.2503567934036255
Epoch 1470, training loss: 62.421268463134766 = 0.07468872517347336 + 10.0 * 6.234658241271973
Epoch 1470, val loss: 1.2546892166137695
Epoch 1480, training loss: 62.41262435913086 = 0.07275932282209396 + 10.0 * 6.233986854553223
Epoch 1480, val loss: 1.259416103363037
Epoch 1490, training loss: 62.418785095214844 = 0.07089801877737045 + 10.0 * 6.23478889465332
Epoch 1490, val loss: 1.2643580436706543
Epoch 1500, training loss: 62.413944244384766 = 0.06908394396305084 + 10.0 * 6.234486103057861
Epoch 1500, val loss: 1.2693718671798706
Epoch 1510, training loss: 62.396541595458984 = 0.06734205782413483 + 10.0 * 6.232920169830322
Epoch 1510, val loss: 1.2740581035614014
Epoch 1520, training loss: 62.39069366455078 = 0.06565631926059723 + 10.0 * 6.232503890991211
Epoch 1520, val loss: 1.279036045074463
Epoch 1530, training loss: 62.45867919921875 = 0.06404425948858261 + 10.0 * 6.2394633293151855
Epoch 1530, val loss: 1.2839597463607788
Epoch 1540, training loss: 62.408138275146484 = 0.06243696063756943 + 10.0 * 6.234570503234863
Epoch 1540, val loss: 1.2876169681549072
Epoch 1550, training loss: 62.414329528808594 = 0.060911547392606735 + 10.0 * 6.235341548919678
Epoch 1550, val loss: 1.293021559715271
Epoch 1560, training loss: 62.372833251953125 = 0.059431299567222595 + 10.0 * 6.231339931488037
Epoch 1560, val loss: 1.2974929809570312
Epoch 1570, training loss: 62.363590240478516 = 0.05801592022180557 + 10.0 * 6.230557441711426
Epoch 1570, val loss: 1.302427887916565
Epoch 1580, training loss: 62.39200210571289 = 0.0566418394446373 + 10.0 * 6.233536243438721
Epoch 1580, val loss: 1.3073903322219849
Epoch 1590, training loss: 62.357418060302734 = 0.05530188977718353 + 10.0 * 6.2302117347717285
Epoch 1590, val loss: 1.3111591339111328
Epoch 1600, training loss: 62.37290954589844 = 0.05399962142109871 + 10.0 * 6.231890678405762
Epoch 1600, val loss: 1.316038727760315
Epoch 1610, training loss: 62.34965515136719 = 0.052747663110494614 + 10.0 * 6.229691028594971
Epoch 1610, val loss: 1.3203949928283691
Epoch 1620, training loss: 62.34523391723633 = 0.05153758078813553 + 10.0 * 6.229369640350342
Epoch 1620, val loss: 1.3253384828567505
Epoch 1630, training loss: 62.3541374206543 = 0.050372567027807236 + 10.0 * 6.230376243591309
Epoch 1630, val loss: 1.329755425453186
Epoch 1640, training loss: 62.379817962646484 = 0.04923652112483978 + 10.0 * 6.233057975769043
Epoch 1640, val loss: 1.33451509475708
Epoch 1650, training loss: 62.356327056884766 = 0.04812237247824669 + 10.0 * 6.230820655822754
Epoch 1650, val loss: 1.3383904695510864
Epoch 1660, training loss: 62.35108947753906 = 0.04704798758029938 + 10.0 * 6.230404376983643
Epoch 1660, val loss: 1.3429604768753052
Epoch 1670, training loss: 62.327049255371094 = 0.04601205512881279 + 10.0 * 6.2281036376953125
Epoch 1670, val loss: 1.3476285934448242
Epoch 1680, training loss: 62.316898345947266 = 0.0450124591588974 + 10.0 * 6.227188587188721
Epoch 1680, val loss: 1.3521766662597656
Epoch 1690, training loss: 62.345924377441406 = 0.044045738875865936 + 10.0 * 6.230187892913818
Epoch 1690, val loss: 1.3568799495697021
Epoch 1700, training loss: 62.36921310424805 = 0.043089840561151505 + 10.0 * 6.232612133026123
Epoch 1700, val loss: 1.3609697818756104
Epoch 1710, training loss: 62.32741928100586 = 0.042155612260103226 + 10.0 * 6.2285261154174805
Epoch 1710, val loss: 1.365049958229065
Epoch 1720, training loss: 62.30648422241211 = 0.0412617065012455 + 10.0 * 6.226521968841553
Epoch 1720, val loss: 1.3694674968719482
Epoch 1730, training loss: 62.29711151123047 = 0.04040795564651489 + 10.0 * 6.225670337677002
Epoch 1730, val loss: 1.3736149072647095
Epoch 1740, training loss: 62.317012786865234 = 0.039578698575496674 + 10.0 * 6.227743625640869
Epoch 1740, val loss: 1.3778570890426636
Epoch 1750, training loss: 62.303558349609375 = 0.03875609487295151 + 10.0 * 6.226480007171631
Epoch 1750, val loss: 1.382256269454956
Epoch 1760, training loss: 62.3001823425293 = 0.03796093538403511 + 10.0 * 6.226222038269043
Epoch 1760, val loss: 1.3865824937820435
Epoch 1770, training loss: 62.30274963378906 = 0.037195321172475815 + 10.0 * 6.226555347442627
Epoch 1770, val loss: 1.390443205833435
Epoch 1780, training loss: 62.30991744995117 = 0.03644849732518196 + 10.0 * 6.227346897125244
Epoch 1780, val loss: 1.3949593305587769
Epoch 1790, training loss: 62.33473587036133 = 0.035730570554733276 + 10.0 * 6.229900360107422
Epoch 1790, val loss: 1.3989304304122925
Epoch 1800, training loss: 62.28617858886719 = 0.03500673547387123 + 10.0 * 6.225117206573486
Epoch 1800, val loss: 1.403195858001709
Epoch 1810, training loss: 62.272544860839844 = 0.03432923182845116 + 10.0 * 6.223821640014648
Epoch 1810, val loss: 1.4072595834732056
Epoch 1820, training loss: 62.28084945678711 = 0.033670514822006226 + 10.0 * 6.224717617034912
Epoch 1820, val loss: 1.4115639925003052
Epoch 1830, training loss: 62.285884857177734 = 0.03302039951086044 + 10.0 * 6.225286483764648
Epoch 1830, val loss: 1.4153432846069336
Epoch 1840, training loss: 62.2853889465332 = 0.03238644823431969 + 10.0 * 6.225300312042236
Epoch 1840, val loss: 1.4188696146011353
Epoch 1850, training loss: 62.27903366088867 = 0.03177139535546303 + 10.0 * 6.22472620010376
Epoch 1850, val loss: 1.4238250255584717
Epoch 1860, training loss: 62.31745529174805 = 0.031173773109912872 + 10.0 * 6.228628158569336
Epoch 1860, val loss: 1.4279611110687256
Epoch 1870, training loss: 62.26250457763672 = 0.03057858906686306 + 10.0 * 6.2231926918029785
Epoch 1870, val loss: 1.4307693243026733
Epoch 1880, training loss: 62.25312805175781 = 0.030012888833880424 + 10.0 * 6.222311496734619
Epoch 1880, val loss: 1.4355496168136597
Epoch 1890, training loss: 62.24441909790039 = 0.029466772451996803 + 10.0 * 6.221495151519775
Epoch 1890, val loss: 1.4392591714859009
Epoch 1900, training loss: 62.241729736328125 = 0.028938522562384605 + 10.0 * 6.221279144287109
Epoch 1900, val loss: 1.4430054426193237
Epoch 1910, training loss: 62.31086730957031 = 0.02842751145362854 + 10.0 * 6.228243827819824
Epoch 1910, val loss: 1.4470012187957764
Epoch 1920, training loss: 62.263885498046875 = 0.027897872030735016 + 10.0 * 6.223598957061768
Epoch 1920, val loss: 1.4507368803024292
Epoch 1930, training loss: 62.25159454345703 = 0.027396829798817635 + 10.0 * 6.222419738769531
Epoch 1930, val loss: 1.4544061422348022
Epoch 1940, training loss: 62.24203872680664 = 0.026914751157164574 + 10.0 * 6.221512317657471
Epoch 1940, val loss: 1.4581817388534546
Epoch 1950, training loss: 62.29558181762695 = 0.0264471173286438 + 10.0 * 6.2269134521484375
Epoch 1950, val loss: 1.461733102798462
Epoch 1960, training loss: 62.25145721435547 = 0.02597982995212078 + 10.0 * 6.222548007965088
Epoch 1960, val loss: 1.465856909751892
Epoch 1970, training loss: 62.23039627075195 = 0.02553091198205948 + 10.0 * 6.220486640930176
Epoch 1970, val loss: 1.4693467617034912
Epoch 1980, training loss: 62.22911071777344 = 0.02510194480419159 + 10.0 * 6.220400810241699
Epoch 1980, val loss: 1.473402738571167
Epoch 1990, training loss: 62.27580642700195 = 0.024680504575371742 + 10.0 * 6.225112438201904
Epoch 1990, val loss: 1.4769264459609985
Epoch 2000, training loss: 62.24699020385742 = 0.024257130920886993 + 10.0 * 6.222273349761963
Epoch 2000, val loss: 1.4803014993667603
Epoch 2010, training loss: 62.24016189575195 = 0.023851634934544563 + 10.0 * 6.221631050109863
Epoch 2010, val loss: 1.4838300943374634
Epoch 2020, training loss: 62.211952209472656 = 0.023451508954167366 + 10.0 * 6.218850135803223
Epoch 2020, val loss: 1.4876282215118408
Epoch 2030, training loss: 62.21527099609375 = 0.023070314899086952 + 10.0 * 6.219220161437988
Epoch 2030, val loss: 1.4914354085922241
Epoch 2040, training loss: 62.2529182434082 = 0.022695232182741165 + 10.0 * 6.2230224609375
Epoch 2040, val loss: 1.4945383071899414
Epoch 2050, training loss: 62.244022369384766 = 0.022328844293951988 + 10.0 * 6.222169399261475
Epoch 2050, val loss: 1.4978455305099487
Epoch 2060, training loss: 62.20671081542969 = 0.021960599347949028 + 10.0 * 6.218474864959717
Epoch 2060, val loss: 1.502129316329956
Epoch 2070, training loss: 62.209354400634766 = 0.021617406979203224 + 10.0 * 6.21877384185791
Epoch 2070, val loss: 1.505536675453186
Epoch 2080, training loss: 62.2706184387207 = 0.021272778511047363 + 10.0 * 6.2249345779418945
Epoch 2080, val loss: 1.5092543363571167
Epoch 2090, training loss: 62.214962005615234 = 0.020931875333189964 + 10.0 * 6.219403266906738
Epoch 2090, val loss: 1.5114942789077759
Epoch 2100, training loss: 62.19736862182617 = 0.020603111013770103 + 10.0 * 6.217676639556885
Epoch 2100, val loss: 1.5158350467681885
Epoch 2110, training loss: 62.19247817993164 = 0.020288629457354546 + 10.0 * 6.21721887588501
Epoch 2110, val loss: 1.5186926126480103
Epoch 2120, training loss: 62.22798156738281 = 0.019979191944003105 + 10.0 * 6.220800399780273
Epoch 2120, val loss: 1.5224536657333374
Epoch 2130, training loss: 62.1965217590332 = 0.01967153698205948 + 10.0 * 6.217684745788574
Epoch 2130, val loss: 1.5256538391113281
Epoch 2140, training loss: 62.20903778076172 = 0.01937241666018963 + 10.0 * 6.218966484069824
Epoch 2140, val loss: 1.5291383266448975
Epoch 2150, training loss: 62.209720611572266 = 0.019077835604548454 + 10.0 * 6.219064235687256
Epoch 2150, val loss: 1.531467080116272
Epoch 2160, training loss: 62.18450927734375 = 0.018791021779179573 + 10.0 * 6.216571807861328
Epoch 2160, val loss: 1.5357754230499268
Epoch 2170, training loss: 62.177127838134766 = 0.018513737246394157 + 10.0 * 6.2158613204956055
Epoch 2170, val loss: 1.5390251874923706
Epoch 2180, training loss: 62.19163513183594 = 0.01824532449245453 + 10.0 * 6.217339038848877
Epoch 2180, val loss: 1.542542815208435
Epoch 2190, training loss: 62.197731018066406 = 0.0179778803139925 + 10.0 * 6.21797513961792
Epoch 2190, val loss: 1.5451709032058716
Epoch 2200, training loss: 62.17862319946289 = 0.017713481560349464 + 10.0 * 6.216091156005859
Epoch 2200, val loss: 1.5482661724090576
Epoch 2210, training loss: 62.164466857910156 = 0.017458830028772354 + 10.0 * 6.214700698852539
Epoch 2210, val loss: 1.5520449876785278
Epoch 2220, training loss: 62.171478271484375 = 0.01721365749835968 + 10.0 * 6.215426445007324
Epoch 2220, val loss: 1.5549116134643555
Epoch 2230, training loss: 62.21266555786133 = 0.016972580924630165 + 10.0 * 6.219569206237793
Epoch 2230, val loss: 1.558151125907898
Epoch 2240, training loss: 62.182373046875 = 0.016728296875953674 + 10.0 * 6.216564655303955
Epoch 2240, val loss: 1.5607435703277588
Epoch 2250, training loss: 62.181312561035156 = 0.01649242639541626 + 10.0 * 6.216482162475586
Epoch 2250, val loss: 1.564357876777649
Epoch 2260, training loss: 62.17732238769531 = 0.016261998564004898 + 10.0 * 6.216105937957764
Epoch 2260, val loss: 1.5674999952316284
Epoch 2270, training loss: 62.163360595703125 = 0.016033751890063286 + 10.0 * 6.214732646942139
Epoch 2270, val loss: 1.5706055164337158
Epoch 2280, training loss: 62.15673828125 = 0.015814680606126785 + 10.0 * 6.214092254638672
Epoch 2280, val loss: 1.5729753971099854
Epoch 2290, training loss: 62.14889144897461 = 0.015603596344590187 + 10.0 * 6.213328838348389
Epoch 2290, val loss: 1.5763134956359863
Epoch 2300, training loss: 62.15641784667969 = 0.015395622700452805 + 10.0 * 6.214102268218994
Epoch 2300, val loss: 1.5789732933044434
Epoch 2310, training loss: 62.19307327270508 = 0.015189239755272865 + 10.0 * 6.217788219451904
Epoch 2310, val loss: 1.582096815109253
Epoch 2320, training loss: 62.22518539428711 = 0.01498476229608059 + 10.0 * 6.221020221710205
Epoch 2320, val loss: 1.5850993394851685
Epoch 2330, training loss: 62.149044036865234 = 0.014776068739593029 + 10.0 * 6.2134270668029785
Epoch 2330, val loss: 1.5880563259124756
Epoch 2340, training loss: 62.14007568359375 = 0.014582186006009579 + 10.0 * 6.212549209594727
Epoch 2340, val loss: 1.5907176733016968
Epoch 2350, training loss: 62.13491439819336 = 0.014396735467016697 + 10.0 * 6.212051868438721
Epoch 2350, val loss: 1.5939997434616089
Epoch 2360, training loss: 62.134437561035156 = 0.014214481227099895 + 10.0 * 6.212022304534912
Epoch 2360, val loss: 1.5970549583435059
Epoch 2370, training loss: 62.20606994628906 = 0.01403747033327818 + 10.0 * 6.219202995300293
Epoch 2370, val loss: 1.600573182106018
Epoch 2380, training loss: 62.183589935302734 = 0.013848288916051388 + 10.0 * 6.216974258422852
Epoch 2380, val loss: 1.602162480354309
Epoch 2390, training loss: 62.156150817871094 = 0.013670059852302074 + 10.0 * 6.214247703552246
Epoch 2390, val loss: 1.6053334474563599
Epoch 2400, training loss: 62.13351058959961 = 0.013496659696102142 + 10.0 * 6.212001323699951
Epoch 2400, val loss: 1.607917308807373
Epoch 2410, training loss: 62.12350845336914 = 0.013330272398889065 + 10.0 * 6.211018085479736
Epoch 2410, val loss: 1.610913872718811
Epoch 2420, training loss: 62.13507843017578 = 0.013167657889425755 + 10.0 * 6.212191104888916
Epoch 2420, val loss: 1.6137921810150146
Epoch 2430, training loss: 62.17354965209961 = 0.013004499487578869 + 10.0 * 6.216054439544678
Epoch 2430, val loss: 1.6162863969802856
Epoch 2440, training loss: 62.14069747924805 = 0.01284173782914877 + 10.0 * 6.212785720825195
Epoch 2440, val loss: 1.6187667846679688
Epoch 2450, training loss: 62.11614227294922 = 0.012684262357652187 + 10.0 * 6.21034574508667
Epoch 2450, val loss: 1.621753454208374
Epoch 2460, training loss: 62.11416244506836 = 0.012534605339169502 + 10.0 * 6.21016263961792
Epoch 2460, val loss: 1.6243165731430054
Epoch 2470, training loss: 62.22392272949219 = 0.012390941381454468 + 10.0 * 6.221153259277344
Epoch 2470, val loss: 1.6269686222076416
Epoch 2480, training loss: 62.15280532836914 = 0.012231062166392803 + 10.0 * 6.214057445526123
Epoch 2480, val loss: 1.6295421123504639
Epoch 2490, training loss: 62.117549896240234 = 0.012086011469364166 + 10.0 * 6.210546493530273
Epoch 2490, val loss: 1.6318953037261963
Epoch 2500, training loss: 62.111724853515625 = 0.011945215985178947 + 10.0 * 6.209978103637695
Epoch 2500, val loss: 1.63460111618042
Epoch 2510, training loss: 62.15026092529297 = 0.011809567920863628 + 10.0 * 6.213845252990723
Epoch 2510, val loss: 1.6368598937988281
Epoch 2520, training loss: 62.1054573059082 = 0.011669019237160683 + 10.0 * 6.209378719329834
Epoch 2520, val loss: 1.640001654624939
Epoch 2530, training loss: 62.10988235473633 = 0.01153636910021305 + 10.0 * 6.209834575653076
Epoch 2530, val loss: 1.642780065536499
Epoch 2540, training loss: 62.13903045654297 = 0.01140657626092434 + 10.0 * 6.212762355804443
Epoch 2540, val loss: 1.644790768623352
Epoch 2550, training loss: 62.13153839111328 = 0.011272983625531197 + 10.0 * 6.212026596069336
Epoch 2550, val loss: 1.6480600833892822
Epoch 2560, training loss: 62.11394119262695 = 0.011144761927425861 + 10.0 * 6.21027946472168
Epoch 2560, val loss: 1.6499830484390259
Epoch 2570, training loss: 62.101661682128906 = 0.011018643155694008 + 10.0 * 6.209064483642578
Epoch 2570, val loss: 1.6530375480651855
Epoch 2580, training loss: 62.09430694580078 = 0.01089587714523077 + 10.0 * 6.208341121673584
Epoch 2580, val loss: 1.6552382707595825
Epoch 2590, training loss: 62.145328521728516 = 0.010778888128697872 + 10.0 * 6.213454723358154
Epoch 2590, val loss: 1.6582454442977905
Epoch 2600, training loss: 62.09768295288086 = 0.010655676946043968 + 10.0 * 6.20870304107666
Epoch 2600, val loss: 1.6599650382995605
Epoch 2610, training loss: 62.08832550048828 = 0.010535363107919693 + 10.0 * 6.2077789306640625
Epoch 2610, val loss: 1.6624293327331543
Epoch 2620, training loss: 62.0850830078125 = 0.01042267121374607 + 10.0 * 6.207466125488281
Epoch 2620, val loss: 1.6648547649383545
Epoch 2630, training loss: 62.087158203125 = 0.010313875041902065 + 10.0 * 6.207684516906738
Epoch 2630, val loss: 1.6671823263168335
Epoch 2640, training loss: 62.1391487121582 = 0.010206776671111584 + 10.0 * 6.212893962860107
Epoch 2640, val loss: 1.66898512840271
Epoch 2650, training loss: 62.1282958984375 = 0.010094382800161839 + 10.0 * 6.211820125579834
Epoch 2650, val loss: 1.672546148300171
Epoch 2660, training loss: 62.10102462768555 = 0.009983275085687637 + 10.0 * 6.209104061126709
Epoch 2660, val loss: 1.6740354299545288
Epoch 2670, training loss: 62.08203887939453 = 0.009878478944301605 + 10.0 * 6.207215785980225
Epoch 2670, val loss: 1.6768783330917358
Epoch 2680, training loss: 62.0789909362793 = 0.009778101928532124 + 10.0 * 6.206921577453613
Epoch 2680, val loss: 1.6787364482879639
Epoch 2690, training loss: 62.10292434692383 = 0.009678960777819157 + 10.0 * 6.209324836730957
Epoch 2690, val loss: 1.6811275482177734
Epoch 2700, training loss: 62.08252716064453 = 0.0095782820135355 + 10.0 * 6.207294940948486
Epoch 2700, val loss: 1.6839970350265503
Epoch 2710, training loss: 62.13146209716797 = 0.009481236338615417 + 10.0 * 6.212198257446289
Epoch 2710, val loss: 1.6858693361282349
Epoch 2720, training loss: 62.07653045654297 = 0.009379739873111248 + 10.0 * 6.206715106964111
Epoch 2720, val loss: 1.6878821849822998
Epoch 2730, training loss: 62.06752395629883 = 0.009284726344048977 + 10.0 * 6.20582389831543
Epoch 2730, val loss: 1.6903703212738037
Epoch 2740, training loss: 62.06854248046875 = 0.009193467907607555 + 10.0 * 6.205935001373291
Epoch 2740, val loss: 1.6929067373275757
Epoch 2750, training loss: 62.107032775878906 = 0.009105212055146694 + 10.0 * 6.209792613983154
Epoch 2750, val loss: 1.6952012777328491
Epoch 2760, training loss: 62.08218002319336 = 0.009012007154524326 + 10.0 * 6.207316875457764
Epoch 2760, val loss: 1.696871280670166
Epoch 2770, training loss: 62.06719207763672 = 0.008919700048863888 + 10.0 * 6.205827236175537
Epoch 2770, val loss: 1.6988781690597534
Epoch 2780, training loss: 62.06428909301758 = 0.008833399042487144 + 10.0 * 6.205545425415039
Epoch 2780, val loss: 1.7013813257217407
Epoch 2790, training loss: 62.08345413208008 = 0.0087504917755723 + 10.0 * 6.207470417022705
Epoch 2790, val loss: 1.7031278610229492
Epoch 2800, training loss: 62.074039459228516 = 0.00866504106670618 + 10.0 * 6.20653772354126
Epoch 2800, val loss: 1.7054411172866821
Epoch 2810, training loss: 62.07820510864258 = 0.008582170121371746 + 10.0 * 6.2069621086120605
Epoch 2810, val loss: 1.7072917222976685
Epoch 2820, training loss: 62.08354568481445 = 0.008500012569129467 + 10.0 * 6.207504749298096
Epoch 2820, val loss: 1.7095739841461182
Epoch 2830, training loss: 62.075801849365234 = 0.008418465033173561 + 10.0 * 6.206738471984863
Epoch 2830, val loss: 1.7119227647781372
Epoch 2840, training loss: 62.067169189453125 = 0.00833894032984972 + 10.0 * 6.205883026123047
Epoch 2840, val loss: 1.7140918970108032
Epoch 2850, training loss: 62.05827331542969 = 0.008262225426733494 + 10.0 * 6.205000877380371
Epoch 2850, val loss: 1.715794324874878
Epoch 2860, training loss: 62.05156707763672 = 0.008186543360352516 + 10.0 * 6.204338073730469
Epoch 2860, val loss: 1.7177037000656128
Epoch 2870, training loss: 62.06746292114258 = 0.008113346062600613 + 10.0 * 6.205935001373291
Epoch 2870, val loss: 1.7196128368377686
Epoch 2880, training loss: 62.105472564697266 = 0.008038369007408619 + 10.0 * 6.209743499755859
Epoch 2880, val loss: 1.7219762802124023
Epoch 2890, training loss: 62.070091247558594 = 0.007961271330714226 + 10.0 * 6.206212997436523
Epoch 2890, val loss: 1.7237467765808105
Epoch 2900, training loss: 62.04948043823242 = 0.00788783747702837 + 10.0 * 6.204159259796143
Epoch 2900, val loss: 1.7255076169967651
Epoch 2910, training loss: 62.062713623046875 = 0.007818778045475483 + 10.0 * 6.205489158630371
Epoch 2910, val loss: 1.7274013757705688
Epoch 2920, training loss: 62.07802963256836 = 0.00774839473888278 + 10.0 * 6.207028388977051
Epoch 2920, val loss: 1.7291940450668335
Epoch 2930, training loss: 62.063228607177734 = 0.0076765944249928 + 10.0 * 6.205555438995361
Epoch 2930, val loss: 1.7316137552261353
Epoch 2940, training loss: 62.04265213012695 = 0.007609142456203699 + 10.0 * 6.2035040855407715
Epoch 2940, val loss: 1.732804536819458
Epoch 2950, training loss: 62.06923294067383 = 0.007544097024947405 + 10.0 * 6.2061686515808105
Epoch 2950, val loss: 1.7343248128890991
Epoch 2960, training loss: 62.05526351928711 = 0.007475874852389097 + 10.0 * 6.204778671264648
Epoch 2960, val loss: 1.7370498180389404
Epoch 2970, training loss: 62.064823150634766 = 0.0074111176654696465 + 10.0 * 6.205740928649902
Epoch 2970, val loss: 1.7387582063674927
Epoch 2980, training loss: 62.06117248535156 = 0.007346516475081444 + 10.0 * 6.205382347106934
Epoch 2980, val loss: 1.740229606628418
Epoch 2990, training loss: 62.060218811035156 = 0.007283288054168224 + 10.0 * 6.205293655395508
Epoch 2990, val loss: 1.7422382831573486
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6555555555555556
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 87.91200256347656 = 1.9434046745300293 + 10.0 * 8.5968599319458
Epoch 0, val loss: 1.93388831615448
Epoch 10, training loss: 87.89665222167969 = 1.9335651397705078 + 10.0 * 8.596308708190918
Epoch 10, val loss: 1.9241061210632324
Epoch 20, training loss: 87.83932495117188 = 1.9212619066238403 + 10.0 * 8.591806411743164
Epoch 20, val loss: 1.9115591049194336
Epoch 30, training loss: 87.51126098632812 = 1.9054491519927979 + 10.0 * 8.56058120727539
Epoch 30, val loss: 1.89552640914917
Epoch 40, training loss: 85.650390625 = 1.8878626823425293 + 10.0 * 8.376253128051758
Epoch 40, val loss: 1.8786898851394653
Epoch 50, training loss: 80.77023315429688 = 1.869871973991394 + 10.0 * 7.890036106109619
Epoch 50, val loss: 1.8618042469024658
Epoch 60, training loss: 77.00121307373047 = 1.8577882051467896 + 10.0 * 7.514342308044434
Epoch 60, val loss: 1.8504376411437988
Epoch 70, training loss: 74.40460968017578 = 1.847137689590454 + 10.0 * 7.255747318267822
Epoch 70, val loss: 1.8400144577026367
Epoch 80, training loss: 71.9344711303711 = 1.8380496501922607 + 10.0 * 7.009641647338867
Epoch 80, val loss: 1.8312938213348389
Epoch 90, training loss: 70.36481475830078 = 1.8295888900756836 + 10.0 * 6.853522777557373
Epoch 90, val loss: 1.8229554891586304
Epoch 100, training loss: 69.43266296386719 = 1.82012140750885 + 10.0 * 6.76125431060791
Epoch 100, val loss: 1.8141579627990723
Epoch 110, training loss: 68.77416229248047 = 1.8106950521469116 + 10.0 * 6.696347236633301
Epoch 110, val loss: 1.8055084943771362
Epoch 120, training loss: 68.29405975341797 = 1.8016499280929565 + 10.0 * 6.649240970611572
Epoch 120, val loss: 1.7970373630523682
Epoch 130, training loss: 67.89196014404297 = 1.79281485080719 + 10.0 * 6.609914302825928
Epoch 130, val loss: 1.788706660270691
Epoch 140, training loss: 67.55719757080078 = 1.784013271331787 + 10.0 * 6.5773186683654785
Epoch 140, val loss: 1.780242681503296
Epoch 150, training loss: 67.27900695800781 = 1.7749112844467163 + 10.0 * 6.55040979385376
Epoch 150, val loss: 1.771475911140442
Epoch 160, training loss: 67.07635498046875 = 1.7652740478515625 + 10.0 * 6.531108379364014
Epoch 160, val loss: 1.7623603343963623
Epoch 170, training loss: 66.86480712890625 = 1.7551461458206177 + 10.0 * 6.510965824127197
Epoch 170, val loss: 1.7525396347045898
Epoch 180, training loss: 66.69123077392578 = 1.7441070079803467 + 10.0 * 6.494711875915527
Epoch 180, val loss: 1.742119312286377
Epoch 190, training loss: 66.53992462158203 = 1.7322124242782593 + 10.0 * 6.480770587921143
Epoch 190, val loss: 1.7308425903320312
Epoch 200, training loss: 66.52713775634766 = 1.7191143035888672 + 10.0 * 6.480802059173584
Epoch 200, val loss: 1.7187974452972412
Epoch 210, training loss: 66.30232238769531 = 1.705008625984192 + 10.0 * 6.459731578826904
Epoch 210, val loss: 1.7054803371429443
Epoch 220, training loss: 66.17867279052734 = 1.689818263053894 + 10.0 * 6.448885440826416
Epoch 220, val loss: 1.691333532333374
Epoch 230, training loss: 66.06402587890625 = 1.6732081174850464 + 10.0 * 6.43908166885376
Epoch 230, val loss: 1.6761009693145752
Epoch 240, training loss: 65.95462799072266 = 1.655220866203308 + 10.0 * 6.429941177368164
Epoch 240, val loss: 1.6596450805664062
Epoch 250, training loss: 65.96308135986328 = 1.6359803676605225 + 10.0 * 6.43271017074585
Epoch 250, val loss: 1.64225172996521
Epoch 260, training loss: 65.78043365478516 = 1.6153043508529663 + 10.0 * 6.416512966156006
Epoch 260, val loss: 1.6237096786499023
Epoch 270, training loss: 65.67182159423828 = 1.593414068222046 + 10.0 * 6.407840728759766
Epoch 270, val loss: 1.6042550802230835
Epoch 280, training loss: 65.69188690185547 = 1.5704010725021362 + 10.0 * 6.4121479988098145
Epoch 280, val loss: 1.5838088989257812
Epoch 290, training loss: 65.51923370361328 = 1.5457897186279297 + 10.0 * 6.397344589233398
Epoch 290, val loss: 1.5626568794250488
Epoch 300, training loss: 65.4098129272461 = 1.5202908515930176 + 10.0 * 6.388952255249023
Epoch 300, val loss: 1.5408449172973633
Epoch 310, training loss: 65.31510925292969 = 1.4937490224838257 + 10.0 * 6.38213586807251
Epoch 310, val loss: 1.51847505569458
Epoch 320, training loss: 65.30345916748047 = 1.4662333726882935 + 10.0 * 6.38372278213501
Epoch 320, val loss: 1.495639443397522
Epoch 330, training loss: 65.16517639160156 = 1.4379682540893555 + 10.0 * 6.372720718383789
Epoch 330, val loss: 1.4724481105804443
Epoch 340, training loss: 65.08757019042969 = 1.4090821743011475 + 10.0 * 6.367848873138428
Epoch 340, val loss: 1.449184775352478
Epoch 350, training loss: 65.02386474609375 = 1.3799296617507935 + 10.0 * 6.36439323425293
Epoch 350, val loss: 1.426108479499817
Epoch 360, training loss: 64.96279907226562 = 1.350304365158081 + 10.0 * 6.3612494468688965
Epoch 360, val loss: 1.4032983779907227
Epoch 370, training loss: 64.8718490600586 = 1.3208287954330444 + 10.0 * 6.355102062225342
Epoch 370, val loss: 1.3808845281600952
Epoch 380, training loss: 64.80565643310547 = 1.2914948463439941 + 10.0 * 6.351416110992432
Epoch 380, val loss: 1.359140157699585
Epoch 390, training loss: 64.83038330078125 = 1.262234091758728 + 10.0 * 6.356814384460449
Epoch 390, val loss: 1.3380414247512817
Epoch 400, training loss: 64.68573760986328 = 1.2335861921310425 + 10.0 * 6.34521484375
Epoch 400, val loss: 1.3173739910125732
Epoch 410, training loss: 64.6185531616211 = 1.2052581310272217 + 10.0 * 6.341329574584961
Epoch 410, val loss: 1.2976384162902832
Epoch 420, training loss: 64.55316925048828 = 1.1776505708694458 + 10.0 * 6.337551593780518
Epoch 420, val loss: 1.2788820266723633
Epoch 430, training loss: 64.49919128417969 = 1.1506472826004028 + 10.0 * 6.3348541259765625
Epoch 430, val loss: 1.2609789371490479
Epoch 440, training loss: 64.44107055664062 = 1.1242892742156982 + 10.0 * 6.3316779136657715
Epoch 440, val loss: 1.2437644004821777
Epoch 450, training loss: 64.42109680175781 = 1.098638653755188 + 10.0 * 6.332245826721191
Epoch 450, val loss: 1.2273142337799072
Epoch 460, training loss: 64.37176513671875 = 1.0736960172653198 + 10.0 * 6.329806804656982
Epoch 460, val loss: 1.2116564512252808
Epoch 470, training loss: 64.29411315917969 = 1.04952073097229 + 10.0 * 6.324459552764893
Epoch 470, val loss: 1.1970967054367065
Epoch 480, training loss: 64.2437744140625 = 1.0261297225952148 + 10.0 * 6.3217644691467285
Epoch 480, val loss: 1.1832126379013062
Epoch 490, training loss: 64.20631408691406 = 1.003463864326477 + 10.0 * 6.320284843444824
Epoch 490, val loss: 1.1700828075408936
Epoch 500, training loss: 64.2452392578125 = 0.9813612699508667 + 10.0 * 6.326387882232666
Epoch 500, val loss: 1.1577180624008179
Epoch 510, training loss: 64.15316009521484 = 0.9600902199745178 + 10.0 * 6.31930685043335
Epoch 510, val loss: 1.1458740234375
Epoch 520, training loss: 64.08409881591797 = 0.9393807053565979 + 10.0 * 6.31447172164917
Epoch 520, val loss: 1.1347914934158325
Epoch 530, training loss: 64.0289535522461 = 0.9194002151489258 + 10.0 * 6.310955047607422
Epoch 530, val loss: 1.1245139837265015
Epoch 540, training loss: 63.980167388916016 = 0.8999540209770203 + 10.0 * 6.308021068572998
Epoch 540, val loss: 1.114897608757019
Epoch 550, training loss: 63.94560241699219 = 0.8809848427772522 + 10.0 * 6.306461811065674
Epoch 550, val loss: 1.1057566404342651
Epoch 560, training loss: 63.9941291809082 = 0.8623428344726562 + 10.0 * 6.313178539276123
Epoch 560, val loss: 1.096984624862671
Epoch 570, training loss: 63.889591217041016 = 0.8441324234008789 + 10.0 * 6.304545879364014
Epoch 570, val loss: 1.0885603427886963
Epoch 580, training loss: 63.852813720703125 = 0.8262925148010254 + 10.0 * 6.302651882171631
Epoch 580, val loss: 1.0806845426559448
Epoch 590, training loss: 63.808570861816406 = 0.8088297843933105 + 10.0 * 6.299973964691162
Epoch 590, val loss: 1.07327139377594
Epoch 600, training loss: 63.77736282348633 = 0.7917549014091492 + 10.0 * 6.298560619354248
Epoch 600, val loss: 1.0662108659744263
Epoch 610, training loss: 63.776248931884766 = 0.774902880191803 + 10.0 * 6.300134658813477
Epoch 610, val loss: 1.0594021081924438
Epoch 620, training loss: 63.73514175415039 = 0.7582260370254517 + 10.0 * 6.297691822052002
Epoch 620, val loss: 1.0531892776489258
Epoch 630, training loss: 63.68902587890625 = 0.7417709827423096 + 10.0 * 6.29472541809082
Epoch 630, val loss: 1.0468050241470337
Epoch 640, training loss: 63.75051498413086 = 0.7254538536071777 + 10.0 * 6.302506446838379
Epoch 640, val loss: 1.0407518148422241
Epoch 650, training loss: 63.64790344238281 = 0.7094895243644714 + 10.0 * 6.293841361999512
Epoch 650, val loss: 1.0350254774093628
Epoch 660, training loss: 63.59120178222656 = 0.6938066482543945 + 10.0 * 6.289739608764648
Epoch 660, val loss: 1.0296896696090698
Epoch 670, training loss: 63.561729431152344 = 0.678349494934082 + 10.0 * 6.2883381843566895
Epoch 670, val loss: 1.0248695611953735
Epoch 680, training loss: 63.532161712646484 = 0.6630734801292419 + 10.0 * 6.2869086265563965
Epoch 680, val loss: 1.020248532295227
Epoch 690, training loss: 63.52957534790039 = 0.6479964256286621 + 10.0 * 6.288157939910889
Epoch 690, val loss: 1.0158449411392212
Epoch 700, training loss: 63.490196228027344 = 0.6331086754798889 + 10.0 * 6.285708427429199
Epoch 700, val loss: 1.0118638277053833
Epoch 710, training loss: 63.49577331542969 = 0.6184683442115784 + 10.0 * 6.2877302169799805
Epoch 710, val loss: 1.0078994035720825
Epoch 720, training loss: 63.44384765625 = 0.6040153503417969 + 10.0 * 6.28398323059082
Epoch 720, val loss: 1.0045653581619263
Epoch 730, training loss: 63.40648651123047 = 0.5898732542991638 + 10.0 * 6.281661033630371
Epoch 730, val loss: 1.001539707183838
Epoch 740, training loss: 63.38144302368164 = 0.5760300755500793 + 10.0 * 6.28054141998291
Epoch 740, val loss: 0.9991874694824219
Epoch 750, training loss: 63.355079650878906 = 0.5623594522476196 + 10.0 * 6.279272079467773
Epoch 750, val loss: 0.9969744682312012
Epoch 760, training loss: 63.3671875 = 0.5489125847816467 + 10.0 * 6.281827449798584
Epoch 760, val loss: 0.9951388835906982
Epoch 770, training loss: 63.39164733886719 = 0.5356733202934265 + 10.0 * 6.285597324371338
Epoch 770, val loss: 0.9939100742340088
Epoch 780, training loss: 63.287864685058594 = 0.5224462151527405 + 10.0 * 6.276541709899902
Epoch 780, val loss: 0.9925289750099182
Epoch 790, training loss: 63.269866943359375 = 0.5095444321632385 + 10.0 * 6.276032447814941
Epoch 790, val loss: 0.9916166067123413
Epoch 800, training loss: 63.24296569824219 = 0.49691158533096313 + 10.0 * 6.274605751037598
Epoch 800, val loss: 0.9913323521614075
Epoch 810, training loss: 63.2423095703125 = 0.4844353497028351 + 10.0 * 6.275787353515625
Epoch 810, val loss: 0.9914255738258362
Epoch 820, training loss: 63.21863555908203 = 0.471993625164032 + 10.0 * 6.274664402008057
Epoch 820, val loss: 0.9915008544921875
Epoch 830, training loss: 63.19487380981445 = 0.4596268832683563 + 10.0 * 6.273524761199951
Epoch 830, val loss: 0.9917853474617004
Epoch 840, training loss: 63.20254898071289 = 0.44754230976104736 + 10.0 * 6.275500297546387
Epoch 840, val loss: 0.9925979971885681
Epoch 850, training loss: 63.15199661254883 = 0.4354653060436249 + 10.0 * 6.271653175354004
Epoch 850, val loss: 0.9934816956520081
Epoch 860, training loss: 63.12378692626953 = 0.4235718250274658 + 10.0 * 6.270021438598633
Epoch 860, val loss: 0.9948278069496155
Epoch 870, training loss: 63.11760330200195 = 0.4117945432662964 + 10.0 * 6.270581245422363
Epoch 870, val loss: 0.9962145686149597
Epoch 880, training loss: 63.11393737792969 = 0.4001483619213104 + 10.0 * 6.271378993988037
Epoch 880, val loss: 0.9979329705238342
Epoch 890, training loss: 63.06887435913086 = 0.38871148228645325 + 10.0 * 6.268016338348389
Epoch 890, val loss: 0.9998469352722168
Epoch 900, training loss: 63.08881378173828 = 0.3773898184299469 + 10.0 * 6.271142482757568
Epoch 900, val loss: 1.0022624731063843
Epoch 910, training loss: 63.085365295410156 = 0.3661876618862152 + 10.0 * 6.271917819976807
Epoch 910, val loss: 1.004837989807129
Epoch 920, training loss: 63.01161193847656 = 0.35528936982154846 + 10.0 * 6.265632152557373
Epoch 920, val loss: 1.0073491334915161
Epoch 930, training loss: 62.98872375488281 = 0.34454184770584106 + 10.0 * 6.264418125152588
Epoch 930, val loss: 1.0106686353683472
Epoch 940, training loss: 62.96232986450195 = 0.3340652585029602 + 10.0 * 6.262826442718506
Epoch 940, val loss: 1.0142375230789185
Epoch 950, training loss: 62.95041275024414 = 0.3237854540348053 + 10.0 * 6.262662887573242
Epoch 950, val loss: 1.0178837776184082
Epoch 960, training loss: 62.9877815246582 = 0.313687264919281 + 10.0 * 6.267409324645996
Epoch 960, val loss: 1.0217559337615967
Epoch 970, training loss: 62.911563873291016 = 0.3038451671600342 + 10.0 * 6.260771751403809
Epoch 970, val loss: 1.0259513854980469
Epoch 980, training loss: 62.92399215698242 = 0.2942539155483246 + 10.0 * 6.262973785400391
Epoch 980, val loss: 1.0306963920593262
Epoch 990, training loss: 62.90186309814453 = 0.2848823666572571 + 10.0 * 6.261698246002197
Epoch 990, val loss: 1.0352598428726196
Epoch 1000, training loss: 62.86691665649414 = 0.2758448123931885 + 10.0 * 6.2591071128845215
Epoch 1000, val loss: 1.0400292873382568
Epoch 1010, training loss: 62.84656524658203 = 0.26708996295928955 + 10.0 * 6.2579474449157715
Epoch 1010, val loss: 1.0454684495925903
Epoch 1020, training loss: 62.830909729003906 = 0.25862374901771545 + 10.0 * 6.257228374481201
Epoch 1020, val loss: 1.0509397983551025
Epoch 1030, training loss: 62.84210205078125 = 0.2504183053970337 + 10.0 * 6.2591681480407715
Epoch 1030, val loss: 1.0566612482070923
Epoch 1040, training loss: 62.810733795166016 = 0.2424485683441162 + 10.0 * 6.256828784942627
Epoch 1040, val loss: 1.0624399185180664
Epoch 1050, training loss: 62.807044982910156 = 0.2347588837146759 + 10.0 * 6.257228374481201
Epoch 1050, val loss: 1.0682564973831177
Epoch 1060, training loss: 62.79738998413086 = 0.22734636068344116 + 10.0 * 6.257004737854004
Epoch 1060, val loss: 1.0744762420654297
Epoch 1070, training loss: 62.772064208984375 = 0.22020567953586578 + 10.0 * 6.255185604095459
Epoch 1070, val loss: 1.0811522006988525
Epoch 1080, training loss: 62.784950256347656 = 0.21333204209804535 + 10.0 * 6.257161617279053
Epoch 1080, val loss: 1.0878084897994995
Epoch 1090, training loss: 62.73904037475586 = 0.20662890374660492 + 10.0 * 6.253241062164307
Epoch 1090, val loss: 1.0941873788833618
Epoch 1100, training loss: 62.72897720336914 = 0.200182244181633 + 10.0 * 6.2528791427612305
Epoch 1100, val loss: 1.1012831926345825
Epoch 1110, training loss: 62.761810302734375 = 0.19400101900100708 + 10.0 * 6.256781101226807
Epoch 1110, val loss: 1.1083167791366577
Epoch 1120, training loss: 62.71236801147461 = 0.18797725439071655 + 10.0 * 6.252439022064209
Epoch 1120, val loss: 1.1152701377868652
Epoch 1130, training loss: 62.71046447753906 = 0.18217763304710388 + 10.0 * 6.252828598022461
Epoch 1130, val loss: 1.1225563287734985
Epoch 1140, training loss: 62.68946075439453 = 0.1766379326581955 + 10.0 * 6.251282215118408
Epoch 1140, val loss: 1.130079984664917
Epoch 1150, training loss: 62.72846221923828 = 0.17128325998783112 + 10.0 * 6.255717754364014
Epoch 1150, val loss: 1.137550711631775
Epoch 1160, training loss: 62.67841339111328 = 0.16605214774608612 + 10.0 * 6.2512359619140625
Epoch 1160, val loss: 1.145202398300171
Epoch 1170, training loss: 62.70899963378906 = 0.16103070974349976 + 10.0 * 6.254796981811523
Epoch 1170, val loss: 1.1529614925384521
Epoch 1180, training loss: 62.67524719238281 = 0.15620702505111694 + 10.0 * 6.251904010772705
Epoch 1180, val loss: 1.1599079370498657
Epoch 1190, training loss: 62.64235305786133 = 0.15158067643642426 + 10.0 * 6.249077320098877
Epoch 1190, val loss: 1.1685259342193604
Epoch 1200, training loss: 62.624813079833984 = 0.14711463451385498 + 10.0 * 6.247769832611084
Epoch 1200, val loss: 1.1762101650238037
Epoch 1210, training loss: 62.633033752441406 = 0.14280842244625092 + 10.0 * 6.249022483825684
Epoch 1210, val loss: 1.1842786073684692
Epoch 1220, training loss: 62.66297912597656 = 0.13863548636436462 + 10.0 * 6.252434730529785
Epoch 1220, val loss: 1.1923555135726929
Epoch 1230, training loss: 62.63922882080078 = 0.13452935218811035 + 10.0 * 6.250470161437988
Epoch 1230, val loss: 1.199878215789795
Epoch 1240, training loss: 62.61116027832031 = 0.1306324452161789 + 10.0 * 6.248052597045898
Epoch 1240, val loss: 1.208251953125
Epoch 1250, training loss: 62.57598876953125 = 0.1268550157546997 + 10.0 * 6.244913578033447
Epoch 1250, val loss: 1.2166961431503296
Epoch 1260, training loss: 62.57701873779297 = 0.12322384864091873 + 10.0 * 6.245379447937012
Epoch 1260, val loss: 1.2252055406570435
Epoch 1270, training loss: 62.64057159423828 = 0.11971938610076904 + 10.0 * 6.252085208892822
Epoch 1270, val loss: 1.2336416244506836
Epoch 1280, training loss: 62.61054992675781 = 0.1162768080830574 + 10.0 * 6.249427318572998
Epoch 1280, val loss: 1.2411490678787231
Epoch 1290, training loss: 62.548622131347656 = 0.11297178268432617 + 10.0 * 6.243565082550049
Epoch 1290, val loss: 1.2497458457946777
Epoch 1300, training loss: 62.5428581237793 = 0.10979757457971573 + 10.0 * 6.2433061599731445
Epoch 1300, val loss: 1.25845468044281
Epoch 1310, training loss: 62.6070442199707 = 0.1067638024687767 + 10.0 * 6.250028133392334
Epoch 1310, val loss: 1.2669715881347656
Epoch 1320, training loss: 62.5319709777832 = 0.10373007506132126 + 10.0 * 6.242824077606201
Epoch 1320, val loss: 1.2749539613723755
Epoch 1330, training loss: 62.518985748291016 = 0.10085088759660721 + 10.0 * 6.241813659667969
Epoch 1330, val loss: 1.2836164236068726
Epoch 1340, training loss: 62.51166534423828 = 0.09807339310646057 + 10.0 * 6.241359233856201
Epoch 1340, val loss: 1.2920712232589722
Epoch 1350, training loss: 62.598209381103516 = 0.09539394080638885 + 10.0 * 6.25028133392334
Epoch 1350, val loss: 1.3012057542800903
Epoch 1360, training loss: 62.52897644042969 = 0.09278693050146103 + 10.0 * 6.243618965148926
Epoch 1360, val loss: 1.308557152748108
Epoch 1370, training loss: 62.523536682128906 = 0.09025024622678757 + 10.0 * 6.24332857131958
Epoch 1370, val loss: 1.3174421787261963
Epoch 1380, training loss: 62.50581359863281 = 0.08781257271766663 + 10.0 * 6.241799831390381
Epoch 1380, val loss: 1.3258863687515259
Epoch 1390, training loss: 62.4807243347168 = 0.08543669432401657 + 10.0 * 6.239528656005859
Epoch 1390, val loss: 1.334149956703186
Epoch 1400, training loss: 62.475135803222656 = 0.08316057175397873 + 10.0 * 6.239197731018066
Epoch 1400, val loss: 1.3426705598831177
Epoch 1410, training loss: 62.49055099487305 = 0.08095622062683105 + 10.0 * 6.240959644317627
Epoch 1410, val loss: 1.350877046585083
Epoch 1420, training loss: 62.53524398803711 = 0.07880701869726181 + 10.0 * 6.245643615722656
Epoch 1420, val loss: 1.3592814207077026
Epoch 1430, training loss: 62.4724006652832 = 0.07673497498035431 + 10.0 * 6.239566326141357
Epoch 1430, val loss: 1.3673993349075317
Epoch 1440, training loss: 62.4615592956543 = 0.07472039759159088 + 10.0 * 6.238683700561523
Epoch 1440, val loss: 1.3756358623504639
Epoch 1450, training loss: 62.44165802001953 = 0.07280030101537704 + 10.0 * 6.236886024475098
Epoch 1450, val loss: 1.3838499784469604
Epoch 1460, training loss: 62.435359954833984 = 0.07093842327594757 + 10.0 * 6.2364420890808105
Epoch 1460, val loss: 1.3920484781265259
Epoch 1470, training loss: 62.5607795715332 = 0.0691385343670845 + 10.0 * 6.24916410446167
Epoch 1470, val loss: 1.399821162223816
Epoch 1480, training loss: 62.45418167114258 = 0.06737080961465836 + 10.0 * 6.238680839538574
Epoch 1480, val loss: 1.4082396030426025
Epoch 1490, training loss: 62.42922592163086 = 0.06567199528217316 + 10.0 * 6.236355304718018
Epoch 1490, val loss: 1.4162129163742065
Epoch 1500, training loss: 62.49999237060547 = 0.06404685974121094 + 10.0 * 6.243594646453857
Epoch 1500, val loss: 1.4242136478424072
Epoch 1510, training loss: 62.43059158325195 = 0.06242396682500839 + 10.0 * 6.236816883087158
Epoch 1510, val loss: 1.4316730499267578
Epoch 1520, training loss: 62.403892517089844 = 0.06089611351490021 + 10.0 * 6.234299659729004
Epoch 1520, val loss: 1.4398669004440308
Epoch 1530, training loss: 62.39989471435547 = 0.05941986292600632 + 10.0 * 6.2340474128723145
Epoch 1530, val loss: 1.4479012489318848
Epoch 1540, training loss: 62.44947052001953 = 0.05799280107021332 + 10.0 * 6.239148139953613
Epoch 1540, val loss: 1.4561256170272827
Epoch 1550, training loss: 62.448307037353516 = 0.05657203868031502 + 10.0 * 6.239173412322998
Epoch 1550, val loss: 1.4628087282180786
Epoch 1560, training loss: 62.414306640625 = 0.05521636828780174 + 10.0 * 6.2359089851379395
Epoch 1560, val loss: 1.4711281061172485
Epoch 1570, training loss: 62.3876838684082 = 0.0539049431681633 + 10.0 * 6.233377933502197
Epoch 1570, val loss: 1.4782682657241821
Epoch 1580, training loss: 62.37364196777344 = 0.052642498165369034 + 10.0 * 6.232100009918213
Epoch 1580, val loss: 1.4862266778945923
Epoch 1590, training loss: 62.39299392700195 = 0.05142299458384514 + 10.0 * 6.234157085418701
Epoch 1590, val loss: 1.493751049041748
Epoch 1600, training loss: 62.40532684326172 = 0.050231099128723145 + 10.0 * 6.235509395599365
Epoch 1600, val loss: 1.5009342432022095
Epoch 1610, training loss: 62.38832092285156 = 0.04905753210186958 + 10.0 * 6.233926296234131
Epoch 1610, val loss: 1.5083800554275513
Epoch 1620, training loss: 62.427608489990234 = 0.047928791493177414 + 10.0 * 6.2379679679870605
Epoch 1620, val loss: 1.5157452821731567
Epoch 1630, training loss: 62.37391662597656 = 0.04685517027974129 + 10.0 * 6.232706069946289
Epoch 1630, val loss: 1.5229008197784424
Epoch 1640, training loss: 62.35718536376953 = 0.04579245299100876 + 10.0 * 6.231139183044434
Epoch 1640, val loss: 1.5303409099578857
Epoch 1650, training loss: 62.34284210205078 = 0.04478379338979721 + 10.0 * 6.229805946350098
Epoch 1650, val loss: 1.537513256072998
Epoch 1660, training loss: 62.361839294433594 = 0.04380933195352554 + 10.0 * 6.231802940368652
Epoch 1660, val loss: 1.544806718826294
Epoch 1670, training loss: 62.367210388183594 = 0.04284161701798439 + 10.0 * 6.232436656951904
Epoch 1670, val loss: 1.5518569946289062
Epoch 1680, training loss: 62.33852767944336 = 0.041903700679540634 + 10.0 * 6.2296624183654785
Epoch 1680, val loss: 1.5588704347610474
Epoch 1690, training loss: 62.3350830078125 = 0.04100038856267929 + 10.0 * 6.229408264160156
Epoch 1690, val loss: 1.5657130479812622
Epoch 1700, training loss: 62.352783203125 = 0.04014221951365471 + 10.0 * 6.231264114379883
Epoch 1700, val loss: 1.572869062423706
Epoch 1710, training loss: 62.36383819580078 = 0.03929000720381737 + 10.0 * 6.232454776763916
Epoch 1710, val loss: 1.5797178745269775
Epoch 1720, training loss: 62.341609954833984 = 0.038440853357315063 + 10.0 * 6.230317115783691
Epoch 1720, val loss: 1.585999846458435
Epoch 1730, training loss: 62.31998062133789 = 0.03765212371945381 + 10.0 * 6.2282328605651855
Epoch 1730, val loss: 1.5931763648986816
Epoch 1740, training loss: 62.330039978027344 = 0.03687693178653717 + 10.0 * 6.229316234588623
Epoch 1740, val loss: 1.6001330614089966
Epoch 1750, training loss: 62.361114501953125 = 0.03611583262681961 + 10.0 * 6.232499599456787
Epoch 1750, val loss: 1.6063657999038696
Epoch 1760, training loss: 62.32275390625 = 0.03537760674953461 + 10.0 * 6.228737831115723
Epoch 1760, val loss: 1.6129064559936523
Epoch 1770, training loss: 62.299991607666016 = 0.03465822711586952 + 10.0 * 6.22653341293335
Epoch 1770, val loss: 1.6190401315689087
Epoch 1780, training loss: 62.29432678222656 = 0.03396584466099739 + 10.0 * 6.226036071777344
Epoch 1780, val loss: 1.6257898807525635
Epoch 1790, training loss: 62.30537033081055 = 0.033298246562480927 + 10.0 * 6.227207183837891
Epoch 1790, val loss: 1.6320185661315918
Epoch 1800, training loss: 62.33189392089844 = 0.0326450951397419 + 10.0 * 6.22992467880249
Epoch 1800, val loss: 1.638317584991455
Epoch 1810, training loss: 62.341339111328125 = 0.03201663866639137 + 10.0 * 6.230932235717773
Epoch 1810, val loss: 1.6449832916259766
Epoch 1820, training loss: 62.34300994873047 = 0.03138398006558418 + 10.0 * 6.2311625480651855
Epoch 1820, val loss: 1.6511585712432861
Epoch 1830, training loss: 62.2996711730957 = 0.03076060488820076 + 10.0 * 6.226891040802002
Epoch 1830, val loss: 1.656907320022583
Epoch 1840, training loss: 62.27988052368164 = 0.030182525515556335 + 10.0 * 6.224969863891602
Epoch 1840, val loss: 1.663272500038147
Epoch 1850, training loss: 62.271873474121094 = 0.02961612120270729 + 10.0 * 6.2242255210876465
Epoch 1850, val loss: 1.669497013092041
Epoch 1860, training loss: 62.30499267578125 = 0.029065122827887535 + 10.0 * 6.227592945098877
Epoch 1860, val loss: 1.6755517721176147
Epoch 1870, training loss: 62.27402877807617 = 0.028521208092570305 + 10.0 * 6.224550724029541
Epoch 1870, val loss: 1.681129813194275
Epoch 1880, training loss: 62.28695297241211 = 0.02799561619758606 + 10.0 * 6.225895881652832
Epoch 1880, val loss: 1.6868672370910645
Epoch 1890, training loss: 62.30881881713867 = 0.02747531421482563 + 10.0 * 6.228134632110596
Epoch 1890, val loss: 1.6928679943084717
Epoch 1900, training loss: 62.281211853027344 = 0.026977108791470528 + 10.0 * 6.225423336029053
Epoch 1900, val loss: 1.698522686958313
Epoch 1910, training loss: 62.269981384277344 = 0.02648928016424179 + 10.0 * 6.224349021911621
Epoch 1910, val loss: 1.7045395374298096
Epoch 1920, training loss: 62.29742431640625 = 0.02601490169763565 + 10.0 * 6.2271409034729
Epoch 1920, val loss: 1.7098884582519531
Epoch 1930, training loss: 62.266117095947266 = 0.025549808517098427 + 10.0 * 6.224056720733643
Epoch 1930, val loss: 1.715810775756836
Epoch 1940, training loss: 62.260398864746094 = 0.02510674111545086 + 10.0 * 6.223528861999512
Epoch 1940, val loss: 1.7212061882019043
Epoch 1950, training loss: 62.2651252746582 = 0.02466869167983532 + 10.0 * 6.224045753479004
Epoch 1950, val loss: 1.7269363403320312
Epoch 1960, training loss: 62.273738861083984 = 0.024241747334599495 + 10.0 * 6.224949836730957
Epoch 1960, val loss: 1.732254981994629
Epoch 1970, training loss: 62.248558044433594 = 0.023810718208551407 + 10.0 * 6.222474575042725
Epoch 1970, val loss: 1.7375991344451904
Epoch 1980, training loss: 62.24171447753906 = 0.023406146094202995 + 10.0 * 6.22183084487915
Epoch 1980, val loss: 1.7431509494781494
Epoch 1990, training loss: 62.32936096191406 = 0.023015689104795456 + 10.0 * 6.230634689331055
Epoch 1990, val loss: 1.7488327026367188
Epoch 2000, training loss: 62.27425765991211 = 0.02261504717171192 + 10.0 * 6.225164413452148
Epoch 2000, val loss: 1.752961277961731
Epoch 2010, training loss: 62.251373291015625 = 0.022235525771975517 + 10.0 * 6.22291374206543
Epoch 2010, val loss: 1.758486032485962
Epoch 2020, training loss: 62.23162078857422 = 0.021865414455533028 + 10.0 * 6.220975399017334
Epoch 2020, val loss: 1.7635313272476196
Epoch 2030, training loss: 62.23555374145508 = 0.021508924663066864 + 10.0 * 6.221404552459717
Epoch 2030, val loss: 1.769080400466919
Epoch 2040, training loss: 62.26266860961914 = 0.021160408854484558 + 10.0 * 6.224150657653809
Epoch 2040, val loss: 1.7740116119384766
Epoch 2050, training loss: 62.22896194458008 = 0.0208180733025074 + 10.0 * 6.2208147048950195
Epoch 2050, val loss: 1.7789274454116821
Epoch 2060, training loss: 62.255126953125 = 0.020483003929257393 + 10.0 * 6.223464488983154
Epoch 2060, val loss: 1.7837797403335571
Epoch 2070, training loss: 62.259220123291016 = 0.020149346441030502 + 10.0 * 6.223906993865967
Epoch 2070, val loss: 1.7883245944976807
Epoch 2080, training loss: 62.21511459350586 = 0.01983102224767208 + 10.0 * 6.2195281982421875
Epoch 2080, val loss: 1.7934458255767822
Epoch 2090, training loss: 62.20695877075195 = 0.019514145329594612 + 10.0 * 6.218744277954102
Epoch 2090, val loss: 1.798346757888794
Epoch 2100, training loss: 62.205326080322266 = 0.01921447180211544 + 10.0 * 6.218611240386963
Epoch 2100, val loss: 1.8030500411987305
Epoch 2110, training loss: 62.21348571777344 = 0.01891952194273472 + 10.0 * 6.219456672668457
Epoch 2110, val loss: 1.80787193775177
Epoch 2120, training loss: 62.28999710083008 = 0.018630312755703926 + 10.0 * 6.227136611938477
Epoch 2120, val loss: 1.8125275373458862
Epoch 2130, training loss: 62.28364181518555 = 0.01835223101079464 + 10.0 * 6.226529121398926
Epoch 2130, val loss: 1.8169951438903809
Epoch 2140, training loss: 62.215816497802734 = 0.018058491870760918 + 10.0 * 6.219775676727295
Epoch 2140, val loss: 1.8212734460830688
Epoch 2150, training loss: 62.201393127441406 = 0.01778556779026985 + 10.0 * 6.218360900878906
Epoch 2150, val loss: 1.8261741399765015
Epoch 2160, training loss: 62.195369720458984 = 0.017522616311907768 + 10.0 * 6.217784881591797
Epoch 2160, val loss: 1.8308966159820557
Epoch 2170, training loss: 62.222110748291016 = 0.017271844670176506 + 10.0 * 6.220483779907227
Epoch 2170, val loss: 1.835795521736145
Epoch 2180, training loss: 62.25229263305664 = 0.017017902806401253 + 10.0 * 6.223527431488037
Epoch 2180, val loss: 1.8395978212356567
Epoch 2190, training loss: 62.20253372192383 = 0.016756348311901093 + 10.0 * 6.2185773849487305
Epoch 2190, val loss: 1.8439316749572754
Epoch 2200, training loss: 62.19188690185547 = 0.016514116898179054 + 10.0 * 6.2175374031066895
Epoch 2200, val loss: 1.8477357625961304
Epoch 2210, training loss: 62.181671142578125 = 0.016278507187962532 + 10.0 * 6.21653938293457
Epoch 2210, val loss: 1.852637529373169
Epoch 2220, training loss: 62.21297836303711 = 0.016050180420279503 + 10.0 * 6.219693183898926
Epoch 2220, val loss: 1.856742262840271
Epoch 2230, training loss: 62.201316833496094 = 0.015822840854525566 + 10.0 * 6.2185492515563965
Epoch 2230, val loss: 1.861043930053711
Epoch 2240, training loss: 62.197513580322266 = 0.015604001469910145 + 10.0 * 6.218191146850586
Epoch 2240, val loss: 1.8650683164596558
Epoch 2250, training loss: 62.1859245300293 = 0.015380918979644775 + 10.0 * 6.21705436706543
Epoch 2250, val loss: 1.8691598176956177
Epoch 2260, training loss: 62.177974700927734 = 0.015170970000326633 + 10.0 * 6.216280460357666
Epoch 2260, val loss: 1.8732913732528687
Epoch 2270, training loss: 62.19361877441406 = 0.01496372651308775 + 10.0 * 6.217865467071533
Epoch 2270, val loss: 1.877130389213562
Epoch 2280, training loss: 62.18042755126953 = 0.014762546867132187 + 10.0 * 6.216566562652588
Epoch 2280, val loss: 1.8813152313232422
Epoch 2290, training loss: 62.206478118896484 = 0.014568150974810123 + 10.0 * 6.219191074371338
Epoch 2290, val loss: 1.885102391242981
Epoch 2300, training loss: 62.19952392578125 = 0.014369070529937744 + 10.0 * 6.218515396118164
Epoch 2300, val loss: 1.889430284500122
Epoch 2310, training loss: 62.16690444946289 = 0.014172857627272606 + 10.0 * 6.215273380279541
Epoch 2310, val loss: 1.892760157585144
Epoch 2320, training loss: 62.16132354736328 = 0.013983656652271748 + 10.0 * 6.214734077453613
Epoch 2320, val loss: 1.896668791770935
Epoch 2330, training loss: 62.162452697753906 = 0.013802447356283665 + 10.0 * 6.214865207672119
Epoch 2330, val loss: 1.9004287719726562
Epoch 2340, training loss: 62.23023223876953 = 0.013625587336719036 + 10.0 * 6.221660614013672
Epoch 2340, val loss: 1.9043455123901367
Epoch 2350, training loss: 62.159793853759766 = 0.013450334779918194 + 10.0 * 6.214634418487549
Epoch 2350, val loss: 1.9079163074493408
Epoch 2360, training loss: 62.1574592590332 = 0.013278327882289886 + 10.0 * 6.214417934417725
Epoch 2360, val loss: 1.9116911888122559
Epoch 2370, training loss: 62.217655181884766 = 0.01311260461807251 + 10.0 * 6.220454216003418
Epoch 2370, val loss: 1.915624737739563
Epoch 2380, training loss: 62.158424377441406 = 0.012941493652760983 + 10.0 * 6.214548110961914
Epoch 2380, val loss: 1.9188182353973389
Epoch 2390, training loss: 62.156558990478516 = 0.012774772942066193 + 10.0 * 6.214378356933594
Epoch 2390, val loss: 1.9222995042800903
Epoch 2400, training loss: 62.18023681640625 = 0.012617716565728188 + 10.0 * 6.216761589050293
Epoch 2400, val loss: 1.9259003400802612
Epoch 2410, training loss: 62.14469909667969 = 0.012460149824619293 + 10.0 * 6.213223934173584
Epoch 2410, val loss: 1.929673433303833
Epoch 2420, training loss: 62.15523147583008 = 0.012310796417295933 + 10.0 * 6.214292049407959
Epoch 2420, val loss: 1.933125376701355
Epoch 2430, training loss: 62.16616439819336 = 0.012161647900938988 + 10.0 * 6.215400218963623
Epoch 2430, val loss: 1.936697006225586
Epoch 2440, training loss: 62.1630744934082 = 0.012011618353426456 + 10.0 * 6.215106010437012
Epoch 2440, val loss: 1.9404844045639038
Epoch 2450, training loss: 62.19205093383789 = 0.011866512708365917 + 10.0 * 6.218018531799316
Epoch 2450, val loss: 1.9436887502670288
Epoch 2460, training loss: 62.1440544128418 = 0.011723201721906662 + 10.0 * 6.21323299407959
Epoch 2460, val loss: 1.9463543891906738
Epoch 2470, training loss: 62.131561279296875 = 0.011583524756133556 + 10.0 * 6.2119975090026855
Epoch 2470, val loss: 1.9500926733016968
Epoch 2480, training loss: 62.12977981567383 = 0.011448976583778858 + 10.0 * 6.2118330001831055
Epoch 2480, val loss: 1.9534069299697876
Epoch 2490, training loss: 62.21919250488281 = 0.011316255666315556 + 10.0 * 6.22078800201416
Epoch 2490, val loss: 1.956359624862671
Epoch 2500, training loss: 62.129493713378906 = 0.011185100302100182 + 10.0 * 6.211831092834473
Epoch 2500, val loss: 1.9599664211273193
Epoch 2510, training loss: 62.11875534057617 = 0.011054161936044693 + 10.0 * 6.210770130157471
Epoch 2510, val loss: 1.9628981351852417
Epoch 2520, training loss: 62.12918472290039 = 0.010929299518465996 + 10.0 * 6.211825370788574
Epoch 2520, val loss: 1.966380000114441
Epoch 2530, training loss: 62.217044830322266 = 0.010814525187015533 + 10.0 * 6.220623016357422
Epoch 2530, val loss: 1.9693961143493652
Epoch 2540, training loss: 62.16154098510742 = 0.010678633116185665 + 10.0 * 6.215086460113525
Epoch 2540, val loss: 1.9725375175476074
Epoch 2550, training loss: 62.14036178588867 = 0.010561796836555004 + 10.0 * 6.212979793548584
Epoch 2550, val loss: 1.975464105606079
Epoch 2560, training loss: 62.11652755737305 = 0.010441506281495094 + 10.0 * 6.21060848236084
Epoch 2560, val loss: 1.9785211086273193
Epoch 2570, training loss: 62.11581039428711 = 0.010329884476959705 + 10.0 * 6.210547924041748
Epoch 2570, val loss: 1.981643557548523
Epoch 2580, training loss: 62.21663284301758 = 0.010219315066933632 + 10.0 * 6.220641136169434
Epoch 2580, val loss: 1.9842910766601562
Epoch 2590, training loss: 62.16686248779297 = 0.010104606859385967 + 10.0 * 6.2156758308410645
Epoch 2590, val loss: 1.987196683883667
Epoch 2600, training loss: 62.13452911376953 = 0.00999330636113882 + 10.0 * 6.212453365325928
Epoch 2600, val loss: 1.9903614521026611
Epoch 2610, training loss: 62.127708435058594 = 0.009884936735033989 + 10.0 * 6.211782455444336
Epoch 2610, val loss: 1.993153691291809
Epoch 2620, training loss: 62.12806701660156 = 0.009781661443412304 + 10.0 * 6.211828708648682
Epoch 2620, val loss: 1.9962044954299927
Epoch 2630, training loss: 62.123321533203125 = 0.009678361006081104 + 10.0 * 6.211364269256592
Epoch 2630, val loss: 1.9990607500076294
Epoch 2640, training loss: 62.12337112426758 = 0.009576046839356422 + 10.0 * 6.211379528045654
Epoch 2640, val loss: 2.0018348693847656
Epoch 2650, training loss: 62.141395568847656 = 0.009473834186792374 + 10.0 * 6.213191986083984
Epoch 2650, val loss: 2.004897356033325
Epoch 2660, training loss: 62.11160659790039 = 0.009373180568218231 + 10.0 * 6.210223197937012
Epoch 2660, val loss: 2.007535934448242
Epoch 2670, training loss: 62.11235046386719 = 0.009277853183448315 + 10.0 * 6.2103071212768555
Epoch 2670, val loss: 2.010211944580078
Epoch 2680, training loss: 62.130706787109375 = 0.009184415452182293 + 10.0 * 6.212152004241943
Epoch 2680, val loss: 2.0130057334899902
Epoch 2690, training loss: 62.10709762573242 = 0.00908876582980156 + 10.0 * 6.209800720214844
Epoch 2690, val loss: 2.0156028270721436
Epoch 2700, training loss: 62.12531661987305 = 0.008997578173875809 + 10.0 * 6.211631774902344
Epoch 2700, val loss: 2.0176239013671875
Epoch 2710, training loss: 62.150211334228516 = 0.008905816823244095 + 10.0 * 6.214130401611328
Epoch 2710, val loss: 2.0203840732574463
Epoch 2720, training loss: 62.10567092895508 = 0.008814217522740364 + 10.0 * 6.209685325622559
Epoch 2720, val loss: 2.0234663486480713
Epoch 2730, training loss: 62.08634567260742 = 0.008726111613214016 + 10.0 * 6.207762241363525
Epoch 2730, val loss: 2.0258142948150635
Epoch 2740, training loss: 62.08427429199219 = 0.008641649037599564 + 10.0 * 6.207563400268555
Epoch 2740, val loss: 2.0286591053009033
Epoch 2750, training loss: 62.081787109375 = 0.008559482172131538 + 10.0 * 6.207322597503662
Epoch 2750, val loss: 2.031247138977051
Epoch 2760, training loss: 62.15570068359375 = 0.008484642021358013 + 10.0 * 6.2147216796875
Epoch 2760, val loss: 2.0337634086608887
Epoch 2770, training loss: 62.13357925415039 = 0.00839652307331562 + 10.0 * 6.212518215179443
Epoch 2770, val loss: 2.035858392715454
Epoch 2780, training loss: 62.11680221557617 = 0.008310425095260143 + 10.0 * 6.210848808288574
Epoch 2780, val loss: 2.0382139682769775
Epoch 2790, training loss: 62.083316802978516 = 0.008230765350162983 + 10.0 * 6.207508563995361
Epoch 2790, val loss: 2.0408761501312256
Epoch 2800, training loss: 62.072975158691406 = 0.008153610862791538 + 10.0 * 6.206482410430908
Epoch 2800, val loss: 2.0435659885406494
Epoch 2810, training loss: 62.08304214477539 = 0.008080365136265755 + 10.0 * 6.207496166229248
Epoch 2810, val loss: 2.0460257530212402
Epoch 2820, training loss: 62.16817855834961 = 0.00800661277025938 + 10.0 * 6.216017246246338
Epoch 2820, val loss: 2.0482444763183594
Epoch 2830, training loss: 62.12985610961914 = 0.007926561869680882 + 10.0 * 6.212193012237549
Epoch 2830, val loss: 2.0502164363861084
Epoch 2840, training loss: 62.088077545166016 = 0.007853003218770027 + 10.0 * 6.208022117614746
Epoch 2840, val loss: 2.052644968032837
Epoch 2850, training loss: 62.07290267944336 = 0.007781753316521645 + 10.0 * 6.206511974334717
Epoch 2850, val loss: 2.055088758468628
Epoch 2860, training loss: 62.08266067504883 = 0.00771510461345315 + 10.0 * 6.207494735717773
Epoch 2860, val loss: 2.057720184326172
Epoch 2870, training loss: 62.13229751586914 = 0.007648257538676262 + 10.0 * 6.212464809417725
Epoch 2870, val loss: 2.060028314590454
Epoch 2880, training loss: 62.09307098388672 = 0.007573125883936882 + 10.0 * 6.208549976348877
Epoch 2880, val loss: 2.0617942810058594
Epoch 2890, training loss: 62.0715446472168 = 0.0075074187479913235 + 10.0 * 6.206403732299805
Epoch 2890, val loss: 2.064319133758545
Epoch 2900, training loss: 62.14454650878906 = 0.007444687653332949 + 10.0 * 6.213709831237793
Epoch 2900, val loss: 2.0665178298950195
Epoch 2910, training loss: 62.07722473144531 = 0.007373488508164883 + 10.0 * 6.206984996795654
Epoch 2910, val loss: 2.0679049491882324
Epoch 2920, training loss: 62.0616340637207 = 0.007306961342692375 + 10.0 * 6.205432891845703
Epoch 2920, val loss: 2.0705175399780273
Epoch 2930, training loss: 62.05979919433594 = 0.007244636304676533 + 10.0 * 6.205255508422852
Epoch 2930, val loss: 2.072727918624878
Epoch 2940, training loss: 62.0594482421875 = 0.007183261215686798 + 10.0 * 6.205226421356201
Epoch 2940, val loss: 2.074922800064087
Epoch 2950, training loss: 62.13496398925781 = 0.007123324554413557 + 10.0 * 6.212784290313721
Epoch 2950, val loss: 2.07745099067688
Epoch 2960, training loss: 62.09013748168945 = 0.007062479853630066 + 10.0 * 6.20830774307251
Epoch 2960, val loss: 2.078472375869751
Epoch 2970, training loss: 62.07320785522461 = 0.007001498248428106 + 10.0 * 6.206620693206787
Epoch 2970, val loss: 2.0809178352355957
Epoch 2980, training loss: 62.06813049316406 = 0.0069431220181286335 + 10.0 * 6.206118583679199
Epoch 2980, val loss: 2.082562208175659
Epoch 2990, training loss: 62.05377197265625 = 0.006884037517011166 + 10.0 * 6.204688549041748
Epoch 2990, val loss: 2.084956407546997
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6555555555555556
0.8028465998945704
=== training gcn model ===
Epoch 0, training loss: 87.92168426513672 = 1.9531378746032715 + 10.0 * 8.596854209899902
Epoch 0, val loss: 1.9475665092468262
Epoch 10, training loss: 87.90635681152344 = 1.9426213502883911 + 10.0 * 8.596373558044434
Epoch 10, val loss: 1.9376256465911865
Epoch 20, training loss: 87.85770416259766 = 1.9300506114959717 + 10.0 * 8.592764854431152
Epoch 20, val loss: 1.9255115985870361
Epoch 30, training loss: 87.59781646728516 = 1.9141818284988403 + 10.0 * 8.568363189697266
Epoch 30, val loss: 1.9101580381393433
Epoch 40, training loss: 86.28582000732422 = 1.8951901197433472 + 10.0 * 8.43906307220459
Epoch 40, val loss: 1.8925446271896362
Epoch 50, training loss: 81.1904525756836 = 1.8749595880508423 + 10.0 * 7.931549072265625
Epoch 50, val loss: 1.8739395141601562
Epoch 60, training loss: 77.008544921875 = 1.8588101863861084 + 10.0 * 7.5149736404418945
Epoch 60, val loss: 1.8603734970092773
Epoch 70, training loss: 73.76040649414062 = 1.8483097553253174 + 10.0 * 7.19120979309082
Epoch 70, val loss: 1.8504079580307007
Epoch 80, training loss: 72.01215362548828 = 1.837101936340332 + 10.0 * 7.017505168914795
Epoch 80, val loss: 1.84000825881958
Epoch 90, training loss: 70.69776916503906 = 1.826568603515625 + 10.0 * 6.887119770050049
Epoch 90, val loss: 1.8305463790893555
Epoch 100, training loss: 69.79146575927734 = 1.8172533512115479 + 10.0 * 6.797421455383301
Epoch 100, val loss: 1.8218899965286255
Epoch 110, training loss: 69.19938659667969 = 1.8078855276107788 + 10.0 * 6.739150047302246
Epoch 110, val loss: 1.8129569292068481
Epoch 120, training loss: 68.71869659423828 = 1.798303246498108 + 10.0 * 6.692039489746094
Epoch 120, val loss: 1.804289698600769
Epoch 130, training loss: 68.3148422241211 = 1.7889268398284912 + 10.0 * 6.652591705322266
Epoch 130, val loss: 1.7958178520202637
Epoch 140, training loss: 67.99771118164062 = 1.7795473337173462 + 10.0 * 6.621816158294678
Epoch 140, val loss: 1.7872568368911743
Epoch 150, training loss: 67.6898193359375 = 1.7698739767074585 + 10.0 * 6.591994285583496
Epoch 150, val loss: 1.7783665657043457
Epoch 160, training loss: 67.43558502197266 = 1.7596651315689087 + 10.0 * 6.567591667175293
Epoch 160, val loss: 1.7690961360931396
Epoch 170, training loss: 67.19071197509766 = 1.7485992908477783 + 10.0 * 6.5442118644714355
Epoch 170, val loss: 1.7591629028320312
Epoch 180, training loss: 67.0015640258789 = 1.7366455793380737 + 10.0 * 6.526491641998291
Epoch 180, val loss: 1.7485445737838745
Epoch 190, training loss: 66.85142517089844 = 1.7235727310180664 + 10.0 * 6.5127854347229
Epoch 190, val loss: 1.736851453781128
Epoch 200, training loss: 66.70358276367188 = 1.7091450691223145 + 10.0 * 6.499444007873535
Epoch 200, val loss: 1.7240310907363892
Epoch 210, training loss: 66.55513000488281 = 1.6934665441513062 + 10.0 * 6.486166477203369
Epoch 210, val loss: 1.710168480873108
Epoch 220, training loss: 66.41349029541016 = 1.6764466762542725 + 10.0 * 6.473704814910889
Epoch 220, val loss: 1.6950763463974
Epoch 230, training loss: 66.31175994873047 = 1.6579675674438477 + 10.0 * 6.46537971496582
Epoch 230, val loss: 1.678731083869934
Epoch 240, training loss: 66.16748046875 = 1.6380807161331177 + 10.0 * 6.452939510345459
Epoch 240, val loss: 1.661250352859497
Epoch 250, training loss: 66.0466079711914 = 1.6168686151504517 + 10.0 * 6.442974090576172
Epoch 250, val loss: 1.6427078247070312
Epoch 260, training loss: 65.93534088134766 = 1.5942554473876953 + 10.0 * 6.434108734130859
Epoch 260, val loss: 1.622984766960144
Epoch 270, training loss: 65.84024047851562 = 1.5703624486923218 + 10.0 * 6.426987171173096
Epoch 270, val loss: 1.6023439168930054
Epoch 280, training loss: 65.73297882080078 = 1.5452196598052979 + 10.0 * 6.41877555847168
Epoch 280, val loss: 1.5807827711105347
Epoch 290, training loss: 65.63441467285156 = 1.5190867185592651 + 10.0 * 6.411532402038574
Epoch 290, val loss: 1.558441162109375
Epoch 300, training loss: 65.54251861572266 = 1.4921759366989136 + 10.0 * 6.40503454208374
Epoch 300, val loss: 1.5358134508132935
Epoch 310, training loss: 65.45709991455078 = 1.4645485877990723 + 10.0 * 6.399255275726318
Epoch 310, val loss: 1.5128027200698853
Epoch 320, training loss: 65.38787078857422 = 1.4362273216247559 + 10.0 * 6.395164489746094
Epoch 320, val loss: 1.4894685745239258
Epoch 330, training loss: 65.29061889648438 = 1.4075676202774048 + 10.0 * 6.388305187225342
Epoch 330, val loss: 1.466272234916687
Epoch 340, training loss: 65.21609497070312 = 1.378582239151001 + 10.0 * 6.383750915527344
Epoch 340, val loss: 1.4430476427078247
Epoch 350, training loss: 65.13771057128906 = 1.3493263721466064 + 10.0 * 6.378838539123535
Epoch 350, val loss: 1.4198837280273438
Epoch 360, training loss: 65.0613784790039 = 1.3199235200881958 + 10.0 * 6.374145030975342
Epoch 360, val loss: 1.3968420028686523
Epoch 370, training loss: 65.02174377441406 = 1.290168046951294 + 10.0 * 6.373157978057861
Epoch 370, val loss: 1.373839259147644
Epoch 380, training loss: 64.92585754394531 = 1.260103464126587 + 10.0 * 6.366575717926025
Epoch 380, val loss: 1.350731611251831
Epoch 390, training loss: 64.84954833984375 = 1.2296355962753296 + 10.0 * 6.3619914054870605
Epoch 390, val loss: 1.3276145458221436
Epoch 400, training loss: 64.78694152832031 = 1.19906747341156 + 10.0 * 6.3587870597839355
Epoch 400, val loss: 1.3047083616256714
Epoch 410, training loss: 64.74800109863281 = 1.168251872062683 + 10.0 * 6.357975006103516
Epoch 410, val loss: 1.2818372249603271
Epoch 420, training loss: 64.65802001953125 = 1.137619972229004 + 10.0 * 6.352039813995361
Epoch 420, val loss: 1.2595174312591553
Epoch 430, training loss: 64.58167266845703 = 1.107138991355896 + 10.0 * 6.347453594207764
Epoch 430, val loss: 1.2375946044921875
Epoch 440, training loss: 64.51559448242188 = 1.0768696069717407 + 10.0 * 6.343872547149658
Epoch 440, val loss: 1.216076135635376
Epoch 450, training loss: 64.45852661132812 = 1.046855092048645 + 10.0 * 6.341166973114014
Epoch 450, val loss: 1.195120930671692
Epoch 460, training loss: 64.41324615478516 = 1.0170572996139526 + 10.0 * 6.339619159698486
Epoch 460, val loss: 1.1748225688934326
Epoch 470, training loss: 64.34888458251953 = 0.9879708886146545 + 10.0 * 6.3360915184021
Epoch 470, val loss: 1.1553033590316772
Epoch 480, training loss: 64.29320526123047 = 0.959600567817688 + 10.0 * 6.333360195159912
Epoch 480, val loss: 1.13675856590271
Epoch 490, training loss: 64.24151611328125 = 0.9319757223129272 + 10.0 * 6.330954074859619
Epoch 490, val loss: 1.119239330291748
Epoch 500, training loss: 64.19670867919922 = 0.9050428867340088 + 10.0 * 6.329166412353516
Epoch 500, val loss: 1.1026380062103271
Epoch 510, training loss: 64.1800537109375 = 0.878908097743988 + 10.0 * 6.330114841461182
Epoch 510, val loss: 1.087080717086792
Epoch 520, training loss: 64.0909652709961 = 0.8537531495094299 + 10.0 * 6.323720932006836
Epoch 520, val loss: 1.0726330280303955
Epoch 530, training loss: 64.04389953613281 = 0.8294399380683899 + 10.0 * 6.321445941925049
Epoch 530, val loss: 1.0591644048690796
Epoch 540, training loss: 64.00010681152344 = 0.8059536814689636 + 10.0 * 6.319415092468262
Epoch 540, val loss: 1.0467170476913452
Epoch 550, training loss: 63.971656799316406 = 0.7832033038139343 + 10.0 * 6.318845272064209
Epoch 550, val loss: 1.0351886749267578
Epoch 560, training loss: 63.93025207519531 = 0.7610684037208557 + 10.0 * 6.31691837310791
Epoch 560, val loss: 1.0244134664535522
Epoch 570, training loss: 63.90946960449219 = 0.7395758628845215 + 10.0 * 6.316989421844482
Epoch 570, val loss: 1.0144001245498657
Epoch 580, training loss: 63.88566207885742 = 0.7189051508903503 + 10.0 * 6.316675662994385
Epoch 580, val loss: 1.0054593086242676
Epoch 590, training loss: 63.811004638671875 = 0.6989182233810425 + 10.0 * 6.311208724975586
Epoch 590, val loss: 0.9970681667327881
Epoch 600, training loss: 63.771995544433594 = 0.6796280741691589 + 10.0 * 6.309237003326416
Epoch 600, val loss: 0.98947674036026
Epoch 610, training loss: 63.73320007324219 = 0.6608982086181641 + 10.0 * 6.307230472564697
Epoch 610, val loss: 0.9825689196586609
Epoch 620, training loss: 63.71617126464844 = 0.642728328704834 + 10.0 * 6.307344436645508
Epoch 620, val loss: 0.9762786030769348
Epoch 630, training loss: 63.70873260498047 = 0.6248989701271057 + 10.0 * 6.308382987976074
Epoch 630, val loss: 0.9700201153755188
Epoch 640, training loss: 63.6384391784668 = 0.607585608959198 + 10.0 * 6.3030853271484375
Epoch 640, val loss: 0.9646857380867004
Epoch 650, training loss: 63.60700225830078 = 0.5908270478248596 + 10.0 * 6.301617622375488
Epoch 650, val loss: 0.9598406553268433
Epoch 660, training loss: 63.58198165893555 = 0.5745324492454529 + 10.0 * 6.300745010375977
Epoch 660, val loss: 0.9555153250694275
Epoch 670, training loss: 63.57620620727539 = 0.5585695505142212 + 10.0 * 6.301763534545898
Epoch 670, val loss: 0.9514857530593872
Epoch 680, training loss: 63.549217224121094 = 0.542950451374054 + 10.0 * 6.300626754760742
Epoch 680, val loss: 0.9478336572647095
Epoch 690, training loss: 63.500343322753906 = 0.5277436375617981 + 10.0 * 6.29725980758667
Epoch 690, val loss: 0.9445940256118774
Epoch 700, training loss: 63.46725845336914 = 0.5129054188728333 + 10.0 * 6.295435428619385
Epoch 700, val loss: 0.9416937828063965
Epoch 710, training loss: 63.44624710083008 = 0.4984177052974701 + 10.0 * 6.294783115386963
Epoch 710, val loss: 0.9393510818481445
Epoch 720, training loss: 63.433982849121094 = 0.48417186737060547 + 10.0 * 6.294981002807617
Epoch 720, val loss: 0.937001645565033
Epoch 730, training loss: 63.41010284423828 = 0.4703058898448944 + 10.0 * 6.293979644775391
Epoch 730, val loss: 0.9354460835456848
Epoch 740, training loss: 63.35875701904297 = 0.45684048533439636 + 10.0 * 6.290191650390625
Epoch 740, val loss: 0.9342354536056519
Epoch 750, training loss: 63.33271789550781 = 0.44373857975006104 + 10.0 * 6.28889799118042
Epoch 750, val loss: 0.9334726333618164
Epoch 760, training loss: 63.45183563232422 = 0.4309994578361511 + 10.0 * 6.302083492279053
Epoch 760, val loss: 0.9332281351089478
Epoch 770, training loss: 63.286495208740234 = 0.4183308780193329 + 10.0 * 6.286816596984863
Epoch 770, val loss: 0.9328359961509705
Epoch 780, training loss: 63.263431549072266 = 0.40615254640579224 + 10.0 * 6.2857279777526855
Epoch 780, val loss: 0.9328489303588867
Epoch 790, training loss: 63.24217224121094 = 0.39437827467918396 + 10.0 * 6.2847795486450195
Epoch 790, val loss: 0.9334218502044678
Epoch 800, training loss: 63.25194549560547 = 0.38290563225746155 + 10.0 * 6.286904335021973
Epoch 800, val loss: 0.9342623949050903
Epoch 810, training loss: 63.21992111206055 = 0.37168338894844055 + 10.0 * 6.284823417663574
Epoch 810, val loss: 0.935523271560669
Epoch 820, training loss: 63.19850540161133 = 0.3607611060142517 + 10.0 * 6.283774375915527
Epoch 820, val loss: 0.9366973042488098
Epoch 830, training loss: 63.16853332519531 = 0.3501887917518616 + 10.0 * 6.281834602355957
Epoch 830, val loss: 0.938413679599762
Epoch 840, training loss: 63.139129638671875 = 0.3399112820625305 + 10.0 * 6.279921531677246
Epoch 840, val loss: 0.9404898285865784
Epoch 850, training loss: 63.13859939575195 = 0.3299098312854767 + 10.0 * 6.280869007110596
Epoch 850, val loss: 0.9427315592765808
Epoch 860, training loss: 63.10636901855469 = 0.3201676607131958 + 10.0 * 6.27862024307251
Epoch 860, val loss: 0.9451329112052917
Epoch 870, training loss: 63.07890319824219 = 0.31071457266807556 + 10.0 * 6.276818752288818
Epoch 870, val loss: 0.948015570640564
Epoch 880, training loss: 63.061134338378906 = 0.3015579283237457 + 10.0 * 6.2759575843811035
Epoch 880, val loss: 0.9510691165924072
Epoch 890, training loss: 63.058231353759766 = 0.29268527030944824 + 10.0 * 6.276554584503174
Epoch 890, val loss: 0.9544225335121155
Epoch 900, training loss: 63.04841995239258 = 0.2840045690536499 + 10.0 * 6.27644157409668
Epoch 900, val loss: 0.9578559994697571
Epoch 910, training loss: 63.01878356933594 = 0.27557986974716187 + 10.0 * 6.274320125579834
Epoch 910, val loss: 0.9614584445953369
Epoch 920, training loss: 63.00014877319336 = 0.26741716265678406 + 10.0 * 6.27327299118042
Epoch 920, val loss: 0.9653383493423462
Epoch 930, training loss: 63.00634765625 = 0.2595345675945282 + 10.0 * 6.274681568145752
Epoch 930, val loss: 0.9693527817726135
Epoch 940, training loss: 62.96812438964844 = 0.25188517570495605 + 10.0 * 6.2716240882873535
Epoch 940, val loss: 0.9737462997436523
Epoch 950, training loss: 62.942874908447266 = 0.24445228278636932 + 10.0 * 6.269842147827148
Epoch 950, val loss: 0.9782484173774719
Epoch 960, training loss: 62.9716911315918 = 0.23724526166915894 + 10.0 * 6.273444652557373
Epoch 960, val loss: 0.982866644859314
Epoch 970, training loss: 62.94261169433594 = 0.2302388846874237 + 10.0 * 6.271237373352051
Epoch 970, val loss: 0.987503170967102
Epoch 980, training loss: 62.907867431640625 = 0.22342921793460846 + 10.0 * 6.268443584442139
Epoch 980, val loss: 0.9924206733703613
Epoch 990, training loss: 62.88593673706055 = 0.21686775982379913 + 10.0 * 6.26690673828125
Epoch 990, val loss: 0.9975594878196716
Epoch 1000, training loss: 62.88827133178711 = 0.2104872316122055 + 10.0 * 6.267778396606445
Epoch 1000, val loss: 1.0027539730072021
Epoch 1010, training loss: 62.88786315917969 = 0.204279825091362 + 10.0 * 6.26835823059082
Epoch 1010, val loss: 1.0081870555877686
Epoch 1020, training loss: 62.853843688964844 = 0.1982371211051941 + 10.0 * 6.265560626983643
Epoch 1020, val loss: 1.0134564638137817
Epoch 1030, training loss: 62.82973861694336 = 0.1924028843641281 + 10.0 * 6.263733863830566
Epoch 1030, val loss: 1.0191364288330078
Epoch 1040, training loss: 62.81231689453125 = 0.18677230179309845 + 10.0 * 6.26255464553833
Epoch 1040, val loss: 1.0248883962631226
Epoch 1050, training loss: 62.8127326965332 = 0.18131005764007568 + 10.0 * 6.2631425857543945
Epoch 1050, val loss: 1.0308009386062622
Epoch 1060, training loss: 62.79850769042969 = 0.17595821619033813 + 10.0 * 6.2622551918029785
Epoch 1060, val loss: 1.0365816354751587
Epoch 1070, training loss: 62.78741455078125 = 0.17075954377651215 + 10.0 * 6.261665344238281
Epoch 1070, val loss: 1.0424253940582275
Epoch 1080, training loss: 62.790103912353516 = 0.165766641497612 + 10.0 * 6.2624335289001465
Epoch 1080, val loss: 1.0485167503356934
Epoch 1090, training loss: 62.755428314208984 = 0.16089469194412231 + 10.0 * 6.259453296661377
Epoch 1090, val loss: 1.0544562339782715
Epoch 1100, training loss: 62.74534606933594 = 0.1561823934316635 + 10.0 * 6.25891637802124
Epoch 1100, val loss: 1.060602068901062
Epoch 1110, training loss: 62.74750518798828 = 0.15163947641849518 + 10.0 * 6.259586811065674
Epoch 1110, val loss: 1.0668811798095703
Epoch 1120, training loss: 62.7185173034668 = 0.14722506701946259 + 10.0 * 6.257129192352295
Epoch 1120, val loss: 1.073155403137207
Epoch 1130, training loss: 62.70888137817383 = 0.1429634541273117 + 10.0 * 6.256591796875
Epoch 1130, val loss: 1.0795812606811523
Epoch 1140, training loss: 62.728939056396484 = 0.13881978392601013 + 10.0 * 6.259011745452881
Epoch 1140, val loss: 1.0858336687088013
Epoch 1150, training loss: 62.732181549072266 = 0.134757861495018 + 10.0 * 6.259742259979248
Epoch 1150, val loss: 1.0921872854232788
Epoch 1160, training loss: 62.70830154418945 = 0.1308235228061676 + 10.0 * 6.257747650146484
Epoch 1160, val loss: 1.0984874963760376
Epoch 1170, training loss: 62.66975402832031 = 0.1270475834608078 + 10.0 * 6.254270553588867
Epoch 1170, val loss: 1.1051682233810425
Epoch 1180, training loss: 62.65715408325195 = 0.12340269237756729 + 10.0 * 6.253375053405762
Epoch 1180, val loss: 1.1117713451385498
Epoch 1190, training loss: 62.65155792236328 = 0.11987678706645966 + 10.0 * 6.253168106079102
Epoch 1190, val loss: 1.1183974742889404
Epoch 1200, training loss: 62.674678802490234 = 0.11644887179136276 + 10.0 * 6.255823135375977
Epoch 1200, val loss: 1.1249850988388062
Epoch 1210, training loss: 62.65190887451172 = 0.11309508234262466 + 10.0 * 6.253881454467773
Epoch 1210, val loss: 1.131306529045105
Epoch 1220, training loss: 62.680965423583984 = 0.10986334830522537 + 10.0 * 6.257110118865967
Epoch 1220, val loss: 1.1377382278442383
Epoch 1230, training loss: 62.620052337646484 = 0.10673753172159195 + 10.0 * 6.251331329345703
Epoch 1230, val loss: 1.1446422338485718
Epoch 1240, training loss: 62.60950469970703 = 0.10370568931102753 + 10.0 * 6.250579833984375
Epoch 1240, val loss: 1.1510053873062134
Epoch 1250, training loss: 62.59484100341797 = 0.10081381350755692 + 10.0 * 6.2494025230407715
Epoch 1250, val loss: 1.1578991413116455
Epoch 1260, training loss: 62.671531677246094 = 0.0980062484741211 + 10.0 * 6.257352352142334
Epoch 1260, val loss: 1.1644953489303589
Epoch 1270, training loss: 62.59880828857422 = 0.09524492174386978 + 10.0 * 6.250356197357178
Epoch 1270, val loss: 1.17093026638031
Epoch 1280, training loss: 62.56785583496094 = 0.09260843694210052 + 10.0 * 6.247524738311768
Epoch 1280, val loss: 1.1776286363601685
Epoch 1290, training loss: 62.59856414794922 = 0.09006322920322418 + 10.0 * 6.250849723815918
Epoch 1290, val loss: 1.1841487884521484
Epoch 1300, training loss: 62.561973571777344 = 0.08757762610912323 + 10.0 * 6.247439384460449
Epoch 1300, val loss: 1.1906404495239258
Epoch 1310, training loss: 62.54642105102539 = 0.08517593890428543 + 10.0 * 6.246124744415283
Epoch 1310, val loss: 1.1972497701644897
Epoch 1320, training loss: 62.53623580932617 = 0.08286987990140915 + 10.0 * 6.245336532592773
Epoch 1320, val loss: 1.2037864923477173
Epoch 1330, training loss: 62.57219314575195 = 0.08064675331115723 + 10.0 * 6.249154567718506
Epoch 1330, val loss: 1.2101458311080933
Epoch 1340, training loss: 62.576778411865234 = 0.07846006006002426 + 10.0 * 6.249831676483154
Epoch 1340, val loss: 1.2168025970458984
Epoch 1350, training loss: 62.53420639038086 = 0.07635149359703064 + 10.0 * 6.245785713195801
Epoch 1350, val loss: 1.22309410572052
Epoch 1360, training loss: 62.51229476928711 = 0.07433558255434036 + 10.0 * 6.243795871734619
Epoch 1360, val loss: 1.229696273803711
Epoch 1370, training loss: 62.505062103271484 = 0.07239535450935364 + 10.0 * 6.243266582489014
Epoch 1370, val loss: 1.2363102436065674
Epoch 1380, training loss: 62.51874923706055 = 0.0705166757106781 + 10.0 * 6.244822978973389
Epoch 1380, val loss: 1.2426977157592773
Epoch 1390, training loss: 62.501800537109375 = 0.06867989152669907 + 10.0 * 6.243311882019043
Epoch 1390, val loss: 1.2490171194076538
Epoch 1400, training loss: 62.51311492919922 = 0.06690436601638794 + 10.0 * 6.2446208000183105
Epoch 1400, val loss: 1.2554422616958618
Epoch 1410, training loss: 62.50782775878906 = 0.0651932880282402 + 10.0 * 6.244263648986816
Epoch 1410, val loss: 1.2619822025299072
Epoch 1420, training loss: 62.49006271362305 = 0.06352941691875458 + 10.0 * 6.2426533699035645
Epoch 1420, val loss: 1.2683637142181396
Epoch 1430, training loss: 62.47864532470703 = 0.061917297542095184 + 10.0 * 6.241672992706299
Epoch 1430, val loss: 1.2745219469070435
Epoch 1440, training loss: 62.460567474365234 = 0.06038139760494232 + 10.0 * 6.240018367767334
Epoch 1440, val loss: 1.2810723781585693
Epoch 1450, training loss: 62.46218490600586 = 0.05889253318309784 + 10.0 * 6.240329265594482
Epoch 1450, val loss: 1.2874685525894165
Epoch 1460, training loss: 62.51433563232422 = 0.05744797736406326 + 10.0 * 6.245688438415527
Epoch 1460, val loss: 1.2935993671417236
Epoch 1470, training loss: 62.485679626464844 = 0.056019127368927 + 10.0 * 6.242966175079346
Epoch 1470, val loss: 1.2994685173034668
Epoch 1480, training loss: 62.494327545166016 = 0.054660700261592865 + 10.0 * 6.243966579437256
Epoch 1480, val loss: 1.3057667016983032
Epoch 1490, training loss: 62.439395904541016 = 0.053324002772569656 + 10.0 * 6.238606929779053
Epoch 1490, val loss: 1.3116422891616821
Epoch 1500, training loss: 62.47031784057617 = 0.05204467475414276 + 10.0 * 6.241827487945557
Epoch 1500, val loss: 1.317537784576416
Epoch 1510, training loss: 62.42561340332031 = 0.05080685764551163 + 10.0 * 6.237480640411377
Epoch 1510, val loss: 1.3237725496292114
Epoch 1520, training loss: 62.42168045043945 = 0.049617551267147064 + 10.0 * 6.23720645904541
Epoch 1520, val loss: 1.3298956155776978
Epoch 1530, training loss: 62.41697311401367 = 0.04846646636724472 + 10.0 * 6.236850738525391
Epoch 1530, val loss: 1.3358516693115234
Epoch 1540, training loss: 62.463951110839844 = 0.04735506698489189 + 10.0 * 6.241659641265869
Epoch 1540, val loss: 1.3418235778808594
Epoch 1550, training loss: 62.435935974121094 = 0.04625657945871353 + 10.0 * 6.2389678955078125
Epoch 1550, val loss: 1.3474125862121582
Epoch 1560, training loss: 62.42267608642578 = 0.04518842697143555 + 10.0 * 6.237748622894287
Epoch 1560, val loss: 1.3530186414718628
Epoch 1570, training loss: 62.42772674560547 = 0.04416625574231148 + 10.0 * 6.238356113433838
Epoch 1570, val loss: 1.3586195707321167
Epoch 1580, training loss: 62.40834045410156 = 0.043171681463718414 + 10.0 * 6.236516952514648
Epoch 1580, val loss: 1.3644770383834839
Epoch 1590, training loss: 62.41056823730469 = 0.04220772907137871 + 10.0 * 6.236835956573486
Epoch 1590, val loss: 1.369759202003479
Epoch 1600, training loss: 62.41862106323242 = 0.04127753898501396 + 10.0 * 6.237734317779541
Epoch 1600, val loss: 1.3752543926239014
Epoch 1610, training loss: 62.39668655395508 = 0.0403699092566967 + 10.0 * 6.235631465911865
Epoch 1610, val loss: 1.3809807300567627
Epoch 1620, training loss: 62.3921012878418 = 0.03949810191988945 + 10.0 * 6.235260486602783
Epoch 1620, val loss: 1.3863824605941772
Epoch 1630, training loss: 62.425941467285156 = 0.03864376246929169 + 10.0 * 6.238729953765869
Epoch 1630, val loss: 1.3915218114852905
Epoch 1640, training loss: 62.3857421875 = 0.03781101852655411 + 10.0 * 6.234793186187744
Epoch 1640, val loss: 1.3969776630401611
Epoch 1650, training loss: 62.37301254272461 = 0.03701310604810715 + 10.0 * 6.233599662780762
Epoch 1650, val loss: 1.40243661403656
Epoch 1660, training loss: 62.39347457885742 = 0.036239705979824066 + 10.0 * 6.235723495483398
Epoch 1660, val loss: 1.4075652360916138
Epoch 1670, training loss: 62.36363220214844 = 0.035476185381412506 + 10.0 * 6.232815742492676
Epoch 1670, val loss: 1.4126056432724
Epoch 1680, training loss: 62.361454010009766 = 0.03474162518978119 + 10.0 * 6.23267126083374
Epoch 1680, val loss: 1.4177430868148804
Epoch 1690, training loss: 62.40776062011719 = 0.03403095901012421 + 10.0 * 6.237372875213623
Epoch 1690, val loss: 1.4224474430084229
Epoch 1700, training loss: 62.35201644897461 = 0.03333539515733719 + 10.0 * 6.231867790222168
Epoch 1700, val loss: 1.428065538406372
Epoch 1710, training loss: 62.34480667114258 = 0.03266152739524841 + 10.0 * 6.23121452331543
Epoch 1710, val loss: 1.4328296184539795
Epoch 1720, training loss: 62.33828353881836 = 0.03201472759246826 + 10.0 * 6.230627059936523
Epoch 1720, val loss: 1.4379093647003174
Epoch 1730, training loss: 62.365604400634766 = 0.03139270842075348 + 10.0 * 6.233421325683594
Epoch 1730, val loss: 1.4430835247039795
Epoch 1740, training loss: 62.35054016113281 = 0.030766110867261887 + 10.0 * 6.231977462768555
Epoch 1740, val loss: 1.4474387168884277
Epoch 1750, training loss: 62.338104248046875 = 0.030158881098031998 + 10.0 * 6.230794429779053
Epoch 1750, val loss: 1.4524377584457397
Epoch 1760, training loss: 62.33250427246094 = 0.02957375906407833 + 10.0 * 6.230292797088623
Epoch 1760, val loss: 1.4569320678710938
Epoch 1770, training loss: 62.34932327270508 = 0.029015807434916496 + 10.0 * 6.232030868530273
Epoch 1770, val loss: 1.461843729019165
Epoch 1780, training loss: 62.33441162109375 = 0.028458109125494957 + 10.0 * 6.230595588684082
Epoch 1780, val loss: 1.4663203954696655
Epoch 1790, training loss: 62.34086990356445 = 0.027922572568058968 + 10.0 * 6.231294631958008
Epoch 1790, val loss: 1.4710770845413208
Epoch 1800, training loss: 62.33544921875 = 0.027399105951189995 + 10.0 * 6.230804920196533
Epoch 1800, val loss: 1.4754866361618042
Epoch 1810, training loss: 62.34402084350586 = 0.026890568435192108 + 10.0 * 6.23171329498291
Epoch 1810, val loss: 1.4801312685012817
Epoch 1820, training loss: 62.34654235839844 = 0.026391291990876198 + 10.0 * 6.232015132904053
Epoch 1820, val loss: 1.4844683408737183
Epoch 1830, training loss: 62.307865142822266 = 0.025899065658450127 + 10.0 * 6.228196620941162
Epoch 1830, val loss: 1.4884886741638184
Epoch 1840, training loss: 62.30375289916992 = 0.02543259784579277 + 10.0 * 6.227831840515137
Epoch 1840, val loss: 1.4928961992263794
Epoch 1850, training loss: 62.303611755371094 = 0.024979792535305023 + 10.0 * 6.227863311767578
Epoch 1850, val loss: 1.497146725654602
Epoch 1860, training loss: 62.33995056152344 = 0.024538014084100723 + 10.0 * 6.231541633605957
Epoch 1860, val loss: 1.5012768507003784
Epoch 1870, training loss: 62.31795883178711 = 0.024104341864585876 + 10.0 * 6.2293853759765625
Epoch 1870, val loss: 1.506066918373108
Epoch 1880, training loss: 62.322818756103516 = 0.0236797034740448 + 10.0 * 6.22991418838501
Epoch 1880, val loss: 1.5098997354507446
Epoch 1890, training loss: 62.29608154296875 = 0.02326309308409691 + 10.0 * 6.2272820472717285
Epoch 1890, val loss: 1.514064908027649
Epoch 1900, training loss: 62.284324645996094 = 0.022862818092107773 + 10.0 * 6.226146221160889
Epoch 1900, val loss: 1.5181487798690796
Epoch 1910, training loss: 62.28794860839844 = 0.022476641461253166 + 10.0 * 6.2265472412109375
Epoch 1910, val loss: 1.5223808288574219
Epoch 1920, training loss: 62.3416633605957 = 0.022102627903223038 + 10.0 * 6.2319560050964355
Epoch 1920, val loss: 1.5263527631759644
Epoch 1930, training loss: 62.30492401123047 = 0.02172040566802025 + 10.0 * 6.228320121765137
Epoch 1930, val loss: 1.5300604104995728
Epoch 1940, training loss: 62.2836799621582 = 0.02135310135781765 + 10.0 * 6.226232528686523
Epoch 1940, val loss: 1.5339235067367554
Epoch 1950, training loss: 62.301055908203125 = 0.02100558578968048 + 10.0 * 6.228005409240723
Epoch 1950, val loss: 1.537971019744873
Epoch 1960, training loss: 62.28079605102539 = 0.020655298605561256 + 10.0 * 6.226014137268066
Epoch 1960, val loss: 1.5415133237838745
Epoch 1970, training loss: 62.289390563964844 = 0.020318225026130676 + 10.0 * 6.226907253265381
Epoch 1970, val loss: 1.545210361480713
Epoch 1980, training loss: 62.28044891357422 = 0.019987881183624268 + 10.0 * 6.226046085357666
Epoch 1980, val loss: 1.5488725900650024
Epoch 1990, training loss: 62.27484893798828 = 0.019667135551571846 + 10.0 * 6.225518226623535
Epoch 1990, val loss: 1.5525413751602173
Epoch 2000, training loss: 62.29692459106445 = 0.01935443840920925 + 10.0 * 6.227756977081299
Epoch 2000, val loss: 1.556059718132019
Epoch 2010, training loss: 62.25909423828125 = 0.019046971574425697 + 10.0 * 6.224004745483398
Epoch 2010, val loss: 1.5599958896636963
Epoch 2020, training loss: 62.262001037597656 = 0.01874971017241478 + 10.0 * 6.224325180053711
Epoch 2020, val loss: 1.5637792348861694
Epoch 2030, training loss: 62.277503967285156 = 0.018458902835845947 + 10.0 * 6.22590446472168
Epoch 2030, val loss: 1.567086935043335
Epoch 2040, training loss: 62.258453369140625 = 0.01817282848060131 + 10.0 * 6.22402811050415
Epoch 2040, val loss: 1.5706219673156738
Epoch 2050, training loss: 62.25146484375 = 0.017892740666866302 + 10.0 * 6.223357200622559
Epoch 2050, val loss: 1.5739126205444336
Epoch 2060, training loss: 62.28523635864258 = 0.017623480409383774 + 10.0 * 6.226761341094971
Epoch 2060, val loss: 1.5773087739944458
Epoch 2070, training loss: 62.25362777709961 = 0.017358707264065742 + 10.0 * 6.223627090454102
Epoch 2070, val loss: 1.5811201333999634
Epoch 2080, training loss: 62.25596237182617 = 0.017096877098083496 + 10.0 * 6.223886489868164
Epoch 2080, val loss: 1.5842326879501343
Epoch 2090, training loss: 62.25881576538086 = 0.016840128228068352 + 10.0 * 6.2241973876953125
Epoch 2090, val loss: 1.5873268842697144
Epoch 2100, training loss: 62.23609924316406 = 0.016592511907219887 + 10.0 * 6.221950531005859
Epoch 2100, val loss: 1.5906803607940674
Epoch 2110, training loss: 62.24493408203125 = 0.016352830454707146 + 10.0 * 6.22285795211792
Epoch 2110, val loss: 1.5940583944320679
Epoch 2120, training loss: 62.282779693603516 = 0.016116062179207802 + 10.0 * 6.226666450500488
Epoch 2120, val loss: 1.5969882011413574
Epoch 2130, training loss: 62.240020751953125 = 0.01587994396686554 + 10.0 * 6.222414016723633
Epoch 2130, val loss: 1.6001633405685425
Epoch 2140, training loss: 62.22600555419922 = 0.01565440744161606 + 10.0 * 6.221035003662109
Epoch 2140, val loss: 1.6035457849502563
Epoch 2150, training loss: 62.22341537475586 = 0.015435412526130676 + 10.0 * 6.220798015594482
Epoch 2150, val loss: 1.6066746711730957
Epoch 2160, training loss: 62.28898620605469 = 0.015226875431835651 + 10.0 * 6.2273759841918945
Epoch 2160, val loss: 1.6100956201553345
Epoch 2170, training loss: 62.269561767578125 = 0.015004762448370457 + 10.0 * 6.2254557609558105
Epoch 2170, val loss: 1.6123902797698975
Epoch 2180, training loss: 62.235408782958984 = 0.014792345464229584 + 10.0 * 6.222061634063721
Epoch 2180, val loss: 1.6155166625976562
Epoch 2190, training loss: 62.21278762817383 = 0.014588232152163982 + 10.0 * 6.219820022583008
Epoch 2190, val loss: 1.6183749437332153
Epoch 2200, training loss: 62.214447021484375 = 0.014393230900168419 + 10.0 * 6.220005512237549
Epoch 2200, val loss: 1.6215381622314453
Epoch 2210, training loss: 62.272483825683594 = 0.014203277416527271 + 10.0 * 6.225828170776367
Epoch 2210, val loss: 1.6245226860046387
Epoch 2220, training loss: 62.25871276855469 = 0.01400668267160654 + 10.0 * 6.224470615386963
Epoch 2220, val loss: 1.6272090673446655
Epoch 2230, training loss: 62.22018051147461 = 0.01381457969546318 + 10.0 * 6.22063684463501
Epoch 2230, val loss: 1.6299834251403809
Epoch 2240, training loss: 62.208763122558594 = 0.013633592054247856 + 10.0 * 6.219512939453125
Epoch 2240, val loss: 1.6329162120819092
Epoch 2250, training loss: 62.24799728393555 = 0.013459566980600357 + 10.0 * 6.223453998565674
Epoch 2250, val loss: 1.6358058452606201
Epoch 2260, training loss: 62.217369079589844 = 0.013280380517244339 + 10.0 * 6.220408916473389
Epoch 2260, val loss: 1.638606071472168
Epoch 2270, training loss: 62.203338623046875 = 0.013105813413858414 + 10.0 * 6.21902322769165
Epoch 2270, val loss: 1.6412159204483032
Epoch 2280, training loss: 62.198875427246094 = 0.012939374893903732 + 10.0 * 6.218593597412109
Epoch 2280, val loss: 1.6440024375915527
Epoch 2290, training loss: 62.26375961303711 = 0.01277981512248516 + 10.0 * 6.225098133087158
Epoch 2290, val loss: 1.6467492580413818
Epoch 2300, training loss: 62.21036911010742 = 0.012608973309397697 + 10.0 * 6.219776153564453
Epoch 2300, val loss: 1.6491197347640991
Epoch 2310, training loss: 62.1902961730957 = 0.012448516674339771 + 10.0 * 6.217784881591797
Epoch 2310, val loss: 1.6517223119735718
Epoch 2320, training loss: 62.18674850463867 = 0.012294212356209755 + 10.0 * 6.217445373535156
Epoch 2320, val loss: 1.6543917655944824
Epoch 2330, training loss: 62.210018157958984 = 0.012144526466727257 + 10.0 * 6.219787120819092
Epoch 2330, val loss: 1.656686782836914
Epoch 2340, training loss: 62.20777130126953 = 0.01199166290462017 + 10.0 * 6.219577789306641
Epoch 2340, val loss: 1.6591663360595703
Epoch 2350, training loss: 62.197689056396484 = 0.011843291111290455 + 10.0 * 6.2185845375061035
Epoch 2350, val loss: 1.6617920398712158
Epoch 2360, training loss: 62.17824935913086 = 0.011696643196046352 + 10.0 * 6.216655254364014
Epoch 2360, val loss: 1.6644188165664673
Epoch 2370, training loss: 62.177024841308594 = 0.011559464037418365 + 10.0 * 6.216546535491943
Epoch 2370, val loss: 1.6670236587524414
Epoch 2380, training loss: 62.18616485595703 = 0.011423586867749691 + 10.0 * 6.217473983764648
Epoch 2380, val loss: 1.6694610118865967
Epoch 2390, training loss: 62.20412063598633 = 0.011288110166788101 + 10.0 * 6.219283103942871
Epoch 2390, val loss: 1.6716903448104858
Epoch 2400, training loss: 62.23359680175781 = 0.011154199950397015 + 10.0 * 6.2222442626953125
Epoch 2400, val loss: 1.6739063262939453
Epoch 2410, training loss: 62.19218063354492 = 0.011020082049071789 + 10.0 * 6.21811580657959
Epoch 2410, val loss: 1.6765429973602295
Epoch 2420, training loss: 62.17759704589844 = 0.01089106872677803 + 10.0 * 6.216670513153076
Epoch 2420, val loss: 1.6786288022994995
Epoch 2430, training loss: 62.190982818603516 = 0.010767544619739056 + 10.0 * 6.218021392822266
Epoch 2430, val loss: 1.6811197996139526
Epoch 2440, training loss: 62.17156982421875 = 0.010642535984516144 + 10.0 * 6.216092586517334
Epoch 2440, val loss: 1.6832289695739746
Epoch 2450, training loss: 62.1920051574707 = 0.010523343458771706 + 10.0 * 6.218148231506348
Epoch 2450, val loss: 1.6856762170791626
Epoch 2460, training loss: 62.17713165283203 = 0.010401853360235691 + 10.0 * 6.216672897338867
Epoch 2460, val loss: 1.6875410079956055
Epoch 2470, training loss: 62.17826843261719 = 0.010284412652254105 + 10.0 * 6.216798305511475
Epoch 2470, val loss: 1.689825177192688
Epoch 2480, training loss: 62.18500518798828 = 0.010170836932957172 + 10.0 * 6.2174835205078125
Epoch 2480, val loss: 1.692168116569519
Epoch 2490, training loss: 62.1661376953125 = 0.01005539856851101 + 10.0 * 6.2156081199646
Epoch 2490, val loss: 1.6941431760787964
Epoch 2500, training loss: 62.15427780151367 = 0.009944970719516277 + 10.0 * 6.214433193206787
Epoch 2500, val loss: 1.6961373090744019
Epoch 2510, training loss: 62.15632247924805 = 0.009837974794209003 + 10.0 * 6.214648246765137
Epoch 2510, val loss: 1.6982709169387817
Epoch 2520, training loss: 62.21076202392578 = 0.00973462499678135 + 10.0 * 6.220102787017822
Epoch 2520, val loss: 1.7003915309906006
Epoch 2530, training loss: 62.184940338134766 = 0.009627415798604488 + 10.0 * 6.217531204223633
Epoch 2530, val loss: 1.7026363611221313
Epoch 2540, training loss: 62.183319091796875 = 0.009520739316940308 + 10.0 * 6.217379570007324
Epoch 2540, val loss: 1.7043602466583252
Epoch 2550, training loss: 62.1652717590332 = 0.009417237713932991 + 10.0 * 6.215585231781006
Epoch 2550, val loss: 1.7063347101211548
Epoch 2560, training loss: 62.154266357421875 = 0.009317263029515743 + 10.0 * 6.2144951820373535
Epoch 2560, val loss: 1.7081477642059326
Epoch 2570, training loss: 62.16656494140625 = 0.009220344945788383 + 10.0 * 6.215734481811523
Epoch 2570, val loss: 1.710182785987854
Epoch 2580, training loss: 62.14595031738281 = 0.009124246425926685 + 10.0 * 6.213682651519775
Epoch 2580, val loss: 1.712157130241394
Epoch 2590, training loss: 62.15223693847656 = 0.009031659923493862 + 10.0 * 6.214320182800293
Epoch 2590, val loss: 1.7143462896347046
Epoch 2600, training loss: 62.19654083251953 = 0.00894095003604889 + 10.0 * 6.218760013580322
Epoch 2600, val loss: 1.716232180595398
Epoch 2610, training loss: 62.15729522705078 = 0.008843175135552883 + 10.0 * 6.214845180511475
Epoch 2610, val loss: 1.7175966501235962
Epoch 2620, training loss: 62.161293029785156 = 0.00875449925661087 + 10.0 * 6.215253829956055
Epoch 2620, val loss: 1.719673991203308
Epoch 2630, training loss: 62.13097381591797 = 0.008661762811243534 + 10.0 * 6.212231159210205
Epoch 2630, val loss: 1.721212387084961
Epoch 2640, training loss: 62.13739776611328 = 0.008576137945055962 + 10.0 * 6.212882041931152
Epoch 2640, val loss: 1.722872257232666
Epoch 2650, training loss: 62.15704345703125 = 0.008492231369018555 + 10.0 * 6.214855194091797
Epoch 2650, val loss: 1.7247651815414429
Epoch 2660, training loss: 62.15348434448242 = 0.00840805284678936 + 10.0 * 6.214507579803467
Epoch 2660, val loss: 1.7265291213989258
Epoch 2670, training loss: 62.14290237426758 = 0.008325602859258652 + 10.0 * 6.2134575843811035
Epoch 2670, val loss: 1.7284198999404907
Epoch 2680, training loss: 62.13167953491211 = 0.00824286974966526 + 10.0 * 6.212343692779541
Epoch 2680, val loss: 1.729819416999817
Epoch 2690, training loss: 62.167816162109375 = 0.008163196966052055 + 10.0 * 6.215965270996094
Epoch 2690, val loss: 1.7313038110733032
Epoch 2700, training loss: 62.152164459228516 = 0.008083316497504711 + 10.0 * 6.214407920837402
Epoch 2700, val loss: 1.733049750328064
Epoch 2710, training loss: 62.152122497558594 = 0.008004920557141304 + 10.0 * 6.214411735534668
Epoch 2710, val loss: 1.734675645828247
Epoch 2720, training loss: 62.13700485229492 = 0.007928537204861641 + 10.0 * 6.212907791137695
Epoch 2720, val loss: 1.7363476753234863
Epoch 2730, training loss: 62.12247848510742 = 0.007852972485125065 + 10.0 * 6.211462497711182
Epoch 2730, val loss: 1.7378934621810913
Epoch 2740, training loss: 62.12471389770508 = 0.007780852261930704 + 10.0 * 6.211693286895752
Epoch 2740, val loss: 1.739626407623291
Epoch 2750, training loss: 62.176429748535156 = 0.007711985148489475 + 10.0 * 6.216871738433838
Epoch 2750, val loss: 1.7414630651474
Epoch 2760, training loss: 62.13178634643555 = 0.007634279318153858 + 10.0 * 6.2124152183532715
Epoch 2760, val loss: 1.7420960664749146
Epoch 2770, training loss: 62.125247955322266 = 0.007562706246972084 + 10.0 * 6.21176815032959
Epoch 2770, val loss: 1.7437022924423218
Epoch 2780, training loss: 62.12409210205078 = 0.0074936156161129475 + 10.0 * 6.211659908294678
Epoch 2780, val loss: 1.7450556755065918
Epoch 2790, training loss: 62.125919342041016 = 0.007424726616591215 + 10.0 * 6.211849689483643
Epoch 2790, val loss: 1.746289610862732
Epoch 2800, training loss: 62.13016128540039 = 0.0073586502112448215 + 10.0 * 6.2122802734375
Epoch 2800, val loss: 1.7477976083755493
Epoch 2810, training loss: 62.126129150390625 = 0.007292365655303001 + 10.0 * 6.211883544921875
Epoch 2810, val loss: 1.7492592334747314
Epoch 2820, training loss: 62.12327575683594 = 0.0072275628335773945 + 10.0 * 6.211604595184326
Epoch 2820, val loss: 1.7507402896881104
Epoch 2830, training loss: 62.10240936279297 = 0.00716273533180356 + 10.0 * 6.209524631500244
Epoch 2830, val loss: 1.7520101070404053
Epoch 2840, training loss: 62.13024139404297 = 0.007101444061845541 + 10.0 * 6.212313652038574
Epoch 2840, val loss: 1.7531002759933472
Epoch 2850, training loss: 62.113319396972656 = 0.007039024960249662 + 10.0 * 6.210628032684326
Epoch 2850, val loss: 1.754599690437317
Epoch 2860, training loss: 62.1191520690918 = 0.0069770957343280315 + 10.0 * 6.211217403411865
Epoch 2860, val loss: 1.756043553352356
Epoch 2870, training loss: 62.130367279052734 = 0.006916744168847799 + 10.0 * 6.212345123291016
Epoch 2870, val loss: 1.7569767236709595
Epoch 2880, training loss: 62.111454010009766 = 0.006854919251054525 + 10.0 * 6.2104597091674805
Epoch 2880, val loss: 1.7581567764282227
Epoch 2890, training loss: 62.102516174316406 = 0.006795996334403753 + 10.0 * 6.209571838378906
Epoch 2890, val loss: 1.7594114542007446
Epoch 2900, training loss: 62.12544631958008 = 0.006740150507539511 + 10.0 * 6.2118706703186035
Epoch 2900, val loss: 1.7607392072677612
Epoch 2910, training loss: 62.10346603393555 = 0.0066815572790801525 + 10.0 * 6.2096781730651855
Epoch 2910, val loss: 1.761755347251892
Epoch 2920, training loss: 62.11386489868164 = 0.006625681649893522 + 10.0 * 6.210723876953125
Epoch 2920, val loss: 1.7631598711013794
Epoch 2930, training loss: 62.12569808959961 = 0.006572354584932327 + 10.0 * 6.211912631988525
Epoch 2930, val loss: 1.7644908428192139
Epoch 2940, training loss: 62.12201690673828 = 0.00651564123108983 + 10.0 * 6.211550235748291
Epoch 2940, val loss: 1.7653579711914062
Epoch 2950, training loss: 62.096187591552734 = 0.006459265016019344 + 10.0 * 6.208972930908203
Epoch 2950, val loss: 1.7661526203155518
Epoch 2960, training loss: 62.084712982177734 = 0.006407985929399729 + 10.0 * 6.207830429077148
Epoch 2960, val loss: 1.7676706314086914
Epoch 2970, training loss: 62.12028121948242 = 0.006358729209750891 + 10.0 * 6.211392402648926
Epoch 2970, val loss: 1.7688462734222412
Epoch 2980, training loss: 62.10141372680664 = 0.00630538584664464 + 10.0 * 6.209510803222656
Epoch 2980, val loss: 1.7694436311721802
Epoch 2990, training loss: 62.080299377441406 = 0.006252465769648552 + 10.0 * 6.207404613494873
Epoch 2990, val loss: 1.7707370519638062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8028465998945704
The final CL Acc:0.67037, 0.02095, The final GNN Acc:0.80320, 0.00050
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13248])
remove edge: torch.Size([2, 7914])
updated graph: torch.Size([2, 10606])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.91598510742188 = 1.9473235607147217 + 10.0 * 8.5968656539917
Epoch 0, val loss: 1.9538236856460571
Epoch 10, training loss: 87.9018783569336 = 1.9373745918273926 + 10.0 * 8.596449851989746
Epoch 10, val loss: 1.9436383247375488
Epoch 20, training loss: 87.85636138916016 = 1.9254246950149536 + 10.0 * 8.593093872070312
Epoch 20, val loss: 1.9312032461166382
Epoch 30, training loss: 87.586669921875 = 1.9098787307739258 + 10.0 * 8.567678451538086
Epoch 30, val loss: 1.9151344299316406
Epoch 40, training loss: 86.10902404785156 = 1.890416145324707 + 10.0 * 8.421860694885254
Epoch 40, val loss: 1.8956167697906494
Epoch 50, training loss: 81.28678131103516 = 1.8695024251937866 + 10.0 * 7.941728591918945
Epoch 50, val loss: 1.8745943307876587
Epoch 60, training loss: 77.56929016113281 = 1.8497358560562134 + 10.0 * 7.57195520401001
Epoch 60, val loss: 1.8549199104309082
Epoch 70, training loss: 75.17804718017578 = 1.8373407125473022 + 10.0 * 7.334070682525635
Epoch 70, val loss: 1.8427345752716064
Epoch 80, training loss: 72.90833282470703 = 1.8269360065460205 + 10.0 * 7.108139991760254
Epoch 80, val loss: 1.8316755294799805
Epoch 90, training loss: 71.2441635131836 = 1.8162113428115845 + 10.0 * 6.942795276641846
Epoch 90, val loss: 1.8213021755218506
Epoch 100, training loss: 70.2417984008789 = 1.8049150705337524 + 10.0 * 6.843688488006592
Epoch 100, val loss: 1.810176968574524
Epoch 110, training loss: 69.32665252685547 = 1.7939883470535278 + 10.0 * 6.753265857696533
Epoch 110, val loss: 1.7994239330291748
Epoch 120, training loss: 68.66666412353516 = 1.7838914394378662 + 10.0 * 6.688277244567871
Epoch 120, val loss: 1.789009928703308
Epoch 130, training loss: 68.2575912475586 = 1.7734347581863403 + 10.0 * 6.648415565490723
Epoch 130, val loss: 1.7782979011535645
Epoch 140, training loss: 67.88490295410156 = 1.7619919776916504 + 10.0 * 6.612290859222412
Epoch 140, val loss: 1.7670015096664429
Epoch 150, training loss: 67.58773040771484 = 1.750443458557129 + 10.0 * 6.583728790283203
Epoch 150, val loss: 1.7556571960449219
Epoch 160, training loss: 67.4071044921875 = 1.738623857498169 + 10.0 * 6.566848278045654
Epoch 160, val loss: 1.7443362474441528
Epoch 170, training loss: 67.10619354248047 = 1.7257750034332275 + 10.0 * 6.538041591644287
Epoch 170, val loss: 1.7323397397994995
Epoch 180, training loss: 66.87711334228516 = 1.7120730876922607 + 10.0 * 6.516504287719727
Epoch 180, val loss: 1.7196377515792847
Epoch 190, training loss: 66.67525482177734 = 1.6972367763519287 + 10.0 * 6.497802257537842
Epoch 190, val loss: 1.7058786153793335
Epoch 200, training loss: 66.52396392822266 = 1.681082844734192 + 10.0 * 6.484288215637207
Epoch 200, val loss: 1.6908493041992188
Epoch 210, training loss: 66.35625457763672 = 1.6633063554763794 + 10.0 * 6.469295024871826
Epoch 210, val loss: 1.6744440793991089
Epoch 220, training loss: 66.20307922363281 = 1.6445834636688232 + 10.0 * 6.4558491706848145
Epoch 220, val loss: 1.657293677330017
Epoch 230, training loss: 66.06133270263672 = 1.6245440244674683 + 10.0 * 6.443678855895996
Epoch 230, val loss: 1.6389883756637573
Epoch 240, training loss: 65.93492126464844 = 1.6031728982925415 + 10.0 * 6.433175086975098
Epoch 240, val loss: 1.6197179555892944
Epoch 250, training loss: 65.82355499267578 = 1.580220341682434 + 10.0 * 6.424333095550537
Epoch 250, val loss: 1.5990506410598755
Epoch 260, training loss: 65.70655822753906 = 1.556249976158142 + 10.0 * 6.415030479431152
Epoch 260, val loss: 1.577492117881775
Epoch 270, training loss: 65.59046173095703 = 1.5310508012771606 + 10.0 * 6.405941009521484
Epoch 270, val loss: 1.55497145652771
Epoch 280, training loss: 65.49171447753906 = 1.5048189163208008 + 10.0 * 6.3986897468566895
Epoch 280, val loss: 1.531785488128662
Epoch 290, training loss: 65.4577407836914 = 1.4774609804153442 + 10.0 * 6.3980278968811035
Epoch 290, val loss: 1.507851004600525
Epoch 300, training loss: 65.3078384399414 = 1.449661135673523 + 10.0 * 6.385818004608154
Epoch 300, val loss: 1.483588695526123
Epoch 310, training loss: 65.22771453857422 = 1.4214121103286743 + 10.0 * 6.3806304931640625
Epoch 310, val loss: 1.4593305587768555
Epoch 320, training loss: 65.14128875732422 = 1.3926990032196045 + 10.0 * 6.37485933303833
Epoch 320, val loss: 1.435064435005188
Epoch 330, training loss: 65.0736312866211 = 1.363763451576233 + 10.0 * 6.3709869384765625
Epoch 330, val loss: 1.410905122756958
Epoch 340, training loss: 65.04286193847656 = 1.3346095085144043 + 10.0 * 6.370825290679932
Epoch 340, val loss: 1.3869012594223022
Epoch 350, training loss: 64.93240356445312 = 1.3057717084884644 + 10.0 * 6.3626627922058105
Epoch 350, val loss: 1.3633240461349487
Epoch 360, training loss: 64.85592651367188 = 1.2770586013793945 + 10.0 * 6.35788631439209
Epoch 360, val loss: 1.3403350114822388
Epoch 370, training loss: 64.78339385986328 = 1.2486851215362549 + 10.0 * 6.353470802307129
Epoch 370, val loss: 1.317889928817749
Epoch 380, training loss: 64.81535339355469 = 1.2204889059066772 + 10.0 * 6.3594865798950195
Epoch 380, val loss: 1.2959754467010498
Epoch 390, training loss: 64.66244506835938 = 1.1929079294204712 + 10.0 * 6.346953868865967
Epoch 390, val loss: 1.2746332883834839
Epoch 400, training loss: 64.5998764038086 = 1.165840744972229 + 10.0 * 6.3434038162231445
Epoch 400, val loss: 1.2540189027786255
Epoch 410, training loss: 64.5845718383789 = 1.1392699480056763 + 10.0 * 6.34453010559082
Epoch 410, val loss: 1.2341266870498657
Epoch 420, training loss: 64.49247741699219 = 1.1132731437683105 + 10.0 * 6.337920665740967
Epoch 420, val loss: 1.2148380279541016
Epoch 430, training loss: 64.43051147460938 = 1.0877822637557983 + 10.0 * 6.334272861480713
Epoch 430, val loss: 1.1962605714797974
Epoch 440, training loss: 64.38826751708984 = 1.0630160570144653 + 10.0 * 6.332525253295898
Epoch 440, val loss: 1.1782736778259277
Epoch 450, training loss: 64.3545913696289 = 1.0385514497756958 + 10.0 * 6.331603527069092
Epoch 450, val loss: 1.1606990098953247
Epoch 460, training loss: 64.28389739990234 = 1.0149370431900024 + 10.0 * 6.326895713806152
Epoch 460, val loss: 1.1439294815063477
Epoch 470, training loss: 64.22996520996094 = 0.9918677806854248 + 10.0 * 6.323809623718262
Epoch 470, val loss: 1.1278226375579834
Epoch 480, training loss: 64.18000793457031 = 0.9694085121154785 + 10.0 * 6.321059703826904
Epoch 480, val loss: 1.1123167276382446
Epoch 490, training loss: 64.21623229980469 = 0.9474003314971924 + 10.0 * 6.326882839202881
Epoch 490, val loss: 1.0973780155181885
Epoch 500, training loss: 64.1044692993164 = 0.925811767578125 + 10.0 * 6.31786584854126
Epoch 500, val loss: 1.0827968120574951
Epoch 510, training loss: 64.08356475830078 = 0.9048477411270142 + 10.0 * 6.317871570587158
Epoch 510, val loss: 1.0687754154205322
Epoch 520, training loss: 64.02205657958984 = 0.8842556476593018 + 10.0 * 6.313780307769775
Epoch 520, val loss: 1.0551985502243042
Epoch 530, training loss: 63.969970703125 = 0.8641635179519653 + 10.0 * 6.310580730438232
Epoch 530, val loss: 1.0423531532287598
Epoch 540, training loss: 63.9297981262207 = 0.8445085287094116 + 10.0 * 6.308528900146484
Epoch 540, val loss: 1.0299545526504517
Epoch 550, training loss: 63.92624282836914 = 0.8251286149024963 + 10.0 * 6.310111045837402
Epoch 550, val loss: 1.01785147190094
Epoch 560, training loss: 63.869163513183594 = 0.8062280416488647 + 10.0 * 6.306293487548828
Epoch 560, val loss: 1.0062488317489624
Epoch 570, training loss: 63.814476013183594 = 0.787748396396637 + 10.0 * 6.302672386169434
Epoch 570, val loss: 0.9951204657554626
Epoch 580, training loss: 63.82042694091797 = 0.7695799469947815 + 10.0 * 6.305084705352783
Epoch 580, val loss: 0.9845218062400818
Epoch 590, training loss: 63.76318359375 = 0.7518231272697449 + 10.0 * 6.301136016845703
Epoch 590, val loss: 0.9741924405097961
Epoch 600, training loss: 63.71681594848633 = 0.7342739701271057 + 10.0 * 6.298254013061523
Epoch 600, val loss: 0.9643065333366394
Epoch 610, training loss: 63.67842483520508 = 0.7171983122825623 + 10.0 * 6.2961225509643555
Epoch 610, val loss: 0.9548715949058533
Epoch 620, training loss: 63.647857666015625 = 0.7003558874130249 + 10.0 * 6.294750213623047
Epoch 620, val loss: 0.9458028078079224
Epoch 630, training loss: 63.671363830566406 = 0.6837274432182312 + 10.0 * 6.298763751983643
Epoch 630, val loss: 0.9372459053993225
Epoch 640, training loss: 63.61085891723633 = 0.6674703359603882 + 10.0 * 6.294339179992676
Epoch 640, val loss: 0.9287593364715576
Epoch 650, training loss: 63.579368591308594 = 0.6514543890953064 + 10.0 * 6.292791366577148
Epoch 650, val loss: 0.9208128452301025
Epoch 660, training loss: 63.532901763916016 = 0.6358729600906372 + 10.0 * 6.289702892303467
Epoch 660, val loss: 0.9132376313209534
Epoch 670, training loss: 63.497589111328125 = 0.6205509305000305 + 10.0 * 6.287703514099121
Epoch 670, val loss: 0.906072735786438
Epoch 680, training loss: 63.51057052612305 = 0.6055490374565125 + 10.0 * 6.290502071380615
Epoch 680, val loss: 0.899307370185852
Epoch 690, training loss: 63.485774993896484 = 0.590628981590271 + 10.0 * 6.289514541625977
Epoch 690, val loss: 0.8928675055503845
Epoch 700, training loss: 63.4281120300293 = 0.5761250257492065 + 10.0 * 6.28519868850708
Epoch 700, val loss: 0.8864578604698181
Epoch 710, training loss: 63.3878173828125 = 0.5619192123413086 + 10.0 * 6.282589912414551
Epoch 710, val loss: 0.8807591199874878
Epoch 720, training loss: 63.365055084228516 = 0.5480110049247742 + 10.0 * 6.281704425811768
Epoch 720, val loss: 0.8752939701080322
Epoch 730, training loss: 63.44639587402344 = 0.5343416929244995 + 10.0 * 6.291205406188965
Epoch 730, val loss: 0.87017422914505
Epoch 740, training loss: 63.33413314819336 = 0.5208550095558167 + 10.0 * 6.281327724456787
Epoch 740, val loss: 0.8651013970375061
Epoch 750, training loss: 63.29840087890625 = 0.5077968239784241 + 10.0 * 6.279060363769531
Epoch 750, val loss: 0.8604841232299805
Epoch 760, training loss: 63.26875686645508 = 0.49509286880493164 + 10.0 * 6.2773661613464355
Epoch 760, val loss: 0.8563187718391418
Epoch 770, training loss: 63.24320602416992 = 0.4826669692993164 + 10.0 * 6.2760539054870605
Epoch 770, val loss: 0.8523545265197754
Epoch 780, training loss: 63.22032928466797 = 0.4704855978488922 + 10.0 * 6.274984359741211
Epoch 780, val loss: 0.8486155867576599
Epoch 790, training loss: 63.285823822021484 = 0.4584785997867584 + 10.0 * 6.2827348709106445
Epoch 790, val loss: 0.8449755311012268
Epoch 800, training loss: 63.260108947753906 = 0.4467175304889679 + 10.0 * 6.281339168548584
Epoch 800, val loss: 0.8416637778282166
Epoch 810, training loss: 63.18793869018555 = 0.4351847767829895 + 10.0 * 6.275275230407715
Epoch 810, val loss: 0.8384374380111694
Epoch 820, training loss: 63.14779281616211 = 0.42395147681236267 + 10.0 * 6.272384166717529
Epoch 820, val loss: 0.8356884121894836
Epoch 830, training loss: 63.120967864990234 = 0.4129596948623657 + 10.0 * 6.270800590515137
Epoch 830, val loss: 0.8331741690635681
Epoch 840, training loss: 63.09707260131836 = 0.4021649658679962 + 10.0 * 6.269490718841553
Epoch 840, val loss: 0.8308219909667969
Epoch 850, training loss: 63.087669372558594 = 0.3915504515171051 + 10.0 * 6.269611835479736
Epoch 850, val loss: 0.8286352157592773
Epoch 860, training loss: 63.06838607788086 = 0.38102251291275024 + 10.0 * 6.268736362457275
Epoch 860, val loss: 0.8262474536895752
Epoch 870, training loss: 63.06957244873047 = 0.37072643637657166 + 10.0 * 6.2698845863342285
Epoch 870, val loss: 0.8243414759635925
Epoch 880, training loss: 63.02821731567383 = 0.3606370687484741 + 10.0 * 6.266757965087891
Epoch 880, val loss: 0.8225966691970825
Epoch 890, training loss: 63.00607681274414 = 0.35072270035743713 + 10.0 * 6.265535354614258
Epoch 890, val loss: 0.8209558129310608
Epoch 900, training loss: 63.07677459716797 = 0.3409876227378845 + 10.0 * 6.273578643798828
Epoch 900, val loss: 0.819348931312561
Epoch 910, training loss: 62.99000930786133 = 0.3313117027282715 + 10.0 * 6.265870094299316
Epoch 910, val loss: 0.8180034756660461
Epoch 920, training loss: 62.96117401123047 = 0.32186034321784973 + 10.0 * 6.2639312744140625
Epoch 920, val loss: 0.8168620467185974
Epoch 930, training loss: 62.94081115722656 = 0.31260234117507935 + 10.0 * 6.262820720672607
Epoch 930, val loss: 0.815730094909668
Epoch 940, training loss: 62.97949981689453 = 0.3035481572151184 + 10.0 * 6.267595291137695
Epoch 940, val loss: 0.8147169351577759
Epoch 950, training loss: 62.92686080932617 = 0.29456087946891785 + 10.0 * 6.263230323791504
Epoch 950, val loss: 0.8136405348777771
Epoch 960, training loss: 62.91096496582031 = 0.2858440577983856 + 10.0 * 6.26251220703125
Epoch 960, val loss: 0.8129599094390869
Epoch 970, training loss: 62.90092849731445 = 0.27731847763061523 + 10.0 * 6.2623610496521
Epoch 970, val loss: 0.8122119307518005
Epoch 980, training loss: 62.8642463684082 = 0.2689538300037384 + 10.0 * 6.259529113769531
Epoch 980, val loss: 0.8118362426757812
Epoch 990, training loss: 62.844398498535156 = 0.2608222961425781 + 10.0 * 6.258357524871826
Epoch 990, val loss: 0.8114727735519409
Epoch 1000, training loss: 62.831878662109375 = 0.25293272733688354 + 10.0 * 6.257894515991211
Epoch 1000, val loss: 0.811358630657196
Epoch 1010, training loss: 62.849571228027344 = 0.24525277316570282 + 10.0 * 6.26043176651001
Epoch 1010, val loss: 0.811327338218689
Epoch 1020, training loss: 62.7987174987793 = 0.23771411180496216 + 10.0 * 6.256100654602051
Epoch 1020, val loss: 0.8113430142402649
Epoch 1030, training loss: 62.79380416870117 = 0.23043853044509888 + 10.0 * 6.256336688995361
Epoch 1030, val loss: 0.811420738697052
Epoch 1040, training loss: 62.86091995239258 = 0.22337926924228668 + 10.0 * 6.263753890991211
Epoch 1040, val loss: 0.8118983507156372
Epoch 1050, training loss: 62.773624420166016 = 0.21643312275409698 + 10.0 * 6.255719184875488
Epoch 1050, val loss: 0.8120698928833008
Epoch 1060, training loss: 62.75005340576172 = 0.20979177951812744 + 10.0 * 6.254025936126709
Epoch 1060, val loss: 0.812724232673645
Epoch 1070, training loss: 62.735504150390625 = 0.20342081785202026 + 10.0 * 6.253208160400391
Epoch 1070, val loss: 0.8135496377944946
Epoch 1080, training loss: 62.71958541870117 = 0.19723811745643616 + 10.0 * 6.25223445892334
Epoch 1080, val loss: 0.8144057989120483
Epoch 1090, training loss: 62.717979431152344 = 0.19122999906539917 + 10.0 * 6.2526750564575195
Epoch 1090, val loss: 0.8154404163360596
Epoch 1100, training loss: 62.7660026550293 = 0.1853829175233841 + 10.0 * 6.25806188583374
Epoch 1100, val loss: 0.8162992000579834
Epoch 1110, training loss: 62.717472076416016 = 0.1797262728214264 + 10.0 * 6.253774642944336
Epoch 1110, val loss: 0.8174530863761902
Epoch 1120, training loss: 62.694366455078125 = 0.17427103221416473 + 10.0 * 6.252009391784668
Epoch 1120, val loss: 0.8189396858215332
Epoch 1130, training loss: 62.663597106933594 = 0.1690145879983902 + 10.0 * 6.249458312988281
Epoch 1130, val loss: 0.820618212223053
Epoch 1140, training loss: 62.66432189941406 = 0.1639549732208252 + 10.0 * 6.250036716461182
Epoch 1140, val loss: 0.8221373558044434
Epoch 1150, training loss: 62.690101623535156 = 0.15905652940273285 + 10.0 * 6.253104209899902
Epoch 1150, val loss: 0.8237622380256653
Epoch 1160, training loss: 62.65163803100586 = 0.1543017476797104 + 10.0 * 6.249733924865723
Epoch 1160, val loss: 0.8252416849136353
Epoch 1170, training loss: 62.64146423339844 = 0.14970934391021729 + 10.0 * 6.249175548553467
Epoch 1170, val loss: 0.8272768259048462
Epoch 1180, training loss: 62.654048919677734 = 0.14528942108154297 + 10.0 * 6.250875949859619
Epoch 1180, val loss: 0.828930675983429
Epoch 1190, training loss: 62.61698913574219 = 0.14099034667015076 + 10.0 * 6.2475996017456055
Epoch 1190, val loss: 0.8311907052993774
Epoch 1200, training loss: 62.59701156616211 = 0.13687023520469666 + 10.0 * 6.24601411819458
Epoch 1200, val loss: 0.8330617547035217
Epoch 1210, training loss: 62.676185607910156 = 0.13287153840065002 + 10.0 * 6.254331588745117
Epoch 1210, val loss: 0.8351831436157227
Epoch 1220, training loss: 62.601993560791016 = 0.12902502715587616 + 10.0 * 6.2472968101501465
Epoch 1220, val loss: 0.8374025821685791
Epoch 1230, training loss: 62.57146072387695 = 0.12529310584068298 + 10.0 * 6.244616508483887
Epoch 1230, val loss: 0.8397339582443237
Epoch 1240, training loss: 62.55959701538086 = 0.12172353267669678 + 10.0 * 6.2437872886657715
Epoch 1240, val loss: 0.8421118855476379
Epoch 1250, training loss: 62.674949645996094 = 0.11824861168861389 + 10.0 * 6.255670070648193
Epoch 1250, val loss: 0.8444397449493408
Epoch 1260, training loss: 62.57291793823242 = 0.11487986892461777 + 10.0 * 6.2458038330078125
Epoch 1260, val loss: 0.8467608690261841
Epoch 1270, training loss: 62.532413482666016 = 0.11162124574184418 + 10.0 * 6.242079257965088
Epoch 1270, val loss: 0.849364697933197
Epoch 1280, training loss: 62.54262924194336 = 0.10850279033184052 + 10.0 * 6.243412971496582
Epoch 1280, val loss: 0.851795494556427
Epoch 1290, training loss: 62.53779983520508 = 0.1054811105132103 + 10.0 * 6.243231773376465
Epoch 1290, val loss: 0.8545012474060059
Epoch 1300, training loss: 62.530006408691406 = 0.10255323350429535 + 10.0 * 6.242745399475098
Epoch 1300, val loss: 0.8571550250053406
Epoch 1310, training loss: 62.54634475708008 = 0.09972904622554779 + 10.0 * 6.244661808013916
Epoch 1310, val loss: 0.8596921563148499
Epoch 1320, training loss: 62.50252151489258 = 0.09696634858846664 + 10.0 * 6.240555763244629
Epoch 1320, val loss: 0.8620375394821167
Epoch 1330, training loss: 62.48670959472656 = 0.09433762729167938 + 10.0 * 6.239237308502197
Epoch 1330, val loss: 0.8649709820747375
Epoch 1340, training loss: 62.486488342285156 = 0.0917956605553627 + 10.0 * 6.239469051361084
Epoch 1340, val loss: 0.8676283359527588
Epoch 1350, training loss: 62.56636047363281 = 0.08931028842926025 + 10.0 * 6.247704982757568
Epoch 1350, val loss: 0.8703281283378601
Epoch 1360, training loss: 62.47053909301758 = 0.08690942823886871 + 10.0 * 6.238362789154053
Epoch 1360, val loss: 0.8729275465011597
Epoch 1370, training loss: 62.461029052734375 = 0.08458966016769409 + 10.0 * 6.237643718719482
Epoch 1370, val loss: 0.8756836652755737
Epoch 1380, training loss: 62.456459045410156 = 0.08235776424407959 + 10.0 * 6.237410068511963
Epoch 1380, val loss: 0.8785440921783447
Epoch 1390, training loss: 62.531192779541016 = 0.08021029829978943 + 10.0 * 6.245098114013672
Epoch 1390, val loss: 0.880943775177002
Epoch 1400, training loss: 62.48495101928711 = 0.07808924466371536 + 10.0 * 6.240685939788818
Epoch 1400, val loss: 0.8844322562217712
Epoch 1410, training loss: 62.44826126098633 = 0.07603888213634491 + 10.0 * 6.237222194671631
Epoch 1410, val loss: 0.8869062662124634
Epoch 1420, training loss: 62.431907653808594 = 0.07408040761947632 + 10.0 * 6.235782623291016
Epoch 1420, val loss: 0.8897441625595093
Epoch 1430, training loss: 62.429412841796875 = 0.07218080759048462 + 10.0 * 6.23572301864624
Epoch 1430, val loss: 0.892792820930481
Epoch 1440, training loss: 62.461509704589844 = 0.07033800333738327 + 10.0 * 6.23911714553833
Epoch 1440, val loss: 0.895503044128418
Epoch 1450, training loss: 62.44236373901367 = 0.06854649633169174 + 10.0 * 6.237381935119629
Epoch 1450, val loss: 0.8986896872520447
Epoch 1460, training loss: 62.41392135620117 = 0.0667942613363266 + 10.0 * 6.234712600708008
Epoch 1460, val loss: 0.9014922380447388
Epoch 1470, training loss: 62.4022216796875 = 0.06512424349784851 + 10.0 * 6.233709812164307
Epoch 1470, val loss: 0.9045006632804871
Epoch 1480, training loss: 62.39221954345703 = 0.06350689381361008 + 10.0 * 6.232871055603027
Epoch 1480, val loss: 0.9075191617012024
Epoch 1490, training loss: 62.424007415771484 = 0.06194817274808884 + 10.0 * 6.2362060546875
Epoch 1490, val loss: 0.9104828834533691
Epoch 1500, training loss: 62.40375900268555 = 0.060399964451789856 + 10.0 * 6.234335899353027
Epoch 1500, val loss: 0.9138132929801941
Epoch 1510, training loss: 62.39348220825195 = 0.05890499800443649 + 10.0 * 6.233457565307617
Epoch 1510, val loss: 0.9164014458656311
Epoch 1520, training loss: 62.39519500732422 = 0.05746455118060112 + 10.0 * 6.233773231506348
Epoch 1520, val loss: 0.9194537997245789
Epoch 1530, training loss: 62.37338638305664 = 0.056075163185596466 + 10.0 * 6.231730937957764
Epoch 1530, val loss: 0.9226844906806946
Epoch 1540, training loss: 62.37141418457031 = 0.05473259091377258 + 10.0 * 6.231667995452881
Epoch 1540, val loss: 0.9256942868232727
Epoch 1550, training loss: 62.382083892822266 = 0.05342412739992142 + 10.0 * 6.232865810394287
Epoch 1550, val loss: 0.9287731647491455
Epoch 1560, training loss: 62.384212493896484 = 0.05215313658118248 + 10.0 * 6.233205795288086
Epoch 1560, val loss: 0.9315863847732544
Epoch 1570, training loss: 62.37677001953125 = 0.0509076751768589 + 10.0 * 6.23258638381958
Epoch 1570, val loss: 0.9347549080848694
Epoch 1580, training loss: 62.35445785522461 = 0.04971328750252724 + 10.0 * 6.230474472045898
Epoch 1580, val loss: 0.9379547238349915
Epoch 1590, training loss: 62.343624114990234 = 0.04855525493621826 + 10.0 * 6.229506969451904
Epoch 1590, val loss: 0.9408140182495117
Epoch 1600, training loss: 62.35881042480469 = 0.04744144156575203 + 10.0 * 6.231137275695801
Epoch 1600, val loss: 0.9442183971405029
Epoch 1610, training loss: 62.34202575683594 = 0.0463450625538826 + 10.0 * 6.229568004608154
Epoch 1610, val loss: 0.9469242095947266
Epoch 1620, training loss: 62.33986282348633 = 0.045276742428541183 + 10.0 * 6.229458808898926
Epoch 1620, val loss: 0.9500318765640259
Epoch 1630, training loss: 62.3827018737793 = 0.04425445944070816 + 10.0 * 6.233844757080078
Epoch 1630, val loss: 0.9530882835388184
Epoch 1640, training loss: 62.330326080322266 = 0.04323875904083252 + 10.0 * 6.228708744049072
Epoch 1640, val loss: 0.9559060335159302
Epoch 1650, training loss: 62.32250213623047 = 0.042266134172677994 + 10.0 * 6.228023529052734
Epoch 1650, val loss: 0.9589817523956299
Epoch 1660, training loss: 62.39193344116211 = 0.041323527693748474 + 10.0 * 6.235060691833496
Epoch 1660, val loss: 0.9616355895996094
Epoch 1670, training loss: 62.32280731201172 = 0.04040610045194626 + 10.0 * 6.228240013122559
Epoch 1670, val loss: 0.9649564623832703
Epoch 1680, training loss: 62.30390167236328 = 0.0395176000893116 + 10.0 * 6.226438522338867
Epoch 1680, val loss: 0.9679217338562012
Epoch 1690, training loss: 62.296932220458984 = 0.038663409650325775 + 10.0 * 6.225827217102051
Epoch 1690, val loss: 0.9709027409553528
Epoch 1700, training loss: 62.32917785644531 = 0.037841323763132095 + 10.0 * 6.229133605957031
Epoch 1700, val loss: 0.9737657308578491
Epoch 1710, training loss: 62.30079650878906 = 0.0370161198079586 + 10.0 * 6.226377964019775
Epoch 1710, val loss: 0.977091372013092
Epoch 1720, training loss: 62.30180740356445 = 0.03622206300497055 + 10.0 * 6.226558685302734
Epoch 1720, val loss: 0.9798948168754578
Epoch 1730, training loss: 62.292850494384766 = 0.03545091301202774 + 10.0 * 6.2257399559021
Epoch 1730, val loss: 0.9827694296836853
Epoch 1740, training loss: 62.31144714355469 = 0.03470738232135773 + 10.0 * 6.2276740074157715
Epoch 1740, val loss: 0.9859074354171753
Epoch 1750, training loss: 62.29133605957031 = 0.03396637365221977 + 10.0 * 6.22573709487915
Epoch 1750, val loss: 0.9883925914764404
Epoch 1760, training loss: 62.279361724853516 = 0.03325135260820389 + 10.0 * 6.224610805511475
Epoch 1760, val loss: 0.9915172457695007
Epoch 1770, training loss: 62.27101516723633 = 0.03257257863879204 + 10.0 * 6.223844051361084
Epoch 1770, val loss: 0.9943885207176208
Epoch 1780, training loss: 62.26559829711914 = 0.031916093081235886 + 10.0 * 6.223368167877197
Epoch 1780, val loss: 0.9974778890609741
Epoch 1790, training loss: 62.30910873413086 = 0.03127620369195938 + 10.0 * 6.227783203125
Epoch 1790, val loss: 1.0003336668014526
Epoch 1800, training loss: 62.293331146240234 = 0.03063739649951458 + 10.0 * 6.226269721984863
Epoch 1800, val loss: 1.0030885934829712
Epoch 1810, training loss: 62.28485107421875 = 0.030015183612704277 + 10.0 * 6.2254838943481445
Epoch 1810, val loss: 1.0057852268218994
Epoch 1820, training loss: 62.25230407714844 = 0.029424510896205902 + 10.0 * 6.222288131713867
Epoch 1820, val loss: 1.0089797973632812
Epoch 1830, training loss: 62.25061798095703 = 0.028853213414549828 + 10.0 * 6.222176551818848
Epoch 1830, val loss: 1.0118781328201294
Epoch 1840, training loss: 62.27425765991211 = 0.028301047161221504 + 10.0 * 6.224595546722412
Epoch 1840, val loss: 1.015098214149475
Epoch 1850, training loss: 62.27719497680664 = 0.027751516550779343 + 10.0 * 6.224944114685059
Epoch 1850, val loss: 1.0175831317901611
Epoch 1860, training loss: 62.24967956542969 = 0.027207419276237488 + 10.0 * 6.222247123718262
Epoch 1860, val loss: 1.0201747417449951
Epoch 1870, training loss: 62.25580596923828 = 0.02669042907655239 + 10.0 * 6.222911357879639
Epoch 1870, val loss: 1.0231918096542358
Epoch 1880, training loss: 62.25513458251953 = 0.02619091421365738 + 10.0 * 6.222894191741943
Epoch 1880, val loss: 1.0257395505905151
Epoch 1890, training loss: 62.235748291015625 = 0.02570420131087303 + 10.0 * 6.221004486083984
Epoch 1890, val loss: 1.029037594795227
Epoch 1900, training loss: 62.24736022949219 = 0.02523142471909523 + 10.0 * 6.222212791442871
Epoch 1900, val loss: 1.0318689346313477
Epoch 1910, training loss: 62.266380310058594 = 0.024767233058810234 + 10.0 * 6.224161148071289
Epoch 1910, val loss: 1.0343623161315918
Epoch 1920, training loss: 62.256683349609375 = 0.02430426888167858 + 10.0 * 6.223237991333008
Epoch 1920, val loss: 1.0370564460754395
Epoch 1930, training loss: 62.23697280883789 = 0.023860391229391098 + 10.0 * 6.221311092376709
Epoch 1930, val loss: 1.0396053791046143
Epoch 1940, training loss: 62.22723388671875 = 0.02343660779297352 + 10.0 * 6.220379829406738
Epoch 1940, val loss: 1.0427377223968506
Epoch 1950, training loss: 62.24969482421875 = 0.02302395924925804 + 10.0 * 6.2226667404174805
Epoch 1950, val loss: 1.0454142093658447
Epoch 1960, training loss: 62.249324798583984 = 0.022616209462285042 + 10.0 * 6.222670555114746
Epoch 1960, val loss: 1.0478236675262451
Epoch 1970, training loss: 62.211219787597656 = 0.022210650146007538 + 10.0 * 6.21890115737915
Epoch 1970, val loss: 1.0506001710891724
Epoch 1980, training loss: 62.2072868347168 = 0.021825145930051804 + 10.0 * 6.218546390533447
Epoch 1980, val loss: 1.0533406734466553
Epoch 1990, training loss: 62.210533142089844 = 0.02145402878522873 + 10.0 * 6.218907833099365
Epoch 1990, val loss: 1.0561288595199585
Epoch 2000, training loss: 62.24986267089844 = 0.021092312410473824 + 10.0 * 6.222877025604248
Epoch 2000, val loss: 1.0587739944458008
Epoch 2010, training loss: 62.2307243347168 = 0.02073146589100361 + 10.0 * 6.220999240875244
Epoch 2010, val loss: 1.0616825819015503
Epoch 2020, training loss: 62.206172943115234 = 0.020375629886984825 + 10.0 * 6.2185797691345215
Epoch 2020, val loss: 1.0640162229537964
Epoch 2030, training loss: 62.195796966552734 = 0.020032504573464394 + 10.0 * 6.217576503753662
Epoch 2030, val loss: 1.0664204359054565
Epoch 2040, training loss: 62.18818664550781 = 0.019705679267644882 + 10.0 * 6.216847896575928
Epoch 2040, val loss: 1.0691865682601929
Epoch 2050, training loss: 62.20317459106445 = 0.019388463348150253 + 10.0 * 6.21837854385376
Epoch 2050, val loss: 1.071887731552124
Epoch 2060, training loss: 62.24408721923828 = 0.019072560593485832 + 10.0 * 6.222501277923584
Epoch 2060, val loss: 1.0742050409317017
Epoch 2070, training loss: 62.22576904296875 = 0.018762074410915375 + 10.0 * 6.220700740814209
Epoch 2070, val loss: 1.076915979385376
Epoch 2080, training loss: 62.189308166503906 = 0.018455153331160545 + 10.0 * 6.217085361480713
Epoch 2080, val loss: 1.0792582035064697
Epoch 2090, training loss: 62.18376922607422 = 0.018161138519644737 + 10.0 * 6.2165608406066895
Epoch 2090, val loss: 1.0817608833312988
Epoch 2100, training loss: 62.19147872924805 = 0.017881294712424278 + 10.0 * 6.217360019683838
Epoch 2100, val loss: 1.0842264890670776
Epoch 2110, training loss: 62.225406646728516 = 0.017603199928998947 + 10.0 * 6.220780372619629
Epoch 2110, val loss: 1.086961269378662
Epoch 2120, training loss: 62.20094299316406 = 0.017326386645436287 + 10.0 * 6.218361854553223
Epoch 2120, val loss: 1.0893925428390503
Epoch 2130, training loss: 62.18208312988281 = 0.01705564185976982 + 10.0 * 6.216502666473389
Epoch 2130, val loss: 1.0919634103775024
Epoch 2140, training loss: 62.19611740112305 = 0.016801342368125916 + 10.0 * 6.217931747436523
Epoch 2140, val loss: 1.0945605039596558
Epoch 2150, training loss: 62.17519760131836 = 0.016543976962566376 + 10.0 * 6.215865135192871
Epoch 2150, val loss: 1.096990942955017
Epoch 2160, training loss: 62.19820022583008 = 0.01629955694079399 + 10.0 * 6.2181901931762695
Epoch 2160, val loss: 1.0994242429733276
Epoch 2170, training loss: 62.16960906982422 = 0.01605203002691269 + 10.0 * 6.21535587310791
Epoch 2170, val loss: 1.101646900177002
Epoch 2180, training loss: 62.17877197265625 = 0.015814874321222305 + 10.0 * 6.2162957191467285
Epoch 2180, val loss: 1.1038480997085571
Epoch 2190, training loss: 62.19042205810547 = 0.015581388026475906 + 10.0 * 6.217483997344971
Epoch 2190, val loss: 1.1065196990966797
Epoch 2200, training loss: 62.16944122314453 = 0.015349631197750568 + 10.0 * 6.215409278869629
Epoch 2200, val loss: 1.108549952507019
Epoch 2210, training loss: 62.155662536621094 = 0.015128891915082932 + 10.0 * 6.214053153991699
Epoch 2210, val loss: 1.1112349033355713
Epoch 2220, training loss: 62.157928466796875 = 0.014916175045073032 + 10.0 * 6.214301109313965
Epoch 2220, val loss: 1.1137250661849976
Epoch 2230, training loss: 62.227352142333984 = 0.014707285910844803 + 10.0 * 6.221264839172363
Epoch 2230, val loss: 1.1160441637039185
Epoch 2240, training loss: 62.194087982177734 = 0.014491183683276176 + 10.0 * 6.217959403991699
Epoch 2240, val loss: 1.1177120208740234
Epoch 2250, training loss: 62.185123443603516 = 0.014284632168710232 + 10.0 * 6.217083930969238
Epoch 2250, val loss: 1.1203151941299438
Epoch 2260, training loss: 62.155860900878906 = 0.014079290442168713 + 10.0 * 6.214178085327148
Epoch 2260, val loss: 1.1224480867385864
Epoch 2270, training loss: 62.13914108276367 = 0.013889194466173649 + 10.0 * 6.212525367736816
Epoch 2270, val loss: 1.1249492168426514
Epoch 2280, training loss: 62.152584075927734 = 0.013704158365726471 + 10.0 * 6.213888168334961
Epoch 2280, val loss: 1.1269603967666626
Epoch 2290, training loss: 62.18132400512695 = 0.013515706174075603 + 10.0 * 6.216780662536621
Epoch 2290, val loss: 1.1292539834976196
Epoch 2300, training loss: 62.14406967163086 = 0.013328333385288715 + 10.0 * 6.213074207305908
Epoch 2300, val loss: 1.1315196752548218
Epoch 2310, training loss: 62.138545989990234 = 0.013151244260370731 + 10.0 * 6.212539196014404
Epoch 2310, val loss: 1.1336981058120728
Epoch 2320, training loss: 62.18025588989258 = 0.012979463674128056 + 10.0 * 6.2167277336120605
Epoch 2320, val loss: 1.1356838941574097
Epoch 2330, training loss: 62.14830780029297 = 0.0128037603572011 + 10.0 * 6.213550567626953
Epoch 2330, val loss: 1.138085961341858
Epoch 2340, training loss: 62.14779281616211 = 0.012636491097509861 + 10.0 * 6.213515281677246
Epoch 2340, val loss: 1.140365719795227
Epoch 2350, training loss: 62.13396072387695 = 0.012470649555325508 + 10.0 * 6.212149143218994
Epoch 2350, val loss: 1.1424764394760132
Epoch 2360, training loss: 62.17534637451172 = 0.012312068603932858 + 10.0 * 6.21630334854126
Epoch 2360, val loss: 1.144379734992981
Epoch 2370, training loss: 62.12554931640625 = 0.012149685062468052 + 10.0 * 6.211339950561523
Epoch 2370, val loss: 1.1467761993408203
Epoch 2380, training loss: 62.11928939819336 = 0.011994658969342709 + 10.0 * 6.210729598999023
Epoch 2380, val loss: 1.1485280990600586
Epoch 2390, training loss: 62.12776565551758 = 0.011846658773720264 + 10.0 * 6.211591720581055
Epoch 2390, val loss: 1.1507683992385864
Epoch 2400, training loss: 62.17535400390625 = 0.011699849739670753 + 10.0 * 6.216365337371826
Epoch 2400, val loss: 1.1527341604232788
Epoch 2410, training loss: 62.130165100097656 = 0.01155377458781004 + 10.0 * 6.2118611335754395
Epoch 2410, val loss: 1.1552610397338867
Epoch 2420, training loss: 62.1202507019043 = 0.011408191174268723 + 10.0 * 6.210884094238281
Epoch 2420, val loss: 1.15706467628479
Epoch 2430, training loss: 62.122039794921875 = 0.01126723550260067 + 10.0 * 6.2110772132873535
Epoch 2430, val loss: 1.1587274074554443
Epoch 2440, training loss: 62.146244049072266 = 0.011133351363241673 + 10.0 * 6.213510990142822
Epoch 2440, val loss: 1.1605045795440674
Epoch 2450, training loss: 62.1496467590332 = 0.010997849516570568 + 10.0 * 6.213864803314209
Epoch 2450, val loss: 1.1634044647216797
Epoch 2460, training loss: 62.126380920410156 = 0.010863302275538445 + 10.0 * 6.211551666259766
Epoch 2460, val loss: 1.1646819114685059
Epoch 2470, training loss: 62.11376953125 = 0.010732686147093773 + 10.0 * 6.21030330657959
Epoch 2470, val loss: 1.167326807975769
Epoch 2480, training loss: 62.10306167602539 = 0.01060898881405592 + 10.0 * 6.209245204925537
Epoch 2480, val loss: 1.169014573097229
Epoch 2490, training loss: 62.09722137451172 = 0.010487235151231289 + 10.0 * 6.208673477172852
Epoch 2490, val loss: 1.1713792085647583
Epoch 2500, training loss: 62.15021896362305 = 0.010370582342147827 + 10.0 * 6.213984489440918
Epoch 2500, val loss: 1.1732738018035889
Epoch 2510, training loss: 62.14790725708008 = 0.010245089419186115 + 10.0 * 6.213766098022461
Epoch 2510, val loss: 1.1747926473617554
Epoch 2520, training loss: 62.122032165527344 = 0.0101188775151968 + 10.0 * 6.211191177368164
Epoch 2520, val loss: 1.176741361618042
Epoch 2530, training loss: 62.105194091796875 = 0.010003510862588882 + 10.0 * 6.209519386291504
Epoch 2530, val loss: 1.1783363819122314
Epoch 2540, training loss: 62.09587860107422 = 0.00989264715462923 + 10.0 * 6.208598613739014
Epoch 2540, val loss: 1.1803518533706665
Epoch 2550, training loss: 62.12863540649414 = 0.009784296154975891 + 10.0 * 6.21188497543335
Epoch 2550, val loss: 1.1818925142288208
Epoch 2560, training loss: 62.094627380371094 = 0.009673717431724072 + 10.0 * 6.208495140075684
Epoch 2560, val loss: 1.1843748092651367
Epoch 2570, training loss: 62.097103118896484 = 0.009566562250256538 + 10.0 * 6.20875358581543
Epoch 2570, val loss: 1.1861728429794312
Epoch 2580, training loss: 62.09888458251953 = 0.009461955167353153 + 10.0 * 6.208942413330078
Epoch 2580, val loss: 1.1877706050872803
Epoch 2590, training loss: 62.12267303466797 = 0.009360581636428833 + 10.0 * 6.211331367492676
Epoch 2590, val loss: 1.1899526119232178
Epoch 2600, training loss: 62.13499069213867 = 0.009258468635380268 + 10.0 * 6.212573051452637
Epoch 2600, val loss: 1.1909704208374023
Epoch 2610, training loss: 62.08899688720703 = 0.009153054095804691 + 10.0 * 6.207984447479248
Epoch 2610, val loss: 1.1930158138275146
Epoch 2620, training loss: 62.08327865600586 = 0.00905591994524002 + 10.0 * 6.207422256469727
Epoch 2620, val loss: 1.1947373151779175
Epoch 2630, training loss: 62.09756851196289 = 0.00896298885345459 + 10.0 * 6.208860397338867
Epoch 2630, val loss: 1.1964588165283203
Epoch 2640, training loss: 62.10532760620117 = 0.008868077769875526 + 10.0 * 6.209645748138428
Epoch 2640, val loss: 1.1980791091918945
Epoch 2650, training loss: 62.09931182861328 = 0.008774255402386189 + 10.0 * 6.209053993225098
Epoch 2650, val loss: 1.1996792554855347
Epoch 2660, training loss: 62.094486236572266 = 0.008682559244334698 + 10.0 * 6.208580493927002
Epoch 2660, val loss: 1.2014094591140747
Epoch 2670, training loss: 62.07762908935547 = 0.008593760430812836 + 10.0 * 6.206903457641602
Epoch 2670, val loss: 1.2029409408569336
Epoch 2680, training loss: 62.07140350341797 = 0.008506384678184986 + 10.0 * 6.206289768218994
Epoch 2680, val loss: 1.2047847509384155
Epoch 2690, training loss: 62.126708984375 = 0.00842147134244442 + 10.0 * 6.211828708648682
Epoch 2690, val loss: 1.2058582305908203
Epoch 2700, training loss: 62.07600021362305 = 0.008335206657648087 + 10.0 * 6.206766605377197
Epoch 2700, val loss: 1.2083503007888794
Epoch 2710, training loss: 62.066627502441406 = 0.008247727528214455 + 10.0 * 6.205838203430176
Epoch 2710, val loss: 1.2096751928329468
Epoch 2720, training loss: 62.07373046875 = 0.008165629580616951 + 10.0 * 6.20655632019043
Epoch 2720, val loss: 1.211276888847351
Epoch 2730, training loss: 62.07786560058594 = 0.008087214082479477 + 10.0 * 6.206977844238281
Epoch 2730, val loss: 1.2129573822021484
Epoch 2740, training loss: 62.068782806396484 = 0.008007386699318886 + 10.0 * 6.206077575683594
Epoch 2740, val loss: 1.2144362926483154
Epoch 2750, training loss: 62.120567321777344 = 0.007934307679533958 + 10.0 * 6.211263179779053
Epoch 2750, val loss: 1.2156912088394165
Epoch 2760, training loss: 62.11467742919922 = 0.0078506488353014 + 10.0 * 6.2106828689575195
Epoch 2760, val loss: 1.2172414064407349
Epoch 2770, training loss: 62.07624816894531 = 0.007773879915475845 + 10.0 * 6.206847190856934
Epoch 2770, val loss: 1.2187188863754272
Epoch 2780, training loss: 62.05699920654297 = 0.007698428351432085 + 10.0 * 6.204930305480957
Epoch 2780, val loss: 1.2206469774246216
Epoch 2790, training loss: 62.05323028564453 = 0.0076296040788292885 + 10.0 * 6.204560279846191
Epoch 2790, val loss: 1.2218682765960693
Epoch 2800, training loss: 62.0834846496582 = 0.00756077840924263 + 10.0 * 6.207592487335205
Epoch 2800, val loss: 1.2234102487564087
Epoch 2810, training loss: 62.05313491821289 = 0.007485751993954182 + 10.0 * 6.204565048217773
Epoch 2810, val loss: 1.2249131202697754
Epoch 2820, training loss: 62.046077728271484 = 0.007413399405777454 + 10.0 * 6.203866481781006
Epoch 2820, val loss: 1.2261097431182861
Epoch 2830, training loss: 62.07518768310547 = 0.007347023114562035 + 10.0 * 6.206784248352051
Epoch 2830, val loss: 1.227487564086914
Epoch 2840, training loss: 62.07158660888672 = 0.007279596757143736 + 10.0 * 6.206430912017822
Epoch 2840, val loss: 1.228959321975708
Epoch 2850, training loss: 62.066139221191406 = 0.007210114970803261 + 10.0 * 6.205893039703369
Epoch 2850, val loss: 1.230530858039856
Epoch 2860, training loss: 62.08945846557617 = 0.007143577095121145 + 10.0 * 6.208231449127197
Epoch 2860, val loss: 1.2317540645599365
Epoch 2870, training loss: 62.039093017578125 = 0.007078831549733877 + 10.0 * 6.2032012939453125
Epoch 2870, val loss: 1.2334926128387451
Epoch 2880, training loss: 62.04262924194336 = 0.007016688119620085 + 10.0 * 6.203561305999756
Epoch 2880, val loss: 1.23479425907135
Epoch 2890, training loss: 62.0521354675293 = 0.006956062745302916 + 10.0 * 6.2045183181762695
Epoch 2890, val loss: 1.2359787225723267
Epoch 2900, training loss: 62.11805725097656 = 0.006893443409353495 + 10.0 * 6.211116313934326
Epoch 2900, val loss: 1.2369359731674194
Epoch 2910, training loss: 62.06488037109375 = 0.0068312943913042545 + 10.0 * 6.205804824829102
Epoch 2910, val loss: 1.2383909225463867
Epoch 2920, training loss: 62.04344177246094 = 0.006769756320863962 + 10.0 * 6.203667163848877
Epoch 2920, val loss: 1.2401916980743408
Epoch 2930, training loss: 62.03490447998047 = 0.006713963113725185 + 10.0 * 6.202818870544434
Epoch 2930, val loss: 1.2415456771850586
Epoch 2940, training loss: 62.04634094238281 = 0.006658551748842001 + 10.0 * 6.203968524932861
Epoch 2940, val loss: 1.243003249168396
Epoch 2950, training loss: 62.08017349243164 = 0.006601513363420963 + 10.0 * 6.207356929779053
Epoch 2950, val loss: 1.244214415550232
Epoch 2960, training loss: 62.05845260620117 = 0.006543129216879606 + 10.0 * 6.205191135406494
Epoch 2960, val loss: 1.2457313537597656
Epoch 2970, training loss: 62.082881927490234 = 0.0064864917658269405 + 10.0 * 6.207639694213867
Epoch 2970, val loss: 1.2466801404953003
Epoch 2980, training loss: 62.04402542114258 = 0.006428592372685671 + 10.0 * 6.203759670257568
Epoch 2980, val loss: 1.2472786903381348
Epoch 2990, training loss: 62.0338134765625 = 0.006375284865498543 + 10.0 * 6.202744007110596
Epoch 2990, val loss: 1.2491861581802368
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 87.93071746826172 = 1.9623245000839233 + 10.0 * 8.59683895111084
Epoch 0, val loss: 1.965154767036438
Epoch 10, training loss: 87.91421508789062 = 1.9522942304611206 + 10.0 * 8.596192359924316
Epoch 10, val loss: 1.9547311067581177
Epoch 20, training loss: 87.85250854492188 = 1.9405312538146973 + 10.0 * 8.591197967529297
Epoch 20, val loss: 1.9422935247421265
Epoch 30, training loss: 87.452392578125 = 1.9260673522949219 + 10.0 * 8.552632331848145
Epoch 30, val loss: 1.9268743991851807
Epoch 40, training loss: 84.95833587646484 = 1.90818190574646 + 10.0 * 8.305015563964844
Epoch 40, val loss: 1.9077978134155273
Epoch 50, training loss: 78.37510681152344 = 1.8878507614135742 + 10.0 * 7.648725986480713
Epoch 50, val loss: 1.8870173692703247
Epoch 60, training loss: 74.56573486328125 = 1.872925043106079 + 10.0 * 7.269281387329102
Epoch 60, val loss: 1.8726664781570435
Epoch 70, training loss: 72.39502716064453 = 1.8610044717788696 + 10.0 * 7.053401947021484
Epoch 70, val loss: 1.8605681657791138
Epoch 80, training loss: 71.00900268554688 = 1.8487282991409302 + 10.0 * 6.916027069091797
Epoch 80, val loss: 1.8486990928649902
Epoch 90, training loss: 69.82003021240234 = 1.83784818649292 + 10.0 * 6.798218727111816
Epoch 90, val loss: 1.8381292819976807
Epoch 100, training loss: 69.06400299072266 = 1.8271442651748657 + 10.0 * 6.723686218261719
Epoch 100, val loss: 1.8279318809509277
Epoch 110, training loss: 68.46691131591797 = 1.8169841766357422 + 10.0 * 6.664992809295654
Epoch 110, val loss: 1.8184397220611572
Epoch 120, training loss: 68.0062026977539 = 1.8077678680419922 + 10.0 * 6.619843482971191
Epoch 120, val loss: 1.8098785877227783
Epoch 130, training loss: 67.67147064208984 = 1.7991151809692383 + 10.0 * 6.587235450744629
Epoch 130, val loss: 1.8017864227294922
Epoch 140, training loss: 67.32408905029297 = 1.7905359268188477 + 10.0 * 6.5533552169799805
Epoch 140, val loss: 1.79366135597229
Epoch 150, training loss: 67.06275939941406 = 1.7819334268569946 + 10.0 * 6.528082370758057
Epoch 150, val loss: 1.7855626344680786
Epoch 160, training loss: 66.85338592529297 = 1.7730610370635986 + 10.0 * 6.508032321929932
Epoch 160, val loss: 1.7773650884628296
Epoch 170, training loss: 66.6318588256836 = 1.7638049125671387 + 10.0 * 6.486805438995361
Epoch 170, val loss: 1.7688796520233154
Epoch 180, training loss: 66.51170349121094 = 1.7539788484573364 + 10.0 * 6.475772380828857
Epoch 180, val loss: 1.7599655389785767
Epoch 190, training loss: 66.31490325927734 = 1.7434086799621582 + 10.0 * 6.457149505615234
Epoch 190, val loss: 1.7504554986953735
Epoch 200, training loss: 66.1619873046875 = 1.7320443391799927 + 10.0 * 6.442994117736816
Epoch 200, val loss: 1.7403435707092285
Epoch 210, training loss: 66.02584838867188 = 1.7197706699371338 + 10.0 * 6.430607795715332
Epoch 210, val loss: 1.7294940948486328
Epoch 220, training loss: 65.93737030029297 = 1.7063469886779785 + 10.0 * 6.423102378845215
Epoch 220, val loss: 1.7177356481552124
Epoch 230, training loss: 65.79824829101562 = 1.6917588710784912 + 10.0 * 6.410649299621582
Epoch 230, val loss: 1.7049496173858643
Epoch 240, training loss: 65.6763687133789 = 1.6760121583938599 + 10.0 * 6.400035858154297
Epoch 240, val loss: 1.6911662817001343
Epoch 250, training loss: 65.64881896972656 = 1.6589382886886597 + 10.0 * 6.398988246917725
Epoch 250, val loss: 1.6762374639511108
Epoch 260, training loss: 65.49576568603516 = 1.6402233839035034 + 10.0 * 6.385554313659668
Epoch 260, val loss: 1.6599256992340088
Epoch 270, training loss: 65.4006576538086 = 1.6201645135879517 + 10.0 * 6.378049373626709
Epoch 270, val loss: 1.6423982381820679
Epoch 280, training loss: 65.30994415283203 = 1.5985244512557983 + 10.0 * 6.3711419105529785
Epoch 280, val loss: 1.623559594154358
Epoch 290, training loss: 65.23011779785156 = 1.5752655267715454 + 10.0 * 6.365485191345215
Epoch 290, val loss: 1.6033341884613037
Epoch 300, training loss: 65.18733215332031 = 1.5503547191619873 + 10.0 * 6.3636980056762695
Epoch 300, val loss: 1.581679344177246
Epoch 310, training loss: 65.08871459960938 = 1.5238192081451416 + 10.0 * 6.356489181518555
Epoch 310, val loss: 1.5585993528366089
Epoch 320, training loss: 65.0015640258789 = 1.4959074258804321 + 10.0 * 6.350565433502197
Epoch 320, val loss: 1.5343592166900635
Epoch 330, training loss: 64.9300537109375 = 1.466575264930725 + 10.0 * 6.346348285675049
Epoch 330, val loss: 1.508965253829956
Epoch 340, training loss: 64.9092025756836 = 1.4359225034713745 + 10.0 * 6.347328186035156
Epoch 340, val loss: 1.48246169090271
Epoch 350, training loss: 64.81481170654297 = 1.4041262865066528 + 10.0 * 6.341068267822266
Epoch 350, val loss: 1.455057978630066
Epoch 360, training loss: 64.73243713378906 = 1.3714278936386108 + 10.0 * 6.336101055145264
Epoch 360, val loss: 1.4270442724227905
Epoch 370, training loss: 64.6821060180664 = 1.3380216360092163 + 10.0 * 6.334408283233643
Epoch 370, val loss: 1.398429036140442
Epoch 380, training loss: 64.61675262451172 = 1.3039435148239136 + 10.0 * 6.3312811851501465
Epoch 380, val loss: 1.3695085048675537
Epoch 390, training loss: 64.53226470947266 = 1.2695518732070923 + 10.0 * 6.326271057128906
Epoch 390, val loss: 1.3402940034866333
Epoch 400, training loss: 64.57585144042969 = 1.2349845170974731 + 10.0 * 6.334086894989014
Epoch 400, val loss: 1.311077356338501
Epoch 410, training loss: 64.41828918457031 = 1.200005054473877 + 10.0 * 6.321828365325928
Epoch 410, val loss: 1.2817519903182983
Epoch 420, training loss: 64.3536605834961 = 1.1653178930282593 + 10.0 * 6.318833827972412
Epoch 420, val loss: 1.2527251243591309
Epoch 430, training loss: 64.29264068603516 = 1.1311038732528687 + 10.0 * 6.316153526306152
Epoch 430, val loss: 1.2244044542312622
Epoch 440, training loss: 64.25338745117188 = 1.097472071647644 + 10.0 * 6.315591812133789
Epoch 440, val loss: 1.196836233139038
Epoch 450, training loss: 64.19538879394531 = 1.064547061920166 + 10.0 * 6.313084125518799
Epoch 450, val loss: 1.1701735258102417
Epoch 460, training loss: 64.1297607421875 = 1.0325924158096313 + 10.0 * 6.309716701507568
Epoch 460, val loss: 1.1444483995437622
Epoch 470, training loss: 64.08201599121094 = 1.001663327217102 + 10.0 * 6.308035373687744
Epoch 470, val loss: 1.1199828386306763
Epoch 480, training loss: 64.0290298461914 = 0.971613347530365 + 10.0 * 6.305741786956787
Epoch 480, val loss: 1.0964913368225098
Epoch 490, training loss: 63.98372268676758 = 0.9428565502166748 + 10.0 * 6.304086685180664
Epoch 490, val loss: 1.0743954181671143
Epoch 500, training loss: 63.93512725830078 = 0.9152372479438782 + 10.0 * 6.3019890785217285
Epoch 500, val loss: 1.053513765335083
Epoch 510, training loss: 63.9054069519043 = 0.888708770275116 + 10.0 * 6.301669597625732
Epoch 510, val loss: 1.0338464975357056
Epoch 520, training loss: 63.869693756103516 = 0.8630081415176392 + 10.0 * 6.300668716430664
Epoch 520, val loss: 1.0150350332260132
Epoch 530, training loss: 63.8022346496582 = 0.8384639024734497 + 10.0 * 6.296377182006836
Epoch 530, val loss: 0.9975872039794922
Epoch 540, training loss: 63.764488220214844 = 0.8148860931396484 + 10.0 * 6.2949604988098145
Epoch 540, val loss: 0.981133222579956
Epoch 550, training loss: 63.7454719543457 = 0.7921720147132874 + 10.0 * 6.295330047607422
Epoch 550, val loss: 0.9657298922538757
Epoch 560, training loss: 63.715702056884766 = 0.7701124548912048 + 10.0 * 6.294559001922607
Epoch 560, val loss: 0.9511717557907104
Epoch 570, training loss: 63.64956283569336 = 0.7489575147628784 + 10.0 * 6.290060520172119
Epoch 570, val loss: 0.9374895691871643
Epoch 580, training loss: 63.61692428588867 = 0.7285351157188416 + 10.0 * 6.288838863372803
Epoch 580, val loss: 0.9248301386833191
Epoch 590, training loss: 63.59187698364258 = 0.7087651491165161 + 10.0 * 6.288311004638672
Epoch 590, val loss: 0.9130156636238098
Epoch 600, training loss: 63.55414581298828 = 0.689454972743988 + 10.0 * 6.286469459533691
Epoch 600, val loss: 0.9018239974975586
Epoch 610, training loss: 63.5265998840332 = 0.6708092093467712 + 10.0 * 6.285578727722168
Epoch 610, val loss: 0.8913516402244568
Epoch 620, training loss: 63.4851188659668 = 0.6527038812637329 + 10.0 * 6.2832417488098145
Epoch 620, val loss: 0.8817241191864014
Epoch 630, training loss: 63.46137619018555 = 0.6350591778755188 + 10.0 * 6.282631874084473
Epoch 630, val loss: 0.872828483581543
Epoch 640, training loss: 63.440128326416016 = 0.6177915334701538 + 10.0 * 6.282233715057373
Epoch 640, val loss: 0.8642637729644775
Epoch 650, training loss: 63.41575622558594 = 0.6009443998336792 + 10.0 * 6.2814812660217285
Epoch 650, val loss: 0.8564907908439636
Epoch 660, training loss: 63.36759567260742 = 0.5845448970794678 + 10.0 * 6.2783050537109375
Epoch 660, val loss: 0.8491820096969604
Epoch 670, training loss: 63.35922622680664 = 0.568516731262207 + 10.0 * 6.279070854187012
Epoch 670, val loss: 0.842553973197937
Epoch 680, training loss: 63.327369689941406 = 0.5528329014778137 + 10.0 * 6.277453422546387
Epoch 680, val loss: 0.8365020751953125
Epoch 690, training loss: 63.29607009887695 = 0.5374914407730103 + 10.0 * 6.275857925415039
Epoch 690, val loss: 0.8310064673423767
Epoch 700, training loss: 63.26476287841797 = 0.522468090057373 + 10.0 * 6.274229526519775
Epoch 700, val loss: 0.8259180784225464
Epoch 710, training loss: 63.255455017089844 = 0.5077506303787231 + 10.0 * 6.274770259857178
Epoch 710, val loss: 0.8213568329811096
Epoch 720, training loss: 63.21004104614258 = 0.49332505464553833 + 10.0 * 6.271671772003174
Epoch 720, val loss: 0.8173586130142212
Epoch 730, training loss: 63.2099494934082 = 0.4791824519634247 + 10.0 * 6.27307653427124
Epoch 730, val loss: 0.8137489557266235
Epoch 740, training loss: 63.166473388671875 = 0.46528539061546326 + 10.0 * 6.270118713378906
Epoch 740, val loss: 0.8104159832000732
Epoch 750, training loss: 63.169189453125 = 0.451667845249176 + 10.0 * 6.27175235748291
Epoch 750, val loss: 0.8078606724739075
Epoch 760, training loss: 63.13994598388672 = 0.43827247619628906 + 10.0 * 6.270167350769043
Epoch 760, val loss: 0.8052188158035278
Epoch 770, training loss: 63.094303131103516 = 0.4251948893070221 + 10.0 * 6.266911029815674
Epoch 770, val loss: 0.8033825159072876
Epoch 780, training loss: 63.067928314208984 = 0.4124436378479004 + 10.0 * 6.265548229217529
Epoch 780, val loss: 0.8020339608192444
Epoch 790, training loss: 63.06358337402344 = 0.39996522665023804 + 10.0 * 6.266362190246582
Epoch 790, val loss: 0.8010392785072327
Epoch 800, training loss: 63.02112579345703 = 0.38770991563796997 + 10.0 * 6.263341426849365
Epoch 800, val loss: 0.8002868890762329
Epoch 810, training loss: 63.00761413574219 = 0.3757573366165161 + 10.0 * 6.263185501098633
Epoch 810, val loss: 0.8000649213790894
Epoch 820, training loss: 63.017784118652344 = 0.36413103342056274 + 10.0 * 6.265365123748779
Epoch 820, val loss: 0.8001344799995422
Epoch 830, training loss: 62.96891403198242 = 0.3527655005455017 + 10.0 * 6.261614799499512
Epoch 830, val loss: 0.8005650639533997
Epoch 840, training loss: 62.936737060546875 = 0.341767817735672 + 10.0 * 6.259497165679932
Epoch 840, val loss: 0.8013674020767212
Epoch 850, training loss: 62.920082092285156 = 0.33109262585639954 + 10.0 * 6.258898735046387
Epoch 850, val loss: 0.8026680946350098
Epoch 860, training loss: 62.985347747802734 = 0.32073619961738586 + 10.0 * 6.266461372375488
Epoch 860, val loss: 0.8042690753936768
Epoch 870, training loss: 62.904029846191406 = 0.31056395173072815 + 10.0 * 6.259346961975098
Epoch 870, val loss: 0.8058662414550781
Epoch 880, training loss: 62.87471008300781 = 0.3008185029029846 + 10.0 * 6.257389068603516
Epoch 880, val loss: 0.8079034686088562
Epoch 890, training loss: 62.848350524902344 = 0.29142993688583374 + 10.0 * 6.255692005157471
Epoch 890, val loss: 0.8105034828186035
Epoch 900, training loss: 62.8403434753418 = 0.282369464635849 + 10.0 * 6.255797386169434
Epoch 900, val loss: 0.8134915232658386
Epoch 910, training loss: 62.83259582519531 = 0.2736019790172577 + 10.0 * 6.255899429321289
Epoch 910, val loss: 0.8166560530662537
Epoch 920, training loss: 62.81907272338867 = 0.2651354968547821 + 10.0 * 6.2553935050964355
Epoch 920, val loss: 0.8201801180839539
Epoch 930, training loss: 62.800148010253906 = 0.25698792934417725 + 10.0 * 6.2543158531188965
Epoch 930, val loss: 0.8237699270248413
Epoch 940, training loss: 62.80829620361328 = 0.24909217655658722 + 10.0 * 6.25592041015625
Epoch 940, val loss: 0.827576220035553
Epoch 950, training loss: 62.76202392578125 = 0.2415781021118164 + 10.0 * 6.252044677734375
Epoch 950, val loss: 0.8317702412605286
Epoch 960, training loss: 62.73764419555664 = 0.23432497680187225 + 10.0 * 6.250331878662109
Epoch 960, val loss: 0.8364177346229553
Epoch 970, training loss: 62.72684097290039 = 0.22737348079681396 + 10.0 * 6.249946594238281
Epoch 970, val loss: 0.8412208557128906
Epoch 980, training loss: 62.769187927246094 = 0.22066457569599152 + 10.0 * 6.254852294921875
Epoch 980, val loss: 0.8463187217712402
Epoch 990, training loss: 62.69590759277344 = 0.21412567794322968 + 10.0 * 6.248178005218506
Epoch 990, val loss: 0.8513123989105225
Epoch 1000, training loss: 62.68096923828125 = 0.2078995555639267 + 10.0 * 6.247306823730469
Epoch 1000, val loss: 0.8566999435424805
Epoch 1010, training loss: 62.67813491821289 = 0.20190761983394623 + 10.0 * 6.247622489929199
Epoch 1010, val loss: 0.8623427748680115
Epoch 1020, training loss: 62.69548797607422 = 0.19609306752681732 + 10.0 * 6.249939441680908
Epoch 1020, val loss: 0.8681617379188538
Epoch 1030, training loss: 62.6461181640625 = 0.19049036502838135 + 10.0 * 6.245562553405762
Epoch 1030, val loss: 0.874071478843689
Epoch 1040, training loss: 62.667381286621094 = 0.18512080609798431 + 10.0 * 6.248226165771484
Epoch 1040, val loss: 0.8801640272140503
Epoch 1050, training loss: 62.62303924560547 = 0.17993076145648956 + 10.0 * 6.2443108558654785
Epoch 1050, val loss: 0.8863047361373901
Epoch 1060, training loss: 62.62166213989258 = 0.1749195009469986 + 10.0 * 6.244674205780029
Epoch 1060, val loss: 0.8926750421524048
Epoch 1070, training loss: 62.62417221069336 = 0.17009951174259186 + 10.0 * 6.2454071044921875
Epoch 1070, val loss: 0.8992541432380676
Epoch 1080, training loss: 62.602535247802734 = 0.1654326617717743 + 10.0 * 6.243710517883301
Epoch 1080, val loss: 0.9061705470085144
Epoch 1090, training loss: 62.582054138183594 = 0.160920187830925 + 10.0 * 6.2421135902404785
Epoch 1090, val loss: 0.9131358861923218
Epoch 1100, training loss: 62.57582092285156 = 0.15655790269374847 + 10.0 * 6.241926193237305
Epoch 1100, val loss: 0.920021116733551
Epoch 1110, training loss: 62.60823440551758 = 0.15235388278961182 + 10.0 * 6.2455878257751465
Epoch 1110, val loss: 0.9274821281433105
Epoch 1120, training loss: 62.56270980834961 = 0.14825105667114258 + 10.0 * 6.241446018218994
Epoch 1120, val loss: 0.9342782497406006
Epoch 1130, training loss: 62.538055419921875 = 0.14429455995559692 + 10.0 * 6.239376068115234
Epoch 1130, val loss: 0.9417058229446411
Epoch 1140, training loss: 62.53172302246094 = 0.14048774540424347 + 10.0 * 6.239123344421387
Epoch 1140, val loss: 0.9489619731903076
Epoch 1150, training loss: 62.55827331542969 = 0.13680966198444366 + 10.0 * 6.2421464920043945
Epoch 1150, val loss: 0.9566237926483154
Epoch 1160, training loss: 62.52339553833008 = 0.13321202993392944 + 10.0 * 6.239018440246582
Epoch 1160, val loss: 0.9640123248100281
Epoch 1170, training loss: 62.51142120361328 = 0.12973487377166748 + 10.0 * 6.238168716430664
Epoch 1170, val loss: 0.9715485572814941
Epoch 1180, training loss: 62.515750885009766 = 0.12636806070804596 + 10.0 * 6.238938331604004
Epoch 1180, val loss: 0.9793881177902222
Epoch 1190, training loss: 62.48878860473633 = 0.12310781329870224 + 10.0 * 6.236567974090576
Epoch 1190, val loss: 0.9868795871734619
Epoch 1200, training loss: 62.497093200683594 = 0.1199597492814064 + 10.0 * 6.23771333694458
Epoch 1200, val loss: 0.9946108460426331
Epoch 1210, training loss: 62.48472213745117 = 0.11688221246004105 + 10.0 * 6.236783981323242
Epoch 1210, val loss: 1.0022375583648682
Epoch 1220, training loss: 62.46796417236328 = 0.1138855591416359 + 10.0 * 6.235407829284668
Epoch 1220, val loss: 1.009621262550354
Epoch 1230, training loss: 62.472537994384766 = 0.11100360751152039 + 10.0 * 6.236153602600098
Epoch 1230, val loss: 1.0172687768936157
Epoch 1240, training loss: 62.443878173828125 = 0.10820911824703217 + 10.0 * 6.233567237854004
Epoch 1240, val loss: 1.0248630046844482
Epoch 1250, training loss: 62.44017791748047 = 0.10549849271774292 + 10.0 * 6.233468055725098
Epoch 1250, val loss: 1.0325069427490234
Epoch 1260, training loss: 62.463924407958984 = 0.10287711769342422 + 10.0 * 6.236104488372803
Epoch 1260, val loss: 1.0402084589004517
Epoch 1270, training loss: 62.44560623168945 = 0.10029920190572739 + 10.0 * 6.234530448913574
Epoch 1270, val loss: 1.0476720333099365
Epoch 1280, training loss: 62.44746780395508 = 0.09780500084161758 + 10.0 * 6.234966278076172
Epoch 1280, val loss: 1.0550498962402344
Epoch 1290, training loss: 62.43513107299805 = 0.09536504000425339 + 10.0 * 6.2339768409729
Epoch 1290, val loss: 1.0623210668563843
Epoch 1300, training loss: 62.40960693359375 = 0.09302050620317459 + 10.0 * 6.231658458709717
Epoch 1300, val loss: 1.0699589252471924
Epoch 1310, training loss: 62.388797760009766 = 0.09075164794921875 + 10.0 * 6.229804515838623
Epoch 1310, val loss: 1.0774258375167847
Epoch 1320, training loss: 62.385005950927734 = 0.08856123685836792 + 10.0 * 6.229644298553467
Epoch 1320, val loss: 1.084816813468933
Epoch 1330, training loss: 62.44590759277344 = 0.08643865585327148 + 10.0 * 6.235947132110596
Epoch 1330, val loss: 1.091883897781372
Epoch 1340, training loss: 62.41377258300781 = 0.08434493839740753 + 10.0 * 6.232942581176758
Epoch 1340, val loss: 1.0997995138168335
Epoch 1350, training loss: 62.3809928894043 = 0.08230864256620407 + 10.0 * 6.229868412017822
Epoch 1350, val loss: 1.106351375579834
Epoch 1360, training loss: 62.36470413208008 = 0.08035510778427124 + 10.0 * 6.2284345626831055
Epoch 1360, val loss: 1.1139357089996338
Epoch 1370, training loss: 62.38475799560547 = 0.07846644520759583 + 10.0 * 6.230628967285156
Epoch 1370, val loss: 1.1212375164031982
Epoch 1380, training loss: 62.37095642089844 = 0.07660943269729614 + 10.0 * 6.229434490203857
Epoch 1380, val loss: 1.1281840801239014
Epoch 1390, training loss: 62.34459686279297 = 0.07480578869581223 + 10.0 * 6.2269792556762695
Epoch 1390, val loss: 1.1352988481521606
Epoch 1400, training loss: 62.35619354248047 = 0.07306098192930222 + 10.0 * 6.228313446044922
Epoch 1400, val loss: 1.1423317193984985
Epoch 1410, training loss: 62.33696365356445 = 0.07136800140142441 + 10.0 * 6.226559638977051
Epoch 1410, val loss: 1.1494697332382202
Epoch 1420, training loss: 62.336483001708984 = 0.06973379850387573 + 10.0 * 6.226675033569336
Epoch 1420, val loss: 1.1564128398895264
Epoch 1430, training loss: 62.37281799316406 = 0.06814458966255188 + 10.0 * 6.230467319488525
Epoch 1430, val loss: 1.1634093523025513
Epoch 1440, training loss: 62.322906494140625 = 0.06656928360462189 + 10.0 * 6.22563362121582
Epoch 1440, val loss: 1.1697359085083008
Epoch 1450, training loss: 62.30720138549805 = 0.0650571882724762 + 10.0 * 6.224214553833008
Epoch 1450, val loss: 1.176486611366272
Epoch 1460, training loss: 62.3068962097168 = 0.06360331177711487 + 10.0 * 6.224329471588135
Epoch 1460, val loss: 1.1832661628723145
Epoch 1470, training loss: 62.363895416259766 = 0.06219080463051796 + 10.0 * 6.230170249938965
Epoch 1470, val loss: 1.1898335218429565
Epoch 1480, training loss: 62.32353210449219 = 0.060790445655584335 + 10.0 * 6.226274013519287
Epoch 1480, val loss: 1.1961994171142578
Epoch 1490, training loss: 62.29770278930664 = 0.05944536626338959 + 10.0 * 6.223825931549072
Epoch 1490, val loss: 1.2025492191314697
Epoch 1500, training loss: 62.28594207763672 = 0.05814072862267494 + 10.0 * 6.222780227661133
Epoch 1500, val loss: 1.2092645168304443
Epoch 1510, training loss: 62.323665618896484 = 0.0568869486451149 + 10.0 * 6.226677894592285
Epoch 1510, val loss: 1.215833306312561
Epoch 1520, training loss: 62.30290222167969 = 0.05564245209097862 + 10.0 * 6.22472620010376
Epoch 1520, val loss: 1.2217028141021729
Epoch 1530, training loss: 62.293670654296875 = 0.054429907351732254 + 10.0 * 6.223924160003662
Epoch 1530, val loss: 1.227968454360962
Epoch 1540, training loss: 62.271202087402344 = 0.05325889587402344 + 10.0 * 6.221794128417969
Epoch 1540, val loss: 1.2342959642410278
Epoch 1550, training loss: 62.26393508911133 = 0.052133701741695404 + 10.0 * 6.221179962158203
Epoch 1550, val loss: 1.2406797409057617
Epoch 1560, training loss: 62.32912063598633 = 0.0510391965508461 + 10.0 * 6.227807998657227
Epoch 1560, val loss: 1.2468998432159424
Epoch 1570, training loss: 62.27040100097656 = 0.04995572939515114 + 10.0 * 6.222044467926025
Epoch 1570, val loss: 1.2526755332946777
Epoch 1580, training loss: 62.25773620605469 = 0.048906028270721436 + 10.0 * 6.220883369445801
Epoch 1580, val loss: 1.258910059928894
Epoch 1590, training loss: 62.30401611328125 = 0.047897420823574066 + 10.0 * 6.225611686706543
Epoch 1590, val loss: 1.2650128602981567
Epoch 1600, training loss: 62.25364303588867 = 0.0469011627137661 + 10.0 * 6.22067403793335
Epoch 1600, val loss: 1.2707017660140991
Epoch 1610, training loss: 62.24158477783203 = 0.04593687132000923 + 10.0 * 6.219564914703369
Epoch 1610, val loss: 1.2767225503921509
Epoch 1620, training loss: 62.236995697021484 = 0.04500795900821686 + 10.0 * 6.219198703765869
Epoch 1620, val loss: 1.2826366424560547
Epoch 1630, training loss: 62.298561096191406 = 0.04411537945270538 + 10.0 * 6.225444316864014
Epoch 1630, val loss: 1.2887697219848633
Epoch 1640, training loss: 62.25950622558594 = 0.04321088269352913 + 10.0 * 6.221629619598389
Epoch 1640, val loss: 1.294081687927246
Epoch 1650, training loss: 62.235870361328125 = 0.042342014610767365 + 10.0 * 6.219352722167969
Epoch 1650, val loss: 1.299796462059021
Epoch 1660, training loss: 62.22772216796875 = 0.04150328412652016 + 10.0 * 6.218621730804443
Epoch 1660, val loss: 1.3052934408187866
Epoch 1670, training loss: 62.233055114746094 = 0.04069391265511513 + 10.0 * 6.219235897064209
Epoch 1670, val loss: 1.3111484050750732
Epoch 1680, training loss: 62.239707946777344 = 0.039905305951833725 + 10.0 * 6.219980239868164
Epoch 1680, val loss: 1.3168013095855713
Epoch 1690, training loss: 62.251495361328125 = 0.039127714931964874 + 10.0 * 6.221236705780029
Epoch 1690, val loss: 1.3223515748977661
Epoch 1700, training loss: 62.221858978271484 = 0.038363561034202576 + 10.0 * 6.218349456787109
Epoch 1700, val loss: 1.3273547887802124
Epoch 1710, training loss: 62.204498291015625 = 0.037629347294569016 + 10.0 * 6.216687202453613
Epoch 1710, val loss: 1.333134651184082
Epoch 1720, training loss: 62.23613357543945 = 0.03691545128822327 + 10.0 * 6.219922065734863
Epoch 1720, val loss: 1.3383806943893433
Epoch 1730, training loss: 62.19926452636719 = 0.03621557727456093 + 10.0 * 6.216304779052734
Epoch 1730, val loss: 1.3437405824661255
Epoch 1740, training loss: 62.20737838745117 = 0.03552377596497536 + 10.0 * 6.2171854972839355
Epoch 1740, val loss: 1.348878026008606
Epoch 1750, training loss: 62.194889068603516 = 0.03486943989992142 + 10.0 * 6.216001987457275
Epoch 1750, val loss: 1.354284644126892
Epoch 1760, training loss: 62.201663970947266 = 0.03423076868057251 + 10.0 * 6.216743469238281
Epoch 1760, val loss: 1.3594694137573242
Epoch 1770, training loss: 62.217193603515625 = 0.03360384330153465 + 10.0 * 6.218358993530273
Epoch 1770, val loss: 1.364609956741333
Epoch 1780, training loss: 62.20024871826172 = 0.032982438802719116 + 10.0 * 6.216726779937744
Epoch 1780, val loss: 1.3693974018096924
Epoch 1790, training loss: 62.19779968261719 = 0.032386280596256256 + 10.0 * 6.216541290283203
Epoch 1790, val loss: 1.3745603561401367
Epoch 1800, training loss: 62.193885803222656 = 0.031807247549295425 + 10.0 * 6.216207981109619
Epoch 1800, val loss: 1.3794620037078857
Epoch 1810, training loss: 62.20656967163086 = 0.031240763142704964 + 10.0 * 6.217532634735107
Epoch 1810, val loss: 1.3843762874603271
Epoch 1820, training loss: 62.193111419677734 = 0.03068213164806366 + 10.0 * 6.216242790222168
Epoch 1820, val loss: 1.389237880706787
Epoch 1830, training loss: 62.17167282104492 = 0.030143290758132935 + 10.0 * 6.214152812957764
Epoch 1830, val loss: 1.394291639328003
Epoch 1840, training loss: 62.16427993774414 = 0.029616333544254303 + 10.0 * 6.213466167449951
Epoch 1840, val loss: 1.3992289304733276
Epoch 1850, training loss: 62.16067886352539 = 0.029106397181749344 + 10.0 * 6.2131571769714355
Epoch 1850, val loss: 1.4039701223373413
Epoch 1860, training loss: 62.184478759765625 = 0.02861104905605316 + 10.0 * 6.2155866622924805
Epoch 1860, val loss: 1.408661961555481
Epoch 1870, training loss: 62.19943618774414 = 0.028119146823883057 + 10.0 * 6.217131614685059
Epoch 1870, val loss: 1.4132789373397827
Epoch 1880, training loss: 62.16147994995117 = 0.027631403878331184 + 10.0 * 6.213385105133057
Epoch 1880, val loss: 1.4176851511001587
Epoch 1890, training loss: 62.159664154052734 = 0.02715839259326458 + 10.0 * 6.213250637054443
Epoch 1890, val loss: 1.4223225116729736
Epoch 1900, training loss: 62.150169372558594 = 0.026704587042331696 + 10.0 * 6.21234655380249
Epoch 1900, val loss: 1.427071213722229
Epoch 1910, training loss: 62.14387512207031 = 0.026264138519763947 + 10.0 * 6.211760997772217
Epoch 1910, val loss: 1.4317289590835571
Epoch 1920, training loss: 62.18767547607422 = 0.025840487331151962 + 10.0 * 6.216183662414551
Epoch 1920, val loss: 1.4360090494155884
Epoch 1930, training loss: 62.154422760009766 = 0.02540842443704605 + 10.0 * 6.2129011154174805
Epoch 1930, val loss: 1.4408890008926392
Epoch 1940, training loss: 62.15583801269531 = 0.024987034499645233 + 10.0 * 6.213085174560547
Epoch 1940, val loss: 1.4448707103729248
Epoch 1950, training loss: 62.1391716003418 = 0.024584000930190086 + 10.0 * 6.211458683013916
Epoch 1950, val loss: 1.4495890140533447
Epoch 1960, training loss: 62.13535690307617 = 0.024193957448005676 + 10.0 * 6.211116313934326
Epoch 1960, val loss: 1.4541188478469849
Epoch 1970, training loss: 62.19855499267578 = 0.023818589746952057 + 10.0 * 6.21747350692749
Epoch 1970, val loss: 1.4585846662521362
Epoch 1980, training loss: 62.14710235595703 = 0.023431137204170227 + 10.0 * 6.212367057800293
Epoch 1980, val loss: 1.4625495672225952
Epoch 1990, training loss: 62.13470458984375 = 0.023060841485857964 + 10.0 * 6.211164474487305
Epoch 1990, val loss: 1.4668350219726562
Epoch 2000, training loss: 62.13002014160156 = 0.022699955850839615 + 10.0 * 6.2107319831848145
Epoch 2000, val loss: 1.4711750745773315
Epoch 2010, training loss: 62.19245910644531 = 0.02235000766813755 + 10.0 * 6.217010974884033
Epoch 2010, val loss: 1.4753801822662354
Epoch 2020, training loss: 62.1422004699707 = 0.022007249295711517 + 10.0 * 6.212019443511963
Epoch 2020, val loss: 1.4795624017715454
Epoch 2030, training loss: 62.131370544433594 = 0.02166595123708248 + 10.0 * 6.210970401763916
Epoch 2030, val loss: 1.4836467504501343
Epoch 2040, training loss: 62.129615783691406 = 0.021340662613511086 + 10.0 * 6.210827827453613
Epoch 2040, val loss: 1.4878357648849487
Epoch 2050, training loss: 62.11583709716797 = 0.02101837657392025 + 10.0 * 6.209481716156006
Epoch 2050, val loss: 1.4920258522033691
Epoch 2060, training loss: 62.11845016479492 = 0.020705867558717728 + 10.0 * 6.209774494171143
Epoch 2060, val loss: 1.4959588050842285
Epoch 2070, training loss: 62.14497375488281 = 0.02040190063416958 + 10.0 * 6.212457180023193
Epoch 2070, val loss: 1.4999500513076782
Epoch 2080, training loss: 62.1392707824707 = 0.020099036395549774 + 10.0 * 6.211916923522949
Epoch 2080, val loss: 1.5039938688278198
Epoch 2090, training loss: 62.12855529785156 = 0.019800083711743355 + 10.0 * 6.210875511169434
Epoch 2090, val loss: 1.507774829864502
Epoch 2100, training loss: 62.12541198730469 = 0.01950960047543049 + 10.0 * 6.210590362548828
Epoch 2100, val loss: 1.5116753578186035
Epoch 2110, training loss: 62.10454559326172 = 0.019225139170885086 + 10.0 * 6.208531856536865
Epoch 2110, val loss: 1.5155643224716187
Epoch 2120, training loss: 62.097496032714844 = 0.0189499631524086 + 10.0 * 6.207854270935059
Epoch 2120, val loss: 1.5194530487060547
Epoch 2130, training loss: 62.107601165771484 = 0.018682369962334633 + 10.0 * 6.208891868591309
Epoch 2130, val loss: 1.523263931274414
Epoch 2140, training loss: 62.13718032836914 = 0.018418654799461365 + 10.0 * 6.211876392364502
Epoch 2140, val loss: 1.5269780158996582
Epoch 2150, training loss: 62.12948989868164 = 0.01816079393029213 + 10.0 * 6.211133003234863
Epoch 2150, val loss: 1.5306835174560547
Epoch 2160, training loss: 62.107364654541016 = 0.017898721620440483 + 10.0 * 6.208946704864502
Epoch 2160, val loss: 1.534468650817871
Epoch 2170, training loss: 62.123600006103516 = 0.01765510067343712 + 10.0 * 6.210594654083252
Epoch 2170, val loss: 1.538096308708191
Epoch 2180, training loss: 62.104652404785156 = 0.01740606501698494 + 10.0 * 6.208724498748779
Epoch 2180, val loss: 1.5418275594711304
Epoch 2190, training loss: 62.087764739990234 = 0.017166875302791595 + 10.0 * 6.207059860229492
Epoch 2190, val loss: 1.545409083366394
Epoch 2200, training loss: 62.087677001953125 = 0.016935454681515694 + 10.0 * 6.207074165344238
Epoch 2200, val loss: 1.5490657091140747
Epoch 2210, training loss: 62.130393981933594 = 0.016713036224246025 + 10.0 * 6.211368083953857
Epoch 2210, val loss: 1.5526853799819946
Epoch 2220, training loss: 62.089202880859375 = 0.016482427716255188 + 10.0 * 6.207272052764893
Epoch 2220, val loss: 1.556255578994751
Epoch 2230, training loss: 62.09535217285156 = 0.016260039061307907 + 10.0 * 6.20790958404541
Epoch 2230, val loss: 1.559486985206604
Epoch 2240, training loss: 62.09718322753906 = 0.016044851392507553 + 10.0 * 6.208113670349121
Epoch 2240, val loss: 1.5628383159637451
Epoch 2250, training loss: 62.088321685791016 = 0.015832869336009026 + 10.0 * 6.207249164581299
Epoch 2250, val loss: 1.5664619207382202
Epoch 2260, training loss: 62.076141357421875 = 0.015624633058905602 + 10.0 * 6.206051826477051
Epoch 2260, val loss: 1.5698673725128174
Epoch 2270, training loss: 62.10694885253906 = 0.01542598195374012 + 10.0 * 6.2091522216796875
Epoch 2270, val loss: 1.5734719038009644
Epoch 2280, training loss: 62.085086822509766 = 0.015225200913846493 + 10.0 * 6.206986427307129
Epoch 2280, val loss: 1.5767263174057007
Epoch 2290, training loss: 62.0678825378418 = 0.015024999156594276 + 10.0 * 6.205285549163818
Epoch 2290, val loss: 1.579946517944336
Epoch 2300, training loss: 62.065155029296875 = 0.014832861721515656 + 10.0 * 6.2050323486328125
Epoch 2300, val loss: 1.5833936929702759
Epoch 2310, training loss: 62.06882095336914 = 0.014647059142589569 + 10.0 * 6.205417156219482
Epoch 2310, val loss: 1.5867228507995605
Epoch 2320, training loss: 62.119930267333984 = 0.014465760439634323 + 10.0 * 6.210546493530273
Epoch 2320, val loss: 1.5900399684906006
Epoch 2330, training loss: 62.101261138916016 = 0.014284704811871052 + 10.0 * 6.208697319030762
Epoch 2330, val loss: 1.5933265686035156
Epoch 2340, training loss: 62.062171936035156 = 0.014101150445640087 + 10.0 * 6.204806804656982
Epoch 2340, val loss: 1.5962061882019043
Epoch 2350, training loss: 62.06398391723633 = 0.013925976119935513 + 10.0 * 6.205005645751953
Epoch 2350, val loss: 1.59958016872406
Epoch 2360, training loss: 62.08272933959961 = 0.013759980909526348 + 10.0 * 6.206896781921387
Epoch 2360, val loss: 1.602888822555542
Epoch 2370, training loss: 62.06366729736328 = 0.013591008260846138 + 10.0 * 6.205007553100586
Epoch 2370, val loss: 1.6059136390686035
Epoch 2380, training loss: 62.06620788574219 = 0.01342614647001028 + 10.0 * 6.205277919769287
Epoch 2380, val loss: 1.6089102029800415
Epoch 2390, training loss: 62.082523345947266 = 0.013265484012663364 + 10.0 * 6.206925868988037
Epoch 2390, val loss: 1.6122113466262817
Epoch 2400, training loss: 62.05617904663086 = 0.0131036676466465 + 10.0 * 6.204307556152344
Epoch 2400, val loss: 1.6150819063186646
Epoch 2410, training loss: 62.076171875 = 0.01294764969497919 + 10.0 * 6.20632266998291
Epoch 2410, val loss: 1.61815345287323
Epoch 2420, training loss: 62.05971908569336 = 0.01279508974403143 + 10.0 * 6.204692363739014
Epoch 2420, val loss: 1.6211146116256714
Epoch 2430, training loss: 62.05681228637695 = 0.012644132599234581 + 10.0 * 6.204416751861572
Epoch 2430, val loss: 1.6240832805633545
Epoch 2440, training loss: 62.07898712158203 = 0.012496059760451317 + 10.0 * 6.206648826599121
Epoch 2440, val loss: 1.627044677734375
Epoch 2450, training loss: 62.059661865234375 = 0.012353122234344482 + 10.0 * 6.204730987548828
Epoch 2450, val loss: 1.630132794380188
Epoch 2460, training loss: 62.07308578491211 = 0.012210018001496792 + 10.0 * 6.206087589263916
Epoch 2460, val loss: 1.6330584287643433
Epoch 2470, training loss: 62.05715560913086 = 0.012066978961229324 + 10.0 * 6.2045087814331055
Epoch 2470, val loss: 1.6357330083847046
Epoch 2480, training loss: 62.04997253417969 = 0.011930038221180439 + 10.0 * 6.2038044929504395
Epoch 2480, val loss: 1.6387580633163452
Epoch 2490, training loss: 62.058780670166016 = 0.0117955282330513 + 10.0 * 6.20469856262207
Epoch 2490, val loss: 1.6416884660720825
Epoch 2500, training loss: 62.06542205810547 = 0.011664021760225296 + 10.0 * 6.205375671386719
Epoch 2500, val loss: 1.6443665027618408
Epoch 2510, training loss: 62.04467010498047 = 0.011531461030244827 + 10.0 * 6.203313827514648
Epoch 2510, val loss: 1.647263526916504
Epoch 2520, training loss: 62.037960052490234 = 0.011404067277908325 + 10.0 * 6.20265531539917
Epoch 2520, val loss: 1.650261402130127
Epoch 2530, training loss: 62.036258697509766 = 0.01127947773784399 + 10.0 * 6.202497959136963
Epoch 2530, val loss: 1.6530779600143433
Epoch 2540, training loss: 62.103267669677734 = 0.011159293353557587 + 10.0 * 6.2092108726501465
Epoch 2540, val loss: 1.655821442604065
Epoch 2550, training loss: 62.06298065185547 = 0.011033312417566776 + 10.0 * 6.20519495010376
Epoch 2550, val loss: 1.6584343910217285
Epoch 2560, training loss: 62.03923034667969 = 0.010910852812230587 + 10.0 * 6.202832221984863
Epoch 2560, val loss: 1.6609381437301636
Epoch 2570, training loss: 62.03004455566406 = 0.01079469919204712 + 10.0 * 6.201924800872803
Epoch 2570, val loss: 1.6638904809951782
Epoch 2580, training loss: 62.04574966430664 = 0.010681187734007835 + 10.0 * 6.203506946563721
Epoch 2580, val loss: 1.6666219234466553
Epoch 2590, training loss: 62.050079345703125 = 0.010568149387836456 + 10.0 * 6.203951358795166
Epoch 2590, val loss: 1.669301152229309
Epoch 2600, training loss: 62.051544189453125 = 0.010457285679876804 + 10.0 * 6.204108715057373
Epoch 2600, val loss: 1.6718969345092773
Epoch 2610, training loss: 62.060768127441406 = 0.010347489267587662 + 10.0 * 6.205041885375977
Epoch 2610, val loss: 1.674524188041687
Epoch 2620, training loss: 62.03439712524414 = 0.010237208567559719 + 10.0 * 6.202415943145752
Epoch 2620, val loss: 1.6771578788757324
Epoch 2630, training loss: 62.065589904785156 = 0.010130233131349087 + 10.0 * 6.205545902252197
Epoch 2630, val loss: 1.6796848773956299
Epoch 2640, training loss: 62.025333404541016 = 0.010025318711996078 + 10.0 * 6.201530933380127
Epoch 2640, val loss: 1.6822136640548706
Epoch 2650, training loss: 62.014286041259766 = 0.00992242619395256 + 10.0 * 6.200436592102051
Epoch 2650, val loss: 1.6848089694976807
Epoch 2660, training loss: 62.02340316772461 = 0.009824277833104134 + 10.0 * 6.201357841491699
Epoch 2660, val loss: 1.6873148679733276
Epoch 2670, training loss: 62.070068359375 = 0.009728716686367989 + 10.0 * 6.206034183502197
Epoch 2670, val loss: 1.6897010803222656
Epoch 2680, training loss: 62.05659484863281 = 0.009628942236304283 + 10.0 * 6.2046966552734375
Epoch 2680, val loss: 1.6923595666885376
Epoch 2690, training loss: 62.022560119628906 = 0.00952811911702156 + 10.0 * 6.201303005218506
Epoch 2690, val loss: 1.6945180892944336
Epoch 2700, training loss: 62.021236419677734 = 0.009434662759304047 + 10.0 * 6.201180458068848
Epoch 2700, val loss: 1.6971246004104614
Epoch 2710, training loss: 62.01546096801758 = 0.009342505596578121 + 10.0 * 6.2006120681762695
Epoch 2710, val loss: 1.6996259689331055
Epoch 2720, training loss: 62.029052734375 = 0.009252582676708698 + 10.0 * 6.201979637145996
Epoch 2720, val loss: 1.7019968032836914
Epoch 2730, training loss: 62.02582550048828 = 0.009163281880319118 + 10.0 * 6.201666355133057
Epoch 2730, val loss: 1.7044795751571655
Epoch 2740, training loss: 62.02006912231445 = 0.009073709137737751 + 10.0 * 6.201099395751953
Epoch 2740, val loss: 1.7068393230438232
Epoch 2750, training loss: 62.03524398803711 = 0.00898611731827259 + 10.0 * 6.202625751495361
Epoch 2750, val loss: 1.7090173959732056
Epoch 2760, training loss: 62.02375411987305 = 0.008899835869669914 + 10.0 * 6.201485633850098
Epoch 2760, val loss: 1.7112067937850952
Epoch 2770, training loss: 62.008663177490234 = 0.008814843371510506 + 10.0 * 6.199984550476074
Epoch 2770, val loss: 1.713731050491333
Epoch 2780, training loss: 62.01131820678711 = 0.00873331818729639 + 10.0 * 6.200258731842041
Epoch 2780, val loss: 1.7159746885299683
Epoch 2790, training loss: 62.03601837158203 = 0.008652838878333569 + 10.0 * 6.202736854553223
Epoch 2790, val loss: 1.7181826829910278
Epoch 2800, training loss: 62.025360107421875 = 0.008571062237024307 + 10.0 * 6.20167875289917
Epoch 2800, val loss: 1.7205554246902466
Epoch 2810, training loss: 62.033653259277344 = 0.00849209912121296 + 10.0 * 6.202516078948975
Epoch 2810, val loss: 1.7226332426071167
Epoch 2820, training loss: 62.02265167236328 = 0.008410381153225899 + 10.0 * 6.2014241218566895
Epoch 2820, val loss: 1.7249001264572144
Epoch 2830, training loss: 62.02123260498047 = 0.008331986144185066 + 10.0 * 6.201290130615234
Epoch 2830, val loss: 1.7268785238265991
Epoch 2840, training loss: 62.04033660888672 = 0.008256807923316956 + 10.0 * 6.203207969665527
Epoch 2840, val loss: 1.72914719581604
Epoch 2850, training loss: 62.00029754638672 = 0.008179208263754845 + 10.0 * 6.199212074279785
Epoch 2850, val loss: 1.7312408685684204
Epoch 2860, training loss: 61.99225997924805 = 0.008107305504381657 + 10.0 * 6.198415279388428
Epoch 2860, val loss: 1.7336126565933228
Epoch 2870, training loss: 61.98978042602539 = 0.00803676899522543 + 10.0 * 6.198174476623535
Epoch 2870, val loss: 1.735819697380066
Epoch 2880, training loss: 61.997257232666016 = 0.007967167533934116 + 10.0 * 6.1989288330078125
Epoch 2880, val loss: 1.737979769706726
Epoch 2890, training loss: 62.066673278808594 = 0.007899845950305462 + 10.0 * 6.205877304077148
Epoch 2890, val loss: 1.7400624752044678
Epoch 2900, training loss: 62.02927017211914 = 0.00782603770494461 + 10.0 * 6.202144145965576
Epoch 2900, val loss: 1.7417463064193726
Epoch 2910, training loss: 62.01472473144531 = 0.0077568888664245605 + 10.0 * 6.20069694519043
Epoch 2910, val loss: 1.7437652349472046
Epoch 2920, training loss: 62.01168441772461 = 0.007688198704272509 + 10.0 * 6.200399875640869
Epoch 2920, val loss: 1.7459298372268677
Epoch 2930, training loss: 61.992225646972656 = 0.007620634976774454 + 10.0 * 6.198460578918457
Epoch 2930, val loss: 1.7480531930923462
Epoch 2940, training loss: 62.0214958190918 = 0.007558469194918871 + 10.0 * 6.201394081115723
Epoch 2940, val loss: 1.7500765323638916
Epoch 2950, training loss: 61.98957443237305 = 0.007491507567465305 + 10.0 * 6.198208332061768
Epoch 2950, val loss: 1.7520965337753296
Epoch 2960, training loss: 61.99241256713867 = 0.0074282316491007805 + 10.0 * 6.198498725891113
Epoch 2960, val loss: 1.7541640996932983
Epoch 2970, training loss: 61.99570083618164 = 0.007367491722106934 + 10.0 * 6.198833465576172
Epoch 2970, val loss: 1.756233811378479
Epoch 2980, training loss: 62.00727462768555 = 0.00730641046538949 + 10.0 * 6.1999969482421875
Epoch 2980, val loss: 1.7581604719161987
Epoch 2990, training loss: 62.03305435180664 = 0.007245149929076433 + 10.0 * 6.20258092880249
Epoch 2990, val loss: 1.7598228454589844
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 87.91400909423828 = 1.9459322690963745 + 10.0 * 8.596807479858398
Epoch 0, val loss: 1.9438962936401367
Epoch 10, training loss: 87.89412689208984 = 1.9364742040634155 + 10.0 * 8.595765113830566
Epoch 10, val loss: 1.9344457387924194
Epoch 20, training loss: 87.8046875 = 1.924753189086914 + 10.0 * 8.587993621826172
Epoch 20, val loss: 1.9222533702850342
Epoch 30, training loss: 87.18997192382812 = 1.9099833965301514 + 10.0 * 8.527998924255371
Epoch 30, val loss: 1.906503438949585
Epoch 40, training loss: 81.82288360595703 = 1.892356276512146 + 10.0 * 7.993052959442139
Epoch 40, val loss: 1.8877439498901367
Epoch 50, training loss: 74.91706848144531 = 1.8750287294387817 + 10.0 * 7.304203987121582
Epoch 50, val loss: 1.8713181018829346
Epoch 60, training loss: 73.0078125 = 1.860445261001587 + 10.0 * 7.114736557006836
Epoch 60, val loss: 1.8571456670761108
Epoch 70, training loss: 71.88092803955078 = 1.8470619916915894 + 10.0 * 7.003386974334717
Epoch 70, val loss: 1.844670295715332
Epoch 80, training loss: 71.01277923583984 = 1.8337115049362183 + 10.0 * 6.917906761169434
Epoch 80, val loss: 1.8322261571884155
Epoch 90, training loss: 70.37694549560547 = 1.821393609046936 + 10.0 * 6.855555057525635
Epoch 90, val loss: 1.8208726644515991
Epoch 100, training loss: 69.62693786621094 = 1.8100292682647705 + 10.0 * 6.781691074371338
Epoch 100, val loss: 1.8104305267333984
Epoch 110, training loss: 68.83412170410156 = 1.800180435180664 + 10.0 * 6.703394412994385
Epoch 110, val loss: 1.8014907836914062
Epoch 120, training loss: 68.21118927001953 = 1.791048288345337 + 10.0 * 6.642014026641846
Epoch 120, val loss: 1.7932006120681763
Epoch 130, training loss: 67.75543212890625 = 1.7817957401275635 + 10.0 * 6.597363471984863
Epoch 130, val loss: 1.7845510244369507
Epoch 140, training loss: 67.38397216796875 = 1.771761178970337 + 10.0 * 6.561221599578857
Epoch 140, val loss: 1.775174856185913
Epoch 150, training loss: 67.0435791015625 = 1.761007308959961 + 10.0 * 6.528256893157959
Epoch 150, val loss: 1.764943242073059
Epoch 160, training loss: 66.76973724365234 = 1.749607801437378 + 10.0 * 6.502012252807617
Epoch 160, val loss: 1.75428307056427
Epoch 170, training loss: 66.56565856933594 = 1.7371608018875122 + 10.0 * 6.482850074768066
Epoch 170, val loss: 1.7428475618362427
Epoch 180, training loss: 66.35211181640625 = 1.723510980606079 + 10.0 * 6.462860107421875
Epoch 180, val loss: 1.7301146984100342
Epoch 190, training loss: 66.18897247314453 = 1.7085597515106201 + 10.0 * 6.4480414390563965
Epoch 190, val loss: 1.7162550687789917
Epoch 200, training loss: 66.08277893066406 = 1.6921193599700928 + 10.0 * 6.439066410064697
Epoch 200, val loss: 1.7010747194290161
Epoch 210, training loss: 65.93219757080078 = 1.6740729808807373 + 10.0 * 6.425812721252441
Epoch 210, val loss: 1.684493064880371
Epoch 220, training loss: 65.81952667236328 = 1.6544729471206665 + 10.0 * 6.416505336761475
Epoch 220, val loss: 1.6664929389953613
Epoch 230, training loss: 65.72139739990234 = 1.6331939697265625 + 10.0 * 6.408820152282715
Epoch 230, val loss: 1.647047519683838
Epoch 240, training loss: 65.6139907836914 = 1.610360860824585 + 10.0 * 6.400362968444824
Epoch 240, val loss: 1.6261532306671143
Epoch 250, training loss: 65.51348114013672 = 1.5859267711639404 + 10.0 * 6.392755508422852
Epoch 250, val loss: 1.6037697792053223
Epoch 260, training loss: 65.42133331298828 = 1.5599607229232788 + 10.0 * 6.38613748550415
Epoch 260, val loss: 1.5801258087158203
Epoch 270, training loss: 65.3277587890625 = 1.53261137008667 + 10.0 * 6.379514694213867
Epoch 270, val loss: 1.5551899671554565
Epoch 280, training loss: 65.25273895263672 = 1.5038440227508545 + 10.0 * 6.374889373779297
Epoch 280, val loss: 1.5292038917541504
Epoch 290, training loss: 65.16575622558594 = 1.474165439605713 + 10.0 * 6.36915922164917
Epoch 290, val loss: 1.5024012327194214
Epoch 300, training loss: 65.07313537597656 = 1.4435603618621826 + 10.0 * 6.36295747756958
Epoch 300, val loss: 1.4750386476516724
Epoch 310, training loss: 64.99890899658203 = 1.412295937538147 + 10.0 * 6.358661651611328
Epoch 310, val loss: 1.4473005533218384
Epoch 320, training loss: 64.94778442382812 = 1.3804290294647217 + 10.0 * 6.3567352294921875
Epoch 320, val loss: 1.4193217754364014
Epoch 330, training loss: 64.83952331542969 = 1.348446249961853 + 10.0 * 6.349107265472412
Epoch 330, val loss: 1.3911879062652588
Epoch 340, training loss: 64.76188659667969 = 1.3163458108901978 + 10.0 * 6.3445539474487305
Epoch 340, val loss: 1.363328456878662
Epoch 350, training loss: 64.6964111328125 = 1.2842129468917847 + 10.0 * 6.341219902038574
Epoch 350, val loss: 1.3357478380203247
Epoch 360, training loss: 64.64607238769531 = 1.2520086765289307 + 10.0 * 6.339406490325928
Epoch 360, val loss: 1.308630108833313
Epoch 370, training loss: 64.5545883178711 = 1.22036612033844 + 10.0 * 6.3334221839904785
Epoch 370, val loss: 1.281830906867981
Epoch 380, training loss: 64.48441314697266 = 1.1889619827270508 + 10.0 * 6.329545021057129
Epoch 380, val loss: 1.255614995956421
Epoch 390, training loss: 64.42265319824219 = 1.1577084064483643 + 10.0 * 6.3264946937561035
Epoch 390, val loss: 1.2298375368118286
Epoch 400, training loss: 64.35271453857422 = 1.1270043849945068 + 10.0 * 6.32257080078125
Epoch 400, val loss: 1.2047094106674194
Epoch 410, training loss: 64.29354095458984 = 1.0966613292694092 + 10.0 * 6.319687843322754
Epoch 410, val loss: 1.1802150011062622
Epoch 420, training loss: 64.29146575927734 = 1.066733717918396 + 10.0 * 6.322473049163818
Epoch 420, val loss: 1.1561959981918335
Epoch 430, training loss: 64.18258666992188 = 1.0370771884918213 + 10.0 * 6.314550876617432
Epoch 430, val loss: 1.1328763961791992
Epoch 440, training loss: 64.1269760131836 = 1.0079599618911743 + 10.0 * 6.311902046203613
Epoch 440, val loss: 1.1101925373077393
Epoch 450, training loss: 64.10139465332031 = 0.9793755412101746 + 10.0 * 6.312201976776123
Epoch 450, val loss: 1.0880084037780762
Epoch 460, training loss: 64.05567932128906 = 0.9511099457740784 + 10.0 * 6.310457229614258
Epoch 460, val loss: 1.0665496587753296
Epoch 470, training loss: 63.98210525512695 = 0.9235747456550598 + 10.0 * 6.305852890014648
Epoch 470, val loss: 1.0456868410110474
Epoch 480, training loss: 63.92624282836914 = 0.8966637253761292 + 10.0 * 6.302958011627197
Epoch 480, val loss: 1.02565336227417
Epoch 490, training loss: 63.926937103271484 = 0.870410144329071 + 10.0 * 6.305652618408203
Epoch 490, val loss: 1.0063225030899048
Epoch 500, training loss: 63.861141204833984 = 0.8449680805206299 + 10.0 * 6.301617622375488
Epoch 500, val loss: 0.9876184463500977
Epoch 510, training loss: 63.797218322753906 = 0.8201593160629272 + 10.0 * 6.29770565032959
Epoch 510, val loss: 0.9699822068214417
Epoch 520, training loss: 63.76852798461914 = 0.7963287830352783 + 10.0 * 6.297219753265381
Epoch 520, val loss: 0.9530910849571228
Epoch 530, training loss: 63.7164421081543 = 0.7734219431877136 + 10.0 * 6.294301986694336
Epoch 530, val loss: 0.9370717406272888
Epoch 540, training loss: 63.7130012512207 = 0.7513121962547302 + 10.0 * 6.296168804168701
Epoch 540, val loss: 0.9219745397567749
Epoch 550, training loss: 63.65385818481445 = 0.7300885915756226 + 10.0 * 6.29237699508667
Epoch 550, val loss: 0.9077861309051514
Epoch 560, training loss: 63.60487365722656 = 0.7097642421722412 + 10.0 * 6.289511203765869
Epoch 560, val loss: 0.8944361209869385
Epoch 570, training loss: 63.57572937011719 = 0.6903648972511292 + 10.0 * 6.288536548614502
Epoch 570, val loss: 0.8819635510444641
Epoch 580, training loss: 63.53755569458008 = 0.6716102361679077 + 10.0 * 6.286594390869141
Epoch 580, val loss: 0.8702319264411926
Epoch 590, training loss: 63.50482940673828 = 0.6537598967552185 + 10.0 * 6.285107135772705
Epoch 590, val loss: 0.8593308925628662
Epoch 600, training loss: 63.47853088378906 = 0.636604905128479 + 10.0 * 6.2841925621032715
Epoch 600, val loss: 0.8492419123649597
Epoch 610, training loss: 63.43977737426758 = 0.6200090050697327 + 10.0 * 6.281976699829102
Epoch 610, val loss: 0.8397433161735535
Epoch 620, training loss: 63.42469024658203 = 0.6041965484619141 + 10.0 * 6.282049179077148
Epoch 620, val loss: 0.8309513330459595
Epoch 630, training loss: 63.38929748535156 = 0.5889242887496948 + 10.0 * 6.2800374031066895
Epoch 630, val loss: 0.8227719068527222
Epoch 640, training loss: 63.36540985107422 = 0.5741569995880127 + 10.0 * 6.279125213623047
Epoch 640, val loss: 0.8151078224182129
Epoch 650, training loss: 63.33302688598633 = 0.5599251985549927 + 10.0 * 6.277310371398926
Epoch 650, val loss: 0.8078659176826477
Epoch 660, training loss: 63.3116340637207 = 0.5460303425788879 + 10.0 * 6.276560306549072
Epoch 660, val loss: 0.8010804057121277
Epoch 670, training loss: 63.28920364379883 = 0.5324385166168213 + 10.0 * 6.275676250457764
Epoch 670, val loss: 0.7946247458457947
Epoch 680, training loss: 63.25716018676758 = 0.5192686915397644 + 10.0 * 6.273789405822754
Epoch 680, val loss: 0.7885560989379883
Epoch 690, training loss: 63.2540168762207 = 0.5063532590866089 + 10.0 * 6.274766445159912
Epoch 690, val loss: 0.7827262282371521
Epoch 700, training loss: 63.20707702636719 = 0.49361997842788696 + 10.0 * 6.271345615386963
Epoch 700, val loss: 0.7772271633148193
Epoch 710, training loss: 63.18503189086914 = 0.4811796247959137 + 10.0 * 6.270385265350342
Epoch 710, val loss: 0.772063672542572
Epoch 720, training loss: 63.16302490234375 = 0.4689105749130249 + 10.0 * 6.269411563873291
Epoch 720, val loss: 0.7670474648475647
Epoch 730, training loss: 63.16553497314453 = 0.4567369818687439 + 10.0 * 6.270879745483398
Epoch 730, val loss: 0.7621510028839111
Epoch 740, training loss: 63.17821502685547 = 0.44475430250167847 + 10.0 * 6.273345947265625
Epoch 740, val loss: 0.7572930455207825
Epoch 750, training loss: 63.111114501953125 = 0.4327686131000519 + 10.0 * 6.267834663391113
Epoch 750, val loss: 0.7525501847267151
Epoch 760, training loss: 63.07183837890625 = 0.42094823718070984 + 10.0 * 6.26508903503418
Epoch 760, val loss: 0.7481620907783508
Epoch 770, training loss: 63.045223236083984 = 0.4092877209186554 + 10.0 * 6.263593673706055
Epoch 770, val loss: 0.7438215613365173
Epoch 780, training loss: 63.093624114990234 = 0.3976764976978302 + 10.0 * 6.269594669342041
Epoch 780, val loss: 0.7395550012588501
Epoch 790, training loss: 63.04191970825195 = 0.38610559701919556 + 10.0 * 6.2655816078186035
Epoch 790, val loss: 0.7352768182754517
Epoch 800, training loss: 62.98519515991211 = 0.37463143467903137 + 10.0 * 6.261056423187256
Epoch 800, val loss: 0.7312372326850891
Epoch 810, training loss: 62.96574020385742 = 0.3632824718952179 + 10.0 * 6.2602458000183105
Epoch 810, val loss: 0.7273849248886108
Epoch 820, training loss: 63.002384185791016 = 0.3520393371582031 + 10.0 * 6.2650346755981445
Epoch 820, val loss: 0.7236732244491577
Epoch 830, training loss: 62.941993713378906 = 0.34083083271980286 + 10.0 * 6.260116100311279
Epoch 830, val loss: 0.7200002074241638
Epoch 840, training loss: 62.90943145751953 = 0.32982173562049866 + 10.0 * 6.257960796356201
Epoch 840, val loss: 0.7166401147842407
Epoch 850, training loss: 62.96077346801758 = 0.3190031349658966 + 10.0 * 6.264176845550537
Epoch 850, val loss: 0.7133505344390869
Epoch 860, training loss: 62.89397430419922 = 0.30814671516418457 + 10.0 * 6.258582592010498
Epoch 860, val loss: 0.710355818271637
Epoch 870, training loss: 62.85672378540039 = 0.2976266145706177 + 10.0 * 6.2559099197387695
Epoch 870, val loss: 0.707624077796936
Epoch 880, training loss: 62.827003479003906 = 0.2874050736427307 + 10.0 * 6.253959655761719
Epoch 880, val loss: 0.7051389217376709
Epoch 890, training loss: 62.823020935058594 = 0.27740228176116943 + 10.0 * 6.254561901092529
Epoch 890, val loss: 0.7029455900192261
Epoch 900, training loss: 62.802188873291016 = 0.26758256554603577 + 10.0 * 6.253460884094238
Epoch 900, val loss: 0.7010452151298523
Epoch 910, training loss: 62.7855224609375 = 0.25803637504577637 + 10.0 * 6.252748489379883
Epoch 910, val loss: 0.6994494199752808
Epoch 920, training loss: 62.77641677856445 = 0.2488042563199997 + 10.0 * 6.252760887145996
Epoch 920, val loss: 0.6981241703033447
Epoch 930, training loss: 62.76005935668945 = 0.23989422619342804 + 10.0 * 6.252016544342041
Epoch 930, val loss: 0.6972265839576721
Epoch 940, training loss: 62.750179290771484 = 0.23126466572284698 + 10.0 * 6.251891136169434
Epoch 940, val loss: 0.6967218518257141
Epoch 950, training loss: 62.71971130371094 = 0.22294804453849792 + 10.0 * 6.249676704406738
Epoch 950, val loss: 0.6962913274765015
Epoch 960, training loss: 62.73127365112305 = 0.2149793654680252 + 10.0 * 6.25162935256958
Epoch 960, val loss: 0.6963471174240112
Epoch 970, training loss: 62.7109260559082 = 0.20722323656082153 + 10.0 * 6.250370502471924
Epoch 970, val loss: 0.6967736482620239
Epoch 980, training loss: 62.676292419433594 = 0.1998496800661087 + 10.0 * 6.247644424438477
Epoch 980, val loss: 0.6972534656524658
Epoch 990, training loss: 62.65949630737305 = 0.19274510443210602 + 10.0 * 6.24667501449585
Epoch 990, val loss: 0.6982962489128113
Epoch 1000, training loss: 62.68159484863281 = 0.18596723675727844 + 10.0 * 6.249562740325928
Epoch 1000, val loss: 0.6994155645370483
Epoch 1010, training loss: 62.64549255371094 = 0.1794196367263794 + 10.0 * 6.246607303619385
Epoch 1010, val loss: 0.7010225057601929
Epoch 1020, training loss: 62.6371955871582 = 0.17317135632038116 + 10.0 * 6.246402263641357
Epoch 1020, val loss: 0.7027186155319214
Epoch 1030, training loss: 62.60703659057617 = 0.16719110310077667 + 10.0 * 6.243984699249268
Epoch 1030, val loss: 0.7047162055969238
Epoch 1040, training loss: 62.60403823852539 = 0.1615200936794281 + 10.0 * 6.244251728057861
Epoch 1040, val loss: 0.7068485021591187
Epoch 1050, training loss: 62.6121711730957 = 0.15603096783161163 + 10.0 * 6.245614051818848
Epoch 1050, val loss: 0.7093333005905151
Epoch 1060, training loss: 62.57337188720703 = 0.15076790750026703 + 10.0 * 6.242260456085205
Epoch 1060, val loss: 0.711837112903595
Epoch 1070, training loss: 62.57160568237305 = 0.1457446664571762 + 10.0 * 6.242586135864258
Epoch 1070, val loss: 0.7147373557090759
Epoch 1080, training loss: 62.59925079345703 = 0.14096999168395996 + 10.0 * 6.245828151702881
Epoch 1080, val loss: 0.7175818085670471
Epoch 1090, training loss: 62.54461669921875 = 0.1363755762577057 + 10.0 * 6.240824222564697
Epoch 1090, val loss: 0.7207955121994019
Epoch 1100, training loss: 62.52705001831055 = 0.13197731971740723 + 10.0 * 6.23950719833374
Epoch 1100, val loss: 0.7240115404129028
Epoch 1110, training loss: 62.58515930175781 = 0.12778298556804657 + 10.0 * 6.245737552642822
Epoch 1110, val loss: 0.7274282574653625
Epoch 1120, training loss: 62.543758392333984 = 0.1237601488828659 + 10.0 * 6.241999626159668
Epoch 1120, val loss: 0.7307253479957581
Epoch 1130, training loss: 62.50421142578125 = 0.11987332999706268 + 10.0 * 6.238433837890625
Epoch 1130, val loss: 0.7342871427536011
Epoch 1140, training loss: 62.49187469482422 = 0.11619536578655243 + 10.0 * 6.237567901611328
Epoch 1140, val loss: 0.7379320859909058
Epoch 1150, training loss: 62.49564743041992 = 0.1126682236790657 + 10.0 * 6.238297939300537
Epoch 1150, val loss: 0.7417277097702026
Epoch 1160, training loss: 62.50628662109375 = 0.10926684737205505 + 10.0 * 6.239701747894287
Epoch 1160, val loss: 0.7455893158912659
Epoch 1170, training loss: 62.488136291503906 = 0.10600103437900543 + 10.0 * 6.238213539123535
Epoch 1170, val loss: 0.7495672702789307
Epoch 1180, training loss: 62.47443389892578 = 0.10284364968538284 + 10.0 * 6.23715877532959
Epoch 1180, val loss: 0.7535153031349182
Epoch 1190, training loss: 62.469539642333984 = 0.0998527929186821 + 10.0 * 6.236968517303467
Epoch 1190, val loss: 0.7574119567871094
Epoch 1200, training loss: 62.470489501953125 = 0.09698803722858429 + 10.0 * 6.237349987030029
Epoch 1200, val loss: 0.7614812254905701
Epoch 1210, training loss: 62.44134521484375 = 0.0941912829875946 + 10.0 * 6.234715461730957
Epoch 1210, val loss: 0.7657265663146973
Epoch 1220, training loss: 62.43061447143555 = 0.0915532186627388 + 10.0 * 6.233906269073486
Epoch 1220, val loss: 0.7699415683746338
Epoch 1230, training loss: 62.453697204589844 = 0.08900420367717743 + 10.0 * 6.236469268798828
Epoch 1230, val loss: 0.7741257548332214
Epoch 1240, training loss: 62.4281120300293 = 0.08653067797422409 + 10.0 * 6.234158515930176
Epoch 1240, val loss: 0.7781877517700195
Epoch 1250, training loss: 62.424346923828125 = 0.08415958285331726 + 10.0 * 6.234018802642822
Epoch 1250, val loss: 0.782542884349823
Epoch 1260, training loss: 62.470821380615234 = 0.08184414356946945 + 10.0 * 6.238897800445557
Epoch 1260, val loss: 0.7868696451187134
Epoch 1270, training loss: 62.4173469543457 = 0.07967155426740646 + 10.0 * 6.233767509460449
Epoch 1270, val loss: 0.7913303971290588
Epoch 1280, training loss: 62.392520904541016 = 0.07753975689411163 + 10.0 * 6.231497764587402
Epoch 1280, val loss: 0.7956253290176392
Epoch 1290, training loss: 62.382205963134766 = 0.07553368806838989 + 10.0 * 6.2306671142578125
Epoch 1290, val loss: 0.8000717163085938
Epoch 1300, training loss: 62.378631591796875 = 0.07358373701572418 + 10.0 * 6.230504512786865
Epoch 1300, val loss: 0.8044251203536987
Epoch 1310, training loss: 62.446720123291016 = 0.07173041999340057 + 10.0 * 6.237498760223389
Epoch 1310, val loss: 0.8086022734642029
Epoch 1320, training loss: 62.40617752075195 = 0.0698525533080101 + 10.0 * 6.233632564544678
Epoch 1320, val loss: 0.8134406805038452
Epoch 1330, training loss: 62.377437591552734 = 0.0681050643324852 + 10.0 * 6.23093318939209
Epoch 1330, val loss: 0.8175430297851562
Epoch 1340, training loss: 62.361454010009766 = 0.0663994774222374 + 10.0 * 6.22950553894043
Epoch 1340, val loss: 0.8221877217292786
Epoch 1350, training loss: 62.375343322753906 = 0.06477295607328415 + 10.0 * 6.231057167053223
Epoch 1350, val loss: 0.8265163898468018
Epoch 1360, training loss: 62.344215393066406 = 0.06318193674087524 + 10.0 * 6.228103160858154
Epoch 1360, val loss: 0.8310111165046692
Epoch 1370, training loss: 62.37250900268555 = 0.06165599822998047 + 10.0 * 6.231085300445557
Epoch 1370, val loss: 0.8355084657669067
Epoch 1380, training loss: 62.361351013183594 = 0.06018069386482239 + 10.0 * 6.230116844177246
Epoch 1380, val loss: 0.8396843671798706
Epoch 1390, training loss: 62.33687210083008 = 0.05873692035675049 + 10.0 * 6.227813243865967
Epoch 1390, val loss: 0.8442500233650208
Epoch 1400, training loss: 62.32309341430664 = 0.05736157298088074 + 10.0 * 6.2265729904174805
Epoch 1400, val loss: 0.8485549688339233
Epoch 1410, training loss: 62.338768005371094 = 0.0560322031378746 + 10.0 * 6.228273391723633
Epoch 1410, val loss: 0.8530046939849854
Epoch 1420, training loss: 62.312435150146484 = 0.054728347808122635 + 10.0 * 6.225770473480225
Epoch 1420, val loss: 0.8574832677841187
Epoch 1430, training loss: 62.336936950683594 = 0.053455933928489685 + 10.0 * 6.228348255157471
Epoch 1430, val loss: 0.8617874383926392
Epoch 1440, training loss: 62.30262756347656 = 0.05225679278373718 + 10.0 * 6.225037097930908
Epoch 1440, val loss: 0.866221010684967
Epoch 1450, training loss: 62.30106735229492 = 0.05109873414039612 + 10.0 * 6.224997043609619
Epoch 1450, val loss: 0.8706626892089844
Epoch 1460, training loss: 62.30739974975586 = 0.04997090995311737 + 10.0 * 6.225742816925049
Epoch 1460, val loss: 0.8750686049461365
Epoch 1470, training loss: 62.327903747558594 = 0.048867303878068924 + 10.0 * 6.227903842926025
Epoch 1470, val loss: 0.8793333768844604
Epoch 1480, training loss: 62.288700103759766 = 0.04778828099370003 + 10.0 * 6.224091529846191
Epoch 1480, val loss: 0.883671224117279
Epoch 1490, training loss: 62.28956985473633 = 0.046757180243730545 + 10.0 * 6.224281311035156
Epoch 1490, val loss: 0.8879665732383728
Epoch 1500, training loss: 62.28024673461914 = 0.045764438807964325 + 10.0 * 6.223448276519775
Epoch 1500, val loss: 0.8923363089561462
Epoch 1510, training loss: 62.28441619873047 = 0.044805411249399185 + 10.0 * 6.223961353302002
Epoch 1510, val loss: 0.8965922594070435
Epoch 1520, training loss: 62.30745315551758 = 0.043867338448762894 + 10.0 * 6.226358413696289
Epoch 1520, val loss: 0.9008907675743103
Epoch 1530, training loss: 62.28291702270508 = 0.04294154420495033 + 10.0 * 6.223997592926025
Epoch 1530, val loss: 0.905095636844635
Epoch 1540, training loss: 62.295841217041016 = 0.04204881936311722 + 10.0 * 6.22537899017334
Epoch 1540, val loss: 0.909540593624115
Epoch 1550, training loss: 62.26823043823242 = 0.04118896275758743 + 10.0 * 6.2227044105529785
Epoch 1550, val loss: 0.9133986234664917
Epoch 1560, training loss: 62.26441192626953 = 0.04034889116883278 + 10.0 * 6.222406387329102
Epoch 1560, val loss: 0.9174187779426575
Epoch 1570, training loss: 62.26480484008789 = 0.0395413339138031 + 10.0 * 6.2225260734558105
Epoch 1570, val loss: 0.9217954874038696
Epoch 1580, training loss: 62.303009033203125 = 0.038757067173719406 + 10.0 * 6.2264251708984375
Epoch 1580, val loss: 0.9257690906524658
Epoch 1590, training loss: 62.261138916015625 = 0.03797372430562973 + 10.0 * 6.222316265106201
Epoch 1590, val loss: 0.9299896955490112
Epoch 1600, training loss: 62.249244689941406 = 0.037230584770441055 + 10.0 * 6.221201419830322
Epoch 1600, val loss: 0.933939516544342
Epoch 1610, training loss: 62.27053451538086 = 0.03651314973831177 + 10.0 * 6.22340202331543
Epoch 1610, val loss: 0.9379514455795288
Epoch 1620, training loss: 62.24622344970703 = 0.03579942509531975 + 10.0 * 6.221042156219482
Epoch 1620, val loss: 0.9421184062957764
Epoch 1630, training loss: 62.253719329833984 = 0.03510762378573418 + 10.0 * 6.221861362457275
Epoch 1630, val loss: 0.9461573362350464
Epoch 1640, training loss: 62.23466491699219 = 0.03444159775972366 + 10.0 * 6.220022201538086
Epoch 1640, val loss: 0.949994683265686
Epoch 1650, training loss: 62.229496002197266 = 0.03379402682185173 + 10.0 * 6.219570159912109
Epoch 1650, val loss: 0.9539869427680969
Epoch 1660, training loss: 62.26343536376953 = 0.03317289054393768 + 10.0 * 6.223026275634766
Epoch 1660, val loss: 0.957966685295105
Epoch 1670, training loss: 62.24003219604492 = 0.03253999724984169 + 10.0 * 6.220749378204346
Epoch 1670, val loss: 0.9620028734207153
Epoch 1680, training loss: 62.22684097290039 = 0.03194144740700722 + 10.0 * 6.219490051269531
Epoch 1680, val loss: 0.9658135175704956
Epoch 1690, training loss: 62.25084686279297 = 0.03135855123400688 + 10.0 * 6.221949100494385
Epoch 1690, val loss: 0.9696395397186279
Epoch 1700, training loss: 62.21870422363281 = 0.03078782930970192 + 10.0 * 6.218791484832764
Epoch 1700, val loss: 0.9734103679656982
Epoch 1710, training loss: 62.23466873168945 = 0.030238132923841476 + 10.0 * 6.220442771911621
Epoch 1710, val loss: 0.9771996140480042
Epoch 1720, training loss: 62.2301025390625 = 0.029693130403757095 + 10.0 * 6.220040798187256
Epoch 1720, val loss: 0.9811474680900574
Epoch 1730, training loss: 62.20800018310547 = 0.029163990169763565 + 10.0 * 6.217883586883545
Epoch 1730, val loss: 0.9849842190742493
Epoch 1740, training loss: 62.199405670166016 = 0.028653502464294434 + 10.0 * 6.217075347900391
Epoch 1740, val loss: 0.9887590408325195
Epoch 1750, training loss: 62.195194244384766 = 0.028162071481347084 + 10.0 * 6.216702938079834
Epoch 1750, val loss: 0.9926192760467529
Epoch 1760, training loss: 62.19338607788086 = 0.02768157608807087 + 10.0 * 6.2165703773498535
Epoch 1760, val loss: 0.9963675141334534
Epoch 1770, training loss: 62.24077224731445 = 0.027221912518143654 + 10.0 * 6.221354961395264
Epoch 1770, val loss: 1.0000144243240356
Epoch 1780, training loss: 62.259849548339844 = 0.02675875648856163 + 10.0 * 6.22330904006958
Epoch 1780, val loss: 1.0033349990844727
Epoch 1790, training loss: 62.20845413208008 = 0.026287324726581573 + 10.0 * 6.218216896057129
Epoch 1790, val loss: 1.0075325965881348
Epoch 1800, training loss: 62.18779373168945 = 0.025843562558293343 + 10.0 * 6.216195106506348
Epoch 1800, val loss: 1.0108962059020996
Epoch 1810, training loss: 62.1820182800293 = 0.025418100878596306 + 10.0 * 6.215660095214844
Epoch 1810, val loss: 1.0146592855453491
Epoch 1820, training loss: 62.178001403808594 = 0.025005705654621124 + 10.0 * 6.215299606323242
Epoch 1820, val loss: 1.0183112621307373
Epoch 1830, training loss: 62.24579620361328 = 0.02460332401096821 + 10.0 * 6.222119331359863
Epoch 1830, val loss: 1.021756649017334
Epoch 1840, training loss: 62.18535614013672 = 0.024197202175855637 + 10.0 * 6.216115951538086
Epoch 1840, val loss: 1.0254592895507812
Epoch 1850, training loss: 62.177650451660156 = 0.023805120959877968 + 10.0 * 6.215384483337402
Epoch 1850, val loss: 1.0288397073745728
Epoch 1860, training loss: 62.20924377441406 = 0.023428546264767647 + 10.0 * 6.218581199645996
Epoch 1860, val loss: 1.0324281454086304
Epoch 1870, training loss: 62.16683578491211 = 0.02305479347705841 + 10.0 * 6.2143778800964355
Epoch 1870, val loss: 1.0358425378799438
Epoch 1880, training loss: 62.16816329956055 = 0.022696584463119507 + 10.0 * 6.2145466804504395
Epoch 1880, val loss: 1.0393673181533813
Epoch 1890, training loss: 62.170997619628906 = 0.02234642393887043 + 10.0 * 6.214865207672119
Epoch 1890, val loss: 1.0426907539367676
Epoch 1900, training loss: 62.19672393798828 = 0.02200772613286972 + 10.0 * 6.217471599578857
Epoch 1900, val loss: 1.0459938049316406
Epoch 1910, training loss: 62.17437744140625 = 0.021658210083842278 + 10.0 * 6.215271949768066
Epoch 1910, val loss: 1.049861192703247
Epoch 1920, training loss: 62.172035217285156 = 0.02132670022547245 + 10.0 * 6.215070724487305
Epoch 1920, val loss: 1.0529004335403442
Epoch 1930, training loss: 62.16312789916992 = 0.021006016060709953 + 10.0 * 6.214211940765381
Epoch 1930, val loss: 1.0564419031143188
Epoch 1940, training loss: 62.15939712524414 = 0.020690277218818665 + 10.0 * 6.213870525360107
Epoch 1940, val loss: 1.0598704814910889
Epoch 1950, training loss: 62.20429229736328 = 0.020387155935168266 + 10.0 * 6.218390464782715
Epoch 1950, val loss: 1.0631458759307861
Epoch 1960, training loss: 62.16169357299805 = 0.02008107677102089 + 10.0 * 6.214161396026611
Epoch 1960, val loss: 1.0663591623306274
Epoch 1970, training loss: 62.14469528198242 = 0.01978064514696598 + 10.0 * 6.212491512298584
Epoch 1970, val loss: 1.0697365999221802
Epoch 1980, training loss: 62.14931106567383 = 0.019495924934744835 + 10.0 * 6.212981224060059
Epoch 1980, val loss: 1.0730270147323608
Epoch 1990, training loss: 62.1906623840332 = 0.019214488565921783 + 10.0 * 6.217144966125488
Epoch 1990, val loss: 1.0761531591415405
Epoch 2000, training loss: 62.16103744506836 = 0.01894170045852661 + 10.0 * 6.21420955657959
Epoch 2000, val loss: 1.079402208328247
Epoch 2010, training loss: 62.141536712646484 = 0.018664348870515823 + 10.0 * 6.212286949157715
Epoch 2010, val loss: 1.0827412605285645
Epoch 2020, training loss: 62.13115310668945 = 0.018403345718979836 + 10.0 * 6.211275100708008
Epoch 2020, val loss: 1.0859454870224
Epoch 2030, training loss: 62.143707275390625 = 0.018149588257074356 + 10.0 * 6.212555885314941
Epoch 2030, val loss: 1.0889326333999634
Epoch 2040, training loss: 62.18486785888672 = 0.017896510660648346 + 10.0 * 6.2166972160339355
Epoch 2040, val loss: 1.0920590162277222
Epoch 2050, training loss: 62.133819580078125 = 0.01763439178466797 + 10.0 * 6.211618423461914
Epoch 2050, val loss: 1.0952236652374268
Epoch 2060, training loss: 62.13209915161133 = 0.017392471432685852 + 10.0 * 6.211470603942871
Epoch 2060, val loss: 1.0985108613967896
Epoch 2070, training loss: 62.12215805053711 = 0.017156507819890976 + 10.0 * 6.210500240325928
Epoch 2070, val loss: 1.101667881011963
Epoch 2080, training loss: 62.132118225097656 = 0.016929784789681435 + 10.0 * 6.21151876449585
Epoch 2080, val loss: 1.1049017906188965
Epoch 2090, training loss: 62.165771484375 = 0.01670212298631668 + 10.0 * 6.214907169342041
Epoch 2090, val loss: 1.1080286502838135
Epoch 2100, training loss: 62.16278839111328 = 0.016471220180392265 + 10.0 * 6.2146315574646
Epoch 2100, val loss: 1.1106789112091064
Epoch 2110, training loss: 62.121803283691406 = 0.016251925379037857 + 10.0 * 6.210555076599121
Epoch 2110, val loss: 1.1139297485351562
Epoch 2120, training loss: 62.11742401123047 = 0.016038911417126656 + 10.0 * 6.210138320922852
Epoch 2120, val loss: 1.1168010234832764
Epoch 2130, training loss: 62.11626052856445 = 0.015830904245376587 + 10.0 * 6.210042953491211
Epoch 2130, val loss: 1.1200584173202515
Epoch 2140, training loss: 62.15848922729492 = 0.015625422820448875 + 10.0 * 6.2142863273620605
Epoch 2140, val loss: 1.1230864524841309
Epoch 2150, training loss: 62.128501892089844 = 0.01542349997907877 + 10.0 * 6.211308002471924
Epoch 2150, val loss: 1.1258150339126587
Epoch 2160, training loss: 62.11382293701172 = 0.015224370174109936 + 10.0 * 6.209859848022461
Epoch 2160, val loss: 1.1288379430770874
Epoch 2170, training loss: 62.13056564331055 = 0.01503213495016098 + 10.0 * 6.21155309677124
Epoch 2170, val loss: 1.1317170858383179
Epoch 2180, training loss: 62.1210823059082 = 0.014837811700999737 + 10.0 * 6.2106242179870605
Epoch 2180, val loss: 1.1348382234573364
Epoch 2190, training loss: 62.12437438964844 = 0.014650086872279644 + 10.0 * 6.210972785949707
Epoch 2190, val loss: 1.137302041053772
Epoch 2200, training loss: 62.1093864440918 = 0.014466391876339912 + 10.0 * 6.209492206573486
Epoch 2200, val loss: 1.140342354774475
Epoch 2210, training loss: 62.09885025024414 = 0.014287822879850864 + 10.0 * 6.208456516265869
Epoch 2210, val loss: 1.1432380676269531
Epoch 2220, training loss: 62.09654235839844 = 0.014113791286945343 + 10.0 * 6.208242893218994
Epoch 2220, val loss: 1.1461328268051147
Epoch 2230, training loss: 62.12055206298828 = 0.013945247046649456 + 10.0 * 6.210660457611084
Epoch 2230, val loss: 1.1489458084106445
Epoch 2240, training loss: 62.122589111328125 = 0.01377171091735363 + 10.0 * 6.21088171005249
Epoch 2240, val loss: 1.151672124862671
Epoch 2250, training loss: 62.097259521484375 = 0.013602945022284985 + 10.0 * 6.208365440368652
Epoch 2250, val loss: 1.1545486450195312
Epoch 2260, training loss: 62.094276428222656 = 0.013440310955047607 + 10.0 * 6.208083629608154
Epoch 2260, val loss: 1.1571511030197144
Epoch 2270, training loss: 62.0925178527832 = 0.013280626386404037 + 10.0 * 6.207923889160156
Epoch 2270, val loss: 1.159952163696289
Epoch 2280, training loss: 62.12596893310547 = 0.01312936469912529 + 10.0 * 6.2112836837768555
Epoch 2280, val loss: 1.1624183654785156
Epoch 2290, training loss: 62.11024475097656 = 0.012970785610377789 + 10.0 * 6.2097272872924805
Epoch 2290, val loss: 1.1651480197906494
Epoch 2300, training loss: 62.08232879638672 = 0.012811684980988503 + 10.0 * 6.20695161819458
Epoch 2300, val loss: 1.1679632663726807
Epoch 2310, training loss: 62.07612991333008 = 0.012665444053709507 + 10.0 * 6.20634651184082
Epoch 2310, val loss: 1.1706883907318115
Epoch 2320, training loss: 62.07686233520508 = 0.012523099780082703 + 10.0 * 6.2064337730407715
Epoch 2320, val loss: 1.1733614206314087
Epoch 2330, training loss: 62.11543273925781 = 0.012386592105031013 + 10.0 * 6.2103047370910645
Epoch 2330, val loss: 1.176092267036438
Epoch 2340, training loss: 62.08003616333008 = 0.012239236384630203 + 10.0 * 6.206779956817627
Epoch 2340, val loss: 1.1786880493164062
Epoch 2350, training loss: 62.0724983215332 = 0.01209944300353527 + 10.0 * 6.206039905548096
Epoch 2350, val loss: 1.1811127662658691
Epoch 2360, training loss: 62.07209396362305 = 0.011966458521783352 + 10.0 * 6.206012725830078
Epoch 2360, val loss: 1.183808445930481
Epoch 2370, training loss: 62.08592987060547 = 0.011834613047540188 + 10.0 * 6.207409858703613
Epoch 2370, val loss: 1.186530351638794
Epoch 2380, training loss: 62.09592056274414 = 0.011703956872224808 + 10.0 * 6.20842170715332
Epoch 2380, val loss: 1.1891366243362427
Epoch 2390, training loss: 62.07659149169922 = 0.011573557741940022 + 10.0 * 6.2065019607543945
Epoch 2390, val loss: 1.191462755203247
Epoch 2400, training loss: 62.073177337646484 = 0.011446893215179443 + 10.0 * 6.206172943115234
Epoch 2400, val loss: 1.1939305067062378
Epoch 2410, training loss: 62.0808219909668 = 0.011324177496135235 + 10.0 * 6.206949710845947
Epoch 2410, val loss: 1.196455478668213
Epoch 2420, training loss: 62.115962982177734 = 0.011202915571630001 + 10.0 * 6.210475921630859
Epoch 2420, val loss: 1.1987180709838867
Epoch 2430, training loss: 62.07123565673828 = 0.011079180054366589 + 10.0 * 6.206015586853027
Epoch 2430, val loss: 1.2013581991195679
Epoch 2440, training loss: 62.05991744995117 = 0.010962231084704399 + 10.0 * 6.204895496368408
Epoch 2440, val loss: 1.2037216424942017
Epoch 2450, training loss: 62.05317687988281 = 0.010847847908735275 + 10.0 * 6.204232692718506
Epoch 2450, val loss: 1.2062448263168335
Epoch 2460, training loss: 62.06014633178711 = 0.010737094096839428 + 10.0 * 6.2049407958984375
Epoch 2460, val loss: 1.2088559865951538
Epoch 2470, training loss: 62.141666412353516 = 0.01062685064971447 + 10.0 * 6.213103771209717
Epoch 2470, val loss: 1.2114636898040771
Epoch 2480, training loss: 62.083431243896484 = 0.01051622349768877 + 10.0 * 6.207291603088379
Epoch 2480, val loss: 1.2132132053375244
Epoch 2490, training loss: 62.04990005493164 = 0.010401866398751736 + 10.0 * 6.203949928283691
Epoch 2490, val loss: 1.2155641317367554
Epoch 2500, training loss: 62.05472946166992 = 0.010296906344592571 + 10.0 * 6.204443454742432
Epoch 2500, val loss: 1.218084692955017
Epoch 2510, training loss: 62.07274627685547 = 0.010196931660175323 + 10.0 * 6.206254959106445
Epoch 2510, val loss: 1.2203874588012695
Epoch 2520, training loss: 62.07041549682617 = 0.010091675445437431 + 10.0 * 6.206032752990723
Epoch 2520, val loss: 1.2229472398757935
Epoch 2530, training loss: 62.05731964111328 = 0.009988673962652683 + 10.0 * 6.204733371734619
Epoch 2530, val loss: 1.2251794338226318
Epoch 2540, training loss: 62.07677459716797 = 0.009889627806842327 + 10.0 * 6.206688404083252
Epoch 2540, val loss: 1.2273638248443604
Epoch 2550, training loss: 62.04977035522461 = 0.009791544638574123 + 10.0 * 6.203997611999512
Epoch 2550, val loss: 1.2296830415725708
Epoch 2560, training loss: 62.03855514526367 = 0.009694782085716724 + 10.0 * 6.20288610458374
Epoch 2560, val loss: 1.2321033477783203
Epoch 2570, training loss: 62.05136489868164 = 0.009602315723896027 + 10.0 * 6.204176425933838
Epoch 2570, val loss: 1.2342561483383179
Epoch 2580, training loss: 62.100406646728516 = 0.009510310366749763 + 10.0 * 6.209089756011963
Epoch 2580, val loss: 1.236332893371582
Epoch 2590, training loss: 62.05363082885742 = 0.00941469892859459 + 10.0 * 6.204421520233154
Epoch 2590, val loss: 1.2388675212860107
Epoch 2600, training loss: 62.040557861328125 = 0.009323430247604847 + 10.0 * 6.203123569488525
Epoch 2600, val loss: 1.2409178018569946
Epoch 2610, training loss: 62.0559196472168 = 0.009236922487616539 + 10.0 * 6.2046685218811035
Epoch 2610, val loss: 1.2432671785354614
Epoch 2620, training loss: 62.063140869140625 = 0.009148391894996166 + 10.0 * 6.205399513244629
Epoch 2620, val loss: 1.2454739809036255
Epoch 2630, training loss: 62.045387268066406 = 0.00906098261475563 + 10.0 * 6.203632831573486
Epoch 2630, val loss: 1.2477155923843384
Epoch 2640, training loss: 62.04335403442383 = 0.008976325392723083 + 10.0 * 6.203437805175781
Epoch 2640, val loss: 1.2500479221343994
Epoch 2650, training loss: 62.04764175415039 = 0.008893116377294064 + 10.0 * 6.2038750648498535
Epoch 2650, val loss: 1.2522075176239014
Epoch 2660, training loss: 62.04595947265625 = 0.00881208572536707 + 10.0 * 6.203714847564697
Epoch 2660, val loss: 1.2539706230163574
Epoch 2670, training loss: 62.0722770690918 = 0.008731083013117313 + 10.0 * 6.20635461807251
Epoch 2670, val loss: 1.2559980154037476
Epoch 2680, training loss: 62.034523010253906 = 0.008649110794067383 + 10.0 * 6.202587604522705
Epoch 2680, val loss: 1.2583695650100708
Epoch 2690, training loss: 62.01973342895508 = 0.00857120007276535 + 10.0 * 6.20111608505249
Epoch 2690, val loss: 1.260359525680542
Epoch 2700, training loss: 62.01910400390625 = 0.008495618589222431 + 10.0 * 6.201060771942139
Epoch 2700, val loss: 1.262529730796814
Epoch 2710, training loss: 62.056983947753906 = 0.008423682302236557 + 10.0 * 6.204855918884277
Epoch 2710, val loss: 1.2646585702896118
Epoch 2720, training loss: 62.02986145019531 = 0.008345181122422218 + 10.0 * 6.202151298522949
Epoch 2720, val loss: 1.266782522201538
Epoch 2730, training loss: 62.02741622924805 = 0.008268265053629875 + 10.0 * 6.2019147872924805
Epoch 2730, val loss: 1.2687530517578125
Epoch 2740, training loss: 62.022029876708984 = 0.00819513387978077 + 10.0 * 6.201383590698242
Epoch 2740, val loss: 1.2708051204681396
Epoch 2750, training loss: 62.044376373291016 = 0.008123425766825676 + 10.0 * 6.203625202178955
Epoch 2750, val loss: 1.2729214429855347
Epoch 2760, training loss: 62.02523422241211 = 0.008052073419094086 + 10.0 * 6.201718330383301
Epoch 2760, val loss: 1.2748980522155762
Epoch 2770, training loss: 62.044864654541016 = 0.007984206080436707 + 10.0 * 6.203688144683838
Epoch 2770, val loss: 1.2768453359603882
Epoch 2780, training loss: 62.02126693725586 = 0.007913549430668354 + 10.0 * 6.201335430145264
Epoch 2780, val loss: 1.2787376642227173
Epoch 2790, training loss: 62.02024459838867 = 0.007847829721868038 + 10.0 * 6.201239585876465
Epoch 2790, val loss: 1.2804173231124878
Epoch 2800, training loss: 62.036376953125 = 0.007782504428178072 + 10.0 * 6.202859401702881
Epoch 2800, val loss: 1.282326102256775
Epoch 2810, training loss: 62.02393341064453 = 0.007714096922427416 + 10.0 * 6.201622009277344
Epoch 2810, val loss: 1.2846965789794922
Epoch 2820, training loss: 62.0105094909668 = 0.007649867795407772 + 10.0 * 6.200285911560059
Epoch 2820, val loss: 1.28660249710083
Epoch 2830, training loss: 62.030479431152344 = 0.007587916683405638 + 10.0 * 6.20228910446167
Epoch 2830, val loss: 1.2884596586227417
Epoch 2840, training loss: 62.025901794433594 = 0.007523624692112207 + 10.0 * 6.20183801651001
Epoch 2840, val loss: 1.2903941869735718
Epoch 2850, training loss: 62.0153694152832 = 0.0074620237573981285 + 10.0 * 6.200790882110596
Epoch 2850, val loss: 1.292173147201538
Epoch 2860, training loss: 62.00288772583008 = 0.007399601396173239 + 10.0 * 6.199548721313477
Epoch 2860, val loss: 1.2940665483474731
Epoch 2870, training loss: 62.00376510620117 = 0.007340547628700733 + 10.0 * 6.199642658233643
Epoch 2870, val loss: 1.2959911823272705
Epoch 2880, training loss: 62.08982467651367 = 0.007282954640686512 + 10.0 * 6.208254337310791
Epoch 2880, val loss: 1.2976157665252686
Epoch 2890, training loss: 62.037139892578125 = 0.007221689913421869 + 10.0 * 6.202991962432861
Epoch 2890, val loss: 1.2996501922607422
Epoch 2900, training loss: 62.019710540771484 = 0.0071613299660384655 + 10.0 * 6.201254844665527
Epoch 2900, val loss: 1.301235318183899
Epoch 2910, training loss: 62.00946807861328 = 0.007104258518666029 + 10.0 * 6.2002363204956055
Epoch 2910, val loss: 1.3033548593521118
Epoch 2920, training loss: 62.005130767822266 = 0.007047547493129969 + 10.0 * 6.199808597564697
Epoch 2920, val loss: 1.3052905797958374
Epoch 2930, training loss: 62.019317626953125 = 0.00699366070330143 + 10.0 * 6.201232433319092
Epoch 2930, val loss: 1.3073126077651978
Epoch 2940, training loss: 61.9998893737793 = 0.0069401939399540424 + 10.0 * 6.1992950439453125
Epoch 2940, val loss: 1.3086498975753784
Epoch 2950, training loss: 62.000247955322266 = 0.006886420771479607 + 10.0 * 6.199336051940918
Epoch 2950, val loss: 1.3104826211929321
Epoch 2960, training loss: 62.04111099243164 = 0.006832493003457785 + 10.0 * 6.203427791595459
Epoch 2960, val loss: 1.312626838684082
Epoch 2970, training loss: 61.99058151245117 = 0.006778029724955559 + 10.0 * 6.198380470275879
Epoch 2970, val loss: 1.3137551546096802
Epoch 2980, training loss: 61.98444747924805 = 0.006725698243826628 + 10.0 * 6.197772026062012
Epoch 2980, val loss: 1.3157378435134888
Epoch 2990, training loss: 61.998313903808594 = 0.00667610764503479 + 10.0 * 6.199163913726807
Epoch 2990, val loss: 1.3176392316818237
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8444913020558777
The final CL Acc:0.78025, 0.01062, The final GNN Acc:0.83957, 0.00358
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11666])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10600])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.91937255859375 = 1.950881004333496 + 10.0 * 8.59684944152832
Epoch 0, val loss: 1.955744743347168
Epoch 10, training loss: 87.90396881103516 = 1.9404799938201904 + 10.0 * 8.596348762512207
Epoch 10, val loss: 1.94480299949646
Epoch 20, training loss: 87.852294921875 = 1.9277530908584595 + 10.0 * 8.59245491027832
Epoch 20, val loss: 1.9311364889144897
Epoch 30, training loss: 87.56502532958984 = 1.9109647274017334 + 10.0 * 8.56540584564209
Epoch 30, val loss: 1.9129661321640015
Epoch 40, training loss: 86.01354217529297 = 1.8911126852035522 + 10.0 * 8.412242889404297
Epoch 40, val loss: 1.8921093940734863
Epoch 50, training loss: 81.25350952148438 = 1.8707658052444458 + 10.0 * 7.938274383544922
Epoch 50, val loss: 1.8713123798370361
Epoch 60, training loss: 77.21790313720703 = 1.854337453842163 + 10.0 * 7.536356449127197
Epoch 60, val loss: 1.8558861017227173
Epoch 70, training loss: 74.21881103515625 = 1.843267560005188 + 10.0 * 7.23755407333374
Epoch 70, val loss: 1.845132827758789
Epoch 80, training loss: 72.3277587890625 = 1.8336498737335205 + 10.0 * 7.049410820007324
Epoch 80, val loss: 1.8354109525680542
Epoch 90, training loss: 70.8426513671875 = 1.826486349105835 + 10.0 * 6.90161657333374
Epoch 90, val loss: 1.8279705047607422
Epoch 100, training loss: 69.69355773925781 = 1.8197399377822876 + 10.0 * 6.787381649017334
Epoch 100, val loss: 1.8205729722976685
Epoch 110, training loss: 69.02544403076172 = 1.812111258506775 + 10.0 * 6.7213335037231445
Epoch 110, val loss: 1.8124195337295532
Epoch 120, training loss: 68.5256576538086 = 1.804551124572754 + 10.0 * 6.672110557556152
Epoch 120, val loss: 1.804485559463501
Epoch 130, training loss: 68.09001922607422 = 1.7972235679626465 + 10.0 * 6.629279613494873
Epoch 130, val loss: 1.7969691753387451
Epoch 140, training loss: 67.76702117919922 = 1.7900071144104004 + 10.0 * 6.597701549530029
Epoch 140, val loss: 1.7896307706832886
Epoch 150, training loss: 67.43977355957031 = 1.7825162410736084 + 10.0 * 6.565725326538086
Epoch 150, val loss: 1.7821121215820312
Epoch 160, training loss: 67.16414642333984 = 1.7745925188064575 + 10.0 * 6.5389556884765625
Epoch 160, val loss: 1.7745577096939087
Epoch 170, training loss: 66.93205261230469 = 1.7662557363510132 + 10.0 * 6.516579627990723
Epoch 170, val loss: 1.7666983604431152
Epoch 180, training loss: 66.72689056396484 = 1.7573214769363403 + 10.0 * 6.4969563484191895
Epoch 180, val loss: 1.7585293054580688
Epoch 190, training loss: 66.56412506103516 = 1.7476398944854736 + 10.0 * 6.4816484451293945
Epoch 190, val loss: 1.749793529510498
Epoch 200, training loss: 66.40978240966797 = 1.7370967864990234 + 10.0 * 6.467268466949463
Epoch 200, val loss: 1.740347981452942
Epoch 210, training loss: 66.2638168334961 = 1.7256200313568115 + 10.0 * 6.45382022857666
Epoch 210, val loss: 1.730283498764038
Epoch 220, training loss: 66.1405258178711 = 1.7131484746932983 + 10.0 * 6.442737579345703
Epoch 220, val loss: 1.719458818435669
Epoch 230, training loss: 66.05616760253906 = 1.6995091438293457 + 10.0 * 6.435665607452393
Epoch 230, val loss: 1.7077107429504395
Epoch 240, training loss: 65.92218780517578 = 1.6846367120742798 + 10.0 * 6.423755168914795
Epoch 240, val loss: 1.6951045989990234
Epoch 250, training loss: 65.8119125366211 = 1.6685950756072998 + 10.0 * 6.414331912994385
Epoch 250, val loss: 1.6815305948257446
Epoch 260, training loss: 65.75181579589844 = 1.6513092517852783 + 10.0 * 6.410051345825195
Epoch 260, val loss: 1.666959285736084
Epoch 270, training loss: 65.63186645507812 = 1.6323895454406738 + 10.0 * 6.399947166442871
Epoch 270, val loss: 1.6513841152191162
Epoch 280, training loss: 65.54115295410156 = 1.6122822761535645 + 10.0 * 6.392886638641357
Epoch 280, val loss: 1.6348472833633423
Epoch 290, training loss: 65.46210479736328 = 1.5908435583114624 + 10.0 * 6.3871259689331055
Epoch 290, val loss: 1.6173632144927979
Epoch 300, training loss: 65.4366455078125 = 1.5680433511734009 + 10.0 * 6.386859893798828
Epoch 300, val loss: 1.5989042520523071
Epoch 310, training loss: 65.3101806640625 = 1.5440038442611694 + 10.0 * 6.376617431640625
Epoch 310, val loss: 1.5795783996582031
Epoch 320, training loss: 65.23150634765625 = 1.518831729888916 + 10.0 * 6.371267318725586
Epoch 320, val loss: 1.5596257448196411
Epoch 330, training loss: 65.15689849853516 = 1.492641568183899 + 10.0 * 6.366425514221191
Epoch 330, val loss: 1.5390410423278809
Epoch 340, training loss: 65.1496353149414 = 1.4653905630111694 + 10.0 * 6.368424415588379
Epoch 340, val loss: 1.5179083347320557
Epoch 350, training loss: 65.03742218017578 = 1.437363624572754 + 10.0 * 6.360005855560303
Epoch 350, val loss: 1.4962791204452515
Epoch 360, training loss: 64.9595718383789 = 1.4089670181274414 + 10.0 * 6.355060577392578
Epoch 360, val loss: 1.4745867252349854
Epoch 370, training loss: 64.88533782958984 = 1.3802963495254517 + 10.0 * 6.350504398345947
Epoch 370, val loss: 1.4529540538787842
Epoch 380, training loss: 64.82089233398438 = 1.3512922525405884 + 10.0 * 6.346960067749023
Epoch 380, val loss: 1.4314883947372437
Epoch 390, training loss: 64.76741027832031 = 1.3220845460891724 + 10.0 * 6.3445329666137695
Epoch 390, val loss: 1.4102542400360107
Epoch 400, training loss: 64.77315521240234 = 1.2933114767074585 + 10.0 * 6.347984790802002
Epoch 400, val loss: 1.3892780542373657
Epoch 410, training loss: 64.64573669433594 = 1.2644672393798828 + 10.0 * 6.338127136230469
Epoch 410, val loss: 1.3688195943832397
Epoch 420, training loss: 64.59269714355469 = 1.2363396883010864 + 10.0 * 6.335635662078857
Epoch 420, val loss: 1.3491071462631226
Epoch 430, training loss: 64.53269958496094 = 1.2087842226028442 + 10.0 * 6.332391738891602
Epoch 430, val loss: 1.3302009105682373
Epoch 440, training loss: 64.4886703491211 = 1.1817102432250977 + 10.0 * 6.330696105957031
Epoch 440, val loss: 1.311930537223816
Epoch 450, training loss: 64.44058227539062 = 1.1551374197006226 + 10.0 * 6.3285441398620605
Epoch 450, val loss: 1.2945102453231812
Epoch 460, training loss: 64.3874740600586 = 1.1293827295303345 + 10.0 * 6.325809001922607
Epoch 460, val loss: 1.2778066396713257
Epoch 470, training loss: 64.36377716064453 = 1.1043238639831543 + 10.0 * 6.3259453773498535
Epoch 470, val loss: 1.2620735168457031
Epoch 480, training loss: 64.29879760742188 = 1.0800087451934814 + 10.0 * 6.321878910064697
Epoch 480, val loss: 1.2471184730529785
Epoch 490, training loss: 64.25272369384766 = 1.0563771724700928 + 10.0 * 6.319634914398193
Epoch 490, val loss: 1.2331161499023438
Epoch 500, training loss: 64.23143005371094 = 1.0334221124649048 + 10.0 * 6.31980037689209
Epoch 500, val loss: 1.219864845275879
Epoch 510, training loss: 64.17510986328125 = 1.01101553440094 + 10.0 * 6.316409587860107
Epoch 510, val loss: 1.207518219947815
Epoch 520, training loss: 64.14146423339844 = 0.9893380403518677 + 10.0 * 6.315212249755859
Epoch 520, val loss: 1.1957557201385498
Epoch 530, training loss: 64.09527587890625 = 0.9683159589767456 + 10.0 * 6.3126959800720215
Epoch 530, val loss: 1.1849297285079956
Epoch 540, training loss: 64.05338287353516 = 0.9477560520172119 + 10.0 * 6.310562610626221
Epoch 540, val loss: 1.1747264862060547
Epoch 550, training loss: 64.0086441040039 = 0.9278865456581116 + 10.0 * 6.308075904846191
Epoch 550, val loss: 1.1652382612228394
Epoch 560, training loss: 63.97032928466797 = 0.9085679650306702 + 10.0 * 6.30617618560791
Epoch 560, val loss: 1.156510591506958
Epoch 570, training loss: 63.972408294677734 = 0.8896902203559875 + 10.0 * 6.308271884918213
Epoch 570, val loss: 1.1483345031738281
Epoch 580, training loss: 63.97673034667969 = 0.8710350394248962 + 10.0 * 6.3105692863464355
Epoch 580, val loss: 1.1407181024551392
Epoch 590, training loss: 63.885658264160156 = 0.8530335426330566 + 10.0 * 6.303262233734131
Epoch 590, val loss: 1.1334757804870605
Epoch 600, training loss: 63.837772369384766 = 0.8356133103370667 + 10.0 * 6.300215721130371
Epoch 600, val loss: 1.1270428895950317
Epoch 610, training loss: 63.80070114135742 = 0.8185402750968933 + 10.0 * 6.298216342926025
Epoch 610, val loss: 1.121196985244751
Epoch 620, training loss: 63.77033615112305 = 0.8018245100975037 + 10.0 * 6.29685115814209
Epoch 620, val loss: 1.1158044338226318
Epoch 630, training loss: 63.78998565673828 = 0.78536057472229 + 10.0 * 6.300462245941162
Epoch 630, val loss: 1.1106914281845093
Epoch 640, training loss: 63.74211883544922 = 0.7692859172821045 + 10.0 * 6.297283172607422
Epoch 640, val loss: 1.1059327125549316
Epoch 650, training loss: 63.69049072265625 = 0.7535647749900818 + 10.0 * 6.293692588806152
Epoch 650, val loss: 1.1018503904342651
Epoch 660, training loss: 63.65353012084961 = 0.7382396459579468 + 10.0 * 6.291529178619385
Epoch 660, val loss: 1.098207950592041
Epoch 670, training loss: 63.627166748046875 = 0.7231725454330444 + 10.0 * 6.290399551391602
Epoch 670, val loss: 1.0948998928070068
Epoch 680, training loss: 63.621726989746094 = 0.7083069682121277 + 10.0 * 6.291342258453369
Epoch 680, val loss: 1.0918546915054321
Epoch 690, training loss: 63.58578109741211 = 0.6937306523323059 + 10.0 * 6.289205074310303
Epoch 690, val loss: 1.0891081094741821
Epoch 700, training loss: 63.54627227783203 = 0.6795570850372314 + 10.0 * 6.2866716384887695
Epoch 700, val loss: 1.0868489742279053
Epoch 710, training loss: 63.56818389892578 = 0.6656166315078735 + 10.0 * 6.290256977081299
Epoch 710, val loss: 1.0848710536956787
Epoch 720, training loss: 63.50144958496094 = 0.6519248485565186 + 10.0 * 6.284952640533447
Epoch 720, val loss: 1.083261489868164
Epoch 730, training loss: 63.47475051879883 = 0.6384844183921814 + 10.0 * 6.283626556396484
Epoch 730, val loss: 1.0818755626678467
Epoch 740, training loss: 63.49345397949219 = 0.6253563761711121 + 10.0 * 6.286809921264648
Epoch 740, val loss: 1.0808314085006714
Epoch 750, training loss: 63.43423843383789 = 0.6122481226921082 + 10.0 * 6.282198905944824
Epoch 750, val loss: 1.079901933670044
Epoch 760, training loss: 63.410316467285156 = 0.5994901657104492 + 10.0 * 6.281082630157471
Epoch 760, val loss: 1.0792796611785889
Epoch 770, training loss: 63.38559341430664 = 0.5868995785713196 + 10.0 * 6.279869556427002
Epoch 770, val loss: 1.07901930809021
Epoch 780, training loss: 63.358001708984375 = 0.5745319128036499 + 10.0 * 6.278347015380859
Epoch 780, val loss: 1.0790016651153564
Epoch 790, training loss: 63.38310623168945 = 0.5623063445091248 + 10.0 * 6.282080173492432
Epoch 790, val loss: 1.0790601968765259
Epoch 800, training loss: 63.34180450439453 = 0.5502251982688904 + 10.0 * 6.279158115386963
Epoch 800, val loss: 1.0793743133544922
Epoch 810, training loss: 63.295806884765625 = 0.5383241772651672 + 10.0 * 6.275748252868652
Epoch 810, val loss: 1.0798991918563843
Epoch 820, training loss: 63.27371597290039 = 0.5266200304031372 + 10.0 * 6.274709701538086
Epoch 820, val loss: 1.0807535648345947
Epoch 830, training loss: 63.301578521728516 = 0.5150771141052246 + 10.0 * 6.278650283813477
Epoch 830, val loss: 1.0817835330963135
Epoch 840, training loss: 63.25550842285156 = 0.5034933686256409 + 10.0 * 6.275201320648193
Epoch 840, val loss: 1.082902193069458
Epoch 850, training loss: 63.221405029296875 = 0.49218130111694336 + 10.0 * 6.272922515869141
Epoch 850, val loss: 1.0842437744140625
Epoch 860, training loss: 63.19200897216797 = 0.48093435168266296 + 10.0 * 6.2711076736450195
Epoch 860, val loss: 1.0857754945755005
Epoch 870, training loss: 63.21403121948242 = 0.4699067771434784 + 10.0 * 6.274412631988525
Epoch 870, val loss: 1.0875959396362305
Epoch 880, training loss: 63.188880920410156 = 0.4588203728199005 + 10.0 * 6.273005962371826
Epoch 880, val loss: 1.0891836881637573
Epoch 890, training loss: 63.13609313964844 = 0.44792017340660095 + 10.0 * 6.268816947937012
Epoch 890, val loss: 1.0911442041397095
Epoch 900, training loss: 63.11199951171875 = 0.43718579411506653 + 10.0 * 6.267481327056885
Epoch 900, val loss: 1.0934561491012573
Epoch 910, training loss: 63.09965133666992 = 0.4265576899051666 + 10.0 * 6.267309188842773
Epoch 910, val loss: 1.0958564281463623
Epoch 920, training loss: 63.142704010009766 = 0.4160347878932953 + 10.0 * 6.272666931152344
Epoch 920, val loss: 1.0982850790023804
Epoch 930, training loss: 63.06782913208008 = 0.4055686593055725 + 10.0 * 6.266226291656494
Epoch 930, val loss: 1.100746750831604
Epoch 940, training loss: 63.04755401611328 = 0.39529597759246826 + 10.0 * 6.265225887298584
Epoch 940, val loss: 1.1036275625228882
Epoch 950, training loss: 63.02700424194336 = 0.3851928412914276 + 10.0 * 6.264181137084961
Epoch 950, val loss: 1.106521725654602
Epoch 960, training loss: 63.066558837890625 = 0.3752708435058594 + 10.0 * 6.269128799438477
Epoch 960, val loss: 1.1096652746200562
Epoch 970, training loss: 63.01340866088867 = 0.3653230369091034 + 10.0 * 6.264808654785156
Epoch 970, val loss: 1.112334132194519
Epoch 980, training loss: 62.98648452758789 = 0.355673611164093 + 10.0 * 6.263081073760986
Epoch 980, val loss: 1.1158387660980225
Epoch 990, training loss: 62.975120544433594 = 0.34615135192871094 + 10.0 * 6.26289701461792
Epoch 990, val loss: 1.1192768812179565
Epoch 1000, training loss: 62.956512451171875 = 0.33682066202163696 + 10.0 * 6.261969089508057
Epoch 1000, val loss: 1.1229292154312134
Epoch 1010, training loss: 62.95612716674805 = 0.3276466131210327 + 10.0 * 6.262847900390625
Epoch 1010, val loss: 1.1267167329788208
Epoch 1020, training loss: 62.92080307006836 = 0.31861355900764465 + 10.0 * 6.260218620300293
Epoch 1020, val loss: 1.1304631233215332
Epoch 1030, training loss: 62.895931243896484 = 0.30978795886039734 + 10.0 * 6.258614540100098
Epoch 1030, val loss: 1.1345850229263306
Epoch 1040, training loss: 62.8950080871582 = 0.3011649549007416 + 10.0 * 6.2593841552734375
Epoch 1040, val loss: 1.1388081312179565
Epoch 1050, training loss: 62.887733459472656 = 0.2926938235759735 + 10.0 * 6.2595038414001465
Epoch 1050, val loss: 1.1430668830871582
Epoch 1060, training loss: 62.871707916259766 = 0.2844240963459015 + 10.0 * 6.258728504180908
Epoch 1060, val loss: 1.1475334167480469
Epoch 1070, training loss: 62.87275314331055 = 0.2762940526008606 + 10.0 * 6.259645938873291
Epoch 1070, val loss: 1.152062177658081
Epoch 1080, training loss: 62.83856201171875 = 0.26838505268096924 + 10.0 * 6.257017612457275
Epoch 1080, val loss: 1.1570494174957275
Epoch 1090, training loss: 62.825172424316406 = 0.2606341242790222 + 10.0 * 6.256453514099121
Epoch 1090, val loss: 1.161789059638977
Epoch 1100, training loss: 62.83468246459961 = 0.25313976407051086 + 10.0 * 6.258154392242432
Epoch 1100, val loss: 1.1669111251831055
Epoch 1110, training loss: 62.8001823425293 = 0.24583116173744202 + 10.0 * 6.255434989929199
Epoch 1110, val loss: 1.1721287965774536
Epoch 1120, training loss: 62.777626037597656 = 0.2387133538722992 + 10.0 * 6.253891468048096
Epoch 1120, val loss: 1.1775214672088623
Epoch 1130, training loss: 62.770957946777344 = 0.23178714513778687 + 10.0 * 6.2539167404174805
Epoch 1130, val loss: 1.1829990148544312
Epoch 1140, training loss: 62.817447662353516 = 0.22506509721279144 + 10.0 * 6.259238243103027
Epoch 1140, val loss: 1.1884771585464478
Epoch 1150, training loss: 62.7718391418457 = 0.2184107005596161 + 10.0 * 6.255342960357666
Epoch 1150, val loss: 1.1939191818237305
Epoch 1160, training loss: 62.728702545166016 = 0.21202881634235382 + 10.0 * 6.251667499542236
Epoch 1160, val loss: 1.1995816230773926
Epoch 1170, training loss: 62.71879196166992 = 0.20586803555488586 + 10.0 * 6.2512922286987305
Epoch 1170, val loss: 1.2055530548095703
Epoch 1180, training loss: 62.70701599121094 = 0.19989153742790222 + 10.0 * 6.2507123947143555
Epoch 1180, val loss: 1.211384654045105
Epoch 1190, training loss: 62.747962951660156 = 0.1940963864326477 + 10.0 * 6.255386829376221
Epoch 1190, val loss: 1.2172633409500122
Epoch 1200, training loss: 62.7452507019043 = 0.18835951387882233 + 10.0 * 6.2556891441345215
Epoch 1200, val loss: 1.2229934930801392
Epoch 1210, training loss: 62.68421173095703 = 0.18288400769233704 + 10.0 * 6.2501325607299805
Epoch 1210, val loss: 1.229291558265686
Epoch 1220, training loss: 62.666236877441406 = 0.17757347226142883 + 10.0 * 6.248866081237793
Epoch 1220, val loss: 1.2355467081069946
Epoch 1230, training loss: 62.65478515625 = 0.1724531352519989 + 10.0 * 6.248232841491699
Epoch 1230, val loss: 1.2418009042739868
Epoch 1240, training loss: 62.64844512939453 = 0.16749778389930725 + 10.0 * 6.24809455871582
Epoch 1240, val loss: 1.2481775283813477
Epoch 1250, training loss: 62.70439529418945 = 0.16267181932926178 + 10.0 * 6.254172325134277
Epoch 1250, val loss: 1.2543925046920776
Epoch 1260, training loss: 62.671417236328125 = 0.15800867974758148 + 10.0 * 6.251340866088867
Epoch 1260, val loss: 1.261069893836975
Epoch 1270, training loss: 62.62700653076172 = 0.1534336656332016 + 10.0 * 6.247357368469238
Epoch 1270, val loss: 1.2671557664871216
Epoch 1280, training loss: 62.61311721801758 = 0.14907118678092957 + 10.0 * 6.246404647827148
Epoch 1280, val loss: 1.2737518548965454
Epoch 1290, training loss: 62.60348129272461 = 0.14485055208206177 + 10.0 * 6.24586296081543
Epoch 1290, val loss: 1.2803945541381836
Epoch 1300, training loss: 62.67048263549805 = 0.14078980684280396 + 10.0 * 6.252969264984131
Epoch 1300, val loss: 1.2872575521469116
Epoch 1310, training loss: 62.61159133911133 = 0.13674844801425934 + 10.0 * 6.24748420715332
Epoch 1310, val loss: 1.2933323383331299
Epoch 1320, training loss: 62.58235168457031 = 0.13291913270950317 + 10.0 * 6.244943141937256
Epoch 1320, val loss: 1.3002018928527832
Epoch 1330, training loss: 62.578426361083984 = 0.12920568883419037 + 10.0 * 6.244922161102295
Epoch 1330, val loss: 1.3068746328353882
Epoch 1340, training loss: 62.588626861572266 = 0.1256226897239685 + 10.0 * 6.24630069732666
Epoch 1340, val loss: 1.3135746717453003
Epoch 1350, training loss: 62.565799713134766 = 0.12209635972976685 + 10.0 * 6.244370460510254
Epoch 1350, val loss: 1.3203082084655762
Epoch 1360, training loss: 62.549434661865234 = 0.1187310516834259 + 10.0 * 6.243070125579834
Epoch 1360, val loss: 1.326852560043335
Epoch 1370, training loss: 62.613624572753906 = 0.11549416184425354 + 10.0 * 6.249813079833984
Epoch 1370, val loss: 1.3336237668991089
Epoch 1380, training loss: 62.55942916870117 = 0.11227930337190628 + 10.0 * 6.244715213775635
Epoch 1380, val loss: 1.3402312994003296
Epoch 1390, training loss: 62.52916717529297 = 0.10922979563474655 + 10.0 * 6.2419939041137695
Epoch 1390, val loss: 1.3470313549041748
Epoch 1400, training loss: 62.522064208984375 = 0.10628379136323929 + 10.0 * 6.241578102111816
Epoch 1400, val loss: 1.3539626598358154
Epoch 1410, training loss: 62.5581169128418 = 0.10345311462879181 + 10.0 * 6.245466232299805
Epoch 1410, val loss: 1.360679268836975
Epoch 1420, training loss: 62.53169631958008 = 0.1006464809179306 + 10.0 * 6.243104934692383
Epoch 1420, val loss: 1.3671348094940186
Epoch 1430, training loss: 62.50435256958008 = 0.09796956181526184 + 10.0 * 6.240638256072998
Epoch 1430, val loss: 1.3741669654846191
Epoch 1440, training loss: 62.50034713745117 = 0.09538470208644867 + 10.0 * 6.2404961585998535
Epoch 1440, val loss: 1.3809561729431152
Epoch 1450, training loss: 62.49359893798828 = 0.09288685768842697 + 10.0 * 6.2400712966918945
Epoch 1450, val loss: 1.3878414630889893
Epoch 1460, training loss: 62.48771667480469 = 0.09048208594322205 + 10.0 * 6.2397236824035645
Epoch 1460, val loss: 1.3948265314102173
Epoch 1470, training loss: 62.521663665771484 = 0.0881364494562149 + 10.0 * 6.243352890014648
Epoch 1470, val loss: 1.401369333267212
Epoch 1480, training loss: 62.486053466796875 = 0.08582428842782974 + 10.0 * 6.240023136138916
Epoch 1480, val loss: 1.4081430435180664
Epoch 1490, training loss: 62.47711181640625 = 0.08364265412092209 + 10.0 * 6.239346981048584
Epoch 1490, val loss: 1.4150805473327637
Epoch 1500, training loss: 62.486968994140625 = 0.08151639252901077 + 10.0 * 6.240545272827148
Epoch 1500, val loss: 1.4217945337295532
Epoch 1510, training loss: 62.45592498779297 = 0.07945360988378525 + 10.0 * 6.23764705657959
Epoch 1510, val loss: 1.4286596775054932
Epoch 1520, training loss: 62.44301986694336 = 0.0774732232093811 + 10.0 * 6.2365546226501465
Epoch 1520, val loss: 1.435448169708252
Epoch 1530, training loss: 62.44457244873047 = 0.07556656002998352 + 10.0 * 6.236900806427002
Epoch 1530, val loss: 1.4422627687454224
Epoch 1540, training loss: 62.4901008605957 = 0.0737103745341301 + 10.0 * 6.241639137268066
Epoch 1540, val loss: 1.448914647102356
Epoch 1550, training loss: 62.448795318603516 = 0.07188805937767029 + 10.0 * 6.2376909255981445
Epoch 1550, val loss: 1.4559271335601807
Epoch 1560, training loss: 62.431854248046875 = 0.07014535367488861 + 10.0 * 6.236170768737793
Epoch 1560, val loss: 1.4626848697662354
Epoch 1570, training loss: 62.45026397705078 = 0.06847327202558517 + 10.0 * 6.2381792068481445
Epoch 1570, val loss: 1.4694602489471436
Epoch 1580, training loss: 62.43245315551758 = 0.06681327521800995 + 10.0 * 6.236563682556152
Epoch 1580, val loss: 1.4756664037704468
Epoch 1590, training loss: 62.40839385986328 = 0.06522133201360703 + 10.0 * 6.234317302703857
Epoch 1590, val loss: 1.4825456142425537
Epoch 1600, training loss: 62.40472412109375 = 0.0637011006474495 + 10.0 * 6.234102249145508
Epoch 1600, val loss: 1.4893224239349365
Epoch 1610, training loss: 62.402565002441406 = 0.06222774460911751 + 10.0 * 6.234033584594727
Epoch 1610, val loss: 1.496042251586914
Epoch 1620, training loss: 62.463321685791016 = 0.06079107150435448 + 10.0 * 6.24025297164917
Epoch 1620, val loss: 1.5025123357772827
Epoch 1630, training loss: 62.4007568359375 = 0.05936945974826813 + 10.0 * 6.2341389656066895
Epoch 1630, val loss: 1.5087753534317017
Epoch 1640, training loss: 62.38420867919922 = 0.058015793561935425 + 10.0 * 6.232619285583496
Epoch 1640, val loss: 1.5155677795410156
Epoch 1650, training loss: 62.38384246826172 = 0.05672347918152809 + 10.0 * 6.2327117919921875
Epoch 1650, val loss: 1.522339105606079
Epoch 1660, training loss: 62.395633697509766 = 0.05546943098306656 + 10.0 * 6.234016418457031
Epoch 1660, val loss: 1.5288143157958984
Epoch 1670, training loss: 62.40901184082031 = 0.05423954874277115 + 10.0 * 6.235476970672607
Epoch 1670, val loss: 1.5348957777023315
Epoch 1680, training loss: 62.37797164916992 = 0.0530206672847271 + 10.0 * 6.232495307922363
Epoch 1680, val loss: 1.5412670373916626
Epoch 1690, training loss: 62.364173889160156 = 0.05185019224882126 + 10.0 * 6.231232643127441
Epoch 1690, val loss: 1.5474525690078735
Epoch 1700, training loss: 62.36833953857422 = 0.05074244737625122 + 10.0 * 6.231759548187256
Epoch 1700, val loss: 1.554129958152771
Epoch 1710, training loss: 62.38822555541992 = 0.04966577887535095 + 10.0 * 6.233855724334717
Epoch 1710, val loss: 1.5603525638580322
Epoch 1720, training loss: 62.35953140258789 = 0.048594556748867035 + 10.0 * 6.231093406677246
Epoch 1720, val loss: 1.5665010213851929
Epoch 1730, training loss: 62.3594856262207 = 0.04757310450077057 + 10.0 * 6.231191158294678
Epoch 1730, val loss: 1.57270348072052
Epoch 1740, training loss: 62.35758590698242 = 0.046581242233514786 + 10.0 * 6.231100559234619
Epoch 1740, val loss: 1.5789549350738525
Epoch 1750, training loss: 62.35967254638672 = 0.04561525583267212 + 10.0 * 6.231405735015869
Epoch 1750, val loss: 1.58510160446167
Epoch 1760, training loss: 62.361000061035156 = 0.04467207193374634 + 10.0 * 6.231632709503174
Epoch 1760, val loss: 1.591013789176941
Epoch 1770, training loss: 62.35095977783203 = 0.04373762384057045 + 10.0 * 6.230721950531006
Epoch 1770, val loss: 1.596850037574768
Epoch 1780, training loss: 62.36102294921875 = 0.04285572096705437 + 10.0 * 6.23181676864624
Epoch 1780, val loss: 1.6027363538742065
Epoch 1790, training loss: 62.34007263183594 = 0.04199354723095894 + 10.0 * 6.2298078536987305
Epoch 1790, val loss: 1.6087286472320557
Epoch 1800, training loss: 62.350704193115234 = 0.041157983243465424 + 10.0 * 6.230954647064209
Epoch 1800, val loss: 1.6145005226135254
Epoch 1810, training loss: 62.35101318359375 = 0.040331073105335236 + 10.0 * 6.231068134307861
Epoch 1810, val loss: 1.6200430393218994
Epoch 1820, training loss: 62.33694076538086 = 0.039539117366075516 + 10.0 * 6.229740142822266
Epoch 1820, val loss: 1.6260768175125122
Epoch 1830, training loss: 62.32297134399414 = 0.03876948729157448 + 10.0 * 6.228420257568359
Epoch 1830, val loss: 1.6318286657333374
Epoch 1840, training loss: 62.31258010864258 = 0.03801940008997917 + 10.0 * 6.227456092834473
Epoch 1840, val loss: 1.6373929977416992
Epoch 1850, training loss: 62.309471130371094 = 0.03729930520057678 + 10.0 * 6.227217197418213
Epoch 1850, val loss: 1.643169641494751
Epoch 1860, training loss: 62.37909698486328 = 0.03661248832941055 + 10.0 * 6.234248161315918
Epoch 1860, val loss: 1.6489274501800537
Epoch 1870, training loss: 62.351356506347656 = 0.0358889140188694 + 10.0 * 6.231546878814697
Epoch 1870, val loss: 1.653988003730774
Epoch 1880, training loss: 62.3139762878418 = 0.03520797938108444 + 10.0 * 6.227876663208008
Epoch 1880, val loss: 1.6594605445861816
Epoch 1890, training loss: 62.2992057800293 = 0.03455083444714546 + 10.0 * 6.226465702056885
Epoch 1890, val loss: 1.664968729019165
Epoch 1900, training loss: 62.304969787597656 = 0.03392421826720238 + 10.0 * 6.227104663848877
Epoch 1900, val loss: 1.6704845428466797
Epoch 1910, training loss: 62.33449935913086 = 0.03330134227871895 + 10.0 * 6.230119705200195
Epoch 1910, val loss: 1.6756384372711182
Epoch 1920, training loss: 62.297149658203125 = 0.03268992155790329 + 10.0 * 6.226446151733398
Epoch 1920, val loss: 1.6810404062271118
Epoch 1930, training loss: 62.283958435058594 = 0.032097961753606796 + 10.0 * 6.225186347961426
Epoch 1930, val loss: 1.6864384412765503
Epoch 1940, training loss: 62.305519104003906 = 0.03153203800320625 + 10.0 * 6.227398872375488
Epoch 1940, val loss: 1.6917273998260498
Epoch 1950, training loss: 62.2894172668457 = 0.030965061858296394 + 10.0 * 6.2258453369140625
Epoch 1950, val loss: 1.6965603828430176
Epoch 1960, training loss: 62.29264450073242 = 0.03041265718638897 + 10.0 * 6.226223468780518
Epoch 1960, val loss: 1.7016018629074097
Epoch 1970, training loss: 62.2957649230957 = 0.02987942285835743 + 10.0 * 6.226588249206543
Epoch 1970, val loss: 1.7065268754959106
Epoch 1980, training loss: 62.286888122558594 = 0.029368653893470764 + 10.0 * 6.225751876831055
Epoch 1980, val loss: 1.712001085281372
Epoch 1990, training loss: 62.27643966674805 = 0.028860988095402718 + 10.0 * 6.224757671356201
Epoch 1990, val loss: 1.7169314622879028
Epoch 2000, training loss: 62.26653289794922 = 0.028369875624775887 + 10.0 * 6.223816394805908
Epoch 2000, val loss: 1.721968412399292
Epoch 2010, training loss: 62.26424026489258 = 0.027894694358110428 + 10.0 * 6.223634719848633
Epoch 2010, val loss: 1.7269458770751953
Epoch 2020, training loss: 62.31769561767578 = 0.027432521805167198 + 10.0 * 6.2290263175964355
Epoch 2020, val loss: 1.7317862510681152
Epoch 2030, training loss: 62.282779693603516 = 0.026962213218212128 + 10.0 * 6.225581645965576
Epoch 2030, val loss: 1.736090064048767
Epoch 2040, training loss: 62.26987838745117 = 0.02651403658092022 + 10.0 * 6.224336624145508
Epoch 2040, val loss: 1.7413636445999146
Epoch 2050, training loss: 62.27695083618164 = 0.026083549484610558 + 10.0 * 6.225086688995361
Epoch 2050, val loss: 1.7461621761322021
Epoch 2060, training loss: 62.25468063354492 = 0.025654559955000877 + 10.0 * 6.222902774810791
Epoch 2060, val loss: 1.7507826089859009
Epoch 2070, training loss: 62.25494384765625 = 0.025237848982214928 + 10.0 * 6.222970485687256
Epoch 2070, val loss: 1.7553645372390747
Epoch 2080, training loss: 62.279136657714844 = 0.02483668550848961 + 10.0 * 6.225430011749268
Epoch 2080, val loss: 1.7598603963851929
Epoch 2090, training loss: 62.25093078613281 = 0.02444094978272915 + 10.0 * 6.222649097442627
Epoch 2090, val loss: 1.7646815776824951
Epoch 2100, training loss: 62.244239807128906 = 0.024050934240221977 + 10.0 * 6.222018718719482
Epoch 2100, val loss: 1.7691019773483276
Epoch 2110, training loss: 62.25522232055664 = 0.023669768124818802 + 10.0 * 6.2231550216674805
Epoch 2110, val loss: 1.7736328840255737
Epoch 2120, training loss: 62.263938903808594 = 0.0233056228607893 + 10.0 * 6.224063396453857
Epoch 2120, val loss: 1.778092622756958
Epoch 2130, training loss: 62.23405456542969 = 0.02294064871966839 + 10.0 * 6.221111297607422
Epoch 2130, val loss: 1.7826381921768188
Epoch 2140, training loss: 62.234619140625 = 0.02258879505097866 + 10.0 * 6.221202850341797
Epoch 2140, val loss: 1.787103533744812
Epoch 2150, training loss: 62.226112365722656 = 0.02224811166524887 + 10.0 * 6.220386505126953
Epoch 2150, val loss: 1.7916474342346191
Epoch 2160, training loss: 62.22736740112305 = 0.021917492151260376 + 10.0 * 6.220544815063477
Epoch 2160, val loss: 1.7961289882659912
Epoch 2170, training loss: 62.318321228027344 = 0.021600304171442986 + 10.0 * 6.229672431945801
Epoch 2170, val loss: 1.8004564046859741
Epoch 2180, training loss: 62.270362854003906 = 0.02125377766788006 + 10.0 * 6.224910736083984
Epoch 2180, val loss: 1.8039671182632446
Epoch 2190, training loss: 62.22804641723633 = 0.020931953564286232 + 10.0 * 6.220711708068848
Epoch 2190, val loss: 1.8084079027175903
Epoch 2200, training loss: 62.25890350341797 = 0.0206233449280262 + 10.0 * 6.223828315734863
Epoch 2200, val loss: 1.8127650022506714
Epoch 2210, training loss: 62.214656829833984 = 0.02032172679901123 + 10.0 * 6.219433784484863
Epoch 2210, val loss: 1.8172624111175537
Epoch 2220, training loss: 62.21210479736328 = 0.020032746717333794 + 10.0 * 6.219207286834717
Epoch 2220, val loss: 1.8214423656463623
Epoch 2230, training loss: 62.211734771728516 = 0.019748033955693245 + 10.0 * 6.219198703765869
Epoch 2230, val loss: 1.8257217407226562
Epoch 2240, training loss: 62.26249694824219 = 0.019470077008008957 + 10.0 * 6.224302768707275
Epoch 2240, val loss: 1.8297632932662964
Epoch 2250, training loss: 62.2242317199707 = 0.019192228093743324 + 10.0 * 6.220503807067871
Epoch 2250, val loss: 1.8337894678115845
Epoch 2260, training loss: 62.23466110229492 = 0.018917266279459 + 10.0 * 6.221574306488037
Epoch 2260, val loss: 1.8378801345825195
Epoch 2270, training loss: 62.208396911621094 = 0.018645307049155235 + 10.0 * 6.218975067138672
Epoch 2270, val loss: 1.8414971828460693
Epoch 2280, training loss: 62.20363998413086 = 0.01838858053088188 + 10.0 * 6.218525409698486
Epoch 2280, val loss: 1.8457396030426025
Epoch 2290, training loss: 62.23143005371094 = 0.01813986524939537 + 10.0 * 6.221329212188721
Epoch 2290, val loss: 1.8495615720748901
Epoch 2300, training loss: 62.19968795776367 = 0.017887800931930542 + 10.0 * 6.218180179595947
Epoch 2300, val loss: 1.8533639907836914
Epoch 2310, training loss: 62.2126579284668 = 0.017648933455348015 + 10.0 * 6.219500541687012
Epoch 2310, val loss: 1.8573492765426636
Epoch 2320, training loss: 62.212120056152344 = 0.0174102783203125 + 10.0 * 6.219470977783203
Epoch 2320, val loss: 1.861096739768982
Epoch 2330, training loss: 62.18703079223633 = 0.017173895612359047 + 10.0 * 6.216985702514648
Epoch 2330, val loss: 1.8647648096084595
Epoch 2340, training loss: 62.204349517822266 = 0.01694890856742859 + 10.0 * 6.218739986419678
Epoch 2340, val loss: 1.868314504623413
Epoch 2350, training loss: 62.22203826904297 = 0.016724007204174995 + 10.0 * 6.220531463623047
Epoch 2350, val loss: 1.871972680091858
Epoch 2360, training loss: 62.216190338134766 = 0.016500581055879593 + 10.0 * 6.219968795776367
Epoch 2360, val loss: 1.875724196434021
Epoch 2370, training loss: 62.20478820800781 = 0.016281679272651672 + 10.0 * 6.218850612640381
Epoch 2370, val loss: 1.8792121410369873
Epoch 2380, training loss: 62.18040084838867 = 0.016072474420070648 + 10.0 * 6.216433048248291
Epoch 2380, val loss: 1.8832240104675293
Epoch 2390, training loss: 62.17933654785156 = 0.015868201851844788 + 10.0 * 6.216346740722656
Epoch 2390, val loss: 1.8868509531021118
Epoch 2400, training loss: 62.180320739746094 = 0.01566977985203266 + 10.0 * 6.216464996337891
Epoch 2400, val loss: 1.8905279636383057
Epoch 2410, training loss: 62.20762252807617 = 0.015475817956030369 + 10.0 * 6.21921443939209
Epoch 2410, val loss: 1.8939464092254639
Epoch 2420, training loss: 62.20279312133789 = 0.015275036916136742 + 10.0 * 6.218751907348633
Epoch 2420, val loss: 1.897294521331787
Epoch 2430, training loss: 62.19086456298828 = 0.015077723190188408 + 10.0 * 6.217578411102295
Epoch 2430, val loss: 1.900426983833313
Epoch 2440, training loss: 62.171939849853516 = 0.014888749457895756 + 10.0 * 6.215704917907715
Epoch 2440, val loss: 1.9042998552322388
Epoch 2450, training loss: 62.163429260253906 = 0.014707215130329132 + 10.0 * 6.214872360229492
Epoch 2450, val loss: 1.9077792167663574
Epoch 2460, training loss: 62.167232513427734 = 0.014531451277434826 + 10.0 * 6.215270042419434
Epoch 2460, val loss: 1.911233901977539
Epoch 2470, training loss: 62.22745895385742 = 0.014359337277710438 + 10.0 * 6.221310138702393
Epoch 2470, val loss: 1.9143925905227661
Epoch 2480, training loss: 62.19364929199219 = 0.014184425584971905 + 10.0 * 6.217946529388428
Epoch 2480, val loss: 1.9181841611862183
Epoch 2490, training loss: 62.18937301635742 = 0.014011377468705177 + 10.0 * 6.217535972595215
Epoch 2490, val loss: 1.921209454536438
Epoch 2500, training loss: 62.179325103759766 = 0.013839972205460072 + 10.0 * 6.216548442840576
Epoch 2500, val loss: 1.9244377613067627
Epoch 2510, training loss: 62.19308853149414 = 0.013678858987987041 + 10.0 * 6.217940807342529
Epoch 2510, val loss: 1.9276059865951538
Epoch 2520, training loss: 62.154144287109375 = 0.0135125033557415 + 10.0 * 6.2140631675720215
Epoch 2520, val loss: 1.9307440519332886
Epoch 2530, training loss: 62.158164978027344 = 0.013356492854654789 + 10.0 * 6.214480876922607
Epoch 2530, val loss: 1.9340547323226929
Epoch 2540, training loss: 62.175575256347656 = 0.013205621391534805 + 10.0 * 6.2162370681762695
Epoch 2540, val loss: 1.9373024702072144
Epoch 2550, training loss: 62.15668487548828 = 0.013052595779299736 + 10.0 * 6.214363098144531
Epoch 2550, val loss: 1.9404062032699585
Epoch 2560, training loss: 62.16127014160156 = 0.012904157862067223 + 10.0 * 6.214836597442627
Epoch 2560, val loss: 1.9438241720199585
Epoch 2570, training loss: 62.197322845458984 = 0.012762470170855522 + 10.0 * 6.218455791473389
Epoch 2570, val loss: 1.9470903873443604
Epoch 2580, training loss: 62.154083251953125 = 0.01260695606470108 + 10.0 * 6.214147567749023
Epoch 2580, val loss: 1.949620008468628
Epoch 2590, training loss: 62.15562057495117 = 0.012468340806663036 + 10.0 * 6.214314937591553
Epoch 2590, val loss: 1.9529993534088135
Epoch 2600, training loss: 62.150550842285156 = 0.012329258024692535 + 10.0 * 6.213822364807129
Epoch 2600, val loss: 1.9561043977737427
Epoch 2610, training loss: 62.137535095214844 = 0.012191049754619598 + 10.0 * 6.212534427642822
Epoch 2610, val loss: 1.9589027166366577
Epoch 2620, training loss: 62.179283142089844 = 0.012060022912919521 + 10.0 * 6.21672248840332
Epoch 2620, val loss: 1.9616378545761108
Epoch 2630, training loss: 62.1478271484375 = 0.011925536207854748 + 10.0 * 6.213590145111084
Epoch 2630, val loss: 1.9648964405059814
Epoch 2640, training loss: 62.142982482910156 = 0.011794239282608032 + 10.0 * 6.213118553161621
Epoch 2640, val loss: 1.9675302505493164
Epoch 2650, training loss: 62.1590576171875 = 0.011667651124298573 + 10.0 * 6.214738845825195
Epoch 2650, val loss: 1.970565915107727
Epoch 2660, training loss: 62.14875411987305 = 0.011540940031409264 + 10.0 * 6.21372127532959
Epoch 2660, val loss: 1.9733631610870361
Epoch 2670, training loss: 62.14729690551758 = 0.011418028734624386 + 10.0 * 6.213587760925293
Epoch 2670, val loss: 1.9761325120925903
Epoch 2680, training loss: 62.16176223754883 = 0.011297095566987991 + 10.0 * 6.2150468826293945
Epoch 2680, val loss: 1.9788583517074585
Epoch 2690, training loss: 62.126220703125 = 0.011177397333085537 + 10.0 * 6.2115044593811035
Epoch 2690, val loss: 1.9818367958068848
Epoch 2700, training loss: 62.14936828613281 = 0.011062677018344402 + 10.0 * 6.213830471038818
Epoch 2700, val loss: 1.9847233295440674
Epoch 2710, training loss: 62.136474609375 = 0.010944287292659283 + 10.0 * 6.212553024291992
Epoch 2710, val loss: 1.9870641231536865
Epoch 2720, training loss: 62.13046646118164 = 0.010832958854734898 + 10.0 * 6.211963176727295
Epoch 2720, val loss: 1.989945888519287
Epoch 2730, training loss: 62.12980651855469 = 0.010723340325057507 + 10.0 * 6.211908340454102
Epoch 2730, val loss: 1.9925130605697632
Epoch 2740, training loss: 62.15766525268555 = 0.010614422149956226 + 10.0 * 6.214704990386963
Epoch 2740, val loss: 1.9950640201568604
Epoch 2750, training loss: 62.140785217285156 = 0.010508354753255844 + 10.0 * 6.213027477264404
Epoch 2750, val loss: 1.9977048635482788
Epoch 2760, training loss: 62.126827239990234 = 0.010402044281363487 + 10.0 * 6.211642265319824
Epoch 2760, val loss: 2.0003879070281982
Epoch 2770, training loss: 62.13706970214844 = 0.010300884954631329 + 10.0 * 6.212677001953125
Epoch 2770, val loss: 2.002995252609253
Epoch 2780, training loss: 62.1304817199707 = 0.010197984986007214 + 10.0 * 6.212028503417969
Epoch 2780, val loss: 2.005347967147827
Epoch 2790, training loss: 62.14419937133789 = 0.010095620527863503 + 10.0 * 6.213410377502441
Epoch 2790, val loss: 2.0079405307769775
Epoch 2800, training loss: 62.11806869506836 = 0.00999440997838974 + 10.0 * 6.2108073234558105
Epoch 2800, val loss: 2.0105583667755127
Epoch 2810, training loss: 62.117008209228516 = 0.0099006537348032 + 10.0 * 6.2107110023498535
Epoch 2810, val loss: 2.0133097171783447
Epoch 2820, training loss: 62.13895034790039 = 0.009808889590203762 + 10.0 * 6.21291446685791
Epoch 2820, val loss: 2.015648365020752
Epoch 2830, training loss: 62.13128662109375 = 0.009708461351692677 + 10.0 * 6.212157726287842
Epoch 2830, val loss: 2.017559051513672
Epoch 2840, training loss: 62.10977554321289 = 0.009614997543394566 + 10.0 * 6.210015773773193
Epoch 2840, val loss: 2.0203518867492676
Epoch 2850, training loss: 62.10366439819336 = 0.009525218047201633 + 10.0 * 6.209414005279541
Epoch 2850, val loss: 2.022820234298706
Epoch 2860, training loss: 62.133338928222656 = 0.00943666510283947 + 10.0 * 6.212389945983887
Epoch 2860, val loss: 2.024925947189331
Epoch 2870, training loss: 62.12035369873047 = 0.009346922859549522 + 10.0 * 6.2111005783081055
Epoch 2870, val loss: 2.027501344680786
Epoch 2880, training loss: 62.10157775878906 = 0.009258776903152466 + 10.0 * 6.209231853485107
Epoch 2880, val loss: 2.0298011302948
Epoch 2890, training loss: 62.09551239013672 = 0.009173360653221607 + 10.0 * 6.208633899688721
Epoch 2890, val loss: 2.0321896076202393
Epoch 2900, training loss: 62.10999298095703 = 0.009092406369745731 + 10.0 * 6.210089683532715
Epoch 2900, val loss: 2.0347495079040527
Epoch 2910, training loss: 62.146324157714844 = 0.00901051890105009 + 10.0 * 6.213731288909912
Epoch 2910, val loss: 2.036886215209961
Epoch 2920, training loss: 62.11960220336914 = 0.008924178779125214 + 10.0 * 6.211068153381348
Epoch 2920, val loss: 2.038698196411133
Epoch 2930, training loss: 62.10240173339844 = 0.008840457536280155 + 10.0 * 6.209356307983398
Epoch 2930, val loss: 2.040977954864502
Epoch 2940, training loss: 62.094783782958984 = 0.008763255551457405 + 10.0 * 6.208601951599121
Epoch 2940, val loss: 2.0434517860412598
Epoch 2950, training loss: 62.09865951538086 = 0.008686527609825134 + 10.0 * 6.2089972496032715
Epoch 2950, val loss: 2.045635461807251
Epoch 2960, training loss: 62.110897064208984 = 0.008610747754573822 + 10.0 * 6.21022891998291
Epoch 2960, val loss: 2.0476884841918945
Epoch 2970, training loss: 62.09115219116211 = 0.008535387925803661 + 10.0 * 6.208261489868164
Epoch 2970, val loss: 2.0501160621643066
Epoch 2980, training loss: 62.13188171386719 = 0.008463827893137932 + 10.0 * 6.212341785430908
Epoch 2980, val loss: 2.0519654750823975
Epoch 2990, training loss: 62.117210388183594 = 0.008385948836803436 + 10.0 * 6.210882663726807
Epoch 2990, val loss: 2.0538101196289062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 87.90303039550781 = 1.9350507259368896 + 10.0 * 8.596797943115234
Epoch 0, val loss: 1.9305146932601929
Epoch 10, training loss: 87.88438415527344 = 1.925362467765808 + 10.0 * 8.595902442932129
Epoch 10, val loss: 1.9210058450698853
Epoch 20, training loss: 87.80972290039062 = 1.9130561351776123 + 10.0 * 8.589666366577148
Epoch 20, val loss: 1.908493161201477
Epoch 30, training loss: 87.38734436035156 = 1.8972221612930298 + 10.0 * 8.549012184143066
Epoch 30, val loss: 1.892304539680481
Epoch 40, training loss: 84.88875579833984 = 1.8798404932022095 + 10.0 * 8.300891876220703
Epoch 40, val loss: 1.8751122951507568
Epoch 50, training loss: 77.93273162841797 = 1.8612319231033325 + 10.0 * 7.607150554656982
Epoch 50, val loss: 1.857013463973999
Epoch 60, training loss: 74.41941833496094 = 1.8486098051071167 + 10.0 * 7.257081031799316
Epoch 60, val loss: 1.84494149684906
Epoch 70, training loss: 72.03903198242188 = 1.8392813205718994 + 10.0 * 7.019975185394287
Epoch 70, val loss: 1.8356136083602905
Epoch 80, training loss: 70.87910461425781 = 1.8303910493850708 + 10.0 * 6.904871463775635
Epoch 80, val loss: 1.8270283937454224
Epoch 90, training loss: 70.02281188964844 = 1.8222591876983643 + 10.0 * 6.8200554847717285
Epoch 90, val loss: 1.8191852569580078
Epoch 100, training loss: 69.29058074951172 = 1.815231204032898 + 10.0 * 6.74753475189209
Epoch 100, val loss: 1.812258243560791
Epoch 110, training loss: 68.65959167480469 = 1.8091528415679932 + 10.0 * 6.685043811798096
Epoch 110, val loss: 1.8059338331222534
Epoch 120, training loss: 68.07107543945312 = 1.803767442703247 + 10.0 * 6.626730918884277
Epoch 120, val loss: 1.8001470565795898
Epoch 130, training loss: 67.64656829833984 = 1.7985121011734009 + 10.0 * 6.584805011749268
Epoch 130, val loss: 1.7945564985275269
Epoch 140, training loss: 67.29856872558594 = 1.7927814722061157 + 10.0 * 6.550579071044922
Epoch 140, val loss: 1.7885775566101074
Epoch 150, training loss: 67.0352783203125 = 1.7866548299789429 + 10.0 * 6.524862766265869
Epoch 150, val loss: 1.7823631763458252
Epoch 160, training loss: 66.79747009277344 = 1.7802895307540894 + 10.0 * 6.501718044281006
Epoch 160, val loss: 1.7758965492248535
Epoch 170, training loss: 66.6068115234375 = 1.7735040187835693 + 10.0 * 6.483330726623535
Epoch 170, val loss: 1.7691385746002197
Epoch 180, training loss: 66.46295928955078 = 1.7661523818969727 + 10.0 * 6.4696807861328125
Epoch 180, val loss: 1.761973261833191
Epoch 190, training loss: 66.28473663330078 = 1.7582356929779053 + 10.0 * 6.45265007019043
Epoch 190, val loss: 1.7543389797210693
Epoch 200, training loss: 66.18113708496094 = 1.749680995941162 + 10.0 * 6.443145751953125
Epoch 200, val loss: 1.7462103366851807
Epoch 210, training loss: 66.0202407836914 = 1.7402338981628418 + 10.0 * 6.4280009269714355
Epoch 210, val loss: 1.737447738647461
Epoch 220, training loss: 65.8945083618164 = 1.7300101518630981 + 10.0 * 6.416449546813965
Epoch 220, val loss: 1.7279905080795288
Epoch 230, training loss: 65.81221008300781 = 1.7187398672103882 + 10.0 * 6.409347057342529
Epoch 230, val loss: 1.7175631523132324
Epoch 240, training loss: 65.6926498413086 = 1.7063746452331543 + 10.0 * 6.398627281188965
Epoch 240, val loss: 1.706204891204834
Epoch 250, training loss: 65.59025573730469 = 1.692932367324829 + 10.0 * 6.3897318840026855
Epoch 250, val loss: 1.6939959526062012
Epoch 260, training loss: 65.53076934814453 = 1.6783010959625244 + 10.0 * 6.385247230529785
Epoch 260, val loss: 1.6807562112808228
Epoch 270, training loss: 65.43914031982422 = 1.6622377634048462 + 10.0 * 6.377690315246582
Epoch 270, val loss: 1.6663256883621216
Epoch 280, training loss: 65.3563003540039 = 1.6448620557785034 + 10.0 * 6.3711442947387695
Epoch 280, val loss: 1.6507740020751953
Epoch 290, training loss: 65.27437591552734 = 1.6261389255523682 + 10.0 * 6.364823818206787
Epoch 290, val loss: 1.6341159343719482
Epoch 300, training loss: 65.20555114746094 = 1.606074571609497 + 10.0 * 6.359947681427002
Epoch 300, val loss: 1.616317868232727
Epoch 310, training loss: 65.15721893310547 = 1.5845422744750977 + 10.0 * 6.357267379760742
Epoch 310, val loss: 1.5973961353302002
Epoch 320, training loss: 65.07463073730469 = 1.5617175102233887 + 10.0 * 6.351291179656982
Epoch 320, val loss: 1.5774693489074707
Epoch 330, training loss: 65.00917053222656 = 1.5376611948013306 + 10.0 * 6.347151279449463
Epoch 330, val loss: 1.5566518306732178
Epoch 340, training loss: 64.94752502441406 = 1.5123586654663086 + 10.0 * 6.343516826629639
Epoch 340, val loss: 1.5349862575531006
Epoch 350, training loss: 64.87877655029297 = 1.4860613346099854 + 10.0 * 6.339271545410156
Epoch 350, val loss: 1.5127986669540405
Epoch 360, training loss: 64.81580352783203 = 1.4589122533798218 + 10.0 * 6.335689067840576
Epoch 360, val loss: 1.4900654554367065
Epoch 370, training loss: 64.77510833740234 = 1.4311206340789795 + 10.0 * 6.3343987464904785
Epoch 370, val loss: 1.467180609703064
Epoch 380, training loss: 64.72093963623047 = 1.4025766849517822 + 10.0 * 6.331836223602295
Epoch 380, val loss: 1.443859338760376
Epoch 390, training loss: 64.64197540283203 = 1.3739230632781982 + 10.0 * 6.326805114746094
Epoch 390, val loss: 1.4207687377929688
Epoch 400, training loss: 64.5743637084961 = 1.3450905084609985 + 10.0 * 6.322927474975586
Epoch 400, val loss: 1.3979673385620117
Epoch 410, training loss: 64.54151916503906 = 1.3162693977355957 + 10.0 * 6.3225250244140625
Epoch 410, val loss: 1.3755548000335693
Epoch 420, training loss: 64.53135681152344 = 1.28742253780365 + 10.0 * 6.324393272399902
Epoch 420, val loss: 1.3532629013061523
Epoch 430, training loss: 64.4311752319336 = 1.2587811946868896 + 10.0 * 6.317239761352539
Epoch 430, val loss: 1.331549882888794
Epoch 440, training loss: 64.36294555664062 = 1.2306134700775146 + 10.0 * 6.313233375549316
Epoch 440, val loss: 1.3105971813201904
Epoch 450, training loss: 64.31375885009766 = 1.2028521299362183 + 10.0 * 6.311090469360352
Epoch 450, val loss: 1.290373682975769
Epoch 460, training loss: 64.26960754394531 = 1.175305962562561 + 10.0 * 6.309430122375488
Epoch 460, val loss: 1.270506739616394
Epoch 470, training loss: 64.22578430175781 = 1.1482757329940796 + 10.0 * 6.307750701904297
Epoch 470, val loss: 1.251412034034729
Epoch 480, training loss: 64.16800689697266 = 1.1216685771942139 + 10.0 * 6.304634094238281
Epoch 480, val loss: 1.233047604560852
Epoch 490, training loss: 64.12198638916016 = 1.0954936742782593 + 10.0 * 6.302649021148682
Epoch 490, val loss: 1.215266466140747
Epoch 500, training loss: 64.09195709228516 = 1.0696666240692139 + 10.0 * 6.302229404449463
Epoch 500, val loss: 1.1979906558990479
Epoch 510, training loss: 64.04396057128906 = 1.0443089008331299 + 10.0 * 6.299964904785156
Epoch 510, val loss: 1.1813726425170898
Epoch 520, training loss: 63.99491500854492 = 1.019473910331726 + 10.0 * 6.297544002532959
Epoch 520, val loss: 1.16536283493042
Epoch 530, training loss: 63.96347427368164 = 0.9950582981109619 + 10.0 * 6.296841621398926
Epoch 530, val loss: 1.149933099746704
Epoch 540, training loss: 63.9234733581543 = 0.9710264801979065 + 10.0 * 6.2952446937561035
Epoch 540, val loss: 1.134948492050171
Epoch 550, training loss: 63.882022857666016 = 0.9475632309913635 + 10.0 * 6.293446063995361
Epoch 550, val loss: 1.1205159425735474
Epoch 560, training loss: 63.83448028564453 = 0.9245286583900452 + 10.0 * 6.290995121002197
Epoch 560, val loss: 1.1066826581954956
Epoch 570, training loss: 63.82996368408203 = 0.9019914269447327 + 10.0 * 6.292797088623047
Epoch 570, val loss: 1.0932395458221436
Epoch 580, training loss: 63.771148681640625 = 0.8798172473907471 + 10.0 * 6.289133071899414
Epoch 580, val loss: 1.0798593759536743
Epoch 590, training loss: 63.72848129272461 = 0.8583421111106873 + 10.0 * 6.287014007568359
Epoch 590, val loss: 1.0674405097961426
Epoch 600, training loss: 63.68766403198242 = 0.8374761939048767 + 10.0 * 6.2850189208984375
Epoch 600, val loss: 1.0556977987289429
Epoch 610, training loss: 63.691776275634766 = 0.8171031475067139 + 10.0 * 6.287467002868652
Epoch 610, val loss: 1.0442326068878174
Epoch 620, training loss: 63.63933181762695 = 0.7972745299339294 + 10.0 * 6.284205436706543
Epoch 620, val loss: 1.0334798097610474
Epoch 630, training loss: 63.59651565551758 = 0.7779715657234192 + 10.0 * 6.281854152679443
Epoch 630, val loss: 1.023036003112793
Epoch 640, training loss: 63.558780670166016 = 0.759266197681427 + 10.0 * 6.279951572418213
Epoch 640, val loss: 1.0133378505706787
Epoch 650, training loss: 63.584625244140625 = 0.7409570217132568 + 10.0 * 6.284367084503174
Epoch 650, val loss: 1.0038232803344727
Epoch 660, training loss: 63.49939727783203 = 0.7231768369674683 + 10.0 * 6.277622222900391
Epoch 660, val loss: 0.9947735667228699
Epoch 670, training loss: 63.47707748413086 = 0.7059857845306396 + 10.0 * 6.277109146118164
Epoch 670, val loss: 0.9863647222518921
Epoch 680, training loss: 63.43843460083008 = 0.6892160177230835 + 10.0 * 6.274921894073486
Epoch 680, val loss: 0.9784069061279297
Epoch 690, training loss: 63.44855499267578 = 0.6729247570037842 + 10.0 * 6.277563095092773
Epoch 690, val loss: 0.9710798263549805
Epoch 700, training loss: 63.41728210449219 = 0.6568083167076111 + 10.0 * 6.276047706604004
Epoch 700, val loss: 0.9635204076766968
Epoch 710, training loss: 63.3843879699707 = 0.6412588357925415 + 10.0 * 6.274312973022461
Epoch 710, val loss: 0.9568150043487549
Epoch 720, training loss: 63.348106384277344 = 0.6261385679244995 + 10.0 * 6.2721967697143555
Epoch 720, val loss: 0.9505610466003418
Epoch 730, training loss: 63.31256103515625 = 0.6114100217819214 + 10.0 * 6.270115375518799
Epoch 730, val loss: 0.9448407292366028
Epoch 740, training loss: 63.37729263305664 = 0.5970975756645203 + 10.0 * 6.278019428253174
Epoch 740, val loss: 0.9393808841705322
Epoch 750, training loss: 63.274166107177734 = 0.5828880071640015 + 10.0 * 6.26912784576416
Epoch 750, val loss: 0.9340083003044128
Epoch 760, training loss: 63.248023986816406 = 0.5691707730293274 + 10.0 * 6.267885208129883
Epoch 760, val loss: 0.9292033910751343
Epoch 770, training loss: 63.220359802246094 = 0.5558183193206787 + 10.0 * 6.266454219818115
Epoch 770, val loss: 0.9249905347824097
Epoch 780, training loss: 63.21207809448242 = 0.5427283644676208 + 10.0 * 6.266934871673584
Epoch 780, val loss: 0.9209617972373962
Epoch 790, training loss: 63.20039367675781 = 0.5297690629959106 + 10.0 * 6.267062187194824
Epoch 790, val loss: 0.9169536828994751
Epoch 800, training loss: 63.177669525146484 = 0.5171090364456177 + 10.0 * 6.266056060791016
Epoch 800, val loss: 0.9135315418243408
Epoch 810, training loss: 63.13897705078125 = 0.504776656627655 + 10.0 * 6.263420104980469
Epoch 810, val loss: 0.9103167057037354
Epoch 820, training loss: 63.14939880371094 = 0.4927005469799042 + 10.0 * 6.265669822692871
Epoch 820, val loss: 0.9074192047119141
Epoch 830, training loss: 63.11397933959961 = 0.4807237684726715 + 10.0 * 6.2633256912231445
Epoch 830, val loss: 0.9047078490257263
Epoch 840, training loss: 63.08904266357422 = 0.4690267741680145 + 10.0 * 6.2620015144348145
Epoch 840, val loss: 0.9023998975753784
Epoch 850, training loss: 63.071128845214844 = 0.4574963450431824 + 10.0 * 6.2613630294799805
Epoch 850, val loss: 0.9002871513366699
Epoch 860, training loss: 63.054378509521484 = 0.44611313939094543 + 10.0 * 6.260826587677002
Epoch 860, val loss: 0.8981939554214478
Epoch 870, training loss: 63.029911041259766 = 0.4349404573440552 + 10.0 * 6.259497165679932
Epoch 870, val loss: 0.8962892889976501
Epoch 880, training loss: 63.0014762878418 = 0.4239586293697357 + 10.0 * 6.257751941680908
Epoch 880, val loss: 0.894904613494873
Epoch 890, training loss: 63.02644348144531 = 0.4131397306919098 + 10.0 * 6.261330604553223
Epoch 890, val loss: 0.8934689164161682
Epoch 900, training loss: 63.027095794677734 = 0.4023776948451996 + 10.0 * 6.262471675872803
Epoch 900, val loss: 0.8917586207389832
Epoch 910, training loss: 62.952186584472656 = 0.3918740153312683 + 10.0 * 6.256031513214111
Epoch 910, val loss: 0.8906915187835693
Epoch 920, training loss: 62.93406677246094 = 0.3815769553184509 + 10.0 * 6.2552490234375
Epoch 920, val loss: 0.8898820281028748
Epoch 930, training loss: 62.913448333740234 = 0.37150147557258606 + 10.0 * 6.254194736480713
Epoch 930, val loss: 0.8889686465263367
Epoch 940, training loss: 62.96586990356445 = 0.3616528809070587 + 10.0 * 6.2604217529296875
Epoch 940, val loss: 0.8884132504463196
Epoch 950, training loss: 62.90782165527344 = 0.3517986536026001 + 10.0 * 6.2556023597717285
Epoch 950, val loss: 0.8878141045570374
Epoch 960, training loss: 62.874359130859375 = 0.3423093259334564 + 10.0 * 6.253205299377441
Epoch 960, val loss: 0.8874045610427856
Epoch 970, training loss: 62.859764099121094 = 0.33296436071395874 + 10.0 * 6.252679824829102
Epoch 970, val loss: 0.8871912956237793
Epoch 980, training loss: 62.832847595214844 = 0.32384729385375977 + 10.0 * 6.250899791717529
Epoch 980, val loss: 0.8870862722396851
Epoch 990, training loss: 62.88337707519531 = 0.31492647528648376 + 10.0 * 6.256844997406006
Epoch 990, val loss: 0.8869816064834595
Epoch 1000, training loss: 62.82346725463867 = 0.3062101900577545 + 10.0 * 6.251725673675537
Epoch 1000, val loss: 0.8874998688697815
Epoch 1010, training loss: 62.80619812011719 = 0.29768696427345276 + 10.0 * 6.250851154327393
Epoch 1010, val loss: 0.8878544569015503
Epoch 1020, training loss: 62.800472259521484 = 0.2893678843975067 + 10.0 * 6.251110553741455
Epoch 1020, val loss: 0.8881670832633972
Epoch 1030, training loss: 62.77418899536133 = 0.28130194544792175 + 10.0 * 6.249288558959961
Epoch 1030, val loss: 0.8890857696533203
Epoch 1040, training loss: 62.752098083496094 = 0.273403525352478 + 10.0 * 6.247869491577148
Epoch 1040, val loss: 0.8896787762641907
Epoch 1050, training loss: 62.742183685302734 = 0.2657678425312042 + 10.0 * 6.247641563415527
Epoch 1050, val loss: 0.8908183574676514
Epoch 1060, training loss: 62.74414825439453 = 0.2582935690879822 + 10.0 * 6.2485857009887695
Epoch 1060, val loss: 0.8919013142585754
Epoch 1070, training loss: 62.7172737121582 = 0.2510148584842682 + 10.0 * 6.246625900268555
Epoch 1070, val loss: 0.8930948376655579
Epoch 1080, training loss: 62.72364807128906 = 0.24395133554935455 + 10.0 * 6.247969627380371
Epoch 1080, val loss: 0.8945317268371582
Epoch 1090, training loss: 62.70973587036133 = 0.23700419068336487 + 10.0 * 6.2472734451293945
Epoch 1090, val loss: 0.8956230282783508
Epoch 1100, training loss: 62.67771911621094 = 0.23034390807151794 + 10.0 * 6.24473762512207
Epoch 1100, val loss: 0.8972392082214355
Epoch 1110, training loss: 62.66242599487305 = 0.22386270761489868 + 10.0 * 6.243856430053711
Epoch 1110, val loss: 0.8988652229309082
Epoch 1120, training loss: 62.653602600097656 = 0.21757875382900238 + 10.0 * 6.243602275848389
Epoch 1120, val loss: 0.9007576107978821
Epoch 1130, training loss: 62.64992141723633 = 0.21147069334983826 + 10.0 * 6.243844985961914
Epoch 1130, val loss: 0.9027260541915894
Epoch 1140, training loss: 62.67381286621094 = 0.2055072784423828 + 10.0 * 6.246830940246582
Epoch 1140, val loss: 0.9045405387878418
Epoch 1150, training loss: 62.63348388671875 = 0.19975163042545319 + 10.0 * 6.243372917175293
Epoch 1150, val loss: 0.9069012403488159
Epoch 1160, training loss: 62.609981536865234 = 0.1941300332546234 + 10.0 * 6.2415852546691895
Epoch 1160, val loss: 0.9089817404747009
Epoch 1170, training loss: 62.63570022583008 = 0.18870443105697632 + 10.0 * 6.244699478149414
Epoch 1170, val loss: 0.9113240838050842
Epoch 1180, training loss: 62.63204574584961 = 0.1834234893321991 + 10.0 * 6.2448625564575195
Epoch 1180, val loss: 0.9136813879013062
Epoch 1190, training loss: 62.583045959472656 = 0.17827896773815155 + 10.0 * 6.240476608276367
Epoch 1190, val loss: 0.9164947867393494
Epoch 1200, training loss: 62.55802917480469 = 0.17332737147808075 + 10.0 * 6.238470077514648
Epoch 1200, val loss: 0.9191391468048096
Epoch 1210, training loss: 62.551151275634766 = 0.16854742169380188 + 10.0 * 6.238260269165039
Epoch 1210, val loss: 0.9220221638679504
Epoch 1220, training loss: 62.61970520019531 = 0.1639147847890854 + 10.0 * 6.245579242706299
Epoch 1220, val loss: 0.9249541163444519
Epoch 1230, training loss: 62.57832717895508 = 0.15938223898410797 + 10.0 * 6.241894721984863
Epoch 1230, val loss: 0.9278672933578491
Epoch 1240, training loss: 62.53285598754883 = 0.15502111613750458 + 10.0 * 6.237783432006836
Epoch 1240, val loss: 0.9308937788009644
Epoch 1250, training loss: 62.516929626464844 = 0.15083228051662445 + 10.0 * 6.23660945892334
Epoch 1250, val loss: 0.93438321352005
Epoch 1260, training loss: 62.50912857055664 = 0.1467779278755188 + 10.0 * 6.23623514175415
Epoch 1260, val loss: 0.9377527236938477
Epoch 1270, training loss: 62.53313446044922 = 0.14284735918045044 + 10.0 * 6.239028453826904
Epoch 1270, val loss: 0.9412604570388794
Epoch 1280, training loss: 62.51581954956055 = 0.13899703323841095 + 10.0 * 6.237682342529297
Epoch 1280, val loss: 0.9445752501487732
Epoch 1290, training loss: 62.48759460449219 = 0.13529084622859955 + 10.0 * 6.235230445861816
Epoch 1290, val loss: 0.9482686519622803
Epoch 1300, training loss: 62.49323654174805 = 0.13171137869358063 + 10.0 * 6.236152648925781
Epoch 1300, val loss: 0.9518831372261047
Epoch 1310, training loss: 62.49356460571289 = 0.128255233168602 + 10.0 * 6.2365312576293945
Epoch 1310, val loss: 0.9557896256446838
Epoch 1320, training loss: 62.481414794921875 = 0.12488014250993729 + 10.0 * 6.235653400421143
Epoch 1320, val loss: 0.959661066532135
Epoch 1330, training loss: 62.46464157104492 = 0.12163175642490387 + 10.0 * 6.2343010902404785
Epoch 1330, val loss: 0.9635019898414612
Epoch 1340, training loss: 62.44735336303711 = 0.11847732216119766 + 10.0 * 6.2328877449035645
Epoch 1340, val loss: 0.9674155712127686
Epoch 1350, training loss: 62.47294998168945 = 0.1154349073767662 + 10.0 * 6.235751152038574
Epoch 1350, val loss: 0.9713737368583679
Epoch 1360, training loss: 62.43806076049805 = 0.11247656494379044 + 10.0 * 6.232558250427246
Epoch 1360, val loss: 0.9755478501319885
Epoch 1370, training loss: 62.434513092041016 = 0.10961617529392242 + 10.0 * 6.232489585876465
Epoch 1370, val loss: 0.9798011779785156
Epoch 1380, training loss: 62.42622756958008 = 0.10683059692382812 + 10.0 * 6.231939792633057
Epoch 1380, val loss: 0.9838298559188843
Epoch 1390, training loss: 62.416908264160156 = 0.10414177924394608 + 10.0 * 6.231276512145996
Epoch 1390, val loss: 0.9880976676940918
Epoch 1400, training loss: 62.41790008544922 = 0.10154422372579575 + 10.0 * 6.231635570526123
Epoch 1400, val loss: 0.9924103617668152
Epoch 1410, training loss: 62.43309020996094 = 0.09902038425207138 + 10.0 * 6.233407020568848
Epoch 1410, val loss: 0.9966580867767334
Epoch 1420, training loss: 62.41516876220703 = 0.09654898196458817 + 10.0 * 6.2318620681762695
Epoch 1420, val loss: 1.0007723569869995
Epoch 1430, training loss: 62.3958625793457 = 0.09417479485273361 + 10.0 * 6.23016881942749
Epoch 1430, val loss: 1.0051112174987793
Epoch 1440, training loss: 62.38604736328125 = 0.0918826088309288 + 10.0 * 6.229416847229004
Epoch 1440, val loss: 1.0094573497772217
Epoch 1450, training loss: 62.379905700683594 = 0.08965827524662018 + 10.0 * 6.229024887084961
Epoch 1450, val loss: 1.013991355895996
Epoch 1460, training loss: 62.38138198852539 = 0.08749748766422272 + 10.0 * 6.229388236999512
Epoch 1460, val loss: 1.0184329748153687
Epoch 1470, training loss: 62.403709411621094 = 0.08539542555809021 + 10.0 * 6.2318315505981445
Epoch 1470, val loss: 1.0228902101516724
Epoch 1480, training loss: 62.367530822753906 = 0.083346888422966 + 10.0 * 6.228418350219727
Epoch 1480, val loss: 1.0271658897399902
Epoch 1490, training loss: 62.35752868652344 = 0.08136681467294693 + 10.0 * 6.227616310119629
Epoch 1490, val loss: 1.031727910041809
Epoch 1500, training loss: 62.391971588134766 = 0.07945634424686432 + 10.0 * 6.2312517166137695
Epoch 1500, val loss: 1.0360863208770752
Epoch 1510, training loss: 62.34663772583008 = 0.07758542895317078 + 10.0 * 6.22690486907959
Epoch 1510, val loss: 1.0407512187957764
Epoch 1520, training loss: 62.34172058105469 = 0.07578295469284058 + 10.0 * 6.226593971252441
Epoch 1520, val loss: 1.0454223155975342
Epoch 1530, training loss: 62.34915542602539 = 0.07403137534856796 + 10.0 * 6.227512359619141
Epoch 1530, val loss: 1.0500119924545288
Epoch 1540, training loss: 62.32314682006836 = 0.07233040779829025 + 10.0 * 6.225081443786621
Epoch 1540, val loss: 1.0544984340667725
Epoch 1550, training loss: 62.32806396484375 = 0.07068078964948654 + 10.0 * 6.225738048553467
Epoch 1550, val loss: 1.0590389966964722
Epoch 1560, training loss: 62.3421630859375 = 0.0690789446234703 + 10.0 * 6.22730827331543
Epoch 1560, val loss: 1.0635830163955688
Epoch 1570, training loss: 62.34649658203125 = 0.06751728802919388 + 10.0 * 6.227898120880127
Epoch 1570, val loss: 1.068277359008789
Epoch 1580, training loss: 62.339698791503906 = 0.06599334627389908 + 10.0 * 6.227370262145996
Epoch 1580, val loss: 1.0724639892578125
Epoch 1590, training loss: 62.30992889404297 = 0.06450788676738739 + 10.0 * 6.224542140960693
Epoch 1590, val loss: 1.077234148979187
Epoch 1600, training loss: 62.308292388916016 = 0.06306958943605423 + 10.0 * 6.224522590637207
Epoch 1600, val loss: 1.0817174911499023
Epoch 1610, training loss: 62.30452346801758 = 0.06169121339917183 + 10.0 * 6.224283218383789
Epoch 1610, val loss: 1.0863029956817627
Epoch 1620, training loss: 62.30458068847656 = 0.060339007526636124 + 10.0 * 6.224423885345459
Epoch 1620, val loss: 1.090954065322876
Epoch 1630, training loss: 62.336490631103516 = 0.05903908237814903 + 10.0 * 6.227745056152344
Epoch 1630, val loss: 1.095513939857483
Epoch 1640, training loss: 62.291629791259766 = 0.05773390829563141 + 10.0 * 6.223389625549316
Epoch 1640, val loss: 1.0999640226364136
Epoch 1650, training loss: 62.2763557434082 = 0.056490857154130936 + 10.0 * 6.221986293792725
Epoch 1650, val loss: 1.104533314704895
Epoch 1660, training loss: 62.26877975463867 = 0.05528479442000389 + 10.0 * 6.221349239349365
Epoch 1660, val loss: 1.1090360879898071
Epoch 1670, training loss: 62.28843307495117 = 0.05412612110376358 + 10.0 * 6.223430633544922
Epoch 1670, val loss: 1.1138229370117188
Epoch 1680, training loss: 62.27280044555664 = 0.05297566205263138 + 10.0 * 6.221982479095459
Epoch 1680, val loss: 1.1180530786514282
Epoch 1690, training loss: 62.274932861328125 = 0.05185693874955177 + 10.0 * 6.2223076820373535
Epoch 1690, val loss: 1.122801661491394
Epoch 1700, training loss: 62.276832580566406 = 0.05077069625258446 + 10.0 * 6.222606182098389
Epoch 1700, val loss: 1.1271030902862549
Epoch 1710, training loss: 62.26078796386719 = 0.0497174896299839 + 10.0 * 6.221107006072998
Epoch 1710, val loss: 1.1316851377487183
Epoch 1720, training loss: 62.27217102050781 = 0.04869754612445831 + 10.0 * 6.222347259521484
Epoch 1720, val loss: 1.1361860036849976
Epoch 1730, training loss: 62.25725555419922 = 0.04770456999540329 + 10.0 * 6.2209553718566895
Epoch 1730, val loss: 1.140899896621704
Epoch 1740, training loss: 62.27269744873047 = 0.04674040898680687 + 10.0 * 6.222595691680908
Epoch 1740, val loss: 1.1451799869537354
Epoch 1750, training loss: 62.25436019897461 = 0.04578845575451851 + 10.0 * 6.2208571434021
Epoch 1750, val loss: 1.149543285369873
Epoch 1760, training loss: 62.233219146728516 = 0.044873107224702835 + 10.0 * 6.21883487701416
Epoch 1760, val loss: 1.1541093587875366
Epoch 1770, training loss: 62.247989654541016 = 0.0439915768802166 + 10.0 * 6.220399856567383
Epoch 1770, val loss: 1.1585912704467773
Epoch 1780, training loss: 62.26176071166992 = 0.04311210662126541 + 10.0 * 6.221864700317383
Epoch 1780, val loss: 1.162830114364624
Epoch 1790, training loss: 62.22989273071289 = 0.04226147010922432 + 10.0 * 6.2187628746032715
Epoch 1790, val loss: 1.167316198348999
Epoch 1800, training loss: 62.23128128051758 = 0.0414462648332119 + 10.0 * 6.2189836502075195
Epoch 1800, val loss: 1.1717307567596436
Epoch 1810, training loss: 62.24535369873047 = 0.040645673871040344 + 10.0 * 6.220470905303955
Epoch 1810, val loss: 1.175877571105957
Epoch 1820, training loss: 62.22566223144531 = 0.039859093725681305 + 10.0 * 6.21858024597168
Epoch 1820, val loss: 1.1804378032684326
Epoch 1830, training loss: 62.21519470214844 = 0.03910406678915024 + 10.0 * 6.21760892868042
Epoch 1830, val loss: 1.1846184730529785
Epoch 1840, training loss: 62.24182891845703 = 0.0383680984377861 + 10.0 * 6.220345973968506
Epoch 1840, val loss: 1.1887152194976807
Epoch 1850, training loss: 62.228389739990234 = 0.03764748573303223 + 10.0 * 6.219074249267578
Epoch 1850, val loss: 1.1935091018676758
Epoch 1860, training loss: 62.21622085571289 = 0.03694514185190201 + 10.0 * 6.2179274559021
Epoch 1860, val loss: 1.1975007057189941
Epoch 1870, training loss: 62.222679138183594 = 0.03626082465052605 + 10.0 * 6.218641757965088
Epoch 1870, val loss: 1.2018985748291016
Epoch 1880, training loss: 62.199462890625 = 0.035591576248407364 + 10.0 * 6.2163872718811035
Epoch 1880, val loss: 1.206043004989624
Epoch 1890, training loss: 62.22867965698242 = 0.03494321182370186 + 10.0 * 6.21937370300293
Epoch 1890, val loss: 1.2102646827697754
Epoch 1900, training loss: 62.193992614746094 = 0.034303970634937286 + 10.0 * 6.215968608856201
Epoch 1900, val loss: 1.2143583297729492
Epoch 1910, training loss: 62.19279479980469 = 0.03368547186255455 + 10.0 * 6.215910911560059
Epoch 1910, val loss: 1.2185684442520142
Epoch 1920, training loss: 62.20952224731445 = 0.03308850899338722 + 10.0 * 6.2176432609558105
Epoch 1920, val loss: 1.2228556871414185
Epoch 1930, training loss: 62.19941711425781 = 0.03249889984726906 + 10.0 * 6.216691970825195
Epoch 1930, val loss: 1.2267881631851196
Epoch 1940, training loss: 62.208763122558594 = 0.031924039125442505 + 10.0 * 6.217683792114258
Epoch 1940, val loss: 1.2307881116867065
Epoch 1950, training loss: 62.18817138671875 = 0.031356193125247955 + 10.0 * 6.215681552886963
Epoch 1950, val loss: 1.2348116636276245
Epoch 1960, training loss: 62.194759368896484 = 0.03081297129392624 + 10.0 * 6.216394901275635
Epoch 1960, val loss: 1.2387343645095825
Epoch 1970, training loss: 62.18281936645508 = 0.03028048202395439 + 10.0 * 6.215253829956055
Epoch 1970, val loss: 1.24285888671875
Epoch 1980, training loss: 62.18844985961914 = 0.029766272753477097 + 10.0 * 6.2158684730529785
Epoch 1980, val loss: 1.2468208074569702
Epoch 1990, training loss: 62.17877960205078 = 0.029257263988256454 + 10.0 * 6.214951992034912
Epoch 1990, val loss: 1.250794529914856
Epoch 2000, training loss: 62.2065544128418 = 0.0287624578922987 + 10.0 * 6.217779159545898
Epoch 2000, val loss: 1.2547708749771118
Epoch 2010, training loss: 62.189762115478516 = 0.028275275602936745 + 10.0 * 6.216148853302002
Epoch 2010, val loss: 1.258379340171814
Epoch 2020, training loss: 62.169525146484375 = 0.027799934148788452 + 10.0 * 6.21417236328125
Epoch 2020, val loss: 1.2622870206832886
Epoch 2030, training loss: 62.16263961791992 = 0.027343783527612686 + 10.0 * 6.213529586791992
Epoch 2030, val loss: 1.2659475803375244
Epoch 2040, training loss: 62.16271209716797 = 0.02689710259437561 + 10.0 * 6.213581562042236
Epoch 2040, val loss: 1.269729733467102
Epoch 2050, training loss: 62.18004608154297 = 0.02645987644791603 + 10.0 * 6.215358734130859
Epoch 2050, val loss: 1.2733731269836426
Epoch 2060, training loss: 62.168182373046875 = 0.026034671813249588 + 10.0 * 6.21421480178833
Epoch 2060, val loss: 1.277276635169983
Epoch 2070, training loss: 62.14774703979492 = 0.02561502531170845 + 10.0 * 6.212213039398193
Epoch 2070, val loss: 1.2810521125793457
Epoch 2080, training loss: 62.15671157836914 = 0.025211626663804054 + 10.0 * 6.2131500244140625
Epoch 2080, val loss: 1.2845181226730347
Epoch 2090, training loss: 62.18913269042969 = 0.024817604571580887 + 10.0 * 6.216431617736816
Epoch 2090, val loss: 1.2882472276687622
Epoch 2100, training loss: 62.17579650878906 = 0.024417994543910027 + 10.0 * 6.215137958526611
Epoch 2100, val loss: 1.2922472953796387
Epoch 2110, training loss: 62.14736557006836 = 0.024030501022934914 + 10.0 * 6.212333679199219
Epoch 2110, val loss: 1.2954133749008179
Epoch 2120, training loss: 62.136390686035156 = 0.023661108687520027 + 10.0 * 6.211272716522217
Epoch 2120, val loss: 1.2992841005325317
Epoch 2130, training loss: 62.14081573486328 = 0.0233016200363636 + 10.0 * 6.211751461029053
Epoch 2130, val loss: 1.3026169538497925
Epoch 2140, training loss: 62.15810012817383 = 0.022947264835238457 + 10.0 * 6.213515281677246
Epoch 2140, val loss: 1.3059982061386108
Epoch 2150, training loss: 62.1440544128418 = 0.022603463381528854 + 10.0 * 6.2121453285217285
Epoch 2150, val loss: 1.3099740743637085
Epoch 2160, training loss: 62.15519332885742 = 0.022262487560510635 + 10.0 * 6.213293075561523
Epoch 2160, val loss: 1.3131498098373413
Epoch 2170, training loss: 62.138771057128906 = 0.02192215621471405 + 10.0 * 6.211684703826904
Epoch 2170, val loss: 1.3166066408157349
Epoch 2180, training loss: 62.12795639038086 = 0.021599706262350082 + 10.0 * 6.210635662078857
Epoch 2180, val loss: 1.3200567960739136
Epoch 2190, training loss: 62.13054275512695 = 0.021280832588672638 + 10.0 * 6.210926055908203
Epoch 2190, val loss: 1.3234163522720337
Epoch 2200, training loss: 62.13348388671875 = 0.020972605794668198 + 10.0 * 6.211251258850098
Epoch 2200, val loss: 1.3268498182296753
Epoch 2210, training loss: 62.14101028442383 = 0.02066917158663273 + 10.0 * 6.212034225463867
Epoch 2210, val loss: 1.330056071281433
Epoch 2220, training loss: 62.15385437011719 = 0.02036445029079914 + 10.0 * 6.213349342346191
Epoch 2220, val loss: 1.3333395719528198
Epoch 2230, training loss: 62.143924713134766 = 0.020069310441613197 + 10.0 * 6.212385654449463
Epoch 2230, val loss: 1.336579442024231
Epoch 2240, training loss: 62.114830017089844 = 0.019778665155172348 + 10.0 * 6.209505081176758
Epoch 2240, val loss: 1.3400540351867676
Epoch 2250, training loss: 62.109039306640625 = 0.019504904747009277 + 10.0 * 6.208953380584717
Epoch 2250, val loss: 1.343384861946106
Epoch 2260, training loss: 62.10930633544922 = 0.019233012571930885 + 10.0 * 6.209007263183594
Epoch 2260, val loss: 1.3466674089431763
Epoch 2270, training loss: 62.156402587890625 = 0.01897185668349266 + 10.0 * 6.213743209838867
Epoch 2270, val loss: 1.3498154878616333
Epoch 2280, training loss: 62.136566162109375 = 0.018691837787628174 + 10.0 * 6.211787223815918
Epoch 2280, val loss: 1.3529045581817627
Epoch 2290, training loss: 62.1112060546875 = 0.01843174733221531 + 10.0 * 6.209277153015137
Epoch 2290, val loss: 1.3561217784881592
Epoch 2300, training loss: 62.100013732910156 = 0.018177345395088196 + 10.0 * 6.208183765411377
Epoch 2300, val loss: 1.3593648672103882
Epoch 2310, training loss: 62.09364318847656 = 0.017935575917363167 + 10.0 * 6.207570552825928
Epoch 2310, val loss: 1.3624426126480103
Epoch 2320, training loss: 62.12150192260742 = 0.01769855245947838 + 10.0 * 6.2103800773620605
Epoch 2320, val loss: 1.365810751914978
Epoch 2330, training loss: 62.10062789916992 = 0.017455363646149635 + 10.0 * 6.208317279815674
Epoch 2330, val loss: 1.368424892425537
Epoch 2340, training loss: 62.121543884277344 = 0.017218491062521935 + 10.0 * 6.210432529449463
Epoch 2340, val loss: 1.3717496395111084
Epoch 2350, training loss: 62.09629821777344 = 0.016989558935165405 + 10.0 * 6.207930564880371
Epoch 2350, val loss: 1.3744693994522095
Epoch 2360, training loss: 62.08837127685547 = 0.016763048246502876 + 10.0 * 6.207160949707031
Epoch 2360, val loss: 1.3775849342346191
Epoch 2370, training loss: 62.08519744873047 = 0.01654592528939247 + 10.0 * 6.206865310668945
Epoch 2370, val loss: 1.3806307315826416
Epoch 2380, training loss: 62.1160888671875 = 0.016336405649781227 + 10.0 * 6.209975242614746
Epoch 2380, val loss: 1.383571743965149
Epoch 2390, training loss: 62.09377670288086 = 0.01612084172666073 + 10.0 * 6.207765579223633
Epoch 2390, val loss: 1.3862138986587524
Epoch 2400, training loss: 62.09365463256836 = 0.015911968424916267 + 10.0 * 6.2077741622924805
Epoch 2400, val loss: 1.3894786834716797
Epoch 2410, training loss: 62.09159851074219 = 0.015709927305579185 + 10.0 * 6.207589149475098
Epoch 2410, val loss: 1.3922739028930664
Epoch 2420, training loss: 62.11406707763672 = 0.01550927571952343 + 10.0 * 6.209855556488037
Epoch 2420, val loss: 1.3949081897735596
Epoch 2430, training loss: 62.092369079589844 = 0.015310032293200493 + 10.0 * 6.207705974578857
Epoch 2430, val loss: 1.39751398563385
Epoch 2440, training loss: 62.07797622680664 = 0.015119852498173714 + 10.0 * 6.20628547668457
Epoch 2440, val loss: 1.4006925821304321
Epoch 2450, training loss: 62.07788848876953 = 0.014932059682905674 + 10.0 * 6.206295967102051
Epoch 2450, val loss: 1.4034252166748047
Epoch 2460, training loss: 62.09605407714844 = 0.014750789850950241 + 10.0 * 6.208130359649658
Epoch 2460, val loss: 1.4059216976165771
Epoch 2470, training loss: 62.073768615722656 = 0.01456462126225233 + 10.0 * 6.205920219421387
Epoch 2470, val loss: 1.40894615650177
Epoch 2480, training loss: 62.08698272705078 = 0.01438953634351492 + 10.0 * 6.207259178161621
Epoch 2480, val loss: 1.4117549657821655
Epoch 2490, training loss: 62.11835479736328 = 0.014215191826224327 + 10.0 * 6.210413932800293
Epoch 2490, val loss: 1.4142361879348755
Epoch 2500, training loss: 62.09526443481445 = 0.01403552945703268 + 10.0 * 6.208123207092285
Epoch 2500, val loss: 1.416986346244812
Epoch 2510, training loss: 62.06642150878906 = 0.013866931200027466 + 10.0 * 6.205255508422852
Epoch 2510, val loss: 1.4198253154754639
Epoch 2520, training loss: 62.05876159667969 = 0.013704750686883926 + 10.0 * 6.204505443572998
Epoch 2520, val loss: 1.4226347208023071
Epoch 2530, training loss: 62.059322357177734 = 0.013547089882194996 + 10.0 * 6.204577445983887
Epoch 2530, val loss: 1.4252846240997314
Epoch 2540, training loss: 62.10075378417969 = 0.013393902219831944 + 10.0 * 6.208735942840576
Epoch 2540, val loss: 1.4278932809829712
Epoch 2550, training loss: 62.0690803527832 = 0.013231105171144009 + 10.0 * 6.20558500289917
Epoch 2550, val loss: 1.430509090423584
Epoch 2560, training loss: 62.066585540771484 = 0.013072805479168892 + 10.0 * 6.20535135269165
Epoch 2560, val loss: 1.4330674409866333
Epoch 2570, training loss: 62.07754898071289 = 0.012922132387757301 + 10.0 * 6.206462860107422
Epoch 2570, val loss: 1.435747504234314
Epoch 2580, training loss: 62.05308151245117 = 0.012771296314895153 + 10.0 * 6.204030990600586
Epoch 2580, val loss: 1.437920093536377
Epoch 2590, training loss: 62.06464767456055 = 0.012628430500626564 + 10.0 * 6.205202102661133
Epoch 2590, val loss: 1.4405752420425415
Epoch 2600, training loss: 62.06125259399414 = 0.012483933940529823 + 10.0 * 6.204876899719238
Epoch 2600, val loss: 1.443101406097412
Epoch 2610, training loss: 62.06006622314453 = 0.012346076779067516 + 10.0 * 6.204771995544434
Epoch 2610, val loss: 1.4455530643463135
Epoch 2620, training loss: 62.070613861083984 = 0.012207720428705215 + 10.0 * 6.205840587615967
Epoch 2620, val loss: 1.4481979608535767
Epoch 2630, training loss: 62.05120086669922 = 0.012068288400769234 + 10.0 * 6.20391321182251
Epoch 2630, val loss: 1.4505051374435425
Epoch 2640, training loss: 62.062862396240234 = 0.011935846880078316 + 10.0 * 6.205092430114746
Epoch 2640, val loss: 1.4532188177108765
Epoch 2650, training loss: 62.076297760009766 = 0.011805057525634766 + 10.0 * 6.206449031829834
Epoch 2650, val loss: 1.4556227922439575
Epoch 2660, training loss: 62.03937530517578 = 0.011671236716210842 + 10.0 * 6.202770233154297
Epoch 2660, val loss: 1.457659363746643
Epoch 2670, training loss: 62.03877258300781 = 0.011545782908797264 + 10.0 * 6.202722549438477
Epoch 2670, val loss: 1.4602938890457153
Epoch 2680, training loss: 62.069129943847656 = 0.011423658579587936 + 10.0 * 6.205770492553711
Epoch 2680, val loss: 1.4625862836837769
Epoch 2690, training loss: 62.05701446533203 = 0.011299388483166695 + 10.0 * 6.204571723937988
Epoch 2690, val loss: 1.46454918384552
Epoch 2700, training loss: 62.058902740478516 = 0.01117401011288166 + 10.0 * 6.20477294921875
Epoch 2700, val loss: 1.467073917388916
Epoch 2710, training loss: 62.04126739501953 = 0.011055036447942257 + 10.0 * 6.203021049499512
Epoch 2710, val loss: 1.4697126150131226
Epoch 2720, training loss: 62.04606628417969 = 0.010938704013824463 + 10.0 * 6.203512668609619
Epoch 2720, val loss: 1.4720696210861206
Epoch 2730, training loss: 62.036476135253906 = 0.010824345052242279 + 10.0 * 6.2025651931762695
Epoch 2730, val loss: 1.4742431640625
Epoch 2740, training loss: 62.07150650024414 = 0.010714845731854439 + 10.0 * 6.206079006195068
Epoch 2740, val loss: 1.4766125679016113
Epoch 2750, training loss: 62.0362663269043 = 0.010599924251437187 + 10.0 * 6.202566623687744
Epoch 2750, val loss: 1.4786036014556885
Epoch 2760, training loss: 62.0265007019043 = 0.010487367399036884 + 10.0 * 6.201601505279541
Epoch 2760, val loss: 1.480783224105835
Epoch 2770, training loss: 62.041229248046875 = 0.010382967069745064 + 10.0 * 6.203084468841553
Epoch 2770, val loss: 1.4830771684646606
Epoch 2780, training loss: 62.034515380859375 = 0.010276412591338158 + 10.0 * 6.202424049377441
Epoch 2780, val loss: 1.4852474927902222
Epoch 2790, training loss: 62.0223388671875 = 0.010170847177505493 + 10.0 * 6.201216697692871
Epoch 2790, val loss: 1.487725853919983
Epoch 2800, training loss: 62.04652786254883 = 0.010071482509374619 + 10.0 * 6.203645706176758
Epoch 2800, val loss: 1.4898921251296997
Epoch 2810, training loss: 62.05189895629883 = 0.009966426528990269 + 10.0 * 6.204193115234375
Epoch 2810, val loss: 1.4915434122085571
Epoch 2820, training loss: 62.0338134765625 = 0.009864321909844875 + 10.0 * 6.202394962310791
Epoch 2820, val loss: 1.4934884309768677
Epoch 2830, training loss: 62.02061462402344 = 0.009765139780938625 + 10.0 * 6.201085090637207
Epoch 2830, val loss: 1.4961044788360596
Epoch 2840, training loss: 62.034393310546875 = 0.009670473635196686 + 10.0 * 6.20247220993042
Epoch 2840, val loss: 1.4980039596557617
Epoch 2850, training loss: 62.04925537109375 = 0.009574219584465027 + 10.0 * 6.203968048095703
Epoch 2850, val loss: 1.4999370574951172
Epoch 2860, training loss: 62.04724884033203 = 0.009481063112616539 + 10.0 * 6.203776836395264
Epoch 2860, val loss: 1.5020618438720703
Epoch 2870, training loss: 62.025726318359375 = 0.009386809542775154 + 10.0 * 6.201633930206299
Epoch 2870, val loss: 1.5044020414352417
Epoch 2880, training loss: 62.01817321777344 = 0.009297851473093033 + 10.0 * 6.200887680053711
Epoch 2880, val loss: 1.5061980485916138
Epoch 2890, training loss: 62.01081466674805 = 0.009208528324961662 + 10.0 * 6.200160503387451
Epoch 2890, val loss: 1.5083202123641968
Epoch 2900, training loss: 62.0255126953125 = 0.009121975861489773 + 10.0 * 6.201639175415039
Epoch 2900, val loss: 1.5101574659347534
Epoch 2910, training loss: 62.029327392578125 = 0.009033935144543648 + 10.0 * 6.202029228210449
Epoch 2910, val loss: 1.511854648590088
Epoch 2920, training loss: 62.05883026123047 = 0.008946629241108894 + 10.0 * 6.204988479614258
Epoch 2920, val loss: 1.5137475728988647
Epoch 2930, training loss: 62.038639068603516 = 0.008861052803695202 + 10.0 * 6.202977657318115
Epoch 2930, val loss: 1.515924334526062
Epoch 2940, training loss: 62.00655746459961 = 0.00877615436911583 + 10.0 * 6.199778079986572
Epoch 2940, val loss: 1.5178419351577759
Epoch 2950, training loss: 62.00050354003906 = 0.008696467615664005 + 10.0 * 6.199180603027344
Epoch 2950, val loss: 1.519959568977356
Epoch 2960, training loss: 62.03059387207031 = 0.008619144558906555 + 10.0 * 6.202197551727295
Epoch 2960, val loss: 1.521981954574585
Epoch 2970, training loss: 62.01029968261719 = 0.008535446599125862 + 10.0 * 6.200176239013672
Epoch 2970, val loss: 1.5233187675476074
Epoch 2980, training loss: 62.012672424316406 = 0.008455178700387478 + 10.0 * 6.2004218101501465
Epoch 2980, val loss: 1.5254898071289062
Epoch 2990, training loss: 62.00327682495117 = 0.008379578590393066 + 10.0 * 6.199489593505859
Epoch 2990, val loss: 1.5273817777633667
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 87.93153381347656 = 1.9631385803222656 + 10.0 * 8.596839904785156
Epoch 0, val loss: 1.9652434587478638
Epoch 10, training loss: 87.913818359375 = 1.9525866508483887 + 10.0 * 8.596123695373535
Epoch 10, val loss: 1.9545152187347412
Epoch 20, training loss: 87.84553527832031 = 1.9396158456802368 + 10.0 * 8.590592384338379
Epoch 20, val loss: 1.9412972927093506
Epoch 30, training loss: 87.43267822265625 = 1.9231128692626953 + 10.0 * 8.550956726074219
Epoch 30, val loss: 1.9245965480804443
Epoch 40, training loss: 84.68999481201172 = 1.9043115377426147 + 10.0 * 8.278568267822266
Epoch 40, val loss: 1.906030297279358
Epoch 50, training loss: 77.17064666748047 = 1.8842315673828125 + 10.0 * 7.528641700744629
Epoch 50, val loss: 1.88662850856781
Epoch 60, training loss: 74.81449890136719 = 1.8699766397476196 + 10.0 * 7.294451713562012
Epoch 60, val loss: 1.8735733032226562
Epoch 70, training loss: 72.55384826660156 = 1.8588181734085083 + 10.0 * 7.069503307342529
Epoch 70, val loss: 1.8629728555679321
Epoch 80, training loss: 71.5843505859375 = 1.8470478057861328 + 10.0 * 6.973730564117432
Epoch 80, val loss: 1.8516836166381836
Epoch 90, training loss: 70.96532440185547 = 1.8356822729110718 + 10.0 * 6.9129638671875
Epoch 90, val loss: 1.840746521949768
Epoch 100, training loss: 70.29017639160156 = 1.8248838186264038 + 10.0 * 6.846529483795166
Epoch 100, val loss: 1.8304444551467896
Epoch 110, training loss: 69.53001403808594 = 1.8168383836746216 + 10.0 * 6.771317005157471
Epoch 110, val loss: 1.8223973512649536
Epoch 120, training loss: 68.80752563476562 = 1.8104718923568726 + 10.0 * 6.699705600738525
Epoch 120, val loss: 1.8153249025344849
Epoch 130, training loss: 68.26954650878906 = 1.8044090270996094 + 10.0 * 6.646514415740967
Epoch 130, val loss: 1.8083544969558716
Epoch 140, training loss: 67.81666564941406 = 1.7980759143829346 + 10.0 * 6.6018595695495605
Epoch 140, val loss: 1.8011624813079834
Epoch 150, training loss: 67.4720230102539 = 1.791395664215088 + 10.0 * 6.5680623054504395
Epoch 150, val loss: 1.7938661575317383
Epoch 160, training loss: 67.21115112304688 = 1.784237265586853 + 10.0 * 6.542690753936768
Epoch 160, val loss: 1.7863245010375977
Epoch 170, training loss: 66.98543548583984 = 1.7766594886779785 + 10.0 * 6.520877838134766
Epoch 170, val loss: 1.7784733772277832
Epoch 180, training loss: 66.78156280517578 = 1.7685861587524414 + 10.0 * 6.501297950744629
Epoch 180, val loss: 1.7703754901885986
Epoch 190, training loss: 66.59911346435547 = 1.7600079774856567 + 10.0 * 6.48391056060791
Epoch 190, val loss: 1.7618876695632935
Epoch 200, training loss: 66.46046447753906 = 1.7507246732711792 + 10.0 * 6.470973968505859
Epoch 200, val loss: 1.7528810501098633
Epoch 210, training loss: 66.30573272705078 = 1.7405258417129517 + 10.0 * 6.4565205574035645
Epoch 210, val loss: 1.7432743310928345
Epoch 220, training loss: 66.1748275756836 = 1.7295550107955933 + 10.0 * 6.444527626037598
Epoch 220, val loss: 1.732962965965271
Epoch 230, training loss: 66.07556915283203 = 1.7176331281661987 + 10.0 * 6.435793399810791
Epoch 230, val loss: 1.7219254970550537
Epoch 240, training loss: 65.9472427368164 = 1.7046624422073364 + 10.0 * 6.424257755279541
Epoch 240, val loss: 1.7100377082824707
Epoch 250, training loss: 65.83194732666016 = 1.6906721591949463 + 10.0 * 6.414127349853516
Epoch 250, val loss: 1.6972767114639282
Epoch 260, training loss: 65.7242202758789 = 1.675536036491394 + 10.0 * 6.4048686027526855
Epoch 260, val loss: 1.6835933923721313
Epoch 270, training loss: 65.66316223144531 = 1.6590924263000488 + 10.0 * 6.400406360626221
Epoch 270, val loss: 1.668825387954712
Epoch 280, training loss: 65.54501342773438 = 1.641550064086914 + 10.0 * 6.390346527099609
Epoch 280, val loss: 1.6531003713607788
Epoch 290, training loss: 65.47189331054688 = 1.6227868795394897 + 10.0 * 6.384911060333252
Epoch 290, val loss: 1.6364737749099731
Epoch 300, training loss: 65.38178253173828 = 1.6027895212173462 + 10.0 * 6.377899169921875
Epoch 300, val loss: 1.6189138889312744
Epoch 310, training loss: 65.30929565429688 = 1.5815787315368652 + 10.0 * 6.372771263122559
Epoch 310, val loss: 1.6005172729492188
Epoch 320, training loss: 65.22769927978516 = 1.5591145753860474 + 10.0 * 6.36685848236084
Epoch 320, val loss: 1.5812567472457886
Epoch 330, training loss: 65.21391296386719 = 1.5355879068374634 + 10.0 * 6.367832183837891
Epoch 330, val loss: 1.5614851713180542
Epoch 340, training loss: 65.09173583984375 = 1.5110406875610352 + 10.0 * 6.35806941986084
Epoch 340, val loss: 1.5410091876983643
Epoch 350, training loss: 65.01351165771484 = 1.4856325387954712 + 10.0 * 6.352787971496582
Epoch 350, val loss: 1.5201916694641113
Epoch 360, training loss: 64.94303894042969 = 1.4594154357910156 + 10.0 * 6.348362922668457
Epoch 360, val loss: 1.499037742614746
Epoch 370, training loss: 64.92842864990234 = 1.4325045347213745 + 10.0 * 6.349592685699463
Epoch 370, val loss: 1.4775983095169067
Epoch 380, training loss: 64.83644104003906 = 1.4048607349395752 + 10.0 * 6.34315824508667
Epoch 380, val loss: 1.4560682773590088
Epoch 390, training loss: 64.76720428466797 = 1.3768019676208496 + 10.0 * 6.339040279388428
Epoch 390, val loss: 1.4345194101333618
Epoch 400, training loss: 64.70561218261719 = 1.3483165502548218 + 10.0 * 6.335729598999023
Epoch 400, val loss: 1.4130618572235107
Epoch 410, training loss: 64.64578247070312 = 1.3194788694381714 + 10.0 * 6.332630634307861
Epoch 410, val loss: 1.391726016998291
Epoch 420, training loss: 64.57907104492188 = 1.2905181646347046 + 10.0 * 6.328855514526367
Epoch 420, val loss: 1.3706905841827393
Epoch 430, training loss: 64.58206176757812 = 1.2614703178405762 + 10.0 * 6.332059383392334
Epoch 430, val loss: 1.3500818014144897
Epoch 440, training loss: 64.47832489013672 = 1.2320703268051147 + 10.0 * 6.324625492095947
Epoch 440, val loss: 1.329609990119934
Epoch 450, training loss: 64.4154281616211 = 1.2030572891235352 + 10.0 * 6.321237087249756
Epoch 450, val loss: 1.309697151184082
Epoch 460, training loss: 64.36369323730469 = 1.1742230653762817 + 10.0 * 6.318946838378906
Epoch 460, val loss: 1.29032564163208
Epoch 470, training loss: 64.36164855957031 = 1.1455857753753662 + 10.0 * 6.321606159210205
Epoch 470, val loss: 1.271414041519165
Epoch 480, training loss: 64.28018951416016 = 1.1172388792037964 + 10.0 * 6.316295146942139
Epoch 480, val loss: 1.2530661821365356
Epoch 490, training loss: 64.22029113769531 = 1.0893927812576294 + 10.0 * 6.313089847564697
Epoch 490, val loss: 1.2353463172912598
Epoch 500, training loss: 64.19132995605469 = 1.0619622468948364 + 10.0 * 6.312936782836914
Epoch 500, val loss: 1.218324065208435
Epoch 510, training loss: 64.1312484741211 = 1.0350373983383179 + 10.0 * 6.309621334075928
Epoch 510, val loss: 1.2019939422607422
Epoch 520, training loss: 64.0831527709961 = 1.008730411529541 + 10.0 * 6.307442665100098
Epoch 520, val loss: 1.1864882707595825
Epoch 530, training loss: 64.03573608398438 = 0.9830356240272522 + 10.0 * 6.305270195007324
Epoch 530, val loss: 1.1716667413711548
Epoch 540, training loss: 64.01874542236328 = 0.9579353928565979 + 10.0 * 6.3060808181762695
Epoch 540, val loss: 1.1575169563293457
Epoch 550, training loss: 63.952903747558594 = 0.9333555698394775 + 10.0 * 6.301954746246338
Epoch 550, val loss: 1.1441917419433594
Epoch 560, training loss: 63.91098403930664 = 0.9095448851585388 + 10.0 * 6.300143718719482
Epoch 560, val loss: 1.1315969228744507
Epoch 570, training loss: 63.98831558227539 = 0.8863207697868347 + 10.0 * 6.31019926071167
Epoch 570, val loss: 1.1198357343673706
Epoch 580, training loss: 63.849308013916016 = 0.8636268973350525 + 10.0 * 6.298567771911621
Epoch 580, val loss: 1.1085723638534546
Epoch 590, training loss: 63.793697357177734 = 0.8416100144386292 + 10.0 * 6.295208930969238
Epoch 590, val loss: 1.098195195198059
Epoch 600, training loss: 63.75179672241211 = 0.8202860355377197 + 10.0 * 6.293150901794434
Epoch 600, val loss: 1.0885050296783447
Epoch 610, training loss: 63.719749450683594 = 0.7994706630706787 + 10.0 * 6.292027950286865
Epoch 610, val loss: 1.0794862508773804
Epoch 620, training loss: 63.71877670288086 = 0.7789961099624634 + 10.0 * 6.293978214263916
Epoch 620, val loss: 1.0707976818084717
Epoch 630, training loss: 63.67367172241211 = 0.7590286135673523 + 10.0 * 6.291464328765869
Epoch 630, val loss: 1.0628374814987183
Epoch 640, training loss: 63.622074127197266 = 0.7395541071891785 + 10.0 * 6.288251876831055
Epoch 640, val loss: 1.0554978847503662
Epoch 650, training loss: 63.67417907714844 = 0.7204557061195374 + 10.0 * 6.295372486114502
Epoch 650, val loss: 1.0485588312149048
Epoch 660, training loss: 63.56374740600586 = 0.7016701698303223 + 10.0 * 6.286207675933838
Epoch 660, val loss: 1.0420905351638794
Epoch 670, training loss: 63.52474594116211 = 0.6833301186561584 + 10.0 * 6.284141540527344
Epoch 670, val loss: 1.0360958576202393
Epoch 680, training loss: 63.495277404785156 = 0.6653842926025391 + 10.0 * 6.282989025115967
Epoch 680, val loss: 1.0305508375167847
Epoch 690, training loss: 63.5183219909668 = 0.647747814655304 + 10.0 * 6.287057399749756
Epoch 690, val loss: 1.025445818901062
Epoch 700, training loss: 63.451690673828125 = 0.6302790641784668 + 10.0 * 6.282141208648682
Epoch 700, val loss: 1.0206960439682007
Epoch 710, training loss: 63.412540435791016 = 0.6131975650787354 + 10.0 * 6.279934406280518
Epoch 710, val loss: 1.0161927938461304
Epoch 720, training loss: 63.426246643066406 = 0.5964654684066772 + 10.0 * 6.282978057861328
Epoch 720, val loss: 1.0121127367019653
Epoch 730, training loss: 63.351497650146484 = 0.5799005627632141 + 10.0 * 6.277159690856934
Epoch 730, val loss: 1.0083415508270264
Epoch 740, training loss: 63.32091522216797 = 0.5637664794921875 + 10.0 * 6.275714874267578
Epoch 740, val loss: 1.0048997402191162
Epoch 750, training loss: 63.303951263427734 = 0.5479440093040466 + 10.0 * 6.275600910186768
Epoch 750, val loss: 1.0018366575241089
Epoch 760, training loss: 63.2824821472168 = 0.5323008894920349 + 10.0 * 6.275018215179443
Epoch 760, val loss: 0.9987108111381531
Epoch 770, training loss: 63.26217269897461 = 0.5169790387153625 + 10.0 * 6.274519443511963
Epoch 770, val loss: 0.9960642457008362
Epoch 780, training loss: 63.21792221069336 = 0.5018749833106995 + 10.0 * 6.271604537963867
Epoch 780, val loss: 0.9934002757072449
Epoch 790, training loss: 63.21138381958008 = 0.487209290266037 + 10.0 * 6.2724175453186035
Epoch 790, val loss: 0.9912247061729431
Epoch 800, training loss: 63.170005798339844 = 0.4727318286895752 + 10.0 * 6.2697272300720215
Epoch 800, val loss: 0.9889623522758484
Epoch 810, training loss: 63.158687591552734 = 0.4586057960987091 + 10.0 * 6.270008087158203
Epoch 810, val loss: 0.987091064453125
Epoch 820, training loss: 63.119895935058594 = 0.44481849670410156 + 10.0 * 6.267507553100586
Epoch 820, val loss: 0.9854466319084167
Epoch 830, training loss: 63.093162536621094 = 0.4313230812549591 + 10.0 * 6.266183853149414
Epoch 830, val loss: 0.9841042757034302
Epoch 840, training loss: 63.09297180175781 = 0.41811949014663696 + 10.0 * 6.26748514175415
Epoch 840, val loss: 0.9829596877098083
Epoch 850, training loss: 63.147491455078125 = 0.4051784574985504 + 10.0 * 6.274231433868408
Epoch 850, val loss: 0.9818021655082703
Epoch 860, training loss: 63.05109786987305 = 0.39244163036346436 + 10.0 * 6.265865802764893
Epoch 860, val loss: 0.980816662311554
Epoch 870, training loss: 63.010311126708984 = 0.3802138566970825 + 10.0 * 6.263009548187256
Epoch 870, val loss: 0.9803395867347717
Epoch 880, training loss: 62.98662567138672 = 0.3683692514896393 + 10.0 * 6.2618255615234375
Epoch 880, val loss: 0.9801010489463806
Epoch 890, training loss: 62.96871566772461 = 0.3568371534347534 + 10.0 * 6.261187553405762
Epoch 890, val loss: 0.9799967408180237
Epoch 900, training loss: 63.070579528808594 = 0.3455830514431 + 10.0 * 6.2724995613098145
Epoch 900, val loss: 0.979975700378418
Epoch 910, training loss: 62.93865203857422 = 0.33451056480407715 + 10.0 * 6.260414123535156
Epoch 910, val loss: 0.9801271557807922
Epoch 920, training loss: 62.92240905761719 = 0.32384422421455383 + 10.0 * 6.259856224060059
Epoch 920, val loss: 0.9806925654411316
Epoch 930, training loss: 62.91347122192383 = 0.31353500485420227 + 10.0 * 6.259993553161621
Epoch 930, val loss: 0.9813130497932434
Epoch 940, training loss: 62.88557434082031 = 0.3034469783306122 + 10.0 * 6.258212566375732
Epoch 940, val loss: 0.9821553826332092
Epoch 950, training loss: 62.858577728271484 = 0.29366710782051086 + 10.0 * 6.256491184234619
Epoch 950, val loss: 0.9833762645721436
Epoch 960, training loss: 62.85086441040039 = 0.2842447757720947 + 10.0 * 6.256661891937256
Epoch 960, val loss: 0.9848836064338684
Epoch 970, training loss: 62.85887908935547 = 0.27506935596466064 + 10.0 * 6.258380889892578
Epoch 970, val loss: 0.986344039440155
Epoch 980, training loss: 62.82554626464844 = 0.26614996790885925 + 10.0 * 6.255939483642578
Epoch 980, val loss: 0.9880807399749756
Epoch 990, training loss: 62.80313491821289 = 0.25757041573524475 + 10.0 * 6.254556179046631
Epoch 990, val loss: 0.9900338649749756
Epoch 1000, training loss: 62.78648376464844 = 0.24923598766326904 + 10.0 * 6.253724575042725
Epoch 1000, val loss: 0.9921451210975647
Epoch 1010, training loss: 62.80450439453125 = 0.24119502305984497 + 10.0 * 6.256330966949463
Epoch 1010, val loss: 0.9944015145301819
Epoch 1020, training loss: 62.75809860229492 = 0.23333463072776794 + 10.0 * 6.252476692199707
Epoch 1020, val loss: 0.9970490336418152
Epoch 1030, training loss: 62.73777770996094 = 0.22575753927230835 + 10.0 * 6.25120210647583
Epoch 1030, val loss: 0.999710202217102
Epoch 1040, training loss: 62.72780990600586 = 0.21844837069511414 + 10.0 * 6.250936031341553
Epoch 1040, val loss: 1.0026003122329712
Epoch 1050, training loss: 62.79747772216797 = 0.21134459972381592 + 10.0 * 6.258613109588623
Epoch 1050, val loss: 1.0057483911514282
Epoch 1060, training loss: 62.70856857299805 = 0.2044447660446167 + 10.0 * 6.250412464141846
Epoch 1060, val loss: 1.0087264776229858
Epoch 1070, training loss: 62.71131896972656 = 0.1977938562631607 + 10.0 * 6.251352787017822
Epoch 1070, val loss: 1.0119881629943848
Epoch 1080, training loss: 62.679805755615234 = 0.1913701295852661 + 10.0 * 6.248843193054199
Epoch 1080, val loss: 1.0155487060546875
Epoch 1090, training loss: 62.6673583984375 = 0.18518956005573273 + 10.0 * 6.2482171058654785
Epoch 1090, val loss: 1.0192214250564575
Epoch 1100, training loss: 62.67058563232422 = 0.17921124398708344 + 10.0 * 6.2491374015808105
Epoch 1100, val loss: 1.023080825805664
Epoch 1110, training loss: 62.66289520263672 = 0.17342573404312134 + 10.0 * 6.248946666717529
Epoch 1110, val loss: 1.0267366170883179
Epoch 1120, training loss: 62.63709259033203 = 0.16781939566135406 + 10.0 * 6.246927261352539
Epoch 1120, val loss: 1.0307133197784424
Epoch 1130, training loss: 62.640289306640625 = 0.1624079942703247 + 10.0 * 6.247788429260254
Epoch 1130, val loss: 1.0349575281143188
Epoch 1140, training loss: 62.60551452636719 = 0.15718276798725128 + 10.0 * 6.244832992553711
Epoch 1140, val loss: 1.0390695333480835
Epoch 1150, training loss: 62.594608306884766 = 0.1521560549736023 + 10.0 * 6.2442450523376465
Epoch 1150, val loss: 1.0433518886566162
Epoch 1160, training loss: 62.64262390136719 = 0.14729838073253632 + 10.0 * 6.249532699584961
Epoch 1160, val loss: 1.0478386878967285
Epoch 1170, training loss: 62.603904724121094 = 0.14260022342205048 + 10.0 * 6.246130466461182
Epoch 1170, val loss: 1.0522209405899048
Epoch 1180, training loss: 62.57632064819336 = 0.13804662227630615 + 10.0 * 6.2438273429870605
Epoch 1180, val loss: 1.0568569898605347
Epoch 1190, training loss: 62.563018798828125 = 0.133701354265213 + 10.0 * 6.242931842803955
Epoch 1190, val loss: 1.061772108078003
Epoch 1200, training loss: 62.60505294799805 = 0.12948928773403168 + 10.0 * 6.247556209564209
Epoch 1200, val loss: 1.066759467124939
Epoch 1210, training loss: 62.56614685058594 = 0.12539802491664886 + 10.0 * 6.244074821472168
Epoch 1210, val loss: 1.0714243650436401
Epoch 1220, training loss: 62.53707504272461 = 0.12147203087806702 + 10.0 * 6.241560459136963
Epoch 1220, val loss: 1.076550841331482
Epoch 1230, training loss: 62.527992248535156 = 0.11769726872444153 + 10.0 * 6.241029262542725
Epoch 1230, val loss: 1.0817424058914185
Epoch 1240, training loss: 62.57636642456055 = 0.11405997723340988 + 10.0 * 6.246230602264404
Epoch 1240, val loss: 1.086937427520752
Epoch 1250, training loss: 62.538883209228516 = 0.11050575971603394 + 10.0 * 6.242837905883789
Epoch 1250, val loss: 1.092252254486084
Epoch 1260, training loss: 62.505069732666016 = 0.1070987656712532 + 10.0 * 6.239797115325928
Epoch 1260, val loss: 1.0975165367126465
Epoch 1270, training loss: 62.509307861328125 = 0.10384112596511841 + 10.0 * 6.240546703338623
Epoch 1270, val loss: 1.1028449535369873
Epoch 1280, training loss: 62.48123550415039 = 0.10069551318883896 + 10.0 * 6.238053798675537
Epoch 1280, val loss: 1.1081904172897339
Epoch 1290, training loss: 62.47269821166992 = 0.09767427295446396 + 10.0 * 6.237502098083496
Epoch 1290, val loss: 1.1133403778076172
Epoch 1300, training loss: 62.50997543334961 = 0.09477333724498749 + 10.0 * 6.241520404815674
Epoch 1300, val loss: 1.1185601949691772
Epoch 1310, training loss: 62.45906448364258 = 0.09192609041929245 + 10.0 * 6.236713886260986
Epoch 1310, val loss: 1.1241633892059326
Epoch 1320, training loss: 62.44998550415039 = 0.08920379728078842 + 10.0 * 6.236078262329102
Epoch 1320, val loss: 1.1295206546783447
Epoch 1330, training loss: 62.49299621582031 = 0.08658595383167267 + 10.0 * 6.240641117095947
Epoch 1330, val loss: 1.1348299980163574
Epoch 1340, training loss: 62.44603729248047 = 0.08407355099916458 + 10.0 * 6.236196517944336
Epoch 1340, val loss: 1.1402384042739868
Epoch 1350, training loss: 62.43497848510742 = 0.08162277936935425 + 10.0 * 6.235335350036621
Epoch 1350, val loss: 1.1455050706863403
Epoch 1360, training loss: 62.44908905029297 = 0.07929183542728424 + 10.0 * 6.2369794845581055
Epoch 1360, val loss: 1.1512960195541382
Epoch 1370, training loss: 62.42523193359375 = 0.0770321786403656 + 10.0 * 6.2348198890686035
Epoch 1370, val loss: 1.1564252376556396
Epoch 1380, training loss: 62.42768096923828 = 0.07486812025308609 + 10.0 * 6.235281467437744
Epoch 1380, val loss: 1.161880612373352
Epoch 1390, training loss: 62.44575119018555 = 0.07276158779859543 + 10.0 * 6.237298965454102
Epoch 1390, val loss: 1.1672863960266113
Epoch 1400, training loss: 62.41118621826172 = 0.07073992490768433 + 10.0 * 6.234044551849365
Epoch 1400, val loss: 1.1726627349853516
Epoch 1410, training loss: 62.4322624206543 = 0.06878116726875305 + 10.0 * 6.2363481521606445
Epoch 1410, val loss: 1.1780076026916504
Epoch 1420, training loss: 62.39073181152344 = 0.06690063327550888 + 10.0 * 6.232382774353027
Epoch 1420, val loss: 1.1833416223526
Epoch 1430, training loss: 62.38833999633789 = 0.06508985161781311 + 10.0 * 6.232325077056885
Epoch 1430, val loss: 1.188520073890686
Epoch 1440, training loss: 62.379608154296875 = 0.06334661692380905 + 10.0 * 6.231626033782959
Epoch 1440, val loss: 1.1939918994903564
Epoch 1450, training loss: 62.428104400634766 = 0.06166374310851097 + 10.0 * 6.2366437911987305
Epoch 1450, val loss: 1.1990841627120972
Epoch 1460, training loss: 62.41640090942383 = 0.06001032888889313 + 10.0 * 6.2356390953063965
Epoch 1460, val loss: 1.2046362161636353
Epoch 1470, training loss: 62.38181686401367 = 0.058437932282686234 + 10.0 * 6.232337951660156
Epoch 1470, val loss: 1.2096079587936401
Epoch 1480, training loss: 62.36867904663086 = 0.05691138282418251 + 10.0 * 6.231176853179932
Epoch 1480, val loss: 1.2150081396102905
Epoch 1490, training loss: 62.42699432373047 = 0.05545278638601303 + 10.0 * 6.237154006958008
Epoch 1490, val loss: 1.220550537109375
Epoch 1500, training loss: 62.37543869018555 = 0.05401880666613579 + 10.0 * 6.232141971588135
Epoch 1500, val loss: 1.2251815795898438
Epoch 1510, training loss: 62.35708236694336 = 0.05264681205153465 + 10.0 * 6.230443477630615
Epoch 1510, val loss: 1.2303205728530884
Epoch 1520, training loss: 62.339996337890625 = 0.0513327494263649 + 10.0 * 6.228866100311279
Epoch 1520, val loss: 1.235645055770874
Epoch 1530, training loss: 62.33558654785156 = 0.05006333813071251 + 10.0 * 6.228552341461182
Epoch 1530, val loss: 1.240889310836792
Epoch 1540, training loss: 62.45073699951172 = 0.04884492978453636 + 10.0 * 6.240189552307129
Epoch 1540, val loss: 1.2459303140640259
Epoch 1550, training loss: 62.352638244628906 = 0.047626420855522156 + 10.0 * 6.230501174926758
Epoch 1550, val loss: 1.25041925907135
Epoch 1560, training loss: 62.317283630371094 = 0.046465251594781876 + 10.0 * 6.227081775665283
Epoch 1560, val loss: 1.255510926246643
Epoch 1570, training loss: 62.3128547668457 = 0.04535548761487007 + 10.0 * 6.226749897003174
Epoch 1570, val loss: 1.2606791257858276
Epoch 1580, training loss: 62.333072662353516 = 0.044283583760261536 + 10.0 * 6.228878974914551
Epoch 1580, val loss: 1.2657955884933472
Epoch 1590, training loss: 62.31510543823242 = 0.04323294013738632 + 10.0 * 6.227187156677246
Epoch 1590, val loss: 1.2703287601470947
Epoch 1600, training loss: 62.311161041259766 = 0.04222455993294716 + 10.0 * 6.226893424987793
Epoch 1600, val loss: 1.2749929428100586
Epoch 1610, training loss: 62.39290237426758 = 0.041241418570280075 + 10.0 * 6.235166072845459
Epoch 1610, val loss: 1.2799243927001953
Epoch 1620, training loss: 62.327178955078125 = 0.04029200226068497 + 10.0 * 6.228688716888428
Epoch 1620, val loss: 1.2842330932617188
Epoch 1630, training loss: 62.2978630065918 = 0.03936741501092911 + 10.0 * 6.225849628448486
Epoch 1630, val loss: 1.2890372276306152
Epoch 1640, training loss: 62.291236877441406 = 0.03848288208246231 + 10.0 * 6.22527551651001
Epoch 1640, val loss: 1.2937475442886353
Epoch 1650, training loss: 62.31846618652344 = 0.03762592375278473 + 10.0 * 6.228084087371826
Epoch 1650, val loss: 1.2983027696609497
Epoch 1660, training loss: 62.281986236572266 = 0.0367889329791069 + 10.0 * 6.224519729614258
Epoch 1660, val loss: 1.3031820058822632
Epoch 1670, training loss: 62.2823371887207 = 0.03597276657819748 + 10.0 * 6.224636554718018
Epoch 1670, val loss: 1.3079372644424438
Epoch 1680, training loss: 62.36338806152344 = 0.03518486022949219 + 10.0 * 6.2328200340271
Epoch 1680, val loss: 1.312379002571106
Epoch 1690, training loss: 62.29962921142578 = 0.03440926969051361 + 10.0 * 6.226521968841553
Epoch 1690, val loss: 1.3166459798812866
Epoch 1700, training loss: 62.2749137878418 = 0.03365878015756607 + 10.0 * 6.224125862121582
Epoch 1700, val loss: 1.3216155767440796
Epoch 1710, training loss: 62.26096725463867 = 0.032940421253442764 + 10.0 * 6.222802639007568
Epoch 1710, val loss: 1.3260880708694458
Epoch 1720, training loss: 62.25986099243164 = 0.03224588558077812 + 10.0 * 6.222761631011963
Epoch 1720, val loss: 1.3307396173477173
Epoch 1730, training loss: 62.300506591796875 = 0.031580060720443726 + 10.0 * 6.226892471313477
Epoch 1730, val loss: 1.3352185487747192
Epoch 1740, training loss: 62.29423904418945 = 0.030906373634934425 + 10.0 * 6.226333141326904
Epoch 1740, val loss: 1.3394274711608887
Epoch 1750, training loss: 62.280765533447266 = 0.03026188351213932 + 10.0 * 6.225050449371338
Epoch 1750, val loss: 1.3442466259002686
Epoch 1760, training loss: 62.25634002685547 = 0.029630577191710472 + 10.0 * 6.222671031951904
Epoch 1760, val loss: 1.348334789276123
Epoch 1770, training loss: 62.26230239868164 = 0.029033783823251724 + 10.0 * 6.223326683044434
Epoch 1770, val loss: 1.3530795574188232
Epoch 1780, training loss: 62.25615692138672 = 0.028448717668652534 + 10.0 * 6.222770690917969
Epoch 1780, val loss: 1.3572931289672852
Epoch 1790, training loss: 62.24243927001953 = 0.02787337452173233 + 10.0 * 6.221456527709961
Epoch 1790, val loss: 1.3612065315246582
Epoch 1800, training loss: 62.239505767822266 = 0.027322523295879364 + 10.0 * 6.221218585968018
Epoch 1800, val loss: 1.3656498193740845
Epoch 1810, training loss: 62.24905776977539 = 0.026785004884004593 + 10.0 * 6.222227096557617
Epoch 1810, val loss: 1.3698710203170776
Epoch 1820, training loss: 62.273193359375 = 0.026262611150741577 + 10.0 * 6.2246928215026855
Epoch 1820, val loss: 1.373996376991272
Epoch 1830, training loss: 62.24966812133789 = 0.025746067985892296 + 10.0 * 6.2223920822143555
Epoch 1830, val loss: 1.3779637813568115
Epoch 1840, training loss: 62.22340393066406 = 0.0252446960657835 + 10.0 * 6.219815731048584
Epoch 1840, val loss: 1.382408857345581
Epoch 1850, training loss: 62.213714599609375 = 0.02476583793759346 + 10.0 * 6.218894958496094
Epoch 1850, val loss: 1.3865923881530762
Epoch 1860, training loss: 62.2529411315918 = 0.02430286444723606 + 10.0 * 6.222863674163818
Epoch 1860, val loss: 1.3905906677246094
Epoch 1870, training loss: 62.21614074707031 = 0.02384250983595848 + 10.0 * 6.219229698181152
Epoch 1870, val loss: 1.3945660591125488
Epoch 1880, training loss: 62.20826721191406 = 0.023393681272864342 + 10.0 * 6.21848726272583
Epoch 1880, val loss: 1.398386836051941
Epoch 1890, training loss: 62.21173858642578 = 0.022964609786868095 + 10.0 * 6.21887731552124
Epoch 1890, val loss: 1.4023399353027344
Epoch 1900, training loss: 62.2447624206543 = 0.022548943758010864 + 10.0 * 6.222221374511719
Epoch 1900, val loss: 1.4060837030410767
Epoch 1910, training loss: 62.19941711425781 = 0.022131897509098053 + 10.0 * 6.217728614807129
Epoch 1910, val loss: 1.410455584526062
Epoch 1920, training loss: 62.21485137939453 = 0.021738294512033463 + 10.0 * 6.219311237335205
Epoch 1920, val loss: 1.4143301248550415
Epoch 1930, training loss: 62.21885299682617 = 0.021351853385567665 + 10.0 * 6.21975040435791
Epoch 1930, val loss: 1.4179638624191284
Epoch 1940, training loss: 62.216251373291016 = 0.020967528223991394 + 10.0 * 6.2195281982421875
Epoch 1940, val loss: 1.4217807054519653
Epoch 1950, training loss: 62.2109375 = 0.020597107708454132 + 10.0 * 6.219034194946289
Epoch 1950, val loss: 1.4255579710006714
Epoch 1960, training loss: 62.18952178955078 = 0.020234638825058937 + 10.0 * 6.216928958892822
Epoch 1960, val loss: 1.429042100906372
Epoch 1970, training loss: 62.17898178100586 = 0.019887272268533707 + 10.0 * 6.215909481048584
Epoch 1970, val loss: 1.4329888820648193
Epoch 1980, training loss: 62.17845916748047 = 0.019551143050193787 + 10.0 * 6.215890884399414
Epoch 1980, val loss: 1.436816930770874
Epoch 1990, training loss: 62.19248580932617 = 0.01922147162258625 + 10.0 * 6.2173261642456055
Epoch 1990, val loss: 1.4404975175857544
Epoch 2000, training loss: 62.21610641479492 = 0.01889496110379696 + 10.0 * 6.21972131729126
Epoch 2000, val loss: 1.4439047574996948
Epoch 2010, training loss: 62.175533294677734 = 0.018571268767118454 + 10.0 * 6.215696334838867
Epoch 2010, val loss: 1.4471601247787476
Epoch 2020, training loss: 62.1762809753418 = 0.018260590732097626 + 10.0 * 6.215802192687988
Epoch 2020, val loss: 1.451055645942688
Epoch 2030, training loss: 62.19196319580078 = 0.017966024577617645 + 10.0 * 6.217399597167969
Epoch 2030, val loss: 1.4545879364013672
Epoch 2040, training loss: 62.200016021728516 = 0.017674090340733528 + 10.0 * 6.218234062194824
Epoch 2040, val loss: 1.4580293893814087
Epoch 2050, training loss: 62.181243896484375 = 0.01737910322844982 + 10.0 * 6.216386318206787
Epoch 2050, val loss: 1.4617663621902466
Epoch 2060, training loss: 62.16374969482422 = 0.01710221916437149 + 10.0 * 6.214664936065674
Epoch 2060, val loss: 1.4649211168289185
Epoch 2070, training loss: 62.18403625488281 = 0.01683419570326805 + 10.0 * 6.216720104217529
Epoch 2070, val loss: 1.46848726272583
Epoch 2080, training loss: 62.16757583618164 = 0.01656346209347248 + 10.0 * 6.21510124206543
Epoch 2080, val loss: 1.4717057943344116
Epoch 2090, training loss: 62.170467376708984 = 0.016304943710565567 + 10.0 * 6.215416431427002
Epoch 2090, val loss: 1.4750134944915771
Epoch 2100, training loss: 62.17600631713867 = 0.016045521944761276 + 10.0 * 6.215996265411377
Epoch 2100, val loss: 1.4782254695892334
Epoch 2110, training loss: 62.16039276123047 = 0.01579427160322666 + 10.0 * 6.2144598960876465
Epoch 2110, val loss: 1.481819987297058
Epoch 2120, training loss: 62.154354095458984 = 0.015552844852209091 + 10.0 * 6.2138800621032715
Epoch 2120, val loss: 1.48517906665802
Epoch 2130, training loss: 62.14601135253906 = 0.015317557379603386 + 10.0 * 6.213069438934326
Epoch 2130, val loss: 1.4882798194885254
Epoch 2140, training loss: 62.14602279663086 = 0.015090073458850384 + 10.0 * 6.213093280792236
Epoch 2140, val loss: 1.4914709329605103
Epoch 2150, training loss: 62.195003509521484 = 0.014868674799799919 + 10.0 * 6.218013286590576
Epoch 2150, val loss: 1.4946391582489014
Epoch 2160, training loss: 62.15524673461914 = 0.014643246307969093 + 10.0 * 6.214060306549072
Epoch 2160, val loss: 1.4978790283203125
Epoch 2170, training loss: 62.18222427368164 = 0.014429441653192043 + 10.0 * 6.2167792320251465
Epoch 2170, val loss: 1.5004292726516724
Epoch 2180, training loss: 62.155059814453125 = 0.01420647744089365 + 10.0 * 6.214085578918457
Epoch 2180, val loss: 1.5042117834091187
Epoch 2190, training loss: 62.14485168457031 = 0.013999787159264088 + 10.0 * 6.213085174560547
Epoch 2190, val loss: 1.5068955421447754
Epoch 2200, training loss: 62.12874984741211 = 0.013798432424664497 + 10.0 * 6.211495399475098
Epoch 2200, val loss: 1.5103901624679565
Epoch 2210, training loss: 62.12495422363281 = 0.013601977378129959 + 10.0 * 6.211134910583496
Epoch 2210, val loss: 1.5133798122406006
Epoch 2220, training loss: 62.136592864990234 = 0.013411447405815125 + 10.0 * 6.212317943572998
Epoch 2220, val loss: 1.5162419080734253
Epoch 2230, training loss: 62.16783142089844 = 0.013222756795585155 + 10.0 * 6.215460777282715
Epoch 2230, val loss: 1.5193092823028564
Epoch 2240, training loss: 62.14716720581055 = 0.013036825694143772 + 10.0 * 6.213412761688232
Epoch 2240, val loss: 1.5223090648651123
Epoch 2250, training loss: 62.173336029052734 = 0.012851723469793797 + 10.0 * 6.216048240661621
Epoch 2250, val loss: 1.5255041122436523
Epoch 2260, training loss: 62.16666030883789 = 0.012672950513660908 + 10.0 * 6.215398788452148
Epoch 2260, val loss: 1.5278569459915161
Epoch 2270, training loss: 62.12679672241211 = 0.012494157068431377 + 10.0 * 6.211430549621582
Epoch 2270, val loss: 1.5308711528778076
Epoch 2280, training loss: 62.120243072509766 = 0.012324078008532524 + 10.0 * 6.21079158782959
Epoch 2280, val loss: 1.53352689743042
Epoch 2290, training loss: 62.1094856262207 = 0.012158835306763649 + 10.0 * 6.209733009338379
Epoch 2290, val loss: 1.536720633506775
Epoch 2300, training loss: 62.10670852661133 = 0.011998791247606277 + 10.0 * 6.209471225738525
Epoch 2300, val loss: 1.5394837856292725
Epoch 2310, training loss: 62.12288284301758 = 0.01184326782822609 + 10.0 * 6.211103916168213
Epoch 2310, val loss: 1.5422582626342773
Epoch 2320, training loss: 62.14823913574219 = 0.011686618439853191 + 10.0 * 6.2136549949646
Epoch 2320, val loss: 1.5449186563491821
Epoch 2330, training loss: 62.1438102722168 = 0.011528152972459793 + 10.0 * 6.213228225708008
Epoch 2330, val loss: 1.5474739074707031
Epoch 2340, training loss: 62.11936569213867 = 0.011377381160855293 + 10.0 * 6.210798740386963
Epoch 2340, val loss: 1.5502749681472778
Epoch 2350, training loss: 62.10795593261719 = 0.0112291956320405 + 10.0 * 6.209672451019287
Epoch 2350, val loss: 1.5528372526168823
Epoch 2360, training loss: 62.177818298339844 = 0.011087723076343536 + 10.0 * 6.216672897338867
Epoch 2360, val loss: 1.5555706024169922
Epoch 2370, training loss: 62.13652420043945 = 0.010943284258246422 + 10.0 * 6.212557792663574
Epoch 2370, val loss: 1.5583394765853882
Epoch 2380, training loss: 62.13941192626953 = 0.010801752097904682 + 10.0 * 6.212861061096191
Epoch 2380, val loss: 1.5610078573226929
Epoch 2390, training loss: 62.09896469116211 = 0.010665391571819782 + 10.0 * 6.208829879760742
Epoch 2390, val loss: 1.5627037286758423
Epoch 2400, training loss: 62.09006118774414 = 0.010533894412219524 + 10.0 * 6.207952976226807
Epoch 2400, val loss: 1.5658178329467773
Epoch 2410, training loss: 62.08824920654297 = 0.010405104607343674 + 10.0 * 6.207784175872803
Epoch 2410, val loss: 1.5684573650360107
Epoch 2420, training loss: 62.11217498779297 = 0.010280212387442589 + 10.0 * 6.210189342498779
Epoch 2420, val loss: 1.5710099935531616
Epoch 2430, training loss: 62.11079788208008 = 0.010151395574212074 + 10.0 * 6.210064888000488
Epoch 2430, val loss: 1.5730268955230713
Epoch 2440, training loss: 62.09663391113281 = 0.010027691721916199 + 10.0 * 6.20866060256958
Epoch 2440, val loss: 1.5755996704101562
Epoch 2450, training loss: 62.11429977416992 = 0.009903915226459503 + 10.0 * 6.210439682006836
Epoch 2450, val loss: 1.5781936645507812
Epoch 2460, training loss: 62.093441009521484 = 0.009783705696463585 + 10.0 * 6.208365440368652
Epoch 2460, val loss: 1.5805631875991821
Epoch 2470, training loss: 62.0886344909668 = 0.009667214937508106 + 10.0 * 6.207896709442139
Epoch 2470, val loss: 1.582645058631897
Epoch 2480, training loss: 62.10349655151367 = 0.009554591961205006 + 10.0 * 6.2093939781188965
Epoch 2480, val loss: 1.585174322128296
Epoch 2490, training loss: 62.091548919677734 = 0.009442174807190895 + 10.0 * 6.2082109451293945
Epoch 2490, val loss: 1.5873825550079346
Epoch 2500, training loss: 62.085208892822266 = 0.009330795146524906 + 10.0 * 6.207587718963623
Epoch 2500, val loss: 1.5900367498397827
Epoch 2510, training loss: 62.07160568237305 = 0.009222311899065971 + 10.0 * 6.206238269805908
Epoch 2510, val loss: 1.5923213958740234
Epoch 2520, training loss: 62.10515594482422 = 0.009117974899709225 + 10.0 * 6.209603786468506
Epoch 2520, val loss: 1.5948257446289062
Epoch 2530, training loss: 62.115482330322266 = 0.009012249298393726 + 10.0 * 6.210646629333496
Epoch 2530, val loss: 1.596357822418213
Epoch 2540, training loss: 62.09452438354492 = 0.008909691125154495 + 10.0 * 6.208561420440674
Epoch 2540, val loss: 1.5987122058868408
Epoch 2550, training loss: 62.067230224609375 = 0.00880519486963749 + 10.0 * 6.2058424949646
Epoch 2550, val loss: 1.600827932357788
Epoch 2560, training loss: 62.06513595581055 = 0.008708447217941284 + 10.0 * 6.2056427001953125
Epoch 2560, val loss: 1.603413701057434
Epoch 2570, training loss: 62.08885955810547 = 0.00861272681504488 + 10.0 * 6.208024501800537
Epoch 2570, val loss: 1.6055880784988403
Epoch 2580, training loss: 62.07988357543945 = 0.008517085574567318 + 10.0 * 6.207136631011963
Epoch 2580, val loss: 1.607717752456665
Epoch 2590, training loss: 62.09375762939453 = 0.008424272760748863 + 10.0 * 6.20853328704834
Epoch 2590, val loss: 1.609774112701416
Epoch 2600, training loss: 62.079307556152344 = 0.00833105482161045 + 10.0 * 6.20709753036499
Epoch 2600, val loss: 1.6119734048843384
Epoch 2610, training loss: 62.089134216308594 = 0.00824123714119196 + 10.0 * 6.208089351654053
Epoch 2610, val loss: 1.6144315004348755
Epoch 2620, training loss: 62.07596206665039 = 0.008149385452270508 + 10.0 * 6.206781387329102
Epoch 2620, val loss: 1.616231918334961
Epoch 2630, training loss: 62.07166290283203 = 0.008063139393925667 + 10.0 * 6.20635986328125
Epoch 2630, val loss: 1.6178964376449585
Epoch 2640, training loss: 62.054603576660156 = 0.007978164590895176 + 10.0 * 6.204662799835205
Epoch 2640, val loss: 1.6204673051834106
Epoch 2650, training loss: 62.05347442626953 = 0.007895015180110931 + 10.0 * 6.2045578956604
Epoch 2650, val loss: 1.6222244501113892
Epoch 2660, training loss: 62.08907699584961 = 0.007815822027623653 + 10.0 * 6.208126068115234
Epoch 2660, val loss: 1.6241819858551025
Epoch 2670, training loss: 62.068538665771484 = 0.007732064463198185 + 10.0 * 6.206080436706543
Epoch 2670, val loss: 1.6259269714355469
Epoch 2680, training loss: 62.066707611083984 = 0.0076491739600896835 + 10.0 * 6.205905914306641
Epoch 2680, val loss: 1.6279668807983398
Epoch 2690, training loss: 62.07845687866211 = 0.0075702182948589325 + 10.0 * 6.207088947296143
Epoch 2690, val loss: 1.6300065517425537
Epoch 2700, training loss: 62.06013488769531 = 0.007492464501410723 + 10.0 * 6.205264091491699
Epoch 2700, val loss: 1.6322860717773438
Epoch 2710, training loss: 62.07070541381836 = 0.007417032960802317 + 10.0 * 6.206328868865967
Epoch 2710, val loss: 1.6339635848999023
Epoch 2720, training loss: 62.05356216430664 = 0.007341010961681604 + 10.0 * 6.204622268676758
Epoch 2720, val loss: 1.6358931064605713
Epoch 2730, training loss: 62.03599166870117 = 0.007267320528626442 + 10.0 * 6.202872276306152
Epoch 2730, val loss: 1.637859582901001
Epoch 2740, training loss: 62.039249420166016 = 0.007196372840553522 + 10.0 * 6.203205585479736
Epoch 2740, val loss: 1.6401805877685547
Epoch 2750, training loss: 62.09563446044922 = 0.0071273683570325375 + 10.0 * 6.208850860595703
Epoch 2750, val loss: 1.6422665119171143
Epoch 2760, training loss: 62.059669494628906 = 0.007056148257106543 + 10.0 * 6.20526123046875
Epoch 2760, val loss: 1.6426377296447754
Epoch 2770, training loss: 62.06831741333008 = 0.006985527463257313 + 10.0 * 6.2061333656311035
Epoch 2770, val loss: 1.6451923847198486
Epoch 2780, training loss: 62.05007553100586 = 0.0069161057472229 + 10.0 * 6.204316139221191
Epoch 2780, val loss: 1.6466878652572632
Epoch 2790, training loss: 62.053550720214844 = 0.006848361808806658 + 10.0 * 6.204670429229736
Epoch 2790, val loss: 1.6489167213439941
Epoch 2800, training loss: 62.03352355957031 = 0.006783842574805021 + 10.0 * 6.20267391204834
Epoch 2800, val loss: 1.6504113674163818
Epoch 2810, training loss: 62.036949157714844 = 0.006719932891428471 + 10.0 * 6.2030229568481445
Epoch 2810, val loss: 1.6525232791900635
Epoch 2820, training loss: 62.05954360961914 = 0.006659186910837889 + 10.0 * 6.205288410186768
Epoch 2820, val loss: 1.6543920040130615
Epoch 2830, training loss: 62.059417724609375 = 0.006592786405235529 + 10.0 * 6.205282688140869
Epoch 2830, val loss: 1.6561661958694458
Epoch 2840, training loss: 62.038246154785156 = 0.006531241815537214 + 10.0 * 6.203171730041504
Epoch 2840, val loss: 1.6576658487319946
Epoch 2850, training loss: 62.030029296875 = 0.006469310261309147 + 10.0 * 6.202355861663818
Epoch 2850, val loss: 1.6591135263442993
Epoch 2860, training loss: 62.06768798828125 = 0.006410087458789349 + 10.0 * 6.206128120422363
Epoch 2860, val loss: 1.6609894037246704
Epoch 2870, training loss: 62.02247619628906 = 0.006350202020257711 + 10.0 * 6.20161247253418
Epoch 2870, val loss: 1.6623677015304565
Epoch 2880, training loss: 62.03563690185547 = 0.006292562000453472 + 10.0 * 6.202934265136719
Epoch 2880, val loss: 1.6640679836273193
Epoch 2890, training loss: 62.065773010253906 = 0.006237526889890432 + 10.0 * 6.205953598022461
Epoch 2890, val loss: 1.665605902671814
Epoch 2900, training loss: 62.026710510253906 = 0.00617880467325449 + 10.0 * 6.202053070068359
Epoch 2900, val loss: 1.667138695716858
Epoch 2910, training loss: 62.0296516418457 = 0.00612312788143754 + 10.0 * 6.202353000640869
Epoch 2910, val loss: 1.6686967611312866
Epoch 2920, training loss: 62.06653594970703 = 0.00607034657150507 + 10.0 * 6.2060465812683105
Epoch 2920, val loss: 1.6704550981521606
Epoch 2930, training loss: 62.024078369140625 = 0.00601354381069541 + 10.0 * 6.201806545257568
Epoch 2930, val loss: 1.6720846891403198
Epoch 2940, training loss: 62.007286071777344 = 0.005960660520941019 + 10.0 * 6.200132369995117
Epoch 2940, val loss: 1.6736479997634888
Epoch 2950, training loss: 62.029685974121094 = 0.005909512750804424 + 10.0 * 6.202377796173096
Epoch 2950, val loss: 1.6751691102981567
Epoch 2960, training loss: 62.06074142456055 = 0.005858063697814941 + 10.0 * 6.205488204956055
Epoch 2960, val loss: 1.677033543586731
Epoch 2970, training loss: 62.016178131103516 = 0.005803984589874744 + 10.0 * 6.201037406921387
Epoch 2970, val loss: 1.6779506206512451
Epoch 2980, training loss: 62.004554748535156 = 0.005754157900810242 + 10.0 * 6.199880123138428
Epoch 2980, val loss: 1.6795493364334106
Epoch 2990, training loss: 62.003143310546875 = 0.005706033669412136 + 10.0 * 6.199743747711182
Epoch 2990, val loss: 1.6814355850219727
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8081180811808119
The final CL Acc:0.71235, 0.01666, The final GNN Acc:0.80847, 0.00217
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13140])
remove edge: torch.Size([2, 7940])
updated graph: torch.Size([2, 10524])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.91631317138672 = 1.9481616020202637 + 10.0 * 8.59681510925293
Epoch 0, val loss: 1.9443931579589844
Epoch 10, training loss: 87.896484375 = 1.9375927448272705 + 10.0 * 8.5958890914917
Epoch 10, val loss: 1.9341636896133423
Epoch 20, training loss: 87.80966186523438 = 1.9244121313095093 + 10.0 * 8.58852481842041
Epoch 20, val loss: 1.9208412170410156
Epoch 30, training loss: 87.2955093383789 = 1.9074088335037231 + 10.0 * 8.538809776306152
Epoch 30, val loss: 1.903329610824585
Epoch 40, training loss: 84.43623352050781 = 1.887466549873352 + 10.0 * 8.254877090454102
Epoch 40, val loss: 1.8834177255630493
Epoch 50, training loss: 78.72735595703125 = 1.8651409149169922 + 10.0 * 7.686221122741699
Epoch 50, val loss: 1.8618991374969482
Epoch 60, training loss: 76.162841796875 = 1.8512405157089233 + 10.0 * 7.431159973144531
Epoch 60, val loss: 1.8487601280212402
Epoch 70, training loss: 73.76688385009766 = 1.8430578708648682 + 10.0 * 7.1923828125
Epoch 70, val loss: 1.8400087356567383
Epoch 80, training loss: 71.49028778076172 = 1.834836721420288 + 10.0 * 6.965544700622559
Epoch 80, val loss: 1.8312146663665771
Epoch 90, training loss: 70.63970947265625 = 1.824756145477295 + 10.0 * 6.881495475769043
Epoch 90, val loss: 1.8207144737243652
Epoch 100, training loss: 69.77598571777344 = 1.8139175176620483 + 10.0 * 6.796206951141357
Epoch 100, val loss: 1.8100823163986206
Epoch 110, training loss: 69.16471099853516 = 1.8053301572799683 + 10.0 * 6.735938549041748
Epoch 110, val loss: 1.8018231391906738
Epoch 120, training loss: 68.72127532958984 = 1.7969492673873901 + 10.0 * 6.692432403564453
Epoch 120, val loss: 1.7938514947891235
Epoch 130, training loss: 68.30078887939453 = 1.7886741161346436 + 10.0 * 6.651211261749268
Epoch 130, val loss: 1.7862586975097656
Epoch 140, training loss: 67.91487884521484 = 1.781270146369934 + 10.0 * 6.613360404968262
Epoch 140, val loss: 1.7792482376098633
Epoch 150, training loss: 67.56874084472656 = 1.7739170789718628 + 10.0 * 6.579482078552246
Epoch 150, val loss: 1.772217869758606
Epoch 160, training loss: 67.279541015625 = 1.7657722234725952 + 10.0 * 6.551376819610596
Epoch 160, val loss: 1.7645231485366821
Epoch 170, training loss: 67.00810241699219 = 1.7567100524902344 + 10.0 * 6.525138854980469
Epoch 170, val loss: 1.7559847831726074
Epoch 180, training loss: 66.76000213623047 = 1.7469418048858643 + 10.0 * 6.501306533813477
Epoch 180, val loss: 1.7467081546783447
Epoch 190, training loss: 66.53658294677734 = 1.7361705303192139 + 10.0 * 6.48004150390625
Epoch 190, val loss: 1.7366498708724976
Epoch 200, training loss: 66.33297729492188 = 1.724457025527954 + 10.0 * 6.460852146148682
Epoch 200, val loss: 1.7255313396453857
Epoch 210, training loss: 66.17173767089844 = 1.7114838361740112 + 10.0 * 6.4460248947143555
Epoch 210, val loss: 1.7133272886276245
Epoch 220, training loss: 66.043701171875 = 1.6971874237060547 + 10.0 * 6.434650897979736
Epoch 220, val loss: 1.699892282485962
Epoch 230, training loss: 65.9281005859375 = 1.6813669204711914 + 10.0 * 6.424673080444336
Epoch 230, val loss: 1.6852803230285645
Epoch 240, training loss: 65.7978286743164 = 1.6643601655960083 + 10.0 * 6.413346767425537
Epoch 240, val loss: 1.6693730354309082
Epoch 250, training loss: 65.70008087158203 = 1.645849585533142 + 10.0 * 6.405423164367676
Epoch 250, val loss: 1.6521987915039062
Epoch 260, training loss: 65.60149383544922 = 1.6258856058120728 + 10.0 * 6.397561073303223
Epoch 260, val loss: 1.6337443590164185
Epoch 270, training loss: 65.50682067871094 = 1.6046637296676636 + 10.0 * 6.390215873718262
Epoch 270, val loss: 1.6140779256820679
Epoch 280, training loss: 65.43977355957031 = 1.5820063352584839 + 10.0 * 6.385776519775391
Epoch 280, val loss: 1.5933380126953125
Epoch 290, training loss: 65.33714294433594 = 1.558341383934021 + 10.0 * 6.377880573272705
Epoch 290, val loss: 1.571333885192871
Epoch 300, training loss: 65.24447631835938 = 1.5334761142730713 + 10.0 * 6.371099948883057
Epoch 300, val loss: 1.5486425161361694
Epoch 310, training loss: 65.18667602539062 = 1.507649540901184 + 10.0 * 6.367902755737305
Epoch 310, val loss: 1.5252450704574585
Epoch 320, training loss: 65.1136245727539 = 1.4811935424804688 + 10.0 * 6.363243103027344
Epoch 320, val loss: 1.500961422920227
Epoch 330, training loss: 65.01045989990234 = 1.453913688659668 + 10.0 * 6.355654716491699
Epoch 330, val loss: 1.4764902591705322
Epoch 340, training loss: 64.9395523071289 = 1.4264049530029297 + 10.0 * 6.351315021514893
Epoch 340, val loss: 1.4517662525177002
Epoch 350, training loss: 64.8850326538086 = 1.3985891342163086 + 10.0 * 6.348644733428955
Epoch 350, val loss: 1.4268487691879272
Epoch 360, training loss: 64.82222747802734 = 1.3703885078430176 + 10.0 * 6.345183849334717
Epoch 360, val loss: 1.401996374130249
Epoch 370, training loss: 64.74153137207031 = 1.3424031734466553 + 10.0 * 6.339913368225098
Epoch 370, val loss: 1.3772045373916626
Epoch 380, training loss: 64.74761199951172 = 1.3144477605819702 + 10.0 * 6.343316555023193
Epoch 380, val loss: 1.3527686595916748
Epoch 390, training loss: 64.6191177368164 = 1.2868809700012207 + 10.0 * 6.333223819732666
Epoch 390, val loss: 1.3285385370254517
Epoch 400, training loss: 64.53829956054688 = 1.2597389221191406 + 10.0 * 6.327856063842773
Epoch 400, val loss: 1.3048648834228516
Epoch 410, training loss: 64.47772979736328 = 1.2329314947128296 + 10.0 * 6.324479579925537
Epoch 410, val loss: 1.2817109823226929
Epoch 420, training loss: 64.47823333740234 = 1.2064892053604126 + 10.0 * 6.327174186706543
Epoch 420, val loss: 1.2589521408081055
Epoch 430, training loss: 64.36121368408203 = 1.1803241968154907 + 10.0 * 6.318089485168457
Epoch 430, val loss: 1.236724615097046
Epoch 440, training loss: 64.31249237060547 = 1.1548012495040894 + 10.0 * 6.315768718719482
Epoch 440, val loss: 1.2150709629058838
Epoch 450, training loss: 64.30803680419922 = 1.1298165321350098 + 10.0 * 6.317821979522705
Epoch 450, val loss: 1.193966031074524
Epoch 460, training loss: 64.20907592773438 = 1.105210781097412 + 10.0 * 6.310386657714844
Epoch 460, val loss: 1.1734050512313843
Epoch 470, training loss: 64.15542602539062 = 1.0811585187911987 + 10.0 * 6.307426452636719
Epoch 470, val loss: 1.1534996032714844
Epoch 480, training loss: 64.11933135986328 = 1.0576272010803223 + 10.0 * 6.306170463562012
Epoch 480, val loss: 1.1341935396194458
Epoch 490, training loss: 64.06875610351562 = 1.0344743728637695 + 10.0 * 6.3034281730651855
Epoch 490, val loss: 1.1152352094650269
Epoch 500, training loss: 64.0128402709961 = 1.0118337869644165 + 10.0 * 6.300100803375244
Epoch 500, val loss: 1.0968469381332397
Epoch 510, training loss: 63.96556091308594 = 0.98966383934021 + 10.0 * 6.2975897789001465
Epoch 510, val loss: 1.0790371894836426
Epoch 520, training loss: 63.98291778564453 = 0.9678171277046204 + 10.0 * 6.301510334014893
Epoch 520, val loss: 1.0616906881332397
Epoch 530, training loss: 63.89061737060547 = 0.9465138912200928 + 10.0 * 6.294410228729248
Epoch 530, val loss: 1.0447003841400146
Epoch 540, training loss: 63.853271484375 = 0.925605058670044 + 10.0 * 6.292766571044922
Epoch 540, val loss: 1.0282434225082397
Epoch 550, training loss: 63.809410095214844 = 0.9051756262779236 + 10.0 * 6.290423393249512
Epoch 550, val loss: 1.0122617483139038
Epoch 560, training loss: 63.79859924316406 = 0.885078489780426 + 10.0 * 6.291352272033691
Epoch 560, val loss: 0.9967671632766724
Epoch 570, training loss: 63.732933044433594 = 0.8651009202003479 + 10.0 * 6.286783218383789
Epoch 570, val loss: 0.9817035794258118
Epoch 580, training loss: 63.69367980957031 = 0.8457241654396057 + 10.0 * 6.28479528427124
Epoch 580, val loss: 0.9669964909553528
Epoch 590, training loss: 63.709938049316406 = 0.8266156315803528 + 10.0 * 6.288332462310791
Epoch 590, val loss: 0.9526724219322205
Epoch 600, training loss: 63.64600372314453 = 0.8076691031455994 + 10.0 * 6.2838335037231445
Epoch 600, val loss: 0.9388771057128906
Epoch 610, training loss: 63.596343994140625 = 0.7892675995826721 + 10.0 * 6.280707359313965
Epoch 610, val loss: 0.9255238771438599
Epoch 620, training loss: 63.552940368652344 = 0.7711271047592163 + 10.0 * 6.278181552886963
Epoch 620, val loss: 0.9125289916992188
Epoch 630, training loss: 63.55324172973633 = 0.7533060312271118 + 10.0 * 6.279993534088135
Epoch 630, val loss: 0.8999596238136292
Epoch 640, training loss: 63.553096771240234 = 0.7357116341590881 + 10.0 * 6.281738758087158
Epoch 640, val loss: 0.8878639936447144
Epoch 650, training loss: 63.46931076049805 = 0.7182029485702515 + 10.0 * 6.275110721588135
Epoch 650, val loss: 0.8761703968048096
Epoch 660, training loss: 63.438514709472656 = 0.7012209296226501 + 10.0 * 6.27372932434082
Epoch 660, val loss: 0.8648175001144409
Epoch 670, training loss: 63.402793884277344 = 0.6844755411148071 + 10.0 * 6.27183198928833
Epoch 670, val loss: 0.8540118336677551
Epoch 680, training loss: 63.39585494995117 = 0.6679653525352478 + 10.0 * 6.272789001464844
Epoch 680, val loss: 0.8436274528503418
Epoch 690, training loss: 63.35776901245117 = 0.6516978740692139 + 10.0 * 6.270606994628906
Epoch 690, val loss: 0.8334465026855469
Epoch 700, training loss: 63.33232498168945 = 0.6356168985366821 + 10.0 * 6.2696709632873535
Epoch 700, val loss: 0.823869526386261
Epoch 710, training loss: 63.30193328857422 = 0.619841992855072 + 10.0 * 6.268208980560303
Epoch 710, val loss: 0.8146346807479858
Epoch 720, training loss: 63.411128997802734 = 0.6042155623435974 + 10.0 * 6.280691623687744
Epoch 720, val loss: 0.8058344721794128
Epoch 730, training loss: 63.27455139160156 = 0.5890040397644043 + 10.0 * 6.2685546875
Epoch 730, val loss: 0.797163724899292
Epoch 740, training loss: 63.22111892700195 = 0.5741245150566101 + 10.0 * 6.264699459075928
Epoch 740, val loss: 0.7890129089355469
Epoch 750, training loss: 63.191932678222656 = 0.5596297979354858 + 10.0 * 6.263230323791504
Epoch 750, val loss: 0.7812901139259338
Epoch 760, training loss: 63.1684455871582 = 0.5453876852989197 + 10.0 * 6.262305736541748
Epoch 760, val loss: 0.7739484310150146
Epoch 770, training loss: 63.14435577392578 = 0.531373143196106 + 10.0 * 6.261298179626465
Epoch 770, val loss: 0.766936719417572
Epoch 780, training loss: 63.162322998046875 = 0.517584502696991 + 10.0 * 6.264473915100098
Epoch 780, val loss: 0.7603365182876587
Epoch 790, training loss: 63.16066360473633 = 0.5040298700332642 + 10.0 * 6.2656636238098145
Epoch 790, val loss: 0.7538326978683472
Epoch 800, training loss: 63.08137130737305 = 0.4908140301704407 + 10.0 * 6.2590556144714355
Epoch 800, val loss: 0.7477813959121704
Epoch 810, training loss: 63.06240463256836 = 0.4779694080352783 + 10.0 * 6.258443355560303
Epoch 810, val loss: 0.7421727776527405
Epoch 820, training loss: 63.041229248046875 = 0.4654253125190735 + 10.0 * 6.257580757141113
Epoch 820, val loss: 0.7369083762168884
Epoch 830, training loss: 63.0735969543457 = 0.4532076120376587 + 10.0 * 6.262038707733154
Epoch 830, val loss: 0.7318241596221924
Epoch 840, training loss: 63.031700134277344 = 0.4411439895629883 + 10.0 * 6.2590556144714355
Epoch 840, val loss: 0.7269943952560425
Epoch 850, training loss: 62.98731994628906 = 0.42946502566337585 + 10.0 * 6.2557854652404785
Epoch 850, val loss: 0.7224170565605164
Epoch 860, training loss: 62.962806701660156 = 0.4181029498577118 + 10.0 * 6.254470348358154
Epoch 860, val loss: 0.71822190284729
Epoch 870, training loss: 63.02085494995117 = 0.40706896781921387 + 10.0 * 6.261378288269043
Epoch 870, val loss: 0.7142086625099182
Epoch 880, training loss: 62.95511245727539 = 0.396231472492218 + 10.0 * 6.255887985229492
Epoch 880, val loss: 0.7104108333587646
Epoch 890, training loss: 62.919490814208984 = 0.3857150375843048 + 10.0 * 6.253377437591553
Epoch 890, val loss: 0.7068690061569214
Epoch 900, training loss: 62.89139175415039 = 0.3755298852920532 + 10.0 * 6.251585960388184
Epoch 900, val loss: 0.7036288380622864
Epoch 910, training loss: 62.89278793334961 = 0.36563217639923096 + 10.0 * 6.252715587615967
Epoch 910, val loss: 0.7006223201751709
Epoch 920, training loss: 62.87024688720703 = 0.35599401593208313 + 10.0 * 6.251425266265869
Epoch 920, val loss: 0.6977394819259644
Epoch 930, training loss: 62.85234832763672 = 0.34669309854507446 + 10.0 * 6.250565528869629
Epoch 930, val loss: 0.6950345039367676
Epoch 940, training loss: 62.83235168457031 = 0.33757176995277405 + 10.0 * 6.249478340148926
Epoch 940, val loss: 0.6926971673965454
Epoch 950, training loss: 62.85505294799805 = 0.3287668824195862 + 10.0 * 6.252628803253174
Epoch 950, val loss: 0.6904932856559753
Epoch 960, training loss: 62.82304763793945 = 0.32014763355255127 + 10.0 * 6.2502899169921875
Epoch 960, val loss: 0.6883686184883118
Epoch 970, training loss: 62.79819869995117 = 0.31173354387283325 + 10.0 * 6.2486467361450195
Epoch 970, val loss: 0.6865365505218506
Epoch 980, training loss: 62.785911560058594 = 0.3035968840122223 + 10.0 * 6.248231410980225
Epoch 980, val loss: 0.6848058700561523
Epoch 990, training loss: 62.77777862548828 = 0.2956513464450836 + 10.0 * 6.248212814331055
Epoch 990, val loss: 0.6831852793693542
Epoch 1000, training loss: 62.749977111816406 = 0.28792837262153625 + 10.0 * 6.246204853057861
Epoch 1000, val loss: 0.6816689372062683
Epoch 1010, training loss: 62.732242584228516 = 0.28044503927230835 + 10.0 * 6.245179653167725
Epoch 1010, val loss: 0.6803154349327087
Epoch 1020, training loss: 62.74147415161133 = 0.27315184473991394 + 10.0 * 6.246832370758057
Epoch 1020, val loss: 0.6791157722473145
Epoch 1030, training loss: 62.71268844604492 = 0.26597562432289124 + 10.0 * 6.24467134475708
Epoch 1030, val loss: 0.6781113743782043
Epoch 1040, training loss: 62.71388244628906 = 0.25901147723197937 + 10.0 * 6.245487213134766
Epoch 1040, val loss: 0.6771042346954346
Epoch 1050, training loss: 62.68552780151367 = 0.2522447109222412 + 10.0 * 6.24332857131958
Epoch 1050, val loss: 0.6762598156929016
Epoch 1060, training loss: 62.69350814819336 = 0.2456524521112442 + 10.0 * 6.244785785675049
Epoch 1060, val loss: 0.675561249256134
Epoch 1070, training loss: 62.66372299194336 = 0.23918315768241882 + 10.0 * 6.2424540519714355
Epoch 1070, val loss: 0.6748207807540894
Epoch 1080, training loss: 62.65604782104492 = 0.2328914850950241 + 10.0 * 6.242315769195557
Epoch 1080, val loss: 0.6741885542869568
Epoch 1090, training loss: 62.634552001953125 = 0.22679747641086578 + 10.0 * 6.240775108337402
Epoch 1090, val loss: 0.6737542748451233
Epoch 1100, training loss: 62.622798919677734 = 0.22083470225334167 + 10.0 * 6.240196704864502
Epoch 1100, val loss: 0.6733940839767456
Epoch 1110, training loss: 62.743408203125 = 0.2149423211812973 + 10.0 * 6.252846717834473
Epoch 1110, val loss: 0.673090398311615
Epoch 1120, training loss: 62.623172760009766 = 0.20929035544395447 + 10.0 * 6.241388320922852
Epoch 1120, val loss: 0.6725873947143555
Epoch 1130, training loss: 62.593048095703125 = 0.20370744168758392 + 10.0 * 6.23893404006958
Epoch 1130, val loss: 0.6723660230636597
Epoch 1140, training loss: 62.584144592285156 = 0.19830477237701416 + 10.0 * 6.238584041595459
Epoch 1140, val loss: 0.672342836856842
Epoch 1150, training loss: 62.58348846435547 = 0.19308429956436157 + 10.0 * 6.239040374755859
Epoch 1150, val loss: 0.6723912954330444
Epoch 1160, training loss: 62.575618743896484 = 0.18790483474731445 + 10.0 * 6.238771438598633
Epoch 1160, val loss: 0.6724179983139038
Epoch 1170, training loss: 62.56638717651367 = 0.18284198641777039 + 10.0 * 6.238354682922363
Epoch 1170, val loss: 0.6724429726600647
Epoch 1180, training loss: 62.54420471191406 = 0.17796166241168976 + 10.0 * 6.236624240875244
Epoch 1180, val loss: 0.6726577281951904
Epoch 1190, training loss: 62.56770706176758 = 0.17321860790252686 + 10.0 * 6.2394490242004395
Epoch 1190, val loss: 0.6730025410652161
Epoch 1200, training loss: 62.536678314208984 = 0.1685430109500885 + 10.0 * 6.236813545227051
Epoch 1200, val loss: 0.673241376876831
Epoch 1210, training loss: 62.53065490722656 = 0.16400764882564545 + 10.0 * 6.236664772033691
Epoch 1210, val loss: 0.6735943555831909
Epoch 1220, training loss: 62.5155029296875 = 0.15959088504314423 + 10.0 * 6.235590934753418
Epoch 1220, val loss: 0.6740964651107788
Epoch 1230, training loss: 62.51435470581055 = 0.15531453490257263 + 10.0 * 6.235903739929199
Epoch 1230, val loss: 0.6746309399604797
Epoch 1240, training loss: 62.51998519897461 = 0.15113738179206848 + 10.0 * 6.236884593963623
Epoch 1240, val loss: 0.6751067638397217
Epoch 1250, training loss: 62.48508071899414 = 0.1470356583595276 + 10.0 * 6.233804225921631
Epoch 1250, val loss: 0.6758084893226624
Epoch 1260, training loss: 62.47988510131836 = 0.1430722177028656 + 10.0 * 6.2336812019348145
Epoch 1260, val loss: 0.6765120625495911
Epoch 1270, training loss: 62.4978141784668 = 0.13922202587127686 + 10.0 * 6.235859394073486
Epoch 1270, val loss: 0.6773350238800049
Epoch 1280, training loss: 62.46393585205078 = 0.13548322021961212 + 10.0 * 6.232845306396484
Epoch 1280, val loss: 0.6782098412513733
Epoch 1290, training loss: 62.46147155761719 = 0.13185764849185944 + 10.0 * 6.232961177825928
Epoch 1290, val loss: 0.6790648102760315
Epoch 1300, training loss: 62.474361419677734 = 0.1283145248889923 + 10.0 * 6.234604835510254
Epoch 1300, val loss: 0.6799914836883545
Epoch 1310, training loss: 62.45622253417969 = 0.12485216557979584 + 10.0 * 6.233137130737305
Epoch 1310, val loss: 0.6810187101364136
Epoch 1320, training loss: 62.455257415771484 = 0.12150967121124268 + 10.0 * 6.23337459564209
Epoch 1320, val loss: 0.6820076107978821
Epoch 1330, training loss: 62.43061447143555 = 0.11825647950172424 + 10.0 * 6.231235980987549
Epoch 1330, val loss: 0.683171272277832
Epoch 1340, training loss: 62.449642181396484 = 0.11510402709245682 + 10.0 * 6.233453750610352
Epoch 1340, val loss: 0.6843811869621277
Epoch 1350, training loss: 62.41025924682617 = 0.11203610152006149 + 10.0 * 6.229822158813477
Epoch 1350, val loss: 0.6854748725891113
Epoch 1360, training loss: 62.401546478271484 = 0.10906603187322617 + 10.0 * 6.229248046875
Epoch 1360, val loss: 0.6866697072982788
Epoch 1370, training loss: 62.393516540527344 = 0.1061948761343956 + 10.0 * 6.228732109069824
Epoch 1370, val loss: 0.6880869269371033
Epoch 1380, training loss: 62.42808151245117 = 0.10343530774116516 + 10.0 * 6.232464790344238
Epoch 1380, val loss: 0.6894819736480713
Epoch 1390, training loss: 62.391090393066406 = 0.10068349540233612 + 10.0 * 6.229040622711182
Epoch 1390, val loss: 0.6906982660293579
Epoch 1400, training loss: 62.39080047607422 = 0.09805633872747421 + 10.0 * 6.229274272918701
Epoch 1400, val loss: 0.6921284794807434
Epoch 1410, training loss: 62.37201690673828 = 0.09551627933979034 + 10.0 * 6.227650165557861
Epoch 1410, val loss: 0.6935648918151855
Epoch 1420, training loss: 62.36228561401367 = 0.0930706113576889 + 10.0 * 6.226921558380127
Epoch 1420, val loss: 0.6951389312744141
Epoch 1430, training loss: 62.45743179321289 = 0.09068675339221954 + 10.0 * 6.2366743087768555
Epoch 1430, val loss: 0.6967357993125916
Epoch 1440, training loss: 62.400428771972656 = 0.08832693845033646 + 10.0 * 6.231210231781006
Epoch 1440, val loss: 0.6979982256889343
Epoch 1450, training loss: 62.34642791748047 = 0.08606202900409698 + 10.0 * 6.226036548614502
Epoch 1450, val loss: 0.6995255947113037
Epoch 1460, training loss: 62.347957611083984 = 0.08389177173376083 + 10.0 * 6.226406574249268
Epoch 1460, val loss: 0.701171338558197
Epoch 1470, training loss: 62.39375305175781 = 0.08177574723958969 + 10.0 * 6.231197834014893
Epoch 1470, val loss: 0.7028668522834778
Epoch 1480, training loss: 62.349056243896484 = 0.07974555343389511 + 10.0 * 6.226931095123291
Epoch 1480, val loss: 0.7043501734733582
Epoch 1490, training loss: 62.33576202392578 = 0.07774710655212402 + 10.0 * 6.225801467895508
Epoch 1490, val loss: 0.7059817314147949
Epoch 1500, training loss: 62.33745574951172 = 0.0758388340473175 + 10.0 * 6.226161479949951
Epoch 1500, val loss: 0.7077469825744629
Epoch 1510, training loss: 62.33100891113281 = 0.07396725565195084 + 10.0 * 6.225704193115234
Epoch 1510, val loss: 0.7093619704246521
Epoch 1520, training loss: 62.32282638549805 = 0.07217426598072052 + 10.0 * 6.225065231323242
Epoch 1520, val loss: 0.7109633684158325
Epoch 1530, training loss: 62.30670928955078 = 0.07042469829320908 + 10.0 * 6.223628520965576
Epoch 1530, val loss: 0.7126954197883606
Epoch 1540, training loss: 62.32295608520508 = 0.06872867047786713 + 10.0 * 6.2254228591918945
Epoch 1540, val loss: 0.7144746780395508
Epoch 1550, training loss: 62.30611801147461 = 0.06708254665136337 + 10.0 * 6.223903656005859
Epoch 1550, val loss: 0.7161596417427063
Epoch 1560, training loss: 62.3203239440918 = 0.06547079235315323 + 10.0 * 6.225485324859619
Epoch 1560, val loss: 0.7178968787193298
Epoch 1570, training loss: 62.300716400146484 = 0.06392478197813034 + 10.0 * 6.223679542541504
Epoch 1570, val loss: 0.7197363376617432
Epoch 1580, training loss: 62.2917366027832 = 0.06241919472813606 + 10.0 * 6.222931861877441
Epoch 1580, val loss: 0.7213935852050781
Epoch 1590, training loss: 62.28402328491211 = 0.060973964631557465 + 10.0 * 6.222304821014404
Epoch 1590, val loss: 0.7231970429420471
Epoch 1600, training loss: 62.32566452026367 = 0.05957876518368721 + 10.0 * 6.226608753204346
Epoch 1600, val loss: 0.7249738574028015
Epoch 1610, training loss: 62.28227996826172 = 0.058179497718811035 + 10.0 * 6.222410202026367
Epoch 1610, val loss: 0.7268186211585999
Epoch 1620, training loss: 62.26729202270508 = 0.05687006562948227 + 10.0 * 6.221042156219482
Epoch 1620, val loss: 0.7286826372146606
Epoch 1630, training loss: 62.258968353271484 = 0.055595602840185165 + 10.0 * 6.220337390899658
Epoch 1630, val loss: 0.7306005358695984
Epoch 1640, training loss: 62.25670623779297 = 0.05436259135603905 + 10.0 * 6.220234394073486
Epoch 1640, val loss: 0.7325079441070557
Epoch 1650, training loss: 62.316898345947266 = 0.05316467955708504 + 10.0 * 6.226373195648193
Epoch 1650, val loss: 0.7343403100967407
Epoch 1660, training loss: 62.33409881591797 = 0.051961928606033325 + 10.0 * 6.228213310241699
Epoch 1660, val loss: 0.7361873388290405
Epoch 1670, training loss: 62.25922775268555 = 0.050800155848264694 + 10.0 * 6.2208428382873535
Epoch 1670, val loss: 0.7378054857254028
Epoch 1680, training loss: 62.250831604003906 = 0.04969356209039688 + 10.0 * 6.220113754272461
Epoch 1680, val loss: 0.7396501898765564
Epoch 1690, training loss: 62.23384094238281 = 0.048630114644765854 + 10.0 * 6.2185211181640625
Epoch 1690, val loss: 0.7415736317634583
Epoch 1700, training loss: 62.229068756103516 = 0.04760381579399109 + 10.0 * 6.218146324157715
Epoch 1700, val loss: 0.743524968624115
Epoch 1710, training loss: 62.25537872314453 = 0.046613242477178574 + 10.0 * 6.220876693725586
Epoch 1710, val loss: 0.7453837990760803
Epoch 1720, training loss: 62.26748275756836 = 0.045620206743478775 + 10.0 * 6.222186088562012
Epoch 1720, val loss: 0.7472167015075684
Epoch 1730, training loss: 62.24982452392578 = 0.04463779926300049 + 10.0 * 6.220518589019775
Epoch 1730, val loss: 0.749004065990448
Epoch 1740, training loss: 62.21697998046875 = 0.043714821338653564 + 10.0 * 6.2173261642456055
Epoch 1740, val loss: 0.7509503960609436
Epoch 1750, training loss: 62.21538543701172 = 0.04283105954527855 + 10.0 * 6.217255592346191
Epoch 1750, val loss: 0.7528623342514038
Epoch 1760, training loss: 62.21554183959961 = 0.04196964204311371 + 10.0 * 6.217357158660889
Epoch 1760, val loss: 0.754791796207428
Epoch 1770, training loss: 62.285118103027344 = 0.041119180619716644 + 10.0 * 6.224400043487549
Epoch 1770, val loss: 0.7566453814506531
Epoch 1780, training loss: 62.22517776489258 = 0.04029165208339691 + 10.0 * 6.218488693237305
Epoch 1780, val loss: 0.7584704756736755
Epoch 1790, training loss: 62.207786560058594 = 0.03949118033051491 + 10.0 * 6.216829776763916
Epoch 1790, val loss: 0.7602883577346802
Epoch 1800, training loss: 62.20243835449219 = 0.03872041404247284 + 10.0 * 6.216372013092041
Epoch 1800, val loss: 0.7622548937797546
Epoch 1810, training loss: 62.26601028442383 = 0.037964995950460434 + 10.0 * 6.222804546356201
Epoch 1810, val loss: 0.7641197443008423
Epoch 1820, training loss: 62.22540283203125 = 0.037222251296043396 + 10.0 * 6.218817710876465
Epoch 1820, val loss: 0.7659397125244141
Epoch 1830, training loss: 62.19583511352539 = 0.03650524839758873 + 10.0 * 6.215932846069336
Epoch 1830, val loss: 0.7677145004272461
Epoch 1840, training loss: 62.19184875488281 = 0.03581372648477554 + 10.0 * 6.215603828430176
Epoch 1840, val loss: 0.7697162628173828
Epoch 1850, training loss: 62.25088119506836 = 0.035132694989442825 + 10.0 * 6.221574783325195
Epoch 1850, val loss: 0.7716289758682251
Epoch 1860, training loss: 62.19569778442383 = 0.03447618708014488 + 10.0 * 6.216122150421143
Epoch 1860, val loss: 0.773197591304779
Epoch 1870, training loss: 62.179988861083984 = 0.0338326133787632 + 10.0 * 6.214615821838379
Epoch 1870, val loss: 0.7750523090362549
Epoch 1880, training loss: 62.17313003540039 = 0.033216699957847595 + 10.0 * 6.213991165161133
Epoch 1880, val loss: 0.7769551873207092
Epoch 1890, training loss: 62.21078109741211 = 0.03262387588620186 + 10.0 * 6.21781587600708
Epoch 1890, val loss: 0.7787903547286987
Epoch 1900, training loss: 62.20051193237305 = 0.03200962767004967 + 10.0 * 6.216850280761719
Epoch 1900, val loss: 0.7804826498031616
Epoch 1910, training loss: 62.187747955322266 = 0.03142165020108223 + 10.0 * 6.215632438659668
Epoch 1910, val loss: 0.7822015881538391
Epoch 1920, training loss: 62.16232681274414 = 0.03086184523999691 + 10.0 * 6.213146686553955
Epoch 1920, val loss: 0.783958375453949
Epoch 1930, training loss: 62.15719223022461 = 0.030322633683681488 + 10.0 * 6.212687015533447
Epoch 1930, val loss: 0.7858415246009827
Epoch 1940, training loss: 62.169151306152344 = 0.029801469296216965 + 10.0 * 6.213934898376465
Epoch 1940, val loss: 0.7876251339912415
Epoch 1950, training loss: 62.190738677978516 = 0.02928360179066658 + 10.0 * 6.2161455154418945
Epoch 1950, val loss: 0.7893125414848328
Epoch 1960, training loss: 62.16151428222656 = 0.028760837391018867 + 10.0 * 6.21327543258667
Epoch 1960, val loss: 0.7911039590835571
Epoch 1970, training loss: 62.168487548828125 = 0.02827279269695282 + 10.0 * 6.2140212059021
Epoch 1970, val loss: 0.7927930951118469
Epoch 1980, training loss: 62.162139892578125 = 0.027783317491412163 + 10.0 * 6.213435649871826
Epoch 1980, val loss: 0.7945544123649597
Epoch 1990, training loss: 62.14759826660156 = 0.027314363047480583 + 10.0 * 6.212028503417969
Epoch 1990, val loss: 0.796287477016449
Epoch 2000, training loss: 62.14259719848633 = 0.026858331635594368 + 10.0 * 6.211573600769043
Epoch 2000, val loss: 0.7980020642280579
Epoch 2010, training loss: 62.170162200927734 = 0.026424748823046684 + 10.0 * 6.214373588562012
Epoch 2010, val loss: 0.7997065186500549
Epoch 2020, training loss: 62.147682189941406 = 0.025978080928325653 + 10.0 * 6.212170600891113
Epoch 2020, val loss: 0.8013906478881836
Epoch 2030, training loss: 62.1507568359375 = 0.025545699521899223 + 10.0 * 6.212521076202393
Epoch 2030, val loss: 0.802962601184845
Epoch 2040, training loss: 62.13985061645508 = 0.025128036737442017 + 10.0 * 6.211472511291504
Epoch 2040, val loss: 0.80467689037323
Epoch 2050, training loss: 62.12887954711914 = 0.024725470691919327 + 10.0 * 6.210415363311768
Epoch 2050, val loss: 0.8064664602279663
Epoch 2060, training loss: 62.12705612182617 = 0.024333497509360313 + 10.0 * 6.210272312164307
Epoch 2060, val loss: 0.8081756830215454
Epoch 2070, training loss: 62.20188522338867 = 0.023953404277563095 + 10.0 * 6.2177934646606445
Epoch 2070, val loss: 0.8098528981208801
Epoch 2080, training loss: 62.142860412597656 = 0.02356315590441227 + 10.0 * 6.211929798126221
Epoch 2080, val loss: 0.8112881779670715
Epoch 2090, training loss: 62.16416549682617 = 0.023187408223748207 + 10.0 * 6.21409797668457
Epoch 2090, val loss: 0.8129748702049255
Epoch 2100, training loss: 62.12665939331055 = 0.022825120016932487 + 10.0 * 6.210383415222168
Epoch 2100, val loss: 0.8145967125892639
Epoch 2110, training loss: 62.120121002197266 = 0.022475283592939377 + 10.0 * 6.20976448059082
Epoch 2110, val loss: 0.8161753416061401
Epoch 2120, training loss: 62.113731384277344 = 0.022130334749817848 + 10.0 * 6.209160327911377
Epoch 2120, val loss: 0.8178774118423462
Epoch 2130, training loss: 62.10923767089844 = 0.02180011011660099 + 10.0 * 6.208743572235107
Epoch 2130, val loss: 0.8195393085479736
Epoch 2140, training loss: 62.18764877319336 = 0.02147987112402916 + 10.0 * 6.216616630554199
Epoch 2140, val loss: 0.8211548924446106
Epoch 2150, training loss: 62.1408805847168 = 0.02114725112915039 + 10.0 * 6.211973190307617
Epoch 2150, val loss: 0.8225833177566528
Epoch 2160, training loss: 62.11559295654297 = 0.020830877125263214 + 10.0 * 6.209475994110107
Epoch 2160, val loss: 0.8241410255432129
Epoch 2170, training loss: 62.12177658081055 = 0.0205239225178957 + 10.0 * 6.21012544631958
Epoch 2170, val loss: 0.8258371353149414
Epoch 2180, training loss: 62.12825012207031 = 0.020217828452587128 + 10.0 * 6.210803031921387
Epoch 2180, val loss: 0.8274452090263367
Epoch 2190, training loss: 62.110511779785156 = 0.01992294378578663 + 10.0 * 6.20905876159668
Epoch 2190, val loss: 0.8287943601608276
Epoch 2200, training loss: 62.100460052490234 = 0.01963701657950878 + 10.0 * 6.20808219909668
Epoch 2200, val loss: 0.8303249478340149
Epoch 2210, training loss: 62.09206771850586 = 0.01935591921210289 + 10.0 * 6.207271099090576
Epoch 2210, val loss: 0.8318633437156677
Epoch 2220, training loss: 62.094932556152344 = 0.01908479817211628 + 10.0 * 6.207584857940674
Epoch 2220, val loss: 0.8334563374519348
Epoch 2230, training loss: 62.14566421508789 = 0.018822845071554184 + 10.0 * 6.212684154510498
Epoch 2230, val loss: 0.8348742723464966
Epoch 2240, training loss: 62.12595748901367 = 0.01854889653623104 + 10.0 * 6.21074104309082
Epoch 2240, val loss: 0.8362849354743958
Epoch 2250, training loss: 62.10026550292969 = 0.018285173922777176 + 10.0 * 6.208198070526123
Epoch 2250, val loss: 0.8378512263298035
Epoch 2260, training loss: 62.10396957397461 = 0.018030352890491486 + 10.0 * 6.208593845367432
Epoch 2260, val loss: 0.8392573595046997
Epoch 2270, training loss: 62.09827423095703 = 0.017785074189305305 + 10.0 * 6.2080488204956055
Epoch 2270, val loss: 0.8407206535339355
Epoch 2280, training loss: 62.134246826171875 = 0.017546387389302254 + 10.0 * 6.211669921875
Epoch 2280, val loss: 0.8421168327331543
Epoch 2290, training loss: 62.0966911315918 = 0.017293289303779602 + 10.0 * 6.207940101623535
Epoch 2290, val loss: 0.8436787724494934
Epoch 2300, training loss: 62.08261489868164 = 0.017065685242414474 + 10.0 * 6.206554889678955
Epoch 2300, val loss: 0.8450582027435303
Epoch 2310, training loss: 62.088443756103516 = 0.01684006303548813 + 10.0 * 6.207159996032715
Epoch 2310, val loss: 0.8465564846992493
Epoch 2320, training loss: 62.08528518676758 = 0.016616908833384514 + 10.0 * 6.20686674118042
Epoch 2320, val loss: 0.8478904962539673
Epoch 2330, training loss: 62.083927154541016 = 0.01639828458428383 + 10.0 * 6.206752777099609
Epoch 2330, val loss: 0.849265992641449
Epoch 2340, training loss: 62.08307647705078 = 0.016182241961359978 + 10.0 * 6.206689357757568
Epoch 2340, val loss: 0.8507041931152344
Epoch 2350, training loss: 62.093143463134766 = 0.015972230583429337 + 10.0 * 6.207716941833496
Epoch 2350, val loss: 0.8521836996078491
Epoch 2360, training loss: 62.07177734375 = 0.015764055773615837 + 10.0 * 6.205601692199707
Epoch 2360, val loss: 0.8534647226333618
Epoch 2370, training loss: 62.081443786621094 = 0.015559614636003971 + 10.0 * 6.206588268280029
Epoch 2370, val loss: 0.8549455404281616
Epoch 2380, training loss: 62.10307312011719 = 0.015358573757112026 + 10.0 * 6.208771705627441
Epoch 2380, val loss: 0.8563194274902344
Epoch 2390, training loss: 62.06686019897461 = 0.015162831172347069 + 10.0 * 6.205169677734375
Epoch 2390, val loss: 0.8574874401092529
Epoch 2400, training loss: 62.055057525634766 = 0.014972232282161713 + 10.0 * 6.20400857925415
Epoch 2400, val loss: 0.8589590191841125
Epoch 2410, training loss: 62.05900192260742 = 0.014789378270506859 + 10.0 * 6.204421043395996
Epoch 2410, val loss: 0.8603449463844299
Epoch 2420, training loss: 62.10047912597656 = 0.01460905373096466 + 10.0 * 6.208586692810059
Epoch 2420, val loss: 0.8617305755615234
Epoch 2430, training loss: 62.06550598144531 = 0.01442332286387682 + 10.0 * 6.205108165740967
Epoch 2430, val loss: 0.8629655241966248
Epoch 2440, training loss: 62.06645965576172 = 0.014245633967220783 + 10.0 * 6.205221652984619
Epoch 2440, val loss: 0.8644300103187561
Epoch 2450, training loss: 62.102317810058594 = 0.014071676880121231 + 10.0 * 6.208824634552002
Epoch 2450, val loss: 0.8657690286636353
Epoch 2460, training loss: 62.06798553466797 = 0.013903740793466568 + 10.0 * 6.205408096313477
Epoch 2460, val loss: 0.8670541048049927
Epoch 2470, training loss: 62.043060302734375 = 0.013732722960412502 + 10.0 * 6.202932834625244
Epoch 2470, val loss: 0.8683180809020996
Epoch 2480, training loss: 62.04439926147461 = 0.013570260256528854 + 10.0 * 6.203083038330078
Epoch 2480, val loss: 0.8696920871734619
Epoch 2490, training loss: 62.04539489746094 = 0.013413899578154087 + 10.0 * 6.203198432922363
Epoch 2490, val loss: 0.8709961175918579
Epoch 2500, training loss: 62.095619201660156 = 0.013258627615869045 + 10.0 * 6.208235740661621
Epoch 2500, val loss: 0.8722884058952332
Epoch 2510, training loss: 62.06342315673828 = 0.013099430128932 + 10.0 * 6.2050323486328125
Epoch 2510, val loss: 0.8734831213951111
Epoch 2520, training loss: 62.057498931884766 = 0.012943720445036888 + 10.0 * 6.204455375671387
Epoch 2520, val loss: 0.8747673034667969
Epoch 2530, training loss: 62.04681396484375 = 0.0127937076613307 + 10.0 * 6.203402042388916
Epoch 2530, val loss: 0.8759687542915344
Epoch 2540, training loss: 62.060428619384766 = 0.012646724469959736 + 10.0 * 6.20477819442749
Epoch 2540, val loss: 0.8772249221801758
Epoch 2550, training loss: 62.05143356323242 = 0.012501377612352371 + 10.0 * 6.203893184661865
Epoch 2550, val loss: 0.8785468935966492
Epoch 2560, training loss: 62.05201721191406 = 0.012359092012047768 + 10.0 * 6.203965663909912
Epoch 2560, val loss: 0.8796366453170776
Epoch 2570, training loss: 62.03877639770508 = 0.012216503731906414 + 10.0 * 6.202656269073486
Epoch 2570, val loss: 0.881010115146637
Epoch 2580, training loss: 62.06901931762695 = 0.012079592794179916 + 10.0 * 6.20569372177124
Epoch 2580, val loss: 0.882254958152771
Epoch 2590, training loss: 62.026451110839844 = 0.011945773847401142 + 10.0 * 6.201450347900391
Epoch 2590, val loss: 0.8833179473876953
Epoch 2600, training loss: 62.02439498901367 = 0.011816028505563736 + 10.0 * 6.201257705688477
Epoch 2600, val loss: 0.884633481502533
Epoch 2610, training loss: 62.02478790283203 = 0.011687629856169224 + 10.0 * 6.201310157775879
Epoch 2610, val loss: 0.8858699798583984
Epoch 2620, training loss: 62.065696716308594 = 0.011565569788217545 + 10.0 * 6.205412864685059
Epoch 2620, val loss: 0.8870164155960083
Epoch 2630, training loss: 62.036155700683594 = 0.011430486105382442 + 10.0 * 6.202472686767578
Epoch 2630, val loss: 0.8882285356521606
Epoch 2640, training loss: 62.026981353759766 = 0.011304323561489582 + 10.0 * 6.201567649841309
Epoch 2640, val loss: 0.8892770409584045
Epoch 2650, training loss: 62.02287292480469 = 0.011183648370206356 + 10.0 * 6.201169013977051
Epoch 2650, val loss: 0.8904996514320374
Epoch 2660, training loss: 62.02215576171875 = 0.011067228391766548 + 10.0 * 6.201108932495117
Epoch 2660, val loss: 0.891690731048584
Epoch 2670, training loss: 62.0339469909668 = 0.010950103402137756 + 10.0 * 6.20229959487915
Epoch 2670, val loss: 0.8928930163383484
Epoch 2680, training loss: 62.04595947265625 = 0.0108342245221138 + 10.0 * 6.203512668609619
Epoch 2680, val loss: 0.8939863443374634
Epoch 2690, training loss: 62.021636962890625 = 0.010715709999203682 + 10.0 * 6.20109224319458
Epoch 2690, val loss: 0.8951429128646851
Epoch 2700, training loss: 62.0120964050293 = 0.01060466654598713 + 10.0 * 6.200149059295654
Epoch 2700, val loss: 0.8961998224258423
Epoch 2710, training loss: 62.030208587646484 = 0.01049395464360714 + 10.0 * 6.201971530914307
Epoch 2710, val loss: 0.8974884748458862
Epoch 2720, training loss: 62.01742935180664 = 0.010385263711214066 + 10.0 * 6.200704574584961
Epoch 2720, val loss: 0.8985689282417297
Epoch 2730, training loss: 62.01361083984375 = 0.010281732305884361 + 10.0 * 6.200333118438721
Epoch 2730, val loss: 0.8995957970619202
Epoch 2740, training loss: 62.00653076171875 = 0.010176370851695538 + 10.0 * 6.1996355056762695
Epoch 2740, val loss: 0.9007931351661682
Epoch 2750, training loss: 62.02918243408203 = 0.01007751189172268 + 10.0 * 6.201910495758057
Epoch 2750, val loss: 0.9018272757530212
Epoch 2760, training loss: 62.02982711791992 = 0.009975473396480083 + 10.0 * 6.2019853591918945
Epoch 2760, val loss: 0.9028538465499878
Epoch 2770, training loss: 62.00836181640625 = 0.009871277958154678 + 10.0 * 6.1998491287231445
Epoch 2770, val loss: 0.9038646817207336
Epoch 2780, training loss: 62.00452423095703 = 0.009773540310561657 + 10.0 * 6.199475288391113
Epoch 2780, val loss: 0.9048553705215454
Epoch 2790, training loss: 62.021358489990234 = 0.009680756367743015 + 10.0 * 6.201167583465576
Epoch 2790, val loss: 0.9059398174285889
Epoch 2800, training loss: 62.020668029785156 = 0.009581102058291435 + 10.0 * 6.201108455657959
Epoch 2800, val loss: 0.9069628715515137
Epoch 2810, training loss: 62.00236511230469 = 0.009485548362135887 + 10.0 * 6.1992878913879395
Epoch 2810, val loss: 0.9079229831695557
Epoch 2820, training loss: 61.99303436279297 = 0.009395010769367218 + 10.0 * 6.198363780975342
Epoch 2820, val loss: 0.90903240442276
Epoch 2830, training loss: 61.98988342285156 = 0.00930669903755188 + 10.0 * 6.198057651519775
Epoch 2830, val loss: 0.9099950194358826
Epoch 2840, training loss: 62.033145904541016 = 0.009220711886882782 + 10.0 * 6.202392578125
Epoch 2840, val loss: 0.91106116771698
Epoch 2850, training loss: 62.01097106933594 = 0.00912742130458355 + 10.0 * 6.200184345245361
Epoch 2850, val loss: 0.9119582176208496
Epoch 2860, training loss: 61.989803314208984 = 0.009039757773280144 + 10.0 * 6.198076248168945
Epoch 2860, val loss: 0.9127642512321472
Epoch 2870, training loss: 62.00079345703125 = 0.008953348733484745 + 10.0 * 6.199183940887451
Epoch 2870, val loss: 0.9139419198036194
Epoch 2880, training loss: 62.042022705078125 = 0.008869561366736889 + 10.0 * 6.203315258026123
Epoch 2880, val loss: 0.9148111343383789
Epoch 2890, training loss: 62.00535202026367 = 0.008786981925368309 + 10.0 * 6.1996564865112305
Epoch 2890, val loss: 0.9157029390335083
Epoch 2900, training loss: 61.98772430419922 = 0.008704178966581821 + 10.0 * 6.197901725769043
Epoch 2900, val loss: 0.9166321158409119
Epoch 2910, training loss: 61.97880554199219 = 0.008624815382063389 + 10.0 * 6.197018146514893
Epoch 2910, val loss: 0.9177079200744629
Epoch 2920, training loss: 61.99013900756836 = 0.008549259975552559 + 10.0 * 6.198159217834473
Epoch 2920, val loss: 0.9186409115791321
Epoch 2930, training loss: 62.02154541015625 = 0.008471677079796791 + 10.0 * 6.20130729675293
Epoch 2930, val loss: 0.9195843935012817
Epoch 2940, training loss: 62.001800537109375 = 0.008388455957174301 + 10.0 * 6.199341297149658
Epoch 2940, val loss: 0.920475423336029
Epoch 2950, training loss: 61.987300872802734 = 0.008312840946018696 + 10.0 * 6.197898864746094
Epoch 2950, val loss: 0.9212887287139893
Epoch 2960, training loss: 61.98385238647461 = 0.008238423615694046 + 10.0 * 6.197561264038086
Epoch 2960, val loss: 0.9222986102104187
Epoch 2970, training loss: 62.000396728515625 = 0.008166216313838959 + 10.0 * 6.199223041534424
Epoch 2970, val loss: 0.9231730699539185
Epoch 2980, training loss: 61.978759765625 = 0.008094309829175472 + 10.0 * 6.197066307067871
Epoch 2980, val loss: 0.9240407943725586
Epoch 2990, training loss: 61.988990783691406 = 0.008024449460208416 + 10.0 * 6.19809627532959
Epoch 2990, val loss: 0.9249200820922852
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 87.91688537597656 = 1.9486831426620483 + 10.0 * 8.596819877624512
Epoch 0, val loss: 1.9517042636871338
Epoch 10, training loss: 87.89918518066406 = 1.9387333393096924 + 10.0 * 8.596044540405273
Epoch 10, val loss: 1.941961407661438
Epoch 20, training loss: 87.82792663574219 = 1.926439642906189 + 10.0 * 8.59014892578125
Epoch 20, val loss: 1.9295780658721924
Epoch 30, training loss: 87.41986083984375 = 1.9107521772384644 + 10.0 * 8.550910949707031
Epoch 30, val loss: 1.9136488437652588
Epoch 40, training loss: 85.23040008544922 = 1.8930898904800415 + 10.0 * 8.333730697631836
Epoch 40, val loss: 1.8962345123291016
Epoch 50, training loss: 78.77040100097656 = 1.873796820640564 + 10.0 * 7.689660549163818
Epoch 50, val loss: 1.8779367208480835
Epoch 60, training loss: 75.18109893798828 = 1.8597724437713623 + 10.0 * 7.332132816314697
Epoch 60, val loss: 1.8662046194076538
Epoch 70, training loss: 72.97588348388672 = 1.848620057106018 + 10.0 * 7.11272668838501
Epoch 70, val loss: 1.855547308921814
Epoch 80, training loss: 71.52125549316406 = 1.8363639116287231 + 10.0 * 6.968488693237305
Epoch 80, val loss: 1.8438405990600586
Epoch 90, training loss: 70.36517333984375 = 1.8252551555633545 + 10.0 * 6.853991508483887
Epoch 90, val loss: 1.8332195281982422
Epoch 100, training loss: 69.60474395751953 = 1.8149958848953247 + 10.0 * 6.778974533081055
Epoch 100, val loss: 1.8232070207595825
Epoch 110, training loss: 68.97113037109375 = 1.8051509857177734 + 10.0 * 6.716597557067871
Epoch 110, val loss: 1.8134666681289673
Epoch 120, training loss: 68.41390228271484 = 1.795915126800537 + 10.0 * 6.66179895401001
Epoch 120, val loss: 1.8044325113296509
Epoch 130, training loss: 67.9560775756836 = 1.7869929075241089 + 10.0 * 6.616908073425293
Epoch 130, val loss: 1.7959522008895874
Epoch 140, training loss: 67.59842681884766 = 1.7779334783554077 + 10.0 * 6.58204984664917
Epoch 140, val loss: 1.7872662544250488
Epoch 150, training loss: 67.32903289794922 = 1.7683353424072266 + 10.0 * 6.556069850921631
Epoch 150, val loss: 1.7781273126602173
Epoch 160, training loss: 67.05652618408203 = 1.7580161094665527 + 10.0 * 6.529850482940674
Epoch 160, val loss: 1.7686359882354736
Epoch 170, training loss: 66.86268615722656 = 1.7468644380569458 + 10.0 * 6.511582374572754
Epoch 170, val loss: 1.7587376832962036
Epoch 180, training loss: 66.63702392578125 = 1.7348283529281616 + 10.0 * 6.490219593048096
Epoch 180, val loss: 1.7480719089508057
Epoch 190, training loss: 66.44891357421875 = 1.7219518423080444 + 10.0 * 6.472696304321289
Epoch 190, val loss: 1.7368141412734985
Epoch 200, training loss: 66.304443359375 = 1.7079246044158936 + 10.0 * 6.459651947021484
Epoch 200, val loss: 1.7245936393737793
Epoch 210, training loss: 66.13920593261719 = 1.6924654245376587 + 10.0 * 6.444674015045166
Epoch 210, val loss: 1.7113158702850342
Epoch 220, training loss: 66.00145721435547 = 1.675661325454712 + 10.0 * 6.432579517364502
Epoch 220, val loss: 1.6969138383865356
Epoch 230, training loss: 65.93956756591797 = 1.657202959060669 + 10.0 * 6.42823600769043
Epoch 230, val loss: 1.6812044382095337
Epoch 240, training loss: 65.77889251708984 = 1.637245535850525 + 10.0 * 6.414165019989014
Epoch 240, val loss: 1.6640516519546509
Epoch 250, training loss: 65.65931701660156 = 1.6158303022384644 + 10.0 * 6.404348850250244
Epoch 250, val loss: 1.6457412242889404
Epoch 260, training loss: 65.55574798583984 = 1.5927722454071045 + 10.0 * 6.396297931671143
Epoch 260, val loss: 1.6260756254196167
Epoch 270, training loss: 65.46257019042969 = 1.5680897235870361 + 10.0 * 6.3894476890563965
Epoch 270, val loss: 1.6050207614898682
Epoch 280, training loss: 65.39942169189453 = 1.5418187379837036 + 10.0 * 6.385760307312012
Epoch 280, val loss: 1.5824682712554932
Epoch 290, training loss: 65.2768783569336 = 1.5140163898468018 + 10.0 * 6.376286029815674
Epoch 290, val loss: 1.5586975812911987
Epoch 300, training loss: 65.18582916259766 = 1.4848169088363647 + 10.0 * 6.370100975036621
Epoch 300, val loss: 1.5337648391723633
Epoch 310, training loss: 65.14643096923828 = 1.4543755054473877 + 10.0 * 6.369205474853516
Epoch 310, val loss: 1.5076783895492554
Epoch 320, training loss: 65.04048156738281 = 1.4227286577224731 + 10.0 * 6.3617753982543945
Epoch 320, val loss: 1.4809231758117676
Epoch 330, training loss: 64.94732666015625 = 1.3903695344924927 + 10.0 * 6.3556952476501465
Epoch 330, val loss: 1.4536125659942627
Epoch 340, training loss: 64.91714477539062 = 1.3573634624481201 + 10.0 * 6.355978012084961
Epoch 340, val loss: 1.4259333610534668
Epoch 350, training loss: 64.7965316772461 = 1.3238648176193237 + 10.0 * 6.347266674041748
Epoch 350, val loss: 1.3981252908706665
Epoch 360, training loss: 64.73456573486328 = 1.2901639938354492 + 10.0 * 6.34443998336792
Epoch 360, val loss: 1.3704851865768433
Epoch 370, training loss: 64.65611267089844 = 1.2564364671707153 + 10.0 * 6.339967727661133
Epoch 370, val loss: 1.3429265022277832
Epoch 380, training loss: 64.58990478515625 = 1.2229225635528564 + 10.0 * 6.336698055267334
Epoch 380, val loss: 1.3160971403121948
Epoch 390, training loss: 64.5206069946289 = 1.1898472309112549 + 10.0 * 6.333075523376465
Epoch 390, val loss: 1.2898337841033936
Epoch 400, training loss: 64.4703369140625 = 1.1571149826049805 + 10.0 * 6.33132266998291
Epoch 400, val loss: 1.2645467519760132
Epoch 410, training loss: 64.43473052978516 = 1.1249685287475586 + 10.0 * 6.3309760093688965
Epoch 410, val loss: 1.2398680448532104
Epoch 420, training loss: 64.34319305419922 = 1.0935478210449219 + 10.0 * 6.32496452331543
Epoch 420, val loss: 1.2164782285690308
Epoch 430, training loss: 64.2750244140625 = 1.0629597902297974 + 10.0 * 6.321206569671631
Epoch 430, val loss: 1.1942732334136963
Epoch 440, training loss: 64.23584747314453 = 1.033480167388916 + 10.0 * 6.320237159729004
Epoch 440, val loss: 1.1735188961029053
Epoch 450, training loss: 64.21590423583984 = 1.0049940347671509 + 10.0 * 6.321091175079346
Epoch 450, val loss: 1.1537355184555054
Epoch 460, training loss: 64.12470245361328 = 0.9777235388755798 + 10.0 * 6.314698219299316
Epoch 460, val loss: 1.13569974899292
Epoch 470, training loss: 64.06709289550781 = 0.9515896439552307 + 10.0 * 6.311550140380859
Epoch 470, val loss: 1.1190050840377808
Epoch 480, training loss: 64.04137420654297 = 0.9265298247337341 + 10.0 * 6.311484336853027
Epoch 480, val loss: 1.1036977767944336
Epoch 490, training loss: 63.98332977294922 = 0.9023127555847168 + 10.0 * 6.308101654052734
Epoch 490, val loss: 1.0892786979675293
Epoch 500, training loss: 63.93342971801758 = 0.879185676574707 + 10.0 * 6.305424690246582
Epoch 500, val loss: 1.076242208480835
Epoch 510, training loss: 63.890567779541016 = 0.8568928241729736 + 10.0 * 6.303367614746094
Epoch 510, val loss: 1.064144492149353
Epoch 520, training loss: 63.86772537231445 = 0.835378885269165 + 10.0 * 6.303234577178955
Epoch 520, val loss: 1.0529001951217651
Epoch 530, training loss: 63.805362701416016 = 0.8145729303359985 + 10.0 * 6.299078941345215
Epoch 530, val loss: 1.0427169799804688
Epoch 540, training loss: 63.77117156982422 = 0.7945486307144165 + 10.0 * 6.297662258148193
Epoch 540, val loss: 1.0332355499267578
Epoch 550, training loss: 63.774139404296875 = 0.775213360786438 + 10.0 * 6.299892425537109
Epoch 550, val loss: 1.0244747400283813
Epoch 560, training loss: 63.71042251586914 = 0.7563725113868713 + 10.0 * 6.29540491104126
Epoch 560, val loss: 1.016387701034546
Epoch 570, training loss: 63.67380905151367 = 0.7382107377052307 + 10.0 * 6.293559551239014
Epoch 570, val loss: 1.0089434385299683
Epoch 580, training loss: 63.63395690917969 = 0.7205994129180908 + 10.0 * 6.291335582733154
Epoch 580, val loss: 1.0021427869796753
Epoch 590, training loss: 63.60187911987305 = 0.7034968733787537 + 10.0 * 6.2898383140563965
Epoch 590, val loss: 0.995961606502533
Epoch 600, training loss: 63.590885162353516 = 0.686830997467041 + 10.0 * 6.2904052734375
Epoch 600, val loss: 0.990348756313324
Epoch 610, training loss: 63.54913330078125 = 0.6707195043563843 + 10.0 * 6.287841320037842
Epoch 610, val loss: 0.98507159948349
Epoch 620, training loss: 63.51191711425781 = 0.6550726294517517 + 10.0 * 6.285684585571289
Epoch 620, val loss: 0.9804239869117737
Epoch 630, training loss: 63.48885726928711 = 0.639853298664093 + 10.0 * 6.284900188446045
Epoch 630, val loss: 0.9762888550758362
Epoch 640, training loss: 63.45217514038086 = 0.6250572800636292 + 10.0 * 6.282711982727051
Epoch 640, val loss: 0.9726048111915588
Epoch 650, training loss: 63.41925811767578 = 0.6106686592102051 + 10.0 * 6.280858993530273
Epoch 650, val loss: 0.969268798828125
Epoch 660, training loss: 63.38945007324219 = 0.5966745615005493 + 10.0 * 6.279277324676514
Epoch 660, val loss: 0.9666035771369934
Epoch 670, training loss: 63.3914794921875 = 0.5830051302909851 + 10.0 * 6.280847549438477
Epoch 670, val loss: 0.9642736315727234
Epoch 680, training loss: 63.349769592285156 = 0.5695850849151611 + 10.0 * 6.278018474578857
Epoch 680, val loss: 0.9623249173164368
Epoch 690, training loss: 63.326534271240234 = 0.5564291477203369 + 10.0 * 6.277010440826416
Epoch 690, val loss: 0.9607235193252563
Epoch 700, training loss: 63.32359313964844 = 0.543536901473999 + 10.0 * 6.278005599975586
Epoch 700, val loss: 0.9594664573669434
Epoch 710, training loss: 63.273799896240234 = 0.5309116244316101 + 10.0 * 6.274289131164551
Epoch 710, val loss: 0.9586393237113953
Epoch 720, training loss: 63.242950439453125 = 0.5185182690620422 + 10.0 * 6.2724432945251465
Epoch 720, val loss: 0.9581629037857056
Epoch 730, training loss: 63.216224670410156 = 0.5062787532806396 + 10.0 * 6.270994663238525
Epoch 730, val loss: 0.9580601453781128
Epoch 740, training loss: 63.235450744628906 = 0.49415123462677 + 10.0 * 6.274129867553711
Epoch 740, val loss: 0.9582309126853943
Epoch 750, training loss: 63.21903610229492 = 0.4819716215133667 + 10.0 * 6.273706436157227
Epoch 750, val loss: 0.9584296345710754
Epoch 760, training loss: 63.15264129638672 = 0.4699512720108032 + 10.0 * 6.268269062042236
Epoch 760, val loss: 0.9590303897857666
Epoch 770, training loss: 63.12953567504883 = 0.4580284059047699 + 10.0 * 6.26715087890625
Epoch 770, val loss: 0.9599058032035828
Epoch 780, training loss: 63.105472564697266 = 0.44613900780677795 + 10.0 * 6.265933036804199
Epoch 780, val loss: 0.9610710740089417
Epoch 790, training loss: 63.10172653198242 = 0.4342591166496277 + 10.0 * 6.266746997833252
Epoch 790, val loss: 0.962376594543457
Epoch 800, training loss: 63.09719467163086 = 0.4223094880580902 + 10.0 * 6.267488479614258
Epoch 800, val loss: 0.9637086987495422
Epoch 810, training loss: 63.04846954345703 = 0.4104401469230652 + 10.0 * 6.263803005218506
Epoch 810, val loss: 0.9653129577636719
Epoch 820, training loss: 63.026493072509766 = 0.39867669343948364 + 10.0 * 6.262781620025635
Epoch 820, val loss: 0.9673193097114563
Epoch 830, training loss: 63.006805419921875 = 0.387015163898468 + 10.0 * 6.261979103088379
Epoch 830, val loss: 0.9695820212364197
Epoch 840, training loss: 63.10871124267578 = 0.37547144293785095 + 10.0 * 6.273324012756348
Epoch 840, val loss: 0.9719899296760559
Epoch 850, training loss: 62.983158111572266 = 0.3638818860054016 + 10.0 * 6.261927604675293
Epoch 850, val loss: 0.9744312167167664
Epoch 860, training loss: 62.958290100097656 = 0.35256874561309814 + 10.0 * 6.2605719566345215
Epoch 860, val loss: 0.9772000908851624
Epoch 870, training loss: 62.92827606201172 = 0.3414938449859619 + 10.0 * 6.258677959442139
Epoch 870, val loss: 0.9803410172462463
Epoch 880, training loss: 62.94121551513672 = 0.3306080400943756 + 10.0 * 6.26106071472168
Epoch 880, val loss: 0.983744204044342
Epoch 890, training loss: 62.890655517578125 = 0.31989631056785583 + 10.0 * 6.257075786590576
Epoch 890, val loss: 0.9872133731842041
Epoch 900, training loss: 62.87285614013672 = 0.30948057770729065 + 10.0 * 6.256337642669678
Epoch 900, val loss: 0.9908915162086487
Epoch 910, training loss: 62.85739517211914 = 0.29933473467826843 + 10.0 * 6.255805969238281
Epoch 910, val loss: 0.99488765001297
Epoch 920, training loss: 62.97665786743164 = 0.2894165515899658 + 10.0 * 6.268723964691162
Epoch 920, val loss: 0.9988738894462585
Epoch 930, training loss: 62.83233642578125 = 0.2798137962818146 + 10.0 * 6.255251884460449
Epoch 930, val loss: 1.0031356811523438
Epoch 940, training loss: 62.80686569213867 = 0.27053117752075195 + 10.0 * 6.253633499145508
Epoch 940, val loss: 1.0076767206192017
Epoch 950, training loss: 62.792640686035156 = 0.2615976929664612 + 10.0 * 6.253104209899902
Epoch 950, val loss: 1.0125484466552734
Epoch 960, training loss: 62.77188491821289 = 0.25294625759124756 + 10.0 * 6.251893997192383
Epoch 960, val loss: 1.0175142288208008
Epoch 970, training loss: 62.75919723510742 = 0.24457131326198578 + 10.0 * 6.251462459564209
Epoch 970, val loss: 1.022653579711914
Epoch 980, training loss: 62.881683349609375 = 0.2364642471075058 + 10.0 * 6.264521598815918
Epoch 980, val loss: 1.0277931690216064
Epoch 990, training loss: 62.732948303222656 = 0.2285827249288559 + 10.0 * 6.250436305999756
Epoch 990, val loss: 1.0328923463821411
Epoch 1000, training loss: 62.72140121459961 = 0.22106324136257172 + 10.0 * 6.250033855438232
Epoch 1000, val loss: 1.0384467840194702
Epoch 1010, training loss: 62.707340240478516 = 0.21386495232582092 + 10.0 * 6.249347686767578
Epoch 1010, val loss: 1.0442620515823364
Epoch 1020, training loss: 62.68876266479492 = 0.20692428946495056 + 10.0 * 6.248183727264404
Epoch 1020, val loss: 1.050355076789856
Epoch 1030, training loss: 62.75946044921875 = 0.20022593438625336 + 10.0 * 6.255923271179199
Epoch 1030, val loss: 1.056374192237854
Epoch 1040, training loss: 62.71232223510742 = 0.19372014701366425 + 10.0 * 6.25186014175415
Epoch 1040, val loss: 1.0625050067901611
Epoch 1050, training loss: 62.65355682373047 = 0.18746480345726013 + 10.0 * 6.246609210968018
Epoch 1050, val loss: 1.0687954425811768
Epoch 1060, training loss: 62.64594268798828 = 0.18148718774318695 + 10.0 * 6.246445655822754
Epoch 1060, val loss: 1.0752604007720947
Epoch 1070, training loss: 62.67908477783203 = 0.17573148012161255 + 10.0 * 6.250335216522217
Epoch 1070, val loss: 1.081840991973877
Epoch 1080, training loss: 62.6603889465332 = 0.17015603184700012 + 10.0 * 6.2490234375
Epoch 1080, val loss: 1.0886036157608032
Epoch 1090, training loss: 62.620731353759766 = 0.1647745817899704 + 10.0 * 6.245595455169678
Epoch 1090, val loss: 1.095133900642395
Epoch 1100, training loss: 62.598445892333984 = 0.15963681042194366 + 10.0 * 6.243880748748779
Epoch 1100, val loss: 1.1021336317062378
Epoch 1110, training loss: 62.58832550048828 = 0.15468287467956543 + 10.0 * 6.243364334106445
Epoch 1110, val loss: 1.1091355085372925
Epoch 1120, training loss: 62.6211051940918 = 0.1499047428369522 + 10.0 * 6.247119903564453
Epoch 1120, val loss: 1.1161061525344849
Epoch 1130, training loss: 62.58059310913086 = 0.14526088535785675 + 10.0 * 6.243533134460449
Epoch 1130, val loss: 1.1232686042785645
Epoch 1140, training loss: 62.61233901977539 = 0.14079906046390533 + 10.0 * 6.2471537590026855
Epoch 1140, val loss: 1.130314826965332
Epoch 1150, training loss: 62.54982376098633 = 0.13648998737335205 + 10.0 * 6.241333484649658
Epoch 1150, val loss: 1.1376497745513916
Epoch 1160, training loss: 62.53398132324219 = 0.13234460353851318 + 10.0 * 6.240163803100586
Epoch 1160, val loss: 1.1450434923171997
Epoch 1170, training loss: 62.523048400878906 = 0.12837354838848114 + 10.0 * 6.239467620849609
Epoch 1170, val loss: 1.1525808572769165
Epoch 1180, training loss: 62.528099060058594 = 0.12454260140657425 + 10.0 * 6.240355491638184
Epoch 1180, val loss: 1.1601072549819946
Epoch 1190, training loss: 62.57915496826172 = 0.12081539630889893 + 10.0 * 6.245833873748779
Epoch 1190, val loss: 1.1673460006713867
Epoch 1200, training loss: 62.52007293701172 = 0.11721011251211166 + 10.0 * 6.240286350250244
Epoch 1200, val loss: 1.1750164031982422
Epoch 1210, training loss: 62.49147415161133 = 0.11377124488353729 + 10.0 * 6.2377705574035645
Epoch 1210, val loss: 1.1826194524765015
Epoch 1220, training loss: 62.480140686035156 = 0.11047080904245377 + 10.0 * 6.236967086791992
Epoch 1220, val loss: 1.1904512643814087
Epoch 1230, training loss: 62.47581100463867 = 0.10728244483470917 + 10.0 * 6.236853122711182
Epoch 1230, val loss: 1.1982402801513672
Epoch 1240, training loss: 62.55744934082031 = 0.10420285910367966 + 10.0 * 6.245324611663818
Epoch 1240, val loss: 1.2059168815612793
Epoch 1250, training loss: 62.5034065246582 = 0.1011846736073494 + 10.0 * 6.240221977233887
Epoch 1250, val loss: 1.213517665863037
Epoch 1260, training loss: 62.47440719604492 = 0.09828943759202957 + 10.0 * 6.237611770629883
Epoch 1260, val loss: 1.2212070226669312
Epoch 1270, training loss: 62.456764221191406 = 0.09552164375782013 + 10.0 * 6.236124515533447
Epoch 1270, val loss: 1.2291167974472046
Epoch 1280, training loss: 62.43741226196289 = 0.09285768866539001 + 10.0 * 6.234455585479736
Epoch 1280, val loss: 1.2370132207870483
Epoch 1290, training loss: 62.443824768066406 = 0.09029056131839752 + 10.0 * 6.235353469848633
Epoch 1290, val loss: 1.2448699474334717
Epoch 1300, training loss: 62.451419830322266 = 0.08779385685920715 + 10.0 * 6.236362457275391
Epoch 1300, val loss: 1.2525173425674438
Epoch 1310, training loss: 62.43069076538086 = 0.08537142723798752 + 10.0 * 6.234531879425049
Epoch 1310, val loss: 1.260240912437439
Epoch 1320, training loss: 62.423492431640625 = 0.08304590731859207 + 10.0 * 6.234044551849365
Epoch 1320, val loss: 1.2681481838226318
Epoch 1330, training loss: 62.473201751708984 = 0.0808182805776596 + 10.0 * 6.239238262176514
Epoch 1330, val loss: 1.2758129835128784
Epoch 1340, training loss: 62.417118072509766 = 0.07861888408660889 + 10.0 * 6.233850002288818
Epoch 1340, val loss: 1.2832434177398682
Epoch 1350, training loss: 62.39443588256836 = 0.0765310525894165 + 10.0 * 6.231790542602539
Epoch 1350, val loss: 1.2909671068191528
Epoch 1360, training loss: 62.389892578125 = 0.07452145963907242 + 10.0 * 6.231537342071533
Epoch 1360, val loss: 1.2986390590667725
Epoch 1370, training loss: 62.43290328979492 = 0.07257508486509323 + 10.0 * 6.236032962799072
Epoch 1370, val loss: 1.3060543537139893
Epoch 1380, training loss: 62.386173248291016 = 0.07066665589809418 + 10.0 * 6.231550693511963
Epoch 1380, val loss: 1.3136271238327026
Epoch 1390, training loss: 62.371883392333984 = 0.06884102523326874 + 10.0 * 6.23030424118042
Epoch 1390, val loss: 1.3209900856018066
Epoch 1400, training loss: 62.390621185302734 = 0.06708154082298279 + 10.0 * 6.232354164123535
Epoch 1400, val loss: 1.328577995300293
Epoch 1410, training loss: 62.390750885009766 = 0.06536557525396347 + 10.0 * 6.23253870010376
Epoch 1410, val loss: 1.3358259201049805
Epoch 1420, training loss: 62.3631706237793 = 0.06371256709098816 + 10.0 * 6.229945659637451
Epoch 1420, val loss: 1.3431209325790405
Epoch 1430, training loss: 62.35106658935547 = 0.062119439244270325 + 10.0 * 6.2288947105407715
Epoch 1430, val loss: 1.3504469394683838
Epoch 1440, training loss: 62.34307098388672 = 0.06058468297123909 + 10.0 * 6.228248596191406
Epoch 1440, val loss: 1.3577890396118164
Epoch 1450, training loss: 62.390525817871094 = 0.059110864996910095 + 10.0 * 6.2331414222717285
Epoch 1450, val loss: 1.365087866783142
Epoch 1460, training loss: 62.35341262817383 = 0.05764804407954216 + 10.0 * 6.229576587677002
Epoch 1460, val loss: 1.3718798160552979
Epoch 1470, training loss: 62.33833694458008 = 0.05623994022607803 + 10.0 * 6.228209495544434
Epoch 1470, val loss: 1.379080057144165
Epoch 1480, training loss: 62.32299041748047 = 0.05488947033882141 + 10.0 * 6.226809978485107
Epoch 1480, val loss: 1.386091947555542
Epoch 1490, training loss: 62.31623458862305 = 0.05359005182981491 + 10.0 * 6.226264476776123
Epoch 1490, val loss: 1.3932186365127563
Epoch 1500, training loss: 62.425960540771484 = 0.052335191518068314 + 10.0 * 6.237362861633301
Epoch 1500, val loss: 1.400018572807312
Epoch 1510, training loss: 62.377201080322266 = 0.05107905715703964 + 10.0 * 6.232612133026123
Epoch 1510, val loss: 1.4068220853805542
Epoch 1520, training loss: 62.30180740356445 = 0.04987310245633125 + 10.0 * 6.225193500518799
Epoch 1520, val loss: 1.4136286973953247
Epoch 1530, training loss: 62.3034782409668 = 0.04872762784361839 + 10.0 * 6.225474834442139
Epoch 1530, val loss: 1.4204537868499756
Epoch 1540, training loss: 62.29934310913086 = 0.047623272985219955 + 10.0 * 6.22517204284668
Epoch 1540, val loss: 1.4272414445877075
Epoch 1550, training loss: 62.33510208129883 = 0.046544600278139114 + 10.0 * 6.228856086730957
Epoch 1550, val loss: 1.4337671995162964
Epoch 1560, training loss: 62.30656051635742 = 0.04549562558531761 + 10.0 * 6.226106643676758
Epoch 1560, val loss: 1.4405421018600464
Epoch 1570, training loss: 62.293331146240234 = 0.04447675123810768 + 10.0 * 6.2248854637146
Epoch 1570, val loss: 1.446996808052063
Epoch 1580, training loss: 62.28834533691406 = 0.04349292814731598 + 10.0 * 6.224485397338867
Epoch 1580, val loss: 1.4535342454910278
Epoch 1590, training loss: 62.276161193847656 = 0.042535070329904556 + 10.0 * 6.223362922668457
Epoch 1590, val loss: 1.4600627422332764
Epoch 1600, training loss: 62.30692672729492 = 0.04161172732710838 + 10.0 * 6.226531505584717
Epoch 1600, val loss: 1.466366171836853
Epoch 1610, training loss: 62.278289794921875 = 0.04070748761296272 + 10.0 * 6.223758220672607
Epoch 1610, val loss: 1.4727662801742554
Epoch 1620, training loss: 62.26265335083008 = 0.03982971981167793 + 10.0 * 6.222282409667969
Epoch 1620, val loss: 1.4790757894515991
Epoch 1630, training loss: 62.310455322265625 = 0.038987837731838226 + 10.0 * 6.227147102355957
Epoch 1630, val loss: 1.4852948188781738
Epoch 1640, training loss: 62.27789306640625 = 0.03815330192446709 + 10.0 * 6.223973751068115
Epoch 1640, val loss: 1.4914711713790894
Epoch 1650, training loss: 62.26881408691406 = 0.037349455058574677 + 10.0 * 6.223146438598633
Epoch 1650, val loss: 1.4976271390914917
Epoch 1660, training loss: 62.250370025634766 = 0.03656810149550438 + 10.0 * 6.221380233764648
Epoch 1660, val loss: 1.503630518913269
Epoch 1670, training loss: 62.24713134765625 = 0.03582024201750755 + 10.0 * 6.221131324768066
Epoch 1670, val loss: 1.5097737312316895
Epoch 1680, training loss: 62.272647857666016 = 0.03509100526571274 + 10.0 * 6.223755836486816
Epoch 1680, val loss: 1.515718698501587
Epoch 1690, training loss: 62.24788284301758 = 0.034374937415122986 + 10.0 * 6.22135066986084
Epoch 1690, val loss: 1.5217745304107666
Epoch 1700, training loss: 62.27193832397461 = 0.033678144216537476 + 10.0 * 6.223825931549072
Epoch 1700, val loss: 1.5274971723556519
Epoch 1710, training loss: 62.23480224609375 = 0.032994646579027176 + 10.0 * 6.220180988311768
Epoch 1710, val loss: 1.5334253311157227
Epoch 1720, training loss: 62.225521087646484 = 0.03234515339136124 + 10.0 * 6.219317436218262
Epoch 1720, val loss: 1.5393680334091187
Epoch 1730, training loss: 62.22549057006836 = 0.031714875251054764 + 10.0 * 6.219377517700195
Epoch 1730, val loss: 1.5451821088790894
Epoch 1740, training loss: 62.259002685546875 = 0.031104883179068565 + 10.0 * 6.222789764404297
Epoch 1740, val loss: 1.5508122444152832
Epoch 1750, training loss: 62.23588943481445 = 0.030490366742014885 + 10.0 * 6.2205400466918945
Epoch 1750, val loss: 1.5564535856246948
Epoch 1760, training loss: 62.2194938659668 = 0.029902128502726555 + 10.0 * 6.218958854675293
Epoch 1760, val loss: 1.5620901584625244
Epoch 1770, training loss: 62.21409606933594 = 0.029334282502532005 + 10.0 * 6.218476295471191
Epoch 1770, val loss: 1.5678454637527466
Epoch 1780, training loss: 62.26591491699219 = 0.02878512255847454 + 10.0 * 6.223712921142578
Epoch 1780, val loss: 1.5733692646026611
Epoch 1790, training loss: 62.2685432434082 = 0.028233636170625687 + 10.0 * 6.2240309715271
Epoch 1790, val loss: 1.5783331394195557
Epoch 1800, training loss: 62.22366714477539 = 0.027702709659934044 + 10.0 * 6.2195963859558105
Epoch 1800, val loss: 1.5842901468276978
Epoch 1810, training loss: 62.20279312133789 = 0.027188491076231003 + 10.0 * 6.217560768127441
Epoch 1810, val loss: 1.5895841121673584
Epoch 1820, training loss: 62.19476318359375 = 0.026694446802139282 + 10.0 * 6.216806888580322
Epoch 1820, val loss: 1.595152735710144
Epoch 1830, training loss: 62.2055549621582 = 0.02621416561305523 + 10.0 * 6.2179341316223145
Epoch 1830, val loss: 1.600520133972168
Epoch 1840, training loss: 62.226959228515625 = 0.025740576907992363 + 10.0 * 6.22012186050415
Epoch 1840, val loss: 1.6058111190795898
Epoch 1850, training loss: 62.21403121948242 = 0.025275584310293198 + 10.0 * 6.218875408172607
Epoch 1850, val loss: 1.6110650300979614
Epoch 1860, training loss: 62.19936752319336 = 0.024821709841489792 + 10.0 * 6.217454433441162
Epoch 1860, val loss: 1.6162508726119995
Epoch 1870, training loss: 62.19047927856445 = 0.024385489523410797 + 10.0 * 6.216609001159668
Epoch 1870, val loss: 1.6215018033981323
Epoch 1880, training loss: 62.18281555175781 = 0.023959768936038017 + 10.0 * 6.215885639190674
Epoch 1880, val loss: 1.6268346309661865
Epoch 1890, training loss: 62.214637756347656 = 0.023546958342194557 + 10.0 * 6.219109058380127
Epoch 1890, val loss: 1.631886601448059
Epoch 1900, training loss: 62.19114303588867 = 0.023132435977458954 + 10.0 * 6.216801166534424
Epoch 1900, val loss: 1.6366914510726929
Epoch 1910, training loss: 62.17061996459961 = 0.022732459008693695 + 10.0 * 6.214788913726807
Epoch 1910, val loss: 1.6418159008026123
Epoch 1920, training loss: 62.18009567260742 = 0.022349845618009567 + 10.0 * 6.2157745361328125
Epoch 1920, val loss: 1.6467841863632202
Epoch 1930, training loss: 62.19711685180664 = 0.021972768008708954 + 10.0 * 6.217514514923096
Epoch 1930, val loss: 1.651565670967102
Epoch 1940, training loss: 62.173282623291016 = 0.02160400152206421 + 10.0 * 6.215167999267578
Epoch 1940, val loss: 1.6565481424331665
Epoch 1950, training loss: 62.17660903930664 = 0.021243838593363762 + 10.0 * 6.215536594390869
Epoch 1950, val loss: 1.6613131761550903
Epoch 1960, training loss: 62.17470932006836 = 0.020894858986139297 + 10.0 * 6.215381622314453
Epoch 1960, val loss: 1.6662629842758179
Epoch 1970, training loss: 62.176815032958984 = 0.020551245659589767 + 10.0 * 6.2156267166137695
Epoch 1970, val loss: 1.6708415746688843
Epoch 1980, training loss: 62.17041015625 = 0.020215844735503197 + 10.0 * 6.215019702911377
Epoch 1980, val loss: 1.6754722595214844
Epoch 1990, training loss: 62.16193389892578 = 0.019887180998921394 + 10.0 * 6.214204788208008
Epoch 1990, val loss: 1.6803420782089233
Epoch 2000, training loss: 62.16755676269531 = 0.01957048289477825 + 10.0 * 6.214798927307129
Epoch 2000, val loss: 1.6847352981567383
Epoch 2010, training loss: 62.16238784790039 = 0.019261157140135765 + 10.0 * 6.214312553405762
Epoch 2010, val loss: 1.6895456314086914
Epoch 2020, training loss: 62.1463737487793 = 0.01895628124475479 + 10.0 * 6.212741851806641
Epoch 2020, val loss: 1.694201946258545
Epoch 2030, training loss: 62.170989990234375 = 0.018663667142391205 + 10.0 * 6.2152323722839355
Epoch 2030, val loss: 1.6987600326538086
Epoch 2040, training loss: 62.160282135009766 = 0.01836904138326645 + 10.0 * 6.214191436767578
Epoch 2040, val loss: 1.7029590606689453
Epoch 2050, training loss: 62.16184997558594 = 0.018084902316331863 + 10.0 * 6.214376449584961
Epoch 2050, val loss: 1.7074007987976074
Epoch 2060, training loss: 62.13813781738281 = 0.017800217494368553 + 10.0 * 6.212033748626709
Epoch 2060, val loss: 1.7117486000061035
Epoch 2070, training loss: 62.129764556884766 = 0.01753155142068863 + 10.0 * 6.211223125457764
Epoch 2070, val loss: 1.7163734436035156
Epoch 2080, training loss: 62.132286071777344 = 0.017270857468247414 + 10.0 * 6.211501598358154
Epoch 2080, val loss: 1.7206718921661377
Epoch 2090, training loss: 62.183528900146484 = 0.01701761595904827 + 10.0 * 6.21665096282959
Epoch 2090, val loss: 1.725005030632019
Epoch 2100, training loss: 62.14266586303711 = 0.01675489917397499 + 10.0 * 6.212591171264648
Epoch 2100, val loss: 1.7291874885559082
Epoch 2110, training loss: 62.16615295410156 = 0.016512997448444366 + 10.0 * 6.214963912963867
Epoch 2110, val loss: 1.7334692478179932
Epoch 2120, training loss: 62.13037109375 = 0.01626402698457241 + 10.0 * 6.2114105224609375
Epoch 2120, val loss: 1.7374242544174194
Epoch 2130, training loss: 62.154296875 = 0.01602793298661709 + 10.0 * 6.213826656341553
Epoch 2130, val loss: 1.7416458129882812
Epoch 2140, training loss: 62.127197265625 = 0.015793107450008392 + 10.0 * 6.2111406326293945
Epoch 2140, val loss: 1.7457351684570312
Epoch 2150, training loss: 62.11677169799805 = 0.015566818416118622 + 10.0 * 6.21012020111084
Epoch 2150, val loss: 1.7498384714126587
Epoch 2160, training loss: 62.115142822265625 = 0.015348432585597038 + 10.0 * 6.20997953414917
Epoch 2160, val loss: 1.753920316696167
Epoch 2170, training loss: 62.132484436035156 = 0.015134141780436039 + 10.0 * 6.211735248565674
Epoch 2170, val loss: 1.7578904628753662
Epoch 2180, training loss: 62.161190032958984 = 0.014919108711183071 + 10.0 * 6.214627265930176
Epoch 2180, val loss: 1.7618247270584106
Epoch 2190, training loss: 62.112632751464844 = 0.014708314090967178 + 10.0 * 6.209792137145996
Epoch 2190, val loss: 1.7657508850097656
Epoch 2200, training loss: 62.10098648071289 = 0.014502319507300854 + 10.0 * 6.208648204803467
Epoch 2200, val loss: 1.769681453704834
Epoch 2210, training loss: 62.09775924682617 = 0.01430569402873516 + 10.0 * 6.208345413208008
Epoch 2210, val loss: 1.7736937999725342
Epoch 2220, training loss: 62.11051940917969 = 0.014115397818386555 + 10.0 * 6.2096405029296875
Epoch 2220, val loss: 1.7775710821151733
Epoch 2230, training loss: 62.12480926513672 = 0.013922781683504581 + 10.0 * 6.21108865737915
Epoch 2230, val loss: 1.7811954021453857
Epoch 2240, training loss: 62.12936782836914 = 0.013733616098761559 + 10.0 * 6.211563587188721
Epoch 2240, val loss: 1.7849476337432861
Epoch 2250, training loss: 62.13972854614258 = 0.013549774885177612 + 10.0 * 6.212617874145508
Epoch 2250, val loss: 1.7886230945587158
Epoch 2260, training loss: 62.10147476196289 = 0.013361734338104725 + 10.0 * 6.208811283111572
Epoch 2260, val loss: 1.7922329902648926
Epoch 2270, training loss: 62.092041015625 = 0.013188663870096207 + 10.0 * 6.207885265350342
Epoch 2270, val loss: 1.795946717262268
Epoch 2280, training loss: 62.097591400146484 = 0.013018937781453133 + 10.0 * 6.208456993103027
Epoch 2280, val loss: 1.7996124029159546
Epoch 2290, training loss: 62.113895416259766 = 0.012851065956056118 + 10.0 * 6.210104465484619
Epoch 2290, val loss: 1.8030877113342285
Epoch 2300, training loss: 62.08905792236328 = 0.012680789455771446 + 10.0 * 6.207637786865234
Epoch 2300, val loss: 1.8069294691085815
Epoch 2310, training loss: 62.08847427368164 = 0.01251982431858778 + 10.0 * 6.207595348358154
Epoch 2310, val loss: 1.8104968070983887
Epoch 2320, training loss: 62.13695526123047 = 0.012363001704216003 + 10.0 * 6.212459087371826
Epoch 2320, val loss: 1.8139463663101196
Epoch 2330, training loss: 62.09824752807617 = 0.012199600227177143 + 10.0 * 6.20860481262207
Epoch 2330, val loss: 1.8175376653671265
Epoch 2340, training loss: 62.08045196533203 = 0.012047162279486656 + 10.0 * 6.206840515136719
Epoch 2340, val loss: 1.8209236860275269
Epoch 2350, training loss: 62.07958221435547 = 0.011897374875843525 + 10.0 * 6.20676851272583
Epoch 2350, val loss: 1.8245326280593872
Epoch 2360, training loss: 62.136993408203125 = 0.011753362603485584 + 10.0 * 6.212523937225342
Epoch 2360, val loss: 1.827823519706726
Epoch 2370, training loss: 62.115638732910156 = 0.011603573337197304 + 10.0 * 6.2104034423828125
Epoch 2370, val loss: 1.830833077430725
Epoch 2380, training loss: 62.080379486083984 = 0.011456427164375782 + 10.0 * 6.206892490386963
Epoch 2380, val loss: 1.8345376253128052
Epoch 2390, training loss: 62.06534194946289 = 0.011319748125970364 + 10.0 * 6.205402374267578
Epoch 2390, val loss: 1.8378475904464722
Epoch 2400, training loss: 62.060943603515625 = 0.011185231618583202 + 10.0 * 6.2049760818481445
Epoch 2400, val loss: 1.8413653373718262
Epoch 2410, training loss: 62.08592987060547 = 0.011055274866521358 + 10.0 * 6.2074875831604
Epoch 2410, val loss: 1.844616174697876
Epoch 2420, training loss: 62.08444595336914 = 0.010919722728431225 + 10.0 * 6.207352638244629
Epoch 2420, val loss: 1.8475290536880493
Epoch 2430, training loss: 62.078765869140625 = 0.010786805301904678 + 10.0 * 6.2067975997924805
Epoch 2430, val loss: 1.8506134748458862
Epoch 2440, training loss: 62.06290817260742 = 0.010658426210284233 + 10.0 * 6.205224990844727
Epoch 2440, val loss: 1.8540841341018677
Epoch 2450, training loss: 62.05740737915039 = 0.01053716242313385 + 10.0 * 6.204687118530273
Epoch 2450, val loss: 1.8573124408721924
Epoch 2460, training loss: 62.06379318237305 = 0.010417248122394085 + 10.0 * 6.2053375244140625
Epoch 2460, val loss: 1.8605574369430542
Epoch 2470, training loss: 62.09642028808594 = 0.01030044723302126 + 10.0 * 6.208611965179443
Epoch 2470, val loss: 1.8636484146118164
Epoch 2480, training loss: 62.05328369140625 = 0.010177869349718094 + 10.0 * 6.204310417175293
Epoch 2480, val loss: 1.8667134046554565
Epoch 2490, training loss: 62.06747817993164 = 0.010064215399324894 + 10.0 * 6.2057414054870605
Epoch 2490, val loss: 1.8696507215499878
Epoch 2500, training loss: 62.0837287902832 = 0.009950057603418827 + 10.0 * 6.207377910614014
Epoch 2500, val loss: 1.8727601766586304
Epoch 2510, training loss: 62.115089416503906 = 0.009835540316998959 + 10.0 * 6.2105255126953125
Epoch 2510, val loss: 1.8757418394088745
Epoch 2520, training loss: 62.0557746887207 = 0.00972202979028225 + 10.0 * 6.2046051025390625
Epoch 2520, val loss: 1.8785427808761597
Epoch 2530, training loss: 62.043128967285156 = 0.00961215328425169 + 10.0 * 6.2033514976501465
Epoch 2530, val loss: 1.8816903829574585
Epoch 2540, training loss: 62.04239273071289 = 0.00950874574482441 + 10.0 * 6.2032880783081055
Epoch 2540, val loss: 1.88479483127594
Epoch 2550, training loss: 62.03984832763672 = 0.009406685829162598 + 10.0 * 6.2030439376831055
Epoch 2550, val loss: 1.8877999782562256
Epoch 2560, training loss: 62.084354400634766 = 0.00930686667561531 + 10.0 * 6.207504749298096
Epoch 2560, val loss: 1.8906205892562866
Epoch 2570, training loss: 62.04115676879883 = 0.009202564135193825 + 10.0 * 6.203195571899414
Epoch 2570, val loss: 1.8934192657470703
Epoch 2580, training loss: 62.04460525512695 = 0.009101461619138718 + 10.0 * 6.203550338745117
Epoch 2580, val loss: 1.8963161706924438
Epoch 2590, training loss: 62.055809020996094 = 0.009004388004541397 + 10.0 * 6.204680442810059
Epoch 2590, val loss: 1.8991328477859497
Epoch 2600, training loss: 62.05507278442383 = 0.008908625692129135 + 10.0 * 6.204616546630859
Epoch 2600, val loss: 1.9020462036132812
Epoch 2610, training loss: 62.0511589050293 = 0.008814717642962933 + 10.0 * 6.2042341232299805
Epoch 2610, val loss: 1.9048383235931396
Epoch 2620, training loss: 62.07049560546875 = 0.008720804005861282 + 10.0 * 6.206177711486816
Epoch 2620, val loss: 1.9075062274932861
Epoch 2630, training loss: 62.036590576171875 = 0.008628129959106445 + 10.0 * 6.20279598236084
Epoch 2630, val loss: 1.9103045463562012
Epoch 2640, training loss: 62.029239654541016 = 0.008540059439837933 + 10.0 * 6.2020697593688965
Epoch 2640, val loss: 1.9131308794021606
Epoch 2650, training loss: 62.052555084228516 = 0.008454492315649986 + 10.0 * 6.204410076141357
Epoch 2650, val loss: 1.9156004190444946
Epoch 2660, training loss: 62.0488395690918 = 0.008364006876945496 + 10.0 * 6.204047203063965
Epoch 2660, val loss: 1.9182825088500977
Epoch 2670, training loss: 62.02938461303711 = 0.008277995511889458 + 10.0 * 6.202110767364502
Epoch 2670, val loss: 1.921055793762207
Epoch 2680, training loss: 62.031158447265625 = 0.008195407688617706 + 10.0 * 6.202296257019043
Epoch 2680, val loss: 1.9238601922988892
Epoch 2690, training loss: 62.031314849853516 = 0.008113035000860691 + 10.0 * 6.202320098876953
Epoch 2690, val loss: 1.9264166355133057
Epoch 2700, training loss: 62.05437088012695 = 0.00803264882415533 + 10.0 * 6.204633712768555
Epoch 2700, val loss: 1.9289586544036865
Epoch 2710, training loss: 62.07048416137695 = 0.007949378341436386 + 10.0 * 6.206253528594971
Epoch 2710, val loss: 1.9311972856521606
Epoch 2720, training loss: 62.03206253051758 = 0.007869345135986805 + 10.0 * 6.202419281005859
Epoch 2720, val loss: 1.9340167045593262
Epoch 2730, training loss: 62.020591735839844 = 0.007790449075400829 + 10.0 * 6.201280117034912
Epoch 2730, val loss: 1.9364283084869385
Epoch 2740, training loss: 62.029693603515625 = 0.007715719752013683 + 10.0 * 6.202197551727295
Epoch 2740, val loss: 1.9390394687652588
Epoch 2750, training loss: 62.042171478271484 = 0.007641443982720375 + 10.0 * 6.203453063964844
Epoch 2750, val loss: 1.9414640665054321
Epoch 2760, training loss: 62.01303482055664 = 0.007565530017018318 + 10.0 * 6.200547218322754
Epoch 2760, val loss: 1.943928599357605
Epoch 2770, training loss: 62.0192985534668 = 0.007494393270462751 + 10.0 * 6.201180458068848
Epoch 2770, val loss: 1.9464068412780762
Epoch 2780, training loss: 62.061492919921875 = 0.007425431627780199 + 10.0 * 6.205406665802002
Epoch 2780, val loss: 1.948706030845642
Epoch 2790, training loss: 62.05436325073242 = 0.007351979613304138 + 10.0 * 6.2047014236450195
Epoch 2790, val loss: 1.9509483575820923
Epoch 2800, training loss: 62.03273010253906 = 0.007279385346919298 + 10.0 * 6.202545166015625
Epoch 2800, val loss: 1.9532665014266968
Epoch 2810, training loss: 62.010189056396484 = 0.007209782488644123 + 10.0 * 6.200297832489014
Epoch 2810, val loss: 1.9554940462112427
Epoch 2820, training loss: 62.00174331665039 = 0.007143604103475809 + 10.0 * 6.199460029602051
Epoch 2820, val loss: 1.9580650329589844
Epoch 2830, training loss: 62.008079528808594 = 0.007079408038407564 + 10.0 * 6.200099945068359
Epoch 2830, val loss: 1.9603945016860962
Epoch 2840, training loss: 62.06983184814453 = 0.007015153765678406 + 10.0 * 6.206281661987305
Epoch 2840, val loss: 1.9624170064926147
Epoch 2850, training loss: 62.03463363647461 = 0.006947742309421301 + 10.0 * 6.202768802642822
Epoch 2850, val loss: 1.964842438697815
Epoch 2860, training loss: 62.01063537597656 = 0.006882885005325079 + 10.0 * 6.200375556945801
Epoch 2860, val loss: 1.967010498046875
Epoch 2870, training loss: 62.02407455444336 = 0.006821685004979372 + 10.0 * 6.201725482940674
Epoch 2870, val loss: 1.9692468643188477
Epoch 2880, training loss: 62.016761779785156 = 0.006759336683899164 + 10.0 * 6.201000213623047
Epoch 2880, val loss: 1.9713584184646606
Epoch 2890, training loss: 62.01169967651367 = 0.00669620418921113 + 10.0 * 6.20050048828125
Epoch 2890, val loss: 1.973577857017517
Epoch 2900, training loss: 62.00759506225586 = 0.006637061480432749 + 10.0 * 6.2000956535339355
Epoch 2900, val loss: 1.9757773876190186
Epoch 2910, training loss: 62.00300598144531 = 0.0065789190120995045 + 10.0 * 6.199642658233643
Epoch 2910, val loss: 1.977903962135315
Epoch 2920, training loss: 62.04024887084961 = 0.006520793307572603 + 10.0 * 6.203372955322266
Epoch 2920, val loss: 1.980042576789856
Epoch 2930, training loss: 62.003421783447266 = 0.006460732780396938 + 10.0 * 6.199696063995361
Epoch 2930, val loss: 1.9818813800811768
Epoch 2940, training loss: 61.99437713623047 = 0.006403196137398481 + 10.0 * 6.198797225952148
Epoch 2940, val loss: 1.984124779701233
Epoch 2950, training loss: 61.992164611816406 = 0.006349158938974142 + 10.0 * 6.198581695556641
Epoch 2950, val loss: 1.9862092733383179
Epoch 2960, training loss: 62.00111770629883 = 0.00629561860114336 + 10.0 * 6.199482440948486
Epoch 2960, val loss: 1.9882559776306152
Epoch 2970, training loss: 62.00074005126953 = 0.0062416670843958855 + 10.0 * 6.1994500160217285
Epoch 2970, val loss: 1.9903494119644165
Epoch 2980, training loss: 62.00434494018555 = 0.00618839543312788 + 10.0 * 6.19981575012207
Epoch 2980, val loss: 1.992227554321289
Epoch 2990, training loss: 62.01231384277344 = 0.006134852766990662 + 10.0 * 6.200617790222168
Epoch 2990, val loss: 1.9940106868743896
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 87.92472076416016 = 1.9564917087554932 + 10.0 * 8.596822738647461
Epoch 0, val loss: 1.9565576314926147
Epoch 10, training loss: 87.9059066772461 = 1.946268081665039 + 10.0 * 8.595964431762695
Epoch 10, val loss: 1.9457883834838867
Epoch 20, training loss: 87.82809448242188 = 1.933656930923462 + 10.0 * 8.589444160461426
Epoch 20, val loss: 1.93216872215271
Epoch 30, training loss: 87.3552474975586 = 1.917586088180542 + 10.0 * 8.543766021728516
Epoch 30, val loss: 1.9147838354110718
Epoch 40, training loss: 84.30828094482422 = 1.8994921445846558 + 10.0 * 8.24087905883789
Epoch 40, val loss: 1.8954739570617676
Epoch 50, training loss: 76.59893798828125 = 1.8787105083465576 + 10.0 * 7.472023010253906
Epoch 50, val loss: 1.874076008796692
Epoch 60, training loss: 73.99520111083984 = 1.8643066883087158 + 10.0 * 7.213089466094971
Epoch 60, val loss: 1.8603190183639526
Epoch 70, training loss: 72.40174102783203 = 1.8531997203826904 + 10.0 * 7.054853439331055
Epoch 70, val loss: 1.8490889072418213
Epoch 80, training loss: 71.209716796875 = 1.8415374755859375 + 10.0 * 6.936817646026611
Epoch 80, val loss: 1.8373736143112183
Epoch 90, training loss: 70.30792999267578 = 1.8312262296676636 + 10.0 * 6.847670555114746
Epoch 90, val loss: 1.8271844387054443
Epoch 100, training loss: 69.52050018310547 = 1.8203582763671875 + 10.0 * 6.77001428604126
Epoch 100, val loss: 1.8165795803070068
Epoch 110, training loss: 68.8541030883789 = 1.811045527458191 + 10.0 * 6.704305648803711
Epoch 110, val loss: 1.8073543310165405
Epoch 120, training loss: 68.40400695800781 = 1.802272915840149 + 10.0 * 6.660173416137695
Epoch 120, val loss: 1.798471450805664
Epoch 130, training loss: 67.96270751953125 = 1.792919635772705 + 10.0 * 6.616978168487549
Epoch 130, val loss: 1.7893158197402954
Epoch 140, training loss: 67.58893585205078 = 1.7838211059570312 + 10.0 * 6.580511569976807
Epoch 140, val loss: 1.7803829908370972
Epoch 150, training loss: 67.31117248535156 = 1.7746132612228394 + 10.0 * 6.553656101226807
Epoch 150, val loss: 1.7712510824203491
Epoch 160, training loss: 67.04615020751953 = 1.7647761106491089 + 10.0 * 6.52813720703125
Epoch 160, val loss: 1.7616020441055298
Epoch 170, training loss: 66.82813262939453 = 1.7542622089385986 + 10.0 * 6.507386684417725
Epoch 170, val loss: 1.7513834238052368
Epoch 180, training loss: 66.64726257324219 = 1.7429914474487305 + 10.0 * 6.490427494049072
Epoch 180, val loss: 1.7404831647872925
Epoch 190, training loss: 66.4761734008789 = 1.7308324575424194 + 10.0 * 6.474534034729004
Epoch 190, val loss: 1.728921890258789
Epoch 200, training loss: 66.33297729492188 = 1.7175546884536743 + 10.0 * 6.46154260635376
Epoch 200, val loss: 1.7165082693099976
Epoch 210, training loss: 66.21376037597656 = 1.7030494213104248 + 10.0 * 6.451070785522461
Epoch 210, val loss: 1.7029587030410767
Epoch 220, training loss: 66.07686614990234 = 1.687209129333496 + 10.0 * 6.438965320587158
Epoch 220, val loss: 1.6884288787841797
Epoch 230, training loss: 65.97821044921875 = 1.6700527667999268 + 10.0 * 6.430816173553467
Epoch 230, val loss: 1.672735571861267
Epoch 240, training loss: 65.8292007446289 = 1.6514170169830322 + 10.0 * 6.417778015136719
Epoch 240, val loss: 1.6558747291564941
Epoch 250, training loss: 65.71556854248047 = 1.6314458847045898 + 10.0 * 6.408411979675293
Epoch 250, val loss: 1.6378055810928345
Epoch 260, training loss: 65.6258316040039 = 1.6098748445510864 + 10.0 * 6.401595115661621
Epoch 260, val loss: 1.6185836791992188
Epoch 270, training loss: 65.52378845214844 = 1.586742877960205 + 10.0 * 6.393704414367676
Epoch 270, val loss: 1.598207950592041
Epoch 280, training loss: 65.41395568847656 = 1.5623356103897095 + 10.0 * 6.385162353515625
Epoch 280, val loss: 1.576852560043335
Epoch 290, training loss: 65.37725067138672 = 1.5364665985107422 + 10.0 * 6.384078025817871
Epoch 290, val loss: 1.554632306098938
Epoch 300, training loss: 65.23357391357422 = 1.5096100568771362 + 10.0 * 6.372395992279053
Epoch 300, val loss: 1.5316025018692017
Epoch 310, training loss: 65.16329956054688 = 1.4817419052124023 + 10.0 * 6.368155479431152
Epoch 310, val loss: 1.508074164390564
Epoch 320, training loss: 65.08660888671875 = 1.452845573425293 + 10.0 * 6.363376140594482
Epoch 320, val loss: 1.4840632677078247
Epoch 330, training loss: 64.98361206054688 = 1.4235444068908691 + 10.0 * 6.356006622314453
Epoch 330, val loss: 1.4599740505218506
Epoch 340, training loss: 64.90581512451172 = 1.3937355279922485 + 10.0 * 6.351207733154297
Epoch 340, val loss: 1.4358441829681396
Epoch 350, training loss: 64.91070556640625 = 1.363478660583496 + 10.0 * 6.35472297668457
Epoch 350, val loss: 1.4116442203521729
Epoch 360, training loss: 64.7754135131836 = 1.333073616027832 + 10.0 * 6.344233989715576
Epoch 360, val loss: 1.387452244758606
Epoch 370, training loss: 64.69291687011719 = 1.3026742935180664 + 10.0 * 6.339024066925049
Epoch 370, val loss: 1.3635705709457397
Epoch 380, training loss: 64.6230697631836 = 1.2722163200378418 + 10.0 * 6.335085391998291
Epoch 380, val loss: 1.339812159538269
Epoch 390, training loss: 64.55769348144531 = 1.242027759552002 + 10.0 * 6.33156681060791
Epoch 390, val loss: 1.3163236379623413
Epoch 400, training loss: 64.48937225341797 = 1.2119532823562622 + 10.0 * 6.327742099761963
Epoch 400, val loss: 1.2931594848632812
Epoch 410, training loss: 64.49020385742188 = 1.182154893875122 + 10.0 * 6.330804824829102
Epoch 410, val loss: 1.2703286409378052
Epoch 420, training loss: 64.37619018554688 = 1.1524704694747925 + 10.0 * 6.322371482849121
Epoch 420, val loss: 1.2478975057601929
Epoch 430, training loss: 64.31473541259766 = 1.123328447341919 + 10.0 * 6.319140911102295
Epoch 430, val loss: 1.2260098457336426
Epoch 440, training loss: 64.25717163085938 = 1.0948412418365479 + 10.0 * 6.316233158111572
Epoch 440, val loss: 1.2047162055969238
Epoch 450, training loss: 64.22452545166016 = 1.0668957233428955 + 10.0 * 6.315762996673584
Epoch 450, val loss: 1.1841118335723877
Epoch 460, training loss: 64.19734954833984 = 1.039508581161499 + 10.0 * 6.315784454345703
Epoch 460, val loss: 1.1642582416534424
Epoch 470, training loss: 64.12896728515625 = 1.0131251811981201 + 10.0 * 6.31158447265625
Epoch 470, val loss: 1.145222783088684
Epoch 480, training loss: 64.0582275390625 = 0.9874363541603088 + 10.0 * 6.307078838348389
Epoch 480, val loss: 1.1270008087158203
Epoch 490, training loss: 64.00933074951172 = 0.9626894593238831 + 10.0 * 6.304664134979248
Epoch 490, val loss: 1.1098281145095825
Epoch 500, training loss: 63.96095657348633 = 0.9388937950134277 + 10.0 * 6.302206516265869
Epoch 500, val loss: 1.093650460243225
Epoch 510, training loss: 63.92640686035156 = 0.9158877730369568 + 10.0 * 6.301052093505859
Epoch 510, val loss: 1.0785269737243652
Epoch 520, training loss: 63.93602752685547 = 0.8935827612876892 + 10.0 * 6.304244518280029
Epoch 520, val loss: 1.0636959075927734
Epoch 530, training loss: 63.85311508178711 = 0.8722888827323914 + 10.0 * 6.2980828285217285
Epoch 530, val loss: 1.0503085851669312
Epoch 540, training loss: 63.79997634887695 = 0.8518818616867065 + 10.0 * 6.294809341430664
Epoch 540, val loss: 1.0379188060760498
Epoch 550, training loss: 63.762508392333984 = 0.8322713971138 + 10.0 * 6.293023586273193
Epoch 550, val loss: 1.0263137817382812
Epoch 560, training loss: 63.726314544677734 = 0.8133026957511902 + 10.0 * 6.291301250457764
Epoch 560, val loss: 1.0154273509979248
Epoch 570, training loss: 63.806941986083984 = 0.7947776913642883 + 10.0 * 6.3012166023254395
Epoch 570, val loss: 1.005138635635376
Epoch 580, training loss: 63.690433502197266 = 0.7767708897590637 + 10.0 * 6.291366100311279
Epoch 580, val loss: 0.9953619837760925
Epoch 590, training loss: 63.632347106933594 = 0.7592839598655701 + 10.0 * 6.287306308746338
Epoch 590, val loss: 0.9864357113838196
Epoch 600, training loss: 63.63479995727539 = 0.7422341704368591 + 10.0 * 6.289256572723389
Epoch 600, val loss: 0.9779093265533447
Epoch 610, training loss: 63.576839447021484 = 0.7256233096122742 + 10.0 * 6.285121440887451
Epoch 610, val loss: 0.9698923826217651
Epoch 620, training loss: 63.53608703613281 = 0.7091211676597595 + 10.0 * 6.282696723937988
Epoch 620, val loss: 0.9622232913970947
Epoch 630, training loss: 63.51411819458008 = 0.6929348111152649 + 10.0 * 6.282118320465088
Epoch 630, val loss: 0.9550350904464722
Epoch 640, training loss: 63.48525619506836 = 0.6768364310264587 + 10.0 * 6.280841827392578
Epoch 640, val loss: 0.9479244351387024
Epoch 650, training loss: 63.45382308959961 = 0.6608239412307739 + 10.0 * 6.279299736022949
Epoch 650, val loss: 0.9413093328475952
Epoch 660, training loss: 63.46257781982422 = 0.64494389295578 + 10.0 * 6.281763553619385
Epoch 660, val loss: 0.9348727464675903
Epoch 670, training loss: 63.404541015625 = 0.629187822341919 + 10.0 * 6.277535438537598
Epoch 670, val loss: 0.9287832379341125
Epoch 680, training loss: 63.369873046875 = 0.6134722828865051 + 10.0 * 6.27564001083374
Epoch 680, val loss: 0.9230114817619324
Epoch 690, training loss: 63.3475227355957 = 0.5979040861129761 + 10.0 * 6.274961948394775
Epoch 690, val loss: 0.9173987507820129
Epoch 700, training loss: 63.33247756958008 = 0.5823533535003662 + 10.0 * 6.275012493133545
Epoch 700, val loss: 0.9121498465538025
Epoch 710, training loss: 63.29816818237305 = 0.5668540000915527 + 10.0 * 6.273131370544434
Epoch 710, val loss: 0.9070362448692322
Epoch 720, training loss: 63.261634826660156 = 0.551444947719574 + 10.0 * 6.271018981933594
Epoch 720, val loss: 0.9021400213241577
Epoch 730, training loss: 63.22852325439453 = 0.5362850427627563 + 10.0 * 6.269223690032959
Epoch 730, val loss: 0.8976633548736572
Epoch 740, training loss: 63.24870300292969 = 0.5212498903274536 + 10.0 * 6.272745132446289
Epoch 740, val loss: 0.893333911895752
Epoch 750, training loss: 63.20566940307617 = 0.506305992603302 + 10.0 * 6.269936561584473
Epoch 750, val loss: 0.8896892666816711
Epoch 760, training loss: 63.17720413208008 = 0.49154284596443176 + 10.0 * 6.268566131591797
Epoch 760, val loss: 0.8859018683433533
Epoch 770, training loss: 63.138397216796875 = 0.4770689308643341 + 10.0 * 6.266132831573486
Epoch 770, val loss: 0.8825861215591431
Epoch 780, training loss: 63.10877990722656 = 0.4629012942314148 + 10.0 * 6.264587879180908
Epoch 780, val loss: 0.8796664476394653
Epoch 790, training loss: 63.08583450317383 = 0.44896069169044495 + 10.0 * 6.263687610626221
Epoch 790, val loss: 0.8770739436149597
Epoch 800, training loss: 63.08544921875 = 0.43532243371009827 + 10.0 * 6.265012741088867
Epoch 800, val loss: 0.8746894598007202
Epoch 810, training loss: 63.085174560546875 = 0.4219415485858917 + 10.0 * 6.266323566436768
Epoch 810, val loss: 0.8726571202278137
Epoch 820, training loss: 63.036468505859375 = 0.40878474712371826 + 10.0 * 6.262768745422363
Epoch 820, val loss: 0.8707007169723511
Epoch 830, training loss: 62.99699401855469 = 0.39610999822616577 + 10.0 * 6.2600884437561035
Epoch 830, val loss: 0.8692305684089661
Epoch 840, training loss: 62.978485107421875 = 0.3838280439376831 + 10.0 * 6.25946569442749
Epoch 840, val loss: 0.8683542013168335
Epoch 850, training loss: 63.01508331298828 = 0.37180864810943604 + 10.0 * 6.264327526092529
Epoch 850, val loss: 0.8675861358642578
Epoch 860, training loss: 62.98625564575195 = 0.3601587116718292 + 10.0 * 6.262609958648682
Epoch 860, val loss: 0.8671671748161316
Epoch 870, training loss: 62.91913986206055 = 0.3488203287124634 + 10.0 * 6.2570319175720215
Epoch 870, val loss: 0.8669844269752502
Epoch 880, training loss: 62.89909744262695 = 0.33792662620544434 + 10.0 * 6.256117343902588
Epoch 880, val loss: 0.8673189878463745
Epoch 890, training loss: 62.879390716552734 = 0.32741981744766235 + 10.0 * 6.255197048187256
Epoch 890, val loss: 0.8679444789886475
Epoch 900, training loss: 62.901756286621094 = 0.31721383333206177 + 10.0 * 6.258454322814941
Epoch 900, val loss: 0.8687586188316345
Epoch 910, training loss: 62.88697052001953 = 0.30733826756477356 + 10.0 * 6.257963180541992
Epoch 910, val loss: 0.8701557517051697
Epoch 920, training loss: 62.84498977661133 = 0.2977488934993744 + 10.0 * 6.254724025726318
Epoch 920, val loss: 0.8713676929473877
Epoch 930, training loss: 62.80952835083008 = 0.28854650259017944 + 10.0 * 6.252098083496094
Epoch 930, val loss: 0.8731756210327148
Epoch 940, training loss: 62.796268463134766 = 0.2796953618526459 + 10.0 * 6.251657485961914
Epoch 940, val loss: 0.8752283453941345
Epoch 950, training loss: 62.81903076171875 = 0.27112624049186707 + 10.0 * 6.254790306091309
Epoch 950, val loss: 0.8774853944778442
Epoch 960, training loss: 62.7785758972168 = 0.2628413140773773 + 10.0 * 6.25157356262207
Epoch 960, val loss: 0.8801055550575256
Epoch 970, training loss: 62.76837158203125 = 0.2548137903213501 + 10.0 * 6.2513556480407715
Epoch 970, val loss: 0.8826906681060791
Epoch 980, training loss: 62.74009323120117 = 0.24710452556610107 + 10.0 * 6.249299049377441
Epoch 980, val loss: 0.8855849504470825
Epoch 990, training loss: 62.73387908935547 = 0.23969228565692902 + 10.0 * 6.24941873550415
Epoch 990, val loss: 0.8887766003608704
Epoch 1000, training loss: 62.71937561035156 = 0.2325163334608078 + 10.0 * 6.248685836791992
Epoch 1000, val loss: 0.8922703862190247
Epoch 1010, training loss: 62.71002197265625 = 0.2255845069885254 + 10.0 * 6.248443603515625
Epoch 1010, val loss: 0.8957687020301819
Epoch 1020, training loss: 62.692081451416016 = 0.21888433396816254 + 10.0 * 6.24731969833374
Epoch 1020, val loss: 0.8994867205619812
Epoch 1030, training loss: 62.702693939208984 = 0.21241776645183563 + 10.0 * 6.249027729034424
Epoch 1030, val loss: 0.9033533334732056
Epoch 1040, training loss: 62.68436050415039 = 0.2061285674571991 + 10.0 * 6.247823238372803
Epoch 1040, val loss: 0.9072521924972534
Epoch 1050, training loss: 62.65422439575195 = 0.20006218552589417 + 10.0 * 6.245416164398193
Epoch 1050, val loss: 0.9114900827407837
Epoch 1060, training loss: 62.644168853759766 = 0.19423474371433258 + 10.0 * 6.244993686676025
Epoch 1060, val loss: 0.9156906008720398
Epoch 1070, training loss: 62.6511344909668 = 0.18858082592487335 + 10.0 * 6.246255397796631
Epoch 1070, val loss: 0.9200481176376343
Epoch 1080, training loss: 62.6384162902832 = 0.18312817811965942 + 10.0 * 6.245528697967529
Epoch 1080, val loss: 0.9247921109199524
Epoch 1090, training loss: 62.61013412475586 = 0.17782650887966156 + 10.0 * 6.243230819702148
Epoch 1090, val loss: 0.9292411804199219
Epoch 1100, training loss: 62.59321594238281 = 0.1727481335401535 + 10.0 * 6.24204683303833
Epoch 1100, val loss: 0.9340330362319946
Epoch 1110, training loss: 62.63264846801758 = 0.16783393919467926 + 10.0 * 6.246481418609619
Epoch 1110, val loss: 0.9387186169624329
Epoch 1120, training loss: 62.584617614746094 = 0.16303317248821259 + 10.0 * 6.24215841293335
Epoch 1120, val loss: 0.9436416029930115
Epoch 1130, training loss: 62.56291961669922 = 0.1584087759256363 + 10.0 * 6.240450859069824
Epoch 1130, val loss: 0.9484189748764038
Epoch 1140, training loss: 62.60908889770508 = 0.15395022928714752 + 10.0 * 6.245513916015625
Epoch 1140, val loss: 0.9533706307411194
Epoch 1150, training loss: 62.55730438232422 = 0.14963982999324799 + 10.0 * 6.240766525268555
Epoch 1150, val loss: 0.9586887955665588
Epoch 1160, training loss: 62.53203201293945 = 0.14543567597866058 + 10.0 * 6.238659858703613
Epoch 1160, val loss: 0.9635578393936157
Epoch 1170, training loss: 62.5197868347168 = 0.14141708612442017 + 10.0 * 6.237836837768555
Epoch 1170, val loss: 0.9688764810562134
Epoch 1180, training loss: 62.57352066040039 = 0.1375109851360321 + 10.0 * 6.243600845336914
Epoch 1180, val loss: 0.9740816354751587
Epoch 1190, training loss: 62.52615737915039 = 0.1336802989244461 + 10.0 * 6.239247798919678
Epoch 1190, val loss: 0.9790988564491272
Epoch 1200, training loss: 62.5047492980957 = 0.1299998164176941 + 10.0 * 6.2374749183654785
Epoch 1200, val loss: 0.9844480156898499
Epoch 1210, training loss: 62.512935638427734 = 0.12644502520561218 + 10.0 * 6.238648891448975
Epoch 1210, val loss: 0.9897325038909912
Epoch 1220, training loss: 62.507850646972656 = 0.12299490720033646 + 10.0 * 6.238485813140869
Epoch 1220, val loss: 0.9950653314590454
Epoch 1230, training loss: 62.50563049316406 = 0.11964653432369232 + 10.0 * 6.238598346710205
Epoch 1230, val loss: 1.0006461143493652
Epoch 1240, training loss: 62.47168731689453 = 0.11638150364160538 + 10.0 * 6.235530376434326
Epoch 1240, val loss: 1.0058547258377075
Epoch 1250, training loss: 62.45399856567383 = 0.11327149718999863 + 10.0 * 6.234072685241699
Epoch 1250, val loss: 1.0113563537597656
Epoch 1260, training loss: 62.4487419128418 = 0.11025680601596832 + 10.0 * 6.233848571777344
Epoch 1260, val loss: 1.0168205499649048
Epoch 1270, training loss: 62.465816497802734 = 0.10733292996883392 + 10.0 * 6.235848426818848
Epoch 1270, val loss: 1.0221400260925293
Epoch 1280, training loss: 62.44875717163086 = 0.1044531837105751 + 10.0 * 6.234430313110352
Epoch 1280, val loss: 1.0276025533676147
Epoch 1290, training loss: 62.443199157714844 = 0.10167104750871658 + 10.0 * 6.234152793884277
Epoch 1290, val loss: 1.0331361293792725
Epoch 1300, training loss: 62.423892974853516 = 0.09897982329130173 + 10.0 * 6.232491493225098
Epoch 1300, val loss: 1.0386334657669067
Epoch 1310, training loss: 62.4108772277832 = 0.09639322757720947 + 10.0 * 6.231448173522949
Epoch 1310, val loss: 1.04421067237854
Epoch 1320, training loss: 62.41409683227539 = 0.09388885647058487 + 10.0 * 6.232020854949951
Epoch 1320, val loss: 1.0497485399246216
Epoch 1330, training loss: 62.42521286010742 = 0.09145098924636841 + 10.0 * 6.233376502990723
Epoch 1330, val loss: 1.0553758144378662
Epoch 1340, training loss: 62.425384521484375 = 0.08905857056379318 + 10.0 * 6.233632564544678
Epoch 1340, val loss: 1.0609091520309448
Epoch 1350, training loss: 62.399593353271484 = 0.08674285560846329 + 10.0 * 6.231285095214844
Epoch 1350, val loss: 1.0662587881088257
Epoch 1360, training loss: 62.425880432128906 = 0.08449741452932358 + 10.0 * 6.234138488769531
Epoch 1360, val loss: 1.0716769695281982
Epoch 1370, training loss: 62.38202667236328 = 0.08231750130653381 + 10.0 * 6.229970932006836
Epoch 1370, val loss: 1.0772130489349365
Epoch 1380, training loss: 62.366607666015625 = 0.0802258551120758 + 10.0 * 6.228638172149658
Epoch 1380, val loss: 1.0826911926269531
Epoch 1390, training loss: 62.36124038696289 = 0.07820342481136322 + 10.0 * 6.2283034324646
Epoch 1390, val loss: 1.0882790088653564
Epoch 1400, training loss: 62.39341354370117 = 0.07625293731689453 + 10.0 * 6.231716156005859
Epoch 1400, val loss: 1.0938735008239746
Epoch 1410, training loss: 62.36064529418945 = 0.07431535422801971 + 10.0 * 6.228632926940918
Epoch 1410, val loss: 1.0991154909133911
Epoch 1420, training loss: 62.36595916748047 = 0.07245386391878128 + 10.0 * 6.229350566864014
Epoch 1420, val loss: 1.1046514511108398
Epoch 1430, training loss: 62.36852264404297 = 0.07066087424755096 + 10.0 * 6.229786396026611
Epoch 1430, val loss: 1.1100939512252808
Epoch 1440, training loss: 62.34518051147461 = 0.06891796737909317 + 10.0 * 6.227626323699951
Epoch 1440, val loss: 1.1157300472259521
Epoch 1450, training loss: 62.34307861328125 = 0.06723622232675552 + 10.0 * 6.227583885192871
Epoch 1450, val loss: 1.1212105751037598
Epoch 1460, training loss: 62.348934173583984 = 0.0656057596206665 + 10.0 * 6.228332996368408
Epoch 1460, val loss: 1.1266672611236572
Epoch 1470, training loss: 62.33734893798828 = 0.06401105225086212 + 10.0 * 6.227334022521973
Epoch 1470, val loss: 1.1320456266403198
Epoch 1480, training loss: 62.3316764831543 = 0.06246941164135933 + 10.0 * 6.2269206047058105
Epoch 1480, val loss: 1.1375503540039062
Epoch 1490, training loss: 62.325645446777344 = 0.06097329035401344 + 10.0 * 6.226467132568359
Epoch 1490, val loss: 1.1430190801620483
Epoch 1500, training loss: 62.31634521484375 = 0.059523455798625946 + 10.0 * 6.225682258605957
Epoch 1500, val loss: 1.1484310626983643
Epoch 1510, training loss: 62.30923080444336 = 0.05812179669737816 + 10.0 * 6.22511100769043
Epoch 1510, val loss: 1.1537765264511108
Epoch 1520, training loss: 62.344635009765625 = 0.05675619840621948 + 10.0 * 6.228787899017334
Epoch 1520, val loss: 1.1591815948486328
Epoch 1530, training loss: 62.33492660522461 = 0.05541297048330307 + 10.0 * 6.227951526641846
Epoch 1530, val loss: 1.1647430658340454
Epoch 1540, training loss: 62.287628173828125 = 0.05410848930478096 + 10.0 * 6.223351955413818
Epoch 1540, val loss: 1.1697442531585693
Epoch 1550, training loss: 62.28483200073242 = 0.052860327064991 + 10.0 * 6.223196983337402
Epoch 1550, val loss: 1.1749675273895264
Epoch 1560, training loss: 62.2786979675293 = 0.051658421754837036 + 10.0 * 6.22270393371582
Epoch 1560, val loss: 1.1805120706558228
Epoch 1570, training loss: 62.30079650878906 = 0.0504896454513073 + 10.0 * 6.225030422210693
Epoch 1570, val loss: 1.1858094930648804
Epoch 1580, training loss: 62.30583190917969 = 0.04932544007897377 + 10.0 * 6.225650787353516
Epoch 1580, val loss: 1.190813660621643
Epoch 1590, training loss: 62.277523040771484 = 0.04820765182375908 + 10.0 * 6.222931861877441
Epoch 1590, val loss: 1.196203589439392
Epoch 1600, training loss: 62.26942443847656 = 0.047124989330768585 + 10.0 * 6.222229957580566
Epoch 1600, val loss: 1.2013493776321411
Epoch 1610, training loss: 62.301849365234375 = 0.04608816280961037 + 10.0 * 6.225575923919678
Epoch 1610, val loss: 1.2067619562149048
Epoch 1620, training loss: 62.26555252075195 = 0.045061927288770676 + 10.0 * 6.222048759460449
Epoch 1620, val loss: 1.2116628885269165
Epoch 1630, training loss: 62.280948638916016 = 0.044071853160858154 + 10.0 * 6.223687648773193
Epoch 1630, val loss: 1.2166461944580078
Epoch 1640, training loss: 62.24891662597656 = 0.043108150362968445 + 10.0 * 6.220580577850342
Epoch 1640, val loss: 1.2221243381500244
Epoch 1650, training loss: 62.24474334716797 = 0.04218040406703949 + 10.0 * 6.220256328582764
Epoch 1650, val loss: 1.22730553150177
Epoch 1660, training loss: 62.23982620239258 = 0.04128539189696312 + 10.0 * 6.21985387802124
Epoch 1660, val loss: 1.2323365211486816
Epoch 1670, training loss: 62.26301193237305 = 0.04041726142168045 + 10.0 * 6.222259521484375
Epoch 1670, val loss: 1.2374680042266846
Epoch 1680, training loss: 62.25612258911133 = 0.03955010324716568 + 10.0 * 6.2216572761535645
Epoch 1680, val loss: 1.2425282001495361
Epoch 1690, training loss: 62.24745178222656 = 0.038703516125679016 + 10.0 * 6.220874786376953
Epoch 1690, val loss: 1.247380256652832
Epoch 1700, training loss: 62.23530578613281 = 0.037885017693042755 + 10.0 * 6.219742298126221
Epoch 1700, val loss: 1.2525185346603394
Epoch 1710, training loss: 62.22506332397461 = 0.03710775449872017 + 10.0 * 6.218795299530029
Epoch 1710, val loss: 1.2575136423110962
Epoch 1720, training loss: 62.29258346557617 = 0.03635809198021889 + 10.0 * 6.225622653961182
Epoch 1720, val loss: 1.2628235816955566
Epoch 1730, training loss: 62.23485565185547 = 0.03558773547410965 + 10.0 * 6.219926834106445
Epoch 1730, val loss: 1.26692795753479
Epoch 1740, training loss: 62.221920013427734 = 0.03486182913184166 + 10.0 * 6.218705654144287
Epoch 1740, val loss: 1.2722609043121338
Epoch 1750, training loss: 62.22248840332031 = 0.03415869548916817 + 10.0 * 6.218832969665527
Epoch 1750, val loss: 1.2767672538757324
Epoch 1760, training loss: 62.24501419067383 = 0.03347933292388916 + 10.0 * 6.221153736114502
Epoch 1760, val loss: 1.2818304300308228
Epoch 1770, training loss: 62.23142623901367 = 0.032807983458042145 + 10.0 * 6.21986198425293
Epoch 1770, val loss: 1.2865467071533203
Epoch 1780, training loss: 62.23074722290039 = 0.032159943133592606 + 10.0 * 6.219858646392822
Epoch 1780, val loss: 1.2913089990615845
Epoch 1790, training loss: 62.211639404296875 = 0.031524524092674255 + 10.0 * 6.218011379241943
Epoch 1790, val loss: 1.296128511428833
Epoch 1800, training loss: 62.199283599853516 = 0.030913513153791428 + 10.0 * 6.216836929321289
Epoch 1800, val loss: 1.300870418548584
Epoch 1810, training loss: 62.19873809814453 = 0.030320705845952034 + 10.0 * 6.216841697692871
Epoch 1810, val loss: 1.3055084943771362
Epoch 1820, training loss: 62.20631408691406 = 0.02974335290491581 + 10.0 * 6.217657089233398
Epoch 1820, val loss: 1.3103361129760742
Epoch 1830, training loss: 62.200950622558594 = 0.029175952076911926 + 10.0 * 6.217177391052246
Epoch 1830, val loss: 1.3148863315582275
Epoch 1840, training loss: 62.22062301635742 = 0.02861993946135044 + 10.0 * 6.219200134277344
Epoch 1840, val loss: 1.3192501068115234
Epoch 1850, training loss: 62.21722412109375 = 0.02807919681072235 + 10.0 * 6.21891450881958
Epoch 1850, val loss: 1.3241100311279297
Epoch 1860, training loss: 62.184322357177734 = 0.027551792562007904 + 10.0 * 6.215676784515381
Epoch 1860, val loss: 1.3284392356872559
Epoch 1870, training loss: 62.184085845947266 = 0.02704533189535141 + 10.0 * 6.215703964233398
Epoch 1870, val loss: 1.3330589532852173
Epoch 1880, training loss: 62.215755462646484 = 0.026555968448519707 + 10.0 * 6.2189202308654785
Epoch 1880, val loss: 1.3375599384307861
Epoch 1890, training loss: 62.173580169677734 = 0.02606603503227234 + 10.0 * 6.214751243591309
Epoch 1890, val loss: 1.3417669534683228
Epoch 1900, training loss: 62.17152404785156 = 0.02559932880103588 + 10.0 * 6.214592456817627
Epoch 1900, val loss: 1.3462730646133423
Epoch 1910, training loss: 62.1673698425293 = 0.025145333260297775 + 10.0 * 6.214222431182861
Epoch 1910, val loss: 1.3506829738616943
Epoch 1920, training loss: 62.19996643066406 = 0.024706022813916206 + 10.0 * 6.217525959014893
Epoch 1920, val loss: 1.3550564050674438
Epoch 1930, training loss: 62.17776870727539 = 0.024261565878987312 + 10.0 * 6.21535062789917
Epoch 1930, val loss: 1.3592439889907837
Epoch 1940, training loss: 62.167022705078125 = 0.02383326180279255 + 10.0 * 6.214318752288818
Epoch 1940, val loss: 1.3635286092758179
Epoch 1950, training loss: 62.180484771728516 = 0.02342178300023079 + 10.0 * 6.2157063484191895
Epoch 1950, val loss: 1.3679951429367065
Epoch 1960, training loss: 62.167842864990234 = 0.023012692108750343 + 10.0 * 6.21448278427124
Epoch 1960, val loss: 1.3717563152313232
Epoch 1970, training loss: 62.19672393798828 = 0.022613562643527985 + 10.0 * 6.217411041259766
Epoch 1970, val loss: 1.376022458076477
Epoch 1980, training loss: 62.15857696533203 = 0.022223351523280144 + 10.0 * 6.213635444641113
Epoch 1980, val loss: 1.3801172971725464
Epoch 1990, training loss: 62.148956298828125 = 0.021849066019058228 + 10.0 * 6.212710380554199
Epoch 1990, val loss: 1.3843764066696167
Epoch 2000, training loss: 62.13909912109375 = 0.02148943394422531 + 10.0 * 6.211760997772217
Epoch 2000, val loss: 1.3885934352874756
Epoch 2010, training loss: 62.137569427490234 = 0.0211404450237751 + 10.0 * 6.211642742156982
Epoch 2010, val loss: 1.3927192687988281
Epoch 2020, training loss: 62.19681930541992 = 0.020800689235329628 + 10.0 * 6.217601776123047
Epoch 2020, val loss: 1.396891713142395
Epoch 2030, training loss: 62.15038299560547 = 0.020450377836823463 + 10.0 * 6.212993144989014
Epoch 2030, val loss: 1.4007344245910645
Epoch 2040, training loss: 62.14864730834961 = 0.02011890709400177 + 10.0 * 6.212852954864502
Epoch 2040, val loss: 1.4048036336898804
Epoch 2050, training loss: 62.179832458496094 = 0.019793158397078514 + 10.0 * 6.216003894805908
Epoch 2050, val loss: 1.4087945222854614
Epoch 2060, training loss: 62.136016845703125 = 0.01947220042347908 + 10.0 * 6.211654186248779
Epoch 2060, val loss: 1.4126441478729248
Epoch 2070, training loss: 62.13670349121094 = 0.019163740798830986 + 10.0 * 6.211753845214844
Epoch 2070, val loss: 1.4167263507843018
Epoch 2080, training loss: 62.150569915771484 = 0.018864739686250687 + 10.0 * 6.213170528411865
Epoch 2080, val loss: 1.4207541942596436
Epoch 2090, training loss: 62.12418746948242 = 0.018567128106951714 + 10.0 * 6.210562229156494
Epoch 2090, val loss: 1.4244787693023682
Epoch 2100, training loss: 62.12546920776367 = 0.018285268917679787 + 10.0 * 6.210718631744385
Epoch 2100, val loss: 1.428571105003357
Epoch 2110, training loss: 62.1856575012207 = 0.01801140606403351 + 10.0 * 6.216764450073242
Epoch 2110, val loss: 1.4325010776519775
Epoch 2120, training loss: 62.140045166015625 = 0.017723703756928444 + 10.0 * 6.2122321128845215
Epoch 2120, val loss: 1.43583345413208
Epoch 2130, training loss: 62.11672592163086 = 0.017452439293265343 + 10.0 * 6.209927558898926
Epoch 2130, val loss: 1.4397233724594116
Epoch 2140, training loss: 62.109012603759766 = 0.017193585634231567 + 10.0 * 6.209181785583496
Epoch 2140, val loss: 1.4435632228851318
Epoch 2150, training loss: 62.12522888183594 = 0.016944626346230507 + 10.0 * 6.2108283042907715
Epoch 2150, val loss: 1.4474339485168457
Epoch 2160, training loss: 62.13422393798828 = 0.016692178323864937 + 10.0 * 6.211752891540527
Epoch 2160, val loss: 1.4509179592132568
Epoch 2170, training loss: 62.11193084716797 = 0.01644168421626091 + 10.0 * 6.2095489501953125
Epoch 2170, val loss: 1.454685926437378
Epoch 2180, training loss: 62.11084747314453 = 0.016204986721277237 + 10.0 * 6.209464073181152
Epoch 2180, val loss: 1.4583251476287842
Epoch 2190, training loss: 62.14340591430664 = 0.015973785892128944 + 10.0 * 6.212743282318115
Epoch 2190, val loss: 1.4618353843688965
Epoch 2200, training loss: 62.121646881103516 = 0.01573496125638485 + 10.0 * 6.2105913162231445
Epoch 2200, val loss: 1.4653164148330688
Epoch 2210, training loss: 62.09809494018555 = 0.015508473850786686 + 10.0 * 6.208258628845215
Epoch 2210, val loss: 1.4691576957702637
Epoch 2220, training loss: 62.094364166259766 = 0.015292497351765633 + 10.0 * 6.207907199859619
Epoch 2220, val loss: 1.4727528095245361
Epoch 2230, training loss: 62.093345642089844 = 0.015081703662872314 + 10.0 * 6.207826137542725
Epoch 2230, val loss: 1.4763950109481812
Epoch 2240, training loss: 62.10894775390625 = 0.014877005480229855 + 10.0 * 6.209406852722168
Epoch 2240, val loss: 1.4800385236740112
Epoch 2250, training loss: 62.10383987426758 = 0.014669306576251984 + 10.0 * 6.208917140960693
Epoch 2250, val loss: 1.483464002609253
Epoch 2260, training loss: 62.146240234375 = 0.014464742504060268 + 10.0 * 6.213177680969238
Epoch 2260, val loss: 1.4867689609527588
Epoch 2270, training loss: 62.09838104248047 = 0.014263859018683434 + 10.0 * 6.208411693572998
Epoch 2270, val loss: 1.4900331497192383
Epoch 2280, training loss: 62.083892822265625 = 0.014067919924855232 + 10.0 * 6.206982612609863
Epoch 2280, val loss: 1.4937338829040527
Epoch 2290, training loss: 62.08657455444336 = 0.013882331550121307 + 10.0 * 6.207269191741943
Epoch 2290, val loss: 1.4971215724945068
Epoch 2300, training loss: 62.11023712158203 = 0.01370114553719759 + 10.0 * 6.209653377532959
Epoch 2300, val loss: 1.5006945133209229
Epoch 2310, training loss: 62.08088684082031 = 0.01351702306419611 + 10.0 * 6.206737041473389
Epoch 2310, val loss: 1.5036985874176025
Epoch 2320, training loss: 62.104827880859375 = 0.01334028784185648 + 10.0 * 6.20914888381958
Epoch 2320, val loss: 1.5069952011108398
Epoch 2330, training loss: 62.09545135498047 = 0.013164623640477657 + 10.0 * 6.208228588104248
Epoch 2330, val loss: 1.5105113983154297
Epoch 2340, training loss: 62.08298110961914 = 0.012993203476071358 + 10.0 * 6.206998825073242
Epoch 2340, val loss: 1.5138593912124634
Epoch 2350, training loss: 62.0712776184082 = 0.012826330028474331 + 10.0 * 6.205845355987549
Epoch 2350, val loss: 1.5171982049942017
Epoch 2360, training loss: 62.10948944091797 = 0.012664920650422573 + 10.0 * 6.209682464599609
Epoch 2360, val loss: 1.5206022262573242
Epoch 2370, training loss: 62.068790435791016 = 0.012499965727329254 + 10.0 * 6.205628871917725
Epoch 2370, val loss: 1.5234298706054688
Epoch 2380, training loss: 62.07505798339844 = 0.012342617847025394 + 10.0 * 6.206271171569824
Epoch 2380, val loss: 1.5269362926483154
Epoch 2390, training loss: 62.08621597290039 = 0.012187555432319641 + 10.0 * 6.20740270614624
Epoch 2390, val loss: 1.5299605131149292
Epoch 2400, training loss: 62.06069564819336 = 0.012036004103720188 + 10.0 * 6.2048659324646
Epoch 2400, val loss: 1.533490538597107
Epoch 2410, training loss: 62.102745056152344 = 0.011892937123775482 + 10.0 * 6.209084987640381
Epoch 2410, val loss: 1.5367834568023682
Epoch 2420, training loss: 62.071617126464844 = 0.01174106728285551 + 10.0 * 6.205987453460693
Epoch 2420, val loss: 1.5395976305007935
Epoch 2430, training loss: 62.0671501159668 = 0.011596115306019783 + 10.0 * 6.205555438995361
Epoch 2430, val loss: 1.5427088737487793
Epoch 2440, training loss: 62.09412384033203 = 0.011456169188022614 + 10.0 * 6.208266735076904
Epoch 2440, val loss: 1.546004056930542
Epoch 2450, training loss: 62.056907653808594 = 0.011316360905766487 + 10.0 * 6.204558849334717
Epoch 2450, val loss: 1.548875093460083
Epoch 2460, training loss: 62.04765701293945 = 0.011183131486177444 + 10.0 * 6.203647136688232
Epoch 2460, val loss: 1.552033543586731
Epoch 2470, training loss: 62.056297302246094 = 0.011053850874304771 + 10.0 * 6.204524040222168
Epoch 2470, val loss: 1.5551468133926392
Epoch 2480, training loss: 62.11617660522461 = 0.01092572696506977 + 10.0 * 6.210525035858154
Epoch 2480, val loss: 1.5581110715866089
Epoch 2490, training loss: 62.072959899902344 = 0.010794864036142826 + 10.0 * 6.206216335296631
Epoch 2490, val loss: 1.5610358715057373
Epoch 2500, training loss: 62.04783630371094 = 0.010665800422430038 + 10.0 * 6.203717231750488
Epoch 2500, val loss: 1.5640581846237183
Epoch 2510, training loss: 62.04033660888672 = 0.010544782504439354 + 10.0 * 6.20297908782959
Epoch 2510, val loss: 1.5669958591461182
Epoch 2520, training loss: 62.08502197265625 = 0.010427522473037243 + 10.0 * 6.207459449768066
Epoch 2520, val loss: 1.5697486400604248
Epoch 2530, training loss: 62.04237747192383 = 0.010305462405085564 + 10.0 * 6.203207015991211
Epoch 2530, val loss: 1.5728894472122192
Epoch 2540, training loss: 62.05678176879883 = 0.010189790278673172 + 10.0 * 6.204659461975098
Epoch 2540, val loss: 1.5758033990859985
Epoch 2550, training loss: 62.087074279785156 = 0.010071346536278725 + 10.0 * 6.207700252532959
Epoch 2550, val loss: 1.578728437423706
Epoch 2560, training loss: 62.04778289794922 = 0.009955674409866333 + 10.0 * 6.203782558441162
Epoch 2560, val loss: 1.5813549757003784
Epoch 2570, training loss: 62.03413391113281 = 0.009846717119216919 + 10.0 * 6.202428817749023
Epoch 2570, val loss: 1.5843853950500488
Epoch 2580, training loss: 62.02764892578125 = 0.009740985929965973 + 10.0 * 6.201790809631348
Epoch 2580, val loss: 1.5873663425445557
Epoch 2590, training loss: 62.04185104370117 = 0.009638060815632343 + 10.0 * 6.203221321105957
Epoch 2590, val loss: 1.5903874635696411
Epoch 2600, training loss: 62.062416076660156 = 0.009533737786114216 + 10.0 * 6.205288410186768
Epoch 2600, val loss: 1.5932402610778809
Epoch 2610, training loss: 62.04862594604492 = 0.009427429176867008 + 10.0 * 6.203919887542725
Epoch 2610, val loss: 1.5955464839935303
Epoch 2620, training loss: 62.05553436279297 = 0.009326805360615253 + 10.0 * 6.204620838165283
Epoch 2620, val loss: 1.598602056503296
Epoch 2630, training loss: 62.02827835083008 = 0.009226400405168533 + 10.0 * 6.201905250549316
Epoch 2630, val loss: 1.601027250289917
Epoch 2640, training loss: 62.03872299194336 = 0.009128959849476814 + 10.0 * 6.2029595375061035
Epoch 2640, val loss: 1.6041089296340942
Epoch 2650, training loss: 62.05678176879883 = 0.00903462152928114 + 10.0 * 6.204774856567383
Epoch 2650, val loss: 1.6066615581512451
Epoch 2660, training loss: 62.03078842163086 = 0.00893783662468195 + 10.0 * 6.202185153961182
Epoch 2660, val loss: 1.6090000867843628
Epoch 2670, training loss: 62.04188919067383 = 0.008846866898238659 + 10.0 * 6.203304290771484
Epoch 2670, val loss: 1.6118861436843872
Epoch 2680, training loss: 62.01732635498047 = 0.008753864094614983 + 10.0 * 6.200857162475586
Epoch 2680, val loss: 1.6145875453948975
Epoch 2690, training loss: 62.02507400512695 = 0.008665809407830238 + 10.0 * 6.201640605926514
Epoch 2690, val loss: 1.61725914478302
Epoch 2700, training loss: 62.07322311401367 = 0.008579178713262081 + 10.0 * 6.2064642906188965
Epoch 2700, val loss: 1.6199365854263306
Epoch 2710, training loss: 62.026615142822266 = 0.008489904925227165 + 10.0 * 6.201812267303467
Epoch 2710, val loss: 1.6222807168960571
Epoch 2720, training loss: 62.02964401245117 = 0.008406504057347775 + 10.0 * 6.202123641967773
Epoch 2720, val loss: 1.6251100301742554
Epoch 2730, training loss: 62.0310173034668 = 0.008321474306285381 + 10.0 * 6.202269554138184
Epoch 2730, val loss: 1.62776780128479
Epoch 2740, training loss: 62.01872253417969 = 0.008238163776695728 + 10.0 * 6.201048374176025
Epoch 2740, val loss: 1.6302984952926636
Epoch 2750, training loss: 62.02029037475586 = 0.008156752213835716 + 10.0 * 6.201213359832764
Epoch 2750, val loss: 1.6328104734420776
Epoch 2760, training loss: 62.0101432800293 = 0.008077554404735565 + 10.0 * 6.200206756591797
Epoch 2760, val loss: 1.6351782083511353
Epoch 2770, training loss: 62.02726745605469 = 0.008000624366104603 + 10.0 * 6.2019267082214355
Epoch 2770, val loss: 1.637860655784607
Epoch 2780, training loss: 62.03310012817383 = 0.007922460325062275 + 10.0 * 6.202517509460449
Epoch 2780, val loss: 1.6401276588439941
Epoch 2790, training loss: 62.03615188598633 = 0.007844087667763233 + 10.0 * 6.202830791473389
Epoch 2790, val loss: 1.6426721811294556
Epoch 2800, training loss: 62.011695861816406 = 0.007767293136566877 + 10.0 * 6.200392723083496
Epoch 2800, val loss: 1.6452172994613647
Epoch 2810, training loss: 62.00457000732422 = 0.007694128900766373 + 10.0 * 6.199687480926514
Epoch 2810, val loss: 1.6478731632232666
Epoch 2820, training loss: 62.01027297973633 = 0.00762370228767395 + 10.0 * 6.200264930725098
Epoch 2820, val loss: 1.6501410007476807
Epoch 2830, training loss: 62.01282501220703 = 0.007552361115813255 + 10.0 * 6.200527191162109
Epoch 2830, val loss: 1.6525955200195312
Epoch 2840, training loss: 62.006195068359375 = 0.007480902597308159 + 10.0 * 6.19987154006958
Epoch 2840, val loss: 1.6547991037368774
Epoch 2850, training loss: 62.00920486450195 = 0.007411982398480177 + 10.0 * 6.200179100036621
Epoch 2850, val loss: 1.6571779251098633
Epoch 2860, training loss: 62.01393127441406 = 0.0073439376428723335 + 10.0 * 6.200658798217773
Epoch 2860, val loss: 1.659808874130249
Epoch 2870, training loss: 62.01884841918945 = 0.0072773597203195095 + 10.0 * 6.201157093048096
Epoch 2870, val loss: 1.66226065158844
Epoch 2880, training loss: 61.99625015258789 = 0.007208348251879215 + 10.0 * 6.198904037475586
Epoch 2880, val loss: 1.6643688678741455
Epoch 2890, training loss: 62.00436019897461 = 0.007143447641283274 + 10.0 * 6.199721336364746
Epoch 2890, val loss: 1.6664512157440186
Epoch 2900, training loss: 62.01350402832031 = 0.007081307005137205 + 10.0 * 6.2006425857543945
Epoch 2900, val loss: 1.668971061706543
Epoch 2910, training loss: 61.994815826416016 = 0.007015427574515343 + 10.0 * 6.198780059814453
Epoch 2910, val loss: 1.6712373495101929
Epoch 2920, training loss: 61.99172592163086 = 0.00695348996669054 + 10.0 * 6.198477268218994
Epoch 2920, val loss: 1.6738942861557007
Epoch 2930, training loss: 61.999237060546875 = 0.006894119083881378 + 10.0 * 6.199234485626221
Epoch 2930, val loss: 1.6760728359222412
Epoch 2940, training loss: 62.01482009887695 = 0.006833286955952644 + 10.0 * 6.200798988342285
Epoch 2940, val loss: 1.6782177686691284
Epoch 2950, training loss: 61.989784240722656 = 0.006772906985133886 + 10.0 * 6.198301315307617
Epoch 2950, val loss: 1.680117130279541
Epoch 2960, training loss: 61.98630905151367 = 0.006714475806802511 + 10.0 * 6.1979594230651855
Epoch 2960, val loss: 1.6827446222305298
Epoch 2970, training loss: 61.99653625488281 = 0.006658134516328573 + 10.0 * 6.19898796081543
Epoch 2970, val loss: 1.6850606203079224
Epoch 2980, training loss: 62.01689147949219 = 0.006602205336093903 + 10.0 * 6.201028823852539
Epoch 2980, val loss: 1.6871788501739502
Epoch 2990, training loss: 62.00212097167969 = 0.006543340161442757 + 10.0 * 6.199557781219482
Epoch 2990, val loss: 1.6892175674438477
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8339483394833949
The final CL Acc:0.75556, 0.02362, The final GNN Acc:0.83641, 0.00179
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11666])
remove edge: torch.Size([2, 9522])
updated graph: torch.Size([2, 10632])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.92669677734375 = 1.9581142663955688 + 10.0 * 8.596858024597168
Epoch 0, val loss: 1.952152967453003
Epoch 10, training loss: 87.91146850585938 = 1.9476181268692017 + 10.0 * 8.59638500213623
Epoch 10, val loss: 1.9421921968460083
Epoch 20, training loss: 87.86299896240234 = 1.9346059560775757 + 10.0 * 8.592839241027832
Epoch 20, val loss: 1.9293659925460815
Epoch 30, training loss: 87.61262512207031 = 1.9177519083023071 + 10.0 * 8.569486618041992
Epoch 30, val loss: 1.9125515222549438
Epoch 40, training loss: 86.43677520751953 = 1.897348165512085 + 10.0 * 8.453943252563477
Epoch 40, val loss: 1.8927744626998901
Epoch 50, training loss: 81.41258239746094 = 1.8757058382034302 + 10.0 * 7.9536871910095215
Epoch 50, val loss: 1.87180757522583
Epoch 60, training loss: 77.13157653808594 = 1.8555488586425781 + 10.0 * 7.527602672576904
Epoch 60, val loss: 1.853432059288025
Epoch 70, training loss: 74.50165557861328 = 1.840801477432251 + 10.0 * 7.266085147857666
Epoch 70, val loss: 1.8395512104034424
Epoch 80, training loss: 72.47724914550781 = 1.8271530866622925 + 10.0 * 7.065009117126465
Epoch 80, val loss: 1.8266503810882568
Epoch 90, training loss: 71.59185028076172 = 1.8148995637893677 + 10.0 * 6.977694511413574
Epoch 90, val loss: 1.8152817487716675
Epoch 100, training loss: 71.02556610107422 = 1.8031072616577148 + 10.0 * 6.922245502471924
Epoch 100, val loss: 1.8040860891342163
Epoch 110, training loss: 70.48714447021484 = 1.7918757200241089 + 10.0 * 6.8695268630981445
Epoch 110, val loss: 1.7938133478164673
Epoch 120, training loss: 69.88481140136719 = 1.7820712327957153 + 10.0 * 6.810274124145508
Epoch 120, val loss: 1.7849093675613403
Epoch 130, training loss: 69.36093139648438 = 1.7731045484542847 + 10.0 * 6.758783340454102
Epoch 130, val loss: 1.7762248516082764
Epoch 140, training loss: 68.88081359863281 = 1.7636439800262451 + 10.0 * 6.71171760559082
Epoch 140, val loss: 1.7672927379608154
Epoch 150, training loss: 68.45858764648438 = 1.7535271644592285 + 10.0 * 6.670506000518799
Epoch 150, val loss: 1.7577048540115356
Epoch 160, training loss: 68.11052703857422 = 1.7425159215927124 + 10.0 * 6.636800765991211
Epoch 160, val loss: 1.7473028898239136
Epoch 170, training loss: 67.82523345947266 = 1.7305052280426025 + 10.0 * 6.609472751617432
Epoch 170, val loss: 1.7359297275543213
Epoch 180, training loss: 67.57312774658203 = 1.7174569368362427 + 10.0 * 6.585566520690918
Epoch 180, val loss: 1.7237082719802856
Epoch 190, training loss: 67.38595581054688 = 1.703304409980774 + 10.0 * 6.568264961242676
Epoch 190, val loss: 1.7105377912521362
Epoch 200, training loss: 67.16438293457031 = 1.6878331899642944 + 10.0 * 6.54765510559082
Epoch 200, val loss: 1.6961963176727295
Epoch 210, training loss: 66.9933853149414 = 1.6710907220840454 + 10.0 * 6.532229423522949
Epoch 210, val loss: 1.6806997060775757
Epoch 220, training loss: 66.86978149414062 = 1.6530096530914307 + 10.0 * 6.521677494049072
Epoch 220, val loss: 1.6639204025268555
Epoch 230, training loss: 66.69581604003906 = 1.6333271265029907 + 10.0 * 6.506248950958252
Epoch 230, val loss: 1.6458330154418945
Epoch 240, training loss: 66.53807830810547 = 1.6122376918792725 + 10.0 * 6.492584228515625
Epoch 240, val loss: 1.6265168190002441
Epoch 250, training loss: 66.39913940429688 = 1.589780569076538 + 10.0 * 6.480936050415039
Epoch 250, val loss: 1.6059712171554565
Epoch 260, training loss: 66.30488586425781 = 1.565847396850586 + 10.0 * 6.473903656005859
Epoch 260, val loss: 1.584306001663208
Epoch 270, training loss: 66.14375305175781 = 1.5406614542007446 + 10.0 * 6.46030855178833
Epoch 270, val loss: 1.561616063117981
Epoch 280, training loss: 66.01258087158203 = 1.5143256187438965 + 10.0 * 6.449825286865234
Epoch 280, val loss: 1.5381884574890137
Epoch 290, training loss: 65.99618530273438 = 1.4870480298995972 + 10.0 * 6.45091438293457
Epoch 290, val loss: 1.5139487981796265
Epoch 300, training loss: 65.82846069335938 = 1.4584617614746094 + 10.0 * 6.436999320983887
Epoch 300, val loss: 1.4891756772994995
Epoch 310, training loss: 65.67791748046875 = 1.4292004108428955 + 10.0 * 6.424871444702148
Epoch 310, val loss: 1.4642844200134277
Epoch 320, training loss: 65.57235717773438 = 1.3995188474655151 + 10.0 * 6.417283535003662
Epoch 320, val loss: 1.439260482788086
Epoch 330, training loss: 65.46896362304688 = 1.369357705116272 + 10.0 * 6.409961223602295
Epoch 330, val loss: 1.4141974449157715
Epoch 340, training loss: 65.39337921142578 = 1.3391464948654175 + 10.0 * 6.405422687530518
Epoch 340, val loss: 1.3896982669830322
Epoch 350, training loss: 65.29117584228516 = 1.3089091777801514 + 10.0 * 6.398226737976074
Epoch 350, val loss: 1.365638256072998
Epoch 360, training loss: 65.19976806640625 = 1.2787436246871948 + 10.0 * 6.3921027183532715
Epoch 360, val loss: 1.3420490026474
Epoch 370, training loss: 65.11984252929688 = 1.2486318349838257 + 10.0 * 6.387121200561523
Epoch 370, val loss: 1.318967580795288
Epoch 380, training loss: 65.094970703125 = 1.2187203168869019 + 10.0 * 6.387625217437744
Epoch 380, val loss: 1.2964493036270142
Epoch 390, training loss: 64.9684829711914 = 1.1892279386520386 + 10.0 * 6.377925872802734
Epoch 390, val loss: 1.2748675346374512
Epoch 400, training loss: 64.88748168945312 = 1.160190224647522 + 10.0 * 6.372729301452637
Epoch 400, val loss: 1.2540217638015747
Epoch 410, training loss: 64.8184814453125 = 1.1316314935684204 + 10.0 * 6.368684768676758
Epoch 410, val loss: 1.2339353561401367
Epoch 420, training loss: 64.86249542236328 = 1.1033622026443481 + 10.0 * 6.375913143157959
Epoch 420, val loss: 1.214582920074463
Epoch 430, training loss: 64.70655822753906 = 1.0756953954696655 + 10.0 * 6.363086223602295
Epoch 430, val loss: 1.1956309080123901
Epoch 440, training loss: 64.64312744140625 = 1.048612356185913 + 10.0 * 6.3594512939453125
Epoch 440, val loss: 1.177498698234558
Epoch 450, training loss: 64.57044982910156 = 1.0221667289733887 + 10.0 * 6.354828357696533
Epoch 450, val loss: 1.1601125001907349
Epoch 460, training loss: 64.51213836669922 = 0.9961845874786377 + 10.0 * 6.351595401763916
Epoch 460, val loss: 1.1433780193328857
Epoch 470, training loss: 64.46398162841797 = 0.970589280128479 + 10.0 * 6.349339485168457
Epoch 470, val loss: 1.127192735671997
Epoch 480, training loss: 64.42830657958984 = 0.9453511834144592 + 10.0 * 6.348295211791992
Epoch 480, val loss: 1.1112529039382935
Epoch 490, training loss: 64.35928344726562 = 0.9205678105354309 + 10.0 * 6.343871116638184
Epoch 490, val loss: 1.0959782600402832
Epoch 500, training loss: 64.30439758300781 = 0.8962516188621521 + 10.0 * 6.340814590454102
Epoch 500, val loss: 1.0810670852661133
Epoch 510, training loss: 64.30094146728516 = 0.8723203539848328 + 10.0 * 6.342862129211426
Epoch 510, val loss: 1.0665907859802246
Epoch 520, training loss: 64.21116638183594 = 0.8485495448112488 + 10.0 * 6.336261749267578
Epoch 520, val loss: 1.052549958229065
Epoch 530, training loss: 64.1693115234375 = 0.8252384066581726 + 10.0 * 6.334406852722168
Epoch 530, val loss: 1.0388572216033936
Epoch 540, training loss: 64.13092041015625 = 0.802342414855957 + 10.0 * 6.332857608795166
Epoch 540, val loss: 1.025328516960144
Epoch 550, training loss: 64.06597137451172 = 0.779568612575531 + 10.0 * 6.328640460968018
Epoch 550, val loss: 1.0123629570007324
Epoch 560, training loss: 64.02027893066406 = 0.7572741508483887 + 10.0 * 6.326300621032715
Epoch 560, val loss: 0.9996447563171387
Epoch 570, training loss: 63.97142791748047 = 0.735291063785553 + 10.0 * 6.32361364364624
Epoch 570, val loss: 0.9875782132148743
Epoch 580, training loss: 63.98513412475586 = 0.7136095762252808 + 10.0 * 6.327152729034424
Epoch 580, val loss: 0.9758114218711853
Epoch 590, training loss: 63.91136932373047 = 0.6923201680183411 + 10.0 * 6.32190465927124
Epoch 590, val loss: 0.9644052982330322
Epoch 600, training loss: 63.857826232910156 = 0.671354353427887 + 10.0 * 6.3186469078063965
Epoch 600, val loss: 0.9535450339317322
Epoch 610, training loss: 63.8560791015625 = 0.6507248282432556 + 10.0 * 6.320535182952881
Epoch 610, val loss: 0.9432322382926941
Epoch 620, training loss: 63.79009246826172 = 0.6306256055831909 + 10.0 * 6.315946578979492
Epoch 620, val loss: 0.9334740042686462
Epoch 630, training loss: 63.74063491821289 = 0.6107500195503235 + 10.0 * 6.312988758087158
Epoch 630, val loss: 0.924166202545166
Epoch 640, training loss: 63.7239990234375 = 0.5913702845573425 + 10.0 * 6.313262939453125
Epoch 640, val loss: 0.9154233932495117
Epoch 650, training loss: 63.668212890625 = 0.5722817778587341 + 10.0 * 6.309593200683594
Epoch 650, val loss: 0.9071573615074158
Epoch 660, training loss: 63.62593078613281 = 0.5536133646965027 + 10.0 * 6.307231903076172
Epoch 660, val loss: 0.8994032740592957
Epoch 670, training loss: 63.60976028442383 = 0.5353343486785889 + 10.0 * 6.307442665100098
Epoch 670, val loss: 0.8923650979995728
Epoch 680, training loss: 63.577083587646484 = 0.5173353552818298 + 10.0 * 6.305974960327148
Epoch 680, val loss: 0.8853791952133179
Epoch 690, training loss: 63.53990936279297 = 0.49974632263183594 + 10.0 * 6.304016590118408
Epoch 690, val loss: 0.8792722821235657
Epoch 700, training loss: 63.49513244628906 = 0.4825313091278076 + 10.0 * 6.301259994506836
Epoch 700, val loss: 0.8736359477043152
Epoch 710, training loss: 63.477203369140625 = 0.46570533514022827 + 10.0 * 6.301149845123291
Epoch 710, val loss: 0.8684091567993164
Epoch 720, training loss: 63.433021545410156 = 0.4492384195327759 + 10.0 * 6.2983784675598145
Epoch 720, val loss: 0.863632321357727
Epoch 730, training loss: 63.39936447143555 = 0.43317216634750366 + 10.0 * 6.296619415283203
Epoch 730, val loss: 0.8594316244125366
Epoch 740, training loss: 63.39067459106445 = 0.4174995720386505 + 10.0 * 6.2973175048828125
Epoch 740, val loss: 0.8556593656539917
Epoch 750, training loss: 63.349605560302734 = 0.40218305587768555 + 10.0 * 6.294742107391357
Epoch 750, val loss: 0.8523402214050293
Epoch 760, training loss: 63.32068634033203 = 0.38725194334983826 + 10.0 * 6.293343544006348
Epoch 760, val loss: 0.8494824171066284
Epoch 770, training loss: 63.302772521972656 = 0.37284085154533386 + 10.0 * 6.292993068695068
Epoch 770, val loss: 0.8471040725708008
Epoch 780, training loss: 63.29033279418945 = 0.3587339222431183 + 10.0 * 6.2931599617004395
Epoch 780, val loss: 0.8450464606285095
Epoch 790, training loss: 63.250614166259766 = 0.34514471888542175 + 10.0 * 6.290546894073486
Epoch 790, val loss: 0.8433888554573059
Epoch 800, training loss: 63.23566818237305 = 0.3319573700428009 + 10.0 * 6.290370941162109
Epoch 800, val loss: 0.8421827554702759
Epoch 810, training loss: 63.21479415893555 = 0.3192250728607178 + 10.0 * 6.289556980133057
Epoch 810, val loss: 0.8411790728569031
Epoch 820, training loss: 63.173675537109375 = 0.3068176805973053 + 10.0 * 6.286685943603516
Epoch 820, val loss: 0.8406749963760376
Epoch 830, training loss: 63.169002532958984 = 0.29488980770111084 + 10.0 * 6.287411212921143
Epoch 830, val loss: 0.8403997421264648
Epoch 840, training loss: 63.14857864379883 = 0.28341758251190186 + 10.0 * 6.286516189575195
Epoch 840, val loss: 0.8405146598815918
Epoch 850, training loss: 63.10760498046875 = 0.27233293652534485 + 10.0 * 6.283527374267578
Epoch 850, val loss: 0.8408564329147339
Epoch 860, training loss: 63.092926025390625 = 0.26169300079345703 + 10.0 * 6.28312349319458
Epoch 860, val loss: 0.8416000604629517
Epoch 870, training loss: 63.1468505859375 = 0.2515256106853485 + 10.0 * 6.289532661437988
Epoch 870, val loss: 0.8426116108894348
Epoch 880, training loss: 63.050235748291016 = 0.24162591993808746 + 10.0 * 6.280860900878906
Epoch 880, val loss: 0.8437140583992004
Epoch 890, training loss: 63.02971267700195 = 0.23225857317447662 + 10.0 * 6.279745578765869
Epoch 890, val loss: 0.8452212810516357
Epoch 900, training loss: 63.04206466674805 = 0.22328631579875946 + 10.0 * 6.2818779945373535
Epoch 900, val loss: 0.8471331596374512
Epoch 910, training loss: 62.99180221557617 = 0.2146620899438858 + 10.0 * 6.277714252471924
Epoch 910, val loss: 0.8491500616073608
Epoch 920, training loss: 62.981685638427734 = 0.206401526927948 + 10.0 * 6.277528285980225
Epoch 920, val loss: 0.8514235615730286
Epoch 930, training loss: 62.97306823730469 = 0.19847694039344788 + 10.0 * 6.277459144592285
Epoch 930, val loss: 0.8539342284202576
Epoch 940, training loss: 62.94392013549805 = 0.19093047082424164 + 10.0 * 6.275299072265625
Epoch 940, val loss: 0.8565226793289185
Epoch 950, training loss: 62.924415588378906 = 0.18369132280349731 + 10.0 * 6.274072170257568
Epoch 950, val loss: 0.8595467805862427
Epoch 960, training loss: 62.91230773925781 = 0.17679093778133392 + 10.0 * 6.2735514640808105
Epoch 960, val loss: 0.8626754283905029
Epoch 970, training loss: 62.92579650878906 = 0.17018024623394012 + 10.0 * 6.275561332702637
Epoch 970, val loss: 0.8660441637039185
Epoch 980, training loss: 62.94793701171875 = 0.1637263000011444 + 10.0 * 6.278420925140381
Epoch 980, val loss: 0.8690409064292908
Epoch 990, training loss: 62.88539123535156 = 0.1576293557882309 + 10.0 * 6.272776126861572
Epoch 990, val loss: 0.8723719716072083
Epoch 1000, training loss: 62.85837173461914 = 0.1518508940935135 + 10.0 * 6.270651817321777
Epoch 1000, val loss: 0.8760946989059448
Epoch 1010, training loss: 62.83842849731445 = 0.1463526040315628 + 10.0 * 6.269207954406738
Epoch 1010, val loss: 0.8798562288284302
Epoch 1020, training loss: 62.822853088378906 = 0.14108486473560333 + 10.0 * 6.268176555633545
Epoch 1020, val loss: 0.8838478326797485
Epoch 1030, training loss: 62.81393051147461 = 0.13603249192237854 + 10.0 * 6.267789840698242
Epoch 1030, val loss: 0.8879066705703735
Epoch 1040, training loss: 62.90693664550781 = 0.13115061819553375 + 10.0 * 6.277578830718994
Epoch 1040, val loss: 0.8918977975845337
Epoch 1050, training loss: 62.83329391479492 = 0.12645353376865387 + 10.0 * 6.270684242248535
Epoch 1050, val loss: 0.8957257866859436
Epoch 1060, training loss: 62.78670120239258 = 0.12201651185750961 + 10.0 * 6.266468524932861
Epoch 1060, val loss: 0.900093138217926
Epoch 1070, training loss: 62.766212463378906 = 0.11778129637241364 + 10.0 * 6.264842987060547
Epoch 1070, val loss: 0.904403805732727
Epoch 1080, training loss: 62.756866455078125 = 0.1137528344988823 + 10.0 * 6.26431131362915
Epoch 1080, val loss: 0.9088626503944397
Epoch 1090, training loss: 62.7722282409668 = 0.1098790392279625 + 10.0 * 6.266234874725342
Epoch 1090, val loss: 0.9131998419761658
Epoch 1100, training loss: 62.743167877197266 = 0.10614613443613052 + 10.0 * 6.263701915740967
Epoch 1100, val loss: 0.9176170229911804
Epoch 1110, training loss: 62.741355895996094 = 0.10255379229784012 + 10.0 * 6.263880252838135
Epoch 1110, val loss: 0.9220951795578003
Epoch 1120, training loss: 62.75419616699219 = 0.09912977367639542 + 10.0 * 6.265506744384766
Epoch 1120, val loss: 0.9264588952064514
Epoch 1130, training loss: 62.70510482788086 = 0.09586621820926666 + 10.0 * 6.260923862457275
Epoch 1130, val loss: 0.9309602379798889
Epoch 1140, training loss: 62.69853210449219 = 0.09274459630250931 + 10.0 * 6.2605791091918945
Epoch 1140, val loss: 0.9355303645133972
Epoch 1150, training loss: 62.68671417236328 = 0.08974581956863403 + 10.0 * 6.259696960449219
Epoch 1150, val loss: 0.9401755332946777
Epoch 1160, training loss: 62.7674674987793 = 0.08688247948884964 + 10.0 * 6.2680583000183105
Epoch 1160, val loss: 0.9447540640830994
Epoch 1170, training loss: 62.68852233886719 = 0.0840679258108139 + 10.0 * 6.260445594787598
Epoch 1170, val loss: 0.9488888382911682
Epoch 1180, training loss: 62.66543197631836 = 0.08141551911830902 + 10.0 * 6.258401393890381
Epoch 1180, val loss: 0.9533860087394714
Epoch 1190, training loss: 62.650516510009766 = 0.07889322191476822 + 10.0 * 6.257162570953369
Epoch 1190, val loss: 0.9581857323646545
Epoch 1200, training loss: 62.6828727722168 = 0.07646363228559494 + 10.0 * 6.260641098022461
Epoch 1200, val loss: 0.9626903533935547
Epoch 1210, training loss: 62.65629959106445 = 0.074123315513134 + 10.0 * 6.258217811584473
Epoch 1210, val loss: 0.9672586917877197
Epoch 1220, training loss: 62.63915252685547 = 0.07185715436935425 + 10.0 * 6.256729602813721
Epoch 1220, val loss: 0.9716029167175293
Epoch 1230, training loss: 62.618778228759766 = 0.06972604990005493 + 10.0 * 6.2549052238464355
Epoch 1230, val loss: 0.97649085521698
Epoch 1240, training loss: 62.609100341796875 = 0.0676719918847084 + 10.0 * 6.254142761230469
Epoch 1240, val loss: 0.9808981418609619
Epoch 1250, training loss: 62.63618087768555 = 0.06570630520582199 + 10.0 * 6.257047176361084
Epoch 1250, val loss: 0.9856265783309937
Epoch 1260, training loss: 62.60747146606445 = 0.06379488855600357 + 10.0 * 6.254367828369141
Epoch 1260, val loss: 0.9899970293045044
Epoch 1270, training loss: 62.61715316772461 = 0.0619594044983387 + 10.0 * 6.255519390106201
Epoch 1270, val loss: 0.9945581555366516
Epoch 1280, training loss: 62.584205627441406 = 0.06018746271729469 + 10.0 * 6.252401828765869
Epoch 1280, val loss: 0.9990215301513672
Epoch 1290, training loss: 62.58461380004883 = 0.058507271111011505 + 10.0 * 6.252610683441162
Epoch 1290, val loss: 1.0035600662231445
Epoch 1300, training loss: 62.572509765625 = 0.0568835474550724 + 10.0 * 6.251562595367432
Epoch 1300, val loss: 1.007940649986267
Epoch 1310, training loss: 62.582054138183594 = 0.05532808601856232 + 10.0 * 6.2526726722717285
Epoch 1310, val loss: 1.0124976634979248
Epoch 1320, training loss: 62.574684143066406 = 0.053826432675123215 + 10.0 * 6.2520856857299805
Epoch 1320, val loss: 1.0171035528182983
Epoch 1330, training loss: 62.55667495727539 = 0.05236780270934105 + 10.0 * 6.250430583953857
Epoch 1330, val loss: 1.021278977394104
Epoch 1340, training loss: 62.63678741455078 = 0.05095886439085007 + 10.0 * 6.258582592010498
Epoch 1340, val loss: 1.025493860244751
Epoch 1350, training loss: 62.545379638671875 = 0.04962077736854553 + 10.0 * 6.249575614929199
Epoch 1350, val loss: 1.0298526287078857
Epoch 1360, training loss: 62.52714538574219 = 0.048320572823286057 + 10.0 * 6.24788236618042
Epoch 1360, val loss: 1.0344055891036987
Epoch 1370, training loss: 62.515995025634766 = 0.04709124565124512 + 10.0 * 6.246890068054199
Epoch 1370, val loss: 1.0386285781860352
Epoch 1380, training loss: 62.518192291259766 = 0.045905377715826035 + 10.0 * 6.247228622436523
Epoch 1380, val loss: 1.0429060459136963
Epoch 1390, training loss: 62.56207275390625 = 0.04474778100848198 + 10.0 * 6.25173282623291
Epoch 1390, val loss: 1.047104001045227
Epoch 1400, training loss: 62.513092041015625 = 0.043623100966215134 + 10.0 * 6.246946811676025
Epoch 1400, val loss: 1.0514119863510132
Epoch 1410, training loss: 62.49612808227539 = 0.04255067929625511 + 10.0 * 6.245357990264893
Epoch 1410, val loss: 1.0556151866912842
Epoch 1420, training loss: 62.49623489379883 = 0.041523728519678116 + 10.0 * 6.245471000671387
Epoch 1420, val loss: 1.0598180294036865
Epoch 1430, training loss: 62.531150817871094 = 0.040529415011405945 + 10.0 * 6.2490620613098145
Epoch 1430, val loss: 1.0640078783035278
Epoch 1440, training loss: 62.50467300415039 = 0.0395413413643837 + 10.0 * 6.2465128898620605
Epoch 1440, val loss: 1.0679680109024048
Epoch 1450, training loss: 62.49332809448242 = 0.038610052317380905 + 10.0 * 6.245471954345703
Epoch 1450, val loss: 1.0719455480575562
Epoch 1460, training loss: 62.50312805175781 = 0.03770311921834946 + 10.0 * 6.246542453765869
Epoch 1460, val loss: 1.075827717781067
Epoch 1470, training loss: 62.47380828857422 = 0.03680911660194397 + 10.0 * 6.24370002746582
Epoch 1470, val loss: 1.0797178745269775
Epoch 1480, training loss: 62.46038818359375 = 0.03597459942102432 + 10.0 * 6.242441177368164
Epoch 1480, val loss: 1.0838388204574585
Epoch 1490, training loss: 62.4922981262207 = 0.03517015650868416 + 10.0 * 6.245712757110596
Epoch 1490, val loss: 1.0879069566726685
Epoch 1500, training loss: 62.45298767089844 = 0.03437252715229988 + 10.0 * 6.241861343383789
Epoch 1500, val loss: 1.0914227962493896
Epoch 1510, training loss: 62.44770812988281 = 0.03359915688633919 + 10.0 * 6.241410732269287
Epoch 1510, val loss: 1.0953956842422485
Epoch 1520, training loss: 62.44120407104492 = 0.032867033034563065 + 10.0 * 6.240833759307861
Epoch 1520, val loss: 1.0991666316986084
Epoch 1530, training loss: 62.45458221435547 = 0.03215606138110161 + 10.0 * 6.242242336273193
Epoch 1530, val loss: 1.1029216051101685
Epoch 1540, training loss: 62.46158981323242 = 0.03145359456539154 + 10.0 * 6.243013858795166
Epoch 1540, val loss: 1.1067911386489868
Epoch 1550, training loss: 62.43472671508789 = 0.030782321467995644 + 10.0 * 6.240394592285156
Epoch 1550, val loss: 1.1105189323425293
Epoch 1560, training loss: 62.423240661621094 = 0.03012920916080475 + 10.0 * 6.239311218261719
Epoch 1560, val loss: 1.1143293380737305
Epoch 1570, training loss: 62.42657470703125 = 0.029502371326088905 + 10.0 * 6.239706993103027
Epoch 1570, val loss: 1.1180912256240845
Epoch 1580, training loss: 62.45833969116211 = 0.02889118157327175 + 10.0 * 6.242944717407227
Epoch 1580, val loss: 1.1217244863510132
Epoch 1590, training loss: 62.425315856933594 = 0.02829614281654358 + 10.0 * 6.239701747894287
Epoch 1590, val loss: 1.125093698501587
Epoch 1600, training loss: 62.428466796875 = 0.027720969170331955 + 10.0 * 6.240074634552002
Epoch 1600, val loss: 1.128691554069519
Epoch 1610, training loss: 62.44070053100586 = 0.027156341820955276 + 10.0 * 6.241354465484619
Epoch 1610, val loss: 1.1322802305221558
Epoch 1620, training loss: 62.41736602783203 = 0.026616590097546577 + 10.0 * 6.239075183868408
Epoch 1620, val loss: 1.1359679698944092
Epoch 1630, training loss: 62.39620590209961 = 0.026087427511811256 + 10.0 * 6.237011909484863
Epoch 1630, val loss: 1.1393860578536987
Epoch 1640, training loss: 62.392024993896484 = 0.025584466755390167 + 10.0 * 6.2366437911987305
Epoch 1640, val loss: 1.1429110765457153
Epoch 1650, training loss: 62.4212532043457 = 0.025096723809838295 + 10.0 * 6.239615440368652
Epoch 1650, val loss: 1.146502137184143
Epoch 1660, training loss: 62.39439392089844 = 0.024610022082924843 + 10.0 * 6.236978530883789
Epoch 1660, val loss: 1.149675965309143
Epoch 1670, training loss: 62.38225173950195 = 0.02413681335747242 + 10.0 * 6.235811710357666
Epoch 1670, val loss: 1.1529673337936401
Epoch 1680, training loss: 62.41793441772461 = 0.02368152141571045 + 10.0 * 6.239425182342529
Epoch 1680, val loss: 1.1561288833618164
Epoch 1690, training loss: 62.39571762084961 = 0.02323523908853531 + 10.0 * 6.237248420715332
Epoch 1690, val loss: 1.1598374843597412
Epoch 1700, training loss: 62.368289947509766 = 0.022808274254202843 + 10.0 * 6.234548091888428
Epoch 1700, val loss: 1.1628727912902832
Epoch 1710, training loss: 62.36161422729492 = 0.022395003587007523 + 10.0 * 6.233922004699707
Epoch 1710, val loss: 1.1662852764129639
Epoch 1720, training loss: 62.379764556884766 = 0.022000769153237343 + 10.0 * 6.235776424407959
Epoch 1720, val loss: 1.1696202754974365
Epoch 1730, training loss: 62.394344329833984 = 0.02160409465432167 + 10.0 * 6.237274169921875
Epoch 1730, val loss: 1.172748327255249
Epoch 1740, training loss: 62.36066436767578 = 0.021204667165875435 + 10.0 * 6.233945846557617
Epoch 1740, val loss: 1.1758381128311157
Epoch 1750, training loss: 62.34881591796875 = 0.020835870876908302 + 10.0 * 6.232798099517822
Epoch 1750, val loss: 1.1791040897369385
Epoch 1760, training loss: 62.36568069458008 = 0.020479504019021988 + 10.0 * 6.234519958496094
Epoch 1760, val loss: 1.182382345199585
Epoch 1770, training loss: 62.36027526855469 = 0.02011890895664692 + 10.0 * 6.234015464782715
Epoch 1770, val loss: 1.1850706338882446
Epoch 1780, training loss: 62.34728240966797 = 0.019764820113778114 + 10.0 * 6.232751846313477
Epoch 1780, val loss: 1.1882116794586182
Epoch 1790, training loss: 62.34299087524414 = 0.019429313018918037 + 10.0 * 6.232356071472168
Epoch 1790, val loss: 1.1910219192504883
Epoch 1800, training loss: 62.363224029541016 = 0.01910690777003765 + 10.0 * 6.234411716461182
Epoch 1800, val loss: 1.1941885948181152
Epoch 1810, training loss: 62.32699203491211 = 0.018786441534757614 + 10.0 * 6.230820655822754
Epoch 1810, val loss: 1.1971091032028198
Epoch 1820, training loss: 62.33154296875 = 0.018479572609066963 + 10.0 * 6.231306552886963
Epoch 1820, val loss: 1.2001463174819946
Epoch 1830, training loss: 62.38140869140625 = 0.018179884180426598 + 10.0 * 6.23632287979126
Epoch 1830, val loss: 1.2032532691955566
Epoch 1840, training loss: 62.3262825012207 = 0.017879251390695572 + 10.0 * 6.23084020614624
Epoch 1840, val loss: 1.2056500911712646
Epoch 1850, training loss: 62.31618881225586 = 0.01758824661374092 + 10.0 * 6.229859828948975
Epoch 1850, val loss: 1.2087665796279907
Epoch 1860, training loss: 62.30693817138672 = 0.017311302945017815 + 10.0 * 6.2289628982543945
Epoch 1860, val loss: 1.2116038799285889
Epoch 1870, training loss: 62.384761810302734 = 0.017040478065609932 + 10.0 * 6.236772060394287
Epoch 1870, val loss: 1.2141311168670654
Epoch 1880, training loss: 62.34804916381836 = 0.01676684431731701 + 10.0 * 6.233128547668457
Epoch 1880, val loss: 1.21726655960083
Epoch 1890, training loss: 62.322914123535156 = 0.01649191603064537 + 10.0 * 6.230642318725586
Epoch 1890, val loss: 1.219689965248108
Epoch 1900, training loss: 62.294883728027344 = 0.01624215394258499 + 10.0 * 6.2278642654418945
Epoch 1900, val loss: 1.2224085330963135
Epoch 1910, training loss: 62.293357849121094 = 0.015999777242541313 + 10.0 * 6.227735996246338
Epoch 1910, val loss: 1.2253667116165161
Epoch 1920, training loss: 62.29153823852539 = 0.015761494636535645 + 10.0 * 6.2275776863098145
Epoch 1920, val loss: 1.2278776168823242
Epoch 1930, training loss: 62.3742790222168 = 0.015530670993030071 + 10.0 * 6.235875129699707
Epoch 1930, val loss: 1.2302933931350708
Epoch 1940, training loss: 62.31868362426758 = 0.01529015600681305 + 10.0 * 6.230339527130127
Epoch 1940, val loss: 1.2333605289459229
Epoch 1950, training loss: 62.320655822753906 = 0.015061727724969387 + 10.0 * 6.230559349060059
Epoch 1950, val loss: 1.2355531454086304
Epoch 1960, training loss: 62.28095626831055 = 0.01483897864818573 + 10.0 * 6.226611614227295
Epoch 1960, val loss: 1.2383569478988647
Epoch 1970, training loss: 62.28174591064453 = 0.014626139774918556 + 10.0 * 6.226712226867676
Epoch 1970, val loss: 1.2408618927001953
Epoch 1980, training loss: 62.30026626586914 = 0.014420783147215843 + 10.0 * 6.2285847663879395
Epoch 1980, val loss: 1.2435264587402344
Epoch 1990, training loss: 62.30500030517578 = 0.014211817644536495 + 10.0 * 6.229078769683838
Epoch 1990, val loss: 1.2459977865219116
Epoch 2000, training loss: 62.28089904785156 = 0.014001271687448025 + 10.0 * 6.22668981552124
Epoch 2000, val loss: 1.2482842206954956
Epoch 2010, training loss: 62.28002166748047 = 0.013805780559778214 + 10.0 * 6.226621627807617
Epoch 2010, val loss: 1.2510311603546143
Epoch 2020, training loss: 62.29100799560547 = 0.013616588898003101 + 10.0 * 6.227738857269287
Epoch 2020, val loss: 1.253381371498108
Epoch 2030, training loss: 62.291587829589844 = 0.013427875004708767 + 10.0 * 6.227816104888916
Epoch 2030, val loss: 1.255700707435608
Epoch 2040, training loss: 62.27189636230469 = 0.013237603940069675 + 10.0 * 6.225865840911865
Epoch 2040, val loss: 1.2579169273376465
Epoch 2050, training loss: 62.25807189941406 = 0.013055388815701008 + 10.0 * 6.224501609802246
Epoch 2050, val loss: 1.2605067491531372
Epoch 2060, training loss: 62.27238845825195 = 0.012882113456726074 + 10.0 * 6.225950717926025
Epoch 2060, val loss: 1.262673258781433
Epoch 2070, training loss: 62.29569625854492 = 0.012707788497209549 + 10.0 * 6.228299140930176
Epoch 2070, val loss: 1.2648537158966064
Epoch 2080, training loss: 62.26555633544922 = 0.01253666914999485 + 10.0 * 6.225302219390869
Epoch 2080, val loss: 1.267454743385315
Epoch 2090, training loss: 62.254886627197266 = 0.012370551005005836 + 10.0 * 6.224251747131348
Epoch 2090, val loss: 1.2695798873901367
Epoch 2100, training loss: 62.27647399902344 = 0.012213143520057201 + 10.0 * 6.226426124572754
Epoch 2100, val loss: 1.2721059322357178
Epoch 2110, training loss: 62.310768127441406 = 0.012053495272994041 + 10.0 * 6.2298712730407715
Epoch 2110, val loss: 1.2740380764007568
Epoch 2120, training loss: 62.2534065246582 = 0.011885381303727627 + 10.0 * 6.224152088165283
Epoch 2120, val loss: 1.276029109954834
Epoch 2130, training loss: 62.23960876464844 = 0.011734498664736748 + 10.0 * 6.222787380218506
Epoch 2130, val loss: 1.278548002243042
Epoch 2140, training loss: 62.231529235839844 = 0.011588852852582932 + 10.0 * 6.221993923187256
Epoch 2140, val loss: 1.2807410955429077
Epoch 2150, training loss: 62.23106002807617 = 0.011446368880569935 + 10.0 * 6.22196102142334
Epoch 2150, val loss: 1.2829262018203735
Epoch 2160, training loss: 62.264625549316406 = 0.01130773313343525 + 10.0 * 6.225331783294678
Epoch 2160, val loss: 1.2853132486343384
Epoch 2170, training loss: 62.25617218017578 = 0.011163126677274704 + 10.0 * 6.224501132965088
Epoch 2170, val loss: 1.2870906591415405
Epoch 2180, training loss: 62.25160217285156 = 0.011020544916391373 + 10.0 * 6.224058151245117
Epoch 2180, val loss: 1.289078712463379
Epoch 2190, training loss: 62.23286056518555 = 0.010884872637689114 + 10.0 * 6.222197532653809
Epoch 2190, val loss: 1.2910903692245483
Epoch 2200, training loss: 62.23969650268555 = 0.010754814371466637 + 10.0 * 6.222894191741943
Epoch 2200, val loss: 1.2934032678604126
Epoch 2210, training loss: 62.243446350097656 = 0.010625021532177925 + 10.0 * 6.223282337188721
Epoch 2210, val loss: 1.2952383756637573
Epoch 2220, training loss: 62.2333869934082 = 0.010495002381503582 + 10.0 * 6.222289085388184
Epoch 2220, val loss: 1.2973912954330444
Epoch 2230, training loss: 62.21873092651367 = 0.010371062904596329 + 10.0 * 6.2208356857299805
Epoch 2230, val loss: 1.2994766235351562
Epoch 2240, training loss: 62.27925109863281 = 0.010249595157802105 + 10.0 * 6.226900100708008
Epoch 2240, val loss: 1.3009544610977173
Epoch 2250, training loss: 62.216373443603516 = 0.010126191191375256 + 10.0 * 6.2206244468688965
Epoch 2250, val loss: 1.303389310836792
Epoch 2260, training loss: 62.20940017700195 = 0.010007557459175587 + 10.0 * 6.219939231872559
Epoch 2260, val loss: 1.3052663803100586
Epoch 2270, training loss: 62.205894470214844 = 0.009893649257719517 + 10.0 * 6.219599723815918
Epoch 2270, val loss: 1.3072649240493774
Epoch 2280, training loss: 62.21189498901367 = 0.009785658679902554 + 10.0 * 6.220211029052734
Epoch 2280, val loss: 1.309375286102295
Epoch 2290, training loss: 62.25759506225586 = 0.00967638660222292 + 10.0 * 6.224791526794434
Epoch 2290, val loss: 1.3112198114395142
Epoch 2300, training loss: 62.23379898071289 = 0.009560596197843552 + 10.0 * 6.222424030303955
Epoch 2300, val loss: 1.31284499168396
Epoch 2310, training loss: 62.20912551879883 = 0.009452772326767445 + 10.0 * 6.219967365264893
Epoch 2310, val loss: 1.3151040077209473
Epoch 2320, training loss: 62.23886489868164 = 0.009347531013190746 + 10.0 * 6.222951889038086
Epoch 2320, val loss: 1.3168830871582031
Epoch 2330, training loss: 62.207115173339844 = 0.009241988882422447 + 10.0 * 6.219787120819092
Epoch 2330, val loss: 1.3185148239135742
Epoch 2340, training loss: 62.2100830078125 = 0.009139683097600937 + 10.0 * 6.220094203948975
Epoch 2340, val loss: 1.3204381465911865
Epoch 2350, training loss: 62.1937255859375 = 0.009041386656463146 + 10.0 * 6.21846866607666
Epoch 2350, val loss: 1.3224440813064575
Epoch 2360, training loss: 62.1951904296875 = 0.008946464397013187 + 10.0 * 6.218624591827393
Epoch 2360, val loss: 1.3242247104644775
Epoch 2370, training loss: 62.22856903076172 = 0.008852752856910229 + 10.0 * 6.22197151184082
Epoch 2370, val loss: 1.3259530067443848
Epoch 2380, training loss: 62.190162658691406 = 0.008752279914915562 + 10.0 * 6.218141078948975
Epoch 2380, val loss: 1.3276861906051636
Epoch 2390, training loss: 62.19630432128906 = 0.008660675957798958 + 10.0 * 6.218764305114746
Epoch 2390, val loss: 1.3292582035064697
Epoch 2400, training loss: 62.21683883666992 = 0.008569510653614998 + 10.0 * 6.220827102661133
Epoch 2400, val loss: 1.3307409286499023
Epoch 2410, training loss: 62.20902633666992 = 0.00847619865089655 + 10.0 * 6.220055103302002
Epoch 2410, val loss: 1.3328166007995605
Epoch 2420, training loss: 62.176509857177734 = 0.008386210538446903 + 10.0 * 6.216812610626221
Epoch 2420, val loss: 1.3343781232833862
Epoch 2430, training loss: 62.170082092285156 = 0.008302309550344944 + 10.0 * 6.216177940368652
Epoch 2430, val loss: 1.3363535404205322
Epoch 2440, training loss: 62.175697326660156 = 0.008220701478421688 + 10.0 * 6.216747760772705
Epoch 2440, val loss: 1.3380846977233887
Epoch 2450, training loss: 62.2076416015625 = 0.008139473386108875 + 10.0 * 6.219950199127197
Epoch 2450, val loss: 1.3398839235305786
Epoch 2460, training loss: 62.18004608154297 = 0.008054000325500965 + 10.0 * 6.217199325561523
Epoch 2460, val loss: 1.341131329536438
Epoch 2470, training loss: 62.18961715698242 = 0.007973984815180302 + 10.0 * 6.218164443969727
Epoch 2470, val loss: 1.3428202867507935
Epoch 2480, training loss: 62.19940185546875 = 0.007891745306551456 + 10.0 * 6.219151020050049
Epoch 2480, val loss: 1.3442792892456055
Epoch 2490, training loss: 62.16740417480469 = 0.007808162830770016 + 10.0 * 6.215959548950195
Epoch 2490, val loss: 1.3458771705627441
Epoch 2500, training loss: 62.153228759765625 = 0.007733408361673355 + 10.0 * 6.214549541473389
Epoch 2500, val loss: 1.3474535942077637
Epoch 2510, training loss: 62.153114318847656 = 0.007661433424800634 + 10.0 * 6.214545249938965
Epoch 2510, val loss: 1.3491770029067993
Epoch 2520, training loss: 62.16403579711914 = 0.0075910952873528 + 10.0 * 6.215644359588623
Epoch 2520, val loss: 1.3508291244506836
Epoch 2530, training loss: 62.22272872924805 = 0.007520375773310661 + 10.0 * 6.221520900726318
Epoch 2530, val loss: 1.3522945642471313
Epoch 2540, training loss: 62.209869384765625 = 0.007438166998326778 + 10.0 * 6.220242977142334
Epoch 2540, val loss: 1.3534667491912842
Epoch 2550, training loss: 62.171630859375 = 0.007368224207311869 + 10.0 * 6.216425895690918
Epoch 2550, val loss: 1.3549487590789795
Epoch 2560, training loss: 62.14961624145508 = 0.007297939155250788 + 10.0 * 6.214231967926025
Epoch 2560, val loss: 1.3563801050186157
Epoch 2570, training loss: 62.143653869628906 = 0.00723209697753191 + 10.0 * 6.213642120361328
Epoch 2570, val loss: 1.3578969240188599
Epoch 2580, training loss: 62.18674850463867 = 0.007167863193899393 + 10.0 * 6.217957973480225
Epoch 2580, val loss: 1.3590962886810303
Epoch 2590, training loss: 62.143096923828125 = 0.00709993252530694 + 10.0 * 6.213599681854248
Epoch 2590, val loss: 1.3609867095947266
Epoch 2600, training loss: 62.147274017333984 = 0.0070332991890609264 + 10.0 * 6.214024066925049
Epoch 2600, val loss: 1.362199068069458
Epoch 2610, training loss: 62.156341552734375 = 0.006969258654862642 + 10.0 * 6.214937210083008
Epoch 2610, val loss: 1.3635797500610352
Epoch 2620, training loss: 62.15509796142578 = 0.006906698923557997 + 10.0 * 6.214818954467773
Epoch 2620, val loss: 1.3648382425308228
Epoch 2630, training loss: 62.16728591918945 = 0.006845532450824976 + 10.0 * 6.216043949127197
Epoch 2630, val loss: 1.3663731813430786
Epoch 2640, training loss: 62.14619827270508 = 0.006781590171158314 + 10.0 * 6.21394157409668
Epoch 2640, val loss: 1.3676148653030396
Epoch 2650, training loss: 62.139434814453125 = 0.006720367819070816 + 10.0 * 6.213271141052246
Epoch 2650, val loss: 1.3689559698104858
Epoch 2660, training loss: 62.12783432006836 = 0.006663045380264521 + 10.0 * 6.2121171951293945
Epoch 2660, val loss: 1.3704832792282104
Epoch 2670, training loss: 62.14470291137695 = 0.006606862880289555 + 10.0 * 6.213809490203857
Epoch 2670, val loss: 1.3718113899230957
Epoch 2680, training loss: 62.16265869140625 = 0.006548754870891571 + 10.0 * 6.215610980987549
Epoch 2680, val loss: 1.3728896379470825
Epoch 2690, training loss: 62.15122985839844 = 0.006491069216281176 + 10.0 * 6.214473724365234
Epoch 2690, val loss: 1.3743302822113037
Epoch 2700, training loss: 62.13557815551758 = 0.006435472052544355 + 10.0 * 6.21291446685791
Epoch 2700, val loss: 1.3756369352340698
Epoch 2710, training loss: 62.155887603759766 = 0.006381813436746597 + 10.0 * 6.2149505615234375
Epoch 2710, val loss: 1.3767139911651611
Epoch 2720, training loss: 62.119266510009766 = 0.006326692178845406 + 10.0 * 6.211293697357178
Epoch 2720, val loss: 1.37815260887146
Epoch 2730, training loss: 62.14582443237305 = 0.0062760720029473305 + 10.0 * 6.213954925537109
Epoch 2730, val loss: 1.37950599193573
Epoch 2740, training loss: 62.1331787109375 = 0.006221434101462364 + 10.0 * 6.212695598602295
Epoch 2740, val loss: 1.3805451393127441
Epoch 2750, training loss: 62.124691009521484 = 0.00616749282926321 + 10.0 * 6.211852073669434
Epoch 2750, val loss: 1.381621241569519
Epoch 2760, training loss: 62.11552429199219 = 0.006118843797594309 + 10.0 * 6.210940361022949
Epoch 2760, val loss: 1.383070468902588
Epoch 2770, training loss: 62.11071014404297 = 0.006070688366889954 + 10.0 * 6.210464000701904
Epoch 2770, val loss: 1.384291410446167
Epoch 2780, training loss: 62.210472106933594 = 0.006026264745742083 + 10.0 * 6.220444679260254
Epoch 2780, val loss: 1.3855167627334595
Epoch 2790, training loss: 62.130985260009766 = 0.005970288999378681 + 10.0 * 6.212501525878906
Epoch 2790, val loss: 1.3865389823913574
Epoch 2800, training loss: 62.104957580566406 = 0.005921163130551577 + 10.0 * 6.209903717041016
Epoch 2800, val loss: 1.387640357017517
Epoch 2810, training loss: 62.10282897949219 = 0.005875934846699238 + 10.0 * 6.209695339202881
Epoch 2810, val loss: 1.3888896703720093
Epoch 2820, training loss: 62.10612106323242 = 0.005831233225762844 + 10.0 * 6.210028648376465
Epoch 2820, val loss: 1.3901199102401733
Epoch 2830, training loss: 62.15743637084961 = 0.005787104833871126 + 10.0 * 6.215165138244629
Epoch 2830, val loss: 1.391330361366272
Epoch 2840, training loss: 62.13711166381836 = 0.005741008557379246 + 10.0 * 6.213137149810791
Epoch 2840, val loss: 1.3923258781433105
Epoch 2850, training loss: 62.158935546875 = 0.005695050582289696 + 10.0 * 6.2153239250183105
Epoch 2850, val loss: 1.3934272527694702
Epoch 2860, training loss: 62.1160888671875 = 0.005647648125886917 + 10.0 * 6.2110443115234375
Epoch 2860, val loss: 1.3940321207046509
Epoch 2870, training loss: 62.09481430053711 = 0.005604145582765341 + 10.0 * 6.208920955657959
Epoch 2870, val loss: 1.3953136205673218
Epoch 2880, training loss: 62.09999084472656 = 0.005563438404351473 + 10.0 * 6.209443092346191
Epoch 2880, val loss: 1.3965773582458496
Epoch 2890, training loss: 62.16417694091797 = 0.00552203506231308 + 10.0 * 6.215865135192871
Epoch 2890, val loss: 1.3976246118545532
Epoch 2900, training loss: 62.116432189941406 = 0.005479646846652031 + 10.0 * 6.211095333099365
Epoch 2900, val loss: 1.3983012437820435
Epoch 2910, training loss: 62.09744644165039 = 0.005437843967229128 + 10.0 * 6.209200859069824
Epoch 2910, val loss: 1.3997811079025269
Epoch 2920, training loss: 62.095882415771484 = 0.005399211775511503 + 10.0 * 6.209048271179199
Epoch 2920, val loss: 1.4005277156829834
Epoch 2930, training loss: 62.11223602294922 = 0.00535982521250844 + 10.0 * 6.210687637329102
Epoch 2930, val loss: 1.4015488624572754
Epoch 2940, training loss: 62.094268798828125 = 0.005320992320775986 + 10.0 * 6.208894729614258
Epoch 2940, val loss: 1.4026165008544922
Epoch 2950, training loss: 62.12721252441406 = 0.005283073056489229 + 10.0 * 6.212193012237549
Epoch 2950, val loss: 1.403826117515564
Epoch 2960, training loss: 62.111087799072266 = 0.005243951454758644 + 10.0 * 6.2105841636657715
Epoch 2960, val loss: 1.4046543836593628
Epoch 2970, training loss: 62.0889778137207 = 0.0052068401128053665 + 10.0 * 6.208376884460449
Epoch 2970, val loss: 1.4056296348571777
Epoch 2980, training loss: 62.093162536621094 = 0.00517157232388854 + 10.0 * 6.208798885345459
Epoch 2980, val loss: 1.4067047834396362
Epoch 2990, training loss: 62.09797668457031 = 0.0051359375938773155 + 10.0 * 6.20928430557251
Epoch 2990, val loss: 1.4076309204101562
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 87.92984771728516 = 1.9611403942108154 + 10.0 * 8.596870422363281
Epoch 0, val loss: 1.9587926864624023
Epoch 10, training loss: 87.91545867919922 = 1.9510427713394165 + 10.0 * 8.596441268920898
Epoch 10, val loss: 1.94945228099823
Epoch 20, training loss: 87.86886596679688 = 1.938916802406311 + 10.0 * 8.592994689941406
Epoch 20, val loss: 1.937867283821106
Epoch 30, training loss: 87.60685729980469 = 1.923450231552124 + 10.0 * 8.568341255187988
Epoch 30, val loss: 1.9229406118392944
Epoch 40, training loss: 86.26776123046875 = 1.9049512147903442 + 10.0 * 8.436281204223633
Epoch 40, val loss: 1.9056487083435059
Epoch 50, training loss: 81.7271957397461 = 1.8860713243484497 + 10.0 * 7.98411226272583
Epoch 50, val loss: 1.887893795967102
Epoch 60, training loss: 77.4898910522461 = 1.8698581457138062 + 10.0 * 7.5620036125183105
Epoch 60, val loss: 1.8733456134796143
Epoch 70, training loss: 73.44068908691406 = 1.8574081659317017 + 10.0 * 7.158328533172607
Epoch 70, val loss: 1.8615214824676514
Epoch 80, training loss: 71.46183776855469 = 1.846328616142273 + 10.0 * 6.961551189422607
Epoch 80, val loss: 1.851118803024292
Epoch 90, training loss: 70.25480651855469 = 1.8357268571853638 + 10.0 * 6.841907978057861
Epoch 90, val loss: 1.8411483764648438
Epoch 100, training loss: 69.4935531616211 = 1.8252369165420532 + 10.0 * 6.76683235168457
Epoch 100, val loss: 1.8314998149871826
Epoch 110, training loss: 68.85012817382812 = 1.8149479627609253 + 10.0 * 6.703517913818359
Epoch 110, val loss: 1.8219503164291382
Epoch 120, training loss: 68.35531616210938 = 1.805602788925171 + 10.0 * 6.654971599578857
Epoch 120, val loss: 1.8128622770309448
Epoch 130, training loss: 67.99300384521484 = 1.7964823246002197 + 10.0 * 6.619651794433594
Epoch 130, val loss: 1.80405592918396
Epoch 140, training loss: 67.67012786865234 = 1.7870948314666748 + 10.0 * 6.588303089141846
Epoch 140, val loss: 1.794877290725708
Epoch 150, training loss: 67.37544250488281 = 1.7775230407714844 + 10.0 * 6.559791564941406
Epoch 150, val loss: 1.7856621742248535
Epoch 160, training loss: 67.15123748779297 = 1.7676575183868408 + 10.0 * 6.538358211517334
Epoch 160, val loss: 1.7762442827224731
Epoch 170, training loss: 66.92678833007812 = 1.756927251815796 + 10.0 * 6.516985893249512
Epoch 170, val loss: 1.766426682472229
Epoch 180, training loss: 66.72016906738281 = 1.745411992073059 + 10.0 * 6.497475624084473
Epoch 180, val loss: 1.7560075521469116
Epoch 190, training loss: 66.5799789428711 = 1.7328547239303589 + 10.0 * 6.484712600708008
Epoch 190, val loss: 1.744784951210022
Epoch 200, training loss: 66.44713592529297 = 1.7189996242523193 + 10.0 * 6.472813129425049
Epoch 200, val loss: 1.7325730323791504
Epoch 210, training loss: 66.27591705322266 = 1.704106092453003 + 10.0 * 6.457180976867676
Epoch 210, val loss: 1.719483494758606
Epoch 220, training loss: 66.14497375488281 = 1.6880061626434326 + 10.0 * 6.445696830749512
Epoch 220, val loss: 1.7054722309112549
Epoch 230, training loss: 66.02782440185547 = 1.6706453561782837 + 10.0 * 6.435717582702637
Epoch 230, val loss: 1.6904182434082031
Epoch 240, training loss: 65.98802185058594 = 1.6520143747329712 + 10.0 * 6.433600902557373
Epoch 240, val loss: 1.674210786819458
Epoch 250, training loss: 65.8418197631836 = 1.6318910121917725 + 10.0 * 6.420992851257324
Epoch 250, val loss: 1.656968593597412
Epoch 260, training loss: 65.7266616821289 = 1.6105563640594482 + 10.0 * 6.411610126495361
Epoch 260, val loss: 1.638759732246399
Epoch 270, training loss: 65.71431732177734 = 1.5877989530563354 + 10.0 * 6.412651538848877
Epoch 270, val loss: 1.6195497512817383
Epoch 280, training loss: 65.55897521972656 = 1.563936471939087 + 10.0 * 6.3995041847229
Epoch 280, val loss: 1.5993062257766724
Epoch 290, training loss: 65.46588134765625 = 1.5389940738677979 + 10.0 * 6.392688274383545
Epoch 290, val loss: 1.5783638954162598
Epoch 300, training loss: 65.38092803955078 = 1.5130600929260254 + 10.0 * 6.386786937713623
Epoch 300, val loss: 1.5567657947540283
Epoch 310, training loss: 65.3030014038086 = 1.4862674474716187 + 10.0 * 6.381673336029053
Epoch 310, val loss: 1.5346801280975342
Epoch 320, training loss: 65.32831573486328 = 1.458699345588684 + 10.0 * 6.386961460113525
Epoch 320, val loss: 1.5123040676116943
Epoch 330, training loss: 65.20843505859375 = 1.4307080507278442 + 10.0 * 6.377772331237793
Epoch 330, val loss: 1.4895685911178589
Epoch 340, training loss: 65.08804321289062 = 1.4024931192398071 + 10.0 * 6.368555068969727
Epoch 340, val loss: 1.4670054912567139
Epoch 350, training loss: 65.01973724365234 = 1.3740731477737427 + 10.0 * 6.364566326141357
Epoch 350, val loss: 1.4445773363113403
Epoch 360, training loss: 64.9760513305664 = 1.345461130142212 + 10.0 * 6.363059043884277
Epoch 360, val loss: 1.4222614765167236
Epoch 370, training loss: 64.91160583496094 = 1.3168071508407593 + 10.0 * 6.3594794273376465
Epoch 370, val loss: 1.399887204170227
Epoch 380, training loss: 64.82709503173828 = 1.2880908250808716 + 10.0 * 6.353899955749512
Epoch 380, val loss: 1.3777811527252197
Epoch 390, training loss: 64.79519653320312 = 1.2593141794204712 + 10.0 * 6.353588104248047
Epoch 390, val loss: 1.3558998107910156
Epoch 400, training loss: 64.71590423583984 = 1.23055100440979 + 10.0 * 6.348535537719727
Epoch 400, val loss: 1.3341352939605713
Epoch 410, training loss: 64.63990020751953 = 1.2019622325897217 + 10.0 * 6.343793869018555
Epoch 410, val loss: 1.3126190900802612
Epoch 420, training loss: 64.58135986328125 = 1.1733919382095337 + 10.0 * 6.34079647064209
Epoch 420, val loss: 1.291322112083435
Epoch 430, training loss: 64.53044128417969 = 1.1449049711227417 + 10.0 * 6.3385539054870605
Epoch 430, val loss: 1.2701184749603271
Epoch 440, training loss: 64.4644546508789 = 1.1164906024932861 + 10.0 * 6.33479642868042
Epoch 440, val loss: 1.2492750883102417
Epoch 450, training loss: 64.40828704833984 = 1.088259220123291 + 10.0 * 6.332003116607666
Epoch 450, val loss: 1.2287578582763672
Epoch 460, training loss: 64.37094116210938 = 1.0602960586547852 + 10.0 * 6.331064701080322
Epoch 460, val loss: 1.2085189819335938
Epoch 470, training loss: 64.30088806152344 = 1.0326623916625977 + 10.0 * 6.326822280883789
Epoch 470, val loss: 1.1885621547698975
Epoch 480, training loss: 64.24435424804688 = 1.005372166633606 + 10.0 * 6.3238983154296875
Epoch 480, val loss: 1.169073462486267
Epoch 490, training loss: 64.19963073730469 = 0.9784773588180542 + 10.0 * 6.322115421295166
Epoch 490, val loss: 1.150141954421997
Epoch 500, training loss: 64.19903564453125 = 0.9518925547599792 + 10.0 * 6.324714660644531
Epoch 500, val loss: 1.1317301988601685
Epoch 510, training loss: 64.12100219726562 = 0.9260488152503967 + 10.0 * 6.31949520111084
Epoch 510, val loss: 1.1135607957839966
Epoch 520, training loss: 64.05179595947266 = 0.9005868434906006 + 10.0 * 6.315120697021484
Epoch 520, val loss: 1.0963228940963745
Epoch 530, training loss: 64.00212097167969 = 0.8758094906806946 + 10.0 * 6.312631130218506
Epoch 530, val loss: 1.0796611309051514
Epoch 540, training loss: 63.956485748291016 = 0.8514891862869263 + 10.0 * 6.310499668121338
Epoch 540, val loss: 1.0636751651763916
Epoch 550, training loss: 63.96502685546875 = 0.8276094794273376 + 10.0 * 6.313741683959961
Epoch 550, val loss: 1.0481927394866943
Epoch 560, training loss: 63.908870697021484 = 0.8042198419570923 + 10.0 * 6.310465335845947
Epoch 560, val loss: 1.0332577228546143
Epoch 570, training loss: 63.84044647216797 = 0.7814111113548279 + 10.0 * 6.305903434753418
Epoch 570, val loss: 1.0188319683074951
Epoch 580, training loss: 63.79585266113281 = 0.7590888738632202 + 10.0 * 6.303676128387451
Epoch 580, val loss: 1.0050681829452515
Epoch 590, training loss: 63.83500671386719 = 0.7372426986694336 + 10.0 * 6.309776306152344
Epoch 590, val loss: 0.9917318820953369
Epoch 600, training loss: 63.72188186645508 = 0.7156525254249573 + 10.0 * 6.300622940063477
Epoch 600, val loss: 0.9791957139968872
Epoch 610, training loss: 63.69097137451172 = 0.6946004033088684 + 10.0 * 6.299637317657471
Epoch 610, val loss: 0.9672610759735107
Epoch 620, training loss: 63.64708709716797 = 0.6740431785583496 + 10.0 * 6.297304630279541
Epoch 620, val loss: 0.9558464884757996
Epoch 630, training loss: 63.69526290893555 = 0.6538141369819641 + 10.0 * 6.304144859313965
Epoch 630, val loss: 0.9450726509094238
Epoch 640, training loss: 63.61372756958008 = 0.6339683532714844 + 10.0 * 6.297976016998291
Epoch 640, val loss: 0.9342348575592041
Epoch 650, training loss: 63.55785369873047 = 0.6145385503768921 + 10.0 * 6.2943315505981445
Epoch 650, val loss: 0.924304187297821
Epoch 660, training loss: 63.51806640625 = 0.5955669283866882 + 10.0 * 6.292250156402588
Epoch 660, val loss: 0.9148861169815063
Epoch 670, training loss: 63.51913833618164 = 0.5770108699798584 + 10.0 * 6.294212818145752
Epoch 670, val loss: 0.9059732556343079
Epoch 680, training loss: 63.466026306152344 = 0.5587117075920105 + 10.0 * 6.290731430053711
Epoch 680, val loss: 0.8975585103034973
Epoch 690, training loss: 63.43520736694336 = 0.5408380031585693 + 10.0 * 6.289437294006348
Epoch 690, val loss: 0.8896397948265076
Epoch 700, training loss: 63.40855026245117 = 0.5233812928199768 + 10.0 * 6.288516998291016
Epoch 700, val loss: 0.8821448087692261
Epoch 710, training loss: 63.383670806884766 = 0.506329357624054 + 10.0 * 6.287734031677246
Epoch 710, val loss: 0.8750506043434143
Epoch 720, training loss: 63.364044189453125 = 0.4896714687347412 + 10.0 * 6.287437438964844
Epoch 720, val loss: 0.8683266639709473
Epoch 730, training loss: 63.348907470703125 = 0.4733431935310364 + 10.0 * 6.2875566482543945
Epoch 730, val loss: 0.8622193932533264
Epoch 740, training loss: 63.303131103515625 = 0.45736566185951233 + 10.0 * 6.284576416015625
Epoch 740, val loss: 0.8565155267715454
Epoch 750, training loss: 63.2659797668457 = 0.4419252574443817 + 10.0 * 6.282405376434326
Epoch 750, val loss: 0.8511176705360413
Epoch 760, training loss: 63.24017333984375 = 0.4268375635147095 + 10.0 * 6.2813334465026855
Epoch 760, val loss: 0.8463302254676819
Epoch 770, training loss: 63.28290557861328 = 0.4120952785015106 + 10.0 * 6.287081241607666
Epoch 770, val loss: 0.841809868812561
Epoch 780, training loss: 63.19837951660156 = 0.39775219559669495 + 10.0 * 6.280062675476074
Epoch 780, val loss: 0.8376466035842896
Epoch 790, training loss: 63.18069839477539 = 0.3838357925415039 + 10.0 * 6.279686450958252
Epoch 790, val loss: 0.8338147401809692
Epoch 800, training loss: 63.14731979370117 = 0.37032079696655273 + 10.0 * 6.277699947357178
Epoch 800, val loss: 0.8304994702339172
Epoch 810, training loss: 63.16046905517578 = 0.3571847677230835 + 10.0 * 6.280328273773193
Epoch 810, val loss: 0.8277503252029419
Epoch 820, training loss: 63.14823913574219 = 0.3444374203681946 + 10.0 * 6.2803802490234375
Epoch 820, val loss: 0.8246884346008301
Epoch 830, training loss: 63.091957092285156 = 0.3320796489715576 + 10.0 * 6.27598762512207
Epoch 830, val loss: 0.8225449323654175
Epoch 840, training loss: 63.056663513183594 = 0.3201567232608795 + 10.0 * 6.273650646209717
Epoch 840, val loss: 0.8205735087394714
Epoch 850, training loss: 63.034812927246094 = 0.30866149067878723 + 10.0 * 6.2726149559021
Epoch 850, val loss: 0.8190914988517761
Epoch 860, training loss: 63.06952667236328 = 0.2975475788116455 + 10.0 * 6.27719783782959
Epoch 860, val loss: 0.8179228901863098
Epoch 870, training loss: 63.049076080322266 = 0.2866690158843994 + 10.0 * 6.276240348815918
Epoch 870, val loss: 0.8168288469314575
Epoch 880, training loss: 62.98469161987305 = 0.27624791860580444 + 10.0 * 6.270844459533691
Epoch 880, val loss: 0.8160954117774963
Epoch 890, training loss: 62.9674072265625 = 0.26626479625701904 + 10.0 * 6.270113945007324
Epoch 890, val loss: 0.8157506585121155
Epoch 900, training loss: 62.963783264160156 = 0.2566443681716919 + 10.0 * 6.270713806152344
Epoch 900, val loss: 0.8157424330711365
Epoch 910, training loss: 62.93917465209961 = 0.24737593531608582 + 10.0 * 6.269179821014404
Epoch 910, val loss: 0.8159257173538208
Epoch 920, training loss: 62.95438003540039 = 0.2384115606546402 + 10.0 * 6.271596908569336
Epoch 920, val loss: 0.8163865208625793
Epoch 930, training loss: 62.89713668823242 = 0.22973988950252533 + 10.0 * 6.266739845275879
Epoch 930, val loss: 0.8171053528785706
Epoch 940, training loss: 62.881431579589844 = 0.22146183252334595 + 10.0 * 6.265996932983398
Epoch 940, val loss: 0.8182863593101501
Epoch 950, training loss: 62.85771942138672 = 0.21355901658535004 + 10.0 * 6.264416217803955
Epoch 950, val loss: 0.8195673227310181
Epoch 960, training loss: 62.8458366394043 = 0.20593374967575073 + 10.0 * 6.26399040222168
Epoch 960, val loss: 0.8211736083030701
Epoch 970, training loss: 62.91628646850586 = 0.1985989362001419 + 10.0 * 6.271768569946289
Epoch 970, val loss: 0.8229615092277527
Epoch 980, training loss: 62.826866149902344 = 0.19151265919208527 + 10.0 * 6.263535499572754
Epoch 980, val loss: 0.824643611907959
Epoch 990, training loss: 62.80539321899414 = 0.18471409380435944 + 10.0 * 6.262067794799805
Epoch 990, val loss: 0.8267226219177246
Epoch 1000, training loss: 62.79372787475586 = 0.17822782695293427 + 10.0 * 6.261549949645996
Epoch 1000, val loss: 0.8290647864341736
Epoch 1010, training loss: 62.80935287475586 = 0.17201729118824005 + 10.0 * 6.263733863830566
Epoch 1010, val loss: 0.8314710259437561
Epoch 1020, training loss: 62.82258605957031 = 0.16600769758224487 + 10.0 * 6.265657901763916
Epoch 1020, val loss: 0.8339430689811707
Epoch 1030, training loss: 62.7608528137207 = 0.16018441319465637 + 10.0 * 6.260066986083984
Epoch 1030, val loss: 0.8367327451705933
Epoch 1040, training loss: 62.7331428527832 = 0.15468864142894745 + 10.0 * 6.257845401763916
Epoch 1040, val loss: 0.8395509719848633
Epoch 1050, training loss: 62.72772979736328 = 0.14941717684268951 + 10.0 * 6.25783109664917
Epoch 1050, val loss: 0.8426923155784607
Epoch 1060, training loss: 62.748661041259766 = 0.1443433165550232 + 10.0 * 6.26043176651001
Epoch 1060, val loss: 0.8459327816963196
Epoch 1070, training loss: 62.75006103515625 = 0.13942566514015198 + 10.0 * 6.261063575744629
Epoch 1070, val loss: 0.8490979671478271
Epoch 1080, training loss: 62.69705581665039 = 0.13474707305431366 + 10.0 * 6.25623083114624
Epoch 1080, val loss: 0.8523727059364319
Epoch 1090, training loss: 62.68589401245117 = 0.13025696575641632 + 10.0 * 6.255563735961914
Epoch 1090, val loss: 0.8559409379959106
Epoch 1100, training loss: 62.719757080078125 = 0.12593834102153778 + 10.0 * 6.2593817710876465
Epoch 1100, val loss: 0.8595419526100159
Epoch 1110, training loss: 62.67119216918945 = 0.12181051820516586 + 10.0 * 6.254938125610352
Epoch 1110, val loss: 0.8631125092506409
Epoch 1120, training loss: 62.649200439453125 = 0.11784444004297256 + 10.0 * 6.253135681152344
Epoch 1120, val loss: 0.8668813705444336
Epoch 1130, training loss: 62.63882064819336 = 0.11404699832201004 + 10.0 * 6.252477169036865
Epoch 1130, val loss: 0.8707072734832764
Epoch 1140, training loss: 62.65180206298828 = 0.11041103303432465 + 10.0 * 6.254138946533203
Epoch 1140, val loss: 0.874618411064148
Epoch 1150, training loss: 62.69439697265625 = 0.10692764073610306 + 10.0 * 6.258747100830078
Epoch 1150, val loss: 0.878438413143158
Epoch 1160, training loss: 62.647926330566406 = 0.10346147418022156 + 10.0 * 6.254446506500244
Epoch 1160, val loss: 0.8822872042655945
Epoch 1170, training loss: 62.610477447509766 = 0.10023002326488495 + 10.0 * 6.2510247230529785
Epoch 1170, val loss: 0.8861933946609497
Epoch 1180, training loss: 62.59202194213867 = 0.09712915867567062 + 10.0 * 6.2494893074035645
Epoch 1180, val loss: 0.8902513980865479
Epoch 1190, training loss: 62.6004638671875 = 0.09416323900222778 + 10.0 * 6.2506303787231445
Epoch 1190, val loss: 0.8943080306053162
Epoch 1200, training loss: 62.59001922607422 = 0.09129878878593445 + 10.0 * 6.249872207641602
Epoch 1200, val loss: 0.8983020782470703
Epoch 1210, training loss: 62.56831741333008 = 0.08853849023580551 + 10.0 * 6.2479777336120605
Epoch 1210, val loss: 0.9023170471191406
Epoch 1220, training loss: 62.589778900146484 = 0.08590757101774216 + 10.0 * 6.250387191772461
Epoch 1220, val loss: 0.9063836336135864
Epoch 1230, training loss: 62.556495666503906 = 0.083351269364357 + 10.0 * 6.247314453125
Epoch 1230, val loss: 0.9104544520378113
Epoch 1240, training loss: 62.549949645996094 = 0.08090604096651077 + 10.0 * 6.246904373168945
Epoch 1240, val loss: 0.9146802425384521
Epoch 1250, training loss: 62.55140686035156 = 0.07855898886919022 + 10.0 * 6.247284889221191
Epoch 1250, val loss: 0.9188246130943298
Epoch 1260, training loss: 62.58402633666992 = 0.07628191262483597 + 10.0 * 6.250774383544922
Epoch 1260, val loss: 0.9228675961494446
Epoch 1270, training loss: 62.531795501708984 = 0.07409240305423737 + 10.0 * 6.245770454406738
Epoch 1270, val loss: 0.926921010017395
Epoch 1280, training loss: 62.52382278442383 = 0.0719882920384407 + 10.0 * 6.24518346786499
Epoch 1280, val loss: 0.9309855103492737
Epoch 1290, training loss: 62.52781295776367 = 0.06998301297426224 + 10.0 * 6.245782852172852
Epoch 1290, val loss: 0.9351524114608765
Epoch 1300, training loss: 62.52586364746094 = 0.06803272664546967 + 10.0 * 6.24578332901001
Epoch 1300, val loss: 0.9393340945243835
Epoch 1310, training loss: 62.53073501586914 = 0.06615165621042252 + 10.0 * 6.246458530426025
Epoch 1310, val loss: 0.943419873714447
Epoch 1320, training loss: 62.49583053588867 = 0.06433788686990738 + 10.0 * 6.243149280548096
Epoch 1320, val loss: 0.9475454092025757
Epoch 1330, training loss: 62.48859405517578 = 0.0626060888171196 + 10.0 * 6.242598533630371
Epoch 1330, val loss: 0.9516619443893433
Epoch 1340, training loss: 62.4908332824707 = 0.06093970313668251 + 10.0 * 6.242989540100098
Epoch 1340, val loss: 0.9558558464050293
Epoch 1350, training loss: 62.534141540527344 = 0.059325020760297775 + 10.0 * 6.247481346130371
Epoch 1350, val loss: 0.9599562883377075
Epoch 1360, training loss: 62.506500244140625 = 0.057725489139556885 + 10.0 * 6.244877815246582
Epoch 1360, val loss: 0.9639526605606079
Epoch 1370, training loss: 62.50798416137695 = 0.05621747672557831 + 10.0 * 6.245176792144775
Epoch 1370, val loss: 0.9679726958274841
Epoch 1380, training loss: 62.473079681396484 = 0.05475384369492531 + 10.0 * 6.241832733154297
Epoch 1380, val loss: 0.9719325304031372
Epoch 1390, training loss: 62.45338821411133 = 0.05334840342402458 + 10.0 * 6.240004062652588
Epoch 1390, val loss: 0.9760873317718506
Epoch 1400, training loss: 62.450042724609375 = 0.052001215517520905 + 10.0 * 6.239804267883301
Epoch 1400, val loss: 0.9800955057144165
Epoch 1410, training loss: 62.50893020629883 = 0.05070347711443901 + 10.0 * 6.245822429656982
Epoch 1410, val loss: 0.9841090440750122
Epoch 1420, training loss: 62.49308776855469 = 0.049429818987846375 + 10.0 * 6.244365692138672
Epoch 1420, val loss: 0.9879457950592041
Epoch 1430, training loss: 62.451107025146484 = 0.048186518251895905 + 10.0 * 6.240292072296143
Epoch 1430, val loss: 0.9917799234390259
Epoch 1440, training loss: 62.42574691772461 = 0.04699971154332161 + 10.0 * 6.237874507904053
Epoch 1440, val loss: 0.995745062828064
Epoch 1450, training loss: 62.4295539855957 = 0.04586448147892952 + 10.0 * 6.238368988037109
Epoch 1450, val loss: 0.9996558427810669
Epoch 1460, training loss: 62.4941291809082 = 0.04475919157266617 + 10.0 * 6.244936943054199
Epoch 1460, val loss: 1.0035884380340576
Epoch 1470, training loss: 62.447349548339844 = 0.04369328171014786 + 10.0 * 6.240365505218506
Epoch 1470, val loss: 1.0072734355926514
Epoch 1480, training loss: 62.42060852050781 = 0.04264512658119202 + 10.0 * 6.237796306610107
Epoch 1480, val loss: 1.011125922203064
Epoch 1490, training loss: 62.40728759765625 = 0.041654929518699646 + 10.0 * 6.236563205718994
Epoch 1490, val loss: 1.014900803565979
Epoch 1500, training loss: 62.429481506347656 = 0.040698032826185226 + 10.0 * 6.23887825012207
Epoch 1500, val loss: 1.018677830696106
Epoch 1510, training loss: 62.40018081665039 = 0.03975798562169075 + 10.0 * 6.236042499542236
Epoch 1510, val loss: 1.0224665403366089
Epoch 1520, training loss: 62.39778518676758 = 0.038848329335451126 + 10.0 * 6.235893726348877
Epoch 1520, val loss: 1.0262435674667358
Epoch 1530, training loss: 62.40889358520508 = 0.037976689636707306 + 10.0 * 6.237092018127441
Epoch 1530, val loss: 1.0299278497695923
Epoch 1540, training loss: 62.41497802734375 = 0.037127137184143066 + 10.0 * 6.2377848625183105
Epoch 1540, val loss: 1.0336028337478638
Epoch 1550, training loss: 62.39535903930664 = 0.03629845753312111 + 10.0 * 6.23590612411499
Epoch 1550, val loss: 1.0373742580413818
Epoch 1560, training loss: 62.39266586303711 = 0.03550407290458679 + 10.0 * 6.235716342926025
Epoch 1560, val loss: 1.0410521030426025
Epoch 1570, training loss: 62.39228820800781 = 0.03473161160945892 + 10.0 * 6.235755443572998
Epoch 1570, val loss: 1.0446974039077759
Epoch 1580, training loss: 62.37854766845703 = 0.033979106694459915 + 10.0 * 6.234457015991211
Epoch 1580, val loss: 1.0482819080352783
Epoch 1590, training loss: 62.37758255004883 = 0.0332566574215889 + 10.0 * 6.234432697296143
Epoch 1590, val loss: 1.0519100427627563
Epoch 1600, training loss: 62.38203430175781 = 0.03255395218729973 + 10.0 * 6.23494815826416
Epoch 1600, val loss: 1.0554835796356201
Epoch 1610, training loss: 62.35847854614258 = 0.03186459466814995 + 10.0 * 6.232661247253418
Epoch 1610, val loss: 1.0590529441833496
Epoch 1620, training loss: 62.400856018066406 = 0.03120993822813034 + 10.0 * 6.236964702606201
Epoch 1620, val loss: 1.0625979900360107
Epoch 1630, training loss: 62.41855239868164 = 0.03055836260318756 + 10.0 * 6.238799095153809
Epoch 1630, val loss: 1.0662132501602173
Epoch 1640, training loss: 62.362632751464844 = 0.029914429411292076 + 10.0 * 6.233271598815918
Epoch 1640, val loss: 1.069326400756836
Epoch 1650, training loss: 62.346431732177734 = 0.029311731457710266 + 10.0 * 6.2317118644714355
Epoch 1650, val loss: 1.073037028312683
Epoch 1660, training loss: 62.34360122680664 = 0.028732415288686752 + 10.0 * 6.231486797332764
Epoch 1660, val loss: 1.0764672756195068
Epoch 1670, training loss: 62.41233444213867 = 0.028167136013507843 + 10.0 * 6.23841667175293
Epoch 1670, val loss: 1.0798966884613037
Epoch 1680, training loss: 62.37440872192383 = 0.027601970359683037 + 10.0 * 6.234680652618408
Epoch 1680, val loss: 1.0831092596054077
Epoch 1690, training loss: 62.339683532714844 = 0.027061225846409798 + 10.0 * 6.23126220703125
Epoch 1690, val loss: 1.0865216255187988
Epoch 1700, training loss: 62.35463333129883 = 0.026541458442807198 + 10.0 * 6.232809066772461
Epoch 1700, val loss: 1.089920163154602
Epoch 1710, training loss: 62.34459686279297 = 0.02602832019329071 + 10.0 * 6.231856822967529
Epoch 1710, val loss: 1.093070149421692
Epoch 1720, training loss: 62.32419204711914 = 0.025534598156809807 + 10.0 * 6.229865550994873
Epoch 1720, val loss: 1.0962828397750854
Epoch 1730, training loss: 62.325443267822266 = 0.025057760998606682 + 10.0 * 6.230038642883301
Epoch 1730, val loss: 1.099575161933899
Epoch 1740, training loss: 62.36268997192383 = 0.024592164903879166 + 10.0 * 6.233809471130371
Epoch 1740, val loss: 1.1026729345321655
Epoch 1750, training loss: 62.3263053894043 = 0.024130888283252716 + 10.0 * 6.230217456817627
Epoch 1750, val loss: 1.1059547662734985
Epoch 1760, training loss: 62.318851470947266 = 0.02369171753525734 + 10.0 * 6.22951602935791
Epoch 1760, val loss: 1.1090372800827026
Epoch 1770, training loss: 62.326786041259766 = 0.0232629906386137 + 10.0 * 6.230352401733398
Epoch 1770, val loss: 1.1121289730072021
Epoch 1780, training loss: 62.300262451171875 = 0.022843899205327034 + 10.0 * 6.2277421951293945
Epoch 1780, val loss: 1.1153740882873535
Epoch 1790, training loss: 62.30699157714844 = 0.02244267798960209 + 10.0 * 6.228455066680908
Epoch 1790, val loss: 1.1184380054473877
Epoch 1800, training loss: 62.3636360168457 = 0.022053079679608345 + 10.0 * 6.234158515930176
Epoch 1800, val loss: 1.1214667558670044
Epoch 1810, training loss: 62.32038116455078 = 0.0216531865298748 + 10.0 * 6.229872703552246
Epoch 1810, val loss: 1.124234676361084
Epoch 1820, training loss: 62.295738220214844 = 0.0212691742926836 + 10.0 * 6.227446556091309
Epoch 1820, val loss: 1.127326488494873
Epoch 1830, training loss: 62.284568786621094 = 0.020909244194626808 + 10.0 * 6.22636604309082
Epoch 1830, val loss: 1.130318522453308
Epoch 1840, training loss: 62.281917572021484 = 0.020559057593345642 + 10.0 * 6.226136207580566
Epoch 1840, val loss: 1.1333262920379639
Epoch 1850, training loss: 62.355079650878906 = 0.02021745592355728 + 10.0 * 6.233486175537109
Epoch 1850, val loss: 1.136106014251709
Epoch 1860, training loss: 62.3214111328125 = 0.019869184121489525 + 10.0 * 6.230154037475586
Epoch 1860, val loss: 1.1390960216522217
Epoch 1870, training loss: 62.287071228027344 = 0.01953618787229061 + 10.0 * 6.2267537117004395
Epoch 1870, val loss: 1.1418286561965942
Epoch 1880, training loss: 62.27737045288086 = 0.01921827718615532 + 10.0 * 6.225815296173096
Epoch 1880, val loss: 1.1447887420654297
Epoch 1890, training loss: 62.28215408325195 = 0.01890977844595909 + 10.0 * 6.226324558258057
Epoch 1890, val loss: 1.1476771831512451
Epoch 1900, training loss: 62.31943893432617 = 0.01860639452934265 + 10.0 * 6.230082988739014
Epoch 1900, val loss: 1.150462031364441
Epoch 1910, training loss: 62.317142486572266 = 0.01829717867076397 + 10.0 * 6.229884624481201
Epoch 1910, val loss: 1.1531835794448853
Epoch 1920, training loss: 62.292903900146484 = 0.01800442859530449 + 10.0 * 6.227489948272705
Epoch 1920, val loss: 1.1558949947357178
Epoch 1930, training loss: 62.264041900634766 = 0.017718670889735222 + 10.0 * 6.224632263183594
Epoch 1930, val loss: 1.1585280895233154
Epoch 1940, training loss: 62.27470016479492 = 0.017446264624595642 + 10.0 * 6.2257256507873535
Epoch 1940, val loss: 1.1613075733184814
Epoch 1950, training loss: 62.279781341552734 = 0.017175601795315742 + 10.0 * 6.226260662078857
Epoch 1950, val loss: 1.1639978885650635
Epoch 1960, training loss: 62.2907829284668 = 0.0169118270277977 + 10.0 * 6.227387428283691
Epoch 1960, val loss: 1.1668668985366821
Epoch 1970, training loss: 62.25408935546875 = 0.01665182039141655 + 10.0 * 6.223743915557861
Epoch 1970, val loss: 1.1693320274353027
Epoch 1980, training loss: 62.246177673339844 = 0.016402291133999825 + 10.0 * 6.222977638244629
Epoch 1980, val loss: 1.1720764636993408
Epoch 1990, training loss: 62.25196075439453 = 0.016162259504199028 + 10.0 * 6.2235798835754395
Epoch 1990, val loss: 1.1748162508010864
Epoch 2000, training loss: 62.29710388183594 = 0.015928931534290314 + 10.0 * 6.2281174659729
Epoch 2000, val loss: 1.1774122714996338
Epoch 2010, training loss: 62.2496452331543 = 0.015680577605962753 + 10.0 * 6.223396301269531
Epoch 2010, val loss: 1.179682970046997
Epoch 2020, training loss: 62.2490348815918 = 0.0154497055336833 + 10.0 * 6.223358631134033
Epoch 2020, val loss: 1.182421326637268
Epoch 2030, training loss: 62.29913330078125 = 0.015231456607580185 + 10.0 * 6.228390216827393
Epoch 2030, val loss: 1.18488347530365
Epoch 2040, training loss: 62.260650634765625 = 0.015008438378572464 + 10.0 * 6.224564552307129
Epoch 2040, val loss: 1.1873080730438232
Epoch 2050, training loss: 62.25059127807617 = 0.01479350682348013 + 10.0 * 6.2235798835754395
Epoch 2050, val loss: 1.1897082328796387
Epoch 2060, training loss: 62.2316780090332 = 0.014581599272787571 + 10.0 * 6.221709728240967
Epoch 2060, val loss: 1.1923229694366455
Epoch 2070, training loss: 62.28946304321289 = 0.014380675740540028 + 10.0 * 6.227508068084717
Epoch 2070, val loss: 1.1945984363555908
Epoch 2080, training loss: 62.23521041870117 = 0.014178268611431122 + 10.0 * 6.222103118896484
Epoch 2080, val loss: 1.1971629858016968
Epoch 2090, training loss: 62.22511672973633 = 0.013984020799398422 + 10.0 * 6.221113204956055
Epoch 2090, val loss: 1.1994487047195435
Epoch 2100, training loss: 62.23731994628906 = 0.013795146718621254 + 10.0 * 6.222352504730225
Epoch 2100, val loss: 1.2019004821777344
Epoch 2110, training loss: 62.221473693847656 = 0.013607414439320564 + 10.0 * 6.2207865715026855
Epoch 2110, val loss: 1.2042174339294434
Epoch 2120, training loss: 62.276336669921875 = 0.013424993492662907 + 10.0 * 6.226291179656982
Epoch 2120, val loss: 1.2064793109893799
Epoch 2130, training loss: 62.24380111694336 = 0.013241119682788849 + 10.0 * 6.223055839538574
Epoch 2130, val loss: 1.208815336227417
Epoch 2140, training loss: 62.22611999511719 = 0.013061326928436756 + 10.0 * 6.221305847167969
Epoch 2140, val loss: 1.211057424545288
Epoch 2150, training loss: 62.20916748046875 = 0.012891688384115696 + 10.0 * 6.219627857208252
Epoch 2150, val loss: 1.2134003639221191
Epoch 2160, training loss: 62.2019157409668 = 0.012728388421237469 + 10.0 * 6.218918800354004
Epoch 2160, val loss: 1.215847134590149
Epoch 2170, training loss: 62.284183502197266 = 0.012572144158184528 + 10.0 * 6.227160930633545
Epoch 2170, val loss: 1.2183282375335693
Epoch 2180, training loss: 62.222408294677734 = 0.012402278371155262 + 10.0 * 6.221000671386719
Epoch 2180, val loss: 1.2199649810791016
Epoch 2190, training loss: 62.21059036254883 = 0.01224039401859045 + 10.0 * 6.219834804534912
Epoch 2190, val loss: 1.2224384546279907
Epoch 2200, training loss: 62.19587326049805 = 0.012086985632777214 + 10.0 * 6.21837854385376
Epoch 2200, val loss: 1.2245182991027832
Epoch 2210, training loss: 62.19483947753906 = 0.011940010823309422 + 10.0 * 6.218289852142334
Epoch 2210, val loss: 1.2267613410949707
Epoch 2220, training loss: 62.30350112915039 = 0.01179784256964922 + 10.0 * 6.229170322418213
Epoch 2220, val loss: 1.2288399934768677
Epoch 2230, training loss: 62.233524322509766 = 0.011648737825453281 + 10.0 * 6.222187519073486
Epoch 2230, val loss: 1.2308961153030396
Epoch 2240, training loss: 62.230735778808594 = 0.011502988636493683 + 10.0 * 6.221923351287842
Epoch 2240, val loss: 1.232961893081665
Epoch 2250, training loss: 62.18211364746094 = 0.01135845948010683 + 10.0 * 6.217075347900391
Epoch 2250, val loss: 1.2350037097930908
Epoch 2260, training loss: 62.196441650390625 = 0.011224090121686459 + 10.0 * 6.218522071838379
Epoch 2260, val loss: 1.237256407737732
Epoch 2270, training loss: 62.23225784301758 = 0.011094508692622185 + 10.0 * 6.222116470336914
Epoch 2270, val loss: 1.2392938137054443
Epoch 2280, training loss: 62.20369338989258 = 0.010961047373712063 + 10.0 * 6.219273567199707
Epoch 2280, val loss: 1.2411407232284546
Epoch 2290, training loss: 62.187034606933594 = 0.01083094347268343 + 10.0 * 6.217620372772217
Epoch 2290, val loss: 1.2432957887649536
Epoch 2300, training loss: 62.173580169677734 = 0.010705403052270412 + 10.0 * 6.216287612915039
Epoch 2300, val loss: 1.2452383041381836
Epoch 2310, training loss: 62.194305419921875 = 0.010586376301944256 + 10.0 * 6.218371868133545
Epoch 2310, val loss: 1.247258186340332
Epoch 2320, training loss: 62.20377731323242 = 0.010463318787515163 + 10.0 * 6.21933126449585
Epoch 2320, val loss: 1.2491421699523926
Epoch 2330, training loss: 62.249183654785156 = 0.010337550193071365 + 10.0 * 6.223884582519531
Epoch 2330, val loss: 1.2508891820907593
Epoch 2340, training loss: 62.18101501464844 = 0.010219582356512547 + 10.0 * 6.2170796394348145
Epoch 2340, val loss: 1.25290846824646
Epoch 2350, training loss: 62.167266845703125 = 0.010102651081979275 + 10.0 * 6.215716361999512
Epoch 2350, val loss: 1.2548245191574097
Epoch 2360, training loss: 62.162384033203125 = 0.009993813931941986 + 10.0 * 6.21523904800415
Epoch 2360, val loss: 1.2569012641906738
Epoch 2370, training loss: 62.18569564819336 = 0.009888357482850552 + 10.0 * 6.217580795288086
Epoch 2370, val loss: 1.2588558197021484
Epoch 2380, training loss: 62.191524505615234 = 0.009777804836630821 + 10.0 * 6.218174934387207
Epoch 2380, val loss: 1.2606006860733032
Epoch 2390, training loss: 62.180423736572266 = 0.009668786078691483 + 10.0 * 6.217075347900391
Epoch 2390, val loss: 1.2623435258865356
Epoch 2400, training loss: 62.19070053100586 = 0.009563947096467018 + 10.0 * 6.218113899230957
Epoch 2400, val loss: 1.2642871141433716
Epoch 2410, training loss: 62.16469192504883 = 0.009457655251026154 + 10.0 * 6.215523719787598
Epoch 2410, val loss: 1.2660695314407349
Epoch 2420, training loss: 62.17763900756836 = 0.009358800947666168 + 10.0 * 6.216828346252441
Epoch 2420, val loss: 1.2679928541183472
Epoch 2430, training loss: 62.16227340698242 = 0.009259494952857494 + 10.0 * 6.215301513671875
Epoch 2430, val loss: 1.2698179483413696
Epoch 2440, training loss: 62.15946578979492 = 0.009163795970380306 + 10.0 * 6.215030193328857
Epoch 2440, val loss: 1.2715059518814087
Epoch 2450, training loss: 62.175785064697266 = 0.009070123545825481 + 10.0 * 6.216671466827393
Epoch 2450, val loss: 1.273376703262329
Epoch 2460, training loss: 62.16599655151367 = 0.008975060656666756 + 10.0 * 6.215702056884766
Epoch 2460, val loss: 1.2751271724700928
Epoch 2470, training loss: 62.15692901611328 = 0.00888059101998806 + 10.0 * 6.214804649353027
Epoch 2470, val loss: 1.276626706123352
Epoch 2480, training loss: 62.1942253112793 = 0.008789571933448315 + 10.0 * 6.218543529510498
Epoch 2480, val loss: 1.2782375812530518
Epoch 2490, training loss: 62.16314697265625 = 0.008697432465851307 + 10.0 * 6.215445041656494
Epoch 2490, val loss: 1.2800129652023315
Epoch 2500, training loss: 62.1467170715332 = 0.008608699776232243 + 10.0 * 6.213810920715332
Epoch 2500, val loss: 1.2815641164779663
Epoch 2510, training loss: 62.13780212402344 = 0.008523576892912388 + 10.0 * 6.21292781829834
Epoch 2510, val loss: 1.2833349704742432
Epoch 2520, training loss: 62.15969467163086 = 0.008440941572189331 + 10.0 * 6.21512508392334
Epoch 2520, val loss: 1.2850005626678467
Epoch 2530, training loss: 62.15017318725586 = 0.008356891572475433 + 10.0 * 6.214181423187256
Epoch 2530, val loss: 1.2865632772445679
Epoch 2540, training loss: 62.14215087890625 = 0.008277502842247486 + 10.0 * 6.213387489318848
Epoch 2540, val loss: 1.2882065773010254
Epoch 2550, training loss: 62.14115524291992 = 0.008197540417313576 + 10.0 * 6.213295936584473
Epoch 2550, val loss: 1.289781093597412
Epoch 2560, training loss: 62.18339920043945 = 0.008119719102978706 + 10.0 * 6.217527866363525
Epoch 2560, val loss: 1.29134202003479
Epoch 2570, training loss: 62.150047302246094 = 0.00803871639072895 + 10.0 * 6.214200973510742
Epoch 2570, val loss: 1.292958378791809
Epoch 2580, training loss: 62.14212417602539 = 0.00796165969222784 + 10.0 * 6.21341609954834
Epoch 2580, val loss: 1.2943904399871826
Epoch 2590, training loss: 62.14970397949219 = 0.00788826309144497 + 10.0 * 6.214181423187256
Epoch 2590, val loss: 1.295926570892334
Epoch 2600, training loss: 62.15687561035156 = 0.007814371027052402 + 10.0 * 6.214906215667725
Epoch 2600, val loss: 1.297174096107483
Epoch 2610, training loss: 62.13373947143555 = 0.007740413770079613 + 10.0 * 6.212599754333496
Epoch 2610, val loss: 1.2988276481628418
Epoch 2620, training loss: 62.122352600097656 = 0.007669014390558004 + 10.0 * 6.211468696594238
Epoch 2620, val loss: 1.3003811836242676
Epoch 2630, training loss: 62.13236999511719 = 0.0076001593843102455 + 10.0 * 6.212477207183838
Epoch 2630, val loss: 1.3018165826797485
Epoch 2640, training loss: 62.17078399658203 = 0.007531499955803156 + 10.0 * 6.216325283050537
Epoch 2640, val loss: 1.303398609161377
Epoch 2650, training loss: 62.178977966308594 = 0.007461801636964083 + 10.0 * 6.217151641845703
Epoch 2650, val loss: 1.3048588037490845
Epoch 2660, training loss: 62.14730453491211 = 0.007391094695776701 + 10.0 * 6.213991165161133
Epoch 2660, val loss: 1.3059494495391846
Epoch 2670, training loss: 62.112876892089844 = 0.007324954029172659 + 10.0 * 6.210555076599121
Epoch 2670, val loss: 1.3076095581054688
Epoch 2680, training loss: 62.11064910888672 = 0.007262727245688438 + 10.0 * 6.210338592529297
Epoch 2680, val loss: 1.309109091758728
Epoch 2690, training loss: 62.12621307373047 = 0.007203089538961649 + 10.0 * 6.2119011878967285
Epoch 2690, val loss: 1.310579538345337
Epoch 2700, training loss: 62.13850784301758 = 0.0071400925517082214 + 10.0 * 6.213136672973633
Epoch 2700, val loss: 1.3119674921035767
Epoch 2710, training loss: 62.14753723144531 = 0.0070751747116446495 + 10.0 * 6.214046001434326
Epoch 2710, val loss: 1.3131060600280762
Epoch 2720, training loss: 62.12249755859375 = 0.007014238741248846 + 10.0 * 6.211548328399658
Epoch 2720, val loss: 1.3144558668136597
Epoch 2730, training loss: 62.097171783447266 = 0.006952730473130941 + 10.0 * 6.20902156829834
Epoch 2730, val loss: 1.3159500360488892
Epoch 2740, training loss: 62.104312896728516 = 0.006896444130688906 + 10.0 * 6.209741592407227
Epoch 2740, val loss: 1.317413568496704
Epoch 2750, training loss: 62.166683197021484 = 0.006842085160315037 + 10.0 * 6.215983867645264
Epoch 2750, val loss: 1.3186614513397217
Epoch 2760, training loss: 62.15968704223633 = 0.006779720075428486 + 10.0 * 6.2152910232543945
Epoch 2760, val loss: 1.319793701171875
Epoch 2770, training loss: 62.11470413208008 = 0.006718033459037542 + 10.0 * 6.210798740386963
Epoch 2770, val loss: 1.3210945129394531
Epoch 2780, training loss: 62.0955696105957 = 0.006663977634161711 + 10.0 * 6.208890438079834
Epoch 2780, val loss: 1.3224338293075562
Epoch 2790, training loss: 62.0888557434082 = 0.006611802149564028 + 10.0 * 6.208224296569824
Epoch 2790, val loss: 1.3238235712051392
Epoch 2800, training loss: 62.103485107421875 = 0.006560742389410734 + 10.0 * 6.209692478179932
Epoch 2800, val loss: 1.325036883354187
Epoch 2810, training loss: 62.12606430053711 = 0.0065070223063230515 + 10.0 * 6.211955547332764
Epoch 2810, val loss: 1.326232671737671
Epoch 2820, training loss: 62.094173431396484 = 0.006450159940868616 + 10.0 * 6.2087721824646
Epoch 2820, val loss: 1.3275383710861206
Epoch 2830, training loss: 62.10627365112305 = 0.006398242898285389 + 10.0 * 6.209987640380859
Epoch 2830, val loss: 1.3287327289581299
Epoch 2840, training loss: 62.13087463378906 = 0.006346496753394604 + 10.0 * 6.2124528884887695
Epoch 2840, val loss: 1.329711675643921
Epoch 2850, training loss: 62.104705810546875 = 0.00629472965374589 + 10.0 * 6.209841251373291
Epoch 2850, val loss: 1.3311182260513306
Epoch 2860, training loss: 62.08445739746094 = 0.006246303208172321 + 10.0 * 6.207821369171143
Epoch 2860, val loss: 1.3323503732681274
Epoch 2870, training loss: 62.080360412597656 = 0.006199244409799576 + 10.0 * 6.20741605758667
Epoch 2870, val loss: 1.3335596323013306
Epoch 2880, training loss: 62.0949592590332 = 0.006153205409646034 + 10.0 * 6.208880424499512
Epoch 2880, val loss: 1.3348263502120972
Epoch 2890, training loss: 62.13459396362305 = 0.006105569191277027 + 10.0 * 6.212848663330078
Epoch 2890, val loss: 1.3359524011611938
Epoch 2900, training loss: 62.107215881347656 = 0.006056780461221933 + 10.0 * 6.210115909576416
Epoch 2900, val loss: 1.3369418382644653
Epoch 2910, training loss: 62.07929611206055 = 0.006010059732943773 + 10.0 * 6.2073283195495605
Epoch 2910, val loss: 1.3381110429763794
Epoch 2920, training loss: 62.07563400268555 = 0.005966877099126577 + 10.0 * 6.206966876983643
Epoch 2920, val loss: 1.339354157447815
Epoch 2930, training loss: 62.07866287231445 = 0.0059250337071716785 + 10.0 * 6.207273960113525
Epoch 2930, val loss: 1.340531349182129
Epoch 2940, training loss: 62.13548278808594 = 0.005884175654500723 + 10.0 * 6.2129597663879395
Epoch 2940, val loss: 1.3416637182235718
Epoch 2950, training loss: 62.10136413574219 = 0.005835087038576603 + 10.0 * 6.209552764892578
Epoch 2950, val loss: 1.3427571058273315
Epoch 2960, training loss: 62.09067916870117 = 0.005792147014290094 + 10.0 * 6.208488941192627
Epoch 2960, val loss: 1.3437620401382446
Epoch 2970, training loss: 62.07651138305664 = 0.005748429335653782 + 10.0 * 6.207076072692871
Epoch 2970, val loss: 1.3446677923202515
Epoch 2980, training loss: 62.08904266357422 = 0.005708443000912666 + 10.0 * 6.208333492279053
Epoch 2980, val loss: 1.3457735776901245
Epoch 2990, training loss: 62.08220291137695 = 0.005667583551257849 + 10.0 * 6.207653522491455
Epoch 2990, val loss: 1.3469473123550415
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8202424881391671
=== training gcn model ===
Epoch 0, training loss: 87.91191864013672 = 1.9431493282318115 + 10.0 * 8.596877098083496
Epoch 0, val loss: 1.941848635673523
Epoch 10, training loss: 87.89848327636719 = 1.9331810474395752 + 10.0 * 8.596529960632324
Epoch 10, val loss: 1.9322408437728882
Epoch 20, training loss: 87.85794830322266 = 1.9209812879562378 + 10.0 * 8.593696594238281
Epoch 20, val loss: 1.9199957847595215
Epoch 30, training loss: 87.6130142211914 = 1.9055362939834595 + 10.0 * 8.570748329162598
Epoch 30, val loss: 1.9044047594070435
Epoch 40, training loss: 85.90563201904297 = 1.8865033388137817 + 10.0 * 8.401912689208984
Epoch 40, val loss: 1.8854725360870361
Epoch 50, training loss: 80.14730072021484 = 1.865504503250122 + 10.0 * 7.828180313110352
Epoch 50, val loss: 1.865031123161316
Epoch 60, training loss: 75.58470916748047 = 1.8510127067565918 + 10.0 * 7.3733696937561035
Epoch 60, val loss: 1.8521394729614258
Epoch 70, training loss: 72.8579330444336 = 1.840538740158081 + 10.0 * 7.101739883422852
Epoch 70, val loss: 1.8421566486358643
Epoch 80, training loss: 71.29784393310547 = 1.8303872346878052 + 10.0 * 6.946744918823242
Epoch 80, val loss: 1.8323370218276978
Epoch 90, training loss: 70.11577606201172 = 1.819576621055603 + 10.0 * 6.829619407653809
Epoch 90, val loss: 1.8222776651382446
Epoch 100, training loss: 69.22808837890625 = 1.809197187423706 + 10.0 * 6.741888999938965
Epoch 100, val loss: 1.812435269355774
Epoch 110, training loss: 68.61295318603516 = 1.7991002798080444 + 10.0 * 6.681385040283203
Epoch 110, val loss: 1.8027710914611816
Epoch 120, training loss: 68.19095611572266 = 1.7886931896209717 + 10.0 * 6.640225887298584
Epoch 120, val loss: 1.7927523851394653
Epoch 130, training loss: 67.84833526611328 = 1.777659296989441 + 10.0 * 6.607067584991455
Epoch 130, val loss: 1.7822129726409912
Epoch 140, training loss: 67.5789566040039 = 1.7659332752227783 + 10.0 * 6.581302642822266
Epoch 140, val loss: 1.7712197303771973
Epoch 150, training loss: 67.3639907836914 = 1.7533562183380127 + 10.0 * 6.561063289642334
Epoch 150, val loss: 1.759590983390808
Epoch 160, training loss: 67.14633178710938 = 1.7396708726882935 + 10.0 * 6.540666580200195
Epoch 160, val loss: 1.7471959590911865
Epoch 170, training loss: 66.95657348632812 = 1.7248704433441162 + 10.0 * 6.523170471191406
Epoch 170, val loss: 1.7339295148849487
Epoch 180, training loss: 66.82695770263672 = 1.7086586952209473 + 10.0 * 6.511829853057861
Epoch 180, val loss: 1.719516634941101
Epoch 190, training loss: 66.65522766113281 = 1.6908427476882935 + 10.0 * 6.496438503265381
Epoch 190, val loss: 1.7039029598236084
Epoch 200, training loss: 66.50633239746094 = 1.6715195178985596 + 10.0 * 6.4834818840026855
Epoch 200, val loss: 1.6869451999664307
Epoch 210, training loss: 66.36991119384766 = 1.6504621505737305 + 10.0 * 6.471944808959961
Epoch 210, val loss: 1.6685672998428345
Epoch 220, training loss: 66.26238250732422 = 1.6276460886001587 + 10.0 * 6.463473320007324
Epoch 220, val loss: 1.648754358291626
Epoch 230, training loss: 66.13964080810547 = 1.6030770540237427 + 10.0 * 6.453656196594238
Epoch 230, val loss: 1.6274492740631104
Epoch 240, training loss: 66.01469421386719 = 1.5769063234329224 + 10.0 * 6.443778991699219
Epoch 240, val loss: 1.6049420833587646
Epoch 250, training loss: 65.9383544921875 = 1.5491756200790405 + 10.0 * 6.43891716003418
Epoch 250, val loss: 1.581243634223938
Epoch 260, training loss: 65.80227661132812 = 1.5199559926986694 + 10.0 * 6.428231716156006
Epoch 260, val loss: 1.5563888549804688
Epoch 270, training loss: 65.69596862792969 = 1.489640474319458 + 10.0 * 6.420632839202881
Epoch 270, val loss: 1.5308362245559692
Epoch 280, training loss: 65.60888671875 = 1.458315372467041 + 10.0 * 6.415057182312012
Epoch 280, val loss: 1.5046947002410889
Epoch 290, training loss: 65.58280944824219 = 1.4261775016784668 + 10.0 * 6.41566276550293
Epoch 290, val loss: 1.4781194925308228
Epoch 300, training loss: 65.4277572631836 = 1.3937064409255981 + 10.0 * 6.40340518951416
Epoch 300, val loss: 1.4515166282653809
Epoch 310, training loss: 65.3138198852539 = 1.3611456155776978 + 10.0 * 6.395267009735107
Epoch 310, val loss: 1.4250586032867432
Epoch 320, training loss: 65.2193832397461 = 1.3286532163619995 + 10.0 * 6.389072895050049
Epoch 320, val loss: 1.3989782333374023
Epoch 330, training loss: 65.19373321533203 = 1.2962794303894043 + 10.0 * 6.389745712280273
Epoch 330, val loss: 1.37324857711792
Epoch 340, training loss: 65.05748748779297 = 1.2640843391418457 + 10.0 * 6.379340171813965
Epoch 340, val loss: 1.3479223251342773
Epoch 350, training loss: 64.96916198730469 = 1.2324477434158325 + 10.0 * 6.373671531677246
Epoch 350, val loss: 1.3232885599136353
Epoch 360, training loss: 64.89783477783203 = 1.2013143301010132 + 10.0 * 6.369651794433594
Epoch 360, val loss: 1.2994014024734497
Epoch 370, training loss: 64.8491439819336 = 1.1706677675247192 + 10.0 * 6.367847919464111
Epoch 370, val loss: 1.2761130332946777
Epoch 380, training loss: 64.75387573242188 = 1.1405665874481201 + 10.0 * 6.361330986022949
Epoch 380, val loss: 1.2535227537155151
Epoch 390, training loss: 64.68346405029297 = 1.1113102436065674 + 10.0 * 6.357215404510498
Epoch 390, val loss: 1.2320079803466797
Epoch 400, training loss: 64.62281036376953 = 1.0827382802963257 + 10.0 * 6.354007244110107
Epoch 400, val loss: 1.2112972736358643
Epoch 410, training loss: 64.60758972167969 = 1.0548248291015625 + 10.0 * 6.355276107788086
Epoch 410, val loss: 1.1914252042770386
Epoch 420, training loss: 64.53292083740234 = 1.0273377895355225 + 10.0 * 6.350558280944824
Epoch 420, val loss: 1.1721220016479492
Epoch 430, training loss: 64.45513916015625 = 1.0006306171417236 + 10.0 * 6.345450401306152
Epoch 430, val loss: 1.1536762714385986
Epoch 440, training loss: 64.38970947265625 = 0.9746088981628418 + 10.0 * 6.341509819030762
Epoch 440, val loss: 1.1361013650894165
Epoch 450, training loss: 64.43462371826172 = 0.9491204619407654 + 10.0 * 6.348550319671631
Epoch 450, val loss: 1.119123101234436
Epoch 460, training loss: 64.30957794189453 = 0.9240668416023254 + 10.0 * 6.338551044464111
Epoch 460, val loss: 1.102879524230957
Epoch 470, training loss: 64.24878692626953 = 0.8997830748558044 + 10.0 * 6.334900856018066
Epoch 470, val loss: 1.0874253511428833
Epoch 480, training loss: 64.19851684570312 = 0.8759816288948059 + 10.0 * 6.332253456115723
Epoch 480, val loss: 1.0726794004440308
Epoch 490, training loss: 64.1591567993164 = 0.8527699112892151 + 10.0 * 6.330638408660889
Epoch 490, val loss: 1.0586576461791992
Epoch 500, training loss: 64.100341796875 = 0.8300280570983887 + 10.0 * 6.32703161239624
Epoch 500, val loss: 1.0453556776046753
Epoch 510, training loss: 64.06342315673828 = 0.8078221678733826 + 10.0 * 6.325560092926025
Epoch 510, val loss: 1.032591700553894
Epoch 520, training loss: 64.185791015625 = 0.7862277626991272 + 10.0 * 6.339956283569336
Epoch 520, val loss: 1.0207751989364624
Epoch 530, training loss: 63.987548828125 = 0.7647194862365723 + 10.0 * 6.322282791137695
Epoch 530, val loss: 1.0090214014053345
Epoch 540, training loss: 63.951995849609375 = 0.743954062461853 + 10.0 * 6.320804119110107
Epoch 540, val loss: 0.9982233047485352
Epoch 550, training loss: 63.90116882324219 = 0.7237074971199036 + 10.0 * 6.317746162414551
Epoch 550, val loss: 0.9881705045700073
Epoch 560, training loss: 63.85799789428711 = 0.7038753032684326 + 10.0 * 6.3154120445251465
Epoch 560, val loss: 0.978808581829071
Epoch 570, training loss: 63.82024383544922 = 0.6843780875205994 + 10.0 * 6.313586235046387
Epoch 570, val loss: 0.9700371623039246
Epoch 580, training loss: 63.92044448852539 = 0.6652962565422058 + 10.0 * 6.325514793395996
Epoch 580, val loss: 0.9619110226631165
Epoch 590, training loss: 63.7755012512207 = 0.6464501619338989 + 10.0 * 6.312905311584473
Epoch 590, val loss: 0.954014778137207
Epoch 600, training loss: 63.734458923339844 = 0.6280763149261475 + 10.0 * 6.310638427734375
Epoch 600, val loss: 0.9468683004379272
Epoch 610, training loss: 63.7044677734375 = 0.6100977659225464 + 10.0 * 6.309437274932861
Epoch 610, val loss: 0.9403647780418396
Epoch 620, training loss: 63.66173553466797 = 0.5925142765045166 + 10.0 * 6.30692195892334
Epoch 620, val loss: 0.9343624114990234
Epoch 630, training loss: 63.691444396972656 = 0.5753740072250366 + 10.0 * 6.3116068840026855
Epoch 630, val loss: 0.9290590286254883
Epoch 640, training loss: 63.60654830932617 = 0.5583457946777344 + 10.0 * 6.3048200607299805
Epoch 640, val loss: 0.9238104224205017
Epoch 650, training loss: 63.560340881347656 = 0.5418708920478821 + 10.0 * 6.301846981048584
Epoch 650, val loss: 0.9194750189781189
Epoch 660, training loss: 63.5357780456543 = 0.5257533192634583 + 10.0 * 6.301002502441406
Epoch 660, val loss: 0.9155815839767456
Epoch 670, training loss: 63.53392791748047 = 0.5099352598190308 + 10.0 * 6.302399635314941
Epoch 670, val loss: 0.9120479822158813
Epoch 680, training loss: 63.50185012817383 = 0.49437862634658813 + 10.0 * 6.300747394561768
Epoch 680, val loss: 0.908922016620636
Epoch 690, training loss: 63.45558166503906 = 0.47927382588386536 + 10.0 * 6.297630786895752
Epoch 690, val loss: 0.9063717126846313
Epoch 700, training loss: 63.42455291748047 = 0.464623361825943 + 10.0 * 6.295992851257324
Epoch 700, val loss: 0.9045081734657288
Epoch 710, training loss: 63.45573043823242 = 0.4503575563430786 + 10.0 * 6.300537109375
Epoch 710, val loss: 0.9029195308685303
Epoch 720, training loss: 63.37507629394531 = 0.4363085627555847 + 10.0 * 6.293876647949219
Epoch 720, val loss: 0.9015538692474365
Epoch 730, training loss: 63.37175369262695 = 0.4227261245250702 + 10.0 * 6.294902801513672
Epoch 730, val loss: 0.9007450342178345
Epoch 740, training loss: 63.32203674316406 = 0.40955835580825806 + 10.0 * 6.291247844696045
Epoch 740, val loss: 0.9003809690475464
Epoch 750, training loss: 63.29409408569336 = 0.39675018191337585 + 10.0 * 6.289734363555908
Epoch 750, val loss: 0.9004496335983276
Epoch 760, training loss: 63.354713439941406 = 0.38433048129081726 + 10.0 * 6.2970380783081055
Epoch 760, val loss: 0.900952935218811
Epoch 770, training loss: 63.314579010009766 = 0.3720952868461609 + 10.0 * 6.294248104095459
Epoch 770, val loss: 0.9011589884757996
Epoch 780, training loss: 63.22386932373047 = 0.36020752787590027 + 10.0 * 6.2863664627075195
Epoch 780, val loss: 0.9021576642990112
Epoch 790, training loss: 63.20545196533203 = 0.3487102687358856 + 10.0 * 6.285674095153809
Epoch 790, val loss: 0.9035143852233887
Epoch 800, training loss: 63.2124137878418 = 0.3375299274921417 + 10.0 * 6.2874884605407715
Epoch 800, val loss: 0.9050723910331726
Epoch 810, training loss: 63.18568420410156 = 0.3266482949256897 + 10.0 * 6.285903453826904
Epoch 810, val loss: 0.9069176912307739
Epoch 820, training loss: 63.13963317871094 = 0.316086083650589 + 10.0 * 6.282354831695557
Epoch 820, val loss: 0.9090003967285156
Epoch 830, training loss: 63.123695373535156 = 0.3058100938796997 + 10.0 * 6.2817888259887695
Epoch 830, val loss: 0.9113603234291077
Epoch 840, training loss: 63.13908386230469 = 0.2958334684371948 + 10.0 * 6.284325122833252
Epoch 840, val loss: 0.9140526056289673
Epoch 850, training loss: 63.0925178527832 = 0.28608912229537964 + 10.0 * 6.280642509460449
Epoch 850, val loss: 0.9168513417243958
Epoch 860, training loss: 63.068870544433594 = 0.2766202986240387 + 10.0 * 6.2792253494262695
Epoch 860, val loss: 0.9199918508529663
Epoch 870, training loss: 63.153507232666016 = 0.26736924052238464 + 10.0 * 6.288613796234131
Epoch 870, val loss: 0.9230978488922119
Epoch 880, training loss: 63.03413391113281 = 0.2584294378757477 + 10.0 * 6.2775702476501465
Epoch 880, val loss: 0.926292359828949
Epoch 890, training loss: 63.017520904541016 = 0.2497202306985855 + 10.0 * 6.276780128479004
Epoch 890, val loss: 0.9298087358474731
Epoch 900, training loss: 62.995391845703125 = 0.24135218560695648 + 10.0 * 6.27540397644043
Epoch 900, val loss: 0.933715283870697
Epoch 910, training loss: 62.97174072265625 = 0.23324349522590637 + 10.0 * 6.273849964141846
Epoch 910, val loss: 0.9378466010093689
Epoch 920, training loss: 62.96835708618164 = 0.22539499402046204 + 10.0 * 6.274296283721924
Epoch 920, val loss: 0.9420303702354431
Epoch 930, training loss: 62.99692916870117 = 0.21776436269283295 + 10.0 * 6.277916431427002
Epoch 930, val loss: 0.946419358253479
Epoch 940, training loss: 62.93950653076172 = 0.21027898788452148 + 10.0 * 6.272922992706299
Epoch 940, val loss: 0.9506183862686157
Epoch 950, training loss: 62.91478729248047 = 0.203065425157547 + 10.0 * 6.271172523498535
Epoch 950, val loss: 0.9552311897277832
Epoch 960, training loss: 62.91751480102539 = 0.19616557657718658 + 10.0 * 6.272134780883789
Epoch 960, val loss: 0.9599727988243103
Epoch 970, training loss: 62.93867492675781 = 0.18943870067596436 + 10.0 * 6.274923801422119
Epoch 970, val loss: 0.9647577404975891
Epoch 980, training loss: 62.91238784790039 = 0.18299098312854767 + 10.0 * 6.272939682006836
Epoch 980, val loss: 0.9697229266166687
Epoch 990, training loss: 62.86365509033203 = 0.176743745803833 + 10.0 * 6.268691062927246
Epoch 990, val loss: 0.9746500849723816
Epoch 1000, training loss: 62.848480224609375 = 0.17074649035930634 + 10.0 * 6.267773628234863
Epoch 1000, val loss: 0.9800218343734741
Epoch 1010, training loss: 62.89350128173828 = 0.16496840119361877 + 10.0 * 6.272853374481201
Epoch 1010, val loss: 0.9853537082672119
Epoch 1020, training loss: 62.84695816040039 = 0.15933533012866974 + 10.0 * 6.268762111663818
Epoch 1020, val loss: 0.9903679490089417
Epoch 1030, training loss: 62.85551834106445 = 0.1539309322834015 + 10.0 * 6.270158767700195
Epoch 1030, val loss: 0.9959449768066406
Epoch 1040, training loss: 62.87055206298828 = 0.14868104457855225 + 10.0 * 6.272187232971191
Epoch 1040, val loss: 1.0012065172195435
Epoch 1050, training loss: 62.80869674682617 = 0.14370471239089966 + 10.0 * 6.2664995193481445
Epoch 1050, val loss: 1.0067602396011353
Epoch 1060, training loss: 62.77759552001953 = 0.13885793089866638 + 10.0 * 6.26387357711792
Epoch 1060, val loss: 1.012401819229126
Epoch 1070, training loss: 62.767189025878906 = 0.1342400312423706 + 10.0 * 6.2632951736450195
Epoch 1070, val loss: 1.0180283784866333
Epoch 1080, training loss: 62.775611877441406 = 0.12977665662765503 + 10.0 * 6.264583587646484
Epoch 1080, val loss: 1.0238345861434937
Epoch 1090, training loss: 62.79093933105469 = 0.12547306716442108 + 10.0 * 6.266546726226807
Epoch 1090, val loss: 1.0293635129928589
Epoch 1100, training loss: 62.7615966796875 = 0.12135928124189377 + 10.0 * 6.264023780822754
Epoch 1100, val loss: 1.0352519750595093
Epoch 1110, training loss: 62.76243209838867 = 0.11735278367996216 + 10.0 * 6.264508247375488
Epoch 1110, val loss: 1.0409716367721558
Epoch 1120, training loss: 62.7230339050293 = 0.11351026594638824 + 10.0 * 6.260952472686768
Epoch 1120, val loss: 1.0467668771743774
Epoch 1130, training loss: 62.70988464355469 = 0.10981769114732742 + 10.0 * 6.260006904602051
Epoch 1130, val loss: 1.0526131391525269
Epoch 1140, training loss: 62.70264434814453 = 0.1062876433134079 + 10.0 * 6.2596354484558105
Epoch 1140, val loss: 1.0584418773651123
Epoch 1150, training loss: 62.796199798583984 = 0.10292379558086395 + 10.0 * 6.269327640533447
Epoch 1150, val loss: 1.0643656253814697
Epoch 1160, training loss: 62.706764221191406 = 0.09957684576511383 + 10.0 * 6.260718822479248
Epoch 1160, val loss: 1.0700736045837402
Epoch 1170, training loss: 62.680233001708984 = 0.09638814628124237 + 10.0 * 6.2583842277526855
Epoch 1170, val loss: 1.075549840927124
Epoch 1180, training loss: 62.67071533203125 = 0.09337268769741058 + 10.0 * 6.257734298706055
Epoch 1180, val loss: 1.0816611051559448
Epoch 1190, training loss: 62.653194427490234 = 0.0904754251241684 + 10.0 * 6.256271839141846
Epoch 1190, val loss: 1.0874286890029907
Epoch 1200, training loss: 62.6811637878418 = 0.08770692348480225 + 10.0 * 6.259345531463623
Epoch 1200, val loss: 1.0932862758636475
Epoch 1210, training loss: 62.675537109375 = 0.08499550819396973 + 10.0 * 6.259054183959961
Epoch 1210, val loss: 1.099147081375122
Epoch 1220, training loss: 62.668121337890625 = 0.08237245678901672 + 10.0 * 6.258574962615967
Epoch 1220, val loss: 1.1046051979064941
Epoch 1230, training loss: 62.649723052978516 = 0.07983891665935516 + 10.0 * 6.256988525390625
Epoch 1230, val loss: 1.1103109121322632
Epoch 1240, training loss: 62.614925384521484 = 0.07746019214391708 + 10.0 * 6.253746509552002
Epoch 1240, val loss: 1.116170048713684
Epoch 1250, training loss: 62.61619186401367 = 0.0751776471734047 + 10.0 * 6.254101753234863
Epoch 1250, val loss: 1.1219755411148071
Epoch 1260, training loss: 62.69908905029297 = 0.07299552112817764 + 10.0 * 6.262609481811523
Epoch 1260, val loss: 1.1278644800186157
Epoch 1270, training loss: 62.65188980102539 = 0.07079287618398666 + 10.0 * 6.2581095695495605
Epoch 1270, val loss: 1.132999300956726
Epoch 1280, training loss: 62.6262092590332 = 0.06872455775737762 + 10.0 * 6.255748271942139
Epoch 1280, val loss: 1.1388142108917236
Epoch 1290, training loss: 62.62440872192383 = 0.06673793494701385 + 10.0 * 6.255766868591309
Epoch 1290, val loss: 1.1443339586257935
Epoch 1300, training loss: 62.58732223510742 = 0.06481774151325226 + 10.0 * 6.2522501945495605
Epoch 1300, val loss: 1.1500189304351807
Epoch 1310, training loss: 62.59027862548828 = 0.06298080086708069 + 10.0 * 6.252729892730713
Epoch 1310, val loss: 1.155507206916809
Epoch 1320, training loss: 62.59251022338867 = 0.061215609312057495 + 10.0 * 6.253129482269287
Epoch 1320, val loss: 1.161081314086914
Epoch 1330, training loss: 62.59602737426758 = 0.05951996147632599 + 10.0 * 6.253650665283203
Epoch 1330, val loss: 1.1666405200958252
Epoch 1340, training loss: 62.5629768371582 = 0.05787030607461929 + 10.0 * 6.2505106925964355
Epoch 1340, val loss: 1.1721479892730713
Epoch 1350, training loss: 62.61246109008789 = 0.05627475306391716 + 10.0 * 6.255618572235107
Epoch 1350, val loss: 1.17751944065094
Epoch 1360, training loss: 62.561866760253906 = 0.05474833399057388 + 10.0 * 6.250711917877197
Epoch 1360, val loss: 1.1829012632369995
Epoch 1370, training loss: 62.55075454711914 = 0.05328869819641113 + 10.0 * 6.249746799468994
Epoch 1370, val loss: 1.1884838342666626
Epoch 1380, training loss: 62.54172134399414 = 0.05187702551484108 + 10.0 * 6.248984336853027
Epoch 1380, val loss: 1.1938649415969849
Epoch 1390, training loss: 62.5650749206543 = 0.050511620938777924 + 10.0 * 6.251456260681152
Epoch 1390, val loss: 1.1991952657699585
Epoch 1400, training loss: 62.56404495239258 = 0.04918534681200981 + 10.0 * 6.251485824584961
Epoch 1400, val loss: 1.204360842704773
Epoch 1410, training loss: 62.55252456665039 = 0.04789530485868454 + 10.0 * 6.250463008880615
Epoch 1410, val loss: 1.2097047567367554
Epoch 1420, training loss: 62.51785659790039 = 0.04667643830180168 + 10.0 * 6.24711799621582
Epoch 1420, val loss: 1.2150299549102783
Epoch 1430, training loss: 62.51362609863281 = 0.045496292412281036 + 10.0 * 6.24681282043457
Epoch 1430, val loss: 1.22036612033844
Epoch 1440, training loss: 62.51049041748047 = 0.044360920786857605 + 10.0 * 6.246613025665283
Epoch 1440, val loss: 1.2256006002426147
Epoch 1450, training loss: 62.553653717041016 = 0.043253906071186066 + 10.0 * 6.251039981842041
Epoch 1450, val loss: 1.230715036392212
Epoch 1460, training loss: 62.562747955322266 = 0.04218687862157822 + 10.0 * 6.252056121826172
Epoch 1460, val loss: 1.2355966567993164
Epoch 1470, training loss: 62.513885498046875 = 0.04115258902311325 + 10.0 * 6.2472734451293945
Epoch 1470, val loss: 1.2408808469772339
Epoch 1480, training loss: 62.488319396972656 = 0.04016869515180588 + 10.0 * 6.244814872741699
Epoch 1480, val loss: 1.2459182739257812
Epoch 1490, training loss: 62.47515869140625 = 0.03921279311180115 + 10.0 * 6.243594646453857
Epoch 1490, val loss: 1.251027226448059
Epoch 1500, training loss: 62.50602722167969 = 0.03830639272928238 + 10.0 * 6.246771812438965
Epoch 1500, val loss: 1.256210446357727
Epoch 1510, training loss: 62.534698486328125 = 0.03741585090756416 + 10.0 * 6.249728202819824
Epoch 1510, val loss: 1.2612148523330688
Epoch 1520, training loss: 62.47420883178711 = 0.0365108884871006 + 10.0 * 6.243769645690918
Epoch 1520, val loss: 1.265649437904358
Epoch 1530, training loss: 62.46681594848633 = 0.03566768020391464 + 10.0 * 6.243114948272705
Epoch 1530, val loss: 1.2706598043441772
Epoch 1540, training loss: 62.47105026245117 = 0.03487610071897507 + 10.0 * 6.243617057800293
Epoch 1540, val loss: 1.2756977081298828
Epoch 1550, training loss: 62.49330520629883 = 0.03409349545836449 + 10.0 * 6.2459211349487305
Epoch 1550, val loss: 1.280530571937561
Epoch 1560, training loss: 62.470367431640625 = 0.03333144634962082 + 10.0 * 6.243703365325928
Epoch 1560, val loss: 1.2851287126541138
Epoch 1570, training loss: 62.467506408691406 = 0.03260614722967148 + 10.0 * 6.243489742279053
Epoch 1570, val loss: 1.2902450561523438
Epoch 1580, training loss: 62.506771087646484 = 0.03190238028764725 + 10.0 * 6.2474870681762695
Epoch 1580, val loss: 1.2949531078338623
Epoch 1590, training loss: 62.45921325683594 = 0.031182756647467613 + 10.0 * 6.24280309677124
Epoch 1590, val loss: 1.2992606163024902
Epoch 1600, training loss: 62.42930221557617 = 0.03051948919892311 + 10.0 * 6.239878177642822
Epoch 1600, val loss: 1.3039774894714355
Epoch 1610, training loss: 62.43466567993164 = 0.02988247014582157 + 10.0 * 6.240478038787842
Epoch 1610, val loss: 1.3087784051895142
Epoch 1620, training loss: 62.44706344604492 = 0.029256707057356834 + 10.0 * 6.241780757904053
Epoch 1620, val loss: 1.31326425075531
Epoch 1630, training loss: 62.483150482177734 = 0.02864142134785652 + 10.0 * 6.245450973510742
Epoch 1630, val loss: 1.3175653219223022
Epoch 1640, training loss: 62.47172927856445 = 0.0280560702085495 + 10.0 * 6.2443671226501465
Epoch 1640, val loss: 1.3223339319229126
Epoch 1650, training loss: 62.422611236572266 = 0.027468815445899963 + 10.0 * 6.239514350891113
Epoch 1650, val loss: 1.3266850709915161
Epoch 1660, training loss: 62.41332244873047 = 0.026922067627310753 + 10.0 * 6.238640308380127
Epoch 1660, val loss: 1.3311712741851807
Epoch 1670, training loss: 62.4058723449707 = 0.02638600580394268 + 10.0 * 6.237948417663574
Epoch 1670, val loss: 1.335707187652588
Epoch 1680, training loss: 62.49699783325195 = 0.02586997300386429 + 10.0 * 6.24711275100708
Epoch 1680, val loss: 1.3402001857757568
Epoch 1690, training loss: 62.42977523803711 = 0.025357691571116447 + 10.0 * 6.240441799163818
Epoch 1690, val loss: 1.3441948890686035
Epoch 1700, training loss: 62.411376953125 = 0.024859948083758354 + 10.0 * 6.238651752471924
Epoch 1700, val loss: 1.3486818075180054
Epoch 1710, training loss: 62.40874099731445 = 0.024387385696172714 + 10.0 * 6.2384352684021
Epoch 1710, val loss: 1.3528324365615845
Epoch 1720, training loss: 62.415977478027344 = 0.023931799456477165 + 10.0 * 6.239204406738281
Epoch 1720, val loss: 1.3573771715164185
Epoch 1730, training loss: 62.42926788330078 = 0.02347586303949356 + 10.0 * 6.240579128265381
Epoch 1730, val loss: 1.3614510297775269
Epoch 1740, training loss: 62.38460922241211 = 0.02301124297082424 + 10.0 * 6.236159801483154
Epoch 1740, val loss: 1.3652647733688354
Epoch 1750, training loss: 62.37302017211914 = 0.022593248635530472 + 10.0 * 6.235042572021484
Epoch 1750, val loss: 1.3694432973861694
Epoch 1760, training loss: 62.367916107177734 = 0.022181663662195206 + 10.0 * 6.2345733642578125
Epoch 1760, val loss: 1.3736180067062378
Epoch 1770, training loss: 62.37239074707031 = 0.021788250654935837 + 10.0 * 6.235060214996338
Epoch 1770, val loss: 1.3777694702148438
Epoch 1780, training loss: 62.4853515625 = 0.021401548758149147 + 10.0 * 6.246395111083984
Epoch 1780, val loss: 1.381710410118103
Epoch 1790, training loss: 62.406185150146484 = 0.02100791037082672 + 10.0 * 6.238517761230469
Epoch 1790, val loss: 1.3858004808425903
Epoch 1800, training loss: 62.3685188293457 = 0.020631369203329086 + 10.0 * 6.23478889465332
Epoch 1800, val loss: 1.3897513151168823
Epoch 1810, training loss: 62.38107681274414 = 0.020271120592951775 + 10.0 * 6.236080646514893
Epoch 1810, val loss: 1.3936513662338257
Epoch 1820, training loss: 62.36720275878906 = 0.019920991733670235 + 10.0 * 6.2347283363342285
Epoch 1820, val loss: 1.3976064920425415
Epoch 1830, training loss: 62.37248611450195 = 0.01958680897951126 + 10.0 * 6.235289573669434
Epoch 1830, val loss: 1.4016577005386353
Epoch 1840, training loss: 62.36172103881836 = 0.019253002479672432 + 10.0 * 6.234246730804443
Epoch 1840, val loss: 1.4055180549621582
Epoch 1850, training loss: 62.38902282714844 = 0.018932921811938286 + 10.0 * 6.237009048461914
Epoch 1850, val loss: 1.4095028638839722
Epoch 1860, training loss: 62.429161071777344 = 0.018617207184433937 + 10.0 * 6.241054534912109
Epoch 1860, val loss: 1.4133529663085938
Epoch 1870, training loss: 62.36465835571289 = 0.018285557627677917 + 10.0 * 6.234637260437012
Epoch 1870, val loss: 1.4165093898773193
Epoch 1880, training loss: 62.341331481933594 = 0.017992796376347542 + 10.0 * 6.232333660125732
Epoch 1880, val loss: 1.4204915761947632
Epoch 1890, training loss: 62.33457946777344 = 0.017701398581266403 + 10.0 * 6.231688022613525
Epoch 1890, val loss: 1.4240925312042236
Epoch 1900, training loss: 62.384639739990234 = 0.01742096245288849 + 10.0 * 6.236721992492676
Epoch 1900, val loss: 1.4275658130645752
Epoch 1910, training loss: 62.3553466796875 = 0.01713523454964161 + 10.0 * 6.233820915222168
Epoch 1910, val loss: 1.4315643310546875
Epoch 1920, training loss: 62.32986068725586 = 0.01686568185687065 + 10.0 * 6.23129940032959
Epoch 1920, val loss: 1.4351215362548828
Epoch 1930, training loss: 62.31971740722656 = 0.016600660979747772 + 10.0 * 6.230311393737793
Epoch 1930, val loss: 1.4387602806091309
Epoch 1940, training loss: 62.360198974609375 = 0.01634940132498741 + 10.0 * 6.234385013580322
Epoch 1940, val loss: 1.442517876625061
Epoch 1950, training loss: 62.34452819824219 = 0.01608967036008835 + 10.0 * 6.23284387588501
Epoch 1950, val loss: 1.4456623792648315
Epoch 1960, training loss: 62.33818817138672 = 0.015835344791412354 + 10.0 * 6.232235431671143
Epoch 1960, val loss: 1.4492161273956299
Epoch 1970, training loss: 62.31869888305664 = 0.01559679675847292 + 10.0 * 6.230309963226318
Epoch 1970, val loss: 1.45278799533844
Epoch 1980, training loss: 62.33856201171875 = 0.015370301902294159 + 10.0 * 6.232319355010986
Epoch 1980, val loss: 1.4562941789627075
Epoch 1990, training loss: 62.37333679199219 = 0.015133772976696491 + 10.0 * 6.235820293426514
Epoch 1990, val loss: 1.4593594074249268
Epoch 2000, training loss: 62.30944061279297 = 0.014906340278685093 + 10.0 * 6.229453086853027
Epoch 2000, val loss: 1.463102102279663
Epoch 2010, training loss: 62.308982849121094 = 0.01469423808157444 + 10.0 * 6.229428768157959
Epoch 2010, val loss: 1.4665857553482056
Epoch 2020, training loss: 62.30307388305664 = 0.014479245990514755 + 10.0 * 6.2288594245910645
Epoch 2020, val loss: 1.4699161052703857
Epoch 2030, training loss: 62.32537841796875 = 0.014276315458118916 + 10.0 * 6.231110572814941
Epoch 2030, val loss: 1.4733216762542725
Epoch 2040, training loss: 62.313175201416016 = 0.014070679433643818 + 10.0 * 6.229910373687744
Epoch 2040, val loss: 1.4766182899475098
Epoch 2050, training loss: 62.35853958129883 = 0.013868916779756546 + 10.0 * 6.234467029571533
Epoch 2050, val loss: 1.4799749851226807
Epoch 2060, training loss: 62.3250617980957 = 0.013669601641595364 + 10.0 * 6.231139183044434
Epoch 2060, val loss: 1.4827756881713867
Epoch 2070, training loss: 62.35818862915039 = 0.013475648127496243 + 10.0 * 6.234471321105957
Epoch 2070, val loss: 1.4859620332717896
Epoch 2080, training loss: 62.29351043701172 = 0.01329073403030634 + 10.0 * 6.22802209854126
Epoch 2080, val loss: 1.489426612854004
Epoch 2090, training loss: 62.28672790527344 = 0.013111471198499203 + 10.0 * 6.227361679077148
Epoch 2090, val loss: 1.492774248123169
Epoch 2100, training loss: 62.28300857543945 = 0.012937040068209171 + 10.0 * 6.2270073890686035
Epoch 2100, val loss: 1.495930790901184
Epoch 2110, training loss: 62.33884048461914 = 0.012770353816449642 + 10.0 * 6.232606887817383
Epoch 2110, val loss: 1.4992189407348633
Epoch 2120, training loss: 62.312347412109375 = 0.012595215812325478 + 10.0 * 6.22997522354126
Epoch 2120, val loss: 1.5022715330123901
Epoch 2130, training loss: 62.298866271972656 = 0.012426706030964851 + 10.0 * 6.228643894195557
Epoch 2130, val loss: 1.5053355693817139
Epoch 2140, training loss: 62.2700309753418 = 0.012255748733878136 + 10.0 * 6.225777626037598
Epoch 2140, val loss: 1.5080809593200684
Epoch 2150, training loss: 62.284095764160156 = 0.012096232734620571 + 10.0 * 6.227200031280518
Epoch 2150, val loss: 1.5111370086669922
Epoch 2160, training loss: 62.30319595336914 = 0.011942041106522083 + 10.0 * 6.229125022888184
Epoch 2160, val loss: 1.5141963958740234
Epoch 2170, training loss: 62.28263854980469 = 0.011791656725108624 + 10.0 * 6.227084636688232
Epoch 2170, val loss: 1.5175492763519287
Epoch 2180, training loss: 62.33678436279297 = 0.011645901948213577 + 10.0 * 6.232513904571533
Epoch 2180, val loss: 1.5205771923065186
Epoch 2190, training loss: 62.28567886352539 = 0.0114869624376297 + 10.0 * 6.227419376373291
Epoch 2190, val loss: 1.5229101181030273
Epoch 2200, training loss: 62.279903411865234 = 0.011343386024236679 + 10.0 * 6.226855754852295
Epoch 2200, val loss: 1.5261784791946411
Epoch 2210, training loss: 62.27181625366211 = 0.011202185414731503 + 10.0 * 6.2260613441467285
Epoch 2210, val loss: 1.5289828777313232
Epoch 2220, training loss: 62.256103515625 = 0.011062047444283962 + 10.0 * 6.224503993988037
Epoch 2220, val loss: 1.5319247245788574
Epoch 2230, training loss: 62.331783294677734 = 0.010925449430942535 + 10.0 * 6.232085704803467
Epoch 2230, val loss: 1.5347431898117065
Epoch 2240, training loss: 62.26149368286133 = 0.010793606750667095 + 10.0 * 6.225069999694824
Epoch 2240, val loss: 1.537638545036316
Epoch 2250, training loss: 62.24825668334961 = 0.010663098655641079 + 10.0 * 6.223759651184082
Epoch 2250, val loss: 1.5407078266143799
Epoch 2260, training loss: 62.271888732910156 = 0.010535337962210178 + 10.0 * 6.22613525390625
Epoch 2260, val loss: 1.543487548828125
Epoch 2270, training loss: 62.274723052978516 = 0.010412289761006832 + 10.0 * 6.226430892944336
Epoch 2270, val loss: 1.5464141368865967
Epoch 2280, training loss: 62.2574348449707 = 0.010286582633852959 + 10.0 * 6.224714756011963
Epoch 2280, val loss: 1.5490015745162964
Epoch 2290, training loss: 62.26249694824219 = 0.010165315121412277 + 10.0 * 6.22523307800293
Epoch 2290, val loss: 1.551767349243164
Epoch 2300, training loss: 62.27360916137695 = 0.010046570561826229 + 10.0 * 6.226356029510498
Epoch 2300, val loss: 1.5543853044509888
Epoch 2310, training loss: 62.33626174926758 = 0.009930026717483997 + 10.0 * 6.232633113861084
Epoch 2310, val loss: 1.5567967891693115
Epoch 2320, training loss: 62.2584114074707 = 0.009815686382353306 + 10.0 * 6.224859714508057
Epoch 2320, val loss: 1.560002326965332
Epoch 2330, training loss: 62.24154281616211 = 0.009702506475150585 + 10.0 * 6.223184108734131
Epoch 2330, val loss: 1.5626122951507568
Epoch 2340, training loss: 62.229122161865234 = 0.009594176895916462 + 10.0 * 6.22195291519165
Epoch 2340, val loss: 1.5653656721115112
Epoch 2350, training loss: 62.229087829589844 = 0.00949066411703825 + 10.0 * 6.221959590911865
Epoch 2350, val loss: 1.5681047439575195
Epoch 2360, training loss: 62.30256652832031 = 0.009392754174768925 + 10.0 * 6.229317665100098
Epoch 2360, val loss: 1.5709797143936157
Epoch 2370, training loss: 62.29636001586914 = 0.009286964312195778 + 10.0 * 6.228707313537598
Epoch 2370, val loss: 1.5736603736877441
Epoch 2380, training loss: 62.24298095703125 = 0.009178824722766876 + 10.0 * 6.223380088806152
Epoch 2380, val loss: 1.5758204460144043
Epoch 2390, training loss: 62.23500442504883 = 0.009073610417544842 + 10.0 * 6.222592830657959
Epoch 2390, val loss: 1.5782309770584106
Epoch 2400, training loss: 62.22404098510742 = 0.008977469056844711 + 10.0 * 6.221506595611572
Epoch 2400, val loss: 1.5809516906738281
Epoch 2410, training loss: 62.25648880004883 = 0.008884042501449585 + 10.0 * 6.22476053237915
Epoch 2410, val loss: 1.5835658311843872
Epoch 2420, training loss: 62.23358154296875 = 0.008792025037109852 + 10.0 * 6.222478866577148
Epoch 2420, val loss: 1.586311936378479
Epoch 2430, training loss: 62.24366760253906 = 0.008698482066392899 + 10.0 * 6.223496913909912
Epoch 2430, val loss: 1.5887951850891113
Epoch 2440, training loss: 62.21718978881836 = 0.00860198587179184 + 10.0 * 6.220858573913574
Epoch 2440, val loss: 1.5910015106201172
Epoch 2450, training loss: 62.22629928588867 = 0.008516158908605576 + 10.0 * 6.221778392791748
Epoch 2450, val loss: 1.5934559106826782
Epoch 2460, training loss: 62.291419982910156 = 0.008429126814007759 + 10.0 * 6.228299140930176
Epoch 2460, val loss: 1.595921277999878
Epoch 2470, training loss: 62.23614501953125 = 0.008341481909155846 + 10.0 * 6.222780227661133
Epoch 2470, val loss: 1.598543405532837
Epoch 2480, training loss: 62.25373840332031 = 0.00825770664960146 + 10.0 * 6.224547863006592
Epoch 2480, val loss: 1.6010676622390747
Epoch 2490, training loss: 62.23126220703125 = 0.008172092027962208 + 10.0 * 6.222309112548828
Epoch 2490, val loss: 1.6034903526306152
Epoch 2500, training loss: 62.21331787109375 = 0.008086268790066242 + 10.0 * 6.220522880554199
Epoch 2500, val loss: 1.6055949926376343
Epoch 2510, training loss: 62.19874954223633 = 0.00800761766731739 + 10.0 * 6.219074249267578
Epoch 2510, val loss: 1.6080939769744873
Epoch 2520, training loss: 62.19337844848633 = 0.007928997278213501 + 10.0 * 6.218544960021973
Epoch 2520, val loss: 1.6104069948196411
Epoch 2530, training loss: 62.25094985961914 = 0.007853054441511631 + 10.0 * 6.22430944442749
Epoch 2530, val loss: 1.612398624420166
Epoch 2540, training loss: 62.303123474121094 = 0.007774851750582457 + 10.0 * 6.229535102844238
Epoch 2540, val loss: 1.614858627319336
Epoch 2550, training loss: 62.22614288330078 = 0.007698433008044958 + 10.0 * 6.221844673156738
Epoch 2550, val loss: 1.6170274019241333
Epoch 2560, training loss: 62.19940185546875 = 0.0076204463839530945 + 10.0 * 6.219178199768066
Epoch 2560, val loss: 1.619590401649475
Epoch 2570, training loss: 62.18681716918945 = 0.007548400200903416 + 10.0 * 6.217926979064941
Epoch 2570, val loss: 1.6218639612197876
Epoch 2580, training loss: 62.19194793701172 = 0.007480110041797161 + 10.0 * 6.218446731567383
Epoch 2580, val loss: 1.6242705583572388
Epoch 2590, training loss: 62.276065826416016 = 0.007411285303533077 + 10.0 * 6.226865291595459
Epoch 2590, val loss: 1.626633882522583
Epoch 2600, training loss: 62.22287368774414 = 0.007341240998357534 + 10.0 * 6.221553325653076
Epoch 2600, val loss: 1.6287707090377808
Epoch 2610, training loss: 62.196571350097656 = 0.007272027432918549 + 10.0 * 6.218930244445801
Epoch 2610, val loss: 1.6310235261917114
Epoch 2620, training loss: 62.19424819946289 = 0.0072053405456244946 + 10.0 * 6.2187042236328125
Epoch 2620, val loss: 1.633324384689331
Epoch 2630, training loss: 62.23259735107422 = 0.007145055569708347 + 10.0 * 6.222545146942139
Epoch 2630, val loss: 1.6357229948043823
Epoch 2640, training loss: 62.18803405761719 = 0.00707451393827796 + 10.0 * 6.218095779418945
Epoch 2640, val loss: 1.6375732421875
Epoch 2650, training loss: 62.19619369506836 = 0.0070080142468214035 + 10.0 * 6.218918800354004
Epoch 2650, val loss: 1.6394834518432617
Epoch 2660, training loss: 62.17988967895508 = 0.0069461544044315815 + 10.0 * 6.217294216156006
Epoch 2660, val loss: 1.6418448686599731
Epoch 2670, training loss: 62.24787139892578 = 0.006886472459882498 + 10.0 * 6.2240986824035645
Epoch 2670, val loss: 1.6439270973205566
Epoch 2680, training loss: 62.231842041015625 = 0.006823386065661907 + 10.0 * 6.222501754760742
Epoch 2680, val loss: 1.6458516120910645
Epoch 2690, training loss: 62.1925163269043 = 0.006762166973203421 + 10.0 * 6.218575477600098
Epoch 2690, val loss: 1.6479486227035522
Epoch 2700, training loss: 62.17636489868164 = 0.006702056620270014 + 10.0 * 6.216966152191162
Epoch 2700, val loss: 1.6499807834625244
Epoch 2710, training loss: 62.237701416015625 = 0.006645443849265575 + 10.0 * 6.223105430603027
Epoch 2710, val loss: 1.6518347263336182
Epoch 2720, training loss: 62.17338943481445 = 0.006587386131286621 + 10.0 * 6.21668004989624
Epoch 2720, val loss: 1.654057264328003
Epoch 2730, training loss: 62.17986297607422 = 0.006532633677124977 + 10.0 * 6.2173333168029785
Epoch 2730, val loss: 1.6564432382583618
Epoch 2740, training loss: 62.18757247924805 = 0.006475916597992182 + 10.0 * 6.218109607696533
Epoch 2740, val loss: 1.6581655740737915
Epoch 2750, training loss: 62.196327209472656 = 0.006421880330890417 + 10.0 * 6.218990802764893
Epoch 2750, val loss: 1.6601953506469727
Epoch 2760, training loss: 62.18250274658203 = 0.0063678729347884655 + 10.0 * 6.217613697052002
Epoch 2760, val loss: 1.662302017211914
Epoch 2770, training loss: 62.17875289916992 = 0.006313038989901543 + 10.0 * 6.2172441482543945
Epoch 2770, val loss: 1.664097547531128
Epoch 2780, training loss: 62.16394805908203 = 0.006261826027184725 + 10.0 * 6.215768337249756
Epoch 2780, val loss: 1.6662194728851318
Epoch 2790, training loss: 62.18925094604492 = 0.006211611907929182 + 10.0 * 6.21830415725708
Epoch 2790, val loss: 1.668041706085205
Epoch 2800, training loss: 62.20730209350586 = 0.006160501390695572 + 10.0 * 6.220114231109619
Epoch 2800, val loss: 1.6699323654174805
Epoch 2810, training loss: 62.19377136230469 = 0.006113340612500906 + 10.0 * 6.218765735626221
Epoch 2810, val loss: 1.6721792221069336
Epoch 2820, training loss: 62.16377258300781 = 0.006060195155441761 + 10.0 * 6.215771198272705
Epoch 2820, val loss: 1.6739730834960938
Epoch 2830, training loss: 62.20058059692383 = 0.00601448118686676 + 10.0 * 6.219456672668457
Epoch 2830, val loss: 1.6761986017227173
Epoch 2840, training loss: 62.16193771362305 = 0.0059643881395459175 + 10.0 * 6.215597152709961
Epoch 2840, val loss: 1.6778186559677124
Epoch 2850, training loss: 62.153587341308594 = 0.005916309077292681 + 10.0 * 6.214766979217529
Epoch 2850, val loss: 1.679577112197876
Epoch 2860, training loss: 62.15044021606445 = 0.005869888700544834 + 10.0 * 6.214457035064697
Epoch 2860, val loss: 1.681599497795105
Epoch 2870, training loss: 62.20125198364258 = 0.005825848318636417 + 10.0 * 6.219542503356934
Epoch 2870, val loss: 1.6832149028778076
Epoch 2880, training loss: 62.15361404418945 = 0.00577949732542038 + 10.0 * 6.214783668518066
Epoch 2880, val loss: 1.6851109266281128
Epoch 2890, training loss: 62.156105041503906 = 0.005736837163567543 + 10.0 * 6.215036869049072
Epoch 2890, val loss: 1.6870908737182617
Epoch 2900, training loss: 62.155860900878906 = 0.005691213998943567 + 10.0 * 6.215016841888428
Epoch 2900, val loss: 1.688850998878479
Epoch 2910, training loss: 62.18109130859375 = 0.0056494781747460365 + 10.0 * 6.217544078826904
Epoch 2910, val loss: 1.690537929534912
Epoch 2920, training loss: 62.17097854614258 = 0.00560713279992342 + 10.0 * 6.216536998748779
Epoch 2920, val loss: 1.6924453973770142
Epoch 2930, training loss: 62.1526985168457 = 0.00556366378441453 + 10.0 * 6.2147135734558105
Epoch 2930, val loss: 1.6942355632781982
Epoch 2940, training loss: 62.15033721923828 = 0.005521855782717466 + 10.0 * 6.214481353759766
Epoch 2940, val loss: 1.696154236793518
Epoch 2950, training loss: 62.1547966003418 = 0.005483420565724373 + 10.0 * 6.214931488037109
Epoch 2950, val loss: 1.698089599609375
Epoch 2960, training loss: 62.199729919433594 = 0.005447772331535816 + 10.0 * 6.219428062438965
Epoch 2960, val loss: 1.6999578475952148
Epoch 2970, training loss: 62.17748260498047 = 0.00540154380723834 + 10.0 * 6.217207908630371
Epoch 2970, val loss: 1.7014827728271484
Epoch 2980, training loss: 62.14005661010742 = 0.005361347459256649 + 10.0 * 6.213469505310059
Epoch 2980, val loss: 1.703073501586914
Epoch 2990, training loss: 62.12984085083008 = 0.005322611890733242 + 10.0 * 6.212451934814453
Epoch 2990, val loss: 1.704728603363037
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8176067474960464
The final CL Acc:0.72469, 0.02874, The final GNN Acc:0.81954, 0.00138
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13102])
remove edge: torch.Size([2, 7966])
updated graph: torch.Size([2, 10512])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.9027099609375 = 1.934374451637268 + 10.0 * 8.596834182739258
Epoch 0, val loss: 1.9362865686416626
Epoch 10, training loss: 87.88536071777344 = 1.9237990379333496 + 10.0 * 8.596156120300293
Epoch 10, val loss: 1.9249670505523682
Epoch 20, training loss: 87.81840515136719 = 1.9111264944076538 + 10.0 * 8.590727806091309
Epoch 20, val loss: 1.9111794233322144
Epoch 30, training loss: 87.46509552001953 = 1.8951923847198486 + 10.0 * 8.556989669799805
Epoch 30, val loss: 1.893809199333191
Epoch 40, training loss: 86.00599670410156 = 1.8766794204711914 + 10.0 * 8.412931442260742
Epoch 40, val loss: 1.8747711181640625
Epoch 50, training loss: 82.33661651611328 = 1.856033205986023 + 10.0 * 8.04805850982666
Epoch 50, val loss: 1.8541513681411743
Epoch 60, training loss: 79.09461975097656 = 1.8366156816482544 + 10.0 * 7.725800514221191
Epoch 60, val loss: 1.8361293077468872
Epoch 70, training loss: 75.13766479492188 = 1.822756290435791 + 10.0 * 7.331491470336914
Epoch 70, val loss: 1.8238017559051514
Epoch 80, training loss: 72.8272705078125 = 1.8124828338623047 + 10.0 * 7.101478576660156
Epoch 80, val loss: 1.8137234449386597
Epoch 90, training loss: 71.45867919921875 = 1.8000656366348267 + 10.0 * 6.965861797332764
Epoch 90, val loss: 1.8020507097244263
Epoch 100, training loss: 70.42898559570312 = 1.788377046585083 + 10.0 * 6.864060878753662
Epoch 100, val loss: 1.7916560173034668
Epoch 110, training loss: 69.74948120117188 = 1.7775123119354248 + 10.0 * 6.797197341918945
Epoch 110, val loss: 1.781936764717102
Epoch 120, training loss: 69.22587585449219 = 1.7651805877685547 + 10.0 * 6.746068954467773
Epoch 120, val loss: 1.7707453966140747
Epoch 130, training loss: 68.72544860839844 = 1.7524421215057373 + 10.0 * 6.697300910949707
Epoch 130, val loss: 1.7590200901031494
Epoch 140, training loss: 68.3855209350586 = 1.7394627332687378 + 10.0 * 6.664606094360352
Epoch 140, val loss: 1.7471615076065063
Epoch 150, training loss: 68.08061981201172 = 1.7248347997665405 + 10.0 * 6.635578155517578
Epoch 150, val loss: 1.733832836151123
Epoch 160, training loss: 67.77574157714844 = 1.7090038061141968 + 10.0 * 6.606673717498779
Epoch 160, val loss: 1.7196067571640015
Epoch 170, training loss: 67.53241729736328 = 1.6920725107192993 + 10.0 * 6.5840349197387695
Epoch 170, val loss: 1.7044862508773804
Epoch 180, training loss: 67.23796081542969 = 1.6736303567886353 + 10.0 * 6.556432723999023
Epoch 180, val loss: 1.6880401372909546
Epoch 190, training loss: 67.00621795654297 = 1.6535059213638306 + 10.0 * 6.535271644592285
Epoch 190, val loss: 1.6703742742538452
Epoch 200, training loss: 66.832763671875 = 1.6311291456222534 + 10.0 * 6.520163536071777
Epoch 200, val loss: 1.6508729457855225
Epoch 210, training loss: 66.63823699951172 = 1.6065415143966675 + 10.0 * 6.503169536590576
Epoch 210, val loss: 1.6295547485351562
Epoch 220, training loss: 66.45890808105469 = 1.58022141456604 + 10.0 * 6.487868785858154
Epoch 220, val loss: 1.6068377494812012
Epoch 230, training loss: 66.28240966796875 = 1.5522023439407349 + 10.0 * 6.473021030426025
Epoch 230, val loss: 1.5828216075897217
Epoch 240, training loss: 66.20954895019531 = 1.522667646408081 + 10.0 * 6.468688011169434
Epoch 240, val loss: 1.5576597452163696
Epoch 250, training loss: 65.98098754882812 = 1.4913110733032227 + 10.0 * 6.448967456817627
Epoch 250, val loss: 1.531069278717041
Epoch 260, training loss: 65.83696746826172 = 1.4589791297912598 + 10.0 * 6.437798976898193
Epoch 260, val loss: 1.5036773681640625
Epoch 270, training loss: 65.70684051513672 = 1.4253647327423096 + 10.0 * 6.42814826965332
Epoch 270, val loss: 1.4755361080169678
Epoch 280, training loss: 65.66307830810547 = 1.3907431364059448 + 10.0 * 6.427233695983887
Epoch 280, val loss: 1.4467161893844604
Epoch 290, training loss: 65.49454498291016 = 1.3556907176971436 + 10.0 * 6.413885116577148
Epoch 290, val loss: 1.417646884918213
Epoch 300, training loss: 65.36698150634766 = 1.3205366134643555 + 10.0 * 6.404644966125488
Epoch 300, val loss: 1.388676643371582
Epoch 310, training loss: 65.26176452636719 = 1.2853087186813354 + 10.0 * 6.397645473480225
Epoch 310, val loss: 1.3599631786346436
Epoch 320, training loss: 65.21197509765625 = 1.2502360343933105 + 10.0 * 6.39617395401001
Epoch 320, val loss: 1.3315924406051636
Epoch 330, training loss: 65.0822982788086 = 1.2157443761825562 + 10.0 * 6.386655330657959
Epoch 330, val loss: 1.303970217704773
Epoch 340, training loss: 64.9958267211914 = 1.1819570064544678 + 10.0 * 6.381387233734131
Epoch 340, val loss: 1.2771745920181274
Epoch 350, training loss: 64.921142578125 = 1.1488085985183716 + 10.0 * 6.377233028411865
Epoch 350, val loss: 1.2511653900146484
Epoch 360, training loss: 64.82125854492188 = 1.1166757345199585 + 10.0 * 6.370458126068115
Epoch 360, val loss: 1.226179599761963
Epoch 370, training loss: 64.74466705322266 = 1.0853567123413086 + 10.0 * 6.365931034088135
Epoch 370, val loss: 1.2020608186721802
Epoch 380, training loss: 64.67919921875 = 1.054868459701538 + 10.0 * 6.362432956695557
Epoch 380, val loss: 1.1787992715835571
Epoch 390, training loss: 64.65972900390625 = 1.025084376335144 + 10.0 * 6.363464832305908
Epoch 390, val loss: 1.1564973592758179
Epoch 400, training loss: 64.53722381591797 = 0.9964722394943237 + 10.0 * 6.354074954986572
Epoch 400, val loss: 1.1351243257522583
Epoch 410, training loss: 64.47798919677734 = 0.9688096642494202 + 10.0 * 6.350917816162109
Epoch 410, val loss: 1.1149139404296875
Epoch 420, training loss: 64.40965270996094 = 0.9420213103294373 + 10.0 * 6.3467631340026855
Epoch 420, val loss: 1.0956206321716309
Epoch 430, training loss: 64.35045623779297 = 0.9160624742507935 + 10.0 * 6.34343957901001
Epoch 430, val loss: 1.0772249698638916
Epoch 440, training loss: 64.33147430419922 = 0.8909333348274231 + 10.0 * 6.344054222106934
Epoch 440, val loss: 1.0597048997879028
Epoch 450, training loss: 64.2629165649414 = 0.8665726184844971 + 10.0 * 6.339634418487549
Epoch 450, val loss: 1.0433707237243652
Epoch 460, training loss: 64.19017028808594 = 0.8431457877159119 + 10.0 * 6.334702491760254
Epoch 460, val loss: 1.0278356075286865
Epoch 470, training loss: 64.14813232421875 = 0.820580244064331 + 10.0 * 6.332755088806152
Epoch 470, val loss: 1.0133672952651978
Epoch 480, training loss: 64.11418151855469 = 0.7987729907035828 + 10.0 * 6.331540584564209
Epoch 480, val loss: 0.9998572468757629
Epoch 490, training loss: 64.05815887451172 = 0.7777650952339172 + 10.0 * 6.328039646148682
Epoch 490, val loss: 0.9873957633972168
Epoch 500, training loss: 64.01898956298828 = 0.7574009895324707 + 10.0 * 6.3261590003967285
Epoch 500, val loss: 0.9755905866622925
Epoch 510, training loss: 63.966426849365234 = 0.7377339601516724 + 10.0 * 6.322869300842285
Epoch 510, val loss: 0.9647684693336487
Epoch 520, training loss: 63.926395416259766 = 0.7186513543128967 + 10.0 * 6.320774555206299
Epoch 520, val loss: 0.9545976519584656
Epoch 530, training loss: 63.90139389038086 = 0.7000988721847534 + 10.0 * 6.32012939453125
Epoch 530, val loss: 0.9450809359550476
Epoch 540, training loss: 63.843196868896484 = 0.6821340322494507 + 10.0 * 6.31610631942749
Epoch 540, val loss: 0.9362511038780212
Epoch 550, training loss: 63.797786712646484 = 0.6647529006004333 + 10.0 * 6.313303470611572
Epoch 550, val loss: 0.9281603097915649
Epoch 560, training loss: 63.75962829589844 = 0.6478644013404846 + 10.0 * 6.311176300048828
Epoch 560, val loss: 0.9206389784812927
Epoch 570, training loss: 63.72688293457031 = 0.6313868165016174 + 10.0 * 6.309549808502197
Epoch 570, val loss: 0.9136388301849365
Epoch 580, training loss: 63.747039794921875 = 0.6152277588844299 + 10.0 * 6.313181400299072
Epoch 580, val loss: 0.9069778323173523
Epoch 590, training loss: 63.6566276550293 = 0.599469006061554 + 10.0 * 6.305716037750244
Epoch 590, val loss: 0.9007956981658936
Epoch 600, training loss: 63.62898635864258 = 0.5840538144111633 + 10.0 * 6.304493427276611
Epoch 600, val loss: 0.8949364423751831
Epoch 610, training loss: 63.59022521972656 = 0.5689710378646851 + 10.0 * 6.302125453948975
Epoch 610, val loss: 0.8896105289459229
Epoch 620, training loss: 63.636295318603516 = 0.5541669726371765 + 10.0 * 6.308212757110596
Epoch 620, val loss: 0.8844608068466187
Epoch 630, training loss: 63.542091369628906 = 0.5395131707191467 + 10.0 * 6.300257682800293
Epoch 630, val loss: 0.8797697424888611
Epoch 640, training loss: 63.505367279052734 = 0.5252206325531006 + 10.0 * 6.2980146408081055
Epoch 640, val loss: 0.8753880262374878
Epoch 650, training loss: 63.472110748291016 = 0.5111954808235168 + 10.0 * 6.296091556549072
Epoch 650, val loss: 0.8712711930274963
Epoch 660, training loss: 63.44010925292969 = 0.4973898231983185 + 10.0 * 6.294271945953369
Epoch 660, val loss: 0.8674812316894531
Epoch 670, training loss: 63.412193298339844 = 0.48374444246292114 + 10.0 * 6.292844772338867
Epoch 670, val loss: 0.8639283180236816
Epoch 680, training loss: 63.46363067626953 = 0.47020015120506287 + 10.0 * 6.299343109130859
Epoch 680, val loss: 0.860657274723053
Epoch 690, training loss: 63.360008239746094 = 0.45671510696411133 + 10.0 * 6.290329456329346
Epoch 690, val loss: 0.857316255569458
Epoch 700, training loss: 63.3382682800293 = 0.4434608519077301 + 10.0 * 6.289480686187744
Epoch 700, val loss: 0.8542184829711914
Epoch 710, training loss: 63.313377380371094 = 0.4303590953350067 + 10.0 * 6.288301944732666
Epoch 710, val loss: 0.8514556288719177
Epoch 720, training loss: 63.285091400146484 = 0.4172733724117279 + 10.0 * 6.2867817878723145
Epoch 720, val loss: 0.8489070534706116
Epoch 730, training loss: 63.25617599487305 = 0.4043012261390686 + 10.0 * 6.285187721252441
Epoch 730, val loss: 0.8464478850364685
Epoch 740, training loss: 63.273826599121094 = 0.39145520329475403 + 10.0 * 6.28823709487915
Epoch 740, val loss: 0.8442481160163879
Epoch 750, training loss: 63.21756362915039 = 0.37869682908058167 + 10.0 * 6.283886909484863
Epoch 750, val loss: 0.8421242237091064
Epoch 760, training loss: 63.181785583496094 = 0.36607375741004944 + 10.0 * 6.281571388244629
Epoch 760, val loss: 0.8404620289802551
Epoch 770, training loss: 63.14942169189453 = 0.3536083400249481 + 10.0 * 6.279581546783447
Epoch 770, val loss: 0.8388093113899231
Epoch 780, training loss: 63.12976837158203 = 0.3412991464138031 + 10.0 * 6.278846740722656
Epoch 780, val loss: 0.8375594019889832
Epoch 790, training loss: 63.154293060302734 = 0.32915759086608887 + 10.0 * 6.282513618469238
Epoch 790, val loss: 0.8365536332130432
Epoch 800, training loss: 63.09207534790039 = 0.31716862320899963 + 10.0 * 6.277490615844727
Epoch 800, val loss: 0.8356146812438965
Epoch 810, training loss: 63.05656433105469 = 0.3054955303668976 + 10.0 * 6.275106906890869
Epoch 810, val loss: 0.835129976272583
Epoch 820, training loss: 63.04695129394531 = 0.2941267788410187 + 10.0 * 6.275282382965088
Epoch 820, val loss: 0.8349326848983765
Epoch 830, training loss: 63.01294708251953 = 0.28301727771759033 + 10.0 * 6.272993087768555
Epoch 830, val loss: 0.835115373134613
Epoch 840, training loss: 63.00709915161133 = 0.2722621560096741 + 10.0 * 6.273483753204346
Epoch 840, val loss: 0.835279643535614
Epoch 850, training loss: 62.969234466552734 = 0.26191505789756775 + 10.0 * 6.2707319259643555
Epoch 850, val loss: 0.8360633254051208
Epoch 860, training loss: 62.94745635986328 = 0.2519312798976898 + 10.0 * 6.269552707672119
Epoch 860, val loss: 0.8371940851211548
Epoch 870, training loss: 63.04492950439453 = 0.24231117963790894 + 10.0 * 6.280261993408203
Epoch 870, val loss: 0.8386884927749634
Epoch 880, training loss: 62.921382904052734 = 0.2329930067062378 + 10.0 * 6.268838882446289
Epoch 880, val loss: 0.8401528596878052
Epoch 890, training loss: 62.895050048828125 = 0.2241491973400116 + 10.0 * 6.267090320587158
Epoch 890, val loss: 0.842139482498169
Epoch 900, training loss: 62.87797546386719 = 0.2156829833984375 + 10.0 * 6.266229152679443
Epoch 900, val loss: 0.8444179892539978
Epoch 910, training loss: 62.874507904052734 = 0.20759141445159912 + 10.0 * 6.2666916847229
Epoch 910, val loss: 0.8470091819763184
Epoch 920, training loss: 62.85221481323242 = 0.19979403913021088 + 10.0 * 6.265242099761963
Epoch 920, val loss: 0.8498643040657043
Epoch 930, training loss: 62.831729888916016 = 0.19232293963432312 + 10.0 * 6.263940811157227
Epoch 930, val loss: 0.8530089259147644
Epoch 940, training loss: 62.81024169921875 = 0.1852116733789444 + 10.0 * 6.262503147125244
Epoch 940, val loss: 0.8563458919525146
Epoch 950, training loss: 62.79481506347656 = 0.1783963143825531 + 10.0 * 6.261641502380371
Epoch 950, val loss: 0.8599310517311096
Epoch 960, training loss: 62.81390380859375 = 0.17186366021633148 + 10.0 * 6.264204025268555
Epoch 960, val loss: 0.8637197017669678
Epoch 970, training loss: 62.812538146972656 = 0.16557642817497253 + 10.0 * 6.26469612121582
Epoch 970, val loss: 0.8678529858589172
Epoch 980, training loss: 62.787540435791016 = 0.15957891941070557 + 10.0 * 6.262795925140381
Epoch 980, val loss: 0.8718889951705933
Epoch 990, training loss: 62.74076843261719 = 0.15386345982551575 + 10.0 * 6.25869083404541
Epoch 990, val loss: 0.8763111233711243
Epoch 1000, training loss: 62.72297668457031 = 0.1484142392873764 + 10.0 * 6.257456302642822
Epoch 1000, val loss: 0.8809488415718079
Epoch 1010, training loss: 62.711368560791016 = 0.14319181442260742 + 10.0 * 6.256817817687988
Epoch 1010, val loss: 0.8857271075248718
Epoch 1020, training loss: 62.710670471191406 = 0.13819736242294312 + 10.0 * 6.257246971130371
Epoch 1020, val loss: 0.8906386494636536
Epoch 1030, training loss: 62.73093032836914 = 0.13334695994853973 + 10.0 * 6.259758472442627
Epoch 1030, val loss: 0.8956701159477234
Epoch 1040, training loss: 62.713008880615234 = 0.12868233025074005 + 10.0 * 6.258432865142822
Epoch 1040, val loss: 0.9005405902862549
Epoch 1050, training loss: 62.67317581176758 = 0.1242419108748436 + 10.0 * 6.2548933029174805
Epoch 1050, val loss: 0.905830979347229
Epoch 1060, training loss: 62.65434646606445 = 0.11999408900737762 + 10.0 * 6.253435134887695
Epoch 1060, val loss: 0.9111526608467102
Epoch 1070, training loss: 62.646881103515625 = 0.11592437326908112 + 10.0 * 6.253095626831055
Epoch 1070, val loss: 0.9165413975715637
Epoch 1080, training loss: 62.666873931884766 = 0.11201781779527664 + 10.0 * 6.255485534667969
Epoch 1080, val loss: 0.9219536781311035
Epoch 1090, training loss: 62.633331298828125 = 0.10824944078922272 + 10.0 * 6.252508163452148
Epoch 1090, val loss: 0.9275633096694946
Epoch 1100, training loss: 62.6333122253418 = 0.10463335365056992 + 10.0 * 6.252867698669434
Epoch 1100, val loss: 0.9330191016197205
Epoch 1110, training loss: 62.64810562133789 = 0.10117097198963165 + 10.0 * 6.254693508148193
Epoch 1110, val loss: 0.9388094544410706
Epoch 1120, training loss: 62.602272033691406 = 0.0978177860379219 + 10.0 * 6.250445365905762
Epoch 1120, val loss: 0.9442495107650757
Epoch 1130, training loss: 62.59257507324219 = 0.09462695568799973 + 10.0 * 6.249794960021973
Epoch 1130, val loss: 0.949978768825531
Epoch 1140, training loss: 62.58190155029297 = 0.09156986325979233 + 10.0 * 6.249033451080322
Epoch 1140, val loss: 0.9556734561920166
Epoch 1150, training loss: 62.5852165222168 = 0.0886346623301506 + 10.0 * 6.249658107757568
Epoch 1150, val loss: 0.9614070057868958
Epoch 1160, training loss: 62.6040153503418 = 0.0858016163110733 + 10.0 * 6.251821517944336
Epoch 1160, val loss: 0.9670851826667786
Epoch 1170, training loss: 62.588356018066406 = 0.08304467052221298 + 10.0 * 6.250531196594238
Epoch 1170, val loss: 0.972834587097168
Epoch 1180, training loss: 62.556800842285156 = 0.0804217979311943 + 10.0 * 6.247637748718262
Epoch 1180, val loss: 0.978621780872345
Epoch 1190, training loss: 62.54256820678711 = 0.07791417092084885 + 10.0 * 6.24646520614624
Epoch 1190, val loss: 0.9843600988388062
Epoch 1200, training loss: 62.53329849243164 = 0.07550884038209915 + 10.0 * 6.245779037475586
Epoch 1200, val loss: 0.9902055263519287
Epoch 1210, training loss: 62.65911102294922 = 0.0731874629855156 + 10.0 * 6.258592128753662
Epoch 1210, val loss: 0.9959717988967896
Epoch 1220, training loss: 62.53653335571289 = 0.07093837112188339 + 10.0 * 6.2465596199035645
Epoch 1220, val loss: 1.00164794921875
Epoch 1230, training loss: 62.52167892456055 = 0.06877641379833221 + 10.0 * 6.245290279388428
Epoch 1230, val loss: 1.0073319673538208
Epoch 1240, training loss: 62.502559661865234 = 0.06672338396310806 + 10.0 * 6.243583679199219
Epoch 1240, val loss: 1.0131765604019165
Epoch 1250, training loss: 62.501625061035156 = 0.06475140154361725 + 10.0 * 6.243687629699707
Epoch 1250, val loss: 1.018946886062622
Epoch 1260, training loss: 62.545013427734375 = 0.06284283846616745 + 10.0 * 6.2482171058654785
Epoch 1260, val loss: 1.0246286392211914
Epoch 1270, training loss: 62.49784851074219 = 0.060996267944574356 + 10.0 * 6.243685245513916
Epoch 1270, val loss: 1.0303739309310913
Epoch 1280, training loss: 62.49394989013672 = 0.05922649800777435 + 10.0 * 6.243472099304199
Epoch 1280, val loss: 1.0360780954360962
Epoch 1290, training loss: 62.494956970214844 = 0.057517629116773605 + 10.0 * 6.243743896484375
Epoch 1290, val loss: 1.0417523384094238
Epoch 1300, training loss: 62.50529861450195 = 0.05587247386574745 + 10.0 * 6.244942665100098
Epoch 1300, val loss: 1.047459363937378
Epoch 1310, training loss: 62.46858215332031 = 0.0542946457862854 + 10.0 * 6.241428852081299
Epoch 1310, val loss: 1.053184986114502
Epoch 1320, training loss: 62.45497131347656 = 0.05277452617883682 + 10.0 * 6.240219593048096
Epoch 1320, val loss: 1.0587646961212158
Epoch 1330, training loss: 62.44786071777344 = 0.051316868513822556 + 10.0 * 6.239654541015625
Epoch 1330, val loss: 1.0644276142120361
Epoch 1340, training loss: 62.50242233276367 = 0.04991079866886139 + 10.0 * 6.245251178741455
Epoch 1340, val loss: 1.0699931383132935
Epoch 1350, training loss: 62.479148864746094 = 0.04853260889649391 + 10.0 * 6.243061542510986
Epoch 1350, val loss: 1.0757044553756714
Epoch 1360, training loss: 62.434722900390625 = 0.04722167178988457 + 10.0 * 6.238749980926514
Epoch 1360, val loss: 1.0810365676879883
Epoch 1370, training loss: 62.42470932006836 = 0.04596056044101715 + 10.0 * 6.237874984741211
Epoch 1370, val loss: 1.086586594581604
Epoch 1380, training loss: 62.4183349609375 = 0.04475376754999161 + 10.0 * 6.237358093261719
Epoch 1380, val loss: 1.0921369791030884
Epoch 1390, training loss: 62.49700927734375 = 0.04358828067779541 + 10.0 * 6.245342254638672
Epoch 1390, val loss: 1.0975779294967651
Epoch 1400, training loss: 62.46282958984375 = 0.042444225400686264 + 10.0 * 6.242038726806641
Epoch 1400, val loss: 1.1027774810791016
Epoch 1410, training loss: 62.40397262573242 = 0.041340455412864685 + 10.0 * 6.236263275146484
Epoch 1410, val loss: 1.108262062072754
Epoch 1420, training loss: 62.40205001831055 = 0.04028838872909546 + 10.0 * 6.236176490783691
Epoch 1420, val loss: 1.113646388053894
Epoch 1430, training loss: 62.392173767089844 = 0.03928590938448906 + 10.0 * 6.235288619995117
Epoch 1430, val loss: 1.1189719438552856
Epoch 1440, training loss: 62.48173522949219 = 0.03832598403096199 + 10.0 * 6.244340896606445
Epoch 1440, val loss: 1.1241718530654907
Epoch 1450, training loss: 62.440460205078125 = 0.037345338612794876 + 10.0 * 6.240311622619629
Epoch 1450, val loss: 1.12947678565979
Epoch 1460, training loss: 62.38214111328125 = 0.036428458988666534 + 10.0 * 6.2345709800720215
Epoch 1460, val loss: 1.1346898078918457
Epoch 1470, training loss: 62.3759765625 = 0.03554689884185791 + 10.0 * 6.234043121337891
Epoch 1470, val loss: 1.1398684978485107
Epoch 1480, training loss: 62.38020706176758 = 0.034699589014053345 + 10.0 * 6.234550952911377
Epoch 1480, val loss: 1.1451351642608643
Epoch 1490, training loss: 62.424781799316406 = 0.033875998109579086 + 10.0 * 6.239090919494629
Epoch 1490, val loss: 1.1502594947814941
Epoch 1500, training loss: 62.39396286010742 = 0.03307696059346199 + 10.0 * 6.236088752746582
Epoch 1500, val loss: 1.1553239822387695
Epoch 1510, training loss: 62.36834716796875 = 0.03230302780866623 + 10.0 * 6.233604431152344
Epoch 1510, val loss: 1.1603349447250366
Epoch 1520, training loss: 62.35304260253906 = 0.03156324848532677 + 10.0 * 6.232148170471191
Epoch 1520, val loss: 1.1653950214385986
Epoch 1530, training loss: 62.36320495605469 = 0.03085237182676792 + 10.0 * 6.2332353591918945
Epoch 1530, val loss: 1.1703852415084839
Epoch 1540, training loss: 62.36073303222656 = 0.03015086241066456 + 10.0 * 6.233057975769043
Epoch 1540, val loss: 1.1754001379013062
Epoch 1550, training loss: 62.38901901245117 = 0.02947647124528885 + 10.0 * 6.235954284667969
Epoch 1550, val loss: 1.1803936958312988
Epoch 1560, training loss: 62.34178161621094 = 0.02881489507853985 + 10.0 * 6.231296539306641
Epoch 1560, val loss: 1.1852363348007202
Epoch 1570, training loss: 62.331634521484375 = 0.02818332053720951 + 10.0 * 6.230345249176025
Epoch 1570, val loss: 1.1901212930679321
Epoch 1580, training loss: 62.32819747924805 = 0.027575992047786713 + 10.0 * 6.230062007904053
Epoch 1580, val loss: 1.1950117349624634
Epoch 1590, training loss: 62.32565689086914 = 0.026989320293068886 + 10.0 * 6.229866981506348
Epoch 1590, val loss: 1.1998239755630493
Epoch 1600, training loss: 62.430809020996094 = 0.026419922709465027 + 10.0 * 6.240438938140869
Epoch 1600, val loss: 1.2045235633850098
Epoch 1610, training loss: 62.332645416259766 = 0.025842072442173958 + 10.0 * 6.230680465698242
Epoch 1610, val loss: 1.209237813949585
Epoch 1620, training loss: 62.322139739990234 = 0.025302069261670113 + 10.0 * 6.229683876037598
Epoch 1620, val loss: 1.2138276100158691
Epoch 1630, training loss: 62.30632781982422 = 0.024781951680779457 + 10.0 * 6.22815465927124
Epoch 1630, val loss: 1.218583345413208
Epoch 1640, training loss: 62.3138313293457 = 0.024282094091176987 + 10.0 * 6.228955268859863
Epoch 1640, val loss: 1.2232284545898438
Epoch 1650, training loss: 62.345916748046875 = 0.023790886625647545 + 10.0 * 6.232212543487549
Epoch 1650, val loss: 1.2277979850769043
Epoch 1660, training loss: 62.323936462402344 = 0.0233127661049366 + 10.0 * 6.230062484741211
Epoch 1660, val loss: 1.2323839664459229
Epoch 1670, training loss: 62.323909759521484 = 0.022845929488539696 + 10.0 * 6.230106353759766
Epoch 1670, val loss: 1.2367905378341675
Epoch 1680, training loss: 62.298831939697266 = 0.022389372810721397 + 10.0 * 6.227644443511963
Epoch 1680, val loss: 1.2413197755813599
Epoch 1690, training loss: 62.28860855102539 = 0.021958447992801666 + 10.0 * 6.226665019989014
Epoch 1690, val loss: 1.2459253072738647
Epoch 1700, training loss: 62.303672790527344 = 0.021537484601140022 + 10.0 * 6.228213310241699
Epoch 1700, val loss: 1.2502915859222412
Epoch 1710, training loss: 62.3030891418457 = 0.021123286336660385 + 10.0 * 6.228196620941162
Epoch 1710, val loss: 1.2545822858810425
Epoch 1720, training loss: 62.296817779541016 = 0.02072235941886902 + 10.0 * 6.227609634399414
Epoch 1720, val loss: 1.25884211063385
Epoch 1730, training loss: 62.31283950805664 = 0.02033771574497223 + 10.0 * 6.229250431060791
Epoch 1730, val loss: 1.2631433010101318
Epoch 1740, training loss: 62.29502487182617 = 0.01995455101132393 + 10.0 * 6.2275071144104
Epoch 1740, val loss: 1.2673808336257935
Epoch 1750, training loss: 62.274620056152344 = 0.019584575667977333 + 10.0 * 6.225503444671631
Epoch 1750, val loss: 1.2717387676239014
Epoch 1760, training loss: 62.26793670654297 = 0.019227810204029083 + 10.0 * 6.2248711585998535
Epoch 1760, val loss: 1.2758854627609253
Epoch 1770, training loss: 62.28790283203125 = 0.01888170838356018 + 10.0 * 6.226902008056641
Epoch 1770, val loss: 1.2801011800765991
Epoch 1780, training loss: 62.26722717285156 = 0.01854332536458969 + 10.0 * 6.224868297576904
Epoch 1780, val loss: 1.284202218055725
Epoch 1790, training loss: 62.298831939697266 = 0.018215525895357132 + 10.0 * 6.228061676025391
Epoch 1790, val loss: 1.2883167266845703
Epoch 1800, training loss: 62.2599983215332 = 0.017888853326439857 + 10.0 * 6.2242112159729
Epoch 1800, val loss: 1.2922941446304321
Epoch 1810, training loss: 62.26646041870117 = 0.017583593726158142 + 10.0 * 6.224887847900391
Epoch 1810, val loss: 1.2963736057281494
Epoch 1820, training loss: 62.271968841552734 = 0.017278529703617096 + 10.0 * 6.22546911239624
Epoch 1820, val loss: 1.300405502319336
Epoch 1830, training loss: 62.25068283081055 = 0.016983024775981903 + 10.0 * 6.22337007522583
Epoch 1830, val loss: 1.3043291568756104
Epoch 1840, training loss: 62.26494598388672 = 0.01669568568468094 + 10.0 * 6.224824905395508
Epoch 1840, val loss: 1.3083945512771606
Epoch 1850, training loss: 62.26621627807617 = 0.01641709730029106 + 10.0 * 6.224979877471924
Epoch 1850, val loss: 1.312258243560791
Epoch 1860, training loss: 62.2802734375 = 0.01613694801926613 + 10.0 * 6.226413726806641
Epoch 1860, val loss: 1.3159304857254028
Epoch 1870, training loss: 62.24405288696289 = 0.015873899683356285 + 10.0 * 6.222817897796631
Epoch 1870, val loss: 1.3198645114898682
Epoch 1880, training loss: 62.2331657409668 = 0.015610100701451302 + 10.0 * 6.221755504608154
Epoch 1880, val loss: 1.3236621618270874
Epoch 1890, training loss: 62.2272834777832 = 0.015363707207143307 + 10.0 * 6.221191883087158
Epoch 1890, val loss: 1.3274800777435303
Epoch 1900, training loss: 62.28271484375 = 0.015120744705200195 + 10.0 * 6.226759433746338
Epoch 1900, val loss: 1.3311656713485718
Epoch 1910, training loss: 62.25676727294922 = 0.014875403605401516 + 10.0 * 6.224189281463623
Epoch 1910, val loss: 1.3349510431289673
Epoch 1920, training loss: 62.2439079284668 = 0.01463871169835329 + 10.0 * 6.222927093505859
Epoch 1920, val loss: 1.3384276628494263
Epoch 1930, training loss: 62.21904373168945 = 0.0144070815294981 + 10.0 * 6.220463752746582
Epoch 1930, val loss: 1.3421834707260132
Epoch 1940, training loss: 62.21046447753906 = 0.014185907319188118 + 10.0 * 6.219627857208252
Epoch 1940, val loss: 1.3458364009857178
Epoch 1950, training loss: 62.239498138427734 = 0.013972774147987366 + 10.0 * 6.222552299499512
Epoch 1950, val loss: 1.3495103120803833
Epoch 1960, training loss: 62.25638198852539 = 0.013752707280218601 + 10.0 * 6.2242631912231445
Epoch 1960, val loss: 1.3527827262878418
Epoch 1970, training loss: 62.233787536621094 = 0.013542847707867622 + 10.0 * 6.222024440765381
Epoch 1970, val loss: 1.3563668727874756
Epoch 1980, training loss: 62.20566177368164 = 0.013336937874555588 + 10.0 * 6.219232559204102
Epoch 1980, val loss: 1.359726071357727
Epoch 1990, training loss: 62.19779968261719 = 0.013141931965947151 + 10.0 * 6.218465805053711
Epoch 1990, val loss: 1.3632546663284302
Epoch 2000, training loss: 62.21351623535156 = 0.012952674180269241 + 10.0 * 6.220056533813477
Epoch 2000, val loss: 1.366739273071289
Epoch 2010, training loss: 62.223636627197266 = 0.012761183083057404 + 10.0 * 6.221087455749512
Epoch 2010, val loss: 1.369957447052002
Epoch 2020, training loss: 62.19858932495117 = 0.012575317174196243 + 10.0 * 6.218601226806641
Epoch 2020, val loss: 1.373245358467102
Epoch 2030, training loss: 62.19165802001953 = 0.012396135367453098 + 10.0 * 6.217926025390625
Epoch 2030, val loss: 1.3766412734985352
Epoch 2040, training loss: 62.207340240478516 = 0.012224731966853142 + 10.0 * 6.21951150894165
Epoch 2040, val loss: 1.3799902200698853
Epoch 2050, training loss: 62.21270751953125 = 0.012050074525177479 + 10.0 * 6.220065593719482
Epoch 2050, val loss: 1.3831902742385864
Epoch 2060, training loss: 62.20299530029297 = 0.011876002885401249 + 10.0 * 6.219111919403076
Epoch 2060, val loss: 1.3864738941192627
Epoch 2070, training loss: 62.181060791015625 = 0.011711371131241322 + 10.0 * 6.216935157775879
Epoch 2070, val loss: 1.389715313911438
Epoch 2080, training loss: 62.174259185791016 = 0.01155297551304102 + 10.0 * 6.216270446777344
Epoch 2080, val loss: 1.3928642272949219
Epoch 2090, training loss: 62.178489685058594 = 0.01140041183680296 + 10.0 * 6.216708660125732
Epoch 2090, val loss: 1.396071195602417
Epoch 2100, training loss: 62.24867630004883 = 0.011250085197389126 + 10.0 * 6.223742485046387
Epoch 2100, val loss: 1.399142861366272
Epoch 2110, training loss: 62.20576477050781 = 0.01108942274004221 + 10.0 * 6.219467639923096
Epoch 2110, val loss: 1.4021881818771362
Epoch 2120, training loss: 62.18192672729492 = 0.010942910797894001 + 10.0 * 6.217098236083984
Epoch 2120, val loss: 1.4053101539611816
Epoch 2130, training loss: 62.186405181884766 = 0.010798797011375427 + 10.0 * 6.217560768127441
Epoch 2130, val loss: 1.4083093404769897
Epoch 2140, training loss: 62.17643356323242 = 0.01065843179821968 + 10.0 * 6.216577529907227
Epoch 2140, val loss: 1.4113682508468628
Epoch 2150, training loss: 62.16440963745117 = 0.01052034366875887 + 10.0 * 6.215388774871826
Epoch 2150, val loss: 1.4145030975341797
Epoch 2160, training loss: 62.20357131958008 = 0.010389100760221481 + 10.0 * 6.219318389892578
Epoch 2160, val loss: 1.417338490486145
Epoch 2170, training loss: 62.17073440551758 = 0.010250124149024487 + 10.0 * 6.216048240661621
Epoch 2170, val loss: 1.420258641242981
Epoch 2180, training loss: 62.15957260131836 = 0.010121929459273815 + 10.0 * 6.214945316314697
Epoch 2180, val loss: 1.423345685005188
Epoch 2190, training loss: 62.181297302246094 = 0.009995498694479465 + 10.0 * 6.217130184173584
Epoch 2190, val loss: 1.42616868019104
Epoch 2200, training loss: 62.15837860107422 = 0.009868157096207142 + 10.0 * 6.214850902557373
Epoch 2200, val loss: 1.4289875030517578
Epoch 2210, training loss: 62.15752029418945 = 0.00974545069038868 + 10.0 * 6.21477746963501
Epoch 2210, val loss: 1.4319242238998413
Epoch 2220, training loss: 62.16496658325195 = 0.009627074934542179 + 10.0 * 6.21553373336792
Epoch 2220, val loss: 1.434843897819519
Epoch 2230, training loss: 62.23249816894531 = 0.009507294744253159 + 10.0 * 6.222299098968506
Epoch 2230, val loss: 1.4375883340835571
Epoch 2240, training loss: 62.16649627685547 = 0.009391180239617825 + 10.0 * 6.215710639953613
Epoch 2240, val loss: 1.4402395486831665
Epoch 2250, training loss: 62.145084381103516 = 0.009276393800973892 + 10.0 * 6.21358060836792
Epoch 2250, val loss: 1.443147897720337
Epoch 2260, training loss: 62.13832092285156 = 0.00916946493089199 + 10.0 * 6.212914943695068
Epoch 2260, val loss: 1.4459115266799927
Epoch 2270, training loss: 62.13525390625 = 0.00906376726925373 + 10.0 * 6.212618827819824
Epoch 2270, val loss: 1.4486510753631592
Epoch 2280, training loss: 62.16530990600586 = 0.008960108272731304 + 10.0 * 6.215634822845459
Epoch 2280, val loss: 1.4513009786605835
Epoch 2290, training loss: 62.15140151977539 = 0.008853473700582981 + 10.0 * 6.214254856109619
Epoch 2290, val loss: 1.4540715217590332
Epoch 2300, training loss: 62.13495635986328 = 0.008749044500291348 + 10.0 * 6.212620735168457
Epoch 2300, val loss: 1.4566643238067627
Epoch 2310, training loss: 62.13438034057617 = 0.008649823255836964 + 10.0 * 6.212573051452637
Epoch 2310, val loss: 1.459518551826477
Epoch 2320, training loss: 62.158355712890625 = 0.008553857915103436 + 10.0 * 6.214980125427246
Epoch 2320, val loss: 1.4621423482894897
Epoch 2330, training loss: 62.13238525390625 = 0.008455881848931313 + 10.0 * 6.212392807006836
Epoch 2330, val loss: 1.4646518230438232
Epoch 2340, training loss: 62.15380096435547 = 0.008360420353710651 + 10.0 * 6.21454381942749
Epoch 2340, val loss: 1.4673069715499878
Epoch 2350, training loss: 62.13042068481445 = 0.008265459910035133 + 10.0 * 6.212215423583984
Epoch 2350, val loss: 1.4698635339736938
Epoch 2360, training loss: 62.12534713745117 = 0.008175890892744064 + 10.0 * 6.211717128753662
Epoch 2360, val loss: 1.4724311828613281
Epoch 2370, training loss: 62.115806579589844 = 0.008087815716862679 + 10.0 * 6.2107720375061035
Epoch 2370, val loss: 1.474995493888855
Epoch 2380, training loss: 62.117523193359375 = 0.008002967573702335 + 10.0 * 6.210951805114746
Epoch 2380, val loss: 1.4775640964508057
Epoch 2390, training loss: 62.15476989746094 = 0.007920959033071995 + 10.0 * 6.214684963226318
Epoch 2390, val loss: 1.479960560798645
Epoch 2400, training loss: 62.144561767578125 = 0.007831385359168053 + 10.0 * 6.213673114776611
Epoch 2400, val loss: 1.4823380708694458
Epoch 2410, training loss: 62.133026123046875 = 0.00774562219157815 + 10.0 * 6.212528228759766
Epoch 2410, val loss: 1.4847561120986938
Epoch 2420, training loss: 62.110740661621094 = 0.0076631177216768265 + 10.0 * 6.210307598114014
Epoch 2420, val loss: 1.4873712062835693
Epoch 2430, training loss: 62.107139587402344 = 0.007585534360259771 + 10.0 * 6.209955215454102
Epoch 2430, val loss: 1.4898368120193481
Epoch 2440, training loss: 62.1148796081543 = 0.007509253453463316 + 10.0 * 6.2107367515563965
Epoch 2440, val loss: 1.4923148155212402
Epoch 2450, training loss: 62.14391326904297 = 0.007433242630213499 + 10.0 * 6.213647842407227
Epoch 2450, val loss: 1.4945766925811768
Epoch 2460, training loss: 62.123924255371094 = 0.00735495425760746 + 10.0 * 6.2116570472717285
Epoch 2460, val loss: 1.4968284368515015
Epoch 2470, training loss: 62.13758087158203 = 0.007280033081769943 + 10.0 * 6.2130303382873535
Epoch 2470, val loss: 1.499242901802063
Epoch 2480, training loss: 62.11750030517578 = 0.007205286994576454 + 10.0 * 6.211029529571533
Epoch 2480, val loss: 1.5016441345214844
Epoch 2490, training loss: 62.104156494140625 = 0.007134519051760435 + 10.0 * 6.209702491760254
Epoch 2490, val loss: 1.503906011581421
Epoch 2500, training loss: 62.13138961791992 = 0.007064785808324814 + 10.0 * 6.212432384490967
Epoch 2500, val loss: 1.506137490272522
Epoch 2510, training loss: 62.10642623901367 = 0.006993010174483061 + 10.0 * 6.2099432945251465
Epoch 2510, val loss: 1.5085018873214722
Epoch 2520, training loss: 62.09319305419922 = 0.006925927009433508 + 10.0 * 6.208626747131348
Epoch 2520, val loss: 1.5107018947601318
Epoch 2530, training loss: 62.09490966796875 = 0.006862004287540913 + 10.0 * 6.208804607391357
Epoch 2530, val loss: 1.512952446937561
Epoch 2540, training loss: 62.13163757324219 = 0.006798827555030584 + 10.0 * 6.212483882904053
Epoch 2540, val loss: 1.515141248703003
Epoch 2550, training loss: 62.097023010253906 = 0.0067289723083376884 + 10.0 * 6.209029197692871
Epoch 2550, val loss: 1.5173122882843018
Epoch 2560, training loss: 62.14177703857422 = 0.006666270550340414 + 10.0 * 6.213510990142822
Epoch 2560, val loss: 1.5195648670196533
Epoch 2570, training loss: 62.089927673339844 = 0.006600767839699984 + 10.0 * 6.2083330154418945
Epoch 2570, val loss: 1.5216234922409058
Epoch 2580, training loss: 62.08111572265625 = 0.006538765039294958 + 10.0 * 6.207457542419434
Epoch 2580, val loss: 1.5238592624664307
Epoch 2590, training loss: 62.077754974365234 = 0.006480187177658081 + 10.0 * 6.207127571105957
Epoch 2590, val loss: 1.5259649753570557
Epoch 2600, training loss: 62.084842681884766 = 0.006422692444175482 + 10.0 * 6.207841873168945
Epoch 2600, val loss: 1.5280531644821167
Epoch 2610, training loss: 62.15487289428711 = 0.006364691536873579 + 10.0 * 6.214850902557373
Epoch 2610, val loss: 1.530029535293579
Epoch 2620, training loss: 62.10425567626953 = 0.00630341237410903 + 10.0 * 6.2097954750061035
Epoch 2620, val loss: 1.5322874784469604
Epoch 2630, training loss: 62.093971252441406 = 0.0062469057738780975 + 10.0 * 6.2087721824646
Epoch 2630, val loss: 1.5343666076660156
Epoch 2640, training loss: 62.07112121582031 = 0.006190168671309948 + 10.0 * 6.206492900848389
Epoch 2640, val loss: 1.5363330841064453
Epoch 2650, training loss: 62.084686279296875 = 0.006138015538454056 + 10.0 * 6.207854747772217
Epoch 2650, val loss: 1.538413643836975
Epoch 2660, training loss: 62.1317138671875 = 0.006084159482270479 + 10.0 * 6.2125630378723145
Epoch 2660, val loss: 1.5404030084609985
Epoch 2670, training loss: 62.08966827392578 = 0.00602760398760438 + 10.0 * 6.208364009857178
Epoch 2670, val loss: 1.5424354076385498
Epoch 2680, training loss: 62.07687759399414 = 0.005974639672785997 + 10.0 * 6.207090377807617
Epoch 2680, val loss: 1.5444543361663818
Epoch 2690, training loss: 62.06644821166992 = 0.005924282129853964 + 10.0 * 6.206052303314209
Epoch 2690, val loss: 1.546565294265747
Epoch 2700, training loss: 62.07154083251953 = 0.005876024253666401 + 10.0 * 6.20656681060791
Epoch 2700, val loss: 1.5485448837280273
Epoch 2710, training loss: 62.10816192626953 = 0.00582735612988472 + 10.0 * 6.210233211517334
Epoch 2710, val loss: 1.5504834651947021
Epoch 2720, training loss: 62.085044860839844 = 0.005774874705821276 + 10.0 * 6.2079267501831055
Epoch 2720, val loss: 1.5522667169570923
Epoch 2730, training loss: 62.08210372924805 = 0.005725951865315437 + 10.0 * 6.207637786865234
Epoch 2730, val loss: 1.55430006980896
Epoch 2740, training loss: 62.060951232910156 = 0.005677929613739252 + 10.0 * 6.205527305603027
Epoch 2740, val loss: 1.5561448335647583
Epoch 2750, training loss: 62.080081939697266 = 0.005631803069263697 + 10.0 * 6.20744514465332
Epoch 2750, val loss: 1.5580871105194092
Epoch 2760, training loss: 62.06972122192383 = 0.005583682097494602 + 10.0 * 6.206413745880127
Epoch 2760, val loss: 1.5598649978637695
Epoch 2770, training loss: 62.05498504638672 = 0.005535618402063847 + 10.0 * 6.204945087432861
Epoch 2770, val loss: 1.561692476272583
Epoch 2780, training loss: 62.0501708984375 = 0.0054916515946388245 + 10.0 * 6.2044677734375
Epoch 2780, val loss: 1.5636709928512573
Epoch 2790, training loss: 62.04770278930664 = 0.005449120886623859 + 10.0 * 6.204225540161133
Epoch 2790, val loss: 1.5655075311660767
Epoch 2800, training loss: 62.05696105957031 = 0.005407605320215225 + 10.0 * 6.205155372619629
Epoch 2800, val loss: 1.5673490762710571
Epoch 2810, training loss: 62.11740493774414 = 0.0053636617958545685 + 10.0 * 6.2112040519714355
Epoch 2810, val loss: 1.5691670179367065
Epoch 2820, training loss: 62.08402633666992 = 0.00531966844573617 + 10.0 * 6.2078704833984375
Epoch 2820, val loss: 1.5708428621292114
Epoch 2830, training loss: 62.04927444458008 = 0.005277220159769058 + 10.0 * 6.204400062561035
Epoch 2830, val loss: 1.5726484060287476
Epoch 2840, training loss: 62.044734954833984 = 0.005237147677689791 + 10.0 * 6.203949928283691
Epoch 2840, val loss: 1.5746195316314697
Epoch 2850, training loss: 62.04106521606445 = 0.005198395345360041 + 10.0 * 6.203586578369141
Epoch 2850, val loss: 1.5762922763824463
Epoch 2860, training loss: 62.06599426269531 = 0.005161573179066181 + 10.0 * 6.206083297729492
Epoch 2860, val loss: 1.578041434288025
Epoch 2870, training loss: 62.054508209228516 = 0.005120445508509874 + 10.0 * 6.204938888549805
Epoch 2870, val loss: 1.5795408487319946
Epoch 2880, training loss: 62.061363220214844 = 0.0050796321593225 + 10.0 * 6.205628395080566
Epoch 2880, val loss: 1.5813021659851074
Epoch 2890, training loss: 62.04159927368164 = 0.005041918251663446 + 10.0 * 6.20365571975708
Epoch 2890, val loss: 1.5830661058425903
Epoch 2900, training loss: 62.05452346801758 = 0.005005639046430588 + 10.0 * 6.204951763153076
Epoch 2900, val loss: 1.584809422492981
Epoch 2910, training loss: 62.08659362792969 = 0.004967772401869297 + 10.0 * 6.208162784576416
Epoch 2910, val loss: 1.5863620042800903
Epoch 2920, training loss: 62.05548858642578 = 0.0049283928237855434 + 10.0 * 6.205056190490723
Epoch 2920, val loss: 1.5880287885665894
Epoch 2930, training loss: 62.04059982299805 = 0.004894079640507698 + 10.0 * 6.203570365905762
Epoch 2930, val loss: 1.589759111404419
Epoch 2940, training loss: 62.02893829345703 = 0.004859050270169973 + 10.0 * 6.2024078369140625
Epoch 2940, val loss: 1.59135901927948
Epoch 2950, training loss: 62.04353332519531 = 0.004825900308787823 + 10.0 * 6.20387077331543
Epoch 2950, val loss: 1.5929832458496094
Epoch 2960, training loss: 62.08245086669922 = 0.00479202251881361 + 10.0 * 6.207766056060791
Epoch 2960, val loss: 1.5944771766662598
Epoch 2970, training loss: 62.04513931274414 = 0.004755998030304909 + 10.0 * 6.204038143157959
Epoch 2970, val loss: 1.5959185361862183
Epoch 2980, training loss: 62.05077362060547 = 0.004722521174699068 + 10.0 * 6.2046051025390625
Epoch 2980, val loss: 1.5976111888885498
Epoch 2990, training loss: 62.044044494628906 = 0.004689498338848352 + 10.0 * 6.203935623168945
Epoch 2990, val loss: 1.5991547107696533
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 87.90834045410156 = 1.9396480321884155 + 10.0 * 8.596868515014648
Epoch 0, val loss: 1.9294499158859253
Epoch 10, training loss: 87.89485931396484 = 1.9297454357147217 + 10.0 * 8.596510887145996
Epoch 10, val loss: 1.9201728105545044
Epoch 20, training loss: 87.85558319091797 = 1.9177721738815308 + 10.0 * 8.593781471252441
Epoch 20, val loss: 1.908630132675171
Epoch 30, training loss: 87.64540100097656 = 1.9022502899169922 + 10.0 * 8.574315071105957
Epoch 30, val loss: 1.8935909271240234
Epoch 40, training loss: 86.61481475830078 = 1.8830819129943848 + 10.0 * 8.473173141479492
Epoch 40, val loss: 1.8759385347366333
Epoch 50, training loss: 82.36860656738281 = 1.86397123336792 + 10.0 * 8.050463676452637
Epoch 50, val loss: 1.859408974647522
Epoch 60, training loss: 77.31950378417969 = 1.8449972867965698 + 10.0 * 7.547451019287109
Epoch 60, val loss: 1.8438208103179932
Epoch 70, training loss: 73.1765365600586 = 1.8311272859573364 + 10.0 * 7.134540557861328
Epoch 70, val loss: 1.831081509590149
Epoch 80, training loss: 71.24988555908203 = 1.815956950187683 + 10.0 * 6.943393230438232
Epoch 80, val loss: 1.8171809911727905
Epoch 90, training loss: 70.36453247070312 = 1.8035590648651123 + 10.0 * 6.85609769821167
Epoch 90, val loss: 1.8059965372085571
Epoch 100, training loss: 69.90216827392578 = 1.7915425300598145 + 10.0 * 6.811062335968018
Epoch 100, val loss: 1.7953424453735352
Epoch 110, training loss: 69.48894500732422 = 1.7791715860366821 + 10.0 * 6.770977020263672
Epoch 110, val loss: 1.784245252609253
Epoch 120, training loss: 69.08196258544922 = 1.7664097547531128 + 10.0 * 6.731555461883545
Epoch 120, val loss: 1.772779107093811
Epoch 130, training loss: 68.73851776123047 = 1.7535474300384521 + 10.0 * 6.6984968185424805
Epoch 130, val loss: 1.7609885931015015
Epoch 140, training loss: 68.4059066772461 = 1.7398734092712402 + 10.0 * 6.666603088378906
Epoch 140, val loss: 1.7483464479446411
Epoch 150, training loss: 68.13275146484375 = 1.7247947454452515 + 10.0 * 6.640795707702637
Epoch 150, val loss: 1.7347108125686646
Epoch 160, training loss: 67.91572570800781 = 1.708144187927246 + 10.0 * 6.620758056640625
Epoch 160, val loss: 1.7197563648223877
Epoch 170, training loss: 67.69243621826172 = 1.6896333694458008 + 10.0 * 6.600279808044434
Epoch 170, val loss: 1.7033814191818237
Epoch 180, training loss: 67.46864318847656 = 1.6696498394012451 + 10.0 * 6.579899311065674
Epoch 180, val loss: 1.6857398748397827
Epoch 190, training loss: 67.24250030517578 = 1.6480880975723267 + 10.0 * 6.559441566467285
Epoch 190, val loss: 1.6666162014007568
Epoch 200, training loss: 67.01815795898438 = 1.6246966123580933 + 10.0 * 6.539346694946289
Epoch 200, val loss: 1.6459494829177856
Epoch 210, training loss: 66.82848358154297 = 1.5992891788482666 + 10.0 * 6.522919654846191
Epoch 210, val loss: 1.6236793994903564
Epoch 220, training loss: 66.60469055175781 = 1.5716594457626343 + 10.0 * 6.503303050994873
Epoch 220, val loss: 1.5996520519256592
Epoch 230, training loss: 66.42188262939453 = 1.5419031381607056 + 10.0 * 6.4879984855651855
Epoch 230, val loss: 1.5739681720733643
Epoch 240, training loss: 66.28219604492188 = 1.5100678205490112 + 10.0 * 6.477212905883789
Epoch 240, val loss: 1.546571135520935
Epoch 250, training loss: 66.12327575683594 = 1.4762742519378662 + 10.0 * 6.464700698852539
Epoch 250, val loss: 1.5177428722381592
Epoch 260, training loss: 65.98948669433594 = 1.440956950187683 + 10.0 * 6.454853057861328
Epoch 260, val loss: 1.487900972366333
Epoch 270, training loss: 65.85205841064453 = 1.404471516609192 + 10.0 * 6.444758415222168
Epoch 270, val loss: 1.457289457321167
Epoch 280, training loss: 65.7273941040039 = 1.3671334981918335 + 10.0 * 6.436025619506836
Epoch 280, val loss: 1.426317811012268
Epoch 290, training loss: 65.6119384765625 = 1.3291704654693604 + 10.0 * 6.428276538848877
Epoch 290, val loss: 1.39510977268219
Epoch 300, training loss: 65.52862548828125 = 1.2908021211624146 + 10.0 * 6.4237823486328125
Epoch 300, val loss: 1.3638614416122437
Epoch 310, training loss: 65.40896606445312 = 1.2521321773529053 + 10.0 * 6.415683746337891
Epoch 310, val loss: 1.3326252698898315
Epoch 320, training loss: 65.31715393066406 = 1.2137782573699951 + 10.0 * 6.410337924957275
Epoch 320, val loss: 1.3018622398376465
Epoch 330, training loss: 65.21354675292969 = 1.1755856275558472 + 10.0 * 6.403796195983887
Epoch 330, val loss: 1.2715431451797485
Epoch 340, training loss: 65.1278076171875 = 1.1378870010375977 + 10.0 * 6.39899206161499
Epoch 340, val loss: 1.241881012916565
Epoch 350, training loss: 65.04395294189453 = 1.1008431911468506 + 10.0 * 6.39431095123291
Epoch 350, val loss: 1.2129048109054565
Epoch 360, training loss: 64.97570037841797 = 1.064621090888977 + 10.0 * 6.39110803604126
Epoch 360, val loss: 1.1848421096801758
Epoch 370, training loss: 64.87574005126953 = 1.0295788049697876 + 10.0 * 6.384616374969482
Epoch 370, val loss: 1.1578316688537598
Epoch 380, training loss: 64.79595947265625 = 0.995608389377594 + 10.0 * 6.380035400390625
Epoch 380, val loss: 1.131990671157837
Epoch 390, training loss: 64.72803497314453 = 0.9627724885940552 + 10.0 * 6.37652587890625
Epoch 390, val loss: 1.1073055267333984
Epoch 400, training loss: 64.65410614013672 = 0.9311326146125793 + 10.0 * 6.372297286987305
Epoch 400, val loss: 1.0836658477783203
Epoch 410, training loss: 64.581787109375 = 0.9007104635238647 + 10.0 * 6.368107795715332
Epoch 410, val loss: 1.0613701343536377
Epoch 420, training loss: 64.51079559326172 = 0.8715804815292358 + 10.0 * 6.363921642303467
Epoch 420, val loss: 1.0402913093566895
Epoch 430, training loss: 64.50070190429688 = 0.8436682820320129 + 10.0 * 6.365703105926514
Epoch 430, val loss: 1.0204076766967773
Epoch 440, training loss: 64.40526580810547 = 0.8167788982391357 + 10.0 * 6.358849048614502
Epoch 440, val loss: 1.0015954971313477
Epoch 450, training loss: 64.33643341064453 = 0.7911401987075806 + 10.0 * 6.35452938079834
Epoch 450, val loss: 0.9841287136077881
Epoch 460, training loss: 64.27681732177734 = 0.7666167616844177 + 10.0 * 6.351019859313965
Epoch 460, val loss: 0.96775883436203
Epoch 470, training loss: 64.23983764648438 = 0.7430496215820312 + 10.0 * 6.349678993225098
Epoch 470, val loss: 0.9525159001350403
Epoch 480, training loss: 64.20848846435547 = 0.7203412652015686 + 10.0 * 6.348814487457275
Epoch 480, val loss: 0.938130259513855
Epoch 490, training loss: 64.12865447998047 = 0.6984279155731201 + 10.0 * 6.34302282333374
Epoch 490, val loss: 0.9246575832366943
Epoch 500, training loss: 64.079833984375 = 0.6774325370788574 + 10.0 * 6.340240478515625
Epoch 500, val loss: 0.9122129082679749
Epoch 510, training loss: 64.03359985351562 = 0.6571903824806213 + 10.0 * 6.337640762329102
Epoch 510, val loss: 0.9005688428878784
Epoch 520, training loss: 64.00334930419922 = 0.637540876865387 + 10.0 * 6.336580753326416
Epoch 520, val loss: 0.8896631598472595
Epoch 530, training loss: 63.95697784423828 = 0.618591845035553 + 10.0 * 6.33383846282959
Epoch 530, val loss: 0.8793080449104309
Epoch 540, training loss: 63.90442657470703 = 0.6002210974693298 + 10.0 * 6.33042049407959
Epoch 540, val loss: 0.8698145747184753
Epoch 550, training loss: 63.857181549072266 = 0.5824839472770691 + 10.0 * 6.327469825744629
Epoch 550, val loss: 0.8609554171562195
Epoch 560, training loss: 63.81443786621094 = 0.5652551054954529 + 10.0 * 6.324918270111084
Epoch 560, val loss: 0.8527197241783142
Epoch 570, training loss: 63.88139343261719 = 0.5484987497329712 + 10.0 * 6.33328914642334
Epoch 570, val loss: 0.8449732065200806
Epoch 580, training loss: 63.73931884765625 = 0.5320116281509399 + 10.0 * 6.320730686187744
Epoch 580, val loss: 0.8375846743583679
Epoch 590, training loss: 63.706443786621094 = 0.5160681009292603 + 10.0 * 6.319037437438965
Epoch 590, val loss: 0.8308214545249939
Epoch 600, training loss: 63.67276382446289 = 0.5005622506141663 + 10.0 * 6.317220211029053
Epoch 600, val loss: 0.8246310353279114
Epoch 610, training loss: 63.63423156738281 = 0.4853818118572235 + 10.0 * 6.314885139465332
Epoch 610, val loss: 0.8187563419342041
Epoch 620, training loss: 63.60556411743164 = 0.4705537259578705 + 10.0 * 6.313500881195068
Epoch 620, val loss: 0.8133369088172913
Epoch 630, training loss: 63.5886116027832 = 0.4560827612876892 + 10.0 * 6.313252925872803
Epoch 630, val loss: 0.808340847492218
Epoch 640, training loss: 63.53665542602539 = 0.441921204328537 + 10.0 * 6.309473514556885
Epoch 640, val loss: 0.8037702441215515
Epoch 650, training loss: 63.51507568359375 = 0.42813003063201904 + 10.0 * 6.308694362640381
Epoch 650, val loss: 0.799497127532959
Epoch 660, training loss: 63.50205993652344 = 0.4144790768623352 + 10.0 * 6.30875825881958
Epoch 660, val loss: 0.7955886721611023
Epoch 670, training loss: 63.43671417236328 = 0.40123897790908813 + 10.0 * 6.3035478591918945
Epoch 670, val loss: 0.7920036315917969
Epoch 680, training loss: 63.40485763549805 = 0.3883444666862488 + 10.0 * 6.301651477813721
Epoch 680, val loss: 0.7889106869697571
Epoch 690, training loss: 63.377655029296875 = 0.3757624626159668 + 10.0 * 6.300189018249512
Epoch 690, val loss: 0.7861331701278687
Epoch 700, training loss: 63.471763610839844 = 0.3634963929653168 + 10.0 * 6.310826778411865
Epoch 700, val loss: 0.7836447358131409
Epoch 710, training loss: 63.34033203125 = 0.35136789083480835 + 10.0 * 6.298896312713623
Epoch 710, val loss: 0.7812692523002625
Epoch 720, training loss: 63.311153411865234 = 0.3397120237350464 + 10.0 * 6.297144412994385
Epoch 720, val loss: 0.7792955040931702
Epoch 730, training loss: 63.27496337890625 = 0.3284464478492737 + 10.0 * 6.294651985168457
Epoch 730, val loss: 0.7778226733207703
Epoch 740, training loss: 63.24516677856445 = 0.3175457715988159 + 10.0 * 6.29276180267334
Epoch 740, val loss: 0.7766754627227783
Epoch 750, training loss: 63.29124450683594 = 0.3069721460342407 + 10.0 * 6.298427104949951
Epoch 750, val loss: 0.7758539319038391
Epoch 760, training loss: 63.21168518066406 = 0.29663243889808655 + 10.0 * 6.291505336761475
Epoch 760, val loss: 0.7752395272254944
Epoch 770, training loss: 63.17938232421875 = 0.28667309880256653 + 10.0 * 6.289270877838135
Epoch 770, val loss: 0.7750042676925659
Epoch 780, training loss: 63.1629753112793 = 0.2770651578903198 + 10.0 * 6.288590908050537
Epoch 780, val loss: 0.7751888632774353
Epoch 790, training loss: 63.138938903808594 = 0.267760306596756 + 10.0 * 6.287117958068848
Epoch 790, val loss: 0.7755672335624695
Epoch 800, training loss: 63.10557556152344 = 0.25878968834877014 + 10.0 * 6.2846784591674805
Epoch 800, val loss: 0.7762494087219238
Epoch 810, training loss: 63.08631134033203 = 0.2501685917377472 + 10.0 * 6.283614158630371
Epoch 810, val loss: 0.777279257774353
Epoch 820, training loss: 63.086456298828125 = 0.24185925722122192 + 10.0 * 6.284459590911865
Epoch 820, val loss: 0.7786381244659424
Epoch 830, training loss: 63.0645751953125 = 0.23375746607780457 + 10.0 * 6.283082008361816
Epoch 830, val loss: 0.780283510684967
Epoch 840, training loss: 63.03371047973633 = 0.22597825527191162 + 10.0 * 6.280773162841797
Epoch 840, val loss: 0.7821019291877747
Epoch 850, training loss: 63.011138916015625 = 0.2185160517692566 + 10.0 * 6.279262065887451
Epoch 850, val loss: 0.7843270301818848
Epoch 860, training loss: 62.99070739746094 = 0.21131575107574463 + 10.0 * 6.277939319610596
Epoch 860, val loss: 0.7868049740791321
Epoch 870, training loss: 63.03811264038086 = 0.20437870919704437 + 10.0 * 6.2833733558654785
Epoch 870, val loss: 0.789461076259613
Epoch 880, training loss: 62.99172592163086 = 0.19760671257972717 + 10.0 * 6.279412269592285
Epoch 880, val loss: 0.7923466563224792
Epoch 890, training loss: 62.96808624267578 = 0.19110925495624542 + 10.0 * 6.277697563171387
Epoch 890, val loss: 0.7955172061920166
Epoch 900, training loss: 62.92857360839844 = 0.1848313957452774 + 10.0 * 6.274374485015869
Epoch 900, val loss: 0.7989336848258972
Epoch 910, training loss: 62.919151306152344 = 0.17879101634025574 + 10.0 * 6.274035930633545
Epoch 910, val loss: 0.8026329278945923
Epoch 920, training loss: 62.910400390625 = 0.17295832931995392 + 10.0 * 6.273744106292725
Epoch 920, val loss: 0.8065093755722046
Epoch 930, training loss: 62.911617279052734 = 0.16732090711593628 + 10.0 * 6.274429798126221
Epoch 930, val loss: 0.8105186820030212
Epoch 940, training loss: 62.86286544799805 = 0.1618289202451706 + 10.0 * 6.270103931427002
Epoch 940, val loss: 0.8147053718566895
Epoch 950, training loss: 62.8556022644043 = 0.15657168626785278 + 10.0 * 6.269903182983398
Epoch 950, val loss: 0.8191799521446228
Epoch 960, training loss: 62.87074661254883 = 0.15152160823345184 + 10.0 * 6.271922588348389
Epoch 960, val loss: 0.8238696455955505
Epoch 970, training loss: 62.83671188354492 = 0.14662228524684906 + 10.0 * 6.269009113311768
Epoch 970, val loss: 0.8285214900970459
Epoch 980, training loss: 62.82097244262695 = 0.14190691709518433 + 10.0 * 6.267906665802002
Epoch 980, val loss: 0.833463728427887
Epoch 990, training loss: 62.84036636352539 = 0.1373702585697174 + 10.0 * 6.270299434661865
Epoch 990, val loss: 0.83853679895401
Epoch 1000, training loss: 62.79933547973633 = 0.13293515145778656 + 10.0 * 6.2666401863098145
Epoch 1000, val loss: 0.8436995148658752
Epoch 1010, training loss: 62.77680587768555 = 0.12868434190750122 + 10.0 * 6.264811992645264
Epoch 1010, val loss: 0.848977267742157
Epoch 1020, training loss: 62.76588439941406 = 0.12458731979131699 + 10.0 * 6.264129638671875
Epoch 1020, val loss: 0.8545278906822205
Epoch 1030, training loss: 62.77999496459961 = 0.12063527852296829 + 10.0 * 6.265935897827148
Epoch 1030, val loss: 0.8601252436637878
Epoch 1040, training loss: 62.754058837890625 = 0.11680284887552261 + 10.0 * 6.263725757598877
Epoch 1040, val loss: 0.865828812122345
Epoch 1050, training loss: 62.751686096191406 = 0.11310464888811111 + 10.0 * 6.263857841491699
Epoch 1050, val loss: 0.871592104434967
Epoch 1060, training loss: 62.72650909423828 = 0.10954320430755615 + 10.0 * 6.261696815490723
Epoch 1060, val loss: 0.8774524331092834
Epoch 1070, training loss: 62.711341857910156 = 0.10610810667276382 + 10.0 * 6.260523319244385
Epoch 1070, val loss: 0.883447527885437
Epoch 1080, training loss: 62.720916748046875 = 0.1028081625699997 + 10.0 * 6.261810779571533
Epoch 1080, val loss: 0.889611005783081
Epoch 1090, training loss: 62.702880859375 = 0.09961248189210892 + 10.0 * 6.260326862335205
Epoch 1090, val loss: 0.8957784175872803
Epoch 1100, training loss: 62.726383209228516 = 0.09653683751821518 + 10.0 * 6.262984275817871
Epoch 1100, val loss: 0.9020805358886719
Epoch 1110, training loss: 62.6783447265625 = 0.09354358911514282 + 10.0 * 6.258480072021484
Epoch 1110, val loss: 0.9082897305488586
Epoch 1120, training loss: 62.66587829589844 = 0.09068281948566437 + 10.0 * 6.257519721984863
Epoch 1120, val loss: 0.914853572845459
Epoch 1130, training loss: 62.653465270996094 = 0.0879482850432396 + 10.0 * 6.256551742553711
Epoch 1130, val loss: 0.9214407801628113
Epoch 1140, training loss: 62.661903381347656 = 0.08530869334936142 + 10.0 * 6.257659435272217
Epoch 1140, val loss: 0.9280931353569031
Epoch 1150, training loss: 62.66185760498047 = 0.08274447917938232 + 10.0 * 6.257911205291748
Epoch 1150, val loss: 0.9346420168876648
Epoch 1160, training loss: 62.62513732910156 = 0.08027524501085281 + 10.0 * 6.254486083984375
Epoch 1160, val loss: 0.941297173500061
Epoch 1170, training loss: 62.614227294921875 = 0.07790690660476685 + 10.0 * 6.253632068634033
Epoch 1170, val loss: 0.9480981826782227
Epoch 1180, training loss: 62.61565017700195 = 0.07563935965299606 + 10.0 * 6.254001140594482
Epoch 1180, val loss: 0.9548810124397278
Epoch 1190, training loss: 62.65888977050781 = 0.07344219088554382 + 10.0 * 6.258544921875
Epoch 1190, val loss: 0.9616486430168152
Epoch 1200, training loss: 62.6553840637207 = 0.07128968834877014 + 10.0 * 6.25840950012207
Epoch 1200, val loss: 0.9682267904281616
Epoch 1210, training loss: 62.59762954711914 = 0.06924408674240112 + 10.0 * 6.252838611602783
Epoch 1210, val loss: 0.9750077128410339
Epoch 1220, training loss: 62.57330322265625 = 0.06727512180805206 + 10.0 * 6.250602722167969
Epoch 1220, val loss: 0.9818364977836609
Epoch 1230, training loss: 62.56489181518555 = 0.06539332866668701 + 10.0 * 6.249949932098389
Epoch 1230, val loss: 0.9886881709098816
Epoch 1240, training loss: 62.576053619384766 = 0.06357874721288681 + 10.0 * 6.251247406005859
Epoch 1240, val loss: 0.9954566955566406
Epoch 1250, training loss: 62.5789794921875 = 0.06181299313902855 + 10.0 * 6.251716613769531
Epoch 1250, val loss: 1.0021976232528687
Epoch 1260, training loss: 62.56999588012695 = 0.06009911000728607 + 10.0 * 6.2509894371032715
Epoch 1260, val loss: 1.00875985622406
Epoch 1270, training loss: 62.543766021728516 = 0.05845070630311966 + 10.0 * 6.248531341552734
Epoch 1270, val loss: 1.0155415534973145
Epoch 1280, training loss: 62.533226013183594 = 0.05688074603676796 + 10.0 * 6.247634410858154
Epoch 1280, val loss: 1.0224159955978394
Epoch 1290, training loss: 62.529266357421875 = 0.055367253720760345 + 10.0 * 6.247389793395996
Epoch 1290, val loss: 1.0291409492492676
Epoch 1300, training loss: 62.6096076965332 = 0.05389881134033203 + 10.0 * 6.255570888519287
Epoch 1300, val loss: 1.0357794761657715
Epoch 1310, training loss: 62.51665496826172 = 0.05245094746351242 + 10.0 * 6.246420383453369
Epoch 1310, val loss: 1.0423493385314941
Epoch 1320, training loss: 62.51523208618164 = 0.05108123645186424 + 10.0 * 6.246415138244629
Epoch 1320, val loss: 1.0490435361862183
Epoch 1330, training loss: 62.50241470336914 = 0.049764227122068405 + 10.0 * 6.245265007019043
Epoch 1330, val loss: 1.0557399988174438
Epoch 1340, training loss: 62.56123352050781 = 0.048492152243852615 + 10.0 * 6.251274108886719
Epoch 1340, val loss: 1.0623191595077515
Epoch 1350, training loss: 62.53651809692383 = 0.04725278913974762 + 10.0 * 6.248926639556885
Epoch 1350, val loss: 1.0688323974609375
Epoch 1360, training loss: 62.490779876708984 = 0.04605274274945259 + 10.0 * 6.244472980499268
Epoch 1360, val loss: 1.0754034519195557
Epoch 1370, training loss: 62.47857666015625 = 0.044911302626132965 + 10.0 * 6.243366718292236
Epoch 1370, val loss: 1.0820122957229614
Epoch 1380, training loss: 62.48497772216797 = 0.04380827024579048 + 10.0 * 6.24411678314209
Epoch 1380, val loss: 1.088578224182129
Epoch 1390, training loss: 62.48823547363281 = 0.042733028531074524 + 10.0 * 6.2445502281188965
Epoch 1390, val loss: 1.094887137413025
Epoch 1400, training loss: 62.471378326416016 = 0.04168713092803955 + 10.0 * 6.242969036102295
Epoch 1400, val loss: 1.1012539863586426
Epoch 1410, training loss: 62.45876693725586 = 0.04068818688392639 + 10.0 * 6.24180793762207
Epoch 1410, val loss: 1.1076818704605103
Epoch 1420, training loss: 62.53861999511719 = 0.03971796855330467 + 10.0 * 6.249890327453613
Epoch 1420, val loss: 1.11392080783844
Epoch 1430, training loss: 62.4818000793457 = 0.038762547075748444 + 10.0 * 6.2443037033081055
Epoch 1430, val loss: 1.1201616525650024
Epoch 1440, training loss: 62.44731140136719 = 0.03784004598855972 + 10.0 * 6.2409467697143555
Epoch 1440, val loss: 1.1263628005981445
Epoch 1450, training loss: 62.437625885009766 = 0.03696860745549202 + 10.0 * 6.240065574645996
Epoch 1450, val loss: 1.1327705383300781
Epoch 1460, training loss: 62.42800521850586 = 0.036127153784036636 + 10.0 * 6.239187717437744
Epoch 1460, val loss: 1.1390864849090576
Epoch 1470, training loss: 62.42317581176758 = 0.035313818603754044 + 10.0 * 6.238786220550537
Epoch 1470, val loss: 1.1453410387039185
Epoch 1480, training loss: 62.436302185058594 = 0.03452489897608757 + 10.0 * 6.240178108215332
Epoch 1480, val loss: 1.1514644622802734
Epoch 1490, training loss: 62.435550689697266 = 0.03374873846769333 + 10.0 * 6.240180015563965
Epoch 1490, val loss: 1.1574879884719849
Epoch 1500, training loss: 62.423316955566406 = 0.0329861156642437 + 10.0 * 6.239033222198486
Epoch 1500, val loss: 1.1633871793746948
Epoch 1510, training loss: 62.413326263427734 = 0.03226222097873688 + 10.0 * 6.238106727600098
Epoch 1510, val loss: 1.169423222541809
Epoch 1520, training loss: 62.41139602661133 = 0.03156285360455513 + 10.0 * 6.237983226776123
Epoch 1520, val loss: 1.1753913164138794
Epoch 1530, training loss: 62.440956115722656 = 0.030883697792887688 + 10.0 * 6.241007328033447
Epoch 1530, val loss: 1.1812623739242554
Epoch 1540, training loss: 62.420875549316406 = 0.030221054330468178 + 10.0 * 6.239065647125244
Epoch 1540, val loss: 1.1872156858444214
Epoch 1550, training loss: 62.401512145996094 = 0.02957586944103241 + 10.0 * 6.237193584442139
Epoch 1550, val loss: 1.1930879354476929
Epoch 1560, training loss: 62.39580535888672 = 0.028956975787878036 + 10.0 * 6.236684799194336
Epoch 1560, val loss: 1.1990444660186768
Epoch 1570, training loss: 62.468753814697266 = 0.02835889346897602 + 10.0 * 6.244039535522461
Epoch 1570, val loss: 1.2048043012619019
Epoch 1580, training loss: 62.40139389038086 = 0.027756068855524063 + 10.0 * 6.237363815307617
Epoch 1580, val loss: 1.2103354930877686
Epoch 1590, training loss: 62.377262115478516 = 0.02719377912580967 + 10.0 * 6.235006809234619
Epoch 1590, val loss: 1.2162072658538818
Epoch 1600, training loss: 62.36920166015625 = 0.02664763852953911 + 10.0 * 6.234255313873291
Epoch 1600, val loss: 1.221947193145752
Epoch 1610, training loss: 62.3635368347168 = 0.026120098307728767 + 10.0 * 6.233741760253906
Epoch 1610, val loss: 1.2277508974075317
Epoch 1620, training loss: 62.384010314941406 = 0.025607941672205925 + 10.0 * 6.235840320587158
Epoch 1620, val loss: 1.2334344387054443
Epoch 1630, training loss: 62.37813949584961 = 0.025099700316786766 + 10.0 * 6.23530387878418
Epoch 1630, val loss: 1.2388815879821777
Epoch 1640, training loss: 62.374855041503906 = 0.02460516430437565 + 10.0 * 6.235024929046631
Epoch 1640, val loss: 1.2445467710494995
Epoch 1650, training loss: 62.39533996582031 = 0.024130279198288918 + 10.0 * 6.237120628356934
Epoch 1650, val loss: 1.250036358833313
Epoch 1660, training loss: 62.36378860473633 = 0.02366269752383232 + 10.0 * 6.234012603759766
Epoch 1660, val loss: 1.255509614944458
Epoch 1670, training loss: 62.35816192626953 = 0.023215604946017265 + 10.0 * 6.233494758605957
Epoch 1670, val loss: 1.2609885931015015
Epoch 1680, training loss: 62.35200119018555 = 0.0227802861481905 + 10.0 * 6.232922077178955
Epoch 1680, val loss: 1.2664413452148438
Epoch 1690, training loss: 62.34872055053711 = 0.02235758677124977 + 10.0 * 6.232636451721191
Epoch 1690, val loss: 1.271891474723816
Epoch 1700, training loss: 62.365882873535156 = 0.021948084235191345 + 10.0 * 6.234393119812012
Epoch 1700, val loss: 1.2773712873458862
Epoch 1710, training loss: 62.346927642822266 = 0.021542495116591454 + 10.0 * 6.23253870010376
Epoch 1710, val loss: 1.2826086282730103
Epoch 1720, training loss: 62.32632827758789 = 0.02114787884056568 + 10.0 * 6.230517864227295
Epoch 1720, val loss: 1.2878310680389404
Epoch 1730, training loss: 62.32973861694336 = 0.020769581198692322 + 10.0 * 6.230896949768066
Epoch 1730, val loss: 1.2930670976638794
Epoch 1740, training loss: 62.38215255737305 = 0.02040066197514534 + 10.0 * 6.236175060272217
Epoch 1740, val loss: 1.2981678247451782
Epoch 1750, training loss: 62.350833892822266 = 0.02003675140440464 + 10.0 * 6.233079433441162
Epoch 1750, val loss: 1.3033711910247803
Epoch 1760, training loss: 62.32448196411133 = 0.019680114462971687 + 10.0 * 6.230480194091797
Epoch 1760, val loss: 1.3083500862121582
Epoch 1770, training loss: 62.31232452392578 = 0.019341984763741493 + 10.0 * 6.2292985916137695
Epoch 1770, val loss: 1.3136039972305298
Epoch 1780, training loss: 62.32634735107422 = 0.0190128143876791 + 10.0 * 6.230733394622803
Epoch 1780, val loss: 1.3186177015304565
Epoch 1790, training loss: 62.34099578857422 = 0.018686212599277496 + 10.0 * 6.232231140136719
Epoch 1790, val loss: 1.3235437870025635
Epoch 1800, training loss: 62.31147003173828 = 0.018365314230322838 + 10.0 * 6.229310512542725
Epoch 1800, val loss: 1.32852303981781
Epoch 1810, training loss: 62.30149841308594 = 0.01805754192173481 + 10.0 * 6.228343963623047
Epoch 1810, val loss: 1.333495020866394
Epoch 1820, training loss: 62.300804138183594 = 0.01776289753615856 + 10.0 * 6.228304386138916
Epoch 1820, val loss: 1.3384981155395508
Epoch 1830, training loss: 62.33844757080078 = 0.017473578453063965 + 10.0 * 6.232097148895264
Epoch 1830, val loss: 1.3433687686920166
Epoch 1840, training loss: 62.30415725708008 = 0.017185071483254433 + 10.0 * 6.228697299957275
Epoch 1840, val loss: 1.3481558561325073
Epoch 1850, training loss: 62.350608825683594 = 0.016905922442674637 + 10.0 * 6.233370304107666
Epoch 1850, val loss: 1.3529330492019653
Epoch 1860, training loss: 62.320865631103516 = 0.016629505902528763 + 10.0 * 6.230423927307129
Epoch 1860, val loss: 1.3575607538223267
Epoch 1870, training loss: 62.288394927978516 = 0.01636170968413353 + 10.0 * 6.227203369140625
Epoch 1870, val loss: 1.362386703491211
Epoch 1880, training loss: 62.28245162963867 = 0.016107408329844475 + 10.0 * 6.226634502410889
Epoch 1880, val loss: 1.3672091960906982
Epoch 1890, training loss: 62.27756118774414 = 0.01586066372692585 + 10.0 * 6.226170063018799
Epoch 1890, val loss: 1.3718856573104858
Epoch 1900, training loss: 62.30266571044922 = 0.015620565041899681 + 10.0 * 6.228704452514648
Epoch 1900, val loss: 1.376530647277832
Epoch 1910, training loss: 62.28710174560547 = 0.015375956892967224 + 10.0 * 6.227172374725342
Epoch 1910, val loss: 1.3810303211212158
Epoch 1920, training loss: 62.291969299316406 = 0.015138586983084679 + 10.0 * 6.227683067321777
Epoch 1920, val loss: 1.3855953216552734
Epoch 1930, training loss: 62.29545211791992 = 0.014908470213413239 + 10.0 * 6.228054523468018
Epoch 1930, val loss: 1.390120267868042
Epoch 1940, training loss: 62.2722282409668 = 0.014681363478302956 + 10.0 * 6.225754737854004
Epoch 1940, val loss: 1.3945950269699097
Epoch 1950, training loss: 62.26423263549805 = 0.014463384635746479 + 10.0 * 6.224977016448975
Epoch 1950, val loss: 1.3990452289581299
Epoch 1960, training loss: 62.26953887939453 = 0.014254217967391014 + 10.0 * 6.225528240203857
Epoch 1960, val loss: 1.4035638570785522
Epoch 1970, training loss: 62.32071304321289 = 0.014047757722437382 + 10.0 * 6.230666160583496
Epoch 1970, val loss: 1.407885193824768
Epoch 1980, training loss: 62.27727127075195 = 0.013836910016834736 + 10.0 * 6.226343631744385
Epoch 1980, val loss: 1.4121582508087158
Epoch 1990, training loss: 62.2523078918457 = 0.013635916635394096 + 10.0 * 6.223866939544678
Epoch 1990, val loss: 1.4165136814117432
Epoch 2000, training loss: 62.252479553222656 = 0.013443133793771267 + 10.0 * 6.223903656005859
Epoch 2000, val loss: 1.420843243598938
Epoch 2010, training loss: 62.271484375 = 0.013256332837045193 + 10.0 * 6.225822925567627
Epoch 2010, val loss: 1.425187587738037
Epoch 2020, training loss: 62.284690856933594 = 0.013070917688310146 + 10.0 * 6.2271623611450195
Epoch 2020, val loss: 1.4294393062591553
Epoch 2030, training loss: 62.243228912353516 = 0.01288201566785574 + 10.0 * 6.223034858703613
Epoch 2030, val loss: 1.4334086179733276
Epoch 2040, training loss: 62.24272918701172 = 0.012703755870461464 + 10.0 * 6.2230024337768555
Epoch 2040, val loss: 1.4377080202102661
Epoch 2050, training loss: 62.23859405517578 = 0.012533693574368954 + 10.0 * 6.2226057052612305
Epoch 2050, val loss: 1.4419574737548828
Epoch 2060, training loss: 62.2503547668457 = 0.012366779148578644 + 10.0 * 6.223798751831055
Epoch 2060, val loss: 1.4461619853973389
Epoch 2070, training loss: 62.26249694824219 = 0.012198720127344131 + 10.0 * 6.225029945373535
Epoch 2070, val loss: 1.4501394033432007
Epoch 2080, training loss: 62.25320053100586 = 0.012029296718537807 + 10.0 * 6.224117279052734
Epoch 2080, val loss: 1.4541746377944946
Epoch 2090, training loss: 62.23860168457031 = 0.011868136934936047 + 10.0 * 6.222673416137695
Epoch 2090, val loss: 1.4581385850906372
Epoch 2100, training loss: 62.23392868041992 = 0.011713881976902485 + 10.0 * 6.222221374511719
Epoch 2100, val loss: 1.4623267650604248
Epoch 2110, training loss: 62.29148864746094 = 0.01156479399651289 + 10.0 * 6.227992057800293
Epoch 2110, val loss: 1.466343641281128
Epoch 2120, training loss: 62.23465347290039 = 0.011409148573875427 + 10.0 * 6.222324371337891
Epoch 2120, val loss: 1.4700486660003662
Epoch 2130, training loss: 62.23833084106445 = 0.011264429427683353 + 10.0 * 6.2227067947387695
Epoch 2130, val loss: 1.474164605140686
Epoch 2140, training loss: 62.251529693603516 = 0.011121226474642754 + 10.0 * 6.224040985107422
Epoch 2140, val loss: 1.4780259132385254
Epoch 2150, training loss: 62.21894454956055 = 0.010977164842188358 + 10.0 * 6.220796585083008
Epoch 2150, val loss: 1.4818111658096313
Epoch 2160, training loss: 62.21584701538086 = 0.010840876959264278 + 10.0 * 6.220500469207764
Epoch 2160, val loss: 1.4856752157211304
Epoch 2170, training loss: 62.238525390625 = 0.010708707384765148 + 10.0 * 6.222781658172607
Epoch 2170, val loss: 1.4895509481430054
Epoch 2180, training loss: 62.23837661743164 = 0.010575108230113983 + 10.0 * 6.222780227661133
Epoch 2180, val loss: 1.4931225776672363
Epoch 2190, training loss: 62.21104049682617 = 0.010441126301884651 + 10.0 * 6.220059871673584
Epoch 2190, val loss: 1.4968758821487427
Epoch 2200, training loss: 62.20597839355469 = 0.010314853861927986 + 10.0 * 6.219566345214844
Epoch 2200, val loss: 1.5007888078689575
Epoch 2210, training loss: 62.22351837158203 = 0.01019355934113264 + 10.0 * 6.221332550048828
Epoch 2210, val loss: 1.5044742822647095
Epoch 2220, training loss: 62.23711013793945 = 0.010069913230836391 + 10.0 * 6.22270393371582
Epoch 2220, val loss: 1.5080139636993408
Epoch 2230, training loss: 62.21392822265625 = 0.009947653859853745 + 10.0 * 6.22039794921875
Epoch 2230, val loss: 1.51154625415802
Epoch 2240, training loss: 62.21810531616211 = 0.009830376133322716 + 10.0 * 6.220827579498291
Epoch 2240, val loss: 1.5152478218078613
Epoch 2250, training loss: 62.21977615356445 = 0.009715739637613297 + 10.0 * 6.221005916595459
Epoch 2250, val loss: 1.518751859664917
Epoch 2260, training loss: 62.196022033691406 = 0.0096037732437253 + 10.0 * 6.218641757965088
Epoch 2260, val loss: 1.522491693496704
Epoch 2270, training loss: 62.20106887817383 = 0.009495801292359829 + 10.0 * 6.2191572189331055
Epoch 2270, val loss: 1.5260945558547974
Epoch 2280, training loss: 62.23503494262695 = 0.009389393962919712 + 10.0 * 6.222564697265625
Epoch 2280, val loss: 1.529639482498169
Epoch 2290, training loss: 62.203025817871094 = 0.009277664124965668 + 10.0 * 6.219374656677246
Epoch 2290, val loss: 1.5327813625335693
Epoch 2300, training loss: 62.191184997558594 = 0.009171743877232075 + 10.0 * 6.218201637268066
Epoch 2300, val loss: 1.5363330841064453
Epoch 2310, training loss: 62.18714904785156 = 0.009070025756955147 + 10.0 * 6.217807769775391
Epoch 2310, val loss: 1.5399507284164429
Epoch 2320, training loss: 62.22071838378906 = 0.008971544913947582 + 10.0 * 6.221174716949463
Epoch 2320, val loss: 1.5431878566741943
Epoch 2330, training loss: 62.19689178466797 = 0.008871243335306644 + 10.0 * 6.218801975250244
Epoch 2330, val loss: 1.5466629266738892
Epoch 2340, training loss: 62.201934814453125 = 0.008772717788815498 + 10.0 * 6.219316005706787
Epoch 2340, val loss: 1.5498034954071045
Epoch 2350, training loss: 62.18680953979492 = 0.00867634266614914 + 10.0 * 6.217813491821289
Epoch 2350, val loss: 1.5533335208892822
Epoch 2360, training loss: 62.200599670410156 = 0.008584228344261646 + 10.0 * 6.219201564788818
Epoch 2360, val loss: 1.5566365718841553
Epoch 2370, training loss: 62.17384719848633 = 0.008492302149534225 + 10.0 * 6.216535568237305
Epoch 2370, val loss: 1.5599989891052246
Epoch 2380, training loss: 62.17256164550781 = 0.008403603918850422 + 10.0 * 6.216415882110596
Epoch 2380, val loss: 1.5633162260055542
Epoch 2390, training loss: 62.20783233642578 = 0.00831716600805521 + 10.0 * 6.219951629638672
Epoch 2390, val loss: 1.566559910774231
Epoch 2400, training loss: 62.188297271728516 = 0.008227488957345486 + 10.0 * 6.2180070877075195
Epoch 2400, val loss: 1.569689393043518
Epoch 2410, training loss: 62.16841125488281 = 0.00814002938568592 + 10.0 * 6.21602725982666
Epoch 2410, val loss: 1.5728743076324463
Epoch 2420, training loss: 62.1766357421875 = 0.008056439459323883 + 10.0 * 6.21685791015625
Epoch 2420, val loss: 1.576072335243225
Epoch 2430, training loss: 62.202003479003906 = 0.007975919172167778 + 10.0 * 6.21940279006958
Epoch 2430, val loss: 1.5792335271835327
Epoch 2440, training loss: 62.17489242553711 = 0.007892432622611523 + 10.0 * 6.216700077056885
Epoch 2440, val loss: 1.582306981086731
Epoch 2450, training loss: 62.163848876953125 = 0.007813266478478909 + 10.0 * 6.215603828430176
Epoch 2450, val loss: 1.5854400396347046
Epoch 2460, training loss: 62.17788314819336 = 0.007736255414783955 + 10.0 * 6.217014789581299
Epoch 2460, val loss: 1.588648796081543
Epoch 2470, training loss: 62.16761016845703 = 0.007659029681235552 + 10.0 * 6.215994834899902
Epoch 2470, val loss: 1.5917221307754517
Epoch 2480, training loss: 62.1990852355957 = 0.007583830505609512 + 10.0 * 6.219150066375732
Epoch 2480, val loss: 1.594794511795044
Epoch 2490, training loss: 62.164512634277344 = 0.007506511639803648 + 10.0 * 6.215700626373291
Epoch 2490, val loss: 1.5975593328475952
Epoch 2500, training loss: 62.15815353393555 = 0.00743387034162879 + 10.0 * 6.215071678161621
Epoch 2500, val loss: 1.6006890535354614
Epoch 2510, training loss: 62.18347930908203 = 0.007363110315054655 + 10.0 * 6.217611789703369
Epoch 2510, val loss: 1.6037737131118774
Epoch 2520, training loss: 62.1650505065918 = 0.007290898822247982 + 10.0 * 6.215775966644287
Epoch 2520, val loss: 1.6065237522125244
Epoch 2530, training loss: 62.14958572387695 = 0.007219565100967884 + 10.0 * 6.214236259460449
Epoch 2530, val loss: 1.609466552734375
Epoch 2540, training loss: 62.14503860473633 = 0.007151872385293245 + 10.0 * 6.2137885093688965
Epoch 2540, val loss: 1.6124927997589111
Epoch 2550, training loss: 62.14606857299805 = 0.007086772006005049 + 10.0 * 6.213898181915283
Epoch 2550, val loss: 1.615423560142517
Epoch 2560, training loss: 62.20786666870117 = 0.007022703066468239 + 10.0 * 6.220084190368652
Epoch 2560, val loss: 1.618212342262268
Epoch 2570, training loss: 62.19192123413086 = 0.006956007331609726 + 10.0 * 6.218496799468994
Epoch 2570, val loss: 1.6209912300109863
Epoch 2580, training loss: 62.15423583984375 = 0.006888105534017086 + 10.0 * 6.2147345542907715
Epoch 2580, val loss: 1.6236211061477661
Epoch 2590, training loss: 62.14510726928711 = 0.006825735792517662 + 10.0 * 6.213828086853027
Epoch 2590, val loss: 1.6263604164123535
Epoch 2600, training loss: 62.14113235473633 = 0.006765080615878105 + 10.0 * 6.213437080383301
Epoch 2600, val loss: 1.6292548179626465
Epoch 2610, training loss: 62.16334915161133 = 0.0067059374414384365 + 10.0 * 6.215664386749268
Epoch 2610, val loss: 1.632043719291687
Epoch 2620, training loss: 62.14613342285156 = 0.0066447011195123196 + 10.0 * 6.213948726654053
Epoch 2620, val loss: 1.6346815824508667
Epoch 2630, training loss: 62.157745361328125 = 0.006585685536265373 + 10.0 * 6.215116024017334
Epoch 2630, val loss: 1.6372733116149902
Epoch 2640, training loss: 62.15806198120117 = 0.006525961682200432 + 10.0 * 6.215153694152832
Epoch 2640, val loss: 1.6399281024932861
Epoch 2650, training loss: 62.13682174682617 = 0.006467494647949934 + 10.0 * 6.213035583496094
Epoch 2650, val loss: 1.6425145864486694
Epoch 2660, training loss: 62.13066482543945 = 0.006412560120224953 + 10.0 * 6.212425231933594
Epoch 2660, val loss: 1.6453125476837158
Epoch 2670, training loss: 62.12589645385742 = 0.006358520593494177 + 10.0 * 6.211953639984131
Epoch 2670, val loss: 1.6480213403701782
Epoch 2680, training loss: 62.149776458740234 = 0.00630652392283082 + 10.0 * 6.214346885681152
Epoch 2680, val loss: 1.6506561040878296
Epoch 2690, training loss: 62.14917755126953 = 0.0062517039477825165 + 10.0 * 6.214292526245117
Epoch 2690, val loss: 1.653082013130188
Epoch 2700, training loss: 62.133033752441406 = 0.006196278613060713 + 10.0 * 6.21268367767334
Epoch 2700, val loss: 1.6555522680282593
Epoch 2710, training loss: 62.12826919555664 = 0.0061441827565431595 + 10.0 * 6.212212562561035
Epoch 2710, val loss: 1.6582471132278442
Epoch 2720, training loss: 62.126686096191406 = 0.006094047799706459 + 10.0 * 6.212059020996094
Epoch 2720, val loss: 1.6608258485794067
Epoch 2730, training loss: 62.13111114501953 = 0.006044164765626192 + 10.0 * 6.2125067710876465
Epoch 2730, val loss: 1.6634337902069092
Epoch 2740, training loss: 62.11861038208008 = 0.005994528066366911 + 10.0 * 6.211261749267578
Epoch 2740, val loss: 1.665773630142212
Epoch 2750, training loss: 62.149024963378906 = 0.005946053192019463 + 10.0 * 6.21430778503418
Epoch 2750, val loss: 1.6681689023971558
Epoch 2760, training loss: 62.14712142944336 = 0.005897377151995897 + 10.0 * 6.214122295379639
Epoch 2760, val loss: 1.6708797216415405
Epoch 2770, training loss: 62.1402473449707 = 0.0058479150757193565 + 10.0 * 6.21343994140625
Epoch 2770, val loss: 1.673111915588379
Epoch 2780, training loss: 62.12080383300781 = 0.0058005028404295444 + 10.0 * 6.21150016784668
Epoch 2780, val loss: 1.6755305528640747
Epoch 2790, training loss: 62.107391357421875 = 0.005755056627094746 + 10.0 * 6.210163593292236
Epoch 2790, val loss: 1.6780743598937988
Epoch 2800, training loss: 62.14535903930664 = 0.005712095182389021 + 10.0 * 6.213964939117432
Epoch 2800, val loss: 1.680530071258545
Epoch 2810, training loss: 62.1020393371582 = 0.005664219614118338 + 10.0 * 6.209637641906738
Epoch 2810, val loss: 1.6828372478485107
Epoch 2820, training loss: 62.0991096496582 = 0.005619291681796312 + 10.0 * 6.209349155426025
Epoch 2820, val loss: 1.6851238012313843
Epoch 2830, training loss: 62.11506652832031 = 0.005576849449425936 + 10.0 * 6.210948944091797
Epoch 2830, val loss: 1.6875118017196655
Epoch 2840, training loss: 62.12995529174805 = 0.005533898249268532 + 10.0 * 6.212441921234131
Epoch 2840, val loss: 1.6897598505020142
Epoch 2850, training loss: 62.10453414916992 = 0.005491035524755716 + 10.0 * 6.209904670715332
Epoch 2850, val loss: 1.6921755075454712
Epoch 2860, training loss: 62.09373474121094 = 0.005449814721941948 + 10.0 * 6.208828449249268
Epoch 2860, val loss: 1.6945176124572754
Epoch 2870, training loss: 62.13001251220703 = 0.005410259589552879 + 10.0 * 6.212460517883301
Epoch 2870, val loss: 1.6966969966888428
Epoch 2880, training loss: 62.10274124145508 = 0.005368454847484827 + 10.0 * 6.209737300872803
Epoch 2880, val loss: 1.6991050243377686
Epoch 2890, training loss: 62.109859466552734 = 0.00532697094604373 + 10.0 * 6.210453510284424
Epoch 2890, val loss: 1.7011631727218628
Epoch 2900, training loss: 62.09918975830078 = 0.005286884494125843 + 10.0 * 6.209390163421631
Epoch 2900, val loss: 1.703566551208496
Epoch 2910, training loss: 62.109718322753906 = 0.005249437410384417 + 10.0 * 6.210446834564209
Epoch 2910, val loss: 1.7057181596755981
Epoch 2920, training loss: 62.12633514404297 = 0.0052106245420873165 + 10.0 * 6.2121124267578125
Epoch 2920, val loss: 1.7079771757125854
Epoch 2930, training loss: 62.098480224609375 = 0.005170914810150862 + 10.0 * 6.2093305587768555
Epoch 2930, val loss: 1.710060477256775
Epoch 2940, training loss: 62.087074279785156 = 0.005134373903274536 + 10.0 * 6.208193778991699
Epoch 2940, val loss: 1.7122249603271484
Epoch 2950, training loss: 62.085731506347656 = 0.005099023226648569 + 10.0 * 6.208063125610352
Epoch 2950, val loss: 1.7145500183105469
Epoch 2960, training loss: 62.1441650390625 = 0.005064148455858231 + 10.0 * 6.213910102844238
Epoch 2960, val loss: 1.7165379524230957
Epoch 2970, training loss: 62.099212646484375 = 0.005026359111070633 + 10.0 * 6.209418296813965
Epoch 2970, val loss: 1.7186096906661987
Epoch 2980, training loss: 62.08590316772461 = 0.004990495275706053 + 10.0 * 6.2080912590026855
Epoch 2980, val loss: 1.7208181619644165
Epoch 2990, training loss: 62.084022521972656 = 0.0049562822096049786 + 10.0 * 6.207906723022461
Epoch 2990, val loss: 1.7230292558670044
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 87.91008758544922 = 1.941369891166687 + 10.0 * 8.596872329711914
Epoch 0, val loss: 1.939016342163086
Epoch 10, training loss: 87.89521026611328 = 1.931761622428894 + 10.0 * 8.596344947814941
Epoch 10, val loss: 1.9292749166488647
Epoch 20, training loss: 87.83938598632812 = 1.9197280406951904 + 10.0 * 8.591965675354004
Epoch 20, val loss: 1.9170433282852173
Epoch 30, training loss: 87.48892211914062 = 1.9038710594177246 + 10.0 * 8.558505058288574
Epoch 30, val loss: 1.9010827541351318
Epoch 40, training loss: 85.43769073486328 = 1.8838483095169067 + 10.0 * 8.35538387298584
Epoch 40, val loss: 1.8815748691558838
Epoch 50, training loss: 80.20387268066406 = 1.8612455129623413 + 10.0 * 7.834262847900391
Epoch 50, val loss: 1.8600809574127197
Epoch 60, training loss: 75.33431243896484 = 1.84642493724823 + 10.0 * 7.348788261413574
Epoch 60, val loss: 1.846716046333313
Epoch 70, training loss: 72.18574523925781 = 1.836319088935852 + 10.0 * 7.034942626953125
Epoch 70, val loss: 1.8371254205703735
Epoch 80, training loss: 70.60071563720703 = 1.8263076543807983 + 10.0 * 6.877440452575684
Epoch 80, val loss: 1.827402949333191
Epoch 90, training loss: 69.69816589355469 = 1.814041256904602 + 10.0 * 6.788412570953369
Epoch 90, val loss: 1.8160381317138672
Epoch 100, training loss: 69.06982421875 = 1.8009841442108154 + 10.0 * 6.726883888244629
Epoch 100, val loss: 1.8042110204696655
Epoch 110, training loss: 68.52992248535156 = 1.7894600629806519 + 10.0 * 6.674046039581299
Epoch 110, val loss: 1.793850064277649
Epoch 120, training loss: 68.12568664550781 = 1.7790141105651855 + 10.0 * 6.63466739654541
Epoch 120, val loss: 1.7843235731124878
Epoch 130, training loss: 67.77529907226562 = 1.7680165767669678 + 10.0 * 6.600728511810303
Epoch 130, val loss: 1.774492621421814
Epoch 140, training loss: 67.55982208251953 = 1.7562967538833618 + 10.0 * 6.580352783203125
Epoch 140, val loss: 1.7641938924789429
Epoch 150, training loss: 67.25502014160156 = 1.743876338005066 + 10.0 * 6.551114559173584
Epoch 150, val loss: 1.7533727884292603
Epoch 160, training loss: 67.0218734741211 = 1.7308100461959839 + 10.0 * 6.529106140136719
Epoch 160, val loss: 1.742121696472168
Epoch 170, training loss: 66.81742095947266 = 1.7167061567306519 + 10.0 * 6.510071277618408
Epoch 170, val loss: 1.7301254272460938
Epoch 180, training loss: 66.68195343017578 = 1.7012441158294678 + 10.0 * 6.498071193695068
Epoch 180, val loss: 1.7171056270599365
Epoch 190, training loss: 66.49571990966797 = 1.6843756437301636 + 10.0 * 6.48113489151001
Epoch 190, val loss: 1.702872633934021
Epoch 200, training loss: 66.35692596435547 = 1.6660255193710327 + 10.0 * 6.469090461730957
Epoch 200, val loss: 1.6874310970306396
Epoch 210, training loss: 66.25408172607422 = 1.6461807489395142 + 10.0 * 6.460790157318115
Epoch 210, val loss: 1.670754313468933
Epoch 220, training loss: 66.10762023925781 = 1.6247673034667969 + 10.0 * 6.448285102844238
Epoch 220, val loss: 1.6528640985488892
Epoch 230, training loss: 65.99958801269531 = 1.6020455360412598 + 10.0 * 6.439754486083984
Epoch 230, val loss: 1.63396418094635
Epoch 240, training loss: 65.88200378417969 = 1.57797110080719 + 10.0 * 6.430402755737305
Epoch 240, val loss: 1.6139581203460693
Epoch 250, training loss: 65.77783966064453 = 1.5527116060256958 + 10.0 * 6.422513008117676
Epoch 250, val loss: 1.593184232711792
Epoch 260, training loss: 65.68303680419922 = 1.52629816532135 + 10.0 * 6.415674209594727
Epoch 260, val loss: 1.5716525316238403
Epoch 270, training loss: 65.57569885253906 = 1.4990419149398804 + 10.0 * 6.407665729522705
Epoch 270, val loss: 1.5494863986968994
Epoch 280, training loss: 65.47538757324219 = 1.4710721969604492 + 10.0 * 6.400432109832764
Epoch 280, val loss: 1.526990294456482
Epoch 290, training loss: 65.49052429199219 = 1.4424724578857422 + 10.0 * 6.404804706573486
Epoch 290, val loss: 1.504229187965393
Epoch 300, training loss: 65.31883239746094 = 1.413475751876831 + 10.0 * 6.390535354614258
Epoch 300, val loss: 1.4813636541366577
Epoch 310, training loss: 65.21509552001953 = 1.384353756904602 + 10.0 * 6.3830742835998535
Epoch 310, val loss: 1.4587228298187256
Epoch 320, training loss: 65.1313247680664 = 1.355080008506775 + 10.0 * 6.37762451171875
Epoch 320, val loss: 1.436203956604004
Epoch 330, training loss: 65.1028060913086 = 1.3257306814193726 + 10.0 * 6.377707481384277
Epoch 330, val loss: 1.4138745069503784
Epoch 340, training loss: 64.98336791992188 = 1.2965363264083862 + 10.0 * 6.368683338165283
Epoch 340, val loss: 1.3918085098266602
Epoch 350, training loss: 64.91017150878906 = 1.2675777673721313 + 10.0 * 6.364259243011475
Epoch 350, val loss: 1.3701831102371216
Epoch 360, training loss: 64.84807586669922 = 1.2389050722122192 + 10.0 * 6.360917091369629
Epoch 360, val loss: 1.349073052406311
Epoch 370, training loss: 64.77314758300781 = 1.210628628730774 + 10.0 * 6.3562517166137695
Epoch 370, val loss: 1.3284481763839722
Epoch 380, training loss: 64.704833984375 = 1.1828572750091553 + 10.0 * 6.352197647094727
Epoch 380, val loss: 1.308434009552002
Epoch 390, training loss: 64.64498901367188 = 1.155687689781189 + 10.0 * 6.3489298820495605
Epoch 390, val loss: 1.2889654636383057
Epoch 400, training loss: 64.61141967773438 = 1.1289446353912354 + 10.0 * 6.348247051239014
Epoch 400, val loss: 1.270088791847229
Epoch 410, training loss: 64.53202819824219 = 1.1030791997909546 + 10.0 * 6.342894554138184
Epoch 410, val loss: 1.2519595623016357
Epoch 420, training loss: 64.47501373291016 = 1.077949047088623 + 10.0 * 6.3397064208984375
Epoch 420, val loss: 1.2346556186676025
Epoch 430, training loss: 64.4134750366211 = 1.0535306930541992 + 10.0 * 6.335994243621826
Epoch 430, val loss: 1.2179919481277466
Epoch 440, training loss: 64.36629486083984 = 1.0299135446548462 + 10.0 * 6.3336381912231445
Epoch 440, val loss: 1.2022497653961182
Epoch 450, training loss: 64.30355834960938 = 1.007110595703125 + 10.0 * 6.329645156860352
Epoch 450, val loss: 1.1873277425765991
Epoch 460, training loss: 64.27446746826172 = 0.9850370287895203 + 10.0 * 6.328942775726318
Epoch 460, val loss: 1.1730561256408691
Epoch 470, training loss: 64.22895812988281 = 0.963465690612793 + 10.0 * 6.326549053192139
Epoch 470, val loss: 1.1594998836517334
Epoch 480, training loss: 64.17097473144531 = 0.9427046179771423 + 10.0 * 6.322827339172363
Epoch 480, val loss: 1.1466543674468994
Epoch 490, training loss: 64.1180191040039 = 0.922460675239563 + 10.0 * 6.319555759429932
Epoch 490, val loss: 1.1344820261001587
Epoch 500, training loss: 64.13556671142578 = 0.9027477502822876 + 10.0 * 6.323282241821289
Epoch 500, val loss: 1.1229157447814941
Epoch 510, training loss: 64.04425048828125 = 0.8835058808326721 + 10.0 * 6.316074371337891
Epoch 510, val loss: 1.1117051839828491
Epoch 520, training loss: 64.00411987304688 = 0.8646966218948364 + 10.0 * 6.3139424324035645
Epoch 520, val loss: 1.1011029481887817
Epoch 530, training loss: 63.96595001220703 = 0.8462914824485779 + 10.0 * 6.3119659423828125
Epoch 530, val loss: 1.091030240058899
Epoch 540, training loss: 63.93363571166992 = 0.82819664478302 + 10.0 * 6.310544013977051
Epoch 540, val loss: 1.0814329385757446
Epoch 550, training loss: 63.90518569946289 = 0.810395359992981 + 10.0 * 6.309479236602783
Epoch 550, val loss: 1.0719780921936035
Epoch 560, training loss: 63.854061126708984 = 0.7928886413574219 + 10.0 * 6.306117057800293
Epoch 560, val loss: 1.0630748271942139
Epoch 570, training loss: 63.811981201171875 = 0.7756404876708984 + 10.0 * 6.303634166717529
Epoch 570, val loss: 1.0544630289077759
Epoch 580, training loss: 63.78329086303711 = 0.7586079835891724 + 10.0 * 6.302468299865723
Epoch 580, val loss: 1.0462634563446045
Epoch 590, training loss: 63.79275894165039 = 0.7417434453964233 + 10.0 * 6.30510139465332
Epoch 590, val loss: 1.038200855255127
Epoch 600, training loss: 63.719757080078125 = 0.7249156832695007 + 10.0 * 6.2994842529296875
Epoch 600, val loss: 1.0304591655731201
Epoch 610, training loss: 63.681880950927734 = 0.7083855867385864 + 10.0 * 6.297349452972412
Epoch 610, val loss: 1.023118019104004
Epoch 620, training loss: 63.65582275390625 = 0.6920499801635742 + 10.0 * 6.296377182006836
Epoch 620, val loss: 1.0160398483276367
Epoch 630, training loss: 63.65796661376953 = 0.6757808923721313 + 10.0 * 6.298218727111816
Epoch 630, val loss: 1.0091423988342285
Epoch 640, training loss: 63.6192741394043 = 0.6596972346305847 + 10.0 * 6.295957565307617
Epoch 640, val loss: 1.0023994445800781
Epoch 650, training loss: 63.55800247192383 = 0.6436490416526794 + 10.0 * 6.291435241699219
Epoch 650, val loss: 0.995926558971405
Epoch 660, training loss: 63.54098892211914 = 0.6277774572372437 + 10.0 * 6.291321277618408
Epoch 660, val loss: 0.9898064732551575
Epoch 670, training loss: 63.5211067199707 = 0.6120003461837769 + 10.0 * 6.290910720825195
Epoch 670, val loss: 0.9838469624519348
Epoch 680, training loss: 63.478965759277344 = 0.5962441563606262 + 10.0 * 6.288271903991699
Epoch 680, val loss: 0.9781684875488281
Epoch 690, training loss: 63.50217056274414 = 0.5806151032447815 + 10.0 * 6.2921552658081055
Epoch 690, val loss: 0.9725862741470337
Epoch 700, training loss: 63.43416976928711 = 0.5650294423103333 + 10.0 * 6.286913871765137
Epoch 700, val loss: 0.9671409130096436
Epoch 710, training loss: 63.39049530029297 = 0.5495935082435608 + 10.0 * 6.284090042114258
Epoch 710, val loss: 0.9619666337966919
Epoch 720, training loss: 63.38053894042969 = 0.5343191027641296 + 10.0 * 6.284621715545654
Epoch 720, val loss: 0.9569289684295654
Epoch 730, training loss: 63.37065124511719 = 0.5190155506134033 + 10.0 * 6.285163402557373
Epoch 730, val loss: 0.9522565007209778
Epoch 740, training loss: 63.34891891479492 = 0.503836989402771 + 10.0 * 6.284508228302002
Epoch 740, val loss: 0.947334885597229
Epoch 750, training loss: 63.288299560546875 = 0.48892661929130554 + 10.0 * 6.279937267303467
Epoch 750, val loss: 0.9428101778030396
Epoch 760, training loss: 63.26408386230469 = 0.4741745591163635 + 10.0 * 6.278990745544434
Epoch 760, val loss: 0.9386188387870789
Epoch 770, training loss: 63.257869720458984 = 0.4596315920352936 + 10.0 * 6.2798237800598145
Epoch 770, val loss: 0.9344763159751892
Epoch 780, training loss: 63.23149490356445 = 0.44516873359680176 + 10.0 * 6.278632640838623
Epoch 780, val loss: 0.9305217266082764
Epoch 790, training loss: 63.21657943725586 = 0.4309116303920746 + 10.0 * 6.278566837310791
Epoch 790, val loss: 0.9267878532409668
Epoch 800, training loss: 63.169822692871094 = 0.41696634888648987 + 10.0 * 6.275285720825195
Epoch 800, val loss: 0.9233312010765076
Epoch 810, training loss: 63.14513397216797 = 0.40331313014030457 + 10.0 * 6.274182319641113
Epoch 810, val loss: 0.9201061725616455
Epoch 820, training loss: 63.137939453125 = 0.3899644911289215 + 10.0 * 6.274797439575195
Epoch 820, val loss: 0.9171638488769531
Epoch 830, training loss: 63.11788558959961 = 0.37687572836875916 + 10.0 * 6.2741007804870605
Epoch 830, val loss: 0.9143081903457642
Epoch 840, training loss: 63.12055206298828 = 0.36408740282058716 + 10.0 * 6.275646686553955
Epoch 840, val loss: 0.9116135239601135
Epoch 850, training loss: 63.05819320678711 = 0.35165175795555115 + 10.0 * 6.270654201507568
Epoch 850, val loss: 0.9091588854789734
Epoch 860, training loss: 63.03499984741211 = 0.3396146595478058 + 10.0 * 6.269538402557373
Epoch 860, val loss: 0.9071182608604431
Epoch 870, training loss: 63.01243209838867 = 0.32793912291526794 + 10.0 * 6.268449306488037
Epoch 870, val loss: 0.9053711295127869
Epoch 880, training loss: 63.03457260131836 = 0.31661704182624817 + 10.0 * 6.271795749664307
Epoch 880, val loss: 0.9039451479911804
Epoch 890, training loss: 62.97672653198242 = 0.3055911958217621 + 10.0 * 6.26711368560791
Epoch 890, val loss: 0.9024060964584351
Epoch 900, training loss: 62.96754837036133 = 0.2949197590351105 + 10.0 * 6.267262935638428
Epoch 900, val loss: 0.9013895392417908
Epoch 910, training loss: 62.963294982910156 = 0.28464674949645996 + 10.0 * 6.267865180969238
Epoch 910, val loss: 0.9005904197692871
Epoch 920, training loss: 62.91810989379883 = 0.2746603190898895 + 10.0 * 6.264344692230225
Epoch 920, val loss: 0.900151252746582
Epoch 930, training loss: 62.9132194519043 = 0.2650907635688782 + 10.0 * 6.26481294631958
Epoch 930, val loss: 0.8999952077865601
Epoch 940, training loss: 62.9084587097168 = 0.2558625638484955 + 10.0 * 6.265259742736816
Epoch 940, val loss: 0.8999583721160889
Epoch 950, training loss: 62.88679885864258 = 0.24698688089847565 + 10.0 * 6.263981342315674
Epoch 950, val loss: 0.9000512361526489
Epoch 960, training loss: 62.87466812133789 = 0.238410085439682 + 10.0 * 6.263625621795654
Epoch 960, val loss: 0.9005294442176819
Epoch 970, training loss: 62.84760284423828 = 0.23014330863952637 + 10.0 * 6.261745929718018
Epoch 970, val loss: 0.901347279548645
Epoch 980, training loss: 62.823307037353516 = 0.22225798666477203 + 10.0 * 6.260104656219482
Epoch 980, val loss: 0.9022217988967896
Epoch 990, training loss: 62.82185745239258 = 0.2146461009979248 + 10.0 * 6.260721206665039
Epoch 990, val loss: 0.9034726023674011
Epoch 1000, training loss: 62.85941696166992 = 0.20730368793010712 + 10.0 * 6.265211582183838
Epoch 1000, val loss: 0.9049062132835388
Epoch 1010, training loss: 62.80132293701172 = 0.2002144306898117 + 10.0 * 6.260110855102539
Epoch 1010, val loss: 0.9061990976333618
Epoch 1020, training loss: 62.77688980102539 = 0.19342954456806183 + 10.0 * 6.258346080780029
Epoch 1020, val loss: 0.9080453515052795
Epoch 1030, training loss: 62.75624084472656 = 0.18698133528232574 + 10.0 * 6.2569260597229
Epoch 1030, val loss: 0.9099444150924683
Epoch 1040, training loss: 62.83258819580078 = 0.18077047169208527 + 10.0 * 6.265181541442871
Epoch 1040, val loss: 0.9120814204216003
Epoch 1050, training loss: 62.769046783447266 = 0.17474961280822754 + 10.0 * 6.259429454803467
Epoch 1050, val loss: 0.9144390821456909
Epoch 1060, training loss: 62.72441864013672 = 0.16895976662635803 + 10.0 * 6.255545616149902
Epoch 1060, val loss: 0.9167572855949402
Epoch 1070, training loss: 62.70463943481445 = 0.16346074640750885 + 10.0 * 6.254117965698242
Epoch 1070, val loss: 0.9194590449333191
Epoch 1080, training loss: 62.71725845336914 = 0.15817227959632874 + 10.0 * 6.255908489227295
Epoch 1080, val loss: 0.922324001789093
Epoch 1090, training loss: 62.68791198730469 = 0.1530502736568451 + 10.0 * 6.253486156463623
Epoch 1090, val loss: 0.9251223802566528
Epoch 1100, training loss: 62.67190933227539 = 0.1481427103281021 + 10.0 * 6.252376556396484
Epoch 1100, val loss: 0.928164541721344
Epoch 1110, training loss: 62.691707611083984 = 0.14342811703681946 + 10.0 * 6.254827976226807
Epoch 1110, val loss: 0.9313512444496155
Epoch 1120, training loss: 62.66315460205078 = 0.13885505497455597 + 10.0 * 6.252429962158203
Epoch 1120, val loss: 0.9346181154251099
Epoch 1130, training loss: 62.64797592163086 = 0.13447946310043335 + 10.0 * 6.251349449157715
Epoch 1130, val loss: 0.9380306601524353
Epoch 1140, training loss: 62.63451385498047 = 0.1302802860736847 + 10.0 * 6.250423431396484
Epoch 1140, val loss: 0.9416124820709229
Epoch 1150, training loss: 62.66761016845703 = 0.12625397741794586 + 10.0 * 6.254135608673096
Epoch 1150, val loss: 0.9452947974205017
Epoch 1160, training loss: 62.62187576293945 = 0.12233378738164902 + 10.0 * 6.2499542236328125
Epoch 1160, val loss: 0.948857307434082
Epoch 1170, training loss: 62.6142463684082 = 0.11857633292675018 + 10.0 * 6.249567031860352
Epoch 1170, val loss: 0.9526202082633972
Epoch 1180, training loss: 62.662315368652344 = 0.11498930305242538 + 10.0 * 6.254732608795166
Epoch 1180, val loss: 0.956476628780365
Epoch 1190, training loss: 62.60929489135742 = 0.11143036186695099 + 10.0 * 6.249786376953125
Epoch 1190, val loss: 0.9605706334114075
Epoch 1200, training loss: 62.58615493774414 = 0.10809222608804703 + 10.0 * 6.247806072235107
Epoch 1200, val loss: 0.9645223021507263
Epoch 1210, training loss: 62.5958251953125 = 0.10486853122711182 + 10.0 * 6.249095439910889
Epoch 1210, val loss: 0.9687261581420898
Epoch 1220, training loss: 62.563682556152344 = 0.10175330936908722 + 10.0 * 6.246192932128906
Epoch 1220, val loss: 0.9728432893753052
Epoch 1230, training loss: 62.5596923828125 = 0.09876836836338043 + 10.0 * 6.246092319488525
Epoch 1230, val loss: 0.9770404100418091
Epoch 1240, training loss: 62.57498550415039 = 0.09590424597263336 + 10.0 * 6.247908115386963
Epoch 1240, val loss: 0.9813348650932312
Epoch 1250, training loss: 62.5571174621582 = 0.09311365336179733 + 10.0 * 6.246400356292725
Epoch 1250, val loss: 0.9854782819747925
Epoch 1260, training loss: 62.53764343261719 = 0.0904037207365036 + 10.0 * 6.244723796844482
Epoch 1260, val loss: 0.9898514151573181
Epoch 1270, training loss: 62.52528762817383 = 0.08783508092164993 + 10.0 * 6.24374532699585
Epoch 1270, val loss: 0.9942113757133484
Epoch 1280, training loss: 62.539730072021484 = 0.08535733073949814 + 10.0 * 6.245437145233154
Epoch 1280, val loss: 0.9986264705657959
Epoch 1290, training loss: 62.51566696166992 = 0.08294475078582764 + 10.0 * 6.243272304534912
Epoch 1290, val loss: 1.003137230873108
Epoch 1300, training loss: 62.52652359008789 = 0.08061432093381882 + 10.0 * 6.244590759277344
Epoch 1300, val loss: 1.0075247287750244
Epoch 1310, training loss: 62.52885818481445 = 0.07837565243244171 + 10.0 * 6.2450480461120605
Epoch 1310, val loss: 1.0120561122894287
Epoch 1320, training loss: 62.505714416503906 = 0.0762113556265831 + 10.0 * 6.242950439453125
Epoch 1320, val loss: 1.0165513753890991
Epoch 1330, training loss: 62.48897171020508 = 0.07413952052593231 + 10.0 * 6.241483211517334
Epoch 1330, val loss: 1.0210810899734497
Epoch 1340, training loss: 62.492496490478516 = 0.07214123010635376 + 10.0 * 6.242035865783691
Epoch 1340, val loss: 1.0256963968276978
Epoch 1350, training loss: 62.541568756103516 = 0.07020341604948044 + 10.0 * 6.24713659286499
Epoch 1350, val loss: 1.0302847623825073
Epoch 1360, training loss: 62.50185775756836 = 0.06833063066005707 + 10.0 * 6.243352890014648
Epoch 1360, val loss: 1.0346157550811768
Epoch 1370, training loss: 62.47601318359375 = 0.06652025133371353 + 10.0 * 6.2409491539001465
Epoch 1370, val loss: 1.0393425226211548
Epoch 1380, training loss: 62.45697784423828 = 0.06479619443416595 + 10.0 * 6.239218235015869
Epoch 1380, val loss: 1.0438591241836548
Epoch 1390, training loss: 62.452301025390625 = 0.06313783675432205 + 10.0 * 6.238916397094727
Epoch 1390, val loss: 1.0485968589782715
Epoch 1400, training loss: 62.53475570678711 = 0.061542414128780365 + 10.0 * 6.247321128845215
Epoch 1400, val loss: 1.05312180519104
Epoch 1410, training loss: 62.47404098510742 = 0.059944286942481995 + 10.0 * 6.241409778594971
Epoch 1410, val loss: 1.0575038194656372
Epoch 1420, training loss: 62.43665313720703 = 0.05843726918101311 + 10.0 * 6.237821578979492
Epoch 1420, val loss: 1.062178611755371
Epoch 1430, training loss: 62.4527473449707 = 0.05698297172784805 + 10.0 * 6.23957633972168
Epoch 1430, val loss: 1.0668787956237793
Epoch 1440, training loss: 62.451534271240234 = 0.05557881295681 + 10.0 * 6.239595413208008
Epoch 1440, val loss: 1.0713704824447632
Epoch 1450, training loss: 62.42475509643555 = 0.054220691323280334 + 10.0 * 6.237053394317627
Epoch 1450, val loss: 1.0759949684143066
Epoch 1460, training loss: 62.414833068847656 = 0.05291035771369934 + 10.0 * 6.236192226409912
Epoch 1460, val loss: 1.080601692199707
Epoch 1470, training loss: 62.40805435180664 = 0.05166395381093025 + 10.0 * 6.2356390953063965
Epoch 1470, val loss: 1.0852851867675781
Epoch 1480, training loss: 62.452091217041016 = 0.05044855549931526 + 10.0 * 6.240164279937744
Epoch 1480, val loss: 1.0900555849075317
Epoch 1490, training loss: 62.411468505859375 = 0.04925576224923134 + 10.0 * 6.2362213134765625
Epoch 1490, val loss: 1.0942107439041138
Epoch 1500, training loss: 62.40534973144531 = 0.048101458698511124 + 10.0 * 6.235724449157715
Epoch 1500, val loss: 1.0988513231277466
Epoch 1510, training loss: 62.39895248413086 = 0.047008760273456573 + 10.0 * 6.235194206237793
Epoch 1510, val loss: 1.103329062461853
Epoch 1520, training loss: 62.4478874206543 = 0.04594836384057999 + 10.0 * 6.240193843841553
Epoch 1520, val loss: 1.1079484224319458
Epoch 1530, training loss: 62.408409118652344 = 0.04489714279770851 + 10.0 * 6.236351013183594
Epoch 1530, val loss: 1.1122878789901733
Epoch 1540, training loss: 62.39895248413086 = 0.04389408603310585 + 10.0 * 6.2355055809021
Epoch 1540, val loss: 1.1167210340499878
Epoch 1550, training loss: 62.38919448852539 = 0.04292779788374901 + 10.0 * 6.234626770019531
Epoch 1550, val loss: 1.121193289756775
Epoch 1560, training loss: 62.39716720581055 = 0.041993964463472366 + 10.0 * 6.235517501831055
Epoch 1560, val loss: 1.1254960298538208
Epoch 1570, training loss: 62.37427520751953 = 0.04107191413640976 + 10.0 * 6.233320236206055
Epoch 1570, val loss: 1.1300134658813477
Epoch 1580, training loss: 62.37338638305664 = 0.04019876569509506 + 10.0 * 6.23331880569458
Epoch 1580, val loss: 1.1343568563461304
Epoch 1590, training loss: 62.39658737182617 = 0.03935239091515541 + 10.0 * 6.235723495483398
Epoch 1590, val loss: 1.13865065574646
Epoch 1600, training loss: 62.364341735839844 = 0.03851788491010666 + 10.0 * 6.2325825691223145
Epoch 1600, val loss: 1.142877459526062
Epoch 1610, training loss: 62.35150909423828 = 0.037711337208747864 + 10.0 * 6.231379508972168
Epoch 1610, val loss: 1.1471911668777466
Epoch 1620, training loss: 62.367034912109375 = 0.03694376349449158 + 10.0 * 6.233008861541748
Epoch 1620, val loss: 1.1515042781829834
Epoch 1630, training loss: 62.364784240722656 = 0.03618517145514488 + 10.0 * 6.2328596115112305
Epoch 1630, val loss: 1.155717134475708
Epoch 1640, training loss: 62.36650466918945 = 0.03545009344816208 + 10.0 * 6.233105659484863
Epoch 1640, val loss: 1.160074234008789
Epoch 1650, training loss: 62.36756134033203 = 0.03472897410392761 + 10.0 * 6.233283042907715
Epoch 1650, val loss: 1.164120078086853
Epoch 1660, training loss: 62.349063873291016 = 0.03404710069298744 + 10.0 * 6.231501579284668
Epoch 1660, val loss: 1.1682820320129395
Epoch 1670, training loss: 62.327693939208984 = 0.033374350517988205 + 10.0 * 6.229432106018066
Epoch 1670, val loss: 1.1725733280181885
Epoch 1680, training loss: 62.32861328125 = 0.032732099294662476 + 10.0 * 6.229588031768799
Epoch 1680, val loss: 1.1767395734786987
Epoch 1690, training loss: 62.40104675292969 = 0.0321132056415081 + 10.0 * 6.236893653869629
Epoch 1690, val loss: 1.180713415145874
Epoch 1700, training loss: 62.37092590332031 = 0.03149455040693283 + 10.0 * 6.233942985534668
Epoch 1700, val loss: 1.1846563816070557
Epoch 1710, training loss: 62.328773498535156 = 0.030879346653819084 + 10.0 * 6.2297892570495605
Epoch 1710, val loss: 1.188784122467041
Epoch 1720, training loss: 62.311038970947266 = 0.030300479382276535 + 10.0 * 6.228074073791504
Epoch 1720, val loss: 1.1927709579467773
Epoch 1730, training loss: 62.314693450927734 = 0.029744857922196388 + 10.0 * 6.228495121002197
Epoch 1730, val loss: 1.1968767642974854
Epoch 1740, training loss: 62.34844207763672 = 0.029201630502939224 + 10.0 * 6.231924057006836
Epoch 1740, val loss: 1.2007890939712524
Epoch 1750, training loss: 62.31999969482422 = 0.028664464130997658 + 10.0 * 6.229133605957031
Epoch 1750, val loss: 1.2047799825668335
Epoch 1760, training loss: 62.329322814941406 = 0.028147418051958084 + 10.0 * 6.230117321014404
Epoch 1760, val loss: 1.2085790634155273
Epoch 1770, training loss: 62.29490280151367 = 0.027632249519228935 + 10.0 * 6.22672700881958
Epoch 1770, val loss: 1.2125909328460693
Epoch 1780, training loss: 62.293880462646484 = 0.02714509889483452 + 10.0 * 6.226673603057861
Epoch 1780, val loss: 1.2165225744247437
Epoch 1790, training loss: 62.37498092651367 = 0.026682870462536812 + 10.0 * 6.234829902648926
Epoch 1790, val loss: 1.2202112674713135
Epoch 1800, training loss: 62.32765579223633 = 0.02619340643286705 + 10.0 * 6.230146408081055
Epoch 1800, val loss: 1.2240855693817139
Epoch 1810, training loss: 62.286598205566406 = 0.025736212730407715 + 10.0 * 6.226086139678955
Epoch 1810, val loss: 1.2277799844741821
Epoch 1820, training loss: 62.27522277832031 = 0.025297051295638084 + 10.0 * 6.224992752075195
Epoch 1820, val loss: 1.2315707206726074
Epoch 1830, training loss: 62.281288146972656 = 0.02487412467598915 + 10.0 * 6.225641250610352
Epoch 1830, val loss: 1.2353570461273193
Epoch 1840, training loss: 62.35384750366211 = 0.024461140856146812 + 10.0 * 6.232938766479492
Epoch 1840, val loss: 1.2390077114105225
Epoch 1850, training loss: 62.30940628051758 = 0.024037033319473267 + 10.0 * 6.228537082672119
Epoch 1850, val loss: 1.242795467376709
Epoch 1860, training loss: 62.27927780151367 = 0.02363789826631546 + 10.0 * 6.225564002990723
Epoch 1860, val loss: 1.2462269067764282
Epoch 1870, training loss: 62.32968521118164 = 0.023251885548233986 + 10.0 * 6.230643272399902
Epoch 1870, val loss: 1.2498877048492432
Epoch 1880, training loss: 62.278160095214844 = 0.02286577969789505 + 10.0 * 6.225529670715332
Epoch 1880, val loss: 1.2535852193832397
Epoch 1890, training loss: 62.26322937011719 = 0.022491250187158585 + 10.0 * 6.224073886871338
Epoch 1890, val loss: 1.2570947408676147
Epoch 1900, training loss: 62.25651168823242 = 0.022135471925139427 + 10.0 * 6.223437309265137
Epoch 1900, val loss: 1.2607879638671875
Epoch 1910, training loss: 62.26908874511719 = 0.021792707964777946 + 10.0 * 6.224729537963867
Epoch 1910, val loss: 1.2643308639526367
Epoch 1920, training loss: 62.297523498535156 = 0.021449774503707886 + 10.0 * 6.227607250213623
Epoch 1920, val loss: 1.2677284479141235
Epoch 1930, training loss: 62.27933883666992 = 0.02109954133629799 + 10.0 * 6.225823879241943
Epoch 1930, val loss: 1.2713481187820435
Epoch 1940, training loss: 62.26621627807617 = 0.020777340978384018 + 10.0 * 6.224543571472168
Epoch 1940, val loss: 1.274664282798767
Epoch 1950, training loss: 62.2558708190918 = 0.020453857257962227 + 10.0 * 6.223541736602783
Epoch 1950, val loss: 1.2782021760940552
Epoch 1960, training loss: 62.24171447753906 = 0.02014598250389099 + 10.0 * 6.222157001495361
Epoch 1960, val loss: 1.2816174030303955
Epoch 1970, training loss: 62.24601745605469 = 0.01984494924545288 + 10.0 * 6.222617149353027
Epoch 1970, val loss: 1.2851343154907227
Epoch 1980, training loss: 62.26160430908203 = 0.019551025703549385 + 10.0 * 6.224205493927002
Epoch 1980, val loss: 1.288486123085022
Epoch 1990, training loss: 62.27710723876953 = 0.019259756430983543 + 10.0 * 6.225784778594971
Epoch 1990, val loss: 1.291741967201233
Epoch 2000, training loss: 62.24435806274414 = 0.018964478746056557 + 10.0 * 6.22253942489624
Epoch 2000, val loss: 1.2949005365371704
Epoch 2010, training loss: 62.24137878417969 = 0.018682967871427536 + 10.0 * 6.222269535064697
Epoch 2010, val loss: 1.2982828617095947
Epoch 2020, training loss: 62.28296661376953 = 0.018417963758111 + 10.0 * 6.226454734802246
Epoch 2020, val loss: 1.301552414894104
Epoch 2030, training loss: 62.24535369873047 = 0.018144559115171432 + 10.0 * 6.222721099853516
Epoch 2030, val loss: 1.3047300577163696
Epoch 2040, training loss: 62.22081756591797 = 0.017882803454995155 + 10.0 * 6.2202935218811035
Epoch 2040, val loss: 1.307999849319458
Epoch 2050, training loss: 62.216915130615234 = 0.01763359270989895 + 10.0 * 6.219927787780762
Epoch 2050, val loss: 1.3112913370132446
Epoch 2060, training loss: 62.21669387817383 = 0.017388837411999702 + 10.0 * 6.219930648803711
Epoch 2060, val loss: 1.3145960569381714
Epoch 2070, training loss: 62.249053955078125 = 0.017153726890683174 + 10.0 * 6.223189830780029
Epoch 2070, val loss: 1.3177467584609985
Epoch 2080, training loss: 62.236724853515625 = 0.01691276766359806 + 10.0 * 6.221981048583984
Epoch 2080, val loss: 1.3206629753112793
Epoch 2090, training loss: 62.22648620605469 = 0.016669167205691338 + 10.0 * 6.220981597900391
Epoch 2090, val loss: 1.3238425254821777
Epoch 2100, training loss: 62.21934509277344 = 0.0164368636906147 + 10.0 * 6.220290660858154
Epoch 2100, val loss: 1.3269816637039185
Epoch 2110, training loss: 62.210968017578125 = 0.01621749810874462 + 10.0 * 6.219475269317627
Epoch 2110, val loss: 1.3300925493240356
Epoch 2120, training loss: 62.21561813354492 = 0.016004100441932678 + 10.0 * 6.219961643218994
Epoch 2120, val loss: 1.3332715034484863
Epoch 2130, training loss: 62.269744873046875 = 0.01579330861568451 + 10.0 * 6.225395202636719
Epoch 2130, val loss: 1.336144208908081
Epoch 2140, training loss: 62.22785186767578 = 0.015579604543745518 + 10.0 * 6.221227169036865
Epoch 2140, val loss: 1.3392589092254639
Epoch 2150, training loss: 62.21095275878906 = 0.015373514033854008 + 10.0 * 6.219557762145996
Epoch 2150, val loss: 1.3423117399215698
Epoch 2160, training loss: 62.242557525634766 = 0.015179106965661049 + 10.0 * 6.222737789154053
Epoch 2160, val loss: 1.3452872037887573
Epoch 2170, training loss: 62.19624328613281 = 0.014976304024457932 + 10.0 * 6.2181267738342285
Epoch 2170, val loss: 1.3482202291488647
Epoch 2180, training loss: 62.20224380493164 = 0.014784920029342175 + 10.0 * 6.218745708465576
Epoch 2180, val loss: 1.351281762123108
Epoch 2190, training loss: 62.19512176513672 = 0.014598095789551735 + 10.0 * 6.218052387237549
Epoch 2190, val loss: 1.354235053062439
Epoch 2200, training loss: 62.260189056396484 = 0.014416554942727089 + 10.0 * 6.2245774269104
Epoch 2200, val loss: 1.3570626974105835
Epoch 2210, training loss: 62.22134780883789 = 0.014232896268367767 + 10.0 * 6.220711708068848
Epoch 2210, val loss: 1.3600518703460693
Epoch 2220, training loss: 62.21101760864258 = 0.014050754718482494 + 10.0 * 6.219696998596191
Epoch 2220, val loss: 1.3628513813018799
Epoch 2230, training loss: 62.193180084228516 = 0.013879754580557346 + 10.0 * 6.217930316925049
Epoch 2230, val loss: 1.3657653331756592
Epoch 2240, training loss: 62.20634078979492 = 0.013714374974370003 + 10.0 * 6.219262599945068
Epoch 2240, val loss: 1.3685020208358765
Epoch 2250, training loss: 62.20941925048828 = 0.013543505221605301 + 10.0 * 6.219587802886963
Epoch 2250, val loss: 1.3713691234588623
Epoch 2260, training loss: 62.187442779541016 = 0.013376102782785892 + 10.0 * 6.217406749725342
Epoch 2260, val loss: 1.37441086769104
Epoch 2270, training loss: 62.19559860229492 = 0.013218577951192856 + 10.0 * 6.21823787689209
Epoch 2270, val loss: 1.3771553039550781
Epoch 2280, training loss: 62.199462890625 = 0.01306070201098919 + 10.0 * 6.218640327453613
Epoch 2280, val loss: 1.3797755241394043
Epoch 2290, training loss: 62.1829719543457 = 0.012901801615953445 + 10.0 * 6.217007160186768
Epoch 2290, val loss: 1.3825485706329346
Epoch 2300, training loss: 62.173561096191406 = 0.012750322930514812 + 10.0 * 6.216081142425537
Epoch 2300, val loss: 1.385399341583252
Epoch 2310, training loss: 62.16609191894531 = 0.01260532345622778 + 10.0 * 6.215348720550537
Epoch 2310, val loss: 1.3882200717926025
Epoch 2320, training loss: 62.23485565185547 = 0.012468094937503338 + 10.0 * 6.222239017486572
Epoch 2320, val loss: 1.390945553779602
Epoch 2330, training loss: 62.19476318359375 = 0.012313855811953545 + 10.0 * 6.218245029449463
Epoch 2330, val loss: 1.3934794664382935
Epoch 2340, training loss: 62.20166015625 = 0.012171588838100433 + 10.0 * 6.218948841094971
Epoch 2340, val loss: 1.3961067199707031
Epoch 2350, training loss: 62.16380310058594 = 0.012031634338200092 + 10.0 * 6.215177059173584
Epoch 2350, val loss: 1.3987842798233032
Epoch 2360, training loss: 62.15714645385742 = 0.011898627504706383 + 10.0 * 6.214524745941162
Epoch 2360, val loss: 1.4015477895736694
Epoch 2370, training loss: 62.17519760131836 = 0.011769842356443405 + 10.0 * 6.216342926025391
Epoch 2370, val loss: 1.4041163921356201
Epoch 2380, training loss: 62.17513656616211 = 0.011639201082289219 + 10.0 * 6.2163496017456055
Epoch 2380, val loss: 1.406702995300293
Epoch 2390, training loss: 62.185420989990234 = 0.011509587988257408 + 10.0 * 6.217391014099121
Epoch 2390, val loss: 1.4095295667648315
Epoch 2400, training loss: 62.18330764770508 = 0.011385402642190456 + 10.0 * 6.21719217300415
Epoch 2400, val loss: 1.4119230508804321
Epoch 2410, training loss: 62.17279815673828 = 0.011260878294706345 + 10.0 * 6.216153621673584
Epoch 2410, val loss: 1.414594054222107
Epoch 2420, training loss: 62.17152404785156 = 0.011138495057821274 + 10.0 * 6.216038703918457
Epoch 2420, val loss: 1.4169774055480957
Epoch 2430, training loss: 62.189212799072266 = 0.011018319055438042 + 10.0 * 6.217819690704346
Epoch 2430, val loss: 1.4194281101226807
Epoch 2440, training loss: 62.15424346923828 = 0.010893779806792736 + 10.0 * 6.214334964752197
Epoch 2440, val loss: 1.4219123125076294
Epoch 2450, training loss: 62.14417266845703 = 0.010780694894492626 + 10.0 * 6.213339328765869
Epoch 2450, val loss: 1.4244791269302368
Epoch 2460, training loss: 62.143367767333984 = 0.01067137997597456 + 10.0 * 6.2132697105407715
Epoch 2460, val loss: 1.427032470703125
Epoch 2470, training loss: 62.20853805541992 = 0.01056591421365738 + 10.0 * 6.219797134399414
Epoch 2470, val loss: 1.429587483406067
Epoch 2480, training loss: 62.15333938598633 = 0.010449213907122612 + 10.0 * 6.21428918838501
Epoch 2480, val loss: 1.431848168373108
Epoch 2490, training loss: 62.14279556274414 = 0.01034208107739687 + 10.0 * 6.213245391845703
Epoch 2490, val loss: 1.4343361854553223
Epoch 2500, training loss: 62.136558532714844 = 0.010237123817205429 + 10.0 * 6.212632179260254
Epoch 2500, val loss: 1.436747431755066
Epoch 2510, training loss: 62.148067474365234 = 0.010137376375496387 + 10.0 * 6.2137932777404785
Epoch 2510, val loss: 1.4392589330673218
Epoch 2520, training loss: 62.17293930053711 = 0.010035068728029728 + 10.0 * 6.216290473937988
Epoch 2520, val loss: 1.4414628744125366
Epoch 2530, training loss: 62.1656379699707 = 0.009934049099683762 + 10.0 * 6.215570449829102
Epoch 2530, val loss: 1.4439315795898438
Epoch 2540, training loss: 62.14518737792969 = 0.009833858348429203 + 10.0 * 6.213535308837891
Epoch 2540, val loss: 1.4460840225219727
Epoch 2550, training loss: 62.133705139160156 = 0.009735221974551678 + 10.0 * 6.21239709854126
Epoch 2550, val loss: 1.4485235214233398
Epoch 2560, training loss: 62.1445198059082 = 0.009642844088375568 + 10.0 * 6.21348762512207
Epoch 2560, val loss: 1.450812578201294
Epoch 2570, training loss: 62.1552734375 = 0.009549670852720737 + 10.0 * 6.214572429656982
Epoch 2570, val loss: 1.4530497789382935
Epoch 2580, training loss: 62.13108444213867 = 0.009450397454202175 + 10.0 * 6.21216344833374
Epoch 2580, val loss: 1.4553582668304443
Epoch 2590, training loss: 62.124717712402344 = 0.009362369775772095 + 10.0 * 6.211535453796387
Epoch 2590, val loss: 1.4576590061187744
Epoch 2600, training loss: 62.16133117675781 = 0.009275095537304878 + 10.0 * 6.215205669403076
Epoch 2600, val loss: 1.4600815773010254
Epoch 2610, training loss: 62.1292724609375 = 0.009185932576656342 + 10.0 * 6.212008476257324
Epoch 2610, val loss: 1.4621050357818604
Epoch 2620, training loss: 62.12900924682617 = 0.0090986592695117 + 10.0 * 6.211991310119629
Epoch 2620, val loss: 1.464303970336914
Epoch 2630, training loss: 62.12055587768555 = 0.009015653282403946 + 10.0 * 6.211153984069824
Epoch 2630, val loss: 1.4666616916656494
Epoch 2640, training loss: 62.17903137207031 = 0.008935729041695595 + 10.0 * 6.217009544372559
Epoch 2640, val loss: 1.4689501523971558
Epoch 2650, training loss: 62.125465393066406 = 0.008849038742482662 + 10.0 * 6.211661338806152
Epoch 2650, val loss: 1.470885992050171
Epoch 2660, training loss: 62.11818313598633 = 0.008764654397964478 + 10.0 * 6.210941791534424
Epoch 2660, val loss: 1.473096489906311
Epoch 2670, training loss: 62.112449645996094 = 0.008687778376042843 + 10.0 * 6.210376262664795
Epoch 2670, val loss: 1.4753341674804688
Epoch 2680, training loss: 62.14043426513672 = 0.008611728437244892 + 10.0 * 6.213181972503662
Epoch 2680, val loss: 1.47763991355896
Epoch 2690, training loss: 62.13246154785156 = 0.008532972075045109 + 10.0 * 6.212392807006836
Epoch 2690, val loss: 1.479557752609253
Epoch 2700, training loss: 62.13056945800781 = 0.008455872535705566 + 10.0 * 6.2122111320495605
Epoch 2700, val loss: 1.4815802574157715
Epoch 2710, training loss: 62.110931396484375 = 0.008379384875297546 + 10.0 * 6.210255146026611
Epoch 2710, val loss: 1.483565330505371
Epoch 2720, training loss: 62.10734558105469 = 0.008307106792926788 + 10.0 * 6.209903717041016
Epoch 2720, val loss: 1.4857017993927002
Epoch 2730, training loss: 62.10870361328125 = 0.008237073197960854 + 10.0 * 6.210046768188477
Epoch 2730, val loss: 1.4878532886505127
Epoch 2740, training loss: 62.126731872558594 = 0.008167830295860767 + 10.0 * 6.211856365203857
Epoch 2740, val loss: 1.4898508787155151
Epoch 2750, training loss: 62.137203216552734 = 0.008096860721707344 + 10.0 * 6.2129106521606445
Epoch 2750, val loss: 1.4920039176940918
Epoch 2760, training loss: 62.119937896728516 = 0.008024471811950207 + 10.0 * 6.211191177368164
Epoch 2760, val loss: 1.4940271377563477
Epoch 2770, training loss: 62.106807708740234 = 0.007955377921462059 + 10.0 * 6.209885597229004
Epoch 2770, val loss: 1.4959619045257568
Epoch 2780, training loss: 62.098060607910156 = 0.007887078449130058 + 10.0 * 6.209017276763916
Epoch 2780, val loss: 1.497911810874939
Epoch 2790, training loss: 62.10459518432617 = 0.00782281905412674 + 10.0 * 6.209677219390869
Epoch 2790, val loss: 1.5000617504119873
Epoch 2800, training loss: 62.185272216796875 = 0.007757688406854868 + 10.0 * 6.217751502990723
Epoch 2800, val loss: 1.5021343231201172
Epoch 2810, training loss: 62.11746597290039 = 0.007691110018640757 + 10.0 * 6.210977554321289
Epoch 2810, val loss: 1.5035874843597412
Epoch 2820, training loss: 62.0942497253418 = 0.007626065518707037 + 10.0 * 6.208662509918213
Epoch 2820, val loss: 1.5056462287902832
Epoch 2830, training loss: 62.089210510253906 = 0.007565699517726898 + 10.0 * 6.208164691925049
Epoch 2830, val loss: 1.507685899734497
Epoch 2840, training loss: 62.12989044189453 = 0.007507088128477335 + 10.0 * 6.212238311767578
Epoch 2840, val loss: 1.509688377380371
Epoch 2850, training loss: 62.0932502746582 = 0.0074446480721235275 + 10.0 * 6.208580493927002
Epoch 2850, val loss: 1.51149582862854
Epoch 2860, training loss: 62.1025505065918 = 0.007384810131043196 + 10.0 * 6.209516525268555
Epoch 2860, val loss: 1.5133687257766724
Epoch 2870, training loss: 62.11506271362305 = 0.007326813414692879 + 10.0 * 6.210773468017578
Epoch 2870, val loss: 1.5152769088745117
Epoch 2880, training loss: 62.08877944946289 = 0.007267977576702833 + 10.0 * 6.208151340484619
Epoch 2880, val loss: 1.5170575380325317
Epoch 2890, training loss: 62.083927154541016 = 0.0072116004303097725 + 10.0 * 6.207671642303467
Epoch 2890, val loss: 1.5190109014511108
Epoch 2900, training loss: 62.115421295166016 = 0.007158088032156229 + 10.0 * 6.210826396942139
Epoch 2900, val loss: 1.5210013389587402
Epoch 2910, training loss: 62.12002182006836 = 0.007099717389792204 + 10.0 * 6.211292266845703
Epoch 2910, val loss: 1.5227999687194824
Epoch 2920, training loss: 62.084712982177734 = 0.0070436992682516575 + 10.0 * 6.207767009735107
Epoch 2920, val loss: 1.5243570804595947
Epoch 2930, training loss: 62.081478118896484 = 0.006990231107920408 + 10.0 * 6.207448959350586
Epoch 2930, val loss: 1.526225209236145
Epoch 2940, training loss: 62.09579849243164 = 0.00693912710994482 + 10.0 * 6.20888614654541
Epoch 2940, val loss: 1.528104543685913
Epoch 2950, training loss: 62.11397933959961 = 0.006886961404234171 + 10.0 * 6.210709571838379
Epoch 2950, val loss: 1.529780626296997
Epoch 2960, training loss: 62.09526062011719 = 0.006832025945186615 + 10.0 * 6.208842754364014
Epoch 2960, val loss: 1.531459927558899
Epoch 2970, training loss: 62.078102111816406 = 0.006781787611544132 + 10.0 * 6.207131862640381
Epoch 2970, val loss: 1.533260703086853
Epoch 2980, training loss: 62.07869338989258 = 0.0067331683821976185 + 10.0 * 6.207196235656738
Epoch 2980, val loss: 1.534976601600647
Epoch 2990, training loss: 62.087886810302734 = 0.0066844020038843155 + 10.0 * 6.208120346069336
Epoch 2990, val loss: 1.5367296934127808
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8413284132841329
The final CL Acc:0.76914, 0.01259, The final GNN Acc:0.83975, 0.00188
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11510])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10494])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.90313720703125 = 1.9349380731582642 + 10.0 * 8.596819877624512
Epoch 0, val loss: 1.9425098896026611
Epoch 10, training loss: 87.88787078857422 = 1.9265021085739136 + 10.0 * 8.596137046813965
Epoch 10, val loss: 1.9337431192398071
Epoch 20, training loss: 87.83048248291016 = 1.9165617227554321 + 10.0 * 8.591391563415527
Epoch 20, val loss: 1.9233436584472656
Epoch 30, training loss: 87.4972152709961 = 1.9037727117538452 + 10.0 * 8.559344291687012
Epoch 30, val loss: 1.9100604057312012
Epoch 40, training loss: 85.35836791992188 = 1.8877724409103394 + 10.0 * 8.34705924987793
Epoch 40, val loss: 1.8938038349151611
Epoch 50, training loss: 77.87548065185547 = 1.8683816194534302 + 10.0 * 7.600709915161133
Epoch 50, val loss: 1.874184250831604
Epoch 60, training loss: 75.2657241821289 = 1.853761911392212 + 10.0 * 7.341196537017822
Epoch 60, val loss: 1.8602219820022583
Epoch 70, training loss: 72.97797393798828 = 1.842005968093872 + 10.0 * 7.1135969161987305
Epoch 70, val loss: 1.8480664491653442
Epoch 80, training loss: 71.47994232177734 = 1.8306101560592651 + 10.0 * 6.964932918548584
Epoch 80, val loss: 1.8361608982086182
Epoch 90, training loss: 70.5482177734375 = 1.820464015007019 + 10.0 * 6.872775554656982
Epoch 90, val loss: 1.8255349397659302
Epoch 100, training loss: 69.96976470947266 = 1.810606837272644 + 10.0 * 6.815915584564209
Epoch 100, val loss: 1.8153637647628784
Epoch 110, training loss: 69.3923110961914 = 1.8016256093978882 + 10.0 * 6.759068012237549
Epoch 110, val loss: 1.8059167861938477
Epoch 120, training loss: 68.82770538330078 = 1.7932417392730713 + 10.0 * 6.703446388244629
Epoch 120, val loss: 1.7970539331436157
Epoch 130, training loss: 68.3622817993164 = 1.7848365306854248 + 10.0 * 6.657744884490967
Epoch 130, val loss: 1.7883567810058594
Epoch 140, training loss: 67.9951171875 = 1.7758357524871826 + 10.0 * 6.621928691864014
Epoch 140, val loss: 1.779194712638855
Epoch 150, training loss: 67.66555786132812 = 1.7660707235336304 + 10.0 * 6.589949131011963
Epoch 150, val loss: 1.7694008350372314
Epoch 160, training loss: 67.4166488647461 = 1.7554070949554443 + 10.0 * 6.566123962402344
Epoch 160, val loss: 1.7589260339736938
Epoch 170, training loss: 67.16738891601562 = 1.7437127828598022 + 10.0 * 6.542367935180664
Epoch 170, val loss: 1.747740387916565
Epoch 180, training loss: 66.96955871582031 = 1.7309023141860962 + 10.0 * 6.523865222930908
Epoch 180, val loss: 1.7356518507003784
Epoch 190, training loss: 66.80046844482422 = 1.7167493104934692 + 10.0 * 6.508371829986572
Epoch 190, val loss: 1.7225744724273682
Epoch 200, training loss: 66.62584686279297 = 1.7012759447097778 + 10.0 * 6.492456912994385
Epoch 200, val loss: 1.7082877159118652
Epoch 210, training loss: 66.47549438476562 = 1.684327483177185 + 10.0 * 6.479116916656494
Epoch 210, val loss: 1.6927722692489624
Epoch 220, training loss: 66.33786010742188 = 1.6656415462493896 + 10.0 * 6.467221736907959
Epoch 220, val loss: 1.67593252658844
Epoch 230, training loss: 66.19991302490234 = 1.6451849937438965 + 10.0 * 6.455472469329834
Epoch 230, val loss: 1.6575819253921509
Epoch 240, training loss: 66.07077026367188 = 1.622894287109375 + 10.0 * 6.444787502288818
Epoch 240, val loss: 1.6377148628234863
Epoch 250, training loss: 65.9710464477539 = 1.5986357927322388 + 10.0 * 6.437241077423096
Epoch 250, val loss: 1.61629319190979
Epoch 260, training loss: 65.85861206054688 = 1.5724107027053833 + 10.0 * 6.428619861602783
Epoch 260, val loss: 1.5933045148849487
Epoch 270, training loss: 65.75375366210938 = 1.5443578958511353 + 10.0 * 6.420939922332764
Epoch 270, val loss: 1.5689793825149536
Epoch 280, training loss: 65.65641784667969 = 1.5146623849868774 + 10.0 * 6.414175033569336
Epoch 280, val loss: 1.5434309244155884
Epoch 290, training loss: 65.56716918945312 = 1.4833941459655762 + 10.0 * 6.4083781242370605
Epoch 290, val loss: 1.5166823863983154
Epoch 300, training loss: 65.47224426269531 = 1.4509425163269043 + 10.0 * 6.402130126953125
Epoch 300, val loss: 1.4892455339431763
Epoch 310, training loss: 65.38736724853516 = 1.4174330234527588 + 10.0 * 6.396993637084961
Epoch 310, val loss: 1.4611843824386597
Epoch 320, training loss: 65.33061218261719 = 1.3831435441970825 + 10.0 * 6.394747257232666
Epoch 320, val loss: 1.4326344728469849
Epoch 330, training loss: 65.22003173828125 = 1.348459243774414 + 10.0 * 6.387156963348389
Epoch 330, val loss: 1.4040800333023071
Epoch 340, training loss: 65.1303939819336 = 1.3136473894119263 + 10.0 * 6.381674766540527
Epoch 340, val loss: 1.375748634338379
Epoch 350, training loss: 65.09447479248047 = 1.278854489326477 + 10.0 * 6.381561756134033
Epoch 350, val loss: 1.3477956056594849
Epoch 360, training loss: 64.97600555419922 = 1.2441861629486084 + 10.0 * 6.37318229675293
Epoch 360, val loss: 1.3202766180038452
Epoch 370, training loss: 64.90479278564453 = 1.2099653482437134 + 10.0 * 6.369482517242432
Epoch 370, val loss: 1.293483853340149
Epoch 380, training loss: 64.85325622558594 = 1.1763310432434082 + 10.0 * 6.367692470550537
Epoch 380, val loss: 1.2675261497497559
Epoch 390, training loss: 64.76535034179688 = 1.1432883739471436 + 10.0 * 6.36220645904541
Epoch 390, val loss: 1.242469072341919
Epoch 400, training loss: 64.70491790771484 = 1.1111088991165161 + 10.0 * 6.359380722045898
Epoch 400, val loss: 1.2184821367263794
Epoch 410, training loss: 64.63690185546875 = 1.079591989517212 + 10.0 * 6.355731010437012
Epoch 410, val loss: 1.1952755451202393
Epoch 420, training loss: 64.56710815429688 = 1.0491843223571777 + 10.0 * 6.351792335510254
Epoch 420, val loss: 1.1736172437667847
Epoch 430, training loss: 64.50991821289062 = 1.0197560787200928 + 10.0 * 6.349016189575195
Epoch 430, val loss: 1.1531379222869873
Epoch 440, training loss: 64.47771453857422 = 0.9912993907928467 + 10.0 * 6.348641395568848
Epoch 440, val loss: 1.133664608001709
Epoch 450, training loss: 64.44894409179688 = 0.9637572765350342 + 10.0 * 6.3485188484191895
Epoch 450, val loss: 1.1153333187103271
Epoch 460, training loss: 64.35629272460938 = 0.9373337626457214 + 10.0 * 6.341896057128906
Epoch 460, val loss: 1.0982648134231567
Epoch 470, training loss: 64.29752349853516 = 0.9119179844856262 + 10.0 * 6.338560581207275
Epoch 470, val loss: 1.0823113918304443
Epoch 480, training loss: 64.25316619873047 = 0.8874431848526001 + 10.0 * 6.336572170257568
Epoch 480, val loss: 1.0673695802688599
Epoch 490, training loss: 64.20480346679688 = 0.8637626767158508 + 10.0 * 6.334104061126709
Epoch 490, val loss: 1.0533945560455322
Epoch 500, training loss: 64.15623474121094 = 0.840846061706543 + 10.0 * 6.331538677215576
Epoch 500, val loss: 1.0401777029037476
Epoch 510, training loss: 64.12081146240234 = 0.8187367916107178 + 10.0 * 6.330207824707031
Epoch 510, val loss: 1.0279104709625244
Epoch 520, training loss: 64.08155822753906 = 0.7972237467765808 + 10.0 * 6.328433513641357
Epoch 520, val loss: 1.016210913658142
Epoch 530, training loss: 64.05036926269531 = 0.7763131856918335 + 10.0 * 6.32740592956543
Epoch 530, val loss: 1.00523841381073
Epoch 540, training loss: 63.99080276489258 = 0.755972683429718 + 10.0 * 6.323482990264893
Epoch 540, val loss: 0.9949443936347961
Epoch 550, training loss: 63.94507598876953 = 0.7361679077148438 + 10.0 * 6.3208909034729
Epoch 550, val loss: 0.9853036403656006
Epoch 560, training loss: 63.93289566040039 = 0.7167472839355469 + 10.0 * 6.321614742279053
Epoch 560, val loss: 0.9759835600852966
Epoch 570, training loss: 63.88261795043945 = 0.6976432800292969 + 10.0 * 6.318497657775879
Epoch 570, val loss: 0.9671715497970581
Epoch 580, training loss: 63.85063934326172 = 0.6788497567176819 + 10.0 * 6.317179203033447
Epoch 580, val loss: 0.9586244821548462
Epoch 590, training loss: 63.80860137939453 = 0.6604912877082825 + 10.0 * 6.314810752868652
Epoch 590, val loss: 0.9505805969238281
Epoch 600, training loss: 63.83708190917969 = 0.6425080895423889 + 10.0 * 6.319457054138184
Epoch 600, val loss: 0.9431175589561462
Epoch 610, training loss: 63.72927474975586 = 0.6247031092643738 + 10.0 * 6.310457229614258
Epoch 610, val loss: 0.9358087182044983
Epoch 620, training loss: 63.69932174682617 = 0.6073009371757507 + 10.0 * 6.309202194213867
Epoch 620, val loss: 0.9290140271186829
Epoch 630, training loss: 63.66019821166992 = 0.5902662873268127 + 10.0 * 6.306993007659912
Epoch 630, val loss: 0.9228146076202393
Epoch 640, training loss: 63.654998779296875 = 0.5735339522361755 + 10.0 * 6.3081464767456055
Epoch 640, val loss: 0.9168380498886108
Epoch 650, training loss: 63.647911071777344 = 0.5570127367973328 + 10.0 * 6.309089660644531
Epoch 650, val loss: 0.9111244678497314
Epoch 660, training loss: 63.572540283203125 = 0.5409182906150818 + 10.0 * 6.303162574768066
Epoch 660, val loss: 0.9060385227203369
Epoch 670, training loss: 63.53413772583008 = 0.5252275466918945 + 10.0 * 6.300890922546387
Epoch 670, val loss: 0.9013684391975403
Epoch 680, training loss: 63.558712005615234 = 0.509907603263855 + 10.0 * 6.304880619049072
Epoch 680, val loss: 0.8970934748649597
Epoch 690, training loss: 63.50017166137695 = 0.4949295222759247 + 10.0 * 6.3005242347717285
Epoch 690, val loss: 0.8932425379753113
Epoch 700, training loss: 63.46540832519531 = 0.4802917242050171 + 10.0 * 6.298511505126953
Epoch 700, val loss: 0.8897589445114136
Epoch 710, training loss: 63.46664047241211 = 0.46605807542800903 + 10.0 * 6.300058364868164
Epoch 710, val loss: 0.8868905901908875
Epoch 720, training loss: 63.40615463256836 = 0.45216280221939087 + 10.0 * 6.295399188995361
Epoch 720, val loss: 0.884222686290741
Epoch 730, training loss: 63.36625289916992 = 0.4386802017688751 + 10.0 * 6.292757511138916
Epoch 730, val loss: 0.8821519017219543
Epoch 740, training loss: 63.34552764892578 = 0.425534188747406 + 10.0 * 6.291999340057373
Epoch 740, val loss: 0.8804545402526855
Epoch 750, training loss: 63.345245361328125 = 0.41271689534187317 + 10.0 * 6.293252944946289
Epoch 750, val loss: 0.8791040778160095
Epoch 760, training loss: 63.29603958129883 = 0.4001174569129944 + 10.0 * 6.289592266082764
Epoch 760, val loss: 0.8779299855232239
Epoch 770, training loss: 63.27817916870117 = 0.3878938555717468 + 10.0 * 6.289028644561768
Epoch 770, val loss: 0.8772150874137878
Epoch 780, training loss: 63.29877471923828 = 0.3760747015476227 + 10.0 * 6.292269706726074
Epoch 780, val loss: 0.8769258260726929
Epoch 790, training loss: 63.235198974609375 = 0.3644453287124634 + 10.0 * 6.287075519561768
Epoch 790, val loss: 0.8767107129096985
Epoch 800, training loss: 63.20722198486328 = 0.35319697856903076 + 10.0 * 6.285402774810791
Epoch 800, val loss: 0.8769865036010742
Epoch 810, training loss: 63.197731018066406 = 0.3422580063343048 + 10.0 * 6.285547256469727
Epoch 810, val loss: 0.8774629235267639
Epoch 820, training loss: 63.15931701660156 = 0.3316042721271515 + 10.0 * 6.282771110534668
Epoch 820, val loss: 0.8783016204833984
Epoch 830, training loss: 63.156978607177734 = 0.3212464153766632 + 10.0 * 6.283573150634766
Epoch 830, val loss: 0.8794751167297363
Epoch 840, training loss: 63.121620178222656 = 0.31115901470184326 + 10.0 * 6.281046390533447
Epoch 840, val loss: 0.880744457244873
Epoch 850, training loss: 63.09395217895508 = 0.3013891577720642 + 10.0 * 6.279256343841553
Epoch 850, val loss: 0.8824991583824158
Epoch 860, training loss: 63.07569122314453 = 0.2919524312019348 + 10.0 * 6.278373718261719
Epoch 860, val loss: 0.884464681148529
Epoch 870, training loss: 63.14101791381836 = 0.28281334042549133 + 10.0 * 6.285820484161377
Epoch 870, val loss: 0.8867705464363098
Epoch 880, training loss: 63.04839324951172 = 0.27382585406303406 + 10.0 * 6.277456760406494
Epoch 880, val loss: 0.8887282013893127
Epoch 890, training loss: 63.021907806396484 = 0.2652602791786194 + 10.0 * 6.275664806365967
Epoch 890, val loss: 0.8914334177970886
Epoch 900, training loss: 63.01414108276367 = 0.2569710910320282 + 10.0 * 6.275717258453369
Epoch 900, val loss: 0.8942947387695312
Epoch 910, training loss: 62.985618591308594 = 0.2488933801651001 + 10.0 * 6.273672580718994
Epoch 910, val loss: 0.8971907496452332
Epoch 920, training loss: 62.965118408203125 = 0.24110093712806702 + 10.0 * 6.272401809692383
Epoch 920, val loss: 0.9003961086273193
Epoch 930, training loss: 62.986480712890625 = 0.23358549177646637 + 10.0 * 6.275289535522461
Epoch 930, val loss: 0.9037081003189087
Epoch 940, training loss: 62.9449348449707 = 0.22630001604557037 + 10.0 * 6.2718634605407715
Epoch 940, val loss: 0.9072644710540771
Epoch 950, training loss: 62.92587661743164 = 0.21925564110279083 + 10.0 * 6.2706618309021
Epoch 950, val loss: 0.9108534455299377
Epoch 960, training loss: 62.90435791015625 = 0.21250739693641663 + 10.0 * 6.2691850662231445
Epoch 960, val loss: 0.9147883057594299
Epoch 970, training loss: 62.92851257324219 = 0.20598389208316803 + 10.0 * 6.272253036499023
Epoch 970, val loss: 0.9187819957733154
Epoch 980, training loss: 62.90251159667969 = 0.1996000111103058 + 10.0 * 6.270291328430176
Epoch 980, val loss: 0.9225913286209106
Epoch 990, training loss: 62.86965560913086 = 0.19349199533462524 + 10.0 * 6.267616271972656
Epoch 990, val loss: 0.9269556999206543
Epoch 1000, training loss: 62.84602737426758 = 0.1875985562801361 + 10.0 * 6.265842914581299
Epoch 1000, val loss: 0.9313122630119324
Epoch 1010, training loss: 62.88216781616211 = 0.1819259375333786 + 10.0 * 6.270024299621582
Epoch 1010, val loss: 0.9359180331230164
Epoch 1020, training loss: 62.8580322265625 = 0.17636743187904358 + 10.0 * 6.268166542053223
Epoch 1020, val loss: 0.9401407837867737
Epoch 1030, training loss: 62.821022033691406 = 0.17102642357349396 + 10.0 * 6.2649993896484375
Epoch 1030, val loss: 0.944912314414978
Epoch 1040, training loss: 62.80473327636719 = 0.16587501764297485 + 10.0 * 6.263885974884033
Epoch 1040, val loss: 0.9495806694030762
Epoch 1050, training loss: 62.7869987487793 = 0.1609407514333725 + 10.0 * 6.262605667114258
Epoch 1050, val loss: 0.954503059387207
Epoch 1060, training loss: 62.80401611328125 = 0.1561494767665863 + 10.0 * 6.264786720275879
Epoch 1060, val loss: 0.9593128561973572
Epoch 1070, training loss: 62.80340576171875 = 0.15150053799152374 + 10.0 * 6.265190601348877
Epoch 1070, val loss: 0.9641737341880798
Epoch 1080, training loss: 62.772918701171875 = 0.14697818458080292 + 10.0 * 6.262594223022461
Epoch 1080, val loss: 0.9691069722175598
Epoch 1090, training loss: 62.748268127441406 = 0.1426566243171692 + 10.0 * 6.260560989379883
Epoch 1090, val loss: 0.9741321802139282
Epoch 1100, training loss: 62.75691604614258 = 0.1384877860546112 + 10.0 * 6.261842727661133
Epoch 1100, val loss: 0.9792948365211487
Epoch 1110, training loss: 62.72124099731445 = 0.13443392515182495 + 10.0 * 6.258680820465088
Epoch 1110, val loss: 0.9845048785209656
Epoch 1120, training loss: 62.74919128417969 = 0.13053859770298004 + 10.0 * 6.261865139007568
Epoch 1120, val loss: 0.989791750907898
Epoch 1130, training loss: 62.719722747802734 = 0.1267348974943161 + 10.0 * 6.259298801422119
Epoch 1130, val loss: 0.9948005080223083
Epoch 1140, training loss: 62.69062423706055 = 0.12307532131671906 + 10.0 * 6.2567548751831055
Epoch 1140, val loss: 1.0000654458999634
Epoch 1150, training loss: 62.7184944152832 = 0.11955489963293076 + 10.0 * 6.259893894195557
Epoch 1150, val loss: 1.0054157972335815
Epoch 1160, training loss: 62.68696212768555 = 0.11610966175794601 + 10.0 * 6.25708532333374
Epoch 1160, val loss: 1.010402798652649
Epoch 1170, training loss: 62.6710090637207 = 0.11279156059026718 + 10.0 * 6.255821704864502
Epoch 1170, val loss: 1.015797734260559
Epoch 1180, training loss: 62.6794548034668 = 0.10959912091493607 + 10.0 * 6.256985664367676
Epoch 1180, val loss: 1.0209734439849854
Epoch 1190, training loss: 62.6767692565918 = 0.10649468004703522 + 10.0 * 6.257027626037598
Epoch 1190, val loss: 1.0262489318847656
Epoch 1200, training loss: 62.637794494628906 = 0.10348416119813919 + 10.0 * 6.2534308433532715
Epoch 1200, val loss: 1.031479001045227
Epoch 1210, training loss: 62.62816619873047 = 0.1005859225988388 + 10.0 * 6.252758026123047
Epoch 1210, val loss: 1.0368444919586182
Epoch 1220, training loss: 62.630226135253906 = 0.0978027954697609 + 10.0 * 6.253242492675781
Epoch 1220, val loss: 1.0422242879867554
Epoch 1230, training loss: 62.66548538208008 = 0.09508371353149414 + 10.0 * 6.257040023803711
Epoch 1230, val loss: 1.0472835302352905
Epoch 1240, training loss: 62.64249801635742 = 0.09244377911090851 + 10.0 * 6.255005359649658
Epoch 1240, val loss: 1.052501916885376
Epoch 1250, training loss: 62.60506820678711 = 0.08990687131881714 + 10.0 * 6.251515865325928
Epoch 1250, val loss: 1.057922601699829
Epoch 1260, training loss: 62.590003967285156 = 0.08745363354682922 + 10.0 * 6.250255107879639
Epoch 1260, val loss: 1.0632116794586182
Epoch 1270, training loss: 62.591793060302734 = 0.0850883200764656 + 10.0 * 6.250670433044434
Epoch 1270, val loss: 1.0684914588928223
Epoch 1280, training loss: 62.61351013183594 = 0.08278711885213852 + 10.0 * 6.253072261810303
Epoch 1280, val loss: 1.0736589431762695
Epoch 1290, training loss: 62.57981872558594 = 0.08054201304912567 + 10.0 * 6.249927520751953
Epoch 1290, val loss: 1.0788816213607788
Epoch 1300, training loss: 62.56873321533203 = 0.07838617265224457 + 10.0 * 6.249034881591797
Epoch 1300, val loss: 1.0841550827026367
Epoch 1310, training loss: 62.58527755737305 = 0.07630964368581772 + 10.0 * 6.25089693069458
Epoch 1310, val loss: 1.0893349647521973
Epoch 1320, training loss: 62.57575225830078 = 0.0742734894156456 + 10.0 * 6.250147819519043
Epoch 1320, val loss: 1.0944936275482178
Epoch 1330, training loss: 62.569210052490234 = 0.0723162516951561 + 10.0 * 6.24968957901001
Epoch 1330, val loss: 1.0996191501617432
Epoch 1340, training loss: 62.53429412841797 = 0.07041558623313904 + 10.0 * 6.246387958526611
Epoch 1340, val loss: 1.1048601865768433
Epoch 1350, training loss: 62.532291412353516 = 0.06859952956438065 + 10.0 * 6.246369361877441
Epoch 1350, val loss: 1.1101101636886597
Epoch 1360, training loss: 62.52928924560547 = 0.0668392926454544 + 10.0 * 6.24624490737915
Epoch 1360, val loss: 1.115394949913025
Epoch 1370, training loss: 62.56465148925781 = 0.06512626260519028 + 10.0 * 6.249952793121338
Epoch 1370, val loss: 1.1205322742462158
Epoch 1380, training loss: 62.5677375793457 = 0.06344330310821533 + 10.0 * 6.250429630279541
Epoch 1380, val loss: 1.125062108039856
Epoch 1390, training loss: 62.52578353881836 = 0.0618281327188015 + 10.0 * 6.246395587921143
Epoch 1390, val loss: 1.130517601966858
Epoch 1400, training loss: 62.507694244384766 = 0.060262084007263184 + 10.0 * 6.244743347167969
Epoch 1400, val loss: 1.1353859901428223
Epoch 1410, training loss: 62.549476623535156 = 0.058766283094882965 + 10.0 * 6.24907112121582
Epoch 1410, val loss: 1.1405354738235474
Epoch 1420, training loss: 62.5043830871582 = 0.05729180574417114 + 10.0 * 6.244709014892578
Epoch 1420, val loss: 1.1455943584442139
Epoch 1430, training loss: 62.49739074707031 = 0.05586966872215271 + 10.0 * 6.244152069091797
Epoch 1430, val loss: 1.1505813598632812
Epoch 1440, training loss: 62.48625183105469 = 0.05449815094470978 + 10.0 * 6.243175506591797
Epoch 1440, val loss: 1.1555899381637573
Epoch 1450, training loss: 62.473785400390625 = 0.05317584425210953 + 10.0 * 6.242060661315918
Epoch 1450, val loss: 1.160618782043457
Epoch 1460, training loss: 62.477848052978516 = 0.0518895648419857 + 10.0 * 6.242595672607422
Epoch 1460, val loss: 1.1655478477478027
Epoch 1470, training loss: 62.48532485961914 = 0.05064035952091217 + 10.0 * 6.243468284606934
Epoch 1470, val loss: 1.1704275608062744
Epoch 1480, training loss: 62.493072509765625 = 0.049423642456531525 + 10.0 * 6.2443647384643555
Epoch 1480, val loss: 1.1753506660461426
Epoch 1490, training loss: 62.48979187011719 = 0.04823845997452736 + 10.0 * 6.244155406951904
Epoch 1490, val loss: 1.1798274517059326
Epoch 1500, training loss: 62.4730224609375 = 0.047095537185668945 + 10.0 * 6.242592811584473
Epoch 1500, val loss: 1.1849192380905151
Epoch 1510, training loss: 62.4476432800293 = 0.045987144112586975 + 10.0 * 6.240165710449219
Epoch 1510, val loss: 1.1896778345108032
Epoch 1520, training loss: 62.44345474243164 = 0.044922709465026855 + 10.0 * 6.239853382110596
Epoch 1520, val loss: 1.1945364475250244
Epoch 1530, training loss: 62.448123931884766 = 0.043893732130527496 + 10.0 * 6.240423202514648
Epoch 1530, val loss: 1.1992944478988647
Epoch 1540, training loss: 62.4318962097168 = 0.042887184768915176 + 10.0 * 6.238900661468506
Epoch 1540, val loss: 1.204060673713684
Epoch 1550, training loss: 62.45204544067383 = 0.04191460832953453 + 10.0 * 6.241013050079346
Epoch 1550, val loss: 1.2087324857711792
Epoch 1560, training loss: 62.424163818359375 = 0.040965888649225235 + 10.0 * 6.2383198738098145
Epoch 1560, val loss: 1.2132911682128906
Epoch 1570, training loss: 62.45026779174805 = 0.040049321949481964 + 10.0 * 6.241021633148193
Epoch 1570, val loss: 1.2180039882659912
Epoch 1580, training loss: 62.414188385009766 = 0.03915940597653389 + 10.0 * 6.2375030517578125
Epoch 1580, val loss: 1.2226355075836182
Epoch 1590, training loss: 62.42622756958008 = 0.03830558806657791 + 10.0 * 6.2387919425964355
Epoch 1590, val loss: 1.227482557296753
Epoch 1600, training loss: 62.420196533203125 = 0.03746720030903816 + 10.0 * 6.238272666931152
Epoch 1600, val loss: 1.2319332361221313
Epoch 1610, training loss: 62.40153503417969 = 0.03665187582373619 + 10.0 * 6.236488342285156
Epoch 1610, val loss: 1.236297369003296
Epoch 1620, training loss: 62.39495086669922 = 0.03587110713124275 + 10.0 * 6.235908031463623
Epoch 1620, val loss: 1.2409844398498535
Epoch 1630, training loss: 62.42108154296875 = 0.03511737659573555 + 10.0 * 6.238596439361572
Epoch 1630, val loss: 1.2453547716140747
Epoch 1640, training loss: 62.40528106689453 = 0.034370824694633484 + 10.0 * 6.237091064453125
Epoch 1640, val loss: 1.2498430013656616
Epoch 1650, training loss: 62.40005111694336 = 0.033646583557128906 + 10.0 * 6.236640453338623
Epoch 1650, val loss: 1.2541311979293823
Epoch 1660, training loss: 62.378238677978516 = 0.032948777079582214 + 10.0 * 6.2345290184021
Epoch 1660, val loss: 1.2586958408355713
Epoch 1670, training loss: 62.377296447753906 = 0.03227924928069115 + 10.0 * 6.234501838684082
Epoch 1670, val loss: 1.263177752494812
Epoch 1680, training loss: 62.38898849487305 = 0.031627871096134186 + 10.0 * 6.235735893249512
Epoch 1680, val loss: 1.26756751537323
Epoch 1690, training loss: 62.44053649902344 = 0.030984796583652496 + 10.0 * 6.240955352783203
Epoch 1690, val loss: 1.271621823310852
Epoch 1700, training loss: 62.38587188720703 = 0.03035244531929493 + 10.0 * 6.235551834106445
Epoch 1700, val loss: 1.2758234739303589
Epoch 1710, training loss: 62.358577728271484 = 0.02974243275821209 + 10.0 * 6.232883453369141
Epoch 1710, val loss: 1.2800709009170532
Epoch 1720, training loss: 62.351444244384766 = 0.029163243249058723 + 10.0 * 6.2322282791137695
Epoch 1720, val loss: 1.2843753099441528
Epoch 1730, training loss: 62.353092193603516 = 0.02860252931714058 + 10.0 * 6.232449054718018
Epoch 1730, val loss: 1.2885243892669678
Epoch 1740, training loss: 62.40561294555664 = 0.028050199151039124 + 10.0 * 6.237756252288818
Epoch 1740, val loss: 1.2924761772155762
Epoch 1750, training loss: 62.354984283447266 = 0.027508152648806572 + 10.0 * 6.232747554779053
Epoch 1750, val loss: 1.2968897819519043
Epoch 1760, training loss: 62.35367965698242 = 0.02698317915201187 + 10.0 * 6.232669830322266
Epoch 1760, val loss: 1.300889253616333
Epoch 1770, training loss: 62.358150482177734 = 0.02647562325000763 + 10.0 * 6.23316764831543
Epoch 1770, val loss: 1.3049027919769287
Epoch 1780, training loss: 62.34149932861328 = 0.025976041331887245 + 10.0 * 6.231552600860596
Epoch 1780, val loss: 1.3090064525604248
Epoch 1790, training loss: 62.350013732910156 = 0.02549518458545208 + 10.0 * 6.232451915740967
Epoch 1790, val loss: 1.3131788969039917
Epoch 1800, training loss: 62.34247589111328 = 0.02502536028623581 + 10.0 * 6.23174524307251
Epoch 1800, val loss: 1.317110300064087
Epoch 1810, training loss: 62.326576232910156 = 0.024564620107412338 + 10.0 * 6.230201244354248
Epoch 1810, val loss: 1.3209654092788696
Epoch 1820, training loss: 62.35020446777344 = 0.02412252314388752 + 10.0 * 6.232607841491699
Epoch 1820, val loss: 1.3251093626022339
Epoch 1830, training loss: 62.34090805053711 = 0.023686934262514114 + 10.0 * 6.231722354888916
Epoch 1830, val loss: 1.3289026021957397
Epoch 1840, training loss: 62.31644821166992 = 0.023258011788129807 + 10.0 * 6.229319095611572
Epoch 1840, val loss: 1.332622766494751
Epoch 1850, training loss: 62.31624221801758 = 0.02284880355000496 + 10.0 * 6.229339122772217
Epoch 1850, val loss: 1.3365238904953003
Epoch 1860, training loss: 62.329444885253906 = 0.02245020493865013 + 10.0 * 6.23069953918457
Epoch 1860, val loss: 1.3403419256210327
Epoch 1870, training loss: 62.33448791503906 = 0.02205604501068592 + 10.0 * 6.231243133544922
Epoch 1870, val loss: 1.3440157175064087
Epoch 1880, training loss: 62.3116569519043 = 0.021670451387763023 + 10.0 * 6.22899866104126
Epoch 1880, val loss: 1.3479472398757935
Epoch 1890, training loss: 62.305728912353516 = 0.02130047418177128 + 10.0 * 6.228442668914795
Epoch 1890, val loss: 1.351809024810791
Epoch 1900, training loss: 62.36963653564453 = 0.020941585302352905 + 10.0 * 6.234869480133057
Epoch 1900, val loss: 1.3555824756622314
Epoch 1910, training loss: 62.31367111206055 = 0.020579570904374123 + 10.0 * 6.22930908203125
Epoch 1910, val loss: 1.358960509300232
Epoch 1920, training loss: 62.29358673095703 = 0.020232418552041054 + 10.0 * 6.227335453033447
Epoch 1920, val loss: 1.3628181219100952
Epoch 1930, training loss: 62.285133361816406 = 0.019899258390069008 + 10.0 * 6.226523399353027
Epoch 1930, val loss: 1.366618275642395
Epoch 1940, training loss: 62.33953857421875 = 0.01957785151898861 + 10.0 * 6.231996059417725
Epoch 1940, val loss: 1.370448350906372
Epoch 1950, training loss: 62.3049430847168 = 0.019251050427556038 + 10.0 * 6.228569030761719
Epoch 1950, val loss: 1.3734780550003052
Epoch 1960, training loss: 62.28818893432617 = 0.018932895734906197 + 10.0 * 6.226925849914551
Epoch 1960, val loss: 1.377326488494873
Epoch 1970, training loss: 62.31200408935547 = 0.01862933300435543 + 10.0 * 6.229337215423584
Epoch 1970, val loss: 1.3805426359176636
Epoch 1980, training loss: 62.28052520751953 = 0.018330218270421028 + 10.0 * 6.226219654083252
Epoch 1980, val loss: 1.3843700885772705
Epoch 1990, training loss: 62.26580810546875 = 0.01803954690694809 + 10.0 * 6.224776744842529
Epoch 1990, val loss: 1.3878580331802368
Epoch 2000, training loss: 62.26800537109375 = 0.01775909587740898 + 10.0 * 6.225024700164795
Epoch 2000, val loss: 1.3914074897766113
Epoch 2010, training loss: 62.294551849365234 = 0.017485877498984337 + 10.0 * 6.227706432342529
Epoch 2010, val loss: 1.3947991132736206
Epoch 2020, training loss: 62.305320739746094 = 0.017208773642778397 + 10.0 * 6.228811264038086
Epoch 2020, val loss: 1.3979830741882324
Epoch 2030, training loss: 62.2612419128418 = 0.016937268897891045 + 10.0 * 6.224430561065674
Epoch 2030, val loss: 1.4012415409088135
Epoch 2040, training loss: 62.25146484375 = 0.01668044924736023 + 10.0 * 6.223478317260742
Epoch 2040, val loss: 1.4048939943313599
Epoch 2050, training loss: 62.246498107910156 = 0.016431620344519615 + 10.0 * 6.223006725311279
Epoch 2050, val loss: 1.4082289934158325
Epoch 2060, training loss: 62.250999450683594 = 0.01619024947285652 + 10.0 * 6.223481178283691
Epoch 2060, val loss: 1.4116700887680054
Epoch 2070, training loss: 62.318111419677734 = 0.015951041132211685 + 10.0 * 6.230216026306152
Epoch 2070, val loss: 1.414773941040039
Epoch 2080, training loss: 62.2973747253418 = 0.01571015641093254 + 10.0 * 6.228166580200195
Epoch 2080, val loss: 1.4179786443710327
Epoch 2090, training loss: 62.24460220336914 = 0.015474737621843815 + 10.0 * 6.222912788391113
Epoch 2090, val loss: 1.4213263988494873
Epoch 2100, training loss: 62.249820709228516 = 0.015250636264681816 + 10.0 * 6.223456859588623
Epoch 2100, val loss: 1.4247369766235352
Epoch 2110, training loss: 62.26841354370117 = 0.015031559392809868 + 10.0 * 6.225338459014893
Epoch 2110, val loss: 1.4278624057769775
Epoch 2120, training loss: 62.240535736083984 = 0.014812706038355827 + 10.0 * 6.222572326660156
Epoch 2120, val loss: 1.430815577507019
Epoch 2130, training loss: 62.25621032714844 = 0.01460378710180521 + 10.0 * 6.224160671234131
Epoch 2130, val loss: 1.4338332414627075
Epoch 2140, training loss: 62.2569465637207 = 0.014396816492080688 + 10.0 * 6.224255084991455
Epoch 2140, val loss: 1.437161922454834
Epoch 2150, training loss: 62.237709045410156 = 0.014193742536008358 + 10.0 * 6.222351551055908
Epoch 2150, val loss: 1.4400010108947754
Epoch 2160, training loss: 62.247798919677734 = 0.013996172696352005 + 10.0 * 6.223380088806152
Epoch 2160, val loss: 1.4430480003356934
Epoch 2170, training loss: 62.24343490600586 = 0.013803405687212944 + 10.0 * 6.222963333129883
Epoch 2170, val loss: 1.44609797000885
Epoch 2180, training loss: 62.22945785522461 = 0.013612966053187847 + 10.0 * 6.221584320068359
Epoch 2180, val loss: 1.449171543121338
Epoch 2190, training loss: 62.23616027832031 = 0.01342792622745037 + 10.0 * 6.222273349761963
Epoch 2190, val loss: 1.4521585702896118
Epoch 2200, training loss: 62.26214599609375 = 0.013247418217360973 + 10.0 * 6.224889755249023
Epoch 2200, val loss: 1.455092430114746
Epoch 2210, training loss: 62.23834991455078 = 0.013068297877907753 + 10.0 * 6.222527980804443
Epoch 2210, val loss: 1.458054780960083
Epoch 2220, training loss: 62.22972106933594 = 0.012891853228211403 + 10.0 * 6.221682548522949
Epoch 2220, val loss: 1.4608569145202637
Epoch 2230, training loss: 62.228702545166016 = 0.012722006998956203 + 10.0 * 6.221598148345947
Epoch 2230, val loss: 1.4639379978179932
Epoch 2240, training loss: 62.2251091003418 = 0.012553594075143337 + 10.0 * 6.221255302429199
Epoch 2240, val loss: 1.466469407081604
Epoch 2250, training loss: 62.26206588745117 = 0.012389485724270344 + 10.0 * 6.2249674797058105
Epoch 2250, val loss: 1.468982219696045
Epoch 2260, training loss: 62.21091842651367 = 0.012226782739162445 + 10.0 * 6.219869136810303
Epoch 2260, val loss: 1.4722644090652466
Epoch 2270, training loss: 62.2027702331543 = 0.01207006350159645 + 10.0 * 6.219069957733154
Epoch 2270, val loss: 1.4749231338500977
Epoch 2280, training loss: 62.20395278930664 = 0.011917849071323872 + 10.0 * 6.219203472137451
Epoch 2280, val loss: 1.4777204990386963
Epoch 2290, training loss: 62.242794036865234 = 0.011769451200962067 + 10.0 * 6.223102569580078
Epoch 2290, val loss: 1.4804527759552002
Epoch 2300, training loss: 62.22275161743164 = 0.011620226316154003 + 10.0 * 6.221113204956055
Epoch 2300, val loss: 1.4832926988601685
Epoch 2310, training loss: 62.223079681396484 = 0.011473533697426319 + 10.0 * 6.221160411834717
Epoch 2310, val loss: 1.485850214958191
Epoch 2320, training loss: 62.220184326171875 = 0.011330069042742252 + 10.0 * 6.220885276794434
Epoch 2320, val loss: 1.4887367486953735
Epoch 2330, training loss: 62.20933151245117 = 0.011188248172402382 + 10.0 * 6.219814300537109
Epoch 2330, val loss: 1.4914518594741821
Epoch 2340, training loss: 62.23341369628906 = 0.0110523896291852 + 10.0 * 6.222236156463623
Epoch 2340, val loss: 1.4940783977508545
Epoch 2350, training loss: 62.18773651123047 = 0.010914169251918793 + 10.0 * 6.217682361602783
Epoch 2350, val loss: 1.4964622259140015
Epoch 2360, training loss: 62.17921447753906 = 0.010783197358250618 + 10.0 * 6.216843128204346
Epoch 2360, val loss: 1.4992470741271973
Epoch 2370, training loss: 62.18144226074219 = 0.01065740454941988 + 10.0 * 6.21707820892334
Epoch 2370, val loss: 1.5019289255142212
Epoch 2380, training loss: 62.206050872802734 = 0.010533006861805916 + 10.0 * 6.219552040100098
Epoch 2380, val loss: 1.504372477531433
Epoch 2390, training loss: 62.20194625854492 = 0.010405858978629112 + 10.0 * 6.219153881072998
Epoch 2390, val loss: 1.5068286657333374
Epoch 2400, training loss: 62.18195343017578 = 0.010281117632985115 + 10.0 * 6.217167377471924
Epoch 2400, val loss: 1.5095276832580566
Epoch 2410, training loss: 62.17832946777344 = 0.010160609148442745 + 10.0 * 6.2168169021606445
Epoch 2410, val loss: 1.5120824575424194
Epoch 2420, training loss: 62.19132614135742 = 0.010044151917099953 + 10.0 * 6.218128204345703
Epoch 2420, val loss: 1.5145039558410645
Epoch 2430, training loss: 62.20378112792969 = 0.009929061867296696 + 10.0 * 6.219385147094727
Epoch 2430, val loss: 1.5169448852539062
Epoch 2440, training loss: 62.1802864074707 = 0.009814020246267319 + 10.0 * 6.217047214508057
Epoch 2440, val loss: 1.5195837020874023
Epoch 2450, training loss: 62.168636322021484 = 0.00970290508121252 + 10.0 * 6.215893268585205
Epoch 2450, val loss: 1.5221551656723022
Epoch 2460, training loss: 62.17510986328125 = 0.009596266783773899 + 10.0 * 6.216551303863525
Epoch 2460, val loss: 1.5246778726577759
Epoch 2470, training loss: 62.209407806396484 = 0.009490463882684708 + 10.0 * 6.219991683959961
Epoch 2470, val loss: 1.5269489288330078
Epoch 2480, training loss: 62.18505096435547 = 0.009379858151078224 + 10.0 * 6.217566967010498
Epoch 2480, val loss: 1.5290170907974243
Epoch 2490, training loss: 62.18941879272461 = 0.009276250377297401 + 10.0 * 6.218014240264893
Epoch 2490, val loss: 1.5315771102905273
Epoch 2500, training loss: 62.160797119140625 = 0.00917276181280613 + 10.0 * 6.21516227722168
Epoch 2500, val loss: 1.534004807472229
Epoch 2510, training loss: 62.157798767089844 = 0.009074007160961628 + 10.0 * 6.214872360229492
Epoch 2510, val loss: 1.536466121673584
Epoch 2520, training loss: 62.16126251220703 = 0.008977614343166351 + 10.0 * 6.21522855758667
Epoch 2520, val loss: 1.5388331413269043
Epoch 2530, training loss: 62.193328857421875 = 0.008881630375981331 + 10.0 * 6.21844482421875
Epoch 2530, val loss: 1.5409685373306274
Epoch 2540, training loss: 62.17141342163086 = 0.00878462940454483 + 10.0 * 6.2162628173828125
Epoch 2540, val loss: 1.543371558189392
Epoch 2550, training loss: 62.15980529785156 = 0.008692088536918163 + 10.0 * 6.215111255645752
Epoch 2550, val loss: 1.5458253622055054
Epoch 2560, training loss: 62.19169998168945 = 0.008600981906056404 + 10.0 * 6.2183098793029785
Epoch 2560, val loss: 1.5480782985687256
Epoch 2570, training loss: 62.1467170715332 = 0.008507694117724895 + 10.0 * 6.213820934295654
Epoch 2570, val loss: 1.5498799085617065
Epoch 2580, training loss: 62.14778518676758 = 0.008421543054282665 + 10.0 * 6.2139363288879395
Epoch 2580, val loss: 1.552260398864746
Epoch 2590, training loss: 62.17097854614258 = 0.008336854167282581 + 10.0 * 6.216264247894287
Epoch 2590, val loss: 1.5546631813049316
Epoch 2600, training loss: 62.156280517578125 = 0.008248641155660152 + 10.0 * 6.214803218841553
Epoch 2600, val loss: 1.5564370155334473
Epoch 2610, training loss: 62.14303970336914 = 0.008162342943251133 + 10.0 * 6.21348762512207
Epoch 2610, val loss: 1.5588659048080444
Epoch 2620, training loss: 62.1353759765625 = 0.008080458268523216 + 10.0 * 6.212729454040527
Epoch 2620, val loss: 1.560936450958252
Epoch 2630, training loss: 62.155670166015625 = 0.008001290261745453 + 10.0 * 6.214766979217529
Epoch 2630, val loss: 1.5630477666854858
Epoch 2640, training loss: 62.163448333740234 = 0.007920116186141968 + 10.0 * 6.215552806854248
Epoch 2640, val loss: 1.5649768114089966
Epoch 2650, training loss: 62.14315414428711 = 0.007840657606720924 + 10.0 * 6.213531494140625
Epoch 2650, val loss: 1.5673539638519287
Epoch 2660, training loss: 62.13128662109375 = 0.0077638705261051655 + 10.0 * 6.212352275848389
Epoch 2660, val loss: 1.5692472457885742
Epoch 2670, training loss: 62.13810729980469 = 0.007689908612519503 + 10.0 * 6.21304178237915
Epoch 2670, val loss: 1.5712532997131348
Epoch 2680, training loss: 62.18699264526367 = 0.007614257279783487 + 10.0 * 6.21793794631958
Epoch 2680, val loss: 1.5730981826782227
Epoch 2690, training loss: 62.141212463378906 = 0.007538714446127415 + 10.0 * 6.213367462158203
Epoch 2690, val loss: 1.575285792350769
Epoch 2700, training loss: 62.12869644165039 = 0.007464787922799587 + 10.0 * 6.212122917175293
Epoch 2700, val loss: 1.57736074924469
Epoch 2710, training loss: 62.14189529418945 = 0.007395693100988865 + 10.0 * 6.213449954986572
Epoch 2710, val loss: 1.57927405834198
Epoch 2720, training loss: 62.138160705566406 = 0.007325364276766777 + 10.0 * 6.213083744049072
Epoch 2720, val loss: 1.5811874866485596
Epoch 2730, training loss: 62.1512336730957 = 0.0072552477940917015 + 10.0 * 6.21439790725708
Epoch 2730, val loss: 1.5831621885299683
Epoch 2740, training loss: 62.12433624267578 = 0.007188176736235619 + 10.0 * 6.211714744567871
Epoch 2740, val loss: 1.5853347778320312
Epoch 2750, training loss: 62.1124153137207 = 0.0071220919489860535 + 10.0 * 6.210529327392578
Epoch 2750, val loss: 1.5873351097106934
Epoch 2760, training loss: 62.15208435058594 = 0.007059590425342321 + 10.0 * 6.214502334594727
Epoch 2760, val loss: 1.5893430709838867
Epoch 2770, training loss: 62.13743209838867 = 0.006992429960519075 + 10.0 * 6.213044166564941
Epoch 2770, val loss: 1.591009259223938
Epoch 2780, training loss: 62.13613510131836 = 0.0069272578693926334 + 10.0 * 6.212920665740967
Epoch 2780, val loss: 1.5928781032562256
Epoch 2790, training loss: 62.110469818115234 = 0.006862950045615435 + 10.0 * 6.210360527038574
Epoch 2790, val loss: 1.5944536924362183
Epoch 2800, training loss: 62.105377197265625 = 0.00680244667455554 + 10.0 * 6.20985746383667
Epoch 2800, val loss: 1.5964936017990112
Epoch 2810, training loss: 62.11284255981445 = 0.006743482314050198 + 10.0 * 6.2106099128723145
Epoch 2810, val loss: 1.5983176231384277
Epoch 2820, training loss: 62.15382385253906 = 0.00668531097471714 + 10.0 * 6.2147135734558105
Epoch 2820, val loss: 1.5999951362609863
Epoch 2830, training loss: 62.13401412963867 = 0.006624431349337101 + 10.0 * 6.212738990783691
Epoch 2830, val loss: 1.6020830869674683
Epoch 2840, training loss: 62.1257209777832 = 0.006564949173480272 + 10.0 * 6.211915493011475
Epoch 2840, val loss: 1.603760838508606
Epoch 2850, training loss: 62.13976287841797 = 0.0065065105445683 + 10.0 * 6.213325500488281
Epoch 2850, val loss: 1.6053953170776367
Epoch 2860, training loss: 62.10901641845703 = 0.006448717322200537 + 10.0 * 6.210257053375244
Epoch 2860, val loss: 1.60707426071167
Epoch 2870, training loss: 62.095314025878906 = 0.0063943746499717236 + 10.0 * 6.208891868591309
Epoch 2870, val loss: 1.609113097190857
Epoch 2880, training loss: 62.101715087890625 = 0.006341707892715931 + 10.0 * 6.209537506103516
Epoch 2880, val loss: 1.610950231552124
Epoch 2890, training loss: 62.16495895385742 = 0.006289785262197256 + 10.0 * 6.215867042541504
Epoch 2890, val loss: 1.6126235723495483
Epoch 2900, training loss: 62.13556671142578 = 0.006232192739844322 + 10.0 * 6.212933540344238
Epoch 2900, val loss: 1.613983392715454
Epoch 2910, training loss: 62.09965515136719 = 0.006179260089993477 + 10.0 * 6.209347724914551
Epoch 2910, val loss: 1.6161131858825684
Epoch 2920, training loss: 62.09314727783203 = 0.006128842476755381 + 10.0 * 6.2087016105651855
Epoch 2920, val loss: 1.6176964044570923
Epoch 2930, training loss: 62.139312744140625 = 0.0060797338373959064 + 10.0 * 6.21332311630249
Epoch 2930, val loss: 1.6194159984588623
Epoch 2940, training loss: 62.08634948730469 = 0.006027400027960539 + 10.0 * 6.208032131195068
Epoch 2940, val loss: 1.6209684610366821
Epoch 2950, training loss: 62.08488845825195 = 0.0059778341092169285 + 10.0 * 6.20789098739624
Epoch 2950, val loss: 1.6229338645935059
Epoch 2960, training loss: 62.0866584777832 = 0.00593037623912096 + 10.0 * 6.208072662353516
Epoch 2960, val loss: 1.624577522277832
Epoch 2970, training loss: 62.14198684692383 = 0.005884471349418163 + 10.0 * 6.2136101722717285
Epoch 2970, val loss: 1.6261869668960571
Epoch 2980, training loss: 62.09806442260742 = 0.00583570497110486 + 10.0 * 6.209222793579102
Epoch 2980, val loss: 1.6277738809585571
Epoch 2990, training loss: 62.12265396118164 = 0.005788520444184542 + 10.0 * 6.211686611175537
Epoch 2990, val loss: 1.6293927431106567
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 87.91923522949219 = 1.9507776498794556 + 10.0 * 8.596845626831055
Epoch 0, val loss: 1.9434353113174438
Epoch 10, training loss: 87.90230560302734 = 1.940083384513855 + 10.0 * 8.596221923828125
Epoch 10, val loss: 1.9333107471466064
Epoch 20, training loss: 87.84061431884766 = 1.9268341064453125 + 10.0 * 8.591378211975098
Epoch 20, val loss: 1.92049241065979
Epoch 30, training loss: 87.46446228027344 = 1.9103031158447266 + 10.0 * 8.555416107177734
Epoch 30, val loss: 1.9046560525894165
Epoch 40, training loss: 85.12841033935547 = 1.8925832509994507 + 10.0 * 8.323582649230957
Epoch 40, val loss: 1.8887022733688354
Epoch 50, training loss: 79.68014526367188 = 1.8747292757034302 + 10.0 * 7.78054141998291
Epoch 50, val loss: 1.8723387718200684
Epoch 60, training loss: 75.38750457763672 = 1.861853837966919 + 10.0 * 7.352564811706543
Epoch 60, val loss: 1.8601216077804565
Epoch 70, training loss: 72.86705780029297 = 1.8508301973342896 + 10.0 * 7.101623058319092
Epoch 70, val loss: 1.8488259315490723
Epoch 80, training loss: 71.25276947021484 = 1.8402979373931885 + 10.0 * 6.941246509552002
Epoch 80, val loss: 1.8384149074554443
Epoch 90, training loss: 70.04191589355469 = 1.8300608396530151 + 10.0 * 6.82118558883667
Epoch 90, val loss: 1.8283723592758179
Epoch 100, training loss: 69.23320007324219 = 1.8198105096817017 + 10.0 * 6.741339206695557
Epoch 100, val loss: 1.818859338760376
Epoch 110, training loss: 68.65797424316406 = 1.8103293180465698 + 10.0 * 6.684764862060547
Epoch 110, val loss: 1.8101201057434082
Epoch 120, training loss: 68.21051788330078 = 1.8014730215072632 + 10.0 * 6.640903949737549
Epoch 120, val loss: 1.801987648010254
Epoch 130, training loss: 67.83139038085938 = 1.793022632598877 + 10.0 * 6.603837013244629
Epoch 130, val loss: 1.7940913438796997
Epoch 140, training loss: 67.51690673828125 = 1.7846733331680298 + 10.0 * 6.573223114013672
Epoch 140, val loss: 1.786213755607605
Epoch 150, training loss: 67.2750015258789 = 1.7760956287384033 + 10.0 * 6.549890995025635
Epoch 150, val loss: 1.7781388759613037
Epoch 160, training loss: 67.05467987060547 = 1.7670234441757202 + 10.0 * 6.528765678405762
Epoch 160, val loss: 1.7697737216949463
Epoch 170, training loss: 66.93170166015625 = 1.7573555707931519 + 10.0 * 6.517434597015381
Epoch 170, val loss: 1.7610294818878174
Epoch 180, training loss: 66.72407531738281 = 1.746995449066162 + 10.0 * 6.497708320617676
Epoch 180, val loss: 1.751784086227417
Epoch 190, training loss: 66.5546875 = 1.7358909845352173 + 10.0 * 6.481879234313965
Epoch 190, val loss: 1.7419003248214722
Epoch 200, training loss: 66.41059875488281 = 1.7238532304763794 + 10.0 * 6.468674659729004
Epoch 200, val loss: 1.7313119173049927
Epoch 210, training loss: 66.35772705078125 = 1.7107648849487305 + 10.0 * 6.464696407318115
Epoch 210, val loss: 1.719881534576416
Epoch 220, training loss: 66.17005157470703 = 1.696452021598816 + 10.0 * 6.447360515594482
Epoch 220, val loss: 1.7075010538101196
Epoch 230, training loss: 66.05511474609375 = 1.6809172630310059 + 10.0 * 6.437419891357422
Epoch 230, val loss: 1.6940972805023193
Epoch 240, training loss: 65.94541931152344 = 1.664064884185791 + 10.0 * 6.428135395050049
Epoch 240, val loss: 1.679581880569458
Epoch 250, training loss: 65.86512756347656 = 1.645798921585083 + 10.0 * 6.421933174133301
Epoch 250, val loss: 1.6639368534088135
Epoch 260, training loss: 65.75072479248047 = 1.626129150390625 + 10.0 * 6.412459850311279
Epoch 260, val loss: 1.6471564769744873
Epoch 270, training loss: 65.66946411132812 = 1.6051063537597656 + 10.0 * 6.406435966491699
Epoch 270, val loss: 1.629286766052246
Epoch 280, training loss: 65.58003234863281 = 1.5827072858810425 + 10.0 * 6.39973258972168
Epoch 280, val loss: 1.6103191375732422
Epoch 290, training loss: 65.5374526977539 = 1.559079647064209 + 10.0 * 6.397837162017822
Epoch 290, val loss: 1.590403437614441
Epoch 300, training loss: 65.41920471191406 = 1.5341179370880127 + 10.0 * 6.3885087966918945
Epoch 300, val loss: 1.5695750713348389
Epoch 310, training loss: 65.33763885498047 = 1.508258581161499 + 10.0 * 6.382937908172607
Epoch 310, val loss: 1.5481685400009155
Epoch 320, training loss: 65.26338958740234 = 1.4815819263458252 + 10.0 * 6.378180503845215
Epoch 320, val loss: 1.5263503789901733
Epoch 330, training loss: 65.27454376220703 = 1.4542016983032227 + 10.0 * 6.3820343017578125
Epoch 330, val loss: 1.5042427778244019
Epoch 340, training loss: 65.1319580078125 = 1.426096796989441 + 10.0 * 6.370586395263672
Epoch 340, val loss: 1.4818848371505737
Epoch 350, training loss: 65.0584945678711 = 1.3977892398834229 + 10.0 * 6.366070747375488
Epoch 350, val loss: 1.4596291780471802
Epoch 360, training loss: 64.99049377441406 = 1.3693567514419556 + 10.0 * 6.362113952636719
Epoch 360, val loss: 1.4376596212387085
Epoch 370, training loss: 64.98345184326172 = 1.3409298658370972 + 10.0 * 6.364252090454102
Epoch 370, val loss: 1.4160841703414917
Epoch 380, training loss: 64.8816146850586 = 1.312638759613037 + 10.0 * 6.356897354125977
Epoch 380, val loss: 1.3949918746948242
Epoch 390, training loss: 64.80538940429688 = 1.2847609519958496 + 10.0 * 6.352062702178955
Epoch 390, val loss: 1.3745408058166504
Epoch 400, training loss: 64.7409896850586 = 1.2573460340499878 + 10.0 * 6.348364353179932
Epoch 400, val loss: 1.3548351526260376
Epoch 410, training loss: 64.7559814453125 = 1.2304039001464844 + 10.0 * 6.35255765914917
Epoch 410, val loss: 1.3357969522476196
Epoch 420, training loss: 64.66594696044922 = 1.2039180994033813 + 10.0 * 6.346202850341797
Epoch 420, val loss: 1.3174006938934326
Epoch 430, training loss: 64.57982635498047 = 1.1779826879501343 + 10.0 * 6.340184211730957
Epoch 430, val loss: 1.299820899963379
Epoch 440, training loss: 64.52465057373047 = 1.1527528762817383 + 10.0 * 6.337189674377441
Epoch 440, val loss: 1.2830463647842407
Epoch 450, training loss: 64.4722900390625 = 1.1280772686004639 + 10.0 * 6.334421157836914
Epoch 450, val loss: 1.2668808698654175
Epoch 460, training loss: 64.47957611083984 = 1.1039098501205444 + 10.0 * 6.33756685256958
Epoch 460, val loss: 1.251394271850586
Epoch 470, training loss: 64.41714477539062 = 1.079888105392456 + 10.0 * 6.333725929260254
Epoch 470, val loss: 1.2361810207366943
Epoch 480, training loss: 64.3291015625 = 1.0565462112426758 + 10.0 * 6.3272552490234375
Epoch 480, val loss: 1.2217140197753906
Epoch 490, training loss: 64.29238891601562 = 1.033628225326538 + 10.0 * 6.325876235961914
Epoch 490, val loss: 1.2077796459197998
Epoch 500, training loss: 64.25480651855469 = 1.0109977722167969 + 10.0 * 6.324380397796631
Epoch 500, val loss: 1.1942301988601685
Epoch 510, training loss: 64.20491027832031 = 0.9886882901191711 + 10.0 * 6.321622371673584
Epoch 510, val loss: 1.1811094284057617
Epoch 520, training loss: 64.16609191894531 = 0.9667375087738037 + 10.0 * 6.3199357986450195
Epoch 520, val loss: 1.168450117111206
Epoch 530, training loss: 64.1275405883789 = 0.944996178150177 + 10.0 * 6.318254470825195
Epoch 530, val loss: 1.1561279296875
Epoch 540, training loss: 64.07929229736328 = 0.9235391616821289 + 10.0 * 6.315575122833252
Epoch 540, val loss: 1.1442315578460693
Epoch 550, training loss: 64.03717803955078 = 0.9023547172546387 + 10.0 * 6.313482761383057
Epoch 550, val loss: 1.132759928703308
Epoch 560, training loss: 64.02616882324219 = 0.8815064430236816 + 10.0 * 6.31446647644043
Epoch 560, val loss: 1.121726393699646
Epoch 570, training loss: 64.01445007324219 = 0.8607321381568909 + 10.0 * 6.315371990203857
Epoch 570, val loss: 1.1110517978668213
Epoch 580, training loss: 63.93265151977539 = 0.8403351902961731 + 10.0 * 6.309231758117676
Epoch 580, val loss: 1.100670576095581
Epoch 590, training loss: 63.888946533203125 = 0.8203049898147583 + 10.0 * 6.306864261627197
Epoch 590, val loss: 1.0909465551376343
Epoch 600, training loss: 63.849761962890625 = 0.8005464673042297 + 10.0 * 6.304921627044678
Epoch 600, val loss: 1.0816383361816406
Epoch 610, training loss: 63.89185333251953 = 0.7810613512992859 + 10.0 * 6.311079025268555
Epoch 610, val loss: 1.0726988315582275
Epoch 620, training loss: 63.80318832397461 = 0.7618035674095154 + 10.0 * 6.304138660430908
Epoch 620, val loss: 1.0642622709274292
Epoch 630, training loss: 63.7777099609375 = 0.7429303526878357 + 10.0 * 6.303477764129639
Epoch 630, val loss: 1.0563284158706665
Epoch 640, training loss: 63.724281311035156 = 0.7244741320610046 + 10.0 * 6.299980640411377
Epoch 640, val loss: 1.049055814743042
Epoch 650, training loss: 63.69253158569336 = 0.7063872814178467 + 10.0 * 6.298614501953125
Epoch 650, val loss: 1.0422245264053345
Epoch 660, training loss: 63.66103744506836 = 0.6886399388313293 + 10.0 * 6.297239780426025
Epoch 660, val loss: 1.035900354385376
Epoch 670, training loss: 63.65230178833008 = 0.6712666749954224 + 10.0 * 6.298103332519531
Epoch 670, val loss: 1.0301207304000854
Epoch 680, training loss: 63.640052795410156 = 0.6541957259178162 + 10.0 * 6.298585891723633
Epoch 680, val loss: 1.0246959924697876
Epoch 690, training loss: 63.59220886230469 = 0.6373777389526367 + 10.0 * 6.295483112335205
Epoch 690, val loss: 1.019808292388916
Epoch 700, training loss: 63.551788330078125 = 0.6210489869117737 + 10.0 * 6.293074131011963
Epoch 700, val loss: 1.0154004096984863
Epoch 710, training loss: 63.54308319091797 = 0.6050052046775818 + 10.0 * 6.2938079833984375
Epoch 710, val loss: 1.0113340616226196
Epoch 720, training loss: 63.496463775634766 = 0.5892435908317566 + 10.0 * 6.290721893310547
Epoch 720, val loss: 1.007717251777649
Epoch 730, training loss: 63.457054138183594 = 0.5739092230796814 + 10.0 * 6.288314342498779
Epoch 730, val loss: 1.0047225952148438
Epoch 740, training loss: 63.47337341308594 = 0.5588505268096924 + 10.0 * 6.291452407836914
Epoch 740, val loss: 1.0020407438278198
Epoch 750, training loss: 63.43840408325195 = 0.5439731478691101 + 10.0 * 6.289443016052246
Epoch 750, val loss: 0.9993480443954468
Epoch 760, training loss: 63.390377044677734 = 0.5294375419616699 + 10.0 * 6.286093711853027
Epoch 760, val loss: 0.9973467588424683
Epoch 770, training loss: 63.35751724243164 = 0.5152291059494019 + 10.0 * 6.284228801727295
Epoch 770, val loss: 0.995509147644043
Epoch 780, training loss: 63.37145233154297 = 0.5013527274131775 + 10.0 * 6.2870097160339355
Epoch 780, val loss: 0.994148313999176
Epoch 790, training loss: 63.35967254638672 = 0.4874950647354126 + 10.0 * 6.287217617034912
Epoch 790, val loss: 0.9925528168678284
Epoch 800, training loss: 63.295143127441406 = 0.4740021824836731 + 10.0 * 6.282114028930664
Epoch 800, val loss: 0.9915115833282471
Epoch 810, training loss: 63.272220611572266 = 0.4608103036880493 + 10.0 * 6.2811408042907715
Epoch 810, val loss: 0.9907968044281006
Epoch 820, training loss: 63.23741912841797 = 0.44785362482070923 + 10.0 * 6.278956413269043
Epoch 820, val loss: 0.9902982115745544
Epoch 830, training loss: 63.21799087524414 = 0.43515077233314514 + 10.0 * 6.278284072875977
Epoch 830, val loss: 0.9900593161582947
Epoch 840, training loss: 63.306724548339844 = 0.4226955473423004 + 10.0 * 6.288403034210205
Epoch 840, val loss: 0.9901193380355835
Epoch 850, training loss: 63.21197509765625 = 0.41032588481903076 + 10.0 * 6.280165195465088
Epoch 850, val loss: 0.9900829195976257
Epoch 860, training loss: 63.175201416015625 = 0.3982461988925934 + 10.0 * 6.277695655822754
Epoch 860, val loss: 0.9904217720031738
Epoch 870, training loss: 63.14200210571289 = 0.38647449016571045 + 10.0 * 6.275552749633789
Epoch 870, val loss: 0.9909940361976624
Epoch 880, training loss: 63.11479187011719 = 0.37504127621650696 + 10.0 * 6.273974895477295
Epoch 880, val loss: 0.9920843243598938
Epoch 890, training loss: 63.15305709838867 = 0.3638724088668823 + 10.0 * 6.278918266296387
Epoch 890, val loss: 0.9933282732963562
Epoch 900, training loss: 63.12535095214844 = 0.35278454422950745 + 10.0 * 6.277256965637207
Epoch 900, val loss: 0.994347333908081
Epoch 910, training loss: 63.06036376953125 = 0.3420729637145996 + 10.0 * 6.271829128265381
Epoch 910, val loss: 0.9959245324134827
Epoch 920, training loss: 63.04252243041992 = 0.33170291781425476 + 10.0 * 6.271081924438477
Epoch 920, val loss: 0.9978410601615906
Epoch 930, training loss: 63.09377670288086 = 0.32159286737442017 + 10.0 * 6.277218341827393
Epoch 930, val loss: 0.9998050332069397
Epoch 940, training loss: 63.042236328125 = 0.3117704689502716 + 10.0 * 6.273046493530273
Epoch 940, val loss: 1.0021953582763672
Epoch 950, training loss: 62.99169158935547 = 0.3021933138370514 + 10.0 * 6.26894998550415
Epoch 950, val loss: 1.0045844316482544
Epoch 960, training loss: 62.9720344543457 = 0.2929937541484833 + 10.0 * 6.267903804779053
Epoch 960, val loss: 1.0074270963668823
Epoch 970, training loss: 63.003944396972656 = 0.28406012058258057 + 10.0 * 6.271988391876221
Epoch 970, val loss: 1.0103505849838257
Epoch 980, training loss: 62.95598602294922 = 0.275346040725708 + 10.0 * 6.268064022064209
Epoch 980, val loss: 1.01340651512146
Epoch 990, training loss: 62.95723342895508 = 0.2669427990913391 + 10.0 * 6.269029140472412
Epoch 990, val loss: 1.0167423486709595
Epoch 1000, training loss: 62.9161491394043 = 0.25881388783454895 + 10.0 * 6.265733242034912
Epoch 1000, val loss: 1.0202112197875977
Epoch 1010, training loss: 62.894126892089844 = 0.250963032245636 + 10.0 * 6.264316558837891
Epoch 1010, val loss: 1.0238871574401855
Epoch 1020, training loss: 62.87900924682617 = 0.24338921904563904 + 10.0 * 6.263562202453613
Epoch 1020, val loss: 1.0278375148773193
Epoch 1030, training loss: 62.883026123046875 = 0.2360914945602417 + 10.0 * 6.264693260192871
Epoch 1030, val loss: 1.032023549079895
Epoch 1040, training loss: 62.862083435058594 = 0.2289818972349167 + 10.0 * 6.263310432434082
Epoch 1040, val loss: 1.0361696481704712
Epoch 1050, training loss: 62.91157913208008 = 0.22212298214435577 + 10.0 * 6.268945693969727
Epoch 1050, val loss: 1.0404244661331177
Epoch 1060, training loss: 62.84840393066406 = 0.21542386710643768 + 10.0 * 6.263298034667969
Epoch 1060, val loss: 1.044975757598877
Epoch 1070, training loss: 62.815269470214844 = 0.20906180143356323 + 10.0 * 6.260621070861816
Epoch 1070, val loss: 1.0495175123214722
Epoch 1080, training loss: 62.79560089111328 = 0.20295006036758423 + 10.0 * 6.259264945983887
Epoch 1080, val loss: 1.054352879524231
Epoch 1090, training loss: 62.79471206665039 = 0.1970527172088623 + 10.0 * 6.259766101837158
Epoch 1090, val loss: 1.0592572689056396
Epoch 1100, training loss: 62.80875015258789 = 0.19131851196289062 + 10.0 * 6.261743068695068
Epoch 1100, val loss: 1.064168095588684
Epoch 1110, training loss: 62.77558135986328 = 0.18574564158916473 + 10.0 * 6.258983612060547
Epoch 1110, val loss: 1.069193959236145
Epoch 1120, training loss: 62.75326919555664 = 0.18041522800922394 + 10.0 * 6.257285118103027
Epoch 1120, val loss: 1.074428677558899
Epoch 1130, training loss: 62.760189056396484 = 0.17533548176288605 + 10.0 * 6.258485317230225
Epoch 1130, val loss: 1.0797624588012695
Epoch 1140, training loss: 62.734352111816406 = 0.17033930122852325 + 10.0 * 6.256401538848877
Epoch 1140, val loss: 1.0848469734191895
Epoch 1150, training loss: 62.71784973144531 = 0.1655304878950119 + 10.0 * 6.255231857299805
Epoch 1150, val loss: 1.0901672840118408
Epoch 1160, training loss: 62.7153434753418 = 0.16092704236507416 + 10.0 * 6.255441665649414
Epoch 1160, val loss: 1.0954419374465942
Epoch 1170, training loss: 62.76319885253906 = 0.15650123357772827 + 10.0 * 6.260669708251953
Epoch 1170, val loss: 1.1009466648101807
Epoch 1180, training loss: 62.71257019042969 = 0.1521133929491043 + 10.0 * 6.256045341491699
Epoch 1180, val loss: 1.1062781810760498
Epoch 1190, training loss: 62.69194412231445 = 0.1479809582233429 + 10.0 * 6.254396438598633
Epoch 1190, val loss: 1.1118937730789185
Epoch 1200, training loss: 62.72931671142578 = 0.14395180344581604 + 10.0 * 6.258536338806152
Epoch 1200, val loss: 1.1172863245010376
Epoch 1210, training loss: 62.67736053466797 = 0.14005222916603088 + 10.0 * 6.253730773925781
Epoch 1210, val loss: 1.1228935718536377
Epoch 1220, training loss: 62.66587448120117 = 0.13629093766212463 + 10.0 * 6.252958297729492
Epoch 1220, val loss: 1.1285194158554077
Epoch 1230, training loss: 62.65763473510742 = 0.13266924023628235 + 10.0 * 6.252496719360352
Epoch 1230, val loss: 1.134289264678955
Epoch 1240, training loss: 62.649898529052734 = 0.12915296852588654 + 10.0 * 6.252074241638184
Epoch 1240, val loss: 1.1398580074310303
Epoch 1250, training loss: 62.62445068359375 = 0.1257554441690445 + 10.0 * 6.249869346618652
Epoch 1250, val loss: 1.1456189155578613
Epoch 1260, training loss: 62.62498092651367 = 0.12248700857162476 + 10.0 * 6.25024938583374
Epoch 1260, val loss: 1.1515294313430786
Epoch 1270, training loss: 62.71515655517578 = 0.11931522935628891 + 10.0 * 6.259583950042725
Epoch 1270, val loss: 1.1571329832077026
Epoch 1280, training loss: 62.63148498535156 = 0.1161847859621048 + 10.0 * 6.251530170440674
Epoch 1280, val loss: 1.1626968383789062
Epoch 1290, training loss: 62.59359359741211 = 0.1131981685757637 + 10.0 * 6.248039722442627
Epoch 1290, val loss: 1.1685413122177124
Epoch 1300, training loss: 62.601863861083984 = 0.11034620553255081 + 10.0 * 6.249151706695557
Epoch 1300, val loss: 1.1744619607925415
Epoch 1310, training loss: 62.596946716308594 = 0.10755318403244019 + 10.0 * 6.248939514160156
Epoch 1310, val loss: 1.180246114730835
Epoch 1320, training loss: 62.588096618652344 = 0.10484473407268524 + 10.0 * 6.248325347900391
Epoch 1320, val loss: 1.1860406398773193
Epoch 1330, training loss: 62.58015060424805 = 0.10223881155252457 + 10.0 * 6.247791290283203
Epoch 1330, val loss: 1.1920185089111328
Epoch 1340, training loss: 62.558258056640625 = 0.09970153123140335 + 10.0 * 6.245855808258057
Epoch 1340, val loss: 1.1977866888046265
Epoch 1350, training loss: 62.60413360595703 = 0.09725338220596313 + 10.0 * 6.250688076019287
Epoch 1350, val loss: 1.2035614252090454
Epoch 1360, training loss: 62.556148529052734 = 0.09485408663749695 + 10.0 * 6.246129512786865
Epoch 1360, val loss: 1.2094236612319946
Epoch 1370, training loss: 62.54601287841797 = 0.09253447502851486 + 10.0 * 6.24534797668457
Epoch 1370, val loss: 1.2150638103485107
Epoch 1380, training loss: 62.562461853027344 = 0.09031626582145691 + 10.0 * 6.247214317321777
Epoch 1380, val loss: 1.2209609746932983
Epoch 1390, training loss: 62.525733947753906 = 0.08813773840665817 + 10.0 * 6.243759632110596
Epoch 1390, val loss: 1.226715326309204
Epoch 1400, training loss: 62.51448440551758 = 0.08603202551603317 + 10.0 * 6.242845058441162
Epoch 1400, val loss: 1.2323814630508423
Epoch 1410, training loss: 62.50954818725586 = 0.08401040732860565 + 10.0 * 6.2425537109375
Epoch 1410, val loss: 1.2383219003677368
Epoch 1420, training loss: 62.54155349731445 = 0.08206227421760559 + 10.0 * 6.2459492683410645
Epoch 1420, val loss: 1.2441593408584595
Epoch 1430, training loss: 62.503238677978516 = 0.08012264966964722 + 10.0 * 6.242311477661133
Epoch 1430, val loss: 1.2497382164001465
Epoch 1440, training loss: 62.49454879760742 = 0.07825705409049988 + 10.0 * 6.241629123687744
Epoch 1440, val loss: 1.255569338798523
Epoch 1450, training loss: 62.485198974609375 = 0.07646140456199646 + 10.0 * 6.24087381362915
Epoch 1450, val loss: 1.2612727880477905
Epoch 1460, training loss: 62.491455078125 = 0.07473093271255493 + 10.0 * 6.241672515869141
Epoch 1460, val loss: 1.2669695615768433
Epoch 1470, training loss: 62.50359344482422 = 0.0730351135134697 + 10.0 * 6.243055820465088
Epoch 1470, val loss: 1.2727530002593994
Epoch 1480, training loss: 62.47887420654297 = 0.07136396318674088 + 10.0 * 6.240750789642334
Epoch 1480, val loss: 1.2783383131027222
Epoch 1490, training loss: 62.485557556152344 = 0.06976408511400223 + 10.0 * 6.241579532623291
Epoch 1490, val loss: 1.284185767173767
Epoch 1500, training loss: 62.46121597290039 = 0.06821217387914658 + 10.0 * 6.239300727844238
Epoch 1500, val loss: 1.2896127700805664
Epoch 1510, training loss: 62.486534118652344 = 0.06672769039869308 + 10.0 * 6.24198055267334
Epoch 1510, val loss: 1.2953613996505737
Epoch 1520, training loss: 62.472129821777344 = 0.06525367498397827 + 10.0 * 6.240687370300293
Epoch 1520, val loss: 1.3008835315704346
Epoch 1530, training loss: 62.44448471069336 = 0.06380778551101685 + 10.0 * 6.238067626953125
Epoch 1530, val loss: 1.3061811923980713
Epoch 1540, training loss: 62.43825149536133 = 0.06243507191538811 + 10.0 * 6.237581729888916
Epoch 1540, val loss: 1.3119266033172607
Epoch 1550, training loss: 62.43666076660156 = 0.06110227480530739 + 10.0 * 6.237555503845215
Epoch 1550, val loss: 1.3175480365753174
Epoch 1560, training loss: 62.496360778808594 = 0.05980849638581276 + 10.0 * 6.243655204772949
Epoch 1560, val loss: 1.3230209350585938
Epoch 1570, training loss: 62.4552001953125 = 0.05852026492357254 + 10.0 * 6.239667892456055
Epoch 1570, val loss: 1.328192949295044
Epoch 1580, training loss: 62.44778060913086 = 0.0572667270898819 + 10.0 * 6.239051342010498
Epoch 1580, val loss: 1.3335332870483398
Epoch 1590, training loss: 62.44672393798828 = 0.056067440658807755 + 10.0 * 6.239065647125244
Epoch 1590, val loss: 1.338888168334961
Epoch 1600, training loss: 62.41066360473633 = 0.054896045476198196 + 10.0 * 6.235576629638672
Epoch 1600, val loss: 1.3443527221679688
Epoch 1610, training loss: 62.412147521972656 = 0.0537731908261776 + 10.0 * 6.235837459564209
Epoch 1610, val loss: 1.349858045578003
Epoch 1620, training loss: 62.4350700378418 = 0.05267789587378502 + 10.0 * 6.238239288330078
Epoch 1620, val loss: 1.3551139831542969
Epoch 1630, training loss: 62.43292236328125 = 0.05159735307097435 + 10.0 * 6.238132476806641
Epoch 1630, val loss: 1.360276699066162
Epoch 1640, training loss: 62.394744873046875 = 0.05054306983947754 + 10.0 * 6.234419822692871
Epoch 1640, val loss: 1.365852952003479
Epoch 1650, training loss: 62.391109466552734 = 0.04952692613005638 + 10.0 * 6.234158515930176
Epoch 1650, val loss: 1.3709567785263062
Epoch 1660, training loss: 62.38682556152344 = 0.04854440316557884 + 10.0 * 6.233828067779541
Epoch 1660, val loss: 1.3763192892074585
Epoch 1670, training loss: 62.42021942138672 = 0.04759166017174721 + 10.0 * 6.237262725830078
Epoch 1670, val loss: 1.3815422058105469
Epoch 1680, training loss: 62.38849639892578 = 0.04663741588592529 + 10.0 * 6.234185695648193
Epoch 1680, val loss: 1.3866629600524902
Epoch 1690, training loss: 62.38264083862305 = 0.045714765787124634 + 10.0 * 6.233692646026611
Epoch 1690, val loss: 1.391845703125
Epoch 1700, training loss: 62.3825569152832 = 0.04482707008719444 + 10.0 * 6.233773231506348
Epoch 1700, val loss: 1.3966947793960571
Epoch 1710, training loss: 62.40983200073242 = 0.04397561028599739 + 10.0 * 6.23658561706543
Epoch 1710, val loss: 1.4020347595214844
Epoch 1720, training loss: 62.397132873535156 = 0.04312397539615631 + 10.0 * 6.235400676727295
Epoch 1720, val loss: 1.4073987007141113
Epoch 1730, training loss: 62.362491607666016 = 0.0422951839864254 + 10.0 * 6.232019901275635
Epoch 1730, val loss: 1.4121423959732056
Epoch 1740, training loss: 62.354576110839844 = 0.04150152578949928 + 10.0 * 6.231307506561279
Epoch 1740, val loss: 1.4174262285232544
Epoch 1750, training loss: 62.418846130371094 = 0.040731899440288544 + 10.0 * 6.237811088562012
Epoch 1750, val loss: 1.4223523139953613
Epoch 1760, training loss: 62.37053680419922 = 0.03996708616614342 + 10.0 * 6.233057022094727
Epoch 1760, val loss: 1.4274431467056274
Epoch 1770, training loss: 62.362552642822266 = 0.03921611234545708 + 10.0 * 6.232333660125732
Epoch 1770, val loss: 1.4323220252990723
Epoch 1780, training loss: 62.36415100097656 = 0.03850388526916504 + 10.0 * 6.232564449310303
Epoch 1780, val loss: 1.437204122543335
Epoch 1790, training loss: 62.36534881591797 = 0.0377960242331028 + 10.0 * 6.232755184173584
Epoch 1790, val loss: 1.4421147108078003
Epoch 1800, training loss: 62.38384246826172 = 0.03710922971367836 + 10.0 * 6.234673500061035
Epoch 1800, val loss: 1.4468889236450195
Epoch 1810, training loss: 62.3386344909668 = 0.03643118962645531 + 10.0 * 6.230220317840576
Epoch 1810, val loss: 1.4517439603805542
Epoch 1820, training loss: 62.32737350463867 = 0.03578472509980202 + 10.0 * 6.229158878326416
Epoch 1820, val loss: 1.4567265510559082
Epoch 1830, training loss: 62.3420295715332 = 0.03515932336449623 + 10.0 * 6.230687141418457
Epoch 1830, val loss: 1.461579442024231
Epoch 1840, training loss: 62.352394104003906 = 0.03453909978270531 + 10.0 * 6.231785774230957
Epoch 1840, val loss: 1.4662011861801147
Epoch 1850, training loss: 62.33224868774414 = 0.03392448276281357 + 10.0 * 6.229832649230957
Epoch 1850, val loss: 1.4708354473114014
Epoch 1860, training loss: 62.319400787353516 = 0.03333785757422447 + 10.0 * 6.228606224060059
Epoch 1860, val loss: 1.4756150245666504
Epoch 1870, training loss: 62.36072540283203 = 0.03277038782835007 + 10.0 * 6.232795238494873
Epoch 1870, val loss: 1.4800140857696533
Epoch 1880, training loss: 62.3854866027832 = 0.032203659415245056 + 10.0 * 6.235328197479248
Epoch 1880, val loss: 1.4852917194366455
Epoch 1890, training loss: 62.328853607177734 = 0.031637392938137054 + 10.0 * 6.229721546173096
Epoch 1890, val loss: 1.4893161058425903
Epoch 1900, training loss: 62.30255126953125 = 0.031108278781175613 + 10.0 * 6.227144241333008
Epoch 1900, val loss: 1.4941521883010864
Epoch 1910, training loss: 62.295928955078125 = 0.030593622475862503 + 10.0 * 6.22653341293335
Epoch 1910, val loss: 1.4988682270050049
Epoch 1920, training loss: 62.346012115478516 = 0.03010513260960579 + 10.0 * 6.231590747833252
Epoch 1920, val loss: 1.503551959991455
Epoch 1930, training loss: 62.29917907714844 = 0.0295878853648901 + 10.0 * 6.226959228515625
Epoch 1930, val loss: 1.5078269243240356
Epoch 1940, training loss: 62.317989349365234 = 0.029101448133587837 + 10.0 * 6.228888511657715
Epoch 1940, val loss: 1.5120881795883179
Epoch 1950, training loss: 62.28859329223633 = 0.02862239070236683 + 10.0 * 6.225996971130371
Epoch 1950, val loss: 1.5165290832519531
Epoch 1960, training loss: 62.28933334350586 = 0.02816932089626789 + 10.0 * 6.22611665725708
Epoch 1960, val loss: 1.5210762023925781
Epoch 1970, training loss: 62.31251907348633 = 0.027725953608751297 + 10.0 * 6.228479385375977
Epoch 1970, val loss: 1.5255955457687378
Epoch 1980, training loss: 62.30927276611328 = 0.02728366106748581 + 10.0 * 6.228199005126953
Epoch 1980, val loss: 1.5299245119094849
Epoch 1990, training loss: 62.28315353393555 = 0.026839954778552055 + 10.0 * 6.225631237030029
Epoch 1990, val loss: 1.5339876413345337
Epoch 2000, training loss: 62.27900695800781 = 0.02642490342259407 + 10.0 * 6.2252583503723145
Epoch 2000, val loss: 1.538479208946228
Epoch 2010, training loss: 62.31572723388672 = 0.02602650783956051 + 10.0 * 6.228970050811768
Epoch 2010, val loss: 1.5427558422088623
Epoch 2020, training loss: 62.27545166015625 = 0.025610633194446564 + 10.0 * 6.224984169006348
Epoch 2020, val loss: 1.5470149517059326
Epoch 2030, training loss: 62.270301818847656 = 0.025220124050974846 + 10.0 * 6.224508285522461
Epoch 2030, val loss: 1.5510772466659546
Epoch 2040, training loss: 62.284446716308594 = 0.02484157681465149 + 10.0 * 6.225960731506348
Epoch 2040, val loss: 1.5553784370422363
Epoch 2050, training loss: 62.277809143066406 = 0.024463597685098648 + 10.0 * 6.225334644317627
Epoch 2050, val loss: 1.559594988822937
Epoch 2060, training loss: 62.31283187866211 = 0.024097483605146408 + 10.0 * 6.228873252868652
Epoch 2060, val loss: 1.5634747743606567
Epoch 2070, training loss: 62.2706413269043 = 0.023728057742118835 + 10.0 * 6.224691390991211
Epoch 2070, val loss: 1.5674227476119995
Epoch 2080, training loss: 62.25414276123047 = 0.023376891389489174 + 10.0 * 6.223076820373535
Epoch 2080, val loss: 1.5715546607971191
Epoch 2090, training loss: 62.24987030029297 = 0.023039638996124268 + 10.0 * 6.222682952880859
Epoch 2090, val loss: 1.5757192373275757
Epoch 2100, training loss: 62.31391143798828 = 0.02271241508424282 + 10.0 * 6.229119777679443
Epoch 2100, val loss: 1.5796778202056885
Epoch 2110, training loss: 62.24952697753906 = 0.022371551021933556 + 10.0 * 6.222715377807617
Epoch 2110, val loss: 1.5834089517593384
Epoch 2120, training loss: 62.2401237487793 = 0.022044887766242027 + 10.0 * 6.221807956695557
Epoch 2120, val loss: 1.5876038074493408
Epoch 2130, training loss: 62.237098693847656 = 0.021733392030000687 + 10.0 * 6.221536636352539
Epoch 2130, val loss: 1.5914981365203857
Epoch 2140, training loss: 62.374908447265625 = 0.02143397554755211 + 10.0 * 6.235347270965576
Epoch 2140, val loss: 1.5951180458068848
Epoch 2150, training loss: 62.28160858154297 = 0.021118195727467537 + 10.0 * 6.226048946380615
Epoch 2150, val loss: 1.5992724895477295
Epoch 2160, training loss: 62.24406433105469 = 0.020816242322325706 + 10.0 * 6.222324848175049
Epoch 2160, val loss: 1.6029576063156128
Epoch 2170, training loss: 62.22836685180664 = 0.02053142711520195 + 10.0 * 6.220783710479736
Epoch 2170, val loss: 1.6068544387817383
Epoch 2180, training loss: 62.2264289855957 = 0.0202525332570076 + 10.0 * 6.220617771148682
Epoch 2180, val loss: 1.6109235286712646
Epoch 2190, training loss: 62.336570739746094 = 0.019983746111392975 + 10.0 * 6.231658458709717
Epoch 2190, val loss: 1.6150462627410889
Epoch 2200, training loss: 62.266029357910156 = 0.01969914883375168 + 10.0 * 6.22463321685791
Epoch 2200, val loss: 1.6181374788284302
Epoch 2210, training loss: 62.22820281982422 = 0.019425511360168457 + 10.0 * 6.220877647399902
Epoch 2210, val loss: 1.621853232383728
Epoch 2220, training loss: 62.25153350830078 = 0.019169019535183907 + 10.0 * 6.223236560821533
Epoch 2220, val loss: 1.6258420944213867
Epoch 2230, training loss: 62.221561431884766 = 0.01891099475324154 + 10.0 * 6.2202653884887695
Epoch 2230, val loss: 1.6292638778686523
Epoch 2240, training loss: 62.24045944213867 = 0.018661154434084892 + 10.0 * 6.222179889678955
Epoch 2240, val loss: 1.632972240447998
Epoch 2250, training loss: 62.22346115112305 = 0.018413126468658447 + 10.0 * 6.2205047607421875
Epoch 2250, val loss: 1.6365655660629272
Epoch 2260, training loss: 62.2196159362793 = 0.018174126744270325 + 10.0 * 6.220144271850586
Epoch 2260, val loss: 1.6401960849761963
Epoch 2270, training loss: 62.248085021972656 = 0.017939012497663498 + 10.0 * 6.2230143547058105
Epoch 2270, val loss: 1.643750786781311
Epoch 2280, training loss: 62.21773147583008 = 0.017710471525788307 + 10.0 * 6.220002174377441
Epoch 2280, val loss: 1.6477251052856445
Epoch 2290, training loss: 62.21417236328125 = 0.017483491450548172 + 10.0 * 6.219668865203857
Epoch 2290, val loss: 1.6508526802062988
Epoch 2300, training loss: 62.22480392456055 = 0.017262283712625504 + 10.0 * 6.220754146575928
Epoch 2300, val loss: 1.6543408632278442
Epoch 2310, training loss: 62.22046661376953 = 0.017043396830558777 + 10.0 * 6.22034215927124
Epoch 2310, val loss: 1.6577409505844116
Epoch 2320, training loss: 62.22454833984375 = 0.016825586557388306 + 10.0 * 6.2207722663879395
Epoch 2320, val loss: 1.6610974073410034
Epoch 2330, training loss: 62.249610900878906 = 0.016618002206087112 + 10.0 * 6.223299503326416
Epoch 2330, val loss: 1.6647506952285767
Epoch 2340, training loss: 62.204219818115234 = 0.01640583574771881 + 10.0 * 6.218781471252441
Epoch 2340, val loss: 1.6678799390792847
Epoch 2350, training loss: 62.20012664794922 = 0.01620657369494438 + 10.0 * 6.218392372131348
Epoch 2350, val loss: 1.6716963052749634
Epoch 2360, training loss: 62.236427307128906 = 0.016009636223316193 + 10.0 * 6.222041606903076
Epoch 2360, val loss: 1.6748663187026978
Epoch 2370, training loss: 62.201839447021484 = 0.01581096276640892 + 10.0 * 6.218602657318115
Epoch 2370, val loss: 1.6781405210494995
Epoch 2380, training loss: 62.18764114379883 = 0.015619753859937191 + 10.0 * 6.217202186584473
Epoch 2380, val loss: 1.6813298463821411
Epoch 2390, training loss: 62.19843673706055 = 0.015436405315995216 + 10.0 * 6.218299865722656
Epoch 2390, val loss: 1.6847118139266968
Epoch 2400, training loss: 62.216773986816406 = 0.015253694728016853 + 10.0 * 6.220151901245117
Epoch 2400, val loss: 1.6878397464752197
Epoch 2410, training loss: 62.18543243408203 = 0.015068216249346733 + 10.0 * 6.217036247253418
Epoch 2410, val loss: 1.691069483757019
Epoch 2420, training loss: 62.19110870361328 = 0.014893877319991589 + 10.0 * 6.217621803283691
Epoch 2420, val loss: 1.6941534280776978
Epoch 2430, training loss: 62.2257080078125 = 0.014723937958478928 + 10.0 * 6.22109842300415
Epoch 2430, val loss: 1.6972447633743286
Epoch 2440, training loss: 62.19950485229492 = 0.014545056968927383 + 10.0 * 6.218495845794678
Epoch 2440, val loss: 1.700758934020996
Epoch 2450, training loss: 62.17693328857422 = 0.014376433566212654 + 10.0 * 6.2162556648254395
Epoch 2450, val loss: 1.7033928632736206
Epoch 2460, training loss: 62.18141555786133 = 0.014215894043445587 + 10.0 * 6.216719627380371
Epoch 2460, val loss: 1.7067333459854126
Epoch 2470, training loss: 62.240413665771484 = 0.014060204848647118 + 10.0 * 6.222635269165039
Epoch 2470, val loss: 1.7097965478897095
Epoch 2480, training loss: 62.18770217895508 = 0.01388884149491787 + 10.0 * 6.217381477355957
Epoch 2480, val loss: 1.7128266096115112
Epoch 2490, training loss: 62.17280960083008 = 0.013730541802942753 + 10.0 * 6.215908050537109
Epoch 2490, val loss: 1.715298056602478
Epoch 2500, training loss: 62.167266845703125 = 0.013579078018665314 + 10.0 * 6.215368747711182
Epoch 2500, val loss: 1.71869695186615
Epoch 2510, training loss: 62.21831512451172 = 0.013434422202408314 + 10.0 * 6.22048807144165
Epoch 2510, val loss: 1.7218060493469238
Epoch 2520, training loss: 62.172794342041016 = 0.013278864324092865 + 10.0 * 6.215951442718506
Epoch 2520, val loss: 1.7246737480163574
Epoch 2530, training loss: 62.15599822998047 = 0.013131549581885338 + 10.0 * 6.214286804199219
Epoch 2530, val loss: 1.7274584770202637
Epoch 2540, training loss: 62.161407470703125 = 0.01299242489039898 + 10.0 * 6.214841365814209
Epoch 2540, val loss: 1.7304350137710571
Epoch 2550, training loss: 62.21592330932617 = 0.0128587381914258 + 10.0 * 6.220306396484375
Epoch 2550, val loss: 1.7333731651306152
Epoch 2560, training loss: 62.1799201965332 = 0.012711752206087112 + 10.0 * 6.216721057891846
Epoch 2560, val loss: 1.736026406288147
Epoch 2570, training loss: 62.16933059692383 = 0.012574009597301483 + 10.0 * 6.2156758308410645
Epoch 2570, val loss: 1.7387510538101196
Epoch 2580, training loss: 62.200687408447266 = 0.012439229525625706 + 10.0 * 6.218824863433838
Epoch 2580, val loss: 1.741413950920105
Epoch 2590, training loss: 62.16158676147461 = 0.012302354909479618 + 10.0 * 6.21492862701416
Epoch 2590, val loss: 1.7440154552459717
Epoch 2600, training loss: 62.14425277709961 = 0.012176296673715115 + 10.0 * 6.213207721710205
Epoch 2600, val loss: 1.7470334768295288
Epoch 2610, training loss: 62.14431381225586 = 0.012054186314344406 + 10.0 * 6.213225841522217
Epoch 2610, val loss: 1.7498723268508911
Epoch 2620, training loss: 62.19752883911133 = 0.011937657371163368 + 10.0 * 6.218559265136719
Epoch 2620, val loss: 1.7527217864990234
Epoch 2630, training loss: 62.17918014526367 = 0.011804244481027126 + 10.0 * 6.216737747192383
Epoch 2630, val loss: 1.7553563117980957
Epoch 2640, training loss: 62.149879455566406 = 0.011680172756314278 + 10.0 * 6.213819980621338
Epoch 2640, val loss: 1.757366418838501
Epoch 2650, training loss: 62.13602066040039 = 0.011558860540390015 + 10.0 * 6.212446212768555
Epoch 2650, val loss: 1.7599716186523438
Epoch 2660, training loss: 62.13467025756836 = 0.01144608948379755 + 10.0 * 6.212322235107422
Epoch 2660, val loss: 1.7626416683197021
Epoch 2670, training loss: 62.160377502441406 = 0.011335281655192375 + 10.0 * 6.214904308319092
Epoch 2670, val loss: 1.7651522159576416
Epoch 2680, training loss: 62.17305374145508 = 0.011220085434615612 + 10.0 * 6.216183662414551
Epoch 2680, val loss: 1.7676295042037964
Epoch 2690, training loss: 62.141048431396484 = 0.011102798394858837 + 10.0 * 6.212994575500488
Epoch 2690, val loss: 1.7700303792953491
Epoch 2700, training loss: 62.143943786621094 = 0.010992559604346752 + 10.0 * 6.213294982910156
Epoch 2700, val loss: 1.7724637985229492
Epoch 2710, training loss: 62.15125274658203 = 0.010885954834520817 + 10.0 * 6.214036464691162
Epoch 2710, val loss: 1.7749289274215698
Epoch 2720, training loss: 62.128604888916016 = 0.010778676718473434 + 10.0 * 6.211782455444336
Epoch 2720, val loss: 1.7774741649627686
Epoch 2730, training loss: 62.12458801269531 = 0.010676573030650616 + 10.0 * 6.211390972137451
Epoch 2730, val loss: 1.7798984050750732
Epoch 2740, training loss: 62.17008590698242 = 0.010579472407698631 + 10.0 * 6.215950965881348
Epoch 2740, val loss: 1.7823131084442139
Epoch 2750, training loss: 62.12822723388672 = 0.010473735630512238 + 10.0 * 6.211775302886963
Epoch 2750, val loss: 1.7846879959106445
Epoch 2760, training loss: 62.13378143310547 = 0.010371199809014797 + 10.0 * 6.212340831756592
Epoch 2760, val loss: 1.7866870164871216
Epoch 2770, training loss: 62.1604118347168 = 0.01027179416269064 + 10.0 * 6.2150139808654785
Epoch 2770, val loss: 1.7889370918273926
Epoch 2780, training loss: 62.12583923339844 = 0.010173125192523003 + 10.0 * 6.21156644821167
Epoch 2780, val loss: 1.7915936708450317
Epoch 2790, training loss: 62.12591552734375 = 0.010078714229166508 + 10.0 * 6.211583614349365
Epoch 2790, val loss: 1.7938697338104248
Epoch 2800, training loss: 62.17971420288086 = 0.009987568482756615 + 10.0 * 6.216972827911377
Epoch 2800, val loss: 1.7957534790039062
Epoch 2810, training loss: 62.14236068725586 = 0.009892833419144154 + 10.0 * 6.213246822357178
Epoch 2810, val loss: 1.798465609550476
Epoch 2820, training loss: 62.128841400146484 = 0.00979919359087944 + 10.0 * 6.211904048919678
Epoch 2820, val loss: 1.800370693206787
Epoch 2830, training loss: 62.113895416259766 = 0.009709225967526436 + 10.0 * 6.210418701171875
Epoch 2830, val loss: 1.8026529550552368
Epoch 2840, training loss: 62.11152648925781 = 0.009624599479138851 + 10.0 * 6.210190296173096
Epoch 2840, val loss: 1.8047864437103271
Epoch 2850, training loss: 62.158939361572266 = 0.00954226404428482 + 10.0 * 6.214940071105957
Epoch 2850, val loss: 1.806921362876892
Epoch 2860, training loss: 62.1334228515625 = 0.009453792124986649 + 10.0 * 6.21239709854126
Epoch 2860, val loss: 1.8094062805175781
Epoch 2870, training loss: 62.1248779296875 = 0.009365838952362537 + 10.0 * 6.211551189422607
Epoch 2870, val loss: 1.8109540939331055
Epoch 2880, training loss: 62.121150970458984 = 0.00928200501948595 + 10.0 * 6.21118688583374
Epoch 2880, val loss: 1.813214659690857
Epoch 2890, training loss: 62.11283493041992 = 0.009200171567499638 + 10.0 * 6.210363388061523
Epoch 2890, val loss: 1.8152363300323486
Epoch 2900, training loss: 62.12559127807617 = 0.009121767245233059 + 10.0 * 6.211647033691406
Epoch 2900, val loss: 1.817386507987976
Epoch 2910, training loss: 62.1100959777832 = 0.009042755700647831 + 10.0 * 6.210104942321777
Epoch 2910, val loss: 1.819333553314209
Epoch 2920, training loss: 62.146785736083984 = 0.008967624977231026 + 10.0 * 6.213781833648682
Epoch 2920, val loss: 1.8214350938796997
Epoch 2930, training loss: 62.105403900146484 = 0.008885595947504044 + 10.0 * 6.209651947021484
Epoch 2930, val loss: 1.8235357999801636
Epoch 2940, training loss: 62.0977783203125 = 0.008809253573417664 + 10.0 * 6.208897113800049
Epoch 2940, val loss: 1.8250656127929688
Epoch 2950, training loss: 62.13190460205078 = 0.00873773917555809 + 10.0 * 6.212316513061523
Epoch 2950, val loss: 1.827125906944275
Epoch 2960, training loss: 62.12592697143555 = 0.008663282729685307 + 10.0 * 6.211726188659668
Epoch 2960, val loss: 1.828865647315979
Epoch 2970, training loss: 62.10477828979492 = 0.008585916832089424 + 10.0 * 6.209619045257568
Epoch 2970, val loss: 1.8309751749038696
Epoch 2980, training loss: 62.092716217041016 = 0.008514676243066788 + 10.0 * 6.208420276641846
Epoch 2980, val loss: 1.8327707052230835
Epoch 2990, training loss: 62.104644775390625 = 0.008448156528174877 + 10.0 * 6.209619522094727
Epoch 2990, val loss: 1.834596037864685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 87.91307067871094 = 1.9445070028305054 + 10.0 * 8.596857070922852
Epoch 0, val loss: 1.9401061534881592
Epoch 10, training loss: 87.89888000488281 = 1.9348363876342773 + 10.0 * 8.596404075622559
Epoch 10, val loss: 1.9308239221572876
Epoch 20, training loss: 87.85139465332031 = 1.9229789972305298 + 10.0 * 8.592841148376465
Epoch 20, val loss: 1.918995976448059
Epoch 30, training loss: 87.56670379638672 = 1.9076085090637207 + 10.0 * 8.565909385681152
Epoch 30, val loss: 1.9034807682037354
Epoch 40, training loss: 85.8075180053711 = 1.889130711555481 + 10.0 * 8.391839027404785
Epoch 40, val loss: 1.8857132196426392
Epoch 50, training loss: 80.51287841796875 = 1.8702092170715332 + 10.0 * 7.864266872406006
Epoch 50, val loss: 1.8681182861328125
Epoch 60, training loss: 76.47799682617188 = 1.8569830656051636 + 10.0 * 7.462101936340332
Epoch 60, val loss: 1.8567147254943848
Epoch 70, training loss: 73.38935852050781 = 1.8485549688339233 + 10.0 * 7.154080390930176
Epoch 70, val loss: 1.84916353225708
Epoch 80, training loss: 70.96084594726562 = 1.8412399291992188 + 10.0 * 6.911960601806641
Epoch 80, val loss: 1.8422971963882446
Epoch 90, training loss: 69.88961791992188 = 1.8322120904922485 + 10.0 * 6.8057403564453125
Epoch 90, val loss: 1.8336973190307617
Epoch 100, training loss: 69.13743591308594 = 1.822048544883728 + 10.0 * 6.731538772583008
Epoch 100, val loss: 1.8244212865829468
Epoch 110, training loss: 68.60088348388672 = 1.8124116659164429 + 10.0 * 6.678847789764404
Epoch 110, val loss: 1.8158237934112549
Epoch 120, training loss: 68.19219970703125 = 1.803563117980957 + 10.0 * 6.638863563537598
Epoch 120, val loss: 1.8078339099884033
Epoch 130, training loss: 67.82359313964844 = 1.7949587106704712 + 10.0 * 6.602863311767578
Epoch 130, val loss: 1.7998566627502441
Epoch 140, training loss: 67.52104187011719 = 1.786158800125122 + 10.0 * 6.573488712310791
Epoch 140, val loss: 1.7916227579116821
Epoch 150, training loss: 67.33535766601562 = 1.7768932580947876 + 10.0 * 6.555846214294434
Epoch 150, val loss: 1.7829368114471436
Epoch 160, training loss: 67.05445098876953 = 1.766855001449585 + 10.0 * 6.528759956359863
Epoch 160, val loss: 1.7737096548080444
Epoch 170, training loss: 66.8518295288086 = 1.7561804056167603 + 10.0 * 6.5095648765563965
Epoch 170, val loss: 1.7639743089675903
Epoch 180, training loss: 66.68202209472656 = 1.74467933177948 + 10.0 * 6.493733882904053
Epoch 180, val loss: 1.7535027265548706
Epoch 190, training loss: 66.52725982666016 = 1.7322200536727905 + 10.0 * 6.479503631591797
Epoch 190, val loss: 1.7421330213546753
Epoch 200, training loss: 66.37419891357422 = 1.7186030149459839 + 10.0 * 6.465559005737305
Epoch 200, val loss: 1.7298431396484375
Epoch 210, training loss: 66.2603530883789 = 1.7038044929504395 + 10.0 * 6.455655097961426
Epoch 210, val loss: 1.7164894342422485
Epoch 220, training loss: 66.11981201171875 = 1.6877189874649048 + 10.0 * 6.443209171295166
Epoch 220, val loss: 1.7020124197006226
Epoch 230, training loss: 66.00023651123047 = 1.67023503780365 + 10.0 * 6.433000087738037
Epoch 230, val loss: 1.6864030361175537
Epoch 240, training loss: 65.89190673828125 = 1.651467204093933 + 10.0 * 6.424044132232666
Epoch 240, val loss: 1.6696107387542725
Epoch 250, training loss: 65.80596923828125 = 1.63123619556427 + 10.0 * 6.417473793029785
Epoch 250, val loss: 1.6516164541244507
Epoch 260, training loss: 65.70986938476562 = 1.6095582246780396 + 10.0 * 6.410031318664551
Epoch 260, val loss: 1.632385492324829
Epoch 270, training loss: 65.6181640625 = 1.58650803565979 + 10.0 * 6.403165340423584
Epoch 270, val loss: 1.6119874715805054
Epoch 280, training loss: 65.52792358398438 = 1.5622096061706543 + 10.0 * 6.396571636199951
Epoch 280, val loss: 1.590591549873352
Epoch 290, training loss: 65.4395751953125 = 1.5366731882095337 + 10.0 * 6.390290260314941
Epoch 290, val loss: 1.5681203603744507
Epoch 300, training loss: 65.36377716064453 = 1.5098412036895752 + 10.0 * 6.3853936195373535
Epoch 300, val loss: 1.544648289680481
Epoch 310, training loss: 65.34648132324219 = 1.4819530248641968 + 10.0 * 6.386452674865723
Epoch 310, val loss: 1.5203595161437988
Epoch 320, training loss: 65.2319107055664 = 1.4531608819961548 + 10.0 * 6.377874851226807
Epoch 320, val loss: 1.495404839515686
Epoch 330, training loss: 65.1499252319336 = 1.4236750602722168 + 10.0 * 6.37262487411499
Epoch 330, val loss: 1.4699809551239014
Epoch 340, training loss: 65.07471466064453 = 1.393561601638794 + 10.0 * 6.368115425109863
Epoch 340, val loss: 1.4441930055618286
Epoch 350, training loss: 65.00524139404297 = 1.3628531694412231 + 10.0 * 6.364238739013672
Epoch 350, val loss: 1.4180355072021484
Epoch 360, training loss: 64.97280883789062 = 1.331581950187683 + 10.0 * 6.3641228675842285
Epoch 360, val loss: 1.3916102647781372
Epoch 370, training loss: 64.89521026611328 = 1.3000707626342773 + 10.0 * 6.359514236450195
Epoch 370, val loss: 1.364902138710022
Epoch 380, training loss: 64.82233428955078 = 1.2682950496673584 + 10.0 * 6.355403900146484
Epoch 380, val loss: 1.3383114337921143
Epoch 390, training loss: 64.76514434814453 = 1.2364871501922607 + 10.0 * 6.352865695953369
Epoch 390, val loss: 1.3118841648101807
Epoch 400, training loss: 64.732666015625 = 1.2047953605651855 + 10.0 * 6.352787017822266
Epoch 400, val loss: 1.2855205535888672
Epoch 410, training loss: 64.64645385742188 = 1.173126220703125 + 10.0 * 6.347332954406738
Epoch 410, val loss: 1.2595528364181519
Epoch 420, training loss: 64.58013153076172 = 1.1417138576507568 + 10.0 * 6.343842029571533
Epoch 420, val loss: 1.2340600490570068
Epoch 430, training loss: 64.51718139648438 = 1.1105388402938843 + 10.0 * 6.340664386749268
Epoch 430, val loss: 1.2090409994125366
Epoch 440, training loss: 64.474365234375 = 1.0796624422073364 + 10.0 * 6.339469909667969
Epoch 440, val loss: 1.184522032737732
Epoch 450, training loss: 64.4212646484375 = 1.0489965677261353 + 10.0 * 6.337226867675781
Epoch 450, val loss: 1.1606003046035767
Epoch 460, training loss: 64.35370635986328 = 1.018919825553894 + 10.0 * 6.3334784507751465
Epoch 460, val loss: 1.1373107433319092
Epoch 470, training loss: 64.3028335571289 = 0.9893413186073303 + 10.0 * 6.3313493728637695
Epoch 470, val loss: 1.1148649454116821
Epoch 480, training loss: 64.24827575683594 = 0.9603238701820374 + 10.0 * 6.328794956207275
Epoch 480, val loss: 1.0932658910751343
Epoch 490, training loss: 64.29876708984375 = 0.9317600727081299 + 10.0 * 6.336700439453125
Epoch 490, val loss: 1.0724806785583496
Epoch 500, training loss: 64.17011260986328 = 0.9040448069572449 + 10.0 * 6.3266072273254395
Epoch 500, val loss: 1.0524964332580566
Epoch 510, training loss: 64.10706329345703 = 0.8770959377288818 + 10.0 * 6.322996616363525
Epoch 510, val loss: 1.0337752103805542
Epoch 520, training loss: 64.05876922607422 = 0.8509014844894409 + 10.0 * 6.320786952972412
Epoch 520, val loss: 1.0161973237991333
Epoch 530, training loss: 64.03300476074219 = 0.8255282044410706 + 10.0 * 6.3207478523254395
Epoch 530, val loss: 0.999697744846344
Epoch 540, training loss: 63.98067092895508 = 0.8008297681808472 + 10.0 * 6.317984104156494
Epoch 540, val loss: 0.9840995669364929
Epoch 550, training loss: 63.969085693359375 = 0.7769482731819153 + 10.0 * 6.3192138671875
Epoch 550, val loss: 0.9696935415267944
Epoch 560, training loss: 63.889617919921875 = 0.7538744807243347 + 10.0 * 6.313574314117432
Epoch 560, val loss: 0.9562775492668152
Epoch 570, training loss: 63.854942321777344 = 0.7316017746925354 + 10.0 * 6.312334060668945
Epoch 570, val loss: 0.9441424608230591
Epoch 580, training loss: 63.81692123413086 = 0.7101325392723083 + 10.0 * 6.310678958892822
Epoch 580, val loss: 0.933106005191803
Epoch 590, training loss: 63.81612014770508 = 0.6893446445465088 + 10.0 * 6.312677383422852
Epoch 590, val loss: 0.923032283782959
Epoch 600, training loss: 63.742305755615234 = 0.6690471172332764 + 10.0 * 6.307325839996338
Epoch 600, val loss: 0.91386878490448
Epoch 610, training loss: 63.70709228515625 = 0.6496043801307678 + 10.0 * 6.30574893951416
Epoch 610, val loss: 0.9056956171989441
Epoch 620, training loss: 63.68806076049805 = 0.6306983828544617 + 10.0 * 6.305736064910889
Epoch 620, val loss: 0.8983235955238342
Epoch 630, training loss: 63.6656608581543 = 0.6123684048652649 + 10.0 * 6.305329322814941
Epoch 630, val loss: 0.8917536735534668
Epoch 640, training loss: 63.62693405151367 = 0.5946670770645142 + 10.0 * 6.303226947784424
Epoch 640, val loss: 0.8861209154129028
Epoch 650, training loss: 63.575767517089844 = 0.5774953365325928 + 10.0 * 6.2998270988464355
Epoch 650, val loss: 0.8812591433525085
Epoch 660, training loss: 63.58658981323242 = 0.5608313083648682 + 10.0 * 6.302575588226318
Epoch 660, val loss: 0.8770039081573486
Epoch 670, training loss: 63.52095031738281 = 0.5446137189865112 + 10.0 * 6.297633647918701
Epoch 670, val loss: 0.8733469843864441
Epoch 680, training loss: 63.485145568847656 = 0.5288875699043274 + 10.0 * 6.295625686645508
Epoch 680, val loss: 0.8703858256340027
Epoch 690, training loss: 63.556121826171875 = 0.5135753154754639 + 10.0 * 6.304254531860352
Epoch 690, val loss: 0.867950975894928
Epoch 700, training loss: 63.4583625793457 = 0.498772531747818 + 10.0 * 6.295958995819092
Epoch 700, val loss: 0.8658223748207092
Epoch 710, training loss: 63.41436767578125 = 0.48430588841438293 + 10.0 * 6.29300594329834
Epoch 710, val loss: 0.8643701672554016
Epoch 720, training loss: 63.37082290649414 = 0.4703465700149536 + 10.0 * 6.290047645568848
Epoch 720, val loss: 0.8634340763092041
Epoch 730, training loss: 63.35075759887695 = 0.4567728042602539 + 10.0 * 6.289398670196533
Epoch 730, val loss: 0.862994909286499
Epoch 740, training loss: 63.383358001708984 = 0.443559855222702 + 10.0 * 6.293979644775391
Epoch 740, val loss: 0.8628779649734497
Epoch 750, training loss: 63.30044937133789 = 0.43065696954727173 + 10.0 * 6.2869791984558105
Epoch 750, val loss: 0.8631832003593445
Epoch 760, training loss: 63.279964447021484 = 0.41814279556274414 + 10.0 * 6.286181926727295
Epoch 760, val loss: 0.863955557346344
Epoch 770, training loss: 63.30039978027344 = 0.40601494908332825 + 10.0 * 6.289438724517822
Epoch 770, val loss: 0.8650382161140442
Epoch 780, training loss: 63.23666000366211 = 0.394212931394577 + 10.0 * 6.284244537353516
Epoch 780, val loss: 0.8662779331207275
Epoch 790, training loss: 63.220245361328125 = 0.3827371299266815 + 10.0 * 6.283751010894775
Epoch 790, val loss: 0.8681208491325378
Epoch 800, training loss: 63.22197341918945 = 0.371622234582901 + 10.0 * 6.285035133361816
Epoch 800, val loss: 0.8701156377792358
Epoch 810, training loss: 63.166648864746094 = 0.36084744334220886 + 10.0 * 6.280580043792725
Epoch 810, val loss: 0.8725377917289734
Epoch 820, training loss: 63.15467071533203 = 0.35040271282196045 + 10.0 * 6.280426979064941
Epoch 820, val loss: 0.8753306865692139
Epoch 830, training loss: 63.15333557128906 = 0.34026825428009033 + 10.0 * 6.281306743621826
Epoch 830, val loss: 0.8782169222831726
Epoch 840, training loss: 63.1103630065918 = 0.3303475081920624 + 10.0 * 6.278001308441162
Epoch 840, val loss: 0.8814767003059387
Epoch 850, training loss: 63.08585739135742 = 0.3208175301551819 + 10.0 * 6.276504039764404
Epoch 850, val loss: 0.885022759437561
Epoch 860, training loss: 63.07760238647461 = 0.3116023540496826 + 10.0 * 6.276599884033203
Epoch 860, val loss: 0.8888748288154602
Epoch 870, training loss: 63.084136962890625 = 0.3026205003261566 + 10.0 * 6.278151512145996
Epoch 870, val loss: 0.8928377628326416
Epoch 880, training loss: 63.07304382324219 = 0.2938503324985504 + 10.0 * 6.277919292449951
Epoch 880, val loss: 0.897000253200531
Epoch 890, training loss: 63.01782989501953 = 0.28540804982185364 + 10.0 * 6.273241996765137
Epoch 890, val loss: 0.9015931487083435
Epoch 900, training loss: 63.008056640625 = 0.277273565530777 + 10.0 * 6.273077964782715
Epoch 900, val loss: 0.9065410494804382
Epoch 910, training loss: 63.055519104003906 = 0.2693731188774109 + 10.0 * 6.278614521026611
Epoch 910, val loss: 0.9115652441978455
Epoch 920, training loss: 63.000579833984375 = 0.2617267072200775 + 10.0 * 6.273885250091553
Epoch 920, val loss: 0.9165025353431702
Epoch 930, training loss: 62.969173431396484 = 0.2542688548564911 + 10.0 * 6.271490573883057
Epoch 930, val loss: 0.9218735694885254
Epoch 940, training loss: 62.95793151855469 = 0.24712911248207092 + 10.0 * 6.271080493927002
Epoch 940, val loss: 0.9272919297218323
Epoch 950, training loss: 62.945579528808594 = 0.24015754461288452 + 10.0 * 6.270542144775391
Epoch 950, val loss: 0.9328740239143372
Epoch 960, training loss: 62.93666458129883 = 0.2334308922290802 + 10.0 * 6.270323276519775
Epoch 960, val loss: 0.9384710788726807
Epoch 970, training loss: 62.91975021362305 = 0.226861834526062 + 10.0 * 6.269289016723633
Epoch 970, val loss: 0.9442909955978394
Epoch 980, training loss: 62.8887939453125 = 0.22049884498119354 + 10.0 * 6.266829490661621
Epoch 980, val loss: 0.9500131011009216
Epoch 990, training loss: 62.93342971801758 = 0.21436545252799988 + 10.0 * 6.27190637588501
Epoch 990, val loss: 0.9561299681663513
Epoch 1000, training loss: 62.876827239990234 = 0.2083909958600998 + 10.0 * 6.266843795776367
Epoch 1000, val loss: 0.9620848298072815
Epoch 1010, training loss: 62.854129791259766 = 0.20259645581245422 + 10.0 * 6.265153408050537
Epoch 1010, val loss: 0.9682807922363281
Epoch 1020, training loss: 62.8394889831543 = 0.19702979922294617 + 10.0 * 6.264245986938477
Epoch 1020, val loss: 0.9745385050773621
Epoch 1030, training loss: 62.9320182800293 = 0.19157959520816803 + 10.0 * 6.274044036865234
Epoch 1030, val loss: 0.980715811252594
Epoch 1040, training loss: 62.81960678100586 = 0.18629057705402374 + 10.0 * 6.263331413269043
Epoch 1040, val loss: 0.9869677424430847
Epoch 1050, training loss: 62.80339813232422 = 0.18119673430919647 + 10.0 * 6.2622199058532715
Epoch 1050, val loss: 0.9932788014411926
Epoch 1060, training loss: 62.78611755371094 = 0.1762595921754837 + 10.0 * 6.260985851287842
Epoch 1060, val loss: 0.9998131394386292
Epoch 1070, training loss: 62.78984069824219 = 0.17150869965553284 + 10.0 * 6.261833190917969
Epoch 1070, val loss: 1.0062916278839111
Epoch 1080, training loss: 62.77482604980469 = 0.16684123873710632 + 10.0 * 6.260798454284668
Epoch 1080, val loss: 1.0127687454223633
Epoch 1090, training loss: 62.75895309448242 = 0.16230295598506927 + 10.0 * 6.259665012359619
Epoch 1090, val loss: 1.0192269086837769
Epoch 1100, training loss: 62.81755065917969 = 0.1579381376504898 + 10.0 * 6.265961170196533
Epoch 1100, val loss: 1.0258945226669312
Epoch 1110, training loss: 62.75326156616211 = 0.1536531299352646 + 10.0 * 6.259961128234863
Epoch 1110, val loss: 1.0324268341064453
Epoch 1120, training loss: 62.724971771240234 = 0.14953215420246124 + 10.0 * 6.257544040679932
Epoch 1120, val loss: 1.0389567613601685
Epoch 1130, training loss: 62.71226501464844 = 0.14555047452449799 + 10.0 * 6.25667142868042
Epoch 1130, val loss: 1.0457488298416138
Epoch 1140, training loss: 62.75242614746094 = 0.14170901477336884 + 10.0 * 6.261071681976318
Epoch 1140, val loss: 1.0523111820220947
Epoch 1150, training loss: 62.73303985595703 = 0.13787244260311127 + 10.0 * 6.259516716003418
Epoch 1150, val loss: 1.0590826272964478
Epoch 1160, training loss: 62.71013641357422 = 0.13417866826057434 + 10.0 * 6.257596015930176
Epoch 1160, val loss: 1.0655848979949951
Epoch 1170, training loss: 62.679569244384766 = 0.13061267137527466 + 10.0 * 6.2548956871032715
Epoch 1170, val loss: 1.0724318027496338
Epoch 1180, training loss: 62.687522888183594 = 0.12718386948108673 + 10.0 * 6.256033897399902
Epoch 1180, val loss: 1.0792759656906128
Epoch 1190, training loss: 62.683162689208984 = 0.12384642660617828 + 10.0 * 6.255931377410889
Epoch 1190, val loss: 1.0860555171966553
Epoch 1200, training loss: 62.671546936035156 = 0.12058796733617783 + 10.0 * 6.255095958709717
Epoch 1200, val loss: 1.0928949117660522
Epoch 1210, training loss: 62.651493072509766 = 0.11741358041763306 + 10.0 * 6.253407955169678
Epoch 1210, val loss: 1.0996485948562622
Epoch 1220, training loss: 62.66526794433594 = 0.11436428874731064 + 10.0 * 6.255090236663818
Epoch 1220, val loss: 1.1064726114273071
Epoch 1230, training loss: 62.63587951660156 = 0.11138761788606644 + 10.0 * 6.252449035644531
Epoch 1230, val loss: 1.1130789518356323
Epoch 1240, training loss: 62.66514205932617 = 0.10850165039300919 + 10.0 * 6.255663871765137
Epoch 1240, val loss: 1.1198532581329346
Epoch 1250, training loss: 62.62740707397461 = 0.10566446185112 + 10.0 * 6.252174377441406
Epoch 1250, val loss: 1.1267513036727905
Epoch 1260, training loss: 62.64504623413086 = 0.10295499861240387 + 10.0 * 6.254209041595459
Epoch 1260, val loss: 1.1334795951843262
Epoch 1270, training loss: 62.604740142822266 = 0.10029058903455734 + 10.0 * 6.2504448890686035
Epoch 1270, val loss: 1.1401782035827637
Epoch 1280, training loss: 62.59507369995117 = 0.0977303758263588 + 10.0 * 6.249734401702881
Epoch 1280, val loss: 1.1468169689178467
Epoch 1290, training loss: 62.58226776123047 = 0.09523925185203552 + 10.0 * 6.2487030029296875
Epoch 1290, val loss: 1.153719425201416
Epoch 1300, training loss: 62.72101974487305 = 0.0928240641951561 + 10.0 * 6.262819766998291
Epoch 1300, val loss: 1.1604095697402954
Epoch 1310, training loss: 62.593257904052734 = 0.09043222665786743 + 10.0 * 6.2502827644348145
Epoch 1310, val loss: 1.1670036315917969
Epoch 1320, training loss: 62.571861267089844 = 0.0881434753537178 + 10.0 * 6.2483720779418945
Epoch 1320, val loss: 1.173688292503357
Epoch 1330, training loss: 62.55430221557617 = 0.08593236654996872 + 10.0 * 6.2468366622924805
Epoch 1330, val loss: 1.1805918216705322
Epoch 1340, training loss: 62.54325866699219 = 0.08380735665559769 + 10.0 * 6.245944976806641
Epoch 1340, val loss: 1.1872872114181519
Epoch 1350, training loss: 62.57516860961914 = 0.08173684775829315 + 10.0 * 6.249342918395996
Epoch 1350, val loss: 1.1939761638641357
Epoch 1360, training loss: 62.55121994018555 = 0.07969480007886887 + 10.0 * 6.247152328491211
Epoch 1360, val loss: 1.2006458044052124
Epoch 1370, training loss: 62.5435676574707 = 0.0777168869972229 + 10.0 * 6.246584892272949
Epoch 1370, val loss: 1.2071532011032104
Epoch 1380, training loss: 62.53763198852539 = 0.07582243531942368 + 10.0 * 6.246181011199951
Epoch 1380, val loss: 1.2138696908950806
Epoch 1390, training loss: 62.54579162597656 = 0.07397827506065369 + 10.0 * 6.247181415557861
Epoch 1390, val loss: 1.2205493450164795
Epoch 1400, training loss: 62.52024841308594 = 0.07218030840158463 + 10.0 * 6.24480676651001
Epoch 1400, val loss: 1.2271941900253296
Epoch 1410, training loss: 62.538814544677734 = 0.07044079899787903 + 10.0 * 6.246837139129639
Epoch 1410, val loss: 1.2336483001708984
Epoch 1420, training loss: 62.53010940551758 = 0.06876006722450256 + 10.0 * 6.2461347579956055
Epoch 1420, val loss: 1.240154504776001
Epoch 1430, training loss: 62.52815628051758 = 0.06712081283330917 + 10.0 * 6.246103763580322
Epoch 1430, val loss: 1.2466070652008057
Epoch 1440, training loss: 62.49417495727539 = 0.06551603972911835 + 10.0 * 6.242865562438965
Epoch 1440, val loss: 1.253259301185608
Epoch 1450, training loss: 62.49994659423828 = 0.06398787349462509 + 10.0 * 6.243596076965332
Epoch 1450, val loss: 1.2597002983093262
Epoch 1460, training loss: 62.507362365722656 = 0.06250666826963425 + 10.0 * 6.244485378265381
Epoch 1460, val loss: 1.266059398651123
Epoch 1470, training loss: 62.494319915771484 = 0.06103600934147835 + 10.0 * 6.24332857131958
Epoch 1470, val loss: 1.272458553314209
Epoch 1480, training loss: 62.4984245300293 = 0.059617504477500916 + 10.0 * 6.243880748748779
Epoch 1480, val loss: 1.2788355350494385
Epoch 1490, training loss: 62.46509552001953 = 0.058252304792404175 + 10.0 * 6.240684509277344
Epoch 1490, val loss: 1.285151481628418
Epoch 1500, training loss: 62.45822525024414 = 0.056933045387268066 + 10.0 * 6.240128993988037
Epoch 1500, val loss: 1.2914091348648071
Epoch 1510, training loss: 62.488346099853516 = 0.055652473121881485 + 10.0 * 6.243269443511963
Epoch 1510, val loss: 1.2975882291793823
Epoch 1520, training loss: 62.471431732177734 = 0.054381005465984344 + 10.0 * 6.241704940795898
Epoch 1520, val loss: 1.304046630859375
Epoch 1530, training loss: 62.464595794677734 = 0.05315816402435303 + 10.0 * 6.241143703460693
Epoch 1530, val loss: 1.3098713159561157
Epoch 1540, training loss: 62.43699645996094 = 0.05197436362504959 + 10.0 * 6.238502025604248
Epoch 1540, val loss: 1.316251277923584
Epoch 1550, training loss: 62.439857482910156 = 0.05084250867366791 + 10.0 * 6.238901615142822
Epoch 1550, val loss: 1.3222901821136475
Epoch 1560, training loss: 62.464168548583984 = 0.04973630607128143 + 10.0 * 6.241443157196045
Epoch 1560, val loss: 1.3282811641693115
Epoch 1570, training loss: 62.45395278930664 = 0.04863811656832695 + 10.0 * 6.2405314445495605
Epoch 1570, val loss: 1.3344039916992188
Epoch 1580, training loss: 62.43284225463867 = 0.0475800521671772 + 10.0 * 6.238526344299316
Epoch 1580, val loss: 1.340325951576233
Epoch 1590, training loss: 62.418556213378906 = 0.046561866998672485 + 10.0 * 6.237199306488037
Epoch 1590, val loss: 1.3463858366012573
Epoch 1600, training loss: 62.461849212646484 = 0.045580074191093445 + 10.0 * 6.241626739501953
Epoch 1600, val loss: 1.3520240783691406
Epoch 1610, training loss: 62.405555725097656 = 0.044597651809453964 + 10.0 * 6.236095905303955
Epoch 1610, val loss: 1.3580982685089111
Epoch 1620, training loss: 62.40534210205078 = 0.043660618364810944 + 10.0 * 6.236168384552002
Epoch 1620, val loss: 1.3639531135559082
Epoch 1630, training loss: 62.39554977416992 = 0.042760882526636124 + 10.0 * 6.235278606414795
Epoch 1630, val loss: 1.3696620464324951
Epoch 1640, training loss: 62.427913665771484 = 0.04189227521419525 + 10.0 * 6.238602161407471
Epoch 1640, val loss: 1.3752448558807373
Epoch 1650, training loss: 62.432918548583984 = 0.04101898893713951 + 10.0 * 6.239190101623535
Epoch 1650, val loss: 1.3811326026916504
Epoch 1660, training loss: 62.42659378051758 = 0.040175292640924454 + 10.0 * 6.238641738891602
Epoch 1660, val loss: 1.3864394426345825
Epoch 1670, training loss: 62.40332794189453 = 0.039352092891931534 + 10.0 * 6.236397743225098
Epoch 1670, val loss: 1.3924784660339355
Epoch 1680, training loss: 62.386051177978516 = 0.03855482116341591 + 10.0 * 6.234749794006348
Epoch 1680, val loss: 1.3977891206741333
Epoch 1690, training loss: 62.390689849853516 = 0.03779241815209389 + 10.0 * 6.235289573669434
Epoch 1690, val loss: 1.4035106897354126
Epoch 1700, training loss: 62.407230377197266 = 0.037053320556879044 + 10.0 * 6.237017631530762
Epoch 1700, val loss: 1.408906102180481
Epoch 1710, training loss: 62.38277816772461 = 0.03630466014146805 + 10.0 * 6.234647274017334
Epoch 1710, val loss: 1.4143824577331543
Epoch 1720, training loss: 62.381996154785156 = 0.03559831529855728 + 10.0 * 6.234639644622803
Epoch 1720, val loss: 1.4199434518814087
Epoch 1730, training loss: 62.41162872314453 = 0.03490510582923889 + 10.0 * 6.237672328948975
Epoch 1730, val loss: 1.4252266883850098
Epoch 1740, training loss: 62.373104095458984 = 0.03423238918185234 + 10.0 * 6.233887195587158
Epoch 1740, val loss: 1.430488109588623
Epoch 1750, training loss: 62.349552154541016 = 0.03357432782649994 + 10.0 * 6.231597900390625
Epoch 1750, val loss: 1.4360219240188599
Epoch 1760, training loss: 62.354637145996094 = 0.03294460102915764 + 10.0 * 6.232169151306152
Epoch 1760, val loss: 1.4412152767181396
Epoch 1770, training loss: 62.418819427490234 = 0.032328907400369644 + 10.0 * 6.238648891448975
Epoch 1770, val loss: 1.4464387893676758
Epoch 1780, training loss: 62.390960693359375 = 0.0317208468914032 + 10.0 * 6.235924243927002
Epoch 1780, val loss: 1.451531171798706
Epoch 1790, training loss: 62.34257125854492 = 0.031116241589188576 + 10.0 * 6.23114538192749
Epoch 1790, val loss: 1.456964373588562
Epoch 1800, training loss: 62.331787109375 = 0.03054698556661606 + 10.0 * 6.230123996734619
Epoch 1800, val loss: 1.4619592428207397
Epoch 1810, training loss: 62.331138610839844 = 0.03000045381486416 + 10.0 * 6.230113983154297
Epoch 1810, val loss: 1.4672842025756836
Epoch 1820, training loss: 62.38190460205078 = 0.02947167307138443 + 10.0 * 6.235243320465088
Epoch 1820, val loss: 1.472304105758667
Epoch 1830, training loss: 62.339698791503906 = 0.028926275670528412 + 10.0 * 6.231077194213867
Epoch 1830, val loss: 1.4773448705673218
Epoch 1840, training loss: 62.3377799987793 = 0.02840610407292843 + 10.0 * 6.230937480926514
Epoch 1840, val loss: 1.482256293296814
Epoch 1850, training loss: 62.35606384277344 = 0.027908358722925186 + 10.0 * 6.232815742492676
Epoch 1850, val loss: 1.4871509075164795
Epoch 1860, training loss: 62.315391540527344 = 0.027405062690377235 + 10.0 * 6.228798866271973
Epoch 1860, val loss: 1.4921777248382568
Epoch 1870, training loss: 62.31623840332031 = 0.026923421770334244 + 10.0 * 6.228931427001953
Epoch 1870, val loss: 1.4969910383224487
Epoch 1880, training loss: 62.30713653564453 = 0.026465531438589096 + 10.0 * 6.228066921234131
Epoch 1880, val loss: 1.501991868019104
Epoch 1890, training loss: 62.30375671386719 = 0.026018051430583 + 10.0 * 6.227774143218994
Epoch 1890, val loss: 1.506731390953064
Epoch 1900, training loss: 62.44677734375 = 0.025592025369405746 + 10.0 * 6.2421183586120605
Epoch 1900, val loss: 1.5112171173095703
Epoch 1910, training loss: 62.363189697265625 = 0.025132952257990837 + 10.0 * 6.2338056564331055
Epoch 1910, val loss: 1.5163594484329224
Epoch 1920, training loss: 62.3032341003418 = 0.02470044605433941 + 10.0 * 6.227853298187256
Epoch 1920, val loss: 1.5208089351654053
Epoch 1930, training loss: 62.31432342529297 = 0.024291619658470154 + 10.0 * 6.229002952575684
Epoch 1930, val loss: 1.5256640911102295
Epoch 1940, training loss: 62.30854415893555 = 0.023892687633633614 + 10.0 * 6.2284650802612305
Epoch 1940, val loss: 1.5302798748016357
Epoch 1950, training loss: 62.28782653808594 = 0.023500824347138405 + 10.0 * 6.2264323234558105
Epoch 1950, val loss: 1.5349137783050537
Epoch 1960, training loss: 62.3109130859375 = 0.02312653698027134 + 10.0 * 6.228778839111328
Epoch 1960, val loss: 1.5393508672714233
Epoch 1970, training loss: 62.30268478393555 = 0.022747956216335297 + 10.0 * 6.227993965148926
Epoch 1970, val loss: 1.5438752174377441
Epoch 1980, training loss: 62.298805236816406 = 0.022375984117388725 + 10.0 * 6.227643013000488
Epoch 1980, val loss: 1.5485174655914307
Epoch 1990, training loss: 62.28217697143555 = 0.022021900862455368 + 10.0 * 6.226015567779541
Epoch 1990, val loss: 1.5529452562332153
Epoch 2000, training loss: 62.348697662353516 = 0.0216749869287014 + 10.0 * 6.232702255249023
Epoch 2000, val loss: 1.5574288368225098
Epoch 2010, training loss: 62.29526138305664 = 0.02132640965282917 + 10.0 * 6.22739315032959
Epoch 2010, val loss: 1.5614948272705078
Epoch 2020, training loss: 62.279701232910156 = 0.020985864102840424 + 10.0 * 6.225871562957764
Epoch 2020, val loss: 1.5659452676773071
Epoch 2030, training loss: 62.266510009765625 = 0.02066347561776638 + 10.0 * 6.224584579467773
Epoch 2030, val loss: 1.5703221559524536
Epoch 2040, training loss: 62.25918960571289 = 0.02035117708146572 + 10.0 * 6.223883628845215
Epoch 2040, val loss: 1.5747653245925903
Epoch 2050, training loss: 62.28355407714844 = 0.020050810649991035 + 10.0 * 6.2263503074646
Epoch 2050, val loss: 1.5790271759033203
Epoch 2060, training loss: 62.27109909057617 = 0.019740579649806023 + 10.0 * 6.225135803222656
Epoch 2060, val loss: 1.5832682847976685
Epoch 2070, training loss: 62.26446533203125 = 0.01943821832537651 + 10.0 * 6.2245025634765625
Epoch 2070, val loss: 1.5874979496002197
Epoch 2080, training loss: 62.258243560791016 = 0.019147220999002457 + 10.0 * 6.223909854888916
Epoch 2080, val loss: 1.5915604829788208
Epoch 2090, training loss: 62.257789611816406 = 0.01886828802525997 + 10.0 * 6.2238922119140625
Epoch 2090, val loss: 1.5958342552185059
Epoch 2100, training loss: 62.29010009765625 = 0.018594272434711456 + 10.0 * 6.227150917053223
Epoch 2100, val loss: 1.5997787714004517
Epoch 2110, training loss: 62.29458236694336 = 0.018313482403755188 + 10.0 * 6.227626800537109
Epoch 2110, val loss: 1.6038758754730225
Epoch 2120, training loss: 62.253662109375 = 0.018040690571069717 + 10.0 * 6.223562240600586
Epoch 2120, val loss: 1.608153223991394
Epoch 2130, training loss: 62.23890686035156 = 0.017783232033252716 + 10.0 * 6.22211217880249
Epoch 2130, val loss: 1.6121474504470825
Epoch 2140, training loss: 62.23942184448242 = 0.01753126084804535 + 10.0 * 6.222188949584961
Epoch 2140, val loss: 1.616316795349121
Epoch 2150, training loss: 62.320396423339844 = 0.017296364530920982 + 10.0 * 6.230309963226318
Epoch 2150, val loss: 1.6203668117523193
Epoch 2160, training loss: 62.25626754760742 = 0.01704028993844986 + 10.0 * 6.2239227294921875
Epoch 2160, val loss: 1.6238712072372437
Epoch 2170, training loss: 62.25761032104492 = 0.016796762123703957 + 10.0 * 6.224081516265869
Epoch 2170, val loss: 1.628096342086792
Epoch 2180, training loss: 62.254032135009766 = 0.016564099118113518 + 10.0 * 6.2237467765808105
Epoch 2180, val loss: 1.631911277770996
Epoch 2190, training loss: 62.23594665527344 = 0.0163347776979208 + 10.0 * 6.22196102142334
Epoch 2190, val loss: 1.635986566543579
Epoch 2200, training loss: 62.24079895019531 = 0.016114449128508568 + 10.0 * 6.222468376159668
Epoch 2200, val loss: 1.6398097276687622
Epoch 2210, training loss: 62.247779846191406 = 0.015895143151283264 + 10.0 * 6.223188400268555
Epoch 2210, val loss: 1.6436891555786133
Epoch 2220, training loss: 62.228057861328125 = 0.0156804658472538 + 10.0 * 6.221237659454346
Epoch 2220, val loss: 1.6475112438201904
Epoch 2230, training loss: 62.25239562988281 = 0.015475280582904816 + 10.0 * 6.223691940307617
Epoch 2230, val loss: 1.651240587234497
Epoch 2240, training loss: 62.24606704711914 = 0.015266809612512589 + 10.0 * 6.223080158233643
Epoch 2240, val loss: 1.6549078226089478
Epoch 2250, training loss: 62.22450637817383 = 0.015066389925777912 + 10.0 * 6.220943927764893
Epoch 2250, val loss: 1.6585955619812012
Epoch 2260, training loss: 62.239871978759766 = 0.014869006350636482 + 10.0 * 6.222500324249268
Epoch 2260, val loss: 1.662455439567566
Epoch 2270, training loss: 62.224884033203125 = 0.01467309519648552 + 10.0 * 6.2210211753845215
Epoch 2270, val loss: 1.6660012006759644
Epoch 2280, training loss: 62.22368240356445 = 0.014484290033578873 + 10.0 * 6.220919609069824
Epoch 2280, val loss: 1.6699457168579102
Epoch 2290, training loss: 62.21192932128906 = 0.01429654285311699 + 10.0 * 6.219763278961182
Epoch 2290, val loss: 1.673414945602417
Epoch 2300, training loss: 62.251182556152344 = 0.014120861887931824 + 10.0 * 6.223706245422363
Epoch 2300, val loss: 1.6771905422210693
Epoch 2310, training loss: 62.21194839477539 = 0.013935605064034462 + 10.0 * 6.219801425933838
Epoch 2310, val loss: 1.6808359622955322
Epoch 2320, training loss: 62.205284118652344 = 0.013760740868747234 + 10.0 * 6.219152450561523
Epoch 2320, val loss: 1.6842442750930786
Epoch 2330, training loss: 62.22472381591797 = 0.013594565913081169 + 10.0 * 6.2211127281188965
Epoch 2330, val loss: 1.6877245903015137
Epoch 2340, training loss: 62.22574996948242 = 0.01342269778251648 + 10.0 * 6.2212324142456055
Epoch 2340, val loss: 1.691347599029541
Epoch 2350, training loss: 62.20713806152344 = 0.013249438256025314 + 10.0 * 6.219388961791992
Epoch 2350, val loss: 1.6947358846664429
Epoch 2360, training loss: 62.19287109375 = 0.013090865686535835 + 10.0 * 6.217978000640869
Epoch 2360, val loss: 1.6983838081359863
Epoch 2370, training loss: 62.24631118774414 = 0.012939810752868652 + 10.0 * 6.223337173461914
Epoch 2370, val loss: 1.701582908630371
Epoch 2380, training loss: 62.19905471801758 = 0.012774759903550148 + 10.0 * 6.2186279296875
Epoch 2380, val loss: 1.705286979675293
Epoch 2390, training loss: 62.19538879394531 = 0.012624228373169899 + 10.0 * 6.218276500701904
Epoch 2390, val loss: 1.7084847688674927
Epoch 2400, training loss: 62.19588851928711 = 0.012476174160838127 + 10.0 * 6.218340873718262
Epoch 2400, val loss: 1.712082862854004
Epoch 2410, training loss: 62.19212341308594 = 0.012327941134572029 + 10.0 * 6.217979431152344
Epoch 2410, val loss: 1.715155005455017
Epoch 2420, training loss: 62.21516799926758 = 0.012188421562314034 + 10.0 * 6.220297813415527
Epoch 2420, val loss: 1.7183486223220825
Epoch 2430, training loss: 62.21668243408203 = 0.012045783922076225 + 10.0 * 6.220463752746582
Epoch 2430, val loss: 1.7217060327529907
Epoch 2440, training loss: 62.184417724609375 = 0.011901766993105412 + 10.0 * 6.217251777648926
Epoch 2440, val loss: 1.725027084350586
Epoch 2450, training loss: 62.176910400390625 = 0.011762640438973904 + 10.0 * 6.216514587402344
Epoch 2450, val loss: 1.728315830230713
Epoch 2460, training loss: 62.2315559387207 = 0.011632499285042286 + 10.0 * 6.221992492675781
Epoch 2460, val loss: 1.731692910194397
Epoch 2470, training loss: 62.18315887451172 = 0.011498255655169487 + 10.0 * 6.217165946960449
Epoch 2470, val loss: 1.7345678806304932
Epoch 2480, training loss: 62.17085266113281 = 0.011367186903953552 + 10.0 * 6.215948581695557
Epoch 2480, val loss: 1.7380974292755127
Epoch 2490, training loss: 62.213653564453125 = 0.011246846988797188 + 10.0 * 6.220240592956543
Epoch 2490, val loss: 1.7409201860427856
Epoch 2500, training loss: 62.183265686035156 = 0.011116348206996918 + 10.0 * 6.217215061187744
Epoch 2500, val loss: 1.7444173097610474
Epoch 2510, training loss: 62.17073440551758 = 0.010990265756845474 + 10.0 * 6.2159743309021
Epoch 2510, val loss: 1.746903896331787
Epoch 2520, training loss: 62.176631927490234 = 0.010873191989958286 + 10.0 * 6.216576099395752
Epoch 2520, val loss: 1.750484585762024
Epoch 2530, training loss: 62.213905334472656 = 0.010757911950349808 + 10.0 * 6.220314979553223
Epoch 2530, val loss: 1.752988576889038
Epoch 2540, training loss: 62.170555114746094 = 0.010633484460413456 + 10.0 * 6.215991973876953
Epoch 2540, val loss: 1.756474256515503
Epoch 2550, training loss: 62.15647506713867 = 0.010520598851144314 + 10.0 * 6.214595317840576
Epoch 2550, val loss: 1.7593858242034912
Epoch 2560, training loss: 62.176658630371094 = 0.010412652045488358 + 10.0 * 6.2166242599487305
Epoch 2560, val loss: 1.762182354927063
Epoch 2570, training loss: 62.18568801879883 = 0.010300299152731895 + 10.0 * 6.217538833618164
Epoch 2570, val loss: 1.7652562856674194
Epoch 2580, training loss: 62.16289138793945 = 0.01018842775374651 + 10.0 * 6.215270042419434
Epoch 2580, val loss: 1.7683216333389282
Epoch 2590, training loss: 62.19342803955078 = 0.010081825777888298 + 10.0 * 6.218334674835205
Epoch 2590, val loss: 1.7711410522460938
Epoch 2600, training loss: 62.16521072387695 = 0.009978177957236767 + 10.0 * 6.2155232429504395
Epoch 2600, val loss: 1.774031400680542
Epoch 2610, training loss: 62.15861129760742 = 0.009872599504888058 + 10.0 * 6.214873790740967
Epoch 2610, val loss: 1.7771399021148682
Epoch 2620, training loss: 62.161102294921875 = 0.009771468117833138 + 10.0 * 6.215132713317871
Epoch 2620, val loss: 1.7800339460372925
Epoch 2630, training loss: 62.1568603515625 = 0.009673057124018669 + 10.0 * 6.214718818664551
Epoch 2630, val loss: 1.782862901687622
Epoch 2640, training loss: 62.19110107421875 = 0.009574081748723984 + 10.0 * 6.2181525230407715
Epoch 2640, val loss: 1.7855390310287476
Epoch 2650, training loss: 62.168670654296875 = 0.009477117098867893 + 10.0 * 6.215919494628906
Epoch 2650, val loss: 1.7883131504058838
Epoch 2660, training loss: 62.14712905883789 = 0.009380879811942577 + 10.0 * 6.213774681091309
Epoch 2660, val loss: 1.7910711765289307
Epoch 2670, training loss: 62.14350128173828 = 0.009289012290537357 + 10.0 * 6.21342134475708
Epoch 2670, val loss: 1.7937289476394653
Epoch 2680, training loss: 62.17542266845703 = 0.009200723841786385 + 10.0 * 6.216622352600098
Epoch 2680, val loss: 1.796406865119934
Epoch 2690, training loss: 62.140506744384766 = 0.009105592034757137 + 10.0 * 6.21314001083374
Epoch 2690, val loss: 1.799342155456543
Epoch 2700, training loss: 62.1541748046875 = 0.009017514996230602 + 10.0 * 6.214515686035156
Epoch 2700, val loss: 1.8022444248199463
Epoch 2710, training loss: 62.145320892333984 = 0.00892939418554306 + 10.0 * 6.213639259338379
Epoch 2710, val loss: 1.804725170135498
Epoch 2720, training loss: 62.155452728271484 = 0.008844620548188686 + 10.0 * 6.21466064453125
Epoch 2720, val loss: 1.807655692100525
Epoch 2730, training loss: 62.1590461730957 = 0.008758516982197762 + 10.0 * 6.215028762817383
Epoch 2730, val loss: 1.8102611303329468
Epoch 2740, training loss: 62.13603591918945 = 0.008672954514622688 + 10.0 * 6.212736129760742
Epoch 2740, val loss: 1.8125718832015991
Epoch 2750, training loss: 62.1270866394043 = 0.008591375313699245 + 10.0 * 6.211849689483643
Epoch 2750, val loss: 1.8155426979064941
Epoch 2760, training loss: 62.140377044677734 = 0.008514666929841042 + 10.0 * 6.213186264038086
Epoch 2760, val loss: 1.8178801536560059
Epoch 2770, training loss: 62.14864730834961 = 0.008434878662228584 + 10.0 * 6.2140212059021
Epoch 2770, val loss: 1.8205435276031494
Epoch 2780, training loss: 62.133827209472656 = 0.008354241028428078 + 10.0 * 6.212547302246094
Epoch 2780, val loss: 1.823164463043213
Epoch 2790, training loss: 62.13909912109375 = 0.008278929628431797 + 10.0 * 6.213082313537598
Epoch 2790, val loss: 1.8256210088729858
Epoch 2800, training loss: 62.13779067993164 = 0.008202795870602131 + 10.0 * 6.212958812713623
Epoch 2800, val loss: 1.8282500505447388
Epoch 2810, training loss: 62.213008880615234 = 0.008130619302392006 + 10.0 * 6.22048807144165
Epoch 2810, val loss: 1.830517053604126
Epoch 2820, training loss: 62.159908294677734 = 0.008053403347730637 + 10.0 * 6.215185642242432
Epoch 2820, val loss: 1.8330987691879272
Epoch 2830, training loss: 62.120361328125 = 0.007973359897732735 + 10.0 * 6.211238861083984
Epoch 2830, val loss: 1.8355640172958374
Epoch 2840, training loss: 62.10633087158203 = 0.00790334865450859 + 10.0 * 6.209842681884766
Epoch 2840, val loss: 1.838194489479065
Epoch 2850, training loss: 62.10334777832031 = 0.007835661992430687 + 10.0 * 6.2095513343811035
Epoch 2850, val loss: 1.8408228158950806
Epoch 2860, training loss: 62.17728805541992 = 0.007769827730953693 + 10.0 * 6.216951847076416
Epoch 2860, val loss: 1.8433403968811035
Epoch 2870, training loss: 62.11209487915039 = 0.007698569446802139 + 10.0 * 6.210439682006836
Epoch 2870, val loss: 1.84523606300354
Epoch 2880, training loss: 62.1038818359375 = 0.00762901920825243 + 10.0 * 6.209625244140625
Epoch 2880, val loss: 1.847807765007019
Epoch 2890, training loss: 62.10389709472656 = 0.007563799154013395 + 10.0 * 6.2096333503723145
Epoch 2890, val loss: 1.8503528833389282
Epoch 2900, training loss: 62.129425048828125 = 0.007501488551497459 + 10.0 * 6.212192535400391
Epoch 2900, val loss: 1.8526448011398315
Epoch 2910, training loss: 62.125736236572266 = 0.0074357227422297 + 10.0 * 6.211830139160156
Epoch 2910, val loss: 1.855076551437378
Epoch 2920, training loss: 62.121578216552734 = 0.007374496199190617 + 10.0 * 6.21142053604126
Epoch 2920, val loss: 1.857155680656433
Epoch 2930, training loss: 62.10865020751953 = 0.007309964392334223 + 10.0 * 6.210134029388428
Epoch 2930, val loss: 1.8596524000167847
Epoch 2940, training loss: 62.1088752746582 = 0.0072475397028028965 + 10.0 * 6.21016263961792
Epoch 2940, val loss: 1.8621277809143066
Epoch 2950, training loss: 62.115577697753906 = 0.0071900333277881145 + 10.0 * 6.210838794708252
Epoch 2950, val loss: 1.8642123937606812
Epoch 2960, training loss: 62.141929626464844 = 0.007127452176064253 + 10.0 * 6.213480472564697
Epoch 2960, val loss: 1.8664592504501343
Epoch 2970, training loss: 62.104454040527344 = 0.007064748089760542 + 10.0 * 6.209738731384277
Epoch 2970, val loss: 1.8684279918670654
Epoch 2980, training loss: 62.09086227416992 = 0.007006800267845392 + 10.0 * 6.208385467529297
Epoch 2980, val loss: 1.8710157871246338
Epoch 2990, training loss: 62.085845947265625 = 0.006951263640075922 + 10.0 * 6.207889556884766
Epoch 2990, val loss: 1.873170256614685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8091723774380601
The final CL Acc:0.71235, 0.03148, The final GNN Acc:0.80812, 0.00228
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13224])
remove edge: torch.Size([2, 8024])
updated graph: torch.Size([2, 10692])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.9146957397461 = 1.946357250213623 + 10.0 * 8.596834182739258
Epoch 0, val loss: 1.9543086290359497
Epoch 10, training loss: 87.89801788330078 = 1.9361190795898438 + 10.0 * 8.596189498901367
Epoch 10, val loss: 1.9436125755310059
Epoch 20, training loss: 87.83955383300781 = 1.9231843948364258 + 10.0 * 8.591636657714844
Epoch 20, val loss: 1.9299522638320923
Epoch 30, training loss: 87.52362060546875 = 1.906217098236084 + 10.0 * 8.561739921569824
Epoch 30, val loss: 1.9119489192962646
Epoch 40, training loss: 85.7537612915039 = 1.8857051134109497 + 10.0 * 8.386805534362793
Epoch 40, val loss: 1.8909422159194946
Epoch 50, training loss: 78.1904296875 = 1.8630967140197754 + 10.0 * 7.63273286819458
Epoch 50, val loss: 1.8676496744155884
Epoch 60, training loss: 74.33016967773438 = 1.8442367315292358 + 10.0 * 7.248592853546143
Epoch 60, val loss: 1.8496540784835815
Epoch 70, training loss: 72.50475311279297 = 1.828962802886963 + 10.0 * 7.0675787925720215
Epoch 70, val loss: 1.835292935371399
Epoch 80, training loss: 71.64018249511719 = 1.8145262002944946 + 10.0 * 6.982565402984619
Epoch 80, val loss: 1.8209409713745117
Epoch 90, training loss: 71.0426254272461 = 1.801345705986023 + 10.0 * 6.92412805557251
Epoch 90, val loss: 1.8079750537872314
Epoch 100, training loss: 70.4441146850586 = 1.7890406847000122 + 10.0 * 6.86550760269165
Epoch 100, val loss: 1.7962439060211182
Epoch 110, training loss: 69.75055694580078 = 1.7782447338104248 + 10.0 * 6.797231197357178
Epoch 110, val loss: 1.7863693237304688
Epoch 120, training loss: 69.11681365966797 = 1.7693889141082764 + 10.0 * 6.734742641448975
Epoch 120, val loss: 1.7782297134399414
Epoch 130, training loss: 68.62915802001953 = 1.760404348373413 + 10.0 * 6.686875343322754
Epoch 130, val loss: 1.7700144052505493
Epoch 140, training loss: 68.17373657226562 = 1.7502468824386597 + 10.0 * 6.6423492431640625
Epoch 140, val loss: 1.761108636856079
Epoch 150, training loss: 67.8360824584961 = 1.7391856908798218 + 10.0 * 6.609689235687256
Epoch 150, val loss: 1.7515240907669067
Epoch 160, training loss: 67.59408569335938 = 1.7265141010284424 + 10.0 * 6.586756706237793
Epoch 160, val loss: 1.7405595779418945
Epoch 170, training loss: 67.35816955566406 = 1.7124285697937012 + 10.0 * 6.564574718475342
Epoch 170, val loss: 1.728255271911621
Epoch 180, training loss: 67.13999938964844 = 1.6975595951080322 + 10.0 * 6.544243812561035
Epoch 180, val loss: 1.7156620025634766
Epoch 190, training loss: 66.9696044921875 = 1.6817814111709595 + 10.0 * 6.528782367706299
Epoch 190, val loss: 1.7022337913513184
Epoch 200, training loss: 66.73992156982422 = 1.6645169258117676 + 10.0 * 6.507540225982666
Epoch 200, val loss: 1.687861680984497
Epoch 210, training loss: 66.55502319335938 = 1.6458486318588257 + 10.0 * 6.490917205810547
Epoch 210, val loss: 1.6721867322921753
Epoch 220, training loss: 66.3980712890625 = 1.6255652904510498 + 10.0 * 6.477250576019287
Epoch 220, val loss: 1.6550627946853638
Epoch 230, training loss: 66.29300689697266 = 1.603110432624817 + 10.0 * 6.468989372253418
Epoch 230, val loss: 1.6363825798034668
Epoch 240, training loss: 66.1308364868164 = 1.579362392425537 + 10.0 * 6.4551472663879395
Epoch 240, val loss: 1.616317868232727
Epoch 250, training loss: 66.00150299072266 = 1.5541181564331055 + 10.0 * 6.444738388061523
Epoch 250, val loss: 1.5953786373138428
Epoch 260, training loss: 65.88150024414062 = 1.5276540517807007 + 10.0 * 6.435384273529053
Epoch 260, val loss: 1.573644995689392
Epoch 270, training loss: 65.7599868774414 = 1.500239372253418 + 10.0 * 6.425975322723389
Epoch 270, val loss: 1.551303744316101
Epoch 280, training loss: 65.64507293701172 = 1.4720906019210815 + 10.0 * 6.417297840118408
Epoch 280, val loss: 1.528638482093811
Epoch 290, training loss: 65.5363998413086 = 1.4432209730148315 + 10.0 * 6.409317493438721
Epoch 290, val loss: 1.5054997205734253
Epoch 300, training loss: 65.44514465332031 = 1.4138635396957397 + 10.0 * 6.403128147125244
Epoch 300, val loss: 1.4825185537338257
Epoch 310, training loss: 65.34571838378906 = 1.3843036890029907 + 10.0 * 6.396141529083252
Epoch 310, val loss: 1.459476113319397
Epoch 320, training loss: 65.24506378173828 = 1.3544437885284424 + 10.0 * 6.38906192779541
Epoch 320, val loss: 1.4367647171020508
Epoch 330, training loss: 65.2303695678711 = 1.3246512413024902 + 10.0 * 6.390571594238281
Epoch 330, val loss: 1.4140697717666626
Epoch 340, training loss: 65.0824203491211 = 1.2945005893707275 + 10.0 * 6.3787922859191895
Epoch 340, val loss: 1.3919450044631958
Epoch 350, training loss: 64.99232482910156 = 1.2645231485366821 + 10.0 * 6.372779846191406
Epoch 350, val loss: 1.3702113628387451
Epoch 360, training loss: 64.9451675415039 = 1.2346749305725098 + 10.0 * 6.371049404144287
Epoch 360, val loss: 1.3489269018173218
Epoch 370, training loss: 64.83502197265625 = 1.2048009634017944 + 10.0 * 6.363022327423096
Epoch 370, val loss: 1.3277274370193481
Epoch 380, training loss: 64.76327514648438 = 1.1749802827835083 + 10.0 * 6.358829021453857
Epoch 380, val loss: 1.3070080280303955
Epoch 390, training loss: 64.70398712158203 = 1.145357370376587 + 10.0 * 6.355863094329834
Epoch 390, val loss: 1.2867419719696045
Epoch 400, training loss: 64.66710662841797 = 1.1155955791473389 + 10.0 * 6.355151176452637
Epoch 400, val loss: 1.266326904296875
Epoch 410, training loss: 64.56111907958984 = 1.0862013101577759 + 10.0 * 6.34749174118042
Epoch 410, val loss: 1.246733546257019
Epoch 420, training loss: 64.50337219238281 = 1.0571621656417847 + 10.0 * 6.344621181488037
Epoch 420, val loss: 1.227532148361206
Epoch 430, training loss: 64.47140502929688 = 1.0283945798873901 + 10.0 * 6.344300746917725
Epoch 430, val loss: 1.2088192701339722
Epoch 440, training loss: 64.39039611816406 = 1.0002304315567017 + 10.0 * 6.339016914367676
Epoch 440, val loss: 1.1902415752410889
Epoch 450, training loss: 64.32575988769531 = 0.9723948240280151 + 10.0 * 6.335336685180664
Epoch 450, val loss: 1.1727068424224854
Epoch 460, training loss: 64.32244873046875 = 0.9452611804008484 + 10.0 * 6.337718963623047
Epoch 460, val loss: 1.1555676460266113
Epoch 470, training loss: 64.23487091064453 = 0.9187479019165039 + 10.0 * 6.331612586975098
Epoch 470, val loss: 1.139337420463562
Epoch 480, training loss: 64.1629867553711 = 0.8932003974914551 + 10.0 * 6.3269782066345215
Epoch 480, val loss: 1.1239906549453735
Epoch 490, training loss: 64.11473846435547 = 0.8684576153755188 + 10.0 * 6.3246283531188965
Epoch 490, val loss: 1.1095926761627197
Epoch 500, training loss: 64.13432312011719 = 0.8443719744682312 + 10.0 * 6.3289947509765625
Epoch 500, val loss: 1.0959832668304443
Epoch 510, training loss: 64.04344940185547 = 0.8212125301361084 + 10.0 * 6.322223663330078
Epoch 510, val loss: 1.0831069946289062
Epoch 520, training loss: 63.98366928100586 = 0.7990453839302063 + 10.0 * 6.318462371826172
Epoch 520, val loss: 1.071315050125122
Epoch 530, training loss: 63.931480407714844 = 0.7777339816093445 + 10.0 * 6.315374851226807
Epoch 530, val loss: 1.060454249382019
Epoch 540, training loss: 63.89426803588867 = 0.757234513759613 + 10.0 * 6.313703536987305
Epoch 540, val loss: 1.0504201650619507
Epoch 550, training loss: 63.85578155517578 = 0.7373620271682739 + 10.0 * 6.31184196472168
Epoch 550, val loss: 1.041139841079712
Epoch 560, training loss: 63.824981689453125 = 0.7183376550674438 + 10.0 * 6.310664176940918
Epoch 560, val loss: 1.0325677394866943
Epoch 570, training loss: 63.81275939941406 = 0.7000492215156555 + 10.0 * 6.311270713806152
Epoch 570, val loss: 1.0248233079910278
Epoch 580, training loss: 63.75414276123047 = 0.6823889017105103 + 10.0 * 6.307175636291504
Epoch 580, val loss: 1.0177805423736572
Epoch 590, training loss: 63.70755386352539 = 0.6655047535896301 + 10.0 * 6.304204940795898
Epoch 590, val loss: 1.011487364768982
Epoch 600, training loss: 63.67251968383789 = 0.6491965055465698 + 10.0 * 6.302332401275635
Epoch 600, val loss: 1.0056886672973633
Epoch 610, training loss: 63.68714141845703 = 0.6333068609237671 + 10.0 * 6.305383205413818
Epoch 610, val loss: 1.000243902206421
Epoch 620, training loss: 63.6316032409668 = 0.618022620677948 + 10.0 * 6.301358222961426
Epoch 620, val loss: 0.9951947331428528
Epoch 630, training loss: 63.57929229736328 = 0.6032505035400391 + 10.0 * 6.297604560852051
Epoch 630, val loss: 0.9907014966011047
Epoch 640, training loss: 63.54401779174805 = 0.5889655351638794 + 10.0 * 6.295505046844482
Epoch 640, val loss: 0.986645519733429
Epoch 650, training loss: 63.5775032043457 = 0.5751051306724548 + 10.0 * 6.3002400398254395
Epoch 650, val loss: 0.9828265309333801
Epoch 660, training loss: 63.512237548828125 = 0.5615086555480957 + 10.0 * 6.29507303237915
Epoch 660, val loss: 0.979464590549469
Epoch 670, training loss: 63.51640701293945 = 0.5484179854393005 + 10.0 * 6.296799182891846
Epoch 670, val loss: 0.9762548208236694
Epoch 680, training loss: 63.460147857666016 = 0.5354812741279602 + 10.0 * 6.292466640472412
Epoch 680, val loss: 0.9733479619026184
Epoch 690, training loss: 63.41123580932617 = 0.5231078267097473 + 10.0 * 6.288812637329102
Epoch 690, val loss: 0.9707926511764526
Epoch 700, training loss: 63.382080078125 = 0.5110090374946594 + 10.0 * 6.287106990814209
Epoch 700, val loss: 0.9685400128364563
Epoch 710, training loss: 63.374053955078125 = 0.4991835951805115 + 10.0 * 6.287487030029297
Epoch 710, val loss: 0.9666022658348083
Epoch 720, training loss: 63.345436096191406 = 0.48755621910095215 + 10.0 * 6.285788059234619
Epoch 720, val loss: 0.9645140767097473
Epoch 730, training loss: 63.31284713745117 = 0.4762412905693054 + 10.0 * 6.283660411834717
Epoch 730, val loss: 0.9629995822906494
Epoch 740, training loss: 63.28815460205078 = 0.46523982286453247 + 10.0 * 6.282291412353516
Epoch 740, val loss: 0.9617058634757996
Epoch 750, training loss: 63.264930725097656 = 0.45449867844581604 + 10.0 * 6.28104305267334
Epoch 750, val loss: 0.9605751037597656
Epoch 760, training loss: 63.294368743896484 = 0.4439126253128052 + 10.0 * 6.285045623779297
Epoch 760, val loss: 0.9595625996589661
Epoch 770, training loss: 63.23800277709961 = 0.4334895610809326 + 10.0 * 6.28045129776001
Epoch 770, val loss: 0.958732545375824
Epoch 780, training loss: 63.20665740966797 = 0.4233722388744354 + 10.0 * 6.278328895568848
Epoch 780, val loss: 0.9581670761108398
Epoch 790, training loss: 63.24831771850586 = 0.41345858573913574 + 10.0 * 6.2834858894348145
Epoch 790, val loss: 0.9576928019523621
Epoch 800, training loss: 63.163177490234375 = 0.40364107489585876 + 10.0 * 6.275953769683838
Epoch 800, val loss: 0.9574263095855713
Epoch 810, training loss: 63.13424301147461 = 0.39412206411361694 + 10.0 * 6.274012088775635
Epoch 810, val loss: 0.957443118095398
Epoch 820, training loss: 63.11951446533203 = 0.38478147983551025 + 10.0 * 6.273473262786865
Epoch 820, val loss: 0.9575700759887695
Epoch 830, training loss: 63.13523483276367 = 0.3755822479724884 + 10.0 * 6.275965213775635
Epoch 830, val loss: 0.9577867388725281
Epoch 840, training loss: 63.07768249511719 = 0.36645492911338806 + 10.0 * 6.271122932434082
Epoch 840, val loss: 0.9581406116485596
Epoch 850, training loss: 63.064945220947266 = 0.35754457116127014 + 10.0 * 6.270740032196045
Epoch 850, val loss: 0.9586891531944275
Epoch 860, training loss: 63.064300537109375 = 0.34876778721809387 + 10.0 * 6.2715535163879395
Epoch 860, val loss: 0.9593737721443176
Epoch 870, training loss: 63.05094909667969 = 0.3401138186454773 + 10.0 * 6.271083354949951
Epoch 870, val loss: 0.9602398872375488
Epoch 880, training loss: 63.01317596435547 = 0.33148279786109924 + 10.0 * 6.268169403076172
Epoch 880, val loss: 0.9610704183578491
Epoch 890, training loss: 62.996768951416016 = 0.3230677545070648 + 10.0 * 6.267370223999023
Epoch 890, val loss: 0.9621614813804626
Epoch 900, training loss: 62.993125915527344 = 0.31478118896484375 + 10.0 * 6.267834663391113
Epoch 900, val loss: 0.9634135961532593
Epoch 910, training loss: 62.971004486083984 = 0.30655142664909363 + 10.0 * 6.266445159912109
Epoch 910, val loss: 0.9646055102348328
Epoch 920, training loss: 62.94377136230469 = 0.29838424921035767 + 10.0 * 6.264538764953613
Epoch 920, val loss: 0.9659988880157471
Epoch 930, training loss: 62.92774963378906 = 0.2903388738632202 + 10.0 * 6.2637410163879395
Epoch 930, val loss: 0.9674239754676819
Epoch 940, training loss: 62.952476501464844 = 0.28240901231765747 + 10.0 * 6.267006874084473
Epoch 940, val loss: 0.9690331220626831
Epoch 950, training loss: 62.918617248535156 = 0.27455174922943115 + 10.0 * 6.264406681060791
Epoch 950, val loss: 0.9707364439964294
Epoch 960, training loss: 62.8795280456543 = 0.2667624056339264 + 10.0 * 6.261276721954346
Epoch 960, val loss: 0.97242671251297
Epoch 970, training loss: 62.8618049621582 = 0.2591308057308197 + 10.0 * 6.26026725769043
Epoch 970, val loss: 0.9743765592575073
Epoch 980, training loss: 62.87984848022461 = 0.2515825927257538 + 10.0 * 6.262826442718506
Epoch 980, val loss: 0.9763415455818176
Epoch 990, training loss: 62.859046936035156 = 0.2441195249557495 + 10.0 * 6.261492729187012
Epoch 990, val loss: 0.9784896373748779
Epoch 1000, training loss: 62.82813262939453 = 0.2367759644985199 + 10.0 * 6.259135723114014
Epoch 1000, val loss: 0.9807185530662537
Epoch 1010, training loss: 62.80635070800781 = 0.22957777976989746 + 10.0 * 6.2576775550842285
Epoch 1010, val loss: 0.9831356406211853
Epoch 1020, training loss: 62.797950744628906 = 0.22253191471099854 + 10.0 * 6.257542133331299
Epoch 1020, val loss: 0.9857157468795776
Epoch 1030, training loss: 62.80113220214844 = 0.21559108793735504 + 10.0 * 6.258553981781006
Epoch 1030, val loss: 0.9883387684822083
Epoch 1040, training loss: 62.76214599609375 = 0.20880286395549774 + 10.0 * 6.255334377288818
Epoch 1040, val loss: 0.9911159873008728
Epoch 1050, training loss: 62.753578186035156 = 0.20217296481132507 + 10.0 * 6.255140781402588
Epoch 1050, val loss: 0.9940297603607178
Epoch 1060, training loss: 62.731178283691406 = 0.19574108719825745 + 10.0 * 6.253543853759766
Epoch 1060, val loss: 0.9971842169761658
Epoch 1070, training loss: 62.771175384521484 = 0.1894616335630417 + 10.0 * 6.258171558380127
Epoch 1070, val loss: 1.0004794597625732
Epoch 1080, training loss: 62.728851318359375 = 0.1832837462425232 + 10.0 * 6.254556655883789
Epoch 1080, val loss: 1.0036108493804932
Epoch 1090, training loss: 62.7427978515625 = 0.1773213893175125 + 10.0 * 6.256547451019287
Epoch 1090, val loss: 1.007306456565857
Epoch 1100, training loss: 62.680702209472656 = 0.1714855432510376 + 10.0 * 6.250921726226807
Epoch 1100, val loss: 1.0106900930404663
Epoch 1110, training loss: 62.67835235595703 = 0.16587916016578674 + 10.0 * 6.251247406005859
Epoch 1110, val loss: 1.0145149230957031
Epoch 1120, training loss: 62.69404220581055 = 0.1604517102241516 + 10.0 * 6.253359317779541
Epoch 1120, val loss: 1.0185315608978271
Epoch 1130, training loss: 62.65685272216797 = 0.15514357388019562 + 10.0 * 6.250170707702637
Epoch 1130, val loss: 1.0225932598114014
Epoch 1140, training loss: 62.63535690307617 = 0.15005731582641602 + 10.0 * 6.24852991104126
Epoch 1140, val loss: 1.0268452167510986
Epoch 1150, training loss: 62.65465545654297 = 0.14513647556304932 + 10.0 * 6.250951766967773
Epoch 1150, val loss: 1.0311660766601562
Epoch 1160, training loss: 62.652164459228516 = 0.14033955335617065 + 10.0 * 6.251182556152344
Epoch 1160, val loss: 1.0356616973876953
Epoch 1170, training loss: 62.6143684387207 = 0.1357155293226242 + 10.0 * 6.247865200042725
Epoch 1170, val loss: 1.0400068759918213
Epoch 1180, training loss: 62.594234466552734 = 0.13126982748508453 + 10.0 * 6.246296405792236
Epoch 1180, val loss: 1.0445624589920044
Epoch 1190, training loss: 62.58348846435547 = 0.1270054131746292 + 10.0 * 6.245648384094238
Epoch 1190, val loss: 1.0492873191833496
Epoch 1200, training loss: 62.601158142089844 = 0.1228942722082138 + 10.0 * 6.24782657623291
Epoch 1200, val loss: 1.0539010763168335
Epoch 1210, training loss: 62.569732666015625 = 0.11890055984258652 + 10.0 * 6.245083332061768
Epoch 1210, val loss: 1.058851718902588
Epoch 1220, training loss: 62.60696792602539 = 0.11505824327468872 + 10.0 * 6.249190807342529
Epoch 1220, val loss: 1.0634517669677734
Epoch 1230, training loss: 62.552024841308594 = 0.11135921627283096 + 10.0 * 6.2440667152404785
Epoch 1230, val loss: 1.0686595439910889
Epoch 1240, training loss: 62.546241760253906 = 0.10780555009841919 + 10.0 * 6.2438435554504395
Epoch 1240, val loss: 1.073708415031433
Epoch 1250, training loss: 62.532676696777344 = 0.10441310703754425 + 10.0 * 6.242826461791992
Epoch 1250, val loss: 1.0788847208023071
Epoch 1260, training loss: 62.582115173339844 = 0.10114295780658722 + 10.0 * 6.2480974197387695
Epoch 1260, val loss: 1.0838086605072021
Epoch 1270, training loss: 62.533660888671875 = 0.09797940403223038 + 10.0 * 6.243567943572998
Epoch 1270, val loss: 1.0894373655319214
Epoch 1280, training loss: 62.516605377197266 = 0.09493619948625565 + 10.0 * 6.242166996002197
Epoch 1280, val loss: 1.094464659690857
Epoch 1290, training loss: 62.528289794921875 = 0.09203276038169861 + 10.0 * 6.243625640869141
Epoch 1290, val loss: 1.099908709526062
Epoch 1300, training loss: 62.4964599609375 = 0.08920251578092575 + 10.0 * 6.240725517272949
Epoch 1300, val loss: 1.1051051616668701
Epoch 1310, training loss: 62.490447998046875 = 0.08650036156177521 + 10.0 * 6.240394592285156
Epoch 1310, val loss: 1.1104658842086792
Epoch 1320, training loss: 62.505523681640625 = 0.08389569818973541 + 10.0 * 6.242162704467773
Epoch 1320, val loss: 1.1156065464019775
Epoch 1330, training loss: 62.47353744506836 = 0.08139284700155258 + 10.0 * 6.2392144203186035
Epoch 1330, val loss: 1.1211544275283813
Epoch 1340, training loss: 62.47731018066406 = 0.07899027317762375 + 10.0 * 6.239831924438477
Epoch 1340, val loss: 1.1266186237335205
Epoch 1350, training loss: 62.48255920410156 = 0.07666395604610443 + 10.0 * 6.240589618682861
Epoch 1350, val loss: 1.1317270994186401
Epoch 1360, training loss: 62.47596740722656 = 0.07441951334476471 + 10.0 * 6.24015474319458
Epoch 1360, val loss: 1.1373313665390015
Epoch 1370, training loss: 62.454681396484375 = 0.07225912809371948 + 10.0 * 6.238242149353027
Epoch 1370, val loss: 1.1426756381988525
Epoch 1380, training loss: 62.4516487121582 = 0.07018700242042542 + 10.0 * 6.2381463050842285
Epoch 1380, val loss: 1.1482985019683838
Epoch 1390, training loss: 62.48347854614258 = 0.06818594038486481 + 10.0 * 6.2415289878845215
Epoch 1390, val loss: 1.1536427736282349
Epoch 1400, training loss: 62.43585205078125 = 0.0662366971373558 + 10.0 * 6.236961364746094
Epoch 1400, val loss: 1.1588776111602783
Epoch 1410, training loss: 62.42731857299805 = 0.06437621265649796 + 10.0 * 6.236294269561768
Epoch 1410, val loss: 1.1641184091567993
Epoch 1420, training loss: 62.41508483886719 = 0.06259522587060928 + 10.0 * 6.235249042510986
Epoch 1420, val loss: 1.16969633102417
Epoch 1430, training loss: 62.42774200439453 = 0.06088244915008545 + 10.0 * 6.236685752868652
Epoch 1430, val loss: 1.1751899719238281
Epoch 1440, training loss: 62.42289733886719 = 0.0592016838490963 + 10.0 * 6.236369609832764
Epoch 1440, val loss: 1.180281400680542
Epoch 1450, training loss: 62.40895080566406 = 0.05757026746869087 + 10.0 * 6.235137939453125
Epoch 1450, val loss: 1.1858136653900146
Epoch 1460, training loss: 62.404991149902344 = 0.05601629242300987 + 10.0 * 6.234897613525391
Epoch 1460, val loss: 1.1906523704528809
Epoch 1470, training loss: 62.39624786376953 = 0.05453528091311455 + 10.0 * 6.234171390533447
Epoch 1470, val loss: 1.1962530612945557
Epoch 1480, training loss: 62.4228630065918 = 0.053102824836969376 + 10.0 * 6.23697566986084
Epoch 1480, val loss: 1.2016433477401733
Epoch 1490, training loss: 62.39272689819336 = 0.051690585911273956 + 10.0 * 6.234103679656982
Epoch 1490, val loss: 1.2063385248184204
Epoch 1500, training loss: 62.39093017578125 = 0.05035516247153282 + 10.0 * 6.234057426452637
Epoch 1500, val loss: 1.2117851972579956
Epoch 1510, training loss: 62.37975311279297 = 0.049046844244003296 + 10.0 * 6.2330708503723145
Epoch 1510, val loss: 1.2167904376983643
Epoch 1520, training loss: 62.431610107421875 = 0.047790881246328354 + 10.0 * 6.238381862640381
Epoch 1520, val loss: 1.2220442295074463
Epoch 1530, training loss: 62.3748664855957 = 0.046561408787965775 + 10.0 * 6.23283052444458
Epoch 1530, val loss: 1.2266889810562134
Epoch 1540, training loss: 62.35987091064453 = 0.045388851314783096 + 10.0 * 6.231448173522949
Epoch 1540, val loss: 1.2319430112838745
Epoch 1550, training loss: 62.35024642944336 = 0.04426354169845581 + 10.0 * 6.230598449707031
Epoch 1550, val loss: 1.236950397491455
Epoch 1560, training loss: 62.34572982788086 = 0.043176911771297455 + 10.0 * 6.230255126953125
Epoch 1560, val loss: 1.2419838905334473
Epoch 1570, training loss: 62.39285659790039 = 0.042126547545194626 + 10.0 * 6.235073089599609
Epoch 1570, val loss: 1.246767520904541
Epoch 1580, training loss: 62.36865234375 = 0.04109426960349083 + 10.0 * 6.232755661010742
Epoch 1580, val loss: 1.2519810199737549
Epoch 1590, training loss: 62.352294921875 = 0.04009392485022545 + 10.0 * 6.231220245361328
Epoch 1590, val loss: 1.256517767906189
Epoch 1600, training loss: 62.344627380371094 = 0.03913577273488045 + 10.0 * 6.230549335479736
Epoch 1600, val loss: 1.2612923383712769
Epoch 1610, training loss: 62.34357833862305 = 0.038208626210689545 + 10.0 * 6.230536937713623
Epoch 1610, val loss: 1.2661492824554443
Epoch 1620, training loss: 62.331600189208984 = 0.037312839180231094 + 10.0 * 6.229428768157959
Epoch 1620, val loss: 1.2713042497634888
Epoch 1630, training loss: 62.3349494934082 = 0.036448560655117035 + 10.0 * 6.229849815368652
Epoch 1630, val loss: 1.2761112451553345
Epoch 1640, training loss: 62.338558197021484 = 0.0356026329100132 + 10.0 * 6.230295658111572
Epoch 1640, val loss: 1.2805339097976685
Epoch 1650, training loss: 62.31303787231445 = 0.034779489040374756 + 10.0 * 6.227826118469238
Epoch 1650, val loss: 1.2850205898284912
Epoch 1660, training loss: 62.30542755126953 = 0.03398891165852547 + 10.0 * 6.22714376449585
Epoch 1660, val loss: 1.2895525693893433
Epoch 1670, training loss: 62.33479690551758 = 0.033230010420084 + 10.0 * 6.230156898498535
Epoch 1670, val loss: 1.2940484285354614
Epoch 1680, training loss: 62.302574157714844 = 0.03247923403978348 + 10.0 * 6.2270097732543945
Epoch 1680, val loss: 1.2987194061279297
Epoch 1690, training loss: 62.30241012573242 = 0.03175527602434158 + 10.0 * 6.227065563201904
Epoch 1690, val loss: 1.303373098373413
Epoch 1700, training loss: 62.308204650878906 = 0.031067609786987305 + 10.0 * 6.227713584899902
Epoch 1700, val loss: 1.3079928159713745
Epoch 1710, training loss: 62.313385009765625 = 0.030394012108445168 + 10.0 * 6.228299140930176
Epoch 1710, val loss: 1.312289834022522
Epoch 1720, training loss: 62.30960464477539 = 0.029735473915934563 + 10.0 * 6.227986812591553
Epoch 1720, val loss: 1.3165395259857178
Epoch 1730, training loss: 62.28193283081055 = 0.02909444458782673 + 10.0 * 6.225283622741699
Epoch 1730, val loss: 1.3208352327346802
Epoch 1740, training loss: 62.27983474731445 = 0.028476174920797348 + 10.0 * 6.225135803222656
Epoch 1740, val loss: 1.3251330852508545
Epoch 1750, training loss: 62.28939437866211 = 0.027880918234586716 + 10.0 * 6.226151466369629
Epoch 1750, val loss: 1.3292022943496704
Epoch 1760, training loss: 62.307071685791016 = 0.027302848175168037 + 10.0 * 6.2279767990112305
Epoch 1760, val loss: 1.3335075378417969
Epoch 1770, training loss: 62.281455993652344 = 0.026737386360764503 + 10.0 * 6.2254719734191895
Epoch 1770, val loss: 1.3376795053482056
Epoch 1780, training loss: 62.27077865600586 = 0.026188114657998085 + 10.0 * 6.224459171295166
Epoch 1780, val loss: 1.3418713808059692
Epoch 1790, training loss: 62.32350540161133 = 0.02566920779645443 + 10.0 * 6.229783535003662
Epoch 1790, val loss: 1.3465054035186768
Epoch 1800, training loss: 62.2909049987793 = 0.025135880336165428 + 10.0 * 6.226576805114746
Epoch 1800, val loss: 1.349648356437683
Epoch 1810, training loss: 62.26445007324219 = 0.024630701169371605 + 10.0 * 6.223981857299805
Epoch 1810, val loss: 1.35378098487854
Epoch 1820, training loss: 62.252567291259766 = 0.024144688621163368 + 10.0 * 6.222842216491699
Epoch 1820, val loss: 1.3577545881271362
Epoch 1830, training loss: 62.24773025512695 = 0.023680271580815315 + 10.0 * 6.222404956817627
Epoch 1830, val loss: 1.3619242906570435
Epoch 1840, training loss: 62.27861022949219 = 0.02322804555296898 + 10.0 * 6.22553825378418
Epoch 1840, val loss: 1.3657264709472656
Epoch 1850, training loss: 62.261558532714844 = 0.022766197100281715 + 10.0 * 6.223879337310791
Epoch 1850, val loss: 1.3693833351135254
Epoch 1860, training loss: 62.255496978759766 = 0.022330375388264656 + 10.0 * 6.223316669464111
Epoch 1860, val loss: 1.3730735778808594
Epoch 1870, training loss: 62.244590759277344 = 0.021904632449150085 + 10.0 * 6.222268581390381
Epoch 1870, val loss: 1.3768885135650635
Epoch 1880, training loss: 62.25197219848633 = 0.021497532725334167 + 10.0 * 6.223047733306885
Epoch 1880, val loss: 1.3805524110794067
Epoch 1890, training loss: 62.25212860107422 = 0.02109610103070736 + 10.0 * 6.2231035232543945
Epoch 1890, val loss: 1.38425874710083
Epoch 1900, training loss: 62.27995681762695 = 0.02070802077651024 + 10.0 * 6.225924968719482
Epoch 1900, val loss: 1.388588309288025
Epoch 1910, training loss: 62.24064636230469 = 0.020322907716035843 + 10.0 * 6.222032070159912
Epoch 1910, val loss: 1.3915483951568604
Epoch 1920, training loss: 62.23967361450195 = 0.019957728683948517 + 10.0 * 6.22197151184082
Epoch 1920, val loss: 1.3957719802856445
Epoch 1930, training loss: 62.25102996826172 = 0.019601814448833466 + 10.0 * 6.223143100738525
Epoch 1930, val loss: 1.3991339206695557
Epoch 1940, training loss: 62.221126556396484 = 0.019246293231844902 + 10.0 * 6.220188140869141
Epoch 1940, val loss: 1.402234673500061
Epoch 1950, training loss: 62.21957778930664 = 0.018908288329839706 + 10.0 * 6.220067024230957
Epoch 1950, val loss: 1.405612587928772
Epoch 1960, training loss: 62.216766357421875 = 0.018580924719572067 + 10.0 * 6.219818592071533
Epoch 1960, val loss: 1.409097671508789
Epoch 1970, training loss: 62.26087951660156 = 0.018262222409248352 + 10.0 * 6.22426176071167
Epoch 1970, val loss: 1.4125605821609497
Epoch 1980, training loss: 62.26206970214844 = 0.0179437268525362 + 10.0 * 6.224412441253662
Epoch 1980, val loss: 1.4165067672729492
Epoch 1990, training loss: 62.24502182006836 = 0.017627626657485962 + 10.0 * 6.222739219665527
Epoch 1990, val loss: 1.418677806854248
Epoch 2000, training loss: 62.21384811401367 = 0.01732570119202137 + 10.0 * 6.21965217590332
Epoch 2000, val loss: 1.4223253726959229
Epoch 2010, training loss: 62.19892501831055 = 0.017035990953445435 + 10.0 * 6.218188762664795
Epoch 2010, val loss: 1.4255404472351074
Epoch 2020, training loss: 62.19650650024414 = 0.0167554821819067 + 10.0 * 6.21797513961792
Epoch 2020, val loss: 1.4286445379257202
Epoch 2030, training loss: 62.25161361694336 = 0.016481701284646988 + 10.0 * 6.223513126373291
Epoch 2030, val loss: 1.4316495656967163
Epoch 2040, training loss: 62.203369140625 = 0.016207968816161156 + 10.0 * 6.218716144561768
Epoch 2040, val loss: 1.434748888015747
Epoch 2050, training loss: 62.21659851074219 = 0.015941334888339043 + 10.0 * 6.220065593719482
Epoch 2050, val loss: 1.437520146369934
Epoch 2060, training loss: 62.19612121582031 = 0.01568320207297802 + 10.0 * 6.218043804168701
Epoch 2060, val loss: 1.4409394264221191
Epoch 2070, training loss: 62.18944549560547 = 0.015434213913977146 + 10.0 * 6.217401027679443
Epoch 2070, val loss: 1.4442254304885864
Epoch 2080, training loss: 62.20933532714844 = 0.015191992744803429 + 10.0 * 6.219414710998535
Epoch 2080, val loss: 1.4473384618759155
Epoch 2090, training loss: 62.213584899902344 = 0.014949705451726913 + 10.0 * 6.219863414764404
Epoch 2090, val loss: 1.4497700929641724
Epoch 2100, training loss: 62.18724060058594 = 0.014713817276060581 + 10.0 * 6.217252731323242
Epoch 2100, val loss: 1.4534032344818115
Epoch 2110, training loss: 62.196414947509766 = 0.014485906809568405 + 10.0 * 6.218193054199219
Epoch 2110, val loss: 1.4563820362091064
Epoch 2120, training loss: 62.19407272338867 = 0.014262055978178978 + 10.0 * 6.217980861663818
Epoch 2120, val loss: 1.4589136838912964
Epoch 2130, training loss: 62.17610549926758 = 0.01404368132352829 + 10.0 * 6.2162065505981445
Epoch 2130, val loss: 1.46172297000885
Epoch 2140, training loss: 62.17537307739258 = 0.013832466676831245 + 10.0 * 6.216154098510742
Epoch 2140, val loss: 1.4642645120620728
Epoch 2150, training loss: 62.202430725097656 = 0.013626782223582268 + 10.0 * 6.218880653381348
Epoch 2150, val loss: 1.4667983055114746
Epoch 2160, training loss: 62.19174575805664 = 0.013421153649687767 + 10.0 * 6.217832565307617
Epoch 2160, val loss: 1.4702764749526978
Epoch 2170, training loss: 62.17884063720703 = 0.013220597058534622 + 10.0 * 6.216561794281006
Epoch 2170, val loss: 1.4728344678878784
Epoch 2180, training loss: 62.16583251953125 = 0.013026777654886246 + 10.0 * 6.215280532836914
Epoch 2180, val loss: 1.4755491018295288
Epoch 2190, training loss: 62.174560546875 = 0.012838603928685188 + 10.0 * 6.216172218322754
Epoch 2190, val loss: 1.4782830476760864
Epoch 2200, training loss: 62.18643569946289 = 0.012654878199100494 + 10.0 * 6.21737813949585
Epoch 2200, val loss: 1.4810105562210083
Epoch 2210, training loss: 62.182132720947266 = 0.012471969239413738 + 10.0 * 6.216966152191162
Epoch 2210, val loss: 1.4836456775665283
Epoch 2220, training loss: 62.18106460571289 = 0.01229269802570343 + 10.0 * 6.216877460479736
Epoch 2220, val loss: 1.48612380027771
Epoch 2230, training loss: 62.16677474975586 = 0.012116149999201298 + 10.0 * 6.215466022491455
Epoch 2230, val loss: 1.4886174201965332
Epoch 2240, training loss: 62.18122100830078 = 0.011948466300964355 + 10.0 * 6.216927528381348
Epoch 2240, val loss: 1.4910680055618286
Epoch 2250, training loss: 62.15464782714844 = 0.011778667569160461 + 10.0 * 6.214286804199219
Epoch 2250, val loss: 1.4935508966445923
Epoch 2260, training loss: 62.185264587402344 = 0.01161982212215662 + 10.0 * 6.217364311218262
Epoch 2260, val loss: 1.4954736232757568
Epoch 2270, training loss: 62.15658950805664 = 0.01145713496953249 + 10.0 * 6.214513301849365
Epoch 2270, val loss: 1.498265266418457
Epoch 2280, training loss: 62.14657211303711 = 0.01130201667547226 + 10.0 * 6.213526725769043
Epoch 2280, val loss: 1.5005401372909546
Epoch 2290, training loss: 62.15105056762695 = 0.011150740087032318 + 10.0 * 6.213990211486816
Epoch 2290, val loss: 1.503008484840393
Epoch 2300, training loss: 62.17477798461914 = 0.011001895181834698 + 10.0 * 6.2163777351379395
Epoch 2300, val loss: 1.5050572156906128
Epoch 2310, training loss: 62.15144348144531 = 0.010856169275939465 + 10.0 * 6.214058876037598
Epoch 2310, val loss: 1.5077883005142212
Epoch 2320, training loss: 62.150779724121094 = 0.010712679475545883 + 10.0 * 6.2140069007873535
Epoch 2320, val loss: 1.5099742412567139
Epoch 2330, training loss: 62.15618133544922 = 0.010571422055363655 + 10.0 * 6.2145609855651855
Epoch 2330, val loss: 1.5128302574157715
Epoch 2340, training loss: 62.14725112915039 = 0.010433406569063663 + 10.0 * 6.213681697845459
Epoch 2340, val loss: 1.5150736570358276
Epoch 2350, training loss: 62.13800048828125 = 0.010298337787389755 + 10.0 * 6.212769985198975
Epoch 2350, val loss: 1.5168653726577759
Epoch 2360, training loss: 62.15081787109375 = 0.01016862690448761 + 10.0 * 6.214064598083496
Epoch 2360, val loss: 1.5186653137207031
Epoch 2370, training loss: 62.148006439208984 = 0.010039370507001877 + 10.0 * 6.213796615600586
Epoch 2370, val loss: 1.5213313102722168
Epoch 2380, training loss: 62.14386749267578 = 0.009912116453051567 + 10.0 * 6.213395595550537
Epoch 2380, val loss: 1.5234100818634033
Epoch 2390, training loss: 62.12894058227539 = 0.009785224683582783 + 10.0 * 6.211915493011475
Epoch 2390, val loss: 1.5256768465042114
Epoch 2400, training loss: 62.12221145629883 = 0.009664912708103657 + 10.0 * 6.211254596710205
Epoch 2400, val loss: 1.5278922319412231
Epoch 2410, training loss: 62.12685012817383 = 0.009548315778374672 + 10.0 * 6.211730003356934
Epoch 2410, val loss: 1.5302051305770874
Epoch 2420, training loss: 62.184608459472656 = 0.00943424180150032 + 10.0 * 6.217517375946045
Epoch 2420, val loss: 1.5325733423233032
Epoch 2430, training loss: 62.1451301574707 = 0.009312035515904427 + 10.0 * 6.2135820388793945
Epoch 2430, val loss: 1.5336601734161377
Epoch 2440, training loss: 62.12154769897461 = 0.009200255386531353 + 10.0 * 6.2112345695495605
Epoch 2440, val loss: 1.536299467086792
Epoch 2450, training loss: 62.13249969482422 = 0.009092330001294613 + 10.0 * 6.212340831756592
Epoch 2450, val loss: 1.5380682945251465
Epoch 2460, training loss: 62.132476806640625 = 0.008984178304672241 + 10.0 * 6.2123494148254395
Epoch 2460, val loss: 1.5404547452926636
Epoch 2470, training loss: 62.14883041381836 = 0.008875420317053795 + 10.0 * 6.213995456695557
Epoch 2470, val loss: 1.5418275594711304
Epoch 2480, training loss: 62.11241912841797 = 0.008769471198320389 + 10.0 * 6.210364818572998
Epoch 2480, val loss: 1.5437190532684326
Epoch 2490, training loss: 62.10853576660156 = 0.008667522110044956 + 10.0 * 6.209986686706543
Epoch 2490, val loss: 1.5454286336898804
Epoch 2500, training loss: 62.134490966796875 = 0.008567738346755505 + 10.0 * 6.212592124938965
Epoch 2500, val loss: 1.547344446182251
Epoch 2510, training loss: 62.12434768676758 = 0.008470411412417889 + 10.0 * 6.211587905883789
Epoch 2510, val loss: 1.5498489141464233
Epoch 2520, training loss: 62.112728118896484 = 0.008372998796403408 + 10.0 * 6.210435390472412
Epoch 2520, val loss: 1.551466703414917
Epoch 2530, training loss: 62.10493469238281 = 0.008277378045022488 + 10.0 * 6.209665775299072
Epoch 2530, val loss: 1.5530145168304443
Epoch 2540, training loss: 62.15233612060547 = 0.008187093771994114 + 10.0 * 6.214415073394775
Epoch 2540, val loss: 1.555006742477417
Epoch 2550, training loss: 62.11323165893555 = 0.008091266267001629 + 10.0 * 6.210514068603516
Epoch 2550, val loss: 1.5566673278808594
Epoch 2560, training loss: 62.11703109741211 = 0.008002778515219688 + 10.0 * 6.210902690887451
Epoch 2560, val loss: 1.5583182573318481
Epoch 2570, training loss: 62.11311340332031 = 0.007912954315543175 + 10.0 * 6.210520267486572
Epoch 2570, val loss: 1.5597026348114014
Epoch 2580, training loss: 62.113365173339844 = 0.00782813224941492 + 10.0 * 6.2105536460876465
Epoch 2580, val loss: 1.5613106489181519
Epoch 2590, training loss: 62.108917236328125 = 0.007741515524685383 + 10.0 * 6.210117816925049
Epoch 2590, val loss: 1.563063383102417
Epoch 2600, training loss: 62.09425735473633 = 0.007659147027879953 + 10.0 * 6.208659648895264
Epoch 2600, val loss: 1.565071940422058
Epoch 2610, training loss: 62.108543395996094 = 0.0075783878564834595 + 10.0 * 6.21009635925293
Epoch 2610, val loss: 1.566576361656189
Epoch 2620, training loss: 62.08621597290039 = 0.007496283855289221 + 10.0 * 6.207871913909912
Epoch 2620, val loss: 1.56808602809906
Epoch 2630, training loss: 62.10966491699219 = 0.0074193524196743965 + 10.0 * 6.210224628448486
Epoch 2630, val loss: 1.5693687200546265
Epoch 2640, training loss: 62.11338806152344 = 0.007338585797697306 + 10.0 * 6.210604667663574
Epoch 2640, val loss: 1.5711349248886108
Epoch 2650, training loss: 62.10327911376953 = 0.007262176368385553 + 10.0 * 6.209601402282715
Epoch 2650, val loss: 1.5727441310882568
Epoch 2660, training loss: 62.10564422607422 = 0.007186563219875097 + 10.0 * 6.209845542907715
Epoch 2660, val loss: 1.5739914178848267
Epoch 2670, training loss: 62.08024215698242 = 0.00711353775113821 + 10.0 * 6.20731258392334
Epoch 2670, val loss: 1.5759973526000977
Epoch 2680, training loss: 62.085533142089844 = 0.007043324876576662 + 10.0 * 6.207849025726318
Epoch 2680, val loss: 1.5774039030075073
Epoch 2690, training loss: 62.112972259521484 = 0.006972296163439751 + 10.0 * 6.210599899291992
Epoch 2690, val loss: 1.5789532661437988
Epoch 2700, training loss: 62.11093521118164 = 0.006900206673890352 + 10.0 * 6.2104034423828125
Epoch 2700, val loss: 1.5798155069351196
Epoch 2710, training loss: 62.074188232421875 = 0.006830853410065174 + 10.0 * 6.206735610961914
Epoch 2710, val loss: 1.5817824602127075
Epoch 2720, training loss: 62.07276153564453 = 0.0067643579095602036 + 10.0 * 6.206599712371826
Epoch 2720, val loss: 1.5833808183670044
Epoch 2730, training loss: 62.119476318359375 = 0.006698852404952049 + 10.0 * 6.211277961730957
Epoch 2730, val loss: 1.5848777294158936
Epoch 2740, training loss: 62.07984161376953 = 0.006630062125623226 + 10.0 * 6.2073211669921875
Epoch 2740, val loss: 1.5855813026428223
Epoch 2750, training loss: 62.08763122558594 = 0.006566752679646015 + 10.0 * 6.208106517791748
Epoch 2750, val loss: 1.5876637697219849
Epoch 2760, training loss: 62.0811653137207 = 0.006502244155853987 + 10.0 * 6.207466125488281
Epoch 2760, val loss: 1.5889029502868652
Epoch 2770, training loss: 62.0716438293457 = 0.006439075339585543 + 10.0 * 6.2065205574035645
Epoch 2770, val loss: 1.5901633501052856
Epoch 2780, training loss: 62.066627502441406 = 0.006379174068570137 + 10.0 * 6.206025123596191
Epoch 2780, val loss: 1.591212272644043
Epoch 2790, training loss: 62.07931137084961 = 0.006320375017821789 + 10.0 * 6.20729923248291
Epoch 2790, val loss: 1.5928081274032593
Epoch 2800, training loss: 62.06460189819336 = 0.006260893307626247 + 10.0 * 6.20583438873291
Epoch 2800, val loss: 1.5942829847335815
Epoch 2810, training loss: 62.09442901611328 = 0.006204207893460989 + 10.0 * 6.208822727203369
Epoch 2810, val loss: 1.5955533981323242
Epoch 2820, training loss: 62.066707611083984 = 0.0061439028941094875 + 10.0 * 6.206056118011475
Epoch 2820, val loss: 1.5964796543121338
Epoch 2830, training loss: 62.066017150878906 = 0.006086330395191908 + 10.0 * 6.205993175506592
Epoch 2830, val loss: 1.5975923538208008
Epoch 2840, training loss: 62.06989669799805 = 0.006030593067407608 + 10.0 * 6.206386566162109
Epoch 2840, val loss: 1.5988166332244873
Epoch 2850, training loss: 62.09492492675781 = 0.005976061336696148 + 10.0 * 6.208894729614258
Epoch 2850, val loss: 1.5998786687850952
Epoch 2860, training loss: 62.0650520324707 = 0.005920162424445152 + 10.0 * 6.205913066864014
Epoch 2860, val loss: 1.6010710000991821
Epoch 2870, training loss: 62.07261657714844 = 0.005867347586899996 + 10.0 * 6.206675052642822
Epoch 2870, val loss: 1.6025431156158447
Epoch 2880, training loss: 62.08049392700195 = 0.005814622156322002 + 10.0 * 6.207468032836914
Epoch 2880, val loss: 1.6034927368164062
Epoch 2890, training loss: 62.055850982666016 = 0.005761085078120232 + 10.0 * 6.2050089836120605
Epoch 2890, val loss: 1.6045444011688232
Epoch 2900, training loss: 62.04326248168945 = 0.005711056757718325 + 10.0 * 6.2037553787231445
Epoch 2900, val loss: 1.605886697769165
Epoch 2910, training loss: 62.048526763916016 = 0.005663975141942501 + 10.0 * 6.204286098480225
Epoch 2910, val loss: 1.6070809364318848
Epoch 2920, training loss: 62.123008728027344 = 0.005616947077214718 + 10.0 * 6.211739540100098
Epoch 2920, val loss: 1.6087493896484375
Epoch 2930, training loss: 62.09531021118164 = 0.0055651115253567696 + 10.0 * 6.208974361419678
Epoch 2930, val loss: 1.6089564561843872
Epoch 2940, training loss: 62.06125259399414 = 0.005514749325811863 + 10.0 * 6.205573558807373
Epoch 2940, val loss: 1.6104528903961182
Epoch 2950, training loss: 62.04335021972656 = 0.005465969908982515 + 10.0 * 6.2037882804870605
Epoch 2950, val loss: 1.610965609550476
Epoch 2960, training loss: 62.04892349243164 = 0.005421676207333803 + 10.0 * 6.204350471496582
Epoch 2960, val loss: 1.612012505531311
Epoch 2970, training loss: 62.0732421875 = 0.005376280751079321 + 10.0 * 6.206786632537842
Epoch 2970, val loss: 1.6131072044372559
Epoch 2980, training loss: 62.06328201293945 = 0.005333353765308857 + 10.0 * 6.205794811248779
Epoch 2980, val loss: 1.6147418022155762
Epoch 2990, training loss: 62.037906646728516 = 0.005286101251840591 + 10.0 * 6.203261852264404
Epoch 2990, val loss: 1.6153595447540283
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 87.93001556396484 = 1.961543083190918 + 10.0 * 8.596847534179688
Epoch 0, val loss: 1.9613120555877686
Epoch 10, training loss: 87.91448974609375 = 1.9512939453125 + 10.0 * 8.596319198608398
Epoch 10, val loss: 1.9505479335784912
Epoch 20, training loss: 87.86563873291016 = 1.939475178718567 + 10.0 * 8.592616081237793
Epoch 20, val loss: 1.9380649328231812
Epoch 30, training loss: 87.59807586669922 = 1.9248583316802979 + 10.0 * 8.56732177734375
Epoch 30, val loss: 1.9227349758148193
Epoch 40, training loss: 85.85383605957031 = 1.907391905784607 + 10.0 * 8.394643783569336
Epoch 40, val loss: 1.9046531915664673
Epoch 50, training loss: 76.96251678466797 = 1.887664556503296 + 10.0 * 7.5074849128723145
Epoch 50, val loss: 1.8844760656356812
Epoch 60, training loss: 73.55875396728516 = 1.8706891536712646 + 10.0 * 7.168806076049805
Epoch 60, val loss: 1.86880624294281
Epoch 70, training loss: 71.7551498413086 = 1.857528805732727 + 10.0 * 6.989762306213379
Epoch 70, val loss: 1.8557581901550293
Epoch 80, training loss: 70.91931915283203 = 1.8440405130386353 + 10.0 * 6.907527923583984
Epoch 80, val loss: 1.8425029516220093
Epoch 90, training loss: 70.27214813232422 = 1.831093192100525 + 10.0 * 6.844105243682861
Epoch 90, val loss: 1.8297796249389648
Epoch 100, training loss: 69.7509536743164 = 1.8191444873809814 + 10.0 * 6.793180465698242
Epoch 100, val loss: 1.8181036710739136
Epoch 110, training loss: 69.32762145996094 = 1.8083871603012085 + 10.0 * 6.751923561096191
Epoch 110, val loss: 1.807703971862793
Epoch 120, training loss: 69.01860046386719 = 1.7977646589279175 + 10.0 * 6.722083568572998
Epoch 120, val loss: 1.7974319458007812
Epoch 130, training loss: 68.72602081298828 = 1.7867560386657715 + 10.0 * 6.6939263343811035
Epoch 130, val loss: 1.7869493961334229
Epoch 140, training loss: 68.44734954833984 = 1.7759625911712646 + 10.0 * 6.667138576507568
Epoch 140, val loss: 1.7767165899276733
Epoch 150, training loss: 68.1806640625 = 1.7653019428253174 + 10.0 * 6.641535758972168
Epoch 150, val loss: 1.7664191722869873
Epoch 160, training loss: 67.92791748046875 = 1.7539231777191162 + 10.0 * 6.617399215698242
Epoch 160, val loss: 1.755726933479309
Epoch 170, training loss: 67.67024993896484 = 1.7420732975006104 + 10.0 * 6.592817783355713
Epoch 170, val loss: 1.7446343898773193
Epoch 180, training loss: 67.44585418701172 = 1.7296546697616577 + 10.0 * 6.571620464324951
Epoch 180, val loss: 1.7329938411712646
Epoch 190, training loss: 67.21601867675781 = 1.7163112163543701 + 10.0 * 6.549970626831055
Epoch 190, val loss: 1.7206919193267822
Epoch 200, training loss: 67.00949096679688 = 1.702031135559082 + 10.0 * 6.530745506286621
Epoch 200, val loss: 1.7076363563537598
Epoch 210, training loss: 66.81234741210938 = 1.6867015361785889 + 10.0 * 6.512564659118652
Epoch 210, val loss: 1.6934294700622559
Epoch 220, training loss: 66.64334106445312 = 1.6700094938278198 + 10.0 * 6.497333526611328
Epoch 220, val loss: 1.678343653678894
Epoch 230, training loss: 66.45085906982422 = 1.6520389318466187 + 10.0 * 6.47988224029541
Epoch 230, val loss: 1.662033200263977
Epoch 240, training loss: 66.2925033569336 = 1.6325364112854004 + 10.0 * 6.465996265411377
Epoch 240, val loss: 1.6444053649902344
Epoch 250, training loss: 66.1644515991211 = 1.6112933158874512 + 10.0 * 6.455316066741943
Epoch 250, val loss: 1.625335693359375
Epoch 260, training loss: 66.01738739013672 = 1.588579535484314 + 10.0 * 6.442881107330322
Epoch 260, val loss: 1.6049305200576782
Epoch 270, training loss: 65.89070892333984 = 1.5643703937530518 + 10.0 * 6.432633876800537
Epoch 270, val loss: 1.5835561752319336
Epoch 280, training loss: 65.7752685546875 = 1.5385276079177856 + 10.0 * 6.423673629760742
Epoch 280, val loss: 1.560890793800354
Epoch 290, training loss: 65.67338562011719 = 1.5110702514648438 + 10.0 * 6.416231632232666
Epoch 290, val loss: 1.5370197296142578
Epoch 300, training loss: 65.59820556640625 = 1.482262134552002 + 10.0 * 6.411594390869141
Epoch 300, val loss: 1.5124212503433228
Epoch 310, training loss: 65.48817443847656 = 1.4525201320648193 + 10.0 * 6.403565406799316
Epoch 310, val loss: 1.4869048595428467
Epoch 320, training loss: 65.39488220214844 = 1.4216063022613525 + 10.0 * 6.397327423095703
Epoch 320, val loss: 1.4607919454574585
Epoch 330, training loss: 65.30784606933594 = 1.3897373676300049 + 10.0 * 6.391810417175293
Epoch 330, val loss: 1.434136986732483
Epoch 340, training loss: 65.23079681396484 = 1.356966257095337 + 10.0 * 6.387382984161377
Epoch 340, val loss: 1.40690279006958
Epoch 350, training loss: 65.15467834472656 = 1.3237361907958984 + 10.0 * 6.38309383392334
Epoch 350, val loss: 1.3795111179351807
Epoch 360, training loss: 65.07315826416016 = 1.2900786399841309 + 10.0 * 6.378308296203613
Epoch 360, val loss: 1.3521544933319092
Epoch 370, training loss: 64.98994445800781 = 1.2560853958129883 + 10.0 * 6.373385906219482
Epoch 370, val loss: 1.3248440027236938
Epoch 380, training loss: 64.91873931884766 = 1.2218517065048218 + 10.0 * 6.369688510894775
Epoch 380, val loss: 1.2975890636444092
Epoch 390, training loss: 64.84896087646484 = 1.1875497102737427 + 10.0 * 6.366140842437744
Epoch 390, val loss: 1.2704490423202515
Epoch 400, training loss: 64.76770782470703 = 1.1531373262405396 + 10.0 * 6.361456871032715
Epoch 400, val loss: 1.2437744140625
Epoch 410, training loss: 64.71770477294922 = 1.1186037063598633 + 10.0 * 6.359910011291504
Epoch 410, val loss: 1.2172698974609375
Epoch 420, training loss: 64.62860107421875 = 1.0847030878067017 + 10.0 * 6.354389667510986
Epoch 420, val loss: 1.1914482116699219
Epoch 430, training loss: 64.56195831298828 = 1.0510202646255493 + 10.0 * 6.351093769073486
Epoch 430, val loss: 1.1664634943008423
Epoch 440, training loss: 64.4852294921875 = 1.0179023742675781 + 10.0 * 6.346733093261719
Epoch 440, val loss: 1.1421524286270142
Epoch 450, training loss: 64.45523834228516 = 0.9852268695831299 + 10.0 * 6.347001075744629
Epoch 450, val loss: 1.1185261011123657
Epoch 460, training loss: 64.39442443847656 = 0.9526724815368652 + 10.0 * 6.344175338745117
Epoch 460, val loss: 1.095607042312622
Epoch 470, training loss: 64.30032348632812 = 0.9212275147438049 + 10.0 * 6.33790922164917
Epoch 470, val loss: 1.0736944675445557
Epoch 480, training loss: 64.24720764160156 = 0.8905539512634277 + 10.0 * 6.335665225982666
Epoch 480, val loss: 1.0527492761611938
Epoch 490, training loss: 64.21821594238281 = 0.8605398535728455 + 10.0 * 6.3357672691345215
Epoch 490, val loss: 1.032753825187683
Epoch 500, training loss: 64.14958190917969 = 0.8314066529273987 + 10.0 * 6.331817150115967
Epoch 500, val loss: 1.0137439966201782
Epoch 510, training loss: 64.08370971679688 = 0.8032612204551697 + 10.0 * 6.328044891357422
Epoch 510, val loss: 0.9958589673042297
Epoch 520, training loss: 64.04054260253906 = 0.776007354259491 + 10.0 * 6.32645320892334
Epoch 520, val loss: 0.9788820743560791
Epoch 530, training loss: 63.99886703491211 = 0.749474048614502 + 10.0 * 6.324939250946045
Epoch 530, val loss: 0.9629628658294678
Epoch 540, training loss: 63.9349365234375 = 0.7238157987594604 + 10.0 * 6.321112155914307
Epoch 540, val loss: 0.947810709476471
Epoch 550, training loss: 63.90748596191406 = 0.6989516019821167 + 10.0 * 6.320853233337402
Epoch 550, val loss: 0.9335655570030212
Epoch 560, training loss: 63.8587532043457 = 0.6750243902206421 + 10.0 * 6.31837272644043
Epoch 560, val loss: 0.9201422333717346
Epoch 570, training loss: 63.794376373291016 = 0.6517829895019531 + 10.0 * 6.3142595291137695
Epoch 570, val loss: 0.9074698090553284
Epoch 580, training loss: 63.75458526611328 = 0.6294510364532471 + 10.0 * 6.31251335144043
Epoch 580, val loss: 0.8957400321960449
Epoch 590, training loss: 63.723331451416016 = 0.6077394485473633 + 10.0 * 6.311559200286865
Epoch 590, val loss: 0.8846134543418884
Epoch 600, training loss: 63.70405960083008 = 0.5866125822067261 + 10.0 * 6.311744689941406
Epoch 600, val loss: 0.8744084239006042
Epoch 610, training loss: 63.66019058227539 = 0.5662357807159424 + 10.0 * 6.309395790100098
Epoch 610, val loss: 0.8644760251045227
Epoch 620, training loss: 63.59819793701172 = 0.5464982390403748 + 10.0 * 6.305170059204102
Epoch 620, val loss: 0.8555073738098145
Epoch 630, training loss: 63.55216598510742 = 0.5274406671524048 + 10.0 * 6.3024725914001465
Epoch 630, val loss: 0.8472460508346558
Epoch 640, training loss: 63.51760482788086 = 0.5090479254722595 + 10.0 * 6.30085563659668
Epoch 640, val loss: 0.8396044969558716
Epoch 650, training loss: 63.520076751708984 = 0.49113067984580994 + 10.0 * 6.302894592285156
Epoch 650, val loss: 0.8324493765830994
Epoch 660, training loss: 63.48388671875 = 0.47386476397514343 + 10.0 * 6.301002025604248
Epoch 660, val loss: 0.8256655335426331
Epoch 670, training loss: 63.41257858276367 = 0.45694395899772644 + 10.0 * 6.295563697814941
Epoch 670, val loss: 0.8196386694908142
Epoch 680, training loss: 63.3806037902832 = 0.4407687783241272 + 10.0 * 6.293983459472656
Epoch 680, val loss: 0.814252495765686
Epoch 690, training loss: 63.37113571166992 = 0.42514166235923767 + 10.0 * 6.294599533081055
Epoch 690, val loss: 0.8093884587287903
Epoch 700, training loss: 63.331764221191406 = 0.4099760353565216 + 10.0 * 6.292178630828857
Epoch 700, val loss: 0.8047022223472595
Epoch 710, training loss: 63.3067741394043 = 0.39524948596954346 + 10.0 * 6.291152477264404
Epoch 710, val loss: 0.8007075190544128
Epoch 720, training loss: 63.269718170166016 = 0.3810853660106659 + 10.0 * 6.288863182067871
Epoch 720, val loss: 0.797175407409668
Epoch 730, training loss: 63.23184585571289 = 0.3674739897251129 + 10.0 * 6.286437034606934
Epoch 730, val loss: 0.7942028641700745
Epoch 740, training loss: 63.205265045166016 = 0.35436514019966125 + 10.0 * 6.28508996963501
Epoch 740, val loss: 0.7916057705879211
Epoch 750, training loss: 63.23475646972656 = 0.3416557013988495 + 10.0 * 6.289309978485107
Epoch 750, val loss: 0.7895175218582153
Epoch 760, training loss: 63.21449661254883 = 0.3293124735355377 + 10.0 * 6.28851842880249
Epoch 760, val loss: 0.787479817867279
Epoch 770, training loss: 63.149383544921875 = 0.3174900412559509 + 10.0 * 6.283189296722412
Epoch 770, val loss: 0.7860881686210632
Epoch 780, training loss: 63.11085510253906 = 0.3062170445919037 + 10.0 * 6.280463695526123
Epoch 780, val loss: 0.7851886749267578
Epoch 790, training loss: 63.08617401123047 = 0.295381098985672 + 10.0 * 6.279079437255859
Epoch 790, val loss: 0.7846620678901672
Epoch 800, training loss: 63.08427429199219 = 0.2849172055721283 + 10.0 * 6.279935836791992
Epoch 800, val loss: 0.7845481634140015
Epoch 810, training loss: 63.05271530151367 = 0.274836927652359 + 10.0 * 6.277787685394287
Epoch 810, val loss: 0.7844465374946594
Epoch 820, training loss: 63.0485954284668 = 0.26507771015167236 + 10.0 * 6.278351783752441
Epoch 820, val loss: 0.7849133014678955
Epoch 830, training loss: 63.00558853149414 = 0.2558220326900482 + 10.0 * 6.27497673034668
Epoch 830, val loss: 0.7857218980789185
Epoch 840, training loss: 62.98713302612305 = 0.24694432318210602 + 10.0 * 6.27401876449585
Epoch 840, val loss: 0.7868187427520752
Epoch 850, training loss: 63.024471282958984 = 0.23839892446994781 + 10.0 * 6.278607368469238
Epoch 850, val loss: 0.7881997227668762
Epoch 860, training loss: 62.973995208740234 = 0.23012864589691162 + 10.0 * 6.274386405944824
Epoch 860, val loss: 0.7894299626350403
Epoch 870, training loss: 62.94268035888672 = 0.2222064882516861 + 10.0 * 6.272047519683838
Epoch 870, val loss: 0.7913669347763062
Epoch 880, training loss: 62.94117736816406 = 0.2146078497171402 + 10.0 * 6.2726569175720215
Epoch 880, val loss: 0.7934218645095825
Epoch 890, training loss: 62.910186767578125 = 0.20728115737438202 + 10.0 * 6.270290374755859
Epoch 890, val loss: 0.795557975769043
Epoch 900, training loss: 62.888038635253906 = 0.20033718645572662 + 10.0 * 6.268770217895508
Epoch 900, val loss: 0.7979809641838074
Epoch 910, training loss: 62.867496490478516 = 0.1936228722333908 + 10.0 * 6.267387390136719
Epoch 910, val loss: 0.800495982170105
Epoch 920, training loss: 62.86371612548828 = 0.18721403181552887 + 10.0 * 6.267650127410889
Epoch 920, val loss: 0.8032791614532471
Epoch 930, training loss: 62.857696533203125 = 0.1810387820005417 + 10.0 * 6.267665863037109
Epoch 930, val loss: 0.8061690330505371
Epoch 940, training loss: 62.82400131225586 = 0.1750391721725464 + 10.0 * 6.264896392822266
Epoch 940, val loss: 0.8091521263122559
Epoch 950, training loss: 62.83634948730469 = 0.1693619191646576 + 10.0 * 6.266698837280273
Epoch 950, val loss: 0.8123639822006226
Epoch 960, training loss: 62.794715881347656 = 0.16387663781642914 + 10.0 * 6.2630839347839355
Epoch 960, val loss: 0.8156587481498718
Epoch 970, training loss: 62.78313446044922 = 0.158622145652771 + 10.0 * 6.262451171875
Epoch 970, val loss: 0.8192362785339355
Epoch 980, training loss: 62.794857025146484 = 0.15362124145030975 + 10.0 * 6.264123439788818
Epoch 980, val loss: 0.822756826877594
Epoch 990, training loss: 62.76481246948242 = 0.14869897067546844 + 10.0 * 6.261610984802246
Epoch 990, val loss: 0.8264303803443909
Epoch 1000, training loss: 62.754249572753906 = 0.14404287934303284 + 10.0 * 6.261020660400391
Epoch 1000, val loss: 0.8300782442092896
Epoch 1010, training loss: 62.7407341003418 = 0.13956044614315033 + 10.0 * 6.260117530822754
Epoch 1010, val loss: 0.8340170383453369
Epoch 1020, training loss: 62.732234954833984 = 0.13527897000312805 + 10.0 * 6.259695529937744
Epoch 1020, val loss: 0.8379607200622559
Epoch 1030, training loss: 62.739013671875 = 0.13113994896411896 + 10.0 * 6.260787010192871
Epoch 1030, val loss: 0.8423064351081848
Epoch 1040, training loss: 62.70283508300781 = 0.12715312838554382 + 10.0 * 6.257568359375
Epoch 1040, val loss: 0.8458703756332397
Epoch 1050, training loss: 62.69587326049805 = 0.12333416938781738 + 10.0 * 6.257254123687744
Epoch 1050, val loss: 0.8505232334136963
Epoch 1060, training loss: 62.70895767211914 = 0.11967272311449051 + 10.0 * 6.2589287757873535
Epoch 1060, val loss: 0.8544785380363464
Epoch 1070, training loss: 62.681884765625 = 0.11614257097244263 + 10.0 * 6.2565741539001465
Epoch 1070, val loss: 0.8587201237678528
Epoch 1080, training loss: 62.661033630371094 = 0.11274001002311707 + 10.0 * 6.254829406738281
Epoch 1080, val loss: 0.8630177974700928
Epoch 1090, training loss: 62.66255187988281 = 0.10947047919034958 + 10.0 * 6.255308151245117
Epoch 1090, val loss: 0.8673949837684631
Epoch 1100, training loss: 62.661685943603516 = 0.10631079971790314 + 10.0 * 6.255537509918213
Epoch 1100, val loss: 0.8716902732849121
Epoch 1110, training loss: 62.66178512573242 = 0.10324918478727341 + 10.0 * 6.255853652954102
Epoch 1110, val loss: 0.8763309121131897
Epoch 1120, training loss: 62.62398910522461 = 0.10033325105905533 + 10.0 * 6.252365589141846
Epoch 1120, val loss: 0.8804397583007812
Epoch 1130, training loss: 62.61247634887695 = 0.0975203886628151 + 10.0 * 6.251495838165283
Epoch 1130, val loss: 0.8850951790809631
Epoch 1140, training loss: 62.602516174316406 = 0.09483358263969421 + 10.0 * 6.250768184661865
Epoch 1140, val loss: 0.8895347118377686
Epoch 1150, training loss: 62.618839263916016 = 0.092232346534729 + 10.0 * 6.252660751342773
Epoch 1150, val loss: 0.8939908742904663
Epoch 1160, training loss: 62.60688400268555 = 0.08969879150390625 + 10.0 * 6.251718521118164
Epoch 1160, val loss: 0.8986039161682129
Epoch 1170, training loss: 62.597965240478516 = 0.08722434192895889 + 10.0 * 6.251074314117432
Epoch 1170, val loss: 0.9029967784881592
Epoch 1180, training loss: 62.59914779663086 = 0.08486524224281311 + 10.0 * 6.251428127288818
Epoch 1180, val loss: 0.9077132344245911
Epoch 1190, training loss: 62.575172424316406 = 0.08259604871273041 + 10.0 * 6.249257564544678
Epoch 1190, val loss: 0.912074089050293
Epoch 1200, training loss: 62.556156158447266 = 0.08042177557945251 + 10.0 * 6.247573375701904
Epoch 1200, val loss: 0.9168258309364319
Epoch 1210, training loss: 62.55122756958008 = 0.07833166420459747 + 10.0 * 6.247289657592773
Epoch 1210, val loss: 0.9215424656867981
Epoch 1220, training loss: 62.59434127807617 = 0.07629897445440292 + 10.0 * 6.251804351806641
Epoch 1220, val loss: 0.9260293841362
Epoch 1230, training loss: 62.5412483215332 = 0.07428278774023056 + 10.0 * 6.246696472167969
Epoch 1230, val loss: 0.9306904077529907
Epoch 1240, training loss: 62.53700637817383 = 0.07237756252288818 + 10.0 * 6.246462821960449
Epoch 1240, val loss: 0.9351898431777954
Epoch 1250, training loss: 62.5195426940918 = 0.0705491155385971 + 10.0 * 6.244899272918701
Epoch 1250, val loss: 0.9399363398551941
Epoch 1260, training loss: 62.53557586669922 = 0.06877884268760681 + 10.0 * 6.246679782867432
Epoch 1260, val loss: 0.9446898102760315
Epoch 1270, training loss: 62.537437438964844 = 0.06705290824174881 + 10.0 * 6.2470383644104
Epoch 1270, val loss: 0.9489209055900574
Epoch 1280, training loss: 62.505653381347656 = 0.06536706537008286 + 10.0 * 6.244028568267822
Epoch 1280, val loss: 0.9539531469345093
Epoch 1290, training loss: 62.51006317138672 = 0.06377173960208893 + 10.0 * 6.244629383087158
Epoch 1290, val loss: 0.958434522151947
Epoch 1300, training loss: 62.52143478393555 = 0.06221222132444382 + 10.0 * 6.245922088623047
Epoch 1300, val loss: 0.9632336497306824
Epoch 1310, training loss: 62.49235916137695 = 0.060718946158885956 + 10.0 * 6.2431640625
Epoch 1310, val loss: 0.9675176739692688
Epoch 1320, training loss: 62.47312927246094 = 0.0592675618827343 + 10.0 * 6.2413859367370605
Epoch 1320, val loss: 0.9723783135414124
Epoch 1330, training loss: 62.49687194824219 = 0.0578695647418499 + 10.0 * 6.243900299072266
Epoch 1330, val loss: 0.9770168662071228
Epoch 1340, training loss: 62.46940612792969 = 0.056501664221286774 + 10.0 * 6.241290092468262
Epoch 1340, val loss: 0.9813052415847778
Epoch 1350, training loss: 62.47671890258789 = 0.055181361734867096 + 10.0 * 6.242154121398926
Epoch 1350, val loss: 0.9860119819641113
Epoch 1360, training loss: 62.48152160644531 = 0.05390243977308273 + 10.0 * 6.242762088775635
Epoch 1360, val loss: 0.9905940294265747
Epoch 1370, training loss: 62.45773696899414 = 0.05264870449900627 + 10.0 * 6.240508556365967
Epoch 1370, val loss: 0.9952121376991272
Epoch 1380, training loss: 62.441009521484375 = 0.051464907824993134 + 10.0 * 6.238954544067383
Epoch 1380, val loss: 0.9998703598976135
Epoch 1390, training loss: 62.441463470458984 = 0.0503147728741169 + 10.0 * 6.239114761352539
Epoch 1390, val loss: 1.0044770240783691
Epoch 1400, training loss: 62.49148178100586 = 0.04920179769396782 + 10.0 * 6.244227886199951
Epoch 1400, val loss: 1.008676528930664
Epoch 1410, training loss: 62.45934295654297 = 0.04807325825095177 + 10.0 * 6.241127014160156
Epoch 1410, val loss: 1.0132235288619995
Epoch 1420, training loss: 62.425472259521484 = 0.047009438276290894 + 10.0 * 6.237846374511719
Epoch 1420, val loss: 1.0179105997085571
Epoch 1430, training loss: 62.42700958251953 = 0.04598555341362953 + 10.0 * 6.238102436065674
Epoch 1430, val loss: 1.022666573524475
Epoch 1440, training loss: 62.456478118896484 = 0.0449984073638916 + 10.0 * 6.241147994995117
Epoch 1440, val loss: 1.026941180229187
Epoch 1450, training loss: 62.41646194458008 = 0.04402059316635132 + 10.0 * 6.237244129180908
Epoch 1450, val loss: 1.031335473060608
Epoch 1460, training loss: 62.40660858154297 = 0.04308503493666649 + 10.0 * 6.236352443695068
Epoch 1460, val loss: 1.0358998775482178
Epoch 1470, training loss: 62.422569274902344 = 0.0421789325773716 + 10.0 * 6.238039016723633
Epoch 1470, val loss: 1.0404109954833984
Epoch 1480, training loss: 62.40464401245117 = 0.04128997400403023 + 10.0 * 6.236335277557373
Epoch 1480, val loss: 1.0447660684585571
Epoch 1490, training loss: 62.40433883666992 = 0.04043876752257347 + 10.0 * 6.236390113830566
Epoch 1490, val loss: 1.049282431602478
Epoch 1500, training loss: 62.437171936035156 = 0.03960542753338814 + 10.0 * 6.2397565841674805
Epoch 1500, val loss: 1.0536723136901855
Epoch 1510, training loss: 62.394630432128906 = 0.03876514360308647 + 10.0 * 6.235586643218994
Epoch 1510, val loss: 1.058034896850586
Epoch 1520, training loss: 62.393592834472656 = 0.037973735481500626 + 10.0 * 6.235561847686768
Epoch 1520, val loss: 1.0625991821289062
Epoch 1530, training loss: 62.404666900634766 = 0.037207670509815216 + 10.0 * 6.236745834350586
Epoch 1530, val loss: 1.066877007484436
Epoch 1540, training loss: 62.38800048828125 = 0.03646595776081085 + 10.0 * 6.235153675079346
Epoch 1540, val loss: 1.071226954460144
Epoch 1550, training loss: 62.36685562133789 = 0.035740576684474945 + 10.0 * 6.233111381530762
Epoch 1550, val loss: 1.0754483938217163
Epoch 1560, training loss: 62.379703521728516 = 0.03503670543432236 + 10.0 * 6.234466552734375
Epoch 1560, val loss: 1.0798550844192505
Epoch 1570, training loss: 62.378746032714844 = 0.03434827923774719 + 10.0 * 6.234439849853516
Epoch 1570, val loss: 1.084242343902588
Epoch 1580, training loss: 62.363346099853516 = 0.033692166209220886 + 10.0 * 6.232965469360352
Epoch 1580, val loss: 1.088429570198059
Epoch 1590, training loss: 62.36533737182617 = 0.03303565829992294 + 10.0 * 6.233230113983154
Epoch 1590, val loss: 1.0927780866622925
Epoch 1600, training loss: 62.374698638916016 = 0.03240593522787094 + 10.0 * 6.23422908782959
Epoch 1600, val loss: 1.0968613624572754
Epoch 1610, training loss: 62.36967849731445 = 0.031780906021595 + 10.0 * 6.233789920806885
Epoch 1610, val loss: 1.100615382194519
Epoch 1620, training loss: 62.349098205566406 = 0.03118240460753441 + 10.0 * 6.2317914962768555
Epoch 1620, val loss: 1.1051667928695679
Epoch 1630, training loss: 62.34170913696289 = 0.030590064823627472 + 10.0 * 6.231112003326416
Epoch 1630, val loss: 1.1092654466629028
Epoch 1640, training loss: 62.33208084106445 = 0.03003501519560814 + 10.0 * 6.2302045822143555
Epoch 1640, val loss: 1.1134496927261353
Epoch 1650, training loss: 62.33534240722656 = 0.029486611485481262 + 10.0 * 6.23058557510376
Epoch 1650, val loss: 1.1174803972244263
Epoch 1660, training loss: 62.37528991699219 = 0.02894742600619793 + 10.0 * 6.2346343994140625
Epoch 1660, val loss: 1.1217530965805054
Epoch 1670, training loss: 62.340911865234375 = 0.028421608731150627 + 10.0 * 6.23124885559082
Epoch 1670, val loss: 1.1258262395858765
Epoch 1680, training loss: 62.33148956298828 = 0.02791159600019455 + 10.0 * 6.230357646942139
Epoch 1680, val loss: 1.129692792892456
Epoch 1690, training loss: 62.36762237548828 = 0.027414927259087563 + 10.0 * 6.234020709991455
Epoch 1690, val loss: 1.133767008781433
Epoch 1700, training loss: 62.32400894165039 = 0.026920201256871223 + 10.0 * 6.229708671569824
Epoch 1700, val loss: 1.1377426385879517
Epoch 1710, training loss: 62.30552673339844 = 0.02645063027739525 + 10.0 * 6.227907657623291
Epoch 1710, val loss: 1.1418802738189697
Epoch 1720, training loss: 62.30266189575195 = 0.025998113676905632 + 10.0 * 6.22766637802124
Epoch 1720, val loss: 1.1459977626800537
Epoch 1730, training loss: 62.3154182434082 = 0.025555606931447983 + 10.0 * 6.2289862632751465
Epoch 1730, val loss: 1.149971842765808
Epoch 1740, training loss: 62.320133209228516 = 0.025113560259342194 + 10.0 * 6.229502201080322
Epoch 1740, val loss: 1.1538156270980835
Epoch 1750, training loss: 62.32248306274414 = 0.024676037952303886 + 10.0 * 6.229780673980713
Epoch 1750, val loss: 1.1571292877197266
Epoch 1760, training loss: 62.305213928222656 = 0.02426144666969776 + 10.0 * 6.228095054626465
Epoch 1760, val loss: 1.161577820777893
Epoch 1770, training loss: 62.31924057006836 = 0.023856353014707565 + 10.0 * 6.229538440704346
Epoch 1770, val loss: 1.1650471687316895
Epoch 1780, training loss: 62.29071044921875 = 0.023451732471585274 + 10.0 * 6.2267255783081055
Epoch 1780, val loss: 1.1693485975265503
Epoch 1790, training loss: 62.2835807800293 = 0.02307034097611904 + 10.0 * 6.226050853729248
Epoch 1790, val loss: 1.1733450889587402
Epoch 1800, training loss: 62.30671691894531 = 0.02269929088652134 + 10.0 * 6.228402137756348
Epoch 1800, val loss: 1.1772266626358032
Epoch 1810, training loss: 62.32571792602539 = 0.022331958636641502 + 10.0 * 6.2303385734558105
Epoch 1810, val loss: 1.1805533170700073
Epoch 1820, training loss: 62.304283142089844 = 0.021968817338347435 + 10.0 * 6.228231430053711
Epoch 1820, val loss: 1.1843836307525635
Epoch 1830, training loss: 62.2812385559082 = 0.02160387672483921 + 10.0 * 6.225963592529297
Epoch 1830, val loss: 1.1882622241973877
Epoch 1840, training loss: 62.26854705810547 = 0.02127133123576641 + 10.0 * 6.224727630615234
Epoch 1840, val loss: 1.1920223236083984
Epoch 1850, training loss: 62.264530181884766 = 0.020939964801073074 + 10.0 * 6.224359035491943
Epoch 1850, val loss: 1.195794939994812
Epoch 1860, training loss: 62.33894729614258 = 0.020616985857486725 + 10.0 * 6.231832981109619
Epoch 1860, val loss: 1.1995620727539062
Epoch 1870, training loss: 62.28666687011719 = 0.020294398069381714 + 10.0 * 6.226637363433838
Epoch 1870, val loss: 1.2028708457946777
Epoch 1880, training loss: 62.27835464477539 = 0.019975122064352036 + 10.0 * 6.2258381843566895
Epoch 1880, val loss: 1.2070497274398804
Epoch 1890, training loss: 62.26752471923828 = 0.019671855494379997 + 10.0 * 6.224785327911377
Epoch 1890, val loss: 1.2105648517608643
Epoch 1900, training loss: 62.25371170043945 = 0.019375422969460487 + 10.0 * 6.223433494567871
Epoch 1900, val loss: 1.2144784927368164
Epoch 1910, training loss: 62.27082824707031 = 0.0190877728164196 + 10.0 * 6.2251739501953125
Epoch 1910, val loss: 1.218132734298706
Epoch 1920, training loss: 62.26278305053711 = 0.0188022218644619 + 10.0 * 6.224398136138916
Epoch 1920, val loss: 1.221638798713684
Epoch 1930, training loss: 62.279266357421875 = 0.01851975917816162 + 10.0 * 6.226074695587158
Epoch 1930, val loss: 1.2250252962112427
Epoch 1940, training loss: 62.2552604675293 = 0.01824861578643322 + 10.0 * 6.223701000213623
Epoch 1940, val loss: 1.2285606861114502
Epoch 1950, training loss: 62.252845764160156 = 0.017980655655264854 + 10.0 * 6.223486423492432
Epoch 1950, val loss: 1.2320784330368042
Epoch 1960, training loss: 62.27085876464844 = 0.017725178971886635 + 10.0 * 6.225313186645508
Epoch 1960, val loss: 1.2356520891189575
Epoch 1970, training loss: 62.249786376953125 = 0.01746782846748829 + 10.0 * 6.223231792449951
Epoch 1970, val loss: 1.239465594291687
Epoch 1980, training loss: 62.23610305786133 = 0.017214089632034302 + 10.0 * 6.221888542175293
Epoch 1980, val loss: 1.2427579164505005
Epoch 1990, training loss: 62.24669647216797 = 0.016975918784737587 + 10.0 * 6.2229719161987305
Epoch 1990, val loss: 1.2460650205612183
Epoch 2000, training loss: 62.2387580871582 = 0.01673833839595318 + 10.0 * 6.222201824188232
Epoch 2000, val loss: 1.2497665882110596
Epoch 2010, training loss: 62.24686813354492 = 0.01650756411254406 + 10.0 * 6.223036289215088
Epoch 2010, val loss: 1.2533600330352783
Epoch 2020, training loss: 62.238651275634766 = 0.01627877540886402 + 10.0 * 6.222237586975098
Epoch 2020, val loss: 1.2562637329101562
Epoch 2030, training loss: 62.26317596435547 = 0.016053540632128716 + 10.0 * 6.224712371826172
Epoch 2030, val loss: 1.2599838972091675
Epoch 2040, training loss: 62.234928131103516 = 0.015824854373931885 + 10.0 * 6.22191047668457
Epoch 2040, val loss: 1.2633112668991089
Epoch 2050, training loss: 62.22960662841797 = 0.015611577779054642 + 10.0 * 6.221399784088135
Epoch 2050, val loss: 1.266703486442566
Epoch 2060, training loss: 62.21965026855469 = 0.015404704958200455 + 10.0 * 6.220424652099609
Epoch 2060, val loss: 1.26990807056427
Epoch 2070, training loss: 62.218971252441406 = 0.015201057307422161 + 10.0 * 6.220376968383789
Epoch 2070, val loss: 1.2734400033950806
Epoch 2080, training loss: 62.225440979003906 = 0.015001000836491585 + 10.0 * 6.221044063568115
Epoch 2080, val loss: 1.276604175567627
Epoch 2090, training loss: 62.23287582397461 = 0.014804140664637089 + 10.0 * 6.22180700302124
Epoch 2090, val loss: 1.2794829607009888
Epoch 2100, training loss: 62.22798538208008 = 0.014608328230679035 + 10.0 * 6.221337795257568
Epoch 2100, val loss: 1.2828313112258911
Epoch 2110, training loss: 62.21949768066406 = 0.014418152160942554 + 10.0 * 6.220507621765137
Epoch 2110, val loss: 1.286271572113037
Epoch 2120, training loss: 62.268707275390625 = 0.014237727969884872 + 10.0 * 6.225447177886963
Epoch 2120, val loss: 1.2893093824386597
Epoch 2130, training loss: 62.228424072265625 = 0.014048632234334946 + 10.0 * 6.221437454223633
Epoch 2130, val loss: 1.2923564910888672
Epoch 2140, training loss: 62.202579498291016 = 0.013863814994692802 + 10.0 * 6.218871593475342
Epoch 2140, val loss: 1.2958563566207886
Epoch 2150, training loss: 62.192161560058594 = 0.013693341054022312 + 10.0 * 6.217846870422363
Epoch 2150, val loss: 1.2988704442977905
Epoch 2160, training loss: 62.19731903076172 = 0.013528067618608475 + 10.0 * 6.218379020690918
Epoch 2160, val loss: 1.3021411895751953
Epoch 2170, training loss: 62.256446838378906 = 0.013362765312194824 + 10.0 * 6.224308490753174
Epoch 2170, val loss: 1.305107593536377
Epoch 2180, training loss: 62.210533142089844 = 0.013191454112529755 + 10.0 * 6.219734191894531
Epoch 2180, val loss: 1.307976245880127
Epoch 2190, training loss: 62.198265075683594 = 0.013031771406531334 + 10.0 * 6.2185235023498535
Epoch 2190, val loss: 1.311228632926941
Epoch 2200, training loss: 62.235660552978516 = 0.01287729199975729 + 10.0 * 6.222278118133545
Epoch 2200, val loss: 1.3139781951904297
Epoch 2210, training loss: 62.22085952758789 = 0.012717696838080883 + 10.0 * 6.220814228057861
Epoch 2210, val loss: 1.3171879053115845
Epoch 2220, training loss: 62.19014358520508 = 0.012561265379190445 + 10.0 * 6.2177581787109375
Epoch 2220, val loss: 1.3202733993530273
Epoch 2230, training loss: 62.18115997314453 = 0.012414038181304932 + 10.0 * 6.216874599456787
Epoch 2230, val loss: 1.3233592510223389
Epoch 2240, training loss: 62.19110870361328 = 0.012270398437976837 + 10.0 * 6.217883586883545
Epoch 2240, val loss: 1.3263319730758667
Epoch 2250, training loss: 62.21498489379883 = 0.012125369161367416 + 10.0 * 6.220285892486572
Epoch 2250, val loss: 1.3294106721878052
Epoch 2260, training loss: 62.18891525268555 = 0.011988433077931404 + 10.0 * 6.2176923751831055
Epoch 2260, val loss: 1.3322718143463135
Epoch 2270, training loss: 62.18463134765625 = 0.011847632005810738 + 10.0 * 6.217278480529785
Epoch 2270, val loss: 1.3352160453796387
Epoch 2280, training loss: 62.18840789794922 = 0.011711720377206802 + 10.0 * 6.217669486999512
Epoch 2280, val loss: 1.3379766941070557
Epoch 2290, training loss: 62.21508026123047 = 0.011580431833863258 + 10.0 * 6.2203497886657715
Epoch 2290, val loss: 1.3405379056930542
Epoch 2300, training loss: 62.176177978515625 = 0.011443046852946281 + 10.0 * 6.216473579406738
Epoch 2300, val loss: 1.3434475660324097
Epoch 2310, training loss: 62.167083740234375 = 0.01131416019052267 + 10.0 * 6.215577125549316
Epoch 2310, val loss: 1.3461692333221436
Epoch 2320, training loss: 62.16225051879883 = 0.011189601384103298 + 10.0 * 6.215106010437012
Epoch 2320, val loss: 1.3492317199707031
Epoch 2330, training loss: 62.161949157714844 = 0.011069661937654018 + 10.0 * 6.215087890625
Epoch 2330, val loss: 1.3522411584854126
Epoch 2340, training loss: 62.216941833496094 = 0.010950053110718727 + 10.0 * 6.220599174499512
Epoch 2340, val loss: 1.3552311658859253
Epoch 2350, training loss: 62.17369079589844 = 0.010826743207871914 + 10.0 * 6.216286659240723
Epoch 2350, val loss: 1.3571232557296753
Epoch 2360, training loss: 62.17962646484375 = 0.010709387250244617 + 10.0 * 6.216891765594482
Epoch 2360, val loss: 1.3600796461105347
Epoch 2370, training loss: 62.178314208984375 = 0.010593751445412636 + 10.0 * 6.216772079467773
Epoch 2370, val loss: 1.362700343132019
Epoch 2380, training loss: 62.18695068359375 = 0.010482891462743282 + 10.0 * 6.217646598815918
Epoch 2380, val loss: 1.3655604124069214
Epoch 2390, training loss: 62.16206359863281 = 0.010366968810558319 + 10.0 * 6.215169429779053
Epoch 2390, val loss: 1.368086576461792
Epoch 2400, training loss: 62.14852523803711 = 0.01025779452174902 + 10.0 * 6.213826656341553
Epoch 2400, val loss: 1.3709837198257446
Epoch 2410, training loss: 62.14649200439453 = 0.010153764858841896 + 10.0 * 6.2136335372924805
Epoch 2410, val loss: 1.3735907077789307
Epoch 2420, training loss: 62.158660888671875 = 0.010053060948848724 + 10.0 * 6.214860916137695
Epoch 2420, val loss: 1.376168131828308
Epoch 2430, training loss: 62.18546676635742 = 0.009948705323040485 + 10.0 * 6.2175517082214355
Epoch 2430, val loss: 1.3784412145614624
Epoch 2440, training loss: 62.154747009277344 = 0.009839979000389576 + 10.0 * 6.21449089050293
Epoch 2440, val loss: 1.3809410333633423
Epoch 2450, training loss: 62.14757537841797 = 0.009735921397805214 + 10.0 * 6.213784217834473
Epoch 2450, val loss: 1.383879542350769
Epoch 2460, training loss: 62.13950729370117 = 0.00963981356471777 + 10.0 * 6.212986946105957
Epoch 2460, val loss: 1.3864775896072388
Epoch 2470, training loss: 62.13784408569336 = 0.009544454514980316 + 10.0 * 6.212830066680908
Epoch 2470, val loss: 1.3889418840408325
Epoch 2480, training loss: 62.23517990112305 = 0.009450610727071762 + 10.0 * 6.222573280334473
Epoch 2480, val loss: 1.3913105726242065
Epoch 2490, training loss: 62.1735954284668 = 0.009357547387480736 + 10.0 * 6.216423988342285
Epoch 2490, val loss: 1.393471360206604
Epoch 2500, training loss: 62.14595031738281 = 0.009257927536964417 + 10.0 * 6.213669300079346
Epoch 2500, val loss: 1.3961682319641113
Epoch 2510, training loss: 62.15003967285156 = 0.009172635152935982 + 10.0 * 6.214086532592773
Epoch 2510, val loss: 1.3985525369644165
Epoch 2520, training loss: 62.17574691772461 = 0.009083161130547523 + 10.0 * 6.216666221618652
Epoch 2520, val loss: 1.4011362791061401
Epoch 2530, training loss: 62.14963912963867 = 0.008992678485810757 + 10.0 * 6.214064598083496
Epoch 2530, val loss: 1.4036052227020264
Epoch 2540, training loss: 62.12803268432617 = 0.008905453607439995 + 10.0 * 6.211912631988525
Epoch 2540, val loss: 1.4058247804641724
Epoch 2550, training loss: 62.123748779296875 = 0.008823607116937637 + 10.0 * 6.211492538452148
Epoch 2550, val loss: 1.408371925354004
Epoch 2560, training loss: 62.16082763671875 = 0.008742294274270535 + 10.0 * 6.215208530426025
Epoch 2560, val loss: 1.41073477268219
Epoch 2570, training loss: 62.123085021972656 = 0.00865970179438591 + 10.0 * 6.211442470550537
Epoch 2570, val loss: 1.4127713441848755
Epoch 2580, training loss: 62.14081954956055 = 0.008578456938266754 + 10.0 * 6.213223934173584
Epoch 2580, val loss: 1.4149267673492432
Epoch 2590, training loss: 62.137184143066406 = 0.008498619310557842 + 10.0 * 6.212868690490723
Epoch 2590, val loss: 1.4174854755401611
Epoch 2600, training loss: 62.155887603759766 = 0.008420241996645927 + 10.0 * 6.214746952056885
Epoch 2600, val loss: 1.419919729232788
Epoch 2610, training loss: 62.12554168701172 = 0.008340653963387012 + 10.0 * 6.2117204666137695
Epoch 2610, val loss: 1.4220548868179321
Epoch 2620, training loss: 62.114280700683594 = 0.008263949304819107 + 10.0 * 6.210601806640625
Epoch 2620, val loss: 1.4246433973312378
Epoch 2630, training loss: 62.11345291137695 = 0.008191362954676151 + 10.0 * 6.210526466369629
Epoch 2630, val loss: 1.4269052743911743
Epoch 2640, training loss: 62.13315963745117 = 0.008120542392134666 + 10.0 * 6.212503910064697
Epoch 2640, val loss: 1.4293714761734009
Epoch 2650, training loss: 62.1510124206543 = 0.008047224953770638 + 10.0 * 6.214296340942383
Epoch 2650, val loss: 1.431036353111267
Epoch 2660, training loss: 62.1242561340332 = 0.007974421605467796 + 10.0 * 6.211628437042236
Epoch 2660, val loss: 1.4331750869750977
Epoch 2670, training loss: 62.11478805541992 = 0.007901783101260662 + 10.0 * 6.210688591003418
Epoch 2670, val loss: 1.4355453252792358
Epoch 2680, training loss: 62.12709045410156 = 0.0078352065756917 + 10.0 * 6.211925506591797
Epoch 2680, val loss: 1.4378528594970703
Epoch 2690, training loss: 62.122982025146484 = 0.007766510359942913 + 10.0 * 6.211521625518799
Epoch 2690, val loss: 1.4399248361587524
Epoch 2700, training loss: 62.125083923339844 = 0.007699916139245033 + 10.0 * 6.211738586425781
Epoch 2700, val loss: 1.4422428607940674
Epoch 2710, training loss: 62.136558532714844 = 0.0076329694129526615 + 10.0 * 6.212892532348633
Epoch 2710, val loss: 1.4444208145141602
Epoch 2720, training loss: 62.10541915893555 = 0.007567476946860552 + 10.0 * 6.209784984588623
Epoch 2720, val loss: 1.4462429285049438
Epoch 2730, training loss: 62.10213851928711 = 0.007506056688725948 + 10.0 * 6.209463119506836
Epoch 2730, val loss: 1.4486744403839111
Epoch 2740, training loss: 62.106666564941406 = 0.007444830145686865 + 10.0 * 6.209921836853027
Epoch 2740, val loss: 1.4507449865341187
Epoch 2750, training loss: 62.14765930175781 = 0.0073835947550833225 + 10.0 * 6.214027404785156
Epoch 2750, val loss: 1.4525032043457031
Epoch 2760, training loss: 62.115684509277344 = 0.007316676899790764 + 10.0 * 6.210836887359619
Epoch 2760, val loss: 1.4544916152954102
Epoch 2770, training loss: 62.09510040283203 = 0.007256125099956989 + 10.0 * 6.208784580230713
Epoch 2770, val loss: 1.456896424293518
Epoch 2780, training loss: 62.09707260131836 = 0.007198972161859274 + 10.0 * 6.208987236022949
Epoch 2780, val loss: 1.459094762802124
Epoch 2790, training loss: 62.13896942138672 = 0.007143470458686352 + 10.0 * 6.21318244934082
Epoch 2790, val loss: 1.4606528282165527
Epoch 2800, training loss: 62.09983825683594 = 0.0070815738290548325 + 10.0 * 6.209275722503662
Epoch 2800, val loss: 1.4629802703857422
Epoch 2810, training loss: 62.09099578857422 = 0.0070228539407253265 + 10.0 * 6.208397388458252
Epoch 2810, val loss: 1.4651552438735962
Epoch 2820, training loss: 62.089962005615234 = 0.0069673508405685425 + 10.0 * 6.20829963684082
Epoch 2820, val loss: 1.4672164916992188
Epoch 2830, training loss: 62.1131706237793 = 0.006915606092661619 + 10.0 * 6.210625648498535
Epoch 2830, val loss: 1.4691606760025024
Epoch 2840, training loss: 62.09575653076172 = 0.006857770029455423 + 10.0 * 6.208889961242676
Epoch 2840, val loss: 1.4711322784423828
Epoch 2850, training loss: 62.13866424560547 = 0.006801482755690813 + 10.0 * 6.213186264038086
Epoch 2850, val loss: 1.4730483293533325
Epoch 2860, training loss: 62.09633255004883 = 0.0067501491867005825 + 10.0 * 6.208958148956299
Epoch 2860, val loss: 1.4748671054840088
Epoch 2870, training loss: 62.08640670776367 = 0.006696376483887434 + 10.0 * 6.207971096038818
Epoch 2870, val loss: 1.476739525794983
Epoch 2880, training loss: 62.09811019897461 = 0.006647697649896145 + 10.0 * 6.209146022796631
Epoch 2880, val loss: 1.4786900281906128
Epoch 2890, training loss: 62.09413528442383 = 0.006596442777663469 + 10.0 * 6.208754062652588
Epoch 2890, val loss: 1.4806452989578247
Epoch 2900, training loss: 62.08924865722656 = 0.00654464028775692 + 10.0 * 6.208270072937012
Epoch 2900, val loss: 1.4829779863357544
Epoch 2910, training loss: 62.09672546386719 = 0.006498291622847319 + 10.0 * 6.2090229988098145
Epoch 2910, val loss: 1.4844629764556885
Epoch 2920, training loss: 62.11757278442383 = 0.006447938270866871 + 10.0 * 6.2111124992370605
Epoch 2920, val loss: 1.4860996007919312
Epoch 2930, training loss: 62.0822868347168 = 0.00639681750908494 + 10.0 * 6.207589149475098
Epoch 2930, val loss: 1.4877302646636963
Epoch 2940, training loss: 62.07606887817383 = 0.006346225738525391 + 10.0 * 6.206972122192383
Epoch 2940, val loss: 1.4902477264404297
Epoch 2950, training loss: 62.06776428222656 = 0.006302110385149717 + 10.0 * 6.206146240234375
Epoch 2950, val loss: 1.4918274879455566
Epoch 2960, training loss: 62.0676383972168 = 0.006257446948438883 + 10.0 * 6.2061381340026855
Epoch 2960, val loss: 1.4936645030975342
Epoch 2970, training loss: 62.10800552368164 = 0.006213534623384476 + 10.0 * 6.210179328918457
Epoch 2970, val loss: 1.495333194732666
Epoch 2980, training loss: 62.09499740600586 = 0.006165891420096159 + 10.0 * 6.208883285522461
Epoch 2980, val loss: 1.4976407289505005
Epoch 2990, training loss: 62.06455993652344 = 0.006120259873569012 + 10.0 * 6.205843925476074
Epoch 2990, val loss: 1.4988698959350586
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 87.90653991699219 = 1.9386142492294312 + 10.0 * 8.596792221069336
Epoch 0, val loss: 1.9425204992294312
Epoch 10, training loss: 87.88734436035156 = 1.9290965795516968 + 10.0 * 8.595824241638184
Epoch 10, val loss: 1.9337531328201294
Epoch 20, training loss: 87.8072280883789 = 1.9175066947937012 + 10.0 * 8.588972091674805
Epoch 20, val loss: 1.922703504562378
Epoch 30, training loss: 87.31886291503906 = 1.9024579524993896 + 10.0 * 8.541640281677246
Epoch 30, val loss: 1.9082212448120117
Epoch 40, training loss: 84.01942443847656 = 1.8844457864761353 + 10.0 * 8.21349811553955
Epoch 40, val loss: 1.8911489248275757
Epoch 50, training loss: 76.20816040039062 = 1.8653984069824219 + 10.0 * 7.434276103973389
Epoch 50, val loss: 1.8736556768417358
Epoch 60, training loss: 73.42191314697266 = 1.8513931035995483 + 10.0 * 7.157052040100098
Epoch 60, val loss: 1.8602806329727173
Epoch 70, training loss: 71.91635131835938 = 1.8386907577514648 + 10.0 * 7.007766246795654
Epoch 70, val loss: 1.8479790687561035
Epoch 80, training loss: 70.76364135742188 = 1.8262970447540283 + 10.0 * 6.893734931945801
Epoch 80, val loss: 1.8363796472549438
Epoch 90, training loss: 70.03791046142578 = 1.815955638885498 + 10.0 * 6.822195529937744
Epoch 90, val loss: 1.8267226219177246
Epoch 100, training loss: 69.53778839111328 = 1.8056466579437256 + 10.0 * 6.773213863372803
Epoch 100, val loss: 1.8168985843658447
Epoch 110, training loss: 68.97988891601562 = 1.796059489250183 + 10.0 * 6.718383312225342
Epoch 110, val loss: 1.8078068494796753
Epoch 120, training loss: 68.47315979003906 = 1.7881134748458862 + 10.0 * 6.66850471496582
Epoch 120, val loss: 1.800100326538086
Epoch 130, training loss: 68.03697204589844 = 1.780491828918457 + 10.0 * 6.62564754486084
Epoch 130, val loss: 1.7925184965133667
Epoch 140, training loss: 67.6901626586914 = 1.7719980478286743 + 10.0 * 6.5918169021606445
Epoch 140, val loss: 1.7843023538589478
Epoch 150, training loss: 67.40975952148438 = 1.762361764907837 + 10.0 * 6.564740180969238
Epoch 150, val loss: 1.7754342555999756
Epoch 160, training loss: 67.15674591064453 = 1.7519757747650146 + 10.0 * 6.540477275848389
Epoch 160, val loss: 1.766188621520996
Epoch 170, training loss: 66.95642852783203 = 1.7410383224487305 + 10.0 * 6.521539211273193
Epoch 170, val loss: 1.7565900087356567
Epoch 180, training loss: 66.76023864746094 = 1.7292057275772095 + 10.0 * 6.503103256225586
Epoch 180, val loss: 1.7463297843933105
Epoch 190, training loss: 66.58106231689453 = 1.7163242101669312 + 10.0 * 6.486473560333252
Epoch 190, val loss: 1.7353531122207642
Epoch 200, training loss: 66.4287109375 = 1.7023558616638184 + 10.0 * 6.472635746002197
Epoch 200, val loss: 1.723532795906067
Epoch 210, training loss: 66.30363464355469 = 1.687160849571228 + 10.0 * 6.461647033691406
Epoch 210, val loss: 1.7107553482055664
Epoch 220, training loss: 66.17816162109375 = 1.6705480813980103 + 10.0 * 6.450761318206787
Epoch 220, val loss: 1.696844220161438
Epoch 230, training loss: 66.05675506591797 = 1.652548909187317 + 10.0 * 6.440420150756836
Epoch 230, val loss: 1.6818554401397705
Epoch 240, training loss: 65.96466064453125 = 1.6331580877304077 + 10.0 * 6.433150768280029
Epoch 240, val loss: 1.665783166885376
Epoch 250, training loss: 65.84275817871094 = 1.6123156547546387 + 10.0 * 6.423044681549072
Epoch 250, val loss: 1.6486129760742188
Epoch 260, training loss: 65.73347473144531 = 1.590065836906433 + 10.0 * 6.414340972900391
Epoch 260, val loss: 1.6302006244659424
Epoch 270, training loss: 65.6318130493164 = 1.5664421319961548 + 10.0 * 6.406537055969238
Epoch 270, val loss: 1.6107347011566162
Epoch 280, training loss: 65.55123138427734 = 1.5414420366287231 + 10.0 * 6.400979042053223
Epoch 280, val loss: 1.5902684926986694
Epoch 290, training loss: 65.45873260498047 = 1.5151029825210571 + 10.0 * 6.394362449645996
Epoch 290, val loss: 1.568932294845581
Epoch 300, training loss: 65.35301971435547 = 1.4875751733779907 + 10.0 * 6.386544704437256
Epoch 300, val loss: 1.546514868736267
Epoch 310, training loss: 65.26481628417969 = 1.4590388536453247 + 10.0 * 6.38057804107666
Epoch 310, val loss: 1.5234707593917847
Epoch 320, training loss: 65.18024444580078 = 1.4295653104782104 + 10.0 * 6.375067710876465
Epoch 320, val loss: 1.4999074935913086
Epoch 330, training loss: 65.09284210205078 = 1.3993653059005737 + 10.0 * 6.36934757232666
Epoch 330, val loss: 1.4758505821228027
Epoch 340, training loss: 65.01947021484375 = 1.368661642074585 + 10.0 * 6.365080833435059
Epoch 340, val loss: 1.4516115188598633
Epoch 350, training loss: 64.95562744140625 = 1.337401032447815 + 10.0 * 6.361822605133057
Epoch 350, val loss: 1.4270685911178589
Epoch 360, training loss: 64.86084747314453 = 1.3060078620910645 + 10.0 * 6.355483531951904
Epoch 360, val loss: 1.402799129486084
Epoch 370, training loss: 64.79122161865234 = 1.2744932174682617 + 10.0 * 6.351672649383545
Epoch 370, val loss: 1.3787144422531128
Epoch 380, training loss: 64.7446517944336 = 1.242948055267334 + 10.0 * 6.350170135498047
Epoch 380, val loss: 1.3549011945724487
Epoch 390, training loss: 64.65925598144531 = 1.2115596532821655 + 10.0 * 6.344769477844238
Epoch 390, val loss: 1.3313779830932617
Epoch 400, training loss: 64.5855484008789 = 1.1804492473602295 + 10.0 * 6.34050989151001
Epoch 400, val loss: 1.308409571647644
Epoch 410, training loss: 64.51921081542969 = 1.1496230363845825 + 10.0 * 6.336958885192871
Epoch 410, val loss: 1.2859008312225342
Epoch 420, training loss: 64.50035095214844 = 1.1190190315246582 + 10.0 * 6.338132858276367
Epoch 420, val loss: 1.2639577388763428
Epoch 430, training loss: 64.39984130859375 = 1.0889257192611694 + 10.0 * 6.331091403961182
Epoch 430, val loss: 1.2425928115844727
Epoch 440, training loss: 64.35340118408203 = 1.0594860315322876 + 10.0 * 6.3293914794921875
Epoch 440, val loss: 1.2221009731292725
Epoch 450, training loss: 64.3047866821289 = 1.0306652784347534 + 10.0 * 6.327412128448486
Epoch 450, val loss: 1.2023478746414185
Epoch 460, training loss: 64.2386703491211 = 1.002350091934204 + 10.0 * 6.323631763458252
Epoch 460, val loss: 1.1831544637680054
Epoch 470, training loss: 64.20668029785156 = 0.9746958017349243 + 10.0 * 6.3231987953186035
Epoch 470, val loss: 1.1648638248443604
Epoch 480, training loss: 64.13424682617188 = 0.947647750377655 + 10.0 * 6.318659782409668
Epoch 480, val loss: 1.1471599340438843
Epoch 490, training loss: 64.09156799316406 = 0.9212550520896912 + 10.0 * 6.317030906677246
Epoch 490, val loss: 1.1302591562271118
Epoch 500, training loss: 64.05461883544922 = 0.8955048322677612 + 10.0 * 6.315911293029785
Epoch 500, val loss: 1.1142123937606812
Epoch 510, training loss: 64.0143051147461 = 0.8703516125679016 + 10.0 * 6.314395427703857
Epoch 510, val loss: 1.0987907648086548
Epoch 520, training loss: 63.964508056640625 = 0.845839262008667 + 10.0 * 6.311866760253906
Epoch 520, val loss: 1.084069848060608
Epoch 530, training loss: 63.906951904296875 = 0.8218547701835632 + 10.0 * 6.308509826660156
Epoch 530, val loss: 1.0699467658996582
Epoch 540, training loss: 63.877742767333984 = 0.7982204556465149 + 10.0 * 6.307951927185059
Epoch 540, val loss: 1.0564817190170288
Epoch 550, training loss: 63.822574615478516 = 0.7752501368522644 + 10.0 * 6.304732322692871
Epoch 550, val loss: 1.0436609983444214
Epoch 560, training loss: 63.794342041015625 = 0.752760112285614 + 10.0 * 6.3041582107543945
Epoch 560, val loss: 1.0315121412277222
Epoch 570, training loss: 63.792327880859375 = 0.7308340072631836 + 10.0 * 6.306149482727051
Epoch 570, val loss: 1.0199440717697144
Epoch 580, training loss: 63.70647430419922 = 0.7093111276626587 + 10.0 * 6.299715995788574
Epoch 580, val loss: 1.0088261365890503
Epoch 590, training loss: 63.669029235839844 = 0.6884975433349609 + 10.0 * 6.29805326461792
Epoch 590, val loss: 0.9985708594322205
Epoch 600, training loss: 63.62807083129883 = 0.6679772734642029 + 10.0 * 6.296009540557861
Epoch 600, val loss: 0.988659679889679
Epoch 610, training loss: 63.631534576416016 = 0.6479014158248901 + 10.0 * 6.29836368560791
Epoch 610, val loss: 0.9793742895126343
Epoch 620, training loss: 63.56923294067383 = 0.6282252073287964 + 10.0 * 6.294100761413574
Epoch 620, val loss: 0.9704732298851013
Epoch 630, training loss: 63.52177429199219 = 0.6090796589851379 + 10.0 * 6.291269302368164
Epoch 630, val loss: 0.9622425436973572
Epoch 640, training loss: 63.52181625366211 = 0.5902953147888184 + 10.0 * 6.293152332305908
Epoch 640, val loss: 0.9543973803520203
Epoch 650, training loss: 63.48566818237305 = 0.5719496011734009 + 10.0 * 6.291371822357178
Epoch 650, val loss: 0.9470601677894592
Epoch 660, training loss: 63.425350189208984 = 0.5539550185203552 + 10.0 * 6.287139415740967
Epoch 660, val loss: 0.9400736689567566
Epoch 670, training loss: 63.404457092285156 = 0.5364142060279846 + 10.0 * 6.28680419921875
Epoch 670, val loss: 0.9335612654685974
Epoch 680, training loss: 63.388328552246094 = 0.5192592144012451 + 10.0 * 6.286906719207764
Epoch 680, val loss: 0.9274399876594543
Epoch 690, training loss: 63.34062194824219 = 0.5024541020393372 + 10.0 * 6.283816814422607
Epoch 690, val loss: 0.9215918779373169
Epoch 700, training loss: 63.31745529174805 = 0.4860720932483673 + 10.0 * 6.283138275146484
Epoch 700, val loss: 0.9162594079971313
Epoch 710, training loss: 63.289024353027344 = 0.4700912833213806 + 10.0 * 6.281893253326416
Epoch 710, val loss: 0.9112911224365234
Epoch 720, training loss: 63.27656173706055 = 0.454478919506073 + 10.0 * 6.282208442687988
Epoch 720, val loss: 0.9066194891929626
Epoch 730, training loss: 63.228206634521484 = 0.43925777077674866 + 10.0 * 6.278894901275635
Epoch 730, val loss: 0.9023714661598206
Epoch 740, training loss: 63.20237350463867 = 0.424415647983551 + 10.0 * 6.277795791625977
Epoch 740, val loss: 0.8984484672546387
Epoch 750, training loss: 63.21128463745117 = 0.4099960923194885 + 10.0 * 6.2801289558410645
Epoch 750, val loss: 0.894856870174408
Epoch 760, training loss: 63.16808319091797 = 0.3958591818809509 + 10.0 * 6.277222633361816
Epoch 760, val loss: 0.8917386531829834
Epoch 770, training loss: 63.12590789794922 = 0.382240355014801 + 10.0 * 6.274366855621338
Epoch 770, val loss: 0.8888553977012634
Epoch 780, training loss: 63.1069450378418 = 0.36903178691864014 + 10.0 * 6.273791313171387
Epoch 780, val loss: 0.8864089250564575
Epoch 790, training loss: 63.18683624267578 = 0.35621026158332825 + 10.0 * 6.283062934875488
Epoch 790, val loss: 0.884270191192627
Epoch 800, training loss: 63.08109664916992 = 0.34363871812820435 + 10.0 * 6.273745536804199
Epoch 800, val loss: 0.8822956085205078
Epoch 810, training loss: 63.03672409057617 = 0.33158376812934875 + 10.0 * 6.270514011383057
Epoch 810, val loss: 0.8808123469352722
Epoch 820, training loss: 63.011356353759766 = 0.3200342357158661 + 10.0 * 6.269132137298584
Epoch 820, val loss: 0.8797004222869873
Epoch 830, training loss: 62.98991394042969 = 0.30887025594711304 + 10.0 * 6.268104553222656
Epoch 830, val loss: 0.8788101673126221
Epoch 840, training loss: 62.9674186706543 = 0.29803988337516785 + 10.0 * 6.266938209533691
Epoch 840, val loss: 0.878219723701477
Epoch 850, training loss: 62.99222946166992 = 0.2875799536705017 + 10.0 * 6.270464897155762
Epoch 850, val loss: 0.8777421116828918
Epoch 860, training loss: 63.010169982910156 = 0.2773975431919098 + 10.0 * 6.273277282714844
Epoch 860, val loss: 0.8775651454925537
Epoch 870, training loss: 62.94105529785156 = 0.26759809255599976 + 10.0 * 6.267345905303955
Epoch 870, val loss: 0.8776189684867859
Epoch 880, training loss: 62.90296173095703 = 0.2582913041114807 + 10.0 * 6.264466762542725
Epoch 880, val loss: 0.8780527114868164
Epoch 890, training loss: 62.87905502319336 = 0.24933964014053345 + 10.0 * 6.2629714012146
Epoch 890, val loss: 0.8787280321121216
Epoch 900, training loss: 62.86054229736328 = 0.24074409902095795 + 10.0 * 6.261979579925537
Epoch 900, val loss: 0.8796040415763855
Epoch 910, training loss: 62.94197463989258 = 0.23248697817325592 + 10.0 * 6.270948886871338
Epoch 910, val loss: 0.8806830644607544
Epoch 920, training loss: 62.8716926574707 = 0.2244042307138443 + 10.0 * 6.264729022979736
Epoch 920, val loss: 0.8817933201789856
Epoch 930, training loss: 62.83411407470703 = 0.2167159467935562 + 10.0 * 6.261739730834961
Epoch 930, val loss: 0.8833798766136169
Epoch 940, training loss: 62.80388259887695 = 0.2094440758228302 + 10.0 * 6.259443759918213
Epoch 940, val loss: 0.8850734829902649
Epoch 950, training loss: 62.78277587890625 = 0.20247972011566162 + 10.0 * 6.258029460906982
Epoch 950, val loss: 0.8870130777359009
Epoch 960, training loss: 62.76921081542969 = 0.1957862228155136 + 10.0 * 6.257342338562012
Epoch 960, val loss: 0.8891683220863342
Epoch 970, training loss: 62.82267761230469 = 0.1893458515405655 + 10.0 * 6.263333320617676
Epoch 970, val loss: 0.8914719223976135
Epoch 980, training loss: 62.77964782714844 = 0.1831447184085846 + 10.0 * 6.259650230407715
Epoch 980, val loss: 0.8938519358634949
Epoch 990, training loss: 62.73372268676758 = 0.1771664023399353 + 10.0 * 6.255655765533447
Epoch 990, val loss: 0.8965542316436768
Epoch 1000, training loss: 62.798362731933594 = 0.17148227989673615 + 10.0 * 6.262688159942627
Epoch 1000, val loss: 0.8992456793785095
Epoch 1010, training loss: 62.74148178100586 = 0.1659686267375946 + 10.0 * 6.257551193237305
Epoch 1010, val loss: 0.9023075103759766
Epoch 1020, training loss: 62.70093536376953 = 0.16070567071437836 + 10.0 * 6.25402307510376
Epoch 1020, val loss: 0.9054294228553772
Epoch 1030, training loss: 62.68281173706055 = 0.15568137168884277 + 10.0 * 6.252713203430176
Epoch 1030, val loss: 0.908697247505188
Epoch 1040, training loss: 62.66972351074219 = 0.15084068477153778 + 10.0 * 6.251888275146484
Epoch 1040, val loss: 0.9122200608253479
Epoch 1050, training loss: 62.67110824584961 = 0.146201953291893 + 10.0 * 6.252490520477295
Epoch 1050, val loss: 0.9156646132469177
Epoch 1060, training loss: 62.65508270263672 = 0.14165595173835754 + 10.0 * 6.2513427734375
Epoch 1060, val loss: 0.9192726612091064
Epoch 1070, training loss: 62.659767150878906 = 0.13731659948825836 + 10.0 * 6.25224494934082
Epoch 1070, val loss: 0.9229491353034973
Epoch 1080, training loss: 62.63345718383789 = 0.13314615190029144 + 10.0 * 6.250031471252441
Epoch 1080, val loss: 0.9268988966941833
Epoch 1090, training loss: 62.617759704589844 = 0.12914395332336426 + 10.0 * 6.248861789703369
Epoch 1090, val loss: 0.9309360384941101
Epoch 1100, training loss: 62.68745803833008 = 0.12530186772346497 + 10.0 * 6.256215572357178
Epoch 1100, val loss: 0.9350761771202087
Epoch 1110, training loss: 62.6400260925293 = 0.12156345695257187 + 10.0 * 6.2518463134765625
Epoch 1110, val loss: 0.9389034509658813
Epoch 1120, training loss: 62.6200065612793 = 0.11795024573802948 + 10.0 * 6.2502055168151855
Epoch 1120, val loss: 0.943150520324707
Epoch 1130, training loss: 62.58351516723633 = 0.11449956893920898 + 10.0 * 6.246901512145996
Epoch 1130, val loss: 0.9474969506263733
Epoch 1140, training loss: 62.579219818115234 = 0.11119262874126434 + 10.0 * 6.246802806854248
Epoch 1140, val loss: 0.9518008828163147
Epoch 1150, training loss: 62.587398529052734 = 0.10799292474985123 + 10.0 * 6.247940540313721
Epoch 1150, val loss: 0.9561497569084167
Epoch 1160, training loss: 62.55733871459961 = 0.10489313304424286 + 10.0 * 6.24524450302124
Epoch 1160, val loss: 0.9605459570884705
Epoch 1170, training loss: 62.57829666137695 = 0.1018943041563034 + 10.0 * 6.247640132904053
Epoch 1170, val loss: 0.9651457667350769
Epoch 1180, training loss: 62.56940841674805 = 0.09901192039251328 + 10.0 * 6.247039794921875
Epoch 1180, val loss: 0.9694453477859497
Epoch 1190, training loss: 62.54716491699219 = 0.09620136767625809 + 10.0 * 6.245096206665039
Epoch 1190, val loss: 0.9741934537887573
Epoch 1200, training loss: 62.538936614990234 = 0.09353245794773102 + 10.0 * 6.244540214538574
Epoch 1200, val loss: 0.978701651096344
Epoch 1210, training loss: 62.536991119384766 = 0.0909401997923851 + 10.0 * 6.24460506439209
Epoch 1210, val loss: 0.9834304451942444
Epoch 1220, training loss: 62.511749267578125 = 0.08844068646430969 + 10.0 * 6.242331027984619
Epoch 1220, val loss: 0.9880468249320984
Epoch 1230, training loss: 62.50160598754883 = 0.08602134883403778 + 10.0 * 6.24155855178833
Epoch 1230, val loss: 0.99281245470047
Epoch 1240, training loss: 62.53163528442383 = 0.08370848745107651 + 10.0 * 6.244792461395264
Epoch 1240, val loss: 0.9973041415214539
Epoch 1250, training loss: 62.53581237792969 = 0.0814155787229538 + 10.0 * 6.245439529418945
Epoch 1250, val loss: 1.0021613836288452
Epoch 1260, training loss: 62.49467849731445 = 0.07920821011066437 + 10.0 * 6.241547107696533
Epoch 1260, val loss: 1.006740689277649
Epoch 1270, training loss: 62.47915267944336 = 0.07710056751966476 + 10.0 * 6.24020528793335
Epoch 1270, val loss: 1.01160728931427
Epoch 1280, training loss: 62.46805191040039 = 0.07507355511188507 + 10.0 * 6.239297866821289
Epoch 1280, val loss: 1.0165250301361084
Epoch 1290, training loss: 62.53489303588867 = 0.07312209904193878 + 10.0 * 6.246176719665527
Epoch 1290, val loss: 1.0212862491607666
Epoch 1300, training loss: 62.49234390258789 = 0.07120828330516815 + 10.0 * 6.2421135902404785
Epoch 1300, val loss: 1.025784969329834
Epoch 1310, training loss: 62.473731994628906 = 0.06934278458356857 + 10.0 * 6.240438938140869
Epoch 1310, val loss: 1.0307577848434448
Epoch 1320, training loss: 62.45175552368164 = 0.06757432967424393 + 10.0 * 6.238418102264404
Epoch 1320, val loss: 1.035431981086731
Epoch 1330, training loss: 62.46090316772461 = 0.06585898995399475 + 10.0 * 6.239504337310791
Epoch 1330, val loss: 1.0402095317840576
Epoch 1340, training loss: 62.43095016479492 = 0.06419260054826736 + 10.0 * 6.23667573928833
Epoch 1340, val loss: 1.0451387166976929
Epoch 1350, training loss: 62.44954299926758 = 0.06259026378393173 + 10.0 * 6.23869514465332
Epoch 1350, val loss: 1.0498690605163574
Epoch 1360, training loss: 62.466793060302734 = 0.061032235622406006 + 10.0 * 6.240576267242432
Epoch 1360, val loss: 1.0545028448104858
Epoch 1370, training loss: 62.43088912963867 = 0.0595153272151947 + 10.0 * 6.237137317657471
Epoch 1370, val loss: 1.059227705001831
Epoch 1380, training loss: 62.41143035888672 = 0.05804500728845596 + 10.0 * 6.2353386878967285
Epoch 1380, val loss: 1.0641683340072632
Epoch 1390, training loss: 62.40388870239258 = 0.056647833436727524 + 10.0 * 6.234724044799805
Epoch 1390, val loss: 1.0688687562942505
Epoch 1400, training loss: 62.41908264160156 = 0.055296048521995544 + 10.0 * 6.2363786697387695
Epoch 1400, val loss: 1.0736247301101685
Epoch 1410, training loss: 62.41823959350586 = 0.053965356200933456 + 10.0 * 6.236427307128906
Epoch 1410, val loss: 1.078261137008667
Epoch 1420, training loss: 62.392616271972656 = 0.052678465843200684 + 10.0 * 6.233994007110596
Epoch 1420, val loss: 1.0828962326049805
Epoch 1430, training loss: 62.38835525512695 = 0.051440540701150894 + 10.0 * 6.233691215515137
Epoch 1430, val loss: 1.087787389755249
Epoch 1440, training loss: 62.415748596191406 = 0.050253406167030334 + 10.0 * 6.236549377441406
Epoch 1440, val loss: 1.0924967527389526
Epoch 1450, training loss: 62.37571334838867 = 0.049088604748249054 + 10.0 * 6.232662677764893
Epoch 1450, val loss: 1.0971628427505493
Epoch 1460, training loss: 62.37306213378906 = 0.04796785116195679 + 10.0 * 6.232509613037109
Epoch 1460, val loss: 1.1019341945648193
Epoch 1470, training loss: 62.406681060791016 = 0.046887531876564026 + 10.0 * 6.2359795570373535
Epoch 1470, val loss: 1.1065340042114258
Epoch 1480, training loss: 62.39426040649414 = 0.04581122100353241 + 10.0 * 6.234845161437988
Epoch 1480, val loss: 1.111155390739441
Epoch 1490, training loss: 62.3745231628418 = 0.04476916790008545 + 10.0 * 6.232975482940674
Epoch 1490, val loss: 1.1158959865570068
Epoch 1500, training loss: 62.348960876464844 = 0.04378043860197067 + 10.0 * 6.230517864227295
Epoch 1500, val loss: 1.1205874681472778
Epoch 1510, training loss: 62.35831069946289 = 0.04282977432012558 + 10.0 * 6.231547832489014
Epoch 1510, val loss: 1.125309944152832
Epoch 1520, training loss: 62.38392639160156 = 0.041898973286151886 + 10.0 * 6.2342023849487305
Epoch 1520, val loss: 1.1298210620880127
Epoch 1530, training loss: 62.35078048706055 = 0.040983423590660095 + 10.0 * 6.2309794425964355
Epoch 1530, val loss: 1.1342391967773438
Epoch 1540, training loss: 62.33975601196289 = 0.04010386765003204 + 10.0 * 6.2299652099609375
Epoch 1540, val loss: 1.1391109228134155
Epoch 1550, training loss: 62.3584098815918 = 0.039258137345314026 + 10.0 * 6.231915473937988
Epoch 1550, val loss: 1.1437264680862427
Epoch 1560, training loss: 62.33266067504883 = 0.038441408425569534 + 10.0 * 6.229422092437744
Epoch 1560, val loss: 1.1479893922805786
Epoch 1570, training loss: 62.33115005493164 = 0.03764395788311958 + 10.0 * 6.229350566864014
Epoch 1570, val loss: 1.1526000499725342
Epoch 1580, training loss: 62.34276580810547 = 0.03685285523533821 + 10.0 * 6.230591297149658
Epoch 1580, val loss: 1.1572973728179932
Epoch 1590, training loss: 62.35032653808594 = 0.036104682832956314 + 10.0 * 6.231421947479248
Epoch 1590, val loss: 1.1615923643112183
Epoch 1600, training loss: 62.31968688964844 = 0.03535505384206772 + 10.0 * 6.228433132171631
Epoch 1600, val loss: 1.1661717891693115
Epoch 1610, training loss: 62.30941390991211 = 0.03465265408158302 + 10.0 * 6.227476119995117
Epoch 1610, val loss: 1.1706103086471558
Epoch 1620, training loss: 62.34135437011719 = 0.033961500972509384 + 10.0 * 6.230739116668701
Epoch 1620, val loss: 1.1751073598861694
Epoch 1630, training loss: 62.30131912231445 = 0.033282600343227386 + 10.0 * 6.226803779602051
Epoch 1630, val loss: 1.1794794797897339
Epoch 1640, training loss: 62.30157470703125 = 0.032630931586027145 + 10.0 * 6.226894378662109
Epoch 1640, val loss: 1.1838141679763794
Epoch 1650, training loss: 62.33689880371094 = 0.03199736028909683 + 10.0 * 6.230490207672119
Epoch 1650, val loss: 1.1881170272827148
Epoch 1660, training loss: 62.28907775878906 = 0.031373701989650726 + 10.0 * 6.225770473480225
Epoch 1660, val loss: 1.1925292015075684
Epoch 1670, training loss: 62.28428649902344 = 0.030772937461733818 + 10.0 * 6.225351333618164
Epoch 1670, val loss: 1.1968045234680176
Epoch 1680, training loss: 62.32916259765625 = 0.030201025307178497 + 10.0 * 6.229896068572998
Epoch 1680, val loss: 1.200968861579895
Epoch 1690, training loss: 62.2822265625 = 0.029612483456730843 + 10.0 * 6.225261211395264
Epoch 1690, val loss: 1.2054541110992432
Epoch 1700, training loss: 62.27568054199219 = 0.029060019180178642 + 10.0 * 6.224661827087402
Epoch 1700, val loss: 1.2094374895095825
Epoch 1710, training loss: 62.269405364990234 = 0.028519311919808388 + 10.0 * 6.224088668823242
Epoch 1710, val loss: 1.2140085697174072
Epoch 1720, training loss: 62.26457977294922 = 0.028005599975585938 + 10.0 * 6.223657131195068
Epoch 1720, val loss: 1.2181071043014526
Epoch 1730, training loss: 62.35387420654297 = 0.027500003576278687 + 10.0 * 6.232637405395508
Epoch 1730, val loss: 1.2223409414291382
Epoch 1740, training loss: 62.32279586791992 = 0.026994820684194565 + 10.0 * 6.229579925537109
Epoch 1740, val loss: 1.2262134552001953
Epoch 1750, training loss: 62.258522033691406 = 0.02649867907166481 + 10.0 * 6.223202705383301
Epoch 1750, val loss: 1.2303993701934814
Epoch 1760, training loss: 62.257328033447266 = 0.026031116023659706 + 10.0 * 6.223129749298096
Epoch 1760, val loss: 1.2345560789108276
Epoch 1770, training loss: 62.29033660888672 = 0.025580057874321938 + 10.0 * 6.226475715637207
Epoch 1770, val loss: 1.238620400428772
Epoch 1780, training loss: 62.24732971191406 = 0.025124505162239075 + 10.0 * 6.222220420837402
Epoch 1780, val loss: 1.2427642345428467
Epoch 1790, training loss: 62.25247573852539 = 0.024687783792614937 + 10.0 * 6.222778797149658
Epoch 1790, val loss: 1.246726393699646
Epoch 1800, training loss: 62.27708053588867 = 0.024268459528684616 + 10.0 * 6.225281238555908
Epoch 1800, val loss: 1.2507036924362183
Epoch 1810, training loss: 62.23897171020508 = 0.02385025843977928 + 10.0 * 6.221512317657471
Epoch 1810, val loss: 1.2547610998153687
Epoch 1820, training loss: 62.24226760864258 = 0.02345145121216774 + 10.0 * 6.22188138961792
Epoch 1820, val loss: 1.2587965726852417
Epoch 1830, training loss: 62.29653549194336 = 0.02306615188717842 + 10.0 * 6.227346897125244
Epoch 1830, val loss: 1.2626160383224487
Epoch 1840, training loss: 62.24122619628906 = 0.022666025906801224 + 10.0 * 6.221856117248535
Epoch 1840, val loss: 1.2665600776672363
Epoch 1850, training loss: 62.2346076965332 = 0.022294938564300537 + 10.0 * 6.221231460571289
Epoch 1850, val loss: 1.2703202962875366
Epoch 1860, training loss: 62.23997116088867 = 0.021929346024990082 + 10.0 * 6.221804141998291
Epoch 1860, val loss: 1.2743802070617676
Epoch 1870, training loss: 62.234130859375 = 0.021576162427663803 + 10.0 * 6.221255302429199
Epoch 1870, val loss: 1.2781635522842407
Epoch 1880, training loss: 62.22450256347656 = 0.02122863195836544 + 10.0 * 6.220327377319336
Epoch 1880, val loss: 1.2819514274597168
Epoch 1890, training loss: 62.254737854003906 = 0.020893707871437073 + 10.0 * 6.223384380340576
Epoch 1890, val loss: 1.2855843305587769
Epoch 1900, training loss: 62.2276496887207 = 0.02055794559419155 + 10.0 * 6.220709323883057
Epoch 1900, val loss: 1.2893282175064087
Epoch 1910, training loss: 62.22934341430664 = 0.020235557109117508 + 10.0 * 6.220910549163818
Epoch 1910, val loss: 1.2930277585983276
Epoch 1920, training loss: 62.23649215698242 = 0.01991865783929825 + 10.0 * 6.2216572761535645
Epoch 1920, val loss: 1.2966705560684204
Epoch 1930, training loss: 62.24828338623047 = 0.019602639600634575 + 10.0 * 6.222867965698242
Epoch 1930, val loss: 1.3007193803787231
Epoch 1940, training loss: 62.20767593383789 = 0.019299020990729332 + 10.0 * 6.218837738037109
Epoch 1940, val loss: 1.3040803670883179
Epoch 1950, training loss: 62.19593811035156 = 0.01900389790534973 + 10.0 * 6.217693328857422
Epoch 1950, val loss: 1.3079990148544312
Epoch 1960, training loss: 62.19865036010742 = 0.01872202195227146 + 10.0 * 6.217992782592773
Epoch 1960, val loss: 1.3116825819015503
Epoch 1970, training loss: 62.245845794677734 = 0.018447397276759148 + 10.0 * 6.2227396965026855
Epoch 1970, val loss: 1.3151978254318237
Epoch 1980, training loss: 62.232547760009766 = 0.01816755346953869 + 10.0 * 6.221437931060791
Epoch 1980, val loss: 1.3184525966644287
Epoch 1990, training loss: 62.2074089050293 = 0.017891017720103264 + 10.0 * 6.21895170211792
Epoch 1990, val loss: 1.3221533298492432
Epoch 2000, training loss: 62.19316101074219 = 0.017631033435463905 + 10.0 * 6.21755313873291
Epoch 2000, val loss: 1.3256020545959473
Epoch 2010, training loss: 62.19587707519531 = 0.01737794652581215 + 10.0 * 6.2178497314453125
Epoch 2010, val loss: 1.3291691541671753
Epoch 2020, training loss: 62.224308013916016 = 0.017131801694631577 + 10.0 * 6.220717430114746
Epoch 2020, val loss: 1.3324570655822754
Epoch 2030, training loss: 62.19852066040039 = 0.01687808148562908 + 10.0 * 6.218164443969727
Epoch 2030, val loss: 1.3362516164779663
Epoch 2040, training loss: 62.194305419921875 = 0.016642456874251366 + 10.0 * 6.217766284942627
Epoch 2040, val loss: 1.3394439220428467
Epoch 2050, training loss: 62.19087600708008 = 0.016405800357460976 + 10.0 * 6.217446804046631
Epoch 2050, val loss: 1.3429008722305298
Epoch 2060, training loss: 62.1927375793457 = 0.016171952709555626 + 10.0 * 6.21765661239624
Epoch 2060, val loss: 1.3463934659957886
Epoch 2070, training loss: 62.2186164855957 = 0.015950553119182587 + 10.0 * 6.220266819000244
Epoch 2070, val loss: 1.349535584449768
Epoch 2080, training loss: 62.188568115234375 = 0.015725258737802505 + 10.0 * 6.217284202575684
Epoch 2080, val loss: 1.3527889251708984
Epoch 2090, training loss: 62.172786712646484 = 0.015506469644606113 + 10.0 * 6.215727806091309
Epoch 2090, val loss: 1.3563395738601685
Epoch 2100, training loss: 62.1689453125 = 0.015300675295293331 + 10.0 * 6.215364456176758
Epoch 2100, val loss: 1.359648585319519
Epoch 2110, training loss: 62.189876556396484 = 0.015098725445568562 + 10.0 * 6.217477798461914
Epoch 2110, val loss: 1.36293363571167
Epoch 2120, training loss: 62.17070007324219 = 0.014894240535795689 + 10.0 * 6.215580940246582
Epoch 2120, val loss: 1.3660107851028442
Epoch 2130, training loss: 62.17778396606445 = 0.014696533791720867 + 10.0 * 6.21630859375
Epoch 2130, val loss: 1.3691521883010864
Epoch 2140, training loss: 62.16801834106445 = 0.014503482729196548 + 10.0 * 6.215351581573486
Epoch 2140, val loss: 1.3724396228790283
Epoch 2150, training loss: 62.18061065673828 = 0.014315743930637836 + 10.0 * 6.216629505157471
Epoch 2150, val loss: 1.375503420829773
Epoch 2160, training loss: 62.256683349609375 = 0.014121905900537968 + 10.0 * 6.2242560386657715
Epoch 2160, val loss: 1.3788120746612549
Epoch 2170, training loss: 62.16432571411133 = 0.013935347087681293 + 10.0 * 6.215039253234863
Epoch 2170, val loss: 1.3819265365600586
Epoch 2180, training loss: 62.152339935302734 = 0.013749636709690094 + 10.0 * 6.2138590812683105
Epoch 2180, val loss: 1.3853074312210083
Epoch 2190, training loss: 62.146766662597656 = 0.01358130294829607 + 10.0 * 6.213318824768066
Epoch 2190, val loss: 1.3884233236312866
Epoch 2200, training loss: 62.141815185546875 = 0.013411303050816059 + 10.0 * 6.2128400802612305
Epoch 2200, val loss: 1.391554594039917
Epoch 2210, training loss: 62.15938186645508 = 0.013251357711851597 + 10.0 * 6.21461296081543
Epoch 2210, val loss: 1.3944193124771118
Epoch 2220, training loss: 62.164833068847656 = 0.01307816244661808 + 10.0 * 6.215175628662109
Epoch 2220, val loss: 1.3975368738174438
Epoch 2230, training loss: 62.16740798950195 = 0.01291122380644083 + 10.0 * 6.215449333190918
Epoch 2230, val loss: 1.4001412391662598
Epoch 2240, training loss: 62.14332962036133 = 0.012747353874146938 + 10.0 * 6.213057994842529
Epoch 2240, val loss: 1.403601884841919
Epoch 2250, training loss: 62.132232666015625 = 0.012595685198903084 + 10.0 * 6.211963653564453
Epoch 2250, val loss: 1.4067238569259644
Epoch 2260, training loss: 62.13056182861328 = 0.01244721096009016 + 10.0 * 6.211811542510986
Epoch 2260, val loss: 1.4096152782440186
Epoch 2270, training loss: 62.153438568115234 = 0.012301463633775711 + 10.0 * 6.214113712310791
Epoch 2270, val loss: 1.412437081336975
Epoch 2280, training loss: 62.14094543457031 = 0.012154795229434967 + 10.0 * 6.212879180908203
Epoch 2280, val loss: 1.4152045249938965
Epoch 2290, training loss: 62.16691207885742 = 0.0120094558224082 + 10.0 * 6.215490341186523
Epoch 2290, val loss: 1.4181017875671387
Epoch 2300, training loss: 62.124595642089844 = 0.011859958991408348 + 10.0 * 6.211273670196533
Epoch 2300, val loss: 1.421195149421692
Epoch 2310, training loss: 62.13097381591797 = 0.011720546521246433 + 10.0 * 6.211925506591797
Epoch 2310, val loss: 1.424189805984497
Epoch 2320, training loss: 62.18268585205078 = 0.011589723639190197 + 10.0 * 6.217109680175781
Epoch 2320, val loss: 1.4268699884414673
Epoch 2330, training loss: 62.127159118652344 = 0.011451716534793377 + 10.0 * 6.211570739746094
Epoch 2330, val loss: 1.4295305013656616
Epoch 2340, training loss: 62.12075424194336 = 0.01132029015570879 + 10.0 * 6.210943222045898
Epoch 2340, val loss: 1.4324238300323486
Epoch 2350, training loss: 62.128028869628906 = 0.011193434707820415 + 10.0 * 6.211683750152588
Epoch 2350, val loss: 1.4350565671920776
Epoch 2360, training loss: 62.147953033447266 = 0.01107018906623125 + 10.0 * 6.213688373565674
Epoch 2360, val loss: 1.4378001689910889
Epoch 2370, training loss: 62.121009826660156 = 0.01094052754342556 + 10.0 * 6.211007118225098
Epoch 2370, val loss: 1.4405580759048462
Epoch 2380, training loss: 62.13887405395508 = 0.010820972733199596 + 10.0 * 6.212805271148682
Epoch 2380, val loss: 1.4431638717651367
Epoch 2390, training loss: 62.12052536010742 = 0.010700549930334091 + 10.0 * 6.210982322692871
Epoch 2390, val loss: 1.4457404613494873
Epoch 2400, training loss: 62.13185119628906 = 0.01058370340615511 + 10.0 * 6.212126731872559
Epoch 2400, val loss: 1.4484018087387085
Epoch 2410, training loss: 62.128047943115234 = 0.010467533022165298 + 10.0 * 6.211758136749268
Epoch 2410, val loss: 1.4511152505874634
Epoch 2420, training loss: 62.11278533935547 = 0.010350790806114674 + 10.0 * 6.2102437019348145
Epoch 2420, val loss: 1.4537540674209595
Epoch 2430, training loss: 62.140052795410156 = 0.01023957971483469 + 10.0 * 6.212981224060059
Epoch 2430, val loss: 1.4564647674560547
Epoch 2440, training loss: 62.11730194091797 = 0.01013138983398676 + 10.0 * 6.21071720123291
Epoch 2440, val loss: 1.4585692882537842
Epoch 2450, training loss: 62.13418960571289 = 0.010020961053669453 + 10.0 * 6.212416648864746
Epoch 2450, val loss: 1.4612796306610107
Epoch 2460, training loss: 62.103939056396484 = 0.00991396326571703 + 10.0 * 6.209402561187744
Epoch 2460, val loss: 1.4638698101043701
Epoch 2470, training loss: 62.10325622558594 = 0.009811241179704666 + 10.0 * 6.209344387054443
Epoch 2470, val loss: 1.4663859605789185
Epoch 2480, training loss: 62.124534606933594 = 0.009709899313747883 + 10.0 * 6.211482524871826
Epoch 2480, val loss: 1.4689284563064575
Epoch 2490, training loss: 62.11839294433594 = 0.00960988737642765 + 10.0 * 6.210878372192383
Epoch 2490, val loss: 1.4711613655090332
Epoch 2500, training loss: 62.0928955078125 = 0.009507262147963047 + 10.0 * 6.208338737487793
Epoch 2500, val loss: 1.4735338687896729
Epoch 2510, training loss: 62.09144973754883 = 0.009410925209522247 + 10.0 * 6.2082037925720215
Epoch 2510, val loss: 1.4761059284210205
Epoch 2520, training loss: 62.11885070800781 = 0.009320329874753952 + 10.0 * 6.210953235626221
Epoch 2520, val loss: 1.4784554243087769
Epoch 2530, training loss: 62.113712310791016 = 0.009221858344972134 + 10.0 * 6.21044921875
Epoch 2530, val loss: 1.4807761907577515
Epoch 2540, training loss: 62.0870361328125 = 0.009124528616666794 + 10.0 * 6.207791328430176
Epoch 2540, val loss: 1.48307204246521
Epoch 2550, training loss: 62.083656311035156 = 0.009032449685037136 + 10.0 * 6.207462310791016
Epoch 2550, val loss: 1.4855892658233643
Epoch 2560, training loss: 62.080360412597656 = 0.008945610374212265 + 10.0 * 6.207141399383545
Epoch 2560, val loss: 1.4879778623580933
Epoch 2570, training loss: 62.08954620361328 = 0.008860573172569275 + 10.0 * 6.208068370819092
Epoch 2570, val loss: 1.4903571605682373
Epoch 2580, training loss: 62.121524810791016 = 0.00877559743821621 + 10.0 * 6.211275100708008
Epoch 2580, val loss: 1.4924397468566895
Epoch 2590, training loss: 62.114044189453125 = 0.008689447306096554 + 10.0 * 6.210535526275635
Epoch 2590, val loss: 1.4947361946105957
Epoch 2600, training loss: 62.09920883178711 = 0.008601394481956959 + 10.0 * 6.2090606689453125
Epoch 2600, val loss: 1.4971932172775269
Epoch 2610, training loss: 62.08859634399414 = 0.008519281633198261 + 10.0 * 6.2080078125
Epoch 2610, val loss: 1.499370813369751
Epoch 2620, training loss: 62.07878112792969 = 0.008438545279204845 + 10.0 * 6.207034111022949
Epoch 2620, val loss: 1.5016920566558838
Epoch 2630, training loss: 62.08576583862305 = 0.0083624804392457 + 10.0 * 6.207740306854248
Epoch 2630, val loss: 1.5037537813186646
Epoch 2640, training loss: 62.123817443847656 = 0.008284799754619598 + 10.0 * 6.21155309677124
Epoch 2640, val loss: 1.5057259798049927
Epoch 2650, training loss: 62.11248779296875 = 0.008204971440136433 + 10.0 * 6.210428237915039
Epoch 2650, val loss: 1.5079487562179565
Epoch 2660, training loss: 62.07804489135742 = 0.008122298866510391 + 10.0 * 6.206992149353027
Epoch 2660, val loss: 1.5100501775741577
Epoch 2670, training loss: 62.066139221191406 = 0.008048929274082184 + 10.0 * 6.205809116363525
Epoch 2670, val loss: 1.5124443769454956
Epoch 2680, training loss: 62.06509017944336 = 0.007976730354130268 + 10.0 * 6.205711364746094
Epoch 2680, val loss: 1.5146312713623047
Epoch 2690, training loss: 62.1518440246582 = 0.007906231097877026 + 10.0 * 6.214393615722656
Epoch 2690, val loss: 1.5168187618255615
Epoch 2700, training loss: 62.09121322631836 = 0.007833866402506828 + 10.0 * 6.208337783813477
Epoch 2700, val loss: 1.5182347297668457
Epoch 2710, training loss: 62.081764221191406 = 0.007758795283734798 + 10.0 * 6.207400321960449
Epoch 2710, val loss: 1.5206305980682373
Epoch 2720, training loss: 62.07552719116211 = 0.007692174986004829 + 10.0 * 6.206783771514893
Epoch 2720, val loss: 1.5226472616195679
Epoch 2730, training loss: 62.056182861328125 = 0.007621948607265949 + 10.0 * 6.204855918884277
Epoch 2730, val loss: 1.5247230529785156
Epoch 2740, training loss: 62.05638885498047 = 0.007556576747447252 + 10.0 * 6.204883098602295
Epoch 2740, val loss: 1.5267173051834106
Epoch 2750, training loss: 62.118404388427734 = 0.007494237273931503 + 10.0 * 6.211091041564941
Epoch 2750, val loss: 1.52864670753479
Epoch 2760, training loss: 62.075843811035156 = 0.007424261886626482 + 10.0 * 6.206841945648193
Epoch 2760, val loss: 1.5307711362838745
Epoch 2770, training loss: 62.073974609375 = 0.007360794581472874 + 10.0 * 6.206661224365234
Epoch 2770, val loss: 1.5324980020523071
Epoch 2780, training loss: 62.110801696777344 = 0.007296798285096884 + 10.0 * 6.210350513458252
Epoch 2780, val loss: 1.5344364643096924
Epoch 2790, training loss: 62.07048797607422 = 0.00723146740347147 + 10.0 * 6.206325531005859
Epoch 2790, val loss: 1.5363428592681885
Epoch 2800, training loss: 62.05577087402344 = 0.0071667772717773914 + 10.0 * 6.204860210418701
Epoch 2800, val loss: 1.5381195545196533
Epoch 2810, training loss: 62.04521560668945 = 0.00710805831477046 + 10.0 * 6.203810691833496
Epoch 2810, val loss: 1.5402554273605347
Epoch 2820, training loss: 62.052852630615234 = 0.0070502362214028835 + 10.0 * 6.204580307006836
Epoch 2820, val loss: 1.5421464443206787
Epoch 2830, training loss: 62.097843170166016 = 0.006993977818638086 + 10.0 * 6.209084987640381
Epoch 2830, val loss: 1.543786644935608
Epoch 2840, training loss: 62.05601119995117 = 0.006929805967956781 + 10.0 * 6.2049078941345215
Epoch 2840, val loss: 1.5457302331924438
Epoch 2850, training loss: 62.043766021728516 = 0.0068748993799090385 + 10.0 * 6.203689098358154
Epoch 2850, val loss: 1.5476324558258057
Epoch 2860, training loss: 62.040870666503906 = 0.006819173227995634 + 10.0 * 6.203404903411865
Epoch 2860, val loss: 1.5496102571487427
Epoch 2870, training loss: 62.11764907836914 = 0.006766322068870068 + 10.0 * 6.211088180541992
Epoch 2870, val loss: 1.5513637065887451
Epoch 2880, training loss: 62.0609130859375 = 0.006708020344376564 + 10.0 * 6.20542049407959
Epoch 2880, val loss: 1.5527108907699585
Epoch 2890, training loss: 62.0623779296875 = 0.00665254658088088 + 10.0 * 6.205572605133057
Epoch 2890, val loss: 1.5546010732650757
Epoch 2900, training loss: 62.057437896728516 = 0.006600273307412863 + 10.0 * 6.205083847045898
Epoch 2900, val loss: 1.5564754009246826
Epoch 2910, training loss: 62.08209228515625 = 0.006544872187077999 + 10.0 * 6.207554817199707
Epoch 2910, val loss: 1.5582869052886963
Epoch 2920, training loss: 62.039283752441406 = 0.006492940243333578 + 10.0 * 6.2032790184021
Epoch 2920, val loss: 1.55988347530365
Epoch 2930, training loss: 62.033023834228516 = 0.006441296078264713 + 10.0 * 6.202658176422119
Epoch 2930, val loss: 1.5617436170578003
Epoch 2940, training loss: 62.02971267700195 = 0.006393052637577057 + 10.0 * 6.202332019805908
Epoch 2940, val loss: 1.5634360313415527
Epoch 2950, training loss: 62.06800079345703 = 0.0063451179303228855 + 10.0 * 6.206165790557861
Epoch 2950, val loss: 1.5651192665100098
Epoch 2960, training loss: 62.036659240722656 = 0.0062930709682404995 + 10.0 * 6.203036308288574
Epoch 2960, val loss: 1.5663920640945435
Epoch 2970, training loss: 62.03385543823242 = 0.006244215648621321 + 10.0 * 6.202761173248291
Epoch 2970, val loss: 1.5682369470596313
Epoch 2980, training loss: 62.03239440917969 = 0.006195900496095419 + 10.0 * 6.202620029449463
Epoch 2980, val loss: 1.5697039365768433
Epoch 2990, training loss: 62.05389404296875 = 0.006150315515697002 + 10.0 * 6.204774379730225
Epoch 2990, val loss: 1.5713974237442017
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8328940432261466
The final CL Acc:0.74691, 0.03439, The final GNN Acc:0.83623, 0.00245
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11652])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10610])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.92800903320312 = 1.9596490859985352 + 10.0 * 8.59683609008789
Epoch 0, val loss: 1.957152009010315
Epoch 10, training loss: 87.91200256347656 = 1.9497970342636108 + 10.0 * 8.596220016479492
Epoch 10, val loss: 1.9477226734161377
Epoch 20, training loss: 87.85355377197266 = 1.937414288520813 + 10.0 * 8.59161376953125
Epoch 20, val loss: 1.9351786375045776
Epoch 30, training loss: 87.54971313476562 = 1.9209426641464233 + 10.0 * 8.56287670135498
Epoch 30, val loss: 1.917982578277588
Epoch 40, training loss: 86.05426025390625 = 1.9019287824630737 + 10.0 * 8.41523265838623
Epoch 40, val loss: 1.8983972072601318
Epoch 50, training loss: 81.69763946533203 = 1.8814237117767334 + 10.0 * 7.981621265411377
Epoch 50, val loss: 1.876823902130127
Epoch 60, training loss: 77.11180114746094 = 1.86598801612854 + 10.0 * 7.524581432342529
Epoch 60, val loss: 1.862303614616394
Epoch 70, training loss: 73.06382751464844 = 1.8542958498001099 + 10.0 * 7.120953559875488
Epoch 70, val loss: 1.851802110671997
Epoch 80, training loss: 71.28176879882812 = 1.8431998491287231 + 10.0 * 6.943856716156006
Epoch 80, val loss: 1.8416250944137573
Epoch 90, training loss: 70.22512817382812 = 1.831276774406433 + 10.0 * 6.839385509490967
Epoch 90, val loss: 1.829986572265625
Epoch 100, training loss: 69.54649353027344 = 1.8194106817245483 + 10.0 * 6.772708415985107
Epoch 100, val loss: 1.8188869953155518
Epoch 110, training loss: 68.93698120117188 = 1.80882728099823 + 10.0 * 6.712815284729004
Epoch 110, val loss: 1.8096415996551514
Epoch 120, training loss: 68.45384216308594 = 1.7992668151855469 + 10.0 * 6.6654582023620605
Epoch 120, val loss: 1.8013808727264404
Epoch 130, training loss: 68.06095886230469 = 1.7901594638824463 + 10.0 * 6.627079963684082
Epoch 130, val loss: 1.7931549549102783
Epoch 140, training loss: 67.73876953125 = 1.7807003259658813 + 10.0 * 6.595807075500488
Epoch 140, val loss: 1.7846988439559937
Epoch 150, training loss: 67.48165130615234 = 1.7709382772445679 + 10.0 * 6.571071624755859
Epoch 150, val loss: 1.7760305404663086
Epoch 160, training loss: 67.26411437988281 = 1.7606995105743408 + 10.0 * 6.550342082977295
Epoch 160, val loss: 1.76698899269104
Epoch 170, training loss: 67.05440521240234 = 1.749726414680481 + 10.0 * 6.530467987060547
Epoch 170, val loss: 1.7573801279067993
Epoch 180, training loss: 66.90296936035156 = 1.737879991531372 + 10.0 * 6.516509056091309
Epoch 180, val loss: 1.7470322847366333
Epoch 190, training loss: 66.72402954101562 = 1.7250133752822876 + 10.0 * 6.49990177154541
Epoch 190, val loss: 1.7358405590057373
Epoch 200, training loss: 66.56554412841797 = 1.711020588874817 + 10.0 * 6.485452175140381
Epoch 200, val loss: 1.723758339881897
Epoch 210, training loss: 66.45832824707031 = 1.6958609819412231 + 10.0 * 6.476246356964111
Epoch 210, val loss: 1.7106585502624512
Epoch 220, training loss: 66.29553985595703 = 1.6792505979537964 + 10.0 * 6.461628437042236
Epoch 220, val loss: 1.6965168714523315
Epoch 230, training loss: 66.17109680175781 = 1.6613603830337524 + 10.0 * 6.4509735107421875
Epoch 230, val loss: 1.6811468601226807
Epoch 240, training loss: 66.06767272949219 = 1.6420972347259521 + 10.0 * 6.4425578117370605
Epoch 240, val loss: 1.6646288633346558
Epoch 250, training loss: 65.96160888671875 = 1.6213488578796387 + 10.0 * 6.43402624130249
Epoch 250, val loss: 1.6469227075576782
Epoch 260, training loss: 65.85767364501953 = 1.5993698835372925 + 10.0 * 6.425830364227295
Epoch 260, val loss: 1.6282132863998413
Epoch 270, training loss: 65.78345489501953 = 1.5762585401535034 + 10.0 * 6.420719623565674
Epoch 270, val loss: 1.6086090803146362
Epoch 280, training loss: 65.67198944091797 = 1.5520107746124268 + 10.0 * 6.411998271942139
Epoch 280, val loss: 1.5881292819976807
Epoch 290, training loss: 65.58895111083984 = 1.527043104171753 + 10.0 * 6.406190872192383
Epoch 290, val loss: 1.5672487020492554
Epoch 300, training loss: 65.49490356445312 = 1.5013952255249023 + 10.0 * 6.399350643157959
Epoch 300, val loss: 1.5459535121917725
Epoch 310, training loss: 65.4398193359375 = 1.4752259254455566 + 10.0 * 6.396459102630615
Epoch 310, val loss: 1.5244083404541016
Epoch 320, training loss: 65.34457397460938 = 1.4487470388412476 + 10.0 * 6.389582633972168
Epoch 320, val loss: 1.5029754638671875
Epoch 330, training loss: 65.25736999511719 = 1.4220390319824219 + 10.0 * 6.383533477783203
Epoch 330, val loss: 1.4816163778305054
Epoch 340, training loss: 65.18321990966797 = 1.3953077793121338 + 10.0 * 6.378791332244873
Epoch 340, val loss: 1.4605408906936646
Epoch 350, training loss: 65.10057830810547 = 1.3687160015106201 + 10.0 * 6.3731865882873535
Epoch 350, val loss: 1.4399163722991943
Epoch 360, training loss: 65.0478744506836 = 1.342230200767517 + 10.0 * 6.3705644607543945
Epoch 360, val loss: 1.419629693031311
Epoch 370, training loss: 64.96793365478516 = 1.3160771131515503 + 10.0 * 6.365185737609863
Epoch 370, val loss: 1.400076150894165
Epoch 380, training loss: 64.88871765136719 = 1.290108323097229 + 10.0 * 6.359860897064209
Epoch 380, val loss: 1.3811219930648804
Epoch 390, training loss: 64.89424133300781 = 1.2645032405853271 + 10.0 * 6.362973690032959
Epoch 390, val loss: 1.3627268075942993
Epoch 400, training loss: 64.77698516845703 = 1.2390718460083008 + 10.0 * 6.353791236877441
Epoch 400, val loss: 1.3447569608688354
Epoch 410, training loss: 64.7086410522461 = 1.2140202522277832 + 10.0 * 6.349462032318115
Epoch 410, val loss: 1.327546238899231
Epoch 420, training loss: 64.64401245117188 = 1.1893311738967896 + 10.0 * 6.345468044281006
Epoch 420, val loss: 1.3110384941101074
Epoch 430, training loss: 64.5819091796875 = 1.164989948272705 + 10.0 * 6.341691493988037
Epoch 430, val loss: 1.2951366901397705
Epoch 440, training loss: 64.53118133544922 = 1.1409043073654175 + 10.0 * 6.3390278816223145
Epoch 440, val loss: 1.2799253463745117
Epoch 450, training loss: 64.48831176757812 = 1.1171247959136963 + 10.0 * 6.337118625640869
Epoch 450, val loss: 1.2650322914123535
Epoch 460, training loss: 64.4171142578125 = 1.0935919284820557 + 10.0 * 6.332352638244629
Epoch 460, val loss: 1.250698447227478
Epoch 470, training loss: 64.37805938720703 = 1.0704699754714966 + 10.0 * 6.330759048461914
Epoch 470, val loss: 1.237101435661316
Epoch 480, training loss: 64.34637451171875 = 1.0474779605865479 + 10.0 * 6.32988977432251
Epoch 480, val loss: 1.223679780960083
Epoch 490, training loss: 64.26422119140625 = 1.0248730182647705 + 10.0 * 6.323934555053711
Epoch 490, val loss: 1.2107529640197754
Epoch 500, training loss: 64.21639251708984 = 1.0025513172149658 + 10.0 * 6.321383953094482
Epoch 500, val loss: 1.198501467704773
Epoch 510, training loss: 64.19522094726562 = 0.9804414510726929 + 10.0 * 6.321477890014648
Epoch 510, val loss: 1.1866121292114258
Epoch 520, training loss: 64.16313934326172 = 0.9585870504379272 + 10.0 * 6.320455074310303
Epoch 520, val loss: 1.1752609014511108
Epoch 530, training loss: 64.08741760253906 = 0.936900794506073 + 10.0 * 6.315051555633545
Epoch 530, val loss: 1.1643120050430298
Epoch 540, training loss: 64.05270385742188 = 0.9155896306037903 + 10.0 * 6.313711643218994
Epoch 540, val loss: 1.153747797012329
Epoch 550, training loss: 63.99342346191406 = 0.8945614099502563 + 10.0 * 6.3098859786987305
Epoch 550, val loss: 1.1435166597366333
Epoch 560, training loss: 63.95447540283203 = 0.8738327026367188 + 10.0 * 6.3080644607543945
Epoch 560, val loss: 1.1338324546813965
Epoch 570, training loss: 63.9860954284668 = 0.8532727956771851 + 10.0 * 6.313282489776611
Epoch 570, val loss: 1.1242471933364868
Epoch 580, training loss: 63.87687683105469 = 0.8331116437911987 + 10.0 * 6.304376602172852
Epoch 580, val loss: 1.1153594255447388
Epoch 590, training loss: 63.857784271240234 = 0.8132339119911194 + 10.0 * 6.304455280303955
Epoch 590, val loss: 1.1070054769515991
Epoch 600, training loss: 63.80620193481445 = 0.7936817407608032 + 10.0 * 6.3012518882751465
Epoch 600, val loss: 1.098856806755066
Epoch 610, training loss: 63.76802062988281 = 0.7744808793067932 + 10.0 * 6.29935359954834
Epoch 610, val loss: 1.0910836458206177
Epoch 620, training loss: 63.730960845947266 = 0.7557210326194763 + 10.0 * 6.2975239753723145
Epoch 620, val loss: 1.0839723348617554
Epoch 630, training loss: 63.7219352722168 = 0.7372918725013733 + 10.0 * 6.298464298248291
Epoch 630, val loss: 1.0773786306381226
Epoch 640, training loss: 63.67808532714844 = 0.7190658450126648 + 10.0 * 6.295901775360107
Epoch 640, val loss: 1.0708001852035522
Epoch 650, training loss: 63.65506362915039 = 0.7012861371040344 + 10.0 * 6.295377731323242
Epoch 650, val loss: 1.0648906230926514
Epoch 660, training loss: 63.609683990478516 = 0.6838865876197815 + 10.0 * 6.292579650878906
Epoch 660, val loss: 1.059341549873352
Epoch 670, training loss: 63.57023620605469 = 0.6668168902397156 + 10.0 * 6.290341854095459
Epoch 670, val loss: 1.054182529449463
Epoch 680, training loss: 63.58183288574219 = 0.6501417756080627 + 10.0 * 6.293169021606445
Epoch 680, val loss: 1.0495084524154663
Epoch 690, training loss: 63.580318450927734 = 0.633685290813446 + 10.0 * 6.294663429260254
Epoch 690, val loss: 1.0447182655334473
Epoch 700, training loss: 63.49989700317383 = 0.6176428198814392 + 10.0 * 6.2882256507873535
Epoch 700, val loss: 1.0406746864318848
Epoch 710, training loss: 63.459720611572266 = 0.601986825466156 + 10.0 * 6.285773277282715
Epoch 710, val loss: 1.036779522895813
Epoch 720, training loss: 63.42741775512695 = 0.5867825746536255 + 10.0 * 6.284063339233398
Epoch 720, val loss: 1.0334994792938232
Epoch 730, training loss: 63.410057067871094 = 0.5718854665756226 + 10.0 * 6.283817291259766
Epoch 730, val loss: 1.0305410623550415
Epoch 740, training loss: 63.394508361816406 = 0.5572503209114075 + 10.0 * 6.283725738525391
Epoch 740, val loss: 1.027640461921692
Epoch 750, training loss: 63.37089538574219 = 0.5429041385650635 + 10.0 * 6.282799243927002
Epoch 750, val loss: 1.0251344442367554
Epoch 760, training loss: 63.3316650390625 = 0.52900230884552 + 10.0 * 6.280266284942627
Epoch 760, val loss: 1.022963523864746
Epoch 770, training loss: 63.332557678222656 = 0.5154584646224976 + 10.0 * 6.281710147857666
Epoch 770, val loss: 1.021235704421997
Epoch 780, training loss: 63.281646728515625 = 0.5021387934684753 + 10.0 * 6.277950763702393
Epoch 780, val loss: 1.0193967819213867
Epoch 790, training loss: 63.274085998535156 = 0.4891948699951172 + 10.0 * 6.278489112854004
Epoch 790, val loss: 1.018054485321045
Epoch 800, training loss: 63.26314163208008 = 0.4765765368938446 + 10.0 * 6.278656482696533
Epoch 800, val loss: 1.0168735980987549
Epoch 810, training loss: 63.216007232666016 = 0.46422892808914185 + 10.0 * 6.275177955627441
Epoch 810, val loss: 1.0159873962402344
Epoch 820, training loss: 63.197261810302734 = 0.45228758454322815 + 10.0 * 6.2744975090026855
Epoch 820, val loss: 1.0153909921646118
Epoch 830, training loss: 63.201881408691406 = 0.44064977765083313 + 10.0 * 6.276123046875
Epoch 830, val loss: 1.0147957801818848
Epoch 840, training loss: 63.17498016357422 = 0.42927616834640503 + 10.0 * 6.274570465087891
Epoch 840, val loss: 1.014549970626831
Epoch 850, training loss: 63.1440544128418 = 0.4182201027870178 + 10.0 * 6.272583484649658
Epoch 850, val loss: 1.0144587755203247
Epoch 860, training loss: 63.11994171142578 = 0.4074798822402954 + 10.0 * 6.271246433258057
Epoch 860, val loss: 1.014601230621338
Epoch 870, training loss: 63.103477478027344 = 0.3969886600971222 + 10.0 * 6.270648956298828
Epoch 870, val loss: 1.014480471611023
Epoch 880, training loss: 63.12040710449219 = 0.38677412271499634 + 10.0 * 6.27336311340332
Epoch 880, val loss: 1.0146148204803467
Epoch 890, training loss: 63.08172607421875 = 0.3768022656440735 + 10.0 * 6.2704925537109375
Epoch 890, val loss: 1.015234112739563
Epoch 900, training loss: 63.04297637939453 = 0.3671098053455353 + 10.0 * 6.267586708068848
Epoch 900, val loss: 1.0158393383026123
Epoch 910, training loss: 63.03612518310547 = 0.3577178418636322 + 10.0 * 6.267840385437012
Epoch 910, val loss: 1.0166465044021606
Epoch 920, training loss: 63.02201843261719 = 0.34850379824638367 + 10.0 * 6.2673516273498535
Epoch 920, val loss: 1.0173896551132202
Epoch 930, training loss: 63.01386260986328 = 0.33945727348327637 + 10.0 * 6.267440319061279
Epoch 930, val loss: 1.0182807445526123
Epoch 940, training loss: 62.98042297363281 = 0.3307361900806427 + 10.0 * 6.264968395233154
Epoch 940, val loss: 1.0194445848464966
Epoch 950, training loss: 62.9633674621582 = 0.32219934463500977 + 10.0 * 6.2641167640686035
Epoch 950, val loss: 1.0204739570617676
Epoch 960, training loss: 62.99203109741211 = 0.3138670325279236 + 10.0 * 6.267816543579102
Epoch 960, val loss: 1.0216485261917114
Epoch 970, training loss: 62.94840621948242 = 0.30567389726638794 + 10.0 * 6.264273166656494
Epoch 970, val loss: 1.0229920148849487
Epoch 980, training loss: 62.93177032470703 = 0.29769209027290344 + 10.0 * 6.2634077072143555
Epoch 980, val loss: 1.024366855621338
Epoch 990, training loss: 62.90055847167969 = 0.2899259626865387 + 10.0 * 6.261063575744629
Epoch 990, val loss: 1.0259398221969604
Epoch 1000, training loss: 62.889102935791016 = 0.2823299169540405 + 10.0 * 6.260677337646484
Epoch 1000, val loss: 1.0275986194610596
Epoch 1010, training loss: 62.887481689453125 = 0.2748964726924896 + 10.0 * 6.261258602142334
Epoch 1010, val loss: 1.0292160511016846
Epoch 1020, training loss: 62.8808708190918 = 0.2676100730895996 + 10.0 * 6.261326313018799
Epoch 1020, val loss: 1.0308606624603271
Epoch 1030, training loss: 62.856441497802734 = 0.2604193091392517 + 10.0 * 6.2596025466918945
Epoch 1030, val loss: 1.0326342582702637
Epoch 1040, training loss: 62.85029983520508 = 0.25345325469970703 + 10.0 * 6.2596845626831055
Epoch 1040, val loss: 1.034513235092163
Epoch 1050, training loss: 62.81829071044922 = 0.24663333594799042 + 10.0 * 6.257165908813477
Epoch 1050, val loss: 1.0367181301116943
Epoch 1060, training loss: 62.80827331542969 = 0.24001248180866241 + 10.0 * 6.256825923919678
Epoch 1060, val loss: 1.038917064666748
Epoch 1070, training loss: 62.83647537231445 = 0.23348398506641388 + 10.0 * 6.260299205780029
Epoch 1070, val loss: 1.0409961938858032
Epoch 1080, training loss: 62.78034973144531 = 0.22704999148845673 + 10.0 * 6.2553300857543945
Epoch 1080, val loss: 1.043222188949585
Epoch 1090, training loss: 62.765289306640625 = 0.22083288431167603 + 10.0 * 6.254445552825928
Epoch 1090, val loss: 1.0455824136734009
Epoch 1100, training loss: 62.758358001708984 = 0.21479004621505737 + 10.0 * 6.254356861114502
Epoch 1100, val loss: 1.0481101274490356
Epoch 1110, training loss: 62.80358123779297 = 0.2088664323091507 + 10.0 * 6.259471416473389
Epoch 1110, val loss: 1.0503780841827393
Epoch 1120, training loss: 62.75739669799805 = 0.20307505130767822 + 10.0 * 6.25543212890625
Epoch 1120, val loss: 1.053740382194519
Epoch 1130, training loss: 62.72665023803711 = 0.19740943610668182 + 10.0 * 6.252923965454102
Epoch 1130, val loss: 1.0560815334320068
Epoch 1140, training loss: 62.71079635620117 = 0.1919570416212082 + 10.0 * 6.2518839836120605
Epoch 1140, val loss: 1.0593316555023193
Epoch 1150, training loss: 62.7442626953125 = 0.1866082400083542 + 10.0 * 6.255765438079834
Epoch 1150, val loss: 1.0621027946472168
Epoch 1160, training loss: 62.692848205566406 = 0.18139804899692535 + 10.0 * 6.251145362854004
Epoch 1160, val loss: 1.065613865852356
Epoch 1170, training loss: 62.6856803894043 = 0.17629468441009521 + 10.0 * 6.250938415527344
Epoch 1170, val loss: 1.0686571598052979
Epoch 1180, training loss: 62.68905258178711 = 0.17136166989803314 + 10.0 * 6.251769065856934
Epoch 1180, val loss: 1.0721081495285034
Epoch 1190, training loss: 62.666603088378906 = 0.16652937233448029 + 10.0 * 6.250007152557373
Epoch 1190, val loss: 1.0756034851074219
Epoch 1200, training loss: 62.65752029418945 = 0.1618642807006836 + 10.0 * 6.249565601348877
Epoch 1200, val loss: 1.0792089700698853
Epoch 1210, training loss: 62.645626068115234 = 0.15733075141906738 + 10.0 * 6.2488298416137695
Epoch 1210, val loss: 1.0829052925109863
Epoch 1220, training loss: 62.645416259765625 = 0.15291474759578705 + 10.0 * 6.2492499351501465
Epoch 1220, val loss: 1.0867162942886353
Epoch 1230, training loss: 62.638858795166016 = 0.14862267673015594 + 10.0 * 6.2490234375
Epoch 1230, val loss: 1.0905650854110718
Epoch 1240, training loss: 62.619117736816406 = 0.14440315961837769 + 10.0 * 6.247471809387207
Epoch 1240, val loss: 1.0941758155822754
Epoch 1250, training loss: 62.605560302734375 = 0.14038333296775818 + 10.0 * 6.246517658233643
Epoch 1250, val loss: 1.0984115600585938
Epoch 1260, training loss: 62.589046478271484 = 0.13647723197937012 + 10.0 * 6.2452569007873535
Epoch 1260, val loss: 1.1024703979492188
Epoch 1270, training loss: 62.62624740600586 = 0.13271290063858032 + 10.0 * 6.249353408813477
Epoch 1270, val loss: 1.1069825887680054
Epoch 1280, training loss: 62.58916091918945 = 0.1290057897567749 + 10.0 * 6.246015548706055
Epoch 1280, val loss: 1.1109492778778076
Epoch 1290, training loss: 62.601593017578125 = 0.12541498243808746 + 10.0 * 6.247617721557617
Epoch 1290, val loss: 1.115682601928711
Epoch 1300, training loss: 62.55815124511719 = 0.1219371035695076 + 10.0 * 6.243621349334717
Epoch 1300, val loss: 1.11949622631073
Epoch 1310, training loss: 62.55278396606445 = 0.11859016865491867 + 10.0 * 6.243419170379639
Epoch 1310, val loss: 1.124018669128418
Epoch 1320, training loss: 62.5673713684082 = 0.115362249314785 + 10.0 * 6.2452006340026855
Epoch 1320, val loss: 1.1286877393722534
Epoch 1330, training loss: 62.54352951049805 = 0.11220839619636536 + 10.0 * 6.2431321144104
Epoch 1330, val loss: 1.1334166526794434
Epoch 1340, training loss: 62.540130615234375 = 0.10916218906641006 + 10.0 * 6.243096828460693
Epoch 1340, val loss: 1.1384243965148926
Epoch 1350, training loss: 62.529266357421875 = 0.10619029402732849 + 10.0 * 6.242307662963867
Epoch 1350, val loss: 1.1427891254425049
Epoch 1360, training loss: 62.5225944519043 = 0.1033320501446724 + 10.0 * 6.241926193237305
Epoch 1360, val loss: 1.14764404296875
Epoch 1370, training loss: 62.53749084472656 = 0.10054366290569305 + 10.0 * 6.24369478225708
Epoch 1370, val loss: 1.152233600616455
Epoch 1380, training loss: 62.53455352783203 = 0.09783606976270676 + 10.0 * 6.243671894073486
Epoch 1380, val loss: 1.1572978496551514
Epoch 1390, training loss: 62.509159088134766 = 0.09520205855369568 + 10.0 * 6.241395473480225
Epoch 1390, val loss: 1.162036657333374
Epoch 1400, training loss: 62.50047302246094 = 0.09267733246088028 + 10.0 * 6.240779399871826
Epoch 1400, val loss: 1.1673624515533447
Epoch 1410, training loss: 62.481990814208984 = 0.09023074805736542 + 10.0 * 6.239176273345947
Epoch 1410, val loss: 1.172238826751709
Epoch 1420, training loss: 62.487091064453125 = 0.08786898851394653 + 10.0 * 6.239922523498535
Epoch 1420, val loss: 1.1773128509521484
Epoch 1430, training loss: 62.492279052734375 = 0.08556801080703735 + 10.0 * 6.240671157836914
Epoch 1430, val loss: 1.1823827028274536
Epoch 1440, training loss: 62.491764068603516 = 0.0833503007888794 + 10.0 * 6.240841388702393
Epoch 1440, val loss: 1.1880935430526733
Epoch 1450, training loss: 62.47200012207031 = 0.08116158097982407 + 10.0 * 6.239083766937256
Epoch 1450, val loss: 1.1927167177200317
Epoch 1460, training loss: 62.462215423583984 = 0.07909010350704193 + 10.0 * 6.238312721252441
Epoch 1460, val loss: 1.1982156038284302
Epoch 1470, training loss: 62.48657989501953 = 0.07705838978290558 + 10.0 * 6.240952491760254
Epoch 1470, val loss: 1.2033535242080688
Epoch 1480, training loss: 62.44977569580078 = 0.07508477568626404 + 10.0 * 6.23746919631958
Epoch 1480, val loss: 1.2087900638580322
Epoch 1490, training loss: 62.43517303466797 = 0.07317369431257248 + 10.0 * 6.236199855804443
Epoch 1490, val loss: 1.2136690616607666
Epoch 1500, training loss: 62.443355560302734 = 0.07135665416717529 + 10.0 * 6.237199783325195
Epoch 1500, val loss: 1.219315767288208
Epoch 1510, training loss: 62.44904708862305 = 0.06955252587795258 + 10.0 * 6.237949371337891
Epoch 1510, val loss: 1.2244839668273926
Epoch 1520, training loss: 62.42961883544922 = 0.0678093358874321 + 10.0 * 6.236180782318115
Epoch 1520, val loss: 1.22950279712677
Epoch 1530, training loss: 62.42094421386719 = 0.06614093482494354 + 10.0 * 6.235480308532715
Epoch 1530, val loss: 1.235082745552063
Epoch 1540, training loss: 62.43258285522461 = 0.06451816856861115 + 10.0 * 6.236806392669678
Epoch 1540, val loss: 1.240017294883728
Epoch 1550, training loss: 62.41434097290039 = 0.06293154507875443 + 10.0 * 6.235140800476074
Epoch 1550, val loss: 1.2451305389404297
Epoch 1560, training loss: 62.39289474487305 = 0.061395931988954544 + 10.0 * 6.233149528503418
Epoch 1560, val loss: 1.2504000663757324
Epoch 1570, training loss: 62.39079666137695 = 0.059918057173490524 + 10.0 * 6.23308801651001
Epoch 1570, val loss: 1.2556027173995972
Epoch 1580, training loss: 62.43959045410156 = 0.05848940089344978 + 10.0 * 6.238110065460205
Epoch 1580, val loss: 1.260302186012268
Epoch 1590, training loss: 62.41286087036133 = 0.057093214243650436 + 10.0 * 6.235576629638672
Epoch 1590, val loss: 1.2657506465911865
Epoch 1600, training loss: 62.379188537597656 = 0.05572650954127312 + 10.0 * 6.232346534729004
Epoch 1600, val loss: 1.2711918354034424
Epoch 1610, training loss: 62.366546630859375 = 0.05442732945084572 + 10.0 * 6.2312116622924805
Epoch 1610, val loss: 1.2762864828109741
Epoch 1620, training loss: 62.36862564086914 = 0.05317889153957367 + 10.0 * 6.231544494628906
Epoch 1620, val loss: 1.2816390991210938
Epoch 1630, training loss: 62.41037368774414 = 0.05195451155304909 + 10.0 * 6.235841751098633
Epoch 1630, val loss: 1.2863192558288574
Epoch 1640, training loss: 62.39087677001953 = 0.05075541138648987 + 10.0 * 6.234012126922607
Epoch 1640, val loss: 1.2921336889266968
Epoch 1650, training loss: 62.3944206237793 = 0.04958570376038551 + 10.0 * 6.234483242034912
Epoch 1650, val loss: 1.297021746635437
Epoch 1660, training loss: 62.35304260253906 = 0.048448797315359116 + 10.0 * 6.230459213256836
Epoch 1660, val loss: 1.3017076253890991
Epoch 1670, training loss: 62.346866607666016 = 0.047371361404657364 + 10.0 * 6.229949474334717
Epoch 1670, val loss: 1.3067582845687866
Epoch 1680, training loss: 62.35997772216797 = 0.04632559418678284 + 10.0 * 6.231365203857422
Epoch 1680, val loss: 1.3116704225540161
Epoch 1690, training loss: 62.34738540649414 = 0.045295171439647675 + 10.0 * 6.230208873748779
Epoch 1690, val loss: 1.3165267705917358
Epoch 1700, training loss: 62.33968734741211 = 0.044299375265836716 + 10.0 * 6.229538917541504
Epoch 1700, val loss: 1.3215516805648804
Epoch 1710, training loss: 62.337745666503906 = 0.043342821300029755 + 10.0 * 6.229440212249756
Epoch 1710, val loss: 1.3262181282043457
Epoch 1720, training loss: 62.36573028564453 = 0.04241213575005531 + 10.0 * 6.2323317527771
Epoch 1720, val loss: 1.3308088779449463
Epoch 1730, training loss: 62.33158874511719 = 0.041506022214889526 + 10.0 * 6.229008197784424
Epoch 1730, val loss: 1.336504340171814
Epoch 1740, training loss: 62.332366943359375 = 0.0406176894903183 + 10.0 * 6.229174613952637
Epoch 1740, val loss: 1.340824842453003
Epoch 1750, training loss: 62.33778381347656 = 0.039763934910297394 + 10.0 * 6.229802131652832
Epoch 1750, val loss: 1.345488429069519
Epoch 1760, training loss: 62.33063507080078 = 0.03893163055181503 + 10.0 * 6.229170322418213
Epoch 1760, val loss: 1.350471019744873
Epoch 1770, training loss: 62.316619873046875 = 0.03811761736869812 + 10.0 * 6.227850437164307
Epoch 1770, val loss: 1.3547979593276978
Epoch 1780, training loss: 62.36342239379883 = 0.037331320345401764 + 10.0 * 6.232609272003174
Epoch 1780, val loss: 1.3594746589660645
Epoch 1790, training loss: 62.314979553222656 = 0.03657066076993942 + 10.0 * 6.227840900421143
Epoch 1790, val loss: 1.3644651174545288
Epoch 1800, training loss: 62.29804992675781 = 0.03582344576716423 + 10.0 * 6.226222515106201
Epoch 1800, val loss: 1.3688526153564453
Epoch 1810, training loss: 62.28834915161133 = 0.03511687368154526 + 10.0 * 6.22532320022583
Epoch 1810, val loss: 1.3735601902008057
Epoch 1820, training loss: 62.31662368774414 = 0.034424930810928345 + 10.0 * 6.228219985961914
Epoch 1820, val loss: 1.3779270648956299
Epoch 1830, training loss: 62.29166793823242 = 0.03373510017991066 + 10.0 * 6.225793361663818
Epoch 1830, val loss: 1.3822927474975586
Epoch 1840, training loss: 62.31048583984375 = 0.03306759148836136 + 10.0 * 6.2277421951293945
Epoch 1840, val loss: 1.3868274688720703
Epoch 1850, training loss: 62.29722213745117 = 0.03241401165723801 + 10.0 * 6.226480960845947
Epoch 1850, val loss: 1.3910645246505737
Epoch 1860, training loss: 62.29517364501953 = 0.03178161010146141 + 10.0 * 6.226339340209961
Epoch 1860, val loss: 1.3951481580734253
Epoch 1870, training loss: 62.28287887573242 = 0.031173493713140488 + 10.0 * 6.225170612335205
Epoch 1870, val loss: 1.3995046615600586
Epoch 1880, training loss: 62.27171325683594 = 0.030585231259465218 + 10.0 * 6.224112510681152
Epoch 1880, val loss: 1.4038578271865845
Epoch 1890, training loss: 62.29924392700195 = 0.03001265972852707 + 10.0 * 6.226922988891602
Epoch 1890, val loss: 1.4074952602386475
Epoch 1900, training loss: 62.278892517089844 = 0.029447367414832115 + 10.0 * 6.224944591522217
Epoch 1900, val loss: 1.4119610786437988
Epoch 1910, training loss: 62.25999450683594 = 0.028900494799017906 + 10.0 * 6.223109245300293
Epoch 1910, val loss: 1.4164637327194214
Epoch 1920, training loss: 62.262901306152344 = 0.028373437002301216 + 10.0 * 6.223452568054199
Epoch 1920, val loss: 1.4203417301177979
Epoch 1930, training loss: 62.308162689208984 = 0.027857206761837006 + 10.0 * 6.228030204772949
Epoch 1930, val loss: 1.4240190982818604
Epoch 1940, training loss: 62.26815414428711 = 0.027348481118679047 + 10.0 * 6.224080562591553
Epoch 1940, val loss: 1.4285413026809692
Epoch 1950, training loss: 62.25048065185547 = 0.026854343712329865 + 10.0 * 6.222362518310547
Epoch 1950, val loss: 1.4324032068252563
Epoch 1960, training loss: 62.262149810791016 = 0.02638525515794754 + 10.0 * 6.223576545715332
Epoch 1960, val loss: 1.4363690614700317
Epoch 1970, training loss: 62.29095458984375 = 0.025921836495399475 + 10.0 * 6.226503372192383
Epoch 1970, val loss: 1.4403489828109741
Epoch 1980, training loss: 62.244468688964844 = 0.025450367480516434 + 10.0 * 6.221901893615723
Epoch 1980, val loss: 1.4438451528549194
Epoch 1990, training loss: 62.239498138427734 = 0.0250055231153965 + 10.0 * 6.221449375152588
Epoch 1990, val loss: 1.4477791786193848
Epoch 2000, training loss: 62.23298263549805 = 0.024583227932453156 + 10.0 * 6.220839977264404
Epoch 2000, val loss: 1.4515705108642578
Epoch 2010, training loss: 62.228092193603516 = 0.024170763790607452 + 10.0 * 6.220392227172852
Epoch 2010, val loss: 1.4553756713867188
Epoch 2020, training loss: 62.281856536865234 = 0.023767923936247826 + 10.0 * 6.225808620452881
Epoch 2020, val loss: 1.459014892578125
Epoch 2030, training loss: 62.305633544921875 = 0.02336207963526249 + 10.0 * 6.228227138519287
Epoch 2030, val loss: 1.4626808166503906
Epoch 2040, training loss: 62.24591827392578 = 0.022950122132897377 + 10.0 * 6.222296714782715
Epoch 2040, val loss: 1.4660629034042358
Epoch 2050, training loss: 62.219818115234375 = 0.022568779066205025 + 10.0 * 6.219725131988525
Epoch 2050, val loss: 1.4698532819747925
Epoch 2060, training loss: 62.22111892700195 = 0.022208839654922485 + 10.0 * 6.21989107131958
Epoch 2060, val loss: 1.473406434059143
Epoch 2070, training loss: 62.26561737060547 = 0.021856525912880898 + 10.0 * 6.2243757247924805
Epoch 2070, val loss: 1.477070927619934
Epoch 2080, training loss: 62.22029113769531 = 0.02149045094847679 + 10.0 * 6.219880104064941
Epoch 2080, val loss: 1.4805309772491455
Epoch 2090, training loss: 62.242156982421875 = 0.021149979904294014 + 10.0 * 6.222100734710693
Epoch 2090, val loss: 1.4840526580810547
Epoch 2100, training loss: 62.20994186401367 = 0.020804794505238533 + 10.0 * 6.218913555145264
Epoch 2100, val loss: 1.4870765209197998
Epoch 2110, training loss: 62.214298248291016 = 0.020479237660765648 + 10.0 * 6.219381809234619
Epoch 2110, val loss: 1.4902961254119873
Epoch 2120, training loss: 62.242305755615234 = 0.020161502063274384 + 10.0 * 6.222214698791504
Epoch 2120, val loss: 1.4941494464874268
Epoch 2130, training loss: 62.206974029541016 = 0.01983799785375595 + 10.0 * 6.218713760375977
Epoch 2130, val loss: 1.4966996908187866
Epoch 2140, training loss: 62.202362060546875 = 0.01953480765223503 + 10.0 * 6.218282699584961
Epoch 2140, val loss: 1.5000642538070679
Epoch 2150, training loss: 62.2098388671875 = 0.019243191927671432 + 10.0 * 6.219059944152832
Epoch 2150, val loss: 1.5036265850067139
Epoch 2160, training loss: 62.224449157714844 = 0.018949836492538452 + 10.0 * 6.220549583435059
Epoch 2160, val loss: 1.5064506530761719
Epoch 2170, training loss: 62.21370315551758 = 0.01864597760140896 + 10.0 * 6.219505786895752
Epoch 2170, val loss: 1.5092110633850098
Epoch 2180, training loss: 62.2092399597168 = 0.018365073949098587 + 10.0 * 6.219087600708008
Epoch 2180, val loss: 1.5120317935943604
Epoch 2190, training loss: 62.187530517578125 = 0.018094077706336975 + 10.0 * 6.216943740844727
Epoch 2190, val loss: 1.5155704021453857
Epoch 2200, training loss: 62.18177795410156 = 0.017831191420555115 + 10.0 * 6.216394901275635
Epoch 2200, val loss: 1.5184450149536133
Epoch 2210, training loss: 62.178428649902344 = 0.01757386326789856 + 10.0 * 6.216085433959961
Epoch 2210, val loss: 1.521353840827942
Epoch 2220, training loss: 62.21841049194336 = 0.017324460670351982 + 10.0 * 6.220108509063721
Epoch 2220, val loss: 1.5238407850265503
Epoch 2230, training loss: 62.210845947265625 = 0.0170668363571167 + 10.0 * 6.2193779945373535
Epoch 2230, val loss: 1.5268609523773193
Epoch 2240, training loss: 62.1824951171875 = 0.016816290095448494 + 10.0 * 6.2165679931640625
Epoch 2240, val loss: 1.5300037860870361
Epoch 2250, training loss: 62.2027587890625 = 0.01657523401081562 + 10.0 * 6.218618392944336
Epoch 2250, val loss: 1.5325335264205933
Epoch 2260, training loss: 62.197872161865234 = 0.0163387693464756 + 10.0 * 6.218153476715088
Epoch 2260, val loss: 1.5353968143463135
Epoch 2270, training loss: 62.18671417236328 = 0.01610828936100006 + 10.0 * 6.217060565948486
Epoch 2270, val loss: 1.5383479595184326
Epoch 2280, training loss: 62.17876434326172 = 0.015886571258306503 + 10.0 * 6.216287612915039
Epoch 2280, val loss: 1.5408765077590942
Epoch 2290, training loss: 62.18547058105469 = 0.015668634325265884 + 10.0 * 6.216980457305908
Epoch 2290, val loss: 1.5437310934066772
Epoch 2300, training loss: 62.16794204711914 = 0.015453984029591084 + 10.0 * 6.215249061584473
Epoch 2300, val loss: 1.546734094619751
Epoch 2310, training loss: 62.18763732910156 = 0.015247281640768051 + 10.0 * 6.217238903045654
Epoch 2310, val loss: 1.5494023561477661
Epoch 2320, training loss: 62.174232482910156 = 0.015038453042507172 + 10.0 * 6.215919494628906
Epoch 2320, val loss: 1.5518701076507568
Epoch 2330, training loss: 62.1724739074707 = 0.014835800044238567 + 10.0 * 6.215764045715332
Epoch 2330, val loss: 1.5546306371688843
Epoch 2340, training loss: 62.164241790771484 = 0.014635405503213406 + 10.0 * 6.21496057510376
Epoch 2340, val loss: 1.5566699504852295
Epoch 2350, training loss: 62.163360595703125 = 0.014442848972976208 + 10.0 * 6.2148919105529785
Epoch 2350, val loss: 1.5591408014297485
Epoch 2360, training loss: 62.177852630615234 = 0.014256606809794903 + 10.0 * 6.216359615325928
Epoch 2360, val loss: 1.561754822731018
Epoch 2370, training loss: 62.18931579589844 = 0.014069071970880032 + 10.0 * 6.217524528503418
Epoch 2370, val loss: 1.564501404762268
Epoch 2380, training loss: 62.16016387939453 = 0.013884982094168663 + 10.0 * 6.214627742767334
Epoch 2380, val loss: 1.56728196144104
Epoch 2390, training loss: 62.15291213989258 = 0.013705337420105934 + 10.0 * 6.213920593261719
Epoch 2390, val loss: 1.569222331047058
Epoch 2400, training loss: 62.154476165771484 = 0.013535059988498688 + 10.0 * 6.214094161987305
Epoch 2400, val loss: 1.572083830833435
Epoch 2410, training loss: 62.16028594970703 = 0.01336660049855709 + 10.0 * 6.214692115783691
Epoch 2410, val loss: 1.5743730068206787
Epoch 2420, training loss: 62.15180969238281 = 0.013193774968385696 + 10.0 * 6.213861465454102
Epoch 2420, val loss: 1.5764480829238892
Epoch 2430, training loss: 62.15973663330078 = 0.013029511086642742 + 10.0 * 6.214670658111572
Epoch 2430, val loss: 1.5785892009735107
Epoch 2440, training loss: 62.18436050415039 = 0.012865163385868073 + 10.0 * 6.217149257659912
Epoch 2440, val loss: 1.5804671049118042
Epoch 2450, training loss: 62.16892623901367 = 0.012702804058790207 + 10.0 * 6.215622425079346
Epoch 2450, val loss: 1.5831350088119507
Epoch 2460, training loss: 62.13752365112305 = 0.012543614022433758 + 10.0 * 6.212498188018799
Epoch 2460, val loss: 1.5853372812271118
Epoch 2470, training loss: 62.12670135498047 = 0.012396248988807201 + 10.0 * 6.211430549621582
Epoch 2470, val loss: 1.5880171060562134
Epoch 2480, training loss: 62.13285827636719 = 0.012250303290784359 + 10.0 * 6.212060928344727
Epoch 2480, val loss: 1.5898655652999878
Epoch 2490, training loss: 62.17866897583008 = 0.012106114998459816 + 10.0 * 6.21665620803833
Epoch 2490, val loss: 1.592212200164795
Epoch 2500, training loss: 62.157466888427734 = 0.011960543692111969 + 10.0 * 6.214550971984863
Epoch 2500, val loss: 1.594078540802002
Epoch 2510, training loss: 62.142215728759766 = 0.011814955621957779 + 10.0 * 6.213040351867676
Epoch 2510, val loss: 1.5964176654815674
Epoch 2520, training loss: 62.1317253112793 = 0.011675994843244553 + 10.0 * 6.212004661560059
Epoch 2520, val loss: 1.5980229377746582
Epoch 2530, training loss: 62.16062927246094 = 0.011540744453668594 + 10.0 * 6.214909076690674
Epoch 2530, val loss: 1.5996038913726807
Epoch 2540, training loss: 62.13197326660156 = 0.011405439116060734 + 10.0 * 6.212056636810303
Epoch 2540, val loss: 1.6024271249771118
Epoch 2550, training loss: 62.11547088623047 = 0.01127434428781271 + 10.0 * 6.210419654846191
Epoch 2550, val loss: 1.6042120456695557
Epoch 2560, training loss: 62.12226867675781 = 0.01114748977124691 + 10.0 * 6.211112022399902
Epoch 2560, val loss: 1.6058605909347534
Epoch 2570, training loss: 62.144954681396484 = 0.011022360064089298 + 10.0 * 6.213393211364746
Epoch 2570, val loss: 1.6075881719589233
Epoch 2580, training loss: 62.16640090942383 = 0.010896747931838036 + 10.0 * 6.215550422668457
Epoch 2580, val loss: 1.609925389289856
Epoch 2590, training loss: 62.120243072509766 = 0.010768788866698742 + 10.0 * 6.210947513580322
Epoch 2590, val loss: 1.612114667892456
Epoch 2600, training loss: 62.106258392333984 = 0.010648210532963276 + 10.0 * 6.209561347961426
Epoch 2600, val loss: 1.6138789653778076
Epoch 2610, training loss: 62.10282897949219 = 0.010535437613725662 + 10.0 * 6.209229469299316
Epoch 2610, val loss: 1.6159937381744385
Epoch 2620, training loss: 62.12433624267578 = 0.010423896834254265 + 10.0 * 6.211390972137451
Epoch 2620, val loss: 1.6176326274871826
Epoch 2630, training loss: 62.125370025634766 = 0.0103079192340374 + 10.0 * 6.211506366729736
Epoch 2630, val loss: 1.619316577911377
Epoch 2640, training loss: 62.11750411987305 = 0.010192223824560642 + 10.0 * 6.210731029510498
Epoch 2640, val loss: 1.6210917234420776
Epoch 2650, training loss: 62.09739303588867 = 0.010080515407025814 + 10.0 * 6.208731174468994
Epoch 2650, val loss: 1.6231528520584106
Epoch 2660, training loss: 62.09370040893555 = 0.009975890628993511 + 10.0 * 6.208372592926025
Epoch 2660, val loss: 1.6246039867401123
Epoch 2670, training loss: 62.110679626464844 = 0.009873847477138042 + 10.0 * 6.210080623626709
Epoch 2670, val loss: 1.6263490915298462
Epoch 2680, training loss: 62.133941650390625 = 0.009770006872713566 + 10.0 * 6.212417125701904
Epoch 2680, val loss: 1.6283893585205078
Epoch 2690, training loss: 62.121421813964844 = 0.009664858691394329 + 10.0 * 6.211175441741943
Epoch 2690, val loss: 1.6300369501113892
Epoch 2700, training loss: 62.115562438964844 = 0.00956200435757637 + 10.0 * 6.210599899291992
Epoch 2700, val loss: 1.6314789056777954
Epoch 2710, training loss: 62.096553802490234 = 0.009463311173021793 + 10.0 * 6.208708763122559
Epoch 2710, val loss: 1.6331219673156738
Epoch 2720, training loss: 62.10762023925781 = 0.00936762522906065 + 10.0 * 6.209825038909912
Epoch 2720, val loss: 1.6344621181488037
Epoch 2730, training loss: 62.089962005615234 = 0.009272512048482895 + 10.0 * 6.20806884765625
Epoch 2730, val loss: 1.636400818824768
Epoch 2740, training loss: 62.10064697265625 = 0.00918215699493885 + 10.0 * 6.209146499633789
Epoch 2740, val loss: 1.6382297277450562
Epoch 2750, training loss: 62.10282897949219 = 0.009090201929211617 + 10.0 * 6.209373950958252
Epoch 2750, val loss: 1.6395621299743652
Epoch 2760, training loss: 62.126060485839844 = 0.008998502045869827 + 10.0 * 6.211706161499023
Epoch 2760, val loss: 1.6408249139785767
Epoch 2770, training loss: 62.10699462890625 = 0.00890644546598196 + 10.0 * 6.209808826446533
Epoch 2770, val loss: 1.6422975063323975
Epoch 2780, training loss: 62.08750915527344 = 0.008818783797323704 + 10.0 * 6.207869052886963
Epoch 2780, val loss: 1.6438194513320923
Epoch 2790, training loss: 62.090702056884766 = 0.008735671639442444 + 10.0 * 6.208196640014648
Epoch 2790, val loss: 1.6456878185272217
Epoch 2800, training loss: 62.1191291809082 = 0.008654243312776089 + 10.0 * 6.211047172546387
Epoch 2800, val loss: 1.647063136100769
Epoch 2810, training loss: 62.087745666503906 = 0.008564488962292671 + 10.0 * 6.207918167114258
Epoch 2810, val loss: 1.6477248668670654
Epoch 2820, training loss: 62.07526397705078 = 0.008484986610710621 + 10.0 * 6.2066779136657715
Epoch 2820, val loss: 1.6497423648834229
Epoch 2830, training loss: 62.08591079711914 = 0.008407020941376686 + 10.0 * 6.20775032043457
Epoch 2830, val loss: 1.6509977579116821
Epoch 2840, training loss: 62.10270309448242 = 0.008328920230269432 + 10.0 * 6.209437370300293
Epoch 2840, val loss: 1.6525477170944214
Epoch 2850, training loss: 62.09019088745117 = 0.008249976672232151 + 10.0 * 6.208193778991699
Epoch 2850, val loss: 1.6540254354476929
Epoch 2860, training loss: 62.08864974975586 = 0.008172277361154556 + 10.0 * 6.208047866821289
Epoch 2860, val loss: 1.6552269458770752
Epoch 2870, training loss: 62.07457733154297 = 0.008094619028270245 + 10.0 * 6.206648349761963
Epoch 2870, val loss: 1.6560426950454712
Epoch 2880, training loss: 62.08066940307617 = 0.008020689710974693 + 10.0 * 6.2072649002075195
Epoch 2880, val loss: 1.657150387763977
Epoch 2890, training loss: 62.08457946777344 = 0.007948460057377815 + 10.0 * 6.207663059234619
Epoch 2890, val loss: 1.6587467193603516
Epoch 2900, training loss: 62.0975456237793 = 0.00787653960287571 + 10.0 * 6.2089667320251465
Epoch 2900, val loss: 1.6600723266601562
Epoch 2910, training loss: 62.07411575317383 = 0.007802510168403387 + 10.0 * 6.206631660461426
Epoch 2910, val loss: 1.661429762840271
Epoch 2920, training loss: 62.06023406982422 = 0.0077329957857728004 + 10.0 * 6.205250263214111
Epoch 2920, val loss: 1.6623989343643188
Epoch 2930, training loss: 62.070125579833984 = 0.007666870951652527 + 10.0 * 6.2062458992004395
Epoch 2930, val loss: 1.6640040874481201
Epoch 2940, training loss: 62.08140563964844 = 0.007600930519402027 + 10.0 * 6.207380294799805
Epoch 2940, val loss: 1.6652470827102661
Epoch 2950, training loss: 62.066776275634766 = 0.007530135568231344 + 10.0 * 6.2059245109558105
Epoch 2950, val loss: 1.6662604808807373
Epoch 2960, training loss: 62.07096862792969 = 0.00746537558734417 + 10.0 * 6.206350326538086
Epoch 2960, val loss: 1.6675373315811157
Epoch 2970, training loss: 62.08823776245117 = 0.007401409093290567 + 10.0 * 6.208083629608154
Epoch 2970, val loss: 1.668735146522522
Epoch 2980, training loss: 62.08890914916992 = 0.007334600668400526 + 10.0 * 6.208157539367676
Epoch 2980, val loss: 1.6692020893096924
Epoch 2990, training loss: 62.057228088378906 = 0.007269189227372408 + 10.0 * 6.204995632171631
Epoch 2990, val loss: 1.6707212924957275
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 87.90034484863281 = 1.9319779872894287 + 10.0 * 8.596837043762207
Epoch 0, val loss: 1.9287046194076538
Epoch 10, training loss: 87.884521484375 = 1.9226936101913452 + 10.0 * 8.596182823181152
Epoch 10, val loss: 1.9201300144195557
Epoch 20, training loss: 87.824462890625 = 1.9117012023925781 + 10.0 * 8.591276168823242
Epoch 20, val loss: 1.9094339609146118
Epoch 30, training loss: 87.4581298828125 = 1.8980157375335693 + 10.0 * 8.556011199951172
Epoch 30, val loss: 1.8957042694091797
Epoch 40, training loss: 85.09925079345703 = 1.882726788520813 + 10.0 * 8.32165241241455
Epoch 40, val loss: 1.8805513381958008
Epoch 50, training loss: 78.17862701416016 = 1.867471694946289 + 10.0 * 7.631115913391113
Epoch 50, val loss: 1.8651427030563354
Epoch 60, training loss: 75.00236511230469 = 1.8543859720230103 + 10.0 * 7.314797878265381
Epoch 60, val loss: 1.8511642217636108
Epoch 70, training loss: 72.70994567871094 = 1.8415617942810059 + 10.0 * 7.086838722229004
Epoch 70, val loss: 1.8379476070404053
Epoch 80, training loss: 71.16868591308594 = 1.8306025266647339 + 10.0 * 6.933808326721191
Epoch 80, val loss: 1.8264812231063843
Epoch 90, training loss: 69.99431610107422 = 1.8219081163406372 + 10.0 * 6.817241191864014
Epoch 90, val loss: 1.8173269033432007
Epoch 100, training loss: 69.17933654785156 = 1.8144203424453735 + 10.0 * 6.7364912033081055
Epoch 100, val loss: 1.8097259998321533
Epoch 110, training loss: 68.53076171875 = 1.8069921731948853 + 10.0 * 6.67237663269043
Epoch 110, val loss: 1.8023897409439087
Epoch 120, training loss: 68.05133056640625 = 1.7995678186416626 + 10.0 * 6.625175952911377
Epoch 120, val loss: 1.7950490713119507
Epoch 130, training loss: 67.66533660888672 = 1.7919741868972778 + 10.0 * 6.58733606338501
Epoch 130, val loss: 1.7874650955200195
Epoch 140, training loss: 67.36018371582031 = 1.7840827703475952 + 10.0 * 6.557610511779785
Epoch 140, val loss: 1.779571771621704
Epoch 150, training loss: 67.11764526367188 = 1.7756810188293457 + 10.0 * 6.534196853637695
Epoch 150, val loss: 1.7712032794952393
Epoch 160, training loss: 66.89454650878906 = 1.7664979696273804 + 10.0 * 6.512804985046387
Epoch 160, val loss: 1.7623635530471802
Epoch 170, training loss: 66.7026596069336 = 1.7565256357192993 + 10.0 * 6.4946136474609375
Epoch 170, val loss: 1.7528718709945679
Epoch 180, training loss: 66.55137634277344 = 1.745559573173523 + 10.0 * 6.480581283569336
Epoch 180, val loss: 1.7425923347473145
Epoch 190, training loss: 66.40914154052734 = 1.7334133386611938 + 10.0 * 6.4675726890563965
Epoch 190, val loss: 1.7313235998153687
Epoch 200, training loss: 66.25707244873047 = 1.7200148105621338 + 10.0 * 6.453705310821533
Epoch 200, val loss: 1.7190301418304443
Epoch 210, training loss: 66.1251220703125 = 1.7052853107452393 + 10.0 * 6.441983699798584
Epoch 210, val loss: 1.7055964469909668
Epoch 220, training loss: 66.02324676513672 = 1.68899405002594 + 10.0 * 6.433424949645996
Epoch 220, val loss: 1.6908456087112427
Epoch 230, training loss: 65.89683532714844 = 1.6710540056228638 + 10.0 * 6.422578811645508
Epoch 230, val loss: 1.6748195886611938
Epoch 240, training loss: 65.79277038574219 = 1.6514661312103271 + 10.0 * 6.414130210876465
Epoch 240, val loss: 1.6574151515960693
Epoch 250, training loss: 65.69615936279297 = 1.6301485300064087 + 10.0 * 6.4066009521484375
Epoch 250, val loss: 1.638701319694519
Epoch 260, training loss: 65.646240234375 = 1.6069525480270386 + 10.0 * 6.403928756713867
Epoch 260, val loss: 1.6184812784194946
Epoch 270, training loss: 65.51307678222656 = 1.5820285081863403 + 10.0 * 6.3931050300598145
Epoch 270, val loss: 1.597005009651184
Epoch 280, training loss: 65.42803955078125 = 1.5555020570755005 + 10.0 * 6.387253761291504
Epoch 280, val loss: 1.57441246509552
Epoch 290, training loss: 65.42767333984375 = 1.5273313522338867 + 10.0 * 6.3900346755981445
Epoch 290, val loss: 1.5507087707519531
Epoch 300, training loss: 65.28815460205078 = 1.4976259469985962 + 10.0 * 6.379052639007568
Epoch 300, val loss: 1.525818943977356
Epoch 310, training loss: 65.19068145751953 = 1.4667799472808838 + 10.0 * 6.372390270233154
Epoch 310, val loss: 1.5003687143325806
Epoch 320, training loss: 65.1024398803711 = 1.4349533319473267 + 10.0 * 6.366748809814453
Epoch 320, val loss: 1.474360466003418
Epoch 330, training loss: 65.03382873535156 = 1.4022575616836548 + 10.0 * 6.363157272338867
Epoch 330, val loss: 1.4480133056640625
Epoch 340, training loss: 64.97686004638672 = 1.3688887357711792 + 10.0 * 6.360796928405762
Epoch 340, val loss: 1.4211775064468384
Epoch 350, training loss: 64.88488006591797 = 1.3351573944091797 + 10.0 * 6.3549723625183105
Epoch 350, val loss: 1.394616961479187
Epoch 360, training loss: 64.81214904785156 = 1.3013739585876465 + 10.0 * 6.351077079772949
Epoch 360, val loss: 1.3684571981430054
Epoch 370, training loss: 64.7467269897461 = 1.2676432132720947 + 10.0 * 6.3479084968566895
Epoch 370, val loss: 1.3427765369415283
Epoch 380, training loss: 64.69483947753906 = 1.2341296672821045 + 10.0 * 6.346070766448975
Epoch 380, val loss: 1.3176977634429932
Epoch 390, training loss: 64.61544036865234 = 1.2011312246322632 + 10.0 * 6.341431140899658
Epoch 390, val loss: 1.2935553789138794
Epoch 400, training loss: 64.56070709228516 = 1.1688679456710815 + 10.0 * 6.339183807373047
Epoch 400, val loss: 1.2705692052841187
Epoch 410, training loss: 64.48970031738281 = 1.137422800064087 + 10.0 * 6.3352274894714355
Epoch 410, val loss: 1.248794674873352
Epoch 420, training loss: 64.43489837646484 = 1.1069388389587402 + 10.0 * 6.3327956199646
Epoch 420, val loss: 1.2282538414001465
Epoch 430, training loss: 64.44947052001953 = 1.0774744749069214 + 10.0 * 6.337199687957764
Epoch 430, val loss: 1.209116816520691
Epoch 440, training loss: 64.33077239990234 = 1.0490487813949585 + 10.0 * 6.32817268371582
Epoch 440, val loss: 1.1912615299224854
Epoch 450, training loss: 64.27486419677734 = 1.021785020828247 + 10.0 * 6.325307846069336
Epoch 450, val loss: 1.1748465299606323
Epoch 460, training loss: 64.22406005859375 = 0.9957168102264404 + 10.0 * 6.322834014892578
Epoch 460, val loss: 1.1599520444869995
Epoch 470, training loss: 64.17916107177734 = 0.9707042574882507 + 10.0 * 6.320845603942871
Epoch 470, val loss: 1.1463243961334229
Epoch 480, training loss: 64.17111206054688 = 0.9466249346733093 + 10.0 * 6.32244873046875
Epoch 480, val loss: 1.133791208267212
Epoch 490, training loss: 64.10454559326172 = 0.9235508441925049 + 10.0 * 6.318099498748779
Epoch 490, val loss: 1.1224316358566284
Epoch 500, training loss: 64.04621124267578 = 0.901533842086792 + 10.0 * 6.314467906951904
Epoch 500, val loss: 1.1122653484344482
Epoch 510, training loss: 64.05802154541016 = 0.8803994655609131 + 10.0 * 6.3177618980407715
Epoch 510, val loss: 1.103161096572876
Epoch 520, training loss: 63.98696517944336 = 0.8600060343742371 + 10.0 * 6.3126959800720215
Epoch 520, val loss: 1.0948113203048706
Epoch 530, training loss: 63.956153869628906 = 0.8403873443603516 + 10.0 * 6.311576843261719
Epoch 530, val loss: 1.0874054431915283
Epoch 540, training loss: 63.9077262878418 = 0.82147616147995 + 10.0 * 6.308625221252441
Epoch 540, val loss: 1.0810151100158691
Epoch 550, training loss: 63.85661697387695 = 0.8032143115997314 + 10.0 * 6.30534029006958
Epoch 550, val loss: 1.0752689838409424
Epoch 560, training loss: 63.8346061706543 = 0.7855226993560791 + 10.0 * 6.304908275604248
Epoch 560, val loss: 1.070223093032837
Epoch 570, training loss: 63.865421295166016 = 0.7682919502258301 + 10.0 * 6.309712886810303
Epoch 570, val loss: 1.0656728744506836
Epoch 580, training loss: 63.78030776977539 = 0.7513600587844849 + 10.0 * 6.302894592285156
Epoch 580, val loss: 1.0614286661148071
Epoch 590, training loss: 63.72932052612305 = 0.735054075717926 + 10.0 * 6.299426555633545
Epoch 590, val loss: 1.0579493045806885
Epoch 600, training loss: 63.69864273071289 = 0.7191976308822632 + 10.0 * 6.29794454574585
Epoch 600, val loss: 1.055035948753357
Epoch 610, training loss: 63.73728942871094 = 0.7035832405090332 + 10.0 * 6.303370475769043
Epoch 610, val loss: 1.0525894165039062
Epoch 620, training loss: 63.67390441894531 = 0.6882107853889465 + 10.0 * 6.298569679260254
Epoch 620, val loss: 1.050164818763733
Epoch 630, training loss: 63.621646881103516 = 0.6731948852539062 + 10.0 * 6.294845104217529
Epoch 630, val loss: 1.0482438802719116
Epoch 640, training loss: 63.5855827331543 = 0.6585227847099304 + 10.0 * 6.29270601272583
Epoch 640, val loss: 1.046971082687378
Epoch 650, training loss: 63.55344009399414 = 0.6440819501876831 + 10.0 * 6.29093599319458
Epoch 650, val loss: 1.0460526943206787
Epoch 660, training loss: 63.55293655395508 = 0.6299031376838684 + 10.0 * 6.292303562164307
Epoch 660, val loss: 1.0454374551773071
Epoch 670, training loss: 63.50698471069336 = 0.6158091425895691 + 10.0 * 6.289117336273193
Epoch 670, val loss: 1.0447776317596436
Epoch 680, training loss: 63.51152038574219 = 0.6019835472106934 + 10.0 * 6.290953636169434
Epoch 680, val loss: 1.044691801071167
Epoch 690, training loss: 63.4605598449707 = 0.5883573293685913 + 10.0 * 6.287220478057861
Epoch 690, val loss: 1.0445291996002197
Epoch 700, training loss: 63.43829345703125 = 0.5750206708908081 + 10.0 * 6.286327362060547
Epoch 700, val loss: 1.0451799631118774
Epoch 710, training loss: 63.4047966003418 = 0.5618605017662048 + 10.0 * 6.2842936515808105
Epoch 710, val loss: 1.0457085371017456
Epoch 720, training loss: 63.424312591552734 = 0.5488512516021729 + 10.0 * 6.287546157836914
Epoch 720, val loss: 1.0466476678848267
Epoch 730, training loss: 63.359458923339844 = 0.5360100269317627 + 10.0 * 6.282344818115234
Epoch 730, val loss: 1.0474892854690552
Epoch 740, training loss: 63.32642364501953 = 0.523431658744812 + 10.0 * 6.280299186706543
Epoch 740, val loss: 1.0489568710327148
Epoch 750, training loss: 63.30455780029297 = 0.5110553503036499 + 10.0 * 6.279350280761719
Epoch 750, val loss: 1.0505770444869995
Epoch 760, training loss: 63.40435028076172 = 0.49881115555763245 + 10.0 * 6.290554046630859
Epoch 760, val loss: 1.0522003173828125
Epoch 770, training loss: 63.26898956298828 = 0.4865950644016266 + 10.0 * 6.2782392501831055
Epoch 770, val loss: 1.053853988647461
Epoch 780, training loss: 63.24655532836914 = 0.47473686933517456 + 10.0 * 6.277182102203369
Epoch 780, val loss: 1.0559272766113281
Epoch 790, training loss: 63.22880172729492 = 0.46312621235847473 + 10.0 * 6.276567459106445
Epoch 790, val loss: 1.0582876205444336
Epoch 800, training loss: 63.217872619628906 = 0.451651930809021 + 10.0 * 6.2766218185424805
Epoch 800, val loss: 1.060705304145813
Epoch 810, training loss: 63.1768684387207 = 0.44033926725387573 + 10.0 * 6.273653030395508
Epoch 810, val loss: 1.063164472579956
Epoch 820, training loss: 63.196632385253906 = 0.4292137324810028 + 10.0 * 6.276741981506348
Epoch 820, val loss: 1.0657371282577515
Epoch 830, training loss: 63.16189956665039 = 0.41819268465042114 + 10.0 * 6.2743706703186035
Epoch 830, val loss: 1.0687404870986938
Epoch 840, training loss: 63.13765335083008 = 0.40739792585372925 + 10.0 * 6.2730255126953125
Epoch 840, val loss: 1.071766972541809
Epoch 850, training loss: 63.113765716552734 = 0.39674195647239685 + 10.0 * 6.271702289581299
Epoch 850, val loss: 1.0747424364089966
Epoch 860, training loss: 63.0933952331543 = 0.3862268030643463 + 10.0 * 6.270716667175293
Epoch 860, val loss: 1.078048586845398
Epoch 870, training loss: 63.06039810180664 = 0.3758999705314636 + 10.0 * 6.268449783325195
Epoch 870, val loss: 1.081291913986206
Epoch 880, training loss: 63.04120635986328 = 0.3657446503639221 + 10.0 * 6.2675461769104
Epoch 880, val loss: 1.0848305225372314
Epoch 890, training loss: 63.0474853515625 = 0.3557658791542053 + 10.0 * 6.269171714782715
Epoch 890, val loss: 1.0884771347045898
Epoch 900, training loss: 63.056671142578125 = 0.345947265625 + 10.0 * 6.2710723876953125
Epoch 900, val loss: 1.0919218063354492
Epoch 910, training loss: 63.017356872558594 = 0.3362690806388855 + 10.0 * 6.26810884475708
Epoch 910, val loss: 1.095779538154602
Epoch 920, training loss: 62.97225570678711 = 0.3268561065196991 + 10.0 * 6.264540195465088
Epoch 920, val loss: 1.0997967720031738
Epoch 930, training loss: 62.95591735839844 = 0.3177036941051483 + 10.0 * 6.263821601867676
Epoch 930, val loss: 1.103990912437439
Epoch 940, training loss: 62.95500946044922 = 0.30870240926742554 + 10.0 * 6.2646307945251465
Epoch 940, val loss: 1.1083625555038452
Epoch 950, training loss: 62.926883697509766 = 0.29982563853263855 + 10.0 * 6.2627058029174805
Epoch 950, val loss: 1.1124950647354126
Epoch 960, training loss: 62.91085433959961 = 0.29115816950798035 + 10.0 * 6.261969566345215
Epoch 960, val loss: 1.116948127746582
Epoch 970, training loss: 62.89973449707031 = 0.2827238142490387 + 10.0 * 6.2617011070251465
Epoch 970, val loss: 1.1217759847640991
Epoch 980, training loss: 62.97568130493164 = 0.2745092511177063 + 10.0 * 6.270117282867432
Epoch 980, val loss: 1.1264307498931885
Epoch 990, training loss: 62.88109588623047 = 0.26636675000190735 + 10.0 * 6.261473178863525
Epoch 990, val loss: 1.1307663917541504
Epoch 1000, training loss: 62.85170364379883 = 0.25853249430656433 + 10.0 * 6.259316921234131
Epoch 1000, val loss: 1.1358588933944702
Epoch 1010, training loss: 62.843814849853516 = 0.2509271800518036 + 10.0 * 6.259288787841797
Epoch 1010, val loss: 1.1410075426101685
Epoch 1020, training loss: 62.857418060302734 = 0.24349075555801392 + 10.0 * 6.261392593383789
Epoch 1020, val loss: 1.145919680595398
Epoch 1030, training loss: 62.82987976074219 = 0.23623116314411163 + 10.0 * 6.259364604949951
Epoch 1030, val loss: 1.15115225315094
Epoch 1040, training loss: 62.80875778198242 = 0.2291640341281891 + 10.0 * 6.257959365844727
Epoch 1040, val loss: 1.156684398651123
Epoch 1050, training loss: 62.80055618286133 = 0.22231577336788177 + 10.0 * 6.257823944091797
Epoch 1050, val loss: 1.1620992422103882
Epoch 1060, training loss: 62.77833938598633 = 0.2156483232975006 + 10.0 * 6.2562689781188965
Epoch 1060, val loss: 1.167805552482605
Epoch 1070, training loss: 62.771034240722656 = 0.20918181538581848 + 10.0 * 6.256185054779053
Epoch 1070, val loss: 1.1734756231307983
Epoch 1080, training loss: 62.77533721923828 = 0.20291019976139069 + 10.0 * 6.257242679595947
Epoch 1080, val loss: 1.179308533668518
Epoch 1090, training loss: 62.781524658203125 = 0.19680680334568024 + 10.0 * 6.258471488952637
Epoch 1090, val loss: 1.1852208375930786
Epoch 1100, training loss: 62.73145294189453 = 0.19090761244297028 + 10.0 * 6.254054546356201
Epoch 1100, val loss: 1.1911370754241943
Epoch 1110, training loss: 62.711143493652344 = 0.18522080779075623 + 10.0 * 6.252592086791992
Epoch 1110, val loss: 1.197421669960022
Epoch 1120, training loss: 62.698551177978516 = 0.17974524199962616 + 10.0 * 6.251880645751953
Epoch 1120, val loss: 1.2037878036499023
Epoch 1130, training loss: 62.73820877075195 = 0.1744241714477539 + 10.0 * 6.256378650665283
Epoch 1130, val loss: 1.2103251218795776
Epoch 1140, training loss: 62.7010498046875 = 0.16918882727622986 + 10.0 * 6.253186225891113
Epoch 1140, val loss: 1.2164899110794067
Epoch 1150, training loss: 62.680789947509766 = 0.16415932774543762 + 10.0 * 6.2516632080078125
Epoch 1150, val loss: 1.2228888273239136
Epoch 1160, training loss: 62.672271728515625 = 0.15932118892669678 + 10.0 * 6.25129508972168
Epoch 1160, val loss: 1.2296332120895386
Epoch 1170, training loss: 62.68089294433594 = 0.15463204681873322 + 10.0 * 6.252625942230225
Epoch 1170, val loss: 1.2362699508666992
Epoch 1180, training loss: 62.66109848022461 = 0.15008875727653503 + 10.0 * 6.251101016998291
Epoch 1180, val loss: 1.2429125308990479
Epoch 1190, training loss: 62.66272735595703 = 0.14569421112537384 + 10.0 * 6.251703262329102
Epoch 1190, val loss: 1.2495872974395752
Epoch 1200, training loss: 62.631591796875 = 0.1414548009634018 + 10.0 * 6.249013423919678
Epoch 1200, val loss: 1.256571888923645
Epoch 1210, training loss: 62.611873626708984 = 0.13735239207744598 + 10.0 * 6.247452259063721
Epoch 1210, val loss: 1.2634994983673096
Epoch 1220, training loss: 62.6203498840332 = 0.13340438902378082 + 10.0 * 6.24869441986084
Epoch 1220, val loss: 1.2704204320907593
Epoch 1230, training loss: 62.63593292236328 = 0.1295575350522995 + 10.0 * 6.250637531280518
Epoch 1230, val loss: 1.277234673500061
Epoch 1240, training loss: 62.608272552490234 = 0.12584277987480164 + 10.0 * 6.2482428550720215
Epoch 1240, val loss: 1.2844533920288086
Epoch 1250, training loss: 62.598758697509766 = 0.12225379794836044 + 10.0 * 6.247650623321533
Epoch 1250, val loss: 1.2913120985031128
Epoch 1260, training loss: 62.596717834472656 = 0.11878839135169983 + 10.0 * 6.247792720794678
Epoch 1260, val loss: 1.2985033988952637
Epoch 1270, training loss: 62.624515533447266 = 0.11540623754262924 + 10.0 * 6.250910758972168
Epoch 1270, val loss: 1.3056474924087524
Epoch 1280, training loss: 62.585384368896484 = 0.11214260011911392 + 10.0 * 6.247323989868164
Epoch 1280, val loss: 1.3127778768539429
Epoch 1290, training loss: 62.54961395263672 = 0.10899010300636292 + 10.0 * 6.244062423706055
Epoch 1290, val loss: 1.3199046850204468
Epoch 1300, training loss: 62.54126739501953 = 0.10596314817667007 + 10.0 * 6.2435302734375
Epoch 1300, val loss: 1.3272143602371216
Epoch 1310, training loss: 62.5489387512207 = 0.10302979499101639 + 10.0 * 6.244590759277344
Epoch 1310, val loss: 1.3344955444335938
Epoch 1320, training loss: 62.56291580200195 = 0.10016407817602158 + 10.0 * 6.246275424957275
Epoch 1320, val loss: 1.3415980339050293
Epoch 1330, training loss: 62.54149627685547 = 0.09736662358045578 + 10.0 * 6.244412899017334
Epoch 1330, val loss: 1.3485925197601318
Epoch 1340, training loss: 62.52595138549805 = 0.09469249099493027 + 10.0 * 6.243125915527344
Epoch 1340, val loss: 1.3561140298843384
Epoch 1350, training loss: 62.523162841796875 = 0.09209916740655899 + 10.0 * 6.243106365203857
Epoch 1350, val loss: 1.363307237625122
Epoch 1360, training loss: 62.50358581542969 = 0.08958719670772552 + 10.0 * 6.241399765014648
Epoch 1360, val loss: 1.3707003593444824
Epoch 1370, training loss: 62.511085510253906 = 0.087166927754879 + 10.0 * 6.242392063140869
Epoch 1370, val loss: 1.3778660297393799
Epoch 1380, training loss: 62.52468490600586 = 0.08479880541563034 + 10.0 * 6.243988990783691
Epoch 1380, val loss: 1.384924054145813
Epoch 1390, training loss: 62.48650360107422 = 0.08249480277299881 + 10.0 * 6.240400791168213
Epoch 1390, val loss: 1.3920230865478516
Epoch 1400, training loss: 62.47812271118164 = 0.0802939310669899 + 10.0 * 6.239782810211182
Epoch 1400, val loss: 1.399633765220642
Epoch 1410, training loss: 62.470375061035156 = 0.07817761600017548 + 10.0 * 6.239219665527344
Epoch 1410, val loss: 1.4068037271499634
Epoch 1420, training loss: 62.50720977783203 = 0.07613270729780197 + 10.0 * 6.243107795715332
Epoch 1420, val loss: 1.4140952825546265
Epoch 1430, training loss: 62.46271514892578 = 0.0741182342171669 + 10.0 * 6.2388596534729
Epoch 1430, val loss: 1.4212985038757324
Epoch 1440, training loss: 62.50932693481445 = 0.072178415954113 + 10.0 * 6.243714809417725
Epoch 1440, val loss: 1.4283555746078491
Epoch 1450, training loss: 62.46839141845703 = 0.0702911764383316 + 10.0 * 6.239809989929199
Epoch 1450, val loss: 1.4352599382400513
Epoch 1460, training loss: 62.473270416259766 = 0.0684889703989029 + 10.0 * 6.240478038787842
Epoch 1460, val loss: 1.4425222873687744
Epoch 1470, training loss: 62.44196319580078 = 0.06673504412174225 + 10.0 * 6.237523078918457
Epoch 1470, val loss: 1.449704647064209
Epoch 1480, training loss: 62.436466217041016 = 0.06505069881677628 + 10.0 * 6.2371416091918945
Epoch 1480, val loss: 1.4570037126541138
Epoch 1490, training loss: 62.47379684448242 = 0.063419409096241 + 10.0 * 6.241037845611572
Epoch 1490, val loss: 1.463760495185852
Epoch 1500, training loss: 62.43057632446289 = 0.061818212270736694 + 10.0 * 6.236876010894775
Epoch 1500, val loss: 1.4709521532058716
Epoch 1510, training loss: 62.418521881103516 = 0.060277059674263 + 10.0 * 6.2358245849609375
Epoch 1510, val loss: 1.4781146049499512
Epoch 1520, training loss: 62.41035842895508 = 0.058796223253011703 + 10.0 * 6.235156059265137
Epoch 1520, val loss: 1.4851350784301758
Epoch 1530, training loss: 62.44410705566406 = 0.057367779314517975 + 10.0 * 6.238673686981201
Epoch 1530, val loss: 1.4922230243682861
Epoch 1540, training loss: 62.421573638916016 = 0.05596792325377464 + 10.0 * 6.236560344696045
Epoch 1540, val loss: 1.498671054840088
Epoch 1550, training loss: 62.4205436706543 = 0.054611630737781525 + 10.0 * 6.236593246459961
Epoch 1550, val loss: 1.5059027671813965
Epoch 1560, training loss: 62.42405319213867 = 0.05329980328679085 + 10.0 * 6.237075328826904
Epoch 1560, val loss: 1.5127389430999756
Epoch 1570, training loss: 62.39508056640625 = 0.05201541632413864 + 10.0 * 6.234306335449219
Epoch 1570, val loss: 1.5196017026901245
Epoch 1580, training loss: 62.389930725097656 = 0.05079752206802368 + 10.0 * 6.233913421630859
Epoch 1580, val loss: 1.526396632194519
Epoch 1590, training loss: 62.476497650146484 = 0.04960547760128975 + 10.0 * 6.24268913269043
Epoch 1590, val loss: 1.53273606300354
Epoch 1600, training loss: 62.392547607421875 = 0.04842367395758629 + 10.0 * 6.23441219329834
Epoch 1600, val loss: 1.540179967880249
Epoch 1610, training loss: 62.36901092529297 = 0.04730839654803276 + 10.0 * 6.232170104980469
Epoch 1610, val loss: 1.546605110168457
Epoch 1620, training loss: 62.3623046875 = 0.04622933268547058 + 10.0 * 6.231607437133789
Epoch 1620, val loss: 1.5535575151443481
Epoch 1630, training loss: 62.36981201171875 = 0.04519185423851013 + 10.0 * 6.232461929321289
Epoch 1630, val loss: 1.5601770877838135
Epoch 1640, training loss: 62.38975143432617 = 0.04416770860552788 + 10.0 * 6.234558582305908
Epoch 1640, val loss: 1.566612720489502
Epoch 1650, training loss: 62.38528823852539 = 0.043162282556295395 + 10.0 * 6.234212398529053
Epoch 1650, val loss: 1.5732508897781372
Epoch 1660, training loss: 62.36796951293945 = 0.0421871654689312 + 10.0 * 6.232578277587891
Epoch 1660, val loss: 1.5796738862991333
Epoch 1670, training loss: 62.357322692871094 = 0.041253481060266495 + 10.0 * 6.231606960296631
Epoch 1670, val loss: 1.5861375331878662
Epoch 1680, training loss: 62.38896179199219 = 0.04035216569900513 + 10.0 * 6.234860897064209
Epoch 1680, val loss: 1.5927013158798218
Epoch 1690, training loss: 62.33974075317383 = 0.039469607174396515 + 10.0 * 6.230027198791504
Epoch 1690, val loss: 1.598958134651184
Epoch 1700, training loss: 62.337646484375 = 0.038624171167612076 + 10.0 * 6.229902267456055
Epoch 1700, val loss: 1.6054556369781494
Epoch 1710, training loss: 62.356468200683594 = 0.0378047414124012 + 10.0 * 6.231866359710693
Epoch 1710, val loss: 1.6118628978729248
Epoch 1720, training loss: 62.37353515625 = 0.03699978440999985 + 10.0 * 6.233653545379639
Epoch 1720, val loss: 1.6176644563674927
Epoch 1730, training loss: 62.336891174316406 = 0.036211661994457245 + 10.0 * 6.230067729949951
Epoch 1730, val loss: 1.6242234706878662
Epoch 1740, training loss: 62.32509231567383 = 0.03545698896050453 + 10.0 * 6.228963375091553
Epoch 1740, val loss: 1.6304633617401123
Epoch 1750, training loss: 62.318058013916016 = 0.034734562039375305 + 10.0 * 6.22833251953125
Epoch 1750, val loss: 1.6367284059524536
Epoch 1760, training loss: 62.36280822753906 = 0.03403208404779434 + 10.0 * 6.232877731323242
Epoch 1760, val loss: 1.6429550647735596
Epoch 1770, training loss: 62.3179931640625 = 0.033325329422950745 + 10.0 * 6.228466987609863
Epoch 1770, val loss: 1.6486005783081055
Epoch 1780, training loss: 62.31328201293945 = 0.03264207765460014 + 10.0 * 6.228064060211182
Epoch 1780, val loss: 1.6547765731811523
Epoch 1790, training loss: 62.30488586425781 = 0.031993500888347626 + 10.0 * 6.227289199829102
Epoch 1790, val loss: 1.6607840061187744
Epoch 1800, training loss: 62.31245040893555 = 0.03136179596185684 + 10.0 * 6.228108882904053
Epoch 1800, val loss: 1.6666942834854126
Epoch 1810, training loss: 62.327613830566406 = 0.030744627118110657 + 10.0 * 6.229686737060547
Epoch 1810, val loss: 1.672636866569519
Epoch 1820, training loss: 62.343116760253906 = 0.03014265187084675 + 10.0 * 6.231297492980957
Epoch 1820, val loss: 1.6784120798110962
Epoch 1830, training loss: 62.30685043334961 = 0.029548266902565956 + 10.0 * 6.2277302742004395
Epoch 1830, val loss: 1.684080719947815
Epoch 1840, training loss: 62.3029670715332 = 0.02898106351494789 + 10.0 * 6.227398872375488
Epoch 1840, val loss: 1.6898975372314453
Epoch 1850, training loss: 62.31753921508789 = 0.028431067243218422 + 10.0 * 6.22891092300415
Epoch 1850, val loss: 1.695310115814209
Epoch 1860, training loss: 62.297996520996094 = 0.02788495272397995 + 10.0 * 6.227011203765869
Epoch 1860, val loss: 1.700877070426941
Epoch 1870, training loss: 62.28643035888672 = 0.027358513325452805 + 10.0 * 6.225907325744629
Epoch 1870, val loss: 1.7068536281585693
Epoch 1880, training loss: 62.280601501464844 = 0.026856554672122 + 10.0 * 6.225374698638916
Epoch 1880, val loss: 1.7123841047286987
Epoch 1890, training loss: 62.29412841796875 = 0.02636909857392311 + 10.0 * 6.226775646209717
Epoch 1890, val loss: 1.718085765838623
Epoch 1900, training loss: 62.3101692199707 = 0.025881947949528694 + 10.0 * 6.228428840637207
Epoch 1900, val loss: 1.7228195667266846
Epoch 1910, training loss: 62.283084869384766 = 0.025401076301932335 + 10.0 * 6.225768089294434
Epoch 1910, val loss: 1.7284742593765259
Epoch 1920, training loss: 62.309749603271484 = 0.024940939620137215 + 10.0 * 6.228480815887451
Epoch 1920, val loss: 1.733546257019043
Epoch 1930, training loss: 62.26396560668945 = 0.024493154138326645 + 10.0 * 6.223947048187256
Epoch 1930, val loss: 1.7391541004180908
Epoch 1940, training loss: 62.2678108215332 = 0.024066656827926636 + 10.0 * 6.224374294281006
Epoch 1940, val loss: 1.7444944381713867
Epoch 1950, training loss: 62.279510498046875 = 0.023650703951716423 + 10.0 * 6.2255859375
Epoch 1950, val loss: 1.7496445178985596
Epoch 1960, training loss: 62.30307388305664 = 0.0232367143034935 + 10.0 * 6.2279839515686035
Epoch 1960, val loss: 1.7545018196105957
Epoch 1970, training loss: 62.26889419555664 = 0.022825205698609352 + 10.0 * 6.224606990814209
Epoch 1970, val loss: 1.7596317529678345
Epoch 1980, training loss: 62.26176834106445 = 0.022433850914239883 + 10.0 * 6.223933219909668
Epoch 1980, val loss: 1.7649974822998047
Epoch 1990, training loss: 62.25756072998047 = 0.022058114409446716 + 10.0 * 6.223550319671631
Epoch 1990, val loss: 1.7700992822647095
Epoch 2000, training loss: 62.27888870239258 = 0.021689744666218758 + 10.0 * 6.225719928741455
Epoch 2000, val loss: 1.775177001953125
Epoch 2010, training loss: 62.248680114746094 = 0.021323271095752716 + 10.0 * 6.222735404968262
Epoch 2010, val loss: 1.7799538373947144
Epoch 2020, training loss: 62.2603645324707 = 0.020971152931451797 + 10.0 * 6.223939418792725
Epoch 2020, val loss: 1.7848068475723267
Epoch 2030, training loss: 62.27070236206055 = 0.020625410601496696 + 10.0 * 6.2250075340271
Epoch 2030, val loss: 1.7898218631744385
Epoch 2040, training loss: 62.25345230102539 = 0.02028750441968441 + 10.0 * 6.223316669464111
Epoch 2040, val loss: 1.7943967580795288
Epoch 2050, training loss: 62.29313278198242 = 0.019957516342401505 + 10.0 * 6.2273173332214355
Epoch 2050, val loss: 1.7993597984313965
Epoch 2060, training loss: 62.236568450927734 = 0.01962810754776001 + 10.0 * 6.221693992614746
Epoch 2060, val loss: 1.8035541772842407
Epoch 2070, training loss: 62.2251091003418 = 0.019316155463457108 + 10.0 * 6.220579147338867
Epoch 2070, val loss: 1.8086497783660889
Epoch 2080, training loss: 62.228050231933594 = 0.019017387181520462 + 10.0 * 6.220903396606445
Epoch 2080, val loss: 1.8132247924804688
Epoch 2090, training loss: 62.25253677368164 = 0.018724758177995682 + 10.0 * 6.223381042480469
Epoch 2090, val loss: 1.817613959312439
Epoch 2100, training loss: 62.25673294067383 = 0.018428180366754532 + 10.0 * 6.223830223083496
Epoch 2100, val loss: 1.8222209215164185
Epoch 2110, training loss: 62.227386474609375 = 0.018134716898202896 + 10.0 * 6.220925331115723
Epoch 2110, val loss: 1.8266901969909668
Epoch 2120, training loss: 62.23301315307617 = 0.017855200916528702 + 10.0 * 6.221515655517578
Epoch 2120, val loss: 1.8313517570495605
Epoch 2130, training loss: 62.23648452758789 = 0.017584849148988724 + 10.0 * 6.221889972686768
Epoch 2130, val loss: 1.8354581594467163
Epoch 2140, training loss: 62.234378814697266 = 0.01731567643582821 + 10.0 * 6.221706390380859
Epoch 2140, val loss: 1.8398408889770508
Epoch 2150, training loss: 62.22700881958008 = 0.01705770567059517 + 10.0 * 6.22099494934082
Epoch 2150, val loss: 1.8443565368652344
Epoch 2160, training loss: 62.23189926147461 = 0.01679905503988266 + 10.0 * 6.22150993347168
Epoch 2160, val loss: 1.848431944847107
Epoch 2170, training loss: 62.21182632446289 = 0.016550473868846893 + 10.0 * 6.219527244567871
Epoch 2170, val loss: 1.8530337810516357
Epoch 2180, training loss: 62.209327697753906 = 0.01631133444607258 + 10.0 * 6.219301700592041
Epoch 2180, val loss: 1.857275366783142
Epoch 2190, training loss: 62.23028564453125 = 0.016076453030109406 + 10.0 * 6.221421241760254
Epoch 2190, val loss: 1.8614124059677124
Epoch 2200, training loss: 62.21394348144531 = 0.015841545537114143 + 10.0 * 6.2198100090026855
Epoch 2200, val loss: 1.865494966506958
Epoch 2210, training loss: 62.239994049072266 = 0.015611512586474419 + 10.0 * 6.222438335418701
Epoch 2210, val loss: 1.8693917989730835
Epoch 2220, training loss: 62.225067138671875 = 0.015384186059236526 + 10.0 * 6.220968246459961
Epoch 2220, val loss: 1.874014139175415
Epoch 2230, training loss: 62.209835052490234 = 0.015165623277425766 + 10.0 * 6.219466686248779
Epoch 2230, val loss: 1.877767562866211
Epoch 2240, training loss: 62.20375061035156 = 0.014953376725316048 + 10.0 * 6.218879699707031
Epoch 2240, val loss: 1.8819680213928223
Epoch 2250, training loss: 62.22417449951172 = 0.014748510904610157 + 10.0 * 6.220942497253418
Epoch 2250, val loss: 1.886134386062622
Epoch 2260, training loss: 62.19452667236328 = 0.014540341682732105 + 10.0 * 6.217998504638672
Epoch 2260, val loss: 1.8898122310638428
Epoch 2270, training loss: 62.197383880615234 = 0.014343798160552979 + 10.0 * 6.21830415725708
Epoch 2270, val loss: 1.89383065700531
Epoch 2280, training loss: 62.25738525390625 = 0.014150029979646206 + 10.0 * 6.224323749542236
Epoch 2280, val loss: 1.8974990844726562
Epoch 2290, training loss: 62.2010612487793 = 0.013951556757092476 + 10.0 * 6.218710899353027
Epoch 2290, val loss: 1.90140700340271
Epoch 2300, training loss: 62.190120697021484 = 0.013764172792434692 + 10.0 * 6.217635631561279
Epoch 2300, val loss: 1.9053982496261597
Epoch 2310, training loss: 62.22239303588867 = 0.01358289085328579 + 10.0 * 6.22088098526001
Epoch 2310, val loss: 1.9088752269744873
Epoch 2320, training loss: 62.1751708984375 = 0.013400193303823471 + 10.0 * 6.216176986694336
Epoch 2320, val loss: 1.9127728939056396
Epoch 2330, training loss: 62.18212890625 = 0.01322939619421959 + 10.0 * 6.21688985824585
Epoch 2330, val loss: 1.9166126251220703
Epoch 2340, training loss: 62.24711990356445 = 0.013058723881840706 + 10.0 * 6.2234063148498535
Epoch 2340, val loss: 1.919636845588684
Epoch 2350, training loss: 62.19916534423828 = 0.012879022397100925 + 10.0 * 6.218628883361816
Epoch 2350, val loss: 1.923777461051941
Epoch 2360, training loss: 62.19215774536133 = 0.012712550349533558 + 10.0 * 6.217944145202637
Epoch 2360, val loss: 1.9271339178085327
Epoch 2370, training loss: 62.172080993652344 = 0.012548808008432388 + 10.0 * 6.2159528732299805
Epoch 2370, val loss: 1.9309816360473633
Epoch 2380, training loss: 62.20065689086914 = 0.012393677607178688 + 10.0 * 6.2188262939453125
Epoch 2380, val loss: 1.9345054626464844
Epoch 2390, training loss: 62.167938232421875 = 0.012234021909534931 + 10.0 * 6.215570449829102
Epoch 2390, val loss: 1.938003659248352
Epoch 2400, training loss: 62.169822692871094 = 0.012081765569746494 + 10.0 * 6.215774059295654
Epoch 2400, val loss: 1.9413865804672241
Epoch 2410, training loss: 62.190940856933594 = 0.01193210855126381 + 10.0 * 6.21790075302124
Epoch 2410, val loss: 1.9451642036437988
Epoch 2420, training loss: 62.171382904052734 = 0.011782832443714142 + 10.0 * 6.2159600257873535
Epoch 2420, val loss: 1.948124647140503
Epoch 2430, training loss: 62.160484313964844 = 0.011636324226856232 + 10.0 * 6.2148847579956055
Epoch 2430, val loss: 1.9515973329544067
Epoch 2440, training loss: 62.17202377319336 = 0.011496320366859436 + 10.0 * 6.216052532196045
Epoch 2440, val loss: 1.954939603805542
Epoch 2450, training loss: 62.20021057128906 = 0.01135758776217699 + 10.0 * 6.21888542175293
Epoch 2450, val loss: 1.9583139419555664
Epoch 2460, training loss: 62.1629524230957 = 0.011214167810976505 + 10.0 * 6.215173721313477
Epoch 2460, val loss: 1.961669683456421
Epoch 2470, training loss: 62.14872741699219 = 0.011081007309257984 + 10.0 * 6.213764667510986
Epoch 2470, val loss: 1.9649968147277832
Epoch 2480, training loss: 62.14677047729492 = 0.010951428674161434 + 10.0 * 6.2135820388793945
Epoch 2480, val loss: 1.968501329421997
Epoch 2490, training loss: 62.20011901855469 = 0.010828574188053608 + 10.0 * 6.218928813934326
Epoch 2490, val loss: 1.9716676473617554
Epoch 2500, training loss: 62.167057037353516 = 0.010692340321838856 + 10.0 * 6.215636253356934
Epoch 2500, val loss: 1.974461317062378
Epoch 2510, training loss: 62.14606475830078 = 0.010564999654889107 + 10.0 * 6.213549613952637
Epoch 2510, val loss: 1.9775506258010864
Epoch 2520, training loss: 62.13746643066406 = 0.010443955659866333 + 10.0 * 6.21270227432251
Epoch 2520, val loss: 1.981089472770691
Epoch 2530, training loss: 62.13954544067383 = 0.01032834593206644 + 10.0 * 6.212921619415283
Epoch 2530, val loss: 1.9842597246170044
Epoch 2540, training loss: 62.206241607666016 = 0.010213530622422695 + 10.0 * 6.219603061676025
Epoch 2540, val loss: 1.9873533248901367
Epoch 2550, training loss: 62.198768615722656 = 0.010092747397720814 + 10.0 * 6.218867301940918
Epoch 2550, val loss: 1.9897385835647583
Epoch 2560, training loss: 62.1522331237793 = 0.009973138570785522 + 10.0 * 6.214226245880127
Epoch 2560, val loss: 1.9931821823120117
Epoch 2570, training loss: 62.13470458984375 = 0.009863030165433884 + 10.0 * 6.212483882904053
Epoch 2570, val loss: 1.9961038827896118
Epoch 2580, training loss: 62.13081359863281 = 0.009756148792803288 + 10.0 * 6.212105751037598
Epoch 2580, val loss: 1.999215841293335
Epoch 2590, training loss: 62.197566986083984 = 0.009654349647462368 + 10.0 * 6.2187910079956055
Epoch 2590, val loss: 2.0018956661224365
Epoch 2600, training loss: 62.168617248535156 = 0.009538968093693256 + 10.0 * 6.215907573699951
Epoch 2600, val loss: 2.005124807357788
Epoch 2610, training loss: 62.137786865234375 = 0.009431078098714352 + 10.0 * 6.212835788726807
Epoch 2610, val loss: 2.0075221061706543
Epoch 2620, training loss: 62.12482833862305 = 0.009328998625278473 + 10.0 * 6.211549758911133
Epoch 2620, val loss: 2.0107498168945312
Epoch 2630, training loss: 62.11746597290039 = 0.00923200324177742 + 10.0 * 6.2108235359191895
Epoch 2630, val loss: 2.0136635303497314
Epoch 2640, training loss: 62.1292839050293 = 0.009137280285358429 + 10.0 * 6.212014675140381
Epoch 2640, val loss: 2.016530990600586
Epoch 2650, training loss: 62.16331481933594 = 0.009038127958774567 + 10.0 * 6.215427875518799
Epoch 2650, val loss: 2.0189125537872314
Epoch 2660, training loss: 62.12649154663086 = 0.008937174454331398 + 10.0 * 6.211755275726318
Epoch 2660, val loss: 2.0217487812042236
Epoch 2670, training loss: 62.13639831542969 = 0.008842572569847107 + 10.0 * 6.2127556800842285
Epoch 2670, val loss: 2.024749755859375
Epoch 2680, training loss: 62.117759704589844 = 0.008750814013183117 + 10.0 * 6.210900783538818
Epoch 2680, val loss: 2.027200937271118
Epoch 2690, training loss: 62.119407653808594 = 0.008662157692015171 + 10.0 * 6.211074352264404
Epoch 2690, val loss: 2.0300889015197754
Epoch 2700, training loss: 62.16015625 = 0.008574612438678741 + 10.0 * 6.215157985687256
Epoch 2700, val loss: 2.0325019359588623
Epoch 2710, training loss: 62.11677551269531 = 0.00848387461155653 + 10.0 * 6.210829257965088
Epoch 2710, val loss: 2.035365104675293
Epoch 2720, training loss: 62.13549041748047 = 0.008398174308240414 + 10.0 * 6.212708950042725
Epoch 2720, val loss: 2.0379104614257812
Epoch 2730, training loss: 62.13125991821289 = 0.008310437202453613 + 10.0 * 6.212294578552246
Epoch 2730, val loss: 2.0402276515960693
Epoch 2740, training loss: 62.117958068847656 = 0.008225333876907825 + 10.0 * 6.210973262786865
Epoch 2740, val loss: 2.042813539505005
Epoch 2750, training loss: 62.11643981933594 = 0.008142401464283466 + 10.0 * 6.210829734802246
Epoch 2750, val loss: 2.0454816818237305
Epoch 2760, training loss: 62.13990020751953 = 0.008064578287303448 + 10.0 * 6.213183403015137
Epoch 2760, val loss: 2.047846555709839
Epoch 2770, training loss: 62.11087417602539 = 0.007982323877513409 + 10.0 * 6.210289001464844
Epoch 2770, val loss: 2.050417184829712
Epoch 2780, training loss: 62.12959289550781 = 0.007905677892267704 + 10.0 * 6.2121686935424805
Epoch 2780, val loss: 2.0529868602752686
Epoch 2790, training loss: 62.12953186035156 = 0.007826198823750019 + 10.0 * 6.212170600891113
Epoch 2790, val loss: 2.054980993270874
Epoch 2800, training loss: 62.112640380859375 = 0.007750272285193205 + 10.0 * 6.210488796234131
Epoch 2800, val loss: 2.057792901992798
Epoch 2810, training loss: 62.095550537109375 = 0.007673645857721567 + 10.0 * 6.208787441253662
Epoch 2810, val loss: 2.060162305831909
Epoch 2820, training loss: 62.096832275390625 = 0.007602953352034092 + 10.0 * 6.208922863006592
Epoch 2820, val loss: 2.0625078678131104
Epoch 2830, training loss: 62.151222229003906 = 0.007532686460763216 + 10.0 * 6.21436882019043
Epoch 2830, val loss: 2.064708948135376
Epoch 2840, training loss: 62.10044479370117 = 0.007457287050783634 + 10.0 * 6.209298610687256
Epoch 2840, val loss: 2.0673069953918457
Epoch 2850, training loss: 62.11488723754883 = 0.007387587334960699 + 10.0 * 6.210749626159668
Epoch 2850, val loss: 2.0695884227752686
Epoch 2860, training loss: 62.10093688964844 = 0.007317340467125177 + 10.0 * 6.209362030029297
Epoch 2860, val loss: 2.071765422821045
Epoch 2870, training loss: 62.142616271972656 = 0.007249800954014063 + 10.0 * 6.213536739349365
Epoch 2870, val loss: 2.07407546043396
Epoch 2880, training loss: 62.0994873046875 = 0.007178463973104954 + 10.0 * 6.209230899810791
Epoch 2880, val loss: 2.0758237838745117
Epoch 2890, training loss: 62.09828186035156 = 0.007111263461410999 + 10.0 * 6.2091169357299805
Epoch 2890, val loss: 2.0783231258392334
Epoch 2900, training loss: 62.08679962158203 = 0.007045995909720659 + 10.0 * 6.207975387573242
Epoch 2900, val loss: 2.0804686546325684
Epoch 2910, training loss: 62.08443069458008 = 0.0069838277995586395 + 10.0 * 6.207744598388672
Epoch 2910, val loss: 2.0829577445983887
Epoch 2920, training loss: 62.110137939453125 = 0.006922333035618067 + 10.0 * 6.210321426391602
Epoch 2920, val loss: 2.084949493408203
Epoch 2930, training loss: 62.10848617553711 = 0.006858814507722855 + 10.0 * 6.21016263961792
Epoch 2930, val loss: 2.086869955062866
Epoch 2940, training loss: 62.10621643066406 = 0.006796782836318016 + 10.0 * 6.209941864013672
Epoch 2940, val loss: 2.0886077880859375
Epoch 2950, training loss: 62.09158706665039 = 0.006735015194863081 + 10.0 * 6.208485126495361
Epoch 2950, val loss: 2.091146469116211
Epoch 2960, training loss: 62.08319091796875 = 0.006676452700048685 + 10.0 * 6.207651615142822
Epoch 2960, val loss: 2.093137741088867
Epoch 2970, training loss: 62.104454040527344 = 0.006619962863624096 + 10.0 * 6.209783554077148
Epoch 2970, val loss: 2.0949385166168213
Epoch 2980, training loss: 62.083831787109375 = 0.00656077079474926 + 10.0 * 6.207726955413818
Epoch 2980, val loss: 2.0972819328308105
Epoch 2990, training loss: 62.12007522583008 = 0.006502967793494463 + 10.0 * 6.211357116699219
Epoch 2990, val loss: 2.0992937088012695
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 87.91975402832031 = 1.9512923955917358 + 10.0 * 8.596845626831055
Epoch 0, val loss: 1.95512855052948
Epoch 10, training loss: 87.90446472167969 = 1.9408828020095825 + 10.0 * 8.596358299255371
Epoch 10, val loss: 1.9440728425979614
Epoch 20, training loss: 87.85526275634766 = 1.9277914762496948 + 10.0 * 8.592747688293457
Epoch 20, val loss: 1.9298726320266724
Epoch 30, training loss: 87.5914535522461 = 1.9105429649353027 + 10.0 * 8.568090438842773
Epoch 30, val loss: 1.9107733964920044
Epoch 40, training loss: 86.25482940673828 = 1.8904454708099365 + 10.0 * 8.43643856048584
Epoch 40, val loss: 1.8893858194351196
Epoch 50, training loss: 82.34491729736328 = 1.8678103685379028 + 10.0 * 8.047710418701172
Epoch 50, val loss: 1.8656388521194458
Epoch 60, training loss: 78.6274642944336 = 1.8486759662628174 + 10.0 * 7.677878379821777
Epoch 60, val loss: 1.8479400873184204
Epoch 70, training loss: 73.81839752197266 = 1.8385164737701416 + 10.0 * 7.197988033294678
Epoch 70, val loss: 1.8399615287780762
Epoch 80, training loss: 71.41731262207031 = 1.8332233428955078 + 10.0 * 6.958409309387207
Epoch 80, val loss: 1.8355658054351807
Epoch 90, training loss: 70.07073974609375 = 1.824357509613037 + 10.0 * 6.824638366699219
Epoch 90, val loss: 1.82708740234375
Epoch 100, training loss: 69.13720703125 = 1.8151429891586304 + 10.0 * 6.73220682144165
Epoch 100, val loss: 1.8183507919311523
Epoch 110, training loss: 68.53163146972656 = 1.8073019981384277 + 10.0 * 6.672432899475098
Epoch 110, val loss: 1.8107677698135376
Epoch 120, training loss: 68.08937072753906 = 1.8000587224960327 + 10.0 * 6.628931522369385
Epoch 120, val loss: 1.8036853075027466
Epoch 130, training loss: 67.74403381347656 = 1.7930251359939575 + 10.0 * 6.595101356506348
Epoch 130, val loss: 1.7968485355377197
Epoch 140, training loss: 67.45177459716797 = 1.7858664989471436 + 10.0 * 6.566590785980225
Epoch 140, val loss: 1.7900445461273193
Epoch 150, training loss: 67.20348358154297 = 1.7786014080047607 + 10.0 * 6.542488098144531
Epoch 150, val loss: 1.7832167148590088
Epoch 160, training loss: 66.9869155883789 = 1.7712602615356445 + 10.0 * 6.521564960479736
Epoch 160, val loss: 1.776464819908142
Epoch 170, training loss: 66.8163833618164 = 1.7635172605514526 + 10.0 * 6.505286693572998
Epoch 170, val loss: 1.769535779953003
Epoch 180, training loss: 66.6286849975586 = 1.755362629890442 + 10.0 * 6.487331867218018
Epoch 180, val loss: 1.7624001502990723
Epoch 190, training loss: 66.47856903076172 = 1.7466977834701538 + 10.0 * 6.47318696975708
Epoch 190, val loss: 1.7549175024032593
Epoch 200, training loss: 66.36343383789062 = 1.737341046333313 + 10.0 * 6.46260929107666
Epoch 200, val loss: 1.7470110654830933
Epoch 210, training loss: 66.21540069580078 = 1.7273985147476196 + 10.0 * 6.4487996101379395
Epoch 210, val loss: 1.73861825466156
Epoch 220, training loss: 66.09593200683594 = 1.7167243957519531 + 10.0 * 6.437921047210693
Epoch 220, val loss: 1.7297017574310303
Epoch 230, training loss: 66.0042495727539 = 1.7052538394927979 + 10.0 * 6.429899215698242
Epoch 230, val loss: 1.7202138900756836
Epoch 240, training loss: 65.9402847290039 = 1.6928915977478027 + 10.0 * 6.424738883972168
Epoch 240, val loss: 1.7100850343704224
Epoch 250, training loss: 65.81267547607422 = 1.6796759366989136 + 10.0 * 6.413300514221191
Epoch 250, val loss: 1.6992437839508057
Epoch 260, training loss: 65.71475982666016 = 1.6655744314193726 + 10.0 * 6.404918670654297
Epoch 260, val loss: 1.6877803802490234
Epoch 270, training loss: 65.63229370117188 = 1.6505900621414185 + 10.0 * 6.398169994354248
Epoch 270, val loss: 1.6755826473236084
Epoch 280, training loss: 65.59005737304688 = 1.6346077919006348 + 10.0 * 6.39554500579834
Epoch 280, val loss: 1.6625573635101318
Epoch 290, training loss: 65.48571014404297 = 1.617612600326538 + 10.0 * 6.386809349060059
Epoch 290, val loss: 1.6488945484161377
Epoch 300, training loss: 65.40292358398438 = 1.5997908115386963 + 10.0 * 6.380313396453857
Epoch 300, val loss: 1.6345373392105103
Epoch 310, training loss: 65.36463165283203 = 1.581043004989624 + 10.0 * 6.378358840942383
Epoch 310, val loss: 1.6195287704467773
Epoch 320, training loss: 65.26143646240234 = 1.5614612102508545 + 10.0 * 6.369997501373291
Epoch 320, val loss: 1.6038581132888794
Epoch 330, training loss: 65.19396209716797 = 1.541082501411438 + 10.0 * 6.365288257598877
Epoch 330, val loss: 1.5876628160476685
Epoch 340, training loss: 65.15709686279297 = 1.5198918581008911 + 10.0 * 6.363720417022705
Epoch 340, val loss: 1.5709329843521118
Epoch 350, training loss: 65.07754516601562 = 1.4980627298355103 + 10.0 * 6.357948303222656
Epoch 350, val loss: 1.55367910861969
Epoch 360, training loss: 65.00106048583984 = 1.4755834341049194 + 10.0 * 6.352547645568848
Epoch 360, val loss: 1.536160945892334
Epoch 370, training loss: 64.94381713867188 = 1.452564001083374 + 10.0 * 6.349125385284424
Epoch 370, val loss: 1.518251895904541
Epoch 380, training loss: 64.88330078125 = 1.4288673400878906 + 10.0 * 6.345442771911621
Epoch 380, val loss: 1.4999662637710571
Epoch 390, training loss: 64.82476043701172 = 1.4047454595565796 + 10.0 * 6.342001438140869
Epoch 390, val loss: 1.4815162420272827
Epoch 400, training loss: 64.766357421875 = 1.3802026510238647 + 10.0 * 6.338615894317627
Epoch 400, val loss: 1.4630182981491089
Epoch 410, training loss: 64.72697448730469 = 1.3552800416946411 + 10.0 * 6.337169170379639
Epoch 410, val loss: 1.444400429725647
Epoch 420, training loss: 64.69189453125 = 1.3299572467803955 + 10.0 * 6.336193561553955
Epoch 420, val loss: 1.4254496097564697
Epoch 430, training loss: 64.60791778564453 = 1.304292917251587 + 10.0 * 6.330362319946289
Epoch 430, val loss: 1.4065208435058594
Epoch 440, training loss: 64.55757141113281 = 1.2783317565917969 + 10.0 * 6.3279242515563965
Epoch 440, val loss: 1.387589693069458
Epoch 450, training loss: 64.51351165771484 = 1.2520525455474854 + 10.0 * 6.326145648956299
Epoch 450, val loss: 1.3683152198791504
Epoch 460, training loss: 64.4576187133789 = 1.2255994081497192 + 10.0 * 6.323201656341553
Epoch 460, val loss: 1.3491535186767578
Epoch 470, training loss: 64.40556335449219 = 1.1990571022033691 + 10.0 * 6.320650577545166
Epoch 470, val loss: 1.3301646709442139
Epoch 480, training loss: 64.35871887207031 = 1.172402024269104 + 10.0 * 6.318631649017334
Epoch 480, val loss: 1.3112335205078125
Epoch 490, training loss: 64.36888122558594 = 1.1456538438796997 + 10.0 * 6.322322368621826
Epoch 490, val loss: 1.2922823429107666
Epoch 500, training loss: 64.28765869140625 = 1.1186944246292114 + 10.0 * 6.316896438598633
Epoch 500, val loss: 1.273436427116394
Epoch 510, training loss: 64.23332977294922 = 1.0919363498687744 + 10.0 * 6.314139366149902
Epoch 510, val loss: 1.2548656463623047
Epoch 520, training loss: 64.18047332763672 = 1.0653865337371826 + 10.0 * 6.311508655548096
Epoch 520, val loss: 1.2366300821304321
Epoch 530, training loss: 64.18099212646484 = 1.0390352010726929 + 10.0 * 6.3141961097717285
Epoch 530, val loss: 1.2186192274093628
Epoch 540, training loss: 64.11279296875 = 1.0127949714660645 + 10.0 * 6.309999942779541
Epoch 540, val loss: 1.200961947441101
Epoch 550, training loss: 64.05148315429688 = 0.9869915246963501 + 10.0 * 6.3064494132995605
Epoch 550, val loss: 1.1838487386703491
Epoch 560, training loss: 64.01455688476562 = 0.9616395831108093 + 10.0 * 6.305291652679443
Epoch 560, val loss: 1.1672168970108032
Epoch 570, training loss: 64.00779724121094 = 0.936669111251831 + 10.0 * 6.307112693786621
Epoch 570, val loss: 1.1509727239608765
Epoch 580, training loss: 63.9401969909668 = 0.9121878743171692 + 10.0 * 6.30280065536499
Epoch 580, val loss: 1.1353133916854858
Epoch 590, training loss: 63.93550109863281 = 0.8881661295890808 + 10.0 * 6.304733753204346
Epoch 590, val loss: 1.1202510595321655
Epoch 600, training loss: 63.87260437011719 = 0.864773690700531 + 10.0 * 6.300783157348633
Epoch 600, val loss: 1.105774998664856
Epoch 610, training loss: 63.820072174072266 = 0.841933012008667 + 10.0 * 6.297813892364502
Epoch 610, val loss: 1.0920995473861694
Epoch 620, training loss: 63.785125732421875 = 0.819724977016449 + 10.0 * 6.296540260314941
Epoch 620, val loss: 1.0790742635726929
Epoch 630, training loss: 63.82064437866211 = 0.7979966402053833 + 10.0 * 6.30226469039917
Epoch 630, val loss: 1.0666271448135376
Epoch 640, training loss: 63.736358642578125 = 0.7768815755844116 + 10.0 * 6.295947551727295
Epoch 640, val loss: 1.0547226667404175
Epoch 650, training loss: 63.68178939819336 = 0.7563910484313965 + 10.0 * 6.292540073394775
Epoch 650, val loss: 1.0437028408050537
Epoch 660, training loss: 63.66098403930664 = 0.7365444302558899 + 10.0 * 6.292443752288818
Epoch 660, val loss: 1.0333960056304932
Epoch 670, training loss: 63.641483306884766 = 0.7172221541404724 + 10.0 * 6.292426109313965
Epoch 670, val loss: 1.023544430732727
Epoch 680, training loss: 63.634521484375 = 0.6984029412269592 + 10.0 * 6.293612003326416
Epoch 680, val loss: 1.014327883720398
Epoch 690, training loss: 63.56244659423828 = 0.6801598072052002 + 10.0 * 6.288228511810303
Epoch 690, val loss: 1.0057640075683594
Epoch 700, training loss: 63.530391693115234 = 0.6625057458877563 + 10.0 * 6.286788463592529
Epoch 700, val loss: 0.9978665113449097
Epoch 710, training loss: 63.52537155151367 = 0.6453949809074402 + 10.0 * 6.287997722625732
Epoch 710, val loss: 0.9904138445854187
Epoch 720, training loss: 63.493099212646484 = 0.6287426352500916 + 10.0 * 6.286435604095459
Epoch 720, val loss: 0.9835955500602722
Epoch 730, training loss: 63.480709075927734 = 0.6125703454017639 + 10.0 * 6.286813735961914
Epoch 730, val loss: 0.9772452116012573
Epoch 740, training loss: 63.43587112426758 = 0.5969959497451782 + 10.0 * 6.2838873863220215
Epoch 740, val loss: 0.9714928269386292
Epoch 750, training loss: 63.40388488769531 = 0.5818834900856018 + 10.0 * 6.282200336456299
Epoch 750, val loss: 0.9662532210350037
Epoch 760, training loss: 63.404701232910156 = 0.5672194957733154 + 10.0 * 6.283748149871826
Epoch 760, val loss: 0.9614498615264893
Epoch 770, training loss: 63.38969802856445 = 0.5529078245162964 + 10.0 * 6.283679008483887
Epoch 770, val loss: 0.9570506811141968
Epoch 780, training loss: 63.34884262084961 = 0.5389807224273682 + 10.0 * 6.2809858322143555
Epoch 780, val loss: 0.9529603123664856
Epoch 790, training loss: 63.30807113647461 = 0.5254863500595093 + 10.0 * 6.278258323669434
Epoch 790, val loss: 0.9494262933731079
Epoch 800, training loss: 63.28469467163086 = 0.512410581111908 + 10.0 * 6.277228355407715
Epoch 800, val loss: 0.9463161826133728
Epoch 810, training loss: 63.29256820678711 = 0.4996952712535858 + 10.0 * 6.279287338256836
Epoch 810, val loss: 0.9434546232223511
Epoch 820, training loss: 63.24846649169922 = 0.4871909022331238 + 10.0 * 6.276127815246582
Epoch 820, val loss: 0.9408993124961853
Epoch 830, training loss: 63.246246337890625 = 0.4750741422176361 + 10.0 * 6.2771172523498535
Epoch 830, val loss: 0.9388253092765808
Epoch 840, training loss: 63.20179748535156 = 0.46322911977767944 + 10.0 * 6.2738566398620605
Epoch 840, val loss: 0.936961829662323
Epoch 850, training loss: 63.184295654296875 = 0.4517097771167755 + 10.0 * 6.273258686065674
Epoch 850, val loss: 0.9354271292686462
Epoch 860, training loss: 63.16946792602539 = 0.440478652715683 + 10.0 * 6.2728986740112305
Epoch 860, val loss: 0.9344268441200256
Epoch 870, training loss: 63.19970703125 = 0.4294774532318115 + 10.0 * 6.277022838592529
Epoch 870, val loss: 0.9333584904670715
Epoch 880, training loss: 63.1294059753418 = 0.4186733663082123 + 10.0 * 6.271073341369629
Epoch 880, val loss: 0.9326868057250977
Epoch 890, training loss: 63.108760833740234 = 0.4081423580646515 + 10.0 * 6.27006196975708
Epoch 890, val loss: 0.9323700070381165
Epoch 900, training loss: 63.10308074951172 = 0.39787113666534424 + 10.0 * 6.2705206871032715
Epoch 900, val loss: 0.9321061968803406
Epoch 910, training loss: 63.084259033203125 = 0.3877798616886139 + 10.0 * 6.26964807510376
Epoch 910, val loss: 0.931976854801178
Epoch 920, training loss: 63.06645584106445 = 0.3778962790966034 + 10.0 * 6.268856048583984
Epoch 920, val loss: 0.9322249293327332
Epoch 930, training loss: 63.04726791381836 = 0.3682136535644531 + 10.0 * 6.267905235290527
Epoch 930, val loss: 0.9325284361839294
Epoch 940, training loss: 63.023197174072266 = 0.35874006152153015 + 10.0 * 6.266445636749268
Epoch 940, val loss: 0.9332596063613892
Epoch 950, training loss: 63.04821014404297 = 0.3494873344898224 + 10.0 * 6.269872188568115
Epoch 950, val loss: 0.9340243339538574
Epoch 960, training loss: 63.002830505371094 = 0.3403642177581787 + 10.0 * 6.266246795654297
Epoch 960, val loss: 0.9348345994949341
Epoch 970, training loss: 63.01286315917969 = 0.33145707845687866 + 10.0 * 6.26814079284668
Epoch 970, val loss: 0.9361373782157898
Epoch 980, training loss: 62.961036682128906 = 0.3227599561214447 + 10.0 * 6.263827323913574
Epoch 980, val loss: 0.9373959302902222
Epoch 990, training loss: 62.93938446044922 = 0.31424474716186523 + 10.0 * 6.262514114379883
Epoch 990, val loss: 0.939022421836853
Epoch 1000, training loss: 62.92264175415039 = 0.3059431314468384 + 10.0 * 6.261670112609863
Epoch 1000, val loss: 0.9406763911247253
Epoch 1010, training loss: 62.940494537353516 = 0.29781994223594666 + 10.0 * 6.264267444610596
Epoch 1010, val loss: 0.9424290657043457
Epoch 1020, training loss: 62.92829132080078 = 0.2898641526699066 + 10.0 * 6.263842582702637
Epoch 1020, val loss: 0.9442365169525146
Epoch 1030, training loss: 62.89093780517578 = 0.2820468246936798 + 10.0 * 6.260889053344727
Epoch 1030, val loss: 0.9461720585823059
Epoch 1040, training loss: 62.878929138183594 = 0.2744735777378082 + 10.0 * 6.260445594787598
Epoch 1040, val loss: 0.9482380151748657
Epoch 1050, training loss: 62.8630256652832 = 0.2671247720718384 + 10.0 * 6.259590148925781
Epoch 1050, val loss: 0.950507402420044
Epoch 1060, training loss: 62.87565612792969 = 0.25995534658432007 + 10.0 * 6.261569976806641
Epoch 1060, val loss: 0.9527315497398376
Epoch 1070, training loss: 62.85435485839844 = 0.2529973089694977 + 10.0 * 6.260135650634766
Epoch 1070, val loss: 0.9553792476654053
Epoch 1080, training loss: 62.81917953491211 = 0.24621976912021637 + 10.0 * 6.257296085357666
Epoch 1080, val loss: 0.9578608870506287
Epoch 1090, training loss: 62.80109405517578 = 0.23965978622436523 + 10.0 * 6.256143569946289
Epoch 1090, val loss: 0.9606342315673828
Epoch 1100, training loss: 62.81338882446289 = 0.23328782618045807 + 10.0 * 6.258009910583496
Epoch 1100, val loss: 0.9633007049560547
Epoch 1110, training loss: 62.7842903137207 = 0.22705337405204773 + 10.0 * 6.255723476409912
Epoch 1110, val loss: 0.966234564781189
Epoch 1120, training loss: 62.77902603149414 = 0.22100810706615448 + 10.0 * 6.255801677703857
Epoch 1120, val loss: 0.9691029787063599
Epoch 1130, training loss: 62.7569694519043 = 0.2151634693145752 + 10.0 * 6.254180431365967
Epoch 1130, val loss: 0.9724282622337341
Epoch 1140, training loss: 62.80253982543945 = 0.20952239632606506 + 10.0 * 6.259301662445068
Epoch 1140, val loss: 0.9756115078926086
Epoch 1150, training loss: 62.75650405883789 = 0.2039400190114975 + 10.0 * 6.255256175994873
Epoch 1150, val loss: 0.9785817265510559
Epoch 1160, training loss: 62.73629379272461 = 0.19859007000923157 + 10.0 * 6.253770351409912
Epoch 1160, val loss: 0.9823852777481079
Epoch 1170, training loss: 62.70870590209961 = 0.19341669976711273 + 10.0 * 6.251528739929199
Epoch 1170, val loss: 0.985870897769928
Epoch 1180, training loss: 62.69778823852539 = 0.18842713534832 + 10.0 * 6.250936031341553
Epoch 1180, val loss: 0.9897201061248779
Epoch 1190, training loss: 62.75425720214844 = 0.18357059359550476 + 10.0 * 6.257068634033203
Epoch 1190, val loss: 0.9933950901031494
Epoch 1200, training loss: 62.70547866821289 = 0.1788218468427658 + 10.0 * 6.2526655197143555
Epoch 1200, val loss: 0.997093141078949
Epoch 1210, training loss: 62.68828201293945 = 0.1742195188999176 + 10.0 * 6.251406192779541
Epoch 1210, val loss: 1.0011928081512451
Epoch 1220, training loss: 62.67123794555664 = 0.16978392004966736 + 10.0 * 6.250145435333252
Epoch 1220, val loss: 1.0050690174102783
Epoch 1230, training loss: 62.68465042114258 = 0.16548222303390503 + 10.0 * 6.251916885375977
Epoch 1230, val loss: 1.0091125965118408
Epoch 1240, training loss: 62.641849517822266 = 0.16127562522888184 + 10.0 * 6.2480573654174805
Epoch 1240, val loss: 1.0131728649139404
Epoch 1250, training loss: 62.63728332519531 = 0.15721945464611053 + 10.0 * 6.248006343841553
Epoch 1250, val loss: 1.0173567533493042
Epoch 1260, training loss: 62.673912048339844 = 0.15329788625240326 + 10.0 * 6.252061367034912
Epoch 1260, val loss: 1.0214751958847046
Epoch 1270, training loss: 62.65707015991211 = 0.149444580078125 + 10.0 * 6.250762462615967
Epoch 1270, val loss: 1.0257043838500977
Epoch 1280, training loss: 62.62376403808594 = 0.1456916183233261 + 10.0 * 6.247807502746582
Epoch 1280, val loss: 1.0299439430236816
Epoch 1290, training loss: 62.61235046386719 = 0.14208939671516418 + 10.0 * 6.247025966644287
Epoch 1290, val loss: 1.03447687625885
Epoch 1300, training loss: 62.640724182128906 = 0.1385909616947174 + 10.0 * 6.250213146209717
Epoch 1300, val loss: 1.0385586023330688
Epoch 1310, training loss: 62.590309143066406 = 0.13517707586288452 + 10.0 * 6.245512962341309
Epoch 1310, val loss: 1.0434046983718872
Epoch 1320, training loss: 62.57952117919922 = 0.1318656951189041 + 10.0 * 6.244765281677246
Epoch 1320, val loss: 1.0478335618972778
Epoch 1330, training loss: 62.61678695678711 = 0.1286456286907196 + 10.0 * 6.248814105987549
Epoch 1330, val loss: 1.0522257089614868
Epoch 1340, training loss: 62.574310302734375 = 0.1255185753107071 + 10.0 * 6.244879245758057
Epoch 1340, val loss: 1.0569616556167603
Epoch 1350, training loss: 62.56188201904297 = 0.1224605143070221 + 10.0 * 6.2439422607421875
Epoch 1350, val loss: 1.0614886283874512
Epoch 1360, training loss: 62.59386444091797 = 0.11950474977493286 + 10.0 * 6.247436046600342
Epoch 1360, val loss: 1.066260576248169
Epoch 1370, training loss: 62.56232833862305 = 0.11661063879728317 + 10.0 * 6.244571685791016
Epoch 1370, val loss: 1.0708614587783813
Epoch 1380, training loss: 62.558815002441406 = 0.11378995329141617 + 10.0 * 6.244502544403076
Epoch 1380, val loss: 1.0757018327713013
Epoch 1390, training loss: 62.552642822265625 = 0.11106839776039124 + 10.0 * 6.244157314300537
Epoch 1390, val loss: 1.0804725885391235
Epoch 1400, training loss: 62.52693557739258 = 0.10841988027095795 + 10.0 * 6.241851329803467
Epoch 1400, val loss: 1.085472822189331
Epoch 1410, training loss: 62.514427185058594 = 0.10585182905197144 + 10.0 * 6.2408576011657715
Epoch 1410, val loss: 1.0904947519302368
Epoch 1420, training loss: 62.5292854309082 = 0.10336694121360779 + 10.0 * 6.242591857910156
Epoch 1420, val loss: 1.0953410863876343
Epoch 1430, training loss: 62.53046417236328 = 0.10092793405056 + 10.0 * 6.242953300476074
Epoch 1430, val loss: 1.1004441976547241
Epoch 1440, training loss: 62.51073455810547 = 0.09855178743600845 + 10.0 * 6.241218090057373
Epoch 1440, val loss: 1.1053576469421387
Epoch 1450, training loss: 62.49282455444336 = 0.09624684602022171 + 10.0 * 6.239657402038574
Epoch 1450, val loss: 1.1105248928070068
Epoch 1460, training loss: 62.49468231201172 = 0.09401006996631622 + 10.0 * 6.240067481994629
Epoch 1460, val loss: 1.115525484085083
Epoch 1470, training loss: 62.5050163269043 = 0.09183725714683533 + 10.0 * 6.2413177490234375
Epoch 1470, val loss: 1.1206238269805908
Epoch 1480, training loss: 62.48704528808594 = 0.08972326666116714 + 10.0 * 6.239732265472412
Epoch 1480, val loss: 1.1260030269622803
Epoch 1490, training loss: 62.510169982910156 = 0.08766087144613266 + 10.0 * 6.242250919342041
Epoch 1490, val loss: 1.1308352947235107
Epoch 1500, training loss: 62.47425842285156 = 0.08564836531877518 + 10.0 * 6.238861083984375
Epoch 1500, val loss: 1.1361734867095947
Epoch 1510, training loss: 62.472354888916016 = 0.08369676023721695 + 10.0 * 6.238865852355957
Epoch 1510, val loss: 1.1412920951843262
Epoch 1520, training loss: 62.524349212646484 = 0.08181405812501907 + 10.0 * 6.244253635406494
Epoch 1520, val loss: 1.1464260816574097
Epoch 1530, training loss: 62.45932388305664 = 0.07992849498987198 + 10.0 * 6.237939357757568
Epoch 1530, val loss: 1.1514569520950317
Epoch 1540, training loss: 62.440673828125 = 0.07811935991048813 + 10.0 * 6.236255168914795
Epoch 1540, val loss: 1.1568595170974731
Epoch 1550, training loss: 62.43104553222656 = 0.07637764513492584 + 10.0 * 6.235466957092285
Epoch 1550, val loss: 1.1620787382125854
Epoch 1560, training loss: 62.4300651550293 = 0.07468428462743759 + 10.0 * 6.235538005828857
Epoch 1560, val loss: 1.167364239692688
Epoch 1570, training loss: 62.557533264160156 = 0.07303957641124725 + 10.0 * 6.248449325561523
Epoch 1570, val loss: 1.1722490787506104
Epoch 1580, training loss: 62.43941879272461 = 0.0713779404759407 + 10.0 * 6.236804008483887
Epoch 1580, val loss: 1.1777353286743164
Epoch 1590, training loss: 62.412349700927734 = 0.06979583948850632 + 10.0 * 6.234255313873291
Epoch 1590, val loss: 1.1831189393997192
Epoch 1600, training loss: 62.420223236083984 = 0.0682692676782608 + 10.0 * 6.235195636749268
Epoch 1600, val loss: 1.188056230545044
Epoch 1610, training loss: 62.47283935546875 = 0.0667753517627716 + 10.0 * 6.240606307983398
Epoch 1610, val loss: 1.1932035684585571
Epoch 1620, training loss: 62.41665267944336 = 0.0653153657913208 + 10.0 * 6.235133647918701
Epoch 1620, val loss: 1.1987481117248535
Epoch 1630, training loss: 62.397830963134766 = 0.06388922035694122 + 10.0 * 6.233394145965576
Epoch 1630, val loss: 1.203912377357483
Epoch 1640, training loss: 62.394805908203125 = 0.06252218782901764 + 10.0 * 6.2332282066345215
Epoch 1640, val loss: 1.2092419862747192
Epoch 1650, training loss: 62.45986557006836 = 0.061189308762550354 + 10.0 * 6.239867687225342
Epoch 1650, val loss: 1.214385986328125
