nohup: ignoring input
run_robust_acc.py:14: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0, add_edge_rate_2=0, attack='none', base_model='GCN', cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0005, cl_num_epochs=200, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, clf_weight=1, config='config.yaml', cont_batch_size=0, cont_weight=1, cuda=True, dataset='Pubmed', debug=True, device_id=1, drop_edge_rate_1=0.2, drop_edge_rate_2=0, drop_feat_rate_1=0.3, drop_feat_rate_2=0.2, dropout=0.5, encoder_model='Grace', hidden=128, if_smoothed=False, inv_weight=1, no_cuda=False, noisy_level=0.3, num_hidden=128, num_proj_hidden=128, prob=0.8, seed=10, select_target_ratio=0.1, tau=0.1, test_model='GCN', train_lr=0.01, weight_decay=0.0005)
beta 0.5
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.58146858215332
Epoch 10, training loss: 9.958293914794922
Epoch 20, training loss: 8.947525978088379
Epoch 30, training loss: 8.55639362335205
Epoch 40, training loss: 8.105399131774902
Epoch 50, training loss: 7.8939337730407715
Epoch 60, training loss: 7.6191864013671875
Epoch 70, training loss: 7.431124687194824
Epoch 80, training loss: 7.316240310668945
Epoch 90, training loss: 7.223387718200684
Epoch 100, training loss: 7.115509510040283
Epoch 110, training loss: 7.0179643630981445
Epoch 120, training loss: 7.053554058074951
Epoch 130, training loss: 6.955539703369141
Epoch 140, training loss: 6.90929651260376
Epoch 150, training loss: 6.845756530761719
Epoch 160, training loss: 6.865368366241455
Epoch 170, training loss: 6.863136291503906
Epoch 180, training loss: 6.817375183105469
Epoch 190, training loss: 6.799342632293701
Epoch 200, training loss: 6.682956218719482
Epoch 210, training loss: 6.723217010498047
Epoch 220, training loss: 6.596027374267578
Epoch 230, training loss: 6.560888767242432
Epoch 240, training loss: 6.5371785163879395
Epoch 250, training loss: 6.56074857711792
Epoch 260, training loss: 6.560696601867676
Epoch 270, training loss: 6.4862565994262695
Epoch 280, training loss: 6.517788410186768
Epoch 290, training loss: 6.56320858001709
Epoch 300, training loss: 6.419577121734619
Epoch 310, training loss: 6.495781898498535
Epoch 320, training loss: 6.439881324768066
Epoch 330, training loss: 6.451060771942139
Epoch 340, training loss: 6.397982120513916
Epoch 350, training loss: 6.450254917144775
Epoch 360, training loss: 6.427250385284424
Epoch 370, training loss: 6.3950324058532715
Epoch 380, training loss: 6.335847854614258
Epoch 390, training loss: 6.300835609436035
Epoch 400, training loss: 6.3160200119018555
Epoch 410, training loss: 6.363386154174805
Epoch 420, training loss: 6.292716979980469
Epoch 430, training loss: 6.32009220123291
Epoch 440, training loss: 6.295248985290527
Epoch 450, training loss: 6.234614372253418
Epoch 460, training loss: 6.2432990074157715
Epoch 470, training loss: 6.284580230712891
Epoch 480, training loss: 6.258627414703369
Epoch 490, training loss: 6.245258331298828
none
Accuracy: 0.744
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.581555366516113
Epoch 10, training loss: 9.999828338623047
Epoch 20, training loss: 9.096238136291504
Epoch 30, training loss: 8.878191947937012
Epoch 40, training loss: 8.484066009521484
Epoch 50, training loss: 8.09534740447998
Epoch 60, training loss: 7.758430480957031
Epoch 70, training loss: 7.6344146728515625
Epoch 80, training loss: 7.44557523727417
Epoch 90, training loss: 7.337958812713623
Epoch 100, training loss: 7.320206165313721
Epoch 110, training loss: 7.306056022644043
Epoch 120, training loss: 7.199951171875
Epoch 130, training loss: 7.118023872375488
Epoch 140, training loss: 7.071034908294678
Epoch 150, training loss: 7.054396152496338
Epoch 160, training loss: 6.994061470031738
Epoch 170, training loss: 6.937362194061279
Epoch 180, training loss: 6.907784938812256
Epoch 190, training loss: 6.845171928405762
Epoch 200, training loss: 6.8498663902282715
Epoch 210, training loss: 6.760643005371094
Epoch 220, training loss: 6.8295698165893555
Epoch 230, training loss: 6.728766918182373
Epoch 240, training loss: 6.734521865844727
Epoch 250, training loss: 6.656083583831787
Epoch 260, training loss: 6.638160228729248
Epoch 270, training loss: 6.642819404602051
Epoch 280, training loss: 6.586805820465088
Epoch 290, training loss: 6.511588096618652
Epoch 300, training loss: 6.5685200691223145
Epoch 310, training loss: 6.676502227783203
Epoch 320, training loss: 6.490536689758301
Epoch 330, training loss: 6.46276330947876
Epoch 340, training loss: 6.482279300689697
Epoch 350, training loss: 6.412838459014893
Epoch 360, training loss: 6.520023345947266
Epoch 370, training loss: 6.431265830993652
Epoch 380, training loss: 6.351218223571777
Epoch 390, training loss: 6.40457010269165
Epoch 400, training loss: 6.338133811950684
Epoch 410, training loss: 6.273137092590332
Epoch 420, training loss: 6.301397800445557
Epoch 430, training loss: 6.268057823181152
Epoch 440, training loss: 6.246401309967041
Epoch 450, training loss: 6.2669453620910645
Epoch 460, training loss: 6.252540111541748
Epoch 470, training loss: 6.243193626403809
Epoch 480, training loss: 6.207804203033447
Epoch 490, training loss: 6.143355846405029
none
Accuracy: 0.781
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.58150863647461
Epoch 10, training loss: 10.102532386779785
Epoch 20, training loss: 8.909050941467285
Epoch 30, training loss: 8.642423629760742
Epoch 40, training loss: 8.29969596862793
Epoch 50, training loss: 8.033538818359375
Epoch 60, training loss: 7.871243000030518
Epoch 70, training loss: 7.625567436218262
Epoch 80, training loss: 7.487297058105469
Epoch 90, training loss: 7.44362211227417
Epoch 100, training loss: 7.277445316314697
Epoch 110, training loss: 7.2594780921936035
Epoch 120, training loss: 7.16984224319458
Epoch 130, training loss: 7.160667419433594
Epoch 140, training loss: 7.073113441467285
Epoch 150, training loss: 7.002137184143066
Epoch 160, training loss: 7.059236526489258
Epoch 170, training loss: 6.902005672454834
Epoch 180, training loss: 6.939793586730957
Epoch 190, training loss: 6.896767616271973
Epoch 200, training loss: 6.847488880157471
Epoch 210, training loss: 6.8286027908325195
Epoch 220, training loss: 6.8506999015808105
Epoch 230, training loss: 6.771284103393555
Epoch 240, training loss: 6.655971050262451
Epoch 250, training loss: 6.654043197631836
Epoch 260, training loss: 6.7232561111450195
Epoch 270, training loss: 6.660475730895996
Epoch 280, training loss: 6.642974853515625
Epoch 290, training loss: 6.553225994110107
Epoch 300, training loss: 6.557610511779785
Epoch 310, training loss: 6.513969421386719
Epoch 320, training loss: 6.556107997894287
Epoch 330, training loss: 6.477720260620117
Epoch 340, training loss: 6.467873573303223
Epoch 350, training loss: 6.418868541717529
Epoch 360, training loss: 6.380212783813477
Epoch 370, training loss: 6.36541223526001
Epoch 380, training loss: 6.388705730438232
Epoch 390, training loss: 6.344982147216797
Epoch 400, training loss: 6.381072044372559
Epoch 410, training loss: 6.340364456176758
Epoch 420, training loss: 6.317111492156982
Epoch 430, training loss: 6.333822250366211
Epoch 440, training loss: 6.3134613037109375
Epoch 450, training loss: 6.276573181152344
Epoch 460, training loss: 6.194472789764404
Epoch 470, training loss: 6.281224727630615
Epoch 480, training loss: 6.226590633392334
Epoch 490, training loss: 6.269574165344238
none
Accuracy: 0.762
beta 0.6
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.581429481506348
Epoch 10, training loss: 9.860967636108398
Epoch 20, training loss: 9.095003128051758
Epoch 30, training loss: 8.7333984375
Epoch 40, training loss: 8.455068588256836
Epoch 50, training loss: 8.133112907409668
Epoch 60, training loss: 7.917988300323486
Epoch 70, training loss: 7.750497817993164
Epoch 80, training loss: 7.571699142456055
Epoch 90, training loss: 7.41709566116333
Epoch 100, training loss: 7.270824432373047
Epoch 110, training loss: 7.182384014129639
Epoch 120, training loss: 7.089730739593506
Epoch 130, training loss: 7.045525550842285
Epoch 140, training loss: 6.981718063354492
Epoch 150, training loss: 7.005296230316162
Epoch 160, training loss: 6.91729211807251
Epoch 170, training loss: 6.881155014038086
Epoch 180, training loss: 6.809751987457275
Epoch 190, training loss: 6.76885461807251
Epoch 200, training loss: 6.792073726654053
Epoch 210, training loss: 6.7221856117248535
Epoch 220, training loss: 6.827688217163086
Epoch 230, training loss: 6.657420635223389
Epoch 240, training loss: 6.6472063064575195
Epoch 250, training loss: 6.660283088684082
Epoch 260, training loss: 6.568851947784424
Epoch 270, training loss: 6.568254470825195
Epoch 280, training loss: 6.548908710479736
Epoch 290, training loss: 6.603323936462402
Epoch 300, training loss: 6.567051410675049
Epoch 310, training loss: 6.539170265197754
Epoch 320, training loss: 6.529510021209717
Epoch 330, training loss: 6.506901741027832
Epoch 340, training loss: 6.58079195022583
Epoch 350, training loss: 6.546042442321777
Epoch 360, training loss: 6.491512775421143
Epoch 370, training loss: 6.553394794464111
Epoch 380, training loss: 6.51597785949707
Epoch 390, training loss: 6.451632499694824
Epoch 400, training loss: 6.471400260925293
Epoch 410, training loss: 6.438145637512207
Epoch 420, training loss: 6.437071323394775
Epoch 430, training loss: 6.409586429595947
Epoch 440, training loss: 6.472602844238281
Epoch 450, training loss: 6.486621856689453
Epoch 460, training loss: 6.390315055847168
Epoch 470, training loss: 6.423395156860352
Epoch 480, training loss: 6.405710220336914
Epoch 490, training loss: 6.39055871963501
none
Accuracy: 0.723
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.581414222717285
Epoch 10, training loss: 9.901925086975098
Epoch 20, training loss: 8.997064590454102
Epoch 30, training loss: 8.624791145324707
Epoch 40, training loss: 8.348323822021484
Epoch 50, training loss: 7.967938423156738
Epoch 60, training loss: 7.801638603210449
Epoch 70, training loss: 7.555529594421387
Epoch 80, training loss: 7.39594030380249
Epoch 90, training loss: 7.3175530433654785
Epoch 100, training loss: 7.281064510345459
Epoch 110, training loss: 7.19153356552124
Epoch 120, training loss: 7.1961774826049805
Epoch 130, training loss: 7.071408748626709
Epoch 140, training loss: 7.086032390594482
Epoch 150, training loss: 7.019675254821777
Epoch 160, training loss: 6.82898473739624
Epoch 170, training loss: 6.883243083953857
Epoch 180, training loss: 6.807255744934082
Epoch 190, training loss: 6.765933513641357
Epoch 200, training loss: 6.7500152587890625
Epoch 210, training loss: 6.6242570877075195
Epoch 220, training loss: 6.677052974700928
Epoch 230, training loss: 6.648983955383301
Epoch 240, training loss: 6.623889923095703
Epoch 250, training loss: 6.55541467666626
Epoch 260, training loss: 6.57039737701416
Epoch 270, training loss: 6.55295467376709
Epoch 280, training loss: 6.550912380218506
Epoch 290, training loss: 6.545955657958984
Epoch 300, training loss: 6.535316467285156
Epoch 310, training loss: 6.533196449279785
Epoch 320, training loss: 6.55895471572876
Epoch 330, training loss: 6.54243803024292
Epoch 340, training loss: 6.462069034576416
Epoch 350, training loss: 6.480340957641602
Epoch 360, training loss: 6.4787492752075195
Epoch 370, training loss: 6.485486030578613
Epoch 380, training loss: 6.503002643585205
Epoch 390, training loss: 6.4982428550720215
Epoch 400, training loss: 6.501623153686523
Epoch 410, training loss: 6.433763027191162
Epoch 420, training loss: 6.457116603851318
Epoch 430, training loss: 6.418004512786865
Epoch 440, training loss: 6.369535446166992
Epoch 450, training loss: 6.411347389221191
Epoch 460, training loss: 6.4426093101501465
Epoch 470, training loss: 6.3822479248046875
Epoch 480, training loss: 6.381663799285889
Epoch 490, training loss: 6.390822410583496
none
Accuracy: 0.758
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.581368446350098
Epoch 10, training loss: 10.060067176818848
Epoch 20, training loss: 8.957432746887207
Epoch 30, training loss: 8.63083267211914
Epoch 40, training loss: 8.320302963256836
Epoch 50, training loss: 7.957749843597412
Epoch 60, training loss: 7.711144924163818
Epoch 70, training loss: 7.487440586090088
Epoch 80, training loss: 7.516929626464844
Epoch 90, training loss: 7.361273288726807
Epoch 100, training loss: 7.304684638977051
Epoch 110, training loss: 7.301536560058594
Epoch 120, training loss: 7.267031669616699
Epoch 130, training loss: 7.186886787414551
Epoch 140, training loss: 7.207596778869629
Epoch 150, training loss: 7.1083173751831055
Epoch 160, training loss: 7.109811782836914
Epoch 170, training loss: 7.049503803253174
Epoch 180, training loss: 7.073302745819092
Epoch 190, training loss: 6.996283531188965
Epoch 200, training loss: 6.918553829193115
Epoch 210, training loss: 6.9460129737854
Epoch 220, training loss: 6.907987117767334
Epoch 230, training loss: 6.8412251472473145
Epoch 240, training loss: 6.7563796043396
Epoch 250, training loss: 6.774824142456055
Epoch 260, training loss: 6.711774826049805
Epoch 270, training loss: 6.682452201843262
Epoch 280, training loss: 6.632506847381592
Epoch 290, training loss: 6.6401262283325195
Epoch 300, training loss: 6.5320868492126465
Epoch 310, training loss: 6.506904602050781
Epoch 320, training loss: 6.502115726470947
Epoch 330, training loss: 6.515318870544434
Epoch 340, training loss: 6.468159198760986
Epoch 350, training loss: 6.507115364074707
Epoch 360, training loss: 6.476633071899414
Epoch 370, training loss: 6.4564714431762695
Epoch 380, training loss: 6.376675128936768
Epoch 390, training loss: 6.392003059387207
Epoch 400, training loss: 6.3547749519348145
Epoch 410, training loss: 6.37667989730835
Epoch 420, training loss: 6.405761241912842
Epoch 430, training loss: 6.336918830871582
Epoch 440, training loss: 6.422537803649902
Epoch 450, training loss: 6.278510570526123
Epoch 460, training loss: 6.313464641571045
Epoch 470, training loss: 6.281434535980225
Epoch 480, training loss: 6.315000057220459
Epoch 490, training loss: 6.2410969734191895
none
Accuracy: 0.76
beta 0.7
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.581480979919434
Epoch 10, training loss: 9.960216522216797
Epoch 20, training loss: 8.998980522155762
Epoch 30, training loss: 8.432069778442383
Epoch 40, training loss: 7.975678443908691
Epoch 50, training loss: 7.785781383514404
Epoch 60, training loss: 7.533766269683838
Epoch 70, training loss: 7.361104965209961
Epoch 80, training loss: 7.2337870597839355
Epoch 90, training loss: 7.190028190612793
Epoch 100, training loss: 7.157012462615967
Epoch 110, training loss: 7.012273788452148
Epoch 120, training loss: 6.985866546630859
Epoch 130, training loss: 6.974870204925537
Epoch 140, training loss: 6.873309135437012
Epoch 150, training loss: 6.862048149108887
Epoch 160, training loss: 6.851299285888672
Epoch 170, training loss: 6.914849758148193
Epoch 180, training loss: 6.849092960357666
Epoch 190, training loss: 6.797962188720703
Epoch 200, training loss: 6.801118850708008
Epoch 210, training loss: 6.7858099937438965
Epoch 220, training loss: 6.71751070022583
Epoch 230, training loss: 6.727823734283447
Epoch 240, training loss: 6.7280473709106445
Epoch 250, training loss: 6.769770622253418
Epoch 260, training loss: 6.689053535461426
Epoch 270, training loss: 6.6044602394104
Epoch 280, training loss: 6.608450889587402
Epoch 290, training loss: 6.6986846923828125
Epoch 300, training loss: 6.628299713134766
Epoch 310, training loss: 6.612615585327148
Epoch 320, training loss: 6.62385892868042
Epoch 330, training loss: 6.493621826171875
Epoch 340, training loss: 6.537111759185791
Epoch 350, training loss: 6.501371383666992
Epoch 360, training loss: 6.430692672729492
Epoch 370, training loss: 6.522439956665039
Epoch 380, training loss: 6.356543064117432
Epoch 390, training loss: 6.437829971313477
Epoch 400, training loss: 6.412527084350586
Epoch 410, training loss: 6.38134241104126
Epoch 420, training loss: 6.362212181091309
Epoch 430, training loss: 6.388864040374756
Epoch 440, training loss: 6.349798202514648
Epoch 450, training loss: 6.300379276275635
Epoch 460, training loss: 6.359262943267822
Epoch 470, training loss: 6.3155436515808105
Epoch 480, training loss: 6.295129776000977
Epoch 490, training loss: 6.286664962768555
none
Accuracy: 0.753
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.581347465515137
Epoch 10, training loss: 9.87082576751709
Epoch 20, training loss: 9.05440902709961
Epoch 30, training loss: 8.686368942260742
Epoch 40, training loss: 8.465466499328613
Epoch 50, training loss: 8.08169937133789
Epoch 60, training loss: 7.86709451675415
Epoch 70, training loss: 7.682049751281738
Epoch 80, training loss: 7.583310127258301
Epoch 90, training loss: 7.355560302734375
Epoch 100, training loss: 7.234906196594238
Epoch 110, training loss: 7.09780740737915
Epoch 120, training loss: 7.091424942016602
Epoch 130, training loss: 6.989817142486572
Epoch 140, training loss: 7.0250372886657715
Epoch 150, training loss: 7.00846529006958
Epoch 160, training loss: 6.97006893157959
Epoch 170, training loss: 6.86421537399292
Epoch 180, training loss: 6.841220378875732
Epoch 190, training loss: 6.829392433166504
Epoch 200, training loss: 6.88694953918457
Epoch 210, training loss: 6.858152389526367
Epoch 220, training loss: 6.8178300857543945
Epoch 230, training loss: 6.886792182922363
Epoch 240, training loss: 6.78770637512207
Epoch 250, training loss: 6.774025917053223
Epoch 260, training loss: 6.8721022605896
Epoch 270, training loss: 6.743948936462402
Epoch 280, training loss: 6.725205421447754
Epoch 290, training loss: 6.7160844802856445
Epoch 300, training loss: 6.684617042541504
Epoch 310, training loss: 6.745985507965088
Epoch 320, training loss: 6.718583106994629
Epoch 330, training loss: 6.625377178192139
Epoch 340, training loss: 6.659915924072266
Epoch 350, training loss: 6.58976411819458
Epoch 360, training loss: 6.687753677368164
Epoch 370, training loss: 6.610767364501953
Epoch 380, training loss: 6.597234725952148
Epoch 390, training loss: 6.565968036651611
Epoch 400, training loss: 6.521100044250488
Epoch 410, training loss: 6.496929168701172
Epoch 420, training loss: 6.49135160446167
Epoch 430, training loss: 6.461609840393066
Epoch 440, training loss: 6.380129814147949
Epoch 450, training loss: 6.419134616851807
Epoch 460, training loss: 6.302396774291992
Epoch 470, training loss: 6.286036491394043
Epoch 480, training loss: 6.358935832977295
Epoch 490, training loss: 6.299408435821533
none
Accuracy: 0.776
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.581463813781738
Epoch 10, training loss: 10.036230087280273
Epoch 20, training loss: 8.94392204284668
Epoch 30, training loss: 8.581193923950195
Epoch 40, training loss: 8.372870445251465
Epoch 50, training loss: 8.121894836425781
Epoch 60, training loss: 7.985011577606201
Epoch 70, training loss: 7.867329120635986
Epoch 80, training loss: 7.752390384674072
Epoch 90, training loss: 7.657001495361328
Epoch 100, training loss: 7.594022750854492
Epoch 110, training loss: 7.431493282318115
Epoch 120, training loss: 7.274116516113281
Epoch 130, training loss: 7.15941047668457
Epoch 140, training loss: 7.120144844055176
Epoch 150, training loss: 7.062830448150635
Epoch 160, training loss: 7.045609474182129
Epoch 170, training loss: 6.923426151275635
Epoch 180, training loss: 6.904767990112305
Epoch 190, training loss: 6.934662342071533
Epoch 200, training loss: 6.96235990524292
Epoch 210, training loss: 6.806179523468018
Epoch 220, training loss: 6.774958610534668
Epoch 230, training loss: 6.762037754058838
Epoch 240, training loss: 6.815852165222168
Epoch 250, training loss: 6.662189483642578
Epoch 260, training loss: 6.744476795196533
Epoch 270, training loss: 6.655553340911865
Epoch 280, training loss: 6.645610809326172
Epoch 290, training loss: 6.629571437835693
Epoch 300, training loss: 6.602100849151611
Epoch 310, training loss: 6.636232376098633
Epoch 320, training loss: 6.558743476867676
Epoch 330, training loss: 6.562534809112549
Epoch 340, training loss: 6.545618057250977
Epoch 350, training loss: 6.525575637817383
Epoch 360, training loss: 6.501191139221191
Epoch 370, training loss: 6.474550247192383
Epoch 380, training loss: 6.465541362762451
Epoch 390, training loss: 6.448399543762207
Epoch 400, training loss: 6.4204020500183105
Epoch 410, training loss: 6.3434014320373535
Epoch 420, training loss: 6.382485866546631
Epoch 430, training loss: 6.407553672790527
Epoch 440, training loss: 6.419367790222168
Epoch 450, training loss: 6.408342361450195
Epoch 460, training loss: 6.374785900115967
Epoch 470, training loss: 6.312534332275391
Epoch 480, training loss: 6.334829807281494
Epoch 490, training loss: 6.344720840454102
none
Accuracy: 0.783
beta 0.9
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.581607818603516
Epoch 10, training loss: 10.116772651672363
Epoch 20, training loss: 9.122968673706055
Epoch 30, training loss: 8.644835472106934
Epoch 40, training loss: 8.403711318969727
Epoch 50, training loss: 8.048070907592773
Epoch 60, training loss: 8.008000373840332
Epoch 70, training loss: 7.847609519958496
Epoch 80, training loss: 7.766079425811768
Epoch 90, training loss: 7.599145889282227
Epoch 100, training loss: 7.512744426727295
Epoch 110, training loss: 7.382209777832031
Epoch 120, training loss: 7.352958679199219
Epoch 130, training loss: 7.14870023727417
Epoch 140, training loss: 7.055569171905518
Epoch 150, training loss: 7.017856121063232
Epoch 160, training loss: 7.000554084777832
Epoch 170, training loss: 6.908469200134277
Epoch 180, training loss: 6.900173664093018
Epoch 190, training loss: 6.9055280685424805
Epoch 200, training loss: 6.841004371643066
Epoch 210, training loss: 6.828821659088135
Epoch 220, training loss: 6.828736305236816
Epoch 230, training loss: 6.763978481292725
Epoch 240, training loss: 6.7676100730896
Epoch 250, training loss: 6.730197429656982
Epoch 260, training loss: 6.675622463226318
Epoch 270, training loss: 6.675934791564941
Epoch 280, training loss: 6.709216594696045
Epoch 290, training loss: 6.591883659362793
Epoch 300, training loss: 6.574775218963623
Epoch 310, training loss: 6.56781530380249
Epoch 320, training loss: 6.542722225189209
Epoch 330, training loss: 6.474222183227539
Epoch 340, training loss: 6.423530101776123
Epoch 350, training loss: 6.523448944091797
Epoch 360, training loss: 6.549314975738525
Epoch 370, training loss: 6.417173862457275
Epoch 380, training loss: 6.455291748046875
Epoch 390, training loss: 6.476256370544434
Epoch 400, training loss: 6.411515235900879
Epoch 410, training loss: 6.389070510864258
Epoch 420, training loss: 6.421390056610107
Epoch 430, training loss: 6.4356513023376465
Epoch 440, training loss: 6.344614028930664
Epoch 450, training loss: 6.322916030883789
Epoch 460, training loss: 6.350729465484619
Epoch 470, training loss: 6.280627250671387
Epoch 480, training loss: 6.399753570556641
Epoch 490, training loss: 6.312789440155029
none
Accuracy: 0.77
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.58151912689209
Epoch 10, training loss: 9.986246109008789
Epoch 20, training loss: 9.057032585144043
Epoch 30, training loss: 8.671221733093262
Epoch 40, training loss: 8.441570281982422
Epoch 50, training loss: 8.117793083190918
Epoch 60, training loss: 7.854326248168945
Epoch 70, training loss: 7.635537624359131
Epoch 80, training loss: 7.486521244049072
Epoch 90, training loss: 7.382974624633789
Epoch 100, training loss: 7.390285968780518
Epoch 110, training loss: 7.382208347320557
Epoch 120, training loss: 7.251108169555664
Epoch 130, training loss: 7.242546081542969
Epoch 140, training loss: 7.18633508682251
Epoch 150, training loss: 7.134759902954102
Epoch 160, training loss: 7.069751262664795
Epoch 170, training loss: 6.966254711151123
Epoch 180, training loss: 6.936422348022461
Epoch 190, training loss: 6.923779487609863
Epoch 200, training loss: 6.880695343017578
Epoch 210, training loss: 6.909969806671143
Epoch 220, training loss: 6.908470630645752
Epoch 230, training loss: 6.809074878692627
Epoch 240, training loss: 6.7841997146606445
Epoch 250, training loss: 6.752960681915283
Epoch 260, training loss: 6.6730637550354
Epoch 270, training loss: 6.640204906463623
Epoch 280, training loss: 6.682583808898926
Epoch 290, training loss: 6.58655309677124
Epoch 300, training loss: 6.539095401763916
Epoch 310, training loss: 6.523962497711182
Epoch 320, training loss: 6.484950542449951
Epoch 330, training loss: 6.473270416259766
Epoch 340, training loss: 6.506763935089111
Epoch 350, training loss: 6.473245143890381
Epoch 360, training loss: 6.4076995849609375
Epoch 370, training loss: 6.41279411315918
Epoch 380, training loss: 6.341851711273193
Epoch 390, training loss: 6.4180006980896
Epoch 400, training loss: 6.292017459869385
Epoch 410, training loss: 6.337098121643066
Epoch 420, training loss: 6.275652885437012
Epoch 430, training loss: 6.3140106201171875
Epoch 440, training loss: 6.279685974121094
Epoch 450, training loss: 6.310173988342285
Epoch 460, training loss: 6.254654407501221
Epoch 470, training loss: 6.25501012802124
Epoch 480, training loss: 6.20190954208374
Epoch 490, training loss: 6.225856304168701
none
Accuracy: 0.733
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.58137321472168
Epoch 10, training loss: 9.863882064819336
Epoch 20, training loss: 9.047490119934082
Epoch 30, training loss: 8.648550033569336
Epoch 40, training loss: 8.31606674194336
Epoch 50, training loss: 8.097471237182617
Epoch 60, training loss: 7.880804061889648
Epoch 70, training loss: 7.810743808746338
Epoch 80, training loss: 7.649383544921875
Epoch 90, training loss: 7.500996112823486
Epoch 100, training loss: 7.397165298461914
Epoch 110, training loss: 7.355532646179199
Epoch 120, training loss: 7.374326705932617
Epoch 130, training loss: 7.2012763023376465
Epoch 140, training loss: 7.239895343780518
Epoch 150, training loss: 7.137370586395264
Epoch 160, training loss: 7.150370121002197
Epoch 170, training loss: 7.131042003631592
Epoch 180, training loss: 6.9488654136657715
Epoch 190, training loss: 6.887598037719727
Epoch 200, training loss: 6.834100246429443
Epoch 210, training loss: 6.754896640777588
Epoch 220, training loss: 6.842155456542969
Epoch 230, training loss: 6.702439785003662
Epoch 240, training loss: 6.640812873840332
Epoch 250, training loss: 6.5860748291015625
Epoch 260, training loss: 6.591074466705322
Epoch 270, training loss: 6.58457088470459
Epoch 280, training loss: 6.578874588012695
Epoch 290, training loss: 6.6677069664001465
Epoch 300, training loss: 6.472357273101807
Epoch 310, training loss: 6.498980522155762
Epoch 320, training loss: 6.458226203918457
Epoch 330, training loss: 6.415936470031738
Epoch 340, training loss: 6.3782453536987305
Epoch 350, training loss: 6.457748889923096
Epoch 360, training loss: 6.321824550628662
Epoch 370, training loss: 6.243897438049316
Epoch 380, training loss: 6.310768127441406
Epoch 390, training loss: 6.3644490242004395
Epoch 400, training loss: 6.296171188354492
Epoch 410, training loss: 6.258939743041992
Epoch 420, training loss: 6.269637107849121
Epoch 430, training loss: 6.165915489196777
Epoch 440, training loss: 6.234999179840088
Epoch 450, training loss: 6.142786979675293
Epoch 460, training loss: 6.153757572174072
Epoch 470, training loss: 6.12190055847168
Epoch 480, training loss: 6.123705863952637
Epoch 490, training loss: 6.146078586578369
none
Accuracy: 0.74
