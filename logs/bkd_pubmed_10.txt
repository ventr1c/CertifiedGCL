Begin epxeriment: cont_weight: 10 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.69926452636719 = 1.1088447570800781 + 10.0 * 10.359042167663574
Epoch 0, val loss: 1.1075553894042969
Epoch 10, training loss: 104.6518783569336 = 1.0964380502700806 + 10.0 * 10.355544090270996
Epoch 10, val loss: 1.094922423362732
Epoch 20, training loss: 104.04349517822266 = 1.0802226066589355 + 10.0 * 10.296327590942383
Epoch 20, val loss: 1.0787547826766968
Epoch 30, training loss: 99.09717559814453 = 1.0648845434188843 + 10.0 * 9.803228378295898
Epoch 30, val loss: 1.06354558467865
Epoch 40, training loss: 96.35368347167969 = 1.0515605211257935 + 10.0 * 9.53021240234375
Epoch 40, val loss: 1.0509523153305054
Epoch 50, training loss: 95.16531372070312 = 1.0401010513305664 + 10.0 * 9.412521362304688
Epoch 50, val loss: 1.039706826210022
Epoch 60, training loss: 94.49117279052734 = 1.0288866758346558 + 10.0 * 9.34622859954834
Epoch 60, val loss: 1.028833270072937
Epoch 70, training loss: 94.27078247070312 = 1.0169700384140015 + 10.0 * 9.3253812789917
Epoch 70, val loss: 1.0174448490142822
Epoch 80, training loss: 93.953369140625 = 1.0072007179260254 + 10.0 * 9.29461669921875
Epoch 80, val loss: 1.008289098739624
Epoch 90, training loss: 93.53057098388672 = 0.9988714456558228 + 10.0 * 9.253170013427734
Epoch 90, val loss: 1.0003100633621216
Epoch 100, training loss: 93.0715560913086 = 0.9888914823532104 + 10.0 * 9.208266258239746
Epoch 100, val loss: 0.9904908537864685
Epoch 110, training loss: 92.80479431152344 = 0.9767993092536926 + 10.0 * 9.182799339294434
Epoch 110, val loss: 0.9786548018455505
Epoch 120, training loss: 92.65475463867188 = 0.9632565379142761 + 10.0 * 9.169149398803711
Epoch 120, val loss: 0.9654276967048645
Epoch 130, training loss: 92.49060821533203 = 0.9488264322280884 + 10.0 * 9.15417766571045
Epoch 130, val loss: 0.9514962434768677
Epoch 140, training loss: 92.32357025146484 = 0.9337676763534546 + 10.0 * 9.1389799118042
Epoch 140, val loss: 0.9370766282081604
Epoch 150, training loss: 92.18151092529297 = 0.91726154088974 + 10.0 * 9.126424789428711
Epoch 150, val loss: 0.9211022853851318
Epoch 160, training loss: 92.05528259277344 = 0.8979092240333557 + 10.0 * 9.115736961364746
Epoch 160, val loss: 0.9022487998008728
Epoch 170, training loss: 91.95397186279297 = 0.8751063346862793 + 10.0 * 9.10788631439209
Epoch 170, val loss: 0.8804071545600891
Epoch 180, training loss: 91.86306762695312 = 0.8488485217094421 + 10.0 * 9.101422309875488
Epoch 180, val loss: 0.8552566170692444
Epoch 190, training loss: 91.779052734375 = 0.8192129135131836 + 10.0 * 9.095983505249023
Epoch 190, val loss: 0.8268826603889465
Epoch 200, training loss: 91.70050811767578 = 0.786403238773346 + 10.0 * 9.091410636901855
Epoch 200, val loss: 0.7955324649810791
Epoch 210, training loss: 91.62201690673828 = 0.7505487203598022 + 10.0 * 9.087146759033203
Epoch 210, val loss: 0.7613877654075623
Epoch 220, training loss: 91.57644653320312 = 0.7120714783668518 + 10.0 * 9.086437225341797
Epoch 220, val loss: 0.7248228788375854
Epoch 230, training loss: 91.49589538574219 = 0.6723946928977966 + 10.0 * 9.08234977722168
Epoch 230, val loss: 0.6874191164970398
Epoch 240, training loss: 91.42973327636719 = 0.633089542388916 + 10.0 * 9.07966423034668
Epoch 240, val loss: 0.6506531238555908
Epoch 250, training loss: 91.36845397949219 = 0.5949192047119141 + 10.0 * 9.077353477478027
Epoch 250, val loss: 0.6151436567306519
Epoch 260, training loss: 91.3287582397461 = 0.5587238073348999 + 10.0 * 9.077003479003906
Epoch 260, val loss: 0.581770658493042
Epoch 270, training loss: 91.26085662841797 = 0.525747537612915 + 10.0 * 9.073511123657227
Epoch 270, val loss: 0.5518811941146851
Epoch 280, training loss: 91.21476745605469 = 0.4966474771499634 + 10.0 * 9.07181167602539
Epoch 280, val loss: 0.5257911682128906
Epoch 290, training loss: 91.15959930419922 = 0.47135642170906067 + 10.0 * 9.06882381439209
Epoch 290, val loss: 0.5034471154212952
Epoch 300, training loss: 91.15272521972656 = 0.4495529234409332 + 10.0 * 9.070317268371582
Epoch 300, val loss: 0.484632670879364
Epoch 310, training loss: 91.07386779785156 = 0.4312497079372406 + 10.0 * 9.064261436462402
Epoch 310, val loss: 0.4690225124359131
Epoch 320, training loss: 91.03809356689453 = 0.4156874716281891 + 10.0 * 9.062240600585938
Epoch 320, val loss: 0.45611080527305603
Epoch 330, training loss: 91.03343200683594 = 0.40228885412216187 + 10.0 * 9.063114166259766
Epoch 330, val loss: 0.4452706277370453
Epoch 340, training loss: 90.98677062988281 = 0.39103010296821594 + 10.0 * 9.059574127197266
Epoch 340, val loss: 0.4366258978843689
Epoch 350, training loss: 90.95478057861328 = 0.3815252184867859 + 10.0 * 9.05732536315918
Epoch 350, val loss: 0.4293745458126068
Epoch 360, training loss: 90.92711639404297 = 0.3731115460395813 + 10.0 * 9.055400848388672
Epoch 360, val loss: 0.423223078250885
Epoch 370, training loss: 90.90132904052734 = 0.36552706360816956 + 10.0 * 9.053580284118652
Epoch 370, val loss: 0.41789308190345764
Epoch 380, training loss: 90.92428588867188 = 0.35871386528015137 + 10.0 * 9.056557655334473
Epoch 380, val loss: 0.41320210695266724
Epoch 390, training loss: 90.86854553222656 = 0.35276010632514954 + 10.0 * 9.051578521728516
Epoch 390, val loss: 0.40946099162101746
Epoch 400, training loss: 90.85039520263672 = 0.3474636971950531 + 10.0 * 9.05029296875
Epoch 400, val loss: 0.4062049090862274
Epoch 410, training loss: 90.82852935791016 = 0.34256917238235474 + 10.0 * 9.048596382141113
Epoch 410, val loss: 0.40331146121025085
Epoch 420, training loss: 90.81356811523438 = 0.3379843831062317 + 10.0 * 9.047558784484863
Epoch 420, val loss: 0.40075066685676575
Epoch 430, training loss: 90.85002899169922 = 0.3337121307849884 + 10.0 * 9.051631927490234
Epoch 430, val loss: 0.3984224200248718
Epoch 440, training loss: 90.79052734375 = 0.32982009649276733 + 10.0 * 9.04607105255127
Epoch 440, val loss: 0.39651918411254883
Epoch 450, training loss: 90.77381134033203 = 0.32621318101882935 + 10.0 * 9.044759750366211
Epoch 450, val loss: 0.39483657479286194
Epoch 460, training loss: 90.75932312011719 = 0.3227628469467163 + 10.0 * 9.043656349182129
Epoch 460, val loss: 0.393286794424057
Epoch 470, training loss: 90.75718688964844 = 0.3194595277309418 + 10.0 * 9.04377269744873
Epoch 470, val loss: 0.3918311297893524
Epoch 480, training loss: 90.74569702148438 = 0.3163432478904724 + 10.0 * 9.042935371398926
Epoch 480, val loss: 0.3906506896018982
Epoch 490, training loss: 90.7228012084961 = 0.31337109208106995 + 10.0 * 9.040943145751953
Epoch 490, val loss: 0.3894959092140198
Epoch 500, training loss: 90.77363586425781 = 0.3104958236217499 + 10.0 * 9.046314239501953
Epoch 500, val loss: 0.3883945047855377
Epoch 510, training loss: 90.73222351074219 = 0.3077698051929474 + 10.0 * 9.042445182800293
Epoch 510, val loss: 0.3875783085823059
Epoch 520, training loss: 90.70350646972656 = 0.30518248677253723 + 10.0 * 9.03983211517334
Epoch 520, val loss: 0.3867841064929962
Epoch 530, training loss: 90.68582153320312 = 0.3026883602142334 + 10.0 * 9.038312911987305
Epoch 530, val loss: 0.3860745429992676
Epoch 540, training loss: 90.67897033691406 = 0.3002488911151886 + 10.0 * 9.037872314453125
Epoch 540, val loss: 0.38535818457603455
Epoch 550, training loss: 90.66925048828125 = 0.297883003950119 + 10.0 * 9.037137031555176
Epoch 550, val loss: 0.38494008779525757
Epoch 560, training loss: 90.6644058227539 = 0.295606404542923 + 10.0 * 9.036879539489746
Epoch 560, val loss: 0.3843506872653961
Epoch 570, training loss: 90.65089416503906 = 0.29339221119880676 + 10.0 * 9.035749435424805
Epoch 570, val loss: 0.3839610815048218
Epoch 580, training loss: 90.65926361083984 = 0.29121828079223633 + 10.0 * 9.03680419921875
Epoch 580, val loss: 0.3836795687675476
Epoch 590, training loss: 90.64564514160156 = 0.28910526633262634 + 10.0 * 9.035654067993164
Epoch 590, val loss: 0.38345471024513245
Epoch 600, training loss: 90.65763092041016 = 0.2870435118675232 + 10.0 * 9.03705883026123
Epoch 600, val loss: 0.38304051756858826
Epoch 610, training loss: 90.62696075439453 = 0.2850554585456848 + 10.0 * 9.03419017791748
Epoch 610, val loss: 0.38296717405319214
Epoch 620, training loss: 90.6105728149414 = 0.2831079363822937 + 10.0 * 9.032746315002441
Epoch 620, val loss: 0.3827454149723053
Epoch 630, training loss: 90.6024169921875 = 0.2811828553676605 + 10.0 * 9.032123565673828
Epoch 630, val loss: 0.3827114701271057
Epoch 640, training loss: 90.64358520507812 = 0.279281347990036 + 10.0 * 9.036430358886719
Epoch 640, val loss: 0.38247159123420715
Epoch 650, training loss: 90.61489868164062 = 0.27743852138519287 + 10.0 * 9.033746719360352
Epoch 650, val loss: 0.3827592134475708
Epoch 660, training loss: 90.58650970458984 = 0.27563992142677307 + 10.0 * 9.031086921691895
Epoch 660, val loss: 0.3827629089355469
Epoch 670, training loss: 90.57479095458984 = 0.2738605737686157 + 10.0 * 9.0300931930542
Epoch 670, val loss: 0.38278499245643616
Epoch 680, training loss: 90.61174774169922 = 0.2721045911312103 + 10.0 * 9.033964157104492
Epoch 680, val loss: 0.3828469216823578
Epoch 690, training loss: 90.59272003173828 = 0.2703933119773865 + 10.0 * 9.032232284545898
Epoch 690, val loss: 0.38303491473197937
Epoch 700, training loss: 90.56536865234375 = 0.26872020959854126 + 10.0 * 9.029664993286133
Epoch 700, val loss: 0.38324347138404846
Epoch 710, training loss: 90.55220794677734 = 0.2670653462409973 + 10.0 * 9.02851390838623
Epoch 710, val loss: 0.3835339844226837
Epoch 720, training loss: 90.55345916748047 = 0.2654207646846771 + 10.0 * 9.028803825378418
Epoch 720, val loss: 0.3837794363498688
Epoch 730, training loss: 90.55370330810547 = 0.26379865407943726 + 10.0 * 9.028989791870117
Epoch 730, val loss: 0.3841146230697632
Epoch 740, training loss: 90.54496002197266 = 0.26220980286598206 + 10.0 * 9.028275489807129
Epoch 740, val loss: 0.3844187259674072
Epoch 750, training loss: 90.5445785522461 = 0.26063981652259827 + 10.0 * 9.028393745422363
Epoch 750, val loss: 0.3847549855709076
Epoch 760, training loss: 90.55610656738281 = 0.25909146666526794 + 10.0 * 9.029701232910156
Epoch 760, val loss: 0.38526561856269836
Epoch 770, training loss: 90.53153991699219 = 0.25756773352622986 + 10.0 * 9.027397155761719
Epoch 770, val loss: 0.3856068551540375
Epoch 780, training loss: 90.5194091796875 = 0.256059855222702 + 10.0 * 9.026334762573242
Epoch 780, val loss: 0.3861403465270996
Epoch 790, training loss: 90.51739501953125 = 0.2545621693134308 + 10.0 * 9.026283264160156
Epoch 790, val loss: 0.38668060302734375
Epoch 800, training loss: 90.53622436523438 = 0.2530807554721832 + 10.0 * 9.028314590454102
Epoch 800, val loss: 0.3871898353099823
Epoch 810, training loss: 90.5224609375 = 0.2516189217567444 + 10.0 * 9.027084350585938
Epoch 810, val loss: 0.38765010237693787
Epoch 820, training loss: 90.50019836425781 = 0.2501797676086426 + 10.0 * 9.025001525878906
Epoch 820, val loss: 0.38838788866996765
Epoch 830, training loss: 90.49683380126953 = 0.24875488877296448 + 10.0 * 9.024807929992676
Epoch 830, val loss: 0.3890388607978821
Epoch 840, training loss: 90.49662017822266 = 0.24733483791351318 + 10.0 * 9.024928092956543
Epoch 840, val loss: 0.389608770608902
Epoch 850, training loss: 90.51747131347656 = 0.24593377113342285 + 10.0 * 9.027153968811035
Epoch 850, val loss: 0.39042195677757263
Epoch 860, training loss: 90.48898315429688 = 0.24455086886882782 + 10.0 * 9.024442672729492
Epoch 860, val loss: 0.39101096987724304
Epoch 870, training loss: 90.48948669433594 = 0.24318291246891022 + 10.0 * 9.024630546569824
Epoch 870, val loss: 0.3918323814868927
Epoch 880, training loss: 90.47657012939453 = 0.2418270707130432 + 10.0 * 9.023473739624023
Epoch 880, val loss: 0.39261847734451294
Epoch 890, training loss: 90.47132873535156 = 0.24047903716564178 + 10.0 * 9.02308464050293
Epoch 890, val loss: 0.39343053102493286
Epoch 900, training loss: 90.48355102539062 = 0.23913973569869995 + 10.0 * 9.02444076538086
Epoch 900, val loss: 0.3943186104297638
Epoch 910, training loss: 90.47653198242188 = 0.23781515657901764 + 10.0 * 9.023871421813965
Epoch 910, val loss: 0.3952367901802063
Epoch 920, training loss: 90.46977233886719 = 0.23651161789894104 + 10.0 * 9.02332592010498
Epoch 920, val loss: 0.39610546827316284
Epoch 930, training loss: 90.45897674560547 = 0.23521655797958374 + 10.0 * 9.02237606048584
Epoch 930, val loss: 0.3970506191253662
Epoch 940, training loss: 90.45950317382812 = 0.23392942547798157 + 10.0 * 9.022557258605957
Epoch 940, val loss: 0.3980061709880829
Epoch 950, training loss: 90.4755859375 = 0.23265698552131653 + 10.0 * 9.024292945861816
Epoch 950, val loss: 0.3991408348083496
Epoch 960, training loss: 90.45603942871094 = 0.2313983142375946 + 10.0 * 9.02246379852295
Epoch 960, val loss: 0.3999760150909424
Epoch 970, training loss: 90.44550323486328 = 0.2301563024520874 + 10.0 * 9.02153491973877
Epoch 970, val loss: 0.4010237753391266
Epoch 980, training loss: 90.44036865234375 = 0.22891730070114136 + 10.0 * 9.02114486694336
Epoch 980, val loss: 0.40199148654937744
Epoch 990, training loss: 90.43566131591797 = 0.22768458724021912 + 10.0 * 9.020797729492188
Epoch 990, val loss: 0.403083860874176
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6851
Flip ASR: 0.6068/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.6881332397461 = 1.0975834131240845 + 10.0 * 10.359055519104004
Epoch 0, val loss: 1.0966955423355103
Epoch 10, training loss: 104.64175415039062 = 1.085702657699585 + 10.0 * 10.355605125427246
Epoch 10, val loss: 1.0846620798110962
Epoch 20, training loss: 104.0127182006836 = 1.0703736543655396 + 10.0 * 10.294234275817871
Epoch 20, val loss: 1.0693310499191284
Epoch 30, training loss: 99.20260620117188 = 1.0552558898925781 + 10.0 * 9.81473445892334
Epoch 30, val loss: 1.0541647672653198
Epoch 40, training loss: 96.585693359375 = 1.0397489070892334 + 10.0 * 9.554594039916992
Epoch 40, val loss: 1.0386766195297241
Epoch 50, training loss: 95.69235229492188 = 1.0254510641098022 + 10.0 * 9.466690063476562
Epoch 50, val loss: 1.0242011547088623
Epoch 60, training loss: 94.6867446899414 = 1.012436032295227 + 10.0 * 9.367430686950684
Epoch 60, val loss: 1.0111726522445679
Epoch 70, training loss: 94.34940338134766 = 0.9951119422912598 + 10.0 * 9.335429191589355
Epoch 70, val loss: 0.9938540458679199
Epoch 80, training loss: 94.06377410888672 = 0.9763981699943542 + 10.0 * 9.308737754821777
Epoch 80, val loss: 0.9757513999938965
Epoch 90, training loss: 93.73239135742188 = 0.9596421718597412 + 10.0 * 9.277275085449219
Epoch 90, val loss: 0.9593504071235657
Epoch 100, training loss: 93.32088470458984 = 0.9407304525375366 + 10.0 * 9.238016128540039
Epoch 100, val loss: 0.9406355023384094
Epoch 110, training loss: 93.05064392089844 = 0.920086145401001 + 10.0 * 9.213055610656738
Epoch 110, val loss: 0.9207072854042053
Epoch 120, training loss: 92.8646011352539 = 0.8980979323387146 + 10.0 * 9.196650505065918
Epoch 120, val loss: 0.899918794631958
Epoch 130, training loss: 92.67967987060547 = 0.8743939399719238 + 10.0 * 9.18052864074707
Epoch 130, val loss: 0.8773484826087952
Epoch 140, training loss: 92.50200653076172 = 0.8502813577651978 + 10.0 * 9.165172576904297
Epoch 140, val loss: 0.853834331035614
Epoch 150, training loss: 92.36665344238281 = 0.823642373085022 + 10.0 * 9.154301643371582
Epoch 150, val loss: 0.8277074098587036
Epoch 160, training loss: 92.28916931152344 = 0.7901306748390198 + 10.0 * 9.149904251098633
Epoch 160, val loss: 0.795357346534729
Epoch 170, training loss: 92.198974609375 = 0.7520293593406677 + 10.0 * 9.144694328308105
Epoch 170, val loss: 0.7588415145874023
Epoch 180, training loss: 92.09426879882812 = 0.7142729759216309 + 10.0 * 9.137999534606934
Epoch 180, val loss: 0.7230753302574158
Epoch 190, training loss: 91.99649810791016 = 0.6777086853981018 + 10.0 * 9.131878852844238
Epoch 190, val loss: 0.6887229681015015
Epoch 200, training loss: 91.8763198852539 = 0.6418090462684631 + 10.0 * 9.123451232910156
Epoch 200, val loss: 0.6551041007041931
Epoch 210, training loss: 91.7605972290039 = 0.6062015295028687 + 10.0 * 9.115439414978027
Epoch 210, val loss: 0.6220681071281433
Epoch 220, training loss: 91.69349670410156 = 0.5715312361717224 + 10.0 * 9.11219596862793
Epoch 220, val loss: 0.5899471640586853
Epoch 230, training loss: 91.57356262207031 = 0.5390233397483826 + 10.0 * 9.103453636169434
Epoch 230, val loss: 0.5606867671012878
Epoch 240, training loss: 91.49319458007812 = 0.5095525979995728 + 10.0 * 9.098363876342773
Epoch 240, val loss: 0.5342414379119873
Epoch 250, training loss: 91.46914672851562 = 0.48305559158325195 + 10.0 * 9.09860897064209
Epoch 250, val loss: 0.5108658671379089
Epoch 260, training loss: 91.37841033935547 = 0.4604255259037018 + 10.0 * 9.091798782348633
Epoch 260, val loss: 0.4915366768836975
Epoch 270, training loss: 91.31907653808594 = 0.4415143132209778 + 10.0 * 9.087756156921387
Epoch 270, val loss: 0.4755621552467346
Epoch 280, training loss: 91.29194641113281 = 0.42546167969703674 + 10.0 * 9.086648941040039
Epoch 280, val loss: 0.4623597264289856
Epoch 290, training loss: 91.2353744506836 = 0.4117238521575928 + 10.0 * 9.082365036010742
Epoch 290, val loss: 0.4512937664985657
Epoch 300, training loss: 91.19361114501953 = 0.39995262026786804 + 10.0 * 9.079365730285645
Epoch 300, val loss: 0.44207829236984253
Epoch 310, training loss: 91.18204498291016 = 0.38968661427497864 + 10.0 * 9.079236030578613
Epoch 310, val loss: 0.4342437982559204
Epoch 320, training loss: 91.14534759521484 = 0.38077759742736816 + 10.0 * 9.076457023620605
Epoch 320, val loss: 0.4278133511543274
Epoch 330, training loss: 91.10123443603516 = 0.3730442225933075 + 10.0 * 9.072818756103516
Epoch 330, val loss: 0.4223206639289856
Epoch 340, training loss: 91.07075500488281 = 0.36619195342063904 + 10.0 * 9.070455551147461
Epoch 340, val loss: 0.41762006282806396
Epoch 350, training loss: 91.04203796386719 = 0.35993117094039917 + 10.0 * 9.06821060180664
Epoch 350, val loss: 0.41349777579307556
Epoch 360, training loss: 91.02373504638672 = 0.35418224334716797 + 10.0 * 9.06695556640625
Epoch 360, val loss: 0.40991756319999695
Epoch 370, training loss: 91.00755310058594 = 0.34902358055114746 + 10.0 * 9.065853118896484
Epoch 370, val loss: 0.4067803919315338
Epoch 380, training loss: 90.964599609375 = 0.3443198502063751 + 10.0 * 9.062027931213379
Epoch 380, val loss: 0.40401357412338257
Epoch 390, training loss: 90.94197082519531 = 0.3398871421813965 + 10.0 * 9.060208320617676
Epoch 390, val loss: 0.40160271525382996
Epoch 400, training loss: 90.9781723022461 = 0.3356562852859497 + 10.0 * 9.064251899719238
Epoch 400, val loss: 0.39932629466056824
Epoch 410, training loss: 90.9246826171875 = 0.3317359983921051 + 10.0 * 9.059294700622559
Epoch 410, val loss: 0.39745622873306274
Epoch 420, training loss: 90.87837219238281 = 0.3281317353248596 + 10.0 * 9.055024147033691
Epoch 420, val loss: 0.3956657350063324
Epoch 430, training loss: 90.86634826660156 = 0.32471030950546265 + 10.0 * 9.054163932800293
Epoch 430, val loss: 0.3941687047481537
Epoch 440, training loss: 90.86438751220703 = 0.3214045464992523 + 10.0 * 9.054298400878906
Epoch 440, val loss: 0.3927563428878784
Epoch 450, training loss: 90.833740234375 = 0.3182525932788849 + 10.0 * 9.051548957824707
Epoch 450, val loss: 0.39154088497161865
Epoch 460, training loss: 90.82586669921875 = 0.3152340352535248 + 10.0 * 9.051063537597656
Epoch 460, val loss: 0.39036959409713745
Epoch 470, training loss: 90.80299377441406 = 0.31234458088874817 + 10.0 * 9.049064636230469
Epoch 470, val loss: 0.38935697078704834
Epoch 480, training loss: 90.78964233398438 = 0.30955585837364197 + 10.0 * 9.048008918762207
Epoch 480, val loss: 0.3884434998035431
Epoch 490, training loss: 90.79041290283203 = 0.3068476617336273 + 10.0 * 9.048357009887695
Epoch 490, val loss: 0.38760167360305786
Epoch 500, training loss: 90.77904510498047 = 0.30424919724464417 + 10.0 * 9.047479629516602
Epoch 500, val loss: 0.3869759142398834
Epoch 510, training loss: 90.75532531738281 = 0.3017497658729553 + 10.0 * 9.045357704162598
Epoch 510, val loss: 0.3863281011581421
Epoch 520, training loss: 90.74171447753906 = 0.29932859539985657 + 10.0 * 9.044238090515137
Epoch 520, val loss: 0.38572388887405396
Epoch 530, training loss: 90.73674011230469 = 0.2969628572463989 + 10.0 * 9.043977737426758
Epoch 530, val loss: 0.38521984219551086
Epoch 540, training loss: 90.7257080078125 = 0.2946561872959137 + 10.0 * 9.043105125427246
Epoch 540, val loss: 0.38491949439048767
Epoch 550, training loss: 90.71993255615234 = 0.2924196124076843 + 10.0 * 9.04275131225586
Epoch 550, val loss: 0.3844844698905945
Epoch 560, training loss: 90.70539093017578 = 0.2902592718601227 + 10.0 * 9.041513442993164
Epoch 560, val loss: 0.38426411151885986
Epoch 570, training loss: 90.69312286376953 = 0.2881443202495575 + 10.0 * 9.040497779846191
Epoch 570, val loss: 0.384064257144928
Epoch 580, training loss: 90.69210052490234 = 0.28605738282203674 + 10.0 * 9.040604591369629
Epoch 580, val loss: 0.3838590979576111
Epoch 590, training loss: 90.70667266845703 = 0.28402021527290344 + 10.0 * 9.042264938354492
Epoch 590, val loss: 0.3837953805923462
Epoch 600, training loss: 90.67475891113281 = 0.2820449769496918 + 10.0 * 9.039271354675293
Epoch 600, val loss: 0.3837326765060425
Epoch 610, training loss: 90.66023254394531 = 0.28011804819107056 + 10.0 * 9.03801155090332
Epoch 610, val loss: 0.3836846947669983
Epoch 620, training loss: 90.67796325683594 = 0.27821606397628784 + 10.0 * 9.0399751663208
Epoch 620, val loss: 0.3837985694408417
Epoch 630, training loss: 90.66383361816406 = 0.27635565400123596 + 10.0 * 9.038747787475586
Epoch 630, val loss: 0.3837220072746277
Epoch 640, training loss: 90.63807678222656 = 0.2745341360569 + 10.0 * 9.036354064941406
Epoch 640, val loss: 0.3838498294353485
Epoch 650, training loss: 90.63114929199219 = 0.27273204922676086 + 10.0 * 9.03584098815918
Epoch 650, val loss: 0.3839900493621826
Epoch 660, training loss: 90.66018676757812 = 0.270944744348526 + 10.0 * 9.038924217224121
Epoch 660, val loss: 0.38409411907196045
Epoch 670, training loss: 90.63056182861328 = 0.2692027986049652 + 10.0 * 9.03613567352295
Epoch 670, val loss: 0.38437512516975403
Epoch 680, training loss: 90.61141204833984 = 0.26748424768447876 + 10.0 * 9.034392356872559
Epoch 680, val loss: 0.3845493793487549
Epoch 690, training loss: 90.60589599609375 = 0.2657822370529175 + 10.0 * 9.034010887145996
Epoch 690, val loss: 0.38479775190353394
Epoch 700, training loss: 90.61493682861328 = 0.26410025358200073 + 10.0 * 9.035083770751953
Epoch 700, val loss: 0.3850972354412079
Epoch 710, training loss: 90.6272964477539 = 0.26244738698005676 + 10.0 * 9.036484718322754
Epoch 710, val loss: 0.38539546728134155
Epoch 720, training loss: 90.58920288085938 = 0.2608264982700348 + 10.0 * 9.032837867736816
Epoch 720, val loss: 0.38581573963165283
Epoch 730, training loss: 90.58028411865234 = 0.2592255473136902 + 10.0 * 9.032106399536133
Epoch 730, val loss: 0.38613593578338623
Epoch 740, training loss: 90.57469940185547 = 0.25762754678726196 + 10.0 * 9.031706809997559
Epoch 740, val loss: 0.38654768466949463
Epoch 750, training loss: 90.57771301269531 = 0.25604864954948425 + 10.0 * 9.032166481018066
Epoch 750, val loss: 0.3869977295398712
Epoch 760, training loss: 90.56364440917969 = 0.2544901371002197 + 10.0 * 9.030915260314941
Epoch 760, val loss: 0.3875197172164917
Epoch 770, training loss: 90.56851196289062 = 0.252956748008728 + 10.0 * 9.03155517578125
Epoch 770, val loss: 0.3880517780780792
Epoch 780, training loss: 90.55020904541016 = 0.2514462471008301 + 10.0 * 9.029875755310059
Epoch 780, val loss: 0.3884999752044678
Epoch 790, training loss: 90.54584503173828 = 0.24993829429149628 + 10.0 * 9.029590606689453
Epoch 790, val loss: 0.3890349864959717
Epoch 800, training loss: 90.58850860595703 = 0.24844838678836823 + 10.0 * 9.034006118774414
Epoch 800, val loss: 0.3896261155605316
Epoch 810, training loss: 90.5490493774414 = 0.24699245393276215 + 10.0 * 9.030205726623535
Epoch 810, val loss: 0.3902799189090729
Epoch 820, training loss: 90.5311508178711 = 0.24554532766342163 + 10.0 * 9.028560638427734
Epoch 820, val loss: 0.3908260464668274
Epoch 830, training loss: 90.53045654296875 = 0.24410997331142426 + 10.0 * 9.028635025024414
Epoch 830, val loss: 0.39151695370674133
Epoch 840, training loss: 90.52206420898438 = 0.2426864504814148 + 10.0 * 9.027937889099121
Epoch 840, val loss: 0.3921753466129303
Epoch 850, training loss: 90.53026580810547 = 0.2412755787372589 + 10.0 * 9.028899192810059
Epoch 850, val loss: 0.39275264739990234
Epoch 860, training loss: 90.5113296508789 = 0.23987405002117157 + 10.0 * 9.027145385742188
Epoch 860, val loss: 0.3936363458633423
Epoch 870, training loss: 90.517333984375 = 0.23848137259483337 + 10.0 * 9.027885437011719
Epoch 870, val loss: 0.3942505717277527
Epoch 880, training loss: 90.49969482421875 = 0.23710119724273682 + 10.0 * 9.026259422302246
Epoch 880, val loss: 0.3951910138130188
Epoch 890, training loss: 90.51024627685547 = 0.23573645949363708 + 10.0 * 9.027451515197754
Epoch 890, val loss: 0.3958927392959595
Epoch 900, training loss: 90.49309539794922 = 0.23437856137752533 + 10.0 * 9.025872230529785
Epoch 900, val loss: 0.3967514634132385
Epoch 910, training loss: 90.49291229248047 = 0.23303990066051483 + 10.0 * 9.02598762512207
Epoch 910, val loss: 0.3975840210914612
Epoch 920, training loss: 90.4835433959961 = 0.23170849680900574 + 10.0 * 9.02518367767334
Epoch 920, val loss: 0.39837974309921265
Epoch 930, training loss: 90.48320770263672 = 0.23038457334041595 + 10.0 * 9.02528190612793
Epoch 930, val loss: 0.39927828311920166
Epoch 940, training loss: 90.47720336914062 = 0.22906816005706787 + 10.0 * 9.024813652038574
Epoch 940, val loss: 0.4002265930175781
Epoch 950, training loss: 90.47997283935547 = 0.22776049375534058 + 10.0 * 9.02522087097168
Epoch 950, val loss: 0.4011583626270294
Epoch 960, training loss: 90.47865295410156 = 0.22646835446357727 + 10.0 * 9.02521800994873
Epoch 960, val loss: 0.40214574337005615
Epoch 970, training loss: 90.47367858886719 = 0.22518932819366455 + 10.0 * 9.024848937988281
Epoch 970, val loss: 0.4030834436416626
Epoch 980, training loss: 90.45578002929688 = 0.22391541302204132 + 10.0 * 9.023186683654785
Epoch 980, val loss: 0.4041118919849396
Epoch 990, training loss: 90.45420837402344 = 0.22264249622821808 + 10.0 * 9.023157119750977
Epoch 990, val loss: 0.4050818085670471
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.6897
Flip ASR: 0.6120/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.69068908691406 = 1.1007639169692993 + 10.0 * 10.358992576599121
Epoch 0, val loss: 1.1011760234832764
Epoch 10, training loss: 104.6358871459961 = 1.0891809463500977 + 10.0 * 10.354670524597168
Epoch 10, val loss: 1.0892634391784668
Epoch 20, training loss: 103.97211456298828 = 1.0738712549209595 + 10.0 * 10.289824485778809
Epoch 20, val loss: 1.073686122894287
Epoch 30, training loss: 99.0025863647461 = 1.0590765476226807 + 10.0 * 9.794351577758789
Epoch 30, val loss: 1.0586851835250854
Epoch 40, training loss: 95.90837097167969 = 1.0479774475097656 + 10.0 * 9.486039161682129
Epoch 40, val loss: 1.047716498374939
Epoch 50, training loss: 94.71121215820312 = 1.0368642807006836 + 10.0 * 9.36743450164795
Epoch 50, val loss: 1.0365183353424072
Epoch 60, training loss: 94.24745178222656 = 1.0251303911209106 + 10.0 * 9.322232246398926
Epoch 60, val loss: 1.0249744653701782
Epoch 70, training loss: 93.82677459716797 = 1.0146697759628296 + 10.0 * 9.281209945678711
Epoch 70, val loss: 1.0150721073150635
Epoch 80, training loss: 93.36265563964844 = 1.005017876625061 + 10.0 * 9.235763549804688
Epoch 80, val loss: 1.0057207345962524
Epoch 90, training loss: 93.06150817871094 = 0.9934845566749573 + 10.0 * 9.206802368164062
Epoch 90, val loss: 0.9945352673530579
Epoch 100, training loss: 92.80122375488281 = 0.9812967777252197 + 10.0 * 9.181992530822754
Epoch 100, val loss: 0.9828929305076599
Epoch 110, training loss: 92.6489486694336 = 0.9683035612106323 + 10.0 * 9.168065071105957
Epoch 110, val loss: 0.9703336358070374
Epoch 120, training loss: 92.4792709350586 = 0.9537296295166016 + 10.0 * 9.152554512023926
Epoch 120, val loss: 0.9563689827919006
Epoch 130, training loss: 92.30880737304688 = 0.9388918876647949 + 10.0 * 9.136991500854492
Epoch 130, val loss: 0.9422498941421509
Epoch 140, training loss: 92.17664337158203 = 0.9228010177612305 + 10.0 * 9.125384330749512
Epoch 140, val loss: 0.9267721176147461
Epoch 150, training loss: 92.07148742675781 = 0.9036462903022766 + 10.0 * 9.11678409576416
Epoch 150, val loss: 0.9084901213645935
Epoch 160, training loss: 91.9682846069336 = 0.8818774819374084 + 10.0 * 9.108640670776367
Epoch 160, val loss: 0.8877086043357849
Epoch 170, training loss: 91.88916778564453 = 0.8569856882095337 + 10.0 * 9.103218078613281
Epoch 170, val loss: 0.8641485571861267
Epoch 180, training loss: 91.79886627197266 = 0.8282603621482849 + 10.0 * 9.097060203552246
Epoch 180, val loss: 0.8366714119911194
Epoch 190, training loss: 91.72081756591797 = 0.7957532405853271 + 10.0 * 9.092506408691406
Epoch 190, val loss: 0.8057724237442017
Epoch 200, training loss: 91.6385269165039 = 0.7606720328330994 + 10.0 * 9.087785720825195
Epoch 200, val loss: 0.7726457118988037
Epoch 210, training loss: 91.55760955810547 = 0.7234088182449341 + 10.0 * 9.083419799804688
Epoch 210, val loss: 0.737513542175293
Epoch 220, training loss: 91.59259796142578 = 0.6839845776557922 + 10.0 * 9.090861320495605
Epoch 220, val loss: 0.7006198167800903
Epoch 230, training loss: 91.4263916015625 = 0.6448295712471008 + 10.0 * 9.078156471252441
Epoch 230, val loss: 0.6643202304840088
Epoch 240, training loss: 91.34466552734375 = 0.6073654890060425 + 10.0 * 9.073729515075684
Epoch 240, val loss: 0.6298366189002991
Epoch 250, training loss: 91.26339721679688 = 0.5719096064567566 + 10.0 * 9.069149017333984
Epoch 250, val loss: 0.5974909663200378
Epoch 260, training loss: 91.19732666015625 = 0.5386449694633484 + 10.0 * 9.065868377685547
Epoch 260, val loss: 0.567386269569397
Epoch 270, training loss: 91.15351104736328 = 0.5082176923751831 + 10.0 * 9.064529418945312
Epoch 270, val loss: 0.5403274297714233
Epoch 280, training loss: 91.09322357177734 = 0.4816814661026001 + 10.0 * 9.06115436553955
Epoch 280, val loss: 0.5170586109161377
Epoch 290, training loss: 91.05107116699219 = 0.4587372839450836 + 10.0 * 9.059232711791992
Epoch 290, val loss: 0.49739089608192444
Epoch 300, training loss: 91.0256576538086 = 0.43905001878738403 + 10.0 * 9.058660507202148
Epoch 300, val loss: 0.4809059500694275
Epoch 310, training loss: 90.98603820800781 = 0.4223864674568176 + 10.0 * 9.056365013122559
Epoch 310, val loss: 0.46733558177948
Epoch 320, training loss: 90.9521255493164 = 0.4082700312137604 + 10.0 * 9.0543851852417
Epoch 320, val loss: 0.4561949372291565
Epoch 330, training loss: 90.92572021484375 = 0.3960844576358795 + 10.0 * 9.052963256835938
Epoch 330, val loss: 0.4468684792518616
Epoch 340, training loss: 90.91899108886719 = 0.3854720890522003 + 10.0 * 9.053351402282715
Epoch 340, val loss: 0.439032644033432
Epoch 350, training loss: 90.89849090576172 = 0.37629589438438416 + 10.0 * 9.05221939086914
Epoch 350, val loss: 0.4326387047767639
Epoch 360, training loss: 90.86835479736328 = 0.36836186051368713 + 10.0 * 9.049999237060547
Epoch 360, val loss: 0.4272491931915283
Epoch 370, training loss: 90.8486099243164 = 0.36133167147636414 + 10.0 * 9.048727989196777
Epoch 370, val loss: 0.42269590497016907
Epoch 380, training loss: 90.8363037109375 = 0.35495737195014954 + 10.0 * 9.048134803771973
Epoch 380, val loss: 0.4187665283679962
Epoch 390, training loss: 90.84778594970703 = 0.3491979241371155 + 10.0 * 9.049859046936035
Epoch 390, val loss: 0.41540050506591797
Epoch 400, training loss: 90.80913543701172 = 0.3440306782722473 + 10.0 * 9.046510696411133
Epoch 400, val loss: 0.4124683737754822
Epoch 410, training loss: 90.78644561767578 = 0.33928364515304565 + 10.0 * 9.044715881347656
Epoch 410, val loss: 0.40994688868522644
Epoch 420, training loss: 90.82426452636719 = 0.33482810854911804 + 10.0 * 9.048943519592285
Epoch 420, val loss: 0.40774261951446533
Epoch 430, training loss: 90.76168823242188 = 0.3307144045829773 + 10.0 * 9.043097496032715
Epoch 430, val loss: 0.4057808816432953
Epoch 440, training loss: 90.74925231933594 = 0.3268846571445465 + 10.0 * 9.042237281799316
Epoch 440, val loss: 0.40400806069374084
Epoch 450, training loss: 90.7376937866211 = 0.32324615120887756 + 10.0 * 9.041444778442383
Epoch 450, val loss: 0.4024987518787384
Epoch 460, training loss: 90.78936004638672 = 0.3197596073150635 + 10.0 * 9.04695987701416
Epoch 460, val loss: 0.4012654721736908
Epoch 470, training loss: 90.72221374511719 = 0.31648150086402893 + 10.0 * 9.040573120117188
Epoch 470, val loss: 0.39997968077659607
Epoch 480, training loss: 90.70787048339844 = 0.3133934438228607 + 10.0 * 9.039447784423828
Epoch 480, val loss: 0.39885202050209045
Epoch 490, training loss: 90.69833374023438 = 0.3104126453399658 + 10.0 * 9.038792610168457
Epoch 490, val loss: 0.397908091545105
Epoch 500, training loss: 90.69338989257812 = 0.30753931403160095 + 10.0 * 9.03858470916748
Epoch 500, val loss: 0.3970857858657837
Epoch 510, training loss: 90.68794250488281 = 0.30481085181236267 + 10.0 * 9.038312911987305
Epoch 510, val loss: 0.3963184654712677
Epoch 520, training loss: 90.67190551757812 = 0.3021862804889679 + 10.0 * 9.036972045898438
Epoch 520, val loss: 0.3956238627433777
Epoch 530, training loss: 90.66352844238281 = 0.29963549971580505 + 10.0 * 9.036389350891113
Epoch 530, val loss: 0.3951248526573181
Epoch 540, training loss: 90.65441131591797 = 0.2971576452255249 + 10.0 * 9.035725593566895
Epoch 540, val loss: 0.39459943771362305
Epoch 550, training loss: 90.63602447509766 = 0.29475292563438416 + 10.0 * 9.034127235412598
Epoch 550, val loss: 0.3941500186920166
Epoch 560, training loss: 90.64655303955078 = 0.2924060523509979 + 10.0 * 9.035414695739746
Epoch 560, val loss: 0.39376211166381836
Epoch 570, training loss: 90.6293716430664 = 0.29012224078178406 + 10.0 * 9.03392505645752
Epoch 570, val loss: 0.3934735953807831
Epoch 580, training loss: 90.61992645263672 = 0.28789883852005005 + 10.0 * 9.033203125
Epoch 580, val loss: 0.39321455359458923
Epoch 590, training loss: 90.61212921142578 = 0.28573304414749146 + 10.0 * 9.032639503479004
Epoch 590, val loss: 0.3930334448814392
Epoch 600, training loss: 90.60059356689453 = 0.28363659977912903 + 10.0 * 9.031695365905762
Epoch 600, val loss: 0.3928712010383606
Epoch 610, training loss: 90.59166717529297 = 0.2815726399421692 + 10.0 * 9.031009674072266
Epoch 610, val loss: 0.39282727241516113
Epoch 620, training loss: 90.61103820800781 = 0.27953800559043884 + 10.0 * 9.033149719238281
Epoch 620, val loss: 0.3928174674510956
Epoch 630, training loss: 90.59662628173828 = 0.2775609493255615 + 10.0 * 9.031907081604004
Epoch 630, val loss: 0.3927837908267975
Epoch 640, training loss: 90.57366943359375 = 0.27562451362609863 + 10.0 * 9.029804229736328
Epoch 640, val loss: 0.3928106129169464
Epoch 650, training loss: 90.57325744628906 = 0.27372461557388306 + 10.0 * 9.029953002929688
Epoch 650, val loss: 0.39293137192726135
Epoch 660, training loss: 90.57019805908203 = 0.2718617022037506 + 10.0 * 9.029833793640137
Epoch 660, val loss: 0.3931213617324829
Epoch 670, training loss: 90.56585693359375 = 0.2700314223766327 + 10.0 * 9.029582023620605
Epoch 670, val loss: 0.39326342940330505
Epoch 680, training loss: 90.54956817626953 = 0.26822832226753235 + 10.0 * 9.0281343460083
Epoch 680, val loss: 0.3934660255908966
Epoch 690, training loss: 90.56389617919922 = 0.26645326614379883 + 10.0 * 9.029744148254395
Epoch 690, val loss: 0.3936949372291565
Epoch 700, training loss: 90.55839538574219 = 0.2647130489349365 + 10.0 * 9.02936840057373
Epoch 700, val loss: 0.39407604932785034
Epoch 710, training loss: 90.53314971923828 = 0.26302003860473633 + 10.0 * 9.027012825012207
Epoch 710, val loss: 0.3943188786506653
Epoch 720, training loss: 90.51960754394531 = 0.26134440302848816 + 10.0 * 9.025826454162598
Epoch 720, val loss: 0.3947063088417053
Epoch 730, training loss: 90.5257568359375 = 0.25967881083488464 + 10.0 * 9.026607513427734
Epoch 730, val loss: 0.39508238434791565
Epoch 740, training loss: 90.51939392089844 = 0.25804197788238525 + 10.0 * 9.026135444641113
Epoch 740, val loss: 0.39568376541137695
Epoch 750, training loss: 90.50860595703125 = 0.2564476728439331 + 10.0 * 9.025216102600098
Epoch 750, val loss: 0.3960513770580292
Epoch 760, training loss: 90.504150390625 = 0.25487029552459717 + 10.0 * 9.024928092956543
Epoch 760, val loss: 0.3966168165206909
Epoch 770, training loss: 90.53813171386719 = 0.25331220030784607 + 10.0 * 9.028482437133789
Epoch 770, val loss: 0.3972790539264679
Epoch 780, training loss: 90.49440002441406 = 0.25177571177482605 + 10.0 * 9.024262428283691
Epoch 780, val loss: 0.39779341220855713
Epoch 790, training loss: 90.48884582519531 = 0.2502639889717102 + 10.0 * 9.023858070373535
Epoch 790, val loss: 0.39842674136161804
Epoch 800, training loss: 90.5051040649414 = 0.24876587092876434 + 10.0 * 9.025633811950684
Epoch 800, val loss: 0.399197518825531
Epoch 810, training loss: 90.5185775756836 = 0.24728697538375854 + 10.0 * 9.027129173278809
Epoch 810, val loss: 0.3998096287250519
Epoch 820, training loss: 90.47193908691406 = 0.24583767354488373 + 10.0 * 9.022610664367676
Epoch 820, val loss: 0.40049922466278076
Epoch 830, training loss: 90.4706802368164 = 0.24440112709999084 + 10.0 * 9.022627830505371
Epoch 830, val loss: 0.40130501985549927
Epoch 840, training loss: 90.46329498291016 = 0.24297019839286804 + 10.0 * 9.022031784057617
Epoch 840, val loss: 0.40209731459617615
Epoch 850, training loss: 90.50639343261719 = 0.24154949188232422 + 10.0 * 9.026484489440918
Epoch 850, val loss: 0.4029098451137543
Epoch 860, training loss: 90.46876525878906 = 0.24015314877033234 + 10.0 * 9.02286148071289
Epoch 860, val loss: 0.4037882685661316
Epoch 870, training loss: 90.4696044921875 = 0.23877809941768646 + 10.0 * 9.023082733154297
Epoch 870, val loss: 0.4046684503555298
Epoch 880, training loss: 90.45094299316406 = 0.23742499947547913 + 10.0 * 9.02135181427002
Epoch 880, val loss: 0.40561535954475403
Epoch 890, training loss: 90.44463348388672 = 0.2360725700855255 + 10.0 * 9.020855903625488
Epoch 890, val loss: 0.4065445065498352
Epoch 900, training loss: 90.47307586669922 = 0.23472779989242554 + 10.0 * 9.023835182189941
Epoch 900, val loss: 0.40759944915771484
Epoch 910, training loss: 90.45747375488281 = 0.23340746760368347 + 10.0 * 9.022406578063965
Epoch 910, val loss: 0.4084993004798889
Epoch 920, training loss: 90.43114471435547 = 0.2320988029241562 + 10.0 * 9.019904136657715
Epoch 920, val loss: 0.40951257944107056
Epoch 930, training loss: 90.42961883544922 = 0.23079921305179596 + 10.0 * 9.019882202148438
Epoch 930, val loss: 0.41063061356544495
Epoch 940, training loss: 90.4255142211914 = 0.22950510680675507 + 10.0 * 9.019600868225098
Epoch 940, val loss: 0.41172105073928833
Epoch 950, training loss: 90.47649383544922 = 0.22822362184524536 + 10.0 * 9.024827003479004
Epoch 950, val loss: 0.4129111170768738
Epoch 960, training loss: 90.42491149902344 = 0.2269592583179474 + 10.0 * 9.019795417785645
Epoch 960, val loss: 0.41395995020866394
Epoch 970, training loss: 90.41620635986328 = 0.22571218013763428 + 10.0 * 9.019048690795898
Epoch 970, val loss: 0.41510888934135437
Epoch 980, training loss: 90.41426086425781 = 0.22447270154953003 + 10.0 * 9.0189790725708
Epoch 980, val loss: 0.4163194000720978
Epoch 990, training loss: 90.44320678710938 = 0.22324424982070923 + 10.0 * 9.02199649810791
Epoch 990, val loss: 0.4176393747329712
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8483
Overall ASR: 0.6805
Flip ASR: 0.6017/1554 nodes
The final ASR:0.68509, 0.00373, Accuracy:0.84948, 0.00086
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97678])
remove edge: torch.Size([2, 79930])
updated graph: torch.Size([2, 88960])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.7003
Flip ASR: 0.6293/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7414
Flip ASR: 0.6795/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72228, 0.01689, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 10 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.67704010009766 = 1.0872465372085571 + 10.0 * 10.358979225158691
Epoch 0, val loss: 1.0862284898757935
Epoch 10, training loss: 104.6212387084961 = 1.0780818462371826 + 10.0 * 10.354315757751465
Epoch 10, val loss: 1.0769929885864258
Epoch 20, training loss: 103.87508392333984 = 1.0667463541030884 + 10.0 * 10.28083324432373
Epoch 20, val loss: 1.0659568309783936
Epoch 30, training loss: 98.17552947998047 = 1.057541847229004 + 10.0 * 9.711798667907715
Epoch 30, val loss: 1.0568749904632568
Epoch 40, training loss: 96.0976333618164 = 1.0444848537445068 + 10.0 * 9.505314826965332
Epoch 40, val loss: 1.0432791709899902
Epoch 50, training loss: 95.17284393310547 = 1.0279732942581177 + 10.0 * 9.4144868850708
Epoch 50, val loss: 1.026897668838501
Epoch 60, training loss: 94.56629180908203 = 1.0126757621765137 + 10.0 * 9.355361938476562
Epoch 60, val loss: 1.0121102333068848
Epoch 70, training loss: 94.22000122070312 = 0.9972566962242126 + 10.0 * 9.322275161743164
Epoch 70, val loss: 0.9972825646400452
Epoch 80, training loss: 93.80715942382812 = 0.985538899898529 + 10.0 * 9.282161712646484
Epoch 80, val loss: 0.9864027500152588
Epoch 90, training loss: 93.26736450195312 = 0.9782081246376038 + 10.0 * 9.228915214538574
Epoch 90, val loss: 0.9794170260429382
Epoch 100, training loss: 92.89192962646484 = 0.969900369644165 + 10.0 * 9.1922025680542
Epoch 100, val loss: 0.970920979976654
Epoch 110, training loss: 92.55176544189453 = 0.957526445388794 + 10.0 * 9.159423828125
Epoch 110, val loss: 0.9587411284446716
Epoch 120, training loss: 92.33574676513672 = 0.9433912038803101 + 10.0 * 9.139235496520996
Epoch 120, val loss: 0.9450697898864746
Epoch 130, training loss: 92.17599487304688 = 0.9260444641113281 + 10.0 * 9.124995231628418
Epoch 130, val loss: 0.9279937744140625
Epoch 140, training loss: 92.02425384521484 = 0.9053707718849182 + 10.0 * 9.11188793182373
Epoch 140, val loss: 0.908185601234436
Epoch 150, training loss: 91.9012222290039 = 0.8828128576278687 + 10.0 * 9.10184097290039
Epoch 150, val loss: 0.8865936994552612
Epoch 160, training loss: 91.80964660644531 = 0.8575887084007263 + 10.0 * 9.095205307006836
Epoch 160, val loss: 0.8623287081718445
Epoch 170, training loss: 91.72966003417969 = 0.8292244076728821 + 10.0 * 9.090044021606445
Epoch 170, val loss: 0.8351449370384216
Epoch 180, training loss: 91.63788604736328 = 0.7979641556739807 + 10.0 * 9.083992004394531
Epoch 180, val loss: 0.8051739931106567
Epoch 190, training loss: 91.56551361083984 = 0.7640160918235779 + 10.0 * 9.08014965057373
Epoch 190, val loss: 0.7727106809616089
Epoch 200, training loss: 91.5162582397461 = 0.727561891078949 + 10.0 * 9.078869819641113
Epoch 200, val loss: 0.7379387021064758
Epoch 210, training loss: 91.43263244628906 = 0.6894809007644653 + 10.0 * 9.074315071105957
Epoch 210, val loss: 0.7021837830543518
Epoch 220, training loss: 91.37825012207031 = 0.6513806581497192 + 10.0 * 9.072687149047852
Epoch 220, val loss: 0.6666149497032166
Epoch 230, training loss: 91.30072784423828 = 0.6145300269126892 + 10.0 * 9.068619728088379
Epoch 230, val loss: 0.6323062777519226
Epoch 240, training loss: 91.24181365966797 = 0.5795112252235413 + 10.0 * 9.066229820251465
Epoch 240, val loss: 0.5998831391334534
Epoch 250, training loss: 91.20271301269531 = 0.5467666387557983 + 10.0 * 9.065594673156738
Epoch 250, val loss: 0.5702077746391296
Epoch 260, training loss: 91.14910125732422 = 0.5175377726554871 + 10.0 * 9.063156127929688
Epoch 260, val loss: 0.5437376499176025
Epoch 270, training loss: 91.08622741699219 = 0.4916762709617615 + 10.0 * 9.059454917907715
Epoch 270, val loss: 0.5206697583198547
Epoch 280, training loss: 91.04597473144531 = 0.4687992334365845 + 10.0 * 9.057718276977539
Epoch 280, val loss: 0.5006537437438965
Epoch 290, training loss: 91.11654663085938 = 0.4485514163970947 + 10.0 * 9.06679916381836
Epoch 290, val loss: 0.4834592342376709
Epoch 300, training loss: 90.97959899902344 = 0.43135586380958557 + 10.0 * 9.054823875427246
Epoch 300, val loss: 0.4690033793449402
Epoch 310, training loss: 90.94461059570312 = 0.4167282283306122 + 10.0 * 9.052788734436035
Epoch 310, val loss: 0.45714521408081055
Epoch 320, training loss: 90.91517639160156 = 0.40409302711486816 + 10.0 * 9.051108360290527
Epoch 320, val loss: 0.4472399652004242
Epoch 330, training loss: 90.89547729492188 = 0.392928808927536 + 10.0 * 9.050254821777344
Epoch 330, val loss: 0.4387739598751068
Epoch 340, training loss: 90.88212585449219 = 0.38300493359565735 + 10.0 * 9.049912452697754
Epoch 340, val loss: 0.4315674304962158
Epoch 350, training loss: 90.85242462158203 = 0.3743078410625458 + 10.0 * 9.047811508178711
Epoch 350, val loss: 0.42547932267189026
Epoch 360, training loss: 90.83687591552734 = 0.36661529541015625 + 10.0 * 9.047025680541992
Epoch 360, val loss: 0.4202865958213806
Epoch 370, training loss: 90.8174819946289 = 0.35978323221206665 + 10.0 * 9.045769691467285
Epoch 370, val loss: 0.41592031717300415
Epoch 380, training loss: 90.79833984375 = 0.3536127209663391 + 10.0 * 9.044472694396973
Epoch 380, val loss: 0.4121517241001129
Epoch 390, training loss: 90.80427551269531 = 0.34796151518821716 + 10.0 * 9.045631408691406
Epoch 390, val loss: 0.40887168049812317
Epoch 400, training loss: 90.77040100097656 = 0.34277310967445374 + 10.0 * 9.042762756347656
Epoch 400, val loss: 0.4059773087501526
Epoch 410, training loss: 90.75499725341797 = 0.3379932940006256 + 10.0 * 9.04170036315918
Epoch 410, val loss: 0.40343695878982544
Epoch 420, training loss: 90.79586791992188 = 0.3335380554199219 + 10.0 * 9.046233177185059
Epoch 420, val loss: 0.4011698365211487
Epoch 430, training loss: 90.74346923828125 = 0.32941728830337524 + 10.0 * 9.04140567779541
Epoch 430, val loss: 0.3992197513580322
Epoch 440, training loss: 90.72017669677734 = 0.3255777359008789 + 10.0 * 9.039460182189941
Epoch 440, val loss: 0.3974402844905853
Epoch 450, training loss: 90.70661926269531 = 0.3219396471977234 + 10.0 * 9.038468360900879
Epoch 450, val loss: 0.39590075612068176
Epoch 460, training loss: 90.76521301269531 = 0.31846827268600464 + 10.0 * 9.04467487335205
Epoch 460, val loss: 0.3944929242134094
Epoch 470, training loss: 90.70528411865234 = 0.3151979148387909 + 10.0 * 9.039008140563965
Epoch 470, val loss: 0.39320626854896545
Epoch 480, training loss: 90.67902374267578 = 0.31211698055267334 + 10.0 * 9.036690711975098
Epoch 480, val loss: 0.39210090041160583
Epoch 490, training loss: 90.67221069335938 = 0.3091609477996826 + 10.0 * 9.03630542755127
Epoch 490, val loss: 0.39109691977500916
Epoch 500, training loss: 90.65887451171875 = 0.30630552768707275 + 10.0 * 9.035257339477539
Epoch 500, val loss: 0.3902689218521118
Epoch 510, training loss: 90.65717315673828 = 0.30357953906059265 + 10.0 * 9.035359382629395
Epoch 510, val loss: 0.38936659693717957
Epoch 520, training loss: 90.64007568359375 = 0.300945907831192 + 10.0 * 9.033912658691406
Epoch 520, val loss: 0.38872677087783813
Epoch 530, training loss: 90.68443298339844 = 0.2983890473842621 + 10.0 * 9.038604736328125
Epoch 530, val loss: 0.38816094398498535
Epoch 540, training loss: 90.6263198852539 = 0.29593321681022644 + 10.0 * 9.033038139343262
Epoch 540, val loss: 0.3875126540660858
Epoch 550, training loss: 90.61727142333984 = 0.2935652434825897 + 10.0 * 9.032370567321777
Epoch 550, val loss: 0.3870004117488861
Epoch 560, training loss: 90.61276245117188 = 0.29125410318374634 + 10.0 * 9.032151222229004
Epoch 560, val loss: 0.38659942150115967
Epoch 570, training loss: 90.62564086914062 = 0.2889917492866516 + 10.0 * 9.03366470336914
Epoch 570, val loss: 0.3861995339393616
Epoch 580, training loss: 90.61341857910156 = 0.2868092358112335 + 10.0 * 9.032660484313965
Epoch 580, val loss: 0.38592037558555603
Epoch 590, training loss: 90.59063720703125 = 0.28470757603645325 + 10.0 * 9.030592918395996
Epoch 590, val loss: 0.38561537861824036
Epoch 600, training loss: 90.58032989501953 = 0.282638818025589 + 10.0 * 9.029768943786621
Epoch 600, val loss: 0.38536080718040466
Epoch 610, training loss: 90.58636474609375 = 0.28059667348861694 + 10.0 * 9.030576705932617
Epoch 610, val loss: 0.3851346969604492
Epoch 620, training loss: 90.5790023803711 = 0.2785903811454773 + 10.0 * 9.030041694641113
Epoch 620, val loss: 0.3849993050098419
Epoch 630, training loss: 90.57305145263672 = 0.2766421139240265 + 10.0 * 9.029641151428223
Epoch 630, val loss: 0.3848978579044342
Epoch 640, training loss: 90.58049774169922 = 0.2747390568256378 + 10.0 * 9.0305757522583
Epoch 640, val loss: 0.3848871886730194
Epoch 650, training loss: 90.5561752319336 = 0.2728833556175232 + 10.0 * 9.028329849243164
Epoch 650, val loss: 0.38483256101608276
Epoch 660, training loss: 90.54335021972656 = 0.2710617780685425 + 10.0 * 9.027228355407715
Epoch 660, val loss: 0.38479870557785034
Epoch 670, training loss: 90.54105377197266 = 0.26925891637802124 + 10.0 * 9.027179718017578
Epoch 670, val loss: 0.38482606410980225
Epoch 680, training loss: 90.55355072021484 = 0.26747795939445496 + 10.0 * 9.028607368469238
Epoch 680, val loss: 0.38497090339660645
Epoch 690, training loss: 90.53895568847656 = 0.2657451033592224 + 10.0 * 9.027320861816406
Epoch 690, val loss: 0.3850202262401581
Epoch 700, training loss: 90.5177993774414 = 0.26405444741249084 + 10.0 * 9.025374412536621
Epoch 700, val loss: 0.385200172662735
Epoch 710, training loss: 90.51453399658203 = 0.26238787174224854 + 10.0 * 9.025214195251465
Epoch 710, val loss: 0.3854120671749115
Epoch 720, training loss: 90.52018737792969 = 0.260730504989624 + 10.0 * 9.025945663452148
Epoch 720, val loss: 0.3856205344200134
Epoch 730, training loss: 90.51142883300781 = 0.2590969502925873 + 10.0 * 9.025233268737793
Epoch 730, val loss: 0.3858914375305176
Epoch 740, training loss: 90.50083923339844 = 0.2575051486492157 + 10.0 * 9.024333000183105
Epoch 740, val loss: 0.38611745834350586
Epoch 750, training loss: 90.49446868896484 = 0.2559312880039215 + 10.0 * 9.02385425567627
Epoch 750, val loss: 0.38646200299263
Epoch 760, training loss: 90.5448989868164 = 0.254366010427475 + 10.0 * 9.029053688049316
Epoch 760, val loss: 0.38674649596214294
Epoch 770, training loss: 90.49185943603516 = 0.2528265416622162 + 10.0 * 9.023903846740723
Epoch 770, val loss: 0.38721150159835815
Epoch 780, training loss: 90.4808120727539 = 0.2513161301612854 + 10.0 * 9.02294921875
Epoch 780, val loss: 0.3876139521598816
Epoch 790, training loss: 90.47515106201172 = 0.24981415271759033 + 10.0 * 9.022533416748047
Epoch 790, val loss: 0.38802966475486755
Epoch 800, training loss: 90.48320007324219 = 0.24831868708133698 + 10.0 * 9.02348804473877
Epoch 800, val loss: 0.3885374963283539
Epoch 810, training loss: 90.47046661376953 = 0.24685098230838776 + 10.0 * 9.022361755371094
Epoch 810, val loss: 0.3889751732349396
Epoch 820, training loss: 90.45841979980469 = 0.24540457129478455 + 10.0 * 9.02130126953125
Epoch 820, val loss: 0.3895460069179535
Epoch 830, training loss: 90.45451354980469 = 0.2439596951007843 + 10.0 * 9.021055221557617
Epoch 830, val loss: 0.3900912404060364
Epoch 840, training loss: 90.50305938720703 = 0.24252061545848846 + 10.0 * 9.026053428649902
Epoch 840, val loss: 0.39065036177635193
Epoch 850, training loss: 90.46514129638672 = 0.24111248552799225 + 10.0 * 9.0224027633667
Epoch 850, val loss: 0.3913925290107727
Epoch 860, training loss: 90.44860076904297 = 0.2397242784500122 + 10.0 * 9.02088737487793
Epoch 860, val loss: 0.3920063376426697
Epoch 870, training loss: 90.43618774414062 = 0.23834729194641113 + 10.0 * 9.019783973693848
Epoch 870, val loss: 0.3926471173763275
Epoch 880, training loss: 90.44308471679688 = 0.2369735836982727 + 10.0 * 9.020610809326172
Epoch 880, val loss: 0.3933183550834656
Epoch 890, training loss: 90.43411254882812 = 0.2356089949607849 + 10.0 * 9.01984977722168
Epoch 890, val loss: 0.3941486179828644
Epoch 900, training loss: 90.43531799316406 = 0.23427218198776245 + 10.0 * 9.02010440826416
Epoch 900, val loss: 0.39486199617385864
Epoch 910, training loss: 90.42000579833984 = 0.2329481691122055 + 10.0 * 9.018705368041992
Epoch 910, val loss: 0.39567044377326965
Epoch 920, training loss: 90.4364013671875 = 0.2316236048936844 + 10.0 * 9.020478248596191
Epoch 920, val loss: 0.3964715301990509
Epoch 930, training loss: 90.43104553222656 = 0.23031282424926758 + 10.0 * 9.020072937011719
Epoch 930, val loss: 0.39747920632362366
Epoch 940, training loss: 90.4094009399414 = 0.22902685403823853 + 10.0 * 9.018037796020508
Epoch 940, val loss: 0.3982616662979126
Epoch 950, training loss: 90.4072036743164 = 0.22775115072727203 + 10.0 * 9.017945289611816
Epoch 950, val loss: 0.3991524577140808
Epoch 960, training loss: 90.40172576904297 = 0.22647586464881897 + 10.0 * 9.017524719238281
Epoch 960, val loss: 0.400113970041275
Epoch 970, training loss: 90.49285125732422 = 0.22521448135375977 + 10.0 * 9.026763916015625
Epoch 970, val loss: 0.4010937511920929
Epoch 980, training loss: 90.39649963378906 = 0.22396184504032135 + 10.0 * 9.017253875732422
Epoch 980, val loss: 0.4021185338497162
Epoch 990, training loss: 90.39501953125 = 0.22273904085159302 + 10.0 * 9.017228126525879
Epoch 990, val loss: 0.40314891934394836
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8463
Overall ASR: 0.7013
Flip ASR: 0.6261/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.6916275024414 = 1.1022738218307495 + 10.0 * 10.358935356140137
Epoch 0, val loss: 1.100574016571045
Epoch 10, training loss: 104.62053680419922 = 1.090233325958252 + 10.0 * 10.35303020477295
Epoch 10, val loss: 1.0883798599243164
Epoch 20, training loss: 103.69754791259766 = 1.0748814344406128 + 10.0 * 10.262266159057617
Epoch 20, val loss: 1.0734723806381226
Epoch 30, training loss: 97.952880859375 = 1.0614867210388184 + 10.0 * 9.689139366149902
Epoch 30, val loss: 1.0606812238693237
Epoch 40, training loss: 96.28941345214844 = 1.048170566558838 + 10.0 * 9.524124145507812
Epoch 40, val loss: 1.0472760200500488
Epoch 50, training loss: 94.9283676147461 = 1.032731056213379 + 10.0 * 9.38956356048584
Epoch 50, val loss: 1.03189218044281
Epoch 60, training loss: 94.45504760742188 = 1.017712116241455 + 10.0 * 9.343732833862305
Epoch 60, val loss: 1.0169014930725098
Epoch 70, training loss: 94.19164276123047 = 1.00187349319458 + 10.0 * 9.318976402282715
Epoch 70, val loss: 1.001237392425537
Epoch 80, training loss: 93.75264739990234 = 0.9882563352584839 + 10.0 * 9.27643871307373
Epoch 80, val loss: 0.9882579445838928
Epoch 90, training loss: 93.24693298339844 = 0.9762309789657593 + 10.0 * 9.227069854736328
Epoch 90, val loss: 0.9765392541885376
Epoch 100, training loss: 92.94329071044922 = 0.9615413546562195 + 10.0 * 9.198175430297852
Epoch 100, val loss: 0.9621768593788147
Epoch 110, training loss: 92.65473937988281 = 0.94721519947052 + 10.0 * 9.17075252532959
Epoch 110, val loss: 0.9484911561012268
Epoch 120, training loss: 92.43682098388672 = 0.9337449669837952 + 10.0 * 9.150307655334473
Epoch 120, val loss: 0.9354000091552734
Epoch 130, training loss: 92.23094177246094 = 0.9178045392036438 + 10.0 * 9.131314277648926
Epoch 130, val loss: 0.9200107455253601
Epoch 140, training loss: 92.13013458251953 = 0.8980464339256287 + 10.0 * 9.123208999633789
Epoch 140, val loss: 0.9010816812515259
Epoch 150, training loss: 91.98662567138672 = 0.875217616558075 + 10.0 * 9.111140251159668
Epoch 150, val loss: 0.8790713548660278
Epoch 160, training loss: 91.8864974975586 = 0.8511143922805786 + 10.0 * 9.103538513183594
Epoch 160, val loss: 0.8559389710426331
Epoch 170, training loss: 91.80361938476562 = 0.8243815302848816 + 10.0 * 9.09792423248291
Epoch 170, val loss: 0.8302081823348999
Epoch 180, training loss: 91.7258529663086 = 0.7936277985572815 + 10.0 * 9.093222618103027
Epoch 180, val loss: 0.8007235527038574
Epoch 190, training loss: 91.63072967529297 = 0.760287880897522 + 10.0 * 9.087044715881348
Epoch 190, val loss: 0.7688609957695007
Epoch 200, training loss: 91.58248901367188 = 0.7252839207649231 + 10.0 * 9.085721015930176
Epoch 200, val loss: 0.7357522249221802
Epoch 210, training loss: 91.459228515625 = 0.6891564130783081 + 10.0 * 9.077007293701172
Epoch 210, val loss: 0.7014957070350647
Epoch 220, training loss: 91.38652038574219 = 0.6524845361709595 + 10.0 * 9.073404312133789
Epoch 220, val loss: 0.6670779585838318
Epoch 230, training loss: 91.32032775878906 = 0.6159234046936035 + 10.0 * 9.070440292358398
Epoch 230, val loss: 0.6330661773681641
Epoch 240, training loss: 91.24996948242188 = 0.5801974534988403 + 10.0 * 9.066976547241211
Epoch 240, val loss: 0.6004059314727783
Epoch 250, training loss: 91.20451354980469 = 0.5466038584709167 + 10.0 * 9.065791130065918
Epoch 250, val loss: 0.5697123408317566
Epoch 260, training loss: 91.14143371582031 = 0.5158626437187195 + 10.0 * 9.062557220458984
Epoch 260, val loss: 0.5423412919044495
Epoch 270, training loss: 91.08548736572266 = 0.4887479543685913 + 10.0 * 9.059674263000488
Epoch 270, val loss: 0.5184817314147949
Epoch 280, training loss: 91.0366439819336 = 0.4650576710700989 + 10.0 * 9.057158470153809
Epoch 280, val loss: 0.498060017824173
Epoch 290, training loss: 91.0391845703125 = 0.44448167085647583 + 10.0 * 9.059470176696777
Epoch 290, val loss: 0.4807778298854828
Epoch 300, training loss: 90.9696273803711 = 0.4270283877849579 + 10.0 * 9.05426025390625
Epoch 300, val loss: 0.4664042890071869
Epoch 310, training loss: 90.9213638305664 = 0.4121897220611572 + 10.0 * 9.05091667175293
Epoch 310, val loss: 0.4544444680213928
Epoch 320, training loss: 90.89513397216797 = 0.39937761425971985 + 10.0 * 9.049575805664062
Epoch 320, val loss: 0.44451597332954407
Epoch 330, training loss: 90.90824127197266 = 0.3882948160171509 + 10.0 * 9.051994323730469
Epoch 330, val loss: 0.43623805046081543
Epoch 340, training loss: 90.841064453125 = 0.3788071572780609 + 10.0 * 9.046225547790527
Epoch 340, val loss: 0.4293345808982849
Epoch 350, training loss: 90.82109069824219 = 0.37060773372650146 + 10.0 * 9.045048713684082
Epoch 350, val loss: 0.423687607049942
Epoch 360, training loss: 90.79844665527344 = 0.36333104968070984 + 10.0 * 9.043511390686035
Epoch 360, val loss: 0.41889068484306335
Epoch 370, training loss: 90.7846450805664 = 0.3567968010902405 + 10.0 * 9.042784690856934
Epoch 370, val loss: 0.4147024154663086
Epoch 380, training loss: 90.75653839111328 = 0.35096248984336853 + 10.0 * 9.040557861328125
Epoch 380, val loss: 0.41115471720695496
Epoch 390, training loss: 90.7426986694336 = 0.3456794321537018 + 10.0 * 9.039701461791992
Epoch 390, val loss: 0.40812593698501587
Epoch 400, training loss: 90.73009490966797 = 0.3408295214176178 + 10.0 * 9.03892707824707
Epoch 400, val loss: 0.4055778980255127
Epoch 410, training loss: 90.72826385498047 = 0.3363686800003052 + 10.0 * 9.039189338684082
Epoch 410, val loss: 0.4031926691532135
Epoch 420, training loss: 90.70326232910156 = 0.33224090933799744 + 10.0 * 9.037102699279785
Epoch 420, val loss: 0.40115758776664734
Epoch 430, training loss: 90.6959457397461 = 0.32839205861091614 + 10.0 * 9.036755561828613
Epoch 430, val loss: 0.3994280695915222
Epoch 440, training loss: 90.67501068115234 = 0.3247912526130676 + 10.0 * 9.035021781921387
Epoch 440, val loss: 0.3978215754032135
Epoch 450, training loss: 90.69535064697266 = 0.32139894366264343 + 10.0 * 9.037395477294922
Epoch 450, val loss: 0.3964259624481201
Epoch 460, training loss: 90.66705322265625 = 0.318177193403244 + 10.0 * 9.034887313842773
Epoch 460, val loss: 0.3951479196548462
Epoch 470, training loss: 90.64801025390625 = 0.31513598561286926 + 10.0 * 9.033287048339844
Epoch 470, val loss: 0.394002228975296
Epoch 480, training loss: 90.64381408691406 = 0.3122231364250183 + 10.0 * 9.033159255981445
Epoch 480, val loss: 0.3930516242980957
Epoch 490, training loss: 90.62820434570312 = 0.30943965911865234 + 10.0 * 9.031876564025879
Epoch 490, val loss: 0.3920873999595642
Epoch 500, training loss: 90.65593719482422 = 0.30677276849746704 + 10.0 * 9.034916877746582
Epoch 500, val loss: 0.3912404179573059
Epoch 510, training loss: 90.6199951171875 = 0.30421310663223267 + 10.0 * 9.031578063964844
Epoch 510, val loss: 0.3905992805957794
Epoch 520, training loss: 90.62467193603516 = 0.3017663359642029 + 10.0 * 9.0322904586792
Epoch 520, val loss: 0.3899354934692383
Epoch 530, training loss: 90.60001373291016 = 0.29940977692604065 + 10.0 * 9.030060768127441
Epoch 530, val loss: 0.38941365480422974
Epoch 540, training loss: 90.6038818359375 = 0.2971217930316925 + 10.0 * 9.030675888061523
Epoch 540, val loss: 0.3888285160064697
Epoch 550, training loss: 90.58439636230469 = 0.2948957681655884 + 10.0 * 9.028949737548828
Epoch 550, val loss: 0.3884800970554352
Epoch 560, training loss: 90.57594299316406 = 0.29273733496665955 + 10.0 * 9.0283203125
Epoch 560, val loss: 0.38801881670951843
Epoch 570, training loss: 90.60237121582031 = 0.2906184196472168 + 10.0 * 9.03117561340332
Epoch 570, val loss: 0.38764524459838867
Epoch 580, training loss: 90.57913208007812 = 0.2885778844356537 + 10.0 * 9.02905559539795
Epoch 580, val loss: 0.3875376582145691
Epoch 590, training loss: 90.56369018554688 = 0.2866050899028778 + 10.0 * 9.027708053588867
Epoch 590, val loss: 0.3871934413909912
Epoch 600, training loss: 90.54866027832031 = 0.28467410802841187 + 10.0 * 9.026398658752441
Epoch 600, val loss: 0.38704925775527954
Epoch 610, training loss: 90.56135559082031 = 0.2827697992324829 + 10.0 * 9.02785873413086
Epoch 610, val loss: 0.38686245679855347
Epoch 620, training loss: 90.55903625488281 = 0.2809177339076996 + 10.0 * 9.027812004089355
Epoch 620, val loss: 0.38681769371032715
Epoch 630, training loss: 90.52861022949219 = 0.279114305973053 + 10.0 * 9.02495002746582
Epoch 630, val loss: 0.38670840859413147
Epoch 640, training loss: 90.53675079345703 = 0.2773452699184418 + 10.0 * 9.025940895080566
Epoch 640, val loss: 0.38668039441108704
Epoch 650, training loss: 90.53136444091797 = 0.2756103277206421 + 10.0 * 9.025575637817383
Epoch 650, val loss: 0.3867509365081787
Epoch 660, training loss: 90.51689147949219 = 0.2739245593547821 + 10.0 * 9.024296760559082
Epoch 660, val loss: 0.386736124753952
Epoch 670, training loss: 90.50720977783203 = 0.27226224541664124 + 10.0 * 9.023494720458984
Epoch 670, val loss: 0.38683992624282837
Epoch 680, training loss: 90.50431060791016 = 0.27061328291893005 + 10.0 * 9.023369789123535
Epoch 680, val loss: 0.3869554102420807
Epoch 690, training loss: 90.50983428955078 = 0.26898884773254395 + 10.0 * 9.024084091186523
Epoch 690, val loss: 0.38713598251342773
Epoch 700, training loss: 90.50438690185547 = 0.26741310954093933 + 10.0 * 9.023697853088379
Epoch 700, val loss: 0.38729527592658997
Epoch 710, training loss: 90.49015808105469 = 0.2658638060092926 + 10.0 * 9.022429466247559
Epoch 710, val loss: 0.3875024616718292
Epoch 720, training loss: 90.48143005371094 = 0.26432448625564575 + 10.0 * 9.021710395812988
Epoch 720, val loss: 0.3877471685409546
Epoch 730, training loss: 90.51588439941406 = 0.26279890537261963 + 10.0 * 9.025308609008789
Epoch 730, val loss: 0.3880186975002289
Epoch 740, training loss: 90.49669647216797 = 0.26131147146224976 + 10.0 * 9.023538589477539
Epoch 740, val loss: 0.3882972002029419
Epoch 750, training loss: 90.47774505615234 = 0.2598545253276825 + 10.0 * 9.021788597106934
Epoch 750, val loss: 0.3886474370956421
Epoch 760, training loss: 90.46481323242188 = 0.2584112286567688 + 10.0 * 9.02064037322998
Epoch 760, val loss: 0.38900497555732727
Epoch 770, training loss: 90.45728302001953 = 0.2569733262062073 + 10.0 * 9.020030975341797
Epoch 770, val loss: 0.38939183950424194
Epoch 780, training loss: 90.4626235961914 = 0.2555428743362427 + 10.0 * 9.020708084106445
Epoch 780, val loss: 0.3898957669734955
Epoch 790, training loss: 90.4636001586914 = 0.2541356384754181 + 10.0 * 9.020946502685547
Epoch 790, val loss: 0.39018478989601135
Epoch 800, training loss: 90.44406127929688 = 0.2527613639831543 + 10.0 * 9.019129753112793
Epoch 800, val loss: 0.39069342613220215
Epoch 810, training loss: 90.4400863647461 = 0.25140050053596497 + 10.0 * 9.018868446350098
Epoch 810, val loss: 0.3912540376186371
Epoch 820, training loss: 90.4402847290039 = 0.2500426769256592 + 10.0 * 9.019023895263672
Epoch 820, val loss: 0.3917812407016754
Epoch 830, training loss: 90.46382904052734 = 0.24869459867477417 + 10.0 * 9.021512985229492
Epoch 830, val loss: 0.3924035429954529
Epoch 840, training loss: 90.43238067626953 = 0.24736395478248596 + 10.0 * 9.018502235412598
Epoch 840, val loss: 0.39291924238204956
Epoch 850, training loss: 90.43553161621094 = 0.24604791402816772 + 10.0 * 9.018948554992676
Epoch 850, val loss: 0.39355894923210144
Epoch 860, training loss: 90.4250259399414 = 0.24474121630191803 + 10.0 * 9.018028259277344
Epoch 860, val loss: 0.39421162009239197
Epoch 870, training loss: 90.42877960205078 = 0.24344348907470703 + 10.0 * 9.018533706665039
Epoch 870, val loss: 0.3948817849159241
Epoch 880, training loss: 90.42526245117188 = 0.24216283857822418 + 10.0 * 9.018309593200684
Epoch 880, val loss: 0.3955842852592468
Epoch 890, training loss: 90.40995025634766 = 0.24089564383029938 + 10.0 * 9.016904830932617
Epoch 890, val loss: 0.3962620794773102
Epoch 900, training loss: 90.4075927734375 = 0.2396346926689148 + 10.0 * 9.016796112060547
Epoch 900, val loss: 0.3970012068748474
Epoch 910, training loss: 90.4653549194336 = 0.2383815348148346 + 10.0 * 9.022697448730469
Epoch 910, val loss: 0.3978008031845093
Epoch 920, training loss: 90.39900970458984 = 0.23715020716190338 + 10.0 * 9.016185760498047
Epoch 920, val loss: 0.3985418379306793
Epoch 930, training loss: 90.39896392822266 = 0.23592980206012726 + 10.0 * 9.016303062438965
Epoch 930, val loss: 0.399355411529541
Epoch 940, training loss: 90.3985595703125 = 0.23471039533615112 + 10.0 * 9.016385078430176
Epoch 940, val loss: 0.40021032094955444
Epoch 950, training loss: 90.4015884399414 = 0.23349958658218384 + 10.0 * 9.016809463500977
Epoch 950, val loss: 0.4010583758354187
Epoch 960, training loss: 90.38818359375 = 0.23230424523353577 + 10.0 * 9.01558780670166
Epoch 960, val loss: 0.40189221501350403
Epoch 970, training loss: 90.38682556152344 = 0.23111487925052643 + 10.0 * 9.015570640563965
Epoch 970, val loss: 0.40278080105781555
Epoch 980, training loss: 90.40238952636719 = 0.2299319952726364 + 10.0 * 9.017245292663574
Epoch 980, val loss: 0.40366747975349426
Epoch 990, training loss: 90.38044738769531 = 0.22875694930553436 + 10.0 * 9.015169143676758
Epoch 990, val loss: 0.4046993553638458
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8508
Overall ASR: 0.6962
Flip ASR: 0.6197/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68164825439453 = 1.0918234586715698 + 10.0 * 10.358983039855957
Epoch 0, val loss: 1.0906836986541748
Epoch 10, training loss: 104.6247787475586 = 1.081573247909546 + 10.0 * 10.354320526123047
Epoch 10, val loss: 1.0801992416381836
Epoch 20, training loss: 103.83061981201172 = 1.0686591863632202 + 10.0 * 10.276196479797363
Epoch 20, val loss: 1.0674980878829956
Epoch 30, training loss: 97.87210083007812 = 1.058281421661377 + 10.0 * 9.681382179260254
Epoch 30, val loss: 1.0572874546051025
Epoch 40, training loss: 95.97555541992188 = 1.0473459959030151 + 10.0 * 9.492820739746094
Epoch 40, val loss: 1.045888900756836
Epoch 50, training loss: 95.24964141845703 = 1.0333952903747559 + 10.0 * 9.421625137329102
Epoch 50, val loss: 1.0317323207855225
Epoch 60, training loss: 94.58445739746094 = 1.0188676118850708 + 10.0 * 9.356558799743652
Epoch 60, val loss: 1.0172233581542969
Epoch 70, training loss: 94.03083038330078 = 1.0037580728530884 + 10.0 * 9.302706718444824
Epoch 70, val loss: 1.00252366065979
Epoch 80, training loss: 93.42430877685547 = 0.9927495121955872 + 10.0 * 9.243155479431152
Epoch 80, val loss: 0.9921319484710693
Epoch 90, training loss: 92.9559097290039 = 0.9840056300163269 + 10.0 * 9.197190284729004
Epoch 90, val loss: 0.9834949374198914
Epoch 100, training loss: 92.66400909423828 = 0.9710316061973572 + 10.0 * 9.16929817199707
Epoch 100, val loss: 0.9707645773887634
Epoch 110, training loss: 92.44945526123047 = 0.9557591676712036 + 10.0 * 9.149370193481445
Epoch 110, val loss: 0.9562192559242249
Epoch 120, training loss: 92.25725555419922 = 0.9412220120429993 + 10.0 * 9.131603240966797
Epoch 120, val loss: 0.942386269569397
Epoch 130, training loss: 92.1242904663086 = 0.9258599281311035 + 10.0 * 9.119843482971191
Epoch 130, val loss: 0.9272229075431824
Epoch 140, training loss: 91.97114562988281 = 0.9085543155670166 + 10.0 * 9.1062593460083
Epoch 140, val loss: 0.9105528593063354
Epoch 150, training loss: 91.81877899169922 = 0.8901398181915283 + 10.0 * 9.092864036560059
Epoch 150, val loss: 0.8929008841514587
Epoch 160, training loss: 91.7784423828125 = 0.8695246577262878 + 10.0 * 9.09089183807373
Epoch 160, val loss: 0.8729902505874634
Epoch 170, training loss: 91.62627410888672 = 0.8448666930198669 + 10.0 * 9.078141212463379
Epoch 170, val loss: 0.8490702509880066
Epoch 180, training loss: 91.54288482666016 = 0.8165830373764038 + 10.0 * 9.072629928588867
Epoch 180, val loss: 0.8219804763793945
Epoch 190, training loss: 91.4696273803711 = 0.7853922843933105 + 10.0 * 9.0684232711792
Epoch 190, val loss: 0.7920337319374084
Epoch 200, training loss: 91.401123046875 = 0.7513328790664673 + 10.0 * 9.06497859954834
Epoch 200, val loss: 0.7595442533493042
Epoch 210, training loss: 91.38521575927734 = 0.7148893475532532 + 10.0 * 9.067032814025879
Epoch 210, val loss: 0.7249417901039124
Epoch 220, training loss: 91.2874526977539 = 0.6774088144302368 + 10.0 * 9.061004638671875
Epoch 220, val loss: 0.6897140741348267
Epoch 230, training loss: 91.21686553955078 = 0.6400647759437561 + 10.0 * 9.057680130004883
Epoch 230, val loss: 0.6548042893409729
Epoch 240, training loss: 91.15621948242188 = 0.6034041047096252 + 10.0 * 9.055281639099121
Epoch 240, val loss: 0.6208136677742004
Epoch 250, training loss: 91.11693572998047 = 0.5682870745658875 + 10.0 * 9.054864883422852
Epoch 250, val loss: 0.5886816382408142
Epoch 260, training loss: 91.052001953125 = 0.5358359813690186 + 10.0 * 9.051616668701172
Epoch 260, val loss: 0.5593507885932922
Epoch 270, training loss: 91.01689910888672 = 0.5065193772315979 + 10.0 * 9.051037788391113
Epoch 270, val loss: 0.5332581400871277
Epoch 280, training loss: 90.97579956054688 = 0.4806984066963196 + 10.0 * 9.04951000213623
Epoch 280, val loss: 0.5105939507484436
Epoch 290, training loss: 90.93211364746094 = 0.45825573801994324 + 10.0 * 9.047386169433594
Epoch 290, val loss: 0.4913420081138611
Epoch 300, training loss: 90.89523315429688 = 0.43891042470932007 + 10.0 * 9.045632362365723
Epoch 300, val loss: 0.4751126766204834
Epoch 310, training loss: 90.88915252685547 = 0.42232581973075867 + 10.0 * 9.046682357788086
Epoch 310, val loss: 0.4616051912307739
Epoch 320, training loss: 90.83521270751953 = 0.40831470489501953 + 10.0 * 9.042689323425293
Epoch 320, val loss: 0.45044898986816406
Epoch 330, training loss: 90.80870819091797 = 0.3963419795036316 + 10.0 * 9.041236877441406
Epoch 330, val loss: 0.44121843576431274
Epoch 340, training loss: 90.78221130371094 = 0.3859459161758423 + 10.0 * 9.039626121520996
Epoch 340, val loss: 0.433456152677536
Epoch 350, training loss: 90.82563781738281 = 0.3767971992492676 + 10.0 * 9.044883728027344
Epoch 350, val loss: 0.42676642537117004
Epoch 360, training loss: 90.7872314453125 = 0.36882323026657104 + 10.0 * 9.041841506958008
Epoch 360, val loss: 0.4213442802429199
Epoch 370, training loss: 90.73017883300781 = 0.36189591884613037 + 10.0 * 9.036828994750977
Epoch 370, val loss: 0.41681110858917236
Epoch 380, training loss: 90.7152328491211 = 0.355747252702713 + 10.0 * 9.035947799682617
Epoch 380, val loss: 0.41281554102897644
Epoch 390, training loss: 90.74541473388672 = 0.35014986991882324 + 10.0 * 9.039525985717773
Epoch 390, val loss: 0.4093717634677887
Epoch 400, training loss: 90.69100952148438 = 0.3450094759464264 + 10.0 * 9.034600257873535
Epoch 400, val loss: 0.4063898026943207
Epoch 410, training loss: 90.67695617675781 = 0.34031516313552856 + 10.0 * 9.033663749694824
Epoch 410, val loss: 0.4036968946456909
Epoch 420, training loss: 90.69195556640625 = 0.3359670042991638 + 10.0 * 9.035598754882812
Epoch 420, val loss: 0.4013267159461975
Epoch 430, training loss: 90.67955780029297 = 0.3319193124771118 + 10.0 * 9.034764289855957
Epoch 430, val loss: 0.39932531118392944
Epoch 440, training loss: 90.64537811279297 = 0.3281800150871277 + 10.0 * 9.031720161437988
Epoch 440, val loss: 0.39741241931915283
Epoch 450, training loss: 90.63229370117188 = 0.32464897632598877 + 10.0 * 9.03076457977295
Epoch 450, val loss: 0.39574071764945984
Epoch 460, training loss: 90.62287139892578 = 0.3212778568267822 + 10.0 * 9.030158996582031
Epoch 460, val loss: 0.39421626925468445
Epoch 470, training loss: 90.62185668945312 = 0.31806620955467224 + 10.0 * 9.030378341674805
Epoch 470, val loss: 0.392919659614563
Epoch 480, training loss: 90.61759185791016 = 0.31505483388900757 + 10.0 * 9.030253410339355
Epoch 480, val loss: 0.39167138934135437
Epoch 490, training loss: 90.60045623779297 = 0.31221020221710205 + 10.0 * 9.028824806213379
Epoch 490, val loss: 0.39050057530403137
Epoch 500, training loss: 90.59640502929688 = 0.30948013067245483 + 10.0 * 9.028692245483398
Epoch 500, val loss: 0.38947421312332153
Epoch 510, training loss: 90.58257293701172 = 0.3068406581878662 + 10.0 * 9.027573585510254
Epoch 510, val loss: 0.38858768343925476
Epoch 520, training loss: 90.57115936279297 = 0.3042968809604645 + 10.0 * 9.02668571472168
Epoch 520, val loss: 0.3877221345901489
Epoch 530, training loss: 90.57237243652344 = 0.30182895064353943 + 10.0 * 9.027054786682129
Epoch 530, val loss: 0.3869595229625702
Epoch 540, training loss: 90.57512664794922 = 0.29944324493408203 + 10.0 * 9.027567863464355
Epoch 540, val loss: 0.3862254023551941
Epoch 550, training loss: 90.55757141113281 = 0.2971484363079071 + 10.0 * 9.026041984558105
Epoch 550, val loss: 0.3856351375579834
Epoch 560, training loss: 90.54781341552734 = 0.2949279248714447 + 10.0 * 9.025288581848145
Epoch 560, val loss: 0.3850364089012146
Epoch 570, training loss: 90.53656005859375 = 0.29275721311569214 + 10.0 * 9.024380683898926
Epoch 570, val loss: 0.3845255672931671
Epoch 580, training loss: 90.58549499511719 = 0.29062286019325256 + 10.0 * 9.029486656188965
Epoch 580, val loss: 0.3840658366680145
Epoch 590, training loss: 90.53993225097656 = 0.2885490953922272 + 10.0 * 9.025137901306152
Epoch 590, val loss: 0.3836832642555237
Epoch 600, training loss: 90.5265121459961 = 0.28653955459594727 + 10.0 * 9.02399730682373
Epoch 600, val loss: 0.3833121955394745
Epoch 610, training loss: 90.51761627197266 = 0.2845727503299713 + 10.0 * 9.023303985595703
Epoch 610, val loss: 0.3830038011074066
Epoch 620, training loss: 90.5435791015625 = 0.2826431095600128 + 10.0 * 9.026093482971191
Epoch 620, val loss: 0.3827379643917084
Epoch 630, training loss: 90.50444030761719 = 0.2807578444480896 + 10.0 * 9.022368431091309
Epoch 630, val loss: 0.3825419843196869
Epoch 640, training loss: 90.49630737304688 = 0.2789161801338196 + 10.0 * 9.02173900604248
Epoch 640, val loss: 0.3823348581790924
Epoch 650, training loss: 90.505859375 = 0.27710217237472534 + 10.0 * 9.022875785827637
Epoch 650, val loss: 0.3822020888328552
Epoch 660, training loss: 90.4888687133789 = 0.27531397342681885 + 10.0 * 9.021355628967285
Epoch 660, val loss: 0.3821360766887665
Epoch 670, training loss: 90.48464965820312 = 0.2735632061958313 + 10.0 * 9.021108627319336
Epoch 670, val loss: 0.3820393979549408
Epoch 680, training loss: 90.48326110839844 = 0.2718335688114166 + 10.0 * 9.021142959594727
Epoch 680, val loss: 0.38204464316368103
Epoch 690, training loss: 90.479248046875 = 0.27013346552848816 + 10.0 * 9.02091121673584
Epoch 690, val loss: 0.3820931017398834
Epoch 700, training loss: 90.47528076171875 = 0.2684628963470459 + 10.0 * 9.020681381225586
Epoch 700, val loss: 0.3821455240249634
Epoch 710, training loss: 90.46739959716797 = 0.2668224573135376 + 10.0 * 9.020057678222656
Epoch 710, val loss: 0.382200688123703
Epoch 720, training loss: 90.45864868164062 = 0.2652071714401245 + 10.0 * 9.019344329833984
Epoch 720, val loss: 0.38232409954071045
Epoch 730, training loss: 90.49852752685547 = 0.26361599564552307 + 10.0 * 9.023490905761719
Epoch 730, val loss: 0.3825169503688812
Epoch 740, training loss: 90.45857238769531 = 0.2620428502559662 + 10.0 * 9.0196533203125
Epoch 740, val loss: 0.38268399238586426
Epoch 750, training loss: 90.4449462890625 = 0.26050058007240295 + 10.0 * 9.018445014953613
Epoch 750, val loss: 0.38287994265556335
Epoch 760, training loss: 90.44145202636719 = 0.2589658498764038 + 10.0 * 9.018248558044434
Epoch 760, val loss: 0.38316693902015686
Epoch 770, training loss: 90.48883056640625 = 0.2574431598186493 + 10.0 * 9.023138046264648
Epoch 770, val loss: 0.3834904432296753
Epoch 780, training loss: 90.43492889404297 = 0.25595152378082275 + 10.0 * 9.017897605895996
Epoch 780, val loss: 0.3838282823562622
Epoch 790, training loss: 90.44245910644531 = 0.2544778287410736 + 10.0 * 9.018797874450684
Epoch 790, val loss: 0.3841513693332672
Epoch 800, training loss: 90.42355346679688 = 0.25302159786224365 + 10.0 * 9.017053604125977
Epoch 800, val loss: 0.3845759332180023
Epoch 810, training loss: 90.41871643066406 = 0.2515847384929657 + 10.0 * 9.01671314239502
Epoch 810, val loss: 0.3849535286426544
Epoch 820, training loss: 90.41326141357422 = 0.2501576840877533 + 10.0 * 9.01630973815918
Epoch 820, val loss: 0.3854045569896698
Epoch 830, training loss: 90.42557525634766 = 0.24873782694339752 + 10.0 * 9.017683029174805
Epoch 830, val loss: 0.3858035206794739
Epoch 840, training loss: 90.42266082763672 = 0.24733297526836395 + 10.0 * 9.017533302307129
Epoch 840, val loss: 0.38658639788627625
Epoch 850, training loss: 90.41720581054688 = 0.24595361948013306 + 10.0 * 9.017125129699707
Epoch 850, val loss: 0.3870195150375366
Epoch 860, training loss: 90.41975402832031 = 0.24459636211395264 + 10.0 * 9.017515182495117
Epoch 860, val loss: 0.38750946521759033
Epoch 870, training loss: 90.39613342285156 = 0.24325549602508545 + 10.0 * 9.015287399291992
Epoch 870, val loss: 0.38817909359931946
Epoch 880, training loss: 90.39562225341797 = 0.24192455410957336 + 10.0 * 9.015369415283203
Epoch 880, val loss: 0.3888515830039978
Epoch 890, training loss: 90.39042663574219 = 0.2405972182750702 + 10.0 * 9.014982223510742
Epoch 890, val loss: 0.3895421624183655
Epoch 900, training loss: 90.42420959472656 = 0.23928168416023254 + 10.0 * 9.018492698669434
Epoch 900, val loss: 0.39025676250457764
Epoch 910, training loss: 90.39856719970703 = 0.2379644811153412 + 10.0 * 9.016060829162598
Epoch 910, val loss: 0.3910004794597626
Epoch 920, training loss: 90.3826675415039 = 0.23667828738689423 + 10.0 * 9.014598846435547
Epoch 920, val loss: 0.391766220331192
Epoch 930, training loss: 90.3926010131836 = 0.23539763689041138 + 10.0 * 9.01572036743164
Epoch 930, val loss: 0.392598032951355
Epoch 940, training loss: 90.37667083740234 = 0.23412294685840607 + 10.0 * 9.014254570007324
Epoch 940, val loss: 0.3933756947517395
Epoch 950, training loss: 90.41459655761719 = 0.23286722600460052 + 10.0 * 9.018173217773438
Epoch 950, val loss: 0.3943031430244446
Epoch 960, training loss: 90.38030242919922 = 0.2316139191389084 + 10.0 * 9.01486873626709
Epoch 960, val loss: 0.39505133032798767
Epoch 970, training loss: 90.3666763305664 = 0.230380579829216 + 10.0 * 9.013629913330078
Epoch 970, val loss: 0.39598557353019714
Epoch 980, training loss: 90.35942840576172 = 0.22914618253707886 + 10.0 * 9.013028144836426
Epoch 980, val loss: 0.3968447744846344
Epoch 990, training loss: 90.3585205078125 = 0.2279178947210312 + 10.0 * 9.013059616088867
Epoch 990, val loss: 0.39782747626304626
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8463
Overall ASR: 0.6897
Flip ASR: 0.6126/1554 nodes
The final ASR:0.69574, 0.00477, Accuracy:0.84779, 0.00215
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97474])
remove edge: torch.Size([2, 79804])
updated graph: torch.Size([2, 88630])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7419
Flip ASR: 0.6802/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72228, 0.01730, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 10 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68531799316406 = 1.0947555303573608 + 10.0 * 10.35905647277832
Epoch 0, val loss: 1.094931960105896
Epoch 10, training loss: 104.63699340820312 = 1.0840892791748047 + 10.0 * 10.355290412902832
Epoch 10, val loss: 1.0840076208114624
Epoch 20, training loss: 103.90006256103516 = 1.0708541870117188 + 10.0 * 10.282920837402344
Epoch 20, val loss: 1.0705316066741943
Epoch 30, training loss: 98.24092102050781 = 1.057012677192688 + 10.0 * 9.718390464782715
Epoch 30, val loss: 1.0560171604156494
Epoch 40, training loss: 96.1920394897461 = 1.0435049533843994 + 10.0 * 9.514853477478027
Epoch 40, val loss: 1.0423024892807007
Epoch 50, training loss: 95.28746032714844 = 1.0312800407409668 + 10.0 * 9.425618171691895
Epoch 50, val loss: 1.0301755666732788
Epoch 60, training loss: 94.49616241455078 = 1.0208265781402588 + 10.0 * 9.347533226013184
Epoch 60, val loss: 1.0200436115264893
Epoch 70, training loss: 94.15322875976562 = 1.0115492343902588 + 10.0 * 9.314167976379395
Epoch 70, val loss: 1.0110429525375366
Epoch 80, training loss: 93.72525024414062 = 1.0030474662780762 + 10.0 * 9.272220611572266
Epoch 80, val loss: 1.0025970935821533
Epoch 90, training loss: 93.2122573852539 = 0.9949585199356079 + 10.0 * 9.22173023223877
Epoch 90, val loss: 0.9946102499961853
Epoch 100, training loss: 92.86177062988281 = 0.9862527251243591 + 10.0 * 9.187551498413086
Epoch 100, val loss: 0.9861688613891602
Epoch 110, training loss: 92.6661605834961 = 0.9757856726646423 + 10.0 * 9.169037818908691
Epoch 110, val loss: 0.9758428931236267
Epoch 120, training loss: 92.48551940917969 = 0.9646412134170532 + 10.0 * 9.152088165283203
Epoch 120, val loss: 0.965027928352356
Epoch 130, training loss: 92.34900665283203 = 0.9526379108428955 + 10.0 * 9.139636993408203
Epoch 130, val loss: 0.9531785845756531
Epoch 140, training loss: 92.2439956665039 = 0.9380436539649963 + 10.0 * 9.130595207214355
Epoch 140, val loss: 0.9392312169075012
Epoch 150, training loss: 92.13644409179688 = 0.9215303659439087 + 10.0 * 9.121491432189941
Epoch 150, val loss: 0.923366129398346
Epoch 160, training loss: 92.07330322265625 = 0.9027727246284485 + 10.0 * 9.117053031921387
Epoch 160, val loss: 0.9050161838531494
Epoch 170, training loss: 91.96072387695312 = 0.8807700276374817 + 10.0 * 9.107995986938477
Epoch 170, val loss: 0.8838732838630676
Epoch 180, training loss: 91.87225341796875 = 0.8558013439178467 + 10.0 * 9.101644515991211
Epoch 180, val loss: 0.8599157929420471
Epoch 190, training loss: 91.79341888427734 = 0.827703058719635 + 10.0 * 9.09657096862793
Epoch 190, val loss: 0.8329506516456604
Epoch 200, training loss: 91.73329162597656 = 0.7962415218353271 + 10.0 * 9.093705177307129
Epoch 200, val loss: 0.802925169467926
Epoch 210, training loss: 91.64505767822266 = 0.762352466583252 + 10.0 * 9.08827018737793
Epoch 210, val loss: 0.7707509994506836
Epoch 220, training loss: 91.58065032958984 = 0.7265979051589966 + 10.0 * 9.085405349731445
Epoch 220, val loss: 0.7369503974914551
Epoch 230, training loss: 91.51019287109375 = 0.6893322467803955 + 10.0 * 9.082086563110352
Epoch 230, val loss: 0.7018699049949646
Epoch 240, training loss: 91.44772338867188 = 0.651321530342102 + 10.0 * 9.07964038848877
Epoch 240, val loss: 0.6663441061973572
Epoch 250, training loss: 91.37045288085938 = 0.613987922668457 + 10.0 * 9.07564640045166
Epoch 250, val loss: 0.6318049430847168
Epoch 260, training loss: 91.31160736083984 = 0.5782244205474854 + 10.0 * 9.073338508605957
Epoch 260, val loss: 0.5990066528320312
Epoch 270, training loss: 91.25642395019531 = 0.5447512865066528 + 10.0 * 9.0711669921875
Epoch 270, val loss: 0.5686920285224915
Epoch 280, training loss: 91.2253646850586 = 0.5143401622772217 + 10.0 * 9.071102142333984
Epoch 280, val loss: 0.5413094162940979
Epoch 290, training loss: 91.18299865722656 = 0.4874393939971924 + 10.0 * 9.069555282592773
Epoch 290, val loss: 0.5177741646766663
Epoch 300, training loss: 91.11625671386719 = 0.4644336700439453 + 10.0 * 9.06518268585205
Epoch 300, val loss: 0.49803099036216736
Epoch 310, training loss: 91.08003997802734 = 0.44459494948387146 + 10.0 * 9.063544273376465
Epoch 310, val loss: 0.48113057017326355
Epoch 320, training loss: 91.04496002197266 = 0.4271993339061737 + 10.0 * 9.061776161193848
Epoch 320, val loss: 0.46677255630493164
Epoch 330, training loss: 91.16593933105469 = 0.4119870066642761 + 10.0 * 9.075395584106445
Epoch 330, val loss: 0.45458775758743286
Epoch 340, training loss: 91.01136016845703 = 0.3991912007331848 + 10.0 * 9.061216354370117
Epoch 340, val loss: 0.4446364939212799
Epoch 350, training loss: 90.97621154785156 = 0.3884877860546112 + 10.0 * 9.058772087097168
Epoch 350, val loss: 0.436575323343277
Epoch 360, training loss: 90.94666290283203 = 0.37913915514945984 + 10.0 * 9.05675220489502
Epoch 360, val loss: 0.42980730533599854
Epoch 370, training loss: 90.94413757324219 = 0.3707166016101837 + 10.0 * 9.057341575622559
Epoch 370, val loss: 0.42390176653862
Epoch 380, training loss: 90.92011260986328 = 0.36323562264442444 + 10.0 * 9.05568790435791
Epoch 380, val loss: 0.41893959045410156
Epoch 390, training loss: 90.89105987548828 = 0.35665005445480347 + 10.0 * 9.053441047668457
Epoch 390, val loss: 0.41472128033638
Epoch 400, training loss: 90.86795043945312 = 0.3506917953491211 + 10.0 * 9.051725387573242
Epoch 400, val loss: 0.411074161529541
Epoch 410, training loss: 90.90293884277344 = 0.3451848328113556 + 10.0 * 9.05577564239502
Epoch 410, val loss: 0.4078848958015442
Epoch 420, training loss: 90.84831237792969 = 0.34020617604255676 + 10.0 * 9.050809860229492
Epoch 420, val loss: 0.40515634417533875
Epoch 430, training loss: 90.82431030273438 = 0.3356962203979492 + 10.0 * 9.048861503601074
Epoch 430, val loss: 0.402787983417511
Epoch 440, training loss: 90.80632019042969 = 0.3314775824546814 + 10.0 * 9.047484397888184
Epoch 440, val loss: 0.4006962776184082
Epoch 450, training loss: 90.80939483642578 = 0.32748132944107056 + 10.0 * 9.04819107055664
Epoch 450, val loss: 0.3988472819328308
Epoch 460, training loss: 90.7786865234375 = 0.3237427771091461 + 10.0 * 9.045494079589844
Epoch 460, val loss: 0.3972189724445343
Epoch 470, training loss: 90.77222442626953 = 0.3202492892742157 + 10.0 * 9.045197486877441
Epoch 470, val loss: 0.3958223760128021
Epoch 480, training loss: 90.75328063964844 = 0.31691834330558777 + 10.0 * 9.043636322021484
Epoch 480, val loss: 0.3945096433162689
Epoch 490, training loss: 90.79429626464844 = 0.3137160837650299 + 10.0 * 9.04805850982666
Epoch 490, val loss: 0.3933141529560089
Epoch 500, training loss: 90.73693084716797 = 0.3106890916824341 + 10.0 * 9.042623519897461
Epoch 500, val loss: 0.3923645317554474
Epoch 510, training loss: 90.72440338134766 = 0.3078267574310303 + 10.0 * 9.041658401489258
Epoch 510, val loss: 0.3914889395236969
Epoch 520, training loss: 90.71501159667969 = 0.3050536513328552 + 10.0 * 9.040995597839355
Epoch 520, val loss: 0.3906930685043335
Epoch 530, training loss: 90.72543334960938 = 0.3023444712162018 + 10.0 * 9.042308807373047
Epoch 530, val loss: 0.38999712467193604
Epoch 540, training loss: 90.70680236816406 = 0.2997313439846039 + 10.0 * 9.0407075881958
Epoch 540, val loss: 0.38925376534461975
Epoch 550, training loss: 90.69568634033203 = 0.2972160279750824 + 10.0 * 9.039846420288086
Epoch 550, val loss: 0.38875481486320496
Epoch 560, training loss: 90.6953353881836 = 0.29477500915527344 + 10.0 * 9.040056228637695
Epoch 560, val loss: 0.3883330821990967
Epoch 570, training loss: 90.67962646484375 = 0.2924312651157379 + 10.0 * 9.038719177246094
Epoch 570, val loss: 0.3878047466278076
Epoch 580, training loss: 90.66504669189453 = 0.29014238715171814 + 10.0 * 9.037489891052246
Epoch 580, val loss: 0.3874277174472809
Epoch 590, training loss: 90.6580581665039 = 0.28788885474205017 + 10.0 * 9.037016868591309
Epoch 590, val loss: 0.3871377408504486
Epoch 600, training loss: 90.70672607421875 = 0.28567445278167725 + 10.0 * 9.042104721069336
Epoch 600, val loss: 0.3868682086467743
Epoch 610, training loss: 90.6435775756836 = 0.2835376560688019 + 10.0 * 9.036004066467285
Epoch 610, val loss: 0.38666248321533203
Epoch 620, training loss: 90.64088439941406 = 0.28145501017570496 + 10.0 * 9.035943031311035
Epoch 620, val loss: 0.38649412989616394
Epoch 630, training loss: 90.63018798828125 = 0.27939823269844055 + 10.0 * 9.035079002380371
Epoch 630, val loss: 0.3863215744495392
Epoch 640, training loss: 90.654052734375 = 0.27736181020736694 + 10.0 * 9.03766918182373
Epoch 640, val loss: 0.3862236738204956
Epoch 650, training loss: 90.65313720703125 = 0.2753787338733673 + 10.0 * 9.037775993347168
Epoch 650, val loss: 0.3863074481487274
Epoch 660, training loss: 90.6186752319336 = 0.2734576165676117 + 10.0 * 9.034521102905273
Epoch 660, val loss: 0.3862547278404236
Epoch 670, training loss: 90.60623168945312 = 0.2715609073638916 + 10.0 * 9.033467292785645
Epoch 670, val loss: 0.3862713575363159
Epoch 680, training loss: 90.61029052734375 = 0.2696765065193176 + 10.0 * 9.034061431884766
Epoch 680, val loss: 0.38638433814048767
Epoch 690, training loss: 90.6007308959961 = 0.26782336831092834 + 10.0 * 9.03329086303711
Epoch 690, val loss: 0.38649094104766846
Epoch 700, training loss: 90.6059341430664 = 0.2660113275051117 + 10.0 * 9.033991813659668
Epoch 700, val loss: 0.3867224156856537
Epoch 710, training loss: 90.59329986572266 = 0.26423588395118713 + 10.0 * 9.032906532287598
Epoch 710, val loss: 0.3868118226528168
Epoch 720, training loss: 90.57941436767578 = 0.262478232383728 + 10.0 * 9.031693458557129
Epoch 720, val loss: 0.38709592819213867
Epoch 730, training loss: 90.57357788085938 = 0.2607307732105255 + 10.0 * 9.03128433227539
Epoch 730, val loss: 0.3873761296272278
Epoch 740, training loss: 90.62841033935547 = 0.25900864601135254 + 10.0 * 9.03693962097168
Epoch 740, val loss: 0.3877064883708954
Epoch 750, training loss: 90.56331634521484 = 0.25733351707458496 + 10.0 * 9.030598640441895
Epoch 750, val loss: 0.38807281851768494
Epoch 760, training loss: 90.56303405761719 = 0.255678653717041 + 10.0 * 9.030735969543457
Epoch 760, val loss: 0.38848012685775757
Epoch 770, training loss: 90.55487823486328 = 0.2540346682071686 + 10.0 * 9.030084609985352
Epoch 770, val loss: 0.38891133666038513
Epoch 780, training loss: 90.57252502441406 = 0.2524046301841736 + 10.0 * 9.032011985778809
Epoch 780, val loss: 0.3893823027610779
Epoch 790, training loss: 90.5607681274414 = 0.2507915794849396 + 10.0 * 9.030997276306152
Epoch 790, val loss: 0.3899465799331665
Epoch 800, training loss: 90.55171966552734 = 0.2492094486951828 + 10.0 * 9.030251502990723
Epoch 800, val loss: 0.3904540538787842
Epoch 810, training loss: 90.54295349121094 = 0.24764840304851532 + 10.0 * 9.02953052520752
Epoch 810, val loss: 0.3910219967365265
Epoch 820, training loss: 90.53784942626953 = 0.24610531330108643 + 10.0 * 9.029173851013184
Epoch 820, val loss: 0.3916076719760895
Epoch 830, training loss: 90.52643585205078 = 0.24457670748233795 + 10.0 * 9.028185844421387
Epoch 830, val loss: 0.3922865688800812
Epoch 840, training loss: 90.53028106689453 = 0.24305325746536255 + 10.0 * 9.028722763061523
Epoch 840, val loss: 0.3929970860481262
Epoch 850, training loss: 90.53001403808594 = 0.24154503643512726 + 10.0 * 9.028846740722656
Epoch 850, val loss: 0.39372920989990234
Epoch 860, training loss: 90.52400207519531 = 0.2400466799736023 + 10.0 * 9.028395652770996
Epoch 860, val loss: 0.3944390118122101
Epoch 870, training loss: 90.54209899902344 = 0.23857349157333374 + 10.0 * 9.030352592468262
Epoch 870, val loss: 0.3953130841255188
Epoch 880, training loss: 90.5152816772461 = 0.23711314797401428 + 10.0 * 9.027816772460938
Epoch 880, val loss: 0.39608001708984375
Epoch 890, training loss: 90.50431060791016 = 0.2356710582971573 + 10.0 * 9.026864051818848
Epoch 890, val loss: 0.39694944024086
Epoch 900, training loss: 90.50346374511719 = 0.234234020113945 + 10.0 * 9.026922225952148
Epoch 900, val loss: 0.3978521525859833
Epoch 910, training loss: 90.5179672241211 = 0.23280936479568481 + 10.0 * 9.028515815734863
Epoch 910, val loss: 0.39886674284935
Epoch 920, training loss: 90.50238037109375 = 0.23139837384223938 + 10.0 * 9.027097702026367
Epoch 920, val loss: 0.3997539281845093
Epoch 930, training loss: 90.49187469482422 = 0.23000217974185944 + 10.0 * 9.0261869430542
Epoch 930, val loss: 0.4007412791252136
Epoch 940, training loss: 90.50345611572266 = 0.22861528396606445 + 10.0 * 9.027483940124512
Epoch 940, val loss: 0.40175607800483704
Epoch 950, training loss: 90.48639678955078 = 0.22723960876464844 + 10.0 * 9.025915145874023
Epoch 950, val loss: 0.40279293060302734
Epoch 960, training loss: 90.49368286132812 = 0.22587645053863525 + 10.0 * 9.02678108215332
Epoch 960, val loss: 0.40384867787361145
Epoch 970, training loss: 90.48444366455078 = 0.22452619671821594 + 10.0 * 9.025991439819336
Epoch 970, val loss: 0.4050750136375427
Epoch 980, training loss: 90.47126007080078 = 0.22318710386753082 + 10.0 * 9.02480697631836
Epoch 980, val loss: 0.40610235929489136
Epoch 990, training loss: 90.46562194824219 = 0.221849262714386 + 10.0 * 9.024377822875977
Epoch 990, val loss: 0.4073008596897125
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8488
Overall ASR: 0.6856
Flip ASR: 0.6081/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68804931640625 = 1.0971146821975708 + 10.0 * 10.35909366607666
Epoch 0, val loss: 1.0968414545059204
Epoch 10, training loss: 104.64820861816406 = 1.0859204530715942 + 10.0 * 10.356228828430176
Epoch 10, val loss: 1.08543860912323
Epoch 20, training loss: 104.12576293945312 = 1.0715627670288086 + 10.0 * 10.305419921875
Epoch 20, val loss: 1.0711090564727783
Epoch 30, training loss: 99.26886749267578 = 1.0574984550476074 + 10.0 * 9.821137428283691
Epoch 30, val loss: 1.0569514036178589
Epoch 40, training loss: 96.02388000488281 = 1.0429664850234985 + 10.0 * 9.498090744018555
Epoch 40, val loss: 1.0422577857971191
Epoch 50, training loss: 94.95149993896484 = 1.0256483554840088 + 10.0 * 9.392584800720215
Epoch 50, val loss: 1.0247185230255127
Epoch 60, training loss: 94.59496307373047 = 1.009565830230713 + 10.0 * 9.358539581298828
Epoch 60, val loss: 1.0088573694229126
Epoch 70, training loss: 94.40203857421875 = 0.9968039989471436 + 10.0 * 9.340523719787598
Epoch 70, val loss: 0.9963127970695496
Epoch 80, training loss: 94.18621826171875 = 0.9856870770454407 + 10.0 * 9.320053100585938
Epoch 80, val loss: 0.9852648377418518
Epoch 90, training loss: 93.92906188964844 = 0.9739983677864075 + 10.0 * 9.295506477355957
Epoch 90, val loss: 0.973789632320404
Epoch 100, training loss: 93.5652847290039 = 0.9610248804092407 + 10.0 * 9.26042652130127
Epoch 100, val loss: 0.9612047076225281
Epoch 110, training loss: 93.0708999633789 = 0.948002815246582 + 10.0 * 9.212289810180664
Epoch 110, val loss: 0.9486386775970459
Epoch 120, training loss: 92.73326110839844 = 0.9345644116401672 + 10.0 * 9.179869651794434
Epoch 120, val loss: 0.9350281357765198
Epoch 130, training loss: 92.58686065673828 = 0.9164738059043884 + 10.0 * 9.167038917541504
Epoch 130, val loss: 0.9171589612960815
Epoch 140, training loss: 92.47016906738281 = 0.8945755958557129 + 10.0 * 9.157559394836426
Epoch 140, val loss: 0.8958677649497986
Epoch 150, training loss: 92.36821746826172 = 0.8710557222366333 + 10.0 * 9.1497163772583
Epoch 150, val loss: 0.8730360865592957
Epoch 160, training loss: 92.26353454589844 = 0.8456210494041443 + 10.0 * 9.141791343688965
Epoch 160, val loss: 0.8482561707496643
Epoch 170, training loss: 92.16980743408203 = 0.8169325590133667 + 10.0 * 9.135287284851074
Epoch 170, val loss: 0.8205786943435669
Epoch 180, training loss: 92.09697723388672 = 0.7842981815338135 + 10.0 * 9.131267547607422
Epoch 180, val loss: 0.7892495393753052
Epoch 190, training loss: 92.02558135986328 = 0.7490277886390686 + 10.0 * 9.127655029296875
Epoch 190, val loss: 0.7555915117263794
Epoch 200, training loss: 91.95633697509766 = 0.7129033803939819 + 10.0 * 9.124342918395996
Epoch 200, val loss: 0.7214285135269165
Epoch 210, training loss: 91.87342071533203 = 0.6772997379302979 + 10.0 * 9.119611740112305
Epoch 210, val loss: 0.6880933046340942
Epoch 220, training loss: 91.77142333984375 = 0.6427667737007141 + 10.0 * 9.112865447998047
Epoch 220, val loss: 0.6557413935661316
Epoch 230, training loss: 91.69469451904297 = 0.6091828942298889 + 10.0 * 9.108551025390625
Epoch 230, val loss: 0.6245524287223816
Epoch 240, training loss: 91.5992431640625 = 0.577551543712616 + 10.0 * 9.102169036865234
Epoch 240, val loss: 0.5956667065620422
Epoch 250, training loss: 91.5259017944336 = 0.5482045412063599 + 10.0 * 9.097769737243652
Epoch 250, val loss: 0.5689510703086853
Epoch 260, training loss: 91.44806671142578 = 0.5208041071891785 + 10.0 * 9.09272575378418
Epoch 260, val loss: 0.5442004799842834
Epoch 270, training loss: 91.39826202392578 = 0.4953697919845581 + 10.0 * 9.090289115905762
Epoch 270, val loss: 0.5216067433357239
Epoch 280, training loss: 91.33706665039062 = 0.47267434000968933 + 10.0 * 9.08643913269043
Epoch 280, val loss: 0.5018846988677979
Epoch 290, training loss: 91.2732162475586 = 0.4530380666255951 + 10.0 * 9.08201789855957
Epoch 290, val loss: 0.48507073521614075
Epoch 300, training loss: 91.2648696899414 = 0.43573856353759766 + 10.0 * 9.082913398742676
Epoch 300, val loss: 0.47082197666168213
Epoch 310, training loss: 91.19973754882812 = 0.42062196135520935 + 10.0 * 9.077911376953125
Epoch 310, val loss: 0.45871707797050476
Epoch 320, training loss: 91.14998626708984 = 0.40776965022087097 + 10.0 * 9.07422161102295
Epoch 320, val loss: 0.4487287104129791
Epoch 330, training loss: 91.1107177734375 = 0.3965718150138855 + 10.0 * 9.071414947509766
Epoch 330, val loss: 0.44046133756637573
Epoch 340, training loss: 91.12616729736328 = 0.38661661744117737 + 10.0 * 9.073954582214355
Epoch 340, val loss: 0.4333834946155548
Epoch 350, training loss: 91.05329895019531 = 0.3779441714286804 + 10.0 * 9.067535400390625
Epoch 350, val loss: 0.42763757705688477
Epoch 360, training loss: 91.02410125732422 = 0.3704131543636322 + 10.0 * 9.06536865234375
Epoch 360, val loss: 0.42275211215019226
Epoch 370, training loss: 90.99368286132812 = 0.363643616437912 + 10.0 * 9.063004493713379
Epoch 370, val loss: 0.41869619488716125
Epoch 380, training loss: 91.00041961669922 = 0.35744231939315796 + 10.0 * 9.064297676086426
Epoch 380, val loss: 0.4151913821697235
Epoch 390, training loss: 91.01300048828125 = 0.351868599653244 + 10.0 * 9.066113471984863
Epoch 390, val loss: 0.41219431161880493
Epoch 400, training loss: 90.94952392578125 = 0.3470286726951599 + 10.0 * 9.060249328613281
Epoch 400, val loss: 0.40968820452690125
Epoch 410, training loss: 90.91619110107422 = 0.34257277846336365 + 10.0 * 9.057361602783203
Epoch 410, val loss: 0.4076118767261505
Epoch 420, training loss: 90.89954376220703 = 0.3383168876171112 + 10.0 * 9.056122779846191
Epoch 420, val loss: 0.4056658148765564
Epoch 430, training loss: 90.8846664428711 = 0.3342479169368744 + 10.0 * 9.055041313171387
Epoch 430, val loss: 0.4039273262023926
Epoch 440, training loss: 90.93474578857422 = 0.33038103580474854 + 10.0 * 9.060436248779297
Epoch 440, val loss: 0.4021413028240204
Epoch 450, training loss: 90.85969543457031 = 0.32677507400512695 + 10.0 * 9.053292274475098
Epoch 450, val loss: 0.4009125232696533
Epoch 460, training loss: 90.84964752197266 = 0.3234160244464874 + 10.0 * 9.05262279510498
Epoch 460, val loss: 0.3996787965297699
Epoch 470, training loss: 90.83419799804688 = 0.3201972544193268 + 10.0 * 9.051400184631348
Epoch 470, val loss: 0.39862146973609924
Epoch 480, training loss: 90.8365478515625 = 0.3170792758464813 + 10.0 * 9.051946640014648
Epoch 480, val loss: 0.3976408541202545
Epoch 490, training loss: 90.82295989990234 = 0.3140769898891449 + 10.0 * 9.050888061523438
Epoch 490, val loss: 0.3966616094112396
Epoch 500, training loss: 90.8050765991211 = 0.31121087074279785 + 10.0 * 9.049386978149414
Epoch 500, val loss: 0.3958013355731964
Epoch 510, training loss: 90.82350158691406 = 0.3084333539009094 + 10.0 * 9.051506996154785
Epoch 510, val loss: 0.39508530497550964
Epoch 520, training loss: 90.79228973388672 = 0.3057737946510315 + 10.0 * 9.048651695251465
Epoch 520, val loss: 0.39435338973999023
Epoch 530, training loss: 90.77606201171875 = 0.3032071590423584 + 10.0 * 9.047285079956055
Epoch 530, val loss: 0.39367443323135376
Epoch 540, training loss: 90.76699829101562 = 0.300686776638031 + 10.0 * 9.046630859375
Epoch 540, val loss: 0.3931769132614136
Epoch 550, training loss: 90.76795196533203 = 0.2982228398323059 + 10.0 * 9.046972274780273
Epoch 550, val loss: 0.39267173409461975
Epoch 560, training loss: 90.75796508789062 = 0.2958662509918213 + 10.0 * 9.046209335327148
Epoch 560, val loss: 0.3922077417373657
Epoch 570, training loss: 90.74486541748047 = 0.2935806214809418 + 10.0 * 9.04512882232666
Epoch 570, val loss: 0.39174792170524597
Epoch 580, training loss: 90.74822998046875 = 0.2913215756416321 + 10.0 * 9.045690536499023
Epoch 580, val loss: 0.39136308431625366
Epoch 590, training loss: 90.72830200195312 = 0.2891085743904114 + 10.0 * 9.043919563293457
Epoch 590, val loss: 0.39108774065971375
Epoch 600, training loss: 90.72420501708984 = 0.2869594097137451 + 10.0 * 9.04372501373291
Epoch 600, val loss: 0.39076608419418335
Epoch 610, training loss: 90.71379852294922 = 0.2848404347896576 + 10.0 * 9.042895317077637
Epoch 610, val loss: 0.39058324694633484
Epoch 620, training loss: 90.75308227539062 = 0.2827474772930145 + 10.0 * 9.047033309936523
Epoch 620, val loss: 0.3904777765274048
Epoch 630, training loss: 90.69975280761719 = 0.2807137370109558 + 10.0 * 9.041903495788574
Epoch 630, val loss: 0.39023301005363464
Epoch 640, training loss: 90.69561004638672 = 0.27872663736343384 + 10.0 * 9.041688919067383
Epoch 640, val loss: 0.39008837938308716
Epoch 650, training loss: 90.68505096435547 = 0.27675679326057434 + 10.0 * 9.0408296585083
Epoch 650, val loss: 0.3900930881500244
Epoch 660, training loss: 90.72520446777344 = 0.27481067180633545 + 10.0 * 9.045039176940918
Epoch 660, val loss: 0.39021173119544983
Epoch 670, training loss: 90.68832397460938 = 0.27290499210357666 + 10.0 * 9.041542053222656
Epoch 670, val loss: 0.3899915814399719
Epoch 680, training loss: 90.69161987304688 = 0.27105191349983215 + 10.0 * 9.042057037353516
Epoch 680, val loss: 0.3900837302207947
Epoch 690, training loss: 90.6735610961914 = 0.26923003792762756 + 10.0 * 9.040432929992676
Epoch 690, val loss: 0.39025983214378357
Epoch 700, training loss: 90.6570053100586 = 0.2674410343170166 + 10.0 * 9.038956642150879
Epoch 700, val loss: 0.39037132263183594
Epoch 710, training loss: 90.65384674072266 = 0.2656651735305786 + 10.0 * 9.038818359375
Epoch 710, val loss: 0.3905407190322876
Epoch 720, training loss: 90.64649200439453 = 0.2639136016368866 + 10.0 * 9.038257598876953
Epoch 720, val loss: 0.3907611072063446
Epoch 730, training loss: 90.6356201171875 = 0.2621978521347046 + 10.0 * 9.037342071533203
Epoch 730, val loss: 0.3909929096698761
Epoch 740, training loss: 90.63328552246094 = 0.2604990005493164 + 10.0 * 9.03727912902832
Epoch 740, val loss: 0.3911709487438202
Epoch 750, training loss: 90.65658569335938 = 0.2588185966014862 + 10.0 * 9.039776802062988
Epoch 750, val loss: 0.3915441334247589
Epoch 760, training loss: 90.62284088134766 = 0.25717172026634216 + 10.0 * 9.036566734313965
Epoch 760, val loss: 0.39199167490005493
Epoch 770, training loss: 90.63788604736328 = 0.25554895401000977 + 10.0 * 9.038233757019043
Epoch 770, val loss: 0.39244088530540466
Epoch 780, training loss: 90.6141357421875 = 0.25394973158836365 + 10.0 * 9.036018371582031
Epoch 780, val loss: 0.39260175824165344
Epoch 790, training loss: 90.60907745361328 = 0.25237101316452026 + 10.0 * 9.035670280456543
Epoch 790, val loss: 0.3932279050350189
Epoch 800, training loss: 90.60498809814453 = 0.2508125901222229 + 10.0 * 9.035417556762695
Epoch 800, val loss: 0.39362797141075134
Epoch 810, training loss: 90.59818267822266 = 0.24928756058216095 + 10.0 * 9.034889221191406
Epoch 810, val loss: 0.39413338899612427
Epoch 820, training loss: 90.5920639038086 = 0.24777953326702118 + 10.0 * 9.034428596496582
Epoch 820, val loss: 0.39464616775512695
Epoch 830, training loss: 90.57931518554688 = 0.24628466367721558 + 10.0 * 9.033303260803223
Epoch 830, val loss: 0.39519137144088745
Epoch 840, training loss: 90.6007080078125 = 0.24479946494102478 + 10.0 * 9.035591125488281
Epoch 840, val loss: 0.39582720398902893
Epoch 850, training loss: 90.58772277832031 = 0.24333685636520386 + 10.0 * 9.034438133239746
Epoch 850, val loss: 0.3964894711971283
Epoch 860, training loss: 90.56871795654297 = 0.24189357459545135 + 10.0 * 9.032682418823242
Epoch 860, val loss: 0.39703211188316345
Epoch 870, training loss: 90.55850982666016 = 0.2404572069644928 + 10.0 * 9.031805038452148
Epoch 870, val loss: 0.3977138102054596
Epoch 880, training loss: 90.57754516601562 = 0.2390303909778595 + 10.0 * 9.033851623535156
Epoch 880, val loss: 0.39831462502479553
Epoch 890, training loss: 90.57975769042969 = 0.23761433362960815 + 10.0 * 9.03421401977539
Epoch 890, val loss: 0.39936017990112305
Epoch 900, training loss: 90.54530334472656 = 0.23623116314411163 + 10.0 * 9.03090763092041
Epoch 900, val loss: 0.39992913603782654
Epoch 910, training loss: 90.54200744628906 = 0.23485401272773743 + 10.0 * 9.030714988708496
Epoch 910, val loss: 0.4007023274898529
Epoch 920, training loss: 90.59528350830078 = 0.23348408937454224 + 10.0 * 9.03618049621582
Epoch 920, val loss: 0.4015803933143616
Epoch 930, training loss: 90.53130340576172 = 0.23213937878608704 + 10.0 * 9.029916763305664
Epoch 930, val loss: 0.4023735523223877
Epoch 940, training loss: 90.52717590332031 = 0.23080982267856598 + 10.0 * 9.02963638305664
Epoch 940, val loss: 0.4032847285270691
Epoch 950, training loss: 90.52092742919922 = 0.22948358952999115 + 10.0 * 9.029144287109375
Epoch 950, val loss: 0.4041636884212494
Epoch 960, training loss: 90.55904388427734 = 0.22816511988639832 + 10.0 * 9.033087730407715
Epoch 960, val loss: 0.4051196277141571
Epoch 970, training loss: 90.52381134033203 = 0.22686275839805603 + 10.0 * 9.029695510864258
Epoch 970, val loss: 0.4060559868812561
Epoch 980, training loss: 90.51146697998047 = 0.22557801008224487 + 10.0 * 9.028589248657227
Epoch 980, val loss: 0.40697357058525085
Epoch 990, training loss: 90.50949096679688 = 0.22429999709129333 + 10.0 * 9.028519630432129
Epoch 990, val loss: 0.40805214643478394
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8442
Overall ASR: 0.7059
Flip ASR: 0.6332/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.69210052490234 = 1.1015275716781616 + 10.0 * 10.359057426452637
Epoch 0, val loss: 1.1013493537902832
Epoch 10, training loss: 104.64874267578125 = 1.0902502536773682 + 10.0 * 10.355849266052246
Epoch 10, val loss: 1.089815616607666
Epoch 20, training loss: 104.1005859375 = 1.076179027557373 + 10.0 * 10.302440643310547
Epoch 20, val loss: 1.075675368309021
Epoch 30, training loss: 99.21800994873047 = 1.06300950050354 + 10.0 * 9.815500259399414
Epoch 30, val loss: 1.0622433423995972
Epoch 40, training loss: 95.88360595703125 = 1.0479520559310913 + 10.0 * 9.483565330505371
Epoch 40, val loss: 1.0468487739562988
Epoch 50, training loss: 94.87211608886719 = 1.0318002700805664 + 10.0 * 9.384031295776367
Epoch 50, val loss: 1.0306930541992188
Epoch 60, training loss: 94.6191177368164 = 1.0162981748580933 + 10.0 * 9.360281944274902
Epoch 60, val loss: 1.0152690410614014
Epoch 70, training loss: 94.38660430908203 = 1.0029710531234741 + 10.0 * 9.338363647460938
Epoch 70, val loss: 1.0019882917404175
Epoch 80, training loss: 94.13441467285156 = 0.9923184514045715 + 10.0 * 9.314209938049316
Epoch 80, val loss: 0.9913032054901123
Epoch 90, training loss: 93.81829833984375 = 0.9805852174758911 + 10.0 * 9.283771514892578
Epoch 90, val loss: 0.9793078303337097
Epoch 100, training loss: 93.44631958007812 = 0.9660403728485107 + 10.0 * 9.248027801513672
Epoch 100, val loss: 0.9646192789077759
Epoch 110, training loss: 93.18800354003906 = 0.9495483636856079 + 10.0 * 9.223845481872559
Epoch 110, val loss: 0.9479846358299255
Epoch 120, training loss: 92.94104766845703 = 0.9304341673851013 + 10.0 * 9.201061248779297
Epoch 120, val loss: 0.9289290904998779
Epoch 130, training loss: 92.70011138916016 = 0.9094433784484863 + 10.0 * 9.17906665802002
Epoch 130, val loss: 0.9085014462471008
Epoch 140, training loss: 92.55901336669922 = 0.8865383863449097 + 10.0 * 9.167247772216797
Epoch 140, val loss: 0.8866153955459595
Epoch 150, training loss: 92.41089630126953 = 0.8608437180519104 + 10.0 * 9.155004501342773
Epoch 150, val loss: 0.8617797493934631
Epoch 160, training loss: 92.3089599609375 = 0.8315098285675049 + 10.0 * 9.147745132446289
Epoch 160, val loss: 0.8332008123397827
Epoch 170, training loss: 92.1928482055664 = 0.7994306087493896 + 10.0 * 9.139341354370117
Epoch 170, val loss: 0.8023514747619629
Epoch 180, training loss: 92.06118774414062 = 0.7654492259025574 + 10.0 * 9.129573822021484
Epoch 180, val loss: 0.7700376510620117
Epoch 190, training loss: 91.92390441894531 = 0.7294743657112122 + 10.0 * 9.1194429397583
Epoch 190, val loss: 0.7356921434402466
Epoch 200, training loss: 91.8161392211914 = 0.690988302230835 + 10.0 * 9.112515449523926
Epoch 200, val loss: 0.699188232421875
Epoch 210, training loss: 91.70943450927734 = 0.6510754227638245 + 10.0 * 9.105835914611816
Epoch 210, val loss: 0.6617287993431091
Epoch 220, training loss: 91.62153625488281 = 0.6117445230484009 + 10.0 * 9.10097885131836
Epoch 220, val loss: 0.6252188086509705
Epoch 230, training loss: 91.55567169189453 = 0.5743772983551025 + 10.0 * 9.098129272460938
Epoch 230, val loss: 0.5909802913665771
Epoch 240, training loss: 91.48341369628906 = 0.5404930114746094 + 10.0 * 9.094292640686035
Epoch 240, val loss: 0.5604143142700195
Epoch 250, training loss: 91.41214752197266 = 0.5105841159820557 + 10.0 * 9.090156555175781
Epoch 250, val loss: 0.5338773131370544
Epoch 260, training loss: 91.35768127441406 = 0.4844155013561249 + 10.0 * 9.087327003479004
Epoch 260, val loss: 0.5110913515090942
Epoch 270, training loss: 91.31781768798828 = 0.4616497755050659 + 10.0 * 9.085617065429688
Epoch 270, val loss: 0.49178382754325867
Epoch 280, training loss: 91.25846862792969 = 0.4423116147518158 + 10.0 * 9.081615447998047
Epoch 280, val loss: 0.47566673159599304
Epoch 290, training loss: 91.20860290527344 = 0.4258347451686859 + 10.0 * 9.078276634216309
Epoch 290, val loss: 0.46232420206069946
Epoch 300, training loss: 91.16757202148438 = 0.4116407036781311 + 10.0 * 9.075592994689941
Epoch 300, val loss: 0.45116668939590454
Epoch 310, training loss: 91.12723541259766 = 0.39932945370674133 + 10.0 * 9.072790145874023
Epoch 310, val loss: 0.44184133410453796
Epoch 320, training loss: 91.10468292236328 = 0.3885711133480072 + 10.0 * 9.071611404418945
Epoch 320, val loss: 0.4339488446712494
Epoch 330, training loss: 91.0781021118164 = 0.3791327178478241 + 10.0 * 9.069896697998047
Epoch 330, val loss: 0.4273602068424225
Epoch 340, training loss: 91.03571319580078 = 0.3708138167858124 + 10.0 * 9.066490173339844
Epoch 340, val loss: 0.4217335879802704
Epoch 350, training loss: 91.00975799560547 = 0.36333781480789185 + 10.0 * 9.064641952514648
Epoch 350, val loss: 0.4168809652328491
Epoch 360, training loss: 91.0649185180664 = 0.3565455973148346 + 10.0 * 9.070837020874023
Epoch 360, val loss: 0.4127063751220703
Epoch 370, training loss: 90.97441864013672 = 0.35047009587287903 + 10.0 * 9.062395095825195
Epoch 370, val loss: 0.4092429578304291
Epoch 380, training loss: 90.9540023803711 = 0.3450545072555542 + 10.0 * 9.060894966125488
Epoch 380, val loss: 0.40616869926452637
Epoch 390, training loss: 90.92774200439453 = 0.3400540053844452 + 10.0 * 9.058768272399902
Epoch 390, val loss: 0.4035516679286957
Epoch 400, training loss: 90.9100570678711 = 0.33531662821769714 + 10.0 * 9.057474136352539
Epoch 400, val loss: 0.4011800289154053
Epoch 410, training loss: 90.94712829589844 = 0.33082956075668335 + 10.0 * 9.061630249023438
Epoch 410, val loss: 0.3990175426006317
Epoch 420, training loss: 90.90904235839844 = 0.3267068564891815 + 10.0 * 9.058233261108398
Epoch 420, val loss: 0.39723411202430725
Epoch 430, training loss: 90.86609649658203 = 0.3229282796382904 + 10.0 * 9.054316520690918
Epoch 430, val loss: 0.3955623507499695
Epoch 440, training loss: 90.85259246826172 = 0.3193441927433014 + 10.0 * 9.053324699401855
Epoch 440, val loss: 0.3941667079925537
Epoch 450, training loss: 90.84356689453125 = 0.3159037232398987 + 10.0 * 9.052766799926758
Epoch 450, val loss: 0.39285826683044434
Epoch 460, training loss: 90.82679748535156 = 0.31265783309936523 + 10.0 * 9.051413536071777
Epoch 460, val loss: 0.39166533946990967
Epoch 470, training loss: 90.81254577636719 = 0.3095533847808838 + 10.0 * 9.050298690795898
Epoch 470, val loss: 0.3905999958515167
Epoch 480, training loss: 90.81489562988281 = 0.30653807520866394 + 10.0 * 9.050835609436035
Epoch 480, val loss: 0.38965171575546265
Epoch 490, training loss: 90.79997253417969 = 0.3036452829837799 + 10.0 * 9.049633026123047
Epoch 490, val loss: 0.38879454135894775
Epoch 500, training loss: 90.7754135131836 = 0.3008803725242615 + 10.0 * 9.047452926635742
Epoch 500, val loss: 0.38795557618141174
Epoch 510, training loss: 90.76441192626953 = 0.29819414019584656 + 10.0 * 9.046621322631836
Epoch 510, val loss: 0.38725462555885315
Epoch 520, training loss: 90.79747009277344 = 0.2955670952796936 + 10.0 * 9.050189971923828
Epoch 520, val loss: 0.3865714967250824
Epoch 530, training loss: 90.7559585571289 = 0.29305440187454224 + 10.0 * 9.046290397644043
Epoch 530, val loss: 0.3859918415546417
Epoch 540, training loss: 90.73535919189453 = 0.29063132405281067 + 10.0 * 9.044472694396973
Epoch 540, val loss: 0.3854786455631256
Epoch 550, training loss: 90.72704315185547 = 0.28824955224990845 + 10.0 * 9.043879508972168
Epoch 550, val loss: 0.3850855529308319
Epoch 560, training loss: 90.71976470947266 = 0.28592050075531006 + 10.0 * 9.043384552001953
Epoch 560, val loss: 0.38464388251304626
Epoch 570, training loss: 90.70372009277344 = 0.28367629647254944 + 10.0 * 9.042004585266113
Epoch 570, val loss: 0.38434216380119324
Epoch 580, training loss: 90.69503021240234 = 0.2814846932888031 + 10.0 * 9.041354179382324
Epoch 580, val loss: 0.38408246636390686
Epoch 590, training loss: 90.74332427978516 = 0.2793174386024475 + 10.0 * 9.04640007019043
Epoch 590, val loss: 0.38382062315940857
Epoch 600, training loss: 90.67517852783203 = 0.2772301137447357 + 10.0 * 9.039794921875
Epoch 600, val loss: 0.38370323181152344
Epoch 610, training loss: 90.66767120361328 = 0.2751968502998352 + 10.0 * 9.039247512817383
Epoch 610, val loss: 0.3835393786430359
Epoch 620, training loss: 90.65714263916016 = 0.2731820344924927 + 10.0 * 9.038395881652832
Epoch 620, val loss: 0.3834839165210724
Epoch 630, training loss: 90.69934844970703 = 0.2711901366710663 + 10.0 * 9.042816162109375
Epoch 630, val loss: 0.38341882824897766
Epoch 640, training loss: 90.65316772460938 = 0.2692597806453705 + 10.0 * 9.03839111328125
Epoch 640, val loss: 0.3834765553474426
Epoch 650, training loss: 90.6370620727539 = 0.26738569140434265 + 10.0 * 9.036967277526855
Epoch 650, val loss: 0.3835222125053406
Epoch 660, training loss: 90.62487030029297 = 0.2655346095561981 + 10.0 * 9.035933494567871
Epoch 660, val loss: 0.3836292028427124
Epoch 670, training loss: 90.62464904785156 = 0.2636951506137848 + 10.0 * 9.03609561920166
Epoch 670, val loss: 0.383769690990448
Epoch 680, training loss: 90.61612701416016 = 0.26188454031944275 + 10.0 * 9.03542423248291
Epoch 680, val loss: 0.3840266168117523
Epoch 690, training loss: 90.61732482910156 = 0.26012474298477173 + 10.0 * 9.035719871520996
Epoch 690, val loss: 0.38417553901672363
Epoch 700, training loss: 90.59724426269531 = 0.25839436054229736 + 10.0 * 9.03388500213623
Epoch 700, val loss: 0.3844861686229706
Epoch 710, training loss: 90.59120178222656 = 0.2566906809806824 + 10.0 * 9.033451080322266
Epoch 710, val loss: 0.3847721219062805
Epoch 720, training loss: 90.5837631225586 = 0.25500020384788513 + 10.0 * 9.032876968383789
Epoch 720, val loss: 0.3851156234741211
Epoch 730, training loss: 90.59278106689453 = 0.2533198595046997 + 10.0 * 9.03394603729248
Epoch 730, val loss: 0.3854967951774597
Epoch 740, training loss: 90.59905242919922 = 0.2516786456108093 + 10.0 * 9.034737586975098
Epoch 740, val loss: 0.3859744071960449
Epoch 750, training loss: 90.56893157958984 = 0.2500801980495453 + 10.0 * 9.031885147094727
Epoch 750, val loss: 0.3864051401615143
Epoch 760, training loss: 90.55892944335938 = 0.24849757552146912 + 10.0 * 9.03104305267334
Epoch 760, val loss: 0.3869003653526306
Epoch 770, training loss: 90.55862426757812 = 0.24692250788211823 + 10.0 * 9.031169891357422
Epoch 770, val loss: 0.387405127286911
Epoch 780, training loss: 90.56558990478516 = 0.24537017941474915 + 10.0 * 9.032022476196289
Epoch 780, val loss: 0.38797488808631897
Epoch 790, training loss: 90.55768585205078 = 0.24385464191436768 + 10.0 * 9.03138256072998
Epoch 790, val loss: 0.38858357071876526
Epoch 800, training loss: 90.54113006591797 = 0.24236559867858887 + 10.0 * 9.029876708984375
Epoch 800, val loss: 0.3892037570476532
Epoch 810, training loss: 90.53422546386719 = 0.24088343977928162 + 10.0 * 9.02933406829834
Epoch 810, val loss: 0.3898616433143616
Epoch 820, training loss: 90.54206085205078 = 0.2394099086523056 + 10.0 * 9.030264854431152
Epoch 820, val loss: 0.3905772268772125
Epoch 830, training loss: 90.52591705322266 = 0.23795472085475922 + 10.0 * 9.028796195983887
Epoch 830, val loss: 0.39127400517463684
Epoch 840, training loss: 90.52108764648438 = 0.23652130365371704 + 10.0 * 9.028456687927246
Epoch 840, val loss: 0.39203882217407227
Epoch 850, training loss: 90.51942443847656 = 0.23509396612644196 + 10.0 * 9.028432846069336
Epoch 850, val loss: 0.3928461968898773
Epoch 860, training loss: 90.5423812866211 = 0.23368026316165924 + 10.0 * 9.03087043762207
Epoch 860, val loss: 0.3936920166015625
Epoch 870, training loss: 90.51343536376953 = 0.23229360580444336 + 10.0 * 9.028114318847656
Epoch 870, val loss: 0.39456456899642944
Epoch 880, training loss: 90.50867462158203 = 0.23091880977153778 + 10.0 * 9.027775764465332
Epoch 880, val loss: 0.3954724669456482
Epoch 890, training loss: 90.5011215209961 = 0.22955811023712158 + 10.0 * 9.027155876159668
Epoch 890, val loss: 0.3963637351989746
Epoch 900, training loss: 90.5129165649414 = 0.22820623219013214 + 10.0 * 9.028470993041992
Epoch 900, val loss: 0.39737871289253235
Epoch 910, training loss: 90.49665069580078 = 0.22686947882175446 + 10.0 * 9.026978492736816
Epoch 910, val loss: 0.3983174264431
Epoch 920, training loss: 90.48749542236328 = 0.225545272231102 + 10.0 * 9.02619457244873
Epoch 920, val loss: 0.39936795830726624
Epoch 930, training loss: 90.50491333007812 = 0.22422602772712708 + 10.0 * 9.028068542480469
Epoch 930, val loss: 0.4003922939300537
Epoch 940, training loss: 90.49632263183594 = 0.2229212522506714 + 10.0 * 9.027339935302734
Epoch 940, val loss: 0.40146252512931824
Epoch 950, training loss: 90.4737548828125 = 0.22164002060890198 + 10.0 * 9.025211334228516
Epoch 950, val loss: 0.4025512635707855
Epoch 960, training loss: 90.46931457519531 = 0.22036117315292358 + 10.0 * 9.024895668029785
Epoch 960, val loss: 0.4036741852760315
Epoch 970, training loss: 90.4906005859375 = 0.21908535063266754 + 10.0 * 9.027151107788086
Epoch 970, val loss: 0.4048079252243042
Epoch 980, training loss: 90.48287200927734 = 0.2178308516740799 + 10.0 * 9.026503562927246
Epoch 980, val loss: 0.40598630905151367
Epoch 990, training loss: 90.45814514160156 = 0.21659068763256073 + 10.0 * 9.024155616760254
Epoch 990, val loss: 0.40714171528816223
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8437
Overall ASR: 0.6897
Flip ASR: 0.6139/1554 nodes
The final ASR:0.69371, 0.00876, Accuracy:0.84559, 0.00228
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97576])
remove edge: torch.Size([2, 79670])
updated graph: torch.Size([2, 88598])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8508
Overall ASR: 0.7622
Flip ASR: 0.7040/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72904, 0.02561, Accuracy:0.85033, 0.00041
Begin epxeriment: cont_weight: 10 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.6934814453125 = 1.1022876501083374 + 10.0 * 10.359119415283203
Epoch 0, val loss: 1.1010106801986694
Epoch 10, training loss: 104.6548843383789 = 1.0909018516540527 + 10.0 * 10.35639762878418
Epoch 10, val loss: 1.0895744562149048
Epoch 20, training loss: 104.10640716552734 = 1.0757033824920654 + 10.0 * 10.303070068359375
Epoch 20, val loss: 1.0744301080703735
Epoch 30, training loss: 99.88780975341797 = 1.0612796545028687 + 10.0 * 9.88265323638916
Epoch 30, val loss: 1.0605181455612183
Epoch 40, training loss: 96.93297576904297 = 1.047676682472229 + 10.0 * 9.588529586791992
Epoch 40, val loss: 1.046792984008789
Epoch 50, training loss: 95.7989730834961 = 1.030236840248108 + 10.0 * 9.476873397827148
Epoch 50, val loss: 1.0289292335510254
Epoch 60, training loss: 94.84088134765625 = 1.0100077390670776 + 10.0 * 9.383087158203125
Epoch 60, val loss: 1.0089387893676758
Epoch 70, training loss: 94.56190490722656 = 0.9878479838371277 + 10.0 * 9.357405662536621
Epoch 70, val loss: 0.9873639345169067
Epoch 80, training loss: 94.27586364746094 = 0.965325117111206 + 10.0 * 9.331053733825684
Epoch 80, val loss: 0.9654898643493652
Epoch 90, training loss: 93.91394805908203 = 0.9429401159286499 + 10.0 * 9.297101020812988
Epoch 90, val loss: 0.9436278939247131
Epoch 100, training loss: 93.46883392333984 = 0.9179359674453735 + 10.0 * 9.25508975982666
Epoch 100, val loss: 0.9190442562103271
Epoch 110, training loss: 93.1307601928711 = 0.8872317671775818 + 10.0 * 9.224352836608887
Epoch 110, val loss: 0.889037549495697
Epoch 120, training loss: 92.8458480834961 = 0.8505037426948547 + 10.0 * 9.19953441619873
Epoch 120, val loss: 0.8535144329071045
Epoch 130, training loss: 92.65203094482422 = 0.8097148537635803 + 10.0 * 9.184231758117676
Epoch 130, val loss: 0.8143745064735413
Epoch 140, training loss: 92.49232482910156 = 0.7654234766960144 + 10.0 * 9.172689437866211
Epoch 140, val loss: 0.7720254063606262
Epoch 150, training loss: 92.37357330322266 = 0.718985378742218 + 10.0 * 9.165458679199219
Epoch 150, val loss: 0.7279255390167236
Epoch 160, training loss: 92.24072265625 = 0.6734637022018433 + 10.0 * 9.156725883483887
Epoch 160, val loss: 0.6855405569076538
Epoch 170, training loss: 92.09972381591797 = 0.6307312846183777 + 10.0 * 9.146899223327637
Epoch 170, val loss: 0.6462578177452087
Epoch 180, training loss: 91.98577880859375 = 0.5916328430175781 + 10.0 * 9.13941478729248
Epoch 180, val loss: 0.6109246015548706
Epoch 190, training loss: 91.88578033447266 = 0.556873619556427 + 10.0 * 9.132890701293945
Epoch 190, val loss: 0.5800873041152954
Epoch 200, training loss: 91.79620361328125 = 0.5266034007072449 + 10.0 * 9.126959800720215
Epoch 200, val loss: 0.5538079738616943
Epoch 210, training loss: 91.7339859008789 = 0.501122236251831 + 10.0 * 9.123286247253418
Epoch 210, val loss: 0.5323881506919861
Epoch 220, training loss: 91.65931701660156 = 0.48004528880119324 + 10.0 * 9.117927551269531
Epoch 220, val loss: 0.5149617791175842
Epoch 230, training loss: 91.62385559082031 = 0.46219930052757263 + 10.0 * 9.116166114807129
Epoch 230, val loss: 0.5006093978881836
Epoch 240, training loss: 91.56170654296875 = 0.4469144344329834 + 10.0 * 9.111478805541992
Epoch 240, val loss: 0.4886631369590759
Epoch 250, training loss: 91.508056640625 = 0.4337146580219269 + 10.0 * 9.107434272766113
Epoch 250, val loss: 0.47861868143081665
Epoch 260, training loss: 91.46111297607422 = 0.42214396595954895 + 10.0 * 9.103897094726562
Epoch 260, val loss: 0.47008201479911804
Epoch 270, training loss: 91.43307495117188 = 0.41177722811698914 + 10.0 * 9.102129936218262
Epoch 270, val loss: 0.4627193808555603
Epoch 280, training loss: 91.36991882324219 = 0.4026196002960205 + 10.0 * 9.09673023223877
Epoch 280, val loss: 0.45632699131965637
Epoch 290, training loss: 91.32903289794922 = 0.3943715989589691 + 10.0 * 9.093465805053711
Epoch 290, val loss: 0.45080068707466125
Epoch 300, training loss: 91.29139709472656 = 0.38671597838401794 + 10.0 * 9.09046745300293
Epoch 300, val loss: 0.4458293318748474
Epoch 310, training loss: 91.26342010498047 = 0.3795102834701538 + 10.0 * 9.088391304016113
Epoch 310, val loss: 0.44122233986854553
Epoch 320, training loss: 91.24081420898438 = 0.372787743806839 + 10.0 * 9.08680248260498
Epoch 320, val loss: 0.4371858239173889
Epoch 330, training loss: 91.20838928222656 = 0.3666515350341797 + 10.0 * 9.084173202514648
Epoch 330, val loss: 0.4335252344608307
Epoch 340, training loss: 91.18305206298828 = 0.36090484261512756 + 10.0 * 9.08221435546875
Epoch 340, val loss: 0.43019595742225647
Epoch 350, training loss: 91.15973663330078 = 0.3553871512413025 + 10.0 * 9.080434799194336
Epoch 350, val loss: 0.42718496918678284
Epoch 360, training loss: 91.21147918701172 = 0.3500717580318451 + 10.0 * 9.086140632629395
Epoch 360, val loss: 0.4244624972343445
Epoch 370, training loss: 91.13627624511719 = 0.3449377715587616 + 10.0 * 9.079133987426758
Epoch 370, val loss: 0.42170804738998413
Epoch 380, training loss: 91.09972381591797 = 0.34010910987854004 + 10.0 * 9.075961112976074
Epoch 380, val loss: 0.41920289397239685
Epoch 390, training loss: 91.08248138427734 = 0.3354896008968353 + 10.0 * 9.074699401855469
Epoch 390, val loss: 0.41698572039604187
Epoch 400, training loss: 91.09063720703125 = 0.3310123682022095 + 10.0 * 9.075963020324707
Epoch 400, val loss: 0.41496965289115906
Epoch 410, training loss: 91.04963684082031 = 0.326639860868454 + 10.0 * 9.07229995727539
Epoch 410, val loss: 0.4131082594394684
Epoch 420, training loss: 91.03120422363281 = 0.32248085737228394 + 10.0 * 9.07087230682373
Epoch 420, val loss: 0.41138654947280884
Epoch 430, training loss: 91.0147705078125 = 0.31849443912506104 + 10.0 * 9.06962776184082
Epoch 430, val loss: 0.4098317325115204
Epoch 440, training loss: 90.99549865722656 = 0.31460511684417725 + 10.0 * 9.068089485168457
Epoch 440, val loss: 0.4084070026874542
Epoch 450, training loss: 90.98047637939453 = 0.31080061197280884 + 10.0 * 9.066967964172363
Epoch 450, val loss: 0.40711307525634766
Epoch 460, training loss: 91.01892852783203 = 0.30707624554634094 + 10.0 * 9.071185111999512
Epoch 460, val loss: 0.4058510661125183
Epoch 470, training loss: 90.95465087890625 = 0.30349624156951904 + 10.0 * 9.065114974975586
Epoch 470, val loss: 0.4048756957054138
Epoch 480, training loss: 90.9396743774414 = 0.30013036727905273 + 10.0 * 9.06395435333252
Epoch 480, val loss: 0.4039449393749237
Epoch 490, training loss: 90.92471313476562 = 0.29686233401298523 + 10.0 * 9.062785148620605
Epoch 490, val loss: 0.40317660570144653
Epoch 500, training loss: 90.90787506103516 = 0.2936365604400635 + 10.0 * 9.061423301696777
Epoch 500, val loss: 0.4025016129016876
Epoch 510, training loss: 90.89860534667969 = 0.29046550393104553 + 10.0 * 9.060813903808594
Epoch 510, val loss: 0.40192678570747375
Epoch 520, training loss: 90.89849853515625 = 0.2873443365097046 + 10.0 * 9.061115264892578
Epoch 520, val loss: 0.4015275239944458
Epoch 530, training loss: 90.88652801513672 = 0.2843391001224518 + 10.0 * 9.060218811035156
Epoch 530, val loss: 0.40116867423057556
Epoch 540, training loss: 90.86621856689453 = 0.28145721554756165 + 10.0 * 9.058476448059082
Epoch 540, val loss: 0.40096405148506165
Epoch 550, training loss: 90.8504638671875 = 0.2786455750465393 + 10.0 * 9.057181358337402
Epoch 550, val loss: 0.4008759558200836
Epoch 560, training loss: 90.8412857055664 = 0.27587372064590454 + 10.0 * 9.056541442871094
Epoch 560, val loss: 0.4009172320365906
Epoch 570, training loss: 90.83067321777344 = 0.27313581109046936 + 10.0 * 9.055753707885742
Epoch 570, val loss: 0.4009881019592285
Epoch 580, training loss: 90.82003784179688 = 0.27047640085220337 + 10.0 * 9.054956436157227
Epoch 580, val loss: 0.40120169520378113
Epoch 590, training loss: 90.80828094482422 = 0.26790913939476013 + 10.0 * 9.054037094116211
Epoch 590, val loss: 0.4014982283115387
Epoch 600, training loss: 90.7986068725586 = 0.2653886675834656 + 10.0 * 9.053321838378906
Epoch 600, val loss: 0.40187984704971313
Epoch 610, training loss: 90.82622528076172 = 0.2629043757915497 + 10.0 * 9.0563325881958
Epoch 610, val loss: 0.4022497236728668
Epoch 620, training loss: 90.79694366455078 = 0.26045048236846924 + 10.0 * 9.053648948669434
Epoch 620, val loss: 0.40300795435905457
Epoch 630, training loss: 90.78109741210938 = 0.2580707371234894 + 10.0 * 9.052302360534668
Epoch 630, val loss: 0.40369874238967896
Epoch 640, training loss: 90.76509094238281 = 0.25574901700019836 + 10.0 * 9.050933837890625
Epoch 640, val loss: 0.4043029248714447
Epoch 650, training loss: 90.75888061523438 = 0.2534599304199219 + 10.0 * 9.050541877746582
Epoch 650, val loss: 0.40500742197036743
Epoch 660, training loss: 90.77684020996094 = 0.25118833780288696 + 10.0 * 9.05256462097168
Epoch 660, val loss: 0.40604740381240845
Epoch 670, training loss: 90.74829864501953 = 0.24895532429218292 + 10.0 * 9.049934387207031
Epoch 670, val loss: 0.4068222641944885
Epoch 680, training loss: 90.7322998046875 = 0.24678261578083038 + 10.0 * 9.048551559448242
Epoch 680, val loss: 0.40774041414260864
Epoch 690, training loss: 90.72347259521484 = 0.24463976919651031 + 10.0 * 9.047883987426758
Epoch 690, val loss: 0.40870901942253113
Epoch 700, training loss: 90.7147445678711 = 0.2424999624490738 + 10.0 * 9.047224044799805
Epoch 700, val loss: 0.4096998870372772
Epoch 710, training loss: 90.7153091430664 = 0.24036845564842224 + 10.0 * 9.047493934631348
Epoch 710, val loss: 0.4108254909515381
Epoch 720, training loss: 90.71035766601562 = 0.23824340105056763 + 10.0 * 9.047211647033691
Epoch 720, val loss: 0.411876380443573
Epoch 730, training loss: 90.69461059570312 = 0.23615975677967072 + 10.0 * 9.045845031738281
Epoch 730, val loss: 0.4129906892776489
Epoch 740, training loss: 90.68722534179688 = 0.23411528766155243 + 10.0 * 9.045310974121094
Epoch 740, val loss: 0.41424310207366943
Epoch 750, training loss: 90.6805648803711 = 0.23207668960094452 + 10.0 * 9.044848442077637
Epoch 750, val loss: 0.415486216545105
Epoch 760, training loss: 90.68185424804688 = 0.23003782331943512 + 10.0 * 9.045181274414062
Epoch 760, val loss: 0.416911244392395
Epoch 770, training loss: 90.66925811767578 = 0.22800393402576447 + 10.0 * 9.0441255569458
Epoch 770, val loss: 0.41807159781455994
Epoch 780, training loss: 90.67283630371094 = 0.22599272429943085 + 10.0 * 9.044684410095215
Epoch 780, val loss: 0.41961461305618286
Epoch 790, training loss: 90.66243743896484 = 0.22399461269378662 + 10.0 * 9.043844223022461
Epoch 790, val loss: 0.42092224955558777
Epoch 800, training loss: 90.64926147460938 = 0.2219967097043991 + 10.0 * 9.042726516723633
Epoch 800, val loss: 0.42222344875335693
Epoch 810, training loss: 90.65098571777344 = 0.22000117599964142 + 10.0 * 9.043098449707031
Epoch 810, val loss: 0.42372986674308777
Epoch 820, training loss: 90.6393814086914 = 0.218002587556839 + 10.0 * 9.04213809967041
Epoch 820, val loss: 0.4251478314399719
Epoch 830, training loss: 90.6470718383789 = 0.2160130888223648 + 10.0 * 9.043106079101562
Epoch 830, val loss: 0.42664438486099243
Epoch 840, training loss: 90.62876892089844 = 0.21403008699417114 + 10.0 * 9.041474342346191
Epoch 840, val loss: 0.4282900393009186
Epoch 850, training loss: 90.61862182617188 = 0.21206043660640717 + 10.0 * 9.040656089782715
Epoch 850, val loss: 0.42992067337036133
Epoch 860, training loss: 90.61521911621094 = 0.2100936472415924 + 10.0 * 9.040513038635254
Epoch 860, val loss: 0.4314735531806946
Epoch 870, training loss: 90.6785659790039 = 0.20812663435935974 + 10.0 * 9.047043800354004
Epoch 870, val loss: 0.4330766201019287
Epoch 880, training loss: 90.60458374023438 = 0.20615865290164948 + 10.0 * 9.03984260559082
Epoch 880, val loss: 0.43503573536872864
Epoch 890, training loss: 90.59910583496094 = 0.2042105793952942 + 10.0 * 9.03948974609375
Epoch 890, val loss: 0.43680334091186523
Epoch 900, training loss: 90.59226989746094 = 0.2022714912891388 + 10.0 * 9.038999557495117
Epoch 900, val loss: 0.43856674432754517
Epoch 910, training loss: 90.58634185791016 = 0.20032665133476257 + 10.0 * 9.038601875305176
Epoch 910, val loss: 0.4404528737068176
Epoch 920, training loss: 90.63047790527344 = 0.19838660955429077 + 10.0 * 9.043209075927734
Epoch 920, val loss: 0.44252312183380127
Epoch 930, training loss: 90.5797348022461 = 0.19644279778003693 + 10.0 * 9.038329124450684
Epoch 930, val loss: 0.4443897008895874
Epoch 940, training loss: 90.57855987548828 = 0.19451485574245453 + 10.0 * 9.03840446472168
Epoch 940, val loss: 0.4463770389556885
Epoch 950, training loss: 90.5844955444336 = 0.19258832931518555 + 10.0 * 9.039190292358398
Epoch 950, val loss: 0.4483186602592468
Epoch 960, training loss: 90.57072448730469 = 0.1906716376543045 + 10.0 * 9.038004875183105
Epoch 960, val loss: 0.45024383068084717
Epoch 970, training loss: 90.5571517944336 = 0.18875530362129211 + 10.0 * 9.036839485168457
Epoch 970, val loss: 0.45238757133483887
Epoch 980, training loss: 90.55220031738281 = 0.18682919442653656 + 10.0 * 9.036537170410156
Epoch 980, val loss: 0.45431646704673767
Epoch 990, training loss: 90.58122253417969 = 0.18491534888744354 + 10.0 * 9.039630889892578
Epoch 990, val loss: 0.45660027861595154
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8442
Overall ASR: 0.6755
Flip ASR: 0.5972/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68470001220703 = 1.093464732170105 + 10.0 * 10.359123229980469
Epoch 0, val loss: 1.0935167074203491
Epoch 10, training loss: 104.6487808227539 = 1.0827901363372803 + 10.0 * 10.356599807739258
Epoch 10, val loss: 1.0826328992843628
Epoch 20, training loss: 104.16127014160156 = 1.068636178970337 + 10.0 * 10.309263229370117
Epoch 20, val loss: 1.0682116746902466
Epoch 30, training loss: 100.7190170288086 = 1.054140329360962 + 10.0 * 9.966487884521484
Epoch 30, val loss: 1.053678274154663
Epoch 40, training loss: 97.3848876953125 = 1.0388641357421875 + 10.0 * 9.634602546691895
Epoch 40, val loss: 1.0379831790924072
Epoch 50, training loss: 95.99332427978516 = 1.0193374156951904 + 10.0 * 9.497398376464844
Epoch 50, val loss: 1.0180094242095947
Epoch 60, training loss: 94.97901153564453 = 0.9968163967132568 + 10.0 * 9.398219108581543
Epoch 60, val loss: 0.9958221316337585
Epoch 70, training loss: 94.4592514038086 = 0.9719003438949585 + 10.0 * 9.348734855651855
Epoch 70, val loss: 0.9716525077819824
Epoch 80, training loss: 94.11669158935547 = 0.9443838596343994 + 10.0 * 9.317231178283691
Epoch 80, val loss: 0.9450286626815796
Epoch 90, training loss: 93.61787414550781 = 0.9155112504959106 + 10.0 * 9.270236015319824
Epoch 90, val loss: 0.9171844720840454
Epoch 100, training loss: 93.3006591796875 = 0.8842071294784546 + 10.0 * 9.241644859313965
Epoch 100, val loss: 0.887008011341095
Epoch 110, training loss: 93.00426483154297 = 0.8476974964141846 + 10.0 * 9.215657234191895
Epoch 110, val loss: 0.8524544835090637
Epoch 120, training loss: 92.74461364746094 = 0.8071151971817017 + 10.0 * 9.193750381469727
Epoch 120, val loss: 0.8145873546600342
Epoch 130, training loss: 92.58834075927734 = 0.7635563611984253 + 10.0 * 9.182477951049805
Epoch 130, val loss: 0.7740592956542969
Epoch 140, training loss: 92.45084381103516 = 0.7174727320671082 + 10.0 * 9.17333698272705
Epoch 140, val loss: 0.7312309145927429
Epoch 150, training loss: 92.3709716796875 = 0.6710740327835083 + 10.0 * 9.169989585876465
Epoch 150, val loss: 0.6893216371536255
Epoch 160, training loss: 92.24134063720703 = 0.6281343698501587 + 10.0 * 9.161320686340332
Epoch 160, val loss: 0.6508142352104187
Epoch 170, training loss: 92.14138793945312 = 0.5894466638565063 + 10.0 * 9.155194282531738
Epoch 170, val loss: 0.61714768409729
Epoch 180, training loss: 92.02803802490234 = 0.5559051036834717 + 10.0 * 9.147212982177734
Epoch 180, val loss: 0.5888007879257202
Epoch 190, training loss: 91.91824340820312 = 0.5277423858642578 + 10.0 * 9.139050483703613
Epoch 190, val loss: 0.5656419992446899
Epoch 200, training loss: 91.8118896484375 = 0.5041710734367371 + 10.0 * 9.13077163696289
Epoch 200, val loss: 0.5469653606414795
Epoch 210, training loss: 91.72651672363281 = 0.4837660491466522 + 10.0 * 9.124275207519531
Epoch 210, val loss: 0.5312948822975159
Epoch 220, training loss: 91.6789321899414 = 0.4658769965171814 + 10.0 * 9.121305465698242
Epoch 220, val loss: 0.5181299448013306
Epoch 230, training loss: 91.6043701171875 = 0.45057883858680725 + 10.0 * 9.115379333496094
Epoch 230, val loss: 0.5072616338729858
Epoch 240, training loss: 91.54452514648438 = 0.43750959634780884 + 10.0 * 9.110701560974121
Epoch 240, val loss: 0.4985036849975586
Epoch 250, training loss: 91.4898910522461 = 0.4261598289012909 + 10.0 * 9.106372833251953
Epoch 250, val loss: 0.49124598503112793
Epoch 260, training loss: 91.43777465820312 = 0.41601598262786865 + 10.0 * 9.10217571258545
Epoch 260, val loss: 0.4850262403488159
Epoch 270, training loss: 91.43157958984375 = 0.40682899951934814 + 10.0 * 9.1024751663208
Epoch 270, val loss: 0.47955259680747986
Epoch 280, training loss: 91.37016296386719 = 0.39836832880973816 + 10.0 * 9.097179412841797
Epoch 280, val loss: 0.47467243671417236
Epoch 290, training loss: 91.30772399902344 = 0.3907647132873535 + 10.0 * 9.091695785522461
Epoch 290, val loss: 0.47038769721984863
Epoch 300, training loss: 91.27131652832031 = 0.383823424577713 + 10.0 * 9.088748931884766
Epoch 300, val loss: 0.46661096811294556
Epoch 310, training loss: 91.25776672363281 = 0.3772713541984558 + 10.0 * 9.088048934936523
Epoch 310, val loss: 0.4631021320819855
Epoch 320, training loss: 91.22100067138672 = 0.3710741698741913 + 10.0 * 9.084993362426758
Epoch 320, val loss: 0.45983803272247314
Epoch 330, training loss: 91.1952896118164 = 0.365322083234787 + 10.0 * 9.08299732208252
Epoch 330, val loss: 0.45689821243286133
Epoch 340, training loss: 91.1701431274414 = 0.35991916060447693 + 10.0 * 9.081022262573242
Epoch 340, val loss: 0.4542617201805115
Epoch 350, training loss: 91.16043853759766 = 0.35476288199424744 + 10.0 * 9.080568313598633
Epoch 350, val loss: 0.45173490047454834
Epoch 360, training loss: 91.14913177490234 = 0.3498174548149109 + 10.0 * 9.079931259155273
Epoch 360, val loss: 0.44944876432418823
Epoch 370, training loss: 91.11577606201172 = 0.3451635241508484 + 10.0 * 9.077061653137207
Epoch 370, val loss: 0.4472992420196533
Epoch 380, training loss: 91.10433959960938 = 0.3407604992389679 + 10.0 * 9.0763578414917
Epoch 380, val loss: 0.44535955786705017
Epoch 390, training loss: 91.10769653320312 = 0.3365280330181122 + 10.0 * 9.077116966247559
Epoch 390, val loss: 0.443563312292099
Epoch 400, training loss: 91.07872009277344 = 0.3324507176876068 + 10.0 * 9.074626922607422
Epoch 400, val loss: 0.44186386466026306
Epoch 410, training loss: 91.05989074707031 = 0.32858356833457947 + 10.0 * 9.07313060760498
Epoch 410, val loss: 0.4402296841144562
Epoch 420, training loss: 91.04315185546875 = 0.32488489151000977 + 10.0 * 9.071826934814453
Epoch 420, val loss: 0.4387943744659424
Epoch 430, training loss: 91.03733825683594 = 0.32127809524536133 + 10.0 * 9.071605682373047
Epoch 430, val loss: 0.4374256730079651
Epoch 440, training loss: 91.00617218017578 = 0.317812442779541 + 10.0 * 9.068836212158203
Epoch 440, val loss: 0.4361257255077362
Epoch 450, training loss: 90.99144744873047 = 0.31448668241500854 + 10.0 * 9.067696571350098
Epoch 450, val loss: 0.4349396824836731
Epoch 460, training loss: 91.00115966796875 = 0.311250239610672 + 10.0 * 9.068990707397461
Epoch 460, val loss: 0.43386149406433105
Epoch 470, training loss: 90.98655700683594 = 0.30806565284729004 + 10.0 * 9.067849159240723
Epoch 470, val loss: 0.43277406692504883
Epoch 480, training loss: 90.95619201660156 = 0.3050089478492737 + 10.0 * 9.065118789672852
Epoch 480, val loss: 0.43180185556411743
Epoch 490, training loss: 90.93742370605469 = 0.3020562529563904 + 10.0 * 9.063536643981934
Epoch 490, val loss: 0.4309752285480499
Epoch 500, training loss: 90.9282455444336 = 0.2991652488708496 + 10.0 * 9.062908172607422
Epoch 500, val loss: 0.43023374676704407
Epoch 510, training loss: 90.92679595947266 = 0.29630714654922485 + 10.0 * 9.063048362731934
Epoch 510, val loss: 0.4292784035205841
Epoch 520, training loss: 90.90746307373047 = 0.2935422360897064 + 10.0 * 9.061391830444336
Epoch 520, val loss: 0.4286969006061554
Epoch 530, training loss: 90.89369201660156 = 0.2908934950828552 + 10.0 * 9.060279846191406
Epoch 530, val loss: 0.42808985710144043
Epoch 540, training loss: 90.87943267822266 = 0.2883017659187317 + 10.0 * 9.059113502502441
Epoch 540, val loss: 0.42759281396865845
Epoch 550, training loss: 90.94343566894531 = 0.2857436537742615 + 10.0 * 9.06576919555664
Epoch 550, val loss: 0.4273504614830017
Epoch 560, training loss: 90.87357330322266 = 0.2832033634185791 + 10.0 * 9.059037208557129
Epoch 560, val loss: 0.4268116056919098
Epoch 570, training loss: 90.85475158691406 = 0.28076404333114624 + 10.0 * 9.057398796081543
Epoch 570, val loss: 0.42639443278312683
Epoch 580, training loss: 90.84088134765625 = 0.27838248014450073 + 10.0 * 9.056249618530273
Epoch 580, val loss: 0.42608094215393066
Epoch 590, training loss: 90.87818908691406 = 0.27602800726890564 + 10.0 * 9.060215950012207
Epoch 590, val loss: 0.42583733797073364
Epoch 600, training loss: 90.8311538696289 = 0.2736802101135254 + 10.0 * 9.055747032165527
Epoch 600, val loss: 0.42565637826919556
Epoch 610, training loss: 90.81884002685547 = 0.27140381932258606 + 10.0 * 9.054743766784668
Epoch 610, val loss: 0.42553287744522095
Epoch 620, training loss: 90.80491638183594 = 0.26916295289993286 + 10.0 * 9.05357551574707
Epoch 620, val loss: 0.42548176646232605
Epoch 630, training loss: 90.85800170898438 = 0.2669399678707123 + 10.0 * 9.059106826782227
Epoch 630, val loss: 0.42553967237472534
Epoch 640, training loss: 90.78971099853516 = 0.26472917199134827 + 10.0 * 9.052497863769531
Epoch 640, val loss: 0.4255339503288269
Epoch 650, training loss: 90.78379821777344 = 0.262571781873703 + 10.0 * 9.052122116088867
Epoch 650, val loss: 0.42563948035240173
Epoch 660, training loss: 90.77742767333984 = 0.26044222712516785 + 10.0 * 9.051698684692383
Epoch 660, val loss: 0.42576131224632263
Epoch 670, training loss: 90.78097534179688 = 0.2583181858062744 + 10.0 * 9.052266120910645
Epoch 670, val loss: 0.42595165967941284
Epoch 680, training loss: 90.76887512207031 = 0.2562171220779419 + 10.0 * 9.051265716552734
Epoch 680, val loss: 0.42625153064727783
Epoch 690, training loss: 90.75865936279297 = 0.2541433870792389 + 10.0 * 9.050451278686523
Epoch 690, val loss: 0.4265299439430237
Epoch 700, training loss: 90.74893951416016 = 0.2520970106124878 + 10.0 * 9.049684524536133
Epoch 700, val loss: 0.4269166886806488
Epoch 710, training loss: 90.7393569946289 = 0.25006505846977234 + 10.0 * 9.048929214477539
Epoch 710, val loss: 0.42728954553604126
Epoch 720, training loss: 90.75126647949219 = 0.24804608523845673 + 10.0 * 9.050321578979492
Epoch 720, val loss: 0.4277729392051697
Epoch 730, training loss: 90.75645446777344 = 0.24603761732578278 + 10.0 * 9.051041603088379
Epoch 730, val loss: 0.4283445179462433
Epoch 740, training loss: 90.74409484863281 = 0.24405261874198914 + 10.0 * 9.050004959106445
Epoch 740, val loss: 0.428782194852829
Epoch 750, training loss: 90.72183990478516 = 0.24210414290428162 + 10.0 * 9.0479736328125
Epoch 750, val loss: 0.4292334318161011
Epoch 760, training loss: 90.70952606201172 = 0.24017196893692017 + 10.0 * 9.046935081481934
Epoch 760, val loss: 0.42979130148887634
Epoch 770, training loss: 90.70281982421875 = 0.23824703693389893 + 10.0 * 9.046457290649414
Epoch 770, val loss: 0.4304184317588806
Epoch 780, training loss: 90.73212432861328 = 0.23632843792438507 + 10.0 * 9.049579620361328
Epoch 780, val loss: 0.4310051202774048
Epoch 790, training loss: 90.72445678710938 = 0.23441097140312195 + 10.0 * 9.049004554748535
Epoch 790, val loss: 0.43190062046051025
Epoch 800, training loss: 90.68695068359375 = 0.23251381516456604 + 10.0 * 9.045443534851074
Epoch 800, val loss: 0.4325174391269684
Epoch 810, training loss: 90.68940734863281 = 0.23064136505126953 + 10.0 * 9.045876502990723
Epoch 810, val loss: 0.433162122964859
Epoch 820, training loss: 90.68711853027344 = 0.22877593338489532 + 10.0 * 9.0458345413208
Epoch 820, val loss: 0.433925986289978
Epoch 830, training loss: 90.67938232421875 = 0.22691361606121063 + 10.0 * 9.045247077941895
Epoch 830, val loss: 0.4349171817302704
Epoch 840, training loss: 90.67625427246094 = 0.22506357729434967 + 10.0 * 9.04511833190918
Epoch 840, val loss: 0.4357214868068695
Epoch 850, training loss: 90.69121551513672 = 0.2232232689857483 + 10.0 * 9.046799659729004
Epoch 850, val loss: 0.43677011132240295
Epoch 860, training loss: 90.65849304199219 = 0.2213943600654602 + 10.0 * 9.043709754943848
Epoch 860, val loss: 0.4373988211154938
Epoch 870, training loss: 90.64688110351562 = 0.21957775950431824 + 10.0 * 9.042730331420898
Epoch 870, val loss: 0.4384211599826813
Epoch 880, training loss: 90.6446533203125 = 0.21776972711086273 + 10.0 * 9.042688369750977
Epoch 880, val loss: 0.4393128454685211
Epoch 890, training loss: 90.6627426147461 = 0.21596291661262512 + 10.0 * 9.044677734375
Epoch 890, val loss: 0.44038042426109314
Epoch 900, training loss: 90.6291275024414 = 0.21416452527046204 + 10.0 * 9.041496276855469
Epoch 900, val loss: 0.441412091255188
Epoch 910, training loss: 90.62769317626953 = 0.21238158643245697 + 10.0 * 9.041531562805176
Epoch 910, val loss: 0.4424748122692108
Epoch 920, training loss: 90.62442016601562 = 0.21059945225715637 + 10.0 * 9.0413818359375
Epoch 920, val loss: 0.4437025487422943
Epoch 930, training loss: 90.63477325439453 = 0.20881785452365875 + 10.0 * 9.042595863342285
Epoch 930, val loss: 0.44486203789711
Epoch 940, training loss: 90.60865783691406 = 0.20704230666160583 + 10.0 * 9.040162086486816
Epoch 940, val loss: 0.445989727973938
Epoch 950, training loss: 90.6266860961914 = 0.20528270304203033 + 10.0 * 9.042140007019043
Epoch 950, val loss: 0.44719189405441284
Epoch 960, training loss: 90.61292266845703 = 0.20352241396903992 + 10.0 * 9.040940284729004
Epoch 960, val loss: 0.4483001232147217
Epoch 970, training loss: 90.60120391845703 = 0.20176726579666138 + 10.0 * 9.03994369506836
Epoch 970, val loss: 0.44980621337890625
Epoch 980, training loss: 90.59258270263672 = 0.20002007484436035 + 10.0 * 9.03925609588623
Epoch 980, val loss: 0.4510595202445984
Epoch 990, training loss: 90.58430480957031 = 0.19827355444431305 + 10.0 * 9.038602828979492
Epoch 990, val loss: 0.45242440700531006
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8437
Overall ASR: 0.6962
Flip ASR: 0.6197/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68795776367188 = 1.0971167087554932 + 10.0 * 10.359084129333496
Epoch 0, val loss: 1.0950521230697632
Epoch 10, training loss: 104.64208984375 = 1.0856480598449707 + 10.0 * 10.355644226074219
Epoch 10, val loss: 1.0836081504821777
Epoch 20, training loss: 104.06277465820312 = 1.071077823638916 + 10.0 * 10.299169540405273
Epoch 20, val loss: 1.0695997476577759
Epoch 30, training loss: 100.4232177734375 = 1.0609750747680664 + 10.0 * 9.936223983764648
Epoch 30, val loss: 1.0606380701065063
Epoch 40, training loss: 97.32887268066406 = 1.0507820844650269 + 10.0 * 9.627809524536133
Epoch 40, val loss: 1.0502510070800781
Epoch 50, training loss: 95.22975158691406 = 1.0310617685317993 + 10.0 * 9.419869422912598
Epoch 50, val loss: 1.0299712419509888
Epoch 60, training loss: 94.67676544189453 = 1.0072832107543945 + 10.0 * 9.366948127746582
Epoch 60, val loss: 1.0065830945968628
Epoch 70, training loss: 94.44781494140625 = 0.9856731295585632 + 10.0 * 9.346214294433594
Epoch 70, val loss: 0.9854906797409058
Epoch 80, training loss: 94.09603118896484 = 0.9664058685302734 + 10.0 * 9.312962532043457
Epoch 80, val loss: 0.9669788479804993
Epoch 90, training loss: 93.62823486328125 = 0.9483290910720825 + 10.0 * 9.267991065979004
Epoch 90, val loss: 0.9497132301330566
Epoch 100, training loss: 93.29873657226562 = 0.9264523983001709 + 10.0 * 9.237228393554688
Epoch 100, val loss: 0.9282772541046143
Epoch 110, training loss: 92.99974060058594 = 0.8960129618644714 + 10.0 * 9.210372924804688
Epoch 110, val loss: 0.8985942006111145
Epoch 120, training loss: 92.68474578857422 = 0.861537516117096 + 10.0 * 9.182321548461914
Epoch 120, val loss: 0.865630030632019
Epoch 130, training loss: 92.51095581054688 = 0.82514888048172 + 10.0 * 9.168581008911133
Epoch 130, val loss: 0.830842912197113
Epoch 140, training loss: 92.3637466430664 = 0.7849778532981873 + 10.0 * 9.157876968383789
Epoch 140, val loss: 0.792586088180542
Epoch 150, training loss: 92.24675750732422 = 0.7423142790794373 + 10.0 * 9.150444030761719
Epoch 150, val loss: 0.7518764734268188
Epoch 160, training loss: 92.11104583740234 = 0.6989697217941284 + 10.0 * 9.141207695007324
Epoch 160, val loss: 0.7113825678825378
Epoch 170, training loss: 91.98886108398438 = 0.6575141549110413 + 10.0 * 9.133134841918945
Epoch 170, val loss: 0.6730499863624573
Epoch 180, training loss: 91.88606262207031 = 0.6188129186630249 + 10.0 * 9.126725196838379
Epoch 180, val loss: 0.6377115845680237
Epoch 190, training loss: 91.78890228271484 = 0.5835405588150024 + 10.0 * 9.120535850524902
Epoch 190, val loss: 0.6060904860496521
Epoch 200, training loss: 91.70071411132812 = 0.5527318120002747 + 10.0 * 9.114797592163086
Epoch 200, val loss: 0.5790925621986389
Epoch 210, training loss: 91.6219253540039 = 0.5264380574226379 + 10.0 * 9.109548568725586
Epoch 210, val loss: 0.5566461086273193
Epoch 220, training loss: 91.55623626708984 = 0.5038710832595825 + 10.0 * 9.105237007141113
Epoch 220, val loss: 0.5380603671073914
Epoch 230, training loss: 91.50263977050781 = 0.4847141206264496 + 10.0 * 9.10179328918457
Epoch 230, val loss: 0.5227391123771667
Epoch 240, training loss: 91.44068145751953 = 0.46876055002212524 + 10.0 * 9.097192764282227
Epoch 240, val loss: 0.5103970170021057
Epoch 250, training loss: 91.38926696777344 = 0.45526358485221863 + 10.0 * 9.093400955200195
Epoch 250, val loss: 0.5003975033760071
Epoch 260, training loss: 91.42683410644531 = 0.44344809651374817 + 10.0 * 9.09833812713623
Epoch 260, val loss: 0.49186453223228455
Epoch 270, training loss: 91.30903625488281 = 0.4330308437347412 + 10.0 * 9.087600708007812
Epoch 270, val loss: 0.48436567187309265
Epoch 280, training loss: 91.27517700195312 = 0.4239676296710968 + 10.0 * 9.085121154785156
Epoch 280, val loss: 0.47810032963752747
Epoch 290, training loss: 91.24373626708984 = 0.4158666729927063 + 10.0 * 9.08278751373291
Epoch 290, val loss: 0.4725897014141083
Epoch 300, training loss: 91.21376037597656 = 0.40843358635902405 + 10.0 * 9.080533027648926
Epoch 300, val loss: 0.46758759021759033
Epoch 310, training loss: 91.21550750732422 = 0.40151676535606384 + 10.0 * 9.081398963928223
Epoch 310, val loss: 0.46290943026542664
Epoch 320, training loss: 91.17057037353516 = 0.39511778950691223 + 10.0 * 9.077545166015625
Epoch 320, val loss: 0.45854121446609497
Epoch 330, training loss: 91.13642120361328 = 0.38924217224121094 + 10.0 * 9.07471752166748
Epoch 330, val loss: 0.4547378122806549
Epoch 340, training loss: 91.11502838134766 = 0.3837071657180786 + 10.0 * 9.073132514953613
Epoch 340, val loss: 0.4510808289051056
Epoch 350, training loss: 91.09356689453125 = 0.3783924877643585 + 10.0 * 9.071516990661621
Epoch 350, val loss: 0.44754448533058167
Epoch 360, training loss: 91.07563781738281 = 0.37333986163139343 + 10.0 * 9.070230484008789
Epoch 360, val loss: 0.44420477747917175
Epoch 370, training loss: 91.05745697021484 = 0.3685336410999298 + 10.0 * 9.068892478942871
Epoch 370, val loss: 0.44114115834236145
Epoch 380, training loss: 91.04521942138672 = 0.3639022409915924 + 10.0 * 9.068132400512695
Epoch 380, val loss: 0.4379364550113678
Epoch 390, training loss: 91.02787017822266 = 0.35949891805648804 + 10.0 * 9.066837310791016
Epoch 390, val loss: 0.43510884046554565
Epoch 400, training loss: 91.00753784179688 = 0.35531437397003174 + 10.0 * 9.065221786499023
Epoch 400, val loss: 0.4324914216995239
Epoch 410, training loss: 90.99046325683594 = 0.3512631952762604 + 10.0 * 9.063920021057129
Epoch 410, val loss: 0.42992326617240906
Epoch 420, training loss: 90.99429321289062 = 0.3473040461540222 + 10.0 * 9.064699172973633
Epoch 420, val loss: 0.42742159962654114
Epoch 430, training loss: 90.98064422607422 = 0.34343791007995605 + 10.0 * 9.063720703125
Epoch 430, val loss: 0.42498087882995605
Epoch 440, training loss: 90.95664978027344 = 0.3397434651851654 + 10.0 * 9.061690330505371
Epoch 440, val loss: 0.42269009351730347
Epoch 450, training loss: 90.93924713134766 = 0.33617040514945984 + 10.0 * 9.060307502746582
Epoch 450, val loss: 0.4205467402935028
Epoch 460, training loss: 90.93006896972656 = 0.3326750695705414 + 10.0 * 9.05974006652832
Epoch 460, val loss: 0.4184083044528961
Epoch 470, training loss: 90.92186737060547 = 0.3292284309864044 + 10.0 * 9.059263229370117
Epoch 470, val loss: 0.4163995087146759
Epoch 480, training loss: 90.90459442138672 = 0.32591062784194946 + 10.0 * 9.057868003845215
Epoch 480, val loss: 0.4144291281700134
Epoch 490, training loss: 90.89249420166016 = 0.3227033317089081 + 10.0 * 9.056979179382324
Epoch 490, val loss: 0.4126560389995575
Epoch 500, training loss: 90.88125610351562 = 0.3195529282093048 + 10.0 * 9.056170463562012
Epoch 500, val loss: 0.41094112396240234
Epoch 510, training loss: 90.93936920166016 = 0.316438764333725 + 10.0 * 9.06229305267334
Epoch 510, val loss: 0.4094066321849823
Epoch 520, training loss: 90.8779067993164 = 0.3134053349494934 + 10.0 * 9.056449890136719
Epoch 520, val loss: 0.4077015221118927
Epoch 530, training loss: 90.85566711425781 = 0.31048840284347534 + 10.0 * 9.05451774597168
Epoch 530, val loss: 0.40633338689804077
Epoch 540, training loss: 90.84732055664062 = 0.30764520168304443 + 10.0 * 9.053967475891113
Epoch 540, val loss: 0.4050009250640869
Epoch 550, training loss: 90.83790588378906 = 0.30484023690223694 + 10.0 * 9.053306579589844
Epoch 550, val loss: 0.4037027955055237
Epoch 560, training loss: 90.82087707519531 = 0.3020782768726349 + 10.0 * 9.0518798828125
Epoch 560, val loss: 0.40253549814224243
Epoch 570, training loss: 90.85034942626953 = 0.29934942722320557 + 10.0 * 9.055100440979004
Epoch 570, val loss: 0.40159788727760315
Epoch 580, training loss: 90.80501556396484 = 0.29667478799819946 + 10.0 * 9.050833702087402
Epoch 580, val loss: 0.4003809094429016
Epoch 590, training loss: 90.79537963867188 = 0.29406440258026123 + 10.0 * 9.050130844116211
Epoch 590, val loss: 0.3994458019733429
Epoch 600, training loss: 90.82350158691406 = 0.2914925813674927 + 10.0 * 9.053200721740723
Epoch 600, val loss: 0.39862024784088135
Epoch 610, training loss: 90.78468322753906 = 0.28895843029022217 + 10.0 * 9.049572944641113
Epoch 610, val loss: 0.3977380096912384
Epoch 620, training loss: 90.76451873779297 = 0.286477655172348 + 10.0 * 9.04780387878418
Epoch 620, val loss: 0.3971123695373535
Epoch 630, training loss: 90.75631713867188 = 0.28402179479599 + 10.0 * 9.047229766845703
Epoch 630, val loss: 0.396481454372406
Epoch 640, training loss: 90.80065155029297 = 0.28158000111579895 + 10.0 * 9.051907539367676
Epoch 640, val loss: 0.3958839476108551
Epoch 650, training loss: 90.74750518798828 = 0.2791648209095001 + 10.0 * 9.046833992004395
Epoch 650, val loss: 0.3954463303089142
Epoch 660, training loss: 90.73114013671875 = 0.2768016755580902 + 10.0 * 9.04543399810791
Epoch 660, val loss: 0.3950909972190857
Epoch 670, training loss: 90.72310638427734 = 0.2744576930999756 + 10.0 * 9.044864654541016
Epoch 670, val loss: 0.3947291970252991
Epoch 680, training loss: 90.73201751708984 = 0.27212122082710266 + 10.0 * 9.045989990234375
Epoch 680, val loss: 0.3945314586162567
Epoch 690, training loss: 90.71875 = 0.2698269784450531 + 10.0 * 9.044892311096191
Epoch 690, val loss: 0.39420661330223083
Epoch 700, training loss: 90.71269989013672 = 0.2675670087337494 + 10.0 * 9.044512748718262
Epoch 700, val loss: 0.3940776288509369
Epoch 710, training loss: 90.69420623779297 = 0.2653149962425232 + 10.0 * 9.042889595031738
Epoch 710, val loss: 0.39392179250717163
Epoch 720, training loss: 90.68547058105469 = 0.2630728483200073 + 10.0 * 9.042240142822266
Epoch 720, val loss: 0.393828809261322
Epoch 730, training loss: 90.67987060546875 = 0.2608266770839691 + 10.0 * 9.04190444946289
Epoch 730, val loss: 0.3938469886779785
Epoch 740, training loss: 90.70612335205078 = 0.2585833966732025 + 10.0 * 9.044754028320312
Epoch 740, val loss: 0.39377403259277344
Epoch 750, training loss: 90.68872833251953 = 0.2563536465167999 + 10.0 * 9.043237686157227
Epoch 750, val loss: 0.3941161632537842
Epoch 760, training loss: 90.66357421875 = 0.25416427850723267 + 10.0 * 9.04094123840332
Epoch 760, val loss: 0.3942070007324219
Epoch 770, training loss: 90.650146484375 = 0.2519751191139221 + 10.0 * 9.039816856384277
Epoch 770, val loss: 0.394464910030365
Epoch 780, training loss: 90.64363098144531 = 0.24976696074008942 + 10.0 * 9.039386749267578
Epoch 780, val loss: 0.3947392702102661
Epoch 790, training loss: 90.68904876708984 = 0.24755072593688965 + 10.0 * 9.044149398803711
Epoch 790, val loss: 0.395112007856369
Epoch 800, training loss: 90.63766479492188 = 0.24534685909748077 + 10.0 * 9.03923225402832
Epoch 800, val loss: 0.39544326066970825
Epoch 810, training loss: 90.62471008300781 = 0.2431603968143463 + 10.0 * 9.038154602050781
Epoch 810, val loss: 0.39588427543640137
Epoch 820, training loss: 90.62029266357422 = 0.24097436666488647 + 10.0 * 9.037931442260742
Epoch 820, val loss: 0.39636462926864624
Epoch 830, training loss: 90.65559387207031 = 0.2387867122888565 + 10.0 * 9.041681289672852
Epoch 830, val loss: 0.3970824182033539
Epoch 840, training loss: 90.6261978149414 = 0.23661066591739655 + 10.0 * 9.038958549499512
Epoch 840, val loss: 0.3974354565143585
Epoch 850, training loss: 90.60108947753906 = 0.23446103930473328 + 10.0 * 9.036663055419922
Epoch 850, val loss: 0.39797455072402954
Epoch 860, training loss: 90.59747314453125 = 0.23231461644172668 + 10.0 * 9.036516189575195
Epoch 860, val loss: 0.3986736536026001
Epoch 870, training loss: 90.60157775878906 = 0.23017165064811707 + 10.0 * 9.037140846252441
Epoch 870, val loss: 0.39922675490379333
Epoch 880, training loss: 90.60331726074219 = 0.22803281247615814 + 10.0 * 9.037528038024902
Epoch 880, val loss: 0.4000084102153778
Epoch 890, training loss: 90.58970642089844 = 0.22592192888259888 + 10.0 * 9.036378860473633
Epoch 890, val loss: 0.4007366895675659
Epoch 900, training loss: 90.57522583007812 = 0.22382903099060059 + 10.0 * 9.035139083862305
Epoch 900, val loss: 0.40147435665130615
Epoch 910, training loss: 90.5683364868164 = 0.22173796594142914 + 10.0 * 9.034659385681152
Epoch 910, val loss: 0.40228214859962463
Epoch 920, training loss: 90.57789611816406 = 0.2196514904499054 + 10.0 * 9.0358247756958
Epoch 920, val loss: 0.4031793475151062
Epoch 930, training loss: 90.56333923339844 = 0.2175704687833786 + 10.0 * 9.034577369689941
Epoch 930, val loss: 0.4040369391441345
Epoch 940, training loss: 90.57460021972656 = 0.21550026535987854 + 10.0 * 9.035909652709961
Epoch 940, val loss: 0.40513089299201965
Epoch 950, training loss: 90.55638122558594 = 0.2134440392255783 + 10.0 * 9.034293174743652
Epoch 950, val loss: 0.40613606572151184
Epoch 960, training loss: 90.54362487792969 = 0.21140435338020325 + 10.0 * 9.033222198486328
Epoch 960, val loss: 0.4071808159351349
Epoch 970, training loss: 90.53761291503906 = 0.20935751497745514 + 10.0 * 9.032825469970703
Epoch 970, val loss: 0.40829983353614807
Epoch 980, training loss: 90.54810333251953 = 0.2073085606098175 + 10.0 * 9.034079551696777
Epoch 980, val loss: 0.40937697887420654
Epoch 990, training loss: 90.52947998046875 = 0.20526263117790222 + 10.0 * 9.032422065734863
Epoch 990, val loss: 0.41092321276664734
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7094
Flip ASR: 0.6377/1554 nodes
The final ASR:0.69371, 0.01399, Accuracy:0.84593, 0.00276
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97214])
remove edge: torch.Size([2, 79808])
updated graph: torch.Size([2, 88374])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.7003
Flip ASR: 0.6293/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7388
Flip ASR: 0.6763/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72143, 0.01595, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 10 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.69593811035156 = 1.1053168773651123 + 10.0 * 10.359062194824219
Epoch 0, val loss: 1.1040021181106567
Epoch 10, training loss: 104.63832092285156 = 1.091676115989685 + 10.0 * 10.35466480255127
Epoch 10, val loss: 1.090047001838684
Epoch 20, training loss: 103.98428344726562 = 1.0740147829055786 + 10.0 * 10.291027069091797
Epoch 20, val loss: 1.0723286867141724
Epoch 30, training loss: 100.33786010742188 = 1.0591912269592285 + 10.0 * 9.92786693572998
Epoch 30, val loss: 1.0575060844421387
Epoch 40, training loss: 97.40770721435547 = 1.0445141792297363 + 10.0 * 9.636319160461426
Epoch 40, val loss: 1.0426113605499268
Epoch 50, training loss: 96.19229125976562 = 1.0258772373199463 + 10.0 * 9.516641616821289
Epoch 50, val loss: 1.0237083435058594
Epoch 60, training loss: 95.5701675415039 = 1.007773995399475 + 10.0 * 9.456239700317383
Epoch 60, val loss: 1.0060986280441284
Epoch 70, training loss: 94.9559555053711 = 0.9901677370071411 + 10.0 * 9.396578788757324
Epoch 70, val loss: 0.9887290000915527
Epoch 80, training loss: 94.58157348632812 = 0.9686923027038574 + 10.0 * 9.361288070678711
Epoch 80, val loss: 0.9673744440078735
Epoch 90, training loss: 94.32061767578125 = 0.9458897113800049 + 10.0 * 9.337472915649414
Epoch 90, val loss: 0.9450539946556091
Epoch 100, training loss: 94.06161499023438 = 0.9238429665565491 + 10.0 * 9.313776969909668
Epoch 100, val loss: 0.9234419465065002
Epoch 110, training loss: 93.60792541503906 = 0.8996751308441162 + 10.0 * 9.270825386047363
Epoch 110, val loss: 0.8996882438659668
Epoch 120, training loss: 93.14518737792969 = 0.8724249601364136 + 10.0 * 9.227276802062988
Epoch 120, val loss: 0.8729221224784851
Epoch 130, training loss: 92.8748779296875 = 0.8401643633842468 + 10.0 * 9.203471183776855
Epoch 130, val loss: 0.8414896130561829
Epoch 140, training loss: 92.68450164794922 = 0.8042781949043274 + 10.0 * 9.18802261352539
Epoch 140, val loss: 0.8071722388267517
Epoch 150, training loss: 92.52765655517578 = 0.7668313980102539 + 10.0 * 9.176082611083984
Epoch 150, val loss: 0.7718707919120789
Epoch 160, training loss: 92.39948272705078 = 0.7278185486793518 + 10.0 * 9.167165756225586
Epoch 160, val loss: 0.734904944896698
Epoch 170, training loss: 92.29845428466797 = 0.6876696944236755 + 10.0 * 9.161078453063965
Epoch 170, val loss: 0.6976450085639954
Epoch 180, training loss: 92.21864318847656 = 0.6482904553413391 + 10.0 * 9.157034873962402
Epoch 180, val loss: 0.6617955565452576
Epoch 190, training loss: 92.10963439941406 = 0.6121379733085632 + 10.0 * 9.149749755859375
Epoch 190, val loss: 0.6291367411613464
Epoch 200, training loss: 91.99853515625 = 0.5794677138328552 + 10.0 * 9.14190673828125
Epoch 200, val loss: 0.6003278493881226
Epoch 210, training loss: 91.92250061035156 = 0.5504260063171387 + 10.0 * 9.137207984924316
Epoch 210, val loss: 0.5751024484634399
Epoch 220, training loss: 91.80493927001953 = 0.5245060324668884 + 10.0 * 9.128043174743652
Epoch 220, val loss: 0.5530774593353271
Epoch 230, training loss: 91.74415588378906 = 0.5012415051460266 + 10.0 * 9.12429141998291
Epoch 230, val loss: 0.533819317817688
Epoch 240, training loss: 91.67185974121094 = 0.48091334104537964 + 10.0 * 9.119094848632812
Epoch 240, val loss: 0.5172137022018433
Epoch 250, training loss: 91.6144027709961 = 0.46319350600242615 + 10.0 * 9.115120887756348
Epoch 250, val loss: 0.5032906532287598
Epoch 260, training loss: 91.56568145751953 = 0.44770556688308716 + 10.0 * 9.111797332763672
Epoch 260, val loss: 0.4914323389530182
Epoch 270, training loss: 91.533447265625 = 0.4340380132198334 + 10.0 * 9.109941482543945
Epoch 270, val loss: 0.4813481271266937
Epoch 280, training loss: 91.47283935546875 = 0.42205822467803955 + 10.0 * 9.105077743530273
Epoch 280, val loss: 0.4728538691997528
Epoch 290, training loss: 91.44300079345703 = 0.41150563955307007 + 10.0 * 9.1031494140625
Epoch 290, val loss: 0.465635746717453
Epoch 300, training loss: 91.38557434082031 = 0.4020863473415375 + 10.0 * 9.098348617553711
Epoch 300, val loss: 0.45930716395378113
Epoch 310, training loss: 91.33458709716797 = 0.39353781938552856 + 10.0 * 9.094104766845703
Epoch 310, val loss: 0.45368415117263794
Epoch 320, training loss: 91.29328155517578 = 0.3857049345970154 + 10.0 * 9.090757369995117
Epoch 320, val loss: 0.44858410954475403
Epoch 330, training loss: 91.26714324951172 = 0.37840399146080017 + 10.0 * 9.088873863220215
Epoch 330, val loss: 0.4439221918582916
Epoch 340, training loss: 91.23455047607422 = 0.3718310594558716 + 10.0 * 9.086271286010742
Epoch 340, val loss: 0.4398590326309204
Epoch 350, training loss: 91.18868255615234 = 0.36589550971984863 + 10.0 * 9.08227825164795
Epoch 350, val loss: 0.4361208975315094
Epoch 360, training loss: 91.15126037597656 = 0.36028367280960083 + 10.0 * 9.079097747802734
Epoch 360, val loss: 0.4326786398887634
Epoch 370, training loss: 91.17451477050781 = 0.35490667819976807 + 10.0 * 9.081960678100586
Epoch 370, val loss: 0.429463267326355
Epoch 380, training loss: 91.11199188232422 = 0.34980303049087524 + 10.0 * 9.07621955871582
Epoch 380, val loss: 0.4264136850833893
Epoch 390, training loss: 91.08003234863281 = 0.34503525495529175 + 10.0 * 9.07349967956543
Epoch 390, val loss: 0.42359355092048645
Epoch 400, training loss: 91.06788635253906 = 0.34051525592803955 + 10.0 * 9.072736740112305
Epoch 400, val loss: 0.42096927762031555
Epoch 410, training loss: 91.03325653076172 = 0.3362289369106293 + 10.0 * 9.069703102111816
Epoch 410, val loss: 0.41849327087402344
Epoch 420, training loss: 91.00849914550781 = 0.33214718103408813 + 10.0 * 9.067635536193848
Epoch 420, val loss: 0.41614723205566406
Epoch 430, training loss: 90.99781036376953 = 0.3282216787338257 + 10.0 * 9.0669584274292
Epoch 430, val loss: 0.4139658808708191
Epoch 440, training loss: 90.99217224121094 = 0.3244296610355377 + 10.0 * 9.066774368286133
Epoch 440, val loss: 0.41194838285446167
Epoch 450, training loss: 90.96533203125 = 0.32084041833877563 + 10.0 * 9.064449310302734
Epoch 450, val loss: 0.4099864959716797
Epoch 460, training loss: 90.94764709472656 = 0.3174313008785248 + 10.0 * 9.063021659851074
Epoch 460, val loss: 0.40813207626342773
Epoch 470, training loss: 90.9455795288086 = 0.31415387988090515 + 10.0 * 9.063142776489258
Epoch 470, val loss: 0.40640702843666077
Epoch 480, training loss: 90.92404174804688 = 0.310989111661911 + 10.0 * 9.061305046081543
Epoch 480, val loss: 0.4049292206764221
Epoch 490, training loss: 90.90308380126953 = 0.30795568227767944 + 10.0 * 9.059513092041016
Epoch 490, val loss: 0.4033174514770508
Epoch 500, training loss: 90.90713500976562 = 0.3050185441970825 + 10.0 * 9.060212135314941
Epoch 500, val loss: 0.40195775032043457
Epoch 510, training loss: 90.91535186767578 = 0.3021639287471771 + 10.0 * 9.061319351196289
Epoch 510, val loss: 0.4007010757923126
Epoch 520, training loss: 90.87078094482422 = 0.2994425594806671 + 10.0 * 9.057133674621582
Epoch 520, val loss: 0.39942052960395813
Epoch 530, training loss: 90.85285186767578 = 0.2968205213546753 + 10.0 * 9.05560302734375
Epoch 530, val loss: 0.3983113169670105
Epoch 540, training loss: 90.84229278564453 = 0.29426056146621704 + 10.0 * 9.054803848266602
Epoch 540, val loss: 0.3972844183444977
Epoch 550, training loss: 90.8633804321289 = 0.2917485237121582 + 10.0 * 9.05716323852539
Epoch 550, val loss: 0.3962479829788208
Epoch 560, training loss: 90.83537292480469 = 0.28929978609085083 + 10.0 * 9.054607391357422
Epoch 560, val loss: 0.39541760087013245
Epoch 570, training loss: 90.81993103027344 = 0.286945104598999 + 10.0 * 9.053298950195312
Epoch 570, val loss: 0.39460235834121704
Epoch 580, training loss: 90.8229751586914 = 0.28465965390205383 + 10.0 * 9.053831100463867
Epoch 580, val loss: 0.3939346969127655
Epoch 590, training loss: 90.79908752441406 = 0.2824271023273468 + 10.0 * 9.051666259765625
Epoch 590, val loss: 0.39320287108421326
Epoch 600, training loss: 90.78507995605469 = 0.2802383005619049 + 10.0 * 9.050484657287598
Epoch 600, val loss: 0.3927004039287567
Epoch 610, training loss: 90.84793853759766 = 0.27808842062950134 + 10.0 * 9.056984901428223
Epoch 610, val loss: 0.39239221811294556
Epoch 620, training loss: 90.77635955810547 = 0.2759874761104584 + 10.0 * 9.050037384033203
Epoch 620, val loss: 0.39183440804481506
Epoch 630, training loss: 90.76200866699219 = 0.27395686507225037 + 10.0 * 9.048805236816406
Epoch 630, val loss: 0.39142537117004395
Epoch 640, training loss: 90.75475311279297 = 0.27195894718170166 + 10.0 * 9.048279762268066
Epoch 640, val loss: 0.39108461141586304
Epoch 650, training loss: 90.79228973388672 = 0.26998665928840637 + 10.0 * 9.052229881286621
Epoch 650, val loss: 0.3907596468925476
Epoch 660, training loss: 90.74491119384766 = 0.26803046464920044 + 10.0 * 9.047688484191895
Epoch 660, val loss: 0.3906857967376709
Epoch 670, training loss: 90.74525451660156 = 0.26612427830696106 + 10.0 * 9.047913551330566
Epoch 670, val loss: 0.3905751705169678
Epoch 680, training loss: 90.72550964355469 = 0.2642475664615631 + 10.0 * 9.046126365661621
Epoch 680, val loss: 0.3904995620250702
Epoch 690, training loss: 90.72076416015625 = 0.2624017596244812 + 10.0 * 9.045835494995117
Epoch 690, val loss: 0.39043283462524414
Epoch 700, training loss: 90.7786865234375 = 0.2605717182159424 + 10.0 * 9.051811218261719
Epoch 700, val loss: 0.3903979957103729
Epoch 710, training loss: 90.7147216796875 = 0.2587617039680481 + 10.0 * 9.0455961227417
Epoch 710, val loss: 0.390376478433609
Epoch 720, training loss: 90.70003509521484 = 0.2569975256919861 + 10.0 * 9.044303894042969
Epoch 720, val loss: 0.3903513252735138
Epoch 730, training loss: 90.69029235839844 = 0.25525781512260437 + 10.0 * 9.043503761291504
Epoch 730, val loss: 0.3903849124908447
Epoch 740, training loss: 90.68321990966797 = 0.2535199522972107 + 10.0 * 9.042970657348633
Epoch 740, val loss: 0.3905036449432373
Epoch 750, training loss: 90.68272399902344 = 0.2517867088317871 + 10.0 * 9.04309368133545
Epoch 750, val loss: 0.39063629508018494
Epoch 760, training loss: 90.67948150634766 = 0.2500613331794739 + 10.0 * 9.04294204711914
Epoch 760, val loss: 0.39082610607147217
Epoch 770, training loss: 90.68033599853516 = 0.24836643040180206 + 10.0 * 9.043196678161621
Epoch 770, val loss: 0.3909805715084076
Epoch 780, training loss: 90.6630630493164 = 0.2466871440410614 + 10.0 * 9.041637420654297
Epoch 780, val loss: 0.3911466598510742
Epoch 790, training loss: 90.6570816040039 = 0.2450144737958908 + 10.0 * 9.041206359863281
Epoch 790, val loss: 0.3914569914340973
Epoch 800, training loss: 90.66331481933594 = 0.24334882199764252 + 10.0 * 9.041996955871582
Epoch 800, val loss: 0.391838401556015
Epoch 810, training loss: 90.65616607666016 = 0.24169674515724182 + 10.0 * 9.041446685791016
Epoch 810, val loss: 0.39206528663635254
Epoch 820, training loss: 90.6438980102539 = 0.24006463587284088 + 10.0 * 9.040383338928223
Epoch 820, val loss: 0.39234450459480286
Epoch 830, training loss: 90.66800689697266 = 0.23843978345394135 + 10.0 * 9.042956352233887
Epoch 830, val loss: 0.39274075627326965
Epoch 840, training loss: 90.63277435302734 = 0.23681843280792236 + 10.0 * 9.039595603942871
Epoch 840, val loss: 0.39319565892219543
Epoch 850, training loss: 90.62065124511719 = 0.23520104587078094 + 10.0 * 9.038545608520508
Epoch 850, val loss: 0.39361241459846497
Epoch 860, training loss: 90.61851501464844 = 0.23358263075351715 + 10.0 * 9.038493156433105
Epoch 860, val loss: 0.39408791065216064
Epoch 870, training loss: 90.64501953125 = 0.23195938766002655 + 10.0 * 9.041306495666504
Epoch 870, val loss: 0.39461550116539
Epoch 880, training loss: 90.63225555419922 = 0.2303478717803955 + 10.0 * 9.040190696716309
Epoch 880, val loss: 0.39505520462989807
Epoch 890, training loss: 90.60720825195312 = 0.22875267267227173 + 10.0 * 9.037845611572266
Epoch 890, val loss: 0.3956449627876282
Epoch 900, training loss: 90.59854888916016 = 0.227150097489357 + 10.0 * 9.037139892578125
Epoch 900, val loss: 0.3961010277271271
Epoch 910, training loss: 90.59081268310547 = 0.22554676234722137 + 10.0 * 9.036526679992676
Epoch 910, val loss: 0.39671310782432556
Epoch 920, training loss: 90.622314453125 = 0.2239428460597992 + 10.0 * 9.039836883544922
Epoch 920, val loss: 0.3973079323768616
Epoch 930, training loss: 90.6078109741211 = 0.22233569622039795 + 10.0 * 9.03854751586914
Epoch 930, val loss: 0.39809754490852356
Epoch 940, training loss: 90.57963562011719 = 0.2207387387752533 + 10.0 * 9.035889625549316
Epoch 940, val loss: 0.39870887994766235
Epoch 950, training loss: 90.57166290283203 = 0.21914111077785492 + 10.0 * 9.035252571105957
Epoch 950, val loss: 0.39943844079971313
Epoch 960, training loss: 90.56986236572266 = 0.217537060379982 + 10.0 * 9.035232543945312
Epoch 960, val loss: 0.40019434690475464
Epoch 970, training loss: 90.59757995605469 = 0.2159307897090912 + 10.0 * 9.038165092468262
Epoch 970, val loss: 0.40103772282600403
Epoch 980, training loss: 90.57381439208984 = 0.21432851254940033 + 10.0 * 9.035948753356934
Epoch 980, val loss: 0.4018055200576782
Epoch 990, training loss: 90.55901336669922 = 0.2127334624528885 + 10.0 * 9.034627914428711
Epoch 990, val loss: 0.4026075005531311
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8524
Overall ASR: 0.7054
Flip ASR: 0.6326/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68496704101562 = 1.0943593978881836 + 10.0 * 10.359060287475586
Epoch 0, val loss: 1.0921318531036377
Epoch 10, training loss: 104.6347427368164 = 1.0838027000427246 + 10.0 * 10.355093955993652
Epoch 10, val loss: 1.0815956592559814
Epoch 20, training loss: 103.92578125 = 1.070093035697937 + 10.0 * 10.285569190979004
Epoch 20, val loss: 1.0682846307754517
Epoch 30, training loss: 99.37543487548828 = 1.0575048923492432 + 10.0 * 9.831792831420898
Epoch 30, val loss: 1.056361198425293
Epoch 40, training loss: 96.52388763427734 = 1.0448533296585083 + 10.0 * 9.547903060913086
Epoch 40, val loss: 1.0435705184936523
Epoch 50, training loss: 95.5267562866211 = 1.0279330015182495 + 10.0 * 9.449882507324219
Epoch 50, val loss: 1.0264465808868408
Epoch 60, training loss: 94.75403594970703 = 1.0093131065368652 + 10.0 * 9.374471664428711
Epoch 60, val loss: 1.0083239078521729
Epoch 70, training loss: 94.38662719726562 = 0.9896435141563416 + 10.0 * 9.33969783782959
Epoch 70, val loss: 0.9890941977500916
Epoch 80, training loss: 93.89725494384766 = 0.968940258026123 + 10.0 * 9.292831420898438
Epoch 80, val loss: 0.9687543511390686
Epoch 90, training loss: 93.31501007080078 = 0.947751522064209 + 10.0 * 9.236725807189941
Epoch 90, val loss: 0.9477332830429077
Epoch 100, training loss: 93.00120544433594 = 0.9208687543869019 + 10.0 * 9.208033561706543
Epoch 100, val loss: 0.9207801818847656
Epoch 110, training loss: 92.84000396728516 = 0.8851130604743958 + 10.0 * 9.195488929748535
Epoch 110, val loss: 0.885796844959259
Epoch 120, training loss: 92.6894760131836 = 0.8443934321403503 + 10.0 * 9.184508323669434
Epoch 120, val loss: 0.8467860221862793
Epoch 130, training loss: 92.50857543945312 = 0.802920937538147 + 10.0 * 9.170565605163574
Epoch 130, val loss: 0.8074713349342346
Epoch 140, training loss: 92.3163070678711 = 0.7612214684486389 + 10.0 * 9.155508041381836
Epoch 140, val loss: 0.7683857679367065
Epoch 150, training loss: 92.17035675048828 = 0.7179146409034729 + 10.0 * 9.145243644714355
Epoch 150, val loss: 0.7281113266944885
Epoch 160, training loss: 92.0494613647461 = 0.67316734790802 + 10.0 * 9.137629508972168
Epoch 160, val loss: 0.687192976474762
Epoch 170, training loss: 91.9371566772461 = 0.6297963261604309 + 10.0 * 9.130735397338867
Epoch 170, val loss: 0.648315966129303
Epoch 180, training loss: 91.83082580566406 = 0.5904009938240051 + 10.0 * 9.124042510986328
Epoch 180, val loss: 0.6135575771331787
Epoch 190, training loss: 91.73779296875 = 0.5555531978607178 + 10.0 * 9.118224143981934
Epoch 190, val loss: 0.5834420323371887
Epoch 200, training loss: 91.6504135131836 = 0.5252238512039185 + 10.0 * 9.112519264221191
Epoch 200, val loss: 0.557903528213501
Epoch 210, training loss: 91.61947631835938 = 0.4995599389076233 + 10.0 * 9.111991882324219
Epoch 210, val loss: 0.5366568565368652
Epoch 220, training loss: 91.50059509277344 = 0.4780553877353668 + 10.0 * 9.102253913879395
Epoch 220, val loss: 0.519702672958374
Epoch 230, training loss: 91.43694305419922 = 0.460164874792099 + 10.0 * 9.097677230834961
Epoch 230, val loss: 0.5058887600898743
Epoch 240, training loss: 91.44183349609375 = 0.4449855089187622 + 10.0 * 9.099684715270996
Epoch 240, val loss: 0.4946352541446686
Epoch 250, training loss: 91.3399887084961 = 0.4319435954093933 + 10.0 * 9.090804100036621
Epoch 250, val loss: 0.4854835867881775
Epoch 260, training loss: 91.29731750488281 = 0.4207632541656494 + 10.0 * 9.087656021118164
Epoch 260, val loss: 0.47771787643432617
Epoch 270, training loss: 91.2528305053711 = 0.41094377636909485 + 10.0 * 9.084188461303711
Epoch 270, val loss: 0.47122225165367126
Epoch 280, training loss: 91.21919250488281 = 0.40215206146240234 + 10.0 * 9.081704139709473
Epoch 280, val loss: 0.4655506908893585
Epoch 290, training loss: 91.1861343383789 = 0.3942015469074249 + 10.0 * 9.079193115234375
Epoch 290, val loss: 0.4604398310184479
Epoch 300, training loss: 91.15853118896484 = 0.3869037330150604 + 10.0 * 9.077162742614746
Epoch 300, val loss: 0.45594921708106995
Epoch 310, training loss: 91.16022491455078 = 0.38012123107910156 + 10.0 * 9.078010559082031
Epoch 310, val loss: 0.4517129361629486
Epoch 320, training loss: 91.11643981933594 = 0.37379899621009827 + 10.0 * 9.074263572692871
Epoch 320, val loss: 0.4478148818016052
Epoch 330, training loss: 91.09733581542969 = 0.3679482340812683 + 10.0 * 9.072938919067383
Epoch 330, val loss: 0.4442090094089508
Epoch 340, training loss: 91.07038116455078 = 0.36248838901519775 + 10.0 * 9.070789337158203
Epoch 340, val loss: 0.4409564435482025
Epoch 350, training loss: 91.05014038085938 = 0.35731521248817444 + 10.0 * 9.069282531738281
Epoch 350, val loss: 0.4378209710121155
Epoch 360, training loss: 91.03064727783203 = 0.3524062931537628 + 10.0 * 9.06782341003418
Epoch 360, val loss: 0.4348646104335785
Epoch 370, training loss: 91.01448822021484 = 0.34772124886512756 + 10.0 * 9.066676139831543
Epoch 370, val loss: 0.43199631571769714
Epoch 380, training loss: 90.99415588378906 = 0.34322330355644226 + 10.0 * 9.065093040466309
Epoch 380, val loss: 0.4293055832386017
Epoch 390, training loss: 90.97438049316406 = 0.3389383852481842 + 10.0 * 9.063544273376465
Epoch 390, val loss: 0.4268874526023865
Epoch 400, training loss: 90.96012878417969 = 0.33487191796302795 + 10.0 * 9.062525749206543
Epoch 400, val loss: 0.4244999885559082
Epoch 410, training loss: 90.94025421142578 = 0.3309645652770996 + 10.0 * 9.060929298400879
Epoch 410, val loss: 0.42236432433128357
Epoch 420, training loss: 90.92765808105469 = 0.3272130787372589 + 10.0 * 9.06004524230957
Epoch 420, val loss: 0.4202421009540558
Epoch 430, training loss: 90.91142272949219 = 0.32360976934432983 + 10.0 * 9.058781623840332
Epoch 430, val loss: 0.4183431565761566
Epoch 440, training loss: 90.9134521484375 = 0.3201560378074646 + 10.0 * 9.059329986572266
Epoch 440, val loss: 0.41662681102752686
Epoch 450, training loss: 90.88368225097656 = 0.3168452978134155 + 10.0 * 9.056683540344238
Epoch 450, val loss: 0.4150846004486084
Epoch 460, training loss: 90.87342834472656 = 0.3136560320854187 + 10.0 * 9.055976867675781
Epoch 460, val loss: 0.41361385583877563
Epoch 470, training loss: 90.88871765136719 = 0.3105579912662506 + 10.0 * 9.057816505432129
Epoch 470, val loss: 0.41237252950668335
Epoch 480, training loss: 90.86608123779297 = 0.3075658977031708 + 10.0 * 9.055851936340332
Epoch 480, val loss: 0.41099417209625244
Epoch 490, training loss: 90.8399658203125 = 0.30466896295547485 + 10.0 * 9.053529739379883
Epoch 490, val loss: 0.40978121757507324
Epoch 500, training loss: 90.8651351928711 = 0.3018670380115509 + 10.0 * 9.056326866149902
Epoch 500, val loss: 0.40883249044418335
Epoch 510, training loss: 90.82271575927734 = 0.2991289794445038 + 10.0 * 9.052358627319336
Epoch 510, val loss: 0.4079596698284149
Epoch 520, training loss: 90.8115463256836 = 0.2964865565299988 + 10.0 * 9.051506042480469
Epoch 520, val loss: 0.4070224165916443
Epoch 530, training loss: 90.79812622070312 = 0.2939101755619049 + 10.0 * 9.050421714782715
Epoch 530, val loss: 0.40623989701271057
Epoch 540, training loss: 90.84405517578125 = 0.29137322306632996 + 10.0 * 9.055268287658691
Epoch 540, val loss: 0.40551650524139404
Epoch 550, training loss: 90.78321075439453 = 0.2888866364955902 + 10.0 * 9.049432754516602
Epoch 550, val loss: 0.40498417615890503
Epoch 560, training loss: 90.76698303222656 = 0.2864714562892914 + 10.0 * 9.048051834106445
Epoch 560, val loss: 0.4044065475463867
Epoch 570, training loss: 90.76339721679688 = 0.2841121554374695 + 10.0 * 9.047928810119629
Epoch 570, val loss: 0.40395069122314453
Epoch 580, training loss: 90.76197052001953 = 0.2817826271057129 + 10.0 * 9.048018455505371
Epoch 580, val loss: 0.40360966324806213
Epoch 590, training loss: 90.7557601928711 = 0.27949008345603943 + 10.0 * 9.047627449035645
Epoch 590, val loss: 0.40326741337776184
Epoch 600, training loss: 90.73286437988281 = 0.2772524356842041 + 10.0 * 9.045560836791992
Epoch 600, val loss: 0.4029603600502014
Epoch 610, training loss: 90.72289276123047 = 0.275058388710022 + 10.0 * 9.044783592224121
Epoch 610, val loss: 0.4028339385986328
Epoch 620, training loss: 90.71005249023438 = 0.27289921045303345 + 10.0 * 9.043715476989746
Epoch 620, val loss: 0.4027366638183594
Epoch 630, training loss: 90.73377227783203 = 0.27075856924057007 + 10.0 * 9.046300888061523
Epoch 630, val loss: 0.4027857482433319
Epoch 640, training loss: 90.716552734375 = 0.2686313986778259 + 10.0 * 9.044792175292969
Epoch 640, val loss: 0.4026840925216675
Epoch 650, training loss: 90.68597412109375 = 0.2665501832962036 + 10.0 * 9.041942596435547
Epoch 650, val loss: 0.4027252793312073
Epoch 660, training loss: 90.68093872070312 = 0.26449495553970337 + 10.0 * 9.041644096374512
Epoch 660, val loss: 0.4028678834438324
Epoch 670, training loss: 90.67711639404297 = 0.26245152950286865 + 10.0 * 9.04146671295166
Epoch 670, val loss: 0.40306228399276733
Epoch 680, training loss: 90.66950988769531 = 0.26043766736984253 + 10.0 * 9.04090690612793
Epoch 680, val loss: 0.40322619676589966
Epoch 690, training loss: 90.65640258789062 = 0.2584518790245056 + 10.0 * 9.039794921875
Epoch 690, val loss: 0.40348580479621887
Epoch 700, training loss: 90.66669464111328 = 0.25648075342178345 + 10.0 * 9.041021347045898
Epoch 700, val loss: 0.40375861525535583
Epoch 710, training loss: 90.67622375488281 = 0.2545176148414612 + 10.0 * 9.042170524597168
Epoch 710, val loss: 0.40423136949539185
Epoch 720, training loss: 90.64176940917969 = 0.25258857011795044 + 10.0 * 9.038918495178223
Epoch 720, val loss: 0.40450409054756165
Epoch 730, training loss: 90.6307144165039 = 0.25068655610084534 + 10.0 * 9.038002967834473
Epoch 730, val loss: 0.4048754870891571
Epoch 740, training loss: 90.63587951660156 = 0.2487928569316864 + 10.0 * 9.038708686828613
Epoch 740, val loss: 0.4053356945514679
Epoch 750, training loss: 90.61414337158203 = 0.24690327048301697 + 10.0 * 9.036724090576172
Epoch 750, val loss: 0.40586650371551514
Epoch 760, training loss: 90.61188507080078 = 0.24503257870674133 + 10.0 * 9.0366849899292
Epoch 760, val loss: 0.4063280522823334
Epoch 770, training loss: 90.6241226196289 = 0.24317660927772522 + 10.0 * 9.038094520568848
Epoch 770, val loss: 0.40690985321998596
Epoch 780, training loss: 90.60669708251953 = 0.24132023751735687 + 10.0 * 9.036538124084473
Epoch 780, val loss: 0.4074825942516327
Epoch 790, training loss: 90.60354614257812 = 0.23947764933109283 + 10.0 * 9.036406517028809
Epoch 790, val loss: 0.4080032408237457
Epoch 800, training loss: 90.59481048583984 = 0.23764640092849731 + 10.0 * 9.03571605682373
Epoch 800, val loss: 0.4087403416633606
Epoch 810, training loss: 90.58473205566406 = 0.2358258068561554 + 10.0 * 9.034891128540039
Epoch 810, val loss: 0.40935981273651123
Epoch 820, training loss: 90.57623291015625 = 0.23401732742786407 + 10.0 * 9.034221649169922
Epoch 820, val loss: 0.4101465344429016
Epoch 830, training loss: 90.58396911621094 = 0.23221467435359955 + 10.0 * 9.035175323486328
Epoch 830, val loss: 0.4108242690563202
Epoch 840, training loss: 90.56873321533203 = 0.23041248321533203 + 10.0 * 9.033831596374512
Epoch 840, val loss: 0.41178765892982483
Epoch 850, training loss: 90.568115234375 = 0.22861427068710327 + 10.0 * 9.033949851989746
Epoch 850, val loss: 0.41242972016334534
Epoch 860, training loss: 90.55757141113281 = 0.22682221233844757 + 10.0 * 9.033075332641602
Epoch 860, val loss: 0.41328176856040955
Epoch 870, training loss: 90.57814025878906 = 0.22503575682640076 + 10.0 * 9.035310745239258
Epoch 870, val loss: 0.414203405380249
Epoch 880, training loss: 90.55487060546875 = 0.223251074552536 + 10.0 * 9.033162117004395
Epoch 880, val loss: 0.41507869958877563
Epoch 890, training loss: 90.54144287109375 = 0.22147448360919952 + 10.0 * 9.031996726989746
Epoch 890, val loss: 0.4160507321357727
Epoch 900, training loss: 90.53707122802734 = 0.21969857811927795 + 10.0 * 9.031737327575684
Epoch 900, val loss: 0.41702765226364136
Epoch 910, training loss: 90.55472564697266 = 0.21792209148406982 + 10.0 * 9.03368091583252
Epoch 910, val loss: 0.41811251640319824
Epoch 920, training loss: 90.5406494140625 = 0.21612925827503204 + 10.0 * 9.032452583312988
Epoch 920, val loss: 0.4190002679824829
Epoch 930, training loss: 90.52921295166016 = 0.21434755623340607 + 10.0 * 9.031486511230469
Epoch 930, val loss: 0.42005228996276855
Epoch 940, training loss: 90.53096771240234 = 0.21256205439567566 + 10.0 * 9.031840324401855
Epoch 940, val loss: 0.42110735177993774
Epoch 950, training loss: 90.5185317993164 = 0.2107791155576706 + 10.0 * 9.03077507019043
Epoch 950, val loss: 0.42229509353637695
Epoch 960, training loss: 90.51301574707031 = 0.20899054408073425 + 10.0 * 9.030402183532715
Epoch 960, val loss: 0.42337775230407715
Epoch 970, training loss: 90.5175552368164 = 0.2072049081325531 + 10.0 * 9.031034469604492
Epoch 970, val loss: 0.42449867725372314
Epoch 980, training loss: 90.50470733642578 = 0.20541752874851227 + 10.0 * 9.029929161071777
Epoch 980, val loss: 0.4257461130619049
Epoch 990, training loss: 90.53300476074219 = 0.20363077521324158 + 10.0 * 9.032938003540039
Epoch 990, val loss: 0.42704668641090393
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8442
Overall ASR: 0.6760
Flip ASR: 0.5952/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68712615966797 = 1.096150517463684 + 10.0 * 10.359097480773926
Epoch 0, val loss: 1.0953618288040161
Epoch 10, training loss: 104.64118194580078 = 1.084394931793213 + 10.0 * 10.35567855834961
Epoch 10, val loss: 1.0832594633102417
Epoch 20, training loss: 104.07833862304688 = 1.0688570737838745 + 10.0 * 10.300948143005371
Epoch 20, val loss: 1.067520022392273
Epoch 30, training loss: 100.77192687988281 = 1.0563784837722778 + 10.0 * 9.97155475616455
Epoch 30, val loss: 1.0553123950958252
Epoch 40, training loss: 97.54006958007812 = 1.0438271760940552 + 10.0 * 9.64962387084961
Epoch 40, val loss: 1.0424610376358032
Epoch 50, training loss: 95.91242980957031 = 1.0239747762680054 + 10.0 * 9.488845825195312
Epoch 50, val loss: 1.0220615863800049
Epoch 60, training loss: 94.87557220458984 = 1.0013344287872314 + 10.0 * 9.387423515319824
Epoch 60, val loss: 0.9999116659164429
Epoch 70, training loss: 94.62603759765625 = 0.9795211553573608 + 10.0 * 9.364651679992676
Epoch 70, val loss: 0.9783957600593567
Epoch 80, training loss: 94.34854888916016 = 0.95595782995224 + 10.0 * 9.339259147644043
Epoch 80, val loss: 0.9549789428710938
Epoch 90, training loss: 94.09093475341797 = 0.9310551881790161 + 10.0 * 9.315988540649414
Epoch 90, val loss: 0.9302070140838623
Epoch 100, training loss: 93.73556518554688 = 0.9039305448532104 + 10.0 * 9.283163070678711
Epoch 100, val loss: 0.9032306671142578
Epoch 110, training loss: 93.30008697509766 = 0.8725616335868835 + 10.0 * 9.242753028869629
Epoch 110, val loss: 0.8721733689308167
Epoch 120, training loss: 92.99270629882812 = 0.8373149633407593 + 10.0 * 9.21553897857666
Epoch 120, val loss: 0.8377554416656494
Epoch 130, training loss: 92.7920150756836 = 0.7990559935569763 + 10.0 * 9.199295997619629
Epoch 130, val loss: 0.8000514507293701
Epoch 140, training loss: 92.59337615966797 = 0.7575225234031677 + 10.0 * 9.183585166931152
Epoch 140, val loss: 0.7593544125556946
Epoch 150, training loss: 92.42814636230469 = 0.7142204642295837 + 10.0 * 9.171392440795898
Epoch 150, val loss: 0.7172904014587402
Epoch 160, training loss: 92.32721710205078 = 0.6688227653503418 + 10.0 * 9.165839195251465
Epoch 160, val loss: 0.6736583709716797
Epoch 170, training loss: 92.23362731933594 = 0.6229471564292908 + 10.0 * 9.161067962646484
Epoch 170, val loss: 0.6307909488677979
Epoch 180, training loss: 92.14230346679688 = 0.5817134380340576 + 10.0 * 9.156059265136719
Epoch 180, val loss: 0.5931357741355896
Epoch 190, training loss: 92.09420776367188 = 0.5467373132705688 + 10.0 * 9.154747009277344
Epoch 190, val loss: 0.5623521208763123
Epoch 200, training loss: 91.96633911132812 = 0.5181143879890442 + 10.0 * 9.14482307434082
Epoch 200, val loss: 0.537054717540741
Epoch 210, training loss: 91.86056518554688 = 0.4939209520816803 + 10.0 * 9.136664390563965
Epoch 210, val loss: 0.5166444778442383
Epoch 220, training loss: 91.80323028564453 = 0.4734379053115845 + 10.0 * 9.132979393005371
Epoch 220, val loss: 0.4998122453689575
Epoch 230, training loss: 91.69715881347656 = 0.4559288024902344 + 10.0 * 9.124122619628906
Epoch 230, val loss: 0.48585471510887146
Epoch 240, training loss: 91.62041473388672 = 0.4408644139766693 + 10.0 * 9.117955207824707
Epoch 240, val loss: 0.47419068217277527
Epoch 250, training loss: 91.54596710205078 = 0.4278053045272827 + 10.0 * 9.11181640625
Epoch 250, val loss: 0.4645772874355316
Epoch 260, training loss: 91.65778350830078 = 0.4162588119506836 + 10.0 * 9.124152183532715
Epoch 260, val loss: 0.4564504027366638
Epoch 270, training loss: 91.45124816894531 = 0.4061926305294037 + 10.0 * 9.10450553894043
Epoch 270, val loss: 0.44948673248291016
Epoch 280, training loss: 91.4123764038086 = 0.3974992632865906 + 10.0 * 9.10148811340332
Epoch 280, val loss: 0.4436817467212677
Epoch 290, training loss: 91.37913513183594 = 0.38995522260665894 + 10.0 * 9.098917961120605
Epoch 290, val loss: 0.4389716684818268
Epoch 300, training loss: 91.3492202758789 = 0.3831997215747833 + 10.0 * 9.096601486206055
Epoch 300, val loss: 0.43488895893096924
Epoch 310, training loss: 91.31962585449219 = 0.37702879309654236 + 10.0 * 9.094259262084961
Epoch 310, val loss: 0.43121469020843506
Epoch 320, training loss: 91.31436157226562 = 0.371410995721817 + 10.0 * 9.094294548034668
Epoch 320, val loss: 0.42797979712486267
Epoch 330, training loss: 91.25922393798828 = 0.3662438988685608 + 10.0 * 9.089298248291016
Epoch 330, val loss: 0.4250006377696991
Epoch 340, training loss: 91.23101806640625 = 0.3615036606788635 + 10.0 * 9.08695125579834
Epoch 340, val loss: 0.42238837480545044
Epoch 350, training loss: 91.20024108886719 = 0.3570408225059509 + 10.0 * 9.084320068359375
Epoch 350, val loss: 0.4199438691139221
Epoch 360, training loss: 91.23573303222656 = 0.3527880311012268 + 10.0 * 9.08829402923584
Epoch 360, val loss: 0.4175994098186493
Epoch 370, training loss: 91.18180847167969 = 0.3486644923686981 + 10.0 * 9.083314895629883
Epoch 370, val loss: 0.415558397769928
Epoch 380, training loss: 91.14608764648438 = 0.3448117971420288 + 10.0 * 9.080127716064453
Epoch 380, val loss: 0.41368624567985535
Epoch 390, training loss: 91.11968994140625 = 0.34121406078338623 + 10.0 * 9.077847480773926
Epoch 390, val loss: 0.41184964776039124
Epoch 400, training loss: 91.0968017578125 = 0.3377370536327362 + 10.0 * 9.075906753540039
Epoch 400, val loss: 0.41030949354171753
Epoch 410, training loss: 91.08941650390625 = 0.33437150716781616 + 10.0 * 9.075504302978516
Epoch 410, val loss: 0.4087944030761719
Epoch 420, training loss: 91.06454467773438 = 0.3311327397823334 + 10.0 * 9.073341369628906
Epoch 420, val loss: 0.40749236941337585
Epoch 430, training loss: 91.04645538330078 = 0.3280741572380066 + 10.0 * 9.07183837890625
Epoch 430, val loss: 0.4061014950275421
Epoch 440, training loss: 91.02991485595703 = 0.32517585158348083 + 10.0 * 9.070474624633789
Epoch 440, val loss: 0.40493881702423096
Epoch 450, training loss: 91.01458740234375 = 0.32236427068710327 + 10.0 * 9.069222450256348
Epoch 450, val loss: 0.4038807153701782
Epoch 460, training loss: 90.99793243408203 = 0.31960979104042053 + 10.0 * 9.067831993103027
Epoch 460, val loss: 0.4028753340244293
Epoch 470, training loss: 90.98501586914062 = 0.3169025182723999 + 10.0 * 9.066811561584473
Epoch 470, val loss: 0.40197277069091797
Epoch 480, training loss: 91.0074691772461 = 0.31425178050994873 + 10.0 * 9.069321632385254
Epoch 480, val loss: 0.40122950077056885
Epoch 490, training loss: 90.96158599853516 = 0.3116975724697113 + 10.0 * 9.06498908996582
Epoch 490, val loss: 0.4004324674606323
Epoch 500, training loss: 90.94265747070312 = 0.30923712253570557 + 10.0 * 9.063342094421387
Epoch 500, val loss: 0.39975300431251526
Epoch 510, training loss: 90.92826080322266 = 0.3068344295024872 + 10.0 * 9.062143325805664
Epoch 510, val loss: 0.399151474237442
Epoch 520, training loss: 90.92828369140625 = 0.3044670522212982 + 10.0 * 9.062381744384766
Epoch 520, val loss: 0.3986562192440033
Epoch 530, training loss: 90.93212127685547 = 0.30212345719337463 + 10.0 * 9.062999725341797
Epoch 530, val loss: 0.39815834164619446
Epoch 540, training loss: 90.89258575439453 = 0.29985588788986206 + 10.0 * 9.059272766113281
Epoch 540, val loss: 0.3977751135826111
Epoch 550, training loss: 90.87822723388672 = 0.29764455556869507 + 10.0 * 9.05805778503418
Epoch 550, val loss: 0.3973844051361084
Epoch 560, training loss: 90.86693572998047 = 0.29546308517456055 + 10.0 * 9.057147026062012
Epoch 560, val loss: 0.3971001207828522
Epoch 570, training loss: 90.96286010742188 = 0.2933002710342407 + 10.0 * 9.066956520080566
Epoch 570, val loss: 0.3969598412513733
Epoch 580, training loss: 90.87998962402344 = 0.2911710739135742 + 10.0 * 9.058881759643555
Epoch 580, val loss: 0.39673468470573425
Epoch 590, training loss: 90.83859252929688 = 0.28909459710121155 + 10.0 * 9.054949760437012
Epoch 590, val loss: 0.3964613080024719
Epoch 600, training loss: 90.8310317993164 = 0.2870623767375946 + 10.0 * 9.054396629333496
Epoch 600, val loss: 0.39637643098831177
Epoch 610, training loss: 90.8185806274414 = 0.28504660725593567 + 10.0 * 9.053353309631348
Epoch 610, val loss: 0.3962876498699188
Epoch 620, training loss: 90.84537506103516 = 0.2830412983894348 + 10.0 * 9.056233406066895
Epoch 620, val loss: 0.3963398039340973
Epoch 630, training loss: 90.81636810302734 = 0.2810494303703308 + 10.0 * 9.053531646728516
Epoch 630, val loss: 0.3962518572807312
Epoch 640, training loss: 90.79698181152344 = 0.27909088134765625 + 10.0 * 9.051789283752441
Epoch 640, val loss: 0.396283358335495
Epoch 650, training loss: 90.79656219482422 = 0.27715715765953064 + 10.0 * 9.051939964294434
Epoch 650, val loss: 0.39641767740249634
Epoch 660, training loss: 90.78852081298828 = 0.2752341330051422 + 10.0 * 9.051328659057617
Epoch 660, val loss: 0.3964134752750397
Epoch 670, training loss: 90.77005004882812 = 0.27333998680114746 + 10.0 * 9.049671173095703
Epoch 670, val loss: 0.39660435914993286
Epoch 680, training loss: 90.76285552978516 = 0.2714601159095764 + 10.0 * 9.049139022827148
Epoch 680, val loss: 0.39676910638809204
Epoch 690, training loss: 90.75814056396484 = 0.26958712935447693 + 10.0 * 9.048855781555176
Epoch 690, val loss: 0.3969411551952362
Epoch 700, training loss: 90.7607421875 = 0.26772820949554443 + 10.0 * 9.049301147460938
Epoch 700, val loss: 0.3972676396369934
Epoch 710, training loss: 90.74932861328125 = 0.2658929228782654 + 10.0 * 9.048343658447266
Epoch 710, val loss: 0.3974640667438507
Epoch 720, training loss: 90.73344421386719 = 0.264082670211792 + 10.0 * 9.04693603515625
Epoch 720, val loss: 0.39777034521102905
Epoch 730, training loss: 90.72439575195312 = 0.2622879445552826 + 10.0 * 9.046210289001465
Epoch 730, val loss: 0.39807602763175964
Epoch 740, training loss: 90.72975158691406 = 0.2605079412460327 + 10.0 * 9.046924591064453
Epoch 740, val loss: 0.3984711170196533
Epoch 750, training loss: 90.72879028320312 = 0.25873813033103943 + 10.0 * 9.047005653381348
Epoch 750, val loss: 0.3987520933151245
Epoch 760, training loss: 90.70753479003906 = 0.2569851577281952 + 10.0 * 9.04505443572998
Epoch 760, val loss: 0.3992200195789337
Epoch 770, training loss: 90.7004165649414 = 0.2552517354488373 + 10.0 * 9.044516563415527
Epoch 770, val loss: 0.39965739846229553
Epoch 780, training loss: 90.70104217529297 = 0.2535286843776703 + 10.0 * 9.044751167297363
Epoch 780, val loss: 0.4000549912452698
Epoch 790, training loss: 90.6883544921875 = 0.2518135607242584 + 10.0 * 9.04365348815918
Epoch 790, val loss: 0.40061336755752563
Epoch 800, training loss: 90.68805694580078 = 0.25011321902275085 + 10.0 * 9.043794631958008
Epoch 800, val loss: 0.40105175971984863
Epoch 810, training loss: 90.67169189453125 = 0.2484257072210312 + 10.0 * 9.042325973510742
Epoch 810, val loss: 0.40155449509620667
Epoch 820, training loss: 90.66633605957031 = 0.24674783647060394 + 10.0 * 9.041958808898926
Epoch 820, val loss: 0.4020736813545227
Epoch 830, training loss: 90.70245361328125 = 0.24507521092891693 + 10.0 * 9.045738220214844
Epoch 830, val loss: 0.40264517068862915
Epoch 840, training loss: 90.66263580322266 = 0.24341639876365662 + 10.0 * 9.041921615600586
Epoch 840, val loss: 0.4032379686832428
Epoch 850, training loss: 90.68125915527344 = 0.24177242815494537 + 10.0 * 9.04394817352295
Epoch 850, val loss: 0.4039209187030792
Epoch 860, training loss: 90.64995574951172 = 0.24014383554458618 + 10.0 * 9.04098129272461
Epoch 860, val loss: 0.40448155999183655
Epoch 870, training loss: 90.63948059082031 = 0.23852330446243286 + 10.0 * 9.040095329284668
Epoch 870, val loss: 0.405148983001709
Epoch 880, training loss: 90.63179779052734 = 0.23690639436244965 + 10.0 * 9.039488792419434
Epoch 880, val loss: 0.405787855386734
Epoch 890, training loss: 90.62645721435547 = 0.2352866530418396 + 10.0 * 9.039117813110352
Epoch 890, val loss: 0.40651071071624756
Epoch 900, training loss: 90.6531982421875 = 0.23366843163967133 + 10.0 * 9.041953086853027
Epoch 900, val loss: 0.40727972984313965
Epoch 910, training loss: 90.64778900146484 = 0.23205915093421936 + 10.0 * 9.041573524475098
Epoch 910, val loss: 0.40804412961006165
Epoch 920, training loss: 90.6255874633789 = 0.23045742511749268 + 10.0 * 9.039512634277344
Epoch 920, val loss: 0.4087918698787689
Epoch 930, training loss: 90.6076889038086 = 0.228861466050148 + 10.0 * 9.037882804870605
Epoch 930, val loss: 0.4096262454986572
Epoch 940, training loss: 90.60798645019531 = 0.22727078199386597 + 10.0 * 9.038071632385254
Epoch 940, val loss: 0.4104135036468506
Epoch 950, training loss: 90.61835479736328 = 0.22568027675151825 + 10.0 * 9.039267539978027
Epoch 950, val loss: 0.41117385029792786
Epoch 960, training loss: 90.60133361816406 = 0.22409085929393768 + 10.0 * 9.037724494934082
Epoch 960, val loss: 0.4121820628643036
Epoch 970, training loss: 90.59170532226562 = 0.22250553965568542 + 10.0 * 9.036920547485352
Epoch 970, val loss: 0.412954181432724
Epoch 980, training loss: 90.58790588378906 = 0.2209220677614212 + 10.0 * 9.036698341369629
Epoch 980, val loss: 0.4139077961444855
Epoch 990, training loss: 90.59290313720703 = 0.21934105455875397 + 10.0 * 9.03735637664795
Epoch 990, val loss: 0.4147696793079376
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8437
Overall ASR: 0.6815
Flip ASR: 0.6023/1554 nodes
The final ASR:0.68763, 0.01276, Accuracy:0.84678, 0.00395
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97596])
remove edge: torch.Size([2, 79816])
updated graph: torch.Size([2, 88764])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7414
Flip ASR: 0.6795/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72211, 0.01711, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 10 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.6793212890625 = 1.0878502130508423 + 10.0 * 10.359147071838379
Epoch 0, val loss: 1.087046504020691
Epoch 10, training loss: 104.64447784423828 = 1.0776785612106323 + 10.0 * 10.356679916381836
Epoch 10, val loss: 1.0766987800598145
Epoch 20, training loss: 104.09529876708984 = 1.0643621683120728 + 10.0 * 10.303093910217285
Epoch 20, val loss: 1.0633257627487183
Epoch 30, training loss: 100.35181427001953 = 1.0508259534835815 + 10.0 * 9.930098533630371
Epoch 30, val loss: 1.0499851703643799
Epoch 40, training loss: 97.46865844726562 = 1.0352461338043213 + 10.0 * 9.643341064453125
Epoch 40, val loss: 1.033964991569519
Epoch 50, training loss: 95.6367416381836 = 1.011725664138794 + 10.0 * 9.462501525878906
Epoch 50, val loss: 1.0102475881576538
Epoch 60, training loss: 94.9418716430664 = 0.9863080978393555 + 10.0 * 9.395556449890137
Epoch 60, val loss: 0.9856820106506348
Epoch 70, training loss: 94.66131591796875 = 0.9616428017616272 + 10.0 * 9.369967460632324
Epoch 70, val loss: 0.9616164565086365
Epoch 80, training loss: 94.41217041015625 = 0.9351033568382263 + 10.0 * 9.34770679473877
Epoch 80, val loss: 0.9355548620223999
Epoch 90, training loss: 94.14873504638672 = 0.9059647917747498 + 10.0 * 9.3242769241333
Epoch 90, val loss: 0.9070709347724915
Epoch 100, training loss: 93.78338623046875 = 0.8731574416160583 + 10.0 * 9.291023254394531
Epoch 100, val loss: 0.8754035234451294
Epoch 110, training loss: 93.3106460571289 = 0.8381029367446899 + 10.0 * 9.247254371643066
Epoch 110, val loss: 0.8421640992164612
Epoch 120, training loss: 92.97473907470703 = 0.8012882471084595 + 10.0 * 9.217345237731934
Epoch 120, val loss: 0.8067604899406433
Epoch 130, training loss: 92.71289825439453 = 0.7585933208465576 + 10.0 * 9.195430755615234
Epoch 130, val loss: 0.7661213874816895
Epoch 140, training loss: 92.50146484375 = 0.7123419046401978 + 10.0 * 9.178912162780762
Epoch 140, val loss: 0.7228477597236633
Epoch 150, training loss: 92.32395935058594 = 0.6674920916557312 + 10.0 * 9.16564655303955
Epoch 150, val loss: 0.6815837621688843
Epoch 160, training loss: 92.17872619628906 = 0.6262915730476379 + 10.0 * 9.155243873596191
Epoch 160, val loss: 0.6443558931350708
Epoch 170, training loss: 92.0509262084961 = 0.5890713334083557 + 10.0 * 9.146184921264648
Epoch 170, val loss: 0.611350953578949
Epoch 180, training loss: 91.93975830078125 = 0.5558302402496338 + 10.0 * 9.138392448425293
Epoch 180, val loss: 0.5825222134590149
Epoch 190, training loss: 91.88695526123047 = 0.5262911319732666 + 10.0 * 9.136066436767578
Epoch 190, val loss: 0.5575412511825562
Epoch 200, training loss: 91.81443786621094 = 0.5005522966384888 + 10.0 * 9.131388664245605
Epoch 200, val loss: 0.5363916754722595
Epoch 210, training loss: 91.75298309326172 = 0.47871312499046326 + 10.0 * 9.127427101135254
Epoch 210, val loss: 0.5190940499305725
Epoch 220, training loss: 91.7077407836914 = 0.46026167273521423 + 10.0 * 9.124748229980469
Epoch 220, val loss: 0.5051357746124268
Epoch 230, training loss: 91.68235778808594 = 0.44453713297843933 + 10.0 * 9.12378215789795
Epoch 230, val loss: 0.4936321973800659
Epoch 240, training loss: 91.6476058959961 = 0.4308606684207916 + 10.0 * 9.121674537658691
Epoch 240, val loss: 0.4845249056816101
Epoch 250, training loss: 91.61417388916016 = 0.4190668761730194 + 10.0 * 9.119510650634766
Epoch 250, val loss: 0.47689610719680786
Epoch 260, training loss: 91.5841293334961 = 0.4086657762527466 + 10.0 * 9.117546081542969
Epoch 260, val loss: 0.47046902775764465
Epoch 270, training loss: 91.55379486083984 = 0.3993077278137207 + 10.0 * 9.115448951721191
Epoch 270, val loss: 0.46501126885414124
Epoch 280, training loss: 91.55635070800781 = 0.3907775282859802 + 10.0 * 9.116557121276855
Epoch 280, val loss: 0.4602974057197571
Epoch 290, training loss: 91.4922103881836 = 0.38300466537475586 + 10.0 * 9.110920906066895
Epoch 290, val loss: 0.45608100295066833
Epoch 300, training loss: 91.46487426757812 = 0.3758969306945801 + 10.0 * 9.10889720916748
Epoch 300, val loss: 0.45237547159194946
Epoch 310, training loss: 91.45755004882812 = 0.3692246079444885 + 10.0 * 9.108832359313965
Epoch 310, val loss: 0.4491037428379059
Epoch 320, training loss: 91.4040756225586 = 0.36298397183418274 + 10.0 * 9.104108810424805
Epoch 320, val loss: 0.44623348116874695
Epoch 330, training loss: 91.37059020996094 = 0.3572351336479187 + 10.0 * 9.101335525512695
Epoch 330, val loss: 0.44349515438079834
Epoch 340, training loss: 91.32783508300781 = 0.3518114387989044 + 10.0 * 9.097601890563965
Epoch 340, val loss: 0.44110119342803955
Epoch 350, training loss: 91.33045959472656 = 0.34658390283584595 + 10.0 * 9.098387718200684
Epoch 350, val loss: 0.43870973587036133
Epoch 360, training loss: 91.27255249023438 = 0.3415008783340454 + 10.0 * 9.09310531616211
Epoch 360, val loss: 0.43672117590904236
Epoch 370, training loss: 91.24700927734375 = 0.3367086350917816 + 10.0 * 9.09103012084961
Epoch 370, val loss: 0.43476322293281555
Epoch 380, training loss: 91.22228240966797 = 0.33211877942085266 + 10.0 * 9.089016914367676
Epoch 380, val loss: 0.43302974104881287
Epoch 390, training loss: 91.20526123046875 = 0.32769855856895447 + 10.0 * 9.087756156921387
Epoch 390, val loss: 0.4313519597053528
Epoch 400, training loss: 91.18868255615234 = 0.32344305515289307 + 10.0 * 9.08652400970459
Epoch 400, val loss: 0.42990779876708984
Epoch 410, training loss: 91.18965148925781 = 0.3193223476409912 + 10.0 * 9.08703327178955
Epoch 410, val loss: 0.42831820249557495
Epoch 420, training loss: 91.150146484375 = 0.31536832451820374 + 10.0 * 9.083477973937988
Epoch 420, val loss: 0.4269786775112152
Epoch 430, training loss: 91.12554168701172 = 0.31159746646881104 + 10.0 * 9.08139419555664
Epoch 430, val loss: 0.4258379638195038
Epoch 440, training loss: 91.10389709472656 = 0.3079245686531067 + 10.0 * 9.079597473144531
Epoch 440, val loss: 0.42476722598075867
Epoch 450, training loss: 91.10985565185547 = 0.3043392300605774 + 10.0 * 9.080552101135254
Epoch 450, val loss: 0.42386987805366516
Epoch 460, training loss: 91.08800506591797 = 0.30079880356788635 + 10.0 * 9.078721046447754
Epoch 460, val loss: 0.4230979382991791
Epoch 470, training loss: 91.05733489990234 = 0.29748937487602234 + 10.0 * 9.075984001159668
Epoch 470, val loss: 0.4219227731227875
Epoch 480, training loss: 91.0416030883789 = 0.29434746503829956 + 10.0 * 9.074725151062012
Epoch 480, val loss: 0.42134249210357666
Epoch 490, training loss: 91.02861785888672 = 0.2912442684173584 + 10.0 * 9.073737144470215
Epoch 490, val loss: 0.42072030901908875
Epoch 500, training loss: 91.06167602539062 = 0.28817611932754517 + 10.0 * 9.077349662780762
Epoch 500, val loss: 0.420208215713501
Epoch 510, training loss: 91.00879669189453 = 0.28518199920654297 + 10.0 * 9.072361946105957
Epoch 510, val loss: 0.41984134912490845
Epoch 520, training loss: 90.995849609375 = 0.282304584980011 + 10.0 * 9.071354866027832
Epoch 520, val loss: 0.4195108115673065
Epoch 530, training loss: 91.0151596069336 = 0.2794981896877289 + 10.0 * 9.073566436767578
Epoch 530, val loss: 0.41929733753204346
Epoch 540, training loss: 90.96842193603516 = 0.276739239692688 + 10.0 * 9.069168090820312
Epoch 540, val loss: 0.4190331995487213
Epoch 550, training loss: 90.9511947631836 = 0.27405694127082825 + 10.0 * 9.067713737487793
Epoch 550, val loss: 0.41890501976013184
Epoch 560, training loss: 90.94673919677734 = 0.2714313566684723 + 10.0 * 9.067530632019043
Epoch 560, val loss: 0.4188673198223114
Epoch 570, training loss: 90.98894500732422 = 0.2688194215297699 + 10.0 * 9.072011947631836
Epoch 570, val loss: 0.4190663695335388
Epoch 580, training loss: 90.9267349243164 = 0.26628604531288147 + 10.0 * 9.066044807434082
Epoch 580, val loss: 0.41905614733695984
Epoch 590, training loss: 90.91027069091797 = 0.26384663581848145 + 10.0 * 9.064641952514648
Epoch 590, val loss: 0.4191448390483856
Epoch 600, training loss: 90.89866638183594 = 0.2614249587059021 + 10.0 * 9.063724517822266
Epoch 600, val loss: 0.41943639516830444
Epoch 610, training loss: 90.90654754638672 = 0.259024977684021 + 10.0 * 9.064752578735352
Epoch 610, val loss: 0.4196329116821289
Epoch 620, training loss: 90.89730072021484 = 0.2566155791282654 + 10.0 * 9.064068794250488
Epoch 620, val loss: 0.4202558100223541
Epoch 630, training loss: 90.87112426757812 = 0.25428399443626404 + 10.0 * 9.061683654785156
Epoch 630, val loss: 0.4204798638820648
Epoch 640, training loss: 90.86710357666016 = 0.2519947588443756 + 10.0 * 9.061511039733887
Epoch 640, val loss: 0.42087653279304504
Epoch 650, training loss: 90.85282897949219 = 0.2497163861989975 + 10.0 * 9.060311317443848
Epoch 650, val loss: 0.421395480632782
Epoch 660, training loss: 90.8478012084961 = 0.24745745956897736 + 10.0 * 9.060033798217773
Epoch 660, val loss: 0.4219174385070801
Epoch 670, training loss: 90.83140563964844 = 0.24520663917064667 + 10.0 * 9.058619499206543
Epoch 670, val loss: 0.4227282702922821
Epoch 680, training loss: 90.82925415039062 = 0.24298349022865295 + 10.0 * 9.058627128601074
Epoch 680, val loss: 0.423275351524353
Epoch 690, training loss: 90.83918762207031 = 0.24078339338302612 + 10.0 * 9.059840202331543
Epoch 690, val loss: 0.4241398274898529
Epoch 700, training loss: 90.81322479248047 = 0.23859141767024994 + 10.0 * 9.057462692260742
Epoch 700, val loss: 0.4247119724750519
Epoch 710, training loss: 90.82322692871094 = 0.2364175021648407 + 10.0 * 9.058680534362793
Epoch 710, val loss: 0.4255010783672333
Epoch 720, training loss: 90.79891204833984 = 0.23424385488033295 + 10.0 * 9.056467056274414
Epoch 720, val loss: 0.42637133598327637
Epoch 730, training loss: 90.78787994384766 = 0.23209528625011444 + 10.0 * 9.055578231811523
Epoch 730, val loss: 0.427166610956192
Epoch 740, training loss: 90.78556060791016 = 0.22995653748512268 + 10.0 * 9.055560111999512
Epoch 740, val loss: 0.42807868123054504
Epoch 750, training loss: 90.80083465576172 = 0.2278193235397339 + 10.0 * 9.05730152130127
Epoch 750, val loss: 0.42891380190849304
Epoch 760, training loss: 90.776611328125 = 0.22568562626838684 + 10.0 * 9.055092811584473
Epoch 760, val loss: 0.4299444854259491
Epoch 770, training loss: 90.7652816772461 = 0.223567932844162 + 10.0 * 9.054171562194824
Epoch 770, val loss: 0.43099185824394226
Epoch 780, training loss: 90.7590560913086 = 0.22145840525627136 + 10.0 * 9.053759574890137
Epoch 780, val loss: 0.4320971965789795
Epoch 790, training loss: 90.75128173828125 = 0.2193535715341568 + 10.0 * 9.053193092346191
Epoch 790, val loss: 0.4333589971065521
Epoch 800, training loss: 90.76033020019531 = 0.21725404262542725 + 10.0 * 9.05430793762207
Epoch 800, val loss: 0.4346310794353485
Epoch 810, training loss: 90.76702117919922 = 0.21515630185604095 + 10.0 * 9.05518627166748
Epoch 810, val loss: 0.4358837902545929
Epoch 820, training loss: 90.7330551147461 = 0.21307235956192017 + 10.0 * 9.051998138427734
Epoch 820, val loss: 0.43688201904296875
Epoch 830, training loss: 90.72076416015625 = 0.21099671721458435 + 10.0 * 9.050976753234863
Epoch 830, val loss: 0.43826383352279663
Epoch 840, training loss: 90.74031829833984 = 0.20893296599388123 + 10.0 * 9.053138732910156
Epoch 840, val loss: 0.43963828682899475
Epoch 850, training loss: 90.72084045410156 = 0.20685802400112152 + 10.0 * 9.051398277282715
Epoch 850, val loss: 0.4408571720123291
