Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11660])
remove edge: torch.Size([2, 9594])
updated graph: torch.Size([2, 10698])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9618343114852905 = 1.9532374143600464 + 0.001 * 8.596863746643066
Epoch 0, val loss: 1.961639404296875
Epoch 10, training loss: 1.9521666765213013 = 1.9435698986053467 + 0.001 * 8.596822738647461
Epoch 10, val loss: 1.9512134790420532
Epoch 20, training loss: 1.940459966659546 = 1.9318633079528809 + 0.001 * 8.59667682647705
Epoch 20, val loss: 1.938288927078247
Epoch 30, training loss: 1.9245115518569946 = 1.9159152507781982 + 0.001 * 8.596315383911133
Epoch 30, val loss: 1.9204117059707642
Epoch 40, training loss: 1.9013198614120483 = 1.8927245140075684 + 0.001 * 8.595404624938965
Epoch 40, val loss: 1.894317388534546
Epoch 50, training loss: 1.8684604167938232 = 1.8598675727844238 + 0.001 * 8.592844009399414
Epoch 50, val loss: 1.8580724000930786
Epoch 60, training loss: 1.8296085596084595 = 1.821024775505066 + 0.001 * 8.583724975585938
Epoch 60, val loss: 1.8182674646377563
Epoch 70, training loss: 1.7976354360580444 = 1.7890918254852295 + 0.001 * 8.543635368347168
Epoch 70, val loss: 1.7909387350082397
Epoch 80, training loss: 1.7659096717834473 = 1.7576005458831787 + 0.001 * 8.30906867980957
Epoch 80, val loss: 1.7660140991210938
Epoch 90, training loss: 1.7214537858963013 = 1.7133665084838867 + 0.001 * 8.087285041809082
Epoch 90, val loss: 1.7288501262664795
Epoch 100, training loss: 1.6597797870635986 = 1.6519200801849365 + 0.001 * 7.859725475311279
Epoch 100, val loss: 1.6761001348495483
Epoch 110, training loss: 1.5764802694320679 = 1.568824291229248 + 0.001 * 7.655980587005615
Epoch 110, val loss: 1.6066546440124512
Epoch 120, training loss: 1.4762827157974243 = 1.4686862230300903 + 0.001 * 7.59653902053833
Epoch 120, val loss: 1.5245083570480347
Epoch 130, training loss: 1.365761637687683 = 1.3582139015197754 + 0.001 * 7.547697067260742
Epoch 130, val loss: 1.4366896152496338
Epoch 140, training loss: 1.2489053010940552 = 1.241440773010254 + 0.001 * 7.464540481567383
Epoch 140, val loss: 1.3439035415649414
Epoch 150, training loss: 1.1293946504592896 = 1.1220566034317017 + 0.001 * 7.338055610656738
Epoch 150, val loss: 1.2505216598510742
Epoch 160, training loss: 1.0135928392410278 = 1.0063108205795288 + 0.001 * 7.281968116760254
Epoch 160, val loss: 1.1616727113723755
Epoch 170, training loss: 0.9087926745414734 = 0.9015222787857056 + 0.001 * 7.2703857421875
Epoch 170, val loss: 1.0834269523620605
Epoch 180, training loss: 0.8195774555206299 = 0.8123306035995483 + 0.001 * 7.246833801269531
Epoch 180, val loss: 1.0198322534561157
Epoch 190, training loss: 0.745518684387207 = 0.7382965683937073 + 0.001 * 7.222109317779541
Epoch 190, val loss: 0.970359742641449
Epoch 200, training loss: 0.6830977201461792 = 0.6759030222892761 + 0.001 * 7.194682598114014
Epoch 200, val loss: 0.9326390027999878
Epoch 210, training loss: 0.6283578872680664 = 0.6211828589439392 + 0.001 * 7.175016403198242
Epoch 210, val loss: 0.9034464359283447
Epoch 220, training loss: 0.5780043601989746 = 0.5708432197570801 + 0.001 * 7.161152362823486
Epoch 220, val loss: 0.8799214959144592
Epoch 230, training loss: 0.5298506021499634 = 0.5226970911026001 + 0.001 * 7.153527736663818
Epoch 230, val loss: 0.860035240650177
Epoch 240, training loss: 0.4825817942619324 = 0.4754292964935303 + 0.001 * 7.15250825881958
Epoch 240, val loss: 0.842837929725647
Epoch 250, training loss: 0.43551406264305115 = 0.4283676743507385 + 0.001 * 7.146379470825195
Epoch 250, val loss: 0.8277597427368164
Epoch 260, training loss: 0.3887142837047577 = 0.38157010078430176 + 0.001 * 7.144185543060303
Epoch 260, val loss: 0.8146974444389343
Epoch 270, training loss: 0.34292349219322205 = 0.3357841372489929 + 0.001 * 7.1393513679504395
Epoch 270, val loss: 0.8045019507408142
Epoch 280, training loss: 0.29959532618522644 = 0.29245850443840027 + 0.001 * 7.136807918548584
Epoch 280, val loss: 0.7978073358535767
Epoch 290, training loss: 0.2601976990699768 = 0.25306493043899536 + 0.001 * 7.13278341293335
Epoch 290, val loss: 0.7951656579971313
Epoch 300, training loss: 0.22554150223731995 = 0.21841441094875336 + 0.001 * 7.12708854675293
Epoch 300, val loss: 0.7966440916061401
Epoch 310, training loss: 0.1956844925880432 = 0.18856169283390045 + 0.001 * 7.122794151306152
Epoch 310, val loss: 0.8017820119857788
Epoch 320, training loss: 0.17018631100654602 = 0.16307145357131958 + 0.001 * 7.11486291885376
Epoch 320, val loss: 0.8099395632743835
Epoch 330, training loss: 0.14847777783870697 = 0.1413426548242569 + 0.001 * 7.135116100311279
Epoch 330, val loss: 0.8204755783081055
Epoch 340, training loss: 0.12991270422935486 = 0.12279531359672546 + 0.001 * 7.117388725280762
Epoch 340, val loss: 0.8327519297599792
Epoch 350, training loss: 0.1140410527586937 = 0.10693421214818954 + 0.001 * 7.106836795806885
Epoch 350, val loss: 0.846289336681366
Epoch 360, training loss: 0.10045427083969116 = 0.09335216879844666 + 0.001 * 7.102103233337402
Epoch 360, val loss: 0.8606348633766174
Epoch 370, training loss: 0.08880214393138885 = 0.08170601725578308 + 0.001 * 7.096129894256592
Epoch 370, val loss: 0.8753853440284729
Epoch 380, training loss: 0.07879549264907837 = 0.07170294970273972 + 0.001 * 7.092543125152588
Epoch 380, val loss: 0.890362560749054
Epoch 390, training loss: 0.0701955258846283 = 0.06310594081878662 + 0.001 * 7.089587688446045
Epoch 390, val loss: 0.9053804874420166
Epoch 400, training loss: 0.06280362606048584 = 0.05571575462818146 + 0.001 * 7.087868690490723
Epoch 400, val loss: 0.9202894568443298
Epoch 410, training loss: 0.0564480684697628 = 0.0493616908788681 + 0.001 * 7.086376667022705
Epoch 410, val loss: 0.9349870681762695
Epoch 420, training loss: 0.05098973587155342 = 0.04389535263180733 + 0.001 * 7.094383716583252
Epoch 420, val loss: 0.9493951797485352
Epoch 430, training loss: 0.04626482352614403 = 0.03918738290667534 + 0.001 * 7.077439785003662
Epoch 430, val loss: 0.963524580001831
Epoch 440, training loss: 0.04220359027385712 = 0.035125214606523514 + 0.001 * 7.078376770019531
Epoch 440, val loss: 0.9772804379463196
Epoch 450, training loss: 0.0386984683573246 = 0.03161181882023811 + 0.001 * 7.086648464202881
Epoch 450, val loss: 0.9906651377677917
Epoch 460, training loss: 0.035636819899082184 = 0.028564760461449623 + 0.001 * 7.0720601081848145
Epoch 460, val loss: 1.0036468505859375
Epoch 470, training loss: 0.03298671543598175 = 0.025913387537002563 + 0.001 * 7.07332706451416
Epoch 470, val loss: 1.0161985158920288
Epoch 480, training loss: 0.030668701976537704 = 0.02359597757458687 + 0.001 * 7.072722911834717
Epoch 480, val loss: 1.0283534526824951
Epoch 490, training loss: 0.02862870693206787 = 0.02155781164765358 + 0.001 * 7.070895195007324
Epoch 490, val loss: 1.0401042699813843
Epoch 500, training loss: 0.026818634942173958 = 0.01975390687584877 + 0.001 * 7.064728260040283
Epoch 500, val loss: 1.05140221118927
Epoch 510, training loss: 0.02521263062953949 = 0.01815122924745083 + 0.001 * 7.0614013671875
Epoch 510, val loss: 1.062242865562439
Epoch 520, training loss: 0.023783138021826744 = 0.01672397367656231 + 0.001 * 7.059163570404053
Epoch 520, val loss: 1.0726691484451294
Epoch 530, training loss: 0.02251998521387577 = 0.015450404025614262 + 0.001 * 7.069581508636475
Epoch 530, val loss: 1.0826793909072876
Epoch 540, training loss: 0.021367281675338745 = 0.014311878010630608 + 0.001 * 7.0554022789001465
Epoch 540, val loss: 1.0922850370407104
Epoch 550, training loss: 0.02034698985517025 = 0.013291729614138603 + 0.001 * 7.055260181427002
Epoch 550, val loss: 1.1015268564224243
Epoch 560, training loss: 0.01943628117442131 = 0.012375438585877419 + 0.001 * 7.0608415603637695
Epoch 560, val loss: 1.1104052066802979
Epoch 570, training loss: 0.01860949769616127 = 0.011550415307283401 + 0.001 * 7.05908203125
Epoch 570, val loss: 1.1189591884613037
Epoch 580, training loss: 0.017852164804935455 = 0.010805689729750156 + 0.001 * 7.046474456787109
Epoch 580, val loss: 1.1271991729736328
Epoch 590, training loss: 0.01717638038098812 = 0.01013161800801754 + 0.001 * 7.044761657714844
Epoch 590, val loss: 1.1351391077041626
Epoch 600, training loss: 0.01657840423285961 = 0.009519943967461586 + 0.001 * 7.058459281921387
Epoch 600, val loss: 1.142763376235962
Epoch 610, training loss: 0.016007259488105774 = 0.008963450789451599 + 0.001 * 7.043809413909912
Epoch 610, val loss: 1.1501628160476685
Epoch 620, training loss: 0.015501820482313633 = 0.008456038311123848 + 0.001 * 7.04578161239624
Epoch 620, val loss: 1.1572717428207397
Epoch 630, training loss: 0.015042221173644066 = 0.007992222905158997 + 0.001 * 7.049997329711914
Epoch 630, val loss: 1.1641514301300049
Epoch 640, training loss: 0.014605456031858921 = 0.00756722129881382 + 0.001 * 7.038234233856201
Epoch 640, val loss: 1.1708028316497803
Epoch 650, training loss: 0.014216146431863308 = 0.007176871877163649 + 0.001 * 7.039274215698242
Epoch 650, val loss: 1.1772300004959106
Epoch 660, training loss: 0.013852415606379509 = 0.006817597895860672 + 0.001 * 7.034817218780518
Epoch 660, val loss: 1.1834540367126465
Epoch 670, training loss: 0.013525834307074547 = 0.006486234255135059 + 0.001 * 7.039600372314453
Epoch 670, val loss: 1.189477562904358
Epoch 680, training loss: 0.013211811892688274 = 0.006180029362440109 + 0.001 * 7.031782150268555
Epoch 680, val loss: 1.1953167915344238
Epoch 690, training loss: 0.012941968627274036 = 0.005896504502743483 + 0.001 * 7.045463562011719
Epoch 690, val loss: 1.200974702835083
Epoch 700, training loss: 0.01267554983496666 = 0.0056334869004786015 + 0.001 * 7.042062759399414
Epoch 700, val loss: 1.2064622640609741
Epoch 710, training loss: 0.012418324127793312 = 0.00538907665759325 + 0.001 * 7.029246807098389
Epoch 710, val loss: 1.211787462234497
Epoch 720, training loss: 0.01218889094889164 = 0.005161549896001816 + 0.001 * 7.027340888977051
Epoch 720, val loss: 1.2169508934020996
Epoch 730, training loss: 0.011981138959527016 = 0.0049494048580527306 + 0.001 * 7.031733989715576
Epoch 730, val loss: 1.2219645977020264
Epoch 740, training loss: 0.0118035189807415 = 0.004751271102577448 + 0.001 * 7.052247524261475
Epoch 740, val loss: 1.2268391847610474
Epoch 750, training loss: 0.011592244729399681 = 0.004565895069390535 + 0.001 * 7.0263495445251465
Epoch 750, val loss: 1.2315740585327148
Epoch 760, training loss: 0.01141859870404005 = 0.004392086062580347 + 0.001 * 7.026512145996094
Epoch 760, val loss: 1.2361913919448853
Epoch 770, training loss: 0.011255964636802673 = 0.004228454083204269 + 0.001 * 7.027509689331055
Epoch 770, val loss: 1.2406902313232422
Epoch 780, training loss: 0.011092513799667358 = 0.0040735285729169846 + 0.001 * 7.018984317779541
Epoch 780, val loss: 1.2450981140136719
Epoch 790, training loss: 0.010945549234747887 = 0.00392589857801795 + 0.001 * 7.019650936126709
Epoch 790, val loss: 1.2494202852249146
Epoch 800, training loss: 0.010801006108522415 = 0.003784647211432457 + 0.001 * 7.016358852386475
Epoch 800, val loss: 1.2536858320236206
Epoch 810, training loss: 0.01066908985376358 = 0.0036493209190666676 + 0.001 * 7.019769191741943
Epoch 810, val loss: 1.2579138278961182
Epoch 820, training loss: 0.010530754923820496 = 0.003519797930493951 + 0.001 * 7.010956287384033
Epoch 820, val loss: 1.262058973312378
Epoch 830, training loss: 0.010413438081741333 = 0.003396041225641966 + 0.001 * 7.017396450042725
Epoch 830, val loss: 1.2661802768707275
Epoch 840, training loss: 0.010289100930094719 = 0.003278000745922327 + 0.001 * 7.011099815368652
Epoch 840, val loss: 1.27024245262146
Epoch 850, training loss: 0.010196471586823463 = 0.0031656285282224417 + 0.001 * 7.030842304229736
Epoch 850, val loss: 1.2742445468902588
Epoch 860, training loss: 0.01006805058568716 = 0.0030587248038500547 + 0.001 * 7.0093255043029785
Epoch 860, val loss: 1.278197169303894
Epoch 870, training loss: 0.009964408352971077 = 0.002957090502604842 + 0.001 * 7.007317543029785
Epoch 870, val loss: 1.282112956047058
Epoch 880, training loss: 0.009867187589406967 = 0.0028605805709958076 + 0.001 * 7.006606578826904
Epoch 880, val loss: 1.2859086990356445
Epoch 890, training loss: 0.009786905720829964 = 0.002768898382782936 + 0.001 * 7.018006324768066
Epoch 890, val loss: 1.2896510362625122
Epoch 900, training loss: 0.00969176460057497 = 0.0026817985344678164 + 0.001 * 7.009965896606445
Epoch 900, val loss: 1.2933349609375
Epoch 910, training loss: 0.009605217725038528 = 0.0025989848654717207 + 0.001 * 7.006232738494873
Epoch 910, val loss: 1.29690682888031
Epoch 920, training loss: 0.009527595713734627 = 0.002520215231925249 + 0.001 * 7.007379531860352
Epoch 920, val loss: 1.300432562828064
Epoch 930, training loss: 0.009438588283956051 = 0.0024451902136206627 + 0.001 * 6.9933977127075195
Epoch 930, val loss: 1.3038698434829712
Epoch 940, training loss: 0.009369217790663242 = 0.0023736993316560984 + 0.001 * 6.995518207550049
Epoch 940, val loss: 1.3072357177734375
Epoch 950, training loss: 0.009293959476053715 = 0.00230550323612988 + 0.001 * 6.988455772399902
Epoch 950, val loss: 1.3105250597000122
Epoch 960, training loss: 0.0092467050999403 = 0.0022403928451240063 + 0.001 * 7.006311416625977
Epoch 960, val loss: 1.3137540817260742
Epoch 970, training loss: 0.009180765599012375 = 0.0021781991235911846 + 0.001 * 7.002566814422607
Epoch 970, val loss: 1.316940426826477
Epoch 980, training loss: 0.009116088971495628 = 0.0021187830716371536 + 0.001 * 6.997305870056152
Epoch 980, val loss: 1.320032000541687
Epoch 990, training loss: 0.009046359919011593 = 0.0020619172137230635 + 0.001 * 6.984442710876465
Epoch 990, val loss: 1.323067545890808
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 1.96323823928833 = 1.954641342163086 + 0.001 * 8.596863746643066
Epoch 0, val loss: 1.9560970067977905
Epoch 10, training loss: 1.9532526731491089 = 1.9446558952331543 + 0.001 * 8.59682559967041
Epoch 10, val loss: 1.9466396570205688
Epoch 20, training loss: 1.9413743019104004 = 1.9327776432037354 + 0.001 * 8.596683502197266
Epoch 20, val loss: 1.9349241256713867
Epoch 30, training loss: 1.9253852367401123 = 1.916788935661316 + 0.001 * 8.596324920654297
Epoch 30, val loss: 1.9188133478164673
Epoch 40, training loss: 1.9023984670639038 = 1.8938031196594238 + 0.001 * 8.595393180847168
Epoch 40, val loss: 1.8957624435424805
Epoch 50, training loss: 1.8701834678649902 = 1.8615909814834595 + 0.001 * 8.592523574829102
Epoch 50, val loss: 1.8645422458648682
Epoch 60, training loss: 1.8312023878097534 = 1.8226211071014404 + 0.001 * 8.581318855285645
Epoch 60, val loss: 1.8293205499649048
Epoch 70, training loss: 1.7957046031951904 = 1.7871713638305664 + 0.001 * 8.53324031829834
Epoch 70, val loss: 1.7997920513153076
Epoch 80, training loss: 1.7602839469909668 = 1.75203275680542 + 0.001 * 8.251181602478027
Epoch 80, val loss: 1.7677499055862427
Epoch 90, training loss: 1.710924744606018 = 1.7028721570968628 + 0.001 * 8.05262565612793
Epoch 90, val loss: 1.7234662771224976
Epoch 100, training loss: 1.6416012048721313 = 1.633634328842163 + 0.001 * 7.966865062713623
Epoch 100, val loss: 1.6637532711029053
Epoch 110, training loss: 1.551384687423706 = 1.5435255765914917 + 0.001 * 7.85915994644165
Epoch 110, val loss: 1.5889252424240112
Epoch 120, training loss: 1.4481607675552368 = 1.4404735565185547 + 0.001 * 7.687223434448242
Epoch 120, val loss: 1.5041974782943726
Epoch 130, training loss: 1.3427895307540894 = 1.33524751663208 + 0.001 * 7.5419921875
Epoch 130, val loss: 1.4194798469543457
Epoch 140, training loss: 1.2417153120040894 = 1.2342140674591064 + 0.001 * 7.501208782196045
Epoch 140, val loss: 1.3409281969070435
Epoch 150, training loss: 1.1466084718704224 = 1.1391654014587402 + 0.001 * 7.44307804107666
Epoch 150, val loss: 1.270255446434021
Epoch 160, training loss: 1.0573129653930664 = 1.0499019622802734 + 0.001 * 7.410955429077148
Epoch 160, val loss: 1.2060258388519287
Epoch 170, training loss: 0.9728585481643677 = 0.9654859304428101 + 0.001 * 7.372634410858154
Epoch 170, val loss: 1.1466801166534424
Epoch 180, training loss: 0.8930689692497253 = 0.8857458233833313 + 0.001 * 7.323169708251953
Epoch 180, val loss: 1.0914934873580933
Epoch 190, training loss: 0.8182281255722046 = 0.8109645843505859 + 0.001 * 7.263524532318115
Epoch 190, val loss: 1.040682077407837
Epoch 200, training loss: 0.7487093806266785 = 0.74148029088974 + 0.001 * 7.229095458984375
Epoch 200, val loss: 0.9944667220115662
Epoch 210, training loss: 0.6848046183586121 = 0.6775828003883362 + 0.001 * 7.221822261810303
Epoch 210, val loss: 0.9540569186210632
Epoch 220, training loss: 0.6263837814331055 = 0.6191662549972534 + 0.001 * 7.21755313873291
Epoch 220, val loss: 0.9201905131340027
Epoch 230, training loss: 0.5730957388877869 = 0.5658859014511108 + 0.001 * 7.209841251373291
Epoch 230, val loss: 0.892577588558197
Epoch 240, training loss: 0.5244162082672119 = 0.5172166228294373 + 0.001 * 7.199563503265381
Epoch 240, val loss: 0.8709740042686462
Epoch 250, training loss: 0.47979992628097534 = 0.47261208295822144 + 0.001 * 7.1878437995910645
Epoch 250, val loss: 0.854597270488739
Epoch 260, training loss: 0.4387071430683136 = 0.4315311014652252 + 0.001 * 7.17603063583374
Epoch 260, val loss: 0.8426486849784851
Epoch 270, training loss: 0.40059003233909607 = 0.39342817664146423 + 0.001 * 7.161869049072266
Epoch 270, val loss: 0.8344088792800903
Epoch 280, training loss: 0.3648601174354553 = 0.35770878195762634 + 0.001 * 7.1513495445251465
Epoch 280, val loss: 0.8291629552841187
Epoch 290, training loss: 0.33082669973373413 = 0.3236826956272125 + 0.001 * 7.143993854522705
Epoch 290, val loss: 0.8260572552680969
Epoch 300, training loss: 0.29773905873298645 = 0.29059967398643494 + 0.001 * 7.1393914222717285
Epoch 300, val loss: 0.824340283870697
Epoch 310, training loss: 0.26506665349006653 = 0.25793537497520447 + 0.001 * 7.13129186630249
Epoch 310, val loss: 0.8233873248100281
Epoch 320, training loss: 0.23300345242023468 = 0.22587266564369202 + 0.001 * 7.130783557891846
Epoch 320, val loss: 0.823328971862793
Epoch 330, training loss: 0.2025085687637329 = 0.19537688791751862 + 0.001 * 7.131681442260742
Epoch 330, val loss: 0.8244408965110779
Epoch 340, training loss: 0.17469152808189392 = 0.16755995154380798 + 0.001 * 7.1315813064575195
Epoch 340, val loss: 0.8270953893661499
Epoch 350, training loss: 0.15020985901355743 = 0.14308010041713715 + 0.001 * 7.129757404327393
Epoch 350, val loss: 0.8319469690322876
Epoch 360, training loss: 0.12917999923229218 = 0.12205102294683456 + 0.001 * 7.128976821899414
Epoch 360, val loss: 0.8391487002372742
Epoch 370, training loss: 0.11139935255050659 = 0.10427405685186386 + 0.001 * 7.125298500061035
Epoch 370, val loss: 0.8484792709350586
Epoch 380, training loss: 0.09649632126092911 = 0.08937010914087296 + 0.001 * 7.126213550567627
Epoch 380, val loss: 0.859494686126709
Epoch 390, training loss: 0.08404260128736496 = 0.07692113518714905 + 0.001 * 7.121466636657715
Epoch 390, val loss: 0.8716828227043152
Epoch 400, training loss: 0.07364225387573242 = 0.06652268022298813 + 0.001 * 7.11957311630249
Epoch 400, val loss: 0.8845740556716919
Epoch 410, training loss: 0.06493929773569107 = 0.05782538279891014 + 0.001 * 7.113915920257568
Epoch 410, val loss: 0.8978160619735718
Epoch 420, training loss: 0.05763869360089302 = 0.050535935908555984 + 0.001 * 7.102758407592773
Epoch 420, val loss: 0.9112284183502197
Epoch 430, training loss: 0.05151062086224556 = 0.044411979615688324 + 0.001 * 7.098639965057373
Epoch 430, val loss: 0.9246407747268677
Epoch 440, training loss: 0.04634814336895943 = 0.039250172674655914 + 0.001 * 7.0979719161987305
Epoch 440, val loss: 0.9379286766052246
Epoch 450, training loss: 0.04197349026799202 = 0.03488324582576752 + 0.001 * 7.090243339538574
Epoch 450, val loss: 0.9510030150413513
Epoch 460, training loss: 0.03826326131820679 = 0.031172478571534157 + 0.001 * 7.090783596038818
Epoch 460, val loss: 0.9638108015060425
Epoch 470, training loss: 0.03508662059903145 = 0.02800414152443409 + 0.001 * 7.082478046417236
Epoch 470, val loss: 0.9763242602348328
Epoch 480, training loss: 0.03236396238207817 = 0.02528553083539009 + 0.001 * 7.078430652618408
Epoch 480, val loss: 0.988507091999054
Epoch 490, training loss: 0.03002743609249592 = 0.02294096350669861 + 0.001 * 7.086472034454346
Epoch 490, val loss: 1.0003710985183716
Epoch 500, training loss: 0.02798355370759964 = 0.02090829610824585 + 0.001 * 7.075257301330566
Epoch 500, val loss: 1.0118647813796997
Epoch 510, training loss: 0.026207301765680313 = 0.019136914983391762 + 0.001 * 7.070387363433838
Epoch 510, val loss: 1.0230083465576172
Epoch 520, training loss: 0.02464383840560913 = 0.017585497349500656 + 0.001 * 7.058341026306152
Epoch 520, val loss: 1.033803939819336
Epoch 530, training loss: 0.023312851786613464 = 0.016220029443502426 + 0.001 * 7.09282112121582
Epoch 530, val loss: 1.044253945350647
Epoch 540, training loss: 0.022064946591854095 = 0.01501272153109312 + 0.001 * 7.052224159240723
Epoch 540, val loss: 1.0543789863586426
Epoch 550, training loss: 0.02099708467721939 = 0.013940462842583656 + 0.001 * 7.056621551513672
Epoch 550, val loss: 1.0641634464263916
Epoch 560, training loss: 0.020031457766890526 = 0.012984136119484901 + 0.001 * 7.047321796417236
Epoch 560, val loss: 1.0736653804779053
Epoch 570, training loss: 0.019175514578819275 = 0.012127727270126343 + 0.001 * 7.047786712646484
Epoch 570, val loss: 1.082854151725769
Epoch 580, training loss: 0.01840837299823761 = 0.01135749090462923 + 0.001 * 7.050881862640381
Epoch 580, val loss: 1.0917772054672241
Epoch 590, training loss: 0.01770666614174843 = 0.010660660453140736 + 0.001 * 7.0460052490234375
Epoch 590, val loss: 1.1004633903503418
Epoch 600, training loss: 0.017083236947655678 = 0.010025426745414734 + 0.001 * 7.057810306549072
Epoch 600, val loss: 1.1089880466461182
Epoch 610, training loss: 0.016486331820487976 = 0.009442996233701706 + 0.001 * 7.043334484100342
Epoch 610, val loss: 1.1173959970474243
Epoch 620, training loss: 0.015938296914100647 = 0.008907293900847435 + 0.001 * 7.031002998352051
Epoch 620, val loss: 1.125687599182129
Epoch 630, training loss: 0.015445124357938766 = 0.008413982577621937 + 0.001 * 7.031141757965088
Epoch 630, val loss: 1.1337999105453491
Epoch 640, training loss: 0.014985945075750351 = 0.007959501817822456 + 0.001 * 7.0264434814453125
Epoch 640, val loss: 1.1417754888534546
Epoch 650, training loss: 0.014567436650395393 = 0.007540497463196516 + 0.001 * 7.026938438415527
Epoch 650, val loss: 1.1496244668960571
Epoch 660, training loss: 0.014188390225172043 = 0.007153949234634638 + 0.001 * 7.034440517425537
Epoch 660, val loss: 1.1572855710983276
Epoch 670, training loss: 0.013824212364852428 = 0.006797081790864468 + 0.001 * 7.027130126953125
Epoch 670, val loss: 1.1647716760635376
Epoch 680, training loss: 0.01349678635597229 = 0.006467307917773724 + 0.001 * 7.029478549957275
Epoch 680, val loss: 1.1720807552337646
Epoch 690, training loss: 0.013190101832151413 = 0.006162087898701429 + 0.001 * 7.028014183044434
Epoch 690, val loss: 1.1792293787002563
Epoch 700, training loss: 0.012907877564430237 = 0.005879144184291363 + 0.001 * 7.028732776641846
Epoch 700, val loss: 1.1861920356750488
Epoch 710, training loss: 0.012630270794034004 = 0.005616388283669949 + 0.001 * 7.013882160186768
Epoch 710, val loss: 1.1930071115493774
Epoch 720, training loss: 0.01239064522087574 = 0.005371791310608387 + 0.001 * 7.018853187561035
Epoch 720, val loss: 1.1996710300445557
Epoch 730, training loss: 0.012168197892606258 = 0.005143528338521719 + 0.001 * 7.024669170379639
Epoch 730, val loss: 1.2062016725540161
Epoch 740, training loss: 0.011941608041524887 = 0.00492975115776062 + 0.001 * 7.011856555938721
Epoch 740, val loss: 1.212634563446045
Epoch 750, training loss: 0.011774718761444092 = 0.0047290693037211895 + 0.001 * 7.045649528503418
Epoch 750, val loss: 1.2189821004867554
Epoch 760, training loss: 0.011555362492799759 = 0.004540739115327597 + 0.001 * 7.014623641967773
Epoch 760, val loss: 1.2252107858657837
Epoch 770, training loss: 0.011388078331947327 = 0.004363464657217264 + 0.001 * 7.024613857269287
Epoch 770, val loss: 1.2313838005065918
Epoch 780, training loss: 0.011206528171896935 = 0.004196626134216785 + 0.001 * 7.00990104675293
Epoch 780, val loss: 1.2374147176742554
Epoch 790, training loss: 0.011036137118935585 = 0.004039356019347906 + 0.001 * 6.9967803955078125
Epoch 790, val loss: 1.2433812618255615
Epoch 800, training loss: 0.0109122134745121 = 0.0038910626899451017 + 0.001 * 7.0211501121521
Epoch 800, val loss: 1.2492777109146118
Epoch 810, training loss: 0.010750377550721169 = 0.003751130308955908 + 0.001 * 6.999247074127197
Epoch 810, val loss: 1.2550538778305054
Epoch 820, training loss: 0.01062078308314085 = 0.0036190703976899385 + 0.001 * 7.001712322235107
Epoch 820, val loss: 1.2607572078704834
Epoch 830, training loss: 0.010484776459634304 = 0.003494453150779009 + 0.001 * 6.990323066711426
Epoch 830, val loss: 1.2663120031356812
Epoch 840, training loss: 0.010378370992839336 = 0.003376879496499896 + 0.001 * 7.001491546630859
Epoch 840, val loss: 1.2717769145965576
Epoch 850, training loss: 0.010248973034322262 = 0.0032659091521054506 + 0.001 * 6.983063697814941
Epoch 850, val loss: 1.2770978212356567
Epoch 860, training loss: 0.010151060298085213 = 0.0031609341967850924 + 0.001 * 6.990126132965088
Epoch 860, val loss: 1.2823495864868164
Epoch 870, training loss: 0.01007993146777153 = 0.0030615502037107944 + 0.001 * 7.018381595611572
Epoch 870, val loss: 1.2874525785446167
Epoch 880, training loss: 0.009952887892723083 = 0.002967463806271553 + 0.001 * 6.985424041748047
Epoch 880, val loss: 1.292524814605713
Epoch 890, training loss: 0.009885457344353199 = 0.0028782139997929335 + 0.001 * 7.007242679595947
Epoch 890, val loss: 1.2974284887313843
Epoch 900, training loss: 0.009777307510375977 = 0.002793736057356 + 0.001 * 6.983571529388428
Epoch 900, val loss: 1.3022359609603882
Epoch 910, training loss: 0.009694414213299751 = 0.0027135557029396296 + 0.001 * 6.980857849121094
Epoch 910, val loss: 1.3069506883621216
Epoch 920, training loss: 0.009604860097169876 = 0.0026374084409326315 + 0.001 * 6.967451572418213
Epoch 920, val loss: 1.311539888381958
Epoch 930, training loss: 0.009532243944704533 = 0.00256492686457932 + 0.001 * 6.967316627502441
Epoch 930, val loss: 1.3160704374313354
Epoch 940, training loss: 0.009483394213020802 = 0.0024959214497357607 + 0.001 * 6.987472057342529
Epoch 940, val loss: 1.3205069303512573
Epoch 950, training loss: 0.009412545710802078 = 0.002430360298603773 + 0.001 * 6.982185363769531
Epoch 950, val loss: 1.3248043060302734
Epoch 960, training loss: 0.009334854781627655 = 0.002367895096540451 + 0.001 * 6.9669599533081055
Epoch 960, val loss: 1.3290410041809082
Epoch 970, training loss: 0.009279494173824787 = 0.0023083193227648735 + 0.001 * 6.971174716949463
Epoch 970, val loss: 1.333148717880249
Epoch 980, training loss: 0.00921451672911644 = 0.0022514620795845985 + 0.001 * 6.9630537033081055
Epoch 980, val loss: 1.3371790647506714
Epoch 990, training loss: 0.009161065332591534 = 0.002196987858042121 + 0.001 * 6.964077472686768
Epoch 990, val loss: 1.3411566019058228
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8017923036373221
=== training gcn model ===
Epoch 0, training loss: 1.982548713684082 = 1.973951816558838 + 0.001 * 8.59687614440918
Epoch 0, val loss: 1.9738585948944092
Epoch 10, training loss: 1.9719154834747314 = 1.9633187055587769 + 0.001 * 8.596832275390625
Epoch 10, val loss: 1.963837742805481
Epoch 20, training loss: 1.9591760635375977 = 1.9505794048309326 + 0.001 * 8.596692085266113
Epoch 20, val loss: 1.9517093896865845
Epoch 30, training loss: 1.9420117139816284 = 1.9334152936935425 + 0.001 * 8.59636402130127
Epoch 30, val loss: 1.9351632595062256
Epoch 40, training loss: 1.917441725730896 = 1.908846139907837 + 0.001 * 8.59556770324707
Epoch 40, val loss: 1.9115808010101318
Epoch 50, training loss: 1.8827917575836182 = 1.8741984367370605 + 0.001 * 8.593324661254883
Epoch 50, val loss: 1.8794190883636475
Epoch 60, training loss: 1.8405029773712158 = 1.831917643547058 + 0.001 * 8.585343360900879
Epoch 60, val loss: 1.8430662155151367
Epoch 70, training loss: 1.8031092882156372 = 1.7945609092712402 + 0.001 * 8.548331260681152
Epoch 70, val loss: 1.8135517835617065
Epoch 80, training loss: 1.7699661254882812 = 1.7616832256317139 + 0.001 * 8.282845497131348
Epoch 80, val loss: 1.7822065353393555
Epoch 90, training loss: 1.725906491279602 = 1.7178370952606201 + 0.001 * 8.069398880004883
Epoch 90, val loss: 1.7396299839019775
Epoch 100, training loss: 1.6649041175842285 = 1.6569324731826782 + 0.001 * 7.9716901779174805
Epoch 100, val loss: 1.6863551139831543
Epoch 110, training loss: 1.585152506828308 = 1.577275037765503 + 0.001 * 7.877427577972412
Epoch 110, val loss: 1.620239496231079
Epoch 120, training loss: 1.4964808225631714 = 1.4887511730194092 + 0.001 * 7.729665756225586
Epoch 120, val loss: 1.5482937097549438
Epoch 130, training loss: 1.4097329378128052 = 1.4022223949432373 + 0.001 * 7.510524272918701
Epoch 130, val loss: 1.477726936340332
Epoch 140, training loss: 1.3255316019058228 = 1.318061351776123 + 0.001 * 7.470278739929199
Epoch 140, val loss: 1.4094862937927246
Epoch 150, training loss: 1.2398099899291992 = 1.2324163913726807 + 0.001 * 7.393544673919678
Epoch 150, val loss: 1.342799425125122
Epoch 160, training loss: 1.149295449256897 = 1.1419591903686523 + 0.001 * 7.336316108703613
Epoch 160, val loss: 1.2752717733383179
Epoch 170, training loss: 1.0530823469161987 = 1.0457837581634521 + 0.001 * 7.298561096191406
Epoch 170, val loss: 1.2049061059951782
Epoch 180, training loss: 0.9531680941581726 = 0.9458805918693542 + 0.001 * 7.287508964538574
Epoch 180, val loss: 1.1325607299804688
Epoch 190, training loss: 0.8542976975440979 = 0.8470121622085571 + 0.001 * 7.285533905029297
Epoch 190, val loss: 1.06157648563385
Epoch 200, training loss: 0.7619443535804749 = 0.7546607255935669 + 0.001 * 7.283625602722168
Epoch 200, val loss: 0.9971808791160583
Epoch 210, training loss: 0.6796360015869141 = 0.6723554134368896 + 0.001 * 7.28056526184082
Epoch 210, val loss: 0.9431129097938538
Epoch 220, training loss: 0.6077728271484375 = 0.6004957556724548 + 0.001 * 7.277077674865723
Epoch 220, val loss: 0.9000446200370789
Epoch 230, training loss: 0.5447044372558594 = 0.5374308824539185 + 0.001 * 7.273559093475342
Epoch 230, val loss: 0.8670691847801208
Epoch 240, training loss: 0.4884095788002014 = 0.4811401963233948 + 0.001 * 7.269379138946533
Epoch 240, val loss: 0.8424814939498901
Epoch 250, training loss: 0.4372557997703552 = 0.42999187111854553 + 0.001 * 7.2639312744140625
Epoch 250, val loss: 0.82445228099823
Epoch 260, training loss: 0.3901366889476776 = 0.38287919759750366 + 0.001 * 7.257490634918213
Epoch 260, val loss: 0.8115281462669373
Epoch 270, training loss: 0.34642118215560913 = 0.3391719162464142 + 0.001 * 7.249270439147949
Epoch 270, val loss: 0.8026344776153564
Epoch 280, training loss: 0.30589666962623596 = 0.2986583113670349 + 0.001 * 7.238353252410889
Epoch 280, val loss: 0.797353982925415
Epoch 290, training loss: 0.2686115801334381 = 0.261384516954422 + 0.001 * 7.227077484130859
Epoch 290, val loss: 0.7952178120613098
Epoch 300, training loss: 0.23474331200122833 = 0.2275298535823822 + 0.001 * 7.2134623527526855
Epoch 300, val loss: 0.7962762713432312
Epoch 310, training loss: 0.20446506142616272 = 0.19725821912288666 + 0.001 * 7.206848621368408
Epoch 310, val loss: 0.8004530668258667
Epoch 320, training loss: 0.1777980476617813 = 0.17059853672981262 + 0.001 * 7.199505805969238
Epoch 320, val loss: 0.8074629306793213
Epoch 330, training loss: 0.15464061498641968 = 0.1474505513906479 + 0.001 * 7.190068244934082
Epoch 330, val loss: 0.8170014023780823
Epoch 340, training loss: 0.13473476469516754 = 0.12754583358764648 + 0.001 * 7.188936710357666
Epoch 340, val loss: 0.8287294507026672
Epoch 350, training loss: 0.11772529780864716 = 0.11054559797048569 + 0.001 * 7.179696559906006
Epoch 350, val loss: 0.8422313332557678
Epoch 360, training loss: 0.10324829071760178 = 0.09607426077127457 + 0.001 * 7.17402982711792
Epoch 360, val loss: 0.8571178913116455
Epoch 370, training loss: 0.09094677120447159 = 0.08377496153116226 + 0.001 * 7.171807289123535
Epoch 370, val loss: 0.8729216456413269
Epoch 380, training loss: 0.08051081746816635 = 0.07333283126354218 + 0.001 * 7.177987575531006
Epoch 380, val loss: 0.8893409371376038
Epoch 390, training loss: 0.07162927836179733 = 0.06446728855371475 + 0.001 * 7.16199254989624
Epoch 390, val loss: 0.905996561050415
Epoch 400, training loss: 0.06409347057342529 = 0.05693130940198898 + 0.001 * 7.162164211273193
Epoch 400, val loss: 0.9226192831993103
Epoch 410, training loss: 0.05767631158232689 = 0.05050988122820854 + 0.001 * 7.16642951965332
Epoch 410, val loss: 0.9390482306480408
Epoch 420, training loss: 0.05217044800519943 = 0.045010633766651154 + 0.001 * 7.159814357757568
Epoch 420, val loss: 0.95515376329422
Epoch 430, training loss: 0.047433506697416306 = 0.040281716734170914 + 0.001 * 7.151791095733643
Epoch 430, val loss: 0.9710269570350647
Epoch 440, training loss: 0.043349508196115494 = 0.03620021417737007 + 0.001 * 7.149293422698975
Epoch 440, val loss: 0.986569881439209
Epoch 450, training loss: 0.03981427475810051 = 0.032666075974702835 + 0.001 * 7.148199558258057
Epoch 450, val loss: 1.0017377138137817
Epoch 460, training loss: 0.0367356538772583 = 0.029595542699098587 + 0.001 * 7.140110492706299
Epoch 460, val loss: 1.0165178775787354
Epoch 470, training loss: 0.03405541181564331 = 0.02691812999546528 + 0.001 * 7.1372809410095215
Epoch 470, val loss: 1.0308854579925537
Epoch 480, training loss: 0.03172413259744644 = 0.02457464300096035 + 0.001 * 7.149488925933838
Epoch 480, val loss: 1.0447897911071777
Epoch 490, training loss: 0.029652418568730354 = 0.022515855729579926 + 0.001 * 7.136561870574951
Epoch 490, val loss: 1.0582901239395142
Epoch 500, training loss: 0.027830032631754875 = 0.0207002405077219 + 0.001 * 7.129791736602783
Epoch 500, val loss: 1.0713378190994263
Epoch 510, training loss: 0.026228563860058784 = 0.019093090668320656 + 0.001 * 7.135472297668457
Epoch 510, val loss: 1.0839695930480957
Epoch 520, training loss: 0.024796079844236374 = 0.01766512170433998 + 0.001 * 7.130958557128906
Epoch 520, val loss: 1.0961493253707886
Epoch 530, training loss: 0.02352195978164673 = 0.01639159582555294 + 0.001 * 7.130364418029785
Epoch 530, val loss: 1.107922077178955
Epoch 540, training loss: 0.022379091009497643 = 0.015251808799803257 + 0.001 * 7.127281188964844
Epoch 540, val loss: 1.1193276643753052
Epoch 550, training loss: 0.021350553259253502 = 0.014228329062461853 + 0.001 * 7.1222243309021
Epoch 550, val loss: 1.1303328275680542
Epoch 560, training loss: 0.02042297273874283 = 0.013306248933076859 + 0.001 * 7.116724491119385
Epoch 560, val loss: 1.1409803628921509
Epoch 570, training loss: 0.019586995244026184 = 0.012472276575863361 + 0.001 * 7.114718437194824
Epoch 570, val loss: 1.151295781135559
Epoch 580, training loss: 0.01882239431142807 = 0.011714212596416473 + 0.001 * 7.108180522918701
Epoch 580, val loss: 1.1613521575927734
Epoch 590, training loss: 0.01813187077641487 = 0.011020800098776817 + 0.001 * 7.111069679260254
Epoch 590, val loss: 1.1711349487304688
Epoch 600, training loss: 0.01748553104698658 = 0.01038297824561596 + 0.001 * 7.1025519371032715
Epoch 600, val loss: 1.180739164352417
Epoch 610, training loss: 0.016889654099941254 = 0.009794815443456173 + 0.001 * 7.094837665557861
Epoch 610, val loss: 1.190125584602356
Epoch 620, training loss: 0.016335096210241318 = 0.00925189908593893 + 0.001 * 7.083195686340332
Epoch 620, val loss: 1.19933021068573
Epoch 630, training loss: 0.015839245170354843 = 0.008750495500862598 + 0.001 * 7.088749408721924
Epoch 630, val loss: 1.2083194255828857
Epoch 640, training loss: 0.015438957139849663 = 0.008287159726023674 + 0.001 * 7.151796817779541
Epoch 640, val loss: 1.2171313762664795
Epoch 650, training loss: 0.014941001310944557 = 0.007859100587666035 + 0.001 * 7.081900596618652
Epoch 650, val loss: 1.2257273197174072
Epoch 660, training loss: 0.014554761350154877 = 0.0074631087481975555 + 0.001 * 7.0916523933410645
Epoch 660, val loss: 1.2340846061706543
Epoch 670, training loss: 0.014166450127959251 = 0.007096373476088047 + 0.001 * 7.070075988769531
Epoch 670, val loss: 1.2422847747802734
Epoch 680, training loss: 0.01382156927138567 = 0.006756333168596029 + 0.001 * 7.065235614776611
Epoch 680, val loss: 1.2502576112747192
Epoch 690, training loss: 0.013536691665649414 = 0.006440731696784496 + 0.001 * 7.095959186553955
Epoch 690, val loss: 1.2580387592315674
Epoch 700, training loss: 0.013226582668721676 = 0.006147525738924742 + 0.001 * 7.079056739807129
Epoch 700, val loss: 1.2656049728393555
Epoch 710, training loss: 0.012971007265150547 = 0.0058746072463691235 + 0.001 * 7.096399784088135
Epoch 710, val loss: 1.2729918956756592
Epoch 720, training loss: 0.012686245143413544 = 0.005620366893708706 + 0.001 * 7.065877437591553
Epoch 720, val loss: 1.2801793813705444
Epoch 730, training loss: 0.01247610803693533 = 0.005383073817938566 + 0.001 * 7.093033790588379
Epoch 730, val loss: 1.2872352600097656
Epoch 740, training loss: 0.012221479788422585 = 0.005161484703421593 + 0.001 * 7.059994220733643
Epoch 740, val loss: 1.2940757274627686
Epoch 750, training loss: 0.012021247297525406 = 0.004954148083925247 + 0.001 * 7.067098617553711
Epoch 750, val loss: 1.3007535934448242
Epoch 760, training loss: 0.011836238205432892 = 0.004759956616908312 + 0.001 * 7.076281547546387
Epoch 760, val loss: 1.3072677850723267
Epoch 770, training loss: 0.011646656319499016 = 0.004577840678393841 + 0.001 * 7.068815231323242
Epoch 770, val loss: 1.3136885166168213
Epoch 780, training loss: 0.011454883962869644 = 0.004406904336065054 + 0.001 * 7.047979831695557
Epoch 780, val loss: 1.319898009300232
Epoch 790, training loss: 0.011296829208731651 = 0.0042462279088795185 + 0.001 * 7.050601482391357
Epoch 790, val loss: 1.3259539604187012
Epoch 800, training loss: 0.011147107928991318 = 0.004095001146197319 + 0.001 * 7.052106857299805
Epoch 800, val loss: 1.331894874572754
Epoch 810, training loss: 0.011008728295564651 = 0.0039525567553937435 + 0.001 * 7.056171894073486
Epoch 810, val loss: 1.3376821279525757
Epoch 820, training loss: 0.0108852069824934 = 0.0038181645795702934 + 0.001 * 7.067042350769043
Epoch 820, val loss: 1.343371868133545
Epoch 830, training loss: 0.01075008325278759 = 0.003691380377858877 + 0.001 * 7.058701992034912
Epoch 830, val loss: 1.348885416984558
Epoch 840, training loss: 0.010606910102069378 = 0.0035715203266590834 + 0.001 * 7.0353899002075195
Epoch 840, val loss: 1.3543014526367188
Epoch 850, training loss: 0.010487128049135208 = 0.003458139020949602 + 0.001 * 7.028989315032959
Epoch 850, val loss: 1.3595759868621826
Epoch 860, training loss: 0.010389815084636211 = 0.003350763348862529 + 0.001 * 7.039051532745361
Epoch 860, val loss: 1.3647576570510864
Epoch 870, training loss: 0.010290954262018204 = 0.0032489909790456295 + 0.001 * 7.04196310043335
Epoch 870, val loss: 1.3698248863220215
Epoch 880, training loss: 0.010183542966842651 = 0.0031524670775979757 + 0.001 * 7.031075477600098
Epoch 880, val loss: 1.3747774362564087
Epoch 890, training loss: 0.010099688544869423 = 0.0030608174856752157 + 0.001 * 7.038870334625244
Epoch 890, val loss: 1.379601240158081
Epoch 900, training loss: 0.010020207613706589 = 0.0029737227596342564 + 0.001 * 7.046483993530273
Epoch 900, val loss: 1.3843348026275635
Epoch 910, training loss: 0.009918630123138428 = 0.0028908993117511272 + 0.001 * 7.027730941772461
Epoch 910, val loss: 1.3889955282211304
Epoch 920, training loss: 0.009861497208476067 = 0.002812032587826252 + 0.001 * 7.049464225769043
Epoch 920, val loss: 1.3935461044311523
Epoch 930, training loss: 0.00976544339209795 = 0.0027370022144168615 + 0.001 * 7.028440475463867
Epoch 930, val loss: 1.3979688882827759
Epoch 940, training loss: 0.009690893813967705 = 0.0026654077228158712 + 0.001 * 7.025485515594482
Epoch 940, val loss: 1.4023369550704956
Epoch 950, training loss: 0.009615601040422916 = 0.002597140846773982 + 0.001 * 7.018460273742676
Epoch 950, val loss: 1.4065742492675781
Epoch 960, training loss: 0.009556370787322521 = 0.00253196875564754 + 0.001 * 7.024401664733887
Epoch 960, val loss: 1.4107452630996704
Epoch 970, training loss: 0.009490936063230038 = 0.002469683764502406 + 0.001 * 7.021252155303955
Epoch 970, val loss: 1.4148586988449097
Epoch 980, training loss: 0.00940992496907711 = 0.0024101622402668 + 0.001 * 6.999762058258057
Epoch 980, val loss: 1.4188554286956787
Epoch 990, training loss: 0.009377690963447094 = 0.0023532568011432886 + 0.001 * 7.0244340896606445
Epoch 990, val loss: 1.4227643013000488
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8028465998945704
The final CL Acc:0.77531, 0.00462, The final GNN Acc:0.80267, 0.00066
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13176])
remove edge: torch.Size([2, 7904])
updated graph: torch.Size([2, 10524])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.945644736289978 = 1.9370478391647339 + 0.001 * 8.596851348876953
Epoch 0, val loss: 1.9336823225021362
Epoch 10, training loss: 1.9349265098571777 = 1.9263297319412231 + 0.001 * 8.59678840637207
Epoch 10, val loss: 1.922918438911438
Epoch 20, training loss: 1.9219238758087158 = 1.9133273363113403 + 0.001 * 8.596580505371094
Epoch 20, val loss: 1.9098596572875977
Epoch 30, training loss: 1.904064416885376 = 1.8954683542251587 + 0.001 * 8.596083641052246
Epoch 30, val loss: 1.8920809030532837
Epoch 40, training loss: 1.8783773183822632 = 1.869782567024231 + 0.001 * 8.59475326538086
Epoch 40, val loss: 1.8670685291290283
Epoch 50, training loss: 1.84330415725708 = 1.8347139358520508 + 0.001 * 8.590190887451172
Epoch 50, val loss: 1.8348205089569092
Epoch 60, training loss: 1.8043708801269531 = 1.7958015203475952 + 0.001 * 8.569367408752441
Epoch 60, val loss: 1.8028007745742798
Epoch 70, training loss: 1.7676661014556885 = 1.7592127323150635 + 0.001 * 8.45333194732666
Epoch 70, val loss: 1.7731660604476929
Epoch 80, training loss: 1.7176593542099 = 1.709683895111084 + 0.001 * 7.975412845611572
Epoch 80, val loss: 1.7264480590820312
Epoch 90, training loss: 1.6486859321594238 = 1.6407690048217773 + 0.001 * 7.9169745445251465
Epoch 90, val loss: 1.6632341146469116
Epoch 100, training loss: 1.5638219118118286 = 1.5559194087982178 + 0.001 * 7.902497291564941
Epoch 100, val loss: 1.594256043434143
Epoch 110, training loss: 1.4757403135299683 = 1.4678657054901123 + 0.001 * 7.8745951652526855
Epoch 110, val loss: 1.5213919878005981
Epoch 120, training loss: 1.3914427757263184 = 1.383657455444336 + 0.001 * 7.785326957702637
Epoch 120, val loss: 1.4539167881011963
Epoch 130, training loss: 1.308582067489624 = 1.3010334968566895 + 0.001 * 7.548608779907227
Epoch 130, val loss: 1.3874454498291016
Epoch 140, training loss: 1.2234132289886475 = 1.2159810066223145 + 0.001 * 7.432199954986572
Epoch 140, val loss: 1.319718599319458
Epoch 150, training loss: 1.1336439847946167 = 1.1262539625167847 + 0.001 * 7.39001989364624
Epoch 150, val loss: 1.2498078346252441
Epoch 160, training loss: 1.0389713048934937 = 1.0316450595855713 + 0.001 * 7.326286315917969
Epoch 160, val loss: 1.1769109964370728
Epoch 170, training loss: 0.9414368867874146 = 0.9341621398925781 + 0.001 * 7.2747368812561035
Epoch 170, val loss: 1.102374792098999
Epoch 180, training loss: 0.8460166454315186 = 0.8387755155563354 + 0.001 * 7.241117000579834
Epoch 180, val loss: 1.0304757356643677
Epoch 190, training loss: 0.7587901949882507 = 0.7515609860420227 + 0.001 * 7.229212284088135
Epoch 190, val loss: 0.966725766658783
Epoch 200, training loss: 0.6832126379013062 = 0.6759910583496094 + 0.001 * 7.22155237197876
Epoch 200, val loss: 0.915202796459198
Epoch 210, training loss: 0.6190008521080017 = 0.6117832064628601 + 0.001 * 7.217645645141602
Epoch 210, val loss: 0.8762883543968201
Epoch 220, training loss: 0.5639922022819519 = 0.5567782521247864 + 0.001 * 7.213946342468262
Epoch 220, val loss: 0.8481586575508118
Epoch 230, training loss: 0.515911340713501 = 0.5087003707885742 + 0.001 * 7.210946083068848
Epoch 230, val loss: 0.8287708759307861
Epoch 240, training loss: 0.472746342420578 = 0.46553799510002136 + 0.001 * 7.2083539962768555
Epoch 240, val loss: 0.8154553174972534
Epoch 250, training loss: 0.4328441321849823 = 0.425638347864151 + 0.001 * 7.205770492553711
Epoch 250, val loss: 0.8064007759094238
Epoch 260, training loss: 0.3952677547931671 = 0.3880648612976074 + 0.001 * 7.202887058258057
Epoch 260, val loss: 0.8006424307823181
Epoch 270, training loss: 0.35986432433128357 = 0.3526650667190552 + 0.001 * 7.19926118850708
Epoch 270, val loss: 0.7981428503990173
Epoch 280, training loss: 0.32694771885871887 = 0.3197529911994934 + 0.001 * 7.194720268249512
Epoch 280, val loss: 0.7991718649864197
Epoch 290, training loss: 0.29682955145835876 = 0.2896409034729004 + 0.001 * 7.188643932342529
Epoch 290, val loss: 0.8040058016777039
Epoch 300, training loss: 0.2694045305252075 = 0.262224406003952 + 0.001 * 7.180114269256592
Epoch 300, val loss: 0.8122439384460449
Epoch 310, training loss: 0.24426977336406708 = 0.23709647357463837 + 0.001 * 7.173305034637451
Epoch 310, val loss: 0.8232502937316895
Epoch 320, training loss: 0.22094476222991943 = 0.21378278732299805 + 0.001 * 7.161967754364014
Epoch 320, val loss: 0.8365545272827148
Epoch 330, training loss: 0.19913367927074432 = 0.19198550283908844 + 0.001 * 7.1481757164001465
Epoch 330, val loss: 0.8519443273544312
Epoch 340, training loss: 0.17878742516040802 = 0.1716502159833908 + 0.001 * 7.137203216552734
Epoch 340, val loss: 0.8692913055419922
Epoch 350, training loss: 0.1600092053413391 = 0.15287910401821136 + 0.001 * 7.130099773406982
Epoch 350, val loss: 0.888398289680481
Epoch 360, training loss: 0.1428826004266739 = 0.13576017320156097 + 0.001 * 7.122429370880127
Epoch 360, val loss: 0.9091246724128723
Epoch 370, training loss: 0.1274120807647705 = 0.12029612064361572 + 0.001 * 7.115959644317627
Epoch 370, val loss: 0.9313679337501526
Epoch 380, training loss: 0.11353148519992828 = 0.10641822218894958 + 0.001 * 7.1132659912109375
Epoch 380, val loss: 0.9549134373664856
Epoch 390, training loss: 0.10114172846078873 = 0.09403105080127716 + 0.001 * 7.110678195953369
Epoch 390, val loss: 0.979529619216919
Epoch 400, training loss: 0.09013544768095016 = 0.08302739262580872 + 0.001 * 7.108056545257568
Epoch 400, val loss: 1.0050020217895508
Epoch 410, training loss: 0.08040904998779297 = 0.07330183684825897 + 0.001 * 7.1072096824646
Epoch 410, val loss: 1.0310344696044922
Epoch 420, training loss: 0.07186149060726166 = 0.06475387513637543 + 0.001 * 7.107614994049072
Epoch 420, val loss: 1.057376742362976
Epoch 430, training loss: 0.06438743323087692 = 0.05728349834680557 + 0.001 * 7.103932857513428
Epoch 430, val loss: 1.0837467908859253
Epoch 440, training loss: 0.05789122357964516 = 0.050788894295692444 + 0.001 * 7.102328300476074
Epoch 440, val loss: 1.1097862720489502
Epoch 450, training loss: 0.05226904898881912 = 0.04516560211777687 + 0.001 * 7.103444576263428
Epoch 450, val loss: 1.1352126598358154
Epoch 460, training loss: 0.047408170998096466 = 0.040307145565748215 + 0.001 * 7.101024150848389
Epoch 460, val loss: 1.1598845720291138
Epoch 470, training loss: 0.04320935532450676 = 0.03610977903008461 + 0.001 * 7.099575996398926
Epoch 470, val loss: 1.1835758686065674
Epoch 480, training loss: 0.03957589715719223 = 0.032478827983140945 + 0.001 * 7.097067356109619
Epoch 480, val loss: 1.206309199333191
Epoch 490, training loss: 0.036434419453144073 = 0.029330851510167122 + 0.001 * 7.103569030761719
Epoch 490, val loss: 1.228053331375122
Epoch 500, training loss: 0.03369014337658882 = 0.026593320071697235 + 0.001 * 7.096822261810303
Epoch 500, val loss: 1.2488315105438232
Epoch 510, training loss: 0.03129898011684418 = 0.024203727021813393 + 0.001 * 7.09525203704834
Epoch 510, val loss: 1.2686843872070312
Epoch 520, training loss: 0.029204130172729492 = 0.022110380232334137 + 0.001 * 7.093748569488525
Epoch 520, val loss: 1.2876445055007935
Epoch 530, training loss: 0.027362335473299026 = 0.020270733162760735 + 0.001 * 7.091602802276611
Epoch 530, val loss: 1.3057861328125
Epoch 540, training loss: 0.025753822177648544 = 0.0186478104442358 + 0.001 * 7.106010913848877
Epoch 540, val loss: 1.323186993598938
Epoch 550, training loss: 0.024302814155817032 = 0.017211008816957474 + 0.001 * 7.091805458068848
Epoch 550, val loss: 1.3397876024246216
Epoch 560, training loss: 0.023023206740617752 = 0.015934359282255173 + 0.001 * 7.088846683502197
Epoch 560, val loss: 1.3557156324386597
Epoch 570, training loss: 0.021883763372898102 = 0.014796588569879532 + 0.001 * 7.087174415588379
Epoch 570, val loss: 1.3710345029830933
Epoch 580, training loss: 0.020864536985754967 = 0.01377879735082388 + 0.001 * 7.0857391357421875
Epoch 580, val loss: 1.3856862783432007
Epoch 590, training loss: 0.019949600100517273 = 0.01286512240767479 + 0.001 * 7.084477424621582
Epoch 590, val loss: 1.3997219800949097
Epoch 600, training loss: 0.01912614144384861 = 0.012042229995131493 + 0.001 * 7.0839104652404785
Epoch 600, val loss: 1.4132039546966553
Epoch 610, training loss: 0.018394658342003822 = 0.011299068108201027 + 0.001 * 7.095589637756348
Epoch 610, val loss: 1.426175594329834
Epoch 620, training loss: 0.017708726227283478 = 0.010625706985592842 + 0.001 * 7.083019256591797
Epoch 620, val loss: 1.4385724067687988
Epoch 630, training loss: 0.017094777897000313 = 0.010013681836426258 + 0.001 * 7.0810956954956055
Epoch 630, val loss: 1.4505200386047363
Epoch 640, training loss: 0.016533728688955307 = 0.009455876424908638 + 0.001 * 7.077852249145508
Epoch 640, val loss: 1.462025761604309
Epoch 650, training loss: 0.016022197902202606 = 0.008946114219725132 + 0.001 * 7.076083660125732
Epoch 650, val loss: 1.473105788230896
Epoch 660, training loss: 0.015557871200144291 = 0.008479072712361813 + 0.001 * 7.078798294067383
Epoch 660, val loss: 1.483782172203064
Epoch 670, training loss: 0.015124762430787086 = 0.00805015116930008 + 0.001 * 7.074611186981201
Epoch 670, val loss: 1.4940812587738037
Epoch 680, training loss: 0.014730631373822689 = 0.007655298803001642 + 0.001 * 7.075332164764404
Epoch 680, val loss: 1.5040056705474854
Epoch 690, training loss: 0.014360373839735985 = 0.007290996145457029 + 0.001 * 7.069377422332764
Epoch 690, val loss: 1.5136010646820068
Epoch 700, training loss: 0.014026990160346031 = 0.006954144220799208 + 0.001 * 7.072845458984375
Epoch 700, val loss: 1.5228585004806519
Epoch 710, training loss: 0.013713874854147434 = 0.006642025895416737 + 0.001 * 7.071848392486572
Epoch 710, val loss: 1.5317960977554321
Epoch 720, training loss: 0.013418948277831078 = 0.006352275609970093 + 0.001 * 7.0666728019714355
Epoch 720, val loss: 1.5404521226882935
Epoch 730, training loss: 0.013144449330866337 = 0.006082809995859861 + 0.001 * 7.061638832092285
Epoch 730, val loss: 1.5488221645355225
Epoch 740, training loss: 0.01290125772356987 = 0.0058317165821790695 + 0.001 * 7.069540500640869
Epoch 740, val loss: 1.5569225549697876
Epoch 750, training loss: 0.01265875156968832 = 0.005597367882728577 + 0.001 * 7.061383247375488
Epoch 750, val loss: 1.5647523403167725
Epoch 760, training loss: 0.01246588584035635 = 0.005378325469791889 + 0.001 * 7.087560176849365
Epoch 760, val loss: 1.5723334550857544
Epoch 770, training loss: 0.012234898284077644 = 0.00517325708642602 + 0.001 * 7.06164026260376
Epoch 770, val loss: 1.5796709060668945
Epoch 780, training loss: 0.012032046914100647 = 0.0049810148775577545 + 0.001 * 7.051031589508057
Epoch 780, val loss: 1.586792230606079
Epoch 790, training loss: 0.011849075555801392 = 0.004800480790436268 + 0.001 * 7.0485944747924805
Epoch 790, val loss: 1.593681812286377
Epoch 800, training loss: 0.011687707155942917 = 0.004630731884390116 + 0.001 * 7.056975364685059
Epoch 800, val loss: 1.6003576517105103
Epoch 810, training loss: 0.011524391360580921 = 0.00447092205286026 + 0.001 * 7.053469181060791
Epoch 810, val loss: 1.6068490743637085
Epoch 820, training loss: 0.011363411322236061 = 0.004320286214351654 + 0.001 * 7.043124198913574
Epoch 820, val loss: 1.6131362915039062
Epoch 830, training loss: 0.011228938587009907 = 0.004178173374384642 + 0.001 * 7.050765037536621
Epoch 830, val loss: 1.6193004846572876
Epoch 840, training loss: 0.011093747802078724 = 0.004043887834995985 + 0.001 * 7.049859523773193
Epoch 840, val loss: 1.6252095699310303
Epoch 850, training loss: 0.010954827070236206 = 0.00391687685623765 + 0.001 * 7.037950038909912
Epoch 850, val loss: 1.6309585571289062
Epoch 860, training loss: 0.010855868458747864 = 0.0037966081872582436 + 0.001 * 7.05925989151001
Epoch 860, val loss: 1.6365629434585571
Epoch 870, training loss: 0.010722975246608257 = 0.0036825889255851507 + 0.001 * 7.040385723114014
Epoch 870, val loss: 1.6419821977615356
Epoch 880, training loss: 0.010608261451125145 = 0.003574426518753171 + 0.001 * 7.033834934234619
Epoch 880, val loss: 1.6472525596618652
Epoch 890, training loss: 0.010518000461161137 = 0.0034716895315796137 + 0.001 * 7.046310901641846
Epoch 890, val loss: 1.6523833274841309
Epoch 900, training loss: 0.010421046987175941 = 0.0033740203361958265 + 0.001 * 7.04702615737915
Epoch 900, val loss: 1.657355785369873
Epoch 910, training loss: 0.010317903943359852 = 0.003281099023297429 + 0.001 * 7.03680419921875
Epoch 910, val loss: 1.6621955633163452
Epoch 920, training loss: 0.010219424031674862 = 0.003192628500983119 + 0.001 * 7.026795387268066
Epoch 920, val loss: 1.6668976545333862
Epoch 930, training loss: 0.010145644657313824 = 0.0031082979403436184 + 0.001 * 7.037346363067627
Epoch 930, val loss: 1.671494483947754
Epoch 940, training loss: 0.010053513571619987 = 0.00302788894623518 + 0.001 * 7.025624752044678
Epoch 940, val loss: 1.6759458780288696
Epoch 950, training loss: 0.009973915293812752 = 0.00295112538151443 + 0.001 * 7.022789478302002
Epoch 950, val loss: 1.680270791053772
Epoch 960, training loss: 0.009944206103682518 = 0.0028778119012713432 + 0.001 * 7.066394329071045
Epoch 960, val loss: 1.6844730377197266
Epoch 970, training loss: 0.009821774438023567 = 0.00280772615224123 + 0.001 * 7.014047622680664
Epoch 970, val loss: 1.688568353652954
Epoch 980, training loss: 0.009759176522493362 = 0.0027406918816268444 + 0.001 * 7.018483638763428
Epoch 980, val loss: 1.6925523281097412
Epoch 990, training loss: 0.009720971807837486 = 0.0026765253860503435 + 0.001 * 7.0444464683532715
Epoch 990, val loss: 1.6964399814605713
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.9555745124816895 = 1.9469777345657349 + 0.001 * 8.596831321716309
Epoch 0, val loss: 1.9427967071533203
Epoch 10, training loss: 1.9451714754104614 = 1.9365746974945068 + 0.001 * 8.596745491027832
Epoch 10, val loss: 1.932737946510315
Epoch 20, training loss: 1.9319109916687012 = 1.9233145713806152 + 0.001 * 8.596466064453125
Epoch 20, val loss: 1.919561505317688
Epoch 30, training loss: 1.9128227233886719 = 1.9042268991470337 + 0.001 * 8.595773696899414
Epoch 30, val loss: 1.9004100561141968
Epoch 40, training loss: 1.8846269845962524 = 1.876033067703247 + 0.001 * 8.593888282775879
Epoch 40, val loss: 1.8723900318145752
Epoch 50, training loss: 1.8463338613510132 = 1.837746500968933 + 0.001 * 8.587342262268066
Epoch 50, val loss: 1.8361536264419556
Epoch 60, training loss: 1.8056344985961914 = 1.7970757484436035 + 0.001 * 8.558785438537598
Epoch 60, val loss: 1.8019332885742188
Epoch 70, training loss: 1.769716739654541 = 1.761309266090393 + 0.001 * 8.407449722290039
Epoch 70, val loss: 1.7734887599945068
Epoch 80, training loss: 1.7230767011642456 = 1.7150763273239136 + 0.001 * 8.000340461730957
Epoch 80, val loss: 1.731765627861023
Epoch 90, training loss: 1.6591228246688843 = 1.6512811183929443 + 0.001 * 7.841673374176025
Epoch 90, val loss: 1.673667073249817
Epoch 100, training loss: 1.5755574703216553 = 1.5679079294204712 + 0.001 * 7.649533748626709
Epoch 100, val loss: 1.6008367538452148
Epoch 110, training loss: 1.4789234399795532 = 1.4714547395706177 + 0.001 * 7.468647480010986
Epoch 110, val loss: 1.5184861421585083
Epoch 120, training loss: 1.3775731325149536 = 1.3701788187026978 + 0.001 * 7.394345760345459
Epoch 120, val loss: 1.4342312812805176
Epoch 130, training loss: 1.2729943990707397 = 1.265662431716919 + 0.001 * 7.331940174102783
Epoch 130, val loss: 1.3477357625961304
Epoch 140, training loss: 1.1646761894226074 = 1.1574194431304932 + 0.001 * 7.256689548492432
Epoch 140, val loss: 1.2590076923370361
Epoch 150, training loss: 1.0552979707717896 = 1.048094630241394 + 0.001 * 7.203334808349609
Epoch 150, val loss: 1.1698476076126099
Epoch 160, training loss: 0.9503554701805115 = 0.9431735277175903 + 0.001 * 7.181941986083984
Epoch 160, val loss: 1.084731936454773
Epoch 170, training loss: 0.8551298379898071 = 0.8479660153388977 + 0.001 * 7.16384744644165
Epoch 170, val loss: 1.0084534883499146
Epoch 180, training loss: 0.7721168994903564 = 0.7649723887443542 + 0.001 * 7.144521236419678
Epoch 180, val loss: 0.9430786967277527
Epoch 190, training loss: 0.7004300951957703 = 0.6933102607727051 + 0.001 * 7.119843006134033
Epoch 190, val loss: 0.8885941505432129
Epoch 200, training loss: 0.6371546387672424 = 0.6300553679466248 + 0.001 * 7.099277496337891
Epoch 200, val loss: 0.8430802226066589
Epoch 210, training loss: 0.5791967511177063 = 0.572114109992981 + 0.001 * 7.082669258117676
Epoch 210, val loss: 0.8045444488525391
Epoch 220, training loss: 0.5249132513999939 = 0.5178384184837341 + 0.001 * 7.0748372077941895
Epoch 220, val loss: 0.771812915802002
Epoch 230, training loss: 0.4741682708263397 = 0.46709689497947693 + 0.001 * 7.071386814117432
Epoch 230, val loss: 0.7446715831756592
Epoch 240, training loss: 0.4276036322116852 = 0.42053431272506714 + 0.001 * 7.069304943084717
Epoch 240, val loss: 0.723470151424408
Epoch 250, training loss: 0.3859255015850067 = 0.37885794043540955 + 0.001 * 7.06754732131958
Epoch 250, val loss: 0.7079489231109619
Epoch 260, training loss: 0.34935635328292847 = 0.3422895073890686 + 0.001 * 7.066859245300293
Epoch 260, val loss: 0.6974703073501587
Epoch 270, training loss: 0.3175770044326782 = 0.3105137348175049 + 0.001 * 7.06325626373291
Epoch 270, val loss: 0.691149890422821
Epoch 280, training loss: 0.28993362188339233 = 0.2828714847564697 + 0.001 * 7.062145233154297
Epoch 280, val loss: 0.688089907169342
Epoch 290, training loss: 0.26558932662010193 = 0.2585311830043793 + 0.001 * 7.058155059814453
Epoch 290, val loss: 0.6874600052833557
Epoch 300, training loss: 0.243704155087471 = 0.23664909601211548 + 0.001 * 7.0550537109375
Epoch 300, val loss: 0.6884576678276062
Epoch 310, training loss: 0.2234981656074524 = 0.21643070876598358 + 0.001 * 7.0674567222595215
Epoch 310, val loss: 0.6904826760292053
Epoch 320, training loss: 0.2042716145515442 = 0.1972174048423767 + 0.001 * 7.054211139678955
Epoch 320, val loss: 0.6930065155029297
Epoch 330, training loss: 0.18564200401306152 = 0.1785944253206253 + 0.001 * 7.047577381134033
Epoch 330, val loss: 0.6955600380897522
Epoch 340, training loss: 0.16748327016830444 = 0.16043958067893982 + 0.001 * 7.043691158294678
Epoch 340, val loss: 0.6980269551277161
Epoch 350, training loss: 0.14998701214790344 = 0.1429472118616104 + 0.001 * 7.039799690246582
Epoch 350, val loss: 0.7004942893981934
Epoch 360, training loss: 0.13351690769195557 = 0.126475989818573 + 0.001 * 7.040920257568359
Epoch 360, val loss: 0.7032856941223145
Epoch 370, training loss: 0.11840929090976715 = 0.11136042326688766 + 0.001 * 7.04886531829834
Epoch 370, val loss: 0.706493616104126
Epoch 380, training loss: 0.10482952743768692 = 0.09779035300016403 + 0.001 * 7.039175033569336
Epoch 380, val loss: 0.7103742957115173
Epoch 390, training loss: 0.09283407777547836 = 0.08580195903778076 + 0.001 * 7.032115459442139
Epoch 390, val loss: 0.7149640917778015
Epoch 400, training loss: 0.08236134797334671 = 0.07533028721809387 + 0.001 * 7.031063079833984
Epoch 400, val loss: 0.7202099561691284
Epoch 410, training loss: 0.07328824698925018 = 0.06625968962907791 + 0.001 * 7.028555393218994
Epoch 410, val loss: 0.7260617017745972
Epoch 420, training loss: 0.06551101058721542 = 0.0584457665681839 + 0.001 * 7.06524133682251
Epoch 420, val loss: 0.7324638366699219
Epoch 430, training loss: 0.058767177164554596 = 0.05174075812101364 + 0.001 * 7.026420593261719
Epoch 430, val loss: 0.7393383979797363
Epoch 440, training loss: 0.05302612483501434 = 0.04599630460143089 + 0.001 * 7.029818058013916
Epoch 440, val loss: 0.7466323971748352
Epoch 450, training loss: 0.04807309806346893 = 0.04104895889759064 + 0.001 * 7.024138450622559
Epoch 450, val loss: 0.7541924118995667
Epoch 460, training loss: 0.04380994290113449 = 0.03678546100854874 + 0.001 * 7.024480819702148
Epoch 460, val loss: 0.762053906917572
Epoch 470, training loss: 0.040121179074048996 = 0.03309785574674606 + 0.001 * 7.023322105407715
Epoch 470, val loss: 0.7700897455215454
Epoch 480, training loss: 0.03692179173231125 = 0.029897641390562057 + 0.001 * 7.024150848388672
Epoch 480, val loss: 0.7781867980957031
Epoch 490, training loss: 0.034133534878492355 = 0.027111021801829338 + 0.001 * 7.0225114822387695
Epoch 490, val loss: 0.7863636612892151
Epoch 500, training loss: 0.031695012003183365 = 0.02467578649520874 + 0.001 * 7.019224643707275
Epoch 500, val loss: 0.7945241928100586
Epoch 510, training loss: 0.029568683356046677 = 0.022539488971233368 + 0.001 * 7.0291948318481445
Epoch 510, val loss: 0.8026270866394043
Epoch 520, training loss: 0.02768256887793541 = 0.02065914496779442 + 0.001 * 7.02342414855957
Epoch 520, val loss: 0.8105834126472473
Epoch 530, training loss: 0.026015691459178925 = 0.018997909501194954 + 0.001 * 7.0177812576293945
Epoch 530, val loss: 0.8183974623680115
Epoch 540, training loss: 0.024540510028600693 = 0.01752527803182602 + 0.001 * 7.015231132507324
Epoch 540, val loss: 0.8260643482208252
Epoch 550, training loss: 0.023230595514178276 = 0.016215534880757332 + 0.001 * 7.015060901641846
Epoch 550, val loss: 0.833564281463623
Epoch 560, training loss: 0.02206343412399292 = 0.015046440064907074 + 0.001 * 7.016993999481201
Epoch 560, val loss: 0.8408896327018738
Epoch 570, training loss: 0.02101186290383339 = 0.013999388553202152 + 0.001 * 7.012474536895752
Epoch 570, val loss: 0.8480213284492493
Epoch 580, training loss: 0.02007276378571987 = 0.013058487325906754 + 0.001 * 7.014275550842285
Epoch 580, val loss: 0.8549878001213074
Epoch 590, training loss: 0.019227249547839165 = 0.012210402637720108 + 0.001 * 7.016846656799316
Epoch 590, val loss: 0.86178058385849
Epoch 600, training loss: 0.018455225974321365 = 0.011443587020039558 + 0.001 * 7.01163911819458
Epoch 600, val loss: 0.8683934807777405
Epoch 610, training loss: 0.017758779227733612 = 0.010748276486992836 + 0.001 * 7.010502815246582
Epoch 610, val loss: 0.8748364448547363
Epoch 620, training loss: 0.017121735960245132 = 0.01011605653911829 + 0.001 * 7.005679130554199
Epoch 620, val loss: 0.8811054825782776
Epoch 630, training loss: 0.016551923006772995 = 0.009539663791656494 + 0.001 * 7.012258052825928
Epoch 630, val loss: 0.8872156739234924
Epoch 640, training loss: 0.01601729542016983 = 0.009012923575937748 + 0.001 * 7.00437068939209
Epoch 640, val loss: 0.8931497931480408
Epoch 650, training loss: 0.01554432325065136 = 0.008530289866030216 + 0.001 * 7.01403284072876
Epoch 650, val loss: 0.8989339470863342
Epoch 660, training loss: 0.015088735148310661 = 0.008087100461125374 + 0.001 * 7.001634120941162
Epoch 660, val loss: 0.9045678377151489
Epoch 670, training loss: 0.014678183943033218 = 0.007679203990846872 + 0.001 * 6.9989800453186035
Epoch 670, val loss: 0.9100512862205505
Epoch 680, training loss: 0.014308612793684006 = 0.007303026504814625 + 0.001 * 7.005586624145508
Epoch 680, val loss: 0.9153961539268494
Epoch 690, training loss: 0.013951344415545464 = 0.006955397315323353 + 0.001 * 6.995947360992432
Epoch 690, val loss: 0.9205982685089111
Epoch 700, training loss: 0.01364370808005333 = 0.006633606739342213 + 0.001 * 7.010100841522217
Epoch 700, val loss: 0.9256693124771118
Epoch 710, training loss: 0.01333426870405674 = 0.006335141137242317 + 0.001 * 6.9991278648376465
Epoch 710, val loss: 0.9306040406227112
Epoch 720, training loss: 0.013054998591542244 = 0.0060577611438930035 + 0.001 * 6.997236728668213
Epoch 720, val loss: 0.935417652130127
Epoch 730, training loss: 0.012792792171239853 = 0.005799530074000359 + 0.001 * 6.993261814117432
Epoch 730, val loss: 0.9401106238365173
Epoch 740, training loss: 0.01255567092448473 = 0.005558772943913937 + 0.001 * 6.9968976974487305
Epoch 740, val loss: 0.9446871876716614
Epoch 750, training loss: 0.012321840971708298 = 0.005333922803401947 + 0.001 * 6.987917423248291
Epoch 750, val loss: 0.9491518139839172
Epoch 760, training loss: 0.01212176214903593 = 0.005123603157699108 + 0.001 * 6.9981584548950195
Epoch 760, val loss: 0.9535040855407715
Epoch 770, training loss: 0.011918413452804089 = 0.004926605615764856 + 0.001 * 6.991807460784912
Epoch 770, val loss: 0.9577518105506897
Epoch 780, training loss: 0.011725787073373795 = 0.004741796292364597 + 0.001 * 6.9839911460876465
Epoch 780, val loss: 0.9618929028511047
Epoch 790, training loss: 0.011564308777451515 = 0.004568206146359444 + 0.001 * 6.996102809906006
Epoch 790, val loss: 0.9659419059753418
Epoch 800, training loss: 0.011392861604690552 = 0.0044049774296581745 + 0.001 * 6.987884521484375
Epoch 800, val loss: 0.9698930382728577
Epoch 810, training loss: 0.011231256648898125 = 0.004251265898346901 + 0.001 * 6.9799909591674805
Epoch 810, val loss: 0.9737591743469238
Epoch 820, training loss: 0.011095969006419182 = 0.004106338135898113 + 0.001 * 6.989631175994873
Epoch 820, val loss: 0.9775304794311523
Epoch 830, training loss: 0.010943470522761345 = 0.003969548270106316 + 0.001 * 6.973921298980713
Epoch 830, val loss: 0.9812204837799072
Epoch 840, training loss: 0.010831035673618317 = 0.003840303746983409 + 0.001 * 6.990731716156006
Epoch 840, val loss: 0.9848228096961975
Epoch 850, training loss: 0.010689529590308666 = 0.0037180702202022076 + 0.001 * 6.971458911895752
Epoch 850, val loss: 0.9883488416671753
Epoch 860, training loss: 0.010571413673460484 = 0.0036023196298629045 + 0.001 * 6.969093322753906
Epoch 860, val loss: 0.991793692111969
Epoch 870, training loss: 0.010461821220815182 = 0.003492586547508836 + 0.001 * 6.969234466552734
Epoch 870, val loss: 0.9951618313789368
Epoch 880, training loss: 0.010374215431511402 = 0.0033885245211422443 + 0.001 * 6.985690593719482
Epoch 880, val loss: 0.9984647631645203
Epoch 890, training loss: 0.010254866443574429 = 0.0032897235359996557 + 0.001 * 6.965142250061035
Epoch 890, val loss: 1.0016871690750122
Epoch 900, training loss: 0.010160696692764759 = 0.0031958112958818674 + 0.001 * 6.9648847579956055
Epoch 900, val loss: 1.0048414468765259
Epoch 910, training loss: 0.010064558126032352 = 0.003106459742411971 + 0.001 * 6.9580979347229
Epoch 910, val loss: 1.0079282522201538
Epoch 920, training loss: 0.009989601559937 = 0.0030214041471481323 + 0.001 * 6.968196868896484
Epoch 920, val loss: 1.0109422206878662
Epoch 930, training loss: 0.009906459599733353 = 0.0029403693042695522 + 0.001 * 6.966089725494385
Epoch 930, val loss: 1.0139046907424927
Epoch 940, training loss: 0.0098226647824049 = 0.0028631072491407394 + 0.001 * 6.959557056427002
Epoch 940, val loss: 1.0167772769927979
Epoch 950, training loss: 0.009748649783432484 = 0.0027893928345292807 + 0.001 * 6.959256172180176
Epoch 950, val loss: 1.0196022987365723
Epoch 960, training loss: 0.009684059768915176 = 0.0027189922984689474 + 0.001 * 6.965067386627197
Epoch 960, val loss: 1.0223603248596191
Epoch 970, training loss: 0.009627080522477627 = 0.0026517165824770927 + 0.001 * 6.975363731384277
Epoch 970, val loss: 1.0250608921051025
Epoch 980, training loss: 0.009533568285405636 = 0.0025874061975628138 + 0.001 * 6.94616174697876
Epoch 980, val loss: 1.0276987552642822
Epoch 990, training loss: 0.009490016847848892 = 0.0025258674286305904 + 0.001 * 6.96414852142334
Epoch 990, val loss: 1.0302869081497192
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 1.9523046016693115 = 1.9437077045440674 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.9422239065170288
Epoch 10, training loss: 1.9427316188812256 = 1.934134840965271 + 0.001 * 8.596793174743652
Epoch 10, val loss: 1.9330523014068604
Epoch 20, training loss: 1.930347204208374 = 1.9217506647109985 + 0.001 * 8.596579551696777
Epoch 20, val loss: 1.9208409786224365
Epoch 30, training loss: 1.9124774932861328 = 1.9038814306259155 + 0.001 * 8.596053123474121
Epoch 30, val loss: 1.9029662609100342
Epoch 40, training loss: 1.8859715461730957 = 1.877376914024353 + 0.001 * 8.594635963439941
Epoch 40, val loss: 1.876693844795227
Epoch 50, training loss: 1.8492960929870605 = 1.8407058715820312 + 0.001 * 8.590242385864258
Epoch 50, val loss: 1.8420583009719849
Epoch 60, training loss: 1.8086960315704346 = 1.8001222610473633 + 0.001 * 8.57374095916748
Epoch 60, val loss: 1.8073638677597046
Epoch 70, training loss: 1.7723222970962524 = 1.7638272047042847 + 0.001 * 8.4951171875
Epoch 70, val loss: 1.7773488759994507
Epoch 80, training loss: 1.7251394987106323 = 1.7170758247375488 + 0.001 * 8.063730239868164
Epoch 80, val loss: 1.7341253757476807
Epoch 90, training loss: 1.65910804271698 = 1.6512150764465332 + 0.001 * 7.892995357513428
Epoch 90, val loss: 1.6739792823791504
Epoch 100, training loss: 1.5726171731948853 = 1.5648393630981445 + 0.001 * 7.777821063995361
Epoch 100, val loss: 1.6008907556533813
Epoch 110, training loss: 1.4751096963882446 = 1.4674948453903198 + 0.001 * 7.614816665649414
Epoch 110, val loss: 1.5211933851242065
Epoch 120, training loss: 1.3772190809249878 = 1.369713544845581 + 0.001 * 7.505516052246094
Epoch 120, val loss: 1.4427064657211304
Epoch 130, training loss: 1.2820303440093994 = 1.274552583694458 + 0.001 * 7.477781295776367
Epoch 130, val loss: 1.3672845363616943
Epoch 140, training loss: 1.1898481845855713 = 1.1824196577072144 + 0.001 * 7.42849063873291
Epoch 140, val loss: 1.2958097457885742
Epoch 150, training loss: 1.1031646728515625 = 1.095803141593933 + 0.001 * 7.36148738861084
Epoch 150, val loss: 1.229999303817749
Epoch 160, training loss: 1.0249357223510742 = 1.0176326036453247 + 0.001 * 7.303089141845703
Epoch 160, val loss: 1.172841191291809
Epoch 170, training loss: 0.9549468159675598 = 0.9476912021636963 + 0.001 * 7.255627632141113
Epoch 170, val loss: 1.1229840517044067
Epoch 180, training loss: 0.8895700573921204 = 0.8823606967926025 + 0.001 * 7.209342002868652
Epoch 180, val loss: 1.0763952732086182
Epoch 190, training loss: 0.8248313069343567 = 0.817648708820343 + 0.001 * 7.182572364807129
Epoch 190, val loss: 1.028971791267395
Epoch 200, training loss: 0.7587416172027588 = 0.7515679597854614 + 0.001 * 7.173641204833984
Epoch 200, val loss: 0.9794495701789856
Epoch 210, training loss: 0.6922900080680847 = 0.685126781463623 + 0.001 * 7.1632256507873535
Epoch 210, val loss: 0.9296723008155823
Epoch 220, training loss: 0.6282170414924622 = 0.6210684776306152 + 0.001 * 7.148536682128906
Epoch 220, val loss: 0.8830459713935852
Epoch 230, training loss: 0.5690612196922302 = 0.5619326829910278 + 0.001 * 7.128523826599121
Epoch 230, val loss: 0.8429505228996277
Epoch 240, training loss: 0.5161985158920288 = 0.5090872645378113 + 0.001 * 7.111270427703857
Epoch 240, val loss: 0.8112165331840515
Epoch 250, training loss: 0.46984702348709106 = 0.4627528488636017 + 0.001 * 7.094183921813965
Epoch 250, val loss: 0.7879483699798584
Epoch 260, training loss: 0.4293716549873352 = 0.4222852885723114 + 0.001 * 7.086355686187744
Epoch 260, val loss: 0.7717111706733704
Epoch 270, training loss: 0.39364302158355713 = 0.386564701795578 + 0.001 * 7.078314781188965
Epoch 270, val loss: 0.7607908248901367
Epoch 280, training loss: 0.3615491986274719 = 0.35447198152542114 + 0.001 * 7.077229976654053
Epoch 280, val loss: 0.7537694573402405
Epoch 290, training loss: 0.3323144316673279 = 0.325237512588501 + 0.001 * 7.076903820037842
Epoch 290, val loss: 0.7496165633201599
Epoch 300, training loss: 0.305462121963501 = 0.2983849346637726 + 0.001 * 7.077178478240967
Epoch 300, val loss: 0.7476222515106201
Epoch 310, training loss: 0.2805432677268982 = 0.2734653353691101 + 0.001 * 7.077919960021973
Epoch 310, val loss: 0.7471998333930969
Epoch 320, training loss: 0.25690656900405884 = 0.2498275488615036 + 0.001 * 7.079024791717529
Epoch 320, val loss: 0.7480071187019348
Epoch 330, training loss: 0.23366791009902954 = 0.22658762335777283 + 0.001 * 7.080285549163818
Epoch 330, val loss: 0.7494924664497375
Epoch 340, training loss: 0.21010644733905792 = 0.20302489399909973 + 0.001 * 7.081549167633057
Epoch 340, val loss: 0.7509965896606445
Epoch 350, training loss: 0.18620836734771729 = 0.1791256219148636 + 0.001 * 7.0827507972717285
Epoch 350, val loss: 0.7524474859237671
Epoch 360, training loss: 0.16297638416290283 = 0.15589255094528198 + 0.001 * 7.0838398933410645
Epoch 360, val loss: 0.7541819214820862
Epoch 370, training loss: 0.14180795848369598 = 0.13472312688827515 + 0.001 * 7.084835529327393
Epoch 370, val loss: 0.7568946480751038
Epoch 380, training loss: 0.12352707982063293 = 0.1164412572979927 + 0.001 * 7.0858235359191895
Epoch 380, val loss: 0.7612713575363159
Epoch 390, training loss: 0.10815561562776566 = 0.10106703639030457 + 0.001 * 7.088581562042236
Epoch 390, val loss: 0.7674552202224731
Epoch 400, training loss: 0.09531644731760025 = 0.08822859078645706 + 0.001 * 7.0878586769104
Epoch 400, val loss: 0.7754011750221252
Epoch 410, training loss: 0.0845758467912674 = 0.07748698443174362 + 0.001 * 7.088858604431152
Epoch 410, val loss: 0.784782886505127
Epoch 420, training loss: 0.07554157823324203 = 0.068451888859272 + 0.001 * 7.089686393737793
Epoch 420, val loss: 0.7951885461807251
Epoch 430, training loss: 0.06788807362318039 = 0.06079762428998947 + 0.001 * 7.090447425842285
Epoch 430, val loss: 0.8063042759895325
Epoch 440, training loss: 0.061358846724033356 = 0.054266247898340225 + 0.001 * 7.092598915100098
Epoch 440, val loss: 0.8178374767303467
Epoch 450, training loss: 0.05574791505932808 = 0.04865441471338272 + 0.001 * 7.093498706817627
Epoch 450, val loss: 0.8296034336090088
Epoch 460, training loss: 0.050897907465696335 = 0.043804626911878586 + 0.001 * 7.093279838562012
Epoch 460, val loss: 0.8414433598518372
Epoch 470, training loss: 0.0466841459274292 = 0.03959064185619354 + 0.001 * 7.093503952026367
Epoch 470, val loss: 0.8532186150550842
Epoch 480, training loss: 0.04300551488995552 = 0.03591194376349449 + 0.001 * 7.093569755554199
Epoch 480, val loss: 0.8648715019226074
Epoch 490, training loss: 0.039780229330062866 = 0.032687198370695114 + 0.001 * 7.093029975891113
Epoch 490, val loss: 0.8763465285301208
Epoch 500, training loss: 0.03694898635149002 = 0.02985064499080181 + 0.001 * 7.098340034484863
Epoch 500, val loss: 0.8875780701637268
Epoch 510, training loss: 0.034439533948898315 = 0.027346139773726463 + 0.001 * 7.093395233154297
Epoch 510, val loss: 0.898575484752655
Epoch 520, training loss: 0.032220128923654556 = 0.025127483531832695 + 0.001 * 7.09264612197876
Epoch 520, val loss: 0.9093176126480103
Epoch 530, training loss: 0.03024790994822979 = 0.02315579354763031 + 0.001 * 7.09211540222168
Epoch 530, val loss: 0.9197906851768494
Epoch 540, training loss: 0.02849004417657852 = 0.021398276090621948 + 0.001 * 7.091768741607666
Epoch 540, val loss: 0.9299855828285217
Epoch 550, training loss: 0.02692485973238945 = 0.01982678845524788 + 0.001 * 7.098071575164795
Epoch 550, val loss: 0.9399012923240662
Epoch 560, training loss: 0.0255100317299366 = 0.018417583778500557 + 0.001 * 7.092447280883789
Epoch 560, val loss: 0.9495220184326172
Epoch 570, training loss: 0.02423941157758236 = 0.017150288447737694 + 0.001 * 7.089123249053955
Epoch 570, val loss: 0.9588571190834045
Epoch 580, training loss: 0.023098085075616837 = 0.016007525846362114 + 0.001 * 7.090559005737305
Epoch 580, val loss: 0.9679234027862549
Epoch 590, training loss: 0.02206066995859146 = 0.014974528923630714 + 0.001 * 7.086140155792236
Epoch 590, val loss: 0.9767293334007263
Epoch 600, training loss: 0.021125352010130882 = 0.01403826754540205 + 0.001 * 7.0870842933654785
Epoch 600, val loss: 0.9852776527404785
Epoch 610, training loss: 0.02027021534740925 = 0.013187441974878311 + 0.001 * 7.082773208618164
Epoch 610, val loss: 0.9935683608055115
Epoch 620, training loss: 0.019491801038384438 = 0.01241243164986372 + 0.001 * 7.079369068145752
Epoch 620, val loss: 1.0016162395477295
Epoch 630, training loss: 0.018803520128130913 = 0.011704797856509686 + 0.001 * 7.098721981048584
Epoch 630, val loss: 1.0094234943389893
Epoch 640, training loss: 0.01814388856291771 = 0.011057204566895962 + 0.001 * 7.086684703826904
Epoch 640, val loss: 1.0169886350631714
Epoch 650, training loss: 0.017542513087391853 = 0.010463235899806023 + 0.001 * 7.079277515411377
Epoch 650, val loss: 1.0243504047393799
Epoch 660, training loss: 0.016988050192594528 = 0.009917283430695534 + 0.001 * 7.070765495300293
Epoch 660, val loss: 1.0314979553222656
Epoch 670, training loss: 0.016486631706357002 = 0.009414383210241795 + 0.001 * 7.0722479820251465
Epoch 670, val loss: 1.0384308099746704
Epoch 680, training loss: 0.016014736145734787 = 0.00895021017640829 + 0.001 * 7.064525604248047
Epoch 680, val loss: 1.0451724529266357
Epoch 690, training loss: 0.015580154955387115 = 0.008521059527993202 + 0.001 * 7.05909538269043
Epoch 690, val loss: 1.0517226457595825
Epoch 700, training loss: 0.015182040631771088 = 0.008123493753373623 + 0.001 * 7.058546543121338
Epoch 700, val loss: 1.0580873489379883
Epoch 710, training loss: 0.014823650009930134 = 0.007754561956971884 + 0.001 * 7.069087505340576
Epoch 710, val loss: 1.0642799139022827
Epoch 720, training loss: 0.014487192034721375 = 0.0074116201139986515 + 0.001 * 7.075571537017822
Epoch 720, val loss: 1.0703039169311523
Epoch 730, training loss: 0.014160364866256714 = 0.007092262618243694 + 0.001 * 7.06810188293457
Epoch 730, val loss: 1.0761644840240479
Epoch 740, training loss: 0.01383560337126255 = 0.006794470362365246 + 0.001 * 7.041132926940918
Epoch 740, val loss: 1.0818744897842407
Epoch 750, training loss: 0.013573233038187027 = 0.006516304798424244 + 0.001 * 7.056927680969238
Epoch 750, val loss: 1.0874348878860474
Epoch 760, training loss: 0.01329994946718216 = 0.006256136577576399 + 0.001 * 7.043812274932861
Epoch 760, val loss: 1.092833161354065
Epoch 770, training loss: 0.013033113442361355 = 0.006012446247041225 + 0.001 * 7.02066707611084
Epoch 770, val loss: 1.098104476928711
Epoch 780, training loss: 0.012831971049308777 = 0.005783896893262863 + 0.001 * 7.048074245452881
Epoch 780, val loss: 1.103225827217102
Epoch 790, training loss: 0.012611905112862587 = 0.005569263827055693 + 0.001 * 7.0426411628723145
Epoch 790, val loss: 1.108237385749817
Epoch 800, training loss: 0.012383553199470043 = 0.0053674704395234585 + 0.001 * 7.016082286834717
Epoch 800, val loss: 1.1131088733673096
Epoch 810, training loss: 0.01219913735985756 = 0.005177497398108244 + 0.001 * 7.021639347076416
Epoch 810, val loss: 1.1178799867630005
Epoch 820, training loss: 0.012009389698505402 = 0.004998404067009687 + 0.001 * 7.010984897613525
Epoch 820, val loss: 1.1225183010101318
Epoch 830, training loss: 0.011875271797180176 = 0.00482946028932929 + 0.001 * 7.045811176300049
Epoch 830, val loss: 1.1270512342453003
Epoch 840, training loss: 0.011681346222758293 = 0.0046698530204594135 + 0.001 * 7.0114922523498535
Epoch 840, val loss: 1.1314795017242432
Epoch 850, training loss: 0.011592085473239422 = 0.004518961068242788 + 0.001 * 7.073123931884766
Epoch 850, val loss: 1.1357897520065308
Epoch 860, training loss: 0.011378101073205471 = 0.004376099910587072 + 0.001 * 7.00200080871582
Epoch 860, val loss: 1.1400225162506104
Epoch 870, training loss: 0.0112686138600111 = 0.004240742884576321 + 0.001 * 7.027870178222656
Epoch 870, val loss: 1.144134521484375
Epoch 880, training loss: 0.0111205093562603 = 0.004112358205020428 + 0.001 * 7.008150577545166
Epoch 880, val loss: 1.1481603384017944
Epoch 890, training loss: 0.01098172552883625 = 0.003990475088357925 + 0.001 * 6.9912495613098145
Epoch 890, val loss: 1.1521012783050537
Epoch 900, training loss: 0.010860527865588665 = 0.0038746932987123728 + 0.001 * 6.985834121704102
Epoch 900, val loss: 1.1559370756149292
Epoch 910, training loss: 0.010740648955106735 = 0.0037645779084414244 + 0.001 * 6.976070404052734
Epoch 910, val loss: 1.1596992015838623
Epoch 920, training loss: 0.010636737570166588 = 0.003659770591184497 + 0.001 * 6.976966857910156
Epoch 920, val loss: 1.163368582725525
Epoch 930, training loss: 0.010529270395636559 = 0.0035599449183791876 + 0.001 * 6.969325542449951
Epoch 930, val loss: 1.16697359085083
Epoch 940, training loss: 0.010437353514134884 = 0.0034647779539227486 + 0.001 * 6.9725751876831055
Epoch 940, val loss: 1.1704814434051514
Epoch 950, training loss: 0.010349921882152557 = 0.003373987041413784 + 0.001 * 6.975934028625488
Epoch 950, val loss: 1.1739115715026855
Epoch 960, training loss: 0.010255939327180386 = 0.003287330735474825 + 0.001 * 6.968608379364014
Epoch 960, val loss: 1.177286148071289
Epoch 970, training loss: 0.010186461731791496 = 0.0032045356929302216 + 0.001 * 6.981925010681152
Epoch 970, val loss: 1.1805779933929443
Epoch 980, training loss: 0.01009635254740715 = 0.003125367686152458 + 0.001 * 6.970984935760498
Epoch 980, val loss: 1.183799386024475
Epoch 990, training loss: 0.010005757212638855 = 0.0030496346298605204 + 0.001 * 6.956122398376465
Epoch 990, val loss: 1.1869596242904663
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8360569319978914
The final CL Acc:0.77284, 0.01062, The final GNN Acc:0.83676, 0.00066
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10530])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.966710090637207 = 1.958113193511963 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.9553776979446411
Epoch 10, training loss: 1.956475019454956 = 1.9478782415390015 + 0.001 * 8.59680461883545
Epoch 10, val loss: 1.9455631971359253
Epoch 20, training loss: 1.9438973665237427 = 1.9353007078170776 + 0.001 * 8.596648216247559
Epoch 20, val loss: 1.933018684387207
Epoch 30, training loss: 1.9264546632766724 = 1.917858362197876 + 0.001 * 8.596272468566895
Epoch 30, val loss: 1.9152178764343262
Epoch 40, training loss: 1.900795578956604 = 1.892200231552124 + 0.001 * 8.595292091369629
Epoch 40, val loss: 1.8889083862304688
Epoch 50, training loss: 1.8647797107696533 = 1.8561877012252808 + 0.001 * 8.592002868652344
Epoch 50, val loss: 1.8533486127853394
Epoch 60, training loss: 1.8246526718139648 = 1.8160744905471802 + 0.001 * 8.578145027160645
Epoch 60, val loss: 1.817893147468567
Epoch 70, training loss: 1.7936116456985474 = 1.785094141960144 + 0.001 * 8.517521858215332
Epoch 70, val loss: 1.7941784858703613
Epoch 80, training loss: 1.7594413757324219 = 1.7512693405151367 + 0.001 * 8.172021865844727
Epoch 80, val loss: 1.7650725841522217
Epoch 90, training loss: 1.712193489074707 = 1.7041891813278198 + 0.001 * 8.00426197052002
Epoch 90, val loss: 1.724104881286621
Epoch 100, training loss: 1.645018458366394 = 1.6371676921844482 + 0.001 * 7.85074520111084
Epoch 100, val loss: 1.6673892736434937
Epoch 110, training loss: 1.5575294494628906 = 1.54988694190979 + 0.001 * 7.642559051513672
Epoch 110, val loss: 1.5948724746704102
Epoch 120, training loss: 1.4593230485916138 = 1.4518202543258667 + 0.001 * 7.502786636352539
Epoch 120, val loss: 1.516298532485962
Epoch 130, training loss: 1.358950138092041 = 1.3514747619628906 + 0.001 * 7.475414276123047
Epoch 130, val loss: 1.4372529983520508
Epoch 140, training loss: 1.2595278024673462 = 1.252102255821228 + 0.001 * 7.425557613372803
Epoch 140, val loss: 1.3610084056854248
Epoch 150, training loss: 1.1618773937225342 = 1.1545206308364868 + 0.001 * 7.356749534606934
Epoch 150, val loss: 1.2878497838974
Epoch 160, training loss: 1.0682716369628906 = 1.0609744787216187 + 0.001 * 7.297216892242432
Epoch 160, val loss: 1.219377040863037
Epoch 170, training loss: 0.9797912836074829 = 0.9725527763366699 + 0.001 * 7.238532066345215
Epoch 170, val loss: 1.155524730682373
Epoch 180, training loss: 0.8960080146789551 = 0.8888168334960938 + 0.001 * 7.191208362579346
Epoch 180, val loss: 1.095442533493042
Epoch 190, training loss: 0.8165777325630188 = 0.8094035387039185 + 0.001 * 7.174184322357178
Epoch 190, val loss: 1.038962483406067
Epoch 200, training loss: 0.7421138882637024 = 0.7349455952644348 + 0.001 * 7.168304920196533
Epoch 200, val loss: 0.9877023100852966
Epoch 210, training loss: 0.673568069934845 = 0.6664084196090698 + 0.001 * 7.1596598625183105
Epoch 210, val loss: 0.9437404870986938
Epoch 220, training loss: 0.6111202239990234 = 0.6039716601371765 + 0.001 * 7.1485772132873535
Epoch 220, val loss: 0.9081268906593323
Epoch 230, training loss: 0.5543950200080872 = 0.547260582447052 + 0.001 * 7.134456157684326
Epoch 230, val loss: 0.8808145523071289
Epoch 240, training loss: 0.5028938055038452 = 0.4957736134529114 + 0.001 * 7.120217323303223
Epoch 240, val loss: 0.8610937595367432
Epoch 250, training loss: 0.45622915029525757 = 0.4491276443004608 + 0.001 * 7.101503372192383
Epoch 250, val loss: 0.8477156758308411
Epoch 260, training loss: 0.4141022562980652 = 0.40701964497566223 + 0.001 * 7.082602500915527
Epoch 260, val loss: 0.8397483825683594
Epoch 270, training loss: 0.37609854340553284 = 0.3690265119075775 + 0.001 * 7.072032451629639
Epoch 270, val loss: 0.8363184928894043
Epoch 280, training loss: 0.3415098488330841 = 0.33444923162460327 + 0.001 * 7.060621738433838
Epoch 280, val loss: 0.836237907409668
Epoch 290, training loss: 0.30948105454444885 = 0.30243033170700073 + 0.001 * 7.0507307052612305
Epoch 290, val loss: 0.8385536670684814
Epoch 300, training loss: 0.2790849804878235 = 0.27203884720802307 + 0.001 * 7.046131134033203
Epoch 300, val loss: 0.842302143573761
Epoch 310, training loss: 0.24958623945713043 = 0.24254465103149414 + 0.001 * 7.041581630706787
Epoch 310, val loss: 0.8470198512077332
Epoch 320, training loss: 0.22073544561862946 = 0.21369855105876923 + 0.001 * 7.036892890930176
Epoch 320, val loss: 0.8524949550628662
Epoch 330, training loss: 0.19301196932792664 = 0.18597882986068726 + 0.001 * 7.033139705657959
Epoch 330, val loss: 0.8587555289268494
Epoch 340, training loss: 0.16734015941619873 = 0.16030758619308472 + 0.001 * 7.032568454742432
Epoch 340, val loss: 0.8660110831260681
Epoch 350, training loss: 0.14449875056743622 = 0.1374734491109848 + 0.001 * 7.025296211242676
Epoch 350, val loss: 0.8745463490486145
Epoch 360, training loss: 0.12480739504098892 = 0.11778683960437775 + 0.001 * 7.020552158355713
Epoch 360, val loss: 0.8843917846679688
Epoch 370, training loss: 0.10812325030565262 = 0.10110823810100555 + 0.001 * 7.015012741088867
Epoch 370, val loss: 0.8954190015792847
Epoch 380, training loss: 0.09410928189754486 = 0.08709830045700073 + 0.001 * 7.010977268218994
Epoch 380, val loss: 0.907308042049408
Epoch 390, training loss: 0.08237709850072861 = 0.07536958903074265 + 0.001 * 7.007505893707275
Epoch 390, val loss: 0.9197473526000977
Epoch 400, training loss: 0.07255639135837555 = 0.06555637717247009 + 0.001 * 7.000010013580322
Epoch 400, val loss: 0.9324784874916077
Epoch 410, training loss: 0.0643351823091507 = 0.057337965816259384 + 0.001 * 6.997220039367676
Epoch 410, val loss: 0.9452114701271057
Epoch 420, training loss: 0.057427868247032166 = 0.050435807555913925 + 0.001 * 6.99206018447876
Epoch 420, val loss: 0.9578322768211365
Epoch 430, training loss: 0.05163652449846268 = 0.044613052159547806 + 0.001 * 7.023473262786865
Epoch 430, val loss: 0.9702315926551819
Epoch 440, training loss: 0.046674907207489014 = 0.039676256477832794 + 0.001 * 6.998652458190918
Epoch 440, val loss: 0.9823523759841919
Epoch 450, training loss: 0.04245826229453087 = 0.03546983376145363 + 0.001 * 6.988426685333252
Epoch 450, val loss: 0.9941734075546265
Epoch 460, training loss: 0.03884563967585564 = 0.031866785138845444 + 0.001 * 6.978854656219482
Epoch 460, val loss: 1.0056335926055908
Epoch 470, training loss: 0.03574010729789734 = 0.028763970360159874 + 0.001 * 6.976136207580566
Epoch 470, val loss: 1.0167204141616821
Epoch 480, training loss: 0.03305933251976967 = 0.026078764349222183 + 0.001 * 6.980569362640381
Epoch 480, val loss: 1.027449607849121
Epoch 490, training loss: 0.030715856701135635 = 0.0237443670630455 + 0.001 * 6.971490383148193
Epoch 490, val loss: 1.0378026962280273
Epoch 500, training loss: 0.028677549213171005 = 0.021705077961087227 + 0.001 * 6.972469806671143
Epoch 500, val loss: 1.0477920770645142
Epoch 510, training loss: 0.026886438950896263 = 0.01991528458893299 + 0.001 * 6.971154689788818
Epoch 510, val loss: 1.057443380355835
Epoch 520, training loss: 0.02530718594789505 = 0.018337486311793327 + 0.001 * 6.969700336456299
Epoch 520, val loss: 1.0667754411697388
Epoch 530, training loss: 0.023907573893666267 = 0.016940707340836525 + 0.001 * 6.966866493225098
Epoch 530, val loss: 1.0757911205291748
Epoch 540, training loss: 0.02268434315919876 = 0.015699177980422974 + 0.001 * 6.985165119171143
Epoch 540, val loss: 1.084494709968567
Epoch 550, training loss: 0.021559197455644608 = 0.014591310173273087 + 0.001 * 6.9678874015808105
Epoch 550, val loss: 1.092922568321228
Epoch 560, training loss: 0.020562004297971725 = 0.013598956167697906 + 0.001 * 6.963048934936523
Epoch 560, val loss: 1.1010640859603882
Epoch 570, training loss: 0.019687799736857414 = 0.012706917710602283 + 0.001 * 6.980881214141846
Epoch 570, val loss: 1.1089385747909546
Epoch 580, training loss: 0.01886623725295067 = 0.01190241426229477 + 0.001 * 6.9638237953186035
Epoch 580, val loss: 1.116585373878479
Epoch 590, training loss: 0.018141958862543106 = 0.011174611747264862 + 0.001 * 6.96734619140625
Epoch 590, val loss: 1.1239715814590454
Epoch 600, training loss: 0.017473746091127396 = 0.010514069348573685 + 0.001 * 6.9596757888793945
Epoch 600, val loss: 1.131131887435913
Epoch 610, training loss: 0.016878535971045494 = 0.009912843815982342 + 0.001 * 6.965691566467285
Epoch 610, val loss: 1.1380763053894043
Epoch 620, training loss: 0.016326263546943665 = 0.0093641746789217 + 0.001 * 6.962088584899902
Epoch 620, val loss: 1.1448050737380981
Epoch 630, training loss: 0.01582060381770134 = 0.008862174116075039 + 0.001 * 6.958430290222168
Epoch 630, val loss: 1.1513346433639526
Epoch 640, training loss: 0.015358378179371357 = 0.008401677012443542 + 0.001 * 6.956700801849365
Epoch 640, val loss: 1.157673716545105
Epoch 650, training loss: 0.014956030994653702 = 0.007978278212249279 + 0.001 * 6.977752208709717
Epoch 650, val loss: 1.1638294458389282
Epoch 660, training loss: 0.014541411772370338 = 0.007588159292936325 + 0.001 * 6.953251838684082
Epoch 660, val loss: 1.1698192358016968
Epoch 670, training loss: 0.014184129424393177 = 0.007227931637316942 + 0.001 * 6.956197261810303
Epoch 670, val loss: 1.1756417751312256
Epoch 680, training loss: 0.013847120106220245 = 0.006894545163959265 + 0.001 * 6.952575206756592
Epoch 680, val loss: 1.18131422996521
Epoch 690, training loss: 0.013551801443099976 = 0.0065853176638484 + 0.001 * 6.9664835929870605
Epoch 690, val loss: 1.186826467514038
Epoch 700, training loss: 0.013255677185952663 = 0.006297861225903034 + 0.001 * 6.957815647125244
Epoch 700, val loss: 1.1922039985656738
Epoch 710, training loss: 0.012976893223822117 = 0.006029720418155193 + 0.001 * 6.94717264175415
Epoch 710, val loss: 1.1974592208862305
Epoch 720, training loss: 0.01274071354418993 = 0.005778205115348101 + 0.001 * 6.962508201599121
Epoch 720, val loss: 1.20260751247406
Epoch 730, training loss: 0.01249578595161438 = 0.00554082402959466 + 0.001 * 6.95496129989624
Epoch 730, val loss: 1.207677960395813
Epoch 740, training loss: 0.012262482196092606 = 0.005315616726875305 + 0.001 * 6.946865081787109
Epoch 740, val loss: 1.2126586437225342
Epoch 750, training loss: 0.01204841397702694 = 0.005101394839584827 + 0.001 * 6.947018623352051
Epoch 750, val loss: 1.217545509338379
Epoch 760, training loss: 0.011844629421830177 = 0.004897648934274912 + 0.001 * 6.946979999542236
Epoch 760, val loss: 1.2223254442214966
Epoch 770, training loss: 0.011649414896965027 = 0.004704061895608902 + 0.001 * 6.945352554321289
Epoch 770, val loss: 1.2270303964614868
Epoch 780, training loss: 0.011476433835923672 = 0.004520433489233255 + 0.001 * 6.955999851226807
Epoch 780, val loss: 1.231637954711914
Epoch 790, training loss: 0.011291044764220715 = 0.004346558824181557 + 0.001 * 6.944485664367676
Epoch 790, val loss: 1.2361427545547485
Epoch 800, training loss: 0.011123986914753914 = 0.004182117525488138 + 0.001 * 6.941869735717773
Epoch 800, val loss: 1.2405576705932617
Epoch 810, training loss: 0.010967650450766087 = 0.004026653245091438 + 0.001 * 6.9409966468811035
Epoch 810, val loss: 1.2448862791061401
Epoch 820, training loss: 0.010826115496456623 = 0.0038798204623162746 + 0.001 * 6.946294784545898
Epoch 820, val loss: 1.2491083145141602
Epoch 830, training loss: 0.010684560984373093 = 0.0037410929799079895 + 0.001 * 6.94346809387207
Epoch 830, val loss: 1.2532511949539185
Epoch 840, training loss: 0.010547693818807602 = 0.0036100251600146294 + 0.001 * 6.9376678466796875
Epoch 840, val loss: 1.2573111057281494
Epoch 850, training loss: 0.010434003546833992 = 0.0034861299209296703 + 0.001 * 6.947872638702393
Epoch 850, val loss: 1.2612749338150024
Epoch 860, training loss: 0.0103068882599473 = 0.003369045676663518 + 0.001 * 6.93784236907959
Epoch 860, val loss: 1.2651532888412476
Epoch 870, training loss: 0.010192546993494034 = 0.0032582804560661316 + 0.001 * 6.934265613555908
Epoch 870, val loss: 1.2689499855041504
Epoch 880, training loss: 0.010094127617776394 = 0.0031534440349787474 + 0.001 * 6.940683364868164
Epoch 880, val loss: 1.272680640220642
Epoch 890, training loss: 0.009981006383895874 = 0.003054186003282666 + 0.001 * 6.926819801330566
Epoch 890, val loss: 1.276318907737732
Epoch 900, training loss: 0.009909912943840027 = 0.0029601496644318104 + 0.001 * 6.949763298034668
Epoch 900, val loss: 1.279883861541748
Epoch 910, training loss: 0.009810814633965492 = 0.002870991127565503 + 0.001 * 6.939823627471924
Epoch 910, val loss: 1.2833614349365234
Epoch 920, training loss: 0.009733796119689941 = 0.002786412136629224 + 0.001 * 6.947383880615234
Epoch 920, val loss: 1.286783218383789
Epoch 930, training loss: 0.009633316658437252 = 0.0027060811407864094 + 0.001 * 6.927235126495361
Epoch 930, val loss: 1.2901123762130737
Epoch 940, training loss: 0.009552831761538982 = 0.0026297455187886953 + 0.001 * 6.923085689544678
Epoch 940, val loss: 1.29338800907135
Epoch 950, training loss: 0.009479476138949394 = 0.0025571382138878107 + 0.001 * 6.922338008880615
Epoch 950, val loss: 1.2965680360794067
Epoch 960, training loss: 0.009421834722161293 = 0.0024880666751414537 + 0.001 * 6.933767318725586
Epoch 960, val loss: 1.299706220626831
Epoch 970, training loss: 0.009343517944216728 = 0.002422263380140066 + 0.001 * 6.921253681182861
Epoch 970, val loss: 1.3027750253677368
Epoch 980, training loss: 0.009285790845751762 = 0.002359547885134816 + 0.001 * 6.926242828369141
Epoch 980, val loss: 1.305787205696106
Epoch 990, training loss: 0.009217575192451477 = 0.0022997483611106873 + 0.001 * 6.917827129364014
Epoch 990, val loss: 1.3087255954742432
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 1.9379669427871704 = 1.9293700456619263 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.9326015710830688
Epoch 10, training loss: 1.9290671348571777 = 1.9204703569412231 + 0.001 * 8.596795082092285
Epoch 10, val loss: 1.923975944519043
Epoch 20, training loss: 1.917654275894165 = 1.9090576171875 + 0.001 * 8.596600532531738
Epoch 20, val loss: 1.9127202033996582
Epoch 30, training loss: 1.9014103412628174 = 1.8928141593933105 + 0.001 * 8.596134185791016
Epoch 30, val loss: 1.8967970609664917
Epoch 40, training loss: 1.877743124961853 = 1.8691481351852417 + 0.001 * 8.594972610473633
Epoch 40, val loss: 1.8741569519042969
Epoch 50, training loss: 1.845880150794983 = 1.8372886180877686 + 0.001 * 8.591590881347656
Epoch 50, val loss: 1.8450254201889038
Epoch 60, training loss: 1.81122624874115 = 1.8026469945907593 + 0.001 * 8.579262733459473
Epoch 60, val loss: 1.814923882484436
Epoch 70, training loss: 1.777034044265747 = 1.7685108184814453 + 0.001 * 8.523177146911621
Epoch 70, val loss: 1.7827458381652832
Epoch 80, training loss: 1.7310301065444946 = 1.722843050956726 + 0.001 * 8.187101364135742
Epoch 80, val loss: 1.7384588718414307
Epoch 90, training loss: 1.6670761108398438 = 1.6591300964355469 + 0.001 * 7.946021556854248
Epoch 90, val loss: 1.6829724311828613
Epoch 100, training loss: 1.5841022729873657 = 1.5761951208114624 + 0.001 * 7.907177448272705
Epoch 100, val loss: 1.6141961812973022
Epoch 110, training loss: 1.4904330968856812 = 1.482592225074768 + 0.001 * 7.840867519378662
Epoch 110, val loss: 1.5365158319473267
Epoch 120, training loss: 1.3947840929031372 = 1.387107014656067 + 0.001 * 7.677067756652832
Epoch 120, val loss: 1.459768295288086
Epoch 130, training loss: 1.3011795282363892 = 1.2936674356460571 + 0.001 * 7.512054920196533
Epoch 130, val loss: 1.3873287439346313
Epoch 140, training loss: 1.2114851474761963 = 1.2040088176727295 + 0.001 * 7.4763054847717285
Epoch 140, val loss: 1.3202931880950928
Epoch 150, training loss: 1.1269645690917969 = 1.1195571422576904 + 0.001 * 7.407414436340332
Epoch 150, val loss: 1.2594658136367798
Epoch 160, training loss: 1.047400712966919 = 1.040043830871582 + 0.001 * 7.356864929199219
Epoch 160, val loss: 1.2037607431411743
Epoch 170, training loss: 0.9715911149978638 = 0.9642856121063232 + 0.001 * 7.305481433868408
Epoch 170, val loss: 1.1510313749313354
Epoch 180, training loss: 0.8981198072433472 = 0.8908504247665405 + 0.001 * 7.269395351409912
Epoch 180, val loss: 1.099640130996704
Epoch 190, training loss: 0.8257577419281006 = 0.8185003995895386 + 0.001 * 7.257321357727051
Epoch 190, val loss: 1.0486940145492554
Epoch 200, training loss: 0.7544792890548706 = 0.7472251653671265 + 0.001 * 7.254095554351807
Epoch 200, val loss: 0.9987809062004089
Epoch 210, training loss: 0.6857662200927734 = 0.6785158514976501 + 0.001 * 7.250371932983398
Epoch 210, val loss: 0.9522061944007874
Epoch 220, training loss: 0.6212310791015625 = 0.6139851808547974 + 0.001 * 7.245903015136719
Epoch 220, val loss: 0.9120535254478455
Epoch 230, training loss: 0.5617260336875916 = 0.5544837713241577 + 0.001 * 7.24223518371582
Epoch 230, val loss: 0.8802825808525085
Epoch 240, training loss: 0.507331907749176 = 0.500092089176178 + 0.001 * 7.2397942543029785
Epoch 240, val loss: 0.8574663400650024
Epoch 250, training loss: 0.4576131999492645 = 0.45037519931793213 + 0.001 * 7.2379984855651855
Epoch 250, val loss: 0.8427373766899109
Epoch 260, training loss: 0.4118421673774719 = 0.40460652112960815 + 0.001 * 7.235658168792725
Epoch 260, val loss: 0.8343445658683777
Epoch 270, training loss: 0.36929407715797424 = 0.36206018924713135 + 0.001 * 7.233874320983887
Epoch 270, val loss: 0.8305728435516357
Epoch 280, training loss: 0.32958558201789856 = 0.3223537504673004 + 0.001 * 7.231832981109619
Epoch 280, val loss: 0.8303330540657043
Epoch 290, training loss: 0.2927008867263794 = 0.28547167778015137 + 0.001 * 7.229219436645508
Epoch 290, val loss: 0.8330474495887756
Epoch 300, training loss: 0.25886744260787964 = 0.2516406178474426 + 0.001 * 7.226820468902588
Epoch 300, val loss: 0.8384050726890564
Epoch 310, training loss: 0.2283450812101364 = 0.2211238294839859 + 0.001 * 7.221256732940674
Epoch 310, val loss: 0.8463144898414612
Epoch 320, training loss: 0.20124536752700806 = 0.19403010606765747 + 0.001 * 7.215266227722168
Epoch 320, val loss: 0.8566915988922119
Epoch 330, training loss: 0.1774570345878601 = 0.17025181651115417 + 0.001 * 7.2052130699157715
Epoch 330, val loss: 0.8692832589149475
Epoch 340, training loss: 0.15673774480819702 = 0.14953380823135376 + 0.001 * 7.203939914703369
Epoch 340, val loss: 0.883721113204956
Epoch 350, training loss: 0.13872873783111572 = 0.13154441118240356 + 0.001 * 7.184330940246582
Epoch 350, val loss: 0.8995761275291443
Epoch 360, training loss: 0.12314941734075546 = 0.11593236029148102 + 0.001 * 7.2170586585998535
Epoch 360, val loss: 0.9165631532669067
Epoch 370, training loss: 0.10955138504505157 = 0.10238543897867203 + 0.001 * 7.165943145751953
Epoch 370, val loss: 0.9343504905700684
Epoch 380, training loss: 0.09776124358177185 = 0.09062692523002625 + 0.001 * 7.134315490722656
Epoch 380, val loss: 0.9526427388191223
Epoch 390, training loss: 0.08753282576799393 = 0.08041662722826004 + 0.001 * 7.116197109222412
Epoch 390, val loss: 0.971237301826477
Epoch 400, training loss: 0.07867348194122314 = 0.07154253870248795 + 0.001 * 7.130942344665527
Epoch 400, val loss: 0.9898534417152405
Epoch 410, training loss: 0.07091420143842697 = 0.06382002681493759 + 0.001 * 7.094171047210693
Epoch 410, val loss: 1.0083274841308594
Epoch 420, training loss: 0.064180888235569 = 0.0570901520550251 + 0.001 * 7.090737342834473
Epoch 420, val loss: 1.0265040397644043
Epoch 430, training loss: 0.058321524411439896 = 0.05121568217873573 + 0.001 * 7.105842113494873
Epoch 430, val loss: 1.0442945957183838
Epoch 440, training loss: 0.05314376577734947 = 0.04607941210269928 + 0.001 * 7.0643534660339355
Epoch 440, val loss: 1.061641812324524
Epoch 450, training loss: 0.04864264279603958 = 0.04158153012394905 + 0.001 * 7.061110973358154
Epoch 450, val loss: 1.078440546989441
Epoch 460, training loss: 0.044694751501083374 = 0.03763558343052864 + 0.001 * 7.059166431427002
Epoch 460, val loss: 1.0946460962295532
Epoch 470, training loss: 0.04122336953878403 = 0.03416740149259567 + 0.001 * 7.055966854095459
Epoch 470, val loss: 1.1102896928787231
Epoch 480, training loss: 0.038163963705301285 = 0.03111281804740429 + 0.001 * 7.051146030426025
Epoch 480, val loss: 1.1253912448883057
Epoch 490, training loss: 0.03546414524316788 = 0.028417052701115608 + 0.001 * 7.047091960906982
Epoch 490, val loss: 1.1398906707763672
Epoch 500, training loss: 0.03307642042636871 = 0.026032520458102226 + 0.001 * 7.043900489807129
Epoch 500, val loss: 1.1538907289505005
Epoch 510, training loss: 0.03097483143210411 = 0.02391815185546875 + 0.001 * 7.0566792488098145
Epoch 510, val loss: 1.167362093925476
Epoch 520, training loss: 0.029085487127304077 = 0.02203815057873726 + 0.001 * 7.047336578369141
Epoch 520, val loss: 1.1802946329116821
Epoch 530, training loss: 0.027397342026233673 = 0.020361997187137604 + 0.001 * 7.035345077514648
Epoch 530, val loss: 1.1927365064620972
Epoch 540, training loss: 0.0259159654378891 = 0.01886323280632496 + 0.001 * 7.052731513977051
Epoch 540, val loss: 1.2047170400619507
Epoch 550, training loss: 0.024561386555433273 = 0.017519427463412285 + 0.001 * 7.041958808898926
Epoch 550, val loss: 1.2161701917648315
Epoch 560, training loss: 0.02334246225655079 = 0.016311518847942352 + 0.001 * 7.030942440032959
Epoch 560, val loss: 1.2272059917449951
Epoch 570, training loss: 0.022251229733228683 = 0.015222274698317051 + 0.001 * 7.028953552246094
Epoch 570, val loss: 1.2378005981445312
Epoch 580, training loss: 0.021272826939821243 = 0.014236319810152054 + 0.001 * 7.036507606506348
Epoch 580, val loss: 1.2480244636535645
Epoch 590, training loss: 0.020374057814478874 = 0.013338434509932995 + 0.001 * 7.035623550415039
Epoch 590, val loss: 1.2579174041748047
Epoch 600, training loss: 0.019543198868632317 = 0.012515585869550705 + 0.001 * 7.027613162994385
Epoch 600, val loss: 1.2674933671951294
Epoch 610, training loss: 0.018784379586577415 = 0.011758646927773952 + 0.001 * 7.025732517242432
Epoch 610, val loss: 1.2768086194992065
Epoch 620, training loss: 0.018083658069372177 = 0.01106170192360878 + 0.001 * 7.021955966949463
Epoch 620, val loss: 1.2858195304870605
Epoch 630, training loss: 0.01745191588997841 = 0.010419769212603569 + 0.001 * 7.0321455001831055
Epoch 630, val loss: 1.294529914855957
Epoch 640, training loss: 0.016850538551807404 = 0.009828600101172924 + 0.001 * 7.021937370300293
Epoch 640, val loss: 1.3029046058654785
Epoch 650, training loss: 0.01630532741546631 = 0.00928314495831728 + 0.001 * 7.022182464599609
Epoch 650, val loss: 1.3109731674194336
Epoch 660, training loss: 0.01579560711979866 = 0.008778692223131657 + 0.001 * 7.016915321350098
Epoch 660, val loss: 1.3187307119369507
Epoch 670, training loss: 0.015330242924392223 = 0.008309749886393547 + 0.001 * 7.0204925537109375
Epoch 670, val loss: 1.3261594772338867
Epoch 680, training loss: 0.01488365326076746 = 0.007872282527387142 + 0.001 * 7.0113701820373535
Epoch 680, val loss: 1.3332949876785278
Epoch 690, training loss: 0.014473783783614635 = 0.0074637215584516525 + 0.001 * 7.010061740875244
Epoch 690, val loss: 1.3401463031768799
Epoch 700, training loss: 0.01409534364938736 = 0.007082258816808462 + 0.001 * 7.013084411621094
Epoch 700, val loss: 1.3467351198196411
Epoch 710, training loss: 0.013734439387917519 = 0.0067264302633702755 + 0.001 * 7.0080084800720215
Epoch 710, val loss: 1.3530778884887695
Epoch 720, training loss: 0.01340975146740675 = 0.006394712720066309 + 0.001 * 7.01503849029541
Epoch 720, val loss: 1.3591718673706055
Epoch 730, training loss: 0.013094865716993809 = 0.006085728295147419 + 0.001 * 7.009137153625488
Epoch 730, val loss: 1.3650381565093994
Epoch 740, training loss: 0.012800049968063831 = 0.005797883495688438 + 0.001 * 7.002166271209717
Epoch 740, val loss: 1.3706965446472168
Epoch 750, training loss: 0.01253633014857769 = 0.0055297561921179295 + 0.001 * 7.00657320022583
Epoch 750, val loss: 1.3761491775512695
Epoch 760, training loss: 0.012277994304895401 = 0.005279903765767813 + 0.001 * 6.998089790344238
Epoch 760, val loss: 1.381382703781128
Epoch 770, training loss: 0.012056340463459492 = 0.005046899896115065 + 0.001 * 7.0094404220581055
Epoch 770, val loss: 1.3864502906799316
Epoch 780, training loss: 0.011830782517790794 = 0.00482950871810317 + 0.001 * 7.001274108886719
Epoch 780, val loss: 1.3913053274154663
Epoch 790, training loss: 0.011621322482824326 = 0.004626401700079441 + 0.001 * 6.99492073059082
Epoch 790, val loss: 1.3960224390029907
Epoch 800, training loss: 0.011433720588684082 = 0.004436434246599674 + 0.001 * 6.997286319732666
Epoch 800, val loss: 1.4005608558654785
Epoch 810, training loss: 0.011258037760853767 = 0.004258522763848305 + 0.001 * 6.999514579772949
Epoch 810, val loss: 1.4049161672592163
Epoch 820, training loss: 0.011080509051680565 = 0.004091228824108839 + 0.001 * 6.989279270172119
Epoch 820, val loss: 1.4091542959213257
Epoch 830, training loss: 0.010925176553428173 = 0.0039329673163592815 + 0.001 * 6.992208957672119
Epoch 830, val loss: 1.413178563117981
Epoch 840, training loss: 0.010795436799526215 = 0.0037823952734470367 + 0.001 * 7.013040542602539
Epoch 840, val loss: 1.4170362949371338
Epoch 850, training loss: 0.010628537274897099 = 0.003638774622231722 + 0.001 * 6.989762306213379
Epoch 850, val loss: 1.4207578897476196
Epoch 860, training loss: 0.010489631444215775 = 0.0035017470363527536 + 0.001 * 6.987884521484375
Epoch 860, val loss: 1.4243260622024536
Epoch 870, training loss: 0.010366452857851982 = 0.0033711609430611134 + 0.001 * 6.9952921867370605
Epoch 870, val loss: 1.4277307987213135
Epoch 880, training loss: 0.010232188738882542 = 0.0032469588331878185 + 0.001 * 6.9852294921875
Epoch 880, val loss: 1.4309991598129272
Epoch 890, training loss: 0.010110894218087196 = 0.0031290510669350624 + 0.001 * 6.9818434715271
Epoch 890, val loss: 1.434205412864685
Epoch 900, training loss: 0.010015006177127361 = 0.0030172793194651604 + 0.001 * 6.9977264404296875
Epoch 900, val loss: 1.4372525215148926
Epoch 910, training loss: 0.009895028546452522 = 0.0029114102944731712 + 0.001 * 6.983617782592773
Epoch 910, val loss: 1.440214991569519
Epoch 920, training loss: 0.009801885113120079 = 0.0028111806605011225 + 0.001 * 6.990704536437988
Epoch 920, val loss: 1.443069338798523
Epoch 930, training loss: 0.009704402647912502 = 0.0027164164930582047 + 0.001 * 6.987985610961914
Epoch 930, val loss: 1.4457792043685913
Epoch 940, training loss: 0.009601875208318233 = 0.0026267520152032375 + 0.001 * 6.975122928619385
Epoch 940, val loss: 1.4484877586364746
Epoch 950, training loss: 0.009512943215668201 = 0.0025419099256396294 + 0.001 * 6.971033096313477
Epoch 950, val loss: 1.451073169708252
Epoch 960, training loss: 0.009459739550948143 = 0.0024615670554339886 + 0.001 * 6.998172760009766
Epoch 960, val loss: 1.4535976648330688
Epoch 970, training loss: 0.009364929050207138 = 0.002385469153523445 + 0.001 * 6.9794602394104
Epoch 970, val loss: 1.4560253620147705
Epoch 980, training loss: 0.00929763913154602 = 0.00231338944286108 + 0.001 * 6.984249591827393
Epoch 980, val loss: 1.4583768844604492
Epoch 990, training loss: 0.00923724751919508 = 0.00224504922516644 + 0.001 * 6.992197513580322
Epoch 990, val loss: 1.4606959819793701
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 1.958152174949646 = 1.9495553970336914 + 0.001 * 8.596819877624512
Epoch 0, val loss: 1.9504733085632324
Epoch 10, training loss: 1.948105812072754 = 1.9395090341567993 + 0.001 * 8.596755981445312
Epoch 10, val loss: 1.940852165222168
Epoch 20, training loss: 1.9359861612319946 = 1.9273896217346191 + 0.001 * 8.596485137939453
Epoch 20, val loss: 1.9286988973617554
Epoch 30, training loss: 1.9191080331802368 = 1.9105123281478882 + 0.001 * 8.595733642578125
Epoch 30, val loss: 1.9112412929534912
Epoch 40, training loss: 1.8942688703536987 = 1.885675311088562 + 0.001 * 8.593546867370605
Epoch 40, val loss: 1.8855414390563965
Epoch 50, training loss: 1.8594770431518555 = 1.850891351699829 + 0.001 * 8.58565902709961
Epoch 50, val loss: 1.8506196737289429
Epoch 60, training loss: 1.8198935985565186 = 1.8113452196121216 + 0.001 * 8.548420906066895
Epoch 60, val loss: 1.8142943382263184
Epoch 70, training loss: 1.786997675895691 = 1.7786831855773926 + 0.001 * 8.314465522766113
Epoch 70, val loss: 1.7871922254562378
Epoch 80, training loss: 1.7490763664245605 = 1.7410537004470825 + 0.001 * 8.022643089294434
Epoch 80, val loss: 1.7547253370285034
Epoch 90, training loss: 1.695102334022522 = 1.6872375011444092 + 0.001 * 7.8648762702941895
Epoch 90, val loss: 1.7097302675247192
Epoch 100, training loss: 1.6204456090927124 = 1.61276113986969 + 0.001 * 7.684428691864014
Epoch 100, val loss: 1.6489100456237793
Epoch 110, training loss: 1.5277400016784668 = 1.52023446559906 + 0.001 * 7.505545616149902
Epoch 110, val loss: 1.574484944343567
Epoch 120, training loss: 1.428080439567566 = 1.4206429719924927 + 0.001 * 7.437439441680908
Epoch 120, val loss: 1.4951364994049072
Epoch 130, training loss: 1.3271938562393188 = 1.3198050260543823 + 0.001 * 7.388844966888428
Epoch 130, val loss: 1.4154337644577026
Epoch 140, training loss: 1.2242425680160522 = 1.216934323310852 + 0.001 * 7.308199882507324
Epoch 140, val loss: 1.3352739810943604
Epoch 150, training loss: 1.1192302703857422 = 1.1119928359985352 + 0.001 * 7.237466335296631
Epoch 150, val loss: 1.2545489072799683
Epoch 160, training loss: 1.0155367851257324 = 1.0083256959915161 + 0.001 * 7.211090087890625
Epoch 160, val loss: 1.176378846168518
Epoch 170, training loss: 0.918039858341217 = 0.9108340740203857 + 0.001 * 7.205809116363525
Epoch 170, val loss: 1.1045653820037842
Epoch 180, training loss: 0.8305636048316956 = 0.8233628869056702 + 0.001 * 7.200704574584961
Epoch 180, val loss: 1.041750431060791
Epoch 190, training loss: 0.754787266254425 = 0.7475900053977966 + 0.001 * 7.197234630584717
Epoch 190, val loss: 0.9894574284553528
Epoch 200, training loss: 0.6900342702865601 = 0.6828413605690002 + 0.001 * 7.192880153656006
Epoch 200, val loss: 0.9478574991226196
Epoch 210, training loss: 0.634080171585083 = 0.6268911957740784 + 0.001 * 7.18895149230957
Epoch 210, val loss: 0.9155176877975464
Epoch 220, training loss: 0.5842881798744202 = 0.5771028995513916 + 0.001 * 7.185259819030762
Epoch 220, val loss: 0.8904991149902344
Epoch 230, training loss: 0.5383450388908386 = 0.5311636328697205 + 0.001 * 7.181414604187012
Epoch 230, val loss: 0.8707651495933533
Epoch 240, training loss: 0.49451249837875366 = 0.48733481764793396 + 0.001 * 7.177666664123535
Epoch 240, val loss: 0.8544939160346985
Epoch 250, training loss: 0.45196259021759033 = 0.4447891116142273 + 0.001 * 7.173476219177246
Epoch 250, val loss: 0.8409373164176941
Epoch 260, training loss: 0.41069623827934265 = 0.4035276174545288 + 0.001 * 7.168613433837891
Epoch 260, val loss: 0.8302782773971558
Epoch 270, training loss: 0.37110188603401184 = 0.3639412522315979 + 0.001 * 7.16063117980957
Epoch 270, val loss: 0.8230180740356445
Epoch 280, training loss: 0.33350449800491333 = 0.3263509273529053 + 0.001 * 7.153566837310791
Epoch 280, val loss: 0.8194411993026733
Epoch 290, training loss: 0.2980540096759796 = 0.29091504216194153 + 0.001 * 7.138981342315674
Epoch 290, val loss: 0.8191359639167786
Epoch 300, training loss: 0.2649155259132385 = 0.2577897608280182 + 0.001 * 7.125750541687012
Epoch 300, val loss: 0.8217930197715759
Epoch 310, training loss: 0.23427464067935944 = 0.22715601325035095 + 0.001 * 7.118626117706299
Epoch 310, val loss: 0.827123761177063
Epoch 320, training loss: 0.20635148882865906 = 0.1992489993572235 + 0.001 * 7.102494239807129
Epoch 320, val loss: 0.8349600434303284
Epoch 330, training loss: 0.1813487857580185 = 0.1742383986711502 + 0.001 * 7.110385417938232
Epoch 330, val loss: 0.8450437784194946
Epoch 340, training loss: 0.15925437211990356 = 0.15216033160686493 + 0.001 * 7.094034194946289
Epoch 340, val loss: 0.8573090434074402
Epoch 350, training loss: 0.13997475802898407 = 0.1328890472650528 + 0.001 * 7.08571720123291
Epoch 350, val loss: 0.8715190291404724
Epoch 360, training loss: 0.12329136580228806 = 0.1162104681134224 + 0.001 * 7.080894470214844
Epoch 360, val loss: 0.8872933983802795
Epoch 370, training loss: 0.10892879217863083 = 0.10184449702501297 + 0.001 * 7.084297180175781
Epoch 370, val loss: 0.9042484164237976
Epoch 380, training loss: 0.09657742828130722 = 0.08949986845254898 + 0.001 * 7.07755708694458
Epoch 380, val loss: 0.9220399260520935
Epoch 390, training loss: 0.08597663789987564 = 0.07890072464942932 + 0.001 * 7.075916290283203
Epoch 390, val loss: 0.9402956962585449
Epoch 400, training loss: 0.07687298953533173 = 0.06979863345623016 + 0.001 * 7.0743536949157715
Epoch 400, val loss: 0.9587582945823669
Epoch 410, training loss: 0.06905194371938705 = 0.0619756318628788 + 0.001 * 7.076314449310303
Epoch 410, val loss: 0.9771563410758972
Epoch 420, training loss: 0.062316324561834335 = 0.05524464324116707 + 0.001 * 7.071681499481201
Epoch 420, val loss: 0.9953283071517944
Epoch 430, training loss: 0.05651306360960007 = 0.04944204166531563 + 0.001 * 7.071022987365723
Epoch 430, val loss: 1.01316237449646
Epoch 440, training loss: 0.05149346962571144 = 0.04442606493830681 + 0.001 * 7.0674052238464355
Epoch 440, val loss: 1.0306015014648438
Epoch 450, training loss: 0.04714483767747879 = 0.040078096091747284 + 0.001 * 7.066742420196533
Epoch 450, val loss: 1.0476113557815552
Epoch 460, training loss: 0.04336516186594963 = 0.03629745543003082 + 0.001 * 7.0677056312561035
Epoch 460, val loss: 1.0641300678253174
Epoch 470, training loss: 0.04005911573767662 = 0.03299928084015846 + 0.001 * 7.059834003448486
Epoch 470, val loss: 1.080183744430542
Epoch 480, training loss: 0.037169940769672394 = 0.03011140786111355 + 0.001 * 7.058534622192383
Epoch 480, val loss: 1.0957297086715698
Epoch 490, training loss: 0.034628815948963165 = 0.027573440223932266 + 0.001 * 7.055376052856445
Epoch 490, val loss: 1.1107836961746216
Epoch 500, training loss: 0.032408371567726135 = 0.02533498778939247 + 0.001 * 7.073384761810303
Epoch 500, val loss: 1.1253552436828613
Epoch 510, training loss: 0.030406249687075615 = 0.02335336059331894 + 0.001 * 7.052889347076416
Epoch 510, val loss: 1.1394401788711548
Epoch 520, training loss: 0.028640786185860634 = 0.0215930063277483 + 0.001 * 7.0477800369262695
Epoch 520, val loss: 1.153040885925293
Epoch 530, training loss: 0.027086667716503143 = 0.020023459568619728 + 0.001 * 7.063206672668457
Epoch 530, val loss: 1.1661524772644043
Epoch 540, training loss: 0.025666790083050728 = 0.01861921325325966 + 0.001 * 7.047575950622559
Epoch 540, val loss: 1.178834080696106
Epoch 550, training loss: 0.024407953023910522 = 0.017358487471938133 + 0.001 * 7.049465656280518
Epoch 550, val loss: 1.191112756729126
Epoch 560, training loss: 0.02326400950551033 = 0.01622319221496582 + 0.001 * 7.040817737579346
Epoch 560, val loss: 1.2029900550842285
Epoch 570, training loss: 0.022231942042708397 = 0.015197494998574257 + 0.001 * 7.034447193145752
Epoch 570, val loss: 1.2144982814788818
Epoch 580, training loss: 0.021303044632077217 = 0.014268125407397747 + 0.001 * 7.034918785095215
Epoch 580, val loss: 1.2256247997283936
Epoch 590, training loss: 0.02046027034521103 = 0.01342364214360714 + 0.001 * 7.036627292633057
Epoch 590, val loss: 1.236396312713623
Epoch 600, training loss: 0.019683199003338814 = 0.012654162012040615 + 0.001 * 7.029036521911621
Epoch 600, val loss: 1.2468568086624146
Epoch 610, training loss: 0.018987812101840973 = 0.011951255612075329 + 0.001 * 7.036556720733643
Epoch 610, val loss: 1.2569780349731445
Epoch 620, training loss: 0.018326375633478165 = 0.011307531036436558 + 0.001 * 7.018843650817871
Epoch 620, val loss: 1.2668200731277466
Epoch 630, training loss: 0.01774711348116398 = 0.010716628283262253 + 0.001 * 7.030485153198242
Epoch 630, val loss: 1.276368498802185
Epoch 640, training loss: 0.017198115587234497 = 0.010172917507588863 + 0.001 * 7.025197982788086
Epoch 640, val loss: 1.2856338024139404
Epoch 650, training loss: 0.016681522130966187 = 0.009671490639448166 + 0.001 * 7.010030269622803
Epoch 650, val loss: 1.2946213483810425
Epoch 660, training loss: 0.016220802441239357 = 0.009208113886415958 + 0.001 * 7.012688159942627
Epoch 660, val loss: 1.3033462762832642
Epoch 670, training loss: 0.015810493379831314 = 0.008778982795774937 + 0.001 * 7.0315093994140625
Epoch 670, val loss: 1.3118411302566528
Epoch 680, training loss: 0.015395374968647957 = 0.008380898274481297 + 0.001 * 7.0144758224487305
Epoch 680, val loss: 1.3200801610946655
Epoch 690, training loss: 0.01502225175499916 = 0.008010823279619217 + 0.001 * 7.0114288330078125
Epoch 690, val loss: 1.3280999660491943
Epoch 700, training loss: 0.014670189470052719 = 0.007666108198463917 + 0.001 * 7.004080772399902
Epoch 700, val loss: 1.3359079360961914
Epoch 710, training loss: 0.014338988810777664 = 0.007344281300902367 + 0.001 * 6.9947075843811035
Epoch 710, val loss: 1.34351646900177
Epoch 720, training loss: 0.014052815735340118 = 0.007043279241770506 + 0.001 * 7.009536266326904
Epoch 720, val loss: 1.3509348630905151
Epoch 730, training loss: 0.013769082725048065 = 0.006761074997484684 + 0.001 * 7.008007049560547
Epoch 730, val loss: 1.3581833839416504
Epoch 740, training loss: 0.013497695326805115 = 0.006495960522443056 + 0.001 * 7.001733779907227
Epoch 740, val loss: 1.365273356437683
Epoch 750, training loss: 0.013232370838522911 = 0.006246439181268215 + 0.001 * 6.985931396484375
Epoch 750, val loss: 1.3721890449523926
Epoch 760, training loss: 0.013016089797019958 = 0.006011094897985458 + 0.001 * 7.0049943923950195
Epoch 760, val loss: 1.3789547681808472
Epoch 770, training loss: 0.01277594268321991 = 0.005788890179246664 + 0.001 * 6.987051963806152
Epoch 770, val loss: 1.385608196258545
Epoch 780, training loss: 0.012574666179716587 = 0.005578864365816116 + 0.001 * 6.9958014488220215
Epoch 780, val loss: 1.3921054601669312
Epoch 790, training loss: 0.012362507171928883 = 0.005380027461796999 + 0.001 * 6.982479572296143
Epoch 790, val loss: 1.3984949588775635
Epoch 800, training loss: 0.012174107134342194 = 0.005191554315388203 + 0.001 * 6.9825520515441895
Epoch 800, val loss: 1.4047434329986572
Epoch 810, training loss: 0.011988090351223946 = 0.005012767855077982 + 0.001 * 6.975322723388672
Epoch 810, val loss: 1.410909652709961
Epoch 820, training loss: 0.011822355911135674 = 0.004843126982450485 + 0.001 * 6.9792280197143555
Epoch 820, val loss: 1.4169447422027588
Epoch 830, training loss: 0.011663831770420074 = 0.004682035651057959 + 0.001 * 6.9817962646484375
Epoch 830, val loss: 1.422869086265564
Epoch 840, training loss: 0.011508136987686157 = 0.004528983496129513 + 0.001 * 6.979153156280518
Epoch 840, val loss: 1.4287046194076538
Epoch 850, training loss: 0.011359470896422863 = 0.00438340287655592 + 0.001 * 6.976067543029785
Epoch 850, val loss: 1.434439778327942
Epoch 860, training loss: 0.011207912117242813 = 0.004244992975145578 + 0.001 * 6.962918758392334
Epoch 860, val loss: 1.4400286674499512
Epoch 870, training loss: 0.011088519357144833 = 0.004113409668207169 + 0.001 * 6.975109577178955
Epoch 870, val loss: 1.4455254077911377
Epoch 880, training loss: 0.010958552360534668 = 0.00398814445361495 + 0.001 * 6.970407962799072
Epoch 880, val loss: 1.4509307146072388
Epoch 890, training loss: 0.010839526541531086 = 0.003868780331686139 + 0.001 * 6.970746040344238
Epoch 890, val loss: 1.4561928510665894
Epoch 900, training loss: 0.010744876228272915 = 0.003755008103325963 + 0.001 * 6.989867687225342
Epoch 900, val loss: 1.4613544940948486
Epoch 910, training loss: 0.010612400248646736 = 0.0036466456949710846 + 0.001 * 6.96575403213501
Epoch 910, val loss: 1.4664497375488281
Epoch 920, training loss: 0.010534177534282207 = 0.003543274709954858 + 0.001 * 6.990902900695801
Epoch 920, val loss: 1.4714038372039795
Epoch 930, training loss: 0.010399118065834045 = 0.0034447479993104935 + 0.001 * 6.95436954498291
Epoch 930, val loss: 1.4762969017028809
Epoch 940, training loss: 0.010316425934433937 = 0.0033506632316857576 + 0.001 * 6.965762615203857
Epoch 940, val loss: 1.4810501337051392
Epoch 950, training loss: 0.010229595005512238 = 0.0032608043402433395 + 0.001 * 6.9687910079956055
Epoch 950, val loss: 1.4857149124145508
Epoch 960, training loss: 0.010123813524842262 = 0.003174934769049287 + 0.001 * 6.948878288269043
Epoch 960, val loss: 1.4902626276016235
Epoch 970, training loss: 0.010047698393464088 = 0.0030928293708711863 + 0.001 * 6.954868793487549
Epoch 970, val loss: 1.4947283267974854
Epoch 980, training loss: 0.009957242757081985 = 0.0030143391340970993 + 0.001 * 6.942903518676758
Epoch 980, val loss: 1.49909508228302
Epoch 990, training loss: 0.009905720129609108 = 0.002939235419034958 + 0.001 * 6.966484546661377
Epoch 990, val loss: 1.5033625364303589
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8118081180811808
The final CL Acc:0.76296, 0.00800, The final GNN Acc:0.81515, 0.00259
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13302])
remove edge: torch.Size([2, 7886])
updated graph: torch.Size([2, 10632])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9623178243637085 = 1.9537209272384644 + 0.001 * 8.596850395202637
Epoch 0, val loss: 1.948643445968628
Epoch 10, training loss: 1.9519141912460327 = 1.9433174133300781 + 0.001 * 8.59681224822998
Epoch 10, val loss: 1.9387915134429932
Epoch 20, training loss: 1.9393994808197021 = 1.930802822113037 + 0.001 * 8.596651077270508
Epoch 20, val loss: 1.926576018333435
Epoch 30, training loss: 1.922120451927185 = 1.9135241508483887 + 0.001 * 8.596267700195312
Epoch 30, val loss: 1.90935218334198
Epoch 40, training loss: 1.8968019485473633 = 1.8882067203521729 + 0.001 * 8.595268249511719
Epoch 40, val loss: 1.884083867073059
Epoch 50, training loss: 1.8609899282455444 = 1.8523977994918823 + 0.001 * 8.592109680175781
Epoch 50, val loss: 1.849579930305481
Epoch 60, training loss: 1.8190206289291382 = 1.810441017150879 + 0.001 * 8.579633712768555
Epoch 60, val loss: 1.8126139640808105
Epoch 70, training loss: 1.7810776233673096 = 1.7725560665130615 + 0.001 * 8.521615028381348
Epoch 70, val loss: 1.7819665670394897
Epoch 80, training loss: 1.7359060049057007 = 1.727743148803711 + 0.001 * 8.162887573242188
Epoch 80, val loss: 1.7417941093444824
Epoch 90, training loss: 1.6736669540405273 = 1.6656674146652222 + 0.001 * 7.999591827392578
Epoch 90, val loss: 1.685154676437378
Epoch 100, training loss: 1.590356707572937 = 1.5824087858200073 + 0.001 * 7.94791316986084
Epoch 100, val loss: 1.6129051446914673
Epoch 110, training loss: 1.4961979389190674 = 1.4883028268814087 + 0.001 * 7.895118236541748
Epoch 110, val loss: 1.5353236198425293
Epoch 120, training loss: 1.4031611680984497 = 1.3953931331634521 + 0.001 * 7.7680158615112305
Epoch 120, val loss: 1.4616676568984985
Epoch 130, training loss: 1.313995361328125 = 1.306412935256958 + 0.001 * 7.5824456214904785
Epoch 130, val loss: 1.394730806350708
Epoch 140, training loss: 1.2276015281677246 = 1.2200591564178467 + 0.001 * 7.542331218719482
Epoch 140, val loss: 1.3320797681808472
Epoch 150, training loss: 1.1449235677719116 = 1.1374317407608032 + 0.001 * 7.491796493530273
Epoch 150, val loss: 1.273428201675415
Epoch 160, training loss: 1.0683526992797852 = 1.0609173774719238 + 0.001 * 7.435321807861328
Epoch 160, val loss: 1.2207623720169067
Epoch 170, training loss: 0.9991354942321777 = 0.9917458295822144 + 0.001 * 7.38967227935791
Epoch 170, val loss: 1.173642873764038
Epoch 180, training loss: 0.9364475011825562 = 0.9291097521781921 + 0.001 * 7.337774276733398
Epoch 180, val loss: 1.1307135820388794
Epoch 190, training loss: 0.877941906452179 = 0.8706612586975098 + 0.001 * 7.280636787414551
Epoch 190, val loss: 1.0899161100387573
Epoch 200, training loss: 0.821139931678772 = 0.813895046710968 + 0.001 * 7.244875431060791
Epoch 200, val loss: 1.0495233535766602
Epoch 210, training loss: 0.7646073698997498 = 0.7573737502098083 + 0.001 * 7.233646392822266
Epoch 210, val loss: 1.008906364440918
Epoch 220, training loss: 0.7083245515823364 = 0.701097846031189 + 0.001 * 7.226722240447998
Epoch 220, val loss: 0.9683155417442322
Epoch 230, training loss: 0.6529806852340698 = 0.6457602977752686 + 0.001 * 7.220358848571777
Epoch 230, val loss: 0.9292607307434082
Epoch 240, training loss: 0.5991880297660828 = 0.5919755697250366 + 0.001 * 7.212483882904053
Epoch 240, val loss: 0.8936089873313904
Epoch 250, training loss: 0.546903133392334 = 0.5396999716758728 + 0.001 * 7.2031636238098145
Epoch 250, val loss: 0.8618704676628113
Epoch 260, training loss: 0.49545758962631226 = 0.4882650077342987 + 0.001 * 7.192574501037598
Epoch 260, val loss: 0.8333520889282227
Epoch 270, training loss: 0.44400519132614136 = 0.4368250072002411 + 0.001 * 7.180180549621582
Epoch 270, val loss: 0.8080012798309326
Epoch 280, training loss: 0.39268893003463745 = 0.3855198919773102 + 0.001 * 7.16903829574585
Epoch 280, val loss: 0.786753237247467
Epoch 290, training loss: 0.3428889811038971 = 0.33573684096336365 + 0.001 * 7.152153015136719
Epoch 290, val loss: 0.770067036151886
Epoch 300, training loss: 0.29666945338249207 = 0.2895313799381256 + 0.001 * 7.138062953948975
Epoch 300, val loss: 0.7587385177612305
Epoch 310, training loss: 0.25561368465423584 = 0.24849838018417358 + 0.001 * 7.115312099456787
Epoch 310, val loss: 0.7525519728660583
Epoch 320, training loss: 0.22031818330287933 = 0.21321456134319305 + 0.001 * 7.103616237640381
Epoch 320, val loss: 0.7509762644767761
Epoch 330, training loss: 0.19050097465515137 = 0.18342259526252747 + 0.001 * 7.078378200531006
Epoch 330, val loss: 0.7530057430267334
Epoch 340, training loss: 0.16555947065353394 = 0.1584968864917755 + 0.001 * 7.062579154968262
Epoch 340, val loss: 0.7578758597373962
Epoch 350, training loss: 0.1447218954563141 = 0.13766491413116455 + 0.001 * 7.056978225708008
Epoch 350, val loss: 0.7649269104003906
Epoch 360, training loss: 0.12720820307731628 = 0.12016155570745468 + 0.001 * 7.046642780303955
Epoch 360, val loss: 0.7735804915428162
Epoch 370, training loss: 0.11238830536603928 = 0.10533449798822403 + 0.001 * 7.053810119628906
Epoch 370, val loss: 0.7834876179695129
Epoch 380, training loss: 0.09971379488706589 = 0.09267693012952805 + 0.001 * 7.036860942840576
Epoch 380, val loss: 0.7942820191383362
Epoch 390, training loss: 0.08883380889892578 = 0.08180243521928787 + 0.001 * 7.031372547149658
Epoch 390, val loss: 0.8056572675704956
Epoch 400, training loss: 0.07945238798856735 = 0.07241714000701904 + 0.001 * 7.035245418548584
Epoch 400, val loss: 0.8173598647117615
Epoch 410, training loss: 0.07132071256637573 = 0.06429124623537064 + 0.001 * 7.029464244842529
Epoch 410, val loss: 0.8292279839515686
Epoch 420, training loss: 0.06426867097616196 = 0.05724484100937843 + 0.001 * 7.02383279800415
Epoch 420, val loss: 0.8410804271697998
Epoch 430, training loss: 0.05815695971250534 = 0.05112672969698906 + 0.001 * 7.030229091644287
Epoch 430, val loss: 0.8528532981872559
Epoch 440, training loss: 0.05283039063215256 = 0.045812640339136124 + 0.001 * 7.017748832702637
Epoch 440, val loss: 0.8644737005233765
Epoch 450, training loss: 0.04820849746465683 = 0.041192132979631424 + 0.001 * 7.016363620758057
Epoch 450, val loss: 0.8758736848831177
Epoch 460, training loss: 0.04419952258467674 = 0.03716960921883583 + 0.001 * 7.029913902282715
Epoch 460, val loss: 0.886997640132904
Epoch 470, training loss: 0.04067882150411606 = 0.03366050496697426 + 0.001 * 7.018315315246582
Epoch 470, val loss: 0.8978664875030518
Epoch 480, training loss: 0.037601009011268616 = 0.030592214316129684 + 0.001 * 7.008796215057373
Epoch 480, val loss: 0.908462643623352
Epoch 490, training loss: 0.03493661433458328 = 0.027901818975806236 + 0.001 * 7.034796714782715
Epoch 490, val loss: 0.9187471270561218
Epoch 500, training loss: 0.0325532928109169 = 0.025535639375448227 + 0.001 * 7.0176544189453125
Epoch 500, val loss: 0.9287311434745789
Epoch 510, training loss: 0.03045346587896347 = 0.023448139429092407 + 0.001 * 7.0053253173828125
Epoch 510, val loss: 0.9384302496910095
Epoch 520, training loss: 0.02860848605632782 = 0.02159985341131687 + 0.001 * 7.008632183074951
Epoch 520, val loss: 0.9477994441986084
Epoch 530, training loss: 0.026956144720315933 = 0.01995774172246456 + 0.001 * 6.998401641845703
Epoch 530, val loss: 0.9568712115287781
Epoch 540, training loss: 0.025518573820590973 = 0.018494058400392532 + 0.001 * 7.024515628814697
Epoch 540, val loss: 0.9656561017036438
Epoch 550, training loss: 0.024187443777918816 = 0.017185037955641747 + 0.001 * 7.002406120300293
Epoch 550, val loss: 0.9741687774658203
Epoch 560, training loss: 0.02302240952849388 = 0.016010627150535583 + 0.001 * 7.011782169342041
Epoch 560, val loss: 0.9823957681655884
Epoch 570, training loss: 0.021952399984002113 = 0.01495354063808918 + 0.001 * 6.99885892868042
Epoch 570, val loss: 0.9903594851493835
Epoch 580, training loss: 0.02099442481994629 = 0.013999279588460922 + 0.001 * 6.995143890380859
Epoch 580, val loss: 0.9980835914611816
Epoch 590, training loss: 0.020127758383750916 = 0.013135136105120182 + 0.001 * 6.992621421813965
Epoch 590, val loss: 1.0055441856384277
Epoch 600, training loss: 0.019356628879904747 = 0.012350372970104218 + 0.001 * 7.006256103515625
Epoch 600, val loss: 1.01280677318573
Epoch 610, training loss: 0.01862308755517006 = 0.011635799892246723 + 0.001 * 6.987288475036621
Epoch 610, val loss: 1.019848346710205
Epoch 620, training loss: 0.01797555945813656 = 0.010983526706695557 + 0.001 * 6.992033004760742
Epoch 620, val loss: 1.0266834497451782
Epoch 630, training loss: 0.017368342727422714 = 0.01038656011223793 + 0.001 * 6.981781482696533
Epoch 630, val loss: 1.0333282947540283
Epoch 640, training loss: 0.016836050897836685 = 0.009838917292654514 + 0.001 * 6.997134208679199
Epoch 640, val loss: 1.0397753715515137
Epoch 650, training loss: 0.016327904537320137 = 0.009335433132946491 + 0.001 * 6.992470741271973
Epoch 650, val loss: 1.0460431575775146
Epoch 660, training loss: 0.015848493203520775 = 0.008871512487530708 + 0.001 * 6.976980686187744
Epoch 660, val loss: 1.0521243810653687
Epoch 670, training loss: 0.015417592599987984 = 0.00844312459230423 + 0.001 * 6.974467754364014
Epoch 670, val loss: 1.0580321550369263
Epoch 680, training loss: 0.015015659853816032 = 0.008046727627515793 + 0.001 * 6.968931674957275
Epoch 680, val loss: 1.0637770891189575
Epoch 690, training loss: 0.014679999090731144 = 0.007679271046072245 + 0.001 * 7.000727653503418
Epoch 690, val loss: 1.0693609714508057
Epoch 700, training loss: 0.014313507825136185 = 0.0073380423709750175 + 0.001 * 6.975465297698975
Epoch 700, val loss: 1.074802041053772
Epoch 710, training loss: 0.013999409973621368 = 0.007020549848675728 + 0.001 * 6.978860378265381
Epoch 710, val loss: 1.0800970792770386
Epoch 720, training loss: 0.013694360852241516 = 0.00672466354444623 + 0.001 * 6.96969747543335
Epoch 720, val loss: 1.0852586030960083
Epoch 730, training loss: 0.013409333303570747 = 0.006448486354202032 + 0.001 * 6.960846424102783
Epoch 730, val loss: 1.090263843536377
Epoch 740, training loss: 0.013161003589630127 = 0.006190293002873659 + 0.001 * 6.970710277557373
Epoch 740, val loss: 1.0951712131500244
Epoch 750, training loss: 0.012918357737362385 = 0.005948565434664488 + 0.001 * 6.969791889190674
Epoch 750, val loss: 1.0999311208724976
Epoch 760, training loss: 0.012690848670899868 = 0.0057219164445996284 + 0.001 * 6.968931674957275
Epoch 760, val loss: 1.1045775413513184
Epoch 770, training loss: 0.012465888634324074 = 0.005509142763912678 + 0.001 * 6.956745624542236
Epoch 770, val loss: 1.1091020107269287
Epoch 780, training loss: 0.012271314859390259 = 0.005309122148901224 + 0.001 * 6.962193012237549
Epoch 780, val loss: 1.113518238067627
Epoch 790, training loss: 0.012077408842742443 = 0.005120863672345877 + 0.001 * 6.956544876098633
Epoch 790, val loss: 1.1178251504898071
Epoch 800, training loss: 0.0119249252602458 = 0.004943443927913904 + 0.001 * 6.981481075286865
Epoch 800, val loss: 1.1220223903656006
Epoch 810, training loss: 0.011736790649592876 = 0.0047760577872395515 + 0.001 * 6.960732460021973
Epoch 810, val loss: 1.1261218786239624
Epoch 820, training loss: 0.01156243309378624 = 0.004617935512214899 + 0.001 * 6.944497585296631
Epoch 820, val loss: 1.1301190853118896
Epoch 830, training loss: 0.011424079537391663 = 0.004468412138521671 + 0.001 * 6.955667018890381
Epoch 830, val loss: 1.1340125799179077
Epoch 840, training loss: 0.011271625757217407 = 0.004326853901147842 + 0.001 * 6.9447712898254395
Epoch 840, val loss: 1.1378329992294312
Epoch 850, training loss: 0.011142740957438946 = 0.004192694090306759 + 0.001 * 6.950046539306641
Epoch 850, val loss: 1.141562581062317
Epoch 860, training loss: 0.011007478460669518 = 0.004065379500389099 + 0.001 * 6.942099094390869
Epoch 860, val loss: 1.1452068090438843
Epoch 870, training loss: 0.010879112407565117 = 0.003944368567317724 + 0.001 * 6.934743404388428
Epoch 870, val loss: 1.148781657218933
Epoch 880, training loss: 0.010766050778329372 = 0.003829146968200803 + 0.001 * 6.936903476715088
Epoch 880, val loss: 1.152277946472168
Epoch 890, training loss: 0.01067269966006279 = 0.0037191796582192183 + 0.001 * 6.953519344329834
Epoch 890, val loss: 1.155700445175171
Epoch 900, training loss: 0.010552600026130676 = 0.003613976761698723 + 0.001 * 6.938623428344727
Epoch 900, val loss: 1.1590707302093506
Epoch 910, training loss: 0.01047139335423708 = 0.003513075178489089 + 0.001 * 6.958317756652832
Epoch 910, val loss: 1.1623915433883667
Epoch 920, training loss: 0.010367896407842636 = 0.003416077233850956 + 0.001 * 6.951818466186523
Epoch 920, val loss: 1.165648341178894
Epoch 930, training loss: 0.010253960266709328 = 0.0033226576633751392 + 0.001 * 6.931301593780518
Epoch 930, val loss: 1.168863296508789
Epoch 940, training loss: 0.010160524398088455 = 0.0032325463835150003 + 0.001 * 6.927978038787842
Epoch 940, val loss: 1.1720116138458252
Epoch 950, training loss: 0.010086690075695515 = 0.003145647468045354 + 0.001 * 6.941042423248291
Epoch 950, val loss: 1.1751248836517334
Epoch 960, training loss: 0.009990047663450241 = 0.0030618400778621435 + 0.001 * 6.9282073974609375
Epoch 960, val loss: 1.1781598329544067
Epoch 970, training loss: 0.0099420715123415 = 0.00298102293163538 + 0.001 * 6.961048603057861
Epoch 970, val loss: 1.1811552047729492
Epoch 980, training loss: 0.009832223877310753 = 0.002903125947341323 + 0.001 * 6.9290971755981445
Epoch 980, val loss: 1.1840898990631104
Epoch 990, training loss: 0.009787719696760178 = 0.0028281088452786207 + 0.001 * 6.959610462188721
Epoch 990, val loss: 1.1869735717773438
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 1.956432819366455 = 1.9478360414505005 + 0.001 * 8.59680461883545
Epoch 0, val loss: 1.9371333122253418
Epoch 10, training loss: 1.9460208415985107 = 1.9374241828918457 + 0.001 * 8.596711158752441
Epoch 10, val loss: 1.926177978515625
Epoch 20, training loss: 1.9331244230270386 = 1.9245280027389526 + 0.001 * 8.596463203430176
Epoch 20, val loss: 1.9125005006790161
Epoch 30, training loss: 1.9153512716293335 = 1.9067554473876953 + 0.001 * 8.595874786376953
Epoch 30, val loss: 1.893716812133789
Epoch 40, training loss: 1.8896349668502808 = 1.8810406923294067 + 0.001 * 8.594318389892578
Epoch 40, val loss: 1.8669875860214233
Epoch 50, training loss: 1.8543212413787842 = 1.8457317352294922 + 0.001 * 8.58950424194336
Epoch 50, val loss: 1.832105278968811
Epoch 60, training loss: 1.8142169713974 = 1.8056460618972778 + 0.001 * 8.570853233337402
Epoch 60, val loss: 1.7963813543319702
Epoch 70, training loss: 1.7776401042938232 = 1.7691521644592285 + 0.001 * 8.487963676452637
Epoch 70, val loss: 1.767921805381775
Epoch 80, training loss: 1.7331963777542114 = 1.7251293659210205 + 0.001 * 8.066978454589844
Epoch 80, val loss: 1.7306663990020752
Epoch 90, training loss: 1.6707842350006104 = 1.662943959236145 + 0.001 * 7.840256214141846
Epoch 90, val loss: 1.674167513847351
Epoch 100, training loss: 1.5866997241973877 = 1.5789583921432495 + 0.001 * 7.741274356842041
Epoch 100, val loss: 1.5996840000152588
Epoch 110, training loss: 1.4866552352905273 = 1.479066252708435 + 0.001 * 7.588949203491211
Epoch 110, val loss: 1.514286756515503
Epoch 120, training loss: 1.3817745447158813 = 1.3743553161621094 + 0.001 * 7.419280052185059
Epoch 120, val loss: 1.4272429943084717
Epoch 130, training loss: 1.2785567045211792 = 1.2711639404296875 + 0.001 * 7.392744541168213
Epoch 130, val loss: 1.345058798789978
Epoch 140, training loss: 1.1782591342926025 = 1.1709048748016357 + 0.001 * 7.354212284088135
Epoch 140, val loss: 1.268284797668457
Epoch 150, training loss: 1.0800567865371704 = 1.0727401971817017 + 0.001 * 7.316544055938721
Epoch 150, val loss: 1.1944594383239746
Epoch 160, training loss: 0.9836509227752686 = 0.9763665199279785 + 0.001 * 7.284414768218994
Epoch 160, val loss: 1.1222436428070068
Epoch 170, training loss: 0.8913856148719788 = 0.8841320872306824 + 0.001 * 7.253498554229736
Epoch 170, val loss: 1.0535286664962769
Epoch 180, training loss: 0.8070606589317322 = 0.7998336553573608 + 0.001 * 7.227007865905762
Epoch 180, val loss: 0.9921955466270447
Epoch 190, training loss: 0.7328290343284607 = 0.7256335616111755 + 0.001 * 7.195449352264404
Epoch 190, val loss: 0.94122713804245
Epoch 200, training loss: 0.6682340502738953 = 0.6610774397850037 + 0.001 * 7.156639099121094
Epoch 200, val loss: 0.9012973308563232
Epoch 210, training loss: 0.6113126277923584 = 0.6041890382766724 + 0.001 * 7.123569011688232
Epoch 210, val loss: 0.8707666993141174
Epoch 220, training loss: 0.5599427819252014 = 0.5528331995010376 + 0.001 * 7.109570503234863
Epoch 220, val loss: 0.8472012281417847
Epoch 230, training loss: 0.5121392607688904 = 0.5050361752510071 + 0.001 * 7.103065490722656
Epoch 230, val loss: 0.8282042145729065
Epoch 240, training loss: 0.466255247592926 = 0.45915457606315613 + 0.001 * 7.100670337677002
Epoch 240, val loss: 0.8124973773956299
Epoch 250, training loss: 0.4211442172527313 = 0.4140470325946808 + 0.001 * 7.097172737121582
Epoch 250, val loss: 0.7998520731925964
Epoch 260, training loss: 0.3765535354614258 = 0.36946138739585876 + 0.001 * 7.092138767242432
Epoch 260, val loss: 0.7905731797218323
Epoch 270, training loss: 0.3332803249359131 = 0.32619524002075195 + 0.001 * 7.0850934982299805
Epoch 270, val loss: 0.7849373817443848
Epoch 280, training loss: 0.2929653823375702 = 0.28588956594467163 + 0.001 * 7.0758185386657715
Epoch 280, val loss: 0.7835567593574524
Epoch 290, training loss: 0.25698333978652954 = 0.24991647899150848 + 0.001 * 7.066864013671875
Epoch 290, val loss: 0.7864260673522949
Epoch 300, training loss: 0.22578614950180054 = 0.21873213350772858 + 0.001 * 7.054009437561035
Epoch 300, val loss: 0.7929946184158325
Epoch 310, training loss: 0.1990794688463211 = 0.19203877449035645 + 0.001 * 7.0406904220581055
Epoch 310, val loss: 0.8026354312896729
Epoch 320, training loss: 0.1762540340423584 = 0.16922199726104736 + 0.001 * 7.032037258148193
Epoch 320, val loss: 0.8147927522659302
Epoch 330, training loss: 0.1566610038280487 = 0.1496371030807495 + 0.001 * 7.023893356323242
Epoch 330, val loss: 0.8289129137992859
Epoch 340, training loss: 0.13974809646606445 = 0.13272680342197418 + 0.001 * 7.0212931632995605
Epoch 340, val loss: 0.8445073962211609
Epoch 350, training loss: 0.12506161630153656 = 0.11804290115833282 + 0.001 * 7.018714427947998
Epoch 350, val loss: 0.8611565828323364
Epoch 360, training loss: 0.1122448667883873 = 0.10522660613059998 + 0.001 * 7.0182576179504395
Epoch 360, val loss: 0.8785490989685059
Epoch 370, training loss: 0.10101541131734848 = 0.09399746358394623 + 0.001 * 7.017945766448975
Epoch 370, val loss: 0.896361768245697
Epoch 380, training loss: 0.09114671498537064 = 0.0841297060251236 + 0.001 * 7.0170111656188965
Epoch 380, val loss: 0.9143544435501099
Epoch 390, training loss: 0.0824572965502739 = 0.07544122636318207 + 0.001 * 7.016067028045654
Epoch 390, val loss: 0.9323325157165527
Epoch 400, training loss: 0.07480011135339737 = 0.06778210401535034 + 0.001 * 7.018005847930908
Epoch 400, val loss: 0.9501563310623169
Epoch 410, training loss: 0.06804459542036057 = 0.061027802526950836 + 0.001 * 7.0167927742004395
Epoch 410, val loss: 0.9677031636238098
Epoch 420, training loss: 0.062083110213279724 = 0.05506669357419014 + 0.001 * 7.016415119171143
Epoch 420, val loss: 0.9849037528038025
Epoch 430, training loss: 0.056818753480911255 = 0.04980306327342987 + 0.001 * 7.015688419342041
Epoch 430, val loss: 1.0017322301864624
Epoch 440, training loss: 0.052170731127262115 = 0.045153334736824036 + 0.001 * 7.017396450042725
Epoch 440, val loss: 1.0181008577346802
Epoch 450, training loss: 0.04805804789066315 = 0.04104173928499222 + 0.001 * 7.016306400299072
Epoch 450, val loss: 1.0340162515640259
Epoch 460, training loss: 0.04441671445965767 = 0.03740101307630539 + 0.001 * 7.0157012939453125
Epoch 460, val loss: 1.049487590789795
Epoch 470, training loss: 0.04118797183036804 = 0.03417238965630531 + 0.001 * 7.0155816078186035
Epoch 470, val loss: 1.0644994974136353
Epoch 480, training loss: 0.0383191742002964 = 0.031305015087127686 + 0.001 * 7.014157772064209
Epoch 480, val loss: 1.0790534019470215
Epoch 490, training loss: 0.035765424370765686 = 0.028752516955137253 + 0.001 * 7.012905120849609
Epoch 490, val loss: 1.0931236743927002
Epoch 500, training loss: 0.033486977219581604 = 0.0264733899384737 + 0.001 * 7.013587951660156
Epoch 500, val loss: 1.1067631244659424
Epoch 510, training loss: 0.03143756464123726 = 0.024427499622106552 + 0.001 * 7.01006555557251
Epoch 510, val loss: 1.1199449300765991
Epoch 520, training loss: 0.029592473059892654 = 0.022582782432436943 + 0.001 * 7.009689807891846
Epoch 520, val loss: 1.1327037811279297
Epoch 530, training loss: 0.027921777218580246 = 0.020915716886520386 + 0.001 * 7.006060600280762
Epoch 530, val loss: 1.1450127363204956
Epoch 540, training loss: 0.02641965262591839 = 0.01940758526325226 + 0.001 * 7.01206636428833
Epoch 540, val loss: 1.1568810939788818
Epoch 550, training loss: 0.025047142058610916 = 0.018042054027318954 + 0.001 * 7.005087375640869
Epoch 550, val loss: 1.1683233976364136
Epoch 560, training loss: 0.023805100470781326 = 0.016804583370685577 + 0.001 * 7.000517845153809
Epoch 560, val loss: 1.1793729066848755
Epoch 570, training loss: 0.022680986672639847 = 0.015682026743888855 + 0.001 * 6.998960018157959
Epoch 570, val loss: 1.1900371313095093
Epoch 580, training loss: 0.0216579157859087 = 0.014662403613328934 + 0.001 * 6.99551248550415
Epoch 580, val loss: 1.2003110647201538
Epoch 590, training loss: 0.020727116614580154 = 0.01373505312949419 + 0.001 * 6.992062091827393
Epoch 590, val loss: 1.2102391719818115
Epoch 600, training loss: 0.019877323880791664 = 0.012890329584479332 + 0.001 * 6.98699426651001
Epoch 600, val loss: 1.219815969467163
Epoch 610, training loss: 0.01910790428519249 = 0.012119598686695099 + 0.001 * 6.988305568695068
Epoch 610, val loss: 1.2290678024291992
Epoch 620, training loss: 0.018412984907627106 = 0.011415194720029831 + 0.001 * 6.9977898597717285
Epoch 620, val loss: 1.2380133867263794
Epoch 630, training loss: 0.017754480242729187 = 0.010770266875624657 + 0.001 * 6.984213829040527
Epoch 630, val loss: 1.2466477155685425
Epoch 640, training loss: 0.0171512458473444 = 0.01017874013632536 + 0.001 * 6.972505569458008
Epoch 640, val loss: 1.2550026178359985
Epoch 650, training loss: 0.016602737829089165 = 0.00963518489152193 + 0.001 * 6.967552661895752
Epoch 650, val loss: 1.2630870342254639
Epoch 660, training loss: 0.016100086271762848 = 0.009134792722761631 + 0.001 * 6.965294361114502
Epoch 660, val loss: 1.2709118127822876
Epoch 670, training loss: 0.015638891607522964 = 0.008673332631587982 + 0.001 * 6.9655585289001465
Epoch 670, val loss: 1.2784820795059204
Epoch 680, training loss: 0.015208999626338482 = 0.008247055113315582 + 0.001 * 6.961944103240967
Epoch 680, val loss: 1.2858343124389648
Epoch 690, training loss: 0.014832179993391037 = 0.007852579466998577 + 0.001 * 6.979600429534912
Epoch 690, val loss: 1.2929418087005615
Epoch 700, training loss: 0.014446237124502659 = 0.0074869198724627495 + 0.001 * 6.959316730499268
Epoch 700, val loss: 1.2998449802398682
Epoch 710, training loss: 0.014104776084423065 = 0.00714744720607996 + 0.001 * 6.9573283195495605
Epoch 710, val loss: 1.3065444231033325
Epoch 720, training loss: 0.013783805072307587 = 0.00683173630386591 + 0.001 * 6.95206880569458
Epoch 720, val loss: 1.313034176826477
Epoch 730, training loss: 0.013487821444869041 = 0.006537680048495531 + 0.001 * 6.950140476226807
Epoch 730, val loss: 1.3193336725234985
Epoch 740, training loss: 0.01321294903755188 = 0.00626346655189991 + 0.001 * 6.949482440948486
Epoch 740, val loss: 1.3254544734954834
Epoch 750, training loss: 0.012956642545759678 = 0.0060073924250900745 + 0.001 * 6.949249744415283
Epoch 750, val loss: 1.3314297199249268
Epoch 760, training loss: 0.012713680043816566 = 0.005767844617366791 + 0.001 * 6.945834636688232
Epoch 760, val loss: 1.3372310400009155
Epoch 770, training loss: 0.01248885691165924 = 0.005543433129787445 + 0.001 * 6.9454240798950195
Epoch 770, val loss: 1.3428752422332764
Epoch 780, training loss: 0.012293320149183273 = 0.005333006847649813 + 0.001 * 6.960313320159912
Epoch 780, val loss: 1.348441481590271
Epoch 790, training loss: 0.01208712998777628 = 0.005135359708219767 + 0.001 * 6.951769828796387
Epoch 790, val loss: 1.353852391242981
Epoch 800, training loss: 0.011885062791407108 = 0.004949221387505531 + 0.001 * 6.935841083526611
Epoch 800, val loss: 1.359062671661377
Epoch 810, training loss: 0.01170683279633522 = 0.004773556720465422 + 0.001 * 6.9332756996154785
Epoch 810, val loss: 1.3641705513000488
Epoch 820, training loss: 0.011547278612852097 = 0.0046076164580881596 + 0.001 * 6.939661979675293
Epoch 820, val loss: 1.369153618812561
Epoch 830, training loss: 0.01138373278081417 = 0.004450355656445026 + 0.001 * 6.933377265930176
Epoch 830, val loss: 1.374043583869934
Epoch 840, training loss: 0.01123193372040987 = 0.004301042761653662 + 0.001 * 6.9308905601501465
Epoch 840, val loss: 1.3787847757339478
Epoch 850, training loss: 0.011085959151387215 = 0.004158994182944298 + 0.001 * 6.92696475982666
Epoch 850, val loss: 1.383431077003479
Epoch 860, training loss: 0.010968338698148727 = 0.0040236571803689 + 0.001 * 6.944680690765381
Epoch 860, val loss: 1.3879345655441284
Epoch 870, training loss: 0.010821018368005753 = 0.00389460357837379 + 0.001 * 6.926414489746094
Epoch 870, val loss: 1.392340064048767
Epoch 880, training loss: 0.010696801356971264 = 0.003771349089220166 + 0.001 * 6.92545223236084
Epoch 880, val loss: 1.396644115447998
Epoch 890, training loss: 0.010584095492959023 = 0.0036535547114908695 + 0.001 * 6.930540084838867
Epoch 890, val loss: 1.4008586406707764
Epoch 900, training loss: 0.01047069113701582 = 0.003541139420121908 + 0.001 * 6.929551601409912
Epoch 900, val loss: 1.4049217700958252
Epoch 910, training loss: 0.010360720567405224 = 0.003433791222050786 + 0.001 * 6.926929473876953
Epoch 910, val loss: 1.4089484214782715
Epoch 920, training loss: 0.010246573016047478 = 0.003331257263198495 + 0.001 * 6.9153151512146
Epoch 920, val loss: 1.4128320217132568
Epoch 930, training loss: 0.010153555311262608 = 0.003233252791687846 + 0.001 * 6.920302391052246
Epoch 930, val loss: 1.4166455268859863
Epoch 940, training loss: 0.010058448649942875 = 0.0031396124977618456 + 0.001 * 6.918835639953613
Epoch 940, val loss: 1.4203664064407349
Epoch 950, training loss: 0.009962281212210655 = 0.0030501876026391983 + 0.001 * 6.912092685699463
Epoch 950, val loss: 1.424021601676941
Epoch 960, training loss: 0.009875917807221413 = 0.0029648360796272755 + 0.001 * 6.911081314086914
Epoch 960, val loss: 1.4275598526000977
Epoch 970, training loss: 0.009811077266931534 = 0.0028832212556153536 + 0.001 * 6.927855491638184
Epoch 970, val loss: 1.4310226440429688
Epoch 980, training loss: 0.009729024022817612 = 0.002805204363539815 + 0.001 * 6.923819541931152
Epoch 980, val loss: 1.434444785118103
Epoch 990, training loss: 0.009652946144342422 = 0.0027306543197482824 + 0.001 * 6.9222917556762695
Epoch 990, val loss: 1.4377607107162476
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8434370057986295
=== training gcn model ===
Epoch 0, training loss: 1.9366414546966553 = 1.9280446767807007 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.9290330410003662
Epoch 10, training loss: 1.927290916442871 = 1.9186941385269165 + 0.001 * 8.596782684326172
Epoch 10, val loss: 1.920328974723816
Epoch 20, training loss: 1.9156380891799927 = 1.9070415496826172 + 0.001 * 8.596585273742676
Epoch 20, val loss: 1.9091349840164185
Epoch 30, training loss: 1.8990237712860107 = 1.890427589416504 + 0.001 * 8.596139907836914
Epoch 30, val loss: 1.8929593563079834
Epoch 40, training loss: 1.8743762969970703 = 1.865781307220459 + 0.001 * 8.595022201538086
Epoch 40, val loss: 1.8693976402282715
Epoch 50, training loss: 1.8405314683914185 = 1.831939935684204 + 0.001 * 8.591590881347656
Epoch 50, val loss: 1.8386530876159668
Epoch 60, training loss: 1.8024567365646362 = 1.7938791513442993 + 0.001 * 8.577628135681152
Epoch 60, val loss: 1.806563377380371
Epoch 70, training loss: 1.7632347345352173 = 1.7547343969345093 + 0.001 * 8.50035572052002
Epoch 70, val loss: 1.7716313600540161
Epoch 80, training loss: 1.7081527709960938 = 1.700105905532837 + 0.001 * 8.04682731628418
Epoch 80, val loss: 1.7192604541778564
Epoch 90, training loss: 1.6324048042297363 = 1.6244970560073853 + 0.001 * 7.907745838165283
Epoch 90, val loss: 1.651319146156311
Epoch 100, training loss: 1.5381426811218262 = 1.5303099155426025 + 0.001 * 7.832727909088135
Epoch 100, val loss: 1.5717183351516724
Epoch 110, training loss: 1.4371929168701172 = 1.4295092821121216 + 0.001 * 7.6836066246032715
Epoch 110, val loss: 1.4849920272827148
Epoch 120, training loss: 1.3363302946090698 = 1.328822135925293 + 0.001 * 7.508180141448975
Epoch 120, val loss: 1.4004473686218262
Epoch 130, training loss: 1.2372876405715942 = 1.2298247814178467 + 0.001 * 7.462875843048096
Epoch 130, val loss: 1.3187140226364136
Epoch 140, training loss: 1.1419790983200073 = 1.1345772743225098 + 0.001 * 7.40186882019043
Epoch 140, val loss: 1.2423471212387085
Epoch 150, training loss: 1.0523629188537598 = 1.0450304746627808 + 0.001 * 7.3324971199035645
Epoch 150, val loss: 1.17234468460083
Epoch 160, training loss: 0.9684238433837891 = 0.961146891117096 + 0.001 * 7.276961326599121
Epoch 160, val loss: 1.1081945896148682
Epoch 170, training loss: 0.8887462615966797 = 0.8815259337425232 + 0.001 * 7.220323085784912
Epoch 170, val loss: 1.0473836660385132
Epoch 180, training loss: 0.8125362992286682 = 0.8053630590438843 + 0.001 * 7.173264503479004
Epoch 180, val loss: 0.9888728260993958
Epoch 190, training loss: 0.740371823310852 = 0.7332305908203125 + 0.001 * 7.141237735748291
Epoch 190, val loss: 0.9341211318969727
Epoch 200, training loss: 0.6734782457351685 = 0.6663576364517212 + 0.001 * 7.120595932006836
Epoch 200, val loss: 0.8856170773506165
Epoch 210, training loss: 0.6126596331596375 = 0.6055582165718079 + 0.001 * 7.101393699645996
Epoch 210, val loss: 0.8453362584114075
Epoch 220, training loss: 0.5579758882522583 = 0.5508934259414673 + 0.001 * 7.0824408531188965
Epoch 220, val loss: 0.8142269849777222
Epoch 230, training loss: 0.5090600252151489 = 0.5019881129264832 + 0.001 * 7.071922302246094
Epoch 230, val loss: 0.7919220924377441
Epoch 240, training loss: 0.46535152196884155 = 0.4582861661911011 + 0.001 * 7.065356731414795
Epoch 240, val loss: 0.777014434337616
Epoch 250, training loss: 0.4262816905975342 = 0.4192214608192444 + 0.001 * 7.060224533081055
Epoch 250, val loss: 0.7679165601730347
Epoch 260, training loss: 0.39138269424438477 = 0.3843242824077606 + 0.001 * 7.0584211349487305
Epoch 260, val loss: 0.7636168599128723
Epoch 270, training loss: 0.36016106605529785 = 0.3531036972999573 + 0.001 * 7.057368755340576
Epoch 270, val loss: 0.7634602785110474
Epoch 280, training loss: 0.33201032876968384 = 0.32495343685150146 + 0.001 * 7.0569000244140625
Epoch 280, val loss: 0.7667120099067688
Epoch 290, training loss: 0.3062831461429596 = 0.29922598600387573 + 0.001 * 7.057162761688232
Epoch 290, val loss: 0.7726733088493347
Epoch 300, training loss: 0.2823750674724579 = 0.27531930804252625 + 0.001 * 7.0557541847229
Epoch 300, val loss: 0.7808961272239685
Epoch 310, training loss: 0.25977566838264465 = 0.2527211904525757 + 0.001 * 7.054483413696289
Epoch 310, val loss: 0.7911702394485474
Epoch 320, training loss: 0.238131582736969 = 0.23107755184173584 + 0.001 * 7.0540266036987305
Epoch 320, val loss: 0.8032301664352417
Epoch 330, training loss: 0.21729496121406555 = 0.21024107933044434 + 0.001 * 7.05387544631958
Epoch 330, val loss: 0.8169224858283997
Epoch 340, training loss: 0.19736099243164062 = 0.1903076469898224 + 0.001 * 7.053341388702393
Epoch 340, val loss: 0.8322007060050964
Epoch 350, training loss: 0.1785804033279419 = 0.1715226173400879 + 0.001 * 7.057787895202637
Epoch 350, val loss: 0.8491233587265015
Epoch 360, training loss: 0.16116975247859955 = 0.1541152000427246 + 0.001 * 7.054548263549805
Epoch 360, val loss: 0.8675747513771057
Epoch 370, training loss: 0.1452140361070633 = 0.1381603181362152 + 0.001 * 7.053712368011475
Epoch 370, val loss: 0.887302577495575
Epoch 380, training loss: 0.13065330684185028 = 0.12359995394945145 + 0.001 * 7.053351402282715
Epoch 380, val loss: 0.9082275629043579
Epoch 390, training loss: 0.11738947033882141 = 0.11033544689416885 + 0.001 * 7.054022312164307
Epoch 390, val loss: 0.9301342964172363
Epoch 400, training loss: 0.10533133149147034 = 0.09827695786952972 + 0.001 * 7.054372787475586
Epoch 400, val loss: 0.9527859091758728
Epoch 410, training loss: 0.09442138671875 = 0.08736860007047653 + 0.001 * 7.052786350250244
Epoch 410, val loss: 0.9759681820869446
Epoch 420, training loss: 0.08462416380643845 = 0.0775710865855217 + 0.001 * 7.053077697753906
Epoch 420, val loss: 0.9994221925735474
Epoch 430, training loss: 0.07588867843151093 = 0.06883547455072403 + 0.001 * 7.053202152252197
Epoch 430, val loss: 1.0230686664581299
Epoch 440, training loss: 0.06816139072179794 = 0.06110883504152298 + 0.001 * 7.052558422088623
Epoch 440, val loss: 1.0467073917388916
Epoch 450, training loss: 0.06138118356466293 = 0.05432913452386856 + 0.001 * 7.052046775817871
Epoch 450, val loss: 1.0702362060546875
Epoch 460, training loss: 0.05547112599015236 = 0.04841906577348709 + 0.001 * 7.052059173583984
Epoch 460, val loss: 1.09348726272583
Epoch 470, training loss: 0.05033775418996811 = 0.043286219239234924 + 0.001 * 7.051535606384277
Epoch 470, val loss: 1.1163204908370972
Epoch 480, training loss: 0.04588482156395912 = 0.03883345052599907 + 0.001 * 7.051370143890381
Epoch 480, val loss: 1.1385924816131592
Epoch 490, training loss: 0.04202055186033249 = 0.034967634826898575 + 0.001 * 7.052916049957275
Epoch 490, val loss: 1.1602215766906738
Epoch 500, training loss: 0.038654714822769165 = 0.03160377964377403 + 0.001 * 7.050936222076416
Epoch 500, val loss: 1.181174397468567
Epoch 510, training loss: 0.03571806848049164 = 0.028669504448771477 + 0.001 * 7.048562526702881
Epoch 510, val loss: 1.2014005184173584
Epoch 520, training loss: 0.03314971178770065 = 0.026102503761649132 + 0.001 * 7.047206401824951
Epoch 520, val loss: 1.2208784818649292
Epoch 530, training loss: 0.03089458867907524 = 0.023848768323659897 + 0.001 * 7.045820236206055
Epoch 530, val loss: 1.2396199703216553
Epoch 540, training loss: 0.028919685631990433 = 0.021864037960767746 + 0.001 * 7.055647850036621
Epoch 540, val loss: 1.2576441764831543
Epoch 550, training loss: 0.027158375829458237 = 0.020110612735152245 + 0.001 * 7.047761917114258
Epoch 550, val loss: 1.27496337890625
Epoch 560, training loss: 0.02559899166226387 = 0.018556619063019753 + 0.001 * 7.042372703552246
Epoch 560, val loss: 1.2915854454040527
Epoch 570, training loss: 0.024214453995227814 = 0.01717459410429001 + 0.001 * 7.039858818054199
Epoch 570, val loss: 1.3075445890426636
Epoch 580, training loss: 0.022983934730291367 = 0.015941208228468895 + 0.001 * 7.042725563049316
Epoch 580, val loss: 1.3228352069854736
Epoch 590, training loss: 0.021875543519854546 = 0.014837009832262993 + 0.001 * 7.038533687591553
Epoch 590, val loss: 1.3375064134597778
Epoch 600, training loss: 0.02088281884789467 = 0.013845509849488735 + 0.001 * 7.037309169769287
Epoch 600, val loss: 1.3516701459884644
Epoch 610, training loss: 0.019985392689704895 = 0.012952105142176151 + 0.001 * 7.03328800201416
Epoch 610, val loss: 1.365283727645874
Epoch 620, training loss: 0.01918093115091324 = 0.012144820764660835 + 0.001 * 7.03610897064209
Epoch 620, val loss: 1.3784149885177612
Epoch 630, training loss: 0.01844385452568531 = 0.011413071304559708 + 0.001 * 7.030782699584961
Epoch 630, val loss: 1.391004204750061
Epoch 640, training loss: 0.017778562381863594 = 0.010747874155640602 + 0.001 * 7.0306878089904785
Epoch 640, val loss: 1.4031140804290771
Epoch 650, training loss: 0.017164550721645355 = 0.010141595266759396 + 0.001 * 7.022953987121582
Epoch 650, val loss: 1.4148021936416626
Epoch 660, training loss: 0.016609210520982742 = 0.009587527252733707 + 0.001 * 7.021683216094971
Epoch 660, val loss: 1.4260560274124146
Epoch 670, training loss: 0.01609686389565468 = 0.00907994620501995 + 0.001 * 7.0169172286987305
Epoch 670, val loss: 1.436890959739685
Epoch 680, training loss: 0.015628919005393982 = 0.008613775484263897 + 0.001 * 7.015142917633057
Epoch 680, val loss: 1.4473682641983032
Epoch 690, training loss: 0.015193738043308258 = 0.008184653706848621 + 0.001 * 7.009084701538086
Epoch 690, val loss: 1.4574871063232422
Epoch 700, training loss: 0.01482407283037901 = 0.007788790389895439 + 0.001 * 7.035282135009766
Epoch 700, val loss: 1.4672675132751465
Epoch 710, training loss: 0.014450209215283394 = 0.00742284394800663 + 0.001 * 7.027365207672119
Epoch 710, val loss: 1.4767117500305176
Epoch 720, training loss: 0.01408989168703556 = 0.007083921693265438 + 0.001 * 7.005969524383545
Epoch 720, val loss: 1.485866665840149
Epoch 730, training loss: 0.013772470876574516 = 0.006769402418285608 + 0.001 * 7.003067493438721
Epoch 730, val loss: 1.494705319404602
Epoch 740, training loss: 0.013505330309271812 = 0.006476979702711105 + 0.001 * 7.028349876403809
Epoch 740, val loss: 1.503277063369751
Epoch 750, training loss: 0.01320670172572136 = 0.006204644683748484 + 0.001 * 7.002057075500488
Epoch 750, val loss: 1.5115551948547363
Epoch 760, training loss: 0.012952431105077267 = 0.005950623657554388 + 0.001 * 7.00180721282959
Epoch 760, val loss: 1.5195729732513428
Epoch 770, training loss: 0.012709084898233414 = 0.005713306833058596 + 0.001 * 6.995777130126953
Epoch 770, val loss: 1.5273613929748535
Epoch 780, training loss: 0.012474112212657928 = 0.005491258576512337 + 0.001 * 6.982852935791016
Epoch 780, val loss: 1.5349035263061523
Epoch 790, training loss: 0.012276027351617813 = 0.005283203441649675 + 0.001 * 6.992823600769043
Epoch 790, val loss: 1.5422292947769165
Epoch 800, training loss: 0.012082556262612343 = 0.005088004283607006 + 0.001 * 6.994552135467529
Epoch 800, val loss: 1.5492961406707764
Epoch 810, training loss: 0.011883038096129894 = 0.004904697649180889 + 0.001 * 6.978340148925781
Epoch 810, val loss: 1.5562129020690918
Epoch 820, training loss: 0.0117112398147583 = 0.004732251632958651 + 0.001 * 6.978987693786621
Epoch 820, val loss: 1.5629122257232666
Epoch 830, training loss: 0.011542863212525845 = 0.004569780081510544 + 0.001 * 6.973083019256592
Epoch 830, val loss: 1.5694067478179932
Epoch 840, training loss: 0.011404846794903278 = 0.004416545387357473 + 0.001 * 6.9883012771606445
Epoch 840, val loss: 1.575715184211731
Epoch 850, training loss: 0.011244295164942741 = 0.004271856974810362 + 0.001 * 6.972438335418701
Epoch 850, val loss: 1.5818307399749756
Epoch 860, training loss: 0.011103611439466476 = 0.004135109484195709 + 0.001 * 6.968501091003418
Epoch 860, val loss: 1.5877995491027832
Epoch 870, training loss: 0.010978592559695244 = 0.00400571059435606 + 0.001 * 6.972881317138672
Epoch 870, val loss: 1.5935592651367188
Epoch 880, training loss: 0.010853881947696209 = 0.0038831382989883423 + 0.001 * 6.970743179321289
Epoch 880, val loss: 1.5991827249526978
Epoch 890, training loss: 0.010729305446147919 = 0.003766923677176237 + 0.001 * 6.962381362915039
Epoch 890, val loss: 1.6046689748764038
Epoch 900, training loss: 0.01063250470906496 = 0.003656636690720916 + 0.001 * 6.975867748260498
Epoch 900, val loss: 1.609976053237915
Epoch 910, training loss: 0.010517317801713943 = 0.0035518882796168327 + 0.001 * 6.965429306030273
Epoch 910, val loss: 1.6151440143585205
Epoch 920, training loss: 0.010427548550069332 = 0.0034522756468504667 + 0.001 * 6.9752726554870605
Epoch 920, val loss: 1.6201940774917603
Epoch 930, training loss: 0.010319482535123825 = 0.0033574621193110943 + 0.001 * 6.962019443511963
Epoch 930, val loss: 1.6251164674758911
Epoch 940, training loss: 0.010224398225545883 = 0.0032671759836375713 + 0.001 * 6.957221508026123
Epoch 940, val loss: 1.629915714263916
Epoch 950, training loss: 0.01015779934823513 = 0.003181114327162504 + 0.001 * 6.976685047149658
Epoch 950, val loss: 1.634575366973877
Epoch 960, training loss: 0.010056217201054096 = 0.0030990205705165863 + 0.001 * 6.957196235656738
Epoch 960, val loss: 1.6391371488571167
Epoch 970, training loss: 0.010029304772615433 = 0.003020652336999774 + 0.001 * 7.0086517333984375
Epoch 970, val loss: 1.6435807943344116
Epoch 980, training loss: 0.009900538250803947 = 0.0029457691125571728 + 0.001 * 6.954769134521484
Epoch 980, val loss: 1.6479283571243286
Epoch 990, training loss: 0.00981991644948721 = 0.00287420186214149 + 0.001 * 6.945713996887207
Epoch 990, val loss: 1.6521955728530884
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8365840801265156
The final CL Acc:0.79012, 0.01823, The final GNN Acc:0.83869, 0.00336
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10514])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9621416330337524 = 1.9535447359085083 + 0.001 * 8.5968656539917
Epoch 0, val loss: 1.9402347803115845
Epoch 10, training loss: 1.9507652521133423 = 1.9421684741973877 + 0.001 * 8.596818923950195
Epoch 10, val loss: 1.9291003942489624
Epoch 20, training loss: 1.9366792440414429 = 1.9280825853347778 + 0.001 * 8.59664249420166
Epoch 20, val loss: 1.91498863697052
Epoch 30, training loss: 1.9170560836791992 = 1.9084599018096924 + 0.001 * 8.596220970153809
Epoch 30, val loss: 1.8951618671417236
Epoch 40, training loss: 1.8888788223266602 = 1.8802837133407593 + 0.001 * 8.595075607299805
Epoch 40, val loss: 1.867081880569458
Epoch 50, training loss: 1.8515257835388184 = 1.842934489250183 + 0.001 * 8.591337203979492
Epoch 50, val loss: 1.8321869373321533
Epoch 60, training loss: 1.8137379884719849 = 1.8051615953445435 + 0.001 * 8.576385498046875
Epoch 60, val loss: 1.802873969078064
Epoch 70, training loss: 1.784042239189148 = 1.7755306959152222 + 0.001 * 8.51153564453125
Epoch 70, val loss: 1.784988522529602
Epoch 80, training loss: 1.7457809448242188 = 1.7375659942626953 + 0.001 * 8.214954376220703
Epoch 80, val loss: 1.7553654909133911
Epoch 90, training loss: 1.6915091276168823 = 1.6833982467651367 + 0.001 * 8.110889434814453
Epoch 90, val loss: 1.7082856893539429
Epoch 100, training loss: 1.6174081563949585 = 1.6093403100967407 + 0.001 * 8.067824363708496
Epoch 100, val loss: 1.6454023122787476
Epoch 110, training loss: 1.5308287143707275 = 1.5227906703948975 + 0.001 * 8.038084030151367
Epoch 110, val loss: 1.5759878158569336
Epoch 120, training loss: 1.441843032836914 = 1.4338579177856445 + 0.001 * 7.985105991363525
Epoch 120, val loss: 1.5059046745300293
Epoch 130, training loss: 1.3524560928344727 = 1.3446619510650635 + 0.001 * 7.794183254241943
Epoch 130, val loss: 1.4383368492126465
Epoch 140, training loss: 1.2594990730285645 = 1.2518905401229858 + 0.001 * 7.608542442321777
Epoch 140, val loss: 1.3696585893630981
Epoch 150, training loss: 1.160470724105835 = 1.1528738737106323 + 0.001 * 7.596903324127197
Epoch 150, val loss: 1.2968533039093018
Epoch 160, training loss: 1.0562537908554077 = 1.048678994178772 + 0.001 * 7.574822425842285
Epoch 160, val loss: 1.2219644784927368
Epoch 170, training loss: 0.951664388179779 = 0.9441192746162415 + 0.001 * 7.545090675354004
Epoch 170, val loss: 1.1485947370529175
Epoch 180, training loss: 0.8535162210464478 = 0.845996081829071 + 0.001 * 7.520138263702393
Epoch 180, val loss: 1.0827035903930664
Epoch 190, training loss: 0.7671030759811401 = 0.7596006989479065 + 0.001 * 7.502351760864258
Epoch 190, val loss: 1.0289230346679688
Epoch 200, training loss: 0.6936095952987671 = 0.6861182451248169 + 0.001 * 7.491355895996094
Epoch 200, val loss: 0.9887024164199829
Epoch 210, training loss: 0.6309837102890015 = 0.6235103607177734 + 0.001 * 7.4733428955078125
Epoch 210, val loss: 0.9611527323722839
Epoch 220, training loss: 0.5764851570129395 = 0.5690447092056274 + 0.001 * 7.440476417541504
Epoch 220, val loss: 0.9442235827445984
Epoch 230, training loss: 0.5279030799865723 = 0.5204920768737793 + 0.001 * 7.4109907150268555
Epoch 230, val loss: 0.935623049736023
Epoch 240, training loss: 0.4836322069168091 = 0.47624993324279785 + 0.001 * 7.382274150848389
Epoch 240, val loss: 0.9334198832511902
Epoch 250, training loss: 0.442685604095459 = 0.43531861901283264 + 0.001 * 7.366971015930176
Epoch 250, val loss: 0.9359543323516846
Epoch 260, training loss: 0.4045601487159729 = 0.39720049500465393 + 0.001 * 7.359646797180176
Epoch 260, val loss: 0.9419687390327454
Epoch 270, training loss: 0.3690846264362335 = 0.36172938346862793 + 0.001 * 7.355229377746582
Epoch 270, val loss: 0.9508171081542969
Epoch 280, training loss: 0.33620795607566833 = 0.3288559317588806 + 0.001 * 7.352038383483887
Epoch 280, val loss: 0.9622002243995667
Epoch 290, training loss: 0.3058015704154968 = 0.29845374822616577 + 0.001 * 7.347813129425049
Epoch 290, val loss: 0.976018488407135
Epoch 300, training loss: 0.27747416496276855 = 0.27012869715690613 + 0.001 * 7.345478534698486
Epoch 300, val loss: 0.9920597672462463
Epoch 310, training loss: 0.2506689131259918 = 0.2433289736509323 + 0.001 * 7.339951992034912
Epoch 310, val loss: 1.0098521709442139
Epoch 320, training loss: 0.224971741437912 = 0.21763552725315094 + 0.001 * 7.336220741271973
Epoch 320, val loss: 1.0294861793518066
Epoch 330, training loss: 0.2004387229681015 = 0.1931023746728897 + 0.001 * 7.3363471031188965
Epoch 330, val loss: 1.0513232946395874
Epoch 340, training loss: 0.177573099732399 = 0.170242041349411 + 0.001 * 7.331056594848633
Epoch 340, val loss: 1.075583577156067
Epoch 350, training loss: 0.1571272611618042 = 0.14980441331863403 + 0.001 * 7.322849750518799
Epoch 350, val loss: 1.1019318103790283
Epoch 360, training loss: 0.13945643603801727 = 0.13213516771793365 + 0.001 * 7.321265697479248
Epoch 360, val loss: 1.1298428773880005
Epoch 370, training loss: 0.12446530908346176 = 0.11714774370193481 + 0.001 * 7.31756591796875
Epoch 370, val loss: 1.1586227416992188
Epoch 380, training loss: 0.11174363642930984 = 0.1044374406337738 + 0.001 * 7.306196212768555
Epoch 380, val loss: 1.1878691911697388
Epoch 390, training loss: 0.10083770751953125 = 0.09352108836174011 + 0.001 * 7.316621780395508
Epoch 390, val loss: 1.2175732851028442
Epoch 400, training loss: 0.09130397439002991 = 0.08401429653167725 + 0.001 * 7.289676189422607
Epoch 400, val loss: 1.2476578950881958
Epoch 410, training loss: 0.0829191505908966 = 0.07564552873373032 + 0.001 * 7.273623466491699
Epoch 410, val loss: 1.2782986164093018
Epoch 420, training loss: 0.07554300874471664 = 0.0682239979505539 + 0.001 * 7.319013595581055
Epoch 420, val loss: 1.3094731569290161
Epoch 430, training loss: 0.06889497488737106 = 0.06161513924598694 + 0.001 * 7.27983283996582
Epoch 430, val loss: 1.3411610126495361
Epoch 440, training loss: 0.0629701241850853 = 0.05572107806801796 + 0.001 * 7.249049186706543
Epoch 440, val loss: 1.373278021812439
Epoch 450, training loss: 0.057698819786310196 = 0.05046501010656357 + 0.001 * 7.233809471130371
Epoch 450, val loss: 1.4055837392807007
Epoch 460, training loss: 0.05300547555088997 = 0.045779161155223846 + 0.001 * 7.226315021514893
Epoch 460, val loss: 1.4378894567489624
Epoch 470, training loss: 0.04882027581334114 = 0.04160319268703461 + 0.001 * 7.217083930969238
Epoch 470, val loss: 1.4700448513031006
Epoch 480, training loss: 0.045112788677215576 = 0.037885140627622604 + 0.001 * 7.227649211883545
Epoch 480, val loss: 1.5018235445022583
Epoch 490, training loss: 0.04180677607655525 = 0.034575462341308594 + 0.001 * 7.231314659118652
Epoch 490, val loss: 1.53313410282135
Epoch 500, training loss: 0.038842059671878815 = 0.031629450619220734 + 0.001 * 7.212609767913818
Epoch 500, val loss: 1.5638105869293213
Epoch 510, training loss: 0.036224670708179474 = 0.029005372896790504 + 0.001 * 7.219297885894775
Epoch 510, val loss: 1.5937765836715698
Epoch 520, training loss: 0.033869560807943344 = 0.026665879413485527 + 0.001 * 7.2036824226379395
Epoch 520, val loss: 1.6230043172836304
Epoch 530, training loss: 0.03178529813885689 = 0.024577969685196877 + 0.001 * 7.2073283195495605
Epoch 530, val loss: 1.6514437198638916
Epoch 540, training loss: 0.02990565076470375 = 0.022711236029863358 + 0.001 * 7.194415092468262
Epoch 540, val loss: 1.6791882514953613
Epoch 550, training loss: 0.028216581791639328 = 0.02103937789797783 + 0.001 * 7.177204608917236
Epoch 550, val loss: 1.7061049938201904
Epoch 560, training loss: 0.026725023984909058 = 0.019538791850209236 + 0.001 * 7.186232089996338
Epoch 560, val loss: 1.732243537902832
Epoch 570, training loss: 0.025362320244312286 = 0.018189391121268272 + 0.001 * 7.172928810119629
Epoch 570, val loss: 1.7576427459716797
Epoch 580, training loss: 0.024153636768460274 = 0.016972871497273445 + 0.001 * 7.180764675140381
Epoch 580, val loss: 1.7822715044021606
Epoch 590, training loss: 0.023035207763314247 = 0.015873480588197708 + 0.001 * 7.161726951599121
Epoch 590, val loss: 1.8061048984527588
Epoch 600, training loss: 0.02203422412276268 = 0.014877860434353352 + 0.001 * 7.156362533569336
Epoch 600, val loss: 1.8292871713638306
Epoch 610, training loss: 0.021136781200766563 = 0.01397379208356142 + 0.001 * 7.162989139556885
Epoch 610, val loss: 1.8517248630523682
Epoch 620, training loss: 0.020313313230872154 = 0.013150750659406185 + 0.001 * 7.162562370300293
Epoch 620, val loss: 1.8735193014144897
Epoch 630, training loss: 0.01955118030309677 = 0.012399825267493725 + 0.001 * 7.151355266571045
Epoch 630, val loss: 1.8946646451950073
Epoch 640, training loss: 0.018862834200263023 = 0.011712029576301575 + 0.001 * 7.15080451965332
Epoch 640, val loss: 1.915211796760559
Epoch 650, training loss: 0.01822226494550705 = 0.011080266907811165 + 0.001 * 7.141998291015625
Epoch 650, val loss: 1.9352527856826782
Epoch 660, training loss: 0.01767253689467907 = 0.010496547445654869 + 0.001 * 7.175989627838135
Epoch 660, val loss: 1.954910159111023
Epoch 670, training loss: 0.017088357359170914 = 0.00995455589145422 + 0.001 * 7.133800506591797
Epoch 670, val loss: 1.9741864204406738
Epoch 680, training loss: 0.016596369445323944 = 0.009448206052184105 + 0.001 * 7.148162364959717
Epoch 680, val loss: 1.9932185411453247
Epoch 690, training loss: 0.016127953305840492 = 0.008974000811576843 + 0.001 * 7.153952121734619
Epoch 690, val loss: 2.0119500160217285
Epoch 700, training loss: 0.01567228138446808 = 0.008529729209840298 + 0.001 * 7.142551898956299
Epoch 700, val loss: 2.030369281768799
Epoch 710, training loss: 0.015244978480041027 = 0.00811338983476162 + 0.001 * 7.131588459014893
Epoch 710, val loss: 2.048572301864624
Epoch 720, training loss: 0.014847598038613796 = 0.007723476737737656 + 0.001 * 7.124121189117432
Epoch 720, val loss: 2.0664546489715576
Epoch 730, training loss: 0.014474678784608841 = 0.007358776405453682 + 0.001 * 7.115901947021484
Epoch 730, val loss: 2.0841023921966553
Epoch 740, training loss: 0.014124000445008278 = 0.0070180511102080345 + 0.001 * 7.105948448181152
Epoch 740, val loss: 2.101339101791382
Epoch 750, training loss: 0.013800244778394699 = 0.006699909456074238 + 0.001 * 7.100335597991943
Epoch 750, val loss: 2.1182994842529297
Epoch 760, training loss: 0.013499153777956963 = 0.006402904633432627 + 0.001 * 7.096248149871826
Epoch 760, val loss: 2.1349496841430664
Epoch 770, training loss: 0.013222629204392433 = 0.006125051993876696 + 0.001 * 7.09757661819458
Epoch 770, val loss: 2.151280164718628
Epoch 780, training loss: 0.013011958450078964 = 0.005865002516657114 + 0.001 * 7.1469550132751465
Epoch 780, val loss: 2.16731595993042
Epoch 790, training loss: 0.012735879980027676 = 0.005621711257845163 + 0.001 * 7.114168167114258
Epoch 790, val loss: 2.1829302310943604
Epoch 800, training loss: 0.012487532570958138 = 0.005393953528255224 + 0.001 * 7.093578815460205
Epoch 800, val loss: 2.1982524394989014
Epoch 810, training loss: 0.0122611653059721 = 0.005180463660508394 + 0.001 * 7.0807013511657715
Epoch 810, val loss: 2.2132465839385986
Epoch 820, training loss: 0.012084286659955978 = 0.004979979246854782 + 0.001 * 7.104306697845459
Epoch 820, val loss: 2.2279162406921387
Epoch 830, training loss: 0.01188728865236044 = 0.00479168351739645 + 0.001 * 7.09560489654541
Epoch 830, val loss: 2.242314100265503
Epoch 840, training loss: 0.011717204004526138 = 0.004614998586475849 + 0.001 * 7.102205753326416
Epoch 840, val loss: 2.256253242492676
Epoch 850, training loss: 0.011525195091962814 = 0.00444870674982667 + 0.001 * 7.076488494873047
Epoch 850, val loss: 2.2699975967407227
Epoch 860, training loss: 0.011357076466083527 = 0.004291962366551161 + 0.001 * 7.065113544464111
Epoch 860, val loss: 2.2834665775299072
Epoch 870, training loss: 0.011206449009478092 = 0.004144289065152407 + 0.001 * 7.062159538269043
Epoch 870, val loss: 2.2965734004974365
Epoch 880, training loss: 0.011083423160016537 = 0.004005037248134613 + 0.001 * 7.078385353088379
Epoch 880, val loss: 2.3094446659088135
Epoch 890, training loss: 0.01094252709299326 = 0.00387343461625278 + 0.001 * 7.069091796875
Epoch 890, val loss: 2.322021722793579
Epoch 900, training loss: 0.01084128301590681 = 0.003748956834897399 + 0.001 * 7.0923261642456055
Epoch 900, val loss: 2.3343284130096436
Epoch 910, training loss: 0.010730838403105736 = 0.0036312045995146036 + 0.001 * 7.099633693695068
Epoch 910, val loss: 2.3462984561920166
Epoch 920, training loss: 0.010586103424429893 = 0.0035197774413973093 + 0.001 * 7.0663251876831055
Epoch 920, val loss: 2.358156204223633
Epoch 930, training loss: 0.010462017729878426 = 0.0034141577780246735 + 0.001 * 7.047859191894531
Epoch 930, val loss: 2.3696649074554443
Epoch 940, training loss: 0.010357540100812912 = 0.003313947468996048 + 0.001 * 7.0435919761657715
Epoch 940, val loss: 2.380967140197754
Epoch 950, training loss: 0.010271789506077766 = 0.0032188259065151215 + 0.001 * 7.052963733673096
Epoch 950, val loss: 2.392019510269165
Epoch 960, training loss: 0.010163554921746254 = 0.0031283770222216845 + 0.001 * 7.035177707672119
Epoch 960, val loss: 2.4028751850128174
Epoch 970, training loss: 0.010093488730490208 = 0.003042213385924697 + 0.001 * 7.051275253295898
Epoch 970, val loss: 2.4135079383850098
Epoch 980, training loss: 0.01005795132368803 = 0.002960153156891465 + 0.001 * 7.097797393798828
Epoch 980, val loss: 2.423915386199951
Epoch 990, training loss: 0.009949798695743084 = 0.0028820165898650885 + 0.001 * 7.067781448364258
Epoch 990, val loss: 2.434199094772339
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8218239325250396
=== training gcn model ===
Epoch 0, training loss: 1.950565218925476 = 1.941968321800232 + 0.001 * 8.596869468688965
Epoch 0, val loss: 1.9273993968963623
Epoch 10, training loss: 1.9398707151412964 = 1.9312739372253418 + 0.001 * 8.596834182739258
Epoch 10, val loss: 1.9172184467315674
Epoch 20, training loss: 1.9268485307693481 = 1.918251872062683 + 0.001 * 8.596686363220215
Epoch 20, val loss: 1.9044530391693115
Epoch 30, training loss: 1.9087228775024414 = 1.900126576423645 + 0.001 * 8.596280097961426
Epoch 30, val loss: 1.8864892721176147
Epoch 40, training loss: 1.8827115297317505 = 1.8741164207458496 + 0.001 * 8.595117568969727
Epoch 40, val loss: 1.8611547946929932
Epoch 50, training loss: 1.848201870918274 = 1.8396105766296387 + 0.001 * 8.591313362121582
Epoch 50, val loss: 1.829991102218628
Epoch 60, training loss: 1.8129379749298096 = 1.8043609857559204 + 0.001 * 8.57699966430664
Epoch 60, val loss: 1.8037967681884766
Epoch 70, training loss: 1.7836508750915527 = 1.7751286029815674 + 0.001 * 8.522221565246582
Epoch 70, val loss: 1.7856913805007935
Epoch 80, training loss: 1.7450382709503174 = 1.7367832660675049 + 0.001 * 8.25496768951416
Epoch 80, val loss: 1.755330204963684
Epoch 90, training loss: 1.6901636123657227 = 1.6820838451385498 + 0.001 * 8.079766273498535
Epoch 90, val loss: 1.7097878456115723
Epoch 100, training loss: 1.6159727573394775 = 1.607948899269104 + 0.001 * 8.023835182189941
Epoch 100, val loss: 1.649837851524353
Epoch 110, training loss: 1.5289453268051147 = 1.520975112915039 + 0.001 * 7.970222473144531
Epoch 110, val loss: 1.582834005355835
Epoch 120, training loss: 1.4396419525146484 = 1.4317710399627686 + 0.001 * 7.870969772338867
Epoch 120, val loss: 1.5183855295181274
Epoch 130, training loss: 1.352939486503601 = 1.3452727794647217 + 0.001 * 7.666647911071777
Epoch 130, val loss: 1.4602084159851074
Epoch 140, training loss: 1.2684268951416016 = 1.260924220085144 + 0.001 * 7.502660274505615
Epoch 140, val loss: 1.4069299697875977
Epoch 150, training loss: 1.1849544048309326 = 1.1775503158569336 + 0.001 * 7.404139995574951
Epoch 150, val loss: 1.3548542261123657
Epoch 160, training loss: 1.1020139455795288 = 1.0947023630142212 + 0.001 * 7.311548709869385
Epoch 160, val loss: 1.3027169704437256
Epoch 170, training loss: 1.019371509552002 = 1.0120940208435059 + 0.001 * 7.277525901794434
Epoch 170, val loss: 1.249846339225769
Epoch 180, training loss: 0.9368680715560913 = 0.9296010136604309 + 0.001 * 7.267083168029785
Epoch 180, val loss: 1.1972789764404297
Epoch 190, training loss: 0.854767918586731 = 0.8475040197372437 + 0.001 * 7.263883590698242
Epoch 190, val loss: 1.14560866355896
Epoch 200, training loss: 0.7742708325386047 = 0.7670099139213562 + 0.001 * 7.260927200317383
Epoch 200, val loss: 1.09510338306427
Epoch 210, training loss: 0.6974871158599854 = 0.6902285218238831 + 0.001 * 7.258569717407227
Epoch 210, val loss: 1.0476634502410889
Epoch 220, training loss: 0.6265634298324585 = 0.6193066835403442 + 0.001 * 7.256740570068359
Epoch 220, val loss: 1.0050407648086548
Epoch 230, training loss: 0.5625314712524414 = 0.555276095867157 + 0.001 * 7.255358695983887
Epoch 230, val loss: 0.9685870409011841
Epoch 240, training loss: 0.505038857460022 = 0.4977846145629883 + 0.001 * 7.254262924194336
Epoch 240, val loss: 0.9389833807945251
Epoch 250, training loss: 0.4528236985206604 = 0.4455704092979431 + 0.001 * 7.253284454345703
Epoch 250, val loss: 0.9156416058540344
Epoch 260, training loss: 0.4043247103691101 = 0.39707234501838684 + 0.001 * 7.252358436584473
Epoch 260, val loss: 0.8977258205413818
Epoch 270, training loss: 0.3583983778953552 = 0.35114723443984985 + 0.001 * 7.251142501831055
Epoch 270, val loss: 0.8843761086463928
Epoch 280, training loss: 0.3147163391113281 = 0.30746695399284363 + 0.001 * 7.249388217926025
Epoch 280, val loss: 0.8749423623085022
Epoch 290, training loss: 0.2738092839717865 = 0.26656249165534973 + 0.001 * 7.246792316436768
Epoch 290, val loss: 0.8692082166671753
Epoch 300, training loss: 0.23666246235370636 = 0.22941960394382477 + 0.001 * 7.242853164672852
Epoch 300, val loss: 0.8671600818634033
Epoch 310, training loss: 0.20408402383327484 = 0.19684627652168274 + 0.001 * 7.237753391265869
Epoch 310, val loss: 0.8686672449111938
Epoch 320, training loss: 0.17626051604747772 = 0.16903352737426758 + 0.001 * 7.226983547210693
Epoch 320, val loss: 0.8733798265457153
Epoch 330, training loss: 0.15282291173934937 = 0.14561030268669128 + 0.001 * 7.212610721588135
Epoch 330, val loss: 0.8810429573059082
Epoch 340, training loss: 0.13316099345684052 = 0.12596000730991364 + 0.001 * 7.200991630554199
Epoch 340, val loss: 0.8911907076835632
Epoch 350, training loss: 0.11664067953824997 = 0.10946351289749146 + 0.001 * 7.177165985107422
Epoch 350, val loss: 0.9033504128456116
Epoch 360, training loss: 0.10275644809007645 = 0.0955861434340477 + 0.001 * 7.1703009605407715
Epoch 360, val loss: 0.9171158671379089
Epoch 370, training loss: 0.0910119041800499 = 0.08386725932359695 + 0.001 * 7.144641399383545
Epoch 370, val loss: 0.9319916367530823
Epoch 380, training loss: 0.08107353001832962 = 0.07393041253089905 + 0.001 * 7.14311408996582
Epoch 380, val loss: 0.9476584196090698
Epoch 390, training loss: 0.07259388267993927 = 0.0654667392373085 + 0.001 * 7.1271443367004395
Epoch 390, val loss: 0.9638776779174805
Epoch 400, training loss: 0.0653582364320755 = 0.05823035538196564 + 0.001 * 7.127883434295654
Epoch 400, val loss: 0.980483889579773
Epoch 410, training loss: 0.05914734676480293 = 0.052020035684108734 + 0.001 * 7.127310752868652
Epoch 410, val loss: 0.9973560571670532
Epoch 420, training loss: 0.05379384383559227 = 0.04666990786790848 + 0.001 * 7.123935222625732
Epoch 420, val loss: 1.0143792629241943
Epoch 430, training loss: 0.04916360601782799 = 0.042043089866638184 + 0.001 * 7.120514392852783
Epoch 430, val loss: 1.031534194946289
Epoch 440, training loss: 0.045146092772483826 = 0.03802606090903282 + 0.001 * 7.120031833648682
Epoch 440, val loss: 1.0487397909164429
Epoch 450, training loss: 0.041647136211395264 = 0.03452377766370773 + 0.001 * 7.123359680175781
Epoch 450, val loss: 1.065956711769104
Epoch 460, training loss: 0.03857617825269699 = 0.031457506120204926 + 0.001 * 7.118671894073486
Epoch 460, val loss: 1.0831021070480347
Epoch 470, training loss: 0.035877712070941925 = 0.02876168303191662 + 0.001 * 7.116026878356934
Epoch 470, val loss: 1.1000590324401855
Epoch 480, training loss: 0.03350147232413292 = 0.026382245123386383 + 0.001 * 7.119228363037109
Epoch 480, val loss: 1.1168317794799805
Epoch 490, training loss: 0.03138658404350281 = 0.024274419993162155 + 0.001 * 7.112163066864014
Epoch 490, val loss: 1.1333882808685303
Epoch 500, training loss: 0.02951282635331154 = 0.022400619462132454 + 0.001 * 7.112205505371094
Epoch 500, val loss: 1.1497200727462769
Epoch 510, training loss: 0.02785681001842022 = 0.02072933502495289 + 0.001 * 7.127474308013916
Epoch 510, val loss: 1.165719747543335
Epoch 520, training loss: 0.026339346542954445 = 0.019233863800764084 + 0.001 * 7.105483055114746
Epoch 520, val loss: 1.1814454793930054
Epoch 530, training loss: 0.024997210130095482 = 0.017891569063067436 + 0.001 * 7.105640411376953
Epoch 530, val loss: 1.1968400478363037
Epoch 540, training loss: 0.023790448904037476 = 0.016683362424373627 + 0.001 * 7.107085704803467
Epoch 540, val loss: 1.2119017839431763
Epoch 550, training loss: 0.022696463391184807 = 0.015592814423143864 + 0.001 * 7.103649139404297
Epoch 550, val loss: 1.2266502380371094
Epoch 560, training loss: 0.021709151566028595 = 0.014605749398469925 + 0.001 * 7.103402137756348
Epoch 560, val loss: 1.241065263748169
Epoch 570, training loss: 0.020799359306693077 = 0.013710064813494682 + 0.001 * 7.089293956756592
Epoch 570, val loss: 1.2551437616348267
Epoch 580, training loss: 0.01998726651072502 = 0.012895186431705952 + 0.001 * 7.0920796394348145
Epoch 580, val loss: 1.2688804864883423
Epoch 590, training loss: 0.019243229180574417 = 0.012152074836194515 + 0.001 * 7.091153144836426
Epoch 590, val loss: 1.2822973728179932
Epoch 600, training loss: 0.018557973206043243 = 0.011472745798528194 + 0.001 * 7.0852274894714355
Epoch 600, val loss: 1.2953760623931885
Epoch 610, training loss: 0.017929047346115112 = 0.010850371792912483 + 0.001 * 7.078675746917725
Epoch 610, val loss: 1.308144211769104
Epoch 620, training loss: 0.017351284623146057 = 0.010278849862515926 + 0.001 * 7.072434902191162
Epoch 620, val loss: 1.3206039667129517
Epoch 630, training loss: 0.016821935772895813 = 0.009752895683050156 + 0.001 * 7.069040298461914
Epoch 630, val loss: 1.3327674865722656
Epoch 640, training loss: 0.01634978875517845 = 0.009268006309866905 + 0.001 * 7.081782817840576
Epoch 640, val loss: 1.3446271419525146
Epoch 650, training loss: 0.015888210386037827 = 0.008820075541734695 + 0.001 * 7.068134307861328
Epoch 650, val loss: 1.3562157154083252
Epoch 660, training loss: 0.015466462820768356 = 0.008405444212257862 + 0.001 * 7.061018466949463
Epoch 660, val loss: 1.3675453662872314
Epoch 670, training loss: 0.01507827639579773 = 0.00802097748965025 + 0.001 * 7.057298183441162
Epoch 670, val loss: 1.3785932064056396
Epoch 680, training loss: 0.01472796592861414 = 0.007663834374397993 + 0.001 * 7.064131259918213
Epoch 680, val loss: 1.3893855810165405
Epoch 690, training loss: 0.014383206143975258 = 0.007331469561904669 + 0.001 * 7.051736831665039
Epoch 690, val loss: 1.3999212980270386
Epoch 700, training loss: 0.014073003083467484 = 0.0070216841995716095 + 0.001 * 7.051319122314453
Epoch 700, val loss: 1.4102281332015991
Epoch 710, training loss: 0.013788966462016106 = 0.006732483860105276 + 0.001 * 7.056482315063477
Epoch 710, val loss: 1.4202772378921509
Epoch 720, training loss: 0.013508870266377926 = 0.00646213348954916 + 0.001 * 7.046736240386963
Epoch 720, val loss: 1.430131196975708
Epoch 730, training loss: 0.013252386823296547 = 0.006208987906575203 + 0.001 * 7.043398380279541
Epoch 730, val loss: 1.439733862876892
Epoch 740, training loss: 0.01301368698477745 = 0.005971616134047508 + 0.001 * 7.042069911956787
Epoch 740, val loss: 1.4491418600082397
Epoch 750, training loss: 0.012804578058421612 = 0.005748560186475515 + 0.001 * 7.0560173988342285
Epoch 750, val loss: 1.458342432975769
Epoch 760, training loss: 0.012585052289068699 = 0.00553817069157958 + 0.001 * 7.046881198883057
Epoch 760, val loss: 1.46738600730896
Epoch 770, training loss: 0.012376011349260807 = 0.005338312592357397 + 0.001 * 7.037698268890381
Epoch 770, val loss: 1.476289987564087
Epoch 780, training loss: 0.012201234698295593 = 0.00514704454690218 + 0.001 * 7.054189682006836
Epoch 780, val loss: 1.4851347208023071
Epoch 790, training loss: 0.012004312127828598 = 0.004963442217558622 + 0.001 * 7.04086971282959
Epoch 790, val loss: 1.4939731359481812
Epoch 800, training loss: 0.011831311509013176 = 0.004787128418684006 + 0.001 * 7.044183254241943
Epoch 800, val loss: 1.5027809143066406
Epoch 810, training loss: 0.011651044711470604 = 0.004618088714778423 + 0.001 * 7.032956123352051
Epoch 810, val loss: 1.5115692615509033
Epoch 820, training loss: 0.011489810422062874 = 0.0044564250856637955 + 0.001 * 7.033384799957275
Epoch 820, val loss: 1.5203255414962769
Epoch 830, training loss: 0.011353325098752975 = 0.004302154760807753 + 0.001 * 7.051170349121094
Epoch 830, val loss: 1.5290415287017822
Epoch 840, training loss: 0.011188928969204426 = 0.004155161324888468 + 0.001 * 7.033767223358154
Epoch 840, val loss: 1.5376548767089844
Epoch 850, training loss: 0.011044027283787727 = 0.004015287384390831 + 0.001 * 7.028739929199219
Epoch 850, val loss: 1.5462157726287842
Epoch 860, training loss: 0.010922730900347233 = 0.003882247256115079 + 0.001 * 7.040482997894287
Epoch 860, val loss: 1.5546482801437378
Epoch 870, training loss: 0.010785797610878944 = 0.00375580252148211 + 0.001 * 7.029994964599609
Epoch 870, val loss: 1.562994122505188
Epoch 880, training loss: 0.010657969862222672 = 0.0036356274504214525 + 0.001 * 7.022341728210449
Epoch 880, val loss: 1.5712053775787354
Epoch 890, training loss: 0.010558122768998146 = 0.0035213888622820377 + 0.001 * 7.036734104156494
Epoch 890, val loss: 1.5792967081069946
Epoch 900, training loss: 0.010444091632962227 = 0.0034128124825656414 + 0.001 * 7.031278610229492
Epoch 900, val loss: 1.5872812271118164
Epoch 910, training loss: 0.010333038866519928 = 0.0033095614053308964 + 0.001 * 7.023477554321289
Epoch 910, val loss: 1.5951181650161743
Epoch 920, training loss: 0.010237565264105797 = 0.0032113646157085896 + 0.001 * 7.026200294494629
Epoch 920, val loss: 1.6028211116790771
Epoch 930, training loss: 0.01014437060803175 = 0.0031179196666926146 + 0.001 * 7.0264506340026855
Epoch 930, val loss: 1.6104258298873901
Epoch 940, training loss: 0.010052049532532692 = 0.003028975334018469 + 0.001 * 7.023073673248291
Epoch 940, val loss: 1.6179109811782837
Epoch 950, training loss: 0.009957638569176197 = 0.00294423452578485 + 0.001 * 7.013403415679932
Epoch 950, val loss: 1.625238299369812
Epoch 960, training loss: 0.009890810586512089 = 0.002863455330953002 + 0.001 * 7.027355194091797
Epoch 960, val loss: 1.6324814558029175
Epoch 970, training loss: 0.009801317006349564 = 0.0027864358853548765 + 0.001 * 7.014880657196045
Epoch 970, val loss: 1.639577031135559
Epoch 980, training loss: 0.009744323790073395 = 0.0027129179798066616 + 0.001 * 7.031405448913574
Epoch 980, val loss: 1.646541953086853
Epoch 990, training loss: 0.009659582749009132 = 0.00264274631626904 + 0.001 * 7.016835689544678
Epoch 990, val loss: 1.6534323692321777
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 1.9543248414993286 = 1.9457279443740845 + 0.001 * 8.596877098083496
Epoch 0, val loss: 1.9457687139511108
Epoch 10, training loss: 1.943721055984497 = 1.9351242780685425 + 0.001 * 8.596813201904297
Epoch 10, val loss: 1.9353110790252686
Epoch 20, training loss: 1.930152177810669 = 1.921555519104004 + 0.001 * 8.596625328063965
Epoch 20, val loss: 1.9215337038040161
Epoch 30, training loss: 1.9109828472137451 = 1.9023866653442383 + 0.001 * 8.596150398254395
Epoch 30, val loss: 1.901774525642395
Epoch 40, training loss: 1.8833589553833008 = 1.874764084815979 + 0.001 * 8.5948486328125
Epoch 40, val loss: 1.8735108375549316
Epoch 50, training loss: 1.8470481634140015 = 1.838457703590393 + 0.001 * 8.590476989746094
Epoch 50, val loss: 1.838376522064209
Epoch 60, training loss: 1.8110599517822266 = 1.8024877309799194 + 0.001 * 8.572226524353027
Epoch 60, val loss: 1.8083152770996094
Epoch 70, training loss: 1.7815666198730469 = 1.7730792760849 + 0.001 * 8.48740005493164
Epoch 70, val loss: 1.7861121892929077
Epoch 80, training loss: 1.7421393394470215 = 1.7338619232177734 + 0.001 * 8.277373313903809
Epoch 80, val loss: 1.752745270729065
Epoch 90, training loss: 1.686723232269287 = 1.6785540580749512 + 0.001 * 8.169215202331543
Epoch 90, val loss: 1.7051533460617065
Epoch 100, training loss: 1.6132328510284424 = 1.6051480770111084 + 0.001 * 8.084808349609375
Epoch 100, val loss: 1.6445608139038086
Epoch 110, training loss: 1.5307613611221313 = 1.5227460861206055 + 0.001 * 8.015259742736816
Epoch 110, val loss: 1.57948899269104
Epoch 120, training loss: 1.4494394063949585 = 1.4415614604949951 + 0.001 * 7.8779191970825195
Epoch 120, val loss: 1.5185655355453491
Epoch 130, training loss: 1.371185541152954 = 1.3634792566299438 + 0.001 * 7.706254959106445
Epoch 130, val loss: 1.4620170593261719
Epoch 140, training loss: 1.2920749187469482 = 1.2844014167785645 + 0.001 * 7.673473834991455
Epoch 140, val loss: 1.4059821367263794
Epoch 150, training loss: 1.2098942995071411 = 1.2022937536239624 + 0.001 * 7.600546836853027
Epoch 150, val loss: 1.3475754261016846
Epoch 160, training loss: 1.1268223524093628 = 1.1193138360977173 + 0.001 * 7.5084710121154785
Epoch 160, val loss: 1.2890422344207764
Epoch 170, training loss: 1.0468443632125854 = 1.0394068956375122 + 0.001 * 7.437445163726807
Epoch 170, val loss: 1.2344841957092285
Epoch 180, training loss: 0.972078800201416 = 0.9646851420402527 + 0.001 * 7.393653869628906
Epoch 180, val loss: 1.1853652000427246
Epoch 190, training loss: 0.9016973972320557 = 0.8943313360214233 + 0.001 * 7.366077899932861
Epoch 190, val loss: 1.1403940916061401
Epoch 200, training loss: 0.8336542248725891 = 0.8263018131256104 + 0.001 * 7.352410316467285
Epoch 200, val loss: 1.0978385210037231
Epoch 210, training loss: 0.7669347524642944 = 0.7595944404602051 + 0.001 * 7.3402862548828125
Epoch 210, val loss: 1.0571966171264648
Epoch 220, training loss: 0.7021791934967041 = 0.694849967956543 + 0.001 * 7.329251289367676
Epoch 220, val loss: 1.019419550895691
Epoch 230, training loss: 0.64012610912323 = 0.6328040361404419 + 0.001 * 7.32204532623291
Epoch 230, val loss: 0.9858273267745972
Epoch 240, training loss: 0.5808751583099365 = 0.5735664367675781 + 0.001 * 7.308723449707031
Epoch 240, val loss: 0.956739068031311
Epoch 250, training loss: 0.5246979594230652 = 0.5174002051353455 + 0.001 * 7.29775333404541
Epoch 250, val loss: 0.9321979284286499
Epoch 260, training loss: 0.47191351652145386 = 0.4646313786506653 + 0.001 * 7.282132625579834
Epoch 260, val loss: 0.9125575423240662
Epoch 270, training loss: 0.42275485396385193 = 0.4154844582080841 + 0.001 * 7.27040433883667
Epoch 270, val loss: 0.8980472683906555
Epoch 280, training loss: 0.37725046277046204 = 0.3699963092803955 + 0.001 * 7.254159450531006
Epoch 280, val loss: 0.8888479471206665
Epoch 290, training loss: 0.33518341183662415 = 0.32793793082237244 + 0.001 * 7.245490550994873
Epoch 290, val loss: 0.884535014629364
Epoch 300, training loss: 0.29612797498703003 = 0.2889024615287781 + 0.001 * 7.225514888763428
Epoch 300, val loss: 0.8843606114387512
Epoch 310, training loss: 0.2597414255142212 = 0.2525222599506378 + 0.001 * 7.219165325164795
Epoch 310, val loss: 0.8875536322593689
Epoch 320, training loss: 0.22594451904296875 = 0.2187301069498062 + 0.001 * 7.214406490325928
Epoch 320, val loss: 0.8935957551002502
Epoch 330, training loss: 0.1950961947441101 = 0.1878845989704132 + 0.001 * 7.211594581604004
Epoch 330, val loss: 0.9023962616920471
Epoch 340, training loss: 0.16773727536201477 = 0.1605267971754074 + 0.001 * 7.2104716300964355
Epoch 340, val loss: 0.9139878153800964
Epoch 350, training loss: 0.1442527174949646 = 0.13702581822872162 + 0.001 * 7.226901531219482
Epoch 350, val loss: 0.9282772541046143
Epoch 360, training loss: 0.12451211363077164 = 0.1172969862818718 + 0.001 * 7.215124130249023
Epoch 360, val loss: 0.9447877407073975
Epoch 370, training loss: 0.10817231982946396 = 0.10096249729394913 + 0.001 * 7.209820747375488
Epoch 370, val loss: 0.9629949331283569
Epoch 380, training loss: 0.0947103500366211 = 0.08750391006469727 + 0.001 * 7.206438064575195
Epoch 380, val loss: 0.9826183319091797
Epoch 390, training loss: 0.08359560370445251 = 0.07638384401798248 + 0.001 * 7.211755752563477
Epoch 390, val loss: 1.00315260887146
Epoch 400, training loss: 0.0743350014090538 = 0.06713014096021652 + 0.001 * 7.204858779907227
Epoch 400, val loss: 1.0242172479629517
Epoch 410, training loss: 0.06656712293624878 = 0.05936453118920326 + 0.001 * 7.202595233917236
Epoch 410, val loss: 1.0456531047821045
Epoch 420, training loss: 0.060007911175489426 = 0.05279756337404251 + 0.001 * 7.210348606109619
Epoch 420, val loss: 1.0671783685684204
Epoch 430, training loss: 0.05441033095121384 = 0.047204647213220596 + 0.001 * 7.205685138702393
Epoch 430, val loss: 1.0885930061340332
Epoch 440, training loss: 0.04960816353559494 = 0.042409446090459824 + 0.001 * 7.1987152099609375
Epoch 440, val loss: 1.1098854541778564
Epoch 450, training loss: 0.04547412693500519 = 0.038272298872470856 + 0.001 * 7.2018280029296875
Epoch 450, val loss: 1.1308711767196655
Epoch 460, training loss: 0.04188039153814316 = 0.0346832275390625 + 0.001 * 7.197163105010986
Epoch 460, val loss: 1.1515228748321533
Epoch 470, training loss: 0.03875115513801575 = 0.03155311942100525 + 0.001 * 7.198037147521973
Epoch 470, val loss: 1.1717782020568848
Epoch 480, training loss: 0.03600619360804558 = 0.028810087591409683 + 0.001 * 7.196104049682617
Epoch 480, val loss: 1.1916650533676147
Epoch 490, training loss: 0.03359758108854294 = 0.026395896449685097 + 0.001 * 7.201685905456543
Epoch 490, val loss: 1.2111051082611084
Epoch 500, training loss: 0.03145640343427658 = 0.024262504652142525 + 0.001 * 7.193900108337402
Epoch 500, val loss: 1.230067253112793
Epoch 510, training loss: 0.029558520764112473 = 0.02236979641020298 + 0.001 * 7.188724517822266
Epoch 510, val loss: 1.2486011981964111
Epoch 520, training loss: 0.027901334688067436 = 0.02068457379937172 + 0.001 * 7.216760158538818
Epoch 520, val loss: 1.2666538953781128
Epoch 530, training loss: 0.026370324194431305 = 0.019178982824087143 + 0.001 * 7.191341876983643
Epoch 530, val loss: 1.284245491027832
Epoch 540, training loss: 0.0250112134963274 = 0.017829379066824913 + 0.001 * 7.181833744049072
Epoch 540, val loss: 1.301376461982727
Epoch 550, training loss: 0.023787934333086014 = 0.016616012901067734 + 0.001 * 7.1719207763671875
Epoch 550, val loss: 1.3180569410324097
Epoch 560, training loss: 0.022692497819662094 = 0.01552184671163559 + 0.001 * 7.170650005340576
Epoch 560, val loss: 1.3342832326889038
Epoch 570, training loss: 0.021711569279432297 = 0.014532336965203285 + 0.001 * 7.179233074188232
Epoch 570, val loss: 1.3500628471374512
Epoch 580, training loss: 0.02082883194088936 = 0.01363504771143198 + 0.001 * 7.193784713745117
Epoch 580, val loss: 1.3654184341430664
Epoch 590, training loss: 0.01999540999531746 = 0.012819287367165089 + 0.001 * 7.176122188568115
Epoch 590, val loss: 1.3803678750991821
Epoch 600, training loss: 0.019230660051107407 = 0.012075765989720821 + 0.001 * 7.154893398284912
Epoch 600, val loss: 1.394910454750061
Epoch 610, training loss: 0.018610192462801933 = 0.011396409943699837 + 0.001 * 7.213782787322998
Epoch 610, val loss: 1.4090518951416016
Epoch 620, training loss: 0.017936304211616516 = 0.010774324648082256 + 0.001 * 7.161980152130127
Epoch 620, val loss: 1.4227869510650635
Epoch 630, training loss: 0.017363552004098892 = 0.010203403420746326 + 0.001 * 7.160149097442627
Epoch 630, val loss: 1.4361460208892822
Epoch 640, training loss: 0.01682867668569088 = 0.009678240865468979 + 0.001 * 7.150435924530029
Epoch 640, val loss: 1.4491772651672363
Epoch 650, training loss: 0.016342082992196083 = 0.009194196201860905 + 0.001 * 7.147886276245117
Epoch 650, val loss: 1.4618258476257324
Epoch 660, training loss: 0.015889033675193787 = 0.008747207000851631 + 0.001 * 7.1418256759643555
Epoch 660, val loss: 1.474160075187683
Epoch 670, training loss: 0.01546933501958847 = 0.0083336615934968 + 0.001 * 7.135673522949219
Epoch 670, val loss: 1.4861764907836914
Epoch 680, training loss: 0.015130404382944107 = 0.007950360886752605 + 0.001 * 7.180042743682861
Epoch 680, val loss: 1.4978680610656738
Epoch 690, training loss: 0.014717312529683113 = 0.007594485767185688 + 0.001 * 7.122826099395752
Epoch 690, val loss: 1.50923490524292
Epoch 700, training loss: 0.014386169612407684 = 0.007263561710715294 + 0.001 * 7.122608184814453
Epoch 700, val loss: 1.5203138589859009
Epoch 710, training loss: 0.014139187522232533 = 0.00695531303063035 + 0.001 * 7.183874130249023
Epoch 710, val loss: 1.5311216115951538
Epoch 720, training loss: 0.013792368583381176 = 0.006667692214250565 + 0.001 * 7.12467622756958
Epoch 720, val loss: 1.5416224002838135
Epoch 730, training loss: 0.01352815330028534 = 0.006398967932909727 + 0.001 * 7.129184722900391
Epoch 730, val loss: 1.5518488883972168
Epoch 740, training loss: 0.013259900733828545 = 0.006147495470941067 + 0.001 * 7.112405300140381
Epoch 740, val loss: 1.5618568658828735
Epoch 750, training loss: 0.013041235506534576 = 0.005911837797611952 + 0.001 * 7.129397869110107
Epoch 750, val loss: 1.5715858936309814
Epoch 760, training loss: 0.01280114334076643 = 0.005690766964107752 + 0.001 * 7.110375881195068
Epoch 760, val loss: 1.581108570098877
Epoch 770, training loss: 0.012583097442984581 = 0.005483014043420553 + 0.001 * 7.100082874298096
Epoch 770, val loss: 1.590369462966919
Epoch 780, training loss: 0.012414162047207355 = 0.0052876221016049385 + 0.001 * 7.126539707183838
Epoch 780, val loss: 1.5994236469268799
Epoch 790, training loss: 0.012242987751960754 = 0.005103612784296274 + 0.001 * 7.139374256134033
Epoch 790, val loss: 1.6082534790039062
Epoch 800, training loss: 0.01203182339668274 = 0.0049301632679998875 + 0.001 * 7.101660251617432
Epoch 800, val loss: 1.6168721914291382
Epoch 810, training loss: 0.011846383102238178 = 0.004766405560076237 + 0.001 * 7.079977035522461
Epoch 810, val loss: 1.6252973079681396
Epoch 820, training loss: 0.011708442121744156 = 0.0046116807498037815 + 0.001 * 7.096761703491211
Epoch 820, val loss: 1.633493423461914
Epoch 830, training loss: 0.011616303585469723 = 0.004465354606509209 + 0.001 * 7.150948524475098
Epoch 830, val loss: 1.6415289640426636
Epoch 840, training loss: 0.01140577532351017 = 0.004326795227825642 + 0.001 * 7.078980445861816
Epoch 840, val loss: 1.6493691205978394
Epoch 850, training loss: 0.011266879737377167 = 0.004195486195385456 + 0.001 * 7.0713934898376465
Epoch 850, val loss: 1.6570179462432861
Epoch 860, training loss: 0.011168851517140865 = 0.004070906434208155 + 0.001 * 7.097944736480713
Epoch 860, val loss: 1.6644929647445679
Epoch 870, training loss: 0.011057963594794273 = 0.003952658269554377 + 0.001 * 7.1053056716918945
Epoch 870, val loss: 1.6718116998672485
Epoch 880, training loss: 0.010922945104539394 = 0.0038402529899030924 + 0.001 * 7.082691669464111
Epoch 880, val loss: 1.6789345741271973
Epoch 890, training loss: 0.010795492678880692 = 0.003733353689312935 + 0.001 * 7.06213903427124
Epoch 890, val loss: 1.685943603515625
Epoch 900, training loss: 0.010696060955524445 = 0.003631601110100746 + 0.001 * 7.064459323883057
Epoch 900, val loss: 1.692794919013977
Epoch 910, training loss: 0.010585825890302658 = 0.003534662537276745 + 0.001 * 7.051163196563721
Epoch 910, val loss: 1.6994863748550415
Epoch 920, training loss: 0.010492642410099506 = 0.0034422583412379026 + 0.001 * 7.050383567810059
Epoch 920, val loss: 1.7060072422027588
Epoch 930, training loss: 0.01039139460772276 = 0.0033541014418005943 + 0.001 * 7.037292957305908
Epoch 930, val loss: 1.7123997211456299
Epoch 940, training loss: 0.010323196649551392 = 0.003269911278039217 + 0.001 * 7.053285121917725
Epoch 940, val loss: 1.7186440229415894
Epoch 950, training loss: 0.010244944132864475 = 0.0031894936691969633 + 0.001 * 7.055450439453125
Epoch 950, val loss: 1.72474205493927
Epoch 960, training loss: 0.010169814340770245 = 0.003112630220130086 + 0.001 * 7.057184219360352
Epoch 960, val loss: 1.7307416200637817
Epoch 970, training loss: 0.010094000026583672 = 0.0030390354804694653 + 0.001 * 7.054964542388916
Epoch 970, val loss: 1.736556053161621
Epoch 980, training loss: 0.010018790140748024 = 0.0029685955960303545 + 0.001 * 7.050193786621094
Epoch 980, val loss: 1.742313027381897
Epoch 990, training loss: 0.009956737980246544 = 0.0029011224396526814 + 0.001 * 7.055614948272705
Epoch 990, val loss: 1.7479041814804077
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.816025303110174
The final CL Acc:0.74938, 0.01522, The final GNN Acc:0.81673, 0.00391
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 7874])
updated graph: torch.Size([2, 10470])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.946446180343628 = 1.9378492832183838 + 0.001 * 8.596855163574219
Epoch 0, val loss: 1.9429619312286377
Epoch 10, training loss: 1.9365509748458862 = 1.9279541969299316 + 0.001 * 8.596797943115234
Epoch 10, val loss: 1.9332046508789062
Epoch 20, training loss: 1.924149513244629 = 1.9155528545379639 + 0.001 * 8.596600532531738
Epoch 20, val loss: 1.920486330986023
Epoch 30, training loss: 1.9065399169921875 = 1.8979438543319702 + 0.001 * 8.596109390258789
Epoch 30, val loss: 1.902017593383789
Epoch 40, training loss: 1.8806225061416626 = 1.8720277547836304 + 0.001 * 8.594748497009277
Epoch 40, val loss: 1.8750996589660645
Epoch 50, training loss: 1.8449302911758423 = 1.8363401889801025 + 0.001 * 8.590045928955078
Epoch 50, val loss: 1.8397319316864014
Epoch 60, training loss: 1.8057535886764526 = 1.7971833944320679 + 0.001 * 8.570195198059082
Epoch 60, val loss: 1.8047206401824951
Epoch 70, training loss: 1.769244909286499 = 1.7607836723327637 + 0.001 * 8.461183547973633
Epoch 70, val loss: 1.7730153799057007
Epoch 80, training loss: 1.7200027704238892 = 1.7119312286376953 + 0.001 * 8.071556091308594
Epoch 80, val loss: 1.727638602256775
Epoch 90, training loss: 1.6519646644592285 = 1.643978238105774 + 0.001 * 7.986405849456787
Epoch 90, val loss: 1.6667604446411133
Epoch 100, training loss: 1.5656992197036743 = 1.557785153388977 + 0.001 * 7.914024829864502
Epoch 100, val loss: 1.5925992727279663
Epoch 110, training loss: 1.4737155437469482 = 1.4659059047698975 + 0.001 * 7.809601306915283
Epoch 110, val loss: 1.5150059461593628
Epoch 120, training loss: 1.383426308631897 = 1.3758412599563599 + 0.001 * 7.585035800933838
Epoch 120, val loss: 1.4424458742141724
Epoch 130, training loss: 1.2945387363433838 = 1.2871874570846558 + 0.001 * 7.3513360023498535
Epoch 130, val loss: 1.374066710472107
Epoch 140, training loss: 1.206920862197876 = 1.1995959281921387 + 0.001 * 7.32496452331543
Epoch 140, val loss: 1.3088783025741577
Epoch 150, training loss: 1.1205084323883057 = 1.1132104396820068 + 0.001 * 7.298051834106445
Epoch 150, val loss: 1.2462023496627808
Epoch 160, training loss: 1.0351299047470093 = 1.0278520584106445 + 0.001 * 7.2778096199035645
Epoch 160, val loss: 1.1839557886123657
Epoch 170, training loss: 0.9510985612869263 = 0.9438290596008301 + 0.001 * 7.269482135772705
Epoch 170, val loss: 1.1210315227508545
Epoch 180, training loss: 0.8696025013923645 = 0.8623372316360474 + 0.001 * 7.265295028686523
Epoch 180, val loss: 1.057674527168274
Epoch 190, training loss: 0.7922794818878174 = 0.7850207686424255 + 0.001 * 7.258713722229004
Epoch 190, val loss: 0.9959444403648376
Epoch 200, training loss: 0.720279335975647 = 0.7130290269851685 + 0.001 * 7.250321388244629
Epoch 200, val loss: 0.9378088116645813
Epoch 210, training loss: 0.653486430644989 = 0.6462462544441223 + 0.001 * 7.240179538726807
Epoch 210, val loss: 0.8847318887710571
Epoch 220, training loss: 0.590945303440094 = 0.5837171673774719 + 0.001 * 7.228108882904053
Epoch 220, val loss: 0.8372990489006042
Epoch 230, training loss: 0.5320140719413757 = 0.5248008370399475 + 0.001 * 7.213217735290527
Epoch 230, val loss: 0.7960113883018494
Epoch 240, training loss: 0.47696807980537415 = 0.4697732925415039 + 0.001 * 7.194790363311768
Epoch 240, val loss: 0.7618869543075562
Epoch 250, training loss: 0.42627063393592834 = 0.41909170150756836 + 0.001 * 7.178936004638672
Epoch 250, val loss: 0.7350865006446838
Epoch 260, training loss: 0.3799765706062317 = 0.37281984090805054 + 0.001 * 7.1567301750183105
Epoch 260, val loss: 0.7146774530410767
Epoch 270, training loss: 0.3377370536327362 = 0.3305996358394623 + 0.001 * 7.137428283691406
Epoch 270, val loss: 0.6991101503372192
Epoch 280, training loss: 0.2991059124469757 = 0.29196956753730774 + 0.001 * 7.136334419250488
Epoch 280, val loss: 0.68709796667099
Epoch 290, training loss: 0.2637718617916107 = 0.2566559612751007 + 0.001 * 7.1158928871154785
Epoch 290, val loss: 0.6777142882347107
Epoch 300, training loss: 0.23174472153186798 = 0.2246343046426773 + 0.001 * 7.1104207038879395
Epoch 300, val loss: 0.6706249713897705
Epoch 310, training loss: 0.20308278501033783 = 0.19597165286540985 + 0.001 * 7.111133098602295
Epoch 310, val loss: 0.6656513214111328
Epoch 320, training loss: 0.1777712106704712 = 0.1706584393978119 + 0.001 * 7.112773895263672
Epoch 320, val loss: 0.6627019643783569
Epoch 330, training loss: 0.15562717616558075 = 0.14851275086402893 + 0.001 * 7.11442232131958
Epoch 330, val loss: 0.6618118286132812
Epoch 340, training loss: 0.1363835334777832 = 0.12926743924617767 + 0.001 * 7.116090297698975
Epoch 340, val loss: 0.6626827120780945
Epoch 350, training loss: 0.11974207311868668 = 0.11262443661689758 + 0.001 * 7.117639541625977
Epoch 350, val loss: 0.6651318073272705
Epoch 360, training loss: 0.10539839416742325 = 0.09827937930822372 + 0.001 * 7.119011878967285
Epoch 360, val loss: 0.6689262390136719
Epoch 370, training loss: 0.09307537972927094 = 0.08595515042543411 + 0.001 * 7.120226860046387
Epoch 370, val loss: 0.6738644242286682
Epoch 380, training loss: 0.08249636739492416 = 0.0753725990653038 + 0.001 * 7.123764991760254
Epoch 380, val loss: 0.6797614693641663
Epoch 390, training loss: 0.0733998492360115 = 0.06627529859542847 + 0.001 * 7.1245503425598145
Epoch 390, val loss: 0.6865038871765137
Epoch 400, training loss: 0.06557279825210571 = 0.058449313044548035 + 0.001 * 7.123486518859863
Epoch 400, val loss: 0.6939936876296997
Epoch 410, training loss: 0.05883840471506119 = 0.051714494824409485 + 0.001 * 7.123909950256348
Epoch 410, val loss: 0.7020814418792725
Epoch 420, training loss: 0.053044673055410385 = 0.04591793939471245 + 0.001 * 7.126732349395752
Epoch 420, val loss: 0.7106513381004333
Epoch 430, training loss: 0.048048511147499084 = 0.040922876447439194 + 0.001 * 7.125633239746094
Epoch 430, val loss: 0.7196274995803833
Epoch 440, training loss: 0.043734218925237656 = 0.0366094633936882 + 0.001 * 7.124754428863525
Epoch 440, val loss: 0.7289347052574158
Epoch 450, training loss: 0.04000343382358551 = 0.03287879005074501 + 0.001 * 7.124643325805664
Epoch 450, val loss: 0.7384473085403442
Epoch 460, training loss: 0.03676703944802284 = 0.02964283525943756 + 0.001 * 7.124202251434326
Epoch 460, val loss: 0.74807208776474
Epoch 470, training loss: 0.03396555036306381 = 0.02682282030582428 + 0.001 * 7.142728805541992
Epoch 470, val loss: 0.7577535510063171
Epoch 480, training loss: 0.03148243576288223 = 0.02435488812625408 + 0.001 * 7.127548694610596
Epoch 480, val loss: 0.7674444913864136
Epoch 490, training loss: 0.029309773817658424 = 0.0221848264336586 + 0.001 * 7.124946594238281
Epoch 490, val loss: 0.7771178483963013
Epoch 500, training loss: 0.027394194155931473 = 0.020270952954888344 + 0.001 * 7.123239994049072
Epoch 500, val loss: 0.7867106795310974
Epoch 510, training loss: 0.02570516988635063 = 0.018576672300696373 + 0.001 * 7.128498077392578
Epoch 510, val loss: 0.7961806058883667
Epoch 520, training loss: 0.024192646145820618 = 0.017070844769477844 + 0.001 * 7.121801853179932
Epoch 520, val loss: 0.805542528629303
Epoch 530, training loss: 0.02284863591194153 = 0.015726570039987564 + 0.001 * 7.12206506729126
Epoch 530, val loss: 0.8147522807121277
Epoch 540, training loss: 0.021644121035933495 = 0.01452257763594389 + 0.001 * 7.121542453765869
Epoch 540, val loss: 0.8238534927368164
Epoch 550, training loss: 0.02056272141635418 = 0.013439791277050972 + 0.001 * 7.12293004989624
Epoch 550, val loss: 0.8328402638435364
Epoch 560, training loss: 0.019579727202653885 = 0.0124591626226902 + 0.001 * 7.120563983917236
Epoch 560, val loss: 0.8418095707893372
Epoch 570, training loss: 0.01868411712348461 = 0.011566291563212872 + 0.001 * 7.117825031280518
Epoch 570, val loss: 0.8507848978042603
Epoch 580, training loss: 0.01788175106048584 = 0.010751590132713318 + 0.001 * 7.130159854888916
Epoch 580, val loss: 0.8597449660301208
Epoch 590, training loss: 0.017126455903053284 = 0.010008600540459156 + 0.001 * 7.117855072021484
Epoch 590, val loss: 0.8686755299568176
Epoch 600, training loss: 0.016446663066744804 = 0.009331530891358852 + 0.001 * 7.115131855010986
Epoch 600, val loss: 0.8775354027748108
Epoch 610, training loss: 0.015829594805836678 = 0.008714915253221989 + 0.001 * 7.114678859710693
Epoch 610, val loss: 0.8863081336021423
Epoch 620, training loss: 0.015267915092408657 = 0.008153403177857399 + 0.001 * 7.114511489868164
Epoch 620, val loss: 0.8949471116065979
Epoch 630, training loss: 0.014754406176507473 = 0.00764179602265358 + 0.001 * 7.11260986328125
Epoch 630, val loss: 0.903451144695282
Epoch 640, training loss: 0.014286791905760765 = 0.007175210863351822 + 0.001 * 7.1115803718566895
Epoch 640, val loss: 0.9117900729179382
Epoch 650, training loss: 0.013858238235116005 = 0.006749126128852367 + 0.001 * 7.109111785888672
Epoch 650, val loss: 0.9199511408805847
Epoch 660, training loss: 0.013474435545504093 = 0.00635941419750452 + 0.001 * 7.115021228790283
Epoch 660, val loss: 0.9279273748397827
Epoch 670, training loss: 0.01311214454472065 = 0.006002350244671106 + 0.001 * 7.109794616699219
Epoch 670, val loss: 0.9357182383537292
Epoch 680, training loss: 0.012780376709997654 = 0.0056746602058410645 + 0.001 * 7.105716228485107
Epoch 680, val loss: 0.9433150291442871
Epoch 690, training loss: 0.01247938722372055 = 0.005373417399823666 + 0.001 * 7.105968952178955
Epoch 690, val loss: 0.9507375359535217
Epoch 700, training loss: 0.012198712676763535 = 0.005096038803458214 + 0.001 * 7.102673053741455
Epoch 700, val loss: 0.9579865336418152
Epoch 710, training loss: 0.011942954733967781 = 0.004840197507292032 + 0.001 * 7.102757453918457
Epoch 710, val loss: 0.9650519490242004
Epoch 720, training loss: 0.011714493855834007 = 0.004603801295161247 + 0.001 * 7.110692501068115
Epoch 720, val loss: 0.9719387292861938
Epoch 730, training loss: 0.011487715877592564 = 0.004385054111480713 + 0.001 * 7.102661609649658
Epoch 730, val loss: 0.9786520600318909
Epoch 740, training loss: 0.011281902901828289 = 0.004182257689535618 + 0.001 * 7.099644660949707
Epoch 740, val loss: 0.9851984977722168
Epoch 750, training loss: 0.011089623905718327 = 0.003993951249867678 + 0.001 * 7.095672130584717
Epoch 750, val loss: 0.9915862083435059
Epoch 760, training loss: 0.010926267132163048 = 0.0038188304752111435 + 0.001 * 7.107436180114746
Epoch 760, val loss: 0.9978176355361938
Epoch 770, training loss: 0.010751938447356224 = 0.0036557677667587996 + 0.001 * 7.096170425415039
Epoch 770, val loss: 1.0038796663284302
Epoch 780, training loss: 0.010595723055303097 = 0.0035036555491387844 + 0.001 * 7.092067241668701
Epoch 780, val loss: 1.0098309516906738
Epoch 790, training loss: 0.010470142588019371 = 0.0033616111613810062 + 0.001 * 7.1085309982299805
Epoch 790, val loss: 1.0156128406524658
Epoch 800, training loss: 0.010317986831068993 = 0.003228911431506276 + 0.001 * 7.089074611663818
Epoch 800, val loss: 1.0212578773498535
Epoch 810, training loss: 0.010192775167524815 = 0.0031046245712786913 + 0.001 * 7.0881500244140625
Epoch 810, val loss: 1.0267903804779053
Epoch 820, training loss: 0.010074915364384651 = 0.0029880127403885126 + 0.001 * 7.086902141571045
Epoch 820, val loss: 1.0321712493896484
Epoch 830, training loss: 0.009980354458093643 = 0.002878506202250719 + 0.001 * 7.101848125457764
Epoch 830, val loss: 1.0374432802200317
Epoch 840, training loss: 0.009864537976682186 = 0.002775473752990365 + 0.001 * 7.089064121246338
Epoch 840, val loss: 1.0425912141799927
Epoch 850, training loss: 0.00976179726421833 = 0.0026784155052155256 + 0.001 * 7.083381175994873
Epoch 850, val loss: 1.0476056337356567
Epoch 860, training loss: 0.009701250120997429 = 0.002586834132671356 + 0.001 * 7.114415645599365
Epoch 860, val loss: 1.0525070428848267
Epoch 870, training loss: 0.009583885781466961 = 0.0025004383642226458 + 0.001 * 7.083447456359863
Epoch 870, val loss: 1.0572772026062012
Epoch 880, training loss: 0.009497608058154583 = 0.0024187422823160887 + 0.001 * 7.0788655281066895
Epoch 880, val loss: 1.0619471073150635
Epoch 890, training loss: 0.009438865818083286 = 0.0023413689341396093 + 0.001 * 7.097496509552002
Epoch 890, val loss: 1.0665228366851807
Epoch 900, training loss: 0.009343869052827358 = 0.0022680361289530993 + 0.001 * 7.075832843780518
Epoch 900, val loss: 1.0710023641586304
Epoch 910, training loss: 0.009298790246248245 = 0.002198390895500779 + 0.001 * 7.100399017333984
Epoch 910, val loss: 1.075405478477478
Epoch 920, training loss: 0.009203738532960415 = 0.00213227909989655 + 0.001 * 7.0714592933654785
Epoch 920, val loss: 1.0797042846679688
Epoch 930, training loss: 0.009160902351140976 = 0.0020693757105618715 + 0.001 * 7.091526031494141
Epoch 930, val loss: 1.0839359760284424
Epoch 940, training loss: 0.00908771064132452 = 0.0020095736254006624 + 0.001 * 7.078136444091797
Epoch 940, val loss: 1.0880714654922485
Epoch 950, training loss: 0.009022549726068974 = 0.0019526045070961118 + 0.001 * 7.069944381713867
Epoch 950, val loss: 1.0921293497085571
Epoch 960, training loss: 0.008975101634860039 = 0.0018982788315042853 + 0.001 * 7.076822280883789
Epoch 960, val loss: 1.09612238407135
Epoch 970, training loss: 0.008918735198676586 = 0.0018465692410245538 + 0.001 * 7.0721659660339355
Epoch 970, val loss: 1.1000334024429321
Epoch 980, training loss: 0.008870786055922508 = 0.0017973014619201422 + 0.001 * 7.073484420776367
Epoch 980, val loss: 1.1038800477981567
Epoch 990, training loss: 0.008817769587039948 = 0.0017504559364169836 + 0.001 * 7.067313194274902
Epoch 990, val loss: 1.1076232194900513
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 1.9517579078674316 = 1.9431610107421875 + 0.001 * 8.596860885620117
Epoch 0, val loss: 1.9384710788726807
Epoch 10, training loss: 1.9416697025299072 = 1.9330729246139526 + 0.001 * 8.596809387207031
Epoch 10, val loss: 1.9281514883041382
Epoch 20, training loss: 1.9294708967208862 = 1.9208742380142212 + 0.001 * 8.596606254577637
Epoch 20, val loss: 1.9155502319335938
Epoch 30, training loss: 1.9125361442565918 = 1.9039400815963745 + 0.001 * 8.596078872680664
Epoch 30, val loss: 1.8982279300689697
Epoch 40, training loss: 1.8878440856933594 = 1.8792493343353271 + 0.001 * 8.594709396362305
Epoch 40, val loss: 1.8735079765319824
Epoch 50, training loss: 1.8533798456192017 = 1.8447892665863037 + 0.001 * 8.590609550476074
Epoch 50, val loss: 1.8406281471252441
Epoch 60, training loss: 1.8130443096160889 = 1.8044687509536743 + 0.001 * 8.575556755065918
Epoch 60, val loss: 1.8060697317123413
Epoch 70, training loss: 1.7762806415557861 = 1.7677664756774902 + 0.001 * 8.514209747314453
Epoch 70, val loss: 1.777944564819336
Epoch 80, training loss: 1.7327977418899536 = 1.7245954275131226 + 0.001 * 8.202308654785156
Epoch 80, val loss: 1.7402101755142212
Epoch 90, training loss: 1.6712706089019775 = 1.6632578372955322 + 0.001 * 8.012725830078125
Epoch 90, val loss: 1.6847047805786133
Epoch 100, training loss: 1.5884175300598145 = 1.5805041790008545 + 0.001 * 7.913346290588379
Epoch 100, val loss: 1.613524079322815
Epoch 110, training loss: 1.4886823892593384 = 1.48092782497406 + 0.001 * 7.754537105560303
Epoch 110, val loss: 1.530457615852356
Epoch 120, training loss: 1.3823049068450928 = 1.374802827835083 + 0.001 * 7.502072811126709
Epoch 120, val loss: 1.4420607089996338
Epoch 130, training loss: 1.274167776107788 = 1.266768455505371 + 0.001 * 7.399336814880371
Epoch 130, val loss: 1.3525660037994385
Epoch 140, training loss: 1.1668751239776611 = 1.1595137119293213 + 0.001 * 7.361470699310303
Epoch 140, val loss: 1.265257477760315
Epoch 150, training loss: 1.0637099742889404 = 1.0563863515853882 + 0.001 * 7.323577404022217
Epoch 150, val loss: 1.1827692985534668
Epoch 160, training loss: 0.9679974913597107 = 0.9606972932815552 + 0.001 * 7.3002142906188965
Epoch 160, val loss: 1.107919692993164
Epoch 170, training loss: 0.880438506603241 = 0.8731406331062317 + 0.001 * 7.297891139984131
Epoch 170, val loss: 1.040405511856079
Epoch 180, training loss: 0.8005826473236084 = 0.7932843565940857 + 0.001 * 7.2983174324035645
Epoch 180, val loss: 0.9792084693908691
Epoch 190, training loss: 0.7282300591468811 = 0.7209330201148987 + 0.001 * 7.297019004821777
Epoch 190, val loss: 0.9244537353515625
Epoch 200, training loss: 0.6630043983459473 = 0.655708372592926 + 0.001 * 7.295995712280273
Epoch 200, val loss: 0.8764278888702393
Epoch 210, training loss: 0.6037116050720215 = 0.5964164733886719 + 0.001 * 7.29510498046875
Epoch 210, val loss: 0.8353652358055115
Epoch 220, training loss: 0.5487351417541504 = 0.5414409637451172 + 0.001 * 7.294190406799316
Epoch 220, val loss: 0.800615668296814
Epoch 230, training loss: 0.49696555733680725 = 0.48967230319976807 + 0.001 * 7.293262958526611
Epoch 230, val loss: 0.7713544964790344
Epoch 240, training loss: 0.44797801971435547 = 0.4406856298446655 + 0.001 * 7.2923903465271
Epoch 240, val loss: 0.7468159794807434
Epoch 250, training loss: 0.4017846882343292 = 0.3944922685623169 + 0.001 * 7.292427062988281
Epoch 250, val loss: 0.7266423106193542
Epoch 260, training loss: 0.3585672974586487 = 0.3512760400772095 + 0.001 * 7.29126501083374
Epoch 260, val loss: 0.7108116149902344
Epoch 270, training loss: 0.3184695541858673 = 0.31117862462997437 + 0.001 * 7.290943145751953
Epoch 270, val loss: 0.6991154551506042
Epoch 280, training loss: 0.2814575731754303 = 0.2741667330265045 + 0.001 * 7.290848731994629
Epoch 280, val loss: 0.690970778465271
Epoch 290, training loss: 0.24751780927181244 = 0.2402268499135971 + 0.001 * 7.290965557098389
Epoch 290, val loss: 0.6860066652297974
Epoch 300, training loss: 0.21673595905303955 = 0.20944365859031677 + 0.001 * 7.292304992675781
Epoch 300, val loss: 0.6841965913772583
Epoch 310, training loss: 0.18926483392715454 = 0.18197190761566162 + 0.001 * 7.2929253578186035
Epoch 310, val loss: 0.6851210594177246
Epoch 320, training loss: 0.1651683896780014 = 0.15787509083747864 + 0.001 * 7.293298721313477
Epoch 320, val loss: 0.6886416077613831
Epoch 330, training loss: 0.14432762563228607 = 0.1370316445827484 + 0.001 * 7.295979022979736
Epoch 330, val loss: 0.6946240663528442
Epoch 340, training loss: 0.1264379620552063 = 0.11914463341236115 + 0.001 * 7.293320655822754
Epoch 340, val loss: 0.7025872468948364
Epoch 350, training loss: 0.11113741248846054 = 0.10384552925825119 + 0.001 * 7.2918829917907715
Epoch 350, val loss: 0.7122088074684143
Epoch 360, training loss: 0.09806253015995026 = 0.09077093005180359 + 0.001 * 7.291600227355957
Epoch 360, val loss: 0.7230749726295471
Epoch 370, training loss: 0.08688497543334961 = 0.07959912717342377 + 0.001 * 7.285844326019287
Epoch 370, val loss: 0.734907865524292
Epoch 380, training loss: 0.07734844833612442 = 0.0700497180223465 + 0.001 * 7.2987284660339355
Epoch 380, val loss: 0.7474113702774048
Epoch 390, training loss: 0.06916068494319916 = 0.06188170984387398 + 0.001 * 7.278973579406738
Epoch 390, val loss: 0.7604166269302368
Epoch 400, training loss: 0.062147438526153564 = 0.0548853799700737 + 0.001 * 7.262059211730957
Epoch 400, val loss: 0.7736926078796387
Epoch 410, training loss: 0.05618782341480255 = 0.04887857660651207 + 0.001 * 7.309247016906738
Epoch 410, val loss: 0.7871066927909851
Epoch 420, training loss: 0.05095244199037552 = 0.04370465874671936 + 0.001 * 7.24778413772583
Epoch 420, val loss: 0.8006214499473572
Epoch 430, training loss: 0.046459101140499115 = 0.039233434945344925 + 0.001 * 7.2256646156311035
Epoch 430, val loss: 0.8140468597412109
Epoch 440, training loss: 0.042594242841005325 = 0.03535515069961548 + 0.001 * 7.239090442657471
Epoch 440, val loss: 0.8273314237594604
Epoch 450, training loss: 0.03917958587408066 = 0.031979355961084366 + 0.001 * 7.200229167938232
Epoch 450, val loss: 0.8404577374458313
Epoch 460, training loss: 0.036215439438819885 = 0.029032090678811073 + 0.001 * 7.183346748352051
Epoch 460, val loss: 0.8533313274383545
Epoch 470, training loss: 0.03368254378437996 = 0.02645045332610607 + 0.001 * 7.232091426849365
Epoch 470, val loss: 0.8659302592277527
Epoch 480, training loss: 0.03137119486927986 = 0.0241819117218256 + 0.001 * 7.189284324645996
Epoch 480, val loss: 0.8782784342765808
Epoch 490, training loss: 0.02936198189854622 = 0.02218162640929222 + 0.001 * 7.180355072021484
Epoch 490, val loss: 0.8903259038925171
Epoch 500, training loss: 0.027572786435484886 = 0.020412379875779152 + 0.001 * 7.160406589508057
Epoch 500, val loss: 0.9020677208900452
Epoch 510, training loss: 0.026021748781204224 = 0.01884257234632969 + 0.001 * 7.17917537689209
Epoch 510, val loss: 0.913531482219696
Epoch 520, training loss: 0.024587610736489296 = 0.017445191740989685 + 0.001 * 7.142418384552002
Epoch 520, val loss: 0.924659252166748
Epoch 530, training loss: 0.02339879423379898 = 0.016197068616747856 + 0.001 * 7.201725482940674
Epoch 530, val loss: 0.9355139136314392
Epoch 540, training loss: 0.022231224924325943 = 0.015078550204634666 + 0.001 * 7.152673244476318
Epoch 540, val loss: 0.9460457563400269
Epoch 550, training loss: 0.021207861602306366 = 0.014072885736823082 + 0.001 * 7.134975910186768
Epoch 550, val loss: 0.9562830328941345
Epoch 560, training loss: 0.020315419882535934 = 0.013165527954697609 + 0.001 * 7.149891376495361
Epoch 560, val loss: 0.9662315249443054
Epoch 570, training loss: 0.019464615732431412 = 0.012344114482402802 + 0.001 * 7.12050199508667
Epoch 570, val loss: 0.9758818745613098
Epoch 580, training loss: 0.018733490258455276 = 0.01159751508384943 + 0.001 * 7.1359758377075195
Epoch 580, val loss: 0.985280454158783
Epoch 590, training loss: 0.01806030422449112 = 0.010915926657617092 + 0.001 * 7.144377708435059
Epoch 590, val loss: 0.9944366216659546
Epoch 600, training loss: 0.0174259003251791 = 0.010291296988725662 + 0.001 * 7.1346025466918945
Epoch 600, val loss: 1.0033975839614868
Epoch 610, training loss: 0.016844794154167175 = 0.009717184118926525 + 0.001 * 7.127610683441162
Epoch 610, val loss: 1.0121750831604004
Epoch 620, training loss: 0.01630864478647709 = 0.009188462980091572 + 0.001 * 7.120182037353516
Epoch 620, val loss: 1.0207329988479614
Epoch 630, training loss: 0.015850789844989777 = 0.008700785227119923 + 0.001 * 7.1500043869018555
Epoch 630, val loss: 1.0291311740875244
Epoch 640, training loss: 0.01536619383841753 = 0.00825047492980957 + 0.001 * 7.115718364715576
Epoch 640, val loss: 1.0373378992080688
Epoch 650, training loss: 0.014936381950974464 = 0.007834072224795818 + 0.001 * 7.102309703826904
Epoch 650, val loss: 1.0453646183013916
Epoch 660, training loss: 0.014537869021296501 = 0.007448793854564428 + 0.001 * 7.089074611663818
Epoch 660, val loss: 1.0532149076461792
Epoch 670, training loss: 0.014179611578583717 = 0.007091847714036703 + 0.001 * 7.08776330947876
Epoch 670, val loss: 1.0608690977096558
Epoch 680, training loss: 0.013866445049643517 = 0.006760938558727503 + 0.001 * 7.10550594329834
Epoch 680, val loss: 1.0683419704437256
Epoch 690, training loss: 0.01353593822568655 = 0.006453786976635456 + 0.001 * 7.082150936126709
Epoch 690, val loss: 1.075610637664795
Epoch 700, training loss: 0.013238403014838696 = 0.00616810517385602 + 0.001 * 7.070297718048096
Epoch 700, val loss: 1.0827170610427856
Epoch 710, training loss: 0.012994556687772274 = 0.0059020305052399635 + 0.001 * 7.092525959014893
Epoch 710, val loss: 1.089638590812683
Epoch 720, training loss: 0.012759959325194359 = 0.00565373245626688 + 0.001 * 7.106225967407227
Epoch 720, val loss: 1.096384048461914
Epoch 730, training loss: 0.012466799467802048 = 0.005421720445156097 + 0.001 * 7.045078754425049
Epoch 730, val loss: 1.102996826171875
Epoch 740, training loss: 0.012306240387260914 = 0.005204368848353624 + 0.001 * 7.101871013641357
Epoch 740, val loss: 1.1094591617584229
Epoch 750, training loss: 0.01206705067306757 = 0.005000649951398373 + 0.001 * 7.066400527954102
Epoch 750, val loss: 1.1157256364822388
Epoch 760, training loss: 0.011861667037010193 = 0.004809084348380566 + 0.001 * 7.052582263946533
Epoch 760, val loss: 1.1218994855880737
Epoch 770, training loss: 0.011674788780510426 = 0.004628450144082308 + 0.001 * 7.046338081359863
Epoch 770, val loss: 1.1279337406158447
Epoch 780, training loss: 0.011489152908325195 = 0.004457741044461727 + 0.001 * 7.031411647796631
Epoch 780, val loss: 1.1338906288146973
Epoch 790, training loss: 0.011345649138092995 = 0.004296060185879469 + 0.001 * 7.049588680267334
Epoch 790, val loss: 1.139716625213623
Epoch 800, training loss: 0.011173496022820473 = 0.004142900928854942 + 0.001 * 7.030594348907471
Epoch 800, val loss: 1.1454567909240723
Epoch 810, training loss: 0.011029286310076714 = 0.00399774918332696 + 0.001 * 7.031537055969238
Epoch 810, val loss: 1.151076078414917
Epoch 820, training loss: 0.010897567495703697 = 0.003860014956444502 + 0.001 * 7.0375518798828125
Epoch 820, val loss: 1.1566259860992432
Epoch 830, training loss: 0.01075035985559225 = 0.003729116404429078 + 0.001 * 7.021243572235107
Epoch 830, val loss: 1.1620769500732422
Epoch 840, training loss: 0.010646486654877663 = 0.0036052789073437452 + 0.001 * 7.041207313537598
Epoch 840, val loss: 1.167380928993225
Epoch 850, training loss: 0.010533620603382587 = 0.003487749258056283 + 0.001 * 7.045870780944824
Epoch 850, val loss: 1.1726669073104858
Epoch 860, training loss: 0.01042000763118267 = 0.0033759220968931913 + 0.001 * 7.0440850257873535
Epoch 860, val loss: 1.177785038948059
Epoch 870, training loss: 0.010319363325834274 = 0.0032695052213966846 + 0.001 * 7.049858093261719
Epoch 870, val loss: 1.182869553565979
Epoch 880, training loss: 0.010181206278502941 = 0.0031682217959314585 + 0.001 * 7.012983798980713
Epoch 880, val loss: 1.1878176927566528
Epoch 890, training loss: 0.010084000416100025 = 0.003071828046813607 + 0.001 * 7.012171745300293
Epoch 890, val loss: 1.1926993131637573
Epoch 900, training loss: 0.009980974718928337 = 0.002980079036206007 + 0.001 * 7.0008955001831055
Epoch 900, val loss: 1.1975066661834717
Epoch 910, training loss: 0.009892561472952366 = 0.002892646472901106 + 0.001 * 6.999914646148682
Epoch 910, val loss: 1.2022020816802979
Epoch 920, training loss: 0.009800052270293236 = 0.002809386933222413 + 0.001 * 6.990664958953857
Epoch 920, val loss: 1.206803798675537
Epoch 930, training loss: 0.009717580862343311 = 0.002730022184550762 + 0.001 * 6.987558364868164
Epoch 930, val loss: 1.2113211154937744
Epoch 940, training loss: 0.009655128233134747 = 0.0026542593259364367 + 0.001 * 7.000868797302246
Epoch 940, val loss: 1.2157632112503052
Epoch 950, training loss: 0.009596453979611397 = 0.002581999870017171 + 0.001 * 7.014453411102295
Epoch 950, val loss: 1.2201120853424072
Epoch 960, training loss: 0.009502818807959557 = 0.002513138111680746 + 0.001 * 6.98967981338501
Epoch 960, val loss: 1.224393606185913
Epoch 970, training loss: 0.009436473250389099 = 0.002447431441396475 + 0.001 * 6.989041328430176
Epoch 970, val loss: 1.228548526763916
Epoch 980, training loss: 0.009374036453664303 = 0.0023847271222621202 + 0.001 * 6.989308834075928
Epoch 980, val loss: 1.2326743602752686
Epoch 990, training loss: 0.009356582537293434 = 0.0023248675279319286 + 0.001 * 7.031714916229248
Epoch 990, val loss: 1.236641526222229
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 1.9478541612625122 = 1.939257264137268 + 0.001 * 8.596853256225586
Epoch 0, val loss: 1.9360871315002441
Epoch 10, training loss: 1.938306212425232 = 1.9297094345092773 + 0.001 * 8.596806526184082
Epoch 10, val loss: 1.9270846843719482
Epoch 20, training loss: 1.9264830350875854 = 1.9178863763809204 + 0.001 * 8.596635818481445
Epoch 20, val loss: 1.9156087636947632
Epoch 30, training loss: 1.909964919090271 = 1.9013687372207642 + 0.001 * 8.596223831176758
Epoch 30, val loss: 1.8994369506835938
Epoch 40, training loss: 1.885912299156189 = 1.877317190170288 + 0.001 * 8.595100402832031
Epoch 40, val loss: 1.8762203454971313
Epoch 50, training loss: 1.8523110151290894 = 1.8437196016311646 + 0.001 * 8.591460227966309
Epoch 50, val loss: 1.8452739715576172
Epoch 60, training loss: 1.8134299516677856 = 1.8048533201217651 + 0.001 * 8.576635360717773
Epoch 60, val loss: 1.8128485679626465
Epoch 70, training loss: 1.7780107259750366 = 1.7695072889328003 + 0.001 * 8.50348949432373
Epoch 70, val loss: 1.7842973470687866
Epoch 80, training loss: 1.7348846197128296 = 1.726706624031067 + 0.001 * 8.1780366897583
Epoch 80, val loss: 1.7437278032302856
Epoch 90, training loss: 1.6749815940856934 = 1.666966199874878 + 0.001 * 8.01537799835205
Epoch 90, val loss: 1.6884403228759766
Epoch 100, training loss: 1.5941277742385864 = 1.5862491130828857 + 0.001 * 7.878714084625244
Epoch 100, val loss: 1.6180554628372192
Epoch 110, training loss: 1.4965022802352905 = 1.4888209104537964 + 0.001 * 7.681331634521484
Epoch 110, val loss: 1.535096287727356
Epoch 120, training loss: 1.392486572265625 = 1.3850502967834473 + 0.001 * 7.436312675476074
Epoch 120, val loss: 1.447795033454895
Epoch 130, training loss: 1.2874053716659546 = 1.2800103425979614 + 0.001 * 7.395074844360352
Epoch 130, val loss: 1.3588591814041138
Epoch 140, training loss: 1.1818883419036865 = 1.1745444536209106 + 0.001 * 7.343916416168213
Epoch 140, val loss: 1.2706218957901
Epoch 150, training loss: 1.0784357786178589 = 1.0711402893066406 + 0.001 * 7.295501708984375
Epoch 150, val loss: 1.1855489015579224
Epoch 160, training loss: 0.9808659553527832 = 0.9735952615737915 + 0.001 * 7.270708084106445
Epoch 160, val loss: 1.1073509454727173
Epoch 170, training loss: 0.8917070627212524 = 0.8844471573829651 + 0.001 * 7.259932041168213
Epoch 170, val loss: 1.0380423069000244
Epoch 180, training loss: 0.8125157356262207 = 0.8052631616592407 + 0.001 * 7.252591133117676
Epoch 180, val loss: 0.9780442118644714
Epoch 190, training loss: 0.7440910935401917 = 0.7368500232696533 + 0.001 * 7.241042137145996
Epoch 190, val loss: 0.9283384680747986
Epoch 200, training loss: 0.6851862072944641 = 0.6779630780220032 + 0.001 * 7.2231011390686035
Epoch 200, val loss: 0.887888491153717
Epoch 210, training loss: 0.6325811743736267 = 0.6253846883773804 + 0.001 * 7.1964640617370605
Epoch 210, val loss: 0.8543092012405396
Epoch 220, training loss: 0.5824830532073975 = 0.5753117799758911 + 0.001 * 7.171268939971924
Epoch 220, val loss: 0.8246705532073975
Epoch 230, training loss: 0.532274067401886 = 0.5251346826553345 + 0.001 * 7.139357089996338
Epoch 230, val loss: 0.7968035936355591
Epoch 240, training loss: 0.48103055357933044 = 0.4739125370979309 + 0.001 * 7.118010997772217
Epoch 240, val loss: 0.7702727913856506
Epoch 250, training loss: 0.42931774258613586 = 0.4222138524055481 + 0.001 * 7.103877544403076
Epoch 250, val loss: 0.7457485795021057
Epoch 260, training loss: 0.3784801959991455 = 0.3713785409927368 + 0.001 * 7.1016645431518555
Epoch 260, val loss: 0.7241020202636719
Epoch 270, training loss: 0.3300881087779999 = 0.3229925334453583 + 0.001 * 7.0955891609191895
Epoch 270, val loss: 0.7059235572814941
Epoch 280, training loss: 0.2855946719646454 = 0.27850350737571716 + 0.001 * 7.091175079345703
Epoch 280, val loss: 0.6914851665496826
Epoch 290, training loss: 0.24598291516304016 = 0.2388952076435089 + 0.001 * 7.087710380554199
Epoch 290, val loss: 0.6811376214027405
Epoch 300, training loss: 0.2116897851228714 = 0.20460101962089539 + 0.001 * 7.088764667510986
Epoch 300, val loss: 0.6748438477516174
Epoch 310, training loss: 0.18256641924381256 = 0.17548424005508423 + 0.001 * 7.082180500030518
Epoch 310, val loss: 0.6723682284355164
Epoch 320, training loss: 0.15812698006629944 = 0.15105150640010834 + 0.001 * 7.075473785400391
Epoch 320, val loss: 0.6733138561248779
Epoch 330, training loss: 0.13772980868816376 = 0.13064277172088623 + 0.001 * 7.087042331695557
Epoch 330, val loss: 0.6771026849746704
Epoch 340, training loss: 0.1206488236784935 = 0.1135823056101799 + 0.001 * 7.066515922546387
Epoch 340, val loss: 0.6832607984542847
Epoch 350, training loss: 0.10632665455341339 = 0.09926283359527588 + 0.001 * 7.063819885253906
Epoch 350, val loss: 0.6913814544677734
Epoch 360, training loss: 0.09425556659698486 = 0.08717150241136551 + 0.001 * 7.084066390991211
Epoch 360, val loss: 0.7010473012924194
Epoch 370, training loss: 0.0839560478925705 = 0.07689919322729111 + 0.001 * 7.056857585906982
Epoch 370, val loss: 0.711954653263092
Epoch 380, training loss: 0.07517402619123459 = 0.06812519580125809 + 0.001 * 7.048829555511475
Epoch 380, val loss: 0.723742663860321
Epoch 390, training loss: 0.06764137744903564 = 0.06059739738702774 + 0.001 * 7.043979644775391
Epoch 390, val loss: 0.7361062169075012
Epoch 400, training loss: 0.06116810813546181 = 0.05411325395107269 + 0.001 * 7.054853439331055
Epoch 400, val loss: 0.748874306678772
Epoch 410, training loss: 0.05555637925863266 = 0.04851062223315239 + 0.001 * 7.045758247375488
Epoch 410, val loss: 0.7618207931518555
Epoch 420, training loss: 0.05068930983543396 = 0.04365571215748787 + 0.001 * 7.033596992492676
Epoch 420, val loss: 0.7748169898986816
Epoch 430, training loss: 0.046467848122119904 = 0.03943607956171036 + 0.001 * 7.031769275665283
Epoch 430, val loss: 0.7877302765846252
Epoch 440, training loss: 0.04279201477766037 = 0.035756975412368774 + 0.001 * 7.035037994384766
Epoch 440, val loss: 0.8004928827285767
Epoch 450, training loss: 0.03956402838230133 = 0.032538317143917084 + 0.001 * 7.02570915222168
Epoch 450, val loss: 0.8130313158035278
Epoch 460, training loss: 0.03674153983592987 = 0.029712574556469917 + 0.001 * 7.028966426849365
Epoch 460, val loss: 0.8253116607666016
Epoch 470, training loss: 0.034248124808073044 = 0.02722332254052162 + 0.001 * 7.024801731109619
Epoch 470, val loss: 0.8373268246650696
Epoch 480, training loss: 0.032042667269706726 = 0.025022482499480247 + 0.001 * 7.020185470581055
Epoch 480, val loss: 0.8490577340126038
Epoch 490, training loss: 0.030087292194366455 = 0.023068593814969063 + 0.001 * 7.018698692321777
Epoch 490, val loss: 0.8605155944824219
Epoch 500, training loss: 0.028350993990898132 = 0.021325381472706795 + 0.001 * 7.025611877441406
Epoch 500, val loss: 0.8717182874679565
Epoch 510, training loss: 0.026780322194099426 = 0.01976073905825615 + 0.001 * 7.019583702087402
Epoch 510, val loss: 0.8826864361763
Epoch 520, training loss: 0.025363849475979805 = 0.018350068479776382 + 0.001 * 7.01378059387207
Epoch 520, val loss: 0.8934746980667114
Epoch 530, training loss: 0.024104412645101547 = 0.017074760049581528 + 0.001 * 7.029653072357178
Epoch 530, val loss: 0.9040619730949402
Epoch 540, training loss: 0.0229354165494442 = 0.015919888392090797 + 0.001 * 7.01552677154541
Epoch 540, val loss: 0.9143997430801392
Epoch 550, training loss: 0.021882854402065277 = 0.014872071333229542 + 0.001 * 7.010783672332764
Epoch 550, val loss: 0.9245449304580688
Epoch 560, training loss: 0.02093377523124218 = 0.013919524848461151 + 0.001 * 7.0142502784729
Epoch 560, val loss: 0.9344499707221985
Epoch 570, training loss: 0.020071979612112045 = 0.013051657006144524 + 0.001 * 7.020322799682617
Epoch 570, val loss: 0.9441717267036438
Epoch 580, training loss: 0.019261879846453667 = 0.012257156893610954 + 0.001 * 7.004722595214844
Epoch 580, val loss: 0.9536818861961365
Epoch 590, training loss: 0.018534421920776367 = 0.011528650298714638 + 0.001 * 7.005771160125732
Epoch 590, val loss: 0.9630681872367859
Epoch 600, training loss: 0.017861202359199524 = 0.010858540423214436 + 0.001 * 7.002662181854248
Epoch 600, val loss: 0.9722102284431458
Epoch 610, training loss: 0.017242755740880966 = 0.01024162769317627 + 0.001 * 7.00112771987915
Epoch 610, val loss: 0.9812113642692566
Epoch 620, training loss: 0.016678376123309135 = 0.009673329070210457 + 0.001 * 7.005046367645264
Epoch 620, val loss: 0.9900145530700684
Epoch 630, training loss: 0.0161723792552948 = 0.009148810058832169 + 0.001 * 7.023568630218506
Epoch 630, val loss: 0.9986529350280762
Epoch 640, training loss: 0.01566779613494873 = 0.008664327673614025 + 0.001 * 7.003468990325928
Epoch 640, val loss: 1.0071074962615967
Epoch 650, training loss: 0.015215843915939331 = 0.008216275833547115 + 0.001 * 6.999567985534668
Epoch 650, val loss: 1.015382170677185
Epoch 660, training loss: 0.014796003699302673 = 0.007801402360200882 + 0.001 * 6.994601249694824
Epoch 660, val loss: 1.0234814882278442
Epoch 670, training loss: 0.014412686228752136 = 0.007416717242449522 + 0.001 * 6.995968341827393
Epoch 670, val loss: 1.031404733657837
Epoch 680, training loss: 0.014049198478460312 = 0.007058440707623959 + 0.001 * 6.990757942199707
Epoch 680, val loss: 1.0392146110534668
Epoch 690, training loss: 0.013724593445658684 = 0.006724670995026827 + 0.001 * 6.999921798706055
Epoch 690, val loss: 1.046852469444275
Epoch 700, training loss: 0.013420935720205307 = 0.006413100752979517 + 0.001 * 7.0078349113464355
Epoch 700, val loss: 1.0543572902679443
Epoch 710, training loss: 0.013107841834425926 = 0.006121767219156027 + 0.001 * 6.986073970794678
Epoch 710, val loss: 1.0617384910583496
Epoch 720, training loss: 0.012855840846896172 = 0.0058486382476985455 + 0.001 * 7.0072021484375
Epoch 720, val loss: 1.0689992904663086
Epoch 730, training loss: 0.012574741616845131 = 0.005592408124357462 + 0.001 * 6.982333183288574
Epoch 730, val loss: 1.076124906539917
Epoch 740, training loss: 0.01233261451125145 = 0.005351771600544453 + 0.001 * 6.980842113494873
Epoch 740, val loss: 1.0831555128097534
Epoch 750, training loss: 0.01210128515958786 = 0.005125448107719421 + 0.001 * 6.975836753845215
Epoch 750, val loss: 1.0900667905807495
Epoch 760, training loss: 0.01189383678138256 = 0.004912421572953463 + 0.001 * 6.981414318084717
Epoch 760, val loss: 1.0968666076660156
Epoch 770, training loss: 0.0116961058229208 = 0.004711789079010487 + 0.001 * 6.984315872192383
Epoch 770, val loss: 1.1035590171813965
Epoch 780, training loss: 0.011498605832457542 = 0.004522155970335007 + 0.001 * 6.976449966430664
Epoch 780, val loss: 1.1101534366607666
Epoch 790, training loss: 0.011312732473015785 = 0.004342278465628624 + 0.001 * 6.970453262329102
Epoch 790, val loss: 1.1166844367980957
Epoch 800, training loss: 0.011146895587444305 = 0.004170951433479786 + 0.001 * 6.975943565368652
Epoch 800, val loss: 1.1231619119644165
Epoch 810, training loss: 0.010981372557580471 = 0.00400729151442647 + 0.001 * 6.974080562591553
Epoch 810, val loss: 1.1296089887619019
Epoch 820, training loss: 0.010839789174497128 = 0.003851062385365367 + 0.001 * 6.9887261390686035
Epoch 820, val loss: 1.1360117197036743
Epoch 830, training loss: 0.010679573751986027 = 0.0037021494936197996 + 0.001 * 6.977423667907715
Epoch 830, val loss: 1.1423767805099487
Epoch 840, training loss: 0.010520847514271736 = 0.003560292301699519 + 0.001 * 6.960555076599121
Epoch 840, val loss: 1.1487032175064087
Epoch 850, training loss: 0.010402003303170204 = 0.003425175789743662 + 0.001 * 6.976827621459961
Epoch 850, val loss: 1.1549776792526245
Epoch 860, training loss: 0.01025039330124855 = 0.0032970390748232603 + 0.001 * 6.953354358673096
Epoch 860, val loss: 1.161193609237671
Epoch 870, training loss: 0.010142220184206963 = 0.003175406949594617 + 0.001 * 6.966812610626221
Epoch 870, val loss: 1.167330026626587
Epoch 880, training loss: 0.010026206262409687 = 0.003060087328776717 + 0.001 * 6.966118812561035
Epoch 880, val loss: 1.173374891281128
Epoch 890, training loss: 0.009919874370098114 = 0.0029508909210562706 + 0.001 * 6.9689836502075195
Epoch 890, val loss: 1.1793302297592163
Epoch 900, training loss: 0.009809048846364021 = 0.002847519936040044 + 0.001 * 6.961528778076172
Epoch 900, val loss: 1.1852003335952759
Epoch 910, training loss: 0.009726597927510738 = 0.0027495981194078922 + 0.001 * 6.976999282836914
Epoch 910, val loss: 1.1909650564193726
Epoch 920, training loss: 0.009620465338230133 = 0.002656827913597226 + 0.001 * 6.963636875152588
Epoch 920, val loss: 1.19663405418396
Epoch 930, training loss: 0.009507555514574051 = 0.0025689322501420975 + 0.001 * 6.938623428344727
Epoch 930, val loss: 1.2022109031677246
Epoch 940, training loss: 0.009425461292266846 = 0.002485585166141391 + 0.001 * 6.939875602722168
Epoch 940, val loss: 1.2076992988586426
Epoch 950, training loss: 0.009371273219585419 = 0.002406599698588252 + 0.001 * 6.9646735191345215
Epoch 950, val loss: 1.2130738496780396
Epoch 960, training loss: 0.00926700234413147 = 0.002331745345145464 + 0.001 * 6.9352569580078125
Epoch 960, val loss: 1.2183316946029663
Epoch 970, training loss: 0.009217631071805954 = 0.002260762732475996 + 0.001 * 6.956868648529053
Epoch 970, val loss: 1.2235177755355835
Epoch 980, training loss: 0.009159279987215996 = 0.0021933619864284992 + 0.001 * 6.965917587280273
Epoch 980, val loss: 1.2285833358764648
Epoch 990, training loss: 0.009086674079298973 = 0.002129318192601204 + 0.001 * 6.957355499267578
Epoch 990, val loss: 1.2335537672042847
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8286768581971534
The final CL Acc:0.82222, 0.01318, The final GNN Acc:0.83588, 0.00545
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11626])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10562])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9506293535232544 = 1.9420325756072998 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.93901526927948
Epoch 10, training loss: 1.9407347440719604 = 1.9321379661560059 + 0.001 * 8.596753120422363
Epoch 10, val loss: 1.9294474124908447
Epoch 20, training loss: 1.9284309148788452 = 1.9198343753814697 + 0.001 * 8.596487998962402
Epoch 20, val loss: 1.917103886604309
Epoch 30, training loss: 1.91141939163208 = 1.9028234481811523 + 0.001 * 8.5958833694458
Epoch 30, val loss: 1.8997633457183838
Epoch 40, training loss: 1.886765956878662 = 1.8781715631484985 + 0.001 * 8.59444522857666
Epoch 40, val loss: 1.8749847412109375
Epoch 50, training loss: 1.853020191192627 = 1.844429850578308 + 0.001 * 8.590293884277344
Epoch 50, val loss: 1.8429081439971924
Epoch 60, training loss: 1.8160576820373535 = 1.8074829578399658 + 0.001 * 8.574748039245605
Epoch 60, val loss: 1.8120784759521484
Epoch 70, training loss: 1.7842656373977661 = 1.775767207145691 + 0.001 * 8.498392105102539
Epoch 70, val loss: 1.7884058952331543
Epoch 80, training loss: 1.7453795671463013 = 1.7372761964797974 + 0.001 * 8.1033353805542
Epoch 80, val loss: 1.7560442686080933
Epoch 90, training loss: 1.6916444301605225 = 1.6836878061294556 + 0.001 * 7.956573009490967
Epoch 90, val loss: 1.710623860359192
Epoch 100, training loss: 1.6178525686264038 = 1.6100910902023315 + 0.001 * 7.761427879333496
Epoch 100, val loss: 1.6511563062667847
Epoch 110, training loss: 1.5255086421966553 = 1.5178884267807007 + 0.001 * 7.620265483856201
Epoch 110, val loss: 1.5794557332992554
Epoch 120, training loss: 1.4233225584030151 = 1.4157993793487549 + 0.001 * 7.5231547355651855
Epoch 120, val loss: 1.5008220672607422
Epoch 130, training loss: 1.3190613985061646 = 1.3116172552108765 + 0.001 * 7.4441094398498535
Epoch 130, val loss: 1.4229427576065063
Epoch 140, training loss: 1.2185096740722656 = 1.2111148834228516 + 0.001 * 7.39476203918457
Epoch 140, val loss: 1.3499319553375244
Epoch 150, training loss: 1.1258540153503418 = 1.1185002326965332 + 0.001 * 7.353775978088379
Epoch 150, val loss: 1.2845854759216309
Epoch 160, training loss: 1.042837142944336 = 1.0355232954025269 + 0.001 * 7.3137993812561035
Epoch 160, val loss: 1.2278863191604614
Epoch 170, training loss: 0.9688130021095276 = 0.9615219235420227 + 0.001 * 7.291070461273193
Epoch 170, val loss: 1.179449439048767
Epoch 180, training loss: 0.9015596508979797 = 0.8942813277244568 + 0.001 * 7.2783403396606445
Epoch 180, val loss: 1.1374304294586182
Epoch 190, training loss: 0.839031994342804 = 0.8317694664001465 + 0.001 * 7.262511253356934
Epoch 190, val loss: 1.0997713804244995
Epoch 200, training loss: 0.7804259657859802 = 0.7731884717941284 + 0.001 * 7.237497806549072
Epoch 200, val loss: 1.0655666589736938
Epoch 210, training loss: 0.7261300086975098 = 0.7189275622367859 + 0.001 * 7.202470779418945
Epoch 210, val loss: 1.0354915857315063
Epoch 220, training loss: 0.6765186786651611 = 0.6693450808525085 + 0.001 * 7.173582553863525
Epoch 220, val loss: 1.0106651782989502
Epoch 230, training loss: 0.6307259202003479 = 0.6235756874084473 + 0.001 * 7.150249004364014
Epoch 230, val loss: 0.9910988211631775
Epoch 240, training loss: 0.5868788361549377 = 0.5797322392463684 + 0.001 * 7.146625518798828
Epoch 240, val loss: 0.9753528833389282
Epoch 250, training loss: 0.5429161190986633 = 0.5357808470726013 + 0.001 * 7.135257244110107
Epoch 250, val loss: 0.9620448350906372
Epoch 260, training loss: 0.4973180890083313 = 0.4901888072490692 + 0.001 * 7.129284858703613
Epoch 260, val loss: 0.9499074220657349
Epoch 270, training loss: 0.44959431886672974 = 0.4424690306186676 + 0.001 * 7.1252899169921875
Epoch 270, val loss: 0.9388313293457031
Epoch 280, training loss: 0.4002395272254944 = 0.39311712980270386 + 0.001 * 7.122405529022217
Epoch 280, val loss: 0.9291648268699646
Epoch 290, training loss: 0.35052767395973206 = 0.34340885281562805 + 0.001 * 7.11881160736084
Epoch 290, val loss: 0.9216545224189758
Epoch 300, training loss: 0.3024170994758606 = 0.2953023612499237 + 0.001 * 7.114743232727051
Epoch 300, val loss: 0.9177631735801697
Epoch 310, training loss: 0.2581903636455536 = 0.2510753273963928 + 0.001 * 7.115037441253662
Epoch 310, val loss: 0.9192818999290466
Epoch 320, training loss: 0.21953879296779633 = 0.2124311476945877 + 0.001 * 7.1076483726501465
Epoch 320, val loss: 0.9268342852592468
Epoch 330, training loss: 0.1869918555021286 = 0.1798899620771408 + 0.001 * 7.101897239685059
Epoch 330, val loss: 0.9400289058685303
Epoch 340, training loss: 0.16011592745780945 = 0.1530100256204605 + 0.001 * 7.1059041023254395
Epoch 340, val loss: 0.9577150940895081
Epoch 350, training loss: 0.13799737393856049 = 0.13090449571609497 + 0.001 * 7.092875003814697
Epoch 350, val loss: 0.9783815741539001
Epoch 360, training loss: 0.11975692212581635 = 0.11266632378101349 + 0.001 * 7.090595245361328
Epoch 360, val loss: 1.000901222229004
Epoch 370, training loss: 0.10460840910673141 = 0.0975169688463211 + 0.001 * 7.091436862945557
Epoch 370, val loss: 1.0242478847503662
Epoch 380, training loss: 0.09192079305648804 = 0.08483588695526123 + 0.001 * 7.084909439086914
Epoch 380, val loss: 1.047908902168274
Epoch 390, training loss: 0.08121467381715775 = 0.0741424411535263 + 0.001 * 7.0722336769104
Epoch 390, val loss: 1.0717052221298218
Epoch 400, training loss: 0.07213394343852997 = 0.06506510078907013 + 0.001 * 7.068846225738525
Epoch 400, val loss: 1.095518946647644
Epoch 410, training loss: 0.06438040733337402 = 0.05731797590851784 + 0.001 * 7.062432289123535
Epoch 410, val loss: 1.1193190813064575
Epoch 420, training loss: 0.057728201150894165 = 0.050672076642513275 + 0.001 * 7.056124210357666
Epoch 420, val loss: 1.1429227590560913
Epoch 430, training loss: 0.05200653523206711 = 0.04495273530483246 + 0.001 * 7.053798675537109
Epoch 430, val loss: 1.1662441492080688
Epoch 440, training loss: 0.04708097502589226 = 0.04002074524760246 + 0.001 * 7.060230731964111
Epoch 440, val loss: 1.1891282796859741
Epoch 450, training loss: 0.0428105928003788 = 0.03576311469078064 + 0.001 * 7.047477722167969
Epoch 450, val loss: 1.2114249467849731
Epoch 460, training loss: 0.03913694620132446 = 0.03208618611097336 + 0.001 * 7.050760269165039
Epoch 460, val loss: 1.233093023300171
Epoch 470, training loss: 0.035950906574726105 = 0.028907494619488716 + 0.001 * 7.043412208557129
Epoch 470, val loss: 1.2540134191513062
Epoch 480, training loss: 0.033187802881002426 = 0.02615305222570896 + 0.001 * 7.034749507904053
Epoch 480, val loss: 1.2741203308105469
Epoch 490, training loss: 0.0307929627597332 = 0.023757433518767357 + 0.001 * 7.035529613494873
Epoch 490, val loss: 1.2933876514434814
Epoch 500, training loss: 0.028696537017822266 = 0.021662279963493347 + 0.001 * 7.0342559814453125
Epoch 500, val loss: 1.3118482828140259
Epoch 510, training loss: 0.02684924006462097 = 0.01981928199529648 + 0.001 * 7.029957294464111
Epoch 510, val loss: 1.3295432329177856
Epoch 520, training loss: 0.025220755487680435 = 0.01819012500345707 + 0.001 * 7.030631065368652
Epoch 520, val loss: 1.3465639352798462
Epoch 530, training loss: 0.023777632042765617 = 0.01674492098391056 + 0.001 * 7.032710552215576
Epoch 530, val loss: 1.362934947013855
Epoch 540, training loss: 0.02249322272837162 = 0.015459218993782997 + 0.001 * 7.034003734588623
Epoch 540, val loss: 1.3787221908569336
Epoch 550, training loss: 0.021327858790755272 = 0.014312230050563812 + 0.001 * 7.015628337860107
Epoch 550, val loss: 1.393941044807434
Epoch 560, training loss: 0.020317688584327698 = 0.013286136090755463 + 0.001 * 7.031551837921143
Epoch 560, val loss: 1.4086227416992188
Epoch 570, training loss: 0.019387051463127136 = 0.01236584223806858 + 0.001 * 7.021207809448242
Epoch 570, val loss: 1.4227790832519531
Epoch 580, training loss: 0.01854744553565979 = 0.011538156308233738 + 0.001 * 7.0092878341674805
Epoch 580, val loss: 1.436474323272705
Epoch 590, training loss: 0.01781226508319378 = 0.010791528970003128 + 0.001 * 7.020736217498779
Epoch 590, val loss: 1.4497047662734985
Epoch 600, training loss: 0.017129112035036087 = 0.01011618785560131 + 0.001 * 7.012923240661621
Epoch 600, val loss: 1.4624629020690918
Epoch 610, training loss: 0.016516568139195442 = 0.009503817185759544 + 0.001 * 7.012750148773193
Epoch 610, val loss: 1.47479248046875
Epoch 620, training loss: 0.01596112921833992 = 0.008947080932557583 + 0.001 * 7.014048099517822
Epoch 620, val loss: 1.4867147207260132
Epoch 630, training loss: 0.015441285446286201 = 0.008439761586487293 + 0.001 * 7.001523017883301
Epoch 630, val loss: 1.4982414245605469
Epoch 640, training loss: 0.014973940327763557 = 0.007976317778229713 + 0.001 * 6.997622013092041
Epoch 640, val loss: 1.5093892812728882
Epoch 650, training loss: 0.01457974687218666 = 0.007551910821348429 + 0.001 * 7.027836322784424
Epoch 650, val loss: 1.5201648473739624
Epoch 660, training loss: 0.014167134650051594 = 0.0071623679250478745 + 0.001 * 7.004766464233398
Epoch 660, val loss: 1.5306310653686523
Epoch 670, training loss: 0.01380479708313942 = 0.0068040210753679276 + 0.001 * 7.0007758140563965
Epoch 670, val loss: 1.5407524108886719
Epoch 680, training loss: 0.013482356444001198 = 0.006473665125668049 + 0.001 * 7.00869083404541
Epoch 680, val loss: 1.550559401512146
Epoch 690, training loss: 0.013159593567252159 = 0.006168496794998646 + 0.001 * 6.9910969734191895
Epoch 690, val loss: 1.5600789785385132
Epoch 700, training loss: 0.012878058478236198 = 0.0058859833516180515 + 0.001 * 6.992074966430664
Epoch 700, val loss: 1.569323182106018
Epoch 710, training loss: 0.012613411992788315 = 0.005623920354992151 + 0.001 * 6.9894914627075195
Epoch 710, val loss: 1.5783196687698364
Epoch 720, training loss: 0.012372062541544437 = 0.005380331538617611 + 0.001 * 6.991730690002441
Epoch 720, val loss: 1.5870152711868286
Epoch 730, training loss: 0.012144292704761028 = 0.0051532709039747715 + 0.001 * 6.991021633148193
Epoch 730, val loss: 1.5955168008804321
Epoch 740, training loss: 0.011964047327637672 = 0.004940658807754517 + 0.001 * 7.023388385772705
Epoch 740, val loss: 1.6037814617156982
Epoch 750, training loss: 0.011719920672476292 = 0.004740338772535324 + 0.001 * 6.979581356048584
Epoch 750, val loss: 1.611971378326416
Epoch 760, training loss: 0.01153024472296238 = 0.004550502169877291 + 0.001 * 6.97974157333374
Epoch 760, val loss: 1.6200562715530396
Epoch 770, training loss: 0.011364823207259178 = 0.004370060283690691 + 0.001 * 6.994762420654297
Epoch 770, val loss: 1.6281028985977173
Epoch 780, training loss: 0.011200430803000927 = 0.004198506474494934 + 0.001 * 7.00192403793335
Epoch 780, val loss: 1.6360695362091064
Epoch 790, training loss: 0.011020863428711891 = 0.004035757854580879 + 0.001 * 6.985104560852051
Epoch 790, val loss: 1.6439695358276367
Epoch 800, training loss: 0.010860316455364227 = 0.0038815694861114025 + 0.001 * 6.97874641418457
Epoch 800, val loss: 1.6517623662948608
Epoch 810, training loss: 0.010721702128648758 = 0.003735507372766733 + 0.001 * 6.986194610595703
Epoch 810, val loss: 1.659446358680725
Epoch 820, training loss: 0.010580872185528278 = 0.003597264876589179 + 0.001 * 6.983606815338135
Epoch 820, val loss: 1.667013168334961
Epoch 830, training loss: 0.01044883020222187 = 0.0034665025305002928 + 0.001 * 6.982327461242676
Epoch 830, val loss: 1.6743804216384888
Epoch 840, training loss: 0.010319970548152924 = 0.0033428333699703217 + 0.001 * 6.977137088775635
Epoch 840, val loss: 1.6816340684890747
Epoch 850, training loss: 0.010221801698207855 = 0.003225879743695259 + 0.001 * 6.995922088623047
Epoch 850, val loss: 1.6887222528457642
Epoch 860, training loss: 0.010094135999679565 = 0.0031152600422501564 + 0.001 * 6.978876113891602
Epoch 860, val loss: 1.695661187171936
Epoch 870, training loss: 0.009970515966415405 = 0.0030105782207101583 + 0.001 * 6.95993709564209
Epoch 870, val loss: 1.7024648189544678
Epoch 880, training loss: 0.009909030050039291 = 0.0029114619828760624 + 0.001 * 6.997567176818848
Epoch 880, val loss: 1.7091070413589478
Epoch 890, training loss: 0.009777148254215717 = 0.002817599568516016 + 0.001 * 6.959548473358154
Epoch 890, val loss: 1.7155787944793701
Epoch 900, training loss: 0.009692028164863586 = 0.0027286766562610865 + 0.001 * 6.963350772857666
Epoch 900, val loss: 1.7219011783599854
Epoch 910, training loss: 0.009607565589249134 = 0.0026443833485245705 + 0.001 * 6.963181972503662
Epoch 910, val loss: 1.7280728816986084
Epoch 920, training loss: 0.009525250643491745 = 0.0025644206907600164 + 0.001 * 6.960829257965088
Epoch 920, val loss: 1.734114170074463
Epoch 930, training loss: 0.00947880744934082 = 0.002488545374944806 + 0.001 * 6.990261554718018
Epoch 930, val loss: 1.7400208711624146
Epoch 940, training loss: 0.009369734674692154 = 0.0024164849892258644 + 0.001 * 6.953248977661133
Epoch 940, val loss: 1.7457740306854248
Epoch 950, training loss: 0.009329243563115597 = 0.002347986213862896 + 0.001 * 6.98125696182251
Epoch 950, val loss: 1.7514073848724365
Epoch 960, training loss: 0.009234055876731873 = 0.0022828730288892984 + 0.001 * 6.9511823654174805
Epoch 960, val loss: 1.7569137811660767
Epoch 970, training loss: 0.00916549563407898 = 0.002220901194959879 + 0.001 * 6.94459342956543
Epoch 970, val loss: 1.7622952461242676
Epoch 980, training loss: 0.009136538952589035 = 0.0021618662867695093 + 0.001 * 6.974672794342041
Epoch 980, val loss: 1.7675292491912842
Epoch 990, training loss: 0.00904757808893919 = 0.0021055969409644604 + 0.001 * 6.941980838775635
Epoch 990, val loss: 1.7726690769195557
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 1.9563013315200806 = 1.9477044343948364 + 0.001 * 8.596851348876953
Epoch 0, val loss: 1.9466317892074585
Epoch 10, training loss: 1.946353793144226 = 1.9377570152282715 + 0.001 * 8.59680461883545
Epoch 10, val loss: 1.9372361898422241
Epoch 20, training loss: 1.9339979887008667 = 1.9254013299942017 + 0.001 * 8.596648216247559
Epoch 20, val loss: 1.9252662658691406
Epoch 30, training loss: 1.9166531562805176 = 1.9080568552017212 + 0.001 * 8.596264839172363
Epoch 30, val loss: 1.9081662893295288
Epoch 40, training loss: 1.8911305665969849 = 1.8825353384017944 + 0.001 * 8.59526252746582
Epoch 40, val loss: 1.8832060098648071
Epoch 50, training loss: 1.855925440788269 = 1.8473334312438965 + 0.001 * 8.592004776000977
Epoch 50, val loss: 1.8502752780914307
Epoch 60, training loss: 1.817469835281372 = 1.8088911771774292 + 0.001 * 8.5786714553833
Epoch 60, val loss: 1.817656397819519
Epoch 70, training loss: 1.7850525379180908 = 1.7765356302261353 + 0.001 * 8.516871452331543
Epoch 70, val loss: 1.7914272546768188
Epoch 80, training loss: 1.7452080249786377 = 1.7370237112045288 + 0.001 * 8.184325218200684
Epoch 80, val loss: 1.7561366558074951
Epoch 90, training loss: 1.6898062229156494 = 1.68179452419281 + 0.001 * 8.011693000793457
Epoch 90, val loss: 1.7087123394012451
Epoch 100, training loss: 1.6134753227233887 = 1.605600357055664 + 0.001 * 7.874998569488525
Epoch 100, val loss: 1.6465011835098267
Epoch 110, training loss: 1.518803358078003 = 1.5110597610473633 + 0.001 * 7.743609428405762
Epoch 110, val loss: 1.5692715644836426
Epoch 120, training loss: 1.4167729616165161 = 1.4091224670410156 + 0.001 * 7.650551795959473
Epoch 120, val loss: 1.489964246749878
Epoch 130, training loss: 1.314795732498169 = 1.307160496711731 + 0.001 * 7.635287761688232
Epoch 130, val loss: 1.4142448902130127
Epoch 140, training loss: 1.215280294418335 = 1.2076882123947144 + 0.001 * 7.592051982879639
Epoch 140, val loss: 1.3448107242584229
Epoch 150, training loss: 1.120133399963379 = 1.1125941276550293 + 0.001 * 7.539272308349609
Epoch 150, val loss: 1.2823630571365356
Epoch 160, training loss: 1.0305615663528442 = 1.0230807065963745 + 0.001 * 7.480905532836914
Epoch 160, val loss: 1.2259817123413086
Epoch 170, training loss: 0.9469341039657593 = 0.939505934715271 + 0.001 * 7.428156852722168
Epoch 170, val loss: 1.174917459487915
Epoch 180, training loss: 0.8689037561416626 = 0.8615107536315918 + 0.001 * 7.393022060394287
Epoch 180, val loss: 1.1284818649291992
Epoch 190, training loss: 0.7960485219955444 = 0.7886784076690674 + 0.001 * 7.370098114013672
Epoch 190, val loss: 1.0867228507995605
Epoch 200, training loss: 0.7284708023071289 = 0.721119225025177 + 0.001 * 7.351587772369385
Epoch 200, val loss: 1.050506591796875
Epoch 210, training loss: 0.6665676832199097 = 0.659240186214447 + 0.001 * 7.327485084533691
Epoch 210, val loss: 1.021165370941162
Epoch 220, training loss: 0.6102466583251953 = 0.6029431819915771 + 0.001 * 7.3034539222717285
Epoch 220, val loss: 0.9996998906135559
Epoch 230, training loss: 0.5587910413742065 = 0.5514992475509644 + 0.001 * 7.291807174682617
Epoch 230, val loss: 0.9861210584640503
Epoch 240, training loss: 0.5114760994911194 = 0.5041919946670532 + 0.001 * 7.284095764160156
Epoch 240, val loss: 0.9801889061927795
Epoch 250, training loss: 0.46775785088539124 = 0.4604787230491638 + 0.001 * 7.279134273529053
Epoch 250, val loss: 0.9809650778770447
Epoch 260, training loss: 0.42725053429603577 = 0.4199731647968292 + 0.001 * 7.277370929718018
Epoch 260, val loss: 0.9872081875801086
Epoch 270, training loss: 0.3895514905452728 = 0.3822753131389618 + 0.001 * 7.276185512542725
Epoch 270, val loss: 0.9980958104133606
Epoch 280, training loss: 0.3541261553764343 = 0.34685102105140686 + 0.001 * 7.275139331817627
Epoch 280, val loss: 1.012823224067688
Epoch 290, training loss: 0.3204658329486847 = 0.31319180130958557 + 0.001 * 7.274033069610596
Epoch 290, val loss: 1.0306752920150757
Epoch 300, training loss: 0.28829535841941833 = 0.2810221314430237 + 0.001 * 7.273239612579346
Epoch 300, val loss: 1.0512748956680298
Epoch 310, training loss: 0.2577322721481323 = 0.25045907497406006 + 0.001 * 7.273202419281006
Epoch 310, val loss: 1.0743815898895264
Epoch 320, training loss: 0.2292523980140686 = 0.22197888791561127 + 0.001 * 7.2735066413879395
Epoch 320, val loss: 1.0999329090118408
Epoch 330, training loss: 0.20330406725406647 = 0.19602994620800018 + 0.001 * 7.274125576019287
Epoch 330, val loss: 1.1278403997421265
Epoch 340, training loss: 0.1800825595855713 = 0.17280764877796173 + 0.001 * 7.274906635284424
Epoch 340, val loss: 1.1575628519058228
Epoch 350, training loss: 0.15957598388195038 = 0.15229739248752594 + 0.001 * 7.278594017028809
Epoch 350, val loss: 1.1886489391326904
Epoch 360, training loss: 0.14161546528339386 = 0.13433805108070374 + 0.001 * 7.277415752410889
Epoch 360, val loss: 1.2206026315689087
Epoch 370, training loss: 0.12597817182540894 = 0.11869984120130539 + 0.001 * 7.278328895568848
Epoch 370, val loss: 1.252865195274353
Epoch 380, training loss: 0.11239796876907349 = 0.10511883348226547 + 0.001 * 7.27913236618042
Epoch 380, val loss: 1.2849581241607666
Epoch 390, training loss: 0.10060840100049973 = 0.093328557908535 + 0.001 * 7.279839992523193
Epoch 390, val loss: 1.316576361656189
Epoch 400, training loss: 0.09036579728126526 = 0.0830829069018364 + 0.001 * 7.282886981964111
Epoch 400, val loss: 1.3475346565246582
Epoch 410, training loss: 0.0814494863152504 = 0.074167899787426 + 0.001 * 7.281586647033691
Epoch 410, val loss: 1.3776835203170776
Epoch 420, training loss: 0.07367584854364395 = 0.06639367341995239 + 0.001 * 7.282175064086914
Epoch 420, val loss: 1.4069249629974365
Epoch 430, training loss: 0.06688112020492554 = 0.059599291533231735 + 0.001 * 7.28183126449585
Epoch 430, val loss: 1.4351845979690552
Epoch 440, training loss: 0.06093369424343109 = 0.05365064740180969 + 0.001 * 7.283045768737793
Epoch 440, val loss: 1.4624710083007812
Epoch 450, training loss: 0.05571357160806656 = 0.04843170568346977 + 0.001 * 7.281867027282715
Epoch 450, val loss: 1.488785982131958
Epoch 460, training loss: 0.05113273859024048 = 0.043843477964401245 + 0.001 * 7.289259910583496
Epoch 460, val loss: 1.5141479969024658
Epoch 470, training loss: 0.04708440601825714 = 0.039802394807338715 + 0.001 * 7.28201150894165
Epoch 470, val loss: 1.5386741161346436
Epoch 480, training loss: 0.04351697117090225 = 0.03623633459210396 + 0.001 * 7.280638217926025
Epoch 480, val loss: 1.562331199645996
Epoch 490, training loss: 0.0403621643781662 = 0.033083055168390274 + 0.001 * 7.279110431671143
Epoch 490, val loss: 1.5851185321807861
Epoch 500, training loss: 0.03757954761385918 = 0.03028908371925354 + 0.001 * 7.290462493896484
Epoch 500, val loss: 1.6071003675460815
Epoch 510, training loss: 0.0350872240960598 = 0.02780820243060589 + 0.001 * 7.279022693634033
Epoch 510, val loss: 1.6282954216003418
Epoch 520, training loss: 0.03287377581000328 = 0.025600332766771317 + 0.001 * 7.273443222045898
Epoch 520, val loss: 1.6487778425216675
Epoch 530, training loss: 0.030906250700354576 = 0.023630602285265923 + 0.001 * 7.27564811706543
Epoch 530, val loss: 1.6685148477554321
Epoch 540, training loss: 0.02914833091199398 = 0.02186911180615425 + 0.001 * 7.279218673706055
Epoch 540, val loss: 1.6875535249710083
Epoch 550, training loss: 0.027556907385587692 = 0.02029009535908699 + 0.001 * 7.266811847686768
Epoch 550, val loss: 1.7059118747711182
Epoch 560, training loss: 0.026156693696975708 = 0.018871046602725983 + 0.001 * 7.285647392272949
Epoch 560, val loss: 1.7236660718917847
Epoch 570, training loss: 0.02485624887049198 = 0.01759256422519684 + 0.001 * 7.263683795928955
Epoch 570, val loss: 1.7407896518707275
Epoch 580, training loss: 0.02370019257068634 = 0.016438107937574387 + 0.001 * 7.262083530426025
Epoch 580, val loss: 1.757357120513916
Epoch 590, training loss: 0.022670868784189224 = 0.015393040142953396 + 0.001 * 7.277827739715576
Epoch 590, val loss: 1.7733361721038818
Epoch 600, training loss: 0.021711064502596855 = 0.014444601722061634 + 0.001 * 7.266462802886963
Epoch 600, val loss: 1.7887744903564453
Epoch 610, training loss: 0.020836232230067253 = 0.013581777922809124 + 0.001 * 7.254454135894775
Epoch 610, val loss: 1.803694486618042
Epoch 620, training loss: 0.020042801275849342 = 0.012795038521289825 + 0.001 * 7.247762680053711
Epoch 620, val loss: 1.8180850744247437
Epoch 630, training loss: 0.019311821088194847 = 0.01207596343010664 + 0.001 * 7.235857963562012
Epoch 630, val loss: 1.8319844007492065
Epoch 640, training loss: 0.018670279532670975 = 0.011417311616241932 + 0.001 * 7.2529683113098145
Epoch 640, val loss: 1.8454252481460571
Epoch 650, training loss: 0.018051298335194588 = 0.010812771506607533 + 0.001 * 7.238526344299316
Epoch 650, val loss: 1.8584083318710327
Epoch 660, training loss: 0.017525628209114075 = 0.010256834328174591 + 0.001 * 7.268794059753418
Epoch 660, val loss: 1.8709781169891357
Epoch 670, training loss: 0.016978038474917412 = 0.009744465351104736 + 0.001 * 7.233572959899902
Epoch 670, val loss: 1.8831099271774292
Epoch 680, training loss: 0.016500508412718773 = 0.009271482937037945 + 0.001 * 7.229024887084961
Epoch 680, val loss: 1.8948332071304321
Epoch 690, training loss: 0.016068145632743835 = 0.00883394293487072 + 0.001 * 7.234203338623047
Epoch 690, val loss: 1.9061896800994873
Epoch 700, training loss: 0.01563555747270584 = 0.008428431116044521 + 0.001 * 7.207126140594482
Epoch 700, val loss: 1.9172042608261108
Epoch 710, training loss: 0.015246039256453514 = 0.008051948621869087 + 0.001 * 7.194089889526367
Epoch 710, val loss: 1.9278138875961304
Epoch 720, training loss: 0.014898001216351986 = 0.007701787631958723 + 0.001 * 7.196213245391846
Epoch 720, val loss: 1.9381293058395386
Epoch 730, training loss: 0.014673530124127865 = 0.007375703193247318 + 0.001 * 7.297826766967773
Epoch 730, val loss: 1.9481092691421509
Epoch 740, training loss: 0.014297368936240673 = 0.00707140052691102 + 0.001 * 7.225967884063721
Epoch 740, val loss: 1.957818865776062
Epoch 750, training loss: 0.01396138221025467 = 0.006787086371332407 + 0.001 * 7.174295425415039
Epoch 750, val loss: 1.9672119617462158
Epoch 760, training loss: 0.013696692883968353 = 0.006521070376038551 + 0.001 * 7.175622463226318
Epoch 760, val loss: 1.9763344526290894
Epoch 770, training loss: 0.013477038592100143 = 0.00627178605645895 + 0.001 * 7.205252647399902
Epoch 770, val loss: 1.9851495027542114
Epoch 780, training loss: 0.013203021138906479 = 0.0060378434136509895 + 0.001 * 7.165177345275879
Epoch 780, val loss: 1.9936939477920532
Epoch 790, training loss: 0.01301552914083004 = 0.00581803172826767 + 0.001 * 7.1974968910217285
Epoch 790, val loss: 2.002013921737671
Epoch 800, training loss: 0.012768951244652271 = 0.005611293949186802 + 0.001 * 7.157657146453857
Epoch 800, val loss: 2.010099172592163
Epoch 810, training loss: 0.012550398707389832 = 0.005416566040366888 + 0.001 * 7.133832931518555
Epoch 810, val loss: 2.0179145336151123
Epoch 820, training loss: 0.012371713295578957 = 0.0052329557947814465 + 0.001 * 7.13875675201416
Epoch 820, val loss: 2.0255587100982666
Epoch 830, training loss: 0.012200528755784035 = 0.005059640388935804 + 0.001 * 7.140888690948486
Epoch 830, val loss: 2.0329935550689697
Epoch 840, training loss: 0.01206962764263153 = 0.004895825870335102 + 0.001 * 7.173801898956299
Epoch 840, val loss: 2.0402112007141113
Epoch 850, training loss: 0.011889461427927017 = 0.004740857984870672 + 0.001 * 7.148603439331055
Epoch 850, val loss: 2.0471653938293457
Epoch 860, training loss: 0.011741832830011845 = 0.0045940871350467205 + 0.001 * 7.147745132446289
Epoch 860, val loss: 2.0539727210998535
Epoch 870, training loss: 0.011589718982577324 = 0.004454956855624914 + 0.001 * 7.134761333465576
Epoch 870, val loss: 2.0606064796447754
Epoch 880, training loss: 0.011452005244791508 = 0.0043229516595602036 + 0.001 * 7.129053115844727
Epoch 880, val loss: 2.067018747329712
Epoch 890, training loss: 0.011302980594336987 = 0.004197611007839441 + 0.001 * 7.1053690910339355
Epoch 890, val loss: 2.0732815265655518
Epoch 900, training loss: 0.011206042021512985 = 0.004078487399965525 + 0.001 * 7.127553939819336
Epoch 900, val loss: 2.079362630844116
Epoch 910, training loss: 0.0110606849193573 = 0.003965138923376799 + 0.001 * 7.095546245574951
Epoch 910, val loss: 2.08532452583313
Epoch 920, training loss: 0.010991217568516731 = 0.0038572363555431366 + 0.001 * 7.1339802742004395
Epoch 920, val loss: 2.091080904006958
Epoch 930, training loss: 0.010922963730990887 = 0.0037544050719588995 + 0.001 * 7.168558597564697
Epoch 930, val loss: 2.096684217453003
Epoch 940, training loss: 0.01075181644409895 = 0.0036563484463840723 + 0.001 * 7.095468044281006
Epoch 940, val loss: 2.102210283279419
Epoch 950, training loss: 0.010678781196475029 = 0.0035627642646431923 + 0.001 * 7.116015911102295
Epoch 950, val loss: 2.107555866241455
Epoch 960, training loss: 0.010545074939727783 = 0.0034733733627945185 + 0.001 * 7.071701526641846
Epoch 960, val loss: 2.1127429008483887
Epoch 970, training loss: 0.010481004603207111 = 0.003387946868315339 + 0.001 * 7.093057632446289
Epoch 970, val loss: 2.117794990539551
Epoch 980, training loss: 0.010416124947369099 = 0.0033062405418604612 + 0.001 * 7.109884262084961
Epoch 980, val loss: 2.122711420059204
Epoch 990, training loss: 0.010326565243303776 = 0.0032280415762215853 + 0.001 * 7.098523139953613
Epoch 990, val loss: 2.1275553703308105
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 1.9760043621063232 = 1.967407464981079 + 0.001 * 8.596857070922852
Epoch 0, val loss: 1.9732487201690674
Epoch 10, training loss: 1.9654481410980225 = 1.9568513631820679 + 0.001 * 8.59680461883545
Epoch 10, val loss: 1.9620517492294312
Epoch 20, training loss: 1.9525959491729736 = 1.9439992904663086 + 0.001 * 8.596632957458496
Epoch 20, val loss: 1.948373556137085
Epoch 30, training loss: 1.9349417686462402 = 1.9263455867767334 + 0.001 * 8.596232414245605
Epoch 30, val loss: 1.9297878742218018
Epoch 40, training loss: 1.9094549417495728 = 1.9008597135543823 + 0.001 * 8.595215797424316
Epoch 40, val loss: 1.90360426902771
Epoch 50, training loss: 1.8737561702728271 = 1.8651639223098755 + 0.001 * 8.59220027923584
Epoch 50, val loss: 1.8684624433517456
Epoch 60, training loss: 1.8321110010147095 = 1.8235305547714233 + 0.001 * 8.580495834350586
Epoch 60, val loss: 1.8313151597976685
Epoch 70, training loss: 1.7976654767990112 = 1.789139986038208 + 0.001 * 8.525474548339844
Epoch 70, val loss: 1.8037705421447754
Epoch 80, training loss: 1.7646161317825317 = 1.7563989162445068 + 0.001 * 8.217259407043457
Epoch 80, val loss: 1.7748873233795166
Epoch 90, training loss: 1.7188197374343872 = 1.710761308670044 + 0.001 * 8.058478355407715
Epoch 90, val loss: 1.7361400127410889
Epoch 100, training loss: 1.6544078588485718 = 1.6465363502502441 + 0.001 * 7.871500492095947
Epoch 100, val loss: 1.6845988035202026
Epoch 110, training loss: 1.568464756011963 = 1.5607630014419556 + 0.001 * 7.701718330383301
Epoch 110, val loss: 1.6172184944152832
Epoch 120, training loss: 1.4684113264083862 = 1.4608389139175415 + 0.001 * 7.572397232055664
Epoch 120, val loss: 1.5393450260162354
Epoch 130, training loss: 1.364943027496338 = 1.3574373722076416 + 0.001 * 7.505616188049316
Epoch 130, val loss: 1.460197925567627
Epoch 140, training loss: 1.2641736268997192 = 1.2567579746246338 + 0.001 * 7.415672302246094
Epoch 140, val loss: 1.3864930868148804
Epoch 150, training loss: 1.1683690547943115 = 1.161032795906067 + 0.001 * 7.336275100708008
Epoch 150, val loss: 1.3202372789382935
Epoch 160, training loss: 1.0778847932815552 = 1.0705890655517578 + 0.001 * 7.295680522918701
Epoch 160, val loss: 1.2611278295516968
Epoch 170, training loss: 0.9920146465301514 = 0.9847273230552673 + 0.001 * 7.287348747253418
Epoch 170, val loss: 1.207082986831665
Epoch 180, training loss: 0.9100785255432129 = 0.9027971029281616 + 0.001 * 7.281427383422852
Epoch 180, val loss: 1.1567238569259644
Epoch 190, training loss: 0.8318198323249817 = 0.8245425224304199 + 0.001 * 7.2773261070251465
Epoch 190, val loss: 1.1093671321868896
Epoch 200, training loss: 0.7579503655433655 = 0.7506776452064514 + 0.001 * 7.272704601287842
Epoch 200, val loss: 1.0659518241882324
Epoch 210, training loss: 0.6894015073776245 = 0.6821339130401611 + 0.001 * 7.267613410949707
Epoch 210, val loss: 1.027637243270874
Epoch 220, training loss: 0.6265024542808533 = 0.6192399263381958 + 0.001 * 7.262523651123047
Epoch 220, val loss: 0.9954231381416321
Epoch 230, training loss: 0.5687611699104309 = 0.5615043044090271 + 0.001 * 7.256865978240967
Epoch 230, val loss: 0.9696906208992004
Epoch 240, training loss: 0.5152737498283386 = 0.5080223679542542 + 0.001 * 7.251367092132568
Epoch 240, val loss: 0.9502481818199158
Epoch 250, training loss: 0.4650873839855194 = 0.45784443616867065 + 0.001 * 7.242939472198486
Epoch 250, val loss: 0.9365334510803223
Epoch 260, training loss: 0.4175359904766083 = 0.41029781103134155 + 0.001 * 7.238173484802246
Epoch 260, val loss: 0.9280973076820374
Epoch 270, training loss: 0.37232717871665955 = 0.36509817838668823 + 0.001 * 7.228997230529785
Epoch 270, val loss: 0.9244582653045654
Epoch 280, training loss: 0.3297842741012573 = 0.322559654712677 + 0.001 * 7.224605083465576
Epoch 280, val loss: 0.9251846075057983
Epoch 290, training loss: 0.29055356979370117 = 0.2833336889743805 + 0.001 * 7.219874858856201
Epoch 290, val loss: 0.9302611947059631
Epoch 300, training loss: 0.2552742063999176 = 0.2480638474225998 + 0.001 * 7.210366249084473
Epoch 300, val loss: 0.9393380284309387
Epoch 310, training loss: 0.2242196947336197 = 0.2170105129480362 + 0.001 * 7.209181308746338
Epoch 310, val loss: 0.9520334005355835
Epoch 320, training loss: 0.19725777208805084 = 0.1900503933429718 + 0.001 * 7.2073845863342285
Epoch 320, val loss: 0.9676403999328613
Epoch 330, training loss: 0.17400644719600677 = 0.16680389642715454 + 0.001 * 7.202555179595947
Epoch 330, val loss: 0.9856211543083191
Epoch 340, training loss: 0.15397851169109344 = 0.14677941799163818 + 0.001 * 7.199090003967285
Epoch 340, val loss: 1.0053937435150146
Epoch 350, training loss: 0.13668198883533478 = 0.12948894500732422 + 0.001 * 7.193037509918213
Epoch 350, val loss: 1.0265475511550903
Epoch 360, training loss: 0.12169433385133743 = 0.11450899392366409 + 0.001 * 7.185341835021973
Epoch 360, val loss: 1.0485937595367432
Epoch 370, training loss: 0.10867346823215485 = 0.10148937255144119 + 0.001 * 7.1840925216674805
Epoch 370, val loss: 1.0712604522705078
Epoch 380, training loss: 0.09731542319059372 = 0.09014088660478592 + 0.001 * 7.174534797668457
Epoch 380, val loss: 1.0942429304122925
Epoch 390, training loss: 0.0874132439494133 = 0.08023080974817276 + 0.001 * 7.182435512542725
Epoch 390, val loss: 1.117315649986267
Epoch 400, training loss: 0.078736312687397 = 0.0715683102607727 + 0.001 * 7.167998790740967
Epoch 400, val loss: 1.1403142213821411
Epoch 410, training loss: 0.07114508002996445 = 0.06399185210466385 + 0.001 * 7.153224945068359
Epoch 410, val loss: 1.1630631685256958
Epoch 420, training loss: 0.06451099365949631 = 0.05736245959997177 + 0.001 * 7.148536205291748
Epoch 420, val loss: 1.185471773147583
Epoch 430, training loss: 0.05869834125041962 = 0.05155915021896362 + 0.001 * 7.139191150665283
Epoch 430, val loss: 1.207482933998108
Epoch 440, training loss: 0.05360890179872513 = 0.04647568613290787 + 0.001 * 7.133214950561523
Epoch 440, val loss: 1.2290656566619873
Epoch 450, training loss: 0.04914200305938721 = 0.04201957955956459 + 0.001 * 7.122422695159912
Epoch 450, val loss: 1.250208854675293
Epoch 460, training loss: 0.04522085189819336 = 0.03810916468501091 + 0.001 * 7.111685752868652
Epoch 460, val loss: 1.2708443403244019
Epoch 470, training loss: 0.04178299754858017 = 0.034672223031520844 + 0.001 * 7.110774040222168
Epoch 470, val loss: 1.2909945249557495
Epoch 480, training loss: 0.038752079010009766 = 0.03164546191692352 + 0.001 * 7.106618404388428
Epoch 480, val loss: 1.3106012344360352
Epoch 490, training loss: 0.03607819601893425 = 0.028973719105124474 + 0.001 * 7.104477405548096
Epoch 490, val loss: 1.3296380043029785
Epoch 500, training loss: 0.033705081790685654 = 0.02660924196243286 + 0.001 * 7.095839500427246
Epoch 500, val loss: 1.3480910062789917
Epoch 510, training loss: 0.03160809352993965 = 0.024511201307177544 + 0.001 * 7.096891403198242
Epoch 510, val loss: 1.3660136461257935
Epoch 520, training loss: 0.029735909774899483 = 0.022644061595201492 + 0.001 * 7.091847896575928
Epoch 520, val loss: 1.3833820819854736
Epoch 530, training loss: 0.028056379407644272 = 0.02097715623676777 + 0.001 * 7.079221725463867
Epoch 530, val loss: 1.4001779556274414
Epoch 540, training loss: 0.02656593918800354 = 0.01948452554643154 + 0.001 * 7.081412315368652
Epoch 540, val loss: 1.416443109512329
Epoch 550, training loss: 0.025224875658750534 = 0.018143808469176292 + 0.001 * 7.081066608428955
Epoch 550, val loss: 1.4322413206100464
Epoch 560, training loss: 0.024011120200157166 = 0.01693575829267502 + 0.001 * 7.075361251831055
Epoch 560, val loss: 1.4475135803222656
Epoch 570, training loss: 0.022915415465831757 = 0.01584302820265293 + 0.001 * 7.072387218475342
Epoch 570, val loss: 1.4623091220855713
Epoch 580, training loss: 0.021923312917351723 = 0.014851039275527 + 0.001 * 7.072272777557373
Epoch 580, val loss: 1.4767149686813354
Epoch 590, training loss: 0.021014811471104622 = 0.013946212828159332 + 0.001 * 7.068598747253418
Epoch 590, val loss: 1.4907115697860718
Epoch 600, training loss: 0.02022673934698105 = 0.01311744935810566 + 0.001 * 7.109289646148682
Epoch 600, val loss: 1.5043410062789917
Epoch 610, training loss: 0.01941932551562786 = 0.012355984188616276 + 0.001 * 7.063340663909912
Epoch 610, val loss: 1.5176124572753906
Epoch 620, training loss: 0.018722590059041977 = 0.011654862202703953 + 0.001 * 7.0677266120910645
Epoch 620, val loss: 1.5305742025375366
Epoch 630, training loss: 0.018079131841659546 = 0.011008343659341335 + 0.001 * 7.070788860321045
Epoch 630, val loss: 1.5432063341140747
Epoch 640, training loss: 0.017474010586738586 = 0.010411649011075497 + 0.001 * 7.062361240386963
Epoch 640, val loss: 1.5555219650268555
Epoch 650, training loss: 0.016919581219553947 = 0.009860413148999214 + 0.001 * 7.059167861938477
Epoch 650, val loss: 1.5675101280212402
Epoch 660, training loss: 0.01641114056110382 = 0.009350748732686043 + 0.001 * 7.060390949249268
Epoch 660, val loss: 1.5791888236999512
Epoch 670, training loss: 0.015954438596963882 = 0.008879019878804684 + 0.001 * 7.075417995452881
Epoch 670, val loss: 1.5905176401138306
Epoch 680, training loss: 0.01550508476793766 = 0.008441971614956856 + 0.001 * 7.063112735748291
Epoch 680, val loss: 1.6015721559524536
Epoch 690, training loss: 0.015095549635589123 = 0.008036545477807522 + 0.001 * 7.059003829956055
Epoch 690, val loss: 1.6123034954071045
Epoch 700, training loss: 0.01470913179218769 = 0.007659736555069685 + 0.001 * 7.0493950843811035
Epoch 700, val loss: 1.6227766275405884
Epoch 710, training loss: 0.014353586360812187 = 0.007308821193873882 + 0.001 * 7.044764518737793
Epoch 710, val loss: 1.6329245567321777
Epoch 720, training loss: 0.014043912291526794 = 0.00698100496083498 + 0.001 * 7.062906742095947
Epoch 720, val loss: 1.6428310871124268
Epoch 730, training loss: 0.013727586716413498 = 0.006673470605164766 + 0.001 * 7.054115295410156
Epoch 730, val loss: 1.6525051593780518
Epoch 740, training loss: 0.013424547389149666 = 0.006383752450346947 + 0.001 * 7.040794849395752
Epoch 740, val loss: 1.66196608543396
Epoch 750, training loss: 0.013157689943909645 = 0.006110052578151226 + 0.001 * 7.04763650894165
Epoch 750, val loss: 1.671149730682373
Epoch 760, training loss: 0.012915747240185738 = 0.005851272959262133 + 0.001 * 7.064474105834961
Epoch 760, val loss: 1.680106282234192
Epoch 770, training loss: 0.01265650987625122 = 0.005606763064861298 + 0.001 * 7.049746990203857
Epoch 770, val loss: 1.68879234790802
Epoch 780, training loss: 0.012452080845832825 = 0.005375669803470373 + 0.001 * 7.076411247253418
Epoch 780, val loss: 1.6973137855529785
Epoch 790, training loss: 0.012209152802824974 = 0.005157438572496176 + 0.001 * 7.0517144203186035
Epoch 790, val loss: 1.7055768966674805
Epoch 800, training loss: 0.011983057484030724 = 0.00495143560692668 + 0.001 * 7.031621932983398
Epoch 800, val loss: 1.7136391401290894
Epoch 810, training loss: 0.01179486233741045 = 0.004756982903927565 + 0.001 * 7.03787899017334
Epoch 810, val loss: 1.7214893102645874
Epoch 820, training loss: 0.01160443015396595 = 0.00457348907366395 + 0.001 * 7.030941009521484
Epoch 820, val loss: 1.7291538715362549
Epoch 830, training loss: 0.011437139473855495 = 0.004400263540446758 + 0.001 * 7.0368757247924805
Epoch 830, val loss: 1.7366265058517456
Epoch 840, training loss: 0.011280226521193981 = 0.004236577544361353 + 0.001 * 7.043648719787598
Epoch 840, val loss: 1.7439453601837158
Epoch 850, training loss: 0.01111561432480812 = 0.004081697203218937 + 0.001 * 7.033916473388672
Epoch 850, val loss: 1.7510919570922852
Epoch 860, training loss: 0.010956864804029465 = 0.003935113549232483 + 0.001 * 7.0217509269714355
Epoch 860, val loss: 1.7580589056015015
Epoch 870, training loss: 0.010818198323249817 = 0.003796215169131756 + 0.001 * 7.0219831466674805
Epoch 870, val loss: 1.7649072408676147
Epoch 880, training loss: 0.010679356753826141 = 0.003664414631202817 + 0.001 * 7.014941692352295
Epoch 880, val loss: 1.771621823310852
Epoch 890, training loss: 0.010559795424342155 = 0.003539257450029254 + 0.001 * 7.020537853240967
Epoch 890, val loss: 1.7781645059585571
Epoch 900, training loss: 0.010453440248966217 = 0.0034202197566628456 + 0.001 * 7.033220291137695
Epoch 900, val loss: 1.7845971584320068
Epoch 910, training loss: 0.010330593213438988 = 0.003307011444121599 + 0.001 * 7.023581504821777
Epoch 910, val loss: 1.7909268140792847
Epoch 920, training loss: 0.010231805965304375 = 0.0031992902513593435 + 0.001 * 7.032515048980713
Epoch 920, val loss: 1.7971261739730835
Epoch 930, training loss: 0.010116422548890114 = 0.0030967877246439457 + 0.001 * 7.019634246826172
Epoch 930, val loss: 1.8031522035598755
Epoch 940, training loss: 0.009999623522162437 = 0.0029992307536303997 + 0.001 * 7.000392913818359
Epoch 940, val loss: 1.8090643882751465
Epoch 950, training loss: 0.009920000098645687 = 0.0029063986148685217 + 0.001 * 7.013600826263428
Epoch 950, val loss: 1.8148486614227295
Epoch 960, training loss: 0.009844105690717697 = 0.0028180486988276243 + 0.001 * 7.02605676651001
Epoch 960, val loss: 1.820473074913025
Epoch 970, training loss: 0.009751112200319767 = 0.0027338205836713314 + 0.001 * 7.017291069030762
Epoch 970, val loss: 1.825993537902832
Epoch 980, training loss: 0.009648071601986885 = 0.0026535973884165287 + 0.001 * 6.994474411010742
Epoch 980, val loss: 1.8313674926757812
Epoch 990, training loss: 0.009578724391758442 = 0.0025770789943635464 + 0.001 * 7.001645088195801
Epoch 990, val loss: 1.8366214036941528
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8149710068529257
The final CL Acc:0.74321, 0.01222, The final GNN Acc:0.80952, 0.00414
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13216])
remove edge: torch.Size([2, 7892])
updated graph: torch.Size([2, 10552])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9642338752746582 = 1.955636978149414 + 0.001 * 8.596839904785156
Epoch 0, val loss: 1.9498603343963623
Epoch 10, training loss: 1.9537816047668457 = 1.9451848268508911 + 0.001 * 8.596795082092285
Epoch 10, val loss: 1.9398329257965088
Epoch 20, training loss: 1.9411224126815796 = 1.9325257539749146 + 0.001 * 8.596611976623535
Epoch 20, val loss: 1.9273401498794556
Epoch 30, training loss: 1.9237099885940552 = 1.9151138067245483 + 0.001 * 8.596162796020508
Epoch 30, val loss: 1.9098221063613892
Epoch 40, training loss: 1.8982689380645752 = 1.8896739482879639 + 0.001 * 8.594979286193848
Epoch 40, val loss: 1.8843252658843994
Epoch 50, training loss: 1.8623361587524414 = 1.8537451028823853 + 0.001 * 8.591066360473633
Epoch 50, val loss: 1.8497238159179688
Epoch 60, training loss: 1.8196414709091187 = 1.8110672235488892 + 0.001 * 8.574275970458984
Epoch 60, val loss: 1.8121274709701538
Epoch 70, training loss: 1.7809100151062012 = 1.772409200668335 + 0.001 * 8.500873565673828
Epoch 70, val loss: 1.782414197921753
Epoch 80, training loss: 1.7388954162597656 = 1.7307219505310059 + 0.001 * 8.173523902893066
Epoch 80, val loss: 1.7481307983398438
Epoch 90, training loss: 1.6799272298812866 = 1.6718969345092773 + 0.001 * 8.030303955078125
Epoch 90, val loss: 1.6963053941726685
Epoch 100, training loss: 1.5994532108306885 = 1.5915703773498535 + 0.001 * 7.882832050323486
Epoch 100, val loss: 1.626320481300354
Epoch 110, training loss: 1.501079797744751 = 1.493375539779663 + 0.001 * 7.704228401184082
Epoch 110, val loss: 1.5457028150558472
Epoch 120, training loss: 1.3958638906478882 = 1.388266682624817 + 0.001 * 7.597231388092041
Epoch 120, val loss: 1.460572600364685
Epoch 130, training loss: 1.2916643619537354 = 1.2840797901153564 + 0.001 * 7.5846052169799805
Epoch 130, val loss: 1.3770548105239868
Epoch 140, training loss: 1.1906732320785522 = 1.1831241846084595 + 0.001 * 7.548994541168213
Epoch 140, val loss: 1.296680212020874
Epoch 150, training loss: 1.0950576066970825 = 1.0875521898269653 + 0.001 * 7.505474090576172
Epoch 150, val loss: 1.2220722436904907
Epoch 160, training loss: 1.0065135955810547 = 0.9990772604942322 + 0.001 * 7.436306953430176
Epoch 160, val loss: 1.1544091701507568
Epoch 170, training loss: 0.9244904518127441 = 0.9171338677406311 + 0.001 * 7.35658073425293
Epoch 170, val loss: 1.0930490493774414
Epoch 180, training loss: 0.8471595644950867 = 0.8398602604866028 + 0.001 * 7.299320697784424
Epoch 180, val loss: 1.035043478012085
Epoch 190, training loss: 0.7740272879600525 = 0.7667697072029114 + 0.001 * 7.257599353790283
Epoch 190, val loss: 0.9798371195793152
Epoch 200, training loss: 0.7060333490371704 = 0.6987918615341187 + 0.001 * 7.241511344909668
Epoch 200, val loss: 0.928464949131012
Epoch 210, training loss: 0.6435827016830444 = 0.6363493204116821 + 0.001 * 7.233404159545898
Epoch 210, val loss: 0.8825170397758484
Epoch 220, training loss: 0.585757851600647 = 0.5785318613052368 + 0.001 * 7.225976467132568
Epoch 220, val loss: 0.842634916305542
Epoch 230, training loss: 0.5312808156013489 = 0.5240638256072998 + 0.001 * 7.216970443725586
Epoch 230, val loss: 0.808829665184021
Epoch 240, training loss: 0.47943755984306335 = 0.47222957015037537 + 0.001 * 7.207981109619141
Epoch 240, val loss: 0.7803504467010498
Epoch 250, training loss: 0.43003442883491516 = 0.42283424735069275 + 0.001 * 7.200182914733887
Epoch 250, val loss: 0.7563904523849487
Epoch 260, training loss: 0.3832911252975464 = 0.3760981261730194 + 0.001 * 7.193002700805664
Epoch 260, val loss: 0.7365720272064209
Epoch 270, training loss: 0.33963045477867126 = 0.33244770765304565 + 0.001 * 7.182748794555664
Epoch 270, val loss: 0.7210099697113037
Epoch 280, training loss: 0.29952919483184814 = 0.29235130548477173 + 0.001 * 7.17789888381958
Epoch 280, val loss: 0.7097090482711792
Epoch 290, training loss: 0.26338306069374084 = 0.25621965527534485 + 0.001 * 7.163417339324951
Epoch 290, val loss: 0.702433705329895
Epoch 300, training loss: 0.23137405514717102 = 0.22421646118164062 + 0.001 * 7.1575927734375
Epoch 300, val loss: 0.6988917589187622
Epoch 310, training loss: 0.20333409309387207 = 0.19618529081344604 + 0.001 * 7.1487956047058105
Epoch 310, val loss: 0.6986771821975708
Epoch 320, training loss: 0.17892026901245117 = 0.1717914491891861 + 0.001 * 7.128813743591309
Epoch 320, val loss: 0.7013930082321167
Epoch 330, training loss: 0.15777724981307983 = 0.1506473273038864 + 0.001 * 7.129924297332764
Epoch 330, val loss: 0.7065283060073853
Epoch 340, training loss: 0.1394607424736023 = 0.1323435753583908 + 0.001 * 7.117166996002197
Epoch 340, val loss: 0.7136778831481934
Epoch 350, training loss: 0.12360576540231705 = 0.11649392545223236 + 0.001 * 7.11184024810791
Epoch 350, val loss: 0.7224286794662476
Epoch 360, training loss: 0.10983648151159286 = 0.1027449443936348 + 0.001 * 7.09153938293457
Epoch 360, val loss: 0.7323917746543884
Epoch 370, training loss: 0.09787648916244507 = 0.09079522639513016 + 0.001 * 7.081264495849609
Epoch 370, val loss: 0.7432857751846313
Epoch 380, training loss: 0.08749817311763763 = 0.0803929790854454 + 0.001 * 7.105191707611084
Epoch 380, val loss: 0.7549325227737427
Epoch 390, training loss: 0.07840346544981003 = 0.07132899016141891 + 0.001 * 7.074477672576904
Epoch 390, val loss: 0.7670677304267883
Epoch 400, training loss: 0.07049639523029327 = 0.0634269043803215 + 0.001 * 7.069493770599365
Epoch 400, val loss: 0.7795835137367249
Epoch 410, training loss: 0.06360111385583878 = 0.056541331112384796 + 0.001 * 7.059782028198242
Epoch 410, val loss: 0.792263925075531
Epoch 420, training loss: 0.057612430304288864 = 0.05054425448179245 + 0.001 * 7.068174362182617
Epoch 420, val loss: 0.8049673438072205
Epoch 430, training loss: 0.05237435922026634 = 0.045321326702833176 + 0.001 * 7.053032875061035
Epoch 430, val loss: 0.8175812363624573
Epoch 440, training loss: 0.047822557389736176 = 0.040769655257463455 + 0.001 * 7.052901744842529
Epoch 440, val loss: 0.8300237059593201
Epoch 450, training loss: 0.04384463652968407 = 0.03679782152175903 + 0.001 * 7.046814441680908
Epoch 450, val loss: 0.8421913981437683
Epoch 460, training loss: 0.040371254086494446 = 0.03332545608282089 + 0.001 * 7.0457987785339355
Epoch 460, val loss: 0.8540456891059875
Epoch 470, training loss: 0.037337131798267365 = 0.030283179134130478 + 0.001 * 7.053953647613525
Epoch 470, val loss: 0.8656083941459656
Epoch 480, training loss: 0.03465794026851654 = 0.027610918506979942 + 0.001 * 7.047023296356201
Epoch 480, val loss: 0.8768487572669983
Epoch 490, training loss: 0.032302018254995346 = 0.02525692991912365 + 0.001 * 7.045087814331055
Epoch 490, val loss: 0.8877940773963928
Epoch 500, training loss: 0.030221834778785706 = 0.02317689172923565 + 0.001 * 7.044942855834961
Epoch 500, val loss: 0.8984150886535645
Epoch 510, training loss: 0.028376586735248566 = 0.021333029493689537 + 0.001 * 7.043557643890381
Epoch 510, val loss: 0.9087278842926025
Epoch 520, training loss: 0.026725750416517258 = 0.019693493843078613 + 0.001 * 7.032256603240967
Epoch 520, val loss: 0.9187225699424744
Epoch 530, training loss: 0.025293830782175064 = 0.01823110692203045 + 0.001 * 7.062722682952881
Epoch 530, val loss: 0.9284085631370544
Epoch 540, training loss: 0.023955106735229492 = 0.016922766342759132 + 0.001 * 7.032341003417969
Epoch 540, val loss: 0.9377768635749817
Epoch 550, training loss: 0.022786909714341164 = 0.01574850082397461 + 0.001 * 7.0384087562561035
Epoch 550, val loss: 0.9468619227409363
Epoch 560, training loss: 0.021725893020629883 = 0.014691313728690147 + 0.001 * 7.034578323364258
Epoch 560, val loss: 0.9556578397750854
Epoch 570, training loss: 0.020761772990226746 = 0.01373700425028801 + 0.001 * 7.0247673988342285
Epoch 570, val loss: 0.9641864895820618
Epoch 580, training loss: 0.019897719845175743 = 0.012873098254203796 + 0.001 * 7.02462100982666
Epoch 580, val loss: 0.9724414944648743
Epoch 590, training loss: 0.01910892128944397 = 0.012088925577700138 + 0.001 * 7.01999568939209
Epoch 590, val loss: 0.9804303050041199
Epoch 600, training loss: 0.018414774909615517 = 0.011375222355127335 + 0.001 * 7.039552688598633
Epoch 600, val loss: 0.9881641268730164
Epoch 610, training loss: 0.017747558653354645 = 0.010724036954343319 + 0.001 * 7.023520469665527
Epoch 610, val loss: 0.9956750869750977
Epoch 620, training loss: 0.017145808786153793 = 0.010128472931683064 + 0.001 * 7.017334938049316
Epoch 620, val loss: 1.0029499530792236
Epoch 630, training loss: 0.01661926507949829 = 0.009582511149346828 + 0.001 * 7.036752700805664
Epoch 630, val loss: 1.010002851486206
Epoch 640, training loss: 0.01609768532216549 = 0.009080945514142513 + 0.001 * 7.016739368438721
Epoch 640, val loss: 1.016840934753418
Epoch 650, training loss: 0.015633277595043182 = 0.008619158528745174 + 0.001 * 7.014118671417236
Epoch 650, val loss: 1.0234702825546265
Epoch 660, training loss: 0.01520191878080368 = 0.008193128742277622 + 0.001 * 7.0087890625
Epoch 660, val loss: 1.0299053192138672
Epoch 670, training loss: 0.014812372624874115 = 0.007799309678375721 + 0.001 * 7.013062477111816
Epoch 670, val loss: 1.0361502170562744
Epoch 680, training loss: 0.014446578919887543 = 0.007434610743075609 + 0.001 * 7.01196813583374
Epoch 680, val loss: 1.0422217845916748
Epoch 690, training loss: 0.014096787199378014 = 0.0070962426252663136 + 0.001 * 7.000543594360352
Epoch 690, val loss: 1.0481129884719849
Epoch 700, training loss: 0.013782871887087822 = 0.006781753618270159 + 0.001 * 7.001117706298828
Epoch 700, val loss: 1.053837537765503
Epoch 710, training loss: 0.013496936298906803 = 0.0064889853820204735 + 0.001 * 7.007950782775879
Epoch 710, val loss: 1.059402346611023
Epoch 720, training loss: 0.013217832893133163 = 0.006215987727046013 + 0.001 * 7.001844882965088
Epoch 720, val loss: 1.064805507659912
Epoch 730, training loss: 0.01297088898718357 = 0.005961071699857712 + 0.001 * 7.009817600250244
Epoch 730, val loss: 1.070062279701233
Epoch 740, training loss: 0.012714643031358719 = 0.005722641479223967 + 0.001 * 6.992001056671143
Epoch 740, val loss: 1.0751746892929077
Epoch 750, training loss: 0.01250157505273819 = 0.005499371327459812 + 0.001 * 7.002203941345215
Epoch 750, val loss: 1.0801571607589722
Epoch 760, training loss: 0.012290392071008682 = 0.005289987660944462 + 0.001 * 7.000404357910156
Epoch 760, val loss: 1.0849958658218384
Epoch 770, training loss: 0.01208670623600483 = 0.005093367770314217 + 0.001 * 6.993337631225586
Epoch 770, val loss: 1.089693307876587
Epoch 780, training loss: 0.011906992644071579 = 0.004908493719995022 + 0.001 * 6.998498439788818
Epoch 780, val loss: 1.094279170036316
Epoch 790, training loss: 0.011719653382897377 = 0.00473447423428297 + 0.001 * 6.985178470611572
Epoch 790, val loss: 1.098741888999939
Epoch 800, training loss: 0.011599394492805004 = 0.00457045529037714 + 0.001 * 7.0289387702941895
Epoch 800, val loss: 1.103086233139038
Epoch 810, training loss: 0.01139645092189312 = 0.004415746778249741 + 0.001 * 6.980703830718994
Epoch 810, val loss: 1.107320785522461
Epoch 820, training loss: 0.011262132786214352 = 0.004269633442163467 + 0.001 * 6.992498874664307
Epoch 820, val loss: 1.1114412546157837
Epoch 830, training loss: 0.011112414300441742 = 0.00413148757070303 + 0.001 * 6.980926036834717
Epoch 830, val loss: 1.1154592037200928
Epoch 840, training loss: 0.010976230725646019 = 0.004000719171017408 + 0.001 * 6.975510597229004
Epoch 840, val loss: 1.1193630695343018
Epoch 850, training loss: 0.010878481902182102 = 0.0038768278900533915 + 0.001 * 7.001653671264648
Epoch 850, val loss: 1.1231704950332642
Epoch 860, training loss: 0.010726822540163994 = 0.0037593625020235777 + 0.001 * 6.9674601554870605
Epoch 860, val loss: 1.1268835067749023
Epoch 870, training loss: 0.010652637109160423 = 0.003647870384156704 + 0.001 * 7.004766464233398
Epoch 870, val loss: 1.1305075883865356
Epoch 880, training loss: 0.010510111227631569 = 0.003541952930390835 + 0.001 * 6.96815824508667
Epoch 880, val loss: 1.1340388059616089
Epoch 890, training loss: 0.010402688756585121 = 0.003441254375502467 + 0.001 * 6.961434364318848
Epoch 890, val loss: 1.1374940872192383
Epoch 900, training loss: 0.01031496375799179 = 0.003345432924106717 + 0.001 * 6.96953010559082
Epoch 900, val loss: 1.1408661603927612
Epoch 910, training loss: 0.010236656293272972 = 0.0032541595865041018 + 0.001 * 6.982496738433838
Epoch 910, val loss: 1.1441493034362793
Epoch 920, training loss: 0.010122954845428467 = 0.003167148446664214 + 0.001 * 6.955805778503418
Epoch 920, val loss: 1.1473547220230103
Epoch 930, training loss: 0.010034930892288685 = 0.0030841354746371508 + 0.001 * 6.950794696807861
Epoch 930, val loss: 1.1504971981048584
Epoch 940, training loss: 0.009954258799552917 = 0.0030048175249248743 + 0.001 * 6.949440956115723
Epoch 940, val loss: 1.1535581350326538
Epoch 950, training loss: 0.009890442714095116 = 0.002928873524069786 + 0.001 * 6.961569309234619
Epoch 950, val loss: 1.156546711921692
Epoch 960, training loss: 0.009808529168367386 = 0.0028558787889778614 + 0.001 * 6.9526495933532715
Epoch 960, val loss: 1.1594831943511963
Epoch 970, training loss: 0.009761206805706024 = 0.0027853180654346943 + 0.001 * 6.975888729095459
Epoch 970, val loss: 1.1623694896697998
Epoch 980, training loss: 0.009690644219517708 = 0.002716825343668461 + 0.001 * 6.973818778991699
Epoch 980, val loss: 1.1651854515075684
Epoch 990, training loss: 0.009599771350622177 = 0.002650014590471983 + 0.001 * 6.949756622314453
Epoch 990, val loss: 1.1679714918136597
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.9535354375839233 = 1.9449386596679688 + 0.001 * 8.596835136413574
Epoch 0, val loss: 1.954849362373352
Epoch 10, training loss: 1.943312406539917 = 1.9347156286239624 + 0.001 * 8.596797943115234
Epoch 10, val loss: 1.9441015720367432
Epoch 20, training loss: 1.9310036897659302 = 1.9224070310592651 + 0.001 * 8.596627235412598
Epoch 20, val loss: 1.9310768842697144
Epoch 30, training loss: 1.9142040014266968 = 1.90560781955719 + 0.001 * 8.596238136291504
Epoch 30, val loss: 1.9131802320480347
Epoch 40, training loss: 1.8900421857833862 = 1.8814468383789062 + 0.001 * 8.595329284667969
Epoch 40, val loss: 1.887693166732788
Epoch 50, training loss: 1.856298565864563 = 1.8477057218551636 + 0.001 * 8.59285831451416
Epoch 50, val loss: 1.8530747890472412
Epoch 60, training loss: 1.815824031829834 = 1.8072395324707031 + 0.001 * 8.584458351135254
Epoch 60, val loss: 1.8140791654586792
Epoch 70, training loss: 1.7771772146224976 = 1.7686254978179932 + 0.001 * 8.551772117614746
Epoch 70, val loss: 1.7794808149337769
Epoch 80, training loss: 1.733190894126892 = 1.7248073816299438 + 0.001 * 8.383526802062988
Epoch 80, val loss: 1.7397817373275757
Epoch 90, training loss: 1.6714221239089966 = 1.6634769439697266 + 0.001 * 7.945130348205566
Epoch 90, val loss: 1.6846330165863037
Epoch 100, training loss: 1.5890814065933228 = 1.5813051462173462 + 0.001 * 7.776281833648682
Epoch 100, val loss: 1.6133484840393066
Epoch 110, training loss: 1.4885170459747314 = 1.4808882474899292 + 0.001 * 7.628854274749756
Epoch 110, val loss: 1.529410481452942
Epoch 120, training loss: 1.3784196376800537 = 1.3709158897399902 + 0.001 * 7.503807067871094
Epoch 120, val loss: 1.4377549886703491
Epoch 130, training loss: 1.2678391933441162 = 1.2604058980941772 + 0.001 * 7.433330535888672
Epoch 130, val loss: 1.3475676774978638
Epoch 140, training loss: 1.1628817319869995 = 1.1555079221725464 + 0.001 * 7.373859405517578
Epoch 140, val loss: 1.263257622718811
Epoch 150, training loss: 1.068650484085083 = 1.0613406896591187 + 0.001 * 7.309818267822266
Epoch 150, val loss: 1.1893404722213745
Epoch 160, training loss: 0.9871816039085388 = 0.9799140095710754 + 0.001 * 7.267597675323486
Epoch 160, val loss: 1.1263830661773682
Epoch 170, training loss: 0.9160528182983398 = 0.9088075757026672 + 0.001 * 7.245240211486816
Epoch 170, val loss: 1.0717989206314087
Epoch 180, training loss: 0.8505237698554993 = 0.8432865142822266 + 0.001 * 7.237284183502197
Epoch 180, val loss: 1.020944595336914
Epoch 190, training loss: 0.7868824601173401 = 0.7796468138694763 + 0.001 * 7.23563289642334
Epoch 190, val loss: 0.9704340100288391
Epoch 200, training loss: 0.7239507436752319 = 0.7167177796363831 + 0.001 * 7.232964038848877
Epoch 200, val loss: 0.9194311499595642
Epoch 210, training loss: 0.6625277400016785 = 0.6552999019622803 + 0.001 * 7.227843284606934
Epoch 210, val loss: 0.8695425987243652
Epoch 220, training loss: 0.603537380695343 = 0.5963165760040283 + 0.001 * 7.2208051681518555
Epoch 220, val loss: 0.8228133916854858
Epoch 230, training loss: 0.5472234487533569 = 0.5400123000144958 + 0.001 * 7.2111711502075195
Epoch 230, val loss: 0.780836820602417
Epoch 240, training loss: 0.49351105093955994 = 0.486313134431839 + 0.001 * 7.197909355163574
Epoch 240, val loss: 0.7440340518951416
Epoch 250, training loss: 0.44254663586616516 = 0.4353669583797455 + 0.001 * 7.179675102233887
Epoch 250, val loss: 0.7128123044967651
Epoch 260, training loss: 0.3945808708667755 = 0.3874242901802063 + 0.001 * 7.156569480895996
Epoch 260, val loss: 0.6874380707740784
Epoch 270, training loss: 0.34978562593460083 = 0.34265050292015076 + 0.001 * 7.13511323928833
Epoch 270, val loss: 0.6675022840499878
Epoch 280, training loss: 0.30836251378059387 = 0.30124431848526 + 0.001 * 7.11818790435791
Epoch 280, val loss: 0.6524515748023987
Epoch 290, training loss: 0.2706463038921356 = 0.26353877782821655 + 0.001 * 7.107534885406494
Epoch 290, val loss: 0.6421881914138794
Epoch 300, training loss: 0.23697686195373535 = 0.22987474501132965 + 0.001 * 7.102121829986572
Epoch 300, val loss: 0.6362464427947998
Epoch 310, training loss: 0.2074662446975708 = 0.2003653645515442 + 0.001 * 7.100880146026611
Epoch 310, val loss: 0.6343290209770203
Epoch 320, training loss: 0.18193763494491577 = 0.1748376041650772 + 0.001 * 7.100030422210693
Epoch 320, val loss: 0.6358431577682495
Epoch 330, training loss: 0.15998782217502594 = 0.1528880000114441 + 0.001 * 7.099815368652344
Epoch 330, val loss: 0.6401717662811279
Epoch 340, training loss: 0.14114481210708618 = 0.13404494524002075 + 0.001 * 7.099871635437012
Epoch 340, val loss: 0.6465785503387451
Epoch 350, training loss: 0.12496854364871979 = 0.1178683340549469 + 0.001 * 7.100212097167969
Epoch 350, val loss: 0.6546251773834229
Epoch 360, training loss: 0.11106789112091064 = 0.10396713763475418 + 0.001 * 7.100753307342529
Epoch 360, val loss: 0.6638036370277405
Epoch 370, training loss: 0.09910250455141068 = 0.09200021624565125 + 0.001 * 7.102290630340576
Epoch 370, val loss: 0.6738502383232117
Epoch 380, training loss: 0.08877073973417282 = 0.08166863769292831 + 0.001 * 7.102104187011719
Epoch 380, val loss: 0.6844018697738647
Epoch 390, training loss: 0.07981991022825241 = 0.07271707057952881 + 0.001 * 7.102842330932617
Epoch 390, val loss: 0.6952934265136719
Epoch 400, training loss: 0.07202941924333572 = 0.06492599844932556 + 0.001 * 7.103423118591309
Epoch 400, val loss: 0.7063654661178589
Epoch 410, training loss: 0.06522060185670853 = 0.05811651796102524 + 0.001 * 7.104086875915527
Epoch 410, val loss: 0.7174568176269531
Epoch 420, training loss: 0.059250470250844955 = 0.05214574187994003 + 0.001 * 7.104727268218994
Epoch 420, val loss: 0.7284998297691345
Epoch 430, training loss: 0.05400374159216881 = 0.04689745232462883 + 0.001 * 7.106287956237793
Epoch 430, val loss: 0.7394576668739319
Epoch 440, training loss: 0.04938210919499397 = 0.042275939136743546 + 0.001 * 7.106168270111084
Epoch 440, val loss: 0.7503199577331543
Epoch 450, training loss: 0.0453072264790535 = 0.03820087015628815 + 0.001 * 7.1063551902771
Epoch 450, val loss: 0.7610676288604736
Epoch 460, training loss: 0.04171014577150345 = 0.03460349515080452 + 0.001 * 7.1066484451293945
Epoch 460, val loss: 0.7716618776321411
Epoch 470, training loss: 0.038531526923179626 = 0.03142384812235832 + 0.001 * 7.1076765060424805
Epoch 470, val loss: 0.7821148633956909
Epoch 480, training loss: 0.03571905568242073 = 0.02861081063747406 + 0.001 * 7.108243942260742
Epoch 480, val loss: 0.7923789024353027
Epoch 490, training loss: 0.033226966857910156 = 0.02611919492483139 + 0.001 * 7.107772350311279
Epoch 490, val loss: 0.8024386763572693
Epoch 500, training loss: 0.03101697564125061 = 0.023909345269203186 + 0.001 * 7.107629299163818
Epoch 500, val loss: 0.8122766613960266
Epoch 510, training loss: 0.02905461937189102 = 0.02194628305733204 + 0.001 * 7.108336925506592
Epoch 510, val loss: 0.8218944072723389
Epoch 520, training loss: 0.027307217940688133 = 0.020199289545416832 + 0.001 * 7.107928276062012
Epoch 520, val loss: 0.831254243850708
Epoch 530, training loss: 0.02574923262000084 = 0.018641434609889984 + 0.001 * 7.107798099517822
Epoch 530, val loss: 0.8403887748718262
Epoch 540, training loss: 0.02435753494501114 = 0.017249343916773796 + 0.001 * 7.108190536499023
Epoch 540, val loss: 0.849273145198822
Epoch 550, training loss: 0.023110294714570045 = 0.016002288088202477 + 0.001 * 7.108006000518799
Epoch 550, val loss: 0.8579462766647339
Epoch 560, training loss: 0.021989909932017326 = 0.014882399700582027 + 0.001 * 7.107509613037109
Epoch 560, val loss: 0.8663804531097412
Epoch 570, training loss: 0.020985299721360207 = 0.013874010182917118 + 0.001 * 7.111288547515869
Epoch 570, val loss: 0.8745990991592407
Epoch 580, training loss: 0.020071251317858696 = 0.012963731773197651 + 0.001 * 7.107519149780273
Epoch 580, val loss: 0.8825476169586182
Epoch 590, training loss: 0.019245849922299385 = 0.012139290571212769 + 0.001 * 7.106558322906494
Epoch 590, val loss: 0.890313982963562
Epoch 600, training loss: 0.018497873097658157 = 0.011389155872166157 + 0.001 * 7.10871696472168
Epoch 600, val loss: 0.8978727459907532
Epoch 610, training loss: 0.017809327691793442 = 0.010703155770897865 + 0.001 * 7.106172561645508
Epoch 610, val loss: 0.9052225947380066
Epoch 620, training loss: 0.01717827282845974 = 0.010073192417621613 + 0.001 * 7.105079650878906
Epoch 620, val loss: 0.9124212861061096
Epoch 630, training loss: 0.016597770154476166 = 0.009493112564086914 + 0.001 * 7.10465669631958
Epoch 630, val loss: 0.9194239377975464
Epoch 640, training loss: 0.016063613817095757 = 0.008957882411777973 + 0.001 * 7.105731010437012
Epoch 640, val loss: 0.9262906312942505
Epoch 650, training loss: 0.01556721143424511 = 0.008463351987302303 + 0.001 * 7.103858947753906
Epoch 650, val loss: 0.9329904317855835
Epoch 660, training loss: 0.015108097344636917 = 0.008005556650459766 + 0.001 * 7.102540493011475
Epoch 660, val loss: 0.939544141292572
Epoch 670, training loss: 0.014682677574455738 = 0.007580965291708708 + 0.001 * 7.101711750030518
Epoch 670, val loss: 0.9459913969039917
Epoch 680, training loss: 0.014295050874352455 = 0.007186721079051495 + 0.001 * 7.108328819274902
Epoch 680, val loss: 0.9523515701293945
Epoch 690, training loss: 0.0139209795743227 = 0.0068202451802790165 + 0.001 * 7.100734233856201
Epoch 690, val loss: 0.9585102200508118
Epoch 700, training loss: 0.013579323887825012 = 0.0064791906625032425 + 0.001 * 7.100133419036865
Epoch 700, val loss: 0.9645861387252808
Epoch 710, training loss: 0.013259274885058403 = 0.00616162084043026 + 0.001 * 7.097653388977051
Epoch 710, val loss: 0.9705411791801453
Epoch 720, training loss: 0.012980877421796322 = 0.005865802988409996 + 0.001 * 7.115074157714844
Epoch 720, val loss: 0.976392388343811
Epoch 730, training loss: 0.01269068755209446 = 0.005590100772678852 + 0.001 * 7.1005859375
Epoch 730, val loss: 0.9820871949195862
Epoch 740, training loss: 0.01242824550718069 = 0.005333048291504383 + 0.001 * 7.095196723937988
Epoch 740, val loss: 0.9876789450645447
Epoch 750, training loss: 0.0121914092451334 = 0.005093392450362444 + 0.001 * 7.098016738891602
Epoch 750, val loss: 0.9931328892707825
Epoch 760, training loss: 0.011960774660110474 = 0.004869791213423014 + 0.001 * 7.090982437133789
Epoch 760, val loss: 0.9984667897224426
Epoch 770, training loss: 0.011754056438803673 = 0.004660940729081631 + 0.001 * 7.09311580657959
Epoch 770, val loss: 1.0036673545837402
Epoch 780, training loss: 0.011554554104804993 = 0.004465687088668346 + 0.001 * 7.0888671875
Epoch 780, val loss: 1.0087666511535645
Epoch 790, training loss: 0.011382361873984337 = 0.0042829676531255245 + 0.001 * 7.09939432144165
Epoch 790, val loss: 1.0137356519699097
Epoch 800, training loss: 0.011209528893232346 = 0.0041122520342469215 + 0.001 * 7.097276210784912
Epoch 800, val loss: 1.0185942649841309
Epoch 810, training loss: 0.011033483780920506 = 0.003952459432184696 + 0.001 * 7.081024169921875
Epoch 810, val loss: 1.023337483406067
Epoch 820, training loss: 0.01087894942611456 = 0.0038025418762117624 + 0.001 * 7.076407432556152
Epoch 820, val loss: 1.0279873609542847
Epoch 830, training loss: 0.010764644481241703 = 0.0036617338191717863 + 0.001 * 7.10291051864624
Epoch 830, val loss: 1.0325430631637573
Epoch 840, training loss: 0.010609615594148636 = 0.003529513254761696 + 0.001 * 7.080102443695068
Epoch 840, val loss: 1.0369627475738525
Epoch 850, training loss: 0.010506026446819305 = 0.003405163763090968 + 0.001 * 7.1008620262146
Epoch 850, val loss: 1.0412721633911133
Epoch 860, training loss: 0.010373705998063087 = 0.003288209903985262 + 0.001 * 7.085495948791504
Epoch 860, val loss: 1.0454858541488647
Epoch 870, training loss: 0.010262462310492992 = 0.0031780784483999014 + 0.001 * 7.084383964538574
Epoch 870, val loss: 1.049631118774414
Epoch 880, training loss: 0.010129889473319054 = 0.00307428278028965 + 0.001 * 7.055606365203857
Epoch 880, val loss: 1.053635835647583
Epoch 890, training loss: 0.01003390271216631 = 0.002976245479658246 + 0.001 * 7.057656764984131
Epoch 890, val loss: 1.0575604438781738
Epoch 900, training loss: 0.009945341385900974 = 0.002883553272113204 + 0.001 * 7.061788082122803
Epoch 900, val loss: 1.061374545097351
Epoch 910, training loss: 0.00983990915119648 = 0.0027960077859461308 + 0.001 * 7.043900966644287
Epoch 910, val loss: 1.0651084184646606
Epoch 920, training loss: 0.009747745469212532 = 0.002713095163926482 + 0.001 * 7.0346503257751465
Epoch 920, val loss: 1.0687609910964966
Epoch 930, training loss: 0.009694038890302181 = 0.002634621225297451 + 0.001 * 7.059417247772217
Epoch 930, val loss: 1.072318196296692
Epoch 940, training loss: 0.009628144092857838 = 0.002560315653681755 + 0.001 * 7.067828178405762
Epoch 940, val loss: 1.0757800340652466
Epoch 950, training loss: 0.009548002853989601 = 0.0024896960239857435 + 0.001 * 7.058306694030762
Epoch 950, val loss: 1.0791763067245483
Epoch 960, training loss: 0.009454715065658092 = 0.002422564895823598 + 0.001 * 7.032149791717529
Epoch 960, val loss: 1.0825018882751465
Epoch 970, training loss: 0.009384292177855968 = 0.002358729252591729 + 0.001 * 7.025562286376953
Epoch 970, val loss: 1.0857363939285278
Epoch 980, training loss: 0.009351551532745361 = 0.0022981909569352865 + 0.001 * 7.053360462188721
Epoch 980, val loss: 1.0889250040054321
Epoch 990, training loss: 0.009276165626943111 = 0.0022405090276151896 + 0.001 * 7.035656452178955
Epoch 990, val loss: 1.0919578075408936
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 1.9431345462799072 = 1.934537649154663 + 0.001 * 8.596858024597168
Epoch 0, val loss: 1.9308003187179565
Epoch 10, training loss: 1.9332683086395264 = 1.9246715307235718 + 0.001 * 8.596818923950195
Epoch 10, val loss: 1.9215196371078491
Epoch 20, training loss: 1.9212521314620972 = 1.9126554727554321 + 0.001 * 8.5966796875
Epoch 20, val loss: 1.9097169637680054
Epoch 30, training loss: 1.9046363830566406 = 1.8960400819778442 + 0.001 * 8.596327781677246
Epoch 30, val loss: 1.8929797410964966
Epoch 40, training loss: 1.8804548978805542 = 1.8718595504760742 + 0.001 * 8.595377922058105
Epoch 40, val loss: 1.8685880899429321
Epoch 50, training loss: 1.847274899482727 = 1.8386825323104858 + 0.001 * 8.5923433303833
Epoch 50, val loss: 1.8365824222564697
Epoch 60, training loss: 1.8107413053512573 = 1.8021607398986816 + 0.001 * 8.580584526062012
Epoch 60, val loss: 1.8049899339675903
Epoch 70, training loss: 1.7775107622146606 = 1.7689807415008545 + 0.001 * 8.529995918273926
Epoch 70, val loss: 1.7778483629226685
Epoch 80, training loss: 1.734147071838379 = 1.7258847951889038 + 0.001 * 8.262262344360352
Epoch 80, val loss: 1.739193081855774
Epoch 90, training loss: 1.673826813697815 = 1.6657506227493286 + 0.001 * 8.076208114624023
Epoch 90, val loss: 1.6858285665512085
Epoch 100, training loss: 1.5936110019683838 = 1.5856270790100098 + 0.001 * 7.983935832977295
Epoch 100, val loss: 1.617021918296814
Epoch 110, training loss: 1.499276876449585 = 1.491408348083496 + 0.001 * 7.868475437164307
Epoch 110, val loss: 1.538420557975769
Epoch 120, training loss: 1.3999409675598145 = 1.3922579288482666 + 0.001 * 7.6830034255981445
Epoch 120, val loss: 1.4576562643051147
Epoch 130, training loss: 1.2987130880355835 = 1.2911872863769531 + 0.001 * 7.5257768630981445
Epoch 130, val loss: 1.3774032592773438
Epoch 140, training loss: 1.1942569017410278 = 1.1867860555648804 + 0.001 * 7.47083854675293
Epoch 140, val loss: 1.2947291135787964
Epoch 150, training loss: 1.0873993635177612 = 1.079967975616455 + 0.001 * 7.431352138519287
Epoch 150, val loss: 1.210469365119934
Epoch 160, training loss: 0.9829174280166626 = 0.9755048155784607 + 0.001 * 7.412588119506836
Epoch 160, val loss: 1.128021478652954
Epoch 170, training loss: 0.8864437937736511 = 0.879051148891449 + 0.001 * 7.392644882202148
Epoch 170, val loss: 1.0521999597549438
Epoch 180, training loss: 0.8020016551017761 = 0.79462730884552 + 0.001 * 7.374350070953369
Epoch 180, val loss: 0.9863068461418152
Epoch 190, training loss: 0.7308592200279236 = 0.723499059677124 + 0.001 * 7.360154628753662
Epoch 190, val loss: 0.9319769740104675
Epoch 200, training loss: 0.6713534593582153 = 0.6640042066574097 + 0.001 * 7.349239349365234
Epoch 200, val loss: 0.8885960578918457
Epoch 210, training loss: 0.6200422048568726 = 0.6127035021781921 + 0.001 * 7.338681697845459
Epoch 210, val loss: 0.8535277247428894
Epoch 220, training loss: 0.5734522938728333 = 0.5661293864250183 + 0.001 * 7.322923183441162
Epoch 220, val loss: 0.8238412737846375
Epoch 230, training loss: 0.5289338827133179 = 0.5216352343559265 + 0.001 * 7.298677921295166
Epoch 230, val loss: 0.7972316145896912
Epoch 240, training loss: 0.48480990529060364 = 0.4775279462337494 + 0.001 * 7.281958103179932
Epoch 240, val loss: 0.7725002765655518
Epoch 250, training loss: 0.44013097882270813 = 0.43286991119384766 + 0.001 * 7.261063575744629
Epoch 250, val loss: 0.7491084337234497
Epoch 260, training loss: 0.3945524990558624 = 0.3872992694377899 + 0.001 * 7.253215312957764
Epoch 260, val loss: 0.727308988571167
Epoch 270, training loss: 0.34861770272254944 = 0.34136849641799927 + 0.001 * 7.249213695526123
Epoch 270, val loss: 0.7077730298042297
Epoch 280, training loss: 0.3037850558757782 = 0.296538770198822 + 0.001 * 7.246279716491699
Epoch 280, val loss: 0.691534698009491
Epoch 290, training loss: 0.2620360553264618 = 0.25479230284690857 + 0.001 * 7.243748188018799
Epoch 290, val loss: 0.6795768737792969
Epoch 300, training loss: 0.22514434158802032 = 0.2179032564163208 + 0.001 * 7.241088390350342
Epoch 300, val loss: 0.6726104617118835
Epoch 310, training loss: 0.19384030997753143 = 0.18660128116607666 + 0.001 * 7.23902702331543
Epoch 310, val loss: 0.6707541942596436
Epoch 320, training loss: 0.16784687340259552 = 0.16060684621334076 + 0.001 * 7.2400336265563965
Epoch 320, val loss: 0.6733155846595764
Epoch 330, training loss: 0.14636676013469696 = 0.13913048803806305 + 0.001 * 7.236267566680908
Epoch 330, val loss: 0.6792216300964355
Epoch 340, training loss: 0.1285487711429596 = 0.12131369113922119 + 0.001 * 7.235077381134033
Epoch 340, val loss: 0.6874853372573853
Epoch 350, training loss: 0.11364887654781342 = 0.10641628503799438 + 0.001 * 7.2325921058654785
Epoch 350, val loss: 0.6973612308502197
Epoch 360, training loss: 0.10106982290744781 = 0.09383861720561981 + 0.001 * 7.231202602386475
Epoch 360, val loss: 0.7083408236503601
Epoch 370, training loss: 0.09035048633813858 = 0.08312194794416428 + 0.001 * 7.228541374206543
Epoch 370, val loss: 0.7200751304626465
Epoch 380, training loss: 0.08114708214998245 = 0.07391908019781113 + 0.001 * 7.228002548217773
Epoch 380, val loss: 0.7322959303855896
Epoch 390, training loss: 0.07319241762161255 = 0.06596437096595764 + 0.001 * 7.2280497550964355
Epoch 390, val loss: 0.7448514699935913
Epoch 400, training loss: 0.06628226488828659 = 0.05905638635158539 + 0.001 * 7.225881576538086
Epoch 400, val loss: 0.7575845122337341
Epoch 410, training loss: 0.060258928686380386 = 0.05303798243403435 + 0.001 * 7.220945358276367
Epoch 410, val loss: 0.7704460620880127
Epoch 420, training loss: 0.05499633029103279 = 0.04778188094496727 + 0.001 * 7.214447498321533
Epoch 420, val loss: 0.7833207845687866
Epoch 430, training loss: 0.050391826778650284 = 0.04318176954984665 + 0.001 * 7.210055828094482
Epoch 430, val loss: 0.7961233258247375
Epoch 440, training loss: 0.0463571697473526 = 0.03914861008524895 + 0.001 * 7.208560943603516
Epoch 440, val loss: 0.808802604675293
Epoch 450, training loss: 0.04281359910964966 = 0.03560410439968109 + 0.001 * 7.209495544433594
Epoch 450, val loss: 0.8212783336639404
Epoch 460, training loss: 0.03969186171889305 = 0.03247978538274765 + 0.001 * 7.212076187133789
Epoch 460, val loss: 0.833524227142334
Epoch 470, training loss: 0.0369308665394783 = 0.029719389975070953 + 0.001 * 7.211478233337402
Epoch 470, val loss: 0.8455105423927307
Epoch 480, training loss: 0.03448425233364105 = 0.02727445587515831 + 0.001 * 7.2097978591918945
Epoch 480, val loss: 0.8572584390640259
Epoch 490, training loss: 0.03230702504515648 = 0.025101926177740097 + 0.001 * 7.205099105834961
Epoch 490, val loss: 0.868713915348053
Epoch 500, training loss: 0.030368652194738388 = 0.023166052997112274 + 0.001 * 7.202599048614502
Epoch 500, val loss: 0.87990802526474
Epoch 510, training loss: 0.02863646112382412 = 0.021436069160699844 + 0.001 * 7.2003912925720215
Epoch 510, val loss: 0.8908443450927734
Epoch 520, training loss: 0.027084477245807648 = 0.019885912537574768 + 0.001 * 7.1985650062561035
Epoch 520, val loss: 0.9015225172042847
Epoch 530, training loss: 0.02568506821990013 = 0.018493033945560455 + 0.001 * 7.192033767700195
Epoch 530, val loss: 0.9119104146957397
Epoch 540, training loss: 0.024422537535429 = 0.017238082364201546 + 0.001 * 7.184454917907715
Epoch 540, val loss: 0.9220542311668396
Epoch 550, training loss: 0.023288574069738388 = 0.01610446907579899 + 0.001 * 7.184103488922119
Epoch 550, val loss: 0.9319027662277222
Epoch 560, training loss: 0.022259585559368134 = 0.015077908523380756 + 0.001 * 7.181676387786865
Epoch 560, val loss: 0.9415010809898376
Epoch 570, training loss: 0.02133793756365776 = 0.014145901426672935 + 0.001 * 7.192035675048828
Epoch 570, val loss: 0.9508199095726013
Epoch 580, training loss: 0.020478136837482452 = 0.013297238387167454 + 0.001 * 7.180899143218994
Epoch 580, val loss: 0.9598958492279053
Epoch 590, training loss: 0.01971280947327614 = 0.012521340511739254 + 0.001 * 7.191468238830566
Epoch 590, val loss: 0.9687368869781494
Epoch 600, training loss: 0.018989574164152145 = 0.011808101087808609 + 0.001 * 7.181473731994629
Epoch 600, val loss: 0.9773464798927307
Epoch 610, training loss: 0.018325496464967728 = 0.011149353347718716 + 0.001 * 7.176143169403076
Epoch 610, val loss: 0.9857538342475891
Epoch 620, training loss: 0.017746208235621452 = 0.010539384558796883 + 0.001 * 7.206822872161865
Epoch 620, val loss: 0.9939935803413391
Epoch 630, training loss: 0.01714484579861164 = 0.009973966516554356 + 0.001 * 7.170878887176514
Epoch 630, val loss: 1.0020421743392944
Epoch 640, training loss: 0.016619814559817314 = 0.0094496114179492 + 0.001 * 7.17020320892334
Epoch 640, val loss: 1.0099754333496094
Epoch 650, training loss: 0.01614455133676529 = 0.00896325334906578 + 0.001 * 7.181297779083252
Epoch 650, val loss: 1.0177303552627563
Epoch 660, training loss: 0.015683047473430634 = 0.008512054570019245 + 0.001 * 7.170993328094482
Epoch 660, val loss: 1.0253509283065796
Epoch 670, training loss: 0.015270760282874107 = 0.008093344047665596 + 0.001 * 7.1774163246154785
Epoch 670, val loss: 1.0328187942504883
Epoch 680, training loss: 0.014876328408718109 = 0.007704578805714846 + 0.001 * 7.171748638153076
Epoch 680, val loss: 1.0401272773742676
Epoch 690, training loss: 0.014503983780741692 = 0.007343295030295849 + 0.001 * 7.160688400268555
Epoch 690, val loss: 1.0472556352615356
Epoch 700, training loss: 0.014171319082379341 = 0.007007257547229528 + 0.001 * 7.164061546325684
Epoch 700, val loss: 1.0542141199111938
Epoch 710, training loss: 0.013877907767891884 = 0.0066943904384970665 + 0.001 * 7.183516979217529
Epoch 710, val loss: 1.0610551834106445
Epoch 720, training loss: 0.013568026944994926 = 0.006402764469385147 + 0.001 * 7.165262699127197
Epoch 720, val loss: 1.0676788091659546
Epoch 730, training loss: 0.013291845098137856 = 0.006130598019808531 + 0.001 * 7.1612467765808105
Epoch 730, val loss: 1.074163556098938
Epoch 740, training loss: 0.013038150034844875 = 0.005876273848116398 + 0.001 * 7.1618757247924805
Epoch 740, val loss: 1.0804851055145264
Epoch 750, training loss: 0.012794038280844688 = 0.00563836982473731 + 0.001 * 7.15566873550415
Epoch 750, val loss: 1.0866714715957642
Epoch 760, training loss: 0.01255844160914421 = 0.005415558349341154 + 0.001 * 7.142882347106934
Epoch 760, val loss: 1.0926786661148071
Epoch 770, training loss: 0.012351352721452713 = 0.005206610541790724 + 0.001 * 7.144741535186768
Epoch 770, val loss: 1.0985496044158936
Epoch 780, training loss: 0.01215645857155323 = 0.005010444670915604 + 0.001 * 7.146013259887695
Epoch 780, val loss: 1.1042588949203491
Epoch 790, training loss: 0.011984569951891899 = 0.004826084244996309 + 0.001 * 7.1584858894348145
Epoch 790, val loss: 1.1098487377166748
Epoch 800, training loss: 0.011799078434705734 = 0.0046525988727808 + 0.001 * 7.146478652954102
Epoch 800, val loss: 1.1152836084365845
Epoch 810, training loss: 0.011644793674349785 = 0.004489202983677387 + 0.001 * 7.155590534210205
Epoch 810, val loss: 1.12060546875
Epoch 820, training loss: 0.011474940925836563 = 0.004335109610110521 + 0.001 * 7.13983154296875
Epoch 820, val loss: 1.1257734298706055
Epoch 830, training loss: 0.011319294571876526 = 0.0041896565817296505 + 0.001 * 7.129637241363525
Epoch 830, val loss: 1.130818486213684
Epoch 840, training loss: 0.011193493381142616 = 0.004052172880619764 + 0.001 * 7.14132022857666
Epoch 840, val loss: 1.1357595920562744
Epoch 850, training loss: 0.011055467650294304 = 0.003922118805348873 + 0.001 * 7.1333489418029785
Epoch 850, val loss: 1.1405651569366455
Epoch 860, training loss: 0.010912827216088772 = 0.0037989995907992125 + 0.001 * 7.113827705383301
Epoch 860, val loss: 1.1452522277832031
Epoch 870, training loss: 0.010822225362062454 = 0.0036823078989982605 + 0.001 * 7.139917373657227
Epoch 870, val loss: 1.1498243808746338
Epoch 880, training loss: 0.010694642551243305 = 0.0035716283600777388 + 0.001 * 7.123013496398926
Epoch 880, val loss: 1.1542956829071045
Epoch 890, training loss: 0.010577769950032234 = 0.0034665423445403576 + 0.001 * 7.111227035522461
Epoch 890, val loss: 1.1586335897445679
Epoch 900, training loss: 0.010473925620317459 = 0.0033666943199932575 + 0.001 * 7.1072306632995605
Epoch 900, val loss: 1.162914752960205
Epoch 910, training loss: 0.01038547046482563 = 0.003271720604971051 + 0.001 * 7.113749980926514
Epoch 910, val loss: 1.1670596599578857
Epoch 920, training loss: 0.01029249094426632 = 0.003181299427524209 + 0.001 * 7.1111907958984375
Epoch 920, val loss: 1.1711426973342896
Epoch 930, training loss: 0.010208423249423504 = 0.003095207503065467 + 0.001 * 7.113215446472168
Epoch 930, val loss: 1.1751317977905273
Epoch 940, training loss: 0.010131532326340675 = 0.0030131423845887184 + 0.001 * 7.118389129638672
Epoch 940, val loss: 1.1790019273757935
Epoch 950, training loss: 0.010044225491583347 = 0.002934893826022744 + 0.001 * 7.1093316078186035
Epoch 950, val loss: 1.1828174591064453
Epoch 960, training loss: 0.009950202889740467 = 0.0028601738158613443 + 0.001 * 7.090028285980225
Epoch 960, val loss: 1.1865097284317017
Epoch 970, training loss: 0.0099180331453681 = 0.0027888170443475246 + 0.001 * 7.129215717315674
Epoch 970, val loss: 1.1901363134384155
Epoch 980, training loss: 0.009838084690272808 = 0.0027206416707485914 + 0.001 * 7.117442607879639
Epoch 980, val loss: 1.1937068700790405
Epoch 990, training loss: 0.009744415991008282 = 0.0026554090436547995 + 0.001 * 7.0890069007873535
Epoch 990, val loss: 1.1971664428710938
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8434370057986295
The final CL Acc:0.81728, 0.00349, The final GNN Acc:0.84010, 0.00237
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9460])
updated graph: torch.Size([2, 10514])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9519739151000977 = 1.9433770179748535 + 0.001 * 8.596853256225586
Epoch 0, val loss: 1.9324439764022827
Epoch 10, training loss: 1.941741704940796 = 1.9331449270248413 + 0.001 * 8.596784591674805
Epoch 10, val loss: 1.9228235483169556
Epoch 20, training loss: 1.9289230108261108 = 1.9203264713287354 + 0.001 * 8.596588134765625
Epoch 20, val loss: 1.9105050563812256
Epoch 30, training loss: 1.9110941886901855 = 1.9024981260299683 + 0.001 * 8.596113204956055
Epoch 30, val loss: 1.893189549446106
Epoch 40, training loss: 1.8855674266815186 = 1.8769725561141968 + 0.001 * 8.594922065734863
Epoch 40, val loss: 1.868557333946228
Epoch 50, training loss: 1.8514394760131836 = 1.8428481817245483 + 0.001 * 8.5912504196167
Epoch 50, val loss: 1.8373584747314453
Epoch 60, training loss: 1.815608263015747 = 1.8070310354232788 + 0.001 * 8.577199935913086
Epoch 60, val loss: 1.8091747760772705
Epoch 70, training loss: 1.7862467765808105 = 1.7777330875396729 + 0.001 * 8.513742446899414
Epoch 70, val loss: 1.7886401414871216
Epoch 80, training loss: 1.749153971672058 = 1.7409679889678955 + 0.001 * 8.185992240905762
Epoch 80, val loss: 1.7569465637207031
Epoch 90, training loss: 1.69710111618042 = 1.689028263092041 + 0.001 * 8.072822570800781
Epoch 90, val loss: 1.7112141847610474
Epoch 100, training loss: 1.6264104843139648 = 1.618396282196045 + 0.001 * 8.014199256896973
Epoch 100, val loss: 1.6521224975585938
Epoch 110, training loss: 1.542480707168579 = 1.5345637798309326 + 0.001 * 7.916952133178711
Epoch 110, val loss: 1.5849801301956177
Epoch 120, training loss: 1.4554404020309448 = 1.4476977586746216 + 0.001 * 7.742644786834717
Epoch 120, val loss: 1.5151933431625366
Epoch 130, training loss: 1.3683128356933594 = 1.360693335533142 + 0.001 * 7.619475364685059
Epoch 130, val loss: 1.4467359781265259
Epoch 140, training loss: 1.2795524597167969 = 1.2719910144805908 + 0.001 * 7.561484336853027
Epoch 140, val loss: 1.3784297704696655
Epoch 150, training loss: 1.1885799169540405 = 1.181107997894287 + 0.001 * 7.4719319343566895
Epoch 150, val loss: 1.3103671073913574
Epoch 160, training loss: 1.0979646444320679 = 1.0905534029006958 + 0.001 * 7.411231994628906
Epoch 160, val loss: 1.2446426153182983
Epoch 170, training loss: 1.0106804370880127 = 1.0033032894134521 + 0.001 * 7.377141952514648
Epoch 170, val loss: 1.183019995689392
Epoch 180, training loss: 0.9276416897773743 = 0.9202791452407837 + 0.001 * 7.362549781799316
Epoch 180, val loss: 1.1260056495666504
Epoch 190, training loss: 0.8481093645095825 = 0.8407551050186157 + 0.001 * 7.35426139831543
Epoch 190, val loss: 1.0728803873062134
Epoch 200, training loss: 0.7715610861778259 = 0.7642142176628113 + 0.001 * 7.346888542175293
Epoch 200, val loss: 1.0231435298919678
Epoch 210, training loss: 0.6988183856010437 = 0.6914771795272827 + 0.001 * 7.341179370880127
Epoch 210, val loss: 0.9772830605506897
Epoch 220, training loss: 0.631737232208252 = 0.6244008541107178 + 0.001 * 7.336396217346191
Epoch 220, val loss: 0.9373289942741394
Epoch 230, training loss: 0.5718185305595398 = 0.5644869208335876 + 0.001 * 7.33160400390625
Epoch 230, val loss: 0.9044639468193054
Epoch 240, training loss: 0.5194050073623657 = 0.5120792388916016 + 0.001 * 7.325765609741211
Epoch 240, val loss: 0.8796407580375671
Epoch 250, training loss: 0.47373658418655396 = 0.4664188027381897 + 0.001 * 7.317783832550049
Epoch 250, val loss: 0.8622213006019592
Epoch 260, training loss: 0.4337214231491089 = 0.42641592025756836 + 0.001 * 7.305506706237793
Epoch 260, val loss: 0.8514419198036194
Epoch 270, training loss: 0.39836612343788147 = 0.3910779058933258 + 0.001 * 7.288217067718506
Epoch 270, val loss: 0.8462546467781067
Epoch 280, training loss: 0.3668716549873352 = 0.35961151123046875 + 0.001 * 7.26015043258667
Epoch 280, val loss: 0.8455792665481567
Epoch 290, training loss: 0.3385377526283264 = 0.3313175439834595 + 0.001 * 7.220201015472412
Epoch 290, val loss: 0.8487345576286316
Epoch 300, training loss: 0.3127167820930481 = 0.30553925037384033 + 0.001 * 7.177525997161865
Epoch 300, val loss: 0.8548341989517212
Epoch 310, training loss: 0.2887798845767975 = 0.2816280424594879 + 0.001 * 7.151845932006836
Epoch 310, val loss: 0.863266110420227
Epoch 320, training loss: 0.2661416530609131 = 0.2589937746524811 + 0.001 * 7.147871017456055
Epoch 320, val loss: 0.8733050227165222
Epoch 330, training loss: 0.24411217868328094 = 0.23698806762695312 + 0.001 * 7.124104976654053
Epoch 330, val loss: 0.8843650221824646
Epoch 340, training loss: 0.22211527824401855 = 0.21499688923358917 + 0.001 * 7.118389129638672
Epoch 340, val loss: 0.8957968950271606
Epoch 350, training loss: 0.19979959726333618 = 0.19268499314785004 + 0.001 * 7.114598274230957
Epoch 350, val loss: 0.9069343209266663
Epoch 360, training loss: 0.1774516999721527 = 0.17034144699573517 + 0.001 * 7.110247611999512
Epoch 360, val loss: 0.9175737500190735
Epoch 370, training loss: 0.15594203770160675 = 0.14883357286453247 + 0.001 * 7.10846471786499
Epoch 370, val loss: 0.9283968806266785
Epoch 380, training loss: 0.13622528314590454 = 0.1291198581457138 + 0.001 * 7.105429649353027
Epoch 380, val loss: 0.9409492015838623
Epoch 390, training loss: 0.11886607855558395 = 0.11176187545061111 + 0.001 * 7.1042022705078125
Epoch 390, val loss: 0.955632746219635
Epoch 400, training loss: 0.10401991009712219 = 0.09691555798053741 + 0.001 * 7.1043548583984375
Epoch 400, val loss: 0.9722850918769836
Epoch 410, training loss: 0.09154010564088821 = 0.08443228155374527 + 0.001 * 7.107827186584473
Epoch 410, val loss: 0.9906256794929504
Epoch 420, training loss: 0.0810801163315773 = 0.0739753320813179 + 0.001 * 7.1047821044921875
Epoch 420, val loss: 1.0101126432418823
Epoch 430, training loss: 0.07228900492191315 = 0.0651833638548851 + 0.001 * 7.105642318725586
Epoch 430, val loss: 1.0304133892059326
Epoch 440, training loss: 0.06485148519277573 = 0.05775020644068718 + 0.001 * 7.101278305053711
Epoch 440, val loss: 1.0512079000473022
Epoch 450, training loss: 0.05853227525949478 = 0.05143143981695175 + 0.001 * 7.100833892822266
Epoch 450, val loss: 1.0721606016159058
Epoch 460, training loss: 0.05313779413700104 = 0.04602950066328049 + 0.001 * 7.108293056488037
Epoch 460, val loss: 1.0930994749069214
Epoch 470, training loss: 0.04848361387848854 = 0.041384439915418625 + 0.001 * 7.099172115325928
Epoch 470, val loss: 1.1138368844985962
Epoch 480, training loss: 0.04446786269545555 = 0.037368837743997574 + 0.001 * 7.099023818969727
Epoch 480, val loss: 1.1342359781265259
Epoch 490, training loss: 0.040978651493787766 = 0.03388076275587082 + 0.001 * 7.09788703918457
Epoch 490, val loss: 1.1542006731033325
Epoch 500, training loss: 0.037938397377729416 = 0.030837977305054665 + 0.001 * 7.100419044494629
Epoch 500, val loss: 1.1736217737197876
Epoch 510, training loss: 0.0352720245718956 = 0.028171803802251816 + 0.001 * 7.100219249725342
Epoch 510, val loss: 1.1924813985824585
Epoch 520, training loss: 0.03293154761195183 = 0.025825735181570053 + 0.001 * 7.105813503265381
Epoch 520, val loss: 1.2107510566711426
Epoch 530, training loss: 0.03085116297006607 = 0.023753182962536812 + 0.001 * 7.0979790687561035
Epoch 530, val loss: 1.2283811569213867
Epoch 540, training loss: 0.029012007638812065 = 0.021915310993790627 + 0.001 * 7.096695899963379
Epoch 540, val loss: 1.2454630136489868
Epoch 550, training loss: 0.02737465314567089 = 0.0202795397490263 + 0.001 * 7.0951128005981445
Epoch 550, val loss: 1.261973261833191
Epoch 560, training loss: 0.02591715380549431 = 0.018818628042936325 + 0.001 * 7.0985260009765625
Epoch 560, val loss: 1.277928352355957
Epoch 570, training loss: 0.024607185274362564 = 0.017510119825601578 + 0.001 * 7.097064018249512
Epoch 570, val loss: 1.2933313846588135
Epoch 580, training loss: 0.023429032415151596 = 0.016334088519215584 + 0.001 * 7.094943523406982
Epoch 580, val loss: 1.308174967765808
Epoch 590, training loss: 0.02236894890666008 = 0.015273679047822952 + 0.001 * 7.095270156860352
Epoch 590, val loss: 1.3225314617156982
Epoch 600, training loss: 0.021411076188087463 = 0.014314440079033375 + 0.001 * 7.096634864807129
Epoch 600, val loss: 1.3364053964614868
Epoch 610, training loss: 0.020539220422506332 = 0.013444118201732635 + 0.001 * 7.095102787017822
Epoch 610, val loss: 1.3498201370239258
Epoch 620, training loss: 0.019745156168937683 = 0.012652506120502949 + 0.001 * 7.092649459838867
Epoch 620, val loss: 1.3628084659576416
Epoch 630, training loss: 0.019027646631002426 = 0.01193056907504797 + 0.001 * 7.097076416015625
Epoch 630, val loss: 1.375365972518921
Epoch 640, training loss: 0.018360109999775887 = 0.011270481161773205 + 0.001 * 7.08962869644165
Epoch 640, val loss: 1.3875120878219604
Epoch 650, training loss: 0.01775556057691574 = 0.010665509849786758 + 0.001 * 7.0900492668151855
Epoch 650, val loss: 1.399282455444336
Epoch 660, training loss: 0.017199434340000153 = 0.010109815746545792 + 0.001 * 7.089617729187012
Epoch 660, val loss: 1.4106874465942383
Epoch 670, training loss: 0.016683686524629593 = 0.009598294273018837 + 0.001 * 7.085391998291016
Epoch 670, val loss: 1.4217498302459717
Epoch 680, training loss: 0.016217054799199104 = 0.009126350283622742 + 0.001 * 7.09070348739624
Epoch 680, val loss: 1.4324841499328613
Epoch 690, training loss: 0.015779219567775726 = 0.0086900619789958 + 0.001 * 7.0891571044921875
Epoch 690, val loss: 1.4428808689117432
Epoch 700, training loss: 0.015368521213531494 = 0.008285998366773129 + 0.001 * 7.082522392272949
Epoch 700, val loss: 1.4529926776885986
Epoch 710, training loss: 0.014997981488704681 = 0.007911093533039093 + 0.001 * 7.086887836456299
Epoch 710, val loss: 1.4627951383590698
Epoch 720, training loss: 0.014645710587501526 = 0.00756262568756938 + 0.001 * 7.083085060119629
Epoch 720, val loss: 1.4723312854766846
Epoch 730, training loss: 0.014319784939289093 = 0.007238151039928198 + 0.001 * 7.081633567810059
Epoch 730, val loss: 1.4816066026687622
Epoch 740, training loss: 0.014013586565852165 = 0.006935160607099533 + 0.001 * 7.07842493057251
Epoch 740, val loss: 1.4906467199325562
Epoch 750, training loss: 0.013743988238275051 = 0.006650769151747227 + 0.001 * 7.093218803405762
Epoch 750, val loss: 1.4994689226150513
Epoch 760, training loss: 0.013462561182677746 = 0.006382211111485958 + 0.001 * 7.080349922180176
Epoch 760, val loss: 1.5081104040145874
Epoch 770, training loss: 0.013197895139455795 = 0.006127373315393925 + 0.001 * 7.070521354675293
Epoch 770, val loss: 1.516645073890686
Epoch 780, training loss: 0.012976689264178276 = 0.005885026417672634 + 0.001 * 7.091662406921387
Epoch 780, val loss: 1.5250773429870605
Epoch 790, training loss: 0.012727417051792145 = 0.0056546409614384174 + 0.001 * 7.0727763175964355
Epoch 790, val loss: 1.5334439277648926
Epoch 800, training loss: 0.01250770129263401 = 0.005435940343886614 + 0.001 * 7.071761131286621
Epoch 800, val loss: 1.5416814088821411
Epoch 810, training loss: 0.012297130189836025 = 0.005228652153164148 + 0.001 * 7.068477630615234
Epoch 810, val loss: 1.5498021841049194
Epoch 820, training loss: 0.01210179552435875 = 0.005032403394579887 + 0.001 * 7.069392204284668
Epoch 820, val loss: 1.5577889680862427
Epoch 830, training loss: 0.011927880346775055 = 0.004846708849072456 + 0.001 * 7.08117151260376
Epoch 830, val loss: 1.5656377077102661
Epoch 840, training loss: 0.011731332167983055 = 0.00467101251706481 + 0.001 * 7.060318946838379
Epoch 840, val loss: 1.5733563899993896
Epoch 850, training loss: 0.011576512828469276 = 0.00450481241568923 + 0.001 * 7.071700572967529
Epoch 850, val loss: 1.5809288024902344
Epoch 860, training loss: 0.011420220136642456 = 0.004347627516835928 + 0.001 * 7.072592735290527
Epoch 860, val loss: 1.5883678197860718
Epoch 870, training loss: 0.011272529140114784 = 0.004198913928121328 + 0.001 * 7.073615550994873
Epoch 870, val loss: 1.5956422090530396
Epoch 880, training loss: 0.011119641363620758 = 0.004058170132339001 + 0.001 * 7.0614705085754395
Epoch 880, val loss: 1.602762222290039
Epoch 890, training loss: 0.01097590196877718 = 0.003924855031073093 + 0.001 * 7.051046371459961
Epoch 890, val loss: 1.6097691059112549
Epoch 900, training loss: 0.010890322737395763 = 0.0037985239177942276 + 0.001 * 7.091798305511475
Epoch 900, val loss: 1.616591453552246
Epoch 910, training loss: 0.01075257919728756 = 0.0036787535063922405 + 0.001 * 7.073824882507324
Epoch 910, val loss: 1.6232866048812866
Epoch 920, training loss: 0.010623331181704998 = 0.003565108170732856 + 0.001 * 7.05822229385376
Epoch 920, val loss: 1.6298307180404663
Epoch 930, training loss: 0.0105219054967165 = 0.00345721747726202 + 0.001 * 7.064687252044678
Epoch 930, val loss: 1.6362324953079224
Epoch 940, training loss: 0.010401792824268341 = 0.0033547780476510525 + 0.001 * 7.047014236450195
Epoch 940, val loss: 1.6425113677978516
Epoch 950, training loss: 0.010311916470527649 = 0.0032574061769992113 + 0.001 * 7.054510116577148
Epoch 950, val loss: 1.648640751838684
Epoch 960, training loss: 0.010214593261480331 = 0.003164788708090782 + 0.001 * 7.0498046875
Epoch 960, val loss: 1.654639482498169
Epoch 970, training loss: 0.010153881274163723 = 0.003076637629419565 + 0.001 * 7.077243328094482
Epoch 970, val loss: 1.6604859828948975
Epoch 980, training loss: 0.01003999076783657 = 0.002992683555930853 + 0.001 * 7.04730749130249
Epoch 980, val loss: 1.666236400604248
Epoch 990, training loss: 0.009951794520020485 = 0.0029126766603440046 + 0.001 * 7.039117336273193
Epoch 990, val loss: 1.671839952468872
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 1.9738060235977173 = 1.9652092456817627 + 0.001 * 8.596738815307617
Epoch 0, val loss: 1.9684231281280518
Epoch 10, training loss: 1.9634042978286743 = 1.9548076391220093 + 0.001 * 8.5966796875
Epoch 10, val loss: 1.95834219455719
Epoch 20, training loss: 1.9507684707641602 = 1.9421720504760742 + 0.001 * 8.596402168273926
Epoch 20, val loss: 1.9455162286758423
Epoch 30, training loss: 1.9330739974975586 = 1.9244784116744995 + 0.001 * 8.595633506774902
Epoch 30, val loss: 1.9270871877670288
Epoch 40, training loss: 1.9069510698318481 = 1.898357629776001 + 0.001 * 8.5933837890625
Epoch 40, val loss: 1.8998336791992188
Epoch 50, training loss: 1.8699833154678345 = 1.8613979816436768 + 0.001 * 8.585349082946777
Epoch 50, val loss: 1.8626257181167603
Epoch 60, training loss: 1.8276808261871338 = 1.8191286325454712 + 0.001 * 8.552145957946777
Epoch 60, val loss: 1.8239647150039673
Epoch 70, training loss: 1.7949888706207275 = 1.7866131067276 + 0.001 * 8.37571907043457
Epoch 70, val loss: 1.797831416130066
Epoch 80, training loss: 1.762669324874878 = 1.7545439004898071 + 0.001 * 8.125432014465332
Epoch 80, val loss: 1.768972396850586
Epoch 90, training loss: 1.718570351600647 = 1.710680603981018 + 0.001 * 7.889743328094482
Epoch 90, val loss: 1.7307368516921997
Epoch 100, training loss: 1.6575554609298706 = 1.649896502494812 + 0.001 * 7.658941745758057
Epoch 100, val loss: 1.6794276237487793
Epoch 110, training loss: 1.57703697681427 = 1.5695821046829224 + 0.001 * 7.454867362976074
Epoch 110, val loss: 1.6118724346160889
Epoch 120, training loss: 1.4843157529830933 = 1.4769643545150757 + 0.001 * 7.351409435272217
Epoch 120, val loss: 1.536088228225708
Epoch 130, training loss: 1.3893057107925415 = 1.3819990158081055 + 0.001 * 7.306698322296143
Epoch 130, val loss: 1.458714485168457
Epoch 140, training loss: 1.2964756488800049 = 1.2891956567764282 + 0.001 * 7.2800092697143555
Epoch 140, val loss: 1.3849927186965942
Epoch 150, training loss: 1.2048498392105103 = 1.1975799798965454 + 0.001 * 7.269907474517822
Epoch 150, val loss: 1.3146307468414307
Epoch 160, training loss: 1.1134666204452515 = 1.1062034368515015 + 0.001 * 7.263230323791504
Epoch 160, val loss: 1.2458184957504272
Epoch 170, training loss: 1.0228132009506226 = 1.0155547857284546 + 0.001 * 7.258427619934082
Epoch 170, val loss: 1.177809476852417
Epoch 180, training loss: 0.9347233176231384 = 0.927471399307251 + 0.001 * 7.251922607421875
Epoch 180, val loss: 1.1125775575637817
Epoch 190, training loss: 0.8521550893783569 = 0.8449139595031738 + 0.001 * 7.24111795425415
Epoch 190, val loss: 1.0515081882476807
Epoch 200, training loss: 0.7775315046310425 = 0.7703092098236084 + 0.001 * 7.222280502319336
Epoch 200, val loss: 0.9973747134208679
Epoch 210, training loss: 0.7113717198371887 = 0.7041813135147095 + 0.001 * 7.1903862953186035
Epoch 210, val loss: 0.9513056874275208
Epoch 220, training loss: 0.6523497700691223 = 0.645196795463562 + 0.001 * 7.152979850769043
Epoch 220, val loss: 0.9124636054039001
Epoch 230, training loss: 0.5981123447418213 = 0.5909909605979919 + 0.001 * 7.12136697769165
Epoch 230, val loss: 0.8795270323753357
Epoch 240, training loss: 0.546739399433136 = 0.5396453738212585 + 0.001 * 7.094044208526611
Epoch 240, val loss: 0.8510708808898926
Epoch 250, training loss: 0.4971977174282074 = 0.49011749029159546 + 0.001 * 7.080230712890625
Epoch 250, val loss: 0.8262825012207031
Epoch 260, training loss: 0.4491136074066162 = 0.4420431852340698 + 0.001 * 7.070426940917969
Epoch 260, val loss: 0.8049713969230652
Epoch 270, training loss: 0.4025203287601471 = 0.3954598605632782 + 0.001 * 7.0604658126831055
Epoch 270, val loss: 0.7869459986686707
Epoch 280, training loss: 0.35780268907546997 = 0.35074910521507263 + 0.001 * 7.053577899932861
Epoch 280, val loss: 0.7722651362419128
Epoch 290, training loss: 0.3155962824821472 = 0.30855095386505127 + 0.001 * 7.045317649841309
Epoch 290, val loss: 0.7610867023468018
Epoch 300, training loss: 0.2765912413597107 = 0.2695578634738922 + 0.001 * 7.033371925354004
Epoch 300, val loss: 0.7537810802459717
Epoch 310, training loss: 0.24123618006706238 = 0.2342136800289154 + 0.001 * 7.022495269775391
Epoch 310, val loss: 0.7502909302711487
Epoch 320, training loss: 0.20968647301197052 = 0.20265595614910126 + 0.001 * 7.030516147613525
Epoch 320, val loss: 0.750592052936554
Epoch 330, training loss: 0.18178296089172363 = 0.1747763305902481 + 0.001 * 7.006630897521973
Epoch 330, val loss: 0.7543756365776062
Epoch 340, training loss: 0.1573774218559265 = 0.1503671109676361 + 0.001 * 7.010313510894775
Epoch 340, val loss: 0.7612278461456299
Epoch 350, training loss: 0.13617144525051117 = 0.12917132675647736 + 0.001 * 7.000115871429443
Epoch 350, val loss: 0.7706775665283203
Epoch 360, training loss: 0.11790122091770172 = 0.11090531200170517 + 0.001 * 6.99591064453125
Epoch 360, val loss: 0.7822238206863403
Epoch 370, training loss: 0.10228259116411209 = 0.09529087692499161 + 0.001 * 6.991714000701904
Epoch 370, val loss: 0.795330822467804
Epoch 380, training loss: 0.0890251100063324 = 0.08203891664743423 + 0.001 * 6.986191749572754
Epoch 380, val loss: 0.8095428347587585
Epoch 390, training loss: 0.0778433158993721 = 0.07085959613323212 + 0.001 * 6.983719825744629
Epoch 390, val loss: 0.8244102001190186
Epoch 400, training loss: 0.06844565272331238 = 0.061468806117773056 + 0.001 * 6.976842403411865
Epoch 400, val loss: 0.8395757079124451
Epoch 410, training loss: 0.06058535352349281 = 0.05359705165028572 + 0.001 * 6.988300800323486
Epoch 410, val loss: 0.8547890186309814
Epoch 420, training loss: 0.05397234112024307 = 0.04699857160449028 + 0.001 * 6.973771095275879
Epoch 420, val loss: 0.8698532581329346
Epoch 430, training loss: 0.048448771238327026 = 0.04145576059818268 + 0.001 * 6.993010520935059
Epoch 430, val loss: 0.8846073746681213
Epoch 440, training loss: 0.04375027120113373 = 0.036783408373594284 + 0.001 * 6.96686315536499
Epoch 440, val loss: 0.8989622592926025
Epoch 450, training loss: 0.03979210928082466 = 0.03282633423805237 + 0.001 * 6.965774059295654
Epoch 450, val loss: 0.912876307964325
Epoch 460, training loss: 0.036438293755054474 = 0.02945759706199169 + 0.001 * 6.980698108673096
Epoch 460, val loss: 0.9262833595275879
Epoch 470, training loss: 0.03353358432650566 = 0.026574214920401573 + 0.001 * 6.95936918258667
Epoch 470, val loss: 0.9392348527908325
Epoch 480, training loss: 0.031080856919288635 = 0.024092068895697594 + 0.001 * 6.988787651062012
Epoch 480, val loss: 0.9517311453819275
Epoch 490, training loss: 0.02890438586473465 = 0.021943258121609688 + 0.001 * 6.961126327514648
Epoch 490, val loss: 0.9637852311134338
Epoch 500, training loss: 0.0270462054759264 = 0.020072635263204575 + 0.001 * 6.973569393157959
Epoch 500, val loss: 0.9753957986831665
Epoch 510, training loss: 0.025392046198248863 = 0.018435772508382797 + 0.001 * 6.956273078918457
Epoch 510, val loss: 0.9866119623184204
Epoch 520, training loss: 0.02394961193203926 = 0.016996150836348534 + 0.001 * 6.953459739685059
Epoch 520, val loss: 0.9973883628845215
Epoch 530, training loss: 0.022718487307429314 = 0.015723656862974167 + 0.001 * 6.99483060836792
Epoch 530, val loss: 1.00775945186615
Epoch 540, training loss: 0.021546224132180214 = 0.01459368783980608 + 0.001 * 6.952535629272461
Epoch 540, val loss: 1.0177724361419678
Epoch 550, training loss: 0.020541783422231674 = 0.013585571199655533 + 0.001 * 6.956212520599365
Epoch 550, val loss: 1.0274336338043213
Epoch 560, training loss: 0.019665835425257683 = 0.012681970372796059 + 0.001 * 6.9838643074035645
Epoch 560, val loss: 1.036752700805664
Epoch 570, training loss: 0.01881331019103527 = 0.011868156492710114 + 0.001 * 6.945152759552002
Epoch 570, val loss: 1.0457475185394287
Epoch 580, training loss: 0.01808284968137741 = 0.011131524108350277 + 0.001 * 6.9513258934021
Epoch 580, val loss: 1.0544179677963257
Epoch 590, training loss: 0.017407840117812157 = 0.010457661934196949 + 0.001 * 6.9501776695251465
Epoch 590, val loss: 1.0629327297210693
Epoch 600, training loss: 0.01678517833352089 = 0.009843230247497559 + 0.001 * 6.941948413848877
Epoch 600, val loss: 1.0712026357650757
Epoch 610, training loss: 0.01622113212943077 = 0.009279631078243256 + 0.001 * 6.941499710083008
Epoch 610, val loss: 1.0792396068572998
Epoch 620, training loss: 0.01569845899939537 = 0.008762502111494541 + 0.001 * 6.9359564781188965
Epoch 620, val loss: 1.0870811939239502
Epoch 630, training loss: 0.015221888199448586 = 0.00828712247312069 + 0.001 * 6.934764862060547
Epoch 630, val loss: 1.0947010517120361
Epoch 640, training loss: 0.014798423275351524 = 0.007849632762372494 + 0.001 * 6.948790073394775
Epoch 640, val loss: 1.1021281480789185
Epoch 650, training loss: 0.014381080865859985 = 0.007446494419127703 + 0.001 * 6.934586524963379
Epoch 650, val loss: 1.1093610525131226
Epoch 660, training loss: 0.014019913971424103 = 0.007074417546391487 + 0.001 * 6.945496082305908
Epoch 660, val loss: 1.1164051294326782
Epoch 670, training loss: 0.013658175244927406 = 0.0067305113188922405 + 0.001 * 6.927663326263428
Epoch 670, val loss: 1.123297929763794
Epoch 680, training loss: 0.01334662176668644 = 0.006412137299776077 + 0.001 * 6.934483528137207
Epoch 680, val loss: 1.1300036907196045
Epoch 690, training loss: 0.013046275824308395 = 0.006117061711847782 + 0.001 * 6.929214000701904
Epoch 690, val loss: 1.1364951133728027
Epoch 700, training loss: 0.012788023799657822 = 0.005843163933604956 + 0.001 * 6.944859504699707
Epoch 700, val loss: 1.1428401470184326
Epoch 710, training loss: 0.012523451820015907 = 0.00558848911896348 + 0.001 * 6.934962749481201
Epoch 710, val loss: 1.1490010023117065
Epoch 720, training loss: 0.012276002205908298 = 0.005351340398192406 + 0.001 * 6.924661636352539
Epoch 720, val loss: 1.1550369262695312
Epoch 730, training loss: 0.01205924991518259 = 0.005130172707140446 + 0.001 * 6.929076671600342
Epoch 730, val loss: 1.1609151363372803
Epoch 740, training loss: 0.011848725378513336 = 0.004923669155687094 + 0.001 * 6.925055503845215
Epoch 740, val loss: 1.1666560173034668
Epoch 750, training loss: 0.011653367429971695 = 0.004730563145130873 + 0.001 * 6.92280387878418
Epoch 750, val loss: 1.1722323894500732
Epoch 760, training loss: 0.011471176519989967 = 0.004549724515527487 + 0.001 * 6.921451568603516
Epoch 760, val loss: 1.177678108215332
Epoch 770, training loss: 0.011303860694169998 = 0.004380119498819113 + 0.001 * 6.923740863800049
Epoch 770, val loss: 1.1829880475997925
Epoch 780, training loss: 0.011140411719679832 = 0.004220863804221153 + 0.001 * 6.9195475578308105
Epoch 780, val loss: 1.1881605386734009
Epoch 790, training loss: 0.010986411944031715 = 0.004071156494319439 + 0.001 * 6.915255069732666
Epoch 790, val loss: 1.193210482597351
Epoch 800, training loss: 0.010845284909009933 = 0.003930248320102692 + 0.001 * 6.915036201477051
Epoch 800, val loss: 1.1981405019760132
Epoch 810, training loss: 0.010715244337916374 = 0.0037974505685269833 + 0.001 * 6.917793273925781
Epoch 810, val loss: 1.2029579877853394
Epoch 820, training loss: 0.010585416108369827 = 0.003672173246741295 + 0.001 * 6.913242816925049
Epoch 820, val loss: 1.2076537609100342
Epoch 830, training loss: 0.010483510792255402 = 0.003553856397047639 + 0.001 * 6.929654121398926
Epoch 830, val loss: 1.2122315168380737
Epoch 840, training loss: 0.010362565517425537 = 0.003442001063376665 + 0.001 * 6.9205641746521
Epoch 840, val loss: 1.216719388961792
Epoch 850, training loss: 0.010245014913380146 = 0.0033361136447638273 + 0.001 * 6.908901214599609
Epoch 850, val loss: 1.2210922241210938
Epoch 860, training loss: 0.010143320076167583 = 0.003235791577026248 + 0.001 * 6.907527923583984
Epoch 860, val loss: 1.2253543138504028
Epoch 870, training loss: 0.010048426687717438 = 0.003140651620924473 + 0.001 * 6.907774448394775
Epoch 870, val loss: 1.2295269966125488
Epoch 880, training loss: 0.00997835025191307 = 0.0030503461603075266 + 0.001 * 6.928003787994385
Epoch 880, val loss: 1.2335902452468872
Epoch 890, training loss: 0.009867624379694462 = 0.002964543178677559 + 0.001 * 6.903080940246582
Epoch 890, val loss: 1.2375911474227905
Epoch 900, training loss: 0.009809081442654133 = 0.002882949309423566 + 0.001 * 6.926131725311279
Epoch 900, val loss: 1.2414857149124146
Epoch 910, training loss: 0.009715603664517403 = 0.002805308671668172 + 0.001 * 6.910294532775879
Epoch 910, val loss: 1.2453045845031738
Epoch 920, training loss: 0.009639676660299301 = 0.0027313553728163242 + 0.001 * 6.908321380615234
Epoch 920, val loss: 1.249031901359558
Epoch 930, training loss: 0.009563906118273735 = 0.002660865429788828 + 0.001 * 6.903040409088135
Epoch 930, val loss: 1.2526825666427612
Epoch 940, training loss: 0.00949908047914505 = 0.0025936090387403965 + 0.001 * 6.905470848083496
Epoch 940, val loss: 1.256245493888855
Epoch 950, training loss: 0.009440919384360313 = 0.0025294071529060602 + 0.001 * 6.9115118980407715
Epoch 950, val loss: 1.25971257686615
Epoch 960, training loss: 0.009371928870677948 = 0.0024680709466338158 + 0.001 * 6.903857707977295
Epoch 960, val loss: 1.2631149291992188
Epoch 970, training loss: 0.009304164908826351 = 0.002409437671303749 + 0.001 * 6.894726753234863
Epoch 970, val loss: 1.2664368152618408
Epoch 980, training loss: 0.009253822267055511 = 0.0023533455096185207 + 0.001 * 6.900476455688477
Epoch 980, val loss: 1.2696818113327026
Epoch 990, training loss: 0.009205672889947891 = 0.002299631480127573 + 0.001 * 6.906040668487549
Epoch 990, val loss: 1.2728663682937622
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 1.9483625888824463 = 1.9397658109664917 + 0.001 * 8.596782684326172
Epoch 0, val loss: 1.9486134052276611
Epoch 10, training loss: 1.938369870185852 = 1.929773211479187 + 0.001 * 8.596689224243164
Epoch 10, val loss: 1.9378868341445923
Epoch 20, training loss: 1.9260048866271973 = 1.9174084663391113 + 0.001 * 8.596418380737305
Epoch 20, val loss: 1.9243749380111694
Epoch 30, training loss: 1.9087203741073608 = 1.9001245498657227 + 0.001 * 8.595819473266602
Epoch 30, val loss: 1.9054332971572876
Epoch 40, training loss: 1.8837425708770752 = 1.8751481771469116 + 0.001 * 8.594389915466309
Epoch 40, val loss: 1.878544807434082
Epoch 50, training loss: 1.8508750200271606 = 1.8422847986221313 + 0.001 * 8.590218544006348
Epoch 50, val loss: 1.8451043367385864
Epoch 60, training loss: 1.8182293176651 = 1.8096550703048706 + 0.001 * 8.574240684509277
Epoch 60, val loss: 1.8158693313598633
Epoch 70, training loss: 1.791294813156128 = 1.782819390296936 + 0.001 * 8.47536849975586
Epoch 70, val loss: 1.7932922840118408
Epoch 80, training loss: 1.7563663721084595 = 1.748319387435913 + 0.001 * 8.047002792358398
Epoch 80, val loss: 1.762444257736206
Epoch 90, training loss: 1.7079640626907349 = 1.7001129388809204 + 0.001 * 7.851118564605713
Epoch 90, val loss: 1.7207685708999634
Epoch 100, training loss: 1.6404907703399658 = 1.6328798532485962 + 0.001 * 7.6109747886657715
Epoch 100, val loss: 1.66454017162323
Epoch 110, training loss: 1.557615876197815 = 1.5501266717910767 + 0.001 * 7.48917818069458
Epoch 110, val loss: 1.596351981163025
Epoch 120, training loss: 1.4662766456604004 = 1.458816409111023 + 0.001 * 7.460266590118408
Epoch 120, val loss: 1.521450400352478
Epoch 130, training loss: 1.3696907758712769 = 1.3622894287109375 + 0.001 * 7.401309967041016
Epoch 130, val loss: 1.4433642625808716
Epoch 140, training loss: 1.266469120979309 = 1.259149432182312 + 0.001 * 7.319638729095459
Epoch 140, val loss: 1.3601188659667969
Epoch 150, training loss: 1.15700364112854 = 1.1497278213500977 + 0.001 * 7.275794982910156
Epoch 150, val loss: 1.2730610370635986
Epoch 160, training loss: 1.045922040939331 = 1.038651466369629 + 0.001 * 7.270553112030029
Epoch 160, val loss: 1.186299204826355
Epoch 170, training loss: 0.9408520460128784 = 0.9335896968841553 + 0.001 * 7.262351989746094
Epoch 170, val loss: 1.1062411069869995
Epoch 180, training loss: 0.8477748036384583 = 0.840519905090332 + 0.001 * 7.25489616394043
Epoch 180, val loss: 1.038102626800537
Epoch 190, training loss: 0.7681604623794556 = 0.760918140411377 + 0.001 * 7.242331504821777
Epoch 190, val loss: 0.9839118123054504
Epoch 200, training loss: 0.6996381878852844 = 0.6924149990081787 + 0.001 * 7.223211288452148
Epoch 200, val loss: 0.9422317743301392
Epoch 210, training loss: 0.6387072801589966 = 0.6315100193023682 + 0.001 * 7.197247505187988
Epoch 210, val loss: 0.9103071689605713
Epoch 220, training loss: 0.5825519561767578 = 0.5753747820854187 + 0.001 * 7.177187442779541
Epoch 220, val loss: 0.885312557220459
Epoch 230, training loss: 0.5294947028160095 = 0.5223468542098999 + 0.001 * 7.147832870483398
Epoch 230, val loss: 0.8651044964790344
Epoch 240, training loss: 0.47884801030158997 = 0.47172024846076965 + 0.001 * 7.127774238586426
Epoch 240, val loss: 0.8488466143608093
Epoch 250, training loss: 0.43078452348709106 = 0.42366400361061096 + 0.001 * 7.120513916015625
Epoch 250, val loss: 0.8373357057571411
Epoch 260, training loss: 0.3858228027820587 = 0.37871167063713074 + 0.001 * 7.111124515533447
Epoch 260, val loss: 0.8312509059906006
Epoch 270, training loss: 0.34444573521614075 = 0.33733823895454407 + 0.001 * 7.107504844665527
Epoch 270, val loss: 0.830374538898468
Epoch 280, training loss: 0.30687272548675537 = 0.2997685372829437 + 0.001 * 7.104188919067383
Epoch 280, val loss: 0.8344094157218933
Epoch 290, training loss: 0.27297109365463257 = 0.265855610370636 + 0.001 * 7.115478038787842
Epoch 290, val loss: 0.8426254987716675
Epoch 300, training loss: 0.24245911836624146 = 0.23535828292369843 + 0.001 * 7.100842475891113
Epoch 300, val loss: 0.8542565107345581
Epoch 310, training loss: 0.2150873988866806 = 0.20799386501312256 + 0.001 * 7.0935282707214355
Epoch 310, val loss: 0.8686079382896423
Epoch 320, training loss: 0.19058482348918915 = 0.18349464237689972 + 0.001 * 7.090179920196533
Epoch 320, val loss: 0.8850434422492981
Epoch 330, training loss: 0.16876456141471863 = 0.16165921092033386 + 0.001 * 7.1053466796875
Epoch 330, val loss: 0.9031506776809692
Epoch 340, training loss: 0.14939819276332855 = 0.1423109769821167 + 0.001 * 7.087215423583984
Epoch 340, val loss: 0.9226112365722656
Epoch 350, training loss: 0.13234926760196686 = 0.12526662647724152 + 0.001 * 7.082639694213867
Epoch 350, val loss: 0.9431390166282654
Epoch 360, training loss: 0.11740583926439285 = 0.1103261336684227 + 0.001 * 7.079705238342285
Epoch 360, val loss: 0.9644725918769836
Epoch 370, training loss: 0.10436663031578064 = 0.09728875011205673 + 0.001 * 7.077876567840576
Epoch 370, val loss: 0.9863715171813965
Epoch 380, training loss: 0.0930282324552536 = 0.08594252169132233 + 0.001 * 7.085708141326904
Epoch 380, val loss: 1.0086544752120972
Epoch 390, training loss: 0.08316302299499512 = 0.07608982175588608 + 0.001 * 7.073201656341553
Epoch 390, val loss: 1.031085729598999
Epoch 400, training loss: 0.07461898028850555 = 0.06754660606384277 + 0.001 * 7.072372913360596
Epoch 400, val loss: 1.0534292459487915
Epoch 410, training loss: 0.0672115907073021 = 0.060140036046504974 + 0.001 * 7.071551322937012
Epoch 410, val loss: 1.0755609273910522
Epoch 420, training loss: 0.06079886481165886 = 0.05371856689453125 + 0.001 * 7.080296516418457
Epoch 420, val loss: 1.097499966621399
Epoch 430, training loss: 0.055218301713466644 = 0.0481453500688076 + 0.001 * 7.072950839996338
Epoch 430, val loss: 1.118932843208313
Epoch 440, training loss: 0.05037284269928932 = 0.043302059173583984 + 0.001 * 7.070784091949463
Epoch 440, val loss: 1.1398990154266357
Epoch 450, training loss: 0.04615195840597153 = 0.039084456861019135 + 0.001 * 7.067501068115234
Epoch 450, val loss: 1.1602284908294678
Epoch 460, training loss: 0.04247134551405907 = 0.03540375828742981 + 0.001 * 7.0675883293151855
Epoch 460, val loss: 1.179978847503662
Epoch 470, training loss: 0.03924836218357086 = 0.03218361735343933 + 0.001 * 7.064744472503662
Epoch 470, val loss: 1.1991395950317383
Epoch 480, training loss: 0.036426544189453125 = 0.029358329251408577 + 0.001 * 7.068214416503906
Epoch 480, val loss: 1.217690348625183
Epoch 490, training loss: 0.03393424674868584 = 0.02687215246260166 + 0.001 * 7.062093734741211
Epoch 490, val loss: 1.235636830329895
Epoch 500, training loss: 0.031751710921525955 = 0.024677449837327003 + 0.001 * 7.074260234832764
Epoch 500, val loss: 1.2529706954956055
Epoch 510, training loss: 0.029797501862049103 = 0.022733785212039948 + 0.001 * 7.063715934753418
Epoch 510, val loss: 1.2697455883026123
Epoch 520, training loss: 0.028067629784345627 = 0.02100672572851181 + 0.001 * 7.060904502868652
Epoch 520, val loss: 1.2859629392623901
Epoch 530, training loss: 0.026526274159550667 = 0.019467027857899666 + 0.001 * 7.059245586395264
Epoch 530, val loss: 1.3016399145126343
Epoch 540, training loss: 0.02514849416911602 = 0.01808995194733143 + 0.001 * 7.058541774749756
Epoch 540, val loss: 1.316773772239685
Epoch 550, training loss: 0.02391001768410206 = 0.01685451529920101 + 0.001 * 7.055501461029053
Epoch 550, val loss: 1.3314019441604614
Epoch 560, training loss: 0.022799542173743248 = 0.015742244198918343 + 0.001 * 7.057298183441162
Epoch 560, val loss: 1.3455232381820679
Epoch 570, training loss: 0.02179061621427536 = 0.014736834913492203 + 0.001 * 7.0537800788879395
Epoch 570, val loss: 1.359217882156372
Epoch 580, training loss: 0.020902512595057487 = 0.013822945766150951 + 0.001 * 7.07956600189209
Epoch 580, val loss: 1.3724732398986816
Epoch 590, training loss: 0.020039940252900124 = 0.01298761647194624 + 0.001 * 7.052323341369629
Epoch 590, val loss: 1.3854434490203857
Epoch 600, training loss: 0.019274916499853134 = 0.01222122274339199 + 0.001 * 7.053694248199463
Epoch 600, val loss: 1.3981682062149048
Epoch 610, training loss: 0.0185676459223032 = 0.011516690254211426 + 0.001 * 7.050955772399902
Epoch 610, val loss: 1.4106818437576294
Epoch 620, training loss: 0.01792004704475403 = 0.010868377052247524 + 0.001 * 7.051669597625732
Epoch 620, val loss: 1.422957181930542
Epoch 630, training loss: 0.017321936786174774 = 0.010271529667079449 + 0.001 * 7.050405979156494
Epoch 630, val loss: 1.434972882270813
Epoch 640, training loss: 0.016774911433458328 = 0.009721657261252403 + 0.001 * 7.053252696990967
Epoch 640, val loss: 1.446728229522705
Epoch 650, training loss: 0.016260236501693726 = 0.009214612655341625 + 0.001 * 7.045622825622559
Epoch 650, val loss: 1.458233118057251
Epoch 660, training loss: 0.015801116824150085 = 0.008746576495468616 + 0.001 * 7.054540157318115
Epoch 660, val loss: 1.4694489240646362
Epoch 670, training loss: 0.015358572825789452 = 0.00831396970897913 + 0.001 * 7.044602870941162
Epoch 670, val loss: 1.4804049730300903
Epoch 680, training loss: 0.014955420047044754 = 0.007913528010249138 + 0.001 * 7.041891098022461
Epoch 680, val loss: 1.4911024570465088
Epoch 690, training loss: 0.01458912342786789 = 0.007542395032942295 + 0.001 * 7.046727657318115
Epoch 690, val loss: 1.5015453100204468
Epoch 700, training loss: 0.01424439623951912 = 0.007197985891252756 + 0.001 * 7.04641056060791
Epoch 700, val loss: 1.5116945505142212
Epoch 710, training loss: 0.013917233794927597 = 0.006877862382680178 + 0.001 * 7.039371013641357
Epoch 710, val loss: 1.521606206893921
Epoch 720, training loss: 0.013613971881568432 = 0.006579867098480463 + 0.001 * 7.034104347229004
Epoch 720, val loss: 1.5312517881393433
Epoch 730, training loss: 0.01337365061044693 = 0.0063020456582307816 + 0.001 * 7.071604251861572
Epoch 730, val loss: 1.5406664609909058
Epoch 740, training loss: 0.013083301484584808 = 0.006042680237442255 + 0.001 * 7.040620803833008
Epoch 740, val loss: 1.5498344898223877
Epoch 750, training loss: 0.012830941006541252 = 0.0058001806028187275 + 0.001 * 7.030760765075684
Epoch 750, val loss: 1.5587843656539917
Epoch 760, training loss: 0.012608050368726254 = 0.0055730766616761684 + 0.001 * 7.03497314453125
Epoch 760, val loss: 1.5675090551376343
Epoch 770, training loss: 0.012388251721858978 = 0.005359970033168793 + 0.001 * 7.028280735015869
Epoch 770, val loss: 1.57602059841156
Epoch 780, training loss: 0.012189686298370361 = 0.005159024614840746 + 0.001 * 7.030661106109619
Epoch 780, val loss: 1.5843900442123413
Epoch 790, training loss: 0.011995170265436172 = 0.004967972170561552 + 0.001 * 7.02719783782959
Epoch 790, val loss: 1.592625617980957
Epoch 800, training loss: 0.011817975901067257 = 0.004784955643117428 + 0.001 * 7.03302001953125
Epoch 800, val loss: 1.6008189916610718
Epoch 810, training loss: 0.0116422763094306 = 0.004609239753335714 + 0.001 * 7.033036231994629
Epoch 810, val loss: 1.6089287996292114
Epoch 820, training loss: 0.011469129472970963 = 0.00444052554666996 + 0.001 * 7.0286030769348145
Epoch 820, val loss: 1.6170103549957275
Epoch 830, training loss: 0.011307507753372192 = 0.004278932698071003 + 0.001 * 7.0285749435424805
Epoch 830, val loss: 1.6249804496765137
Epoch 840, training loss: 0.011148902587592602 = 0.004124626982957125 + 0.001 * 7.024275302886963
Epoch 840, val loss: 1.6328685283660889
Epoch 850, training loss: 0.01102064736187458 = 0.00397765776142478 + 0.001 * 7.0429887771606445
Epoch 850, val loss: 1.6406164169311523
Epoch 860, training loss: 0.010853800922632217 = 0.0038379556499421597 + 0.001 * 7.015844345092773
Epoch 860, val loss: 1.6481891870498657
Epoch 870, training loss: 0.010726770386099815 = 0.003705291310325265 + 0.001 * 7.02147912979126
Epoch 870, val loss: 1.655622959136963
Epoch 880, training loss: 0.010592767037451267 = 0.0035793937277048826 + 0.001 * 7.013373374938965
Epoch 880, val loss: 1.6628971099853516
Epoch 890, training loss: 0.010496203787624836 = 0.0034599897917360067 + 0.001 * 7.036213397979736
Epoch 890, val loss: 1.6699942350387573
Epoch 900, training loss: 0.01036359928548336 = 0.0033466476015746593 + 0.001 * 7.016952037811279
Epoch 900, val loss: 1.6769239902496338
Epoch 910, training loss: 0.010251546278595924 = 0.003239208832383156 + 0.001 * 7.012336730957031
Epoch 910, val loss: 1.6836636066436768
Epoch 920, training loss: 0.010160205885767937 = 0.003137266030535102 + 0.001 * 7.022939682006836
Epoch 920, val loss: 1.6902376413345337
Epoch 930, training loss: 0.010054861195385456 = 0.0030405318830162287 + 0.001 * 7.014329433441162
Epoch 930, val loss: 1.6966365575790405
Epoch 940, training loss: 0.009959487244486809 = 0.0029486389830708504 + 0.001 * 7.010848045349121
Epoch 940, val loss: 1.7028982639312744
Epoch 950, training loss: 0.00986133236438036 = 0.002861317479982972 + 0.001 * 7.000014305114746
Epoch 950, val loss: 1.7089672088623047
Epoch 960, training loss: 0.009780095890164375 = 0.002778260037302971 + 0.001 * 7.001834869384766
Epoch 960, val loss: 1.714902400970459
Epoch 970, training loss: 0.009698433801531792 = 0.002699219388887286 + 0.001 * 6.999213695526123
Epoch 970, val loss: 1.720673680305481
Epoch 980, training loss: 0.009637609124183655 = 0.002623943844810128 + 0.001 * 7.013664722442627
Epoch 980, val loss: 1.7263433933258057
Epoch 990, training loss: 0.009549184702336788 = 0.002552223624661565 + 0.001 * 6.996960639953613
Epoch 990, val loss: 1.7318192720413208
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8207696362677913
The final CL Acc:0.76543, 0.03148, The final GNN Acc:0.81567, 0.00409
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13254])
remove edge: torch.Size([2, 7814])
updated graph: torch.Size([2, 10512])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9660121202468872 = 1.9574153423309326 + 0.001 * 8.596809387207031
Epoch 0, val loss: 1.9541114568710327
Epoch 10, training loss: 1.955876111984253 = 1.9472793340682983 + 0.001 * 8.59675121307373
Epoch 10, val loss: 1.9446686506271362
Epoch 20, training loss: 1.9434330463409424 = 1.934836506843567 + 0.001 * 8.596540451049805
Epoch 20, val loss: 1.9327596426010132
Epoch 30, training loss: 1.9259223937988281 = 1.9173263311386108 + 0.001 * 8.596049308776855
Epoch 30, val loss: 1.9158650636672974
Epoch 40, training loss: 1.8999656438827515 = 1.8913707733154297 + 0.001 * 8.59485912322998
Epoch 40, val loss: 1.8911490440368652
Epoch 50, training loss: 1.862830638885498 = 1.8542393445968628 + 0.001 * 8.591278076171875
Epoch 50, val loss: 1.8574469089508057
Epoch 60, training loss: 1.8186378479003906 = 1.8100601434707642 + 0.001 * 8.577658653259277
Epoch 60, val loss: 1.8209987878799438
Epoch 70, training loss: 1.7789840698242188 = 1.7704654932022095 + 0.001 * 8.518587112426758
Epoch 70, val loss: 1.7904107570648193
Epoch 80, training loss: 1.7356847524642944 = 1.7274644374847412 + 0.001 * 8.22031021118164
Epoch 80, val loss: 1.7505848407745361
Epoch 90, training loss: 1.676650047302246 = 1.6686391830444336 + 0.001 * 8.01091194152832
Epoch 90, val loss: 1.6952333450317383
Epoch 100, training loss: 1.598300814628601 = 1.5904672145843506 + 0.001 * 7.833648681640625
Epoch 100, val loss: 1.6268112659454346
Epoch 110, training loss: 1.5030944347381592 = 1.495442271232605 + 0.001 * 7.652193546295166
Epoch 110, val loss: 1.5484063625335693
Epoch 120, training loss: 1.4007127285003662 = 1.3931204080581665 + 0.001 * 7.592290878295898
Epoch 120, val loss: 1.465600609779358
Epoch 130, training loss: 1.298250436782837 = 1.2906851768493652 + 0.001 * 7.565300464630127
Epoch 130, val loss: 1.3838592767715454
Epoch 140, training loss: 1.1979701519012451 = 1.1904454231262207 + 0.001 * 7.52476167678833
Epoch 140, val loss: 1.3040411472320557
Epoch 150, training loss: 1.1015377044677734 = 1.0940682888031006 + 0.001 * 7.469358444213867
Epoch 150, val loss: 1.2271114587783813
Epoch 160, training loss: 1.010924220085144 = 1.0035102367401123 + 0.001 * 7.4139814376831055
Epoch 160, val loss: 1.1548354625701904
Epoch 170, training loss: 0.9268487691879272 = 0.9194895029067993 + 0.001 * 7.359271049499512
Epoch 170, val loss: 1.08845055103302
Epoch 180, training loss: 0.8490170836448669 = 0.8417263627052307 + 0.001 * 7.290741443634033
Epoch 180, val loss: 1.0283055305480957
Epoch 190, training loss: 0.7774473428726196 = 0.7702089548110962 + 0.001 * 7.23836612701416
Epoch 190, val loss: 0.9755600690841675
Epoch 200, training loss: 0.712315022945404 = 0.705103874206543 + 0.001 * 7.211144924163818
Epoch 200, val loss: 0.9315888285636902
Epoch 210, training loss: 0.6527882814407349 = 0.6456072330474854 + 0.001 * 7.181037425994873
Epoch 210, val loss: 0.8964932560920715
Epoch 220, training loss: 0.5974417328834534 = 0.5902831554412842 + 0.001 * 7.158560752868652
Epoch 220, val loss: 0.8686627149581909
Epoch 230, training loss: 0.5449756383895874 = 0.5378315448760986 + 0.001 * 7.144120216369629
Epoch 230, val loss: 0.8461925387382507
Epoch 240, training loss: 0.49454349279403687 = 0.4874047636985779 + 0.001 * 7.13873291015625
Epoch 240, val loss: 0.8278783559799194
Epoch 250, training loss: 0.44583457708358765 = 0.4386988878250122 + 0.001 * 7.135690689086914
Epoch 250, val loss: 0.8135854005813599
Epoch 260, training loss: 0.3992061913013458 = 0.39207157492637634 + 0.001 * 7.134610176086426
Epoch 260, val loss: 0.8038594722747803
Epoch 270, training loss: 0.3555847406387329 = 0.34845268726348877 + 0.001 * 7.132058620452881
Epoch 270, val loss: 0.799178421497345
Epoch 280, training loss: 0.3159637749195099 = 0.3088342249393463 + 0.001 * 7.129542350769043
Epoch 280, val loss: 0.8000673651695251
Epoch 290, training loss: 0.28065985441207886 = 0.2735329866409302 + 0.001 * 7.126861572265625
Epoch 290, val loss: 0.806447446346283
Epoch 300, training loss: 0.24914486706256866 = 0.2420188933610916 + 0.001 * 7.125977993011475
Epoch 300, val loss: 0.817620575428009
Epoch 310, training loss: 0.22068031132221222 = 0.21355776488780975 + 0.001 * 7.122549057006836
Epoch 310, val loss: 0.8328354954719543
Epoch 320, training loss: 0.19488279521465302 = 0.1877625435590744 + 0.001 * 7.120258331298828
Epoch 320, val loss: 0.8513444066047668
Epoch 330, training loss: 0.17168736457824707 = 0.16457030177116394 + 0.001 * 7.117055892944336
Epoch 330, val loss: 0.8724139928817749
Epoch 340, training loss: 0.1511128693819046 = 0.14399980008602142 + 0.001 * 7.113061904907227
Epoch 340, val loss: 0.8954989314079285
Epoch 350, training loss: 0.13310417532920837 = 0.12598679959774017 + 0.001 * 7.117379188537598
Epoch 350, val loss: 0.9200957417488098
Epoch 360, training loss: 0.11747230589389801 = 0.11036694794893265 + 0.001 * 7.1053547859191895
Epoch 360, val loss: 0.945683479309082
Epoch 370, training loss: 0.10400853306055069 = 0.09690766036510468 + 0.001 * 7.100871562957764
Epoch 370, val loss: 0.9718426465988159
Epoch 380, training loss: 0.09244322776794434 = 0.08534751087427139 + 0.001 * 7.095717430114746
Epoch 380, val loss: 0.9982491731643677
Epoch 390, training loss: 0.08254198729991913 = 0.07543104141950607 + 0.001 * 7.110942840576172
Epoch 390, val loss: 1.0245968103408813
Epoch 400, training loss: 0.07400383055210114 = 0.06691855937242508 + 0.001 * 7.085274696350098
Epoch 400, val loss: 1.0505777597427368
Epoch 410, training loss: 0.06667961925268173 = 0.05959784612059593 + 0.001 * 7.0817742347717285
Epoch 410, val loss: 1.0760509967803955
Epoch 420, training loss: 0.06036131829023361 = 0.05328650027513504 + 0.001 * 7.074817180633545
Epoch 420, val loss: 1.1009317636489868
Epoch 430, training loss: 0.0549410879611969 = 0.04782992973923683 + 0.001 * 7.1111578941345215
Epoch 430, val loss: 1.125166654586792
Epoch 440, training loss: 0.050166886299848557 = 0.04309709742665291 + 0.001 * 7.069788932800293
Epoch 440, val loss: 1.148635983467102
Epoch 450, training loss: 0.04604330658912659 = 0.038978658616542816 + 0.001 * 7.064646244049072
Epoch 450, val loss: 1.1713335514068604
Epoch 460, training loss: 0.04244508221745491 = 0.03538205847144127 + 0.001 * 7.063024997711182
Epoch 460, val loss: 1.1932756900787354
Epoch 470, training loss: 0.03928990662097931 = 0.03223024308681488 + 0.001 * 7.059665203094482
Epoch 470, val loss: 1.2144297361373901
Epoch 480, training loss: 0.03651471808552742 = 0.029458729550242424 + 0.001 * 7.055989742279053
Epoch 480, val loss: 1.2348452806472778
Epoch 490, training loss: 0.03407658636569977 = 0.027012759819626808 + 0.001 * 7.063825607299805
Epoch 490, val loss: 1.2545429468154907
Epoch 500, training loss: 0.03191636875271797 = 0.024846946820616722 + 0.001 * 7.069422245025635
Epoch 500, val loss: 1.2734308242797852
Epoch 510, training loss: 0.029976528137922287 = 0.022922620177268982 + 0.001 * 7.053908348083496
Epoch 510, val loss: 1.2916595935821533
Epoch 520, training loss: 0.028257882222533226 = 0.021206943318247795 + 0.001 * 7.050938129425049
Epoch 520, val loss: 1.3092283010482788
Epoch 530, training loss: 0.026721343398094177 = 0.019672473892569542 + 0.001 * 7.048868179321289
Epoch 530, val loss: 1.3261605501174927
Epoch 540, training loss: 0.02534521371126175 = 0.0182957760989666 + 0.001 * 7.049436569213867
Epoch 540, val loss: 1.3424736261367798
Epoch 550, training loss: 0.024103650823235512 = 0.017056897282600403 + 0.001 * 7.046753883361816
Epoch 550, val loss: 1.3581918478012085
Epoch 560, training loss: 0.022984998300671577 = 0.015938837081193924 + 0.001 * 7.046160697937012
Epoch 560, val loss: 1.3733656406402588
Epoch 570, training loss: 0.021970896050333977 = 0.014926975592970848 + 0.001 * 7.043920516967773
Epoch 570, val loss: 1.3880163431167603
Epoch 580, training loss: 0.02106083184480667 = 0.014008769765496254 + 0.001 * 7.052062034606934
Epoch 580, val loss: 1.40216863155365
Epoch 590, training loss: 0.020214907824993134 = 0.01317319180816412 + 0.001 * 7.041715621948242
Epoch 590, val loss: 1.4158580303192139
Epoch 600, training loss: 0.019453715533018112 = 0.0124103594571352 + 0.001 * 7.043356418609619
Epoch 600, val loss: 1.4291218519210815
Epoch 610, training loss: 0.01875174790620804 = 0.011710656806826591 + 0.001 * 7.0410919189453125
Epoch 610, val loss: 1.4419937133789062
Epoch 620, training loss: 0.01810813881456852 = 0.011065253056585789 + 0.001 * 7.042884826660156
Epoch 620, val loss: 1.4545629024505615
Epoch 630, training loss: 0.017502712085843086 = 0.010467562824487686 + 0.001 * 7.035149097442627
Epoch 630, val loss: 1.4668782949447632
Epoch 640, training loss: 0.01694566011428833 = 0.00991304125636816 + 0.001 * 7.032618045806885
Epoch 640, val loss: 1.4789763689041138
Epoch 650, training loss: 0.016438499093055725 = 0.009398273192346096 + 0.001 * 7.040226459503174
Epoch 650, val loss: 1.4908411502838135
Epoch 660, training loss: 0.01595323160290718 = 0.008920351043343544 + 0.001 * 7.032879829406738
Epoch 660, val loss: 1.5024701356887817
Epoch 670, training loss: 0.015509529039263725 = 0.008476466871798038 + 0.001 * 7.033061504364014
Epoch 670, val loss: 1.5138540267944336
Epoch 680, training loss: 0.015102876350283623 = 0.008063987828791142 + 0.001 * 7.038887977600098
Epoch 680, val loss: 1.5249781608581543
Epoch 690, training loss: 0.014715690165758133 = 0.007680477574467659 + 0.001 * 7.035212516784668
Epoch 690, val loss: 1.5358349084854126
Epoch 700, training loss: 0.01435331255197525 = 0.0073236580938100815 + 0.001 * 7.029653549194336
Epoch 700, val loss: 1.5464210510253906
Epoch 710, training loss: 0.01403622142970562 = 0.006991127505898476 + 0.001 * 7.045093536376953
Epoch 710, val loss: 1.5567750930786133
Epoch 720, training loss: 0.01371048018336296 = 0.006680731661617756 + 0.001 * 7.029748439788818
Epoch 720, val loss: 1.5668632984161377
Epoch 730, training loss: 0.013413571752607822 = 0.00639022933319211 + 0.001 * 7.023342132568359
Epoch 730, val loss: 1.5767583847045898
Epoch 740, training loss: 0.013141411356627941 = 0.006117315962910652 + 0.001 * 7.024095058441162
Epoch 740, val loss: 1.5865130424499512
Epoch 750, training loss: 0.012881167232990265 = 0.005859951488673687 + 0.001 * 7.021215915679932
Epoch 750, val loss: 1.5961201190948486
Epoch 760, training loss: 0.012632183730602264 = 0.0056165182031691074 + 0.001 * 7.015665054321289
Epoch 760, val loss: 1.6056267023086548
Epoch 770, training loss: 0.012404043227434158 = 0.005385994911193848 + 0.001 * 7.018048286437988
Epoch 770, val loss: 1.6150504350662231
Epoch 780, training loss: 0.01218881644308567 = 0.005167595110833645 + 0.001 * 7.02122163772583
Epoch 780, val loss: 1.6244142055511475
Epoch 790, training loss: 0.011979274451732635 = 0.004960749298334122 + 0.001 * 7.018524169921875
Epoch 790, val loss: 1.6337065696716309
Epoch 800, training loss: 0.011775846593081951 = 0.004764928016811609 + 0.001 * 7.010918140411377
Epoch 800, val loss: 1.6429097652435303
Epoch 810, training loss: 0.011591306887567043 = 0.004579650703817606 + 0.001 * 7.011655807495117
Epoch 810, val loss: 1.6520318984985352
Epoch 820, training loss: 0.011418132111430168 = 0.004404453095048666 + 0.001 * 7.013679027557373
Epoch 820, val loss: 1.6610440015792847
Epoch 830, training loss: 0.011248530820012093 = 0.004238910973072052 + 0.001 * 7.00961971282959
Epoch 830, val loss: 1.6699373722076416
Epoch 840, training loss: 0.011086765676736832 = 0.004082476254552603 + 0.001 * 7.004289150238037
Epoch 840, val loss: 1.6787134408950806
Epoch 850, training loss: 0.010935491882264614 = 0.003934641368687153 + 0.001 * 7.000850200653076
Epoch 850, val loss: 1.68732750415802
Epoch 860, training loss: 0.010795359499752522 = 0.0037948661483824253 + 0.001 * 7.000493049621582
Epoch 860, val loss: 1.6958014965057373
Epoch 870, training loss: 0.01065918616950512 = 0.003662632778286934 + 0.001 * 6.996552467346191
Epoch 870, val loss: 1.7041428089141846
Epoch 880, training loss: 0.010542531497776508 = 0.0035375296138226986 + 0.001 * 7.005001544952393
Epoch 880, val loss: 1.7123420238494873
Epoch 890, training loss: 0.010421565733850002 = 0.003419122425839305 + 0.001 * 7.002443313598633
Epoch 890, val loss: 1.7203550338745117
Epoch 900, training loss: 0.010323625057935715 = 0.0033069835044443607 + 0.001 * 7.016641616821289
Epoch 900, val loss: 1.7282418012619019
Epoch 910, training loss: 0.010193741880357265 = 0.003200748935341835 + 0.001 * 6.992992401123047
Epoch 910, val loss: 1.7359511852264404
Epoch 920, training loss: 0.010088572278618813 = 0.0031000282615423203 + 0.001 * 6.988543510437012
Epoch 920, val loss: 1.7435317039489746
Epoch 930, training loss: 0.010003730654716492 = 0.003004432190209627 + 0.001 * 6.999298572540283
Epoch 930, val loss: 1.7509660720825195
Epoch 940, training loss: 0.009920895099639893 = 0.0029136729426681995 + 0.001 * 7.007221698760986
Epoch 940, val loss: 1.7582560777664185
Epoch 950, training loss: 0.009826287627220154 = 0.002827463671565056 + 0.001 * 6.998824119567871
Epoch 950, val loss: 1.7653849124908447
Epoch 960, training loss: 0.009756278246641159 = 0.002745469333603978 + 0.001 * 7.01080846786499
Epoch 960, val loss: 1.7723926305770874
Epoch 970, training loss: 0.009669664315879345 = 0.0026674659457057714 + 0.001 * 7.002198219299316
Epoch 970, val loss: 1.7792390584945679
Epoch 980, training loss: 0.009578445926308632 = 0.0025932183489203453 + 0.001 * 6.985226631164551
Epoch 980, val loss: 1.785947322845459
Epoch 990, training loss: 0.009525857865810394 = 0.0025224776472896338 + 0.001 * 7.003379821777344
Epoch 990, val loss: 1.7925459146499634
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8318397469688983
=== training gcn model ===
Epoch 0, training loss: 1.9639004468917847 = 1.9553035497665405 + 0.001 * 8.596843719482422
Epoch 0, val loss: 1.9589959383010864
Epoch 10, training loss: 1.9537773132324219 = 1.9451805353164673 + 0.001 * 8.596766471862793
Epoch 10, val loss: 1.9488143920898438
Epoch 20, training loss: 1.9413719177246094 = 1.9327753782272339 + 0.001 * 8.596540451049805
Epoch 20, val loss: 1.9363301992416382
Epoch 30, training loss: 1.9242007732391357 = 1.9156047105789185 + 0.001 * 8.596033096313477
Epoch 30, val loss: 1.9191551208496094
Epoch 40, training loss: 1.8990870714187622 = 1.89049232006073 + 0.001 * 8.594775199890137
Epoch 40, val loss: 1.8946309089660645
Epoch 50, training loss: 1.863396167755127 = 1.8548054695129395 + 0.001 * 8.59068775177002
Epoch 50, val loss: 1.8610676527023315
Epoch 60, training loss: 1.8203978538513184 = 1.8118256330490112 + 0.001 * 8.572181701660156
Epoch 60, val loss: 1.8233963251113892
Epoch 70, training loss: 1.7812843322753906 = 1.77280855178833 + 0.001 * 8.475823402404785
Epoch 70, val loss: 1.7910314798355103
Epoch 80, training loss: 1.7394967079162598 = 1.7313929796218872 + 0.001 * 8.103737831115723
Epoch 80, val loss: 1.7518291473388672
Epoch 90, training loss: 1.682034969329834 = 1.6740649938583374 + 0.001 * 7.969935417175293
Epoch 90, val loss: 1.6984082460403442
Epoch 100, training loss: 1.6039907932281494 = 1.596189260482788 + 0.001 * 7.801513671875
Epoch 100, val loss: 1.630340576171875
Epoch 110, training loss: 1.5077327489852905 = 1.500174880027771 + 0.001 * 7.557897567749023
Epoch 110, val loss: 1.5492079257965088
Epoch 120, training loss: 1.4057061672210693 = 1.3983104228973389 + 0.001 * 7.395719051361084
Epoch 120, val loss: 1.4655858278274536
Epoch 130, training loss: 1.305856704711914 = 1.2984907627105713 + 0.001 * 7.365942478179932
Epoch 130, val loss: 1.3853986263275146
Epoch 140, training loss: 1.208434820175171 = 1.2010929584503174 + 0.001 * 7.341836929321289
Epoch 140, val loss: 1.3093239068984985
Epoch 150, training loss: 1.1146106719970703 = 1.107300877571106 + 0.001 * 7.309848785400391
Epoch 150, val loss: 1.2370071411132812
Epoch 160, training loss: 1.0265436172485352 = 1.0192594528198242 + 0.001 * 7.284169673919678
Epoch 160, val loss: 1.170073390007019
Epoch 170, training loss: 0.9454734921455383 = 0.9382079839706421 + 0.001 * 7.265501022338867
Epoch 170, val loss: 1.1090288162231445
Epoch 180, training loss: 0.8709129691123962 = 0.8636617660522461 + 0.001 * 7.2512030601501465
Epoch 180, val loss: 1.0526729822158813
Epoch 190, training loss: 0.8012639284133911 = 0.7940287590026855 + 0.001 * 7.235176086425781
Epoch 190, val loss: 0.9994441270828247
Epoch 200, training loss: 0.735561728477478 = 0.7283488512039185 + 0.001 * 7.212896347045898
Epoch 200, val loss: 0.9489697217941284
Epoch 210, training loss: 0.6735386252403259 = 0.6663581728935242 + 0.001 * 7.180428981781006
Epoch 210, val loss: 0.9018542170524597
Epoch 220, training loss: 0.6145499348640442 = 0.6073985695838928 + 0.001 * 7.151357173919678
Epoch 220, val loss: 0.8583131432533264
Epoch 230, training loss: 0.5571609735488892 = 0.5500380992889404 + 0.001 * 7.122851848602295
Epoch 230, val loss: 0.8184121251106262
Epoch 240, training loss: 0.5000634789466858 = 0.4929581582546234 + 0.001 * 7.105315685272217
Epoch 240, val loss: 0.7818654179573059
Epoch 250, training loss: 0.44285356998443604 = 0.43575939536094666 + 0.001 * 7.094172477722168
Epoch 250, val loss: 0.7490454316139221
Epoch 260, training loss: 0.3863637447357178 = 0.37927719950675964 + 0.001 * 7.086545944213867
Epoch 260, val loss: 0.7202140688896179
Epoch 270, training loss: 0.3322858512401581 = 0.32520729303359985 + 0.001 * 7.078548431396484
Epoch 270, val loss: 0.6961415410041809
Epoch 280, training loss: 0.2827867567539215 = 0.2757158875465393 + 0.001 * 7.07086181640625
Epoch 280, val loss: 0.678112268447876
Epoch 290, training loss: 0.2396082580089569 = 0.2325429767370224 + 0.001 * 7.065279483795166
Epoch 290, val loss: 0.6669081449508667
Epoch 300, training loss: 0.20343439280986786 = 0.1963679939508438 + 0.001 * 7.066404819488525
Epoch 300, val loss: 0.6624756455421448
Epoch 310, training loss: 0.17384423315525055 = 0.1667872816324234 + 0.001 * 7.056949138641357
Epoch 310, val loss: 0.6639050245285034
Epoch 320, training loss: 0.14983969926834106 = 0.1427851766347885 + 0.001 * 7.054523468017578
Epoch 320, val loss: 0.6698011159896851
Epoch 330, training loss: 0.13028742372989655 = 0.12323492765426636 + 0.001 * 7.052489757537842
Epoch 330, val loss: 0.6787983179092407
Epoch 340, training loss: 0.11422379314899445 = 0.10717227309942245 + 0.001 * 7.051522731781006
Epoch 340, val loss: 0.6899005174636841
Epoch 350, training loss: 0.10089026391506195 = 0.09383754432201385 + 0.001 * 7.052718162536621
Epoch 350, val loss: 0.7023383975028992
Epoch 360, training loss: 0.08970551937818527 = 0.08265451341867447 + 0.001 * 7.051003456115723
Epoch 360, val loss: 0.7156226634979248
Epoch 370, training loss: 0.0802401527762413 = 0.07319192588329315 + 0.001 * 7.048224925994873
Epoch 370, val loss: 0.729498565196991
Epoch 380, training loss: 0.07217375934123993 = 0.06512226164340973 + 0.001 * 7.0515007972717285
Epoch 380, val loss: 0.7436797618865967
Epoch 390, training loss: 0.06524387001991272 = 0.05819712206721306 + 0.001 * 7.046751022338867
Epoch 390, val loss: 0.758039116859436
Epoch 400, training loss: 0.059282053261995316 = 0.05222244933247566 + 0.001 * 7.059603214263916
Epoch 400, val loss: 0.7724254727363586
Epoch 410, training loss: 0.05409009009599686 = 0.04704419523477554 + 0.001 * 7.04589319229126
Epoch 410, val loss: 0.786765456199646
Epoch 420, training loss: 0.04958576336503029 = 0.04253889247775078 + 0.001 * 7.046872138977051
Epoch 420, val loss: 0.8009501099586487
Epoch 430, training loss: 0.04564886912703514 = 0.03860442712903023 + 0.001 * 7.044442653656006
Epoch 430, val loss: 0.8149405121803284
Epoch 440, training loss: 0.04219961166381836 = 0.035156019032001495 + 0.001 * 7.0435919761657715
Epoch 440, val loss: 0.8287152647972107
Epoch 450, training loss: 0.039167337119579315 = 0.03212301433086395 + 0.001 * 7.044321537017822
Epoch 450, val loss: 0.8422242999076843
Epoch 460, training loss: 0.03649620711803436 = 0.029446108266711235 + 0.001 * 7.050097465515137
Epoch 460, val loss: 0.8554368615150452
Epoch 470, training loss: 0.034119848161935806 = 0.02707553654909134 + 0.001 * 7.044311046600342
Epoch 470, val loss: 0.8683751821517944
Epoch 480, training loss: 0.032009273767471313 = 0.024968549609184265 + 0.001 * 7.0407257080078125
Epoch 480, val loss: 0.8810190558433533
Epoch 490, training loss: 0.030128851532936096 = 0.023089181631803513 + 0.001 * 7.039670467376709
Epoch 490, val loss: 0.8933398127555847
Epoch 500, training loss: 0.028445858508348465 = 0.021407410502433777 + 0.001 * 7.038447856903076
Epoch 500, val loss: 0.90537029504776
Epoch 510, training loss: 0.02693851664662361 = 0.01989784650504589 + 0.001 * 7.040670871734619
Epoch 510, val loss: 0.9170663952827454
Epoch 520, training loss: 0.025580953806638718 = 0.018538711592555046 + 0.001 * 7.04224157333374
Epoch 520, val loss: 0.9284734725952148
Epoch 530, training loss: 0.024347949773073196 = 0.01731152832508087 + 0.001 * 7.036421298980713
Epoch 530, val loss: 0.9396013021469116
Epoch 540, training loss: 0.023234928026795387 = 0.016200413927435875 + 0.001 * 7.034514427185059
Epoch 540, val loss: 0.9504246711730957
Epoch 550, training loss: 0.022235209122300148 = 0.01519166212528944 + 0.001 * 7.043546199798584
Epoch 550, val loss: 0.9609758257865906
Epoch 560, training loss: 0.021306298673152924 = 0.014273626729846 + 0.001 * 7.032670497894287
Epoch 560, val loss: 0.9712428450584412
Epoch 570, training loss: 0.020467504858970642 = 0.013436094857752323 + 0.001 * 7.0314106941223145
Epoch 570, val loss: 0.9812406301498413
Epoch 580, training loss: 0.019702434539794922 = 0.012670195661485195 + 0.001 * 7.0322394371032715
Epoch 580, val loss: 0.9909946918487549
Epoch 590, training loss: 0.019001513719558716 = 0.011968336999416351 + 0.001 * 7.033175468444824
Epoch 590, val loss: 1.0004806518554688
Epoch 600, training loss: 0.018352432176470757 = 0.011323795653879642 + 0.001 * 7.028635501861572
Epoch 600, val loss: 1.0097612142562866
Epoch 610, training loss: 0.017772190272808075 = 0.010730758309364319 + 0.001 * 7.041430950164795
Epoch 610, val loss: 1.0187926292419434
Epoch 620, training loss: 0.017215386033058167 = 0.010184016078710556 + 0.001 * 7.031370162963867
Epoch 620, val loss: 1.0276085138320923
Epoch 630, training loss: 0.016706014052033424 = 0.009679020382463932 + 0.001 * 7.026993274688721
Epoch 630, val loss: 1.0362037420272827
Epoch 640, training loss: 0.016237445175647736 = 0.009211721830070019 + 0.001 * 7.025722980499268
Epoch 640, val loss: 1.0446295738220215
Epoch 650, training loss: 0.01581239141523838 = 0.00877855159342289 + 0.001 * 7.033839702606201
Epoch 650, val loss: 1.0528391599655151
Epoch 660, training loss: 0.015403000637888908 = 0.008376381359994411 + 0.001 * 7.0266194343566895
Epoch 660, val loss: 1.0608834028244019
Epoch 670, training loss: 0.015023896470665932 = 0.008002416230738163 + 0.001 * 7.021480083465576
Epoch 670, val loss: 1.068721890449524
Epoch 680, training loss: 0.014676345512270927 = 0.007654124870896339 + 0.001 * 7.022220134735107
Epoch 680, val loss: 1.0764102935791016
Epoch 690, training loss: 0.014358420856297016 = 0.007329224143177271 + 0.001 * 7.029196262359619
Epoch 690, val loss: 1.0838961601257324
Epoch 700, training loss: 0.014044550247490406 = 0.007025716360658407 + 0.001 * 7.018833637237549
Epoch 700, val loss: 1.0912288427352905
Epoch 710, training loss: 0.013757696375250816 = 0.006741806399077177 + 0.001 * 7.015890121459961
Epoch 710, val loss: 1.0984123945236206
Epoch 720, training loss: 0.013507421128451824 = 0.006475856527686119 + 0.001 * 7.031564235687256
Epoch 720, val loss: 1.105426549911499
Epoch 730, training loss: 0.013258294202387333 = 0.00622636778280139 + 0.001 * 7.031926155090332
Epoch 730, val loss: 1.1123292446136475
Epoch 740, training loss: 0.013010776601731777 = 0.005992047023028135 + 0.001 * 7.018729209899902
Epoch 740, val loss: 1.119072437286377
Epoch 750, training loss: 0.012783218175172806 = 0.0057717179879546165 + 0.001 * 7.011499404907227
Epoch 750, val loss: 1.1256612539291382
Epoch 760, training loss: 0.012571185827255249 = 0.0055643185041844845 + 0.001 * 7.006867408752441
Epoch 760, val loss: 1.1321384906768799
Epoch 770, training loss: 0.012381297536194324 = 0.005368845071643591 + 0.001 * 7.012452125549316
Epoch 770, val loss: 1.1384831666946411
Epoch 780, training loss: 0.012203419581055641 = 0.005184398498386145 + 0.001 * 7.019021034240723
Epoch 780, val loss: 1.1446812152862549
Epoch 790, training loss: 0.012021591886878014 = 0.005010210908949375 + 0.001 * 7.011381149291992
Epoch 790, val loss: 1.1507840156555176
Epoch 800, training loss: 0.011852191761136055 = 0.004845507442951202 + 0.001 * 7.006684303283691
Epoch 800, val loss: 1.1567528247833252
Epoch 810, training loss: 0.011690428480505943 = 0.004689639899879694 + 0.001 * 7.000788688659668
Epoch 810, val loss: 1.1626029014587402
Epoch 820, training loss: 0.011544620618224144 = 0.00454197870567441 + 0.001 * 7.002641201019287
Epoch 820, val loss: 1.1683449745178223
Epoch 830, training loss: 0.011429035104811192 = 0.004401962272822857 + 0.001 * 7.027072429656982
Epoch 830, val loss: 1.1739720106124878
Epoch 840, training loss: 0.01127183623611927 = 0.004269087687134743 + 0.001 * 7.002748012542725
Epoch 840, val loss: 1.179503083229065
Epoch 850, training loss: 0.011142617091536522 = 0.004142885562032461 + 0.001 * 6.999731063842773
Epoch 850, val loss: 1.1849290132522583
Epoch 860, training loss: 0.01101359911262989 = 0.004022889770567417 + 0.001 * 6.990708827972412
Epoch 860, val loss: 1.1902486085891724
Epoch 870, training loss: 0.010897559113800526 = 0.003908632788807154 + 0.001 * 6.988925933837891
Epoch 870, val loss: 1.195466160774231
Epoch 880, training loss: 0.0107951695099473 = 0.003799787489697337 + 0.001 * 6.995381832122803
Epoch 880, val loss: 1.2006409168243408
Epoch 890, training loss: 0.010683758184313774 = 0.003695922438055277 + 0.001 * 6.987835884094238
Epoch 890, val loss: 1.2056670188903809
Epoch 900, training loss: 0.010589413344860077 = 0.0035965817514806986 + 0.001 * 6.992831230163574
Epoch 900, val loss: 1.210650086402893
Epoch 910, training loss: 0.010503904893994331 = 0.0035012750886380672 + 0.001 * 7.002629280090332
Epoch 910, val loss: 1.2155383825302124
Epoch 920, training loss: 0.010404438711702824 = 0.0034094464499503374 + 0.001 * 6.994991779327393
Epoch 920, val loss: 1.2203900814056396
Epoch 930, training loss: 0.01031606737524271 = 0.003320553107187152 + 0.001 * 6.995514392852783
Epoch 930, val loss: 1.2251940965652466
Epoch 940, training loss: 0.01022569090127945 = 0.003234204137697816 + 0.001 * 6.991486072540283
Epoch 940, val loss: 1.2300152778625488
Epoch 950, training loss: 0.010131167247891426 = 0.0031501357443630695 + 0.001 * 6.9810309410095215
Epoch 950, val loss: 1.23485267162323
Epoch 960, training loss: 0.010047217831015587 = 0.0030681497883051634 + 0.001 * 6.979067802429199
Epoch 960, val loss: 1.2396938800811768
Epoch 970, training loss: 0.009993555024266243 = 0.0029882604721933603 + 0.001 * 7.005293846130371
Epoch 970, val loss: 1.2446120977401733
Epoch 980, training loss: 0.00988693255931139 = 0.0029106340371072292 + 0.001 * 6.9762983322143555
Epoch 980, val loss: 1.249552607536316
Epoch 990, training loss: 0.009826400317251682 = 0.002835236955434084 + 0.001 * 6.99116325378418
Epoch 990, val loss: 1.2544939517974854
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8444913020558777
=== training gcn model ===
Epoch 0, training loss: 1.9611870050430298 = 1.9525902271270752 + 0.001 * 8.59681510925293
Epoch 0, val loss: 1.9486815929412842
Epoch 10, training loss: 1.9507428407669067 = 1.9421460628509521 + 0.001 * 8.59675407409668
Epoch 10, val loss: 1.93927001953125
Epoch 20, training loss: 1.9380356073379517 = 1.9294390678405762 + 0.001 * 8.59653091430664
Epoch 20, val loss: 1.9275474548339844
Epoch 30, training loss: 1.9203736782073975 = 1.9117777347564697 + 0.001 * 8.595988273620605
Epoch 30, val loss: 1.9110864400863647
Epoch 40, training loss: 1.8944976329803467 = 1.885903000831604 + 0.001 * 8.594585418701172
Epoch 40, val loss: 1.8871418237686157
Epoch 50, training loss: 1.85798180103302 = 1.8493915796279907 + 0.001 * 8.590202331542969
Epoch 50, val loss: 1.8547338247299194
Epoch 60, training loss: 1.8148550987243652 = 1.8062822818756104 + 0.001 * 8.572761535644531
Epoch 60, val loss: 1.8194921016693115
Epoch 70, training loss: 1.7748441696166992 = 1.7663562297821045 + 0.001 * 8.4879150390625
Epoch 70, val loss: 1.786745548248291
Epoch 80, training loss: 1.7268034219741821 = 1.7187068462371826 + 0.001 * 8.096529960632324
Epoch 80, val loss: 1.7394975423812866
Epoch 90, training loss: 1.6600099802017212 = 1.652022123336792 + 0.001 * 7.987911224365234
Epoch 90, val loss: 1.6761164665222168
Epoch 100, training loss: 1.573801875114441 = 1.565893530845642 + 0.001 * 7.908325672149658
Epoch 100, val loss: 1.6025139093399048
Epoch 110, training loss: 1.4788341522216797 = 1.4710341691970825 + 0.001 * 7.8000288009643555
Epoch 110, val loss: 1.5227410793304443
Epoch 120, training loss: 1.3857799768447876 = 1.3781687021255493 + 0.001 * 7.61123514175415
Epoch 120, val loss: 1.4444429874420166
Epoch 130, training loss: 1.2962665557861328 = 1.2887482643127441 + 0.001 * 7.518345355987549
Epoch 130, val loss: 1.3699219226837158
Epoch 140, training loss: 1.2065798044204712 = 1.1990948915481567 + 0.001 * 7.484871864318848
Epoch 140, val loss: 1.296491026878357
Epoch 150, training loss: 1.1133079528808594 = 1.1058578491210938 + 0.001 * 7.450109958648682
Epoch 150, val loss: 1.2213588953018188
Epoch 160, training loss: 1.014385461807251 = 1.0069797039031982 + 0.001 * 7.405704498291016
Epoch 160, val loss: 1.1422033309936523
Epoch 170, training loss: 0.9115132093429565 = 0.9041576385498047 + 0.001 * 7.355588436126709
Epoch 170, val loss: 1.0611133575439453
Epoch 180, training loss: 0.8113940358161926 = 0.804096519947052 + 0.001 * 7.297492980957031
Epoch 180, val loss: 0.984573483467102
Epoch 190, training loss: 0.7216172218322754 = 0.7143664360046387 + 0.001 * 7.250766754150391
Epoch 190, val loss: 0.920537531375885
Epoch 200, training loss: 0.6454491019248962 = 0.6382166743278503 + 0.001 * 7.232439994812012
Epoch 200, val loss: 0.8726358413696289
Epoch 210, training loss: 0.5818378329277039 = 0.5746126770973206 + 0.001 * 7.225147247314453
Epoch 210, val loss: 0.839722752571106
Epoch 220, training loss: 0.5280514359474182 = 0.5208291411399841 + 0.001 * 7.222315788269043
Epoch 220, val loss: 0.8179970979690552
Epoch 230, training loss: 0.4811229407787323 = 0.47390514612197876 + 0.001 * 7.217802047729492
Epoch 230, val loss: 0.8035477995872498
Epoch 240, training loss: 0.4387371242046356 = 0.43152377009391785 + 0.001 * 7.213349342346191
Epoch 240, val loss: 0.793767511844635
Epoch 250, training loss: 0.399372398853302 = 0.39216354489326477 + 0.001 * 7.208863735198975
Epoch 250, val loss: 0.7875486612319946
Epoch 260, training loss: 0.36245226860046387 = 0.3552468717098236 + 0.001 * 7.205410480499268
Epoch 260, val loss: 0.7846306562423706
Epoch 270, training loss: 0.32787036895751953 = 0.3206678330898285 + 0.001 * 7.202539920806885
Epoch 270, val loss: 0.7851653695106506
Epoch 280, training loss: 0.2955767810344696 = 0.28837674856185913 + 0.001 * 7.200019836425781
Epoch 280, val loss: 0.7890739440917969
Epoch 290, training loss: 0.26545172929763794 = 0.2582534849643707 + 0.001 * 7.198246479034424
Epoch 290, val loss: 0.7959057688713074
Epoch 300, training loss: 0.23752480745315552 = 0.2303258776664734 + 0.001 * 7.198923110961914
Epoch 300, val loss: 0.8056370615959167
Epoch 310, training loss: 0.21197843551635742 = 0.20478197932243347 + 0.001 * 7.1964521408081055
Epoch 310, val loss: 0.8184555768966675
Epoch 320, training loss: 0.18902996182441711 = 0.1818336546421051 + 0.001 * 7.19630765914917
Epoch 320, val loss: 0.8343340754508972
Epoch 330, training loss: 0.16869068145751953 = 0.1614949107170105 + 0.001 * 7.195768356323242
Epoch 330, val loss: 0.8530553579330444
Epoch 340, training loss: 0.1507716178894043 = 0.14357610046863556 + 0.001 * 7.195523738861084
Epoch 340, val loss: 0.8742349147796631
Epoch 350, training loss: 0.13499005138874054 = 0.12779559195041656 + 0.001 * 7.194461345672607
Epoch 350, val loss: 0.8974981307983398
Epoch 360, training loss: 0.1210750937461853 = 0.11388206481933594 + 0.001 * 7.193027496337891
Epoch 360, val loss: 0.9223785996437073
Epoch 370, training loss: 0.1087828129529953 = 0.10159295052289963 + 0.001 * 7.189864635467529
Epoch 370, val loss: 0.9483667612075806
Epoch 380, training loss: 0.09791624546051025 = 0.0907212421298027 + 0.001 * 7.195003032684326
Epoch 380, val loss: 0.9751182198524475
Epoch 390, training loss: 0.08828005939722061 = 0.08109406381845474 + 0.001 * 7.185993194580078
Epoch 390, val loss: 1.0022547245025635
Epoch 400, training loss: 0.0797465518116951 = 0.07256631553173065 + 0.001 * 7.180235385894775
Epoch 400, val loss: 1.0294721126556396
Epoch 410, training loss: 0.07218904793262482 = 0.06501695513725281 + 0.001 * 7.172093391418457
Epoch 410, val loss: 1.056635856628418
Epoch 420, training loss: 0.06549905985593796 = 0.05833517014980316 + 0.001 * 7.1638922691345215
Epoch 420, val loss: 1.083472728729248
Epoch 430, training loss: 0.0595865324139595 = 0.052426282316446304 + 0.001 * 7.160249710083008
Epoch 430, val loss: 1.1098213195800781
Epoch 440, training loss: 0.054352931678295135 = 0.04720383137464523 + 0.001 * 7.149099349975586
Epoch 440, val loss: 1.135608196258545
Epoch 450, training loss: 0.049732331186532974 = 0.04259045049548149 + 0.001 * 7.141880035400391
Epoch 450, val loss: 1.1606801748275757
Epoch 460, training loss: 0.04565008357167244 = 0.038517486304044724 + 0.001 * 7.132596015930176
Epoch 460, val loss: 1.184955358505249
Epoch 470, training loss: 0.0420554056763649 = 0.03491978719830513 + 0.001 * 7.135618209838867
Epoch 470, val loss: 1.2083864212036133
Epoch 480, training loss: 0.03886950761079788 = 0.03173883631825447 + 0.001 * 7.13067102432251
Epoch 480, val loss: 1.2309991121292114
Epoch 490, training loss: 0.036080945283174515 = 0.02892252989113331 + 0.001 * 7.158414840698242
Epoch 490, val loss: 1.2528033256530762
Epoch 500, training loss: 0.03354782983660698 = 0.026425400748848915 + 0.001 * 7.122427940368652
Epoch 500, val loss: 1.2737928628921509
Epoch 510, training loss: 0.03131425008177757 = 0.024206504225730896 + 0.001 * 7.1077446937561035
Epoch 510, val loss: 1.2939835786819458
Epoch 520, training loss: 0.029326777905225754 = 0.022231046110391617 + 0.001 * 7.095731735229492
Epoch 520, val loss: 1.3134739398956299
Epoch 530, training loss: 0.0275774784386158 = 0.020468518137931824 + 0.001 * 7.108959197998047
Epoch 530, val loss: 1.3322243690490723
Epoch 540, training loss: 0.025988858193159103 = 0.018893029540777206 + 0.001 * 7.095828533172607
Epoch 540, val loss: 1.3503414392471313
Epoch 550, training loss: 0.024559710174798965 = 0.017481783404946327 + 0.001 * 7.0779266357421875
Epoch 550, val loss: 1.367774248123169
Epoch 560, training loss: 0.02333378791809082 = 0.01621515490114689 + 0.001 * 7.118632793426514
Epoch 560, val loss: 1.3846547603607178
Epoch 570, training loss: 0.022160248830914497 = 0.015076279640197754 + 0.001 * 7.083968162536621
Epoch 570, val loss: 1.400904893875122
Epoch 580, training loss: 0.021121321246027946 = 0.01405006367713213 + 0.001 * 7.071256637573242
Epoch 580, val loss: 1.416597604751587
Epoch 590, training loss: 0.020175036042928696 = 0.01312336977571249 + 0.001 * 7.051666736602783
Epoch 590, val loss: 1.4317432641983032
Epoch 600, training loss: 0.019344063475728035 = 0.012284662574529648 + 0.001 * 7.0594000816345215
Epoch 600, val loss: 1.4464185237884521
Epoch 610, training loss: 0.018612174317240715 = 0.011523804627358913 + 0.001 * 7.088369369506836
Epoch 610, val loss: 1.4606090784072876
Epoch 620, training loss: 0.017888139933347702 = 0.010832098312675953 + 0.001 * 7.056041240692139
Epoch 620, val loss: 1.4743272066116333
Epoch 630, training loss: 0.017249075695872307 = 0.01020172331482172 + 0.001 * 7.047351360321045
Epoch 630, val loss: 1.487584114074707
Epoch 640, training loss: 0.01666400581598282 = 0.009626204147934914 + 0.001 * 7.037801742553711
Epoch 640, val loss: 1.5004347562789917
Epoch 650, training loss: 0.0161408893764019 = 0.009099585004150867 + 0.001 * 7.0413031578063965
Epoch 650, val loss: 1.5128812789916992
Epoch 660, training loss: 0.01568738743662834 = 0.00861655455082655 + 0.001 * 7.070831775665283
Epoch 660, val loss: 1.5249441862106323
Epoch 670, training loss: 0.015204347670078278 = 0.008172700181603432 + 0.001 * 7.031647205352783
Epoch 670, val loss: 1.5366874933242798
Epoch 680, training loss: 0.014791782945394516 = 0.007763872388750315 + 0.001 * 7.027909755706787
Epoch 680, val loss: 1.548029899597168
Epoch 690, training loss: 0.014394909143447876 = 0.007386560086160898 + 0.001 * 7.00834846496582
Epoch 690, val loss: 1.5590614080429077
Epoch 700, training loss: 0.014055001549422741 = 0.007037797011435032 + 0.001 * 7.017204284667969
Epoch 700, val loss: 1.569748878479004
Epoch 710, training loss: 0.013769242912530899 = 0.0067146853543818 + 0.001 * 7.0545573234558105
Epoch 710, val loss: 1.5801533460617065
Epoch 720, training loss: 0.013447174802422523 = 0.006414886098355055 + 0.001 * 7.032289028167725
Epoch 720, val loss: 1.5902156829833984
Epoch 730, training loss: 0.013139498420059681 = 0.00613622460514307 + 0.001 * 7.0032734870910645
Epoch 730, val loss: 1.600045919418335
Epoch 740, training loss: 0.012880448251962662 = 0.005876734387129545 + 0.001 * 7.003714084625244
Epoch 740, val loss: 1.6095978021621704
Epoch 750, training loss: 0.012653836980462074 = 0.005634701810777187 + 0.001 * 7.019134998321533
Epoch 750, val loss: 1.618848204612732
Epoch 760, training loss: 0.01243237592279911 = 0.005408686585724354 + 0.001 * 7.023688316345215
Epoch 760, val loss: 1.6278959512710571
Epoch 770, training loss: 0.012198066338896751 = 0.005197262391448021 + 0.001 * 7.000803470611572
Epoch 770, val loss: 1.6366831064224243
Epoch 780, training loss: 0.011993350461125374 = 0.004999204073101282 + 0.001 * 6.994145393371582
Epoch 780, val loss: 1.6452690362930298
Epoch 790, training loss: 0.01181126944720745 = 0.004813400097191334 + 0.001 * 6.997869491577148
Epoch 790, val loss: 1.6535961627960205
Epoch 800, training loss: 0.011625709012150764 = 0.004638846032321453 + 0.001 * 6.986863136291504
Epoch 800, val loss: 1.6617354154586792
Epoch 810, training loss: 0.011540649458765984 = 0.0044746180064976215 + 0.001 * 7.066030979156494
Epoch 810, val loss: 1.6696974039077759
Epoch 820, training loss: 0.011344612576067448 = 0.004320019390434027 + 0.001 * 7.024592876434326
Epoch 820, val loss: 1.677401065826416
Epoch 830, training loss: 0.011158972978591919 = 0.0041742511093616486 + 0.001 * 6.9847211837768555
Epoch 830, val loss: 1.6849312782287598
Epoch 840, training loss: 0.011015160009264946 = 0.004036641214042902 + 0.001 * 6.978518962860107
Epoch 840, val loss: 1.6922742128372192
Epoch 850, training loss: 0.010885994881391525 = 0.0039066132158041 + 0.001 * 6.979381561279297
Epoch 850, val loss: 1.6994534730911255
Epoch 860, training loss: 0.010778697207570076 = 0.0037835962139070034 + 0.001 * 6.995100498199463
Epoch 860, val loss: 1.7064785957336426
Epoch 870, training loss: 0.010664481669664383 = 0.0036671177949756384 + 0.001 * 6.997363090515137
Epoch 870, val loss: 1.713304042816162
Epoch 880, training loss: 0.010525437071919441 = 0.0035566999576985836 + 0.001 * 6.968736171722412
Epoch 880, val loss: 1.7199872732162476
Epoch 890, training loss: 0.010422112420201302 = 0.003451914293691516 + 0.001 * 6.970198154449463
Epoch 890, val loss: 1.7265503406524658
Epoch 900, training loss: 0.010311662219464779 = 0.0033523968886584044 + 0.001 * 6.959264755249023
Epoch 900, val loss: 1.7328959703445435
Epoch 910, training loss: 0.01023823581635952 = 0.0032578124664723873 + 0.001 * 6.980422496795654
Epoch 910, val loss: 1.7391375303268433
Epoch 920, training loss: 0.010148290544748306 = 0.0031678681261837482 + 0.001 * 6.980422496795654
Epoch 920, val loss: 1.7452096939086914
Epoch 930, training loss: 0.010049465112388134 = 0.0030822386033833027 + 0.001 * 6.967226028442383
Epoch 930, val loss: 1.7511574029922485
Epoch 940, training loss: 0.009964130818843842 = 0.0030006268061697483 + 0.001 * 6.963503360748291
Epoch 940, val loss: 1.7570103406906128
Epoch 950, training loss: 0.009878093376755714 = 0.0029228124767541885 + 0.001 * 6.955280780792236
Epoch 950, val loss: 1.7626534700393677
Epoch 960, training loss: 0.00981699675321579 = 0.0028485385701060295 + 0.001 * 6.968457221984863
Epoch 960, val loss: 1.7682055234909058
Epoch 970, training loss: 0.00972848478704691 = 0.002777625573799014 + 0.001 * 6.9508585929870605
Epoch 970, val loss: 1.7736986875534058
Epoch 980, training loss: 0.0096930842846632 = 0.002709876047447324 + 0.001 * 6.983207702636719
Epoch 980, val loss: 1.7790215015411377
Epoch 990, training loss: 0.009597677737474442 = 0.0026450955774635077 + 0.001 * 6.952581882476807
Epoch 990, val loss: 1.7841976881027222
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8365840801265156
The final CL Acc:0.79630, 0.01983, The final GNN Acc:0.83764, 0.00522
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11696])
remove edge: torch.Size([2, 9564])
updated graph: torch.Size([2, 10704])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9620881080627441 = 1.9534912109375 + 0.001 * 8.596858024597168
Epoch 0, val loss: 1.943028450012207
Epoch 10, training loss: 1.9514527320861816 = 1.942855954170227 + 0.001 * 8.596813201904297
Epoch 10, val loss: 1.9333062171936035
Epoch 20, training loss: 1.9386122226715088 = 1.9300155639648438 + 0.001 * 8.596658706665039
Epoch 20, val loss: 1.9214578866958618
Epoch 30, training loss: 1.9209784269332886 = 1.9123821258544922 + 0.001 * 8.596263885498047
Epoch 30, val loss: 1.9051380157470703
Epoch 40, training loss: 1.8955281972885132 = 1.8869329690933228 + 0.001 * 8.595181465148926
Epoch 40, val loss: 1.8821080923080444
Epoch 50, training loss: 1.8605678081512451 = 1.8519760370254517 + 0.001 * 8.591789245605469
Epoch 50, val loss: 1.8525046110153198
Epoch 60, training loss: 1.822003722190857 = 1.8134244680404663 + 0.001 * 8.57923412322998
Epoch 60, val loss: 1.8241143226623535
Epoch 70, training loss: 1.7913377285003662 = 1.7828086614608765 + 0.001 * 8.529007911682129
Epoch 70, val loss: 1.8025193214416504
Epoch 80, training loss: 1.7562358379364014 = 1.7479575872421265 + 0.001 * 8.278220176696777
Epoch 80, val loss: 1.768920660018921
Epoch 90, training loss: 1.707165241241455 = 1.698995590209961 + 0.001 * 8.169620513916016
Epoch 90, val loss: 1.7247377634048462
Epoch 100, training loss: 1.6390780210494995 = 1.6309736967086792 + 0.001 * 8.104363441467285
Epoch 100, val loss: 1.668344497680664
Epoch 110, training loss: 1.554603934288025 = 1.5465505123138428 + 0.001 * 8.053455352783203
Epoch 110, val loss: 1.598017692565918
Epoch 120, training loss: 1.465553879737854 = 1.4575785398483276 + 0.001 * 7.975337028503418
Epoch 120, val loss: 1.5243072509765625
Epoch 130, training loss: 1.3788702487945557 = 1.3711347579956055 + 0.001 * 7.735498428344727
Epoch 130, val loss: 1.4537831544876099
Epoch 140, training loss: 1.2956452369689941 = 1.2880280017852783 + 0.001 * 7.617199420928955
Epoch 140, val loss: 1.3888033628463745
Epoch 150, training loss: 1.2145963907241821 = 1.2070446014404297 + 0.001 * 7.551824569702148
Epoch 150, val loss: 1.3277344703674316
Epoch 160, training loss: 1.134155511856079 = 1.1266510486602783 + 0.001 * 7.504500389099121
Epoch 160, val loss: 1.2691482305526733
Epoch 170, training loss: 1.0529899597167969 = 1.0455408096313477 + 0.001 * 7.449198246002197
Epoch 170, val loss: 1.2106373310089111
Epoch 180, training loss: 0.97145676612854 = 0.9640637040138245 + 0.001 * 7.393082141876221
Epoch 180, val loss: 1.151443600654602
Epoch 190, training loss: 0.8916618227958679 = 0.8842936158180237 + 0.001 * 7.368195533752441
Epoch 190, val loss: 1.093101978302002
Epoch 200, training loss: 0.8162953853607178 = 0.8089327216148376 + 0.001 * 7.362677574157715
Epoch 200, val loss: 1.0385044813156128
Epoch 210, training loss: 0.7472105622291565 = 0.7398554682731628 + 0.001 * 7.355091571807861
Epoch 210, val loss: 0.9900166988372803
Epoch 220, training loss: 0.6845218539237976 = 0.6771745681762695 + 0.001 * 7.347301959991455
Epoch 220, val loss: 0.9487375020980835
Epoch 230, training loss: 0.626789927482605 = 0.6194503307342529 + 0.001 * 7.339596748352051
Epoch 230, val loss: 0.9139841198921204
Epoch 240, training loss: 0.572289764881134 = 0.564956545829773 + 0.001 * 7.333228588104248
Epoch 240, val loss: 0.8846601247787476
Epoch 250, training loss: 0.5202210545539856 = 0.5128923058509827 + 0.001 * 7.328773021697998
Epoch 250, val loss: 0.8610460162162781
Epoch 260, training loss: 0.4707848131656647 = 0.46345946192741394 + 0.001 * 7.325340747833252
Epoch 260, val loss: 0.8440776467323303
Epoch 270, training loss: 0.4246622622013092 = 0.4173406660556793 + 0.001 * 7.321610450744629
Epoch 270, val loss: 0.834017276763916
Epoch 280, training loss: 0.3822353780269623 = 0.37491703033447266 + 0.001 * 7.318350791931152
Epoch 280, val loss: 0.8304291367530823
Epoch 290, training loss: 0.34355950355529785 = 0.3362415134906769 + 0.001 * 7.3179826736450195
Epoch 290, val loss: 0.8322654962539673
Epoch 300, training loss: 0.30844059586524963 = 0.3011261224746704 + 0.001 * 7.314480304718018
Epoch 300, val loss: 0.8382351398468018
Epoch 310, training loss: 0.2766140103340149 = 0.26930034160614014 + 0.001 * 7.313656806945801
Epoch 310, val loss: 0.8475128412246704
Epoch 320, training loss: 0.24779312312602997 = 0.24048151075839996 + 0.001 * 7.311618328094482
Epoch 320, val loss: 0.8592151403427124
Epoch 330, training loss: 0.22171439230442047 = 0.21440258622169495 + 0.001 * 7.311811923980713
Epoch 330, val loss: 0.8728655576705933
Epoch 340, training loss: 0.19814293086528778 = 0.19082927703857422 + 0.001 * 7.313655376434326
Epoch 340, val loss: 0.8880555033683777
Epoch 350, training loss: 0.17687761783599854 = 0.16956523060798645 + 0.001 * 7.312379360198975
Epoch 350, val loss: 0.9044944047927856
Epoch 360, training loss: 0.15775632858276367 = 0.15044523775577545 + 0.001 * 7.311097621917725
Epoch 360, val loss: 0.9217653274536133
Epoch 370, training loss: 0.14064641296863556 = 0.13333585858345032 + 0.001 * 7.310555458068848
Epoch 370, val loss: 0.9398460388183594
Epoch 380, training loss: 0.12541919946670532 = 0.1181061714887619 + 0.001 * 7.313027858734131
Epoch 380, val loss: 0.9585456848144531
Epoch 390, training loss: 0.1119324266910553 = 0.10461932420730591 + 0.001 * 7.3130998611450195
Epoch 390, val loss: 0.9776381850242615
Epoch 400, training loss: 0.10004240274429321 = 0.09273160994052887 + 0.001 * 7.310793399810791
Epoch 400, val loss: 0.9970922470092773
Epoch 410, training loss: 0.0896046832203865 = 0.08229590207338333 + 0.001 * 7.308779716491699
Epoch 410, val loss: 1.0167202949523926
Epoch 420, training loss: 0.08048345148563385 = 0.07316530495882034 + 0.001 * 7.318148612976074
Epoch 420, val loss: 1.0364238023757935
Epoch 430, training loss: 0.07250618189573288 = 0.0651995986700058 + 0.001 * 7.306580066680908
Epoch 430, val loss: 1.0560863018035889
Epoch 440, training loss: 0.06557509303092957 = 0.05826648324728012 + 0.001 * 7.308609485626221
Epoch 440, val loss: 1.0755195617675781
Epoch 450, training loss: 0.059554677456617355 = 0.05223535746335983 + 0.001 * 7.319321155548096
Epoch 450, val loss: 1.0946669578552246
Epoch 460, training loss: 0.05428328365087509 = 0.04698651283979416 + 0.001 * 7.296771049499512
Epoch 460, val loss: 1.1134835481643677
Epoch 470, training loss: 0.04971637949347496 = 0.04241268336772919 + 0.001 * 7.303696155548096
Epoch 470, val loss: 1.1318247318267822
Epoch 480, training loss: 0.04572706297039986 = 0.03841902315616608 + 0.001 * 7.308038234710693
Epoch 480, val loss: 1.1497631072998047
Epoch 490, training loss: 0.042230378836393356 = 0.034924332052469254 + 0.001 * 7.306046962738037
Epoch 490, val loss: 1.1671340465545654
Epoch 500, training loss: 0.03915073350071907 = 0.03185677528381348 + 0.001 * 7.293956756591797
Epoch 500, val loss: 1.1840040683746338
Epoch 510, training loss: 0.0364341139793396 = 0.02915513701736927 + 0.001 * 7.278977394104004
Epoch 510, val loss: 1.2003649473190308
Epoch 520, training loss: 0.03404698148369789 = 0.026768367737531662 + 0.001 * 7.278614521026611
Epoch 520, val loss: 1.2162061929702759
Epoch 530, training loss: 0.031948842108249664 = 0.024653488770127296 + 0.001 * 7.295352935791016
Epoch 530, val loss: 1.2315316200256348
Epoch 540, training loss: 0.030069978907704353 = 0.02277342416346073 + 0.001 * 7.2965545654296875
Epoch 540, val loss: 1.246381163597107
Epoch 550, training loss: 0.028398245573043823 = 0.021095165982842445 + 0.001 * 7.303079605102539
Epoch 550, val loss: 1.2606443166732788
Epoch 560, training loss: 0.0268559530377388 = 0.019588952884078026 + 0.001 * 7.2669997215271
Epoch 560, val loss: 1.2744888067245483
Epoch 570, training loss: 0.025496013462543488 = 0.01822793111205101 + 0.001 * 7.268082618713379
Epoch 570, val loss: 1.2878098487854004
Epoch 580, training loss: 0.024252427741885185 = 0.016993118450045586 + 0.001 * 7.259308815002441
Epoch 580, val loss: 1.3006818294525146
Epoch 590, training loss: 0.023144155740737915 = 0.015870746225118637 + 0.001 * 7.27340841293335
Epoch 590, val loss: 1.3131414651870728
Epoch 600, training loss: 0.022134874016046524 = 0.01484975591301918 + 0.001 * 7.285117149353027
Epoch 600, val loss: 1.3251862525939941
Epoch 610, training loss: 0.02117958292365074 = 0.013920214958488941 + 0.001 * 7.2593674659729
Epoch 610, val loss: 1.3368874788284302
Epoch 620, training loss: 0.020337050780653954 = 0.013072737492620945 + 0.001 * 7.264312744140625
Epoch 620, val loss: 1.34818434715271
Epoch 630, training loss: 0.019557364284992218 = 0.012299105525016785 + 0.001 * 7.258259296417236
Epoch 630, val loss: 1.3591313362121582
Epoch 640, training loss: 0.018841657787561417 = 0.011592031456530094 + 0.001 * 7.2496256828308105
Epoch 640, val loss: 1.3697617053985596
Epoch 650, training loss: 0.01819678768515587 = 0.01094440184533596 + 0.001 * 7.252385139465332
Epoch 650, val loss: 1.3800362348556519
Epoch 660, training loss: 0.017619812861084938 = 0.010350222699344158 + 0.001 * 7.269590377807617
Epoch 660, val loss: 1.3900649547576904
Epoch 670, training loss: 0.017041444778442383 = 0.009804215282201767 + 0.001 * 7.23723030090332
Epoch 670, val loss: 1.399711012840271
Epoch 680, training loss: 0.01654282584786415 = 0.0093016242608428 + 0.001 * 7.241201400756836
Epoch 680, val loss: 1.4090741872787476
Epoch 690, training loss: 0.016071395948529243 = 0.00883789174258709 + 0.001 * 7.233503818511963
Epoch 690, val loss: 1.4181987047195435
Epoch 700, training loss: 0.015660986304283142 = 0.008409066125750542 + 0.001 * 7.251919269561768
Epoch 700, val loss: 1.427047610282898
Epoch 710, training loss: 0.015238801017403603 = 0.008011583238840103 + 0.001 * 7.227217197418213
Epoch 710, val loss: 1.4355891942977905
Epoch 720, training loss: 0.014871339313685894 = 0.007642251439392567 + 0.001 * 7.2290873527526855
Epoch 720, val loss: 1.443923830986023
Epoch 730, training loss: 0.0145194623619318 = 0.007298564538359642 + 0.001 * 7.220897674560547
Epoch 730, val loss: 1.4519658088684082
Epoch 740, training loss: 0.014220163226127625 = 0.006977986078709364 + 0.001 * 7.242177486419678
Epoch 740, val loss: 1.4599233865737915
Epoch 750, training loss: 0.013908280991017818 = 0.006678140722215176 + 0.001 * 7.23013973236084
Epoch 750, val loss: 1.4676212072372437
Epoch 760, training loss: 0.013613030314445496 = 0.00639642309397459 + 0.001 * 7.216607570648193
Epoch 760, val loss: 1.475100040435791
Epoch 770, training loss: 0.013391229324042797 = 0.006130310706794262 + 0.001 * 7.260918140411377
Epoch 770, val loss: 1.482483983039856
Epoch 780, training loss: 0.013090642169117928 = 0.005877859424799681 + 0.001 * 7.212782382965088
Epoch 780, val loss: 1.4897856712341309
Epoch 790, training loss: 0.012872101739048958 = 0.005637200083583593 + 0.001 * 7.2349019050598145
Epoch 790, val loss: 1.496997356414795
Epoch 800, training loss: 0.012606242671608925 = 0.005407818127423525 + 0.001 * 7.198424339294434
Epoch 800, val loss: 1.5042604207992554
Epoch 810, training loss: 0.012471679598093033 = 0.005189406219869852 + 0.001 * 7.2822723388671875
Epoch 810, val loss: 1.5113812685012817
Epoch 820, training loss: 0.012180767953395844 = 0.0049820588901638985 + 0.001 * 7.1987080574035645
Epoch 820, val loss: 1.5184301137924194
Epoch 830, training loss: 0.011973226442933083 = 0.004785739351063967 + 0.001 * 7.187486171722412
Epoch 830, val loss: 1.5254048109054565
Epoch 840, training loss: 0.011785859242081642 = 0.004600041080266237 + 0.001 * 7.185817241668701
Epoch 840, val loss: 1.5322223901748657
Epoch 850, training loss: 0.011627822183072567 = 0.004424538929015398 + 0.001 * 7.203282833099365
Epoch 850, val loss: 1.5389833450317383
Epoch 860, training loss: 0.011442635208368301 = 0.004259075503796339 + 0.001 * 7.183558940887451
Epoch 860, val loss: 1.5455892086029053
Epoch 870, training loss: 0.01134781539440155 = 0.004103036597371101 + 0.001 * 7.244778633117676
Epoch 870, val loss: 1.552066445350647
Epoch 880, training loss: 0.011226855218410492 = 0.0039558554999530315 + 0.001 * 7.270999431610107
Epoch 880, val loss: 1.5583680868148804
Epoch 890, training loss: 0.01100830640643835 = 0.003816963406279683 + 0.001 * 7.191342830657959
Epoch 890, val loss: 1.5646482706069946
Epoch 900, training loss: 0.010890595614910126 = 0.003685843665152788 + 0.001 * 7.204751968383789
Epoch 900, val loss: 1.5706888437271118
Epoch 910, training loss: 0.010735489428043365 = 0.003561927704140544 + 0.001 * 7.173561096191406
Epoch 910, val loss: 1.5766441822052002
Epoch 920, training loss: 0.010627390816807747 = 0.003444801550358534 + 0.001 * 7.182589530944824
Epoch 920, val loss: 1.582477331161499
Epoch 930, training loss: 0.01049399096518755 = 0.0033340449444949627 + 0.001 * 7.159945487976074
Epoch 930, val loss: 1.5881211757659912
Epoch 940, training loss: 0.010410447604954243 = 0.003229330061003566 + 0.001 * 7.181117057800293
Epoch 940, val loss: 1.593652606010437
Epoch 950, training loss: 0.010310105979442596 = 0.003130305092781782 + 0.001 * 7.179800033569336
Epoch 950, val loss: 1.5990428924560547
Epoch 960, training loss: 0.010216182097792625 = 0.0030365188140422106 + 0.001 * 7.179662704467773
Epoch 960, val loss: 1.6043561697006226
Epoch 970, training loss: 0.010113128460943699 = 0.00294762896373868 + 0.001 * 7.165499210357666
Epoch 970, val loss: 1.609513282775879
Epoch 980, training loss: 0.010039562359452248 = 0.0028633077163249254 + 0.001 * 7.1762542724609375
Epoch 980, val loss: 1.6145336627960205
Epoch 990, training loss: 0.00996924564242363 = 0.0027832756750285625 + 0.001 * 7.185970306396484
Epoch 990, val loss: 1.6194653511047363
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 1.9513912200927734 = 1.9427943229675293 + 0.001 * 8.596866607666016
Epoch 0, val loss: 1.9370508193969727
Epoch 10, training loss: 1.9416577816009521 = 1.9330610036849976 + 0.001 * 8.596820831298828
Epoch 10, val loss: 1.9268766641616821
Epoch 20, training loss: 1.9299278259277344 = 1.9213311672210693 + 0.001 * 8.596673965454102
Epoch 20, val loss: 1.9143331050872803
Epoch 30, training loss: 1.9139963388442993 = 1.905400037765503 + 0.001 * 8.596343040466309
Epoch 30, val loss: 1.8972351551055908
Epoch 40, training loss: 1.8910852670669556 = 1.8824896812438965 + 0.001 * 8.595552444458008
Epoch 40, val loss: 1.8727961778640747
Epoch 50, training loss: 1.8592289686203003 = 1.8506355285644531 + 0.001 * 8.593475341796875
Epoch 50, val loss: 1.84016752243042
Epoch 60, training loss: 1.8213450908660889 = 1.8127583265304565 + 0.001 * 8.58675479888916
Epoch 60, val loss: 1.8053953647613525
Epoch 70, training loss: 1.7848610877990723 = 1.7763006687164307 + 0.001 * 8.56036376953125
Epoch 70, val loss: 1.7778425216674805
Epoch 80, training loss: 1.744163155555725 = 1.7357563972473145 + 0.001 * 8.406728744506836
Epoch 80, val loss: 1.7478562593460083
Epoch 90, training loss: 1.6890157461166382 = 1.6808725595474243 + 0.001 * 8.143202781677246
Epoch 90, val loss: 1.701159954071045
Epoch 100, training loss: 1.6140193939208984 = 1.606135606765747 + 0.001 * 7.883769989013672
Epoch 100, val loss: 1.636080026626587
Epoch 110, training loss: 1.51900053024292 = 1.5114102363586426 + 0.001 * 7.59032678604126
Epoch 110, val loss: 1.557262659072876
Epoch 120, training loss: 1.410119652748108 = 1.402665376663208 + 0.001 * 7.454298496246338
Epoch 120, val loss: 1.4689228534698486
Epoch 130, training loss: 1.2959104776382446 = 1.2884935140609741 + 0.001 * 7.416971683502197
Epoch 130, val loss: 1.3768812417984009
Epoch 140, training loss: 1.1824095249176025 = 1.1750084161758423 + 0.001 * 7.401080131530762
Epoch 140, val loss: 1.2865850925445557
Epoch 150, training loss: 1.0738834142684937 = 1.0664968490600586 + 0.001 * 7.38655424118042
Epoch 150, val loss: 1.201171636581421
Epoch 160, training loss: 0.973152756690979 = 0.9657740592956543 + 0.001 * 7.378725051879883
Epoch 160, val loss: 1.1233710050582886
Epoch 170, training loss: 0.8820640444755554 = 0.8746907711029053 + 0.001 * 7.373301982879639
Epoch 170, val loss: 1.0560423135757446
Epoch 180, training loss: 0.8018600344657898 = 0.7944934964179993 + 0.001 * 7.366519451141357
Epoch 180, val loss: 1.000361680984497
Epoch 190, training loss: 0.7320714592933655 = 0.7247169613838196 + 0.001 * 7.354526519775391
Epoch 190, val loss: 0.9562056660652161
Epoch 200, training loss: 0.6706041693687439 = 0.6632698774337769 + 0.001 * 7.334296703338623
Epoch 200, val loss: 0.9213224053382874
Epoch 210, training loss: 0.6145505309104919 = 0.607244610786438 + 0.001 * 7.305896282196045
Epoch 210, val loss: 0.8931657075881958
Epoch 220, training loss: 0.5616438984870911 = 0.55436110496521 + 0.001 * 7.282814025878906
Epoch 220, val loss: 0.8694369196891785
Epoch 230, training loss: 0.5109818577766418 = 0.5037142038345337 + 0.001 * 7.267633438110352
Epoch 230, val loss: 0.8492344617843628
Epoch 240, training loss: 0.46255743503570557 = 0.45529505610466003 + 0.001 * 7.2623820304870605
Epoch 240, val loss: 0.8326627612113953
Epoch 250, training loss: 0.4163873493671417 = 0.409127801656723 + 0.001 * 7.259533405303955
Epoch 250, val loss: 0.8197122812271118
Epoch 260, training loss: 0.37221387028694153 = 0.36495423316955566 + 0.001 * 7.25963830947876
Epoch 260, val loss: 0.8100367784500122
Epoch 270, training loss: 0.3297516107559204 = 0.3224915862083435 + 0.001 * 7.260022163391113
Epoch 270, val loss: 0.8030834794044495
Epoch 280, training loss: 0.28917768597602844 = 0.2819163501262665 + 0.001 * 7.261337757110596
Epoch 280, val loss: 0.7986251711845398
Epoch 290, training loss: 0.2511637210845947 = 0.24390023946762085 + 0.001 * 7.263479232788086
Epoch 290, val loss: 0.7972296476364136
Epoch 300, training loss: 0.21663610637187958 = 0.2093716859817505 + 0.001 * 7.264415264129639
Epoch 300, val loss: 0.799264669418335
Epoch 310, training loss: 0.18628716468811035 = 0.17902064323425293 + 0.001 * 7.2665228843688965
Epoch 310, val loss: 0.8049136996269226
Epoch 320, training loss: 0.16028472781181335 = 0.15301649272441864 + 0.001 * 7.2682366371154785
Epoch 320, val loss: 0.814037024974823
Epoch 330, training loss: 0.13835515081882477 = 0.13108161091804504 + 0.001 * 7.273544788360596
Epoch 330, val loss: 0.8260536789894104
Epoch 340, training loss: 0.1200113594532013 = 0.11273791640996933 + 0.001 * 7.273438930511475
Epoch 340, val loss: 0.8403341174125671
Epoch 350, training loss: 0.10472401231527328 = 0.09744904190301895 + 0.001 * 7.274970531463623
Epoch 350, val loss: 0.856217622756958
Epoch 360, training loss: 0.09198039025068283 = 0.08470462262630463 + 0.001 * 7.275765419006348
Epoch 360, val loss: 0.8731603026390076
Epoch 370, training loss: 0.08133507519960403 = 0.07405737042427063 + 0.001 * 7.27770471572876
Epoch 370, val loss: 0.8907097578048706
Epoch 380, training loss: 0.07240631431341171 = 0.06512698531150818 + 0.001 * 7.279328346252441
Epoch 380, val loss: 0.9086127281188965
Epoch 390, training loss: 0.06488043069839478 = 0.057600315660238266 + 0.001 * 7.280116558074951
Epoch 390, val loss: 0.926651656627655
Epoch 400, training loss: 0.05850117653608322 = 0.05121995508670807 + 0.001 * 7.281220436096191
Epoch 400, val loss: 0.9446296691894531
Epoch 410, training loss: 0.05306407809257507 = 0.04578224569559097 + 0.001 * 7.281830310821533
Epoch 410, val loss: 0.9624087810516357
Epoch 420, training loss: 0.04840679094195366 = 0.041122693568468094 + 0.001 * 7.284095764160156
Epoch 420, val loss: 0.9799178242683411
Epoch 430, training loss: 0.04439060017466545 = 0.03710838407278061 + 0.001 * 7.282214641571045
Epoch 430, val loss: 0.9970670342445374
Epoch 440, training loss: 0.04091685265302658 = 0.03363233059644699 + 0.001 * 7.284523010253906
Epoch 440, val loss: 1.0138771533966064
Epoch 450, training loss: 0.03789004310965538 = 0.030607622116804123 + 0.001 * 7.282421112060547
Epoch 450, val loss: 1.0302761793136597
Epoch 460, training loss: 0.03524501249194145 = 0.027962787076830864 + 0.001 * 7.282225608825684
Epoch 460, val loss: 1.0462578535079956
Epoch 470, training loss: 0.03292325139045715 = 0.02563984878361225 + 0.001 * 7.283403396606445
Epoch 470, val loss: 1.0618064403533936
Epoch 480, training loss: 0.030871767550706863 = 0.023590585216879845 + 0.001 * 7.281182289123535
Epoch 480, val loss: 1.0769466161727905
Epoch 490, training loss: 0.02905501052737236 = 0.02177545242011547 + 0.001 * 7.279557228088379
Epoch 490, val loss: 1.0916471481323242
Epoch 500, training loss: 0.027440816164016724 = 0.02016134187579155 + 0.001 * 7.279472827911377
Epoch 500, val loss: 1.1059086322784424
Epoch 510, training loss: 0.02599925920367241 = 0.018720630556344986 + 0.001 * 7.278627395629883
Epoch 510, val loss: 1.119738221168518
Epoch 520, training loss: 0.024719011038541794 = 0.01743009313941002 + 0.001 * 7.28891658782959
Epoch 520, val loss: 1.1331822872161865
Epoch 530, training loss: 0.02354784682393074 = 0.016270127147436142 + 0.001 * 7.277718544006348
Epoch 530, val loss: 1.1461890935897827
Epoch 540, training loss: 0.022498801350593567 = 0.015224121510982513 + 0.001 * 7.274680137634277
Epoch 540, val loss: 1.158850073814392
Epoch 550, training loss: 0.021556558087468147 = 0.014277859590947628 + 0.001 * 7.278697967529297
Epoch 550, val loss: 1.1710914373397827
Epoch 560, training loss: 0.02069070003926754 = 0.013419310562312603 + 0.001 * 7.271388530731201
Epoch 560, val loss: 1.1830238103866577
Epoch 570, training loss: 0.019909529015421867 = 0.012638225220143795 + 0.001 * 7.271304130554199
Epoch 570, val loss: 1.194576621055603
Epoch 580, training loss: 0.019197802990674973 = 0.011925676837563515 + 0.001 * 7.272124767303467
Epoch 580, val loss: 1.205824375152588
Epoch 590, training loss: 0.018538933247327805 = 0.011273927055299282 + 0.001 * 7.265006065368652
Epoch 590, val loss: 1.2167383432388306
Epoch 600, training loss: 0.017943574115633965 = 0.010676336474716663 + 0.001 * 7.267237663269043
Epoch 600, val loss: 1.227363109588623
Epoch 610, training loss: 0.017392830923199654 = 0.010127139277756214 + 0.001 * 7.265691757202148
Epoch 610, val loss: 1.2376983165740967
Epoch 620, training loss: 0.016893917694687843 = 0.009621298871934414 + 0.001 * 7.272617816925049
Epoch 620, val loss: 1.2477525472640991
Epoch 630, training loss: 0.016415825113654137 = 0.009154467843472958 + 0.001 * 7.261356353759766
Epoch 630, val loss: 1.257529616355896
Epoch 640, training loss: 0.016012005507946014 = 0.00872265174984932 + 0.001 * 7.289353847503662
Epoch 640, val loss: 1.2670584917068481
Epoch 650, training loss: 0.015572858974337578 = 0.008322492241859436 + 0.001 * 7.2503662109375
Epoch 650, val loss: 1.2763251066207886
Epoch 660, training loss: 0.015198484063148499 = 0.007951023057103157 + 0.001 * 7.247460842132568
Epoch 660, val loss: 1.2853562831878662
Epoch 670, training loss: 0.014884132891893387 = 0.00760549446567893 + 0.001 * 7.278637409210205
Epoch 670, val loss: 1.2941657304763794
Epoch 680, training loss: 0.014529230073094368 = 0.007283610757440329 + 0.001 * 7.24561882019043
Epoch 680, val loss: 1.3027528524398804
Epoch 690, training loss: 0.014259619638323784 = 0.006983296480029821 + 0.001 * 7.276322841644287
Epoch 690, val loss: 1.3111388683319092
Epoch 700, training loss: 0.013934780843555927 = 0.006702642887830734 + 0.001 * 7.232137680053711
Epoch 700, val loss: 1.319306492805481
Epoch 710, training loss: 0.013667508959770203 = 0.006440029013901949 + 0.001 * 7.227479934692383
Epoch 710, val loss: 1.3272650241851807
Epoch 720, training loss: 0.013426022604107857 = 0.006193841341882944 + 0.001 * 7.232180595397949
Epoch 720, val loss: 1.3350534439086914
Epoch 730, training loss: 0.013207231648266315 = 0.00596278952434659 + 0.001 * 7.244441986083984
Epoch 730, val loss: 1.3426446914672852
Epoch 740, training loss: 0.012971602380275726 = 0.005745676811784506 + 0.001 * 7.225925445556641
Epoch 740, val loss: 1.3500816822052002
Epoch 750, training loss: 0.012764520943164825 = 0.005541371647268534 + 0.001 * 7.223149299621582
Epoch 750, val loss: 1.35732102394104
Epoch 760, training loss: 0.012598451226949692 = 0.005348920822143555 + 0.001 * 7.249529838562012
Epoch 760, val loss: 1.3643903732299805
Epoch 770, training loss: 0.012389609590172768 = 0.005167396739125252 + 0.001 * 7.222211837768555
Epoch 770, val loss: 1.3713188171386719
Epoch 780, training loss: 0.012215152382850647 = 0.004996030125766993 + 0.001 * 7.219121932983398
Epoch 780, val loss: 1.3781249523162842
Epoch 790, training loss: 0.01204271987080574 = 0.004834021907299757 + 0.001 * 7.208697319030762
Epoch 790, val loss: 1.3847297430038452
Epoch 800, training loss: 0.01185116171836853 = 0.004680751357227564 + 0.001 * 7.170410633087158
Epoch 800, val loss: 1.3912122249603271
Epoch 810, training loss: 0.01171833649277687 = 0.004535505082458258 + 0.001 * 7.182831287384033
Epoch 810, val loss: 1.397568702697754
Epoch 820, training loss: 0.011594397947192192 = 0.004397697746753693 + 0.001 * 7.196700096130371
Epoch 820, val loss: 1.4037944078445435
Epoch 830, training loss: 0.011502832174301147 = 0.004266856703907251 + 0.001 * 7.23597526550293
Epoch 830, val loss: 1.4098750352859497
Epoch 840, training loss: 0.011318965815007687 = 0.0041425107046961784 + 0.001 * 7.176454544067383
Epoch 840, val loss: 1.4159038066864014
Epoch 850, training loss: 0.011199216358363628 = 0.004024088382720947 + 0.001 * 7.1751275062561035
Epoch 850, val loss: 1.4217931032180786
Epoch 860, training loss: 0.011082420125603676 = 0.003911223262548447 + 0.001 * 7.171195983886719
Epoch 860, val loss: 1.4275530576705933
Epoch 870, training loss: 0.010949445888400078 = 0.003803514176979661 + 0.001 * 7.145931720733643
Epoch 870, val loss: 1.433267593383789
Epoch 880, training loss: 0.01088178064674139 = 0.0037006251513957977 + 0.001 * 7.181155204772949
Epoch 880, val loss: 1.4388781785964966
Epoch 890, training loss: 0.010795345529913902 = 0.0036022060085088015 + 0.001 * 7.19313907623291
Epoch 890, val loss: 1.4444698095321655
Epoch 900, training loss: 0.01063602976500988 = 0.0035079261288046837 + 0.001 * 7.128102779388428
Epoch 900, val loss: 1.449928879737854
Epoch 910, training loss: 0.010560093447566032 = 0.003417674917727709 + 0.001 * 7.142417907714844
Epoch 910, val loss: 1.455348014831543
Epoch 920, training loss: 0.010496220551431179 = 0.0033311781007796526 + 0.001 * 7.165041923522949
Epoch 920, val loss: 1.4606704711914062
Epoch 930, training loss: 0.010375607758760452 = 0.0032483036629855633 + 0.001 * 7.1273040771484375
Epoch 930, val loss: 1.4659513235092163
Epoch 940, training loss: 0.010267937555909157 = 0.003168720519170165 + 0.001 * 7.099216938018799
Epoch 940, val loss: 1.471116542816162
Epoch 950, training loss: 0.010262326337397099 = 0.0030922412406653166 + 0.001 * 7.1700849533081055
Epoch 950, val loss: 1.4762407541275024
Epoch 960, training loss: 0.010135922580957413 = 0.0030188693199306726 + 0.001 * 7.117053031921387
Epoch 960, val loss: 1.4813275337219238
Epoch 970, training loss: 0.010105175897479057 = 0.0029484101105481386 + 0.001 * 7.156764984130859
Epoch 970, val loss: 1.4863228797912598
Epoch 980, training loss: 0.010011343285441399 = 0.002880672924220562 + 0.001 * 7.130669593811035
Epoch 980, val loss: 1.491224765777588
Epoch 990, training loss: 0.009953327476978302 = 0.0028155548498034477 + 0.001 * 7.1377716064453125
Epoch 990, val loss: 1.4961150884628296
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 1.9493657350540161 = 1.940768837928772 + 0.001 * 8.596846580505371
Epoch 0, val loss: 1.9360342025756836
Epoch 10, training loss: 1.9391076564788818 = 1.9305108785629272 + 0.001 * 8.596786499023438
Epoch 10, val loss: 1.9267587661743164
Epoch 20, training loss: 1.926439881324768 = 1.9178433418273926 + 0.001 * 8.596573829650879
Epoch 20, val loss: 1.9151227474212646
Epoch 30, training loss: 1.9088300466537476 = 1.9002339839935303 + 0.001 * 8.596013069152832
Epoch 30, val loss: 1.8990263938903809
Epoch 40, training loss: 1.8835293054580688 = 1.8749349117279053 + 0.001 * 8.594449996948242
Epoch 40, val loss: 1.876476526260376
Epoch 50, training loss: 1.8504198789596558 = 1.8418304920196533 + 0.001 * 8.589338302612305
Epoch 50, val loss: 1.8489478826522827
Epoch 60, training loss: 1.817482829093933 = 1.8089138269424438 + 0.001 * 8.568973541259766
Epoch 60, val loss: 1.824662208557129
Epoch 70, training loss: 1.7906614542007446 = 1.782188892364502 + 0.001 * 8.472541809082031
Epoch 70, val loss: 1.8021674156188965
Epoch 80, training loss: 1.7555382251739502 = 1.7473586797714233 + 0.001 * 8.179498672485352
Epoch 80, val loss: 1.7672581672668457
Epoch 90, training loss: 1.7060904502868652 = 1.697975754737854 + 0.001 * 8.114693641662598
Epoch 90, val loss: 1.7223420143127441
Epoch 100, training loss: 1.6373413801193237 = 1.6292790174484253 + 0.001 * 8.062410354614258
Epoch 100, val loss: 1.6638120412826538
Epoch 110, training loss: 1.55586838722229 = 1.5478408336639404 + 0.001 * 8.027605056762695
Epoch 110, val loss: 1.5963337421417236
Epoch 120, training loss: 1.4729374647140503 = 1.4649577140808105 + 0.001 * 7.979750156402588
Epoch 120, val loss: 1.5280871391296387
Epoch 130, training loss: 1.3926321268081665 = 1.3848025798797607 + 0.001 * 7.8295159339904785
Epoch 130, val loss: 1.4631930589675903
Epoch 140, training loss: 1.313368320465088 = 1.305823564529419 + 0.001 * 7.544793128967285
Epoch 140, val loss: 1.4005182981491089
Epoch 150, training loss: 1.2338768243789673 = 1.226394772529602 + 0.001 * 7.481993198394775
Epoch 150, val loss: 1.3392525911331177
Epoch 160, training loss: 1.1533616781234741 = 1.1459424495697021 + 0.001 * 7.419278144836426
Epoch 160, val loss: 1.2788952589035034
Epoch 170, training loss: 1.0705653429031372 = 1.0631598234176636 + 0.001 * 7.405496120452881
Epoch 170, val loss: 1.2174347639083862
Epoch 180, training loss: 0.9844266772270203 = 0.9770277738571167 + 0.001 * 7.398913383483887
Epoch 180, val loss: 1.1534554958343506
Epoch 190, training loss: 0.8960716128349304 = 0.8886794447898865 + 0.001 * 7.392149448394775
Epoch 190, val loss: 1.0875896215438843
Epoch 200, training loss: 0.8085923790931702 = 0.8012072443962097 + 0.001 * 7.385108947753906
Epoch 200, val loss: 1.0234957933425903
Epoch 210, training loss: 0.7258205413818359 = 0.7184423804283142 + 0.001 * 7.3781890869140625
Epoch 210, val loss: 0.9655153751373291
Epoch 220, training loss: 0.6508700251579285 = 0.6434997916221619 + 0.001 * 7.37025260925293
Epoch 220, val loss: 0.9171422123908997
Epoch 230, training loss: 0.585123598575592 = 0.5777643918991089 + 0.001 * 7.359185218811035
Epoch 230, val loss: 0.8795982003211975
Epoch 240, training loss: 0.5279704332351685 = 0.5206288695335388 + 0.001 * 7.341544151306152
Epoch 240, val loss: 0.8522722125053406
Epoch 250, training loss: 0.47742408514022827 = 0.47010907530784607 + 0.001 * 7.315017223358154
Epoch 250, val loss: 0.8330862522125244
Epoch 260, training loss: 0.4311562776565552 = 0.42385876178741455 + 0.001 * 7.297518253326416
Epoch 260, val loss: 0.8195836544036865
Epoch 270, training loss: 0.387251615524292 = 0.37997186183929443 + 0.001 * 7.279754638671875
Epoch 270, val loss: 0.8099352717399597
Epoch 280, training loss: 0.3447631001472473 = 0.3374888300895691 + 0.001 * 7.274279594421387
Epoch 280, val loss: 0.8028697967529297
Epoch 290, training loss: 0.3038281500339508 = 0.29655447602272034 + 0.001 * 7.273673057556152
Epoch 290, val loss: 0.7980661988258362
Epoch 300, training loss: 0.26529282331466675 = 0.25801724195480347 + 0.001 * 7.275594711303711
Epoch 300, val loss: 0.7962571382522583
Epoch 310, training loss: 0.23015834391117096 = 0.22288209199905396 + 0.001 * 7.276248931884766
Epoch 310, val loss: 0.7982372641563416
Epoch 320, training loss: 0.19909153878688812 = 0.19181472063064575 + 0.001 * 7.276815891265869
Epoch 320, val loss: 0.8042133450508118
Epoch 330, training loss: 0.1723024547100067 = 0.16502459347248077 + 0.001 * 7.277865886688232
Epoch 330, val loss: 0.8140261769294739
Epoch 340, training loss: 0.1495359092950821 = 0.14225663244724274 + 0.001 * 7.279272079467773
Epoch 340, val loss: 0.8269074559211731
Epoch 350, training loss: 0.13031286001205444 = 0.12303302437067032 + 0.001 * 7.279841899871826
Epoch 350, val loss: 0.8422139883041382
Epoch 360, training loss: 0.11409574747085571 = 0.10681474953889847 + 0.001 * 7.281000137329102
Epoch 360, val loss: 0.8592690229415894
Epoch 370, training loss: 0.10038517415523529 = 0.09310083091259003 + 0.001 * 7.28433895111084
Epoch 370, val loss: 0.8775366544723511
Epoch 380, training loss: 0.08874984830617905 = 0.08146879076957703 + 0.001 * 7.281059265136719
Epoch 380, val loss: 0.8967409133911133
Epoch 390, training loss: 0.07885637879371643 = 0.07157480716705322 + 0.001 * 7.281572341918945
Epoch 390, val loss: 0.9164643883705139
Epoch 400, training loss: 0.07041805982589722 = 0.06313703954219818 + 0.001 * 7.281022071838379
Epoch 400, val loss: 0.9365295171737671
Epoch 410, training loss: 0.0632147416472435 = 0.05592834949493408 + 0.001 * 7.286391735076904
Epoch 410, val loss: 0.9566609859466553
Epoch 420, training loss: 0.05704498663544655 = 0.04976419731974602 + 0.001 * 7.280790328979492
Epoch 420, val loss: 0.9767337441444397
Epoch 430, training loss: 0.05176275223493576 = 0.04448259249329567 + 0.001 * 7.280158519744873
Epoch 430, val loss: 0.9964742064476013
Epoch 440, training loss: 0.04721970856189728 = 0.03994087129831314 + 0.001 * 7.278836727142334
Epoch 440, val loss: 1.0158363580703735
Epoch 450, training loss: 0.04330720379948616 = 0.036021627485752106 + 0.001 * 7.285576820373535
Epoch 450, val loss: 1.0347521305084229
Epoch 460, training loss: 0.03990395739674568 = 0.03262641653418541 + 0.001 * 7.277539253234863
Epoch 460, val loss: 1.0531668663024902
Epoch 470, training loss: 0.03694968670606613 = 0.02967258356511593 + 0.001 * 7.27710485458374
Epoch 470, val loss: 1.0710241794586182
Epoch 480, training loss: 0.034369274973869324 = 0.027091538533568382 + 0.001 * 7.277738094329834
Epoch 480, val loss: 1.08832848072052
Epoch 490, training loss: 0.03210315853357315 = 0.024826811626553535 + 0.001 * 7.276345252990723
Epoch 490, val loss: 1.1050541400909424
Epoch 500, training loss: 0.030119746923446655 = 0.022831719368696213 + 0.001 * 7.288026809692383
Epoch 500, val loss: 1.1212767362594604
Epoch 510, training loss: 0.028342491015791893 = 0.021066781133413315 + 0.001 * 7.27570915222168
Epoch 510, val loss: 1.1369110345840454
Epoch 520, training loss: 0.026772867888212204 = 0.019499218091368675 + 0.001 * 7.273649215698242
Epoch 520, val loss: 1.1520135402679443
Epoch 530, training loss: 0.025374187156558037 = 0.018101448193192482 + 0.001 * 7.272738456726074
Epoch 530, val loss: 1.166608214378357
Epoch 540, training loss: 0.024125710129737854 = 0.016850445419549942 + 0.001 * 7.275265216827393
Epoch 540, val loss: 1.1807209253311157
Epoch 550, training loss: 0.02299843542277813 = 0.015726802870631218 + 0.001 * 7.271632194519043
Epoch 550, val loss: 1.1943352222442627
Epoch 560, training loss: 0.021992821246385574 = 0.014714297838509083 + 0.001 * 7.278522968292236
Epoch 560, val loss: 1.207515001296997
Epoch 570, training loss: 0.0210663303732872 = 0.013799023814499378 + 0.001 * 7.267306804656982
Epoch 570, val loss: 1.220255732536316
Epoch 580, training loss: 0.020235484465956688 = 0.012969013303518295 + 0.001 * 7.266470909118652
Epoch 580, val loss: 1.2325491905212402
Epoch 590, training loss: 0.019478177651762962 = 0.0122140571475029 + 0.001 * 7.264119625091553
Epoch 590, val loss: 1.2444450855255127
Epoch 600, training loss: 0.018792033195495605 = 0.011525561101734638 + 0.001 * 7.266472816467285
Epoch 600, val loss: 1.2559858560562134
Epoch 610, training loss: 0.018181461840867996 = 0.010896049439907074 + 0.001 * 7.285411357879639
Epoch 610, val loss: 1.2671054601669312
Epoch 620, training loss: 0.01758449524641037 = 0.010319041088223457 + 0.001 * 7.2654547691345215
Epoch 620, val loss: 1.277909755706787
Epoch 630, training loss: 0.017049089074134827 = 0.009788828901946545 + 0.001 * 7.260260105133057
Epoch 630, val loss: 1.2883840799331665
Epoch 640, training loss: 0.016558296978473663 = 0.009300466626882553 + 0.001 * 7.25783109664917
Epoch 640, val loss: 1.298529028892517
Epoch 650, training loss: 0.01610526628792286 = 0.008849245496094227 + 0.001 * 7.256021022796631
Epoch 650, val loss: 1.3084509372711182
Epoch 660, training loss: 0.015715723857283592 = 0.0084309633821249 + 0.001 * 7.284759521484375
Epoch 660, val loss: 1.3180733919143677
Epoch 670, training loss: 0.0153017807751894 = 0.00804162584245205 + 0.001 * 7.2601542472839355
Epoch 670, val loss: 1.3275039196014404
Epoch 680, training loss: 0.014931007288396358 = 0.007677585817873478 + 0.001 * 7.253421306610107
Epoch 680, val loss: 1.3367494344711304
Epoch 690, training loss: 0.014592701569199562 = 0.007336143869906664 + 0.001 * 7.256557464599609
Epoch 690, val loss: 1.3458374738693237
Epoch 700, training loss: 0.01427172776311636 = 0.007015405222773552 + 0.001 * 7.256322383880615
Epoch 700, val loss: 1.354765772819519
Epoch 710, training loss: 0.013972427695989609 = 0.006713838316500187 + 0.001 * 7.258589267730713
Epoch 710, val loss: 1.3635172843933105
Epoch 720, training loss: 0.013688109815120697 = 0.006430155131965876 + 0.001 * 7.257953643798828
Epoch 720, val loss: 1.3721239566802979
Epoch 730, training loss: 0.013417025096714497 = 0.00616328464820981 + 0.001 * 7.253740310668945
Epoch 730, val loss: 1.380536437034607
Epoch 740, training loss: 0.013153741136193275 = 0.005912186577916145 + 0.001 * 7.2415547370910645
Epoch 740, val loss: 1.3887593746185303
Epoch 750, training loss: 0.012919582426548004 = 0.00567584065720439 + 0.001 * 7.243741035461426
Epoch 750, val loss: 1.396787405014038
Epoch 760, training loss: 0.012706732377409935 = 0.0054532866925001144 + 0.001 * 7.253444671630859
Epoch 760, val loss: 1.4046152830123901
Epoch 770, training loss: 0.012481652200222015 = 0.005243644583970308 + 0.001 * 7.238007068634033
Epoch 770, val loss: 1.4122769832611084
Epoch 780, training loss: 0.01231696829199791 = 0.005046057049185038 + 0.001 * 7.270910263061523
Epoch 780, val loss: 1.4196882247924805
Epoch 790, training loss: 0.012097854167222977 = 0.0048597720451653 + 0.001 * 7.238081932067871
Epoch 790, val loss: 1.4269969463348389
Epoch 800, training loss: 0.011921190656721592 = 0.004684035666286945 + 0.001 * 7.237154483795166
Epoch 800, val loss: 1.4340465068817139
Epoch 810, training loss: 0.011743447743356228 = 0.004518168978393078 + 0.001 * 7.225278377532959
Epoch 810, val loss: 1.4409981966018677
Epoch 820, training loss: 0.011593153700232506 = 0.004361472558230162 + 0.001 * 7.231680393218994
Epoch 820, val loss: 1.4477260112762451
Epoch 830, training loss: 0.011451657861471176 = 0.004213280044496059 + 0.001 * 7.238377094268799
Epoch 830, val loss: 1.4542657136917114
Epoch 840, training loss: 0.011292431503534317 = 0.004073088523000479 + 0.001 * 7.2193427085876465
Epoch 840, val loss: 1.4606633186340332
Epoch 850, training loss: 0.011163595132529736 = 0.003940373193472624 + 0.001 * 7.223221778869629
Epoch 850, val loss: 1.4669297933578491
Epoch 860, training loss: 0.011033154092729092 = 0.0038146188016980886 + 0.001 * 7.21853494644165
Epoch 860, val loss: 1.4729963541030884
Epoch 870, training loss: 0.010972591117024422 = 0.003695388790220022 + 0.001 * 7.2772016525268555
Epoch 870, val loss: 1.4789270162582397
Epoch 880, training loss: 0.010793976485729218 = 0.0035822021309286356 + 0.001 * 7.211773872375488
Epoch 880, val loss: 1.484757900238037
Epoch 890, training loss: 0.010691270232200623 = 0.00347459246404469 + 0.001 * 7.216677665710449
Epoch 890, val loss: 1.4904119968414307
Epoch 900, training loss: 0.010595658794045448 = 0.00337206874974072 + 0.001 * 7.2235894203186035
Epoch 900, val loss: 1.495957851409912
Epoch 910, training loss: 0.01048209983855486 = 0.003274285001680255 + 0.001 * 7.2078142166137695
Epoch 910, val loss: 1.5013353824615479
Epoch 920, training loss: 0.010395510122179985 = 0.0031807769555598497 + 0.001 * 7.214733123779297
Epoch 920, val loss: 1.506632924079895
Epoch 930, training loss: 0.010308257304131985 = 0.003091144608333707 + 0.001 * 7.2171125411987305
Epoch 930, val loss: 1.5117946863174438
Epoch 940, training loss: 0.010229094885289669 = 0.0030050461646169424 + 0.001 * 7.224048137664795
Epoch 940, val loss: 1.5168882608413696
Epoch 950, training loss: 0.010121078230440617 = 0.002922232961282134 + 0.001 * 7.198844909667969
Epoch 950, val loss: 1.5218979120254517
Epoch 960, training loss: 0.010045013390481472 = 0.0028426714707165956 + 0.001 * 7.202341556549072
Epoch 960, val loss: 1.526769757270813
Epoch 970, training loss: 0.00995566975325346 = 0.0027660636696964502 + 0.001 * 7.189605712890625
Epoch 970, val loss: 1.5315495729446411
Epoch 980, training loss: 0.009921927936375141 = 0.002692309906706214 + 0.001 * 7.229617595672607
Epoch 980, val loss: 1.5362697839736938
Epoch 990, training loss: 0.009801996871829033 = 0.0026212248485535383 + 0.001 * 7.180771350860596
Epoch 990, val loss: 1.5408921241760254
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8033737480231946
The final CL Acc:0.75556, 0.01839, The final GNN Acc:0.80478, 0.00199
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13242])
remove edge: torch.Size([2, 7780])
updated graph: torch.Size([2, 10466])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.956876516342163 = 1.948279619216919 + 0.001 * 8.596863746643066
Epoch 0, val loss: 1.9436211585998535
Epoch 10, training loss: 1.9470616579055786 = 1.938464879989624 + 0.001 * 8.596817016601562
Epoch 10, val loss: 1.9345024824142456
Epoch 20, training loss: 1.9346470832824707 = 1.9260504245758057 + 0.001 * 8.59666919708252
Epoch 20, val loss: 1.9227392673492432
Epoch 30, training loss: 1.9169321060180664 = 1.90833580493927 + 0.001 * 8.596314430236816
Epoch 30, val loss: 1.9057682752609253
Epoch 40, training loss: 1.8903894424438477 = 1.8817939758300781 + 0.001 * 8.595415115356445
Epoch 40, val loss: 1.880602478981018
Epoch 50, training loss: 1.8528474569320679 = 1.844254732131958 + 0.001 * 8.59277629852295
Epoch 50, val loss: 1.84664785861969
Epoch 60, training loss: 1.8109955787658691 = 1.8024121522903442 + 0.001 * 8.583425521850586
Epoch 60, val loss: 1.812647819519043
Epoch 70, training loss: 1.7756493091583252 = 1.7671046257019043 + 0.001 * 8.544742584228516
Epoch 70, val loss: 1.7845056056976318
Epoch 80, training loss: 1.7317272424697876 = 1.7233918905258179 + 0.001 * 8.33537769317627
Epoch 80, val loss: 1.7428308725357056
Epoch 90, training loss: 1.6694273948669434 = 1.6613434553146362 + 0.001 * 8.083898544311523
Epoch 90, val loss: 1.686295509338379
Epoch 100, training loss: 1.586829662322998 = 1.578806757926941 + 0.001 * 8.022844314575195
Epoch 100, val loss: 1.6161671876907349
Epoch 110, training loss: 1.4935929775238037 = 1.4856138229370117 + 0.001 * 7.979171276092529
Epoch 110, val loss: 1.5398213863372803
Epoch 120, training loss: 1.4023983478546143 = 1.3944876194000244 + 0.001 * 7.910675525665283
Epoch 120, val loss: 1.4672214984893799
Epoch 130, training loss: 1.3174713850021362 = 1.3097264766693115 + 0.001 * 7.744885444641113
Epoch 130, val loss: 1.4011722803115845
Epoch 140, training loss: 1.237981915473938 = 1.2304408550262451 + 0.001 * 7.541085243225098
Epoch 140, val loss: 1.3406294584274292
Epoch 150, training loss: 1.1623913049697876 = 1.1549293994903564 + 0.001 * 7.461912631988525
Epoch 150, val loss: 1.2838367223739624
Epoch 160, training loss: 1.0874063968658447 = 1.0800124406814575 + 0.001 * 7.393980026245117
Epoch 160, val loss: 1.227264404296875
Epoch 170, training loss: 1.0092217922210693 = 1.0018470287322998 + 0.001 * 7.3747878074646
Epoch 170, val loss: 1.166901707649231
Epoch 180, training loss: 0.9265080690383911 = 0.9191415905952454 + 0.001 * 7.3664937019348145
Epoch 180, val loss: 1.1014472246170044
Epoch 190, training loss: 0.8415505290031433 = 0.8341913223266602 + 0.001 * 7.359216213226318
Epoch 190, val loss: 1.0332566499710083
Epoch 200, training loss: 0.7588350176811218 = 0.7514832615852356 + 0.001 * 7.351778507232666
Epoch 200, val loss: 0.9668959975242615
Epoch 210, training loss: 0.6825690269470215 = 0.6752240657806396 + 0.001 * 7.344981670379639
Epoch 210, val loss: 0.9073461890220642
Epoch 220, training loss: 0.6149227619171143 = 0.6075844168663025 + 0.001 * 7.338334083557129
Epoch 220, val loss: 0.8572511076927185
Epoch 230, training loss: 0.5557729005813599 = 0.5484418272972107 + 0.001 * 7.331060886383057
Epoch 230, val loss: 0.8169553279876709
Epoch 240, training loss: 0.5035484433174133 = 0.4962259829044342 + 0.001 * 7.322484016418457
Epoch 240, val loss: 0.7848623394966125
Epoch 250, training loss: 0.45620274543762207 = 0.4488915801048279 + 0.001 * 7.311166763305664
Epoch 250, val loss: 0.7594394087791443
Epoch 260, training loss: 0.41188716888427734 = 0.4045926034450531 + 0.001 * 7.2945685386657715
Epoch 260, val loss: 0.738975465297699
Epoch 270, training loss: 0.3694795072078705 = 0.36220332980155945 + 0.001 * 7.276163101196289
Epoch 270, val loss: 0.7222204804420471
Epoch 280, training loss: 0.3287815749645233 = 0.3215363919734955 + 0.001 * 7.245175361633301
Epoch 280, val loss: 0.7085405588150024
Epoch 290, training loss: 0.29038262367248535 = 0.28315022587776184 + 0.001 * 7.232412338256836
Epoch 290, val loss: 0.6977161765098572
Epoch 300, training loss: 0.2549879848957062 = 0.24779576063156128 + 0.001 * 7.192234992980957
Epoch 300, val loss: 0.6899421811103821
Epoch 310, training loss: 0.22318652272224426 = 0.21601848304271698 + 0.001 * 7.168040752410889
Epoch 310, val loss: 0.6850621104240417
Epoch 320, training loss: 0.19519448280334473 = 0.18803571164608002 + 0.001 * 7.158772945404053
Epoch 320, val loss: 0.6830235123634338
Epoch 330, training loss: 0.1709340661764145 = 0.16378867626190186 + 0.001 * 7.145388603210449
Epoch 330, val loss: 0.6834340691566467
Epoch 340, training loss: 0.15013736486434937 = 0.14298728108406067 + 0.001 * 7.150081634521484
Epoch 340, val loss: 0.6858624219894409
Epoch 350, training loss: 0.13232704997062683 = 0.12518462538719177 + 0.001 * 7.142426490783691
Epoch 350, val loss: 0.6899210214614868
Epoch 360, training loss: 0.11703582853078842 = 0.10989253968000412 + 0.001 * 7.143289089202881
Epoch 360, val loss: 0.6952940821647644
Epoch 370, training loss: 0.10384499281644821 = 0.09669696539640427 + 0.001 * 7.1480255126953125
Epoch 370, val loss: 0.7017337083816528
Epoch 380, training loss: 0.09243220090866089 = 0.08528561145067215 + 0.001 * 7.146592617034912
Epoch 380, val loss: 0.7091476917266846
Epoch 390, training loss: 0.08256623148918152 = 0.07542061805725098 + 0.001 * 7.145615100860596
Epoch 390, val loss: 0.7173765301704407
Epoch 400, training loss: 0.07405083626508713 = 0.06690388172864914 + 0.001 * 7.146951675415039
Epoch 400, val loss: 0.7263975739479065
Epoch 410, training loss: 0.0667029470205307 = 0.05955672636628151 + 0.001 * 7.146216869354248
Epoch 410, val loss: 0.7360084652900696
Epoch 420, training loss: 0.06036772206425667 = 0.0532173290848732 + 0.001 * 7.150392055511475
Epoch 420, val loss: 0.7460506558418274
Epoch 430, training loss: 0.05488388240337372 = 0.047737184911966324 + 0.001 * 7.146696090698242
Epoch 430, val loss: 0.7564390897750854
Epoch 440, training loss: 0.050131238996982574 = 0.04298463463783264 + 0.001 * 7.146605491638184
Epoch 440, val loss: 0.767082154750824
Epoch 450, training loss: 0.04599791392683983 = 0.038848280906677246 + 0.001 * 7.149633407592773
Epoch 450, val loss: 0.777874767780304
Epoch 460, training loss: 0.04238630086183548 = 0.035235244780778885 + 0.001 * 7.15105676651001
Epoch 460, val loss: 0.7887587547302246
Epoch 470, training loss: 0.03921433910727501 = 0.032067906111478806 + 0.001 * 7.146432399749756
Epoch 470, val loss: 0.7996212840080261
Epoch 480, training loss: 0.036426253616809845 = 0.029280787333846092 + 0.001 * 7.145464897155762
Epoch 480, val loss: 0.8104202747344971
Epoch 490, training loss: 0.033963631838560104 = 0.026819508522748947 + 0.001 * 7.144123554229736
Epoch 490, val loss: 0.8210828304290771
Epoch 500, training loss: 0.03178338333964348 = 0.02463851496577263 + 0.001 * 7.144867420196533
Epoch 500, val loss: 0.8315595984458923
Epoch 510, training loss: 0.02984200231730938 = 0.022699568420648575 + 0.001 * 7.1424336433410645
Epoch 510, val loss: 0.841832160949707
Epoch 520, training loss: 0.028118636459112167 = 0.02097058668732643 + 0.001 * 7.148049354553223
Epoch 520, val loss: 0.8518874645233154
Epoch 530, training loss: 0.026567945256829262 = 0.019424183294177055 + 0.001 * 7.143762111663818
Epoch 530, val loss: 0.8617041707038879
Epoch 540, training loss: 0.02517784759402275 = 0.018037186935544014 + 0.001 * 7.140660285949707
Epoch 540, val loss: 0.8712765574455261
Epoch 550, training loss: 0.023932619020342827 = 0.016789549961686134 + 0.001 * 7.143068313598633
Epoch 550, val loss: 0.8805827498435974
Epoch 560, training loss: 0.02280125394463539 = 0.015664320439100266 + 0.001 * 7.136933326721191
Epoch 560, val loss: 0.8896450996398926
Epoch 570, training loss: 0.02178296260535717 = 0.014646998606622219 + 0.001 * 7.135962963104248
Epoch 570, val loss: 0.8984636664390564
Epoch 580, training loss: 0.020860979333519936 = 0.01372499205172062 + 0.001 * 7.135987281799316
Epoch 580, val loss: 0.9070287346839905
Epoch 590, training loss: 0.020024757832288742 = 0.012887310236692429 + 0.001 * 7.137448310852051
Epoch 590, val loss: 0.9153500199317932
Epoch 600, training loss: 0.019256290048360825 = 0.012124472297728062 + 0.001 * 7.131817817687988
Epoch 600, val loss: 0.9234592914581299
Epoch 610, training loss: 0.018559932708740234 = 0.011428107507526875 + 0.001 * 7.131824016571045
Epoch 610, val loss: 0.9313429594039917
Epoch 620, training loss: 0.017919128760695457 = 0.010790986940264702 + 0.001 * 7.128141403198242
Epoch 620, val loss: 0.9390081763267517
Epoch 630, training loss: 0.01733412593603134 = 0.010206716135144234 + 0.001 * 7.127410411834717
Epoch 630, val loss: 0.9464701414108276
Epoch 640, training loss: 0.016809135675430298 = 0.0096698347479105 + 0.001 * 7.139301300048828
Epoch 640, val loss: 0.9537245035171509
Epoch 650, training loss: 0.016304150223731995 = 0.009175503626465797 + 0.001 * 7.128645896911621
Epoch 650, val loss: 0.96077960729599
Epoch 660, training loss: 0.01584218628704548 = 0.008719414472579956 + 0.001 * 7.122771739959717
Epoch 660, val loss: 0.9676638245582581
Epoch 670, training loss: 0.01542747113853693 = 0.008297810330986977 + 0.001 * 7.129660606384277
Epoch 670, val loss: 0.9743576645851135
Epoch 680, training loss: 0.015022115781903267 = 0.00790737196803093 + 0.001 * 7.114743232727051
Epoch 680, val loss: 0.9808790683746338
Epoch 690, training loss: 0.014672242105007172 = 0.007545135449618101 + 0.001 * 7.127105712890625
Epoch 690, val loss: 0.9872357249259949
Epoch 700, training loss: 0.014328443445265293 = 0.0072085619904100895 + 0.001 * 7.1198811531066895
Epoch 700, val loss: 0.9934285283088684
Epoch 710, training loss: 0.014012264087796211 = 0.006895292084664106 + 0.001 * 7.116971015930176
Epoch 710, val loss: 0.9994469285011292
Epoch 720, training loss: 0.013716071844100952 = 0.00660322792828083 + 0.001 * 7.112843990325928
Epoch 720, val loss: 1.005324363708496
Epoch 730, training loss: 0.013441705144941807 = 0.006330519914627075 + 0.001 * 7.111185073852539
Epoch 730, val loss: 1.0110604763031006
Epoch 740, training loss: 0.013188318349421024 = 0.006075520068407059 + 0.001 * 7.112797737121582
Epoch 740, val loss: 1.0166575908660889
Epoch 750, training loss: 0.012943621724843979 = 0.00583674106746912 + 0.001 * 7.1068806648254395
Epoch 750, val loss: 1.022126317024231
Epoch 760, training loss: 0.012720242142677307 = 0.005612891633063555 + 0.001 * 7.107350826263428
Epoch 760, val loss: 1.0274516344070435
Epoch 770, training loss: 0.012511350214481354 = 0.005402747541666031 + 0.001 * 7.108603000640869
Epoch 770, val loss: 1.0326652526855469
Epoch 780, training loss: 0.012301869690418243 = 0.005205165129154921 + 0.001 * 7.096704006195068
Epoch 780, val loss: 1.0377520322799683
Epoch 790, training loss: 0.01213337853550911 = 0.0050191860646009445 + 0.001 * 7.114192008972168
Epoch 790, val loss: 1.0427286624908447
Epoch 800, training loss: 0.011943688616156578 = 0.004843945149332285 + 0.001 * 7.099742889404297
Epoch 800, val loss: 1.0475910902023315
Epoch 810, training loss: 0.011783121153712273 = 0.004678587429225445 + 0.001 * 7.1045331954956055
Epoch 810, val loss: 1.0523335933685303
Epoch 820, training loss: 0.011615908704698086 = 0.004522399045526981 + 0.001 * 7.093509197235107
Epoch 820, val loss: 1.0569952726364136
Epoch 830, training loss: 0.011461589485406876 = 0.004374746233224869 + 0.001 * 7.086843013763428
Epoch 830, val loss: 1.0615379810333252
Epoch 840, training loss: 0.01133987121284008 = 0.004235037602484226 + 0.001 * 7.104833126068115
Epoch 840, val loss: 1.0659847259521484
Epoch 850, training loss: 0.011188870295882225 = 0.004102685023099184 + 0.001 * 7.086185455322266
Epoch 850, val loss: 1.0703200101852417
Epoch 860, training loss: 0.0110931983217597 = 0.0039771683514118195 + 0.001 * 7.116029739379883
Epoch 860, val loss: 1.0745753049850464
Epoch 870, training loss: 0.010944690555334091 = 0.003858038689941168 + 0.001 * 7.08665132522583
Epoch 870, val loss: 1.078731656074524
Epoch 880, training loss: 0.010823643766343594 = 0.003744879039004445 + 0.001 * 7.078763961791992
Epoch 880, val loss: 1.0827981233596802
Epoch 890, training loss: 0.010711216367781162 = 0.003637295449152589 + 0.001 * 7.073920249938965
Epoch 890, val loss: 1.0867856740951538
Epoch 900, training loss: 0.010607567615807056 = 0.003534960560500622 + 0.001 * 7.072606563568115
Epoch 900, val loss: 1.0906906127929688
Epoch 910, training loss: 0.010509029030799866 = 0.0034374946262687445 + 0.001 * 7.071534156799316
Epoch 910, val loss: 1.094508409500122
Epoch 920, training loss: 0.010423876345157623 = 0.0033446052111685276 + 0.001 * 7.079270362854004
Epoch 920, val loss: 1.0982476472854614
Epoch 930, training loss: 0.010321025736629963 = 0.003256016643717885 + 0.001 * 7.065009117126465
Epoch 930, val loss: 1.1018961668014526
Epoch 940, training loss: 0.010242994874715805 = 0.0031714686192572117 + 0.001 * 7.071525573730469
Epoch 940, val loss: 1.1055032014846802
Epoch 950, training loss: 0.010149683803319931 = 0.00309071596711874 + 0.001 * 7.0589680671691895
Epoch 950, val loss: 1.1090031862258911
Epoch 960, training loss: 0.010104049928486347 = 0.0030135244596749544 + 0.001 * 7.090525150299072
Epoch 960, val loss: 1.1124178171157837
Epoch 970, training loss: 0.01000463031232357 = 0.0029397308826446533 + 0.001 * 7.064898490905762
Epoch 970, val loss: 1.1157965660095215
Epoch 980, training loss: 0.009950589388608932 = 0.002869084943085909 + 0.001 * 7.081503868103027
Epoch 980, val loss: 1.1191127300262451
Epoch 990, training loss: 0.009850969538092613 = 0.0028014371637254953 + 0.001 * 7.049531936645508
Epoch 990, val loss: 1.1223262548446655
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 1.9503812789916992 = 1.941784381866455 + 0.001 * 8.596837043762207
Epoch 0, val loss: 1.9313026666641235
Epoch 10, training loss: 1.9400498867034912 = 1.9314531087875366 + 0.001 * 8.596773147583008
Epoch 10, val loss: 1.9208444356918335
Epoch 20, training loss: 1.9274448156356812 = 1.9188482761383057 + 0.001 * 8.596546173095703
Epoch 20, val loss: 1.9079372882843018
Epoch 30, training loss: 1.9100757837295532 = 1.9014798402786255 + 0.001 * 8.595951080322266
Epoch 30, val loss: 1.8904070854187012
Epoch 40, training loss: 1.8849539756774902 = 1.8763598203659058 + 0.001 * 8.594195365905762
Epoch 40, val loss: 1.8659108877182007
Epoch 50, training loss: 1.8506239652633667 = 1.8420358896255493 + 0.001 * 8.588106155395508
Epoch 50, val loss: 1.834632396697998
Epoch 60, training loss: 1.8120571374893188 = 1.8034942150115967 + 0.001 * 8.56290340423584
Epoch 60, val loss: 1.803868055343628
Epoch 70, training loss: 1.7755120992660522 = 1.7670624256134033 + 0.001 * 8.449676513671875
Epoch 70, val loss: 1.7774543762207031
Epoch 80, training loss: 1.7307720184326172 = 1.7226405143737793 + 0.001 * 8.13146686553955
Epoch 80, val loss: 1.7398674488067627
Epoch 90, training loss: 1.6690782308578491 = 1.6610829830169678 + 0.001 * 7.995248317718506
Epoch 90, val loss: 1.6845049858093262
Epoch 100, training loss: 1.5873358249664307 = 1.5794975757598877 + 0.001 * 7.838255405426025
Epoch 100, val loss: 1.614646077156067
Epoch 110, training loss: 1.4906220436096191 = 1.4829134941101074 + 0.001 * 7.708524227142334
Epoch 110, val loss: 1.536323070526123
Epoch 120, training loss: 1.3891812562942505 = 1.3815280199050903 + 0.001 * 7.65325403213501
Epoch 120, val loss: 1.4545655250549316
Epoch 130, training loss: 1.289435863494873 = 1.281833291053772 + 0.001 * 7.602566242218018
Epoch 130, val loss: 1.375901460647583
Epoch 140, training loss: 1.1951109170913696 = 1.1875834465026855 + 0.001 * 7.527441501617432
Epoch 140, val loss: 1.301984190940857
Epoch 150, training loss: 1.108708143234253 = 1.1012603044509888 + 0.001 * 7.447862148284912
Epoch 150, val loss: 1.2350419759750366
Epoch 160, training loss: 1.0325371026992798 = 1.025132656097412 + 0.001 * 7.40447998046875
Epoch 160, val loss: 1.1777149438858032
Epoch 170, training loss: 0.9658650755882263 = 0.9584901332855225 + 0.001 * 7.374945640563965
Epoch 170, val loss: 1.1294206380844116
Epoch 180, training loss: 0.9049015045166016 = 0.8975477814674377 + 0.001 * 7.353743553161621
Epoch 180, val loss: 1.0860661268234253
Epoch 190, training loss: 0.8447901606559753 = 0.8374497294425964 + 0.001 * 7.3404059410095215
Epoch 190, val loss: 1.0429470539093018
Epoch 200, training loss: 0.7813766598701477 = 0.7740516066551208 + 0.001 * 7.325036525726318
Epoch 200, val loss: 0.9963629841804504
Epoch 210, training loss: 0.7126404643058777 = 0.7053354382514954 + 0.001 * 7.305025100708008
Epoch 210, val loss: 0.9451758861541748
Epoch 220, training loss: 0.6398071646690369 = 0.6325228810310364 + 0.001 * 7.284285068511963
Epoch 220, val loss: 0.8918539881706238
Epoch 230, training loss: 0.5666103959083557 = 0.5593497157096863 + 0.001 * 7.260672092437744
Epoch 230, val loss: 0.8409020304679871
Epoch 240, training loss: 0.4975014328956604 = 0.490261048078537 + 0.001 * 7.240385055541992
Epoch 240, val loss: 0.7969081997871399
Epoch 250, training loss: 0.43543457984924316 = 0.42821404337882996 + 0.001 * 7.220536231994629
Epoch 250, val loss: 0.7627339363098145
Epoch 260, training loss: 0.3812411427497864 = 0.37402835488319397 + 0.001 * 7.212782859802246
Epoch 260, val loss: 0.7389150857925415
Epoch 270, training loss: 0.3340555727481842 = 0.32684844732284546 + 0.001 * 7.207135200500488
Epoch 270, val loss: 0.7240859866142273
Epoch 280, training loss: 0.29234033823013306 = 0.2851378619670868 + 0.001 * 7.2024736404418945
Epoch 280, val loss: 0.7163932919502258
Epoch 290, training loss: 0.2548690438270569 = 0.2476685345172882 + 0.001 * 7.20049524307251
Epoch 290, val loss: 0.7144248485565186
Epoch 300, training loss: 0.2211838811635971 = 0.21398824453353882 + 0.001 * 7.195633411407471
Epoch 300, val loss: 0.7171891927719116
Epoch 310, training loss: 0.19143199920654297 = 0.18423894047737122 + 0.001 * 7.193051815032959
Epoch 310, val loss: 0.7240902185440063
Epoch 320, training loss: 0.16578561067581177 = 0.15859486162662506 + 0.001 * 7.190751075744629
Epoch 320, val loss: 0.7345154881477356
Epoch 330, training loss: 0.14409327507019043 = 0.13690488040447235 + 0.001 * 7.188398361206055
Epoch 330, val loss: 0.7476689219474792
Epoch 340, training loss: 0.1259137988090515 = 0.11872642487287521 + 0.001 * 7.187368392944336
Epoch 340, val loss: 0.7628013491630554
Epoch 350, training loss: 0.11070100218057632 = 0.10351306945085526 + 0.001 * 7.187931060791016
Epoch 350, val loss: 0.7792186141014099
Epoch 360, training loss: 0.09793052822351456 = 0.09074413776397705 + 0.001 * 7.186388969421387
Epoch 360, val loss: 0.7964056134223938
Epoch 370, training loss: 0.08715946972370148 = 0.07997466623783112 + 0.001 * 7.184805870056152
Epoch 370, val loss: 0.8140447735786438
Epoch 380, training loss: 0.07802316546440125 = 0.07083975523710251 + 0.001 * 7.183412075042725
Epoch 380, val loss: 0.831885576248169
Epoch 390, training loss: 0.070230633020401 = 0.0630478635430336 + 0.001 * 7.18276834487915
Epoch 390, val loss: 0.8497862815856934
Epoch 400, training loss: 0.06355337798595428 = 0.05636947974562645 + 0.001 * 7.183895111083984
Epoch 400, val loss: 0.8675881028175354
Epoch 410, training loss: 0.05780286714434624 = 0.050619371235370636 + 0.001 * 7.18349552154541
Epoch 410, val loss: 0.885150671005249
Epoch 420, training loss: 0.05282736197113991 = 0.04564560949802399 + 0.001 * 7.181753635406494
Epoch 420, val loss: 0.9024031162261963
Epoch 430, training loss: 0.04850341007113457 = 0.04132356122136116 + 0.001 * 7.179849147796631
Epoch 430, val loss: 0.9192696809768677
Epoch 440, training loss: 0.04472936689853668 = 0.037550922483205795 + 0.001 * 7.178443431854248
Epoch 440, val loss: 0.9357144236564636
Epoch 450, training loss: 0.04142534360289574 = 0.034242402762174606 + 0.001 * 7.1829400062561035
Epoch 450, val loss: 0.9517320394515991
Epoch 460, training loss: 0.03851614519953728 = 0.03132825717329979 + 0.001 * 7.187887668609619
Epoch 460, val loss: 0.9673094749450684
Epoch 470, training loss: 0.0359300896525383 = 0.028749149292707443 + 0.001 * 7.180941581726074
Epoch 470, val loss: 0.9823893308639526
Epoch 480, training loss: 0.03363315761089325 = 0.026457130908966064 + 0.001 * 7.176027774810791
Epoch 480, val loss: 0.9970512390136719
Epoch 490, training loss: 0.03158513456583023 = 0.02441275492310524 + 0.001 * 7.172379493713379
Epoch 490, val loss: 1.0112934112548828
Epoch 500, training loss: 0.0297594852745533 = 0.022582706063985825 + 0.001 * 7.176779270172119
Epoch 500, val loss: 1.0251437425613403
Epoch 510, training loss: 0.02812766656279564 = 0.02093937247991562 + 0.001 * 7.18829345703125
Epoch 510, val loss: 1.0386027097702026
Epoch 520, training loss: 0.026628270745277405 = 0.019459232687950134 + 0.001 * 7.16903829574585
Epoch 520, val loss: 1.0516796112060547
Epoch 530, training loss: 0.02529006451368332 = 0.01812245324254036 + 0.001 * 7.167612075805664
Epoch 530, val loss: 1.0643737316131592
Epoch 540, training loss: 0.024075910449028015 = 0.016912220045924187 + 0.001 * 7.163690090179443
Epoch 540, val loss: 1.0767236948013306
Epoch 550, training loss: 0.02299165539443493 = 0.015814030542969704 + 0.001 * 7.177624702453613
Epoch 550, val loss: 1.088708758354187
Epoch 560, training loss: 0.02197420224547386 = 0.014815336093306541 + 0.001 * 7.158864974975586
Epoch 560, val loss: 1.1003907918930054
Epoch 570, training loss: 0.0210714153945446 = 0.013905436731874943 + 0.001 * 7.165977478027344
Epoch 570, val loss: 1.1117563247680664
Epoch 580, training loss: 0.020228950306773186 = 0.013074735179543495 + 0.001 * 7.154215335845947
Epoch 580, val loss: 1.122772455215454
Epoch 590, training loss: 0.01946147158741951 = 0.012314906343817711 + 0.001 * 7.146564960479736
Epoch 590, val loss: 1.1335276365280151
Epoch 600, training loss: 0.018804676830768585 = 0.01161857508122921 + 0.001 * 7.186100482940674
Epoch 600, val loss: 1.1439844369888306
Epoch 610, training loss: 0.018126102164387703 = 0.010979292914271355 + 0.001 * 7.146808624267578
Epoch 610, val loss: 1.154151439666748
Epoch 620, training loss: 0.01759210042655468 = 0.01039138063788414 + 0.001 * 7.200719356536865
Epoch 620, val loss: 1.1640435457229614
Epoch 630, training loss: 0.01698584109544754 = 0.009849734604358673 + 0.001 * 7.136105060577393
Epoch 630, val loss: 1.1736818552017212
Epoch 640, training loss: 0.016483351588249207 = 0.00934987049549818 + 0.001 * 7.133480072021484
Epoch 640, val loss: 1.183044195175171
Epoch 650, training loss: 0.01604863442480564 = 0.008887778967618942 + 0.001 * 7.160855770111084
Epoch 650, val loss: 1.1921591758728027
Epoch 660, training loss: 0.015601877123117447 = 0.008459868840873241 + 0.001 * 7.142007350921631
Epoch 660, val loss: 1.2010598182678223
Epoch 670, training loss: 0.01518692634999752 = 0.008063015528023243 + 0.001 * 7.123910903930664
Epoch 670, val loss: 1.2097163200378418
Epoch 680, training loss: 0.014805193990468979 = 0.007694372907280922 + 0.001 * 7.110820293426514
Epoch 680, val loss: 1.218159794807434
Epoch 690, training loss: 0.014461442828178406 = 0.007351398933678865 + 0.001 * 7.110043525695801
Epoch 690, val loss: 1.2263861894607544
Epoch 700, training loss: 0.014154823496937752 = 0.0070318724028766155 + 0.001 * 7.122950077056885
Epoch 700, val loss: 1.2344069480895996
Epoch 710, training loss: 0.013844326138496399 = 0.006733778864145279 + 0.001 * 7.110547065734863
Epoch 710, val loss: 1.2422196865081787
Epoch 720, training loss: 0.013586143031716347 = 0.00645526684820652 + 0.001 * 7.130876541137695
Epoch 720, val loss: 1.2498321533203125
Epoch 730, training loss: 0.013293630443513393 = 0.006194683723151684 + 0.001 * 7.098946571350098
Epoch 730, val loss: 1.2572755813598633
Epoch 740, training loss: 0.013050667010247707 = 0.005950582679361105 + 0.001 * 7.100083827972412
Epoch 740, val loss: 1.2645128965377808
Epoch 750, training loss: 0.012827935628592968 = 0.00572162214666605 + 0.001 * 7.106313228607178
Epoch 750, val loss: 1.271620273590088
Epoch 760, training loss: 0.012608395889401436 = 0.005506576504558325 + 0.001 * 7.101818561553955
Epoch 760, val loss: 1.2785394191741943
Epoch 770, training loss: 0.012393992394208908 = 0.005304368678480387 + 0.001 * 7.08962345123291
Epoch 770, val loss: 1.2852939367294312
Epoch 780, training loss: 0.012224830687046051 = 0.005114033352583647 + 0.001 * 7.110796928405762
Epoch 780, val loss: 1.2919000387191772
Epoch 790, training loss: 0.01202896423637867 = 0.00493465643376112 + 0.001 * 7.094307899475098
Epoch 790, val loss: 1.2983533143997192
Epoch 800, training loss: 0.011874179355800152 = 0.004765428602695465 + 0.001 * 7.108750343322754
Epoch 800, val loss: 1.3046716451644897
Epoch 810, training loss: 0.011683868244290352 = 0.00460558757185936 + 0.001 * 7.078280448913574
Epoch 810, val loss: 1.310853123664856
Epoch 820, training loss: 0.011527812108397484 = 0.004454481415450573 + 0.001 * 7.073329925537109
Epoch 820, val loss: 1.3168858289718628
Epoch 830, training loss: 0.011387459933757782 = 0.004311489872634411 + 0.001 * 7.075969219207764
Epoch 830, val loss: 1.322806477546692
Epoch 840, training loss: 0.011265110224485397 = 0.004176021087914705 + 0.001 * 7.089089393615723
Epoch 840, val loss: 1.3285703659057617
Epoch 850, training loss: 0.011117867194116116 = 0.0040476019494235516 + 0.001 * 7.07026481628418
Epoch 850, val loss: 1.3342443704605103
Epoch 860, training loss: 0.010995018295943737 = 0.003925736993551254 + 0.001 * 7.069281101226807
Epoch 860, val loss: 1.3397789001464844
Epoch 870, training loss: 0.010896938852965832 = 0.003809988731518388 + 0.001 * 7.086949825286865
Epoch 870, val loss: 1.3452181816101074
Epoch 880, training loss: 0.010771322064101696 = 0.0036999403964728117 + 0.001 * 7.071381092071533
Epoch 880, val loss: 1.350525140762329
Epoch 890, training loss: 0.010659166611731052 = 0.0035952599719166756 + 0.001 * 7.063906192779541
Epoch 890, val loss: 1.3557180166244507
Epoch 900, training loss: 0.01055152527987957 = 0.0034955907613039017 + 0.001 * 7.055934429168701
Epoch 900, val loss: 1.3608393669128418
Epoch 910, training loss: 0.010455702431499958 = 0.0034006107598543167 + 0.001 * 7.055091381072998
Epoch 910, val loss: 1.3658169507980347
Epoch 920, training loss: 0.010360339656472206 = 0.003310054074972868 + 0.001 * 7.0502848625183105
Epoch 920, val loss: 1.370717167854309
Epoch 930, training loss: 0.010275053791701794 = 0.0032236434053629637 + 0.001 * 7.05141019821167
Epoch 930, val loss: 1.375517725944519
Epoch 940, training loss: 0.010185258463025093 = 0.0031411193776875734 + 0.001 * 7.044138431549072
Epoch 940, val loss: 1.3802217245101929
Epoch 950, training loss: 0.010114931501448154 = 0.0030622573103755713 + 0.001 * 7.052674293518066
Epoch 950, val loss: 1.38483726978302
Epoch 960, training loss: 0.010030129924416542 = 0.0029868497513234615 + 0.001 * 7.043280124664307
Epoch 960, val loss: 1.389376163482666
Epoch 970, training loss: 0.009954624809324741 = 0.0029146948363631964 + 0.001 * 7.039929389953613
Epoch 970, val loss: 1.3938108682632446
Epoch 980, training loss: 0.009903310798108578 = 0.0028455958236008883 + 0.001 * 7.057714939117432
Epoch 980, val loss: 1.398168683052063
Epoch 990, training loss: 0.009825550951063633 = 0.002779406728222966 + 0.001 * 7.046144008636475
Epoch 990, val loss: 1.4024546146392822
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8434370057986295
=== training gcn model ===
Epoch 0, training loss: 1.9549834728240967 = 1.9463865756988525 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.9421271085739136
Epoch 10, training loss: 1.943763256072998 = 1.9351664781570435 + 0.001 * 8.596763610839844
Epoch 10, val loss: 1.9309515953063965
Epoch 20, training loss: 1.9293047189712524 = 1.920708179473877 + 0.001 * 8.596522331237793
Epoch 20, val loss: 1.91616952419281
Epoch 30, training loss: 1.9085546731948853 = 1.8999587297439575 + 0.001 * 8.595931053161621
Epoch 30, val loss: 1.894974708557129
Epoch 40, training loss: 1.8783128261566162 = 1.8697184324264526 + 0.001 * 8.594369888305664
Epoch 40, val loss: 1.8649104833602905
Epoch 50, training loss: 1.838736891746521 = 1.8301472663879395 + 0.001 * 8.589579582214355
Epoch 50, val loss: 1.8285268545150757
Epoch 60, training loss: 1.8003371953964233 = 1.7917661666870117 + 0.001 * 8.571026802062988
Epoch 60, val loss: 1.7983479499816895
Epoch 70, training loss: 1.7653151750564575 = 1.756839394569397 + 0.001 * 8.475741386413574
Epoch 70, val loss: 1.7701842784881592
Epoch 80, training loss: 1.7158149480819702 = 1.7076784372329712 + 0.001 * 8.13652229309082
Epoch 80, val loss: 1.7265386581420898
Epoch 90, training loss: 1.6482943296432495 = 1.640223503112793 + 0.001 * 8.070801734924316
Epoch 90, val loss: 1.6683878898620605
Epoch 100, training loss: 1.564133644104004 = 1.5560990571975708 + 0.001 * 8.034567832946777
Epoch 100, val loss: 1.5992556810379028
Epoch 110, training loss: 1.4786553382873535 = 1.4706648588180542 + 0.001 * 7.990470886230469
Epoch 110, val loss: 1.5301648378372192
Epoch 120, training loss: 1.3996124267578125 = 1.3917473554611206 + 0.001 * 7.865074157714844
Epoch 120, val loss: 1.4687979221343994
Epoch 130, training loss: 1.3239948749542236 = 1.3163734674453735 + 0.001 * 7.621377944946289
Epoch 130, val loss: 1.411037802696228
Epoch 140, training loss: 1.2465037107467651 = 1.2389096021652222 + 0.001 * 7.594089508056641
Epoch 140, val loss: 1.3513891696929932
Epoch 150, training loss: 1.1633402109146118 = 1.1557902097702026 + 0.001 * 7.550033092498779
Epoch 150, val loss: 1.2878109216690063
Epoch 160, training loss: 1.074403166770935 = 1.0668954849243164 + 0.001 * 7.507718086242676
Epoch 160, val loss: 1.2213146686553955
Epoch 170, training loss: 0.9820738434791565 = 0.9745985269546509 + 0.001 * 7.475337505340576
Epoch 170, val loss: 1.1523669958114624
Epoch 180, training loss: 0.8904514908790588 = 0.8830075263977051 + 0.001 * 7.443939685821533
Epoch 180, val loss: 1.084101915359497
Epoch 190, training loss: 0.8042882680892944 = 0.7968859076499939 + 0.001 * 7.402383804321289
Epoch 190, val loss: 1.020371675491333
Epoch 200, training loss: 0.7268835306167603 = 0.7195214629173279 + 0.001 * 7.362060070037842
Epoch 200, val loss: 0.9650818109512329
Epoch 210, training loss: 0.6590505838394165 = 0.6517021059989929 + 0.001 * 7.348453998565674
Epoch 210, val loss: 0.9198154807090759
Epoch 220, training loss: 0.5996503829956055 = 0.5923037528991699 + 0.001 * 7.346626281738281
Epoch 220, val loss: 0.8844540119171143
Epoch 230, training loss: 0.5468331575393677 = 0.5394883751869202 + 0.001 * 7.344797611236572
Epoch 230, val loss: 0.8576394319534302
Epoch 240, training loss: 0.4989995062351227 = 0.491658478975296 + 0.001 * 7.341038703918457
Epoch 240, val loss: 0.8378369808197021
Epoch 250, training loss: 0.45491206645965576 = 0.4475748836994171 + 0.001 * 7.337184906005859
Epoch 250, val loss: 0.8236792087554932
Epoch 260, training loss: 0.41370198130607605 = 0.4063694179058075 + 0.001 * 7.332563400268555
Epoch 260, val loss: 0.8141453862190247
Epoch 270, training loss: 0.37482360005378723 = 0.36749565601348877 + 0.001 * 7.327949047088623
Epoch 270, val loss: 0.8080117106437683
Epoch 280, training loss: 0.3380211889743805 = 0.33070334792137146 + 0.001 * 7.3178534507751465
Epoch 280, val loss: 0.8044758439064026
Epoch 290, training loss: 0.3032131791114807 = 0.2959020733833313 + 0.001 * 7.311104774475098
Epoch 290, val loss: 0.8032650947570801
Epoch 300, training loss: 0.2705594003200531 = 0.26326778531074524 + 0.001 * 7.291626930236816
Epoch 300, val loss: 0.8046684265136719
Epoch 310, training loss: 0.2404434233903885 = 0.23317214846611023 + 0.001 * 7.2712721824646
Epoch 310, val loss: 0.8087639212608337
Epoch 320, training loss: 0.2131732702255249 = 0.2058904618024826 + 0.001 * 7.282811164855957
Epoch 320, val loss: 0.8155530095100403
Epoch 330, training loss: 0.18882915377616882 = 0.18157409131526947 + 0.001 * 7.255055904388428
Epoch 330, val loss: 0.8247964382171631
Epoch 340, training loss: 0.1674167811870575 = 0.16017946600914001 + 0.001 * 7.237318992614746
Epoch 340, val loss: 0.8366486430168152
Epoch 350, training loss: 0.14868152141571045 = 0.1414589285850525 + 0.001 * 7.222599983215332
Epoch 350, val loss: 0.8507975935935974
Epoch 360, training loss: 0.13233168423175812 = 0.1251058429479599 + 0.001 * 7.225836753845215
Epoch 360, val loss: 0.8668906092643738
Epoch 370, training loss: 0.11804284155368805 = 0.11083073168992996 + 0.001 * 7.212105751037598
Epoch 370, val loss: 0.8845797181129456
Epoch 380, training loss: 0.10556977242231369 = 0.09836765378713608 + 0.001 * 7.202116012573242
Epoch 380, val loss: 0.9033179879188538
Epoch 390, training loss: 0.09468366205692291 = 0.08748137205839157 + 0.001 * 7.202291011810303
Epoch 390, val loss: 0.9227845072746277
Epoch 400, training loss: 0.08516662567853928 = 0.07796641439199448 + 0.001 * 7.200211524963379
Epoch 400, val loss: 0.9426754713058472
Epoch 410, training loss: 0.07684312760829926 = 0.06964435428380966 + 0.001 * 7.19877290725708
Epoch 410, val loss: 0.9627695679664612
Epoch 420, training loss: 0.06956295669078827 = 0.06236260011792183 + 0.001 * 7.2003583908081055
Epoch 420, val loss: 0.9828720688819885
Epoch 430, training loss: 0.06319230049848557 = 0.055985771119594574 + 0.001 * 7.206526279449463
Epoch 430, val loss: 1.0027495622634888
Epoch 440, training loss: 0.05760246142745018 = 0.05040128156542778 + 0.001 * 7.201180934906006
Epoch 440, val loss: 1.022172212600708
Epoch 450, training loss: 0.052701663225889206 = 0.04550516977906227 + 0.001 * 7.1964945793151855
Epoch 450, val loss: 1.041220784187317
Epoch 460, training loss: 0.04841585084795952 = 0.04120740294456482 + 0.001 * 7.208448886871338
Epoch 460, val loss: 1.0597668886184692
Epoch 470, training loss: 0.04462837800383568 = 0.03742824122309685 + 0.001 * 7.200137615203857
Epoch 470, val loss: 1.0779070854187012
Epoch 480, training loss: 0.04129766672849655 = 0.03409871459007263 + 0.001 * 7.198951721191406
Epoch 480, val loss: 1.0955252647399902
Epoch 490, training loss: 0.038351502269506454 = 0.03115847148001194 + 0.001 * 7.1930317878723145
Epoch 490, val loss: 1.1126258373260498
Epoch 500, training loss: 0.0357687883079052 = 0.028555698692798615 + 0.001 * 7.213089466094971
Epoch 500, val loss: 1.1292200088500977
Epoch 510, training loss: 0.03343932703137398 = 0.02624507248401642 + 0.001 * 7.194253444671631
Epoch 510, val loss: 1.1452345848083496
Epoch 520, training loss: 0.03137689083814621 = 0.02418818697333336 + 0.001 * 7.188703536987305
Epoch 520, val loss: 1.1607766151428223
Epoch 530, training loss: 0.029539089649915695 = 0.022352317348122597 + 0.001 * 7.186772346496582
Epoch 530, val loss: 1.1758710145950317
Epoch 540, training loss: 0.027908682823181152 = 0.020708978176116943 + 0.001 * 7.199704647064209
Epoch 540, val loss: 1.1904792785644531
Epoch 550, training loss: 0.026426896452903748 = 0.019234085455536842 + 0.001 * 7.192809581756592
Epoch 550, val loss: 1.204648733139038
Epoch 560, training loss: 0.02509230561554432 = 0.017906934022903442 + 0.001 * 7.185371398925781
Epoch 560, val loss: 1.2184044122695923
Epoch 570, training loss: 0.023890815675258636 = 0.016709517687559128 + 0.001 * 7.181298732757568
Epoch 570, val loss: 1.2317067384719849
Epoch 580, training loss: 0.02281426265835762 = 0.015626292675733566 + 0.001 * 7.1879706382751465
Epoch 580, val loss: 1.2446117401123047
Epoch 590, training loss: 0.02181839384138584 = 0.01464392151683569 + 0.001 * 7.174471378326416
Epoch 590, val loss: 1.2571030855178833
Epoch 600, training loss: 0.02092505246400833 = 0.013750838115811348 + 0.001 * 7.1742143630981445
Epoch 600, val loss: 1.2692164182662964
Epoch 610, training loss: 0.02011096477508545 = 0.012937037274241447 + 0.001 * 7.173927307128906
Epoch 610, val loss: 1.2809687852859497
Epoch 620, training loss: 0.01936623454093933 = 0.012193786911666393 + 0.001 * 7.172447681427002
Epoch 620, val loss: 1.2923632860183716
Epoch 630, training loss: 0.018692869693040848 = 0.011513559147715569 + 0.001 * 7.1793107986450195
Epoch 630, val loss: 1.3034380674362183
Epoch 640, training loss: 0.018049068748950958 = 0.010889554396271706 + 0.001 * 7.159514427185059
Epoch 640, val loss: 1.3141759634017944
Epoch 650, training loss: 0.01747341826558113 = 0.01031600870192051 + 0.001 * 7.157408237457275
Epoch 650, val loss: 1.3245996236801147
Epoch 660, training loss: 0.016942694783210754 = 0.009787747636437416 + 0.001 * 7.1549458503723145
Epoch 660, val loss: 1.3347091674804688
Epoch 670, training loss: 0.016457675024867058 = 0.009300237521529198 + 0.001 * 7.157437801361084
Epoch 670, val loss: 1.3445284366607666
Epoch 680, training loss: 0.016013342887163162 = 0.008849501609802246 + 0.001 * 7.163840293884277
Epoch 680, val loss: 1.3540594577789307
Epoch 690, training loss: 0.015598675236105919 = 0.008432029746472836 + 0.001 * 7.166645050048828
Epoch 690, val loss: 1.3633016347885132
Epoch 700, training loss: 0.01521051675081253 = 0.008044755086302757 + 0.001 * 7.165761470794678
Epoch 700, val loss: 1.3722823858261108
Epoch 710, training loss: 0.014826660044491291 = 0.0076849269680678844 + 0.001 * 7.141732692718506
Epoch 710, val loss: 1.3810070753097534
Epoch 720, training loss: 0.014489514753222466 = 0.007350037340074778 + 0.001 * 7.139476776123047
Epoch 720, val loss: 1.3894869089126587
Epoch 730, training loss: 0.014168670400977135 = 0.007037794217467308 + 0.001 * 7.130875587463379
Epoch 730, val loss: 1.3977168798446655
Epoch 740, training loss: 0.013902900740504265 = 0.006746263243257999 + 0.001 * 7.156637191772461
Epoch 740, val loss: 1.4057037830352783
Epoch 750, training loss: 0.013624724000692368 = 0.006473707500845194 + 0.001 * 7.151016712188721
Epoch 750, val loss: 1.4134881496429443
Epoch 760, training loss: 0.013349789194762707 = 0.006218526512384415 + 0.001 * 7.131262302398682
Epoch 760, val loss: 1.4210591316223145
Epoch 770, training loss: 0.013100729323923588 = 0.005979267414659262 + 0.001 * 7.121461391448975
Epoch 770, val loss: 1.4284144639968872
Epoch 780, training loss: 0.012894490733742714 = 0.005754644051194191 + 0.001 * 7.139845848083496
Epoch 780, val loss: 1.435579776763916
Epoch 790, training loss: 0.012654386460781097 = 0.005543505772948265 + 0.001 * 7.110879898071289
Epoch 790, val loss: 1.4425398111343384
Epoch 800, training loss: 0.012465694919228554 = 0.005344819277524948 + 0.001 * 7.120875835418701
Epoch 800, val loss: 1.4493218660354614
Epoch 810, training loss: 0.012294754385948181 = 0.005157622508704662 + 0.001 * 7.137131214141846
Epoch 810, val loss: 1.4559221267700195
Epoch 820, training loss: 0.012089556083083153 = 0.004981036763638258 + 0.001 * 7.108519554138184
Epoch 820, val loss: 1.4623216390609741
Epoch 830, training loss: 0.01192441489547491 = 0.004814322106540203 + 0.001 * 7.110092639923096
Epoch 830, val loss: 1.468586802482605
Epoch 840, training loss: 0.011750992387533188 = 0.004656739067286253 + 0.001 * 7.094252586364746
Epoch 840, val loss: 1.4746699333190918
Epoch 850, training loss: 0.011605668812990189 = 0.00450762826949358 + 0.001 * 7.098039627075195
Epoch 850, val loss: 1.4805988073349
Epoch 860, training loss: 0.011472193524241447 = 0.004366402048617601 + 0.001 * 7.105791091918945
Epoch 860, val loss: 1.486371636390686
Epoch 870, training loss: 0.011341690085828304 = 0.004232493694871664 + 0.001 * 7.109196186065674
Epoch 870, val loss: 1.4919800758361816
Epoch 880, training loss: 0.011203199625015259 = 0.004105437081307173 + 0.001 * 7.097761631011963
Epoch 880, val loss: 1.4974793195724487
Epoch 890, training loss: 0.011067874729633331 = 0.003984759096056223 + 0.001 * 7.0831146240234375
Epoch 890, val loss: 1.5028108358383179
Epoch 900, training loss: 0.010972012765705585 = 0.003870022716000676 + 0.001 * 7.101989269256592
Epoch 900, val loss: 1.5080201625823975
Epoch 910, training loss: 0.010846377350389957 = 0.003760846331715584 + 0.001 * 7.085530757904053
Epoch 910, val loss: 1.5130895376205444
Epoch 920, training loss: 0.010747885331511497 = 0.003656869288533926 + 0.001 * 7.0910162925720215
Epoch 920, val loss: 1.5180590152740479
Epoch 930, training loss: 0.010636609978973866 = 0.003557750955224037 + 0.001 * 7.078858852386475
Epoch 930, val loss: 1.522878885269165
Epoch 940, training loss: 0.01056721992790699 = 0.003463119501248002 + 0.001 * 7.104099750518799
Epoch 940, val loss: 1.52756929397583
Epoch 950, training loss: 0.010466058738529682 = 0.0033727141562849283 + 0.001 * 7.093344688415527
Epoch 950, val loss: 1.5321687459945679
Epoch 960, training loss: 0.010364757850766182 = 0.0032862797379493713 + 0.001 * 7.0784783363342285
Epoch 960, val loss: 1.5366628170013428
Epoch 970, training loss: 0.010288983583450317 = 0.003203559434041381 + 0.001 * 7.085423946380615
Epoch 970, val loss: 1.541015625
Epoch 980, training loss: 0.010182017460465431 = 0.003124315058812499 + 0.001 * 7.05770206451416
Epoch 980, val loss: 1.5452576875686646
Epoch 990, training loss: 0.010120670311152935 = 0.0030483193695545197 + 0.001 * 7.07235050201416
Epoch 990, val loss: 1.5494071245193481
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8392198207696363
The final CL Acc:0.81358, 0.01429, The final GNN Acc:0.83904, 0.00366
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10546])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9514917135238647 = 1.9428948163986206 + 0.001 * 8.596845626831055
Epoch 0, val loss: 1.9405114650726318
Epoch 10, training loss: 1.9413623809814453 = 1.9327656030654907 + 0.001 * 8.596787452697754
Epoch 10, val loss: 1.9306151866912842
Epoch 20, training loss: 1.9289923906326294 = 1.9203957319259644 + 0.001 * 8.596606254577637
Epoch 20, val loss: 1.9179253578186035
Epoch 30, training loss: 1.9119758605957031 = 1.9033796787261963 + 0.001 * 8.596182823181152
Epoch 30, val loss: 1.8998196125030518
Epoch 40, training loss: 1.8875796794891357 = 1.8789845705032349 + 0.001 * 8.595097541809082
Epoch 40, val loss: 1.8733221292495728
Epoch 50, training loss: 1.8543195724487305 = 1.845727801322937 + 0.001 * 8.591822624206543
Epoch 50, val loss: 1.837992548942566
Epoch 60, training loss: 1.8172649145126343 = 1.8086854219436646 + 0.001 * 8.579437255859375
Epoch 60, val loss: 1.8018361330032349
Epoch 70, training loss: 1.7848517894744873 = 1.7763303518295288 + 0.001 * 8.521446228027344
Epoch 70, val loss: 1.7751822471618652
Epoch 80, training loss: 1.745408535003662 = 1.7371900081634521 + 0.001 * 8.218491554260254
Epoch 80, val loss: 1.743306279182434
Epoch 90, training loss: 1.6897026300430298 = 1.6816750764846802 + 0.001 * 8.02756404876709
Epoch 90, val loss: 1.6952329874038696
Epoch 100, training loss: 1.6120350360870361 = 1.604246735572815 + 0.001 * 7.788350582122803
Epoch 100, val loss: 1.6269659996032715
Epoch 110, training loss: 1.5138744115829468 = 1.5061745643615723 + 0.001 * 7.6998395919799805
Epoch 110, val loss: 1.5436897277832031
Epoch 120, training loss: 1.4043289422988892 = 1.3966560363769531 + 0.001 * 7.6729254722595215
Epoch 120, val loss: 1.4542196989059448
Epoch 130, training loss: 1.290799617767334 = 1.2831660509109497 + 0.001 * 7.633537292480469
Epoch 130, val loss: 1.3650972843170166
Epoch 140, training loss: 1.1767668724060059 = 1.1691886186599731 + 0.001 * 7.578242301940918
Epoch 140, val loss: 1.2801611423492432
Epoch 150, training loss: 1.066711664199829 = 1.0592142343521118 + 0.001 * 7.497404098510742
Epoch 150, val loss: 1.2015365362167358
Epoch 160, training loss: 0.9659968018531799 = 0.9585741758346558 + 0.001 * 7.422643184661865
Epoch 160, val loss: 1.1322563886642456
Epoch 170, training loss: 0.8777252435684204 = 0.8703386783599854 + 0.001 * 7.386542797088623
Epoch 170, val loss: 1.0737850666046143
Epoch 180, training loss: 0.802497386932373 = 0.7951452732086182 + 0.001 * 7.352114677429199
Epoch 180, val loss: 1.0264334678649902
Epoch 190, training loss: 0.7390173673629761 = 0.731702446937561 + 0.001 * 7.314905166625977
Epoch 190, val loss: 0.9894337058067322
Epoch 200, training loss: 0.684489905834198 = 0.6772086024284363 + 0.001 * 7.281280040740967
Epoch 200, val loss: 0.9606224894523621
Epoch 210, training loss: 0.6356223225593567 = 0.6283623576164246 + 0.001 * 7.259965896606445
Epoch 210, val loss: 0.9370799660682678
Epoch 220, training loss: 0.5894212126731873 = 0.5821709036827087 + 0.001 * 7.250283241271973
Epoch 220, val loss: 0.9163362383842468
Epoch 230, training loss: 0.5438622832298279 = 0.5366178154945374 + 0.001 * 7.244475364685059
Epoch 230, val loss: 0.8969529867172241
Epoch 240, training loss: 0.49792492389678955 = 0.4906836152076721 + 0.001 * 7.2413129806518555
Epoch 240, val loss: 0.8787685632705688
Epoch 250, training loss: 0.45152243971824646 = 0.44428443908691406 + 0.001 * 7.238014221191406
Epoch 250, val loss: 0.8623053431510925
Epoch 260, training loss: 0.4052203595638275 = 0.3979836404323578 + 0.001 * 7.236719608306885
Epoch 260, val loss: 0.848435640335083
Epoch 270, training loss: 0.36011895537376404 = 0.3528839647769928 + 0.001 * 7.234979152679443
Epoch 270, val loss: 0.8377081751823425
Epoch 280, training loss: 0.31747129559516907 = 0.3102368712425232 + 0.001 * 7.234434604644775
Epoch 280, val loss: 0.8306005597114563
Epoch 290, training loss: 0.27831074595451355 = 0.27107614278793335 + 0.001 * 7.234594821929932
Epoch 290, val loss: 0.8273323178291321
Epoch 300, training loss: 0.24318277835845947 = 0.23594793677330017 + 0.001 * 7.234838485717773
Epoch 300, val loss: 0.8280448913574219
Epoch 310, training loss: 0.21222518384456635 = 0.20498986542224884 + 0.001 * 7.235316753387451
Epoch 310, val loss: 0.8323168754577637
Epoch 320, training loss: 0.1851830631494522 = 0.17794594168663025 + 0.001 * 7.237115383148193
Epoch 320, val loss: 0.8396007418632507
Epoch 330, training loss: 0.1616731435060501 = 0.15443557500839233 + 0.001 * 7.237575054168701
Epoch 330, val loss: 0.8493067026138306
Epoch 340, training loss: 0.1413111537694931 = 0.13407348096370697 + 0.001 * 7.237666606903076
Epoch 340, val loss: 0.8607730865478516
Epoch 350, training loss: 0.12372002005577087 = 0.11648092418909073 + 0.001 * 7.2390923500061035
Epoch 350, val loss: 0.8734877705574036
Epoch 360, training loss: 0.10855750739574432 = 0.10132128745317459 + 0.001 * 7.2362189292907715
Epoch 360, val loss: 0.8869929909706116
Epoch 370, training loss: 0.09552697837352753 = 0.08829044550657272 + 0.001 * 7.236535549163818
Epoch 370, val loss: 0.9010574817657471
Epoch 380, training loss: 0.08435387164354324 = 0.07711535692214966 + 0.001 * 7.2385149002075195
Epoch 380, val loss: 0.9153580665588379
Epoch 390, training loss: 0.07478534430265427 = 0.06755097955465317 + 0.001 * 7.234362602233887
Epoch 390, val loss: 0.9296935796737671
Epoch 400, training loss: 0.06660816073417664 = 0.05937715992331505 + 0.001 * 7.231002330780029
Epoch 400, val loss: 0.9439728856086731
Epoch 410, training loss: 0.05962459743022919 = 0.05239609628915787 + 0.001 * 7.228500843048096
Epoch 410, val loss: 0.9580813646316528
Epoch 420, training loss: 0.053656671196222305 = 0.046430736780166626 + 0.001 * 7.225932598114014
Epoch 420, val loss: 0.9719355702400208
Epoch 430, training loss: 0.048549771308898926 = 0.04132568836212158 + 0.001 * 7.224084377288818
Epoch 430, val loss: 0.9854636788368225
Epoch 440, training loss: 0.04416593164205551 = 0.036948028951883316 + 0.001 * 7.217902183532715
Epoch 440, val loss: 0.9987453818321228
Epoch 450, training loss: 0.040404073894023895 = 0.03318393975496292 + 0.001 * 7.220133304595947
Epoch 450, val loss: 1.0116862058639526
Epoch 460, training loss: 0.037169598042964935 = 0.029937254264950752 + 0.001 * 7.2323431968688965
Epoch 460, val loss: 1.0242236852645874
Epoch 470, training loss: 0.03433360531926155 = 0.027125757187604904 + 0.001 * 7.207846641540527
Epoch 470, val loss: 1.0364257097244263
Epoch 480, training loss: 0.03187466412782669 = 0.024678388610482216 + 0.001 * 7.196274280548096
Epoch 480, val loss: 1.0482324361801147
Epoch 490, training loss: 0.02973192371428013 = 0.02253597043454647 + 0.001 * 7.195952892303467
Epoch 490, val loss: 1.059660792350769
Epoch 500, training loss: 0.027833648025989532 = 0.02064807340502739 + 0.001 * 7.185574054718018
Epoch 500, val loss: 1.070739507675171
Epoch 510, training loss: 0.026162417605519295 = 0.018976928666234016 + 0.001 * 7.185489177703857
Epoch 510, val loss: 1.0814474821090698
Epoch 520, training loss: 0.024687491357326508 = 0.017492545768618584 + 0.001 * 7.194944381713867
Epoch 520, val loss: 1.0918468236923218
Epoch 530, training loss: 0.023361507803201675 = 0.016170334070920944 + 0.001 * 7.191173076629639
Epoch 530, val loss: 1.1019272804260254
Epoch 540, training loss: 0.022181911394000053 = 0.01498994417488575 + 0.001 * 7.191966533660889
Epoch 540, val loss: 1.1116909980773926
Epoch 550, training loss: 0.021110575646162033 = 0.013933421112596989 + 0.001 * 7.177154541015625
Epoch 550, val loss: 1.1211583614349365
Epoch 560, training loss: 0.02015616185963154 = 0.012985089793801308 + 0.001 * 7.171071529388428
Epoch 560, val loss: 1.1303486824035645
Epoch 570, training loss: 0.019303034991025925 = 0.01213169191032648 + 0.001 * 7.171341896057129
Epoch 570, val loss: 1.1392872333526611
Epoch 580, training loss: 0.018523871898651123 = 0.01136172004044056 + 0.001 * 7.16215181350708
Epoch 580, val loss: 1.147952675819397
Epoch 590, training loss: 0.0178238432854414 = 0.010664919391274452 + 0.001 * 7.158924102783203
Epoch 590, val loss: 1.1563704013824463
Epoch 600, training loss: 0.017191940918564796 = 0.01003257930278778 + 0.001 * 7.159361362457275
Epoch 600, val loss: 1.1645656824111938
Epoch 610, training loss: 0.016621261835098267 = 0.009457278996706009 + 0.001 * 7.16398286819458
Epoch 610, val loss: 1.1724853515625
Epoch 620, training loss: 0.01609315723180771 = 0.008932682685554028 + 0.001 * 7.16047477722168
Epoch 620, val loss: 1.1802058219909668
Epoch 630, training loss: 0.015613148920238018 = 0.00845305435359478 + 0.001 * 7.160094261169434
Epoch 630, val loss: 1.1877110004425049
Epoch 640, training loss: 0.015167568810284138 = 0.008013362996280193 + 0.001 * 7.154205322265625
Epoch 640, val loss: 1.1950016021728516
Epoch 650, training loss: 0.014774598181247711 = 0.007609363179653883 + 0.001 * 7.1652350425720215
Epoch 650, val loss: 1.202099323272705
Epoch 660, training loss: 0.014396261423826218 = 0.007237400393933058 + 0.001 * 7.158860683441162
Epoch 660, val loss: 1.2089958190917969
Epoch 670, training loss: 0.014049993827939034 = 0.006894190330058336 + 0.001 * 7.1558027267456055
Epoch 670, val loss: 1.2157174348831177
Epoch 680, training loss: 0.013724792748689651 = 0.006576809566468 + 0.001 * 7.147983074188232
Epoch 680, val loss: 1.2222392559051514
Epoch 690, training loss: 0.013429194688796997 = 0.006282771937549114 + 0.001 * 7.146422386169434
Epoch 690, val loss: 1.2285958528518677
Epoch 700, training loss: 0.013146011158823967 = 0.006009796634316444 + 0.001 * 7.136214256286621
Epoch 700, val loss: 1.2347954511642456
Epoch 710, training loss: 0.01289137452840805 = 0.005755879450589418 + 0.001 * 7.135495185852051
Epoch 710, val loss: 1.2408424615859985
Epoch 720, training loss: 0.012655628845095634 = 0.0055193365551531315 + 0.001 * 7.13629150390625
Epoch 720, val loss: 1.2467129230499268
Epoch 730, training loss: 0.012430851347744465 = 0.005298646632581949 + 0.001 * 7.132204532623291
Epoch 730, val loss: 1.2524611949920654
Epoch 740, training loss: 0.012224547564983368 = 0.005092373117804527 + 0.001 * 7.132173538208008
Epoch 740, val loss: 1.2580450773239136
Epoch 750, training loss: 0.012021422386169434 = 0.004899244289845228 + 0.001 * 7.1221771240234375
Epoch 750, val loss: 1.2635166645050049
Epoch 760, training loss: 0.011841338127851486 = 0.004717998206615448 + 0.001 * 7.123339653015137
Epoch 760, val loss: 1.2688602209091187
Epoch 770, training loss: 0.011678867042064667 = 0.004547289572656155 + 0.001 * 7.131577014923096
Epoch 770, val loss: 1.27407968044281
Epoch 780, training loss: 0.011496356688439846 = 0.004385818727314472 + 0.001 * 7.110537528991699
Epoch 780, val loss: 1.279217004776001
Epoch 790, training loss: 0.011359333992004395 = 0.0042322236113250256 + 0.001 * 7.127110004425049
Epoch 790, val loss: 1.2842870950698853
Epoch 800, training loss: 0.011195031926035881 = 0.004085381980985403 + 0.001 * 7.109650135040283
Epoch 800, val loss: 1.2893098592758179
Epoch 810, training loss: 0.011061223223805428 = 0.003944567404687405 + 0.001 * 7.116654872894287
Epoch 810, val loss: 1.2943319082260132
Epoch 820, training loss: 0.010930158197879791 = 0.0038093882612884045 + 0.001 * 7.120769023895264
Epoch 820, val loss: 1.2993038892745972
Epoch 830, training loss: 0.010803966782987118 = 0.0036795383784919977 + 0.001 * 7.124427795410156
Epoch 830, val loss: 1.3042629957199097
Epoch 840, training loss: 0.010662907734513283 = 0.0035548824816942215 + 0.001 * 7.108024597167969
Epoch 840, val loss: 1.3092139959335327
Epoch 850, training loss: 0.01056675799190998 = 0.0034350894857198 + 0.001 * 7.131668567657471
Epoch 850, val loss: 1.314127802848816
Epoch 860, training loss: 0.010430549271404743 = 0.0033200045581907034 + 0.001 * 7.110544681549072
Epoch 860, val loss: 1.3190091848373413
Epoch 870, training loss: 0.010335385799407959 = 0.0032094044145196676 + 0.001 * 7.125980854034424
Epoch 870, val loss: 1.3238489627838135
Epoch 880, training loss: 0.010185622610151768 = 0.0031036159489303827 + 0.001 * 7.082006454467773
Epoch 880, val loss: 1.3286464214324951
Epoch 890, training loss: 0.010102660395205021 = 0.003002403536811471 + 0.001 * 7.10025691986084
Epoch 890, val loss: 1.333381175994873
Epoch 900, training loss: 0.010009228251874447 = 0.0029057078063488007 + 0.001 * 7.103519916534424
Epoch 900, val loss: 1.3380464315414429
Epoch 910, training loss: 0.009910963475704193 = 0.0028132463339716196 + 0.001 * 7.097716808319092
Epoch 910, val loss: 1.342676043510437
Epoch 920, training loss: 0.009812545031309128 = 0.002724976046010852 + 0.001 * 7.087568759918213
Epoch 920, val loss: 1.3472111225128174
Epoch 930, training loss: 0.009734955616295338 = 0.002640760038048029 + 0.001 * 7.094195365905762
Epoch 930, val loss: 1.3516929149627686
Epoch 940, training loss: 0.009630847722291946 = 0.002560407156124711 + 0.001 * 7.07043981552124
Epoch 940, val loss: 1.3561224937438965
Epoch 950, training loss: 0.009559398517012596 = 0.002483814023435116 + 0.001 * 7.075584411621094
Epoch 950, val loss: 1.3604719638824463
Epoch 960, training loss: 0.009523759596049786 = 0.0024108614306896925 + 0.001 * 7.1128973960876465
Epoch 960, val loss: 1.3647125959396362
Epoch 970, training loss: 0.009417668916285038 = 0.002341525861993432 + 0.001 * 7.07614278793335
Epoch 970, val loss: 1.3689179420471191
Epoch 980, training loss: 0.009336492046713829 = 0.0022754515521228313 + 0.001 * 7.061039924621582
Epoch 980, val loss: 1.3730287551879883
Epoch 990, training loss: 0.009278997778892517 = 0.002212440362200141 + 0.001 * 7.06655740737915
Epoch 990, val loss: 1.3770555257797241
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.801265155508698
=== training gcn model ===
Epoch 0, training loss: 1.9486809968948364 = 1.9400842189788818 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.9325687885284424
Epoch 10, training loss: 1.939426064491272 = 1.9308292865753174 + 0.001 * 8.596780776977539
Epoch 10, val loss: 1.9241862297058105
Epoch 20, training loss: 1.928221344947815 = 1.91962468624115 + 0.001 * 8.596607208251953
Epoch 20, val loss: 1.913691520690918
Epoch 30, training loss: 1.9129236936569214 = 1.9043275117874146 + 0.001 * 8.596221923828125
Epoch 30, val loss: 1.8990684747695923
Epoch 40, training loss: 1.8909251689910889 = 1.8823298215866089 + 0.001 * 8.595292091369629
Epoch 40, val loss: 1.8779970407485962
Epoch 50, training loss: 1.860246181488037 = 1.8516535758972168 + 0.001 * 8.592643737792969
Epoch 50, val loss: 1.8495317697525024
Epoch 60, training loss: 1.8238649368286133 = 1.8152817487716675 + 0.001 * 8.583196640014648
Epoch 60, val loss: 1.8184328079223633
Epoch 70, training loss: 1.7892178297042847 = 1.780679702758789 + 0.001 * 8.538067817687988
Epoch 70, val loss: 1.79048752784729
Epoch 80, training loss: 1.7487154006958008 = 1.7404981851577759 + 0.001 * 8.217238426208496
Epoch 80, val loss: 1.7535173892974854
Epoch 90, training loss: 1.692372441291809 = 1.6843544244766235 + 0.001 * 8.018022537231445
Epoch 90, val loss: 1.7011566162109375
Epoch 100, training loss: 1.6156889200210571 = 1.6077560186386108 + 0.001 * 7.932953357696533
Epoch 100, val loss: 1.6339322328567505
Epoch 110, training loss: 1.5216041803359985 = 1.513787031173706 + 0.001 * 7.817132949829102
Epoch 110, val loss: 1.5548794269561768
Epoch 120, training loss: 1.4203294515609741 = 1.4127535820007324 + 0.001 * 7.575889587402344
Epoch 120, val loss: 1.4726629257202148
Epoch 130, training loss: 1.319124698638916 = 1.3116562366485596 + 0.001 * 7.46848726272583
Epoch 130, val loss: 1.3941853046417236
Epoch 140, training loss: 1.2204782962799072 = 1.2130950689315796 + 0.001 * 7.383230686187744
Epoch 140, val loss: 1.3205753564834595
Epoch 150, training loss: 1.1244999170303345 = 1.1171504259109497 + 0.001 * 7.349486827850342
Epoch 150, val loss: 1.2499269247055054
Epoch 160, training loss: 1.030439853668213 = 1.0230885744094849 + 0.001 * 7.351334571838379
Epoch 160, val loss: 1.1806888580322266
Epoch 170, training loss: 0.9394355416297913 = 0.9320878982543945 + 0.001 * 7.347659587860107
Epoch 170, val loss: 1.1136200428009033
Epoch 180, training loss: 0.8545957207679749 = 0.8472504019737244 + 0.001 * 7.345309257507324
Epoch 180, val loss: 1.0518656969070435
Epoch 190, training loss: 0.7786171436309814 = 0.7712768316268921 + 0.001 * 7.340339660644531
Epoch 190, val loss: 0.9985100626945496
Epoch 200, training loss: 0.7117679715156555 = 0.7044348120689392 + 0.001 * 7.33315896987915
Epoch 200, val loss: 0.954994261264801
Epoch 210, training loss: 0.6520829796791077 = 0.6447612047195435 + 0.001 * 7.321795463562012
Epoch 210, val loss: 0.9201084971427917
Epoch 220, training loss: 0.5971996784210205 = 0.5898972749710083 + 0.001 * 7.30242395401001
Epoch 220, val loss: 0.8916649222373962
Epoch 230, training loss: 0.545513927936554 = 0.5382437705993652 + 0.001 * 7.270143985748291
Epoch 230, val loss: 0.867860734462738
Epoch 240, training loss: 0.4962731897830963 = 0.4890359044075012 + 0.001 * 7.2372870445251465
Epoch 240, val loss: 0.8471658825874329
Epoch 250, training loss: 0.4491806924343109 = 0.44196897745132446 + 0.001 * 7.21171760559082
Epoch 250, val loss: 0.8290165066719055
Epoch 260, training loss: 0.4041133224964142 = 0.3969208896160126 + 0.001 * 7.1924285888671875
Epoch 260, val loss: 0.8135521411895752
Epoch 270, training loss: 0.36115679144859314 = 0.3539673089981079 + 0.001 * 7.1894755363464355
Epoch 270, val loss: 0.8009219765663147
Epoch 280, training loss: 0.3204905688762665 = 0.31331196427345276 + 0.001 * 7.178618431091309
Epoch 280, val loss: 0.7907669544219971
Epoch 290, training loss: 0.28248119354248047 = 0.2753090262413025 + 0.001 * 7.172156810760498
Epoch 290, val loss: 0.7831371426582336
Epoch 300, training loss: 0.24749711155891418 = 0.2403295338153839 + 0.001 * 7.167584419250488
Epoch 300, val loss: 0.7780730128288269
Epoch 310, training loss: 0.2159038484096527 = 0.2087380290031433 + 0.001 * 7.165823459625244
Epoch 310, val loss: 0.7754880785942078
Epoch 320, training loss: 0.18793129920959473 = 0.18077391386032104 + 0.001 * 7.157378673553467
Epoch 320, val loss: 0.7753661274909973
Epoch 330, training loss: 0.16359923779964447 = 0.15644963085651398 + 0.001 * 7.149611473083496
Epoch 330, val loss: 0.777772068977356
Epoch 340, training loss: 0.14270932972431183 = 0.13555504381656647 + 0.001 * 7.154280185699463
Epoch 340, val loss: 0.7824788689613342
Epoch 350, training loss: 0.12488431483507156 = 0.11774691939353943 + 0.001 * 7.137393474578857
Epoch 350, val loss: 0.7892524003982544
Epoch 360, training loss: 0.10975826531648636 = 0.10263103246688843 + 0.001 * 7.12723445892334
Epoch 360, val loss: 0.7976723909378052
Epoch 370, training loss: 0.09694003313779831 = 0.08981599658727646 + 0.001 * 7.1240339279174805
Epoch 370, val loss: 0.8073889017105103
Epoch 380, training loss: 0.08605938404798508 = 0.07894468307495117 + 0.001 * 7.114698886871338
Epoch 380, val loss: 0.8179904222488403
Epoch 390, training loss: 0.07680836319923401 = 0.06970185041427612 + 0.001 * 7.106516361236572
Epoch 390, val loss: 0.8291565775871277
Epoch 400, training loss: 0.0689438134431839 = 0.061821646988391876 + 0.001 * 7.122168064117432
Epoch 400, val loss: 0.840728759765625
Epoch 410, training loss: 0.062179286032915115 = 0.05508030578494072 + 0.001 * 7.09898042678833
Epoch 410, val loss: 0.8525266051292419
Epoch 420, training loss: 0.056382082402706146 = 0.049289532005786896 + 0.001 * 7.092548370361328
Epoch 420, val loss: 0.864396333694458
Epoch 430, training loss: 0.05139034986495972 = 0.04429353028535843 + 0.001 * 7.096818923950195
Epoch 430, val loss: 0.8763148784637451
Epoch 440, training loss: 0.04704554006457329 = 0.03996308147907257 + 0.001 * 7.082458019256592
Epoch 440, val loss: 0.8881393074989319
Epoch 450, training loss: 0.04330357909202576 = 0.036193545907735825 + 0.001 * 7.110032558441162
Epoch 450, val loss: 0.8998767137527466
Epoch 460, training loss: 0.039987124502658844 = 0.032898567616939545 + 0.001 * 7.088558197021484
Epoch 460, val loss: 0.91153484582901
Epoch 470, training loss: 0.037092555314302444 = 0.03000730089843273 + 0.001 * 7.085255146026611
Epoch 470, val loss: 0.9229615330696106
Epoch 480, training loss: 0.03453574702143669 = 0.02746080607175827 + 0.001 * 7.074939727783203
Epoch 480, val loss: 0.9341888427734375
Epoch 490, training loss: 0.03229978680610657 = 0.0252094529569149 + 0.001 * 7.090335369110107
Epoch 490, val loss: 0.9451625347137451
Epoch 500, training loss: 0.03028421476483345 = 0.023212408646941185 + 0.001 * 7.071805477142334
Epoch 500, val loss: 0.9558954238891602
Epoch 510, training loss: 0.028503119945526123 = 0.021434728056192398 + 0.001 * 7.068390846252441
Epoch 510, val loss: 0.9663721323013306
Epoch 520, training loss: 0.026933450251817703 = 0.01984747126698494 + 0.001 * 7.085977554321289
Epoch 520, val loss: 0.976557195186615
Epoch 530, training loss: 0.025500735267996788 = 0.018426448106765747 + 0.001 * 7.074286460876465
Epoch 530, val loss: 0.9864619374275208
Epoch 540, training loss: 0.024222761392593384 = 0.017150506377220154 + 0.001 * 7.072255611419678
Epoch 540, val loss: 0.9960899353027344
Epoch 550, training loss: 0.02306763082742691 = 0.01600174978375435 + 0.001 * 7.065880298614502
Epoch 550, val loss: 1.0054274797439575
Epoch 560, training loss: 0.022038917988538742 = 0.014964488334953785 + 0.001 * 7.074429035186768
Epoch 560, val loss: 1.01448655128479
Epoch 570, training loss: 0.021087903529405594 = 0.014025441370904446 + 0.001 * 7.062462329864502
Epoch 570, val loss: 1.0232963562011719
Epoch 580, training loss: 0.020234405994415283 = 0.013173209503293037 + 0.001 * 7.0611958503723145
Epoch 580, val loss: 1.0318527221679688
Epoch 590, training loss: 0.019469372928142548 = 0.01239786297082901 + 0.001 * 7.071510314941406
Epoch 590, val loss: 1.0401418209075928
Epoch 600, training loss: 0.018756644800305367 = 0.011690786108374596 + 0.001 * 7.065857887268066
Epoch 600, val loss: 1.048162817955017
Epoch 610, training loss: 0.018099546432495117 = 0.011044428683817387 + 0.001 * 7.055118083953857
Epoch 610, val loss: 1.0559812784194946
Epoch 620, training loss: 0.017534378916025162 = 0.010452205315232277 + 0.001 * 7.082172393798828
Epoch 620, val loss: 1.0635452270507812
Epoch 630, training loss: 0.016962144523859024 = 0.00990842655301094 + 0.001 * 7.053718090057373
Epoch 630, val loss: 1.0708762407302856
Epoch 640, training loss: 0.016464270651340485 = 0.00940807443112135 + 0.001 * 7.056196212768555
Epoch 640, val loss: 1.0780160427093506
Epoch 650, training loss: 0.015998302027583122 = 0.008946682326495647 + 0.001 * 7.051620006561279
Epoch 650, val loss: 1.0849441289901733
Epoch 660, training loss: 0.015583406202495098 = 0.008520402014255524 + 0.001 * 7.063004016876221
Epoch 660, val loss: 1.0916414260864258
Epoch 670, training loss: 0.015176165848970413 = 0.008125852793455124 + 0.001 * 7.050312519073486
Epoch 670, val loss: 1.0981626510620117
Epoch 680, training loss: 0.014813169836997986 = 0.0077599273063242435 + 0.001 * 7.0532426834106445
Epoch 680, val loss: 1.1045185327529907
Epoch 690, training loss: 0.014470377936959267 = 0.007419969420880079 + 0.001 * 7.050408840179443
Epoch 690, val loss: 1.110665202140808
Epoch 700, training loss: 0.01415075920522213 = 0.0071035888977348804 + 0.001 * 7.0471696853637695
Epoch 700, val loss: 1.1166622638702393
Epoch 710, training loss: 0.01386168785393238 = 0.006808671168982983 + 0.001 * 7.05301570892334
Epoch 710, val loss: 1.122495412826538
Epoch 720, training loss: 0.013583119958639145 = 0.006533345207571983 + 0.001 * 7.049774169921875
Epoch 720, val loss: 1.128199577331543
Epoch 730, training loss: 0.013321449980139732 = 0.006275870371609926 + 0.001 * 7.045579433441162
Epoch 730, val loss: 1.1337181329727173
Epoch 740, training loss: 0.01308859046548605 = 0.0060347625985741615 + 0.001 * 7.05382776260376
Epoch 740, val loss: 1.1391264200210571
Epoch 750, training loss: 0.012854227796196938 = 0.005808698944747448 + 0.001 * 7.045528888702393
Epoch 750, val loss: 1.144358515739441
Epoch 760, training loss: 0.012631292454898357 = 0.005596408620476723 + 0.001 * 7.034883499145508
Epoch 760, val loss: 1.1494730710983276
Epoch 770, training loss: 0.012438097968697548 = 0.005396801047027111 + 0.001 * 7.04129695892334
Epoch 770, val loss: 1.1544592380523682
Epoch 780, training loss: 0.012248514220118523 = 0.00520889088511467 + 0.001 * 7.039623260498047
Epoch 780, val loss: 1.1593163013458252
Epoch 790, training loss: 0.012069067917764187 = 0.005031782202422619 + 0.001 * 7.037285327911377
Epoch 790, val loss: 1.1640435457229614
Epoch 800, training loss: 0.011906065046787262 = 0.004864655900746584 + 0.001 * 7.041408538818359
Epoch 800, val loss: 1.16863214969635
Epoch 810, training loss: 0.011735964566469193 = 0.004706746898591518 + 0.001 * 7.02921724319458
Epoch 810, val loss: 1.1731535196304321
Epoch 820, training loss: 0.011586017906665802 = 0.0045574079267680645 + 0.001 * 7.0286102294921875
Epoch 820, val loss: 1.1775352954864502
Epoch 830, training loss: 0.011442750692367554 = 0.004416021518409252 + 0.001 * 7.026729106903076
Epoch 830, val loss: 1.181803822517395
Epoch 840, training loss: 0.011332716792821884 = 0.004282035864889622 + 0.001 * 7.050680160522461
Epoch 840, val loss: 1.1859922409057617
Epoch 850, training loss: 0.011189838871359825 = 0.004154949449002743 + 0.001 * 7.034889221191406
Epoch 850, val loss: 1.1900726556777954
Epoch 860, training loss: 0.011067020706832409 = 0.004034295212477446 + 0.001 * 7.0327253341674805
Epoch 860, val loss: 1.1940585374832153
Epoch 870, training loss: 0.010957624763250351 = 0.003919633105397224 + 0.001 * 7.037991523742676
Epoch 870, val loss: 1.197954773902893
Epoch 880, training loss: 0.010833291336894035 = 0.0038105794228613377 + 0.001 * 7.022711753845215
Epoch 880, val loss: 1.2017605304718018
Epoch 890, training loss: 0.010736999101936817 = 0.0037067518569529057 + 0.001 * 7.030246734619141
Epoch 890, val loss: 1.2054802179336548
Epoch 900, training loss: 0.010641360655426979 = 0.00360781978815794 + 0.001 * 7.033540725708008
Epoch 900, val loss: 1.2090798616409302
Epoch 910, training loss: 0.010534691624343395 = 0.0035134940408170223 + 0.001 * 7.021197319030762
Epoch 910, val loss: 1.2126294374465942
Epoch 920, training loss: 0.010454695671796799 = 0.003423471702262759 + 0.001 * 7.031223773956299
Epoch 920, val loss: 1.2160577774047852
Epoch 930, training loss: 0.010349517688155174 = 0.0033375155180692673 + 0.001 * 7.012001991271973
Epoch 930, val loss: 1.2194701433181763
Epoch 940, training loss: 0.010268792510032654 = 0.0032553700730204582 + 0.001 * 7.013422012329102
Epoch 940, val loss: 1.2227697372436523
Epoch 950, training loss: 0.01020102109760046 = 0.003176808590069413 + 0.001 * 7.024211883544922
Epoch 950, val loss: 1.226021409034729
Epoch 960, training loss: 0.010113055817782879 = 0.0031016322318464518 + 0.001 * 7.011423110961914
Epoch 960, val loss: 1.2291706800460815
Epoch 970, training loss: 0.010053756646811962 = 0.003029648447409272 + 0.001 * 7.024107933044434
Epoch 970, val loss: 1.23225736618042
Epoch 980, training loss: 0.009970962069928646 = 0.0029606784228235483 + 0.001 * 7.010283470153809
Epoch 980, val loss: 1.2353053092956543
Epoch 990, training loss: 0.009901942685246468 = 0.0028945475351065397 + 0.001 * 7.007394790649414
Epoch 990, val loss: 1.2382467985153198
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 1.9732248783111572 = 1.964627981185913 + 0.001 * 8.59687614440918
Epoch 0, val loss: 1.9661996364593506
Epoch 10, training loss: 1.962124228477478 = 1.9535273313522339 + 0.001 * 8.596842765808105
Epoch 10, val loss: 1.9547808170318604
Epoch 20, training loss: 1.9483487606048584 = 1.9397521018981934 + 0.001 * 8.596711158752441
Epoch 20, val loss: 1.9403083324432373
Epoch 30, training loss: 1.929145336151123 = 1.920548915863037 + 0.001 * 8.59636402130127
Epoch 30, val loss: 1.9196655750274658
Epoch 40, training loss: 1.9011884927749634 = 1.8925930261611938 + 0.001 * 8.595484733581543
Epoch 40, val loss: 1.8893883228302002
Epoch 50, training loss: 1.862919807434082 = 1.854326844215393 + 0.001 * 8.592987060546875
Epoch 50, val loss: 1.8490630388259888
Epoch 60, training loss: 1.822036623954773 = 1.813452124595642 + 0.001 * 8.5845308303833
Epoch 60, val loss: 1.8101927042007446
Epoch 70, training loss: 1.790610671043396 = 1.7820603847503662 + 0.001 * 8.550291061401367
Epoch 70, val loss: 1.7849106788635254
Epoch 80, training loss: 1.7539656162261963 = 1.7456321716308594 + 0.001 * 8.333416938781738
Epoch 80, val loss: 1.7553945779800415
Epoch 90, training loss: 1.7034106254577637 = 1.6952382326126099 + 0.001 * 8.172438621520996
Epoch 90, val loss: 1.7126846313476562
Epoch 100, training loss: 1.6331863403320312 = 1.6251237392425537 + 0.001 * 8.062616348266602
Epoch 100, val loss: 1.6527737379074097
Epoch 110, training loss: 1.5458097457885742 = 1.5378739833831787 + 0.001 * 7.935727119445801
Epoch 110, val loss: 1.5803855657577515
Epoch 120, training loss: 1.4528166055679321 = 1.4450936317443848 + 0.001 * 7.722950458526611
Epoch 120, val loss: 1.5072118043899536
Epoch 130, training loss: 1.3621172904968262 = 1.3545663356781006 + 0.001 * 7.550905227661133
Epoch 130, val loss: 1.4400845766067505
Epoch 140, training loss: 1.2733345031738281 = 1.265835165977478 + 0.001 * 7.499332904815674
Epoch 140, val loss: 1.3783349990844727
Epoch 150, training loss: 1.1852548122406006 = 1.1778059005737305 + 0.001 * 7.44885778427124
Epoch 150, val loss: 1.3202385902404785
Epoch 160, training loss: 1.0991947650909424 = 1.0917763710021973 + 0.001 * 7.418373107910156
Epoch 160, val loss: 1.2647072076797485
Epoch 170, training loss: 1.0170073509216309 = 1.0096004009246826 + 0.001 * 7.406890392303467
Epoch 170, val loss: 1.212516188621521
Epoch 180, training loss: 0.9394016265869141 = 0.9319982528686523 + 0.001 * 7.403347492218018
Epoch 180, val loss: 1.1630874872207642
Epoch 190, training loss: 0.8659684658050537 = 0.8585671186447144 + 0.001 * 7.401328086853027
Epoch 190, val loss: 1.1154230833053589
Epoch 200, training loss: 0.7963998913764954 = 0.7890008687973022 + 0.001 * 7.399031639099121
Epoch 200, val loss: 1.06941819190979
Epoch 210, training loss: 0.7312552332878113 = 0.7238587141036987 + 0.001 * 7.396539688110352
Epoch 210, val loss: 1.0257498025894165
Epoch 220, training loss: 0.6714470982551575 = 0.6640538573265076 + 0.001 * 7.393251419067383
Epoch 220, val loss: 0.9863402247428894
Epoch 230, training loss: 0.6172114610671997 = 0.6098227500915527 + 0.001 * 7.3887104988098145
Epoch 230, val loss: 0.9520424008369446
Epoch 240, training loss: 0.5678780674934387 = 0.5604951977729797 + 0.001 * 7.382846832275391
Epoch 240, val loss: 0.9235886931419373
Epoch 250, training loss: 0.522392213344574 = 0.5150172710418701 + 0.001 * 7.374969482421875
Epoch 250, val loss: 0.9009997248649597
Epoch 260, training loss: 0.47982150316238403 = 0.4724583029747009 + 0.001 * 7.363204479217529
Epoch 260, val loss: 0.8839107155799866
Epoch 270, training loss: 0.43947261571884155 = 0.43212518095970154 + 0.001 * 7.347438335418701
Epoch 270, val loss: 0.8722763061523438
Epoch 280, training loss: 0.40091466903686523 = 0.3935847580432892 + 0.001 * 7.329916477203369
Epoch 280, val loss: 0.8655338883399963
Epoch 290, training loss: 0.36405450105667114 = 0.3567451238632202 + 0.001 * 7.309377670288086
Epoch 290, val loss: 0.8637645840644836
Epoch 300, training loss: 0.329182505607605 = 0.32189199328422546 + 0.001 * 7.290500640869141
Epoch 300, val loss: 0.8664414882659912
Epoch 310, training loss: 0.2964521646499634 = 0.2891716957092285 + 0.001 * 7.280470848083496
Epoch 310, val loss: 0.8725858330726624
Epoch 320, training loss: 0.26552140712738037 = 0.2582477033138275 + 0.001 * 7.273692607879639
Epoch 320, val loss: 0.8805510997772217
Epoch 330, training loss: 0.23594039678573608 = 0.22866693139076233 + 0.001 * 7.273461818695068
Epoch 330, val loss: 0.8891682028770447
Epoch 340, training loss: 0.2075643688440323 = 0.20029205083847046 + 0.001 * 7.272312164306641
Epoch 340, val loss: 0.8974408507347107
Epoch 350, training loss: 0.1809341162443161 = 0.1736636459827423 + 0.001 * 7.2704644203186035
Epoch 350, val loss: 0.9059370160102844
Epoch 360, training loss: 0.15686532855033875 = 0.14959292113780975 + 0.001 * 7.272409439086914
Epoch 360, val loss: 0.9150984883308411
Epoch 370, training loss: 0.13585568964481354 = 0.12858888506889343 + 0.001 * 7.266797065734863
Epoch 370, val loss: 0.9252637028694153
Epoch 380, training loss: 0.11792469024658203 = 0.11066138744354248 + 0.001 * 7.263303756713867
Epoch 380, val loss: 0.9369551539421082
Epoch 390, training loss: 0.10284249484539032 = 0.09558005630970001 + 0.001 * 7.262439250946045
Epoch 390, val loss: 0.9499810338020325
Epoch 400, training loss: 0.09021224081516266 = 0.08295513689517975 + 0.001 * 7.257101058959961
Epoch 400, val loss: 0.9641547203063965
Epoch 410, training loss: 0.07961970567703247 = 0.072368323802948 + 0.001 * 7.251377582550049
Epoch 410, val loss: 0.9789736270904541
Epoch 420, training loss: 0.07074353098869324 = 0.06346467137336731 + 0.001 * 7.2788567543029785
Epoch 420, val loss: 0.9944455623626709
Epoch 430, training loss: 0.06318839639425278 = 0.055937811732292175 + 0.001 * 7.250584125518799
Epoch 430, val loss: 1.010014295578003
Epoch 440, training loss: 0.056787025183439255 = 0.04954224079847336 + 0.001 * 7.244784355163574
Epoch 440, val loss: 1.0256309509277344
Epoch 450, training loss: 0.05132170021533966 = 0.04408100247383118 + 0.001 * 7.24069881439209
Epoch 450, val loss: 1.0411007404327393
Epoch 460, training loss: 0.04663505405187607 = 0.039397746324539185 + 0.001 * 7.237306118011475
Epoch 460, val loss: 1.0563156604766846
Epoch 470, training loss: 0.04260103777050972 = 0.035366419702768326 + 0.001 * 7.234618663787842
Epoch 470, val loss: 1.0712400674819946
Epoch 480, training loss: 0.03911745920777321 = 0.03188485652208328 + 0.001 * 7.232602119445801
Epoch 480, val loss: 1.0858688354492188
Epoch 490, training loss: 0.03610120713710785 = 0.02886815369129181 + 0.001 * 7.233054161071777
Epoch 490, val loss: 1.1001120805740356
Epoch 500, training loss: 0.03347797691822052 = 0.026245122775435448 + 0.001 * 7.232852458953857
Epoch 500, val loss: 1.113986611366272
Epoch 510, training loss: 0.031185869127511978 = 0.023955415934324265 + 0.001 * 7.230452537536621
Epoch 510, val loss: 1.1274744272232056
Epoch 520, training loss: 0.029174206778407097 = 0.02194817177951336 + 0.001 * 7.226034641265869
Epoch 520, val loss: 1.1406149864196777
Epoch 530, training loss: 0.02740533836185932 = 0.020181376487016678 + 0.001 * 7.223961353302002
Epoch 530, val loss: 1.153349757194519
Epoch 540, training loss: 0.025841783732175827 = 0.018619917333126068 + 0.001 * 7.221866130828857
Epoch 540, val loss: 1.1657313108444214
Epoch 550, training loss: 0.024469397962093353 = 0.017234379425644875 + 0.001 * 7.235018253326416
Epoch 550, val loss: 1.1777263879776
Epoch 560, training loss: 0.023222293704748154 = 0.01600022427737713 + 0.001 * 7.22206974029541
Epoch 560, val loss: 1.189371943473816
Epoch 570, training loss: 0.022113356739282608 = 0.0148968156427145 + 0.001 * 7.216540813446045
Epoch 570, val loss: 1.2006648778915405
Epoch 580, training loss: 0.021121446043252945 = 0.013906729407608509 + 0.001 * 7.214717388153076
Epoch 580, val loss: 1.2116105556488037
Epoch 590, training loss: 0.020236540585756302 = 0.013015307486057281 + 0.001 * 7.221232891082764
Epoch 590, val loss: 1.2222470045089722
Epoch 600, training loss: 0.019422607496380806 = 0.012210042215883732 + 0.001 * 7.2125654220581055
Epoch 600, val loss: 1.2325633764266968
Epoch 610, training loss: 0.018690200522542 = 0.01148039661347866 + 0.001 * 7.209804058074951
Epoch 610, val loss: 1.2425756454467773
Epoch 620, training loss: 0.01802646368741989 = 0.010817294009029865 + 0.001 * 7.209169387817383
Epoch 620, val loss: 1.2522810697555542
Epoch 630, training loss: 0.01741888001561165 = 0.010212971828877926 + 0.001 * 7.205908298492432
Epoch 630, val loss: 1.2617080211639404
Epoch 640, training loss: 0.01686958782374859 = 0.009660723619163036 + 0.001 * 7.208863258361816
Epoch 640, val loss: 1.2708624601364136
Epoch 650, training loss: 0.016356762498617172 = 0.009154771454632282 + 0.001 * 7.201991081237793
Epoch 650, val loss: 1.2797443866729736
Epoch 660, training loss: 0.01589668169617653 = 0.00869011227041483 + 0.001 * 7.206569671630859
Epoch 660, val loss: 1.2883837223052979
Epoch 670, training loss: 0.015461776405572891 = 0.008262389339506626 + 0.001 * 7.199386119842529
Epoch 670, val loss: 1.296790599822998
Epoch 680, training loss: 0.015062302350997925 = 0.00786777213215828 + 0.001 * 7.194530487060547
Epoch 680, val loss: 1.3049590587615967
Epoch 690, training loss: 0.014699321240186691 = 0.007502947933971882 + 0.001 * 7.1963725090026855
Epoch 690, val loss: 1.312921404838562
Epoch 700, training loss: 0.014360792003571987 = 0.00716498913243413 + 0.001 * 7.195802688598633
Epoch 700, val loss: 1.3206524848937988
Epoch 710, training loss: 0.014064265415072441 = 0.006851288955658674 + 0.001 * 7.212976455688477
Epoch 710, val loss: 1.3282006978988647
Epoch 720, training loss: 0.013745279982686043 = 0.006559582892805338 + 0.001 * 7.185696601867676
Epoch 720, val loss: 1.3355448246002197
Epoch 730, training loss: 0.013470163568854332 = 0.006287829484790564 + 0.001 * 7.1823344230651855
Epoch 730, val loss: 1.3427016735076904
Epoch 740, training loss: 0.013231475837528706 = 0.0060342526994645596 + 0.001 * 7.197222709655762
Epoch 740, val loss: 1.3496861457824707
Epoch 750, training loss: 0.012981949374079704 = 0.005797264631837606 + 0.001 * 7.184684753417969
Epoch 750, val loss: 1.3564940690994263
Epoch 760, training loss: 0.012752067297697067 = 0.005575438030064106 + 0.001 * 7.176629066467285
Epoch 760, val loss: 1.363152265548706
Epoch 770, training loss: 0.012547829188406467 = 0.00536748394370079 + 0.001 * 7.180345058441162
Epoch 770, val loss: 1.3696446418762207
Epoch 780, training loss: 0.012362102046608925 = 0.005172248929738998 + 0.001 * 7.189852714538574
Epoch 780, val loss: 1.3759852647781372
Epoch 790, training loss: 0.01216691080480814 = 0.004988625179976225 + 0.001 * 7.178285121917725
Epoch 790, val loss: 1.3821808099746704
Epoch 800, training loss: 0.011985654011368752 = 0.0048153940588235855 + 0.001 * 7.170259952545166
Epoch 800, val loss: 1.3882801532745361
Epoch 810, training loss: 0.011810874566435814 = 0.004650904797017574 + 0.001 * 7.159969806671143
Epoch 810, val loss: 1.3943463563919067
Epoch 820, training loss: 0.011671232990920544 = 0.004493520129472017 + 0.001 * 7.177712440490723
Epoch 820, val loss: 1.4003984928131104
Epoch 830, training loss: 0.011507445946335793 = 0.004342357628047466 + 0.001 * 7.165088653564453
Epoch 830, val loss: 1.4064757823944092
Epoch 840, training loss: 0.011351538822054863 = 0.004196804016828537 + 0.001 * 7.1547346115112305
Epoch 840, val loss: 1.4125638008117676
Epoch 850, training loss: 0.011237949132919312 = 0.0040568276308476925 + 0.001 * 7.181121349334717
Epoch 850, val loss: 1.4186055660247803
Epoch 860, training loss: 0.011084684170782566 = 0.003922651056200266 + 0.001 * 7.162032604217529
Epoch 860, val loss: 1.4246151447296143
Epoch 870, training loss: 0.010948841460049152 = 0.0037942787166684866 + 0.001 * 7.154561996459961
Epoch 870, val loss: 1.430567979812622
Epoch 880, training loss: 0.010838250629603863 = 0.0036717418115586042 + 0.001 * 7.166508674621582
Epoch 880, val loss: 1.4364265203475952
Epoch 890, training loss: 0.010705535300076008 = 0.0035549444146454334 + 0.001 * 7.150590419769287
Epoch 890, val loss: 1.4422359466552734
Epoch 900, training loss: 0.010585091076791286 = 0.0034437207505106926 + 0.001 * 7.141369819641113
Epoch 900, val loss: 1.4479405879974365
Epoch 910, training loss: 0.010500789619982243 = 0.0033378733787685633 + 0.001 * 7.16291618347168
Epoch 910, val loss: 1.4535630941390991
Epoch 920, training loss: 0.010371804237365723 = 0.003237225115299225 + 0.001 * 7.134579181671143
Epoch 920, val loss: 1.4590932130813599
Epoch 930, training loss: 0.010274277999997139 = 0.0031415061093866825 + 0.001 * 7.132771968841553
Epoch 930, val loss: 1.4645112752914429
Epoch 940, training loss: 0.010190675035119057 = 0.0030505068134516478 + 0.001 * 7.140168190002441
Epoch 940, val loss: 1.4698407649993896
Epoch 950, training loss: 0.010112937539815903 = 0.0029639226850122213 + 0.001 * 7.149014472961426
Epoch 950, val loss: 1.4750375747680664
Epoch 960, training loss: 0.010049678385257721 = 0.002881495049223304 + 0.001 * 7.168183326721191
Epoch 960, val loss: 1.480144739151001
Epoch 970, training loss: 0.009923601523041725 = 0.002803004812449217 + 0.001 * 7.120595932006836
Epoch 970, val loss: 1.4851527214050293
Epoch 980, training loss: 0.009879332035779953 = 0.002728218212723732 + 0.001 * 7.151113033294678
Epoch 980, val loss: 1.4900729656219482
Epoch 990, training loss: 0.009767163544893265 = 0.0026569650508463383 + 0.001 * 7.110198497772217
Epoch 990, val loss: 1.4948842525482178
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.7949393779652083
The final CL Acc:0.74321, 0.01222, The final GNN Acc:0.80302, 0.00742
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13228])
remove edge: torch.Size([2, 7852])
updated graph: torch.Size([2, 10524])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9564834833145142 = 1.94788658618927 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.9443001747131348
Epoch 10, training loss: 1.9462711811065674 = 1.9376744031906128 + 0.001 * 8.596790313720703
Epoch 10, val loss: 1.933689832687378
Epoch 20, training loss: 1.93378746509552 = 1.925190806388855 + 0.001 * 8.59661865234375
Epoch 20, val loss: 1.9205024242401123
Epoch 30, training loss: 1.9164915084838867 = 1.9078953266143799 + 0.001 * 8.596199035644531
Epoch 30, val loss: 1.9020930528640747
Epoch 40, training loss: 1.891201376914978 = 1.8826061487197876 + 0.001 * 8.595199584960938
Epoch 40, val loss: 1.8753916025161743
Epoch 50, training loss: 1.8556565046310425 = 1.8470641374588013 + 0.001 * 8.592403411865234
Epoch 50, val loss: 1.8393484354019165
Epoch 60, training loss: 1.8147014379501343 = 1.8061188459396362 + 0.001 * 8.582649230957031
Epoch 60, val loss: 1.801990032196045
Epoch 70, training loss: 1.7812519073486328 = 1.772708773612976 + 0.001 * 8.543086051940918
Epoch 70, val loss: 1.7761350870132446
Epoch 80, training loss: 1.7438616752624512 = 1.735514521598816 + 0.001 * 8.347166061401367
Epoch 80, val loss: 1.744181752204895
Epoch 90, training loss: 1.6918461322784424 = 1.683779239654541 + 0.001 * 8.066866874694824
Epoch 90, val loss: 1.6977388858795166
Epoch 100, training loss: 1.6218163967132568 = 1.6138434410095215 + 0.001 * 7.972964763641357
Epoch 100, val loss: 1.636115312576294
Epoch 110, training loss: 1.5345112085342407 = 1.5266187191009521 + 0.001 * 7.892440319061279
Epoch 110, val loss: 1.560492992401123
Epoch 120, training loss: 1.439883828163147 = 1.432096242904663 + 0.001 * 7.787566661834717
Epoch 120, val loss: 1.4805902242660522
Epoch 130, training loss: 1.3445990085601807 = 1.3369687795639038 + 0.001 * 7.630266189575195
Epoch 130, val loss: 1.4012389183044434
Epoch 140, training loss: 1.249218225479126 = 1.2417447566986084 + 0.001 * 7.473489284515381
Epoch 140, val loss: 1.3237532377243042
Epoch 150, training loss: 1.152894139289856 = 1.1454542875289917 + 0.001 * 7.4398884773254395
Epoch 150, val loss: 1.2469366788864136
Epoch 160, training loss: 1.0559090375900269 = 1.0485039949417114 + 0.001 * 7.405060291290283
Epoch 160, val loss: 1.1706961393356323
Epoch 170, training loss: 0.9596582055091858 = 0.9522749185562134 + 0.001 * 7.383289813995361
Epoch 170, val loss: 1.0960218906402588
Epoch 180, training loss: 0.8668714165687561 = 0.8595098853111267 + 0.001 * 7.361553192138672
Epoch 180, val loss: 1.0244444608688354
Epoch 190, training loss: 0.7804462313652039 = 0.7731069922447205 + 0.001 * 7.339240074157715
Epoch 190, val loss: 0.9577955603599548
Epoch 200, training loss: 0.702212393283844 = 0.6948950886726379 + 0.001 * 7.317275047302246
Epoch 200, val loss: 0.8979389071464539
Epoch 210, training loss: 0.6323621869087219 = 0.6250669360160828 + 0.001 * 7.295270919799805
Epoch 210, val loss: 0.8460487723350525
Epoch 220, training loss: 0.5700511336326599 = 0.562775731086731 + 0.001 * 7.275420188903809
Epoch 220, val loss: 0.8022782206535339
Epoch 230, training loss: 0.5140881538391113 = 0.506833553314209 + 0.001 * 7.25462007522583
Epoch 230, val loss: 0.766642689704895
Epoch 240, training loss: 0.4635013937950134 = 0.4562709331512451 + 0.001 * 7.2304511070251465
Epoch 240, val loss: 0.7382112741470337
Epoch 250, training loss: 0.41753435134887695 = 0.4103204607963562 + 0.001 * 7.21389102935791
Epoch 250, val loss: 0.7158528566360474
Epoch 260, training loss: 0.37558412551879883 = 0.36837783455848694 + 0.001 * 7.206299781799316
Epoch 260, val loss: 0.6979880332946777
Epoch 270, training loss: 0.3372335731983185 = 0.33002859354019165 + 0.001 * 7.2049689292907715
Epoch 270, val loss: 0.6832302212715149
Epoch 280, training loss: 0.3020438849925995 = 0.29483917355537415 + 0.001 * 7.204704284667969
Epoch 280, val loss: 0.6707030534744263
Epoch 290, training loss: 0.2696372866630554 = 0.262432724237442 + 0.001 * 7.20456600189209
Epoch 290, val loss: 0.6600384712219238
Epoch 300, training loss: 0.23978650569915771 = 0.2325822412967682 + 0.001 * 7.204259872436523
Epoch 300, val loss: 0.6512355804443359
Epoch 310, training loss: 0.2124900370836258 = 0.20528410375118256 + 0.001 * 7.205936431884766
Epoch 310, val loss: 0.6444479823112488
Epoch 320, training loss: 0.18790362775325775 = 0.18069885671138763 + 0.001 * 7.204774856567383
Epoch 320, val loss: 0.6399659514427185
Epoch 330, training loss: 0.16615287959575653 = 0.15894931554794312 + 0.001 * 7.20355749130249
Epoch 330, val loss: 0.6379574537277222
Epoch 340, training loss: 0.1471838802099228 = 0.13998067378997803 + 0.001 * 7.2032084465026855
Epoch 340, val loss: 0.6384462118148804
Epoch 350, training loss: 0.1307561844587326 = 0.12355164438486099 + 0.001 * 7.20453405380249
Epoch 350, val loss: 0.6412343382835388
Epoch 360, training loss: 0.1165347546339035 = 0.10932905972003937 + 0.001 * 7.205693244934082
Epoch 360, val loss: 0.6459452509880066
Epoch 370, training loss: 0.10419195145368576 = 0.09699045121669769 + 0.001 * 7.201500415802002
Epoch 370, val loss: 0.6522440910339355
Epoch 380, training loss: 0.09344932436943054 = 0.0862487331032753 + 0.001 * 7.200589179992676
Epoch 380, val loss: 0.6598043441772461
Epoch 390, training loss: 0.08407235890626907 = 0.07686891406774521 + 0.001 * 7.2034478187561035
Epoch 390, val loss: 0.6683520674705505
Epoch 400, training loss: 0.07586080580949783 = 0.06866336613893509 + 0.001 * 7.197437286376953
Epoch 400, val loss: 0.6776347160339355
Epoch 410, training loss: 0.06867096573114395 = 0.06147492676973343 + 0.001 * 7.196042060852051
Epoch 410, val loss: 0.6874580979347229
Epoch 420, training loss: 0.06236821413040161 = 0.055171605199575424 + 0.001 * 7.196609020233154
Epoch 420, val loss: 0.697692334651947
Epoch 430, training loss: 0.05683539807796478 = 0.04963458701968193 + 0.001 * 7.200809478759766
Epoch 430, val loss: 0.7081636190414429
Epoch 440, training loss: 0.05196249485015869 = 0.0447629950940609 + 0.001 * 7.199498653411865
Epoch 440, val loss: 0.7187318801879883
Epoch 450, training loss: 0.047665923833847046 = 0.04047422856092453 + 0.001 * 7.191693305969238
Epoch 450, val loss: 0.7293175458908081
Epoch 460, training loss: 0.04388221353292465 = 0.0366952084004879 + 0.001 * 7.18700647354126
Epoch 460, val loss: 0.7398844361305237
Epoch 470, training loss: 0.04054524376988411 = 0.03336261212825775 + 0.001 * 7.1826300621032715
Epoch 470, val loss: 0.7503837943077087
Epoch 480, training loss: 0.03760192543268204 = 0.030420200899243355 + 0.001 * 7.18172550201416
Epoch 480, val loss: 0.7607346177101135
Epoch 490, training loss: 0.03500037267804146 = 0.027818061411380768 + 0.001 * 7.182312488555908
Epoch 490, val loss: 0.7708905339241028
Epoch 500, training loss: 0.03269689530134201 = 0.025511575862765312 + 0.001 * 7.185317516326904
Epoch 500, val loss: 0.7808479070663452
Epoch 510, training loss: 0.030647549778223038 = 0.02346249483525753 + 0.001 * 7.185055732727051
Epoch 510, val loss: 0.7905842661857605
Epoch 520, training loss: 0.028822319582104683 = 0.02163735404610634 + 0.001 * 7.18496561050415
Epoch 520, val loss: 0.8000854849815369
Epoch 530, training loss: 0.027188504114747047 = 0.02000732533633709 + 0.001 * 7.181179046630859
Epoch 530, val loss: 0.8093474507331848
Epoch 540, training loss: 0.02572714164853096 = 0.01854766346514225 + 0.001 * 7.179478168487549
Epoch 540, val loss: 0.8183665871620178
Epoch 550, training loss: 0.02441447228193283 = 0.017236897721886635 + 0.001 * 7.177574157714844
Epoch 550, val loss: 0.8271468281745911
Epoch 560, training loss: 0.02324078418314457 = 0.016056707128882408 + 0.001 * 7.184076309204102
Epoch 560, val loss: 0.8356756567955017
Epoch 570, training loss: 0.022160332649946213 = 0.01499145571142435 + 0.001 * 7.168875694274902
Epoch 570, val loss: 0.8439745903015137
Epoch 580, training loss: 0.02119164913892746 = 0.014027458615601063 + 0.001 * 7.16418981552124
Epoch 580, val loss: 0.8520258069038391
Epoch 590, training loss: 0.020340383052825928 = 0.013152912259101868 + 0.001 * 7.187469482421875
Epoch 590, val loss: 0.8598573207855225
Epoch 600, training loss: 0.01951371505856514 = 0.012357501313090324 + 0.001 * 7.156212329864502
Epoch 600, val loss: 0.8674556612968445
Epoch 610, training loss: 0.01878213696181774 = 0.01163236889988184 + 0.001 * 7.1497673988342285
Epoch 610, val loss: 0.8748335838317871
Epoch 620, training loss: 0.01812339574098587 = 0.010969684459269047 + 0.001 * 7.153711318969727
Epoch 620, val loss: 0.8820032477378845
Epoch 630, training loss: 0.01751580275595188 = 0.010362709872424603 + 0.001 * 7.153092861175537
Epoch 630, val loss: 0.8889623880386353
Epoch 640, training loss: 0.01694299653172493 = 0.009805746376514435 + 0.001 * 7.137250900268555
Epoch 640, val loss: 0.8957118988037109
Epoch 650, training loss: 0.01642986200749874 = 0.009293541312217712 + 0.001 * 7.1363205909729
Epoch 650, val loss: 0.9022901058197021
Epoch 660, training loss: 0.015950189903378487 = 0.008821631781756878 + 0.001 * 7.128557205200195
Epoch 660, val loss: 0.9086893796920776
Epoch 670, training loss: 0.015558920800685883 = 0.00838589109480381 + 0.001 * 7.173028945922852
Epoch 670, val loss: 0.9149107933044434
Epoch 680, training loss: 0.015112895518541336 = 0.007982848212122917 + 0.001 * 7.13004732131958
Epoch 680, val loss: 0.9209542870521545
Epoch 690, training loss: 0.014735471457242966 = 0.007609341759234667 + 0.001 * 7.126128673553467
Epoch 690, val loss: 0.9268385767936707
Epoch 700, training loss: 0.014374539256095886 = 0.007262676488608122 + 0.001 * 7.111862659454346
Epoch 700, val loss: 0.932555615901947
Epoch 710, training loss: 0.014055999927222729 = 0.006940355058759451 + 0.001 * 7.115644454956055
Epoch 710, val loss: 0.938122570514679
Epoch 720, training loss: 0.013754133135080338 = 0.006640210747718811 + 0.001 * 7.113922119140625
Epoch 720, val loss: 0.9435383081436157
Epoch 730, training loss: 0.013492757454514503 = 0.006360247265547514 + 0.001 * 7.132509231567383
Epoch 730, val loss: 0.948809027671814
Epoch 740, training loss: 0.013230713084340096 = 0.006098770536482334 + 0.001 * 7.1319427490234375
Epoch 740, val loss: 0.9539328217506409
Epoch 750, training loss: 0.01295948214828968 = 0.005854180548340082 + 0.001 * 7.105301380157471
Epoch 750, val loss: 0.9589316844940186
Epoch 760, training loss: 0.012761364690959454 = 0.005625083111226559 + 0.001 * 7.1362810134887695
Epoch 760, val loss: 0.9637966752052307
Epoch 770, training loss: 0.012498477473855019 = 0.0054102386347949505 + 0.001 * 7.088238716125488
Epoch 770, val loss: 0.9685397744178772
Epoch 780, training loss: 0.012310076504945755 = 0.0052084787748754025 + 0.001 * 7.101596832275391
Epoch 780, val loss: 0.9731521010398865
Epoch 790, training loss: 0.012120313942432404 = 0.005018776748329401 + 0.001 * 7.101536273956299
Epoch 790, val loss: 0.9776499271392822
Epoch 800, training loss: 0.011930705979466438 = 0.004840149078518152 + 0.001 * 7.0905561447143555
Epoch 800, val loss: 0.9820364713668823
Epoch 810, training loss: 0.011752930469810963 = 0.004671832546591759 + 0.001 * 7.081097602844238
Epoch 810, val loss: 0.9863092303276062
Epoch 820, training loss: 0.011611003428697586 = 0.004513033200055361 + 0.001 * 7.097970485687256
Epoch 820, val loss: 0.9904863238334656
Epoch 830, training loss: 0.011418748646974564 = 0.00436303811147809 + 0.001 * 7.055710315704346
Epoch 830, val loss: 0.9945478439331055
Epoch 840, training loss: 0.011303967796266079 = 0.0042212363332509995 + 0.001 * 7.082731246948242
Epoch 840, val loss: 0.9985143542289734
Epoch 850, training loss: 0.011166315525770187 = 0.004087036941200495 + 0.001 * 7.079277992248535
Epoch 850, val loss: 1.0023876428604126
Epoch 860, training loss: 0.01103944145143032 = 0.003959788009524345 + 0.001 * 7.079653739929199
Epoch 860, val loss: 1.0061800479888916
Epoch 870, training loss: 0.010960480198264122 = 0.00383882038295269 + 0.001 * 7.121659278869629
Epoch 870, val loss: 1.009895920753479
Epoch 880, training loss: 0.010766157880425453 = 0.0037232739850878716 + 0.001 * 7.042883396148682
Epoch 880, val loss: 1.0135365724563599
Epoch 890, training loss: 0.01067150104790926 = 0.003612286876887083 + 0.001 * 7.059213638305664
Epoch 890, val loss: 1.0171260833740234
Epoch 900, training loss: 0.010539106093347073 = 0.00350518268533051 + 0.001 * 7.0339226722717285
Epoch 900, val loss: 1.0206682682037354
Epoch 910, training loss: 0.010438621044158936 = 0.0034015183337032795 + 0.001 * 7.037102222442627
Epoch 910, val loss: 1.0241773128509521
Epoch 920, training loss: 0.010347114875912666 = 0.0033011315390467644 + 0.001 * 7.04598331451416
Epoch 920, val loss: 1.0276434421539307
Epoch 930, training loss: 0.010262010619044304 = 0.0032040374353528023 + 0.001 * 7.057972431182861
Epoch 930, val loss: 1.0310803651809692
Epoch 940, training loss: 0.010157270357012749 = 0.00311028934083879 + 0.001 * 7.046980381011963
Epoch 940, val loss: 1.0344687700271606
Epoch 950, training loss: 0.010048765689134598 = 0.0030200169421732426 + 0.001 * 7.028748512268066
Epoch 950, val loss: 1.0378146171569824
Epoch 960, training loss: 0.00994853489100933 = 0.002933224430307746 + 0.001 * 7.015309810638428
Epoch 960, val loss: 1.04111647605896
Epoch 970, training loss: 0.009889391250908375 = 0.0028498454485088587 + 0.001 * 7.039545059204102
Epoch 970, val loss: 1.0443511009216309
Epoch 980, training loss: 0.009783908724784851 = 0.0027698264457285404 + 0.001 * 7.014081954956055
Epoch 980, val loss: 1.04753577709198
Epoch 990, training loss: 0.00972217321395874 = 0.002693156013265252 + 0.001 * 7.029016971588135
Epoch 990, val loss: 1.0506609678268433
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 1.9422760009765625 = 1.933679223060608 + 0.001 * 8.596814155578613
Epoch 0, val loss: 1.9281315803527832
Epoch 10, training loss: 1.9332897663116455 = 1.924692988395691 + 0.001 * 8.596770286560059
Epoch 10, val loss: 1.918787956237793
Epoch 20, training loss: 1.9222497940063477 = 1.9136531352996826 + 0.001 * 8.596598625183105
Epoch 20, val loss: 1.9072353839874268
Epoch 30, training loss: 1.9067184925079346 = 1.8981223106384277 + 0.001 * 8.596200942993164
Epoch 30, val loss: 1.8911757469177246
Epoch 40, training loss: 1.8837568759918213 = 1.8751616477966309 + 0.001 * 8.595203399658203
Epoch 40, val loss: 1.8679908514022827
Epoch 50, training loss: 1.8512762784957886 = 1.842684030532837 + 0.001 * 8.59222412109375
Epoch 50, val loss: 1.8368397951126099
Epoch 60, training loss: 1.8122777938842773 = 1.8036974668502808 + 0.001 * 8.58036994934082
Epoch 60, val loss: 1.8032549619674683
Epoch 70, training loss: 1.7745569944381714 = 1.7660387754440308 + 0.001 * 8.518226623535156
Epoch 70, val loss: 1.7737544775009155
Epoch 80, training loss: 1.7286704778671265 = 1.7205462455749512 + 0.001 * 8.124256134033203
Epoch 80, val loss: 1.7330135107040405
Epoch 90, training loss: 1.6637437343597412 = 1.6558068990707397 + 0.001 * 7.936875343322754
Epoch 90, val loss: 1.6737430095672607
Epoch 100, training loss: 1.5762776136398315 = 1.568483591079712 + 0.001 * 7.793989658355713
Epoch 100, val loss: 1.5983275175094604
Epoch 110, training loss: 1.4705220460891724 = 1.4629405736923218 + 0.001 * 7.581472873687744
Epoch 110, val loss: 1.5073434114456177
Epoch 120, training loss: 1.357309103012085 = 1.3498075008392334 + 0.001 * 7.501587390899658
Epoch 120, val loss: 1.4114584922790527
Epoch 130, training loss: 1.2442768812179565 = 1.236814260482788 + 0.001 * 7.4625630378723145
Epoch 130, val loss: 1.3178592920303345
Epoch 140, training loss: 1.1336857080459595 = 1.1262766122817993 + 0.001 * 7.409142017364502
Epoch 140, val loss: 1.2286744117736816
Epoch 150, training loss: 1.0258620977401733 = 1.0185202360153198 + 0.001 * 7.341890335083008
Epoch 150, val loss: 1.1439235210418701
Epoch 160, training loss: 0.9219191074371338 = 0.9146417379379272 + 0.001 * 7.277349472045898
Epoch 160, val loss: 1.0637435913085938
Epoch 170, training loss: 0.8249456882476807 = 0.8177005052566528 + 0.001 * 7.2451934814453125
Epoch 170, val loss: 0.9904525279998779
Epoch 180, training loss: 0.7384908199310303 = 0.7312672138214111 + 0.001 * 7.223610877990723
Epoch 180, val loss: 0.927277147769928
Epoch 190, training loss: 0.6640663743019104 = 0.6568644642829895 + 0.001 * 7.201902866363525
Epoch 190, val loss: 0.8763748407363892
Epoch 200, training loss: 0.6008639931678772 = 0.5936846137046814 + 0.001 * 7.17935848236084
Epoch 200, val loss: 0.8381474018096924
Epoch 210, training loss: 0.5469203591346741 = 0.5397543907165527 + 0.001 * 7.165944576263428
Epoch 210, val loss: 0.8107411861419678
Epoch 220, training loss: 0.4999813735485077 = 0.4928200840950012 + 0.001 * 7.161282539367676
Epoch 220, val loss: 0.7912852168083191
Epoch 230, training loss: 0.4578319489955902 = 0.4506717622280121 + 0.001 * 7.16019344329834
Epoch 230, val loss: 0.7768643498420715
Epoch 240, training loss: 0.41860321164131165 = 0.4114431142807007 + 0.001 * 7.160088539123535
Epoch 240, val loss: 0.765477180480957
Epoch 250, training loss: 0.38098669052124023 = 0.37382620573043823 + 0.001 * 7.160494327545166
Epoch 250, val loss: 0.7560576796531677
Epoch 260, training loss: 0.3443503975868225 = 0.337190181016922 + 0.001 * 7.160215377807617
Epoch 260, val loss: 0.7479429244995117
Epoch 270, training loss: 0.30871549248695374 = 0.30155375599861145 + 0.001 * 7.161749839782715
Epoch 270, val loss: 0.7410489916801453
Epoch 280, training loss: 0.27455490827560425 = 0.26739510893821716 + 0.001 * 7.159796714782715
Epoch 280, val loss: 0.7354511022567749
Epoch 290, training loss: 0.24248172342777252 = 0.23532317578792572 + 0.001 * 7.158552646636963
Epoch 290, val loss: 0.7314685583114624
Epoch 300, training loss: 0.21298399567604065 = 0.20582325756549835 + 0.001 * 7.160745143890381
Epoch 300, val loss: 0.7293308973312378
Epoch 310, training loss: 0.18634235858917236 = 0.1791873574256897 + 0.001 * 7.154997825622559
Epoch 310, val loss: 0.7293034791946411
Epoch 320, training loss: 0.16268090903759003 = 0.15552955865859985 + 0.001 * 7.151346206665039
Epoch 320, val loss: 0.7314352989196777
Epoch 330, training loss: 0.14195053279399872 = 0.1348012238740921 + 0.001 * 7.149305820465088
Epoch 330, val loss: 0.7357038259506226
Epoch 340, training loss: 0.1239820271730423 = 0.11683529615402222 + 0.001 * 7.146730899810791
Epoch 340, val loss: 0.7421032786369324
Epoch 350, training loss: 0.10852744430303574 = 0.1013883426785469 + 0.001 * 7.139098644256592
Epoch 350, val loss: 0.7503784894943237
Epoch 360, training loss: 0.09529381990432739 = 0.08816377073526382 + 0.001 * 7.130046367645264
Epoch 360, val loss: 0.7603692412376404
Epoch 370, training loss: 0.0840182676911354 = 0.07689159363508224 + 0.001 * 7.126673221588135
Epoch 370, val loss: 0.7713791728019714
Epoch 380, training loss: 0.07440486550331116 = 0.06728848814964294 + 0.001 * 7.116378307342529
Epoch 380, val loss: 0.7833546996116638
Epoch 390, training loss: 0.06622172892093658 = 0.059112392365932465 + 0.001 * 7.109337329864502
Epoch 390, val loss: 0.7959895133972168
Epoch 400, training loss: 0.05924864858388901 = 0.05214922875165939 + 0.001 * 7.099418640136719
Epoch 400, val loss: 0.8088122606277466
Epoch 410, training loss: 0.053304433822631836 = 0.04621368274092674 + 0.001 * 7.090751647949219
Epoch 410, val loss: 0.8218757510185242
Epoch 420, training loss: 0.04823170602321625 = 0.041145626455545425 + 0.001 * 7.086081027984619
Epoch 420, val loss: 0.8349883556365967
Epoch 430, training loss: 0.043890148401260376 = 0.03680937737226486 + 0.001 * 7.080769062042236
Epoch 430, val loss: 0.8479691743850708
Epoch 440, training loss: 0.040159545838832855 = 0.03308837115764618 + 0.001 * 7.071173667907715
Epoch 440, val loss: 0.8607458472251892
Epoch 450, training loss: 0.036958202719688416 = 0.0298834890127182 + 0.001 * 7.074711799621582
Epoch 450, val loss: 0.8733294606208801
Epoch 460, training loss: 0.034168221056461334 = 0.02711089700460434 + 0.001 * 7.057322978973389
Epoch 460, val loss: 0.8856732845306396
Epoch 470, training loss: 0.03175776079297066 = 0.024701496586203575 + 0.001 * 7.056265354156494
Epoch 470, val loss: 0.8977527022361755
Epoch 480, training loss: 0.029659485444426537 = 0.022598212584853172 + 0.001 * 7.061272144317627
Epoch 480, val loss: 0.9094936847686768
Epoch 490, training loss: 0.02780389040708542 = 0.020753612741827965 + 0.001 * 7.050276279449463
Epoch 490, val loss: 0.9209536910057068
Epoch 500, training loss: 0.02620772458612919 = 0.01912842132151127 + 0.001 * 7.07930326461792
Epoch 500, val loss: 0.9321300387382507
Epoch 510, training loss: 0.024731330573558807 = 0.017690233886241913 + 0.001 * 7.041095733642578
Epoch 510, val loss: 0.9430144429206848
Epoch 520, training loss: 0.0234596598893404 = 0.016411947086453438 + 0.001 * 7.047712802886963
Epoch 520, val loss: 0.953590989112854
Epoch 530, training loss: 0.02231975831091404 = 0.015270971693098545 + 0.001 * 7.048786163330078
Epoch 530, val loss: 0.9638997316360474
Epoch 540, training loss: 0.021282855421304703 = 0.014247211627662182 + 0.001 * 7.035643100738525
Epoch 540, val loss: 0.9739688634872437
Epoch 550, training loss: 0.020347965881228447 = 0.013322253711521626 + 0.001 * 7.025712013244629
Epoch 550, val loss: 0.9838148355484009
Epoch 560, training loss: 0.019524630159139633 = 0.012481046840548515 + 0.001 * 7.043582916259766
Epoch 560, val loss: 0.9935035705566406
Epoch 570, training loss: 0.01874992437660694 = 0.011713087558746338 + 0.001 * 7.036837100982666
Epoch 570, val loss: 1.003050446510315
Epoch 580, training loss: 0.01802867278456688 = 0.01101021096110344 + 0.001 * 7.018460750579834
Epoch 580, val loss: 1.0124608278274536
Epoch 590, training loss: 0.01743226684629917 = 0.010366089642047882 + 0.001 * 7.066176891326904
Epoch 590, val loss: 1.021732211112976
Epoch 600, training loss: 0.01680751144886017 = 0.009775501675903797 + 0.001 * 7.032008647918701
Epoch 600, val loss: 1.0308496952056885
Epoch 610, training loss: 0.016250213608145714 = 0.009233142249286175 + 0.001 * 7.017071723937988
Epoch 610, val loss: 1.0397940874099731
Epoch 620, training loss: 0.01575702801346779 = 0.00873439759016037 + 0.001 * 7.0226311683654785
Epoch 620, val loss: 1.0485844612121582
Epoch 630, training loss: 0.015291979536414146 = 0.008275295607745647 + 0.001 * 7.016683578491211
Epoch 630, val loss: 1.0571839809417725
Epoch 640, training loss: 0.014873162843286991 = 0.007852037437260151 + 0.001 * 7.021124839782715
Epoch 640, val loss: 1.0656020641326904
Epoch 650, training loss: 0.014478249475359917 = 0.007461213041096926 + 0.001 * 7.017035484313965
Epoch 650, val loss: 1.0738285779953003
Epoch 660, training loss: 0.014104461297392845 = 0.007099799811840057 + 0.001 * 7.0046610832214355
Epoch 660, val loss: 1.0818564891815186
Epoch 670, training loss: 0.013764997944235802 = 0.006765082478523254 + 0.001 * 6.999914646148682
Epoch 670, val loss: 1.089707612991333
Epoch 680, training loss: 0.013466758653521538 = 0.006454473827034235 + 0.001 * 7.012284278869629
Epoch 680, val loss: 1.0973632335662842
Epoch 690, training loss: 0.013165375217795372 = 0.0061659240163862705 + 0.001 * 6.999450206756592
Epoch 690, val loss: 1.1048696041107178
Epoch 700, training loss: 0.012904582545161247 = 0.005897409748286009 + 0.001 * 7.007172584533691
Epoch 700, val loss: 1.112183928489685
Epoch 710, training loss: 0.012655321508646011 = 0.005647111684083939 + 0.001 * 7.008209228515625
Epoch 710, val loss: 1.1193312406539917
Epoch 720, training loss: 0.012413294985890388 = 0.005413413047790527 + 0.001 * 6.999882221221924
Epoch 720, val loss: 1.1263033151626587
Epoch 730, training loss: 0.012186911888420582 = 0.005194738041609526 + 0.001 * 6.992173671722412
Epoch 730, val loss: 1.1331279277801514
Epoch 740, training loss: 0.011988524347543716 = 0.004989605862647295 + 0.001 * 6.998918533325195
Epoch 740, val loss: 1.139817714691162
Epoch 750, training loss: 0.011790085583925247 = 0.004796815570443869 + 0.001 * 6.993269920349121
Epoch 750, val loss: 1.1463788747787476
Epoch 760, training loss: 0.011599059216678143 = 0.004615141078829765 + 0.001 * 6.983917713165283
Epoch 760, val loss: 1.152826189994812
Epoch 770, training loss: 0.011427669785916805 = 0.004443408455699682 + 0.001 * 6.9842610359191895
Epoch 770, val loss: 1.159180760383606
Epoch 780, training loss: 0.011285752058029175 = 0.004280790686607361 + 0.001 * 7.0049614906311035
Epoch 780, val loss: 1.165429711341858
Epoch 790, training loss: 0.011103450320661068 = 0.004126594867557287 + 0.001 * 6.976855278015137
Epoch 790, val loss: 1.1715933084487915
Epoch 800, training loss: 0.01097789779305458 = 0.003980069421231747 + 0.001 * 6.997827529907227
Epoch 800, val loss: 1.1776940822601318
Epoch 810, training loss: 0.010835453867912292 = 0.003840880934149027 + 0.001 * 6.994572639465332
Epoch 810, val loss: 1.1836999654769897
Epoch 820, training loss: 0.010691247880458832 = 0.0037086820229887962 + 0.001 * 6.982565879821777
Epoch 820, val loss: 1.1896443367004395
Epoch 830, training loss: 0.010580475442111492 = 0.003582994220778346 + 0.001 * 6.997481346130371
Epoch 830, val loss: 1.1955217123031616
Epoch 840, training loss: 0.01044820062816143 = 0.0034633963368833065 + 0.001 * 6.984804153442383
Epoch 840, val loss: 1.2012871503829956
Epoch 850, training loss: 0.01034012995660305 = 0.0033496147952973843 + 0.001 * 6.990514278411865
Epoch 850, val loss: 1.206999659538269
Epoch 860, training loss: 0.010210614651441574 = 0.003241478931158781 + 0.001 * 6.96913480758667
Epoch 860, val loss: 1.2126092910766602
Epoch 870, training loss: 0.010114794597029686 = 0.003138698171824217 + 0.001 * 6.976095676422119
Epoch 870, val loss: 1.2180869579315186
Epoch 880, training loss: 0.010032392106950283 = 0.0030409295577555895 + 0.001 * 6.991461753845215
Epoch 880, val loss: 1.2235076427459717
Epoch 890, training loss: 0.009920193813741207 = 0.0029478801880031824 + 0.001 * 6.972313404083252
Epoch 890, val loss: 1.2288225889205933
Epoch 900, training loss: 0.009832853451371193 = 0.0028592494782060385 + 0.001 * 6.97360372543335
Epoch 900, val loss: 1.2340495586395264
Epoch 910, training loss: 0.009753992781043053 = 0.00277486233972013 + 0.001 * 6.979129791259766
Epoch 910, val loss: 1.2391846179962158
Epoch 920, training loss: 0.009662935510277748 = 0.0026944985147565603 + 0.001 * 6.968436241149902
Epoch 920, val loss: 1.24425208568573
Epoch 930, training loss: 0.009574714116752148 = 0.0026179307606071234 + 0.001 * 6.956782817840576
Epoch 930, val loss: 1.2491894960403442
Epoch 940, training loss: 0.009493607096374035 = 0.002544906223192811 + 0.001 * 6.948700904846191
Epoch 940, val loss: 1.2540535926818848
Epoch 950, training loss: 0.009440070018172264 = 0.0024751978926360607 + 0.001 * 6.964872360229492
Epoch 950, val loss: 1.2588332891464233
Epoch 960, training loss: 0.009386694990098476 = 0.0024086821358650923 + 0.001 * 6.978012561798096
Epoch 960, val loss: 1.2635170221328735
Epoch 970, training loss: 0.009295023046433926 = 0.0023451747838407755 + 0.001 * 6.94984769821167
Epoch 970, val loss: 1.268104910850525
Epoch 980, training loss: 0.009228154085576534 = 0.0022845251951366663 + 0.001 * 6.943628787994385
Epoch 980, val loss: 1.272625207901001
Epoch 990, training loss: 0.009174350649118423 = 0.0022265836596488953 + 0.001 * 6.94776725769043
Epoch 990, val loss: 1.2770456075668335
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 1.9486103057861328 = 1.9400134086608887 + 0.001 * 8.59683895111084
Epoch 0, val loss: 1.942143440246582
Epoch 10, training loss: 1.9388872385025024 = 1.9302904605865479 + 0.001 * 8.59678840637207
Epoch 10, val loss: 1.9325929880142212
Epoch 20, training loss: 1.9268476963043213 = 1.9182510375976562 + 0.001 * 8.5966157913208
Epoch 20, val loss: 1.9203087091445923
Epoch 30, training loss: 1.9098033905029297 = 1.9012072086334229 + 0.001 * 8.596200942993164
Epoch 30, val loss: 1.9025795459747314
Epoch 40, training loss: 1.884779691696167 = 1.8761844635009766 + 0.001 * 8.595200538635254
Epoch 40, val loss: 1.8765769004821777
Epoch 50, training loss: 1.8503117561340332 = 1.8417195081710815 + 0.001 * 8.592269897460938
Epoch 50, val loss: 1.8421322107315063
Epoch 60, training loss: 1.8118730783462524 = 1.803291916847229 + 0.001 * 8.581175804138184
Epoch 60, val loss: 1.807134985923767
Epoch 70, training loss: 1.7757796049118042 = 1.7672510147094727 + 0.001 * 8.528596878051758
Epoch 70, val loss: 1.7758468389511108
Epoch 80, training loss: 1.7289053201675415 = 1.7207144498825073 + 0.001 * 8.190852165222168
Epoch 80, val loss: 1.7329490184783936
Epoch 90, training loss: 1.6642543077468872 = 1.6562604904174805 + 0.001 * 7.993873596191406
Epoch 90, val loss: 1.6745562553405762
Epoch 100, training loss: 1.5785081386566162 = 1.5706597566604614 + 0.001 * 7.848341464996338
Epoch 100, val loss: 1.5994452238082886
Epoch 110, training loss: 1.4808299541473389 = 1.4731645584106445 + 0.001 * 7.66535758972168
Epoch 110, val loss: 1.5165246725082397
Epoch 120, training loss: 1.3835039138793945 = 1.3760284185409546 + 0.001 * 7.4755144119262695
Epoch 120, val loss: 1.4371031522750854
Epoch 130, training loss: 1.2878705263137817 = 1.2804590463638306 + 0.001 * 7.411468982696533
Epoch 130, val loss: 1.3618190288543701
Epoch 140, training loss: 1.1911659240722656 = 1.1838113069534302 + 0.001 * 7.354558944702148
Epoch 140, val loss: 1.2873798608779907
Epoch 150, training loss: 1.0938531160354614 = 1.0865296125411987 + 0.001 * 7.3235578536987305
Epoch 150, val loss: 1.212346076965332
Epoch 160, training loss: 0.99809730052948 = 0.9907935261726379 + 0.001 * 7.30377721786499
Epoch 160, val loss: 1.1383167505264282
Epoch 170, training loss: 0.9056910276412964 = 0.8983978033065796 + 0.001 * 7.293221950531006
Epoch 170, val loss: 1.0662226676940918
Epoch 180, training loss: 0.8179064393043518 = 0.8106191754341125 + 0.001 * 7.287250518798828
Epoch 180, val loss: 0.997207760810852
Epoch 190, training loss: 0.7360072135925293 = 0.7287237644195557 + 0.001 * 7.283458232879639
Epoch 190, val loss: 0.9333521127700806
Epoch 200, training loss: 0.66105055809021 = 0.6537694334983826 + 0.001 * 7.281134605407715
Epoch 200, val loss: 0.8770354986190796
Epoch 210, training loss: 0.5933766961097717 = 0.5860969424247742 + 0.001 * 7.279751300811768
Epoch 210, val loss: 0.8299723863601685
Epoch 220, training loss: 0.5328512191772461 = 0.5255721807479858 + 0.001 * 7.279017448425293
Epoch 220, val loss: 0.7930902242660522
Epoch 230, training loss: 0.4789275825023651 = 0.4716489017009735 + 0.001 * 7.278687953948975
Epoch 230, val loss: 0.7655394673347473
Epoch 240, training loss: 0.4307611286640167 = 0.42348313331604004 + 0.001 * 7.2779927253723145
Epoch 240, val loss: 0.745724618434906
Epoch 250, training loss: 0.38737815618515015 = 0.38010016083717346 + 0.001 * 7.277988433837891
Epoch 250, val loss: 0.7319738864898682
Epoch 260, training loss: 0.34798499941825867 = 0.34071171283721924 + 0.001 * 7.273285388946533
Epoch 260, val loss: 0.7230051755905151
Epoch 270, training loss: 0.31195124983787537 = 0.3046843707561493 + 0.001 * 7.266878604888916
Epoch 270, val loss: 0.7177489995956421
Epoch 280, training loss: 0.27873626351356506 = 0.27147725224494934 + 0.001 * 7.259014129638672
Epoch 280, val loss: 0.7155990600585938
Epoch 290, training loss: 0.2479662299156189 = 0.24071645736694336 + 0.001 * 7.249772071838379
Epoch 290, val loss: 0.7162841558456421
Epoch 300, training loss: 0.21948330104351044 = 0.21224172413349152 + 0.001 * 7.241572856903076
Epoch 300, val loss: 0.7194164991378784
Epoch 310, training loss: 0.19338344037532806 = 0.18614819645881653 + 0.001 * 7.23524808883667
Epoch 310, val loss: 0.7248029112815857
Epoch 320, training loss: 0.16986912488937378 = 0.16263563930988312 + 0.001 * 7.233484745025635
Epoch 320, val loss: 0.7324191331863403
Epoch 330, training loss: 0.14903634786605835 = 0.14180955290794373 + 0.001 * 7.226799011230469
Epoch 330, val loss: 0.7420769929885864
Epoch 340, training loss: 0.13083918392658234 = 0.12360459566116333 + 0.001 * 7.234590530395508
Epoch 340, val loss: 0.7535297870635986
Epoch 350, training loss: 0.11507350206375122 = 0.10784590989351273 + 0.001 * 7.227595329284668
Epoch 350, val loss: 0.7663515210151672
Epoch 360, training loss: 0.10152336210012436 = 0.09430024027824402 + 0.001 * 7.223121166229248
Epoch 360, val loss: 0.7802150249481201
Epoch 370, training loss: 0.08993633836507797 = 0.08271359652280807 + 0.001 * 7.222743988037109
Epoch 370, val loss: 0.7947072982788086
Epoch 380, training loss: 0.08004999160766602 = 0.07282668352127075 + 0.001 * 7.2233052253723145
Epoch 380, val loss: 0.8095728158950806
Epoch 390, training loss: 0.07160988450050354 = 0.06438734382390976 + 0.001 * 7.2225422859191895
Epoch 390, val loss: 0.8246245980262756
Epoch 400, training loss: 0.06439053267240524 = 0.057168181985616684 + 0.001 * 7.22235107421875
Epoch 400, val loss: 0.8396880626678467
Epoch 410, training loss: 0.05819754675030708 = 0.05097479000687599 + 0.001 * 7.222754955291748
Epoch 410, val loss: 0.8546863794326782
Epoch 420, training loss: 0.05286584794521332 = 0.04564472287893295 + 0.001 * 7.221124172210693
Epoch 420, val loss: 0.8695773482322693
Epoch 430, training loss: 0.04825932905077934 = 0.0410420224070549 + 0.001 * 7.217306137084961
Epoch 430, val loss: 0.8842652440071106
Epoch 440, training loss: 0.04426984861493111 = 0.03705129027366638 + 0.001 * 7.218559265136719
Epoch 440, val loss: 0.8987621665000916
Epoch 450, training loss: 0.04079505428671837 = 0.033577077090740204 + 0.001 * 7.217977523803711
Epoch 450, val loss: 0.9130159020423889
Epoch 460, training loss: 0.037764739245176315 = 0.03054075501859188 + 0.001 * 7.223984718322754
Epoch 460, val loss: 0.9269799590110779
Epoch 470, training loss: 0.03509499877691269 = 0.027878237888216972 + 0.001 * 7.216760635375977
Epoch 470, val loss: 0.9405946135520935
Epoch 480, training loss: 0.03274755924940109 = 0.025535520166158676 + 0.001 * 7.212039947509766
Epoch 480, val loss: 0.9539059400558472
Epoch 490, training loss: 0.030682824552059174 = 0.02346702665090561 + 0.001 * 7.215796947479248
Epoch 490, val loss: 0.966832160949707
Epoch 500, training loss: 0.028848160058259964 = 0.021634478121995926 + 0.001 * 7.213682174682617
Epoch 500, val loss: 0.9793890118598938
Epoch 510, training loss: 0.027216820046305656 = 0.020005904138088226 + 0.001 * 7.2109150886535645
Epoch 510, val loss: 0.9915839433670044
Epoch 520, training loss: 0.025766626000404358 = 0.018553679808974266 + 0.001 * 7.212946891784668
Epoch 520, val loss: 1.0034397840499878
Epoch 530, training loss: 0.024466969072818756 = 0.017254682257771492 + 0.001 * 7.212285995483398
Epoch 530, val loss: 1.0149379968643188
Epoch 540, training loss: 0.02329517900943756 = 0.016088996082544327 + 0.001 * 7.206181526184082
Epoch 540, val loss: 1.0260844230651855
Epoch 550, training loss: 0.022248990833759308 = 0.015039604157209396 + 0.001 * 7.209385395050049
Epoch 550, val loss: 1.0368989706039429
Epoch 560, training loss: 0.021301820874214172 = 0.014091926626861095 + 0.001 * 7.209893703460693
Epoch 560, val loss: 1.047402262687683
Epoch 570, training loss: 0.020436011254787445 = 0.013233580626547337 + 0.001 * 7.20242977142334
Epoch 570, val loss: 1.0575718879699707
Epoch 580, training loss: 0.019655674695968628 = 0.012453872710466385 + 0.001 * 7.2018022537231445
Epoch 580, val loss: 1.0674268007278442
Epoch 590, training loss: 0.01894882135093212 = 0.011743622832000256 + 0.001 * 7.205197811126709
Epoch 590, val loss: 1.0769751071929932
Epoch 600, training loss: 0.01829661801457405 = 0.01109493337571621 + 0.001 * 7.20168399810791
Epoch 600, val loss: 1.0862467288970947
Epoch 610, training loss: 0.01769665628671646 = 0.01050082128494978 + 0.001 * 7.195833683013916
Epoch 610, val loss: 1.0952433347702026
Epoch 620, training loss: 0.01716151274740696 = 0.009955080226063728 + 0.001 * 7.206432342529297
Epoch 620, val loss: 1.1039929389953613
Epoch 630, training loss: 0.01665186695754528 = 0.00945222843438387 + 0.001 * 7.199638366699219
Epoch 630, val loss: 1.1125118732452393
Epoch 640, training loss: 0.016180245205760002 = 0.00898703932762146 + 0.001 * 7.193204879760742
Epoch 640, val loss: 1.1207891702651978
Epoch 650, training loss: 0.01574285700917244 = 0.008554642088711262 + 0.001 * 7.18821382522583
Epoch 650, val loss: 1.1289170980453491
Epoch 660, training loss: 0.01535145565867424 = 0.008150901645421982 + 0.001 * 7.2005534172058105
Epoch 660, val loss: 1.1368653774261475
Epoch 670, training loss: 0.014964351430535316 = 0.007773116230964661 + 0.001 * 7.191235065460205
Epoch 670, val loss: 1.144653081893921
Epoch 680, training loss: 0.01461330708116293 = 0.007418921682983637 + 0.001 * 7.194385051727295
Epoch 680, val loss: 1.152347445487976
Epoch 690, training loss: 0.014273634180426598 = 0.007086456287652254 + 0.001 * 7.187177658081055
Epoch 690, val loss: 1.1599023342132568
Epoch 700, training loss: 0.013953464105725288 = 0.00677438173443079 + 0.001 * 7.179081439971924
Epoch 700, val loss: 1.1673250198364258
Epoch 710, training loss: 0.013656632974743843 = 0.006481448654085398 + 0.001 * 7.1751837730407715
Epoch 710, val loss: 1.174630045890808
Epoch 720, training loss: 0.01338125579059124 = 0.006206527817994356 + 0.001 * 7.174727439880371
Epoch 720, val loss: 1.1817840337753296
Epoch 730, training loss: 0.01313433051109314 = 0.005948357284069061 + 0.001 * 7.185973167419434
Epoch 730, val loss: 1.1887896060943604
Epoch 740, training loss: 0.012875476852059364 = 0.005705969873815775 + 0.001 * 7.169506072998047
Epoch 740, val loss: 1.1956396102905273
Epoch 750, training loss: 0.0126731526106596 = 0.0054781874641776085 + 0.001 * 7.19496488571167
Epoch 750, val loss: 1.2023589611053467
Epoch 760, training loss: 0.01244373805820942 = 0.005264254752546549 + 0.001 * 7.179482460021973
Epoch 760, val loss: 1.2088936567306519
Epoch 770, training loss: 0.012253297492861748 = 0.005063055548816919 + 0.001 * 7.190241813659668
Epoch 770, val loss: 1.2153104543685913
Epoch 780, training loss: 0.012038321234285831 = 0.004873730707913637 + 0.001 * 7.164590358734131
Epoch 780, val loss: 1.2215747833251953
Epoch 790, training loss: 0.011868303641676903 = 0.004695323295891285 + 0.001 * 7.172980308532715
Epoch 790, val loss: 1.2276992797851562
Epoch 800, training loss: 0.01169426366686821 = 0.004527099896222353 + 0.001 * 7.167163848876953
Epoch 800, val loss: 1.2336530685424805
Epoch 810, training loss: 0.011516755446791649 = 0.0043683587573468685 + 0.001 * 7.148396968841553
Epoch 810, val loss: 1.2395027875900269
Epoch 820, training loss: 0.011389074847102165 = 0.004218521062284708 + 0.001 * 7.170553684234619
Epoch 820, val loss: 1.2452207803726196
Epoch 830, training loss: 0.01124473288655281 = 0.0040770941413939 + 0.001 * 7.167638778686523
Epoch 830, val loss: 1.250769019126892
Epoch 840, training loss: 0.01111399196088314 = 0.003943291958421469 + 0.001 * 7.170699119567871
Epoch 840, val loss: 1.2562193870544434
Epoch 850, training loss: 0.010958278551697731 = 0.0038165454752743244 + 0.001 * 7.141732692718506
Epoch 850, val loss: 1.2615338563919067
Epoch 860, training loss: 0.010851699858903885 = 0.0036964728496968746 + 0.001 * 7.155227184295654
Epoch 860, val loss: 1.2667356729507446
Epoch 870, training loss: 0.010737268254160881 = 0.0035826500970870256 + 0.001 * 7.154617786407471
Epoch 870, val loss: 1.271819829940796
Epoch 880, training loss: 0.010629242286086082 = 0.003474772907793522 + 0.001 * 7.154468536376953
Epoch 880, val loss: 1.2767665386199951
Epoch 890, training loss: 0.010486122220754623 = 0.0033722815569490194 + 0.001 * 7.113840103149414
Epoch 890, val loss: 1.2816076278686523
Epoch 900, training loss: 0.010404097847640514 = 0.0032748274970799685 + 0.001 * 7.129269599914551
Epoch 900, val loss: 1.2863458395004272
Epoch 910, training loss: 0.010325244627892971 = 0.0031821599695831537 + 0.001 * 7.143084526062012
Epoch 910, val loss: 1.2909616231918335
Epoch 920, training loss: 0.010241081938147545 = 0.0030940878205001354 + 0.001 * 7.146994113922119
Epoch 920, val loss: 1.295501708984375
Epoch 930, training loss: 0.010119892656803131 = 0.0030101933516561985 + 0.001 * 7.10969877243042
Epoch 930, val loss: 1.299892783164978
Epoch 940, training loss: 0.01003310177475214 = 0.002930308459326625 + 0.001 * 7.102792739868164
Epoch 940, val loss: 1.304213285446167
Epoch 950, training loss: 0.009997915476560593 = 0.0028540852945297956 + 0.001 * 7.143829822540283
Epoch 950, val loss: 1.3084133863449097
Epoch 960, training loss: 0.009901728481054306 = 0.002781405346468091 + 0.001 * 7.120323181152344
Epoch 960, val loss: 1.3124949932098389
Epoch 970, training loss: 0.009793587028980255 = 0.002711927518248558 + 0.001 * 7.081658840179443
Epoch 970, val loss: 1.316511869430542
Epoch 980, training loss: 0.00973504688590765 = 0.0026454904582351446 + 0.001 * 7.089555740356445
Epoch 980, val loss: 1.3204388618469238
Epoch 990, training loss: 0.009679621085524559 = 0.002581835025921464 + 0.001 * 7.097785949707031
Epoch 990, val loss: 1.3243228197097778
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.838165524512388
The final CL Acc:0.81852, 0.01318, The final GNN Acc:0.84080, 0.00188
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10614])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9740707874298096 = 1.965474009513855 + 0.001 * 8.596817016601562
Epoch 0, val loss: 1.9605470895767212
Epoch 10, training loss: 1.9630367755889893 = 1.9544399976730347 + 0.001 * 8.596747398376465
Epoch 10, val loss: 1.9499938488006592
Epoch 20, training loss: 1.9492918252944946 = 1.9406952857971191 + 0.001 * 8.59652042388916
Epoch 20, val loss: 1.9362404346466064
Epoch 30, training loss: 1.9297446012496948 = 1.921148657798767 + 0.001 * 8.595958709716797
Epoch 30, val loss: 1.9160237312316895
Epoch 40, training loss: 1.9004727602005005 = 1.891878366470337 + 0.001 * 8.594450950622559
Epoch 40, val loss: 1.8854531049728394
Epoch 50, training loss: 1.859831690788269 = 1.8512417078018188 + 0.001 * 8.589973449707031
Epoch 50, val loss: 1.8446545600891113
Epoch 60, training loss: 1.8186898231506348 = 1.8101145029067993 + 0.001 * 8.575362205505371
Epoch 60, val loss: 1.808099389076233
Epoch 70, training loss: 1.791015625 = 1.7824985980987549 + 0.001 * 8.517006874084473
Epoch 70, val loss: 1.7868223190307617
Epoch 80, training loss: 1.7567284107208252 = 1.7485181093215942 + 0.001 * 8.210248947143555
Epoch 80, val loss: 1.7580034732818604
Epoch 90, training loss: 1.7105481624603271 = 1.7025015354156494 + 0.001 * 8.046584129333496
Epoch 90, val loss: 1.7181531190872192
Epoch 100, training loss: 1.6455280780792236 = 1.6376349925994873 + 0.001 * 7.893069267272949
Epoch 100, val loss: 1.6630979776382446
Epoch 110, training loss: 1.5623841285705566 = 1.554670810699463 + 0.001 * 7.7133564949035645
Epoch 110, val loss: 1.5935581922531128
Epoch 120, training loss: 1.4709442853927612 = 1.463334560394287 + 0.001 * 7.609731197357178
Epoch 120, val loss: 1.5182851552963257
Epoch 130, training loss: 1.379996657371521 = 1.3723783493041992 + 0.001 * 7.61831521987915
Epoch 130, val loss: 1.4443528652191162
Epoch 140, training loss: 1.288926362991333 = 1.2813105583190918 + 0.001 * 7.6158342361450195
Epoch 140, val loss: 1.372625470161438
Epoch 150, training loss: 1.1954538822174072 = 1.1878430843353271 + 0.001 * 7.610830307006836
Epoch 150, val loss: 1.29888916015625
Epoch 160, training loss: 1.0996495485305786 = 1.0920437574386597 + 0.001 * 7.605739593505859
Epoch 160, val loss: 1.2241939306259155
Epoch 170, training loss: 1.0043776035308838 = 0.9967827200889587 + 0.001 * 7.594856262207031
Epoch 170, val loss: 1.1517640352249146
Epoch 180, training loss: 0.9132382273674011 = 0.9056618213653564 + 0.001 * 7.576422214508057
Epoch 180, val loss: 1.084896206855774
Epoch 190, training loss: 0.8290001153945923 = 0.8214579820632935 + 0.001 * 7.542107582092285
Epoch 190, val loss: 1.0264248847961426
Epoch 200, training loss: 0.7531895637512207 = 0.7457228302955627 + 0.001 * 7.466760635375977
Epoch 200, val loss: 0.9774770140647888
Epoch 210, training loss: 0.6862501502037048 = 0.6788798570632935 + 0.001 * 7.3702802658081055
Epoch 210, val loss: 0.9386044144630432
Epoch 220, training loss: 0.6278021335601807 = 0.6204307675361633 + 0.001 * 7.371357440948486
Epoch 220, val loss: 0.9095577597618103
Epoch 230, training loss: 0.5761910080909729 = 0.5688375234603882 + 0.001 * 7.353512763977051
Epoch 230, val loss: 0.8894244432449341
Epoch 240, training loss: 0.5296485424041748 = 0.5223005414009094 + 0.001 * 7.347975730895996
Epoch 240, val loss: 0.8765514492988586
Epoch 250, training loss: 0.486726313829422 = 0.47938597202301025 + 0.001 * 7.340334892272949
Epoch 250, val loss: 0.8689014315605164
Epoch 260, training loss: 0.44661277532577515 = 0.4392792284488678 + 0.001 * 7.333542823791504
Epoch 260, val loss: 0.8651138544082642
Epoch 270, training loss: 0.40907275676727295 = 0.40174636244773865 + 0.001 * 7.3263983726501465
Epoch 270, val loss: 0.8645846843719482
Epoch 280, training loss: 0.374294638633728 = 0.36697643995285034 + 0.001 * 7.318200588226318
Epoch 280, val loss: 0.8668333888053894
Epoch 290, training loss: 0.34258079528808594 = 0.3352695107460022 + 0.001 * 7.311295509338379
Epoch 290, val loss: 0.8717671632766724
Epoch 300, training loss: 0.3140150010585785 = 0.30671724677085876 + 0.001 * 7.2977519035339355
Epoch 300, val loss: 0.8792470097541809
Epoch 310, training loss: 0.28843653202056885 = 0.2811509966850281 + 0.001 * 7.285543441772461
Epoch 310, val loss: 0.8890377879142761
Epoch 320, training loss: 0.2653869390487671 = 0.2581111490726471 + 0.001 * 7.27578592300415
Epoch 320, val loss: 0.9008767008781433
Epoch 330, training loss: 0.24412216246128082 = 0.23686344921588898 + 0.001 * 7.258719444274902
Epoch 330, val loss: 0.9143894910812378
Epoch 340, training loss: 0.2238089144229889 = 0.21652944386005402 + 0.001 * 7.279474258422852
Epoch 340, val loss: 0.9291108250617981
Epoch 350, training loss: 0.2035672515630722 = 0.1963345855474472 + 0.001 * 7.232666969299316
Epoch 350, val loss: 0.9451843500137329
Epoch 360, training loss: 0.18329781293869019 = 0.1760774552822113 + 0.001 * 7.22035551071167
Epoch 360, val loss: 0.9626947641372681
Epoch 370, training loss: 0.1637059897184372 = 0.1564984917640686 + 0.001 * 7.207494258880615
Epoch 370, val loss: 0.9818083643913269
Epoch 380, training loss: 0.14584508538246155 = 0.13864970207214355 + 0.001 * 7.195388317108154
Epoch 380, val loss: 1.002026081085205
Epoch 390, training loss: 0.13023318350315094 = 0.12301722913980484 + 0.001 * 7.21595573425293
Epoch 390, val loss: 1.0228601694107056
Epoch 400, training loss: 0.11660511791706085 = 0.10941024869680405 + 0.001 * 7.194868087768555
Epoch 400, val loss: 1.0442453622817993
Epoch 410, training loss: 0.10467543452978134 = 0.0974978655576706 + 0.001 * 7.177565574645996
Epoch 410, val loss: 1.0665559768676758
Epoch 420, training loss: 0.0942000076174736 = 0.08701149374246597 + 0.001 * 7.188516616821289
Epoch 420, val loss: 1.0899887084960938
Epoch 430, training loss: 0.0849376767873764 = 0.07776063680648804 + 0.001 * 7.177037715911865
Epoch 430, val loss: 1.114160418510437
Epoch 440, training loss: 0.07679176330566406 = 0.06961356848478317 + 0.001 * 7.178195953369141
Epoch 440, val loss: 1.1385273933410645
Epoch 450, training loss: 0.06960727274417877 = 0.06245040148496628 + 0.001 * 7.156867980957031
Epoch 450, val loss: 1.1630033254623413
Epoch 460, training loss: 0.06329979747533798 = 0.05615159124135971 + 0.001 * 7.1482038497924805
Epoch 460, val loss: 1.1873178482055664
Epoch 470, training loss: 0.057762112468481064 = 0.05060994252562523 + 0.001 * 7.152169704437256
Epoch 470, val loss: 1.2112125158309937
Epoch 480, training loss: 0.05287649482488632 = 0.04573524370789528 + 0.001 * 7.141250133514404
Epoch 480, val loss: 1.2348449230194092
Epoch 490, training loss: 0.04857746139168739 = 0.041440580040216446 + 0.001 * 7.1368818283081055
Epoch 490, val loss: 1.2579516172409058
Epoch 500, training loss: 0.04478316754102707 = 0.03765218332409859 + 0.001 * 7.130981922149658
Epoch 500, val loss: 1.2804852724075317
Epoch 510, training loss: 0.04143378511071205 = 0.03430876508355141 + 0.001 * 7.125018119812012
Epoch 510, val loss: 1.3024611473083496
Epoch 520, training loss: 0.038484811782836914 = 0.031350623816251755 + 0.001 * 7.134189128875732
Epoch 520, val loss: 1.3238334655761719
Epoch 530, training loss: 0.035848554223775864 = 0.02872784249484539 + 0.001 * 7.1207098960876465
Epoch 530, val loss: 1.3446201086044312
Epoch 540, training loss: 0.03351769596338272 = 0.026398345828056335 + 0.001 * 7.119349479675293
Epoch 540, val loss: 1.364723563194275
Epoch 550, training loss: 0.03146596625447273 = 0.02432374283671379 + 0.001 * 7.1422224044799805
Epoch 550, val loss: 1.3841562271118164
Epoch 560, training loss: 0.029580922797322273 = 0.022469645366072655 + 0.001 * 7.1112775802612305
Epoch 560, val loss: 1.4029638767242432
Epoch 570, training loss: 0.027926955372095108 = 0.020811155438423157 + 0.001 * 7.115799903869629
Epoch 570, val loss: 1.421128273010254
Epoch 580, training loss: 0.026432586833834648 = 0.019324220716953278 + 0.001 * 7.108366012573242
Epoch 580, val loss: 1.4386502504348755
Epoch 590, training loss: 0.02511819638311863 = 0.017987098544836044 + 0.001 * 7.131097316741943
Epoch 590, val loss: 1.4556432962417603
Epoch 600, training loss: 0.023885304108262062 = 0.016781741753220558 + 0.001 * 7.103561878204346
Epoch 600, val loss: 1.4720853567123413
Epoch 610, training loss: 0.0227944515645504 = 0.015692127868533134 + 0.001 * 7.102322578430176
Epoch 610, val loss: 1.4879295825958252
Epoch 620, training loss: 0.021813591942191124 = 0.014704672619700432 + 0.001 * 7.108919143676758
Epoch 620, val loss: 1.5032461881637573
Epoch 630, training loss: 0.020909110084176064 = 0.013807695358991623 + 0.001 * 7.101413726806641
Epoch 630, val loss: 1.5180665254592896
Epoch 640, training loss: 0.020091434940695763 = 0.012991154566407204 + 0.001 * 7.100279808044434
Epoch 640, val loss: 1.5323525667190552
Epoch 650, training loss: 0.01934138312935829 = 0.012246101163327694 + 0.001 * 7.095281600952148
Epoch 650, val loss: 1.5462487936019897
Epoch 660, training loss: 0.018657328560948372 = 0.011564921587705612 + 0.001 * 7.092406272888184
Epoch 660, val loss: 1.5595818758010864
Epoch 670, training loss: 0.01805226132273674 = 0.010940924286842346 + 0.001 * 7.111336708068848
Epoch 670, val loss: 1.5725796222686768
Epoch 680, training loss: 0.0174612607806921 = 0.010368027724325657 + 0.001 * 7.09323263168335
Epoch 680, val loss: 1.585129737854004
Epoch 690, training loss: 0.01693321391940117 = 0.009840751998126507 + 0.001 * 7.092461585998535
Epoch 690, val loss: 1.597350835800171
Epoch 700, training loss: 0.016439568251371384 = 0.009354393929243088 + 0.001 * 7.085174083709717
Epoch 700, val loss: 1.6091673374176025
Epoch 710, training loss: 0.01600661315023899 = 0.008904951624572277 + 0.001 * 7.101661205291748
Epoch 710, val loss: 1.6205973625183105
Epoch 720, training loss: 0.015587681904435158 = 0.00848885253071785 + 0.001 * 7.0988287925720215
Epoch 720, val loss: 1.6316970586776733
Epoch 730, training loss: 0.015190385282039642 = 0.008102938532829285 + 0.001 * 7.087446212768555
Epoch 730, val loss: 1.6424516439437866
Epoch 740, training loss: 0.014824749901890755 = 0.007744452450424433 + 0.001 * 7.080296516418457
Epoch 740, val loss: 1.6528913974761963
Epoch 750, training loss: 0.01450415886938572 = 0.007411012891680002 + 0.001 * 7.093146324157715
Epoch 750, val loss: 1.663002848625183
Epoch 760, training loss: 0.014185493811964989 = 0.007100109476596117 + 0.001 * 7.085383415222168
Epoch 760, val loss: 1.6728700399398804
Epoch 770, training loss: 0.013891739770770073 = 0.006809852551668882 + 0.001 * 7.0818867683410645
Epoch 770, val loss: 1.6824008226394653
Epoch 780, training loss: 0.013617876917123795 = 0.0065383934415876865 + 0.001 * 7.079483509063721
Epoch 780, val loss: 1.6916264295578003
Epoch 790, training loss: 0.013358278200030327 = 0.0062841265462338924 + 0.001 * 7.074151515960693
Epoch 790, val loss: 1.7005890607833862
Epoch 800, training loss: 0.013118792325258255 = 0.006045672111213207 + 0.001 * 7.073119640350342
Epoch 800, val loss: 1.709301471710205
Epoch 810, training loss: 0.012899312190711498 = 0.005821714177727699 + 0.001 * 7.077597618103027
Epoch 810, val loss: 1.7177460193634033
Epoch 820, training loss: 0.01268252544105053 = 0.005610975902527571 + 0.001 * 7.071549415588379
Epoch 820, val loss: 1.7260419130325317
Epoch 830, training loss: 0.012524345889687538 = 0.00541218277066946 + 0.001 * 7.112163066864014
Epoch 830, val loss: 1.7340850830078125
Epoch 840, training loss: 0.012297785840928555 = 0.005224534310400486 + 0.001 * 7.073251247406006
Epoch 840, val loss: 1.7418594360351562
Epoch 850, training loss: 0.012113334611058235 = 0.005046545993536711 + 0.001 * 7.066788196563721
Epoch 850, val loss: 1.74944007396698
Epoch 860, training loss: 0.011939002200961113 = 0.004875746089965105 + 0.001 * 7.063255786895752
Epoch 860, val loss: 1.7568340301513672
Epoch 870, training loss: 0.011788266710937023 = 0.004709792323410511 + 0.001 * 7.078474044799805
Epoch 870, val loss: 1.7641074657440186
Epoch 880, training loss: 0.011608654633164406 = 0.0045478204265236855 + 0.001 * 7.060833930969238
Epoch 880, val loss: 1.7715210914611816
Epoch 890, training loss: 0.01146622747182846 = 0.004389934241771698 + 0.001 * 7.076292991638184
Epoch 890, val loss: 1.7791600227355957
Epoch 900, training loss: 0.011312548071146011 = 0.004236927255988121 + 0.001 * 7.075620174407959
Epoch 900, val loss: 1.7870497703552246
Epoch 910, training loss: 0.011146614328026772 = 0.004089248366653919 + 0.001 * 7.057365894317627
Epoch 910, val loss: 1.7949827909469604
Epoch 920, training loss: 0.011005457490682602 = 0.0039471895433962345 + 0.001 * 7.058267116546631
Epoch 920, val loss: 1.802956461906433
Epoch 930, training loss: 0.010887266136705875 = 0.0038109689485281706 + 0.001 * 7.076297283172607
Epoch 930, val loss: 1.810991644859314
Epoch 940, training loss: 0.010751800611615181 = 0.003680695779621601 + 0.001 * 7.071105003356934
Epoch 940, val loss: 1.818934440612793
Epoch 950, training loss: 0.01063183881342411 = 0.003556252224370837 + 0.001 * 7.075586318969727
Epoch 950, val loss: 1.8268746137619019
Epoch 960, training loss: 0.01049466710537672 = 0.0034377446863800287 + 0.001 * 7.05692195892334
Epoch 960, val loss: 1.8346067667007446
Epoch 970, training loss: 0.010374564677476883 = 0.003324802964925766 + 0.001 * 7.049761772155762
Epoch 970, val loss: 1.8422949314117432
Epoch 980, training loss: 0.010266158729791641 = 0.0032171509228646755 + 0.001 * 7.049007415771484
Epoch 980, val loss: 1.8498653173446655
Epoch 990, training loss: 0.010166455991566181 = 0.0031146069522947073 + 0.001 * 7.051848411560059
Epoch 990, val loss: 1.857290267944336
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8007380073800738
=== training gcn model ===
Epoch 0, training loss: 1.9586368799209595 = 1.9500401020050049 + 0.001 * 8.596786499023438
Epoch 0, val loss: 1.9471487998962402
Epoch 10, training loss: 1.9478380680084229 = 1.9392412900924683 + 0.001 * 8.596725463867188
Epoch 10, val loss: 1.9366732835769653
Epoch 20, training loss: 1.9343222379684448 = 1.9257256984710693 + 0.001 * 8.596527099609375
Epoch 20, val loss: 1.9231072664260864
Epoch 30, training loss: 1.9148743152618408 = 1.9062782526016235 + 0.001 * 8.596085548400879
Epoch 30, val loss: 1.903161644935608
Epoch 40, training loss: 1.8861751556396484 = 1.8775802850723267 + 0.001 * 8.594910621643066
Epoch 40, val loss: 1.8739571571350098
Epoch 50, training loss: 1.8486473560333252 = 1.8400565385818481 + 0.001 * 8.590851783752441
Epoch 50, val loss: 1.8377948999404907
Epoch 60, training loss: 1.8139193058013916 = 1.8053452968597412 + 0.001 * 8.573983192443848
Epoch 60, val loss: 1.8083890676498413
Epoch 70, training loss: 1.7865161895751953 = 1.778033971786499 + 0.001 * 8.482242584228516
Epoch 70, val loss: 1.7857253551483154
Epoch 80, training loss: 1.7476589679718018 = 1.739505410194397 + 0.001 * 8.153498649597168
Epoch 80, val loss: 1.7520874738693237
Epoch 90, training loss: 1.6921403408050537 = 1.6840758323669434 + 0.001 * 8.064529418945312
Epoch 90, val loss: 1.7043261528015137
Epoch 100, training loss: 1.6173824071884155 = 1.6093571186065674 + 0.001 * 8.025238990783691
Epoch 100, val loss: 1.6415040493011475
Epoch 110, training loss: 1.5340691804885864 = 1.5260599851608276 + 0.001 * 8.009175300598145
Epoch 110, val loss: 1.5737526416778564
Epoch 120, training loss: 1.453689694404602 = 1.4456892013549805 + 0.001 * 8.00044059753418
Epoch 120, val loss: 1.5111291408538818
Epoch 130, training loss: 1.3788988590240479 = 1.3709149360656738 + 0.001 * 7.983877182006836
Epoch 130, val loss: 1.4568731784820557
Epoch 140, training loss: 1.3072320222854614 = 1.2993141412734985 + 0.001 * 7.917877674102783
Epoch 140, val loss: 1.4082798957824707
Epoch 150, training loss: 1.2369884252548218 = 1.2293117046356201 + 0.001 * 7.676743030548096
Epoch 150, val loss: 1.362899661064148
Epoch 160, training loss: 1.1685268878936768 = 1.1609351634979248 + 0.001 * 7.591720104217529
Epoch 160, val loss: 1.3211334943771362
Epoch 170, training loss: 1.1011165380477905 = 1.0935893058776855 + 0.001 * 7.5272135734558105
Epoch 170, val loss: 1.2819536924362183
Epoch 180, training loss: 1.0340577363967896 = 1.0265984535217285 + 0.001 * 7.459303855895996
Epoch 180, val loss: 1.242788553237915
Epoch 190, training loss: 0.9676553606987 = 0.9602367877960205 + 0.001 * 7.4185991287231445
Epoch 190, val loss: 1.2034289836883545
Epoch 200, training loss: 0.9022485017776489 = 0.8948391079902649 + 0.001 * 7.409376621246338
Epoch 200, val loss: 1.1639189720153809
Epoch 210, training loss: 0.8375434875488281 = 0.8301386833190918 + 0.001 * 7.404830455780029
Epoch 210, val loss: 1.12489652633667
Epoch 220, training loss: 0.7728385329246521 = 0.7654390931129456 + 0.001 * 7.399415493011475
Epoch 220, val loss: 1.0853822231292725
Epoch 230, training loss: 0.7081193327903748 = 0.7007253766059875 + 0.001 * 7.393930912017822
Epoch 230, val loss: 1.0458004474639893
Epoch 240, training loss: 0.6446955800056458 = 0.6373076438903809 + 0.001 * 7.387959003448486
Epoch 240, val loss: 1.0074580907821655
Epoch 250, training loss: 0.5845184326171875 = 0.5771375894546509 + 0.001 * 7.380821228027344
Epoch 250, val loss: 0.9729036688804626
Epoch 260, training loss: 0.5288448333740234 = 0.5214740037918091 + 0.001 * 7.3708086013793945
Epoch 260, val loss: 0.9436456561088562
Epoch 270, training loss: 0.47741568088531494 = 0.47006040811538696 + 0.001 * 7.355269432067871
Epoch 270, val loss: 0.9203323721885681
Epoch 280, training loss: 0.42875733971595764 = 0.4214117228984833 + 0.001 * 7.345608711242676
Epoch 280, val loss: 0.9017695784568787
Epoch 290, training loss: 0.3815153241157532 = 0.3741953670978546 + 0.001 * 7.319958209991455
Epoch 290, val loss: 0.8867520093917847
Epoch 300, training loss: 0.33559927344322205 = 0.3283020853996277 + 0.001 * 7.2971906661987305
Epoch 300, val loss: 0.8748031258583069
Epoch 310, training loss: 0.2923514246940613 = 0.2850742042064667 + 0.001 * 7.277219772338867
Epoch 310, val loss: 0.8664217591285706
Epoch 320, training loss: 0.2534157633781433 = 0.24614781141281128 + 0.001 * 7.267951488494873
Epoch 320, val loss: 0.862357497215271
Epoch 330, training loss: 0.21946042776107788 = 0.21219561994075775 + 0.001 * 7.2648024559021
Epoch 330, val loss: 0.8630313873291016
Epoch 340, training loss: 0.19028787314891815 = 0.1830352544784546 + 0.001 * 7.252617835998535
Epoch 340, val loss: 0.8680692911148071
Epoch 350, training loss: 0.1654469221830368 = 0.15819621086120605 + 0.001 * 7.250709533691406
Epoch 350, val loss: 0.876563549041748
Epoch 360, training loss: 0.14437787234783173 = 0.13712911307811737 + 0.001 * 7.248762130737305
Epoch 360, val loss: 0.8877413272857666
Epoch 370, training loss: 0.12653310596942902 = 0.11928690969944 + 0.001 * 7.246192932128906
Epoch 370, val loss: 0.9008376002311707
Epoch 380, training loss: 0.11137326806783676 = 0.10413070023059845 + 0.001 * 7.2425689697265625
Epoch 380, val loss: 0.9151597619056702
Epoch 390, training loss: 0.09844178706407547 = 0.09120289981365204 + 0.001 * 7.2388834953308105
Epoch 390, val loss: 0.930182158946991
Epoch 400, training loss: 0.0873577669262886 = 0.08012151718139648 + 0.001 * 7.236251354217529
Epoch 400, val loss: 0.9455679059028625
Epoch 410, training loss: 0.07783018797636032 = 0.07059429585933685 + 0.001 * 7.2358903884887695
Epoch 410, val loss: 0.9610909819602966
Epoch 420, training loss: 0.06962847709655762 = 0.06239256635308266 + 0.001 * 7.235912322998047
Epoch 420, val loss: 0.976679801940918
Epoch 430, training loss: 0.06255456060171127 = 0.055323198437690735 + 0.001 * 7.231358528137207
Epoch 430, val loss: 0.9921849966049194
Epoch 440, training loss: 0.05645330250263214 = 0.0492243692278862 + 0.001 * 7.228930950164795
Epoch 440, val loss: 1.0074735879898071
Epoch 450, training loss: 0.05118437856435776 = 0.04395847022533417 + 0.001 * 7.225908279418945
Epoch 450, val loss: 1.0225458145141602
Epoch 460, training loss: 0.046623412519693375 = 0.03940539062023163 + 0.001 * 7.21802282333374
Epoch 460, val loss: 1.0372629165649414
Epoch 470, training loss: 0.04268273338675499 = 0.03546251356601715 + 0.001 * 7.220220565795898
Epoch 470, val loss: 1.0516306161880493
Epoch 480, training loss: 0.03925158828496933 = 0.03204185143113136 + 0.001 * 7.209736347198486
Epoch 480, val loss: 1.0655865669250488
Epoch 490, training loss: 0.036283403635025024 = 0.02906648814678192 + 0.001 * 7.216917037963867
Epoch 490, val loss: 1.0791469812393188
Epoch 500, training loss: 0.03367970883846283 = 0.026469580829143524 + 0.001 * 7.210129737854004
Epoch 500, val loss: 1.0923311710357666
Epoch 510, training loss: 0.031402260065078735 = 0.024195093661546707 + 0.001 * 7.20716667175293
Epoch 510, val loss: 1.1051124334335327
Epoch 520, training loss: 0.029390450567007065 = 0.022195568308234215 + 0.001 * 7.194881916046143
Epoch 520, val loss: 1.117492437362671
Epoch 530, training loss: 0.027630973607301712 = 0.020431535318493843 + 0.001 * 7.199437141418457
Epoch 530, val loss: 1.1294925212860107
Epoch 540, training loss: 0.026057133451104164 = 0.01886913925409317 + 0.001 * 7.187994480133057
Epoch 540, val loss: 1.141087532043457
Epoch 550, training loss: 0.024659685790538788 = 0.017480093985795975 + 0.001 * 7.179590702056885
Epoch 550, val loss: 1.1522679328918457
Epoch 560, training loss: 0.023426122963428497 = 0.016240617260336876 + 0.001 * 7.185505390167236
Epoch 560, val loss: 1.1630947589874268
Epoch 570, training loss: 0.022305268794298172 = 0.015130659565329552 + 0.001 * 7.174609661102295
Epoch 570, val loss: 1.1735968589782715
Epoch 580, training loss: 0.021309789270162582 = 0.01413320004940033 + 0.001 * 7.176589012145996
Epoch 580, val loss: 1.183712363243103
Epoch 590, training loss: 0.020397908985614777 = 0.013233962468802929 + 0.001 * 7.16394567489624
Epoch 590, val loss: 1.1935068368911743
Epoch 600, training loss: 0.019590364769101143 = 0.012420651502907276 + 0.001 * 7.169712543487549
Epoch 600, val loss: 1.20298433303833
Epoch 610, training loss: 0.018846848979592323 = 0.011682761833071709 + 0.001 * 7.164086818695068
Epoch 610, val loss: 1.2121546268463135
Epoch 620, training loss: 0.01820049248635769 = 0.011011330410838127 + 0.001 * 7.189161777496338
Epoch 620, val loss: 1.2210277318954468
Epoch 630, training loss: 0.01758032850921154 = 0.01039874367415905 + 0.001 * 7.181584358215332
Epoch 630, val loss: 1.2296453714370728
Epoch 640, training loss: 0.017009716480970383 = 0.009838382713496685 + 0.001 * 7.171333312988281
Epoch 640, val loss: 1.238000750541687
Epoch 650, training loss: 0.016484040766954422 = 0.009324440732598305 + 0.001 * 7.159599304199219
Epoch 650, val loss: 1.2461073398590088
Epoch 660, training loss: 0.016007952392101288 = 0.008851966820657253 + 0.001 * 7.155984401702881
Epoch 660, val loss: 1.2539787292480469
Epoch 670, training loss: 0.015566172078251839 = 0.008416629396378994 + 0.001 * 7.149542331695557
Epoch 670, val loss: 1.2616297006607056
Epoch 680, training loss: 0.015160257928073406 = 0.008014654740691185 + 0.001 * 7.145602703094482
Epoch 680, val loss: 1.269058346748352
Epoch 690, training loss: 0.014789930544793606 = 0.007642699405550957 + 0.001 * 7.147230625152588
Epoch 690, val loss: 1.2763112783432007
Epoch 700, training loss: 0.014445649459958076 = 0.007297893054783344 + 0.001 * 7.1477556228637695
Epoch 700, val loss: 1.283366084098816
Epoch 710, training loss: 0.014124805107712746 = 0.006977621000260115 + 0.001 * 7.147183895111084
Epoch 710, val loss: 1.290215015411377
Epoch 720, training loss: 0.013824006542563438 = 0.006679601036012173 + 0.001 * 7.144405841827393
Epoch 720, val loss: 1.2969192266464233
Epoch 730, training loss: 0.013541977852582932 = 0.006401849910616875 + 0.001 * 7.140127182006836
Epoch 730, val loss: 1.303444266319275
Epoch 740, training loss: 0.013284394517540932 = 0.006142545025795698 + 0.001 * 7.141849517822266
Epoch 740, val loss: 1.3097763061523438
Epoch 750, training loss: 0.013032142072916031 = 0.005900081712752581 + 0.001 * 7.1320600509643555
Epoch 750, val loss: 1.3159700632095337
Epoch 760, training loss: 0.012808280065655708 = 0.005673018284142017 + 0.001 * 7.135261535644531
Epoch 760, val loss: 1.3220022916793823
Epoch 770, training loss: 0.012604696676135063 = 0.005460054613649845 + 0.001 * 7.144642353057861
Epoch 770, val loss: 1.3278944492340088
Epoch 780, training loss: 0.012386295944452286 = 0.005260045174509287 + 0.001 * 7.12624979019165
Epoch 780, val loss: 1.333634614944458
Epoch 790, training loss: 0.012187650427222252 = 0.0050719790160655975 + 0.001 * 7.115671157836914
Epoch 790, val loss: 1.3392291069030762
Epoch 800, training loss: 0.012013208121061325 = 0.0048949019983410835 + 0.001 * 7.1183061599731445
Epoch 800, val loss: 1.3446894884109497
Epoch 810, training loss: 0.011840565130114555 = 0.004727981053292751 + 0.001 * 7.112583160400391
Epoch 810, val loss: 1.3500415086746216
Epoch 820, training loss: 0.011692838743329048 = 0.004570430144667625 + 0.001 * 7.122407913208008
Epoch 820, val loss: 1.3552402257919312
Epoch 830, training loss: 0.011537676677107811 = 0.004421550780534744 + 0.001 * 7.116125583648682
Epoch 830, val loss: 1.3603280782699585
Epoch 840, training loss: 0.011391345411539078 = 0.004280710127204657 + 0.001 * 7.110634803771973
Epoch 840, val loss: 1.36529541015625
Epoch 850, training loss: 0.011263243854045868 = 0.004147357773035765 + 0.001 * 7.115886211395264
Epoch 850, val loss: 1.3701664209365845
Epoch 860, training loss: 0.011122703552246094 = 0.0040209610015153885 + 0.001 * 7.101742744445801
Epoch 860, val loss: 1.3749363422393799
Epoch 870, training loss: 0.011011181399226189 = 0.003901025978848338 + 0.001 * 7.11015510559082
Epoch 870, val loss: 1.3795812129974365
Epoch 880, training loss: 0.01090999972075224 = 0.003787141526117921 + 0.001 * 7.122857570648193
Epoch 880, val loss: 1.3841347694396973
Epoch 890, training loss: 0.010799283161759377 = 0.0036789034493267536 + 0.001 * 7.1203789710998535
Epoch 890, val loss: 1.3885974884033203
Epoch 900, training loss: 0.01070871390402317 = 0.0035759389866143465 + 0.001 * 7.132774829864502
Epoch 900, val loss: 1.3929520845413208
Epoch 910, training loss: 0.010582325980067253 = 0.0034778958652168512 + 0.001 * 7.104430198669434
Epoch 910, val loss: 1.3972265720367432
Epoch 920, training loss: 0.010484246537089348 = 0.0033844755962491035 + 0.001 * 7.099771022796631
Epoch 920, val loss: 1.4014068841934204
Epoch 930, training loss: 0.010384878143668175 = 0.003295375034213066 + 0.001 * 7.089503288269043
Epoch 930, val loss: 1.405490756034851
Epoch 940, training loss: 0.01032106764614582 = 0.0032103476114571095 + 0.001 * 7.110720157623291
Epoch 940, val loss: 1.4094953536987305
Epoch 950, training loss: 0.010220716707408428 = 0.003129148855805397 + 0.001 * 7.091567516326904
Epoch 950, val loss: 1.4134407043457031
Epoch 960, training loss: 0.010178507305681705 = 0.003051543841138482 + 0.001 * 7.126963138580322
Epoch 960, val loss: 1.4172909259796143
Epoch 970, training loss: 0.0100764911621809 = 0.0029773125424981117 + 0.001 * 7.099177837371826
Epoch 970, val loss: 1.421059489250183
Epoch 980, training loss: 0.010007350705564022 = 0.002906281966716051 + 0.001 * 7.101068496704102
Epoch 980, val loss: 1.4247522354125977
Epoch 990, training loss: 0.00991785153746605 = 0.0028382318560034037 + 0.001 * 7.079619407653809
Epoch 990, val loss: 1.4283974170684814
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.801265155508698
=== training gcn model ===
Epoch 0, training loss: 1.9542748928070068 = 1.9456779956817627 + 0.001 * 8.596837997436523
Epoch 0, val loss: 1.955464482307434
Epoch 10, training loss: 1.9439793825149536 = 1.935382604598999 + 0.001 * 8.596773147583008
Epoch 10, val loss: 1.9446101188659668
Epoch 20, training loss: 1.9311721324920654 = 1.92257559299469 + 0.001 * 8.596592903137207
Epoch 20, val loss: 1.9308921098709106
Epoch 30, training loss: 1.9131782054901123 = 1.9045820236206055 + 0.001 * 8.596184730529785
Epoch 30, val loss: 1.9116631746292114
Epoch 40, training loss: 1.8870203495025635 = 1.878425121307373 + 0.001 * 8.595198631286621
Epoch 40, val loss: 1.8841198682785034
Epoch 50, training loss: 1.8522682189941406 = 1.8436757326126099 + 0.001 * 8.592522621154785
Epoch 50, val loss: 1.8490175008773804
Epoch 60, training loss: 1.817944049835205 = 1.8093600273132324 + 0.001 * 8.58397388458252
Epoch 60, val loss: 1.8170955181121826
Epoch 70, training loss: 1.7901291847229004 = 1.7815779447555542 + 0.001 * 8.551299095153809
Epoch 70, val loss: 1.7914820909500122
Epoch 80, training loss: 1.7530595064163208 = 1.7447103261947632 + 0.001 * 8.349157333374023
Epoch 80, val loss: 1.7578513622283936
Epoch 90, training loss: 1.700777292251587 = 1.6926789283752441 + 0.001 * 8.098329544067383
Epoch 90, val loss: 1.7124494314193726
Epoch 100, training loss: 1.6284518241882324 = 1.620438575744629 + 0.001 * 8.01321029663086
Epoch 100, val loss: 1.6509015560150146
Epoch 110, training loss: 1.5422612428665161 = 1.5343018770217896 + 0.001 * 7.959400177001953
Epoch 110, val loss: 1.5790326595306396
Epoch 120, training loss: 1.454681396484375 = 1.446825623512268 + 0.001 * 7.855757713317871
Epoch 120, val loss: 1.508316993713379
Epoch 130, training loss: 1.370096206665039 = 1.3623762130737305 + 0.001 * 7.719940662384033
Epoch 130, val loss: 1.444003939628601
Epoch 140, training loss: 1.2849162817001343 = 1.2772057056427002 + 0.001 * 7.7106032371521
Epoch 140, val loss: 1.380919337272644
Epoch 150, training loss: 1.1970916986465454 = 1.189414143562317 + 0.001 * 7.677587985992432
Epoch 150, val loss: 1.316379427909851
Epoch 160, training loss: 1.1085723638534546 = 1.100921869277954 + 0.001 * 7.65050745010376
Epoch 160, val loss: 1.2512915134429932
Epoch 170, training loss: 1.0227044820785522 = 1.0150853395462036 + 0.001 * 7.619084358215332
Epoch 170, val loss: 1.1893644332885742
Epoch 180, training loss: 0.9416548013687134 = 0.9340783357620239 + 0.001 * 7.576473236083984
Epoch 180, val loss: 1.1323198080062866
Epoch 190, training loss: 0.8660433888435364 = 0.8585329055786133 + 0.001 * 7.510469913482666
Epoch 190, val loss: 1.0806337594985962
Epoch 200, training loss: 0.7964083552360535 = 0.7889800667762756 + 0.001 * 7.428300380706787
Epoch 200, val loss: 1.0353761911392212
Epoch 210, training loss: 0.7332266569137573 = 0.7258179783821106 + 0.001 * 7.408653736114502
Epoch 210, val loss: 0.9968821406364441
Epoch 220, training loss: 0.6758450269699097 = 0.6684482097625732 + 0.001 * 7.396805763244629
Epoch 220, val loss: 0.9651659727096558
Epoch 230, training loss: 0.6226910352706909 = 0.6152974963188171 + 0.001 * 7.393566131591797
Epoch 230, val loss: 0.939383327960968
Epoch 240, training loss: 0.5721661448478699 = 0.5647774934768677 + 0.001 * 7.388679027557373
Epoch 240, val loss: 0.9182847738265991
Epoch 250, training loss: 0.5231627225875854 = 0.5157791972160339 + 0.001 * 7.383512020111084
Epoch 250, val loss: 0.9008406400680542
Epoch 260, training loss: 0.47534483671188354 = 0.46796703338623047 + 0.001 * 7.377796649932861
Epoch 260, val loss: 0.8871427774429321
Epoch 270, training loss: 0.4293016493320465 = 0.421933114528656 + 0.001 * 7.368526935577393
Epoch 270, val loss: 0.8780723810195923
Epoch 280, training loss: 0.3863356411457062 = 0.3789796531200409 + 0.001 * 7.355990886688232
Epoch 280, val loss: 0.8745039105415344
Epoch 290, training loss: 0.34773534536361694 = 0.3403990566730499 + 0.001 * 7.336293697357178
Epoch 290, val loss: 0.876861572265625
Epoch 300, training loss: 0.31398364901542664 = 0.30663833022117615 + 0.001 * 7.345309257507324
Epoch 300, val loss: 0.8847560882568359
Epoch 310, training loss: 0.2844926118850708 = 0.2771915793418884 + 0.001 * 7.301034927368164
Epoch 310, val loss: 0.8971198797225952
Epoch 320, training loss: 0.25828060507774353 = 0.25101613998413086 + 0.001 * 7.264456272125244
Epoch 320, val loss: 0.9126097559928894
Epoch 330, training loss: 0.23417577147483826 = 0.2269417941570282 + 0.001 * 7.233978748321533
Epoch 330, val loss: 0.9299927353858948
Epoch 340, training loss: 0.21117909252643585 = 0.2039504200220108 + 0.001 * 7.228671073913574
Epoch 340, val loss: 0.9484971761703491
Epoch 350, training loss: 0.18874305486679077 = 0.18152758479118347 + 0.001 * 7.215465545654297
Epoch 350, val loss: 0.9677385091781616
Epoch 360, training loss: 0.1670626997947693 = 0.1598701775074005 + 0.001 * 7.192514419555664
Epoch 360, val loss: 0.987531304359436
Epoch 370, training loss: 0.14683596789836884 = 0.1396562159061432 + 0.001 * 7.179754257202148
Epoch 370, val loss: 1.0081524848937988
Epoch 380, training loss: 0.1287049502134323 = 0.12153573334217072 + 0.001 * 7.169222831726074
Epoch 380, val loss: 1.0297266244888306
Epoch 390, training loss: 0.11291242390871048 = 0.10574866831302643 + 0.001 * 7.163757801055908
Epoch 390, val loss: 1.0521652698516846
Epoch 400, training loss: 0.09931856393814087 = 0.09216170758008957 + 0.001 * 7.156853675842285
Epoch 400, val loss: 1.0753763914108276
Epoch 410, training loss: 0.08768349140882492 = 0.08052884787321091 + 0.001 * 7.154641628265381
Epoch 410, val loss: 1.0991294384002686
Epoch 420, training loss: 0.07775983959436417 = 0.07061204314231873 + 0.001 * 7.147794723510742
Epoch 420, val loss: 1.123036503791809
Epoch 430, training loss: 0.06932243704795837 = 0.06217709928750992 + 0.001 * 7.145339012145996
Epoch 430, val loss: 1.1469764709472656
Epoch 440, training loss: 0.06214192137122154 = 0.054996851831674576 + 0.001 * 7.1450676918029785
Epoch 440, val loss: 1.170605182647705
Epoch 450, training loss: 0.056012142449617386 = 0.04886249452829361 + 0.001 * 7.149646282196045
Epoch 450, val loss: 1.1937963962554932
Epoch 460, training loss: 0.05074245110154152 = 0.043600115925073624 + 0.001 * 7.142335414886475
Epoch 460, val loss: 1.216591477394104
Epoch 470, training loss: 0.04620534926652908 = 0.03907095268368721 + 0.001 * 7.134397506713867
Epoch 470, val loss: 1.2387809753417969
Epoch 480, training loss: 0.042292024940252304 = 0.03515733778476715 + 0.001 * 7.134685516357422
Epoch 480, val loss: 1.2603565454483032
Epoch 490, training loss: 0.03890231251716614 = 0.03176553174853325 + 0.001 * 7.136780738830566
Epoch 490, val loss: 1.281274437904358
Epoch 500, training loss: 0.0359615683555603 = 0.028814973309636116 + 0.001 * 7.146595478057861
Epoch 500, val loss: 1.3014748096466064
Epoch 510, training loss: 0.03337498754262924 = 0.026238862425088882 + 0.001 * 7.136125564575195
Epoch 510, val loss: 1.3209494352340698
Epoch 520, training loss: 0.03111591562628746 = 0.023981627076864243 + 0.001 * 7.1342878341674805
Epoch 520, val loss: 1.3397104740142822
Epoch 530, training loss: 0.02912304550409317 = 0.021996159106492996 + 0.001 * 7.126885414123535
Epoch 530, val loss: 1.3577888011932373
Epoch 540, training loss: 0.027368128299713135 = 0.020242545753717422 + 0.001 * 7.125583171844482
Epoch 540, val loss: 1.375212550163269
Epoch 550, training loss: 0.02581784501671791 = 0.018688300624489784 + 0.001 * 7.129545211791992
Epoch 550, val loss: 1.3919979333877563
Epoch 560, training loss: 0.024425240233540535 = 0.01730567403137684 + 0.001 * 7.119565486907959
Epoch 560, val loss: 1.4081562757492065
Epoch 570, training loss: 0.023190192878246307 = 0.016071338206529617 + 0.001 * 7.118853569030762
Epoch 570, val loss: 1.423741340637207
Epoch 580, training loss: 0.02208852395415306 = 0.014965422451496124 + 0.001 * 7.123101711273193
Epoch 580, val loss: 1.4387829303741455
Epoch 590, training loss: 0.02108803763985634 = 0.013971260748803616 + 0.001 * 7.116776466369629
Epoch 590, val loss: 1.4532721042633057
Epoch 600, training loss: 0.020199429243803024 = 0.013074666261672974 + 0.001 * 7.124762535095215
Epoch 600, val loss: 1.4672452211380005
Epoch 610, training loss: 0.019377034157514572 = 0.012263660319149494 + 0.001 * 7.113372802734375
Epoch 610, val loss: 1.480757474899292
Epoch 620, training loss: 0.01864590123295784 = 0.011527864262461662 + 0.001 * 7.118037700653076
Epoch 620, val loss: 1.4938076734542847
Epoch 630, training loss: 0.017970969900488853 = 0.010858473367989063 + 0.001 * 7.112496376037598
Epoch 630, val loss: 1.5064178705215454
Epoch 640, training loss: 0.017365949228405952 = 0.010247903876006603 + 0.001 * 7.118044376373291
Epoch 640, val loss: 1.5186160802841187
Epoch 650, training loss: 0.016808826476335526 = 0.00968953873962164 + 0.001 * 7.119287490844727
Epoch 650, val loss: 1.5304063558578491
Epoch 660, training loss: 0.016284147277474403 = 0.009177659638226032 + 0.001 * 7.106486797332764
Epoch 660, val loss: 1.5418195724487305
Epoch 670, training loss: 0.015811393037438393 = 0.008707307279109955 + 0.001 * 7.104085445404053
Epoch 670, val loss: 1.5528883934020996
Epoch 680, training loss: 0.01538174320012331 = 0.00827412772923708 + 0.001 * 7.107614994049072
Epoch 680, val loss: 1.5636022090911865
Epoch 690, training loss: 0.014981074258685112 = 0.007874397560954094 + 0.001 * 7.1066765785217285
Epoch 690, val loss: 1.5739827156066895
Epoch 700, training loss: 0.014601651579141617 = 0.007504770997911692 + 0.001 * 7.096879959106445
Epoch 700, val loss: 1.5840561389923096
Epoch 710, training loss: 0.01425943709909916 = 0.0071623180992901325 + 0.001 * 7.097118854522705
Epoch 710, val loss: 1.5938411951065063
Epoch 720, training loss: 0.013971718028187752 = 0.006844416726380587 + 0.001 * 7.127301216125488
Epoch 720, val loss: 1.6033267974853516
Epoch 730, training loss: 0.013642131350934505 = 0.006548811215907335 + 0.001 * 7.093319892883301
Epoch 730, val loss: 1.6125454902648926
Epoch 740, training loss: 0.013370508328080177 = 0.006273475009948015 + 0.001 * 7.0970330238342285
Epoch 740, val loss: 1.6215088367462158
Epoch 750, training loss: 0.01311663631349802 = 0.006016572006046772 + 0.001 * 7.100063800811768
Epoch 750, val loss: 1.6302090883255005
Epoch 760, training loss: 0.01286962628364563 = 0.005776513833552599 + 0.001 * 7.093112468719482
Epoch 760, val loss: 1.6386668682098389
Epoch 770, training loss: 0.012646599672734737 = 0.005551868584007025 + 0.001 * 7.094730854034424
Epoch 770, val loss: 1.6468966007232666
Epoch 780, training loss: 0.0124331871047616 = 0.005341329146176577 + 0.001 * 7.091857433319092
Epoch 780, val loss: 1.6549099683761597
Epoch 790, training loss: 0.012228726409375668 = 0.005143748130649328 + 0.001 * 7.084978103637695
Epoch 790, val loss: 1.6626989841461182
Epoch 800, training loss: 0.012044371105730534 = 0.004958095494657755 + 0.001 * 7.086275100708008
Epoch 800, val loss: 1.670277714729309
Epoch 810, training loss: 0.011879784986376762 = 0.004783399868756533 + 0.001 * 7.096385478973389
Epoch 810, val loss: 1.67766273021698
Epoch 820, training loss: 0.011717410758137703 = 0.0046188365668058395 + 0.001 * 7.098574161529541
Epoch 820, val loss: 1.6848570108413696
Epoch 830, training loss: 0.011549495160579681 = 0.004463737830519676 + 0.001 * 7.085757255554199
Epoch 830, val loss: 1.691908359527588
Epoch 840, training loss: 0.011394803412258625 = 0.004317400977015495 + 0.001 * 7.077402114868164
Epoch 840, val loss: 1.6987671852111816
Epoch 850, training loss: 0.011254885233938694 = 0.0041790856048464775 + 0.001 * 7.075799465179443
Epoch 850, val loss: 1.7054420709609985
Epoch 860, training loss: 0.011133205145597458 = 0.00404821103438735 + 0.001 * 7.084993362426758
Epoch 860, val loss: 1.7119524478912354
Epoch 870, training loss: 0.011016380041837692 = 0.00392422778531909 + 0.001 * 7.092151641845703
Epoch 870, val loss: 1.718306303024292
Epoch 880, training loss: 0.01089559867978096 = 0.0038067016284912825 + 0.001 * 7.088896751403809
Epoch 880, val loss: 1.7244964838027954
Epoch 890, training loss: 0.010773792862892151 = 0.003695156890898943 + 0.001 * 7.0786356925964355
Epoch 890, val loss: 1.7305512428283691
Epoch 900, training loss: 0.010664472356438637 = 0.0035892094019800425 + 0.001 * 7.075263023376465
Epoch 900, val loss: 1.7364554405212402
Epoch 910, training loss: 0.010566383600234985 = 0.003488471731543541 + 0.001 * 7.077911376953125
Epoch 910, val loss: 1.7422125339508057
Epoch 920, training loss: 0.010475060902535915 = 0.0033926202449947596 + 0.001 * 7.08243989944458
Epoch 920, val loss: 1.7478312253952026
Epoch 930, training loss: 0.010364849120378494 = 0.0033013157080858946 + 0.001 * 7.063533306121826
Epoch 930, val loss: 1.753326416015625
Epoch 940, training loss: 0.010290434584021568 = 0.00321429455652833 + 0.001 * 7.076139450073242
Epoch 940, val loss: 1.7586852312088013
Epoch 950, training loss: 0.010194497182965279 = 0.0031312797218561172 + 0.001 * 7.063217639923096
Epoch 950, val loss: 1.7639110088348389
Epoch 960, training loss: 0.01010788045823574 = 0.003052063286304474 + 0.001 * 7.055816650390625
Epoch 960, val loss: 1.7690197229385376
Epoch 970, training loss: 0.010034448467195034 = 0.002976398216560483 + 0.001 * 7.05804967880249
Epoch 970, val loss: 1.7740223407745361
Epoch 980, training loss: 0.009959348477423191 = 0.002904059598222375 + 0.001 * 7.055288791656494
Epoch 980, val loss: 1.7789057493209839
Epoch 990, training loss: 0.009896804578602314 = 0.0028348590712994337 + 0.001 * 7.06194543838501
Epoch 990, val loss: 1.7836859226226807
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.801265155508698
The final CL Acc:0.75185, 0.01386, The final GNN Acc:0.80109, 0.00025
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13126])
remove edge: torch.Size([2, 8048])
updated graph: torch.Size([2, 10618])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9538463354110718 = 1.9452495574951172 + 0.001 * 8.596821784973145
Epoch 0, val loss: 1.9449162483215332
Epoch 10, training loss: 1.9434468746185303 = 1.9348500967025757 + 0.001 * 8.596765518188477
Epoch 10, val loss: 1.9341950416564941
Epoch 20, training loss: 1.931047797203064 = 1.9224512577056885 + 0.001 * 8.596559524536133
Epoch 20, val loss: 1.9213200807571411
Epoch 30, training loss: 1.9140750169754028 = 1.9054789543151855 + 0.001 * 8.596033096313477
Epoch 30, val loss: 1.9037197828292847
Epoch 40, training loss: 1.8893556594848633 = 1.8807610273361206 + 0.001 * 8.594655990600586
Epoch 40, val loss: 1.8783981800079346
Epoch 50, training loss: 1.8545751571655273 = 1.8459845781326294 + 0.001 * 8.590633392333984
Epoch 50, val loss: 1.8442840576171875
Epoch 60, training loss: 1.8136318922042847 = 1.8050552606582642 + 0.001 * 8.576579093933105
Epoch 60, val loss: 1.807707667350769
Epoch 70, training loss: 1.7758997678756714 = 1.767385482788086 + 0.001 * 8.514302253723145
Epoch 70, val loss: 1.776831865310669
Epoch 80, training loss: 1.7302218675613403 = 1.7220518589019775 + 0.001 * 8.170050621032715
Epoch 80, val loss: 1.7357031106948853
Epoch 90, training loss: 1.665644884109497 = 1.6576552391052246 + 0.001 * 7.989615440368652
Epoch 90, val loss: 1.6772725582122803
Epoch 100, training loss: 1.5804141759872437 = 1.5725260972976685 + 0.001 * 7.888106346130371
Epoch 100, val loss: 1.603692889213562
Epoch 110, training loss: 1.4818304777145386 = 1.4741199016571045 + 0.001 * 7.710554599761963
Epoch 110, val loss: 1.5201573371887207
Epoch 120, training loss: 1.3801912069320679 = 1.3726681470870972 + 0.001 * 7.523099899291992
Epoch 120, val loss: 1.4352881908416748
Epoch 130, training loss: 1.2802555561065674 = 1.2727389335632324 + 0.001 * 7.516648292541504
Epoch 130, val loss: 1.3540152311325073
Epoch 140, training loss: 1.183107852935791 = 1.1756317615509033 + 0.001 * 7.47613525390625
Epoch 140, val loss: 1.2779226303100586
Epoch 150, training loss: 1.0891845226287842 = 1.0817365646362305 + 0.001 * 7.447907447814941
Epoch 150, val loss: 1.2063130140304565
Epoch 160, training loss: 0.998773455619812 = 0.9913603067398071 + 0.001 * 7.4131245613098145
Epoch 160, val loss: 1.1384985446929932
Epoch 170, training loss: 0.9127630591392517 = 0.9053965210914612 + 0.001 * 7.3665571212768555
Epoch 170, val loss: 1.0740718841552734
Epoch 180, training loss: 0.8321489095687866 = 0.8248358964920044 + 0.001 * 7.313004493713379
Epoch 180, val loss: 1.0131947994232178
Epoch 190, training loss: 0.7579778432846069 = 0.7506998181343079 + 0.001 * 7.2780303955078125
Epoch 190, val loss: 0.9569984674453735
Epoch 200, training loss: 0.6906284689903259 = 0.6833749413490295 + 0.001 * 7.253502368927002
Epoch 200, val loss: 0.906613290309906
Epoch 210, training loss: 0.6293174624443054 = 0.622088611125946 + 0.001 * 7.2288312911987305
Epoch 210, val loss: 0.862440824508667
Epoch 220, training loss: 0.5723783373832703 = 0.565166711807251 + 0.001 * 7.211631774902344
Epoch 220, val loss: 0.8240026831626892
Epoch 230, training loss: 0.5183961391448975 = 0.5111978650093079 + 0.001 * 7.198286533355713
Epoch 230, val loss: 0.7911055088043213
Epoch 240, training loss: 0.4668372571468353 = 0.4596465528011322 + 0.001 * 7.1906962394714355
Epoch 240, val loss: 0.7638751268386841
Epoch 250, training loss: 0.41783666610717773 = 0.4106515347957611 + 0.001 * 7.185144424438477
Epoch 250, val loss: 0.7421860694885254
Epoch 260, training loss: 0.3718540370464325 = 0.3646734058856964 + 0.001 * 7.180639266967773
Epoch 260, val loss: 0.7253044247627258
Epoch 270, training loss: 0.32939034700393677 = 0.3222113251686096 + 0.001 * 7.179020881652832
Epoch 270, val loss: 0.7127856016159058
Epoch 280, training loss: 0.2907857894897461 = 0.2836119830608368 + 0.001 * 7.1737961769104
Epoch 280, val loss: 0.7039942145347595
Epoch 290, training loss: 0.25618815422058105 = 0.24901778995990753 + 0.001 * 7.170376777648926
Epoch 290, val loss: 0.6985355615615845
Epoch 300, training loss: 0.22558966279029846 = 0.21842211484909058 + 0.001 * 7.167542457580566
Epoch 300, val loss: 0.6961037516593933
Epoch 310, training loss: 0.1988263875246048 = 0.19166117906570435 + 0.001 * 7.165210723876953
Epoch 310, val loss: 0.6962690353393555
Epoch 320, training loss: 0.1755983829498291 = 0.16843700408935547 + 0.001 * 7.161378383636475
Epoch 320, val loss: 0.6986249685287476
Epoch 330, training loss: 0.15552452206611633 = 0.1483657956123352 + 0.001 * 7.158719539642334
Epoch 330, val loss: 0.7027634978294373
Epoch 340, training loss: 0.13818104565143585 = 0.131026029586792 + 0.001 * 7.155019283294678
Epoch 340, val loss: 0.7082837224006653
Epoch 350, training loss: 0.12316209077835083 = 0.11601215600967407 + 0.001 * 7.149936199188232
Epoch 350, val loss: 0.7148439884185791
Epoch 360, training loss: 0.1101221889257431 = 0.10297784209251404 + 0.001 * 7.144348621368408
Epoch 360, val loss: 0.722202479839325
Epoch 370, training loss: 0.09875896573066711 = 0.09162212908267975 + 0.001 * 7.136837005615234
Epoch 370, val loss: 0.7301753163337708
Epoch 380, training loss: 0.08882977813482285 = 0.08170024305582047 + 0.001 * 7.12953519821167
Epoch 380, val loss: 0.7386255860328674
Epoch 390, training loss: 0.08013810217380524 = 0.07301439344882965 + 0.001 * 7.123711109161377
Epoch 390, val loss: 0.7474585175514221
Epoch 400, training loss: 0.07251384854316711 = 0.06539895385503769 + 0.001 * 7.114892959594727
Epoch 400, val loss: 0.7565572261810303
Epoch 410, training loss: 0.06582312285900116 = 0.058715105056762695 + 0.001 * 7.108020305633545
Epoch 410, val loss: 0.7658243775367737
Epoch 420, training loss: 0.0599413737654686 = 0.052844222635030746 + 0.001 * 7.0971503257751465
Epoch 420, val loss: 0.7752304673194885
Epoch 430, training loss: 0.05478416383266449 = 0.04768465459346771 + 0.001 * 7.099506855010986
Epoch 430, val loss: 0.784745454788208
Epoch 440, training loss: 0.050240930169820786 = 0.043146368116140366 + 0.001 * 7.09456205368042
Epoch 440, val loss: 0.7943028211593628
Epoch 450, training loss: 0.046236682683229446 = 0.039150334894657135 + 0.001 * 7.086349010467529
Epoch 450, val loss: 0.8038420677185059
Epoch 460, training loss: 0.04271171987056732 = 0.035627588629722595 + 0.001 * 7.084129810333252
Epoch 460, val loss: 0.8133196830749512
Epoch 470, training loss: 0.03959965705871582 = 0.032517071813344955 + 0.001 * 7.082584381103516
Epoch 470, val loss: 0.8227141499519348
Epoch 480, training loss: 0.036852288991212845 = 0.02976551093161106 + 0.001 * 7.086778163909912
Epoch 480, val loss: 0.8319870829582214
Epoch 490, training loss: 0.03440801054239273 = 0.027325298637151718 + 0.001 * 7.082711219787598
Epoch 490, val loss: 0.8411557674407959
Epoch 500, training loss: 0.03223380446434021 = 0.025152329355478287 + 0.001 * 7.081475257873535
Epoch 500, val loss: 0.8501335978507996
Epoch 510, training loss: 0.030285678803920746 = 0.023204468190670013 + 0.001 * 7.081209659576416
Epoch 510, val loss: 0.8589502573013306
Epoch 520, training loss: 0.0285237617790699 = 0.021444328129291534 + 0.001 * 7.079434394836426
Epoch 520, val loss: 0.8675796389579773
Epoch 530, training loss: 0.026933826506137848 = 0.019853243604302406 + 0.001 * 7.080583572387695
Epoch 530, val loss: 0.8761038780212402
Epoch 540, training loss: 0.025492120534181595 = 0.018412094563245773 + 0.001 * 7.080026149749756
Epoch 540, val loss: 0.8843896389007568
Epoch 550, training loss: 0.02418653480708599 = 0.017107661813497543 + 0.001 * 7.078872203826904
Epoch 550, val loss: 0.8925063014030457
Epoch 560, training loss: 0.023003902286291122 = 0.01592600718140602 + 0.001 * 7.07789421081543
Epoch 560, val loss: 0.9004685878753662
Epoch 570, training loss: 0.021934978663921356 = 0.014854882843792439 + 0.001 * 7.080094337463379
Epoch 570, val loss: 0.9081990718841553
Epoch 580, training loss: 0.020962201058864594 = 0.013883089646697044 + 0.001 * 7.079110145568848
Epoch 580, val loss: 0.9157779216766357
Epoch 590, training loss: 0.020075960084795952 = 0.013000380247831345 + 0.001 * 7.075579643249512
Epoch 590, val loss: 0.9231559038162231
Epoch 600, training loss: 0.019280409440398216 = 0.012197303585708141 + 0.001 * 7.083105564117432
Epoch 600, val loss: 0.9303470253944397
Epoch 610, training loss: 0.01854114979505539 = 0.011465420015156269 + 0.001 * 7.075729846954346
Epoch 610, val loss: 0.9373401403427124
Epoch 620, training loss: 0.017872001975774765 = 0.010797250084578991 + 0.001 * 7.074751377105713
Epoch 620, val loss: 0.9441772699356079
Epoch 630, training loss: 0.017259176820516586 = 0.01018614787608385 + 0.001 * 7.073028564453125
Epoch 630, val loss: 0.9508053064346313
Epoch 640, training loss: 0.016706271097064018 = 0.009626111947000027 + 0.001 * 7.080158710479736
Epoch 640, val loss: 0.957270085811615
Epoch 650, training loss: 0.016186567023396492 = 0.009111958555877209 + 0.001 * 7.074607849121094
Epoch 650, val loss: 0.9635723233222961
Epoch 660, training loss: 0.015709288418293 = 0.008639016188681126 + 0.001 * 7.070271968841553
Epoch 660, val loss: 0.9697124361991882
Epoch 670, training loss: 0.015276733785867691 = 0.008203109726309776 + 0.001 * 7.073624134063721
Epoch 670, val loss: 0.9757047295570374
Epoch 680, training loss: 0.014872467145323753 = 0.007800647988915443 + 0.001 * 7.071818828582764
Epoch 680, val loss: 0.981532096862793
Epoch 690, training loss: 0.014497945085167885 = 0.007428343873471022 + 0.001 * 7.069600582122803
Epoch 690, val loss: 0.9872215986251831
Epoch 700, training loss: 0.014150743372738361 = 0.007083336357027292 + 0.001 * 7.06740665435791
Epoch 700, val loss: 0.9927600026130676
Epoch 710, training loss: 0.013831619173288345 = 0.006763085722923279 + 0.001 * 7.068532466888428
Epoch 710, val loss: 0.9981593489646912
Epoch 720, training loss: 0.013531915843486786 = 0.006465356796979904 + 0.001 * 7.066558361053467
Epoch 720, val loss: 1.0034235715866089
Epoch 730, training loss: 0.01325377356261015 = 0.006188080180436373 + 0.001 * 7.065692901611328
Epoch 730, val loss: 1.0085748434066772
Epoch 740, training loss: 0.012993207201361656 = 0.005929404869675636 + 0.001 * 7.0638017654418945
Epoch 740, val loss: 1.0135842561721802
Epoch 750, training loss: 0.012754268944263458 = 0.005687709432095289 + 0.001 * 7.066559791564941
Epoch 750, val loss: 1.0184829235076904
Epoch 760, training loss: 0.012531481683254242 = 0.005461584310978651 + 0.001 * 7.069897651672363
Epoch 760, val loss: 1.0232677459716797
Epoch 770, training loss: 0.012310781516134739 = 0.0052497354336082935 + 0.001 * 7.0610456466674805
Epoch 770, val loss: 1.027928113937378
Epoch 780, training loss: 0.0121114831417799 = 0.005050953011959791 + 0.001 * 7.0605292320251465
Epoch 780, val loss: 1.0324914455413818
Epoch 790, training loss: 0.011922921985387802 = 0.004864186514168978 + 0.001 * 7.058734893798828
Epoch 790, val loss: 1.0369547605514526
Epoch 800, training loss: 0.011782355606555939 = 0.0046884953044354916 + 0.001 * 7.093859672546387
Epoch 800, val loss: 1.0413106679916382
Epoch 810, training loss: 0.011586036533117294 = 0.004523066338151693 + 0.001 * 7.062969207763672
Epoch 810, val loss: 1.045562982559204
Epoch 820, training loss: 0.011426502838730812 = 0.0043670847080647945 + 0.001 * 7.059417247772217
Epoch 820, val loss: 1.049706220626831
Epoch 830, training loss: 0.011276238597929478 = 0.004219826776534319 + 0.001 * 7.056411266326904
Epoch 830, val loss: 1.0537598133087158
Epoch 840, training loss: 0.011134084314107895 = 0.004080657847225666 + 0.001 * 7.053426742553711
Epoch 840, val loss: 1.0577300786972046
Epoch 850, training loss: 0.011001311242580414 = 0.003948980011045933 + 0.001 * 7.052330493927002
Epoch 850, val loss: 1.0616137981414795
Epoch 860, training loss: 0.010885289870202541 = 0.0038243085145950317 + 0.001 * 7.060980796813965
Epoch 860, val loss: 1.0654053688049316
Epoch 870, training loss: 0.010762342251837254 = 0.003706173738464713 + 0.001 * 7.056168556213379
Epoch 870, val loss: 1.069122552871704
Epoch 880, training loss: 0.010645398870110512 = 0.0035940990783274174 + 0.001 * 7.051299095153809
Epoch 880, val loss: 1.0727564096450806
Epoch 890, training loss: 0.010552719235420227 = 0.003487640991806984 + 0.001 * 7.065078258514404
Epoch 890, val loss: 1.0763205289840698
Epoch 900, training loss: 0.01043806504458189 = 0.003386417403817177 + 0.001 * 7.051647186279297
Epoch 900, val loss: 1.0798107385635376
Epoch 910, training loss: 0.010337081737816334 = 0.003290075110271573 + 0.001 * 7.047006130218506
Epoch 910, val loss: 1.0832325220108032
Epoch 920, training loss: 0.010252105072140694 = 0.003198199672624469 + 0.001 * 7.053905487060547
Epoch 920, val loss: 1.0865880250930786
Epoch 930, training loss: 0.010154826566576958 = 0.0031104260124266148 + 0.001 * 7.044400215148926
Epoch 930, val loss: 1.0898818969726562
Epoch 940, training loss: 0.010073265060782433 = 0.003026288701221347 + 0.001 * 7.046975612640381
Epoch 940, val loss: 1.0931562185287476
Epoch 950, training loss: 0.009995171800255775 = 0.0029452997259795666 + 0.001 * 7.049872398376465
Epoch 950, val loss: 1.0964422225952148
Epoch 960, training loss: 0.009916894137859344 = 0.002866961294785142 + 0.001 * 7.049932956695557
Epoch 960, val loss: 1.0997127294540405
Epoch 970, training loss: 0.009843267500400543 = 0.0027908801566809416 + 0.001 * 7.05238676071167
Epoch 970, val loss: 1.1030546426773071
Epoch 980, training loss: 0.009760051034390926 = 0.0027168213855475187 + 0.001 * 7.043229579925537
Epoch 980, val loss: 1.1064393520355225
Epoch 990, training loss: 0.009691606275737286 = 0.002644623862579465 + 0.001 * 7.046982288360596
Epoch 990, val loss: 1.1099200248718262
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 1.9532430171966553 = 1.9446462392807007 + 0.001 * 8.596760749816895
Epoch 0, val loss: 1.9495660066604614
Epoch 10, training loss: 1.9437229633331299 = 1.9351263046264648 + 0.001 * 8.596688270568848
Epoch 10, val loss: 1.9394632577896118
Epoch 20, training loss: 1.9320199489593506 = 1.9234235286712646 + 0.001 * 8.596420288085938
Epoch 20, val loss: 1.9267964363098145
Epoch 30, training loss: 1.9154918193817139 = 1.9068959951400757 + 0.001 * 8.595787048339844
Epoch 30, val loss: 1.9088023900985718
Epoch 40, training loss: 1.8908084630966187 = 1.8822141885757446 + 0.001 * 8.594245910644531
Epoch 40, val loss: 1.88226318359375
Epoch 50, training loss: 1.8557524681091309 = 1.8471627235412598 + 0.001 * 8.58974552154541
Epoch 50, val loss: 1.8460280895233154
Epoch 60, training loss: 1.8147168159484863 = 1.806143879890442 + 0.001 * 8.572962760925293
Epoch 60, val loss: 1.8070862293243408
Epoch 70, training loss: 1.776960015296936 = 1.7684731483459473 + 0.001 * 8.486831665039062
Epoch 70, val loss: 1.7752724885940552
Epoch 80, training loss: 1.7318732738494873 = 1.7238097190856934 + 0.001 * 8.063528060913086
Epoch 80, val loss: 1.736833930015564
Epoch 90, training loss: 1.6699028015136719 = 1.6620599031448364 + 0.001 * 7.8428497314453125
Epoch 90, val loss: 1.6819381713867188
Epoch 100, training loss: 1.5867915153503418 = 1.5791645050048828 + 0.001 * 7.627040863037109
Epoch 100, val loss: 1.608485460281372
Epoch 110, training loss: 1.4876089096069336 = 1.4801374673843384 + 0.001 * 7.471468925476074
Epoch 110, val loss: 1.5244137048721313
Epoch 120, training loss: 1.38334059715271 = 1.375928521156311 + 0.001 * 7.412055969238281
Epoch 120, val loss: 1.4399477243423462
Epoch 130, training loss: 1.280906319618225 = 1.2735233306884766 + 0.001 * 7.383007049560547
Epoch 130, val loss: 1.3597064018249512
Epoch 140, training loss: 1.1830086708068848 = 1.1756618022918701 + 0.001 * 7.346924781799316
Epoch 140, val loss: 1.284964919090271
Epoch 150, training loss: 1.092545747756958 = 1.0852371454238892 + 0.001 * 7.308547496795654
Epoch 150, val loss: 1.2163137197494507
Epoch 160, training loss: 1.0118622779846191 = 1.004604458808899 + 0.001 * 7.2578253746032715
Epoch 160, val loss: 1.1560511589050293
Epoch 170, training loss: 0.9402098059654236 = 0.9330048561096191 + 0.001 * 7.204935073852539
Epoch 170, val loss: 1.103379726409912
Epoch 180, training loss: 0.8742722272872925 = 0.8671010136604309 + 0.001 * 7.1711883544921875
Epoch 180, val loss: 1.055539608001709
Epoch 190, training loss: 0.810433566570282 = 0.8032779097557068 + 0.001 * 7.155667304992676
Epoch 190, val loss: 1.0094894170761108
Epoch 200, training loss: 0.7465550899505615 = 0.7394067645072937 + 0.001 * 7.148326396942139
Epoch 200, val loss: 0.9634730815887451
Epoch 210, training loss: 0.6823083758354187 = 0.6751652956008911 + 0.001 * 7.143054008483887
Epoch 210, val loss: 0.918115496635437
Epoch 220, training loss: 0.6184364557266235 = 0.6112962365150452 + 0.001 * 7.140204429626465
Epoch 220, val loss: 0.8751598000526428
Epoch 230, training loss: 0.5560580492019653 = 0.5489201545715332 + 0.001 * 7.137899875640869
Epoch 230, val loss: 0.8368332982063293
Epoch 240, training loss: 0.4958508014678955 = 0.48871538043022156 + 0.001 * 7.135431289672852
Epoch 240, val loss: 0.8037969470024109
Epoch 250, training loss: 0.4381333589553833 = 0.4310007393360138 + 0.001 * 7.13261604309082
Epoch 250, val loss: 0.7758798003196716
Epoch 260, training loss: 0.38319194316864014 = 0.37606173753738403 + 0.001 * 7.13020133972168
Epoch 260, val loss: 0.7521374225616455
Epoch 270, training loss: 0.33148133754730225 = 0.32435187697410583 + 0.001 * 7.129456996917725
Epoch 270, val loss: 0.7318335175514221
Epoch 280, training loss: 0.2837555408477783 = 0.2766292989253998 + 0.001 * 7.126250267028809
Epoch 280, val loss: 0.7148689031600952
Epoch 290, training loss: 0.2408892959356308 = 0.23376545310020447 + 0.001 * 7.123837947845459
Epoch 290, val loss: 0.7014512419700623
Epoch 300, training loss: 0.20361189544200897 = 0.19649048149585724 + 0.001 * 7.121407985687256
Epoch 300, val loss: 0.6920885443687439
Epoch 310, training loss: 0.1721845120191574 = 0.16506360471248627 + 0.001 * 7.120908260345459
Epoch 310, val loss: 0.6869910955429077
Epoch 320, training loss: 0.14627715945243835 = 0.13916105031967163 + 0.001 * 7.116108417510986
Epoch 320, val loss: 0.6859480142593384
Epoch 330, training loss: 0.12517814338207245 = 0.11806429177522659 + 0.001 * 7.1138482093811035
Epoch 330, val loss: 0.6885779500007629
Epoch 340, training loss: 0.10805783420801163 = 0.10094739496707916 + 0.001 * 7.110437393188477
Epoch 340, val loss: 0.6943026781082153
Epoch 350, training loss: 0.0941401869058609 = 0.08703385293483734 + 0.001 * 7.106331825256348
Epoch 350, val loss: 0.7023401856422424
Epoch 360, training loss: 0.08278919011354446 = 0.0756700187921524 + 0.001 * 7.119169235229492
Epoch 360, val loss: 0.7119980454444885
Epoch 370, training loss: 0.07341659069061279 = 0.0663117840886116 + 0.001 * 7.104808807373047
Epoch 370, val loss: 0.722638726234436
Epoch 380, training loss: 0.0656207799911499 = 0.058523811399936676 + 0.001 * 7.096966743469238
Epoch 380, val loss: 0.7337735891342163
Epoch 390, training loss: 0.059064775705337524 = 0.051971521228551865 + 0.001 * 7.0932536125183105
Epoch 390, val loss: 0.7451795339584351
Epoch 400, training loss: 0.05349833518266678 = 0.04640955850481987 + 0.001 * 7.088774681091309
Epoch 400, val loss: 0.7566304802894592
Epoch 410, training loss: 0.048752617090940475 = 0.041648972779512405 + 0.001 * 7.103645324707031
Epoch 410, val loss: 0.7680959701538086
Epoch 420, training loss: 0.04462972655892372 = 0.03754648193717003 + 0.001 * 7.083242893218994
Epoch 420, val loss: 0.7795101404190063
Epoch 430, training loss: 0.04107033088803291 = 0.033990442752838135 + 0.001 * 7.079887390136719
Epoch 430, val loss: 0.7908291816711426
Epoch 440, training loss: 0.037980690598487854 = 0.030891047790646553 + 0.001 * 7.089641094207764
Epoch 440, val loss: 0.8020216822624207
Epoch 450, training loss: 0.0352601557970047 = 0.02817658707499504 + 0.001 * 7.083568572998047
Epoch 450, val loss: 0.8129565119743347
Epoch 460, training loss: 0.03285859525203705 = 0.025789398699998856 + 0.001 * 7.06919527053833
Epoch 460, val loss: 0.8236560225486755
Epoch 470, training loss: 0.03075202740728855 = 0.023681486025452614 + 0.001 * 7.070540428161621
Epoch 470, val loss: 0.8340857028961182
Epoch 480, training loss: 0.028880156576633453 = 0.021812956780195236 + 0.001 * 7.067198753356934
Epoch 480, val loss: 0.8442454934120178
Epoch 490, training loss: 0.027211736887693405 = 0.0201509241014719 + 0.001 * 7.060811996459961
Epoch 490, val loss: 0.8541216254234314
Epoch 500, training loss: 0.025731727480888367 = 0.01866748556494713 + 0.001 * 7.064241886138916
Epoch 500, val loss: 0.8637145161628723
Epoch 510, training loss: 0.02438797801733017 = 0.01733921840786934 + 0.001 * 7.048758506774902
Epoch 510, val loss: 0.8730124235153198
Epoch 520, training loss: 0.023191973567008972 = 0.016146134585142136 + 0.001 * 7.045837879180908
Epoch 520, val loss: 0.8820492029190063
Epoch 530, training loss: 0.022126197814941406 = 0.015071160160005093 + 0.001 * 7.055036544799805
Epoch 530, val loss: 0.8908246159553528
Epoch 540, training loss: 0.021146699786186218 = 0.014099694788455963 + 0.001 * 7.047003746032715
Epoch 540, val loss: 0.8993379473686218
Epoch 550, training loss: 0.020259570330381393 = 0.013219282031059265 + 0.001 * 7.040287971496582
Epoch 550, val loss: 0.9076244235038757
Epoch 560, training loss: 0.019452137872576714 = 0.012419243343174458 + 0.001 * 7.032894134521484
Epoch 560, val loss: 0.9156565070152283
Epoch 570, training loss: 0.018723169341683388 = 0.011690312065184116 + 0.001 * 7.032857418060303
Epoch 570, val loss: 0.9234614372253418
Epoch 580, training loss: 0.01807353086769581 = 0.011024472303688526 + 0.001 * 7.04905891418457
Epoch 580, val loss: 0.9310591220855713
Epoch 590, training loss: 0.017441395670175552 = 0.010414845310151577 + 0.001 * 7.026549339294434
Epoch 590, val loss: 0.9384356737136841
Epoch 600, training loss: 0.016882555559277534 = 0.009855377487838268 + 0.001 * 7.027177810668945
Epoch 600, val loss: 0.9456270933151245
Epoch 610, training loss: 0.016378061845898628 = 0.0093408627435565 + 0.001 * 7.037198543548584
Epoch 610, val loss: 0.9525958299636841
Epoch 620, training loss: 0.015885325148701668 = 0.008866648189723492 + 0.001 * 7.018676280975342
Epoch 620, val loss: 0.9593833684921265
Epoch 630, training loss: 0.015466868877410889 = 0.008428708650171757 + 0.001 * 7.038159370422363
Epoch 630, val loss: 0.9660089612007141
Epoch 640, training loss: 0.01504298485815525 = 0.008023357950150967 + 0.001 * 7.019626140594482
Epoch 640, val loss: 0.9724620580673218
Epoch 650, training loss: 0.014688684605062008 = 0.007646909914910793 + 0.001 * 7.041774272918701
Epoch 650, val loss: 0.9787646532058716
Epoch 660, training loss: 0.014307323843240738 = 0.007295314222574234 + 0.001 * 7.012009143829346
Epoch 660, val loss: 0.9849358201026917
Epoch 670, training loss: 0.013973323628306389 = 0.006964021362364292 + 0.001 * 7.009302616119385
Epoch 670, val loss: 0.9910396933555603
Epoch 680, training loss: 0.013656786642968655 = 0.006649735849350691 + 0.001 * 7.007050514221191
Epoch 680, val loss: 0.9971905946731567
Epoch 690, training loss: 0.013348231092095375 = 0.006350850686430931 + 0.001 * 6.997380256652832
Epoch 690, val loss: 1.0033804178237915
Epoch 700, training loss: 0.01309250295162201 = 0.006066914182156324 + 0.001 * 7.025588035583496
Epoch 700, val loss: 1.0095442533493042
Epoch 710, training loss: 0.012802038341760635 = 0.005797856021672487 + 0.001 * 7.004181385040283
Epoch 710, val loss: 1.0156856775283813
Epoch 720, training loss: 0.01253775879740715 = 0.005542780738323927 + 0.001 * 6.994977951049805
Epoch 720, val loss: 1.0217775106430054
Epoch 730, training loss: 0.012297654524445534 = 0.00530129624530673 + 0.001 * 6.996357440948486
Epoch 730, val loss: 1.0278147459030151
Epoch 740, training loss: 0.012070121243596077 = 0.005072098225355148 + 0.001 * 6.998022079467773
Epoch 740, val loss: 1.0338276624679565
Epoch 750, training loss: 0.01184111274778843 = 0.0048514618538320065 + 0.001 * 6.989650249481201
Epoch 750, val loss: 1.0398961305618286
Epoch 760, training loss: 0.011628612875938416 = 0.004643217194825411 + 0.001 * 6.985395431518555
Epoch 760, val loss: 1.0458627939224243
Epoch 770, training loss: 0.011448383331298828 = 0.00444754445925355 + 0.001 * 7.0008392333984375
Epoch 770, val loss: 1.0516974925994873
Epoch 780, training loss: 0.011248868890106678 = 0.0042622871696949005 + 0.001 * 6.986581325531006
Epoch 780, val loss: 1.0574125051498413
Epoch 790, training loss: 0.011085689067840576 = 0.004087324254214764 + 0.001 * 6.998363971710205
Epoch 790, val loss: 1.0631908178329468
Epoch 800, training loss: 0.010916365310549736 = 0.003921751398593187 + 0.001 * 6.9946136474609375
Epoch 800, val loss: 1.0689102411270142
Epoch 810, training loss: 0.01074708066880703 = 0.0037648286670446396 + 0.001 * 6.98225212097168
Epoch 810, val loss: 1.0745927095413208
Epoch 820, training loss: 0.010602698661386967 = 0.0036162359174340963 + 0.001 * 6.986462116241455
Epoch 820, val loss: 1.0801385641098022
Epoch 830, training loss: 0.010466469451785088 = 0.003475659992545843 + 0.001 * 6.990808486938477
Epoch 830, val loss: 1.0856671333312988
Epoch 840, training loss: 0.010324087925255299 = 0.003342775395140052 + 0.001 * 6.981311798095703
Epoch 840, val loss: 1.0911359786987305
Epoch 850, training loss: 0.010198456235229969 = 0.0032169162295758724 + 0.001 * 6.981539726257324
Epoch 850, val loss: 1.0965406894683838
Epoch 860, training loss: 0.010073080658912659 = 0.003097840351983905 + 0.001 * 6.9752397537231445
Epoch 860, val loss: 1.101897120475769
Epoch 870, training loss: 0.009992968291044235 = 0.0029850020073354244 + 0.001 * 7.0079665184021
Epoch 870, val loss: 1.1071598529815674
Epoch 880, training loss: 0.009863206185400486 = 0.0028784123715013266 + 0.001 * 6.984793663024902
Epoch 880, val loss: 1.112356185913086
Epoch 890, training loss: 0.009756699204444885 = 0.0027773550245910883 + 0.001 * 6.979343414306641
Epoch 890, val loss: 1.1174979209899902
Epoch 900, training loss: 0.009635684080421925 = 0.0026816281024366617 + 0.001 * 6.954055309295654
Epoch 900, val loss: 1.1225502490997314
Epoch 910, training loss: 0.009561561048030853 = 0.002590832067653537 + 0.001 * 6.970728874206543
Epoch 910, val loss: 1.1275343894958496
Epoch 920, training loss: 0.009477322921156883 = 0.0025046069640666246 + 0.001 * 6.972715854644775
Epoch 920, val loss: 1.1324679851531982
Epoch 930, training loss: 0.009384868666529655 = 0.002422731602564454 + 0.001 * 6.962136268615723
Epoch 930, val loss: 1.1373274326324463
Epoch 940, training loss: 0.009300393983721733 = 0.002344922861084342 + 0.001 * 6.955470561981201
Epoch 940, val loss: 1.1421289443969727
Epoch 950, training loss: 0.00922350399196148 = 0.002271024975925684 + 0.001 * 6.952478408813477
Epoch 950, val loss: 1.146865963935852
Epoch 960, training loss: 0.009189680218696594 = 0.0022007389925420284 + 0.001 * 6.988940238952637
Epoch 960, val loss: 1.1515252590179443
Epoch 970, training loss: 0.009118013083934784 = 0.002133928006514907 + 0.001 * 6.9840850830078125
Epoch 970, val loss: 1.1561771631240845
Epoch 980, training loss: 0.009034201502799988 = 0.0020704290363937616 + 0.001 * 6.963771820068359
Epoch 980, val loss: 1.1606742143630981
Epoch 990, training loss: 0.008972564712166786 = 0.002010009717196226 + 0.001 * 6.962554454803467
Epoch 990, val loss: 1.1651618480682373
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.9610717296600342 = 1.9524749517440796 + 0.001 * 8.596796989440918
Epoch 0, val loss: 1.960185170173645
Epoch 10, training loss: 1.9506487846374512 = 1.9420520067214966 + 0.001 * 8.596726417541504
Epoch 10, val loss: 1.9492883682250977
Epoch 20, training loss: 1.937784194946289 = 1.9291876554489136 + 0.001 * 8.59648323059082
Epoch 20, val loss: 1.935850739479065
Epoch 30, training loss: 1.919700026512146 = 1.9111042022705078 + 0.001 * 8.595841407775879
Epoch 30, val loss: 1.9171502590179443
Epoch 40, training loss: 1.8930407762527466 = 1.8844467401504517 + 0.001 * 8.594005584716797
Epoch 40, val loss: 1.8902889490127563
Epoch 50, training loss: 1.855863094329834 = 1.8472753763198853 + 0.001 * 8.587761878967285
Epoch 50, val loss: 1.854550838470459
Epoch 60, training loss: 1.813149333000183 = 1.8045876026153564 + 0.001 * 8.561745643615723
Epoch 60, val loss: 1.8167330026626587
Epoch 70, training loss: 1.7744942903518677 = 1.7660720348358154 + 0.001 * 8.422235488891602
Epoch 70, val loss: 1.7840670347213745
Epoch 80, training loss: 1.72818922996521 = 1.7201064825057983 + 0.001 * 8.082793235778809
Epoch 80, val loss: 1.7421443462371826
Epoch 90, training loss: 1.663342833518982 = 1.655396580696106 + 0.001 * 7.946267127990723
Epoch 90, val loss: 1.686225414276123
Epoch 100, training loss: 1.579113245010376 = 1.5712611675262451 + 0.001 * 7.852059841156006
Epoch 100, val loss: 1.6182382106781006
Epoch 110, training loss: 1.483828067779541 = 1.4761269092559814 + 0.001 * 7.701159954071045
Epoch 110, val loss: 1.5416446924209595
Epoch 120, training loss: 1.388303518295288 = 1.380823016166687 + 0.001 * 7.480557918548584
Epoch 120, val loss: 1.466981291770935
Epoch 130, training loss: 1.2968344688415527 = 1.2894234657287598 + 0.001 * 7.410946369171143
Epoch 130, val loss: 1.3966742753982544
Epoch 140, training loss: 1.2078044414520264 = 1.2004597187042236 + 0.001 * 7.344668865203857
Epoch 140, val loss: 1.3297287225723267
Epoch 150, training loss: 1.1209185123443604 = 1.1136236190795898 + 0.001 * 7.294933319091797
Epoch 150, val loss: 1.2650153636932373
Epoch 160, training loss: 1.0371378660202026 = 1.0298727750778198 + 0.001 * 7.265053749084473
Epoch 160, val loss: 1.2020069360733032
Epoch 170, training loss: 0.9568194150924683 = 0.9495667815208435 + 0.001 * 7.252641201019287
Epoch 170, val loss: 1.1404366493225098
Epoch 180, training loss: 0.8792941570281982 = 0.8720443844795227 + 0.001 * 7.24978494644165
Epoch 180, val loss: 1.0797221660614014
Epoch 190, training loss: 0.8038142323493958 = 0.7965663075447083 + 0.001 * 7.247934818267822
Epoch 190, val loss: 1.0198595523834229
Epoch 200, training loss: 0.7303124666213989 = 0.7230671644210815 + 0.001 * 7.245291709899902
Epoch 200, val loss: 0.961970329284668
Epoch 210, training loss: 0.6595203876495361 = 0.6522781252861023 + 0.001 * 7.242269039154053
Epoch 210, val loss: 0.9073495864868164
Epoch 220, training loss: 0.5922906398773193 = 0.5850514769554138 + 0.001 * 7.23916482925415
Epoch 220, val loss: 0.8581939935684204
Epoch 230, training loss: 0.5288074016571045 = 0.5215713977813721 + 0.001 * 7.235997676849365
Epoch 230, val loss: 0.8154556155204773
Epoch 240, training loss: 0.4689887762069702 = 0.4617561399936676 + 0.001 * 7.232645511627197
Epoch 240, val loss: 0.779211699962616
Epoch 250, training loss: 0.41302481293678284 = 0.40579596161842346 + 0.001 * 7.228858947753906
Epoch 250, val loss: 0.7491058707237244
Epoch 260, training loss: 0.36149388551712036 = 0.35426977276802063 + 0.001 * 7.2241058349609375
Epoch 260, val loss: 0.7246702909469604
Epoch 270, training loss: 0.31501632928848267 = 0.30779722332954407 + 0.001 * 7.219113349914551
Epoch 270, val loss: 0.7055484652519226
Epoch 280, training loss: 0.2738448679447174 = 0.2666340172290802 + 0.001 * 7.210849285125732
Epoch 280, val loss: 0.6915038824081421
Epoch 290, training loss: 0.23784540593624115 = 0.23064514994621277 + 0.001 * 7.200251579284668
Epoch 290, val loss: 0.6820275783538818
Epoch 300, training loss: 0.20666353404521942 = 0.19946704804897308 + 0.001 * 7.196490287780762
Epoch 300, val loss: 0.6764374375343323
Epoch 310, training loss: 0.1798141747713089 = 0.172636017203331 + 0.001 * 7.178152561187744
Epoch 310, val loss: 0.6741330623626709
Epoch 320, training loss: 0.15681372582912445 = 0.14964859187602997 + 0.001 * 7.165140628814697
Epoch 320, val loss: 0.6746353507041931
Epoch 330, training loss: 0.1371292620897293 = 0.12998676300048828 + 0.001 * 7.142501354217529
Epoch 330, val loss: 0.677555501461029
Epoch 340, training loss: 0.12031110376119614 = 0.11316950619220734 + 0.001 * 7.141595840454102
Epoch 340, val loss: 0.6824772357940674
Epoch 350, training loss: 0.10588736832141876 = 0.09876914322376251 + 0.001 * 7.11822509765625
Epoch 350, val loss: 0.6890230178833008
Epoch 360, training loss: 0.09353774040937424 = 0.08641758561134338 + 0.001 * 7.120157241821289
Epoch 360, val loss: 0.6969149112701416
Epoch 370, training loss: 0.08292149752378464 = 0.07580327987670898 + 0.001 * 7.118217468261719
Epoch 370, val loss: 0.7058692574501038
Epoch 380, training loss: 0.07377642393112183 = 0.06666392087936401 + 0.001 * 7.1125006675720215
Epoch 380, val loss: 0.7155910730361938
Epoch 390, training loss: 0.06590291857719421 = 0.058792844414711 + 0.001 * 7.110073089599609
Epoch 390, val loss: 0.7259427309036255
Epoch 400, training loss: 0.05912735313177109 = 0.052016496658325195 + 0.001 * 7.110854148864746
Epoch 400, val loss: 0.7367411255836487
Epoch 410, training loss: 0.05328810214996338 = 0.0461886040866375 + 0.001 * 7.099499702453613
Epoch 410, val loss: 0.7478302121162415
Epoch 420, training loss: 0.04827827215194702 = 0.041181620210409164 + 0.001 * 7.09665060043335
Epoch 420, val loss: 0.7591108679771423
Epoch 430, training loss: 0.04397840052843094 = 0.03688012808561325 + 0.001 * 7.098271369934082
Epoch 430, val loss: 0.7704455256462097
Epoch 440, training loss: 0.04026596620678902 = 0.033179979771375656 + 0.001 * 7.08598518371582
Epoch 440, val loss: 0.7816830277442932
Epoch 450, training loss: 0.03707128390669823 = 0.029987260699272156 + 0.001 * 7.0840229988098145
Epoch 450, val loss: 0.792787492275238
Epoch 460, training loss: 0.034317098557949066 = 0.027221057564020157 + 0.001 * 7.096039295196533
Epoch 460, val loss: 0.8037165999412537
Epoch 470, training loss: 0.0318915918469429 = 0.024813687428832054 + 0.001 * 7.077902793884277
Epoch 470, val loss: 0.8144319653511047
Epoch 480, training loss: 0.029789471998810768 = 0.022708866745233536 + 0.001 * 7.080604553222656
Epoch 480, val loss: 0.8248910903930664
Epoch 490, training loss: 0.027923237532377243 = 0.020860327407717705 + 0.001 * 7.0629096031188965
Epoch 490, val loss: 0.835081934928894
Epoch 500, training loss: 0.026306770741939545 = 0.019229376688599586 + 0.001 * 7.077394008636475
Epoch 500, val loss: 0.8450031280517578
Epoch 510, training loss: 0.024843137711286545 = 0.017784154042601585 + 0.001 * 7.058982849121094
Epoch 510, val loss: 0.8546626567840576
Epoch 520, training loss: 0.023550506681203842 = 0.01649841107428074 + 0.001 * 7.052094459533691
Epoch 520, val loss: 0.8640192747116089
Epoch 530, training loss: 0.022403139621019363 = 0.015349735505878925 + 0.001 * 7.053404331207275
Epoch 530, val loss: 0.8731476664543152
Epoch 540, training loss: 0.021359672769904137 = 0.014319694600999355 + 0.001 * 7.039977550506592
Epoch 540, val loss: 0.8820161819458008
Epoch 550, training loss: 0.020445063710212708 = 0.013392778113484383 + 0.001 * 7.0522847175598145
Epoch 550, val loss: 0.8906393647193909
Epoch 560, training loss: 0.01960148476064205 = 0.012555927038192749 + 0.001 * 7.045557975769043
Epoch 560, val loss: 0.8990135192871094
Epoch 570, training loss: 0.018828371539711952 = 0.011797863058745861 + 0.001 * 7.030508518218994
Epoch 570, val loss: 0.9071784019470215
Epoch 580, training loss: 0.018157949671149254 = 0.011109184473752975 + 0.001 * 7.048764705657959
Epoch 580, val loss: 0.9151004552841187
Epoch 590, training loss: 0.017505090683698654 = 0.010481758043169975 + 0.001 * 7.023332118988037
Epoch 590, val loss: 0.922829270362854
Epoch 600, training loss: 0.016977917402982712 = 0.009908556938171387 + 0.001 * 7.069360256195068
Epoch 600, val loss: 0.930350661277771
Epoch 610, training loss: 0.016417842358350754 = 0.009383538737893105 + 0.001 * 7.034303665161133
Epoch 610, val loss: 0.9376901984214783
Epoch 620, training loss: 0.01592707261443138 = 0.008901459164917469 + 0.001 * 7.025614261627197
Epoch 620, val loss: 0.9448370933532715
Epoch 630, training loss: 0.015474321320652962 = 0.008457737974822521 + 0.001 * 7.016582489013672
Epoch 630, val loss: 0.9518169164657593
Epoch 640, training loss: 0.015071582980453968 = 0.008048374205827713 + 0.001 * 7.0232086181640625
Epoch 640, val loss: 0.9586076736450195
Epoch 650, training loss: 0.014711066149175167 = 0.007669929880648851 + 0.001 * 7.041135787963867
Epoch 650, val loss: 0.9652377367019653
Epoch 660, training loss: 0.014330198056995869 = 0.007319388911128044 + 0.001 * 7.010808944702148
Epoch 660, val loss: 0.9717251062393188
Epoch 670, training loss: 0.014013972133398056 = 0.006994051858782768 + 0.001 * 7.019920349121094
Epoch 670, val loss: 0.9780246019363403
Epoch 680, training loss: 0.013700110837817192 = 0.00669159134849906 + 0.001 * 7.008519172668457
Epoch 680, val loss: 0.9841771721839905
Epoch 690, training loss: 0.013408251106739044 = 0.006409887690097094 + 0.001 * 6.998363494873047
Epoch 690, val loss: 0.9901978373527527
Epoch 700, training loss: 0.013144975528120995 = 0.006147067993879318 + 0.001 * 6.997906684875488
Epoch 700, val loss: 0.9960613250732422
Epoch 710, training loss: 0.01292380876839161 = 0.00590146379545331 + 0.001 * 7.022344589233398
Epoch 710, val loss: 1.0018011331558228
Epoch 720, training loss: 0.012665937654674053 = 0.0056716063991189 + 0.001 * 6.994330883026123
Epoch 720, val loss: 1.007426381111145
Epoch 730, training loss: 0.012456705793738365 = 0.005456140264868736 + 0.001 * 7.000565528869629
Epoch 730, val loss: 1.0129129886627197
Epoch 740, training loss: 0.012249183841049671 = 0.005253887735307217 + 0.001 * 6.995296001434326
Epoch 740, val loss: 1.0182909965515137
Epoch 750, training loss: 0.012055747210979462 = 0.0050638108514249325 + 0.001 * 6.991936206817627
Epoch 750, val loss: 1.0235416889190674
Epoch 760, training loss: 0.011871673166751862 = 0.004884956404566765 + 0.001 * 6.986715793609619
Epoch 760, val loss: 1.028686761856079
Epoch 770, training loss: 0.011698465794324875 = 0.004716427996754646 + 0.001 * 6.9820380210876465
Epoch 770, val loss: 1.0337258577346802
Epoch 780, training loss: 0.011562500149011612 = 0.004557414911687374 + 0.001 * 7.00508451461792
Epoch 780, val loss: 1.0386395454406738
Epoch 790, training loss: 0.01140640489757061 = 0.004407198168337345 + 0.001 * 6.999207019805908
Epoch 790, val loss: 1.0434834957122803
Epoch 800, training loss: 0.011250704526901245 = 0.004264978691935539 + 0.001 * 6.985724925994873
Epoch 800, val loss: 1.0482298135757446
Epoch 810, training loss: 0.011121485382318497 = 0.004129901062697172 + 0.001 * 6.991584300994873
Epoch 810, val loss: 1.0529261827468872
Epoch 820, training loss: 0.01099693588912487 = 0.004000861197710037 + 0.001 * 6.996074676513672
Epoch 820, val loss: 1.0575883388519287
Epoch 830, training loss: 0.010886361822485924 = 0.00387660157866776 + 0.001 * 7.009759902954102
Epoch 830, val loss: 1.062303066253662
Epoch 840, training loss: 0.01073954626917839 = 0.0037562530487775803 + 0.001 * 6.983293056488037
Epoch 840, val loss: 1.067070722579956
Epoch 850, training loss: 0.010614247992634773 = 0.003639416303485632 + 0.001 * 6.974831581115723
Epoch 850, val loss: 1.071924090385437
Epoch 860, training loss: 0.010505878366529942 = 0.0035260457079857588 + 0.001 * 6.979832649230957
Epoch 860, val loss: 1.076850175857544
Epoch 870, training loss: 0.010393945500254631 = 0.0034163056407123804 + 0.001 * 6.977639198303223
Epoch 870, val loss: 1.0818469524383545
Epoch 880, training loss: 0.010277476161718369 = 0.0033105502370744944 + 0.001 * 6.966925144195557
Epoch 880, val loss: 1.0868500471115112
Epoch 890, training loss: 0.010193983092904091 = 0.0032088784500956535 + 0.001 * 6.985104560852051
Epoch 890, val loss: 1.0918655395507812
Epoch 900, training loss: 0.010077574290335178 = 0.0031113512814044952 + 0.001 * 6.966222763061523
Epoch 900, val loss: 1.0968551635742188
Epoch 910, training loss: 0.009992741979658604 = 0.0030180441681295633 + 0.001 * 6.974697113037109
Epoch 910, val loss: 1.1017951965332031
Epoch 920, training loss: 0.009900229051709175 = 0.002928911242634058 + 0.001 * 6.971317291259766
Epoch 920, val loss: 1.1066993474960327
Epoch 930, training loss: 0.009840515442192554 = 0.0028437767177820206 + 0.001 * 6.996738433837891
Epoch 930, val loss: 1.1115412712097168
Epoch 940, training loss: 0.009738991037011147 = 0.0027626219671219587 + 0.001 * 6.9763689041137695
Epoch 940, val loss: 1.1162797212600708
Epoch 950, training loss: 0.009680837392807007 = 0.0026851180009543896 + 0.001 * 6.995718479156494
Epoch 950, val loss: 1.1209732294082642
Epoch 960, training loss: 0.009586811996996403 = 0.0026111681945621967 + 0.001 * 6.975643634796143
Epoch 960, val loss: 1.1255695819854736
Epoch 970, training loss: 0.009498598985373974 = 0.00254053040407598 + 0.001 * 6.958068370819092
Epoch 970, val loss: 1.1301000118255615
Epoch 980, training loss: 0.009430667385458946 = 0.002473076805472374 + 0.001 * 6.957590579986572
Epoch 980, val loss: 1.1345365047454834
Epoch 990, training loss: 0.009368108585476875 = 0.002408646047115326 + 0.001 * 6.959462642669678
Epoch 990, val loss: 1.13889479637146
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8360569319978914
The final CL Acc:0.82346, 0.00630, The final GNN Acc:0.83711, 0.00114
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10508])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9554303884506226 = 1.9468334913253784 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.9395893812179565
Epoch 10, training loss: 1.9460290670394897 = 1.9374322891235352 + 0.001 * 8.596806526184082
Epoch 10, val loss: 1.930475115776062
Epoch 20, training loss: 1.9343476295471191 = 1.925750970840454 + 0.001 * 8.59664535522461
Epoch 20, val loss: 1.9187135696411133
Epoch 30, training loss: 1.9179801940917969 = 1.9093838930130005 + 0.001 * 8.596245765686035
Epoch 30, val loss: 1.9018000364303589
Epoch 40, training loss: 1.8939496278762817 = 1.8853545188903809 + 0.001 * 8.595159530639648
Epoch 40, val loss: 1.876863718032837
Epoch 50, training loss: 1.8607206344604492 = 1.8521289825439453 + 0.001 * 8.59166145324707
Epoch 50, val loss: 1.8435803651809692
Epoch 60, training loss: 1.8244974613189697 = 1.815919041633606 + 0.001 * 8.578449249267578
Epoch 60, val loss: 1.8109538555145264
Epoch 70, training loss: 1.7961950302124023 = 1.787672996520996 + 0.001 * 8.522038459777832
Epoch 70, val loss: 1.7897311449050903
Epoch 80, training loss: 1.764430046081543 = 1.7561542987823486 + 0.001 * 8.275754928588867
Epoch 80, val loss: 1.7647762298583984
Epoch 90, training loss: 1.7199070453643799 = 1.7117340564727783 + 0.001 * 8.172942161560059
Epoch 90, val loss: 1.7264965772628784
Epoch 100, training loss: 1.656499981880188 = 1.6484829187393188 + 0.001 * 8.01705265045166
Epoch 100, val loss: 1.671725869178772
Epoch 110, training loss: 1.5727442502975464 = 1.5649515390396118 + 0.001 * 7.792690277099609
Epoch 110, val loss: 1.6017616987228394
Epoch 120, training loss: 1.4768728017807007 = 1.4692221879959106 + 0.001 * 7.650651454925537
Epoch 120, val loss: 1.5242199897766113
Epoch 130, training loss: 1.3761991262435913 = 1.368592381477356 + 0.001 * 7.606786251068115
Epoch 130, val loss: 1.4447418451309204
Epoch 140, training loss: 1.2750831842422485 = 1.2675458192825317 + 0.001 * 7.537316799163818
Epoch 140, val loss: 1.3690271377563477
Epoch 150, training loss: 1.1771987676620483 = 1.1697089672088623 + 0.001 * 7.489745140075684
Epoch 150, val loss: 1.2983487844467163
Epoch 160, training loss: 1.085974097251892 = 1.0785273313522339 + 0.001 * 7.446746826171875
Epoch 160, val loss: 1.2348320484161377
Epoch 170, training loss: 1.002987027168274 = 0.9955929517745972 + 0.001 * 7.394035816192627
Epoch 170, val loss: 1.1792405843734741
Epoch 180, training loss: 0.9268883466720581 = 0.9195292592048645 + 0.001 * 7.359096527099609
Epoch 180, val loss: 1.1294703483581543
Epoch 190, training loss: 0.8556017279624939 = 0.848259449005127 + 0.001 * 7.3423051834106445
Epoch 190, val loss: 1.0835672616958618
Epoch 200, training loss: 0.7881668210029602 = 0.7808383703231812 + 0.001 * 7.328463077545166
Epoch 200, val loss: 1.0409207344055176
Epoch 210, training loss: 0.7247306108474731 = 0.7174146771430969 + 0.001 * 7.315938949584961
Epoch 210, val loss: 1.001496434211731
Epoch 220, training loss: 0.6654883027076721 = 0.658179759979248 + 0.001 * 7.308522701263428
Epoch 220, val loss: 0.9659760594367981
Epoch 230, training loss: 0.6096158623695374 = 0.6023133993148804 + 0.001 * 7.302468776702881
Epoch 230, val loss: 0.9348553419113159
Epoch 240, training loss: 0.555963397026062 = 0.5486637949943542 + 0.001 * 7.299585819244385
Epoch 240, val loss: 0.9082349538803101
Epoch 250, training loss: 0.5038737654685974 = 0.4965771734714508 + 0.001 * 7.296575546264648
Epoch 250, val loss: 0.8864445686340332
Epoch 260, training loss: 0.45355114340782166 = 0.4462459683418274 + 0.001 * 7.3051605224609375
Epoch 260, val loss: 0.8694090247154236
Epoch 270, training loss: 0.40559765696525574 = 0.39829954504966736 + 0.001 * 7.298099517822266
Epoch 270, val loss: 0.8569296598434448
Epoch 280, training loss: 0.36068299412727356 = 0.3533884286880493 + 0.001 * 7.2945733070373535
Epoch 280, val loss: 0.8486401438713074
Epoch 290, training loss: 0.319290429353714 = 0.31199586391448975 + 0.001 * 7.2945756912231445
Epoch 290, val loss: 0.8440223932266235
Epoch 300, training loss: 0.281747430562973 = 0.2744522988796234 + 0.001 * 7.295144081115723
Epoch 300, val loss: 0.8429027199745178
Epoch 310, training loss: 0.2482023388147354 = 0.24090269207954407 + 0.001 * 7.2996416091918945
Epoch 310, val loss: 0.8448994159698486
Epoch 320, training loss: 0.21860365569591522 = 0.21130621433258057 + 0.001 * 7.297433853149414
Epoch 320, val loss: 0.8497423529624939
Epoch 330, training loss: 0.1927376091480255 = 0.18543995916843414 + 0.001 * 7.297646522521973
Epoch 330, val loss: 0.8570830225944519
Epoch 340, training loss: 0.1702655851840973 = 0.16296641528606415 + 0.001 * 7.299167633056641
Epoch 340, val loss: 0.866518497467041
Epoch 350, training loss: 0.1507856696844101 = 0.14348703622817993 + 0.001 * 7.298635959625244
Epoch 350, val loss: 0.8775566220283508
Epoch 360, training loss: 0.13390521705150604 = 0.1266060173511505 + 0.001 * 7.299197196960449
Epoch 360, val loss: 0.8899164795875549
Epoch 370, training loss: 0.11925095319747925 = 0.11195457726716995 + 0.001 * 7.296376705169678
Epoch 370, val loss: 0.9032173156738281
Epoch 380, training loss: 0.10651011019945145 = 0.09921443462371826 + 0.001 * 7.29567813873291
Epoch 380, val loss: 0.9173117280006409
Epoch 390, training loss: 0.09541545063257217 = 0.08812034130096436 + 0.001 * 7.295107841491699
Epoch 390, val loss: 0.931911289691925
Epoch 400, training loss: 0.0857466459274292 = 0.07844731956720352 + 0.001 * 7.299325466156006
Epoch 400, val loss: 0.9468380212783813
Epoch 410, training loss: 0.07730291783809662 = 0.07000533491373062 + 0.001 * 7.297585487365723
Epoch 410, val loss: 0.9619357585906982
Epoch 420, training loss: 0.06992553919553757 = 0.06263143569231033 + 0.001 * 7.294103622436523
Epoch 420, val loss: 0.977164626121521
Epoch 430, training loss: 0.06347687542438507 = 0.056185051798820496 + 0.001 * 7.2918267250061035
Epoch 430, val loss: 0.9923055171966553
Epoch 440, training loss: 0.057826653122901917 = 0.05054394155740738 + 0.001 * 7.282712459564209
Epoch 440, val loss: 1.0073168277740479
Epoch 450, training loss: 0.052892450243234634 = 0.045600976794958115 + 0.001 * 7.291471481323242
Epoch 450, val loss: 1.022119402885437
Epoch 460, training loss: 0.04854603856801987 = 0.04126372188329697 + 0.001 * 7.282315731048584
Epoch 460, val loss: 1.0366246700286865
Epoch 470, training loss: 0.044726528227329254 = 0.03744999319314957 + 0.001 * 7.276533126831055
Epoch 470, val loss: 1.0508025884628296
Epoch 480, training loss: 0.04138526692986488 = 0.03408953920006752 + 0.001 * 7.295727252960205
Epoch 480, val loss: 1.0646157264709473
Epoch 490, training loss: 0.03839421644806862 = 0.031122323125600815 + 0.001 * 7.271892547607422
Epoch 490, val loss: 1.0780093669891357
Epoch 500, training loss: 0.035758063197135925 = 0.028495892882347107 + 0.001 * 7.262170314788818
Epoch 500, val loss: 1.0910142660140991
Epoch 510, training loss: 0.03341953083872795 = 0.026165364310145378 + 0.001 * 7.254167079925537
Epoch 510, val loss: 1.1035438776016235
Epoch 520, training loss: 0.03134582191705704 = 0.02409241534769535 + 0.001 * 7.253407001495361
Epoch 520, val loss: 1.1156556606292725
Epoch 530, training loss: 0.029505081474781036 = 0.022243408486247063 + 0.001 * 7.261672496795654
Epoch 530, val loss: 1.1273596286773682
Epoch 540, training loss: 0.02784324809908867 = 0.020589904859662056 + 0.001 * 7.2533440589904785
Epoch 540, val loss: 1.1386797428131104
Epoch 550, training loss: 0.02634035237133503 = 0.01910739578306675 + 0.001 * 7.232956886291504
Epoch 550, val loss: 1.1495742797851562
Epoch 560, training loss: 0.025009743869304657 = 0.017774688079953194 + 0.001 * 7.2350544929504395
Epoch 560, val loss: 1.1601136922836304
Epoch 570, training loss: 0.023797746747732162 = 0.016573501750826836 + 0.001 * 7.224244117736816
Epoch 570, val loss: 1.1702698469161987
Epoch 580, training loss: 0.02271968685090542 = 0.01548822596669197 + 0.001 * 7.231461048126221
Epoch 580, val loss: 1.1800718307495117
Epoch 590, training loss: 0.021726414561271667 = 0.014505245722830296 + 0.001 * 7.221169471740723
Epoch 590, val loss: 1.18952476978302
Epoch 600, training loss: 0.020826291292905807 = 0.013612574897706509 + 0.001 * 7.213716983795166
Epoch 600, val loss: 1.198649287223816
Epoch 610, training loss: 0.020024046301841736 = 0.012800092808902264 + 0.001 * 7.223952770233154
Epoch 610, val loss: 1.2074742317199707
Epoch 620, training loss: 0.01928955875337124 = 0.012058878317475319 + 0.001 * 7.230680465698242
Epoch 620, val loss: 1.2160035371780396
Epoch 630, training loss: 0.01858002506196499 = 0.011381265707314014 + 0.001 * 7.198758602142334
Epoch 630, val loss: 1.224249005317688
Epoch 640, training loss: 0.017963694408535957 = 0.01076031569391489 + 0.001 * 7.203378677368164
Epoch 640, val loss: 1.2322176694869995
Epoch 650, training loss: 0.017391115427017212 = 0.010190097615122795 + 0.001 * 7.201017379760742
Epoch 650, val loss: 1.2399367094039917
Epoch 660, training loss: 0.016881143674254417 = 0.009665361605584621 + 0.001 * 7.215781211853027
Epoch 660, val loss: 1.2473822832107544
Epoch 670, training loss: 0.01635434478521347 = 0.009181588888168335 + 0.001 * 7.172756195068359
Epoch 670, val loss: 1.2546074390411377
Epoch 680, training loss: 0.015910685062408447 = 0.008734768256545067 + 0.001 * 7.1759161949157715
Epoch 680, val loss: 1.261582374572754
Epoch 690, training loss: 0.015510772354900837 = 0.008321285247802734 + 0.001 * 7.189486980438232
Epoch 690, val loss: 1.2683472633361816
Epoch 700, training loss: 0.015094762668013573 = 0.007938014343380928 + 0.001 * 7.156748294830322
Epoch 700, val loss: 1.2748826742172241
Epoch 710, training loss: 0.014748616144061089 = 0.0075821285136044025 + 0.001 * 7.166487216949463
Epoch 710, val loss: 1.2812260389328003
Epoch 720, training loss: 0.014417607337236404 = 0.007251117844134569 + 0.001 * 7.1664886474609375
Epoch 720, val loss: 1.2873696088790894
Epoch 730, training loss: 0.014093202538788319 = 0.006942733656615019 + 0.001 * 7.150468349456787
Epoch 730, val loss: 1.293330192565918
Epoch 740, training loss: 0.013827752321958542 = 0.006654985249042511 + 0.001 * 7.172766208648682
Epoch 740, val loss: 1.2991231679916382
Epoch 750, training loss: 0.013520968146622181 = 0.006386103108525276 + 0.001 * 7.134864807128906
Epoch 750, val loss: 1.3047261238098145
Epoch 760, training loss: 0.013267207890748978 = 0.006134521681815386 + 0.001 * 7.132686138153076
Epoch 760, val loss: 1.3101671934127808
Epoch 770, training loss: 0.013026848435401917 = 0.0058987936936318874 + 0.001 * 7.128055095672607
Epoch 770, val loss: 1.315447211265564
Epoch 780, training loss: 0.01280318945646286 = 0.005677633453160524 + 0.001 * 7.125555038452148
Epoch 780, val loss: 1.3205816745758057
Epoch 790, training loss: 0.012598082423210144 = 0.00546985911205411 + 0.001 * 7.128222465515137
Epoch 790, val loss: 1.325573444366455
Epoch 800, training loss: 0.012402668595314026 = 0.005274464376270771 + 0.001 * 7.128203392028809
Epoch 800, val loss: 1.3304057121276855
Epoch 810, training loss: 0.012203142046928406 = 0.005090454593300819 + 0.001 * 7.112686634063721
Epoch 810, val loss: 1.3351079225540161
Epoch 820, training loss: 0.01204181183129549 = 0.004916985984891653 + 0.001 * 7.124825477600098
Epoch 820, val loss: 1.3396703004837036
Epoch 830, training loss: 0.011902492493391037 = 0.004753278102725744 + 0.001 * 7.149214744567871
Epoch 830, val loss: 1.3441214561462402
Epoch 840, training loss: 0.011722620576620102 = 0.004598602186888456 + 0.001 * 7.12401819229126
Epoch 840, val loss: 1.3484306335449219
Epoch 850, training loss: 0.011572463437914848 = 0.004452299326658249 + 0.001 * 7.120163440704346
Epoch 850, val loss: 1.3526268005371094
Epoch 860, training loss: 0.011418298818171024 = 0.004313793499022722 + 0.001 * 7.1045050621032715
Epoch 860, val loss: 1.3566954135894775
Epoch 870, training loss: 0.011292977258563042 = 0.004182527307420969 + 0.001 * 7.110450267791748
Epoch 870, val loss: 1.3606523275375366
Epoch 880, training loss: 0.011166408658027649 = 0.004057929385453463 + 0.001 * 7.108479022979736
Epoch 880, val loss: 1.3645228147506714
Epoch 890, training loss: 0.01109432615339756 = 0.003939742222428322 + 0.001 * 7.154582977294922
Epoch 890, val loss: 1.3682719469070435
Epoch 900, training loss: 0.010916702449321747 = 0.0038274466060101986 + 0.001 * 7.089254856109619
Epoch 900, val loss: 1.3719000816345215
Epoch 910, training loss: 0.010819283314049244 = 0.003720503533259034 + 0.001 * 7.098779201507568
Epoch 910, val loss: 1.3754535913467407
Epoch 920, training loss: 0.010730178095400333 = 0.0036189111415296793 + 0.001 * 7.111266613006592
Epoch 920, val loss: 1.3788903951644897
Epoch 930, training loss: 0.010624134913086891 = 0.0035219653509557247 + 0.001 * 7.102169513702393
Epoch 930, val loss: 1.3822457790374756
Epoch 940, training loss: 0.01054316759109497 = 0.0034296358935534954 + 0.001 * 7.113531589508057
Epoch 940, val loss: 1.3855069875717163
Epoch 950, training loss: 0.010463965125381947 = 0.0033415129873901606 + 0.001 * 7.122452259063721
Epoch 950, val loss: 1.3886915445327759
Epoch 960, training loss: 0.010347827337682247 = 0.003257377538830042 + 0.001 * 7.090449333190918
Epoch 960, val loss: 1.3917845487594604
Epoch 970, training loss: 0.010266639292240143 = 0.003176990896463394 + 0.001 * 7.089648723602295
Epoch 970, val loss: 1.3947917222976685
Epoch 980, training loss: 0.01018381118774414 = 0.003100218018516898 + 0.001 * 7.083592414855957
Epoch 980, val loss: 1.3977059125900269
Epoch 990, training loss: 0.010142389684915543 = 0.003026610007509589 + 0.001 * 7.115778923034668
Epoch 990, val loss: 1.4005672931671143
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 1.9488205909729004 = 1.9402236938476562 + 0.001 * 8.596839904785156
Epoch 0, val loss: 1.9409765005111694
Epoch 10, training loss: 1.9382539987564087 = 1.929657220840454 + 0.001 * 8.596787452697754
Epoch 10, val loss: 1.9301843643188477
Epoch 20, training loss: 1.925147294998169 = 1.916550636291504 + 0.001 * 8.596606254577637
Epoch 20, val loss: 1.9162681102752686
Epoch 30, training loss: 1.907097578048706 = 1.8985013961791992 + 0.001 * 8.596151351928711
Epoch 30, val loss: 1.8967366218566895
Epoch 40, training loss: 1.8816336393356323 = 1.8730387687683105 + 0.001 * 8.594913482666016
Epoch 40, val loss: 1.8693721294403076
Epoch 50, training loss: 1.84870445728302 = 1.840113878250122 + 0.001 * 8.59057331085205
Epoch 50, val loss: 1.8354005813598633
Epoch 60, training loss: 1.8159878253936768 = 1.8074157238006592 + 0.001 * 8.57211971282959
Epoch 60, val loss: 1.8051378726959229
Epoch 70, training loss: 1.7887628078460693 = 1.7802815437316895 + 0.001 * 8.481245040893555
Epoch 70, val loss: 1.7830573320388794
Epoch 80, training loss: 1.7515380382537842 = 1.7433258295059204 + 0.001 * 8.21223258972168
Epoch 80, val loss: 1.752768635749817
Epoch 90, training loss: 1.6980860233306885 = 1.6900427341461182 + 0.001 * 8.043343544006348
Epoch 90, val loss: 1.7086762189865112
Epoch 100, training loss: 1.6237635612487793 = 1.6159298419952393 + 0.001 * 7.833681106567383
Epoch 100, val loss: 1.64725923538208
Epoch 110, training loss: 1.5331162214279175 = 1.5254229307174683 + 0.001 * 7.693253993988037
Epoch 110, val loss: 1.5726515054702759
Epoch 120, training loss: 1.4355446100234985 = 1.4278628826141357 + 0.001 * 7.681771755218506
Epoch 120, val loss: 1.4937798976898193
Epoch 130, training loss: 1.3352344036102295 = 1.3275924921035767 + 0.001 * 7.641869068145752
Epoch 130, val loss: 1.4157558679580688
Epoch 140, training loss: 1.2334229946136475 = 1.2258306741714478 + 0.001 * 7.592316627502441
Epoch 140, val loss: 1.3395283222198486
Epoch 150, training loss: 1.132420301437378 = 1.1248979568481445 + 0.001 * 7.522384166717529
Epoch 150, val loss: 1.266126036643982
Epoch 160, training loss: 1.035286545753479 = 1.0278288125991821 + 0.001 * 7.45772647857666
Epoch 160, val loss: 1.1967051029205322
Epoch 170, training loss: 0.9442770481109619 = 0.936872661113739 + 0.001 * 7.404377460479736
Epoch 170, val loss: 1.1319745779037476
Epoch 180, training loss: 0.8614413738250732 = 0.8540753722190857 + 0.001 * 7.366025924682617
Epoch 180, val loss: 1.0737873315811157
Epoch 190, training loss: 0.7883865237236023 = 0.7810371518135071 + 0.001 * 7.349395751953125
Epoch 190, val loss: 1.023772954940796
Epoch 200, training loss: 0.725124180316925 = 0.7177847027778625 + 0.001 * 7.3394598960876465
Epoch 200, val loss: 0.9830438494682312
Epoch 210, training loss: 0.6697965860366821 = 0.662472665309906 + 0.001 * 7.323950290679932
Epoch 210, val loss: 0.9511067867279053
Epoch 220, training loss: 0.6195468306541443 = 0.6122362017631531 + 0.001 * 7.310621738433838
Epoch 220, val loss: 0.9264965057373047
Epoch 230, training loss: 0.5718086361885071 = 0.5645124912261963 + 0.001 * 7.296165943145752
Epoch 230, val loss: 0.9069952964782715
Epoch 240, training loss: 0.5252709984779358 = 0.5179949998855591 + 0.001 * 7.276017189025879
Epoch 240, val loss: 0.8908729553222656
Epoch 250, training loss: 0.479739785194397 = 0.4724745750427246 + 0.001 * 7.265219688415527
Epoch 250, val loss: 0.8777843713760376
Epoch 260, training loss: 0.4357900619506836 = 0.42853185534477234 + 0.001 * 7.258212089538574
Epoch 260, val loss: 0.8685131072998047
Epoch 270, training loss: 0.3943454623222351 = 0.38708940148353577 + 0.001 * 7.256065368652344
Epoch 270, val loss: 0.8640481233596802
Epoch 280, training loss: 0.3561744689941406 = 0.3489202857017517 + 0.001 * 7.254192352294922
Epoch 280, val loss: 0.8648831248283386
Epoch 290, training loss: 0.32145726680755615 = 0.3142063021659851 + 0.001 * 7.250950336456299
Epoch 290, val loss: 0.8707140684127808
Epoch 300, training loss: 0.2896740138530731 = 0.2824266850948334 + 0.001 * 7.247333526611328
Epoch 300, val loss: 0.8805380463600159
Epoch 310, training loss: 0.26015904545783997 = 0.25291118025779724 + 0.001 * 7.247857570648193
Epoch 310, val loss: 0.89356529712677
Epoch 320, training loss: 0.2326057106256485 = 0.22535519301891327 + 0.001 * 7.250516414642334
Epoch 320, val loss: 0.909157931804657
Epoch 330, training loss: 0.20718152821063995 = 0.19993223249912262 + 0.001 * 7.249298095703125
Epoch 330, val loss: 0.926752507686615
Epoch 340, training loss: 0.18421481549739838 = 0.1769665628671646 + 0.001 * 7.248254299163818
Epoch 340, val loss: 0.9456984996795654
Epoch 350, training loss: 0.16385136544704437 = 0.15660357475280762 + 0.001 * 7.2477850914001465
Epoch 350, val loss: 0.965665340423584
Epoch 360, training loss: 0.14594440162181854 = 0.13869868218898773 + 0.001 * 7.245712757110596
Epoch 360, val loss: 0.9864466786384583
Epoch 370, training loss: 0.13025450706481934 = 0.12299279123544693 + 0.001 * 7.261710166931152
Epoch 370, val loss: 1.0078978538513184
Epoch 380, training loss: 0.11648107320070267 = 0.10922951996326447 + 0.001 * 7.2515549659729
Epoch 380, val loss: 1.0298596620559692
Epoch 390, training loss: 0.10441267490386963 = 0.09716538339853287 + 0.001 * 7.247289657592773
Epoch 390, val loss: 1.0524771213531494
Epoch 400, training loss: 0.09383582323789597 = 0.0865892693400383 + 0.001 * 7.246551513671875
Epoch 400, val loss: 1.0755620002746582
Epoch 410, training loss: 0.08455871790647507 = 0.07731214165687561 + 0.001 * 7.246575355529785
Epoch 410, val loss: 1.0990467071533203
Epoch 420, training loss: 0.07641936093568802 = 0.06916297972202301 + 0.001 * 7.256380081176758
Epoch 420, val loss: 1.1227434873580933
Epoch 430, training loss: 0.06925585120916367 = 0.06200821325182915 + 0.001 * 7.247635841369629
Epoch 430, val loss: 1.146436095237732
Epoch 440, training loss: 0.06296161562204361 = 0.05571598932147026 + 0.001 * 7.245626926422119
Epoch 440, val loss: 1.1700464487075806
Epoch 450, training loss: 0.057419318705797195 = 0.05017279088497162 + 0.001 * 7.24652624130249
Epoch 450, val loss: 1.1935099363327026
Epoch 460, training loss: 0.052526429295539856 = 0.04528248682618141 + 0.001 * 7.24394416809082
Epoch 460, val loss: 1.2166328430175781
Epoch 470, training loss: 0.04821416735649109 = 0.04096580669283867 + 0.001 * 7.248359203338623
Epoch 470, val loss: 1.239296317100525
Epoch 480, training loss: 0.04440215229988098 = 0.03715571388602257 + 0.001 * 7.246438503265381
Epoch 480, val loss: 1.261414647102356
Epoch 490, training loss: 0.041033513844013214 = 0.03378888592123985 + 0.001 * 7.244626522064209
Epoch 490, val loss: 1.2829879522323608
Epoch 500, training loss: 0.03805457800626755 = 0.03081062249839306 + 0.001 * 7.243955612182617
Epoch 500, val loss: 1.3039203882217407
Epoch 510, training loss: 0.035413291305303574 = 0.028174180537462234 + 0.001 * 7.239109992980957
Epoch 510, val loss: 1.3241713047027588
Epoch 520, training loss: 0.033085744827985764 = 0.025834543630480766 + 0.001 * 7.251201629638672
Epoch 520, val loss: 1.3437542915344238
Epoch 530, training loss: 0.030987393110990524 = 0.023749837651848793 + 0.001 * 7.237555503845215
Epoch 530, val loss: 1.3626855611801147
Epoch 540, training loss: 0.029126577079296112 = 0.021886173635721207 + 0.001 * 7.2404022216796875
Epoch 540, val loss: 1.3809894323349
Epoch 550, training loss: 0.027449559420347214 = 0.02021542564034462 + 0.001 * 7.234132289886475
Epoch 550, val loss: 1.3987095355987549
Epoch 560, training loss: 0.025949830189347267 = 0.01871437020599842 + 0.001 * 7.235459804534912
Epoch 560, val loss: 1.4158124923706055
Epoch 570, training loss: 0.024596422910690308 = 0.01736351288855076 + 0.001 * 7.232908725738525
Epoch 570, val loss: 1.4323556423187256
Epoch 580, training loss: 0.023376917466521263 = 0.016145652160048485 + 0.001 * 7.231265544891357
Epoch 580, val loss: 1.448304295539856
Epoch 590, training loss: 0.022275928407907486 = 0.015045546926558018 + 0.001 * 7.230380058288574
Epoch 590, val loss: 1.4636789560317993
Epoch 600, training loss: 0.021277369931340218 = 0.014049921184778214 + 0.001 * 7.2274489402771
Epoch 600, val loss: 1.4785422086715698
Epoch 610, training loss: 0.020378123968839645 = 0.013147108256816864 + 0.001 * 7.231016159057617
Epoch 610, val loss: 1.492933988571167
Epoch 620, training loss: 0.01955273002386093 = 0.012326763942837715 + 0.001 * 7.225966453552246
Epoch 620, val loss: 1.5068762302398682
Epoch 630, training loss: 0.01879896968603134 = 0.011579923331737518 + 0.001 * 7.2190470695495605
Epoch 630, val loss: 1.5203660726547241
Epoch 640, training loss: 0.018116619437932968 = 0.010898880660533905 + 0.001 * 7.217739582061768
Epoch 640, val loss: 1.5334136486053467
Epoch 650, training loss: 0.01750018075108528 = 0.010276309214532375 + 0.001 * 7.223872184753418
Epoch 650, val loss: 1.5460665225982666
Epoch 660, training loss: 0.016924269497394562 = 0.00970609299838543 + 0.001 * 7.218175888061523
Epoch 660, val loss: 1.558337688446045
Epoch 670, training loss: 0.01639428548514843 = 0.00918260682374239 + 0.001 * 7.211678981781006
Epoch 670, val loss: 1.5702379941940308
Epoch 680, training loss: 0.015918374061584473 = 0.008701047860085964 + 0.001 * 7.2173261642456055
Epoch 680, val loss: 1.581792950630188
Epoch 690, training loss: 0.015471434220671654 = 0.008257399313151836 + 0.001 * 7.214034557342529
Epoch 690, val loss: 1.5929875373840332
Epoch 700, training loss: 0.015062261372804642 = 0.007848074659705162 + 0.001 * 7.21418571472168
Epoch 700, val loss: 1.603869915008545
Epoch 710, training loss: 0.014677321538329124 = 0.007469664793461561 + 0.001 * 7.207655906677246
Epoch 710, val loss: 1.6144286394119263
Epoch 720, training loss: 0.014331155456602573 = 0.007119223941117525 + 0.001 * 7.211931228637695
Epoch 720, val loss: 1.6246929168701172
Epoch 730, training loss: 0.013992620632052422 = 0.00679409597069025 + 0.001 * 7.198523998260498
Epoch 730, val loss: 1.634702205657959
Epoch 740, training loss: 0.013698825612664223 = 0.006491908337920904 + 0.001 * 7.2069172859191895
Epoch 740, val loss: 1.6444350481033325
Epoch 750, training loss: 0.013406923040747643 = 0.006210696883499622 + 0.001 * 7.196225643157959
Epoch 750, val loss: 1.65388822555542
Epoch 760, training loss: 0.013137646950781345 = 0.0059484983794391155 + 0.001 * 7.189148426055908
Epoch 760, val loss: 1.663103699684143
Epoch 770, training loss: 0.01290055550634861 = 0.005703685339540243 + 0.001 * 7.19687032699585
Epoch 770, val loss: 1.6720759868621826
Epoch 780, training loss: 0.012677926570177078 = 0.005474780220538378 + 0.001 * 7.203146457672119
Epoch 780, val loss: 1.6807948350906372
Epoch 790, training loss: 0.012447990477085114 = 0.005260455422103405 + 0.001 * 7.187534332275391
Epoch 790, val loss: 1.6893037557601929
Epoch 800, training loss: 0.01224279310554266 = 0.005059550516307354 + 0.001 * 7.183242321014404
Epoch 800, val loss: 1.6975864171981812
Epoch 810, training loss: 0.012060798704624176 = 0.004870965611189604 + 0.001 * 7.189833164215088
Epoch 810, val loss: 1.7056561708450317
Epoch 820, training loss: 0.01189468801021576 = 0.004693710710853338 + 0.001 * 7.200976371765137
Epoch 820, val loss: 1.7135145664215088
Epoch 830, training loss: 0.011701056733727455 = 0.004526899661868811 + 0.001 * 7.174156665802002
Epoch 830, val loss: 1.7212055921554565
Epoch 840, training loss: 0.011551637202501297 = 0.004369760863482952 + 0.001 * 7.181876182556152
Epoch 840, val loss: 1.7286782264709473
Epoch 850, training loss: 0.011416979134082794 = 0.004221622366458178 + 0.001 * 7.195356369018555
Epoch 850, val loss: 1.7359704971313477
Epoch 860, training loss: 0.011254582554101944 = 0.004081736318767071 + 0.001 * 7.17284631729126
Epoch 860, val loss: 1.7430912256240845
Epoch 870, training loss: 0.01112244464457035 = 0.0039495131932199 + 0.001 * 7.172930717468262
Epoch 870, val loss: 1.7500616312026978
Epoch 880, training loss: 0.011024638079106808 = 0.0038243746384978294 + 0.001 * 7.200263023376465
Epoch 880, val loss: 1.7568583488464355
Epoch 890, training loss: 0.010888882912695408 = 0.0037059022579342127 + 0.001 * 7.182980060577393
Epoch 890, val loss: 1.7634812593460083
Epoch 900, training loss: 0.010742184706032276 = 0.003593633184209466 + 0.001 * 7.1485514640808105
Epoch 900, val loss: 1.769984245300293
Epoch 910, training loss: 0.01064094714820385 = 0.0034870775416493416 + 0.001 * 7.15386962890625
Epoch 910, val loss: 1.7763152122497559
Epoch 920, training loss: 0.010554114356637001 = 0.0033858665265142918 + 0.001 * 7.168247222900391
Epoch 920, val loss: 1.7825112342834473
Epoch 930, training loss: 0.010459525510668755 = 0.0032897102646529675 + 0.001 * 7.169815540313721
Epoch 930, val loss: 1.7885783910751343
Epoch 940, training loss: 0.01035669632256031 = 0.0031982080545276403 + 0.001 * 7.158487796783447
Epoch 940, val loss: 1.7944846153259277
Epoch 950, training loss: 0.010267173871397972 = 0.003111089114099741 + 0.001 * 7.1560845375061035
Epoch 950, val loss: 1.8002535104751587
Epoch 960, training loss: 0.010191923007369041 = 0.0030280831269919872 + 0.001 * 7.163839340209961
Epoch 960, val loss: 1.8059076070785522
Epoch 970, training loss: 0.010108415968716145 = 0.002948922337964177 + 0.001 * 7.159493446350098
Epoch 970, val loss: 1.811481237411499
Epoch 980, training loss: 0.010011320933699608 = 0.0028732800856232643 + 0.001 * 7.138040542602539
Epoch 980, val loss: 1.8168833255767822
Epoch 990, training loss: 0.009980362839996815 = 0.0028008983936160803 + 0.001 * 7.179463863372803
Epoch 990, val loss: 1.8222391605377197
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 1.9535080194473267 = 1.9449111223220825 + 0.001 * 8.596871376037598
Epoch 0, val loss: 1.9460721015930176
Epoch 10, training loss: 1.9439775943756104 = 1.9353808164596558 + 0.001 * 8.596837043762207
Epoch 10, val loss: 1.9373201131820679
Epoch 20, training loss: 1.9321409463882446 = 1.9235442876815796 + 0.001 * 8.596701622009277
Epoch 20, val loss: 1.9261964559555054
Epoch 30, training loss: 1.9155194759368896 = 1.9069230556488037 + 0.001 * 8.59638500213623
Epoch 30, val loss: 1.910447597503662
Epoch 40, training loss: 1.8912404775619507 = 1.8826448917388916 + 0.001 * 8.595585823059082
Epoch 40, val loss: 1.8878141641616821
Epoch 50, training loss: 1.8582537174224854 = 1.8496603965759277 + 0.001 * 8.593334197998047
Epoch 50, val loss: 1.8586174249649048
Epoch 60, training loss: 1.823136568069458 = 1.8145506381988525 + 0.001 * 8.585914611816406
Epoch 60, val loss: 1.8302029371261597
Epoch 70, training loss: 1.792463779449463 = 1.783907175064087 + 0.001 * 8.556612014770508
Epoch 70, val loss: 1.803776502609253
Epoch 80, training loss: 1.7538108825683594 = 1.7454301118850708 + 0.001 * 8.380806922912598
Epoch 80, val loss: 1.7663191556930542
Epoch 90, training loss: 1.6999990940093994 = 1.6918436288833618 + 0.001 * 8.155522346496582
Epoch 90, val loss: 1.7181373834609985
Epoch 100, training loss: 1.626141905784607 = 1.6180651187896729 + 0.001 * 8.076753616333008
Epoch 100, val loss: 1.6557543277740479
Epoch 110, training loss: 1.537743330001831 = 1.5297504663467407 + 0.001 * 7.992891311645508
Epoch 110, val loss: 1.582846760749817
Epoch 120, training loss: 1.4459397792816162 = 1.4380955696105957 + 0.001 * 7.844221591949463
Epoch 120, val loss: 1.5107797384262085
Epoch 130, training loss: 1.3550643920898438 = 1.34732186794281 + 0.001 * 7.742480754852295
Epoch 130, val loss: 1.4417564868927002
Epoch 140, training loss: 1.2645467519760132 = 1.256822943687439 + 0.001 * 7.723828315734863
Epoch 140, val loss: 1.3745641708374023
Epoch 150, training loss: 1.1763559579849243 = 1.1686748266220093 + 0.001 * 7.681105136871338
Epoch 150, val loss: 1.3104193210601807
Epoch 160, training loss: 1.0948710441589355 = 1.087250828742981 + 0.001 * 7.620266437530518
Epoch 160, val loss: 1.2537660598754883
Epoch 170, training loss: 1.0223989486694336 = 1.0148496627807617 + 0.001 * 7.549290180206299
Epoch 170, val loss: 1.206910252571106
Epoch 180, training loss: 0.957561731338501 = 0.9500924348831177 + 0.001 * 7.4692840576171875
Epoch 180, val loss: 1.1683974266052246
Epoch 190, training loss: 0.8968268036842346 = 0.8893911242485046 + 0.001 * 7.435672283172607
Epoch 190, val loss: 1.1340887546539307
Epoch 200, training loss: 0.8363181948661804 = 0.8288894891738892 + 0.001 * 7.428705215454102
Epoch 200, val loss: 1.100335955619812
Epoch 210, training loss: 0.7734795808792114 = 0.7660561800003052 + 0.001 * 7.423402309417725
Epoch 210, val loss: 1.0651615858078003
Epoch 220, training loss: 0.7079534530639648 = 0.7005355358123779 + 0.001 * 7.417909622192383
Epoch 220, val loss: 1.0284976959228516
Epoch 230, training loss: 0.6415941715240479 = 0.634181022644043 + 0.001 * 7.413122653961182
Epoch 230, val loss: 0.9926441311836243
Epoch 240, training loss: 0.5768957138061523 = 0.5694867372512817 + 0.001 * 7.4089789390563965
Epoch 240, val loss: 0.9609284400939941
Epoch 250, training loss: 0.5157390832901001 = 0.5083327889442444 + 0.001 * 7.4063191413879395
Epoch 250, val loss: 0.9354327321052551
Epoch 260, training loss: 0.45903295278549194 = 0.4516298174858093 + 0.001 * 7.403120994567871
Epoch 260, val loss: 0.9169557094573975
Epoch 270, training loss: 0.4069517254829407 = 0.39955174922943115 + 0.001 * 7.399970531463623
Epoch 270, val loss: 0.9047322273254395
Epoch 280, training loss: 0.359242707490921 = 0.35184353590011597 + 0.001 * 7.399177551269531
Epoch 280, val loss: 0.8982285857200623
Epoch 290, training loss: 0.31574007868766785 = 0.3083452582359314 + 0.001 * 7.394834041595459
Epoch 290, val loss: 0.8965141177177429
Epoch 300, training loss: 0.2765456736087799 = 0.26915663480758667 + 0.001 * 7.3890252113342285
Epoch 300, val loss: 0.8995431661605835
Epoch 310, training loss: 0.2417328655719757 = 0.23435218632221222 + 0.001 * 7.380671501159668
Epoch 310, val loss: 0.9067595601081848
Epoch 320, training loss: 0.21114671230316162 = 0.20376665890216827 + 0.001 * 7.38004732131958
Epoch 320, val loss: 0.9177641272544861
Epoch 330, training loss: 0.1844346821308136 = 0.1770704835653305 + 0.001 * 7.364195823669434
Epoch 330, val loss: 0.931892454624176
Epoch 340, training loss: 0.16122695803642273 = 0.15387679636478424 + 0.001 * 7.35015869140625
Epoch 340, val loss: 0.9487003087997437
Epoch 350, training loss: 0.14116692543029785 = 0.13381217420101166 + 0.001 * 7.354751110076904
Epoch 350, val loss: 0.9674025774002075
Epoch 360, training loss: 0.12389499694108963 = 0.11656109988689423 + 0.001 * 7.333894729614258
Epoch 360, val loss: 0.9875127673149109
Epoch 370, training loss: 0.10913728177547455 = 0.10181305557489395 + 0.001 * 7.324227333068848
Epoch 370, val loss: 1.0084969997406006
Epoch 380, training loss: 0.09657173603773117 = 0.08924275636672974 + 0.001 * 7.328980922698975
Epoch 380, val loss: 1.0298004150390625
Epoch 390, training loss: 0.08584292978048325 = 0.0785318985581398 + 0.001 * 7.311033725738525
Epoch 390, val loss: 1.0509839057922363
Epoch 400, training loss: 0.07669907808303833 = 0.06938757002353668 + 0.001 * 7.311511039733887
Epoch 400, val loss: 1.0718204975128174
Epoch 410, training loss: 0.0688658133149147 = 0.06155501306056976 + 0.001 * 7.3108015060424805
Epoch 410, val loss: 1.092121958732605
Epoch 420, training loss: 0.06212921813130379 = 0.054822761565446854 + 0.001 * 7.30645751953125
Epoch 420, val loss: 1.111867904663086
Epoch 430, training loss: 0.05631933733820915 = 0.049014829099178314 + 0.001 * 7.304507732391357
Epoch 430, val loss: 1.1310253143310547
Epoch 440, training loss: 0.05129029601812363 = 0.043987177312374115 + 0.001 * 7.30311918258667
Epoch 440, val loss: 1.149586796760559
Epoch 450, training loss: 0.046923816204071045 = 0.03962072730064392 + 0.001 * 7.3030877113342285
Epoch 450, val loss: 1.167515516281128
Epoch 460, training loss: 0.04312077909708023 = 0.035817600786685944 + 0.001 * 7.303179740905762
Epoch 460, val loss: 1.1848541498184204
Epoch 470, training loss: 0.03979785740375519 = 0.03249426558613777 + 0.001 * 7.303591251373291
Epoch 470, val loss: 1.201547384262085
Epoch 480, training loss: 0.0368875190615654 = 0.029581250622868538 + 0.001 * 7.306269645690918
Epoch 480, val loss: 1.2176644802093506
Epoch 490, training loss: 0.03431938961148262 = 0.0270196795463562 + 0.001 * 7.299709320068359
Epoch 490, val loss: 1.233197569847107
Epoch 500, training loss: 0.03206397593021393 = 0.02476048469543457 + 0.001 * 7.303490161895752
Epoch 500, val loss: 1.2481800317764282
Epoch 510, training loss: 0.030058909207582474 = 0.022761447355151176 + 0.001 * 7.297461986541748
Epoch 510, val loss: 1.2626349925994873
Epoch 520, training loss: 0.02828640304505825 = 0.02098694071173668 + 0.001 * 7.299461364746094
Epoch 520, val loss: 1.2766085863113403
Epoch 530, training loss: 0.026704007759690285 = 0.019406884908676147 + 0.001 * 7.297122955322266
Epoch 530, val loss: 1.2900421619415283
Epoch 540, training loss: 0.025296898558735847 = 0.017995819449424744 + 0.001 * 7.3010783195495605
Epoch 540, val loss: 1.3030052185058594
Epoch 550, training loss: 0.024025533348321915 = 0.01673184148967266 + 0.001 * 7.293691158294678
Epoch 550, val loss: 1.3155417442321777
Epoch 560, training loss: 0.022887270897626877 = 0.015596132725477219 + 0.001 * 7.291136741638184
Epoch 560, val loss: 1.3276338577270508
Epoch 570, training loss: 0.021863561123609543 = 0.01457258965820074 + 0.001 * 7.2909722328186035
Epoch 570, val loss: 1.3393194675445557
Epoch 580, training loss: 0.02093566209077835 = 0.013647510670125484 + 0.001 * 7.288151264190674
Epoch 580, val loss: 1.3506193161010742
Epoch 590, training loss: 0.020094826817512512 = 0.01280907541513443 + 0.001 * 7.285750389099121
Epoch 590, val loss: 1.361549973487854
Epoch 600, training loss: 0.0193344596773386 = 0.01204712875187397 + 0.001 * 7.287330150604248
Epoch 600, val loss: 1.3721362352371216
Epoch 610, training loss: 0.018636789172887802 = 0.011352854780852795 + 0.001 * 7.283934116363525
Epoch 610, val loss: 1.3823884725570679
Epoch 620, training loss: 0.018006039783358574 = 0.01071856077760458 + 0.001 * 7.287478446960449
Epoch 620, val loss: 1.3923231363296509
Epoch 630, training loss: 0.01741909235715866 = 0.01013793982565403 + 0.001 * 7.28115177154541
Epoch 630, val loss: 1.4019393920898438
Epoch 640, training loss: 0.016898956149816513 = 0.009605126455426216 + 0.001 * 7.293829917907715
Epoch 640, val loss: 1.4112643003463745
Epoch 650, training loss: 0.01638799160718918 = 0.009115127846598625 + 0.001 * 7.272863388061523
Epoch 650, val loss: 1.4203299283981323
Epoch 660, training loss: 0.015942934900522232 = 0.008663563057780266 + 0.001 * 7.279371738433838
Epoch 660, val loss: 1.4291108846664429
Epoch 670, training loss: 0.015525653958320618 = 0.008246582932770252 + 0.001 * 7.2790703773498535
Epoch 670, val loss: 1.4376314878463745
Epoch 680, training loss: 0.015130803920328617 = 0.007860744372010231 + 0.001 * 7.270059108734131
Epoch 680, val loss: 1.4459044933319092
Epoch 690, training loss: 0.014779521152377129 = 0.007503054570406675 + 0.001 * 7.276466369628906
Epoch 690, val loss: 1.4539440870285034
Epoch 700, training loss: 0.014437082223594189 = 0.0071709249168634415 + 0.001 * 7.266157150268555
Epoch 700, val loss: 1.4617619514465332
Epoch 710, training loss: 0.01412426121532917 = 0.006861919537186623 + 0.001 * 7.2623419761657715
Epoch 710, val loss: 1.4693567752838135
Epoch 720, training loss: 0.013839222490787506 = 0.0065737515687942505 + 0.001 * 7.265470504760742
Epoch 720, val loss: 1.4767519235610962
Epoch 730, training loss: 0.013557584956288338 = 0.006304412614554167 + 0.001 * 7.253171920776367
Epoch 730, val loss: 1.483960747718811
Epoch 740, training loss: 0.013306992128491402 = 0.006051880773156881 + 0.001 * 7.255111217498779
Epoch 740, val loss: 1.4909803867340088
Epoch 750, training loss: 0.013063708320260048 = 0.005814285948872566 + 0.001 * 7.2494215965271
Epoch 750, val loss: 1.497799277305603
Epoch 760, training loss: 0.012841464951634407 = 0.005590307526290417 + 0.001 * 7.251157283782959
Epoch 760, val loss: 1.5044623613357544
Epoch 770, training loss: 0.012618841603398323 = 0.00537846377119422 + 0.001 * 7.240376949310303
Epoch 770, val loss: 1.5109494924545288
Epoch 780, training loss: 0.01241675391793251 = 0.005177765153348446 + 0.001 * 7.238987922668457
Epoch 780, val loss: 1.5172621011734009
Epoch 790, training loss: 0.012232623994350433 = 0.00498774042353034 + 0.001 * 7.2448835372924805
Epoch 790, val loss: 1.5234204530715942
Epoch 800, training loss: 0.012043459340929985 = 0.004807612858712673 + 0.001 * 7.235846042633057
Epoch 800, val loss: 1.5294270515441895
Epoch 810, training loss: 0.011889107525348663 = 0.004637033212929964 + 0.001 * 7.252074241638184
Epoch 810, val loss: 1.5352839231491089
Epoch 820, training loss: 0.011714944615960121 = 0.004475270863622427 + 0.001 * 7.239673137664795
Epoch 820, val loss: 1.540987491607666
Epoch 830, training loss: 0.011575196869671345 = 0.004321756307035685 + 0.001 * 7.2534403800964355
Epoch 830, val loss: 1.5465235710144043
Epoch 840, training loss: 0.01140938326716423 = 0.004175998270511627 + 0.001 * 7.23338508605957
Epoch 840, val loss: 1.5519870519638062
Epoch 850, training loss: 0.011288595385849476 = 0.0040377224795520306 + 0.001 * 7.250872611999512
Epoch 850, val loss: 1.5572867393493652
Epoch 860, training loss: 0.011144882999360561 = 0.003906508442014456 + 0.001 * 7.23837423324585
Epoch 860, val loss: 1.5624756813049316
Epoch 870, training loss: 0.011022092774510384 = 0.00378201506100595 + 0.001 * 7.240077018737793
Epoch 870, val loss: 1.5675177574157715
Epoch 880, training loss: 0.010884616523981094 = 0.0036637962330132723 + 0.001 * 7.22081995010376
Epoch 880, val loss: 1.572440266609192
Epoch 890, training loss: 0.0107892956584692 = 0.0035515453200787306 + 0.001 * 7.2377495765686035
Epoch 890, val loss: 1.5772953033447266
Epoch 900, training loss: 0.010656067170202732 = 0.0034447733778506517 + 0.001 * 7.211293697357178
Epoch 900, val loss: 1.5819987058639526
Epoch 910, training loss: 0.010542978532612324 = 0.0033432466443628073 + 0.001 * 7.199731349945068
Epoch 910, val loss: 1.5865788459777832
Epoch 920, training loss: 0.010434507392346859 = 0.0032467395067214966 + 0.001 * 7.187767505645752
Epoch 920, val loss: 1.5910621881484985
Epoch 930, training loss: 0.01042824424803257 = 0.003154955804347992 + 0.001 * 7.273288726806641
Epoch 930, val loss: 1.5954676866531372
Epoch 940, training loss: 0.01027323491871357 = 0.0030675833113491535 + 0.001 * 7.20565128326416
Epoch 940, val loss: 1.5997333526611328
Epoch 950, training loss: 0.010207942686975002 = 0.0029843284282833338 + 0.001 * 7.223613739013672
Epoch 950, val loss: 1.6039433479309082
Epoch 960, training loss: 0.010114441625773907 = 0.0029050081502646208 + 0.001 * 7.209433078765869
Epoch 960, val loss: 1.6080104112625122
Epoch 970, training loss: 0.010038869455456734 = 0.0028292767237871885 + 0.001 * 7.209592819213867
Epoch 970, val loss: 1.6120234727859497
Epoch 980, training loss: 0.009937090799212456 = 0.002756956499069929 + 0.001 * 7.180134296417236
Epoch 980, val loss: 1.6159121990203857
Epoch 990, training loss: 0.009872240014374256 = 0.002687930827960372 + 0.001 * 7.1843085289001465
Epoch 990, val loss: 1.6197028160095215
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8118081180811808
The final CL Acc:0.75679, 0.01944, The final GNN Acc:0.81269, 0.00163
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13292])
remove edge: torch.Size([2, 7876])
updated graph: torch.Size([2, 10612])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9698293209075928 = 1.9612324237823486 + 0.001 * 8.596841812133789
Epoch 0, val loss: 1.9610835313796997
Epoch 10, training loss: 1.9585806131362915 = 1.949983835220337 + 0.001 * 8.596803665161133
Epoch 10, val loss: 1.9501347541809082
Epoch 20, training loss: 1.9449504613876343 = 1.9363538026809692 + 0.001 * 8.596659660339355
Epoch 20, val loss: 1.9365178346633911
Epoch 30, training loss: 1.9261548519134521 = 1.9175585508346558 + 0.001 * 8.596331596374512
Epoch 30, val loss: 1.9176045656204224
Epoch 40, training loss: 1.8988107442855835 = 1.890215277671814 + 0.001 * 8.595490455627441
Epoch 40, val loss: 1.890349268913269
Epoch 50, training loss: 1.8606857061386108 = 1.8520927429199219 + 0.001 * 8.593012809753418
Epoch 50, val loss: 1.8538739681243896
Epoch 60, training loss: 1.8170944452285767 = 1.8085105419158936 + 0.001 * 8.583946228027344
Epoch 60, val loss: 1.815761685371399
Epoch 70, training loss: 1.7788066864013672 = 1.7702620029449463 + 0.001 * 8.544679641723633
Epoch 70, val loss: 1.7845239639282227
Epoch 80, training loss: 1.7354296445846558 = 1.727110505104065 + 0.001 * 8.319127082824707
Epoch 80, val loss: 1.744598388671875
Epoch 90, training loss: 1.6766489744186401 = 1.6685197353363037 + 0.001 * 8.129257202148438
Epoch 90, val loss: 1.6901519298553467
Epoch 100, training loss: 1.5978922843933105 = 1.5898512601852417 + 0.001 * 8.041064262390137
Epoch 100, val loss: 1.6213253736495972
Epoch 110, training loss: 1.502989411354065 = 1.4950329065322876 + 0.001 * 7.956511974334717
Epoch 110, val loss: 1.541049599647522
Epoch 120, training loss: 1.4032864570617676 = 1.3954908847808838 + 0.001 * 7.795543193817139
Epoch 120, val loss: 1.459991216659546
Epoch 130, training loss: 1.3056102991104126 = 1.297973871231079 + 0.001 * 7.6363935470581055
Epoch 130, val loss: 1.3812979459762573
Epoch 140, training loss: 1.2123419046401978 = 1.204721450805664 + 0.001 * 7.620469093322754
Epoch 140, val loss: 1.3079392910003662
Epoch 150, training loss: 1.1234045028686523 = 1.1158289909362793 + 0.001 * 7.575470447540283
Epoch 150, val loss: 1.2382256984710693
Epoch 160, training loss: 1.0391343832015991 = 1.0315965414047241 + 0.001 * 7.537845611572266
Epoch 160, val loss: 1.1722275018692017
Epoch 170, training loss: 0.9597881436347961 = 0.9522873163223267 + 0.001 * 7.500802516937256
Epoch 170, val loss: 1.1092603206634521
Epoch 180, training loss: 0.8851856589317322 = 0.8777298331260681 + 0.001 * 7.455852031707764
Epoch 180, val loss: 1.0493637323379517
Epoch 190, training loss: 0.8145622611045837 = 0.8071591854095459 + 0.001 * 7.4030866622924805
Epoch 190, val loss: 0.9924811720848083
Epoch 200, training loss: 0.7474241852760315 = 0.7400728464126587 + 0.001 * 7.3513641357421875
Epoch 200, val loss: 0.9395591020584106
Epoch 210, training loss: 0.683881402015686 = 0.6765786409378052 + 0.001 * 7.302753925323486
Epoch 210, val loss: 0.8928964734077454
Epoch 220, training loss: 0.6246591210365295 = 0.6173852682113647 + 0.001 * 7.273849010467529
Epoch 220, val loss: 0.8542592525482178
Epoch 230, training loss: 0.5702250599861145 = 0.5629633069038391 + 0.001 * 7.2617316246032715
Epoch 230, val loss: 0.8242698311805725
Epoch 240, training loss: 0.5204588770866394 = 0.5132004618644714 + 0.001 * 7.2584309577941895
Epoch 240, val loss: 0.8022550344467163
Epoch 250, training loss: 0.4747617542743683 = 0.4675035774707794 + 0.001 * 7.258184432983398
Epoch 250, val loss: 0.7868167161941528
Epoch 260, training loss: 0.4323770999908447 = 0.425118625164032 + 0.001 * 7.258471488952637
Epoch 260, val loss: 0.7764418721199036
Epoch 270, training loss: 0.3928573727607727 = 0.38559913635253906 + 0.001 * 7.258228302001953
Epoch 270, val loss: 0.770240306854248
Epoch 280, training loss: 0.3560844957828522 = 0.34882697463035583 + 0.001 * 7.257531642913818
Epoch 280, val loss: 0.7680790424346924
Epoch 290, training loss: 0.3219393491744995 = 0.31468284130096436 + 0.001 * 7.25649881362915
Epoch 290, val loss: 0.7698479294776917
Epoch 300, training loss: 0.29025208950042725 = 0.2829968333244324 + 0.001 * 7.255245685577393
Epoch 300, val loss: 0.7750266194343567
Epoch 310, training loss: 0.26087337732315063 = 0.253619521856308 + 0.001 * 7.253847122192383
Epoch 310, val loss: 0.7829515933990479
Epoch 320, training loss: 0.233663409948349 = 0.22640985250473022 + 0.001 * 7.253564357757568
Epoch 320, val loss: 0.7932593822479248
Epoch 330, training loss: 0.20855188369750977 = 0.2013007402420044 + 0.001 * 7.251148700714111
Epoch 330, val loss: 0.805632472038269
Epoch 340, training loss: 0.18554222583770752 = 0.17829273641109467 + 0.001 * 7.249491214752197
Epoch 340, val loss: 0.819803774356842
Epoch 350, training loss: 0.16467243432998657 = 0.15742450952529907 + 0.001 * 7.24793004989624
Epoch 350, val loss: 0.8353720307350159
Epoch 360, training loss: 0.1459447592496872 = 0.13869589567184448 + 0.001 * 7.248863220214844
Epoch 360, val loss: 0.8520257472991943
Epoch 370, training loss: 0.12930341064929962 = 0.1220574826002121 + 0.001 * 7.2459282875061035
Epoch 370, val loss: 0.8694925308227539
Epoch 380, training loss: 0.11464522033929825 = 0.10740091651678085 + 0.001 * 7.2443037033081055
Epoch 380, val loss: 0.8873909115791321
Epoch 390, training loss: 0.10180776566267014 = 0.09456520527601242 + 0.001 * 7.242556571960449
Epoch 390, val loss: 0.905532717704773
Epoch 400, training loss: 0.09060673415660858 = 0.08336444944143295 + 0.001 * 7.242285251617432
Epoch 400, val loss: 0.9237070083618164
Epoch 410, training loss: 0.0808512270450592 = 0.07360843569040298 + 0.001 * 7.242788791656494
Epoch 410, val loss: 0.9416929483413696
Epoch 420, training loss: 0.07235689461231232 = 0.06511916220188141 + 0.001 * 7.237730026245117
Epoch 420, val loss: 0.9593873023986816
Epoch 430, training loss: 0.06497178971767426 = 0.05773679167032242 + 0.001 * 7.234996795654297
Epoch 430, val loss: 0.9765955209732056
Epoch 440, training loss: 0.05854951962828636 = 0.05131763964891434 + 0.001 * 7.231879234313965
Epoch 440, val loss: 0.9932578802108765
Epoch 450, training loss: 0.052972372621297836 = 0.04573628306388855 + 0.001 * 7.236090183258057
Epoch 450, val loss: 1.0092854499816895
Epoch 460, training loss: 0.048105522990226746 = 0.04088156670331955 + 0.001 * 7.2239556312561035
Epoch 460, val loss: 1.0246117115020752
Epoch 470, training loss: 0.04387637972831726 = 0.03665589168667793 + 0.001 * 7.220486640930176
Epoch 470, val loss: 1.0392457246780396
Epoch 480, training loss: 0.04020022600889206 = 0.032973792403936386 + 0.001 * 7.226433277130127
Epoch 480, val loss: 1.0531775951385498
Epoch 490, training loss: 0.03697076812386513 = 0.029759997501969337 + 0.001 * 7.210770130157471
Epoch 490, val loss: 1.0664877891540527
Epoch 500, training loss: 0.03414996340870857 = 0.026949789375066757 + 0.001 * 7.2001729011535645
Epoch 500, val loss: 1.0790410041809082
Epoch 510, training loss: 0.03168855234980583 = 0.02448710985481739 + 0.001 * 7.201441287994385
Epoch 510, val loss: 1.0910197496414185
Epoch 520, training loss: 0.029506437480449677 = 0.022323431447148323 + 0.001 * 7.1830058097839355
Epoch 520, val loss: 1.102326512336731
Epoch 530, training loss: 0.027638066560029984 = 0.020417513325810432 + 0.001 * 7.220553874969482
Epoch 530, val loss: 1.113037347793579
Epoch 540, training loss: 0.025906065478920937 = 0.0187339186668396 + 0.001 * 7.172145843505859
Epoch 540, val loss: 1.123212218284607
Epoch 550, training loss: 0.02441670559346676 = 0.017242351546883583 + 0.001 * 7.174353122711182
Epoch 550, val loss: 1.1328343152999878
Epoch 560, training loss: 0.02308421954512596 = 0.015916872769594193 + 0.001 * 7.167346954345703
Epoch 560, val loss: 1.1419665813446045
Epoch 570, training loss: 0.02189129777252674 = 0.014735485427081585 + 0.001 * 7.155811309814453
Epoch 570, val loss: 1.1506479978561401
Epoch 580, training loss: 0.02084367349743843 = 0.01367932092398405 + 0.001 * 7.164351463317871
Epoch 580, val loss: 1.1588711738586426
Epoch 590, training loss: 0.019892938435077667 = 0.01273218635469675 + 0.001 * 7.160751819610596
Epoch 590, val loss: 1.1667207479476929
Epoch 600, training loss: 0.019040394574403763 = 0.011880340054631233 + 0.001 * 7.160053730010986
Epoch 600, val loss: 1.1741830110549927
Epoch 610, training loss: 0.018281692638993263 = 0.011111839674413204 + 0.001 * 7.169852256774902
Epoch 610, val loss: 1.1812987327575684
Epoch 620, training loss: 0.01757369562983513 = 0.010416559875011444 + 0.001 * 7.157135486602783
Epoch 620, val loss: 1.1880543231964111
Epoch 630, training loss: 0.016935573890805244 = 0.009785933420062065 + 0.001 * 7.1496405601501465
Epoch 630, val loss: 1.1945427656173706
Epoch 640, training loss: 0.016354752704501152 = 0.009212309494614601 + 0.001 * 7.1424431800842285
Epoch 640, val loss: 1.200731873512268
Epoch 650, training loss: 0.015821121633052826 = 0.008689247071743011 + 0.001 * 7.1318745613098145
Epoch 650, val loss: 1.2066313028335571
Epoch 660, training loss: 0.015339383855462074 = 0.008211107924580574 + 0.001 * 7.1282758712768555
Epoch 660, val loss: 1.2122857570648193
Epoch 670, training loss: 0.014904673211276531 = 0.007772955112159252 + 0.001 * 7.131717681884766
Epoch 670, val loss: 1.2177060842514038
Epoch 680, training loss: 0.014507503248751163 = 0.007370620500296354 + 0.001 * 7.136882305145264
Epoch 680, val loss: 1.222901463508606
Epoch 690, training loss: 0.014129363000392914 = 0.007000416982918978 + 0.001 * 7.128945827484131
Epoch 690, val loss: 1.2278838157653809
Epoch 700, training loss: 0.01377725787460804 = 0.0066589899361133575 + 0.001 * 7.118267059326172
Epoch 700, val loss: 1.2326723337173462
Epoch 710, training loss: 0.013468995690345764 = 0.006343431305140257 + 0.001 * 7.125564098358154
Epoch 710, val loss: 1.237267255783081
Epoch 720, training loss: 0.013170009478926659 = 0.006051329430192709 + 0.001 * 7.118679046630859
Epoch 720, val loss: 1.241667628288269
Epoch 730, training loss: 0.01290488988161087 = 0.005780362524092197 + 0.001 * 7.124526500701904
Epoch 730, val loss: 1.2459105253219604
Epoch 740, training loss: 0.012654091231524944 = 0.005528534296900034 + 0.001 * 7.125556468963623
Epoch 740, val loss: 1.2499902248382568
Epoch 750, training loss: 0.012408627197146416 = 0.005294121336191893 + 0.001 * 7.114505767822266
Epoch 750, val loss: 1.2539267539978027
Epoch 760, training loss: 0.012175760231912136 = 0.0050755213014781475 + 0.001 * 7.100238800048828
Epoch 760, val loss: 1.2577134370803833
Epoch 770, training loss: 0.011975683271884918 = 0.004871390759944916 + 0.001 * 7.104292869567871
Epoch 770, val loss: 1.261346459388733
Epoch 780, training loss: 0.011795497499406338 = 0.004680503625422716 + 0.001 * 7.114993572235107
Epoch 780, val loss: 1.2648855447769165
Epoch 790, training loss: 0.011612351052463055 = 0.004501692019402981 + 0.001 * 7.110658645629883
Epoch 790, val loss: 1.2682727575302124
Epoch 800, training loss: 0.011441340669989586 = 0.004334025084972382 + 0.001 * 7.1073150634765625
Epoch 800, val loss: 1.271566390991211
Epoch 810, training loss: 0.011279590427875519 = 0.004176550544798374 + 0.001 * 7.1030402183532715
Epoch 810, val loss: 1.2747247219085693
Epoch 820, training loss: 0.011115012690424919 = 0.0040284921415150166 + 0.001 * 7.086520195007324
Epoch 820, val loss: 1.2777941226959229
Epoch 830, training loss: 0.01097346842288971 = 0.0038890799041837454 + 0.001 * 7.084388256072998
Epoch 830, val loss: 1.280745267868042
Epoch 840, training loss: 0.010840986855328083 = 0.0037576688919216394 + 0.001 * 7.083317279815674
Epoch 840, val loss: 1.2836215496063232
Epoch 850, training loss: 0.010717232711613178 = 0.0036336586344987154 + 0.001 * 7.083573818206787
Epoch 850, val loss: 1.2863657474517822
Epoch 860, training loss: 0.01060121413320303 = 0.0035164891742169857 + 0.001 * 7.084724426269531
Epoch 860, val loss: 1.2890642881393433
Epoch 870, training loss: 0.010508033446967602 = 0.003405658295378089 + 0.001 * 7.10237455368042
Epoch 870, val loss: 1.2916624546051025
Epoch 880, training loss: 0.01037288922816515 = 0.0033007694873958826 + 0.001 * 7.07211971282959
Epoch 880, val loss: 1.294175624847412
Epoch 890, training loss: 0.010283531621098518 = 0.0032013687305152416 + 0.001 * 7.082162380218506
Epoch 890, val loss: 1.296622633934021
Epoch 900, training loss: 0.01017572171986103 = 0.0031070748809725046 + 0.001 * 7.068646430969238
Epoch 900, val loss: 1.298970341682434
Epoch 910, training loss: 0.010092170909047127 = 0.0030175503343343735 + 0.001 * 7.074619770050049
Epoch 910, val loss: 1.3012768030166626
Epoch 920, training loss: 0.00999742466956377 = 0.002932465635240078 + 0.001 * 7.064958572387695
Epoch 920, val loss: 1.3034857511520386
Epoch 930, training loss: 0.009931026957929134 = 0.0028515260200947523 + 0.001 * 7.079500198364258
Epoch 930, val loss: 1.305651307106018
Epoch 940, training loss: 0.009846331551671028 = 0.0027744886465370655 + 0.001 * 7.071842670440674
Epoch 940, val loss: 1.3077360391616821
Epoch 950, training loss: 0.009775253012776375 = 0.0027010871563106775 + 0.001 * 7.0741658210754395
Epoch 950, val loss: 1.309778094291687
Epoch 960, training loss: 0.009707572869956493 = 0.0026311143301427364 + 0.001 * 7.076457977294922
Epoch 960, val loss: 1.3117568492889404
Epoch 970, training loss: 0.009628858417272568 = 0.002564344322308898 + 0.001 * 7.064513683319092
Epoch 970, val loss: 1.3136651515960693
Epoch 980, training loss: 0.009553644806146622 = 0.0025005806237459183 + 0.001 * 7.053063869476318
Epoch 980, val loss: 1.315535306930542
Epoch 990, training loss: 0.00952522736042738 = 0.0024396490771323442 + 0.001 * 7.085577487945557
Epoch 990, val loss: 1.3173441886901855
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.9529494047164917 = 1.944352626800537 + 0.001 * 8.596820831298828
Epoch 0, val loss: 1.9421793222427368
Epoch 10, training loss: 1.943221926689148 = 1.9346251487731934 + 0.001 * 8.596755981445312
Epoch 10, val loss: 1.9321460723876953
Epoch 20, training loss: 1.9311543703079224 = 1.9225578308105469 + 0.001 * 8.596556663513184
Epoch 20, val loss: 1.9195829629898071
Epoch 30, training loss: 1.9141336679458618 = 1.9055376052856445 + 0.001 * 8.596120834350586
Epoch 30, val loss: 1.9018012285232544
Epoch 40, training loss: 1.8890252113342285 = 1.8804301023483276 + 0.001 * 8.59505558013916
Epoch 40, val loss: 1.8757861852645874
Epoch 50, training loss: 1.8537445068359375 = 1.8451526165008545 + 0.001 * 8.591928482055664
Epoch 50, val loss: 1.8407896757125854
Epoch 60, training loss: 1.8133364915847778 = 1.8047564029693604 + 0.001 * 8.58004379272461
Epoch 60, val loss: 1.804868221282959
Epoch 70, training loss: 1.777917504310608 = 1.769393801689148 + 0.001 * 8.523656845092773
Epoch 70, val loss: 1.7770533561706543
Epoch 80, training loss: 1.7368566989898682 = 1.7286603450775146 + 0.001 * 8.196349143981934
Epoch 80, val loss: 1.741361141204834
Epoch 90, training loss: 1.6796231269836426 = 1.671608328819275 + 0.001 * 8.014798164367676
Epoch 90, val loss: 1.68953275680542
Epoch 100, training loss: 1.6022911071777344 = 1.5944738388061523 + 0.001 * 7.817267417907715
Epoch 100, val loss: 1.621620774269104
Epoch 110, training loss: 1.5063129663467407 = 1.4987318515777588 + 0.001 * 7.58107328414917
Epoch 110, val loss: 1.540323257446289
Epoch 120, training loss: 1.3994174003601074 = 1.3920117616653442 + 0.001 * 7.405622482299805
Epoch 120, val loss: 1.4503768682479858
Epoch 130, training loss: 1.2892615795135498 = 1.281869649887085 + 0.001 * 7.391966342926025
Epoch 130, val loss: 1.360174298286438
Epoch 140, training loss: 1.1804944276809692 = 1.1731204986572266 + 0.001 * 7.373874187469482
Epoch 140, val loss: 1.2727655172348022
Epoch 150, training loss: 1.0764727592468262 = 1.069112777709961 + 0.001 * 7.359961032867432
Epoch 150, val loss: 1.190464973449707
Epoch 160, training loss: 0.9791088700294495 = 0.9717614650726318 + 0.001 * 7.34742546081543
Epoch 160, val loss: 1.1137118339538574
Epoch 170, training loss: 0.8890191912651062 = 0.8816832304000854 + 0.001 * 7.335960388183594
Epoch 170, val loss: 1.0434646606445312
Epoch 180, training loss: 0.8059547543525696 = 0.7986276745796204 + 0.001 * 7.327107906341553
Epoch 180, val loss: 0.9793459177017212
Epoch 190, training loss: 0.7295567393302917 = 0.7222363352775574 + 0.001 * 7.320427894592285
Epoch 190, val loss: 0.9215081334114075
Epoch 200, training loss: 0.659669816493988 = 0.6523537039756775 + 0.001 * 7.316127777099609
Epoch 200, val loss: 0.8706367611885071
Epoch 210, training loss: 0.5958197116851807 = 0.5885064601898193 + 0.001 * 7.313225269317627
Epoch 210, val loss: 0.8270249366760254
Epoch 220, training loss: 0.5371076464653015 = 0.5297977328300476 + 0.001 * 7.309935092926025
Epoch 220, val loss: 0.7900567650794983
Epoch 230, training loss: 0.48251765966415405 = 0.4752132296562195 + 0.001 * 7.304421901702881
Epoch 230, val loss: 0.7583104968070984
Epoch 240, training loss: 0.4310917258262634 = 0.4237973690032959 + 0.001 * 7.294350624084473
Epoch 240, val loss: 0.730529248714447
Epoch 250, training loss: 0.3823707401752472 = 0.3750881254673004 + 0.001 * 7.282599925994873
Epoch 250, val loss: 0.7061259746551514
Epoch 260, training loss: 0.33644622564315796 = 0.3291892111301422 + 0.001 * 7.257002353668213
Epoch 260, val loss: 0.6852843761444092
Epoch 270, training loss: 0.29377496242523193 = 0.2865314185619354 + 0.001 * 7.243538856506348
Epoch 270, val loss: 0.6685718894004822
Epoch 280, training loss: 0.25479593873023987 = 0.2475774884223938 + 0.001 * 7.218443870544434
Epoch 280, val loss: 0.6564427614212036
Epoch 290, training loss: 0.21983619034290314 = 0.21263088285923004 + 0.001 * 7.20530891418457
Epoch 290, val loss: 0.6489415764808655
Epoch 300, training loss: 0.1891120821237564 = 0.1819087713956833 + 0.001 * 7.203313827514648
Epoch 300, val loss: 0.645805299282074
Epoch 310, training loss: 0.16265878081321716 = 0.15546220541000366 + 0.001 * 7.196582317352295
Epoch 310, val loss: 0.6465737223625183
Epoch 320, training loss: 0.14028631150722504 = 0.13309408724308014 + 0.001 * 7.1922197341918945
Epoch 320, val loss: 0.6506538391113281
Epoch 330, training loss: 0.12157090753316879 = 0.11438192427158356 + 0.001 * 7.188985824584961
Epoch 330, val loss: 0.657439649105072
Epoch 340, training loss: 0.10599017888307571 = 0.09880432486534119 + 0.001 * 7.185851097106934
Epoch 340, val loss: 0.666308581829071
Epoch 350, training loss: 0.09302447736263275 = 0.08583937585353851 + 0.001 * 7.1850972175598145
Epoch 350, val loss: 0.6767387986183167
Epoch 360, training loss: 0.08219868689775467 = 0.07502084225416183 + 0.001 * 7.1778411865234375
Epoch 360, val loss: 0.6882574558258057
Epoch 370, training loss: 0.07312297075986862 = 0.06595199555158615 + 0.001 * 7.170977592468262
Epoch 370, val loss: 0.700421154499054
Epoch 380, training loss: 0.06547481566667557 = 0.05830742046236992 + 0.001 * 7.167396068572998
Epoch 380, val loss: 0.7129935026168823
Epoch 390, training loss: 0.05899176746606827 = 0.051826924085617065 + 0.001 * 7.16484260559082
Epoch 390, val loss: 0.7257343530654907
Epoch 400, training loss: 0.053468313068151474 = 0.04630080610513687 + 0.001 * 7.167507648468018
Epoch 400, val loss: 0.738455057144165
Epoch 410, training loss: 0.04871370643377304 = 0.04156125709414482 + 0.001 * 7.1524505615234375
Epoch 410, val loss: 0.751060426235199
Epoch 420, training loss: 0.04462851583957672 = 0.03747425973415375 + 0.001 * 7.154257774353027
Epoch 420, val loss: 0.7635208368301392
Epoch 430, training loss: 0.04106246680021286 = 0.033931881189346313 + 0.001 * 7.130587100982666
Epoch 430, val loss: 0.7757202982902527
Epoch 440, training loss: 0.037970855832099915 = 0.030845850706100464 + 0.001 * 7.125002861022949
Epoch 440, val loss: 0.787648618221283
Epoch 450, training loss: 0.035267725586891174 = 0.02814486064016819 + 0.001 * 7.122866630554199
Epoch 450, val loss: 0.7992373704910278
Epoch 460, training loss: 0.03288843110203743 = 0.02577040158212185 + 0.001 * 7.118029594421387
Epoch 460, val loss: 0.8105151057243347
Epoch 470, training loss: 0.030797332525253296 = 0.02367446757853031 + 0.001 * 7.122865200042725
Epoch 470, val loss: 0.8214706778526306
Epoch 480, training loss: 0.028932543471455574 = 0.021817456930875778 + 0.001 * 7.115086555480957
Epoch 480, val loss: 0.8320908546447754
Epoch 490, training loss: 0.02727551944553852 = 0.020166074857115746 + 0.001 * 7.1094441413879395
Epoch 490, val loss: 0.8423892259597778
Epoch 500, training loss: 0.025818902999162674 = 0.018692361190915108 + 0.001 * 7.126542091369629
Epoch 500, val loss: 0.8523658514022827
Epoch 510, training loss: 0.024478096514940262 = 0.017372967675328255 + 0.001 * 7.105128288269043
Epoch 510, val loss: 0.8620508313179016
Epoch 520, training loss: 0.02329247258603573 = 0.016187941655516624 + 0.001 * 7.104530334472656
Epoch 520, val loss: 0.8714258074760437
Epoch 530, training loss: 0.02221726067364216 = 0.015120030380785465 + 0.001 * 7.097230434417725
Epoch 530, val loss: 0.8805243372917175
Epoch 540, training loss: 0.021248726174235344 = 0.014154922217130661 + 0.001 * 7.093803405761719
Epoch 540, val loss: 0.8893263936042786
Epoch 550, training loss: 0.020369071513414383 = 0.013280205428600311 + 0.001 * 7.088865756988525
Epoch 550, val loss: 0.8978661894798279
Epoch 560, training loss: 0.019576771184802055 = 0.012485306710004807 + 0.001 * 7.091463565826416
Epoch 560, val loss: 0.9061427116394043
Epoch 570, training loss: 0.018848281353712082 = 0.011761064641177654 + 0.001 * 7.087215423583984
Epoch 570, val loss: 0.9141746163368225
Epoch 580, training loss: 0.0181916281580925 = 0.011099500581622124 + 0.001 * 7.092126846313477
Epoch 580, val loss: 0.9219616055488586
Epoch 590, training loss: 0.017579685896635056 = 0.010493817739188671 + 0.001 * 7.085868835449219
Epoch 590, val loss: 0.9295315742492676
Epoch 600, training loss: 0.017021508887410164 = 0.009938053786754608 + 0.001 * 7.083454608917236
Epoch 600, val loss: 0.9368749856948853
Epoch 610, training loss: 0.016505807638168335 = 0.009426917880773544 + 0.001 * 7.0788893699646
Epoch 610, val loss: 0.9440182447433472
Epoch 620, training loss: 0.01604260317981243 = 0.008955853059887886 + 0.001 * 7.08674955368042
Epoch 620, val loss: 0.9509451389312744
Epoch 630, training loss: 0.015598125755786896 = 0.008520824834704399 + 0.001 * 7.077301025390625
Epoch 630, val loss: 0.9576895236968994
Epoch 640, training loss: 0.015201684087514877 = 0.008118247613310814 + 0.001 * 7.083436012268066
Epoch 640, val loss: 0.9642438888549805
Epoch 650, training loss: 0.014833326451480389 = 0.007745074573904276 + 0.001 * 7.08825159072876
Epoch 650, val loss: 0.9706150889396667
Epoch 660, training loss: 0.014471923001110554 = 0.007398491725325584 + 0.001 * 7.073431015014648
Epoch 660, val loss: 0.9768153429031372
Epoch 670, training loss: 0.01415660697966814 = 0.007076111622154713 + 0.001 * 7.0804948806762695
Epoch 670, val loss: 0.9828608632087708
Epoch 680, training loss: 0.013849628157913685 = 0.006775747984647751 + 0.001 * 7.073879718780518
Epoch 680, val loss: 0.9887388944625854
Epoch 690, training loss: 0.013557210564613342 = 0.006495434325188398 + 0.001 * 7.0617756843566895
Epoch 690, val loss: 0.9944556355476379
Epoch 700, training loss: 0.01329701766371727 = 0.006233410909771919 + 0.001 * 7.063606262207031
Epoch 700, val loss: 1.0000109672546387
Epoch 710, training loss: 0.01305314525961876 = 0.005988146644085646 + 0.001 * 7.064998149871826
Epoch 710, val loss: 1.0054341554641724
Epoch 720, training loss: 0.012834951281547546 = 0.005758246872574091 + 0.001 * 7.076704025268555
Epoch 720, val loss: 1.0107131004333496
Epoch 730, training loss: 0.01259668543934822 = 0.005542489234358072 + 0.001 * 7.054195404052734
Epoch 730, val loss: 1.0158671140670776
Epoch 740, training loss: 0.012403911910951138 = 0.005339702591300011 + 0.001 * 7.064208984375
Epoch 740, val loss: 1.020887851715088
Epoch 750, training loss: 0.012196894735097885 = 0.005148889031261206 + 0.001 * 7.0480055809021
Epoch 750, val loss: 1.025780439376831
Epoch 760, training loss: 0.012034837156534195 = 0.004969106055796146 + 0.001 * 7.065730094909668
Epoch 760, val loss: 1.030557632446289
Epoch 770, training loss: 0.01185157522559166 = 0.00479954294860363 + 0.001 * 7.052031517028809
Epoch 770, val loss: 1.0352120399475098
Epoch 780, training loss: 0.011684935539960861 = 0.0046394322998821735 + 0.001 * 7.04550313949585
Epoch 780, val loss: 1.0397484302520752
Epoch 790, training loss: 0.01152932271361351 = 0.004488098435103893 + 0.001 * 7.041224479675293
Epoch 790, val loss: 1.0441755056381226
Epoch 800, training loss: 0.011402763426303864 = 0.004344879183918238 + 0.001 * 7.0578837394714355
Epoch 800, val loss: 1.0485109090805054
Epoch 810, training loss: 0.011260243132710457 = 0.0042092385701835155 + 0.001 * 7.051003932952881
Epoch 810, val loss: 1.0527325868606567
Epoch 820, training loss: 0.011117920279502869 = 0.004080584272742271 + 0.001 * 7.0373358726501465
Epoch 820, val loss: 1.0568500757217407
Epoch 830, training loss: 0.010995317250490189 = 0.003958331421017647 + 0.001 * 7.036985397338867
Epoch 830, val loss: 1.060877799987793
Epoch 840, training loss: 0.010890745557844639 = 0.003841960337013006 + 0.001 * 7.0487847328186035
Epoch 840, val loss: 1.0648118257522583
Epoch 850, training loss: 0.01075937319546938 = 0.003730786731466651 + 0.001 * 7.028586387634277
Epoch 850, val loss: 1.0686787366867065
Epoch 860, training loss: 0.010644963011145592 = 0.003623983357101679 + 0.001 * 7.020979404449463
Epoch 860, val loss: 1.072507381439209
Epoch 870, training loss: 0.010534260421991348 = 0.0035208973567932844 + 0.001 * 7.013362884521484
Epoch 870, val loss: 1.076277256011963
Epoch 880, training loss: 0.010433873161673546 = 0.003421222325414419 + 0.001 * 7.012650489807129
Epoch 880, val loss: 1.080021619796753
Epoch 890, training loss: 0.010341661050915718 = 0.003324906574562192 + 0.001 * 7.016754150390625
Epoch 890, val loss: 1.0837491750717163
Epoch 900, training loss: 0.010249149054288864 = 0.0032318471930921078 + 0.001 * 7.017301559448242
Epoch 900, val loss: 1.0874298810958862
Epoch 910, training loss: 0.010195152834057808 = 0.0031420232262462378 + 0.001 * 7.05312967300415
Epoch 910, val loss: 1.091099739074707
Epoch 920, training loss: 0.010073475539684296 = 0.003055418375879526 + 0.001 * 7.018056869506836
Epoch 920, val loss: 1.0947110652923584
Epoch 930, training loss: 0.009979070164263248 = 0.002972034038975835 + 0.001 * 7.007036209106445
Epoch 930, val loss: 1.0983033180236816
Epoch 940, training loss: 0.00989740900695324 = 0.002891820389777422 + 0.001 * 7.005588054656982
Epoch 940, val loss: 1.101854681968689
Epoch 950, training loss: 0.009823666885495186 = 0.00281468965113163 + 0.001 * 7.008976936340332
Epoch 950, val loss: 1.105347990989685
Epoch 960, training loss: 0.009747885167598724 = 0.0027405968867242336 + 0.001 * 7.007287502288818
Epoch 960, val loss: 1.1087932586669922
Epoch 970, training loss: 0.009677532128989697 = 0.002669430570676923 + 0.001 * 7.008101463317871
Epoch 970, val loss: 1.1121488809585571
Epoch 980, training loss: 0.009605873376131058 = 0.0026010817382484674 + 0.001 * 7.004791736602783
Epoch 980, val loss: 1.1154969930648804
Epoch 990, training loss: 0.009524527005851269 = 0.002535479608923197 + 0.001 * 6.989047050476074
Epoch 990, val loss: 1.1187736988067627
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 1.9426348209381104 = 1.9340380430221558 + 0.001 * 8.596835136413574
Epoch 0, val loss: 1.9314824342727661
Epoch 10, training loss: 1.9330271482467651 = 1.9244303703308105 + 0.001 * 8.596782684326172
Epoch 10, val loss: 1.9215799570083618
Epoch 20, training loss: 1.9213509559631348 = 1.9127544164657593 + 0.001 * 8.59658432006836
Epoch 20, val loss: 1.9094619750976562
Epoch 30, training loss: 1.9052566289901733 = 1.896660566329956 + 0.001 * 8.596085548400879
Epoch 30, val loss: 1.8929505348205566
Epoch 40, training loss: 1.8820149898529053 = 1.873420238494873 + 0.001 * 8.594789505004883
Epoch 40, val loss: 1.869554877281189
Epoch 50, training loss: 1.8493037223815918 = 1.8407130241394043 + 0.001 * 8.590742111206055
Epoch 50, val loss: 1.838119387626648
Epoch 60, training loss: 1.8096870183944702 = 1.8011126518249512 + 0.001 * 8.574339866638184
Epoch 60, val loss: 1.8032180070877075
Epoch 70, training loss: 1.7696022987365723 = 1.7611145973205566 + 0.001 * 8.487687110900879
Epoch 70, val loss: 1.7699679136276245
Epoch 80, training loss: 1.7197521924972534 = 1.7116200923919678 + 0.001 * 8.132118225097656
Epoch 80, val loss: 1.7255991697311401
Epoch 90, training loss: 1.6490757465362549 = 1.6410925388336182 + 0.001 * 7.983220100402832
Epoch 90, val loss: 1.6629340648651123
Epoch 100, training loss: 1.556929588317871 = 1.549108624458313 + 0.001 * 7.8210062980651855
Epoch 100, val loss: 1.5856674909591675
Epoch 110, training loss: 1.4511051177978516 = 1.4435049295425415 + 0.001 * 7.600210189819336
Epoch 110, val loss: 1.4964711666107178
Epoch 120, training loss: 1.343245029449463 = 1.335762858390808 + 0.001 * 7.482110977172852
Epoch 120, val loss: 1.4087976217269897
Epoch 130, training loss: 1.240807056427002 = 1.2334072589874268 + 0.001 * 7.3997931480407715
Epoch 130, val loss: 1.3286519050598145
Epoch 140, training loss: 1.1466634273529053 = 1.1393121480941772 + 0.001 * 7.351232528686523
Epoch 140, val loss: 1.25808584690094
Epoch 150, training loss: 1.0597493648529053 = 1.0524159669876099 + 0.001 * 7.3334245681762695
Epoch 150, val loss: 1.1935091018676758
Epoch 160, training loss: 0.9776944518089294 = 0.9703770875930786 + 0.001 * 7.317388534545898
Epoch 160, val loss: 1.1316356658935547
Epoch 170, training loss: 0.8992312550544739 = 0.8919348120689392 + 0.001 * 7.296418190002441
Epoch 170, val loss: 1.070957064628601
Epoch 180, training loss: 0.8242565393447876 = 0.8169839978218079 + 0.001 * 7.272557258605957
Epoch 180, val loss: 1.01203191280365
Epoch 190, training loss: 0.7536988258361816 = 0.7464380264282227 + 0.001 * 7.260819911956787
Epoch 190, val loss: 0.9560253024101257
Epoch 200, training loss: 0.6888287663459778 = 0.681580126285553 + 0.001 * 7.248666286468506
Epoch 200, val loss: 0.9051748514175415
Epoch 210, training loss: 0.6299179792404175 = 0.622672975063324 + 0.001 * 7.245018005371094
Epoch 210, val loss: 0.8611583113670349
Epoch 220, training loss: 0.5764370560646057 = 0.5691961646080017 + 0.001 * 7.240904808044434
Epoch 220, val loss: 0.8245223760604858
Epoch 230, training loss: 0.527980625629425 = 0.5207435488700867 + 0.001 * 7.237059116363525
Epoch 230, val loss: 0.7955237030982971
Epoch 240, training loss: 0.48463767766952515 = 0.4774056077003479 + 0.001 * 7.232082843780518
Epoch 240, val loss: 0.7742045521736145
Epoch 250, training loss: 0.4462433159351349 = 0.43901699781417847 + 0.001 * 7.226308345794678
Epoch 250, val loss: 0.759915828704834
Epoch 260, training loss: 0.4117550551891327 = 0.4045355021953583 + 0.001 * 7.21953821182251
Epoch 260, val loss: 0.7509710788726807
Epoch 270, training loss: 0.37972885370254517 = 0.37251636385917664 + 0.001 * 7.21249532699585
Epoch 270, val loss: 0.7457497715950012
Epoch 280, training loss: 0.3491094410419464 = 0.3419017791748047 + 0.001 * 7.207657814025879
Epoch 280, val loss: 0.7430785298347473
Epoch 290, training loss: 0.31955471634864807 = 0.31235677003860474 + 0.001 * 7.197950839996338
Epoch 290, val loss: 0.7423490881919861
Epoch 300, training loss: 0.2912580370903015 = 0.2840706408023834 + 0.001 * 7.187397480010986
Epoch 300, val loss: 0.7431139349937439
Epoch 310, training loss: 0.26442578434944153 = 0.25724488496780396 + 0.001 * 7.180908203125
Epoch 310, val loss: 0.7449914813041687
Epoch 320, training loss: 0.23900876939296722 = 0.2318316102027893 + 0.001 * 7.177151679992676
Epoch 320, val loss: 0.7476090788841248
Epoch 330, training loss: 0.21481074392795563 = 0.2076331526041031 + 0.001 * 7.177585601806641
Epoch 330, val loss: 0.75049889087677
Epoch 340, training loss: 0.1917567104101181 = 0.18458503484725952 + 0.001 * 7.1716742515563965
Epoch 340, val loss: 0.7536979913711548
Epoch 350, training loss: 0.1700526773929596 = 0.16287605464458466 + 0.001 * 7.176626205444336
Epoch 350, val loss: 0.7573000192642212
Epoch 360, training loss: 0.15001292526721954 = 0.14284741878509521 + 0.001 * 7.165510654449463
Epoch 360, val loss: 0.7615631818771362
Epoch 370, training loss: 0.13195103406906128 = 0.12478552758693695 + 0.001 * 7.165510654449463
Epoch 370, val loss: 0.766501247882843
Epoch 380, training loss: 0.11596978455781937 = 0.10880088806152344 + 0.001 * 7.168898105621338
Epoch 380, val loss: 0.7722683548927307
Epoch 390, training loss: 0.10200117528438568 = 0.09483285993337631 + 0.001 * 7.168313503265381
Epoch 390, val loss: 0.7786946892738342
Epoch 400, training loss: 0.08989038318395615 = 0.0827244445681572 + 0.001 * 7.165937423706055
Epoch 400, val loss: 0.7856096625328064
Epoch 410, training loss: 0.07946982979774475 = 0.07229751348495483 + 0.001 * 7.172314167022705
Epoch 410, val loss: 0.7929893136024475
Epoch 420, training loss: 0.07053640484809875 = 0.0633702501654625 + 0.001 * 7.166153907775879
Epoch 420, val loss: 0.8006560802459717
Epoch 430, training loss: 0.0629250779747963 = 0.05575811117887497 + 0.001 * 7.166965961456299
Epoch 430, val loss: 0.8085873126983643
Epoch 440, training loss: 0.05645071715116501 = 0.04928012192249298 + 0.001 * 7.170596122741699
Epoch 440, val loss: 0.816686749458313
Epoch 450, training loss: 0.05093375965952873 = 0.043767817318439484 + 0.001 * 7.165942668914795
Epoch 450, val loss: 0.8248592019081116
Epoch 460, training loss: 0.0462333969771862 = 0.03906703367829323 + 0.001 * 7.1663618087768555
Epoch 460, val loss: 0.8332338333129883
Epoch 470, training loss: 0.04220655933022499 = 0.03504427149891853 + 0.001 * 7.162288665771484
Epoch 470, val loss: 0.841552197933197
Epoch 480, training loss: 0.0387524850666523 = 0.031588342040777206 + 0.001 * 7.164141654968262
Epoch 480, val loss: 0.849895715713501
Epoch 490, training loss: 0.03576712682843208 = 0.02860540710389614 + 0.001 * 7.161720275878906
Epoch 490, val loss: 0.8582229614257812
Epoch 500, training loss: 0.03318275511264801 = 0.026018191128969193 + 0.001 * 7.164562702178955
Epoch 500, val loss: 0.8665328025817871
Epoch 510, training loss: 0.030928190797567368 = 0.023763595148921013 + 0.001 * 7.164594650268555
Epoch 510, val loss: 0.8747211694717407
Epoch 520, training loss: 0.028945166617631912 = 0.021789224818348885 + 0.001 * 7.155942440032959
Epoch 520, val loss: 0.8828094005584717
Epoch 530, training loss: 0.027209490537643433 = 0.020051991567015648 + 0.001 * 7.157498836517334
Epoch 530, val loss: 0.8907427191734314
Epoch 540, training loss: 0.025670140981674194 = 0.01851661503314972 + 0.001 * 7.153524875640869
Epoch 540, val loss: 0.8985655307769775
Epoch 550, training loss: 0.02430848777294159 = 0.017153752967715263 + 0.001 * 7.1547346115112305
Epoch 550, val loss: 0.9061760902404785
Epoch 560, training loss: 0.023098677396774292 = 0.015938932076096535 + 0.001 * 7.159745216369629
Epoch 560, val loss: 0.9136685132980347
Epoch 570, training loss: 0.022012105211615562 = 0.014852042309939861 + 0.001 * 7.160062313079834
Epoch 570, val loss: 0.9209661483764648
Epoch 580, training loss: 0.021026041358709335 = 0.013875908218324184 + 0.001 * 7.150132179260254
Epoch 580, val loss: 0.928107500076294
Epoch 590, training loss: 0.02014271542429924 = 0.012996113859117031 + 0.001 * 7.146600246429443
Epoch 590, val loss: 0.9350481629371643
Epoch 600, training loss: 0.01934782974421978 = 0.012200405821204185 + 0.001 * 7.147423267364502
Epoch 600, val loss: 0.9418637752532959
Epoch 610, training loss: 0.018635720014572144 = 0.011478524655103683 + 0.001 * 7.157196044921875
Epoch 610, val loss: 0.9484980702400208
Epoch 620, training loss: 0.017969146370887756 = 0.010821746662259102 + 0.001 * 7.147400379180908
Epoch 620, val loss: 0.9549636244773865
Epoch 630, training loss: 0.01736130565404892 = 0.010222611017525196 + 0.001 * 7.1386942863464355
Epoch 630, val loss: 0.961273193359375
Epoch 640, training loss: 0.016820300370454788 = 0.009674432687461376 + 0.001 * 7.145866870880127
Epoch 640, val loss: 0.9674335718154907
Epoch 650, training loss: 0.016312461346387863 = 0.009171645157039165 + 0.001 * 7.140816688537598
Epoch 650, val loss: 0.973456621170044
Epoch 660, training loss: 0.01584278792142868 = 0.008709408342838287 + 0.001 * 7.1333794593811035
Epoch 660, val loss: 0.9793108701705933
Epoch 670, training loss: 0.01541672833263874 = 0.00828333105891943 + 0.001 * 7.133396625518799
Epoch 670, val loss: 0.9850311279296875
Epoch 680, training loss: 0.015095693990588188 = 0.00788971409201622 + 0.001 * 7.205979347229004
Epoch 680, val loss: 0.9906283617019653
Epoch 690, training loss: 0.014661352150142193 = 0.007525430992245674 + 0.001 * 7.135921001434326
Epoch 690, val loss: 0.9960955381393433
Epoch 700, training loss: 0.01431839819997549 = 0.007187609560787678 + 0.001 * 7.130788326263428
Epoch 700, val loss: 1.0014115571975708
Epoch 710, training loss: 0.013999586924910545 = 0.006873728707432747 + 0.001 * 7.125858306884766
Epoch 710, val loss: 1.0066243410110474
Epoch 720, training loss: 0.013705981895327568 = 0.006581566296517849 + 0.001 * 7.124415874481201
Epoch 720, val loss: 1.0117089748382568
Epoch 730, training loss: 0.01345164142549038 = 0.00630911672487855 + 0.001 * 7.142524719238281
Epoch 730, val loss: 1.016675353050232
Epoch 740, training loss: 0.013183800503611565 = 0.006054671015590429 + 0.001 * 7.129128456115723
Epoch 740, val loss: 1.02153480052948
Epoch 750, training loss: 0.012950762175023556 = 0.005816683638840914 + 0.001 * 7.134078025817871
Epoch 750, val loss: 1.026277780532837
Epoch 760, training loss: 0.012721292674541473 = 0.005593744572252035 + 0.001 * 7.127547264099121
Epoch 760, val loss: 1.0309162139892578
Epoch 770, training loss: 0.012502679601311684 = 0.005384583491832018 + 0.001 * 7.118096351623535
Epoch 770, val loss: 1.0354442596435547
Epoch 780, training loss: 0.012294931337237358 = 0.005188083741813898 + 0.001 * 7.106847763061523
Epoch 780, val loss: 1.03986656665802
Epoch 790, training loss: 0.012116964906454086 = 0.0050032297149300575 + 0.001 * 7.113734245300293
Epoch 790, val loss: 1.044206976890564
Epoch 800, training loss: 0.011930478736758232 = 0.00482913525775075 + 0.001 * 7.101343154907227
Epoch 800, val loss: 1.0484492778778076
Epoch 810, training loss: 0.01176043413579464 = 0.004664964973926544 + 0.001 * 7.095468521118164
Epoch 810, val loss: 1.052594780921936
Epoch 820, training loss: 0.011670702137053013 = 0.00450997706502676 + 0.001 * 7.160724639892578
Epoch 820, val loss: 1.056658148765564
Epoch 830, training loss: 0.011463597416877747 = 0.004363556858152151 + 0.001 * 7.100039958953857
Epoch 830, val loss: 1.0606054067611694
Epoch 840, training loss: 0.011329589411616325 = 0.004225032404065132 + 0.001 * 7.104557037353516
Epoch 840, val loss: 1.0644949674606323
Epoch 850, training loss: 0.011225360445678234 = 0.004093854688107967 + 0.001 * 7.131505489349365
Epoch 850, val loss: 1.0682997703552246
Epoch 860, training loss: 0.011055604554712772 = 0.0039694965817034245 + 0.001 * 7.0861077308654785
Epoch 860, val loss: 1.0720267295837402
Epoch 870, training loss: 0.010963023640215397 = 0.0038514763582497835 + 0.001 * 7.111546516418457
Epoch 870, val loss: 1.0756781101226807
Epoch 880, training loss: 0.010834269225597382 = 0.0037393718957901 + 0.001 * 7.0948967933654785
Epoch 880, val loss: 1.079262375831604
Epoch 890, training loss: 0.010728154331445694 = 0.0036327983252704144 + 0.001 * 7.09535551071167
Epoch 890, val loss: 1.082772970199585
Epoch 900, training loss: 0.010620582848787308 = 0.0035313954576849937 + 0.001 * 7.089187145233154
Epoch 900, val loss: 1.0862089395523071
Epoch 910, training loss: 0.010525338351726532 = 0.0034348303452134132 + 0.001 * 7.090507984161377
Epoch 910, val loss: 1.0895878076553345
Epoch 920, training loss: 0.010495886206626892 = 0.0033427930902689695 + 0.001 * 7.153092861175537
Epoch 920, val loss: 1.0928869247436523
Epoch 930, training loss: 0.010349174030125141 = 0.0032550375908613205 + 0.001 * 7.0941362380981445
Epoch 930, val loss: 1.096116065979004
Epoch 940, training loss: 0.010253019630908966 = 0.0031712683849036694 + 0.001 * 7.081751346588135
Epoch 940, val loss: 1.099294662475586
Epoch 950, training loss: 0.010168869979679585 = 0.0030912335496395826 + 0.001 * 7.077635765075684
Epoch 950, val loss: 1.102403998374939
Epoch 960, training loss: 0.010102636180818081 = 0.003014711895957589 + 0.001 * 7.087923526763916
Epoch 960, val loss: 1.1054611206054688
Epoch 970, training loss: 0.010032613761723042 = 0.0029414661694318056 + 0.001 * 7.091146945953369
Epoch 970, val loss: 1.1084647178649902
Epoch 980, training loss: 0.009942203760147095 = 0.0028713266365230083 + 0.001 * 7.0708770751953125
Epoch 980, val loss: 1.1113935708999634
Epoch 990, training loss: 0.009868831373751163 = 0.002804096322506666 + 0.001 * 7.064734935760498
Epoch 990, val loss: 1.1142970323562622
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8397469688982605
The final CL Acc:0.79012, 0.02290, The final GNN Acc:0.83764, 0.00197
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9398])
updated graph: torch.Size([2, 10436])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.932642936706543 = 1.9240460395812988 + 0.001 * 8.5968656539917
Epoch 0, val loss: 1.91720449924469
Epoch 10, training loss: 1.92365300655365 = 1.9150562286376953 + 0.001 * 8.596823692321777
Epoch 10, val loss: 1.9084298610687256
Epoch 20, training loss: 1.9124476909637451 = 1.90385103225708 + 0.001 * 8.596704483032227
Epoch 20, val loss: 1.8971282243728638
Epoch 30, training loss: 1.896574854850769 = 1.887978434562683 + 0.001 * 8.596437454223633
Epoch 30, val loss: 1.8809268474578857
Epoch 40, training loss: 1.8732715845108032 = 1.864675760269165 + 0.001 * 8.595864295959473
Epoch 40, val loss: 1.8575987815856934
Epoch 50, training loss: 1.8415206670761108 = 1.8329261541366577 + 0.001 * 8.594498634338379
Epoch 50, val loss: 1.8277168273925781
Epoch 60, training loss: 1.8068143129348755 = 1.798223614692688 + 0.001 * 8.590696334838867
Epoch 60, val loss: 1.79945707321167
Epoch 70, training loss: 1.7730079889297485 = 1.764430046081543 + 0.001 * 8.577953338623047
Epoch 70, val loss: 1.773802638053894
Epoch 80, training loss: 1.727070927619934 = 1.7185651063919067 + 0.001 * 8.505788803100586
Epoch 80, val loss: 1.7340525388717651
Epoch 90, training loss: 1.663535475730896 = 1.6554667949676514 + 0.001 * 8.068623542785645
Epoch 90, val loss: 1.6775327920913696
Epoch 100, training loss: 1.5819405317306519 = 1.5739741325378418 + 0.001 * 7.966355323791504
Epoch 100, val loss: 1.607600212097168
Epoch 110, training loss: 1.4891527891159058 = 1.4813640117645264 + 0.001 * 7.788760185241699
Epoch 110, val loss: 1.5307867527008057
Epoch 120, training loss: 1.390636682510376 = 1.3829705715179443 + 0.001 * 7.666163921356201
Epoch 120, val loss: 1.4501343965530396
Epoch 130, training loss: 1.2880984544754028 = 1.2804362773895264 + 0.001 * 7.6621785163879395
Epoch 130, val loss: 1.3697373867034912
Epoch 140, training loss: 1.1827245950698853 = 1.1750853061676025 + 0.001 * 7.63928747177124
Epoch 140, val loss: 1.2889175415039062
Epoch 150, training loss: 1.0779227018356323 = 1.070302963256836 + 0.001 * 7.619696617126465
Epoch 150, val loss: 1.2116234302520752
Epoch 160, training loss: 0.9779770970344543 = 0.9703838229179382 + 0.001 * 7.593276023864746
Epoch 160, val loss: 1.1407344341278076
Epoch 170, training loss: 0.8858920931816101 = 0.8783365488052368 + 0.001 * 7.555523872375488
Epoch 170, val loss: 1.0778582096099854
Epoch 180, training loss: 0.8032053709030151 = 0.7956985831260681 + 0.001 * 7.506816387176514
Epoch 180, val loss: 1.0237687826156616
Epoch 190, training loss: 0.7301015257835388 = 0.7226467132568359 + 0.001 * 7.454794406890869
Epoch 190, val loss: 0.9781060218811035
Epoch 200, training loss: 0.6654891967773438 = 0.6580694317817688 + 0.001 * 7.419754981994629
Epoch 200, val loss: 0.9406082034111023
Epoch 210, training loss: 0.607641875743866 = 0.6002338528633118 + 0.001 * 7.408050060272217
Epoch 210, val loss: 0.9104795455932617
Epoch 220, training loss: 0.5550003051757812 = 0.5475960969924927 + 0.001 * 7.404188632965088
Epoch 220, val loss: 0.8869630098342896
Epoch 230, training loss: 0.5066817998886108 = 0.49928444623947144 + 0.001 * 7.397372722625732
Epoch 230, val loss: 0.8693640232086182
Epoch 240, training loss: 0.4623079001903534 = 0.4549197852611542 + 0.001 * 7.388112545013428
Epoch 240, val loss: 0.8572505712509155
Epoch 250, training loss: 0.4214942455291748 = 0.4141179919242859 + 0.001 * 7.376245498657227
Epoch 250, val loss: 0.850530207157135
Epoch 260, training loss: 0.3838006556034088 = 0.376443088054657 + 0.001 * 7.357568740844727
Epoch 260, val loss: 0.8486592769622803
Epoch 270, training loss: 0.3488102853298187 = 0.3414725065231323 + 0.001 * 7.337785720825195
Epoch 270, val loss: 0.8509312272071838
Epoch 280, training loss: 0.3160760700702667 = 0.3087567090988159 + 0.001 * 7.319363117218018
Epoch 280, val loss: 0.856819212436676
Epoch 290, training loss: 0.2851656973361969 = 0.27786725759506226 + 0.001 * 7.298439979553223
Epoch 290, val loss: 0.8658139705657959
Epoch 300, training loss: 0.25586584210395813 = 0.24857261776924133 + 0.001 * 7.293234825134277
Epoch 300, val loss: 0.8775795698165894
Epoch 310, training loss: 0.2282128930091858 = 0.2209288775920868 + 0.001 * 7.2840142250061035
Epoch 310, val loss: 0.8915059566497803
Epoch 320, training loss: 0.2025013267993927 = 0.19521765410900116 + 0.001 * 7.28366756439209
Epoch 320, val loss: 0.9075984954833984
Epoch 330, training loss: 0.17906713485717773 = 0.17178753018379211 + 0.001 * 7.279606342315674
Epoch 330, val loss: 0.9259161949157715
Epoch 340, training loss: 0.15810464322566986 = 0.15082518756389618 + 0.001 * 7.279455184936523
Epoch 340, val loss: 0.9462939500808716
Epoch 350, training loss: 0.13962015509605408 = 0.13233916461467743 + 0.001 * 7.280988693237305
Epoch 350, val loss: 0.9683718085289001
Epoch 360, training loss: 0.12349091470241547 = 0.11620724946260452 + 0.001 * 7.283666610717773
Epoch 360, val loss: 0.9919743537902832
Epoch 370, training loss: 0.10950762033462524 = 0.10222503542900085 + 0.001 * 7.282581806182861
Epoch 370, val loss: 1.0165971517562866
Epoch 380, training loss: 0.09743174910545349 = 0.09015145897865295 + 0.001 * 7.28028678894043
Epoch 380, val loss: 1.0418105125427246
Epoch 390, training loss: 0.0870198905467987 = 0.07973872870206833 + 0.001 * 7.281158924102783
Epoch 390, val loss: 1.067222237586975
Epoch 400, training loss: 0.07803083211183548 = 0.0707482248544693 + 0.001 * 7.2826056480407715
Epoch 400, val loss: 1.092587947845459
Epoch 410, training loss: 0.07024196535348892 = 0.06296244263648987 + 0.001 * 7.279520511627197
Epoch 410, val loss: 1.1175432205200195
Epoch 420, training loss: 0.06347069889307022 = 0.05619213730096817 + 0.001 * 7.278561115264893
Epoch 420, val loss: 1.1420825719833374
Epoch 430, training loss: 0.057571351528167725 = 0.05029123276472092 + 0.001 * 7.280116558074951
Epoch 430, val loss: 1.1660985946655273
Epoch 440, training loss: 0.05241747945547104 = 0.04514000192284584 + 0.001 * 7.2774763107299805
Epoch 440, val loss: 1.1896142959594727
Epoch 450, training loss: 0.04791167750954628 = 0.040637411177158356 + 0.001 * 7.274266242980957
Epoch 450, val loss: 1.2124322652816772
Epoch 460, training loss: 0.04397229850292206 = 0.03669668734073639 + 0.001 * 7.275611877441406
Epoch 460, val loss: 1.234479308128357
Epoch 470, training loss: 0.040517572313547134 = 0.033242568373680115 + 0.001 * 7.2750043869018555
Epoch 470, val loss: 1.2557282447814941
Epoch 480, training loss: 0.03747821971774101 = 0.03020879626274109 + 0.001 * 7.269423484802246
Epoch 480, val loss: 1.2761467695236206
Epoch 490, training loss: 0.034803781658411026 = 0.027538498863577843 + 0.001 * 7.265281677246094
Epoch 490, val loss: 1.295736312866211
Epoch 500, training loss: 0.0324491485953331 = 0.025182198733091354 + 0.001 * 7.26694917678833
Epoch 500, val loss: 1.314518690109253
Epoch 510, training loss: 0.030362173914909363 = 0.023097289726138115 + 0.001 * 7.264884948730469
Epoch 510, val loss: 1.3325221538543701
Epoch 520, training loss: 0.028503982350230217 = 0.021246882155537605 + 0.001 * 7.257099628448486
Epoch 520, val loss: 1.349769949913025
Epoch 530, training loss: 0.026878533884882927 = 0.019597694277763367 + 0.001 * 7.280838966369629
Epoch 530, val loss: 1.3664062023162842
Epoch 540, training loss: 0.025376226752996445 = 0.018122298642992973 + 0.001 * 7.253927230834961
Epoch 540, val loss: 1.3822572231292725
Epoch 550, training loss: 0.02404797077178955 = 0.01679741032421589 + 0.001 * 7.250560760498047
Epoch 550, val loss: 1.397552728652954
Epoch 560, training loss: 0.022846750915050507 = 0.015603632666170597 + 0.001 * 7.2431182861328125
Epoch 560, val loss: 1.412250280380249
Epoch 570, training loss: 0.021771341562271118 = 0.01452525332570076 + 0.001 * 7.246087074279785
Epoch 570, val loss: 1.4262927770614624
Epoch 580, training loss: 0.02080257050693035 = 0.013549691066145897 + 0.001 * 7.2528791427612305
Epoch 580, val loss: 1.4397447109222412
Epoch 590, training loss: 0.019898751750588417 = 0.012665157206356525 + 0.001 * 7.233593940734863
Epoch 590, val loss: 1.4526615142822266
Epoch 600, training loss: 0.01909128949046135 = 0.011861505918204784 + 0.001 * 7.22978401184082
Epoch 600, val loss: 1.465013861656189
Epoch 610, training loss: 0.01835092343389988 = 0.011129981838166714 + 0.001 * 7.220941066741943
Epoch 610, val loss: 1.476904273033142
Epoch 620, training loss: 0.017682332545518875 = 0.010462929494678974 + 0.001 * 7.219403266906738
Epoch 620, val loss: 1.4882749319076538
Epoch 630, training loss: 0.01707204431295395 = 0.00985369086265564 + 0.001 * 7.218353748321533
Epoch 630, val loss: 1.4992245435714722
Epoch 640, training loss: 0.01661185920238495 = 0.009296105243265629 + 0.001 * 7.31575345993042
Epoch 640, val loss: 1.5097576379776
Epoch 650, training loss: 0.016014419496059418 = 0.008785071782767773 + 0.001 * 7.2293477058410645
Epoch 650, val loss: 1.519878625869751
Epoch 660, training loss: 0.015530739910900593 = 0.008315837942063808 + 0.001 * 7.214901447296143
Epoch 660, val loss: 1.529647946357727
Epoch 670, training loss: 0.015109589323401451 = 0.007883977144956589 + 0.001 * 7.225611209869385
Epoch 670, val loss: 1.5390288829803467
Epoch 680, training loss: 0.014698979444801807 = 0.007485810201615095 + 0.001 * 7.213169097900391
Epoch 680, val loss: 1.5480629205703735
Epoch 690, training loss: 0.014330725185573101 = 0.007118122652173042 + 0.001 * 7.212602138519287
Epoch 690, val loss: 1.5568208694458008
Epoch 700, training loss: 0.013975026085972786 = 0.006778043694794178 + 0.001 * 7.196981430053711
Epoch 700, val loss: 1.5652484893798828
Epoch 710, training loss: 0.013647841289639473 = 0.006462922785431147 + 0.001 * 7.18491792678833
Epoch 710, val loss: 1.5734047889709473
Epoch 720, training loss: 0.01333867758512497 = 0.0061703938990831375 + 0.001 * 7.168282985687256
Epoch 720, val loss: 1.5813088417053223
Epoch 730, training loss: 0.013082017190754414 = 0.0058984956704080105 + 0.001 * 7.183521270751953
Epoch 730, val loss: 1.588937520980835
Epoch 740, training loss: 0.01281983032822609 = 0.005645305383950472 + 0.001 * 7.174525260925293
Epoch 740, val loss: 1.5963237285614014
Epoch 750, training loss: 0.012585984542965889 = 0.005409162491559982 + 0.001 * 7.176821231842041
Epoch 750, val loss: 1.6034820079803467
Epoch 760, training loss: 0.012363114394247532 = 0.005188621114939451 + 0.001 * 7.174492835998535
Epoch 760, val loss: 1.6104193925857544
Epoch 770, training loss: 0.012177344411611557 = 0.004982356913387775 + 0.001 * 7.194987773895264
Epoch 770, val loss: 1.6171271800994873
Epoch 780, training loss: 0.011960795149207115 = 0.004789249040186405 + 0.001 * 7.17154598236084
Epoch 780, val loss: 1.6236776113510132
Epoch 790, training loss: 0.011792249977588654 = 0.004608092829585075 + 0.001 * 7.184156894683838
Epoch 790, val loss: 1.6299999952316284
Epoch 800, training loss: 0.011598555371165276 = 0.004437997471541166 + 0.001 * 7.160557746887207
Epoch 800, val loss: 1.6362019777297974
Epoch 810, training loss: 0.011420294642448425 = 0.004278083331882954 + 0.001 * 7.142210960388184
Epoch 810, val loss: 1.6421482563018799
Epoch 820, training loss: 0.011266529560089111 = 0.004127549938857555 + 0.001 * 7.138979911804199
Epoch 820, val loss: 1.6479690074920654
Epoch 830, training loss: 0.011126682162284851 = 0.003985678777098656 + 0.001 * 7.141002655029297
Epoch 830, val loss: 1.6535894870758057
Epoch 840, training loss: 0.01100640557706356 = 0.0038518677465617657 + 0.001 * 7.154538154602051
Epoch 840, val loss: 1.6590940952301025
Epoch 850, training loss: 0.01087173167616129 = 0.0037255019415169954 + 0.001 * 7.1462297439575195
Epoch 850, val loss: 1.6644201278686523
Epoch 860, training loss: 0.010750822722911835 = 0.003606007434427738 + 0.001 * 7.144815444946289
Epoch 860, val loss: 1.6696051359176636
Epoch 870, training loss: 0.010618885047733784 = 0.0034929276444017887 + 0.001 * 7.125957012176514
Epoch 870, val loss: 1.674656867980957
Epoch 880, training loss: 0.010541483759880066 = 0.00338579248636961 + 0.001 * 7.155691623687744
Epoch 880, val loss: 1.6795616149902344
Epoch 890, training loss: 0.01043287105858326 = 0.0032842308282852173 + 0.001 * 7.148640155792236
Epoch 890, val loss: 1.684388279914856
Epoch 900, training loss: 0.01033719815313816 = 0.0031878070440143347 + 0.001 * 7.149390697479248
Epoch 900, val loss: 1.689035415649414
Epoch 910, training loss: 0.010226392187178135 = 0.003096195636317134 + 0.001 * 7.130196571350098
Epoch 910, val loss: 1.6935937404632568
Epoch 920, training loss: 0.01011100597679615 = 0.003009035252034664 + 0.001 * 7.101970672607422
Epoch 920, val loss: 1.6980409622192383
Epoch 930, training loss: 0.01003712136298418 = 0.002925976412370801 + 0.001 * 7.11114501953125
Epoch 930, val loss: 1.7023587226867676
Epoch 940, training loss: 0.009965505450963974 = 0.002846685005351901 + 0.001 * 7.118819713592529
Epoch 940, val loss: 1.7066301107406616
Epoch 950, training loss: 0.009885701350867748 = 0.0027707351837307215 + 0.001 * 7.114965438842773
Epoch 950, val loss: 1.7107253074645996
Epoch 960, training loss: 0.009815436787903309 = 0.0026977485977113247 + 0.001 * 7.117687702178955
Epoch 960, val loss: 1.7147241830825806
Epoch 970, training loss: 0.009740286506712437 = 0.0026273366529494524 + 0.001 * 7.112949371337891
Epoch 970, val loss: 1.7186121940612793
Epoch 980, training loss: 0.009646624326705933 = 0.002559392713010311 + 0.001 * 7.087230682373047
Epoch 980, val loss: 1.722421646118164
Epoch 990, training loss: 0.009585984982550144 = 0.0024936648551374674 + 0.001 * 7.092319488525391
Epoch 990, val loss: 1.7260773181915283
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8028465998945704
=== training gcn model ===
Epoch 0, training loss: 1.9621838331222534 = 1.9535870552062988 + 0.001 * 8.59682559967041
Epoch 0, val loss: 1.9376788139343262
Epoch 10, training loss: 1.9517097473144531 = 1.9431129693984985 + 0.001 * 8.59677505493164
Epoch 10, val loss: 1.9276549816131592
Epoch 20, training loss: 1.938963532447815 = 1.93036687374115 + 0.001 * 8.596610069274902
Epoch 20, val loss: 1.9149898290634155
Epoch 30, training loss: 1.921441912651062 = 1.9128456115722656 + 0.001 * 8.596245765686035
Epoch 30, val loss: 1.8972941637039185
Epoch 40, training loss: 1.8961403369903564 = 1.8875449895858765 + 0.001 * 8.595393180847168
Epoch 40, val loss: 1.8720124959945679
Epoch 50, training loss: 1.8613988161087036 = 1.8528058528900146 + 0.001 * 8.592945098876953
Epoch 50, val loss: 1.8390873670578003
Epoch 60, training loss: 1.8222488164901733 = 1.8136646747589111 + 0.001 * 8.584169387817383
Epoch 60, val loss: 1.8067270517349243
Epoch 70, training loss: 1.789242148399353 = 1.7806954383850098 + 0.001 * 8.54673957824707
Epoch 70, val loss: 1.784850001335144
Epoch 80, training loss: 1.7537624835968018 = 1.7454668283462524 + 0.001 * 8.295685768127441
Epoch 80, val loss: 1.7575359344482422
Epoch 90, training loss: 1.7049988508224487 = 1.69686758518219 + 0.001 * 8.131314277648926
Epoch 90, val loss: 1.7134883403778076
Epoch 100, training loss: 1.63771653175354 = 1.6297625303268433 + 0.001 * 7.954036712646484
Epoch 100, val loss: 1.6523956060409546
Epoch 110, training loss: 1.5513967275619507 = 1.5437190532684326 + 0.001 * 7.677614688873291
Epoch 110, val loss: 1.5780476331710815
Epoch 120, training loss: 1.4522961378097534 = 1.4447641372680664 + 0.001 * 7.532052040100098
Epoch 120, val loss: 1.4946612119674683
Epoch 130, training loss: 1.346764087677002 = 1.3392813205718994 + 0.001 * 7.482807636260986
Epoch 130, val loss: 1.4085544347763062
Epoch 140, training loss: 1.2383850812911987 = 1.2309404611587524 + 0.001 * 7.444653034210205
Epoch 140, val loss: 1.3246911764144897
Epoch 150, training loss: 1.129866123199463 = 1.1224371194839478 + 0.001 * 7.429018020629883
Epoch 150, val loss: 1.2443987131118774
Epoch 160, training loss: 1.0247849225997925 = 1.0173604488372803 + 0.001 * 7.424431800842285
Epoch 160, val loss: 1.1695526838302612
Epoch 170, training loss: 0.9263765811920166 = 0.9189565181732178 + 0.001 * 7.420066833496094
Epoch 170, val loss: 1.1019819974899292
Epoch 180, training loss: 0.8368088603019714 = 0.8293958902359009 + 0.001 * 7.412998676300049
Epoch 180, val loss: 1.0422508716583252
Epoch 190, training loss: 0.757307767868042 = 0.7499077916145325 + 0.001 * 7.399978160858154
Epoch 190, val loss: 0.9914288520812988
Epoch 200, training loss: 0.6877394914627075 = 0.6803669929504395 + 0.001 * 7.372497081756592
Epoch 200, val loss: 0.9492683410644531
Epoch 210, training loss: 0.6265574097633362 = 0.619235634803772 + 0.001 * 7.321763515472412
Epoch 210, val loss: 0.9149631857872009
Epoch 220, training loss: 0.5717772841453552 = 0.5644988417625427 + 0.001 * 7.278464317321777
Epoch 220, val loss: 0.8870895504951477
Epoch 230, training loss: 0.5216624736785889 = 0.5144184827804565 + 0.001 * 7.244006156921387
Epoch 230, val loss: 0.8646795749664307
Epoch 240, training loss: 0.4750841557979584 = 0.46785640716552734 + 0.001 * 7.22774076461792
Epoch 240, val loss: 0.8471313118934631
Epoch 250, training loss: 0.4312399625778198 = 0.4240145981311798 + 0.001 * 7.225356578826904
Epoch 250, val loss: 0.833832323551178
Epoch 260, training loss: 0.3895898759365082 = 0.38236764073371887 + 0.001 * 7.222235679626465
Epoch 260, val loss: 0.8239986300468445
Epoch 270, training loss: 0.34972602128982544 = 0.34250688552856445 + 0.001 * 7.219123363494873
Epoch 270, val loss: 0.81684410572052
Epoch 280, training loss: 0.31138166785240173 = 0.30415382981300354 + 0.001 * 7.227832317352295
Epoch 280, val loss: 0.8119222521781921
Epoch 290, training loss: 0.27455076575279236 = 0.2673327922821045 + 0.001 * 7.217977523803711
Epoch 290, val loss: 0.8088898062705994
Epoch 300, training loss: 0.23973679542541504 = 0.2325238138437271 + 0.001 * 7.21297550201416
Epoch 300, val loss: 0.8081976175308228
Epoch 310, training loss: 0.20771408081054688 = 0.200503870844841 + 0.001 * 7.210208892822266
Epoch 310, val loss: 0.8101447224617004
Epoch 320, training loss: 0.1791611909866333 = 0.17195317149162292 + 0.001 * 7.20802640914917
Epoch 320, val loss: 0.8154348731040955
Epoch 330, training loss: 0.15434861183166504 = 0.1471436321735382 + 0.001 * 7.20497989654541
Epoch 330, val loss: 0.8238723278045654
Epoch 340, training loss: 0.13316984474658966 = 0.12596867978572845 + 0.001 * 7.201167583465576
Epoch 340, val loss: 0.8354159593582153
Epoch 350, training loss: 0.11531016230583191 = 0.10809700936079025 + 0.001 * 7.213155746459961
Epoch 350, val loss: 0.8494654297828674
Epoch 360, training loss: 0.10029786080121994 = 0.09310033172369003 + 0.001 * 7.197531700134277
Epoch 360, val loss: 0.8654531240463257
Epoch 370, training loss: 0.08773336559534073 = 0.08053971827030182 + 0.001 * 7.193643569946289
Epoch 370, val loss: 0.8828576803207397
Epoch 380, training loss: 0.07721029967069626 = 0.07001888006925583 + 0.001 * 7.191416263580322
Epoch 380, val loss: 0.9012152552604675
Epoch 390, training loss: 0.0683823823928833 = 0.061200011521577835 + 0.001 * 7.182368755340576
Epoch 390, val loss: 0.9200829267501831
Epoch 400, training loss: 0.06096580624580383 = 0.05378997325897217 + 0.001 * 7.175834655761719
Epoch 400, val loss: 0.9392167329788208
Epoch 410, training loss: 0.05472484230995178 = 0.04754429683089256 + 0.001 * 7.180544853210449
Epoch 410, val loss: 0.9583469033241272
Epoch 420, training loss: 0.04943494871258736 = 0.04226016625761986 + 0.001 * 7.174783706665039
Epoch 420, val loss: 0.9772937893867493
Epoch 430, training loss: 0.044932104647159576 = 0.03776981309056282 + 0.001 * 7.162289619445801
Epoch 430, val loss: 0.9959982633590698
Epoch 440, training loss: 0.04109983891248703 = 0.03393619880080223 + 0.001 * 7.163639545440674
Epoch 440, val loss: 1.0143234729766846
Epoch 450, training loss: 0.03779509663581848 = 0.030646109953522682 + 0.001 * 7.148988246917725
Epoch 450, val loss: 1.0322060585021973
Epoch 460, training loss: 0.03496086597442627 = 0.027807967737317085 + 0.001 * 7.152895927429199
Epoch 460, val loss: 1.049638271331787
Epoch 470, training loss: 0.032545313239097595 = 0.02534698322415352 + 0.001 * 7.198328495025635
Epoch 470, val loss: 1.0665167570114136
Epoch 480, training loss: 0.03034118190407753 = 0.023201556876301765 + 0.001 * 7.139623641967773
Epoch 480, val loss: 1.0829391479492188
Epoch 490, training loss: 0.02846185490489006 = 0.021321432664990425 + 0.001 * 7.140420913696289
Epoch 490, val loss: 1.0988184213638306
Epoch 500, training loss: 0.02679692953824997 = 0.0196656733751297 + 0.001 * 7.131256103515625
Epoch 500, val loss: 1.1141948699951172
Epoch 510, training loss: 0.025335874408483505 = 0.018200572580099106 + 0.001 * 7.13530158996582
Epoch 510, val loss: 1.1291147470474243
Epoch 520, training loss: 0.02402246557176113 = 0.016898447647690773 + 0.001 * 7.12401819229126
Epoch 520, val loss: 1.1435561180114746
Epoch 530, training loss: 0.022888757288455963 = 0.015736090019345284 + 0.001 * 7.152667999267578
Epoch 530, val loss: 1.1575477123260498
Epoch 540, training loss: 0.021827340126037598 = 0.014694341458380222 + 0.001 * 7.132998466491699
Epoch 540, val loss: 1.171099305152893
Epoch 550, training loss: 0.020884772762656212 = 0.01375720463693142 + 0.001 * 7.127568244934082
Epoch 550, val loss: 1.1842517852783203
Epoch 560, training loss: 0.020033782348036766 = 0.01291114091873169 + 0.001 * 7.122640609741211
Epoch 560, val loss: 1.1970003843307495
Epoch 570, training loss: 0.01925469934940338 = 0.012144745327532291 + 0.001 * 7.109954833984375
Epoch 570, val loss: 1.209357500076294
Epoch 580, training loss: 0.01857655867934227 = 0.011448296718299389 + 0.001 * 7.128262042999268
Epoch 580, val loss: 1.22136390209198
Epoch 590, training loss: 0.017923887819051743 = 0.010813499800860882 + 0.001 * 7.11038875579834
Epoch 590, val loss: 1.233009696006775
Epoch 600, training loss: 0.017338154837489128 = 0.010233242064714432 + 0.001 * 7.104911804199219
Epoch 600, val loss: 1.244315505027771
Epoch 610, training loss: 0.016817700117826462 = 0.009701461531221867 + 0.001 * 7.116239070892334
Epoch 610, val loss: 1.255300760269165
Epoch 620, training loss: 0.016325542703270912 = 0.00921289436519146 + 0.001 * 7.1126484870910645
Epoch 620, val loss: 1.265995740890503
Epoch 630, training loss: 0.01585843414068222 = 0.008762914687395096 + 0.001 * 7.095519065856934
Epoch 630, val loss: 1.276381015777588
Epoch 640, training loss: 0.015471447259187698 = 0.008347561582922935 + 0.001 * 7.123885631561279
Epoch 640, val loss: 1.286495327949524
Epoch 650, training loss: 0.01506009604781866 = 0.007963348180055618 + 0.001 * 7.096747398376465
Epoch 650, val loss: 1.296332597732544
Epoch 660, training loss: 0.014702348969876766 = 0.0076071699149906635 + 0.001 * 7.095178604125977
Epoch 660, val loss: 1.3058979511260986
Epoch 670, training loss: 0.014388708397746086 = 0.0072763534262776375 + 0.001 * 7.1123552322387695
Epoch 670, val loss: 1.31521737575531
Epoch 680, training loss: 0.014070643112063408 = 0.006968550384044647 + 0.001 * 7.1020917892456055
Epoch 680, val loss: 1.324307918548584
Epoch 690, training loss: 0.013768594712018967 = 0.006681598722934723 + 0.001 * 7.086996078491211
Epoch 690, val loss: 1.333160638809204
Epoch 700, training loss: 0.013526412658393383 = 0.006413666531443596 + 0.001 * 7.112745761871338
Epoch 700, val loss: 1.3418062925338745
Epoch 710, training loss: 0.013257171958684921 = 0.0061630913987755775 + 0.001 * 7.094079971313477
Epoch 710, val loss: 1.3502362966537476
Epoch 720, training loss: 0.013018578290939331 = 0.005928391125053167 + 0.001 * 7.090187072753906
Epoch 720, val loss: 1.3584563732147217
Epoch 730, training loss: 0.012789719738066196 = 0.005708238109946251 + 0.001 * 7.081481456756592
Epoch 730, val loss: 1.3664835691452026
Epoch 740, training loss: 0.012585864402353764 = 0.005501443054527044 + 0.001 * 7.084421157836914
Epoch 740, val loss: 1.3743298053741455
Epoch 750, training loss: 0.012381250038743019 = 0.005306954495608807 + 0.001 * 7.0742950439453125
Epoch 750, val loss: 1.3819915056228638
Epoch 760, training loss: 0.012197148986160755 = 0.005123675800859928 + 0.001 * 7.07347297668457
Epoch 760, val loss: 1.3894834518432617
Epoch 770, training loss: 0.01203526183962822 = 0.00495071429759264 + 0.001 * 7.0845465660095215
Epoch 770, val loss: 1.3967974185943604
Epoch 780, training loss: 0.011875243857502937 = 0.004787292797118425 + 0.001 * 7.087951183319092
Epoch 780, val loss: 1.4039456844329834
Epoch 790, training loss: 0.011697518639266491 = 0.004632583353668451 + 0.001 * 7.064934730529785
Epoch 790, val loss: 1.4109668731689453
Epoch 800, training loss: 0.011560165323317051 = 0.004485825076699257 + 0.001 * 7.074339866638184
Epoch 800, val loss: 1.4178205728530884
Epoch 810, training loss: 0.011430160142481327 = 0.004346286412328482 + 0.001 * 7.083873271942139
Epoch 810, val loss: 1.4245389699935913
Epoch 820, training loss: 0.011278316378593445 = 0.004213256761431694 + 0.001 * 7.065058708190918
Epoch 820, val loss: 1.4311468601226807
Epoch 830, training loss: 0.01115725003182888 = 0.004086154513061047 + 0.001 * 7.0710954666137695
Epoch 830, val loss: 1.4376250505447388
Epoch 840, training loss: 0.011035826988518238 = 0.003964452538639307 + 0.001 * 7.07137393951416
Epoch 840, val loss: 1.4439754486083984
Epoch 850, training loss: 0.0109094874933362 = 0.0038477752823382616 + 0.001 * 7.061712265014648
Epoch 850, val loss: 1.4502402544021606
Epoch 860, training loss: 0.010798640549182892 = 0.0037357821129262447 + 0.001 * 7.0628581047058105
Epoch 860, val loss: 1.456396460533142
Epoch 870, training loss: 0.010684662498533726 = 0.003628216916695237 + 0.001 * 7.056445121765137
Epoch 870, val loss: 1.462430477142334
Epoch 880, training loss: 0.010582263581454754 = 0.003524862229824066 + 0.001 * 7.057401180267334
Epoch 880, val loss: 1.468383550643921
Epoch 890, training loss: 0.010501820594072342 = 0.0034255553036928177 + 0.001 * 7.076265335083008
Epoch 890, val loss: 1.4742681980133057
Epoch 900, training loss: 0.010383150540292263 = 0.0033302081283181906 + 0.001 * 7.052941799163818
Epoch 900, val loss: 1.4800223112106323
Epoch 910, training loss: 0.010284017771482468 = 0.003238649806007743 + 0.001 * 7.045367240905762
Epoch 910, val loss: 1.4856866598129272
Epoch 920, training loss: 0.010194225236773491 = 0.0031507136300206184 + 0.001 * 7.043511867523193
Epoch 920, val loss: 1.4912787675857544
Epoch 930, training loss: 0.01011066697537899 = 0.0030662170611321926 + 0.001 * 7.044449806213379
Epoch 930, val loss: 1.4967670440673828
Epoch 940, training loss: 0.010026260279119015 = 0.002984865102916956 + 0.001 * 7.0413947105407715
Epoch 940, val loss: 1.5021623373031616
Epoch 950, training loss: 0.009945951402187347 = 0.0029062528628855944 + 0.001 * 7.039698600769043
Epoch 950, val loss: 1.5075280666351318
Epoch 960, training loss: 0.009876120835542679 = 0.0028295842930674553 + 0.001 * 7.046535968780518
Epoch 960, val loss: 1.512826919555664
Epoch 970, training loss: 0.00978818628937006 = 0.0027542293537408113 + 0.001 * 7.033956527709961
Epoch 970, val loss: 1.5182392597198486
Epoch 980, training loss: 0.009723708964884281 = 0.0026798758190125227 + 0.001 * 7.043832778930664
Epoch 980, val loss: 1.5237741470336914
Epoch 990, training loss: 0.009662887081503868 = 0.0026065714191645384 + 0.001 * 7.0563154220581055
Epoch 990, val loss: 1.5293368101119995
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 1.9701988697052002 = 1.961601972579956 + 0.001 * 8.59684944152832
Epoch 0, val loss: 1.9679509401321411
Epoch 10, training loss: 1.9594706296920776 = 1.950873851776123 + 0.001 * 8.596808433532715
Epoch 10, val loss: 1.9572592973709106
Epoch 20, training loss: 1.9467040300369263 = 1.9381073713302612 + 0.001 * 8.596648216247559
Epoch 20, val loss: 1.9445099830627441
Epoch 30, training loss: 1.9293525218963623 = 1.9207563400268555 + 0.001 * 8.59623908996582
Epoch 30, val loss: 1.9273802042007446
Epoch 40, training loss: 1.904344916343689 = 1.8957496881484985 + 0.001 * 8.595197677612305
Epoch 40, val loss: 1.9031177759170532
Epoch 50, training loss: 1.8691339492797852 = 1.860541820526123 + 0.001 * 8.592171669006348
Epoch 50, val loss: 1.8704090118408203
Epoch 60, training loss: 1.8276846408843994 = 1.8191030025482178 + 0.001 * 8.5816068649292
Epoch 60, val loss: 1.835060715675354
Epoch 70, training loss: 1.792041540145874 = 1.7835043668746948 + 0.001 * 8.537121772766113
Epoch 70, val loss: 1.8055654764175415
Epoch 80, training loss: 1.7559177875518799 = 1.7476327419281006 + 0.001 * 8.284985542297363
Epoch 80, val loss: 1.7707575559616089
Epoch 90, training loss: 1.7059173583984375 = 1.6977593898773193 + 0.001 * 8.157997131347656
Epoch 90, val loss: 1.7259454727172852
Epoch 100, training loss: 1.6365700960159302 = 1.6285662651062012 + 0.001 * 8.003863334655762
Epoch 100, val loss: 1.667859435081482
Epoch 110, training loss: 1.546239972114563 = 1.5384007692337036 + 0.001 * 7.839207649230957
Epoch 110, val loss: 1.5917500257492065
Epoch 120, training loss: 1.443594217300415 = 1.4358996152877808 + 0.001 * 7.6945881843566895
Epoch 120, val loss: 1.505842685699463
Epoch 130, training loss: 1.3387778997421265 = 1.3311054706573486 + 0.001 * 7.672372341156006
Epoch 130, val loss: 1.4184643030166626
Epoch 140, training loss: 1.2342008352279663 = 1.22657310962677 + 0.001 * 7.627718925476074
Epoch 140, val loss: 1.3346843719482422
Epoch 150, training loss: 1.129909634590149 = 1.1223301887512207 + 0.001 * 7.579414367675781
Epoch 150, val loss: 1.2538726329803467
Epoch 160, training loss: 1.0273593664169312 = 1.0198426246643066 + 0.001 * 7.5167155265808105
Epoch 160, val loss: 1.177614688873291
Epoch 170, training loss: 0.9298733472824097 = 0.9224066138267517 + 0.001 * 7.466725826263428
Epoch 170, val loss: 1.107877492904663
Epoch 180, training loss: 0.8416805863380432 = 0.8342335224151611 + 0.001 * 7.447091102600098
Epoch 180, val loss: 1.0473394393920898
Epoch 190, training loss: 0.7654851675033569 = 0.7580491900444031 + 0.001 * 7.435952186584473
Epoch 190, val loss: 0.9978211522102356
Epoch 200, training loss: 0.7007986307144165 = 0.6933691501617432 + 0.001 * 7.4294538497924805
Epoch 200, val loss: 0.9588637948036194
Epoch 210, training loss: 0.6444454789161682 = 0.6370212435722351 + 0.001 * 7.424224853515625
Epoch 210, val loss: 0.9281603693962097
Epoch 220, training loss: 0.5926554203033447 = 0.5852466225624084 + 0.001 * 7.408812999725342
Epoch 220, val loss: 0.9034555554389954
Epoch 230, training loss: 0.5430188179016113 = 0.5356243252754211 + 0.001 * 7.394477844238281
Epoch 230, val loss: 0.8831443190574646
Epoch 240, training loss: 0.4946741759777069 = 0.48729994893074036 + 0.001 * 7.374239444732666
Epoch 240, val loss: 0.8669580221176147
Epoch 250, training loss: 0.44774070382118225 = 0.44038069248199463 + 0.001 * 7.360015392303467
Epoch 250, val loss: 0.8550301790237427
Epoch 260, training loss: 0.4025619626045227 = 0.3952323794364929 + 0.001 * 7.329597473144531
Epoch 260, val loss: 0.8474573493003845
Epoch 270, training loss: 0.3596911132335663 = 0.3523820638656616 + 0.001 * 7.309037685394287
Epoch 270, val loss: 0.844177782535553
Epoch 280, training loss: 0.31961923837661743 = 0.3123176693916321 + 0.001 * 7.301569938659668
Epoch 280, val loss: 0.8450655341148376
Epoch 290, training loss: 0.2827434241771698 = 0.27545708417892456 + 0.001 * 7.286347389221191
Epoch 290, val loss: 0.8498856425285339
Epoch 300, training loss: 0.24939176440238953 = 0.24210810661315918 + 0.001 * 7.283658981323242
Epoch 300, val loss: 0.8586791157722473
Epoch 310, training loss: 0.21973437070846558 = 0.21245719492435455 + 0.001 * 7.277169227600098
Epoch 310, val loss: 0.8712566494941711
Epoch 320, training loss: 0.1937485933303833 = 0.18647393584251404 + 0.001 * 7.274660587310791
Epoch 320, val loss: 0.8873165845870972
Epoch 330, training loss: 0.17119920253753662 = 0.16392521560192108 + 0.001 * 7.273982524871826
Epoch 330, val loss: 0.9062615633010864
Epoch 340, training loss: 0.1516895294189453 = 0.1444205343723297 + 0.001 * 7.268989562988281
Epoch 340, val loss: 0.9274393916130066
Epoch 350, training loss: 0.13479575514793396 = 0.12752822041511536 + 0.001 * 7.267533302307129
Epoch 350, val loss: 0.9502292275428772
Epoch 360, training loss: 0.12011215090751648 = 0.11284839361906052 + 0.001 * 7.263754844665527
Epoch 360, val loss: 0.9741587042808533
Epoch 370, training loss: 0.10730995237827301 = 0.10004814714193344 + 0.001 * 7.261803150177002
Epoch 370, val loss: 0.998924195766449
Epoch 380, training loss: 0.096115842461586 = 0.08885589987039566 + 0.001 * 7.259940147399902
Epoch 380, val loss: 1.0241873264312744
Epoch 390, training loss: 0.08630993962287903 = 0.0790514126420021 + 0.001 * 7.25852632522583
Epoch 390, val loss: 1.0497652292251587
Epoch 400, training loss: 0.07770030200481415 = 0.0704534500837326 + 0.001 * 7.246853828430176
Epoch 400, val loss: 1.075455904006958
Epoch 410, training loss: 0.07015145570039749 = 0.06290873140096664 + 0.001 * 7.242725372314453
Epoch 410, val loss: 1.1011697053909302
Epoch 420, training loss: 0.06359799206256866 = 0.056287311017513275 + 0.001 * 7.3106794357299805
Epoch 420, val loss: 1.1266998052597046
Epoch 430, training loss: 0.05771661177277565 = 0.05047614127397537 + 0.001 * 7.240469932556152
Epoch 430, val loss: 1.1519407033920288
Epoch 440, training loss: 0.052609674632549286 = 0.04537597671151161 + 0.001 * 7.2336955070495605
Epoch 440, val loss: 1.1767855882644653
Epoch 450, training loss: 0.0481305830180645 = 0.04090012237429619 + 0.001 * 7.230460166931152
Epoch 450, val loss: 1.2011358737945557
Epoch 460, training loss: 0.04421136528253555 = 0.036971401423215866 + 0.001 * 7.239963054656982
Epoch 460, val loss: 1.2248986959457397
Epoch 470, training loss: 0.040749046951532364 = 0.03352054953575134 + 0.001 * 7.228496551513672
Epoch 470, val loss: 1.2480359077453613
Epoch 480, training loss: 0.03770500048995018 = 0.030485762283205986 + 0.001 * 7.21923828125
Epoch 480, val loss: 1.2705165147781372
Epoch 490, training loss: 0.035034552216529846 = 0.02781234122812748 + 0.001 * 7.222211837768555
Epoch 490, val loss: 1.292316198348999
Epoch 500, training loss: 0.032664842903614044 = 0.02545224502682686 + 0.001 * 7.212597846984863
Epoch 500, val loss: 1.3134492635726929
Epoch 510, training loss: 0.03058212250471115 = 0.023363878950476646 + 0.001 * 7.218242168426514
Epoch 510, val loss: 1.3338476419448853
Epoch 520, training loss: 0.028732361271977425 = 0.021511327475309372 + 0.001 * 7.221033096313477
Epoch 520, val loss: 1.3536138534545898
Epoch 530, training loss: 0.027069341391324997 = 0.01986331306397915 + 0.001 * 7.206028461456299
Epoch 530, val loss: 1.3727010488510132
Epoch 540, training loss: 0.02561374567449093 = 0.01839306764304638 + 0.001 * 7.220678329467773
Epoch 540, val loss: 1.3911465406417847
Epoch 550, training loss: 0.024295585229992867 = 0.01707756146788597 + 0.001 * 7.218023777008057
Epoch 550, val loss: 1.409013032913208
Epoch 560, training loss: 0.023101842030882835 = 0.015896780416369438 + 0.001 * 7.205061912536621
Epoch 560, val loss: 1.426255226135254
Epoch 570, training loss: 0.022021926939487457 = 0.014832040295004845 + 0.001 * 7.18988561630249
Epoch 570, val loss: 1.4429084062576294
Epoch 580, training loss: 0.02108004502952099 = 0.013866392895579338 + 0.001 * 7.213651180267334
Epoch 580, val loss: 1.4589077234268188
Epoch 590, training loss: 0.020176034420728683 = 0.012986564077436924 + 0.001 * 7.189468860626221
Epoch 590, val loss: 1.4742900133132935
Epoch 600, training loss: 0.01936977356672287 = 0.012182896956801414 + 0.001 * 7.186875343322754
Epoch 600, val loss: 1.489079475402832
Epoch 610, training loss: 0.01863616704940796 = 0.011447848752140999 + 0.001 * 7.188317775726318
Epoch 610, val loss: 1.5032840967178345
Epoch 620, training loss: 0.017968937754631042 = 0.010774899274110794 + 0.001 * 7.1940388679504395
Epoch 620, val loss: 1.5169613361358643
Epoch 630, training loss: 0.017339978367090225 = 0.010158058255910873 + 0.001 * 7.181920528411865
Epoch 630, val loss: 1.5301392078399658
Epoch 640, training loss: 0.016781799495220184 = 0.009592097252607346 + 0.001 * 7.18970251083374
Epoch 640, val loss: 1.5428155660629272
Epoch 650, training loss: 0.016251949593424797 = 0.00907222367823124 + 0.001 * 7.1797261238098145
Epoch 650, val loss: 1.5550264120101929
Epoch 660, training loss: 0.015763461589813232 = 0.008593863807618618 + 0.001 * 7.169597148895264
Epoch 660, val loss: 1.5667906999588013
Epoch 670, training loss: 0.01533015351742506 = 0.008153104223310947 + 0.001 * 7.177049160003662
Epoch 670, val loss: 1.5781649351119995
Epoch 680, training loss: 0.014945266768336296 = 0.007746307644993067 + 0.001 * 7.198958873748779
Epoch 680, val loss: 1.589124321937561
Epoch 690, training loss: 0.01454651914536953 = 0.007370299194008112 + 0.001 * 7.176219463348389
Epoch 690, val loss: 1.5997142791748047
Epoch 700, training loss: 0.014195600524544716 = 0.007022181525826454 + 0.001 * 7.173418998718262
Epoch 700, val loss: 1.6099461317062378
Epoch 710, training loss: 0.013845343142747879 = 0.006699306890368462 + 0.001 * 7.146035671234131
Epoch 710, val loss: 1.6198234558105469
Epoch 720, training loss: 0.01356123574078083 = 0.0063993544317781925 + 0.001 * 7.161881446838379
Epoch 720, val loss: 1.6293542385101318
Epoch 730, training loss: 0.013265266083180904 = 0.0061203897930681705 + 0.001 * 7.144876003265381
Epoch 730, val loss: 1.6385835409164429
Epoch 740, training loss: 0.013022184371948242 = 0.005860521458089352 + 0.001 * 7.1616621017456055
Epoch 740, val loss: 1.647513508796692
Epoch 750, training loss: 0.012761227786540985 = 0.005617998074740171 + 0.001 * 7.143229961395264
Epoch 750, val loss: 1.6561710834503174
Epoch 760, training loss: 0.012539312243461609 = 0.005391323938965797 + 0.001 * 7.1479878425598145
Epoch 760, val loss: 1.6645468473434448
Epoch 770, training loss: 0.012317400425672531 = 0.005179290194064379 + 0.001 * 7.13810920715332
Epoch 770, val loss: 1.6726675033569336
Epoch 780, training loss: 0.012122386135160923 = 0.004980639088898897 + 0.001 * 7.141746520996094
Epoch 780, val loss: 1.680488109588623
Epoch 790, training loss: 0.011920823715627193 = 0.004794271197170019 + 0.001 * 7.126552104949951
Epoch 790, val loss: 1.6880896091461182
Epoch 800, training loss: 0.011736994609236717 = 0.004619233310222626 + 0.001 * 7.117761611938477
Epoch 800, val loss: 1.695502758026123
Epoch 810, training loss: 0.011575577780604362 = 0.004454543814063072 + 0.001 * 7.121033191680908
Epoch 810, val loss: 1.7026500701904297
Epoch 820, training loss: 0.0114391865208745 = 0.004299471620470285 + 0.001 * 7.13971471786499
Epoch 820, val loss: 1.7096173763275146
Epoch 830, training loss: 0.011297844350337982 = 0.004153282381594181 + 0.001 * 7.144562244415283
Epoch 830, val loss: 1.7163492441177368
Epoch 840, training loss: 0.0111690703779459 = 0.004015346989035606 + 0.001 * 7.153722763061523
Epoch 840, val loss: 1.7229080200195312
Epoch 850, training loss: 0.011014498770236969 = 0.0038850153796374798 + 0.001 * 7.129483222961426
Epoch 850, val loss: 1.7292498350143433
Epoch 860, training loss: 0.010893091559410095 = 0.0037617534399032593 + 0.001 * 7.131338119506836
Epoch 860, val loss: 1.7354027032852173
Epoch 870, training loss: 0.010765373706817627 = 0.0036450866609811783 + 0.001 * 7.120286464691162
Epoch 870, val loss: 1.7413817644119263
Epoch 880, training loss: 0.01066865399479866 = 0.003534510964527726 + 0.001 * 7.1341423988342285
Epoch 880, val loss: 1.7472244501113892
Epoch 890, training loss: 0.010539223439991474 = 0.003429646836593747 + 0.001 * 7.109576225280762
Epoch 890, val loss: 1.7528661489486694
Epoch 900, training loss: 0.010439177975058556 = 0.0033301054500043392 + 0.001 * 7.109071731567383
Epoch 900, val loss: 1.7583658695220947
Epoch 910, training loss: 0.010323284193873405 = 0.0032355051953345537 + 0.001 * 7.087778568267822
Epoch 910, val loss: 1.7636934518814087
Epoch 920, training loss: 0.010255628265440464 = 0.0031455245334655046 + 0.001 * 7.110103130340576
Epoch 920, val loss: 1.768910527229309
Epoch 930, training loss: 0.010182151570916176 = 0.0030598784796893597 + 0.001 * 7.1222734451293945
Epoch 930, val loss: 1.7739664316177368
Epoch 940, training loss: 0.010092925280332565 = 0.0029783202335238457 + 0.001 * 7.114604473114014
Epoch 940, val loss: 1.7788523435592651
Epoch 950, training loss: 0.010016902349889278 = 0.0029006015975028276 + 0.001 * 7.116300106048584
Epoch 950, val loss: 1.7836520671844482
Epoch 960, training loss: 0.009908702224493027 = 0.0028264184948056936 + 0.001 * 7.0822834968566895
Epoch 960, val loss: 1.7882753610610962
Epoch 970, training loss: 0.009852256625890732 = 0.0027556056156754494 + 0.001 * 7.09665060043335
Epoch 970, val loss: 1.7927793264389038
Epoch 980, training loss: 0.009805628098547459 = 0.0026879324577748775 + 0.001 * 7.117695331573486
Epoch 980, val loss: 1.7971580028533936
Epoch 990, training loss: 0.009747753851115704 = 0.002623225562274456 + 0.001 * 7.124527931213379
Epoch 990, val loss: 1.8014518022537231
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.801265155508698
The final CL Acc:0.77531, 0.02463, The final GNN Acc:0.80373, 0.00245
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13184])
remove edge: torch.Size([2, 7820])
updated graph: torch.Size([2, 10448])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9632611274719238 = 1.9546643495559692 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.9526137113571167
Epoch 10, training loss: 1.9525232315063477 = 1.943926453590393 + 0.001 * 8.596778869628906
Epoch 10, val loss: 1.9424264430999756
Epoch 20, training loss: 1.9393044710159302 = 1.9307078123092651 + 0.001 * 8.596607208251953
Epoch 20, val loss: 1.9295378923416138
Epoch 30, training loss: 1.9208465814590454 = 1.9122503995895386 + 0.001 * 8.596227645874023
Epoch 30, val loss: 1.9111872911453247
Epoch 40, training loss: 1.8935270309448242 = 1.8849316835403442 + 0.001 * 8.59536075592041
Epoch 40, val loss: 1.884142518043518
Epoch 50, training loss: 1.8552056550979614 = 1.8466126918792725 + 0.001 * 8.592928886413574
Epoch 50, val loss: 1.8478477001190186
Epoch 60, training loss: 1.8126118183135986 = 1.8040276765823364 + 0.001 * 8.584166526794434
Epoch 60, val loss: 1.8116344213485718
Epoch 70, training loss: 1.7765289545059204 = 1.767983317375183 + 0.001 * 8.545647621154785
Epoch 70, val loss: 1.7827646732330322
Epoch 80, training loss: 1.732305645942688 = 1.7240183353424072 + 0.001 * 8.287347793579102
Epoch 80, val loss: 1.742113471031189
Epoch 90, training loss: 1.6708184480667114 = 1.6627637147903442 + 0.001 * 8.054705619812012
Epoch 90, val loss: 1.685995101928711
Epoch 100, training loss: 1.5876165628433228 = 1.5796514749526978 + 0.001 * 7.965064525604248
Epoch 100, val loss: 1.6133475303649902
Epoch 110, training loss: 1.4908753633499146 = 1.4830080270767212 + 0.001 * 7.867325305938721
Epoch 110, val loss: 1.5328892469406128
Epoch 120, training loss: 1.3925999402999878 = 1.3848671913146973 + 0.001 * 7.7327680587768555
Epoch 120, val loss: 1.4539484977722168
Epoch 130, training loss: 1.2982608079910278 = 1.2906244993209839 + 0.001 * 7.636340618133545
Epoch 130, val loss: 1.3807182312011719
Epoch 140, training loss: 1.2074666023254395 = 1.199935793876648 + 0.001 * 7.530811309814453
Epoch 140, val loss: 1.3112927675247192
Epoch 150, training loss: 1.1214169263839722 = 1.113983392715454 + 0.001 * 7.433485507965088
Epoch 150, val loss: 1.2464473247528076
Epoch 160, training loss: 1.0431829690933228 = 1.0357946157455444 + 0.001 * 7.388307571411133
Epoch 160, val loss: 1.188425898551941
Epoch 170, training loss: 0.9742317795753479 = 0.9668574333190918 + 0.001 * 7.3743672370910645
Epoch 170, val loss: 1.137846827507019
Epoch 180, training loss: 0.9126749038696289 = 0.9053221940994263 + 0.001 * 7.352729320526123
Epoch 180, val loss: 1.092403531074524
Epoch 190, training loss: 0.8548048734664917 = 0.8474842309951782 + 0.001 * 7.320626258850098
Epoch 190, val loss: 1.0485433340072632
Epoch 200, training loss: 0.7971860766410828 = 0.7899027466773987 + 0.001 * 7.283312797546387
Epoch 200, val loss: 1.0035278797149658
Epoch 210, training loss: 0.7381942868232727 = 0.7309312224388123 + 0.001 * 7.263057231903076
Epoch 210, val loss: 0.9569306373596191
Epoch 220, training loss: 0.6782134175300598 = 0.6709650754928589 + 0.001 * 7.2483696937561035
Epoch 220, val loss: 0.9103406667709351
Epoch 230, training loss: 0.6188557744026184 = 0.6116127967834473 + 0.001 * 7.242998123168945
Epoch 230, val loss: 0.8667573928833008
Epoch 240, training loss: 0.5615240931510925 = 0.554282009601593 + 0.001 * 7.2420806884765625
Epoch 240, val loss: 0.8285006880760193
Epoch 250, training loss: 0.5066599249839783 = 0.4994185268878937 + 0.001 * 7.241400241851807
Epoch 250, val loss: 0.7962558269500732
Epoch 260, training loss: 0.454033762216568 = 0.44679322838783264 + 0.001 * 7.240535736083984
Epoch 260, val loss: 0.7695767879486084
Epoch 270, training loss: 0.4034201204776764 = 0.39618048071861267 + 0.001 * 7.239650249481201
Epoch 270, val loss: 0.7472867369651794
Epoch 280, training loss: 0.3551410436630249 = 0.3479021191596985 + 0.001 * 7.238918781280518
Epoch 280, val loss: 0.728472888469696
Epoch 290, training loss: 0.31001439690589905 = 0.30277591943740845 + 0.001 * 7.238466262817383
Epoch 290, val loss: 0.7128202319145203
Epoch 300, training loss: 0.268940806388855 = 0.2617025375366211 + 0.001 * 7.238271236419678
Epoch 300, val loss: 0.7003943920135498
Epoch 310, training loss: 0.2325984090566635 = 0.22536003589630127 + 0.001 * 7.238365650177002
Epoch 310, val loss: 0.6914924383163452
Epoch 320, training loss: 0.20126429200172424 = 0.19402553141117096 + 0.001 * 7.2387614250183105
Epoch 320, val loss: 0.6863143444061279
Epoch 330, training loss: 0.1747824251651764 = 0.16754184663295746 + 0.001 * 7.24058198928833
Epoch 330, val loss: 0.6848292350769043
Epoch 340, training loss: 0.15263308584690094 = 0.14539285004138947 + 0.001 * 7.240231513977051
Epoch 340, val loss: 0.6866714358329773
Epoch 350, training loss: 0.13412043452262878 = 0.12687969207763672 + 0.001 * 7.24073600769043
Epoch 350, val loss: 0.691310703754425
Epoch 360, training loss: 0.1185535192489624 = 0.11131240427494049 + 0.001 * 7.241113185882568
Epoch 360, val loss: 0.6981487274169922
Epoch 370, training loss: 0.10534673929214478 = 0.09810353070497513 + 0.001 * 7.243210792541504
Epoch 370, val loss: 0.7066214680671692
Epoch 380, training loss: 0.09404224157333374 = 0.08680056035518646 + 0.001 * 7.241682529449463
Epoch 380, val loss: 0.7163169384002686
Epoch 390, training loss: 0.08429798483848572 = 0.07705610245466232 + 0.001 * 7.241880893707275
Epoch 390, val loss: 0.7268833518028259
Epoch 400, training loss: 0.07584801316261292 = 0.06860783696174622 + 0.001 * 7.240179538726807
Epoch 400, val loss: 0.7380970120429993
Epoch 410, training loss: 0.0684945210814476 = 0.061254583299160004 + 0.001 * 7.239940166473389
Epoch 410, val loss: 0.7497812509536743
Epoch 420, training loss: 0.06207604706287384 = 0.05483756214380264 + 0.001 * 7.238485813140869
Epoch 420, val loss: 0.7617676258087158
Epoch 430, training loss: 0.056464679539203644 = 0.04922838509082794 + 0.001 * 7.236293315887451
Epoch 430, val loss: 0.7739489078521729
Epoch 440, training loss: 0.05155259370803833 = 0.04431933909654617 + 0.001 * 7.233255386352539
Epoch 440, val loss: 0.7862541675567627
Epoch 450, training loss: 0.04725577309727669 = 0.04001787304878235 + 0.001 * 7.237898826599121
Epoch 450, val loss: 0.7986213564872742
Epoch 460, training loss: 0.04347237944602966 = 0.036242902278900146 + 0.001 * 7.229478359222412
Epoch 460, val loss: 0.8109544515609741
Epoch 470, training loss: 0.040145669132471085 = 0.032924946397542953 + 0.001 * 7.220721244812012
Epoch 470, val loss: 0.8232100605964661
Epoch 480, training loss: 0.03723662346601486 = 0.03000263310968876 + 0.001 * 7.233992099761963
Epoch 480, val loss: 0.8353173732757568
Epoch 490, training loss: 0.0346442312002182 = 0.027423018589615822 + 0.001 * 7.2212138175964355
Epoch 490, val loss: 0.847234308719635
Epoch 500, training loss: 0.03234368562698364 = 0.025140805169939995 + 0.001 * 7.202880382537842
Epoch 500, val loss: 0.8589157462120056
Epoch 510, training loss: 0.030311476439237595 = 0.02311600372195244 + 0.001 * 7.19547176361084
Epoch 510, val loss: 0.8703621625900269
Epoch 520, training loss: 0.028517264872789383 = 0.0213143453001976 + 0.001 * 7.202919960021973
Epoch 520, val loss: 0.8815168738365173
Epoch 530, training loss: 0.02689068391919136 = 0.019706889986991882 + 0.001 * 7.183792591094971
Epoch 530, val loss: 0.8924056887626648
Epoch 540, training loss: 0.025444772094488144 = 0.018268421292304993 + 0.001 * 7.176351070404053
Epoch 540, val loss: 0.9030210375785828
Epoch 550, training loss: 0.02415458858013153 = 0.016977583989501 + 0.001 * 7.177004337310791
Epoch 550, val loss: 0.9133332967758179
Epoch 560, training loss: 0.022984907031059265 = 0.01581624709069729 + 0.001 * 7.168659687042236
Epoch 560, val loss: 0.9233797192573547
Epoch 570, training loss: 0.02196080982685089 = 0.01476840116083622 + 0.001 * 7.192408561706543
Epoch 570, val loss: 0.9331411719322205
Epoch 580, training loss: 0.020957060158252716 = 0.013820696622133255 + 0.001 * 7.136362075805664
Epoch 580, val loss: 0.942618727684021
Epoch 590, training loss: 0.020089156925678253 = 0.012961331754922867 + 0.001 * 7.127824306488037
Epoch 590, val loss: 0.9518205523490906
Epoch 600, training loss: 0.019312148913741112 = 0.012180144898593426 + 0.001 * 7.132003307342529
Epoch 600, val loss: 0.9607607126235962
Epoch 610, training loss: 0.01859191618859768 = 0.01146834995597601 + 0.001 * 7.123566150665283
Epoch 610, val loss: 0.969441831111908
Epoch 620, training loss: 0.018060388043522835 = 0.010818244889378548 + 0.001 * 7.242142677307129
Epoch 620, val loss: 0.977877140045166
Epoch 630, training loss: 0.017347652465105057 = 0.01022328156977892 + 0.001 * 7.12437105178833
Epoch 630, val loss: 0.9860615134239197
Epoch 640, training loss: 0.01678759790956974 = 0.009677564725279808 + 0.001 * 7.110032558441162
Epoch 640, val loss: 0.9940168857574463
Epoch 650, training loss: 0.01628834195435047 = 0.009175869636237621 + 0.001 * 7.112472057342529
Epoch 650, val loss: 1.0017462968826294
Epoch 660, training loss: 0.015831997618079185 = 0.008713739924132824 + 0.001 * 7.118257522583008
Epoch 660, val loss: 1.0092569589614868
Epoch 670, training loss: 0.01539013534784317 = 0.008287230506539345 + 0.001 * 7.102904796600342
Epoch 670, val loss: 1.0165480375289917
Epoch 680, training loss: 0.014990055002272129 = 0.007892847061157227 + 0.001 * 7.097207546234131
Epoch 680, val loss: 1.0236382484436035
Epoch 690, training loss: 0.014618093147873878 = 0.007527478504925966 + 0.001 * 7.090613842010498
Epoch 690, val loss: 1.0305461883544922
Epoch 700, training loss: 0.014291495084762573 = 0.007188418414443731 + 0.001 * 7.103076934814453
Epoch 700, val loss: 1.0372710227966309
Epoch 710, training loss: 0.013965624384582043 = 0.006873208563774824 + 0.001 * 7.0924153327941895
Epoch 710, val loss: 1.0438141822814941
Epoch 720, training loss: 0.013662477023899555 = 0.006579718552529812 + 0.001 * 7.082757949829102
Epoch 720, val loss: 1.0501872301101685
Epoch 730, training loss: 0.013397116214036942 = 0.006306002382189035 + 0.001 * 7.091113090515137
Epoch 730, val loss: 1.0563889741897583
Epoch 740, training loss: 0.013133535161614418 = 0.006050298921763897 + 0.001 * 7.083235740661621
Epoch 740, val loss: 1.062435269355774
Epoch 750, training loss: 0.012900194153189659 = 0.005810961592942476 + 0.001 * 7.089231967926025
Epoch 750, val loss: 1.068339467048645
Epoch 760, training loss: 0.012670568190515041 = 0.0055863033048808575 + 0.001 * 7.084264755249023
Epoch 760, val loss: 1.074096918106079
Epoch 770, training loss: 0.012455855496227741 = 0.005374465603381395 + 0.001 * 7.081389427185059
Epoch 770, val loss: 1.0797266960144043
Epoch 780, training loss: 0.012248029932379723 = 0.00517377071082592 + 0.001 * 7.074258327484131
Epoch 780, val loss: 1.085247278213501
Epoch 790, training loss: 0.012044268660247326 = 0.004982884041965008 + 0.001 * 7.061384201049805
Epoch 790, val loss: 1.0906648635864258
Epoch 800, training loss: 0.011880217120051384 = 0.004800972528755665 + 0.001 * 7.079244136810303
Epoch 800, val loss: 1.095990777015686
Epoch 810, training loss: 0.011687219142913818 = 0.004627317190170288 + 0.001 * 7.059901237487793
Epoch 810, val loss: 1.1012452840805054
Epoch 820, training loss: 0.011529774405062199 = 0.004461808130145073 + 0.001 * 7.067965984344482
Epoch 820, val loss: 1.1064001321792603
Epoch 830, training loss: 0.011364152655005455 = 0.0043043093755841255 + 0.001 * 7.059842586517334
Epoch 830, val loss: 1.1114822626113892
Epoch 840, training loss: 0.011217204853892326 = 0.004154382273554802 + 0.001 * 7.0628228187561035
Epoch 840, val loss: 1.1164692640304565
Epoch 850, training loss: 0.011069266125559807 = 0.004011724144220352 + 0.001 * 7.057541847229004
Epoch 850, val loss: 1.1213594675064087
Epoch 860, training loss: 0.010936206206679344 = 0.003876111935824156 + 0.001 * 7.060094356536865
Epoch 860, val loss: 1.126158595085144
Epoch 870, training loss: 0.010821573436260223 = 0.00374720711261034 + 0.001 * 7.074365615844727
Epoch 870, val loss: 1.1308660507202148
Epoch 880, training loss: 0.010690232738852501 = 0.0036248457618057728 + 0.001 * 7.0653862953186035
Epoch 880, val loss: 1.1354840993881226
Epoch 890, training loss: 0.010573016479611397 = 0.0035087093710899353 + 0.001 * 7.06430721282959
Epoch 890, val loss: 1.1399896144866943
Epoch 900, training loss: 0.010440019890666008 = 0.003398409578949213 + 0.001 * 7.041609764099121
Epoch 900, val loss: 1.144413948059082
Epoch 910, training loss: 0.010336296632885933 = 0.0032937652431428432 + 0.001 * 7.042531490325928
Epoch 910, val loss: 1.1487486362457275
Epoch 920, training loss: 0.010254062712192535 = 0.0031943977810442448 + 0.001 * 7.059664726257324
Epoch 920, val loss: 1.1529871225357056
Epoch 930, training loss: 0.010142314247786999 = 0.003100051311776042 + 0.001 * 7.042263031005859
Epoch 930, val loss: 1.1571470499038696
Epoch 940, training loss: 0.01004816498607397 = 0.003010302083566785 + 0.001 * 7.037862777709961
Epoch 940, val loss: 1.1611937284469604
Epoch 950, training loss: 0.009960168972611427 = 0.0029249079525470734 + 0.001 * 7.0352606773376465
Epoch 950, val loss: 1.1651772260665894
Epoch 960, training loss: 0.009873200207948685 = 0.0028436430729925632 + 0.001 * 7.0295562744140625
Epoch 960, val loss: 1.169072151184082
Epoch 970, training loss: 0.009817120619118214 = 0.0027662927750498056 + 0.001 * 7.050827503204346
Epoch 970, val loss: 1.1729005575180054
Epoch 980, training loss: 0.009727639146149158 = 0.0026925792917609215 + 0.001 * 7.035059452056885
Epoch 980, val loss: 1.1766269207000732
Epoch 990, training loss: 0.009670482948422432 = 0.002622339641675353 + 0.001 * 7.04814338684082
Epoch 990, val loss: 1.1802898645401
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.951365351676941 = 1.9427684545516968 + 0.001 * 8.596846580505371
Epoch 0, val loss: 1.9335753917694092
Epoch 10, training loss: 1.9413840770721436 = 1.932787299156189 + 0.001 * 8.596819877624512
Epoch 10, val loss: 1.9237112998962402
Epoch 20, training loss: 1.9293403625488281 = 1.920743703842163 + 0.001 * 8.59666919708252
Epoch 20, val loss: 1.9118882417678833
Epoch 30, training loss: 1.91267728805542 = 1.9040809869766235 + 0.001 * 8.596307754516602
Epoch 30, val loss: 1.89545738697052
Epoch 40, training loss: 1.8884589672088623 = 1.8798636198043823 + 0.001 * 8.595366477966309
Epoch 40, val loss: 1.8719466924667358
Epoch 50, training loss: 1.8548331260681152 = 1.8462406396865845 + 0.001 * 8.592437744140625
Epoch 50, val loss: 1.8407174348831177
Epoch 60, training loss: 1.815816879272461 = 1.8072360754013062 + 0.001 * 8.58084487915039
Epoch 60, val loss: 1.807946801185608
Epoch 70, training loss: 1.7787832021713257 = 1.7702594995498657 + 0.001 * 8.523728370666504
Epoch 70, val loss: 1.7794610261917114
Epoch 80, training loss: 1.7331894636154175 = 1.7250250577926636 + 0.001 * 8.164450645446777
Epoch 80, val loss: 1.7400761842727661
Epoch 90, training loss: 1.6687763929367065 = 1.6607635021209717 + 0.001 * 8.012914657592773
Epoch 90, val loss: 1.6810345649719238
Epoch 100, training loss: 1.584375262260437 = 1.5764158964157104 + 0.001 * 7.959399700164795
Epoch 100, val loss: 1.6062965393066406
Epoch 110, training loss: 1.4886474609375 = 1.480724573135376 + 0.001 * 7.9229021072387695
Epoch 110, val loss: 1.5260601043701172
Epoch 120, training loss: 1.3913110494613647 = 1.3834787607192993 + 0.001 * 7.832273483276367
Epoch 120, val loss: 1.4476149082183838
Epoch 130, training loss: 1.2958345413208008 = 1.2881438732147217 + 0.001 * 7.690684795379639
Epoch 130, val loss: 1.3739285469055176
Epoch 140, training loss: 1.2040235996246338 = 1.1963841915130615 + 0.001 * 7.639372825622559
Epoch 140, val loss: 1.3048757314682007
Epoch 150, training loss: 1.1181325912475586 = 1.1105798482894897 + 0.001 * 7.552801132202148
Epoch 150, val loss: 1.2414823770523071
Epoch 160, training loss: 1.0398482084274292 = 1.0324112176895142 + 0.001 * 7.436983585357666
Epoch 160, val loss: 1.1852383613586426
Epoch 170, training loss: 0.9690287113189697 = 0.9616703987121582 + 0.001 * 7.358301639556885
Epoch 170, val loss: 1.1356394290924072
Epoch 180, training loss: 0.9035875201225281 = 0.8962361812591553 + 0.001 * 7.35132360458374
Epoch 180, val loss: 1.0904217958450317
Epoch 190, training loss: 0.8406883478164673 = 0.8333407044410706 + 0.001 * 7.347631454467773
Epoch 190, val loss: 1.0478219985961914
Epoch 200, training loss: 0.7782277464866638 = 0.7708857655525208 + 0.001 * 7.341972827911377
Epoch 200, val loss: 1.0063248872756958
Epoch 210, training loss: 0.715219259262085 = 0.7078815698623657 + 0.001 * 7.337667465209961
Epoch 210, val loss: 0.9646028876304626
Epoch 220, training loss: 0.6514025330543518 = 0.6440694332122803 + 0.001 * 7.333120822906494
Epoch 220, val loss: 0.9223589301109314
Epoch 230, training loss: 0.5872247219085693 = 0.5798971652984619 + 0.001 * 7.3275346755981445
Epoch 230, val loss: 0.8794913291931152
Epoch 240, training loss: 0.5238078236579895 = 0.5164878964424133 + 0.001 * 7.319945335388184
Epoch 240, val loss: 0.8370740413665771
Epoch 250, training loss: 0.4626777768135071 = 0.45536962151527405 + 0.001 * 7.308157444000244
Epoch 250, val loss: 0.7968102693557739
Epoch 260, training loss: 0.4051876962184906 = 0.39789488911628723 + 0.001 * 7.292807102203369
Epoch 260, val loss: 0.7605569362640381
Epoch 270, training loss: 0.3523397147655487 = 0.34506702423095703 + 0.001 * 7.272680282592773
Epoch 270, val loss: 0.7294807434082031
Epoch 280, training loss: 0.3047098219394684 = 0.2974594831466675 + 0.001 * 7.250331401824951
Epoch 280, val loss: 0.7041242122650146
Epoch 290, training loss: 0.2626801133155823 = 0.25544244050979614 + 0.001 * 7.23766565322876
Epoch 290, val loss: 0.6845767498016357
Epoch 300, training loss: 0.22639749944210052 = 0.2191707193851471 + 0.001 * 7.226783275604248
Epoch 300, val loss: 0.6706488728523254
Epoch 310, training loss: 0.19569168984889984 = 0.18847066164016724 + 0.001 * 7.221033096313477
Epoch 310, val loss: 0.6619405150413513
Epoch 320, training loss: 0.1700110286474228 = 0.16278788447380066 + 0.001 * 7.223150730133057
Epoch 320, val loss: 0.6577785611152649
Epoch 330, training loss: 0.1485641896724701 = 0.1413448303937912 + 0.001 * 7.219351291656494
Epoch 330, val loss: 0.6573253870010376
Epoch 340, training loss: 0.13056422770023346 = 0.12334683537483215 + 0.001 * 7.217388153076172
Epoch 340, val loss: 0.6598402261734009
Epoch 350, training loss: 0.11533212661743164 = 0.10811484605073929 + 0.001 * 7.2172770500183105
Epoch 350, val loss: 0.6646773815155029
Epoch 360, training loss: 0.10233470052480698 = 0.09511870890855789 + 0.001 * 7.215989112854004
Epoch 360, val loss: 0.6713229417800903
Epoch 370, training loss: 0.09117048233747482 = 0.08395494520664215 + 0.001 * 7.2155351638793945
Epoch 370, val loss: 0.6793772578239441
Epoch 380, training loss: 0.08153568208217621 = 0.07431985437870026 + 0.001 * 7.215827465057373
Epoch 380, val loss: 0.6885051131248474
Epoch 390, training loss: 0.07319121062755585 = 0.06597495824098587 + 0.001 * 7.216254234313965
Epoch 390, val loss: 0.6984869837760925
Epoch 400, training loss: 0.06594670563936234 = 0.05873080715537071 + 0.001 * 7.2158989906311035
Epoch 400, val loss: 0.7090936303138733
Epoch 410, training loss: 0.05964723229408264 = 0.05243278667330742 + 0.001 * 7.214444637298584
Epoch 410, val loss: 0.7201216816902161
Epoch 420, training loss: 0.05416141822934151 = 0.04694795608520508 + 0.001 * 7.213463306427002
Epoch 420, val loss: 0.731428325176239
Epoch 430, training loss: 0.04937776178121567 = 0.042164865881204605 + 0.001 * 7.212896347045898
Epoch 430, val loss: 0.7428884506225586
Epoch 440, training loss: 0.04520304128527641 = 0.03798884525895119 + 0.001 * 7.214195728302002
Epoch 440, val loss: 0.7543782591819763
Epoch 450, training loss: 0.041548579931259155 = 0.034337367862463 + 0.001 * 7.2112135887146
Epoch 450, val loss: 0.7658402323722839
Epoch 460, training loss: 0.038350462913513184 = 0.031139222905039787 + 0.001 * 7.211238861083984
Epoch 460, val loss: 0.7771920561790466
Epoch 470, training loss: 0.03554298356175423 = 0.02833237498998642 + 0.001 * 7.210607528686523
Epoch 470, val loss: 0.7883954644203186
Epoch 480, training loss: 0.03307201713323593 = 0.02586316131055355 + 0.001 * 7.20885705947876
Epoch 480, val loss: 0.7994110584259033
Epoch 490, training loss: 0.030891643837094307 = 0.023685043677687645 + 0.001 * 7.206599235534668
Epoch 490, val loss: 0.8102143406867981
Epoch 500, training loss: 0.028971049934625626 = 0.02175845578312874 + 0.001 * 7.2125935554504395
Epoch 500, val loss: 0.8207705020904541
Epoch 510, training loss: 0.02725391648709774 = 0.020049454644322395 + 0.001 * 7.204461097717285
Epoch 510, val loss: 0.8310911059379578
Epoch 520, training loss: 0.025743849575519562 = 0.018528752028942108 + 0.001 * 7.215097427368164
Epoch 520, val loss: 0.8411628603935242
Epoch 530, training loss: 0.02437533810734749 = 0.017171546816825867 + 0.001 * 7.203790664672852
Epoch 530, val loss: 0.8509683012962341
Epoch 540, training loss: 0.0231553316116333 = 0.01595638133585453 + 0.001 * 7.198948860168457
Epoch 540, val loss: 0.8605042695999146
Epoch 550, training loss: 0.022061554715037346 = 0.014865189790725708 + 0.001 * 7.196363925933838
Epoch 550, val loss: 0.8697887063026428
Epoch 560, training loss: 0.021081428974866867 = 0.013882330618798733 + 0.001 * 7.199097633361816
Epoch 560, val loss: 0.8788223266601562
Epoch 570, training loss: 0.020190177485346794 = 0.012994503602385521 + 0.001 * 7.195672988891602
Epoch 570, val loss: 0.8876075744628906
Epoch 580, training loss: 0.019382815808057785 = 0.012189956381917 + 0.001 * 7.192860126495361
Epoch 580, val loss: 0.8961623311042786
Epoch 590, training loss: 0.018646428361535072 = 0.01145845651626587 + 0.001 * 7.187971591949463
Epoch 590, val loss: 0.9044905304908752
Epoch 600, training loss: 0.017977990210056305 = 0.010790416039526463 + 0.001 * 7.187573432922363
Epoch 600, val loss: 0.9126291871070862
Epoch 610, training loss: 0.017364181578159332 = 0.010177933610975742 + 0.001 * 7.186246871948242
Epoch 610, val loss: 0.9205914735794067
Epoch 620, training loss: 0.016841158270835876 = 0.00961433257907629 + 0.001 * 7.226826190948486
Epoch 620, val loss: 0.9283739328384399
Epoch 630, training loss: 0.01628754287958145 = 0.009094016626477242 + 0.001 * 7.193526744842529
Epoch 630, val loss: 0.9359900951385498
Epoch 640, training loss: 0.015791520476341248 = 0.008612788282334805 + 0.001 * 7.178732395172119
Epoch 640, val loss: 0.9434488415718079
Epoch 650, training loss: 0.015339214354753494 = 0.008166927844285965 + 0.001 * 7.172286033630371
Epoch 650, val loss: 0.9507578611373901
Epoch 660, training loss: 0.014929103665053844 = 0.007753605954349041 + 0.001 * 7.175497531890869
Epoch 660, val loss: 0.9579238295555115
Epoch 670, training loss: 0.014533703215420246 = 0.0073705255053937435 + 0.001 * 7.163177490234375
Epoch 670, val loss: 0.9649486541748047
Epoch 680, training loss: 0.014188222587108612 = 0.007014797534793615 + 0.001 * 7.17342472076416
Epoch 680, val loss: 0.9718315005302429
Epoch 690, training loss: 0.013842758722603321 = 0.006684446707367897 + 0.001 * 7.15831184387207
Epoch 690, val loss: 0.9785628914833069
Epoch 700, training loss: 0.013558708131313324 = 0.006377368234097958 + 0.001 * 7.181340217590332
Epoch 700, val loss: 0.985150158405304
Epoch 710, training loss: 0.013246729969978333 = 0.006091564428061247 + 0.001 * 7.155165672302246
Epoch 710, val loss: 0.9915941953659058
Epoch 720, training loss: 0.013006082735955715 = 0.005825222469866276 + 0.001 * 7.1808600425720215
Epoch 720, val loss: 0.9979016184806824
Epoch 730, training loss: 0.012728802859783173 = 0.005576782859861851 + 0.001 * 7.15201997756958
Epoch 730, val loss: 1.0040653944015503
Epoch 740, training loss: 0.012510825879871845 = 0.005344796925783157 + 0.001 * 7.1660284996032715
Epoch 740, val loss: 1.0100901126861572
Epoch 750, training loss: 0.012273280881345272 = 0.005127888172864914 + 0.001 * 7.145392417907715
Epoch 750, val loss: 1.0159857273101807
Epoch 760, training loss: 0.012087317183613777 = 0.004924983251839876 + 0.001 * 7.162333011627197
Epoch 760, val loss: 1.0217444896697998
Epoch 770, training loss: 0.011901769787073135 = 0.004735073074698448 + 0.001 * 7.166696548461914
Epoch 770, val loss: 1.0273693799972534
Epoch 780, training loss: 0.011704986914992332 = 0.004557070322334766 + 0.001 * 7.147915840148926
Epoch 780, val loss: 1.03287672996521
Epoch 790, training loss: 0.011488493531942368 = 0.0043898774310946465 + 0.001 * 7.0986151695251465
Epoch 790, val loss: 1.0382570028305054
Epoch 800, training loss: 0.011393233202397823 = 0.004232595209032297 + 0.001 * 7.160637855529785
Epoch 800, val loss: 1.0435245037078857
Epoch 810, training loss: 0.011212557554244995 = 0.004084618762135506 + 0.001 * 7.127938747406006
Epoch 810, val loss: 1.048674464225769
Epoch 820, training loss: 0.011042286641895771 = 0.003945259843021631 + 0.001 * 7.097026348114014
Epoch 820, val loss: 1.0536866188049316
Epoch 830, training loss: 0.010905592702329159 = 0.003813977586105466 + 0.001 * 7.091615200042725
Epoch 830, val loss: 1.0586085319519043
Epoch 840, training loss: 0.010772103443741798 = 0.0036900844424962997 + 0.001 * 7.082019329071045
Epoch 840, val loss: 1.0634181499481201
Epoch 850, training loss: 0.010672974400222301 = 0.00357308192178607 + 0.001 * 7.0998921394348145
Epoch 850, val loss: 1.068114161491394
Epoch 860, training loss: 0.010572409257292747 = 0.0034624680411070585 + 0.001 * 7.109941005706787
Epoch 860, val loss: 1.0727145671844482
Epoch 870, training loss: 0.01046522706747055 = 0.003357687033712864 + 0.001 * 7.107540130615234
Epoch 870, val loss: 1.0772095918655396
Epoch 880, training loss: 0.010376639664173126 = 0.003258364973589778 + 0.001 * 7.118274211883545
Epoch 880, val loss: 1.081602931022644
Epoch 890, training loss: 0.010269463062286377 = 0.003164200112223625 + 0.001 * 7.1052632331848145
Epoch 890, val loss: 1.0859184265136719
Epoch 900, training loss: 0.01017592940479517 = 0.003074795473366976 + 0.001 * 7.101133823394775
Epoch 900, val loss: 1.0901238918304443
Epoch 910, training loss: 0.010072395205497742 = 0.002989785745739937 + 0.001 * 7.082608699798584
Epoch 910, val loss: 1.0942531824111938
Epoch 920, training loss: 0.00998281966894865 = 0.002908928319811821 + 0.001 * 7.0738911628723145
Epoch 920, val loss: 1.0982989072799683
Epoch 930, training loss: 0.009913339279592037 = 0.002831975696608424 + 0.001 * 7.081363201141357
Epoch 930, val loss: 1.1022474765777588
Epoch 940, training loss: 0.009856262244284153 = 0.0027586650103330612 + 0.001 * 7.097597122192383
Epoch 940, val loss: 1.1061303615570068
Epoch 950, training loss: 0.009765051305294037 = 0.002688795328140259 + 0.001 * 7.0762553215026855
Epoch 950, val loss: 1.1099094152450562
Epoch 960, training loss: 0.009669242426753044 = 0.002622161526232958 + 0.001 * 7.0470805168151855
Epoch 960, val loss: 1.1136224269866943
Epoch 970, training loss: 0.009632849134504795 = 0.0025584991089999676 + 0.001 * 7.074349880218506
Epoch 970, val loss: 1.117270588874817
Epoch 980, training loss: 0.009550356306135654 = 0.00249761575832963 + 0.001 * 7.052740097045898
Epoch 980, val loss: 1.120841145515442
Epoch 990, training loss: 0.009509129449725151 = 0.0024392991326749325 + 0.001 * 7.069830417633057
Epoch 990, val loss: 1.1243538856506348
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 1.9601974487304688 = 1.9516005516052246 + 0.001 * 8.596848487854004
Epoch 0, val loss: 1.9506664276123047
Epoch 10, training loss: 1.9499098062515259 = 1.9413130283355713 + 0.001 * 8.596808433532715
Epoch 10, val loss: 1.940186619758606
Epoch 20, training loss: 1.9371417760849 = 1.9285451173782349 + 0.001 * 8.596665382385254
Epoch 20, val loss: 1.9269418716430664
Epoch 30, training loss: 1.9192681312561035 = 1.9106718301773071 + 0.001 * 8.596292495727539
Epoch 30, val loss: 1.9081708192825317
Epoch 40, training loss: 1.8931095600128174 = 1.8845142126083374 + 0.001 * 8.595318794250488
Epoch 40, val loss: 1.8808178901672363
Epoch 50, training loss: 1.8566038608551025 = 1.8480112552642822 + 0.001 * 8.592557907104492
Epoch 50, val loss: 1.844086766242981
Epoch 60, training loss: 1.815619945526123 = 1.8070366382598877 + 0.001 * 8.583283424377441
Epoch 60, val loss: 1.807122826576233
Epoch 70, training loss: 1.781064748764038 = 1.7725176811218262 + 0.001 * 8.547109603881836
Epoch 70, val loss: 1.7805042266845703
Epoch 80, training loss: 1.7399784326553345 = 1.7316396236419678 + 0.001 * 8.3388032913208
Epoch 80, val loss: 1.7463561296463013
Epoch 90, training loss: 1.6820274591445923 = 1.6739122867584229 + 0.001 * 8.115222930908203
Epoch 90, val loss: 1.695665717124939
Epoch 100, training loss: 1.6027863025665283 = 1.5947954654693604 + 0.001 * 7.990877628326416
Epoch 100, val loss: 1.6268408298492432
Epoch 110, training loss: 1.5070984363555908 = 1.4992378950119019 + 0.001 * 7.8605217933654785
Epoch 110, val loss: 1.5461511611938477
Epoch 120, training loss: 1.4058107137680054 = 1.3981045484542847 + 0.001 * 7.706207275390625
Epoch 120, val loss: 1.4636099338531494
Epoch 130, training loss: 1.3051198720932007 = 1.2975136041641235 + 0.001 * 7.606221675872803
Epoch 130, val loss: 1.3833930492401123
Epoch 140, training loss: 1.2070229053497314 = 1.1994825601577759 + 0.001 * 7.540393829345703
Epoch 140, val loss: 1.3072017431259155
Epoch 150, training loss: 1.1138513088226318 = 1.106410264968872 + 0.001 * 7.441087245941162
Epoch 150, val loss: 1.2357542514801025
Epoch 160, training loss: 1.0278205871582031 = 1.020439863204956 + 0.001 * 7.380717754364014
Epoch 160, val loss: 1.171064019203186
Epoch 170, training loss: 0.9488857984542847 = 0.9415184259414673 + 0.001 * 7.367382049560547
Epoch 170, val loss: 1.1120736598968506
Epoch 180, training loss: 0.8743429183959961 = 0.8669775724411011 + 0.001 * 7.365372180938721
Epoch 180, val loss: 1.0562396049499512
Epoch 190, training loss: 0.801571786403656 = 0.7942129969596863 + 0.001 * 7.358809471130371
Epoch 190, val loss: 1.0011398792266846
Epoch 200, training loss: 0.7304785847663879 = 0.7231267094612122 + 0.001 * 7.351853847503662
Epoch 200, val loss: 0.9476819038391113
Epoch 210, training loss: 0.6634820103645325 = 0.656136691570282 + 0.001 * 7.345338821411133
Epoch 210, val loss: 0.8991676568984985
Epoch 220, training loss: 0.6032255291938782 = 0.5958871245384216 + 0.001 * 7.338390827178955
Epoch 220, val loss: 0.8587201833724976
Epoch 230, training loss: 0.5506564378738403 = 0.5433254241943359 + 0.001 * 7.331024169921875
Epoch 230, val loss: 0.8275609612464905
Epoch 240, training loss: 0.5049195885658264 = 0.49759650230407715 + 0.001 * 7.323095798492432
Epoch 240, val loss: 0.8045486211776733
Epoch 250, training loss: 0.4640977680683136 = 0.4567839503288269 + 0.001 * 7.313814640045166
Epoch 250, val loss: 0.7875822186470032
Epoch 260, training loss: 0.426047146320343 = 0.4187435209751129 + 0.001 * 7.303630828857422
Epoch 260, val loss: 0.7742815017700195
Epoch 270, training loss: 0.3888151943683624 = 0.38152629137039185 + 0.001 * 7.288913726806641
Epoch 270, val loss: 0.7626813650131226
Epoch 280, training loss: 0.3512866795063019 = 0.34401723742485046 + 0.001 * 7.269455909729004
Epoch 280, val loss: 0.7518600821495056
Epoch 290, training loss: 0.31347671151161194 = 0.306220680475235 + 0.001 * 7.256043910980225
Epoch 290, val loss: 0.741901695728302
Epoch 300, training loss: 0.2765297591686249 = 0.2692927122116089 + 0.001 * 7.237054347991943
Epoch 300, val loss: 0.7334879040718079
Epoch 310, training loss: 0.24211035668849945 = 0.23488497734069824 + 0.001 * 7.225372314453125
Epoch 310, val loss: 0.7276472449302673
Epoch 320, training loss: 0.21147894859313965 = 0.20425976812839508 + 0.001 * 7.219180583953857
Epoch 320, val loss: 0.7249303460121155
Epoch 330, training loss: 0.18501128256320953 = 0.17779596149921417 + 0.001 * 7.215322494506836
Epoch 330, val loss: 0.725529670715332
Epoch 340, training loss: 0.16241653263568878 = 0.15520378947257996 + 0.001 * 7.212741851806641
Epoch 340, val loss: 0.7289336919784546
Epoch 350, training loss: 0.1431444138288498 = 0.1359337717294693 + 0.001 * 7.210644721984863
Epoch 350, val loss: 0.7345775365829468
Epoch 360, training loss: 0.12665586173534393 = 0.11944771558046341 + 0.001 * 7.208141803741455
Epoch 360, val loss: 0.7419462203979492
Epoch 370, training loss: 0.1124916523694992 = 0.10528474301099777 + 0.001 * 7.206907272338867
Epoch 370, val loss: 0.7506358027458191
Epoch 380, training loss: 0.10027710348367691 = 0.09306969493627548 + 0.001 * 7.207411289215088
Epoch 380, val loss: 0.76035076379776
Epoch 390, training loss: 0.08970176428556442 = 0.08249490708112717 + 0.001 * 7.206855773925781
Epoch 390, val loss: 0.770879328250885
Epoch 400, training loss: 0.0805128812789917 = 0.07330666482448578 + 0.001 * 7.206214427947998
Epoch 400, val loss: 0.781981885433197
Epoch 410, training loss: 0.07250555604696274 = 0.06530135869979858 + 0.001 * 7.204200267791748
Epoch 410, val loss: 0.793552041053772
Epoch 420, training loss: 0.06551581621170044 = 0.058312930166721344 + 0.001 * 7.202885627746582
Epoch 420, val loss: 0.805446207523346
Epoch 430, training loss: 0.059405963867902756 = 0.05220159515738487 + 0.001 * 7.2043681144714355
Epoch 430, val loss: 0.817533552646637
Epoch 440, training loss: 0.05404524877667427 = 0.046844277530908585 + 0.001 * 7.200970649719238
Epoch 440, val loss: 0.8297038078308105
Epoch 450, training loss: 0.0493398979306221 = 0.042140692472457886 + 0.001 * 7.199206829071045
Epoch 450, val loss: 0.8418900966644287
Epoch 460, training loss: 0.04520725831389427 = 0.03800459951162338 + 0.001 * 7.202657699584961
Epoch 460, val loss: 0.8540034890174866
Epoch 470, training loss: 0.041561223566532135 = 0.03436446189880371 + 0.001 * 7.1967620849609375
Epoch 470, val loss: 0.866020143032074
Epoch 480, training loss: 0.038349613547325134 = 0.031157175078988075 + 0.001 * 7.192437648773193
Epoch 480, val loss: 0.8778694868087769
Epoch 490, training loss: 0.03551950678229332 = 0.02832818031311035 + 0.001 * 7.19132661819458
Epoch 490, val loss: 0.8894880414009094
Epoch 500, training loss: 0.03302163630723953 = 0.02582884021103382 + 0.001 * 7.192794322967529
Epoch 500, val loss: 0.9008393287658691
Epoch 510, training loss: 0.030806491151452065 = 0.023616034537553787 + 0.001 * 7.190456390380859
Epoch 510, val loss: 0.9119472503662109
Epoch 520, training loss: 0.028835918754339218 = 0.02165273390710354 + 0.001 * 7.183185577392578
Epoch 520, val loss: 0.9227948188781738
Epoch 530, training loss: 0.027122998610138893 = 0.01990712247788906 + 0.001 * 7.215875148773193
Epoch 530, val loss: 0.9333510398864746
Epoch 540, training loss: 0.02553076669573784 = 0.018351055681705475 + 0.001 * 7.179710388183594
Epoch 540, val loss: 0.9436237812042236
Epoch 550, training loss: 0.024132495746016502 = 0.016960756853222847 + 0.001 * 7.171739101409912
Epoch 550, val loss: 0.9536328315734863
Epoch 560, training loss: 0.02288462035357952 = 0.01571556366980076 + 0.001 * 7.169056415557861
Epoch 560, val loss: 0.9633488655090332
Epoch 570, training loss: 0.021814357489347458 = 0.014597704634070396 + 0.001 * 7.216651916503906
Epoch 570, val loss: 0.9727909564971924
Epoch 580, training loss: 0.020757077261805534 = 0.013591918163001537 + 0.001 * 7.165159225463867
Epoch 580, val loss: 0.9819661378860474
Epoch 590, training loss: 0.019845053553581238 = 0.01268476340919733 + 0.001 * 7.160289764404297
Epoch 590, val loss: 0.990887463092804
Epoch 600, training loss: 0.019021809101104736 = 0.011864627711474895 + 0.001 * 7.157180309295654
Epoch 600, val loss: 0.9995542168617249
Epoch 610, training loss: 0.018289579078555107 = 0.011121354065835476 + 0.001 * 7.168225288391113
Epoch 610, val loss: 1.007968544960022
Epoch 620, training loss: 0.017604004591703415 = 0.010446219705045223 + 0.001 * 7.1577839851379395
Epoch 620, val loss: 1.016147255897522
Epoch 630, training loss: 0.016982171684503555 = 0.009831595234572887 + 0.001 * 7.150576591491699
Epoch 630, val loss: 1.0240916013717651
Epoch 640, training loss: 0.01641875132918358 = 0.009270853362977505 + 0.001 * 7.147897720336914
Epoch 640, val loss: 1.0318025350570679
Epoch 650, training loss: 0.01590658910572529 = 0.008758115582168102 + 0.001 * 7.148473262786865
Epoch 650, val loss: 1.0392872095108032
Epoch 660, training loss: 0.015441457740962505 = 0.008288219571113586 + 0.001 * 7.153237819671631
Epoch 660, val loss: 1.0465681552886963
Epoch 670, training loss: 0.015005381777882576 = 0.007856732234358788 + 0.001 * 7.148648738861084
Epoch 670, val loss: 1.053627610206604
Epoch 680, training loss: 0.01460771169513464 = 0.007459660060703754 + 0.001 * 7.1480512619018555
Epoch 680, val loss: 1.0604850053787231
Epoch 690, training loss: 0.014237415045499802 = 0.007093535270541906 + 0.001 * 7.143878936767578
Epoch 690, val loss: 1.0671645402908325
Epoch 700, training loss: 0.013889189809560776 = 0.006755269598215818 + 0.001 * 7.133920192718506
Epoch 700, val loss: 1.0736315250396729
Epoch 710, training loss: 0.01358158327639103 = 0.006442164536565542 + 0.001 * 7.139418125152588
Epoch 710, val loss: 1.0799243450164795
Epoch 720, training loss: 0.013286265544593334 = 0.006151851266622543 + 0.001 * 7.134413719177246
Epoch 720, val loss: 1.0860601663589478
Epoch 730, training loss: 0.013014513999223709 = 0.005882172845304012 + 0.001 * 7.132340431213379
Epoch 730, val loss: 1.0920319557189941
Epoch 740, training loss: 0.012779353186488152 = 0.005631224252283573 + 0.001 * 7.148128509521484
Epoch 740, val loss: 1.097835659980774
Epoch 750, training loss: 0.012531225569546223 = 0.00539735984057188 + 0.001 * 7.1338653564453125
Epoch 750, val loss: 1.103490948677063
Epoch 760, training loss: 0.012309533543884754 = 0.005179071798920631 + 0.001 * 7.1304612159729
Epoch 760, val loss: 1.1089863777160645
Epoch 770, training loss: 0.01212023664265871 = 0.004974983166903257 + 0.001 * 7.1452531814575195
Epoch 770, val loss: 1.1143484115600586
Epoch 780, training loss: 0.011908400803804398 = 0.004783916752785444 + 0.001 * 7.124483585357666
Epoch 780, val loss: 1.119566559791565
Epoch 790, training loss: 0.011731517501175404 = 0.004604732617735863 + 0.001 * 7.126784324645996
Epoch 790, val loss: 1.124657392501831
Epoch 800, training loss: 0.011552632786333561 = 0.004436232149600983 + 0.001 * 7.116400241851807
Epoch 800, val loss: 1.129639744758606
Epoch 810, training loss: 0.011432390660047531 = 0.004276756662875414 + 0.001 * 7.155633926391602
Epoch 810, val loss: 1.134542465209961
Epoch 820, training loss: 0.01124420017004013 = 0.004124458413571119 + 0.001 * 7.119741916656494
Epoch 820, val loss: 1.1393816471099854
Epoch 830, training loss: 0.011113639920949936 = 0.0039781322702765465 + 0.001 * 7.135507106781006
Epoch 830, val loss: 1.1441550254821777
Epoch 840, training loss: 0.010959981009364128 = 0.0038374452851712704 + 0.001 * 7.122535228729248
Epoch 840, val loss: 1.1487741470336914
Epoch 850, training loss: 0.010811473242938519 = 0.003702429821714759 + 0.001 * 7.109043121337891
Epoch 850, val loss: 1.1533292531967163
Epoch 860, training loss: 0.01069783978164196 = 0.003573383204638958 + 0.001 * 7.124456882476807
Epoch 860, val loss: 1.1577208042144775
Epoch 870, training loss: 0.010555911809206009 = 0.0034503513015806675 + 0.001 * 7.105560779571533
Epoch 870, val loss: 1.1620367765426636
Epoch 880, training loss: 0.010510889813303947 = 0.003333148779347539 + 0.001 * 7.177740573883057
Epoch 880, val loss: 1.166216254234314
Epoch 890, training loss: 0.010327532887458801 = 0.003221707185730338 + 0.001 * 7.105824947357178
Epoch 890, val loss: 1.1702650785446167
Epoch 900, training loss: 0.010219618678092957 = 0.0031157999765127897 + 0.001 * 7.103818416595459
Epoch 900, val loss: 1.1742849349975586
Epoch 910, training loss: 0.01011172030121088 = 0.0030150245875120163 + 0.001 * 7.096695423126221
Epoch 910, val loss: 1.178133249282837
Epoch 920, training loss: 0.010043236427009106 = 0.0029188424814492464 + 0.001 * 7.124393939971924
Epoch 920, val loss: 1.1819003820419312
Epoch 930, training loss: 0.009932152926921844 = 0.002826652955263853 + 0.001 * 7.105499744415283
Epoch 930, val loss: 1.185525894165039
Epoch 940, training loss: 0.009834707714617252 = 0.002738003386184573 + 0.001 * 7.096704006195068
Epoch 940, val loss: 1.1890767812728882
Epoch 950, training loss: 0.009748982265591621 = 0.002652533818036318 + 0.001 * 7.096447467803955
Epoch 950, val loss: 1.1925486326217651
Epoch 960, training loss: 0.009654005989432335 = 0.002569950418546796 + 0.001 * 7.084054946899414
Epoch 960, val loss: 1.19600510597229
Epoch 970, training loss: 0.009582249447703362 = 0.002490232465788722 + 0.001 * 7.092016220092773
Epoch 970, val loss: 1.1993921995162964
Epoch 980, training loss: 0.00950470007956028 = 0.0024134069681167603 + 0.001 * 7.0912933349609375
Epoch 980, val loss: 1.2027674913406372
Epoch 990, training loss: 0.009436892345547676 = 0.002339641796424985 + 0.001 * 7.097249984741211
Epoch 990, val loss: 1.2060909271240234
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.845545598313126
The final CL Acc:0.81975, 0.01492, The final GNN Acc:0.83992, 0.00407
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10562])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9461793899536133 = 1.9375826120376587 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.9444645643234253
Epoch 10, training loss: 1.9364317655563354 = 1.9278349876403809 + 0.001 * 8.59676456451416
Epoch 10, val loss: 1.9339457750320435
Epoch 20, training loss: 1.9241410493850708 = 1.9155445098876953 + 0.001 * 8.596567153930664
Epoch 20, val loss: 1.92042076587677
Epoch 30, training loss: 1.9068775177001953 = 1.8982813358306885 + 0.001 * 8.596125602722168
Epoch 30, val loss: 1.9011653661727905
Epoch 40, training loss: 1.881539225578308 = 1.8729441165924072 + 0.001 * 8.595067977905273
Epoch 40, val loss: 1.87295663356781
Epoch 50, training loss: 1.8474760055541992 = 1.8388839960098267 + 0.001 * 8.59205150604248
Epoch 50, val loss: 1.8362466096878052
Epoch 60, training loss: 1.8129632472991943 = 1.8043817281723022 + 0.001 * 8.581459045410156
Epoch 60, val loss: 1.802694320678711
Epoch 70, training loss: 1.784699559211731 = 1.7761648893356323 + 0.001 * 8.534632682800293
Epoch 70, val loss: 1.7790862321853638
Epoch 80, training loss: 1.746584415435791 = 1.7383097410202026 + 0.001 * 8.274664878845215
Epoch 80, val loss: 1.748125433921814
Epoch 90, training loss: 1.6936712265014648 = 1.685545563697815 + 0.001 * 8.125696182250977
Epoch 90, val loss: 1.7043818235397339
Epoch 100, training loss: 1.621948480606079 = 1.613981008529663 + 0.001 * 7.967523574829102
Epoch 100, val loss: 1.6463627815246582
Epoch 110, training loss: 1.5370537042617798 = 1.5293232202529907 + 0.001 * 7.730495929718018
Epoch 110, val loss: 1.5793896913528442
Epoch 120, training loss: 1.4485353231430054 = 1.441046118736267 + 0.001 * 7.489185333251953
Epoch 120, val loss: 1.5107285976409912
Epoch 130, training loss: 1.3601914644241333 = 1.3526958227157593 + 0.001 * 7.49562931060791
Epoch 130, val loss: 1.4436020851135254
Epoch 140, training loss: 1.2693642377853394 = 1.261899709701538 + 0.001 * 7.464575290679932
Epoch 140, val loss: 1.374350905418396
Epoch 150, training loss: 1.1748791933059692 = 1.1674326658248901 + 0.001 * 7.446486473083496
Epoch 150, val loss: 1.301841378211975
Epoch 160, training loss: 1.0785413980484009 = 1.0711140632629395 + 0.001 * 7.427364826202393
Epoch 160, val loss: 1.2278378009796143
Epoch 170, training loss: 0.9831609725952148 = 0.9757451415061951 + 0.001 * 7.415855884552002
Epoch 170, val loss: 1.154706358909607
Epoch 180, training loss: 0.89131760597229 = 0.8839076161384583 + 0.001 * 7.410000324249268
Epoch 180, val loss: 1.0844371318817139
Epoch 190, training loss: 0.8053312301635742 = 0.7979265451431274 + 0.001 * 7.404693603515625
Epoch 190, val loss: 1.0192939043045044
Epoch 200, training loss: 0.727180540561676 = 0.7197816967964172 + 0.001 * 7.398818016052246
Epoch 200, val loss: 0.9603903293609619
Epoch 210, training loss: 0.6578037738800049 = 0.6504130363464355 + 0.001 * 7.390740871429443
Epoch 210, val loss: 0.9094930291175842
Epoch 220, training loss: 0.5966331958770752 = 0.5892553925514221 + 0.001 * 7.377827167510986
Epoch 220, val loss: 0.8675021529197693
Epoch 230, training loss: 0.5420597791671753 = 0.5347042083740234 + 0.001 * 7.3555707931518555
Epoch 230, val loss: 0.834271252155304
Epoch 240, training loss: 0.492097944021225 = 0.4847663640975952 + 0.001 * 7.331577301025391
Epoch 240, val loss: 0.8088529109954834
Epoch 250, training loss: 0.4453982412815094 = 0.43810683488845825 + 0.001 * 7.291411399841309
Epoch 250, val loss: 0.790009081363678
Epoch 260, training loss: 0.4015468657016754 = 0.3942769169807434 + 0.001 * 7.269943714141846
Epoch 260, val loss: 0.776360273361206
Epoch 270, training loss: 0.36063218116760254 = 0.35338711738586426 + 0.001 * 7.245068073272705
Epoch 270, val loss: 0.7668725252151489
Epoch 280, training loss: 0.32299891114234924 = 0.31576406955718994 + 0.001 * 7.234828472137451
Epoch 280, val loss: 0.7612111568450928
Epoch 290, training loss: 0.28887444734573364 = 0.28164583444595337 + 0.001 * 7.228601455688477
Epoch 290, val loss: 0.7591956853866577
Epoch 300, training loss: 0.2582429349422455 = 0.2510157823562622 + 0.001 * 7.227166652679443
Epoch 300, val loss: 0.7605942487716675
Epoch 310, training loss: 0.23083189129829407 = 0.2236059457063675 + 0.001 * 7.2259521484375
Epoch 310, val loss: 0.7649044394493103
Epoch 320, training loss: 0.20619404315948486 = 0.1989688128232956 + 0.001 * 7.225223541259766
Epoch 320, val loss: 0.771443784236908
Epoch 330, training loss: 0.18385949730873108 = 0.1766367405653 + 0.001 * 7.222751140594482
Epoch 330, val loss: 0.7797532677650452
Epoch 340, training loss: 0.16351726651191711 = 0.15629222989082336 + 0.001 * 7.225036144256592
Epoch 340, val loss: 0.7894230484962463
Epoch 350, training loss: 0.14502370357513428 = 0.137802854180336 + 0.001 * 7.22084379196167
Epoch 350, val loss: 0.800284743309021
Epoch 360, training loss: 0.12836185097694397 = 0.1211426854133606 + 0.001 * 7.219158172607422
Epoch 360, val loss: 0.8123078346252441
Epoch 370, training loss: 0.11350449919700623 = 0.10628748685121536 + 0.001 * 7.217010021209717
Epoch 370, val loss: 0.8252751231193542
Epoch 380, training loss: 0.10041483491659164 = 0.09317553788423538 + 0.001 * 7.239295959472656
Epoch 380, val loss: 0.839056134223938
Epoch 390, training loss: 0.08891507983207703 = 0.08170005679130554 + 0.001 * 7.215023040771484
Epoch 390, val loss: 0.8535221219062805
Epoch 400, training loss: 0.07894257456064224 = 0.07173066586256027 + 0.001 * 7.211905479431152
Epoch 400, val loss: 0.8683463931083679
Epoch 410, training loss: 0.07033172994852066 = 0.06312300264835358 + 0.001 * 7.208730220794678
Epoch 410, val loss: 0.8833709955215454
Epoch 420, training loss: 0.06294090300798416 = 0.05572295933961868 + 0.001 * 7.217946529388428
Epoch 420, val loss: 0.8984193205833435
Epoch 430, training loss: 0.0565803125500679 = 0.04937554895877838 + 0.001 * 7.204763412475586
Epoch 430, val loss: 0.9133670926094055
Epoch 440, training loss: 0.05113624781370163 = 0.04393349960446358 + 0.001 * 7.202749729156494
Epoch 440, val loss: 0.9281189441680908
Epoch 450, training loss: 0.04646499454975128 = 0.03926359862089157 + 0.001 * 7.2013959884643555
Epoch 450, val loss: 0.9425473213195801
Epoch 460, training loss: 0.04243660345673561 = 0.03524685651063919 + 0.001 * 7.189745903015137
Epoch 460, val loss: 0.9566351771354675
Epoch 470, training loss: 0.0389975868165493 = 0.03178137540817261 + 0.001 * 7.216211795806885
Epoch 470, val loss: 0.9704067707061768
Epoch 480, training loss: 0.03597423434257507 = 0.028779761865735054 + 0.001 * 7.19447135925293
Epoch 480, val loss: 0.9838109612464905
Epoch 490, training loss: 0.03334523364901543 = 0.026169907301664352 + 0.001 * 7.175324440002441
Epoch 490, val loss: 0.9968702793121338
Epoch 500, training loss: 0.03107560984790325 = 0.023891234770417213 + 0.001 * 7.184375286102295
Epoch 500, val loss: 1.0095255374908447
Epoch 510, training loss: 0.029063178226351738 = 0.021893199533224106 + 0.001 * 7.169978141784668
Epoch 510, val loss: 1.021762728691101
Epoch 520, training loss: 0.027308043092489243 = 0.02013396844267845 + 0.001 * 7.174073219299316
Epoch 520, val loss: 1.0336155891418457
Epoch 530, training loss: 0.025730770081281662 = 0.01857874169945717 + 0.001 * 7.152027606964111
Epoch 530, val loss: 1.0451170206069946
Epoch 540, training loss: 0.024385672062635422 = 0.017198368906974792 + 0.001 * 7.187302112579346
Epoch 540, val loss: 1.0562134981155396
Epoch 550, training loss: 0.023141935467720032 = 0.015968317165970802 + 0.001 * 7.173617362976074
Epoch 550, val loss: 1.0669952630996704
Epoch 560, training loss: 0.02203049324452877 = 0.014868193306028843 + 0.001 * 7.162299633026123
Epoch 560, val loss: 1.0774126052856445
Epoch 570, training loss: 0.021029654890298843 = 0.013880700804293156 + 0.001 * 7.148952960968018
Epoch 570, val loss: 1.08750319480896
Epoch 580, training loss: 0.020134948194026947 = 0.012991335242986679 + 0.001 * 7.143612384796143
Epoch 580, val loss: 1.0972404479980469
Epoch 590, training loss: 0.019347455352544785 = 0.01218780130147934 + 0.001 * 7.159653186798096
Epoch 590, val loss: 1.1067038774490356
Epoch 600, training loss: 0.018603038042783737 = 0.01145948376506567 + 0.001 * 7.1435546875
Epoch 600, val loss: 1.11587655544281
Epoch 610, training loss: 0.01794532500207424 = 0.010797372087836266 + 0.001 * 7.147952079772949
Epoch 610, val loss: 1.1247270107269287
Epoch 620, training loss: 0.01733766496181488 = 0.010193822905421257 + 0.001 * 7.143841743469238
Epoch 620, val loss: 1.1333181858062744
Epoch 630, training loss: 0.01679578609764576 = 0.009642168879508972 + 0.001 * 7.1536173820495605
Epoch 630, val loss: 1.1416693925857544
Epoch 640, training loss: 0.01627151295542717 = 0.009136839769780636 + 0.001 * 7.134671688079834
Epoch 640, val loss: 1.1497950553894043
Epoch 650, training loss: 0.015811488032341003 = 0.008672700263559818 + 0.001 * 7.138787269592285
Epoch 650, val loss: 1.1576186418533325
Epoch 660, training loss: 0.015370318666100502 = 0.008245410397648811 + 0.001 * 7.124907493591309
Epoch 660, val loss: 1.1652494668960571
Epoch 670, training loss: 0.014971008524298668 = 0.007851094007492065 + 0.001 * 7.119913578033447
Epoch 670, val loss: 1.1726500988006592
Epoch 680, training loss: 0.01460237056016922 = 0.007486481685191393 + 0.001 * 7.115889072418213
Epoch 680, val loss: 1.1798399686813354
Epoch 690, training loss: 0.014294229447841644 = 0.00714865094050765 + 0.001 * 7.145578861236572
Epoch 690, val loss: 1.186832308769226
Epoch 700, training loss: 0.013948437757790089 = 0.006835066247731447 + 0.001 * 7.1133713722229
Epoch 700, val loss: 1.1936525106430054
Epoch 710, training loss: 0.01368077751249075 = 0.006543450057506561 + 0.001 * 7.137327194213867
Epoch 710, val loss: 1.200273036956787
Epoch 720, training loss: 0.0133904367685318 = 0.006271809339523315 + 0.001 * 7.118627071380615
Epoch 720, val loss: 1.2067444324493408
Epoch 730, training loss: 0.013131929561495781 = 0.00601832615211606 + 0.001 * 7.113602638244629
Epoch 730, val loss: 1.2129911184310913
Epoch 740, training loss: 0.012906471267342567 = 0.005781433545053005 + 0.001 * 7.12503719329834
Epoch 740, val loss: 1.2190991640090942
Epoch 750, training loss: 0.012691696174442768 = 0.0055596898309886456 + 0.001 * 7.1320061683654785
Epoch 750, val loss: 1.2250522375106812
Epoch 760, training loss: 0.012453081086277962 = 0.005351804196834564 + 0.001 * 7.101276874542236
Epoch 760, val loss: 1.2308316230773926
Epoch 770, training loss: 0.012265454977750778 = 0.005156694445759058 + 0.001 * 7.108760356903076
Epoch 770, val loss: 1.2364667654037476
Epoch 780, training loss: 0.012085380963981152 = 0.004973284434527159 + 0.001 * 7.112096309661865
Epoch 780, val loss: 1.2419674396514893
Epoch 790, training loss: 0.011929194442927837 = 0.004800677299499512 + 0.001 * 7.128516674041748
Epoch 790, val loss: 1.247321605682373
Epoch 800, training loss: 0.011751295998692513 = 0.004637913312762976 + 0.001 * 7.113382816314697
Epoch 800, val loss: 1.2525346279144287
Epoch 810, training loss: 0.011591112241148949 = 0.0044841887429356575 + 0.001 * 7.106922626495361
Epoch 810, val loss: 1.257608413696289
Epoch 820, training loss: 0.011459733359515667 = 0.004338388796895742 + 0.001 * 7.121344089508057
Epoch 820, val loss: 1.2625579833984375
Epoch 830, training loss: 0.011308329179883003 = 0.004199407063424587 + 0.001 * 7.108922004699707
Epoch 830, val loss: 1.26738703250885
Epoch 840, training loss: 0.011174589395523071 = 0.004066040739417076 + 0.001 * 7.108547687530518
Epoch 840, val loss: 1.2720776796340942
Epoch 850, training loss: 0.011030438356101513 = 0.003937651868909597 + 0.001 * 7.0927863121032715
Epoch 850, val loss: 1.2766467332839966
Epoch 860, training loss: 0.010905765928328037 = 0.0038138823583722115 + 0.001 * 7.091883182525635
Epoch 860, val loss: 1.281116008758545
Epoch 870, training loss: 0.010776124894618988 = 0.003694683313369751 + 0.001 * 7.0814409255981445
Epoch 870, val loss: 1.2854501008987427
Epoch 880, training loss: 0.010726278647780418 = 0.0035801606718450785 + 0.001 * 7.146117687225342
Epoch 880, val loss: 1.2897223234176636
Epoch 890, training loss: 0.010562196373939514 = 0.0034703679848462343 + 0.001 * 7.091828346252441
Epoch 890, val loss: 1.2938863039016724
Epoch 900, training loss: 0.010470505803823471 = 0.003365263808518648 + 0.001 * 7.1052422523498535
Epoch 900, val loss: 1.2979493141174316
Epoch 910, training loss: 0.010350901633501053 = 0.0032647796906530857 + 0.001 * 7.086122035980225
Epoch 910, val loss: 1.3018701076507568
Epoch 920, training loss: 0.010247871279716492 = 0.0031688271556049585 + 0.001 * 7.079043865203857
Epoch 920, val loss: 1.3056814670562744
Epoch 930, training loss: 0.010155844502151012 = 0.0030772057361900806 + 0.001 * 7.078638553619385
Epoch 930, val loss: 1.3094154596328735
Epoch 940, training loss: 0.010068971663713455 = 0.0029898155480623245 + 0.001 * 7.079155445098877
Epoch 940, val loss: 1.3130329847335815
Epoch 950, training loss: 0.009981589391827583 = 0.0029063932597637177 + 0.001 * 7.0751953125
Epoch 950, val loss: 1.3164958953857422
Epoch 960, training loss: 0.009900226257741451 = 0.002826888347044587 + 0.001 * 7.073338031768799
Epoch 960, val loss: 1.3198442459106445
Epoch 970, training loss: 0.009831242263317108 = 0.002751046558842063 + 0.001 * 7.080195426940918
Epoch 970, val loss: 1.3231165409088135
Epoch 980, training loss: 0.009758910164237022 = 0.0026784909423440695 + 0.001 * 7.080419063568115
Epoch 980, val loss: 1.3262803554534912
Epoch 990, training loss: 0.009671758860349655 = 0.0026089693419635296 + 0.001 * 7.062788963317871
Epoch 990, val loss: 1.3293471336364746
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 1.947008490562439 = 1.9384115934371948 + 0.001 * 8.59684944152832
Epoch 0, val loss: 1.9399852752685547
Epoch 10, training loss: 1.9382206201553345 = 1.9296238422393799 + 0.001 * 8.596813201904297
Epoch 10, val loss: 1.931605339050293
Epoch 20, training loss: 1.9275469779968262 = 1.9189503192901611 + 0.001 * 8.59667682647705
Epoch 20, val loss: 1.9209771156311035
Epoch 30, training loss: 1.9128432273864746 = 1.9042468070983887 + 0.001 * 8.59638786315918
Epoch 30, val loss: 1.9058204889297485
Epoch 40, training loss: 1.891417384147644 = 1.8828216791152954 + 0.001 * 8.595746040344238
Epoch 40, val loss: 1.8834372758865356
Epoch 50, training loss: 1.8611212968826294 = 1.852527141571045 + 0.001 * 8.594182968139648
Epoch 50, val loss: 1.852394938468933
Epoch 60, training loss: 1.8249881267547607 = 1.8163983821868896 + 0.001 * 8.589714050292969
Epoch 60, val loss: 1.8177404403686523
Epoch 70, training loss: 1.7928187847137451 = 1.784245491027832 + 0.001 * 8.573264122009277
Epoch 70, val loss: 1.7897001504898071
Epoch 80, training loss: 1.758294701576233 = 1.7498234510421753 + 0.001 * 8.471277236938477
Epoch 80, val loss: 1.7590733766555786
Epoch 90, training loss: 1.7091566324234009 = 1.701109766960144 + 0.001 * 8.046813011169434
Epoch 90, val loss: 1.71574068069458
Epoch 100, training loss: 1.641822338104248 = 1.6340121030807495 + 0.001 * 7.81026554107666
Epoch 100, val loss: 1.6565136909484863
Epoch 110, training loss: 1.5547963380813599 = 1.5471761226654053 + 0.001 * 7.620168209075928
Epoch 110, val loss: 1.583255648612976
Epoch 120, training loss: 1.453079342842102 = 1.4455198049545288 + 0.001 * 7.559500217437744
Epoch 120, val loss: 1.4978668689727783
Epoch 130, training loss: 1.342650294303894 = 1.3351436853408813 + 0.001 * 7.506551742553711
Epoch 130, val loss: 1.4058729410171509
Epoch 140, training loss: 1.2275372743606567 = 1.2200732231140137 + 0.001 * 7.464006423950195
Epoch 140, val loss: 1.3108690977096558
Epoch 150, training loss: 1.1117780208587646 = 1.104339361190796 + 0.001 * 7.438689708709717
Epoch 150, val loss: 1.2164788246154785
Epoch 160, training loss: 1.0007115602493286 = 0.9932969808578491 + 0.001 * 7.414559364318848
Epoch 160, val loss: 1.1279289722442627
Epoch 170, training loss: 0.898344874382019 = 0.890953540802002 + 0.001 * 7.391314506530762
Epoch 170, val loss: 1.0488168001174927
Epoch 180, training loss: 0.8068735003471375 = 0.7995011806488037 + 0.001 * 7.372306823730469
Epoch 180, val loss: 0.9805400967597961
Epoch 190, training loss: 0.7276163101196289 = 0.7202611565589905 + 0.001 * 7.355154037475586
Epoch 190, val loss: 0.9245775938034058
Epoch 200, training loss: 0.6602846384048462 = 0.6529527902603149 + 0.001 * 7.331871032714844
Epoch 200, val loss: 0.8812423348426819
Epoch 210, training loss: 0.6026496291160583 = 0.5953488945960999 + 0.001 * 7.300720691680908
Epoch 210, val loss: 0.8488022685050964
Epoch 220, training loss: 0.5521639585494995 = 0.5448870658874512 + 0.001 * 7.276900768280029
Epoch 220, val loss: 0.8254331350326538
Epoch 230, training loss: 0.5068334937095642 = 0.4995739459991455 + 0.001 * 7.2595648765563965
Epoch 230, val loss: 0.8091380000114441
Epoch 240, training loss: 0.4650821089744568 = 0.45783036947250366 + 0.001 * 7.25173807144165
Epoch 240, val loss: 0.7973881959915161
Epoch 250, training loss: 0.42567628622055054 = 0.4184269607067108 + 0.001 * 7.249314785003662
Epoch 250, val loss: 0.788692831993103
Epoch 260, training loss: 0.3879640996456146 = 0.38071686029434204 + 0.001 * 7.247244834899902
Epoch 260, val loss: 0.7820534110069275
Epoch 270, training loss: 0.3518580496311188 = 0.34461256861686707 + 0.001 * 7.245491981506348
Epoch 270, val loss: 0.7771854400634766
Epoch 280, training loss: 0.31767603754997253 = 0.3104322850704193 + 0.001 * 7.243742942810059
Epoch 280, val loss: 0.7741903066635132
Epoch 290, training loss: 0.2858448922634125 = 0.2786036431789398 + 0.001 * 7.241250514984131
Epoch 290, val loss: 0.7734529376029968
Epoch 300, training loss: 0.2565577030181885 = 0.2493184357881546 + 0.001 * 7.239280700683594
Epoch 300, val loss: 0.7751612067222595
Epoch 310, training loss: 0.22978562116622925 = 0.22255246341228485 + 0.001 * 7.233152866363525
Epoch 310, val loss: 0.7792565226554871
Epoch 320, training loss: 0.20541547238826752 = 0.19818608462810516 + 0.001 * 7.22939395904541
Epoch 320, val loss: 0.7857104539871216
Epoch 330, training loss: 0.18332235515117645 = 0.17610102891921997 + 0.001 * 7.2213263511657715
Epoch 330, val loss: 0.7941429615020752
Epoch 340, training loss: 0.16344688832759857 = 0.1562233418226242 + 0.001 * 7.223540306091309
Epoch 340, val loss: 0.8044293522834778
Epoch 350, training loss: 0.1456959843635559 = 0.13848784565925598 + 0.001 * 7.208137512207031
Epoch 350, val loss: 0.8162922859191895
Epoch 360, training loss: 0.1299709975719452 = 0.12277285009622574 + 0.001 * 7.198153972625732
Epoch 360, val loss: 0.8295318484306335
Epoch 370, training loss: 0.1161268949508667 = 0.10893119126558304 + 0.001 * 7.195706367492676
Epoch 370, val loss: 0.8439173698425293
Epoch 380, training loss: 0.10397421568632126 = 0.09677959233522415 + 0.001 * 7.194622039794922
Epoch 380, val loss: 0.8593424558639526
Epoch 390, training loss: 0.0933249443769455 = 0.08614204078912735 + 0.001 * 7.182902812957764
Epoch 390, val loss: 0.8755371570587158
Epoch 400, training loss: 0.08403031527996063 = 0.07684613764286041 + 0.001 * 7.184177875518799
Epoch 400, val loss: 0.8922778367996216
Epoch 410, training loss: 0.07591645419597626 = 0.06873378157615662 + 0.001 * 7.182672500610352
Epoch 410, val loss: 0.9094334244728088
Epoch 420, training loss: 0.06882421672344208 = 0.061654336750507355 + 0.001 * 7.169879913330078
Epoch 420, val loss: 0.9268596172332764
Epoch 430, training loss: 0.06263793259859085 = 0.05547073110938072 + 0.001 * 7.167201995849609
Epoch 430, val loss: 0.9442930221557617
Epoch 440, training loss: 0.05723375827074051 = 0.05006309971213341 + 0.001 * 7.170659065246582
Epoch 440, val loss: 0.9615873098373413
Epoch 450, training loss: 0.052484191954135895 = 0.04532425105571747 + 0.001 * 7.159942626953125
Epoch 450, val loss: 0.9786885380744934
Epoch 460, training loss: 0.048324525356292725 = 0.04116176813840866 + 0.001 * 7.16275691986084
Epoch 460, val loss: 0.995549201965332
Epoch 470, training loss: 0.0446515753865242 = 0.03749566525220871 + 0.001 * 7.155911445617676
Epoch 470, val loss: 1.0120714902877808
Epoch 480, training loss: 0.0414205826818943 = 0.03425756096839905 + 0.001 * 7.163021564483643
Epoch 480, val loss: 1.0282913446426392
Epoch 490, training loss: 0.038543082773685455 = 0.03138948976993561 + 0.001 * 7.15359354019165
Epoch 490, val loss: 1.04410719871521
Epoch 500, training loss: 0.03599340841174126 = 0.028841668739914894 + 0.001 * 7.151740550994873
Epoch 500, val loss: 1.0595972537994385
Epoch 510, training loss: 0.03371790051460266 = 0.026571646332740784 + 0.001 * 7.146254062652588
Epoch 510, val loss: 1.0746601819992065
Epoch 520, training loss: 0.031685613095760345 = 0.02454320341348648 + 0.001 * 7.142411231994629
Epoch 520, val loss: 1.0893230438232422
Epoch 530, training loss: 0.02986927144229412 = 0.02272571437060833 + 0.001 * 7.143556118011475
Epoch 530, val loss: 1.1035864353179932
Epoch 540, training loss: 0.02823096327483654 = 0.0210928563028574 + 0.001 * 7.138106346130371
Epoch 540, val loss: 1.117403507232666
Epoch 550, training loss: 0.02676558680832386 = 0.01962219551205635 + 0.001 * 7.1433916091918945
Epoch 550, val loss: 1.1307944059371948
Epoch 560, training loss: 0.025427263230085373 = 0.018294405192136765 + 0.001 * 7.132857799530029
Epoch 560, val loss: 1.1438044309616089
Epoch 570, training loss: 0.024222109466791153 = 0.017092609778046608 + 0.001 * 7.129498481750488
Epoch 570, val loss: 1.1564081907272339
Epoch 580, training loss: 0.023130057379603386 = 0.016002235934138298 + 0.001 * 7.127821445465088
Epoch 580, val loss: 1.1686073541641235
Epoch 590, training loss: 0.02214365452528 = 0.015010594390332699 + 0.001 * 7.133059024810791
Epoch 590, val loss: 1.1804343461990356
Epoch 600, training loss: 0.021236587315797806 = 0.014106626622378826 + 0.001 * 7.129960060119629
Epoch 600, val loss: 1.1918977499008179
Epoch 610, training loss: 0.02041027694940567 = 0.013280915096402168 + 0.001 * 7.129360675811768
Epoch 610, val loss: 1.2030103206634521
Epoch 620, training loss: 0.019649716094136238 = 0.012524978257715702 + 0.001 * 7.12473726272583
Epoch 620, val loss: 1.2138137817382812
Epoch 630, training loss: 0.018956512212753296 = 0.011831541545689106 + 0.001 * 7.124969482421875
Epoch 630, val loss: 1.2242825031280518
Epoch 640, training loss: 0.01831415854394436 = 0.011194215156137943 + 0.001 * 7.119942665100098
Epoch 640, val loss: 1.2344239950180054
Epoch 650, training loss: 0.01773689314723015 = 0.010607309639453888 + 0.001 * 7.129583358764648
Epoch 650, val loss: 1.2442702054977417
Epoch 660, training loss: 0.01717940904200077 = 0.01006585918366909 + 0.001 * 7.11354923248291
Epoch 660, val loss: 1.2538291215896606
Epoch 670, training loss: 0.01667606085538864 = 0.009565473534166813 + 0.001 * 7.110586643218994
Epoch 670, val loss: 1.2631264925003052
Epoch 680, training loss: 0.016208697110414505 = 0.009102159179747105 + 0.001 * 7.106537342071533
Epoch 680, val loss: 1.272160530090332
Epoch 690, training loss: 0.01579936593770981 = 0.008672472089529037 + 0.001 * 7.126893043518066
Epoch 690, val loss: 1.2809711694717407
Epoch 700, training loss: 0.015388745814561844 = 0.008273324929177761 + 0.001 * 7.115420341491699
Epoch 700, val loss: 1.2894690036773682
Epoch 710, training loss: 0.015004618093371391 = 0.007902046665549278 + 0.001 * 7.102570533752441
Epoch 710, val loss: 1.2977815866470337
Epoch 720, training loss: 0.01465296745300293 = 0.007556074298918247 + 0.001 * 7.096892833709717
Epoch 720, val loss: 1.3058429956436157
Epoch 730, training loss: 0.014327116310596466 = 0.0072332690469920635 + 0.001 * 7.093847274780273
Epoch 730, val loss: 1.3136647939682007
Epoch 740, training loss: 0.014036498963832855 = 0.006931664422154427 + 0.001 * 7.104834079742432
Epoch 740, val loss: 1.3213410377502441
Epoch 750, training loss: 0.01373862475156784 = 0.006649463903158903 + 0.001 * 7.089160442352295
Epoch 750, val loss: 1.3287739753723145
Epoch 760, training loss: 0.013484933413565159 = 0.006385098677128553 + 0.001 * 7.099834442138672
Epoch 760, val loss: 1.3360484838485718
Epoch 770, training loss: 0.01324646919965744 = 0.006137087009847164 + 0.001 * 7.109382152557373
Epoch 770, val loss: 1.343111276626587
Epoch 780, training loss: 0.012993553653359413 = 0.005904178600758314 + 0.001 * 7.089375019073486
Epoch 780, val loss: 1.3499871492385864
Epoch 790, training loss: 0.012790251523256302 = 0.005685151554644108 + 0.001 * 7.105099201202393
Epoch 790, val loss: 1.3567110300064087
Epoch 800, training loss: 0.012563450261950493 = 0.005478952080011368 + 0.001 * 7.084498405456543
Epoch 800, val loss: 1.3632484674453735
Epoch 810, training loss: 0.012365476228296757 = 0.005284626502543688 + 0.001 * 7.0808491706848145
Epoch 810, val loss: 1.369649887084961
Epoch 820, training loss: 0.012180859223008156 = 0.00510128028690815 + 0.001 * 7.079577922821045
Epoch 820, val loss: 1.3758776187896729
Epoch 830, training loss: 0.012010632082819939 = 0.004928121343255043 + 0.001 * 7.082509994506836
Epoch 830, val loss: 1.3819600343704224
Epoch 840, training loss: 0.01186012476682663 = 0.004764432553201914 + 0.001 * 7.095692157745361
Epoch 840, val loss: 1.387912631034851
Epoch 850, training loss: 0.011702102608978748 = 0.004609496332705021 + 0.001 * 7.092606067657471
Epoch 850, val loss: 1.3936737775802612
Epoch 860, training loss: 0.011542913503944874 = 0.004462792072445154 + 0.001 * 7.080121040344238
Epoch 860, val loss: 1.3993332386016846
Epoch 870, training loss: 0.011402256786823273 = 0.004323695786297321 + 0.001 * 7.0785603523254395
Epoch 870, val loss: 1.404862403869629
Epoch 880, training loss: 0.011268150992691517 = 0.0041916873306035995 + 0.001 * 7.076463222503662
Epoch 880, val loss: 1.4102298021316528
Epoch 890, training loss: 0.011152179911732674 = 0.004066316410899162 + 0.001 * 7.0858635902404785
Epoch 890, val loss: 1.4155142307281494
Epoch 900, training loss: 0.011038405820727348 = 0.003947166725993156 + 0.001 * 7.091238975524902
Epoch 900, val loss: 1.4206491708755493
Epoch 910, training loss: 0.010918926447629929 = 0.0038337879814207554 + 0.001 * 7.085138320922852
Epoch 910, val loss: 1.4256566762924194
Epoch 920, training loss: 0.01079242117702961 = 0.00372587819583714 + 0.001 * 7.066542625427246
Epoch 920, val loss: 1.4305546283721924
Epoch 930, training loss: 0.010687503032386303 = 0.0036230511032044888 + 0.001 * 7.064451694488525
Epoch 930, val loss: 1.4353387355804443
Epoch 940, training loss: 0.010585382580757141 = 0.003525015665218234 + 0.001 * 7.060366630554199
Epoch 940, val loss: 1.440028429031372
Epoch 950, training loss: 0.010502618737518787 = 0.0034314768854528666 + 0.001 * 7.071141719818115
Epoch 950, val loss: 1.4446158409118652
Epoch 960, training loss: 0.010404782369732857 = 0.003342163749039173 + 0.001 * 7.062618255615234
Epoch 960, val loss: 1.4490540027618408
Epoch 970, training loss: 0.010330867022275925 = 0.0032568504102528095 + 0.001 * 7.074016571044922
Epoch 970, val loss: 1.4534300565719604
Epoch 980, training loss: 0.010236737318336964 = 0.003175244200974703 + 0.001 * 7.061492919921875
Epoch 980, val loss: 1.4576797485351562
Epoch 990, training loss: 0.010150399059057236 = 0.003097211243584752 + 0.001 * 7.053187370300293
Epoch 990, val loss: 1.4618498086929321
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 1.946043610572815 = 1.9374467134475708 + 0.001 * 8.596844673156738
Epoch 0, val loss: 1.9334696531295776
Epoch 10, training loss: 1.935532569885254 = 1.9269357919692993 + 0.001 * 8.596789360046387
Epoch 10, val loss: 1.92215096950531
Epoch 20, training loss: 1.9224622249603271 = 1.913865566253662 + 0.001 * 8.596632957458496
Epoch 20, val loss: 1.9079347848892212
Epoch 30, training loss: 1.9042590856552124 = 1.895662784576416 + 0.001 * 8.596277236938477
Epoch 30, val loss: 1.8881590366363525
Epoch 40, training loss: 1.8783258199691772 = 1.8697303533554077 + 0.001 * 8.595419883728027
Epoch 40, val loss: 1.8605021238327026
Epoch 50, training loss: 1.845213770866394 = 1.8366206884384155 + 0.001 * 8.593067169189453
Epoch 50, val loss: 1.8271628618240356
Epoch 60, training loss: 1.8138301372528076 = 1.8052449226379395 + 0.001 * 8.58524227142334
Epoch 60, val loss: 1.7997276782989502
Epoch 70, training loss: 1.7860103845596313 = 1.7774578332901 + 0.001 * 8.552569389343262
Epoch 70, val loss: 1.778620958328247
Epoch 80, training loss: 1.7472686767578125 = 1.738917589187622 + 0.001 * 8.351112365722656
Epoch 80, val loss: 1.7479288578033447
Epoch 90, training loss: 1.6919422149658203 = 1.6838462352752686 + 0.001 * 8.096007347106934
Epoch 90, val loss: 1.7019318342208862
Epoch 100, training loss: 1.6167619228363037 = 1.608716368675232 + 0.001 * 8.045547485351562
Epoch 100, val loss: 1.6398189067840576
Epoch 110, training loss: 1.5301074981689453 = 1.5220866203308105 + 0.001 * 8.02083683013916
Epoch 110, val loss: 1.5714645385742188
Epoch 120, training loss: 1.4431530237197876 = 1.4351507425308228 + 0.001 * 8.002302169799805
Epoch 120, val loss: 1.5069682598114014
Epoch 130, training loss: 1.3596959114074707 = 1.351743221282959 + 0.001 * 7.952682018280029
Epoch 130, val loss: 1.4501866102218628
Epoch 140, training loss: 1.278273344039917 = 1.2704684734344482 + 0.001 * 7.804911136627197
Epoch 140, val loss: 1.398421287536621
Epoch 150, training loss: 1.1993728876113892 = 1.1917093992233276 + 0.001 * 7.663430690765381
Epoch 150, val loss: 1.3504875898361206
Epoch 160, training loss: 1.124237060546875 = 1.116694688796997 + 0.001 * 7.542420387268066
Epoch 160, val loss: 1.3066364526748657
Epoch 170, training loss: 1.053308129310608 = 1.0458295345306396 + 0.001 * 7.478639602661133
Epoch 170, val loss: 1.2649275064468384
Epoch 180, training loss: 0.9858890771865845 = 0.978438138961792 + 0.001 * 7.450911998748779
Epoch 180, val loss: 1.2236144542694092
Epoch 190, training loss: 0.9208849668502808 = 0.9134514927864075 + 0.001 * 7.433483123779297
Epoch 190, val loss: 1.1818749904632568
Epoch 200, training loss: 0.8568599224090576 = 0.8494415879249573 + 0.001 * 7.4183478355407715
Epoch 200, val loss: 1.1391730308532715
Epoch 210, training loss: 0.7926162481307983 = 0.7852070331573486 + 0.001 * 7.409191131591797
Epoch 210, val loss: 1.0956249237060547
Epoch 220, training loss: 0.7282702326774597 = 0.7208677530288696 + 0.001 * 7.402493476867676
Epoch 220, val loss: 1.0516215562820435
Epoch 230, training loss: 0.6652829051017761 = 0.657884955406189 + 0.001 * 7.397931098937988
Epoch 230, val loss: 1.0091466903686523
Epoch 240, training loss: 0.6054412722587585 = 0.5980466604232788 + 0.001 * 7.394618988037109
Epoch 240, val loss: 0.9703347682952881
Epoch 250, training loss: 0.5496634840965271 = 0.5422716736793518 + 0.001 * 7.391801834106445
Epoch 250, val loss: 0.9369137287139893
Epoch 260, training loss: 0.49760136008262634 = 0.4902128279209137 + 0.001 * 7.388542652130127
Epoch 260, val loss: 0.9091613292694092
Epoch 270, training loss: 0.4483951926231384 = 0.4410114288330078 + 0.001 * 7.383751392364502
Epoch 270, val loss: 0.887309193611145
Epoch 280, training loss: 0.4015957713127136 = 0.3942205309867859 + 0.001 * 7.375239372253418
Epoch 280, val loss: 0.8716492056846619
Epoch 290, training loss: 0.35786956548690796 = 0.3505113124847412 + 0.001 * 7.358251094818115
Epoch 290, val loss: 0.8623548746109009
Epoch 300, training loss: 0.3185442388057709 = 0.3112086057662964 + 0.001 * 7.335634231567383
Epoch 300, val loss: 0.8596805930137634
Epoch 310, training loss: 0.284365177154541 = 0.2770591080188751 + 0.001 * 7.306064128875732
Epoch 310, val loss: 0.8632916212081909
Epoch 320, training loss: 0.25492778420448303 = 0.24764752388000488 + 0.001 * 7.280252456665039
Epoch 320, val loss: 0.8719961643218994
Epoch 330, training loss: 0.22904570400714874 = 0.22177962958812714 + 0.001 * 7.266078472137451
Epoch 330, val loss: 0.884076714515686
Epoch 340, training loss: 0.20539167523384094 = 0.19812820851802826 + 0.001 * 7.263467788696289
Epoch 340, val loss: 0.897865891456604
Epoch 350, training loss: 0.1831667423248291 = 0.17590494453907013 + 0.001 * 7.261792182922363
Epoch 350, val loss: 0.9122326374053955
Epoch 360, training loss: 0.1622491478919983 = 0.154987171292305 + 0.001 * 7.261975288391113
Epoch 360, val loss: 0.9271690845489502
Epoch 370, training loss: 0.14295093715190887 = 0.13569006323814392 + 0.001 * 7.260879039764404
Epoch 370, val loss: 0.943220853805542
Epoch 380, training loss: 0.12564003467559814 = 0.11837446689605713 + 0.001 * 7.265567302703857
Epoch 380, val loss: 0.960944414138794
Epoch 390, training loss: 0.11046423763036728 = 0.10320229828357697 + 0.001 * 7.261939525604248
Epoch 390, val loss: 0.9806725382804871
Epoch 400, training loss: 0.09735335409641266 = 0.0900854542851448 + 0.001 * 7.26789665222168
Epoch 400, val loss: 1.0024926662445068
Epoch 410, training loss: 0.0861043781042099 = 0.07884135842323303 + 0.001 * 7.263020992279053
Epoch 410, val loss: 1.0261688232421875
Epoch 420, training loss: 0.07648353278636932 = 0.06922467052936554 + 0.001 * 7.258861064910889
Epoch 420, val loss: 1.051212191581726
Epoch 430, training loss: 0.06827173382043839 = 0.06101006641983986 + 0.001 * 7.261666774749756
Epoch 430, val loss: 1.0771068334579468
Epoch 440, training loss: 0.06125026196241379 = 0.0539940781891346 + 0.001 * 7.2561821937561035
Epoch 440, val loss: 1.1032929420471191
Epoch 450, training loss: 0.055247947573661804 = 0.04799409583210945 + 0.001 * 7.253851413726807
Epoch 450, val loss: 1.129396677017212
Epoch 460, training loss: 0.050104714930057526 = 0.042849909514188766 + 0.001 * 7.254806041717529
Epoch 460, val loss: 1.1551580429077148
Epoch 470, training loss: 0.045678529888391495 = 0.03842515870928764 + 0.001 * 7.253370761871338
Epoch 470, val loss: 1.1805044412612915
Epoch 480, training loss: 0.04185059294104576 = 0.03460549935698509 + 0.001 * 7.245092391967773
Epoch 480, val loss: 1.2052996158599854
Epoch 490, training loss: 0.038536280393600464 = 0.03129499778151512 + 0.001 * 7.241282939910889
Epoch 490, val loss: 1.229475975036621
Epoch 500, training loss: 0.03565562516450882 = 0.028415372595191002 + 0.001 * 7.24025297164917
Epoch 500, val loss: 1.2529957294464111
Epoch 510, training loss: 0.03312516212463379 = 0.025899983942508698 + 0.001 * 7.2251763343811035
Epoch 510, val loss: 1.2758313417434692
Epoch 520, training loss: 0.030921045690774918 = 0.02369380183517933 + 0.001 * 7.227242469787598
Epoch 520, val loss: 1.2979936599731445
Epoch 530, training loss: 0.02897847630083561 = 0.021751614287495613 + 0.001 * 7.226861476898193
Epoch 530, val loss: 1.3195143938064575
Epoch 540, training loss: 0.027280358597636223 = 0.020035015419125557 + 0.001 * 7.245342254638672
Epoch 540, val loss: 1.340383768081665
Epoch 550, training loss: 0.025720804929733276 = 0.01851213350892067 + 0.001 * 7.2086710929870605
Epoch 550, val loss: 1.3606024980545044
Epoch 560, training loss: 0.024357065558433533 = 0.017156269401311874 + 0.001 * 7.2007951736450195
Epoch 560, val loss: 1.3801974058151245
Epoch 570, training loss: 0.02320341393351555 = 0.0159449465572834 + 0.001 * 7.258466720581055
Epoch 570, val loss: 1.399186134338379
Epoch 580, training loss: 0.022061845287680626 = 0.0148590337485075 + 0.001 * 7.202811241149902
Epoch 580, val loss: 1.41758131980896
Epoch 590, training loss: 0.02106916904449463 = 0.01388221513479948 + 0.001 * 7.186953067779541
Epoch 590, val loss: 1.4354280233383179
Epoch 600, training loss: 0.020180867984890938 = 0.013000722974538803 + 0.001 * 7.180144786834717
Epoch 600, val loss: 1.4527279138565063
Epoch 610, training loss: 0.019400104880332947 = 0.012202879413962364 + 0.001 * 7.197225093841553
Epoch 610, val loss: 1.4695037603378296
Epoch 620, training loss: 0.018656333908438683 = 0.011478695087134838 + 0.001 * 7.177639007568359
Epoch 620, val loss: 1.4857792854309082
Epoch 630, training loss: 0.01798984594643116 = 0.010819495655596256 + 0.001 * 7.170350074768066
Epoch 630, val loss: 1.501560926437378
Epoch 640, training loss: 0.017378341406583786 = 0.010217799805104733 + 0.001 * 7.160542011260986
Epoch 640, val loss: 1.5168702602386475
Epoch 650, training loss: 0.016833795234560966 = 0.009667212143540382 + 0.001 * 7.1665825843811035
Epoch 650, val loss: 1.5317262411117554
Epoch 660, training loss: 0.01633516140282154 = 0.009162160567939281 + 0.001 * 7.172999858856201
Epoch 660, val loss: 1.5461605787277222
Epoch 670, training loss: 0.015854110941290855 = 0.008697832003235817 + 0.001 * 7.156278610229492
Epoch 670, val loss: 1.5601760149002075
Epoch 680, training loss: 0.015423338860273361 = 0.00826992467045784 + 0.001 * 7.153413772583008
Epoch 680, val loss: 1.573804259300232
Epoch 690, training loss: 0.015018673613667488 = 0.007874780334532261 + 0.001 * 7.143892765045166
Epoch 690, val loss: 1.5870575904846191
Epoch 700, training loss: 0.014649314805865288 = 0.00750917149707675 + 0.001 * 7.140142440795898
Epoch 700, val loss: 1.5999481678009033
Epoch 710, training loss: 0.014314587228000164 = 0.0071702213026583195 + 0.001 * 7.1443657875061035
Epoch 710, val loss: 1.6124918460845947
Epoch 720, training loss: 0.013990896753966808 = 0.006855399813503027 + 0.001 * 7.135496616363525
Epoch 720, val loss: 1.624706506729126
Epoch 730, training loss: 0.013703695498406887 = 0.00656249700114131 + 0.001 * 7.14119815826416
Epoch 730, val loss: 1.6365854740142822
Epoch 740, training loss: 0.013415883295238018 = 0.006289535667747259 + 0.001 * 7.126347064971924
Epoch 740, val loss: 1.6481573581695557
Epoch 750, training loss: 0.013175148516893387 = 0.006034718826413155 + 0.001 * 7.140429973602295
Epoch 750, val loss: 1.6594387292861938
Epoch 760, training loss: 0.012948838993906975 = 0.005796436220407486 + 0.001 * 7.152402877807617
Epoch 760, val loss: 1.6704366207122803
Epoch 770, training loss: 0.012711290270090103 = 0.0055733490735292435 + 0.001 * 7.137940883636475
Epoch 770, val loss: 1.6811577081680298
Epoch 780, training loss: 0.012484955601394176 = 0.00536407670006156 + 0.001 * 7.12087869644165
Epoch 780, val loss: 1.6916184425354004
Epoch 790, training loss: 0.012294839136302471 = 0.005167454946786165 + 0.001 * 7.127383708953857
Epoch 790, val loss: 1.7018252611160278
Epoch 800, training loss: 0.012100161984562874 = 0.004982393700629473 + 0.001 * 7.117767333984375
Epoch 800, val loss: 1.7117855548858643
Epoch 810, training loss: 0.011936386115849018 = 0.004807646851986647 + 0.001 * 7.128738880157471
Epoch 810, val loss: 1.721509337425232
Epoch 820, training loss: 0.011774532496929169 = 0.004642080515623093 + 0.001 * 7.13245153427124
Epoch 820, val loss: 1.730999231338501
Epoch 830, training loss: 0.011613176204264164 = 0.004484645090997219 + 0.001 * 7.128530979156494
Epoch 830, val loss: 1.7402772903442383
Epoch 840, training loss: 0.011437230743467808 = 0.004334585275501013 + 0.001 * 7.102644920349121
Epoch 840, val loss: 1.7493619918823242
Epoch 850, training loss: 0.01130293495953083 = 0.004191386513411999 + 0.001 * 7.111547470092773
Epoch 850, val loss: 1.7582917213439941
Epoch 860, training loss: 0.011171700432896614 = 0.004054512362927198 + 0.001 * 7.117187976837158
Epoch 860, val loss: 1.7670660018920898
Epoch 870, training loss: 0.011026041582226753 = 0.003923683427274227 + 0.001 * 7.102357387542725
Epoch 870, val loss: 1.7757302522659302
Epoch 880, training loss: 0.010896502062678337 = 0.0037988012190908194 + 0.001 * 7.097700595855713
Epoch 880, val loss: 1.7842676639556885
Epoch 890, training loss: 0.01078764908015728 = 0.003679645946249366 + 0.001 * 7.10800313949585
Epoch 890, val loss: 1.7926691770553589
Epoch 900, training loss: 0.010702051222324371 = 0.003565858118236065 + 0.001 * 7.136192321777344
Epoch 900, val loss: 1.8009462356567383
Epoch 910, training loss: 0.010559090413153172 = 0.003457285463809967 + 0.001 * 7.101804733276367
Epoch 910, val loss: 1.8091074228286743
Epoch 920, training loss: 0.010451022535562515 = 0.0033535656984895468 + 0.001 * 7.097456932067871
Epoch 920, val loss: 1.8171497583389282
Epoch 930, training loss: 0.010392209514975548 = 0.0032545607537031174 + 0.001 * 7.137648105621338
Epoch 930, val loss: 1.8250837326049805
Epoch 940, training loss: 0.01023885328322649 = 0.0031600964721292257 + 0.001 * 7.078756809234619
Epoch 940, val loss: 1.8329074382781982
Epoch 950, training loss: 0.010159596800804138 = 0.003069826401770115 + 0.001 * 7.0897698402404785
Epoch 950, val loss: 1.8406274318695068
Epoch 960, training loss: 0.01006370596587658 = 0.002983562648296356 + 0.001 * 7.080143451690674
Epoch 960, val loss: 1.8482458591461182
Epoch 970, training loss: 0.009978755377233028 = 0.002901129424571991 + 0.001 * 7.077625751495361
Epoch 970, val loss: 1.8557618856430054
Epoch 980, training loss: 0.009914066642522812 = 0.002822316251695156 + 0.001 * 7.091750621795654
Epoch 980, val loss: 1.8631870746612549
Epoch 990, training loss: 0.009821821004152298 = 0.002746871905401349 + 0.001 * 7.074948310852051
Epoch 990, val loss: 1.8705250024795532
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8128624143384291
The final CL Acc:0.75556, 0.00800, The final GNN Acc:0.80794, 0.00358
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13172])
remove edge: torch.Size([2, 7914])
updated graph: torch.Size([2, 10530])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9610108137130737 = 1.9524140357971191 + 0.001 * 8.59681224822998
Epoch 0, val loss: 1.9440951347351074
Epoch 10, training loss: 1.9501361846923828 = 1.9415394067764282 + 0.001 * 8.59673023223877
Epoch 10, val loss: 1.9329447746276855
Epoch 20, training loss: 1.9367855787277222 = 1.9281890392303467 + 0.001 * 8.596482276916504
Epoch 20, val loss: 1.9191679954528809
Epoch 30, training loss: 1.9183733463287354 = 1.9097774028778076 + 0.001 * 8.595890045166016
Epoch 30, val loss: 1.9002217054367065
Epoch 40, training loss: 1.891669511795044 = 1.88307523727417 + 0.001 * 8.594324111938477
Epoch 40, val loss: 1.8732327222824097
Epoch 50, training loss: 1.8547585010528564 = 1.846169352531433 + 0.001 * 8.589166641235352
Epoch 50, val loss: 1.8379762172698975
Epoch 60, training loss: 1.8135074377059937 = 1.804940104484558 + 0.001 * 8.567352294921875
Epoch 60, val loss: 1.8032149076461792
Epoch 70, training loss: 1.7786039113998413 = 1.7701665163040161 + 0.001 * 8.43739128112793
Epoch 70, val loss: 1.7778043746948242
Epoch 80, training loss: 1.737096905708313 = 1.7289379835128784 + 0.001 * 8.158931732177734
Epoch 80, val loss: 1.7430247068405151
Epoch 90, training loss: 1.677249789237976 = 1.669232964515686 + 0.001 * 8.01677417755127
Epoch 90, val loss: 1.6901130676269531
Epoch 100, training loss: 1.5972949266433716 = 1.5893737077713013 + 0.001 * 7.921247959136963
Epoch 100, val loss: 1.6215074062347412
Epoch 110, training loss: 1.5036718845367432 = 1.4958438873291016 + 0.001 * 7.828019618988037
Epoch 110, val loss: 1.5441009998321533
Epoch 120, training loss: 1.4082119464874268 = 1.4006065130233765 + 0.001 * 7.605467796325684
Epoch 120, val loss: 1.4676992893218994
Epoch 130, training loss: 1.3160486221313477 = 1.3086116313934326 + 0.001 * 7.436942100524902
Epoch 130, val loss: 1.3961964845657349
Epoch 140, training loss: 1.2275991439819336 = 1.2202377319335938 + 0.001 * 7.361431121826172
Epoch 140, val loss: 1.3291234970092773
Epoch 150, training loss: 1.1427265405654907 = 1.1353909969329834 + 0.001 * 7.335521697998047
Epoch 150, val loss: 1.2654865980148315
Epoch 160, training loss: 1.0623321533203125 = 1.0550038814544678 + 0.001 * 7.328286647796631
Epoch 160, val loss: 1.2053452730178833
Epoch 170, training loss: 0.9870167374610901 = 0.9796932339668274 + 0.001 * 7.323495864868164
Epoch 170, val loss: 1.1483629941940308
Epoch 180, training loss: 0.9160429239273071 = 0.9087255597114563 + 0.001 * 7.317391395568848
Epoch 180, val loss: 1.0933513641357422
Epoch 190, training loss: 0.8472544550895691 = 0.8399470448493958 + 0.001 * 7.3074164390563965
Epoch 190, val loss: 1.0387955904006958
Epoch 200, training loss: 0.7788013815879822 = 0.7715092897415161 + 0.001 * 7.292085647583008
Epoch 200, val loss: 0.9839692115783691
Epoch 210, training loss: 0.7109507322311401 = 0.7036815881729126 + 0.001 * 7.269161701202393
Epoch 210, val loss: 0.9307261109352112
Epoch 220, training loss: 0.646255612373352 = 0.6390175819396973 + 0.001 * 7.238009929656982
Epoch 220, val loss: 0.8825147151947021
Epoch 230, training loss: 0.5876398682594299 = 0.5804283618927002 + 0.001 * 7.211476802825928
Epoch 230, val loss: 0.8425013422966003
Epoch 240, training loss: 0.5361606478691101 = 0.5289628505706787 + 0.001 * 7.197821617126465
Epoch 240, val loss: 0.8114575147628784
Epoch 250, training loss: 0.4909396469593048 = 0.48375391960144043 + 0.001 * 7.185716152191162
Epoch 250, val loss: 0.788277268409729
Epoch 260, training loss: 0.45038124918937683 = 0.443200021982193 + 0.001 * 7.181232452392578
Epoch 260, val loss: 0.7711514830589294
Epoch 270, training loss: 0.4129774570465088 = 0.4057989716529846 + 0.001 * 7.1784772872924805
Epoch 270, val loss: 0.7584894895553589
Epoch 280, training loss: 0.3776549696922302 = 0.3704788386821747 + 0.001 * 7.176133155822754
Epoch 280, val loss: 0.7489742636680603
Epoch 290, training loss: 0.34377601742744446 = 0.3366027772426605 + 0.001 * 7.173228740692139
Epoch 290, val loss: 0.7415461540222168
Epoch 300, training loss: 0.31107765436172485 = 0.30390679836273193 + 0.001 * 7.170840740203857
Epoch 300, val loss: 0.7356148362159729
Epoch 310, training loss: 0.2795630097389221 = 0.2723952531814575 + 0.001 * 7.167767524719238
Epoch 310, val loss: 0.730914831161499
Epoch 320, training loss: 0.24938221275806427 = 0.24221739172935486 + 0.001 * 7.164816856384277
Epoch 320, val loss: 0.7272217273712158
Epoch 330, training loss: 0.22084860503673553 = 0.21368367969989777 + 0.001 * 7.164920806884766
Epoch 330, val loss: 0.7246403098106384
Epoch 340, training loss: 0.19435589015483856 = 0.18719519674777985 + 0.001 * 7.160695552825928
Epoch 340, val loss: 0.723397970199585
Epoch 350, training loss: 0.17036612331867218 = 0.1632101833820343 + 0.001 * 7.155946254730225
Epoch 350, val loss: 0.7237192988395691
Epoch 360, training loss: 0.1491914987564087 = 0.14203789830207825 + 0.001 * 7.153599262237549
Epoch 360, val loss: 0.7258503437042236
Epoch 370, training loss: 0.13092251121997833 = 0.12377114593982697 + 0.001 * 7.151362895965576
Epoch 370, val loss: 0.729783833026886
Epoch 380, training loss: 0.11535418033599854 = 0.108211450278759 + 0.001 * 7.14272928237915
Epoch 380, val loss: 0.735528290271759
Epoch 390, training loss: 0.10212907940149307 = 0.0949895977973938 + 0.001 * 7.139482498168945
Epoch 390, val loss: 0.7428386211395264
Epoch 400, training loss: 0.09085270017385483 = 0.08371637016534805 + 0.001 * 7.136329174041748
Epoch 400, val loss: 0.7514252662658691
Epoch 410, training loss: 0.08118592202663422 = 0.0740533396601677 + 0.001 * 7.132584571838379
Epoch 410, val loss: 0.761141836643219
Epoch 420, training loss: 0.0728716105222702 = 0.0657387375831604 + 0.001 * 7.132872104644775
Epoch 420, val loss: 0.7717165350914001
Epoch 430, training loss: 0.06568147242069244 = 0.05857214331626892 + 0.001 * 7.109330177307129
Epoch 430, val loss: 0.7830060124397278
Epoch 440, training loss: 0.059500064700841904 = 0.05239686742424965 + 0.001 * 7.103198051452637
Epoch 440, val loss: 0.7947642207145691
Epoch 450, training loss: 0.05417286604642868 = 0.047074094414711 + 0.001 * 7.0987725257873535
Epoch 450, val loss: 0.8068940043449402
Epoch 460, training loss: 0.04957691207528114 = 0.04247962683439255 + 0.001 * 7.097285270690918
Epoch 460, val loss: 0.8192598819732666
Epoch 470, training loss: 0.045588068664073944 = 0.03850269690155983 + 0.001 * 7.08536958694458
Epoch 470, val loss: 0.8317418098449707
Epoch 480, training loss: 0.04220430552959442 = 0.03504739701747894 + 0.001 * 7.1569085121154785
Epoch 480, val loss: 0.8442703485488892
Epoch 490, training loss: 0.039121441543102264 = 0.032030656933784485 + 0.001 * 7.090785503387451
Epoch 490, val loss: 0.8566914200782776
Epoch 500, training loss: 0.036465853452682495 = 0.029382657259702682 + 0.001 * 7.083195209503174
Epoch 500, val loss: 0.8689953684806824
Epoch 510, training loss: 0.034125152975320816 = 0.02704586833715439 + 0.001 * 7.079283237457275
Epoch 510, val loss: 0.8811073303222656
Epoch 520, training loss: 0.03204752132296562 = 0.024974534288048744 + 0.001 * 7.0729851722717285
Epoch 520, val loss: 0.8930156826972961
Epoch 530, training loss: 0.030206667259335518 = 0.023130271583795547 + 0.001 * 7.0763959884643555
Epoch 530, val loss: 0.9046623110771179
Epoch 540, training loss: 0.028557704761624336 = 0.02148142084479332 + 0.001 * 7.076282978057861
Epoch 540, val loss: 0.9161147475242615
Epoch 550, training loss: 0.02706953138113022 = 0.020001890137791634 + 0.001 * 7.067640781402588
Epoch 550, val loss: 0.9273008704185486
Epoch 560, training loss: 0.025767864659428596 = 0.018669353798031807 + 0.001 * 7.098510265350342
Epoch 560, val loss: 0.938266396522522
Epoch 570, training loss: 0.024532930925488472 = 0.017465388402342796 + 0.001 * 7.067542552947998
Epoch 570, val loss: 0.9489259123802185
Epoch 580, training loss: 0.02343892678618431 = 0.016374345868825912 + 0.001 * 7.064581394195557
Epoch 580, val loss: 0.959350049495697
Epoch 590, training loss: 0.02246139943599701 = 0.015383006073534489 + 0.001 * 7.078393936157227
Epoch 590, val loss: 0.9695098996162415
Epoch 600, training loss: 0.021546578034758568 = 0.014479655772447586 + 0.001 * 7.066921234130859
Epoch 600, val loss: 0.9794349670410156
Epoch 610, training loss: 0.020716305822134018 = 0.013654391281306744 + 0.001 * 7.061914443969727
Epoch 610, val loss: 0.9891269207000732
Epoch 620, training loss: 0.019979245960712433 = 0.01289779506623745 + 0.001 * 7.081451416015625
Epoch 620, val loss: 0.9985976815223694
Epoch 630, training loss: 0.019266191869974136 = 0.012201155535876751 + 0.001 * 7.065034866333008
Epoch 630, val loss: 1.0078208446502686
Epoch 640, training loss: 0.018613025546073914 = 0.011556071229279041 + 0.001 * 7.056953430175781
Epoch 640, val loss: 1.0169150829315186
Epoch 650, training loss: 0.018016280606389046 = 0.010956446640193462 + 0.001 * 7.059834003448486
Epoch 650, val loss: 1.0258333683013916
Epoch 660, training loss: 0.017454154789447784 = 0.010398230515420437 + 0.001 * 7.055924415588379
Epoch 660, val loss: 1.0346288681030273
Epoch 670, training loss: 0.01694048009812832 = 0.009878380224108696 + 0.001 * 7.062099456787109
Epoch 670, val loss: 1.0432915687561035
Epoch 680, training loss: 0.01644263230264187 = 0.00939419586211443 + 0.001 * 7.048435688018799
Epoch 680, val loss: 1.0518039464950562
Epoch 690, training loss: 0.01599487103521824 = 0.008943145163357258 + 0.001 * 7.0517258644104
Epoch 690, val loss: 1.060165286064148
Epoch 700, training loss: 0.015577897429466248 = 0.008522826246917248 + 0.001 * 7.055070400238037
Epoch 700, val loss: 1.0683934688568115
Epoch 710, training loss: 0.015189802274107933 = 0.008130925707519054 + 0.001 * 7.058875560760498
Epoch 710, val loss: 1.0764533281326294
Epoch 720, training loss: 0.014817386865615845 = 0.007765288930386305 + 0.001 * 7.052097320556641
Epoch 720, val loss: 1.0843510627746582
Epoch 730, training loss: 0.014469858258962631 = 0.007423835340887308 + 0.001 * 7.046022415161133
Epoch 730, val loss: 1.0921075344085693
Epoch 740, training loss: 0.014148712158203125 = 0.007104683667421341 + 0.001 * 7.0440287590026855
Epoch 740, val loss: 1.099692702293396
Epoch 750, training loss: 0.013873185962438583 = 0.006806067656725645 + 0.001 * 7.0671186447143555
Epoch 750, val loss: 1.1071498394012451
Epoch 760, training loss: 0.01356219407171011 = 0.006526344455778599 + 0.001 * 7.035849094390869
Epoch 760, val loss: 1.1144318580627441
Epoch 770, training loss: 0.013294046744704247 = 0.006264034193009138 + 0.001 * 7.030012130737305
Epoch 770, val loss: 1.1215678453445435
Epoch 780, training loss: 0.013075914233922958 = 0.006017790175974369 + 0.001 * 7.058123588562012
Epoch 780, val loss: 1.128551721572876
Epoch 790, training loss: 0.012834768742322922 = 0.005786393769085407 + 0.001 * 7.048375129699707
Epoch 790, val loss: 1.135407567024231
Epoch 800, training loss: 0.012595659121870995 = 0.005568709224462509 + 0.001 * 7.026949882507324
Epoch 800, val loss: 1.1420913934707642
Epoch 810, training loss: 0.012394096702337265 = 0.00536373583599925 + 0.001 * 7.030361175537109
Epoch 810, val loss: 1.1487106084823608
Epoch 820, training loss: 0.012201998382806778 = 0.005170523654669523 + 0.001 * 7.031475067138672
Epoch 820, val loss: 1.1551034450531006
Epoch 830, training loss: 0.012024040333926678 = 0.004988214932382107 + 0.001 * 7.035825252532959
Epoch 830, val loss: 1.1614042520523071
Epoch 840, training loss: 0.011844974011182785 = 0.0048160431906580925 + 0.001 * 7.028930187225342
Epoch 840, val loss: 1.1675344705581665
Epoch 850, training loss: 0.011686486192047596 = 0.00465326476842165 + 0.001 * 7.033221244812012
Epoch 850, val loss: 1.173568844795227
Epoch 860, training loss: 0.011527888476848602 = 0.004499219823628664 + 0.001 * 7.02866792678833
Epoch 860, val loss: 1.1794726848602295
Epoch 870, training loss: 0.011395984329283237 = 0.004353323020040989 + 0.001 * 7.042661190032959
Epoch 870, val loss: 1.1852458715438843
Epoch 880, training loss: 0.011221623048186302 = 0.0042149838991463184 + 0.001 * 7.006639003753662
Epoch 880, val loss: 1.190887451171875
Epoch 890, training loss: 0.01109677366912365 = 0.004083714447915554 + 0.001 * 7.013058662414551
Epoch 890, val loss: 1.1964075565338135
Epoch 900, training loss: 0.010959934443235397 = 0.003959048073738813 + 0.001 * 7.000885486602783
Epoch 900, val loss: 1.2018362283706665
Epoch 910, training loss: 0.010841291397809982 = 0.0038405447266995907 + 0.001 * 7.000746250152588
Epoch 910, val loss: 1.2071714401245117
Epoch 920, training loss: 0.010729365050792694 = 0.0037277983501553535 + 0.001 * 7.001566410064697
Epoch 920, val loss: 1.2123478651046753
Epoch 930, training loss: 0.01064317487180233 = 0.003620447590947151 + 0.001 * 7.0227274894714355
Epoch 930, val loss: 1.2174800634384155
Epoch 940, training loss: 0.010531910695135593 = 0.003518151817843318 + 0.001 * 7.013758182525635
Epoch 940, val loss: 1.2224801778793335
Epoch 950, training loss: 0.010445005260407925 = 0.003420598106458783 + 0.001 * 7.024406909942627
Epoch 950, val loss: 1.22738516330719
Epoch 960, training loss: 0.010341855697333813 = 0.003327462822198868 + 0.001 * 7.014392375946045
Epoch 960, val loss: 1.2321857213974
Epoch 970, training loss: 0.010259770788252354 = 0.003238482167944312 + 0.001 * 7.02128791809082
Epoch 970, val loss: 1.2369085550308228
Epoch 980, training loss: 0.010160146281123161 = 0.003153382334858179 + 0.001 * 7.006763458251953
Epoch 980, val loss: 1.2415300607681274
Epoch 990, training loss: 0.010057467967271805 = 0.0030719512142241 + 0.001 * 6.98551607131958
Epoch 990, val loss: 1.2461389303207397
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 1.9760868549346924 = 1.9674900770187378 + 0.001 * 8.596816062927246
Epoch 0, val loss: 1.9653958082199097
Epoch 10, training loss: 1.9650763273239136 = 1.956479549407959 + 0.001 * 8.596763610839844
Epoch 10, val loss: 1.955076813697815
Epoch 20, training loss: 1.9520387649536133 = 1.9434422254562378 + 0.001 * 8.59659194946289
Epoch 20, val loss: 1.9426413774490356
Epoch 30, training loss: 1.9344823360443115 = 1.9258861541748047 + 0.001 * 8.596221923828125
Epoch 30, val loss: 1.925719976425171
Epoch 40, training loss: 1.9089726209640503 = 1.9003772735595703 + 0.001 * 8.595390319824219
Epoch 40, val loss: 1.9013739824295044
Epoch 50, training loss: 1.8723645210266113 = 1.8637714385986328 + 0.001 * 8.593062400817871
Epoch 50, val loss: 1.8677374124526978
Epoch 60, training loss: 1.8268532752990723 = 1.81826913356781 + 0.001 * 8.584195137023926
Epoch 60, val loss: 1.8287734985351562
Epoch 70, training loss: 1.7839007377624512 = 1.7753585577011108 + 0.001 * 8.542170524597168
Epoch 70, val loss: 1.7936155796051025
Epoch 80, training loss: 1.7415776252746582 = 1.733291506767273 + 0.001 * 8.286147117614746
Epoch 80, val loss: 1.752580165863037
Epoch 90, training loss: 1.683318853378296 = 1.6752985715866089 + 0.001 * 8.020238876342773
Epoch 90, val loss: 1.6963199377059937
Epoch 100, training loss: 1.6054874658584595 = 1.597523808479309 + 0.001 * 7.963639736175537
Epoch 100, val loss: 1.6287323236465454
Epoch 110, training loss: 1.5147963762283325 = 1.5068511962890625 + 0.001 * 7.945218563079834
Epoch 110, val loss: 1.554377555847168
Epoch 120, training loss: 1.4253900051116943 = 1.4174576997756958 + 0.001 * 7.932356834411621
Epoch 120, val loss: 1.4807578325271606
Epoch 130, training loss: 1.3437894582748413 = 1.3358842134475708 + 0.001 * 7.905264854431152
Epoch 130, val loss: 1.4142987728118896
Epoch 140, training loss: 1.2697100639343262 = 1.261901617050171 + 0.001 * 7.808436393737793
Epoch 140, val loss: 1.3552504777908325
Epoch 150, training loss: 1.2008737325668335 = 1.1932889223098755 + 0.001 * 7.584782123565674
Epoch 150, val loss: 1.3021117448806763
Epoch 160, training loss: 1.1345665454864502 = 1.1270675659179688 + 0.001 * 7.499028205871582
Epoch 160, val loss: 1.2519952058792114
Epoch 170, training loss: 1.0672483444213867 = 1.0598347187042236 + 0.001 * 7.413578987121582
Epoch 170, val loss: 1.2011934518814087
Epoch 180, training loss: 0.9968087077140808 = 0.9894503951072693 + 0.001 * 7.358285903930664
Epoch 180, val loss: 1.147119402885437
Epoch 190, training loss: 0.9233822226524353 = 0.9160504341125488 + 0.001 * 7.331793785095215
Epoch 190, val loss: 1.0902706384658813
Epoch 200, training loss: 0.8483695983886719 = 0.8410476446151733 + 0.001 * 7.321942329406738
Epoch 200, val loss: 1.0323659181594849
Epoch 210, training loss: 0.7734599113464355 = 0.7661470770835876 + 0.001 * 7.312829971313477
Epoch 210, val loss: 0.975432813167572
Epoch 220, training loss: 0.7001101970672607 = 0.6928094625473022 + 0.001 * 7.300723552703857
Epoch 220, val loss: 0.9215878844261169
Epoch 230, training loss: 0.629741907119751 = 0.6224582195281982 + 0.001 * 7.283670425415039
Epoch 230, val loss: 0.8723910450935364
Epoch 240, training loss: 0.5634139776229858 = 0.5561577081680298 + 0.001 * 7.256260395050049
Epoch 240, val loss: 0.8289336562156677
Epoch 250, training loss: 0.5016772150993347 = 0.49446234107017517 + 0.001 * 7.214852333068848
Epoch 250, val loss: 0.7913864254951477
Epoch 260, training loss: 0.44448214769363403 = 0.4372938871383667 + 0.001 * 7.188265800476074
Epoch 260, val loss: 0.7595458626747131
Epoch 270, training loss: 0.3914281129837036 = 0.38426393270492554 + 0.001 * 7.164178371429443
Epoch 270, val loss: 0.7328236699104309
Epoch 280, training loss: 0.3422929048538208 = 0.3351413309574127 + 0.001 * 7.151561737060547
Epoch 280, val loss: 0.7103725671768188
Epoch 290, training loss: 0.29738733172416687 = 0.2902461290359497 + 0.001 * 7.141216278076172
Epoch 290, val loss: 0.6919324994087219
Epoch 300, training loss: 0.2573363482952118 = 0.25020498037338257 + 0.001 * 7.1313676834106445
Epoch 300, val loss: 0.6775274872779846
Epoch 310, training loss: 0.22254294157028198 = 0.21542218327522278 + 0.001 * 7.120756149291992
Epoch 310, val loss: 0.6673728227615356
Epoch 320, training loss: 0.19292975962162018 = 0.18581683933734894 + 0.001 * 7.112924575805664
Epoch 320, val loss: 0.6612414121627808
Epoch 330, training loss: 0.16798746585845947 = 0.1608847826719284 + 0.001 * 7.102675914764404
Epoch 330, val loss: 0.6587072014808655
Epoch 340, training loss: 0.14703236520290375 = 0.13993585109710693 + 0.001 * 7.09650993347168
Epoch 340, val loss: 0.6591453552246094
Epoch 350, training loss: 0.12937888503074646 = 0.12228450179100037 + 0.001 * 7.09437894821167
Epoch 350, val loss: 0.6618756651878357
Epoch 360, training loss: 0.1144304871559143 = 0.10733625292778015 + 0.001 * 7.094231605529785
Epoch 360, val loss: 0.6664231419563293
Epoch 370, training loss: 0.10169731825590134 = 0.09460245072841644 + 0.001 * 7.094864368438721
Epoch 370, val loss: 0.6722962856292725
Epoch 380, training loss: 0.09079428017139435 = 0.08369848132133484 + 0.001 * 7.095798015594482
Epoch 380, val loss: 0.6791872978210449
Epoch 390, training loss: 0.08140555024147034 = 0.07430871576070786 + 0.001 * 7.096837520599365
Epoch 390, val loss: 0.6869462728500366
Epoch 400, training loss: 0.07328517735004425 = 0.0661875531077385 + 0.001 * 7.0976243019104
Epoch 400, val loss: 0.6953274607658386
Epoch 410, training loss: 0.06623876094818115 = 0.059139788150787354 + 0.001 * 7.098974704742432
Epoch 410, val loss: 0.704204261302948
Epoch 420, training loss: 0.060108691453933716 = 0.05300937965512276 + 0.001 * 7.099310874938965
Epoch 420, val loss: 0.7134738564491272
Epoch 430, training loss: 0.05476713180541992 = 0.047667182981967926 + 0.001 * 7.099948406219482
Epoch 430, val loss: 0.722984790802002
Epoch 440, training loss: 0.050102606415748596 = 0.0430019237101078 + 0.001 * 7.100682258605957
Epoch 440, val loss: 0.7326453924179077
Epoch 450, training loss: 0.046021267771720886 = 0.03892011567950249 + 0.001 * 7.1011528968811035
Epoch 450, val loss: 0.7423659563064575
Epoch 460, training loss: 0.04244400933384895 = 0.03534116595983505 + 0.001 * 7.102842330932617
Epoch 460, val loss: 0.7520321011543274
Epoch 470, training loss: 0.03929769620299339 = 0.03219512477517128 + 0.001 * 7.102570533752441
Epoch 470, val loss: 0.7616029977798462
Epoch 480, training loss: 0.036525290459394455 = 0.029423117637634277 + 0.001 * 7.102173805236816
Epoch 480, val loss: 0.7710648775100708
Epoch 490, training loss: 0.034073054790496826 = 0.026970701292157173 + 0.001 * 7.102352142333984
Epoch 490, val loss: 0.7803540229797363
Epoch 500, training loss: 0.03189130499958992 = 0.02478860318660736 + 0.001 * 7.102701187133789
Epoch 500, val loss: 0.7894711494445801
Epoch 510, training loss: 0.029941005632281303 = 0.022837482392787933 + 0.001 * 7.103522777557373
Epoch 510, val loss: 0.7983964681625366
Epoch 520, training loss: 0.028191544115543365 = 0.021088672801852226 + 0.001 * 7.102869987487793
Epoch 520, val loss: 0.8071190118789673
Epoch 530, training loss: 0.02662145532667637 = 0.01951906457543373 + 0.001 * 7.102390289306641
Epoch 530, val loss: 0.8156440258026123
Epoch 540, training loss: 0.0252105463296175 = 0.018108515068888664 + 0.001 * 7.102031230926514
Epoch 540, val loss: 0.8239550590515137
Epoch 550, training loss: 0.02394021302461624 = 0.016838673502206802 + 0.001 * 7.101539134979248
Epoch 550, val loss: 0.8320438861846924
Epoch 560, training loss: 0.022794604301452637 = 0.015693338587880135 + 0.001 * 7.1012654304504395
Epoch 560, val loss: 0.839911937713623
Epoch 570, training loss: 0.021760789677500725 = 0.014658504165709019 + 0.001 * 7.102284908294678
Epoch 570, val loss: 0.8475788235664368
Epoch 580, training loss: 0.020821336656808853 = 0.01372139248996973 + 0.001 * 7.099944114685059
Epoch 580, val loss: 0.8550150990486145
Epoch 590, training loss: 0.01997067593038082 = 0.012870840728282928 + 0.001 * 7.09983491897583
Epoch 590, val loss: 0.862240195274353
Epoch 600, training loss: 0.019196195527911186 = 0.012097052298486233 + 0.001 * 7.0991435050964355
Epoch 600, val loss: 0.8692533373832703
Epoch 610, training loss: 0.018489113077521324 = 0.011391478590667248 + 0.001 * 7.0976338386535645
Epoch 610, val loss: 0.8760566115379333
Epoch 620, training loss: 0.01784541644155979 = 0.010746719315648079 + 0.001 * 7.098696708679199
Epoch 620, val loss: 0.8826602101325989
Epoch 630, training loss: 0.017253929749131203 = 0.010156184434890747 + 0.001 * 7.097745418548584
Epoch 630, val loss: 0.8890696167945862
Epoch 640, training loss: 0.016712874174118042 = 0.009614247828722 + 0.001 * 7.098626136779785
Epoch 640, val loss: 0.8952911496162415
Epoch 650, training loss: 0.01621064357459545 = 0.00911585334688425 + 0.001 * 7.094789981842041
Epoch 650, val loss: 0.9013336300849915
Epoch 660, training loss: 0.01574937254190445 = 0.008656427264213562 + 0.001 * 7.092945098876953
Epoch 660, val loss: 0.9072183966636658
Epoch 670, training loss: 0.015326999127864838 = 0.00823196116834879 + 0.001 * 7.095037937164307
Epoch 670, val loss: 0.9129460453987122
Epoch 680, training loss: 0.014928879216313362 = 0.007838835008442402 + 0.001 * 7.090044021606445
Epoch 680, val loss: 0.9185293912887573
Epoch 690, training loss: 0.014563442207872868 = 0.007473507430404425 + 0.001 * 7.089934349060059
Epoch 690, val loss: 0.9239963889122009
Epoch 700, training loss: 0.014219535514712334 = 0.007132838945835829 + 0.001 * 7.086696147918701
Epoch 700, val loss: 0.9293649196624756
Epoch 710, training loss: 0.01390167884528637 = 0.006814152002334595 + 0.001 * 7.087526798248291
Epoch 710, val loss: 0.9346495866775513
Epoch 720, training loss: 0.013599108904600143 = 0.006515299901366234 + 0.001 * 7.083808422088623
Epoch 720, val loss: 0.939843475818634
Epoch 730, training loss: 0.013317881152033806 = 0.006234663538634777 + 0.001 * 7.083216667175293
Epoch 730, val loss: 0.9449643492698669
Epoch 740, training loss: 0.013079438358545303 = 0.0059708599001169205 + 0.001 * 7.108578681945801
Epoch 740, val loss: 0.9499934315681458
Epoch 750, training loss: 0.012815294787287712 = 0.005722784902900457 + 0.001 * 7.0925092697143555
Epoch 750, val loss: 0.9549293518066406
Epoch 760, training loss: 0.01256987452507019 = 0.005489397794008255 + 0.001 * 7.0804762840271
Epoch 760, val loss: 0.959754228591919
Epoch 770, training loss: 0.012346094474196434 = 0.005269782152026892 + 0.001 * 7.07631254196167
Epoch 770, val loss: 0.96450275182724
Epoch 780, training loss: 0.012135008350014687 = 0.005063062999397516 + 0.001 * 7.0719451904296875
Epoch 780, val loss: 0.9691509008407593
Epoch 790, training loss: 0.012013085186481476 = 0.0048684291541576385 + 0.001 * 7.144655704498291
Epoch 790, val loss: 0.9737052917480469
Epoch 800, training loss: 0.011776353232562542 = 0.0046851313672959805 + 0.001 * 7.091221332550049
Epoch 800, val loss: 0.978141188621521
Epoch 810, training loss: 0.011585783213376999 = 0.00451238825917244 + 0.001 * 7.073394775390625
Epoch 810, val loss: 0.9824999570846558
Epoch 820, training loss: 0.011429870501160622 = 0.004349489230662584 + 0.001 * 7.080380916595459
Epoch 820, val loss: 0.9867620468139648
Epoch 830, training loss: 0.0112709179520607 = 0.004195764195173979 + 0.001 * 7.075153350830078
Epoch 830, val loss: 0.9909162521362305
Epoch 840, training loss: 0.011136587709188461 = 0.004050601273775101 + 0.001 * 7.0859856605529785
Epoch 840, val loss: 0.9949816465377808
Epoch 850, training loss: 0.010965296998620033 = 0.003913447726517916 + 0.001 * 7.051848411560059
Epoch 850, val loss: 0.9989626407623291
Epoch 860, training loss: 0.01083650253713131 = 0.003783741733059287 + 0.001 * 7.052760124206543
Epoch 860, val loss: 1.002853274345398
Epoch 870, training loss: 0.010716100223362446 = 0.0036609831731766462 + 0.001 * 7.055117130279541
Epoch 870, val loss: 1.006652593612671
Epoch 880, training loss: 0.010596895590424538 = 0.003544706152752042 + 0.001 * 7.052188873291016
Epoch 880, val loss: 1.0103744268417358
Epoch 890, training loss: 0.01049733441323042 = 0.0034344804007560015 + 0.001 * 7.0628533363342285
Epoch 890, val loss: 1.0140131711959839
Epoch 900, training loss: 0.010384298861026764 = 0.0033299385104328394 + 0.001 * 7.0543599128723145
Epoch 900, val loss: 1.0175780057907104
Epoch 910, training loss: 0.010282986797392368 = 0.0032306585926562548 + 0.001 * 7.052327632904053
Epoch 910, val loss: 1.0210626125335693
Epoch 920, training loss: 0.01017850637435913 = 0.003136351238936186 + 0.001 * 7.0421552658081055
Epoch 920, val loss: 1.0244783163070679
Epoch 930, training loss: 0.010127317160367966 = 0.003046667668968439 + 0.001 * 7.080649375915527
Epoch 930, val loss: 1.0278127193450928
Epoch 940, training loss: 0.01000926923006773 = 0.0029613589867949486 + 0.001 * 7.047909736633301
Epoch 940, val loss: 1.0310832262039185
Epoch 950, training loss: 0.00991313997656107 = 0.0028801183216273785 + 0.001 * 7.033021450042725
Epoch 950, val loss: 1.0342832803726196
Epoch 960, training loss: 0.009865891188383102 = 0.002802681177854538 + 0.001 * 7.063209533691406
Epoch 960, val loss: 1.0374000072479248
Epoch 970, training loss: 0.009775379672646523 = 0.0027288771234452724 + 0.001 * 7.046502590179443
Epoch 970, val loss: 1.0404683351516724
Epoch 980, training loss: 0.009714562445878983 = 0.002658441197127104 + 0.001 * 7.056121349334717
Epoch 980, val loss: 1.0434739589691162
Epoch 990, training loss: 0.009619764983654022 = 0.002591177821159363 + 0.001 * 7.0285868644714355
Epoch 990, val loss: 1.0464003086090088
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 1.9720733165740967 = 1.9634764194488525 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.9544705152511597
Epoch 10, training loss: 1.961389183998108 = 1.9527924060821533 + 0.001 * 8.596803665161133
Epoch 10, val loss: 1.9440642595291138
Epoch 20, training loss: 1.9486366510391235 = 1.9400399923324585 + 0.001 * 8.596660614013672
Epoch 20, val loss: 1.9311293363571167
Epoch 30, training loss: 1.9312727451324463 = 1.92267644405365 + 0.001 * 8.59634017944336
Epoch 30, val loss: 1.9129356145858765
Epoch 40, training loss: 1.9058171510696411 = 1.897221565246582 + 0.001 * 8.595596313476562
Epoch 40, val loss: 1.8859447240829468
Epoch 50, training loss: 1.8688578605651855 = 1.8602641820907593 + 0.001 * 8.593646049499512
Epoch 50, val loss: 1.847855806350708
Epoch 60, training loss: 1.8226522207260132 = 1.8140649795532227 + 0.001 * 8.58726978302002
Epoch 60, val loss: 1.804550290107727
Epoch 70, training loss: 1.7798171043395996 = 1.7712581157684326 + 0.001 * 8.558928489685059
Epoch 70, val loss: 1.7719253301620483
Epoch 80, training loss: 1.7356271743774414 = 1.7272725105285645 + 0.001 * 8.354629516601562
Epoch 80, val loss: 1.7390260696411133
Epoch 90, training loss: 1.6744539737701416 = 1.6665040254592896 + 0.001 * 7.949916839599609
Epoch 90, val loss: 1.6868187189102173
Epoch 100, training loss: 1.593035340309143 = 1.5853097438812256 + 0.001 * 7.725594520568848
Epoch 100, val loss: 1.6136481761932373
Epoch 110, training loss: 1.4934567213058472 = 1.4858229160308838 + 0.001 * 7.633799076080322
Epoch 110, val loss: 1.5290765762329102
Epoch 120, training loss: 1.3837544918060303 = 1.3761457204818726 + 0.001 * 7.608816623687744
Epoch 120, val loss: 1.4391484260559082
Epoch 130, training loss: 1.2691599130630493 = 1.261551022529602 + 0.001 * 7.608845233917236
Epoch 130, val loss: 1.347632646560669
Epoch 140, training loss: 1.1523196697235107 = 1.1447174549102783 + 0.001 * 7.6021623611450195
Epoch 140, val loss: 1.2553640604019165
Epoch 150, training loss: 1.0378968715667725 = 1.0303007364273071 + 0.001 * 7.596088886260986
Epoch 150, val loss: 1.166803002357483
Epoch 160, training loss: 0.9322784543037415 = 0.9246901869773865 + 0.001 * 7.588272571563721
Epoch 160, val loss: 1.0864627361297607
Epoch 170, training loss: 0.8403778672218323 = 0.8328019380569458 + 0.001 * 7.575901031494141
Epoch 170, val loss: 1.020316481590271
Epoch 180, training loss: 0.7634266018867493 = 0.7558711171150208 + 0.001 * 7.555505275726318
Epoch 180, val loss: 0.9684332013130188
Epoch 190, training loss: 0.6988879442214966 = 0.6913665533065796 + 0.001 * 7.521389007568359
Epoch 190, val loss: 0.9282355308532715
Epoch 200, training loss: 0.6427085399627686 = 0.6352410316467285 + 0.001 * 7.467507362365723
Epoch 200, val loss: 0.8961279392242432
Epoch 210, training loss: 0.5912872552871704 = 0.5838801264762878 + 0.001 * 7.4071269035339355
Epoch 210, val loss: 0.8690489530563354
Epoch 220, training loss: 0.5423551201820374 = 0.5349730253219604 + 0.001 * 7.382105350494385
Epoch 220, val loss: 0.8455399870872498
Epoch 230, training loss: 0.49495670199394226 = 0.4875998795032501 + 0.001 * 7.356822967529297
Epoch 230, val loss: 0.8251093029975891
Epoch 240, training loss: 0.449173241853714 = 0.44182899594306946 + 0.001 * 7.344245910644531
Epoch 240, val loss: 0.8080684542655945
Epoch 250, training loss: 0.405432790517807 = 0.3980989158153534 + 0.001 * 7.333866119384766
Epoch 250, val loss: 0.7947776913642883
Epoch 260, training loss: 0.36421361565589905 = 0.3568893373012543 + 0.001 * 7.324289321899414
Epoch 260, val loss: 0.7855437397956848
Epoch 270, training loss: 0.32580581307411194 = 0.3184893727302551 + 0.001 * 7.31645393371582
Epoch 270, val loss: 0.7805795073509216
Epoch 280, training loss: 0.29032590985298157 = 0.2830201983451843 + 0.001 * 7.305716514587402
Epoch 280, val loss: 0.7796635627746582
Epoch 290, training loss: 0.2578963339328766 = 0.25059089064598083 + 0.001 * 7.305450439453125
Epoch 290, val loss: 0.7824878692626953
Epoch 300, training loss: 0.22860212624073029 = 0.22131159901618958 + 0.001 * 7.290529251098633
Epoch 300, val loss: 0.7887747287750244
Epoch 310, training loss: 0.20251144468784332 = 0.19522595405578613 + 0.001 * 7.285486221313477
Epoch 310, val loss: 0.7982982993125916
Epoch 320, training loss: 0.17954331636428833 = 0.1722603291273117 + 0.001 * 7.282992362976074
Epoch 320, val loss: 0.8108874559402466
Epoch 330, training loss: 0.15947432816028595 = 0.15220266580581665 + 0.001 * 7.271661758422852
Epoch 330, val loss: 0.8261748552322388
Epoch 340, training loss: 0.14203187823295593 = 0.13476069271564484 + 0.001 * 7.271190166473389
Epoch 340, val loss: 0.8437475562095642
Epoch 350, training loss: 0.12686701118946075 = 0.11960268765687943 + 0.001 * 7.264317989349365
Epoch 350, val loss: 0.8631291389465332
Epoch 360, training loss: 0.11366681754589081 = 0.10640975087881088 + 0.001 * 7.257065773010254
Epoch 360, val loss: 0.8838000297546387
Epoch 370, training loss: 0.10220108926296234 = 0.09490233659744263 + 0.001 * 7.298752307891846
Epoch 370, val loss: 0.9053049683570862
Epoch 380, training loss: 0.09210298955440521 = 0.08483921736478806 + 0.001 * 7.26377534866333
Epoch 380, val loss: 0.9273139238357544
Epoch 390, training loss: 0.08326517790555954 = 0.0760175958275795 + 0.001 * 7.2475810050964355
Epoch 390, val loss: 0.9494900107383728
Epoch 400, training loss: 0.07551008462905884 = 0.06826860457658768 + 0.001 * 7.241476058959961
Epoch 400, val loss: 0.9716378450393677
Epoch 410, training loss: 0.06868671625852585 = 0.06145109236240387 + 0.001 * 7.235622406005859
Epoch 410, val loss: 0.9935856461524963
Epoch 420, training loss: 0.06267864257097244 = 0.055444661527872086 + 0.001 * 7.233980178833008
Epoch 420, val loss: 1.0151845216751099
Epoch 430, training loss: 0.05737680196762085 = 0.050147153437137604 + 0.001 * 7.229649066925049
Epoch 430, val loss: 1.0363494157791138
Epoch 440, training loss: 0.05269530043005943 = 0.045469049364328384 + 0.001 * 7.2262492179870605
Epoch 440, val loss: 1.0570026636123657
Epoch 450, training loss: 0.04854848235845566 = 0.041332684457302094 + 0.001 * 7.215796947479248
Epoch 450, val loss: 1.0771023035049438
Epoch 460, training loss: 0.044884540140628815 = 0.03767057880759239 + 0.001 * 7.213962078094482
Epoch 460, val loss: 1.0966390371322632
Epoch 470, training loss: 0.041652239859104156 = 0.034423284232616425 + 0.001 * 7.228955268859863
Epoch 470, val loss: 1.115616798400879
Epoch 480, training loss: 0.03874296694993973 = 0.031538594514131546 + 0.001 * 7.204372406005859
Epoch 480, val loss: 1.1339975595474243
Epoch 490, training loss: 0.036173783242702484 = 0.028971491381525993 + 0.001 * 7.202292442321777
Epoch 490, val loss: 1.151808500289917
Epoch 500, training loss: 0.033877987414598465 = 0.026682302355766296 + 0.001 * 7.195684432983398
Epoch 500, val loss: 1.1690552234649658
Epoch 510, training loss: 0.03187340125441551 = 0.02463673986494541 + 0.001 * 7.236659526824951
Epoch 510, val loss: 1.185752272605896
Epoch 520, training loss: 0.030001727864146233 = 0.02280508354306221 + 0.001 * 7.196643829345703
Epoch 520, val loss: 1.2019115686416626
Epoch 530, training loss: 0.028343239799141884 = 0.021161336451768875 + 0.001 * 7.1819024085998535
Epoch 530, val loss: 1.217552661895752
Epoch 540, training loss: 0.026858478784561157 = 0.01968279667198658 + 0.001 * 7.1756815910339355
Epoch 540, val loss: 1.23268723487854
Epoch 550, training loss: 0.02552325278520584 = 0.018349576741456985 + 0.001 * 7.173676490783691
Epoch 550, val loss: 1.2473335266113281
Epoch 560, training loss: 0.024315662682056427 = 0.017144570127129555 + 0.001 * 7.171091079711914
Epoch 560, val loss: 1.2615219354629517
Epoch 570, training loss: 0.023232005536556244 = 0.01605294458568096 + 0.001 * 7.179061412811279
Epoch 570, val loss: 1.2752594947814941
Epoch 580, training loss: 0.022230694070458412 = 0.015061763115227222 + 0.001 * 7.1689300537109375
Epoch 580, val loss: 1.2885644435882568
Epoch 590, training loss: 0.02132200077176094 = 0.014159642159938812 + 0.001 * 7.162358283996582
Epoch 590, val loss: 1.301438808441162
Epoch 600, training loss: 0.020494822412729263 = 0.013336677104234695 + 0.001 * 7.158143997192383
Epoch 600, val loss: 1.3139201402664185
Epoch 610, training loss: 0.0197460874915123 = 0.012584317475557327 + 0.001 * 7.161770343780518
Epoch 610, val loss: 1.3260149955749512
Epoch 620, training loss: 0.01903632842004299 = 0.011895013973116875 + 0.001 * 7.141313552856445
Epoch 620, val loss: 1.337732195854187
Epoch 630, training loss: 0.018462933599948883 = 0.01126221101731062 + 0.001 * 7.200723171234131
Epoch 630, val loss: 1.349087119102478
Epoch 640, training loss: 0.017818758264183998 = 0.010680046863853931 + 0.001 * 7.1387104988098145
Epoch 640, val loss: 1.3600986003875732
Epoch 650, training loss: 0.01728028804063797 = 0.010143375024199486 + 0.001 * 7.136912822723389
Epoch 650, val loss: 1.3707770109176636
Epoch 660, training loss: 0.016780607402324677 = 0.009647713974118233 + 0.001 * 7.132893085479736
Epoch 660, val loss: 1.3811448812484741
Epoch 670, training loss: 0.016331752762198448 = 0.009189068339765072 + 0.001 * 7.142683506011963
Epoch 670, val loss: 1.3912078142166138
Epoch 680, training loss: 0.015909593552350998 = 0.008763915859162807 + 0.001 * 7.1456780433654785
Epoch 680, val loss: 1.4009737968444824
Epoch 690, training loss: 0.015484973788261414 = 0.00836914498358965 + 0.001 * 7.115828514099121
Epoch 690, val loss: 1.4104691743850708
Epoch 700, training loss: 0.015127763152122498 = 0.008002001792192459 + 0.001 * 7.12576150894165
Epoch 700, val loss: 1.419687271118164
Epoch 710, training loss: 0.014791743829846382 = 0.007659931667149067 + 0.001 * 7.131811141967773
Epoch 710, val loss: 1.4286471605300903
Epoch 720, training loss: 0.014456136152148247 = 0.007340786512941122 + 0.001 * 7.115349769592285
Epoch 720, val loss: 1.4373658895492554
Epoch 730, training loss: 0.014151003211736679 = 0.007042563054710627 + 0.001 * 7.108439922332764
Epoch 730, val loss: 1.4458452463150024
Epoch 740, training loss: 0.01388697698712349 = 0.006763487122952938 + 0.001 * 7.1234893798828125
Epoch 740, val loss: 1.4540939331054688
Epoch 750, training loss: 0.013600016012787819 = 0.006501979194581509 + 0.001 * 7.098036766052246
Epoch 750, val loss: 1.462117314338684
Epoch 760, training loss: 0.013351310975849628 = 0.006256617605686188 + 0.001 * 7.094693183898926
Epoch 760, val loss: 1.4699350595474243
Epoch 770, training loss: 0.013114040717482567 = 0.0060260663740336895 + 0.001 * 7.0879740715026855
Epoch 770, val loss: 1.4775428771972656
Epoch 780, training loss: 0.012900080531835556 = 0.0058091483078897 + 0.001 * 7.0909318923950195
Epoch 780, val loss: 1.4849570989608765
Epoch 790, training loss: 0.012710977345705032 = 0.005604511126875877 + 0.001 * 7.1064653396606445
Epoch 790, val loss: 1.4922090768814087
Epoch 800, training loss: 0.012507534585893154 = 0.005410444922745228 + 0.001 * 7.0970892906188965
Epoch 800, val loss: 1.4993534088134766
Epoch 810, training loss: 0.0123205054551363 = 0.00522471172735095 + 0.001 * 7.0957932472229
Epoch 810, val loss: 1.5065042972564697
Epoch 820, training loss: 0.012133965268731117 = 0.0050463308580219746 + 0.001 * 7.0876336097717285
Epoch 820, val loss: 1.5137057304382324
Epoch 830, training loss: 0.01198558323085308 = 0.004874399397522211 + 0.001 * 7.1111836433410645
Epoch 830, val loss: 1.5209956169128418
Epoch 840, training loss: 0.011778373271226883 = 0.00470888614654541 + 0.001 * 7.069486618041992
Epoch 840, val loss: 1.5283477306365967
Epoch 850, training loss: 0.011636579409241676 = 0.004549688193947077 + 0.001 * 7.086890697479248
Epoch 850, val loss: 1.5357435941696167
Epoch 860, training loss: 0.011513005010783672 = 0.004397064913064241 + 0.001 * 7.115939617156982
Epoch 860, val loss: 1.543144941329956
Epoch 870, training loss: 0.011313581839203835 = 0.004251076839864254 + 0.001 * 7.062504291534424
Epoch 870, val loss: 1.5505309104919434
Epoch 880, training loss: 0.011182388290762901 = 0.0041115907952189445 + 0.001 * 7.070796966552734
Epoch 880, val loss: 1.5578627586364746
Epoch 890, training loss: 0.011061991564929485 = 0.003978548105806112 + 0.001 * 7.0834431648254395
Epoch 890, val loss: 1.5651209354400635
Epoch 900, training loss: 0.010962958447635174 = 0.0038517434149980545 + 0.001 * 7.111214637756348
Epoch 900, val loss: 1.5722860097885132
Epoch 910, training loss: 0.010808796621859074 = 0.003731057746335864 + 0.001 * 7.077738285064697
Epoch 910, val loss: 1.5793424844741821
Epoch 920, training loss: 0.010671279393136501 = 0.003616080852225423 + 0.001 * 7.0551981925964355
Epoch 920, val loss: 1.5863009691238403
Epoch 930, training loss: 0.010567554272711277 = 0.0035065144766122103 + 0.001 * 7.061039447784424
Epoch 930, val loss: 1.5931442975997925
Epoch 940, training loss: 0.010449251160025597 = 0.0034021346364170313 + 0.001 * 7.047116279602051
Epoch 940, val loss: 1.5998735427856445
Epoch 950, training loss: 0.010334566235542297 = 0.003302624449133873 + 0.001 * 7.0319414138793945
Epoch 950, val loss: 1.606480598449707
Epoch 960, training loss: 0.010273260995745659 = 0.0032076940406113863 + 0.001 * 7.065566539764404
Epoch 960, val loss: 1.612975001335144
Epoch 970, training loss: 0.010179225355386734 = 0.003117029555141926 + 0.001 * 7.062195301055908
Epoch 970, val loss: 1.6193702220916748
Epoch 980, training loss: 0.01007883157581091 = 0.0030303741805255413 + 0.001 * 7.048457145690918
Epoch 980, val loss: 1.6256550550460815
Epoch 990, training loss: 0.009972035884857178 = 0.002947381464764476 + 0.001 * 7.024653911590576
Epoch 990, val loss: 1.6318414211273193
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8392198207696363
The final CL Acc:0.81235, 0.01772, The final GNN Acc:0.83922, 0.00000
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9546])
updated graph: torch.Size([2, 10588])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9537913799285889 = 1.9451944828033447 + 0.001 * 8.596846580505371
Epoch 0, val loss: 1.9443868398666382
Epoch 10, training loss: 1.9441208839416504 = 1.9355241060256958 + 0.001 * 8.596784591674805
Epoch 10, val loss: 1.9351223707199097
Epoch 20, training loss: 1.9322782754898071 = 1.9236817359924316 + 0.001 * 8.596589088439941
Epoch 20, val loss: 1.9233696460723877
Epoch 30, training loss: 1.915858507156372 = 1.9072624444961548 + 0.001 * 8.596071243286133
Epoch 30, val loss: 1.9067612886428833
Epoch 40, training loss: 1.891927719116211 = 1.8833330869674683 + 0.001 * 8.594611167907715
Epoch 40, val loss: 1.8827307224273682
Epoch 50, training loss: 1.858733892440796 = 1.8501440286636353 + 0.001 * 8.589874267578125
Epoch 50, val loss: 1.8508422374725342
Epoch 60, training loss: 1.82161283493042 = 1.8130419254302979 + 0.001 * 8.570858001708984
Epoch 60, val loss: 1.818701982498169
Epoch 70, training loss: 1.7913755178451538 = 1.782907485961914 + 0.001 * 8.468055725097656
Epoch 70, val loss: 1.7943148612976074
Epoch 80, training loss: 1.755180835723877 = 1.7469502687454224 + 0.001 * 8.23060131072998
Epoch 80, val loss: 1.7617319822311401
Epoch 90, training loss: 1.703228235244751 = 1.6951377391815186 + 0.001 * 8.09047794342041
Epoch 90, val loss: 1.7172917127609253
Epoch 100, training loss: 1.6298182010650635 = 1.6219323873519897 + 0.001 * 7.885773658752441
Epoch 100, val loss: 1.656706690788269
Epoch 110, training loss: 1.5388314723968506 = 1.5311332941055298 + 0.001 * 7.698178768157959
Epoch 110, val loss: 1.582095980644226
Epoch 120, training loss: 1.44315505027771 = 1.4354822635650635 + 0.001 * 7.672756195068359
Epoch 120, val loss: 1.5045117139816284
Epoch 130, training loss: 1.348910927772522 = 1.341271996498108 + 0.001 * 7.638877868652344
Epoch 130, val loss: 1.4293444156646729
Epoch 140, training loss: 1.2549598217010498 = 1.2473686933517456 + 0.001 * 7.591089725494385
Epoch 140, val loss: 1.3558284044265747
Epoch 150, training loss: 1.1610119342803955 = 1.153467059135437 + 0.001 * 7.544858932495117
Epoch 150, val loss: 1.2833162546157837
Epoch 160, training loss: 1.0694007873535156 = 1.0618958473205566 + 0.001 * 7.504938125610352
Epoch 160, val loss: 1.2139681577682495
Epoch 170, training loss: 0.9829243421554565 = 0.9754636287689209 + 0.001 * 7.4607038497924805
Epoch 170, val loss: 1.150496244430542
Epoch 180, training loss: 0.9022834300994873 = 0.8948433995246887 + 0.001 * 7.440048694610596
Epoch 180, val loss: 1.092977523803711
Epoch 190, training loss: 0.826836884021759 = 0.8193985223770142 + 0.001 * 7.438360214233398
Epoch 190, val loss: 1.0404853820800781
Epoch 200, training loss: 0.7564577460289001 = 0.7490232586860657 + 0.001 * 7.434503078460693
Epoch 200, val loss: 0.9930853843688965
Epoch 210, training loss: 0.6918720602989197 = 0.6844431757926941 + 0.001 * 7.428856372833252
Epoch 210, val loss: 0.9516164064407349
Epoch 220, training loss: 0.6329153180122375 = 0.6254913210868835 + 0.001 * 7.4239983558654785
Epoch 220, val loss: 0.9166609644889832
Epoch 230, training loss: 0.578238308429718 = 0.5708190202713013 + 0.001 * 7.41926908493042
Epoch 230, val loss: 0.8879535794258118
Epoch 240, training loss: 0.5266462564468384 = 0.5192320942878723 + 0.001 * 7.4141693115234375
Epoch 240, val loss: 0.8649693131446838
Epoch 250, training loss: 0.47784510254859924 = 0.4704361855983734 + 0.001 * 7.408924102783203
Epoch 250, val loss: 0.8471646904945374
Epoch 260, training loss: 0.43213340640068054 = 0.4247302711009979 + 0.001 * 7.403142929077148
Epoch 260, val loss: 0.8339974880218506
Epoch 270, training loss: 0.3898412883281708 = 0.38244590163230896 + 0.001 * 7.395384311676025
Epoch 270, val loss: 0.8252806067466736
Epoch 280, training loss: 0.3511161208152771 = 0.34373095631599426 + 0.001 * 7.38515043258667
Epoch 280, val loss: 0.8206955790519714
Epoch 290, training loss: 0.315708190202713 = 0.30833715200424194 + 0.001 * 7.371048927307129
Epoch 290, val loss: 0.8192785978317261
Epoch 300, training loss: 0.2831573784351349 = 0.27580201625823975 + 0.001 * 7.355368137359619
Epoch 300, val loss: 0.8198484778404236
Epoch 310, training loss: 0.2529223561286926 = 0.24558521807193756 + 0.001 * 7.337139129638672
Epoch 310, val loss: 0.8214215040206909
Epoch 320, training loss: 0.2245553880929947 = 0.21723859012126923 + 0.001 * 7.316790580749512
Epoch 320, val loss: 0.8234850764274597
Epoch 330, training loss: 0.19785168766975403 = 0.19053898751735687 + 0.001 * 7.312692642211914
Epoch 330, val loss: 0.8258091807365417
Epoch 340, training loss: 0.1729486584663391 = 0.16564758121967316 + 0.001 * 7.301072597503662
Epoch 340, val loss: 0.8286162614822388
Epoch 350, training loss: 0.15020158886909485 = 0.14293578267097473 + 0.001 * 7.265812397003174
Epoch 350, val loss: 0.8321926593780518
Epoch 360, training loss: 0.13004355132579803 = 0.1227492019534111 + 0.001 * 7.294346332550049
Epoch 360, val loss: 0.8369708061218262
Epoch 370, training loss: 0.11246545612812042 = 0.10519932955503464 + 0.001 * 7.26612663269043
Epoch 370, val loss: 0.8432727456092834
Epoch 380, training loss: 0.0974326878786087 = 0.0901772752404213 + 0.001 * 7.25541353225708
Epoch 380, val loss: 0.8509572744369507
Epoch 390, training loss: 0.0847078338265419 = 0.07746011763811111 + 0.001 * 7.247716903686523
Epoch 390, val loss: 0.8598708510398865
Epoch 400, training loss: 0.07401596009731293 = 0.06677243858575821 + 0.001 * 7.243520736694336
Epoch 400, val loss: 0.8696779012680054
Epoch 410, training loss: 0.0650850385427475 = 0.05782974511384964 + 0.001 * 7.2552900314331055
Epoch 410, val loss: 0.8800767660140991
Epoch 420, training loss: 0.05760040134191513 = 0.05035934969782829 + 0.001 * 7.241052627563477
Epoch 420, val loss: 0.8908522129058838
Epoch 430, training loss: 0.05135726183652878 = 0.04411355406045914 + 0.001 * 7.243705749511719
Epoch 430, val loss: 0.901757001876831
Epoch 440, training loss: 0.04612297937273979 = 0.03887428715825081 + 0.001 * 7.248691558837891
Epoch 440, val loss: 0.9126695394515991
Epoch 450, training loss: 0.04170122742652893 = 0.03446037322282791 + 0.001 * 7.2408528327941895
Epoch 450, val loss: 0.9235409498214722
Epoch 460, training loss: 0.03795405477285385 = 0.03072347305715084 + 0.001 * 7.2305827140808105
Epoch 460, val loss: 0.9342462420463562
Epoch 470, training loss: 0.034792460501194 = 0.02754364348948002 + 0.001 * 7.248816967010498
Epoch 470, val loss: 0.9446956515312195
Epoch 480, training loss: 0.032059408724308014 = 0.024823257699608803 + 0.001 * 7.236151218414307
Epoch 480, val loss: 0.9548421502113342
Epoch 490, training loss: 0.02970896102488041 = 0.022482920438051224 + 0.001 * 7.226040363311768
Epoch 490, val loss: 0.9647185206413269
Epoch 500, training loss: 0.027682283893227577 = 0.020458726212382317 + 0.001 * 7.223557472229004
Epoch 500, val loss: 0.9742608070373535
Epoch 510, training loss: 0.02592078596353531 = 0.018698884174227715 + 0.001 * 7.221900463104248
Epoch 510, val loss: 0.9835004806518555
Epoch 520, training loss: 0.024380456656217575 = 0.017160793766379356 + 0.001 * 7.219661712646484
Epoch 520, val loss: 0.9924211502075195
Epoch 530, training loss: 0.023032313212752342 = 0.015809793025255203 + 0.001 * 7.222519874572754
Epoch 530, val loss: 1.0010428428649902
Epoch 540, training loss: 0.02184334583580494 = 0.014616807922720909 + 0.001 * 7.226537227630615
Epoch 540, val loss: 1.00938880443573
Epoch 550, training loss: 0.020765073597431183 = 0.013557185418903828 + 0.001 * 7.207888126373291
Epoch 550, val loss: 1.0174659490585327
Epoch 560, training loss: 0.019825849682092667 = 0.012609604746103287 + 0.001 * 7.216243743896484
Epoch 560, val loss: 1.0252788066864014
Epoch 570, training loss: 0.01897210255265236 = 0.011756700463593006 + 0.001 * 7.215401649475098
Epoch 570, val loss: 1.0328401327133179
Epoch 580, training loss: 0.01818820834159851 = 0.010985421016812325 + 0.001 * 7.202785968780518
Epoch 580, val loss: 1.0401649475097656
Epoch 590, training loss: 0.017531519755721092 = 0.010285814292728901 + 0.001 * 7.245705604553223
Epoch 590, val loss: 1.0472278594970703
Epoch 600, training loss: 0.016856329515576363 = 0.009650129824876785 + 0.001 * 7.206198692321777
Epoch 600, val loss: 1.0540440082550049
Epoch 610, training loss: 0.016291148960590363 = 0.009071425534784794 + 0.001 * 7.219722270965576
Epoch 610, val loss: 1.0606515407562256
Epoch 620, training loss: 0.015731487423181534 = 0.008543802425265312 + 0.001 * 7.187683582305908
Epoch 620, val loss: 1.0670549869537354
Epoch 630, training loss: 0.015249454416334629 = 0.008061897940933704 + 0.001 * 7.187556266784668
Epoch 630, val loss: 1.0732395648956299
Epoch 640, training loss: 0.014800507575273514 = 0.007621025666594505 + 0.001 * 7.1794819831848145
Epoch 640, val loss: 1.079245924949646
Epoch 650, training loss: 0.014398027211427689 = 0.00721689872443676 + 0.001 * 7.18112850189209
Epoch 650, val loss: 1.085047721862793
Epoch 660, training loss: 0.01402883231639862 = 0.006845844443887472 + 0.001 * 7.182987689971924
Epoch 660, val loss: 1.0906500816345215
Epoch 670, training loss: 0.013682754710316658 = 0.006504427175968885 + 0.001 * 7.1783270835876465
Epoch 670, val loss: 1.0960856676101685
Epoch 680, training loss: 0.013359013944864273 = 0.006189625710248947 + 0.001 * 7.1693878173828125
Epoch 680, val loss: 1.1013529300689697
Epoch 690, training loss: 0.013109426014125347 = 0.005898764822632074 + 0.001 * 7.210660934448242
Epoch 690, val loss: 1.1064579486846924
Epoch 700, training loss: 0.012787492014467716 = 0.005629339255392551 + 0.001 * 7.1581525802612305
Epoch 700, val loss: 1.1114319562911987
Epoch 710, training loss: 0.012551292777061462 = 0.005378746893256903 + 0.001 * 7.172544956207275
Epoch 710, val loss: 1.1162883043289185
Epoch 720, training loss: 0.012312974780797958 = 0.005144597962498665 + 0.001 * 7.1683759689331055
Epoch 720, val loss: 1.1210931539535522
Epoch 730, training loss: 0.012093913741409779 = 0.0049249762669205666 + 0.001 * 7.1689372062683105
Epoch 730, val loss: 1.1258447170257568
Epoch 740, training loss: 0.011871140450239182 = 0.0047184620052576065 + 0.001 * 7.152677536010742
Epoch 740, val loss: 1.130537748336792
Epoch 750, training loss: 0.011678164824843407 = 0.0045240591280162334 + 0.001 * 7.1541056632995605
Epoch 750, val loss: 1.1351906061172485
Epoch 760, training loss: 0.011508770287036896 = 0.004340945743024349 + 0.001 * 7.1678242683410645
Epoch 760, val loss: 1.1397992372512817
Epoch 770, training loss: 0.011317218653857708 = 0.004168477840721607 + 0.001 * 7.148740291595459
Epoch 770, val loss: 1.1443389654159546
Epoch 780, training loss: 0.011149250902235508 = 0.0040059769526124 + 0.001 * 7.143273830413818
Epoch 780, val loss: 1.1488171815872192
Epoch 790, training loss: 0.010991415940225124 = 0.003852895461022854 + 0.001 * 7.138520240783691
Epoch 790, val loss: 1.1532014608383179
Epoch 800, training loss: 0.01086449809372425 = 0.003708604024723172 + 0.001 * 7.155893325805664
Epoch 800, val loss: 1.1575087308883667
Epoch 810, training loss: 0.01071775984019041 = 0.0035726099740713835 + 0.001 * 7.1451497077941895
Epoch 810, val loss: 1.1617404222488403
Epoch 820, training loss: 0.010570607148110867 = 0.0034443631302565336 + 0.001 * 7.126244068145752
Epoch 820, val loss: 1.1658533811569214
Epoch 830, training loss: 0.010453945957124233 = 0.0033233703579753637 + 0.001 * 7.130575656890869
Epoch 830, val loss: 1.169891595840454
Epoch 840, training loss: 0.010355419479310513 = 0.0032091254834085703 + 0.001 * 7.146293640136719
Epoch 840, val loss: 1.1738357543945312
Epoch 850, training loss: 0.010223083198070526 = 0.0031011991668492556 + 0.001 * 7.121883869171143
Epoch 850, val loss: 1.177683711051941
Epoch 860, training loss: 0.010142896324396133 = 0.0029991576448082924 + 0.001 * 7.14373779296875
Epoch 860, val loss: 1.1814266443252563
Epoch 870, training loss: 0.010043375194072723 = 0.002902709413319826 + 0.001 * 7.1406660079956055
Epoch 870, val loss: 1.1851050853729248
Epoch 880, training loss: 0.00991661474108696 = 0.002811388811096549 + 0.001 * 7.105226039886475
Epoch 880, val loss: 1.1886693239212036
Epoch 890, training loss: 0.00986469630151987 = 0.0027248880360275507 + 0.001 * 7.13980770111084
Epoch 890, val loss: 1.1921536922454834
Epoch 900, training loss: 0.009742247872054577 = 0.0026428934652358294 + 0.001 * 7.099353790283203
Epoch 900, val loss: 1.1955450773239136
Epoch 910, training loss: 0.009673330932855606 = 0.0025651194155216217 + 0.001 * 7.108210563659668
Epoch 910, val loss: 1.1988301277160645
Epoch 920, training loss: 0.009593802504241467 = 0.002491229446604848 + 0.001 * 7.102572917938232
Epoch 920, val loss: 1.2020825147628784
Epoch 930, training loss: 0.009517855010926723 = 0.0024209904950112104 + 0.001 * 7.096864223480225
Epoch 930, val loss: 1.2052271366119385
Epoch 940, training loss: 0.009440132416784763 = 0.002354190219193697 + 0.001 * 7.085941791534424
Epoch 940, val loss: 1.2082951068878174
Epoch 950, training loss: 0.009396390989422798 = 0.0022906034719198942 + 0.001 * 7.10578727722168
Epoch 950, val loss: 1.2112860679626465
Epoch 960, training loss: 0.009319374337792397 = 0.0022300584241747856 + 0.001 * 7.089314937591553
Epoch 960, val loss: 1.2142014503479004
Epoch 970, training loss: 0.009264021180570126 = 0.0021723529789596796 + 0.001 * 7.091668128967285
Epoch 970, val loss: 1.2170332670211792
Epoch 980, training loss: 0.009206020273268223 = 0.0021172920241951942 + 0.001 * 7.088727951049805
Epoch 980, val loss: 1.2197984457015991
Epoch 990, training loss: 0.009188484400510788 = 0.0020647733472287655 + 0.001 * 7.1237101554870605
Epoch 990, val loss: 1.2225042581558228
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 1.9450404644012451 = 1.936443567276001 + 0.001 * 8.596872329711914
Epoch 0, val loss: 1.9396337270736694
Epoch 10, training loss: 1.9351993799209595 = 1.9266026020050049 + 0.001 * 8.59682846069336
Epoch 10, val loss: 1.9296282529830933
Epoch 20, training loss: 1.922654628753662 = 1.914057970046997 + 0.001 * 8.596692085266113
Epoch 20, val loss: 1.9170728921890259
Epoch 30, training loss: 1.9048572778701782 = 1.8962609767913818 + 0.001 * 8.596357345581055
Epoch 30, val loss: 1.899377703666687
Epoch 40, training loss: 1.8791701793670654 = 1.870574712753296 + 0.001 * 8.595467567443848
Epoch 40, val loss: 1.8742573261260986
Epoch 50, training loss: 1.8453723192214966 = 1.8367795944213867 + 0.001 * 8.592741966247559
Epoch 50, val loss: 1.8428384065628052
Epoch 60, training loss: 1.8106212615966797 = 1.8020384311676025 + 0.001 * 8.582849502563477
Epoch 60, val loss: 1.8126413822174072
Epoch 70, training loss: 1.778213381767273 = 1.7696757316589355 + 0.001 * 8.537598609924316
Epoch 70, val loss: 1.7829452753067017
Epoch 80, training loss: 1.7331525087356567 = 1.7249068021774292 + 0.001 * 8.245712280273438
Epoch 80, val loss: 1.7412693500518799
Epoch 90, training loss: 1.66905677318573 = 1.6609429121017456 + 0.001 * 8.113898277282715
Epoch 90, val loss: 1.6860629320144653
Epoch 100, training loss: 1.5847620964050293 = 1.5766710042953491 + 0.001 * 8.091073036193848
Epoch 100, val loss: 1.616256833076477
Epoch 110, training loss: 1.489736795425415 = 1.4816569089889526 + 0.001 * 8.07991886138916
Epoch 110, val loss: 1.5377447605133057
Epoch 120, training loss: 1.3947243690490723 = 1.386667013168335 + 0.001 * 8.057361602783203
Epoch 120, val loss: 1.4610687494277954
Epoch 130, training loss: 1.3032623529434204 = 1.2952814102172852 + 0.001 * 7.980899810791016
Epoch 130, val loss: 1.3892920017242432
Epoch 140, training loss: 1.2170125246047974 = 1.20918869972229 + 0.001 * 7.823780059814453
Epoch 140, val loss: 1.3248648643493652
Epoch 150, training loss: 1.1372277736663818 = 1.1294384002685547 + 0.001 * 7.7893452644348145
Epoch 150, val loss: 1.2685686349868774
Epoch 160, training loss: 1.0632719993591309 = 1.0555369853973389 + 0.001 * 7.735019683837891
Epoch 160, val loss: 1.2191803455352783
Epoch 170, training loss: 0.9945626854896545 = 0.9868699908256531 + 0.001 * 7.692712783813477
Epoch 170, val loss: 1.1743172407150269
Epoch 180, training loss: 0.9304822087287903 = 0.9228277206420898 + 0.001 * 7.654514312744141
Epoch 180, val loss: 1.1325823068618774
Epoch 190, training loss: 0.8691229820251465 = 0.8615071177482605 + 0.001 * 7.615869045257568
Epoch 190, val loss: 1.0917426347732544
Epoch 200, training loss: 0.8082406520843506 = 0.8006740808486938 + 0.001 * 7.566600799560547
Epoch 200, val loss: 1.050217628479004
Epoch 210, training loss: 0.7471680045127869 = 0.7396435141563416 + 0.001 * 7.524468898773193
Epoch 210, val loss: 1.0076978206634521
Epoch 220, training loss: 0.6869553923606873 = 0.679443359375 + 0.001 * 7.512017250061035
Epoch 220, val loss: 0.9656999111175537
Epoch 230, training loss: 0.6287165284156799 = 0.6212120652198792 + 0.001 * 7.504467010498047
Epoch 230, val loss: 0.9266824722290039
Epoch 240, training loss: 0.5725570917129517 = 0.5650628209114075 + 0.001 * 7.4942545890808105
Epoch 240, val loss: 0.8928127884864807
Epoch 250, training loss: 0.518332302570343 = 0.5108518004417419 + 0.001 * 7.480525493621826
Epoch 250, val loss: 0.8656381964683533
Epoch 260, training loss: 0.46636539697647095 = 0.4589039981365204 + 0.001 * 7.461400508880615
Epoch 260, val loss: 0.84586101770401
Epoch 270, training loss: 0.41730499267578125 = 0.4098731577396393 + 0.001 * 7.431829929351807
Epoch 270, val loss: 0.8334305882453918
Epoch 280, training loss: 0.3715977072715759 = 0.3641975224018097 + 0.001 * 7.400174140930176
Epoch 280, val loss: 0.8280423879623413
Epoch 290, training loss: 0.3295019865036011 = 0.32212209701538086 + 0.001 * 7.379898548126221
Epoch 290, val loss: 0.8285688757896423
Epoch 300, training loss: 0.2911832332611084 = 0.2838148772716522 + 0.001 * 7.368356704711914
Epoch 300, val loss: 0.8340157866477966
Epoch 310, training loss: 0.2568158209323883 = 0.24944865703582764 + 0.001 * 7.367172718048096
Epoch 310, val loss: 0.8438427448272705
Epoch 320, training loss: 0.22644662857055664 = 0.21908171474933624 + 0.001 * 7.364919662475586
Epoch 320, val loss: 0.8576611876487732
Epoch 330, training loss: 0.19989760220050812 = 0.19253340363502502 + 0.001 * 7.364198684692383
Epoch 330, val loss: 0.874834418296814
Epoch 340, training loss: 0.1768006980419159 = 0.169436514377594 + 0.001 * 7.364190578460693
Epoch 340, val loss: 0.8945997357368469
Epoch 350, training loss: 0.1567213237285614 = 0.14935782551765442 + 0.001 * 7.363495826721191
Epoch 350, val loss: 0.9162382483482361
Epoch 360, training loss: 0.13925288617610931 = 0.13188186287879944 + 0.001 * 7.371025085449219
Epoch 360, val loss: 0.939219057559967
Epoch 370, training loss: 0.124009869992733 = 0.11664632707834244 + 0.001 * 7.363544464111328
Epoch 370, val loss: 0.9630283117294312
Epoch 380, training loss: 0.11070558428764343 = 0.10334125906229019 + 0.001 * 7.364327430725098
Epoch 380, val loss: 0.9873445630073547
Epoch 390, training loss: 0.09907017648220062 = 0.09170674532651901 + 0.001 * 7.36343240737915
Epoch 390, val loss: 1.011783480644226
Epoch 400, training loss: 0.08889131247997284 = 0.08152696490287781 + 0.001 * 7.364349842071533
Epoch 400, val loss: 1.036165475845337
Epoch 410, training loss: 0.07998115569353104 = 0.07261684536933899 + 0.001 * 7.364310264587402
Epoch 410, val loss: 1.0603549480438232
Epoch 420, training loss: 0.07218112796545029 = 0.06481729447841644 + 0.001 * 7.363830089569092
Epoch 420, val loss: 1.084172248840332
Epoch 430, training loss: 0.06535404175519943 = 0.05799083411693573 + 0.001 * 7.363205432891846
Epoch 430, val loss: 1.107488989830017
Epoch 440, training loss: 0.0593787357211113 = 0.052017029374837875 + 0.001 * 7.3617072105407715
Epoch 440, val loss: 1.1302465200424194
Epoch 450, training loss: 0.05415188893675804 = 0.046789444983005524 + 0.001 * 7.3624444007873535
Epoch 450, val loss: 1.15243661403656
Epoch 460, training loss: 0.04957253485918045 = 0.04221181571483612 + 0.001 * 7.360720634460449
Epoch 460, val loss: 1.1739686727523804
Epoch 470, training loss: 0.04555552452802658 = 0.03819785267114639 + 0.001 * 7.357669830322266
Epoch 470, val loss: 1.1948091983795166
Epoch 480, training loss: 0.04205656796693802 = 0.03467332571744919 + 0.001 * 7.383242130279541
Epoch 480, val loss: 1.2149381637573242
Epoch 490, training loss: 0.03892955556511879 = 0.03157290071249008 + 0.001 * 7.356655597686768
Epoch 490, val loss: 1.234374761581421
Epoch 500, training loss: 0.0361948125064373 = 0.028840404003858566 + 0.001 * 7.354408264160156
Epoch 500, val loss: 1.2531193494796753
Epoch 510, training loss: 0.033777471631765366 = 0.026426414027810097 + 0.001 * 7.351056098937988
Epoch 510, val loss: 1.2711467742919922
Epoch 520, training loss: 0.03164222463965416 = 0.024288875982165337 + 0.001 * 7.353348255157471
Epoch 520, val loss: 1.2885119915008545
Epoch 530, training loss: 0.02973705157637596 = 0.02239060029387474 + 0.001 * 7.346450328826904
Epoch 530, val loss: 1.3052445650100708
Epoch 540, training loss: 0.028055749833583832 = 0.02070010080933571 + 0.001 * 7.355648040771484
Epoch 540, val loss: 1.3213516473770142
Epoch 550, training loss: 0.026528527960181236 = 0.019190305843949318 + 0.001 * 7.338222026824951
Epoch 550, val loss: 1.336837887763977
Epoch 560, training loss: 0.025175563991069794 = 0.017838023602962494 + 0.001 * 7.337540149688721
Epoch 560, val loss: 1.3517705202102661
Epoch 570, training loss: 0.02396693080663681 = 0.016623342409729958 + 0.001 * 7.343587398529053
Epoch 570, val loss: 1.3661437034606934
Epoch 580, training loss: 0.022858675569295883 = 0.015529063530266285 + 0.001 * 7.3296122550964355
Epoch 580, val loss: 1.380021572113037
Epoch 590, training loss: 0.02187826670706272 = 0.014540391974151134 + 0.001 * 7.337874412536621
Epoch 590, val loss: 1.393396019935608
Epoch 600, training loss: 0.021022766828536987 = 0.013644620776176453 + 0.001 * 7.378145694732666
Epoch 600, val loss: 1.4062923192977905
Epoch 610, training loss: 0.02015548199415207 = 0.012830943800508976 + 0.001 * 7.32453727722168
Epoch 610, val loss: 1.4187119007110596
Epoch 620, training loss: 0.019419748336076736 = 0.012089916504919529 + 0.001 * 7.329831123352051
Epoch 620, val loss: 1.4307191371917725
Epoch 630, training loss: 0.018761077895760536 = 0.011413319036364555 + 0.001 * 7.347757816314697
Epoch 630, val loss: 1.44231116771698
Epoch 640, training loss: 0.01809421367943287 = 0.0107941385358572 + 0.001 * 7.300074100494385
Epoch 640, val loss: 1.4534900188446045
Epoch 650, training loss: 0.017540860921144485 = 0.010226200334727764 + 0.001 * 7.314659118652344
Epoch 650, val loss: 1.4642999172210693
Epoch 660, training loss: 0.016983002424240112 = 0.00970410369336605 + 0.001 * 7.278897285461426
Epoch 660, val loss: 1.4747421741485596
Epoch 670, training loss: 0.016526659950613976 = 0.009223061613738537 + 0.001 * 7.303597450256348
Epoch 670, val loss: 1.4848328828811646
Epoch 680, training loss: 0.016109313815832138 = 0.008778953924775124 + 0.001 * 7.33035945892334
Epoch 680, val loss: 1.4946409463882446
Epoch 690, training loss: 0.015637865290045738 = 0.0083681121468544 + 0.001 * 7.269752502441406
Epoch 690, val loss: 1.504117727279663
Epoch 700, training loss: 0.01527315005660057 = 0.007987366989254951 + 0.001 * 7.285782337188721
Epoch 700, val loss: 1.513304591178894
Epoch 710, training loss: 0.014935139566659927 = 0.007633855100721121 + 0.001 * 7.301283836364746
Epoch 710, val loss: 1.522214651107788
Epoch 720, training loss: 0.014574806205928326 = 0.00730507168918848 + 0.001 * 7.2697343826293945
Epoch 720, val loss: 1.5308446884155273
Epoch 730, training loss: 0.014255146495997906 = 0.006998782511800528 + 0.001 * 7.256363868713379
Epoch 730, val loss: 1.5392321348190308
Epoch 740, training loss: 0.013957554474473 = 0.006712950766086578 + 0.001 * 7.244602680206299
Epoch 740, val loss: 1.5473912954330444
Epoch 750, training loss: 0.013722767122089863 = 0.006445846054702997 + 0.001 * 7.276920795440674
Epoch 750, val loss: 1.555289626121521
Epoch 760, training loss: 0.013466840609908104 = 0.00619583809748292 + 0.001 * 7.271002292633057
Epoch 760, val loss: 1.5629687309265137
Epoch 770, training loss: 0.013210836797952652 = 0.005961494520306587 + 0.001 * 7.24934196472168
Epoch 770, val loss: 1.5704420804977417
Epoch 780, training loss: 0.013009735383093357 = 0.005741542670875788 + 0.001 * 7.268192291259766
Epoch 780, val loss: 1.5777230262756348
Epoch 790, training loss: 0.01278868317604065 = 0.005534821189939976 + 0.001 * 7.253861904144287
Epoch 790, val loss: 1.5847679376602173
Epoch 800, training loss: 0.012588982470333576 = 0.0053402879275381565 + 0.001 * 7.24869441986084
Epoch 800, val loss: 1.591668725013733
Epoch 810, training loss: 0.012382847256958485 = 0.005156978033483028 + 0.001 * 7.2258687019348145
Epoch 810, val loss: 1.59837007522583
Epoch 820, training loss: 0.012203929014503956 = 0.004984082654118538 + 0.001 * 7.219846248626709
Epoch 820, val loss: 1.6048970222473145
Epoch 830, training loss: 0.0120334941893816 = 0.0048208278603851795 + 0.001 * 7.212665557861328
Epoch 830, val loss: 1.6112546920776367
Epoch 840, training loss: 0.011884797364473343 = 0.004666488151997328 + 0.001 * 7.21830940246582
Epoch 840, val loss: 1.6174447536468506
Epoch 850, training loss: 0.011750109493732452 = 0.004520421847701073 + 0.001 * 7.229687690734863
Epoch 850, val loss: 1.623492956161499
Epoch 860, training loss: 0.01159367710351944 = 0.004382036160677671 + 0.001 * 7.211640357971191
Epoch 860, val loss: 1.6293518543243408
Epoch 870, training loss: 0.011441085487604141 = 0.004250817932188511 + 0.001 * 7.1902666091918945
Epoch 870, val loss: 1.6351096630096436
Epoch 880, training loss: 0.011316796764731407 = 0.004126265645027161 + 0.001 * 7.190531253814697
Epoch 880, val loss: 1.6406962871551514
Epoch 890, training loss: 0.011204583570361137 = 0.0040079141035676 + 0.001 * 7.196669578552246
Epoch 890, val loss: 1.6461776494979858
Epoch 900, training loss: 0.011087765917181969 = 0.0038953390903770924 + 0.001 * 7.192426681518555
Epoch 900, val loss: 1.651497721672058
Epoch 910, training loss: 0.010979666374623775 = 0.0037881089374423027 + 0.001 * 7.191556930541992
Epoch 910, val loss: 1.6567175388336182
Epoch 920, training loss: 0.010897824540734291 = 0.003685921197757125 + 0.001 * 7.211903095245361
Epoch 920, val loss: 1.661845088005066
Epoch 930, training loss: 0.010796155780553818 = 0.003588356776162982 + 0.001 * 7.207798480987549
Epoch 930, val loss: 1.6668215990066528
Epoch 940, training loss: 0.01069684699177742 = 0.003494991920888424 + 0.001 * 7.201854228973389
Epoch 940, val loss: 1.6717089414596558
Epoch 950, training loss: 0.010618450120091438 = 0.003405447583645582 + 0.001 * 7.2130022048950195
Epoch 950, val loss: 1.6764945983886719
Epoch 960, training loss: 0.010512489825487137 = 0.003319269511848688 + 0.001 * 7.193220615386963
Epoch 960, val loss: 1.6811801195144653
Epoch 970, training loss: 0.010402788408100605 = 0.0032359561882913113 + 0.001 * 7.166831970214844
Epoch 970, val loss: 1.6857047080993652
Epoch 980, training loss: 0.010320383124053478 = 0.0031551853753626347 + 0.001 * 7.165197372436523
Epoch 980, val loss: 1.690209150314331
Epoch 990, training loss: 0.010249726474285126 = 0.0030764685943722725 + 0.001 * 7.173257350921631
Epoch 990, val loss: 1.6945849657058716
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 1.9454714059829712 = 1.936874508857727 + 0.001 * 8.596866607666016
Epoch 0, val loss: 1.9349660873413086
Epoch 10, training loss: 1.9352803230285645 = 1.9266835451126099 + 0.001 * 8.596826553344727
Epoch 10, val loss: 1.9250967502593994
Epoch 20, training loss: 1.922723412513733 = 1.9141267538070679 + 0.001 * 8.596692085266113
Epoch 20, val loss: 1.9125336408615112
Epoch 30, training loss: 1.9055309295654297 = 1.8969345092773438 + 0.001 * 8.596399307250977
Epoch 30, val loss: 1.8950386047363281
Epoch 40, training loss: 1.8812530040740967 = 1.872657299041748 + 0.001 * 8.595748901367188
Epoch 40, val loss: 1.8705410957336426
Epoch 50, training loss: 1.8491606712341309 = 1.840566635131836 + 0.001 * 8.594030380249023
Epoch 50, val loss: 1.839637279510498
Epoch 60, training loss: 1.8148008584976196 = 1.806212306022644 + 0.001 * 8.588506698608398
Epoch 60, val loss: 1.809790015220642
Epoch 70, training loss: 1.7832485437393188 = 1.7746814489364624 + 0.001 * 8.567124366760254
Epoch 70, val loss: 1.783760666847229
Epoch 80, training loss: 1.7430709600448608 = 1.7346309423446655 + 0.001 * 8.43998908996582
Epoch 80, val loss: 1.7486777305603027
Epoch 90, training loss: 1.686861515045166 = 1.6786693334579468 + 0.001 * 8.192176818847656
Epoch 90, val loss: 1.700537919998169
Epoch 100, training loss: 1.6105380058288574 = 1.602553129196167 + 0.001 * 7.984874725341797
Epoch 100, val loss: 1.6369755268096924
Epoch 110, training loss: 1.5172736644744873 = 1.5095406770706177 + 0.001 * 7.733006000518799
Epoch 110, val loss: 1.5618337392807007
Epoch 120, training loss: 1.416069507598877 = 1.4084713459014893 + 0.001 * 7.598219871520996
Epoch 120, val loss: 1.4819849729537964
Epoch 130, training loss: 1.3150712251663208 = 1.3075482845306396 + 0.001 * 7.5229339599609375
Epoch 130, val loss: 1.4037913084030151
Epoch 140, training loss: 1.216521978378296 = 1.2090307474136353 + 0.001 * 7.491260051727295
Epoch 140, val loss: 1.3281975984573364
Epoch 150, training loss: 1.1194465160369873 = 1.1119661331176758 + 0.001 * 7.480423927307129
Epoch 150, val loss: 1.253730297088623
Epoch 160, training loss: 1.0240734815597534 = 1.0166021585464478 + 0.001 * 7.471376419067383
Epoch 160, val loss: 1.1809098720550537
Epoch 170, training loss: 0.9321144223213196 = 0.9246507883071899 + 0.001 * 7.463652610778809
Epoch 170, val loss: 1.1114649772644043
Epoch 180, training loss: 0.8464428782463074 = 0.8389830589294434 + 0.001 * 7.459799766540527
Epoch 180, val loss: 1.0481534004211426
Epoch 190, training loss: 0.7695702910423279 = 0.7621129155158997 + 0.001 * 7.457367420196533
Epoch 190, val loss: 0.9927489161491394
Epoch 200, training loss: 0.7021233439445496 = 0.6946696043014526 + 0.001 * 7.45374059677124
Epoch 200, val loss: 0.9464138746261597
Epoch 210, training loss: 0.6425184011459351 = 0.6350702047348022 + 0.001 * 7.448180198669434
Epoch 210, val loss: 0.9082896709442139
Epoch 220, training loss: 0.5878551602363586 = 0.5804146528244019 + 0.001 * 7.440493583679199
Epoch 220, val loss: 0.8764182925224304
Epoch 230, training loss: 0.5352810025215149 = 0.5278506875038147 + 0.001 * 7.430295467376709
Epoch 230, val loss: 0.8484926819801331
Epoch 240, training loss: 0.4831187427043915 = 0.47570163011550903 + 0.001 * 7.417123317718506
Epoch 240, val loss: 0.8226363062858582
Epoch 250, training loss: 0.43111857771873474 = 0.4237205386161804 + 0.001 * 7.398037910461426
Epoch 250, val loss: 0.7984031438827515
Epoch 260, training loss: 0.38025012612342834 = 0.3728716969490051 + 0.001 * 7.378424167633057
Epoch 260, val loss: 0.7761195302009583
Epoch 270, training loss: 0.33200621604919434 = 0.32463768124580383 + 0.001 * 7.368546009063721
Epoch 270, val loss: 0.7570163011550903
Epoch 280, training loss: 0.287715882062912 = 0.2803683578968048 + 0.001 * 7.347536563873291
Epoch 280, val loss: 0.7419537305831909
Epoch 290, training loss: 0.24818773567676544 = 0.24083812534809113 + 0.001 * 7.349615097045898
Epoch 290, val loss: 0.7317042946815491
Epoch 300, training loss: 0.21362492442131042 = 0.20629183948040009 + 0.001 * 7.333082675933838
Epoch 300, val loss: 0.7261583209037781
Epoch 310, training loss: 0.1839703917503357 = 0.1766337752342224 + 0.001 * 7.336615085601807
Epoch 310, val loss: 0.7251557111740112
Epoch 320, training loss: 0.15882685780525208 = 0.15149378776550293 + 0.001 * 7.333064079284668
Epoch 320, val loss: 0.7281484603881836
Epoch 330, training loss: 0.1376410573720932 = 0.13031405210494995 + 0.001 * 7.327000617980957
Epoch 330, val loss: 0.7342527508735657
Epoch 340, training loss: 0.11981363594532013 = 0.11249282956123352 + 0.001 * 7.320809841156006
Epoch 340, val loss: 0.7425768375396729
Epoch 350, training loss: 0.10479218512773514 = 0.09746965020895004 + 0.001 * 7.322533130645752
Epoch 350, val loss: 0.7524641752243042
Epoch 360, training loss: 0.092086061835289 = 0.08477210253477097 + 0.001 * 7.313962459564209
Epoch 360, val loss: 0.763510525226593
Epoch 370, training loss: 0.08132537454366684 = 0.07401004433631897 + 0.001 * 7.315328598022461
Epoch 370, val loss: 0.7753379344940186
Epoch 380, training loss: 0.07218118011951447 = 0.0648699402809143 + 0.001 * 7.311236381530762
Epoch 380, val loss: 0.7876370549201965
Epoch 390, training loss: 0.06440360844135284 = 0.05709460750222206 + 0.001 * 7.308997631072998
Epoch 390, val loss: 0.8001604676246643
Epoch 400, training loss: 0.05777278169989586 = 0.05046907439827919 + 0.001 * 7.303706645965576
Epoch 400, val loss: 0.8127447366714478
Epoch 410, training loss: 0.052122291177511215 = 0.044811759144067764 + 0.001 * 7.310532093048096
Epoch 410, val loss: 0.8252673149108887
Epoch 420, training loss: 0.04726356640458107 = 0.039965275675058365 + 0.001 * 7.298289775848389
Epoch 420, val loss: 0.8376030325889587
Epoch 430, training loss: 0.04309302940964699 = 0.035796020179986954 + 0.001 * 7.29701042175293
Epoch 430, val loss: 0.8497706055641174
Epoch 440, training loss: 0.03948685899376869 = 0.03219715505838394 + 0.001 * 7.2897047996521
Epoch 440, val loss: 0.8616674542427063
Epoch 450, training loss: 0.036360617727041245 = 0.029078610241413116 + 0.001 * 7.282005786895752
Epoch 450, val loss: 0.8732948303222656
Epoch 460, training loss: 0.03366176411509514 = 0.026367001235485077 + 0.001 * 7.294763565063477
Epoch 460, val loss: 0.8845497965812683
Epoch 470, training loss: 0.03128497675061226 = 0.02399938926100731 + 0.001 * 7.28558874130249
Epoch 470, val loss: 0.8955193758010864
Epoch 480, training loss: 0.029193326830863953 = 0.02192385494709015 + 0.001 * 7.269470691680908
Epoch 480, val loss: 0.9061242938041687
Epoch 490, training loss: 0.027399932965636253 = 0.020097626373171806 + 0.001 * 7.302305698394775
Epoch 490, val loss: 0.9164273738861084
Epoch 500, training loss: 0.025752298533916473 = 0.018485311418771744 + 0.001 * 7.266985893249512
Epoch 500, val loss: 0.9263922572135925
Epoch 510, training loss: 0.024315783753991127 = 0.017056822776794434 + 0.001 * 7.258961200714111
Epoch 510, val loss: 0.9360597133636475
Epoch 520, training loss: 0.023060649633407593 = 0.01578661799430847 + 0.001 * 7.274031639099121
Epoch 520, val loss: 0.9453815817832947
Epoch 530, training loss: 0.0219020564109087 = 0.014653302729129791 + 0.001 * 7.248753547668457
Epoch 530, val loss: 0.9544174671173096
Epoch 540, training loss: 0.020883597433567047 = 0.013638448901474476 + 0.001 * 7.245147228240967
Epoch 540, val loss: 0.9631645083427429
Epoch 550, training loss: 0.01996796950697899 = 0.012726841494441032 + 0.001 * 7.241128444671631
Epoch 550, val loss: 0.9716505408287048
Epoch 560, training loss: 0.019149675965309143 = 0.011905411258339882 + 0.001 * 7.244264602661133
Epoch 560, val loss: 0.9798448085784912
Epoch 570, training loss: 0.018419409170746803 = 0.011163127608597279 + 0.001 * 7.256281852722168
Epoch 570, val loss: 0.9877970218658447
Epoch 580, training loss: 0.017723437398672104 = 0.010490315034985542 + 0.001 * 7.233122825622559
Epoch 580, val loss: 0.995491623878479
Epoch 590, training loss: 0.0171070359647274 = 0.009878666140139103 + 0.001 * 7.22836971282959
Epoch 590, val loss: 1.002912998199463
Epoch 600, training loss: 0.016559213399887085 = 0.009321263991296291 + 0.001 * 7.237948417663574
Epoch 600, val loss: 1.0101318359375
Epoch 610, training loss: 0.016048872843384743 = 0.00881209596991539 + 0.001 * 7.236776828765869
Epoch 610, val loss: 1.0171175003051758
Epoch 620, training loss: 0.015583885833621025 = 0.008345721289515495 + 0.001 * 7.23816442489624
Epoch 620, val loss: 1.023902416229248
Epoch 630, training loss: 0.015139086171984673 = 0.007917490787804127 + 0.001 * 7.221595287322998
Epoch 630, val loss: 1.0304808616638184
Epoch 640, training loss: 0.014765310101211071 = 0.00752340629696846 + 0.001 * 7.241903305053711
Epoch 640, val loss: 1.036868691444397
Epoch 650, training loss: 0.014373170211911201 = 0.007160053122788668 + 0.001 * 7.2131171226501465
Epoch 650, val loss: 1.04304838180542
Epoch 660, training loss: 0.01409703865647316 = 0.0068242899142205715 + 0.001 * 7.272747993469238
Epoch 660, val loss: 1.0490752458572388
Epoch 670, training loss: 0.01371668465435505 = 0.006513416301459074 + 0.001 * 7.203267574310303
Epoch 670, val loss: 1.0549291372299194
Epoch 680, training loss: 0.013429682701826096 = 0.006224989891052246 + 0.001 * 7.2046918869018555
Epoch 680, val loss: 1.0606000423431396
Epoch 690, training loss: 0.013162355870008469 = 0.005956399720162153 + 0.001 * 7.205955505371094
Epoch 690, val loss: 1.066165804862976
Epoch 700, training loss: 0.012939674779772758 = 0.005704517476260662 + 0.001 * 7.235156536102295
Epoch 700, val loss: 1.0715904235839844
Epoch 710, training loss: 0.012676477432250977 = 0.005466873291879892 + 0.001 * 7.209604263305664
Epoch 710, val loss: 1.0769968032836914
Epoch 720, training loss: 0.012478557415306568 = 0.005241270177066326 + 0.001 * 7.2372870445251465
Epoch 720, val loss: 1.0824037790298462
Epoch 730, training loss: 0.012223657220602036 = 0.005027226638048887 + 0.001 * 7.196430206298828
Epoch 730, val loss: 1.0877424478530884
Epoch 740, training loss: 0.012014083564281464 = 0.0048242476768791676 + 0.001 * 7.189835071563721
Epoch 740, val loss: 1.0930883884429932
Epoch 750, training loss: 0.011812553741037846 = 0.004632191266864538 + 0.001 * 7.180362224578857
Epoch 750, val loss: 1.098371982574463
Epoch 760, training loss: 0.01164478249847889 = 0.0044507673010230064 + 0.001 * 7.1940155029296875
Epoch 760, val loss: 1.1036080121994019
Epoch 770, training loss: 0.011468915268778801 = 0.00427953852340579 + 0.001 * 7.189375877380371
Epoch 770, val loss: 1.1087507009506226
Epoch 780, training loss: 0.01132485643029213 = 0.004117969889193773 + 0.001 * 7.2068867683410645
Epoch 780, val loss: 1.11381995677948
Epoch 790, training loss: 0.011157549917697906 = 0.003965470008552074 + 0.001 * 7.192079544067383
Epoch 790, val loss: 1.1188255548477173
Epoch 800, training loss: 0.010995388962328434 = 0.0038212526123970747 + 0.001 * 7.174136161804199
Epoch 800, val loss: 1.1237244606018066
Epoch 810, training loss: 0.010865136981010437 = 0.0036845142021775246 + 0.001 * 7.180622577667236
Epoch 810, val loss: 1.1285830736160278
Epoch 820, training loss: 0.010730846785008907 = 0.0035545118153095245 + 0.001 * 7.176334857940674
Epoch 820, val loss: 1.1333622932434082
Epoch 830, training loss: 0.010612674988806248 = 0.0034305837471038103 + 0.001 * 7.182090759277344
Epoch 830, val loss: 1.138118028640747
Epoch 840, training loss: 0.01050060149282217 = 0.0033123455941677094 + 0.001 * 7.188255786895752
Epoch 840, val loss: 1.1428170204162598
Epoch 850, training loss: 0.01036420464515686 = 0.003199509810656309 + 0.001 * 7.164694786071777
Epoch 850, val loss: 1.1474248170852661
Epoch 860, training loss: 0.010261749848723412 = 0.0030920403078198433 + 0.001 * 7.169708728790283
Epoch 860, val loss: 1.1520204544067383
Epoch 870, training loss: 0.010183286853134632 = 0.0029895089101046324 + 0.001 * 7.193777561187744
Epoch 870, val loss: 1.1565300226211548
Epoch 880, training loss: 0.010048683732748032 = 0.002891690470278263 + 0.001 * 7.156993389129639
Epoch 880, val loss: 1.1609771251678467
Epoch 890, training loss: 0.009941104799509048 = 0.0027984234038740396 + 0.001 * 7.142681121826172
Epoch 890, val loss: 1.1653621196746826
Epoch 900, training loss: 0.009870968759059906 = 0.0027095871046185493 + 0.001 * 7.161380767822266
Epoch 900, val loss: 1.1696704626083374
Epoch 910, training loss: 0.00976671651005745 = 0.002625132678076625 + 0.001 * 7.1415839195251465
Epoch 910, val loss: 1.1738895177841187
Epoch 920, training loss: 0.009723938070237637 = 0.002544933930039406 + 0.001 * 7.179003715515137
Epoch 920, val loss: 1.178048014640808
Epoch 930, training loss: 0.009622685611248016 = 0.0024685985408723354 + 0.001 * 7.154086112976074
Epoch 930, val loss: 1.182120680809021
Epoch 940, training loss: 0.009527530521154404 = 0.002395975636318326 + 0.001 * 7.13155460357666
Epoch 940, val loss: 1.1861006021499634
Epoch 950, training loss: 0.009470867924392223 = 0.002326855668798089 + 0.001 * 7.144011974334717
Epoch 950, val loss: 1.1900265216827393
Epoch 960, training loss: 0.009401663206517696 = 0.002261002082377672 + 0.001 * 7.140660762786865
Epoch 960, val loss: 1.1938663721084595
Epoch 970, training loss: 0.009319350123405457 = 0.002198212081566453 + 0.001 * 7.121138095855713
Epoch 970, val loss: 1.1976004838943481
Epoch 980, training loss: 0.009258411824703217 = 0.0021384956780821085 + 0.001 * 7.119915962219238
Epoch 980, val loss: 1.201317548751831
Epoch 990, training loss: 0.009242604486644268 = 0.0020816088654100895 + 0.001 * 7.1609954833984375
Epoch 990, val loss: 1.2049341201782227
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8170795993674222
The final CL Acc:0.77654, 0.02188, The final GNN Acc:0.81567, 0.00131
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13226])
remove edge: torch.Size([2, 7894])
updated graph: torch.Size([2, 10564])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9474903345108032 = 1.9388935565948486 + 0.001 * 8.59682846069336
Epoch 0, val loss: 1.9315001964569092
Epoch 10, training loss: 1.9376267194747925 = 1.929029941558838 + 0.001 * 8.596750259399414
Epoch 10, val loss: 1.9218659400939941
Epoch 20, training loss: 1.9252949953079224 = 1.9166984558105469 + 0.001 * 8.596524238586426
Epoch 20, val loss: 1.9092603921890259
Epoch 30, training loss: 1.907968282699585 = 1.8993723392486572 + 0.001 * 8.595973014831543
Epoch 30, val loss: 1.891080379486084
Epoch 40, training loss: 1.8827499151229858 = 1.8741554021835327 + 0.001 * 8.594465255737305
Epoch 40, val loss: 1.8646436929702759
Epoch 50, training loss: 1.8482491970062256 = 1.839659571647644 + 0.001 * 8.58965015411377
Epoch 50, val loss: 1.8300414085388184
Epoch 60, training loss: 1.8100312948226929 = 1.8014607429504395 + 0.001 * 8.570521354675293
Epoch 60, val loss: 1.7956585884094238
Epoch 70, training loss: 1.7751805782318115 = 1.766711950302124 + 0.001 * 8.468657493591309
Epoch 70, val loss: 1.7685297727584839
Epoch 80, training loss: 1.7314448356628418 = 1.7232784032821655 + 0.001 * 8.166435241699219
Epoch 80, val loss: 1.732023000717163
Epoch 90, training loss: 1.6700924634933472 = 1.6620632410049438 + 0.001 * 8.02923583984375
Epoch 90, val loss: 1.67703115940094
Epoch 100, training loss: 1.588758945465088 = 1.5809098482131958 + 0.001 * 7.84913444519043
Epoch 100, val loss: 1.6051095724105835
Epoch 110, training loss: 1.4947752952575684 = 1.4871536493301392 + 0.001 * 7.6216230392456055
Epoch 110, val loss: 1.526139259338379
Epoch 120, training loss: 1.3964799642562866 = 1.3889455795288086 + 0.001 * 7.5344319343566895
Epoch 120, val loss: 1.4440417289733887
Epoch 130, training loss: 1.2946444749832153 = 1.2871756553649902 + 0.001 * 7.468844890594482
Epoch 130, val loss: 1.3614205121994019
Epoch 140, training loss: 1.1874548196792603 = 1.1800484657287598 + 0.001 * 7.406311988830566
Epoch 140, val loss: 1.2753770351409912
Epoch 150, training loss: 1.0770456790924072 = 1.0696661472320557 + 0.001 * 7.379493236541748
Epoch 150, val loss: 1.1877385377883911
Epoch 160, training loss: 0.9690179824829102 = 0.9616582989692688 + 0.001 * 7.359709739685059
Epoch 160, val loss: 1.1026198863983154
Epoch 170, training loss: 0.8697305917739868 = 0.8623855710029602 + 0.001 * 7.3449931144714355
Epoch 170, val loss: 1.026175618171692
Epoch 180, training loss: 0.7832684516906738 = 0.7759363651275635 + 0.001 * 7.332106113433838
Epoch 180, val loss: 0.9620410799980164
Epoch 190, training loss: 0.7101242542266846 = 0.7028095722198486 + 0.001 * 7.314688205718994
Epoch 190, val loss: 0.911880612373352
Epoch 200, training loss: 0.6481463313102722 = 0.6408575177192688 + 0.001 * 7.288833141326904
Epoch 200, val loss: 0.8741697072982788
Epoch 210, training loss: 0.5946834683418274 = 0.5874229073524475 + 0.001 * 7.260549545288086
Epoch 210, val loss: 0.8462761640548706
Epoch 220, training loss: 0.5476551651954651 = 0.5404194593429565 + 0.001 * 7.235714435577393
Epoch 220, val loss: 0.8256468176841736
Epoch 230, training loss: 0.5056272745132446 = 0.4984070360660553 + 0.001 * 7.220254421234131
Epoch 230, val loss: 0.8104482889175415
Epoch 240, training loss: 0.4674113988876343 = 0.4601989686489105 + 0.001 * 7.212433338165283
Epoch 240, val loss: 0.7991963624954224
Epoch 250, training loss: 0.43189483880996704 = 0.4246852695941925 + 0.001 * 7.209574222564697
Epoch 250, val loss: 0.7907033562660217
Epoch 260, training loss: 0.39817100763320923 = 0.3909622132778168 + 0.001 * 7.208805561065674
Epoch 260, val loss: 0.7842172980308533
Epoch 270, training loss: 0.36571237444877625 = 0.35850390791893005 + 0.001 * 7.208466529846191
Epoch 270, val loss: 0.7791461944580078
Epoch 280, training loss: 0.33440902829170227 = 0.3272008001804352 + 0.001 * 7.208232879638672
Epoch 280, val loss: 0.7752195000648499
Epoch 290, training loss: 0.30436813831329346 = 0.29715949296951294 + 0.001 * 7.208655834197998
Epoch 290, val loss: 0.7722514271736145
Epoch 300, training loss: 0.2755613327026367 = 0.26835179328918457 + 0.001 * 7.209529399871826
Epoch 300, val loss: 0.7700716257095337
Epoch 310, training loss: 0.24774707853794098 = 0.24053630232810974 + 0.001 * 7.210776329040527
Epoch 310, val loss: 0.7685900926589966
Epoch 320, training loss: 0.22075991332530975 = 0.21354763209819794 + 0.001 * 7.212281703948975
Epoch 320, val loss: 0.7674124836921692
Epoch 330, training loss: 0.1948937475681305 = 0.18767909705638885 + 0.001 * 7.21464729309082
Epoch 330, val loss: 0.7668294906616211
Epoch 340, training loss: 0.17083346843719482 = 0.16361767053604126 + 0.001 * 7.2158026695251465
Epoch 340, val loss: 0.7671558856964111
Epoch 350, training loss: 0.14922893047332764 = 0.14201217889785767 + 0.001 * 7.216745376586914
Epoch 350, val loss: 0.7689998149871826
Epoch 360, training loss: 0.13035200536251068 = 0.12313397228717804 + 0.001 * 7.218036651611328
Epoch 360, val loss: 0.7722654342651367
Epoch 370, training loss: 0.11414121091365814 = 0.10692182183265686 + 0.001 * 7.219388961791992
Epoch 370, val loss: 0.7772296071052551
Epoch 380, training loss: 0.10031425207853317 = 0.09309400618076324 + 0.001 * 7.220244884490967
Epoch 380, val loss: 0.78349769115448
Epoch 390, training loss: 0.08852875232696533 = 0.0813080370426178 + 0.001 * 7.2207136154174805
Epoch 390, val loss: 0.7908845543861389
Epoch 400, training loss: 0.07846706360578537 = 0.0712445005774498 + 0.001 * 7.222565174102783
Epoch 400, val loss: 0.7990789413452148
Epoch 410, training loss: 0.06985756009817123 = 0.0626356452703476 + 0.001 * 7.221911907196045
Epoch 410, val loss: 0.8078331351280212
Epoch 420, training loss: 0.06247830390930176 = 0.05525600537657738 + 0.001 * 7.222296714782715
Epoch 420, val loss: 0.8170134425163269
Epoch 430, training loss: 0.05613989755511284 = 0.048917606472969055 + 0.001 * 7.222290515899658
Epoch 430, val loss: 0.826418936252594
Epoch 440, training loss: 0.05067751556634903 = 0.04345529153943062 + 0.001 * 7.222224712371826
Epoch 440, val loss: 0.8359199166297913
Epoch 450, training loss: 0.045954156666994095 = 0.038731612265110016 + 0.001 * 7.222545623779297
Epoch 450, val loss: 0.845467746257782
Epoch 460, training loss: 0.04186418280005455 = 0.034642353653907776 + 0.001 * 7.221829414367676
Epoch 460, val loss: 0.8549635410308838
Epoch 470, training loss: 0.0383215956389904 = 0.031099537387490273 + 0.001 * 7.2220587730407715
Epoch 470, val loss: 0.8644281625747681
Epoch 480, training loss: 0.03526075556874275 = 0.02802778221666813 + 0.001 * 7.232973575592041
Epoch 480, val loss: 0.8737403154373169
Epoch 490, training loss: 0.0325799398124218 = 0.0253594983369112 + 0.001 * 7.22044038772583
Epoch 490, val loss: 0.8829579949378967
Epoch 500, training loss: 0.030253376811742783 = 0.023036273196339607 + 0.001 * 7.217103958129883
Epoch 500, val loss: 0.8919785022735596
Epoch 510, training loss: 0.028223102912306786 = 0.02100757509469986 + 0.001 * 7.215527534484863
Epoch 510, val loss: 0.9008040428161621
Epoch 520, training loss: 0.02644963189959526 = 0.01923021674156189 + 0.001 * 7.219415664672852
Epoch 520, val loss: 0.9093981385231018
Epoch 530, training loss: 0.024885710328817368 = 0.017667226493358612 + 0.001 * 7.218482971191406
Epoch 530, val loss: 0.9178059697151184
Epoch 540, training loss: 0.02349700778722763 = 0.016287723556160927 + 0.001 * 7.20928430557251
Epoch 540, val loss: 0.9259940385818481
Epoch 550, training loss: 0.022273913025856018 = 0.015065163373947144 + 0.001 * 7.2087483406066895
Epoch 550, val loss: 0.9339460730552673
Epoch 560, training loss: 0.02117966301739216 = 0.013976133428514004 + 0.001 * 7.203528881072998
Epoch 560, val loss: 0.9416995644569397
Epoch 570, training loss: 0.020205765962600708 = 0.012998961843550205 + 0.001 * 7.2068047523498535
Epoch 570, val loss: 0.9492389559745789
Epoch 580, training loss: 0.0193278007209301 = 0.012116610072553158 + 0.001 * 7.211189270019531
Epoch 580, val loss: 0.95659339427948
Epoch 590, training loss: 0.018523937091231346 = 0.011316872201859951 + 0.001 * 7.207064151763916
Epoch 590, val loss: 0.9637981057167053
Epoch 600, training loss: 0.017769798636436462 = 0.010590614750981331 + 0.001 * 7.179184436798096
Epoch 600, val loss: 0.9708366394042969
Epoch 610, training loss: 0.01712266355752945 = 0.009930169209837914 + 0.001 * 7.192493915557861
Epoch 610, val loss: 0.9777092933654785
Epoch 620, training loss: 0.016503913328051567 = 0.00932876393198967 + 0.001 * 7.175149440765381
Epoch 620, val loss: 0.9844041466712952
Epoch 630, training loss: 0.015953097492456436 = 0.008780275471508503 + 0.001 * 7.17282247543335
Epoch 630, val loss: 0.9909409284591675
Epoch 640, training loss: 0.015496155247092247 = 0.0082792267203331 + 0.001 * 7.216928005218506
Epoch 640, val loss: 0.9973389506340027
Epoch 650, training loss: 0.014996321871876717 = 0.007820927537977695 + 0.001 * 7.175394535064697
Epoch 650, val loss: 1.003522276878357
Epoch 660, training loss: 0.014551570639014244 = 0.007400750648230314 + 0.001 * 7.150820255279541
Epoch 660, val loss: 1.0095921754837036
Epoch 670, training loss: 0.014161056838929653 = 0.007014820352196693 + 0.001 * 7.146235942840576
Epoch 670, val loss: 1.0155010223388672
Epoch 680, training loss: 0.01380886323750019 = 0.006659732665866613 + 0.001 * 7.149130344390869
Epoch 680, val loss: 1.0212514400482178
Epoch 690, training loss: 0.013469014316797256 = 0.00633240444585681 + 0.001 * 7.1366095542907715
Epoch 690, val loss: 1.0268316268920898
Epoch 700, training loss: 0.013172173872590065 = 0.006029987707734108 + 0.001 * 7.142186164855957
Epoch 700, val loss: 1.0322855710983276
Epoch 710, training loss: 0.012879841029644012 = 0.0057501704432070255 + 0.001 * 7.129669666290283
Epoch 710, val loss: 1.037596583366394
Epoch 720, training loss: 0.012744245119392872 = 0.005490746349096298 + 0.001 * 7.253498554229736
Epoch 720, val loss: 1.0427451133728027
Epoch 730, training loss: 0.012412060052156448 = 0.00524989003315568 + 0.001 * 7.162169933319092
Epoch 730, val loss: 1.0478085279464722
Epoch 740, training loss: 0.012137356214225292 = 0.005025850143283606 + 0.001 * 7.111505508422852
Epoch 740, val loss: 1.05272376537323
Epoch 750, training loss: 0.011926647275686264 = 0.0048170797526836395 + 0.001 * 7.109566688537598
Epoch 750, val loss: 1.0575159788131714
Epoch 760, training loss: 0.011731396429240704 = 0.00462221959605813 + 0.001 * 7.1091766357421875
Epoch 760, val loss: 1.062197208404541
Epoch 770, training loss: 0.011615756899118423 = 0.004440042655915022 + 0.001 * 7.175714015960693
Epoch 770, val loss: 1.0667369365692139
Epoch 780, training loss: 0.011386347934603691 = 0.004269558470696211 + 0.001 * 7.1167893409729
Epoch 780, val loss: 1.0712034702301025
Epoch 790, training loss: 0.01125628873705864 = 0.004109750501811504 + 0.001 * 7.146537780761719
Epoch 790, val loss: 1.0755146741867065
Epoch 800, training loss: 0.01106991060078144 = 0.003959754481911659 + 0.001 * 7.11015510559082
Epoch 800, val loss: 1.0798002481460571
Epoch 810, training loss: 0.010923966765403748 = 0.003818809986114502 + 0.001 * 7.105156898498535
Epoch 810, val loss: 1.0838963985443115
Epoch 820, training loss: 0.010776042938232422 = 0.0036861873231828213 + 0.001 * 7.089855194091797
Epoch 820, val loss: 1.08794105052948
Epoch 830, training loss: 0.01064867153763771 = 0.0035612096544355154 + 0.001 * 7.087461948394775
Epoch 830, val loss: 1.0918757915496826
Epoch 840, training loss: 0.010530797764658928 = 0.003443374764174223 + 0.001 * 7.087423324584961
Epoch 840, val loss: 1.0957151651382446
Epoch 850, training loss: 0.010424206964671612 = 0.0033320181537419558 + 0.001 * 7.092188358306885
Epoch 850, val loss: 1.0994805097579956
Epoch 860, training loss: 0.010314171202480793 = 0.0032267842907458544 + 0.001 * 7.087386131286621
Epoch 860, val loss: 1.1031537055969238
Epoch 870, training loss: 0.010238484479486942 = 0.003127171192318201 + 0.001 * 7.1113128662109375
Epoch 870, val loss: 1.106749415397644
Epoch 880, training loss: 0.010104704648256302 = 0.003032844513654709 + 0.001 * 7.071859359741211
Epoch 880, val loss: 1.1102021932601929
Epoch 890, training loss: 0.010039216838777065 = 0.0029434303287416697 + 0.001 * 7.0957865715026855
Epoch 890, val loss: 1.113657832145691
Epoch 900, training loss: 0.009952422231435776 = 0.0028585861437022686 + 0.001 * 7.093836307525635
Epoch 900, val loss: 1.1169803142547607
Epoch 910, training loss: 0.009857484139502048 = 0.002777958521619439 + 0.001 * 7.079524993896484
Epoch 910, val loss: 1.1202441453933716
Epoch 920, training loss: 0.009799281135201454 = 0.002701307414099574 + 0.001 * 7.097973346710205
Epoch 920, val loss: 1.1234370470046997
Epoch 930, training loss: 0.009682076051831245 = 0.0026283676270395517 + 0.001 * 7.053708076477051
Epoch 930, val loss: 1.1265567541122437
Epoch 940, training loss: 0.009624980390071869 = 0.0025589270517230034 + 0.001 * 7.0660529136657715
Epoch 940, val loss: 1.1296354532241821
Epoch 950, training loss: 0.009548250585794449 = 0.0024926806800067425 + 0.001 * 7.055570125579834
Epoch 950, val loss: 1.1326202154159546
Epoch 960, training loss: 0.00951752346009016 = 0.002429450862109661 + 0.001 * 7.088072299957275
Epoch 960, val loss: 1.1355836391448975
Epoch 970, training loss: 0.009441879577934742 = 0.0023689987137913704 + 0.001 * 7.072880744934082
Epoch 970, val loss: 1.138434886932373
Epoch 980, training loss: 0.009359374642372131 = 0.0023110490292310715 + 0.001 * 7.0483245849609375
Epoch 980, val loss: 1.1412842273712158
Epoch 990, training loss: 0.009296000935137272 = 0.002255322178825736 + 0.001 * 7.040678024291992
Epoch 990, val loss: 1.1440598964691162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8476541908276226
=== training gcn model ===
Epoch 0, training loss: 1.963058590888977 = 1.954461693763733 + 0.001 * 8.596863746643066
Epoch 0, val loss: 1.9504601955413818
Epoch 10, training loss: 1.9527803659439087 = 1.944183588027954 + 0.001 * 8.596822738647461
Epoch 10, val loss: 1.9400569200515747
Epoch 20, training loss: 1.940305233001709 = 1.931708574295044 + 0.001 * 8.596692085266113
Epoch 20, val loss: 1.9274615049362183
Epoch 30, training loss: 1.9229525327682495 = 1.9143562316894531 + 0.001 * 8.596359252929688
Epoch 30, val loss: 1.9098429679870605
Epoch 40, training loss: 1.8974922895431519 = 1.8888968229293823 + 0.001 * 8.595463752746582
Epoch 40, val loss: 1.8840705156326294
Epoch 50, training loss: 1.8613247871398926 = 1.8527321815490723 + 0.001 * 8.592635154724121
Epoch 50, val loss: 1.8486806154251099
Epoch 60, training loss: 1.8177160024642944 = 1.8091342449188232 + 0.001 * 8.581792831420898
Epoch 60, val loss: 1.8094456195831299
Epoch 70, training loss: 1.7777115106582642 = 1.7691726684570312 + 0.001 * 8.538862228393555
Epoch 70, val loss: 1.7775179147720337
Epoch 80, training loss: 1.7342106103897095 = 1.7258816957473755 + 0.001 * 8.328877449035645
Epoch 80, val loss: 1.7407147884368896
Epoch 90, training loss: 1.674531102180481 = 1.6663957834243774 + 0.001 * 8.135284423828125
Epoch 90, val loss: 1.6882399320602417
Epoch 100, training loss: 1.5939029455184937 = 1.5860096216201782 + 0.001 * 7.893286228179932
Epoch 100, val loss: 1.6180824041366577
Epoch 110, training loss: 1.4920018911361694 = 1.4844200611114502 + 0.001 * 7.581864356994629
Epoch 110, val loss: 1.5313888788223267
Epoch 120, training loss: 1.3753043413162231 = 1.3678632974624634 + 0.001 * 7.441064357757568
Epoch 120, val loss: 1.433302402496338
Epoch 130, training loss: 1.2520978450775146 = 1.2446750402450562 + 0.001 * 7.422850131988525
Epoch 130, val loss: 1.3317080736160278
Epoch 140, training loss: 1.1290119886398315 = 1.1216048002243042 + 0.001 * 7.407225608825684
Epoch 140, val loss: 1.2315292358398438
Epoch 150, training loss: 1.0122642517089844 = 1.004870891571045 + 0.001 * 7.393301486968994
Epoch 150, val loss: 1.1381150484085083
Epoch 160, training loss: 0.9074439406394958 = 0.9000555276870728 + 0.001 * 7.3883891105651855
Epoch 160, val loss: 1.0553685426712036
Epoch 170, training loss: 0.8178235292434692 = 0.8104372024536133 + 0.001 * 7.386301517486572
Epoch 170, val loss: 0.9863232374191284
Epoch 180, training loss: 0.7435850501060486 = 0.736200749874115 + 0.001 * 7.3843231201171875
Epoch 180, val loss: 0.9313245415687561
Epoch 190, training loss: 0.6823104023933411 = 0.6749286651611328 + 0.001 * 7.381720542907715
Epoch 190, val loss: 0.8889304995536804
Epoch 200, training loss: 0.6300587058067322 = 0.6226807832717896 + 0.001 * 7.3779497146606445
Epoch 200, val loss: 0.8556894063949585
Epoch 210, training loss: 0.58277827501297 = 0.5754064917564392 + 0.001 * 7.371775150299072
Epoch 210, val loss: 0.8278499841690063
Epoch 220, training loss: 0.5372532606124878 = 0.5298924446105957 + 0.001 * 7.360827445983887
Epoch 220, val loss: 0.8026406764984131
Epoch 230, training loss: 0.491609662771225 = 0.48426884412765503 + 0.001 * 7.340822219848633
Epoch 230, val loss: 0.7785691022872925
Epoch 240, training loss: 0.4453754127025604 = 0.4380697011947632 + 0.001 * 7.305723190307617
Epoch 240, val loss: 0.755515456199646
Epoch 250, training loss: 0.39926066994667053 = 0.39199501276016235 + 0.001 * 7.2656660079956055
Epoch 250, val loss: 0.7339206337928772
Epoch 260, training loss: 0.35457298159599304 = 0.3473273813724518 + 0.001 * 7.245593547821045
Epoch 260, val loss: 0.7144334316253662
Epoch 270, training loss: 0.31253305077552795 = 0.30529913306236267 + 0.001 * 7.233932018280029
Epoch 270, val loss: 0.6975710391998291
Epoch 280, training loss: 0.27387645840644836 = 0.26664412021636963 + 0.001 * 7.232345104217529
Epoch 280, val loss: 0.6835463643074036
Epoch 290, training loss: 0.2388887107372284 = 0.2316557914018631 + 0.001 * 7.232914447784424
Epoch 290, val loss: 0.6725437641143799
Epoch 300, training loss: 0.20766934752464294 = 0.20043699443340302 + 0.001 * 7.232345104217529
Epoch 300, val loss: 0.6646410226821899
Epoch 310, training loss: 0.18024827539920807 = 0.17301754653453827 + 0.001 * 7.230725288391113
Epoch 310, val loss: 0.6598227620124817
Epoch 320, training loss: 0.15654465556144714 = 0.1493166983127594 + 0.001 * 7.227950572967529
Epoch 320, val loss: 0.6579301953315735
Epoch 330, training loss: 0.13632021844387054 = 0.12909647822380066 + 0.001 * 7.223734378814697
Epoch 330, val loss: 0.6586858034133911
Epoch 340, training loss: 0.11921263486146927 = 0.11197991669178009 + 0.001 * 7.2327189445495605
Epoch 340, val loss: 0.6617039442062378
Epoch 350, training loss: 0.1047397255897522 = 0.09752258658409119 + 0.001 * 7.217135906219482
Epoch 350, val loss: 0.666590690612793
Epoch 360, training loss: 0.09249995648860931 = 0.0852888897061348 + 0.001 * 7.211065769195557
Epoch 360, val loss: 0.6729879379272461
Epoch 370, training loss: 0.08209940046072006 = 0.0748961940407753 + 0.001 * 7.203204154968262
Epoch 370, val loss: 0.6805558204650879
Epoch 380, training loss: 0.07322623580694199 = 0.0660317987203598 + 0.001 * 7.194436550140381
Epoch 380, val loss: 0.6890468597412109
Epoch 390, training loss: 0.06563360244035721 = 0.058448102325201035 + 0.001 * 7.185502052307129
Epoch 390, val loss: 0.6982307434082031
Epoch 400, training loss: 0.059123408049345016 = 0.05194266140460968 + 0.001 * 7.180746078491211
Epoch 400, val loss: 0.7079164385795593
Epoch 410, training loss: 0.053525201976299286 = 0.046351734548807144 + 0.001 * 7.173468112945557
Epoch 410, val loss: 0.7179286479949951
Epoch 420, training loss: 0.04871288314461708 = 0.04153721034526825 + 0.001 * 7.175673484802246
Epoch 420, val loss: 0.7281057834625244
Epoch 430, training loss: 0.04455135762691498 = 0.03738084062933922 + 0.001 * 7.1705145835876465
Epoch 430, val loss: 0.738329291343689
Epoch 440, training loss: 0.040947720408439636 = 0.0337810143828392 + 0.001 * 7.166703701019287
Epoch 440, val loss: 0.7484934329986572
Epoch 450, training loss: 0.03782062605023384 = 0.030652189627289772 + 0.001 * 7.168435096740723
Epoch 450, val loss: 0.7585530281066895
Epoch 460, training loss: 0.03508547693490982 = 0.02792181633412838 + 0.001 * 7.163660049438477
Epoch 460, val loss: 0.7684062719345093
Epoch 470, training loss: 0.032689131796360016 = 0.02552943117916584 + 0.001 * 7.159702301025391
Epoch 470, val loss: 0.7780483365058899
Epoch 480, training loss: 0.03058740310370922 = 0.02342434599995613 + 0.001 * 7.16305685043335
Epoch 480, val loss: 0.7874628901481628
Epoch 490, training loss: 0.028723474591970444 = 0.021564681082963943 + 0.001 * 7.158792018890381
Epoch 490, val loss: 0.796631395816803
Epoch 500, training loss: 0.027071956545114517 = 0.01991543360054493 + 0.001 * 7.156522274017334
Epoch 500, val loss: 0.8055378794670105
Epoch 510, training loss: 0.02561064437031746 = 0.01844731718301773 + 0.001 * 7.163325786590576
Epoch 510, val loss: 0.8142014145851135
Epoch 520, training loss: 0.02429279498755932 = 0.017135661095380783 + 0.001 * 7.157134056091309
Epoch 520, val loss: 0.8226316571235657
Epoch 530, training loss: 0.023112056776881218 = 0.01595980115234852 + 0.001 * 7.152255058288574
Epoch 530, val loss: 0.830808162689209
Epoch 540, training loss: 0.02205716073513031 = 0.014902130700647831 + 0.001 * 7.155029296875
Epoch 540, val loss: 0.8387416005134583
Epoch 550, training loss: 0.02109980210661888 = 0.013947749510407448 + 0.001 * 7.152052402496338
Epoch 550, val loss: 0.8464382886886597
Epoch 560, training loss: 0.020237356424331665 = 0.013084025122225285 + 0.001 * 7.1533308029174805
Epoch 560, val loss: 0.8539223074913025
Epoch 570, training loss: 0.019451476633548737 = 0.012300078757107258 + 0.001 * 7.151397228240967
Epoch 570, val loss: 0.8611948490142822
Epoch 580, training loss: 0.01873680204153061 = 0.011586555279791355 + 0.001 * 7.1502461433410645
Epoch 580, val loss: 0.8682550191879272
Epoch 590, training loss: 0.01808539591729641 = 0.010935427621006966 + 0.001 * 7.14996862411499
Epoch 590, val loss: 0.8751150369644165
Epoch 600, training loss: 0.017484664916992188 = 0.010339751839637756 + 0.001 * 7.144911766052246
Epoch 600, val loss: 0.8817858695983887
Epoch 610, training loss: 0.01694299280643463 = 0.009793468751013279 + 0.001 * 7.149523735046387
Epoch 610, val loss: 0.8882682919502258
Epoch 620, training loss: 0.016439853236079216 = 0.009291315451264381 + 0.001 * 7.148538112640381
Epoch 620, val loss: 0.894577145576477
Epoch 630, training loss: 0.015969686210155487 = 0.00882872287184 + 0.001 * 7.140964031219482
Epoch 630, val loss: 0.9007081985473633
Epoch 640, training loss: 0.015555830672383308 = 0.0084016602486372 + 0.001 * 7.15416955947876
Epoch 640, val loss: 0.9066736102104187
Epoch 650, training loss: 0.015148518607020378 = 0.00800663884729147 + 0.001 * 7.141879081726074
Epoch 650, val loss: 0.9124820232391357
Epoch 660, training loss: 0.014777535572648048 = 0.0076405759900808334 + 0.001 * 7.136958599090576
Epoch 660, val loss: 0.9181413054466248
Epoch 670, training loss: 0.014437681064009666 = 0.007300706580281258 + 0.001 * 7.136973857879639
Epoch 670, val loss: 0.9236523509025574
Epoch 680, training loss: 0.014132551848888397 = 0.006984595209360123 + 0.001 * 7.147956848144531
Epoch 680, val loss: 0.929022490978241
Epoch 690, training loss: 0.013826293870806694 = 0.006690126843750477 + 0.001 * 7.136166095733643
Epoch 690, val loss: 0.9342496395111084
Epoch 700, training loss: 0.013547460548579693 = 0.006415368989109993 + 0.001 * 7.132091045379639
Epoch 700, val loss: 0.9393540024757385
Epoch 710, training loss: 0.013292016461491585 = 0.006158557720482349 + 0.001 * 7.133458614349365
Epoch 710, val loss: 0.9443284869194031
Epoch 720, training loss: 0.013048935681581497 = 0.005918185226619244 + 0.001 * 7.130749702453613
Epoch 720, val loss: 0.9491875767707825
Epoch 730, training loss: 0.012826280668377876 = 0.0056928955018520355 + 0.001 * 7.1333842277526855
Epoch 730, val loss: 0.9539294242858887
Epoch 740, training loss: 0.012610536068677902 = 0.005481433589011431 + 0.001 * 7.1291022300720215
Epoch 740, val loss: 0.9585621953010559
Epoch 750, training loss: 0.012410362251102924 = 0.005282674916088581 + 0.001 * 7.127686977386475
Epoch 750, val loss: 0.9630779027938843
Epoch 760, training loss: 0.012225354090332985 = 0.005095671862363815 + 0.001 * 7.1296820640563965
Epoch 760, val loss: 0.9674907326698303
Epoch 770, training loss: 0.012073650024831295 = 0.004919487051665783 + 0.001 * 7.154162406921387
Epoch 770, val loss: 0.9718012809753418
Epoch 780, training loss: 0.01188418734818697 = 0.004753300454467535 + 0.001 * 7.130886554718018
Epoch 780, val loss: 0.9760082364082336
Epoch 790, training loss: 0.011722883209586143 = 0.004596383310854435 + 0.001 * 7.126500129699707
Epoch 790, val loss: 0.9801204204559326
Epoch 800, training loss: 0.011570245027542114 = 0.004448019899427891 + 0.001 * 7.1222243309021
Epoch 800, val loss: 0.9841368794441223
Epoch 810, training loss: 0.011430356651544571 = 0.0043076337315142155 + 0.001 * 7.122722148895264
Epoch 810, val loss: 0.9880685210227966
Epoch 820, training loss: 0.011305687949061394 = 0.004174631554633379 + 0.001 * 7.131056308746338
Epoch 820, val loss: 0.9919049143791199
Epoch 830, training loss: 0.011170458048582077 = 0.004048505797982216 + 0.001 * 7.121952056884766
Epoch 830, val loss: 0.9956663846969604
Epoch 840, training loss: 0.011045996099710464 = 0.003928797785192728 + 0.001 * 7.117197513580322
Epoch 840, val loss: 0.9993462562561035
Epoch 850, training loss: 0.010937919840216637 = 0.0038150744512677193 + 0.001 * 7.122844696044922
Epoch 850, val loss: 1.0029492378234863
Epoch 860, training loss: 0.010818391107022762 = 0.003706942545250058 + 0.001 * 7.111448287963867
Epoch 860, val loss: 1.006471872329712
Epoch 870, training loss: 0.010722226463258266 = 0.003604020457714796 + 0.001 * 7.118205547332764
Epoch 870, val loss: 1.0099269151687622
Epoch 880, training loss: 0.01062213908880949 = 0.003505925415083766 + 0.001 * 7.116213798522949
Epoch 880, val loss: 1.013309121131897
Epoch 890, training loss: 0.010562941431999207 = 0.0034122331999242306 + 0.001 * 7.150707244873047
Epoch 890, val loss: 1.0166420936584473
Epoch 900, training loss: 0.01043346431106329 = 0.003322429722175002 + 0.001 * 7.111034393310547
Epoch 900, val loss: 1.0199289321899414
Epoch 910, training loss: 0.010346625931560993 = 0.0032357845921069384 + 0.001 * 7.110841274261475
Epoch 910, val loss: 1.023215413093567
Epoch 920, training loss: 0.010257929563522339 = 0.0031515660230070353 + 0.001 * 7.106362819671631
Epoch 920, val loss: 1.0265034437179565
Epoch 930, training loss: 0.0101739801466465 = 0.0030693276785314083 + 0.001 * 7.104651927947998
Epoch 930, val loss: 1.0297880172729492
Epoch 940, training loss: 0.010103367269039154 = 0.002988845109939575 + 0.001 * 7.114521503448486
Epoch 940, val loss: 1.0330641269683838
Epoch 950, training loss: 0.010011790320277214 = 0.0029102524276822805 + 0.001 * 7.101537704467773
Epoch 950, val loss: 1.036280632019043
Epoch 960, training loss: 0.009940549731254578 = 0.00283368700183928 + 0.001 * 7.1068620681762695
Epoch 960, val loss: 1.0394748449325562
Epoch 970, training loss: 0.009861399419605732 = 0.0027593099512159824 + 0.001 * 7.102088928222656
Epoch 970, val loss: 1.0426206588745117
Epoch 980, training loss: 0.009786400012671947 = 0.002687357133254409 + 0.001 * 7.0990424156188965
Epoch 980, val loss: 1.045709252357483
Epoch 990, training loss: 0.009717699140310287 = 0.0026179125998169184 + 0.001 * 7.099785804748535
Epoch 990, val loss: 1.0487403869628906
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 1.9678606986999512 = 1.9592639207839966 + 0.001 * 8.596817970275879
Epoch 0, val loss: 1.971160650253296
Epoch 10, training loss: 1.9576165676116943 = 1.9490197896957397 + 0.001 * 8.596748352050781
Epoch 10, val loss: 1.9610570669174194
Epoch 20, training loss: 1.9446890354156494 = 1.936092495918274 + 0.001 * 8.596532821655273
Epoch 20, val loss: 1.947927713394165
Epoch 30, training loss: 1.9264780282974243 = 1.9178820848464966 + 0.001 * 8.59598159790039
Epoch 30, val loss: 1.9290518760681152
Epoch 40, training loss: 1.8998711109161377 = 1.891276478767395 + 0.001 * 8.594572067260742
Epoch 40, val loss: 1.9015384912490845
Epoch 50, training loss: 1.8626106977462769 = 1.854020357131958 + 0.001 * 8.590381622314453
Epoch 50, val loss: 1.8642222881317139
Epoch 60, training loss: 1.819435715675354 = 1.8108603954315186 + 0.001 * 8.575261116027832
Epoch 60, val loss: 1.8238548040390015
Epoch 70, training loss: 1.781538724899292 = 1.7730239629745483 + 0.001 * 8.514814376831055
Epoch 70, val loss: 1.7898430824279785
Epoch 80, training loss: 1.7390944957733154 = 1.7308601140975952 + 0.001 * 8.234336853027344
Epoch 80, val loss: 1.7494287490844727
Epoch 90, training loss: 1.6800897121429443 = 1.6720287799835205 + 0.001 * 8.060933113098145
Epoch 90, val loss: 1.6954456567764282
Epoch 100, training loss: 1.5990312099456787 = 1.5911697149276733 + 0.001 * 7.861457824707031
Epoch 100, val loss: 1.6237603425979614
Epoch 110, training loss: 1.4983586072921753 = 1.4907244443893433 + 0.001 * 7.634145259857178
Epoch 110, val loss: 1.5370360612869263
Epoch 120, training loss: 1.3888862133026123 = 1.3815044164657593 + 0.001 * 7.38185453414917
Epoch 120, val loss: 1.4452896118164062
Epoch 130, training loss: 1.2793784141540527 = 1.272017240524292 + 0.001 * 7.361213207244873
Epoch 130, val loss: 1.3554491996765137
Epoch 140, training loss: 1.1720619201660156 = 1.1647077798843384 + 0.001 * 7.3541388511657715
Epoch 140, val loss: 1.268915057182312
Epoch 150, training loss: 1.0688259601593018 = 1.0614843368530273 + 0.001 * 7.341597080230713
Epoch 150, val loss: 1.1861810684204102
Epoch 160, training loss: 0.9718363881111145 = 0.9645019173622131 + 0.001 * 7.334468364715576
Epoch 160, val loss: 1.1088006496429443
Epoch 170, training loss: 0.8825950622558594 = 0.8752647638320923 + 0.001 * 7.330314636230469
Epoch 170, val loss: 1.0369044542312622
Epoch 180, training loss: 0.8022292256355286 = 0.7949039340019226 + 0.001 * 7.325295448303223
Epoch 180, val loss: 0.97240149974823
Epoch 190, training loss: 0.7312559485435486 = 0.7239386439323425 + 0.001 * 7.317293167114258
Epoch 190, val loss: 0.9161719083786011
Epoch 200, training loss: 0.6688870787620544 = 0.6615818738937378 + 0.001 * 7.305201053619385
Epoch 200, val loss: 0.8689960837364197
Epoch 210, training loss: 0.6127438545227051 = 0.6054563522338867 + 0.001 * 7.287521839141846
Epoch 210, val loss: 0.829427182674408
Epoch 220, training loss: 0.5600053071975708 = 0.5527430176734924 + 0.001 * 7.26231575012207
Epoch 220, val loss: 0.7954539060592651
Epoch 230, training loss: 0.5087622404098511 = 0.5015289187431335 + 0.001 * 7.2333478927612305
Epoch 230, val loss: 0.765190601348877
Epoch 240, training loss: 0.45838919281959534 = 0.451179176568985 + 0.001 * 7.210014343261719
Epoch 240, val loss: 0.7378684282302856
Epoch 250, training loss: 0.40907955169677734 = 0.40188169479370117 + 0.001 * 7.197868347167969
Epoch 250, val loss: 0.7129939198493958
Epoch 260, training loss: 0.3614778220653534 = 0.3542855381965637 + 0.001 * 7.192277431488037
Epoch 260, val loss: 0.6905812621116638
Epoch 270, training loss: 0.3164520561695099 = 0.30925998091697693 + 0.001 * 7.192082405090332
Epoch 270, val loss: 0.6706507205963135
Epoch 280, training loss: 0.2749776244163513 = 0.26778364181518555 + 0.001 * 7.193994045257568
Epoch 280, val loss: 0.6536590456962585
Epoch 290, training loss: 0.23803676664829254 = 0.23084093630313873 + 0.001 * 7.19582462310791
Epoch 290, val loss: 0.6402151584625244
Epoch 300, training loss: 0.20602770149707794 = 0.19882997870445251 + 0.001 * 7.197728633880615
Epoch 300, val loss: 0.6306653618812561
Epoch 310, training loss: 0.17884686589241028 = 0.17164745926856995 + 0.001 * 7.19940185546875
Epoch 310, val loss: 0.6250494122505188
Epoch 320, training loss: 0.15597642958164215 = 0.14877550303936005 + 0.001 * 7.200931549072266
Epoch 320, val loss: 0.6228447556495667
Epoch 330, training loss: 0.13674725592136383 = 0.12954489886760712 + 0.001 * 7.202361106872559
Epoch 330, val loss: 0.6236798763275146
Epoch 340, training loss: 0.120513416826725 = 0.11330972611904144 + 0.001 * 7.203688144683838
Epoch 340, val loss: 0.626984179019928
Epoch 350, training loss: 0.10672822594642639 = 0.09952333569526672 + 0.001 * 7.204890251159668
Epoch 350, val loss: 0.6323569416999817
Epoch 360, training loss: 0.09495186060667038 = 0.08774591982364655 + 0.001 * 7.2059407234191895
Epoch 360, val loss: 0.6394034624099731
Epoch 370, training loss: 0.08483319729566574 = 0.07762625068426132 + 0.001 * 7.206943035125732
Epoch 370, val loss: 0.6478080153465271
Epoch 380, training loss: 0.07609044015407562 = 0.06888257712125778 + 0.001 * 7.207861423492432
Epoch 380, val loss: 0.6572694182395935
Epoch 390, training loss: 0.06849512457847595 = 0.0612870454788208 + 0.001 * 7.208076000213623
Epoch 390, val loss: 0.6675947904586792
Epoch 400, training loss: 0.06187247484922409 = 0.05466414988040924 + 0.001 * 7.208324909210205
Epoch 400, val loss: 0.6785207390785217
Epoch 410, training loss: 0.0560891218483448 = 0.04888032749295235 + 0.001 * 7.208793640136719
Epoch 410, val loss: 0.6898864507675171
Epoch 420, training loss: 0.05103473365306854 = 0.04382595419883728 + 0.001 * 7.208777904510498
Epoch 420, val loss: 0.701495885848999
Epoch 430, training loss: 0.046619582921266556 = 0.039408110082149506 + 0.001 * 7.211471080780029
Epoch 430, val loss: 0.7132704854011536
Epoch 440, training loss: 0.04275447502732277 = 0.035546038299798965 + 0.001 * 7.208437442779541
Epoch 440, val loss: 0.7250550389289856
Epoch 450, training loss: 0.039373740553855896 = 0.032167948782444 + 0.001 * 7.20579195022583
Epoch 450, val loss: 0.7367696166038513
Epoch 460, training loss: 0.036417897790670395 = 0.029209712520241737 + 0.001 * 7.208185195922852
Epoch 460, val loss: 0.7483153939247131
Epoch 470, training loss: 0.03381594270467758 = 0.026614468544721603 + 0.001 * 7.20147180557251
Epoch 470, val loss: 0.7596408128738403
Epoch 480, training loss: 0.03153134509921074 = 0.024332493543624878 + 0.001 * 7.198850154876709
Epoch 480, val loss: 0.7706806659698486
Epoch 490, training loss: 0.02952832169830799 = 0.0223203357309103 + 0.001 * 7.207985877990723
Epoch 490, val loss: 0.7814456224441528
Epoch 500, training loss: 0.027733683586120605 = 0.02054056152701378 + 0.001 * 7.193121433258057
Epoch 500, val loss: 0.7918895483016968
Epoch 510, training loss: 0.026151541620492935 = 0.018961457535624504 + 0.001 * 7.1900835037231445
Epoch 510, val loss: 0.8020220994949341
Epoch 520, training loss: 0.024738602340221405 = 0.017555639147758484 + 0.001 * 7.1829633712768555
Epoch 520, val loss: 0.8118586540222168
Epoch 530, training loss: 0.02348059043288231 = 0.01629992201924324 + 0.001 * 7.180668354034424
Epoch 530, val loss: 0.821374773979187
Epoch 540, training loss: 0.022359654307365417 = 0.01517465803772211 + 0.001 * 7.184995174407959
Epoch 540, val loss: 0.8306061625480652
Epoch 550, training loss: 0.02132527343928814 = 0.014163047075271606 + 0.001 * 7.16222620010376
Epoch 550, val loss: 0.8395169973373413
Epoch 560, training loss: 0.02043277770280838 = 0.013250692747533321 + 0.001 * 7.1820855140686035
Epoch 560, val loss: 0.8481577038764954
Epoch 570, training loss: 0.01957372948527336 = 0.012425442226231098 + 0.001 * 7.148285865783691
Epoch 570, val loss: 0.8565187454223633
Epoch 580, training loss: 0.01881575956940651 = 0.011676798574626446 + 0.001 * 7.1389617919921875
Epoch 580, val loss: 0.8646059036254883
Epoch 590, training loss: 0.018197791650891304 = 0.010995805263519287 + 0.001 * 7.2019853591918945
Epoch 590, val loss: 0.8724489212036133
Epoch 600, training loss: 0.017525689676404 = 0.010374710895121098 + 0.001 * 7.150979042053223
Epoch 600, val loss: 0.8800147771835327
Epoch 610, training loss: 0.016935953870415688 = 0.009806727059185505 + 0.001 * 7.129226207733154
Epoch 610, val loss: 0.8873815536499023
Epoch 620, training loss: 0.016418494284152985 = 0.00928611122071743 + 0.001 * 7.132383823394775
Epoch 620, val loss: 0.8945208191871643
Epoch 630, training loss: 0.01593072898685932 = 0.00880773551762104 + 0.001 * 7.122993469238281
Epoch 630, val loss: 0.9014197587966919
Epoch 640, training loss: 0.015485831536352634 = 0.008367124944925308 + 0.001 * 7.118706226348877
Epoch 640, val loss: 0.9081339240074158
Epoch 650, training loss: 0.015099028125405312 = 0.007960564456880093 + 0.001 * 7.138463497161865
Epoch 650, val loss: 0.9146450161933899
Epoch 660, training loss: 0.01468566246330738 = 0.007584600243717432 + 0.001 * 7.101062297821045
Epoch 660, val loss: 0.9209773540496826
Epoch 670, training loss: 0.014351624995470047 = 0.00723633449524641 + 0.001 * 7.115290641784668
Epoch 670, val loss: 0.9271256327629089
Epoch 680, training loss: 0.01400320790708065 = 0.006913053337484598 + 0.001 * 7.090153694152832
Epoch 680, val loss: 0.933090329170227
Epoch 690, training loss: 0.013717785477638245 = 0.006612412631511688 + 0.001 * 7.105371952056885
Epoch 690, val loss: 0.9388989210128784
Epoch 700, training loss: 0.01343083567917347 = 0.006332441233098507 + 0.001 * 7.098393440246582
Epoch 700, val loss: 0.9445207118988037
Epoch 710, training loss: 0.013171899132430553 = 0.0060712783597409725 + 0.001 * 7.100620269775391
Epoch 710, val loss: 0.9500162601470947
Epoch 720, training loss: 0.012899531051516533 = 0.005827181972563267 + 0.001 * 7.0723490715026855
Epoch 720, val loss: 0.9553526639938354
Epoch 730, training loss: 0.012710729613900185 = 0.005598811432719231 + 0.001 * 7.111917972564697
Epoch 730, val loss: 0.9605488181114197
Epoch 740, training loss: 0.012467090040445328 = 0.005384788382798433 + 0.001 * 7.082300662994385
Epoch 740, val loss: 0.9655972123146057
Epoch 750, training loss: 0.012265607714653015 = 0.005183958914130926 + 0.001 * 7.081648826599121
Epoch 750, val loss: 0.9705379009246826
Epoch 760, training loss: 0.01205778494477272 = 0.004995201714336872 + 0.001 * 7.0625834465026855
Epoch 760, val loss: 0.9753314256668091
Epoch 770, training loss: 0.01191122829914093 = 0.004817637614905834 + 0.001 * 7.093589782714844
Epoch 770, val loss: 0.9800262451171875
Epoch 780, training loss: 0.011715784668922424 = 0.0046503557823598385 + 0.001 * 7.065428256988525
Epoch 780, val loss: 0.9845656752586365
Epoch 790, training loss: 0.011554338969290257 = 0.004492621868848801 + 0.001 * 7.061716556549072
Epoch 790, val loss: 0.989041268825531
Epoch 800, training loss: 0.011438056826591492 = 0.004343647509813309 + 0.001 * 7.094408988952637
Epoch 800, val loss: 0.9933798313140869
Epoch 810, training loss: 0.011267627589404583 = 0.004202814772725105 + 0.001 * 7.064812660217285
Epoch 810, val loss: 0.9976276755332947
Epoch 820, training loss: 0.011138413101434708 = 0.004069351591169834 + 0.001 * 7.069060802459717
Epoch 820, val loss: 1.0017591714859009
Epoch 830, training loss: 0.01100239623337984 = 0.003942652139812708 + 0.001 * 7.059743881225586
Epoch 830, val loss: 1.0058174133300781
Epoch 840, training loss: 0.010894600301980972 = 0.0038220700807869434 + 0.001 * 7.072529315948486
Epoch 840, val loss: 1.0097764730453491
Epoch 850, training loss: 0.0107597466558218 = 0.0037066792137920856 + 0.001 * 7.053067684173584
Epoch 850, val loss: 1.013685703277588
Epoch 860, training loss: 0.01066918857395649 = 0.0035959749948233366 + 0.001 * 7.07321310043335
Epoch 860, val loss: 1.0175715684890747
Epoch 870, training loss: 0.010517771355807781 = 0.0034893155097961426 + 0.001 * 7.02845573425293
Epoch 870, val loss: 1.0214115381240845
Epoch 880, training loss: 0.010408326983451843 = 0.003386488649994135 + 0.001 * 7.0218377113342285
Epoch 880, val loss: 1.0252519845962524
Epoch 890, training loss: 0.010329853743314743 = 0.003287264611572027 + 0.001 * 7.04258918762207
Epoch 890, val loss: 1.0290858745574951
Epoch 900, training loss: 0.010233957320451736 = 0.0031915931031107903 + 0.001 * 7.042364120483398
Epoch 900, val loss: 1.0328881740570068
Epoch 910, training loss: 0.010120883584022522 = 0.0030993581749498844 + 0.001 * 7.021524429321289
Epoch 910, val loss: 1.0367051362991333
Epoch 920, training loss: 0.010136749595403671 = 0.003010435961186886 + 0.001 * 7.126313209533691
Epoch 920, val loss: 1.0405055284500122
Epoch 930, training loss: 0.009945337660610676 = 0.0029250544030219316 + 0.001 * 7.020282745361328
Epoch 930, val loss: 1.044264554977417
Epoch 940, training loss: 0.0098817627876997 = 0.0028429378289729357 + 0.001 * 7.038825035095215
Epoch 940, val loss: 1.0480159521102905
Epoch 950, training loss: 0.009775958023965359 = 0.0027640846092253923 + 0.001 * 7.0118727684021
Epoch 950, val loss: 1.0517016649246216
Epoch 960, training loss: 0.00968458503484726 = 0.0026884921826422215 + 0.001 * 6.996092796325684
Epoch 960, val loss: 1.0553748607635498
Epoch 970, training loss: 0.009633839130401611 = 0.002616012468934059 + 0.001 * 7.017826080322266
Epoch 970, val loss: 1.0589995384216309
Epoch 980, training loss: 0.0095523027703166 = 0.0025466280058026314 + 0.001 * 7.005674362182617
Epoch 980, val loss: 1.062538981437683
Epoch 990, training loss: 0.009483875706791878 = 0.0024802191182971 + 0.001 * 7.003655910491943
Epoch 990, val loss: 1.0660665035247803
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8518518518518519
0.8376383763837639
The final CL Acc:0.82469, 0.01944, The final GNN Acc:0.84238, 0.00411
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10534])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9710739850997925 = 1.9624770879745483 + 0.001 * 8.59685230255127
Epoch 0, val loss: 1.9672152996063232
Epoch 10, training loss: 1.9606350660324097 = 1.952038288116455 + 0.001 * 8.596785545349121
Epoch 10, val loss: 1.9561591148376465
Epoch 20, training loss: 1.9475101232528687 = 1.9389135837554932 + 0.001 * 8.596570014953613
Epoch 20, val loss: 1.94195556640625
Epoch 30, training loss: 1.9290746450424194 = 1.9204785823822021 + 0.001 * 8.596041679382324
Epoch 30, val loss: 1.9220246076583862
Epoch 40, training loss: 1.902079701423645 = 1.8934849500656128 + 0.001 * 8.594695091247559
Epoch 40, val loss: 1.893397331237793
Epoch 50, training loss: 1.865310788154602 = 1.8567200899124146 + 0.001 * 8.590670585632324
Epoch 50, val loss: 1.8564406633377075
Epoch 60, training loss: 1.8267217874526978 = 1.818145751953125 + 0.001 * 8.57601261138916
Epoch 60, val loss: 1.822399616241455
Epoch 70, training loss: 1.8005644083023071 = 1.7920557260513306 + 0.001 * 8.508662223815918
Epoch 70, val loss: 1.803285837173462
Epoch 80, training loss: 1.7722197771072388 = 1.7640540599822998 + 0.001 * 8.165735244750977
Epoch 80, val loss: 1.7789667844772339
Epoch 90, training loss: 1.7324435710906982 = 1.7243677377700806 + 0.001 * 8.075773239135742
Epoch 90, val loss: 1.7451642751693726
Epoch 100, training loss: 1.6754339933395386 = 1.6674518585205078 + 0.001 * 7.982146739959717
Epoch 100, val loss: 1.6978694200515747
Epoch 110, training loss: 1.5998046398162842 = 1.5919028520584106 + 0.001 * 7.901739120483398
Epoch 110, val loss: 1.634970784187317
Epoch 120, training loss: 1.5175260305404663 = 1.5097665786743164 + 0.001 * 7.759495735168457
Epoch 120, val loss: 1.5683107376098633
Epoch 130, training loss: 1.439784288406372 = 1.4323424100875854 + 0.001 * 7.441882133483887
Epoch 130, val loss: 1.5075794458389282
Epoch 140, training loss: 1.3666878938674927 = 1.3593388795852661 + 0.001 * 7.349058151245117
Epoch 140, val loss: 1.4546452760696411
Epoch 150, training loss: 1.29417085647583 = 1.2868496179580688 + 0.001 * 7.321229934692383
Epoch 150, val loss: 1.404402494430542
Epoch 160, training loss: 1.2202930450439453 = 1.2129853963851929 + 0.001 * 7.307602405548096
Epoch 160, val loss: 1.3532918691635132
Epoch 170, training loss: 1.1449131965637207 = 1.1376166343688965 + 0.001 * 7.296566009521484
Epoch 170, val loss: 1.3013055324554443
Epoch 180, training loss: 1.0674139261245728 = 1.0601210594177246 + 0.001 * 7.292843341827393
Epoch 180, val loss: 1.2461901903152466
Epoch 190, training loss: 0.9867143034934998 = 0.9794224500656128 + 0.001 * 7.291836738586426
Epoch 190, val loss: 1.1862914562225342
Epoch 200, training loss: 0.9029200673103333 = 0.8956285715103149 + 0.001 * 7.291487216949463
Epoch 200, val loss: 1.1220005750656128
Epoch 210, training loss: 0.8180022835731506 = 0.8107118010520935 + 0.001 * 7.29047966003418
Epoch 210, val loss: 1.0562297105789185
Epoch 220, training loss: 0.7355414628982544 = 0.728253185749054 + 0.001 * 7.288277626037598
Epoch 220, val loss: 0.9937698841094971
Epoch 230, training loss: 0.6593806743621826 = 0.6520954370498657 + 0.001 * 7.285216808319092
Epoch 230, val loss: 0.939115583896637
Epoch 240, training loss: 0.5919298529624939 = 0.5846478939056396 + 0.001 * 7.281952381134033
Epoch 240, val loss: 0.8947773575782776
Epoch 250, training loss: 0.5333236455917358 = 0.526044487953186 + 0.001 * 7.279172897338867
Epoch 250, val loss: 0.8602985143661499
Epoch 260, training loss: 0.48183825612068176 = 0.47456130385398865 + 0.001 * 7.276937484741211
Epoch 260, val loss: 0.8334257006645203
Epoch 270, training loss: 0.43501558899879456 = 0.4277404546737671 + 0.001 * 7.275132179260254
Epoch 270, val loss: 0.811352550983429
Epoch 280, training loss: 0.39078330993652344 = 0.38350966572761536 + 0.001 * 7.273640155792236
Epoch 280, val loss: 0.792238175868988
Epoch 290, training loss: 0.3479425609111786 = 0.34067007899284363 + 0.001 * 7.272496223449707
Epoch 290, val loss: 0.77511066198349
Epoch 300, training loss: 0.30644625425338745 = 0.2991742491722107 + 0.001 * 7.272005081176758
Epoch 300, val loss: 0.7598276138305664
Epoch 310, training loss: 0.2672162353992462 = 0.2599448561668396 + 0.001 * 7.271383762359619
Epoch 310, val loss: 0.7471874952316284
Epoch 320, training loss: 0.23153850436210632 = 0.224266916513443 + 0.001 * 7.2715911865234375
Epoch 320, val loss: 0.7377574443817139
Epoch 330, training loss: 0.20029625296592712 = 0.19302445650100708 + 0.001 * 7.271788597106934
Epoch 330, val loss: 0.7319297790527344
Epoch 340, training loss: 0.1736210584640503 = 0.16634894907474518 + 0.001 * 7.27210807800293
Epoch 340, val loss: 0.7296827435493469
Epoch 350, training loss: 0.15109479427337646 = 0.14382337033748627 + 0.001 * 7.271423816680908
Epoch 350, val loss: 0.7308642864227295
Epoch 360, training loss: 0.13208797574043274 = 0.12481803447008133 + 0.001 * 7.269935607910156
Epoch 360, val loss: 0.735175371170044
Epoch 370, training loss: 0.11599665135145187 = 0.10873007774353027 + 0.001 * 7.266576290130615
Epoch 370, val loss: 0.7421631813049316
Epoch 380, training loss: 0.10233622044324875 = 0.09507058560848236 + 0.001 * 7.265636444091797
Epoch 380, val loss: 0.7512953281402588
Epoch 390, training loss: 0.09069954603910446 = 0.08344706892967224 + 0.001 * 7.252474308013916
Epoch 390, val loss: 0.7621323466300964
Epoch 400, training loss: 0.0807737484574318 = 0.07352600991725922 + 0.001 * 7.247735023498535
Epoch 400, val loss: 0.7742631435394287
Epoch 410, training loss: 0.07226096093654633 = 0.06502439081668854 + 0.001 * 7.236570358276367
Epoch 410, val loss: 0.7873435616493225
Epoch 420, training loss: 0.06491464376449585 = 0.05770491436123848 + 0.001 * 7.209731578826904
Epoch 420, val loss: 0.8010739684104919
Epoch 430, training loss: 0.058573972433805466 = 0.0513775572180748 + 0.001 * 7.196415901184082
Epoch 430, val loss: 0.8152710795402527
Epoch 440, training loss: 0.053071290254592896 = 0.045890651643276215 + 0.001 * 7.180639266967773
Epoch 440, val loss: 0.8297338485717773
Epoch 450, training loss: 0.04830761253833771 = 0.041121695190668106 + 0.001 * 7.185918807983398
Epoch 450, val loss: 0.8442952036857605
Epoch 460, training loss: 0.04412906989455223 = 0.0369696319103241 + 0.001 * 7.15943717956543
Epoch 460, val loss: 0.8588032722473145
Epoch 470, training loss: 0.04051899164915085 = 0.03334968537092209 + 0.001 * 7.169307231903076
Epoch 470, val loss: 0.873109757900238
Epoch 480, training loss: 0.03735940903425217 = 0.030187388882040977 + 0.001 * 7.172019958496094
Epoch 480, val loss: 0.8871518969535828
Epoch 490, training loss: 0.034569233655929565 = 0.02741418033838272 + 0.001 * 7.155054092407227
Epoch 490, val loss: 0.9008817672729492
Epoch 500, training loss: 0.032137077301740646 = 0.024973271414637566 + 0.001 * 7.163806438446045
Epoch 500, val loss: 0.914297342300415
Epoch 510, training loss: 0.029964692890644073 = 0.02281818352639675 + 0.001 * 7.146508693695068
Epoch 510, val loss: 0.9273391962051392
Epoch 520, training loss: 0.028044376522302628 = 0.02091047540307045 + 0.001 * 7.133901596069336
Epoch 520, val loss: 0.9400334358215332
Epoch 530, training loss: 0.026355884969234467 = 0.019218148663640022 + 0.001 * 7.137736797332764
Epoch 530, val loss: 0.952323317527771
Epoch 540, training loss: 0.02485916018486023 = 0.01771366596221924 + 0.001 * 7.14549446105957
Epoch 540, val loss: 0.9642323851585388
Epoch 550, training loss: 0.023501494899392128 = 0.016373036429286003 + 0.001 * 7.128457546234131
Epoch 550, val loss: 0.9757281541824341
Epoch 560, training loss: 0.02230146899819374 = 0.015175306238234043 + 0.001 * 7.126162528991699
Epoch 560, val loss: 0.9868541359901428
Epoch 570, training loss: 0.021232135593891144 = 0.014102371409535408 + 0.001 * 7.129763603210449
Epoch 570, val loss: 0.9975961446762085
Epoch 580, training loss: 0.020308028906583786 = 0.013138619251549244 + 0.001 * 7.169408321380615
Epoch 580, val loss: 1.007987141609192
Epoch 590, training loss: 0.019396880641579628 = 0.012270684354007244 + 0.001 * 7.126195907592773
Epoch 590, val loss: 1.018036961555481
Epoch 600, training loss: 0.01860782876610756 = 0.011486933566629887 + 0.001 * 7.120894432067871
Epoch 600, val loss: 1.0277553796768188
Epoch 610, training loss: 0.017885670065879822 = 0.010777202434837818 + 0.001 * 7.108466148376465
Epoch 610, val loss: 1.0371499061584473
Epoch 620, training loss: 0.017237994819879532 = 0.010132845491170883 + 0.001 * 7.105149745941162
Epoch 620, val loss: 1.0462448596954346
Epoch 630, training loss: 0.016672808676958084 = 0.009546344168484211 + 0.001 * 7.126463413238525
Epoch 630, val loss: 1.0550577640533447
Epoch 640, training loss: 0.01611417718231678 = 0.009011207148432732 + 0.001 * 7.102970123291016
Epoch 640, val loss: 1.063575029373169
Epoch 650, training loss: 0.015621732920408249 = 0.00852170493453741 + 0.001 * 7.100027561187744
Epoch 650, val loss: 1.071824073791504
Epoch 660, training loss: 0.01516889501363039 = 0.0080728679895401 + 0.001 * 7.09602689743042
Epoch 660, val loss: 1.0798381567001343
Epoch 670, training loss: 0.014774048700928688 = 0.007660474628210068 + 0.001 * 7.113574028015137
Epoch 670, val loss: 1.0876113176345825
Epoch 680, training loss: 0.014376340433955193 = 0.007280740886926651 + 0.001 * 7.09559965133667
Epoch 680, val loss: 1.095158576965332
Epoch 690, training loss: 0.01402653381228447 = 0.00693031819537282 + 0.001 * 7.09621524810791
Epoch 690, val loss: 1.1024733781814575
Epoch 700, training loss: 0.013691671192646027 = 0.0066062877885997295 + 0.001 * 7.08538293838501
Epoch 700, val loss: 1.1095918416976929
Epoch 710, training loss: 0.013390771113336086 = 0.006306052673608065 + 0.001 * 7.084718227386475
Epoch 710, val loss: 1.1165133714675903
Epoch 720, training loss: 0.013116542249917984 = 0.006027379538863897 + 0.001 * 7.089162826538086
Epoch 720, val loss: 1.1232349872589111
Epoch 730, training loss: 0.012861812487244606 = 0.005768225062638521 + 0.001 * 7.093587398529053
Epoch 730, val loss: 1.12978196144104
Epoch 740, training loss: 0.012619907036423683 = 0.005526791326701641 + 0.001 * 7.093115329742432
Epoch 740, val loss: 1.136157512664795
Epoch 750, training loss: 0.012388508766889572 = 0.0053013586439192295 + 0.001 * 7.087149620056152
Epoch 750, val loss: 1.1423652172088623
Epoch 760, training loss: 0.012181183323264122 = 0.005090334452688694 + 0.001 * 7.090848922729492
Epoch 760, val loss: 1.1484386920928955
Epoch 770, training loss: 0.011981820687651634 = 0.0048918710090219975 + 0.001 * 7.089949131011963
Epoch 770, val loss: 1.1543898582458496
Epoch 780, training loss: 0.01179883349686861 = 0.004704190883785486 + 0.001 * 7.094642162322998
Epoch 780, val loss: 1.1602673530578613
Epoch 790, training loss: 0.011612679809331894 = 0.004525930155068636 + 0.001 * 7.08674955368042
Epoch 790, val loss: 1.1660935878753662
Epoch 800, training loss: 0.011448743753135204 = 0.00435612304136157 + 0.001 * 7.092620372772217
Epoch 800, val loss: 1.1718840599060059
Epoch 810, training loss: 0.011273102834820747 = 0.0041943760588765144 + 0.001 * 7.078725814819336
Epoch 810, val loss: 1.1776052713394165
Epoch 820, training loss: 0.011100969277322292 = 0.004040449857711792 + 0.001 * 7.060519218444824
Epoch 820, val loss: 1.183288812637329
Epoch 830, training loss: 0.010967647656798363 = 0.003894141409546137 + 0.001 * 7.073505878448486
Epoch 830, val loss: 1.188899040222168
Epoch 840, training loss: 0.010832550935447216 = 0.0037552511785179377 + 0.001 * 7.07729959487915
Epoch 840, val loss: 1.1944218873977661
Epoch 850, training loss: 0.01068707462400198 = 0.0036235267762094736 + 0.001 * 7.063547611236572
Epoch 850, val loss: 1.1998542547225952
Epoch 860, training loss: 0.010560077615082264 = 0.0034986522514373064 + 0.001 * 7.06142520904541
Epoch 860, val loss: 1.2051903009414673
Epoch 870, training loss: 0.010429438203573227 = 0.003380330977961421 + 0.001 * 7.049107074737549
Epoch 870, val loss: 1.2104108333587646
Epoch 880, training loss: 0.010325229726731777 = 0.0032682258170098066 + 0.001 * 7.057003974914551
Epoch 880, val loss: 1.2155404090881348
Epoch 890, training loss: 0.01023155264556408 = 0.0031620317604392767 + 0.001 * 7.069520473480225
Epoch 890, val loss: 1.2205524444580078
Epoch 900, training loss: 0.01014997623860836 = 0.0030614612624049187 + 0.001 * 7.088515281677246
Epoch 900, val loss: 1.2254743576049805
Epoch 910, training loss: 0.010025220923125744 = 0.0029661436565220356 + 0.001 * 7.05907678604126
Epoch 910, val loss: 1.230270266532898
Epoch 920, training loss: 0.009948424994945526 = 0.00287571270018816 + 0.001 * 7.072712421417236
Epoch 920, val loss: 1.2349830865859985
Epoch 930, training loss: 0.009848266839981079 = 0.0027899034321308136 + 0.001 * 7.05836296081543
Epoch 930, val loss: 1.2395968437194824
Epoch 940, training loss: 0.009768889285624027 = 0.0027084120083600283 + 0.001 * 7.060477256774902
Epoch 940, val loss: 1.244113802909851
Epoch 950, training loss: 0.009669878520071507 = 0.0026310377288609743 + 0.001 * 7.038840293884277
Epoch 950, val loss: 1.248532772064209
Epoch 960, training loss: 0.009615173563361168 = 0.0025574727915227413 + 0.001 * 7.0577006340026855
Epoch 960, val loss: 1.2528960704803467
Epoch 970, training loss: 0.009531770832836628 = 0.0024875050876289606 + 0.001 * 7.044265270233154
Epoch 970, val loss: 1.2571415901184082
Epoch 980, training loss: 0.009471118450164795 = 0.00242091016843915 + 0.001 * 7.050208568572998
Epoch 980, val loss: 1.2613199949264526
Epoch 990, training loss: 0.009385216981172562 = 0.0023574905935674906 + 0.001 * 7.027725696563721
Epoch 990, val loss: 1.2653902769088745
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 1.9744038581848145 = 1.9658069610595703 + 0.001 * 8.596857070922852
Epoch 0, val loss: 1.966074824333191
Epoch 10, training loss: 1.963661551475525 = 1.9550647735595703 + 0.001 * 8.596817016601562
Epoch 10, val loss: 1.9548627138137817
Epoch 20, training loss: 1.9507060050964355 = 1.9421093463897705 + 0.001 * 8.596668243408203
Epoch 20, val loss: 1.9414291381835938
Epoch 30, training loss: 1.933165192604065 = 1.9245688915252686 + 0.001 * 8.596321105957031
Epoch 30, val loss: 1.9232523441314697
Epoch 40, training loss: 1.9079883098602295 = 1.89939284324646 + 0.001 * 8.59552001953125
Epoch 40, val loss: 1.8972581624984741
Epoch 50, training loss: 1.8723715543746948 = 1.8637781143188477 + 0.001 * 8.59338092803955
Epoch 50, val loss: 1.8616732358932495
Epoch 60, training loss: 1.829000473022461 = 1.820414423942566 + 0.001 * 8.58607006072998
Epoch 60, val loss: 1.822044849395752
Epoch 70, training loss: 1.789543867111206 = 1.7809878587722778 + 0.001 * 8.556023597717285
Epoch 70, val loss: 1.7917133569717407
Epoch 80, training loss: 1.7508625984191895 = 1.7424951791763306 + 0.001 * 8.36739730834961
Epoch 80, val loss: 1.7613812685012817
Epoch 90, training loss: 1.6985334157943726 = 1.6905725002288818 + 0.001 * 7.960878372192383
Epoch 90, val loss: 1.7152314186096191
Epoch 100, training loss: 1.6274968385696411 = 1.6197932958602905 + 0.001 * 7.7035813331604
Epoch 100, val loss: 1.6524109840393066
Epoch 110, training loss: 1.5333764553070068 = 1.5257524251937866 + 0.001 * 7.624032020568848
Epoch 110, val loss: 1.5727860927581787
Epoch 120, training loss: 1.4195127487182617 = 1.4119501113891602 + 0.001 * 7.562631607055664
Epoch 120, val loss: 1.4757378101348877
Epoch 130, training loss: 1.2964757680892944 = 1.2889556884765625 + 0.001 * 7.52008056640625
Epoch 130, val loss: 1.372717261314392
Epoch 140, training loss: 1.1764291524887085 = 1.1689692735671997 + 0.001 * 7.459863185882568
Epoch 140, val loss: 1.273195743560791
Epoch 150, training loss: 1.0663564205169678 = 1.058980107307434 + 0.001 * 7.376280784606934
Epoch 150, val loss: 1.184802770614624
Epoch 160, training loss: 0.9679309725761414 = 0.9605879783630371 + 0.001 * 7.342978000640869
Epoch 160, val loss: 1.1087796688079834
Epoch 170, training loss: 0.8813413381576538 = 0.8739968538284302 + 0.001 * 7.344505786895752
Epoch 170, val loss: 1.0448946952819824
Epoch 180, training loss: 0.8068078756332397 = 0.7994694709777832 + 0.001 * 7.338397026062012
Epoch 180, val loss: 0.9928125739097595
Epoch 190, training loss: 0.7435421347618103 = 0.7362053394317627 + 0.001 * 7.336817264556885
Epoch 190, val loss: 0.9514713883399963
Epoch 200, training loss: 0.6890941858291626 = 0.6817620396614075 + 0.001 * 7.332130432128906
Epoch 200, val loss: 0.9188726544380188
Epoch 210, training loss: 0.6403841376304626 = 0.6330583095550537 + 0.001 * 7.325819969177246
Epoch 210, val loss: 0.8924318552017212
Epoch 220, training loss: 0.5948759913444519 = 0.5875585675239563 + 0.001 * 7.317397117614746
Epoch 220, val loss: 0.8698344230651855
Epoch 230, training loss: 0.5509393215179443 = 0.5436342358589172 + 0.001 * 7.30505895614624
Epoch 230, val loss: 0.8496167659759521
Epoch 240, training loss: 0.5078569650650024 = 0.5005689859390259 + 0.001 * 7.287997245788574
Epoch 240, val loss: 0.8312953114509583
Epoch 250, training loss: 0.46553030610084534 = 0.4582609236240387 + 0.001 * 7.269383907318115
Epoch 250, val loss: 0.815268337726593
Epoch 260, training loss: 0.4242880046367645 = 0.41704440116882324 + 0.001 * 7.243588924407959
Epoch 260, val loss: 0.8022266030311584
Epoch 270, training loss: 0.38474562764167786 = 0.3775244951248169 + 0.001 * 7.2211384773254395
Epoch 270, val loss: 0.7930135130882263
Epoch 280, training loss: 0.3474651575088501 = 0.3402559757232666 + 0.001 * 7.209194183349609
Epoch 280, val loss: 0.7880829572677612
Epoch 290, training loss: 0.3127886950969696 = 0.3055913746356964 + 0.001 * 7.197315216064453
Epoch 290, val loss: 0.7876071929931641
Epoch 300, training loss: 0.2808855175971985 = 0.27369359135627747 + 0.001 * 7.191926956176758
Epoch 300, val loss: 0.7910789251327515
Epoch 310, training loss: 0.25177180767059326 = 0.2445787936449051 + 0.001 * 7.193024635314941
Epoch 310, val loss: 0.7979072332382202
Epoch 320, training loss: 0.22532813251018524 = 0.2181355357170105 + 0.001 * 7.192590236663818
Epoch 320, val loss: 0.8075879216194153
Epoch 330, training loss: 0.20136773586273193 = 0.19417782127857208 + 0.001 * 7.189913749694824
Epoch 330, val loss: 0.8196147680282593
Epoch 340, training loss: 0.1796833574771881 = 0.1724957376718521 + 0.001 * 7.187625408172607
Epoch 340, val loss: 0.8334823250770569
Epoch 350, training loss: 0.16006922721862793 = 0.1528853476047516 + 0.001 * 7.183876991271973
Epoch 350, val loss: 0.8487308025360107
Epoch 360, training loss: 0.14237885177135468 = 0.13519695401191711 + 0.001 * 7.1818928718566895
Epoch 360, val loss: 0.8649915456771851
Epoch 370, training loss: 0.12650564312934875 = 0.11931294202804565 + 0.001 * 7.192704200744629
Epoch 370, val loss: 0.8820701837539673
Epoch 380, training loss: 0.11229954659938812 = 0.10512324422597885 + 0.001 * 7.176300525665283
Epoch 380, val loss: 0.8996457457542419
Epoch 390, training loss: 0.09970484673976898 = 0.0925317332148552 + 0.001 * 7.173109531402588
Epoch 390, val loss: 0.9175264239311218
Epoch 400, training loss: 0.08860848098993301 = 0.08143861591815948 + 0.001 * 7.169864177703857
Epoch 400, val loss: 0.9355553984642029
Epoch 410, training loss: 0.07890988141298294 = 0.07174570858478546 + 0.001 * 7.164170742034912
Epoch 410, val loss: 0.9535682797431946
Epoch 420, training loss: 0.07049915194511414 = 0.06333829462528229 + 0.001 * 7.160858631134033
Epoch 420, val loss: 0.9713219404220581
Epoch 430, training loss: 0.06324818730354309 = 0.05608205124735832 + 0.001 * 7.166138172149658
Epoch 430, val loss: 0.9888009428977966
Epoch 440, training loss: 0.05698798969388008 = 0.04983488470315933 + 0.001 * 7.153103828430176
Epoch 440, val loss: 1.0058637857437134
Epoch 450, training loss: 0.05162710323929787 = 0.04445778951048851 + 0.001 * 7.169312953948975
Epoch 450, val loss: 1.0224511623382568
Epoch 460, training loss: 0.046964745968580246 = 0.03982529789209366 + 0.001 * 7.139446258544922
Epoch 460, val loss: 1.0384862422943115
Epoch 470, training loss: 0.042981892824172974 = 0.0358266606926918 + 0.001 * 7.155231475830078
Epoch 470, val loss: 1.053911805152893
Epoch 480, training loss: 0.03950083255767822 = 0.03236453980207443 + 0.001 * 7.136291027069092
Epoch 480, val loss: 1.0687259435653687
Epoch 490, training loss: 0.03650103881955147 = 0.029357269406318665 + 0.001 * 7.143770694732666
Epoch 490, val loss: 1.0829260349273682
Epoch 500, training loss: 0.03386421874165535 = 0.026734797284007072 + 0.001 * 7.129422664642334
Epoch 500, val loss: 1.096543312072754
Epoch 510, training loss: 0.03155837208032608 = 0.024439215660095215 + 0.001 * 7.119154930114746
Epoch 510, val loss: 1.1096032857894897
Epoch 520, training loss: 0.029541343450546265 = 0.022421926259994507 + 0.001 * 7.119415760040283
Epoch 520, val loss: 1.1221343278884888
Epoch 530, training loss: 0.027748245745897293 = 0.02064264565706253 + 0.001 * 7.1055989265441895
Epoch 530, val loss: 1.1341533660888672
Epoch 540, training loss: 0.026179099455475807 = 0.01906677894294262 + 0.001 * 7.112320423126221
Epoch 540, val loss: 1.1457021236419678
Epoch 550, training loss: 0.02477950043976307 = 0.017665758728981018 + 0.001 * 7.113741874694824
Epoch 550, val loss: 1.1567968130111694
Epoch 560, training loss: 0.02351284772157669 = 0.016415543854236603 + 0.001 * 7.09730339050293
Epoch 560, val loss: 1.1674294471740723
Epoch 570, training loss: 0.022395912557840347 = 0.015294332057237625 + 0.001 * 7.101579666137695
Epoch 570, val loss: 1.1776856184005737
Epoch 580, training loss: 0.021376140415668488 = 0.01428082212805748 + 0.001 * 7.095317840576172
Epoch 580, val loss: 1.187648057937622
Epoch 590, training loss: 0.020449329167604446 = 0.013357263058423996 + 0.001 * 7.092066287994385
Epoch 590, val loss: 1.1973124742507935
Epoch 600, training loss: 0.019602064043283463 = 0.012512723915278912 + 0.001 * 7.0893402099609375
Epoch 600, val loss: 1.2067004442214966
Epoch 610, training loss: 0.018826762214303017 = 0.011739789508283138 + 0.001 * 7.086972236633301
Epoch 610, val loss: 1.2158567905426025
Epoch 620, training loss: 0.018116993829607964 = 0.011032242327928543 + 0.001 * 7.084751605987549
Epoch 620, val loss: 1.2247674465179443
Epoch 630, training loss: 0.01747424155473709 = 0.010384404100477695 + 0.001 * 7.089837074279785
Epoch 630, val loss: 1.2334126234054565
Epoch 640, training loss: 0.016892269253730774 = 0.009790861047804356 + 0.001 * 7.101408004760742
Epoch 640, val loss: 1.24178946018219
Epoch 650, training loss: 0.016321899369359016 = 0.009246578440070152 + 0.001 * 7.075320720672607
Epoch 650, val loss: 1.2499550580978394
Epoch 660, training loss: 0.015839237719774246 = 0.00874677300453186 + 0.001 * 7.092463970184326
Epoch 660, val loss: 1.2578734159469604
Epoch 670, training loss: 0.015360288321971893 = 0.008287177421152592 + 0.001 * 7.073110103607178
Epoch 670, val loss: 1.2655410766601562
Epoch 680, training loss: 0.014944097958505154 = 0.007863983511924744 + 0.001 * 7.080113887786865
Epoch 680, val loss: 1.27297842502594
Epoch 690, training loss: 0.014558706432580948 = 0.007473652251064777 + 0.001 * 7.085054397583008
Epoch 690, val loss: 1.280187964439392
Epoch 700, training loss: 0.014207681640982628 = 0.007113015744835138 + 0.001 * 7.094666004180908
Epoch 700, val loss: 1.2871811389923096
Epoch 710, training loss: 0.013852040283381939 = 0.006779170129448175 + 0.001 * 7.072869777679443
Epoch 710, val loss: 1.2939711809158325
Epoch 720, training loss: 0.013552794232964516 = 0.0064696320332586765 + 0.001 * 7.0831618309021
Epoch 720, val loss: 1.3005567789077759
Epoch 730, training loss: 0.013250802643597126 = 0.006182207725942135 + 0.001 * 7.068594455718994
Epoch 730, val loss: 1.306931734085083
Epoch 740, training loss: 0.012981639243662357 = 0.005914965178817511 + 0.001 * 7.066673755645752
Epoch 740, val loss: 1.3131577968597412
Epoch 750, training loss: 0.012745045125484467 = 0.005666007287800312 + 0.001 * 7.079038143157959
Epoch 750, val loss: 1.319175124168396
Epoch 760, training loss: 0.012488903477787971 = 0.005433807149529457 + 0.001 * 7.055095672607422
Epoch 760, val loss: 1.325028419494629
Epoch 770, training loss: 0.012278180569410324 = 0.005216874182224274 + 0.001 * 7.061305999755859
Epoch 770, val loss: 1.330725908279419
Epoch 780, training loss: 0.012103326618671417 = 0.00501394085586071 + 0.001 * 7.089385032653809
Epoch 780, val loss: 1.33625066280365
Epoch 790, training loss: 0.011875385418534279 = 0.004823863040655851 + 0.001 * 7.051522731781006
Epoch 790, val loss: 1.341607689857483
Epoch 800, training loss: 0.011697757989168167 = 0.004645585082471371 + 0.001 * 7.052173137664795
Epoch 800, val loss: 1.3468314409255981
Epoch 810, training loss: 0.011521810665726662 = 0.004478090908378363 + 0.001 * 7.043719291687012
Epoch 810, val loss: 1.3519338369369507
Epoch 820, training loss: 0.011395375244319439 = 0.004320531617850065 + 0.001 * 7.074843406677246
Epoch 820, val loss: 1.3568816184997559
Epoch 830, training loss: 0.011229855939745903 = 0.004172119777649641 + 0.001 * 7.057735443115234
Epoch 830, val loss: 1.361678123474121
Epoch 840, training loss: 0.011086381040513515 = 0.004032202530652285 + 0.001 * 7.054178237915039
Epoch 840, val loss: 1.366357445716858
Epoch 850, training loss: 0.010931083001196384 = 0.003900128183886409 + 0.001 * 7.030954837799072
Epoch 850, val loss: 1.3709046840667725
Epoch 860, training loss: 0.010826696641743183 = 0.003775322576984763 + 0.001 * 7.0513739585876465
Epoch 860, val loss: 1.375403642654419
Epoch 870, training loss: 0.0107044642791152 = 0.003657268825918436 + 0.001 * 7.047194957733154
Epoch 870, val loss: 1.379730224609375
Epoch 880, training loss: 0.010605955496430397 = 0.0035454838071018457 + 0.001 * 7.060471534729004
Epoch 880, val loss: 1.3839495182037354
Epoch 890, training loss: 0.01048924308270216 = 0.0034395556431263685 + 0.001 * 7.049686908721924
Epoch 890, val loss: 1.38809335231781
Epoch 900, training loss: 0.010369986295700073 = 0.003339036600664258 + 0.001 * 7.030949592590332
Epoch 900, val loss: 1.3920503854751587
Epoch 910, training loss: 0.010292903520166874 = 0.003243560204282403 + 0.001 * 7.049343109130859
Epoch 910, val loss: 1.3960105180740356
Epoch 920, training loss: 0.010194174014031887 = 0.003152811899781227 + 0.001 * 7.0413618087768555
Epoch 920, val loss: 1.3998262882232666
Epoch 930, training loss: 0.010090217925608158 = 0.0030665236990898848 + 0.001 * 7.023694038391113
Epoch 930, val loss: 1.4035835266113281
Epoch 940, training loss: 0.010013102553784847 = 0.0029844155069440603 + 0.001 * 7.0286865234375
Epoch 940, val loss: 1.4072784185409546
Epoch 950, training loss: 0.00991719402372837 = 0.0029062035027891397 + 0.001 * 7.010990142822266
Epoch 950, val loss: 1.4108262062072754
Epoch 960, training loss: 0.009891211986541748 = 0.0028316136449575424 + 0.001 * 7.059597492218018
Epoch 960, val loss: 1.414337396621704
Epoch 970, training loss: 0.009787289425730705 = 0.002760467352345586 + 0.001 * 7.026821613311768
Epoch 970, val loss: 1.4177367687225342
Epoch 980, training loss: 0.009710523299872875 = 0.002692526439204812 + 0.001 * 7.017996311187744
Epoch 980, val loss: 1.4210338592529297
Epoch 990, training loss: 0.00964171253144741 = 0.002627616748213768 + 0.001 * 7.014095783233643
Epoch 990, val loss: 1.4243252277374268
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 1.9673229455947876 = 1.9587260484695435 + 0.001 * 8.596863746643066
Epoch 0, val loss: 1.9520342350006104
Epoch 10, training loss: 1.9566925764083862 = 1.9480957984924316 + 0.001 * 8.596806526184082
Epoch 10, val loss: 1.94217050075531
Epoch 20, training loss: 1.9431520700454712 = 1.9345554113388062 + 0.001 * 8.596617698669434
Epoch 20, val loss: 1.9294829368591309
Epoch 30, training loss: 1.923780083656311 = 1.9151839017868042 + 0.001 * 8.596141815185547
Epoch 30, val loss: 1.911325454711914
Epoch 40, training loss: 1.8948915004730225 = 1.8862967491149902 + 0.001 * 8.594779968261719
Epoch 40, val loss: 1.884811282157898
Epoch 50, training loss: 1.8552043437957764 = 1.8466142416000366 + 0.001 * 8.590084075927734
Epoch 50, val loss: 1.8508358001708984
Epoch 60, training loss: 1.8145960569381714 = 1.8060259819030762 + 0.001 * 8.57010269165039
Epoch 60, val loss: 1.8207740783691406
Epoch 70, training loss: 1.784902572631836 = 1.7764338254928589 + 0.001 * 8.468741416931152
Epoch 70, val loss: 1.7971559762954712
Epoch 80, training loss: 1.748171329498291 = 1.7400108575820923 + 0.001 * 8.160421371459961
Epoch 80, val loss: 1.7609378099441528
Epoch 90, training loss: 1.6967774629592896 = 1.688689112663269 + 0.001 * 8.088385581970215
Epoch 90, val loss: 1.716070294380188
Epoch 100, training loss: 1.6250615119934082 = 1.6170275211334229 + 0.001 * 8.033931732177734
Epoch 100, val loss: 1.6568843126296997
Epoch 110, training loss: 1.5383129119873047 = 1.5303215980529785 + 0.001 * 7.991273403167725
Epoch 110, val loss: 1.5856258869171143
Epoch 120, training loss: 1.4503810405731201 = 1.442470908164978 + 0.001 * 7.910126686096191
Epoch 120, val loss: 1.51430344581604
Epoch 130, training loss: 1.3676187992095947 = 1.3599162101745605 + 0.001 * 7.702605724334717
Epoch 130, val loss: 1.4486589431762695
Epoch 140, training loss: 1.2877228260040283 = 1.2801052331924438 + 0.001 * 7.617635726928711
Epoch 140, val loss: 1.3868941068649292
Epoch 150, training loss: 1.2069590091705322 = 1.1994125843048096 + 0.001 * 7.546401500701904
Epoch 150, val loss: 1.3255701065063477
Epoch 160, training loss: 1.1243600845336914 = 1.1169027090072632 + 0.001 * 7.457398891448975
Epoch 160, val loss: 1.2639870643615723
Epoch 170, training loss: 1.0414925813674927 = 1.0341122150421143 + 0.001 * 7.380326271057129
Epoch 170, val loss: 1.202971339225769
Epoch 180, training loss: 0.9602813124656677 = 0.9529399871826172 + 0.001 * 7.341297626495361
Epoch 180, val loss: 1.1443421840667725
Epoch 190, training loss: 0.8819743394851685 = 0.8746451735496521 + 0.001 * 7.329157829284668
Epoch 190, val loss: 1.0891309976577759
Epoch 200, training loss: 0.8071699142456055 = 0.7998513579368591 + 0.001 * 7.318528175354004
Epoch 200, val loss: 1.03836989402771
Epoch 210, training loss: 0.7365214228630066 = 0.7292088866233826 + 0.001 * 7.312532424926758
Epoch 210, val loss: 0.9931511878967285
Epoch 220, training loss: 0.6707179546356201 = 0.6634096503257751 + 0.001 * 7.308276176452637
Epoch 220, val loss: 0.9541959166526794
Epoch 230, training loss: 0.610168993473053 = 0.6028661727905273 + 0.001 * 7.302835941314697
Epoch 230, val loss: 0.9218280911445618
Epoch 240, training loss: 0.555357038974762 = 0.5480586886405945 + 0.001 * 7.298375129699707
Epoch 240, val loss: 0.896261990070343
Epoch 250, training loss: 0.506379246711731 = 0.499085932970047 + 0.001 * 7.293321132659912
Epoch 250, val loss: 0.8772174119949341
Epoch 260, training loss: 0.4626920521259308 = 0.45540279150009155 + 0.001 * 7.289259433746338
Epoch 260, val loss: 0.8638206124305725
Epoch 270, training loss: 0.42333996295928955 = 0.4160556495189667 + 0.001 * 7.284322261810303
Epoch 270, val loss: 0.8548808693885803
Epoch 280, training loss: 0.3872952461242676 = 0.38001713156700134 + 0.001 * 7.278122425079346
Epoch 280, val loss: 0.8494828939437866
Epoch 290, training loss: 0.35361191630363464 = 0.34633857011795044 + 0.001 * 7.273333549499512
Epoch 290, val loss: 0.8468881845474243
Epoch 300, training loss: 0.3214971423149109 = 0.3142283856868744 + 0.001 * 7.268743991851807
Epoch 300, val loss: 0.8462134003639221
Epoch 310, training loss: 0.2903355062007904 = 0.2830721437931061 + 0.001 * 7.263370037078857
Epoch 310, val loss: 0.8468021750450134
Epoch 320, training loss: 0.2598002851009369 = 0.2525426149368286 + 0.001 * 7.257665157318115
Epoch 320, val loss: 0.8483725190162659
Epoch 330, training loss: 0.2300015240907669 = 0.22273984551429749 + 0.001 * 7.261682510375977
Epoch 330, val loss: 0.8511524200439453
Epoch 340, training loss: 0.2014698088169098 = 0.19421854615211487 + 0.001 * 7.251255512237549
Epoch 340, val loss: 0.855655312538147
Epoch 350, training loss: 0.17506608366966248 = 0.16781772673130035 + 0.001 * 7.248355388641357
Epoch 350, val loss: 0.86236572265625
Epoch 360, training loss: 0.1514783799648285 = 0.14423315227031708 + 0.001 * 7.24523401260376
Epoch 360, val loss: 0.8717035055160522
Epoch 370, training loss: 0.1309799700975418 = 0.12373704463243484 + 0.001 * 7.242930889129639
Epoch 370, val loss: 0.883603572845459
Epoch 380, training loss: 0.11343666166067123 = 0.1061956062912941 + 0.001 * 7.241052627563477
Epoch 380, val loss: 0.8977007865905762
Epoch 390, training loss: 0.0985829159617424 = 0.09134279191493988 + 0.001 * 7.240121841430664
Epoch 390, val loss: 0.9137458801269531
Epoch 400, training loss: 0.08607296645641327 = 0.07883761078119278 + 0.001 * 7.235358715057373
Epoch 400, val loss: 0.931140124797821
Epoch 410, training loss: 0.07558546960353851 = 0.06833250820636749 + 0.001 * 7.252962589263916
Epoch 410, val loss: 0.9495202898979187
Epoch 420, training loss: 0.06675978004932404 = 0.05952601134777069 + 0.001 * 7.233765602111816
Epoch 420, val loss: 0.9685053825378418
Epoch 430, training loss: 0.05937463417649269 = 0.052145298570394516 + 0.001 * 7.229336738586426
Epoch 430, val loss: 0.9876438975334167
Epoch 440, training loss: 0.05317509174346924 = 0.04594884812831879 + 0.001 * 7.2262420654296875
Epoch 440, val loss: 1.0066670179367065
Epoch 450, training loss: 0.04795149341225624 = 0.04072749242186546 + 0.001 * 7.2239990234375
Epoch 450, val loss: 1.0253173112869263
Epoch 460, training loss: 0.04352762550115585 = 0.036307331174612045 + 0.001 * 7.220292568206787
Epoch 460, val loss: 1.0435097217559814
Epoch 470, training loss: 0.039764661341905594 = 0.03254677727818489 + 0.001 * 7.21788215637207
Epoch 470, val loss: 1.0611132383346558
Epoch 480, training loss: 0.036547109484672546 = 0.029330849647521973 + 0.001 * 7.216258525848389
Epoch 480, val loss: 1.0781105756759644
Epoch 490, training loss: 0.03377838805317879 = 0.026566285640001297 + 0.001 * 7.212102890014648
Epoch 490, val loss: 1.0944828987121582
Epoch 500, training loss: 0.03138962388038635 = 0.024177370592951775 + 0.001 * 7.212254524230957
Epoch 500, val loss: 1.1101722717285156
Epoch 510, training loss: 0.029307739809155464 = 0.022101830691099167 + 0.001 * 7.205908298492432
Epoch 510, val loss: 1.1252317428588867
Epoch 520, training loss: 0.0274973064661026 = 0.020289123058319092 + 0.001 * 7.208183765411377
Epoch 520, val loss: 1.1396946907043457
Epoch 530, training loss: 0.025916974991559982 = 0.01869768276810646 + 0.001 * 7.219292163848877
Epoch 530, val loss: 1.1536215543746948
Epoch 540, training loss: 0.024495432153344154 = 0.017293762415647507 + 0.001 * 7.201668739318848
Epoch 540, val loss: 1.1669585704803467
Epoch 550, training loss: 0.02324502356350422 = 0.016048861667513847 + 0.001 * 7.196161270141602
Epoch 550, val loss: 1.1797822713851929
Epoch 560, training loss: 0.0221566092222929 = 0.014938576146960258 + 0.001 * 7.218032360076904
Epoch 560, val loss: 1.1921480894088745
Epoch 570, training loss: 0.021138262003660202 = 0.013941198587417603 + 0.001 * 7.197063446044922
Epoch 570, val loss: 1.2040470838546753
Epoch 580, training loss: 0.02022632583975792 = 0.01303884107619524 + 0.001 * 7.187483310699463
Epoch 580, val loss: 1.2155964374542236
Epoch 590, training loss: 0.019416747614741325 = 0.012218409217894077 + 0.001 * 7.198338031768799
Epoch 590, val loss: 1.2268903255462646
Epoch 600, training loss: 0.01864708587527275 = 0.011470407247543335 + 0.001 * 7.17667818069458
Epoch 600, val loss: 1.2380032539367676
Epoch 610, training loss: 0.017964418977499008 = 0.010787400417029858 + 0.001 * 7.177018165588379
Epoch 610, val loss: 1.2489134073257446
Epoch 620, training loss: 0.01734844222664833 = 0.01016306597739458 + 0.001 * 7.185375213623047
Epoch 620, val loss: 1.2596062421798706
Epoch 630, training loss: 0.016774186864495277 = 0.009591727517545223 + 0.001 * 7.182458400726318
Epoch 630, val loss: 1.2700941562652588
Epoch 640, training loss: 0.01624785177409649 = 0.00906805694103241 + 0.001 * 7.1797943115234375
Epoch 640, val loss: 1.2803486585617065
Epoch 650, training loss: 0.015747923403978348 = 0.008587410673499107 + 0.001 * 7.160512447357178
Epoch 650, val loss: 1.290367841720581
Epoch 660, training loss: 0.01533706858754158 = 0.008145570755004883 + 0.001 * 7.191496849060059
Epoch 660, val loss: 1.3001554012298584
Epoch 670, training loss: 0.014919126406311989 = 0.007738806307315826 + 0.001 * 7.180319309234619
Epoch 670, val loss: 1.3096994161605835
Epoch 680, training loss: 0.014530438929796219 = 0.007363629061728716 + 0.001 * 7.16680908203125
Epoch 680, val loss: 1.319016456604004
Epoch 690, training loss: 0.014196532778441906 = 0.007016982417553663 + 0.001 * 7.1795501708984375
Epoch 690, val loss: 1.3280997276306152
Epoch 700, training loss: 0.013847503811120987 = 0.006696145050227642 + 0.001 * 7.151358604431152
Epoch 700, val loss: 1.3369789123535156
Epoch 710, training loss: 0.01357809454202652 = 0.006398641038686037 + 0.001 * 7.1794538497924805
Epoch 710, val loss: 1.3456100225448608
Epoch 720, training loss: 0.013266715221107006 = 0.006122336722910404 + 0.001 * 7.144378185272217
Epoch 720, val loss: 1.354030966758728
Epoch 730, training loss: 0.013017754070460796 = 0.005865282844752073 + 0.001 * 7.15247106552124
Epoch 730, val loss: 1.3622241020202637
Epoch 740, training loss: 0.012750995345413685 = 0.005625810474157333 + 0.001 * 7.125184535980225
Epoch 740, val loss: 1.3702090978622437
Epoch 750, training loss: 0.012554815039038658 = 0.005402369890362024 + 0.001 * 7.152444362640381
Epoch 750, val loss: 1.3779935836791992
Epoch 760, training loss: 0.01232098788022995 = 0.00519352313131094 + 0.001 * 7.1274638175964355
Epoch 760, val loss: 1.385542869567871
Epoch 770, training loss: 0.012118582613766193 = 0.004998036660254002 + 0.001 * 7.120545387268066
Epoch 770, val loss: 1.3929417133331299
Epoch 780, training loss: 0.011942867189645767 = 0.004814793821424246 + 0.001 * 7.128073215484619
Epoch 780, val loss: 1.4001280069351196
Epoch 790, training loss: 0.011751402169466019 = 0.004642789717763662 + 0.001 * 7.108612537384033
Epoch 790, val loss: 1.4071695804595947
Epoch 800, training loss: 0.011606564745306969 = 0.004481111653149128 + 0.001 * 7.125452995300293
Epoch 800, val loss: 1.41403329372406
Epoch 810, training loss: 0.011477107182145119 = 0.004328893963247538 + 0.001 * 7.148212432861328
Epoch 810, val loss: 1.4207406044006348
Epoch 820, training loss: 0.011329900473356247 = 0.00418546237051487 + 0.001 * 7.144437313079834
Epoch 820, val loss: 1.4272679090499878
Epoch 830, training loss: 0.011157948523759842 = 0.004050123505294323 + 0.001 * 7.107824325561523
Epoch 830, val loss: 1.43362295627594
Epoch 840, training loss: 0.01101480983197689 = 0.003922254778444767 + 0.001 * 7.092554092407227
Epoch 840, val loss: 1.4398462772369385
Epoch 850, training loss: 0.010922116227447987 = 0.0038013034500181675 + 0.001 * 7.12081241607666
Epoch 850, val loss: 1.4459202289581299
Epoch 860, training loss: 0.010828682221472263 = 0.003686795709654689 + 0.001 * 7.141886234283447
Epoch 860, val loss: 1.4518544673919678
Epoch 870, training loss: 0.01073146890848875 = 0.0035782353952527046 + 0.001 * 7.153233051300049
Epoch 870, val loss: 1.45768141746521
Epoch 880, training loss: 0.010610678233206272 = 0.0034752313513308764 + 0.001 * 7.135446548461914
Epoch 880, val loss: 1.4633378982543945
Epoch 890, training loss: 0.010485192760825157 = 0.0033774450421333313 + 0.001 * 7.107747554779053
Epoch 890, val loss: 1.4689371585845947
Epoch 900, training loss: 0.010374634526669979 = 0.003284461097791791 + 0.001 * 7.090173244476318
Epoch 900, val loss: 1.4744012355804443
Epoch 910, training loss: 0.0102843614295125 = 0.0031959814950823784 + 0.001 * 7.088379383087158
Epoch 910, val loss: 1.479806900024414
Epoch 920, training loss: 0.010190621018409729 = 0.0031117554754018784 + 0.001 * 7.0788655281066895
Epoch 920, val loss: 1.4851062297821045
Epoch 930, training loss: 0.010124611668288708 = 0.003031454747542739 + 0.001 * 7.093156814575195
Epoch 930, val loss: 1.4902725219726562
Epoch 940, training loss: 0.010024232789874077 = 0.002954880241304636 + 0.001 * 7.069351673126221
Epoch 940, val loss: 1.4953292608261108
Epoch 950, training loss: 0.009987873956561089 = 0.002881797729060054 + 0.001 * 7.106075763702393
Epoch 950, val loss: 1.500303030014038
Epoch 960, training loss: 0.009909258224070072 = 0.002811974147334695 + 0.001 * 7.097283363342285
Epoch 960, val loss: 1.505212426185608
Epoch 970, training loss: 0.009804975241422653 = 0.002745244652032852 + 0.001 * 7.059730052947998
Epoch 970, val loss: 1.5099785327911377
Epoch 980, training loss: 0.00973787996917963 = 0.0026814064476639032 + 0.001 * 7.056473255157471
Epoch 980, val loss: 1.5147053003311157
Epoch 990, training loss: 0.009685099124908447 = 0.002620282117277384 + 0.001 * 7.064816474914551
Epoch 990, val loss: 1.5193281173706055
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.812335266209805
The final CL Acc:0.74198, 0.00630, The final GNN Acc:0.81163, 0.00348
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13276])
remove edge: torch.Size([2, 7910])
updated graph: torch.Size([2, 10630])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9614568948745728 = 1.9528601169586182 + 0.001 * 8.596832275390625
Epoch 0, val loss: 1.9568058252334595
Epoch 10, training loss: 1.9510849714279175 = 1.942488193511963 + 0.001 * 8.59675121307373
Epoch 10, val loss: 1.946934700012207
Epoch 20, training loss: 1.938047170639038 = 1.9294506311416626 + 0.001 * 8.596535682678223
Epoch 20, val loss: 1.9341773986816406
Epoch 30, training loss: 1.9196813106536865 = 1.9110852479934692 + 0.001 * 8.596040725708008
Epoch 30, val loss: 1.9158074855804443
Epoch 40, training loss: 1.8925343751907349 = 1.8839396238327026 + 0.001 * 8.594772338867188
Epoch 40, val loss: 1.888584017753601
Epoch 50, training loss: 1.8543095588684082 = 1.8457188606262207 + 0.001 * 8.590675354003906
Epoch 50, val loss: 1.8512903451919556
Epoch 60, training loss: 1.8100565671920776 = 1.8014832735061646 + 0.001 * 8.573296546936035
Epoch 60, val loss: 1.8108879327774048
Epoch 70, training loss: 1.7703757286071777 = 1.7618917226791382 + 0.001 * 8.484026908874512
Epoch 70, val loss: 1.776007890701294
Epoch 80, training loss: 1.7234535217285156 = 1.7154122591018677 + 0.001 * 8.041220664978027
Epoch 80, val loss: 1.7311123609542847
Epoch 90, training loss: 1.6590988636016846 = 1.651279091835022 + 0.001 * 7.819748401641846
Epoch 90, val loss: 1.670802116394043
Epoch 100, training loss: 1.5745785236358643 = 1.566969871520996 + 0.001 * 7.608605861663818
Epoch 100, val loss: 1.5951554775238037
Epoch 110, training loss: 1.474416971206665 = 1.4669774770736694 + 0.001 * 7.439457416534424
Epoch 110, val loss: 1.5085357427597046
Epoch 120, training loss: 1.3675427436828613 = 1.3601608276367188 + 0.001 * 7.381871700286865
Epoch 120, val loss: 1.4180214405059814
Epoch 130, training loss: 1.2594926357269287 = 1.2521586418151855 + 0.001 * 7.333981990814209
Epoch 130, val loss: 1.328339695930481
Epoch 140, training loss: 1.1531261205673218 = 1.1458203792572021 + 0.001 * 7.305761814117432
Epoch 140, val loss: 1.2413965463638306
Epoch 150, training loss: 1.0507420301437378 = 1.0434569120407104 + 0.001 * 7.285091400146484
Epoch 150, val loss: 1.1577950716018677
Epoch 160, training loss: 0.9548107981681824 = 0.9475398659706116 + 0.001 * 7.270910263061523
Epoch 160, val loss: 1.0794687271118164
Epoch 170, training loss: 0.8668592572212219 = 0.8595957159996033 + 0.001 * 7.263540267944336
Epoch 170, val loss: 1.0079450607299805
Epoch 180, training loss: 0.7875791192054749 = 0.7803206443786621 + 0.001 * 7.258485794067383
Epoch 180, val loss: 0.9443983435630798
Epoch 190, training loss: 0.7168790102005005 = 0.7096245884895325 + 0.001 * 7.254406452178955
Epoch 190, val loss: 0.8899837732315063
Epoch 200, training loss: 0.6534863710403442 = 0.646237313747406 + 0.001 * 7.249027729034424
Epoch 200, val loss: 0.8443124294281006
Epoch 210, training loss: 0.5954591631889343 = 0.5882183909416199 + 0.001 * 7.240759372711182
Epoch 210, val loss: 0.8062268495559692
Epoch 220, training loss: 0.5414076447486877 = 0.5341793894767761 + 0.001 * 7.228255748748779
Epoch 220, val loss: 0.7742093205451965
Epoch 230, training loss: 0.49070993065834045 = 0.48349183797836304 + 0.001 * 7.218101978302002
Epoch 230, val loss: 0.7466995120048523
Epoch 240, training loss: 0.4431909918785095 = 0.43599385023117065 + 0.001 * 7.197143077850342
Epoch 240, val loss: 0.7227896451950073
Epoch 250, training loss: 0.39888086915016174 = 0.3916986286640167 + 0.001 * 7.182234764099121
Epoch 250, val loss: 0.7022894024848938
Epoch 260, training loss: 0.35770276188850403 = 0.35053542256355286 + 0.001 * 7.167352676391602
Epoch 260, val loss: 0.6852743625640869
Epoch 270, training loss: 0.319350004196167 = 0.312181293964386 + 0.001 * 7.168701171875
Epoch 270, val loss: 0.671747088432312
Epoch 280, training loss: 0.2833094596862793 = 0.2761567533016205 + 0.001 * 7.152704238891602
Epoch 280, val loss: 0.6610603928565979
Epoch 290, training loss: 0.2492292821407318 = 0.24208103120326996 + 0.001 * 7.148246765136719
Epoch 290, val loss: 0.652525782585144
Epoch 300, training loss: 0.21711131930351257 = 0.20996639132499695 + 0.001 * 7.144932746887207
Epoch 300, val loss: 0.6459373235702515
Epoch 310, training loss: 0.18743623793125153 = 0.18029473721981049 + 0.001 * 7.1414947509765625
Epoch 310, val loss: 0.6412699222564697
Epoch 320, training loss: 0.16091805696487427 = 0.15377305448055267 + 0.001 * 7.145002841949463
Epoch 320, val loss: 0.6387691497802734
Epoch 330, training loss: 0.138019397854805 = 0.1308821737766266 + 0.001 * 7.137228488922119
Epoch 330, val loss: 0.6387651562690735
Epoch 340, training loss: 0.1187688410282135 = 0.11163266748189926 + 0.001 * 7.13617467880249
Epoch 340, val loss: 0.6412360072135925
Epoch 350, training loss: 0.10280287265777588 = 0.0956692025065422 + 0.001 * 7.133673667907715
Epoch 350, val loss: 0.6459855437278748
Epoch 360, training loss: 0.08961203694343567 = 0.08247711509466171 + 0.001 * 7.134922504425049
Epoch 360, val loss: 0.6527130603790283
Epoch 370, training loss: 0.0786731168627739 = 0.07154291123151779 + 0.001 * 7.130208492279053
Epoch 370, val loss: 0.6610692143440247
Epoch 380, training loss: 0.0695604607462883 = 0.06243043392896652 + 0.001 * 7.130028247833252
Epoch 380, val loss: 0.6707477569580078
Epoch 390, training loss: 0.06191980466246605 = 0.05479110777378082 + 0.001 * 7.128696918487549
Epoch 390, val loss: 0.6813097596168518
Epoch 400, training loss: 0.05547640472650528 = 0.0483531728386879 + 0.001 * 7.12322998046875
Epoch 400, val loss: 0.6924987435340881
Epoch 410, training loss: 0.05002998560667038 = 0.042903557419776917 + 0.001 * 7.126429557800293
Epoch 410, val loss: 0.7040888071060181
Epoch 420, training loss: 0.04538591578602791 = 0.03827114775776863 + 0.001 * 7.1147661209106445
Epoch 420, val loss: 0.7158327102661133
Epoch 430, training loss: 0.04142589867115021 = 0.034316495060920715 + 0.001 * 7.109402179718018
Epoch 430, val loss: 0.7275829911231995
Epoch 440, training loss: 0.03804723173379898 = 0.030925599858164787 + 0.001 * 7.121631622314453
Epoch 440, val loss: 0.73927241563797
Epoch 450, training loss: 0.0351087749004364 = 0.028003081679344177 + 0.001 * 7.1056928634643555
Epoch 450, val loss: 0.750709056854248
Epoch 460, training loss: 0.032563403248786926 = 0.02547147125005722 + 0.001 * 7.091933727264404
Epoch 460, val loss: 0.7618956565856934
Epoch 470, training loss: 0.030367666855454445 = 0.023267002776265144 + 0.001 * 7.100663185119629
Epoch 470, val loss: 0.7728181481361389
Epoch 480, training loss: 0.028427250683307648 = 0.02133740298449993 + 0.001 * 7.089848041534424
Epoch 480, val loss: 0.7834264636039734
Epoch 490, training loss: 0.02671767771244049 = 0.019640101119875908 + 0.001 * 7.077575206756592
Epoch 490, val loss: 0.7937183380126953
Epoch 500, training loss: 0.025215163826942444 = 0.018139950931072235 + 0.001 * 7.075212478637695
Epoch 500, val loss: 0.8037205934524536
Epoch 510, training loss: 0.02388722263276577 = 0.01680806651711464 + 0.001 * 7.079156398773193
Epoch 510, val loss: 0.8134083151817322
Epoch 520, training loss: 0.02269589900970459 = 0.01562059111893177 + 0.001 * 7.075307846069336
Epoch 520, val loss: 0.8228179812431335
Epoch 530, training loss: 0.021618036553263664 = 0.014557567425072193 + 0.001 * 7.0604681968688965
Epoch 530, val loss: 0.8319231271743774
Epoch 540, training loss: 0.020664136856794357 = 0.01360241323709488 + 0.001 * 7.061722278594971
Epoch 540, val loss: 0.8407869935035706
Epoch 550, training loss: 0.019791124388575554 = 0.012741157785058022 + 0.001 * 7.049966335296631
Epoch 550, val loss: 0.8493829965591431
Epoch 560, training loss: 0.01900566928088665 = 0.011961938813328743 + 0.001 * 7.043730735778809
Epoch 560, val loss: 0.8576993942260742
Epoch 570, training loss: 0.018342941999435425 = 0.011254684068262577 + 0.001 * 7.0882568359375
Epoch 570, val loss: 0.8657786250114441
Epoch 580, training loss: 0.017657505348324776 = 0.010610894300043583 + 0.001 * 7.0466108322143555
Epoch 580, val loss: 0.8736100196838379
Epoch 590, training loss: 0.017065510153770447 = 0.010023141279816628 + 0.001 * 7.042368412017822
Epoch 590, val loss: 0.8812284469604492
Epoch 600, training loss: 0.01653129979968071 = 0.009485156275331974 + 0.001 * 7.046142101287842
Epoch 600, val loss: 0.888640820980072
Epoch 610, training loss: 0.016025155782699585 = 0.008991525508463383 + 0.001 * 7.033630847930908
Epoch 610, val loss: 0.8958436846733093
Epoch 620, training loss: 0.015574274584650993 = 0.008537481538951397 + 0.001 * 7.036792755126953
Epoch 620, val loss: 0.9028600454330444
Epoch 630, training loss: 0.015148291364312172 = 0.008118931204080582 + 0.001 * 7.029360294342041
Epoch 630, val loss: 0.9096510410308838
Epoch 640, training loss: 0.014756014570593834 = 0.00773225910961628 + 0.001 * 7.023754596710205
Epoch 640, val loss: 0.916290283203125
Epoch 650, training loss: 0.014414483681321144 = 0.007374332752078772 + 0.001 * 7.040150165557861
Epoch 650, val loss: 0.9227490425109863
Epoch 660, training loss: 0.014064803719520569 = 0.00704234279692173 + 0.001 * 7.0224609375
Epoch 660, val loss: 0.9290345311164856
Epoch 670, training loss: 0.013760663568973541 = 0.006733860354870558 + 0.001 * 7.0268025398254395
Epoch 670, val loss: 0.9351505637168884
Epoch 680, training loss: 0.013470860198140144 = 0.006446727551519871 + 0.001 * 7.02413272857666
Epoch 680, val loss: 0.9411132335662842
Epoch 690, training loss: 0.01319525670260191 = 0.006178965326398611 + 0.001 * 7.01629114151001
Epoch 690, val loss: 0.9469239711761475
Epoch 700, training loss: 0.012954586185514927 = 0.00592888705432415 + 0.001 * 7.025698661804199
Epoch 700, val loss: 0.9525871872901917
Epoch 710, training loss: 0.012713117524981499 = 0.005694979336112738 + 0.001 * 7.018137454986572
Epoch 710, val loss: 0.9581090807914734
Epoch 720, training loss: 0.012484228238463402 = 0.005475848913192749 + 0.001 * 7.008378505706787
Epoch 720, val loss: 0.9635016322135925
Epoch 730, training loss: 0.012311024591326714 = 0.005270279012620449 + 0.001 * 7.040744781494141
Epoch 730, val loss: 0.9687493443489075
Epoch 740, training loss: 0.012092256918549538 = 0.00507719162851572 + 0.001 * 7.0150651931762695
Epoch 740, val loss: 0.9738914370536804
Epoch 750, training loss: 0.01189291663467884 = 0.004895597230643034 + 0.001 * 6.99731969833374
Epoch 750, val loss: 0.978890061378479
Epoch 760, training loss: 0.011728592216968536 = 0.004724577069282532 + 0.001 * 7.00401496887207
Epoch 760, val loss: 0.9837896227836609
Epoch 770, training loss: 0.011567099019885063 = 0.004563361406326294 + 0.001 * 7.003737926483154
Epoch 770, val loss: 0.9885607957839966
Epoch 780, training loss: 0.011440569534897804 = 0.004411167930811644 + 0.001 * 7.029400825500488
Epoch 780, val loss: 0.9932349324226379
Epoch 790, training loss: 0.01126429345458746 = 0.004267383832484484 + 0.001 * 6.996909141540527
Epoch 790, val loss: 0.9978106617927551
Epoch 800, training loss: 0.01112077385187149 = 0.00413135951384902 + 0.001 * 6.989414691925049
Epoch 800, val loss: 1.002274513244629
Epoch 810, training loss: 0.01098824106156826 = 0.0040025352500379086 + 0.001 * 6.9857048988342285
Epoch 810, val loss: 1.0066330432891846
Epoch 820, training loss: 0.010876663029193878 = 0.0038804253563284874 + 0.001 * 6.996237754821777
Epoch 820, val loss: 1.0108935832977295
Epoch 830, training loss: 0.010759434662759304 = 0.0037645858246833086 + 0.001 * 6.994848251342773
Epoch 830, val loss: 1.0150773525238037
Epoch 840, training loss: 0.010650036856532097 = 0.0036545712500810623 + 0.001 * 6.99546480178833
Epoch 840, val loss: 1.019152283668518
Epoch 850, training loss: 0.010545277036726475 = 0.0035500021185725927 + 0.001 * 6.995275020599365
Epoch 850, val loss: 1.0231528282165527
Epoch 860, training loss: 0.01043326873332262 = 0.0034505249932408333 + 0.001 * 6.982743263244629
Epoch 860, val loss: 1.0270590782165527
Epoch 870, training loss: 0.010335549712181091 = 0.0033558164723217487 + 0.001 * 6.979732990264893
Epoch 870, val loss: 1.0309032201766968
Epoch 880, training loss: 0.01024380512535572 = 0.003265564562752843 + 0.001 * 6.978240013122559
Epoch 880, val loss: 1.0346596240997314
Epoch 890, training loss: 0.010155926458537579 = 0.003179497318342328 + 0.001 * 6.976428985595703
Epoch 890, val loss: 1.0383336544036865
Epoch 900, training loss: 0.010103209875524044 = 0.003097361419349909 + 0.001 * 7.005847930908203
Epoch 900, val loss: 1.041946530342102
Epoch 910, training loss: 0.009976739063858986 = 0.003018936375156045 + 0.001 * 6.957802772521973
Epoch 910, val loss: 1.045470118522644
Epoch 920, training loss: 0.009958098642528057 = 0.002943984931334853 + 0.001 * 7.014112949371338
Epoch 920, val loss: 1.0489290952682495
Epoch 930, training loss: 0.009833761490881443 = 0.0028723073191940784 + 0.001 * 6.961453914642334
Epoch 930, val loss: 1.0523334741592407
Epoch 940, training loss: 0.009784202091395855 = 0.002803707029670477 + 0.001 * 6.980494499206543
Epoch 940, val loss: 1.0556561946868896
Epoch 950, training loss: 0.009701520204544067 = 0.002738029696047306 + 0.001 * 6.9634904861450195
Epoch 950, val loss: 1.0589109659194946
Epoch 960, training loss: 0.009657593443989754 = 0.00267508951947093 + 0.001 * 6.982503890991211
Epoch 960, val loss: 1.0621144771575928
Epoch 970, training loss: 0.00957225076854229 = 0.002614747965708375 + 0.001 * 6.957502841949463
Epoch 970, val loss: 1.0652505159378052
Epoch 980, training loss: 0.009511813521385193 = 0.002556848805397749 + 0.001 * 6.9549641609191895
Epoch 980, val loss: 1.068326473236084
Epoch 990, training loss: 0.009459932334721088 = 0.0025012760888785124 + 0.001 * 6.958655834197998
Epoch 990, val loss: 1.0713379383087158
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 1.9706332683563232 = 1.962036371231079 + 0.001 * 8.596858024597168
Epoch 0, val loss: 1.9577363729476929
Epoch 10, training loss: 1.959912657737732 = 1.9513158798217773 + 0.001 * 8.59680461883545
Epoch 10, val loss: 1.9473037719726562
Epoch 20, training loss: 1.946657419204712 = 1.9380607604980469 + 0.001 * 8.596646308898926
Epoch 20, val loss: 1.9337750673294067
Epoch 30, training loss: 1.928366780281067 = 1.9197704792022705 + 0.001 * 8.596281051635742
Epoch 30, val loss: 1.9148218631744385
Epoch 40, training loss: 1.9019131660461426 = 1.8933178186416626 + 0.001 * 8.595362663269043
Epoch 40, val loss: 1.887593150138855
Epoch 50, training loss: 1.865080714225769 = 1.8564882278442383 + 0.001 * 8.592509269714355
Epoch 50, val loss: 1.8512089252471924
Epoch 60, training loss: 1.8221032619476318 = 1.8135223388671875 + 0.001 * 8.580953598022461
Epoch 60, val loss: 1.8127743005752563
Epoch 70, training loss: 1.7829419374465942 = 1.7744196653366089 + 0.001 * 8.522269248962402
Epoch 70, val loss: 1.7822346687316895
Epoch 80, training loss: 1.7398855686187744 = 1.7317389249801636 + 0.001 * 8.146649360656738
Epoch 80, val loss: 1.7469478845596313
Epoch 90, training loss: 1.682011604309082 = 1.67405366897583 + 0.001 * 7.957942962646484
Epoch 90, val loss: 1.6957682371139526
Epoch 100, training loss: 1.6039444208145142 = 1.5962255001068115 + 0.001 * 7.718944549560547
Epoch 100, val loss: 1.6270960569381714
Epoch 110, training loss: 1.5083255767822266 = 1.5007903575897217 + 0.001 * 7.535167694091797
Epoch 110, val loss: 1.5452337265014648
Epoch 120, training loss: 1.405352234840393 = 1.397889494895935 + 0.001 * 7.46269416809082
Epoch 120, val loss: 1.4598692655563354
Epoch 130, training loss: 1.3023324012756348 = 1.2948945760726929 + 0.001 * 7.437869071960449
Epoch 130, val loss: 1.3779640197753906
Epoch 140, training loss: 1.201685905456543 = 1.1942774057388306 + 0.001 * 7.408504962921143
Epoch 140, val loss: 1.3000282049179077
Epoch 150, training loss: 1.1039323806762695 = 1.0965481996536255 + 0.001 * 7.384130954742432
Epoch 150, val loss: 1.2243335247039795
Epoch 160, training loss: 1.0105689764022827 = 1.0032120943069458 + 0.001 * 7.356832981109619
Epoch 160, val loss: 1.152093768119812
Epoch 170, training loss: 0.9233870506286621 = 0.916052520275116 + 0.001 * 7.334533214569092
Epoch 170, val loss: 1.0850937366485596
Epoch 180, training loss: 0.8437244296073914 = 0.8364034295082092 + 0.001 * 7.320999622344971
Epoch 180, val loss: 1.024317979812622
Epoch 190, training loss: 0.7728995084762573 = 0.7655861377716064 + 0.001 * 7.313355445861816
Epoch 190, val loss: 0.9715349078178406
Epoch 200, training loss: 0.7109864950180054 = 0.7036795616149902 + 0.001 * 7.306923866271973
Epoch 200, val loss: 0.9271389245986938
Epoch 210, training loss: 0.6565157771110535 = 0.649215817451477 + 0.001 * 7.29994010925293
Epoch 210, val loss: 0.8902974724769592
Epoch 220, training loss: 0.607097327709198 = 0.5998057126998901 + 0.001 * 7.2916059494018555
Epoch 220, val loss: 0.8594992756843567
Epoch 230, training loss: 0.5605419874191284 = 0.5532605648040771 + 0.001 * 7.281399250030518
Epoch 230, val loss: 0.8330649733543396
Epoch 240, training loss: 0.515610933303833 = 0.5083410143852234 + 0.001 * 7.269904613494873
Epoch 240, val loss: 0.8100923299789429
Epoch 250, training loss: 0.47190773487091064 = 0.4646497666835785 + 0.001 * 7.257953643798828
Epoch 250, val loss: 0.7900567650794983
Epoch 260, training loss: 0.4294423758983612 = 0.422198086977005 + 0.001 * 7.244284152984619
Epoch 260, val loss: 0.7728061079978943
Epoch 270, training loss: 0.388414591550827 = 0.38118240237236023 + 0.001 * 7.232183933258057
Epoch 270, val loss: 0.7588075399398804
Epoch 280, training loss: 0.3490638732910156 = 0.34184837341308594 + 0.001 * 7.215489864349365
Epoch 280, val loss: 0.7486996054649353
Epoch 290, training loss: 0.3116413950920105 = 0.3044397532939911 + 0.001 * 7.201647758483887
Epoch 290, val loss: 0.7427765727043152
Epoch 300, training loss: 0.2763838768005371 = 0.26918596029281616 + 0.001 * 7.197904586791992
Epoch 300, val loss: 0.7410016059875488
Epoch 310, training loss: 0.24355480074882507 = 0.2363751232624054 + 0.001 * 7.179683208465576
Epoch 310, val loss: 0.7430535554885864
Epoch 320, training loss: 0.21348071098327637 = 0.2063094824552536 + 0.001 * 7.171228408813477
Epoch 320, val loss: 0.7486562132835388
Epoch 330, training loss: 0.18636377155780792 = 0.1791701763868332 + 0.001 * 7.19359016418457
Epoch 330, val loss: 0.7576583027839661
Epoch 340, training loss: 0.16225357353687286 = 0.15508213639259338 + 0.001 * 7.171435832977295
Epoch 340, val loss: 0.7693880200386047
Epoch 350, training loss: 0.14118829369544983 = 0.13402926921844482 + 0.001 * 7.1590166091918945
Epoch 350, val loss: 0.7835851907730103
Epoch 360, training loss: 0.12303667515516281 = 0.11588649451732635 + 0.001 * 7.150177001953125
Epoch 360, val loss: 0.7996447682380676
Epoch 370, training loss: 0.10759497433900833 = 0.10041310638189316 + 0.001 * 7.181865692138672
Epoch 370, val loss: 0.8173128366470337
Epoch 380, training loss: 0.09443654119968414 = 0.08728919178247452 + 0.001 * 7.147351264953613
Epoch 380, val loss: 0.8360300064086914
Epoch 390, training loss: 0.08332180976867676 = 0.07618143409490585 + 0.001 * 7.140378952026367
Epoch 390, val loss: 0.8554294109344482
Epoch 400, training loss: 0.07390501350164413 = 0.06676661968231201 + 0.001 * 7.138394832611084
Epoch 400, val loss: 0.8752996325492859
Epoch 410, training loss: 0.06590185314416885 = 0.05876663327217102 + 0.001 * 7.135217666625977
Epoch 410, val loss: 0.8952216506004333
Epoch 420, training loss: 0.05908672511577606 = 0.051955074071884155 + 0.001 * 7.131650924682617
Epoch 420, val loss: 0.9150195121765137
Epoch 430, training loss: 0.05327608808875084 = 0.04614107310771942 + 0.001 * 7.13501501083374
Epoch 430, val loss: 0.9345795512199402
Epoch 440, training loss: 0.048298902809619904 = 0.04116438701748848 + 0.001 * 7.134515762329102
Epoch 440, val loss: 0.9537352323532104
Epoch 450, training loss: 0.04401438310742378 = 0.036889251321554184 + 0.001 * 7.125131130218506
Epoch 450, val loss: 0.9724178910255432
Epoch 460, training loss: 0.0403241366147995 = 0.03320091590285301 + 0.001 * 7.1232218742370605
Epoch 460, val loss: 0.9905396699905396
Epoch 470, training loss: 0.037126366049051285 = 0.030004078522324562 + 0.001 * 7.122286319732666
Epoch 470, val loss: 1.0080678462982178
Epoch 480, training loss: 0.03435314819216728 = 0.02722012810409069 + 0.001 * 7.133018493652344
Epoch 480, val loss: 1.0249754190444946
Epoch 490, training loss: 0.03191095218062401 = 0.024785082787275314 + 0.001 * 7.125868320465088
Epoch 490, val loss: 1.0413274765014648
Epoch 500, training loss: 0.02976308763027191 = 0.02264583483338356 + 0.001 * 7.117251396179199
Epoch 500, val loss: 1.0571179389953613
Epoch 510, training loss: 0.02789676934480667 = 0.020758990198373795 + 0.001 * 7.137778282165527
Epoch 510, val loss: 1.0723590850830078
Epoch 520, training loss: 0.026200957596302032 = 0.019088560715317726 + 0.001 * 7.11239767074585
Epoch 520, val loss: 1.0870710611343384
Epoch 530, training loss: 0.0247153639793396 = 0.01760423555970192 + 0.001 * 7.111128807067871
Epoch 530, val loss: 1.1012988090515137
Epoch 540, training loss: 0.023394592106342316 = 0.016279898583889008 + 0.001 * 7.114692687988281
Epoch 540, val loss: 1.1150741577148438
Epoch 550, training loss: 0.022203918546438217 = 0.015093856491148472 + 0.001 * 7.110062122344971
Epoch 550, val loss: 1.1284290552139282
Epoch 560, training loss: 0.0211329385638237 = 0.014026672579348087 + 0.001 * 7.106266498565674
Epoch 560, val loss: 1.1414217948913574
Epoch 570, training loss: 0.020162314176559448 = 0.013062562793493271 + 0.001 * 7.0997514724731445
Epoch 570, val loss: 1.1541001796722412
Epoch 580, training loss: 0.019298596307635307 = 0.01218919176608324 + 0.001 * 7.109403610229492
Epoch 580, val loss: 1.1663912534713745
Epoch 590, training loss: 0.01849554479122162 = 0.011396449990570545 + 0.001 * 7.099094390869141
Epoch 590, val loss: 1.1784168481826782
Epoch 600, training loss: 0.017772281542420387 = 0.010675642639398575 + 0.001 * 7.096639156341553
Epoch 600, val loss: 1.1900871992111206
Epoch 610, training loss: 0.017103254795074463 = 0.010019559413194656 + 0.001 * 7.083695888519287
Epoch 610, val loss: 1.201447606086731
Epoch 620, training loss: 0.016501426696777344 = 0.00942156370729208 + 0.001 * 7.079862117767334
Epoch 620, val loss: 1.2124950885772705
Epoch 630, training loss: 0.015957707539200783 = 0.00887543335556984 + 0.001 * 7.082273483276367
Epoch 630, val loss: 1.2232328653335571
Epoch 640, training loss: 0.015473573468625546 = 0.008375875651836395 + 0.001 * 7.0976972579956055
Epoch 640, val loss: 1.2336722612380981
Epoch 650, training loss: 0.014994548633694649 = 0.007918063551187515 + 0.001 * 7.076484203338623
Epoch 650, val loss: 1.2438246011734009
Epoch 660, training loss: 0.014587130397558212 = 0.007497820071876049 + 0.001 * 7.089310646057129
Epoch 660, val loss: 1.2536817789077759
Epoch 670, training loss: 0.014177118428051472 = 0.007111365906894207 + 0.001 * 7.065752029418945
Epoch 670, val loss: 1.263270378112793
Epoch 680, training loss: 0.013827703893184662 = 0.006755216047167778 + 0.001 * 7.072487831115723
Epoch 680, val loss: 1.272611141204834
Epoch 690, training loss: 0.01350915152579546 = 0.006426542531698942 + 0.001 * 7.082608699798584
Epoch 690, val loss: 1.2816909551620483
Epoch 700, training loss: 0.01319872122257948 = 0.006122593302279711 + 0.001 * 7.076127529144287
Epoch 700, val loss: 1.290533423423767
Epoch 710, training loss: 0.01290813460946083 = 0.005841081030666828 + 0.001 * 7.06705379486084
Epoch 710, val loss: 1.2991650104522705
Epoch 720, training loss: 0.012639930471777916 = 0.005579948890954256 + 0.001 * 7.059981822967529
Epoch 720, val loss: 1.3075367212295532
Epoch 730, training loss: 0.01239830069243908 = 0.005337199661880732 + 0.001 * 7.061100006103516
Epoch 730, val loss: 1.3157060146331787
Epoch 740, training loss: 0.012156414799392223 = 0.0051112393848598 + 0.001 * 7.045175075531006
Epoch 740, val loss: 1.3236775398254395
Epoch 750, training loss: 0.011945582926273346 = 0.004900513216853142 + 0.001 * 7.045068740844727
Epoch 750, val loss: 1.3314141035079956
Epoch 760, training loss: 0.011767005547881126 = 0.0047037964686751366 + 0.001 * 7.063208103179932
Epoch 760, val loss: 1.3389750719070435
Epoch 770, training loss: 0.011571568436920643 = 0.004519784357398748 + 0.001 * 7.051783561706543
Epoch 770, val loss: 1.3463382720947266
Epoch 780, training loss: 0.01137730572372675 = 0.004347521811723709 + 0.001 * 7.029783725738525
Epoch 780, val loss: 1.3535473346710205
Epoch 790, training loss: 0.01121396105736494 = 0.004185915924608707 + 0.001 * 7.028044700622559
Epoch 790, val loss: 1.3605549335479736
Epoch 800, training loss: 0.0110525693744421 = 0.004034190904349089 + 0.001 * 7.018377780914307
Epoch 800, val loss: 1.3674150705337524
Epoch 810, training loss: 0.01091680210083723 = 0.0038915672339498997 + 0.001 * 7.025234699249268
Epoch 810, val loss: 1.3741111755371094
Epoch 820, training loss: 0.010832569561898708 = 0.0037572779692709446 + 0.001 * 7.075291156768799
Epoch 820, val loss: 1.3806599378585815
Epoch 830, training loss: 0.010652516037225723 = 0.0036307452246546745 + 0.001 * 7.021770000457764
Epoch 830, val loss: 1.3870148658752441
Epoch 840, training loss: 0.010538078844547272 = 0.003511385526508093 + 0.001 * 7.026692867279053
Epoch 840, val loss: 1.3932440280914307
Epoch 850, training loss: 0.010420355945825577 = 0.00339864706620574 + 0.001 * 7.021708011627197
Epoch 850, val loss: 1.399309515953064
Epoch 860, training loss: 0.010295078158378601 = 0.0032920357771217823 + 0.001 * 7.003042221069336
Epoch 860, val loss: 1.4052362442016602
Epoch 870, training loss: 0.010243323631584644 = 0.0031910885591059923 + 0.001 * 7.052235126495361
Epoch 870, val loss: 1.41103196144104
Epoch 880, training loss: 0.010095116682350636 = 0.0030954733956605196 + 0.001 * 6.999642848968506
Epoch 880, val loss: 1.416683316230774
Epoch 890, training loss: 0.010005520656704903 = 0.003004796337336302 + 0.001 * 7.000723838806152
Epoch 890, val loss: 1.4221938848495483
Epoch 900, training loss: 0.009924422018229961 = 0.002918732352554798 + 0.001 * 7.0056891441345215
Epoch 900, val loss: 1.4275810718536377
Epoch 910, training loss: 0.009845927357673645 = 0.002836993895471096 + 0.001 * 7.008933067321777
Epoch 910, val loss: 1.432848334312439
Epoch 920, training loss: 0.009768718853592873 = 0.0027592794504016638 + 0.001 * 7.009439468383789
Epoch 920, val loss: 1.4380007982254028
Epoch 930, training loss: 0.00968389306217432 = 0.002685281913727522 + 0.001 * 6.998610973358154
Epoch 930, val loss: 1.443047285079956
Epoch 940, training loss: 0.009619750082492828 = 0.0026147805619984865 + 0.001 * 7.004969120025635
Epoch 940, val loss: 1.4479827880859375
Epoch 950, training loss: 0.009571087546646595 = 0.0025475663132965565 + 0.001 * 7.0235209465026855
Epoch 950, val loss: 1.452805757522583
Epoch 960, training loss: 0.009484444744884968 = 0.0024835027288645506 + 0.001 * 7.000941753387451
Epoch 960, val loss: 1.4575556516647339
Epoch 970, training loss: 0.009443609043955803 = 0.002422299236059189 + 0.001 * 7.021309852600098
Epoch 970, val loss: 1.4621586799621582
Epoch 980, training loss: 0.009360101073980331 = 0.0023638522252440453 + 0.001 * 6.996249198913574
Epoch 980, val loss: 1.4666798114776611
Epoch 990, training loss: 0.009311772882938385 = 0.002307961927726865 + 0.001 * 7.003810405731201
Epoch 990, val loss: 1.471116065979004
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8313125988402742
=== training gcn model ===
Epoch 0, training loss: 1.9475438594818115 = 1.9389469623565674 + 0.001 * 8.596848487854004
Epoch 0, val loss: 1.9514126777648926
Epoch 10, training loss: 1.9379029273986816 = 1.929306149482727 + 0.001 * 8.596770286560059
Epoch 10, val loss: 1.9413789510726929
Epoch 20, training loss: 1.9261740446090698 = 1.9175775051116943 + 0.001 * 8.59654712677002
Epoch 20, val loss: 1.929057240486145
Epoch 30, training loss: 1.909989833831787 = 1.9013937711715698 + 0.001 * 8.59602165222168
Epoch 30, val loss: 1.9121668338775635
Epoch 40, training loss: 1.886476993560791 = 1.8778823614120483 + 0.001 * 8.594688415527344
Epoch 40, val loss: 1.8879055976867676
Epoch 50, training loss: 1.8532522916793823 = 1.8446614742279053 + 0.001 * 8.59077262878418
Epoch 50, val loss: 1.854699969291687
Epoch 60, training loss: 1.8130725622177124 = 1.8044965267181396 + 0.001 * 8.576068878173828
Epoch 60, val loss: 1.8164300918579102
Epoch 70, training loss: 1.7740787267684937 = 1.765578031539917 + 0.001 * 8.500738143920898
Epoch 70, val loss: 1.7803856134414673
Epoch 80, training loss: 1.729500412940979 = 1.7214651107788086 + 0.001 * 8.035321235656738
Epoch 80, val loss: 1.738100528717041
Epoch 90, training loss: 1.667992115020752 = 1.6601945161819458 + 0.001 * 7.797565937042236
Epoch 90, val loss: 1.6830013990402222
Epoch 100, training loss: 1.587116003036499 = 1.5794821977615356 + 0.001 * 7.63383674621582
Epoch 100, val loss: 1.6147053241729736
Epoch 110, training loss: 1.4891180992126465 = 1.4816186428070068 + 0.001 * 7.499420166015625
Epoch 110, val loss: 1.530961036682129
Epoch 120, training loss: 1.3819129467010498 = 1.3744739294052124 + 0.001 * 7.439000129699707
Epoch 120, val loss: 1.4382425546646118
Epoch 130, training loss: 1.2714766263961792 = 1.2640938758850098 + 0.001 * 7.3827056884765625
Epoch 130, val loss: 1.3422821760177612
Epoch 140, training loss: 1.160563349723816 = 1.153217077255249 + 0.001 * 7.346277236938477
Epoch 140, val loss: 1.2468061447143555
Epoch 150, training loss: 1.0519553422927856 = 1.0446256399154663 + 0.001 * 7.32974910736084
Epoch 150, val loss: 1.1544688940048218
Epoch 160, training loss: 0.9493050575256348 = 0.9419901371002197 + 0.001 * 7.314901351928711
Epoch 160, val loss: 1.0684969425201416
Epoch 170, training loss: 0.8550336360931396 = 0.8477354049682617 + 0.001 * 7.298233509063721
Epoch 170, val loss: 0.9909375309944153
Epoch 180, training loss: 0.7708348035812378 = 0.7635548114776611 + 0.001 * 7.280017375946045
Epoch 180, val loss: 0.9233319163322449
Epoch 190, training loss: 0.696998119354248 = 0.6897339224815369 + 0.001 * 7.264181137084961
Epoch 190, val loss: 0.8664090037345886
Epoch 200, training loss: 0.6323820352554321 = 0.6251282095909119 + 0.001 * 7.253796577453613
Epoch 200, val loss: 0.8196362257003784
Epoch 210, training loss: 0.5750588178634644 = 0.5678109526634216 + 0.001 * 7.247893333435059
Epoch 210, val loss: 0.7817299962043762
Epoch 220, training loss: 0.523162305355072 = 0.5159187912940979 + 0.001 * 7.243521690368652
Epoch 220, val loss: 0.7510468363761902
Epoch 230, training loss: 0.47516533732414246 = 0.4679265320301056 + 0.001 * 7.238803386688232
Epoch 230, val loss: 0.7261679172515869
Epoch 240, training loss: 0.42982062697410583 = 0.42258644104003906 + 0.001 * 7.234184265136719
Epoch 240, val loss: 0.7055556178092957
Epoch 250, training loss: 0.386265367269516 = 0.3790373206138611 + 0.001 * 7.228059768676758
Epoch 250, val loss: 0.6877981424331665
Epoch 260, training loss: 0.34427404403686523 = 0.3370519280433655 + 0.001 * 7.2221245765686035
Epoch 260, val loss: 0.6723373532295227
Epoch 270, training loss: 0.3042783737182617 = 0.2970622181892395 + 0.001 * 7.216141700744629
Epoch 270, val loss: 0.659119188785553
Epoch 280, training loss: 0.2670867443084717 = 0.2598767578601837 + 0.001 * 7.209975242614746
Epoch 280, val loss: 0.6485522985458374
Epoch 290, training loss: 0.2334672063589096 = 0.2262638509273529 + 0.001 * 7.203361988067627
Epoch 290, val loss: 0.6409164071083069
Epoch 300, training loss: 0.20381827652454376 = 0.1966230571269989 + 0.001 * 7.195224285125732
Epoch 300, val loss: 0.6362121105194092
Epoch 310, training loss: 0.17815105617046356 = 0.17096436023712158 + 0.001 * 7.186692714691162
Epoch 310, val loss: 0.6342802047729492
Epoch 320, training loss: 0.15619277954101562 = 0.14901894330978394 + 0.001 * 7.173842906951904
Epoch 320, val loss: 0.6348073482513428
Epoch 330, training loss: 0.13752481341362 = 0.1303691864013672 + 0.001 * 7.155632972717285
Epoch 330, val loss: 0.6374595761299133
Epoch 340, training loss: 0.12168510258197784 = 0.1145390123128891 + 0.001 * 7.146093845367432
Epoch 340, val loss: 0.6419305801391602
Epoch 350, training loss: 0.10820095241069794 = 0.10106684267520905 + 0.001 * 7.1341118812561035
Epoch 350, val loss: 0.6478995680809021
Epoch 360, training loss: 0.096663698554039 = 0.08954217284917831 + 0.001 * 7.121521472930908
Epoch 360, val loss: 0.6550323367118835
Epoch 370, training loss: 0.08673413097858429 = 0.07962388545274734 + 0.001 * 7.110246181488037
Epoch 370, val loss: 0.6630659699440002
Epoch 380, training loss: 0.07814999669790268 = 0.07103896141052246 + 0.001 * 7.1110358238220215
Epoch 380, val loss: 0.6718431115150452
Epoch 390, training loss: 0.0706806480884552 = 0.06357120722532272 + 0.001 * 7.109439849853516
Epoch 390, val loss: 0.6811922192573547
Epoch 400, training loss: 0.06415626406669617 = 0.05705178529024124 + 0.001 * 7.104477405548096
Epoch 400, val loss: 0.6909916400909424
Epoch 410, training loss: 0.05844938009977341 = 0.05134621635079384 + 0.001 * 7.103165149688721
Epoch 410, val loss: 0.7011189460754395
Epoch 420, training loss: 0.05344465374946594 = 0.046342089772224426 + 0.001 * 7.102561950683594
Epoch 420, val loss: 0.7114444971084595
Epoch 430, training loss: 0.049042146652936935 = 0.04194621369242668 + 0.001 * 7.095932960510254
Epoch 430, val loss: 0.7218884825706482
Epoch 440, training loss: 0.04517781734466553 = 0.038078512996435165 + 0.001 * 7.099304676055908
Epoch 440, val loss: 0.7323858141899109
Epoch 450, training loss: 0.04176808148622513 = 0.03466895967721939 + 0.001 * 7.099123001098633
Epoch 450, val loss: 0.7428537607192993
Epoch 460, training loss: 0.03874925523996353 = 0.03165372833609581 + 0.001 * 7.095527172088623
Epoch 460, val loss: 0.7532594799995422
Epoch 470, training loss: 0.03606295958161354 = 0.028974423184990883 + 0.001 * 7.088535785675049
Epoch 470, val loss: 0.763641357421875
Epoch 480, training loss: 0.03367414325475693 = 0.026584560051560402 + 0.001 * 7.08958101272583
Epoch 480, val loss: 0.7740042209625244
Epoch 490, training loss: 0.03155303746461868 = 0.024448225274682045 + 0.001 * 7.104813575744629
Epoch 490, val loss: 0.7843273878097534
Epoch 500, training loss: 0.02962433733046055 = 0.022535808384418488 + 0.001 * 7.088528156280518
Epoch 500, val loss: 0.7945231795310974
Epoch 510, training loss: 0.027904463931918144 = 0.020821284502744675 + 0.001 * 7.083179473876953
Epoch 510, val loss: 0.8045822978019714
Epoch 520, training loss: 0.02635778859257698 = 0.019281404092907906 + 0.001 * 7.076383590698242
Epoch 520, val loss: 0.8144688010215759
Epoch 530, training loss: 0.024967804551124573 = 0.01789565570652485 + 0.001 * 7.072147369384766
Epoch 530, val loss: 0.8241739273071289
Epoch 540, training loss: 0.023711595684289932 = 0.016646381467580795 + 0.001 * 7.065213680267334
Epoch 540, val loss: 0.833647608757019
Epoch 550, training loss: 0.022589348256587982 = 0.015517625957727432 + 0.001 * 7.071721076965332
Epoch 550, val loss: 0.8429147005081177
Epoch 560, training loss: 0.021556932479143143 = 0.014495592564344406 + 0.001 * 7.061338424682617
Epoch 560, val loss: 0.851936399936676
Epoch 570, training loss: 0.020650122314691544 = 0.013568061403930187 + 0.001 * 7.082061290740967
Epoch 570, val loss: 0.8607495427131653
Epoch 580, training loss: 0.019795618951320648 = 0.012724551372230053 + 0.001 * 7.071068286895752
Epoch 580, val loss: 0.8693258762359619
Epoch 590, training loss: 0.019012130796909332 = 0.011955772526562214 + 0.001 * 7.0563578605651855
Epoch 590, val loss: 0.8776928186416626
Epoch 600, training loss: 0.01829790137708187 = 0.011253447271883488 + 0.001 * 7.044453144073486
Epoch 600, val loss: 0.8858387470245361
Epoch 610, training loss: 0.01765110343694687 = 0.010610475204885006 + 0.001 * 7.040628910064697
Epoch 610, val loss: 0.8937614560127258
Epoch 620, training loss: 0.017071817070245743 = 0.010020768269896507 + 0.001 * 7.05104923248291
Epoch 620, val loss: 0.9014778733253479
Epoch 630, training loss: 0.01653130166232586 = 0.00947884377092123 + 0.001 * 7.052457332611084
Epoch 630, val loss: 0.9089913964271545
Epoch 640, training loss: 0.016009559854865074 = 0.008979865349829197 + 0.001 * 7.029694080352783
Epoch 640, val loss: 0.916299045085907
Epoch 650, training loss: 0.015587829984724522 = 0.008519548922777176 + 0.001 * 7.0682806968688965
Epoch 650, val loss: 0.9234184622764587
Epoch 660, training loss: 0.015126949176192284 = 0.0080941841006279 + 0.001 * 7.0327653884887695
Epoch 660, val loss: 0.9303513169288635
Epoch 670, training loss: 0.0147202517837286 = 0.00770044419914484 + 0.001 * 7.0198073387146
Epoch 670, val loss: 0.9370935559272766
Epoch 680, training loss: 0.014364413917064667 = 0.007335479836910963 + 0.001 * 7.028934001922607
Epoch 680, val loss: 0.9436722993850708
Epoch 690, training loss: 0.014029469341039658 = 0.00699653523042798 + 0.001 * 7.032933235168457
Epoch 690, val loss: 0.9500650763511658
Epoch 700, training loss: 0.013701977208256721 = 0.006681294180452824 + 0.001 * 7.0206828117370605
Epoch 700, val loss: 0.9562825560569763
Epoch 710, training loss: 0.013427945785224438 = 0.006387573201209307 + 0.001 * 7.040372371673584
Epoch 710, val loss: 0.9623420834541321
Epoch 720, training loss: 0.01313999854028225 = 0.006113590206950903 + 0.001 * 7.026408672332764
Epoch 720, val loss: 0.9682705402374268
Epoch 730, training loss: 0.012855272740125656 = 0.005857617594301701 + 0.001 * 6.997654914855957
Epoch 730, val loss: 0.9740346074104309
Epoch 740, training loss: 0.012618263252079487 = 0.005618205294013023 + 0.001 * 7.000057697296143
Epoch 740, val loss: 0.9796730279922485
Epoch 750, training loss: 0.012442934326827526 = 0.005393965169787407 + 0.001 * 7.04896879196167
Epoch 750, val loss: 0.9851552248001099
Epoch 760, training loss: 0.012184214778244495 = 0.005183690693229437 + 0.001 * 7.000523567199707
Epoch 760, val loss: 0.9905189871788025
Epoch 770, training loss: 0.011980407871305943 = 0.004986222367733717 + 0.001 * 6.994184970855713
Epoch 770, val loss: 0.9957590699195862
Epoch 780, training loss: 0.011801309883594513 = 0.004800647962838411 + 0.001 * 7.000661849975586
Epoch 780, val loss: 1.0008538961410522
Epoch 790, training loss: 0.011629289016127586 = 0.004626000765711069 + 0.001 * 7.003287315368652
Epoch 790, val loss: 1.0058469772338867
Epoch 800, training loss: 0.011438723653554916 = 0.004461393225938082 + 0.001 * 6.977329730987549
Epoch 800, val loss: 1.0106956958770752
Epoch 810, training loss: 0.01132780872285366 = 0.0043061235919594765 + 0.001 * 7.021684646606445
Epoch 810, val loss: 1.0154229402542114
Epoch 820, training loss: 0.011128135025501251 = 0.004159518051892519 + 0.001 * 6.968616962432861
Epoch 820, val loss: 1.0200543403625488
Epoch 830, training loss: 0.011003188788890839 = 0.00402092793956399 + 0.001 * 6.982260227203369
Epoch 830, val loss: 1.0245670080184937
Epoch 840, training loss: 0.010874171741306782 = 0.00388981681317091 + 0.001 * 6.984354496002197
Epoch 840, val loss: 1.028983473777771
Epoch 850, training loss: 0.010752678848803043 = 0.0037656393833458424 + 0.001 * 6.987039089202881
Epoch 850, val loss: 1.0332903861999512
Epoch 860, training loss: 0.010627005249261856 = 0.0036478897091001272 + 0.001 * 6.9791154861450195
Epoch 860, val loss: 1.037489652633667
Epoch 870, training loss: 0.01052200235426426 = 0.0035362066701054573 + 0.001 * 6.985795021057129
Epoch 870, val loss: 1.0416122674942017
Epoch 880, training loss: 0.01039925403892994 = 0.003430146723985672 + 0.001 * 6.969107627868652
Epoch 880, val loss: 1.0456197261810303
Epoch 890, training loss: 0.010303657501935959 = 0.003329351544380188 + 0.001 * 6.974306106567383
Epoch 890, val loss: 1.0495469570159912
Epoch 900, training loss: 0.010199522599577904 = 0.0032334711868315935 + 0.001 * 6.96605110168457
Epoch 900, val loss: 1.0533883571624756
Epoch 910, training loss: 0.010107364505529404 = 0.0031422143802046776 + 0.001 * 6.965149402618408
Epoch 910, val loss: 1.0571372509002686
Epoch 920, training loss: 0.01002767775207758 = 0.0030552868265658617 + 0.001 * 6.972390651702881
Epoch 920, val loss: 1.0608235597610474
Epoch 930, training loss: 0.009948795661330223 = 0.0029724366031587124 + 0.001 * 6.976358890533447
Epoch 930, val loss: 1.0644174814224243
Epoch 940, training loss: 0.009874463081359863 = 0.002893370809033513 + 0.001 * 6.9810919761657715
Epoch 940, val loss: 1.0679295063018799
Epoch 950, training loss: 0.009771150536835194 = 0.0028179052751511335 + 0.001 * 6.953244686126709
Epoch 950, val loss: 1.0713598728179932
Epoch 960, training loss: 0.009706614539027214 = 0.0027457945980131626 + 0.001 * 6.960819244384766
Epoch 960, val loss: 1.0747326612472534
Epoch 970, training loss: 0.009638526476919651 = 0.0026768825482577085 + 0.001 * 6.961643218994141
Epoch 970, val loss: 1.0780292749404907
Epoch 980, training loss: 0.009550225920975208 = 0.002610954223200679 + 0.001 * 6.939271450042725
Epoch 980, val loss: 1.0812482833862305
Epoch 990, training loss: 0.009494757279753685 = 0.0025478568859398365 + 0.001 * 6.946900367736816
Epoch 990, val loss: 1.084404706954956
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.8328940432261466
The final CL Acc:0.81852, 0.01684, The final GNN Acc:0.83377, 0.00245
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9538])
updated graph: torch.Size([2, 10556])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9780588150024414 = 1.9694619178771973 + 0.001 * 8.596848487854004
Epoch 0, val loss: 1.9611965417861938
Epoch 10, training loss: 1.966781497001648 = 1.9581847190856934 + 0.001 * 8.59677505493164
Epoch 10, val loss: 1.950616717338562
Epoch 20, training loss: 1.9528056383132935 = 1.944209098815918 + 0.001 * 8.596565246582031
Epoch 20, val loss: 1.9373044967651367
Epoch 30, training loss: 1.9332834482192993 = 1.924687385559082 + 0.001 * 8.596027374267578
Epoch 30, val loss: 1.9185445308685303
Epoch 40, training loss: 1.9046519994735718 = 1.8960574865341187 + 0.001 * 8.59454345703125
Epoch 40, val loss: 1.8911489248275757
Epoch 50, training loss: 1.864891767501831 = 1.856302261352539 + 0.001 * 8.589447021484375
Epoch 50, val loss: 1.8548049926757812
Epoch 60, training loss: 1.8208109140396118 = 1.812243938446045 + 0.001 * 8.567017555236816
Epoch 60, val loss: 1.8190455436706543
Epoch 70, training loss: 1.786448359489441 = 1.77798593044281 + 0.001 * 8.462379455566406
Epoch 70, val loss: 1.793459177017212
Epoch 80, training loss: 1.74875807762146 = 1.740618348121643 + 0.001 * 8.139738082885742
Epoch 80, val loss: 1.7595694065093994
Epoch 90, training loss: 1.69683837890625 = 1.6888023614883423 + 0.001 * 8.035969734191895
Epoch 90, val loss: 1.7152678966522217
Epoch 100, training loss: 1.6256283521652222 = 1.6176738739013672 + 0.001 * 7.954533576965332
Epoch 100, val loss: 1.657910704612732
Epoch 110, training loss: 1.5367212295532227 = 1.5288712978363037 + 0.001 * 7.849879741668701
Epoch 110, val loss: 1.5874505043029785
Epoch 120, training loss: 1.4424939155578613 = 1.4348427057266235 + 0.001 * 7.651196002960205
Epoch 120, val loss: 1.5149133205413818
Epoch 130, training loss: 1.3504292964935303 = 1.3428800106048584 + 0.001 * 7.549309253692627
Epoch 130, val loss: 1.445832371711731
Epoch 140, training loss: 1.258487343788147 = 1.250961422920227 + 0.001 * 7.525899887084961
Epoch 140, val loss: 1.3782203197479248
Epoch 150, training loss: 1.1633116006851196 = 1.1558345556259155 + 0.001 * 7.477067947387695
Epoch 150, val loss: 1.3088924884796143
Epoch 160, training loss: 1.0654356479644775 = 1.0580215454101562 + 0.001 * 7.414058208465576
Epoch 160, val loss: 1.2379932403564453
Epoch 170, training loss: 0.9686069488525391 = 0.9612413048744202 + 0.001 * 7.365657806396484
Epoch 170, val loss: 1.1681214570999146
Epoch 180, training loss: 0.8774768710136414 = 0.8701265454292297 + 0.001 * 7.350313186645508
Epoch 180, val loss: 1.1028486490249634
Epoch 190, training loss: 0.796006977558136 = 0.7886663675308228 + 0.001 * 7.3406147956848145
Epoch 190, val loss: 1.0452420711517334
Epoch 200, training loss: 0.7255825400352478 = 0.7182526588439941 + 0.001 * 7.329868316650391
Epoch 200, val loss: 0.9972114562988281
Epoch 210, training loss: 0.664387047290802 = 0.6570719480514526 + 0.001 * 7.315075397491455
Epoch 210, val loss: 0.9578487873077393
Epoch 220, training loss: 0.6087352633476257 = 0.6014441251754761 + 0.001 * 7.291136741638184
Epoch 220, val loss: 0.9247395396232605
Epoch 230, training loss: 0.5552958250045776 = 0.5480400323867798 + 0.001 * 7.255772590637207
Epoch 230, val loss: 0.8955954313278198
Epoch 240, training loss: 0.5023564696311951 = 0.49513259530067444 + 0.001 * 7.223890781402588
Epoch 240, val loss: 0.8694955706596375
Epoch 250, training loss: 0.4499801993370056 = 0.44277849793434143 + 0.001 * 7.201691150665283
Epoch 250, val loss: 0.8470792174339294
Epoch 260, training loss: 0.3995005190372467 = 0.39231395721435547 + 0.001 * 7.186553478240967
Epoch 260, val loss: 0.8294287919998169
Epoch 270, training loss: 0.35248008370399475 = 0.34529781341552734 + 0.001 * 7.182272911071777
Epoch 270, val loss: 0.8170028924942017
Epoch 280, training loss: 0.3098827004432678 = 0.3027019798755646 + 0.001 * 7.180721759796143
Epoch 280, val loss: 0.8099905252456665
Epoch 290, training loss: 0.2718128263950348 = 0.2646327316761017 + 0.001 * 7.180084228515625
Epoch 290, val loss: 0.8072896599769592
Epoch 300, training loss: 0.2379380762577057 = 0.2307591736316681 + 0.001 * 7.178899765014648
Epoch 300, val loss: 0.8079745769500732
Epoch 310, training loss: 0.20789581537246704 = 0.20071524381637573 + 0.001 * 7.18056583404541
Epoch 310, val loss: 0.8113937973976135
Epoch 320, training loss: 0.181401789188385 = 0.1742246448993683 + 0.001 * 7.177150726318359
Epoch 320, val loss: 0.8170239329338074
Epoch 330, training loss: 0.1582196205854416 = 0.15104323625564575 + 0.001 * 7.176383972167969
Epoch 330, val loss: 0.8245022892951965
Epoch 340, training loss: 0.13808773458003998 = 0.1309128701686859 + 0.001 * 7.174861431121826
Epoch 340, val loss: 0.8334230780601501
Epoch 350, training loss: 0.12072443962097168 = 0.11355029791593552 + 0.001 * 7.1741437911987305
Epoch 350, val loss: 0.843650758266449
Epoch 360, training loss: 0.1058245524764061 = 0.09865006059408188 + 0.001 * 7.174492835998535
Epoch 360, val loss: 0.8548803329467773
Epoch 370, training loss: 0.09307967126369476 = 0.08590665459632874 + 0.001 * 7.1730194091796875
Epoch 370, val loss: 0.8669349551200867
Epoch 380, training loss: 0.08220527321100235 = 0.07503359019756317 + 0.001 * 7.171684265136719
Epoch 380, val loss: 0.8796030282974243
Epoch 390, training loss: 0.07294316589832306 = 0.06577044725418091 + 0.001 * 7.172718524932861
Epoch 390, val loss: 0.8926974534988403
Epoch 400, training loss: 0.065057173371315 = 0.05788431316614151 + 0.001 * 7.172860145568848
Epoch 400, val loss: 0.9059706926345825
Epoch 410, training loss: 0.05834063142538071 = 0.051169391721487045 + 0.001 * 7.171239376068115
Epoch 410, val loss: 0.9193178415298462
Epoch 420, training loss: 0.052614033222198486 = 0.04544421657919884 + 0.001 * 7.169816493988037
Epoch 420, val loss: 0.9325876235961914
Epoch 430, training loss: 0.04771997779607773 = 0.04055161774158478 + 0.001 * 7.168360233306885
Epoch 430, val loss: 0.9456813335418701
Epoch 440, training loss: 0.043524861335754395 = 0.03635740280151367 + 0.001 * 7.167457103729248
Epoch 440, val loss: 0.9585484266281128
Epoch 450, training loss: 0.0399152934551239 = 0.032748326659202576 + 0.001 * 7.166964530944824
Epoch 450, val loss: 0.9710507988929749
Epoch 460, training loss: 0.0367952361702919 = 0.02963034063577652 + 0.001 * 7.164894104003906
Epoch 460, val loss: 0.9832013249397278
Epoch 470, training loss: 0.03408648446202278 = 0.02692469395697117 + 0.001 * 7.1617889404296875
Epoch 470, val loss: 0.9949806332588196
Epoch 480, training loss: 0.03172888606786728 = 0.024566346779465675 + 0.001 * 7.162540435791016
Epoch 480, val loss: 1.006322979927063
Epoch 490, training loss: 0.029659921303391457 = 0.02250167541205883 + 0.001 * 7.158246040344238
Epoch 490, val loss: 1.0173003673553467
Epoch 500, training loss: 0.02784639596939087 = 0.02068609930574894 + 0.001 * 7.1602959632873535
Epoch 500, val loss: 1.0278767347335815
Epoch 510, training loss: 0.02623452991247177 = 0.01908256858587265 + 0.001 * 7.151961326599121
Epoch 510, val loss: 1.0380743741989136
Epoch 520, training loss: 0.024812057614326477 = 0.017660431563854218 + 0.001 * 7.1516265869140625
Epoch 520, val loss: 1.0479140281677246
Epoch 530, training loss: 0.023539606481790543 = 0.01639378070831299 + 0.001 * 7.1458258628845215
Epoch 530, val loss: 1.0573856830596924
Epoch 540, training loss: 0.022408965975046158 = 0.015261318534612656 + 0.001 * 7.147646903991699
Epoch 540, val loss: 1.0665363073349
Epoch 550, training loss: 0.021388262510299683 = 0.014245123602449894 + 0.001 * 7.143138885498047
Epoch 550, val loss: 1.0753945112228394
Epoch 560, training loss: 0.020477116107940674 = 0.013329990208148956 + 0.001 * 7.1471266746521
Epoch 560, val loss: 1.083917498588562
Epoch 570, training loss: 0.019641172140836716 = 0.01250314898788929 + 0.001 * 7.138021945953369
Epoch 570, val loss: 1.0921252965927124
Epoch 580, training loss: 0.018926342949271202 = 0.011753703467547894 + 0.001 * 7.1726393699646
Epoch 580, val loss: 1.1000782251358032
Epoch 590, training loss: 0.0182068832218647 = 0.011072306893765926 + 0.001 * 7.134575843811035
Epoch 590, val loss: 1.1077433824539185
Epoch 600, training loss: 0.017587587237358093 = 0.010451067239046097 + 0.001 * 7.1365203857421875
Epoch 600, val loss: 1.1151821613311768
Epoch 610, training loss: 0.017012929543852806 = 0.009883059188723564 + 0.001 * 7.129870414733887
Epoch 610, val loss: 1.1223669052124023
Epoch 620, training loss: 0.016506990417838097 = 0.009362447075545788 + 0.001 * 7.144543170928955
Epoch 620, val loss: 1.1293246746063232
Epoch 630, training loss: 0.016016578301787376 = 0.008884166367352009 + 0.001 * 7.132411479949951
Epoch 630, val loss: 1.1360745429992676
Epoch 640, training loss: 0.015557993203401566 = 0.008443715050816536 + 0.001 * 7.1142778396606445
Epoch 640, val loss: 1.1426169872283936
Epoch 650, training loss: 0.015198443084955215 = 0.00803723931312561 + 0.001 * 7.161203861236572
Epoch 650, val loss: 1.1489489078521729
Epoch 660, training loss: 0.01477344986051321 = 0.007661334704607725 + 0.001 * 7.112114906311035
Epoch 660, val loss: 1.1550871133804321
Epoch 670, training loss: 0.014433671720325947 = 0.007313000503927469 + 0.001 * 7.120670795440674
Epoch 670, val loss: 1.1610445976257324
Epoch 680, training loss: 0.014093178324401379 = 0.0069895717315375805 + 0.001 * 7.103606224060059
Epoch 680, val loss: 1.1668275594711304
Epoch 690, training loss: 0.01379088219255209 = 0.0066888220608234406 + 0.001 * 7.102059841156006
Epoch 690, val loss: 1.1724348068237305
Epoch 700, training loss: 0.013522982597351074 = 0.006408614572137594 + 0.001 * 7.114367961883545
Epoch 700, val loss: 1.1779025793075562
Epoch 710, training loss: 0.013261686079204082 = 0.0061471713706851006 + 0.001 * 7.114514350891113
Epoch 710, val loss: 1.1831834316253662
Epoch 720, training loss: 0.013044812716543674 = 0.0059027825482189655 + 0.001 * 7.142029762268066
Epoch 720, val loss: 1.1883327960968018
Epoch 730, training loss: 0.012788334861397743 = 0.005674068816006184 + 0.001 * 7.114266395568848
Epoch 730, val loss: 1.1933493614196777
Epoch 740, training loss: 0.012567668221890926 = 0.005459647625684738 + 0.001 * 7.108020305633545
Epoch 740, val loss: 1.1981950998306274
Epoch 750, training loss: 0.012365331873297691 = 0.005258331540971994 + 0.001 * 7.107000350952148
Epoch 750, val loss: 1.2029520273208618
Epoch 760, training loss: 0.012182936072349548 = 0.005069111939519644 + 0.001 * 7.113824367523193
Epoch 760, val loss: 1.2075506448745728
Epoch 770, training loss: 0.011973168700933456 = 0.0048909964971244335 + 0.001 * 7.082172393798828
Epoch 770, val loss: 1.212032437324524
Epoch 780, training loss: 0.011788127943873405 = 0.004723190329968929 + 0.001 * 7.064937591552734
Epoch 780, val loss: 1.216403841972351
Epoch 790, training loss: 0.011634882539510727 = 0.004564872942864895 + 0.001 * 7.070008754730225
Epoch 790, val loss: 1.2206482887268066
Epoch 800, training loss: 0.011491529643535614 = 0.004415326751768589 + 0.001 * 7.076201915740967
Epoch 800, val loss: 1.224778175354004
Epoch 810, training loss: 0.011336401104927063 = 0.0042739142663776875 + 0.001 * 7.06248664855957
Epoch 810, val loss: 1.228829264640808
Epoch 820, training loss: 0.011218969710171223 = 0.004140032455325127 + 0.001 * 7.07893705368042
Epoch 820, val loss: 1.2327367067337036
Epoch 830, training loss: 0.011116206645965576 = 0.0040131667628884315 + 0.001 * 7.103039264678955
Epoch 830, val loss: 1.236576795578003
Epoch 840, training loss: 0.010967504233121872 = 0.0038928817957639694 + 0.001 * 7.074622631072998
Epoch 840, val loss: 1.2403054237365723
Epoch 850, training loss: 0.010855451226234436 = 0.0037786453031003475 + 0.001 * 7.076805114746094
Epoch 850, val loss: 1.2439601421356201
Epoch 860, training loss: 0.010733095929026604 = 0.003670122241601348 + 0.001 * 7.062973499298096
Epoch 860, val loss: 1.2475007772445679
Epoch 870, training loss: 0.010611926205456257 = 0.003566903294995427 + 0.001 * 7.045022487640381
Epoch 870, val loss: 1.2509655952453613
Epoch 880, training loss: 0.01052408292889595 = 0.0034686606377363205 + 0.001 * 7.055421352386475
Epoch 880, val loss: 1.2543487548828125
Epoch 890, training loss: 0.010440707206726074 = 0.003375112544745207 + 0.001 * 7.065594673156738
Epoch 890, val loss: 1.257641077041626
Epoch 900, training loss: 0.01033887080848217 = 0.003285893239080906 + 0.001 * 7.052976608276367
Epoch 900, val loss: 1.2608802318572998
Epoch 910, training loss: 0.01025080680847168 = 0.0032007857225835323 + 0.001 * 7.050020694732666
Epoch 910, val loss: 1.2640190124511719
Epoch 920, training loss: 0.010152691975235939 = 0.003119506174698472 + 0.001 * 7.0331854820251465
Epoch 920, val loss: 1.2671077251434326
Epoch 930, training loss: 0.01008437480777502 = 0.0030418746173381805 + 0.001 * 7.042500019073486
Epoch 930, val loss: 1.2701177597045898
Epoch 940, training loss: 0.010009235702455044 = 0.002967618405818939 + 0.001 * 7.041616916656494
Epoch 940, val loss: 1.2730661630630493
Epoch 950, training loss: 0.009926732629537582 = 0.002896602964028716 + 0.001 * 7.0301289558410645
Epoch 950, val loss: 1.2759183645248413
Epoch 960, training loss: 0.009856628254055977 = 0.0028285756707191467 + 0.001 * 7.028052806854248
Epoch 960, val loss: 1.27872633934021
Epoch 970, training loss: 0.009790596552193165 = 0.0027634159196168184 + 0.001 * 7.0271806716918945
Epoch 970, val loss: 1.2814691066741943
Epoch 980, training loss: 0.009734728373587132 = 0.002700899029150605 + 0.001 * 7.033829212188721
Epoch 980, val loss: 1.284156322479248
Epoch 990, training loss: 0.009660055860877037 = 0.0026409681886434555 + 0.001 * 7.019087314605713
Epoch 990, val loss: 1.2867616415023804
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 1.957858920097351 = 1.949262022972107 + 0.001 * 8.59685230255127
Epoch 0, val loss: 1.9521671533584595
Epoch 10, training loss: 1.947325587272644 = 1.9387288093566895 + 0.001 * 8.59678840637207
Epoch 10, val loss: 1.941117525100708
Epoch 20, training loss: 1.9341751337051392 = 1.9255785942077637 + 0.001 * 8.596583366394043
Epoch 20, val loss: 1.927149772644043
Epoch 30, training loss: 1.9157285690307617 = 1.9071325063705444 + 0.001 * 8.596047401428223
Epoch 30, val loss: 1.907568097114563
Epoch 40, training loss: 1.8888778686523438 = 1.8802833557128906 + 0.001 * 8.594552040100098
Epoch 40, val loss: 1.8795652389526367
Epoch 50, training loss: 1.8530395030975342 = 1.844449758529663 + 0.001 * 8.589739799499512
Epoch 50, val loss: 1.8439867496490479
Epoch 60, training loss: 1.8168106079101562 = 1.8082391023635864 + 0.001 * 8.571532249450684
Epoch 60, val loss: 1.812106966972351
Epoch 70, training loss: 1.7867802381515503 = 1.7782878875732422 + 0.001 * 8.492382049560547
Epoch 70, val loss: 1.7876191139221191
Epoch 80, training loss: 1.7473913431167603 = 1.739241600036621 + 0.001 * 8.149683952331543
Epoch 80, val loss: 1.7539750337600708
Epoch 90, training loss: 1.6918184757232666 = 1.683773398399353 + 0.001 * 8.045048713684082
Epoch 90, val loss: 1.7069896459579468
Epoch 100, training loss: 1.615588903427124 = 1.6076163053512573 + 0.001 * 7.972642421722412
Epoch 100, val loss: 1.6443761587142944
Epoch 110, training loss: 1.525830864906311 = 1.5178941488265991 + 0.001 * 7.93672513961792
Epoch 110, val loss: 1.5719678401947021
Epoch 120, training loss: 1.435121774673462 = 1.4272414445877075 + 0.001 * 7.880324840545654
Epoch 120, val loss: 1.5015053749084473
Epoch 130, training loss: 1.3487991094589233 = 1.3410857915878296 + 0.001 * 7.713375091552734
Epoch 130, val loss: 1.4377747774124146
Epoch 140, training loss: 1.2675096988677979 = 1.2599656581878662 + 0.001 * 7.54405403137207
Epoch 140, val loss: 1.381974697113037
Epoch 150, training loss: 1.190433144569397 = 1.182953953742981 + 0.001 * 7.479246616363525
Epoch 150, val loss: 1.3320651054382324
Epoch 160, training loss: 1.1145082712173462 = 1.10708487033844 + 0.001 * 7.423401832580566
Epoch 160, val loss: 1.2852094173431396
Epoch 170, training loss: 1.0361050367355347 = 1.028729796409607 + 0.001 * 7.375271797180176
Epoch 170, val loss: 1.2364941835403442
Epoch 180, training loss: 0.9538306593894958 = 0.946509599685669 + 0.001 * 7.321042060852051
Epoch 180, val loss: 1.1837173700332642
Epoch 190, training loss: 0.8696959614753723 = 0.8624011874198914 + 0.001 * 7.294784069061279
Epoch 190, val loss: 1.1288025379180908
Epoch 200, training loss: 0.7878660559654236 = 0.7805839776992798 + 0.001 * 7.282104015350342
Epoch 200, val loss: 1.0758994817733765
Epoch 210, training loss: 0.7122841477394104 = 0.7050185203552246 + 0.001 * 7.265620708465576
Epoch 210, val loss: 1.0292607545852661
Epoch 220, training loss: 0.6447829008102417 = 0.6375406980514526 + 0.001 * 7.242231845855713
Epoch 220, val loss: 0.9917213916778564
Epoch 230, training loss: 0.5849395394325256 = 0.5777243971824646 + 0.001 * 7.215116024017334
Epoch 230, val loss: 0.9635170102119446
Epoch 240, training loss: 0.5312967896461487 = 0.5241068005561829 + 0.001 * 7.189960956573486
Epoch 240, val loss: 0.9437569975852966
Epoch 250, training loss: 0.4822130501270294 = 0.4750460982322693 + 0.001 * 7.166961193084717
Epoch 250, val loss: 0.9307806491851807
Epoch 260, training loss: 0.4363662600517273 = 0.4292108416557312 + 0.001 * 7.155426979064941
Epoch 260, val loss: 0.9230257868766785
Epoch 270, training loss: 0.3928753733634949 = 0.3857308626174927 + 0.001 * 7.144518852233887
Epoch 270, val loss: 0.9189452528953552
Epoch 280, training loss: 0.35154110193252563 = 0.34440287947654724 + 0.001 * 7.138220310211182
Epoch 280, val loss: 0.9175043106079102
Epoch 290, training loss: 0.31262290477752686 = 0.30548858642578125 + 0.001 * 7.134317398071289
Epoch 290, val loss: 0.9182015657424927
Epoch 300, training loss: 0.27651384472846985 = 0.2693817913532257 + 0.001 * 7.132051467895508
Epoch 300, val loss: 0.920894205570221
Epoch 310, training loss: 0.24354727566242218 = 0.23641888797283173 + 0.001 * 7.128388404846191
Epoch 310, val loss: 0.9257044792175293
Epoch 320, training loss: 0.21390964090824127 = 0.20678246021270752 + 0.001 * 7.127180099487305
Epoch 320, val loss: 0.9327178597450256
Epoch 330, training loss: 0.18763381242752075 = 0.1805114597082138 + 0.001 * 7.122350215911865
Epoch 330, val loss: 0.9418620467185974
Epoch 340, training loss: 0.16463039815425873 = 0.1575116217136383 + 0.001 * 7.118777751922607
Epoch 340, val loss: 0.9531084895133972
Epoch 350, training loss: 0.14468753337860107 = 0.13757199048995972 + 0.001 * 7.115545749664307
Epoch 350, val loss: 0.9661770462989807
Epoch 360, training loss: 0.1275036633014679 = 0.12039203941822052 + 0.001 * 7.111619472503662
Epoch 360, val loss: 0.9807252287864685
Epoch 370, training loss: 0.1127345934510231 = 0.10562703013420105 + 0.001 * 7.107563018798828
Epoch 370, val loss: 0.9964023232460022
Epoch 380, training loss: 0.10006500035524368 = 0.0929439514875412 + 0.001 * 7.121048927307129
Epoch 380, val loss: 1.0127835273742676
Epoch 390, training loss: 0.08913726359605789 = 0.0820336788892746 + 0.001 * 7.103587627410889
Epoch 390, val loss: 1.0295262336730957
Epoch 400, training loss: 0.07972513884305954 = 0.07262495905160904 + 0.001 * 7.1001811027526855
Epoch 400, val loss: 1.0464129447937012
Epoch 410, training loss: 0.07159087806940079 = 0.06449204683303833 + 0.001 * 7.098829746246338
Epoch 410, val loss: 1.063191533088684
Epoch 420, training loss: 0.06454536318778992 = 0.057444702833890915 + 0.001 * 7.100658416748047
Epoch 420, val loss: 1.0797404050827026
Epoch 430, training loss: 0.05842532962560654 = 0.0513308122754097 + 0.001 * 7.094515800476074
Epoch 430, val loss: 1.095937728881836
Epoch 440, training loss: 0.05311928316950798 = 0.04602067917585373 + 0.001 * 7.098604679107666
Epoch 440, val loss: 1.1116561889648438
Epoch 450, training loss: 0.048496730625629425 = 0.041404105722904205 + 0.001 * 7.092626094818115
Epoch 450, val loss: 1.1268895864486694
Epoch 460, training loss: 0.044472940266132355 = 0.03738446533679962 + 0.001 * 7.088476657867432
Epoch 460, val loss: 1.1415637731552124
Epoch 470, training loss: 0.04096515476703644 = 0.033877547830343246 + 0.001 * 7.087604999542236
Epoch 470, val loss: 1.1556874513626099
Epoch 480, training loss: 0.03789759427309036 = 0.030809540301561356 + 0.001 * 7.08805513381958
Epoch 480, val loss: 1.1692782640457153
Epoch 490, training loss: 0.03520415723323822 = 0.028117230162024498 + 0.001 * 7.086928367614746
Epoch 490, val loss: 1.1823264360427856
Epoch 500, training loss: 0.032829269766807556 = 0.025747058913111687 + 0.001 * 7.082211494445801
Epoch 500, val loss: 1.1948728561401367
Epoch 510, training loss: 0.030732449144124985 = 0.023653628304600716 + 0.001 * 7.078821659088135
Epoch 510, val loss: 1.2068573236465454
Epoch 520, training loss: 0.02888873778283596 = 0.02179788053035736 + 0.001 * 7.09085750579834
Epoch 520, val loss: 1.2183911800384521
Epoch 530, training loss: 0.027229806408286095 = 0.02014736458659172 + 0.001 * 7.0824408531188965
Epoch 530, val loss: 1.229436993598938
Epoch 540, training loss: 0.025750447064638138 = 0.01867448352277279 + 0.001 * 7.075962066650391
Epoch 540, val loss: 1.2400462627410889
Epoch 550, training loss: 0.024428414180874825 = 0.017355618998408318 + 0.001 * 7.0727949142456055
Epoch 550, val loss: 1.2502446174621582
Epoch 560, training loss: 0.023242834955453873 = 0.016170985996723175 + 0.001 * 7.071849346160889
Epoch 560, val loss: 1.2600516080856323
Epoch 570, training loss: 0.02217436209321022 = 0.015103689394891262 + 0.001 * 7.070672035217285
Epoch 570, val loss: 1.2694871425628662
Epoch 580, training loss: 0.021213475614786148 = 0.01413937471807003 + 0.001 * 7.074100494384766
Epoch 580, val loss: 1.2786012887954712
Epoch 590, training loss: 0.020336437970399857 = 0.013265613466501236 + 0.001 * 7.0708231925964355
Epoch 590, val loss: 1.2873824834823608
Epoch 600, training loss: 0.01953752711415291 = 0.012471665628254414 + 0.001 * 7.06586217880249
Epoch 600, val loss: 1.2958430051803589
Epoch 610, training loss: 0.018846439197659492 = 0.011748392134904861 + 0.001 * 7.098047256469727
Epoch 610, val loss: 1.3040395975112915
Epoch 620, training loss: 0.018160797655582428 = 0.011087861843407154 + 0.001 * 7.072935581207275
Epoch 620, val loss: 1.3119434118270874
Epoch 630, training loss: 0.017546048387885094 = 0.010483122430741787 + 0.001 * 7.062925338745117
Epoch 630, val loss: 1.3195616006851196
Epoch 640, training loss: 0.01699114963412285 = 0.009928159415721893 + 0.001 * 7.062988758087158
Epoch 640, val loss: 1.3269355297088623
Epoch 650, training loss: 0.016479063779115677 = 0.009417944587767124 + 0.001 * 7.0611186027526855
Epoch 650, val loss: 1.3340543508529663
Epoch 660, training loss: 0.01601366698741913 = 0.008947844617068768 + 0.001 * 7.065822124481201
Epoch 660, val loss: 1.3409425020217896
Epoch 670, training loss: 0.015572646632790565 = 0.008513763546943665 + 0.001 * 7.058882236480713
Epoch 670, val loss: 1.347637414932251
Epoch 680, training loss: 0.01517654862254858 = 0.008112161420285702 + 0.001 * 7.06438684463501
Epoch 680, val loss: 1.3540972471237183
Epoch 690, training loss: 0.014797640964388847 = 0.007739943917840719 + 0.001 * 7.057696342468262
Epoch 690, val loss: 1.3603851795196533
Epoch 700, training loss: 0.014455212280154228 = 0.00739429984241724 + 0.001 * 7.060911655426025
Epoch 700, val loss: 1.3664734363555908
Epoch 710, training loss: 0.014126095920801163 = 0.007072756998240948 + 0.001 * 7.053338527679443
Epoch 710, val loss: 1.3723665475845337
Epoch 720, training loss: 0.013832531869411469 = 0.00677314680069685 + 0.001 * 7.059384822845459
Epoch 720, val loss: 1.3781075477600098
Epoch 730, training loss: 0.013546030968427658 = 0.006493551656603813 + 0.001 * 7.052478790283203
Epoch 730, val loss: 1.3836548328399658
Epoch 740, training loss: 0.013286637142300606 = 0.006232208572328091 + 0.001 * 7.054427623748779
Epoch 740, val loss: 1.3890488147735596
Epoch 750, training loss: 0.013045299798250198 = 0.005987586919218302 + 0.001 * 7.057712078094482
Epoch 750, val loss: 1.3942995071411133
Epoch 760, training loss: 0.012810258194804192 = 0.005758256185799837 + 0.001 * 7.052001476287842
Epoch 760, val loss: 1.399409294128418
Epoch 770, training loss: 0.012591393664479256 = 0.005542495287954807 + 0.001 * 7.048898696899414
Epoch 770, val loss: 1.4043481349945068
Epoch 780, training loss: 0.012390593066811562 = 0.005338023416697979 + 0.001 * 7.0525689125061035
Epoch 780, val loss: 1.409131646156311
Epoch 790, training loss: 0.012188034132122993 = 0.005142565350979567 + 0.001 * 7.045467853546143
Epoch 790, val loss: 1.4137935638427734
Epoch 800, training loss: 0.012011583894491196 = 0.004954903852194548 + 0.001 * 7.056680202484131
Epoch 800, val loss: 1.4183437824249268
Epoch 810, training loss: 0.011823656037449837 = 0.004774749744683504 + 0.001 * 7.048905849456787
Epoch 810, val loss: 1.4228085279464722
Epoch 820, training loss: 0.011643340811133385 = 0.004602237604558468 + 0.001 * 7.041102886199951
Epoch 820, val loss: 1.4271544218063354
Epoch 830, training loss: 0.011501500383019447 = 0.004437488038092852 + 0.001 * 7.064011573791504
Epoch 830, val loss: 1.431402564048767
Epoch 840, training loss: 0.011321302503347397 = 0.0042805480770766735 + 0.001 * 7.040754318237305
Epoch 840, val loss: 1.4355814456939697
Epoch 850, training loss: 0.011171610094606876 = 0.004131329711526632 + 0.001 * 7.040279865264893
Epoch 850, val loss: 1.4396281242370605
Epoch 860, training loss: 0.011041063815355301 = 0.0039896066300570965 + 0.001 * 7.051456928253174
Epoch 860, val loss: 1.4436697959899902
Epoch 870, training loss: 0.010896487161517143 = 0.0038551182951778173 + 0.001 * 7.0413689613342285
Epoch 870, val loss: 1.4475066661834717
Epoch 880, training loss: 0.01076391153037548 = 0.0037275832146406174 + 0.001 * 7.036327362060547
Epoch 880, val loss: 1.451319694519043
Epoch 890, training loss: 0.01064596138894558 = 0.003606615122407675 + 0.001 * 7.0393452644348145
Epoch 890, val loss: 1.4550223350524902
Epoch 900, training loss: 0.010532534681260586 = 0.0034918375313282013 + 0.001 * 7.040696620941162
Epoch 900, val loss: 1.458645224571228
Epoch 910, training loss: 0.01042194664478302 = 0.0033828881569206715 + 0.001 * 7.039058685302734
Epoch 910, val loss: 1.462231993675232
Epoch 920, training loss: 0.010311289690434933 = 0.00327937095426023 + 0.001 * 7.031918525695801
Epoch 920, val loss: 1.4657219648361206
Epoch 930, training loss: 0.010214357636868954 = 0.003180829808115959 + 0.001 * 7.033527374267578
Epoch 930, val loss: 1.4691405296325684
Epoch 940, training loss: 0.010127214714884758 = 0.0030868512112647295 + 0.001 * 7.040363311767578
Epoch 940, val loss: 1.4724829196929932
Epoch 950, training loss: 0.010036421939730644 = 0.0029969944152981043 + 0.001 * 7.039426803588867
Epoch 950, val loss: 1.4757988452911377
Epoch 960, training loss: 0.009939393028616905 = 0.0029108263552188873 + 0.001 * 7.028566837310791
Epoch 960, val loss: 1.4790313243865967
Epoch 970, training loss: 0.009856460615992546 = 0.002827979624271393 + 0.001 * 7.028480052947998
Epoch 970, val loss: 1.4822187423706055
Epoch 980, training loss: 0.009775349870324135 = 0.0027482653968036175 + 0.001 * 7.027083873748779
Epoch 980, val loss: 1.4853172302246094
Epoch 990, training loss: 0.009695028886198997 = 0.0026714876294136047 + 0.001 * 7.023541450500488
Epoch 990, val loss: 1.488445520401001
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 1.9671322107315063 = 1.9585353136062622 + 0.001 * 8.59683895111084
Epoch 0, val loss: 1.9661587476730347
Epoch 10, training loss: 1.9562010765075684 = 1.9476042985916138 + 0.001 * 8.596765518188477
Epoch 10, val loss: 1.9547711610794067
Epoch 20, training loss: 1.9425188302993774 = 1.933922290802002 + 0.001 * 8.596532821655273
Epoch 20, val loss: 1.9402642250061035
Epoch 30, training loss: 1.923171043395996 = 1.9145750999450684 + 0.001 * 8.595993995666504
Epoch 30, val loss: 1.9195942878723145
Epoch 40, training loss: 1.894777536392212 = 1.8861829042434692 + 0.001 * 8.594666481018066
Epoch 40, val loss: 1.8894716501235962
Epoch 50, training loss: 1.8561745882034302 = 1.8475837707519531 + 0.001 * 8.590848922729492
Epoch 50, val loss: 1.8500949144363403
Epoch 60, training loss: 1.8164602518081665 = 1.8078821897506714 + 0.001 * 8.578030586242676
Epoch 60, val loss: 1.8135149478912354
Epoch 70, training loss: 1.7882636785507202 = 1.779733419418335 + 0.001 * 8.530256271362305
Epoch 70, val loss: 1.7899302244186401
Epoch 80, training loss: 1.7524127960205078 = 1.7441469430923462 + 0.001 * 8.2658052444458
Epoch 80, val loss: 1.7588214874267578
Epoch 90, training loss: 1.7022360563278198 = 1.6941393613815308 + 0.001 * 8.096736907958984
Epoch 90, val loss: 1.716672658920288
Epoch 100, training loss: 1.632356882095337 = 1.6243321895599365 + 0.001 * 8.024746894836426
Epoch 100, val loss: 1.6586941480636597
Epoch 110, training loss: 1.5479238033294678 = 1.5399420261383057 + 0.001 * 7.981720447540283
Epoch 110, val loss: 1.5905698537826538
Epoch 120, training loss: 1.4611488580703735 = 1.453233242034912 + 0.001 * 7.915633201599121
Epoch 120, val loss: 1.5230478048324585
Epoch 130, training loss: 1.3775273561477661 = 1.369820475578308 + 0.001 * 7.706853866577148
Epoch 130, val loss: 1.4612319469451904
Epoch 140, training loss: 1.294936180114746 = 1.2874892950057983 + 0.001 * 7.446835517883301
Epoch 140, val loss: 1.4031119346618652
Epoch 150, training loss: 1.2102047204971313 = 1.202783226966858 + 0.001 * 7.4214348793029785
Epoch 150, val loss: 1.344537615776062
Epoch 160, training loss: 1.1221234798431396 = 1.1147253513336182 + 0.001 * 7.398069381713867
Epoch 160, val loss: 1.2845083475112915
Epoch 170, training loss: 1.0319859981536865 = 1.02461576461792 + 0.001 * 7.370278358459473
Epoch 170, val loss: 1.2226520776748657
Epoch 180, training loss: 0.9423837661743164 = 0.9350354075431824 + 0.001 * 7.348388195037842
Epoch 180, val loss: 1.1598129272460938
Epoch 190, training loss: 0.8564552664756775 = 0.8491227626800537 + 0.001 * 7.332480430603027
Epoch 190, val loss: 1.0981833934783936
Epoch 200, training loss: 0.7767974734306335 = 0.7694761157035828 + 0.001 * 7.321343898773193
Epoch 200, val loss: 1.041472315788269
Epoch 210, training loss: 0.7049461603164673 = 0.6976326704025269 + 0.001 * 7.3135151863098145
Epoch 210, val loss: 0.9923704862594604
Epoch 220, training loss: 0.6410347819328308 = 0.6337257027626038 + 0.001 * 7.30910587310791
Epoch 220, val loss: 0.9523963928222656
Epoch 230, training loss: 0.5842036604881287 = 0.5768970251083374 + 0.001 * 7.306613922119141
Epoch 230, val loss: 0.9220694899559021
Epoch 240, training loss: 0.5335169434547424 = 0.526212215423584 + 0.001 * 7.304734230041504
Epoch 240, val loss: 0.9009287357330322
Epoch 250, training loss: 0.488248735666275 = 0.4809459149837494 + 0.001 * 7.302832126617432
Epoch 250, val loss: 0.8879985809326172
Epoch 260, training loss: 0.4473276138305664 = 0.4400273561477661 + 0.001 * 7.300263404846191
Epoch 260, val loss: 0.8813195824623108
Epoch 270, training loss: 0.4094005823135376 = 0.40210410952568054 + 0.001 * 7.29647970199585
Epoch 270, val loss: 0.8786566257476807
Epoch 280, training loss: 0.3731965720653534 = 0.36590564250946045 + 0.001 * 7.290937423706055
Epoch 280, val loss: 0.8779224753379822
Epoch 290, training loss: 0.33792901039123535 = 0.3306458592414856 + 0.001 * 7.283138275146484
Epoch 290, val loss: 0.8779661655426025
Epoch 300, training loss: 0.3033144176006317 = 0.29604241251945496 + 0.001 * 7.272007942199707
Epoch 300, val loss: 0.8780789375305176
Epoch 310, training loss: 0.26949238777160645 = 0.2622324228286743 + 0.001 * 7.259975910186768
Epoch 310, val loss: 0.8779964447021484
Epoch 320, training loss: 0.2370138168334961 = 0.22977425158023834 + 0.001 * 7.239566326141357
Epoch 320, val loss: 0.8778908848762512
Epoch 330, training loss: 0.2066686451435089 = 0.19944500923156738 + 0.001 * 7.2236328125
Epoch 330, val loss: 0.8788008093833923
Epoch 340, training loss: 0.17921565473079681 = 0.17201340198516846 + 0.001 * 7.202258586883545
Epoch 340, val loss: 0.8813774585723877
Epoch 350, training loss: 0.15512806177139282 = 0.14792950451374054 + 0.001 * 7.198554515838623
Epoch 350, val loss: 0.8864115476608276
Epoch 360, training loss: 0.1343734711408615 = 0.12720298767089844 + 0.001 * 7.1704840660095215
Epoch 360, val loss: 0.8938845992088318
Epoch 370, training loss: 0.11674551665782928 = 0.10956962406635284 + 0.001 * 7.175893783569336
Epoch 370, val loss: 0.9033910036087036
Epoch 380, training loss: 0.10181450843811035 = 0.09464641660451889 + 0.001 * 7.168090343475342
Epoch 380, val loss: 0.9144869446754456
Epoch 390, training loss: 0.08918775618076324 = 0.08203373849391937 + 0.001 * 7.154018402099609
Epoch 390, val loss: 0.9269132614135742
Epoch 400, training loss: 0.07852578163146973 = 0.07138139754533768 + 0.001 * 7.144383907318115
Epoch 400, val loss: 0.9402546286582947
Epoch 410, training loss: 0.06953178346157074 = 0.062382739037275314 + 0.001 * 7.149045944213867
Epoch 410, val loss: 0.9543491005897522
Epoch 420, training loss: 0.06192096695303917 = 0.054775647819042206 + 0.001 * 7.145318508148193
Epoch 420, val loss: 0.9687946438789368
Epoch 430, training loss: 0.055467430502176285 = 0.048334814608097076 + 0.001 * 7.132615566253662
Epoch 430, val loss: 0.9834648966789246
Epoch 440, training loss: 0.04999607801437378 = 0.04286942631006241 + 0.001 * 7.126652717590332
Epoch 440, val loss: 0.998100996017456
Epoch 450, training loss: 0.04535819962620735 = 0.03821546211838722 + 0.001 * 7.14273738861084
Epoch 450, val loss: 1.0126259326934814
Epoch 460, training loss: 0.04136389121413231 = 0.03423616662621498 + 0.001 * 7.1277241706848145
Epoch 460, val loss: 1.0268566608428955
Epoch 470, training loss: 0.03792911767959595 = 0.030818212777376175 + 0.001 * 7.110903739929199
Epoch 470, val loss: 1.0407181978225708
Epoch 480, training loss: 0.034976776689291 = 0.027868738397955894 + 0.001 * 7.108038425445557
Epoch 480, val loss: 1.0542134046554565
Epoch 490, training loss: 0.03242161497473717 = 0.025311795994639397 + 0.001 * 7.109817028045654
Epoch 490, val loss: 1.0672985315322876
Epoch 500, training loss: 0.030195558443665504 = 0.02308463118970394 + 0.001 * 7.110927104949951
Epoch 500, val loss: 1.0799306631088257
Epoch 510, training loss: 0.02823050320148468 = 0.02113550528883934 + 0.001 * 7.094998359680176
Epoch 510, val loss: 1.09214448928833
Epoch 520, training loss: 0.026518944650888443 = 0.019422244280576706 + 0.001 * 7.096701145172119
Epoch 520, val loss: 1.1038843393325806
Epoch 530, training loss: 0.02500069886445999 = 0.017909912392497063 + 0.001 * 7.090786933898926
Epoch 530, val loss: 1.115222692489624
Epoch 540, training loss: 0.02367461659014225 = 0.016569258645176888 + 0.001 * 7.105357646942139
Epoch 540, val loss: 1.1261454820632935
Epoch 550, training loss: 0.02245911955833435 = 0.015376118943095207 + 0.001 * 7.082999229431152
Epoch 550, val loss: 1.1366732120513916
Epoch 560, training loss: 0.02141820639371872 = 0.014310067519545555 + 0.001 * 7.108138561248779
Epoch 560, val loss: 1.1468387842178345
Epoch 570, training loss: 0.020440401509404182 = 0.013354022987186909 + 0.001 * 7.08637809753418
Epoch 570, val loss: 1.1566333770751953
Epoch 580, training loss: 0.019573476165533066 = 0.01249354425817728 + 0.001 * 7.079931735992432
Epoch 580, val loss: 1.166116714477539
Epoch 590, training loss: 0.01879412680864334 = 0.0117162074893713 + 0.001 * 7.077919960021973
Epoch 590, val loss: 1.175248384475708
Epoch 600, training loss: 0.018077978864312172 = 0.011010020039975643 + 0.001 * 7.067958354949951
Epoch 600, val loss: 1.1840908527374268
Epoch 610, training loss: 0.017462534829974174 = 0.010363391600549221 + 0.001 * 7.0991435050964355
Epoch 610, val loss: 1.1927505731582642
Epoch 620, training loss: 0.01683899387717247 = 0.009767935611307621 + 0.001 * 7.071056842803955
Epoch 620, val loss: 1.2012813091278076
Epoch 630, training loss: 0.016283556818962097 = 0.009217958897352219 + 0.001 * 7.065598487854004
Epoch 630, val loss: 1.2096872329711914
Epoch 640, training loss: 0.01577109843492508 = 0.008709548972547054 + 0.001 * 7.061550140380859
Epoch 640, val loss: 1.2179762125015259
Epoch 650, training loss: 0.015296999365091324 = 0.008239504881203175 + 0.001 * 7.057494640350342
Epoch 650, val loss: 1.2261115312576294
Epoch 660, training loss: 0.014885863289237022 = 0.007804861757904291 + 0.001 * 7.0810017585754395
Epoch 660, val loss: 1.2341053485870361
Epoch 670, training loss: 0.014456501230597496 = 0.007403039839118719 + 0.001 * 7.053461074829102
Epoch 670, val loss: 1.2419201135635376
Epoch 680, training loss: 0.014115072786808014 = 0.007031328044831753 + 0.001 * 7.083745002746582
Epoch 680, val loss: 1.2495712041854858
Epoch 690, training loss: 0.013748536817729473 = 0.00668726721778512 + 0.001 * 7.061269283294678
Epoch 690, val loss: 1.2570631504058838
Epoch 700, training loss: 0.013437403365969658 = 0.006368482485413551 + 0.001 * 7.068920135498047
Epoch 700, val loss: 1.2643667459487915
Epoch 710, training loss: 0.013118753209710121 = 0.006072758696973324 + 0.001 * 7.045994758605957
Epoch 710, val loss: 1.2715013027191162
Epoch 720, training loss: 0.012842766009271145 = 0.005798161495476961 + 0.001 * 7.044604301452637
Epoch 720, val loss: 1.2784485816955566
Epoch 730, training loss: 0.012582809664309025 = 0.005542822182178497 + 0.001 * 7.039987087249756
Epoch 730, val loss: 1.2852200269699097
Epoch 740, training loss: 0.012344625778496265 = 0.005305060185492039 + 0.001 * 7.039565086364746
Epoch 740, val loss: 1.2918264865875244
Epoch 750, training loss: 0.012127183377742767 = 0.005083371419459581 + 0.001 * 7.043812274932861
Epoch 750, val loss: 1.298253059387207
Epoch 760, training loss: 0.011934852227568626 = 0.004876363091170788 + 0.001 * 7.058488845825195
Epoch 760, val loss: 1.3045141696929932
Epoch 770, training loss: 0.011714838445186615 = 0.004682834260165691 + 0.001 * 7.032004356384277
Epoch 770, val loss: 1.3106029033660889
Epoch 780, training loss: 0.011547094210982323 = 0.004501641262322664 + 0.001 * 7.045453071594238
Epoch 780, val loss: 1.3165251016616821
Epoch 790, training loss: 0.011363053694367409 = 0.0043318141251802444 + 0.001 * 7.031238555908203
Epoch 790, val loss: 1.3223124742507935
Epoch 800, training loss: 0.01120704784989357 = 0.004172406159341335 + 0.001 * 7.034641265869141
Epoch 800, val loss: 1.3279242515563965
Epoch 810, training loss: 0.011052561923861504 = 0.004022576380521059 + 0.001 * 7.029984951019287
Epoch 810, val loss: 1.333395004272461
Epoch 820, training loss: 0.010910015553236008 = 0.003881579963490367 + 0.001 * 7.028435707092285
Epoch 820, val loss: 1.3387296199798584
Epoch 830, training loss: 0.010781023651361465 = 0.0037487659137696028 + 0.001 * 7.032257556915283
Epoch 830, val loss: 1.3439189195632935
Epoch 840, training loss: 0.010648655705153942 = 0.003623500233516097 + 0.001 * 7.025155544281006
Epoch 840, val loss: 1.3489888906478882
Epoch 850, training loss: 0.01055153738707304 = 0.0035052357707172632 + 0.001 * 7.046300888061523
Epoch 850, val loss: 1.3539210557937622
Epoch 860, training loss: 0.010423144325613976 = 0.0033935089595615864 + 0.001 * 7.029634475708008
Epoch 860, val loss: 1.3587199449539185
Epoch 870, training loss: 0.010328532196581364 = 0.003287771949544549 + 0.001 * 7.040760040283203
Epoch 870, val loss: 1.363412857055664
Epoch 880, training loss: 0.010203259065747261 = 0.0031876543071120977 + 0.001 * 7.015604496002197
Epoch 880, val loss: 1.3679611682891846
Epoch 890, training loss: 0.010116915218532085 = 0.0030927297193557024 + 0.001 * 7.024184703826904
Epoch 890, val loss: 1.3724154233932495
Epoch 900, training loss: 0.010028519667685032 = 0.003002675948664546 + 0.001 * 7.025843620300293
Epoch 900, val loss: 1.3767333030700684
Epoch 910, training loss: 0.009931015782058239 = 0.0029171379283070564 + 0.001 * 7.0138773918151855
Epoch 910, val loss: 1.3809665441513062
Epoch 920, training loss: 0.009843112900853157 = 0.0028358453419059515 + 0.001 * 7.007266998291016
Epoch 920, val loss: 1.3850727081298828
Epoch 930, training loss: 0.009766526520252228 = 0.002758496906608343 + 0.001 * 7.008029937744141
Epoch 930, val loss: 1.3890942335128784
Epoch 940, training loss: 0.009695855900645256 = 0.002684872830286622 + 0.001 * 7.010982990264893
Epoch 940, val loss: 1.392992615699768
Epoch 950, training loss: 0.009621353819966316 = 0.0026147097814828157 + 0.001 * 7.006643772125244
Epoch 950, val loss: 1.3968180418014526
Epoch 960, training loss: 0.009551001712679863 = 0.0025478119496256113 + 0.001 * 7.0031890869140625
Epoch 960, val loss: 1.4005341529846191
Epoch 970, training loss: 0.009496109560132027 = 0.0024839607067406178 + 0.001 * 7.012147903442383
Epoch 970, val loss: 1.404173493385315
Epoch 980, training loss: 0.009465736337006092 = 0.002423003548756242 + 0.001 * 7.0427327156066895
Epoch 980, val loss: 1.4076972007751465
Epoch 990, training loss: 0.009369133971631527 = 0.0023647490888834 + 0.001 * 7.004384517669678
Epoch 990, val loss: 1.411163568496704
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8139167105956774
The final CL Acc:0.79506, 0.02463, The final GNN Acc:0.81251, 0.00163
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13116])
remove edge: torch.Size([2, 7866])
updated graph: torch.Size([2, 10426])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9503854513168335 = 1.941788673400879 + 0.001 * 8.596826553344727
Epoch 0, val loss: 1.9428415298461914
Epoch 10, training loss: 1.939766526222229 = 1.9311697483062744 + 0.001 * 8.596755981445312
Epoch 10, val loss: 1.9320956468582153
Epoch 20, training loss: 1.9265145063400269 = 1.9179179668426514 + 0.001 * 8.596505165100098
Epoch 20, val loss: 1.9187763929367065
Epoch 30, training loss: 1.907683253288269 = 1.8990873098373413 + 0.001 * 8.595917701721191
Epoch 30, val loss: 1.900102138519287
Epoch 40, training loss: 1.8798192739486694 = 1.8712248802185059 + 0.001 * 8.594432830810547
Epoch 40, val loss: 1.8731696605682373
Epoch 50, training loss: 1.8418806791305542 = 1.833290934562683 + 0.001 * 8.589715003967285
Epoch 50, val loss: 1.838772177696228
Epoch 60, training loss: 1.8016210794448853 = 1.7930514812469482 + 0.001 * 8.569540977478027
Epoch 60, val loss: 1.8059052228927612
Epoch 70, training loss: 1.764286994934082 = 1.75582754611969 + 0.001 * 8.459437370300293
Epoch 70, val loss: 1.773529052734375
Epoch 80, training loss: 1.7125362157821655 = 1.7044799327850342 + 0.001 * 8.056329727172852
Epoch 80, val loss: 1.7237486839294434
Epoch 90, training loss: 1.6417874097824097 = 1.6337932348251343 + 0.001 * 7.994211673736572
Epoch 90, val loss: 1.6590646505355835
Epoch 100, training loss: 1.5539883375167847 = 1.5460141897201538 + 0.001 * 7.974204063415527
Epoch 100, val loss: 1.584632158279419
Epoch 110, training loss: 1.4639314413070679 = 1.4559744596481323 + 0.001 * 7.956923484802246
Epoch 110, val loss: 1.5084043741226196
Epoch 120, training loss: 1.3806196451187134 = 1.3727073669433594 + 0.001 * 7.912292957305908
Epoch 120, val loss: 1.4406269788742065
Epoch 130, training loss: 1.3030284643173218 = 1.2952979803085327 + 0.001 * 7.730532646179199
Epoch 130, val loss: 1.378529667854309
Epoch 140, training loss: 1.228326439857483 = 1.2209229469299316 + 0.001 * 7.403540134429932
Epoch 140, val loss: 1.320399522781372
Epoch 150, training loss: 1.1548796892166138 = 1.1475329399108887 + 0.001 * 7.346756935119629
Epoch 150, val loss: 1.264885663986206
Epoch 160, training loss: 1.0803674459457397 = 1.0730791091918945 + 0.001 * 7.288339138031006
Epoch 160, val loss: 1.209545373916626
Epoch 170, training loss: 1.0033795833587646 = 0.9961256980895996 + 0.001 * 7.253876686096191
Epoch 170, val loss: 1.1521148681640625
Epoch 180, training loss: 0.9244896173477173 = 0.9172654747962952 + 0.001 * 7.224137306213379
Epoch 180, val loss: 1.0928912162780762
Epoch 190, training loss: 0.845916748046875 = 0.8387213349342346 + 0.001 * 7.195422172546387
Epoch 190, val loss: 1.0339816808700562
Epoch 200, training loss: 0.769985020160675 = 0.7628146409988403 + 0.001 * 7.170350074768066
Epoch 200, val loss: 0.977780818939209
Epoch 210, training loss: 0.6987587809562683 = 0.6916033625602722 + 0.001 * 7.155391693115234
Epoch 210, val loss: 0.9265751242637634
Epoch 220, training loss: 0.6338402628898621 = 0.6266884803771973 + 0.001 * 7.151777267456055
Epoch 220, val loss: 0.8820427060127258
Epoch 230, training loss: 0.5761393904685974 = 0.5689902305603027 + 0.001 * 7.149174690246582
Epoch 230, val loss: 0.8456351161003113
Epoch 240, training loss: 0.5253329873085022 = 0.5181843042373657 + 0.001 * 7.148676872253418
Epoch 240, val loss: 0.8171052932739258
Epoch 250, training loss: 0.47973546385765076 = 0.47258761525154114 + 0.001 * 7.147861003875732
Epoch 250, val loss: 0.7950596809387207
Epoch 260, training loss: 0.43709489703178406 = 0.42994818091392517 + 0.001 * 7.1467180252075195
Epoch 260, val loss: 0.7773036956787109
Epoch 270, training loss: 0.39550891518592834 = 0.3883637487888336 + 0.001 * 7.145158767700195
Epoch 270, val loss: 0.7619993090629578
Epoch 280, training loss: 0.3541211783885956 = 0.34697791934013367 + 0.001 * 7.143270492553711
Epoch 280, val loss: 0.7483444809913635
Epoch 290, training loss: 0.31338897347450256 = 0.30624768137931824 + 0.001 * 7.141286373138428
Epoch 290, val loss: 0.7365089654922485
Epoch 300, training loss: 0.27473825216293335 = 0.26759883761405945 + 0.001 * 7.13942813873291
Epoch 300, val loss: 0.7270278930664062
Epoch 310, training loss: 0.2397276908159256 = 0.23258957266807556 + 0.001 * 7.138110637664795
Epoch 310, val loss: 0.7206945419311523
Epoch 320, training loss: 0.2092270851135254 = 0.20209024846553802 + 0.001 * 7.136831283569336
Epoch 320, val loss: 0.7179096341133118
Epoch 330, training loss: 0.18324290215969086 = 0.17610660195350647 + 0.001 * 7.136301517486572
Epoch 330, val loss: 0.7185505628585815
Epoch 340, training loss: 0.16123263537883759 = 0.1540968418121338 + 0.001 * 7.135798454284668
Epoch 340, val loss: 0.72207111120224
Epoch 350, training loss: 0.14254896342754364 = 0.13541236519813538 + 0.001 * 7.136599540710449
Epoch 350, val loss: 0.7278005480766296
Epoch 360, training loss: 0.1265956461429596 = 0.11946012824773788 + 0.001 * 7.135524272918701
Epoch 360, val loss: 0.7352270483970642
Epoch 370, training loss: 0.11289099603891373 = 0.10575517266988754 + 0.001 * 7.135819911956787
Epoch 370, val loss: 0.7439051270484924
Epoch 380, training loss: 0.10104483366012573 = 0.09390915185213089 + 0.001 * 7.135683059692383
Epoch 380, val loss: 0.7534887194633484
Epoch 390, training loss: 0.09074924141168594 = 0.08361383527517319 + 0.001 * 7.135403156280518
Epoch 390, val loss: 0.7636894583702087
Epoch 400, training loss: 0.08176552504301071 = 0.07463046163320541 + 0.001 * 7.135064125061035
Epoch 400, val loss: 0.774290919303894
Epoch 410, training loss: 0.07390371710062027 = 0.06676918268203735 + 0.001 * 7.13453483581543
Epoch 410, val loss: 0.785190999507904
Epoch 420, training loss: 0.06701117753982544 = 0.059878040105104446 + 0.001 * 7.133138179779053
Epoch 420, val loss: 0.7962813973426819
Epoch 430, training loss: 0.06096474081277847 = 0.05383087322115898 + 0.001 * 7.133866786956787
Epoch 430, val loss: 0.8074538111686707
Epoch 440, training loss: 0.05565083771944046 = 0.04852025583386421 + 0.001 * 7.130580902099609
Epoch 440, val loss: 0.8186253309249878
Epoch 450, training loss: 0.05097832903265953 = 0.04385165125131607 + 0.001 * 7.126678943634033
Epoch 450, val loss: 0.8296952843666077
Epoch 460, training loss: 0.04686965420842171 = 0.03974224254488945 + 0.001 * 7.127411842346191
Epoch 460, val loss: 0.8406259417533875
Epoch 470, training loss: 0.04324423149228096 = 0.03612029179930687 + 0.001 * 7.123938083648682
Epoch 470, val loss: 0.8513796925544739
Epoch 480, training loss: 0.040039125829935074 = 0.032922737300395966 + 0.001 * 7.11638879776001
Epoch 480, val loss: 0.8619048595428467
Epoch 490, training loss: 0.03721797093749046 = 0.03009394183754921 + 0.001 * 7.124027729034424
Epoch 490, val loss: 0.8721780180931091
Epoch 500, training loss: 0.03469548374414444 = 0.027585921809077263 + 0.001 * 7.109561443328857
Epoch 500, val loss: 0.8822115063667297
Epoch 510, training loss: 0.03245686739683151 = 0.025357887148857117 + 0.001 * 7.098978042602539
Epoch 510, val loss: 0.8920004963874817
Epoch 520, training loss: 0.030480464920401573 = 0.023373307660222054 + 0.001 * 7.107157230377197
Epoch 520, val loss: 0.9015170335769653
Epoch 530, training loss: 0.028691938146948814 = 0.02160121686756611 + 0.001 * 7.0907206535339355
Epoch 530, val loss: 0.9107630252838135
Epoch 540, training loss: 0.027102455496788025 = 0.020014729350805283 + 0.001 * 7.087724685668945
Epoch 540, val loss: 0.9197806715965271
Epoch 550, training loss: 0.025681989267468452 = 0.018590588122606277 + 0.001 * 7.091400146484375
Epoch 550, val loss: 0.9285308122634888
Epoch 560, training loss: 0.024391330778598785 = 0.017309043556451797 + 0.001 * 7.082287788391113
Epoch 560, val loss: 0.9370366930961609
Epoch 570, training loss: 0.023235365748405457 = 0.0161526370793581 + 0.001 * 7.082728385925293
Epoch 570, val loss: 0.9453083872795105
Epoch 580, training loss: 0.022189943119883537 = 0.015106402337551117 + 0.001 * 7.083540439605713
Epoch 580, val loss: 0.953330934047699
Epoch 590, training loss: 0.021233826875686646 = 0.014157675206661224 + 0.001 * 7.076152324676514
Epoch 590, val loss: 0.9611222147941589
Epoch 600, training loss: 0.020376427099108696 = 0.013295182958245277 + 0.001 * 7.081243515014648
Epoch 600, val loss: 0.9686880707740784
Epoch 610, training loss: 0.019584424793720245 = 0.012509199790656567 + 0.001 * 7.075223922729492
Epoch 610, val loss: 0.9760342240333557
Epoch 620, training loss: 0.018859364092350006 = 0.01179131492972374 + 0.001 * 7.06804895401001
Epoch 620, val loss: 0.9831808805465698
Epoch 630, training loss: 0.018196094781160355 = 0.011134233325719833 + 0.001 * 7.061861038208008
Epoch 630, val loss: 0.990109920501709
Epoch 640, training loss: 0.0175920557230711 = 0.010531489737331867 + 0.001 * 7.060564994812012
Epoch 640, val loss: 0.9968469738960266
Epoch 650, training loss: 0.01703687757253647 = 0.009977499954402447 + 0.001 * 7.059378147125244
Epoch 650, val loss: 1.0033951997756958
Epoch 660, training loss: 0.01652638055384159 = 0.009467287920415401 + 0.001 * 7.0590925216674805
Epoch 660, val loss: 1.0097706317901611
Epoch 670, training loss: 0.016061853617429733 = 0.008996475487947464 + 0.001 * 7.0653767585754395
Epoch 670, val loss: 1.0159673690795898
Epoch 680, training loss: 0.01561448723077774 = 0.008561158552765846 + 0.001 * 7.053328514099121
Epoch 680, val loss: 1.0219899415969849
Epoch 690, training loss: 0.015210762619972229 = 0.008157951757311821 + 0.001 * 7.0528106689453125
Epoch 690, val loss: 1.0278598070144653
Epoch 700, training loss: 0.014858432114124298 = 0.007783902809023857 + 0.001 * 7.074528694152832
Epoch 700, val loss: 1.033565640449524
Epoch 710, training loss: 0.014491038396954536 = 0.007436290383338928 + 0.001 * 7.054747104644775
Epoch 710, val loss: 1.0391345024108887
Epoch 720, training loss: 0.014159462414681911 = 0.007112732157111168 + 0.001 * 7.046730041503906
Epoch 720, val loss: 1.0445502996444702
Epoch 730, training loss: 0.013862605206668377 = 0.006811072118580341 + 0.001 * 7.051532745361328
Epoch 730, val loss: 1.0498261451721191
Epoch 740, training loss: 0.013581138104200363 = 0.006529428530484438 + 0.001 * 7.051708698272705
Epoch 740, val loss: 1.0549817085266113
Epoch 750, training loss: 0.013310255482792854 = 0.0062660956755280495 + 0.001 * 7.044159889221191
Epoch 750, val loss: 1.0599920749664307
Epoch 760, training loss: 0.013070641085505486 = 0.006019530817866325 + 0.001 * 7.051109790802002
Epoch 760, val loss: 1.064888596534729
Epoch 770, training loss: 0.012825741432607174 = 0.0057884096167981625 + 0.001 * 7.037331581115723
Epoch 770, val loss: 1.0696542263031006
Epoch 780, training loss: 0.012616248801350594 = 0.0055714743211865425 + 0.001 * 7.044774055480957
Epoch 780, val loss: 1.0742944478988647
Epoch 790, training loss: 0.012414464727044106 = 0.0053675612434744835 + 0.001 * 7.046903610229492
Epoch 790, val loss: 1.0788259506225586
Epoch 800, training loss: 0.012209312990307808 = 0.005175643600523472 + 0.001 * 7.033669471740723
Epoch 800, val loss: 1.0832544565200806
Epoch 810, training loss: 0.012036345899105072 = 0.004994834307581186 + 0.001 * 7.041511535644531
Epoch 810, val loss: 1.0875656604766846
Epoch 820, training loss: 0.01185605302453041 = 0.004824284929782152 + 0.001 * 7.031768321990967
Epoch 820, val loss: 1.0917822122573853
Epoch 830, training loss: 0.011708960868418217 = 0.0046632615849375725 + 0.001 * 7.045699119567871
Epoch 830, val loss: 1.0959084033966064
Epoch 840, training loss: 0.011544929817318916 = 0.0045110611245036125 + 0.001 * 7.033867835998535
Epoch 840, val loss: 1.0999287366867065
Epoch 850, training loss: 0.011399788782000542 = 0.004367086570709944 + 0.001 * 7.032702445983887
Epoch 850, val loss: 1.1038681268692017
Epoch 860, training loss: 0.011252105236053467 = 0.004230733960866928 + 0.001 * 7.021370887756348
Epoch 860, val loss: 1.107704997062683
Epoch 870, training loss: 0.011116954497992992 = 0.004101464990526438 + 0.001 * 7.015489101409912
Epoch 870, val loss: 1.111456274986267
Epoch 880, training loss: 0.011009162291884422 = 0.0039788200519979 + 0.001 * 7.0303425788879395
Epoch 880, val loss: 1.115127444267273
Epoch 890, training loss: 0.010906025767326355 = 0.0038623414002358913 + 0.001 * 7.0436835289001465
Epoch 890, val loss: 1.118720531463623
Epoch 900, training loss: 0.010768221691250801 = 0.003751614596694708 + 0.001 * 7.016606330871582
Epoch 900, val loss: 1.1222283840179443
Epoch 910, training loss: 0.010656996630132198 = 0.0036462959833443165 + 0.001 * 7.010700225830078
Epoch 910, val loss: 1.125674843788147
Epoch 920, training loss: 0.010568216443061829 = 0.003546039341017604 + 0.001 * 7.022176742553711
Epoch 920, val loss: 1.1290377378463745
Epoch 930, training loss: 0.010460878722369671 = 0.0034505159128457308 + 0.001 * 7.01036262512207
Epoch 930, val loss: 1.1323245763778687
Epoch 940, training loss: 0.010375868529081345 = 0.003359423950314522 + 0.001 * 7.016444206237793
Epoch 940, val loss: 1.135543704032898
Epoch 950, training loss: 0.010281267575919628 = 0.003272507106885314 + 0.001 * 7.00875997543335
Epoch 950, val loss: 1.1386972665786743
Epoch 960, training loss: 0.010259654372930527 = 0.0031895162537693977 + 0.001 * 7.070138454437256
Epoch 960, val loss: 1.1417787075042725
Epoch 970, training loss: 0.010112387128174305 = 0.0031101745553314686 + 0.001 * 7.002212047576904
Epoch 970, val loss: 1.1447949409484863
Epoch 980, training loss: 0.01003663893789053 = 0.0030343045946210623 + 0.001 * 7.002333641052246
Epoch 980, val loss: 1.1477415561676025
Epoch 990, training loss: 0.009958785027265549 = 0.0029616630636155605 + 0.001 * 6.997122287750244
Epoch 990, val loss: 1.1506359577178955
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.9559129476547241 = 1.9473161697387695 + 0.001 * 8.596829414367676
Epoch 0, val loss: 1.950984239578247
Epoch 10, training loss: 1.94581139087677 = 1.9372146129608154 + 0.001 * 8.596772193908691
Epoch 10, val loss: 1.940506100654602
Epoch 20, training loss: 1.9333373308181763 = 1.9247407913208008 + 0.001 * 8.596536636352539
Epoch 20, val loss: 1.9274274110794067
Epoch 30, training loss: 1.9155598878860474 = 1.9069639444351196 + 0.001 * 8.595909118652344
Epoch 30, val loss: 1.9088026285171509
Epoch 40, training loss: 1.8890750408172607 = 1.8804808855056763 + 0.001 * 8.59417724609375
Epoch 40, val loss: 1.8812005519866943
Epoch 50, training loss: 1.8521573543548584 = 1.8435691595077515 + 0.001 * 8.588141441345215
Epoch 50, val loss: 1.8440464735031128
Epoch 60, training loss: 1.810588002204895 = 1.8020269870758057 + 0.001 * 8.561059951782227
Epoch 60, val loss: 1.805422067642212
Epoch 70, training loss: 1.7734724283218384 = 1.7650630474090576 + 0.001 * 8.409343719482422
Epoch 70, val loss: 1.773595929145813
Epoch 80, training loss: 1.7269608974456787 = 1.7188712358474731 + 0.001 * 8.089672088623047
Epoch 80, val loss: 1.7319989204406738
Epoch 90, training loss: 1.66225266456604 = 1.6543673276901245 + 0.001 * 7.885317802429199
Epoch 90, val loss: 1.6745703220367432
Epoch 100, training loss: 1.575782299041748 = 1.568093180656433 + 0.001 * 7.689060688018799
Epoch 100, val loss: 1.6005752086639404
Epoch 110, training loss: 1.4737977981567383 = 1.4662156105041504 + 0.001 * 7.582150936126709
Epoch 110, val loss: 1.51381254196167
Epoch 120, training loss: 1.3656580448150635 = 1.3580924272537231 + 0.001 * 7.565653324127197
Epoch 120, val loss: 1.4240844249725342
Epoch 130, training loss: 1.2565044164657593 = 1.2489712238311768 + 0.001 * 7.533172130584717
Epoch 130, val loss: 1.3345683813095093
Epoch 140, training loss: 1.1491965055465698 = 1.1416975259780884 + 0.001 * 7.499001502990723
Epoch 140, val loss: 1.247740387916565
Epoch 150, training loss: 1.047508716583252 = 1.0400748252868652 + 0.001 * 7.433858394622803
Epoch 150, val loss: 1.166808009147644
Epoch 160, training loss: 0.9542367458343506 = 0.9468985199928284 + 0.001 * 7.338203430175781
Epoch 160, val loss: 1.0932592153549194
Epoch 170, training loss: 0.8697206377983093 = 0.8624498844146729 + 0.001 * 7.270766735076904
Epoch 170, val loss: 1.0269098281860352
Epoch 180, training loss: 0.7935056686401367 = 0.7862996459007263 + 0.001 * 7.205995559692383
Epoch 180, val loss: 0.967613935470581
Epoch 190, training loss: 0.7252272367477417 = 0.71805340051651 + 0.001 * 7.1738104820251465
Epoch 190, val loss: 0.9161716103553772
Epoch 200, training loss: 0.663695752620697 = 0.6565374135971069 + 0.001 * 7.158348560333252
Epoch 200, val loss: 0.8730005025863647
Epoch 210, training loss: 0.6069306135177612 = 0.5997824668884277 + 0.001 * 7.148118495941162
Epoch 210, val loss: 0.8378489017486572
Epoch 220, training loss: 0.5533099174499512 = 0.5461674928665161 + 0.001 * 7.142454147338867
Epoch 220, val loss: 0.8096259236335754
Epoch 230, training loss: 0.5022669434547424 = 0.49512797594070435 + 0.001 * 7.138979911804199
Epoch 230, val loss: 0.7873948216438293
Epoch 240, training loss: 0.45413076877593994 = 0.4469923675060272 + 0.001 * 7.138391971588135
Epoch 240, val loss: 0.7703478336334229
Epoch 250, training loss: 0.40952956676483154 = 0.4023910462856293 + 0.001 * 7.138510227203369
Epoch 250, val loss: 0.758531928062439
Epoch 260, training loss: 0.36859405040740967 = 0.3614562451839447 + 0.001 * 7.1378021240234375
Epoch 260, val loss: 0.7522069811820984
Epoch 270, training loss: 0.3309377431869507 = 0.3237987160682678 + 0.001 * 7.139040946960449
Epoch 270, val loss: 0.7506459951400757
Epoch 280, training loss: 0.29606103897094727 = 0.2889269292354584 + 0.001 * 7.134102821350098
Epoch 280, val loss: 0.7529765963554382
Epoch 290, training loss: 0.26378166675567627 = 0.2566472291946411 + 0.001 * 7.13443660736084
Epoch 290, val loss: 0.758712112903595
Epoch 300, training loss: 0.2342662513256073 = 0.22713331878185272 + 0.001 * 7.132936954498291
Epoch 300, val loss: 0.7675454616546631
Epoch 310, training loss: 0.20773909986019135 = 0.20060613751411438 + 0.001 * 7.132960796356201
Epoch 310, val loss: 0.7792196869850159
Epoch 320, training loss: 0.18424835801124573 = 0.17711655795574188 + 0.001 * 7.131804943084717
Epoch 320, val loss: 0.7935118079185486
Epoch 330, training loss: 0.16364753246307373 = 0.1565174013376236 + 0.001 * 7.130129337310791
Epoch 330, val loss: 0.810030996799469
Epoch 340, training loss: 0.14566786587238312 = 0.13853876292705536 + 0.001 * 7.129108905792236
Epoch 340, val loss: 0.8282450437545776
Epoch 350, training loss: 0.12999096512794495 = 0.12286288291215897 + 0.001 * 7.128086566925049
Epoch 350, val loss: 0.8478291034698486
Epoch 360, training loss: 0.11630633473396301 = 0.10918205231428146 + 0.001 * 7.124279499053955
Epoch 360, val loss: 0.868314802646637
Epoch 370, training loss: 0.1043505147099495 = 0.0972234234213829 + 0.001 * 7.127091407775879
Epoch 370, val loss: 0.8893887996673584
Epoch 380, training loss: 0.09387467801570892 = 0.086750827729702 + 0.001 * 7.123851776123047
Epoch 380, val loss: 0.9107133150100708
Epoch 390, training loss: 0.08468172699213028 = 0.07756379246711731 + 0.001 * 7.117931842803955
Epoch 390, val loss: 0.9321497678756714
Epoch 400, training loss: 0.07661039382219315 = 0.06949271261692047 + 0.001 * 7.117678642272949
Epoch 400, val loss: 0.9534726738929749
Epoch 410, training loss: 0.06951087713241577 = 0.06239387392997742 + 0.001 * 7.11699914932251
Epoch 410, val loss: 0.9745795130729675
Epoch 420, training loss: 0.06325498968362808 = 0.05614081025123596 + 0.001 * 7.114181995391846
Epoch 420, val loss: 0.9954044818878174
Epoch 430, training loss: 0.05773372948169708 = 0.050625029951334 + 0.001 * 7.108699798583984
Epoch 430, val loss: 1.0157917737960815
Epoch 440, training loss: 0.052850645035505295 = 0.04575350508093834 + 0.001 * 7.097139835357666
Epoch 440, val loss: 1.0357201099395752
Epoch 450, training loss: 0.04855354502797127 = 0.04144604131579399 + 0.001 * 7.107502460479736
Epoch 450, val loss: 1.0551458597183228
Epoch 460, training loss: 0.0447222962975502 = 0.03763430938124657 + 0.001 * 7.087986946105957
Epoch 460, val loss: 1.074041724205017
Epoch 470, training loss: 0.04134253412485123 = 0.03425779566168785 + 0.001 * 7.084738731384277
Epoch 470, val loss: 1.0923627614974976
Epoch 480, training loss: 0.03836799040436745 = 0.03126399219036102 + 0.001 * 7.103996753692627
Epoch 480, val loss: 1.1100986003875732
Epoch 490, training loss: 0.03569493070244789 = 0.02860591560602188 + 0.001 * 7.089015007019043
Epoch 490, val loss: 1.1272560358047485
Epoch 500, training loss: 0.03331945836544037 = 0.02624283917248249 + 0.001 * 7.076618671417236
Epoch 500, val loss: 1.143820881843567
Epoch 510, training loss: 0.031227154657244682 = 0.024138348177075386 + 0.001 * 7.088806629180908
Epoch 510, val loss: 1.1598466634750366
Epoch 520, training loss: 0.029321471229195595 = 0.022260786965489388 + 0.001 * 7.060683727264404
Epoch 520, val loss: 1.1753031015396118
Epoch 530, training loss: 0.027641238644719124 = 0.020582089200615883 + 0.001 * 7.059148788452148
Epoch 530, val loss: 1.1902544498443604
Epoch 540, training loss: 0.02613680250942707 = 0.01907827891409397 + 0.001 * 7.058523654937744
Epoch 540, val loss: 1.2046903371810913
Epoch 550, training loss: 0.024785662069916725 = 0.017728116363286972 + 0.001 * 7.0575456619262695
Epoch 550, val loss: 1.218631386756897
Epoch 560, training loss: 0.02359471470117569 = 0.016512714326381683 + 0.001 * 7.082000255584717
Epoch 560, val loss: 1.2320868968963623
Epoch 570, training loss: 0.022475579753518105 = 0.015413878485560417 + 0.001 * 7.061700820922852
Epoch 570, val loss: 1.2451279163360596
Epoch 580, training loss: 0.02147131785750389 = 0.014411073178052902 + 0.001 * 7.060243606567383
Epoch 580, val loss: 1.257864236831665
Epoch 590, training loss: 0.02052830159664154 = 0.013489524833858013 + 0.001 * 7.038775444030762
Epoch 590, val loss: 1.2703649997711182
Epoch 600, training loss: 0.019686199724674225 = 0.01264154352247715 + 0.001 * 7.044656276702881
Epoch 600, val loss: 1.2826423645019531
Epoch 610, training loss: 0.018912101164460182 = 0.011862102895975113 + 0.001 * 7.049997329711914
Epoch 610, val loss: 1.2946432828903198
Epoch 620, training loss: 0.0181900542229414 = 0.011146357282996178 + 0.001 * 7.043696880340576
Epoch 620, val loss: 1.3063710927963257
Epoch 630, training loss: 0.017520293593406677 = 0.010489474050700665 + 0.001 * 7.030819892883301
Epoch 630, val loss: 1.3178173303604126
Epoch 640, training loss: 0.016926318407058716 = 0.009886613115668297 + 0.001 * 7.039705753326416
Epoch 640, val loss: 1.3289672136306763
Epoch 650, training loss: 0.016359876841306686 = 0.009333078749477863 + 0.001 * 7.026796817779541
Epoch 650, val loss: 1.339850664138794
Epoch 660, training loss: 0.01585584506392479 = 0.008824421092867851 + 0.001 * 7.031424522399902
Epoch 660, val loss: 1.3504523038864136
Epoch 670, training loss: 0.015385466627776623 = 0.00835646502673626 + 0.001 * 7.029001235961914
Epoch 670, val loss: 1.3607597351074219
Epoch 680, training loss: 0.014947771094739437 = 0.007925312034785748 + 0.001 * 7.022458553314209
Epoch 680, val loss: 1.370793104171753
Epoch 690, training loss: 0.014555415138602257 = 0.007527518086135387 + 0.001 * 7.027896881103516
Epoch 690, val loss: 1.3805292844772339
Epoch 700, training loss: 0.014187674969434738 = 0.007159878499805927 + 0.001 * 7.027796745300293
Epoch 700, val loss: 1.3900035619735718
Epoch 710, training loss: 0.013846958056092262 = 0.006819469388574362 + 0.001 * 7.027487754821777
Epoch 710, val loss: 1.3992059230804443
Epoch 720, training loss: 0.01352597214281559 = 0.006503728684037924 + 0.001 * 7.022243022918701
Epoch 720, val loss: 1.4081637859344482
Epoch 730, training loss: 0.013239633291959763 = 0.00621025962755084 + 0.001 * 7.029372692108154
Epoch 730, val loss: 1.416878581047058
Epoch 740, training loss: 0.012959359213709831 = 0.0059368000365793705 + 0.001 * 7.022558212280273
Epoch 740, val loss: 1.4253804683685303
Epoch 750, training loss: 0.012695207260549068 = 0.005681200418621302 + 0.001 * 7.014006614685059
Epoch 750, val loss: 1.4336318969726562
Epoch 760, training loss: 0.012461690232157707 = 0.005441599991172552 + 0.001 * 7.020090579986572
Epoch 760, val loss: 1.4417078495025635
Epoch 770, training loss: 0.012246137484908104 = 0.00521635077893734 + 0.001 * 7.029787063598633
Epoch 770, val loss: 1.4495795965194702
Epoch 780, training loss: 0.012000860646367073 = 0.005003992933779955 + 0.001 * 6.996866703033447
Epoch 780, val loss: 1.45724356174469
Epoch 790, training loss: 0.011824356392025948 = 0.0048035322688519955 + 0.001 * 7.0208234786987305
Epoch 790, val loss: 1.4647094011306763
Epoch 800, training loss: 0.011626319959759712 = 0.004614036995917559 + 0.001 * 7.012282848358154
Epoch 800, val loss: 1.471969723701477
Epoch 810, training loss: 0.011427764780819416 = 0.004434789996594191 + 0.001 * 6.992974281311035
Epoch 810, val loss: 1.4789870977401733
Epoch 820, training loss: 0.011257314123213291 = 0.004265157040208578 + 0.001 * 6.992156982421875
Epoch 820, val loss: 1.485821008682251
Epoch 830, training loss: 0.01110854372382164 = 0.0041045849211514 + 0.001 * 7.003958225250244
Epoch 830, val loss: 1.4924079179763794
Epoch 840, training loss: 0.010942233726382256 = 0.003952613100409508 + 0.001 * 6.989619731903076
Epoch 840, val loss: 1.4987722635269165
Epoch 850, training loss: 0.010805279016494751 = 0.003808712586760521 + 0.001 * 6.9965667724609375
Epoch 850, val loss: 1.5049153566360474
Epoch 860, training loss: 0.010661172680556774 = 0.003672467777505517 + 0.001 * 6.988704681396484
Epoch 860, val loss: 1.5108410120010376
Epoch 870, training loss: 0.010532700456678867 = 0.0035434209275990725 + 0.001 * 6.989278793334961
Epoch 870, val loss: 1.5165791511535645
Epoch 880, training loss: 0.01041458360850811 = 0.003421224420890212 + 0.001 * 6.993358612060547
Epoch 880, val loss: 1.5221394300460815
Epoch 890, training loss: 0.01027609407901764 = 0.003305458929389715 + 0.001 * 6.970635414123535
Epoch 890, val loss: 1.5275119543075562
Epoch 900, training loss: 0.010169794782996178 = 0.0031957675237208605 + 0.001 * 6.974027156829834
Epoch 900, val loss: 1.5327050685882568
Epoch 910, training loss: 0.010063083842396736 = 0.00309177883900702 + 0.001 * 6.971304416656494
Epoch 910, val loss: 1.537730097770691
Epoch 920, training loss: 0.009983268566429615 = 0.0029931359458714724 + 0.001 * 6.990131855010986
Epoch 920, val loss: 1.5426170825958252
Epoch 930, training loss: 0.009872128255665302 = 0.0028996053151786327 + 0.001 * 6.972522735595703
Epoch 930, val loss: 1.5472902059555054
Epoch 940, training loss: 0.009805557318031788 = 0.0028108307160437107 + 0.001 * 6.994726181030273
Epoch 940, val loss: 1.551875352859497
Epoch 950, training loss: 0.00972425751388073 = 0.0027264896780252457 + 0.001 * 6.997766971588135
Epoch 950, val loss: 1.556288480758667
Epoch 960, training loss: 0.009606536477804184 = 0.002646298613399267 + 0.001 * 6.960237979888916
Epoch 960, val loss: 1.5605432987213135
Epoch 970, training loss: 0.009528063237667084 = 0.0025700139813125134 + 0.001 * 6.958049297332764
Epoch 970, val loss: 1.5646767616271973
Epoch 980, training loss: 0.009460119530558586 = 0.0024973955005407333 + 0.001 * 6.962723255157471
Epoch 980, val loss: 1.5686522722244263
Epoch 990, training loss: 0.009373797103762627 = 0.0024282310623675585 + 0.001 * 6.945565700531006
Epoch 990, val loss: 1.5725133419036865
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 1.968767762184143 = 1.9601709842681885 + 0.001 * 8.596796989440918
Epoch 0, val loss: 1.9574390649795532
Epoch 10, training loss: 1.9567794799804688 = 1.9481828212738037 + 0.001 * 8.596702575683594
Epoch 10, val loss: 1.9458179473876953
Epoch 20, training loss: 1.941756248474121 = 1.9331598281860352 + 0.001 * 8.596440315246582
Epoch 20, val loss: 1.9307241439819336
Epoch 30, training loss: 1.9206819534301758 = 1.9120861291885376 + 0.001 * 8.595833778381348
Epoch 30, val loss: 1.9092018604278564
Epoch 40, training loss: 1.8899873495101929 = 1.8813931941986084 + 0.001 * 8.594198226928711
Epoch 40, val loss: 1.8781956434249878
Epoch 50, training loss: 1.8489195108413696 = 1.840330958366394 + 0.001 * 8.58853530883789
Epoch 50, val loss: 1.839079737663269
Epoch 60, training loss: 1.8077454566955566 = 1.7991827726364136 + 0.001 * 8.562653541564941
Epoch 60, val loss: 1.8052133321762085
Epoch 70, training loss: 1.7755612134933472 = 1.7671483755111694 + 0.001 * 8.412847518920898
Epoch 70, val loss: 1.7806923389434814
Epoch 80, training loss: 1.733609914779663 = 1.725549578666687 + 0.001 * 8.060338973999023
Epoch 80, val loss: 1.7432979345321655
Epoch 90, training loss: 1.6758559942245483 = 1.6679221391677856 + 0.001 * 7.933867454528809
Epoch 90, val loss: 1.6918340921401978
Epoch 100, training loss: 1.598839282989502 = 1.591041922569275 + 0.001 * 7.797307014465332
Epoch 100, val loss: 1.6263272762298584
Epoch 110, training loss: 1.5106606483459473 = 1.5030622482299805 + 0.001 * 7.598372936248779
Epoch 110, val loss: 1.555033564567566
Epoch 120, training loss: 1.4243049621582031 = 1.4168225526809692 + 0.001 * 7.4824676513671875
Epoch 120, val loss: 1.4864294528961182
Epoch 130, training loss: 1.3420617580413818 = 1.3345874547958374 + 0.001 * 7.474318027496338
Epoch 130, val loss: 1.4225053787231445
Epoch 140, training loss: 1.2599310874938965 = 1.2524909973144531 + 0.001 * 7.4401092529296875
Epoch 140, val loss: 1.3588415384292603
Epoch 150, training loss: 1.174363613128662 = 1.166968584060669 + 0.001 * 7.395025730133057
Epoch 150, val loss: 1.2920526266098022
Epoch 160, training loss: 1.085096836090088 = 1.0777584314346313 + 0.001 * 7.3384199142456055
Epoch 160, val loss: 1.2230095863342285
Epoch 170, training loss: 0.9947510957717896 = 0.9874645471572876 + 0.001 * 7.2865681648254395
Epoch 170, val loss: 1.1538870334625244
Epoch 180, training loss: 0.9064691066741943 = 0.8992221355438232 + 0.001 * 7.246962070465088
Epoch 180, val loss: 1.0873665809631348
Epoch 190, training loss: 0.8232085704803467 = 0.8159877061843872 + 0.001 * 7.220848560333252
Epoch 190, val loss: 1.0253660678863525
Epoch 200, training loss: 0.7474491596221924 = 0.7402480244636536 + 0.001 * 7.2011189460754395
Epoch 200, val loss: 0.970242977142334
Epoch 210, training loss: 0.6800769567489624 = 0.6729004383087158 + 0.001 * 7.176501750946045
Epoch 210, val loss: 0.923676073551178
Epoch 220, training loss: 0.6200034618377686 = 0.612853467464447 + 0.001 * 7.150006294250488
Epoch 220, val loss: 0.8858639001846313
Epoch 230, training loss: 0.5654941201210022 = 0.5583742260932922 + 0.001 * 7.119877815246582
Epoch 230, val loss: 0.8561589121818542
Epoch 240, training loss: 0.5155685544013977 = 0.5084682703018188 + 0.001 * 7.100281238555908
Epoch 240, val loss: 0.8337469100952148
Epoch 250, training loss: 0.4699757993221283 = 0.4628888964653015 + 0.001 * 7.086902141571045
Epoch 250, val loss: 0.8175951838493347
Epoch 260, training loss: 0.4284781515598297 = 0.42139631509780884 + 0.001 * 7.081827640533447
Epoch 260, val loss: 0.806885838508606
Epoch 270, training loss: 0.3906538784503937 = 0.3835728168487549 + 0.001 * 7.081060409545898
Epoch 270, val loss: 0.8013197183609009
Epoch 280, training loss: 0.3558567464351654 = 0.34877946972846985 + 0.001 * 7.077268600463867
Epoch 280, val loss: 0.8006035089492798
Epoch 290, training loss: 0.3234533667564392 = 0.3163768947124481 + 0.001 * 7.076474666595459
Epoch 290, val loss: 0.8040055632591248
Epoch 300, training loss: 0.2929077744483948 = 0.28583231568336487 + 0.001 * 7.0754618644714355
Epoch 300, val loss: 0.8105188608169556
Epoch 310, training loss: 0.263893723487854 = 0.25681909918785095 + 0.001 * 7.074628829956055
Epoch 310, val loss: 0.8195644021034241
Epoch 320, training loss: 0.23637565970420837 = 0.2293018251657486 + 0.001 * 7.073840618133545
Epoch 320, val loss: 0.8308421969413757
Epoch 330, training loss: 0.2106303721666336 = 0.2035573273897171 + 0.001 * 7.073047161102295
Epoch 330, val loss: 0.8443224430084229
Epoch 340, training loss: 0.1870410442352295 = 0.17996853590011597 + 0.001 * 7.072510719299316
Epoch 340, val loss: 0.8599812984466553
Epoch 350, training loss: 0.1658516824245453 = 0.15877966582775116 + 0.001 * 7.072022914886475
Epoch 350, val loss: 0.8776102066040039
Epoch 360, training loss: 0.1470940113067627 = 0.14002010226249695 + 0.001 * 7.073914051055908
Epoch 360, val loss: 0.8970834016799927
Epoch 370, training loss: 0.1306249499320984 = 0.12355256080627441 + 0.001 * 7.072389602661133
Epoch 370, val loss: 0.9181480407714844
Epoch 380, training loss: 0.11625092476606369 = 0.10917903482913971 + 0.001 * 7.071890354156494
Epoch 380, val loss: 0.9402867555618286
Epoch 390, training loss: 0.1037546843290329 = 0.0966830775141716 + 0.001 * 7.07160758972168
Epoch 390, val loss: 0.9631410837173462
Epoch 400, training loss: 0.09293066710233688 = 0.08585900068283081 + 0.001 * 7.071665287017822
Epoch 400, val loss: 0.9864542484283447
Epoch 410, training loss: 0.08357204496860504 = 0.0764954537153244 + 0.001 * 7.076590538024902
Epoch 410, val loss: 1.0099389553070068
Epoch 420, training loss: 0.07546766847372055 = 0.06839540600776672 + 0.001 * 7.072262763977051
Epoch 420, val loss: 1.0333518981933594
Epoch 430, training loss: 0.06844361871480942 = 0.06137171760201454 + 0.001 * 7.071898937225342
Epoch 430, val loss: 1.0565356016159058
Epoch 440, training loss: 0.06233089044690132 = 0.05525919795036316 + 0.001 * 7.071691513061523
Epoch 440, val loss: 1.0793226957321167
Epoch 450, training loss: 0.05699022486805916 = 0.04991910234093666 + 0.001 * 7.071122169494629
Epoch 450, val loss: 1.1015902757644653
Epoch 460, training loss: 0.05230800062417984 = 0.04523756727576256 + 0.001 * 7.070431232452393
Epoch 460, val loss: 1.1232631206512451
Epoch 470, training loss: 0.04819370433688164 = 0.041119180619716644 + 0.001 * 7.07452392578125
Epoch 470, val loss: 1.144288420677185
Epoch 480, training loss: 0.0445546954870224 = 0.03748372197151184 + 0.001 * 7.070971488952637
Epoch 480, val loss: 1.1646921634674072
Epoch 490, training loss: 0.04133433848619461 = 0.03426606208086014 + 0.001 * 7.068274021148682
Epoch 490, val loss: 1.1844818592071533
Epoch 500, training loss: 0.03847687318921089 = 0.031410302966833115 + 0.001 * 7.06657075881958
Epoch 500, val loss: 1.2036563158035278
Epoch 510, training loss: 0.03593933582305908 = 0.02886880189180374 + 0.001 * 7.070534706115723
Epoch 510, val loss: 1.2222404479980469
Epoch 520, training loss: 0.033667176961898804 = 0.026601318269968033 + 0.001 * 7.065857887268066
Epoch 520, val loss: 1.2401946783065796
Epoch 530, training loss: 0.03163516893982887 = 0.0245733093470335 + 0.001 * 7.061858654022217
Epoch 530, val loss: 1.2575820684432983
Epoch 540, training loss: 0.02981557324528694 = 0.02275524102151394 + 0.001 * 7.06033182144165
Epoch 540, val loss: 1.2744117975234985
Epoch 550, training loss: 0.02818271890282631 = 0.021121816709637642 + 0.001 * 7.060901641845703
Epoch 550, val loss: 1.2907122373580933
Epoch 560, training loss: 0.026712216436862946 = 0.019650984555482864 + 0.001 * 7.061230659484863
Epoch 560, val loss: 1.3064912557601929
Epoch 570, training loss: 0.025379354134202003 = 0.01832343451678753 + 0.001 * 7.055919170379639
Epoch 570, val loss: 1.3217922449111938
Epoch 580, training loss: 0.024187162518501282 = 0.017122672870755196 + 0.001 * 7.064488887786865
Epoch 580, val loss: 1.3366060256958008
Epoch 590, training loss: 0.023088980466127396 = 0.01603405363857746 + 0.001 * 7.054925441741943
Epoch 590, val loss: 1.3509198427200317
Epoch 600, training loss: 0.02209414169192314 = 0.0150448614731431 + 0.001 * 7.049279689788818
Epoch 600, val loss: 1.3648017644882202
Epoch 610, training loss: 0.02119101583957672 = 0.01414392702281475 + 0.001 * 7.047087669372559
Epoch 610, val loss: 1.3782336711883545
Epoch 620, training loss: 0.02037738636136055 = 0.01332166325300932 + 0.001 * 7.055721759796143
Epoch 620, val loss: 1.3912633657455444
Epoch 630, training loss: 0.019627397879958153 = 0.012569685466587543 + 0.001 * 7.057711601257324
Epoch 630, val loss: 1.4038492441177368
Epoch 640, training loss: 0.018926475197076797 = 0.011880643665790558 + 0.001 * 7.045830726623535
Epoch 640, val loss: 1.416045904159546
Epoch 650, training loss: 0.01828962378203869 = 0.01124787237495184 + 0.001 * 7.041751384735107
Epoch 650, val loss: 1.4278876781463623
Epoch 660, training loss: 0.017705556005239487 = 0.010665571317076683 + 0.001 * 7.039984226226807
Epoch 660, val loss: 1.4393606185913086
Epoch 670, training loss: 0.017166482284665108 = 0.010128671303391457 + 0.001 * 7.037810802459717
Epoch 670, val loss: 1.4505151510238647
Epoch 680, training loss: 0.01667289435863495 = 0.009632760658860207 + 0.001 * 7.040133953094482
Epoch 680, val loss: 1.4613419771194458
Epoch 690, training loss: 0.016210544854402542 = 0.009173874743282795 + 0.001 * 7.0366692543029785
Epoch 690, val loss: 1.4718530178070068
Epoch 700, training loss: 0.01578536443412304 = 0.008748475462198257 + 0.001 * 7.036888599395752
Epoch 700, val loss: 1.4820713996887207
Epoch 710, training loss: 0.015385614708065987 = 0.008353468962013721 + 0.001 * 7.0321455001831055
Epoch 710, val loss: 1.4919970035552979
Epoch 720, training loss: 0.015016008168458939 = 0.007986077107489109 + 0.001 * 7.029930114746094
Epoch 720, val loss: 1.501651644706726
Epoch 730, training loss: 0.014674149453639984 = 0.007643854711204767 + 0.001 * 7.030294895172119
Epoch 730, val loss: 1.5110313892364502
Epoch 740, training loss: 0.014358706772327423 = 0.007324569392949343 + 0.001 * 7.034137725830078
Epoch 740, val loss: 1.5201530456542969
Epoch 750, training loss: 0.01405411958694458 = 0.007026242557913065 + 0.001 * 7.027876377105713
Epoch 750, val loss: 1.529035210609436
Epoch 760, training loss: 0.01377007458359003 = 0.006747111678123474 + 0.001 * 7.02296257019043
Epoch 760, val loss: 1.5376865863800049
Epoch 770, training loss: 0.01351228915154934 = 0.006485581863671541 + 0.001 * 7.026707649230957
Epoch 770, val loss: 1.546094536781311
Epoch 780, training loss: 0.013281846418976784 = 0.006240254268050194 + 0.001 * 7.041591167449951
Epoch 780, val loss: 1.5542843341827393
Epoch 790, training loss: 0.013034678995609283 = 0.006009791977703571 + 0.001 * 7.0248870849609375
Epoch 790, val loss: 1.562263011932373
Epoch 800, training loss: 0.012828064151108265 = 0.005793056916445494 + 0.001 * 7.035006999969482
Epoch 800, val loss: 1.5700575113296509
Epoch 810, training loss: 0.012618834152817726 = 0.005588940344750881 + 0.001 * 7.02989387512207
Epoch 810, val loss: 1.5776280164718628
Epoch 820, training loss: 0.012423393316566944 = 0.005396523978561163 + 0.001 * 7.02686882019043
Epoch 820, val loss: 1.5850368738174438
Epoch 830, training loss: 0.012229546904563904 = 0.005214922595769167 + 0.001 * 7.014623641967773
Epoch 830, val loss: 1.5922528505325317
Epoch 840, training loss: 0.012062140740454197 = 0.005043371114879847 + 0.001 * 7.018769264221191
Epoch 840, val loss: 1.5993050336837769
Epoch 850, training loss: 0.011890370398759842 = 0.004881115630269051 + 0.001 * 7.0092549324035645
Epoch 850, val loss: 1.606179118156433
Epoch 860, training loss: 0.011742985807359219 = 0.004727515392005444 + 0.001 * 7.015470027923584
Epoch 860, val loss: 1.6128911972045898
Epoch 870, training loss: 0.01162666641175747 = 0.004581959918141365 + 0.001 * 7.04470682144165
Epoch 870, val loss: 1.619456171989441
Epoch 880, training loss: 0.011447567492723465 = 0.004443880170583725 + 0.001 * 7.003686904907227
Epoch 880, val loss: 1.625841498374939
Epoch 890, training loss: 0.011317647993564606 = 0.004312813747674227 + 0.001 * 7.004834175109863
Epoch 890, val loss: 1.6321065425872803
Epoch 900, training loss: 0.011237450875341892 = 0.004188264720141888 + 0.001 * 7.049185752868652
Epoch 900, val loss: 1.6382272243499756
Epoch 910, training loss: 0.011066358536481857 = 0.004069805145263672 + 0.001 * 6.996552467346191
Epoch 910, val loss: 1.6441874504089355
Epoch 920, training loss: 0.010951783508062363 = 0.003957064356654882 + 0.001 * 6.9947190284729
Epoch 920, val loss: 1.6500407457351685
Epoch 930, training loss: 0.0108448201790452 = 0.0038496770430356264 + 0.001 * 6.995142936706543
Epoch 930, val loss: 1.6557477712631226
Epoch 940, training loss: 0.010754724964499474 = 0.003747303271666169 + 0.001 * 7.007421493530273
Epoch 940, val loss: 1.661360263824463
Epoch 950, training loss: 0.01067652553319931 = 0.003649634774774313 + 0.001 * 7.026890754699707
Epoch 950, val loss: 1.6668261289596558
Epoch 960, training loss: 0.010573318228125572 = 0.0035563975106924772 + 0.001 * 7.01692008972168
Epoch 960, val loss: 1.6721725463867188
Epoch 970, training loss: 0.01045701652765274 = 0.003467325121164322 + 0.001 * 6.989691734313965
Epoch 970, val loss: 1.6774412393569946
Epoch 980, training loss: 0.010368447750806808 = 0.003382174763828516 + 0.001 * 6.986273288726807
Epoch 980, val loss: 1.6825642585754395
Epoch 990, training loss: 0.010287941433489323 = 0.003300707321614027 + 0.001 * 6.987233638763428
Epoch 990, val loss: 1.6876325607299805
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.838165524512388
The final CL Acc:0.80617, 0.02270, The final GNN Acc:0.83694, 0.00090
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10600])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9714865684509277 = 1.9628896713256836 + 0.001 * 8.596869468688965
Epoch 0, val loss: 1.9658831357955933
Epoch 10, training loss: 1.9611384868621826 = 1.952541708946228 + 0.001 * 8.596822738647461
Epoch 10, val loss: 1.9557210206985474
Epoch 20, training loss: 1.9482269287109375 = 1.9396302700042725 + 0.001 * 8.596671104431152
Epoch 20, val loss: 1.9423744678497314
Epoch 30, training loss: 1.9300684928894043 = 1.921472191810608 + 0.001 * 8.596325874328613
Epoch 30, val loss: 1.9230120182037354
Epoch 40, training loss: 1.9029432535171509 = 1.8943477869033813 + 0.001 * 8.59549617767334
Epoch 40, val loss: 1.8940861225128174
Epoch 50, training loss: 1.864689826965332 = 1.856096625328064 + 0.001 * 8.593146324157715
Epoch 50, val loss: 1.855010986328125
Epoch 60, training loss: 1.822608232498169 = 1.8140231370925903 + 0.001 * 8.585112571716309
Epoch 60, val loss: 1.816351294517517
Epoch 70, training loss: 1.7892820835113525 = 1.7807286977767944 + 0.001 * 8.553403854370117
Epoch 70, val loss: 1.7888120412826538
Epoch 80, training loss: 1.752032995223999 = 1.743657112121582 + 0.001 * 8.375876426696777
Epoch 80, val loss: 1.754963755607605
Epoch 90, training loss: 1.701516032218933 = 1.6933821439743042 + 0.001 * 8.133879661560059
Epoch 90, val loss: 1.7085459232330322
Epoch 100, training loss: 1.6321338415145874 = 1.6241350173950195 + 0.001 * 7.998872756958008
Epoch 100, val loss: 1.6463230848312378
Epoch 110, training loss: 1.5452579259872437 = 1.537442922592163 + 0.001 * 7.815032958984375
Epoch 110, val loss: 1.569724678993225
Epoch 120, training loss: 1.450338363647461 = 1.442664623260498 + 0.001 * 7.67377233505249
Epoch 120, val loss: 1.4890713691711426
Epoch 130, training loss: 1.355454921722412 = 1.347811222076416 + 0.001 * 7.643649578094482
Epoch 130, val loss: 1.4125436544418335
Epoch 140, training loss: 1.263173222541809 = 1.2555742263793945 + 0.001 * 7.599036693572998
Epoch 140, val loss: 1.3411287069320679
Epoch 150, training loss: 1.1746941804885864 = 1.1671390533447266 + 0.001 * 7.555071830749512
Epoch 150, val loss: 1.2749590873718262
Epoch 160, training loss: 1.0910325050354004 = 1.083532452583313 + 0.001 * 7.500028610229492
Epoch 160, val loss: 1.213046908378601
Epoch 170, training loss: 1.0123740434646606 = 1.0049381256103516 + 0.001 * 7.435949325561523
Epoch 170, val loss: 1.1553946733474731
Epoch 180, training loss: 0.9382890462875366 = 0.9308933019638062 + 0.001 * 7.395727157592773
Epoch 180, val loss: 1.101203441619873
Epoch 190, training loss: 0.8676208853721619 = 0.8602425456047058 + 0.001 * 7.378335475921631
Epoch 190, val loss: 1.0507413148880005
Epoch 200, training loss: 0.7999326586723328 = 0.7925751209259033 + 0.001 * 7.357545852661133
Epoch 200, val loss: 1.0049365758895874
Epoch 210, training loss: 0.7357586622238159 = 0.7284313440322876 + 0.001 * 7.327320098876953
Epoch 210, val loss: 0.9646828174591064
Epoch 220, training loss: 0.6757642030715942 = 0.6684564352035522 + 0.001 * 7.307793140411377
Epoch 220, val loss: 0.9304622411727905
Epoch 230, training loss: 0.6202974915504456 = 0.6130024194717407 + 0.001 * 7.295089244842529
Epoch 230, val loss: 0.9022530317306519
Epoch 240, training loss: 0.5694668292999268 = 0.5621769428253174 + 0.001 * 7.289890289306641
Epoch 240, val loss: 0.8795297741889954
Epoch 250, training loss: 0.5230963826179504 = 0.5158080458641052 + 0.001 * 7.28835391998291
Epoch 250, val loss: 0.8619606494903564
Epoch 260, training loss: 0.4807674288749695 = 0.47348055243492126 + 0.001 * 7.286877155303955
Epoch 260, val loss: 0.8489460945129395
Epoch 270, training loss: 0.44181323051452637 = 0.4345284700393677 + 0.001 * 7.284755229949951
Epoch 270, val loss: 0.8397629261016846
Epoch 280, training loss: 0.405416339635849 = 0.3981276750564575 + 0.001 * 7.288655757904053
Epoch 280, val loss: 0.8340251445770264
Epoch 290, training loss: 0.3706796169281006 = 0.36339718103408813 + 0.001 * 7.282432556152344
Epoch 290, val loss: 0.8310803771018982
Epoch 300, training loss: 0.33662471175193787 = 0.32934507727622986 + 0.001 * 7.279625415802002
Epoch 300, val loss: 0.8301587104797363
Epoch 310, training loss: 0.3025156259536743 = 0.29523807764053345 + 0.001 * 7.277555465698242
Epoch 310, val loss: 0.8307600021362305
Epoch 320, training loss: 0.268343985080719 = 0.2610691487789154 + 0.001 * 7.274843692779541
Epoch 320, val loss: 0.8325266242027283
Epoch 330, training loss: 0.235164076089859 = 0.22787199914455414 + 0.001 * 7.292084217071533
Epoch 330, val loss: 0.8359366059303284
Epoch 340, training loss: 0.20436808466911316 = 0.19709116220474243 + 0.001 * 7.276927947998047
Epoch 340, val loss: 0.8416150808334351
Epoch 350, training loss: 0.17708227038383484 = 0.16980937123298645 + 0.001 * 7.27289342880249
Epoch 350, val loss: 0.8497776985168457
Epoch 360, training loss: 0.15366463363170624 = 0.14639399945735931 + 0.001 * 7.270636081695557
Epoch 360, val loss: 0.8604280352592468
Epoch 370, training loss: 0.13388469815254211 = 0.1266164630651474 + 0.001 * 7.2682414054870605
Epoch 370, val loss: 0.8729897141456604
Epoch 380, training loss: 0.11724698543548584 = 0.10998154431581497 + 0.001 * 7.265438079833984
Epoch 380, val loss: 0.8869361281394958
Epoch 390, training loss: 0.10324349999427795 = 0.09595591574907303 + 0.001 * 7.287581443786621
Epoch 390, val loss: 0.9016979932785034
Epoch 400, training loss: 0.09131661802530289 = 0.08405805379152298 + 0.001 * 7.258565425872803
Epoch 400, val loss: 0.9168851375579834
Epoch 410, training loss: 0.0811576098203659 = 0.07390415668487549 + 0.001 * 7.253454685211182
Epoch 410, val loss: 0.9321944713592529
Epoch 420, training loss: 0.07246081531047821 = 0.06519590318202972 + 0.001 * 7.264913558959961
Epoch 420, val loss: 0.947465717792511
Epoch 430, training loss: 0.06495331972837448 = 0.05770403891801834 + 0.001 * 7.24928092956543
Epoch 430, val loss: 0.9626677632331848
Epoch 440, training loss: 0.058491818606853485 = 0.051248617470264435 + 0.001 * 7.243199825286865
Epoch 440, val loss: 0.9776393175125122
Epoch 450, training loss: 0.052915431559085846 = 0.045682393014431 + 0.001 * 7.23303747177124
Epoch 450, val loss: 0.992279589176178
Epoch 460, training loss: 0.048130061477422714 = 0.04088020697236061 + 0.001 * 7.24985408782959
Epoch 460, val loss: 1.0066072940826416
Epoch 470, training loss: 0.04396658018231392 = 0.03673136234283447 + 0.001 * 7.235217094421387
Epoch 470, val loss: 1.0204930305480957
Epoch 480, training loss: 0.04034971073269844 = 0.0331389456987381 + 0.001 * 7.2107648849487305
Epoch 480, val loss: 1.0339268445968628
Epoch 490, training loss: 0.037220440804958344 = 0.03001958690583706 + 0.001 * 7.200854301452637
Epoch 490, val loss: 1.046899437904358
Epoch 500, training loss: 0.03450337052345276 = 0.02730303816497326 + 0.001 * 7.200331687927246
Epoch 500, val loss: 1.059403657913208
Epoch 510, training loss: 0.032122932374477386 = 0.024928094819188118 + 0.001 * 7.19483757019043
Epoch 510, val loss: 1.0714521408081055
Epoch 520, training loss: 0.030033137649297714 = 0.0228436142206192 + 0.001 * 7.189524173736572
Epoch 520, val loss: 1.0830824375152588
Epoch 530, training loss: 0.028199760243296623 = 0.021006984636187553 + 0.001 * 7.192775249481201
Epoch 530, val loss: 1.094204068183899
Epoch 540, training loss: 0.02656598947942257 = 0.01938219927251339 + 0.001 * 7.1837897300720215
Epoch 540, val loss: 1.1048948764801025
Epoch 550, training loss: 0.025115881115198135 = 0.01793946884572506 + 0.001 * 7.176412105560303
Epoch 550, val loss: 1.1151986122131348
Epoch 560, training loss: 0.023835400119423866 = 0.016653884202241898 + 0.001 * 7.181515216827393
Epoch 560, val loss: 1.1250945329666138
Epoch 570, training loss: 0.022674821317195892 = 0.015503952279686928 + 0.001 * 7.170867919921875
Epoch 570, val loss: 1.1346571445465088
Epoch 580, training loss: 0.021640675142407417 = 0.014471525326371193 + 0.001 * 7.169149875640869
Epoch 580, val loss: 1.1438548564910889
Epoch 590, training loss: 0.02071034349501133 = 0.013541445136070251 + 0.001 * 7.16889762878418
Epoch 590, val loss: 1.1527190208435059
Epoch 600, training loss: 0.019885586574673653 = 0.012700837105512619 + 0.001 * 7.184749126434326
Epoch 600, val loss: 1.1612504720687866
Epoch 610, training loss: 0.019106652587652206 = 0.011938925832509995 + 0.001 * 7.167725563049316
Epoch 610, val loss: 1.1694848537445068
Epoch 620, training loss: 0.01840578019618988 = 0.011246283538639545 + 0.001 * 7.1594953536987305
Epoch 620, val loss: 1.1774126291275024
Epoch 630, training loss: 0.017779869958758354 = 0.010614828206598759 + 0.001 * 7.165041923522949
Epoch 630, val loss: 1.1850788593292236
Epoch 640, training loss: 0.017214884981513023 = 0.010037614963948727 + 0.001 * 7.177268981933594
Epoch 640, val loss: 1.192490577697754
Epoch 650, training loss: 0.01666482910513878 = 0.009508696384727955 + 0.001 * 7.156131744384766
Epoch 650, val loss: 1.1996654272079468
Epoch 660, training loss: 0.016177522018551826 = 0.009022850543260574 + 0.001 * 7.154671669006348
Epoch 660, val loss: 1.206614375114441
Epoch 670, training loss: 0.015734845772385597 = 0.008575579151511192 + 0.001 * 7.159266948699951
Epoch 670, val loss: 1.213361382484436
Epoch 680, training loss: 0.015316061675548553 = 0.008162962272763252 + 0.001 * 7.153099536895752
Epoch 680, val loss: 1.2198925018310547
Epoch 690, training loss: 0.014938097447156906 = 0.007781435735523701 + 0.001 * 7.156661033630371
Epoch 690, val loss: 1.2262178659439087
Epoch 700, training loss: 0.014580590650439262 = 0.007427949924021959 + 0.001 * 7.1526408195495605
Epoch 700, val loss: 1.2323869466781616
Epoch 710, training loss: 0.014240538701415062 = 0.007099861744791269 + 0.001 * 7.140676021575928
Epoch 710, val loss: 1.2383415699005127
Epoch 720, training loss: 0.013940872624516487 = 0.00679478095844388 + 0.001 * 7.146091938018799
Epoch 720, val loss: 1.2441537380218506
Epoch 730, training loss: 0.013644814491271973 = 0.006510620471090078 + 0.001 * 7.134194374084473
Epoch 730, val loss: 1.2497913837432861
Epoch 740, training loss: 0.013384649530053139 = 0.0062454817816615105 + 0.001 * 7.139167785644531
Epoch 740, val loss: 1.255268931388855
Epoch 750, training loss: 0.013129768893122673 = 0.0059977308847010136 + 0.001 * 7.132037162780762
Epoch 750, val loss: 1.260587215423584
Epoch 760, training loss: 0.012897273525595665 = 0.005765884183347225 + 0.001 * 7.131389141082764
Epoch 760, val loss: 1.2657626867294312
Epoch 770, training loss: 0.012705333530902863 = 0.005548570305109024 + 0.001 * 7.156762599945068
Epoch 770, val loss: 1.2707937955856323
Epoch 780, training loss: 0.012480949983000755 = 0.005344645120203495 + 0.001 * 7.1363043785095215
Epoch 780, val loss: 1.275696039199829
Epoch 790, training loss: 0.012289238162338734 = 0.005152977537363768 + 0.001 * 7.136260509490967
Epoch 790, val loss: 1.2804756164550781
Epoch 800, training loss: 0.012103577144443989 = 0.004972645081579685 + 0.001 * 7.130931854248047
Epoch 800, val loss: 1.2851202487945557
Epoch 810, training loss: 0.011933641508221626 = 0.004802718758583069 + 0.001 * 7.130921840667725
Epoch 810, val loss: 1.2896418571472168
Epoch 820, training loss: 0.0117642218247056 = 0.004642462823539972 + 0.001 * 7.121758460998535
Epoch 820, val loss: 1.294037103652954
Epoch 830, training loss: 0.011613382957875729 = 0.004491094965487719 + 0.001 * 7.122287750244141
Epoch 830, val loss: 1.2983287572860718
Epoch 840, training loss: 0.011472378857433796 = 0.00434799212962389 + 0.001 * 7.124386310577393
Epoch 840, val loss: 1.3025151491165161
Epoch 850, training loss: 0.01131821796298027 = 0.004212593659758568 + 0.001 * 7.105624198913574
Epoch 850, val loss: 1.3065804243087769
Epoch 860, training loss: 0.011234434321522713 = 0.004084342159330845 + 0.001 * 7.150091648101807
Epoch 860, val loss: 1.3105559349060059
Epoch 870, training loss: 0.011082255281507969 = 0.003962686751037836 + 0.001 * 7.119568347930908
Epoch 870, val loss: 1.3144210577011108
Epoch 880, training loss: 0.010981193743646145 = 0.003847208572551608 + 0.001 * 7.1339850425720215
Epoch 880, val loss: 1.3182060718536377
Epoch 890, training loss: 0.0108545683324337 = 0.0037375513929873705 + 0.001 * 7.117016315460205
Epoch 890, val loss: 1.3218775987625122
Epoch 900, training loss: 0.010773880407214165 = 0.0036332537420094013 + 0.001 * 7.140626907348633
Epoch 900, val loss: 1.3254778385162354
Epoch 910, training loss: 0.010648390278220177 = 0.0035340397153049707 + 0.001 * 7.114349842071533
Epoch 910, val loss: 1.328978180885315
Epoch 920, training loss: 0.01059883926063776 = 0.003439488587900996 + 0.001 * 7.1593499183654785
Epoch 920, val loss: 1.3324086666107178
Epoch 930, training loss: 0.01044791005551815 = 0.0033494189847260714 + 0.001 * 7.098491191864014
Epoch 930, val loss: 1.3357471227645874
Epoch 940, training loss: 0.010361981578171253 = 0.0032634625677019358 + 0.001 * 7.0985188484191895
Epoch 940, val loss: 1.3390154838562012
Epoch 950, training loss: 0.010290799662470818 = 0.0031814295798540115 + 0.001 * 7.10936975479126
Epoch 950, val loss: 1.342194676399231
Epoch 960, training loss: 0.01022596936672926 = 0.003103080438449979 + 0.001 * 7.122888565063477
Epoch 960, val loss: 1.3453007936477661
Epoch 970, training loss: 0.01012650690972805 = 0.003028137842193246 + 0.001 * 7.0983686447143555
Epoch 970, val loss: 1.3483355045318604
Epoch 980, training loss: 0.010043635964393616 = 0.002956472570076585 + 0.001 * 7.08716344833374
Epoch 980, val loss: 1.3513051271438599
Epoch 990, training loss: 0.009979646652936935 = 0.002887857612222433 + 0.001 * 7.0917887687683105
Epoch 990, val loss: 1.3542110919952393
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 1.9702212810516357 = 1.9616243839263916 + 0.001 * 8.596863746643066
Epoch 0, val loss: 1.9651941061019897
Epoch 10, training loss: 1.9593218564987183 = 1.9507250785827637 + 0.001 * 8.596819877624512
Epoch 10, val loss: 1.954583764076233
Epoch 20, training loss: 1.9460028409957886 = 1.9374061822891235 + 0.001 * 8.596674919128418
Epoch 20, val loss: 1.9409908056259155
Epoch 30, training loss: 1.9275659322738647 = 1.9189696311950684 + 0.001 * 8.596317291259766
Epoch 30, val loss: 1.9215551614761353
Epoch 40, training loss: 1.9005931615829468 = 1.8919978141784668 + 0.001 * 8.595376014709473
Epoch 40, val loss: 1.89289128780365
Epoch 50, training loss: 1.8631224632263184 = 1.854529857635498 + 0.001 * 8.592645645141602
Epoch 50, val loss: 1.8544952869415283
Epoch 60, training loss: 1.8223398923873901 = 1.8137564659118652 + 0.001 * 8.583452224731445
Epoch 60, val loss: 1.8174242973327637
Epoch 70, training loss: 1.789664387702942 = 1.7811152935028076 + 0.001 * 8.549110412597656
Epoch 70, val loss: 1.7918627262115479
Epoch 80, training loss: 1.7514947652816772 = 1.7431291341781616 + 0.001 * 8.36564826965332
Epoch 80, val loss: 1.75874924659729
Epoch 90, training loss: 1.6992552280426025 = 1.6911617517471313 + 0.001 * 8.093493461608887
Epoch 90, val loss: 1.7114630937576294
Epoch 100, training loss: 1.6271748542785645 = 1.6191984415054321 + 0.001 * 7.976444244384766
Epoch 100, val loss: 1.6474175453186035
Epoch 110, training loss: 1.5369973182678223 = 1.5291781425476074 + 0.001 * 7.819126605987549
Epoch 110, val loss: 1.5697083473205566
Epoch 120, training loss: 1.4390825033187866 = 1.4314112663269043 + 0.001 * 7.671250343322754
Epoch 120, val loss: 1.4881994724273682
Epoch 130, training loss: 1.3434159755706787 = 1.335776686668396 + 0.001 * 7.639289855957031
Epoch 130, val loss: 1.4129133224487305
Epoch 140, training loss: 1.2524240016937256 = 1.2448279857635498 + 0.001 * 7.59597110748291
Epoch 140, val loss: 1.3442977666854858
Epoch 150, training loss: 1.1659026145935059 = 1.1583564281463623 + 0.001 * 7.546226501464844
Epoch 150, val loss: 1.2801705598831177
Epoch 160, training loss: 1.0848610401153564 = 1.0773656368255615 + 0.001 * 7.49544095993042
Epoch 160, val loss: 1.2212941646575928
Epoch 170, training loss: 1.0098295211791992 = 1.00237238407135 + 0.001 * 7.457196235656738
Epoch 170, val loss: 1.1676373481750488
Epoch 180, training loss: 0.9394264817237854 = 0.9320034384727478 + 0.001 * 7.423036575317383
Epoch 180, val loss: 1.1174317598342896
Epoch 190, training loss: 0.8718849420547485 = 0.864501416683197 + 0.001 * 7.383530616760254
Epoch 190, val loss: 1.0695210695266724
Epoch 200, training loss: 0.8062916398048401 = 0.798934817314148 + 0.001 * 7.356822967529297
Epoch 200, val loss: 1.0234135389328003
Epoch 210, training loss: 0.7428302764892578 = 0.7354885339736938 + 0.001 * 7.341719627380371
Epoch 210, val loss: 0.9805117249488831
Epoch 220, training loss: 0.6823386549949646 = 0.6750108003616333 + 0.001 * 7.327849864959717
Epoch 220, val loss: 0.9425796270370483
Epoch 230, training loss: 0.6253718733787537 = 0.6180616617202759 + 0.001 * 7.310214996337891
Epoch 230, val loss: 0.9111146926879883
Epoch 240, training loss: 0.5718767046928406 = 0.5645850896835327 + 0.001 * 7.291640281677246
Epoch 240, val loss: 0.8864928483963013
Epoch 250, training loss: 0.5215120315551758 = 0.5142418742179871 + 0.001 * 7.270153045654297
Epoch 250, val loss: 0.8683134317398071
Epoch 260, training loss: 0.47395190596580505 = 0.46669673919677734 + 0.001 * 7.2551703453063965
Epoch 260, val loss: 0.8554445505142212
Epoch 270, training loss: 0.42901289463043213 = 0.4217695891857147 + 0.001 * 7.243312358856201
Epoch 270, val loss: 0.8471860885620117
Epoch 280, training loss: 0.38671958446502686 = 0.3794873058795929 + 0.001 * 7.2322916984558105
Epoch 280, val loss: 0.8429611921310425
Epoch 290, training loss: 0.34722110629081726 = 0.33999404311180115 + 0.001 * 7.227066993713379
Epoch 290, val loss: 0.842410147190094
Epoch 300, training loss: 0.3105461299419403 = 0.3033195734024048 + 0.001 * 7.2265543937683105
Epoch 300, val loss: 0.8451424241065979
Epoch 310, training loss: 0.2764827311038971 = 0.2692554295063019 + 0.001 * 7.227293491363525
Epoch 310, val loss: 0.8509714603424072
Epoch 320, training loss: 0.24473856389522552 = 0.23751239478588104 + 0.001 * 7.226171493530273
Epoch 320, val loss: 0.8594536781311035
Epoch 330, training loss: 0.2152516096830368 = 0.20802512764930725 + 0.001 * 7.226478576660156
Epoch 330, val loss: 0.8702656030654907
Epoch 340, training loss: 0.18823204934597015 = 0.18100771307945251 + 0.001 * 7.224330902099609
Epoch 340, val loss: 0.883177638053894
Epoch 350, training loss: 0.16397641599178314 = 0.1567513793706894 + 0.001 * 7.225043296813965
Epoch 350, val loss: 0.8980609178543091
Epoch 360, training loss: 0.1426357477903366 = 0.13541294634342194 + 0.001 * 7.2227959632873535
Epoch 360, val loss: 0.9147911071777344
Epoch 370, training loss: 0.12415548413991928 = 0.11693395674228668 + 0.001 * 7.221523761749268
Epoch 370, val loss: 0.9330180883407593
Epoch 380, training loss: 0.10831204056739807 = 0.10109212249517441 + 0.001 * 7.2199177742004395
Epoch 380, val loss: 0.952492356300354
Epoch 390, training loss: 0.09482164680957794 = 0.08760270476341248 + 0.001 * 7.218944549560547
Epoch 390, val loss: 0.9728987216949463
Epoch 400, training loss: 0.08338406682014465 = 0.0761629045009613 + 0.001 * 7.221165180206299
Epoch 400, val loss: 0.9937562346458435
Epoch 410, training loss: 0.07370009273290634 = 0.06648322194814682 + 0.001 * 7.216867923736572
Epoch 410, val loss: 1.0148526430130005
Epoch 420, training loss: 0.06551568955183029 = 0.05830024555325508 + 0.001 * 7.215444564819336
Epoch 420, val loss: 1.035916805267334
Epoch 430, training loss: 0.058594994246959686 = 0.05137931555509567 + 0.001 * 7.215678691864014
Epoch 430, val loss: 1.056747317314148
Epoch 440, training loss: 0.052724551409482956 = 0.04551189765334129 + 0.001 * 7.212654113769531
Epoch 440, val loss: 1.0771359205245972
Epoch 450, training loss: 0.0477430522441864 = 0.04052146151661873 + 0.001 * 7.2215895652771
Epoch 450, val loss: 1.0969692468643188
Epoch 460, training loss: 0.043471649289131165 = 0.03626083955168724 + 0.001 * 7.210808753967285
Epoch 460, val loss: 1.1161705255508423
Epoch 470, training loss: 0.03982618823647499 = 0.03260837495326996 + 0.001 * 7.2178120613098145
Epoch 470, val loss: 1.1347081661224365
Epoch 480, training loss: 0.03667372837662697 = 0.029462464153766632 + 0.001 * 7.211263179779053
Epoch 480, val loss: 1.1525405645370483
Epoch 490, training loss: 0.033943209797143936 = 0.026740113273262978 + 0.001 * 7.203097343444824
Epoch 490, val loss: 1.1697248220443726
Epoch 500, training loss: 0.031574953347444534 = 0.024373149499297142 + 0.001 * 7.201803684234619
Epoch 500, val loss: 1.1862245798110962
Epoch 510, training loss: 0.029515985399484634 = 0.022305233404040337 + 0.001 * 7.210752010345459
Epoch 510, val loss: 1.2021104097366333
Epoch 520, training loss: 0.02768472582101822 = 0.020490098744630814 + 0.001 * 7.194627285003662
Epoch 520, val loss: 1.217365026473999
Epoch 530, training loss: 0.02607748657464981 = 0.018889544531702995 + 0.001 * 7.18794059753418
Epoch 530, val loss: 1.2320430278778076
Epoch 540, training loss: 0.024660028517246246 = 0.017471978440880775 + 0.001 * 7.188048839569092
Epoch 540, val loss: 1.2461587190628052
Epoch 550, training loss: 0.02338692545890808 = 0.016211511567234993 + 0.001 * 7.175413131713867
Epoch 550, val loss: 1.2597434520721436
Epoch 560, training loss: 0.022282946854829788 = 0.01508623082190752 + 0.001 * 7.196715354919434
Epoch 560, val loss: 1.2728173732757568
Epoch 570, training loss: 0.021243466064333916 = 0.01407745759934187 + 0.001 * 7.166008472442627
Epoch 570, val loss: 1.2854253053665161
Epoch 580, training loss: 0.02034960314631462 = 0.013168826699256897 + 0.001 * 7.180775165557861
Epoch 580, val loss: 1.2975879907608032
Epoch 590, training loss: 0.019518887624144554 = 0.012347106821835041 + 0.001 * 7.171780586242676
Epoch 590, val loss: 1.3093763589859009
Epoch 600, training loss: 0.01875842548906803 = 0.011598377488553524 + 0.001 * 7.1600470542907715
Epoch 600, val loss: 1.320885181427002
Epoch 610, training loss: 0.018078207969665527 = 0.010908588767051697 + 0.001 * 7.169618606567383
Epoch 610, val loss: 1.3322701454162598
Epoch 620, training loss: 0.01741154119372368 = 0.010269038379192352 + 0.001 * 7.142502784729004
Epoch 620, val loss: 1.3435989618301392
Epoch 630, training loss: 0.01684708334505558 = 0.009674925357103348 + 0.001 * 7.1721577644348145
Epoch 630, val loss: 1.35489022731781
Epoch 640, training loss: 0.016271457076072693 = 0.009124242700636387 + 0.001 * 7.147213935852051
Epoch 640, val loss: 1.3660638332366943
Epoch 650, training loss: 0.01575099490582943 = 0.008614309132099152 + 0.001 * 7.136685371398926
Epoch 650, val loss: 1.377098560333252
Epoch 660, training loss: 0.015288776718080044 = 0.008143020793795586 + 0.001 * 7.145755767822266
Epoch 660, val loss: 1.3878990411758423
Epoch 670, training loss: 0.014836708083748817 = 0.007707970216870308 + 0.001 * 7.128737926483154
Epoch 670, val loss: 1.3985048532485962
Epoch 680, training loss: 0.014425734058022499 = 0.007306273095309734 + 0.001 * 7.119460582733154
Epoch 680, val loss: 1.4088642597198486
Epoch 690, training loss: 0.014069657772779465 = 0.006935300305485725 + 0.001 * 7.134357452392578
Epoch 690, val loss: 1.4189718961715698
Epoch 700, training loss: 0.01372411847114563 = 0.0065923091024160385 + 0.001 * 7.131809234619141
Epoch 700, val loss: 1.428813099861145
Epoch 710, training loss: 0.013386608101427555 = 0.006274975370615721 + 0.001 * 7.111632347106934
Epoch 710, val loss: 1.438428521156311
Epoch 720, training loss: 0.01309896633028984 = 0.005981114227324724 + 0.001 * 7.117852210998535
Epoch 720, val loss: 1.4477894306182861
Epoch 730, training loss: 0.012823382392525673 = 0.0057085310108959675 + 0.001 * 7.1148505210876465
Epoch 730, val loss: 1.4569175243377686
Epoch 740, training loss: 0.012568920850753784 = 0.005455374717712402 + 0.001 * 7.113546371459961
Epoch 740, val loss: 1.465788722038269
Epoch 750, training loss: 0.012339930981397629 = 0.0052200122736394405 + 0.001 * 7.1199188232421875
Epoch 750, val loss: 1.4744738340377808
Epoch 760, training loss: 0.012113627046346664 = 0.005000729579478502 + 0.001 * 7.1128973960876465
Epoch 760, val loss: 1.4829089641571045
Epoch 770, training loss: 0.011877915821969509 = 0.004796204622834921 + 0.001 * 7.0817108154296875
Epoch 770, val loss: 1.491162896156311
Epoch 780, training loss: 0.011683396995067596 = 0.004605130758136511 + 0.001 * 7.078266143798828
Epoch 780, val loss: 1.4992135763168335
Epoch 790, training loss: 0.011510593816637993 = 0.004426336847245693 + 0.001 * 7.084256172180176
Epoch 790, val loss: 1.507051944732666
Epoch 800, training loss: 0.01135205291211605 = 0.004258868284523487 + 0.001 * 7.093184471130371
Epoch 800, val loss: 1.514710545539856
Epoch 810, training loss: 0.011216022074222565 = 0.004101812373846769 + 0.001 * 7.114209175109863
Epoch 810, val loss: 1.5221892595291138
Epoch 820, training loss: 0.011041879653930664 = 0.003954271320253611 + 0.001 * 7.0876078605651855
Epoch 820, val loss: 1.5294615030288696
Epoch 830, training loss: 0.010910934768617153 = 0.0038155147340148687 + 0.001 * 7.095419406890869
Epoch 830, val loss: 1.5366003513336182
Epoch 840, training loss: 0.010753171518445015 = 0.0036848699674010277 + 0.001 * 7.068301677703857
Epoch 840, val loss: 1.5435491800308228
Epoch 850, training loss: 0.010623475536704063 = 0.003561806632205844 + 0.001 * 7.061668872833252
Epoch 850, val loss: 1.550337553024292
Epoch 860, training loss: 0.010538225993514061 = 0.0034456891007721424 + 0.001 * 7.092536926269531
Epoch 860, val loss: 1.5569868087768555
Epoch 870, training loss: 0.010408557020127773 = 0.003335977206006646 + 0.001 * 7.072579860687256
Epoch 870, val loss: 1.5634759664535522
Epoch 880, training loss: 0.010293098166584969 = 0.003232246730476618 + 0.001 * 7.060851097106934
Epoch 880, val loss: 1.5698039531707764
Epoch 890, training loss: 0.010213173925876617 = 0.003134057391434908 + 0.001 * 7.079115867614746
Epoch 890, val loss: 1.5759953260421753
Epoch 900, training loss: 0.010106503963470459 = 0.0030409698374569416 + 0.001 * 7.065533638000488
Epoch 900, val loss: 1.5820715427398682
Epoch 910, training loss: 0.009992687962949276 = 0.002952717477455735 + 0.001 * 7.039970397949219
Epoch 910, val loss: 1.587992548942566
Epoch 920, training loss: 0.009913611225783825 = 0.0028689266182482243 + 0.001 * 7.044684410095215
Epoch 920, val loss: 1.5937833786010742
Epoch 930, training loss: 0.009851953946053982 = 0.0027892857324332 + 0.001 * 7.0626678466796875
Epoch 930, val loss: 1.5994470119476318
Epoch 940, training loss: 0.009769322350621223 = 0.0027135703712701797 + 0.001 * 7.055751800537109
Epoch 940, val loss: 1.604982852935791
Epoch 950, training loss: 0.009685716591775417 = 0.0026415439788252115 + 0.001 * 7.044172286987305
Epoch 950, val loss: 1.6104358434677124
Epoch 960, training loss: 0.009607141837477684 = 0.0025729015469551086 + 0.001 * 7.034240245819092
Epoch 960, val loss: 1.6157383918762207
Epoch 970, training loss: 0.009564800187945366 = 0.002507431199774146 + 0.001 * 7.057368755340576
Epoch 970, val loss: 1.6209499835968018
Epoch 980, training loss: 0.009485417976975441 = 0.002444968093186617 + 0.001 * 7.040449142456055
Epoch 980, val loss: 1.6260429620742798
Epoch 990, training loss: 0.00941390823572874 = 0.0023853552993386984 + 0.001 * 7.028553009033203
Epoch 990, val loss: 1.6310359239578247
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 1.9365512132644653 = 1.9279543161392212 + 0.001 * 8.59687614440918
Epoch 0, val loss: 1.9195516109466553
Epoch 10, training loss: 1.9264886379241943 = 1.9178918600082397 + 0.001 * 8.596820831298828
Epoch 10, val loss: 1.9099050760269165
Epoch 20, training loss: 1.9142347574234009 = 1.9056380987167358 + 0.001 * 8.596654891967773
Epoch 20, val loss: 1.8979421854019165
Epoch 30, training loss: 1.8976171016693115 = 1.8890209197998047 + 0.001 * 8.596238136291504
Epoch 30, val loss: 1.8815100193023682
Epoch 40, training loss: 1.8742064237594604 = 1.86561119556427 + 0.001 * 8.595210075378418
Epoch 40, val loss: 1.8585443496704102
Epoch 50, training loss: 1.8438339233398438 = 1.8352415561676025 + 0.001 * 8.592354774475098
Epoch 50, val loss: 1.8305178880691528
Epoch 60, training loss: 1.8126779794692993 = 1.8040951490402222 + 0.001 * 8.582789421081543
Epoch 60, val loss: 1.80558443069458
Epoch 70, training loss: 1.783337950706482 = 1.774795651435852 + 0.001 * 8.542302131652832
Epoch 70, val loss: 1.7832229137420654
Epoch 80, training loss: 1.7438441514968872 = 1.7355709075927734 + 0.001 * 8.273275375366211
Epoch 80, val loss: 1.749347448348999
Epoch 90, training loss: 1.6883083581924438 = 1.6802294254302979 + 0.001 * 8.078927993774414
Epoch 90, val loss: 1.6998153924942017
Epoch 100, training loss: 1.6127943992614746 = 1.6048766374588013 + 0.001 * 7.91775369644165
Epoch 100, val loss: 1.6336482763290405
Epoch 110, training loss: 1.5230827331542969 = 1.5153355598449707 + 0.001 * 7.747130870819092
Epoch 110, val loss: 1.5588951110839844
Epoch 120, training loss: 1.426565408706665 = 1.4188793897628784 + 0.001 * 7.6860456466674805
Epoch 120, val loss: 1.481648564338684
Epoch 130, training loss: 1.3275443315505981 = 1.3199031352996826 + 0.001 * 7.641207695007324
Epoch 130, val loss: 1.4051262140274048
Epoch 140, training loss: 1.2270236015319824 = 1.21945059299469 + 0.001 * 7.573063373565674
Epoch 140, val loss: 1.3291656970977783
Epoch 150, training loss: 1.1269268989562988 = 1.1194523572921753 + 0.001 * 7.474550247192383
Epoch 150, val loss: 1.2547968626022339
Epoch 160, training loss: 1.0301624536514282 = 1.0227737426757812 + 0.001 * 7.3886847496032715
Epoch 160, val loss: 1.184828281402588
Epoch 170, training loss: 0.938933253288269 = 0.9315651655197144 + 0.001 * 7.3680830001831055
Epoch 170, val loss: 1.120940089225769
Epoch 180, training loss: 0.8544051647186279 = 0.8470455408096313 + 0.001 * 7.359614372253418
Epoch 180, val loss: 1.064136266708374
Epoch 190, training loss: 0.7771807312965393 = 0.7698276042938232 + 0.001 * 7.353126525878906
Epoch 190, val loss: 1.013715147972107
Epoch 200, training loss: 0.7070152163505554 = 0.6996638774871826 + 0.001 * 7.351325035095215
Epoch 200, val loss: 0.9702460169792175
Epoch 210, training loss: 0.6426345109939575 = 0.635285496711731 + 0.001 * 7.3490142822265625
Epoch 210, val loss: 0.9330183863639832
Epoch 220, training loss: 0.5820724964141846 = 0.5747254490852356 + 0.001 * 7.347076416015625
Epoch 220, val loss: 0.9008352160453796
Epoch 230, training loss: 0.5239837169647217 = 0.5166382789611816 + 0.001 * 7.345430374145508
Epoch 230, val loss: 0.8728405237197876
Epoch 240, training loss: 0.468283474445343 = 0.46093881130218506 + 0.001 * 7.344671726226807
Epoch 240, val loss: 0.8493055701255798
Epoch 250, training loss: 0.41602247953414917 = 0.40867850184440613 + 0.001 * 7.343964576721191
Epoch 250, val loss: 0.8312107920646667
Epoch 260, training loss: 0.36807936429977417 = 0.36073535680770874 + 0.001 * 7.344013690948486
Epoch 260, val loss: 0.819329559803009
Epoch 270, training loss: 0.3248370289802551 = 0.317492812871933 + 0.001 * 7.344201564788818
Epoch 270, val loss: 0.8133867979049683
Epoch 280, training loss: 0.28612250089645386 = 0.2787778079509735 + 0.001 * 7.344705581665039
Epoch 280, val loss: 0.8123560547828674
Epoch 290, training loss: 0.25159499049186707 = 0.24425090849399567 + 0.001 * 7.344071388244629
Epoch 290, val loss: 0.8152227401733398
Epoch 300, training loss: 0.22092953324317932 = 0.21358636021614075 + 0.001 * 7.343173027038574
Epoch 300, val loss: 0.8212565183639526
Epoch 310, training loss: 0.1938508003950119 = 0.18651120364665985 + 0.001 * 7.339599132537842
Epoch 310, val loss: 0.8298229575157166
Epoch 320, training loss: 0.1700955480337143 = 0.1627584546804428 + 0.001 * 7.337100028991699
Epoch 320, val loss: 0.8405570983886719
Epoch 330, training loss: 0.14936569333076477 = 0.14203806221485138 + 0.001 * 7.327635765075684
Epoch 330, val loss: 0.853072464466095
Epoch 340, training loss: 0.13136808574199677 = 0.12404225021600723 + 0.001 * 7.325828552246094
Epoch 340, val loss: 0.8670282959938049
Epoch 350, training loss: 0.11577722430229187 = 0.1084698885679245 + 0.001 * 7.30733585357666
Epoch 350, val loss: 0.8821797966957092
Epoch 360, training loss: 0.10233262926340103 = 0.09503069519996643 + 0.001 * 7.3019328117370605
Epoch 360, val loss: 0.898159384727478
Epoch 370, training loss: 0.09074036031961441 = 0.08345617353916168 + 0.001 * 7.284183025360107
Epoch 370, val loss: 0.9146735668182373
Epoch 380, training loss: 0.08077315986156464 = 0.0734940767288208 + 0.001 * 7.279086112976074
Epoch 380, val loss: 0.9314919710159302
Epoch 390, training loss: 0.07218766212463379 = 0.06491115689277649 + 0.001 * 7.276505470275879
Epoch 390, val loss: 0.9484579563140869
Epoch 400, training loss: 0.06476925313472748 = 0.05751493200659752 + 0.001 * 7.2543230056762695
Epoch 400, val loss: 0.9653300046920776
Epoch 410, training loss: 0.05840102210640907 = 0.05113844946026802 + 0.001 * 7.262571334838867
Epoch 410, val loss: 0.982070803642273
Epoch 420, training loss: 0.05289455130696297 = 0.04563450813293457 + 0.001 * 7.2600417137146
Epoch 420, val loss: 0.9985582232475281
Epoch 430, training loss: 0.04812506586313248 = 0.0408761240541935 + 0.001 * 7.2489423751831055
Epoch 430, val loss: 1.0146855115890503
Epoch 440, training loss: 0.04400308430194855 = 0.036753829568624496 + 0.001 * 7.2492547035217285
Epoch 440, val loss: 1.030379056930542
Epoch 450, training loss: 0.040416453033685684 = 0.03317321836948395 + 0.001 * 7.2432332038879395
Epoch 450, val loss: 1.0456472635269165
Epoch 460, training loss: 0.037296194583177567 = 0.03005298227071762 + 0.001 * 7.243210792541504
Epoch 460, val loss: 1.0604649782180786
Epoch 470, training loss: 0.03456425294280052 = 0.02732515148818493 + 0.001 * 7.239100933074951
Epoch 470, val loss: 1.0748063325881958
Epoch 480, training loss: 0.03220575302839279 = 0.02493203803896904 + 0.001 * 7.273714542388916
Epoch 480, val loss: 1.0886918306350708
Epoch 490, training loss: 0.030069291591644287 = 0.022825533524155617 + 0.001 * 7.2437567710876465
Epoch 490, val loss: 1.10207200050354
Epoch 500, training loss: 0.028196845203638077 = 0.020965194329619408 + 0.001 * 7.2316508293151855
Epoch 500, val loss: 1.1149895191192627
Epoch 510, training loss: 0.026567071676254272 = 0.019316881895065308 + 0.001 * 7.250188827514648
Epoch 510, val loss: 1.1274774074554443
Epoch 520, training loss: 0.02508672885596752 = 0.017851699143648148 + 0.001 * 7.235029220581055
Epoch 520, val loss: 1.1395444869995117
Epoch 530, training loss: 0.02377307415008545 = 0.016545135527849197 + 0.001 * 7.2279372215271
Epoch 530, val loss: 1.151208519935608
Epoch 540, training loss: 0.022610165178775787 = 0.015376170165836811 + 0.001 * 7.23399543762207
Epoch 540, val loss: 1.162477970123291
Epoch 550, training loss: 0.021550584584474564 = 0.014326907694339752 + 0.001 * 7.223675727844238
Epoch 550, val loss: 1.173377275466919
Epoch 560, training loss: 0.020599087700247765 = 0.013382256962358952 + 0.001 * 7.216830730438232
Epoch 560, val loss: 1.1838749647140503
Epoch 570, training loss: 0.019745979458093643 = 0.012529117986559868 + 0.001 * 7.216861724853516
Epoch 570, val loss: 1.1940604448318481
Epoch 580, training loss: 0.018984561786055565 = 0.011756415478885174 + 0.001 * 7.228145599365234
Epoch 580, val loss: 1.2039045095443726
Epoch 590, training loss: 0.018289808183908463 = 0.011054566130042076 + 0.001 * 7.235241889953613
Epoch 590, val loss: 1.213452935218811
Epoch 600, training loss: 0.017627481371164322 = 0.0104153947904706 + 0.001 * 7.212087154388428
Epoch 600, val loss: 1.2226980924606323
Epoch 610, training loss: 0.017038142308592796 = 0.009831912815570831 + 0.001 * 7.2062296867370605
Epoch 610, val loss: 1.2316796779632568
Epoch 620, training loss: 0.016502520069479942 = 0.009297966957092285 + 0.001 * 7.204552173614502
Epoch 620, val loss: 1.2403956651687622
Epoch 630, training loss: 0.01602574810385704 = 0.008808162994682789 + 0.001 * 7.217583656311035
Epoch 630, val loss: 1.2488534450531006
Epoch 640, training loss: 0.015557797625660896 = 0.008357829414308071 + 0.001 * 7.199967384338379
Epoch 640, val loss: 1.2571057081222534
Epoch 650, training loss: 0.015139369294047356 = 0.007942931726574898 + 0.001 * 7.196437358856201
Epoch 650, val loss: 1.265100121498108
Epoch 660, training loss: 0.014762986451387405 = 0.007559870835393667 + 0.001 * 7.203114986419678
Epoch 660, val loss: 1.2728803157806396
Epoch 670, training loss: 0.014405796304345131 = 0.00720532750710845 + 0.001 * 7.20046854019165
Epoch 670, val loss: 1.2804852724075317
Epoch 680, training loss: 0.014054556377232075 = 0.0068760644644498825 + 0.001 * 7.178491592407227
Epoch 680, val loss: 1.2879009246826172
Epoch 690, training loss: 0.013758689165115356 = 0.0065688323229551315 + 0.001 * 7.189857006072998
Epoch 690, val loss: 1.2951780557632446
Epoch 700, training loss: 0.013469558209180832 = 0.006280556321144104 + 0.001 * 7.189001560211182
Epoch 700, val loss: 1.3023418188095093
Epoch 710, training loss: 0.013193339109420776 = 0.006009172182530165 + 0.001 * 7.184166431427002
Epoch 710, val loss: 1.309382438659668
Epoch 720, training loss: 0.012964753434062004 = 0.005753200501203537 + 0.001 * 7.211552619934082
Epoch 720, val loss: 1.316335916519165
Epoch 730, training loss: 0.01270284317433834 = 0.005511755123734474 + 0.001 * 7.191087245941162
Epoch 730, val loss: 1.3232303857803345
Epoch 740, training loss: 0.012451902963221073 = 0.005284170154482126 + 0.001 * 7.167732238769531
Epoch 740, val loss: 1.329986810684204
Epoch 750, training loss: 0.012249477207660675 = 0.0050698136910796165 + 0.001 * 7.17966365814209
Epoch 750, val loss: 1.33659827709198
Epoch 760, training loss: 0.012047497555613518 = 0.004867947660386562 + 0.001 * 7.179549694061279
Epoch 760, val loss: 1.3431730270385742
Epoch 770, training loss: 0.011848718859255314 = 0.004677947610616684 + 0.001 * 7.17077112197876
Epoch 770, val loss: 1.3495787382125854
Epoch 780, training loss: 0.01164606586098671 = 0.0044990479946136475 + 0.001 * 7.147017955780029
Epoch 780, val loss: 1.355910062789917
Epoch 790, training loss: 0.011574865318834782 = 0.004330542869865894 + 0.001 * 7.244322299957275
Epoch 790, val loss: 1.3621399402618408
Epoch 800, training loss: 0.011334572918713093 = 0.00417183805257082 + 0.001 * 7.162734508514404
Epoch 800, val loss: 1.3682093620300293
Epoch 810, training loss: 0.011159611865878105 = 0.0040222457610070705 + 0.001 * 7.13736629486084
Epoch 810, val loss: 1.3741711378097534
Epoch 820, training loss: 0.011048644781112671 = 0.003881153417751193 + 0.001 * 7.1674909591674805
Epoch 820, val loss: 1.3800045251846313
Epoch 830, training loss: 0.010906621813774109 = 0.0037479964084923267 + 0.001 * 7.158625602722168
Epoch 830, val loss: 1.3857923746109009
Epoch 840, training loss: 0.010748622938990593 = 0.0036222056951373816 + 0.001 * 7.12641716003418
Epoch 840, val loss: 1.391396164894104
Epoch 850, training loss: 0.010641099885106087 = 0.003503284649923444 + 0.001 * 7.137814998626709
Epoch 850, val loss: 1.3969497680664062
Epoch 860, training loss: 0.010551810264587402 = 0.003390755970031023 + 0.001 * 7.161054611206055
Epoch 860, val loss: 1.4024040699005127
Epoch 870, training loss: 0.010421127080917358 = 0.003284223610535264 + 0.001 * 7.136903285980225
Epoch 870, val loss: 1.4077081680297852
Epoch 880, training loss: 0.010303691029548645 = 0.0031832766253501177 + 0.001 * 7.1204142570495605
Epoch 880, val loss: 1.412949562072754
Epoch 890, training loss: 0.010221126489341259 = 0.003087520133703947 + 0.001 * 7.13360595703125
Epoch 890, val loss: 1.4180821180343628
Epoch 900, training loss: 0.010148907080292702 = 0.00299665960483253 + 0.001 * 7.152246952056885
Epoch 900, val loss: 1.4231553077697754
Epoch 910, training loss: 0.010071017779409885 = 0.002910331357270479 + 0.001 * 7.160686016082764
Epoch 910, val loss: 1.4280563592910767
Epoch 920, training loss: 0.00993989035487175 = 0.0028282662387937307 + 0.001 * 7.111623764038086
Epoch 920, val loss: 1.432909369468689
Epoch 930, training loss: 0.009857634082436562 = 0.00275021162815392 + 0.001 * 7.107421875
Epoch 930, val loss: 1.4376806020736694
Epoch 940, training loss: 0.009831182658672333 = 0.0026758438907563686 + 0.001 * 7.155338764190674
Epoch 940, val loss: 1.4423359632492065
Epoch 950, training loss: 0.009731306694447994 = 0.0026049665175378323 + 0.001 * 7.126339912414551
Epoch 950, val loss: 1.4469599723815918
Epoch 960, training loss: 0.009640239179134369 = 0.00253731245175004 + 0.001 * 7.102926731109619
Epoch 960, val loss: 1.4514886140823364
Epoch 970, training loss: 0.009628593921661377 = 0.0024725093971937895 + 0.001 * 7.156084060668945
Epoch 970, val loss: 1.4560000896453857
Epoch 980, training loss: 0.009528258815407753 = 0.002410248387604952 + 0.001 * 7.118010520935059
Epoch 980, val loss: 1.4603914022445679
Epoch 990, training loss: 0.009447941556572914 = 0.002350175753235817 + 0.001 * 7.097765922546387
Epoch 990, val loss: 1.4648571014404297
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8096995255666843
The final CL Acc:0.76790, 0.01944, The final GNN Acc:0.81093, 0.00174
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13188])
remove edge: torch.Size([2, 7898])
updated graph: torch.Size([2, 10530])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9584778547286987 = 1.9498809576034546 + 0.001 * 8.596875190734863
Epoch 0, val loss: 1.9511759281158447
Epoch 10, training loss: 1.9479693174362183 = 1.9393725395202637 + 0.001 * 8.596826553344727
Epoch 10, val loss: 1.9410158395767212
Epoch 20, training loss: 1.9346615076065063 = 1.9260648488998413 + 0.001 * 8.596664428710938
Epoch 20, val loss: 1.927800178527832
Epoch 30, training loss: 1.9155361652374268 = 1.90693998336792 + 0.001 * 8.596229553222656
Epoch 30, val loss: 1.9087214469909668
Epoch 40, training loss: 1.8870892524719238 = 1.8784942626953125 + 0.001 * 8.594963073730469
Epoch 40, val loss: 1.8808693885803223
Epoch 50, training loss: 1.8476699590682983 = 1.83907949924469 + 0.001 * 8.59050178527832
Epoch 50, val loss: 1.8441327810287476
Epoch 60, training loss: 1.804379940032959 = 1.7958096265792847 + 0.001 * 8.570292472839355
Epoch 60, val loss: 1.8073394298553467
Epoch 70, training loss: 1.7670167684555054 = 1.758544921875 + 0.001 * 8.471890449523926
Epoch 70, val loss: 1.7757505178451538
Epoch 80, training loss: 1.7195159196853638 = 1.7113587856292725 + 0.001 * 8.15713882446289
Epoch 80, val loss: 1.7321038246154785
Epoch 90, training loss: 1.6533392667770386 = 1.6452758312225342 + 0.001 * 8.063420295715332
Epoch 90, val loss: 1.674663782119751
Epoch 100, training loss: 1.566192388534546 = 1.5582332611083984 + 0.001 * 7.95914888381958
Epoch 100, val loss: 1.6025416851043701
Epoch 110, training loss: 1.4662121534347534 = 1.4584075212478638 + 0.001 * 7.8046417236328125
Epoch 110, val loss: 1.5208882093429565
Epoch 120, training loss: 1.364289402961731 = 1.3565878868103027 + 0.001 * 7.701574325561523
Epoch 120, val loss: 1.4371020793914795
Epoch 130, training loss: 1.2663475275039673 = 1.2586520910263062 + 0.001 * 7.695380210876465
Epoch 130, val loss: 1.3570622205734253
Epoch 140, training loss: 1.173755407333374 = 1.1660906076431274 + 0.001 * 7.664857387542725
Epoch 140, val loss: 1.2823625802993774
Epoch 150, training loss: 1.0869497060775757 = 1.0793089866638184 + 0.001 * 7.6407670974731445
Epoch 150, val loss: 1.2137826681137085
Epoch 160, training loss: 1.006198763847351 = 0.9985896348953247 + 0.001 * 7.6090922355651855
Epoch 160, val loss: 1.1518064737319946
Epoch 170, training loss: 0.9303712844848633 = 0.9228034615516663 + 0.001 * 7.567829132080078
Epoch 170, val loss: 1.0943963527679443
Epoch 180, training loss: 0.8583954572677612 = 0.8508750796318054 + 0.001 * 7.520395278930664
Epoch 180, val loss: 1.04020357131958
Epoch 190, training loss: 0.7904965877532959 = 0.7830143570899963 + 0.001 * 7.4822282791137695
Epoch 190, val loss: 0.9890574216842651
Epoch 200, training loss: 0.7276476621627808 = 0.7202176451683044 + 0.001 * 7.429997444152832
Epoch 200, val loss: 0.9425011277198792
Epoch 210, training loss: 0.6704607009887695 = 0.6630781292915344 + 0.001 * 7.382584571838379
Epoch 210, val loss: 0.9019051790237427
Epoch 220, training loss: 0.6183859705924988 = 0.6110360026359558 + 0.001 * 7.349985599517822
Epoch 220, val loss: 0.8677957653999329
Epoch 230, training loss: 0.5701797604560852 = 0.5628543496131897 + 0.001 * 7.325434684753418
Epoch 230, val loss: 0.8395037651062012
Epoch 240, training loss: 0.5251783728599548 = 0.5178733468055725 + 0.001 * 7.305030822753906
Epoch 240, val loss: 0.8164201378822327
Epoch 250, training loss: 0.48323461413383484 = 0.4759448170661926 + 0.001 * 7.289801120758057
Epoch 250, val loss: 0.798153817653656
Epoch 260, training loss: 0.4445207118988037 = 0.437237411737442 + 0.001 * 7.283295154571533
Epoch 260, val loss: 0.7844932079315186
Epoch 270, training loss: 0.40915390849113464 = 0.4018750488758087 + 0.001 * 7.27885103225708
Epoch 270, val loss: 0.7754489183425903
Epoch 280, training loss: 0.3770640194416046 = 0.36979344487190247 + 0.001 * 7.2705817222595215
Epoch 280, val loss: 0.7709216475486755
Epoch 290, training loss: 0.3479596972465515 = 0.34069159626960754 + 0.001 * 7.268100261688232
Epoch 290, val loss: 0.7705996036529541
Epoch 300, training loss: 0.3214017152786255 = 0.3141401708126068 + 0.001 * 7.261552810668945
Epoch 300, val loss: 0.7738420963287354
Epoch 310, training loss: 0.2968495786190033 = 0.28959283232688904 + 0.001 * 7.25674295425415
Epoch 310, val loss: 0.7798699736595154
Epoch 320, training loss: 0.2735765874385834 = 0.26632481813430786 + 0.001 * 7.251765251159668
Epoch 320, val loss: 0.787892758846283
Epoch 330, training loss: 0.2506486177444458 = 0.2433660328388214 + 0.001 * 7.282598495483398
Epoch 330, val loss: 0.7969945669174194
Epoch 340, training loss: 0.22706463932991028 = 0.21981602907180786 + 0.001 * 7.248606204986572
Epoch 340, val loss: 0.8063850402832031
Epoch 350, training loss: 0.20281629264354706 = 0.1955714374780655 + 0.001 * 7.244853973388672
Epoch 350, val loss: 0.8156259655952454
Epoch 360, training loss: 0.17886757850646973 = 0.17163154482841492 + 0.001 * 7.236036777496338
Epoch 360, val loss: 0.8250807523727417
Epoch 370, training loss: 0.15664903819561005 = 0.1494167447090149 + 0.001 * 7.232290267944336
Epoch 370, val loss: 0.8356854319572449
Epoch 380, training loss: 0.13709495961666107 = 0.12984733283519745 + 0.001 * 7.247631072998047
Epoch 380, val loss: 0.8482773303985596
Epoch 390, training loss: 0.12033689767122269 = 0.11311031132936478 + 0.001 * 7.2265825271606445
Epoch 390, val loss: 0.8631080985069275
Epoch 400, training loss: 0.10617673397064209 = 0.09895212203264236 + 0.001 * 7.224613666534424
Epoch 400, val loss: 0.879833996295929
Epoch 410, training loss: 0.0942058190703392 = 0.08698175847530365 + 0.001 * 7.224062442779541
Epoch 410, val loss: 0.8980002999305725
Epoch 420, training loss: 0.0840451642870903 = 0.0768146961927414 + 0.001 * 7.230464935302734
Epoch 420, val loss: 0.917083203792572
Epoch 430, training loss: 0.07535102218389511 = 0.06813058257102966 + 0.001 * 7.220437526702881
Epoch 430, val loss: 0.9366344809532166
Epoch 440, training loss: 0.06789281219244003 = 0.0606742687523365 + 0.001 * 7.218541622161865
Epoch 440, val loss: 0.9563033580780029
Epoch 450, training loss: 0.06146322935819626 = 0.05424410104751587 + 0.001 * 7.2191290855407715
Epoch 450, val loss: 0.9757860898971558
Epoch 460, training loss: 0.055896468460559845 = 0.04867781326174736 + 0.001 * 7.218653202056885
Epoch 460, val loss: 0.9949119091033936
Epoch 470, training loss: 0.05105729401111603 = 0.04384295269846916 + 0.001 * 7.21434211730957
Epoch 470, val loss: 1.0135760307312012
Epoch 480, training loss: 0.046840257942676544 = 0.03962917998433113 + 0.001 * 7.2110772132873535
Epoch 480, val loss: 1.0317009687423706
Epoch 490, training loss: 0.043160390108823776 = 0.03594546020030975 + 0.001 * 7.214928150177002
Epoch 490, val loss: 1.0492722988128662
Epoch 500, training loss: 0.03992544114589691 = 0.032714925706386566 + 0.001 * 7.210513114929199
Epoch 500, val loss: 1.0662591457366943
Epoch 510, training loss: 0.037080928683280945 = 0.029872743412852287 + 0.001 * 7.208183765411377
Epoch 510, val loss: 1.0826925039291382
Epoch 520, training loss: 0.034576721489429474 = 0.02736445888876915 + 0.001 * 7.21226167678833
Epoch 520, val loss: 1.09859037399292
Epoch 530, training loss: 0.03234853968024254 = 0.025144115090370178 + 0.001 * 7.204425811767578
Epoch 530, val loss: 1.1139315366744995
Epoch 540, training loss: 0.030373847112059593 = 0.023172441869974136 + 0.001 * 7.201404571533203
Epoch 540, val loss: 1.1287440061569214
Epoch 550, training loss: 0.028621483594179153 = 0.021416254341602325 + 0.001 * 7.20522928237915
Epoch 550, val loss: 1.143034815788269
Epoch 560, training loss: 0.027045663446187973 = 0.019847359508275986 + 0.001 * 7.198304176330566
Epoch 560, val loss: 1.1568444967269897
Epoch 570, training loss: 0.025640562176704407 = 0.01844138838350773 + 0.001 * 7.199173927307129
Epoch 570, val loss: 1.1701889038085938
Epoch 580, training loss: 0.024376047775149345 = 0.01717778667807579 + 0.001 * 7.19826078414917
Epoch 580, val loss: 1.1831077337265015
Epoch 590, training loss: 0.023239832371473312 = 0.016038991510868073 + 0.001 * 7.200840473175049
Epoch 590, val loss: 1.1956137418746948
Epoch 600, training loss: 0.022201858460903168 = 0.015009832568466663 + 0.001 * 7.192026138305664
Epoch 600, val loss: 1.2076916694641113
Epoch 610, training loss: 0.021276473999023438 = 0.014077099040150642 + 0.001 * 7.199375629425049
Epoch 610, val loss: 1.2193787097930908
Epoch 620, training loss: 0.020422914996743202 = 0.013229621574282646 + 0.001 * 7.193293571472168
Epoch 620, val loss: 1.230707049369812
Epoch 630, training loss: 0.019645411521196365 = 0.012457616627216339 + 0.001 * 7.187795639038086
Epoch 630, val loss: 1.241675615310669
Epoch 640, training loss: 0.01894528791308403 = 0.011752691119909286 + 0.001 * 7.192596912384033
Epoch 640, val loss: 1.252318263053894
Epoch 650, training loss: 0.01829277165234089 = 0.011107435449957848 + 0.001 * 7.185336112976074
Epoch 650, val loss: 1.2626303434371948
Epoch 660, training loss: 0.017702115699648857 = 0.010515440255403519 + 0.001 * 7.18667459487915
Epoch 660, val loss: 1.272640585899353
Epoch 670, training loss: 0.017161903902888298 = 0.00997118093073368 + 0.001 * 7.190722465515137
Epoch 670, val loss: 1.2823357582092285
Epoch 680, training loss: 0.016648180782794952 = 0.009469753131270409 + 0.001 * 7.178426742553711
Epoch 680, val loss: 1.291763424873352
Epoch 690, training loss: 0.01619097590446472 = 0.009006906300783157 + 0.001 * 7.184069633483887
Epoch 690, val loss: 1.300923228263855
Epoch 700, training loss: 0.015754682943224907 = 0.008578818291425705 + 0.001 * 7.1758646965026855
Epoch 700, val loss: 1.3098106384277344
Epoch 710, training loss: 0.015360502526164055 = 0.008182089775800705 + 0.001 * 7.178412914276123
Epoch 710, val loss: 1.3184545040130615
Epoch 720, training loss: 0.014987243339419365 = 0.007813818752765656 + 0.001 * 7.173424243927002
Epoch 720, val loss: 1.326857328414917
Epoch 730, training loss: 0.014641214162111282 = 0.007471390534192324 + 0.001 * 7.16982364654541
Epoch 730, val loss: 1.3350179195404053
Epoch 740, training loss: 0.014321397989988327 = 0.007152453530579805 + 0.001 * 7.168943881988525
Epoch 740, val loss: 1.3429532051086426
Epoch 750, training loss: 0.014031540602445602 = 0.006854938343167305 + 0.001 * 7.176601886749268
Epoch 750, val loss: 1.350676417350769
Epoch 760, training loss: 0.013751132413744926 = 0.006576996296644211 + 0.001 * 7.174135208129883
Epoch 760, val loss: 1.3581929206848145
Epoch 770, training loss: 0.013491785153746605 = 0.006316929589956999 + 0.001 * 7.174855709075928
Epoch 770, val loss: 1.3655130863189697
Epoch 780, training loss: 0.01323414221405983 = 0.006073253694921732 + 0.001 * 7.160888671875
Epoch 780, val loss: 1.3726415634155273
Epoch 790, training loss: 0.013032926246523857 = 0.0058446298353374004 + 0.001 * 7.188296318054199
Epoch 790, val loss: 1.379583716392517
Epoch 800, training loss: 0.012785803526639938 = 0.00562983937561512 + 0.001 * 7.155964374542236
Epoch 800, val loss: 1.386351466178894
Epoch 810, training loss: 0.012591730803251266 = 0.005427793599665165 + 0.001 * 7.163937091827393
Epoch 810, val loss: 1.3929535150527954
Epoch 820, training loss: 0.012387885712087154 = 0.00523749552667141 + 0.001 * 7.150389671325684
Epoch 820, val loss: 1.3993901014328003
Epoch 830, training loss: 0.01222018152475357 = 0.00505808973684907 + 0.001 * 7.1620917320251465
Epoch 830, val loss: 1.4056686162948608
Epoch 840, training loss: 0.012033890932798386 = 0.004888735245913267 + 0.001 * 7.145155906677246
Epoch 840, val loss: 1.4117980003356934
Epoch 850, training loss: 0.011871565133333206 = 0.0047287060879170895 + 0.001 * 7.142858028411865
Epoch 850, val loss: 1.4177826642990112
Epoch 860, training loss: 0.011722348630428314 = 0.0045773121528327465 + 0.001 * 7.145035743713379
Epoch 860, val loss: 1.4236302375793457
Epoch 870, training loss: 0.011598195880651474 = 0.0044339862652122974 + 0.001 * 7.164208889007568
Epoch 870, val loss: 1.4293301105499268
Epoch 880, training loss: 0.011465316638350487 = 0.004298119805753231 + 0.001 * 7.167196750640869
Epoch 880, val loss: 1.4348992109298706
Epoch 890, training loss: 0.011308202520012856 = 0.00416922802105546 + 0.001 * 7.138974666595459
Epoch 890, val loss: 1.440337896347046
Epoch 900, training loss: 0.01122111827135086 = 0.0040468075312674046 + 0.001 * 7.174309730529785
Epoch 900, val loss: 1.4456486701965332
Epoch 910, training loss: 0.011054961942136288 = 0.003930394537746906 + 0.001 * 7.124567031860352
Epoch 910, val loss: 1.4508525133132935
Epoch 920, training loss: 0.010979663580656052 = 0.0038195541128516197 + 0.001 * 7.16010856628418
Epoch 920, val loss: 1.4559400081634521
Epoch 930, training loss: 0.010844684205949306 = 0.0037138196639716625 + 0.001 * 7.130864143371582
Epoch 930, val loss: 1.4609150886535645
Epoch 940, training loss: 0.010751528665423393 = 0.0036125960759818554 + 0.001 * 7.138932704925537
Epoch 940, val loss: 1.4658128023147583
Epoch 950, training loss: 0.01063193753361702 = 0.0035152940545231104 + 0.001 * 7.116642951965332
Epoch 950, val loss: 1.4706319570541382
Epoch 960, training loss: 0.010535609908401966 = 0.0034214064944535494 + 0.001 * 7.114202976226807
Epoch 960, val loss: 1.4753907918930054
Epoch 970, training loss: 0.010444056242704391 = 0.003330569714307785 + 0.001 * 7.113485813140869
Epoch 970, val loss: 1.4800941944122314
Epoch 980, training loss: 0.010390750132501125 = 0.0032425944227725267 + 0.001 * 7.148155212402344
Epoch 980, val loss: 1.4847488403320312
Epoch 990, training loss: 0.010291128419339657 = 0.003157423110678792 + 0.001 * 7.133705139160156
Epoch 990, val loss: 1.4893532991409302
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 1.9413398504257202 = 1.9327430725097656 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.929142951965332
Epoch 10, training loss: 1.9321595430374146 = 1.92356276512146 + 0.001 * 8.596763610839844
Epoch 10, val loss: 1.9196580648422241
Epoch 20, training loss: 1.921116828918457 = 1.9125202894210815 + 0.001 * 8.59653091430664
Epoch 20, val loss: 1.908361554145813
Epoch 30, training loss: 1.906073808670044 = 1.8974778652191162 + 0.001 * 8.59598159790039
Epoch 30, val loss: 1.8931829929351807
Epoch 40, training loss: 1.884192705154419 = 1.8755981922149658 + 0.001 * 8.594532012939453
Epoch 40, val loss: 1.8715623617172241
Epoch 50, training loss: 1.8530796766281128 = 1.8444898128509521 + 0.001 * 8.589815139770508
Epoch 50, val loss: 1.8421536684036255
Epoch 60, training loss: 1.8153141736984253 = 1.80674409866333 + 0.001 * 8.570112228393555
Epoch 60, val loss: 1.8097238540649414
Epoch 70, training loss: 1.7796026468276978 = 1.7711251974105835 + 0.001 * 8.47745418548584
Epoch 70, val loss: 1.7816556692123413
Epoch 80, training loss: 1.7395621538162231 = 1.7313302755355835 + 0.001 * 8.231822967529297
Epoch 80, val loss: 1.7458676099777222
Epoch 90, training loss: 1.683189868927002 = 1.67521333694458 + 0.001 * 7.976480007171631
Epoch 90, val loss: 1.694229006767273
Epoch 100, training loss: 1.6071200370788574 = 1.599497675895691 + 0.001 * 7.6224164962768555
Epoch 100, val loss: 1.628106951713562
Epoch 110, training loss: 1.5111759901046753 = 1.5037058591842651 + 0.001 * 7.470166206359863
Epoch 110, val loss: 1.5484216213226318
Epoch 120, training loss: 1.4034435749053955 = 1.3960652351379395 + 0.001 * 7.37830114364624
Epoch 120, val loss: 1.4576481580734253
Epoch 130, training loss: 1.2929115295410156 = 1.2855587005615234 + 0.001 * 7.352782726287842
Epoch 130, val loss: 1.3647212982177734
Epoch 140, training loss: 1.1862187385559082 = 1.1788781881332397 + 0.001 * 7.340596675872803
Epoch 140, val loss: 1.2745685577392578
Epoch 150, training loss: 1.086162805557251 = 1.0788344144821167 + 0.001 * 7.328364372253418
Epoch 150, val loss: 1.1893775463104248
Epoch 160, training loss: 0.9920656681060791 = 0.98475182056427 + 0.001 * 7.313858509063721
Epoch 160, val loss: 1.1084930896759033
Epoch 170, training loss: 0.9024884104728699 = 0.8951870203018188 + 0.001 * 7.301413059234619
Epoch 170, val loss: 1.0315831899642944
Epoch 180, training loss: 0.8169963955879211 = 0.8097032904624939 + 0.001 * 7.2930989265441895
Epoch 180, val loss: 0.9593806266784668
Epoch 190, training loss: 0.7363879084587097 = 0.7291001081466675 + 0.001 * 7.287784099578857
Epoch 190, val loss: 0.8931105136871338
Epoch 200, training loss: 0.6618041396141052 = 0.6545204520225525 + 0.001 * 7.283672332763672
Epoch 200, val loss: 0.8341001272201538
Epoch 210, training loss: 0.5932379961013794 = 0.5859590768814087 + 0.001 * 7.278942108154297
Epoch 210, val loss: 0.7826934456825256
Epoch 220, training loss: 0.5295256972312927 = 0.5222527384757996 + 0.001 * 7.272942543029785
Epoch 220, val loss: 0.7379730939865112
Epoch 230, training loss: 0.4693978726863861 = 0.46213215589523315 + 0.001 * 7.265719890594482
Epoch 230, val loss: 0.6988570094108582
Epoch 240, training loss: 0.41253915429115295 = 0.40528178215026855 + 0.001 * 7.257379531860352
Epoch 240, val loss: 0.6654210686683655
Epoch 250, training loss: 0.35947203636169434 = 0.3522244989871979 + 0.001 * 7.247539520263672
Epoch 250, val loss: 0.6375659108161926
Epoch 260, training loss: 0.31098610162734985 = 0.303749680519104 + 0.001 * 7.236420154571533
Epoch 260, val loss: 0.615290105342865
Epoch 270, training loss: 0.2676948010921478 = 0.26046624779701233 + 0.001 * 7.228548526763916
Epoch 270, val loss: 0.5986595749855042
Epoch 280, training loss: 0.22982579469680786 = 0.22261565923690796 + 0.001 * 7.210129737854004
Epoch 280, val loss: 0.5872690081596375
Epoch 290, training loss: 0.1972922831773758 = 0.19008447229862213 + 0.001 * 7.207815170288086
Epoch 290, val loss: 0.5806170701980591
Epoch 300, training loss: 0.16968487203121185 = 0.16249476373195648 + 0.001 * 7.19010591506958
Epoch 300, val loss: 0.5780091285705566
Epoch 310, training loss: 0.14644408226013184 = 0.13926367461681366 + 0.001 * 7.180403709411621
Epoch 310, val loss: 0.5787292122840881
Epoch 320, training loss: 0.12695173919200897 = 0.1197730153799057 + 0.001 * 7.178725242614746
Epoch 320, val loss: 0.5821305513381958
Epoch 330, training loss: 0.11061226576566696 = 0.10344287008047104 + 0.001 * 7.169394016265869
Epoch 330, val loss: 0.5876906514167786
Epoch 340, training loss: 0.09692643582820892 = 0.0897589698433876 + 0.001 * 7.167463779449463
Epoch 340, val loss: 0.5949656367301941
Epoch 350, training loss: 0.08543513715267181 = 0.07827502489089966 + 0.001 * 7.160111427307129
Epoch 350, val loss: 0.6035519242286682
Epoch 360, training loss: 0.07576317340135574 = 0.0686078667640686 + 0.001 * 7.155303478240967
Epoch 360, val loss: 0.6131165623664856
Epoch 370, training loss: 0.06759744882583618 = 0.0604407824575901 + 0.001 * 7.15666389465332
Epoch 370, val loss: 0.6233570575714111
Epoch 380, training loss: 0.06065353751182556 = 0.053510673344135284 + 0.001 * 7.142863750457764
Epoch 380, val loss: 0.6341035962104797
Epoch 390, training loss: 0.05475873872637749 = 0.04760346561670303 + 0.001 * 7.155272006988525
Epoch 390, val loss: 0.6451019048690796
Epoch 400, training loss: 0.04968181625008583 = 0.04254494607448578 + 0.001 * 7.136871337890625
Epoch 400, val loss: 0.656250536441803
Epoch 410, training loss: 0.04531031474471092 = 0.03819343075156212 + 0.001 * 7.116884231567383
Epoch 410, val loss: 0.6673647165298462
Epoch 420, training loss: 0.04159132391214371 = 0.034432604908943176 + 0.001 * 7.158716678619385
Epoch 420, val loss: 0.6784257292747498
Epoch 430, training loss: 0.03828654810786247 = 0.03116842731833458 + 0.001 * 7.1181206703186035
Epoch 430, val loss: 0.6893472671508789
Epoch 440, training loss: 0.03543105348944664 = 0.02832312509417534 + 0.001 * 7.10792875289917
Epoch 440, val loss: 0.7001010179519653
Epoch 450, training loss: 0.03293570131063461 = 0.02583278901875019 + 0.001 * 7.10291051864624
Epoch 450, val loss: 0.7106214761734009
Epoch 460, training loss: 0.030745968222618103 = 0.023644620552659035 + 0.001 * 7.101347923278809
Epoch 460, val loss: 0.720933198928833
Epoch 470, training loss: 0.028885742649435997 = 0.021714728325605392 + 0.001 * 7.171013832092285
Epoch 470, val loss: 0.7309933304786682
Epoch 480, training loss: 0.027092162519693375 = 0.02000645361840725 + 0.001 * 7.085708141326904
Epoch 480, val loss: 0.7407823204994202
Epoch 490, training loss: 0.025560207664966583 = 0.018488937988877296 + 0.001 * 7.071269989013672
Epoch 490, val loss: 0.7503185272216797
Epoch 500, training loss: 0.024200234562158585 = 0.017134997993707657 + 0.001 * 7.0652360916137695
Epoch 500, val loss: 0.759647786617279
Epoch 510, training loss: 0.022982146590948105 = 0.015919242054224014 + 0.001 * 7.062903881072998
Epoch 510, val loss: 0.7687786817550659
Epoch 520, training loss: 0.021875305101275444 = 0.014820042066276073 + 0.001 * 7.055263042449951
Epoch 520, val loss: 0.7778151631355286
Epoch 530, training loss: 0.020924631506204605 = 0.013821112923324108 + 0.001 * 7.103518486022949
Epoch 530, val loss: 0.7867547273635864
Epoch 540, training loss: 0.019963588565587997 = 0.012911696918308735 + 0.001 * 7.051891803741455
Epoch 540, val loss: 0.7955837845802307
Epoch 550, training loss: 0.019135136157274246 = 0.012082930654287338 + 0.001 * 7.052204608917236
Epoch 550, val loss: 0.8042967319488525
Epoch 560, training loss: 0.01840771548449993 = 0.011327373795211315 + 0.001 * 7.080341815948486
Epoch 560, val loss: 0.8128658533096313
Epoch 570, training loss: 0.017694098874926567 = 0.010637949220836163 + 0.001 * 7.056149959564209
Epoch 570, val loss: 0.8212615847587585
Epoch 580, training loss: 0.017048288136720657 = 0.01000835932791233 + 0.001 * 7.039927959442139
Epoch 580, val loss: 0.8294855356216431
Epoch 590, training loss: 0.016467174515128136 = 0.009432603605091572 + 0.001 * 7.034570693969727
Epoch 590, val loss: 0.8375188112258911
Epoch 600, training loss: 0.015947682783007622 = 0.008905302733182907 + 0.001 * 7.042379856109619
Epoch 600, val loss: 0.8453698754310608
Epoch 610, training loss: 0.015460716560482979 = 0.008421600796282291 + 0.001 * 7.039115905761719
Epoch 610, val loss: 0.853006899356842
Epoch 620, training loss: 0.015022184699773788 = 0.007977099157869816 + 0.001 * 7.045085430145264
Epoch 620, val loss: 0.8604665398597717
Epoch 630, training loss: 0.014586448669433594 = 0.007567946799099445 + 0.001 * 7.018501281738281
Epoch 630, val loss: 0.8677439093589783
Epoch 640, training loss: 0.014227118343114853 = 0.007190699689090252 + 0.001 * 7.036418914794922
Epoch 640, val loss: 0.874841034412384
Epoch 650, training loss: 0.013888838700950146 = 0.0068421983160078526 + 0.001 * 7.046639919281006
Epoch 650, val loss: 0.8817639946937561
Epoch 660, training loss: 0.013551738113164902 = 0.0065197208896279335 + 0.001 * 7.032017230987549
Epoch 660, val loss: 0.8885223269462585
Epoch 670, training loss: 0.01325475238263607 = 0.006220776122063398 + 0.001 * 7.033976078033447
Epoch 670, val loss: 0.8951159715652466
Epoch 680, training loss: 0.012981154024600983 = 0.00594313582405448 + 0.001 * 7.038018226623535
Epoch 680, val loss: 0.9015589356422424
Epoch 690, training loss: 0.012690701521933079 = 0.005684965290129185 + 0.001 * 7.005735874176025
Epoch 690, val loss: 0.9078315496444702
Epoch 700, training loss: 0.012453360483050346 = 0.005444498732686043 + 0.001 * 7.008861064910889
Epoch 700, val loss: 0.9139670133590698
Epoch 710, training loss: 0.012249374762177467 = 0.0052201272919774055 + 0.001 * 7.029247760772705
Epoch 710, val loss: 0.9199453592300415
Epoch 720, training loss: 0.012038248591125011 = 0.005010505672544241 + 0.001 * 7.027742385864258
Epoch 720, val loss: 0.9257990121841431
Epoch 730, training loss: 0.011817995458841324 = 0.004814347252249718 + 0.001 * 7.003647327423096
Epoch 730, val loss: 0.9315021634101868
Epoch 740, training loss: 0.011624155566096306 = 0.0046305530704557896 + 0.001 * 6.993602275848389
Epoch 740, val loss: 0.9370771646499634
Epoch 750, training loss: 0.011503288522362709 = 0.004457993432879448 + 0.001 * 7.045294761657715
Epoch 750, val loss: 0.942523181438446
Epoch 760, training loss: 0.011308813467621803 = 0.004295647609978914 + 0.001 * 7.013165473937988
Epoch 760, val loss: 0.9478368759155273
Epoch 770, training loss: 0.011151734739542007 = 0.004142160061746836 + 0.001 * 7.009574890136719
Epoch 770, val loss: 0.9530457854270935
Epoch 780, training loss: 0.010994544252753258 = 0.003996321465820074 + 0.001 * 6.998222827911377
Epoch 780, val loss: 0.9581810832023621
Epoch 790, training loss: 0.010858004912734032 = 0.003856810973957181 + 0.001 * 7.001194000244141
Epoch 790, val loss: 0.9632571935653687
Epoch 800, training loss: 0.010739331133663654 = 0.0037229612935334444 + 0.001 * 7.016369342803955
Epoch 800, val loss: 0.9683208465576172
Epoch 810, training loss: 0.010600024834275246 = 0.0035943298134952784 + 0.001 * 7.00569486618042
Epoch 810, val loss: 0.9733548760414124
Epoch 820, training loss: 0.010460129007697105 = 0.003470858559012413 + 0.001 * 6.9892706871032715
Epoch 820, val loss: 0.9783720970153809
Epoch 830, training loss: 0.010372234508395195 = 0.0033524520695209503 + 0.001 * 7.019781589508057
Epoch 830, val loss: 0.9833744764328003
Epoch 840, training loss: 0.010222353041172028 = 0.003239154815673828 + 0.001 * 6.9831976890563965
Epoch 840, val loss: 0.9883337020874023
Epoch 850, training loss: 0.010093905963003635 = 0.003130860859528184 + 0.001 * 6.9630446434021
Epoch 850, val loss: 0.9932170510292053
Epoch 860, training loss: 0.009998451918363571 = 0.003027549711987376 + 0.001 * 6.9709014892578125
Epoch 860, val loss: 0.9980983138084412
Epoch 870, training loss: 0.009914424270391464 = 0.0029291019309312105 + 0.001 * 6.985321998596191
Epoch 870, val loss: 1.0028389692306519
Epoch 880, training loss: 0.009791380725800991 = 0.002835304709151387 + 0.001 * 6.956076145172119
Epoch 880, val loss: 1.0075907707214355
Epoch 890, training loss: 0.009757108055055141 = 0.0027459629345685244 + 0.001 * 7.011145114898682
Epoch 890, val loss: 1.0122556686401367
Epoch 900, training loss: 0.009643195196986198 = 0.00266094459220767 + 0.001 * 6.982250213623047
Epoch 900, val loss: 1.0167980194091797
Epoch 910, training loss: 0.00958196446299553 = 0.00257995817810297 + 0.001 * 7.002006530761719
Epoch 910, val loss: 1.0212808847427368
Epoch 920, training loss: 0.009467152878642082 = 0.0025029678363353014 + 0.001 * 6.964184284210205
Epoch 920, val loss: 1.0257058143615723
Epoch 930, training loss: 0.009431777521967888 = 0.002429601503536105 + 0.001 * 7.002175331115723
Epoch 930, val loss: 1.0300453901290894
Epoch 940, training loss: 0.00932249240577221 = 0.002359790960326791 + 0.001 * 6.962701320648193
Epoch 940, val loss: 1.0342891216278076
Epoch 950, training loss: 0.0092523954808712 = 0.002293260069563985 + 0.001 * 6.95913553237915
Epoch 950, val loss: 1.0384814739227295
Epoch 960, training loss: 0.009190176613628864 = 0.002229870529845357 + 0.001 * 6.960306167602539
Epoch 960, val loss: 1.0425931215286255
Epoch 970, training loss: 0.00915437564253807 = 0.0021694751922041178 + 0.001 * 6.984899997711182
Epoch 970, val loss: 1.0466070175170898
Epoch 980, training loss: 0.009080932475626469 = 0.0021118924487382174 + 0.001 * 6.9690399169921875
Epoch 980, val loss: 1.0505496263504028
Epoch 990, training loss: 0.009020550176501274 = 0.002056894823908806 + 0.001 * 6.9636549949646
Epoch 990, val loss: 1.0544538497924805
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 1.9381061792373657 = 1.9295092821121216 + 0.001 * 8.596879959106445
Epoch 0, val loss: 1.9275166988372803
Epoch 10, training loss: 1.9288703203201294 = 1.9202734231948853 + 0.001 * 8.59685230255127
Epoch 10, val loss: 1.917984127998352
Epoch 20, training loss: 1.917819619178772 = 1.9092228412628174 + 0.001 * 8.596736907958984
Epoch 20, val loss: 1.9065423011779785
Epoch 30, training loss: 1.9027926921844482 = 1.8941961526870728 + 0.001 * 8.596485137939453
Epoch 30, val loss: 1.8911173343658447
Epoch 40, training loss: 1.8812036514282227 = 1.872607707977295 + 0.001 * 8.595906257629395
Epoch 40, val loss: 1.8693965673446655
Epoch 50, training loss: 1.8510246276855469 = 1.8424303531646729 + 0.001 * 8.594321250915527
Epoch 50, val loss: 1.8403061628341675
Epoch 60, training loss: 1.8135358095169067 = 1.8049468994140625 + 0.001 * 8.588921546936035
Epoch 60, val loss: 1.8067023754119873
Epoch 70, training loss: 1.7726560831069946 = 1.764091968536377 + 0.001 * 8.564107894897461
Epoch 70, val loss: 1.7724735736846924
Epoch 80, training loss: 1.7222548723220825 = 1.7138590812683105 + 0.001 * 8.395736694335938
Epoch 80, val loss: 1.729169249534607
Epoch 90, training loss: 1.6515134572982788 = 1.6434544324874878 + 0.001 * 8.058991432189941
Epoch 90, val loss: 1.6675552129745483
Epoch 100, training loss: 1.5575827360153198 = 1.5497058629989624 + 0.001 * 7.876920700073242
Epoch 100, val loss: 1.588025450706482
Epoch 110, training loss: 1.4450212717056274 = 1.4373410940170288 + 0.001 * 7.680150508880615
Epoch 110, val loss: 1.4956732988357544
Epoch 120, training loss: 1.3244869709014893 = 1.3168857097625732 + 0.001 * 7.601292610168457
Epoch 120, val loss: 1.3980876207351685
Epoch 130, training loss: 1.205365777015686 = 1.1978096961975098 + 0.001 * 7.55608606338501
Epoch 130, val loss: 1.3048871755599976
Epoch 140, training loss: 1.0949152708053589 = 1.0874240398406982 + 0.001 * 7.491246700286865
Epoch 140, val loss: 1.2217460870742798
Epoch 150, training loss: 0.9970337748527527 = 0.9895978569984436 + 0.001 * 7.435923099517822
Epoch 150, val loss: 1.1510637998580933
Epoch 160, training loss: 0.9108524918556213 = 0.9034337997436523 + 0.001 * 7.418708801269531
Epoch 160, val loss: 1.090760588645935
Epoch 170, training loss: 0.8333557844161987 = 0.8259404897689819 + 0.001 * 7.415277004241943
Epoch 170, val loss: 1.0375800132751465
Epoch 180, training loss: 0.7627441883087158 = 0.7553350329399109 + 0.001 * 7.409130573272705
Epoch 180, val loss: 0.9900606274604797
Epoch 190, training loss: 0.6984517574310303 = 0.6910481452941895 + 0.001 * 7.403619289398193
Epoch 190, val loss: 0.9484596252441406
Epoch 200, training loss: 0.6397711038589478 = 0.6323738694190979 + 0.001 * 7.397244453430176
Epoch 200, val loss: 0.9130634665489197
Epoch 210, training loss: 0.5853716135025024 = 0.5779818296432495 + 0.001 * 7.389798164367676
Epoch 210, val loss: 0.8831247091293335
Epoch 220, training loss: 0.5337799787521362 = 0.5263991355895996 + 0.001 * 7.380853652954102
Epoch 220, val loss: 0.8572477102279663
Epoch 230, training loss: 0.48378580808639526 = 0.4764159619808197 + 0.001 * 7.369842052459717
Epoch 230, val loss: 0.8339158892631531
Epoch 240, training loss: 0.4347374439239502 = 0.4273795187473297 + 0.001 * 7.357937335968018
Epoch 240, val loss: 0.8124558329582214
Epoch 250, training loss: 0.3867572844028473 = 0.3794127106666565 + 0.001 * 7.344574451446533
Epoch 250, val loss: 0.793242335319519
Epoch 260, training loss: 0.3405982553958893 = 0.33327335119247437 + 0.001 * 7.32491397857666
Epoch 260, val loss: 0.7773032784461975
Epoch 270, training loss: 0.29754483699798584 = 0.29024139046669006 + 0.001 * 7.303442478179932
Epoch 270, val loss: 0.76544588804245
Epoch 280, training loss: 0.2587706744670868 = 0.25148114562034607 + 0.001 * 7.289536952972412
Epoch 280, val loss: 0.7580351233482361
Epoch 290, training loss: 0.22488699853420258 = 0.21761766076087952 + 0.001 * 7.269331932067871
Epoch 290, val loss: 0.7549270391464233
Epoch 300, training loss: 0.19582973420619965 = 0.1885722577571869 + 0.001 * 7.257469177246094
Epoch 300, val loss: 0.7556027770042419
Epoch 310, training loss: 0.1710844486951828 = 0.1638309806585312 + 0.001 * 7.253473281860352
Epoch 310, val loss: 0.7594369053840637
Epoch 320, training loss: 0.1500035673379898 = 0.14275741577148438 + 0.001 * 7.246157646179199
Epoch 320, val loss: 0.7658326029777527
Epoch 330, training loss: 0.13200770318508148 = 0.12476279586553574 + 0.001 * 7.244906425476074
Epoch 330, val loss: 0.7742853164672852
Epoch 340, training loss: 0.11660396307706833 = 0.1093599945306778 + 0.001 * 7.243967533111572
Epoch 340, val loss: 0.7843899130821228
Epoch 350, training loss: 0.10339269787073135 = 0.0961470678448677 + 0.001 * 7.24562931060791
Epoch 350, val loss: 0.7958760857582092
Epoch 360, training loss: 0.0920395702123642 = 0.0847964733839035 + 0.001 * 7.243095397949219
Epoch 360, val loss: 0.8084961771965027
Epoch 370, training loss: 0.0822812169790268 = 0.07503635436296463 + 0.001 * 7.244858741760254
Epoch 370, val loss: 0.8220103979110718
Epoch 380, training loss: 0.07387425005435944 = 0.0666339173913002 + 0.001 * 7.240328788757324
Epoch 380, val loss: 0.8361828923225403
Epoch 390, training loss: 0.06662841886281967 = 0.05938993766903877 + 0.001 * 7.238481521606445
Epoch 390, val loss: 0.8507992029190063
Epoch 400, training loss: 0.06037309393286705 = 0.05313621833920479 + 0.001 * 7.236876010894775
Epoch 400, val loss: 0.8656912446022034
Epoch 410, training loss: 0.05496075004339218 = 0.04772564023733139 + 0.001 * 7.235107421875
Epoch 410, val loss: 0.8807277679443359
Epoch 420, training loss: 0.05026126652956009 = 0.04303298518061638 + 0.001 * 7.228280544281006
Epoch 420, val loss: 0.8957874774932861
Epoch 430, training loss: 0.04618052393198013 = 0.03895242139697075 + 0.001 * 7.228103160858154
Epoch 430, val loss: 0.9107151031494141
Epoch 440, training loss: 0.04261632636189461 = 0.03539315611124039 + 0.001 * 7.22316837310791
Epoch 440, val loss: 0.9254657030105591
Epoch 450, training loss: 0.03949932008981705 = 0.03227762132883072 + 0.001 * 7.2216997146606445
Epoch 450, val loss: 0.9399372935295105
Epoch 460, training loss: 0.036782894283533096 = 0.029540518298745155 + 0.001 * 7.242376327514648
Epoch 460, val loss: 0.9541301727294922
Epoch 470, training loss: 0.034343916922807693 = 0.02712702378630638 + 0.001 * 7.216893196105957
Epoch 470, val loss: 0.9679579138755798
Epoch 480, training loss: 0.03220360726118088 = 0.024990960955619812 + 0.001 * 7.212647914886475
Epoch 480, val loss: 0.9814462065696716
Epoch 490, training loss: 0.03030567429959774 = 0.02309328131377697 + 0.001 * 7.212392807006836
Epoch 490, val loss: 0.9945528507232666
Epoch 500, training loss: 0.02861391194164753 = 0.02140100486576557 + 0.001 * 7.212906360626221
Epoch 500, val loss: 1.0073121786117554
Epoch 510, training loss: 0.027092264965176582 = 0.01988658495247364 + 0.001 * 7.205679416656494
Epoch 510, val loss: 1.0197205543518066
Epoch 520, training loss: 0.025731706991791725 = 0.01852668635547161 + 0.001 * 7.205020904541016
Epoch 520, val loss: 1.0317566394805908
Epoch 530, training loss: 0.024504702538251877 = 0.017301583662629128 + 0.001 * 7.203119277954102
Epoch 530, val loss: 1.0434820652008057
Epoch 540, training loss: 0.02340482734143734 = 0.016194457188248634 + 0.001 * 7.21036958694458
Epoch 540, val loss: 1.054848551750183
Epoch 550, training loss: 0.022387392818927765 = 0.015190912410616875 + 0.001 * 7.196479320526123
Epoch 550, val loss: 1.0658986568450928
Epoch 560, training loss: 0.021471921354532242 = 0.014278645627200603 + 0.001 * 7.193275451660156
Epoch 560, val loss: 1.0766348838806152
Epoch 570, training loss: 0.020659180358052254 = 0.013447093777358532 + 0.001 * 7.212085723876953
Epoch 570, val loss: 1.087083101272583
Epoch 580, training loss: 0.01987539604306221 = 0.012687289156019688 + 0.001 * 7.188105583190918
Epoch 580, val loss: 1.0972347259521484
Epoch 590, training loss: 0.019176464527845383 = 0.011991317383944988 + 0.001 * 7.185147285461426
Epoch 590, val loss: 1.1070986986160278
Epoch 600, training loss: 0.018540553748607635 = 0.011352323926985264 + 0.001 * 7.188229560852051
Epoch 600, val loss: 1.1167161464691162
Epoch 610, training loss: 0.01794855110347271 = 0.0107644097879529 + 0.001 * 7.184141635894775
Epoch 610, val loss: 1.1260782480239868
Epoch 620, training loss: 0.017410004511475563 = 0.010222396813333035 + 0.001 * 7.1876068115234375
Epoch 620, val loss: 1.1352016925811768
Epoch 630, training loss: 0.016902528703212738 = 0.009721646085381508 + 0.001 * 7.180883407592773
Epoch 630, val loss: 1.1440643072128296
Epoch 640, training loss: 0.016448281705379486 = 0.009258171543478966 + 0.001 * 7.190110206604004
Epoch 640, val loss: 1.152713418006897
Epoch 650, training loss: 0.016006290912628174 = 0.008828436024487019 + 0.001 * 7.177854061126709
Epoch 650, val loss: 1.1611348390579224
Epoch 660, training loss: 0.015609859488904476 = 0.008429246954619884 + 0.001 * 7.180612087249756
Epoch 660, val loss: 1.169356346130371
Epoch 670, training loss: 0.015226172283291817 = 0.008057850413024426 + 0.001 * 7.168321132659912
Epoch 670, val loss: 1.1773720979690552
Epoch 680, training loss: 0.0149003267288208 = 0.0077117024920880795 + 0.001 * 7.188623428344727
Epoch 680, val loss: 1.185196876525879
Epoch 690, training loss: 0.014559721574187279 = 0.007388636935502291 + 0.001 * 7.171084880828857
Epoch 690, val loss: 1.1928120851516724
Epoch 700, training loss: 0.014258131384849548 = 0.007086628582328558 + 0.001 * 7.171502590179443
Epoch 700, val loss: 1.2002538442611694
Epoch 710, training loss: 0.013981547206640244 = 0.006803927477449179 + 0.001 * 7.177619457244873
Epoch 710, val loss: 1.2075128555297852
Epoch 720, training loss: 0.013695353642106056 = 0.0065389713272452354 + 0.001 * 7.1563825607299805
Epoch 720, val loss: 1.2146116495132446
Epoch 730, training loss: 0.013451097533106804 = 0.006290283519774675 + 0.001 * 7.160813808441162
Epoch 730, val loss: 1.2215347290039062
Epoch 740, training loss: 0.013216163963079453 = 0.006056585814803839 + 0.001 * 7.1595778465271
Epoch 740, val loss: 1.228301763534546
Epoch 750, training loss: 0.013008741661906242 = 0.005836725700646639 + 0.001 * 7.17201566696167
Epoch 750, val loss: 1.2349110841751099
Epoch 760, training loss: 0.012785842642188072 = 0.005629606079310179 + 0.001 * 7.156236171722412
Epoch 760, val loss: 1.241369605064392
Epoch 770, training loss: 0.012578812427818775 = 0.005434287711977959 + 0.001 * 7.144524574279785
Epoch 770, val loss: 1.2476879358291626
Epoch 780, training loss: 0.012401854619383812 = 0.005249904468655586 + 0.001 * 7.151949882507324
Epoch 780, val loss: 1.2538721561431885
Epoch 790, training loss: 0.012213153764605522 = 0.0050756367854774 + 0.001 * 7.137516498565674
Epoch 790, val loss: 1.2599170207977295
Epoch 800, training loss: 0.012042321264743805 = 0.004910781513899565 + 0.001 * 7.1315388679504395
Epoch 800, val loss: 1.265837550163269
Epoch 810, training loss: 0.011889778077602386 = 0.004754636902362108 + 0.001 * 7.135140419006348
Epoch 810, val loss: 1.2716271877288818
Epoch 820, training loss: 0.011743105947971344 = 0.0046065980568528175 + 0.001 * 7.136507511138916
Epoch 820, val loss: 1.2773001194000244
Epoch 830, training loss: 0.011593131348490715 = 0.004466067999601364 + 0.001 * 7.127062797546387
Epoch 830, val loss: 1.2828608751296997
Epoch 840, training loss: 0.011469213292002678 = 0.004332493990659714 + 0.001 * 7.136719226837158
Epoch 840, val loss: 1.28831148147583
Epoch 850, training loss: 0.011335328221321106 = 0.0042052846401929855 + 0.001 * 7.130042552947998
Epoch 850, val loss: 1.2936782836914062
Epoch 860, training loss: 0.011207183822989464 = 0.00408391235396266 + 0.001 * 7.123270511627197
Epoch 860, val loss: 1.2989697456359863
Epoch 870, training loss: 0.011084768921136856 = 0.003967783413827419 + 0.001 * 7.116985321044922
Epoch 870, val loss: 1.3042051792144775
Epoch 880, training loss: 0.010986432433128357 = 0.0038565206341445446 + 0.001 * 7.129911422729492
Epoch 880, val loss: 1.3093934059143066
Epoch 890, training loss: 0.010874178260564804 = 0.0037496068980544806 + 0.001 * 7.124571323394775
Epoch 890, val loss: 1.3145581483840942
Epoch 900, training loss: 0.010763026773929596 = 0.0036466883029788733 + 0.001 * 7.116337776184082
Epoch 900, val loss: 1.3197063207626343
Epoch 910, training loss: 0.010693300515413284 = 0.003547416999936104 + 0.001 * 7.145883560180664
Epoch 910, val loss: 1.3248629570007324
Epoch 920, training loss: 0.010576710104942322 = 0.003451782977208495 + 0.001 * 7.124927043914795
Epoch 920, val loss: 1.3299860954284668
Epoch 930, training loss: 0.010468699969351292 = 0.0033594241831451654 + 0.001 * 7.1092753410339355
Epoch 930, val loss: 1.3351333141326904
Epoch 940, training loss: 0.010379908606410027 = 0.0032702991738915443 + 0.001 * 7.1096086502075195
Epoch 940, val loss: 1.3402482271194458
Epoch 950, training loss: 0.010273046791553497 = 0.0031843269243836403 + 0.001 * 7.088719367980957
Epoch 950, val loss: 1.345359444618225
Epoch 960, training loss: 0.010232934728264809 = 0.003101336769759655 + 0.001 * 7.131597995758057
Epoch 960, val loss: 1.3504552841186523
Epoch 970, training loss: 0.010133566334843636 = 0.003021369455382228 + 0.001 * 7.112196445465088
Epoch 970, val loss: 1.3555246591567993
Epoch 980, training loss: 0.010063083842396736 = 0.0029441476799547672 + 0.001 * 7.118935585021973
Epoch 980, val loss: 1.360590934753418
Epoch 990, training loss: 0.009967194870114326 = 0.002869685646146536 + 0.001 * 7.097508907318115
Epoch 990, val loss: 1.3656386137008667
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8402741170268846
The final CL Acc:0.79877, 0.02444, The final GNN Acc:0.83711, 0.00228
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11580])
remove edge: torch.Size([2, 9448])
updated graph: torch.Size([2, 10472])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9597086906433105 = 1.951111912727356 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.9445197582244873
Epoch 10, training loss: 1.9486314058303833 = 1.9400346279144287 + 0.001 * 8.59677505493164
Epoch 10, val loss: 1.9337430000305176
Epoch 20, training loss: 1.9346508979797363 = 1.9260542392730713 + 0.001 * 8.596603393554688
Epoch 20, val loss: 1.919676661491394
Epoch 30, training loss: 1.9151605367660522 = 1.9065643548965454 + 0.001 * 8.596199989318848
Epoch 30, val loss: 1.8998357057571411
Epoch 40, training loss: 1.8873876333236694 = 1.8787925243377686 + 0.001 * 8.595162391662598
Epoch 40, val loss: 1.8720684051513672
Epoch 50, training loss: 1.8513392210006714 = 1.8427470922470093 + 0.001 * 8.5921630859375
Epoch 50, val loss: 1.8383840322494507
Epoch 60, training loss: 1.8158409595489502 = 1.8072589635849 + 0.001 * 8.581951141357422
Epoch 60, val loss: 1.8102190494537354
Epoch 70, training loss: 1.7850537300109863 = 1.7765175104141235 + 0.001 * 8.53620433807373
Epoch 70, val loss: 1.7880239486694336
Epoch 80, training loss: 1.7448593378067017 = 1.7366347312927246 + 0.001 * 8.224631309509277
Epoch 80, val loss: 1.7550904750823975
Epoch 90, training loss: 1.6887081861495972 = 1.6806458234786987 + 0.001 * 8.062406539916992
Epoch 90, val loss: 1.7074143886566162
Epoch 100, training loss: 1.613147258758545 = 1.6051380634307861 + 0.001 * 8.00919246673584
Epoch 100, val loss: 1.6446467638015747
Epoch 110, training loss: 1.5269731283187866 = 1.5189944505691528 + 0.001 * 7.978652477264404
Epoch 110, val loss: 1.5765595436096191
Epoch 120, training loss: 1.4411031007766724 = 1.4331879615783691 + 0.001 * 7.915175437927246
Epoch 120, val loss: 1.5132780075073242
Epoch 130, training loss: 1.3578518629074097 = 1.350067138671875 + 0.001 * 7.784667015075684
Epoch 130, val loss: 1.4557266235351562
Epoch 140, training loss: 1.2752684354782104 = 1.2675589323043823 + 0.001 * 7.709512233734131
Epoch 140, val loss: 1.4001847505569458
Epoch 150, training loss: 1.1933742761611938 = 1.185693621635437 + 0.001 * 7.680688381195068
Epoch 150, val loss: 1.3459290266036987
Epoch 160, training loss: 1.1148061752319336 = 1.1071785688400269 + 0.001 * 7.6276021003723145
Epoch 160, val loss: 1.29408860206604
Epoch 170, training loss: 1.042263150215149 = 1.034744381904602 + 0.001 * 7.518792629241943
Epoch 170, val loss: 1.247040033340454
Epoch 180, training loss: 0.9761866927146912 = 0.9687916040420532 + 0.001 * 7.395080089569092
Epoch 180, val loss: 1.2049826383590698
Epoch 190, training loss: 0.9148350358009338 = 0.9074493646621704 + 0.001 * 7.385641574859619
Epoch 190, val loss: 1.1664233207702637
Epoch 200, training loss: 0.8550872802734375 = 0.8477063775062561 + 0.001 * 7.3809051513671875
Epoch 200, val loss: 1.1289479732513428
Epoch 210, training loss: 0.7942366600036621 = 0.7868611812591553 + 0.001 * 7.375467300415039
Epoch 210, val loss: 1.0909810066223145
Epoch 220, training loss: 0.7313149571418762 = 0.723942756652832 + 0.001 * 7.372206687927246
Epoch 220, val loss: 1.0521528720855713
Epoch 230, training loss: 0.6674220561981201 = 0.6600547432899475 + 0.001 * 7.367307186126709
Epoch 230, val loss: 1.0146969556808472
Epoch 240, training loss: 0.604844331741333 = 0.5974818468093872 + 0.001 * 7.362465858459473
Epoch 240, val loss: 0.9822016954421997
Epoch 250, training loss: 0.5456967353820801 = 0.5383399128913879 + 0.001 * 7.356801509857178
Epoch 250, val loss: 0.9571718573570251
Epoch 260, training loss: 0.49150002002716064 = 0.4841488003730774 + 0.001 * 7.351220607757568
Epoch 260, val loss: 0.9405674934387207
Epoch 270, training loss: 0.4426315724849701 = 0.43528681993484497 + 0.001 * 7.34474515914917
Epoch 270, val loss: 0.9318047761917114
Epoch 280, training loss: 0.398657888174057 = 0.39132028818130493 + 0.001 * 7.337599277496338
Epoch 280, val loss: 0.9293437004089355
Epoch 290, training loss: 0.3589666783809662 = 0.3516395688056946 + 0.001 * 7.327120780944824
Epoch 290, val loss: 0.931672215461731
Epoch 300, training loss: 0.3229602873325348 = 0.31564491987228394 + 0.001 * 7.3153557777404785
Epoch 300, val loss: 0.9375934600830078
Epoch 310, training loss: 0.29004743695259094 = 0.28274300694465637 + 0.001 * 7.304418563842773
Epoch 310, val loss: 0.9463481903076172
Epoch 320, training loss: 0.2596854567527771 = 0.2523948550224304 + 0.001 * 7.290613174438477
Epoch 320, val loss: 0.9573642015457153
Epoch 330, training loss: 0.23159977793693542 = 0.22430485486984253 + 0.001 * 7.294928073883057
Epoch 330, val loss: 0.9704605937004089
Epoch 340, training loss: 0.20571269094944 = 0.19843824207782745 + 0.001 * 7.274449825286865
Epoch 340, val loss: 0.9852100610733032
Epoch 350, training loss: 0.18218180537223816 = 0.1749226599931717 + 0.001 * 7.259145259857178
Epoch 350, val loss: 1.0014584064483643
Epoch 360, training loss: 0.16108320653438568 = 0.1538311392068863 + 0.001 * 7.252064228057861
Epoch 360, val loss: 1.019446611404419
Epoch 370, training loss: 0.14235694706439972 = 0.1351023018360138 + 0.001 * 7.254644870758057
Epoch 370, val loss: 1.0388301610946655
Epoch 380, training loss: 0.12584905326366425 = 0.11860740184783936 + 0.001 * 7.241644859313965
Epoch 380, val loss: 1.0595638751983643
Epoch 390, training loss: 0.11142922192811966 = 0.104196697473526 + 0.001 * 7.232525825500488
Epoch 390, val loss: 1.0812886953353882
Epoch 400, training loss: 0.09889771789312363 = 0.09167080372571945 + 0.001 * 7.226917266845703
Epoch 400, val loss: 1.103589653968811
Epoch 410, training loss: 0.08804167807102203 = 0.0808209702372551 + 0.001 * 7.220705986022949
Epoch 410, val loss: 1.1263222694396973
Epoch 420, training loss: 0.07865317165851593 = 0.07143420726060867 + 0.001 * 7.218961238861084
Epoch 420, val loss: 1.1490954160690308
Epoch 430, training loss: 0.07052972167730331 = 0.06332096457481384 + 0.001 * 7.208754062652588
Epoch 430, val loss: 1.1718090772628784
Epoch 440, training loss: 0.0635073184967041 = 0.05630527064204216 + 0.001 * 7.202044486999512
Epoch 440, val loss: 1.1941839456558228
Epoch 450, training loss: 0.05743005871772766 = 0.05023547634482384 + 0.001 * 7.194582462310791
Epoch 450, val loss: 1.2161929607391357
Epoch 460, training loss: 0.052172448486089706 = 0.04497801139950752 + 0.001 * 7.194436073303223
Epoch 460, val loss: 1.2376292943954468
Epoch 470, training loss: 0.04759583994746208 = 0.04041464626789093 + 0.001 * 7.1811933517456055
Epoch 470, val loss: 1.2584694623947144
Epoch 480, training loss: 0.04362985119223595 = 0.03644613176584244 + 0.001 * 7.183717727661133
Epoch 480, val loss: 1.2787797451019287
Epoch 490, training loss: 0.040174998342990875 = 0.03298678249120712 + 0.001 * 7.188216686248779
Epoch 490, val loss: 1.2983797788619995
Epoch 500, training loss: 0.03713220730423927 = 0.029962748289108276 + 0.001 * 7.169460296630859
Epoch 500, val loss: 1.3173545598983765
Epoch 510, training loss: 0.034477028995752335 = 0.027311421930789948 + 0.001 * 7.165607929229736
Epoch 510, val loss: 1.3356558084487915
Epoch 520, training loss: 0.03214172273874283 = 0.02497960813343525 + 0.001 * 7.162112236022949
Epoch 520, val loss: 1.3533577919006348
Epoch 530, training loss: 0.03008590079843998 = 0.022922372445464134 + 0.001 * 7.163527965545654
Epoch 530, val loss: 1.3704116344451904
Epoch 540, training loss: 0.028261736035346985 = 0.02110167406499386 + 0.001 * 7.160060882568359
Epoch 540, val loss: 1.386892318725586
Epoch 550, training loss: 0.026644065976142883 = 0.0194855909794569 + 0.001 * 7.158475399017334
Epoch 550, val loss: 1.4027273654937744
Epoch 560, training loss: 0.02520054206252098 = 0.018046271055936813 + 0.001 * 7.154270648956299
Epoch 560, val loss: 1.4180071353912354
Epoch 570, training loss: 0.02392592281103134 = 0.016760019585490227 + 0.001 * 7.165903091430664
Epoch 570, val loss: 1.4327192306518555
Epoch 580, training loss: 0.02276337705552578 = 0.015606259927153587 + 0.001 * 7.157116889953613
Epoch 580, val loss: 1.4469090700149536
Epoch 590, training loss: 0.021721143275499344 = 0.014566182158887386 + 0.001 * 7.154959678649902
Epoch 590, val loss: 1.4605368375778198
Epoch 600, training loss: 0.020772624760866165 = 0.013623589649796486 + 0.001 * 7.14903450012207
Epoch 600, val loss: 1.4737508296966553
Epoch 610, training loss: 0.019931096583604813 = 0.012764466926455498 + 0.001 * 7.166628360748291
Epoch 610, val loss: 1.4864959716796875
Epoch 620, training loss: 0.019127044826745987 = 0.01197808887809515 + 0.001 * 7.148955345153809
Epoch 620, val loss: 1.4987648725509644
Epoch 630, training loss: 0.01840173825621605 = 0.01125562097877264 + 0.001 * 7.146115779876709
Epoch 630, val loss: 1.5106441974639893
Epoch 640, training loss: 0.017744246870279312 = 0.01059037633240223 + 0.001 * 7.15386962890625
Epoch 640, val loss: 1.5221946239471436
Epoch 650, training loss: 0.01712794229388237 = 0.00997722428292036 + 0.001 * 7.150717735290527
Epoch 650, val loss: 1.5333499908447266
Epoch 660, training loss: 0.016554612666368484 = 0.009411594830453396 + 0.001 * 7.143017768859863
Epoch 660, val loss: 1.544195294380188
Epoch 670, training loss: 0.01603102684020996 = 0.00888940878212452 + 0.001 * 7.141617298126221
Epoch 670, val loss: 1.5547056198120117
Epoch 680, training loss: 0.015564349479973316 = 0.008407117798924446 + 0.001 * 7.157231330871582
Epoch 680, val loss: 1.5648866891860962
Epoch 690, training loss: 0.01510641910135746 = 0.007961849682033062 + 0.001 * 7.14456844329834
Epoch 690, val loss: 1.574815034866333
Epoch 700, training loss: 0.01469072513282299 = 0.007550136651843786 + 0.001 * 7.140588760375977
Epoch 700, val loss: 1.5844265222549438
Epoch 710, training loss: 0.01431185845285654 = 0.007169022224843502 + 0.001 * 7.142836093902588
Epoch 710, val loss: 1.5937418937683105
Epoch 720, training loss: 0.013952480629086494 = 0.006815983913838863 + 0.001 * 7.1364970207214355
Epoch 720, val loss: 1.6027882099151611
Epoch 730, training loss: 0.013641138561069965 = 0.006488501094281673 + 0.001 * 7.152637004852295
Epoch 730, val loss: 1.6115880012512207
Epoch 740, training loss: 0.01332642138004303 = 0.006184530444443226 + 0.001 * 7.141890525817871
Epoch 740, val loss: 1.6201059818267822
Epoch 750, training loss: 0.013042319566011429 = 0.005901960656046867 + 0.001 * 7.140357971191406
Epoch 750, val loss: 1.6284213066101074
Epoch 760, training loss: 0.012778712436556816 = 0.005638919770717621 + 0.001 * 7.139792442321777
Epoch 760, val loss: 1.6364636421203613
Epoch 770, training loss: 0.012529395520687103 = 0.005393616389483213 + 0.001 * 7.13577938079834
Epoch 770, val loss: 1.644310474395752
Epoch 780, training loss: 0.012295477092266083 = 0.005164280068129301 + 0.001 * 7.131196975708008
Epoch 780, val loss: 1.651955485343933
Epoch 790, training loss: 0.012084685266017914 = 0.004949072375893593 + 0.001 * 7.1356120109558105
Epoch 790, val loss: 1.6593961715698242
Epoch 800, training loss: 0.011876692995429039 = 0.004746226128190756 + 0.001 * 7.130466938018799
Epoch 800, val loss: 1.6666793823242188
Epoch 810, training loss: 0.011681207455694675 = 0.004554207902401686 + 0.001 * 7.126999378204346
Epoch 810, val loss: 1.6738444566726685
Epoch 820, training loss: 0.011498425155878067 = 0.004371889866888523 + 0.001 * 7.126534461975098
Epoch 820, val loss: 1.6809346675872803
Epoch 830, training loss: 0.01133718341588974 = 0.00419828575104475 + 0.001 * 7.138896942138672
Epoch 830, val loss: 1.6880228519439697
Epoch 840, training loss: 0.01116921380162239 = 0.004033300094306469 + 0.001 * 7.135913848876953
Epoch 840, val loss: 1.695136308670044
Epoch 850, training loss: 0.01099942997097969 = 0.0038756998255848885 + 0.001 * 7.123729228973389
Epoch 850, val loss: 1.7023168802261353
Epoch 860, training loss: 0.010865641757845879 = 0.0037245138082653284 + 0.001 * 7.141128063201904
Epoch 860, val loss: 1.7095696926116943
Epoch 870, training loss: 0.010703635402023792 = 0.0035796421580016613 + 0.001 * 7.123992919921875
Epoch 870, val loss: 1.7169147729873657
Epoch 880, training loss: 0.010566234588623047 = 0.0034410045482218266 + 0.001 * 7.125229835510254
Epoch 880, val loss: 1.7242895364761353
Epoch 890, training loss: 0.010427765548229218 = 0.003308878280222416 + 0.001 * 7.118887424468994
Epoch 890, val loss: 1.7316207885742188
Epoch 900, training loss: 0.010302042588591576 = 0.003183231921866536 + 0.001 * 7.118810176849365
Epoch 900, val loss: 1.738929033279419
Epoch 910, training loss: 0.010185150429606438 = 0.0030640631448477507 + 0.001 * 7.121086597442627
Epoch 910, val loss: 1.746145248413086
Epoch 920, training loss: 0.010067623108625412 = 0.0029511782340705395 + 0.001 * 7.1164445877075195
Epoch 920, val loss: 1.7533015012741089
Epoch 930, training loss: 0.00996489729732275 = 0.0028444165363907814 + 0.001 * 7.120480537414551
Epoch 930, val loss: 1.7603299617767334
Epoch 940, training loss: 0.009862683713436127 = 0.002743523335084319 + 0.001 * 7.119160175323486
Epoch 940, val loss: 1.7672176361083984
Epoch 950, training loss: 0.009769206866621971 = 0.002648197812959552 + 0.001 * 7.121008396148682
Epoch 950, val loss: 1.7739756107330322
Epoch 960, training loss: 0.009674819186329842 = 0.002558336593210697 + 0.001 * 7.116481781005859
Epoch 960, val loss: 1.780551791191101
Epoch 970, training loss: 0.009583511389791965 = 0.002473512664437294 + 0.001 * 7.1099982261657715
Epoch 970, val loss: 1.7869832515716553
Epoch 980, training loss: 0.009501061402261257 = 0.0023933881893754005 + 0.001 * 7.107672691345215
Epoch 980, val loss: 1.7932535409927368
Epoch 990, training loss: 0.009434294886887074 = 0.0023176756221801043 + 0.001 * 7.116619110107422
Epoch 990, val loss: 1.7993583679199219
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 1.9501019716262817 = 1.9415050745010376 + 0.001 * 8.59684944152832
Epoch 0, val loss: 1.940949559211731
Epoch 10, training loss: 1.9409743547439575 = 1.932377576828003 + 0.001 * 8.596822738647461
Epoch 10, val loss: 1.9314398765563965
Epoch 20, training loss: 1.930079698562622 = 1.921483039855957 + 0.001 * 8.596700668334961
Epoch 20, val loss: 1.9200739860534668
Epoch 30, training loss: 1.9148441553115845 = 1.9062477350234985 + 0.001 * 8.596419334411621
Epoch 30, val loss: 1.9043099880218506
Epoch 40, training loss: 1.8922302722930908 = 1.8836344480514526 + 0.001 * 8.595771789550781
Epoch 40, val loss: 1.881141185760498
Epoch 50, training loss: 1.8594650030136108 = 1.850870966911316 + 0.001 * 8.594014167785645
Epoch 50, val loss: 1.848697304725647
Epoch 60, training loss: 1.8198562860488892 = 1.8112682104110718 + 0.001 * 8.588040351867676
Epoch 60, val loss: 1.8127943277359009
Epoch 70, training loss: 1.783259391784668 = 1.7746970653533936 + 0.001 * 8.562311172485352
Epoch 70, val loss: 1.7832057476043701
Epoch 80, training loss: 1.7393860816955566 = 1.7309924364089966 + 0.001 * 8.393698692321777
Epoch 80, val loss: 1.7455546855926514
Epoch 90, training loss: 1.6773110628128052 = 1.6692677736282349 + 0.001 * 8.043314933776855
Epoch 90, val loss: 1.6905395984649658
Epoch 100, training loss: 1.5930440425872803 = 1.5851281881332397 + 0.001 * 7.9158453941345215
Epoch 100, val loss: 1.6178818941116333
Epoch 110, training loss: 1.4906660318374634 = 1.4829341173171997 + 0.001 * 7.731921195983887
Epoch 110, val loss: 1.5328720808029175
Epoch 120, training loss: 1.3801063299179077 = 1.3724970817565918 + 0.001 * 7.609253883361816
Epoch 120, val loss: 1.4442423582077026
Epoch 130, training loss: 1.2697590589523315 = 1.2621541023254395 + 0.001 * 7.604978084564209
Epoch 130, val loss: 1.3595620393753052
Epoch 140, training loss: 1.1634583473205566 = 1.1558722257614136 + 0.001 * 7.586174488067627
Epoch 140, val loss: 1.2803852558135986
Epoch 150, training loss: 1.0640177726745605 = 1.056452989578247 + 0.001 * 7.5648040771484375
Epoch 150, val loss: 1.2082743644714355
Epoch 160, training loss: 0.973127007484436 = 0.965582013130188 + 0.001 * 7.54501485824585
Epoch 160, val loss: 1.144056797027588
Epoch 170, training loss: 0.8910077810287476 = 0.883478581905365 + 0.001 * 7.52921724319458
Epoch 170, val loss: 1.0879751443862915
Epoch 180, training loss: 0.8173575401306152 = 0.8098395466804504 + 0.001 * 7.518020153045654
Epoch 180, val loss: 1.0401692390441895
Epoch 190, training loss: 0.7515007257461548 = 0.7439937591552734 + 0.001 * 7.5069756507873535
Epoch 190, val loss: 1.0010021924972534
Epoch 200, training loss: 0.6923477649688721 = 0.6848562359809875 + 0.001 * 7.4915008544921875
Epoch 200, val loss: 0.9704134464263916
Epoch 210, training loss: 0.6387606859207153 = 0.6312909722328186 + 0.001 * 7.469703197479248
Epoch 210, val loss: 0.948137104511261
Epoch 220, training loss: 0.5897722244262695 = 0.582329511642456 + 0.001 * 7.442717552185059
Epoch 220, val loss: 0.9332570433616638
Epoch 230, training loss: 0.5446794033050537 = 0.5372617244720459 + 0.001 * 7.417693138122559
Epoch 230, val loss: 0.9249072670936584
Epoch 240, training loss: 0.5028641223907471 = 0.4954793453216553 + 0.001 * 7.384799957275391
Epoch 240, val loss: 0.92205411195755
Epoch 250, training loss: 0.4634997546672821 = 0.45614907145500183 + 0.001 * 7.350691318511963
Epoch 250, val loss: 0.9236630797386169
Epoch 260, training loss: 0.4258263111114502 = 0.4185061454772949 + 0.001 * 7.320170879364014
Epoch 260, val loss: 0.928618311882019
Epoch 270, training loss: 0.3893612027168274 = 0.3820594251155853 + 0.001 * 7.301775932312012
Epoch 270, val loss: 0.9364495277404785
Epoch 280, training loss: 0.35387152433395386 = 0.34657996892929077 + 0.001 * 7.291568756103516
Epoch 280, val loss: 0.9467496275901794
Epoch 290, training loss: 0.3193114399909973 = 0.31203094124794006 + 0.001 * 7.280489444732666
Epoch 290, val loss: 0.9594277739524841
Epoch 300, training loss: 0.2859705984592438 = 0.27869901061058044 + 0.001 * 7.271579265594482
Epoch 300, val loss: 0.9746540188789368
Epoch 310, training loss: 0.2544432282447815 = 0.24717450141906738 + 0.001 * 7.268730640411377
Epoch 310, val loss: 0.9924225807189941
Epoch 320, training loss: 0.22545406222343445 = 0.21819902956485748 + 0.001 * 7.255031108856201
Epoch 320, val loss: 1.0131282806396484
Epoch 330, training loss: 0.19958630204200745 = 0.19232356548309326 + 0.001 * 7.262730598449707
Epoch 330, val loss: 1.0365872383117676
Epoch 340, training loss: 0.17688781023025513 = 0.16964419186115265 + 0.001 * 7.243625640869141
Epoch 340, val loss: 1.062329888343811
Epoch 350, training loss: 0.15715277194976807 = 0.1499200314283371 + 0.001 * 7.232737064361572
Epoch 350, val loss: 1.0898650884628296
Epoch 360, training loss: 0.13999469578266144 = 0.13276995718479156 + 0.001 * 7.22474479675293
Epoch 360, val loss: 1.118594765663147
Epoch 370, training loss: 0.12503623962402344 = 0.11781751364469528 + 0.001 * 7.218729496002197
Epoch 370, val loss: 1.148094892501831
Epoch 380, training loss: 0.11195354163646698 = 0.1047409176826477 + 0.001 * 7.212620735168457
Epoch 380, val loss: 1.1778749227523804
Epoch 390, training loss: 0.10047925263643265 = 0.09327070415019989 + 0.001 * 7.208549499511719
Epoch 390, val loss: 1.2079064846038818
Epoch 400, training loss: 0.09039846062660217 = 0.08319534361362457 + 0.001 * 7.203117847442627
Epoch 400, val loss: 1.2378849983215332
Epoch 410, training loss: 0.081534743309021 = 0.07433664798736572 + 0.001 * 7.198097229003906
Epoch 410, val loss: 1.2676647901535034
Epoch 420, training loss: 0.07374285161495209 = 0.06654644757509232 + 0.001 * 7.196403503417969
Epoch 420, val loss: 1.2970918416976929
Epoch 430, training loss: 0.0668940618634224 = 0.059696197509765625 + 0.001 * 7.197864055633545
Epoch 430, val loss: 1.3260074853897095
Epoch 440, training loss: 0.06086262688040733 = 0.05367355793714523 + 0.001 * 7.189070224761963
Epoch 440, val loss: 1.3543390035629272
Epoch 450, training loss: 0.0555683933198452 = 0.04837885499000549 + 0.001 * 7.1895365715026855
Epoch 450, val loss: 1.38202965259552
Epoch 460, training loss: 0.050915300846099854 = 0.04372403398156166 + 0.001 * 7.191267490386963
Epoch 460, val loss: 1.4089516401290894
Epoch 470, training loss: 0.04681318998336792 = 0.03962847962975502 + 0.001 * 7.184711456298828
Epoch 470, val loss: 1.4351187944412231
Epoch 480, training loss: 0.0432041697204113 = 0.036015212535858154 + 0.001 * 7.1889567375183105
Epoch 480, val loss: 1.4605023860931396
Epoch 490, training loss: 0.03999499976634979 = 0.032814424484968185 + 0.001 * 7.180574417114258
Epoch 490, val loss: 1.4851303100585938
Epoch 500, training loss: 0.03715208172798157 = 0.02997126244008541 + 0.001 * 7.180820941925049
Epoch 500, val loss: 1.50913405418396
Epoch 510, training loss: 0.03462037071585655 = 0.027442796155810356 + 0.001 * 7.177574634552002
Epoch 510, val loss: 1.5324386358261108
Epoch 520, training loss: 0.03236725181341171 = 0.025191210210323334 + 0.001 * 7.176040172576904
Epoch 520, val loss: 1.5550816059112549
Epoch 530, training loss: 0.030354466289281845 = 0.023184074088931084 + 0.001 * 7.1703925132751465
Epoch 530, val loss: 1.5770398378372192
Epoch 540, training loss: 0.028568977490067482 = 0.02139185182750225 + 0.001 * 7.177125453948975
Epoch 540, val loss: 1.598266839981079
Epoch 550, training loss: 0.026961367577314377 = 0.019788578152656555 + 0.001 * 7.172790050506592
Epoch 550, val loss: 1.6188386678695679
Epoch 560, training loss: 0.02551821619272232 = 0.01835113950073719 + 0.001 * 7.16707706451416
Epoch 560, val loss: 1.6387404203414917
Epoch 570, training loss: 0.02422349900007248 = 0.017059821635484695 + 0.001 * 7.163677215576172
Epoch 570, val loss: 1.6579861640930176
Epoch 580, training loss: 0.023054761812090874 = 0.01589721068739891 + 0.001 * 7.157550811767578
Epoch 580, val loss: 1.676609754562378
Epoch 590, training loss: 0.022009363397955894 = 0.014848102815449238 + 0.001 * 7.161259651184082
Epoch 590, val loss: 1.6946066617965698
Epoch 600, training loss: 0.021058794111013412 = 0.013899262994527817 + 0.001 * 7.1595306396484375
Epoch 600, val loss: 1.7120096683502197
Epoch 610, training loss: 0.020185556262731552 = 0.013038977980613708 + 0.001 * 7.146577835083008
Epoch 610, val loss: 1.7288180589675903
Epoch 620, training loss: 0.019420890137553215 = 0.012257076799869537 + 0.001 * 7.163812637329102
Epoch 620, val loss: 1.7450604438781738
Epoch 630, training loss: 0.018696986138820648 = 0.011544689536094666 + 0.001 * 7.15229606628418
Epoch 630, val loss: 1.760764241218567
Epoch 640, training loss: 0.018041450530290604 = 0.010894102044403553 + 0.001 * 7.147347450256348
Epoch 640, val loss: 1.775974988937378
Epoch 650, training loss: 0.01747416891157627 = 0.010298524983227253 + 0.001 * 7.175643444061279
Epoch 650, val loss: 1.7906988859176636
Epoch 660, training loss: 0.01689448393881321 = 0.00975226890295744 + 0.001 * 7.142214298248291
Epoch 660, val loss: 1.8049404621124268
Epoch 670, training loss: 0.01638450101017952 = 0.009250080212950706 + 0.001 * 7.134420871734619
Epoch 670, val loss: 1.818740725517273
Epoch 680, training loss: 0.015926513820886612 = 0.008787454105913639 + 0.001 * 7.139059543609619
Epoch 680, val loss: 1.8320964574813843
Epoch 690, training loss: 0.015492824837565422 = 0.008360418491065502 + 0.001 * 7.132405757904053
Epoch 690, val loss: 1.8450437784194946
Epoch 700, training loss: 0.015100399032235146 = 0.007965502329170704 + 0.001 * 7.134896755218506
Epoch 700, val loss: 1.8575961589813232
Epoch 710, training loss: 0.014744741842150688 = 0.007599581032991409 + 0.001 * 7.14516019821167
Epoch 710, val loss: 1.8697618246078491
Epoch 720, training loss: 0.014392433688044548 = 0.007259942591190338 + 0.001 * 7.132490634918213
Epoch 720, val loss: 1.8815557956695557
Epoch 730, training loss: 0.01409422792494297 = 0.006944159045815468 + 0.001 * 7.150068759918213
Epoch 730, val loss: 1.8929916620254517
Epoch 740, training loss: 0.013778530061244965 = 0.006650104187428951 + 0.001 * 7.128425598144531
Epoch 740, val loss: 1.90406334400177
Epoch 750, training loss: 0.013489807024598122 = 0.006375840865075588 + 0.001 * 7.113966464996338
Epoch 750, val loss: 1.914804458618164
Epoch 760, training loss: 0.013239270076155663 = 0.006119625177234411 + 0.001 * 7.119645118713379
Epoch 760, val loss: 1.9252533912658691
Epoch 770, training loss: 0.01300763338804245 = 0.005879939999431372 + 0.001 * 7.127693176269531
Epoch 770, val loss: 1.9353870153427124
Epoch 780, training loss: 0.012777319177985191 = 0.005655386485159397 + 0.001 * 7.1219329833984375
Epoch 780, val loss: 1.9452288150787354
Epoch 790, training loss: 0.012567585334181786 = 0.005444689188152552 + 0.001 * 7.122895240783691
Epoch 790, val loss: 1.9548001289367676
Epoch 800, training loss: 0.01235433854162693 = 0.005246721673756838 + 0.001 * 7.107616424560547
Epoch 800, val loss: 1.96412193775177
Epoch 810, training loss: 0.01215756218880415 = 0.005060557741671801 + 0.001 * 7.097003936767578
Epoch 810, val loss: 1.9731897115707397
Epoch 820, training loss: 0.011981826275587082 = 0.004885254427790642 + 0.001 * 7.096571922302246
Epoch 820, val loss: 1.981990098953247
Epoch 830, training loss: 0.011827344074845314 = 0.004719994030892849 + 0.001 * 7.107349395751953
Epoch 830, val loss: 1.9905650615692139
Epoch 840, training loss: 0.011676225811243057 = 0.004564020317047834 + 0.001 * 7.112205505371094
Epoch 840, val loss: 1.9988937377929688
Epoch 850, training loss: 0.011509114876389503 = 0.0044166650623083115 + 0.001 * 7.09244966506958
Epoch 850, val loss: 2.00699782371521
Epoch 860, training loss: 0.01136592123657465 = 0.0042772856540977955 + 0.001 * 7.088635444641113
Epoch 860, val loss: 2.014895439147949
Epoch 870, training loss: 0.011227406561374664 = 0.004145296756178141 + 0.001 * 7.0821099281311035
Epoch 870, val loss: 2.022566080093384
Epoch 880, training loss: 0.011115560308098793 = 0.004020220600068569 + 0.001 * 7.095339298248291
Epoch 880, val loss: 2.030025005340576
Epoch 890, training loss: 0.010991121642291546 = 0.0039015784859657288 + 0.001 * 7.089542865753174
Epoch 890, val loss: 2.037276029586792
Epoch 900, training loss: 0.0108644999563694 = 0.003788917325437069 + 0.001 * 7.075582981109619
Epoch 900, val loss: 2.0443599224090576
Epoch 910, training loss: 0.010811247862875462 = 0.0036818755324929953 + 0.001 * 7.1293721199035645
Epoch 910, val loss: 2.051244020462036
Epoch 920, training loss: 0.010650085285305977 = 0.0035800375044345856 + 0.001 * 7.070046901702881
Epoch 920, val loss: 2.0579538345336914
Epoch 930, training loss: 0.010559439659118652 = 0.003483118722215295 + 0.001 * 7.076320171356201
Epoch 930, val loss: 2.064487934112549
Epoch 940, training loss: 0.01046908088028431 = 0.0033907864708453417 + 0.001 * 7.078293800354004
Epoch 940, val loss: 2.070869207382202
Epoch 950, training loss: 0.010364571586251259 = 0.003302742727100849 + 0.001 * 7.061829090118408
Epoch 950, val loss: 2.0770928859710693
Epoch 960, training loss: 0.010296376422047615 = 0.0032187181059271097 + 0.001 * 7.077657699584961
Epoch 960, val loss: 2.083158016204834
Epoch 970, training loss: 0.010214542038738728 = 0.003138498868793249 + 0.001 * 7.076042652130127
Epoch 970, val loss: 2.089054584503174
Epoch 980, training loss: 0.010128345340490341 = 0.003061828436329961 + 0.001 * 7.066516399383545
Epoch 980, val loss: 2.0948071479797363
Epoch 990, training loss: 0.010066506452858448 = 0.0029885112307965755 + 0.001 * 7.0779948234558105
Epoch 990, val loss: 2.1004109382629395
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 1.9467469453811646 = 1.9381500482559204 + 0.001 * 8.596839904785156
Epoch 0, val loss: 1.9412224292755127
Epoch 10, training loss: 1.9377355575561523 = 1.9291387796401978 + 0.001 * 8.596802711486816
Epoch 10, val loss: 1.932601809501648
Epoch 20, training loss: 1.9268826246261597 = 1.9182859659194946 + 0.001 * 8.596656799316406
Epoch 20, val loss: 1.9216432571411133
Epoch 30, training loss: 1.9119843244552612 = 1.9033880233764648 + 0.001 * 8.596299171447754
Epoch 30, val loss: 1.9061577320098877
Epoch 40, training loss: 1.8901832103729248 = 1.8815878629684448 + 0.001 * 8.595379829406738
Epoch 40, val loss: 1.8833645582199097
Epoch 50, training loss: 1.8590075969696045 = 1.8504148721694946 + 0.001 * 8.592711448669434
Epoch 50, val loss: 1.8515633344650269
Epoch 60, training loss: 1.8211982250213623 = 1.8126147985458374 + 0.001 * 8.583430290222168
Epoch 60, val loss: 1.8156993389129639
Epoch 70, training loss: 1.7870169878005981 = 1.7784720659255981 + 0.001 * 8.544962882995605
Epoch 70, val loss: 1.786964774131775
Epoch 80, training loss: 1.7496131658554077 = 1.7413272857666016 + 0.001 * 8.28589916229248
Epoch 80, val loss: 1.7551993131637573
Epoch 90, training loss: 1.696561574935913 = 1.688550591468811 + 0.001 * 8.01092529296875
Epoch 90, val loss: 1.7095346450805664
Epoch 100, training loss: 1.6235768795013428 = 1.6157960891723633 + 0.001 * 7.780768871307373
Epoch 100, val loss: 1.6477965116500854
Epoch 110, training loss: 1.5295164585113525 = 1.521834135055542 + 0.001 * 7.6823601722717285
Epoch 110, val loss: 1.5687649250030518
Epoch 120, training loss: 1.4208441972732544 = 1.4132041931152344 + 0.001 * 7.639969825744629
Epoch 120, val loss: 1.478959321975708
Epoch 130, training loss: 1.3073514699935913 = 1.2997552156448364 + 0.001 * 7.596303939819336
Epoch 130, val loss: 1.3870635032653809
Epoch 140, training loss: 1.1975280046463013 = 1.189992070198059 + 0.001 * 7.535944938659668
Epoch 140, val loss: 1.3007607460021973
Epoch 150, training loss: 1.0964263677597046 = 1.0889556407928467 + 0.001 * 7.470700263977051
Epoch 150, val loss: 1.223563551902771
Epoch 160, training loss: 1.0056500434875488 = 0.9982244372367859 + 0.001 * 7.4255805015563965
Epoch 160, val loss: 1.1563268899917603
Epoch 170, training loss: 0.9239934682846069 = 0.9165874719619751 + 0.001 * 7.405989646911621
Epoch 170, val loss: 1.0974671840667725
Epoch 180, training loss: 0.8498120307922363 = 0.8424140810966492 + 0.001 * 7.39792537689209
Epoch 180, val loss: 1.0457793474197388
Epoch 190, training loss: 0.7825781106948853 = 0.7751848697662354 + 0.001 * 7.393214225769043
Epoch 190, val loss: 1.0007015466690063
Epoch 200, training loss: 0.7223943471908569 = 0.7150119543075562 + 0.001 * 7.382367134094238
Epoch 200, val loss: 0.9627545475959778
Epoch 210, training loss: 0.668785810470581 = 0.6614192724227905 + 0.001 * 7.366512298583984
Epoch 210, val loss: 0.9325403571128845
Epoch 220, training loss: 0.6200394630432129 = 0.612699031829834 + 0.001 * 7.340460777282715
Epoch 220, val loss: 0.9090386629104614
Epoch 230, training loss: 0.5738086104393005 = 0.5664843916893005 + 0.001 * 7.324210166931152
Epoch 230, val loss: 0.8902222514152527
Epoch 240, training loss: 0.5282716155052185 = 0.520972728729248 + 0.001 * 7.298880100250244
Epoch 240, val loss: 0.8743370175361633
Epoch 250, training loss: 0.4828251898288727 = 0.47554492950439453 + 0.001 * 7.280264854431152
Epoch 250, val loss: 0.8611612319946289
Epoch 260, training loss: 0.4379846751689911 = 0.4307103753089905 + 0.001 * 7.274301528930664
Epoch 260, val loss: 0.8507912755012512
Epoch 270, training loss: 0.39467287063598633 = 0.3874029517173767 + 0.001 * 7.269924163818359
Epoch 270, val loss: 0.8436505198478699
Epoch 280, training loss: 0.35372674465179443 = 0.3464588224887848 + 0.001 * 7.267935752868652
Epoch 280, val loss: 0.8398052453994751
Epoch 290, training loss: 0.3154914975166321 = 0.30822479724884033 + 0.001 * 7.266714572906494
Epoch 290, val loss: 0.8385687470436096
Epoch 300, training loss: 0.27986979484558105 = 0.2726040184497833 + 0.001 * 7.265766143798828
Epoch 300, val loss: 0.8398456573486328
Epoch 310, training loss: 0.24673207104206085 = 0.239465594291687 + 0.001 * 7.26647424697876
Epoch 310, val loss: 0.8428869247436523
Epoch 320, training loss: 0.21622422337532043 = 0.2089630514383316 + 0.001 * 7.26117467880249
Epoch 320, val loss: 0.8475711345672607
Epoch 330, training loss: 0.18867333233356476 = 0.18141506612300873 + 0.001 * 7.258266925811768
Epoch 330, val loss: 0.8540146350860596
Epoch 340, training loss: 0.1642991453409195 = 0.15703950822353363 + 0.001 * 7.259631156921387
Epoch 340, val loss: 0.8622941970825195
Epoch 350, training loss: 0.14310386776924133 = 0.135844424366951 + 0.001 * 7.259446144104004
Epoch 350, val loss: 0.8724226355552673
Epoch 360, training loss: 0.12487649917602539 = 0.11762567609548569 + 0.001 * 7.250826358795166
Epoch 360, val loss: 0.8844100832939148
Epoch 370, training loss: 0.10930901765823364 = 0.10206416994333267 + 0.001 * 7.244845867156982
Epoch 370, val loss: 0.8978592753410339
Epoch 380, training loss: 0.09606550633907318 = 0.08881621807813644 + 0.001 * 7.249287128448486
Epoch 380, val loss: 0.9125388860702515
Epoch 390, training loss: 0.08479026705026627 = 0.07755184173583984 + 0.001 * 7.238425254821777
Epoch 390, val loss: 0.9280813336372375
Epoch 400, training loss: 0.07520616054534912 = 0.06797435134649277 + 0.001 * 7.231809616088867
Epoch 400, val loss: 0.9441962242126465
Epoch 410, training loss: 0.06705164909362793 = 0.059824224561452866 + 0.001 * 7.227423667907715
Epoch 410, val loss: 0.9605109095573425
Epoch 420, training loss: 0.060108259320259094 = 0.05287990719079971 + 0.001 * 7.2283525466918945
Epoch 420, val loss: 0.9769151210784912
Epoch 430, training loss: 0.054180484265089035 = 0.04695254936814308 + 0.001 * 7.22793436050415
Epoch 430, val loss: 0.9931526184082031
Epoch 440, training loss: 0.049113236367702484 = 0.04188260808587074 + 0.001 * 7.230627059936523
Epoch 440, val loss: 1.0092252492904663
Epoch 450, training loss: 0.044748395681381226 = 0.03753182664513588 + 0.001 * 7.2165703773498535
Epoch 450, val loss: 1.024969220161438
Epoch 460, training loss: 0.0409969724714756 = 0.033786166459321976 + 0.001 * 7.210804462432861
Epoch 460, val loss: 1.0403518676757812
Epoch 470, training loss: 0.0377589613199234 = 0.030550841242074966 + 0.001 * 7.2081217765808105
Epoch 470, val loss: 1.0553691387176514
Epoch 480, training loss: 0.03495797514915466 = 0.02774461731314659 + 0.001 * 7.2133564949035645
Epoch 480, val loss: 1.0698986053466797
Epoch 490, training loss: 0.03250444307923317 = 0.0252996813505888 + 0.001 * 7.204761981964111
Epoch 490, val loss: 1.0840033292770386
Epoch 500, training loss: 0.03036288358271122 = 0.023160528391599655 + 0.001 * 7.202354431152344
Epoch 500, val loss: 1.0976601839065552
Epoch 510, training loss: 0.028477635234594345 = 0.02128082886338234 + 0.001 * 7.196805953979492
Epoch 510, val loss: 1.1109033823013306
Epoch 520, training loss: 0.02681579999625683 = 0.019621707499027252 + 0.001 * 7.194091796875
Epoch 520, val loss: 1.1236907243728638
Epoch 530, training loss: 0.02534208819270134 = 0.018151292577385902 + 0.001 * 7.1907958984375
Epoch 530, val loss: 1.1360628604888916
Epoch 540, training loss: 0.02403966337442398 = 0.01684282161295414 + 0.001 * 7.196840763092041
Epoch 540, val loss: 1.148005723953247
Epoch 550, training loss: 0.02286253310739994 = 0.015673840418457985 + 0.001 * 7.188692092895508
Epoch 550, val loss: 1.1595884561538696
Epoch 560, training loss: 0.021816840395331383 = 0.014625736512243748 + 0.001 * 7.191102981567383
Epoch 560, val loss: 1.1707842350006104
Epoch 570, training loss: 0.02088112384080887 = 0.013682665303349495 + 0.001 * 7.198459148406982
Epoch 570, val loss: 1.181634545326233
Epoch 580, training loss: 0.02002641186118126 = 0.012831205502152443 + 0.001 * 7.1952056884765625
Epoch 580, val loss: 1.1921137571334839
Epoch 590, training loss: 0.019247492775321007 = 0.012059979140758514 + 0.001 * 7.18751335144043
Epoch 590, val loss: 1.202284574508667
Epoch 600, training loss: 0.018541978672146797 = 0.011359315365552902 + 0.001 * 7.182662487030029
Epoch 600, val loss: 1.212164044380188
Epoch 610, training loss: 0.017908604815602303 = 0.010720937512814999 + 0.001 * 7.187667369842529
Epoch 610, val loss: 1.2217069864273071
Epoch 620, training loss: 0.017321741208434105 = 0.010137730278074741 + 0.001 * 7.184010982513428
Epoch 620, val loss: 1.230972409248352
Epoch 630, training loss: 0.016777152195572853 = 0.00960348267108202 + 0.001 * 7.173669338226318
Epoch 630, val loss: 1.2399725914001465
Epoch 640, training loss: 0.016285840421915054 = 0.009112917818129063 + 0.001 * 7.172921180725098
Epoch 640, val loss: 1.2486902475357056
Epoch 650, training loss: 0.015833359211683273 = 0.008661413565278053 + 0.001 * 7.171944618225098
Epoch 650, val loss: 1.2571592330932617
Epoch 660, training loss: 0.015430516563355923 = 0.00824489165097475 + 0.001 * 7.185624599456787
Epoch 660, val loss: 1.2653883695602417
Epoch 670, training loss: 0.015029951930046082 = 0.007859867066144943 + 0.001 * 7.1700849533081055
Epoch 670, val loss: 1.2733674049377441
Epoch 680, training loss: 0.014669284224510193 = 0.007503211498260498 + 0.001 * 7.166072368621826
Epoch 680, val loss: 1.2811180353164673
Epoch 690, training loss: 0.01433168537914753 = 0.007172238547354937 + 0.001 * 7.1594462394714355
Epoch 690, val loss: 1.288640022277832
Epoch 700, training loss: 0.014022912830114365 = 0.006864509079605341 + 0.001 * 7.158403396606445
Epoch 700, val loss: 1.2959704399108887
Epoch 710, training loss: 0.013742310926318169 = 0.006577897351235151 + 0.001 * 7.164412975311279
Epoch 710, val loss: 1.303083896636963
Epoch 720, training loss: 0.013468765653669834 = 0.0063105253502726555 + 0.001 * 7.158239841461182
Epoch 720, val loss: 1.310023546218872
Epoch 730, training loss: 0.013214926235377789 = 0.006060673855245113 + 0.001 * 7.154252052307129
Epoch 730, val loss: 1.3167914152145386
Epoch 740, training loss: 0.012976570054888725 = 0.005826835986226797 + 0.001 * 7.149734020233154
Epoch 740, val loss: 1.3233953714370728
Epoch 750, training loss: 0.012767200358211994 = 0.005607638508081436 + 0.001 * 7.159561634063721
Epoch 750, val loss: 1.3298243284225464
Epoch 760, training loss: 0.012557407841086388 = 0.0054018497467041016 + 0.001 * 7.155557632446289
Epoch 760, val loss: 1.336110234260559
Epoch 770, training loss: 0.012371513061225414 = 0.005208240821957588 + 0.001 * 7.163271903991699
Epoch 770, val loss: 1.3422378301620483
Epoch 780, training loss: 0.012175098992884159 = 0.005025765858590603 + 0.001 * 7.1493330001831055
Epoch 780, val loss: 1.3482252359390259
Epoch 790, training loss: 0.011997733265161514 = 0.00485328771173954 + 0.001 * 7.144445896148682
Epoch 790, val loss: 1.354142427444458
Epoch 800, training loss: 0.011837025173008442 = 0.004689689259976149 + 0.001 * 7.147335529327393
Epoch 800, val loss: 1.3599711656570435
Epoch 810, training loss: 0.01166742667555809 = 0.0045339735224843025 + 0.001 * 7.133452892303467
Epoch 810, val loss: 1.3657678365707397
Epoch 820, training loss: 0.011542887426912785 = 0.004385349340736866 + 0.001 * 7.157537937164307
Epoch 820, val loss: 1.3715343475341797
Epoch 830, training loss: 0.011393122375011444 = 0.0042433058843016624 + 0.001 * 7.149815559387207
Epoch 830, val loss: 1.3772896528244019
Epoch 840, training loss: 0.01125561073422432 = 0.004107446409761906 + 0.001 * 7.148163318634033
Epoch 840, val loss: 1.3829962015151978
Epoch 850, training loss: 0.011119497939944267 = 0.003977480810135603 + 0.001 * 7.142017364501953
Epoch 850, val loss: 1.3886947631835938
Epoch 860, training loss: 0.011005304753780365 = 0.003853224217891693 + 0.001 * 7.1520795822143555
Epoch 860, val loss: 1.3943296670913696
Epoch 870, training loss: 0.010864303447306156 = 0.0037344410084187984 + 0.001 * 7.129862308502197
Epoch 870, val loss: 1.3999208211898804
Epoch 880, training loss: 0.01076540257781744 = 0.0036209155805408955 + 0.001 * 7.144486427307129
Epoch 880, val loss: 1.4054539203643799
Epoch 890, training loss: 0.01064817514270544 = 0.0035124914720654488 + 0.001 * 7.135683536529541
Epoch 890, val loss: 1.410921335220337
Epoch 900, training loss: 0.010554333217442036 = 0.003408983815461397 + 0.001 * 7.145349025726318
Epoch 900, val loss: 1.4163284301757812
Epoch 910, training loss: 0.010445665568113327 = 0.003310193307697773 + 0.001 * 7.135472297668457
Epoch 910, val loss: 1.4216463565826416
Epoch 920, training loss: 0.010349423624575138 = 0.0032158985268324614 + 0.001 * 7.1335248947143555
Epoch 920, val loss: 1.4268966913223267
Epoch 930, training loss: 0.010223165154457092 = 0.003125871066004038 + 0.001 * 7.097293853759766
Epoch 930, val loss: 1.4320652484893799
Epoch 940, training loss: 0.010140543803572655 = 0.0030399286188185215 + 0.001 * 7.100614547729492
Epoch 940, val loss: 1.4371392726898193
Epoch 950, training loss: 0.010091032832860947 = 0.002957858145236969 + 0.001 * 7.133174419403076
Epoch 950, val loss: 1.4421348571777344
Epoch 960, training loss: 0.00998107623308897 = 0.0028794778045266867 + 0.001 * 7.101598262786865
Epoch 960, val loss: 1.4470312595367432
Epoch 970, training loss: 0.009919572621583939 = 0.002804545918479562 + 0.001 * 7.115026473999023
Epoch 970, val loss: 1.4518274068832397
Epoch 980, training loss: 0.009816614910960197 = 0.0027329041622579098 + 0.001 * 7.083710670471191
Epoch 980, val loss: 1.45657217502594
Epoch 990, training loss: 0.009780295193195343 = 0.002664372092112899 + 0.001 * 7.115922451019287
Epoch 990, val loss: 1.4612270593643188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8081180811808119
The final CL Acc:0.75309, 0.02124, The final GNN Acc:0.81392, 0.00415
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13282])
remove edge: torch.Size([2, 7962])
updated graph: torch.Size([2, 10688])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9589742422103882 = 1.950377345085144 + 0.001 * 8.596841812133789
Epoch 0, val loss: 1.9391708374023438
Epoch 10, training loss: 1.94920814037323 = 1.9406113624572754 + 0.001 * 8.5968017578125
Epoch 10, val loss: 1.9302926063537598
Epoch 20, training loss: 1.93757164478302 = 1.928974986076355 + 0.001 * 8.596637725830078
Epoch 20, val loss: 1.9194166660308838
Epoch 30, training loss: 1.9213913679122925 = 1.912795066833496 + 0.001 * 8.596259117126465
Epoch 30, val loss: 1.9039031267166138
Epoch 40, training loss: 1.897150993347168 = 1.888555645942688 + 0.001 * 8.595303535461426
Epoch 40, val loss: 1.8807861804962158
Epoch 50, training loss: 1.8618203401565552 = 1.8532280921936035 + 0.001 * 8.592222213745117
Epoch 50, val loss: 1.8485221862792969
Epoch 60, training loss: 1.8189313411712646 = 1.8103522062301636 + 0.001 * 8.579075813293457
Epoch 60, val loss: 1.8130110502243042
Epoch 70, training loss: 1.7796705961227417 = 1.7711576223373413 + 0.001 * 8.512954711914062
Epoch 70, val loss: 1.7835763692855835
Epoch 80, training loss: 1.7362141609191895 = 1.7280477285385132 + 0.001 * 8.166402816772461
Epoch 80, val loss: 1.7456785440444946
Epoch 90, training loss: 1.676047921180725 = 1.6680457592010498 + 0.001 * 8.002130508422852
Epoch 90, val loss: 1.6900556087493896
Epoch 100, training loss: 1.5952174663543701 = 1.587381362915039 + 0.001 * 7.836098670959473
Epoch 100, val loss: 1.6179358959197998
Epoch 110, training loss: 1.4955812692642212 = 1.4879018068313599 + 0.001 * 7.679500579833984
Epoch 110, val loss: 1.5343292951583862
Epoch 120, training loss: 1.3879703283309937 = 1.380372166633606 + 0.001 * 7.598124027252197
Epoch 120, val loss: 1.4456392526626587
Epoch 130, training loss: 1.2832646369934082 = 1.2757176160812378 + 0.001 * 7.546966552734375
Epoch 130, val loss: 1.3628062009811401
Epoch 140, training loss: 1.1868033409118652 = 1.1793526411056519 + 0.001 * 7.450747013092041
Epoch 140, val loss: 1.28667151927948
Epoch 150, training loss: 1.1012721061706543 = 1.0939230918884277 + 0.001 * 7.349072456359863
Epoch 150, val loss: 1.2202253341674805
Epoch 160, training loss: 1.027878761291504 = 1.020585060119629 + 0.001 * 7.293658256530762
Epoch 160, val loss: 1.1640868186950684
Epoch 170, training loss: 0.9650726318359375 = 0.957794725894928 + 0.001 * 7.277902603149414
Epoch 170, val loss: 1.1175810098648071
Epoch 180, training loss: 0.9090554714202881 = 0.9017847776412964 + 0.001 * 7.2706990242004395
Epoch 180, val loss: 1.0773719549179077
Epoch 190, training loss: 0.8551824688911438 = 0.8479161262512207 + 0.001 * 7.2663187980651855
Epoch 190, val loss: 1.0391356945037842
Epoch 200, training loss: 0.7998694777488708 = 0.7926072478294373 + 0.001 * 7.262211799621582
Epoch 200, val loss: 0.9990357756614685
Epoch 210, training loss: 0.7417044043540955 = 0.7344459295272827 + 0.001 * 7.258488655090332
Epoch 210, val loss: 0.9562889933586121
Epoch 220, training loss: 0.6811818480491638 = 0.673926830291748 + 0.001 * 7.254998683929443
Epoch 220, val loss: 0.9121455550193787
Epoch 230, training loss: 0.6195176243782043 = 0.612266480922699 + 0.001 * 7.2511305809021
Epoch 230, val loss: 0.8683956861495972
Epoch 240, training loss: 0.5574572682380676 = 0.5502104163169861 + 0.001 * 7.246881008148193
Epoch 240, val loss: 0.8262398838996887
Epoch 250, training loss: 0.4956561028957367 = 0.48841437697410583 + 0.001 * 7.241732120513916
Epoch 250, val loss: 0.7863088250160217
Epoch 260, training loss: 0.4355529546737671 = 0.42831742763519287 + 0.001 * 7.235540866851807
Epoch 260, val loss: 0.7498741149902344
Epoch 270, training loss: 0.3793170154094696 = 0.37209266424179077 + 0.001 * 7.224342346191406
Epoch 270, val loss: 0.7184326648712158
Epoch 280, training loss: 0.32905274629592896 = 0.3218415379524231 + 0.001 * 7.211199760437012
Epoch 280, val loss: 0.693596363067627
Epoch 290, training loss: 0.2857277989387512 = 0.2785327136516571 + 0.001 * 7.195098876953125
Epoch 290, val loss: 0.6759015917778015
Epoch 300, training loss: 0.24908392131328583 = 0.24190638959407806 + 0.001 * 7.177532196044922
Epoch 300, val loss: 0.6650840640068054
Epoch 310, training loss: 0.21819648146629333 = 0.21102485060691833 + 0.001 * 7.171637535095215
Epoch 310, val loss: 0.6598325967788696
Epoch 320, training loss: 0.19198843836784363 = 0.18483050167560577 + 0.001 * 7.15794038772583
Epoch 320, val loss: 0.6589035987854004
Epoch 330, training loss: 0.16957545280456543 = 0.16242243349552155 + 0.001 * 7.153014183044434
Epoch 330, val loss: 0.6611002087593079
Epoch 340, training loss: 0.15025043487548828 = 0.14310339093208313 + 0.001 * 7.147049903869629
Epoch 340, val loss: 0.6655075550079346
Epoch 350, training loss: 0.13348691165447235 = 0.12634429335594177 + 0.001 * 7.1426167488098145
Epoch 350, val loss: 0.6713480353355408
Epoch 360, training loss: 0.11888417601585388 = 0.11174647510051727 + 0.001 * 7.137698173522949
Epoch 360, val loss: 0.6781208515167236
Epoch 370, training loss: 0.10613902658224106 = 0.099002406001091 + 0.001 * 7.136621475219727
Epoch 370, val loss: 0.6855453848838806
Epoch 380, training loss: 0.09499453008174896 = 0.08786702901124954 + 0.001 * 7.127498149871826
Epoch 380, val loss: 0.6934695243835449
Epoch 390, training loss: 0.08525560796260834 = 0.07813690602779388 + 0.001 * 7.118697643280029
Epoch 390, val loss: 0.701749861240387
Epoch 400, training loss: 0.07675394415855408 = 0.06964023411273956 + 0.001 * 7.113706111907959
Epoch 400, val loss: 0.7103570699691772
Epoch 410, training loss: 0.06933069229125977 = 0.06222521513700485 + 0.001 * 7.105475902557373
Epoch 410, val loss: 0.7192568778991699
Epoch 420, training loss: 0.06285688281059265 = 0.05575438588857651 + 0.001 * 7.102496147155762
Epoch 420, val loss: 0.728395938873291
Epoch 430, training loss: 0.05720878392457962 = 0.05010266974568367 + 0.001 * 7.106114864349365
Epoch 430, val loss: 0.7376872301101685
Epoch 440, training loss: 0.05225399136543274 = 0.045157913118600845 + 0.001 * 7.096079349517822
Epoch 440, val loss: 0.7471221089363098
Epoch 450, training loss: 0.04792546108365059 = 0.040823835879564285 + 0.001 * 7.101625919342041
Epoch 450, val loss: 0.7566089034080505
Epoch 460, training loss: 0.04409848526120186 = 0.03701583296060562 + 0.001 * 7.082651138305664
Epoch 460, val loss: 0.7660701870918274
Epoch 470, training loss: 0.040745243430137634 = 0.03366215527057648 + 0.001 * 7.083088397979736
Epoch 470, val loss: 0.7754602432250977
Epoch 480, training loss: 0.037783119827508926 = 0.03070211410522461 + 0.001 * 7.08100700378418
Epoch 480, val loss: 0.7847492098808289
Epoch 490, training loss: 0.03515918552875519 = 0.02808084525167942 + 0.001 * 7.078338146209717
Epoch 490, val loss: 0.7938919067382812
Epoch 500, training loss: 0.032828450202941895 = 0.025752313435077667 + 0.001 * 7.076135635375977
Epoch 500, val loss: 0.8028497695922852
Epoch 510, training loss: 0.030750762671232224 = 0.023673394694924355 + 0.001 * 7.077367782592773
Epoch 510, val loss: 0.8116486668586731
Epoch 520, training loss: 0.028884822502732277 = 0.021809039637446404 + 0.001 * 7.075782775878906
Epoch 520, val loss: 0.8202974796295166
Epoch 530, training loss: 0.027213165536522865 = 0.02012859843671322 + 0.001 * 7.084567070007324
Epoch 530, val loss: 0.8288541436195374
Epoch 540, training loss: 0.025684915482997894 = 0.01860722340643406 + 0.001 * 7.077691555023193
Epoch 540, val loss: 0.837350070476532
Epoch 550, training loss: 0.024302184581756592 = 0.017227778211236 + 0.001 * 7.074406623840332
Epoch 550, val loss: 0.845820963382721
Epoch 560, training loss: 0.02304813824594021 = 0.015977049246430397 + 0.001 * 7.0710883140563965
Epoch 560, val loss: 0.8542564511299133
Epoch 570, training loss: 0.021921668201684952 = 0.014843269251286983 + 0.001 * 7.078398704528809
Epoch 570, val loss: 0.86260986328125
Epoch 580, training loss: 0.020887959748506546 = 0.013815468177199364 + 0.001 * 7.072490692138672
Epoch 580, val loss: 0.8708587884902954
Epoch 590, training loss: 0.01995183154940605 = 0.012883283197879791 + 0.001 * 7.068548202514648
Epoch 590, val loss: 0.8789907097816467
Epoch 600, training loss: 0.01910858415067196 = 0.012037000618875027 + 0.001 * 7.0715837478637695
Epoch 600, val loss: 0.8869975805282593
Epoch 610, training loss: 0.018336942419409752 = 0.011267895810306072 + 0.001 * 7.069046497344971
Epoch 610, val loss: 0.8948686122894287
Epoch 620, training loss: 0.017650991678237915 = 0.010567988269031048 + 0.001 * 7.083002090454102
Epoch 620, val loss: 0.9026088118553162
Epoch 630, training loss: 0.016996756196022034 = 0.009930008091032505 + 0.001 * 7.066746711730957
Epoch 630, val loss: 0.9101711511611938
Epoch 640, training loss: 0.016415655612945557 = 0.009347448125481606 + 0.001 * 7.068207263946533
Epoch 640, val loss: 0.9175812602043152
Epoch 650, training loss: 0.015878647565841675 = 0.008814550004899502 + 0.001 * 7.064096450805664
Epoch 650, val loss: 0.924803614616394
Epoch 660, training loss: 0.015405314974486828 = 0.008326176553964615 + 0.001 * 7.079138278961182
Epoch 660, val loss: 0.9318562746047974
Epoch 670, training loss: 0.014938369393348694 = 0.007877773605287075 + 0.001 * 7.060595989227295
Epoch 670, val loss: 0.9387505650520325
Epoch 680, training loss: 0.014529095962643623 = 0.007465391419827938 + 0.001 * 7.063704490661621
Epoch 680, val loss: 0.9454830884933472
Epoch 690, training loss: 0.014145122841000557 = 0.007085398305207491 + 0.001 * 7.059723854064941
Epoch 690, val loss: 0.9520556330680847
Epoch 700, training loss: 0.013798560947179794 = 0.006734653376042843 + 0.001 * 7.063907146453857
Epoch 700, val loss: 0.9584739208221436
Epoch 710, training loss: 0.013474997133016586 = 0.0064103445038199425 + 0.001 * 7.064651966094971
Epoch 710, val loss: 0.9647446274757385
Epoch 720, training loss: 0.013164606876671314 = 0.006109954323619604 + 0.001 * 7.054652214050293
Epoch 720, val loss: 0.970856249332428
Epoch 730, training loss: 0.012906910851597786 = 0.005831221118569374 + 0.001 * 7.075689792633057
Epoch 730, val loss: 0.9768351316452026
Epoch 740, training loss: 0.012628449127078056 = 0.005572238937020302 + 0.001 * 7.056210517883301
Epoch 740, val loss: 0.9826639890670776
Epoch 750, training loss: 0.012385528534650803 = 0.005331188905984163 + 0.001 * 7.05433988571167
Epoch 750, val loss: 0.9883594512939453
Epoch 760, training loss: 0.012158900499343872 = 0.005106572061777115 + 0.001 * 7.052327632904053
Epoch 760, val loss: 0.993914783000946
Epoch 770, training loss: 0.011948741972446442 = 0.004896881524473429 + 0.001 * 7.0518598556518555
Epoch 770, val loss: 0.9993377923965454
Epoch 780, training loss: 0.011759636923670769 = 0.004700828343629837 + 0.001 * 7.058808326721191
Epoch 780, val loss: 1.0046420097351074
Epoch 790, training loss: 0.011567758396267891 = 0.004517338704317808 + 0.001 * 7.050418853759766
Epoch 790, val loss: 1.0098166465759277
Epoch 800, training loss: 0.01138943899422884 = 0.004345293622463942 + 0.001 * 7.044145107269287
Epoch 800, val loss: 1.0148770809173584
Epoch 810, training loss: 0.011227230541408062 = 0.0041838050819933414 + 0.001 * 7.0434250831604
Epoch 810, val loss: 1.0198206901550293
Epoch 820, training loss: 0.011077772825956345 = 0.004032038617879152 + 0.001 * 7.04573392868042
Epoch 820, val loss: 1.0246596336364746
Epoch 830, training loss: 0.010930776596069336 = 0.003889248240739107 + 0.001 * 7.041528224945068
Epoch 830, val loss: 1.0293961763381958
Epoch 840, training loss: 0.010794640518724918 = 0.003754751058295369 + 0.001 * 7.039888858795166
Epoch 840, val loss: 1.034023404121399
Epoch 850, training loss: 0.010663401335477829 = 0.0036279140040278435 + 0.001 * 7.035487651824951
Epoch 850, val loss: 1.038552165031433
Epoch 860, training loss: 0.010546872392296791 = 0.0035081501118838787 + 0.001 * 7.038722038269043
Epoch 860, val loss: 1.042982816696167
Epoch 870, training loss: 0.010439824312925339 = 0.0033949674107134342 + 0.001 * 7.044856071472168
Epoch 870, val loss: 1.047316074371338
Epoch 880, training loss: 0.01032678410410881 = 0.00328789371997118 + 0.001 * 7.038889408111572
Epoch 880, val loss: 1.0515543222427368
Epoch 890, training loss: 0.01022505946457386 = 0.003186493646353483 + 0.001 * 7.038565635681152
Epoch 890, val loss: 1.0556931495666504
Epoch 900, training loss: 0.01013638824224472 = 0.003090354846790433 + 0.001 * 7.0460333824157715
Epoch 900, val loss: 1.059751272201538
Epoch 910, training loss: 0.01003575511276722 = 0.0029991664923727512 + 0.001 * 7.036587715148926
Epoch 910, val loss: 1.0637223720550537
Epoch 920, training loss: 0.009947353973984718 = 0.0029125497676432133 + 0.001 * 7.034804344177246
Epoch 920, val loss: 1.0676043033599854
Epoch 930, training loss: 0.009858744218945503 = 0.0028302492573857307 + 0.001 * 7.0284953117370605
Epoch 930, val loss: 1.0714091062545776
Epoch 940, training loss: 0.009784518741071224 = 0.002751955995336175 + 0.001 * 7.032562732696533
Epoch 940, val loss: 1.075139045715332
Epoch 950, training loss: 0.00969823356717825 = 0.002677412936463952 + 0.001 * 7.020820140838623
Epoch 950, val loss: 1.0787863731384277
Epoch 960, training loss: 0.00962813850492239 = 0.0026063949335366488 + 0.001 * 7.021743297576904
Epoch 960, val loss: 1.0823612213134766
Epoch 970, training loss: 0.009561934508383274 = 0.0025386724155396223 + 0.001 * 7.023262023925781
Epoch 970, val loss: 1.0858606100082397
Epoch 980, training loss: 0.009502663277089596 = 0.0024740409571677446 + 0.001 * 7.028621673583984
Epoch 980, val loss: 1.0893023014068604
Epoch 990, training loss: 0.00944073498249054 = 0.0024123131297528744 + 0.001 * 7.028420925140381
Epoch 990, val loss: 1.0926589965820312
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 1.955387830734253 = 1.9467910528182983 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.9471256732940674
Epoch 10, training loss: 1.9451745748519897 = 1.9365777969360352 + 0.001 * 8.596797943115234
Epoch 10, val loss: 1.9366291761398315
Epoch 20, training loss: 1.9330894947052002 = 1.9244928359985352 + 0.001 * 8.596641540527344
Epoch 20, val loss: 1.9238710403442383
Epoch 30, training loss: 1.9166384935379028 = 1.9080421924591064 + 0.001 * 8.596297264099121
Epoch 30, val loss: 1.906585693359375
Epoch 40, training loss: 1.8926482200622559 = 1.8840527534484863 + 0.001 * 8.595476150512695
Epoch 40, val loss: 1.88165283203125
Epoch 50, training loss: 1.8586230278015137 = 1.850029706954956 + 0.001 * 8.593271255493164
Epoch 50, val loss: 1.847471833229065
Epoch 60, training loss: 1.8175281286239624 = 1.808942198753357 + 0.001 * 8.585963249206543
Epoch 60, val loss: 1.8093855381011963
Epoch 70, training loss: 1.779341220855713 = 1.7707842588424683 + 0.001 * 8.556961059570312
Epoch 70, val loss: 1.7773483991622925
Epoch 80, training loss: 1.737095832824707 = 1.728708267211914 + 0.001 * 8.387541770935059
Epoch 80, val loss: 1.7400000095367432
Epoch 90, training loss: 1.678216576576233 = 1.6701529026031494 + 0.001 * 8.06362247467041
Epoch 90, val loss: 1.6868095397949219
Epoch 100, training loss: 1.5981481075286865 = 1.5902761220932007 + 0.001 * 7.872039794921875
Epoch 100, val loss: 1.6158770322799683
Epoch 110, training loss: 1.497541069984436 = 1.4898922443389893 + 0.001 * 7.648776531219482
Epoch 110, val loss: 1.5299413204193115
Epoch 120, training loss: 1.38534414768219 = 1.3778526782989502 + 0.001 * 7.491430759429932
Epoch 120, val loss: 1.4362140893936157
Epoch 130, training loss: 1.2724661827087402 = 1.2650145292282104 + 0.001 * 7.451663017272949
Epoch 130, val loss: 1.3449043035507202
Epoch 140, training loss: 1.1671428680419922 = 1.1597553491592407 + 0.001 * 7.3874735832214355
Epoch 140, val loss: 1.262766718864441
Epoch 150, training loss: 1.0745478868484497 = 1.0672210454940796 + 0.001 * 7.326857089996338
Epoch 150, val loss: 1.192853569984436
Epoch 160, training loss: 0.9960454106330872 = 0.9887816309928894 + 0.001 * 7.2637786865234375
Epoch 160, val loss: 1.1358481645584106
Epoch 170, training loss: 0.9289206862449646 = 0.9217014908790588 + 0.001 * 7.219191074371338
Epoch 170, val loss: 1.0883976221084595
Epoch 180, training loss: 0.8683105707168579 = 0.8611081838607788 + 0.001 * 7.202389717102051
Epoch 180, val loss: 1.0455952882766724
Epoch 190, training loss: 0.8096308708190918 = 0.8024346828460693 + 0.001 * 7.196191310882568
Epoch 190, val loss: 1.0028786659240723
Epoch 200, training loss: 0.7501063346862793 = 0.7429140210151672 + 0.001 * 7.192302703857422
Epoch 200, val loss: 0.9575589895248413
Epoch 210, training loss: 0.6892070174217224 = 0.682016909122467 + 0.001 * 7.190104961395264
Epoch 210, val loss: 0.9095495939254761
Epoch 220, training loss: 0.627973735332489 = 0.6207840442657471 + 0.001 * 7.189673900604248
Epoch 220, val loss: 0.8604992628097534
Epoch 230, training loss: 0.5675343871116638 = 0.5603450536727905 + 0.001 * 7.1893181800842285
Epoch 230, val loss: 0.8132399916648865
Epoch 240, training loss: 0.5084432363510132 = 0.5012547373771667 + 0.001 * 7.188485145568848
Epoch 240, val loss: 0.7691318392753601
Epoch 250, training loss: 0.4511171579360962 = 0.44393008947372437 + 0.001 * 7.187068462371826
Epoch 250, val loss: 0.7294006943702698
Epoch 260, training loss: 0.39624977111816406 = 0.3890645205974579 + 0.001 * 7.185259819030762
Epoch 260, val loss: 0.6945199966430664
Epoch 270, training loss: 0.3448981046676636 = 0.33771437406539917 + 0.001 * 7.183724880218506
Epoch 270, val loss: 0.6647740006446838
Epoch 280, training loss: 0.2981856167316437 = 0.291003555059433 + 0.001 * 7.182068824768066
Epoch 280, val loss: 0.6400846838951111
Epoch 290, training loss: 0.2568459212779999 = 0.24966496229171753 + 0.001 * 7.18094539642334
Epoch 290, val loss: 0.6202473044395447
Epoch 300, training loss: 0.22110134363174438 = 0.21392080187797546 + 0.001 * 7.180540561676025
Epoch 300, val loss: 0.6049855351448059
Epoch 310, training loss: 0.19076459109783173 = 0.18358387053012848 + 0.001 * 7.180721759796143
Epoch 310, val loss: 0.5940958857536316
Epoch 320, training loss: 0.16534125804901123 = 0.1581599861383438 + 0.001 * 7.181270599365234
Epoch 320, val loss: 0.5870939493179321
Epoch 330, training loss: 0.14415687322616577 = 0.1369747519493103 + 0.001 * 7.182126522064209
Epoch 330, val loss: 0.5835047960281372
Epoch 340, training loss: 0.12647712230682373 = 0.1192934662103653 + 0.001 * 7.183663368225098
Epoch 340, val loss: 0.582722008228302
Epoch 350, training loss: 0.11161036789417267 = 0.1044260635972023 + 0.001 * 7.184304714202881
Epoch 350, val loss: 0.5841353535652161
Epoch 360, training loss: 0.0989869013428688 = 0.09180174022912979 + 0.001 * 7.1851630210876465
Epoch 360, val loss: 0.5872528553009033
Epoch 370, training loss: 0.08816928416490555 = 0.08098311722278595 + 0.001 * 7.186169147491455
Epoch 370, val loss: 0.591654896736145
Epoch 380, training loss: 0.07883618772029877 = 0.07164917886257172 + 0.001 * 7.187007427215576
Epoch 380, val loss: 0.5970419645309448
Epoch 390, training loss: 0.07074995338916779 = 0.06356215476989746 + 0.001 * 7.18779993057251
Epoch 390, val loss: 0.6031948924064636
Epoch 400, training loss: 0.0637243464589119 = 0.056535106152296066 + 0.001 * 7.18923807144165
Epoch 400, val loss: 0.6099266409873962
Epoch 410, training loss: 0.05761052668094635 = 0.050421327352523804 + 0.001 * 7.189198970794678
Epoch 410, val loss: 0.6171157360076904
Epoch 420, training loss: 0.05229077488183975 = 0.045100949704647064 + 0.001 * 7.189826488494873
Epoch 420, val loss: 0.6246576905250549
Epoch 430, training loss: 0.04766058549284935 = 0.04046928882598877 + 0.001 * 7.1912970542907715
Epoch 430, val loss: 0.6324770450592041
Epoch 440, training loss: 0.04362548887729645 = 0.03643529862165451 + 0.001 * 7.190188884735107
Epoch 440, val loss: 0.6404155492782593
Epoch 450, training loss: 0.04010846093297005 = 0.03291817009449005 + 0.001 * 7.190290927886963
Epoch 450, val loss: 0.6484062671661377
Epoch 460, training loss: 0.03703673556447029 = 0.029846590012311935 + 0.001 * 7.1901445388793945
Epoch 460, val loss: 0.6564059257507324
Epoch 470, training loss: 0.034348826855421066 = 0.027159059420228004 + 0.001 * 7.189766883850098
Epoch 470, val loss: 0.6643452048301697
Epoch 480, training loss: 0.03198959678411484 = 0.02480090595781803 + 0.001 * 7.188689708709717
Epoch 480, val loss: 0.6721875071525574
Epoch 490, training loss: 0.029913270846009254 = 0.022725773975253105 + 0.001 * 7.187497138977051
Epoch 490, val loss: 0.679902970790863
Epoch 500, training loss: 0.0280831940472126 = 0.020893685519695282 + 0.001 * 7.189507961273193
Epoch 500, val loss: 0.6875054836273193
Epoch 510, training loss: 0.026456065475940704 = 0.019270872697234154 + 0.001 * 7.185192108154297
Epoch 510, val loss: 0.6949425339698792
Epoch 520, training loss: 0.0250102486461401 = 0.017828766256570816 + 0.001 * 7.181481838226318
Epoch 520, val loss: 0.7021981477737427
Epoch 530, training loss: 0.023744123056530952 = 0.016542857512831688 + 0.001 * 7.20126485824585
Epoch 530, val loss: 0.7092567682266235
Epoch 540, training loss: 0.02257419377565384 = 0.01539216935634613 + 0.001 * 7.182023525238037
Epoch 540, val loss: 0.7161353230476379
Epoch 550, training loss: 0.021530654281377792 = 0.014359104447066784 + 0.001 * 7.171550273895264
Epoch 550, val loss: 0.722831666469574
Epoch 560, training loss: 0.020600108429789543 = 0.013428554870188236 + 0.001 * 7.171553134918213
Epoch 560, val loss: 0.7293482422828674
Epoch 570, training loss: 0.01974925957620144 = 0.012587723322212696 + 0.001 * 7.161535739898682
Epoch 570, val loss: 0.7357009053230286
Epoch 580, training loss: 0.01898990385234356 = 0.011825733818113804 + 0.001 * 7.164169788360596
Epoch 580, val loss: 0.7418757081031799
Epoch 590, training loss: 0.018291302025318146 = 0.011132949031889439 + 0.001 * 7.158352851867676
Epoch 590, val loss: 0.7478749752044678
Epoch 600, training loss: 0.017650838941335678 = 0.010500536300241947 + 0.001 * 7.150301933288574
Epoch 600, val loss: 0.7537202835083008
Epoch 610, training loss: 0.017056794837117195 = 0.009919331409037113 + 0.001 * 7.137463569641113
Epoch 610, val loss: 0.759464681148529
Epoch 620, training loss: 0.01649216003715992 = 0.009381196461617947 + 0.001 * 7.110962867736816
Epoch 620, val loss: 0.7651183009147644
Epoch 630, training loss: 0.015988454222679138 = 0.00887950137257576 + 0.001 * 7.108953475952148
Epoch 630, val loss: 0.7706756591796875
Epoch 640, training loss: 0.01552656851708889 = 0.008410466834902763 + 0.001 * 7.1161017417907715
Epoch 640, val loss: 0.7762018442153931
Epoch 650, training loss: 0.015071028843522072 = 0.007971839979290962 + 0.001 * 7.099188804626465
Epoch 650, val loss: 0.7816063761711121
Epoch 660, training loss: 0.014671243727207184 = 0.007561579812318087 + 0.001 * 7.109663486480713
Epoch 660, val loss: 0.7869758605957031
Epoch 670, training loss: 0.01426224410533905 = 0.007178292144089937 + 0.001 * 7.083950996398926
Epoch 670, val loss: 0.7922434210777283
Epoch 680, training loss: 0.013925900682806969 = 0.006820920389145613 + 0.001 * 7.104979515075684
Epoch 680, val loss: 0.7974424362182617
Epoch 690, training loss: 0.013548208400607109 = 0.0064878338016569614 + 0.001 * 7.060373783111572
Epoch 690, val loss: 0.802515983581543
Epoch 700, training loss: 0.01323530450463295 = 0.006177295930683613 + 0.001 * 7.058008193969727
Epoch 700, val loss: 0.80751633644104
Epoch 710, training loss: 0.012950596399605274 = 0.005887829698622227 + 0.001 * 7.0627665519714355
Epoch 710, val loss: 0.8124063611030579
Epoch 720, training loss: 0.012682177126407623 = 0.0056181130930781364 + 0.001 * 7.064063549041748
Epoch 720, val loss: 0.8172134160995483
Epoch 730, training loss: 0.012441935949027538 = 0.005366643890738487 + 0.001 * 7.075291633605957
Epoch 730, val loss: 0.8219020962715149
Epoch 740, training loss: 0.012179767712950706 = 0.005132058169692755 + 0.001 * 7.0477094650268555
Epoch 740, val loss: 0.8264841437339783
Epoch 750, training loss: 0.011998513713479042 = 0.004912988282740116 + 0.001 * 7.085525035858154
Epoch 750, val loss: 0.8309723734855652
Epoch 760, training loss: 0.011753156781196594 = 0.0047082859091460705 + 0.001 * 7.044870376586914
Epoch 760, val loss: 0.8353868126869202
Epoch 770, training loss: 0.011550613678991795 = 0.0045167445205152035 + 0.001 * 7.033868789672852
Epoch 770, val loss: 0.8396863341331482
Epoch 780, training loss: 0.011385373771190643 = 0.0043373703956604 + 0.001 * 7.048003673553467
Epoch 780, val loss: 0.8438913226127625
Epoch 790, training loss: 0.011206422001123428 = 0.004169325809925795 + 0.001 * 7.03709602355957
Epoch 790, val loss: 0.8479984402656555
Epoch 800, training loss: 0.011064998805522919 = 0.004011629614979029 + 0.001 * 7.053369045257568
Epoch 800, val loss: 0.8520306944847107
Epoch 810, training loss: 0.010922841727733612 = 0.0038635008968412876 + 0.001 * 7.059340953826904
Epoch 810, val loss: 0.8559703230857849
Epoch 820, training loss: 0.010768219828605652 = 0.0037241948302835226 + 0.001 * 7.044024467468262
Epoch 820, val loss: 0.8598036766052246
Epoch 830, training loss: 0.010638242587447166 = 0.003593029920011759 + 0.001 * 7.0452117919921875
Epoch 830, val loss: 0.8635762333869934
Epoch 840, training loss: 0.010502109304070473 = 0.0034694268833845854 + 0.001 * 7.032682418823242
Epoch 840, val loss: 0.8672437071800232
Epoch 850, training loss: 0.01043386198580265 = 0.0033528271596878767 + 0.001 * 7.081034183502197
Epoch 850, val loss: 0.870857834815979
Epoch 860, training loss: 0.010282857343554497 = 0.003242792095988989 + 0.001 * 7.040064811706543
Epoch 860, val loss: 0.874370813369751
Epoch 870, training loss: 0.010150863789021969 = 0.003138795029371977 + 0.001 * 7.012068271636963
Epoch 870, val loss: 0.8778003454208374
Epoch 880, training loss: 0.010051025077700615 = 0.0030404345598071814 + 0.001 * 7.010590553283691
Epoch 880, val loss: 0.8811571002006531
Epoch 890, training loss: 0.009983759373426437 = 0.002947289729490876 + 0.001 * 7.036469459533691
Epoch 890, val loss: 0.8844540119171143
Epoch 900, training loss: 0.009893420152366161 = 0.002858994295820594 + 0.001 * 7.034425258636475
Epoch 900, val loss: 0.8876765966415405
Epoch 910, training loss: 0.009806359186768532 = 0.002775222063064575 + 0.001 * 7.031137466430664
Epoch 910, val loss: 0.890830934047699
Epoch 920, training loss: 0.009719468653202057 = 0.0026956950314342976 + 0.001 * 7.023773670196533
Epoch 920, val loss: 0.8939141631126404
Epoch 930, training loss: 0.009630325250327587 = 0.0026201370637863874 + 0.001 * 7.010188102722168
Epoch 930, val loss: 0.8969300389289856
Epoch 940, training loss: 0.009555517695844173 = 0.0025482887867838144 + 0.001 * 7.007228851318359
Epoch 940, val loss: 0.8998843431472778
Epoch 950, training loss: 0.009509492665529251 = 0.0024798912927508354 + 0.001 * 7.029601097106934
Epoch 950, val loss: 0.9027647972106934
Epoch 960, training loss: 0.009403269737958908 = 0.0024147506337612867 + 0.001 * 6.988519191741943
Epoch 960, val loss: 0.9056098461151123
Epoch 970, training loss: 0.00936923734843731 = 0.002352649113163352 + 0.001 * 7.01658821105957
Epoch 970, val loss: 0.9083966016769409
Epoch 980, training loss: 0.009295852854847908 = 0.0022934339940547943 + 0.001 * 7.0024189949035645
Epoch 980, val loss: 0.9111027717590332
Epoch 990, training loss: 0.00921696238219738 = 0.002236917382106185 + 0.001 * 6.980044364929199
Epoch 990, val loss: 0.9137686491012573
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8481481481481482
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.9500601291656494 = 1.9414633512496948 + 0.001 * 8.596829414367676
Epoch 0, val loss: 1.9451490640640259
Epoch 10, training loss: 1.94045090675354 = 1.9318541288375854 + 0.001 * 8.596763610839844
Epoch 10, val loss: 1.9350773096084595
Epoch 20, training loss: 1.9283313751220703 = 1.9197348356246948 + 0.001 * 8.596549987792969
Epoch 20, val loss: 1.922387719154358
Epoch 30, training loss: 1.9109470844268799 = 1.9023510217666626 + 0.001 * 8.596047401428223
Epoch 30, val loss: 1.904363751411438
Epoch 40, training loss: 1.8849899768829346 = 1.8763951063156128 + 0.001 * 8.594831466674805
Epoch 40, val loss: 1.877793788909912
Epoch 50, training loss: 1.848915696144104 = 1.8403245210647583 + 0.001 * 8.591140747070312
Epoch 50, val loss: 1.8423786163330078
Epoch 60, training loss: 1.8085743188858032 = 1.7999976873397827 + 0.001 * 8.576642036437988
Epoch 60, val loss: 1.8060579299926758
Epoch 70, training loss: 1.771371603012085 = 1.762866735458374 + 0.001 * 8.504888534545898
Epoch 70, val loss: 1.774025559425354
Epoch 80, training loss: 1.7228342294692993 = 1.7147417068481445 + 0.001 * 8.092538833618164
Epoch 80, val loss: 1.7292081117630005
Epoch 90, training loss: 1.6557210683822632 = 1.6477724313735962 + 0.001 * 7.9486212730407715
Epoch 90, val loss: 1.6681137084960938
Epoch 100, training loss: 1.5676668882369995 = 1.5598305463790894 + 0.001 * 7.836399555206299
Epoch 100, val loss: 1.5916913747787476
Epoch 110, training loss: 1.4675287008285522 = 1.4598593711853027 + 0.001 * 7.669380187988281
Epoch 110, val loss: 1.5046855211257935
Epoch 120, training loss: 1.3637685775756836 = 1.356239914894104 + 0.001 * 7.528640270233154
Epoch 120, val loss: 1.4168369770050049
Epoch 130, training loss: 1.258430004119873 = 1.2509138584136963 + 0.001 * 7.516094207763672
Epoch 130, val loss: 1.3289200067520142
Epoch 140, training loss: 1.15291166305542 = 1.1454336643218994 + 0.001 * 7.477967262268066
Epoch 140, val loss: 1.2429935932159424
Epoch 150, training loss: 1.0498871803283691 = 1.042449951171875 + 0.001 * 7.437186241149902
Epoch 150, val loss: 1.161244511604309
Epoch 160, training loss: 0.9515421986579895 = 0.9441414475440979 + 0.001 * 7.400779724121094
Epoch 160, val loss: 1.0849553346633911
Epoch 170, training loss: 0.8597334623336792 = 0.8523717522621155 + 0.001 * 7.361692905426025
Epoch 170, val loss: 1.0147989988327026
Epoch 180, training loss: 0.7762863039970398 = 0.7689668536186218 + 0.001 * 7.319447040557861
Epoch 180, val loss: 0.9527162909507751
Epoch 190, training loss: 0.7023753523826599 = 0.6950888633728027 + 0.001 * 7.286477565765381
Epoch 190, val loss: 0.900243878364563
Epoch 200, training loss: 0.6378053426742554 = 0.6305325031280518 + 0.001 * 7.272838592529297
Epoch 200, val loss: 0.8582682013511658
Epoch 210, training loss: 0.5810911655426025 = 0.5738236904144287 + 0.001 * 7.267482757568359
Epoch 210, val loss: 0.8261299133300781
Epoch 220, training loss: 0.530390739440918 = 0.5231360197067261 + 0.001 * 7.254744052886963
Epoch 220, val loss: 0.8020794987678528
Epoch 230, training loss: 0.48404401540756226 = 0.47680580615997314 + 0.001 * 7.238213539123535
Epoch 230, val loss: 0.7842782735824585
Epoch 240, training loss: 0.4408165514469147 = 0.4335988163948059 + 0.001 * 7.217740058898926
Epoch 240, val loss: 0.7711796760559082
Epoch 250, training loss: 0.39996641874313354 = 0.39276769757270813 + 0.001 * 7.198733329772949
Epoch 250, val loss: 0.7616339325904846
Epoch 260, training loss: 0.3611855208873749 = 0.35400626063346863 + 0.001 * 7.179271697998047
Epoch 260, val loss: 0.7549538612365723
Epoch 270, training loss: 0.32454347610473633 = 0.31737396121025085 + 0.001 * 7.1695237159729
Epoch 270, val loss: 0.7509347796440125
Epoch 280, training loss: 0.2902812659740448 = 0.28312069177627563 + 0.001 * 7.160575866699219
Epoch 280, val loss: 0.7497203946113586
Epoch 290, training loss: 0.2587023377418518 = 0.25154808163642883 + 0.001 * 7.154247760772705
Epoch 290, val loss: 0.7514649629592896
Epoch 300, training loss: 0.23002058267593384 = 0.2228669971227646 + 0.001 * 7.153589725494385
Epoch 300, val loss: 0.7561724781990051
Epoch 310, training loss: 0.20431019365787506 = 0.1971578449010849 + 0.001 * 7.152347087860107
Epoch 310, val loss: 0.7637428045272827
Epoch 320, training loss: 0.181514710187912 = 0.17436401546001434 + 0.001 * 7.150691986083984
Epoch 320, val loss: 0.7739386558532715
Epoch 330, training loss: 0.1614740490913391 = 0.15432530641555786 + 0.001 * 7.148736000061035
Epoch 330, val loss: 0.7863113880157471
Epoch 340, training loss: 0.1439639776945114 = 0.13681621849536896 + 0.001 * 7.147763729095459
Epoch 340, val loss: 0.8005946278572083
Epoch 350, training loss: 0.12871606647968292 = 0.12156699597835541 + 0.001 * 7.149063587188721
Epoch 350, val loss: 0.8164747953414917
Epoch 360, training loss: 0.11543656885623932 = 0.1082882285118103 + 0.001 * 7.14833927154541
Epoch 360, val loss: 0.833533525466919
Epoch 370, training loss: 0.1038518100976944 = 0.09670518338680267 + 0.001 * 7.146628379821777
Epoch 370, val loss: 0.8514903783798218
Epoch 380, training loss: 0.09372223913669586 = 0.08657567948102951 + 0.001 * 7.146559238433838
Epoch 380, val loss: 0.8700414299964905
Epoch 390, training loss: 0.08483894914388657 = 0.07768982648849487 + 0.001 * 7.149125576019287
Epoch 390, val loss: 0.8889639377593994
Epoch 400, training loss: 0.07701990753412247 = 0.0698716938495636 + 0.001 * 7.148213863372803
Epoch 400, val loss: 0.9080768823623657
Epoch 410, training loss: 0.07012592256069183 = 0.06297868490219116 + 0.001 * 7.147237300872803
Epoch 410, val loss: 0.9272209405899048
Epoch 420, training loss: 0.06403915584087372 = 0.05689278990030289 + 0.001 * 7.146368980407715
Epoch 420, val loss: 0.9462018609046936
Epoch 430, training loss: 0.05866377800703049 = 0.05151510611176491 + 0.001 * 7.1486711502075195
Epoch 430, val loss: 0.9649630784988403
Epoch 440, training loss: 0.05390632897615433 = 0.046758051961660385 + 0.001 * 7.148277759552002
Epoch 440, val loss: 0.9834104180335999
Epoch 450, training loss: 0.0496901273727417 = 0.042544398456811905 + 0.001 * 7.145727157592773
Epoch 450, val loss: 1.001493215560913
Epoch 460, training loss: 0.045952208340168 = 0.03880739212036133 + 0.001 * 7.144816875457764
Epoch 460, val loss: 1.0191724300384521
Epoch 470, training loss: 0.042632102966308594 = 0.03548787161707878 + 0.001 * 7.144229412078857
Epoch 470, val loss: 1.0364278554916382
Epoch 480, training loss: 0.0396801196038723 = 0.03253374248743057 + 0.001 * 7.146376609802246
Epoch 480, val loss: 1.0532406568527222
Epoch 490, training loss: 0.03704608231782913 = 0.029900098219513893 + 0.001 * 7.145983695983887
Epoch 490, val loss: 1.0695910453796387
Epoch 500, training loss: 0.03469075262546539 = 0.027547787874937057 + 0.001 * 7.142963886260986
Epoch 500, val loss: 1.0854742527008057
Epoch 510, training loss: 0.03258431330323219 = 0.02544225938618183 + 0.001 * 7.142054557800293
Epoch 510, val loss: 1.1008962392807007
Epoch 520, training loss: 0.03069405071437359 = 0.02355346828699112 + 0.001 * 7.1405816078186035
Epoch 520, val loss: 1.1158701181411743
Epoch 530, training loss: 0.02900015003979206 = 0.021855313330888748 + 0.001 * 7.144836902618408
Epoch 530, val loss: 1.1304259300231934
Epoch 540, training loss: 0.02747594192624092 = 0.020325247198343277 + 0.001 * 7.150694847106934
Epoch 540, val loss: 1.1445621252059937
Epoch 550, training loss: 0.026084329932928085 = 0.018943706527352333 + 0.001 * 7.140623569488525
Epoch 550, val loss: 1.1582682132720947
Epoch 560, training loss: 0.02483132667839527 = 0.01769348792731762 + 0.001 * 7.137837886810303
Epoch 560, val loss: 1.1715481281280518
Epoch 570, training loss: 0.023695796728134155 = 0.0165594220161438 + 0.001 * 7.136373996734619
Epoch 570, val loss: 1.1844403743743896
Epoch 580, training loss: 0.022662896662950516 = 0.015528516843914986 + 0.001 * 7.1343793869018555
Epoch 580, val loss: 1.1969773769378662
Epoch 590, training loss: 0.0217319056391716 = 0.014589400961995125 + 0.001 * 7.1425042152404785
Epoch 590, val loss: 1.2091712951660156
Epoch 600, training loss: 0.020866634324193 = 0.013732079416513443 + 0.001 * 7.134554386138916
Epoch 600, val loss: 1.221021294593811
Epoch 610, training loss: 0.02007925510406494 = 0.012947759591042995 + 0.001 * 7.131494045257568
Epoch 610, val loss: 1.2325403690338135
Epoch 620, training loss: 0.01935870386660099 = 0.012228753417730331 + 0.001 * 7.129949569702148
Epoch 620, val loss: 1.2437251806259155
Epoch 630, training loss: 0.018707115203142166 = 0.011568397283554077 + 0.001 * 7.138718128204346
Epoch 630, val loss: 1.2545807361602783
Epoch 640, training loss: 0.018089011311531067 = 0.010960781015455723 + 0.001 * 7.128230094909668
Epoch 640, val loss: 1.2651159763336182
Epoch 650, training loss: 0.017524082213640213 = 0.010400583036243916 + 0.001 * 7.123498439788818
Epoch 650, val loss: 1.2753593921661377
Epoch 660, training loss: 0.017038879916071892 = 0.009883168153464794 + 0.001 * 7.155711650848389
Epoch 660, val loss: 1.2853171825408936
Epoch 670, training loss: 0.01653308793902397 = 0.009404458105564117 + 0.001 * 7.128629684448242
Epoch 670, val loss: 1.295007348060608
Epoch 680, training loss: 0.016081012785434723 = 0.008960782550275326 + 0.001 * 7.1202287673950195
Epoch 680, val loss: 1.3044381141662598
Epoch 690, training loss: 0.015665899962186813 = 0.008548871614038944 + 0.001 * 7.117028713226318
Epoch 690, val loss: 1.313623070716858
Epoch 700, training loss: 0.015290498733520508 = 0.008165853098034859 + 0.001 * 7.124645709991455
Epoch 700, val loss: 1.3225752115249634
Epoch 710, training loss: 0.014930324628949165 = 0.00780914444476366 + 0.001 * 7.121179580688477
Epoch 710, val loss: 1.3312911987304688
Epoch 720, training loss: 0.01459297351539135 = 0.007476442493498325 + 0.001 * 7.116530418395996
Epoch 720, val loss: 1.3397871255874634
Epoch 730, training loss: 0.014281883835792542 = 0.007165661547333002 + 0.001 * 7.116222381591797
Epoch 730, val loss: 1.3480665683746338
Epoch 740, training loss: 0.013980088755488396 = 0.00687495619058609 + 0.001 * 7.105132579803467
Epoch 740, val loss: 1.3561416864395142
Epoch 750, training loss: 0.01374685950577259 = 0.0066026668064296246 + 0.001 * 7.144192218780518
Epoch 750, val loss: 1.364008903503418
Epoch 760, training loss: 0.013454107567667961 = 0.006347343325614929 + 0.001 * 7.1067633628845215
Epoch 760, val loss: 1.371666431427002
Epoch 770, training loss: 0.013207338750362396 = 0.006107564549893141 + 0.001 * 7.09977388381958
Epoch 770, val loss: 1.379151463508606
Epoch 780, training loss: 0.012990512885153294 = 0.0058821458369493484 + 0.001 * 7.1083664894104
Epoch 780, val loss: 1.386452555656433
Epoch 790, training loss: 0.012792740017175674 = 0.005669973790645599 + 0.001 * 7.12276554107666
Epoch 790, val loss: 1.393571138381958
Epoch 800, training loss: 0.012567505240440369 = 0.0054700481705367565 + 0.001 * 7.097456932067871
Epoch 800, val loss: 1.4005275964736938
Epoch 810, training loss: 0.012369688600301743 = 0.00528146093711257 + 0.001 * 7.088226795196533
Epoch 810, val loss: 1.407319188117981
Epoch 820, training loss: 0.012193698436021805 = 0.0051033576019108295 + 0.001 * 7.090341091156006
Epoch 820, val loss: 1.4139567613601685
Epoch 830, training loss: 0.012020684778690338 = 0.004934998694807291 + 0.001 * 7.085686206817627
Epoch 830, val loss: 1.4204341173171997
Epoch 840, training loss: 0.011860568076372147 = 0.0047756885178387165 + 0.001 * 7.0848798751831055
Epoch 840, val loss: 1.4267727136611938
Epoch 850, training loss: 0.011715714819729328 = 0.004624790046364069 + 0.001 * 7.090924263000488
Epoch 850, val loss: 1.4329639673233032
Epoch 860, training loss: 0.01155625656247139 = 0.004481746349483728 + 0.001 * 7.07451057434082
Epoch 860, val loss: 1.4390246868133545
Epoch 870, training loss: 0.01143084466457367 = 0.004346014466136694 + 0.001 * 7.084829330444336
Epoch 870, val loss: 1.4449541568756104
Epoch 880, training loss: 0.011295398697257042 = 0.004217108711600304 + 0.001 * 7.07828950881958
Epoch 880, val loss: 1.4507536888122559
Epoch 890, training loss: 0.011184223927557468 = 0.004094586241990328 + 0.001 * 7.089637279510498
Epoch 890, val loss: 1.456423282623291
Epoch 900, training loss: 0.011059984564781189 = 0.00397801399230957 + 0.001 * 7.081970691680908
Epoch 900, val loss: 1.4619653224945068
Epoch 910, training loss: 0.010940022766590118 = 0.00386704970151186 + 0.001 * 7.072973251342773
Epoch 910, val loss: 1.467389464378357
Epoch 920, training loss: 0.010871237143874168 = 0.0037612896412611008 + 0.001 * 7.1099467277526855
Epoch 920, val loss: 1.472705364227295
Epoch 930, training loss: 0.010720796883106232 = 0.0036604562774300575 + 0.001 * 7.060340404510498
Epoch 930, val loss: 1.477886438369751
Epoch 940, training loss: 0.01064018253237009 = 0.0035642392467707396 + 0.001 * 7.0759429931640625
Epoch 940, val loss: 1.4829617738723755
Epoch 950, training loss: 0.01056332141160965 = 0.0034723656717687845 + 0.001 * 7.09095573425293
Epoch 950, val loss: 1.4879306554794312
Epoch 960, training loss: 0.010434135794639587 = 0.0033845107536762953 + 0.001 * 7.049624443054199
Epoch 960, val loss: 1.4927959442138672
Epoch 970, training loss: 0.010372912511229515 = 0.0033003638964146376 + 0.001 * 7.0725483894348145
Epoch 970, val loss: 1.497555136680603
Epoch 980, training loss: 0.010272081010043621 = 0.0032196189276874065 + 0.001 * 7.052461624145508
Epoch 980, val loss: 1.5022048950195312
Epoch 990, training loss: 0.010214364156126976 = 0.0031418618746101856 + 0.001 * 7.072502613067627
Epoch 990, val loss: 1.5067542791366577
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.838165524512388
The final CL Acc:0.82840, 0.01429, The final GNN Acc:0.83711, 0.00149
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11548])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10522])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.949991226196289 = 1.941394329071045 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.926451325416565
Epoch 10, training loss: 1.9395275115966797 = 1.930930733680725 + 0.001 * 8.596769332885742
Epoch 10, val loss: 1.9163089990615845
Epoch 20, training loss: 1.9262471199035645 = 1.917650580406189 + 0.001 * 8.596567153930664
Epoch 20, val loss: 1.9028040170669556
Epoch 30, training loss: 1.9077510833740234 = 1.8991550207138062 + 0.001 * 8.596088409423828
Epoch 30, val loss: 1.8836627006530762
Epoch 40, training loss: 1.8813831806182861 = 1.8727881908416748 + 0.001 * 8.594949722290039
Epoch 40, val loss: 1.8564379215240479
Epoch 50, training loss: 1.846792221069336 = 1.8382002115249634 + 0.001 * 8.591961860656738
Epoch 50, val loss: 1.822713017463684
Epoch 60, training loss: 1.8112621307373047 = 1.8026796579360962 + 0.001 * 8.582422256469727
Epoch 60, val loss: 1.7933269739151
Epoch 70, training loss: 1.7792532444000244 = 1.7707087993621826 + 0.001 * 8.544455528259277
Epoch 70, val loss: 1.7716504335403442
Epoch 80, training loss: 1.7382310628890991 = 1.7299082279205322 + 0.001 * 8.32287311553955
Epoch 80, val loss: 1.7402044534683228
Epoch 90, training loss: 1.680643916130066 = 1.6725914478302002 + 0.001 * 8.052523612976074
Epoch 90, val loss: 1.6906009912490845
Epoch 100, training loss: 1.6037126779556274 = 1.5958082675933838 + 0.001 * 7.9043707847595215
Epoch 100, val loss: 1.6242144107818604
Epoch 110, training loss: 1.5134648084640503 = 1.5057826042175293 + 0.001 * 7.682251930236816
Epoch 110, val loss: 1.5510716438293457
Epoch 120, training loss: 1.4177465438842773 = 1.4102014303207397 + 0.001 * 7.5450758934021
Epoch 120, val loss: 1.4768996238708496
Epoch 130, training loss: 1.3198598623275757 = 1.3123509883880615 + 0.001 * 7.508906364440918
Epoch 130, val loss: 1.40478515625
Epoch 140, training loss: 1.2186588048934937 = 1.2111921310424805 + 0.001 * 7.466675758361816
Epoch 140, val loss: 1.3311384916305542
Epoch 150, training loss: 1.113909125328064 = 1.1064563989639282 + 0.001 * 7.452774524688721
Epoch 150, val loss: 1.2546532154083252
Epoch 160, training loss: 1.0090528726577759 = 1.001608967781067 + 0.001 * 7.443954944610596
Epoch 160, val loss: 1.1782097816467285
Epoch 170, training loss: 0.909467875957489 = 0.9020337462425232 + 0.001 * 7.434106349945068
Epoch 170, val loss: 1.1067091226577759
Epoch 180, training loss: 0.8194794058799744 = 0.8120564818382263 + 0.001 * 7.422911167144775
Epoch 180, val loss: 1.0438779592514038
Epoch 190, training loss: 0.7408265471458435 = 0.7334179878234863 + 0.001 * 7.408565044403076
Epoch 190, val loss: 0.9912343621253967
Epoch 200, training loss: 0.6722072958946228 = 0.6648149490356445 + 0.001 * 7.392319202423096
Epoch 200, val loss: 0.9478391408920288
Epoch 210, training loss: 0.6104646325111389 = 0.6030892133712769 + 0.001 * 7.375439643859863
Epoch 210, val loss: 0.9118761420249939
Epoch 220, training loss: 0.5526461601257324 = 0.545286238193512 + 0.001 * 7.359917163848877
Epoch 220, val loss: 0.8814363479614258
Epoch 230, training loss: 0.49724531173706055 = 0.4898991286754608 + 0.001 * 7.3461689949035645
Epoch 230, val loss: 0.8556613922119141
Epoch 240, training loss: 0.44416698813438416 = 0.43683475255966187 + 0.001 * 7.332235336303711
Epoch 240, val loss: 0.8343167901039124
Epoch 250, training loss: 0.3942406177520752 = 0.3869220018386841 + 0.001 * 7.3186140060424805
Epoch 250, val loss: 0.8173763751983643
Epoch 260, training loss: 0.3484770357608795 = 0.3411738872528076 + 0.001 * 7.303156852722168
Epoch 260, val loss: 0.8048102855682373
Epoch 270, training loss: 0.3073824644088745 = 0.30009695887565613 + 0.001 * 7.285515785217285
Epoch 270, val loss: 0.796448826789856
Epoch 280, training loss: 0.27076342701911926 = 0.2634895443916321 + 0.001 * 7.273890495300293
Epoch 280, val loss: 0.7916746735572815
Epoch 290, training loss: 0.23794521391391754 = 0.2306831330060959 + 0.001 * 7.262082099914551
Epoch 290, val loss: 0.78971266746521
Epoch 300, training loss: 0.20833894610404968 = 0.20108823478221893 + 0.001 * 7.250707626342773
Epoch 300, val loss: 0.7899703979492188
Epoch 310, training loss: 0.181743323802948 = 0.1744956523180008 + 0.001 * 7.247664451599121
Epoch 310, val loss: 0.7922703623771667
Epoch 320, training loss: 0.1581840068101883 = 0.15093950927257538 + 0.001 * 7.244503498077393
Epoch 320, val loss: 0.7965790629386902
Epoch 330, training loss: 0.1377096027135849 = 0.13046637177467346 + 0.001 * 7.243233680725098
Epoch 330, val loss: 0.8027516007423401
Epoch 340, training loss: 0.1201733872294426 = 0.11293108761310577 + 0.001 * 7.2422990798950195
Epoch 340, val loss: 0.8105895519256592
Epoch 350, training loss: 0.10525795072317123 = 0.09801825881004333 + 0.001 * 7.239694595336914
Epoch 350, val loss: 0.8197864294052124
Epoch 360, training loss: 0.0926106721162796 = 0.08537158370018005 + 0.001 * 7.239091873168945
Epoch 360, val loss: 0.829998254776001
Epoch 370, training loss: 0.0818978026509285 = 0.07465916872024536 + 0.001 * 7.238636493682861
Epoch 370, val loss: 0.8409278392791748
Epoch 380, training loss: 0.07282082736492157 = 0.06558170914649963 + 0.001 * 7.2391204833984375
Epoch 380, val loss: 0.8523393273353577
Epoch 390, training loss: 0.06511077284812927 = 0.05787178874015808 + 0.001 * 7.238983631134033
Epoch 390, val loss: 0.8640806674957275
Epoch 400, training loss: 0.05854147672653198 = 0.05130407586693764 + 0.001 * 7.237401008605957
Epoch 400, val loss: 0.8760281801223755
Epoch 410, training loss: 0.05292592942714691 = 0.04568932205438614 + 0.001 * 7.236608028411865
Epoch 410, val loss: 0.8880550861358643
Epoch 420, training loss: 0.04810766130685806 = 0.04087124019861221 + 0.001 * 7.236422538757324
Epoch 420, val loss: 0.9000373482704163
Epoch 430, training loss: 0.04395517334342003 = 0.036720480769872665 + 0.001 * 7.2346906661987305
Epoch 430, val loss: 0.9119151830673218
Epoch 440, training loss: 0.04036344215273857 = 0.03313015028834343 + 0.001 * 7.2332916259765625
Epoch 440, val loss: 0.9236364960670471
Epoch 450, training loss: 0.037244509905576706 = 0.030011925846338272 + 0.001 * 7.232583522796631
Epoch 450, val loss: 0.9351286292076111
Epoch 460, training loss: 0.03452947735786438 = 0.02729208953678608 + 0.001 * 7.2373881340026855
Epoch 460, val loss: 0.9463825821876526
Epoch 470, training loss: 0.032140474766492844 = 0.02491062320768833 + 0.001 * 7.2298502922058105
Epoch 470, val loss: 0.9573456645011902
Epoch 480, training loss: 0.030045412480831146 = 0.02281721867620945 + 0.001 * 7.228193759918213
Epoch 480, val loss: 0.9680067300796509
Epoch 490, training loss: 0.028200626373291016 = 0.02096940204501152 + 0.001 * 7.231224536895752
Epoch 490, val loss: 0.9784052968025208
Epoch 500, training loss: 0.02656320482492447 = 0.019332295283675194 + 0.001 * 7.230909824371338
Epoch 500, val loss: 0.9884989261627197
Epoch 510, training loss: 0.025102509185671806 = 0.017876796424388885 + 0.001 * 7.225712776184082
Epoch 510, val loss: 0.9983147382736206
Epoch 520, training loss: 0.0238051638007164 = 0.016578281298279762 + 0.001 * 7.22688102722168
Epoch 520, val loss: 1.0078352689743042
Epoch 530, training loss: 0.022641411051154137 = 0.015416004694998264 + 0.001 * 7.225405693054199
Epoch 530, val loss: 1.0170912742614746
Epoch 540, training loss: 0.021595217287540436 = 0.01437222957611084 + 0.001 * 7.222987174987793
Epoch 540, val loss: 1.0260789394378662
Epoch 550, training loss: 0.020656537264585495 = 0.013431916013360023 + 0.001 * 7.2246198654174805
Epoch 550, val loss: 1.0348073244094849
Epoch 560, training loss: 0.019808096811175346 = 0.0125823263078928 + 0.001 * 7.225769519805908
Epoch 560, val loss: 1.0432727336883545
Epoch 570, training loss: 0.01903531886637211 = 0.011812398210167885 + 0.001 * 7.222920894622803
Epoch 570, val loss: 1.0515077114105225
Epoch 580, training loss: 0.018337737768888474 = 0.011112698353827 + 0.001 * 7.225038528442383
Epoch 580, val loss: 1.0594873428344727
Epoch 590, training loss: 0.017693951725959778 = 0.010475206188857555 + 0.001 * 7.21874475479126
Epoch 590, val loss: 1.0672564506530762
Epoch 600, training loss: 0.01711292751133442 = 0.009892838075757027 + 0.001 * 7.220088481903076
Epoch 600, val loss: 1.0747909545898438
Epoch 610, training loss: 0.01657325029373169 = 0.009359591640532017 + 0.001 * 7.213659286499023
Epoch 610, val loss: 1.0820859670639038
Epoch 620, training loss: 0.01612153835594654 = 0.008870171383023262 + 0.001 * 7.25136661529541
Epoch 620, val loss: 1.0891915559768677
Epoch 630, training loss: 0.015633583068847656 = 0.008420011959969997 + 0.001 * 7.213571548461914
Epoch 630, val loss: 1.0960915088653564
Epoch 640, training loss: 0.01521259918808937 = 0.008005082607269287 + 0.001 * 7.207515716552734
Epoch 640, val loss: 1.1027973890304565
Epoch 650, training loss: 0.014833923429250717 = 0.007621826604008675 + 0.001 * 7.21209716796875
Epoch 650, val loss: 1.1093326807022095
Epoch 660, training loss: 0.01447354070842266 = 0.00726716686040163 + 0.001 * 7.20637321472168
Epoch 660, val loss: 1.1156845092773438
Epoch 670, training loss: 0.014146778732538223 = 0.006938323378562927 + 0.001 * 7.208454608917236
Epoch 670, val loss: 1.121875524520874
Epoch 680, training loss: 0.013839732855558395 = 0.006632882170379162 + 0.001 * 7.206851005554199
Epoch 680, val loss: 1.1278982162475586
Epoch 690, training loss: 0.013554951176047325 = 0.006348683964461088 + 0.001 * 7.206266403198242
Epoch 690, val loss: 1.1337636709213257
Epoch 700, training loss: 0.013284601271152496 = 0.006083815358579159 + 0.001 * 7.200785160064697
Epoch 700, val loss: 1.1394834518432617
Epoch 710, training loss: 0.013032336719334126 = 0.0058365510776638985 + 0.001 * 7.1957855224609375
Epoch 710, val loss: 1.1450526714324951
Epoch 720, training loss: 0.012796148657798767 = 0.0056053828448057175 + 0.001 * 7.190764904022217
Epoch 720, val loss: 1.1504833698272705
Epoch 730, training loss: 0.012592748738825321 = 0.005388977006077766 + 0.001 * 7.203771591186523
Epoch 730, val loss: 1.155777931213379
Epoch 740, training loss: 0.012379242107272148 = 0.005186073482036591 + 0.001 * 7.193168640136719
Epoch 740, val loss: 1.1609382629394531
Epoch 750, training loss: 0.012186340987682343 = 0.0049955821596086025 + 0.001 * 7.190758228302002
Epoch 750, val loss: 1.1659760475158691
Epoch 760, training loss: 0.012005968019366264 = 0.004816468805074692 + 0.001 * 7.1894989013671875
Epoch 760, val loss: 1.170893907546997
Epoch 770, training loss: 0.01185693871229887 = 0.0046478817239403725 + 0.001 * 7.209056854248047
Epoch 770, val loss: 1.1756967306137085
Epoch 780, training loss: 0.01166765671223402 = 0.00448897760361433 + 0.001 * 7.1786789894104
Epoch 780, val loss: 1.1803654432296753
Epoch 790, training loss: 0.011532259173691273 = 0.004339062608778477 + 0.001 * 7.1931962966918945
Epoch 790, val loss: 1.1849498748779297
Epoch 800, training loss: 0.011372015811502934 = 0.004197418689727783 + 0.001 * 7.174596786499023
Epoch 800, val loss: 1.1894099712371826
Epoch 810, training loss: 0.011236305348575115 = 0.004063364118337631 + 0.001 * 7.172940731048584
Epoch 810, val loss: 1.1937904357910156
Epoch 820, training loss: 0.011108681559562683 = 0.003936157561838627 + 0.001 * 7.1725239753723145
Epoch 820, val loss: 1.1980817317962646
Epoch 830, training loss: 0.010990981012582779 = 0.003814960364252329 + 0.001 * 7.176020622253418
Epoch 830, val loss: 1.2023216485977173
Epoch 840, training loss: 0.010870173573493958 = 0.0036987813655287027 + 0.001 * 7.171391487121582
Epoch 840, val loss: 1.2065409421920776
Epoch 850, training loss: 0.01076568104326725 = 0.0035867972765117884 + 0.001 * 7.178883075714111
Epoch 850, val loss: 1.2107752561569214
Epoch 860, training loss: 0.010671433061361313 = 0.003478636033833027 + 0.001 * 7.1927971839904785
Epoch 860, val loss: 1.2150214910507202
Epoch 870, training loss: 0.010548785328865051 = 0.003374204970896244 + 0.001 * 7.174580097198486
Epoch 870, val loss: 1.219326376914978
Epoch 880, training loss: 0.010422691702842712 = 0.003273374168202281 + 0.001 * 7.149317264556885
Epoch 880, val loss: 1.2236493825912476
Epoch 890, training loss: 0.010352633893489838 = 0.0031762239523231983 + 0.001 * 7.17641019821167
Epoch 890, val loss: 1.2280025482177734
Epoch 900, training loss: 0.010229958221316338 = 0.003082909155637026 + 0.001 * 7.147048473358154
Epoch 900, val loss: 1.2323678731918335
Epoch 910, training loss: 0.010196086019277573 = 0.0029933892656117678 + 0.001 * 7.202696323394775
Epoch 910, val loss: 1.2367192506790161
Epoch 920, training loss: 0.010044505819678307 = 0.0029076961800456047 + 0.001 * 7.1368088722229
Epoch 920, val loss: 1.2410296201705933
Epoch 930, training loss: 0.010000579990446568 = 0.0028256692457944155 + 0.001 * 7.174910545349121
Epoch 930, val loss: 1.2453203201293945
Epoch 940, training loss: 0.009869986213743687 = 0.002747251419350505 + 0.001 * 7.122734546661377
Epoch 940, val loss: 1.249554991722107
Epoch 950, training loss: 0.009822683408856392 = 0.0026723204646259546 + 0.001 * 7.150362968444824
Epoch 950, val loss: 1.2537565231323242
Epoch 960, training loss: 0.00977039523422718 = 0.0026007192209362984 + 0.001 * 7.169676303863525
Epoch 960, val loss: 1.2578785419464111
Epoch 970, training loss: 0.009668055921792984 = 0.0025323620066046715 + 0.001 * 7.135693073272705
Epoch 970, val loss: 1.261942982673645
Epoch 980, training loss: 0.009660914540290833 = 0.0024670669808983803 + 0.001 * 7.19384765625
Epoch 980, val loss: 1.2659273147583008
Epoch 990, training loss: 0.009524447843432426 = 0.0024047107435762882 + 0.001 * 7.119736194610596
Epoch 990, val loss: 1.2698140144348145
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8017923036373221
=== training gcn model ===
Epoch 0, training loss: 1.9594231843948364 = 1.9508264064788818 + 0.001 * 8.596793174743652
Epoch 0, val loss: 1.9521199464797974
Epoch 10, training loss: 1.9497790336608887 = 1.9411823749542236 + 0.001 * 8.59671688079834
Epoch 10, val loss: 1.9429701566696167
Epoch 20, training loss: 1.937616229057312 = 1.929019808769226 + 0.001 * 8.596477508544922
Epoch 20, val loss: 1.9309422969818115
Epoch 30, training loss: 1.9199984073638916 = 1.9114024639129639 + 0.001 * 8.595885276794434
Epoch 30, val loss: 1.9130226373672485
Epoch 40, training loss: 1.8936342000961304 = 1.8850398063659668 + 0.001 * 8.594343185424805
Epoch 40, val loss: 1.8861480951309204
Epoch 50, training loss: 1.8572015762329102 = 1.8486119508743286 + 0.001 * 8.589627265930176
Epoch 50, val loss: 1.8502522706985474
Epoch 60, training loss: 1.818678855895996 = 1.8101061582565308 + 0.001 * 8.572702407836914
Epoch 60, val loss: 1.8158656358718872
Epoch 70, training loss: 1.7883949279785156 = 1.779898762702942 + 0.001 * 8.496197700500488
Epoch 70, val loss: 1.7908234596252441
Epoch 80, training loss: 1.7518932819366455 = 1.743754506111145 + 0.001 * 8.13874340057373
Epoch 80, val loss: 1.7586050033569336
Epoch 90, training loss: 1.7019598484039307 = 1.6939525604248047 + 0.001 * 8.007244110107422
Epoch 90, val loss: 1.7148184776306152
Epoch 100, training loss: 1.6335060596466064 = 1.625631332397461 + 0.001 * 7.874751091003418
Epoch 100, val loss: 1.656923770904541
Epoch 110, training loss: 1.5502220392227173 = 1.5425046682357788 + 0.001 * 7.717368125915527
Epoch 110, val loss: 1.590346336364746
Epoch 120, training loss: 1.4613680839538574 = 1.4538040161132812 + 0.001 * 7.564060688018799
Epoch 120, val loss: 1.5221996307373047
Epoch 130, training loss: 1.3736697435379028 = 1.3661469221115112 + 0.001 * 7.522879600524902
Epoch 130, val loss: 1.4563673734664917
Epoch 140, training loss: 1.2867196798324585 = 1.2792502641677856 + 0.001 * 7.469381809234619
Epoch 140, val loss: 1.3930634260177612
Epoch 150, training loss: 1.200395941734314 = 1.19296395778656 + 0.001 * 7.431957721710205
Epoch 150, val loss: 1.3307276964187622
Epoch 160, training loss: 1.1174439191818237 = 1.1100482940673828 + 0.001 * 7.395641803741455
Epoch 160, val loss: 1.2724367380142212
Epoch 170, training loss: 1.0410890579223633 = 1.033738136291504 + 0.001 * 7.350947856903076
Epoch 170, val loss: 1.2208757400512695
Epoch 180, training loss: 0.9721788167953491 = 0.9648607969284058 + 0.001 * 7.317995548248291
Epoch 180, val loss: 1.1768933534622192
Epoch 190, training loss: 0.9090519547462463 = 0.9017475843429565 + 0.001 * 7.3043975830078125
Epoch 190, val loss: 1.139240026473999
Epoch 200, training loss: 0.8493995666503906 = 0.8421022295951843 + 0.001 * 7.297352313995361
Epoch 200, val loss: 1.1061220169067383
Epoch 210, training loss: 0.7912976741790771 = 0.7840107083320618 + 0.001 * 7.2869672775268555
Epoch 210, val loss: 1.0760999917984009
Epoch 220, training loss: 0.7329332232475281 = 0.7256624102592468 + 0.001 * 7.270821571350098
Epoch 220, val loss: 1.0473746061325073
Epoch 230, training loss: 0.6726715564727783 = 0.6654268503189087 + 0.001 * 7.244679927825928
Epoch 230, val loss: 1.0182595252990723
Epoch 240, training loss: 0.6101102232933044 = 0.6028999090194702 + 0.001 * 7.2103142738342285
Epoch 240, val loss: 0.9885468482971191
Epoch 250, training loss: 0.5467004776000977 = 0.5395177006721497 + 0.001 * 7.182799339294434
Epoch 250, val loss: 0.9598343372344971
Epoch 260, training loss: 0.4848441779613495 = 0.47768059372901917 + 0.001 * 7.163592338562012
Epoch 260, val loss: 0.9350903034210205
Epoch 270, training loss: 0.4266927242279053 = 0.4195426106452942 + 0.001 * 7.150120735168457
Epoch 270, val loss: 0.9165836572647095
Epoch 280, training loss: 0.3734927177429199 = 0.36634561419487 + 0.001 * 7.1471076011657715
Epoch 280, val loss: 0.9045577049255371
Epoch 290, training loss: 0.3255493938922882 = 0.3184073865413666 + 0.001 * 7.142012596130371
Epoch 290, val loss: 0.8978357911109924
Epoch 300, training loss: 0.2826548218727112 = 0.27551373839378357 + 0.001 * 7.141082763671875
Epoch 300, val loss: 0.8950672149658203
Epoch 310, training loss: 0.24455322325229645 = 0.2374122589826584 + 0.001 * 7.140968322753906
Epoch 310, val loss: 0.895479142665863
Epoch 320, training loss: 0.21115154027938843 = 0.20401135087013245 + 0.001 * 7.140191555023193
Epoch 320, val loss: 0.8987301588058472
Epoch 330, training loss: 0.18236994743347168 = 0.1752275973558426 + 0.001 * 7.142354488372803
Epoch 330, val loss: 0.9044281840324402
Epoch 340, training loss: 0.15794533491134644 = 0.15080378949642181 + 0.001 * 7.141539573669434
Epoch 340, val loss: 0.9122477769851685
Epoch 350, training loss: 0.13741204142570496 = 0.13027246296405792 + 0.001 * 7.139578342437744
Epoch 350, val loss: 0.9220377802848816
Epoch 360, training loss: 0.12018781155347824 = 0.113048255443573 + 0.001 * 7.139557361602783
Epoch 360, val loss: 0.9334843158721924
Epoch 370, training loss: 0.10569114238023758 = 0.09855180233716965 + 0.001 * 7.139337539672852
Epoch 370, val loss: 0.9462003707885742
Epoch 380, training loss: 0.09342645853757858 = 0.0862865075469017 + 0.001 * 7.13994836807251
Epoch 380, val loss: 0.9598336815834045
Epoch 390, training loss: 0.08298290520906448 = 0.075844407081604 + 0.001 * 7.138498783111572
Epoch 390, val loss: 0.9741073250770569
Epoch 400, training loss: 0.07403749227523804 = 0.06690285354852676 + 0.001 * 7.13463830947876
Epoch 400, val loss: 0.988865852355957
Epoch 410, training loss: 0.06635251641273499 = 0.05921497568488121 + 0.001 * 7.137539386749268
Epoch 410, val loss: 1.003918170928955
Epoch 420, training loss: 0.0597183033823967 = 0.05258830636739731 + 0.001 * 7.1299967765808105
Epoch 420, val loss: 1.019166350364685
Epoch 430, training loss: 0.0540006086230278 = 0.046869345009326935 + 0.001 * 7.131263256072998
Epoch 430, val loss: 1.0344911813735962
Epoch 440, training loss: 0.04905400425195694 = 0.041929278522729874 + 0.001 * 7.124727249145508
Epoch 440, val loss: 1.0498077869415283
Epoch 450, training loss: 0.0447709746658802 = 0.037655048072338104 + 0.001 * 7.115926742553711
Epoch 450, val loss: 1.0650213956832886
Epoch 460, training loss: 0.04106574505567551 = 0.033951062709093094 + 0.001 * 7.114682674407959
Epoch 460, val loss: 1.0800769329071045
Epoch 470, training loss: 0.037842851132154465 = 0.03073304519057274 + 0.001 * 7.109807014465332
Epoch 470, val loss: 1.0949153900146484
Epoch 480, training loss: 0.035037294030189514 = 0.027929017320275307 + 0.001 * 7.1082763671875
Epoch 480, val loss: 1.1094465255737305
Epoch 490, training loss: 0.032571785151958466 = 0.025478199124336243 + 0.001 * 7.093585968017578
Epoch 490, val loss: 1.1236321926116943
Epoch 500, training loss: 0.030446432530879974 = 0.023328015580773354 + 0.001 * 7.1184163093566895
Epoch 500, val loss: 1.1374683380126953
Epoch 510, training loss: 0.028533004224300385 = 0.021434461697936058 + 0.001 * 7.098541736602783
Epoch 510, val loss: 1.1509217023849487
Epoch 520, training loss: 0.026918567717075348 = 0.01976034604012966 + 0.001 * 7.158220291137695
Epoch 520, val loss: 1.1639662981033325
Epoch 530, training loss: 0.025347497314214706 = 0.01827189326286316 + 0.001 * 7.07560396194458
Epoch 530, val loss: 1.1766302585601807
Epoch 540, training loss: 0.024020208045840263 = 0.016941837966442108 + 0.001 * 7.078369140625
Epoch 540, val loss: 1.1888911724090576
Epoch 550, training loss: 0.022867005318403244 = 0.01574590802192688 + 0.001 * 7.121098041534424
Epoch 550, val loss: 1.2008137702941895
Epoch 560, training loss: 0.021744143217802048 = 0.014665878377854824 + 0.001 * 7.07826566696167
Epoch 560, val loss: 1.2123967409133911
Epoch 570, training loss: 0.020752007141709328 = 0.013687734492123127 + 0.001 * 7.064272880554199
Epoch 570, val loss: 1.223665475845337
Epoch 580, training loss: 0.019858304411172867 = 0.012800080701708794 + 0.001 * 7.058224201202393
Epoch 580, val loss: 1.234606146812439
Epoch 590, training loss: 0.019128065556287766 = 0.011993139050900936 + 0.001 * 7.1349263191223145
Epoch 590, val loss: 1.245245337486267
Epoch 600, training loss: 0.018325526267290115 = 0.011258314363658428 + 0.001 * 7.067211151123047
Epoch 600, val loss: 1.255576252937317
Epoch 610, training loss: 0.017632856965065002 = 0.010587708093225956 + 0.001 * 7.0451483726501465
Epoch 610, val loss: 1.2656129598617554
Epoch 620, training loss: 0.017017856240272522 = 0.009973268955945969 + 0.001 * 7.044587135314941
Epoch 620, val loss: 1.275367021560669
Epoch 630, training loss: 0.01645682565867901 = 0.00940775778144598 + 0.001 * 7.049067497253418
Epoch 630, val loss: 1.2848763465881348
Epoch 640, training loss: 0.01593211479485035 = 0.008885343559086323 + 0.001 * 7.046771049499512
Epoch 640, val loss: 1.29416823387146
Epoch 650, training loss: 0.01544244959950447 = 0.008401093073189259 + 0.001 * 7.041356563568115
Epoch 650, val loss: 1.3032567501068115
Epoch 660, training loss: 0.015007906593382359 = 0.007951873354613781 + 0.001 * 7.056033134460449
Epoch 660, val loss: 1.3121477365493774
Epoch 670, training loss: 0.014577802270650864 = 0.007534793112426996 + 0.001 * 7.043008327484131
Epoch 670, val loss: 1.3208515644073486
Epoch 680, training loss: 0.014185469597578049 = 0.0071473438292741776 + 0.001 * 7.038125991821289
Epoch 680, val loss: 1.32938814163208
Epoch 690, training loss: 0.013832404278218746 = 0.006786972749978304 + 0.001 * 7.045431137084961
Epoch 690, val loss: 1.3377755880355835
Epoch 700, training loss: 0.013500131666660309 = 0.006451153662055731 + 0.001 * 7.048977851867676
Epoch 700, val loss: 1.3460060358047485
Epoch 710, training loss: 0.013165662065148354 = 0.006137480027973652 + 0.001 * 7.028181552886963
Epoch 710, val loss: 1.354101300239563
Epoch 720, training loss: 0.012887208722531796 = 0.00584412831813097 + 0.001 * 7.043079853057861
Epoch 720, val loss: 1.3620671033859253
Epoch 730, training loss: 0.012598613277077675 = 0.005569512955844402 + 0.001 * 7.02910041809082
Epoch 730, val loss: 1.3698889017105103
Epoch 740, training loss: 0.012334800325334072 = 0.0053121475502848625 + 0.001 * 7.022652626037598
Epoch 740, val loss: 1.3775793313980103
Epoch 750, training loss: 0.01209489069879055 = 0.005071060732007027 + 0.001 * 7.023828983306885
Epoch 750, val loss: 1.3851234912872314
Epoch 760, training loss: 0.011883044615387917 = 0.004845145624130964 + 0.001 * 7.037898063659668
Epoch 760, val loss: 1.3925246000289917
Epoch 770, training loss: 0.011677293106913567 = 0.004633605480194092 + 0.001 * 7.04368782043457
Epoch 770, val loss: 1.3997523784637451
Epoch 780, training loss: 0.011451398953795433 = 0.004435565322637558 + 0.001 * 7.015833854675293
Epoch 780, val loss: 1.4068187475204468
Epoch 790, training loss: 0.011273160576820374 = 0.004250022582709789 + 0.001 * 7.023137092590332
Epoch 790, val loss: 1.4137409925460815
Epoch 800, training loss: 0.011104905977845192 = 0.004076148848980665 + 0.001 * 7.028756618499756
Epoch 800, val loss: 1.420505404472351
Epoch 810, training loss: 0.010947883129119873 = 0.003913188353180885 + 0.001 * 7.034694671630859
Epoch 810, val loss: 1.4271247386932373
Epoch 820, training loss: 0.010777600109577179 = 0.0037604179233312607 + 0.001 * 7.017181396484375
Epoch 820, val loss: 1.4335752725601196
Epoch 830, training loss: 0.01061960682272911 = 0.0036170503590255976 + 0.001 * 7.002555847167969
Epoch 830, val loss: 1.4398866891860962
Epoch 840, training loss: 0.01050633005797863 = 0.0034823280293494463 + 0.001 * 7.024001598358154
Epoch 840, val loss: 1.4460580348968506
Epoch 850, training loss: 0.010361084714531898 = 0.0033555864356458187 + 0.001 * 7.00549840927124
Epoch 850, val loss: 1.4520750045776367
Epoch 860, training loss: 0.010270141065120697 = 0.0032358961179852486 + 0.001 * 7.034245014190674
Epoch 860, val loss: 1.4579585790634155
Epoch 870, training loss: 0.010132811963558197 = 0.003123366041108966 + 0.001 * 7.0094451904296875
Epoch 870, val loss: 1.4637519121170044
Epoch 880, training loss: 0.010011233389377594 = 0.00301728630438447 + 0.001 * 6.9939470291137695
Epoch 880, val loss: 1.4693509340286255
Epoch 890, training loss: 0.009904470294713974 = 0.0029171383939683437 + 0.001 * 6.987331390380859
Epoch 890, val loss: 1.4748179912567139
Epoch 900, training loss: 0.009830791503190994 = 0.0028224403504282236 + 0.001 * 7.008350372314453
Epoch 900, val loss: 1.4801727533340454
Epoch 910, training loss: 0.009729905985295773 = 0.002732848981395364 + 0.001 * 6.99705696105957
Epoch 910, val loss: 1.485377550125122
Epoch 920, training loss: 0.009635204449295998 = 0.002648010617122054 + 0.001 * 6.987193584442139
Epoch 920, val loss: 1.490507960319519
Epoch 930, training loss: 0.00954936072230339 = 0.002567544113844633 + 0.001 * 6.981815814971924
Epoch 930, val loss: 1.4955193996429443
Epoch 940, training loss: 0.00949043408036232 = 0.0024910971987992525 + 0.001 * 6.999336242675781
Epoch 940, val loss: 1.500427007675171
Epoch 950, training loss: 0.009433936327695847 = 0.0024184477515518665 + 0.001 * 7.0154876708984375
Epoch 950, val loss: 1.505251169204712
Epoch 960, training loss: 0.009324601851403713 = 0.0023492854088544846 + 0.001 * 6.975316047668457
Epoch 960, val loss: 1.5099595785140991
Epoch 970, training loss: 0.009286919608712196 = 0.002283411333337426 + 0.001 * 7.0035080909729
Epoch 970, val loss: 1.5145989656448364
Epoch 980, training loss: 0.009197946637868881 = 0.0022206539288163185 + 0.001 * 6.977292060852051
Epoch 980, val loss: 1.5191572904586792
Epoch 990, training loss: 0.009145228192210197 = 0.002160838805139065 + 0.001 * 6.984389305114746
Epoch 990, val loss: 1.52359938621521
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 1.949608564376831 = 1.941011667251587 + 0.001 * 8.596837043762207
Epoch 0, val loss: 1.9322274923324585
Epoch 10, training loss: 1.9402906894683838 = 1.9316939115524292 + 0.001 * 8.596795082092285
Epoch 10, val loss: 1.9231011867523193
Epoch 20, training loss: 1.9287770986557007 = 1.9201804399490356 + 0.001 * 8.596632957458496
Epoch 20, val loss: 1.9112637042999268
Epoch 30, training loss: 1.9125522375106812 = 1.9039559364318848 + 0.001 * 8.59626293182373
Epoch 30, val loss: 1.8942545652389526
Epoch 40, training loss: 1.8887273073196411 = 1.8801318407058716 + 0.001 * 8.595410346984863
Epoch 40, val loss: 1.869356632232666
Epoch 50, training loss: 1.855704426765442 = 1.847111463546753 + 0.001 * 8.593021392822266
Epoch 50, val loss: 1.8363319635391235
Epoch 60, training loss: 1.818317174911499 = 1.8097327947616577 + 0.001 * 8.58443832397461
Epoch 60, val loss: 1.8034799098968506
Epoch 70, training loss: 1.7848355770111084 = 1.7762924432754517 + 0.001 * 8.543185234069824
Epoch 70, val loss: 1.7792214155197144
Epoch 80, training loss: 1.7438805103302002 = 1.7356586456298828 + 0.001 * 8.22184944152832
Epoch 80, val loss: 1.747145175933838
Epoch 90, training loss: 1.6863415241241455 = 1.6783993244171143 + 0.001 * 7.942153453826904
Epoch 90, val loss: 1.6980032920837402
Epoch 100, training loss: 1.607133150100708 = 1.599415898323059 + 0.001 * 7.717242240905762
Epoch 100, val loss: 1.6304078102111816
Epoch 110, training loss: 1.5088493824005127 = 1.5012470483779907 + 0.001 * 7.602319717407227
Epoch 110, val loss: 1.5500625371932983
Epoch 120, training loss: 1.399476170539856 = 1.391893744468689 + 0.001 * 7.582460880279541
Epoch 120, val loss: 1.4626792669296265
Epoch 130, training loss: 1.2841808795928955 = 1.2766201496124268 + 0.001 * 7.560712814331055
Epoch 130, val loss: 1.373429536819458
Epoch 140, training loss: 1.1657980680465698 = 1.1582794189453125 + 0.001 * 7.518677711486816
Epoch 140, val loss: 1.2835317850112915
Epoch 150, training loss: 1.048727035522461 = 1.0412800312042236 + 0.001 * 7.446992874145508
Epoch 150, val loss: 1.196555495262146
Epoch 160, training loss: 0.9403107166290283 = 0.9329543709754944 + 0.001 * 7.356335639953613
Epoch 160, val loss: 1.1193363666534424
Epoch 170, training loss: 0.8463738560676575 = 0.8390556573867798 + 0.001 * 7.318201065063477
Epoch 170, val loss: 1.057137131690979
Epoch 180, training loss: 0.7679247260093689 = 0.760606050491333 + 0.001 * 7.318688869476318
Epoch 180, val loss: 1.0106346607208252
Epoch 190, training loss: 0.7021554112434387 = 0.6948444843292236 + 0.001 * 7.310897350311279
Epoch 190, val loss: 0.9769694209098816
Epoch 200, training loss: 0.644936203956604 = 0.6376274228096008 + 0.001 * 7.308806896209717
Epoch 200, val loss: 0.9516240954399109
Epoch 210, training loss: 0.5924462080001831 = 0.5851407647132874 + 0.001 * 7.305429458618164
Epoch 210, val loss: 0.9302077889442444
Epoch 220, training loss: 0.5419214367866516 = 0.5346201062202454 + 0.001 * 7.301304817199707
Epoch 220, val loss: 0.9100030660629272
Epoch 230, training loss: 0.4919101595878601 = 0.48461559414863586 + 0.001 * 7.294555187225342
Epoch 230, val loss: 0.8902517557144165
Epoch 240, training loss: 0.4420755207538605 = 0.43479084968566895 + 0.001 * 7.284677028656006
Epoch 240, val loss: 0.8709549903869629
Epoch 250, training loss: 0.39280229806900024 = 0.385529100894928 + 0.001 * 7.273199558258057
Epoch 250, val loss: 0.8528712391853333
Epoch 260, training loss: 0.34501662850379944 = 0.3377538025379181 + 0.001 * 7.2628350257873535
Epoch 260, val loss: 0.8369394540786743
Epoch 270, training loss: 0.29980987310409546 = 0.29255369305610657 + 0.001 * 7.256183624267578
Epoch 270, val loss: 0.8241439461708069
Epoch 280, training loss: 0.2583020329475403 = 0.2510542869567871 + 0.001 * 7.247738838195801
Epoch 280, val loss: 0.8156862854957581
Epoch 290, training loss: 0.2214043289422989 = 0.21416395902633667 + 0.001 * 7.240371227264404
Epoch 290, val loss: 0.812106728553772
Epoch 300, training loss: 0.18955151736736298 = 0.18231263756752014 + 0.001 * 7.238885402679443
Epoch 300, val loss: 0.813385009765625
Epoch 310, training loss: 0.16260060667991638 = 0.15536275506019592 + 0.001 * 7.237843990325928
Epoch 310, val loss: 0.8191066980361938
Epoch 320, training loss: 0.14003217220306396 = 0.13279591500759125 + 0.001 * 7.236255168914795
Epoch 320, val loss: 0.828339695930481
Epoch 330, training loss: 0.12119407951831818 = 0.11395885795354843 + 0.001 * 7.235219478607178
Epoch 330, val loss: 0.8401771783828735
Epoch 340, training loss: 0.10545963048934937 = 0.09822617471218109 + 0.001 * 7.233456134796143
Epoch 340, val loss: 0.8537067174911499
Epoch 350, training loss: 0.09228606522083282 = 0.08505463600158691 + 0.001 * 7.231426239013672
Epoch 350, val loss: 0.8681643009185791
Epoch 360, training loss: 0.08122372627258301 = 0.07399412989616394 + 0.001 * 7.229595184326172
Epoch 360, val loss: 0.8830772042274475
Epoch 370, training loss: 0.07190253585577011 = 0.06467574834823608 + 0.001 * 7.226784706115723
Epoch 370, val loss: 0.8981038928031921
Epoch 380, training loss: 0.06403175741434097 = 0.05680331215262413 + 0.001 * 7.228447914123535
Epoch 380, val loss: 0.9130155444145203
Epoch 390, training loss: 0.05735549330711365 = 0.050133444368839264 + 0.001 * 7.222049713134766
Epoch 390, val loss: 0.9276785850524902
Epoch 400, training loss: 0.051688529551029205 = 0.044466692954301834 + 0.001 * 7.221835136413574
Epoch 400, val loss: 0.9419798254966736
Epoch 410, training loss: 0.04685578495264053 = 0.03963799029588699 + 0.001 * 7.217794895172119
Epoch 410, val loss: 0.9558555483818054
Epoch 420, training loss: 0.04272235929965973 = 0.03550867363810539 + 0.001 * 7.21368408203125
Epoch 420, val loss: 0.9693265557289124
Epoch 430, training loss: 0.039175376296043396 = 0.031962450593709946 + 0.001 * 7.212924480438232
Epoch 430, val loss: 0.9823787212371826
Epoch 440, training loss: 0.03611525893211365 = 0.02890356257557869 + 0.001 * 7.211696624755859
Epoch 440, val loss: 0.9950206875801086
Epoch 450, training loss: 0.03345908224582672 = 0.02625291235744953 + 0.001 * 7.20617151260376
Epoch 450, val loss: 1.0072370767593384
Epoch 460, training loss: 0.03114294819533825 = 0.02394513227045536 + 0.001 * 7.197815895080566
Epoch 460, val loss: 1.0190553665161133
Epoch 470, training loss: 0.029122192412614822 = 0.02192661538720131 + 0.001 * 7.195576190948486
Epoch 470, val loss: 1.0304827690124512
Epoch 480, training loss: 0.02735261805355549 = 0.020153062418103218 + 0.001 * 7.199554920196533
Epoch 480, val loss: 1.0415388345718384
Epoch 490, training loss: 0.02578394114971161 = 0.018587829545140266 + 0.001 * 7.196111679077148
Epoch 490, val loss: 1.0522445440292358
Epoch 500, training loss: 0.024390172213315964 = 0.017200585454702377 + 0.001 * 7.189587116241455
Epoch 500, val loss: 1.0625922679901123
Epoch 510, training loss: 0.023148924112319946 = 0.015965960919857025 + 0.001 * 7.1829633712768555
Epoch 510, val loss: 1.0726076364517212
Epoch 520, training loss: 0.02203497476875782 = 0.014862923882901669 + 0.001 * 7.1720499992370605
Epoch 520, val loss: 1.0823041200637817
Epoch 530, training loss: 0.021042436361312866 = 0.013873694464564323 + 0.001 * 7.168741703033447
Epoch 530, val loss: 1.0917028188705444
Epoch 540, training loss: 0.020142072811722755 = 0.012983443215489388 + 0.001 * 7.158629894256592
Epoch 540, val loss: 1.100799560546875
Epoch 550, training loss: 0.0193353109061718 = 0.012179587036371231 + 0.001 * 7.155722618103027
Epoch 550, val loss: 1.1096253395080566
Epoch 560, training loss: 0.018601952120661736 = 0.011451350525021553 + 0.001 * 7.150601863861084
Epoch 560, val loss: 1.1181639432907104
Epoch 570, training loss: 0.01795598492026329 = 0.0107896002009511 + 0.001 * 7.166383266448975
Epoch 570, val loss: 1.1264617443084717
Epoch 580, training loss: 0.01733335852622986 = 0.010186496190726757 + 0.001 * 7.146862030029297
Epoch 580, val loss: 1.1345051527023315
Epoch 590, training loss: 0.016782499849796295 = 0.009635327383875847 + 0.001 * 7.147172451019287
Epoch 590, val loss: 1.1422992944717407
Epoch 600, training loss: 0.016291629523038864 = 0.009130354970693588 + 0.001 * 7.161273956298828
Epoch 600, val loss: 1.1498993635177612
Epoch 610, training loss: 0.01580461859703064 = 0.008666619658470154 + 0.001 * 7.137998580932617
Epoch 610, val loss: 1.1572643518447876
Epoch 620, training loss: 0.015371521934866905 = 0.008239633403718472 + 0.001 * 7.131887912750244
Epoch 620, val loss: 1.1644203662872314
Epoch 630, training loss: 0.014973537996411324 = 0.007845614105463028 + 0.001 * 7.127923011779785
Epoch 630, val loss: 1.1713813543319702
Epoch 640, training loss: 0.01466385368257761 = 0.007481270004063845 + 0.001 * 7.182583332061768
Epoch 640, val loss: 1.178171157836914
Epoch 650, training loss: 0.014282001182436943 = 0.007143702823668718 + 0.001 * 7.138298034667969
Epoch 650, val loss: 1.1847375631332397
Epoch 660, training loss: 0.013963655568659306 = 0.006830349564552307 + 0.001 * 7.133305549621582
Epoch 660, val loss: 1.1911485195159912
Epoch 670, training loss: 0.013659212738275528 = 0.006538894027471542 + 0.001 * 7.12031888961792
Epoch 670, val loss: 1.1973683834075928
Epoch 680, training loss: 0.013397187925875187 = 0.006267313379794359 + 0.001 * 7.129874229431152
Epoch 680, val loss: 1.2034339904785156
Epoch 690, training loss: 0.013140300288796425 = 0.006013915408402681 + 0.001 * 7.126384735107422
Epoch 690, val loss: 1.209335446357727
Epoch 700, training loss: 0.012905659154057503 = 0.00577702559530735 + 0.001 * 7.128632545471191
Epoch 700, val loss: 1.215090036392212
Epoch 710, training loss: 0.012671436183154583 = 0.00555532006546855 + 0.001 * 7.116115570068359
Epoch 710, val loss: 1.2207015752792358
Epoch 720, training loss: 0.012467855587601662 = 0.005347453989088535 + 0.001 * 7.120401382446289
Epoch 720, val loss: 1.2261728048324585
Epoch 730, training loss: 0.01225750520825386 = 0.005152272526174784 + 0.001 * 7.105232238769531
Epoch 730, val loss: 1.2314934730529785
Epoch 740, training loss: 0.012100743129849434 = 0.004968761000782251 + 0.001 * 7.131981372833252
Epoch 740, val loss: 1.2367041110992432
Epoch 750, training loss: 0.01189904659986496 = 0.004796061664819717 + 0.001 * 7.102984428405762
Epoch 750, val loss: 1.2417726516723633
Epoch 760, training loss: 0.011735750362277031 = 0.004633300006389618 + 0.001 * 7.102449417114258
Epoch 760, val loss: 1.2467402219772339
Epoch 770, training loss: 0.011585073545575142 = 0.004479713272303343 + 0.001 * 7.105360507965088
Epoch 770, val loss: 1.2515616416931152
Epoch 780, training loss: 0.011431487277150154 = 0.0043345969170331955 + 0.001 * 7.096889495849609
Epoch 780, val loss: 1.2562975883483887
Epoch 790, training loss: 0.011295231059193611 = 0.004197366070002317 + 0.001 * 7.097865104675293
Epoch 790, val loss: 1.2609230279922485
Epoch 800, training loss: 0.011167509481310844 = 0.00406748428940773 + 0.001 * 7.100025177001953
Epoch 800, val loss: 1.2654279470443726
Epoch 810, training loss: 0.01103050634264946 = 0.0039443569257855415 + 0.001 * 7.086149215698242
Epoch 810, val loss: 1.269841194152832
Epoch 820, training loss: 0.010922634974122047 = 0.0038275625556707382 + 0.001 * 7.095071792602539
Epoch 820, val loss: 1.2741575241088867
Epoch 830, training loss: 0.010825317353010178 = 0.0037166653200984 + 0.001 * 7.108652114868164
Epoch 830, val loss: 1.2783803939819336
Epoch 840, training loss: 0.010715664364397526 = 0.003611252410337329 + 0.001 * 7.104412078857422
Epoch 840, val loss: 1.2824969291687012
Epoch 850, training loss: 0.01060334499925375 = 0.0035110395401716232 + 0.001 * 7.0923051834106445
Epoch 850, val loss: 1.2865352630615234
Epoch 860, training loss: 0.010525290854275227 = 0.0034156031906604767 + 0.001 * 7.109687328338623
Epoch 860, val loss: 1.2904894351959229
Epoch 870, training loss: 0.010401008650660515 = 0.0033247130922973156 + 0.001 * 7.076295852661133
Epoch 870, val loss: 1.294336199760437
Epoch 880, training loss: 0.010309462435543537 = 0.003238034900277853 + 0.001 * 7.071427345275879
Epoch 880, val loss: 1.298126220703125
Epoch 890, training loss: 0.010228192433714867 = 0.0031553246080875397 + 0.001 * 7.072866916656494
Epoch 890, val loss: 1.3018373250961304
Epoch 900, training loss: 0.010151633992791176 = 0.0030763272661715746 + 0.001 * 7.075306415557861
Epoch 900, val loss: 1.3054758310317993
Epoch 910, training loss: 0.010074392892420292 = 0.00300084357149899 + 0.001 * 7.073549270629883
Epoch 910, val loss: 1.309024691581726
Epoch 920, training loss: 0.00999382697045803 = 0.0029286560602486134 + 0.001 * 7.065170764923096
Epoch 920, val loss: 1.3125156164169312
Epoch 930, training loss: 0.009962488897144794 = 0.002859586151316762 + 0.001 * 7.102902412414551
Epoch 930, val loss: 1.3159301280975342
Epoch 940, training loss: 0.009854134172201157 = 0.0027934210374951363 + 0.001 * 7.060713291168213
Epoch 940, val loss: 1.3192708492279053
Epoch 950, training loss: 0.00978868082165718 = 0.0027300529181957245 + 0.001 * 7.058627605438232
Epoch 950, val loss: 1.322558045387268
Epoch 960, training loss: 0.00974195171147585 = 0.002669287147000432 + 0.001 * 7.0726637840271
Epoch 960, val loss: 1.3257532119750977
Epoch 970, training loss: 0.009666706435382366 = 0.002610988449305296 + 0.001 * 7.055717468261719
Epoch 970, val loss: 1.3289074897766113
Epoch 980, training loss: 0.009610089473426342 = 0.002555000362917781 + 0.001 * 7.055088520050049
Epoch 980, val loss: 1.3319898843765259
Epoch 990, training loss: 0.00956192146986723 = 0.0025011340621858835 + 0.001 * 7.060786724090576
Epoch 990, val loss: 1.3350332975387573
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.7981022667369532
The final CL Acc:0.76543, 0.00873, The final GNN Acc:0.80197, 0.00323
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13222])
remove edge: torch.Size([2, 7980])
updated graph: torch.Size([2, 10646])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9440635442733765 = 1.9354667663574219 + 0.001 * 8.59681510925293
Epoch 0, val loss: 1.9318732023239136
Epoch 10, training loss: 1.9336711168289185 = 1.9250743389129639 + 0.001 * 8.596719741821289
Epoch 10, val loss: 1.9221464395523071
Epoch 20, training loss: 1.9207814931869507 = 1.9121849536895752 + 0.001 * 8.596485137939453
Epoch 20, val loss: 1.9097973108291626
Epoch 30, training loss: 1.903064489364624 = 1.8944685459136963 + 0.001 * 8.595929145812988
Epoch 30, val loss: 1.8927241563796997
Epoch 40, training loss: 1.8777761459350586 = 1.869181752204895 + 0.001 * 8.594420433044434
Epoch 40, val loss: 1.8687139749526978
Epoch 50, training loss: 1.8438156843185425 = 1.8352264165878296 + 0.001 * 8.589238166809082
Epoch 50, val loss: 1.8381301164627075
Epoch 60, training loss: 1.8061145544052124 = 1.797548532485962 + 0.001 * 8.566043853759766
Epoch 60, val loss: 1.8071786165237427
Epoch 70, training loss: 1.7694311141967773 = 1.7609864473342896 + 0.001 * 8.444690704345703
Epoch 70, val loss: 1.775936245918274
Epoch 80, training loss: 1.719188928604126 = 1.711163878440857 + 0.001 * 8.025065422058105
Epoch 80, val loss: 1.72769033908844
Epoch 90, training loss: 1.648052453994751 = 1.6401208639144897 + 0.001 * 7.931607246398926
Epoch 90, val loss: 1.6625109910964966
Epoch 100, training loss: 1.5580134391784668 = 1.5501264333724976 + 0.001 * 7.887032985687256
Epoch 100, val loss: 1.5862367153167725
Epoch 110, training loss: 1.461950421333313 = 1.4541348218917847 + 0.001 * 7.8155598640441895
Epoch 110, val loss: 1.505121111869812
Epoch 120, training loss: 1.3708568811416626 = 1.363203525543213 + 0.001 * 7.653332233428955
Epoch 120, val loss: 1.4300259351730347
Epoch 130, training loss: 1.2863354682922363 = 1.2787998914718628 + 0.001 * 7.535540580749512
Epoch 130, val loss: 1.3619896173477173
Epoch 140, training loss: 1.2068737745285034 = 1.1993526220321655 + 0.001 * 7.521163463592529
Epoch 140, val loss: 1.2995741367340088
Epoch 150, training loss: 1.1288264989852905 = 1.1213299036026 + 0.001 * 7.496553897857666
Epoch 150, val loss: 1.2397130727767944
Epoch 160, training loss: 1.048447608947754 = 1.0409724712371826 + 0.001 * 7.475079536437988
Epoch 160, val loss: 1.1780375242233276
Epoch 170, training loss: 0.9644536375999451 = 0.9570064544677734 + 0.001 * 7.4471755027771
Epoch 170, val loss: 1.1126033067703247
Epoch 180, training loss: 0.8789503574371338 = 0.8715499639511108 + 0.001 * 7.400402545928955
Epoch 180, val loss: 1.0459067821502686
Epoch 190, training loss: 0.7960081100463867 = 0.7886800169944763 + 0.001 * 7.328091144561768
Epoch 190, val loss: 0.981956422328949
Epoch 200, training loss: 0.7197180390357971 = 0.712433397769928 + 0.001 * 7.284623622894287
Epoch 200, val loss: 0.9254205822944641
Epoch 210, training loss: 0.6524116396903992 = 0.6451288461685181 + 0.001 * 7.282778263092041
Epoch 210, val loss: 0.8792598843574524
Epoch 220, training loss: 0.5941277742385864 = 0.5868489742279053 + 0.001 * 7.278826713562012
Epoch 220, val loss: 0.8443600535392761
Epoch 230, training loss: 0.5433827042579651 = 0.5361060500144958 + 0.001 * 7.276669979095459
Epoch 230, val loss: 0.8196629285812378
Epoch 240, training loss: 0.49820247292518616 = 0.49092820286750793 + 0.001 * 7.274259567260742
Epoch 240, val loss: 0.802993893623352
Epoch 250, training loss: 0.45681771636009216 = 0.44954419136047363 + 0.001 * 7.273528099060059
Epoch 250, val loss: 0.7921252250671387
Epoch 260, training loss: 0.41794803738594055 = 0.41067785024642944 + 0.001 * 7.27017879486084
Epoch 260, val loss: 0.7852510213851929
Epoch 270, training loss: 0.38090917468070984 = 0.37364161014556885 + 0.001 * 7.267576694488525
Epoch 270, val loss: 0.7811363935470581
Epoch 280, training loss: 0.34548819065093994 = 0.3382239043712616 + 0.001 * 7.26428747177124
Epoch 280, val loss: 0.7791717052459717
Epoch 290, training loss: 0.3117727041244507 = 0.3045128881931305 + 0.001 * 7.259819030761719
Epoch 290, val loss: 0.7789969444274902
Epoch 300, training loss: 0.27997255325317383 = 0.272719144821167 + 0.001 * 7.253421306610107
Epoch 300, val loss: 0.780738353729248
Epoch 310, training loss: 0.2503827214241028 = 0.24313679337501526 + 0.001 * 7.245927810668945
Epoch 310, val loss: 0.7848701477050781
Epoch 320, training loss: 0.2233061045408249 = 0.21607288718223572 + 0.001 * 7.23321533203125
Epoch 320, val loss: 0.791828989982605
Epoch 330, training loss: 0.1989433467388153 = 0.19172193109989166 + 0.001 * 7.2214155197143555
Epoch 330, val loss: 0.8018158078193665
Epoch 340, training loss: 0.17729805409908295 = 0.17009225487709045 + 0.001 * 7.2058000564575195
Epoch 340, val loss: 0.8146250247955322
Epoch 350, training loss: 0.15822109580039978 = 0.15102937817573547 + 0.001 * 7.191719055175781
Epoch 350, val loss: 0.829668402671814
Epoch 360, training loss: 0.14147305488586426 = 0.13430380821228027 + 0.001 * 7.169246673583984
Epoch 360, val loss: 0.8465927243232727
Epoch 370, training loss: 0.12682335078716278 = 0.11965624988079071 + 0.001 * 7.167099475860596
Epoch 370, val loss: 0.8648894429206848
Epoch 380, training loss: 0.11397834122180939 = 0.10682722181081772 + 0.001 * 7.151115894317627
Epoch 380, val loss: 0.8840868473052979
Epoch 390, training loss: 0.1027129590511322 = 0.09557368606328964 + 0.001 * 7.139276027679443
Epoch 390, val loss: 0.9040080904960632
Epoch 400, training loss: 0.09280981123447418 = 0.08567701280117035 + 0.001 * 7.132800579071045
Epoch 400, val loss: 0.9244611859321594
Epoch 410, training loss: 0.08406512439250946 = 0.07694775611162186 + 0.001 * 7.117364883422852
Epoch 410, val loss: 0.9452213048934937
Epoch 420, training loss: 0.07634993642568588 = 0.06923213601112366 + 0.001 * 7.117798805236816
Epoch 420, val loss: 0.9661418795585632
Epoch 430, training loss: 0.06952370703220367 = 0.062401819974184036 + 0.001 * 7.121885299682617
Epoch 430, val loss: 0.9870755672454834
Epoch 440, training loss: 0.06345513463020325 = 0.05634979158639908 + 0.001 * 7.10534143447876
Epoch 440, val loss: 1.0078707933425903
Epoch 450, training loss: 0.05808757618069649 = 0.05098550021648407 + 0.001 * 7.102076053619385
Epoch 450, val loss: 1.028429388999939
Epoch 460, training loss: 0.053325578570365906 = 0.046227578073740005 + 0.001 * 7.097998142242432
Epoch 460, val loss: 1.0486273765563965
Epoch 470, training loss: 0.04909913241863251 = 0.04200194403529167 + 0.001 * 7.097187519073486
Epoch 470, val loss: 1.0684280395507812
Epoch 480, training loss: 0.04534385725855827 = 0.038242559880018234 + 0.001 * 7.101295471191406
Epoch 480, val loss: 1.0878318548202515
Epoch 490, training loss: 0.04198654368519783 = 0.03489245846867561 + 0.001 * 7.094086170196533
Epoch 490, val loss: 1.1068518161773682
Epoch 500, training loss: 0.03899425268173218 = 0.031906165182590485 + 0.001 * 7.088085174560547
Epoch 500, val loss: 1.1254825592041016
Epoch 510, training loss: 0.03632991015911102 = 0.02924213744699955 + 0.001 * 7.087770938873291
Epoch 510, val loss: 1.1436793804168701
Epoch 520, training loss: 0.03394629806280136 = 0.02686411142349243 + 0.001 * 7.082186698913574
Epoch 520, val loss: 1.1614121198654175
Epoch 530, training loss: 0.0318201519548893 = 0.024739602580666542 + 0.001 * 7.080548286437988
Epoch 530, val loss: 1.1786412000656128
Epoch 540, training loss: 0.02992580085992813 = 0.022838525474071503 + 0.001 * 7.08727502822876
Epoch 540, val loss: 1.1953736543655396
Epoch 550, training loss: 0.028217321261763573 = 0.021134618669748306 + 0.001 * 7.08270263671875
Epoch 550, val loss: 1.2116143703460693
Epoch 560, training loss: 0.026680268347263336 = 0.019604425877332687 + 0.001 * 7.07584285736084
Epoch 560, val loss: 1.2273402214050293
Epoch 570, training loss: 0.025307102128863335 = 0.01822732202708721 + 0.001 * 7.079779624938965
Epoch 570, val loss: 1.2425941228866577
Epoch 580, training loss: 0.0240575410425663 = 0.016985248774290085 + 0.001 * 7.072292804718018
Epoch 580, val loss: 1.2573723793029785
Epoch 590, training loss: 0.02293764054775238 = 0.015862567350268364 + 0.001 * 7.0750732421875
Epoch 590, val loss: 1.2716737985610962
Epoch 600, training loss: 0.021921221166849136 = 0.014845595695078373 + 0.001 * 7.075624942779541
Epoch 600, val loss: 1.2855331897735596
Epoch 610, training loss: 0.02098892070353031 = 0.01392210554331541 + 0.001 * 7.066814422607422
Epoch 610, val loss: 1.2989846467971802
Epoch 620, training loss: 0.020160384476184845 = 0.013081622309982777 + 0.001 * 7.078761100769043
Epoch 620, val loss: 1.3120077848434448
Epoch 630, training loss: 0.019386662170290947 = 0.012314993888139725 + 0.001 * 7.071667671203613
Epoch 630, val loss: 1.3246325254440308
Epoch 640, training loss: 0.01867852173745632 = 0.011614198796451092 + 0.001 * 7.0643229484558105
Epoch 640, val loss: 1.3368752002716064
Epoch 650, training loss: 0.018033817410469055 = 0.010972230695188046 + 0.001 * 7.061586856842041
Epoch 650, val loss: 1.3487515449523926
Epoch 660, training loss: 0.01744789071381092 = 0.010382983833551407 + 0.001 * 7.064906120300293
Epoch 660, val loss: 1.360275149345398
Epoch 670, training loss: 0.016898972913622856 = 0.00984104536473751 + 0.001 * 7.057926654815674
Epoch 670, val loss: 1.3714511394500732
Epoch 680, training loss: 0.016407063230872154 = 0.00934166181832552 + 0.001 * 7.065401554107666
Epoch 680, val loss: 1.3822911977767944
Epoch 690, training loss: 0.01594032160937786 = 0.008880557492375374 + 0.001 * 7.059764385223389
Epoch 690, val loss: 1.3928271532058716
Epoch 700, training loss: 0.015509962104260921 = 0.008454031310975552 + 0.001 * 7.0559306144714355
Epoch 700, val loss: 1.4030619859695435
Epoch 710, training loss: 0.015116849914193153 = 0.008058786392211914 + 0.001 * 7.058062553405762
Epoch 710, val loss: 1.413016676902771
Epoch 720, training loss: 0.014745134860277176 = 0.007691918406635523 + 0.001 * 7.053215980529785
Epoch 720, val loss: 1.422688364982605
Epoch 730, training loss: 0.014410337433218956 = 0.007350828964263201 + 0.001 * 7.059508323669434
Epoch 730, val loss: 1.4320995807647705
Epoch 740, training loss: 0.01408404391258955 = 0.007033208850771189 + 0.001 * 7.050834655761719
Epoch 740, val loss: 1.4412552118301392
Epoch 750, training loss: 0.013790694065392017 = 0.006736965849995613 + 0.001 * 7.053728103637695
Epoch 750, val loss: 1.4501680135726929
Epoch 760, training loss: 0.013507152907550335 = 0.0064602927304804325 + 0.001 * 7.0468597412109375
Epoch 760, val loss: 1.4588453769683838
Epoch 770, training loss: 0.013248393312096596 = 0.0062014819122850895 + 0.001 * 7.046910762786865
Epoch 770, val loss: 1.4672971963882446
Epoch 780, training loss: 0.013029305264353752 = 0.005959068424999714 + 0.001 * 7.070237159729004
Epoch 780, val loss: 1.4755222797393799
Epoch 790, training loss: 0.01278110221028328 = 0.005731702316552401 + 0.001 * 7.0493998527526855
Epoch 790, val loss: 1.4835360050201416
Epoch 800, training loss: 0.012564357370138168 = 0.005518143996596336 + 0.001 * 7.046212673187256
Epoch 800, val loss: 1.4913523197174072
Epoch 810, training loss: 0.012359313666820526 = 0.00531730055809021 + 0.001 * 7.0420122146606445
Epoch 810, val loss: 1.4989770650863647
Epoch 820, training loss: 0.012165827676653862 = 0.005128182005137205 + 0.001 * 7.0376458168029785
Epoch 820, val loss: 1.5064116716384888
Epoch 830, training loss: 0.011999020352959633 = 0.004949829075485468 + 0.001 * 7.049190998077393
Epoch 830, val loss: 1.5136692523956299
Epoch 840, training loss: 0.011834423057734966 = 0.004781458992511034 + 0.001 * 7.052963733673096
Epoch 840, val loss: 1.5207428932189941
Epoch 850, training loss: 0.011665967293083668 = 0.004622191656380892 + 0.001 * 7.0437750816345215
Epoch 850, val loss: 1.52765953540802
Epoch 860, training loss: 0.011509161442518234 = 0.004471012391149998 + 0.001 * 7.038149356842041
Epoch 860, val loss: 1.534436821937561
Epoch 870, training loss: 0.011362548917531967 = 0.004326907452195883 + 0.001 * 7.035640716552734
Epoch 870, val loss: 1.5411028861999512
Epoch 880, training loss: 0.011224174872040749 = 0.004188906867057085 + 0.001 * 7.035267353057861
Epoch 880, val loss: 1.5476802587509155
Epoch 890, training loss: 0.011094236746430397 = 0.004056208301335573 + 0.001 * 7.038028717041016
Epoch 890, val loss: 1.5542019605636597
Epoch 900, training loss: 0.010958489961922169 = 0.003928335849195719 + 0.001 * 7.030153751373291
Epoch 900, val loss: 1.5606772899627686
Epoch 910, training loss: 0.01083760429173708 = 0.0038050084840506315 + 0.001 * 7.032595634460449
Epoch 910, val loss: 1.5671017169952393
Epoch 920, training loss: 0.010725852102041245 = 0.0036861745174974203 + 0.001 * 7.039677619934082
Epoch 920, val loss: 1.5734800100326538
Epoch 930, training loss: 0.010603383183479309 = 0.0035718348808586597 + 0.001 * 7.031548500061035
Epoch 930, val loss: 1.5798014402389526
Epoch 940, training loss: 0.010483949445188046 = 0.0034619395155459642 + 0.001 * 7.022009372711182
Epoch 940, val loss: 1.5860536098480225
Epoch 950, training loss: 0.010381733998656273 = 0.0033564832992851734 + 0.001 * 7.025250434875488
Epoch 950, val loss: 1.5922311544418335
Epoch 960, training loss: 0.010279679670929909 = 0.0032554930076003075 + 0.001 * 7.024186611175537
Epoch 960, val loss: 1.5983238220214844
Epoch 970, training loss: 0.010183554142713547 = 0.0031588778365403414 + 0.001 * 7.024676322937012
Epoch 970, val loss: 1.6043211221694946
Epoch 980, training loss: 0.010092348791658878 = 0.00306642591021955 + 0.001 * 7.025922775268555
Epoch 980, val loss: 1.6102246046066284
Epoch 990, training loss: 0.009995394386351109 = 0.0029780121985822916 + 0.001 * 7.01738166809082
Epoch 990, val loss: 1.6160341501235962
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 1.9641354084014893 = 1.9555386304855347 + 0.001 * 8.596785545349121
Epoch 0, val loss: 1.9429246187210083
Epoch 10, training loss: 1.9532079696655273 = 1.9446111917495728 + 0.001 * 8.596721649169922
Epoch 10, val loss: 1.9322292804718018
Epoch 20, training loss: 1.940260648727417 = 1.9316641092300415 + 0.001 * 8.596491813659668
Epoch 20, val loss: 1.919550895690918
Epoch 30, training loss: 1.9226351976394653 = 1.9140392541885376 + 0.001 * 8.595978736877441
Epoch 30, val loss: 1.902465581893921
Epoch 40, training loss: 1.8970533609390259 = 1.8884586095809937 + 0.001 * 8.594780921936035
Epoch 40, val loss: 1.8780566453933716
Epoch 50, training loss: 1.8610399961471558 = 1.8524489402770996 + 0.001 * 8.59109878540039
Epoch 50, val loss: 1.8453835248947144
Epoch 60, training loss: 1.818821668624878 = 1.8102458715438843 + 0.001 * 8.57579231262207
Epoch 60, val loss: 1.8108959197998047
Epoch 70, training loss: 1.780444622039795 = 1.7719497680664062 + 0.001 * 8.494810104370117
Epoch 70, val loss: 1.7827672958374023
Epoch 80, training loss: 1.7361232042312622 = 1.7280608415603638 + 0.001 * 8.062387466430664
Epoch 80, val loss: 1.7441518306732178
Epoch 90, training loss: 1.6738500595092773 = 1.6659296751022339 + 0.001 * 7.920355796813965
Epoch 90, val loss: 1.6853837966918945
Epoch 100, training loss: 1.592362642288208 = 1.5845063924789429 + 0.001 * 7.856196880340576
Epoch 100, val loss: 1.6130040884017944
Epoch 110, training loss: 1.4999380111694336 = 1.4921798706054688 + 0.001 * 7.758102893829346
Epoch 110, val loss: 1.5365561246871948
Epoch 120, training loss: 1.4087755680084229 = 1.4012837409973145 + 0.001 * 7.491817951202393
Epoch 120, val loss: 1.4642101526260376
Epoch 130, training loss: 1.3222308158874512 = 1.3148366212844849 + 0.001 * 7.394165992736816
Epoch 130, val loss: 1.3975789546966553
Epoch 140, training loss: 1.238490343093872 = 1.231138825416565 + 0.001 * 7.351499080657959
Epoch 140, val loss: 1.3345255851745605
Epoch 150, training loss: 1.156562328338623 = 1.1492609977722168 + 0.001 * 7.3013176918029785
Epoch 150, val loss: 1.2735216617584229
Epoch 160, training loss: 1.0768816471099854 = 1.0696210861206055 + 0.001 * 7.260523319244385
Epoch 160, val loss: 1.2156373262405396
Epoch 170, training loss: 0.9998413920402527 = 0.9926071763038635 + 0.001 * 7.234218120574951
Epoch 170, val loss: 1.1603089570999146
Epoch 180, training loss: 0.9252801537513733 = 0.918064534664154 + 0.001 * 7.215607166290283
Epoch 180, val loss: 1.1072088479995728
Epoch 190, training loss: 0.8518943786621094 = 0.8447020649909973 + 0.001 * 7.192283630371094
Epoch 190, val loss: 1.0555126667022705
Epoch 200, training loss: 0.7787418365478516 = 0.7715763449668884 + 0.001 * 7.1654815673828125
Epoch 200, val loss: 1.0050703287124634
Epoch 210, training loss: 0.7066432237625122 = 0.6994972229003906 + 0.001 * 7.146025657653809
Epoch 210, val loss: 0.9570344090461731
Epoch 220, training loss: 0.6375842690467834 = 0.6304469704627991 + 0.001 * 7.137304306030273
Epoch 220, val loss: 0.9142193794250488
Epoch 230, training loss: 0.5731006860733032 = 0.5659664869308472 + 0.001 * 7.134178161621094
Epoch 230, val loss: 0.8785781264305115
Epoch 240, training loss: 0.5136886239051819 = 0.5065562725067139 + 0.001 * 7.132373332977295
Epoch 240, val loss: 0.8509122133255005
Epoch 250, training loss: 0.45912614464759827 = 0.4519941806793213 + 0.001 * 7.131972789764404
Epoch 250, val loss: 0.8308656811714172
Epoch 260, training loss: 0.4091227054595947 = 0.401991069316864 + 0.001 * 7.131636619567871
Epoch 260, val loss: 0.81736159324646
Epoch 270, training loss: 0.36344608664512634 = 0.3563149869441986 + 0.001 * 7.13110876083374
Epoch 270, val loss: 0.8092595934867859
Epoch 280, training loss: 0.32189977169036865 = 0.3147692382335663 + 0.001 * 7.130539894104004
Epoch 280, val loss: 0.8057476282119751
Epoch 290, training loss: 0.28431761264801025 = 0.2771875262260437 + 0.001 * 7.130074501037598
Epoch 290, val loss: 0.8063309192657471
Epoch 300, training loss: 0.25060585141181946 = 0.24347363412380219 + 0.001 * 7.1322150230407715
Epoch 300, val loss: 0.8106741905212402
Epoch 310, training loss: 0.2206869125366211 = 0.21355612576007843 + 0.001 * 7.1307854652404785
Epoch 310, val loss: 0.8185192346572876
Epoch 320, training loss: 0.194417342543602 = 0.18728740513324738 + 0.001 * 7.129936695098877
Epoch 320, val loss: 0.8295905590057373
Epoch 330, training loss: 0.17153649032115936 = 0.16440653800964355 + 0.001 * 7.129947662353516
Epoch 330, val loss: 0.8434417247772217
Epoch 340, training loss: 0.1517069786787033 = 0.14457687735557556 + 0.001 * 7.130094528198242
Epoch 340, val loss: 0.8595733642578125
Epoch 350, training loss: 0.13456173241138458 = 0.12743113934993744 + 0.001 * 7.130596160888672
Epoch 350, val loss: 0.8775189518928528
Epoch 360, training loss: 0.11973435431718826 = 0.11260321736335754 + 0.001 * 7.131139278411865
Epoch 360, val loss: 0.8967678546905518
Epoch 370, training loss: 0.10689730197191238 = 0.09976624697446823 + 0.001 * 7.131052017211914
Epoch 370, val loss: 0.9168249368667603
Epoch 380, training loss: 0.09576766937971115 = 0.08863645046949387 + 0.001 * 7.131218433380127
Epoch 380, val loss: 0.9373310804367065
Epoch 390, training loss: 0.0860978439450264 = 0.07896659523248672 + 0.001 * 7.131246566772461
Epoch 390, val loss: 0.9581186771392822
Epoch 400, training loss: 0.07767115533351898 = 0.07053720206022263 + 0.001 * 7.133954048156738
Epoch 400, val loss: 0.978965163230896
Epoch 410, training loss: 0.07030470669269562 = 0.06317369639873505 + 0.001 * 7.131007671356201
Epoch 410, val loss: 0.9996669292449951
Epoch 420, training loss: 0.06386126577854156 = 0.05673022195696831 + 0.001 * 7.131042957305908
Epoch 420, val loss: 1.020180583000183
Epoch 430, training loss: 0.058212533593177795 = 0.05108211934566498 + 0.001 * 7.130415439605713
Epoch 430, val loss: 1.0404037237167358
Epoch 440, training loss: 0.05325276032090187 = 0.046123143285512924 + 0.001 * 7.129615306854248
Epoch 440, val loss: 1.0602607727050781
Epoch 450, training loss: 0.04889091104269028 = 0.04176222160458565 + 0.001 * 7.128689765930176
Epoch 450, val loss: 1.0796802043914795
Epoch 460, training loss: 0.04504793882369995 = 0.03792090341448784 + 0.001 * 7.127033233642578
Epoch 460, val loss: 1.098657488822937
Epoch 470, training loss: 0.04165642336010933 = 0.03453077748417854 + 0.001 * 7.125644683837891
Epoch 470, val loss: 1.1171276569366455
Epoch 480, training loss: 0.03866098076105118 = 0.03153347969055176 + 0.001 * 7.127501010894775
Epoch 480, val loss: 1.135105013847351
Epoch 490, training loss: 0.035998523235321045 = 0.028878511860966682 + 0.001 * 7.120008945465088
Epoch 490, val loss: 1.152602195739746
Epoch 500, training loss: 0.03363741934299469 = 0.026521192863583565 + 0.001 * 7.116225242614746
Epoch 500, val loss: 1.1695879697799683
Epoch 510, training loss: 0.031546276062726974 = 0.024423470720648766 + 0.001 * 7.122806072235107
Epoch 510, val loss: 1.1860586404800415
Epoch 520, training loss: 0.029668768867850304 = 0.02255241945385933 + 0.001 * 7.116349220275879
Epoch 520, val loss: 1.2020108699798584
Epoch 530, training loss: 0.02798544242978096 = 0.020879384130239487 + 0.001 * 7.106057167053223
Epoch 530, val loss: 1.217505693435669
Epoch 540, training loss: 0.02647997997701168 = 0.019379213452339172 + 0.001 * 7.100766181945801
Epoch 540, val loss: 1.232506275177002
Epoch 550, training loss: 0.025277916342020035 = 0.018030565232038498 + 0.001 * 7.247350692749023
Epoch 550, val loss: 1.2470303773880005
Epoch 560, training loss: 0.0239376500248909 = 0.01681509241461754 + 0.001 * 7.122556209564209
Epoch 560, val loss: 1.2610688209533691
Epoch 570, training loss: 0.022816825658082962 = 0.015716927126049995 + 0.001 * 7.0998992919921875
Epoch 570, val loss: 1.274659276008606
Epoch 580, training loss: 0.021812256425619125 = 0.014722117222845554 + 0.001 * 7.090139865875244
Epoch 580, val loss: 1.287833571434021
Epoch 590, training loss: 0.0209003034979105 = 0.01381873618811369 + 0.001 * 7.08156681060791
Epoch 590, val loss: 1.3005876541137695
Epoch 600, training loss: 0.020101351663470268 = 0.0129963094368577 + 0.001 * 7.10504150390625
Epoch 600, val loss: 1.312938928604126
Epoch 610, training loss: 0.019323036074638367 = 0.012245859950780869 + 0.001 * 7.077175617218018
Epoch 610, val loss: 1.3249176740646362
Epoch 620, training loss: 0.018640030175447464 = 0.011559574864804745 + 0.001 * 7.080455303192139
Epoch 620, val loss: 1.3365341424942017
Epoch 630, training loss: 0.018007349222898483 = 0.010930570773780346 + 0.001 * 7.07677698135376
Epoch 630, val loss: 1.3477835655212402
Epoch 640, training loss: 0.01741924323141575 = 0.010352836921811104 + 0.001 * 7.06640625
Epoch 640, val loss: 1.358688473701477
Epoch 650, training loss: 0.01688959077000618 = 0.009821091778576374 + 0.001 * 7.068498611450195
Epoch 650, val loss: 1.369259238243103
Epoch 660, training loss: 0.01639430597424507 = 0.009330686181783676 + 0.001 * 7.0636186599731445
Epoch 660, val loss: 1.3795210123062134
Epoch 670, training loss: 0.01595616340637207 = 0.008877506479620934 + 0.001 * 7.07865571975708
Epoch 670, val loss: 1.3894604444503784
Epoch 680, training loss: 0.015504498034715652 = 0.008458022959530354 + 0.001 * 7.046475410461426
Epoch 680, val loss: 1.3991199731826782
Epoch 690, training loss: 0.015118638053536415 = 0.008069014176726341 + 0.001 * 7.049623489379883
Epoch 690, val loss: 1.4085049629211426
Epoch 700, training loss: 0.014752598479390144 = 0.007707639131695032 + 0.001 * 7.044958591461182
Epoch 700, val loss: 1.4176193475723267
Epoch 710, training loss: 0.014434965327382088 = 0.007371400948613882 + 0.001 * 7.063563823699951
Epoch 710, val loss: 1.4264963865280151
Epoch 720, training loss: 0.014096332713961601 = 0.00705803232267499 + 0.001 * 7.038300514221191
Epoch 720, val loss: 1.4351288080215454
Epoch 730, training loss: 0.013799061067402363 = 0.006765554193407297 + 0.001 * 7.033506393432617
Epoch 730, val loss: 1.4435169696807861
Epoch 740, training loss: 0.013521451503038406 = 0.006492144428193569 + 0.001 * 7.029306888580322
Epoch 740, val loss: 1.4516793489456177
Epoch 750, training loss: 0.013262316584587097 = 0.0062362076714634895 + 0.001 * 7.026108264923096
Epoch 750, val loss: 1.4596151113510132
Epoch 760, training loss: 0.013058971613645554 = 0.005996291525661945 + 0.001 * 7.062680244445801
Epoch 760, val loss: 1.4673523902893066
Epoch 770, training loss: 0.01280609518289566 = 0.00577113963663578 + 0.001 * 7.0349555015563965
Epoch 770, val loss: 1.4748893976211548
Epoch 780, training loss: 0.012586411088705063 = 0.005559559911489487 + 0.001 * 7.02685022354126
Epoch 780, val loss: 1.4822241067886353
Epoch 790, training loss: 0.012383624911308289 = 0.005360463634133339 + 0.001 * 7.023160457611084
Epoch 790, val loss: 1.4893780946731567
Epoch 800, training loss: 0.012196201831102371 = 0.005172913894057274 + 0.001 * 7.023288249969482
Epoch 800, val loss: 1.4963425397872925
Epoch 810, training loss: 0.01203143410384655 = 0.004996029194444418 + 0.001 * 7.035404205322266
Epoch 810, val loss: 1.5031460523605347
Epoch 820, training loss: 0.011847497895359993 = 0.004829004406929016 + 0.001 * 7.018492698669434
Epoch 820, val loss: 1.5097805261611938
Epoch 830, training loss: 0.011691572144627571 = 0.004671150352805853 + 0.001 * 7.020421981811523
Epoch 830, val loss: 1.5162585973739624
Epoch 840, training loss: 0.011544168926775455 = 0.004521800205111504 + 0.001 * 7.022368431091309
Epoch 840, val loss: 1.522573709487915
Epoch 850, training loss: 0.011399021372199059 = 0.004380360711365938 + 0.001 * 7.018660068511963
Epoch 850, val loss: 1.528748631477356
Epoch 860, training loss: 0.011266333982348442 = 0.004246258642524481 + 0.001 * 7.020074844360352
Epoch 860, val loss: 1.534774661064148
Epoch 870, training loss: 0.01112683117389679 = 0.004119005054235458 + 0.001 * 7.00782585144043
Epoch 870, val loss: 1.5406602621078491
Epoch 880, training loss: 0.011013118550181389 = 0.003998149186372757 + 0.001 * 7.014969348907471
Epoch 880, val loss: 1.54640531539917
Epoch 890, training loss: 0.01089447271078825 = 0.003883268451318145 + 0.001 * 7.011203765869141
Epoch 890, val loss: 1.5520304441452026
Epoch 900, training loss: 0.010805631056427956 = 0.0037739828694611788 + 0.001 * 7.0316481590271
Epoch 900, val loss: 1.55752432346344
Epoch 910, training loss: 0.010677504353225231 = 0.003669954836368561 + 0.001 * 7.007549285888672
Epoch 910, val loss: 1.562897801399231
Epoch 920, training loss: 0.010584349744021893 = 0.0035708192735910416 + 0.001 * 7.013530254364014
Epoch 920, val loss: 1.5681504011154175
Epoch 930, training loss: 0.01050419732928276 = 0.0034762988798320293 + 0.001 * 7.027898788452148
Epoch 930, val loss: 1.5732969045639038
Epoch 940, training loss: 0.010394824668765068 = 0.003386064898222685 + 0.001 * 7.00875997543335
Epoch 940, val loss: 1.5783103704452515
Epoch 950, training loss: 0.010308617725968361 = 0.0032999026589095592 + 0.001 * 7.0087151527404785
Epoch 950, val loss: 1.583217740058899
Epoch 960, training loss: 0.010210348293185234 = 0.0032174955122172832 + 0.001 * 6.992852210998535
Epoch 960, val loss: 1.5880192518234253
Epoch 970, training loss: 0.010133933275938034 = 0.0031386169139295816 + 0.001 * 6.995316028594971
Epoch 970, val loss: 1.5927120447158813
Epoch 980, training loss: 0.010073100216686726 = 0.003063025651499629 + 0.001 * 7.010074138641357
Epoch 980, val loss: 1.59732985496521
Epoch 990, training loss: 0.009988983161747456 = 0.00299045885913074 + 0.001 * 6.998524188995361
Epoch 990, val loss: 1.6018530130386353
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 1.9329490661621094 = 1.9243522882461548 + 0.001 * 8.596832275390625
Epoch 0, val loss: 1.9193990230560303
Epoch 10, training loss: 1.9239702224731445 = 1.91537344455719 + 0.001 * 8.596774101257324
Epoch 10, val loss: 1.9109275341033936
Epoch 20, training loss: 1.9126070737838745 = 1.904010534286499 + 0.001 * 8.596595764160156
Epoch 20, val loss: 1.8997999429702759
Epoch 30, training loss: 1.8965108394622803 = 1.8879146575927734 + 0.001 * 8.596203804016113
Epoch 30, val loss: 1.8837823867797852
Epoch 40, training loss: 1.8728855848312378 = 1.8642903566360474 + 0.001 * 8.595227241516113
Epoch 40, val loss: 1.8604594469070435
Epoch 50, training loss: 1.840096116065979 = 1.8315036296844482 + 0.001 * 8.592443466186523
Epoch 50, val loss: 1.8294548988342285
Epoch 60, training loss: 1.80205237865448 = 1.793470025062561 + 0.001 * 8.582319259643555
Epoch 60, val loss: 1.796288013458252
Epoch 70, training loss: 1.7623088359832764 = 1.7537758350372314 + 0.001 * 8.533035278320312
Epoch 70, val loss: 1.7621331214904785
Epoch 80, training loss: 1.7083821296691895 = 1.7001948356628418 + 0.001 * 8.187275886535645
Epoch 80, val loss: 1.7130541801452637
Epoch 90, training loss: 1.633420467376709 = 1.6255079507827759 + 0.001 * 7.912464141845703
Epoch 90, val loss: 1.646363377571106
Epoch 100, training loss: 1.5381134748458862 = 1.5302780866622925 + 0.001 * 7.835332870483398
Epoch 100, val loss: 1.5646377801895142
Epoch 110, training loss: 1.431932806968689 = 1.4242051839828491 + 0.001 * 7.727593898773193
Epoch 110, val loss: 1.4736077785491943
Epoch 120, training loss: 1.3247478008270264 = 1.3171414136886597 + 0.001 * 7.606332778930664
Epoch 120, val loss: 1.3856264352798462
Epoch 130, training loss: 1.2239131927490234 = 1.2163782119750977 + 0.001 * 7.534931182861328
Epoch 130, val loss: 1.305724024772644
Epoch 140, training loss: 1.1333701610565186 = 1.1259467601776123 + 0.001 * 7.42343282699585
Epoch 140, val loss: 1.2370033264160156
Epoch 150, training loss: 1.053117036819458 = 1.0457907915115356 + 0.001 * 7.326246738433838
Epoch 150, val loss: 1.1787147521972656
Epoch 160, training loss: 0.9811061024665833 = 0.9738216996192932 + 0.001 * 7.284428596496582
Epoch 160, val loss: 1.128231167793274
Epoch 170, training loss: 0.9145796895027161 = 0.9073008894920349 + 0.001 * 7.27878475189209
Epoch 170, val loss: 1.0822958946228027
Epoch 180, training loss: 0.8503379821777344 = 0.8430593013763428 + 0.001 * 7.278696537017822
Epoch 180, val loss: 1.037665605545044
Epoch 190, training loss: 0.7856081128120422 = 0.7783326506614685 + 0.001 * 7.2754597663879395
Epoch 190, val loss: 0.9918980598449707
Epoch 200, training loss: 0.7189235687255859 = 0.7116517424583435 + 0.001 * 7.271812915802002
Epoch 200, val loss: 0.9440353512763977
Epoch 210, training loss: 0.6503413319587708 = 0.6430721879005432 + 0.001 * 7.269163131713867
Epoch 210, val loss: 0.8946148157119751
Epoch 220, training loss: 0.5815436840057373 = 0.5742767453193665 + 0.001 * 7.266918182373047
Epoch 220, val loss: 0.8452540040016174
Epoch 230, training loss: 0.5150986909866333 = 0.5078340768814087 + 0.001 * 7.2646403312683105
Epoch 230, val loss: 0.7986041307449341
Epoch 240, training loss: 0.45309463143348694 = 0.44583243131637573 + 0.001 * 7.262213230133057
Epoch 240, val loss: 0.7570483684539795
Epoch 250, training loss: 0.39641785621643066 = 0.3891581892967224 + 0.001 * 7.259680271148682
Epoch 250, val loss: 0.7216808795928955
Epoch 260, training loss: 0.3450898826122284 = 0.33783289790153503 + 0.001 * 7.2569990158081055
Epoch 260, val loss: 0.6927007436752319
Epoch 270, training loss: 0.29892829060554504 = 0.2916741371154785 + 0.001 * 7.2541584968566895
Epoch 270, val loss: 0.6698727011680603
Epoch 280, training loss: 0.2577941417694092 = 0.25054240226745605 + 0.001 * 7.251733779907227
Epoch 280, val loss: 0.6526203751564026
Epoch 290, training loss: 0.2216765135526657 = 0.21442767977714539 + 0.001 * 7.248828411102295
Epoch 290, val loss: 0.6405878663063049
Epoch 300, training loss: 0.19045452773571014 = 0.1832086145877838 + 0.001 * 7.245907306671143
Epoch 300, val loss: 0.6333130598068237
Epoch 310, training loss: 0.16381798684597015 = 0.15657612681388855 + 0.001 * 7.241861343383789
Epoch 310, val loss: 0.6304762959480286
Epoch 320, training loss: 0.14133816957473755 = 0.13409492373466492 + 0.001 * 7.2432379722595215
Epoch 320, val loss: 0.6315516233444214
Epoch 330, training loss: 0.12247021496295929 = 0.11523838341236115 + 0.001 * 7.2318315505981445
Epoch 330, val loss: 0.635999321937561
Epoch 340, training loss: 0.10668962448835373 = 0.09946379065513611 + 0.001 * 7.225832939147949
Epoch 340, val loss: 0.6430150866508484
Epoch 350, training loss: 0.09348966181278229 = 0.08626969903707504 + 0.001 * 7.219960689544678
Epoch 350, val loss: 0.6519766449928284
Epoch 360, training loss: 0.08241782337427139 = 0.07521169632673264 + 0.001 * 7.206125259399414
Epoch 360, val loss: 0.6623181700706482
Epoch 370, training loss: 0.07311687618494034 = 0.06591713428497314 + 0.001 * 7.199739933013916
Epoch 370, val loss: 0.67352294921875
Epoch 380, training loss: 0.06530939042568207 = 0.05807570740580559 + 0.001 * 7.2336812019348145
Epoch 380, val loss: 0.6853436231613159
Epoch 390, training loss: 0.05861799046397209 = 0.05143255740404129 + 0.001 * 7.185431957244873
Epoch 390, val loss: 0.6975148320198059
Epoch 400, training loss: 0.052955109626054764 = 0.0457783043384552 + 0.001 * 7.176804542541504
Epoch 400, val loss: 0.7097817063331604
Epoch 410, training loss: 0.04810577258467674 = 0.040942370891571045 + 0.001 * 7.163402557373047
Epoch 410, val loss: 0.7220820784568787
Epoch 420, training loss: 0.043964285403490067 = 0.03678632900118828 + 0.001 * 7.177955150604248
Epoch 420, val loss: 0.7343047261238098
Epoch 430, training loss: 0.040346600115299225 = 0.03319677710533142 + 0.001 * 7.149820804595947
Epoch 430, val loss: 0.7463393211364746
Epoch 440, training loss: 0.03722033277153969 = 0.030080333352088928 + 0.001 * 7.139998912811279
Epoch 440, val loss: 0.7581323981285095
Epoch 450, training loss: 0.03450193628668785 = 0.02735634706914425 + 0.001 * 7.145587921142578
Epoch 450, val loss: 0.7697145938873291
Epoch 460, training loss: 0.03209259361028671 = 0.024964964017271996 + 0.001 * 7.127628803253174
Epoch 460, val loss: 0.7809855937957764
Epoch 470, training loss: 0.029982231557369232 = 0.022856580093503 + 0.001 * 7.125651836395264
Epoch 470, val loss: 0.7919720411300659
Epoch 480, training loss: 0.02810700424015522 = 0.020991111174225807 + 0.001 * 7.115893363952637
Epoch 480, val loss: 0.802756130695343
Epoch 490, training loss: 0.02644817903637886 = 0.019335823133587837 + 0.001 * 7.112356662750244
Epoch 490, val loss: 0.8131955862045288
Epoch 500, training loss: 0.024967428296804428 = 0.017863044515252113 + 0.001 * 7.104384422302246
Epoch 500, val loss: 0.8233138918876648
Epoch 510, training loss: 0.023646259680390358 = 0.016548847779631615 + 0.001 * 7.097411632537842
Epoch 510, val loss: 0.8331663012504578
Epoch 520, training loss: 0.022470848634839058 = 0.015372815541923046 + 0.001 * 7.098032474517822
Epoch 520, val loss: 0.8427014350891113
Epoch 530, training loss: 0.02140820026397705 = 0.014317311346530914 + 0.001 * 7.090888500213623
Epoch 530, val loss: 0.8519561290740967
Epoch 540, training loss: 0.020458877086639404 = 0.01336717139929533 + 0.001 * 7.091704368591309
Epoch 540, val loss: 0.8609333634376526
Epoch 550, training loss: 0.019600320607423782 = 0.012509551830589771 + 0.001 * 7.090768337249756
Epoch 550, val loss: 0.8696305155754089
Epoch 560, training loss: 0.018820933997631073 = 0.011733164079487324 + 0.001 * 7.087769508361816
Epoch 560, val loss: 0.8780593872070312
Epoch 570, training loss: 0.01810423657298088 = 0.011027444154024124 + 0.001 * 7.076791763305664
Epoch 570, val loss: 0.886246919631958
Epoch 580, training loss: 0.017510561272501945 = 0.010384096764028072 + 0.001 * 7.126463890075684
Epoch 580, val loss: 0.8942350149154663
Epoch 590, training loss: 0.01687648519873619 = 0.009796200320124626 + 0.001 * 7.08028507232666
Epoch 590, val loss: 0.9019933342933655
Epoch 600, training loss: 0.01633414439857006 = 0.009257011115550995 + 0.001 * 7.0771331787109375
Epoch 600, val loss: 0.9095515608787537
Epoch 610, training loss: 0.015829913318157196 = 0.008760860189795494 + 0.001 * 7.069053649902344
Epoch 610, val loss: 0.9168999195098877
Epoch 620, training loss: 0.015376847237348557 = 0.008302860893309116 + 0.001 * 7.073985576629639
Epoch 620, val loss: 0.924079418182373
Epoch 630, training loss: 0.014940893277525902 = 0.007878988981246948 + 0.001 * 7.061903476715088
Epoch 630, val loss: 0.9310741424560547
Epoch 640, training loss: 0.014546491205692291 = 0.007485235575586557 + 0.001 * 7.061254978179932
Epoch 640, val loss: 0.9379801154136658
Epoch 650, training loss: 0.014178531244397163 = 0.007117739878594875 + 0.001 * 7.060791015625
Epoch 650, val loss: 0.9447677731513977
Epoch 660, training loss: 0.013883581385016441 = 0.00677399942651391 + 0.001 * 7.109580993652344
Epoch 660, val loss: 0.9514873623847961
Epoch 670, training loss: 0.013521827757358551 = 0.006451611872762442 + 0.001 * 7.070215702056885
Epoch 670, val loss: 0.9581440091133118
Epoch 680, training loss: 0.013200504705309868 = 0.006148132029920816 + 0.001 * 7.052372932434082
Epoch 680, val loss: 0.9647158980369568
Epoch 690, training loss: 0.012917115353047848 = 0.005862131714820862 + 0.001 * 7.054983139038086
Epoch 690, val loss: 0.9712088108062744
Epoch 700, training loss: 0.012651992961764336 = 0.005592312663793564 + 0.001 * 7.059679985046387
Epoch 700, val loss: 0.9776157140731812
Epoch 710, training loss: 0.012376461178064346 = 0.00533689372241497 + 0.001 * 7.039566993713379
Epoch 710, val loss: 0.9839989542961121
Epoch 720, training loss: 0.012178763747215271 = 0.005095074884593487 + 0.001 * 7.083689212799072
Epoch 720, val loss: 0.9903196096420288
Epoch 730, training loss: 0.01191619411110878 = 0.004866921342909336 + 0.001 * 7.0492730140686035
Epoch 730, val loss: 0.9965762495994568
Epoch 740, training loss: 0.011708920821547508 = 0.004651794210076332 + 0.001 * 7.057126522064209
Epoch 740, val loss: 1.002772331237793
Epoch 750, training loss: 0.01148921251296997 = 0.004449167754501104 + 0.001 * 7.040044784545898
Epoch 750, val loss: 1.0088855028152466
Epoch 760, training loss: 0.011294128373265266 = 0.004258312750607729 + 0.001 * 7.0358147621154785
Epoch 760, val loss: 1.0149458646774292
Epoch 770, training loss: 0.011105360463261604 = 0.004078736994415522 + 0.001 * 7.026623725891113
Epoch 770, val loss: 1.0209035873413086
Epoch 780, training loss: 0.010946795344352722 = 0.003909926395863295 + 0.001 * 7.036868572235107
Epoch 780, val loss: 1.0267630815505981
Epoch 790, training loss: 0.01077382080256939 = 0.0037514488212764263 + 0.001 * 7.022371292114258
Epoch 790, val loss: 1.0325086116790771
Epoch 800, training loss: 0.010631750337779522 = 0.003602517768740654 + 0.001 * 7.029232025146484
Epoch 800, val loss: 1.0381526947021484
Epoch 810, training loss: 0.010489117354154587 = 0.003462493419647217 + 0.001 * 7.026623725891113
Epoch 810, val loss: 1.0436856746673584
Epoch 820, training loss: 0.010356167331337929 = 0.003330686129629612 + 0.001 * 7.0254807472229
Epoch 820, val loss: 1.049110770225525
Epoch 830, training loss: 0.01023703534156084 = 0.0032066102139651775 + 0.001 * 7.03042459487915
Epoch 830, val loss: 1.0544371604919434
Epoch 840, training loss: 0.010112475603818893 = 0.003089841455221176 + 0.001 * 7.022634506225586
Epoch 840, val loss: 1.0596585273742676
Epoch 850, training loss: 0.009993716143071651 = 0.0029798171017318964 + 0.001 * 7.0138983726501465
Epoch 850, val loss: 1.0647603273391724
Epoch 860, training loss: 0.009904610924422741 = 0.002876007230952382 + 0.001 * 7.0286030769348145
Epoch 860, val loss: 1.0697599649429321
Epoch 870, training loss: 0.009805919602513313 = 0.002778101246803999 + 0.001 * 7.02781867980957
Epoch 870, val loss: 1.0746678113937378
Epoch 880, training loss: 0.009699709713459015 = 0.002685749903321266 + 0.001 * 7.0139594078063965
Epoch 880, val loss: 1.079437017440796
Epoch 890, training loss: 0.009613792411983013 = 0.0025984460953623056 + 0.001 * 7.015345573425293
Epoch 890, val loss: 1.0841397047042847
Epoch 900, training loss: 0.009512514807283878 = 0.002515752101317048 + 0.001 * 6.996762752532959
Epoch 900, val loss: 1.0887418985366821
Epoch 910, training loss: 0.009439492598176003 = 0.002437437418848276 + 0.001 * 7.0020551681518555
Epoch 910, val loss: 1.0932717323303223
Epoch 920, training loss: 0.009361273609101772 = 0.002363357227295637 + 0.001 * 6.997916221618652
Epoch 920, val loss: 1.0976417064666748
Epoch 930, training loss: 0.009297920390963554 = 0.0022930775303393602 + 0.001 * 7.004842281341553
Epoch 930, val loss: 1.1019498109817505
Epoch 940, training loss: 0.009234895929694176 = 0.002226372715085745 + 0.001 * 7.008523464202881
Epoch 940, val loss: 1.1061543226242065
Epoch 950, training loss: 0.009194863960146904 = 0.002163017401471734 + 0.001 * 7.031846523284912
Epoch 950, val loss: 1.1102782487869263
Epoch 960, training loss: 0.00908257719129324 = 0.0021028397604823112 + 0.001 * 6.979737281799316
Epoch 960, val loss: 1.1142921447753906
Epoch 970, training loss: 0.009034854359924793 = 0.0020456050988286734 + 0.001 * 6.989249229431152
Epoch 970, val loss: 1.1182199716567993
Epoch 980, training loss: 0.008983638137578964 = 0.001991128083318472 + 0.001 * 6.9925103187561035
Epoch 980, val loss: 1.1220782995224
Epoch 990, training loss: 0.008943584747612476 = 0.0019391925306990743 + 0.001 * 7.004391670227051
Epoch 990, val loss: 1.1258338689804077
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8365840801265156
The final CL Acc:0.80370, 0.01090, The final GNN Acc:0.83658, 0.00215
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9414])
updated graph: torch.Size([2, 10436])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9553884267807007 = 1.9467915296554565 + 0.001 * 8.596877098083496
Epoch 0, val loss: 1.9522300958633423
Epoch 10, training loss: 1.945986270904541 = 1.9373894929885864 + 0.001 * 8.596834182739258
Epoch 10, val loss: 1.9432188272476196
Epoch 20, training loss: 1.9345442056655884 = 1.9259475469589233 + 0.001 * 8.596697807312012
Epoch 20, val loss: 1.931788444519043
Epoch 30, training loss: 1.918898105621338 = 1.9103018045425415 + 0.001 * 8.596342086791992
Epoch 30, val loss: 1.9158096313476562
Epoch 40, training loss: 1.8964054584503174 = 1.8878101110458374 + 0.001 * 8.595389366149902
Epoch 40, val loss: 1.8926926851272583
Epoch 50, training loss: 1.8651275634765625 = 1.8565350770950317 + 0.001 * 8.592496871948242
Epoch 50, val loss: 1.86122465133667
Epoch 60, training loss: 1.8288241624832153 = 1.820241928100586 + 0.001 * 8.582256317138672
Epoch 60, val loss: 1.8267837762832642
Epoch 70, training loss: 1.7965061664581299 = 1.7879633903503418 + 0.001 * 8.542828559875488
Epoch 70, val loss: 1.797236442565918
Epoch 80, training loss: 1.7609896659851074 = 1.7526437044143677 + 0.001 * 8.345970153808594
Epoch 80, val loss: 1.7641489505767822
Epoch 90, training loss: 1.711517333984375 = 1.7033251523971558 + 0.001 * 8.192211151123047
Epoch 90, val loss: 1.7209111452102661
Epoch 100, training loss: 1.6422293186187744 = 1.6341615915298462 + 0.001 * 8.067754745483398
Epoch 100, val loss: 1.6622179746627808
Epoch 110, training loss: 1.553112268447876 = 1.5451774597167969 + 0.001 * 7.9348225593566895
Epoch 110, val loss: 1.5890100002288818
Epoch 120, training loss: 1.4521421194076538 = 1.4444127082824707 + 0.001 * 7.729427337646484
Epoch 120, val loss: 1.5066444873809814
Epoch 130, training loss: 1.3486926555633545 = 1.3411011695861816 + 0.001 * 7.591535568237305
Epoch 130, val loss: 1.4245843887329102
Epoch 140, training loss: 1.247591257095337 = 1.2400238513946533 + 0.001 * 7.567455291748047
Epoch 140, val loss: 1.3461425304412842
Epoch 150, training loss: 1.1526366472244263 = 1.145103931427002 + 0.001 * 7.5326924324035645
Epoch 150, val loss: 1.2747095823287964
Epoch 160, training loss: 1.0676370859146118 = 1.060130000114441 + 0.001 * 7.507058620452881
Epoch 160, val loss: 1.2128427028656006
Epoch 170, training loss: 0.993323564529419 = 0.9858250617980957 + 0.001 * 7.498522758483887
Epoch 170, val loss: 1.1603853702545166
Epoch 180, training loss: 0.9270526170730591 = 0.919558584690094 + 0.001 * 7.494029521942139
Epoch 180, val loss: 1.1146286725997925
Epoch 190, training loss: 0.8653208017349243 = 0.8578302264213562 + 0.001 * 7.490591049194336
Epoch 190, val loss: 1.072018027305603
Epoch 200, training loss: 0.8059029579162598 = 0.7984161972999573 + 0.001 * 7.486739158630371
Epoch 200, val loss: 1.0307248830795288
Epoch 210, training loss: 0.7487787008285522 = 0.7412971258163452 + 0.001 * 7.481557846069336
Epoch 210, val loss: 0.990826427936554
Epoch 220, training loss: 0.6950011253356934 = 0.6875280141830444 + 0.001 * 7.473101615905762
Epoch 220, val loss: 0.9535208344459534
Epoch 230, training loss: 0.6449113488197327 = 0.637453019618988 + 0.001 * 7.458348274230957
Epoch 230, val loss: 0.9198229908943176
Epoch 240, training loss: 0.5974186062812805 = 0.5899813175201416 + 0.001 * 7.437260627746582
Epoch 240, val loss: 0.8893312811851501
Epoch 250, training loss: 0.5509560108184814 = 0.5435498952865601 + 0.001 * 7.406125068664551
Epoch 250, val loss: 0.8611149787902832
Epoch 260, training loss: 0.504771888256073 = 0.4973948299884796 + 0.001 * 7.377062797546387
Epoch 260, val loss: 0.8352436423301697
Epoch 270, training loss: 0.4591999650001526 = 0.4518468976020813 + 0.001 * 7.353056907653809
Epoch 270, val loss: 0.8128765225410461
Epoch 280, training loss: 0.41536203026771545 = 0.4080265462398529 + 0.001 * 7.3354973793029785
Epoch 280, val loss: 0.7950612306594849
Epoch 290, training loss: 0.37454497814178467 = 0.36721786856651306 + 0.001 * 7.32710075378418
Epoch 290, val loss: 0.7820743322372437
Epoch 300, training loss: 0.33744367957115173 = 0.33012187480926514 + 0.001 * 7.321802616119385
Epoch 300, val loss: 0.7735297679901123
Epoch 310, training loss: 0.3039165139198303 = 0.2965935468673706 + 0.001 * 7.322976589202881
Epoch 310, val loss: 0.7690434455871582
Epoch 320, training loss: 0.2733389139175415 = 0.26602867245674133 + 0.001 * 7.310247421264648
Epoch 320, val loss: 0.7678457498550415
Epoch 330, training loss: 0.24506300687789917 = 0.23775802552700043 + 0.001 * 7.304973602294922
Epoch 330, val loss: 0.7695228457450867
Epoch 340, training loss: 0.21868470311164856 = 0.21138416230678558 + 0.001 * 7.3005475997924805
Epoch 340, val loss: 0.7735918164253235
Epoch 350, training loss: 0.1942124366760254 = 0.18690235912799835 + 0.001 * 7.310078144073486
Epoch 350, val loss: 0.7797945141792297
Epoch 360, training loss: 0.17185616493225098 = 0.16455644369125366 + 0.001 * 7.299717903137207
Epoch 360, val loss: 0.787905216217041
Epoch 370, training loss: 0.15185220539569855 = 0.14456124603748322 + 0.001 * 7.290955543518066
Epoch 370, val loss: 0.797563910484314
Epoch 380, training loss: 0.13422730565071106 = 0.12694190442562103 + 0.001 * 7.285395622253418
Epoch 380, val loss: 0.8083292245864868
Epoch 390, training loss: 0.11882658302783966 = 0.11154479533433914 + 0.001 * 7.281789302825928
Epoch 390, val loss: 0.8198744654655457
Epoch 400, training loss: 0.10543283075094223 = 0.09815355390310287 + 0.001 * 7.279273509979248
Epoch 400, val loss: 0.8319551348686218
Epoch 410, training loss: 0.09381280094385147 = 0.0865437388420105 + 0.001 * 7.269063949584961
Epoch 410, val loss: 0.8443899750709534
Epoch 420, training loss: 0.08376236259937286 = 0.07650081068277359 + 0.001 * 7.261550426483154
Epoch 420, val loss: 0.8570751547813416
Epoch 430, training loss: 0.07511086761951447 = 0.06782393902540207 + 0.001 * 7.286924362182617
Epoch 430, val loss: 0.8698979616165161
Epoch 440, training loss: 0.06758154183626175 = 0.06032747030258179 + 0.001 * 7.254073619842529
Epoch 440, val loss: 0.8827809691429138
Epoch 450, training loss: 0.06109316274523735 = 0.05384260043501854 + 0.001 * 7.250560760498047
Epoch 450, val loss: 0.895717203617096
Epoch 460, training loss: 0.05546657741069794 = 0.048223212361335754 + 0.001 * 7.243362903594971
Epoch 460, val loss: 0.9086542129516602
Epoch 470, training loss: 0.05063009634613991 = 0.043342672288417816 + 0.001 * 7.287423133850098
Epoch 470, val loss: 0.9215120673179626
Epoch 480, training loss: 0.04634224995970726 = 0.03909377008676529 + 0.001 * 7.24847936630249
Epoch 480, val loss: 0.9341979622840881
Epoch 490, training loss: 0.04261946678161621 = 0.035384878516197205 + 0.001 * 7.234588146209717
Epoch 490, val loss: 0.9466710686683655
Epoch 500, training loss: 0.039371564984321594 = 0.032138895243406296 + 0.001 * 7.232668399810791
Epoch 500, val loss: 0.9588893055915833
Epoch 510, training loss: 0.036519840359687805 = 0.029289554804563522 + 0.001 * 7.23028564453125
Epoch 510, val loss: 0.9708202481269836
Epoch 520, training loss: 0.03401198983192444 = 0.02678108960390091 + 0.001 * 7.230899810791016
Epoch 520, val loss: 0.9824365973472595
Epoch 530, training loss: 0.03179505094885826 = 0.024566039443016052 + 0.001 * 7.229011058807373
Epoch 530, val loss: 0.993731677532196
Epoch 540, training loss: 0.02984258346259594 = 0.022604472935199738 + 0.001 * 7.238109588623047
Epoch 540, val loss: 1.0046947002410889
Epoch 550, training loss: 0.028089385479688644 = 0.02086239866912365 + 0.001 * 7.226985931396484
Epoch 550, val loss: 1.0153429508209229
Epoch 560, training loss: 0.02653241530060768 = 0.019309913739562035 + 0.001 * 7.222501754760742
Epoch 560, val loss: 1.0256690979003906
Epoch 570, training loss: 0.02514546923339367 = 0.017922187224030495 + 0.001 * 7.223281383514404
Epoch 570, val loss: 1.035660982131958
Epoch 580, training loss: 0.023899205029010773 = 0.016677938401699066 + 0.001 * 7.221267223358154
Epoch 580, val loss: 1.0453364849090576
Epoch 590, training loss: 0.022776685655117035 = 0.015559196472167969 + 0.001 * 7.2174882888793945
Epoch 590, val loss: 1.054697871208191
Epoch 600, training loss: 0.021767647936940193 = 0.014550233259797096 + 0.001 * 7.217413902282715
Epoch 600, val loss: 1.0637587308883667
Epoch 610, training loss: 0.0208586473017931 = 0.013637788593769073 + 0.001 * 7.220858097076416
Epoch 610, val loss: 1.0725315809249878
Epoch 620, training loss: 0.020022977143526077 = 0.01281033270061016 + 0.001 * 7.212645053863525
Epoch 620, val loss: 1.0810319185256958
Epoch 630, training loss: 0.019271664321422577 = 0.01205790601670742 + 0.001 * 7.213757514953613
Epoch 630, val loss: 1.0892598628997803
Epoch 640, training loss: 0.018581150099635124 = 0.011371636763215065 + 0.001 * 7.209512710571289
Epoch 640, val loss: 1.097252368927002
Epoch 650, training loss: 0.017961857840418816 = 0.010743111371994019 + 0.001 * 7.218745708465576
Epoch 650, val loss: 1.105036973953247
Epoch 660, training loss: 0.017374521121382713 = 0.010164737701416016 + 0.001 * 7.209782600402832
Epoch 660, val loss: 1.1126617193222046
Epoch 670, training loss: 0.016832273453474045 = 0.00962988380342722 + 0.001 * 7.202389240264893
Epoch 670, val loss: 1.1201876401901245
Epoch 680, training loss: 0.01633153297007084 = 0.00913379155099392 + 0.001 * 7.19774055480957
Epoch 680, val loss: 1.127612590789795
Epoch 690, training loss: 0.01586984097957611 = 0.008672626689076424 + 0.001 * 7.1972150802612305
Epoch 690, val loss: 1.1349469423294067
Epoch 700, training loss: 0.015450894832611084 = 0.008242430165410042 + 0.001 * 7.208463668823242
Epoch 700, val loss: 1.142207384109497
Epoch 710, training loss: 0.015036464668810368 = 0.007839378900825977 + 0.001 * 7.197085380554199
Epoch 710, val loss: 1.1493802070617676
Epoch 720, training loss: 0.014668749645352364 = 0.007460607215762138 + 0.001 * 7.208141803741455
Epoch 720, val loss: 1.156463384628296
Epoch 730, training loss: 0.014293868094682693 = 0.00710423244163394 + 0.001 * 7.189634799957275
Epoch 730, val loss: 1.1634728908538818
Epoch 740, training loss: 0.013983430340886116 = 0.006767522543668747 + 0.001 * 7.215908050537109
Epoch 740, val loss: 1.1703948974609375
Epoch 750, training loss: 0.01363444235175848 = 0.006448248866945505 + 0.001 * 7.186192989349365
Epoch 750, val loss: 1.177258014678955
Epoch 760, training loss: 0.013350492343306541 = 0.0061455173417925835 + 0.001 * 7.204974174499512
Epoch 760, val loss: 1.1840637922286987
Epoch 770, training loss: 0.013048207387328148 = 0.005858795717358589 + 0.001 * 7.189411163330078
Epoch 770, val loss: 1.1907840967178345
Epoch 780, training loss: 0.012774575501680374 = 0.005587823688983917 + 0.001 * 7.186751365661621
Epoch 780, val loss: 1.197413682937622
Epoch 790, training loss: 0.01251203753054142 = 0.00533245550468564 + 0.001 * 7.179581165313721
Epoch 790, val loss: 1.2039529085159302
Epoch 800, training loss: 0.01227431371808052 = 0.005092404317110777 + 0.001 * 7.18190860748291
Epoch 800, val loss: 1.210366129875183
Epoch 810, training loss: 0.012041410431265831 = 0.0048669446259737015 + 0.001 * 7.174465656280518
Epoch 810, val loss: 1.2166495323181152
Epoch 820, training loss: 0.011850770562887192 = 0.004655245691537857 + 0.001 * 7.195525169372559
Epoch 820, val loss: 1.2228208780288696
Epoch 830, training loss: 0.011621633544564247 = 0.004456859547644854 + 0.001 * 7.164773464202881
Epoch 830, val loss: 1.2288328409194946
Epoch 840, training loss: 0.01143353059887886 = 0.004270830657333136 + 0.001 * 7.162699222564697
Epoch 840, val loss: 1.2347135543823242
Epoch 850, training loss: 0.01128297671675682 = 0.0040962654165923595 + 0.001 * 7.186711311340332
Epoch 850, val loss: 1.2404654026031494
Epoch 860, training loss: 0.0111067621037364 = 0.003932597115635872 + 0.001 * 7.174164772033691
Epoch 860, val loss: 1.2460744380950928
Epoch 870, training loss: 0.010932916775345802 = 0.003778984770178795 + 0.001 * 7.153931617736816
Epoch 870, val loss: 1.2515443563461304
Epoch 880, training loss: 0.01078063901513815 = 0.0036345352418720722 + 0.001 * 7.146103382110596
Epoch 880, val loss: 1.2568905353546143
Epoch 890, training loss: 0.010654675774276257 = 0.003498586593195796 + 0.001 * 7.156088829040527
Epoch 890, val loss: 1.2620999813079834
Epoch 900, training loss: 0.010525000281631947 = 0.0033704889938235283 + 0.001 * 7.154510974884033
Epoch 900, val loss: 1.2672300338745117
Epoch 910, training loss: 0.010402241721749306 = 0.0032497430220246315 + 0.001 * 7.152498722076416
Epoch 910, val loss: 1.2722190618515015
Epoch 920, training loss: 0.010292742401361465 = 0.0031357617117464542 + 0.001 * 7.156980991363525
Epoch 920, val loss: 1.2771368026733398
Epoch 930, training loss: 0.010167363099753857 = 0.003027953440323472 + 0.001 * 7.139409065246582
Epoch 930, val loss: 1.2819558382034302
Epoch 940, training loss: 0.010069438256323338 = 0.00292574823834002 + 0.001 * 7.14369010925293
Epoch 940, val loss: 1.2867099046707153
Epoch 950, training loss: 0.009998640976846218 = 0.0028288487810641527 + 0.001 * 7.1697916984558105
Epoch 950, val loss: 1.2914000749588013
Epoch 960, training loss: 0.009889381937682629 = 0.002736853202804923 + 0.001 * 7.152528285980225
Epoch 960, val loss: 1.2960270643234253
Epoch 970, training loss: 0.009772084653377533 = 0.0026495070196688175 + 0.001 * 7.122576713562012
Epoch 970, val loss: 1.300594449043274
Epoch 980, training loss: 0.009700636379420757 = 0.002566444920375943 + 0.001 * 7.134191513061523
Epoch 980, val loss: 1.3051080703735352
Epoch 990, training loss: 0.009653997607529163 = 0.0024875218514353037 + 0.001 * 7.166475772857666
Epoch 990, val loss: 1.3095561265945435
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 1.9541585445404053 = 1.9455616474151611 + 0.001 * 8.596860885620117
Epoch 0, val loss: 1.9414634704589844
Epoch 10, training loss: 1.9443832635879517 = 1.935786485671997 + 0.001 * 8.596820831298828
Epoch 10, val loss: 1.93211829662323
Epoch 20, training loss: 1.9325708150863647 = 1.9239741563796997 + 0.001 * 8.596683502197266
Epoch 20, val loss: 1.9205145835876465
Epoch 30, training loss: 1.9163461923599243 = 1.9077497720718384 + 0.001 * 8.596379280090332
Epoch 30, val loss: 1.9042085409164429
Epoch 40, training loss: 1.89277982711792 = 1.8841842412948608 + 0.001 * 8.595641136169434
Epoch 40, val loss: 1.8804703950881958
Epoch 50, training loss: 1.8596540689468384 = 1.8510605096817017 + 0.001 * 8.593509674072266
Epoch 50, val loss: 1.8480441570281982
Epoch 60, training loss: 1.8205070495605469 = 1.8119205236434937 + 0.001 * 8.586527824401855
Epoch 60, val loss: 1.8128215074539185
Epoch 70, training loss: 1.785630702972412 = 1.7770708799362183 + 0.001 * 8.55978775024414
Epoch 70, val loss: 1.7844244241714478
Epoch 80, training loss: 1.7470550537109375 = 1.7386412620544434 + 0.001 * 8.413824081420898
Epoch 80, val loss: 1.750122308731079
Epoch 90, training loss: 1.692685604095459 = 1.6845413446426392 + 0.001 * 8.144224166870117
Epoch 90, val loss: 1.7015070915222168
Epoch 100, training loss: 1.6188242435455322 = 1.6108160018920898 + 0.001 * 8.008293151855469
Epoch 100, val loss: 1.6377285718917847
Epoch 110, training loss: 1.5272538661956787 = 1.5194249153137207 + 0.001 * 7.82896089553833
Epoch 110, val loss: 1.5606731176376343
Epoch 120, training loss: 1.4257807731628418 = 1.4181722402572632 + 0.001 * 7.60849142074585
Epoch 120, val loss: 1.476235032081604
Epoch 130, training loss: 1.3196170330047607 = 1.312057614326477 + 0.001 * 7.5594706535339355
Epoch 130, val loss: 1.3890663385391235
Epoch 140, training loss: 1.210234522819519 = 1.2027578353881836 + 0.001 * 7.476742744445801
Epoch 140, val loss: 1.3012968301773071
Epoch 150, training loss: 1.1001954078674316 = 1.092759609222412 + 0.001 * 7.435803413391113
Epoch 150, val loss: 1.2152944803237915
Epoch 160, training loss: 0.9934683442115784 = 0.9860491156578064 + 0.001 * 7.419250011444092
Epoch 160, val loss: 1.1341513395309448
Epoch 170, training loss: 0.8935739398002625 = 0.8861690759658813 + 0.001 * 7.4048638343811035
Epoch 170, val loss: 1.0605045557022095
Epoch 180, training loss: 0.8025806546211243 = 0.7951855063438416 + 0.001 * 7.395163536071777
Epoch 180, val loss: 0.9961342215538025
Epoch 190, training loss: 0.7214058637619019 = 0.7140156030654907 + 0.001 * 7.390256404876709
Epoch 190, val loss: 0.9418254494667053
Epoch 200, training loss: 0.6496371030807495 = 0.6422492861747742 + 0.001 * 7.3877973556518555
Epoch 200, val loss: 0.8979911208152771
Epoch 210, training loss: 0.5857498645782471 = 0.5783634781837463 + 0.001 * 7.386359214782715
Epoch 210, val loss: 0.863860011100769
Epoch 220, training loss: 0.5281729102134705 = 0.5207880735397339 + 0.001 * 7.384826183319092
Epoch 220, val loss: 0.8380270004272461
Epoch 230, training loss: 0.4757860004901886 = 0.46840327978134155 + 0.001 * 7.3827223777771
Epoch 230, val loss: 0.8187965750694275
Epoch 240, training loss: 0.4277436435222626 = 0.4203639626502991 + 0.001 * 7.3796892166137695
Epoch 240, val loss: 0.8043447136878967
Epoch 250, training loss: 0.38343143463134766 = 0.3760560154914856 + 0.001 * 7.3754072189331055
Epoch 250, val loss: 0.7934980392456055
Epoch 260, training loss: 0.3424663841724396 = 0.33509618043899536 + 0.001 * 7.370204448699951
Epoch 260, val loss: 0.7855191826820374
Epoch 270, training loss: 0.3046742081642151 = 0.2973136305809021 + 0.001 * 7.360565662384033
Epoch 270, val loss: 0.7798616886138916
Epoch 280, training loss: 0.27007386088371277 = 0.26272818446159363 + 0.001 * 7.345664024353027
Epoch 280, val loss: 0.7763141393661499
Epoch 290, training loss: 0.23876215517520905 = 0.23142988979816437 + 0.001 * 7.332269191741943
Epoch 290, val loss: 0.7748459577560425
Epoch 300, training loss: 0.2107948362827301 = 0.20347976684570312 + 0.001 * 7.315064430236816
Epoch 300, val loss: 0.7754129767417908
Epoch 310, training loss: 0.1861492097377777 = 0.17881430685520172 + 0.001 * 7.334904670715332
Epoch 310, val loss: 0.7780354022979736
Epoch 320, training loss: 0.16455113887786865 = 0.15726156532764435 + 0.001 * 7.289578914642334
Epoch 320, val loss: 0.782543420791626
Epoch 330, training loss: 0.145810067653656 = 0.1385210007429123 + 0.001 * 7.289069652557373
Epoch 330, val loss: 0.788622260093689
Epoch 340, training loss: 0.12952964007854462 = 0.12224945425987244 + 0.001 * 7.280186653137207
Epoch 340, val loss: 0.7959519028663635
Epoch 350, training loss: 0.11538533121347427 = 0.10810974985361099 + 0.001 * 7.275579452514648
Epoch 350, val loss: 0.8042359352111816
Epoch 360, training loss: 0.1030661091208458 = 0.0957925096154213 + 0.001 * 7.27359676361084
Epoch 360, val loss: 0.8132380247116089
Epoch 370, training loss: 0.09230735898017883 = 0.08503421396017075 + 0.001 * 7.273141860961914
Epoch 370, val loss: 0.8227856159210205
Epoch 380, training loss: 0.08288881182670593 = 0.07561662793159485 + 0.001 * 7.272187232971191
Epoch 380, val loss: 0.8327684998512268
Epoch 390, training loss: 0.07463574409484863 = 0.06736519932746887 + 0.001 * 7.27054500579834
Epoch 390, val loss: 0.8430164456367493
Epoch 400, training loss: 0.06740454584360123 = 0.06013345718383789 + 0.001 * 7.2710862159729
Epoch 400, val loss: 0.8533992171287537
Epoch 410, training loss: 0.06107228249311447 = 0.053796447813510895 + 0.001 * 7.2758331298828125
Epoch 410, val loss: 0.8638418912887573
Epoch 420, training loss: 0.055515602231025696 = 0.048245225101709366 + 0.001 * 7.2703776359558105
Epoch 420, val loss: 0.8742809891700745
Epoch 430, training loss: 0.05065004527568817 = 0.04338141530752182 + 0.001 * 7.2686285972595215
Epoch 430, val loss: 0.8846496343612671
Epoch 440, training loss: 0.04638846963644028 = 0.03911840170621872 + 0.001 * 7.270066261291504
Epoch 440, val loss: 0.8949041366577148
Epoch 450, training loss: 0.04264308512210846 = 0.03537696227431297 + 0.001 * 7.2661237716674805
Epoch 450, val loss: 0.9050241112709045
Epoch 460, training loss: 0.0393504723906517 = 0.03208262473344803 + 0.001 * 7.267847537994385
Epoch 460, val loss: 0.9149962067604065
Epoch 470, training loss: 0.036436889320611954 = 0.02917393669486046 + 0.001 * 7.262953281402588
Epoch 470, val loss: 0.9247918128967285
Epoch 480, training loss: 0.03386743739247322 = 0.0265995804220438 + 0.001 * 7.267858028411865
Epoch 480, val loss: 0.9344441890716553
Epoch 490, training loss: 0.03157820925116539 = 0.024317484349012375 + 0.001 * 7.260725498199463
Epoch 490, val loss: 0.9439137578010559
Epoch 500, training loss: 0.0295477956533432 = 0.02229175716638565 + 0.001 * 7.25603723526001
Epoch 500, val loss: 0.9532060623168945
Epoch 510, training loss: 0.027746565639972687 = 0.020490862429142 + 0.001 * 7.255703926086426
Epoch 510, val loss: 0.9623008370399475
Epoch 520, training loss: 0.02614102140069008 = 0.018887221813201904 + 0.001 * 7.2537994384765625
Epoch 520, val loss: 0.9711986184120178
Epoch 530, training loss: 0.024722540751099586 = 0.017456423491239548 + 0.001 * 7.266116619110107
Epoch 530, val loss: 0.9798998832702637
Epoch 540, training loss: 0.02342720702290535 = 0.016177034005522728 + 0.001 * 7.250173091888428
Epoch 540, val loss: 0.9883800148963928
Epoch 550, training loss: 0.022281330078840256 = 0.015030233189463615 + 0.001 * 7.251096248626709
Epoch 550, val loss: 0.9966630935668945
Epoch 560, training loss: 0.021268460899591446 = 0.01399977132678032 + 0.001 * 7.26869010925293
Epoch 560, val loss: 1.004743218421936
Epoch 570, training loss: 0.020320776849985123 = 0.01307164877653122 + 0.001 * 7.249127388000488
Epoch 570, val loss: 1.0126179456710815
Epoch 580, training loss: 0.019476108253002167 = 0.012233580462634563 + 0.001 * 7.242528438568115
Epoch 580, val loss: 1.0203051567077637
Epoch 590, training loss: 0.018714725971221924 = 0.011474684812128544 + 0.001 * 7.2400407791137695
Epoch 590, val loss: 1.027819275856018
Epoch 600, training loss: 0.01802453026175499 = 0.010785818099975586 + 0.001 * 7.238711833953857
Epoch 600, val loss: 1.035135269165039
Epoch 610, training loss: 0.01739603281021118 = 0.010158856399357319 + 0.001 * 7.237175464630127
Epoch 610, val loss: 1.0422875881195068
Epoch 620, training loss: 0.01681561768054962 = 0.009586743079125881 + 0.001 * 7.228873252868652
Epoch 620, val loss: 1.0492557287216187
Epoch 630, training loss: 0.01629672385752201 = 0.009063336998224258 + 0.001 * 7.233386516571045
Epoch 630, val loss: 1.0560553073883057
Epoch 640, training loss: 0.01582336612045765 = 0.008583410643041134 + 0.001 * 7.239954471588135
Epoch 640, val loss: 1.0627021789550781
Epoch 650, training loss: 0.015371320769190788 = 0.00814245268702507 + 0.001 * 7.228867530822754
Epoch 650, val loss: 1.0691642761230469
Epoch 660, training loss: 0.01497563160955906 = 0.007736446801573038 + 0.001 * 7.2391839027404785
Epoch 660, val loss: 1.0754973888397217
Epoch 670, training loss: 0.014590874314308167 = 0.007361792493611574 + 0.001 * 7.229081153869629
Epoch 670, val loss: 1.0816715955734253
Epoch 680, training loss: 0.014238097704946995 = 0.0070154196582734585 + 0.001 * 7.222677707672119
Epoch 680, val loss: 1.0877125263214111
Epoch 690, training loss: 0.013915027491748333 = 0.006694523151963949 + 0.001 * 7.220503807067871
Epoch 690, val loss: 1.0936256647109985
Epoch 700, training loss: 0.013617929071187973 = 0.006396750453859568 + 0.001 * 7.221177577972412
Epoch 700, val loss: 1.099402666091919
Epoch 710, training loss: 0.013325406238436699 = 0.00611991947516799 + 0.001 * 7.205486297607422
Epoch 710, val loss: 1.1050552129745483
Epoch 720, training loss: 0.013064160011708736 = 0.005862126592546701 + 0.001 * 7.202033042907715
Epoch 720, val loss: 1.1105852127075195
Epoch 730, training loss: 0.012857424095273018 = 0.005621681455522776 + 0.001 * 7.235742092132568
Epoch 730, val loss: 1.1159862279891968
Epoch 740, training loss: 0.012606998905539513 = 0.005397059489041567 + 0.001 * 7.209939002990723
Epoch 740, val loss: 1.1212871074676514
Epoch 750, training loss: 0.012433500960469246 = 0.005186901893466711 + 0.001 * 7.246598243713379
Epoch 750, val loss: 1.126460075378418
Epoch 760, training loss: 0.012182267382740974 = 0.004990003537386656 + 0.001 * 7.192263126373291
Epoch 760, val loss: 1.1315326690673828
Epoch 770, training loss: 0.011991693638265133 = 0.004805273842066526 + 0.001 * 7.186419486999512
Epoch 770, val loss: 1.1365019083023071
Epoch 780, training loss: 0.011807404458522797 = 0.004631678573787212 + 0.001 * 7.17572546005249
Epoch 780, val loss: 1.1413675546646118
Epoch 790, training loss: 0.011644922196865082 = 0.004468364641070366 + 0.001 * 7.176556587219238
Epoch 790, val loss: 1.146131992340088
Epoch 800, training loss: 0.011501631699502468 = 0.004314536694437265 + 0.001 * 7.187094688415527
Epoch 800, val loss: 1.1508232355117798
Epoch 810, training loss: 0.011372718960046768 = 0.0041694846004247665 + 0.001 * 7.20323371887207
Epoch 810, val loss: 1.1553975343704224
Epoch 820, training loss: 0.011237062513828278 = 0.00403256481513381 + 0.001 * 7.204497337341309
Epoch 820, val loss: 1.1598927974700928
Epoch 830, training loss: 0.011110926046967506 = 0.003903163829818368 + 0.001 * 7.207762241363525
Epoch 830, val loss: 1.1643034219741821
Epoch 840, training loss: 0.010951248928904533 = 0.003780761733651161 + 0.001 * 7.170487403869629
Epoch 840, val loss: 1.1686216592788696
Epoch 850, training loss: 0.010842991061508656 = 0.003664851188659668 + 0.001 * 7.178139686584473
Epoch 850, val loss: 1.1728800535202026
Epoch 860, training loss: 0.010699390433728695 = 0.0035549954045563936 + 0.001 * 7.144394874572754
Epoch 860, val loss: 1.1770553588867188
Epoch 870, training loss: 0.010622376576066017 = 0.0034507657401263714 + 0.001 * 7.1716108322143555
Epoch 870, val loss: 1.1811554431915283
Epoch 880, training loss: 0.010505705140531063 = 0.0033517847768962383 + 0.001 * 7.1539201736450195
Epoch 880, val loss: 1.185180902481079
Epoch 890, training loss: 0.010447666980326176 = 0.003257713047787547 + 0.001 * 7.189953327178955
Epoch 890, val loss: 1.1891429424285889
Epoch 900, training loss: 0.010325111448764801 = 0.0031682560220360756 + 0.001 * 7.15685510635376
Epoch 900, val loss: 1.193001389503479
Epoch 910, training loss: 0.010244826786220074 = 0.0030830767937004566 + 0.001 * 7.161749839782715
Epoch 910, val loss: 1.1968235969543457
Epoch 920, training loss: 0.010168402455747128 = 0.003001911798492074 + 0.001 * 7.166490077972412
Epoch 920, val loss: 1.2005494832992554
Epoch 930, training loss: 0.01006508432328701 = 0.0029245149344205856 + 0.001 * 7.140568733215332
Epoch 930, val loss: 1.204229474067688
Epoch 940, training loss: 0.010006362572312355 = 0.0028506643138825893 + 0.001 * 7.155698299407959
Epoch 940, val loss: 1.2078710794448853
Epoch 950, training loss: 0.009890655055642128 = 0.0027801417745649815 + 0.001 * 7.1105122566223145
Epoch 950, val loss: 1.211414098739624
Epoch 960, training loss: 0.009910487569868565 = 0.00271277385763824 + 0.001 * 7.197713375091553
Epoch 960, val loss: 1.2149285078048706
Epoch 970, training loss: 0.009761488065123558 = 0.002648361027240753 + 0.001 * 7.113126754760742
Epoch 970, val loss: 1.2183666229248047
Epoch 980, training loss: 0.009870962239801884 = 0.002586726099252701 + 0.001 * 7.284235954284668
Epoch 980, val loss: 1.221782922744751
Epoch 990, training loss: 0.009635101072490215 = 0.0025277435779571533 + 0.001 * 7.107357025146484
Epoch 990, val loss: 1.225070834159851
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 1.957039713859558 = 1.948442816734314 + 0.001 * 8.59684944152832
Epoch 0, val loss: 1.9488812685012817
Epoch 10, training loss: 1.947054386138916 = 1.9384576082229614 + 0.001 * 8.596806526184082
Epoch 10, val loss: 1.9384381771087646
Epoch 20, training loss: 1.9349849224090576 = 1.9263882637023926 + 0.001 * 8.596627235412598
Epoch 20, val loss: 1.9256047010421753
Epoch 30, training loss: 1.9182853698730469 = 1.90968918800354 + 0.001 * 8.596198081970215
Epoch 30, val loss: 1.9077922105789185
Epoch 40, training loss: 1.8938195705413818 = 1.885224461555481 + 0.001 * 8.595131874084473
Epoch 40, val loss: 1.8818697929382324
Epoch 50, training loss: 1.8593586683273315 = 1.8507667779922485 + 0.001 * 8.59192943572998
Epoch 50, val loss: 1.846652626991272
Epoch 60, training loss: 1.820249319076538 = 1.8116687536239624 + 0.001 * 8.580620765686035
Epoch 60, val loss: 1.8105138540267944
Epoch 70, training loss: 1.7874006032943726 = 1.7788721323013306 + 0.001 * 8.52846908569336
Epoch 70, val loss: 1.7839300632476807
Epoch 80, training loss: 1.7493295669555664 = 1.7410608530044556 + 0.001 * 8.268709182739258
Epoch 80, val loss: 1.7511581182479858
Epoch 90, training loss: 1.6972512006759644 = 1.689153790473938 + 0.001 * 8.097461700439453
Epoch 90, val loss: 1.7041172981262207
Epoch 100, training loss: 1.6257069110870361 = 1.6179097890853882 + 0.001 * 7.7971086502075195
Epoch 100, val loss: 1.6407116651535034
Epoch 110, training loss: 1.5365993976593018 = 1.5290286540985107 + 0.001 * 7.570772647857666
Epoch 110, val loss: 1.5653290748596191
Epoch 120, training loss: 1.43755304813385 = 1.4300355911254883 + 0.001 * 7.51743221282959
Epoch 120, val loss: 1.4844504594802856
Epoch 130, training loss: 1.334509015083313 = 1.3270418643951416 + 0.001 * 7.467165946960449
Epoch 130, val loss: 1.4010121822357178
Epoch 140, training loss: 1.2307778596878052 = 1.223339557647705 + 0.001 * 7.4383463859558105
Epoch 140, val loss: 1.319701910018921
Epoch 150, training loss: 1.130004644393921 = 1.122593879699707 + 0.001 * 7.410816669464111
Epoch 150, val loss: 1.2428417205810547
Epoch 160, training loss: 1.036285161972046 = 1.0288926362991333 + 0.001 * 7.392495155334473
Epoch 160, val loss: 1.1739184856414795
Epoch 170, training loss: 0.9510201811790466 = 0.9436336755752563 + 0.001 * 7.386490821838379
Epoch 170, val loss: 1.1132583618164062
Epoch 180, training loss: 0.8726926445960999 = 0.865309476852417 + 0.001 * 7.383147239685059
Epoch 180, val loss: 1.0589048862457275
Epoch 190, training loss: 0.7993610501289368 = 0.7919819355010986 + 0.001 * 7.379128456115723
Epoch 190, val loss: 1.0090199708938599
Epoch 200, training loss: 0.730495035648346 = 0.7231200933456421 + 0.001 * 7.3749566078186035
Epoch 200, val loss: 0.9632081389427185
Epoch 210, training loss: 0.6664645075798035 = 0.6590945720672607 + 0.001 * 7.369933605194092
Epoch 210, val loss: 0.9224576950073242
Epoch 220, training loss: 0.6071204543113708 = 0.5997569561004639 + 0.001 * 7.363491535186768
Epoch 220, val loss: 0.8872138261795044
Epoch 230, training loss: 0.5515174865722656 = 0.5441617369651794 + 0.001 * 7.35575008392334
Epoch 230, val loss: 0.8573507070541382
Epoch 240, training loss: 0.49856096506118774 = 0.49120911955833435 + 0.001 * 7.35185432434082
Epoch 240, val loss: 0.8326650261878967
Epoch 250, training loss: 0.4474948048591614 = 0.44015592336654663 + 0.001 * 7.338872909545898
Epoch 250, val loss: 0.8126562237739563
Epoch 260, training loss: 0.3980907201766968 = 0.39076119661331177 + 0.001 * 7.329536437988281
Epoch 260, val loss: 0.7974305748939514
Epoch 270, training loss: 0.350599467754364 = 0.34328413009643555 + 0.001 * 7.315342903137207
Epoch 270, val loss: 0.7867904901504517
Epoch 280, training loss: 0.30579790472984314 = 0.2984869182109833 + 0.001 * 7.310983180999756
Epoch 280, val loss: 0.7807556390762329
Epoch 290, training loss: 0.2647240459918976 = 0.2574300765991211 + 0.001 * 7.293980598449707
Epoch 290, val loss: 0.7792812585830688
Epoch 300, training loss: 0.22825562953948975 = 0.2209717333316803 + 0.001 * 7.283899784088135
Epoch 300, val loss: 0.7821323871612549
Epoch 310, training loss: 0.19674639403820038 = 0.18947471678256989 + 0.001 * 7.271678924560547
Epoch 310, val loss: 0.7884663939476013
Epoch 320, training loss: 0.17005175352096558 = 0.1627868115901947 + 0.001 * 7.264936923980713
Epoch 320, val loss: 0.7977702021598816
Epoch 330, training loss: 0.1476641744375229 = 0.14039084315299988 + 0.001 * 7.273336410522461
Epoch 330, val loss: 0.8093166351318359
Epoch 340, training loss: 0.1288841813802719 = 0.12163199484348297 + 0.001 * 7.252191543579102
Epoch 340, val loss: 0.8225793838500977
Epoch 350, training loss: 0.11312703043222427 = 0.1058761328458786 + 0.001 * 7.250894069671631
Epoch 350, val loss: 0.8370962738990784
Epoch 360, training loss: 0.09983574599027634 = 0.09258630871772766 + 0.001 * 7.24943733215332
Epoch 360, val loss: 0.8524578213691711
Epoch 370, training loss: 0.08857542276382446 = 0.08132816851139069 + 0.001 * 7.247252464294434
Epoch 370, val loss: 0.868454098701477
Epoch 380, training loss: 0.07899417728185654 = 0.07175092399120331 + 0.001 * 7.243254661560059
Epoch 380, val loss: 0.8848010301589966
Epoch 390, training loss: 0.07081389427185059 = 0.06356776505708694 + 0.001 * 7.246127605438232
Epoch 390, val loss: 0.901383638381958
Epoch 400, training loss: 0.0637841522693634 = 0.056547947227954865 + 0.001 * 7.236203193664551
Epoch 400, val loss: 0.9179994463920593
Epoch 410, training loss: 0.0577511191368103 = 0.05050234869122505 + 0.001 * 7.248770713806152
Epoch 410, val loss: 0.9345259070396423
Epoch 420, training loss: 0.05250683054327965 = 0.04527933895587921 + 0.001 * 7.227492809295654
Epoch 420, val loss: 0.9508422017097473
Epoch 430, training loss: 0.0479816272854805 = 0.04075298830866814 + 0.001 * 7.228639602661133
Epoch 430, val loss: 0.9668876528739929
Epoch 440, training loss: 0.04404323548078537 = 0.03681446611881256 + 0.001 * 7.228770732879639
Epoch 440, val loss: 0.9826033711433411
Epoch 450, training loss: 0.0405975878238678 = 0.03337447717785835 + 0.001 * 7.223110675811768
Epoch 450, val loss: 0.9979528188705444
Epoch 460, training loss: 0.0375763438642025 = 0.030359413474798203 + 0.001 * 7.216930389404297
Epoch 460, val loss: 1.0129185914993286
Epoch 470, training loss: 0.034938301891088486 = 0.027707824483513832 + 0.001 * 7.230476379394531
Epoch 470, val loss: 1.0274927616119385
Epoch 480, training loss: 0.03257904574275017 = 0.025370430201292038 + 0.001 * 7.208616256713867
Epoch 480, val loss: 1.041666865348816
Epoch 490, training loss: 0.03052019700407982 = 0.023303158581256866 + 0.001 * 7.217038154602051
Epoch 490, val loss: 1.0554170608520508
Epoch 500, training loss: 0.028682341799139977 = 0.02146964520215988 + 0.001 * 7.212696552276611
Epoch 500, val loss: 1.0687357187271118
Epoch 510, training loss: 0.027036868035793304 = 0.01983809284865856 + 0.001 * 7.198775768280029
Epoch 510, val loss: 1.0816240310668945
Epoch 520, training loss: 0.02558724209666252 = 0.01838204264640808 + 0.001 * 7.205199241638184
Epoch 520, val loss: 1.0941414833068848
Epoch 530, training loss: 0.024274809285998344 = 0.0170787051320076 + 0.001 * 7.196104049682617
Epoch 530, val loss: 1.1062297821044922
Epoch 540, training loss: 0.02309635654091835 = 0.015908578410744667 + 0.001 * 7.187778949737549
Epoch 540, val loss: 1.1179592609405518
Epoch 550, training loss: 0.022048568353056908 = 0.014854930341243744 + 0.001 * 7.193637847900391
Epoch 550, val loss: 1.1293209791183472
Epoch 560, training loss: 0.021090637892484665 = 0.01390361599624157 + 0.001 * 7.187021732330322
Epoch 560, val loss: 1.140334963798523
Epoch 570, training loss: 0.020214617252349854 = 0.013042028062045574 + 0.001 * 7.17258882522583
Epoch 570, val loss: 1.151008129119873
Epoch 580, training loss: 0.01944415457546711 = 0.012259886600077152 + 0.001 * 7.184267997741699
Epoch 580, val loss: 1.1613550186157227
Epoch 590, training loss: 0.018734825775027275 = 0.011547770351171494 + 0.001 * 7.187055587768555
Epoch 590, val loss: 1.1713659763336182
Epoch 600, training loss: 0.01806611195206642 = 0.010897906497120857 + 0.001 * 7.168205261230469
Epoch 600, val loss: 1.1811028718948364
Epoch 610, training loss: 0.017473254352808 = 0.010303275659680367 + 0.001 * 7.169978141784668
Epoch 610, val loss: 1.190537929534912
Epoch 620, training loss: 0.01692461408674717 = 0.0097579350695014 + 0.001 * 7.166678428649902
Epoch 620, val loss: 1.1996827125549316
Epoch 630, training loss: 0.016420533880591393 = 0.009256672114133835 + 0.001 * 7.163861274719238
Epoch 630, val loss: 1.2085816860198975
Epoch 640, training loss: 0.01595531776547432 = 0.008794980123639107 + 0.001 * 7.160336494445801
Epoch 640, val loss: 1.217225193977356
Epoch 650, training loss: 0.015510836616158485 = 0.008368906565010548 + 0.001 * 7.141929626464844
Epoch 650, val loss: 1.2255975008010864
Epoch 660, training loss: 0.015131480991840363 = 0.007974877953529358 + 0.001 * 7.1566033363342285
Epoch 660, val loss: 1.2337461709976196
Epoch 670, training loss: 0.014772651717066765 = 0.007609681226313114 + 0.001 * 7.162970542907715
Epoch 670, val loss: 1.2416741847991943
Epoch 680, training loss: 0.014413491822779179 = 0.007270718924701214 + 0.001 * 7.142772674560547
Epoch 680, val loss: 1.249373435974121
Epoch 690, training loss: 0.014126477763056755 = 0.006955559831112623 + 0.001 * 7.170917987823486
Epoch 690, val loss: 1.2568501234054565
Epoch 700, training loss: 0.01379770040512085 = 0.006661924533545971 + 0.001 * 7.135775089263916
Epoch 700, val loss: 1.2641394138336182
Epoch 710, training loss: 0.013526435010135174 = 0.006388046313077211 + 0.001 * 7.138388156890869
Epoch 710, val loss: 1.2712372541427612
Epoch 720, training loss: 0.01334376260638237 = 0.006132118869572878 + 0.001 * 7.211642742156982
Epoch 720, val loss: 1.2781431674957275
Epoch 730, training loss: 0.013032166287302971 = 0.00589266000315547 + 0.001 * 7.139505386352539
Epoch 730, val loss: 1.2848966121673584
Epoch 740, training loss: 0.012800141237676144 = 0.00566828204318881 + 0.001 * 7.131858825683594
Epoch 740, val loss: 1.2914373874664307
Epoch 750, training loss: 0.012596319429576397 = 0.005457762628793716 + 0.001 * 7.138556480407715
Epoch 750, val loss: 1.2978112697601318
Epoch 760, training loss: 0.01237998902797699 = 0.0052598766051232815 + 0.001 * 7.120111465454102
Epoch 760, val loss: 1.304056167602539
Epoch 770, training loss: 0.012188652530312538 = 0.005073665175586939 + 0.001 * 7.114986896514893
Epoch 770, val loss: 1.3101222515106201
Epoch 780, training loss: 0.011998925358057022 = 0.004898269660770893 + 0.001 * 7.100655555725098
Epoch 780, val loss: 1.3160406351089478
Epoch 790, training loss: 0.011842665262520313 = 0.004732835106551647 + 0.001 * 7.109829902648926
Epoch 790, val loss: 1.3218289613723755
Epoch 800, training loss: 0.011685997247695923 = 0.004576599225401878 + 0.001 * 7.109397888183594
Epoch 800, val loss: 1.3274626731872559
Epoch 810, training loss: 0.011532561853528023 = 0.004428956191986799 + 0.001 * 7.103604793548584
Epoch 810, val loss: 1.3329856395721436
Epoch 820, training loss: 0.01139373891055584 = 0.004289234988391399 + 0.001 * 7.104503631591797
Epoch 820, val loss: 1.3383467197418213
Epoch 830, training loss: 0.01126248762011528 = 0.004156916402280331 + 0.001 * 7.1055707931518555
Epoch 830, val loss: 1.3436188697814941
Epoch 840, training loss: 0.011123130097985268 = 0.00403145095333457 + 0.001 * 7.091679096221924
Epoch 840, val loss: 1.3487505912780762
Epoch 850, training loss: 0.011034085415303707 = 0.003912358079105616 + 0.001 * 7.121726989746094
Epoch 850, val loss: 1.3537707328796387
Epoch 860, training loss: 0.010904714465141296 = 0.003799241501837969 + 0.001 * 7.105473041534424
Epoch 860, val loss: 1.3586633205413818
Epoch 870, training loss: 0.010757514275610447 = 0.003691786201670766 + 0.001 * 7.065728187561035
Epoch 870, val loss: 1.3634705543518066
Epoch 880, training loss: 0.010663358494639397 = 0.00358951766975224 + 0.001 * 7.073840618133545
Epoch 880, val loss: 1.368120789527893
Epoch 890, training loss: 0.010589754208922386 = 0.003492133691906929 + 0.001 * 7.097620010375977
Epoch 890, val loss: 1.3727039098739624
Epoch 900, training loss: 0.010471150279045105 = 0.0033992996904999018 + 0.001 * 7.071849822998047
Epoch 900, val loss: 1.3771781921386719
Epoch 910, training loss: 0.010385829024016857 = 0.0033107807394117117 + 0.001 * 7.075047492980957
Epoch 910, val loss: 1.3815624713897705
Epoch 920, training loss: 0.010322588495910168 = 0.0032262697350233793 + 0.001 * 7.09631872177124
Epoch 920, val loss: 1.3858221769332886
Epoch 930, training loss: 0.010232007130980492 = 0.0031455508433282375 + 0.001 * 7.086455821990967
Epoch 930, val loss: 1.3900322914123535
Epoch 940, training loss: 0.010139998979866505 = 0.003068390302360058 + 0.001 * 7.071608543395996
Epoch 940, val loss: 1.3941340446472168
Epoch 950, training loss: 0.010045036673545837 = 0.002994571113958955 + 0.001 * 7.050465106964111
Epoch 950, val loss: 1.3981664180755615
Epoch 960, training loss: 0.009979324415326118 = 0.0029238974675536156 + 0.001 * 7.055426120758057
Epoch 960, val loss: 1.4020980596542358
Epoch 970, training loss: 0.009935915470123291 = 0.002856197999790311 + 0.001 * 7.079716682434082
Epoch 970, val loss: 1.4059758186340332
Epoch 980, training loss: 0.009830666705965996 = 0.002791354199871421 + 0.001 * 7.03931188583374
Epoch 980, val loss: 1.4097411632537842
Epoch 990, training loss: 0.00978281069546938 = 0.0027291944716125727 + 0.001 * 7.053615570068359
Epoch 990, val loss: 1.413421630859375
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.797575118608329
The final CL Acc:0.76667, 0.01571, The final GNN Acc:0.81216, 0.01037
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13182])
remove edge: torch.Size([2, 7832])
updated graph: torch.Size([2, 10458])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.94634211063385 = 1.937745213508606 + 0.001 * 8.596863746643066
Epoch 0, val loss: 1.9327892065048218
Epoch 10, training loss: 1.9375098943710327 = 1.9289131164550781 + 0.001 * 8.596834182739258
Epoch 10, val loss: 1.9246324300765991
Epoch 20, training loss: 1.9269142150878906 = 1.9183175563812256 + 0.001 * 8.59671688079834
Epoch 20, val loss: 1.9146027565002441
Epoch 30, training loss: 1.9123787879943848 = 1.9037823677062988 + 0.001 * 8.59644603729248
Epoch 30, val loss: 1.9004803895950317
Epoch 40, training loss: 1.8911195993423462 = 1.882523775100708 + 0.001 * 8.595779418945312
Epoch 40, val loss: 1.8797292709350586
Epoch 50, training loss: 1.8603496551513672 = 1.8517557382583618 + 0.001 * 8.593889236450195
Epoch 50, val loss: 1.8505631685256958
Epoch 60, training loss: 1.8213707208633423 = 1.8127832412719727 + 0.001 * 8.58750057220459
Epoch 60, val loss: 1.8162742853164673
Epoch 70, training loss: 1.782321810722351 = 1.7737622261047363 + 0.001 * 8.559613227844238
Epoch 70, val loss: 1.7848761081695557
Epoch 80, training loss: 1.7385869026184082 = 1.7302074432373047 + 0.001 * 8.379514694213867
Epoch 80, val loss: 1.7466742992401123
Epoch 90, training loss: 1.6770923137664795 = 1.6689757108688354 + 0.001 * 8.116641998291016
Epoch 90, val loss: 1.6907122135162354
Epoch 100, training loss: 1.595456838607788 = 1.5874472856521606 + 0.001 * 8.009504318237305
Epoch 100, val loss: 1.621314525604248
Epoch 110, training loss: 1.4995522499084473 = 1.4916752576828003 + 0.001 * 7.876981258392334
Epoch 110, val loss: 1.5443063974380493
Epoch 120, training loss: 1.4019970893859863 = 1.394307017326355 + 0.001 * 7.690130710601807
Epoch 120, val loss: 1.4673991203308105
Epoch 130, training loss: 1.3078372478485107 = 1.3001989126205444 + 0.001 * 7.638306617736816
Epoch 130, val loss: 1.3944346904754639
Epoch 140, training loss: 1.2162278890609741 = 1.2086644172668457 + 0.001 * 7.563436031341553
Epoch 140, val loss: 1.3244092464447021
Epoch 150, training loss: 1.1277379989624023 = 1.120284914970398 + 0.001 * 7.453064441680908
Epoch 150, val loss: 1.2567282915115356
Epoch 160, training loss: 1.0449793338775635 = 1.0376332998275757 + 0.001 * 7.3460893630981445
Epoch 160, val loss: 1.1949903964996338
Epoch 170, training loss: 0.9695020914077759 = 0.9621984958648682 + 0.001 * 7.303570747375488
Epoch 170, val loss: 1.140920877456665
Epoch 180, training loss: 0.8995800614356995 = 0.8922997117042542 + 0.001 * 7.280352592468262
Epoch 180, val loss: 1.0915594100952148
Epoch 190, training loss: 0.8323900699615479 = 0.8251346945762634 + 0.001 * 7.255380153656006
Epoch 190, val loss: 1.044192910194397
Epoch 200, training loss: 0.7662839889526367 = 0.7590411305427551 + 0.001 * 7.242878437042236
Epoch 200, val loss: 0.9970331192016602
Epoch 210, training loss: 0.7012971043586731 = 0.6940610408782959 + 0.001 * 7.2360639572143555
Epoch 210, val loss: 0.9507186412811279
Epoch 220, training loss: 0.6385270357131958 = 0.6312933564186096 + 0.001 * 7.2336883544921875
Epoch 220, val loss: 0.9068173170089722
Epoch 230, training loss: 0.578961968421936 = 0.5717309713363647 + 0.001 * 7.230987071990967
Epoch 230, val loss: 0.8671512007713318
Epoch 240, training loss: 0.523110032081604 = 0.5158812403678894 + 0.001 * 7.228800296783447
Epoch 240, val loss: 0.832560658454895
Epoch 250, training loss: 0.47083765268325806 = 0.46361014246940613 + 0.001 * 7.227510929107666
Epoch 250, val loss: 0.8029998540878296
Epoch 260, training loss: 0.42166849970817566 = 0.41444212198257446 + 0.001 * 7.2263641357421875
Epoch 260, val loss: 0.7779039144515991
Epoch 270, training loss: 0.37516680359840393 = 0.3679414987564087 + 0.001 * 7.225301742553711
Epoch 270, val loss: 0.7567074298858643
Epoch 280, training loss: 0.3311629593372345 = 0.3239383101463318 + 0.001 * 7.224638938903809
Epoch 280, val loss: 0.7389820218086243
Epoch 290, training loss: 0.28994396328926086 = 0.28271913528442383 + 0.001 * 7.224819660186768
Epoch 290, val loss: 0.7245131134986877
Epoch 300, training loss: 0.25205427408218384 = 0.24483074247837067 + 0.001 * 7.223545551300049
Epoch 300, val loss: 0.7133049964904785
Epoch 310, training loss: 0.21802552044391632 = 0.2108023315668106 + 0.001 * 7.223195552825928
Epoch 310, val loss: 0.7057733535766602
Epoch 320, training loss: 0.1880967617034912 = 0.18087376654148102 + 0.001 * 7.222996711730957
Epoch 320, val loss: 0.7021535038948059
Epoch 330, training loss: 0.1621914952993393 = 0.1549685001373291 + 0.001 * 7.223001480102539
Epoch 330, val loss: 0.7024044990539551
Epoch 340, training loss: 0.14003971219062805 = 0.1328149139881134 + 0.001 * 7.224793910980225
Epoch 340, val loss: 0.7062631249427795
Epoch 350, training loss: 0.12125339359045029 = 0.11402948945760727 + 0.001 * 7.223906517028809
Epoch 350, val loss: 0.713453471660614
Epoch 360, training loss: 0.10542329400777817 = 0.09819896519184113 + 0.001 * 7.22432804107666
Epoch 360, val loss: 0.7232545614242554
Epoch 370, training loss: 0.09213089197874069 = 0.08490601181983948 + 0.001 * 7.224883079528809
Epoch 370, val loss: 0.735099196434021
Epoch 380, training loss: 0.08098876476287842 = 0.07376359403133392 + 0.001 * 7.225167274475098
Epoch 380, val loss: 0.7484380602836609
Epoch 390, training loss: 0.07165302336215973 = 0.06442758440971375 + 0.001 * 7.225442409515381
Epoch 390, val loss: 0.7626609206199646
Epoch 400, training loss: 0.06382033973932266 = 0.056594666093587875 + 0.001 * 7.225673675537109
Epoch 400, val loss: 0.7774327993392944
Epoch 410, training loss: 0.05723656713962555 = 0.05000884085893631 + 0.001 * 7.227725028991699
Epoch 410, val loss: 0.7924886345863342
Epoch 420, training loss: 0.051677048206329346 = 0.04444996640086174 + 0.001 * 7.227081775665283
Epoch 420, val loss: 0.8075303435325623
Epoch 430, training loss: 0.046961862593889236 = 0.03973546624183655 + 0.001 * 7.226396560668945
Epoch 430, val loss: 0.8224452137947083
Epoch 440, training loss: 0.042940109968185425 = 0.03571406751871109 + 0.001 * 7.226040363311768
Epoch 440, val loss: 0.8370804190635681
Epoch 450, training loss: 0.03949332609772682 = 0.032264430075883865 + 0.001 * 7.228894233703613
Epoch 450, val loss: 0.8513675928115845
Epoch 460, training loss: 0.03651353344321251 = 0.02928747981786728 + 0.001 * 7.226054668426514
Epoch 460, val loss: 0.865273118019104
Epoch 470, training loss: 0.03392897546291351 = 0.02670399472117424 + 0.001 * 7.224981307983398
Epoch 470, val loss: 0.8787209987640381
Epoch 480, training loss: 0.0316760428249836 = 0.024449089542031288 + 0.001 * 7.22695255279541
Epoch 480, val loss: 0.8917906284332275
Epoch 490, training loss: 0.029695704579353333 = 0.02247070148587227 + 0.001 * 7.225001811981201
Epoch 490, val loss: 0.9044278264045715
Epoch 500, training loss: 0.027955150231719017 = 0.0207261610776186 + 0.001 * 7.2289886474609375
Epoch 500, val loss: 0.9166593551635742
Epoch 510, training loss: 0.02640354633331299 = 0.01918076165020466 + 0.001 * 7.222784519195557
Epoch 510, val loss: 0.9284646511077881
Epoch 520, training loss: 0.025026902556419373 = 0.017805567011237144 + 0.001 * 7.221334934234619
Epoch 520, val loss: 0.9399010539054871
Epoch 530, training loss: 0.02379860356450081 = 0.016576724126935005 + 0.001 * 7.2218780517578125
Epoch 530, val loss: 0.9509929418563843
Epoch 540, training loss: 0.022697702050209045 = 0.015474400483071804 + 0.001 * 7.223302364349365
Epoch 540, val loss: 0.9617136716842651
Epoch 550, training loss: 0.021701088175177574 = 0.01448200736194849 + 0.001 * 7.219080924987793
Epoch 550, val loss: 0.9720942974090576
Epoch 560, training loss: 0.020802879706025124 = 0.013585459440946579 + 0.001 * 7.2174201011657715
Epoch 560, val loss: 0.982138991355896
Epoch 570, training loss: 0.020000729709863663 = 0.012772435322403908 + 0.001 * 7.228293418884277
Epoch 570, val loss: 0.9919126033782959
Epoch 580, training loss: 0.019249076023697853 = 0.012031185440719128 + 0.001 * 7.217890739440918
Epoch 580, val loss: 1.001448631286621
Epoch 590, training loss: 0.018567495048046112 = 0.011352770030498505 + 0.001 * 7.214724063873291
Epoch 590, val loss: 1.0107481479644775
Epoch 600, training loss: 0.017942514270544052 = 0.010728503577411175 + 0.001 * 7.214011192321777
Epoch 600, val loss: 1.0198971033096313
Epoch 610, training loss: 0.017360426485538483 = 0.010151674039661884 + 0.001 * 7.208751201629639
Epoch 610, val loss: 1.0289424657821655
Epoch 620, training loss: 0.016833409667015076 = 0.009617321193218231 + 0.001 * 7.216087818145752
Epoch 620, val loss: 1.037880301475525
Epoch 630, training loss: 0.016328072175383568 = 0.009121643379330635 + 0.001 * 7.2064290046691895
Epoch 630, val loss: 1.0467073917388916
Epoch 640, training loss: 0.01586713269352913 = 0.008661486208438873 + 0.001 * 7.2056450843811035
Epoch 640, val loss: 1.055418610572815
Epoch 650, training loss: 0.015449589118361473 = 0.008234139531850815 + 0.001 * 7.215449333190918
Epoch 650, val loss: 1.064002513885498
Epoch 660, training loss: 0.01504625752568245 = 0.007837069220840931 + 0.001 * 7.2091875076293945
Epoch 660, val loss: 1.0724585056304932
Epoch 670, training loss: 0.014661790803074837 = 0.007467822637408972 + 0.001 * 7.193967819213867
Epoch 670, val loss: 1.0807414054870605
Epoch 680, training loss: 0.014320316724479198 = 0.007123899180442095 + 0.001 * 7.196417331695557
Epoch 680, val loss: 1.0888811349868774
Epoch 690, training loss: 0.014001228846609592 = 0.006802233401685953 + 0.001 * 7.198995113372803
Epoch 690, val loss: 1.0968832969665527
Epoch 700, training loss: 0.013683360069990158 = 0.006499481853097677 + 0.001 * 7.183877944946289
Epoch 700, val loss: 1.1047509908676147
Epoch 710, training loss: 0.013404371216893196 = 0.006213418208062649 + 0.001 * 7.190953254699707
Epoch 710, val loss: 1.112538456916809
Epoch 720, training loss: 0.01312112994492054 = 0.005943125579506159 + 0.001 * 7.178003787994385
Epoch 720, val loss: 1.1202009916305542
Epoch 730, training loss: 0.012871594168245792 = 0.005687923636287451 + 0.001 * 7.1836700439453125
Epoch 730, val loss: 1.1277400255203247
Epoch 740, training loss: 0.01261816080659628 = 0.005447205156087875 + 0.001 * 7.170955181121826
Epoch 740, val loss: 1.1351779699325562
Epoch 750, training loss: 0.012389210984110832 = 0.005220159888267517 + 0.001 * 7.169051170349121
Epoch 750, val loss: 1.1424654722213745
Epoch 760, training loss: 0.012186329811811447 = 0.005006641149520874 + 0.001 * 7.179688930511475
Epoch 760, val loss: 1.1496175527572632
Epoch 770, training loss: 0.011983726173639297 = 0.004805909004062414 + 0.001 * 7.177816390991211
Epoch 770, val loss: 1.1566364765167236
Epoch 780, training loss: 0.011814502999186516 = 0.004617004655301571 + 0.001 * 7.197498321533203
Epoch 780, val loss: 1.1634865999221802
Epoch 790, training loss: 0.011621839366853237 = 0.004439396318048239 + 0.001 * 7.182442665100098
Epoch 790, val loss: 1.1701802015304565
Epoch 800, training loss: 0.011442230083048344 = 0.004272238817065954 + 0.001 * 7.1699910163879395
Epoch 800, val loss: 1.176753044128418
Epoch 810, training loss: 0.011280793696641922 = 0.004114815965294838 + 0.001 * 7.165977478027344
Epoch 810, val loss: 1.1831382513046265
Epoch 820, training loss: 0.01112336479127407 = 0.0039666397497057915 + 0.001 * 7.15672492980957
Epoch 820, val loss: 1.189424753189087
Epoch 830, training loss: 0.011041914112865925 = 0.0038269676733762026 + 0.001 * 7.2149457931518555
Epoch 830, val loss: 1.195561170578003
Epoch 840, training loss: 0.01085628941655159 = 0.0036952639929950237 + 0.001 * 7.161024570465088
Epoch 840, val loss: 1.2014507055282593
Epoch 850, training loss: 0.010704845190048218 = 0.0035710574593394995 + 0.001 * 7.133787631988525
Epoch 850, val loss: 1.2072888612747192
Epoch 860, training loss: 0.010581152513623238 = 0.003453770186752081 + 0.001 * 7.127381324768066
Epoch 860, val loss: 1.2129451036453247
Epoch 870, training loss: 0.010483727790415287 = 0.0033429714385420084 + 0.001 * 7.140756130218506
Epoch 870, val loss: 1.2184786796569824
Epoch 880, training loss: 0.010370939038693905 = 0.0032381380442529917 + 0.001 * 7.132800579071045
Epoch 880, val loss: 1.2238895893096924
Epoch 890, training loss: 0.01028466410934925 = 0.0031388569623231888 + 0.001 * 7.145806789398193
Epoch 890, val loss: 1.2291505336761475
Epoch 900, training loss: 0.010165493004024029 = 0.00304474588483572 + 0.001 * 7.120746612548828
Epoch 900, val loss: 1.2342580556869507
Epoch 910, training loss: 0.010091318748891354 = 0.002955475589260459 + 0.001 * 7.135842800140381
Epoch 910, val loss: 1.239279866218567
Epoch 920, training loss: 0.01000160165131092 = 0.0028708241879940033 + 0.001 * 7.130776882171631
Epoch 920, val loss: 1.244131326675415
Epoch 930, training loss: 0.009902690537273884 = 0.0027904692105948925 + 0.001 * 7.112220764160156
Epoch 930, val loss: 1.248894453048706
Epoch 940, training loss: 0.009849736467003822 = 0.002714086789637804 + 0.001 * 7.135649681091309
Epoch 940, val loss: 1.2535743713378906
Epoch 950, training loss: 0.009734304621815681 = 0.0026414620224386454 + 0.001 * 7.0928425788879395
Epoch 950, val loss: 1.25809645652771
Epoch 960, training loss: 0.009705737233161926 = 0.002572313416749239 + 0.001 * 7.133423805236816
Epoch 960, val loss: 1.2625350952148438
Epoch 970, training loss: 0.009641826152801514 = 0.002506449120119214 + 0.001 * 7.135376453399658
Epoch 970, val loss: 1.2668224573135376
Epoch 980, training loss: 0.009522993117570877 = 0.0024436316452920437 + 0.001 * 7.0793609619140625
Epoch 980, val loss: 1.271026372909546
Epoch 990, training loss: 0.009498748928308487 = 0.002383657731115818 + 0.001 * 7.115091323852539
Epoch 990, val loss: 1.2750948667526245
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8434370057986295
=== training gcn model ===
Epoch 0, training loss: 1.9509468078613281 = 1.942349910736084 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.938532829284668
Epoch 10, training loss: 1.9404767751693726 = 1.931879997253418 + 0.001 * 8.596753120422363
Epoch 10, val loss: 1.9287949800491333
Epoch 20, training loss: 1.9272674322128296 = 1.918670892715454 + 0.001 * 8.596532821655273
Epoch 20, val loss: 1.9162907600402832
Epoch 30, training loss: 1.9087141752243042 = 1.900118112564087 + 0.001 * 8.596067428588867
Epoch 30, val loss: 1.898596167564392
Epoch 40, training loss: 1.8816521167755127 = 1.8730571269989014 + 0.001 * 8.594985008239746
Epoch 40, val loss: 1.8732712268829346
Epoch 50, training loss: 1.8446396589279175 = 1.836047887802124 + 0.001 * 8.59177017211914
Epoch 50, val loss: 1.8402353525161743
Epoch 60, training loss: 1.8030571937561035 = 1.7944774627685547 + 0.001 * 8.579739570617676
Epoch 60, val loss: 1.80575692653656
Epoch 70, training loss: 1.7644028663635254 = 1.7558790445327759 + 0.001 * 8.523866653442383
Epoch 70, val loss: 1.7725322246551514
Epoch 80, training loss: 1.7160242795944214 = 1.7077796459197998 + 0.001 * 8.244611740112305
Epoch 80, val loss: 1.7259668111801147
Epoch 90, training loss: 1.6489759683609009 = 1.6408746242523193 + 0.001 * 8.101308822631836
Epoch 90, val loss: 1.6640965938568115
Epoch 100, training loss: 1.5620691776275635 = 1.5540798902511597 + 0.001 * 7.989309787750244
Epoch 100, val loss: 1.588727593421936
Epoch 110, training loss: 1.46359384059906 = 1.4557628631591797 + 0.001 * 7.8310112953186035
Epoch 110, val loss: 1.5055902004241943
Epoch 120, training loss: 1.3627394437789917 = 1.3551313877105713 + 0.001 * 7.608021259307861
Epoch 120, val loss: 1.4209504127502441
Epoch 130, training loss: 1.2623218297958374 = 1.2547770738601685 + 0.001 * 7.544707298278809
Epoch 130, val loss: 1.3370321989059448
Epoch 140, training loss: 1.1633970737457275 = 1.1559644937515259 + 0.001 * 7.4326171875
Epoch 140, val loss: 1.2558221817016602
Epoch 150, training loss: 1.0685943365097046 = 1.0612461566925049 + 0.001 * 7.348220348358154
Epoch 150, val loss: 1.1792864799499512
Epoch 160, training loss: 0.9801276922225952 = 0.9728421568870544 + 0.001 * 7.28555154800415
Epoch 160, val loss: 1.1101876497268677
Epoch 170, training loss: 0.8979995250701904 = 0.8907509446144104 + 0.001 * 7.248561859130859
Epoch 170, val loss: 1.0485464334487915
Epoch 180, training loss: 0.8210915327072144 = 0.8138699531555176 + 0.001 * 7.22158145904541
Epoch 180, val loss: 0.9930946826934814
Epoch 190, training loss: 0.7488918304443359 = 0.7416867613792419 + 0.001 * 7.2050700187683105
Epoch 190, val loss: 0.9431414008140564
Epoch 200, training loss: 0.6819300651550293 = 0.6747317910194397 + 0.001 * 7.198301315307617
Epoch 200, val loss: 0.899227499961853
Epoch 210, training loss: 0.6208946704864502 = 0.6136994957923889 + 0.001 * 7.195155143737793
Epoch 210, val loss: 0.8620604276657104
Epoch 220, training loss: 0.5660942196846008 = 0.5589004158973694 + 0.001 * 7.19382381439209
Epoch 220, val loss: 0.8322733044624329
Epoch 230, training loss: 0.5171148777008057 = 0.5099220275878906 + 0.001 * 7.1928558349609375
Epoch 230, val loss: 0.8095948100090027
Epoch 240, training loss: 0.47307679057121277 = 0.4658854007720947 + 0.001 * 7.19139289855957
Epoch 240, val loss: 0.7931333184242249
Epoch 250, training loss: 0.4329693019390106 = 0.42577943205833435 + 0.001 * 7.189857006072998
Epoch 250, val loss: 0.7818489074707031
Epoch 260, training loss: 0.3958706259727478 = 0.38868215680122375 + 0.001 * 7.18846321105957
Epoch 260, val loss: 0.7745532989501953
Epoch 270, training loss: 0.36109718680381775 = 0.35390958189964294 + 0.001 * 7.187607288360596
Epoch 270, val loss: 0.7702792286872864
Epoch 280, training loss: 0.32822203636169434 = 0.32103583216667175 + 0.001 * 7.186202526092529
Epoch 280, val loss: 0.7682796716690063
Epoch 290, training loss: 0.2970624566078186 = 0.2898767292499542 + 0.001 * 7.185734748840332
Epoch 290, val loss: 0.7679917812347412
Epoch 300, training loss: 0.26769906282424927 = 0.26051369309425354 + 0.001 * 7.1853837966918945
Epoch 300, val loss: 0.7692282199859619
Epoch 310, training loss: 0.24036341905593872 = 0.23317821323871613 + 0.001 * 7.185204982757568
Epoch 310, val loss: 0.7720100283622742
Epoch 320, training loss: 0.21531955897808075 = 0.20813331007957458 + 0.001 * 7.186247825622559
Epoch 320, val loss: 0.7762605547904968
Epoch 330, training loss: 0.19274914264678955 = 0.18556217849254608 + 0.001 * 7.186963081359863
Epoch 330, val loss: 0.7820001840591431
Epoch 340, training loss: 0.17263172566890717 = 0.16544699668884277 + 0.001 * 7.184722423553467
Epoch 340, val loss: 0.7890926003456116
Epoch 350, training loss: 0.15482041239738464 = 0.1476360261440277 + 0.001 * 7.184391498565674
Epoch 350, val loss: 0.7975254654884338
Epoch 360, training loss: 0.13907580077648163 = 0.13189098238945007 + 0.001 * 7.184813976287842
Epoch 360, val loss: 0.8071095943450928
Epoch 370, training loss: 0.12515297532081604 = 0.11797086149454117 + 0.001 * 7.182108402252197
Epoch 370, val loss: 0.8176831007003784
Epoch 380, training loss: 0.11283333599567413 = 0.10565195977687836 + 0.001 * 7.18137788772583
Epoch 380, val loss: 0.8289203643798828
Epoch 390, training loss: 0.10190796107053757 = 0.09472969174385071 + 0.001 * 7.178267002105713
Epoch 390, val loss: 0.8405895829200745
Epoch 400, training loss: 0.09220507740974426 = 0.08503148704767227 + 0.001 * 7.173590183258057
Epoch 400, val loss: 0.8525861501693726
Epoch 410, training loss: 0.08358421921730042 = 0.07641663402318954 + 0.001 * 7.1675848960876465
Epoch 410, val loss: 0.8648151755332947
Epoch 420, training loss: 0.07592377811670303 = 0.06876367330551147 + 0.001 * 7.160102844238281
Epoch 420, val loss: 0.8772705793380737
Epoch 430, training loss: 0.06912652403116226 = 0.0619700588285923 + 0.001 * 7.156464576721191
Epoch 430, val loss: 0.8898951411247253
Epoch 440, training loss: 0.06308873742818832 = 0.05594393610954285 + 0.001 * 7.144802093505859
Epoch 440, val loss: 0.9025952219963074
Epoch 450, training loss: 0.0577467642724514 = 0.05060427263379097 + 0.001 * 7.142492771148682
Epoch 450, val loss: 0.915337324142456
Epoch 460, training loss: 0.05300317704677582 = 0.045876599848270416 + 0.001 * 7.126574993133545
Epoch 460, val loss: 0.9280375242233276
Epoch 470, training loss: 0.04881453514099121 = 0.04169275984168053 + 0.001 * 7.1217732429504395
Epoch 470, val loss: 0.9406376481056213
Epoch 480, training loss: 0.04511842876672745 = 0.03798941895365715 + 0.001 * 7.1290106773376465
Epoch 480, val loss: 0.9530898332595825
Epoch 490, training loss: 0.041829872876405716 = 0.03470948338508606 + 0.001 * 7.120389461517334
Epoch 490, val loss: 0.9653745293617249
Epoch 500, training loss: 0.03891143202781677 = 0.03180122748017311 + 0.001 * 7.11020565032959
Epoch 500, val loss: 0.977434515953064
Epoch 510, training loss: 0.03632715716958046 = 0.029217764735221863 + 0.001 * 7.109393119812012
Epoch 510, val loss: 0.9892739653587341
Epoch 520, training loss: 0.0340425930917263 = 0.02691865712404251 + 0.001 * 7.123936653137207
Epoch 520, val loss: 1.0008625984191895
Epoch 530, training loss: 0.03197338432073593 = 0.024867592379450798 + 0.001 * 7.105791091918945
Epoch 530, val loss: 1.0121910572052002
Epoch 540, training loss: 0.030138473957777023 = 0.023033013567328453 + 0.001 * 7.105460166931152
Epoch 540, val loss: 1.0232490301132202
Epoch 550, training loss: 0.02849321812391281 = 0.02138790674507618 + 0.001 * 7.105311870574951
Epoch 550, val loss: 1.0340243577957153
Epoch 560, training loss: 0.02701137401163578 = 0.01990862376987934 + 0.001 * 7.102749347686768
Epoch 560, val loss: 1.0445226430892944
Epoch 570, training loss: 0.025678206235170364 = 0.018574915826320648 + 0.001 * 7.103290557861328
Epoch 570, val loss: 1.054744839668274
Epoch 580, training loss: 0.0244731605052948 = 0.01736934669315815 + 0.001 * 7.103814125061035
Epoch 580, val loss: 1.0647015571594238
Epoch 590, training loss: 0.02337651327252388 = 0.01627676747739315 + 0.001 * 7.099746227264404
Epoch 590, val loss: 1.0743790864944458
Epoch 600, training loss: 0.022381020709872246 = 0.015284090302884579 + 0.001 * 7.096930503845215
Epoch 600, val loss: 1.0837959051132202
Epoch 610, training loss: 0.021475596353411674 = 0.014379860833287239 + 0.001 * 7.095734596252441
Epoch 610, val loss: 1.092957854270935
Epoch 620, training loss: 0.02066384255886078 = 0.013554236851632595 + 0.001 * 7.10960578918457
Epoch 620, val loss: 1.1018511056900024
Epoch 630, training loss: 0.01989700458943844 = 0.012798693031072617 + 0.001 * 7.098310947418213
Epoch 630, val loss: 1.1105328798294067
Epoch 640, training loss: 0.01920008473098278 = 0.012105678208172321 + 0.001 * 7.0944061279296875
Epoch 640, val loss: 1.1189724206924438
Epoch 650, training loss: 0.01855916902422905 = 0.011468696407973766 + 0.001 * 7.09047269821167
Epoch 650, val loss: 1.1271896362304688
Epoch 660, training loss: 0.017971765249967575 = 0.010881992056965828 + 0.001 * 7.089773654937744
Epoch 660, val loss: 1.135188341140747
Epoch 670, training loss: 0.017434371635317802 = 0.010340584442019463 + 0.001 * 7.093786716461182
Epoch 670, val loss: 1.1429617404937744
Epoch 680, training loss: 0.016928965225815773 = 0.009839965961873531 + 0.001 * 7.088998317718506
Epoch 680, val loss: 1.1505353450775146
Epoch 690, training loss: 0.016459692269563675 = 0.009376203641295433 + 0.001 * 7.0834879875183105
Epoch 690, val loss: 1.1579214334487915
Epoch 700, training loss: 0.016034435480833054 = 0.008945828303694725 + 0.001 * 7.088605880737305
Epoch 700, val loss: 1.1650983095169067
Epoch 710, training loss: 0.015630435198545456 = 0.008545762859284878 + 0.001 * 7.0846710205078125
Epoch 710, val loss: 1.172102689743042
Epoch 720, training loss: 0.015253452584147453 = 0.008173276670277119 + 0.001 * 7.080175399780273
Epoch 720, val loss: 1.1789264678955078
Epoch 730, training loss: 0.014904489740729332 = 0.007825900800526142 + 0.001 * 7.078588008880615
Epoch 730, val loss: 1.1855777502059937
Epoch 740, training loss: 0.014581440947949886 = 0.007501439191401005 + 0.001 * 7.080001354217529
Epoch 740, val loss: 1.192068099975586
Epoch 750, training loss: 0.014276908710598946 = 0.007197989616543055 + 0.001 * 7.078918933868408
Epoch 750, val loss: 1.1983754634857178
Epoch 760, training loss: 0.013989349827170372 = 0.006913778372108936 + 0.001 * 7.075571060180664
Epoch 760, val loss: 1.2045464515686035
Epoch 770, training loss: 0.013732252642512321 = 0.006647223141044378 + 0.001 * 7.085028648376465
Epoch 770, val loss: 1.2105562686920166
Epoch 780, training loss: 0.013479260727763176 = 0.006396934390068054 + 0.001 * 7.0823259353637695
Epoch 780, val loss: 1.2164329290390015
Epoch 790, training loss: 0.013232479803264141 = 0.006161574274301529 + 0.001 * 7.0709052085876465
Epoch 790, val loss: 1.2221788167953491
Epoch 800, training loss: 0.013009365648031235 = 0.005940017756074667 + 0.001 * 7.069347858428955
Epoch 800, val loss: 1.227770447731018
Epoch 810, training loss: 0.012795476242899895 = 0.005731168203055859 + 0.001 * 7.06430721282959
Epoch 810, val loss: 1.2332395315170288
Epoch 820, training loss: 0.012608997523784637 = 0.005534135736525059 + 0.001 * 7.074862003326416
Epoch 820, val loss: 1.2385826110839844
Epoch 830, training loss: 0.012407418340444565 = 0.005348020698875189 + 0.001 * 7.059397220611572
Epoch 830, val loss: 1.2437933683395386
Epoch 840, training loss: 0.012233520857989788 = 0.0051720417104661465 + 0.001 * 7.061478614807129
Epoch 840, val loss: 1.248887300491333
Epoch 850, training loss: 0.012061171233654022 = 0.005005485378205776 + 0.001 * 7.055685520172119
Epoch 850, val loss: 1.2538505792617798
Epoch 860, training loss: 0.011907882988452911 = 0.004847674630582333 + 0.001 * 7.060208320617676
Epoch 860, val loss: 1.2587008476257324
Epoch 870, training loss: 0.011776339262723923 = 0.004697964061051607 + 0.001 * 7.078375339508057
Epoch 870, val loss: 1.2634555101394653
Epoch 880, training loss: 0.011623663827776909 = 0.004555451683700085 + 0.001 * 7.068212032318115
Epoch 880, val loss: 1.268114447593689
Epoch 890, training loss: 0.011486774310469627 = 0.004418958909809589 + 0.001 * 7.067814826965332
Epoch 890, val loss: 1.2726962566375732
Epoch 900, training loss: 0.011342819780111313 = 0.004287156276404858 + 0.001 * 7.055662631988525
Epoch 900, val loss: 1.2772433757781982
Epoch 910, training loss: 0.011217258870601654 = 0.004159329459071159 + 0.001 * 7.057928562164307
Epoch 910, val loss: 1.2817610502243042
Epoch 920, training loss: 0.011073261499404907 = 0.004035208839923143 + 0.001 * 7.038052558898926
Epoch 920, val loss: 1.2862777709960938
Epoch 930, training loss: 0.010980136692523956 = 0.003914924338459969 + 0.001 * 7.065211772918701
Epoch 930, val loss: 1.290793776512146
Epoch 940, training loss: 0.010842774994671345 = 0.0037986531388014555 + 0.001 * 7.044121742248535
Epoch 940, val loss: 1.2952971458435059
Epoch 950, training loss: 0.010734199546277523 = 0.003686546580865979 + 0.001 * 7.047652244567871
Epoch 950, val loss: 1.299780011177063
Epoch 960, training loss: 0.010618342086672783 = 0.003578759729862213 + 0.001 * 7.039582252502441
Epoch 960, val loss: 1.3041802644729614
Epoch 970, training loss: 0.010527215898036957 = 0.0034753333311527967 + 0.001 * 7.051881790161133
Epoch 970, val loss: 1.3085744380950928
Epoch 980, training loss: 0.01040775515139103 = 0.0033762133680284023 + 0.001 * 7.031541347503662
Epoch 980, val loss: 1.3128662109375
Epoch 990, training loss: 0.010311309248209 = 0.0032812724821269512 + 0.001 * 7.030036926269531
Epoch 990, val loss: 1.3171254396438599
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 1.9660650491714478 = 1.9574682712554932 + 0.001 * 8.596832275390625
Epoch 0, val loss: 1.9571210145950317
Epoch 10, training loss: 1.9544930458068848 = 1.9458962678909302 + 0.001 * 8.59677791595459
Epoch 10, val loss: 1.9454007148742676
Epoch 20, training loss: 1.9402334690093994 = 1.9316368103027344 + 0.001 * 8.596607208251953
Epoch 20, val loss: 1.9310038089752197
Epoch 30, training loss: 1.9203822612762451 = 1.9117860794067383 + 0.001 * 8.596192359924316
Epoch 30, val loss: 1.9107645750045776
Epoch 40, training loss: 1.8915362358093262 = 1.8829411268234253 + 0.001 * 8.595101356506348
Epoch 40, val loss: 1.881646752357483
Epoch 50, training loss: 1.8525019884109497 = 1.8439102172851562 + 0.001 * 8.59176254272461
Epoch 50, val loss: 1.8443232774734497
Epoch 60, training loss: 1.8112871646881104 = 1.8027076721191406 + 0.001 * 8.579511642456055
Epoch 60, val loss: 1.809415578842163
Epoch 70, training loss: 1.778118371963501 = 1.7695891857147217 + 0.001 * 8.529241561889648
Epoch 70, val loss: 1.783030390739441
Epoch 80, training loss: 1.7363560199737549 = 1.7280977964401245 + 0.001 * 8.258201599121094
Epoch 80, val loss: 1.7455768585205078
Epoch 90, training loss: 1.677514910697937 = 1.6694227457046509 + 0.001 * 8.092217445373535
Epoch 90, val loss: 1.694115161895752
Epoch 100, training loss: 1.5974498987197876 = 1.589438796043396 + 0.001 * 8.011131286621094
Epoch 100, val loss: 1.6260857582092285
Epoch 110, training loss: 1.5040884017944336 = 1.496148943901062 + 0.001 * 7.9394426345825195
Epoch 110, val loss: 1.5491054058074951
Epoch 120, training loss: 1.4093377590179443 = 1.4015374183654785 + 0.001 * 7.800352096557617
Epoch 120, val loss: 1.4711717367172241
Epoch 130, training loss: 1.3155313730239868 = 1.3078685998916626 + 0.001 * 7.662798881530762
Epoch 130, val loss: 1.3945503234863281
Epoch 140, training loss: 1.2199409008026123 = 1.2122893333435059 + 0.001 * 7.651528358459473
Epoch 140, val loss: 1.3161795139312744
Epoch 150, training loss: 1.1206556558609009 = 1.1130529642105103 + 0.001 * 7.602665901184082
Epoch 150, val loss: 1.23536217212677
Epoch 160, training loss: 1.0200483798980713 = 1.0124921798706055 + 0.001 * 7.556152820587158
Epoch 160, val loss: 1.1548852920532227
Epoch 170, training loss: 0.9232035875320435 = 0.9157248139381409 + 0.001 * 7.478784561157227
Epoch 170, val loss: 1.0787326097488403
Epoch 180, training loss: 0.8353751301765442 = 0.8279882669448853 + 0.001 * 7.386848449707031
Epoch 180, val loss: 1.0112451314926147
Epoch 190, training loss: 0.7598292827606201 = 0.7524643540382385 + 0.001 * 7.364938735961914
Epoch 190, val loss: 0.9549784064292908
Epoch 200, training loss: 0.6964018940925598 = 0.6890456676483154 + 0.001 * 7.356225967407227
Epoch 200, val loss: 0.9101168513298035
Epoch 210, training loss: 0.64211106300354 = 0.6347590088844299 + 0.001 * 7.352038383483887
Epoch 210, val loss: 0.8746481537818909
Epoch 220, training loss: 0.5935386419296265 = 0.5861908793449402 + 0.001 * 7.347754001617432
Epoch 220, val loss: 0.8459928035736084
Epoch 230, training loss: 0.5484979748725891 = 0.5411525964736938 + 0.001 * 7.345365524291992
Epoch 230, val loss: 0.8224236965179443
Epoch 240, training loss: 0.5060794949531555 = 0.498740553855896 + 0.001 * 7.338922500610352
Epoch 240, val loss: 0.8032139539718628
Epoch 250, training loss: 0.46593451499938965 = 0.4585990309715271 + 0.001 * 7.3354902267456055
Epoch 250, val loss: 0.7878437638282776
Epoch 260, training loss: 0.4276963770389557 = 0.4203641414642334 + 0.001 * 7.332225799560547
Epoch 260, val loss: 0.7760002017021179
Epoch 270, training loss: 0.3911997079849243 = 0.38387051224708557 + 0.001 * 7.329180717468262
Epoch 270, val loss: 0.7676473259925842
Epoch 280, training loss: 0.3566729724407196 = 0.34934669733047485 + 0.001 * 7.326269149780273
Epoch 280, val loss: 0.7628375887870789
Epoch 290, training loss: 0.324645072221756 = 0.3173232078552246 + 0.001 * 7.321873664855957
Epoch 290, val loss: 0.761715292930603
Epoch 300, training loss: 0.2954356074333191 = 0.2881195843219757 + 0.001 * 7.316033840179443
Epoch 300, val loss: 0.7641636729240417
Epoch 310, training loss: 0.26883408427238464 = 0.26152390241622925 + 0.001 * 7.310192108154297
Epoch 310, val loss: 0.7695481181144714
Epoch 320, training loss: 0.24432551860809326 = 0.23702308535575867 + 0.001 * 7.302431106567383
Epoch 320, val loss: 0.7772079110145569
Epoch 330, training loss: 0.22149556875228882 = 0.21419990062713623 + 0.001 * 7.295670509338379
Epoch 330, val loss: 0.7867716550827026
Epoch 340, training loss: 0.2001093626022339 = 0.19283360242843628 + 0.001 * 7.275759696960449
Epoch 340, val loss: 0.797992467880249
Epoch 350, training loss: 0.1801750361919403 = 0.17288681864738464 + 0.001 * 7.288210391998291
Epoch 350, val loss: 0.8107045888900757
Epoch 360, training loss: 0.16169555485248566 = 0.15443912148475647 + 0.001 * 7.256429195404053
Epoch 360, val loss: 0.8247601389884949
Epoch 370, training loss: 0.14480648934841156 = 0.13757073879241943 + 0.001 * 7.235750198364258
Epoch 370, val loss: 0.8400356769561768
Epoch 380, training loss: 0.12953323125839233 = 0.12230999767780304 + 0.001 * 7.223230361938477
Epoch 380, val loss: 0.8564392328262329
Epoch 390, training loss: 0.11583629995584488 = 0.10861280560493469 + 0.001 * 7.223494052886963
Epoch 390, val loss: 0.8735782504081726
Epoch 400, training loss: 0.10360439121723175 = 0.09638913720846176 + 0.001 * 7.2152533531188965
Epoch 400, val loss: 0.8912403583526611
Epoch 410, training loss: 0.09274044632911682 = 0.08553322404623032 + 0.001 * 7.207221508026123
Epoch 410, val loss: 0.9090803265571594
Epoch 420, training loss: 0.08313841372728348 = 0.07593514025211334 + 0.001 * 7.203269958496094
Epoch 420, val loss: 0.9268499612808228
Epoch 430, training loss: 0.07469211518764496 = 0.06748639047145844 + 0.001 * 7.2057204246521
Epoch 430, val loss: 0.9444694519042969
Epoch 440, training loss: 0.06728006154298782 = 0.06007857620716095 + 0.001 * 7.201484680175781
Epoch 440, val loss: 0.9618565440177917
Epoch 450, training loss: 0.060800548642873764 = 0.05360040441155434 + 0.001 * 7.2001423835754395
Epoch 450, val loss: 0.9789000153541565
Epoch 460, training loss: 0.05513588711619377 = 0.04793982952833176 + 0.001 * 7.196058750152588
Epoch 460, val loss: 0.9955827593803406
Epoch 470, training loss: 0.05018952861428261 = 0.042996641248464584 + 0.001 * 7.192885875701904
Epoch 470, val loss: 1.0118523836135864
Epoch 480, training loss: 0.04589468240737915 = 0.0386812686920166 + 0.001 * 7.213411331176758
Epoch 480, val loss: 1.027703881263733
Epoch 490, training loss: 0.042110368609428406 = 0.034912921488285065 + 0.001 * 7.19744873046875
Epoch 490, val loss: 1.0430241823196411
Epoch 500, training loss: 0.03880836069583893 = 0.03161695972084999 + 0.001 * 7.191399574279785
Epoch 500, val loss: 1.0577991008758545
Epoch 510, training loss: 0.035929497331380844 = 0.028728019446134567 + 0.001 * 7.201477527618408
Epoch 510, val loss: 1.0720504522323608
Epoch 520, training loss: 0.03337941691279411 = 0.026189178228378296 + 0.001 * 7.190237522125244
Epoch 520, val loss: 1.0857335329055786
Epoch 530, training loss: 0.03113524243235588 = 0.02395096980035305 + 0.001 * 7.184271335601807
Epoch 530, val loss: 1.0989166498184204
Epoch 540, training loss: 0.029157860204577446 = 0.02197178639471531 + 0.001 * 7.1860737800598145
Epoch 540, val loss: 1.1115936040878296
Epoch 550, training loss: 0.027392614632844925 = 0.020215777680277824 + 0.001 * 7.176836967468262
Epoch 550, val loss: 1.1237848997116089
Epoch 560, training loss: 0.0258281659334898 = 0.018652964383363724 + 0.001 * 7.175201416015625
Epoch 560, val loss: 1.1355019807815552
Epoch 570, training loss: 0.02443750575184822 = 0.01725778728723526 + 0.001 * 7.179717540740967
Epoch 570, val loss: 1.1468000411987305
Epoch 580, training loss: 0.0231778584420681 = 0.016009017825126648 + 0.001 * 7.168839931488037
Epoch 580, val loss: 1.1576828956604004
Epoch 590, training loss: 0.022055907174944878 = 0.014888155274093151 + 0.001 * 7.167751312255859
Epoch 590, val loss: 1.1681692600250244
Epoch 600, training loss: 0.02108805440366268 = 0.013879455626010895 + 0.001 * 7.208599090576172
Epoch 600, val loss: 1.17829167842865
Epoch 610, training loss: 0.020132508128881454 = 0.01296930480748415 + 0.001 * 7.163203239440918
Epoch 610, val loss: 1.1880242824554443
Epoch 620, training loss: 0.019303716719150543 = 0.01214599609375 + 0.001 * 7.157721042633057
Epoch 620, val loss: 1.1974495649337769
Epoch 630, training loss: 0.018580486997961998 = 0.01139923557639122 + 0.001 * 7.181251525878906
Epoch 630, val loss: 1.2065238952636719
Epoch 640, training loss: 0.017876992002129555 = 0.01072023343294859 + 0.001 * 7.1567583084106445
Epoch 640, val loss: 1.2153112888336182
Epoch 650, training loss: 0.017266834154725075 = 0.010101420804858208 + 0.001 * 7.165412425994873
Epoch 650, val loss: 1.2237814664840698
Epoch 660, training loss: 0.01668332703411579 = 0.009536267258226871 + 0.001 * 7.147059917449951
Epoch 660, val loss: 1.2319921255111694
Epoch 670, training loss: 0.016161078587174416 = 0.009019003249704838 + 0.001 * 7.142075061798096
Epoch 670, val loss: 1.2398977279663086
Epoch 680, training loss: 0.01568419486284256 = 0.008544459007680416 + 0.001 * 7.139735698699951
Epoch 680, val loss: 1.247572660446167
Epoch 690, training loss: 0.015262540429830551 = 0.008108161389827728 + 0.001 * 7.154378414154053
Epoch 690, val loss: 1.2549999952316284
Epoch 700, training loss: 0.014848406426608562 = 0.007706221658736467 + 0.001 * 7.142184257507324
Epoch 700, val loss: 1.2621556520462036
Epoch 710, training loss: 0.014463849365711212 = 0.007335208356380463 + 0.001 * 7.128640651702881
Epoch 710, val loss: 1.2691034078598022
Epoch 720, training loss: 0.01412692479789257 = 0.006992048118263483 + 0.001 * 7.134875774383545
Epoch 720, val loss: 1.2758233547210693
Epoch 730, training loss: 0.013821324333548546 = 0.006674046162515879 + 0.001 * 7.147278308868408
Epoch 730, val loss: 1.282359004020691
Epoch 740, training loss: 0.013501325622200966 = 0.006378832273185253 + 0.001 * 7.12249231338501
Epoch 740, val loss: 1.2886816263198853
Epoch 750, training loss: 0.013222994282841682 = 0.006104325409978628 + 0.001 * 7.118668556213379
Epoch 750, val loss: 1.2947982549667358
Epoch 760, training loss: 0.012960841879248619 = 0.005848629865795374 + 0.001 * 7.112212181091309
Epoch 760, val loss: 1.3007434606552124
Epoch 770, training loss: 0.012732590548694134 = 0.005610086023807526 + 0.001 * 7.122504234313965
Epoch 770, val loss: 1.3065049648284912
Epoch 780, training loss: 0.012500356882810593 = 0.0053871870040893555 + 0.001 * 7.113169193267822
Epoch 780, val loss: 1.312087893486023
Epoch 790, training loss: 0.012298166751861572 = 0.0051785786636173725 + 0.001 * 7.119587421417236
Epoch 790, val loss: 1.3175263404846191
Epoch 800, training loss: 0.012092595919966698 = 0.004983057267963886 + 0.001 * 7.109538555145264
Epoch 800, val loss: 1.3227869272232056
Epoch 810, training loss: 0.011923696845769882 = 0.00479960348457098 + 0.001 * 7.1240925788879395
Epoch 810, val loss: 1.3279168605804443
Epoch 820, training loss: 0.011727608740329742 = 0.004627286922186613 + 0.001 * 7.1003217697143555
Epoch 820, val loss: 1.3328803777694702
Epoch 830, training loss: 0.011571893468499184 = 0.004465132486075163 + 0.001 * 7.106760501861572
Epoch 830, val loss: 1.3377172946929932
Epoch 840, training loss: 0.01139799039810896 = 0.004312346689403057 + 0.001 * 7.085643291473389
Epoch 840, val loss: 1.3424040079116821
Epoch 850, training loss: 0.011262444779276848 = 0.004168243147432804 + 0.001 * 7.094201564788818
Epoch 850, val loss: 1.3469743728637695
Epoch 860, training loss: 0.011132972314953804 = 0.004032145719975233 + 0.001 * 7.100825786590576
Epoch 860, val loss: 1.3514312505722046
Epoch 870, training loss: 0.010984448716044426 = 0.0039034774526953697 + 0.001 * 7.080970764160156
Epoch 870, val loss: 1.3557559251785278
Epoch 880, training loss: 0.010880531743168831 = 0.0037816495168954134 + 0.001 * 7.09888219833374
Epoch 880, val loss: 1.3599750995635986
Epoch 890, training loss: 0.010743437334895134 = 0.003666093572974205 + 0.001 * 7.077342987060547
Epoch 890, val loss: 1.3640929460525513
Epoch 900, training loss: 0.010694430209696293 = 0.003556234296411276 + 0.001 * 7.138195514678955
Epoch 900, val loss: 1.3680918216705322
Epoch 910, training loss: 0.010539362207055092 = 0.0034516234882175922 + 0.001 * 7.087738990783691
Epoch 910, val loss: 1.3720142841339111
Epoch 920, training loss: 0.01042119786143303 = 0.0033516976982355118 + 0.001 * 7.06950044631958
Epoch 920, val loss: 1.3758323192596436
Epoch 930, training loss: 0.010359575040638447 = 0.0032558911480009556 + 0.001 * 7.1036834716796875
Epoch 930, val loss: 1.379560112953186
Epoch 940, training loss: 0.010241739451885223 = 0.003163859946653247 + 0.001 * 7.077879428863525
Epoch 940, val loss: 1.3832050561904907
Epoch 950, training loss: 0.010134244337677956 = 0.0030752969905734062 + 0.001 * 7.058947563171387
Epoch 950, val loss: 1.3867665529251099
Epoch 960, training loss: 0.010076894424855709 = 0.002989916829392314 + 0.001 * 7.086977481842041
Epoch 960, val loss: 1.390235424041748
Epoch 970, training loss: 0.009979099966585636 = 0.0029075711499899626 + 0.001 * 7.071528434753418
Epoch 970, val loss: 1.3936620950698853
Epoch 980, training loss: 0.009922236204147339 = 0.0028281956911087036 + 0.001 * 7.0940399169921875
Epoch 980, val loss: 1.397020697593689
Epoch 990, training loss: 0.009827226400375366 = 0.0027519045397639275 + 0.001 * 7.075322151184082
Epoch 990, val loss: 1.400303840637207
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8355297838692674
The final CL Acc:0.80247, 0.01492, The final GNN Acc:0.84027, 0.00342
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9444])
updated graph: torch.Size([2, 10526])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9685441255569458 = 1.9599472284317017 + 0.001 * 8.596869468688965
Epoch 0, val loss: 1.9615569114685059
Epoch 10, training loss: 1.9584341049194336 = 1.9498372077941895 + 0.001 * 8.596842765808105
Epoch 10, val loss: 1.9522520303726196
Epoch 20, training loss: 1.9464339017868042 = 1.9378371238708496 + 0.001 * 8.596736907958984
Epoch 20, val loss: 1.9408659934997559
Epoch 30, training loss: 1.9299370050430298 = 1.9213404655456543 + 0.001 * 8.5964937210083
Epoch 30, val loss: 1.9249093532562256
Epoch 40, training loss: 1.9055908918380737 = 1.896994948387146 + 0.001 * 8.595939636230469
Epoch 40, val loss: 1.901387095451355
Epoch 50, training loss: 1.87074875831604 = 1.8621541261672974 + 0.001 * 8.59458065032959
Epoch 50, val loss: 1.8687916994094849
Epoch 60, training loss: 1.8295235633850098 = 1.8209327459335327 + 0.001 * 8.590795516967773
Epoch 60, val loss: 1.83354651927948
Epoch 70, training loss: 1.7951455116271973 = 1.78656804561615 + 0.001 * 8.57748031616211
Epoch 70, val loss: 1.806742548942566
Epoch 80, training loss: 1.7601438760757446 = 1.7516390085220337 + 0.001 * 8.504894256591797
Epoch 80, val loss: 1.773596167564392
Epoch 90, training loss: 1.7117985486984253 = 1.7036640644073486 + 0.001 * 8.134482383728027
Epoch 90, val loss: 1.7290788888931274
Epoch 100, training loss: 1.6450644731521606 = 1.6370165348052979 + 0.001 * 8.04794692993164
Epoch 100, val loss: 1.6729451417922974
Epoch 110, training loss: 1.5600128173828125 = 1.5520169734954834 + 0.001 * 7.995828151702881
Epoch 110, val loss: 1.6032463312149048
Epoch 120, training loss: 1.4662299156188965 = 1.4583144187927246 + 0.001 * 7.915555953979492
Epoch 120, val loss: 1.5284720659255981
Epoch 130, training loss: 1.3732377290725708 = 1.3655632734298706 + 0.001 * 7.674510478973389
Epoch 130, val loss: 1.4565870761871338
Epoch 140, training loss: 1.2817233800888062 = 1.274200439453125 + 0.001 * 7.522994518280029
Epoch 140, val loss: 1.3876471519470215
Epoch 150, training loss: 1.191312551498413 = 1.1838366985321045 + 0.001 * 7.475862979888916
Epoch 150, val loss: 1.3200297355651855
Epoch 160, training loss: 1.103294014930725 = 1.095845103263855 + 0.001 * 7.448863983154297
Epoch 160, val loss: 1.2547898292541504
Epoch 170, training loss: 1.0201359987258911 = 1.0127036571502686 + 0.001 * 7.432394981384277
Epoch 170, val loss: 1.1945503950119019
Epoch 180, training loss: 0.9424107074737549 = 0.9349870681762695 + 0.001 * 7.423633098602295
Epoch 180, val loss: 1.1393392086029053
Epoch 190, training loss: 0.8690221309661865 = 0.8616040349006653 + 0.001 * 7.418086051940918
Epoch 190, val loss: 1.0882625579833984
Epoch 200, training loss: 0.7984442710876465 = 0.79103022813797 + 0.001 * 7.414068222045898
Epoch 200, val loss: 1.0400805473327637
Epoch 210, training loss: 0.7300212383270264 = 0.7226100564002991 + 0.001 * 7.4112091064453125
Epoch 210, val loss: 0.9944847822189331
Epoch 220, training loss: 0.6643894910812378 = 0.6569811105728149 + 0.001 * 7.408393383026123
Epoch 220, val loss: 0.9524010419845581
Epoch 230, training loss: 0.6028786897659302 = 0.5954738259315491 + 0.001 * 7.404872417449951
Epoch 230, val loss: 0.9149482250213623
Epoch 240, training loss: 0.5465073585510254 = 0.5391071438789368 + 0.001 * 7.400228977203369
Epoch 240, val loss: 0.8835101127624512
Epoch 250, training loss: 0.49532589316368103 = 0.4879322946071625 + 0.001 * 7.393594741821289
Epoch 250, val loss: 0.8581207394599915
Epoch 260, training loss: 0.44854801893234253 = 0.44116413593292236 + 0.001 * 7.383882522583008
Epoch 260, val loss: 0.8378939032554626
Epoch 270, training loss: 0.4051605761051178 = 0.39778825640678406 + 0.001 * 7.3723297119140625
Epoch 270, val loss: 0.8217791318893433
Epoch 280, training loss: 0.3644726872444153 = 0.357115775346756 + 0.001 * 7.356905460357666
Epoch 280, val loss: 0.8090579509735107
Epoch 290, training loss: 0.3262222707271576 = 0.3188881278038025 + 0.001 * 7.334147930145264
Epoch 290, val loss: 0.7992128133773804
Epoch 300, training loss: 0.2904137670993805 = 0.28308984637260437 + 0.001 * 7.323916435241699
Epoch 300, val loss: 0.7919894456863403
Epoch 310, training loss: 0.25710248947143555 = 0.2498047947883606 + 0.001 * 7.29768705368042
Epoch 310, val loss: 0.7874402403831482
Epoch 320, training loss: 0.2264477014541626 = 0.21916833519935608 + 0.001 * 7.279361724853516
Epoch 320, val loss: 0.7857112884521484
Epoch 330, training loss: 0.19861894845962524 = 0.19134362041950226 + 0.001 * 7.275325775146484
Epoch 330, val loss: 0.7867468595504761
Epoch 340, training loss: 0.17371505498886108 = 0.1664474606513977 + 0.001 * 7.267594337463379
Epoch 340, val loss: 0.7904430031776428
Epoch 350, training loss: 0.151741623878479 = 0.14447897672653198 + 0.001 * 7.262653350830078
Epoch 350, val loss: 0.796704888343811
Epoch 360, training loss: 0.1325675994157791 = 0.12530829012393951 + 0.001 * 7.259309768676758
Epoch 360, val loss: 0.8053004741668701
Epoch 370, training loss: 0.1159801259636879 = 0.10872087627649307 + 0.001 * 7.259252071380615
Epoch 370, val loss: 0.8159163594245911
Epoch 380, training loss: 0.10171304643154144 = 0.09445946663618088 + 0.001 * 7.253582954406738
Epoch 380, val loss: 0.8282358050346375
Epoch 390, training loss: 0.08950404822826385 = 0.08225417882204056 + 0.001 * 7.249868392944336
Epoch 390, val loss: 0.8419157266616821
Epoch 400, training loss: 0.07909420132637024 = 0.0718432143330574 + 0.001 * 7.25098991394043
Epoch 400, val loss: 0.8565161228179932
Epoch 410, training loss: 0.07022453099489212 = 0.0629814937710762 + 0.001 * 7.243035316467285
Epoch 410, val loss: 0.8716679215431213
Epoch 420, training loss: 0.06268250197172165 = 0.05544353649020195 + 0.001 * 7.238964080810547
Epoch 420, val loss: 0.8870640397071838
Epoch 430, training loss: 0.05625869706273079 = 0.049026258289813995 + 0.001 * 7.232437610626221
Epoch 430, val loss: 0.9024514555931091
Epoch 440, training loss: 0.05078328028321266 = 0.04355081543326378 + 0.001 * 7.23246431350708
Epoch 440, val loss: 0.9176979660987854
Epoch 450, training loss: 0.04609079286456108 = 0.038865432143211365 + 0.001 * 7.225359916687012
Epoch 450, val loss: 0.9326847195625305
Epoch 460, training loss: 0.04207124188542366 = 0.0348428450524807 + 0.001 * 7.228397369384766
Epoch 460, val loss: 0.9473469257354736
Epoch 470, training loss: 0.03859999030828476 = 0.03137632831931114 + 0.001 * 7.223662376403809
Epoch 470, val loss: 0.9616333246231079
Epoch 480, training loss: 0.03559841960668564 = 0.02837798185646534 + 0.001 * 7.2204389572143555
Epoch 480, val loss: 0.9755618572235107
Epoch 490, training loss: 0.032983776181936264 = 0.025773825123906136 + 0.001 * 7.209950923919678
Epoch 490, val loss: 0.9891144633293152
Epoch 500, training loss: 0.030719250440597534 = 0.023502426221966743 + 0.001 * 7.216824054718018
Epoch 500, val loss: 1.0022895336151123
Epoch 510, training loss: 0.028723448514938354 = 0.021513434126973152 + 0.001 * 7.210014820098877
Epoch 510, val loss: 1.0150935649871826
Epoch 520, training loss: 0.02696942910552025 = 0.019764499738812447 + 0.001 * 7.204927921295166
Epoch 520, val loss: 1.0275222063064575
Epoch 530, training loss: 0.02541794255375862 = 0.018219944089651108 + 0.001 * 7.197999000549316
Epoch 530, val loss: 1.0395715236663818
Epoch 540, training loss: 0.02405913732945919 = 0.01685049943625927 + 0.001 * 7.2086381912231445
Epoch 540, val loss: 1.051257610321045
Epoch 550, training loss: 0.022827351465821266 = 0.015631677582859993 + 0.001 * 7.19567346572876
Epoch 550, val loss: 1.0625845193862915
Epoch 560, training loss: 0.021731648594141006 = 0.014542948454618454 + 0.001 * 7.188698768615723
Epoch 560, val loss: 1.073556661605835
Epoch 570, training loss: 0.020757004618644714 = 0.013566861860454082 + 0.001 * 7.190142631530762
Epoch 570, val loss: 1.0841827392578125
Epoch 580, training loss: 0.019872117787599564 = 0.012688716873526573 + 0.001 * 7.183401107788086
Epoch 580, val loss: 1.094488501548767
Epoch 590, training loss: 0.01910017430782318 = 0.011896144598722458 + 0.001 * 7.2040300369262695
Epoch 590, val loss: 1.1044906377792358
Epoch 600, training loss: 0.01837054267525673 = 0.011178714223206043 + 0.001 * 7.191828727722168
Epoch 600, val loss: 1.114193081855774
Epoch 610, training loss: 0.017705414444208145 = 0.010527235455811024 + 0.001 * 7.178177833557129
Epoch 610, val loss: 1.1235809326171875
Epoch 620, training loss: 0.017116611823439598 = 0.009933850727975368 + 0.001 * 7.182761192321777
Epoch 620, val loss: 1.1326963901519775
Epoch 630, training loss: 0.01656433567404747 = 0.009391959756612778 + 0.001 * 7.1723761558532715
Epoch 630, val loss: 1.141544222831726
Epoch 640, training loss: 0.016087990254163742 = 0.008895832113921642 + 0.001 * 7.192158222198486
Epoch 640, val loss: 1.1501585245132446
Epoch 650, training loss: 0.015620958060026169 = 0.008440502919256687 + 0.001 * 7.180454730987549
Epoch 650, val loss: 1.158512830734253
Epoch 660, training loss: 0.01519136130809784 = 0.008021610789000988 + 0.001 * 7.169750213623047
Epoch 660, val loss: 1.1666432619094849
Epoch 670, training loss: 0.01480040792375803 = 0.007635321002453566 + 0.001 * 7.16508674621582
Epoch 670, val loss: 1.1745344400405884
Epoch 680, training loss: 0.014448218047618866 = 0.007278400007635355 + 0.001 * 7.169817924499512
Epoch 680, val loss: 1.1822388172149658
Epoch 690, training loss: 0.01412228588014841 = 0.006947934627532959 + 0.001 * 7.174350738525391
Epoch 690, val loss: 1.1897259950637817
Epoch 700, training loss: 0.013811919838190079 = 0.0066413599997758865 + 0.001 * 7.170559406280518
Epoch 700, val loss: 1.1970242261886597
Epoch 710, training loss: 0.01351807452738285 = 0.0063562821596860886 + 0.001 * 7.161791801452637
Epoch 710, val loss: 1.2041441202163696
Epoch 720, training loss: 0.01324513740837574 = 0.006090623326599598 + 0.001 * 7.154513835906982
Epoch 720, val loss: 1.2110939025878906
Epoch 730, training loss: 0.012996713630855083 = 0.00584231736138463 + 0.001 * 7.154396057128906
Epoch 730, val loss: 1.2179044485092163
Epoch 740, training loss: 0.0127607062458992 = 0.005609696265310049 + 0.001 * 7.1510090827941895
Epoch 740, val loss: 1.22456955909729
Epoch 750, training loss: 0.012545881792902946 = 0.005390884354710579 + 0.001 * 7.154996871948242
Epoch 750, val loss: 1.2311468124389648
Epoch 760, training loss: 0.012351898476481438 = 0.005184385925531387 + 0.001 * 7.167511940002441
Epoch 760, val loss: 1.2376333475112915
Epoch 770, training loss: 0.0121476911008358 = 0.004988929722458124 + 0.001 * 7.158761024475098
Epoch 770, val loss: 1.2440624237060547
Epoch 780, training loss: 0.011974044144153595 = 0.004803606774657965 + 0.001 * 7.170437335968018
Epoch 780, val loss: 1.2504440546035767
Epoch 790, training loss: 0.011779572814702988 = 0.0046281698159873486 + 0.001 * 7.151402950286865
Epoch 790, val loss: 1.2567474842071533
Epoch 800, training loss: 0.011607147753238678 = 0.004461793694645166 + 0.001 * 7.145354270935059
Epoch 800, val loss: 1.2629672288894653
Epoch 810, training loss: 0.011456780135631561 = 0.00430416502058506 + 0.001 * 7.152614593505859
Epoch 810, val loss: 1.2691233158111572
Epoch 820, training loss: 0.011322060599923134 = 0.004154703114181757 + 0.001 * 7.167356491088867
Epoch 820, val loss: 1.275189757347107
Epoch 830, training loss: 0.011160353198647499 = 0.00401302007958293 + 0.001 * 7.147332191467285
Epoch 830, val loss: 1.2811717987060547
Epoch 840, training loss: 0.011010229587554932 = 0.0038788013625890017 + 0.001 * 7.131427764892578
Epoch 840, val loss: 1.287065863609314
Epoch 850, training loss: 0.010880681686103344 = 0.003751443699002266 + 0.001 * 7.129237651824951
Epoch 850, val loss: 1.2928721904754639
Epoch 860, training loss: 0.010782996192574501 = 0.0036305440589785576 + 0.001 * 7.15245246887207
Epoch 860, val loss: 1.2986016273498535
Epoch 870, training loss: 0.010656064376235008 = 0.0035158470273017883 + 0.001 * 7.140216827392578
Epoch 870, val loss: 1.304197072982788
Epoch 880, training loss: 0.010557333938777447 = 0.0034071062691509724 + 0.001 * 7.1502275466918945
Epoch 880, val loss: 1.3097261190414429
Epoch 890, training loss: 0.010427940636873245 = 0.003303912468254566 + 0.001 * 7.124027729034424
Epoch 890, val loss: 1.3151224851608276
Epoch 900, training loss: 0.010327152907848358 = 0.0032058251090347767 + 0.001 * 7.1213274002075195
Epoch 900, val loss: 1.3204387426376343
Epoch 910, training loss: 0.010247522965073586 = 0.0031125263776630163 + 0.001 * 7.13499641418457
Epoch 910, val loss: 1.3256537914276123
Epoch 920, training loss: 0.010130142793059349 = 0.0030237827450037003 + 0.001 * 7.106359481811523
Epoch 920, val loss: 1.3307695388793945
Epoch 930, training loss: 0.010067791678011417 = 0.0029394112061709166 + 0.001 * 7.128379821777344
Epoch 930, val loss: 1.335815191268921
Epoch 940, training loss: 0.009983927011489868 = 0.0028590981382876635 + 0.001 * 7.124828338623047
Epoch 940, val loss: 1.3407329320907593
Epoch 950, training loss: 0.009885557927191257 = 0.002782596042379737 + 0.001 * 7.102961540222168
Epoch 950, val loss: 1.3455804586410522
Epoch 960, training loss: 0.009815169498324394 = 0.002709593391045928 + 0.001 * 7.105576038360596
Epoch 960, val loss: 1.3503416776657104
Epoch 970, training loss: 0.009744024835526943 = 0.002639927202835679 + 0.001 * 7.10409688949585
Epoch 970, val loss: 1.35501229763031
Epoch 980, training loss: 0.00967391300946474 = 0.0025734349619597197 + 0.001 * 7.100477695465088
Epoch 980, val loss: 1.359604835510254
Epoch 990, training loss: 0.009631912223994732 = 0.002509890589863062 + 0.001 * 7.122021198272705
Epoch 990, val loss: 1.3640867471694946
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 1.9527864456176758 = 1.9441896677017212 + 0.001 * 8.596797943115234
Epoch 0, val loss: 1.9482204914093018
Epoch 10, training loss: 1.942394733428955 = 1.9337979555130005 + 0.001 * 8.596735954284668
Epoch 10, val loss: 1.9373681545257568
Epoch 20, training loss: 1.9294118881225586 = 1.920815348625183 + 0.001 * 8.59655475616455
Epoch 20, val loss: 1.9237219095230103
Epoch 30, training loss: 1.911163568496704 = 1.9025673866271973 + 0.001 * 8.596134185791016
Epoch 30, val loss: 1.904474139213562
Epoch 40, training loss: 1.8847836256027222 = 1.8761885166168213 + 0.001 * 8.595147132873535
Epoch 40, val loss: 1.8770264387130737
Epoch 50, training loss: 1.8498597145080566 = 1.8412671089172363 + 0.001 * 8.592549324035645
Epoch 50, val loss: 1.8425227403640747
Epoch 60, training loss: 1.8149101734161377 = 1.8063260316848755 + 0.001 * 8.584185600280762
Epoch 60, val loss: 1.8121113777160645
Epoch 70, training loss: 1.7849526405334473 = 1.7764025926589966 + 0.001 * 8.54999828338623
Epoch 70, val loss: 1.7881405353546143
Epoch 80, training loss: 1.7464455366134644 = 1.7381196022033691 + 0.001 * 8.3259916305542
Epoch 80, val loss: 1.7549750804901123
Epoch 90, training loss: 1.6924599409103394 = 1.6844037771224976 + 0.001 * 8.056106567382812
Epoch 90, val loss: 1.7067523002624512
Epoch 100, training loss: 1.619755744934082 = 1.6117967367172241 + 0.001 * 7.958980083465576
Epoch 100, val loss: 1.6431132555007935
Epoch 110, training loss: 1.5335501432418823 = 1.5257139205932617 + 0.001 * 7.836279392242432
Epoch 110, val loss: 1.571622610092163
Epoch 120, training loss: 1.4428250789642334 = 1.4351543188095093 + 0.001 * 7.670738220214844
Epoch 120, val loss: 1.4989064931869507
Epoch 130, training loss: 1.3511033058166504 = 1.3434962034225464 + 0.001 * 7.6070733070373535
Epoch 130, val loss: 1.4273993968963623
Epoch 140, training loss: 1.2580832242965698 = 1.250505805015564 + 0.001 * 7.577406406402588
Epoch 140, val loss: 1.3561465740203857
Epoch 150, training loss: 1.165518879890442 = 1.1579886674880981 + 0.001 * 7.530221462249756
Epoch 150, val loss: 1.286200761795044
Epoch 160, training loss: 1.0767236948013306 = 1.0692387819290161 + 0.001 * 7.484854698181152
Epoch 160, val loss: 1.2202032804489136
Epoch 170, training loss: 0.9937987923622131 = 0.9863601922988892 + 0.001 * 7.438607692718506
Epoch 170, val loss: 1.1603208780288696
Epoch 180, training loss: 0.9160799384117126 = 0.908689022064209 + 0.001 * 7.39094352722168
Epoch 180, val loss: 1.1059094667434692
Epoch 190, training loss: 0.8423125743865967 = 0.8349522352218628 + 0.001 * 7.360359191894531
Epoch 190, val loss: 1.055964469909668
Epoch 200, training loss: 0.7724375128746033 = 0.76508629322052 + 0.001 * 7.351219177246094
Epoch 200, val loss: 1.010741949081421
Epoch 210, training loss: 0.7076724171638489 = 0.7003244757652283 + 0.001 * 7.347919464111328
Epoch 210, val loss: 0.9717064499855042
Epoch 220, training loss: 0.649124264717102 = 0.6417824029922485 + 0.001 * 7.341853141784668
Epoch 220, val loss: 0.9407368302345276
Epoch 230, training loss: 0.596716582775116 = 0.5893828868865967 + 0.001 * 7.3337202072143555
Epoch 230, val loss: 0.9181936383247375
Epoch 240, training loss: 0.5495979189872742 = 0.5422776937484741 + 0.001 * 7.320228099822998
Epoch 240, val loss: 0.9032556414604187
Epoch 250, training loss: 0.5065970420837402 = 0.49929726123809814 + 0.001 * 7.299801826477051
Epoch 250, val loss: 0.8944351077079773
Epoch 260, training loss: 0.4662940502166748 = 0.4590194523334503 + 0.001 * 7.27459716796875
Epoch 260, val loss: 0.8902043104171753
Epoch 270, training loss: 0.42735785245895386 = 0.42011019587516785 + 0.001 * 7.2476630210876465
Epoch 270, val loss: 0.8890411853790283
Epoch 280, training loss: 0.38892194628715515 = 0.38169726729393005 + 0.001 * 7.224688529968262
Epoch 280, val loss: 0.8908500075340271
Epoch 290, training loss: 0.35098057985305786 = 0.34376275539398193 + 0.001 * 7.217822551727295
Epoch 290, val loss: 0.8955684900283813
Epoch 300, training loss: 0.3142322897911072 = 0.3070231080055237 + 0.001 * 7.20916748046875
Epoch 300, val loss: 0.9040828943252563
Epoch 310, training loss: 0.27960386872291565 = 0.2723950147628784 + 0.001 * 7.208842754364014
Epoch 310, val loss: 0.9166733622550964
Epoch 320, training loss: 0.24776092171669006 = 0.24055522680282593 + 0.001 * 7.2057013511657715
Epoch 320, val loss: 0.9334211945533752
Epoch 330, training loss: 0.21905885636806488 = 0.21185611188411713 + 0.001 * 7.202744007110596
Epoch 330, val loss: 0.9540722966194153
Epoch 340, training loss: 0.19349545240402222 = 0.1862965226173401 + 0.001 * 7.198934555053711
Epoch 340, val loss: 0.9779742360115051
Epoch 350, training loss: 0.17088021337985992 = 0.16368621587753296 + 0.001 * 7.193995952606201
Epoch 350, val loss: 1.0037906169891357
Epoch 360, training loss: 0.15099473297595978 = 0.14380943775177002 + 0.001 * 7.185295104980469
Epoch 360, val loss: 1.0313633680343628
Epoch 370, training loss: 0.13351313769817352 = 0.1263294368982315 + 0.001 * 7.183703422546387
Epoch 370, val loss: 1.0595890283584595
Epoch 380, training loss: 0.11812548339366913 = 0.11094996333122253 + 0.001 * 7.175521373748779
Epoch 380, val loss: 1.0881009101867676
Epoch 390, training loss: 0.1046290248632431 = 0.09746462106704712 + 0.001 * 7.164401531219482
Epoch 390, val loss: 1.1163861751556396
Epoch 400, training loss: 0.0929093286395073 = 0.08573544025421143 + 0.001 * 7.173886299133301
Epoch 400, val loss: 1.143934965133667
Epoch 410, training loss: 0.08276810497045517 = 0.07561731338500977 + 0.001 * 7.150794506072998
Epoch 410, val loss: 1.170702338218689
Epoch 420, training loss: 0.07407093793153763 = 0.06692598760128021 + 0.001 * 7.144949436187744
Epoch 420, val loss: 1.1965864896774292
Epoch 430, training loss: 0.0665922462940216 = 0.059456076472997665 + 0.001 * 7.136171817779541
Epoch 430, val loss: 1.221461534500122
Epoch 440, training loss: 0.06017589196562767 = 0.053019266575574875 + 0.001 * 7.156624794006348
Epoch 440, val loss: 1.2454514503479004
Epoch 450, training loss: 0.054586563259363174 = 0.04745456203818321 + 0.001 * 7.13200044631958
Epoch 450, val loss: 1.268385648727417
Epoch 460, training loss: 0.049754418432712555 = 0.04262732341885567 + 0.001 * 7.127094268798828
Epoch 460, val loss: 1.2904776334762573
Epoch 470, training loss: 0.045548900961875916 = 0.038427166640758514 + 0.001 * 7.121734142303467
Epoch 470, val loss: 1.3117660284042358
Epoch 480, training loss: 0.04188231751322746 = 0.034761302173137665 + 0.001 * 7.121013641357422
Epoch 480, val loss: 1.332188367843628
Epoch 490, training loss: 0.0386652834713459 = 0.0315537266433239 + 0.001 * 7.111556053161621
Epoch 490, val loss: 1.3518412113189697
Epoch 500, training loss: 0.03585514426231384 = 0.028740214183926582 + 0.001 * 7.114928245544434
Epoch 500, val loss: 1.370840072631836
Epoch 510, training loss: 0.03337655961513519 = 0.026264963671565056 + 0.001 * 7.1115946769714355
Epoch 510, val loss: 1.3891712427139282
Epoch 520, training loss: 0.03119508922100067 = 0.024082280695438385 + 0.001 * 7.112809181213379
Epoch 520, val loss: 1.4068018198013306
Epoch 530, training loss: 0.029260806739330292 = 0.022151850163936615 + 0.001 * 7.108956336975098
Epoch 530, val loss: 1.4237630367279053
Epoch 540, training loss: 0.027540910989046097 = 0.020439641550183296 + 0.001 * 7.101269721984863
Epoch 540, val loss: 1.4401191473007202
Epoch 550, training loss: 0.026012739166617393 = 0.01891634427011013 + 0.001 * 7.096394062042236
Epoch 550, val loss: 1.4559038877487183
Epoch 560, training loss: 0.024670103564858437 = 0.017556974664330482 + 0.001 * 7.113128185272217
Epoch 560, val loss: 1.4711582660675049
Epoch 570, training loss: 0.023443501442670822 = 0.016340160742402077 + 0.001 * 7.103339672088623
Epoch 570, val loss: 1.4858235120773315
Epoch 580, training loss: 0.02234342321753502 = 0.015247415751218796 + 0.001 * 7.096007347106934
Epoch 580, val loss: 1.4999569654464722
Epoch 590, training loss: 0.021351289004087448 = 0.014262902550399303 + 0.001 * 7.088386535644531
Epoch 590, val loss: 1.5136430263519287
Epoch 600, training loss: 0.020475512370467186 = 0.013372743502259254 + 0.001 * 7.102768898010254
Epoch 600, val loss: 1.5268417596817017
Epoch 610, training loss: 0.019661754369735718 = 0.01256437785923481 + 0.001 * 7.097376823425293
Epoch 610, val loss: 1.5396244525909424
Epoch 620, training loss: 0.01891614869236946 = 0.011825256049633026 + 0.001 * 7.090891361236572
Epoch 620, val loss: 1.5520312786102295
Epoch 630, training loss: 0.018233636394143105 = 0.011146079748868942 + 0.001 * 7.087555885314941
Epoch 630, val loss: 1.5640474557876587
Epoch 640, training loss: 0.017615554854273796 = 0.01052029337733984 + 0.001 * 7.095261096954346
Epoch 640, val loss: 1.5756685733795166
Epoch 650, training loss: 0.01702679879963398 = 0.00994296558201313 + 0.001 * 7.083832740783691
Epoch 650, val loss: 1.5868617296218872
Epoch 660, training loss: 0.016488268971443176 = 0.009409886784851551 + 0.001 * 7.078381538391113
Epoch 660, val loss: 1.5977128744125366
Epoch 670, training loss: 0.015993377193808556 = 0.008917227387428284 + 0.001 * 7.0761494636535645
Epoch 670, val loss: 1.6081422567367554
Epoch 680, training loss: 0.015536880120635033 = 0.0084616020321846 + 0.001 * 7.075277328491211
Epoch 680, val loss: 1.6182087659835815
Epoch 690, training loss: 0.015113401226699352 = 0.008039898239076138 + 0.001 * 7.073502540588379
Epoch 690, val loss: 1.6279469728469849
Epoch 700, training loss: 0.014718394726514816 = 0.007649201434105635 + 0.001 * 7.069192409515381
Epoch 700, val loss: 1.637291669845581
Epoch 710, training loss: 0.01435299776494503 = 0.0072868624702095985 + 0.001 * 7.066135406494141
Epoch 710, val loss: 1.6464189291000366
Epoch 720, training loss: 0.014028242789208889 = 0.006950545124709606 + 0.001 * 7.077697277069092
Epoch 720, val loss: 1.6552305221557617
Epoch 730, training loss: 0.013711265288293362 = 0.006637898739427328 + 0.001 * 7.073366165161133
Epoch 730, val loss: 1.6638041734695435
Epoch 740, training loss: 0.013411257416009903 = 0.006346826907247305 + 0.001 * 7.064429759979248
Epoch 740, val loss: 1.672058343887329
Epoch 750, training loss: 0.013141890987753868 = 0.0060754925943911076 + 0.001 * 7.0663981437683105
Epoch 750, val loss: 1.6800811290740967
Epoch 760, training loss: 0.012889595702290535 = 0.005822236184030771 + 0.001 * 7.067359447479248
Epoch 760, val loss: 1.687822937965393
Epoch 770, training loss: 0.01265808381140232 = 0.00558555219322443 + 0.001 * 7.072530746459961
Epoch 770, val loss: 1.6953665018081665
Epoch 780, training loss: 0.012422166764736176 = 0.0053640748374164104 + 0.001 * 7.058091640472412
Epoch 780, val loss: 1.7026361227035522
Epoch 790, training loss: 0.012211080640554428 = 0.005156536120921373 + 0.001 * 7.054543972015381
Epoch 790, val loss: 1.7097344398498535
Epoch 800, training loss: 0.012025349773466587 = 0.004961803089827299 + 0.001 * 7.063546180725098
Epoch 800, val loss: 1.7166016101837158
Epoch 810, training loss: 0.011836551129817963 = 0.004778911825269461 + 0.001 * 7.0576395988464355
Epoch 810, val loss: 1.7232835292816162
Epoch 820, training loss: 0.01166125200688839 = 0.00460692448541522 + 0.001 * 7.05432653427124
Epoch 820, val loss: 1.7297526597976685
Epoch 830, training loss: 0.01150667667388916 = 0.004445027559995651 + 0.001 * 7.061648845672607
Epoch 830, val loss: 1.7360607385635376
Epoch 840, training loss: 0.011340443044900894 = 0.004292401950806379 + 0.001 * 7.048040390014648
Epoch 840, val loss: 1.742222547531128
Epoch 850, training loss: 0.011197146028280258 = 0.004148409701883793 + 0.001 * 7.048735618591309
Epoch 850, val loss: 1.7481788396835327
Epoch 860, training loss: 0.011052552610635757 = 0.004012380726635456 + 0.001 * 7.0401716232299805
Epoch 860, val loss: 1.7539989948272705
Epoch 870, training loss: 0.010922783054411411 = 0.0038837704341858625 + 0.001 * 7.0390119552612305
Epoch 870, val loss: 1.7596579790115356
Epoch 880, training loss: 0.010800570249557495 = 0.0037620605435222387 + 0.001 * 7.038509368896484
Epoch 880, val loss: 1.765189290046692
Epoch 890, training loss: 0.010699181817471981 = 0.0036467930767685175 + 0.001 * 7.0523881912231445
Epoch 890, val loss: 1.770585298538208
Epoch 900, training loss: 0.010578038170933723 = 0.003537507727742195 + 0.001 * 7.040530204772949
Epoch 900, val loss: 1.775802731513977
Epoch 910, training loss: 0.010480636730790138 = 0.0034338217228651047 + 0.001 * 7.046814441680908
Epoch 910, val loss: 1.7809523344039917
Epoch 920, training loss: 0.010370349511504173 = 0.0033353338949382305 + 0.001 * 7.035014629364014
Epoch 920, val loss: 1.7859200239181519
Epoch 930, training loss: 0.010304229333996773 = 0.0032417108304798603 + 0.001 * 7.062518119812012
Epoch 930, val loss: 1.7908015251159668
Epoch 940, training loss: 0.010190006345510483 = 0.003152624238282442 + 0.001 * 7.037381649017334
Epoch 940, val loss: 1.7955186367034912
Epoch 950, training loss: 0.010105595923960209 = 0.0030678303446620703 + 0.001 * 7.037765026092529
Epoch 950, val loss: 1.8001981973648071
Epoch 960, training loss: 0.010016035288572311 = 0.0029870178550481796 + 0.001 * 7.029016971588135
Epoch 960, val loss: 1.8047130107879639
Epoch 970, training loss: 0.009946031495928764 = 0.0029099516104906797 + 0.001 * 7.036079406738281
Epoch 970, val loss: 1.8091131448745728
Epoch 980, training loss: 0.00987003929913044 = 0.002836429513990879 + 0.001 * 7.033609867095947
Epoch 980, val loss: 1.8134207725524902
Epoch 990, training loss: 0.009809497743844986 = 0.002766238758340478 + 0.001 * 7.0432586669921875
Epoch 990, val loss: 1.8176459074020386
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 1.957445740699768 = 1.9488489627838135 + 0.001 * 8.59682846069336
Epoch 0, val loss: 1.948557734489441
Epoch 10, training loss: 1.9469352960586548 = 1.9383385181427002 + 0.001 * 8.596760749816895
Epoch 10, val loss: 1.9378445148468018
Epoch 20, training loss: 1.9339454174041748 = 1.9253488779067993 + 0.001 * 8.59659194946289
Epoch 20, val loss: 1.9244540929794312
Epoch 30, training loss: 1.9158090353012085 = 1.9072128534317017 + 0.001 * 8.596223831176758
Epoch 30, val loss: 1.9057954549789429
Epoch 40, training loss: 1.8895044326782227 = 1.8809090852737427 + 0.001 * 8.595366477966309
Epoch 40, val loss: 1.8794617652893066
Epoch 50, training loss: 1.8547087907791138 = 1.8461158275604248 + 0.001 * 8.592972755432129
Epoch 50, val loss: 1.847273826599121
Epoch 60, training loss: 1.8205572366714478 = 1.8119730949401855 + 0.001 * 8.584150314331055
Epoch 60, val loss: 1.8209714889526367
Epoch 70, training loss: 1.7947970628738403 = 1.7862582206726074 + 0.001 * 8.538851737976074
Epoch 70, val loss: 1.8027350902557373
Epoch 80, training loss: 1.7612804174423218 = 1.753057837486267 + 0.001 * 8.222542762756348
Epoch 80, val loss: 1.773666501045227
Epoch 90, training loss: 1.7144213914871216 = 1.706303596496582 + 0.001 * 8.117822647094727
Epoch 90, val loss: 1.7336000204086304
Epoch 100, training loss: 1.6471261978149414 = 1.639094591140747 + 0.001 * 8.031623840332031
Epoch 100, val loss: 1.6792395114898682
Epoch 110, training loss: 1.5632920265197754 = 1.5553773641586304 + 0.001 * 7.914703369140625
Epoch 110, val loss: 1.6144816875457764
Epoch 120, training loss: 1.4744353294372559 = 1.4667327404022217 + 0.001 * 7.702593803405762
Epoch 120, val loss: 1.547998309135437
Epoch 130, training loss: 1.3871814012527466 = 1.379549264907837 + 0.001 * 7.632193565368652
Epoch 130, val loss: 1.4859315156936646
Epoch 140, training loss: 1.3009992837905884 = 1.2934575080871582 + 0.001 * 7.541816711425781
Epoch 140, val loss: 1.4252642393112183
Epoch 150, training loss: 1.2135499715805054 = 1.2060669660568237 + 0.001 * 7.483057975769043
Epoch 150, val loss: 1.3629858493804932
Epoch 160, training loss: 1.1251229047775269 = 1.1176784038543701 + 0.001 * 7.444507598876953
Epoch 160, val loss: 1.29896879196167
Epoch 170, training loss: 1.0381923913955688 = 1.0307759046554565 + 0.001 * 7.416435241699219
Epoch 170, val loss: 1.236515998840332
Epoch 180, training loss: 0.9545249938964844 = 0.9471257328987122 + 0.001 * 7.3992695808410645
Epoch 180, val loss: 1.1787177324295044
Epoch 190, training loss: 0.8746025562286377 = 0.8672155737876892 + 0.001 * 7.386970520019531
Epoch 190, val loss: 1.127218246459961
Epoch 200, training loss: 0.7987201809883118 = 0.7913404107093811 + 0.001 * 7.379786014556885
Epoch 200, val loss: 1.0823333263397217
Epoch 210, training loss: 0.7278435230255127 = 0.7204713225364685 + 0.001 * 7.372218608856201
Epoch 210, val loss: 1.0431050062179565
Epoch 220, training loss: 0.6627069711685181 = 0.6553409695625305 + 0.001 * 7.3660197257995605
Epoch 220, val loss: 1.0095069408416748
Epoch 230, training loss: 0.6032567620277405 = 0.5958960652351379 + 0.001 * 7.360702037811279
Epoch 230, val loss: 0.9814321994781494
Epoch 240, training loss: 0.5490001440048218 = 0.5416440963745117 + 0.001 * 7.356075286865234
Epoch 240, val loss: 0.9582287669181824
Epoch 250, training loss: 0.4994657337665558 = 0.4921174943447113 + 0.001 * 7.348241806030273
Epoch 250, val loss: 0.9398140907287598
Epoch 260, training loss: 0.4540907144546509 = 0.4467490613460541 + 0.001 * 7.341653823852539
Epoch 260, val loss: 0.9259831309318542
Epoch 270, training loss: 0.4122142195701599 = 0.404878169298172 + 0.001 * 7.3360443115234375
Epoch 270, val loss: 0.9166483283042908
Epoch 280, training loss: 0.37317898869514465 = 0.3658539354801178 + 0.001 * 7.3250579833984375
Epoch 280, val loss: 0.9112219214439392
Epoch 290, training loss: 0.33660888671875 = 0.32928964495658875 + 0.001 * 7.319244384765625
Epoch 290, val loss: 0.9094477891921997
Epoch 300, training loss: 0.3022684156894684 = 0.2949635684490204 + 0.001 * 7.304838180541992
Epoch 300, val loss: 0.9109784364700317
Epoch 310, training loss: 0.27001020312309265 = 0.2627035677433014 + 0.001 * 7.306639671325684
Epoch 310, val loss: 0.9154434204101562
Epoch 320, training loss: 0.23954342305660248 = 0.23225943744182587 + 0.001 * 7.2839789390563965
Epoch 320, val loss: 0.9218886494636536
Epoch 330, training loss: 0.2107115238904953 = 0.20343834161758423 + 0.001 * 7.273186206817627
Epoch 330, val loss: 0.929695188999176
Epoch 340, training loss: 0.18361437320709229 = 0.17634758353233337 + 0.001 * 7.2667927742004395
Epoch 340, val loss: 0.938636839389801
Epoch 350, training loss: 0.15866637229919434 = 0.15141580998897552 + 0.001 * 7.25055456161499
Epoch 350, val loss: 0.9487887024879456
Epoch 360, training loss: 0.13640601933002472 = 0.12915970385074615 + 0.001 * 7.246318817138672
Epoch 360, val loss: 0.9602179527282715
Epoch 370, training loss: 0.11711882799863815 = 0.10989279299974442 + 0.001 * 7.226032733917236
Epoch 370, val loss: 0.973080575466156
Epoch 380, training loss: 0.10085228085517883 = 0.09361845999956131 + 0.001 * 7.233819007873535
Epoch 380, val loss: 0.9872618317604065
Epoch 390, training loss: 0.08733318001031876 = 0.08008010685443878 + 0.001 * 7.253073215484619
Epoch 390, val loss: 1.002557635307312
Epoch 400, training loss: 0.07610731571912766 = 0.06890032440423965 + 0.001 * 7.206990718841553
Epoch 400, val loss: 1.0186898708343506
Epoch 410, training loss: 0.06689788401126862 = 0.05968042463064194 + 0.001 * 7.217457294464111
Epoch 410, val loss: 1.0353301763534546
Epoch 420, training loss: 0.059234388172626495 = 0.05205186828970909 + 0.001 * 7.182520389556885
Epoch 420, val loss: 1.0521445274353027
Epoch 430, training loss: 0.052895817905664444 = 0.04570642113685608 + 0.001 * 7.189395904541016
Epoch 430, val loss: 1.0689231157302856
Epoch 440, training loss: 0.047584593296051025 = 0.04039612412452698 + 0.001 * 7.188469409942627
Epoch 440, val loss: 1.0854250192642212
Epoch 450, training loss: 0.043085791170597076 = 0.035924024879932404 + 0.001 * 7.161765098571777
Epoch 450, val loss: 1.1016184091567993
Epoch 460, training loss: 0.03929618373513222 = 0.032134514302015305 + 0.001 * 7.161667823791504
Epoch 460, val loss: 1.1173971891403198
Epoch 470, training loss: 0.03603891283273697 = 0.028903622180223465 + 0.001 * 7.135292053222656
Epoch 470, val loss: 1.1327743530273438
Epoch 480, training loss: 0.03327399492263794 = 0.026131683960556984 + 0.001 * 7.142309665679932
Epoch 480, val loss: 1.1476854085922241
Epoch 490, training loss: 0.03090035170316696 = 0.023739546537399292 + 0.001 * 7.16080379486084
Epoch 490, val loss: 1.1621885299682617
Epoch 500, training loss: 0.02880038134753704 = 0.02166198007762432 + 0.001 * 7.138401031494141
Epoch 500, val loss: 1.1762889623641968
Epoch 510, training loss: 0.02698221616446972 = 0.019847387447953224 + 0.001 * 7.134829044342041
Epoch 510, val loss: 1.1898653507232666
Epoch 520, training loss: 0.025432730093598366 = 0.018255742266774178 + 0.001 * 7.176987171173096
Epoch 520, val loss: 1.2030376195907593
Epoch 530, training loss: 0.023973004892468452 = 0.016852770000696182 + 0.001 * 7.120234489440918
Epoch 530, val loss: 1.2158088684082031
Epoch 540, training loss: 0.022711416706442833 = 0.015610100701451302 + 0.001 * 7.101315021514893
Epoch 540, val loss: 1.228202223777771
Epoch 550, training loss: 0.021633200347423553 = 0.014504441060125828 + 0.001 * 7.128758907318115
Epoch 550, val loss: 1.2402162551879883
Epoch 560, training loss: 0.0206252783536911 = 0.013516832143068314 + 0.001 * 7.10844612121582
Epoch 560, val loss: 1.2518680095672607
Epoch 570, training loss: 0.019723115488886833 = 0.012631013058125973 + 0.001 * 7.09210205078125
Epoch 570, val loss: 1.2631874084472656
Epoch 580, training loss: 0.0189475417137146 = 0.011833623051643372 + 0.001 * 7.113918781280518
Epoch 580, val loss: 1.2741389274597168
Epoch 590, training loss: 0.018256638199090958 = 0.01111313235014677 + 0.001 * 7.143505573272705
Epoch 590, val loss: 1.2847651243209839
Epoch 600, training loss: 0.017544150352478027 = 0.010460217483341694 + 0.001 * 7.083932399749756
Epoch 600, val loss: 1.2950778007507324
Epoch 610, training loss: 0.01697244681417942 = 0.009866533800959587 + 0.001 * 7.105912685394287
Epoch 610, val loss: 1.305113673210144
Epoch 620, training loss: 0.016422728076577187 = 0.009325176477432251 + 0.001 * 7.097551345825195
Epoch 620, val loss: 1.314835548400879
Epoch 630, training loss: 0.015919353812932968 = 0.008830208331346512 + 0.001 * 7.089146137237549
Epoch 630, val loss: 1.3242868185043335
Epoch 640, training loss: 0.015465027652680874 = 0.008376347832381725 + 0.001 * 7.088679313659668
Epoch 640, val loss: 1.3334687948226929
Epoch 650, training loss: 0.015089843422174454 = 0.007959201000630856 + 0.001 * 7.130641460418701
Epoch 650, val loss: 1.3424251079559326
Epoch 660, training loss: 0.01463591679930687 = 0.007574883289635181 + 0.001 * 7.061033725738525
Epoch 660, val loss: 1.3511168956756592
Epoch 670, training loss: 0.0143089909106493 = 0.007219996303319931 + 0.001 * 7.08899450302124
Epoch 670, val loss: 1.3595836162567139
Epoch 680, training loss: 0.013957623392343521 = 0.0068916636519134045 + 0.001 * 7.0659589767456055
Epoch 680, val loss: 1.367849588394165
Epoch 690, training loss: 0.013665847480297089 = 0.006587197072803974 + 0.001 * 7.078649997711182
Epoch 690, val loss: 1.3758913278579712
Epoch 700, training loss: 0.013389181345701218 = 0.0063043031841516495 + 0.001 * 7.084878444671631
Epoch 700, val loss: 1.3837060928344727
Epoch 710, training loss: 0.01310957595705986 = 0.00604105181992054 + 0.001 * 7.06852388381958
Epoch 710, val loss: 1.391358733177185
Epoch 720, training loss: 0.012862738221883774 = 0.005795641802251339 + 0.001 * 7.067095756530762
Epoch 720, val loss: 1.3988184928894043
Epoch 730, training loss: 0.012629283592104912 = 0.005566491279751062 + 0.001 * 7.062791347503662
Epoch 730, val loss: 1.4060947895050049
Epoch 740, training loss: 0.012430009432137012 = 0.005352137144654989 + 0.001 * 7.077871799468994
Epoch 740, val loss: 1.4132003784179688
Epoch 750, training loss: 0.012202201411128044 = 0.005151362624019384 + 0.001 * 7.050838947296143
Epoch 750, val loss: 1.4201239347457886
Epoch 760, training loss: 0.012013264931738377 = 0.004962994251400232 + 0.001 * 7.0502705574035645
Epoch 760, val loss: 1.4268938302993774
Epoch 770, training loss: 0.01182456687092781 = 0.004786093719303608 + 0.001 * 7.038473129272461
Epoch 770, val loss: 1.4335216283798218
Epoch 780, training loss: 0.011681284755468369 = 0.00461968407034874 + 0.001 * 7.0615997314453125
Epoch 780, val loss: 1.4399865865707397
Epoch 790, training loss: 0.011518711224198341 = 0.00446299696341157 + 0.001 * 7.0557146072387695
Epoch 790, val loss: 1.4462984800338745
Epoch 800, training loss: 0.011371908709406853 = 0.004315265920013189 + 0.001 * 7.056642532348633
Epoch 800, val loss: 1.4524978399276733
Epoch 810, training loss: 0.01120542362332344 = 0.0041757565923035145 + 0.001 * 7.029666900634766
Epoch 810, val loss: 1.458536982536316
Epoch 820, training loss: 0.01107165589928627 = 0.004043913912028074 + 0.001 * 7.027740955352783
Epoch 820, val loss: 1.4644479751586914
Epoch 830, training loss: 0.010954674333333969 = 0.003919188864529133 + 0.001 * 7.035485744476318
Epoch 830, val loss: 1.470249891281128
Epoch 840, training loss: 0.010871329344809055 = 0.0038010836578905582 + 0.001 * 7.070245265960693
Epoch 840, val loss: 1.4759174585342407
Epoch 850, training loss: 0.010729828849434853 = 0.003689049743115902 + 0.001 * 7.040778160095215
Epoch 850, val loss: 1.4814560413360596
Epoch 860, training loss: 0.01060057245194912 = 0.0035827921237796545 + 0.001 * 7.01777982711792
Epoch 860, val loss: 1.4868943691253662
Epoch 870, training loss: 0.010516362264752388 = 0.0034818234853446484 + 0.001 * 7.034538745880127
Epoch 870, val loss: 1.492209792137146
Epoch 880, training loss: 0.010411547496914864 = 0.003385788295418024 + 0.001 * 7.025758266448975
Epoch 880, val loss: 1.4974132776260376
Epoch 890, training loss: 0.01031356118619442 = 0.0032944660633802414 + 0.001 * 7.019095420837402
Epoch 890, val loss: 1.5025056600570679
Epoch 900, training loss: 0.010279984213411808 = 0.003207460278645158 + 0.001 * 7.072523593902588
Epoch 900, val loss: 1.5075057744979858
Epoch 910, training loss: 0.010142305865883827 = 0.003124550450593233 + 0.001 * 7.017754554748535
Epoch 910, val loss: 1.5124118328094482
Epoch 920, training loss: 0.010050798766314983 = 0.003045442281290889 + 0.001 * 7.005356311798096
Epoch 920, val loss: 1.5172133445739746
Epoch 930, training loss: 0.009984460659325123 = 0.002969898981973529 + 0.001 * 7.014561653137207
Epoch 930, val loss: 1.5219026803970337
Epoch 940, training loss: 0.009891333058476448 = 0.002897766185924411 + 0.001 * 6.993566513061523
Epoch 940, val loss: 1.526519536972046
Epoch 950, training loss: 0.009841869585216045 = 0.0028287810273468494 + 0.001 * 7.013088226318359
Epoch 950, val loss: 1.5310429334640503
Epoch 960, training loss: 0.009785917587578297 = 0.0027628398966044188 + 0.001 * 7.023077487945557
Epoch 960, val loss: 1.5354729890823364
Epoch 970, training loss: 0.009718071669340134 = 0.0026996431406587362 + 0.001 * 7.018428325653076
Epoch 970, val loss: 1.539815902709961
Epoch 980, training loss: 0.00964848231524229 = 0.002639160957187414 + 0.001 * 7.009321212768555
Epoch 980, val loss: 1.5440868139266968
Epoch 990, training loss: 0.00960663054138422 = 0.002581134205684066 + 0.001 * 7.025496006011963
Epoch 990, val loss: 1.5482566356658936
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.804955192409067
The final CL Acc:0.74938, 0.03345, The final GNN Acc:0.80460, 0.00025
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13192])
remove edge: torch.Size([2, 8024])
updated graph: torch.Size([2, 10660])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.969330072402954 = 1.9607332944869995 + 0.001 * 8.59682846069336
Epoch 0, val loss: 1.9544097185134888
Epoch 10, training loss: 1.9589741230010986 = 1.950377345085144 + 0.001 * 8.596779823303223
Epoch 10, val loss: 1.9447424411773682
Epoch 20, training loss: 1.9466791152954102 = 1.9380825757980347 + 0.001 * 8.596595764160156
Epoch 20, val loss: 1.9330726861953735
Epoch 30, training loss: 1.9298107624053955 = 1.9212145805358887 + 0.001 * 8.59615707397461
Epoch 30, val loss: 1.9168766736984253
Epoch 40, training loss: 1.9048748016357422 = 1.8962796926498413 + 0.001 * 8.595054626464844
Epoch 40, val loss: 1.8932883739471436
Epoch 50, training loss: 1.8689179420471191 = 1.8603264093399048 + 0.001 * 8.591556549072266
Epoch 50, val loss: 1.8608672618865967
Epoch 60, training loss: 1.82533597946167 = 1.816758155822754 + 0.001 * 8.57779312133789
Epoch 60, val loss: 1.8252649307250977
Epoch 70, training loss: 1.7872023582458496 = 1.7786788940429688 + 0.001 * 8.523412704467773
Epoch 70, val loss: 1.7962507009506226
Epoch 80, training loss: 1.7486218214035034 = 1.7403606176376343 + 0.001 * 8.261213302612305
Epoch 80, val loss: 1.7594082355499268
Epoch 90, training loss: 1.6941642761230469 = 1.6860425472259521 + 0.001 * 8.121696472167969
Epoch 90, val loss: 1.7081735134124756
Epoch 100, training loss: 1.6193606853485107 = 1.611316442489624 + 0.001 * 8.044295310974121
Epoch 100, val loss: 1.644024133682251
Epoch 110, training loss: 1.5275993347167969 = 1.5196003913879395 + 0.001 * 7.998953819274902
Epoch 110, val loss: 1.568480372428894
Epoch 120, training loss: 1.4333008527755737 = 1.425352692604065 + 0.001 * 7.948185920715332
Epoch 120, val loss: 1.4904799461364746
Epoch 130, training loss: 1.3458112478256226 = 1.3380036354064941 + 0.001 * 7.807664394378662
Epoch 130, val loss: 1.4185857772827148
Epoch 140, training loss: 1.2660058736801147 = 1.2584704160690308 + 0.001 * 7.535441875457764
Epoch 140, val loss: 1.3546065092086792
Epoch 150, training loss: 1.1916919946670532 = 1.1841844320297241 + 0.001 * 7.5075764656066895
Epoch 150, val loss: 1.2966606616973877
Epoch 160, training loss: 1.119080901145935 = 1.111622929573059 + 0.001 * 7.458009243011475
Epoch 160, val loss: 1.2411409616470337
Epoch 170, training loss: 1.0431573390960693 = 1.0357396602630615 + 0.001 * 7.417664051055908
Epoch 170, val loss: 1.1820658445358276
Epoch 180, training loss: 0.9607767462730408 = 0.9534061551094055 + 0.001 * 7.370607376098633
Epoch 180, val loss: 1.116532802581787
Epoch 190, training loss: 0.8729903101921082 = 0.8656476736068726 + 0.001 * 7.342621803283691
Epoch 190, val loss: 1.0457227230072021
Epoch 200, training loss: 0.7842783331871033 = 0.7769457697868347 + 0.001 * 7.332557678222656
Epoch 200, val loss: 0.9749184846878052
Epoch 210, training loss: 0.6995984315872192 = 0.6922746300697327 + 0.001 * 7.323784351348877
Epoch 210, val loss: 0.9100375175476074
Epoch 220, training loss: 0.6222426891326904 = 0.6149293184280396 + 0.001 * 7.313354015350342
Epoch 220, val loss: 0.8548440933227539
Epoch 230, training loss: 0.5533719658851624 = 0.5460752248764038 + 0.001 * 7.296720504760742
Epoch 230, val loss: 0.810672402381897
Epoch 240, training loss: 0.4927612841129303 = 0.4854872226715088 + 0.001 * 7.27406644821167
Epoch 240, val loss: 0.7772516012191772
Epoch 250, training loss: 0.4395023286342621 = 0.43225330114364624 + 0.001 * 7.249014854431152
Epoch 250, val loss: 0.7534046173095703
Epoch 260, training loss: 0.39259064197540283 = 0.3853635787963867 + 0.001 * 7.2270731925964355
Epoch 260, val loss: 0.7371450066566467
Epoch 270, training loss: 0.351150244474411 = 0.3439394235610962 + 0.001 * 7.210814952850342
Epoch 270, val loss: 0.7265441417694092
Epoch 280, training loss: 0.31429365277290344 = 0.3070879280567169 + 0.001 * 7.20572566986084
Epoch 280, val loss: 0.7200305461883545
Epoch 290, training loss: 0.2811320126056671 = 0.2739260494709015 + 0.001 * 7.205962181091309
Epoch 290, val loss: 0.7164384722709656
Epoch 300, training loss: 0.2509392499923706 = 0.24373435974121094 + 0.001 * 7.204875469207764
Epoch 300, val loss: 0.7152035236358643
Epoch 310, training loss: 0.22337891161441803 = 0.21617361903190613 + 0.001 * 7.205295085906982
Epoch 310, val loss: 0.7160378098487854
Epoch 320, training loss: 0.1985028088092804 = 0.19129636883735657 + 0.001 * 7.20643949508667
Epoch 320, val loss: 0.7189737558364868
Epoch 330, training loss: 0.17648489773273468 = 0.16927948594093323 + 0.001 * 7.205417156219482
Epoch 330, val loss: 0.7241370677947998
Epoch 340, training loss: 0.15731382369995117 = 0.150105819106102 + 0.001 * 7.207999229431152
Epoch 340, val loss: 0.7313212156295776
Epoch 350, training loss: 0.14069661498069763 = 0.1334914267063141 + 0.001 * 7.205189228057861
Epoch 350, val loss: 0.7401797771453857
Epoch 360, training loss: 0.1262439638376236 = 0.11903901398181915 + 0.001 * 7.204949378967285
Epoch 360, val loss: 0.7503613233566284
Epoch 370, training loss: 0.11357924342155457 = 0.10637027770280838 + 0.001 * 7.208964824676514
Epoch 370, val loss: 0.7615184783935547
Epoch 380, training loss: 0.10240622609853745 = 0.09520240873098373 + 0.001 * 7.2038164138793945
Epoch 380, val loss: 0.7734208703041077
Epoch 390, training loss: 0.09251569956541061 = 0.08531199395656586 + 0.001 * 7.2037034034729
Epoch 390, val loss: 0.785860538482666
Epoch 400, training loss: 0.08372244238853455 = 0.0765199139714241 + 0.001 * 7.2025275230407715
Epoch 400, val loss: 0.7987085580825806
Epoch 410, training loss: 0.07588758319616318 = 0.06868557631969452 + 0.001 * 7.202009677886963
Epoch 410, val loss: 0.8118181824684143
Epoch 420, training loss: 0.06890570372343063 = 0.061701733618974686 + 0.001 * 7.203967094421387
Epoch 420, val loss: 0.8251413702964783
Epoch 430, training loss: 0.06267910450696945 = 0.055479101836681366 + 0.001 * 7.19999885559082
Epoch 430, val loss: 0.838624119758606
Epoch 440, training loss: 0.05714063718914986 = 0.04994366690516472 + 0.001 * 7.1969685554504395
Epoch 440, val loss: 0.852152407169342
Epoch 450, training loss: 0.0522255077958107 = 0.04503042250871658 + 0.001 * 7.1950860023498535
Epoch 450, val loss: 0.8656818270683289
Epoch 460, training loss: 0.047883424907922745 = 0.040679413825273514 + 0.001 * 7.204011917114258
Epoch 460, val loss: 0.879097044467926
Epoch 470, training loss: 0.04402779042720795 = 0.03683251142501831 + 0.001 * 7.195277690887451
Epoch 470, val loss: 0.8923342227935791
Epoch 480, training loss: 0.04062328487634659 = 0.03343461453914642 + 0.001 * 7.188669204711914
Epoch 480, val loss: 0.9053462147712708
Epoch 490, training loss: 0.03762941434979439 = 0.03043399006128311 + 0.001 * 7.195425510406494
Epoch 490, val loss: 0.9180945158004761
Epoch 500, training loss: 0.034968458116054535 = 0.02778230607509613 + 0.001 * 7.186152458190918
Epoch 500, val loss: 0.9305338859558105
Epoch 510, training loss: 0.03262289986014366 = 0.02543598972260952 + 0.001 * 7.18690824508667
Epoch 510, val loss: 0.9426486492156982
Epoch 520, training loss: 0.03053334355354309 = 0.023356197401881218 + 0.001 * 7.177145481109619
Epoch 520, val loss: 0.9543936252593994
Epoch 530, training loss: 0.02867760695517063 = 0.02150871977210045 + 0.001 * 7.168886184692383
Epoch 530, val loss: 0.9657832384109497
Epoch 540, training loss: 0.027023006230592728 = 0.019863290712237358 + 0.001 * 7.159714698791504
Epoch 540, val loss: 0.9768191576004028
Epoch 550, training loss: 0.025587929412722588 = 0.018394067883491516 + 0.001 * 7.19386100769043
Epoch 550, val loss: 0.987490177154541
Epoch 560, training loss: 0.02424033358693123 = 0.017078544944524765 + 0.001 * 7.1617889404296875
Epoch 560, val loss: 0.9978227615356445
Epoch 570, training loss: 0.0230440404266119 = 0.01589750126004219 + 0.001 * 7.146539211273193
Epoch 570, val loss: 1.0078370571136475
Epoch 580, training loss: 0.021976063027977943 = 0.014833904802799225 + 0.001 * 7.142158031463623
Epoch 580, val loss: 1.0175151824951172
Epoch 590, training loss: 0.02103019692003727 = 0.01387358270585537 + 0.001 * 7.156614303588867
Epoch 590, val loss: 1.0268898010253906
Epoch 600, training loss: 0.020135916769504547 = 0.013004215434193611 + 0.001 * 7.131700038909912
Epoch 600, val loss: 1.0359679460525513
Epoch 610, training loss: 0.019359581172466278 = 0.012214954011142254 + 0.001 * 7.144626140594482
Epoch 610, val loss: 1.0447602272033691
Epoch 620, training loss: 0.018616564571857452 = 0.01149653922766447 + 0.001 * 7.12002420425415
Epoch 620, val loss: 1.0532699823379517
Epoch 630, training loss: 0.01801036112010479 = 0.010840978473424911 + 0.001 * 7.169382095336914
Epoch 630, val loss: 1.0615229606628418
Epoch 640, training loss: 0.017371686175465584 = 0.010241414420306683 + 0.001 * 7.1302714347839355
Epoch 640, val loss: 1.0695135593414307
Epoch 650, training loss: 0.01679389551281929 = 0.009691734798252583 + 0.001 * 7.102160930633545
Epoch 650, val loss: 1.077275037765503
Epoch 660, training loss: 0.01635248400270939 = 0.00918654166162014 + 0.001 * 7.1659417152404785
Epoch 660, val loss: 1.0848157405853271
Epoch 670, training loss: 0.015831943601369858 = 0.00872130785137415 + 0.001 * 7.110635757446289
Epoch 670, val loss: 1.0921276807785034
Epoch 680, training loss: 0.015417026355862617 = 0.008291984908282757 + 0.001 * 7.125041484832764
Epoch 680, val loss: 1.0992355346679688
Epoch 690, training loss: 0.015033001080155373 = 0.007895009592175484 + 0.001 * 7.137990951538086
Epoch 690, val loss: 1.1061344146728516
Epoch 700, training loss: 0.014609910547733307 = 0.007527302019298077 + 0.001 * 7.082608222961426
Epoch 700, val loss: 1.1128345727920532
Epoch 710, training loss: 0.014283050782978535 = 0.00718606635928154 + 0.001 * 7.096983909606934
Epoch 710, val loss: 1.1193478107452393
Epoch 720, training loss: 0.013941183686256409 = 0.006868823431432247 + 0.001 * 7.072360515594482
Epoch 720, val loss: 1.1256660223007202
Epoch 730, training loss: 0.013640172779560089 = 0.006573340855538845 + 0.001 * 7.066831588745117
Epoch 730, val loss: 1.1318275928497314
Epoch 740, training loss: 0.013380728662014008 = 0.006297762040048838 + 0.001 * 7.082966327667236
Epoch 740, val loss: 1.137816309928894
Epoch 750, training loss: 0.013099923729896545 = 0.0060403114184737206 + 0.001 * 7.059611797332764
Epoch 750, val loss: 1.1436513662338257
Epoch 760, training loss: 0.012891082093119621 = 0.005799456965178251 + 0.001 * 7.0916242599487305
Epoch 760, val loss: 1.149325966835022
Epoch 770, training loss: 0.01262978557497263 = 0.005573806818574667 + 0.001 * 7.055978298187256
Epoch 770, val loss: 1.1548562049865723
Epoch 780, training loss: 0.01241622306406498 = 0.005362100433558226 + 0.001 * 7.054121971130371
Epoch 780, val loss: 1.1602442264556885
Epoch 790, training loss: 0.012227287515997887 = 0.005163222085684538 + 0.001 * 7.064064979553223
Epoch 790, val loss: 1.1654900312423706
Epoch 800, training loss: 0.01203673891723156 = 0.004976170137524605 + 0.001 * 7.060567855834961
Epoch 800, val loss: 1.1706037521362305
Epoch 810, training loss: 0.011842949315905571 = 0.004800014663487673 + 0.001 * 7.042934894561768
Epoch 810, val loss: 1.1755872964859009
Epoch 820, training loss: 0.011674324981868267 = 0.004633941687643528 + 0.001 * 7.0403828620910645
Epoch 820, val loss: 1.1804448366165161
Epoch 830, training loss: 0.011537356302142143 = 0.004477157257497311 + 0.001 * 7.0601983070373535
Epoch 830, val loss: 1.185182809829712
Epoch 840, training loss: 0.011361472308635712 = 0.004329035058617592 + 0.001 * 7.032436847686768
Epoch 840, val loss: 1.189803123474121
Epoch 850, training loss: 0.011214373633265495 = 0.004188896622508764 + 0.001 * 7.025476455688477
Epoch 850, val loss: 1.1943124532699585
Epoch 860, training loss: 0.011082981713116169 = 0.004056192934513092 + 0.001 * 7.026788234710693
Epoch 860, val loss: 1.1987148523330688
Epoch 870, training loss: 0.010984399355947971 = 0.003930417355149984 + 0.001 * 7.053981781005859
Epoch 870, val loss: 1.2030073404312134
Epoch 880, training loss: 0.010834632441401482 = 0.003811117960140109 + 0.001 * 7.0235137939453125
Epoch 880, val loss: 1.207197904586792
Epoch 890, training loss: 0.010712209157645702 = 0.00369781581684947 + 0.001 * 7.014392852783203
Epoch 890, val loss: 1.2112964391708374
Epoch 900, training loss: 0.010628008283674717 = 0.0035901328083127737 + 0.001 * 7.037875175476074
Epoch 900, val loss: 1.2152905464172363
Epoch 910, training loss: 0.01050051674246788 = 0.003487711539492011 + 0.001 * 7.012804985046387
Epoch 910, val loss: 1.219197392463684
Epoch 920, training loss: 0.01042921468615532 = 0.003390222555026412 + 0.001 * 7.038991451263428
Epoch 920, val loss: 1.223017692565918
Epoch 930, training loss: 0.010301225818693638 = 0.003297339892014861 + 0.001 * 7.003885269165039
Epoch 930, val loss: 1.2267425060272217
Epoch 940, training loss: 0.010215511545538902 = 0.0032087969593703747 + 0.001 * 7.006714820861816
Epoch 940, val loss: 1.230377435684204
Epoch 950, training loss: 0.010124102234840393 = 0.003124305047094822 + 0.001 * 6.999797344207764
Epoch 950, val loss: 1.233933925628662
Epoch 960, training loss: 0.010039274580776691 = 0.0030436173547059298 + 0.001 * 6.995656490325928
Epoch 960, val loss: 1.2374120950698853
Epoch 970, training loss: 0.009961011819541454 = 0.0029665001202374697 + 0.001 * 6.994511127471924
Epoch 970, val loss: 1.2408082485198975
Epoch 980, training loss: 0.009929144755005836 = 0.0028927582316100597 + 0.001 * 7.036386013031006
Epoch 980, val loss: 1.2441372871398926
Epoch 990, training loss: 0.009815366938710213 = 0.002822212176397443 + 0.001 * 6.993154048919678
Epoch 990, val loss: 1.2473853826522827
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 1.9537709951400757 = 1.945174217224121 + 0.001 * 8.596807479858398
Epoch 0, val loss: 1.9381136894226074
Epoch 10, training loss: 1.9438022375106812 = 1.9352054595947266 + 0.001 * 8.596733093261719
Epoch 10, val loss: 1.9286072254180908
Epoch 20, training loss: 1.9317175149917603 = 1.9231209754943848 + 0.001 * 8.596497535705566
Epoch 20, val loss: 1.9167096614837646
Epoch 30, training loss: 1.9150183200836182 = 1.9064223766326904 + 0.001 * 8.59598159790039
Epoch 30, val loss: 1.8998924493789673
Epoch 40, training loss: 1.890522837638855 = 1.8819280862808228 + 0.001 * 8.594768524169922
Epoch 40, val loss: 1.875227689743042
Epoch 50, training loss: 1.8558599948883057 = 1.84726881980896 + 0.001 * 8.591195106506348
Epoch 50, val loss: 1.8417723178863525
Epoch 60, training loss: 1.8151580095291138 = 1.8065810203552246 + 0.001 * 8.576942443847656
Epoch 60, val loss: 1.8064044713974
Epoch 70, training loss: 1.7796262502670288 = 1.7711198329925537 + 0.001 * 8.50642204284668
Epoch 70, val loss: 1.7796283960342407
Epoch 80, training loss: 1.7396310567855835 = 1.731475830078125 + 0.001 * 8.155278205871582
Epoch 80, val loss: 1.7460591793060303
Epoch 90, training loss: 1.6833785772323608 = 1.675376057624817 + 0.001 * 8.002528190612793
Epoch 90, val loss: 1.6964154243469238
Epoch 100, training loss: 1.6082727909088135 = 1.6003893613815308 + 0.001 * 7.883488178253174
Epoch 100, val loss: 1.6331323385238647
Epoch 110, training loss: 1.5192457437515259 = 1.5115032196044922 + 0.001 * 7.742545127868652
Epoch 110, val loss: 1.5604873895645142
Epoch 120, training loss: 1.4276663064956665 = 1.4202156066894531 + 0.001 * 7.450727462768555
Epoch 120, val loss: 1.4866032600402832
Epoch 130, training loss: 1.3392829895019531 = 1.3319207429885864 + 0.001 * 7.362222194671631
Epoch 130, val loss: 1.4178410768508911
Epoch 140, training loss: 1.2531981468200684 = 1.2458717823028564 + 0.001 * 7.32635498046875
Epoch 140, val loss: 1.351852297782898
Epoch 150, training loss: 1.1667667627334595 = 1.159463882446289 + 0.001 * 7.30289363861084
Epoch 150, val loss: 1.2856916189193726
Epoch 160, training loss: 1.0790139436721802 = 1.0717295408248901 + 0.001 * 7.284450054168701
Epoch 160, val loss: 1.218238115310669
Epoch 170, training loss: 0.9900994896888733 = 0.9828311800956726 + 0.001 * 7.268301963806152
Epoch 170, val loss: 1.1485432386398315
Epoch 180, training loss: 0.9015341997146606 = 0.8942825794219971 + 0.001 * 7.251619815826416
Epoch 180, val loss: 1.077347755432129
Epoch 190, training loss: 0.8155902624130249 = 0.8083568215370178 + 0.001 * 7.2334442138671875
Epoch 190, val loss: 1.007070541381836
Epoch 200, training loss: 0.7344068288803101 = 0.7271900177001953 + 0.001 * 7.216794013977051
Epoch 200, val loss: 0.9406193494796753
Epoch 210, training loss: 0.6599551439285278 = 0.6527467370033264 + 0.001 * 7.208408832550049
Epoch 210, val loss: 0.8814261555671692
Epoch 220, training loss: 0.5934709310531616 = 0.5862643122673035 + 0.001 * 7.206593990325928
Epoch 220, val loss: 0.8320146799087524
Epoch 230, training loss: 0.5348727703094482 = 0.527665913105011 + 0.001 * 7.206856727600098
Epoch 230, val loss: 0.7932395935058594
Epoch 240, training loss: 0.4830065369606018 = 0.47579994797706604 + 0.001 * 7.206575393676758
Epoch 240, val loss: 0.7637040615081787
Epoch 250, training loss: 0.436385840177536 = 0.4291796088218689 + 0.001 * 7.2062249183654785
Epoch 250, val loss: 0.7411875128746033
Epoch 260, training loss: 0.3938769996166229 = 0.3866710364818573 + 0.001 * 7.205966472625732
Epoch 260, val loss: 0.7236821055412292
Epoch 270, training loss: 0.35487642884254456 = 0.34767067432403564 + 0.001 * 7.205740928649902
Epoch 270, val loss: 0.7098237872123718
Epoch 280, training loss: 0.31915560364723206 = 0.31195008754730225 + 0.001 * 7.205512046813965
Epoch 280, val loss: 0.6988789439201355
Epoch 290, training loss: 0.2865274250507355 = 0.2793222963809967 + 0.001 * 7.205116271972656
Epoch 290, val loss: 0.6904212236404419
Epoch 300, training loss: 0.2566901445388794 = 0.2494850903749466 + 0.001 * 7.205068111419678
Epoch 300, val loss: 0.6842012405395508
Epoch 310, training loss: 0.22946086525917053 = 0.2222578078508377 + 0.001 * 7.203057289123535
Epoch 310, val loss: 0.680325984954834
Epoch 320, training loss: 0.20488516986370087 = 0.19768545031547546 + 0.001 * 7.199725151062012
Epoch 320, val loss: 0.6790573000907898
Epoch 330, training loss: 0.18303638696670532 = 0.17583860456943512 + 0.001 * 7.197779655456543
Epoch 330, val loss: 0.6805022358894348
Epoch 340, training loss: 0.1638481616973877 = 0.15666218101978302 + 0.001 * 7.185983657836914
Epoch 340, val loss: 0.6845320463180542
Epoch 350, training loss: 0.14706626534461975 = 0.13988934457302094 + 0.001 * 7.1769208908081055
Epoch 350, val loss: 0.6907143592834473
Epoch 360, training loss: 0.13234244287014008 = 0.1251765787601471 + 0.001 * 7.165858745574951
Epoch 360, val loss: 0.6984944343566895
Epoch 370, training loss: 0.11934643983840942 = 0.11220096796751022 + 0.001 * 7.145470142364502
Epoch 370, val loss: 0.707318127155304
Epoch 380, training loss: 0.10783840715885162 = 0.1006973460316658 + 0.001 * 7.1410603523254395
Epoch 380, val loss: 0.7167478799819946
Epoch 390, training loss: 0.09757872670888901 = 0.09044996649026871 + 0.001 * 7.128758430480957
Epoch 390, val loss: 0.7265374064445496
Epoch 400, training loss: 0.08842508494853973 = 0.08129768073558807 + 0.001 * 7.127403259277344
Epoch 400, val loss: 0.7366793155670166
Epoch 410, training loss: 0.08023349195718765 = 0.07310774177312851 + 0.001 * 7.125751495361328
Epoch 410, val loss: 0.7470722198486328
Epoch 420, training loss: 0.07290297001600266 = 0.06577710807323456 + 0.001 * 7.125860214233398
Epoch 420, val loss: 0.7576830387115479
Epoch 430, training loss: 0.06634296476840973 = 0.059217669069767 + 0.001 * 7.125298500061035
Epoch 430, val loss: 0.7685012817382812
Epoch 440, training loss: 0.06048106774687767 = 0.05335598811507225 + 0.001 * 7.125080585479736
Epoch 440, val loss: 0.7795024514198303
Epoch 450, training loss: 0.055249787867069244 = 0.048124782741069794 + 0.001 * 7.125004291534424
Epoch 450, val loss: 0.790583610534668
Epoch 460, training loss: 0.05059276521205902 = 0.04346795380115509 + 0.001 * 7.124809265136719
Epoch 460, val loss: 0.8017353415489197
Epoch 470, training loss: 0.04645276069641113 = 0.03932787850499153 + 0.001 * 7.124883651733398
Epoch 470, val loss: 0.8127885460853577
Epoch 480, training loss: 0.04277742654085159 = 0.03565232828259468 + 0.001 * 7.125097751617432
Epoch 480, val loss: 0.8237528204917908
Epoch 490, training loss: 0.03951704874634743 = 0.03239255025982857 + 0.001 * 7.124499320983887
Epoch 490, val loss: 0.8345465660095215
Epoch 500, training loss: 0.03662711754441261 = 0.029503395780920982 + 0.001 * 7.123720169067383
Epoch 500, val loss: 0.845098614692688
Epoch 510, training loss: 0.0340658575296402 = 0.026943041011691093 + 0.001 * 7.122816562652588
Epoch 510, val loss: 0.855417788028717
Epoch 520, training loss: 0.03179477900266647 = 0.024670926854014397 + 0.001 * 7.123851776123047
Epoch 520, val loss: 0.8654675483703613
Epoch 530, training loss: 0.029775289818644524 = 0.022652091458439827 + 0.001 * 7.123197555541992
Epoch 530, val loss: 0.8752709031105042
Epoch 540, training loss: 0.027976445853710175 = 0.02085576392710209 + 0.001 * 7.120682716369629
Epoch 540, val loss: 0.8848127126693726
Epoch 550, training loss: 0.026374604552984238 = 0.0192547719925642 + 0.001 * 7.119832992553711
Epoch 550, val loss: 0.8941125869750977
Epoch 560, training loss: 0.024943113327026367 = 0.017824536189436913 + 0.001 * 7.118577003479004
Epoch 560, val loss: 0.9031215310096741
Epoch 570, training loss: 0.023666491732001305 = 0.016543978825211525 + 0.001 * 7.122512340545654
Epoch 570, val loss: 0.9118779897689819
Epoch 580, training loss: 0.022511761635541916 = 0.015394564718008041 + 0.001 * 7.117196083068848
Epoch 580, val loss: 0.9203584790229797
Epoch 590, training loss: 0.021476253867149353 = 0.01436056662350893 + 0.001 * 7.115685939788818
Epoch 590, val loss: 0.9286365509033203
Epoch 600, training loss: 0.02054397016763687 = 0.01342741958796978 + 0.001 * 7.116549968719482
Epoch 600, val loss: 0.9366397857666016
Epoch 610, training loss: 0.019697977229952812 = 0.012582918629050255 + 0.001 * 7.115058422088623
Epoch 610, val loss: 0.9444174766540527
Epoch 620, training loss: 0.018928024917840958 = 0.011815227568149567 + 0.001 * 7.112797737121582
Epoch 620, val loss: 0.9519960880279541
Epoch 630, training loss: 0.018221674486994743 = 0.011112812906503677 + 0.001 * 7.108861446380615
Epoch 630, val loss: 0.9593850374221802
Epoch 640, training loss: 0.017597584053874016 = 0.010466495528817177 + 0.001 * 7.131088733673096
Epoch 640, val loss: 0.9665592908859253
Epoch 650, training loss: 0.01698208786547184 = 0.009870020672678947 + 0.001 * 7.112066745758057
Epoch 650, val loss: 0.9735956192016602
Epoch 660, training loss: 0.01642325520515442 = 0.009318896569311619 + 0.001 * 7.104357719421387
Epoch 660, val loss: 0.9804773330688477
Epoch 670, training loss: 0.015912065282464027 = 0.008809472434222698 + 0.001 * 7.102592468261719
Epoch 670, val loss: 0.9871765375137329
Epoch 680, training loss: 0.015471354126930237 = 0.008338568732142448 + 0.001 * 7.132784843444824
Epoch 680, val loss: 0.9937360882759094
Epoch 690, training loss: 0.01500488817691803 = 0.007903323508799076 + 0.001 * 7.101563930511475
Epoch 690, val loss: 1.0001139640808105
Epoch 700, training loss: 0.014630427584052086 = 0.007500792853534222 + 0.001 * 7.129633903503418
Epoch 700, val loss: 1.0063447952270508
Epoch 710, training loss: 0.014231457374989986 = 0.00712825171649456 + 0.001 * 7.10320520401001
Epoch 710, val loss: 1.0124168395996094
Epoch 720, training loss: 0.013877080753445625 = 0.006783068645745516 + 0.001 * 7.0940117835998535
Epoch 720, val loss: 1.0183403491973877
Epoch 730, training loss: 0.013583931140601635 = 0.006462864577770233 + 0.001 * 7.121066093444824
Epoch 730, val loss: 1.0241094827651978
Epoch 740, training loss: 0.01326831802725792 = 0.0061655365861952305 + 0.001 * 7.102781772613525
Epoch 740, val loss: 1.029744029045105
Epoch 750, training loss: 0.012980598025023937 = 0.005889148451387882 + 0.001 * 7.09144926071167
Epoch 750, val loss: 1.0352321863174438
Epoch 760, training loss: 0.012735575437545776 = 0.005631874315440655 + 0.001 * 7.103700637817383
Epoch 760, val loss: 1.0405826568603516
Epoch 770, training loss: 0.01248285360634327 = 0.005392078775912523 + 0.001 * 7.090774059295654
Epoch 770, val loss: 1.0457837581634521
Epoch 780, training loss: 0.012263987213373184 = 0.005168271251022816 + 0.001 * 7.095715045928955
Epoch 780, val loss: 1.0508689880371094
Epoch 790, training loss: 0.012042557820677757 = 0.00495912553742528 + 0.001 * 7.083431720733643
Epoch 790, val loss: 1.0557994842529297
Epoch 800, training loss: 0.011858459562063217 = 0.004763443022966385 + 0.001 * 7.0950164794921875
Epoch 800, val loss: 1.0606410503387451
Epoch 810, training loss: 0.011665325611829758 = 0.004580127075314522 + 0.001 * 7.085198879241943
Epoch 810, val loss: 1.0653389692306519
Epoch 820, training loss: 0.011520566418766975 = 0.004408152773976326 + 0.001 * 7.112412929534912
Epoch 820, val loss: 1.0699384212493896
Epoch 830, training loss: 0.011323973536491394 = 0.004246632102876902 + 0.001 * 7.077341079711914
Epoch 830, val loss: 1.07443106174469
Epoch 840, training loss: 0.011164499446749687 = 0.004094734787940979 + 0.001 * 7.069763660430908
Epoch 840, val loss: 1.0788031816482544
Epoch 850, training loss: 0.011045627295970917 = 0.003951744642108679 + 0.001 * 7.0938825607299805
Epoch 850, val loss: 1.08308744430542
Epoch 860, training loss: 0.010885554365813732 = 0.0038169638719409704 + 0.001 * 7.06859016418457
Epoch 860, val loss: 1.0872472524642944
Epoch 870, training loss: 0.010769335553050041 = 0.0036897887475788593 + 0.001 * 7.079546928405762
Epoch 870, val loss: 1.0913290977478027
Epoch 880, training loss: 0.010645728558301926 = 0.0035696469713002443 + 0.001 * 7.076080799102783
Epoch 880, val loss: 1.095310091972351
Epoch 890, training loss: 0.01051003485918045 = 0.0034560461062937975 + 0.001 * 7.053987979888916
Epoch 890, val loss: 1.0991942882537842
Epoch 900, training loss: 0.010411011055111885 = 0.003348513273522258 + 0.001 * 7.062497615814209
Epoch 900, val loss: 1.1030100584030151
Epoch 910, training loss: 0.010305347852408886 = 0.003246678737923503 + 0.001 * 7.058668613433838
Epoch 910, val loss: 1.1067250967025757
Epoch 920, training loss: 0.010208986699581146 = 0.003150138072669506 + 0.001 * 7.0588483810424805
Epoch 920, val loss: 1.1103651523590088
Epoch 930, training loss: 0.010107694193720818 = 0.0030585513450205326 + 0.001 * 7.049141883850098
Epoch 930, val loss: 1.1139166355133057
Epoch 940, training loss: 0.01003587618470192 = 0.002971579320728779 + 0.001 * 7.064297199249268
Epoch 940, val loss: 1.1173932552337646
Epoch 950, training loss: 0.009933818131685257 = 0.002888874616473913 + 0.001 * 7.044943332672119
Epoch 950, val loss: 1.1207818984985352
Epoch 960, training loss: 0.00986296497285366 = 0.0028101771604269743 + 0.001 * 7.052787780761719
Epoch 960, val loss: 1.124123215675354
Epoch 970, training loss: 0.009843740612268448 = 0.002735262969508767 + 0.001 * 7.108477592468262
Epoch 970, val loss: 1.1273882389068604
Epoch 980, training loss: 0.00972314178943634 = 0.002663843333721161 + 0.001 * 7.059297561645508
Epoch 980, val loss: 1.1305612325668335
Epoch 990, training loss: 0.009649336338043213 = 0.0025957641191780567 + 0.001 * 7.053571701049805
Epoch 990, val loss: 1.1336599588394165
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 1.950865626335144 = 1.9422688484191895 + 0.001 * 8.596826553344727
Epoch 0, val loss: 1.934422254562378
Epoch 10, training loss: 1.9414050579071045 = 1.93280827999115 + 0.001 * 8.596789360046387
Epoch 10, val loss: 1.9256964921951294
Epoch 20, training loss: 1.9299381971359253 = 1.9213415384292603 + 0.001 * 8.596635818481445
Epoch 20, val loss: 1.9147415161132812
Epoch 30, training loss: 1.914015293121338 = 1.9054189920425415 + 0.001 * 8.596311569213867
Epoch 30, val loss: 1.8992691040039062
Epoch 40, training loss: 1.8905315399169922 = 1.881935954093933 + 0.001 * 8.595587730407715
Epoch 40, val loss: 1.876657485961914
Epoch 50, training loss: 1.8567684888839722 = 1.848174810409546 + 0.001 * 8.593646049499512
Epoch 50, val loss: 1.8454089164733887
Epoch 60, training loss: 1.8157753944396973 = 1.8071883916854858 + 0.001 * 8.587000846862793
Epoch 60, val loss: 1.8107925653457642
Epoch 70, training loss: 1.7785618305206299 = 1.770005464553833 + 0.001 * 8.556407928466797
Epoch 70, val loss: 1.7815531492233276
Epoch 80, training loss: 1.7370229959487915 = 1.728701114654541 + 0.001 * 8.321854591369629
Epoch 80, val loss: 1.7435802221298218
Epoch 90, training loss: 1.677568793296814 = 1.66951584815979 + 0.001 * 8.052887916564941
Epoch 90, val loss: 1.6905750036239624
Epoch 100, training loss: 1.596413016319275 = 1.5884705781936646 + 0.001 * 7.942403793334961
Epoch 100, val loss: 1.621766209602356
Epoch 110, training loss: 1.4956690073013306 = 1.4878380298614502 + 0.001 * 7.830976486206055
Epoch 110, val loss: 1.5362865924835205
Epoch 120, training loss: 1.3868056535720825 = 1.3791871070861816 + 0.001 * 7.618524551391602
Epoch 120, val loss: 1.4444156885147095
Epoch 130, training loss: 1.2785266637802124 = 1.2709919214248657 + 0.001 * 7.53471040725708
Epoch 130, val loss: 1.354722261428833
Epoch 140, training loss: 1.173194169998169 = 1.165737271308899 + 0.001 * 7.456940174102783
Epoch 140, val loss: 1.2693816423416138
Epoch 150, training loss: 1.0715570449829102 = 1.064163327217102 + 0.001 * 7.393691539764404
Epoch 150, val loss: 1.1891214847564697
Epoch 160, training loss: 0.9746233224868774 = 0.9672631621360779 + 0.001 * 7.360149383544922
Epoch 160, val loss: 1.1140109300613403
Epoch 170, training loss: 0.8832657337188721 = 0.8759228587150574 + 0.001 * 7.3428874015808105
Epoch 170, val loss: 1.0441266298294067
Epoch 180, training loss: 0.7987601161003113 = 0.7914339303970337 + 0.001 * 7.326187610626221
Epoch 180, val loss: 0.980319082736969
Epoch 190, training loss: 0.7225756049156189 = 0.7152729034423828 + 0.001 * 7.302721977233887
Epoch 190, val loss: 0.923794686794281
Epoch 200, training loss: 0.655032753944397 = 0.6477629542350769 + 0.001 * 7.269793510437012
Epoch 200, val loss: 0.8757256865501404
Epoch 210, training loss: 0.5944925546646118 = 0.5872475504875183 + 0.001 * 7.245029926300049
Epoch 210, val loss: 0.8358669281005859
Epoch 220, training loss: 0.5387383699417114 = 0.5315098166465759 + 0.001 * 7.228576183319092
Epoch 220, val loss: 0.8029295206069946
Epoch 230, training loss: 0.48631903529167175 = 0.47909754514694214 + 0.001 * 7.221480369567871
Epoch 230, val loss: 0.7757282853126526
Epoch 240, training loss: 0.4364120364189148 = 0.4291944205760956 + 0.001 * 7.217616081237793
Epoch 240, val loss: 0.7532447576522827
Epoch 250, training loss: 0.3885757327079773 = 0.3813604712486267 + 0.001 * 7.2152581214904785
Epoch 250, val loss: 0.7343262434005737
Epoch 260, training loss: 0.3427809178829193 = 0.3355676233768463 + 0.001 * 7.213284492492676
Epoch 260, val loss: 0.7182984948158264
Epoch 270, training loss: 0.29949459433555603 = 0.29227980971336365 + 0.001 * 7.214794635772705
Epoch 270, val loss: 0.7048784494400024
Epoch 280, training loss: 0.2594773471355438 = 0.2522667944431305 + 0.001 * 7.210555076599121
Epoch 280, val loss: 0.6943017244338989
Epoch 290, training loss: 0.22353728115558624 = 0.21632976830005646 + 0.001 * 7.207507133483887
Epoch 290, val loss: 0.687133252620697
Epoch 300, training loss: 0.1921469271183014 = 0.18494200706481934 + 0.001 * 7.204912185668945
Epoch 300, val loss: 0.6837613582611084
Epoch 310, training loss: 0.16533981263637543 = 0.15813398361206055 + 0.001 * 7.205826759338379
Epoch 310, val loss: 0.68429034948349
Epoch 320, training loss: 0.1427544206380844 = 0.13555486500263214 + 0.001 * 7.199555397033691
Epoch 320, val loss: 0.688345193862915
Epoch 330, training loss: 0.12385988980531693 = 0.11666326224803925 + 0.001 * 7.196623802185059
Epoch 330, val loss: 0.6953975558280945
Epoch 340, training loss: 0.10808844864368439 = 0.10087838768959045 + 0.001 * 7.210058212280273
Epoch 340, val loss: 0.7048318386077881
Epoch 350, training loss: 0.09485450387001038 = 0.08766201138496399 + 0.001 * 7.1924943923950195
Epoch 350, val loss: 0.7160128355026245
Epoch 360, training loss: 0.0837453082203865 = 0.07655929774045944 + 0.001 * 7.1860127449035645
Epoch 360, val loss: 0.7284044623374939
Epoch 370, training loss: 0.074388287961483 = 0.06719984114170074 + 0.001 * 7.1884446144104
Epoch 370, val loss: 0.74162757396698
Epoch 380, training loss: 0.0664639100432396 = 0.05928567051887512 + 0.001 * 7.178242206573486
Epoch 380, val loss: 0.7552871108055115
Epoch 390, training loss: 0.05973993241786957 = 0.052570924162864685 + 0.001 * 7.169005870819092
Epoch 390, val loss: 0.7691558599472046
Epoch 400, training loss: 0.05402526259422302 = 0.04685305431485176 + 0.001 * 7.172207832336426
Epoch 400, val loss: 0.7830625772476196
Epoch 410, training loss: 0.04913618043065071 = 0.04196525365114212 + 0.001 * 7.170925617218018
Epoch 410, val loss: 0.7968449592590332
Epoch 420, training loss: 0.04491741955280304 = 0.03776877373456955 + 0.001 * 7.148643493652344
Epoch 420, val loss: 0.8104392886161804
Epoch 430, training loss: 0.041307441890239716 = 0.03414921835064888 + 0.001 * 7.1582231521606445
Epoch 430, val loss: 0.8237698078155518
Epoch 440, training loss: 0.038164980709552765 = 0.031012022867798805 + 0.001 * 7.15295934677124
Epoch 440, val loss: 0.8367845416069031
Epoch 450, training loss: 0.03542691096663475 = 0.028279347345232964 + 0.001 * 7.147563934326172
Epoch 450, val loss: 0.8494634032249451
Epoch 460, training loss: 0.033017147332429886 = 0.025887666270136833 + 0.001 * 7.129480361938477
Epoch 460, val loss: 0.8617873787879944
Epoch 470, training loss: 0.030914893373847008 = 0.023784704506397247 + 0.001 * 7.130188465118408
Epoch 470, val loss: 0.8737722635269165
Epoch 480, training loss: 0.02905460074543953 = 0.0219273678958416 + 0.001 * 7.127231597900391
Epoch 480, val loss: 0.8854030966758728
Epoch 490, training loss: 0.02739419788122177 = 0.020279893651604652 + 0.001 * 7.114304542541504
Epoch 490, val loss: 0.8966948986053467
Epoch 500, training loss: 0.025921883061528206 = 0.018812725320458412 + 0.001 * 7.109157562255859
Epoch 500, val loss: 0.9076590538024902
Epoch 510, training loss: 0.02462320402264595 = 0.017501046881079674 + 0.001 * 7.122156143188477
Epoch 510, val loss: 0.9182779788970947
Epoch 520, training loss: 0.02343037910759449 = 0.016324056312441826 + 0.001 * 7.106322765350342
Epoch 520, val loss: 0.9285884499549866
Epoch 530, training loss: 0.02237323671579361 = 0.015264290384948254 + 0.001 * 7.108946323394775
Epoch 530, val loss: 0.9385830760002136
Epoch 540, training loss: 0.02140791341662407 = 0.014306887984275818 + 0.001 * 7.101025581359863
Epoch 540, val loss: 0.9482871890068054
Epoch 550, training loss: 0.020542224869132042 = 0.013439318165183067 + 0.001 * 7.102906227111816
Epoch 550, val loss: 0.9576990008354187
Epoch 560, training loss: 0.01975250616669655 = 0.012650764547288418 + 0.001 * 7.101741313934326
Epoch 560, val loss: 0.9668365716934204
Epoch 570, training loss: 0.019028374925255775 = 0.011932017281651497 + 0.001 * 7.096357345581055
Epoch 570, val loss: 0.975718080997467
Epoch 580, training loss: 0.018365198746323586 = 0.011275227181613445 + 0.001 * 7.089970588684082
Epoch 580, val loss: 0.984347403049469
Epoch 590, training loss: 0.01776031404733658 = 0.01067348849028349 + 0.001 * 7.086824417114258
Epoch 590, val loss: 0.992741584777832
Epoch 600, training loss: 0.01722656935453415 = 0.010120847262442112 + 0.001 * 7.1057209968566895
Epoch 600, val loss: 1.0008964538574219
Epoch 610, training loss: 0.016710201278328896 = 0.009612194262444973 + 0.001 * 7.098006725311279
Epoch 610, val loss: 1.00882887840271
Epoch 620, training loss: 0.016224797815084457 = 0.009142974391579628 + 0.001 * 7.081823348999023
Epoch 620, val loss: 1.0165421962738037
Epoch 630, training loss: 0.015790002420544624 = 0.008709223009645939 + 0.001 * 7.080779075622559
Epoch 630, val loss: 1.0240452289581299
Epoch 640, training loss: 0.01538514718413353 = 0.008307474665343761 + 0.001 * 7.077672004699707
Epoch 640, val loss: 1.0313529968261719
Epoch 650, training loss: 0.015010182745754719 = 0.007934617809951305 + 0.001 * 7.075564384460449
Epoch 650, val loss: 1.0384678840637207
Epoch 660, training loss: 0.014662215486168861 = 0.007587992586195469 + 0.001 * 7.074223041534424
Epoch 660, val loss: 1.045403003692627
Epoch 670, training loss: 0.014351388439536095 = 0.0072652124799788 + 0.001 * 7.086175441741943
Epoch 670, val loss: 1.0521519184112549
Epoch 680, training loss: 0.01404130831360817 = 0.006964018568396568 + 0.001 * 7.077289581298828
Epoch 680, val loss: 1.058743953704834
Epoch 690, training loss: 0.013761788606643677 = 0.006682356353849173 + 0.001 * 7.079431533813477
Epoch 690, val loss: 1.06517493724823
Epoch 700, training loss: 0.01349154207855463 = 0.006418420467525721 + 0.001 * 7.073121070861816
Epoch 700, val loss: 1.0714489221572876
Epoch 710, training loss: 0.013248054310679436 = 0.006170769687741995 + 0.001 * 7.07728385925293
Epoch 710, val loss: 1.0775880813598633
Epoch 720, training loss: 0.013001618906855583 = 0.005938022397458553 + 0.001 * 7.063596248626709
Epoch 720, val loss: 1.0835765600204468
Epoch 730, training loss: 0.012782401405274868 = 0.005718814674764872 + 0.001 * 7.063586235046387
Epoch 730, val loss: 1.0894453525543213
Epoch 740, training loss: 0.012563565745949745 = 0.005511978175491095 + 0.001 * 7.0515875816345215
Epoch 740, val loss: 1.0951894521713257
Epoch 750, training loss: 0.012365059927105904 = 0.005316446069628 + 0.001 * 7.048613548278809
Epoch 750, val loss: 1.1008237600326538
Epoch 760, training loss: 0.012180522084236145 = 0.005131350830197334 + 0.001 * 7.04917049407959
Epoch 760, val loss: 1.1063563823699951
Epoch 770, training loss: 0.012025099247694016 = 0.004955939017236233 + 0.001 * 7.069159984588623
Epoch 770, val loss: 1.1117838621139526
Epoch 780, training loss: 0.011856047436594963 = 0.0047895158641040325 + 0.001 * 7.066530704498291
Epoch 780, val loss: 1.1171139478683472
Epoch 790, training loss: 0.011680894531309605 = 0.004631441552191973 + 0.001 * 7.049452781677246
Epoch 790, val loss: 1.1223418712615967
Epoch 800, training loss: 0.011518977582454681 = 0.004481112584471703 + 0.001 * 7.037865161895752
Epoch 800, val loss: 1.1274890899658203
Epoch 810, training loss: 0.011387035250663757 = 0.004338029772043228 + 0.001 * 7.049005508422852
Epoch 810, val loss: 1.132559061050415
Epoch 820, training loss: 0.01124708354473114 = 0.004201753996312618 + 0.001 * 7.045328617095947
Epoch 820, val loss: 1.1375281810760498
Epoch 830, training loss: 0.011117691174149513 = 0.004071868024766445 + 0.001 * 7.045822620391846
Epoch 830, val loss: 1.1424331665039062
Epoch 840, training loss: 0.010984272696077824 = 0.003948058933019638 + 0.001 * 7.036213397979736
Epoch 840, val loss: 1.1472455263137817
Epoch 850, training loss: 0.010869964025914669 = 0.0038301367312669754 + 0.001 * 7.0398268699646
Epoch 850, val loss: 1.1519752740859985
Epoch 860, training loss: 0.010759116150438786 = 0.003717656945809722 + 0.001 * 7.041459083557129
Epoch 860, val loss: 1.156643271446228
Epoch 870, training loss: 0.010637657716870308 = 0.003610320156440139 + 0.001 * 7.027337074279785
Epoch 870, val loss: 1.1612132787704468
Epoch 880, training loss: 0.010530415922403336 = 0.0035078562796115875 + 0.001 * 7.022559642791748
Epoch 880, val loss: 1.1657226085662842
Epoch 890, training loss: 0.010455694980919361 = 0.0034099717158824205 + 0.001 * 7.045722484588623
Epoch 890, val loss: 1.1701395511627197
Epoch 900, training loss: 0.01036839373409748 = 0.003316507674753666 + 0.001 * 7.051886081695557
Epoch 900, val loss: 1.1745002269744873
Epoch 910, training loss: 0.010264699347317219 = 0.0032271642703562975 + 0.001 * 7.037534713745117
Epoch 910, val loss: 1.178760290145874
Epoch 920, training loss: 0.010165581479668617 = 0.0031416972633451223 + 0.001 * 7.023883819580078
Epoch 920, val loss: 1.1829770803451538
Epoch 930, training loss: 0.010097995400428772 = 0.0030598784796893597 + 0.001 * 7.038115978240967
Epoch 930, val loss: 1.1871098279953003
Epoch 940, training loss: 0.010022650472819805 = 0.0029815398156642914 + 0.001 * 7.041110515594482
Epoch 940, val loss: 1.1911590099334717
Epoch 950, training loss: 0.009939472191035748 = 0.002906493144109845 + 0.001 * 7.032978534698486
Epoch 950, val loss: 1.1951709985733032
Epoch 960, training loss: 0.009893289767205715 = 0.002834542188793421 + 0.001 * 7.058747291564941
Epoch 960, val loss: 1.1991063356399536
Epoch 970, training loss: 0.009810851886868477 = 0.0027656361926347017 + 0.001 * 7.045215129852295
Epoch 970, val loss: 1.2029534578323364
Epoch 980, training loss: 0.00971955806016922 = 0.0026995805092155933 + 0.001 * 7.019977569580078
Epoch 980, val loss: 1.2067664861679077
Epoch 990, training loss: 0.009638040326535702 = 0.002636138116940856 + 0.001 * 7.001901626586914
Epoch 990, val loss: 1.210482120513916
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8434370057986295
The final CL Acc:0.81111, 0.01210, The final GNN Acc:0.83799, 0.00386
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10578])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.942183256149292 = 1.9335864782333374 + 0.001 * 8.596810340881348
Epoch 0, val loss: 1.9277164936065674
Epoch 10, training loss: 1.9323055744171143 = 1.9237087965011597 + 0.001 * 8.596759796142578
Epoch 10, val loss: 1.9173558950424194
Epoch 20, training loss: 1.9203182458877563 = 1.9117217063903809 + 0.001 * 8.596577644348145
Epoch 20, val loss: 1.904555082321167
Epoch 30, training loss: 1.9038835763931274 = 1.8952873945236206 + 0.001 * 8.596182823181152
Epoch 30, val loss: 1.8871129751205444
Epoch 40, training loss: 1.8803484439849854 = 1.8717530965805054 + 0.001 * 8.595307350158691
Epoch 40, val loss: 1.862716794013977
Epoch 50, training loss: 1.848991870880127 = 1.8403987884521484 + 0.001 * 8.593035697937012
Epoch 50, val loss: 1.8321268558502197
Epoch 60, training loss: 1.8162782192230225 = 1.8076928853988647 + 0.001 * 8.585294723510742
Epoch 60, val loss: 1.8044543266296387
Epoch 70, training loss: 1.7870750427246094 = 1.7785261869430542 + 0.001 * 8.548851013183594
Epoch 70, val loss: 1.7823251485824585
Epoch 80, training loss: 1.7483245134353638 = 1.7400436401367188 + 0.001 * 8.280850410461426
Epoch 80, val loss: 1.7499229907989502
Epoch 90, training loss: 1.6937545537948608 = 1.6856744289398193 + 0.001 * 8.080126762390137
Epoch 90, val loss: 1.7022327184677124
Epoch 100, training loss: 1.6195789575576782 = 1.6115782260894775 + 0.001 * 8.000714302062988
Epoch 100, val loss: 1.637403964996338
Epoch 110, training loss: 1.5330076217651367 = 1.525101900100708 + 0.001 * 7.905664920806885
Epoch 110, val loss: 1.563233733177185
Epoch 120, training loss: 1.4439665079116821 = 1.4362484216690063 + 0.001 * 7.718143463134766
Epoch 120, val loss: 1.4893382787704468
Epoch 130, training loss: 1.3557089567184448 = 1.3481898307800293 + 0.001 * 7.519078254699707
Epoch 130, val loss: 1.4199503660202026
Epoch 140, training loss: 1.2667651176452637 = 1.2593228816986084 + 0.001 * 7.442272186279297
Epoch 140, val loss: 1.353910207748413
Epoch 150, training loss: 1.176713466644287 = 1.1693062782287598 + 0.001 * 7.407167911529541
Epoch 150, val loss: 1.290234923362732
Epoch 160, training loss: 1.085907220840454 = 1.078519582748413 + 0.001 * 7.38762092590332
Epoch 160, val loss: 1.228771686553955
Epoch 170, training loss: 0.9947565793991089 = 0.9873807430267334 + 0.001 * 7.375842094421387
Epoch 170, val loss: 1.1682771444320679
Epoch 180, training loss: 0.9047158360481262 = 0.8973450064659119 + 0.001 * 7.37083101272583
Epoch 180, val loss: 1.1091468334197998
Epoch 190, training loss: 0.8185317516326904 = 0.8111642003059387 + 0.001 * 7.367547035217285
Epoch 190, val loss: 1.0526078939437866
Epoch 200, training loss: 0.7383529543876648 = 0.7309894561767578 + 0.001 * 7.363526821136475
Epoch 200, val loss: 1.0016783475875854
Epoch 210, training loss: 0.6647704243659973 = 0.6574117541313171 + 0.001 * 7.35867977142334
Epoch 210, val loss: 0.9570881128311157
Epoch 220, training loss: 0.5971302390098572 = 0.5897780060768127 + 0.001 * 7.352243423461914
Epoch 220, val loss: 0.91937255859375
Epoch 230, training loss: 0.5345092415809631 = 0.5271664261817932 + 0.001 * 7.3428144454956055
Epoch 230, val loss: 0.8878447413444519
Epoch 240, training loss: 0.4759472906589508 = 0.46861952543258667 + 0.001 * 7.3277668952941895
Epoch 240, val loss: 0.8617380857467651
Epoch 250, training loss: 0.420576810836792 = 0.41326904296875 + 0.001 * 7.307773590087891
Epoch 250, val loss: 0.8396492600440979
Epoch 260, training loss: 0.3678763508796692 = 0.3605954647064209 + 0.001 * 7.2808966636657715
Epoch 260, val loss: 0.8208242058753967
Epoch 270, training loss: 0.3180156350135803 = 0.31075480580329895 + 0.001 * 7.260831356048584
Epoch 270, val loss: 0.8048456311225891
Epoch 280, training loss: 0.2719213366508484 = 0.2646808922290802 + 0.001 * 7.240440368652344
Epoch 280, val loss: 0.792353093624115
Epoch 290, training loss: 0.23082244396209717 = 0.22359071671962738 + 0.001 * 7.231723785400391
Epoch 290, val loss: 0.7842140793800354
Epoch 300, training loss: 0.19543586671352386 = 0.18820635974407196 + 0.001 * 7.229501247406006
Epoch 300, val loss: 0.7813792824745178
Epoch 310, training loss: 0.16570532321929932 = 0.15847919881343842 + 0.001 * 7.226125240325928
Epoch 310, val loss: 0.7837744951248169
Epoch 320, training loss: 0.141078919172287 = 0.13385222852230072 + 0.001 * 7.226688385009766
Epoch 320, val loss: 0.7909066081047058
Epoch 330, training loss: 0.12078934907913208 = 0.11356382071971893 + 0.001 * 7.225526809692383
Epoch 330, val loss: 0.8018603324890137
Epoch 340, training loss: 0.1040993332862854 = 0.09687403589487076 + 0.001 * 7.225297927856445
Epoch 340, val loss: 0.8156710863113403
Epoch 350, training loss: 0.09034693986177444 = 0.08312710374593735 + 0.001 * 7.219836235046387
Epoch 350, val loss: 0.8314202427864075
Epoch 360, training loss: 0.07899278402328491 = 0.07177489250898361 + 0.001 * 7.217891693115234
Epoch 360, val loss: 0.8483353853225708
Epoch 370, training loss: 0.06958089768886566 = 0.06236553192138672 + 0.001 * 7.215365886688232
Epoch 370, val loss: 0.8658012747764587
Epoch 380, training loss: 0.06175114959478378 = 0.054533373564481735 + 0.001 * 7.217775344848633
Epoch 380, val loss: 0.8834612369537354
Epoch 390, training loss: 0.055195216089487076 = 0.04798387363553047 + 0.001 * 7.211342811584473
Epoch 390, val loss: 0.9010025262832642
Epoch 400, training loss: 0.049685992300510406 = 0.042478758841753006 + 0.001 * 7.207231521606445
Epoch 400, val loss: 0.9182652235031128
Epoch 410, training loss: 0.0450354740023613 = 0.037826038897037506 + 0.001 * 7.209435939788818
Epoch 410, val loss: 0.9351391792297363
Epoch 420, training loss: 0.04106893017888069 = 0.033871639519929886 + 0.001 * 7.19728946685791
Epoch 420, val loss: 0.9515592455863953
Epoch 430, training loss: 0.03768611699342728 = 0.030491463840007782 + 0.001 * 7.1946516036987305
Epoch 430, val loss: 0.9674557447433472
Epoch 440, training loss: 0.03479570150375366 = 0.027585497125983238 + 0.001 * 7.210206031799316
Epoch 440, val loss: 0.9828404188156128
Epoch 450, training loss: 0.032254207879304886 = 0.025073179975152016 + 0.001 * 7.181028842926025
Epoch 450, val loss: 0.9977197647094727
Epoch 460, training loss: 0.030064860358834267 = 0.022889278829097748 + 0.001 * 7.175581455230713
Epoch 460, val loss: 1.012080430984497
Epoch 470, training loss: 0.028154347091913223 = 0.020980387926101685 + 0.001 * 7.173957824707031
Epoch 470, val loss: 1.0259490013122559
Epoch 480, training loss: 0.02647279016673565 = 0.019303850829601288 + 0.001 * 7.168939590454102
Epoch 480, val loss: 1.0393222570419312
Epoch 490, training loss: 0.024984210729599 = 0.017824294045567513 + 0.001 * 7.159916877746582
Epoch 490, val loss: 1.0522162914276123
Epoch 500, training loss: 0.02368445135653019 = 0.01651262491941452 + 0.001 * 7.171826362609863
Epoch 500, val loss: 1.0646458864212036
Epoch 510, training loss: 0.022507859393954277 = 0.015344739891588688 + 0.001 * 7.163119792938232
Epoch 510, val loss: 1.0766668319702148
Epoch 520, training loss: 0.02145349606871605 = 0.014300608076155186 + 0.001 * 7.152887344360352
Epoch 520, val loss: 1.0882611274719238
Epoch 530, training loss: 0.020520223304629326 = 0.013363519683480263 + 0.001 * 7.156702995300293
Epoch 530, val loss: 1.099473237991333
Epoch 540, training loss: 0.019671842455863953 = 0.012519344687461853 + 0.001 * 7.1524977684021
Epoch 540, val loss: 1.1103039979934692
Epoch 550, training loss: 0.018898170441389084 = 0.01175602525472641 + 0.001 * 7.142145156860352
Epoch 550, val loss: 1.120786428451538
Epoch 560, training loss: 0.01819727011024952 = 0.011063243262469769 + 0.001 * 7.134027004241943
Epoch 560, val loss: 1.1309345960617065
Epoch 570, training loss: 0.01756594330072403 = 0.010432004928588867 + 0.001 * 7.133938789367676
Epoch 570, val loss: 1.140767216682434
Epoch 580, training loss: 0.0169887226074934 = 0.009854506701231003 + 0.001 * 7.134215831756592
Epoch 580, val loss: 1.1503151655197144
Epoch 590, training loss: 0.016469525173306465 = 0.009324045851826668 + 0.001 * 7.14547872543335
Epoch 590, val loss: 1.1595655679702759
Epoch 600, training loss: 0.015959572046995163 = 0.008835249580442905 + 0.001 * 7.124321937561035
Epoch 600, val loss: 1.1685959100723267
Epoch 610, training loss: 0.015515953302383423 = 0.00838350597769022 + 0.001 * 7.132447242736816
Epoch 610, val loss: 1.1774061918258667
Epoch 620, training loss: 0.015101946890354156 = 0.007965176366269588 + 0.001 * 7.136769771575928
Epoch 620, val loss: 1.1859811544418335
Epoch 630, training loss: 0.014699790626764297 = 0.007577198091894388 + 0.001 * 7.122592449188232
Epoch 630, val loss: 1.1943434476852417
Epoch 640, training loss: 0.014334326609969139 = 0.007216929458081722 + 0.001 * 7.117396354675293
Epoch 640, val loss: 1.202510952949524
Epoch 650, training loss: 0.01399844791740179 = 0.006881951820105314 + 0.001 * 7.116495609283447
Epoch 650, val loss: 1.2104514837265015
Epoch 660, training loss: 0.013685422018170357 = 0.006570090539753437 + 0.001 * 7.115331649780273
Epoch 660, val loss: 1.2182186841964722
Epoch 670, training loss: 0.013410460203886032 = 0.006279399152845144 + 0.001 * 7.131060600280762
Epoch 670, val loss: 1.2258297204971313
Epoch 680, training loss: 0.01312236674129963 = 0.006008286960422993 + 0.001 * 7.114079475402832
Epoch 680, val loss: 1.2331998348236084
Epoch 690, training loss: 0.012868393212556839 = 0.005755309015512466 + 0.001 * 7.113083362579346
Epoch 690, val loss: 1.2404215335845947
Epoch 700, training loss: 0.012628501281142235 = 0.005518909078091383 + 0.001 * 7.109592437744141
Epoch 700, val loss: 1.2474812269210815
Epoch 710, training loss: 0.012424835935235023 = 0.005297721363604069 + 0.001 * 7.127114772796631
Epoch 710, val loss: 1.2543914318084717
Epoch 720, training loss: 0.012208767235279083 = 0.005090455524623394 + 0.001 * 7.118310928344727
Epoch 720, val loss: 1.2611048221588135
Epoch 730, training loss: 0.012012913823127747 = 0.0048960307613015175 + 0.001 * 7.116882801055908
Epoch 730, val loss: 1.2677099704742432
Epoch 740, training loss: 0.011821649968624115 = 0.00471344543620944 + 0.001 * 7.1082048416137695
Epoch 740, val loss: 1.274121880531311
Epoch 750, training loss: 0.011642048135399818 = 0.0045417919754981995 + 0.001 * 7.100255489349365
Epoch 750, val loss: 1.2804012298583984
Epoch 760, training loss: 0.011481866240501404 = 0.004380254540592432 + 0.001 * 7.101611614227295
Epoch 760, val loss: 1.2865475416183472
Epoch 770, training loss: 0.011333281174302101 = 0.004228149075061083 + 0.001 * 7.10513162612915
Epoch 770, val loss: 1.2925643920898438
Epoch 780, training loss: 0.011187099851667881 = 0.004084690939635038 + 0.001 * 7.102408409118652
Epoch 780, val loss: 1.2984583377838135
Epoch 790, training loss: 0.011046171188354492 = 0.003949200268834829 + 0.001 * 7.096971035003662
Epoch 790, val loss: 1.3041683435440063
Epoch 800, training loss: 0.010914577171206474 = 0.003821166232228279 + 0.001 * 7.093410015106201
Epoch 800, val loss: 1.3097989559173584
Epoch 810, training loss: 0.010787233710289001 = 0.003700032364577055 + 0.001 * 7.0872015953063965
Epoch 810, val loss: 1.315271019935608
Epoch 820, training loss: 0.010684216395020485 = 0.0035852938890457153 + 0.001 * 7.0989227294921875
Epoch 820, val loss: 1.3206559419631958
Epoch 830, training loss: 0.010562100447714329 = 0.0034765389282256365 + 0.001 * 7.0855607986450195
Epoch 830, val loss: 1.3259004354476929
Epoch 840, training loss: 0.010469820350408554 = 0.0033733490854501724 + 0.001 * 7.096470355987549
Epoch 840, val loss: 1.3310869932174683
Epoch 850, training loss: 0.010359130799770355 = 0.0032753567211329937 + 0.001 * 7.083773136138916
Epoch 850, val loss: 1.336152195930481
Epoch 860, training loss: 0.010274762287735939 = 0.0031822030432522297 + 0.001 * 7.09255838394165
Epoch 860, val loss: 1.3411120176315308
Epoch 870, training loss: 0.010183747857809067 = 0.0030936324037611485 + 0.001 * 7.090114593505859
Epoch 870, val loss: 1.3459607362747192
Epoch 880, training loss: 0.010094364173710346 = 0.003009309759363532 + 0.001 * 7.08505392074585
Epoch 880, val loss: 1.3506883382797241
Epoch 890, training loss: 0.010004955343902111 = 0.0029289850499480963 + 0.001 * 7.075969696044922
Epoch 890, val loss: 1.3553696870803833
Epoch 900, training loss: 0.009926272556185722 = 0.002852394711226225 + 0.001 * 7.073876857757568
Epoch 900, val loss: 1.359918475151062
Epoch 910, training loss: 0.009852378629148006 = 0.0027793091721832752 + 0.001 * 7.073069095611572
Epoch 910, val loss: 1.3644108772277832
Epoch 920, training loss: 0.009777780622243881 = 0.0027095437981188297 + 0.001 * 7.068236827850342
Epoch 920, val loss: 1.3688011169433594
Epoch 930, training loss: 0.009711053222417831 = 0.0026429255958646536 + 0.001 * 7.068127155303955
Epoch 930, val loss: 1.3731118440628052
Epoch 940, training loss: 0.009657159447669983 = 0.0025792373344302177 + 0.001 * 7.077921390533447
Epoch 940, val loss: 1.377379298210144
Epoch 950, training loss: 0.009603498503565788 = 0.0025183213874697685 + 0.001 * 7.085176944732666
Epoch 950, val loss: 1.381531834602356
Epoch 960, training loss: 0.009537356905639172 = 0.002460031071677804 + 0.001 * 7.077325820922852
Epoch 960, val loss: 1.3856297731399536
Epoch 970, training loss: 0.009474717080593109 = 0.002404238563030958 + 0.001 * 7.070478439331055
Epoch 970, val loss: 1.3896280527114868
Epoch 980, training loss: 0.009414497762918472 = 0.002350769005715847 + 0.001 * 7.0637288093566895
Epoch 980, val loss: 1.3935933113098145
Epoch 990, training loss: 0.009355789981782436 = 0.0022994764149188995 + 0.001 * 7.0563130378723145
Epoch 990, val loss: 1.3974511623382568
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 1.9561527967453003 = 1.9475560188293457 + 0.001 * 8.596795082092285
Epoch 0, val loss: 1.947799801826477
Epoch 10, training loss: 1.9468185901641846 = 1.93822181224823 + 0.001 * 8.596753120422363
Epoch 10, val loss: 1.939076542854309
Epoch 20, training loss: 1.9356735944747925 = 1.927077054977417 + 0.001 * 8.596556663513184
Epoch 20, val loss: 1.9282113313674927
Epoch 30, training loss: 1.9203983545303345 = 1.9118022918701172 + 0.001 * 8.596102714538574
Epoch 30, val loss: 1.9131051301956177
Epoch 40, training loss: 1.8981497287750244 = 1.889554738998413 + 0.001 * 8.594964981079102
Epoch 40, val loss: 1.8911795616149902
Epoch 50, training loss: 1.866358995437622 = 1.8577673435211182 + 0.001 * 8.591681480407715
Epoch 50, val loss: 1.8606573343276978
Epoch 60, training loss: 1.827515959739685 = 1.8189359903335571 + 0.001 * 8.579981803894043
Epoch 60, val loss: 1.825965166091919
Epoch 70, training loss: 1.7933083772659302 = 1.7847826480865479 + 0.001 * 8.525678634643555
Epoch 70, val loss: 1.7982566356658936
Epoch 80, training loss: 1.7601063251495361 = 1.7518783807754517 + 0.001 * 8.227930068969727
Epoch 80, val loss: 1.7676043510437012
Epoch 90, training loss: 1.7142904996871948 = 1.706300139427185 + 0.001 * 7.990323543548584
Epoch 90, val loss: 1.7243351936340332
Epoch 100, training loss: 1.6519612073898315 = 1.6442303657531738 + 0.001 * 7.730826377868652
Epoch 100, val loss: 1.668739676475525
Epoch 110, training loss: 1.570145606994629 = 1.5625085830688477 + 0.001 * 7.637032508850098
Epoch 110, val loss: 1.5976650714874268
Epoch 120, training loss: 1.4717724323272705 = 1.4642244577407837 + 0.001 * 7.548008441925049
Epoch 120, val loss: 1.5130292177200317
Epoch 130, training loss: 1.365949034690857 = 1.3584991693496704 + 0.001 * 7.449838638305664
Epoch 130, val loss: 1.4254865646362305
Epoch 140, training loss: 1.2598696947097778 = 1.2524971961975098 + 0.001 * 7.372469902038574
Epoch 140, val loss: 1.3404512405395508
Epoch 150, training loss: 1.1572738885879517 = 1.149945616722107 + 0.001 * 7.32828950881958
Epoch 150, val loss: 1.2618945837020874
Epoch 160, training loss: 1.0591696500778198 = 1.0518730878829956 + 0.001 * 7.296607971191406
Epoch 160, val loss: 1.1892253160476685
Epoch 170, training loss: 0.9661086201667786 = 0.9588388204574585 + 0.001 * 7.269811153411865
Epoch 170, val loss: 1.1233049631118774
Epoch 180, training loss: 0.8796343207359314 = 0.8723875880241394 + 0.001 * 7.246727466583252
Epoch 180, val loss: 1.0652892589569092
Epoch 190, training loss: 0.8015360236167908 = 0.7943026423454285 + 0.001 * 7.233386039733887
Epoch 190, val loss: 1.016411542892456
Epoch 200, training loss: 0.7322443127632141 = 0.7250157594680786 + 0.001 * 7.228582382202148
Epoch 200, val loss: 0.9772905111312866
Epoch 210, training loss: 0.6701229214668274 = 0.6628987789154053 + 0.001 * 7.224142074584961
Epoch 210, val loss: 0.9471489191055298
Epoch 220, training loss: 0.6124029755592346 = 0.6051825881004333 + 0.001 * 7.220398426055908
Epoch 220, val loss: 0.9240137934684753
Epoch 230, training loss: 0.5567902326583862 = 0.5495740175247192 + 0.001 * 7.21623420715332
Epoch 230, val loss: 0.9063804149627686
Epoch 240, training loss: 0.5027172565460205 = 0.495504766702652 + 0.001 * 7.212494373321533
Epoch 240, val loss: 0.8930575847625732
Epoch 250, training loss: 0.4512852132320404 = 0.44408026337623596 + 0.001 * 7.204957008361816
Epoch 250, val loss: 0.8841241598129272
Epoch 260, training loss: 0.40420249104499817 = 0.39700448513031006 + 0.001 * 7.197995185852051
Epoch 260, val loss: 0.879715621471405
Epoch 270, training loss: 0.36244988441467285 = 0.3552608788013458 + 0.001 * 7.189015865325928
Epoch 270, val loss: 0.8795108795166016
Epoch 280, training loss: 0.3257487714290619 = 0.31856849789619446 + 0.001 * 7.180277347564697
Epoch 280, val loss: 0.8824126720428467
Epoch 290, training loss: 0.2930600941181183 = 0.285887211561203 + 0.001 * 7.172895431518555
Epoch 290, val loss: 0.8873560428619385
Epoch 300, training loss: 0.26323285698890686 = 0.2560591399669647 + 0.001 * 7.1737260818481445
Epoch 300, val loss: 0.8934018015861511
Epoch 310, training loss: 0.23533415794372559 = 0.22817279398441315 + 0.001 * 7.161359786987305
Epoch 310, val loss: 0.8999165296554565
Epoch 320, training loss: 0.20892465114593506 = 0.20176945626735687 + 0.001 * 7.155200958251953
Epoch 320, val loss: 0.9068182110786438
Epoch 330, training loss: 0.18406736850738525 = 0.17691637575626373 + 0.001 * 7.150994777679443
Epoch 330, val loss: 0.914448082447052
Epoch 340, training loss: 0.16116458177566528 = 0.15401321649551392 + 0.001 * 7.151371479034424
Epoch 340, val loss: 0.9232309460639954
Epoch 350, training loss: 0.1406223177909851 = 0.1334705948829651 + 0.001 * 7.151724815368652
Epoch 350, val loss: 0.9333316087722778
Epoch 360, training loss: 0.12260528653860092 = 0.11545579135417938 + 0.001 * 7.149496078491211
Epoch 360, val loss: 0.9447240233421326
Epoch 370, training loss: 0.10704083740711212 = 0.09989330917596817 + 0.001 * 7.147526264190674
Epoch 370, val loss: 0.957163393497467
Epoch 380, training loss: 0.09370973706245422 = 0.08656133711338043 + 0.001 * 7.148401260375977
Epoch 380, val loss: 0.9702858328819275
Epoch 390, training loss: 0.08234430849552155 = 0.07519521564245224 + 0.001 * 7.149095058441162
Epoch 390, val loss: 0.9839025139808655
Epoch 400, training loss: 0.07268370687961578 = 0.06553734838962555 + 0.001 * 7.146355628967285
Epoch 400, val loss: 0.9976741075515747
Epoch 410, training loss: 0.06449192762374878 = 0.05734854191541672 + 0.001 * 7.143385887145996
Epoch 410, val loss: 1.0113487243652344
Epoch 420, training loss: 0.057553112506866455 = 0.050410736352205276 + 0.001 * 7.142374038696289
Epoch 420, val loss: 1.0247856378555298
Epoch 430, training loss: 0.051673393696546555 = 0.044528547674417496 + 0.001 * 7.144845008850098
Epoch 430, val loss: 1.0379105806350708
Epoch 440, training loss: 0.04667326807975769 = 0.03953062370419502 + 0.001 * 7.14264440536499
Epoch 440, val loss: 1.0506473779678345
Epoch 450, training loss: 0.04241722822189331 = 0.03527111932635307 + 0.001 * 7.1461100578308105
Epoch 450, val loss: 1.0629032850265503
Epoch 460, training loss: 0.03876805678009987 = 0.03162737935781479 + 0.001 * 7.140678405761719
Epoch 460, val loss: 1.0746415853500366
Epoch 470, training loss: 0.03563486784696579 = 0.02849605865776539 + 0.001 * 7.138809680938721
Epoch 470, val loss: 1.0859346389770508
Epoch 480, training loss: 0.0329279899597168 = 0.025792401283979416 + 0.001 * 7.135587692260742
Epoch 480, val loss: 1.096774935722351
Epoch 490, training loss: 0.030588408932089806 = 0.023446813225746155 + 0.001 * 7.141595840454102
Epoch 490, val loss: 1.1071863174438477
Epoch 500, training loss: 0.028537973761558533 = 0.021402135491371155 + 0.001 * 7.135837078094482
Epoch 500, val loss: 1.1171659231185913
Epoch 510, training loss: 0.02674376592040062 = 0.01961122266948223 + 0.001 * 7.132544040679932
Epoch 510, val loss: 1.126751184463501
Epoch 520, training loss: 0.025169482454657555 = 0.018035495653748512 + 0.001 * 7.133986473083496
Epoch 520, val loss: 1.1359786987304688
Epoch 530, training loss: 0.023771965876221657 = 0.016643013805150986 + 0.001 * 7.128951549530029
Epoch 530, val loss: 1.144832968711853
Epoch 540, training loss: 0.022535031661391258 = 0.015407278202474117 + 0.001 * 7.127753734588623
Epoch 540, val loss: 1.1533634662628174
Epoch 550, training loss: 0.021436071023344994 = 0.01430634967982769 + 0.001 * 7.129721164703369
Epoch 550, val loss: 1.161563754081726
Epoch 560, training loss: 0.02044849283993244 = 0.01332169771194458 + 0.001 * 7.126794815063477
Epoch 560, val loss: 1.1694284677505493
Epoch 570, training loss: 0.01957501657307148 = 0.012437905184924603 + 0.001 * 7.137111186981201
Epoch 570, val loss: 1.1770274639129639
Epoch 580, training loss: 0.018767710775136948 = 0.011641877703368664 + 0.001 * 7.1258320808410645
Epoch 580, val loss: 1.1843149662017822
Epoch 590, training loss: 0.018045172095298767 = 0.010922374203801155 + 0.001 * 7.122797012329102
Epoch 590, val loss: 1.1913481950759888
Epoch 600, training loss: 0.01740768738090992 = 0.010269459336996078 + 0.001 * 7.138227939605713
Epoch 600, val loss: 1.1981353759765625
Epoch 610, training loss: 0.0167924165725708 = 0.009673613123595715 + 0.001 * 7.11880350112915
Epoch 610, val loss: 1.2046791315078735
Epoch 620, training loss: 0.016241997480392456 = 0.00912618637084961 + 0.001 * 7.115810394287109
Epoch 620, val loss: 1.2109863758087158
Epoch 630, training loss: 0.015739891678094864 = 0.008620928041636944 + 0.001 * 7.118962287902832
Epoch 630, val loss: 1.2170459032058716
Epoch 640, training loss: 0.015271816402673721 = 0.008153737522661686 + 0.001 * 7.118078231811523
Epoch 640, val loss: 1.222923994064331
Epoch 650, training loss: 0.01483541913330555 = 0.007721500005573034 + 0.001 * 7.113919258117676
Epoch 650, val loss: 1.2286101579666138
Epoch 660, training loss: 0.014433614909648895 = 0.00732151186093688 + 0.001 * 7.112102508544922
Epoch 660, val loss: 1.234139323234558
Epoch 670, training loss: 0.014060970395803452 = 0.006951230112463236 + 0.001 * 7.109739780426025
Epoch 670, val loss: 1.2394987344741821
Epoch 680, training loss: 0.013714021071791649 = 0.006608246359974146 + 0.001 * 7.105774402618408
Epoch 680, val loss: 1.2447131872177124
Epoch 690, training loss: 0.013423719443380833 = 0.0062902807258069515 + 0.001 * 7.133438587188721
Epoch 690, val loss: 1.2497674226760864
Epoch 700, training loss: 0.013108832761645317 = 0.005995267070829868 + 0.001 * 7.113565921783447
Epoch 700, val loss: 1.2546684741973877
Epoch 710, training loss: 0.012823398225009441 = 0.005721185356378555 + 0.001 * 7.102212429046631
Epoch 710, val loss: 1.2594313621520996
Epoch 720, training loss: 0.012570841237902641 = 0.005466226488351822 + 0.001 * 7.104613780975342
Epoch 720, val loss: 1.264061450958252
Epoch 730, training loss: 0.012329962104558945 = 0.005228772293776274 + 0.001 * 7.101190090179443
Epoch 730, val loss: 1.2685600519180298
Epoch 740, training loss: 0.012101499363780022 = 0.005007356870919466 + 0.001 * 7.094142436981201
Epoch 740, val loss: 1.2729400396347046
Epoch 750, training loss: 0.011903678998351097 = 0.004800653085112572 + 0.001 * 7.103025913238525
Epoch 750, val loss: 1.2771697044372559
Epoch 760, training loss: 0.011704107746481895 = 0.004607411567121744 + 0.001 * 7.096695899963379
Epoch 760, val loss: 1.2813231945037842
Epoch 770, training loss: 0.011525517329573631 = 0.004426570609211922 + 0.001 * 7.098947048187256
Epoch 770, val loss: 1.2853459119796753
Epoch 780, training loss: 0.011345753446221352 = 0.004257053602486849 + 0.001 * 7.088698863983154
Epoch 780, val loss: 1.289276361465454
Epoch 790, training loss: 0.011196006089448929 = 0.004097944125533104 + 0.001 * 7.098062038421631
Epoch 790, val loss: 1.2931125164031982
Epoch 800, training loss: 0.011033985763788223 = 0.00394845474511385 + 0.001 * 7.085531234741211
Epoch 800, val loss: 1.2968496084213257
Epoch 810, training loss: 0.010901182889938354 = 0.003807859029620886 + 0.001 * 7.093324184417725
Epoch 810, val loss: 1.3004945516586304
Epoch 820, training loss: 0.010759266093373299 = 0.0036754708271473646 + 0.001 * 7.083794593811035
Epoch 820, val loss: 1.3040481805801392
Epoch 830, training loss: 0.010663846507668495 = 0.0035506621934473515 + 0.001 * 7.113183975219727
Epoch 830, val loss: 1.307517170906067
Epoch 840, training loss: 0.010503644123673439 = 0.003432885743677616 + 0.001 * 7.070757865905762
Epoch 840, val loss: 1.3108882904052734
Epoch 850, training loss: 0.010406766086816788 = 0.0033216378651559353 + 0.001 * 7.085128307342529
Epoch 850, val loss: 1.3141753673553467
Epoch 860, training loss: 0.010282105766236782 = 0.0032164438161998987 + 0.001 * 7.065661907196045
Epoch 860, val loss: 1.317395567893982
Epoch 870, training loss: 0.01018481981009245 = 0.0031168924178928137 + 0.001 * 7.067927360534668
Epoch 870, val loss: 1.3205327987670898
Epoch 880, training loss: 0.0100901173427701 = 0.0030225920490920544 + 0.001 * 7.0675249099731445
Epoch 880, val loss: 1.3235983848571777
Epoch 890, training loss: 0.010006183758378029 = 0.0029331818222999573 + 0.001 * 7.073001861572266
Epoch 890, val loss: 1.326590895652771
Epoch 900, training loss: 0.009906909428536892 = 0.0028483180794864893 + 0.001 * 7.058591365814209
Epoch 900, val loss: 1.3295173645019531
Epoch 910, training loss: 0.00982221495360136 = 0.002767694415524602 + 0.001 * 7.054520130157471
Epoch 910, val loss: 1.3323748111724854
Epoch 920, training loss: 0.009742975234985352 = 0.0026910207234323025 + 0.001 * 7.0519537925720215
Epoch 920, val loss: 1.3351722955703735
Epoch 930, training loss: 0.00968192145228386 = 0.0026180888526141644 + 0.001 * 7.063832759857178
Epoch 930, val loss: 1.337902307510376
Epoch 940, training loss: 0.009604819118976593 = 0.0025486606173217297 + 0.001 * 7.056158542633057
Epoch 940, val loss: 1.340564250946045
Epoch 950, training loss: 0.009526312351226807 = 0.0024825013242661953 + 0.001 * 7.043810844421387
Epoch 950, val loss: 1.3431766033172607
Epoch 960, training loss: 0.009480983950197697 = 0.0024194016586989164 + 0.001 * 7.061581611633301
Epoch 960, val loss: 1.345726490020752
Epoch 970, training loss: 0.009439641609787941 = 0.0023591856006532907 + 0.001 * 7.080455780029297
Epoch 970, val loss: 1.3482142686843872
Epoch 980, training loss: 0.009375875815749168 = 0.002301678527146578 + 0.001 * 7.0741963386535645
Epoch 980, val loss: 1.3506604433059692
Epoch 990, training loss: 0.009289821609854698 = 0.0022467216476798058 + 0.001 * 7.043099880218506
Epoch 990, val loss: 1.3530442714691162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 1.9650921821594238 = 1.9564954042434692 + 0.001 * 8.596831321716309
Epoch 0, val loss: 1.9491658210754395
Epoch 10, training loss: 1.9542725086212158 = 1.9456757307052612 + 0.001 * 8.59679126739502
Epoch 10, val loss: 1.939057469367981
Epoch 20, training loss: 1.9407517910003662 = 1.9321551322937012 + 0.001 * 8.596649169921875
Epoch 20, val loss: 1.9259910583496094
Epoch 30, training loss: 1.921675443649292 = 1.9130791425704956 + 0.001 * 8.596318244934082
Epoch 30, val loss: 1.9072672128677368
Epoch 40, training loss: 1.8935751914978027 = 1.8849797248840332 + 0.001 * 8.5955171585083
Epoch 40, val loss: 1.8799402713775635
Epoch 50, training loss: 1.8552687168121338 = 1.8466753959655762 + 0.001 * 8.593308448791504
Epoch 50, val loss: 1.8447728157043457
Epoch 60, training loss: 1.8161152601242065 = 1.8075295686721802 + 0.001 * 8.585652351379395
Epoch 60, val loss: 1.8136866092681885
Epoch 70, training loss: 1.7870606184005737 = 1.7785074710845947 + 0.001 * 8.553154945373535
Epoch 70, val loss: 1.7914714813232422
Epoch 80, training loss: 1.748683214187622 = 1.740338921546936 + 0.001 * 8.344292640686035
Epoch 80, val loss: 1.7544540166854858
Epoch 90, training loss: 1.694472312927246 = 1.6863751411437988 + 0.001 * 8.09714126586914
Epoch 90, val loss: 1.7039731740951538
Epoch 100, training loss: 1.620604395866394 = 1.6125415563583374 + 0.001 * 8.06285572052002
Epoch 100, val loss: 1.6382801532745361
Epoch 110, training loss: 1.5367026329040527 = 1.528649091720581 + 0.001 * 8.053566932678223
Epoch 110, val loss: 1.5652955770492554
Epoch 120, training loss: 1.4563676118850708 = 1.44831383228302 + 0.001 * 8.05378532409668
Epoch 120, val loss: 1.4983195066452026
Epoch 130, training loss: 1.3838987350463867 = 1.3758455514907837 + 0.001 * 8.053231239318848
Epoch 130, val loss: 1.4415894746780396
Epoch 140, training loss: 1.3172307014465332 = 1.3091821670532227 + 0.001 * 8.048482894897461
Epoch 140, val loss: 1.393093466758728
Epoch 150, training loss: 1.2547528743743896 = 1.2467195987701416 + 0.001 * 8.033220291137695
Epoch 150, val loss: 1.3505011796951294
Epoch 160, training loss: 1.1946204900741577 = 1.186649203300476 + 0.001 * 7.971235752105713
Epoch 160, val loss: 1.3122403621673584
Epoch 170, training loss: 1.13347589969635 = 1.1258448362350464 + 0.001 * 7.63111686706543
Epoch 170, val loss: 1.2763224840164185
Epoch 180, training loss: 1.068174958229065 = 1.0606476068496704 + 0.001 * 7.527402877807617
Epoch 180, val loss: 1.238913655281067
Epoch 190, training loss: 0.9962930083274841 = 0.9888293147087097 + 0.001 * 7.463677883148193
Epoch 190, val loss: 1.1974678039550781
Epoch 200, training loss: 0.9181130528450012 = 0.9106882810592651 + 0.001 * 7.424755573272705
Epoch 200, val loss: 1.1524800062179565
Epoch 210, training loss: 0.8369958400726318 = 0.8295809030532837 + 0.001 * 7.414955139160156
Epoch 210, val loss: 1.1065237522125244
Epoch 220, training loss: 0.7579360008239746 = 0.7505279779434204 + 0.001 * 7.408022880554199
Epoch 220, val loss: 1.063794732093811
Epoch 230, training loss: 0.6854066848754883 = 0.6780072450637817 + 0.001 * 7.399438381195068
Epoch 230, val loss: 1.0285357236862183
Epoch 240, training loss: 0.6217235922813416 = 0.6143344044685364 + 0.001 * 7.389190196990967
Epoch 240, val loss: 1.0026191473007202
Epoch 250, training loss: 0.5668941140174866 = 0.559519350528717 + 0.001 * 7.374774932861328
Epoch 250, val loss: 0.9857301115989685
Epoch 260, training loss: 0.5194606781005859 = 0.5121080279350281 + 0.001 * 7.352657318115234
Epoch 260, val loss: 0.9761319756507874
Epoch 270, training loss: 0.4774346947669983 = 0.4701140820980072 + 0.001 * 7.320620536804199
Epoch 270, val loss: 0.9717838764190674
Epoch 280, training loss: 0.43896880745887756 = 0.43164587020874023 + 0.001 * 7.3229451179504395
Epoch 280, val loss: 0.97107994556427
Epoch 290, training loss: 0.4024962782859802 = 0.3952025771141052 + 0.001 * 7.293705940246582
Epoch 290, val loss: 0.9723870158195496
Epoch 300, training loss: 0.3672049045562744 = 0.35992640256881714 + 0.001 * 7.278514862060547
Epoch 300, val loss: 0.9751017093658447
Epoch 310, training loss: 0.3330436944961548 = 0.3257664740085602 + 0.001 * 7.277228355407715
Epoch 310, val loss: 0.9792339205741882
Epoch 320, training loss: 0.3005561828613281 = 0.29327937960624695 + 0.001 * 7.276795387268066
Epoch 320, val loss: 0.9854481816291809
Epoch 330, training loss: 0.27035605907440186 = 0.2630794942378998 + 0.001 * 7.276554584503174
Epoch 330, val loss: 0.993970513343811
Epoch 340, training loss: 0.24260231852531433 = 0.23532605171203613 + 0.001 * 7.27626895904541
Epoch 340, val loss: 1.0051867961883545
Epoch 350, training loss: 0.21705207228660583 = 0.20977625250816345 + 0.001 * 7.275821685791016
Epoch 350, val loss: 1.0194416046142578
Epoch 360, training loss: 0.19349884986877441 = 0.186223566532135 + 0.001 * 7.275289535522461
Epoch 360, val loss: 1.0365285873413086
Epoch 370, training loss: 0.171919584274292 = 0.16464483737945557 + 0.001 * 7.274750232696533
Epoch 370, val loss: 1.0562893152236938
Epoch 380, training loss: 0.15238921344280243 = 0.14511509239673615 + 0.001 * 7.274127006530762
Epoch 380, val loss: 1.0787925720214844
Epoch 390, training loss: 0.13499175012111664 = 0.12771838903427124 + 0.001 * 7.273359298706055
Epoch 390, val loss: 1.1036992073059082
Epoch 400, training loss: 0.11975988000631332 = 0.11248746514320374 + 0.001 * 7.2724175453186035
Epoch 400, val loss: 1.1304594278335571
Epoch 410, training loss: 0.1065138503909111 = 0.09924262017011642 + 0.001 * 7.2712273597717285
Epoch 410, val loss: 1.1588279008865356
Epoch 420, training loss: 0.0950043573975563 = 0.0877343937754631 + 0.001 * 7.269963264465332
Epoch 420, val loss: 1.1879076957702637
Epoch 430, training loss: 0.08501055091619492 = 0.07774053514003754 + 0.001 * 7.270015239715576
Epoch 430, val loss: 1.2170450687408447
Epoch 440, training loss: 0.07634132355451584 = 0.06907445192337036 + 0.001 * 7.26686954498291
Epoch 440, val loss: 1.2458643913269043
Epoch 450, training loss: 0.06882654875516891 = 0.06156346574425697 + 0.001 * 7.263082504272461
Epoch 450, val loss: 1.274164080619812
Epoch 460, training loss: 0.06231356039643288 = 0.0550498440861702 + 0.001 * 7.263715744018555
Epoch 460, val loss: 1.3016184568405151
Epoch 470, training loss: 0.05665034055709839 = 0.0493924617767334 + 0.001 * 7.257877349853516
Epoch 470, val loss: 1.3282712697982788
Epoch 480, training loss: 0.05172194167971611 = 0.04446952044963837 + 0.001 * 7.252421855926514
Epoch 480, val loss: 1.354013204574585
Epoch 490, training loss: 0.047421421855688095 = 0.040177009999752045 + 0.001 * 7.244411468505859
Epoch 490, val loss: 1.3788492679595947
Epoch 500, training loss: 0.04366302490234375 = 0.03642428666353226 + 0.001 * 7.2387375831604
Epoch 500, val loss: 1.402789831161499
Epoch 510, training loss: 0.04036746546626091 = 0.03313423693180084 + 0.001 * 7.233226776123047
Epoch 510, val loss: 1.4257737398147583
Epoch 520, training loss: 0.0374564602971077 = 0.030241960659623146 + 0.001 * 7.214498996734619
Epoch 520, val loss: 1.44789457321167
Epoch 530, training loss: 0.03489147499203682 = 0.02769223041832447 + 0.001 * 7.199243068695068
Epoch 530, val loss: 1.4691689014434814
Epoch 540, training loss: 0.03268183395266533 = 0.025436706840991974 + 0.001 * 7.245126247406006
Epoch 540, val loss: 1.4895479679107666
Epoch 550, training loss: 0.030634663999080658 = 0.023435674607753754 + 0.001 * 7.19898796081543
Epoch 550, val loss: 1.509088158607483
Epoch 560, training loss: 0.02883615717291832 = 0.02165507897734642 + 0.001 * 7.18107795715332
Epoch 560, val loss: 1.527891993522644
Epoch 570, training loss: 0.027242902666330338 = 0.020065365359187126 + 0.001 * 7.177536487579346
Epoch 570, val loss: 1.5459442138671875
Epoch 580, training loss: 0.025818053632974625 = 0.018641676753759384 + 0.001 * 7.176376819610596
Epoch 580, val loss: 1.563305377960205
Epoch 590, training loss: 0.02452334761619568 = 0.01736319437623024 + 0.001 * 7.160151958465576
Epoch 590, val loss: 1.5800095796585083
Epoch 600, training loss: 0.023371729999780655 = 0.016211552545428276 + 0.001 * 7.160177707672119
Epoch 600, val loss: 1.5960355997085571
Epoch 610, training loss: 0.022354083135724068 = 0.015171228908002377 + 0.001 * 7.182853698730469
Epoch 610, val loss: 1.611466407775879
Epoch 620, training loss: 0.021388601511716843 = 0.014228710904717445 + 0.001 * 7.159890651702881
Epoch 620, val loss: 1.6262969970703125
Epoch 630, training loss: 0.020527664572000504 = 0.013372540473937988 + 0.001 * 7.155124664306641
Epoch 630, val loss: 1.640589714050293
Epoch 640, training loss: 0.01974036544561386 = 0.012592768296599388 + 0.001 * 7.14759635925293
Epoch 640, val loss: 1.6543246507644653
Epoch 650, training loss: 0.019034139811992645 = 0.011880695819854736 + 0.001 * 7.153442859649658
Epoch 650, val loss: 1.667595624923706
Epoch 660, training loss: 0.018368439748883247 = 0.011228795163333416 + 0.001 * 7.139644622802734
Epoch 660, val loss: 1.6803507804870605
Epoch 670, training loss: 0.0177726112306118 = 0.010630895383656025 + 0.001 * 7.141716003417969
Epoch 670, val loss: 1.692659854888916
Epoch 680, training loss: 0.017228752374649048 = 0.010081348940730095 + 0.001 * 7.147403240203857
Epoch 680, val loss: 1.7045096158981323
Epoch 690, training loss: 0.016712019219994545 = 0.009575193747878075 + 0.001 * 7.1368255615234375
Epoch 690, val loss: 1.7159708738327026
Epoch 700, training loss: 0.01624933071434498 = 0.009107878431677818 + 0.001 * 7.141451358795166
Epoch 700, val loss: 1.7270256280899048
Epoch 710, training loss: 0.01581120863556862 = 0.008675631135702133 + 0.001 * 7.135576248168945
Epoch 710, val loss: 1.7376443147659302
Epoch 720, training loss: 0.015423739328980446 = 0.008275016210973263 + 0.001 * 7.148723125457764
Epoch 720, val loss: 1.7479404211044312
Epoch 730, training loss: 0.015032870694994926 = 0.007903065532445908 + 0.001 * 7.129804611206055
Epoch 730, val loss: 1.7578948736190796
Epoch 740, training loss: 0.014697580598294735 = 0.007557108998298645 + 0.001 * 7.140471458435059
Epoch 740, val loss: 1.7675220966339111
Epoch 750, training loss: 0.014356549829244614 = 0.007234802469611168 + 0.001 * 7.121747016906738
Epoch 750, val loss: 1.776840329170227
Epoch 760, training loss: 0.014055520296096802 = 0.0069340672343969345 + 0.001 * 7.121452808380127
Epoch 760, val loss: 1.7858800888061523
Epoch 770, training loss: 0.013787005096673965 = 0.006653057876974344 + 0.001 * 7.133947372436523
Epoch 770, val loss: 1.794644832611084
Epoch 780, training loss: 0.01351467426866293 = 0.006390097085386515 + 0.001 * 7.124577045440674
Epoch 780, val loss: 1.803137183189392
Epoch 790, training loss: 0.013274765573441982 = 0.006143640726804733 + 0.001 * 7.131124496459961
Epoch 790, val loss: 1.8113484382629395
Epoch 800, training loss: 0.013031186535954475 = 0.005912334658205509 + 0.001 * 7.118852138519287
Epoch 800, val loss: 1.8193050622940063
Epoch 810, training loss: 0.012801080010831356 = 0.005694941151887178 + 0.001 * 7.106138706207275
Epoch 810, val loss: 1.827026605606079
Epoch 820, training loss: 0.012601888738572598 = 0.005490375682711601 + 0.001 * 7.111512660980225
Epoch 820, val loss: 1.8345152139663696
Epoch 830, training loss: 0.01240265741944313 = 0.0052976347506046295 + 0.001 * 7.10502290725708
Epoch 830, val loss: 1.8417490720748901
Epoch 840, training loss: 0.012228868901729584 = 0.005115687381476164 + 0.001 * 7.1131815910339355
Epoch 840, val loss: 1.8487876653671265
Epoch 850, training loss: 0.012044066563248634 = 0.004943732637912035 + 0.001 * 7.100333213806152
Epoch 850, val loss: 1.8556112051010132
Epoch 860, training loss: 0.011900139972567558 = 0.004780793562531471 + 0.001 * 7.119346618652344
Epoch 860, val loss: 1.8622262477874756
Epoch 870, training loss: 0.011719770729541779 = 0.0046258000656962395 + 0.001 * 7.093970775604248
Epoch 870, val loss: 1.8686128854751587
Epoch 880, training loss: 0.01157393865287304 = 0.004477672278881073 + 0.001 * 7.096266269683838
Epoch 880, val loss: 1.8748191595077515
Epoch 890, training loss: 0.011430835351347923 = 0.004335638135671616 + 0.001 * 7.095196723937988
Epoch 890, val loss: 1.8808273077011108
Epoch 900, training loss: 0.011302562430500984 = 0.0041990322060883045 + 0.001 * 7.103530406951904
Epoch 900, val loss: 1.88666570186615
Epoch 910, training loss: 0.011156202293932438 = 0.004067534115165472 + 0.001 * 7.088667869567871
Epoch 910, val loss: 1.892317295074463
Epoch 920, training loss: 0.011031151749193668 = 0.003941062372177839 + 0.001 * 7.090088844299316
Epoch 920, val loss: 1.8978248834609985
Epoch 930, training loss: 0.010926036164164543 = 0.003819510107859969 + 0.001 * 7.106525897979736
Epoch 930, val loss: 1.9031496047973633
Epoch 940, training loss: 0.01080464106053114 = 0.0037027234211564064 + 0.001 * 7.101917266845703
Epoch 940, val loss: 1.9083195924758911
Epoch 950, training loss: 0.010678057558834553 = 0.0035906617995351553 + 0.001 * 7.087395668029785
Epoch 950, val loss: 1.9133257865905762
Epoch 960, training loss: 0.01058464590460062 = 0.0034832314122468233 + 0.001 * 7.101413726806641
Epoch 960, val loss: 1.9181987047195435
Epoch 970, training loss: 0.010464576072990894 = 0.003380303969606757 + 0.001 * 7.084271430969238
Epoch 970, val loss: 1.9228904247283936
Epoch 980, training loss: 0.010364128276705742 = 0.0032817076425999403 + 0.001 * 7.082420349121094
Epoch 980, val loss: 1.927445650100708
Epoch 990, training loss: 0.010266025550663471 = 0.0031873362604528666 + 0.001 * 7.078689098358154
Epoch 990, val loss: 1.9318411350250244
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7037037037037037
0.8128624143384291
The final CL Acc:0.72963, 0.02283, The final GNN Acc:0.81427, 0.00108
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13190])
remove edge: torch.Size([2, 7944])
updated graph: torch.Size([2, 10578])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.948870301246643 = 1.9402735233306885 + 0.001 * 8.59682559967041
Epoch 0, val loss: 1.9439817667007446
Epoch 10, training loss: 1.9384639263153076 = 1.929867148399353 + 0.001 * 8.596778869628906
Epoch 10, val loss: 1.9331977367401123
Epoch 20, training loss: 1.9254112243652344 = 1.9168145656585693 + 0.001 * 8.596620559692383
Epoch 20, val loss: 1.91946542263031
Epoch 30, training loss: 1.9066460132598877 = 1.8980498313903809 + 0.001 * 8.596233367919922
Epoch 30, val loss: 1.8997832536697388
Epoch 40, training loss: 1.8789795637130737 = 1.8703843355178833 + 0.001 * 8.595231056213379
Epoch 40, val loss: 1.871358036994934
Epoch 50, training loss: 1.8418222665786743 = 1.8332300186157227 + 0.001 * 8.592196464538574
Epoch 50, val loss: 1.8353108167648315
Epoch 60, training loss: 1.8032406568527222 = 1.7946603298187256 + 0.001 * 8.58033275604248
Epoch 60, val loss: 1.8018474578857422
Epoch 70, training loss: 1.767628788948059 = 1.7591089010238647 + 0.001 * 8.519940376281738
Epoch 70, val loss: 1.7708712816238403
Epoch 80, training loss: 1.717800259590149 = 1.7096209526062012 + 0.001 * 8.179269790649414
Epoch 80, val loss: 1.724804401397705
Epoch 90, training loss: 1.6491947174072266 = 1.6411733627319336 + 0.001 * 8.021307945251465
Epoch 90, val loss: 1.6630525588989258
Epoch 100, training loss: 1.563158631324768 = 1.5551793575286865 + 0.001 * 7.979302883148193
Epoch 100, val loss: 1.5893456935882568
Epoch 110, training loss: 1.474043369293213 = 1.4660944938659668 + 0.001 * 7.948906898498535
Epoch 110, val loss: 1.5153226852416992
Epoch 120, training loss: 1.3888763189315796 = 1.3809959888458252 + 0.001 * 7.880307674407959
Epoch 120, val loss: 1.449018120765686
Epoch 130, training loss: 1.3059818744659424 = 1.2982572317123413 + 0.001 * 7.7246928215026855
Epoch 130, val loss: 1.385803461074829
Epoch 140, training loss: 1.2234340906143188 = 1.2158676385879517 + 0.001 * 7.566506385803223
Epoch 140, val loss: 1.324422836303711
Epoch 150, training loss: 1.1428422927856445 = 1.1353434324264526 + 0.001 * 7.498863697052002
Epoch 150, val loss: 1.2663534879684448
Epoch 160, training loss: 1.0668127536773682 = 1.0594009160995483 + 0.001 * 7.411849498748779
Epoch 160, val loss: 1.2128877639770508
Epoch 170, training loss: 0.9966875910758972 = 0.9893335103988647 + 0.001 * 7.354062080383301
Epoch 170, val loss: 1.1636238098144531
Epoch 180, training loss: 0.9320107102394104 = 0.9246773719787598 + 0.001 * 7.333336353302002
Epoch 180, val loss: 1.117364525794983
Epoch 190, training loss: 0.8711447715759277 = 0.8638113737106323 + 0.001 * 7.333408832550049
Epoch 190, val loss: 1.0722026824951172
Epoch 200, training loss: 0.8123828172683716 = 0.805050253868103 + 0.001 * 7.332590579986572
Epoch 200, val loss: 1.027212142944336
Epoch 210, training loss: 0.755194365978241 = 0.747866153717041 + 0.001 * 7.328214168548584
Epoch 210, val loss: 0.9825742244720459
Epoch 220, training loss: 0.7002401351928711 = 0.6929172873497009 + 0.001 * 7.32284688949585
Epoch 220, val loss: 0.9398604035377502
Epoch 230, training loss: 0.6483328342437744 = 0.6410185098648071 + 0.001 * 7.314343452453613
Epoch 230, val loss: 0.9008124470710754
Epoch 240, training loss: 0.5995745062828064 = 0.5922759175300598 + 0.001 * 7.298585414886475
Epoch 240, val loss: 0.8671722412109375
Epoch 250, training loss: 0.5535340905189514 = 0.5462636351585388 + 0.001 * 7.270474433898926
Epoch 250, val loss: 0.8394231796264648
Epoch 260, training loss: 0.5101702809333801 = 0.5029333829879761 + 0.001 * 7.236874580383301
Epoch 260, val loss: 0.8176382184028625
Epoch 270, training loss: 0.4696159362792969 = 0.46240729093551636 + 0.001 * 7.208638668060303
Epoch 270, val loss: 0.801446259021759
Epoch 280, training loss: 0.4313367009162903 = 0.42415356636047363 + 0.001 * 7.183122634887695
Epoch 280, val loss: 0.7898109555244446
Epoch 290, training loss: 0.394034743309021 = 0.3868621587753296 + 0.001 * 7.172578811645508
Epoch 290, val loss: 0.7812384366989136
Epoch 300, training loss: 0.35660266876220703 = 0.3494342565536499 + 0.001 * 7.168406963348389
Epoch 300, val loss: 0.774314820766449
Epoch 310, training loss: 0.3188101649284363 = 0.311642587184906 + 0.001 * 7.1675639152526855
Epoch 310, val loss: 0.7684521675109863
Epoch 320, training loss: 0.2814798653125763 = 0.2743123769760132 + 0.001 * 7.167482376098633
Epoch 320, val loss: 0.7639575600624084
Epoch 330, training loss: 0.24606844782829285 = 0.23890146613121033 + 0.001 * 7.166982173919678
Epoch 330, val loss: 0.7614819407463074
Epoch 340, training loss: 0.2139650285243988 = 0.206798255443573 + 0.001 * 7.16677713394165
Epoch 340, val loss: 0.761874258518219
Epoch 350, training loss: 0.18593977391719818 = 0.17877303063869476 + 0.001 * 7.166749000549316
Epoch 350, val loss: 0.7656170725822449
Epoch 360, training loss: 0.16204790771007538 = 0.15488164126873016 + 0.001 * 7.166266441345215
Epoch 360, val loss: 0.7729367017745972
Epoch 370, training loss: 0.14187362790107727 = 0.13470740616321564 + 0.001 * 7.166215419769287
Epoch 370, val loss: 0.7833997011184692
Epoch 380, training loss: 0.1248433068394661 = 0.11767695844173431 + 0.001 * 7.166348457336426
Epoch 380, val loss: 0.7962816953659058
Epoch 390, training loss: 0.11038828641176224 = 0.10322200506925583 + 0.001 * 7.1662821769714355
Epoch 390, val loss: 0.8108720779418945
Epoch 400, training loss: 0.09803534299135208 = 0.09086915105581284 + 0.001 * 7.166190147399902
Epoch 400, val loss: 0.8266087770462036
Epoch 410, training loss: 0.08741257339715958 = 0.08024652302265167 + 0.001 * 7.166047096252441
Epoch 410, val loss: 0.8430106043815613
Epoch 420, training loss: 0.07822339236736298 = 0.07105717808008194 + 0.001 * 7.166214466094971
Epoch 420, val loss: 0.8597579598426819
Epoch 430, training loss: 0.07024025171995163 = 0.06307460367679596 + 0.001 * 7.165648937225342
Epoch 430, val loss: 0.8766193389892578
Epoch 440, training loss: 0.06328797340393066 = 0.0561230294406414 + 0.001 * 7.164946556091309
Epoch 440, val loss: 0.8933866620063782
Epoch 450, training loss: 0.05722210183739662 = 0.050057560205459595 + 0.001 * 7.164539813995361
Epoch 450, val loss: 0.9099094271659851
Epoch 460, training loss: 0.05192255228757858 = 0.04475865885615349 + 0.001 * 7.163894176483154
Epoch 460, val loss: 0.9261223077774048
Epoch 470, training loss: 0.04728178307414055 = 0.04011906683444977 + 0.001 * 7.162714958190918
Epoch 470, val loss: 0.9419564604759216
Epoch 480, training loss: 0.0432116724550724 = 0.03605072200298309 + 0.001 * 7.16094970703125
Epoch 480, val loss: 0.9573162794113159
Epoch 490, training loss: 0.03964178264141083 = 0.03248223662376404 + 0.001 * 7.1595458984375
Epoch 490, val loss: 0.9721739888191223
Epoch 500, training loss: 0.03651074692606926 = 0.02935156598687172 + 0.001 * 7.159181118011475
Epoch 500, val loss: 0.9864987730979919
Epoch 510, training loss: 0.033758558332920074 = 0.02660318836569786 + 0.001 * 7.155371189117432
Epoch 510, val loss: 1.0003175735473633
Epoch 520, training loss: 0.03133983165025711 = 0.02418791502714157 + 0.001 * 7.15191650390625
Epoch 520, val loss: 1.0136452913284302
Epoch 530, training loss: 0.02920994721353054 = 0.022062230855226517 + 0.001 * 7.147716045379639
Epoch 530, val loss: 1.026463270187378
Epoch 540, training loss: 0.027331825345754623 = 0.020187633112072945 + 0.001 * 7.144192695617676
Epoch 540, val loss: 1.0387557744979858
Epoch 550, training loss: 0.025673426687717438 = 0.01853058859705925 + 0.001 * 7.142837047576904
Epoch 550, val loss: 1.050612211227417
Epoch 560, training loss: 0.024205898866057396 = 0.017061470076441765 + 0.001 * 7.144428730010986
Epoch 560, val loss: 1.061956763267517
Epoch 570, training loss: 0.022894013673067093 = 0.0157553069293499 + 0.001 * 7.138706207275391
Epoch 570, val loss: 1.0728405714035034
Epoch 580, training loss: 0.021725604310631752 = 0.014590387232601643 + 0.001 * 7.135217189788818
Epoch 580, val loss: 1.0833009481430054
Epoch 590, training loss: 0.020664582028985023 = 0.013548382557928562 + 0.001 * 7.116199016571045
Epoch 590, val loss: 1.0933440923690796
Epoch 600, training loss: 0.01972099579870701 = 0.012613570317626 + 0.001 * 7.107424736022949
Epoch 600, val loss: 1.102989912033081
Epoch 610, training loss: 0.01890270784497261 = 0.011772481724619865 + 0.001 * 7.130224704742432
Epoch 610, val loss: 1.112257957458496
Epoch 620, training loss: 0.01810990273952484 = 0.011013477109372616 + 0.001 * 7.096425533294678
Epoch 620, val loss: 1.1211917400360107
Epoch 630, training loss: 0.017421793192625046 = 0.01032660435885191 + 0.001 * 7.095188140869141
Epoch 630, val loss: 1.1297959089279175
Epoch 640, training loss: 0.016792774200439453 = 0.009703359566628933 + 0.001 * 7.089415073394775
Epoch 640, val loss: 1.1380963325500488
Epoch 650, training loss: 0.016234315931797028 = 0.009136386215686798 + 0.001 * 7.097928524017334
Epoch 650, val loss: 1.1460996866226196
Epoch 660, training loss: 0.01572135090827942 = 0.008619306609034538 + 0.001 * 7.102044105529785
Epoch 660, val loss: 1.1538275480270386
Epoch 670, training loss: 0.015241705812513828 = 0.008146584965288639 + 0.001 * 7.095120429992676
Epoch 670, val loss: 1.1612919569015503
Epoch 680, training loss: 0.014789490960538387 = 0.007713352330029011 + 0.001 * 7.076138496398926
Epoch 680, val loss: 1.1685051918029785
Epoch 690, training loss: 0.014410126954317093 = 0.007315438240766525 + 0.001 * 7.094688892364502
Epoch 690, val loss: 1.1754381656646729
Epoch 700, training loss: 0.014035142958164215 = 0.006949133239686489 + 0.001 * 7.086009979248047
Epoch 700, val loss: 1.1821770668029785
Epoch 710, training loss: 0.01368380431085825 = 0.006611244287341833 + 0.001 * 7.072559833526611
Epoch 710, val loss: 1.188700795173645
Epoch 720, training loss: 0.01339101791381836 = 0.006298943888396025 + 0.001 * 7.092073917388916
Epoch 720, val loss: 1.1950336694717407
Epoch 730, training loss: 0.013094944879412651 = 0.006009699776768684 + 0.001 * 7.085244178771973
Epoch 730, val loss: 1.2011511325836182
Epoch 740, training loss: 0.012803998775780201 = 0.005741321016103029 + 0.001 * 7.062677383422852
Epoch 740, val loss: 1.2070820331573486
Epoch 750, training loss: 0.012554952874779701 = 0.0054918439127504826 + 0.001 * 7.063107967376709
Epoch 750, val loss: 1.2128336429595947
Epoch 760, training loss: 0.012327771633863449 = 0.00525942025706172 + 0.001 * 7.0683512687683105
Epoch 760, val loss: 1.2184224128723145
Epoch 770, training loss: 0.012108208611607552 = 0.005042464938014746 + 0.001 * 7.065743446350098
Epoch 770, val loss: 1.2238540649414062
Epoch 780, training loss: 0.011913936585187912 = 0.004839684814214706 + 0.001 * 7.074251651763916
Epoch 780, val loss: 1.2290993928909302
Epoch 790, training loss: 0.011712964624166489 = 0.004649786278605461 + 0.001 * 7.063178062438965
Epoch 790, val loss: 1.2342215776443481
Epoch 800, training loss: 0.011544477194547653 = 0.004471584688872099 + 0.001 * 7.072891712188721
Epoch 800, val loss: 1.2391974925994873
Epoch 810, training loss: 0.011414939537644386 = 0.004304040223360062 + 0.001 * 7.110898971557617
Epoch 810, val loss: 1.2440414428710938
Epoch 820, training loss: 0.01120971329510212 = 0.004146353807300329 + 0.001 * 7.06335973739624
Epoch 820, val loss: 1.2487424612045288
Epoch 830, training loss: 0.011046216823160648 = 0.003997665364295244 + 0.001 * 7.048551082611084
Epoch 830, val loss: 1.2533243894577026
Epoch 840, training loss: 0.010932045057415962 = 0.0038572640623897314 + 0.001 * 7.074780464172363
Epoch 840, val loss: 1.2577890157699585
Epoch 850, training loss: 0.010776862502098083 = 0.0037245198618620634 + 0.001 * 7.052341938018799
Epoch 850, val loss: 1.262144923210144
Epoch 860, training loss: 0.010642320849001408 = 0.0035989040043205023 + 0.001 * 7.043416500091553
Epoch 860, val loss: 1.266381859779358
Epoch 870, training loss: 0.010518534108996391 = 0.0034797927364706993 + 0.001 * 7.038741588592529
Epoch 870, val loss: 1.2705293893814087
Epoch 880, training loss: 0.010425160638988018 = 0.0033667716197669506 + 0.001 * 7.058388710021973
Epoch 880, val loss: 1.2745726108551025
Epoch 890, training loss: 0.010308493860065937 = 0.00325941015034914 + 0.001 * 7.049083232879639
Epoch 890, val loss: 1.2785307168960571
Epoch 900, training loss: 0.010190161876380444 = 0.00315745803527534 + 0.001 * 7.032703876495361
Epoch 900, val loss: 1.2823740243911743
Epoch 910, training loss: 0.010087982751429081 = 0.003060435177758336 + 0.001 * 7.0275468826293945
Epoch 910, val loss: 1.286142110824585
Epoch 920, training loss: 0.010017616674304008 = 0.0029680770821869373 + 0.001 * 7.049539089202881
Epoch 920, val loss: 1.2898231744766235
Epoch 930, training loss: 0.009917894378304482 = 0.0028800780419260263 + 0.001 * 7.037816047668457
Epoch 930, val loss: 1.293412446975708
Epoch 940, training loss: 0.00983805675059557 = 0.0027960657607764006 + 0.001 * 7.041990756988525
Epoch 940, val loss: 1.2969250679016113
Epoch 950, training loss: 0.009748270735144615 = 0.002715826267376542 + 0.001 * 7.032444000244141
Epoch 950, val loss: 1.300368070602417
Epoch 960, training loss: 0.009662608616054058 = 0.0026389972772449255 + 0.001 * 7.023610591888428
Epoch 960, val loss: 1.30374014377594
Epoch 970, training loss: 0.009585890918970108 = 0.0025653461925685406 + 0.001 * 7.020544528961182
Epoch 970, val loss: 1.3070344924926758
Epoch 980, training loss: 0.009539512917399406 = 0.002494529355317354 + 0.001 * 7.04498291015625
Epoch 980, val loss: 1.3102736473083496
Epoch 990, training loss: 0.009459733963012695 = 0.002426409162580967 + 0.001 * 7.033324241638184
Epoch 990, val loss: 1.3134571313858032
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.980642318725586 = 1.9720454216003418 + 0.001 * 8.596837997436523
Epoch 0, val loss: 1.971659541130066
Epoch 10, training loss: 1.9690980911254883 = 1.9605013132095337 + 0.001 * 8.596781730651855
Epoch 10, val loss: 1.9597243070602417
Epoch 20, training loss: 1.955143690109253 = 1.9465471506118774 + 0.001 * 8.596592903137207
Epoch 20, val loss: 1.9451221227645874
Epoch 30, training loss: 1.9358617067337036 = 1.9272655248641968 + 0.001 * 8.596144676208496
Epoch 30, val loss: 1.9249852895736694
Epoch 40, training loss: 1.9076005220413208 = 1.89900541305542 + 0.001 * 8.595060348510742
Epoch 40, val loss: 1.8959764242172241
Epoch 50, training loss: 1.8676811456680298 = 1.8590890169143677 + 0.001 * 8.59212875366211
Epoch 50, val loss: 1.8565634489059448
Epoch 60, training loss: 1.8205124139785767 = 1.8119298219680786 + 0.001 * 8.582550048828125
Epoch 60, val loss: 1.8140203952789307
Epoch 70, training loss: 1.779577374458313 = 1.7710362672805786 + 0.001 * 8.541107177734375
Epoch 70, val loss: 1.7810636758804321
Epoch 80, training loss: 1.7381080389022827 = 1.7298550605773926 + 0.001 * 8.253031730651855
Epoch 80, val loss: 1.7447103261947632
Epoch 90, training loss: 1.68124258518219 = 1.6731902360916138 + 0.001 * 8.0523681640625
Epoch 90, val loss: 1.6922402381896973
Epoch 100, training loss: 1.6045883893966675 = 1.5966732501983643 + 0.001 * 7.915161609649658
Epoch 100, val loss: 1.6230124235153198
Epoch 110, training loss: 1.5111454725265503 = 1.503395676612854 + 0.001 * 7.7498459815979
Epoch 110, val loss: 1.5421911478042603
Epoch 120, training loss: 1.411719799041748 = 1.4041664600372314 + 0.001 * 7.55336332321167
Epoch 120, val loss: 1.458330512046814
Epoch 130, training loss: 1.312963843345642 = 1.3054752349853516 + 0.001 * 7.488553047180176
Epoch 130, val loss: 1.3774309158325195
Epoch 140, training loss: 1.2167727947235107 = 1.2093303203582764 + 0.001 * 7.442470550537109
Epoch 140, val loss: 1.3003976345062256
Epoch 150, training loss: 1.124604344367981 = 1.117185115814209 + 0.001 * 7.419271945953369
Epoch 150, val loss: 1.2276649475097656
Epoch 160, training loss: 1.0381509065628052 = 1.030754566192627 + 0.001 * 7.396317481994629
Epoch 160, val loss: 1.160111665725708
Epoch 170, training loss: 0.9578726291656494 = 0.9504938125610352 + 0.001 * 7.378836631774902
Epoch 170, val loss: 1.0979397296905518
Epoch 180, training loss: 0.8827579617500305 = 0.875392496585846 + 0.001 * 7.365470886230469
Epoch 180, val loss: 1.0401499271392822
Epoch 190, training loss: 0.8108057975769043 = 0.8034515380859375 + 0.001 * 7.354275703430176
Epoch 190, val loss: 0.9848061800003052
Epoch 200, training loss: 0.740352213382721 = 0.7330083250999451 + 0.001 * 7.34385871887207
Epoch 200, val loss: 0.9312885403633118
Epoch 210, training loss: 0.6712040305137634 = 0.6638697981834412 + 0.001 * 7.334219455718994
Epoch 210, val loss: 0.8794653415679932
Epoch 220, training loss: 0.6041991710662842 = 0.5968738794326782 + 0.001 * 7.325277328491211
Epoch 220, val loss: 0.8304611444473267
Epoch 230, training loss: 0.5404632091522217 = 0.5331466794013977 + 0.001 * 7.316543102264404
Epoch 230, val loss: 0.7855686545372009
Epoch 240, training loss: 0.48091766238212585 = 0.4736100137233734 + 0.001 * 7.30765438079834
Epoch 240, val loss: 0.746101975440979
Epoch 250, training loss: 0.42637431621551514 = 0.4190754294395447 + 0.001 * 7.298880577087402
Epoch 250, val loss: 0.7131137847900391
Epoch 260, training loss: 0.37753257155418396 = 0.37024158239364624 + 0.001 * 7.290999412536621
Epoch 260, val loss: 0.6870819330215454
Epoch 270, training loss: 0.3347526788711548 = 0.3274673521518707 + 0.001 * 7.285325050354004
Epoch 270, val loss: 0.6676573157310486
Epoch 280, training loss: 0.2976687550544739 = 0.2903893291950226 + 0.001 * 7.279435157775879
Epoch 280, val loss: 0.6540532112121582
Epoch 290, training loss: 0.26523128151893616 = 0.2579561769962311 + 0.001 * 7.275091648101807
Epoch 290, val loss: 0.6450276374816895
Epoch 300, training loss: 0.2361885905265808 = 0.2289174348115921 + 0.001 * 7.271152496337891
Epoch 300, val loss: 0.6395969390869141
Epoch 310, training loss: 0.2095932513475418 = 0.2023257613182068 + 0.001 * 7.267485618591309
Epoch 310, val loss: 0.6369211673736572
Epoch 320, training loss: 0.18508875370025635 = 0.17782224714756012 + 0.001 * 7.266500949859619
Epoch 320, val loss: 0.6364157795906067
Epoch 330, training loss: 0.16281676292419434 = 0.15555299818515778 + 0.001 * 7.263769626617432
Epoch 330, val loss: 0.6377242207527161
Epoch 340, training loss: 0.14300231635570526 = 0.1357429176568985 + 0.001 * 7.259397029876709
Epoch 340, val loss: 0.6405915021896362
Epoch 350, training loss: 0.12570902705192566 = 0.11845186352729797 + 0.001 * 7.257168769836426
Epoch 350, val loss: 0.6448440551757812
Epoch 360, training loss: 0.11078187823295593 = 0.10352781414985657 + 0.001 * 7.254065990447998
Epoch 360, val loss: 0.650255024433136
Epoch 370, training loss: 0.09796004742383957 = 0.09070746600627899 + 0.001 * 7.25258207321167
Epoch 370, val loss: 0.6566439270973206
Epoch 380, training loss: 0.08695293962955475 = 0.07970025390386581 + 0.001 * 7.252682685852051
Epoch 380, val loss: 0.6639279127120972
Epoch 390, training loss: 0.07748997211456299 = 0.07023835182189941 + 0.001 * 7.251621246337891
Epoch 390, val loss: 0.6720868945121765
Epoch 400, training loss: 0.06934910267591476 = 0.06210753694176674 + 0.001 * 7.241565704345703
Epoch 400, val loss: 0.6809341907501221
Epoch 410, training loss: 0.06235314533114433 = 0.05511493980884552 + 0.001 * 7.238204479217529
Epoch 410, val loss: 0.6902428269386292
Epoch 420, training loss: 0.05633202940225601 = 0.04909364879131317 + 0.001 * 7.238381862640381
Epoch 420, val loss: 0.6999446153640747
Epoch 430, training loss: 0.051125530153512955 = 0.04389842599630356 + 0.001 * 7.227104663848877
Epoch 430, val loss: 0.7099451422691345
Epoch 440, training loss: 0.04664042592048645 = 0.03940371051430702 + 0.001 * 7.2367143630981445
Epoch 440, val loss: 0.7200809717178345
Epoch 450, training loss: 0.042725615203380585 = 0.03550278767943382 + 0.001 * 7.222828388214111
Epoch 450, val loss: 0.7302015423774719
Epoch 460, training loss: 0.03931942582130432 = 0.032105181366205215 + 0.001 * 7.214242935180664
Epoch 460, val loss: 0.7403147220611572
Epoch 470, training loss: 0.036345224827528 = 0.029135489836335182 + 0.001 * 7.2097344398498535
Epoch 470, val loss: 0.7503072023391724
Epoch 480, training loss: 0.03373628854751587 = 0.026530997827649117 + 0.001 * 7.205289840698242
Epoch 480, val loss: 0.7601568698883057
Epoch 490, training loss: 0.031431153416633606 = 0.02423923648893833 + 0.001 * 7.19191837310791
Epoch 490, val loss: 0.7698137164115906
Epoch 500, training loss: 0.029436694458127022 = 0.02221599407494068 + 0.001 * 7.220700263977051
Epoch 500, val loss: 0.7792654037475586
Epoch 510, training loss: 0.027602892369031906 = 0.020424408838152885 + 0.001 * 7.178483009338379
Epoch 510, val loss: 0.788469135761261
Epoch 520, training loss: 0.026008838787674904 = 0.018832977861166 + 0.001 * 7.175860404968262
Epoch 520, val loss: 0.7974549531936646
Epoch 530, training loss: 0.024590950459241867 = 0.01741497591137886 + 0.001 * 7.175974369049072
Epoch 530, val loss: 0.8061951398849487
Epoch 540, training loss: 0.02332555688917637 = 0.016147814691066742 + 0.001 * 7.177742004394531
Epoch 540, val loss: 0.814700722694397
Epoch 550, training loss: 0.02217467501759529 = 0.01501219067722559 + 0.001 * 7.162484169006348
Epoch 550, val loss: 0.822969377040863
Epoch 560, training loss: 0.0211679395288229 = 0.01399154681712389 + 0.001 * 7.176392555236816
Epoch 560, val loss: 0.831009566783905
Epoch 570, training loss: 0.020230542868375778 = 0.013071579858660698 + 0.001 * 7.158961772918701
Epoch 570, val loss: 0.8388074636459351
Epoch 580, training loss: 0.019406862556934357 = 0.012239933013916016 + 0.001 * 7.166928768157959
Epoch 580, val loss: 0.8464110493659973
Epoch 590, training loss: 0.01862862892448902 = 0.01148622389882803 + 0.001 * 7.142404079437256
Epoch 590, val loss: 0.8537732362747192
Epoch 600, training loss: 0.01794527843594551 = 0.010801391676068306 + 0.001 * 7.143886089324951
Epoch 600, val loss: 0.8609336018562317
Epoch 610, training loss: 0.01732834056019783 = 0.010177543386816978 + 0.001 * 7.150797367095947
Epoch 610, val loss: 0.8678922653198242
Epoch 620, training loss: 0.016733674332499504 = 0.009607821702957153 + 0.001 * 7.125852584838867
Epoch 620, val loss: 0.8746442794799805
Epoch 630, training loss: 0.016257084906101227 = 0.009086305275559425 + 0.001 * 7.170780181884766
Epoch 630, val loss: 0.8812353610992432
Epoch 640, training loss: 0.0157343540340662 = 0.008607951924204826 + 0.001 * 7.126401901245117
Epoch 640, val loss: 0.8876307606697083
Epoch 650, training loss: 0.015337929129600525 = 0.008168117143213749 + 0.001 * 7.169812202453613
Epoch 650, val loss: 0.8938461542129517
Epoch 660, training loss: 0.014882206916809082 = 0.007762829773128033 + 0.001 * 7.119376182556152
Epoch 660, val loss: 0.8998965620994568
Epoch 670, training loss: 0.014505341649055481 = 0.00738859036937356 + 0.001 * 7.116751194000244
Epoch 670, val loss: 0.90579754114151
Epoch 680, training loss: 0.014154152013361454 = 0.00704212486743927 + 0.001 * 7.112026691436768
Epoch 680, val loss: 0.9115170836448669
Epoch 690, training loss: 0.013908855617046356 = 0.0067206681706011295 + 0.001 * 7.1881866455078125
Epoch 690, val loss: 0.9170911908149719
Epoch 700, training loss: 0.013519215397536755 = 0.00642183143645525 + 0.001 * 7.097383499145508
Epoch 700, val loss: 0.9225171208381653
Epoch 710, training loss: 0.013304239138960838 = 0.006143004633486271 + 0.001 * 7.1612348556518555
Epoch 710, val loss: 0.9278317093849182
Epoch 720, training loss: 0.012972481548786163 = 0.00588270602747798 + 0.001 * 7.089775085449219
Epoch 720, val loss: 0.9330040216445923
Epoch 730, training loss: 0.012729686684906483 = 0.0056390478275716305 + 0.001 * 7.090638637542725
Epoch 730, val loss: 0.9380505084991455
Epoch 740, training loss: 0.012535342015326023 = 0.0054106395691633224 + 0.001 * 7.124701976776123
Epoch 740, val loss: 0.9429819583892822
Epoch 750, training loss: 0.012288019061088562 = 0.005196358542889357 + 0.001 * 7.0916595458984375
Epoch 750, val loss: 0.9478148221969604
Epoch 760, training loss: 0.01207910105586052 = 0.004994985647499561 + 0.001 * 7.084115505218506
Epoch 760, val loss: 0.9525201916694641
Epoch 770, training loss: 0.011904998682439327 = 0.004805551841855049 + 0.001 * 7.0994462966918945
Epoch 770, val loss: 0.9571395516395569
Epoch 780, training loss: 0.011687531135976315 = 0.0046270014718174934 + 0.001 * 7.0605292320251465
Epoch 780, val loss: 0.9616733193397522
Epoch 790, training loss: 0.011529214680194855 = 0.004458809737116098 + 0.001 * 7.070404529571533
Epoch 790, val loss: 0.966117262840271
Epoch 800, training loss: 0.011354507878422737 = 0.004299913067370653 + 0.001 * 7.054594993591309
Epoch 800, val loss: 0.970459520816803
Epoch 810, training loss: 0.011230364441871643 = 0.004149919375777245 + 0.001 * 7.080445289611816
Epoch 810, val loss: 0.9747397899627686
Epoch 820, training loss: 0.011093882843852043 = 0.0040069096721708775 + 0.001 * 7.086973190307617
Epoch 820, val loss: 0.9789429903030396
Epoch 830, training loss: 0.010920600965619087 = 0.00387042504735291 + 0.001 * 7.050175189971924
Epoch 830, val loss: 0.9831721782684326
Epoch 840, training loss: 0.010813074186444283 = 0.00374102545902133 + 0.001 * 7.072047710418701
Epoch 840, val loss: 0.9873566627502441
Epoch 850, training loss: 0.010678104124963284 = 0.0036194417625665665 + 0.001 * 7.058661937713623
Epoch 850, val loss: 0.9915133118629456
Epoch 860, training loss: 0.010584629140794277 = 0.0035044436808675528 + 0.001 * 7.0801849365234375
Epoch 860, val loss: 0.995491087436676
Epoch 870, training loss: 0.010442999191582203 = 0.003395249368622899 + 0.001 * 7.047749042510986
Epoch 870, val loss: 0.9993670582771301
Epoch 880, training loss: 0.010328210890293121 = 0.003291638335213065 + 0.001 * 7.036572456359863
Epoch 880, val loss: 1.0031583309173584
Epoch 890, training loss: 0.01023327186703682 = 0.0031935186125338078 + 0.001 * 7.03975248336792
Epoch 890, val loss: 1.0068458318710327
Epoch 900, training loss: 0.01011685747653246 = 0.0030999835580587387 + 0.001 * 7.016873359680176
Epoch 900, val loss: 1.0105127096176147
Epoch 910, training loss: 0.010055881924927235 = 0.003010866232216358 + 0.001 * 7.045015335083008
Epoch 910, val loss: 1.01410710811615
Epoch 920, training loss: 0.009991873987019062 = 0.002925800858065486 + 0.001 * 7.066072940826416
Epoch 920, val loss: 1.0175639390945435
Epoch 930, training loss: 0.009857306256890297 = 0.002845281269401312 + 0.001 * 7.012024402618408
Epoch 930, val loss: 1.0210498571395874
Epoch 940, training loss: 0.00977560505270958 = 0.0027678925544023514 + 0.001 * 7.0077128410339355
Epoch 940, val loss: 1.0244389772415161
Epoch 950, training loss: 0.00974082201719284 = 0.0026941029354929924 + 0.001 * 7.046719074249268
Epoch 950, val loss: 1.0277525186538696
Epoch 960, training loss: 0.009683795273303986 = 0.0026240264996886253 + 0.001 * 7.0597686767578125
Epoch 960, val loss: 1.0310587882995605
Epoch 970, training loss: 0.009555315598845482 = 0.0025568592827767134 + 0.001 * 6.99845552444458
Epoch 970, val loss: 1.0341928005218506
Epoch 980, training loss: 0.009509454481303692 = 0.002492703264579177 + 0.001 * 7.016751289367676
Epoch 980, val loss: 1.037354588508606
Epoch 990, training loss: 0.00944824144244194 = 0.0024312876630574465 + 0.001 * 7.016953468322754
Epoch 990, val loss: 1.0404038429260254
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 1.9458280801773071 = 1.9372313022613525 + 0.001 * 8.596813201904297
Epoch 0, val loss: 1.9288181066513062
Epoch 10, training loss: 1.9361603260040283 = 1.9275635480880737 + 0.001 * 8.596722602844238
Epoch 10, val loss: 1.9194525480270386
Epoch 20, training loss: 1.9238526821136475 = 1.915256142616272 + 0.001 * 8.59648609161377
Epoch 20, val loss: 1.9069995880126953
Epoch 30, training loss: 1.9062527418136597 = 1.897656798362732 + 0.001 * 8.595973014831543
Epoch 30, val loss: 1.8888639211654663
Epoch 40, training loss: 1.8800134658813477 = 1.8714187145233154 + 0.001 * 8.594768524169922
Epoch 40, val loss: 1.862294316291809
Epoch 50, training loss: 1.8437058925628662 = 1.8351147174835205 + 0.001 * 8.591116905212402
Epoch 50, val loss: 1.8277136087417603
Epoch 60, training loss: 1.8034645318984985 = 1.7948877811431885 + 0.001 * 8.576711654663086
Epoch 60, val loss: 1.794032335281372
Epoch 70, training loss: 1.7662897109985352 = 1.7577804327011108 + 0.001 * 8.509302139282227
Epoch 70, val loss: 1.7648454904556274
Epoch 80, training loss: 1.7167282104492188 = 1.7085723876953125 + 0.001 * 8.155818939208984
Epoch 80, val loss: 1.7210370302200317
Epoch 90, training loss: 1.6483595371246338 = 1.6403894424438477 + 0.001 * 7.970059394836426
Epoch 90, val loss: 1.6606463193893433
Epoch 100, training loss: 1.5597107410430908 = 1.5518686771392822 + 0.001 * 7.842085361480713
Epoch 100, val loss: 1.5861021280288696
Epoch 110, training loss: 1.4577999114990234 = 1.4501210451126099 + 0.001 * 7.678904056549072
Epoch 110, val loss: 1.5003262758255005
Epoch 120, training loss: 1.354003667831421 = 1.346389651298523 + 0.001 * 7.614013671875
Epoch 120, val loss: 1.4161945581436157
Epoch 130, training loss: 1.2533544301986694 = 1.2457369565963745 + 0.001 * 7.617478847503662
Epoch 130, val loss: 1.3356651067733765
Epoch 140, training loss: 1.1576193571090698 = 1.1500201225280762 + 0.001 * 7.599257469177246
Epoch 140, val loss: 1.2610280513763428
Epoch 150, training loss: 1.0688316822052002 = 1.0612459182739258 + 0.001 * 7.585790157318115
Epoch 150, val loss: 1.1944446563720703
Epoch 160, training loss: 0.9877698421478271 = 0.9802042841911316 + 0.001 * 7.565563678741455
Epoch 160, val loss: 1.1356934309005737
Epoch 170, training loss: 0.9130011796951294 = 0.9054663181304932 + 0.001 * 7.534859657287598
Epoch 170, val loss: 1.0825494527816772
Epoch 180, training loss: 0.8427338004112244 = 0.8352475166320801 + 0.001 * 7.486295700073242
Epoch 180, val loss: 1.0324169397354126
Epoch 190, training loss: 0.776518702507019 = 0.7691026329994202 + 0.001 * 7.416077613830566
Epoch 190, val loss: 0.9845848083496094
Epoch 200, training loss: 0.7148731350898743 = 0.7075095176696777 + 0.001 * 7.363608360290527
Epoch 200, val loss: 0.9402241110801697
Epoch 210, training loss: 0.6580947041511536 = 0.6507512927055359 + 0.001 * 7.343421459197998
Epoch 210, val loss: 0.9005958437919617
Epoch 220, training loss: 0.6057247519493103 = 0.5983899235725403 + 0.001 * 7.334810733795166
Epoch 220, val loss: 0.8664511442184448
Epoch 230, training loss: 0.5569301843643188 = 0.5496006011962891 + 0.001 * 7.329577922821045
Epoch 230, val loss: 0.8382192254066467
Epoch 240, training loss: 0.5110195875167847 = 0.5036951303482056 + 0.001 * 7.3244805335998535
Epoch 240, val loss: 0.8156975507736206
Epoch 250, training loss: 0.4672437608242035 = 0.4599277973175049 + 0.001 * 7.315976142883301
Epoch 250, val loss: 0.7980627417564392
Epoch 260, training loss: 0.424928218126297 = 0.4176212251186371 + 0.001 * 7.3070068359375
Epoch 260, val loss: 0.7842045426368713
Epoch 270, training loss: 0.38369372487068176 = 0.3763986825942993 + 0.001 * 7.295034408569336
Epoch 270, val loss: 0.77334064245224
Epoch 280, training loss: 0.34367021918296814 = 0.3363908529281616 + 0.001 * 7.279374599456787
Epoch 280, val loss: 0.7650899887084961
Epoch 290, training loss: 0.3054294288158417 = 0.2981584966182709 + 0.001 * 7.270941734313965
Epoch 290, val loss: 0.7597412467002869
Epoch 300, training loss: 0.2697581946849823 = 0.2625125050544739 + 0.001 * 7.245696067810059
Epoch 300, val loss: 0.7577770948410034
Epoch 310, training loss: 0.237458735704422 = 0.23021362721920013 + 0.001 * 7.245100975036621
Epoch 310, val loss: 0.759304404258728
Epoch 320, training loss: 0.20891717076301575 = 0.20170380175113678 + 0.001 * 7.213366508483887
Epoch 320, val loss: 0.7642442584037781
Epoch 330, training loss: 0.18417006731033325 = 0.17698317766189575 + 0.001 * 7.186891078948975
Epoch 330, val loss: 0.7723292708396912
Epoch 340, training loss: 0.16292068362236023 = 0.15574043989181519 + 0.001 * 7.1802449226379395
Epoch 340, val loss: 0.7833523750305176
Epoch 350, training loss: 0.14470010995864868 = 0.13754284381866455 + 0.001 * 7.157270431518555
Epoch 350, val loss: 0.7966859340667725
Epoch 360, training loss: 0.1291019171476364 = 0.12194811552762985 + 0.001 * 7.15379524230957
Epoch 360, val loss: 0.8117859959602356
Epoch 370, training loss: 0.11568620800971985 = 0.10854240506887436 + 0.001 * 7.143799781799316
Epoch 370, val loss: 0.8281985521316528
Epoch 380, training loss: 0.10409898310899734 = 0.09696203470230103 + 0.001 * 7.13694953918457
Epoch 380, val loss: 0.8454552292823792
Epoch 390, training loss: 0.09402385354042053 = 0.08689717948436737 + 0.001 * 7.126670837402344
Epoch 390, val loss: 0.8632721900939941
Epoch 400, training loss: 0.08522641658782959 = 0.07810280472040176 + 0.001 * 7.1236090660095215
Epoch 400, val loss: 0.8814688920974731
Epoch 410, training loss: 0.07750581204891205 = 0.07038440555334091 + 0.001 * 7.1214094161987305
Epoch 410, val loss: 0.8998532891273499
Epoch 420, training loss: 0.0706905871629715 = 0.06358589231967926 + 0.001 * 7.104692459106445
Epoch 420, val loss: 0.9182921648025513
Epoch 430, training loss: 0.06469215452671051 = 0.05758244916796684 + 0.001 * 7.109701633453369
Epoch 430, val loss: 0.9366987347602844
Epoch 440, training loss: 0.059375472366809845 = 0.05226927623152733 + 0.001 * 7.106195449829102
Epoch 440, val loss: 0.9549658894538879
Epoch 450, training loss: 0.05466073378920555 = 0.04755927994847298 + 0.001 * 7.101452827453613
Epoch 450, val loss: 0.9730207920074463
Epoch 460, training loss: 0.05047071725130081 = 0.043376948684453964 + 0.001 * 7.0937700271606445
Epoch 460, val loss: 0.9908178448677063
Epoch 470, training loss: 0.0467720627784729 = 0.039655882865190506 + 0.001 * 7.116179943084717
Epoch 470, val loss: 1.008307695388794
Epoch 480, training loss: 0.04343121126294136 = 0.036338839679956436 + 0.001 * 7.092371463775635
Epoch 480, val loss: 1.0254665613174438
Epoch 490, training loss: 0.04046653211116791 = 0.03337516263127327 + 0.001 * 7.09136962890625
Epoch 490, val loss: 1.0422636270523071
Epoch 500, training loss: 0.03781026601791382 = 0.030721696093678474 + 0.001 * 7.088571548461914
Epoch 500, val loss: 1.0587128400802612
Epoch 510, training loss: 0.0354287251830101 = 0.028341202065348625 + 0.001 * 7.087522029876709
Epoch 510, val loss: 1.074806571006775
Epoch 520, training loss: 0.03329276293516159 = 0.02620137855410576 + 0.001 * 7.091384410858154
Epoch 520, val loss: 1.0905508995056152
Epoch 530, training loss: 0.03135925903916359 = 0.024274073541164398 + 0.001 * 7.085184574127197
Epoch 530, val loss: 1.1059027910232544
Epoch 540, training loss: 0.029616007581353188 = 0.022534910589456558 + 0.001 * 7.081096172332764
Epoch 540, val loss: 1.1208738088607788
Epoch 550, training loss: 0.028042195364832878 = 0.020962674170732498 + 0.001 * 7.079521179199219
Epoch 550, val loss: 1.1354560852050781
Epoch 560, training loss: 0.026624415069818497 = 0.019538618624210358 + 0.001 * 7.085795879364014
Epoch 560, val loss: 1.1496527194976807
Epoch 570, training loss: 0.025334294885396957 = 0.01824646070599556 + 0.001 * 7.08783483505249
Epoch 570, val loss: 1.1634836196899414
Epoch 580, training loss: 0.024154625833034515 = 0.01707187481224537 + 0.001 * 7.08275032043457
Epoch 580, val loss: 1.1769095659255981
Epoch 590, training loss: 0.023076683282852173 = 0.016002211719751358 + 0.001 * 7.074470520019531
Epoch 590, val loss: 1.1899850368499756
Epoch 600, training loss: 0.022120043635368347 = 0.015026471577584743 + 0.001 * 7.093571186065674
Epoch 600, val loss: 1.2027016878128052
Epoch 610, training loss: 0.021219339221715927 = 0.014134763740003109 + 0.001 * 7.084575653076172
Epoch 610, val loss: 1.2150596380233765
Epoch 620, training loss: 0.020398003980517387 = 0.013318318873643875 + 0.001 * 7.079684734344482
Epoch 620, val loss: 1.2270859479904175
Epoch 630, training loss: 0.019644256681203842 = 0.012569500133395195 + 0.001 * 7.0747551918029785
Epoch 630, val loss: 1.238775610923767
Epoch 640, training loss: 0.018949583172798157 = 0.011881516315042973 + 0.001 * 7.068066596984863
Epoch 640, val loss: 1.250144124031067
Epoch 650, training loss: 0.018321046605706215 = 0.011248350143432617 + 0.001 * 7.072696685791016
Epoch 650, val loss: 1.2611972093582153
Epoch 660, training loss: 0.017735790461301804 = 0.010664649307727814 + 0.001 * 7.071139812469482
Epoch 660, val loss: 1.2719541788101196
Epoch 670, training loss: 0.017193056643009186 = 0.010125608183443546 + 0.001 * 7.06744909286499
Epoch 670, val loss: 1.282402753829956
Epoch 680, training loss: 0.01670827716588974 = 0.009626995772123337 + 0.001 * 7.08128023147583
Epoch 680, val loss: 1.2925636768341064
Epoch 690, training loss: 0.01623770222067833 = 0.009165097959339619 + 0.001 * 7.072603225708008
Epoch 690, val loss: 1.3024561405181885
Epoch 700, training loss: 0.015801846981048584 = 0.008736545220017433 + 0.001 * 7.065300941467285
Epoch 700, val loss: 1.3120899200439453
Epoch 710, training loss: 0.015399678610265255 = 0.008338352665305138 + 0.001 * 7.061325550079346
Epoch 710, val loss: 1.3214476108551025
Epoch 720, training loss: 0.015031613409519196 = 0.007967794314026833 + 0.001 * 7.06381893157959
Epoch 720, val loss: 1.330558180809021
Epoch 730, training loss: 0.014680569991469383 = 0.007622444536536932 + 0.001 * 7.058125019073486
Epoch 730, val loss: 1.3394452333450317
Epoch 740, training loss: 0.014357564970850945 = 0.007300196681171656 + 0.001 * 7.057367324829102
Epoch 740, val loss: 1.34809410572052
Epoch 750, training loss: 0.01405539084225893 = 0.0069990744814276695 + 0.001 * 7.056315898895264
Epoch 750, val loss: 1.3565129041671753
Epoch 760, training loss: 0.013774832710623741 = 0.006717294920235872 + 0.001 * 7.05753755569458
Epoch 760, val loss: 1.3647100925445557
Epoch 770, training loss: 0.013510407879948616 = 0.006453267764300108 + 0.001 * 7.0571393966674805
Epoch 770, val loss: 1.372702717781067
Epoch 780, training loss: 0.013258963823318481 = 0.00620557414367795 + 0.001 * 7.053389072418213
Epoch 780, val loss: 1.380503535270691
Epoch 790, training loss: 0.013027545064687729 = 0.005972931161522865 + 0.001 * 7.0546135902404785
Epoch 790, val loss: 1.3881038427352905
Epoch 800, training loss: 0.012806828133761883 = 0.005754161160439253 + 0.001 * 7.052666664123535
Epoch 800, val loss: 1.3955222368240356
Epoch 810, training loss: 0.012602020055055618 = 0.0055481987074017525 + 0.001 * 7.053820610046387
Epoch 810, val loss: 1.4027597904205322
Epoch 820, training loss: 0.012408709153532982 = 0.005354058463126421 + 0.001 * 7.054650783538818
Epoch 820, val loss: 1.4098256826400757
Epoch 830, training loss: 0.012221161276102066 = 0.005170868709683418 + 0.001 * 7.050292015075684
Epoch 830, val loss: 1.416728138923645
Epoch 840, training loss: 0.01206339430063963 = 0.004997854586690664 + 0.001 * 7.065539360046387
Epoch 840, val loss: 1.4234673976898193
Epoch 850, training loss: 0.011886253952980042 = 0.004834281746298075 + 0.001 * 7.051972389221191
Epoch 850, val loss: 1.4300408363342285
Epoch 860, training loss: 0.01173129491508007 = 0.004679477773606777 + 0.001 * 7.051817417144775
Epoch 860, val loss: 1.4364700317382812
Epoch 870, training loss: 0.01158151589334011 = 0.004532842896878719 + 0.001 * 7.048672199249268
Epoch 870, val loss: 1.442757487297058
Epoch 880, training loss: 0.01144612766802311 = 0.004393809475004673 + 0.001 * 7.052318096160889
Epoch 880, val loss: 1.4488933086395264
Epoch 890, training loss: 0.011312589049339294 = 0.00426188251003623 + 0.001 * 7.050705909729004
Epoch 890, val loss: 1.4548866748809814
Epoch 900, training loss: 0.011177372187376022 = 0.004136587493121624 + 0.001 * 7.04078483581543
Epoch 900, val loss: 1.4607685804367065
Epoch 910, training loss: 0.011065170168876648 = 0.004017482977360487 + 0.001 * 7.047687530517578
Epoch 910, val loss: 1.4665093421936035
Epoch 920, training loss: 0.010940849781036377 = 0.003904172917827964 + 0.001 * 7.036676406860352
Epoch 920, val loss: 1.4721227884292603
Epoch 930, training loss: 0.010830521583557129 = 0.003796287579461932 + 0.001 * 7.034233570098877
Epoch 930, val loss: 1.4776203632354736
Epoch 940, training loss: 0.010746362619102001 = 0.0036934989038854837 + 0.001 * 7.052863121032715
Epoch 940, val loss: 1.4829837083816528
Epoch 950, training loss: 0.010632195509970188 = 0.003595480928197503 + 0.001 * 7.03671407699585
Epoch 950, val loss: 1.4882491827011108
Epoch 960, training loss: 0.010534335859119892 = 0.0035019495990127325 + 0.001 * 7.032386302947998
Epoch 960, val loss: 1.4934114217758179
Epoch 970, training loss: 0.010448688641190529 = 0.0034126359969377518 + 0.001 * 7.0360517501831055
Epoch 970, val loss: 1.4984545707702637
Epoch 980, training loss: 0.010358271189033985 = 0.0033272826112806797 + 0.001 * 7.0309882164001465
Epoch 980, val loss: 1.5033953189849854
Epoch 990, training loss: 0.010271284729242325 = 0.0032456545159220695 + 0.001 * 7.025630474090576
Epoch 990, val loss: 1.5082424879074097
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8397469688982605
The final CL Acc:0.79753, 0.00630, The final GNN Acc:0.83729, 0.00245
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10630])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.956943154335022 = 1.9483463764190674 + 0.001 * 8.596829414367676
Epoch 0, val loss: 1.9529812335968018
Epoch 10, training loss: 1.9469528198242188 = 1.9383560419082642 + 0.001 * 8.596793174743652
Epoch 10, val loss: 1.942780613899231
Epoch 20, training loss: 1.9348366260528564 = 1.9262399673461914 + 0.001 * 8.596640586853027
Epoch 20, val loss: 1.93026864528656
Epoch 30, training loss: 1.9181227684020996 = 1.9095264673233032 + 0.001 * 8.596263885498047
Epoch 30, val loss: 1.9131442308425903
Epoch 40, training loss: 1.8938971757888794 = 1.8853018283843994 + 0.001 * 8.5953369140625
Epoch 40, val loss: 1.8886231184005737
Epoch 50, training loss: 1.8605074882507324 = 1.851914644241333 + 0.001 * 8.592790603637695
Epoch 50, val loss: 1.8560410737991333
Epoch 60, training loss: 1.823380947113037 = 1.8147966861724854 + 0.001 * 8.584318161010742
Epoch 60, val loss: 1.8226665258407593
Epoch 70, training loss: 1.793073058128357 = 1.7845230102539062 + 0.001 * 8.550081253051758
Epoch 70, val loss: 1.7962265014648438
Epoch 80, training loss: 1.7577930688858032 = 1.749447226524353 + 0.001 * 8.34587287902832
Epoch 80, val loss: 1.7618385553359985
Epoch 90, training loss: 1.7071455717086792 = 1.6990278959274292 + 0.001 * 8.117632865905762
Epoch 90, val loss: 1.7164088487625122
Epoch 100, training loss: 1.6350253820419312 = 1.6270062923431396 + 0.001 * 8.019121170043945
Epoch 100, val loss: 1.655103325843811
Epoch 110, training loss: 1.5452920198440552 = 1.5373220443725586 + 0.001 * 7.969968795776367
Epoch 110, val loss: 1.581309199333191
Epoch 120, training loss: 1.4518460035324097 = 1.4439387321472168 + 0.001 * 7.907219409942627
Epoch 120, val loss: 1.5063822269439697
Epoch 130, training loss: 1.362205147743225 = 1.3544796705245972 + 0.001 * 7.72549295425415
Epoch 130, val loss: 1.437296986579895
Epoch 140, training loss: 1.2765189409255981 = 1.26901376247406 + 0.001 * 7.505137920379639
Epoch 140, val loss: 1.3746585845947266
Epoch 150, training loss: 1.1939688920974731 = 1.186488389968872 + 0.001 * 7.48046350479126
Epoch 150, val loss: 1.3164933919906616
Epoch 160, training loss: 1.1126574277877808 = 1.1051939725875854 + 0.001 * 7.46340274810791
Epoch 160, val loss: 1.2603715658187866
Epoch 170, training loss: 1.0303374528884888 = 1.0228830575942993 + 0.001 * 7.454366683959961
Epoch 170, val loss: 1.2032592296600342
Epoch 180, training loss: 0.9466636776924133 = 0.9392139315605164 + 0.001 * 7.44974422454834
Epoch 180, val loss: 1.144566297531128
Epoch 190, training loss: 0.8645968437194824 = 0.8571513891220093 + 0.001 * 7.445455074310303
Epoch 190, val loss: 1.085841417312622
Epoch 200, training loss: 0.7885000109672546 = 0.7810588479042053 + 0.001 * 7.441173076629639
Epoch 200, val loss: 1.0315101146697998
Epoch 210, training loss: 0.7210678458213806 = 0.7136319875717163 + 0.001 * 7.435873985290527
Epoch 210, val loss: 0.9843754768371582
Epoch 220, training loss: 0.6617045998573303 = 0.6542758345603943 + 0.001 * 7.428745269775391
Epoch 220, val loss: 0.9446977972984314
Epoch 230, training loss: 0.607659101486206 = 0.6002398729324341 + 0.001 * 7.419217109680176
Epoch 230, val loss: 0.9109697937965393
Epoch 240, training loss: 0.5560984015464783 = 0.5486918687820435 + 0.001 * 7.406510829925537
Epoch 240, val loss: 0.8818598389625549
Epoch 250, training loss: 0.5054439902305603 = 0.49805450439453125 + 0.001 * 7.38950252532959
Epoch 250, val loss: 0.8566147089004517
Epoch 260, training loss: 0.45546773076057434 = 0.4481048882007599 + 0.001 * 7.362848281860352
Epoch 260, val loss: 0.8354254364967346
Epoch 270, training loss: 0.4067526161670685 = 0.3994107246398926 + 0.001 * 7.341883182525635
Epoch 270, val loss: 0.8184253573417664
Epoch 280, training loss: 0.36022600531578064 = 0.3529118597507477 + 0.001 * 7.314158916473389
Epoch 280, val loss: 0.8057212829589844
Epoch 290, training loss: 0.31707701086997986 = 0.3097790777683258 + 0.001 * 7.297924995422363
Epoch 290, val loss: 0.7968767285346985
Epoch 300, training loss: 0.27814510464668274 = 0.27085572481155396 + 0.001 * 7.289391994476318
Epoch 300, val loss: 0.7918354868888855
Epoch 310, training loss: 0.24371913075447083 = 0.23643073439598083 + 0.001 * 7.288393020629883
Epoch 310, val loss: 0.7903462648391724
Epoch 320, training loss: 0.21361595392227173 = 0.20633016526699066 + 0.001 * 7.285781383514404
Epoch 320, val loss: 0.7918534278869629
Epoch 330, training loss: 0.18744459748268127 = 0.18015924096107483 + 0.001 * 7.285355091094971
Epoch 330, val loss: 0.7957552075386047
Epoch 340, training loss: 0.16471229493618011 = 0.15742743015289307 + 0.001 * 7.284860610961914
Epoch 340, val loss: 0.8017714023590088
Epoch 350, training loss: 0.1449735015630722 = 0.1376858353614807 + 0.001 * 7.287664413452148
Epoch 350, val loss: 0.8094354271888733
Epoch 360, training loss: 0.12783044576644897 = 0.12054608017206192 + 0.001 * 7.284367084503174
Epoch 360, val loss: 0.8185256719589233
Epoch 370, training loss: 0.11296799033880234 = 0.10568355023860931 + 0.001 * 7.284437656402588
Epoch 370, val loss: 0.8288647532463074
Epoch 380, training loss: 0.1001032218337059 = 0.09281951189041138 + 0.001 * 7.283707141876221
Epoch 380, val loss: 0.840059220790863
Epoch 390, training loss: 0.08899048715829849 = 0.08170747011899948 + 0.001 * 7.283013820648193
Epoch 390, val loss: 0.8519254326820374
Epoch 400, training loss: 0.07940579950809479 = 0.07212318480014801 + 0.001 * 7.282618045806885
Epoch 400, val loss: 0.8643067479133606
Epoch 410, training loss: 0.07114363461732864 = 0.06386290490627289 + 0.001 * 7.280730724334717
Epoch 410, val loss: 0.8769541382789612
Epoch 420, training loss: 0.06403498351573944 = 0.056745003908872604 + 0.001 * 7.289977073669434
Epoch 420, val loss: 0.8897852301597595
Epoch 430, training loss: 0.057882171124219894 = 0.05060415342450142 + 0.001 * 7.278016567230225
Epoch 430, val loss: 0.902550458908081
Epoch 440, training loss: 0.05257045477628708 = 0.04529719799757004 + 0.001 * 7.273257255554199
Epoch 440, val loss: 0.9152587652206421
Epoch 450, training loss: 0.047973405569791794 = 0.04070034995675087 + 0.001 * 7.273055076599121
Epoch 450, val loss: 0.9278178215026855
Epoch 460, training loss: 0.043974995613098145 = 0.03670855239033699 + 0.001 * 7.266441345214844
Epoch 460, val loss: 0.940201461315155
Epoch 470, training loss: 0.04049147292971611 = 0.03323125094175339 + 0.001 * 7.260221481323242
Epoch 470, val loss: 0.9523146152496338
Epoch 480, training loss: 0.03746576979756355 = 0.030192522332072258 + 0.001 * 7.2732462882995605
Epoch 480, val loss: 0.964123547077179
Epoch 490, training loss: 0.03478517755866051 = 0.027528192847967148 + 0.001 * 7.256984233856201
Epoch 490, val loss: 0.975651741027832
Epoch 500, training loss: 0.03243513032793999 = 0.025184165686368942 + 0.001 * 7.250962734222412
Epoch 500, val loss: 0.9868939518928528
Epoch 510, training loss: 0.030368175357580185 = 0.023115191608667374 + 0.001 * 7.252984523773193
Epoch 510, val loss: 0.9978100657463074
Epoch 520, training loss: 0.02852785587310791 = 0.021282490342855453 + 0.001 * 7.245365142822266
Epoch 520, val loss: 1.0084480047225952
Epoch 530, training loss: 0.026885870844125748 = 0.01965359039604664 + 0.001 * 7.23228120803833
Epoch 530, val loss: 1.0187691450119019
Epoch 540, training loss: 0.02543514035642147 = 0.01820102334022522 + 0.001 * 7.234116077423096
Epoch 540, val loss: 1.0287864208221436
Epoch 550, training loss: 0.02413710579276085 = 0.016901662573218346 + 0.001 * 7.235442161560059
Epoch 550, val loss: 1.0385583639144897
Epoch 560, training loss: 0.022967832162976265 = 0.015735790133476257 + 0.001 * 7.23204231262207
Epoch 560, val loss: 1.0479999780654907
Epoch 570, training loss: 0.021904049441218376 = 0.014686333946883678 + 0.001 * 7.217714786529541
Epoch 570, val loss: 1.0571919679641724
Epoch 580, training loss: 0.02096456103026867 = 0.013738816604018211 + 0.001 * 7.225743770599365
Epoch 580, val loss: 1.0661042928695679
Epoch 590, training loss: 0.02010262757539749 = 0.012879774905741215 + 0.001 * 7.222851753234863
Epoch 590, val loss: 1.0748056173324585
Epoch 600, training loss: 0.01931069977581501 = 0.012098219245672226 + 0.001 * 7.212480068206787
Epoch 600, val loss: 1.0832726955413818
Epoch 610, training loss: 0.018597956746816635 = 0.01138446293771267 + 0.001 * 7.213493347167969
Epoch 610, val loss: 1.0915685892105103
Epoch 620, training loss: 0.017944496124982834 = 0.010730499401688576 + 0.001 * 7.2139973640441895
Epoch 620, val loss: 1.0996500253677368
Epoch 630, training loss: 0.017352033406496048 = 0.01012950949370861 + 0.001 * 7.222523212432861
Epoch 630, val loss: 1.1075794696807861
Epoch 640, training loss: 0.016785919666290283 = 0.009576153941452503 + 0.001 * 7.20976448059082
Epoch 640, val loss: 1.1153291463851929
Epoch 650, training loss: 0.016271641477942467 = 0.009065443649888039 + 0.001 * 7.206197261810303
Epoch 650, val loss: 1.1228941679000854
Epoch 660, training loss: 0.015797214582562447 = 0.008593741804361343 + 0.001 * 7.203473091125488
Epoch 660, val loss: 1.1303002834320068
Epoch 670, training loss: 0.015357090160250664 = 0.008157486096024513 + 0.001 * 7.199603080749512
Epoch 670, val loss: 1.1375471353530884
Epoch 680, training loss: 0.014949695207178593 = 0.007753498386591673 + 0.001 * 7.196196556091309
Epoch 680, val loss: 1.1446239948272705
Epoch 690, training loss: 0.014582387171685696 = 0.007378863636404276 + 0.001 * 7.2035231590271
Epoch 690, val loss: 1.1515580415725708
Epoch 700, training loss: 0.014221355319023132 = 0.007031038869172335 + 0.001 * 7.190316200256348
Epoch 700, val loss: 1.158333659172058
Epoch 710, training loss: 0.013898996636271477 = 0.0067076487466692924 + 0.001 * 7.191347599029541
Epoch 710, val loss: 1.16494619846344
Epoch 720, training loss: 0.013621697202324867 = 0.006406641099601984 + 0.001 * 7.215055465698242
Epoch 720, val loss: 1.1714015007019043
Epoch 730, training loss: 0.013315826654434204 = 0.006126271095126867 + 0.001 * 7.1895551681518555
Epoch 730, val loss: 1.1776961088180542
Epoch 740, training loss: 0.013050124980509281 = 0.005864689592272043 + 0.001 * 7.185434818267822
Epoch 740, val loss: 1.1838682889938354
Epoch 750, training loss: 0.012804599478840828 = 0.005620250012725592 + 0.001 * 7.184349536895752
Epoch 750, val loss: 1.189880609512329
Epoch 760, training loss: 0.012577220797538757 = 0.005391390062868595 + 0.001 * 7.185831069946289
Epoch 760, val loss: 1.1957803964614868
Epoch 770, training loss: 0.012404954992234707 = 0.005176641047000885 + 0.001 * 7.228313446044922
Epoch 770, val loss: 1.2015477418899536
Epoch 780, training loss: 0.012164948508143425 = 0.004974802490323782 + 0.001 * 7.190145492553711
Epoch 780, val loss: 1.207230567932129
Epoch 790, training loss: 0.011961067095398903 = 0.004784206859767437 + 0.001 * 7.1768598556518555
Epoch 790, val loss: 1.212825059890747
Epoch 800, training loss: 0.01178375817835331 = 0.004603556357324123 + 0.001 * 7.180201053619385
Epoch 800, val loss: 1.218392014503479
Epoch 810, training loss: 0.011604567989706993 = 0.004431793931871653 + 0.001 * 7.172774314880371
Epoch 810, val loss: 1.2239426374435425
Epoch 820, training loss: 0.011441300623118877 = 0.004268303979188204 + 0.001 * 7.172996520996094
Epoch 820, val loss: 1.2294780015945435
Epoch 830, training loss: 0.011286331340670586 = 0.004112614784389734 + 0.001 * 7.173716068267822
Epoch 830, val loss: 1.2350093126296997
Epoch 840, training loss: 0.0111930463463068 = 0.003964471165090799 + 0.001 * 7.228575229644775
Epoch 840, val loss: 1.2404911518096924
Epoch 850, training loss: 0.010994533076882362 = 0.003824096405878663 + 0.001 * 7.170435905456543
Epoch 850, val loss: 1.2459782361984253
Epoch 860, training loss: 0.010870016179978848 = 0.0036908502224832773 + 0.001 * 7.179165840148926
Epoch 860, val loss: 1.2513490915298462
Epoch 870, training loss: 0.010741723701357841 = 0.0035645298194140196 + 0.001 * 7.1771931648254395
Epoch 870, val loss: 1.2567209005355835
Epoch 880, training loss: 0.010616801679134369 = 0.0034447340294718742 + 0.001 * 7.172066688537598
Epoch 880, val loss: 1.2620131969451904
Epoch 890, training loss: 0.010488014668226242 = 0.003331208834424615 + 0.001 * 7.156805038452148
Epoch 890, val loss: 1.2672401666641235
Epoch 900, training loss: 0.010379495099186897 = 0.0032237297855317593 + 0.001 * 7.155765056610107
Epoch 900, val loss: 1.2723591327667236
Epoch 910, training loss: 0.010294224135577679 = 0.003122031921520829 + 0.001 * 7.172191619873047
Epoch 910, val loss: 1.277407169342041
Epoch 920, training loss: 0.01018261257559061 = 0.0030257811304181814 + 0.001 * 7.15683126449585
Epoch 920, val loss: 1.2823288440704346
Epoch 930, training loss: 0.01008575689047575 = 0.0029345464427024126 + 0.001 * 7.151209831237793
Epoch 930, val loss: 1.2871750593185425
Epoch 940, training loss: 0.010003591887652874 = 0.002847996773198247 + 0.001 * 7.155594825744629
Epoch 940, val loss: 1.2918996810913086
Epoch 950, training loss: 0.009928589686751366 = 0.0027658238541334867 + 0.001 * 7.1627655029296875
Epoch 950, val loss: 1.2965630292892456
Epoch 960, training loss: 0.009835644625127316 = 0.0026877818163484335 + 0.001 * 7.147862434387207
Epoch 960, val loss: 1.30112624168396
Epoch 970, training loss: 0.009807014837861061 = 0.0026135591324418783 + 0.001 * 7.193455696105957
Epoch 970, val loss: 1.305591344833374
Epoch 980, training loss: 0.009698588401079178 = 0.0025430864188820124 + 0.001 * 7.155501365661621
Epoch 980, val loss: 1.3099669218063354
Epoch 990, training loss: 0.009617474861443043 = 0.002475976711139083 + 0.001 * 7.14149808883667
Epoch 990, val loss: 1.314226746559143
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 1.9529669284820557 = 1.9443700313568115 + 0.001 * 8.596843719482422
Epoch 0, val loss: 1.9491832256317139
Epoch 10, training loss: 1.9427751302719116 = 1.934178352355957 + 0.001 * 8.59677505493164
Epoch 10, val loss: 1.938507080078125
Epoch 20, training loss: 1.9298690557479858 = 1.9212725162506104 + 0.001 * 8.596595764160156
Epoch 20, val loss: 1.924870491027832
Epoch 30, training loss: 1.9114798307418823 = 1.9028836488723755 + 0.001 * 8.596179962158203
Epoch 30, val loss: 1.9054615497589111
Epoch 40, training loss: 1.884469747543335 = 1.8758745193481445 + 0.001 * 8.595200538635254
Epoch 40, val loss: 1.8772953748703003
Epoch 50, training loss: 1.8486353158950806 = 1.8400425910949707 + 0.001 * 8.592720031738281
Epoch 50, val loss: 1.8416801691055298
Epoch 60, training loss: 1.814233660697937 = 1.8056485652923584 + 0.001 * 8.585129737854004
Epoch 60, val loss: 1.8110684156417847
Epoch 70, training loss: 1.786789059638977 = 1.7782326936721802 + 0.001 * 8.556327819824219
Epoch 70, val loss: 1.7869237661361694
Epoch 80, training loss: 1.7493966817855835 = 1.7410240173339844 + 0.001 * 8.372715950012207
Epoch 80, val loss: 1.7534562349319458
Epoch 90, training loss: 1.6975599527359009 = 1.6893742084503174 + 0.001 * 8.185749053955078
Epoch 90, val loss: 1.709077000617981
Epoch 100, training loss: 1.6256201267242432 = 1.6175273656845093 + 0.001 * 8.092702865600586
Epoch 100, val loss: 1.6486657857894897
Epoch 110, training loss: 1.5401279926300049 = 1.5321234464645386 + 0.001 * 8.004602432250977
Epoch 110, val loss: 1.5798872709274292
Epoch 120, training loss: 1.4517570734024048 = 1.443909764289856 + 0.001 * 7.847296714782715
Epoch 120, val loss: 1.5114140510559082
Epoch 130, training loss: 1.3645637035369873 = 1.35684072971344 + 0.001 * 7.722955226898193
Epoch 130, val loss: 1.4454165697097778
Epoch 140, training loss: 1.2758952379226685 = 1.2681894302368164 + 0.001 * 7.705791473388672
Epoch 140, val loss: 1.3787355422973633
Epoch 150, training loss: 1.1845201253890991 = 1.1768821477890015 + 0.001 * 7.637922286987305
Epoch 150, val loss: 1.3098394870758057
Epoch 160, training loss: 1.0936379432678223 = 1.0860893726348877 + 0.001 * 7.548607349395752
Epoch 160, val loss: 1.2416138648986816
Epoch 170, training loss: 1.007577657699585 = 1.0000708103179932 + 0.001 * 7.506885051727295
Epoch 170, val loss: 1.1783545017242432
Epoch 180, training loss: 0.9282063841819763 = 0.9206972718238831 + 0.001 * 7.509121894836426
Epoch 180, val loss: 1.1214401721954346
Epoch 190, training loss: 0.8550939559936523 = 0.8475860953330994 + 0.001 * 7.50786018371582
Epoch 190, val loss: 1.0704666376113892
Epoch 200, training loss: 0.7879855036735535 = 0.7804781198501587 + 0.001 * 7.507405757904053
Epoch 200, val loss: 1.0252362489700317
Epoch 210, training loss: 0.7275994420051575 = 0.7200921177864075 + 0.001 * 7.5073347091674805
Epoch 210, val loss: 0.9864856600761414
Epoch 220, training loss: 0.6740602254867554 = 0.6665547490119934 + 0.001 * 7.5055012702941895
Epoch 220, val loss: 0.9549853205680847
Epoch 230, training loss: 0.6260432600975037 = 0.6185426115989685 + 0.001 * 7.5006208419799805
Epoch 230, val loss: 0.9298183917999268
Epoch 240, training loss: 0.5814613103866577 = 0.5739713311195374 + 0.001 * 7.489990234375
Epoch 240, val loss: 0.9095033407211304
Epoch 250, training loss: 0.5385138392448425 = 0.5310463905334473 + 0.001 * 7.467457294464111
Epoch 250, val loss: 0.8920938372612
Epoch 260, training loss: 0.4961784780025482 = 0.48874691128730774 + 0.001 * 7.4315690994262695
Epoch 260, val loss: 0.8762854933738708
Epoch 270, training loss: 0.45405876636505127 = 0.44666922092437744 + 0.001 * 7.3895392417907715
Epoch 270, val loss: 0.8615129590034485
Epoch 280, training loss: 0.4120514392852783 = 0.40468350052833557 + 0.001 * 7.367943286895752
Epoch 280, val loss: 0.8478795886039734
Epoch 290, training loss: 0.37011656165122986 = 0.3627590239048004 + 0.001 * 7.357544422149658
Epoch 290, val loss: 0.8355175256729126
Epoch 300, training loss: 0.3284858763217926 = 0.3211314380168915 + 0.001 * 7.354449272155762
Epoch 300, val loss: 0.8243600130081177
Epoch 310, training loss: 0.2879060208797455 = 0.28055259585380554 + 0.001 * 7.3534159660339355
Epoch 310, val loss: 0.8144941329956055
Epoch 320, training loss: 0.24969585239887238 = 0.24234257638454437 + 0.001 * 7.353280544281006
Epoch 320, val loss: 0.8066184520721436
Epoch 330, training loss: 0.21521273255348206 = 0.20785930752754211 + 0.001 * 7.353427410125732
Epoch 330, val loss: 0.8020156621932983
Epoch 340, training loss: 0.18531730771064758 = 0.17796339094638824 + 0.001 * 7.353922367095947
Epoch 340, val loss: 0.8015139102935791
Epoch 350, training loss: 0.16012051701545715 = 0.15276461839675903 + 0.001 * 7.355898380279541
Epoch 350, val loss: 0.8053261041641235
Epoch 360, training loss: 0.1391543298959732 = 0.13179704546928406 + 0.001 * 7.357285499572754
Epoch 360, val loss: 0.8131020069122314
Epoch 370, training loss: 0.1217154935002327 = 0.11435824632644653 + 0.001 * 7.357247352600098
Epoch 370, val loss: 0.8240697979927063
Epoch 380, training loss: 0.10712148994207382 = 0.09976299852132797 + 0.001 * 7.358493328094482
Epoch 380, val loss: 0.8375164270401001
Epoch 390, training loss: 0.09480438381433487 = 0.0874447226524353 + 0.001 * 7.359659194946289
Epoch 390, val loss: 0.8526793718338013
Epoch 400, training loss: 0.08432814478874207 = 0.07696378976106644 + 0.001 * 7.364357948303223
Epoch 400, val loss: 0.8690183758735657
Epoch 410, training loss: 0.07536028325557709 = 0.06799809634685516 + 0.001 * 7.362188339233398
Epoch 410, val loss: 0.8861595392227173
Epoch 420, training loss: 0.0676625445485115 = 0.06029968708753586 + 0.001 * 7.362856864929199
Epoch 420, val loss: 0.9037741422653198
Epoch 430, training loss: 0.06103416532278061 = 0.053669918328523636 + 0.001 * 7.364246845245361
Epoch 430, val loss: 0.9215672612190247
Epoch 440, training loss: 0.055313028395175934 = 0.047949399799108505 + 0.001 * 7.363626480102539
Epoch 440, val loss: 0.9394024610519409
Epoch 450, training loss: 0.050371523946523666 = 0.043007414788007736 + 0.001 * 7.364109039306641
Epoch 450, val loss: 0.9570912718772888
Epoch 460, training loss: 0.04609871655702591 = 0.038730669766664505 + 0.001 * 7.368045806884766
Epoch 460, val loss: 0.974555253982544
Epoch 470, training loss: 0.04238558188080788 = 0.03502056375145912 + 0.001 * 7.365016460418701
Epoch 470, val loss: 0.9916828274726868
Epoch 480, training loss: 0.03916202113032341 = 0.031792525202035904 + 0.001 * 7.369495391845703
Epoch 480, val loss: 1.0083776712417603
Epoch 490, training loss: 0.03633825108408928 = 0.02897382527589798 + 0.001 * 7.364425182342529
Epoch 490, val loss: 1.0246238708496094
Epoch 500, training loss: 0.03386783227324486 = 0.026502514258027077 + 0.001 * 7.365317344665527
Epoch 500, val loss: 1.0404324531555176
Epoch 510, training loss: 0.03168853744864464 = 0.02432536520063877 + 0.001 * 7.3631720542907715
Epoch 510, val loss: 1.0558000802993774
Epoch 520, training loss: 0.029758494347333908 = 0.02239738032221794 + 0.001 * 7.361114501953125
Epoch 520, val loss: 1.0707403421401978
Epoch 530, training loss: 0.028046276420354843 = 0.020680954679846764 + 0.001 * 7.365322113037109
Epoch 530, val loss: 1.0852715969085693
Epoch 540, training loss: 0.02650769054889679 = 0.01914687640964985 + 0.001 * 7.360814571380615
Epoch 540, val loss: 1.0993784666061401
Epoch 550, training loss: 0.02513045258820057 = 0.017771175131201744 + 0.001 * 7.35927677154541
Epoch 550, val loss: 1.113115668296814
Epoch 560, training loss: 0.02389296516776085 = 0.01653439737856388 + 0.001 * 7.358567237854004
Epoch 560, val loss: 1.1264417171478271
Epoch 570, training loss: 0.02277824468910694 = 0.015419679693877697 + 0.001 * 7.358564853668213
Epoch 570, val loss: 1.1393895149230957
Epoch 580, training loss: 0.021775437518954277 = 0.014411346055567265 + 0.001 * 7.364090442657471
Epoch 580, val loss: 1.1519711017608643
Epoch 590, training loss: 0.020852569490671158 = 0.013493946753442287 + 0.001 * 7.358622074127197
Epoch 590, val loss: 1.1642602682113647
Epoch 600, training loss: 0.020003225654363632 = 0.012652824632823467 + 0.001 * 7.350400924682617
Epoch 600, val loss: 1.1763616800308228
Epoch 610, training loss: 0.019226768985390663 = 0.011878828518092632 + 0.001 * 7.347939491271973
Epoch 610, val loss: 1.188313364982605
Epoch 620, training loss: 0.018512969836592674 = 0.011166122741997242 + 0.001 * 7.346847057342529
Epoch 620, val loss: 1.2001111507415771
Epoch 630, training loss: 0.017862046137452126 = 0.010510354302823544 + 0.001 * 7.351691246032715
Epoch 630, val loss: 1.2117518186569214
Epoch 640, training loss: 0.017249559983611107 = 0.009907148778438568 + 0.001 * 7.342411041259766
Epoch 640, val loss: 1.2232110500335693
Epoch 650, training loss: 0.01672307774424553 = 0.009352508932352066 + 0.001 * 7.370569229125977
Epoch 650, val loss: 1.2344626188278198
Epoch 660, training loss: 0.016193760558962822 = 0.008842725306749344 + 0.001 * 7.351035118103027
Epoch 660, val loss: 1.2454909086227417
Epoch 670, training loss: 0.015714043751358986 = 0.008373682387173176 + 0.001 * 7.340360641479492
Epoch 670, val loss: 1.2562847137451172
Epoch 680, training loss: 0.01526716724038124 = 0.007941697724163532 + 0.001 * 7.325469017028809
Epoch 680, val loss: 1.2668319940567017
Epoch 690, training loss: 0.014918096363544464 = 0.0075433566235005856 + 0.001 * 7.374738693237305
Epoch 690, val loss: 1.277116060256958
Epoch 700, training loss: 0.014521641656756401 = 0.007175706792622805 + 0.001 * 7.3459343910217285
Epoch 700, val loss: 1.2871479988098145
Epoch 710, training loss: 0.014171881601214409 = 0.006835460662841797 + 0.001 * 7.33642053604126
Epoch 710, val loss: 1.2969393730163574
Epoch 720, training loss: 0.013859186321496964 = 0.0065201022662222385 + 0.001 * 7.339084148406982
Epoch 720, val loss: 1.306490421295166
Epoch 730, training loss: 0.013558145612478256 = 0.0062269531190395355 + 0.001 * 7.331192493438721
Epoch 730, val loss: 1.3158090114593506
Epoch 740, training loss: 0.013267404399812222 = 0.0059537445195019245 + 0.001 * 7.31365966796875
Epoch 740, val loss: 1.3249446153640747
Epoch 750, training loss: 0.01303798332810402 = 0.005698531400412321 + 0.001 * 7.339451313018799
Epoch 750, val loss: 1.3338688611984253
Epoch 760, training loss: 0.012797126546502113 = 0.005459714215248823 + 0.001 * 7.337411403656006
Epoch 760, val loss: 1.3426035642623901
Epoch 770, training loss: 0.012550519779324532 = 0.005235942546278238 + 0.001 * 7.314577102661133
Epoch 770, val loss: 1.3511614799499512
Epoch 780, training loss: 0.012336909770965576 = 0.005025891121476889 + 0.001 * 7.311018943786621
Epoch 780, val loss: 1.3595348596572876
Epoch 790, training loss: 0.012151487171649933 = 0.00482864398509264 + 0.001 * 7.322842597961426
Epoch 790, val loss: 1.3677401542663574
Epoch 800, training loss: 0.011960948817431927 = 0.004643262829631567 + 0.001 * 7.317685604095459
Epoch 800, val loss: 1.3757833242416382
Epoch 810, training loss: 0.01177242211997509 = 0.004468873608857393 + 0.001 * 7.303548336029053
Epoch 810, val loss: 1.383657455444336
Epoch 820, training loss: 0.011591356247663498 = 0.004304752219468355 + 0.001 * 7.2866034507751465
Epoch 820, val loss: 1.3913226127624512
Epoch 830, training loss: 0.011455636471509933 = 0.0041501279920339584 + 0.001 * 7.305508613586426
Epoch 830, val loss: 1.3988423347473145
Epoch 840, training loss: 0.0113017987459898 = 0.004004292655736208 + 0.001 * 7.2975053787231445
Epoch 840, val loss: 1.4061952829360962
Epoch 850, training loss: 0.011249314062297344 = 0.003866695100441575 + 0.001 * 7.3826189041137695
Epoch 850, val loss: 1.413387656211853
Epoch 860, training loss: 0.010993055999279022 = 0.003736919490620494 + 0.001 * 7.256136417388916
Epoch 860, val loss: 1.4204320907592773
Epoch 870, training loss: 0.010927677154541016 = 0.003614345332607627 + 0.001 * 7.313331604003906
Epoch 870, val loss: 1.4272894859313965
Epoch 880, training loss: 0.010737059637904167 = 0.003498502541333437 + 0.001 * 7.238556385040283
Epoch 880, val loss: 1.43401300907135
Epoch 890, training loss: 0.010632030665874481 = 0.0033890623599290848 + 0.001 * 7.242968559265137
Epoch 890, val loss: 1.440552830696106
Epoch 900, training loss: 0.010579623281955719 = 0.0032854934688657522 + 0.001 * 7.294129848480225
Epoch 900, val loss: 1.4469355344772339
Epoch 910, training loss: 0.010466289706528187 = 0.0031873460393399 + 0.001 * 7.2789435386657715
Epoch 910, val loss: 1.4532232284545898
Epoch 920, training loss: 0.010387029498815536 = 0.00309427035972476 + 0.001 * 7.292758941650391
Epoch 920, val loss: 1.4593586921691895
Epoch 930, training loss: 0.010320985689759254 = 0.0030059264972805977 + 0.001 * 7.315058708190918
Epoch 930, val loss: 1.4653078317642212
Epoch 940, training loss: 0.010252977721393108 = 0.002922098385170102 + 0.001 * 7.330878734588623
Epoch 940, val loss: 1.4711366891860962
Epoch 950, training loss: 0.010130791924893856 = 0.0028424339834600687 + 0.001 * 7.288357734680176
Epoch 950, val loss: 1.4768704175949097
Epoch 960, training loss: 0.010015279985964298 = 0.0027667128015309572 + 0.001 * 7.248566627502441
Epoch 960, val loss: 1.4824484586715698
Epoch 970, training loss: 0.00989469327032566 = 0.002694668946787715 + 0.001 * 7.200023651123047
Epoch 970, val loss: 1.4878851175308228
Epoch 980, training loss: 0.009838595055043697 = 0.0026260060258209705 + 0.001 * 7.212588787078857
Epoch 980, val loss: 1.4932231903076172
Epoch 990, training loss: 0.009847799316048622 = 0.0025605587288737297 + 0.001 * 7.2872395515441895
Epoch 990, val loss: 1.498420238494873
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 1.955765962600708 = 1.9471690654754639 + 0.001 * 8.596864700317383
Epoch 0, val loss: 1.9405875205993652
Epoch 10, training loss: 1.94593346118927 = 1.9373366832733154 + 0.001 * 8.596831321716309
Epoch 10, val loss: 1.9300973415374756
Epoch 20, training loss: 1.9339207410812378 = 1.9253240823745728 + 0.001 * 8.596714973449707
Epoch 20, val loss: 1.9171233177185059
Epoch 30, training loss: 1.917299747467041 = 1.908703327178955 + 0.001 * 8.596473693847656
Epoch 30, val loss: 1.8991827964782715
Epoch 40, training loss: 1.8930456638336182 = 1.8844497203826904 + 0.001 * 8.595955848693848
Epoch 40, val loss: 1.8732695579528809
Epoch 50, training loss: 1.8591797351837158 = 1.8505851030349731 + 0.001 * 8.594648361206055
Epoch 50, val loss: 1.8382714986801147
Epoch 60, training loss: 1.8202787637710571 = 1.8116881847381592 + 0.001 * 8.590575218200684
Epoch 60, val loss: 1.8016799688339233
Epoch 70, training loss: 1.7862540483474731 = 1.7776787281036377 + 0.001 * 8.575369834899902
Epoch 70, val loss: 1.7743724584579468
Epoch 80, training loss: 1.7472127676010132 = 1.7387155294418335 + 0.001 * 8.497292518615723
Epoch 80, val loss: 1.7420796155929565
Epoch 90, training loss: 1.6919571161270142 = 1.6838710308074951 + 0.001 * 8.086101531982422
Epoch 90, val loss: 1.6934515237808228
Epoch 100, training loss: 1.616557240486145 = 1.6086866855621338 + 0.001 * 7.870594501495361
Epoch 100, val loss: 1.6260932683944702
Epoch 110, training loss: 1.5217673778533936 = 1.5139832496643066 + 0.001 * 7.78408145904541
Epoch 110, val loss: 1.544933557510376
Epoch 120, training loss: 1.4141267538070679 = 1.406368613243103 + 0.001 * 7.7581868171691895
Epoch 120, val loss: 1.455175757408142
Epoch 130, training loss: 1.298651933670044 = 1.2909168004989624 + 0.001 * 7.735116481781006
Epoch 130, val loss: 1.3611177206039429
Epoch 140, training loss: 1.1796776056289673 = 1.1719621419906616 + 0.001 * 7.715441703796387
Epoch 140, val loss: 1.267236351966858
Epoch 150, training loss: 1.0639276504516602 = 1.0562386512756348 + 0.001 * 7.688973426818848
Epoch 150, val loss: 1.1786798238754272
Epoch 160, training loss: 0.9578794240951538 = 0.9502373933792114 + 0.001 * 7.64204216003418
Epoch 160, val loss: 1.1002663373947144
Epoch 170, training loss: 0.8649395704269409 = 0.8573682308197021 + 0.001 * 7.571362495422363
Epoch 170, val loss: 1.035588026046753
Epoch 180, training loss: 0.7853293418884277 = 0.777800977230072 + 0.001 * 7.528375148773193
Epoch 180, val loss: 0.9852350354194641
Epoch 190, training loss: 0.7168580293655396 = 0.7093656063079834 + 0.001 * 7.492452621459961
Epoch 190, val loss: 0.9466850161552429
Epoch 200, training loss: 0.6564099788665771 = 0.6489281058311462 + 0.001 * 7.481868743896484
Epoch 200, val loss: 0.9170810580253601
Epoch 210, training loss: 0.6016054749488831 = 0.5941276550292969 + 0.001 * 7.477841854095459
Epoch 210, val loss: 0.8943083882331848
Epoch 220, training loss: 0.5510855913162231 = 0.5436140298843384 + 0.001 * 7.4715447425842285
Epoch 220, val loss: 0.8772324919700623
Epoch 230, training loss: 0.5041060447692871 = 0.4966396391391754 + 0.001 * 7.4664225578308105
Epoch 230, val loss: 0.8650790452957153
Epoch 240, training loss: 0.46018290519714355 = 0.45272812247276306 + 0.001 * 7.454769134521484
Epoch 240, val loss: 0.857217013835907
Epoch 250, training loss: 0.4189586341381073 = 0.41151729226112366 + 0.001 * 7.4413323402404785
Epoch 250, val loss: 0.8531598448753357
Epoch 260, training loss: 0.3802470862865448 = 0.37281811237335205 + 0.001 * 7.428971767425537
Epoch 260, val loss: 0.8528813719749451
Epoch 270, training loss: 0.34395477175712585 = 0.33654895424842834 + 0.001 * 7.405828475952148
Epoch 270, val loss: 0.8566347360610962
Epoch 280, training loss: 0.31017032265663147 = 0.30277660489082336 + 0.001 * 7.39370584487915
Epoch 280, val loss: 0.864765465259552
Epoch 290, training loss: 0.2789835035800934 = 0.27161261439323425 + 0.001 * 7.3708930015563965
Epoch 290, val loss: 0.8773651123046875
Epoch 300, training loss: 0.25044888257980347 = 0.2431078553199768 + 0.001 * 7.341033458709717
Epoch 300, val loss: 0.8943900465965271
Epoch 310, training loss: 0.22456416487693787 = 0.21723969280719757 + 0.001 * 7.324476718902588
Epoch 310, val loss: 0.9154769778251648
Epoch 320, training loss: 0.20125393569469452 = 0.19393044710159302 + 0.001 * 7.323485851287842
Epoch 320, val loss: 0.9398678541183472
Epoch 330, training loss: 0.18035641312599182 = 0.17306138575077057 + 0.001 * 7.295034408569336
Epoch 330, val loss: 0.9668669700622559
Epoch 340, training loss: 0.16176755726337433 = 0.15447503328323364 + 0.001 * 7.2925214767456055
Epoch 340, val loss: 0.9958066344261169
Epoch 350, training loss: 0.14527396857738495 = 0.13798299431800842 + 0.001 * 7.29096794128418
Epoch 350, val loss: 1.0260571241378784
Epoch 360, training loss: 0.13067097961902618 = 0.12338370829820633 + 0.001 * 7.28727388381958
Epoch 360, val loss: 1.057079792022705
Epoch 370, training loss: 0.11774411052465439 = 0.11046981811523438 + 0.001 * 7.274293422698975
Epoch 370, val loss: 1.0884779691696167
Epoch 380, training loss: 0.10632194578647614 = 0.09904913604259491 + 0.001 * 7.27280855178833
Epoch 380, val loss: 1.1199512481689453
Epoch 390, training loss: 0.09621773660182953 = 0.08894689381122589 + 0.001 * 7.270842552185059
Epoch 390, val loss: 1.1512926816940308
Epoch 400, training loss: 0.08728157728910446 = 0.08000489324331284 + 0.001 * 7.276687145233154
Epoch 400, val loss: 1.1823102235794067
Epoch 410, training loss: 0.0793444886803627 = 0.07208126783370972 + 0.001 * 7.263218879699707
Epoch 410, val loss: 1.2128437757492065
Epoch 420, training loss: 0.07231179624795914 = 0.06505173444747925 + 0.001 * 7.260058403015137
Epoch 420, val loss: 1.2427771091461182
Epoch 430, training loss: 0.06607350707054138 = 0.05881022661924362 + 0.001 * 7.263280868530273
Epoch 430, val loss: 1.2720203399658203
Epoch 440, training loss: 0.060520585626363754 = 0.05326513200998306 + 0.001 * 7.2554545402526855
Epoch 440, val loss: 1.3006213903427124
Epoch 450, training loss: 0.05559854209423065 = 0.04833745211362839 + 0.001 * 7.2610883712768555
Epoch 450, val loss: 1.3285468816757202
Epoch 460, training loss: 0.05120883882045746 = 0.04395856708288193 + 0.001 * 7.250272274017334
Epoch 460, val loss: 1.3557782173156738
Epoch 470, training loss: 0.04731591045856476 = 0.04006589949131012 + 0.001 * 7.250008583068848
Epoch 470, val loss: 1.3822661638259888
Epoch 480, training loss: 0.04385528713464737 = 0.03660302236676216 + 0.001 * 7.252263069152832
Epoch 480, val loss: 1.407963514328003
Epoch 490, training loss: 0.04076234623789787 = 0.03351976349949837 + 0.001 * 7.24258279800415
Epoch 490, val loss: 1.432897925376892
Epoch 500, training loss: 0.03801047056913376 = 0.030770547688007355 + 0.001 * 7.239922046661377
Epoch 500, val loss: 1.4570703506469727
Epoch 510, training loss: 0.03555350750684738 = 0.028310826048254967 + 0.001 * 7.242680549621582
Epoch 510, val loss: 1.4805234670639038
Epoch 520, training loss: 0.033333417028188705 = 0.02609797567129135 + 0.001 * 7.235440731048584
Epoch 520, val loss: 1.5033453702926636
Epoch 530, training loss: 0.03134212642908096 = 0.02410169318318367 + 0.001 * 7.240434646606445
Epoch 530, val loss: 1.5255995988845825
Epoch 540, training loss: 0.029532941058278084 = 0.022299867123365402 + 0.001 * 7.2330732345581055
Epoch 540, val loss: 1.547317624092102
Epoch 550, training loss: 0.027901295572519302 = 0.020672854036092758 + 0.001 * 7.228440284729004
Epoch 550, val loss: 1.568462610244751
Epoch 560, training loss: 0.02643083781003952 = 0.019202863797545433 + 0.001 * 7.227972984313965
Epoch 560, val loss: 1.5890439748764038
Epoch 570, training loss: 0.0251021608710289 = 0.01787353865802288 + 0.001 * 7.228621006011963
Epoch 570, val loss: 1.6090469360351562
Epoch 580, training loss: 0.023898471146821976 = 0.016670232638716698 + 0.001 * 7.228237152099609
Epoch 580, val loss: 1.6284695863723755
Epoch 590, training loss: 0.022802388295531273 = 0.015579666011035442 + 0.001 * 7.22272253036499
Epoch 590, val loss: 1.6472846269607544
Epoch 600, training loss: 0.02181248739361763 = 0.014589359983801842 + 0.001 * 7.223127841949463
Epoch 600, val loss: 1.6655230522155762
Epoch 610, training loss: 0.020918525755405426 = 0.013688473962247372 + 0.001 * 7.230051040649414
Epoch 610, val loss: 1.6831942796707153
Epoch 620, training loss: 0.02008303999900818 = 0.012867426499724388 + 0.001 * 7.21561336517334
Epoch 620, val loss: 1.7002886533737183
Epoch 630, training loss: 0.01934024691581726 = 0.012117607519030571 + 0.001 * 7.222638130187988
Epoch 630, val loss: 1.7168585062026978
Epoch 640, training loss: 0.01864471845328808 = 0.01143150869756937 + 0.001 * 7.213210105895996
Epoch 640, val loss: 1.7328767776489258
Epoch 650, training loss: 0.018022548407316208 = 0.010802553035318851 + 0.001 * 7.219995975494385
Epoch 650, val loss: 1.7484036684036255
Epoch 660, training loss: 0.017435980960726738 = 0.010224763303995132 + 0.001 * 7.211217403411865
Epoch 660, val loss: 1.7634245157241821
Epoch 670, training loss: 0.01690174639225006 = 0.009692980907857418 + 0.001 * 7.208765506744385
Epoch 670, val loss: 1.7780033349990845
Epoch 680, training loss: 0.01640317589044571 = 0.009202640503644943 + 0.001 * 7.200534820556641
Epoch 680, val loss: 1.7921291589736938
Epoch 690, training loss: 0.01596122980117798 = 0.008749688975512981 + 0.001 * 7.2115397453308105
Epoch 690, val loss: 1.8058372735977173
Epoch 700, training loss: 0.015536986291408539 = 0.008330561220645905 + 0.001 * 7.206425189971924
Epoch 700, val loss: 1.8191007375717163
Epoch 710, training loss: 0.015137703157961369 = 0.007942034862935543 + 0.001 * 7.195667743682861
Epoch 710, val loss: 1.8319891691207886
Epoch 720, training loss: 0.014781084842979908 = 0.007581127341836691 + 0.001 * 7.199957370758057
Epoch 720, val loss: 1.8445097208023071
Epoch 730, training loss: 0.014438195154070854 = 0.007244952954351902 + 0.001 * 7.19324254989624
Epoch 730, val loss: 1.8566648960113525
Epoch 740, training loss: 0.014121754094958305 = 0.00693044438958168 + 0.001 * 7.191309452056885
Epoch 740, val loss: 1.8685543537139893
Epoch 750, training loss: 0.013825612142682076 = 0.006634442135691643 + 0.001 * 7.191169738769531
Epoch 750, val loss: 1.8802286386489868
Epoch 760, training loss: 0.013551093637943268 = 0.006354541517794132 + 0.001 * 7.196551322937012
Epoch 760, val loss: 1.8917231559753418
Epoch 770, training loss: 0.013278848491609097 = 0.006089392118155956 + 0.001 * 7.189455986022949
Epoch 770, val loss: 1.903065800666809
Epoch 780, training loss: 0.013019423931837082 = 0.0058382428251206875 + 0.001 * 7.181180953979492
Epoch 780, val loss: 1.9142403602600098
Epoch 790, training loss: 0.012788636609911919 = 0.00560060003772378 + 0.001 * 7.1880364418029785
Epoch 790, val loss: 1.925257682800293
Epoch 800, training loss: 0.012560487724840641 = 0.0053760516457259655 + 0.001 * 7.184435844421387
Epoch 800, val loss: 1.9360772371292114
Epoch 810, training loss: 0.012352406978607178 = 0.005164049100130796 + 0.001 * 7.188356876373291
Epoch 810, val loss: 1.9467384815216064
Epoch 820, training loss: 0.012140178121626377 = 0.004964012186974287 + 0.001 * 7.176165580749512
Epoch 820, val loss: 1.957188606262207
Epoch 830, training loss: 0.011961361393332481 = 0.004775297362357378 + 0.001 * 7.186063289642334
Epoch 830, val loss: 1.9674606323242188
Epoch 840, training loss: 0.01176734734326601 = 0.004597288556396961 + 0.001 * 7.170058250427246
Epoch 840, val loss: 1.9775168895721436
Epoch 850, training loss: 0.011615481227636337 = 0.004429324064403772 + 0.001 * 7.186156749725342
Epoch 850, val loss: 1.9873747825622559
Epoch 860, training loss: 0.01145949400961399 = 0.004270739853382111 + 0.001 * 7.188753604888916
Epoch 860, val loss: 1.9970589876174927
Epoch 870, training loss: 0.011303810402750969 = 0.004120705649256706 + 0.001 * 7.183104515075684
Epoch 870, val loss: 2.0065255165100098
Epoch 880, training loss: 0.011152086779475212 = 0.003978176042437553 + 0.001 * 7.173910140991211
Epoch 880, val loss: 2.0158934593200684
Epoch 890, training loss: 0.011003950610756874 = 0.003841984551399946 + 0.001 * 7.161965847015381
Epoch 890, val loss: 2.025172710418701
Epoch 900, training loss: 0.010881584137678146 = 0.0037111931014806032 + 0.001 * 7.170390605926514
Epoch 900, val loss: 2.034424304962158
Epoch 910, training loss: 0.01074295025318861 = 0.0035854072775691748 + 0.001 * 7.1575422286987305
Epoch 910, val loss: 2.0436480045318604
Epoch 920, training loss: 0.010621190071105957 = 0.0034644287079572678 + 0.001 * 7.1567606925964355
Epoch 920, val loss: 2.0528597831726074
Epoch 930, training loss: 0.01051498856395483 = 0.003348227823153138 + 0.001 * 7.166759967803955
Epoch 930, val loss: 2.0620219707489014
Epoch 940, training loss: 0.0104018934071064 = 0.003237109864130616 + 0.001 * 7.164783477783203
Epoch 940, val loss: 2.071089506149292
Epoch 950, training loss: 0.01029388327151537 = 0.0031310375779867172 + 0.001 * 7.162845134735107
Epoch 950, val loss: 2.0800280570983887
Epoch 960, training loss: 0.010194007307291031 = 0.0030298102647066116 + 0.001 * 7.164196491241455
Epoch 960, val loss: 2.088895797729492
Epoch 970, training loss: 0.010095100849866867 = 0.0029333890415728092 + 0.001 * 7.1617112159729
Epoch 970, val loss: 2.0976033210754395
Epoch 980, training loss: 0.009998747147619724 = 0.002841697772964835 + 0.001 * 7.157049179077148
Epoch 980, val loss: 2.106203079223633
Epoch 990, training loss: 0.009897246956825256 = 0.0027545010671019554 + 0.001 * 7.142745494842529
Epoch 990, val loss: 2.1146366596221924
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8112809699525567
The final CL Acc:0.77037, 0.02419, The final GNN Acc:0.81673, 0.00400
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13266])
remove edge: torch.Size([2, 7868])
updated graph: torch.Size([2, 10578])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9426040649414062 = 1.934007167816162 + 0.001 * 8.596869468688965
Epoch 0, val loss: 1.93744957447052
Epoch 10, training loss: 1.9326390027999878 = 1.9240422248840332 + 0.001 * 8.596823692321777
Epoch 10, val loss: 1.9268258810043335
Epoch 20, training loss: 1.9202572107315063 = 1.9116605520248413 + 0.001 * 8.596641540527344
Epoch 20, val loss: 1.913514256477356
Epoch 30, training loss: 1.9028435945510864 = 1.8942474126815796 + 0.001 * 8.596150398254395
Epoch 30, val loss: 1.8949586153030396
Epoch 40, training loss: 1.8776780366897583 = 1.8690831661224365 + 0.001 * 8.59481143951416
Epoch 40, val loss: 1.8687766790390015
Epoch 50, training loss: 1.8436262607574463 = 1.8350353240966797 + 0.001 * 8.590893745422363
Epoch 50, val loss: 1.8350507020950317
Epoch 60, training loss: 1.80611252784729 = 1.7975351810455322 + 0.001 * 8.577371597290039
Epoch 60, val loss: 1.8014510869979858
Epoch 70, training loss: 1.7713676691055298 = 1.762845516204834 + 0.001 * 8.522153854370117
Epoch 70, val loss: 1.7721949815750122
Epoch 80, training loss: 1.7254078388214111 = 1.717163324356079 + 0.001 * 8.244457244873047
Epoch 80, val loss: 1.7306801080703735
Epoch 90, training loss: 1.6610407829284668 = 1.6529393196105957 + 0.001 * 8.101454734802246
Epoch 90, val loss: 1.6733969449996948
Epoch 100, training loss: 1.5787379741668701 = 1.5706738233566284 + 0.001 * 8.064154624938965
Epoch 100, val loss: 1.604487657546997
Epoch 110, training loss: 1.4869047403335571 = 1.4788551330566406 + 0.001 * 8.049661636352539
Epoch 110, val loss: 1.5279933214187622
Epoch 120, training loss: 1.3961684703826904 = 1.3881332874298096 + 0.001 * 8.035210609436035
Epoch 120, val loss: 1.454535722732544
Epoch 130, training loss: 1.3109967708587646 = 1.3029963970184326 + 0.001 * 8.000411033630371
Epoch 130, val loss: 1.3870457410812378
Epoch 140, training loss: 1.2302764654159546 = 1.2224147319793701 + 0.001 * 7.861741065979004
Epoch 140, val loss: 1.3257783651351929
Epoch 150, training loss: 1.1517356634140015 = 1.1441216468811035 + 0.001 * 7.6140642166137695
Epoch 150, val loss: 1.267588496208191
Epoch 160, training loss: 1.072867512702942 = 1.0653420686721802 + 0.001 * 7.525417804718018
Epoch 160, val loss: 1.2086268663406372
Epoch 170, training loss: 0.9920430183410645 = 0.9845671057701111 + 0.001 * 7.475890159606934
Epoch 170, val loss: 1.146844744682312
Epoch 180, training loss: 0.9101051092147827 = 0.9026369452476501 + 0.001 * 7.468191623687744
Epoch 180, val loss: 1.0827887058258057
Epoch 190, training loss: 0.829624593257904 = 0.8221650123596191 + 0.001 * 7.459582328796387
Epoch 190, val loss: 1.0188448429107666
Epoch 200, training loss: 0.7534993290901184 = 0.7460463047027588 + 0.001 * 7.4530534744262695
Epoch 200, val loss: 0.9584759473800659
Epoch 210, training loss: 0.6831338405609131 = 0.6756864190101624 + 0.001 * 7.447440147399902
Epoch 210, val loss: 0.9036622643470764
Epoch 220, training loss: 0.6182790398597717 = 0.6108366847038269 + 0.001 * 7.442354679107666
Epoch 220, val loss: 0.8554689884185791
Epoch 230, training loss: 0.5577148199081421 = 0.5502774119377136 + 0.001 * 7.437411785125732
Epoch 230, val loss: 0.8134965896606445
Epoch 240, training loss: 0.5004376769065857 = 0.49300527572631836 + 0.001 * 7.432419300079346
Epoch 240, val loss: 0.7773551344871521
Epoch 250, training loss: 0.4459984600543976 = 0.4385710060596466 + 0.001 * 7.4274516105651855
Epoch 250, val loss: 0.7462261915206909
Epoch 260, training loss: 0.39457836747169495 = 0.3871555030345917 + 0.001 * 7.422858715057373
Epoch 260, val loss: 0.7198593616485596
Epoch 270, training loss: 0.34673115611076355 = 0.3393125534057617 + 0.001 * 7.41860818862915
Epoch 270, val loss: 0.6978901028633118
Epoch 280, training loss: 0.30310267210006714 = 0.29568812251091003 + 0.001 * 7.414543628692627
Epoch 280, val loss: 0.6801273226737976
Epoch 290, training loss: 0.26413777470588684 = 0.2567274272441864 + 0.001 * 7.410345554351807
Epoch 290, val loss: 0.6667894124984741
Epoch 300, training loss: 0.22997748851776123 = 0.222572460770607 + 0.001 * 7.405022144317627
Epoch 300, val loss: 0.6578129529953003
Epoch 310, training loss: 0.20045173168182373 = 0.19305525720119476 + 0.001 * 7.396472454071045
Epoch 310, val loss: 0.6528899073600769
Epoch 320, training loss: 0.17519041895866394 = 0.16780398786067963 + 0.001 * 7.386434555053711
Epoch 320, val loss: 0.6515131592750549
Epoch 330, training loss: 0.1536942422389984 = 0.14632190763950348 + 0.001 * 7.372340679168701
Epoch 330, val loss: 0.653114914894104
Epoch 340, training loss: 0.13542349636554718 = 0.12806463241577148 + 0.001 * 7.358868598937988
Epoch 340, val loss: 0.6571187376976013
Epoch 350, training loss: 0.11985614150762558 = 0.11250517517328262 + 0.001 * 7.3509650230407715
Epoch 350, val loss: 0.6630755662918091
Epoch 360, training loss: 0.10652849823236465 = 0.09918680042028427 + 0.001 * 7.341695785522461
Epoch 360, val loss: 0.670575737953186
Epoch 370, training loss: 0.09507709741592407 = 0.08773712068796158 + 0.001 * 7.339977741241455
Epoch 370, val loss: 0.6792576909065247
Epoch 380, training loss: 0.08519050478935242 = 0.0778549388051033 + 0.001 * 7.33556604385376
Epoch 380, val loss: 0.6887738108634949
Epoch 390, training loss: 0.07663169503211975 = 0.0692974328994751 + 0.001 * 7.334264278411865
Epoch 390, val loss: 0.6988686919212341
Epoch 400, training loss: 0.06919726729393005 = 0.06186511740088463 + 0.001 * 7.332146644592285
Epoch 400, val loss: 0.7093853950500488
Epoch 410, training loss: 0.06272583454847336 = 0.055395446717739105 + 0.001 * 7.330385208129883
Epoch 410, val loss: 0.7201019525527954
Epoch 420, training loss: 0.05708686262369156 = 0.04975360259413719 + 0.001 * 7.333261489868164
Epoch 420, val loss: 0.7309781312942505
Epoch 430, training loss: 0.05215221270918846 = 0.04482530802488327 + 0.001 * 7.326904296875
Epoch 430, val loss: 0.741866946220398
Epoch 440, training loss: 0.04783715307712555 = 0.040512584149837494 + 0.001 * 7.324569225311279
Epoch 440, val loss: 0.7527464628219604
Epoch 450, training loss: 0.04405611380934715 = 0.03673209995031357 + 0.001 * 7.324012279510498
Epoch 450, val loss: 0.7635275721549988
Epoch 460, training loss: 0.040732044726610184 = 0.033411771059036255 + 0.001 * 7.320272445678711
Epoch 460, val loss: 0.7741602659225464
Epoch 470, training loss: 0.037805479019880295 = 0.03048914484679699 + 0.001 * 7.3163323402404785
Epoch 470, val loss: 0.7846541404724121
Epoch 480, training loss: 0.03522750735282898 = 0.027910154312849045 + 0.001 * 7.3173508644104
Epoch 480, val loss: 0.7949673533439636
Epoch 490, training loss: 0.03293979912996292 = 0.025628432631492615 + 0.001 * 7.311366081237793
Epoch 490, val loss: 0.8050618767738342
Epoch 500, training loss: 0.03090984746813774 = 0.02360343188047409 + 0.001 * 7.306416034698486
Epoch 500, val loss: 0.8149111270904541
Epoch 510, training loss: 0.029105260968208313 = 0.021801212802529335 + 0.001 * 7.304048538208008
Epoch 510, val loss: 0.8245365619659424
Epoch 520, training loss: 0.027490437030792236 = 0.020192379131913185 + 0.001 * 7.2980570793151855
Epoch 520, val loss: 0.8339108824729919
Epoch 530, training loss: 0.026045802980661392 = 0.018751833587884903 + 0.001 * 7.293969631195068
Epoch 530, val loss: 0.8430225253105164
Epoch 540, training loss: 0.024749204516410828 = 0.017458153888583183 + 0.001 * 7.291049957275391
Epoch 540, val loss: 0.8518872857093811
Epoch 550, training loss: 0.02357131987810135 = 0.016292762011289597 + 0.001 * 7.278556823730469
Epoch 550, val loss: 0.8604990839958191
Epoch 560, training loss: 0.02253679186105728 = 0.015239923261106014 + 0.001 * 7.296867847442627
Epoch 560, val loss: 0.8688897490501404
Epoch 570, training loss: 0.02155963145196438 = 0.014286307618021965 + 0.001 * 7.2733235359191895
Epoch 570, val loss: 0.8770402073860168
Epoch 580, training loss: 0.02069449983537197 = 0.013420164585113525 + 0.001 * 7.27433443069458
Epoch 580, val loss: 0.884951651096344
Epoch 590, training loss: 0.019896462559700012 = 0.012631372548639774 + 0.001 * 7.265090465545654
Epoch 590, val loss: 0.8926577568054199
Epoch 600, training loss: 0.019168803468346596 = 0.011911378242075443 + 0.001 * 7.257424831390381
Epoch 600, val loss: 0.9001491069793701
Epoch 610, training loss: 0.018504804000258446 = 0.011252757161855698 + 0.001 * 7.252046585083008
Epoch 610, val loss: 0.9074178338050842
Epoch 620, training loss: 0.017931709066033363 = 0.01064880471676588 + 0.001 * 7.282904148101807
Epoch 620, val loss: 0.9144802689552307
Epoch 630, training loss: 0.01734016276896 = 0.010093776509165764 + 0.001 * 7.246386528015137
Epoch 630, val loss: 0.9213513135910034
Epoch 640, training loss: 0.01684047281742096 = 0.009582399390637875 + 0.001 * 7.258073329925537
Epoch 640, val loss: 0.9280175566673279
Epoch 650, training loss: 0.016361253336071968 = 0.009110338054597378 + 0.001 * 7.250914573669434
Epoch 650, val loss: 0.934517502784729
Epoch 660, training loss: 0.015913531184196472 = 0.008673682808876038 + 0.001 * 7.239849090576172
Epoch 660, val loss: 0.9408427476882935
Epoch 670, training loss: 0.015504639595746994 = 0.008269104175269604 + 0.001 * 7.235535144805908
Epoch 670, val loss: 0.9470146298408508
Epoch 680, training loss: 0.01513156108558178 = 0.007893646135926247 + 0.001 * 7.237914562225342
Epoch 680, val loss: 0.9530162811279297
Epoch 690, training loss: 0.014792464673519135 = 0.007544512394815683 + 0.001 * 7.247951507568359
Epoch 690, val loss: 0.9588455557823181
Epoch 700, training loss: 0.0144490422680974 = 0.007219342514872551 + 0.001 * 7.229699611663818
Epoch 700, val loss: 0.964551568031311
Epoch 710, training loss: 0.01414727233350277 = 0.006916007958352566 + 0.001 * 7.231263637542725
Epoch 710, val loss: 0.9700943827629089
Epoch 720, training loss: 0.013858893886208534 = 0.006632643286138773 + 0.001 * 7.226250648498535
Epoch 720, val loss: 0.9755141735076904
Epoch 730, training loss: 0.0135938860476017 = 0.006367539055645466 + 0.001 * 7.226346492767334
Epoch 730, val loss: 0.9807884097099304
Epoch 740, training loss: 0.013345703482627869 = 0.006119140423834324 + 0.001 * 7.2265625
Epoch 740, val loss: 0.9859490394592285
Epoch 750, training loss: 0.013110170140862465 = 0.005886094179004431 + 0.001 * 7.2240753173828125
Epoch 750, val loss: 0.9909783005714417
Epoch 760, training loss: 0.012888390570878983 = 0.005667177960276604 + 0.001 * 7.221212387084961
Epoch 760, val loss: 0.9958865642547607
Epoch 770, training loss: 0.012676212936639786 = 0.00546121085062623 + 0.001 * 7.215001583099365
Epoch 770, val loss: 1.0006803274154663
Epoch 780, training loss: 0.012482644990086555 = 0.005267089698463678 + 0.001 * 7.215554714202881
Epoch 780, val loss: 1.0053677558898926
Epoch 790, training loss: 0.012287724763154984 = 0.005083369556814432 + 0.001 * 7.204354763031006
Epoch 790, val loss: 1.0099583864212036
Epoch 800, training loss: 0.012116044759750366 = 0.00490840058773756 + 0.001 * 7.207643508911133
Epoch 800, val loss: 1.014475703239441
Epoch 810, training loss: 0.011946317739784718 = 0.004740362521260977 + 0.001 * 7.2059550285339355
Epoch 810, val loss: 1.018943190574646
Epoch 820, training loss: 0.011778060346841812 = 0.0045782639645040035 + 0.001 * 7.199796199798584
Epoch 820, val loss: 1.0234003067016602
Epoch 830, training loss: 0.011621931567788124 = 0.004421747289597988 + 0.001 * 7.200183391571045
Epoch 830, val loss: 1.0278499126434326
Epoch 840, training loss: 0.011480229906737804 = 0.004270736128091812 + 0.001 * 7.209493637084961
Epoch 840, val loss: 1.0322989225387573
Epoch 850, training loss: 0.011327975429594517 = 0.004125360865145922 + 0.001 * 7.2026143074035645
Epoch 850, val loss: 1.036731481552124
Epoch 860, training loss: 0.0111871138215065 = 0.003985639661550522 + 0.001 * 7.201473236083984
Epoch 860, val loss: 1.0411674976348877
Epoch 870, training loss: 0.011036068201065063 = 0.0038516181521117687 + 0.001 * 7.184449672698975
Epoch 870, val loss: 1.0456053018569946
Epoch 880, training loss: 0.010910012759268284 = 0.00372309610247612 + 0.001 * 7.186916351318359
Epoch 880, val loss: 1.0500023365020752
Epoch 890, training loss: 0.010793285444378853 = 0.003599752439185977 + 0.001 * 7.193532466888428
Epoch 890, val loss: 1.0544260740280151
Epoch 900, training loss: 0.010665399953722954 = 0.0034812502562999725 + 0.001 * 7.184149742126465
Epoch 900, val loss: 1.058846116065979
Epoch 910, training loss: 0.010551152750849724 = 0.003367295255884528 + 0.001 * 7.183857440948486
Epoch 910, val loss: 1.0633156299591064
Epoch 920, training loss: 0.010442757047712803 = 0.0032574699725955725 + 0.001 * 7.185286521911621
Epoch 920, val loss: 1.067836880683899
Epoch 930, training loss: 0.010332860052585602 = 0.0031513809226453304 + 0.001 * 7.181479454040527
Epoch 930, val loss: 1.0724074840545654
Epoch 940, training loss: 0.010222028009593487 = 0.0030489962082356215 + 0.001 * 7.173031806945801
Epoch 940, val loss: 1.077051043510437
Epoch 950, training loss: 0.01013562735170126 = 0.002950262976810336 + 0.001 * 7.185364246368408
Epoch 950, val loss: 1.0816981792449951
Epoch 960, training loss: 0.010012517683207989 = 0.0028553707525134087 + 0.001 * 7.157146453857422
Epoch 960, val loss: 1.0864288806915283
Epoch 970, training loss: 0.009933860041201115 = 0.00276427180506289 + 0.001 * 7.1695876121521
Epoch 970, val loss: 1.0911059379577637
Epoch 980, training loss: 0.009856132790446281 = 0.0026769619435071945 + 0.001 * 7.179171085357666
Epoch 980, val loss: 1.095815896987915
Epoch 990, training loss: 0.009766168892383575 = 0.002593389479443431 + 0.001 * 7.172779083251953
Epoch 990, val loss: 1.1005042791366577
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 1.9427037239074707 = 1.9341068267822266 + 0.001 * 8.596869468688965
Epoch 0, val loss: 1.9338315725326538
Epoch 10, training loss: 1.9327337741851807 = 1.924136996269226 + 0.001 * 8.596814155578613
Epoch 10, val loss: 1.9242221117019653
Epoch 20, training loss: 1.9200332164764404 = 1.9114365577697754 + 0.001 * 8.596659660339355
Epoch 20, val loss: 1.9115325212478638
Epoch 30, training loss: 1.9020981788635254 = 1.893501877784729 + 0.001 * 8.596293449401855
Epoch 30, val loss: 1.8933948278427124
Epoch 40, training loss: 1.8759852647781372 = 1.8673899173736572 + 0.001 * 8.595396995544434
Epoch 40, val loss: 1.8673725128173828
Epoch 50, training loss: 1.840600609779358 = 1.832007884979248 + 0.001 * 8.592764854431152
Epoch 50, val loss: 1.8339424133300781
Epoch 60, training loss: 1.8015425205230713 = 1.7929599285125732 + 0.001 * 8.582636833190918
Epoch 60, val loss: 1.8001084327697754
Epoch 70, training loss: 1.763163447380066 = 1.754629373550415 + 0.001 * 8.53408145904541
Epoch 70, val loss: 1.7663906812667847
Epoch 80, training loss: 1.7112751007080078 = 1.7030320167541504 + 0.001 * 8.243026733398438
Epoch 80, val loss: 1.7168511152267456
Epoch 90, training loss: 1.6393963098526 = 1.6313402652740479 + 0.001 * 8.05599308013916
Epoch 90, val loss: 1.6511223316192627
Epoch 100, training loss: 1.5472145080566406 = 1.5392056703567505 + 0.001 * 8.008798599243164
Epoch 100, val loss: 1.571632981300354
Epoch 110, training loss: 1.4462645053863525 = 1.438295841217041 + 0.001 * 7.968721866607666
Epoch 110, val loss: 1.485011339187622
Epoch 120, training loss: 1.3459196090698242 = 1.3380458354949951 + 0.001 * 7.87379264831543
Epoch 120, val loss: 1.4019381999969482
Epoch 130, training loss: 1.2494006156921387 = 1.2417734861373901 + 0.001 * 7.627091407775879
Epoch 130, val loss: 1.3240678310394287
Epoch 140, training loss: 1.1574375629425049 = 1.1498796939849854 + 0.001 * 7.557872295379639
Epoch 140, val loss: 1.2514524459838867
Epoch 150, training loss: 1.0687596797943115 = 1.0612635612487793 + 0.001 * 7.496160507202148
Epoch 150, val loss: 1.1831146478652954
Epoch 160, training loss: 0.9824526309967041 = 0.9749853014945984 + 0.001 * 7.467300891876221
Epoch 160, val loss: 1.1168618202209473
Epoch 170, training loss: 0.8986910581588745 = 0.8912374973297119 + 0.001 * 7.453550338745117
Epoch 170, val loss: 1.0525944232940674
Epoch 180, training loss: 0.8185802102088928 = 0.8111348152160645 + 0.001 * 7.445376396179199
Epoch 180, val loss: 0.9912686944007874
Epoch 190, training loss: 0.7433591485023499 = 0.7359188795089722 + 0.001 * 7.440240383148193
Epoch 190, val loss: 0.9343090057373047
Epoch 200, training loss: 0.6739909052848816 = 0.6665552258491516 + 0.001 * 7.435701847076416
Epoch 200, val loss: 0.8832878470420837
Epoch 210, training loss: 0.6107680201530457 = 0.6033375859260559 + 0.001 * 7.430405616760254
Epoch 210, val loss: 0.8393303751945496
Epoch 220, training loss: 0.553096354007721 = 0.5456730127334595 + 0.001 * 7.423327445983887
Epoch 220, val loss: 0.8023397922515869
Epoch 230, training loss: 0.49984216690063477 = 0.49243006110191345 + 0.001 * 7.412105083465576
Epoch 230, val loss: 0.7714971899986267
Epoch 240, training loss: 0.44978809356689453 = 0.4423956573009491 + 0.001 * 7.392435073852539
Epoch 240, val loss: 0.7454596757888794
Epoch 250, training loss: 0.4021080434322357 = 0.39474794268608093 + 0.001 * 7.360103130340576
Epoch 250, val loss: 0.7233523726463318
Epoch 260, training loss: 0.35660427808761597 = 0.3492640256881714 + 0.001 * 7.34023904800415
Epoch 260, val loss: 0.7047649025917053
Epoch 270, training loss: 0.3135700523853302 = 0.3062504231929779 + 0.001 * 7.319627285003662
Epoch 270, val loss: 0.68967205286026
Epoch 280, training loss: 0.27352437376976013 = 0.2662118077278137 + 0.001 * 7.3125691413879395
Epoch 280, val loss: 0.6783540844917297
Epoch 290, training loss: 0.23698705434799194 = 0.22967606782913208 + 0.001 * 7.3109822273254395
Epoch 290, val loss: 0.6709612011909485
Epoch 300, training loss: 0.20438770949840546 = 0.1970769613981247 + 0.001 * 7.310743808746338
Epoch 300, val loss: 0.6675611138343811
Epoch 310, training loss: 0.17592838406562805 = 0.1686176061630249 + 0.001 * 7.310774803161621
Epoch 310, val loss: 0.6680556535720825
Epoch 320, training loss: 0.1515321433544159 = 0.1442212015390396 + 0.001 * 7.3109450340271
Epoch 320, val loss: 0.6722028851509094
Epoch 330, training loss: 0.13090211153030396 = 0.1235908716917038 + 0.001 * 7.311234951019287
Epoch 330, val loss: 0.6794617772102356
Epoch 340, training loss: 0.11360686272382736 = 0.10629536211490631 + 0.001 * 7.311498641967773
Epoch 340, val loss: 0.6891086101531982
Epoch 350, training loss: 0.09916559606790543 = 0.09185247868299484 + 0.001 * 7.313114166259766
Epoch 350, val loss: 0.7005908489227295
Epoch 360, training loss: 0.08711577951908112 = 0.07980269193649292 + 0.001 * 7.313085556030273
Epoch 360, val loss: 0.7132719159126282
Epoch 370, training loss: 0.07704409956932068 = 0.06973158568143845 + 0.001 * 7.31251335144043
Epoch 370, val loss: 0.7267717123031616
Epoch 380, training loss: 0.06859961152076721 = 0.06128794699907303 + 0.001 * 7.311661243438721
Epoch 380, val loss: 0.7406931519508362
Epoch 390, training loss: 0.06148727238178253 = 0.05417574569582939 + 0.001 * 7.311527729034424
Epoch 390, val loss: 0.7548230886459351
Epoch 400, training loss: 0.05546189472079277 = 0.04815161973237991 + 0.001 * 7.310274124145508
Epoch 400, val loss: 0.7689272165298462
Epoch 410, training loss: 0.05032765865325928 = 0.04301760718226433 + 0.001 * 7.310051918029785
Epoch 410, val loss: 0.7829335331916809
Epoch 420, training loss: 0.04592426121234894 = 0.03861634433269501 + 0.001 * 7.307918548583984
Epoch 420, val loss: 0.7967395186424255
Epoch 430, training loss: 0.04212982952594757 = 0.03482190519571304 + 0.001 * 7.307925701141357
Epoch 430, val loss: 0.8102688193321228
Epoch 440, training loss: 0.03883928060531616 = 0.031533580273389816 + 0.001 * 7.30570125579834
Epoch 440, val loss: 0.8234818577766418
Epoch 450, training loss: 0.03597958758473396 = 0.028669308871030807 + 0.001 * 7.310276985168457
Epoch 450, val loss: 0.8363952040672302
Epoch 460, training loss: 0.033463358879089355 = 0.02616359107196331 + 0.001 * 7.299765586853027
Epoch 460, val loss: 0.8489296436309814
Epoch 470, training loss: 0.031262677162885666 = 0.02396218664944172 + 0.001 * 7.3004889488220215
Epoch 470, val loss: 0.8611043095588684
Epoch 480, training loss: 0.029314333572983742 = 0.022020705044269562 + 0.001 * 7.293627738952637
Epoch 480, val loss: 0.872901439666748
Epoch 490, training loss: 0.02760772407054901 = 0.0203016959130764 + 0.001 * 7.306028842926025
Epoch 490, val loss: 0.8844060301780701
Epoch 500, training loss: 0.026056470349431038 = 0.01877424865961075 + 0.001 * 7.28222131729126
Epoch 500, val loss: 0.8955128788948059
Epoch 510, training loss: 0.024690832942724228 = 0.0174117274582386 + 0.001 * 7.279104709625244
Epoch 510, val loss: 0.9062528610229492
Epoch 520, training loss: 0.023514937609434128 = 0.016192426905035973 + 0.001 * 7.322509288787842
Epoch 520, val loss: 0.9166743755340576
Epoch 530, training loss: 0.022380050271749496 = 0.015097714960575104 + 0.001 * 7.282334804534912
Epoch 530, val loss: 0.926751434803009
Epoch 540, training loss: 0.021377133205533028 = 0.014111601747572422 + 0.001 * 7.265531063079834
Epoch 540, val loss: 0.9365367889404297
Epoch 550, training loss: 0.020502403378486633 = 0.013220496475696564 + 0.001 * 7.281907558441162
Epoch 550, val loss: 0.9460138082504272
Epoch 560, training loss: 0.01966746337711811 = 0.012413272634148598 + 0.001 * 7.254190444946289
Epoch 560, val loss: 0.9551644921302795
Epoch 570, training loss: 0.01894383504986763 = 0.011679816991090775 + 0.001 * 7.264018535614014
Epoch 570, val loss: 0.9640581011772156
Epoch 580, training loss: 0.018290197476744652 = 0.01101157907396555 + 0.001 * 7.2786173820495605
Epoch 580, val loss: 0.9726960062980652
Epoch 590, training loss: 0.017661508172750473 = 0.01040131039917469 + 0.001 * 7.26019811630249
Epoch 590, val loss: 0.9810726642608643
Epoch 600, training loss: 0.017078984528779984 = 0.00984244979918003 + 0.001 * 7.236535549163818
Epoch 600, val loss: 0.9892004132270813
Epoch 610, training loss: 0.016572240740060806 = 0.009329475462436676 + 0.001 * 7.242764472961426
Epoch 610, val loss: 0.9970815181732178
Epoch 620, training loss: 0.016104334965348244 = 0.008857466280460358 + 0.001 * 7.246867656707764
Epoch 620, val loss: 1.0047404766082764
Epoch 630, training loss: 0.015682119876146317 = 0.008422229439020157 + 0.001 * 7.259890079498291
Epoch 630, val loss: 1.012189507484436
Epoch 640, training loss: 0.015270322561264038 = 0.008020095527172089 + 0.001 * 7.250226974487305
Epoch 640, val loss: 1.0194284915924072
Epoch 650, training loss: 0.014863397926092148 = 0.007647819351404905 + 0.001 * 7.215577602386475
Epoch 650, val loss: 1.0264780521392822
Epoch 660, training loss: 0.014517231844365597 = 0.0073024807497859 + 0.001 * 7.21475076675415
Epoch 660, val loss: 1.033345341682434
Epoch 670, training loss: 0.014195198193192482 = 0.006981574464589357 + 0.001 * 7.213624000549316
Epoch 670, val loss: 1.0400222539901733
Epoch 680, training loss: 0.013879124075174332 = 0.006682918407022953 + 0.001 * 7.196205139160156
Epoch 680, val loss: 1.0465160608291626
Epoch 690, training loss: 0.01363532617688179 = 0.006404426880180836 + 0.001 * 7.230898380279541
Epoch 690, val loss: 1.0528438091278076
Epoch 700, training loss: 0.013356080278754234 = 0.006144475657492876 + 0.001 * 7.21160364151001
Epoch 700, val loss: 1.0590089559555054
Epoch 710, training loss: 0.013095738366246223 = 0.005901365075260401 + 0.001 * 7.19437313079834
Epoch 710, val loss: 1.0650115013122559
Epoch 720, training loss: 0.012864156626164913 = 0.005673638079315424 + 0.001 * 7.190518379211426
Epoch 720, val loss: 1.0708820819854736
Epoch 730, training loss: 0.012670420110225677 = 0.005460101645439863 + 0.001 * 7.210318565368652
Epoch 730, val loss: 1.076614499092102
Epoch 740, training loss: 0.012489067390561104 = 0.005259521305561066 + 0.001 * 7.229545593261719
Epoch 740, val loss: 1.08219313621521
Epoch 750, training loss: 0.012253931723535061 = 0.005070891696959734 + 0.001 * 7.183039665222168
Epoch 750, val loss: 1.0876505374908447
Epoch 760, training loss: 0.012067044153809547 = 0.004893292672932148 + 0.001 * 7.173750877380371
Epoch 760, val loss: 1.0929731130599976
Epoch 770, training loss: 0.011908842250704765 = 0.004725925158709288 + 0.001 * 7.18291711807251
Epoch 770, val loss: 1.0981812477111816
Epoch 780, training loss: 0.011739427223801613 = 0.004567937459796667 + 0.001 * 7.1714887619018555
Epoch 780, val loss: 1.1032731533050537
Epoch 790, training loss: 0.011592227965593338 = 0.004418674856424332 + 0.001 * 7.173552513122559
Epoch 790, val loss: 1.108248233795166
Epoch 800, training loss: 0.011446492746472359 = 0.004277567844837904 + 0.001 * 7.168924808502197
Epoch 800, val loss: 1.1131089925765991
Epoch 810, training loss: 0.011316211894154549 = 0.004143949132412672 + 0.001 * 7.172262191772461
Epoch 810, val loss: 1.1178555488586426
Epoch 820, training loss: 0.011182762682437897 = 0.0040173339657485485 + 0.001 * 7.1654276847839355
Epoch 820, val loss: 1.1225160360336304
Epoch 830, training loss: 0.011058252304792404 = 0.0038972157053649426 + 0.001 * 7.161036014556885
Epoch 830, val loss: 1.1270614862442017
Epoch 840, training loss: 0.010955058969557285 = 0.0037831959780305624 + 0.001 * 7.171862602233887
Epoch 840, val loss: 1.1315230131149292
Epoch 850, training loss: 0.010832645930349827 = 0.0036748507991433144 + 0.001 * 7.157794952392578
Epoch 850, val loss: 1.1358819007873535
Epoch 860, training loss: 0.010720879770815372 = 0.003571799723431468 + 0.001 * 7.1490797996521
Epoch 860, val loss: 1.1401481628417969
Epoch 870, training loss: 0.010651366785168648 = 0.003473726101219654 + 0.001 * 7.177640438079834
Epoch 870, val loss: 1.1443496942520142
Epoch 880, training loss: 0.010522290132939816 = 0.0033802783582359552 + 0.001 * 7.142011642456055
Epoch 880, val loss: 1.1484298706054688
Epoch 890, training loss: 0.010441483929753304 = 0.003291181055828929 + 0.001 * 7.150302410125732
Epoch 890, val loss: 1.152457594871521
Epoch 900, training loss: 0.010370331816375256 = 0.003206182038411498 + 0.001 * 7.164149761199951
Epoch 900, val loss: 1.1563889980316162
Epoch 910, training loss: 0.01027675624936819 = 0.003125052200630307 + 0.001 * 7.151703357696533
Epoch 910, val loss: 1.1602617502212524
Epoch 920, training loss: 0.01019467692822218 = 0.003047499805688858 + 0.001 * 7.147176742553711
Epoch 920, val loss: 1.1640174388885498
Epoch 930, training loss: 0.010109156370162964 = 0.002973392140120268 + 0.001 * 7.135763645172119
Epoch 930, val loss: 1.1677464246749878
Epoch 940, training loss: 0.010071555152535439 = 0.0029024553950875998 + 0.001 * 7.1690993309021
Epoch 940, val loss: 1.1713861227035522
Epoch 950, training loss: 0.009978972375392914 = 0.002834555460140109 + 0.001 * 7.144416332244873
Epoch 950, val loss: 1.1749416589736938
Epoch 960, training loss: 0.009884815663099289 = 0.0027694841846823692 + 0.001 * 7.115330696105957
Epoch 960, val loss: 1.1784521341323853
Epoch 970, training loss: 0.009851586073637009 = 0.0027071447111666203 + 0.001 * 7.144441604614258
Epoch 970, val loss: 1.1819032430648804
Epoch 980, training loss: 0.009763062931597233 = 0.0026473633479326963 + 0.001 * 7.11569881439209
Epoch 980, val loss: 1.185273289680481
Epoch 990, training loss: 0.009734170511364937 = 0.002589934039860964 + 0.001 * 7.144235610961914
Epoch 990, val loss: 1.1886035203933716
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 1.957352638244629 = 1.9487557411193848 + 0.001 * 8.596855163574219
Epoch 0, val loss: 1.9511739015579224
Epoch 10, training loss: 1.9472730159759521 = 1.9386762380599976 + 0.001 * 8.596817016601562
Epoch 10, val loss: 1.9415719509124756
Epoch 20, training loss: 1.9346768856048584 = 1.9260802268981934 + 0.001 * 8.596674919128418
Epoch 20, val loss: 1.9290496110916138
Epoch 30, training loss: 1.916758418083191 = 1.9081621170043945 + 0.001 * 8.596343994140625
Epoch 30, val loss: 1.9108967781066895
Epoch 40, training loss: 1.8902500867843628 = 1.8816545009613037 + 0.001 * 8.59556770324707
Epoch 40, val loss: 1.8840548992156982
Epoch 50, training loss: 1.8534958362579346 = 1.844902515411377 + 0.001 * 8.593353271484375
Epoch 50, val loss: 1.8480186462402344
Epoch 60, training loss: 1.813036561012268 = 1.8044512271881104 + 0.001 * 8.585329055786133
Epoch 60, val loss: 1.8116024732589722
Epoch 70, training loss: 1.7795177698135376 = 1.7709659337997437 + 0.001 * 8.55185604095459
Epoch 70, val loss: 1.7828528881072998
Epoch 80, training loss: 1.7377934455871582 = 1.7294305562973022 + 0.001 * 8.362890243530273
Epoch 80, val loss: 1.7442091703414917
Epoch 90, training loss: 1.6781975030899048 = 1.6700572967529297 + 0.001 * 8.140193939208984
Epoch 90, val loss: 1.6906459331512451
Epoch 100, training loss: 1.5970426797866821 = 1.5889949798583984 + 0.001 * 8.047735214233398
Epoch 100, val loss: 1.6200473308563232
Epoch 110, training loss: 1.5018792152404785 = 1.4938827753067017 + 0.001 * 7.996477127075195
Epoch 110, val loss: 1.5404243469238281
Epoch 120, training loss: 1.4069772958755493 = 1.3990540504455566 + 0.001 * 7.923189640045166
Epoch 120, val loss: 1.4640032052993774
Epoch 130, training loss: 1.3174103498458862 = 1.309619665145874 + 0.001 * 7.790736198425293
Epoch 130, val loss: 1.3940366506576538
Epoch 140, training loss: 1.2311171293258667 = 1.2234137058258057 + 0.001 * 7.703420639038086
Epoch 140, val loss: 1.3288289308547974
Epoch 150, training loss: 1.148024082183838 = 1.1403855085372925 + 0.001 * 7.638611316680908
Epoch 150, val loss: 1.2670931816101074
Epoch 160, training loss: 1.0698274374008179 = 1.0622771978378296 + 0.001 * 7.550267696380615
Epoch 160, val loss: 1.2095805406570435
Epoch 170, training loss: 0.9974274635314941 = 0.9899495244026184 + 0.001 * 7.477947235107422
Epoch 170, val loss: 1.1565672159194946
Epoch 180, training loss: 0.9298468828201294 = 0.922409176826477 + 0.001 * 7.437691688537598
Epoch 180, val loss: 1.1066360473632812
Epoch 190, training loss: 0.8644344806671143 = 0.8570079207420349 + 0.001 * 7.426571369171143
Epoch 190, val loss: 1.0580275058746338
Epoch 200, training loss: 0.7985385060310364 = 0.7911188006401062 + 0.001 * 7.419692516326904
Epoch 200, val loss: 1.0088539123535156
Epoch 210, training loss: 0.7313901782035828 = 0.7239805459976196 + 0.001 * 7.4096455574035645
Epoch 210, val loss: 0.958824872970581
Epoch 220, training loss: 0.6647568941116333 = 0.6573612689971924 + 0.001 * 7.395650386810303
Epoch 220, val loss: 0.9100412726402283
Epoch 230, training loss: 0.6021441221237183 = 0.594767689704895 + 0.001 * 7.37640905380249
Epoch 230, val loss: 0.8662849068641663
Epoch 240, training loss: 0.5466490387916565 = 0.5392922759056091 + 0.001 * 7.356735706329346
Epoch 240, val loss: 0.8309531807899475
Epoch 250, training loss: 0.49908843636512756 = 0.4917515218257904 + 0.001 * 7.336908340454102
Epoch 250, val loss: 0.8049429655075073
Epoch 260, training loss: 0.45807865262031555 = 0.4507576823234558 + 0.001 * 7.320982933044434
Epoch 260, val loss: 0.786582887172699
Epoch 270, training loss: 0.42122387886047363 = 0.4139118194580078 + 0.001 * 7.312048435211182
Epoch 270, val loss: 0.7731950283050537
Epoch 280, training loss: 0.3863118588924408 = 0.37900933623313904 + 0.001 * 7.30251932144165
Epoch 280, val loss: 0.7628328800201416
Epoch 290, training loss: 0.35201600193977356 = 0.3447161614894867 + 0.001 * 7.299853801727295
Epoch 290, val loss: 0.7546107769012451
Epoch 300, training loss: 0.3180643618106842 = 0.3107663094997406 + 0.001 * 7.2980499267578125
Epoch 300, val loss: 0.7484275698661804
Epoch 310, training loss: 0.2849719226360321 = 0.277675062417984 + 0.001 * 7.296866416931152
Epoch 310, val loss: 0.7444393634796143
Epoch 320, training loss: 0.2536119222640991 = 0.24631619453430176 + 0.001 * 7.295726776123047
Epoch 320, val loss: 0.7428587079048157
Epoch 330, training loss: 0.2247322052717209 = 0.21743695437908173 + 0.001 * 7.295257091522217
Epoch 330, val loss: 0.7436723113059998
Epoch 340, training loss: 0.19874191284179688 = 0.19144678115844727 + 0.001 * 7.295134544372559
Epoch 340, val loss: 0.7468447089195251
Epoch 350, training loss: 0.17575782537460327 = 0.16846325993537903 + 0.001 * 7.294559001922607
Epoch 350, val loss: 0.7522196173667908
Epoch 360, training loss: 0.15564295649528503 = 0.14834821224212646 + 0.001 * 7.29473876953125
Epoch 360, val loss: 0.7593961358070374
Epoch 370, training loss: 0.13812808692455292 = 0.13083124160766602 + 0.001 * 7.296844482421875
Epoch 370, val loss: 0.7681361436843872
Epoch 380, training loss: 0.12288736552000046 = 0.115590900182724 + 0.001 * 7.296466827392578
Epoch 380, val loss: 0.7781316041946411
Epoch 390, training loss: 0.10962656140327454 = 0.10233002156019211 + 0.001 * 7.296535968780518
Epoch 390, val loss: 0.7892001271247864
Epoch 400, training loss: 0.098082035779953 = 0.09078527241945267 + 0.001 * 7.296762466430664
Epoch 400, val loss: 0.8009615540504456
Epoch 410, training loss: 0.08803152292966843 = 0.08073210716247559 + 0.001 * 7.299412727355957
Epoch 410, val loss: 0.8133195638656616
Epoch 420, training loss: 0.0792778953909874 = 0.07197874784469604 + 0.001 * 7.299148082733154
Epoch 420, val loss: 0.8260540962219238
Epoch 430, training loss: 0.07164919376373291 = 0.06435110419988632 + 0.001 * 7.298088550567627
Epoch 430, val loss: 0.8389788269996643
Epoch 440, training loss: 0.0649927407503128 = 0.057694610208272934 + 0.001 * 7.298130035400391
Epoch 440, val loss: 0.851955235004425
Epoch 450, training loss: 0.0591699592769146 = 0.05187276750802994 + 0.001 * 7.297190189361572
Epoch 450, val loss: 0.8648217916488647
Epoch 460, training loss: 0.05405902490019798 = 0.04676090553402901 + 0.001 * 7.29811954498291
Epoch 460, val loss: 0.8776004910469055
Epoch 470, training loss: 0.049551043659448624 = 0.04225528985261917 + 0.001 * 7.29575252532959
Epoch 470, val loss: 0.8902234435081482
Epoch 480, training loss: 0.04556373506784439 = 0.03826947137713432 + 0.001 * 7.294262409210205
Epoch 480, val loss: 0.9026267528533936
Epoch 490, training loss: 0.042029477655887604 = 0.03473712503910065 + 0.001 * 7.292354106903076
Epoch 490, val loss: 0.914802074432373
Epoch 500, training loss: 0.03892464190721512 = 0.03160649165511131 + 0.001 * 7.318148612976074
Epoch 500, val loss: 0.9267434477806091
Epoch 510, training loss: 0.03612377494573593 = 0.028829902410507202 + 0.001 * 7.293874263763428
Epoch 510, val loss: 0.9383725523948669
Epoch 520, training loss: 0.0336524099111557 = 0.02636606991291046 + 0.001 * 7.286341190338135
Epoch 520, val loss: 0.949773371219635
Epoch 530, training loss: 0.03145861253142357 = 0.024176407605409622 + 0.001 * 7.282204627990723
Epoch 530, val loss: 0.9608134031295776
Epoch 540, training loss: 0.02955479547381401 = 0.02222706377506256 + 0.001 * 7.327731609344482
Epoch 540, val loss: 0.9715797901153564
Epoch 550, training loss: 0.02777617983520031 = 0.02048889920115471 + 0.001 * 7.287280082702637
Epoch 550, val loss: 0.9820020794868469
Epoch 560, training loss: 0.026205724105238914 = 0.018932996317744255 + 0.001 * 7.2727274894714355
Epoch 560, val loss: 0.9921470284461975
Epoch 570, training loss: 0.024800967425107956 = 0.01753338985145092 + 0.001 * 7.267577171325684
Epoch 570, val loss: 1.0019493103027344
Epoch 580, training loss: 0.023538250476121902 = 0.0162692591547966 + 0.001 * 7.2689900398254395
Epoch 580, val loss: 1.0115137100219727
Epoch 590, training loss: 0.022387098520994186 = 0.015125207602977753 + 0.001 * 7.261889457702637
Epoch 590, val loss: 1.0207778215408325
Epoch 600, training loss: 0.021332167088985443 = 0.014088953845202923 + 0.001 * 7.24321174621582
Epoch 600, val loss: 1.0298008918762207
Epoch 610, training loss: 0.020386595278978348 = 0.013149646110832691 + 0.001 * 7.236949920654297
Epoch 610, val loss: 1.038571834564209
Epoch 620, training loss: 0.01954910159111023 = 0.012297432869672775 + 0.001 * 7.251668930053711
Epoch 620, val loss: 1.0470823049545288
Epoch 630, training loss: 0.01874827966094017 = 0.011523343622684479 + 0.001 * 7.224936485290527
Epoch 630, val loss: 1.0553295612335205
Epoch 640, training loss: 0.018076498061418533 = 0.01081913523375988 + 0.001 * 7.2573628425598145
Epoch 640, val loss: 1.0633517503738403
Epoch 650, training loss: 0.017425447702407837 = 0.010177419520914555 + 0.001 * 7.248027801513672
Epoch 650, val loss: 1.0711324214935303
Epoch 660, training loss: 0.01682397536933422 = 0.009591530077159405 + 0.001 * 7.232444763183594
Epoch 660, val loss: 1.078689455986023
Epoch 670, training loss: 0.0162733793258667 = 0.009055565111339092 + 0.001 * 7.217813491821289
Epoch 670, val loss: 1.0860531330108643
Epoch 680, training loss: 0.015782391652464867 = 0.00856438186019659 + 0.001 * 7.218009948730469
Epoch 680, val loss: 1.0931886434555054
Epoch 690, training loss: 0.01532469317317009 = 0.008113289251923561 + 0.001 * 7.2114033699035645
Epoch 690, val loss: 1.1001052856445312
Epoch 700, training loss: 0.014906618744134903 = 0.0076980628073215485 + 0.001 * 7.208555698394775
Epoch 700, val loss: 1.1068273782730103
Epoch 710, training loss: 0.014533399604260921 = 0.0073151299729943275 + 0.001 * 7.218269348144531
Epoch 710, val loss: 1.1133445501327515
Epoch 720, training loss: 0.014165464788675308 = 0.006961352191865444 + 0.001 * 7.204111576080322
Epoch 720, val loss: 1.1196997165679932
Epoch 730, training loss: 0.013849103823304176 = 0.006633960176259279 + 0.001 * 7.21514368057251
Epoch 730, val loss: 1.125872254371643
Epoch 740, training loss: 0.01354210078716278 = 0.0063303872011601925 + 0.001 * 7.2117133140563965
Epoch 740, val loss: 1.1318691968917847
Epoch 750, training loss: 0.013251659460365772 = 0.006048377603292465 + 0.001 * 7.203281402587891
Epoch 750, val loss: 1.137691617012024
Epoch 760, training loss: 0.013002505525946617 = 0.005785970948636532 + 0.001 * 7.21653413772583
Epoch 760, val loss: 1.143344759941101
Epoch 770, training loss: 0.012737208977341652 = 0.00554144661873579 + 0.001 * 7.195761680603027
Epoch 770, val loss: 1.1488419771194458
Epoch 780, training loss: 0.012508553452789783 = 0.005313235800713301 + 0.001 * 7.195317268371582
Epoch 780, val loss: 1.1542038917541504
Epoch 790, training loss: 0.012292006053030491 = 0.005099918227642775 + 0.001 * 7.192087650299072
Epoch 790, val loss: 1.159395456314087
Epoch 800, training loss: 0.012097815982997417 = 0.0049002389423549175 + 0.001 * 7.197576522827148
Epoch 800, val loss: 1.1644773483276367
Epoch 810, training loss: 0.01190418004989624 = 0.004713038448244333 + 0.001 * 7.191140651702881
Epoch 810, val loss: 1.1693965196609497
Epoch 820, training loss: 0.01173468679189682 = 0.004537318367511034 + 0.001 * 7.1973676681518555
Epoch 820, val loss: 1.1741963624954224
Epoch 830, training loss: 0.01156070176512003 = 0.004372144117951393 + 0.001 * 7.188557147979736
Epoch 830, val loss: 1.1788541078567505
Epoch 840, training loss: 0.011399844661355019 = 0.004216679371893406 + 0.001 * 7.183164596557617
Epoch 840, val loss: 1.1834056377410889
Epoch 850, training loss: 0.011267871595919132 = 0.004070207476615906 + 0.001 * 7.1976637840271
Epoch 850, val loss: 1.187835454940796
Epoch 860, training loss: 0.011119459755718708 = 0.003932073246687651 + 0.001 * 7.1873860359191895
Epoch 860, val loss: 1.1921594142913818
Epoch 870, training loss: 0.010990414768457413 = 0.0038016627077013254 + 0.001 * 7.188751697540283
Epoch 870, val loss: 1.1963657140731812
Epoch 880, training loss: 0.010855874978005886 = 0.0036783774849027395 + 0.001 * 7.177496910095215
Epoch 880, val loss: 1.20048189163208
Epoch 890, training loss: 0.01076669804751873 = 0.0035617134999483824 + 0.001 * 7.204984188079834
Epoch 890, val loss: 1.204487919807434
Epoch 900, training loss: 0.010632215067744255 = 0.003451254917308688 + 0.001 * 7.180959701538086
Epoch 900, val loss: 1.2083756923675537
Epoch 910, training loss: 0.010523822158575058 = 0.0033465130254626274 + 0.001 * 7.177308559417725
Epoch 910, val loss: 1.2121987342834473
Epoch 920, training loss: 0.010426770895719528 = 0.0032471150625497103 + 0.001 * 7.1796555519104
Epoch 920, val loss: 1.2158970832824707
Epoch 930, training loss: 0.010346878319978714 = 0.003152675461024046 + 0.001 * 7.194202899932861
Epoch 930, val loss: 1.2195369005203247
Epoch 940, training loss: 0.010228993371129036 = 0.0030628510285168886 + 0.001 * 7.166142463684082
Epoch 940, val loss: 1.2230808734893799
Epoch 950, training loss: 0.010160533711314201 = 0.002977316500619054 + 0.001 * 7.1832170486450195
Epoch 950, val loss: 1.2265268564224243
Epoch 960, training loss: 0.010069720447063446 = 0.0028957724571228027 + 0.001 * 7.173947811126709
Epoch 960, val loss: 1.2298957109451294
Epoch 970, training loss: 0.009973482228815556 = 0.0028178992215543985 + 0.001 * 7.155582904815674
Epoch 970, val loss: 1.2331945896148682
Epoch 980, training loss: 0.00990406796336174 = 0.002743383636698127 + 0.001 * 7.160684108734131
Epoch 980, val loss: 1.2364126443862915
Epoch 990, training loss: 0.009835927747189999 = 0.0026719113811850548 + 0.001 * 7.164016246795654
Epoch 990, val loss: 1.2395647764205933
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8376383763837639
The final CL Acc:0.81111, 0.01571, The final GNN Acc:0.83992, 0.00163
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10552])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9726414680480957 = 1.9640445709228516 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.966246485710144
Epoch 10, training loss: 1.962377667427063 = 1.9537808895111084 + 0.001 * 8.596819877624512
Epoch 10, val loss: 1.9565832614898682
Epoch 20, training loss: 1.9499447345733643 = 1.9413480758666992 + 0.001 * 8.596692085266113
Epoch 20, val loss: 1.9445642232894897
Epoch 30, training loss: 1.9325326681137085 = 1.9239362478256226 + 0.001 * 8.596382141113281
Epoch 30, val loss: 1.927439570426941
Epoch 40, training loss: 1.9066139459609985 = 1.89801824092865 + 0.001 * 8.595656394958496
Epoch 40, val loss: 1.9019198417663574
Epoch 50, training loss: 1.8690791130065918 = 1.8604854345321655 + 0.001 * 8.593729972839355
Epoch 50, val loss: 1.8660130500793457
Epoch 60, training loss: 1.8241243362426758 = 1.8155368566513062 + 0.001 * 8.587512969970703
Epoch 60, val loss: 1.8261021375656128
Epoch 70, training loss: 1.7863526344299316 = 1.77778959274292 + 0.001 * 8.563078880310059
Epoch 70, val loss: 1.794242262840271
Epoch 80, training loss: 1.7484039068222046 = 1.7399907112121582 + 0.001 * 8.413235664367676
Epoch 80, val loss: 1.7571052312850952
Epoch 90, training loss: 1.6966089010238647 = 1.688460350036621 + 0.001 * 8.148528099060059
Epoch 90, val loss: 1.708551049232483
Epoch 100, training loss: 1.6269973516464233 = 1.6190752983093262 + 0.001 * 7.922085285186768
Epoch 100, val loss: 1.6476978063583374
Epoch 110, training loss: 1.5390764474868774 = 1.531373143196106 + 0.001 * 7.703343868255615
Epoch 110, val loss: 1.5719411373138428
Epoch 120, training loss: 1.4403457641601562 = 1.4327235221862793 + 0.001 * 7.622208595275879
Epoch 120, val loss: 1.4897804260253906
Epoch 130, training loss: 1.3406624794006348 = 1.3330705165863037 + 0.001 * 7.591935157775879
Epoch 130, val loss: 1.4096958637237549
Epoch 140, training loss: 1.243276596069336 = 1.2357174158096313 + 0.001 * 7.559121608734131
Epoch 140, val loss: 1.3341020345687866
Epoch 150, training loss: 1.14926278591156 = 1.1417304277420044 + 0.001 * 7.532397270202637
Epoch 150, val loss: 1.2627599239349365
Epoch 160, training loss: 1.059956669807434 = 1.0524544715881348 + 0.001 * 7.502218246459961
Epoch 160, val loss: 1.1957043409347534
Epoch 170, training loss: 0.9765537977218628 = 0.9690821170806885 + 0.001 * 7.471653461456299
Epoch 170, val loss: 1.1337417364120483
Epoch 180, training loss: 0.899614691734314 = 0.8921692967414856 + 0.001 * 7.445399761199951
Epoch 180, val loss: 1.0773135423660278
Epoch 190, training loss: 0.8291447758674622 = 0.8217267990112305 + 0.001 * 7.417961597442627
Epoch 190, val loss: 1.026585578918457
Epoch 200, training loss: 0.7645867466926575 = 0.7572081685066223 + 0.001 * 7.3785881996154785
Epoch 200, val loss: 0.98162841796875
Epoch 210, training loss: 0.7051897644996643 = 0.6978523135185242 + 0.001 * 7.337440490722656
Epoch 210, val loss: 0.9426869750022888
Epoch 220, training loss: 0.6496692299842834 = 0.6423547863960266 + 0.001 * 7.314443111419678
Epoch 220, val loss: 0.9092794060707092
Epoch 230, training loss: 0.5963961482048035 = 0.5890926718711853 + 0.001 * 7.303501605987549
Epoch 230, val loss: 0.880593478679657
Epoch 240, training loss: 0.5441879034042358 = 0.5368885397911072 + 0.001 * 7.2993388175964355
Epoch 240, val loss: 0.8557411432266235
Epoch 250, training loss: 0.49293479323387146 = 0.4856366515159607 + 0.001 * 7.298131465911865
Epoch 250, val loss: 0.8349353075027466
Epoch 260, training loss: 0.44368210434913635 = 0.43638476729393005 + 0.001 * 7.297325134277344
Epoch 260, val loss: 0.8189394474029541
Epoch 270, training loss: 0.39757758378982544 = 0.3902807831764221 + 0.001 * 7.296798229217529
Epoch 270, val loss: 0.808228075504303
Epoch 280, training loss: 0.3551347255706787 = 0.34783846139907837 + 0.001 * 7.296253204345703
Epoch 280, val loss: 0.8025233745574951
Epoch 290, training loss: 0.31631889939308167 = 0.3090215027332306 + 0.001 * 7.297397136688232
Epoch 290, val loss: 0.8013646602630615
Epoch 300, training loss: 0.2809463143348694 = 0.2736493647098541 + 0.001 * 7.296962261199951
Epoch 300, val loss: 0.8039371967315674
Epoch 310, training loss: 0.24887049198150635 = 0.24157477915287018 + 0.001 * 7.295706272125244
Epoch 310, val loss: 0.8099976778030396
Epoch 320, training loss: 0.2199605256319046 = 0.21266353130340576 + 0.001 * 7.296995162963867
Epoch 320, val loss: 0.8190925717353821
Epoch 330, training loss: 0.19408510625362396 = 0.18678870797157288 + 0.001 * 7.296404838562012
Epoch 330, val loss: 0.8307574987411499
Epoch 340, training loss: 0.171098992228508 = 0.1637997180223465 + 0.001 * 7.299277305603027
Epoch 340, val loss: 0.8446344137191772
Epoch 350, training loss: 0.15081942081451416 = 0.14352130889892578 + 0.001 * 7.298110485076904
Epoch 350, val loss: 0.8603057861328125
Epoch 360, training loss: 0.13306596875190735 = 0.1257677972316742 + 0.001 * 7.2981672286987305
Epoch 360, val loss: 0.8773201704025269
Epoch 370, training loss: 0.11762094497680664 = 0.11032237857580185 + 0.001 * 7.298569202423096
Epoch 370, val loss: 0.8953626155853271
Epoch 380, training loss: 0.10424327105283737 = 0.09694486856460571 + 0.001 * 7.298400402069092
Epoch 380, val loss: 0.9141461849212646
Epoch 390, training loss: 0.09268559515476227 = 0.08538416028022766 + 0.001 * 7.3014326095581055
Epoch 390, val loss: 0.9334466457366943
Epoch 400, training loss: 0.08269612491130829 = 0.07539764046669006 + 0.001 * 7.298487663269043
Epoch 400, val loss: 0.9530959725379944
Epoch 410, training loss: 0.07406861335039139 = 0.06676999479532242 + 0.001 * 7.298618316650391
Epoch 410, val loss: 0.9729292988777161
Epoch 420, training loss: 0.066617950797081 = 0.05931542441248894 + 0.001 * 7.302526473999023
Epoch 420, val loss: 0.9928573966026306
Epoch 430, training loss: 0.06017252057790756 = 0.05287312716245651 + 0.001 * 7.299391269683838
Epoch 430, val loss: 1.0127040147781372
Epoch 440, training loss: 0.05459956079721451 = 0.047299619764089584 + 0.001 * 7.299938678741455
Epoch 440, val loss: 1.032334804534912
Epoch 450, training loss: 0.04976900666952133 = 0.04247095808386803 + 0.001 * 7.2980499267578125
Epoch 450, val loss: 1.0517367124557495
Epoch 460, training loss: 0.04557640850543976 = 0.03827943280339241 + 0.001 * 7.296974182128906
Epoch 460, val loss: 1.0707743167877197
Epoch 470, training loss: 0.04192864149808884 = 0.03463231027126312 + 0.001 * 7.296329498291016
Epoch 470, val loss: 1.089381456375122
Epoch 480, training loss: 0.038744863122701645 = 0.03144960477948189 + 0.001 * 7.295256614685059
Epoch 480, val loss: 1.1075090169906616
Epoch 490, training loss: 0.03595767170190811 = 0.02866367995738983 + 0.001 * 7.29399299621582
Epoch 490, val loss: 1.1251236200332642
Epoch 500, training loss: 0.0335119254887104 = 0.026216762140393257 + 0.001 * 7.295162677764893
Epoch 500, val loss: 1.142255187034607
Epoch 510, training loss: 0.03135332465171814 = 0.024060416966676712 + 0.001 * 7.292906761169434
Epoch 510, val loss: 1.1588562726974487
Epoch 520, training loss: 0.029452737420797348 = 0.022153466939926147 + 0.001 * 7.299270153045654
Epoch 520, val loss: 1.1749522686004639
Epoch 530, training loss: 0.027751892805099487 = 0.0204617939889431 + 0.001 * 7.290098190307617
Epoch 530, val loss: 1.1905364990234375
Epoch 540, training loss: 0.02626028284430504 = 0.01895616203546524 + 0.001 * 7.30411958694458
Epoch 540, val loss: 1.2056024074554443
Epoch 550, training loss: 0.024890700355172157 = 0.01761109009385109 + 0.001 * 7.279609680175781
Epoch 550, val loss: 1.2202026844024658
Epoch 560, training loss: 0.023682741448283195 = 0.016405336558818817 + 0.001 * 7.277404308319092
Epoch 560, val loss: 1.2343169450759888
Epoch 570, training loss: 0.022595981135964394 = 0.015320994891226292 + 0.001 * 7.274985313415527
Epoch 570, val loss: 1.2479616403579712
Epoch 580, training loss: 0.021617859601974487 = 0.014342717826366425 + 0.001 * 7.275140762329102
Epoch 580, val loss: 1.2611312866210938
Epoch 590, training loss: 0.020735103636980057 = 0.013457493856549263 + 0.001 * 7.2776103019714355
Epoch 590, val loss: 1.273870825767517
Epoch 600, training loss: 0.01992824114859104 = 0.012654115445911884 + 0.001 * 7.274125576019287
Epoch 600, val loss: 1.2862097024917603
Epoch 610, training loss: 0.019178029149770737 = 0.011922860518097878 + 0.001 * 7.255168437957764
Epoch 610, val loss: 1.298150658607483
Epoch 620, training loss: 0.01853157766163349 = 0.011255449615418911 + 0.001 * 7.276127815246582
Epoch 620, val loss: 1.3097035884857178
Epoch 630, training loss: 0.017913641408085823 = 0.010644735768437386 + 0.001 * 7.268905162811279
Epoch 630, val loss: 1.3208831548690796
Epoch 640, training loss: 0.017346464097499847 = 0.010084593668580055 + 0.001 * 7.261870861053467
Epoch 640, val loss: 1.3317010402679443
Epoch 650, training loss: 0.01681133732199669 = 0.009569767862558365 + 0.001 * 7.241568565368652
Epoch 650, val loss: 1.342177391052246
Epoch 660, training loss: 0.01636158674955368 = 0.00909552164375782 + 0.001 * 7.266064167022705
Epoch 660, val loss: 1.3523544073104858
Epoch 670, training loss: 0.015926936641335487 = 0.008657692931592464 + 0.001 * 7.269243240356445
Epoch 670, val loss: 1.3622273206710815
Epoch 680, training loss: 0.01549316942691803 = 0.008252667263150215 + 0.001 * 7.24050235748291
Epoch 680, val loss: 1.3718044757843018
Epoch 690, training loss: 0.015108907595276833 = 0.007877274416387081 + 0.001 * 7.231632709503174
Epoch 690, val loss: 1.3811105489730835
Epoch 700, training loss: 0.014767535030841827 = 0.007528787944465876 + 0.001 * 7.2387471199035645
Epoch 700, val loss: 1.3901362419128418
Epoch 710, training loss: 0.014414348639547825 = 0.0072047826834023 + 0.001 * 7.20956563949585
Epoch 710, val loss: 1.3988984823226929
Epoch 720, training loss: 0.014111941680312157 = 0.006902872584760189 + 0.001 * 7.20906925201416
Epoch 720, val loss: 1.4074143171310425
Epoch 730, training loss: 0.01385608222335577 = 0.0066210925579071045 + 0.001 * 7.234989166259766
Epoch 730, val loss: 1.4157205820083618
Epoch 740, training loss: 0.013566325418651104 = 0.006357710342854261 + 0.001 * 7.208614826202393
Epoch 740, val loss: 1.4237788915634155
Epoch 750, training loss: 0.01333889551460743 = 0.006111163645982742 + 0.001 * 7.227732181549072
Epoch 750, val loss: 1.431638240814209
Epoch 760, training loss: 0.013087591156363487 = 0.00588004058226943 + 0.001 * 7.207550525665283
Epoch 760, val loss: 1.4392672777175903
Epoch 770, training loss: 0.01291954331099987 = 0.005663060117512941 + 0.001 * 7.2564826011657715
Epoch 770, val loss: 1.4467417001724243
Epoch 780, training loss: 0.01268676109611988 = 0.005459110252559185 + 0.001 * 7.227650165557861
Epoch 780, val loss: 1.453965663909912
Epoch 790, training loss: 0.012459407560527325 = 0.005267174448817968 + 0.001 * 7.192232608795166
Epoch 790, val loss: 1.4610449075698853
Epoch 800, training loss: 0.012282062321901321 = 0.005086282733827829 + 0.001 * 7.195778846740723
Epoch 800, val loss: 1.4679354429244995
Epoch 810, training loss: 0.01209623645991087 = 0.004915622528642416 + 0.001 * 7.1806135177612305
Epoch 810, val loss: 1.4746754169464111
Epoch 820, training loss: 0.011935604736208916 = 0.004754421301186085 + 0.001 * 7.181183338165283
Epoch 820, val loss: 1.4812408685684204
Epoch 830, training loss: 0.011798625811934471 = 0.004602015484124422 + 0.001 * 7.196609973907471
Epoch 830, val loss: 1.4876664876937866
Epoch 840, training loss: 0.011665278114378452 = 0.004457738716155291 + 0.001 * 7.207539081573486
Epoch 840, val loss: 1.4939494132995605
Epoch 850, training loss: 0.011515986174345016 = 0.004321022424846888 + 0.001 * 7.194963455200195
Epoch 850, val loss: 1.5000617504119873
Epoch 860, training loss: 0.011360731907188892 = 0.004191359970718622 + 0.001 * 7.169371604919434
Epoch 860, val loss: 1.50603449344635
Epoch 870, training loss: 0.011238929815590382 = 0.0040682717226445675 + 0.001 * 7.170657634735107
Epoch 870, val loss: 1.511895775794983
Epoch 880, training loss: 0.01112833246588707 = 0.00395130505785346 + 0.001 * 7.177026748657227
Epoch 880, val loss: 1.5176349878311157
Epoch 890, training loss: 0.011007238179445267 = 0.003840075805783272 + 0.001 * 7.1671624183654785
Epoch 890, val loss: 1.5232223272323608
Epoch 900, training loss: 0.010908198542892933 = 0.0037342372816056013 + 0.001 * 7.1739606857299805
Epoch 900, val loss: 1.528721809387207
Epoch 910, training loss: 0.010801782831549644 = 0.0036334297619760036 + 0.001 * 7.168352127075195
Epoch 910, val loss: 1.5340887308120728
Epoch 920, training loss: 0.010744966566562653 = 0.0035373116843402386 + 0.001 * 7.20765495300293
Epoch 920, val loss: 1.539329171180725
Epoch 930, training loss: 0.010604487732052803 = 0.00344564369879663 + 0.001 * 7.158843517303467
Epoch 930, val loss: 1.5445014238357544
Epoch 940, training loss: 0.010545192286372185 = 0.0033581354655325413 + 0.001 * 7.187057018280029
Epoch 940, val loss: 1.5495527982711792
Epoch 950, training loss: 0.010418836027383804 = 0.0032745450735092163 + 0.001 * 7.144290924072266
Epoch 950, val loss: 1.5545319318771362
Epoch 960, training loss: 0.010352101176977158 = 0.003194643883034587 + 0.001 * 7.15745735168457
Epoch 960, val loss: 1.559370994567871
Epoch 970, training loss: 0.010260979644954205 = 0.0031181888189166784 + 0.001 * 7.142790794372559
Epoch 970, val loss: 1.5641683340072632
Epoch 980, training loss: 0.01020045392215252 = 0.0030450166668742895 + 0.001 * 7.155436992645264
Epoch 980, val loss: 1.5688326358795166
Epoch 990, training loss: 0.010107088834047318 = 0.0029748985543847084 + 0.001 * 7.132189750671387
Epoch 990, val loss: 1.573440432548523
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 1.947372555732727 = 1.9387757778167725 + 0.001 * 8.596822738647461
Epoch 0, val loss: 1.9410823583602905
Epoch 10, training loss: 1.9378957748413086 = 1.929298996925354 + 0.001 * 8.596766471862793
Epoch 10, val loss: 1.930943489074707
Epoch 20, training loss: 1.926515817642212 = 1.9179192781448364 + 0.001 * 8.59659194946289
Epoch 20, val loss: 1.918574571609497
Epoch 30, training loss: 1.9110620021820068 = 1.9024658203125 + 0.001 * 8.596221923828125
Epoch 30, val loss: 1.9016802310943604
Epoch 40, training loss: 1.888850450515747 = 1.880255103111267 + 0.001 * 8.595396041870117
Epoch 40, val loss: 1.8776627779006958
Epoch 50, training loss: 1.8575868606567383 = 1.8489935398101807 + 0.001 * 8.593320846557617
Epoch 50, val loss: 1.8450477123260498
Epoch 60, training loss: 1.8199392557144165 = 1.8113518953323364 + 0.001 * 8.587357521057129
Epoch 60, val loss: 1.809478521347046
Epoch 70, training loss: 1.7840358018875122 = 1.7754695415496826 + 0.001 * 8.56624984741211
Epoch 70, val loss: 1.7811572551727295
Epoch 80, training loss: 1.7433077096939087 = 1.7348668575286865 + 0.001 * 8.440906524658203
Epoch 80, val loss: 1.7497845888137817
Epoch 90, training loss: 1.6865042448043823 = 1.6784837245941162 + 0.001 * 8.020546913146973
Epoch 90, val loss: 1.7009024620056152
Epoch 100, training loss: 1.6089094877243042 = 1.6011569499969482 + 0.001 * 7.7525811195373535
Epoch 100, val loss: 1.6340914964675903
Epoch 110, training loss: 1.5105836391448975 = 1.5028990507125854 + 0.001 * 7.6846089363098145
Epoch 110, val loss: 1.552445888519287
Epoch 120, training loss: 1.3989626169204712 = 1.3913124799728394 + 0.001 * 7.6501054763793945
Epoch 120, val loss: 1.4609270095825195
Epoch 130, training loss: 1.2841570377349854 = 1.2765223979949951 + 0.001 * 7.634632587432861
Epoch 130, val loss: 1.3671990633010864
Epoch 140, training loss: 1.1740553379058838 = 1.1664371490478516 + 0.001 * 7.618137836456299
Epoch 140, val loss: 1.2782131433486938
Epoch 150, training loss: 1.071176290512085 = 1.0635839700698853 + 0.001 * 7.592352867126465
Epoch 150, val loss: 1.1953694820404053
Epoch 160, training loss: 0.9750937223434448 = 0.9675569534301758 + 0.001 * 7.536762237548828
Epoch 160, val loss: 1.1198593378067017
Epoch 170, training loss: 0.8867594599723816 = 0.8793190717697144 + 0.001 * 7.440383434295654
Epoch 170, val loss: 1.0532054901123047
Epoch 180, training loss: 0.8080224990844727 = 0.8006038069725037 + 0.001 * 7.418679714202881
Epoch 180, val loss: 0.9972373843193054
Epoch 190, training loss: 0.739006757736206 = 0.7316007614135742 + 0.001 * 7.405979156494141
Epoch 190, val loss: 0.9526069760322571
Epoch 200, training loss: 0.6782047748565674 = 0.6708009839057922 + 0.001 * 7.403776168823242
Epoch 200, val loss: 0.9179876446723938
Epoch 210, training loss: 0.62355637550354 = 0.6161600351333618 + 0.001 * 7.39635705947876
Epoch 210, val loss: 0.8917679190635681
Epoch 220, training loss: 0.573356032371521 = 0.5659703612327576 + 0.001 * 7.385666370391846
Epoch 220, val loss: 0.8721417188644409
Epoch 230, training loss: 0.5264766812324524 = 0.5191078186035156 + 0.001 * 7.368872165679932
Epoch 230, val loss: 0.8577888011932373
Epoch 240, training loss: 0.4823993146419525 = 0.4750504493713379 + 0.001 * 7.348867893218994
Epoch 240, val loss: 0.8476229310035706
Epoch 250, training loss: 0.4411717355251312 = 0.4338573217391968 + 0.001 * 7.314412593841553
Epoch 250, val loss: 0.8419048190116882
Epoch 260, training loss: 0.403166264295578 = 0.39588066935539246 + 0.001 * 7.28558874130249
Epoch 260, val loss: 0.8410824537277222
Epoch 270, training loss: 0.36857783794403076 = 0.36132070422172546 + 0.001 * 7.2571330070495605
Epoch 270, val loss: 0.8450018763542175
Epoch 280, training loss: 0.33725276589393616 = 0.330007940530777 + 0.001 * 7.244812488555908
Epoch 280, val loss: 0.8528666496276855
Epoch 290, training loss: 0.3088119924068451 = 0.3015887439250946 + 0.001 * 7.223258972167969
Epoch 290, val loss: 0.8642032742500305
Epoch 300, training loss: 0.2828613519668579 = 0.27564680576324463 + 0.001 * 7.214539051055908
Epoch 300, val loss: 0.8784828186035156
Epoch 310, training loss: 0.2589089274406433 = 0.25170624256134033 + 0.001 * 7.202680587768555
Epoch 310, val loss: 0.8951394557952881
Epoch 320, training loss: 0.23649543523788452 = 0.22930341958999634 + 0.001 * 7.192022800445557
Epoch 320, val loss: 0.9138872027397156
Epoch 330, training loss: 0.21523873507976532 = 0.2080516219139099 + 0.001 * 7.187106132507324
Epoch 330, val loss: 0.9345415830612183
Epoch 340, training loss: 0.1949358433485031 = 0.18775039911270142 + 0.001 * 7.1854472160339355
Epoch 340, val loss: 0.9568879008293152
Epoch 350, training loss: 0.1755898892879486 = 0.16840951144695282 + 0.001 * 7.180374622344971
Epoch 350, val loss: 0.9807801246643066
Epoch 360, training loss: 0.1573711484670639 = 0.15019315481185913 + 0.001 * 7.177999496459961
Epoch 360, val loss: 1.0061720609664917
Epoch 370, training loss: 0.1404906064271927 = 0.13331162929534912 + 0.001 * 7.1789774894714355
Epoch 370, val loss: 1.0326440334320068
Epoch 380, training loss: 0.12510116398334503 = 0.1179232969880104 + 0.001 * 7.17786169052124
Epoch 380, val loss: 1.0598115921020508
Epoch 390, training loss: 0.11125794798135757 = 0.10408138483762741 + 0.001 * 7.1765594482421875
Epoch 390, val loss: 1.0872327089309692
Epoch 400, training loss: 0.0989295095205307 = 0.09175237268209457 + 0.001 * 7.1771392822265625
Epoch 400, val loss: 1.114486575126648
Epoch 410, training loss: 0.08802752196788788 = 0.08085347712039948 + 0.001 * 7.174046516418457
Epoch 410, val loss: 1.1413443088531494
Epoch 420, training loss: 0.07845225185155869 = 0.07128157466650009 + 0.001 * 7.170675277709961
Epoch 420, val loss: 1.1676077842712402
Epoch 430, training loss: 0.07009657472372055 = 0.06292814016342163 + 0.001 * 7.168437480926514
Epoch 430, val loss: 1.1932684183120728
Epoch 440, training loss: 0.06284809857606888 = 0.05567669868469238 + 0.001 * 7.171398162841797
Epoch 440, val loss: 1.2182093858718872
Epoch 450, training loss: 0.05657217279076576 = 0.049404896795749664 + 0.001 * 7.167276382446289
Epoch 450, val loss: 1.242294430732727
Epoch 460, training loss: 0.0511527955532074 = 0.04398959502577782 + 0.001 * 7.1631999015808105
Epoch 460, val loss: 1.2655014991760254
Epoch 470, training loss: 0.04647475853562355 = 0.03931214660406113 + 0.001 * 7.16261100769043
Epoch 470, val loss: 1.287770390510559
Epoch 480, training loss: 0.04242236912250519 = 0.035264816135168076 + 0.001 * 7.157552242279053
Epoch 480, val loss: 1.309071660041809
Epoch 490, training loss: 0.038919415324926376 = 0.03175368905067444 + 0.001 * 7.165726184844971
Epoch 490, val loss: 1.32947838306427
Epoch 500, training loss: 0.035857945680618286 = 0.028699412941932678 + 0.001 * 7.158531665802002
Epoch 500, val loss: 1.348948359489441
Epoch 510, training loss: 0.033190831542015076 = 0.026034513488411903 + 0.001 * 7.156316757202148
Epoch 510, val loss: 1.367583155632019
Epoch 520, training loss: 0.030853211879730225 = 0.023702533915638924 + 0.001 * 7.150677680969238
Epoch 520, val loss: 1.3854539394378662
Epoch 530, training loss: 0.028805892914533615 = 0.021655123680830002 + 0.001 * 7.1507697105407715
Epoch 530, val loss: 1.4025664329528809
Epoch 540, training loss: 0.026997974142432213 = 0.019852392375469208 + 0.001 * 7.145581245422363
Epoch 540, val loss: 1.4189668893814087
Epoch 550, training loss: 0.025405582040548325 = 0.018260248005390167 + 0.001 * 7.145334720611572
Epoch 550, val loss: 1.4346624612808228
Epoch 560, training loss: 0.024004105478525162 = 0.016849840059876442 + 0.001 * 7.1542649269104
Epoch 560, val loss: 1.449690580368042
Epoch 570, training loss: 0.022742390632629395 = 0.01559667568653822 + 0.001 * 7.145715236663818
Epoch 570, val loss: 1.464121699333191
Epoch 580, training loss: 0.021624915301799774 = 0.0144798057153821 + 0.001 * 7.145108222961426
Epoch 580, val loss: 1.4779325723648071
Epoch 590, training loss: 0.020620137453079224 = 0.013481318019330502 + 0.001 * 7.138820171356201
Epoch 590, val loss: 1.4911816120147705
Epoch 600, training loss: 0.019724298268556595 = 0.012585706077516079 + 0.001 * 7.13859224319458
Epoch 600, val loss: 1.5039010047912598
Epoch 610, training loss: 0.01893722452223301 = 0.011779788881540298 + 0.001 * 7.157434940338135
Epoch 610, val loss: 1.5161083936691284
Epoch 620, training loss: 0.018184956163167953 = 0.01105236541479826 + 0.001 * 7.132589340209961
Epoch 620, val loss: 1.5278146266937256
Epoch 630, training loss: 0.01751563511788845 = 0.010393863543868065 + 0.001 * 7.121771335601807
Epoch 630, val loss: 1.5390862226486206
Epoch 640, training loss: 0.016962261870503426 = 0.009796075522899628 + 0.001 * 7.166186332702637
Epoch 640, val loss: 1.5499272346496582
Epoch 650, training loss: 0.01638202928006649 = 0.009251466020941734 + 0.001 * 7.1305623054504395
Epoch 650, val loss: 1.560351014137268
Epoch 660, training loss: 0.015866776928305626 = 0.008753685280680656 + 0.001 * 7.113091945648193
Epoch 660, val loss: 1.5704336166381836
Epoch 670, training loss: 0.015403972938656807 = 0.008296629413962364 + 0.001 * 7.107343673706055
Epoch 670, val loss: 1.5802218914031982
Epoch 680, training loss: 0.015002690255641937 = 0.007874849252402782 + 0.001 * 7.127840995788574
Epoch 680, val loss: 1.5897629261016846
Epoch 690, training loss: 0.014591716229915619 = 0.0074841477908194065 + 0.001 * 7.107568740844727
Epoch 690, val loss: 1.5990949869155884
Epoch 700, training loss: 0.014234066009521484 = 0.00712121557444334 + 0.001 * 7.112849712371826
Epoch 700, val loss: 1.608256220817566
Epoch 710, training loss: 0.013880403712391853 = 0.0067833964712917805 + 0.001 * 7.097006320953369
Epoch 710, val loss: 1.6172659397125244
Epoch 720, training loss: 0.013563331216573715 = 0.006468548439443111 + 0.001 * 7.094782829284668
Epoch 720, val loss: 1.6261165142059326
Epoch 730, training loss: 0.01326802745461464 = 0.006174838170409203 + 0.001 * 7.093188762664795
Epoch 730, val loss: 1.634803295135498
Epoch 740, training loss: 0.012990977615118027 = 0.0059007094241678715 + 0.001 * 7.090267658233643
Epoch 740, val loss: 1.6433199644088745
Epoch 750, training loss: 0.012734653428196907 = 0.0056446753442287445 + 0.001 * 7.089977264404297
Epoch 750, val loss: 1.6516592502593994
Epoch 760, training loss: 0.012491212226450443 = 0.0054053752683103085 + 0.001 * 7.085836410522461
Epoch 760, val loss: 1.6598057746887207
Epoch 770, training loss: 0.01228346861898899 = 0.005181566812098026 + 0.001 * 7.101901531219482
Epoch 770, val loss: 1.6677613258361816
Epoch 780, training loss: 0.012070918455719948 = 0.004972170107066631 + 0.001 * 7.098748207092285
Epoch 780, val loss: 1.6755561828613281
Epoch 790, training loss: 0.01187230460345745 = 0.004776081070303917 + 0.001 * 7.096222877502441
Epoch 790, val loss: 1.6831414699554443
Epoch 800, training loss: 0.011714393272995949 = 0.004592281766235828 + 0.001 * 7.1221113204956055
Epoch 800, val loss: 1.6905272006988525
Epoch 810, training loss: 0.01149960607290268 = 0.004419838078320026 + 0.001 * 7.079767227172852
Epoch 810, val loss: 1.6977486610412598
Epoch 820, training loss: 0.01134227029979229 = 0.004257910884916782 + 0.001 * 7.0843586921691895
Epoch 820, val loss: 1.70477294921875
Epoch 830, training loss: 0.011219928041100502 = 0.0041056061163544655 + 0.001 * 7.114321231842041
Epoch 830, val loss: 1.7116605043411255
Epoch 840, training loss: 0.011058224365115166 = 0.003962332848459482 + 0.001 * 7.095890998840332
Epoch 840, val loss: 1.7183330059051514
Epoch 850, training loss: 0.01090131513774395 = 0.003827363485470414 + 0.001 * 7.073951244354248
Epoch 850, val loss: 1.7248719930648804
Epoch 860, training loss: 0.01077219471335411 = 0.0037001764867454767 + 0.001 * 7.072018146514893
Epoch 860, val loss: 1.7312315702438354
Epoch 870, training loss: 0.010677590034902096 = 0.003580109216272831 + 0.001 * 7.097480297088623
Epoch 870, val loss: 1.73744535446167
Epoch 880, training loss: 0.010551778599619865 = 0.0034667211584746838 + 0.001 * 7.085056781768799
Epoch 880, val loss: 1.7434731721878052
Epoch 890, training loss: 0.01044350303709507 = 0.003359490539878607 + 0.001 * 7.084012508392334
Epoch 890, val loss: 1.7493877410888672
Epoch 900, training loss: 0.010323954746127129 = 0.0032580632250756025 + 0.001 * 7.065891265869141
Epoch 900, val loss: 1.7551233768463135
Epoch 910, training loss: 0.010225926525890827 = 0.003161913249641657 + 0.001 * 7.0640130043029785
Epoch 910, val loss: 1.7607481479644775
Epoch 920, training loss: 0.01013876311480999 = 0.0030707053374499083 + 0.001 * 7.068057537078857
Epoch 920, val loss: 1.7662180662155151
Epoch 930, training loss: 0.010046105831861496 = 0.0029841535724699497 + 0.001 * 7.061951637268066
Epoch 930, val loss: 1.7715824842453003
Epoch 940, training loss: 0.009971443563699722 = 0.0029019322246313095 + 0.001 * 7.0695109367370605
Epoch 940, val loss: 1.7768179178237915
Epoch 950, training loss: 0.009892704896628857 = 0.0028237844817340374 + 0.001 * 7.068920135498047
Epoch 950, val loss: 1.7818994522094727
Epoch 960, training loss: 0.00981720071285963 = 0.0027494083624333143 + 0.001 * 7.0677924156188965
Epoch 960, val loss: 1.786881923675537
Epoch 970, training loss: 0.009729227051138878 = 0.0026785614900290966 + 0.001 * 7.050665855407715
Epoch 970, val loss: 1.7917653322219849
Epoch 980, training loss: 0.009685985743999481 = 0.0026110210455954075 + 0.001 * 7.07496452331543
Epoch 980, val loss: 1.7965319156646729
Epoch 990, training loss: 0.009597588330507278 = 0.002546648494899273 + 0.001 * 7.050939559936523
Epoch 990, val loss: 1.8011794090270996
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 1.955391764640808 = 1.9467949867248535 + 0.001 * 8.596807479858398
Epoch 0, val loss: 1.9405895471572876
Epoch 10, training loss: 1.945308804512024 = 1.9367120265960693 + 0.001 * 8.596760749816895
Epoch 10, val loss: 1.9309649467468262
Epoch 20, training loss: 1.9331434965133667 = 1.9245469570159912 + 0.001 * 8.59658432006836
Epoch 20, val loss: 1.9188499450683594
Epoch 30, training loss: 1.916063904762268 = 1.9074677228927612 + 0.001 * 8.596173286437988
Epoch 30, val loss: 1.9013484716415405
Epoch 40, training loss: 1.8909249305725098 = 1.8823298215866089 + 0.001 * 8.595114707946777
Epoch 40, val loss: 1.875615119934082
Epoch 50, training loss: 1.8560919761657715 = 1.8475000858306885 + 0.001 * 8.591920852661133
Epoch 50, val loss: 1.8417176008224487
Epoch 60, training loss: 1.8186653852462769 = 1.810085415840149 + 0.001 * 8.579975128173828
Epoch 60, val loss: 1.809813141822815
Epoch 70, training loss: 1.7892036437988281 = 1.780678391456604 + 0.001 * 8.525267601013184
Epoch 70, val loss: 1.787866473197937
Epoch 80, training loss: 1.7543079853057861 = 1.7460598945617676 + 0.001 * 8.248146057128906
Epoch 80, val loss: 1.7580205202102661
Epoch 90, training loss: 1.70619535446167 = 1.6980798244476318 + 0.001 * 8.115473747253418
Epoch 90, val loss: 1.7151862382888794
Epoch 100, training loss: 1.639696717262268 = 1.6317250728607178 + 0.001 * 7.971626281738281
Epoch 100, val loss: 1.6565083265304565
Epoch 110, training loss: 1.557636022567749 = 1.5498361587524414 + 0.001 * 7.799901008605957
Epoch 110, val loss: 1.5854487419128418
Epoch 120, training loss: 1.4700874090194702 = 1.462516188621521 + 0.001 * 7.5712571144104
Epoch 120, val loss: 1.5119800567626953
Epoch 130, training loss: 1.3835407495498657 = 1.3760395050048828 + 0.001 * 7.50124979019165
Epoch 130, val loss: 1.4408342838287354
Epoch 140, training loss: 1.2950570583343506 = 1.2875994443893433 + 0.001 * 7.457586288452148
Epoch 140, val loss: 1.3687646389007568
Epoch 150, training loss: 1.2021763324737549 = 1.1947263479232788 + 0.001 * 7.450034141540527
Epoch 150, val loss: 1.2936625480651855
Epoch 160, training loss: 1.1045727729797363 = 1.097132921218872 + 0.001 * 7.4399094581604
Epoch 160, val loss: 1.2157843112945557
Epoch 170, training loss: 1.0044968128204346 = 0.9970656633377075 + 0.001 * 7.431110858917236
Epoch 170, val loss: 1.1368986368179321
Epoch 180, training loss: 0.9055426120758057 = 0.8981165289878845 + 0.001 * 7.426098346710205
Epoch 180, val loss: 1.0601539611816406
Epoch 190, training loss: 0.8121371865272522 = 0.8047149181365967 + 0.001 * 7.422240734100342
Epoch 190, val loss: 0.989608645439148
Epoch 200, training loss: 0.7277805805206299 = 0.7203618288040161 + 0.001 * 7.418741703033447
Epoch 200, val loss: 0.9295333027839661
Epoch 210, training loss: 0.6541186571121216 = 0.6467027068138123 + 0.001 * 7.415925979614258
Epoch 210, val loss: 0.8818453550338745
Epoch 220, training loss: 0.5908995866775513 = 0.5834860801696777 + 0.001 * 7.41351842880249
Epoch 220, val loss: 0.8465596437454224
Epoch 230, training loss: 0.5368707776069641 = 0.5294594764709473 + 0.001 * 7.411310195922852
Epoch 230, val loss: 0.8219664096832275
Epoch 240, training loss: 0.4905029237270355 = 0.48309388756752014 + 0.001 * 7.4090495109558105
Epoch 240, val loss: 0.8058367371559143
Epoch 250, training loss: 0.45009535551071167 = 0.44268906116485596 + 0.001 * 7.406305313110352
Epoch 250, val loss: 0.7956637740135193
Epoch 260, training loss: 0.4138548970222473 = 0.40645283460617065 + 0.001 * 7.402055740356445
Epoch 260, val loss: 0.7893794775009155
Epoch 270, training loss: 0.380257248878479 = 0.37286102771759033 + 0.001 * 7.396213054656982
Epoch 270, val loss: 0.7856488823890686
Epoch 280, training loss: 0.3482494354248047 = 0.34086474776268005 + 0.001 * 7.384683132171631
Epoch 280, val loss: 0.7835959196090698
Epoch 290, training loss: 0.3172508180141449 = 0.3098844885826111 + 0.001 * 7.366330623626709
Epoch 290, val loss: 0.7829660773277283
Epoch 300, training loss: 0.28711745142936707 = 0.2797650098800659 + 0.001 * 7.352439880371094
Epoch 300, val loss: 0.7837725281715393
Epoch 310, training loss: 0.2580185830593109 = 0.2506897747516632 + 0.001 * 7.328794956207275
Epoch 310, val loss: 0.7862160801887512
Epoch 320, training loss: 0.23034973442554474 = 0.22304733097553253 + 0.001 * 7.302399635314941
Epoch 320, val loss: 0.790550172328949
Epoch 330, training loss: 0.20465056598186493 = 0.19735980033874512 + 0.001 * 7.290760040283203
Epoch 330, val loss: 0.7970308661460876
Epoch 340, training loss: 0.181328684091568 = 0.17405545711517334 + 0.001 * 7.273233890533447
Epoch 340, val loss: 0.8057904243469238
Epoch 350, training loss: 0.16056732833385468 = 0.15330097079277039 + 0.001 * 7.266350269317627
Epoch 350, val loss: 0.816418468952179
Epoch 360, training loss: 0.14227819442749023 = 0.13502465188503265 + 0.001 * 7.253540515899658
Epoch 360, val loss: 0.8286368250846863
Epoch 370, training loss: 0.12626221776008606 = 0.11901626735925674 + 0.001 * 7.245952129364014
Epoch 370, val loss: 0.8421972990036011
Epoch 380, training loss: 0.11226043850183487 = 0.10502354800701141 + 0.001 * 7.236887454986572
Epoch 380, val loss: 0.8568485975265503
Epoch 390, training loss: 0.10004892200231552 = 0.0927993506193161 + 0.001 * 7.249571323394775
Epoch 390, val loss: 0.8723483085632324
Epoch 400, training loss: 0.08936179429292679 = 0.0821327492594719 + 0.001 * 7.229042053222656
Epoch 400, val loss: 0.8884490728378296
Epoch 410, training loss: 0.08005665242671967 = 0.07283833622932434 + 0.001 * 7.218314170837402
Epoch 410, val loss: 0.9049490690231323
Epoch 420, training loss: 0.0719887837767601 = 0.06474479287862778 + 0.001 * 7.243987560272217
Epoch 420, val loss: 0.9216243624687195
Epoch 430, training loss: 0.06491240859031677 = 0.057703740894794464 + 0.001 * 7.208665370941162
Epoch 430, val loss: 0.9382844567298889
Epoch 440, training loss: 0.05878860875964165 = 0.051582545042037964 + 0.001 * 7.206061840057373
Epoch 440, val loss: 0.9548099040985107
Epoch 450, training loss: 0.05347529798746109 = 0.04626170173287392 + 0.001 * 7.213596343994141
Epoch 450, val loss: 0.9711490273475647
Epoch 460, training loss: 0.048837218433618546 = 0.04163556918501854 + 0.001 * 7.201649188995361
Epoch 460, val loss: 0.9871857762336731
Epoch 470, training loss: 0.044809967279434204 = 0.037609852850437164 + 0.001 * 7.200112342834473
Epoch 470, val loss: 1.0029574632644653
Epoch 480, training loss: 0.041328370571136475 = 0.034099023789167404 + 0.001 * 7.229344844818115
Epoch 480, val loss: 1.018318772315979
Epoch 490, training loss: 0.038223639130592346 = 0.031028853729367256 + 0.001 * 7.194786071777344
Epoch 490, val loss: 1.0332542657852173
Epoch 500, training loss: 0.035529494285583496 = 0.0283353291451931 + 0.001 * 7.194162845611572
Epoch 500, val loss: 1.0477545261383057
Epoch 510, training loss: 0.033163152635097504 = 0.02596554160118103 + 0.001 * 7.197608947753906
Epoch 510, val loss: 1.0617859363555908
Epoch 520, training loss: 0.031062932685017586 = 0.02387288212776184 + 0.001 * 7.19005012512207
Epoch 520, val loss: 1.0753703117370605
Epoch 530, training loss: 0.029232000932097435 = 0.022018147632479668 + 0.001 * 7.213852405548096
Epoch 530, val loss: 1.0885170698165894
Epoch 540, training loss: 0.027560163289308548 = 0.02036859840154648 + 0.001 * 7.19156551361084
Epoch 540, val loss: 1.1011929512023926
Epoch 550, training loss: 0.026078591123223305 = 0.0188964381814003 + 0.001 * 7.18215274810791
Epoch 550, val loss: 1.1134952306747437
Epoch 560, training loss: 0.02477451041340828 = 0.0175778865814209 + 0.001 * 7.196623802185059
Epoch 560, val loss: 1.125385046005249
Epoch 570, training loss: 0.023572862148284912 = 0.016392873600125313 + 0.001 * 7.17998743057251
Epoch 570, val loss: 1.1368719339370728
Epoch 580, training loss: 0.022534215822815895 = 0.015324754640460014 + 0.001 * 7.209460735321045
Epoch 580, val loss: 1.1479710340499878
Epoch 590, training loss: 0.021544868126511574 = 0.014359211549162865 + 0.001 * 7.18565559387207
Epoch 590, val loss: 1.1586995124816895
Epoch 600, training loss: 0.020664194598793983 = 0.013483674265444279 + 0.001 * 7.1805195808410645
Epoch 600, val loss: 1.1690690517425537
Epoch 610, training loss: 0.019866930320858955 = 0.012687827460467815 + 0.001 * 7.179101943969727
Epoch 610, val loss: 1.1791183948516846
Epoch 620, training loss: 0.019147951155900955 = 0.011962170712649822 + 0.001 * 7.185779094696045
Epoch 620, val loss: 1.1888347864151
Epoch 630, training loss: 0.018476441502571106 = 0.011298859491944313 + 0.001 * 7.177582740783691
Epoch 630, val loss: 1.1982322931289673
Epoch 640, training loss: 0.017862001433968544 = 0.010691044852137566 + 0.001 * 7.170956134796143
Epoch 640, val loss: 1.2073084115982056
Epoch 650, training loss: 0.017300551757216454 = 0.01013272162526846 + 0.001 * 7.167829990386963
Epoch 650, val loss: 1.216112732887268
Epoch 660, training loss: 0.016809336841106415 = 0.009618920274078846 + 0.001 * 7.19041633605957
Epoch 660, val loss: 1.2246263027191162
Epoch 670, training loss: 0.016317743808031082 = 0.00914499070495367 + 0.001 * 7.17275333404541
Epoch 670, val loss: 1.2328890562057495
Epoch 680, training loss: 0.015882400795817375 = 0.008707039058208466 + 0.001 * 7.175361156463623
Epoch 680, val loss: 1.240916132926941
Epoch 690, training loss: 0.01545606181025505 = 0.00830148160457611 + 0.001 * 7.1545796394348145
Epoch 690, val loss: 1.248705267906189
Epoch 700, training loss: 0.015089821070432663 = 0.007925255224108696 + 0.001 * 7.164565563201904
Epoch 700, val loss: 1.2562528848648071
Epoch 710, training loss: 0.014729530550539494 = 0.007575630210340023 + 0.001 * 7.153900146484375
Epoch 710, val loss: 1.2635406255722046
Epoch 720, training loss: 0.0144041758030653 = 0.007250131573528051 + 0.001 * 7.154044151306152
Epoch 720, val loss: 1.2706518173217773
Epoch 730, training loss: 0.014092322438955307 = 0.006946593057364225 + 0.001 * 7.145729064941406
Epoch 730, val loss: 1.2775431871414185
Epoch 740, training loss: 0.013810116797685623 = 0.006663097068667412 + 0.001 * 7.147019386291504
Epoch 740, val loss: 1.2842280864715576
Epoch 750, training loss: 0.013563808053731918 = 0.006397933233529329 + 0.001 * 7.165874004364014
Epoch 750, val loss: 1.290756344795227
Epoch 760, training loss: 0.013296445831656456 = 0.0061495061963796616 + 0.001 * 7.146938800811768
Epoch 760, val loss: 1.2970679998397827
Epoch 770, training loss: 0.013062737882137299 = 0.005916472990065813 + 0.001 * 7.146265029907227
Epoch 770, val loss: 1.3032158613204956
Epoch 780, training loss: 0.012839014641940594 = 0.0056975590996444225 + 0.001 * 7.141455173492432
Epoch 780, val loss: 1.309200406074524
Epoch 790, training loss: 0.01265636831521988 = 0.005491758231073618 + 0.001 * 7.164610385894775
Epoch 790, val loss: 1.315024495124817
Epoch 800, training loss: 0.012446996755897999 = 0.00529795465990901 + 0.001 * 7.149041652679443
Epoch 800, val loss: 1.320652961730957
Epoch 810, training loss: 0.0122530497610569 = 0.005115231964737177 + 0.001 * 7.137816905975342
Epoch 810, val loss: 1.3261668682098389
Epoch 820, training loss: 0.012077683582901955 = 0.004942766390740871 + 0.001 * 7.134916305541992
Epoch 820, val loss: 1.3315203189849854
Epoch 830, training loss: 0.011928997933864594 = 0.004779808223247528 + 0.001 * 7.149188995361328
Epoch 830, val loss: 1.336728811264038
Epoch 840, training loss: 0.011747559532523155 = 0.004625668283551931 + 0.001 * 7.121891498565674
Epoch 840, val loss: 1.3418322801589966
Epoch 850, training loss: 0.011617421172559261 = 0.0044797249138355255 + 0.001 * 7.137695789337158
Epoch 850, val loss: 1.346800446510315
Epoch 860, training loss: 0.011462409049272537 = 0.00434143329039216 + 0.001 * 7.120975494384766
Epoch 860, val loss: 1.351619839668274
Epoch 870, training loss: 0.011336489580571651 = 0.004210254643112421 + 0.001 * 7.126234531402588
Epoch 870, val loss: 1.3563183546066284
Epoch 880, training loss: 0.011233198456466198 = 0.004085705615580082 + 0.001 * 7.147492408752441
Epoch 880, val loss: 1.3608900308609009
Epoch 890, training loss: 0.011080719530582428 = 0.003967371303588152 + 0.001 * 7.11334753036499
Epoch 890, val loss: 1.3653762340545654
Epoch 900, training loss: 0.010960946790874004 = 0.0038548503071069717 + 0.001 * 7.106096267700195
Epoch 900, val loss: 1.3697216510772705
Epoch 910, training loss: 0.010874358005821705 = 0.0037477638106793165 + 0.001 * 7.126593589782715
Epoch 910, val loss: 1.373957872390747
Epoch 920, training loss: 0.01076444424688816 = 0.003645749296993017 + 0.001 * 7.11869478225708
Epoch 920, val loss: 1.3781182765960693
Epoch 930, training loss: 0.010642575100064278 = 0.0035485050175338984 + 0.001 * 7.094069480895996
Epoch 930, val loss: 1.3821239471435547
Epoch 940, training loss: 0.010560893453657627 = 0.0034557010512799025 + 0.001 * 7.105192184448242
Epoch 940, val loss: 1.3860546350479126
Epoch 950, training loss: 0.010486968792974949 = 0.003367123892530799 + 0.001 * 7.119844436645508
Epoch 950, val loss: 1.3898669481277466
Epoch 960, training loss: 0.01038329303264618 = 0.0032824985682964325 + 0.001 * 7.100794792175293
Epoch 960, val loss: 1.3935991525650024
Epoch 970, training loss: 0.01030318159610033 = 0.0032015854958444834 + 0.001 * 7.101595401763916
Epoch 970, val loss: 1.397210717201233
Epoch 980, training loss: 0.01024260651320219 = 0.0031241814140230417 + 0.001 * 7.118424892425537
Epoch 980, val loss: 1.4007503986358643
Epoch 990, training loss: 0.010153968818485737 = 0.003050076775252819 + 0.001 * 7.103891849517822
Epoch 990, val loss: 1.4042084217071533
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.816025303110174
The final CL Acc:0.75185, 0.02619, The final GNN Acc:0.81304, 0.00212
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13182])
remove edge: torch.Size([2, 7842])
updated graph: torch.Size([2, 10468])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.967948317527771 = 1.9593515396118164 + 0.001 * 8.596831321716309
Epoch 0, val loss: 1.9662662744522095
Epoch 10, training loss: 1.957410454750061 = 1.9488136768341064 + 0.001 * 8.596760749816895
Epoch 10, val loss: 1.9551538228988647
Epoch 20, training loss: 1.944353699684143 = 1.9357571601867676 + 0.001 * 8.59654712677002
Epoch 20, val loss: 1.9415159225463867
Epoch 30, training loss: 1.9260550737380981 = 1.9174590110778809 + 0.001 * 8.596061706542969
Epoch 30, val loss: 1.92250657081604
Epoch 40, training loss: 1.8991286754608154 = 1.8905338048934937 + 0.001 * 8.594869613647461
Epoch 40, val loss: 1.8948743343353271
Epoch 50, training loss: 1.8612810373306274 = 1.8526898622512817 + 0.001 * 8.591164588928223
Epoch 50, val loss: 1.8574090003967285
Epoch 60, training loss: 1.8177064657211304 = 1.8091301918029785 + 0.001 * 8.576329231262207
Epoch 60, val loss: 1.8179364204406738
Epoch 70, training loss: 1.7815377712249756 = 1.773032546043396 + 0.001 * 8.505258560180664
Epoch 70, val loss: 1.7881275415420532
Epoch 80, training loss: 1.7412484884262085 = 1.7330601215362549 + 0.001 * 8.188368797302246
Epoch 80, val loss: 1.7513216733932495
Epoch 90, training loss: 1.6851040124893188 = 1.6771130561828613 + 0.001 * 7.990963935852051
Epoch 90, val loss: 1.7003368139266968
Epoch 100, training loss: 1.609940767288208 = 1.602183222770691 + 0.001 * 7.757572174072266
Epoch 100, val loss: 1.6357886791229248
Epoch 110, training loss: 1.5169481039047241 = 1.5093858242034912 + 0.001 * 7.562259674072266
Epoch 110, val loss: 1.5579601526260376
Epoch 120, training loss: 1.415976643562317 = 1.4084837436676025 + 0.001 * 7.492853164672852
Epoch 120, val loss: 1.4734516143798828
Epoch 130, training loss: 1.3138130903244019 = 1.306363821029663 + 0.001 * 7.44925594329834
Epoch 130, val loss: 1.3883843421936035
Epoch 140, training loss: 1.2117029428482056 = 1.2042721509933472 + 0.001 * 7.4307966232299805
Epoch 140, val loss: 1.3031548261642456
Epoch 150, training loss: 1.111258625984192 = 1.1038390398025513 + 0.001 * 7.419626235961914
Epoch 150, val loss: 1.21902596950531
Epoch 160, training loss: 1.0145272016525269 = 1.0071208477020264 + 0.001 * 7.406323432922363
Epoch 160, val loss: 1.137529969215393
Epoch 170, training loss: 0.9246141314506531 = 0.9172200560569763 + 0.001 * 7.394054889678955
Epoch 170, val loss: 1.0628786087036133
Epoch 180, training loss: 0.8448805212974548 = 0.8374936580657959 + 0.001 * 7.3868489265441895
Epoch 180, val loss: 0.9987400770187378
Epoch 190, training loss: 0.7775617837905884 = 0.7701774835586548 + 0.001 * 7.384306907653809
Epoch 190, val loss: 0.9470016360282898
Epoch 200, training loss: 0.7223778367042542 = 0.7149951457977295 + 0.001 * 7.382691383361816
Epoch 200, val loss: 0.9077494740486145
Epoch 210, training loss: 0.6767331957817078 = 0.6693529486656189 + 0.001 * 7.380275726318359
Epoch 210, val loss: 0.8786081671714783
Epoch 220, training loss: 0.6371386051177979 = 0.6297627687454224 + 0.001 * 7.3758320808410645
Epoch 220, val loss: 0.8559569120407104
Epoch 230, training loss: 0.6002811193466187 = 0.5929132699966431 + 0.001 * 7.367847442626953
Epoch 230, val loss: 0.83661288022995
Epoch 240, training loss: 0.5636836290359497 = 0.5563299059867859 + 0.001 * 7.353733062744141
Epoch 240, val loss: 0.8185921907424927
Epoch 250, training loss: 0.5260785818099976 = 0.5187455415725708 + 0.001 * 7.333033561706543
Epoch 250, val loss: 0.8006948828697205
Epoch 260, training loss: 0.4872724413871765 = 0.4799637198448181 + 0.001 * 7.308712005615234
Epoch 260, val loss: 0.7831507325172424
Epoch 270, training loss: 0.4476063549518585 = 0.4403224587440491 + 0.001 * 7.283898830413818
Epoch 270, val loss: 0.7666378021240234
Epoch 280, training loss: 0.4074244499206543 = 0.4001785218715668 + 0.001 * 7.245938301086426
Epoch 280, val loss: 0.7520952224731445
Epoch 290, training loss: 0.3672257661819458 = 0.3599989116191864 + 0.001 * 7.226863861083984
Epoch 290, val loss: 0.7398860454559326
Epoch 300, training loss: 0.3279110789299011 = 0.3206848204135895 + 0.001 * 7.226258754730225
Epoch 300, val loss: 0.7302888631820679
Epoch 310, training loss: 0.29067569971084595 = 0.28346025943756104 + 0.001 * 7.215445041656494
Epoch 310, val loss: 0.7235955595970154
Epoch 320, training loss: 0.2564952075481415 = 0.24928893148899078 + 0.001 * 7.20626163482666
Epoch 320, val loss: 0.7201100587844849
Epoch 330, training loss: 0.22578470408916473 = 0.2185724824666977 + 0.001 * 7.212214469909668
Epoch 330, val loss: 0.7199442386627197
Epoch 340, training loss: 0.1985422670841217 = 0.19133973121643066 + 0.001 * 7.202536106109619
Epoch 340, val loss: 0.7228217720985413
Epoch 350, training loss: 0.17462781071662903 = 0.16742417216300964 + 0.001 * 7.203634262084961
Epoch 350, val loss: 0.7283426523208618
Epoch 360, training loss: 0.15377788245677948 = 0.14658303558826447 + 0.001 * 7.194847106933594
Epoch 360, val loss: 0.7361506223678589
Epoch 370, training loss: 0.13572509586811066 = 0.12852291762828827 + 0.001 * 7.202178478240967
Epoch 370, val loss: 0.746059775352478
Epoch 380, training loss: 0.12011173367500305 = 0.11291410028934479 + 0.001 * 7.1976318359375
Epoch 380, val loss: 0.757641077041626
Epoch 390, training loss: 0.10662281513214111 = 0.09943205863237381 + 0.001 * 7.190759658813477
Epoch 390, val loss: 0.7705249190330505
Epoch 400, training loss: 0.09497497230768204 = 0.08777594566345215 + 0.001 * 7.199029445648193
Epoch 400, val loss: 0.7843132615089417
Epoch 410, training loss: 0.08487790077924728 = 0.0776888057589531 + 0.001 * 7.189094543457031
Epoch 410, val loss: 0.7988007664680481
Epoch 420, training loss: 0.07612847536802292 = 0.0689457580447197 + 0.001 * 7.182714462280273
Epoch 420, val loss: 0.8136747479438782
Epoch 430, training loss: 0.0685606449842453 = 0.06136012449860573 + 0.001 * 7.200520992279053
Epoch 430, val loss: 0.82871413230896
Epoch 440, training loss: 0.061958178877830505 = 0.05477292463183403 + 0.001 * 7.185254096984863
Epoch 440, val loss: 0.8437961339950562
Epoch 450, training loss: 0.05623461306095123 = 0.049046147614717484 + 0.001 * 7.18846321105957
Epoch 450, val loss: 0.8587685823440552
Epoch 460, training loss: 0.05123906582593918 = 0.0440598763525486 + 0.001 * 7.179189682006836
Epoch 460, val loss: 0.8735610842704773
Epoch 470, training loss: 0.046883415430784225 = 0.03971090167760849 + 0.001 * 7.172513008117676
Epoch 470, val loss: 0.8881145119667053
Epoch 480, training loss: 0.04307703301310539 = 0.035910673439502716 + 0.001 * 7.166358947753906
Epoch 480, val loss: 0.9023575186729431
Epoch 490, training loss: 0.03976501151919365 = 0.03258281573653221 + 0.001 * 7.182196617126465
Epoch 490, val loss: 0.9162634015083313
Epoch 500, training loss: 0.03681676462292671 = 0.029660657048225403 + 0.001 * 7.156108379364014
Epoch 500, val loss: 0.9298344254493713
Epoch 510, training loss: 0.03425479307770729 = 0.027087625116109848 + 0.001 * 7.167168140411377
Epoch 510, val loss: 0.9430605173110962
Epoch 520, training loss: 0.03197373449802399 = 0.02481577731668949 + 0.001 * 7.157957553863525
Epoch 520, val loss: 0.9559023380279541
Epoch 530, training loss: 0.029964877292513847 = 0.022803621366620064 + 0.001 * 7.161255359649658
Epoch 530, val loss: 0.9683637619018555
Epoch 540, training loss: 0.02816115692257881 = 0.021016377955675125 + 0.001 * 7.144777774810791
Epoch 540, val loss: 0.9804248809814453
Epoch 550, training loss: 0.02658589370548725 = 0.019424011930823326 + 0.001 * 7.161881446838379
Epoch 550, val loss: 0.9921103119850159
Epoch 560, training loss: 0.025134865194559097 = 0.018001237884163857 + 0.001 * 7.1336259841918945
Epoch 560, val loss: 1.0034475326538086
Epoch 570, training loss: 0.023864498361945152 = 0.016725849360227585 + 0.001 * 7.13864803314209
Epoch 570, val loss: 1.0144082307815552
Epoch 580, training loss: 0.022719599306583405 = 0.01557923574000597 + 0.001 * 7.140363693237305
Epoch 580, val loss: 1.0250465869903564
Epoch 590, training loss: 0.021675772964954376 = 0.014545343816280365 + 0.001 * 7.130427837371826
Epoch 590, val loss: 1.0353690385818481
Epoch 600, training loss: 0.0207377877086401 = 0.013610610738396645 + 0.001 * 7.127176761627197
Epoch 600, val loss: 1.0453647375106812
Epoch 610, training loss: 0.019903983920812607 = 0.012763289734721184 + 0.001 * 7.1406941413879395
Epoch 610, val loss: 1.0550546646118164
Epoch 620, training loss: 0.019105326384305954 = 0.011992981657385826 + 0.001 * 7.1123433113098145
Epoch 620, val loss: 1.0644384622573853
Epoch 630, training loss: 0.018411699682474136 = 0.011290838941931725 + 0.001 * 7.120860576629639
Epoch 630, val loss: 1.0735487937927246
Epoch 640, training loss: 0.017767267301678658 = 0.010649011470377445 + 0.001 * 7.118256092071533
Epoch 640, val loss: 1.0823702812194824
Epoch 650, training loss: 0.017226368188858032 = 0.010060188360512257 + 0.001 * 7.16618013381958
Epoch 650, val loss: 1.0909181833267212
Epoch 660, training loss: 0.016653764992952347 = 0.009518487378954887 + 0.001 * 7.1352763175964355
Epoch 660, val loss: 1.0992861986160278
Epoch 670, training loss: 0.01611444354057312 = 0.009018615819513798 + 0.001 * 7.095827102661133
Epoch 670, val loss: 1.1074162721633911
Epoch 680, training loss: 0.01567637175321579 = 0.008556080050766468 + 0.001 * 7.120291709899902
Epoch 680, val loss: 1.1153500080108643
Epoch 690, training loss: 0.015241679735481739 = 0.00812724232673645 + 0.001 * 7.114437103271484
Epoch 690, val loss: 1.1231032609939575
Epoch 700, training loss: 0.014842372387647629 = 0.007728899363428354 + 0.001 * 7.1134724617004395
Epoch 700, val loss: 1.1306923627853394
Epoch 710, training loss: 0.014477226883172989 = 0.007358398754149675 + 0.001 * 7.1188273429870605
Epoch 710, val loss: 1.1380733251571655
Epoch 720, training loss: 0.014120957814157009 = 0.007013405207544565 + 0.001 * 7.1075520515441895
Epoch 720, val loss: 1.1453077793121338
Epoch 730, training loss: 0.01377631351351738 = 0.00669193034991622 + 0.001 * 7.0843825340271
Epoch 730, val loss: 1.1523715257644653
Epoch 740, training loss: 0.013489131815731525 = 0.006391784176230431 + 0.001 * 7.097347259521484
Epoch 740, val loss: 1.1592594385147095
Epoch 750, training loss: 0.013206727802753448 = 0.006111093796789646 + 0.001 * 7.095633506774902
Epoch 750, val loss: 1.1660188436508179
Epoch 760, training loss: 0.012950222939252853 = 0.0058484445326030254 + 0.001 * 7.101778507232666
Epoch 760, val loss: 1.1726018190383911
Epoch 770, training loss: 0.012735741212964058 = 0.005602210760116577 + 0.001 * 7.133530139923096
Epoch 770, val loss: 1.1790711879730225
Epoch 780, training loss: 0.012446733191609383 = 0.005370900500565767 + 0.001 * 7.075832843780518
Epoch 780, val loss: 1.1854130029678345
Epoch 790, training loss: 0.01222936436533928 = 0.0051534296944737434 + 0.001 * 7.075933933258057
Epoch 790, val loss: 1.191598892211914
Epoch 800, training loss: 0.012018824927508831 = 0.00494860066100955 + 0.001 * 7.070223808288574
Epoch 800, val loss: 1.1976960897445679
Epoch 810, training loss: 0.011822119355201721 = 0.004755435511469841 + 0.001 * 7.066683292388916
Epoch 810, val loss: 1.203676462173462
Epoch 820, training loss: 0.011668363586068153 = 0.004573221784085035 + 0.001 * 7.095141887664795
Epoch 820, val loss: 1.2095484733581543
Epoch 830, training loss: 0.011475084349513054 = 0.00440128892660141 + 0.001 * 7.073795318603516
Epoch 830, val loss: 1.2152929306030273
Epoch 840, training loss: 0.011319246143102646 = 0.004238877911120653 + 0.001 * 7.0803680419921875
Epoch 840, val loss: 1.220931053161621
Epoch 850, training loss: 0.011167256161570549 = 0.004085435997694731 + 0.001 * 7.081820487976074
Epoch 850, val loss: 1.226461410522461
Epoch 860, training loss: 0.011005701497197151 = 0.003940185531973839 + 0.001 * 7.065515995025635
Epoch 860, val loss: 1.2318735122680664
Epoch 870, training loss: 0.010864006355404854 = 0.003802639665082097 + 0.001 * 7.061366081237793
Epoch 870, val loss: 1.237178087234497
Epoch 880, training loss: 0.010732281021773815 = 0.0036723183002322912 + 0.001 * 7.059962749481201
Epoch 880, val loss: 1.2423763275146484
Epoch 890, training loss: 0.010585034266114235 = 0.0035488682333379984 + 0.001 * 7.036165237426758
Epoch 890, val loss: 1.2474676370620728
Epoch 900, training loss: 0.010478302836418152 = 0.00343180145137012 + 0.001 * 7.046501159667969
Epoch 900, val loss: 1.252458930015564
Epoch 910, training loss: 0.010357850231230259 = 0.0033206844236701727 + 0.001 * 7.037165641784668
Epoch 910, val loss: 1.257333517074585
Epoch 920, training loss: 0.01027856394648552 = 0.0032152875792235136 + 0.001 * 7.0632758140563965
Epoch 920, val loss: 1.2621281147003174
Epoch 930, training loss: 0.010147595778107643 = 0.0031151636503636837 + 0.001 * 7.0324320793151855
Epoch 930, val loss: 1.2667994499206543
Epoch 940, training loss: 0.010077178478240967 = 0.0030200150795280933 + 0.001 * 7.057162761688232
Epoch 940, val loss: 1.2713816165924072
Epoch 950, training loss: 0.009965334087610245 = 0.002929515903815627 + 0.001 * 7.035818099975586
Epoch 950, val loss: 1.2758407592773438
Epoch 960, training loss: 0.0099217863753438 = 0.0028434537816792727 + 0.001 * 7.07833194732666
Epoch 960, val loss: 1.2802549600601196
Epoch 970, training loss: 0.009810497052967548 = 0.0027614871505647898 + 0.001 * 7.049009799957275
Epoch 970, val loss: 1.2845304012298584
Epoch 980, training loss: 0.009714707732200623 = 0.0026834136806428432 + 0.001 * 7.031294345855713
Epoch 980, val loss: 1.2887344360351562
Epoch 990, training loss: 0.009654199704527855 = 0.0026089719031006098 + 0.001 * 7.04522705078125
Epoch 990, val loss: 1.2928484678268433
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.9458208084106445 = 1.93722403049469 + 0.001 * 8.596823692321777
Epoch 0, val loss: 1.930288314819336
Epoch 10, training loss: 1.9356606006622314 = 1.9270638227462769 + 0.001 * 8.596760749816895
Epoch 10, val loss: 1.9201735258102417
Epoch 20, training loss: 1.9231104850769043 = 1.9145139455795288 + 0.001 * 8.596555709838867
Epoch 20, val loss: 1.907243251800537
Epoch 30, training loss: 1.9054962396621704 = 1.8969001770019531 + 0.001 * 8.596071243286133
Epoch 30, val loss: 1.8889302015304565
Epoch 40, training loss: 1.8797630071640015 = 1.8711681365966797 + 0.001 * 8.594849586486816
Epoch 40, val loss: 1.862573266029358
Epoch 50, training loss: 1.844474196434021 = 1.8358830213546753 + 0.001 * 8.59118938446045
Epoch 50, val loss: 1.8283984661102295
Epoch 60, training loss: 1.8049362897872925 = 1.796358585357666 + 0.001 * 8.57773208618164
Epoch 60, val loss: 1.7946114540100098
Epoch 70, training loss: 1.767461895942688 = 1.7589484453201294 + 0.001 * 8.513505935668945
Epoch 70, val loss: 1.7659685611724854
Epoch 80, training loss: 1.719070315361023 = 1.7108805179595947 + 0.001 * 8.189848899841309
Epoch 80, val loss: 1.7251516580581665
Epoch 90, training loss: 1.6512974500656128 = 1.6432368755340576 + 0.001 * 8.060577392578125
Epoch 90, val loss: 1.6655149459838867
Epoch 100, training loss: 1.564670443534851 = 1.5567106008529663 + 0.001 * 7.959889888763428
Epoch 100, val loss: 1.5909266471862793
Epoch 110, training loss: 1.4663804769515991 = 1.4586225748062134 + 0.001 * 7.757876396179199
Epoch 110, val loss: 1.5093797445297241
Epoch 120, training loss: 1.3644261360168457 = 1.356931447982788 + 0.001 * 7.494699954986572
Epoch 120, val loss: 1.4279903173446655
Epoch 130, training loss: 1.2622222900390625 = 1.2547526359558105 + 0.001 * 7.469621181488037
Epoch 130, val loss: 1.3481230735778809
Epoch 140, training loss: 1.161025047302246 = 1.1536026000976562 + 0.001 * 7.422432899475098
Epoch 140, val loss: 1.2705472707748413
Epoch 150, training loss: 1.0637675523757935 = 1.0563639402389526 + 0.001 * 7.403561115264893
Epoch 150, val loss: 1.1969856023788452
Epoch 160, training loss: 0.9723830819129944 = 0.9649866819381714 + 0.001 * 7.396409034729004
Epoch 160, val loss: 1.128379225730896
Epoch 170, training loss: 0.8866187930107117 = 0.8792311549186707 + 0.001 * 7.387610912322998
Epoch 170, val loss: 1.0632723569869995
Epoch 180, training loss: 0.8051250576972961 = 0.797747790813446 + 0.001 * 7.377283096313477
Epoch 180, val loss: 1.0005728006362915
Epoch 190, training loss: 0.7275835871696472 = 0.7202174663543701 + 0.001 * 7.3661041259765625
Epoch 190, val loss: 0.9400689005851746
Epoch 200, training loss: 0.6550422310829163 = 0.6476889848709106 + 0.001 * 7.3532185554504395
Epoch 200, val loss: 0.8837769031524658
Epoch 210, training loss: 0.5885772109031677 = 0.5812395811080933 + 0.001 * 7.337621688842773
Epoch 210, val loss: 0.8343805074691772
Epoch 220, training loss: 0.5282530188560486 = 0.5209355354309082 + 0.001 * 7.3174591064453125
Epoch 220, val loss: 0.7936474680900574
Epoch 230, training loss: 0.4735647141933441 = 0.4662623405456543 + 0.001 * 7.302372932434082
Epoch 230, val loss: 0.7623500227928162
Epoch 240, training loss: 0.424142450094223 = 0.41686660051345825 + 0.001 * 7.275845527648926
Epoch 240, val loss: 0.7396578788757324
Epoch 250, training loss: 0.38011205196380615 = 0.37285158038139343 + 0.001 * 7.260469436645508
Epoch 250, val loss: 0.72427898645401
Epoch 260, training loss: 0.3415249288082123 = 0.33427849411964417 + 0.001 * 7.246442794799805
Epoch 260, val loss: 0.715002179145813
Epoch 270, training loss: 0.30788862705230713 = 0.30065304040908813 + 0.001 * 7.235583782196045
Epoch 270, val loss: 0.7105011343955994
Epoch 280, training loss: 0.27814778685569763 = 0.27092236280441284 + 0.001 * 7.225414276123047
Epoch 280, val loss: 0.7093322277069092
Epoch 290, training loss: 0.25108686089515686 = 0.24387064576148987 + 0.001 * 7.2162089347839355
Epoch 290, val loss: 0.7101240158081055
Epoch 300, training loss: 0.2257588654756546 = 0.21854975819587708 + 0.001 * 7.209106922149658
Epoch 300, val loss: 0.7121258974075317
Epoch 310, training loss: 0.20177392661571503 = 0.19457100331783295 + 0.001 * 7.202916622161865
Epoch 310, val loss: 0.7150339484214783
Epoch 320, training loss: 0.1792796403169632 = 0.17208364605903625 + 0.001 * 7.195997714996338
Epoch 320, val loss: 0.7188714742660522
Epoch 330, training loss: 0.15870633721351624 = 0.15151499211788177 + 0.001 * 7.191349029541016
Epoch 330, val loss: 0.7239892482757568
Epoch 340, training loss: 0.1404259353876114 = 0.13322965800762177 + 0.001 * 7.1962738037109375
Epoch 340, val loss: 0.7306750416755676
Epoch 350, training loss: 0.12449203431606293 = 0.11730663478374481 + 0.001 * 7.18540096282959
Epoch 350, val loss: 0.739105224609375
Epoch 360, training loss: 0.11078104376792908 = 0.10360095649957657 + 0.001 * 7.180087089538574
Epoch 360, val loss: 0.7492932081222534
Epoch 370, training loss: 0.09903557598590851 = 0.0918552428483963 + 0.001 * 7.18033504486084
Epoch 370, val loss: 0.7609770894050598
Epoch 380, training loss: 0.08896408975124359 = 0.08178363740444183 + 0.001 * 7.180450439453125
Epoch 380, val loss: 0.7738593816757202
Epoch 390, training loss: 0.08028409630060196 = 0.0731109008193016 + 0.001 * 7.173196315765381
Epoch 390, val loss: 0.78764808177948
Epoch 400, training loss: 0.07277500629425049 = 0.06560749560594559 + 0.001 * 7.167507171630859
Epoch 400, val loss: 0.8020211458206177
Epoch 410, training loss: 0.06624722480773926 = 0.05908311530947685 + 0.001 * 7.164106369018555
Epoch 410, val loss: 0.8167577385902405
Epoch 420, training loss: 0.06054800748825073 = 0.05338340252637863 + 0.001 * 7.164603233337402
Epoch 420, val loss: 0.8316780924797058
Epoch 430, training loss: 0.05554730072617531 = 0.048383526504039764 + 0.001 * 7.163775444030762
Epoch 430, val loss: 0.8465868830680847
Epoch 440, training loss: 0.05114593729376793 = 0.04398147016763687 + 0.001 * 7.164467811584473
Epoch 440, val loss: 0.8614293932914734
Epoch 450, training loss: 0.04724879190325737 = 0.04009382426738739 + 0.001 * 7.154968738555908
Epoch 450, val loss: 0.8761536478996277
Epoch 460, training loss: 0.043811019510030746 = 0.03665095940232277 + 0.001 * 7.160060882568359
Epoch 460, val loss: 0.8907331824302673
Epoch 470, training loss: 0.040746260434389114 = 0.033594269305467606 + 0.001 * 7.1519904136657715
Epoch 470, val loss: 0.9050847291946411
Epoch 480, training loss: 0.038023609668016434 = 0.030873209238052368 + 0.001 * 7.150400638580322
Epoch 480, val loss: 0.91915363073349
Epoch 490, training loss: 0.03559014946222305 = 0.02844507060945034 + 0.001 * 7.145080089569092
Epoch 490, val loss: 0.9329333305358887
Epoch 500, training loss: 0.03342611342668533 = 0.02627299726009369 + 0.001 * 7.1531147956848145
Epoch 500, val loss: 0.9464377164840698
Epoch 510, training loss: 0.03146500140428543 = 0.02432507462799549 + 0.001 * 7.139926910400391
Epoch 510, val loss: 0.9596189260482788
Epoch 520, training loss: 0.029711417853832245 = 0.02257387340068817 + 0.001 * 7.137545108795166
Epoch 520, val loss: 0.9725191593170166
Epoch 530, training loss: 0.02813754975795746 = 0.02099578082561493 + 0.001 * 7.1417694091796875
Epoch 530, val loss: 0.9850872755050659
Epoch 540, training loss: 0.026716308668255806 = 0.019570516422390938 + 0.001 * 7.145792007446289
Epoch 540, val loss: 0.9973338842391968
Epoch 550, training loss: 0.025420429185032845 = 0.018280306831002235 + 0.001 * 7.1401214599609375
Epoch 550, val loss: 1.0093026161193848
Epoch 560, training loss: 0.024242008104920387 = 0.01710980199277401 + 0.001 * 7.132205963134766
Epoch 560, val loss: 1.0209423303604126
Epoch 570, training loss: 0.023176129907369614 = 0.016045408323407173 + 0.001 * 7.130720615386963
Epoch 570, val loss: 1.0322612524032593
Epoch 580, training loss: 0.02222238853573799 = 0.01507547590881586 + 0.001 * 7.146912574768066
Epoch 580, val loss: 1.0433051586151123
Epoch 590, training loss: 0.021318670362234116 = 0.01418974343687296 + 0.001 * 7.128927707672119
Epoch 590, val loss: 1.0540586709976196
Epoch 600, training loss: 0.02050640806555748 = 0.013379245065152645 + 0.001 * 7.127161502838135
Epoch 600, val loss: 1.064515233039856
Epoch 610, training loss: 0.019768424332141876 = 0.012636198662221432 + 0.001 * 7.132225036621094
Epoch 610, val loss: 1.0747276544570923
Epoch 620, training loss: 0.01908186823129654 = 0.011953569948673248 + 0.001 * 7.128297328948975
Epoch 620, val loss: 1.0846631526947021
Epoch 630, training loss: 0.01845083199441433 = 0.011325214989483356 + 0.001 * 7.125617027282715
Epoch 630, val loss: 1.0943111181259155
Epoch 640, training loss: 0.017863336950540543 = 0.01074574701488018 + 0.001 * 7.117590427398682
Epoch 640, val loss: 1.1037161350250244
Epoch 650, training loss: 0.017347658053040504 = 0.010210328735411167 + 0.001 * 7.137328624725342
Epoch 650, val loss: 1.1128592491149902
Epoch 660, training loss: 0.016842208802700043 = 0.009714841842651367 + 0.001 * 7.127365589141846
Epoch 660, val loss: 1.121773600578308
Epoch 670, training loss: 0.01636986806988716 = 0.009255504235625267 + 0.001 * 7.114363193511963
Epoch 670, val loss: 1.1304409503936768
Epoch 680, training loss: 0.015967149287462234 = 0.008828921243548393 + 0.001 * 7.138227462768555
Epoch 680, val loss: 1.1389002799987793
Epoch 690, training loss: 0.015547838062047958 = 0.008432170376181602 + 0.001 * 7.115667819976807
Epoch 690, val loss: 1.147141456604004
Epoch 700, training loss: 0.015173795633018017 = 0.008062594570219517 + 0.001 * 7.11120080947876
Epoch 700, val loss: 1.1551789045333862
Epoch 710, training loss: 0.01483934372663498 = 0.007717831525951624 + 0.001 * 7.121512413024902
Epoch 710, val loss: 1.1630103588104248
Epoch 720, training loss: 0.014515393413603306 = 0.0073957741260528564 + 0.001 * 7.119618892669678
Epoch 720, val loss: 1.1706405878067017
Epoch 730, training loss: 0.01420225016772747 = 0.0070944796316325665 + 0.001 * 7.107769966125488
Epoch 730, val loss: 1.1780987977981567
Epoch 740, training loss: 0.01392088271677494 = 0.0068122814409434795 + 0.001 * 7.108601093292236
Epoch 740, val loss: 1.1853604316711426
Epoch 750, training loss: 0.013653461821377277 = 0.006547590252012014 + 0.001 * 7.105871200561523
Epoch 750, val loss: 1.1924489736557007
Epoch 760, training loss: 0.013396112248301506 = 0.00629902770742774 + 0.001 * 7.097084045410156
Epoch 760, val loss: 1.1993637084960938
Epoch 770, training loss: 0.013170948252081871 = 0.006065314169973135 + 0.001 * 7.10563325881958
Epoch 770, val loss: 1.2061185836791992
Epoch 780, training loss: 0.01294679380953312 = 0.005845296662300825 + 0.001 * 7.10149621963501
Epoch 780, val loss: 1.2126935720443726
Epoch 790, training loss: 0.012739698402583599 = 0.005637963768094778 + 0.001 * 7.101734161376953
Epoch 790, val loss: 1.2191390991210938
Epoch 800, training loss: 0.012539312243461609 = 0.005442358087748289 + 0.001 * 7.096953392028809
Epoch 800, val loss: 1.2254120111465454
Epoch 810, training loss: 0.012356165796518326 = 0.005257604643702507 + 0.001 * 7.098560333251953
Epoch 810, val loss: 1.231554388999939
Epoch 820, training loss: 0.012178333476185799 = 0.00508295651525259 + 0.001 * 7.095376968383789
Epoch 820, val loss: 1.2375459671020508
Epoch 830, training loss: 0.012003522366285324 = 0.00491765420883894 + 0.001 * 7.085867881774902
Epoch 830, val loss: 1.2434006929397583
Epoch 840, training loss: 0.011865807697176933 = 0.004761059768497944 + 0.001 * 7.104748249053955
Epoch 840, val loss: 1.2491074800491333
Epoch 850, training loss: 0.011700000613927841 = 0.004612590651959181 + 0.001 * 7.087409973144531
Epoch 850, val loss: 1.2547016143798828
Epoch 860, training loss: 0.011580541729927063 = 0.004471711814403534 + 0.001 * 7.108829498291016
Epoch 860, val loss: 1.2601600885391235
Epoch 870, training loss: 0.011425936594605446 = 0.004337903577834368 + 0.001 * 7.088033199310303
Epoch 870, val loss: 1.2655055522918701
Epoch 880, training loss: 0.011292427778244019 = 0.004210697021335363 + 0.001 * 7.081730842590332
Epoch 880, val loss: 1.2707222700119019
Epoch 890, training loss: 0.0111834816634655 = 0.004089686553925276 + 0.001 * 7.093794345855713
Epoch 890, val loss: 1.2758285999298096
Epoch 900, training loss: 0.011062372475862503 = 0.003974474500864744 + 0.001 * 7.087898254394531
Epoch 900, val loss: 1.2808095216751099
Epoch 910, training loss: 0.010937407612800598 = 0.003864699974656105 + 0.001 * 7.072706699371338
Epoch 910, val loss: 1.285688877105713
Epoch 920, training loss: 0.010835245251655579 = 0.003760017454624176 + 0.001 * 7.075226783752441
Epoch 920, val loss: 1.2904601097106934
Epoch 930, training loss: 0.010736280120909214 = 0.0036601314786821604 + 0.001 * 7.076148509979248
Epoch 930, val loss: 1.2951356172561646
Epoch 940, training loss: 0.01062830537557602 = 0.0035647451877593994 + 0.001 * 7.0635600090026855
Epoch 940, val loss: 1.299696922302246
Epoch 950, training loss: 0.010559453628957272 = 0.0034735773224383593 + 0.001 * 7.085875511169434
Epoch 950, val loss: 1.3041763305664062
Epoch 960, training loss: 0.01047468837350607 = 0.0033863915596157312 + 0.001 * 7.088296413421631
Epoch 960, val loss: 1.3085532188415527
Epoch 970, training loss: 0.01036115176975727 = 0.0033029834739863873 + 0.001 * 7.058167457580566
Epoch 970, val loss: 1.3128644227981567
Epoch 980, training loss: 0.010282199829816818 = 0.003223131177946925 + 0.001 * 7.05906867980957
Epoch 980, val loss: 1.3170603513717651
Epoch 990, training loss: 0.01020093820989132 = 0.0031466432847082615 + 0.001 * 7.054295063018799
Epoch 990, val loss: 1.3211746215820312
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.950644850730896 = 1.9420479536056519 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.9354673624038696
Epoch 10, training loss: 1.9407036304473877 = 1.932106852531433 + 0.001 * 8.596769332885742
Epoch 10, val loss: 1.926391363143921
Epoch 20, training loss: 1.9280102252960205 = 1.919413685798645 + 0.001 * 8.596546173095703
Epoch 20, val loss: 1.914521336555481
Epoch 30, training loss: 1.9097257852554321 = 1.9011298418045044 + 0.001 * 8.595985412597656
Epoch 30, val loss: 1.8973060846328735
Epoch 40, training loss: 1.8824982643127441 = 1.873903751373291 + 0.001 * 8.594484329223633
Epoch 40, val loss: 1.8721632957458496
Epoch 50, training loss: 1.845227837562561 = 1.8366378545761108 + 0.001 * 8.58998966217041
Epoch 50, val loss: 1.8398773670196533
Epoch 60, training loss: 1.8053679466247559 = 1.7967941761016846 + 0.001 * 8.573824882507324
Epoch 60, val loss: 1.8086563348770142
Epoch 70, training loss: 1.768204689025879 = 1.7597073316574097 + 0.001 * 8.497342109680176
Epoch 70, val loss: 1.7764323949813843
Epoch 80, training loss: 1.7171454429626465 = 1.7089781761169434 + 0.001 * 8.1673002243042
Epoch 80, val loss: 1.7262274026870728
Epoch 90, training loss: 1.6470348834991455 = 1.6389704942703247 + 0.001 * 8.064388275146484
Epoch 90, val loss: 1.6623594760894775
Epoch 100, training loss: 1.5596730709075928 = 1.551637053489685 + 0.001 * 8.035961151123047
Epoch 100, val loss: 1.5888084173202515
Epoch 110, training loss: 1.4714281558990479 = 1.4633997678756714 + 0.001 * 8.02839469909668
Epoch 110, val loss: 1.5148770809173584
Epoch 120, training loss: 1.3917031288146973 = 1.3836764097213745 + 0.001 * 8.026761054992676
Epoch 120, val loss: 1.4495033025741577
Epoch 130, training loss: 1.320138692855835 = 1.3121131658554077 + 0.001 * 8.025550842285156
Epoch 130, val loss: 1.3923743963241577
Epoch 140, training loss: 1.2555434703826904 = 1.2475239038467407 + 0.001 * 8.019598007202148
Epoch 140, val loss: 1.3425476551055908
Epoch 150, training loss: 1.1965047121047974 = 1.1885091066360474 + 0.001 * 7.9955830574035645
Epoch 150, val loss: 1.299685001373291
Epoch 160, training loss: 1.1386590003967285 = 1.130768060684204 + 0.001 * 7.89089298248291
Epoch 160, val loss: 1.2600048780441284
Epoch 170, training loss: 1.0766462087631226 = 1.0690240859985352 + 0.001 * 7.622114658355713
Epoch 170, val loss: 1.2172521352767944
Epoch 180, training loss: 1.0067776441574097 = 0.999297559261322 + 0.001 * 7.480051517486572
Epoch 180, val loss: 1.1672840118408203
Epoch 190, training loss: 0.9278061389923096 = 0.9203641414642334 + 0.001 * 7.441974639892578
Epoch 190, val loss: 1.1094510555267334
Epoch 200, training loss: 0.8416318893432617 = 0.8341941833496094 + 0.001 * 7.437713146209717
Epoch 200, val loss: 1.0457344055175781
Epoch 210, training loss: 0.7530490756034851 = 0.745622456073761 + 0.001 * 7.42661714553833
Epoch 210, val loss: 0.9802528619766235
Epoch 220, training loss: 0.6675237417221069 = 0.6601086854934692 + 0.001 * 7.41503381729126
Epoch 220, val loss: 0.9178723096847534
Epoch 230, training loss: 0.5889204740524292 = 0.5815193057060242 + 0.001 * 7.401169300079346
Epoch 230, val loss: 0.8620443344116211
Epoch 240, training loss: 0.5186900496482849 = 0.5113064050674438 + 0.001 * 7.383630275726318
Epoch 240, val loss: 0.8147136569023132
Epoch 250, training loss: 0.45640191435813904 = 0.4490432143211365 + 0.001 * 7.358706951141357
Epoch 250, val loss: 0.7759808301925659
Epoch 260, training loss: 0.4009229838848114 = 0.39358851313591003 + 0.001 * 7.334474563598633
Epoch 260, val loss: 0.745067834854126
Epoch 270, training loss: 0.3513128161430359 = 0.3439996540546417 + 0.001 * 7.313161849975586
Epoch 270, val loss: 0.7206454277038574
Epoch 280, training loss: 0.30705541372299194 = 0.29975438117980957 + 0.001 * 7.301017761230469
Epoch 280, val loss: 0.7015693187713623
Epoch 290, training loss: 0.26791754364967346 = 0.26062071323394775 + 0.001 * 7.296837329864502
Epoch 290, val loss: 0.6871117949485779
Epoch 300, training loss: 0.2337140142917633 = 0.226419135928154 + 0.001 * 7.2948784828186035
Epoch 300, val loss: 0.6767995953559875
Epoch 310, training loss: 0.20414121448993683 = 0.19684775173664093 + 0.001 * 7.293460369110107
Epoch 310, val loss: 0.6702700853347778
Epoch 320, training loss: 0.17876064777374268 = 0.1714676022529602 + 0.001 * 7.2930450439453125
Epoch 320, val loss: 0.6670215129852295
Epoch 330, training loss: 0.15705606341362 = 0.14976267516613007 + 0.001 * 7.293392181396484
Epoch 330, val loss: 0.6665545105934143
Epoch 340, training loss: 0.13849689066410065 = 0.13120359182357788 + 0.001 * 7.2932915687561035
Epoch 340, val loss: 0.6683982610702515
Epoch 350, training loss: 0.12259769439697266 = 0.11530473828315735 + 0.001 * 7.292952537536621
Epoch 350, val loss: 0.6722230315208435
Epoch 360, training loss: 0.10894545167684555 = 0.1016521230340004 + 0.001 * 7.293327808380127
Epoch 360, val loss: 0.677666425704956
Epoch 370, training loss: 0.09718840569257736 = 0.08989404886960983 + 0.001 * 7.294357776641846
Epoch 370, val loss: 0.6844280958175659
Epoch 380, training loss: 0.08703180402517319 = 0.0797380805015564 + 0.001 * 7.293721675872803
Epoch 380, val loss: 0.6923134922981262
Epoch 390, training loss: 0.07823827117681503 = 0.07094305753707886 + 0.001 * 7.295214653015137
Epoch 390, val loss: 0.7010642290115356
Epoch 400, training loss: 0.07060325145721436 = 0.06330866366624832 + 0.001 * 7.294588565826416
Epoch 400, val loss: 0.7105095982551575
Epoch 410, training loss: 0.06396225839853287 = 0.0566679984331131 + 0.001 * 7.2942609786987305
Epoch 410, val loss: 0.7204239368438721
Epoch 420, training loss: 0.05818093940615654 = 0.050880350172519684 + 0.001 * 7.300590515136719
Epoch 420, val loss: 0.7306435704231262
Epoch 430, training loss: 0.05312231183052063 = 0.04582624137401581 + 0.001 * 7.296071529388428
Epoch 430, val loss: 0.741016149520874
Epoch 440, training loss: 0.04869780316948891 = 0.04140330106019974 + 0.001 * 7.294501781463623
Epoch 440, val loss: 0.7514316439628601
Epoch 450, training loss: 0.044818464666604996 = 0.03752382844686508 + 0.001 * 7.29463529586792
Epoch 450, val loss: 0.7617872357368469
Epoch 460, training loss: 0.041407614946365356 = 0.03411264345049858 + 0.001 * 7.294971466064453
Epoch 460, val loss: 0.7720392942428589
Epoch 470, training loss: 0.03840203955769539 = 0.03110627271234989 + 0.001 * 7.295765399932861
Epoch 470, val loss: 0.7821483612060547
Epoch 480, training loss: 0.03574367240071297 = 0.02844982221722603 + 0.001 * 7.293848514556885
Epoch 480, val loss: 0.792099118232727
Epoch 490, training loss: 0.03339284658432007 = 0.026096375659108162 + 0.001 * 7.296472549438477
Epoch 490, val loss: 0.8018563985824585
Epoch 500, training loss: 0.03129810839891434 = 0.024005582556128502 + 0.001 * 7.292524337768555
Epoch 500, val loss: 0.8113954067230225
Epoch 510, training loss: 0.02943328768014908 = 0.02214297465980053 + 0.001 * 7.29031229019165
Epoch 510, val loss: 0.8207168579101562
Epoch 520, training loss: 0.027777371928095818 = 0.02047910913825035 + 0.001 * 7.298262596130371
Epoch 520, val loss: 0.8297997713088989
Epoch 530, training loss: 0.026278922334313393 = 0.018988577648997307 + 0.001 * 7.290344715118408
Epoch 530, val loss: 0.8386467695236206
Epoch 540, training loss: 0.024937991052865982 = 0.017650026828050613 + 0.001 * 7.287963390350342
Epoch 540, val loss: 0.8472691774368286
Epoch 550, training loss: 0.023738130927085876 = 0.01644478179514408 + 0.001 * 7.293349266052246
Epoch 550, val loss: 0.855643093585968
Epoch 560, training loss: 0.02264087274670601 = 0.015356620773673058 + 0.001 * 7.284251689910889
Epoch 560, val loss: 0.8637763857841492
Epoch 570, training loss: 0.021654855459928513 = 0.01437163446098566 + 0.001 * 7.28322172164917
Epoch 570, val loss: 0.871684730052948
Epoch 580, training loss: 0.02075563371181488 = 0.013477860949933529 + 0.001 * 7.277771472930908
Epoch 580, val loss: 0.8793696165084839
Epoch 590, training loss: 0.019952556118369102 = 0.012664947658777237 + 0.001 * 7.2876081466674805
Epoch 590, val loss: 0.8868312835693359
Epoch 600, training loss: 0.01920023374259472 = 0.011923701502382755 + 0.001 * 7.27653169631958
Epoch 600, val loss: 0.8940573930740356
Epoch 610, training loss: 0.018548524007201195 = 0.01124635897576809 + 0.001 * 7.302164554595947
Epoch 610, val loss: 0.9010873436927795
Epoch 620, training loss: 0.017899995669722557 = 0.010626031085848808 + 0.001 * 7.2739644050598145
Epoch 620, val loss: 0.9079294204711914
Epoch 630, training loss: 0.017321016639471054 = 0.010056734085083008 + 0.001 * 7.264281749725342
Epoch 630, val loss: 0.914574921131134
Epoch 640, training loss: 0.01679772511124611 = 0.009533182717859745 + 0.001 * 7.264543056488037
Epoch 640, val loss: 0.9210423827171326
Epoch 650, training loss: 0.016319425776600838 = 0.00905072595924139 + 0.001 * 7.268700122833252
Epoch 650, val loss: 0.9273301959037781
Epoch 660, training loss: 0.015873940661549568 = 0.008605255745351315 + 0.001 * 7.268684387207031
Epoch 660, val loss: 0.9334520697593689
Epoch 670, training loss: 0.015435226261615753 = 0.008193195797502995 + 0.001 * 7.242030143737793
Epoch 670, val loss: 0.9394133687019348
Epoch 680, training loss: 0.015044135972857475 = 0.00781136192381382 + 0.001 * 7.232773780822754
Epoch 680, val loss: 0.9452294707298279
Epoch 690, training loss: 0.014693650417029858 = 0.00745690381154418 + 0.001 * 7.236746311187744
Epoch 690, val loss: 0.9508870244026184
Epoch 700, training loss: 0.01438136212527752 = 0.007127349730581045 + 0.001 * 7.254011631011963
Epoch 700, val loss: 0.9563887119293213
Epoch 710, training loss: 0.014052780345082283 = 0.006820437964051962 + 0.001 * 7.23234224319458
Epoch 710, val loss: 0.9617630839347839
Epoch 720, training loss: 0.013748080469667912 = 0.006534172687679529 + 0.001 * 7.213907241821289
Epoch 720, val loss: 0.9669954776763916
Epoch 730, training loss: 0.013481752946972847 = 0.006266736425459385 + 0.001 * 7.215015888214111
Epoch 730, val loss: 0.9721060395240784
Epoch 740, training loss: 0.013247467577457428 = 0.0060165696777403355 + 0.001 * 7.230896949768066
Epoch 740, val loss: 0.9770709276199341
Epoch 750, training loss: 0.012993041425943375 = 0.0057822223752737045 + 0.001 * 7.210819244384766
Epoch 750, val loss: 0.9819318056106567
Epoch 760, training loss: 0.012799765914678574 = 0.005562352482229471 + 0.001 * 7.237412929534912
Epoch 760, val loss: 0.9866677522659302
Epoch 770, training loss: 0.012578184716403484 = 0.005355823319405317 + 0.001 * 7.222361087799072
Epoch 770, val loss: 0.9912989735603333
Epoch 780, training loss: 0.012357071042060852 = 0.005161612760275602 + 0.001 * 7.195457458496094
Epoch 780, val loss: 0.9957998991012573
Epoch 790, training loss: 0.012170422822237015 = 0.004978763870894909 + 0.001 * 7.1916584968566895
Epoch 790, val loss: 1.0001955032348633
Epoch 800, training loss: 0.01199459657073021 = 0.004806425888091326 + 0.001 * 7.188170433044434
Epoch 800, val loss: 1.0044970512390137
Epoch 810, training loss: 0.011833778582513332 = 0.004643755033612251 + 0.001 * 7.190023422241211
Epoch 810, val loss: 1.0087083578109741
Epoch 820, training loss: 0.011683657765388489 = 0.004489943850785494 + 0.001 * 7.193713188171387
Epoch 820, val loss: 1.012819528579712
Epoch 830, training loss: 0.01154879666864872 = 0.004343938082456589 + 0.001 * 7.204858779907227
Epoch 830, val loss: 1.0168585777282715
Epoch 840, training loss: 0.011383176781237125 = 0.004204713739454746 + 0.001 * 7.178462505340576
Epoch 840, val loss: 1.0208122730255127
Epoch 850, training loss: 0.011240480467677116 = 0.004071214701980352 + 0.001 * 7.169265270233154
Epoch 850, val loss: 1.0247385501861572
Epoch 860, training loss: 0.011153425090014935 = 0.003942604176700115 + 0.001 * 7.21082067489624
Epoch 860, val loss: 1.0286462306976318
Epoch 870, training loss: 0.010988612659275532 = 0.0038185089360922575 + 0.001 * 7.170103549957275
Epoch 870, val loss: 1.0325435400009155
Epoch 880, training loss: 0.010890456847846508 = 0.0036988204810768366 + 0.001 * 7.191636085510254
Epoch 880, val loss: 1.0364429950714111
Epoch 890, training loss: 0.010741852223873138 = 0.003583593526855111 + 0.001 * 7.158257961273193
Epoch 890, val loss: 1.0402865409851074
Epoch 900, training loss: 0.010626834817230701 = 0.003472856478765607 + 0.001 * 7.153977870941162
Epoch 900, val loss: 1.0441566705703735
Epoch 910, training loss: 0.01053826417773962 = 0.0033666137605905533 + 0.001 * 7.171649932861328
Epoch 910, val loss: 1.047971248626709
Epoch 920, training loss: 0.0104322275146842 = 0.003264859551563859 + 0.001 * 7.167367935180664
Epoch 920, val loss: 1.051733136177063
Epoch 930, training loss: 0.010322192683815956 = 0.0031675277277827263 + 0.001 * 7.154664516448975
Epoch 930, val loss: 1.0554687976837158
Epoch 940, training loss: 0.010236489586532116 = 0.003074534237384796 + 0.001 * 7.161954879760742
Epoch 940, val loss: 1.0591316223144531
Epoch 950, training loss: 0.010146189481019974 = 0.0029857426416128874 + 0.001 * 7.1604461669921875
Epoch 950, val loss: 1.062747597694397
Epoch 960, training loss: 0.010041841305792332 = 0.002900941064581275 + 0.001 * 7.140900135040283
Epoch 960, val loss: 1.06631600856781
Epoch 970, training loss: 0.009983949363231659 = 0.0028199644293636084 + 0.001 * 7.163984298706055
Epoch 970, val loss: 1.0698304176330566
Epoch 980, training loss: 0.009895776398479939 = 0.0027426511514931917 + 0.001 * 7.153125286102295
Epoch 980, val loss: 1.0732721090316772
Epoch 990, training loss: 0.009808095172047615 = 0.002668827073648572 + 0.001 * 7.139267444610596
Epoch 990, val loss: 1.076663613319397
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8360569319978914
The final CL Acc:0.80494, 0.00175, The final GNN Acc:0.83746, 0.00099
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10562])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9639089107513428 = 1.9553121328353882 + 0.001 * 8.596780776977539
Epoch 0, val loss: 1.9517616033554077
Epoch 10, training loss: 1.953964352607727 = 1.9453675746917725 + 0.001 * 8.596718788146973
Epoch 10, val loss: 1.9414836168289185
Epoch 20, training loss: 1.9413464069366455 = 1.93274986743927 + 0.001 * 8.59652042388916
Epoch 20, val loss: 1.9283697605133057
Epoch 30, training loss: 1.9231021404266357 = 1.9145060777664185 + 0.001 * 8.596101760864258
Epoch 30, val loss: 1.9097027778625488
Epoch 40, training loss: 1.8955559730529785 = 1.886960744857788 + 0.001 * 8.59518814086914
Epoch 40, val loss: 1.882264256477356
Epoch 50, training loss: 1.8568859100341797 = 1.8482931852340698 + 0.001 * 8.592753410339355
Epoch 50, val loss: 1.8458983898162842
Epoch 60, training loss: 1.814113974571228 = 1.8055297136306763 + 0.001 * 8.584269523620605
Epoch 60, val loss: 1.8100314140319824
Epoch 70, training loss: 1.777320384979248 = 1.7687791585922241 + 0.001 * 8.541247367858887
Epoch 70, val loss: 1.7812544107437134
Epoch 80, training loss: 1.731224536895752 = 1.7229840755462646 + 0.001 * 8.240455627441406
Epoch 80, val loss: 1.7398219108581543
Epoch 90, training loss: 1.6675927639007568 = 1.6595218181610107 + 0.001 * 8.071002960205078
Epoch 90, val loss: 1.6822458505630493
Epoch 100, training loss: 1.5831297636032104 = 1.575127363204956 + 0.001 * 8.00238037109375
Epoch 100, val loss: 1.609645962715149
Epoch 110, training loss: 1.48723304271698 = 1.4793180227279663 + 0.001 * 7.915071964263916
Epoch 110, val loss: 1.529658555984497
Epoch 120, training loss: 1.3921804428100586 = 1.3844717741012573 + 0.001 * 7.708659648895264
Epoch 120, val loss: 1.4552587270736694
Epoch 130, training loss: 1.3030160665512085 = 1.2953709363937378 + 0.001 * 7.645071983337402
Epoch 130, val loss: 1.3896375894546509
Epoch 140, training loss: 1.2195675373077393 = 1.211946964263916 + 0.001 * 7.620558261871338
Epoch 140, val loss: 1.3322330713272095
Epoch 150, training loss: 1.13944673538208 = 1.1318422555923462 + 0.001 * 7.604466915130615
Epoch 150, val loss: 1.2808195352554321
Epoch 160, training loss: 1.0599554777145386 = 1.0523781776428223 + 0.001 * 7.5773024559021
Epoch 160, val loss: 1.2307803630828857
Epoch 170, training loss: 0.9804173111915588 = 0.9728787541389465 + 0.001 * 7.5385308265686035
Epoch 170, val loss: 1.181857705116272
Epoch 180, training loss: 0.902157723903656 = 0.8946817517280579 + 0.001 * 7.475958824157715
Epoch 180, val loss: 1.1348594427108765
Epoch 190, training loss: 0.8273037075996399 = 0.819888710975647 + 0.001 * 7.415018081665039
Epoch 190, val loss: 1.0923666954040527
Epoch 200, training loss: 0.7577391862869263 = 0.7503396272659302 + 0.001 * 7.399539470672607
Epoch 200, val loss: 1.0563154220581055
Epoch 210, training loss: 0.6944484114646912 = 0.6870540380477905 + 0.001 * 7.394359588623047
Epoch 210, val loss: 1.0275884866714478
Epoch 220, training loss: 0.6376709342002869 = 0.630279541015625 + 0.001 * 7.391364097595215
Epoch 220, val loss: 1.0062657594680786
Epoch 230, training loss: 0.5870664715766907 = 0.5796834230422974 + 0.001 * 7.383070468902588
Epoch 230, val loss: 0.9917100667953491
Epoch 240, training loss: 0.5417703986167908 = 0.5343983769416809 + 0.001 * 7.371993541717529
Epoch 240, val loss: 0.9829372763633728
Epoch 250, training loss: 0.5006629228591919 = 0.49331071972846985 + 0.001 * 7.35218620300293
Epoch 250, val loss: 0.9790287613868713
Epoch 260, training loss: 0.46258777379989624 = 0.4552629590034485 + 0.001 * 7.324828147888184
Epoch 260, val loss: 0.979052722454071
Epoch 270, training loss: 0.4265882670879364 = 0.4192894995212555 + 0.001 * 7.298755645751953
Epoch 270, val loss: 0.9822376370429993
Epoch 280, training loss: 0.3919321298599243 = 0.38465607166290283 + 0.001 * 7.276051044464111
Epoch 280, val loss: 0.9878484606742859
Epoch 290, training loss: 0.3582804799079895 = 0.3510277569293976 + 0.001 * 7.252731800079346
Epoch 290, val loss: 0.9961768388748169
Epoch 300, training loss: 0.32573947310447693 = 0.3184983432292938 + 0.001 * 7.241119861602783
Epoch 300, val loss: 1.0073710680007935
Epoch 310, training loss: 0.2947683036327362 = 0.2875252962112427 + 0.001 * 7.2430009841918945
Epoch 310, val loss: 1.021661639213562
Epoch 320, training loss: 0.2658986449241638 = 0.25866493582725525 + 0.001 * 7.233717918395996
Epoch 320, val loss: 1.0394593477249146
Epoch 330, training loss: 0.23948991298675537 = 0.23225869238376617 + 0.001 * 7.231227397918701
Epoch 330, val loss: 1.0608465671539307
Epoch 340, training loss: 0.21558021008968353 = 0.20835061371326447 + 0.001 * 7.22960090637207
Epoch 340, val loss: 1.085279941558838
Epoch 350, training loss: 0.19403038918972015 = 0.18680375814437866 + 0.001 * 7.2266316413879395
Epoch 350, val loss: 1.1123379468917847
Epoch 360, training loss: 0.1746409833431244 = 0.16741825640201569 + 0.001 * 7.222726821899414
Epoch 360, val loss: 1.141381859779358
Epoch 370, training loss: 0.1572214961051941 = 0.14999814331531525 + 0.001 * 7.223357677459717
Epoch 370, val loss: 1.171999454498291
Epoch 380, training loss: 0.14156807959079742 = 0.13435664772987366 + 0.001 * 7.211432933807373
Epoch 380, val loss: 1.2038637399673462
Epoch 390, training loss: 0.12754377722740173 = 0.12033258378505707 + 0.001 * 7.21119499206543
Epoch 390, val loss: 1.2365367412567139
Epoch 400, training loss: 0.1149926409125328 = 0.10778304934501648 + 0.001 * 7.209589958190918
Epoch 400, val loss: 1.269675850868225
Epoch 410, training loss: 0.10377655923366547 = 0.09657742828130722 + 0.001 * 7.199127674102783
Epoch 410, val loss: 1.302923321723938
Epoch 420, training loss: 0.09378967434167862 = 0.08659520000219345 + 0.001 * 7.194475173950195
Epoch 420, val loss: 1.336242914199829
Epoch 430, training loss: 0.08490685373544693 = 0.07772181183099747 + 0.001 * 7.185040473937988
Epoch 430, val loss: 1.369360327720642
Epoch 440, training loss: 0.07704111188650131 = 0.06984859704971313 + 0.001 * 7.192511081695557
Epoch 440, val loss: 1.4021244049072266
Epoch 450, training loss: 0.07004603743553162 = 0.06287401169538498 + 0.001 * 7.172028541564941
Epoch 450, val loss: 1.434402346611023
Epoch 460, training loss: 0.06386721134185791 = 0.056703127920627594 + 0.001 * 7.164083480834961
Epoch 460, val loss: 1.466200590133667
Epoch 470, training loss: 0.0584050789475441 = 0.051248084753751755 + 0.001 * 7.156994342803955
Epoch 470, val loss: 1.4973798990249634
Epoch 480, training loss: 0.053591467440128326 = 0.04642665013670921 + 0.001 * 7.164816856384277
Epoch 480, val loss: 1.5279028415679932
Epoch 490, training loss: 0.04931274801492691 = 0.04216466844081879 + 0.001 * 7.1480793952941895
Epoch 490, val loss: 1.5577396154403687
Epoch 500, training loss: 0.045565709471702576 = 0.038394056260585785 + 0.001 * 7.171654224395752
Epoch 500, val loss: 1.5868679285049438
Epoch 510, training loss: 0.0422067828476429 = 0.03505478799343109 + 0.001 * 7.151993274688721
Epoch 510, val loss: 1.6152364015579224
Epoch 520, training loss: 0.039237525314092636 = 0.03209227696061134 + 0.001 * 7.145248889923096
Epoch 520, val loss: 1.642821192741394
Epoch 530, training loss: 0.036605268716812134 = 0.02945946902036667 + 0.001 * 7.145799160003662
Epoch 530, val loss: 1.6695985794067383
Epoch 540, training loss: 0.034263040870428085 = 0.027115212753415108 + 0.001 * 7.147828578948975
Epoch 540, val loss: 1.6956285238265991
Epoch 550, training loss: 0.032161105424165726 = 0.025023506954312325 + 0.001 * 7.137598514556885
Epoch 550, val loss: 1.7208894491195679
Epoch 560, training loss: 0.03026929497718811 = 0.023152751848101616 + 0.001 * 7.116543769836426
Epoch 560, val loss: 1.7453733682632446
Epoch 570, training loss: 0.02859313040971756 = 0.021475646644830704 + 0.001 * 7.117483139038086
Epoch 570, val loss: 1.7691160440444946
Epoch 580, training loss: 0.027078455314040184 = 0.019968532025814056 + 0.001 * 7.109922885894775
Epoch 580, val loss: 1.7920812368392944
Epoch 590, training loss: 0.025717684999108315 = 0.018610859289765358 + 0.001 * 7.106825828552246
Epoch 590, val loss: 1.8143707513809204
Epoch 600, training loss: 0.024486184120178223 = 0.01738481968641281 + 0.001 * 7.101363182067871
Epoch 600, val loss: 1.8359135389328003
Epoch 610, training loss: 0.023371661081910133 = 0.016275057569146156 + 0.001 * 7.096602916717529
Epoch 610, val loss: 1.8567852973937988
Epoch 620, training loss: 0.02237137407064438 = 0.01526732835918665 + 0.001 * 7.1040449142456055
Epoch 620, val loss: 1.8769804239273071
Epoch 630, training loss: 0.0214419923722744 = 0.014348097145557404 + 0.001 * 7.093894958496094
Epoch 630, val loss: 1.8965948820114136
Epoch 640, training loss: 0.020598987117409706 = 0.013504586182534695 + 0.001 * 7.094399929046631
Epoch 640, val loss: 1.9156962633132935
Epoch 650, training loss: 0.01984134502708912 = 0.012727566063404083 + 0.001 * 7.113778114318848
Epoch 650, val loss: 1.9342176914215088
Epoch 660, training loss: 0.01910294219851494 = 0.012010944075882435 + 0.001 * 7.091997146606445
Epoch 660, val loss: 1.9522218704223633
Epoch 670, training loss: 0.01843072846531868 = 0.011349383741617203 + 0.001 * 7.081345081329346
Epoch 670, val loss: 1.9696756601333618
Epoch 680, training loss: 0.017818059772253036 = 0.01073831133544445 + 0.001 * 7.079747676849365
Epoch 680, val loss: 1.98654305934906
Epoch 690, training loss: 0.017273573204874992 = 0.010173610411584377 + 0.001 * 7.0999627113342285
Epoch 690, val loss: 2.002774953842163
Epoch 700, training loss: 0.016740592196583748 = 0.009651610627770424 + 0.001 * 7.0889811515808105
Epoch 700, val loss: 2.018465995788574
Epoch 710, training loss: 0.016251543536782265 = 0.009168538264930248 + 0.001 * 7.083004951477051
Epoch 710, val loss: 2.033640146255493
Epoch 720, training loss: 0.01584722101688385 = 0.008721026591956615 + 0.001 * 7.126194000244141
Epoch 720, val loss: 2.0482099056243896
Epoch 730, training loss: 0.015383280813694 = 0.008306164294481277 + 0.001 * 7.0771164894104
Epoch 730, val loss: 2.0623059272766113
Epoch 740, training loss: 0.014999441802501678 = 0.007920981384813786 + 0.001 * 7.078459739685059
Epoch 740, val loss: 2.075916290283203
Epoch 750, training loss: 0.014661502093076706 = 0.007562860380858183 + 0.001 * 7.098641872406006
Epoch 750, val loss: 2.089038848876953
Epoch 760, training loss: 0.014314485713839531 = 0.007229489274322987 + 0.001 * 7.084996700286865
Epoch 760, val loss: 2.101665496826172
Epoch 770, training loss: 0.013996174558997154 = 0.006918720435351133 + 0.001 * 7.077454090118408
Epoch 770, val loss: 2.1138787269592285
Epoch 780, training loss: 0.013695343397557735 = 0.0066286674700677395 + 0.001 * 7.066675662994385
Epoch 780, val loss: 2.1256539821624756
Epoch 790, training loss: 0.013428973034024239 = 0.006357565056532621 + 0.001 * 7.071407794952393
Epoch 790, val loss: 2.1370272636413574
Epoch 800, training loss: 0.0131894052028656 = 0.00610391516238451 + 0.001 * 7.0854902267456055
Epoch 800, val loss: 2.1481053829193115
Epoch 810, training loss: 0.012925304472446442 = 0.0058663031086325645 + 0.001 * 7.0590009689331055
Epoch 810, val loss: 2.1587281227111816
Epoch 820, training loss: 0.012703844346106052 = 0.005643440410494804 + 0.001 * 7.060403823852539
Epoch 820, val loss: 2.1689834594726562
Epoch 830, training loss: 0.01249409094452858 = 0.005434171296656132 + 0.001 * 7.059919357299805
Epoch 830, val loss: 2.178906202316284
Epoch 840, training loss: 0.012295854277908802 = 0.005237384233623743 + 0.001 * 7.058469772338867
Epoch 840, val loss: 2.1885600090026855
Epoch 850, training loss: 0.012107574380934238 = 0.005052141845226288 + 0.001 * 7.055432319641113
Epoch 850, val loss: 2.1978578567504883
Epoch 860, training loss: 0.011933904141187668 = 0.0048775565810501575 + 0.001 * 7.056347846984863
Epoch 860, val loss: 2.206892251968384
Epoch 870, training loss: 0.011762397363781929 = 0.004712830763310194 + 0.001 * 7.049566745758057
Epoch 870, val loss: 2.2156260013580322
Epoch 880, training loss: 0.011629901826381683 = 0.004557216539978981 + 0.001 * 7.072685241699219
Epoch 880, val loss: 2.224017858505249
Epoch 890, training loss: 0.011475141160190105 = 0.004410083405673504 + 0.001 * 7.065057277679443
Epoch 890, val loss: 2.232203483581543
Epoch 900, training loss: 0.011320232413709164 = 0.004270843230187893 + 0.001 * 7.049388885498047
Epoch 900, val loss: 2.2401881217956543
Epoch 910, training loss: 0.011179862543940544 = 0.004138939082622528 + 0.001 * 7.040923118591309
Epoch 910, val loss: 2.2478411197662354
Epoch 920, training loss: 0.011059094220399857 = 0.004013855010271072 + 0.001 * 7.045238971710205
Epoch 920, val loss: 2.255260467529297
Epoch 930, training loss: 0.01093501877039671 = 0.003895149566233158 + 0.001 * 7.0398688316345215
Epoch 930, val loss: 2.2624402046203613
Epoch 940, training loss: 0.01082330010831356 = 0.0037824129685759544 + 0.001 * 7.040886402130127
Epoch 940, val loss: 2.269420623779297
Epoch 950, training loss: 0.010730057023465633 = 0.003675250569358468 + 0.001 * 7.054806232452393
Epoch 950, val loss: 2.276155710220337
Epoch 960, training loss: 0.010626696981489658 = 0.0035732665564864874 + 0.001 * 7.053430080413818
Epoch 960, val loss: 2.2827162742614746
Epoch 970, training loss: 0.010506926104426384 = 0.003476143116131425 + 0.001 * 7.030782699584961
Epoch 970, val loss: 2.2890679836273193
Epoch 980, training loss: 0.010422790423035622 = 0.0033836006186902523 + 0.001 * 7.03918981552124
Epoch 980, val loss: 2.2951834201812744
Epoch 990, training loss: 0.010343006812036037 = 0.003295344766229391 + 0.001 * 7.047661781311035
Epoch 990, val loss: 2.3011655807495117
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 1.9394314289093018 = 1.9308346509933472 + 0.001 * 8.596803665161133
Epoch 0, val loss: 1.9317508935928345
Epoch 10, training loss: 1.9298909902572632 = 1.9212942123413086 + 0.001 * 8.596741676330566
Epoch 10, val loss: 1.921818494796753
Epoch 20, training loss: 1.91796875 = 1.9093722105026245 + 0.001 * 8.596574783325195
Epoch 20, val loss: 1.909433364868164
Epoch 30, training loss: 1.901244878768921 = 1.892648696899414 + 0.001 * 8.596232414245605
Epoch 30, val loss: 1.8922244310379028
Epoch 40, training loss: 1.8768537044525146 = 1.8682582378387451 + 0.001 * 8.595453262329102
Epoch 40, val loss: 1.8677290678024292
Epoch 50, training loss: 1.8440368175506592 = 1.8354434967041016 + 0.001 * 8.593356132507324
Epoch 50, val loss: 1.836639404296875
Epoch 60, training loss: 1.8098735809326172 = 1.801287293434143 + 0.001 * 8.586328506469727
Epoch 60, val loss: 1.8085265159606934
Epoch 70, training loss: 1.7794550657272339 = 1.770897388458252 + 0.001 * 8.557656288146973
Epoch 70, val loss: 1.7853076457977295
Epoch 80, training loss: 1.7387253046035767 = 1.7303428649902344 + 0.001 * 8.382410049438477
Epoch 80, val loss: 1.7502636909484863
Epoch 90, training loss: 1.681203842163086 = 1.673142910003662 + 0.001 * 8.06092357635498
Epoch 90, val loss: 1.7006449699401855
Epoch 100, training loss: 1.6039711236953735 = 1.5959993600845337 + 0.001 * 7.971820831298828
Epoch 100, val loss: 1.6366806030273438
Epoch 110, training loss: 1.5119065046310425 = 1.5040727853775024 + 0.001 * 7.833669662475586
Epoch 110, val loss: 1.5625839233398438
Epoch 120, training loss: 1.4124853610992432 = 1.4048057794570923 + 0.001 * 7.6795454025268555
Epoch 120, val loss: 1.4846254587173462
Epoch 130, training loss: 1.3095442056655884 = 1.3018933534622192 + 0.001 * 7.650801658630371
Epoch 130, val loss: 1.4055317640304565
Epoch 140, training loss: 1.2028754949569702 = 1.195260763168335 + 0.001 * 7.614722728729248
Epoch 140, val loss: 1.3246519565582275
Epoch 150, training loss: 1.0950486660003662 = 1.0874959230422974 + 0.001 * 7.552718162536621
Epoch 150, val loss: 1.2448129653930664
Epoch 160, training loss: 0.991518497467041 = 0.9840303659439087 + 0.001 * 7.48811674118042
Epoch 160, val loss: 1.1706687211990356
Epoch 170, training loss: 0.8974505066871643 = 0.8899882435798645 + 0.001 * 7.4622721672058105
Epoch 170, val loss: 1.1065618991851807
Epoch 180, training loss: 0.8155868649482727 = 0.808135449886322 + 0.001 * 7.451408386230469
Epoch 180, val loss: 1.0548278093338013
Epoch 190, training loss: 0.7459827661514282 = 0.7385450601577759 + 0.001 * 7.437720775604248
Epoch 190, val loss: 1.0150996446609497
Epoch 200, training loss: 0.6862679719924927 = 0.6788411736488342 + 0.001 * 7.426812648773193
Epoch 200, val loss: 0.9851093292236328
Epoch 210, training loss: 0.6332427859306335 = 0.6258249878883362 + 0.001 * 7.417825698852539
Epoch 210, val loss: 0.9619250297546387
Epoch 220, training loss: 0.5842422246932983 = 0.5768319368362427 + 0.001 * 7.410294532775879
Epoch 220, val loss: 0.9430596232414246
Epoch 230, training loss: 0.537612795829773 = 0.5302112102508545 + 0.001 * 7.401556015014648
Epoch 230, val loss: 0.9272037148475647
Epoch 240, training loss: 0.49227258563041687 = 0.4848841428756714 + 0.001 * 7.388432502746582
Epoch 240, val loss: 0.9139987826347351
Epoch 250, training loss: 0.4475345313549042 = 0.4401675760746002 + 0.001 * 7.366941928863525
Epoch 250, val loss: 0.9031544923782349
Epoch 260, training loss: 0.4031393826007843 = 0.3958008587360382 + 0.001 * 7.338526725769043
Epoch 260, val loss: 0.8947030305862427
Epoch 270, training loss: 0.35938122868537903 = 0.3520755469799042 + 0.001 * 7.30567741394043
Epoch 270, val loss: 0.8888943791389465
Epoch 280, training loss: 0.317069411277771 = 0.3097914159297943 + 0.001 * 7.278002738952637
Epoch 280, val loss: 0.886398196220398
Epoch 290, training loss: 0.2772805392742157 = 0.2700135111808777 + 0.001 * 7.26701545715332
Epoch 290, val loss: 0.8873576521873474
Epoch 300, training loss: 0.24102166295051575 = 0.23375748097896576 + 0.001 * 7.264176368713379
Epoch 300, val loss: 0.8916102051734924
Epoch 310, training loss: 0.20902059972286224 = 0.20175990462303162 + 0.001 * 7.260697841644287
Epoch 310, val loss: 0.8990722298622131
Epoch 320, training loss: 0.18146386742591858 = 0.17420203983783722 + 0.001 * 7.261829853057861
Epoch 320, val loss: 0.9095208048820496
Epoch 330, training loss: 0.15804165601730347 = 0.150779128074646 + 0.001 * 7.2625250816345215
Epoch 330, val loss: 0.9223413467407227
Epoch 340, training loss: 0.13821744918823242 = 0.1309531033039093 + 0.001 * 7.264341831207275
Epoch 340, val loss: 0.9371706247329712
Epoch 350, training loss: 0.12141023576259613 = 0.11414501816034317 + 0.001 * 7.2652153968811035
Epoch 350, val loss: 0.9533804655075073
Epoch 360, training loss: 0.1071057990193367 = 0.0998394787311554 + 0.001 * 7.26632022857666
Epoch 360, val loss: 0.9705718159675598
Epoch 370, training loss: 0.09488538652658463 = 0.087616927921772 + 0.001 * 7.268457889556885
Epoch 370, val loss: 0.9884127974510193
Epoch 380, training loss: 0.0844067856669426 = 0.07713848352432251 + 0.001 * 7.268301486968994
Epoch 380, val loss: 1.0066144466400146
Epoch 390, training loss: 0.07540490478277206 = 0.06813572347164154 + 0.001 * 7.26917839050293
Epoch 390, val loss: 1.02504563331604
Epoch 400, training loss: 0.06766163557767868 = 0.0603911429643631 + 0.001 * 7.27049446105957
Epoch 400, val loss: 1.0434458255767822
Epoch 410, training loss: 0.060989271849393845 = 0.053718842566013336 + 0.001 * 7.270429611206055
Epoch 410, val loss: 1.0618224143981934
Epoch 420, training loss: 0.055233292281627655 = 0.04796071723103523 + 0.001 * 7.272575378417969
Epoch 420, val loss: 1.079999327659607
Epoch 430, training loss: 0.05025585740804672 = 0.04298114776611328 + 0.001 * 7.274709701538086
Epoch 430, val loss: 1.097965955734253
Epoch 440, training loss: 0.0459359847009182 = 0.038663942366838455 + 0.001 * 7.272040843963623
Epoch 440, val loss: 1.1156058311462402
Epoch 450, training loss: 0.04218286648392677 = 0.034911543130874634 + 0.001 * 7.271322727203369
Epoch 450, val loss: 1.132956624031067
Epoch 460, training loss: 0.03891191631555557 = 0.03164021670818329 + 0.001 * 7.271698951721191
Epoch 460, val loss: 1.1499378681182861
Epoch 470, training loss: 0.03604954481124878 = 0.02877913974225521 + 0.001 * 7.2704057693481445
Epoch 470, val loss: 1.1664952039718628
Epoch 480, training loss: 0.0335383266210556 = 0.02626856416463852 + 0.001 * 7.269761085510254
Epoch 480, val loss: 1.182674765586853
Epoch 490, training loss: 0.03133021667599678 = 0.024058785289525986 + 0.001 * 7.271432399749756
Epoch 490, val loss: 1.1983448266983032
Epoch 500, training loss: 0.02937798947095871 = 0.022107627242803574 + 0.001 * 7.27036190032959
Epoch 500, val loss: 1.2136088609695435
Epoch 510, training loss: 0.02764928713440895 = 0.02037881687283516 + 0.001 * 7.270469665527344
Epoch 510, val loss: 1.2283809185028076
Epoch 520, training loss: 0.026108665391802788 = 0.018842048943042755 + 0.001 * 7.266615867614746
Epoch 520, val loss: 1.2427172660827637
Epoch 530, training loss: 0.024746598675847054 = 0.01747123710811138 + 0.001 * 7.275361061096191
Epoch 530, val loss: 1.256570816040039
Epoch 540, training loss: 0.023512940853834152 = 0.016244448721408844 + 0.001 * 7.268492698669434
Epoch 540, val loss: 1.2699637413024902
Epoch 550, training loss: 0.0224068071693182 = 0.015143271535634995 + 0.001 * 7.263535976409912
Epoch 550, val loss: 1.28291916847229
Epoch 560, training loss: 0.021410983055830002 = 0.014151673763990402 + 0.001 * 7.259308338165283
Epoch 560, val loss: 1.295451283454895
Epoch 570, training loss: 0.02051885612308979 = 0.013256077654659748 + 0.001 * 7.2627787590026855
Epoch 570, val loss: 1.3075685501098633
Epoch 580, training loss: 0.019738903269171715 = 0.01244485005736351 + 0.001 * 7.294053554534912
Epoch 580, val loss: 1.3192884922027588
Epoch 590, training loss: 0.01896805316209793 = 0.011708102189004421 + 0.001 * 7.259950160980225
Epoch 590, val loss: 1.33061683177948
Epoch 600, training loss: 0.018288441002368927 = 0.011037206277251244 + 0.001 * 7.251234531402588
Epoch 600, val loss: 1.3416080474853516
Epoch 610, training loss: 0.017673704773187637 = 0.01042466051876545 + 0.001 * 7.249044895172119
Epoch 610, val loss: 1.3522242307662964
Epoch 620, training loss: 0.017113102599978447 = 0.00986404251307249 + 0.001 * 7.249059200286865
Epoch 620, val loss: 1.362523078918457
Epoch 630, training loss: 0.01660076528787613 = 0.009349659085273743 + 0.001 * 7.251104831695557
Epoch 630, val loss: 1.3725037574768066
Epoch 640, training loss: 0.016129489988088608 = 0.008876626379787922 + 0.001 * 7.25286340713501
Epoch 640, val loss: 1.382167100906372
Epoch 650, training loss: 0.01567554846405983 = 0.008440644480288029 + 0.001 * 7.234902858734131
Epoch 650, val loss: 1.391548991203308
Epoch 660, training loss: 0.015269647352397442 = 0.008037998341023922 + 0.001 * 7.2316484451293945
Epoch 660, val loss: 1.4006532430648804
Epoch 670, training loss: 0.014892484992742538 = 0.0076653785072267056 + 0.001 * 7.227106094360352
Epoch 670, val loss: 1.409462332725525
Epoch 680, training loss: 0.014552321285009384 = 0.0073198662139475346 + 0.001 * 7.232454776763916
Epoch 680, val loss: 1.4180569648742676
Epoch 690, training loss: 0.01421497855335474 = 0.006998921278864145 + 0.001 * 7.216056823730469
Epoch 690, val loss: 1.4263858795166016
Epoch 700, training loss: 0.0139253418892622 = 0.006700285244733095 + 0.001 * 7.225056171417236
Epoch 700, val loss: 1.434467077255249
Epoch 710, training loss: 0.013658491894602776 = 0.006421960424631834 + 0.001 * 7.236530780792236
Epoch 710, val loss: 1.4423325061798096
Epoch 720, training loss: 0.013380009680986404 = 0.006162180099636316 + 0.001 * 7.217828750610352
Epoch 720, val loss: 1.4499844312667847
Epoch 730, training loss: 0.013111559674143791 = 0.0059193214401602745 + 0.001 * 7.192237854003906
Epoch 730, val loss: 1.4574378728866577
Epoch 740, training loss: 0.012889137491583824 = 0.0056919618509709835 + 0.001 * 7.1971755027771
Epoch 740, val loss: 1.4646692276000977
Epoch 750, training loss: 0.012671751901507378 = 0.005478797014802694 + 0.001 * 7.1929545402526855
Epoch 750, val loss: 1.4717109203338623
Epoch 760, training loss: 0.0125188697129488 = 0.0052786655724048615 + 0.001 * 7.240203857421875
Epoch 760, val loss: 1.4785737991333008
Epoch 770, training loss: 0.012275081127882004 = 0.005090486258268356 + 0.001 * 7.18459415435791
Epoch 770, val loss: 1.4852899312973022
Epoch 780, training loss: 0.01209085714071989 = 0.004913367796689272 + 0.001 * 7.177488803863525
Epoch 780, val loss: 1.4918264150619507
Epoch 790, training loss: 0.011943712830543518 = 0.004746444057673216 + 0.001 * 7.197268486022949
Epoch 790, val loss: 1.4981498718261719
Epoch 800, training loss: 0.011783557012677193 = 0.004588973708450794 + 0.001 * 7.194582462310791
Epoch 800, val loss: 1.5043725967407227
Epoch 810, training loss: 0.011599401012063026 = 0.004440228920429945 + 0.001 * 7.159171104431152
Epoch 810, val loss: 1.5104330778121948
Epoch 820, training loss: 0.011480271816253662 = 0.004299577325582504 + 0.001 * 7.180694103240967
Epoch 820, val loss: 1.5163325071334839
Epoch 830, training loss: 0.011320812627673149 = 0.004166434518992901 + 0.001 * 7.154377460479736
Epoch 830, val loss: 1.5221186876296997
Epoch 840, training loss: 0.011189494282007217 = 0.00404029805213213 + 0.001 * 7.149195194244385
Epoch 840, val loss: 1.5277317762374878
Epoch 850, training loss: 0.01109602302312851 = 0.003920663148164749 + 0.001 * 7.175359725952148
Epoch 850, val loss: 1.5332554578781128
Epoch 860, training loss: 0.011025801301002502 = 0.003807084634900093 + 0.001 * 7.218716621398926
Epoch 860, val loss: 1.5386266708374023
Epoch 870, training loss: 0.01084144040942192 = 0.0036991629749536514 + 0.001 * 7.142277717590332
Epoch 870, val loss: 1.5438710451126099
Epoch 880, training loss: 0.010755471885204315 = 0.0035965158604085445 + 0.001 * 7.158956050872803
Epoch 880, val loss: 1.5490132570266724
Epoch 890, training loss: 0.010623883455991745 = 0.003498802660033107 + 0.001 * 7.125080585479736
Epoch 890, val loss: 1.5540467500686646
Epoch 900, training loss: 0.010570273734629154 = 0.003405736293643713 + 0.001 * 7.164536952972412
Epoch 900, val loss: 1.558945894241333
Epoch 910, training loss: 0.010472388006746769 = 0.0033170038368552923 + 0.001 * 7.155383586883545
Epoch 910, val loss: 1.5637632608413696
Epoch 920, training loss: 0.010388572700321674 = 0.00323232333175838 + 0.001 * 7.156248569488525
Epoch 920, val loss: 1.5684572458267212
Epoch 930, training loss: 0.01027633249759674 = 0.0031514682341367006 + 0.001 * 7.124864101409912
Epoch 930, val loss: 1.5730607509613037
Epoch 940, training loss: 0.01020544208586216 = 0.00307413749396801 + 0.001 * 7.131304740905762
Epoch 940, val loss: 1.5775920152664185
Epoch 950, training loss: 0.010120200924575329 = 0.003000019583851099 + 0.001 * 7.120181083679199
Epoch 950, val loss: 1.581999659538269
Epoch 960, training loss: 0.010049219243228436 = 0.0029286069329828024 + 0.001 * 7.120611667633057
Epoch 960, val loss: 1.5863871574401855
Epoch 970, training loss: 0.009977533482015133 = 0.002859247149899602 + 0.001 * 7.118285655975342
Epoch 970, val loss: 1.5907312631607056
Epoch 980, training loss: 0.009978493675589561 = 0.0027912482619285583 + 0.001 * 7.187244415283203
Epoch 980, val loss: 1.5951510667800903
Epoch 990, training loss: 0.009828782640397549 = 0.002724362537264824 + 0.001 * 7.104419708251953
Epoch 990, val loss: 1.5996497869491577
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 1.9521691799163818 = 1.9435724020004272 + 0.001 * 8.5968017578125
Epoch 0, val loss: 1.9443730115890503
Epoch 10, training loss: 1.9420472383499146 = 1.93345046043396 + 0.001 * 8.596722602844238
Epoch 10, val loss: 1.9349769353866577
Epoch 20, training loss: 1.9298639297485352 = 1.9212673902511597 + 0.001 * 8.596505165100098
Epoch 20, val loss: 1.9231750965118408
Epoch 30, training loss: 1.913194179534912 = 1.9045981168746948 + 0.001 * 8.596041679382324
Epoch 30, val loss: 1.9066609144210815
Epoch 40, training loss: 1.889264702796936 = 1.8806697130203247 + 0.001 * 8.594969749450684
Epoch 40, val loss: 1.8828895092010498
Epoch 50, training loss: 1.8563015460968018 = 1.8477095365524292 + 0.001 * 8.592016220092773
Epoch 50, val loss: 1.8512723445892334
Epoch 60, training loss: 1.8187808990478516 = 1.8101996183395386 + 0.001 * 8.581324577331543
Epoch 60, val loss: 1.818419098854065
Epoch 70, training loss: 1.785596489906311 = 1.7770670652389526 + 0.001 * 8.529417991638184
Epoch 70, val loss: 1.791285514831543
Epoch 80, training loss: 1.7468925714492798 = 1.7386683225631714 + 0.001 * 8.224278450012207
Epoch 80, val loss: 1.7564479112625122
Epoch 90, training loss: 1.6939237117767334 = 1.6859089136123657 + 0.001 * 8.014854431152344
Epoch 90, val loss: 1.7100878953933716
Epoch 100, training loss: 1.6204235553741455 = 1.6126444339752197 + 0.001 * 7.77908182144165
Epoch 100, val loss: 1.648733377456665
Epoch 110, training loss: 1.52834153175354 = 1.5207630395889282 + 0.001 * 7.578500747680664
Epoch 110, val loss: 1.573226809501648
Epoch 120, training loss: 1.426397442817688 = 1.418876051902771 + 0.001 * 7.521335601806641
Epoch 120, val loss: 1.4918955564498901
Epoch 130, training loss: 1.3218806982040405 = 1.3144032955169678 + 0.001 * 7.477417945861816
Epoch 130, val loss: 1.410687804222107
Epoch 140, training loss: 1.2200778722763062 = 1.212628960609436 + 0.001 * 7.448944568634033
Epoch 140, val loss: 1.3336715698242188
Epoch 150, training loss: 1.1250388622283936 = 1.1176135540008545 + 0.001 * 7.42526912689209
Epoch 150, val loss: 1.2641408443450928
Epoch 160, training loss: 1.0397312641143799 = 1.032322883605957 + 0.001 * 7.408390522003174
Epoch 160, val loss: 1.203736424446106
Epoch 170, training loss: 0.9637458324432373 = 0.9563509225845337 + 0.001 * 7.39492654800415
Epoch 170, val loss: 1.1516506671905518
Epoch 180, training loss: 0.8939854502677917 = 0.8866066336631775 + 0.001 * 7.378794193267822
Epoch 180, val loss: 1.1045451164245605
Epoch 190, training loss: 0.8265758752822876 = 0.8192201256752014 + 0.001 * 7.355732440948486
Epoch 190, val loss: 1.0589628219604492
Epoch 200, training loss: 0.7589161396026611 = 0.7515978813171387 + 0.001 * 7.318284034729004
Epoch 200, val loss: 1.0129700899124146
Epoch 210, training loss: 0.6905381679534912 = 0.6832515001296997 + 0.001 * 7.286675930023193
Epoch 210, val loss: 0.9667786955833435
Epoch 220, training loss: 0.6227376461029053 = 0.6154974102973938 + 0.001 * 7.240231513977051
Epoch 220, val loss: 0.9225442409515381
Epoch 230, training loss: 0.5576351881027222 = 0.5504140853881836 + 0.001 * 7.221113204956055
Epoch 230, val loss: 0.8830692768096924
Epoch 240, training loss: 0.49682408571243286 = 0.48961320519447327 + 0.001 * 7.21087121963501
Epoch 240, val loss: 0.8501750826835632
Epoch 250, training loss: 0.44097211956977844 = 0.4337663948535919 + 0.001 * 7.205712795257568
Epoch 250, val loss: 0.8244448900222778
Epoch 260, training loss: 0.3900471031665802 = 0.38285064697265625 + 0.001 * 7.196441650390625
Epoch 260, val loss: 0.80503910779953
Epoch 270, training loss: 0.34370481967926025 = 0.3365153670310974 + 0.001 * 7.189460277557373
Epoch 270, val loss: 0.7904034852981567
Epoch 280, training loss: 0.3016268312931061 = 0.2944362461566925 + 0.001 * 7.19057035446167
Epoch 280, val loss: 0.7795764207839966
Epoch 290, training loss: 0.26369014382362366 = 0.2565126121044159 + 0.001 * 7.177536487579346
Epoch 290, val loss: 0.7721832394599915
Epoch 300, training loss: 0.2299596667289734 = 0.22278760373592377 + 0.001 * 7.172069549560547
Epoch 300, val loss: 0.7680336236953735
Epoch 310, training loss: 0.2004222571849823 = 0.19324718415737152 + 0.001 * 7.175065517425537
Epoch 310, val loss: 0.7671209573745728
Epoch 320, training loss: 0.17487047612667084 = 0.1677047312259674 + 0.001 * 7.165738582611084
Epoch 320, val loss: 0.7693130970001221
Epoch 330, training loss: 0.1529528945684433 = 0.14576847851276398 + 0.001 * 7.184410095214844
Epoch 330, val loss: 0.774372398853302
Epoch 340, training loss: 0.1341405212879181 = 0.12697750329971313 + 0.001 * 7.163014888763428
Epoch 340, val loss: 0.7818998694419861
Epoch 350, training loss: 0.11805429309606552 = 0.11089269071817398 + 0.001 * 7.1615986824035645
Epoch 350, val loss: 0.7914025783538818
Epoch 360, training loss: 0.1042747050523758 = 0.09711669385433197 + 0.001 * 7.1580095291137695
Epoch 360, val loss: 0.8024277687072754
Epoch 370, training loss: 0.09248367697000504 = 0.0853159949183464 + 0.001 * 7.167680263519287
Epoch 370, val loss: 0.8146007657051086
Epoch 380, training loss: 0.08236303180456161 = 0.07520884275436401 + 0.001 * 7.154188632965088
Epoch 380, val loss: 0.8275624513626099
Epoch 390, training loss: 0.07370484620332718 = 0.0665488988161087 + 0.001 * 7.155949592590332
Epoch 390, val loss: 0.8409464955329895
Epoch 400, training loss: 0.06627551466226578 = 0.05911894887685776 + 0.001 * 7.1565656661987305
Epoch 400, val loss: 0.8545017242431641
Epoch 410, training loss: 0.05987777188420296 = 0.05272829160094261 + 0.001 * 7.14948034286499
Epoch 410, val loss: 0.8680145144462585
Epoch 420, training loss: 0.05436725169420242 = 0.04721858352422714 + 0.001 * 7.148666858673096
Epoch 420, val loss: 0.8814031481742859
Epoch 430, training loss: 0.0496012419462204 = 0.04245374724268913 + 0.001 * 7.147492408752441
Epoch 430, val loss: 0.8945981860160828
Epoch 440, training loss: 0.045464809983968735 = 0.03831925243139267 + 0.001 * 7.145556926727295
Epoch 440, val loss: 0.9075639843940735
Epoch 450, training loss: 0.04186481609940529 = 0.034719862043857574 + 0.001 * 7.144952297210693
Epoch 450, val loss: 0.9202490448951721
Epoch 460, training loss: 0.03871418908238411 = 0.03157611936330795 + 0.001 * 7.138068675994873
Epoch 460, val loss: 0.9326290488243103
Epoch 470, training loss: 0.03596252575516701 = 0.028820833191275597 + 0.001 * 7.141690731048584
Epoch 470, val loss: 0.9447134137153625
Epoch 480, training loss: 0.0335405059158802 = 0.026397159323096275 + 0.001 * 7.143344879150391
Epoch 480, val loss: 0.9564687013626099
Epoch 490, training loss: 0.03139493241906166 = 0.02425697259604931 + 0.001 * 7.137959003448486
Epoch 490, val loss: 0.9678962826728821
Epoch 500, training loss: 0.029503442347049713 = 0.022360635921359062 + 0.001 * 7.142806053161621
Epoch 500, val loss: 0.9789972305297852
Epoch 510, training loss: 0.02780909836292267 = 0.020674578845500946 + 0.001 * 7.134518623352051
Epoch 510, val loss: 0.9897592067718506
Epoch 520, training loss: 0.026299409568309784 = 0.01917087659239769 + 0.001 * 7.128532409667969
Epoch 520, val loss: 1.0002135038375854
Epoch 530, training loss: 0.024950845167040825 = 0.01782483048737049 + 0.001 * 7.126014709472656
Epoch 530, val loss: 1.0103877782821655
Epoch 540, training loss: 0.023764826357364655 = 0.016615936532616615 + 0.001 * 7.148890495300293
Epoch 540, val loss: 1.0202559232711792
Epoch 550, training loss: 0.022645067423582077 = 0.015526917763054371 + 0.001 * 7.118149280548096
Epoch 550, val loss: 1.0298311710357666
Epoch 560, training loss: 0.021682288497686386 = 0.014542840421199799 + 0.001 * 7.1394476890563965
Epoch 560, val loss: 1.0391335487365723
Epoch 570, training loss: 0.020767439156770706 = 0.013651048764586449 + 0.001 * 7.116389751434326
Epoch 570, val loss: 1.048168659210205
Epoch 580, training loss: 0.01997550204396248 = 0.012840577401220798 + 0.001 * 7.134923458099365
Epoch 580, val loss: 1.05694580078125
Epoch 590, training loss: 0.019206838682293892 = 0.012102114036679268 + 0.001 * 7.104724884033203
Epoch 590, val loss: 1.0654648542404175
Epoch 600, training loss: 0.018531229346990585 = 0.011427578516304493 + 0.001 * 7.103649616241455
Epoch 600, val loss: 1.0737457275390625
Epoch 610, training loss: 0.01792544685304165 = 0.010809879750013351 + 0.001 * 7.115567207336426
Epoch 610, val loss: 1.0817840099334717
Epoch 620, training loss: 0.01734944060444832 = 0.010242908261716366 + 0.001 * 7.106531620025635
Epoch 620, val loss: 1.089604139328003
Epoch 630, training loss: 0.016825055703520775 = 0.009721384383738041 + 0.001 * 7.103671073913574
Epoch 630, val loss: 1.0971964597702026
Epoch 640, training loss: 0.01634204387664795 = 0.009240631945431232 + 0.001 * 7.101410865783691
Epoch 640, val loss: 1.1045821905136108
Epoch 650, training loss: 0.015887469053268433 = 0.00879659503698349 + 0.001 * 7.090872764587402
Epoch 650, val loss: 1.1117751598358154
Epoch 660, training loss: 0.015492002479732037 = 0.008385589346289635 + 0.001 * 7.106412887573242
Epoch 660, val loss: 1.1187701225280762
Epoch 670, training loss: 0.015094701200723648 = 0.00800444558262825 + 0.001 * 7.090254783630371
Epoch 670, val loss: 1.1255834102630615
Epoch 680, training loss: 0.014742286875844002 = 0.007650386076420546 + 0.001 * 7.091900825500488
Epoch 680, val loss: 1.1322216987609863
Epoch 690, training loss: 0.014399758540093899 = 0.007320914883166552 + 0.001 * 7.078843116760254
Epoch 690, val loss: 1.1386946439743042
Epoch 700, training loss: 0.014126035384833813 = 0.0070138354785740376 + 0.001 * 7.112199783325195
Epoch 700, val loss: 1.1449954509735107
Epoch 710, training loss: 0.013797962106764317 = 0.006727191619575024 + 0.001 * 7.070770263671875
Epoch 710, val loss: 1.151149868965149
Epoch 720, training loss: 0.013539313338696957 = 0.006459183059632778 + 0.001 * 7.080130100250244
Epoch 720, val loss: 1.1571519374847412
Epoch 730, training loss: 0.01329043135046959 = 0.0062082502990961075 + 0.001 * 7.082180976867676
Epoch 730, val loss: 1.1629973649978638
Epoch 740, training loss: 0.01305272988975048 = 0.005972955841571093 + 0.001 * 7.079773426055908
Epoch 740, val loss: 1.1687092781066895
Epoch 750, training loss: 0.012811601161956787 = 0.005752036813646555 + 0.001 * 7.059564590454102
Epoch 750, val loss: 1.1742819547653198
Epoch 760, training loss: 0.012615356594324112 = 0.005544350482523441 + 0.001 * 7.071005344390869
Epoch 760, val loss: 1.1797157526016235
Epoch 770, training loss: 0.012407127767801285 = 0.005348867271095514 + 0.001 * 7.058260440826416
Epoch 770, val loss: 1.1850272417068481
Epoch 780, training loss: 0.012223638594150543 = 0.00516463303938508 + 0.001 * 7.059005260467529
Epoch 780, val loss: 1.1902142763137817
Epoch 790, training loss: 0.012044722214341164 = 0.004990807268768549 + 0.001 * 7.053915023803711
Epoch 790, val loss: 1.1952813863754272
Epoch 800, training loss: 0.011879684403538704 = 0.0048266383819282055 + 0.001 * 7.053045749664307
Epoch 800, val loss: 1.2002320289611816
Epoch 810, training loss: 0.011732861399650574 = 0.004671402275562286 + 0.001 * 7.061459064483643
Epoch 810, val loss: 1.2050679922103882
Epoch 820, training loss: 0.011587873101234436 = 0.0045244586654007435 + 0.001 * 7.063414573669434
Epoch 820, val loss: 1.2098015546798706
Epoch 830, training loss: 0.011442168615758419 = 0.004385218024253845 + 0.001 * 7.056950092315674
Epoch 830, val loss: 1.214417815208435
Epoch 840, training loss: 0.011307237669825554 = 0.0042531671933829784 + 0.001 * 7.054069995880127
Epoch 840, val loss: 1.2189451456069946
Epoch 850, training loss: 0.011170534417033195 = 0.004127814434468746 + 0.001 * 7.042719841003418
Epoch 850, val loss: 1.2233788967132568
Epoch 860, training loss: 0.011058702133595943 = 0.004008684307336807 + 0.001 * 7.050017356872559
Epoch 860, val loss: 1.2276971340179443
Epoch 870, training loss: 0.010944690555334091 = 0.0038953835610300303 + 0.001 * 7.049306392669678
Epoch 870, val loss: 1.2319512367248535
Epoch 880, training loss: 0.01083029992878437 = 0.0037875440903007984 + 0.001 * 7.042755603790283
Epoch 880, val loss: 1.2361029386520386
Epoch 890, training loss: 0.010726042091846466 = 0.0036848513409495354 + 0.001 * 7.0411906242370605
Epoch 890, val loss: 1.2401736974716187
Epoch 900, training loss: 0.01064450852572918 = 0.003586932085454464 + 0.001 * 7.057575702667236
Epoch 900, val loss: 1.244166612625122
Epoch 910, training loss: 0.010529354214668274 = 0.0034935125149786472 + 0.001 * 7.035841464996338
Epoch 910, val loss: 1.2480604648590088
Epoch 920, training loss: 0.010441472753882408 = 0.0034043232444673777 + 0.001 * 7.037149429321289
Epoch 920, val loss: 1.251882553100586
Epoch 930, training loss: 0.010347336530685425 = 0.003319140989333391 + 0.001 * 7.028194904327393
Epoch 930, val loss: 1.2556339502334595
Epoch 940, training loss: 0.010268149897456169 = 0.0032376907765865326 + 0.001 * 7.030459403991699
Epoch 940, val loss: 1.2593029737472534
Epoch 950, training loss: 0.010190537199378014 = 0.003159761428833008 + 0.001 * 7.03077507019043
Epoch 950, val loss: 1.2629021406173706
Epoch 960, training loss: 0.010127871297299862 = 0.003085182048380375 + 0.001 * 7.042688846588135
Epoch 960, val loss: 1.2664345502853394
Epoch 970, training loss: 0.010046795010566711 = 0.0030137337744235992 + 0.001 * 7.033061504364014
Epoch 970, val loss: 1.269898533821106
Epoch 980, training loss: 0.009976676665246487 = 0.0029452515300363302 + 0.001 * 7.031424522399902
Epoch 980, val loss: 1.2732819318771362
Epoch 990, training loss: 0.009930562227964401 = 0.0028795739635825157 + 0.001 * 7.050987243652344
Epoch 990, val loss: 1.2766166925430298
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8133895624670533
The final CL Acc:0.76667, 0.02362, The final GNN Acc:0.81427, 0.00287
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13220])
remove edge: torch.Size([2, 7970])
updated graph: torch.Size([2, 10634])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9427943229675293 = 1.9341975450515747 + 0.001 * 8.596820831298828
Epoch 0, val loss: 1.9319937229156494
Epoch 10, training loss: 1.932719349861145 = 1.9241225719451904 + 0.001 * 8.596770286560059
Epoch 10, val loss: 1.9221946001052856
Epoch 20, training loss: 1.9202795028686523 = 1.9116829633712769 + 0.001 * 8.596582412719727
Epoch 20, val loss: 1.9096732139587402
Epoch 30, training loss: 1.9029417037963867 = 1.8943455219268799 + 0.001 * 8.596144676208496
Epoch 30, val loss: 1.8919751644134521
Epoch 40, training loss: 1.877746820449829 = 1.8691518306732178 + 0.001 * 8.595047950744629
Epoch 40, val loss: 1.8664343357086182
Epoch 50, training loss: 1.8434696197509766 = 1.834877848625183 + 0.001 * 8.591814994812012
Epoch 50, val loss: 1.8332269191741943
Epoch 60, training loss: 1.8056892156600952 = 1.797108769416809 + 0.001 * 8.580490112304688
Epoch 60, val loss: 1.8001776933670044
Epoch 70, training loss: 1.7698372602462769 = 1.761305570602417 + 0.001 * 8.531726837158203
Epoch 70, val loss: 1.770139455795288
Epoch 80, training loss: 1.722320795059204 = 1.7140562534332275 + 0.001 * 8.264577865600586
Epoch 80, val loss: 1.7265089750289917
Epoch 90, training loss: 1.6563962697982788 = 1.6483348608016968 + 0.001 * 8.061407089233398
Epoch 90, val loss: 1.665954351425171
Epoch 100, training loss: 1.5700830221176147 = 1.5622121095657349 + 0.001 * 7.870907306671143
Epoch 100, val loss: 1.5902503728866577
Epoch 110, training loss: 1.4702993631362915 = 1.4627132415771484 + 0.001 * 7.586176872253418
Epoch 110, val loss: 1.5046451091766357
Epoch 120, training loss: 1.3665465116500854 = 1.3590611219406128 + 0.001 * 7.485394477844238
Epoch 120, val loss: 1.4188382625579834
Epoch 130, training loss: 1.2638194561004639 = 1.256367564201355 + 0.001 * 7.451924800872803
Epoch 130, val loss: 1.3372197151184082
Epoch 140, training loss: 1.1641064882278442 = 1.1566742658615112 + 0.001 * 7.432216167449951
Epoch 140, val loss: 1.259620189666748
Epoch 150, training loss: 1.0695319175720215 = 1.062119722366333 + 0.001 * 7.41225004196167
Epoch 150, val loss: 1.1867984533309937
Epoch 160, training loss: 0.9813333749771118 = 0.9739342927932739 + 0.001 * 7.39905309677124
Epoch 160, val loss: 1.118660807609558
Epoch 170, training loss: 0.8989414572715759 = 0.8915534019470215 + 0.001 * 7.3880743980407715
Epoch 170, val loss: 1.0546208620071411
Epoch 180, training loss: 0.8214429020881653 = 0.8140639662742615 + 0.001 * 7.378950595855713
Epoch 180, val loss: 0.9942258596420288
Epoch 190, training loss: 0.7484140396118164 = 0.7410433292388916 + 0.001 * 7.37073278427124
Epoch 190, val loss: 0.9379734992980957
Epoch 200, training loss: 0.6804770231246948 = 0.6731130480766296 + 0.001 * 7.363958835601807
Epoch 200, val loss: 0.8873822093009949
Epoch 210, training loss: 0.6185120940208435 = 0.6111534833908081 + 0.001 * 7.358584403991699
Epoch 210, val loss: 0.8442553877830505
Epoch 220, training loss: 0.5628852844238281 = 0.5555313229560852 + 0.001 * 7.353936672210693
Epoch 220, val loss: 0.8091241121292114
Epoch 230, training loss: 0.5132554769515991 = 0.5059059858322144 + 0.001 * 7.349467754364014
Epoch 230, val loss: 0.7815316319465637
Epoch 240, training loss: 0.46862486004829407 = 0.46128013730049133 + 0.001 * 7.344710350036621
Epoch 240, val loss: 0.7599582076072693
Epoch 250, training loss: 0.4276196360588074 = 0.42028021812438965 + 0.001 * 7.3394317626953125
Epoch 250, val loss: 0.7425459623336792
Epoch 260, training loss: 0.38909533619880676 = 0.38176196813583374 + 0.001 * 7.333362579345703
Epoch 260, val loss: 0.7277795076370239
Epoch 270, training loss: 0.35254478454589844 = 0.34521785378456116 + 0.001 * 7.326927661895752
Epoch 270, val loss: 0.715087890625
Epoch 280, training loss: 0.31804150342941284 = 0.310722291469574 + 0.001 * 7.319224834442139
Epoch 280, val loss: 0.7045304179191589
Epoch 290, training loss: 0.2858477532863617 = 0.27854299545288086 + 0.001 * 7.304759502410889
Epoch 290, val loss: 0.6963931322097778
Epoch 300, training loss: 0.25595444440841675 = 0.248663991689682 + 0.001 * 7.290441989898682
Epoch 300, val loss: 0.6907232999801636
Epoch 310, training loss: 0.22807270288467407 = 0.22080674767494202 + 0.001 * 7.265959739685059
Epoch 310, val loss: 0.6874719262123108
Epoch 320, training loss: 0.20197387039661407 = 0.19472193717956543 + 0.001 * 7.251938343048096
Epoch 320, val loss: 0.6865577101707458
Epoch 330, training loss: 0.17766810953617096 = 0.17043817043304443 + 0.001 * 7.2299346923828125
Epoch 330, val loss: 0.6880300045013428
Epoch 340, training loss: 0.15547122061252594 = 0.14825016260147095 + 0.001 * 7.221063613891602
Epoch 340, val loss: 0.6917479038238525
Epoch 350, training loss: 0.13569220900535583 = 0.12847372889518738 + 0.001 * 7.218472957611084
Epoch 350, val loss: 0.6976014971733093
Epoch 360, training loss: 0.11844676733016968 = 0.1112247034907341 + 0.001 * 7.22206449508667
Epoch 360, val loss: 0.7053101658821106
Epoch 370, training loss: 0.10362185537815094 = 0.09640470147132874 + 0.001 * 7.2171502113342285
Epoch 370, val loss: 0.7145258784294128
Epoch 380, training loss: 0.09098851680755615 = 0.0837719663977623 + 0.001 * 7.216546535491943
Epoch 380, val loss: 0.7250879406929016
Epoch 390, training loss: 0.08026652038097382 = 0.07304957509040833 + 0.001 * 7.216944694519043
Epoch 390, val loss: 0.7367178201675415
Epoch 400, training loss: 0.07118558883666992 = 0.06396827846765518 + 0.001 * 7.217313766479492
Epoch 400, val loss: 0.7492620348930359
Epoch 410, training loss: 0.06349468231201172 = 0.056277208030223846 + 0.001 * 7.217477798461914
Epoch 410, val loss: 0.7623886466026306
Epoch 420, training loss: 0.0569690465927124 = 0.04975153133273125 + 0.001 * 7.2175164222717285
Epoch 420, val loss: 0.7758445739746094
Epoch 430, training loss: 0.05141695588827133 = 0.04419960826635361 + 0.001 * 7.217349052429199
Epoch 430, val loss: 0.7894518971443176
Epoch 440, training loss: 0.04667513817548752 = 0.03945818543434143 + 0.001 * 7.216950416564941
Epoch 440, val loss: 0.8030595183372498
Epoch 450, training loss: 0.04262251406908035 = 0.03539324179291725 + 0.001 * 7.2292704582214355
Epoch 450, val loss: 0.8165004849433899
Epoch 460, training loss: 0.039112258702516556 = 0.031893305480480194 + 0.001 * 7.218954086303711
Epoch 460, val loss: 0.8296797275543213
Epoch 470, training loss: 0.03608595207333565 = 0.028866373002529144 + 0.001 * 7.219577789306641
Epoch 470, val loss: 0.8425875902175903
Epoch 480, training loss: 0.033454529941082 = 0.02623685635626316 + 0.001 * 7.2176737785339355
Epoch 480, val loss: 0.8551399111747742
Epoch 490, training loss: 0.031155822798609734 = 0.02394263818860054 + 0.001 * 7.213184833526611
Epoch 490, val loss: 0.8672972917556763
Epoch 500, training loss: 0.029144950211048126 = 0.021931786090135574 + 0.001 * 7.213164329528809
Epoch 500, val loss: 0.8790929913520813
Epoch 510, training loss: 0.027375109493732452 = 0.02016204595565796 + 0.001 * 7.213063716888428
Epoch 510, val loss: 0.8905045986175537
Epoch 520, training loss: 0.02580549195408821 = 0.018598105758428574 + 0.001 * 7.207385063171387
Epoch 520, val loss: 0.9015316963195801
Epoch 530, training loss: 0.024428024888038635 = 0.0172105822712183 + 0.001 * 7.217442989349365
Epoch 530, val loss: 0.912149965763092
Epoch 540, training loss: 0.02317952737212181 = 0.015974948182702065 + 0.001 * 7.204579830169678
Epoch 540, val loss: 0.9224320650100708
Epoch 550, training loss: 0.022073596715927124 = 0.014870327897369862 + 0.001 * 7.203269004821777
Epoch 550, val loss: 0.9323304891586304
Epoch 560, training loss: 0.021082594990730286 = 0.013879218138754368 + 0.001 * 7.2033772468566895
Epoch 560, val loss: 0.9419001340866089
Epoch 570, training loss: 0.020182881504297256 = 0.012987099587917328 + 0.001 * 7.195781230926514
Epoch 570, val loss: 0.9511196613311768
Epoch 580, training loss: 0.01940460130572319 = 0.012181369587779045 + 0.001 * 7.223230361938477
Epoch 580, val loss: 0.9600715637207031
Epoch 590, training loss: 0.018640074878931046 = 0.011451302096247673 + 0.001 * 7.188772678375244
Epoch 590, val loss: 0.9686628580093384
Epoch 600, training loss: 0.018019389361143112 = 0.0107877803966403 + 0.001 * 7.231608867645264
Epoch 600, val loss: 0.977013349533081
Epoch 610, training loss: 0.017377540469169617 = 0.010183043777942657 + 0.001 * 7.194495677947998
Epoch 610, val loss: 0.9850625395774841
Epoch 620, training loss: 0.01680946908891201 = 0.009630388580262661 + 0.001 * 7.179080009460449
Epoch 620, val loss: 0.992855966091156
Epoch 630, training loss: 0.016327468678355217 = 0.009123989380896091 + 0.001 * 7.203478813171387
Epoch 630, val loss: 1.0004208087921143
Epoch 640, training loss: 0.015821902081370354 = 0.008658642880618572 + 0.001 * 7.163259506225586
Epoch 640, val loss: 1.0077165365219116
Epoch 650, training loss: 0.015390182845294476 = 0.008229157887399197 + 0.001 * 7.161024570465088
Epoch 650, val loss: 1.0148298740386963
Epoch 660, training loss: 0.015020282939076424 = 0.00783002283424139 + 0.001 * 7.1902594566345215
Epoch 660, val loss: 1.0218008756637573
Epoch 670, training loss: 0.014630159363150597 = 0.007456648163497448 + 0.001 * 7.173511505126953
Epoch 670, val loss: 1.028657078742981
Epoch 680, training loss: 0.014270955696702003 = 0.0071063111536204815 + 0.001 * 7.16464376449585
Epoch 680, val loss: 1.0354574918746948
Epoch 690, training loss: 0.013946057297289371 = 0.006777357310056686 + 0.001 * 7.168699741363525
Epoch 690, val loss: 1.0422073602676392
Epoch 700, training loss: 0.013596933335065842 = 0.006468605250120163 + 0.001 * 7.128328323364258
Epoch 700, val loss: 1.048885464668274
Epoch 710, training loss: 0.013368112035095692 = 0.006179039366543293 + 0.001 * 7.189072132110596
Epoch 710, val loss: 1.0555013418197632
Epoch 720, training loss: 0.013038639910519123 = 0.005907685495913029 + 0.001 * 7.130954265594482
Epoch 720, val loss: 1.0620275735855103
Epoch 730, training loss: 0.012818114832043648 = 0.005653382278978825 + 0.001 * 7.164732456207275
Epoch 730, val loss: 1.0684655904769897
Epoch 740, training loss: 0.012580424547195435 = 0.005415124353021383 + 0.001 * 7.165299892425537
Epoch 740, val loss: 1.0747992992401123
Epoch 750, training loss: 0.012294359505176544 = 0.005192000884562731 + 0.001 * 7.102358818054199
Epoch 750, val loss: 1.0810117721557617
Epoch 760, training loss: 0.012126298621296883 = 0.004982844926416874 + 0.001 * 7.143453598022461
Epoch 760, val loss: 1.087092638015747
Epoch 770, training loss: 0.011929638683795929 = 0.004786586854606867 + 0.001 * 7.1430511474609375
Epoch 770, val loss: 1.0930854082107544
Epoch 780, training loss: 0.011693470180034637 = 0.004602359142154455 + 0.001 * 7.0911102294921875
Epoch 780, val loss: 1.0989254713058472
Epoch 790, training loss: 0.011544797569513321 = 0.004429328255355358 + 0.001 * 7.115468502044678
Epoch 790, val loss: 1.104628086090088
Epoch 800, training loss: 0.011399291455745697 = 0.004266615025699139 + 0.001 * 7.132676601409912
Epoch 800, val loss: 1.1102255582809448
Epoch 810, training loss: 0.011212067678570747 = 0.004113544709980488 + 0.001 * 7.098522186279297
Epoch 810, val loss: 1.1156995296478271
Epoch 820, training loss: 0.011057090014219284 = 0.003969320096075535 + 0.001 * 7.087769508361816
Epoch 820, val loss: 1.121041178703308
Epoch 830, training loss: 0.010931565426290035 = 0.0038333621341735125 + 0.001 * 7.098202705383301
Epoch 830, val loss: 1.1262739896774292
Epoch 840, training loss: 0.010781600140035152 = 0.003705032169818878 + 0.001 * 7.076567649841309
Epoch 840, val loss: 1.1313852071762085
Epoch 850, training loss: 0.010666093789041042 = 0.0035838033072650433 + 0.001 * 7.082290172576904
Epoch 850, val loss: 1.1363837718963623
Epoch 860, training loss: 0.010566461831331253 = 0.003469133749604225 + 0.001 * 7.097327709197998
Epoch 860, val loss: 1.141265869140625
Epoch 870, training loss: 0.010438947007060051 = 0.0033606470096856356 + 0.001 * 7.078299522399902
Epoch 870, val loss: 1.146048665046692
Epoch 880, training loss: 0.010406887158751488 = 0.003257843665778637 + 0.001 * 7.149043083190918
Epoch 880, val loss: 1.1507049798965454
Epoch 890, training loss: 0.010230131447315216 = 0.0031603872776031494 + 0.001 * 7.0697431564331055
Epoch 890, val loss: 1.1552530527114868
Epoch 900, training loss: 0.010185942985117435 = 0.0030678973998874426 + 0.001 * 7.118044853210449
Epoch 900, val loss: 1.1597357988357544
Epoch 910, training loss: 0.010041841305792332 = 0.002980070188641548 + 0.001 * 7.061770915985107
Epoch 910, val loss: 1.164061188697815
Epoch 920, training loss: 0.010013489052653313 = 0.0028965568635612726 + 0.001 * 7.116931915283203
Epoch 920, val loss: 1.1683590412139893
Epoch 930, training loss: 0.009869576431810856 = 0.0028171304147690535 + 0.001 * 7.052445888519287
Epoch 930, val loss: 1.1724976301193237
Epoch 940, training loss: 0.009802605025470257 = 0.0027414981741458178 + 0.001 * 7.061106204986572
Epoch 940, val loss: 1.1766141653060913
Epoch 950, training loss: 0.00974894780665636 = 0.0026693991385400295 + 0.001 * 7.079548358917236
Epoch 950, val loss: 1.1806045770645142
Epoch 960, training loss: 0.009665374644100666 = 0.0026006801053881645 + 0.001 * 7.064694404602051
Epoch 960, val loss: 1.1845067739486694
Epoch 970, training loss: 0.009620721451938152 = 0.0025350749492645264 + 0.001 * 7.085646152496338
Epoch 970, val loss: 1.1883258819580078
Epoch 980, training loss: 0.009525116533041 = 0.0024724213872104883 + 0.001 * 7.052695274353027
Epoch 980, val loss: 1.1920912265777588
Epoch 990, training loss: 0.009483653120696545 = 0.002412558300420642 + 0.001 * 7.071094036102295
Epoch 990, val loss: 1.1957708597183228
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 1.96047043800354 = 1.9518736600875854 + 0.001 * 8.59676742553711
Epoch 0, val loss: 1.9474421739578247
Epoch 10, training loss: 1.948993444442749 = 1.940396785736084 + 0.001 * 8.596691131591797
Epoch 10, val loss: 1.937098503112793
Epoch 20, training loss: 1.9350299835205078 = 1.9264335632324219 + 0.001 * 8.596417427062988
Epoch 20, val loss: 1.9243448972702026
Epoch 30, training loss: 1.9158024787902832 = 1.907206654548645 + 0.001 * 8.59579086303711
Epoch 30, val loss: 1.9067715406417847
Epoch 40, training loss: 1.888152837753296 = 1.8795586824417114 + 0.001 * 8.594188690185547
Epoch 40, val loss: 1.8819235563278198
Epoch 50, training loss: 1.850597620010376 = 1.8420088291168213 + 0.001 * 8.58873176574707
Epoch 50, val loss: 1.8497706651687622
Epoch 60, training loss: 1.8087902069091797 = 1.8002264499664307 + 0.001 * 8.56370735168457
Epoch 60, val loss: 1.8164033889770508
Epoch 70, training loss: 1.7694969177246094 = 1.7610710859298706 + 0.001 * 8.42582893371582
Epoch 70, val loss: 1.7823858261108398
Epoch 80, training loss: 1.720342755317688 = 1.7122424840927124 + 0.001 * 8.100310325622559
Epoch 80, val loss: 1.734399437904358
Epoch 90, training loss: 1.653362512588501 = 1.6453580856323242 + 0.001 * 8.004426002502441
Epoch 90, val loss: 1.6738280057907104
Epoch 100, training loss: 1.5697557926177979 = 1.561795711517334 + 0.001 * 7.960054397583008
Epoch 100, val loss: 1.6061854362487793
Epoch 110, training loss: 1.482107162475586 = 1.4741950035095215 + 0.001 * 7.912184715270996
Epoch 110, val loss: 1.534144401550293
Epoch 120, training loss: 1.399577260017395 = 1.3917771577835083 + 0.001 * 7.800079822540283
Epoch 120, val loss: 1.4669684171676636
Epoch 130, training loss: 1.3218839168548584 = 1.3142176866531372 + 0.001 * 7.6662702560424805
Epoch 130, val loss: 1.4024778604507446
Epoch 140, training loss: 1.245373249053955 = 1.237723708152771 + 0.001 * 7.649585723876953
Epoch 140, val loss: 1.3393378257751465
Epoch 150, training loss: 1.168575644493103 = 1.160955548286438 + 0.001 * 7.620102405548096
Epoch 150, val loss: 1.2773088216781616
Epoch 160, training loss: 1.0927062034606934 = 1.0851101875305176 + 0.001 * 7.596027374267578
Epoch 160, val loss: 1.2174853086471558
Epoch 170, training loss: 1.019163727760315 = 1.0116045475006104 + 0.001 * 7.559138774871826
Epoch 170, val loss: 1.1615499258041382
Epoch 180, training loss: 0.9490262866020203 = 0.9415246844291687 + 0.001 * 7.501619815826416
Epoch 180, val loss: 1.1090483665466309
Epoch 190, training loss: 0.8829715251922607 = 0.8755517601966858 + 0.001 * 7.419747352600098
Epoch 190, val loss: 1.0603559017181396
Epoch 200, training loss: 0.8212041854858398 = 0.8138499855995178 + 0.001 * 7.354175567626953
Epoch 200, val loss: 1.0157489776611328
Epoch 210, training loss: 0.7633320093154907 = 0.7560092806816101 + 0.001 * 7.322750091552734
Epoch 210, val loss: 0.9755260348320007
Epoch 220, training loss: 0.7082018256187439 = 0.7008958458900452 + 0.001 * 7.305996417999268
Epoch 220, val loss: 0.9394608736038208
Epoch 230, training loss: 0.6544677019119263 = 0.6471689343452454 + 0.001 * 7.298742294311523
Epoch 230, val loss: 0.906520664691925
Epoch 240, training loss: 0.6014780402183533 = 0.5941876173019409 + 0.001 * 7.290395736694336
Epoch 240, val loss: 0.8757470846176147
Epoch 250, training loss: 0.5496712327003479 = 0.5423935651779175 + 0.001 * 7.277683258056641
Epoch 250, val loss: 0.846905529499054
Epoch 260, training loss: 0.49981966614723206 = 0.49255597591400146 + 0.001 * 7.263696193695068
Epoch 260, val loss: 0.8207106590270996
Epoch 270, training loss: 0.45205962657928467 = 0.4448148012161255 + 0.001 * 7.244838714599609
Epoch 270, val loss: 0.7984070181846619
Epoch 280, training loss: 0.4057430326938629 = 0.3985156714916229 + 0.001 * 7.22736930847168
Epoch 280, val loss: 0.7810880541801453
Epoch 290, training loss: 0.3605479598045349 = 0.3533328175544739 + 0.001 * 7.215134143829346
Epoch 290, val loss: 0.7696557641029358
Epoch 300, training loss: 0.3171510696411133 = 0.30994144082069397 + 0.001 * 7.20962381362915
Epoch 300, val loss: 0.7644990086555481
Epoch 310, training loss: 0.2768925428390503 = 0.2696884870529175 + 0.001 * 7.2040696144104
Epoch 310, val loss: 0.7652002573013306
Epoch 320, training loss: 0.24093472957611084 = 0.2337295264005661 + 0.001 * 7.205206871032715
Epoch 320, val loss: 0.7712069153785706
Epoch 330, training loss: 0.20972052216529846 = 0.2025202363729477 + 0.001 * 7.200290203094482
Epoch 330, val loss: 0.7818145155906677
Epoch 340, training loss: 0.18302972614765167 = 0.17583045363426208 + 0.001 * 7.199276447296143
Epoch 340, val loss: 0.795829176902771
Epoch 350, training loss: 0.1602923423051834 = 0.1530950516462326 + 0.001 * 7.19728422164917
Epoch 350, val loss: 0.8123358488082886
Epoch 360, training loss: 0.14090187847614288 = 0.13370642066001892 + 0.001 * 7.1954522132873535
Epoch 360, val loss: 0.8308520317077637
Epoch 370, training loss: 0.12435034662485123 = 0.11714962869882584 + 0.001 * 7.200716018676758
Epoch 370, val loss: 0.8509668707847595
Epoch 380, training loss: 0.11019632965326309 = 0.10300322622060776 + 0.001 * 7.193100929260254
Epoch 380, val loss: 0.8722826838493347
Epoch 390, training loss: 0.09808440506458282 = 0.09089520573616028 + 0.001 * 7.189198970794678
Epoch 390, val loss: 0.8943626284599304
Epoch 400, training loss: 0.08769447356462479 = 0.0805080384016037 + 0.001 * 7.186438083648682
Epoch 400, val loss: 0.9168490767478943
Epoch 410, training loss: 0.07875188440084457 = 0.071569062769413 + 0.001 * 7.1828203201293945
Epoch 410, val loss: 0.9395098090171814
Epoch 420, training loss: 0.07103005051612854 = 0.06384941190481186 + 0.001 * 7.18063497543335
Epoch 420, val loss: 0.9621109366416931
Epoch 430, training loss: 0.0643334612250328 = 0.05715668201446533 + 0.001 * 7.176779270172119
Epoch 430, val loss: 0.9844379425048828
Epoch 440, training loss: 0.05850626528263092 = 0.05133238434791565 + 0.001 * 7.1738786697387695
Epoch 440, val loss: 1.006402611732483
Epoch 450, training loss: 0.05342032015323639 = 0.04624832794070244 + 0.001 * 7.171993732452393
Epoch 450, val loss: 1.0279080867767334
Epoch 460, training loss: 0.048960261046886444 = 0.04179694503545761 + 0.001 * 7.163315296173096
Epoch 460, val loss: 1.0489367246627808
Epoch 470, training loss: 0.04505199193954468 = 0.03788837790489197 + 0.001 * 7.163614273071289
Epoch 470, val loss: 1.0694109201431274
Epoch 480, training loss: 0.04161171615123749 = 0.034447453916072845 + 0.001 * 7.164261341094971
Epoch 480, val loss: 1.089337706565857
Epoch 490, training loss: 0.038578521460294724 = 0.03141077607870102 + 0.001 * 7.167743682861328
Epoch 490, val loss: 1.1087125539779663
Epoch 500, training loss: 0.03587023913860321 = 0.028724197298288345 + 0.001 * 7.146039962768555
Epoch 500, val loss: 1.1274977922439575
Epoch 510, training loss: 0.03349190577864647 = 0.026341533288359642 + 0.001 * 7.150371551513672
Epoch 510, val loss: 1.1456950902938843
Epoch 520, training loss: 0.03136330470442772 = 0.024223212152719498 + 0.001 * 7.140093803405762
Epoch 520, val loss: 1.1633437871932983
Epoch 530, training loss: 0.029480231925845146 = 0.022335387766361237 + 0.001 * 7.144843578338623
Epoch 530, val loss: 1.180457353591919
Epoch 540, training loss: 0.027789834886789322 = 0.020648999139666557 + 0.001 * 7.140835285186768
Epoch 540, val loss: 1.1970274448394775
Epoch 550, training loss: 0.02627195604145527 = 0.019138621166348457 + 0.001 * 7.133334636688232
Epoch 550, val loss: 1.2130796909332275
Epoch 560, training loss: 0.02491435781121254 = 0.017782365903258324 + 0.001 * 7.131990909576416
Epoch 560, val loss: 1.2286442518234253
Epoch 570, training loss: 0.02371055632829666 = 0.01656140387058258 + 0.001 * 7.149152755737305
Epoch 570, val loss: 1.2437361478805542
Epoch 580, training loss: 0.02258923649787903 = 0.015458586625754833 + 0.001 * 7.130650043487549
Epoch 580, val loss: 1.2583632469177246
Epoch 590, training loss: 0.021587688475847244 = 0.01445675827562809 + 0.001 * 7.130928993225098
Epoch 590, val loss: 1.2726246118545532
Epoch 600, training loss: 0.02066796086728573 = 0.01354044396430254 + 0.001 * 7.127517223358154
Epoch 600, val loss: 1.286658525466919
Epoch 610, training loss: 0.019828569144010544 = 0.012699182145297527 + 0.001 * 7.1293864250183105
Epoch 610, val loss: 1.3004878759384155
Epoch 620, training loss: 0.019055448472499847 = 0.01192582305520773 + 0.001 * 7.129624366760254
Epoch 620, val loss: 1.3141281604766846
Epoch 630, training loss: 0.01833999715745449 = 0.0112147880718112 + 0.001 * 7.125208377838135
Epoch 630, val loss: 1.327555775642395
Epoch 640, training loss: 0.01768241822719574 = 0.01056104525923729 + 0.001 * 7.121372222900391
Epoch 640, val loss: 1.3407255411148071
Epoch 650, training loss: 0.01707797311246395 = 0.009959876537322998 + 0.001 * 7.118096828460693
Epoch 650, val loss: 1.3536421060562134
Epoch 660, training loss: 0.016526399180293083 = 0.009406843222677708 + 0.001 * 7.119555950164795
Epoch 660, val loss: 1.366263508796692
Epoch 670, training loss: 0.01601274684071541 = 0.008897697553038597 + 0.001 * 7.115049362182617
Epoch 670, val loss: 1.3785781860351562
Epoch 680, training loss: 0.015553695149719715 = 0.008428514003753662 + 0.001 * 7.125180721282959
Epoch 680, val loss: 1.3905912637710571
Epoch 690, training loss: 0.015125399455428123 = 0.00799569021910429 + 0.001 * 7.129708290100098
Epoch 690, val loss: 1.4023139476776123
Epoch 700, training loss: 0.014715023338794708 = 0.0075958785600960255 + 0.001 * 7.119143962860107
Epoch 700, val loss: 1.4137096405029297
Epoch 710, training loss: 0.014339706860482693 = 0.007226037792861462 + 0.001 * 7.113668918609619
Epoch 710, val loss: 1.4248305559158325
Epoch 720, training loss: 0.013992039486765862 = 0.006883428432047367 + 0.001 * 7.108611106872559
Epoch 720, val loss: 1.4356403350830078
Epoch 730, training loss: 0.01367142889648676 = 0.006565575022250414 + 0.001 * 7.10585355758667
Epoch 730, val loss: 1.4461828470230103
Epoch 740, training loss: 0.013388228602707386 = 0.006270264741033316 + 0.001 * 7.1179633140563965
Epoch 740, val loss: 1.4564306735992432
Epoch 750, training loss: 0.01309729740023613 = 0.005995540414005518 + 0.001 * 7.101756572723389
Epoch 750, val loss: 1.4664180278778076
Epoch 760, training loss: 0.012841046787798405 = 0.005739562213420868 + 0.001 * 7.101484298706055
Epoch 760, val loss: 1.4761501550674438
Epoch 770, training loss: 0.012613107450306416 = 0.005500705912709236 + 0.001 * 7.112401008605957
Epoch 770, val loss: 1.4856325387954712
Epoch 780, training loss: 0.012381738051772118 = 0.005277540534734726 + 0.001 * 7.104196548461914
Epoch 780, val loss: 1.494866967201233
Epoch 790, training loss: 0.012172898277640343 = 0.005068764556199312 + 0.001 * 7.104133605957031
Epoch 790, val loss: 1.5038971900939941
Epoch 800, training loss: 0.011982793919742107 = 0.004873200319707394 + 0.001 * 7.109593391418457
Epoch 800, val loss: 1.512668251991272
Epoch 810, training loss: 0.011785987764596939 = 0.004689756780862808 + 0.001 * 7.096230983734131
Epoch 810, val loss: 1.5212364196777344
Epoch 820, training loss: 0.01161528006196022 = 0.004517477937042713 + 0.001 * 7.097801685333252
Epoch 820, val loss: 1.5295838117599487
Epoch 830, training loss: 0.011447855271399021 = 0.004355482291430235 + 0.001 * 7.092372417449951
Epoch 830, val loss: 1.5377330780029297
Epoch 840, training loss: 0.011284930631518364 = 0.004202976357191801 + 0.001 * 7.081954002380371
Epoch 840, val loss: 1.545674443244934
Epoch 850, training loss: 0.011145025491714478 = 0.004059232771396637 + 0.001 * 7.0857930183410645
Epoch 850, val loss: 1.5534393787384033
Epoch 860, training loss: 0.01102009043097496 = 0.003923631273210049 + 0.001 * 7.096458911895752
Epoch 860, val loss: 1.5610103607177734
Epoch 870, training loss: 0.010872391983866692 = 0.0037955567240715027 + 0.001 * 7.076834678649902
Epoch 870, val loss: 1.5684040784835815
Epoch 880, training loss: 0.010754059068858624 = 0.0036744712851941586 + 0.001 * 7.079587459564209
Epoch 880, val loss: 1.5756210088729858
Epoch 890, training loss: 0.010641742497682571 = 0.0035598704125732183 + 0.001 * 7.081871509552002
Epoch 890, val loss: 1.5826665163040161
Epoch 900, training loss: 0.010522278025746346 = 0.0034513219725340605 + 0.001 * 7.070955753326416
Epoch 900, val loss: 1.5895682573318481
Epoch 910, training loss: 0.010420204140245914 = 0.003348398255184293 + 0.001 * 7.071805953979492
Epoch 910, val loss: 1.5963071584701538
Epoch 920, training loss: 0.010321281850337982 = 0.003250728128477931 + 0.001 * 7.070553779602051
Epoch 920, val loss: 1.602900505065918
Epoch 930, training loss: 0.010237631388008595 = 0.0031579420901834965 + 0.001 * 7.079689025878906
Epoch 930, val loss: 1.6093441247940063
Epoch 940, training loss: 0.010143927298486233 = 0.0030697546899318695 + 0.001 * 7.074172496795654
Epoch 940, val loss: 1.6156538724899292
Epoch 950, training loss: 0.010051446035504341 = 0.0029858483467251062 + 0.001 * 7.065597057342529
Epoch 950, val loss: 1.6218243837356567
Epoch 960, training loss: 0.009978796355426311 = 0.002905950415879488 + 0.001 * 7.072845458984375
Epoch 960, val loss: 1.6278687715530396
Epoch 970, training loss: 0.009891783818602562 = 0.0028298150282353163 + 0.001 * 7.061968803405762
Epoch 970, val loss: 1.6337802410125732
Epoch 980, training loss: 0.009836167097091675 = 0.0027572093531489372 + 0.001 * 7.0789570808410645
Epoch 980, val loss: 1.63956880569458
Epoch 990, training loss: 0.009783383458852768 = 0.0026879252400249243 + 0.001 * 7.095457553863525
Epoch 990, val loss: 1.6452385187149048
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 1.9414594173431396 = 1.932862639427185 + 0.001 * 8.59679889678955
Epoch 0, val loss: 1.9259469509124756
Epoch 10, training loss: 1.931619644165039 = 1.9230228662490845 + 0.001 * 8.596734046936035
Epoch 10, val loss: 1.9158343076705933
Epoch 20, training loss: 1.9195256233215332 = 1.9109290838241577 + 0.001 * 8.596535682678223
Epoch 20, val loss: 1.9033790826797485
Epoch 30, training loss: 1.9025496244430542 = 1.8939534425735474 + 0.001 * 8.596122741699219
Epoch 30, val loss: 1.8862148523330688
Epoch 40, training loss: 1.8779240846633911 = 1.8693288564682007 + 0.001 * 8.595187187194824
Epoch 40, val loss: 1.862011194229126
Epoch 50, training loss: 1.8445781469345093 = 1.835985779762268 + 0.001 * 8.592390060424805
Epoch 50, val loss: 1.8311488628387451
Epoch 60, training loss: 1.8081430196762085 = 1.7995615005493164 + 0.001 * 8.581558227539062
Epoch 60, val loss: 1.8012721538543701
Epoch 70, training loss: 1.775080919265747 = 1.7665562629699707 + 0.001 * 8.52468490600586
Epoch 70, val loss: 1.7756997346878052
Epoch 80, training loss: 1.7307089567184448 = 1.722564458847046 + 0.001 * 8.14453125
Epoch 80, val loss: 1.736897587776184
Epoch 90, training loss: 1.6673365831375122 = 1.6593523025512695 + 0.001 * 7.984323501586914
Epoch 90, val loss: 1.6808993816375732
Epoch 100, training loss: 1.5846480131149292 = 1.5767428874969482 + 0.001 * 7.90512752532959
Epoch 100, val loss: 1.6121351718902588
Epoch 110, training loss: 1.4910320043563843 = 1.4832526445388794 + 0.001 * 7.7794084548950195
Epoch 110, val loss: 1.5354325771331787
Epoch 120, training loss: 1.3967212438583374 = 1.3890881538391113 + 0.001 * 7.633081912994385
Epoch 120, val loss: 1.459883689880371
Epoch 130, training loss: 1.3038034439086914 = 1.2962440252304077 + 0.001 * 7.559418678283691
Epoch 130, val loss: 1.3871556520462036
Epoch 140, training loss: 1.210246205329895 = 1.2027769088745117 + 0.001 * 7.469290733337402
Epoch 140, val loss: 1.314549207687378
Epoch 150, training loss: 1.1175129413604736 = 1.1101136207580566 + 0.001 * 7.399372100830078
Epoch 150, val loss: 1.2431062459945679
Epoch 160, training loss: 1.0294158458709717 = 1.022037148475647 + 0.001 * 7.378650665283203
Epoch 160, val loss: 1.1762933731079102
Epoch 170, training loss: 0.9477587342262268 = 0.9403887987136841 + 0.001 * 7.369917869567871
Epoch 170, val loss: 1.1154053211212158
Epoch 180, training loss: 0.8710769414901733 = 0.8637160062789917 + 0.001 * 7.360952854156494
Epoch 180, val loss: 1.0589537620544434
Epoch 190, training loss: 0.7964786887168884 = 0.7891239523887634 + 0.001 * 7.354721546173096
Epoch 190, val loss: 1.0035912990570068
Epoch 200, training loss: 0.7218984365463257 = 0.7145476341247559 + 0.001 * 7.350801467895508
Epoch 200, val loss: 0.9476181268692017
Epoch 210, training loss: 0.6470300555229187 = 0.6396818161010742 + 0.001 * 7.348237991333008
Epoch 210, val loss: 0.8909119963645935
Epoch 220, training loss: 0.5731561779975891 = 0.5658107399940491 + 0.001 * 7.345431804656982
Epoch 220, val loss: 0.8352873921394348
Epoch 230, training loss: 0.5025472044944763 = 0.4952058494091034 + 0.001 * 7.341331481933594
Epoch 230, val loss: 0.7832878232002258
Epoch 240, training loss: 0.43772178888320923 = 0.43038734793663025 + 0.001 * 7.334434986114502
Epoch 240, val loss: 0.737981379032135
Epoch 250, training loss: 0.3797084391117096 = 0.37238606810569763 + 0.001 * 7.32238245010376
Epoch 250, val loss: 0.7005352973937988
Epoch 260, training loss: 0.3282984793186188 = 0.32099515199661255 + 0.001 * 7.303322792053223
Epoch 260, val loss: 0.670585036277771
Epoch 270, training loss: 0.28296419978141785 = 0.2756752371788025 + 0.001 * 7.2889723777771
Epoch 270, val loss: 0.6472053527832031
Epoch 280, training loss: 0.24314366281032562 = 0.235882967710495 + 0.001 * 7.260700225830078
Epoch 280, val loss: 0.629395067691803
Epoch 290, training loss: 0.20847655832767487 = 0.2012353241443634 + 0.001 * 7.24122953414917
Epoch 290, val loss: 0.6165144443511963
Epoch 300, training loss: 0.17864979803562164 = 0.1714179515838623 + 0.001 * 7.231844902038574
Epoch 300, val loss: 0.6081644296646118
Epoch 310, training loss: 0.15330016613006592 = 0.1460804045200348 + 0.001 * 7.219755172729492
Epoch 310, val loss: 0.6039502024650574
Epoch 320, training loss: 0.1320028305053711 = 0.12478688359260559 + 0.001 * 7.2159504890441895
Epoch 320, val loss: 0.6034151911735535
Epoch 330, training loss: 0.11421575397253036 = 0.1070028766989708 + 0.001 * 7.21287727355957
Epoch 330, val loss: 0.6059820652008057
Epoch 340, training loss: 0.09939660131931305 = 0.09218503534793854 + 0.001 * 7.2115654945373535
Epoch 340, val loss: 0.6110507845878601
Epoch 350, training loss: 0.08705297857522964 = 0.07984244078397751 + 0.001 * 7.210540771484375
Epoch 350, val loss: 0.6179938912391663
Epoch 360, training loss: 0.07676050812005997 = 0.06954526901245117 + 0.001 * 7.215240955352783
Epoch 360, val loss: 0.6262996792793274
Epoch 370, training loss: 0.0681384727358818 = 0.060926806181669235 + 0.001 * 7.211665153503418
Epoch 370, val loss: 0.6355565786361694
Epoch 380, training loss: 0.06089210510253906 = 0.053684595972299576 + 0.001 * 7.207508563995361
Epoch 380, val loss: 0.645481526851654
Epoch 390, training loss: 0.054778553545475006 = 0.047566235065460205 + 0.001 * 7.212319850921631
Epoch 390, val loss: 0.6557785868644714
Epoch 400, training loss: 0.049581512808799744 = 0.0423678420484066 + 0.001 * 7.21367073059082
Epoch 400, val loss: 0.6663020253181458
Epoch 410, training loss: 0.04513015225529671 = 0.03792588412761688 + 0.001 * 7.204267978668213
Epoch 410, val loss: 0.6768618822097778
Epoch 420, training loss: 0.041311442852020264 = 0.03410973399877548 + 0.001 * 7.201709747314453
Epoch 420, val loss: 0.6873737573623657
Epoch 430, training loss: 0.038016561418771744 = 0.03081418015062809 + 0.001 * 7.2023820877075195
Epoch 430, val loss: 0.6977490186691284
Epoch 440, training loss: 0.035154376178979874 = 0.027954528108239174 + 0.001 * 7.199846267700195
Epoch 440, val loss: 0.7079516649246216
Epoch 450, training loss: 0.032663505524396896 = 0.02546202950179577 + 0.001 * 7.20147705078125
Epoch 450, val loss: 0.7179458737373352
Epoch 460, training loss: 0.03047812357544899 = 0.023279501125216484 + 0.001 * 7.198622703552246
Epoch 460, val loss: 0.7277095913887024
Epoch 470, training loss: 0.028555551543831825 = 0.02135997638106346 + 0.001 * 7.1955742835998535
Epoch 470, val loss: 0.7372260093688965
Epoch 480, training loss: 0.02686621993780136 = 0.019665110856294632 + 0.001 * 7.201109886169434
Epoch 480, val loss: 0.7465043067932129
Epoch 490, training loss: 0.025356724858283997 = 0.01816253736615181 + 0.001 * 7.194187164306641
Epoch 490, val loss: 0.7555258870124817
Epoch 500, training loss: 0.024013102054595947 = 0.016825348138809204 + 0.001 * 7.1877546310424805
Epoch 500, val loss: 0.7642951607704163
Epoch 510, training loss: 0.022823700681328773 = 0.01563083380460739 + 0.001 * 7.19286584854126
Epoch 510, val loss: 0.7728139758110046
Epoch 520, training loss: 0.02174486219882965 = 0.014560185372829437 + 0.001 * 7.184675693511963
Epoch 520, val loss: 0.7810866236686707
Epoch 530, training loss: 0.020797884091734886 = 0.013597220182418823 + 0.001 * 7.200664043426514
Epoch 530, val loss: 0.7891300916671753
Epoch 540, training loss: 0.01991852931678295 = 0.012728407047688961 + 0.001 * 7.190122127532959
Epoch 540, val loss: 0.7969534397125244
Epoch 550, training loss: 0.019114747643470764 = 0.011942197568714619 + 0.001 * 7.172549724578857
Epoch 550, val loss: 0.8045428991317749
Epoch 560, training loss: 0.018395379185676575 = 0.011228623799979687 + 0.001 * 7.166755676269531
Epoch 560, val loss: 0.8119342923164368
Epoch 570, training loss: 0.017769377678632736 = 0.010579169727861881 + 0.001 * 7.1902079582214355
Epoch 570, val loss: 0.8191208243370056
Epoch 580, training loss: 0.017163101583719254 = 0.009986553341150284 + 0.001 * 7.176547050476074
Epoch 580, val loss: 0.8261066675186157
Epoch 590, training loss: 0.016656160354614258 = 0.009444366209208965 + 0.001 * 7.211794376373291
Epoch 590, val loss: 0.832919716835022
Epoch 600, training loss: 0.016107534989714622 = 0.00894710049033165 + 0.001 * 7.160434722900391
Epoch 600, val loss: 0.8395185470581055
Epoch 610, training loss: 0.015645133331418037 = 0.008489972911775112 + 0.001 * 7.155159950256348
Epoch 610, val loss: 0.8459532856941223
Epoch 620, training loss: 0.015225104987621307 = 0.008068800903856754 + 0.001 * 7.156303405761719
Epoch 620, val loss: 0.8522108793258667
Epoch 630, training loss: 0.014827996492385864 = 0.007679938338696957 + 0.001 * 7.14805793762207
Epoch 630, val loss: 0.8582978844642639
Epoch 640, training loss: 0.014522173441946507 = 0.007320207543671131 + 0.001 * 7.20196533203125
Epoch 640, val loss: 0.8642469048500061
Epoch 650, training loss: 0.014127952978014946 = 0.006986875087022781 + 0.001 * 7.141077995300293
Epoch 650, val loss: 0.8700066208839417
Epoch 660, training loss: 0.013815958052873611 = 0.006677336059510708 + 0.001 * 7.1386213302612305
Epoch 660, val loss: 0.8756455779075623
Epoch 670, training loss: 0.013511709868907928 = 0.006389331538230181 + 0.001 * 7.122377395629883
Epoch 670, val loss: 0.8811401724815369
Epoch 680, training loss: 0.013237783685326576 = 0.0061209434643387794 + 0.001 * 7.116840362548828
Epoch 680, val loss: 0.8865117430686951
Epoch 690, training loss: 0.013028069399297237 = 0.005870477762073278 + 0.001 * 7.157591342926025
Epoch 690, val loss: 0.891734778881073
Epoch 700, training loss: 0.012761358171701431 = 0.005636369809508324 + 0.001 * 7.124987602233887
Epoch 700, val loss: 0.8968258500099182
Epoch 710, training loss: 0.012574495747685432 = 0.0054172188974916935 + 0.001 * 7.157276153564453
Epoch 710, val loss: 0.9018113017082214
Epoch 720, training loss: 0.012342378497123718 = 0.005211762152612209 + 0.001 * 7.130616188049316
Epoch 720, val loss: 0.906663715839386
Epoch 730, training loss: 0.012128850445151329 = 0.005018906202167273 + 0.001 * 7.109943866729736
Epoch 730, val loss: 0.9114160537719727
Epoch 740, training loss: 0.012064031325280666 = 0.004837610758841038 + 0.001 * 7.2264204025268555
Epoch 740, val loss: 0.9160387516021729
Epoch 750, training loss: 0.011767501011490822 = 0.0046670082956552505 + 0.001 * 7.10049295425415
Epoch 750, val loss: 0.920567512512207
Epoch 760, training loss: 0.011627105996012688 = 0.004506239667534828 + 0.001 * 7.120865821838379
Epoch 760, val loss: 0.9250016808509827
Epoch 770, training loss: 0.011456038802862167 = 0.0043545677326619625 + 0.001 * 7.101471424102783
Epoch 770, val loss: 0.9293239712715149
Epoch 780, training loss: 0.011311616748571396 = 0.0042113144882023335 + 0.001 * 7.100301742553711
Epoch 780, val loss: 0.9335557222366333
Epoch 790, training loss: 0.011146695353090763 = 0.0040758405812084675 + 0.001 * 7.070854663848877
Epoch 790, val loss: 0.9377025365829468
Epoch 800, training loss: 0.011030493304133415 = 0.00394763657823205 + 0.001 * 7.082856178283691
Epoch 800, val loss: 0.9417229294776917
Epoch 810, training loss: 0.010897746309638023 = 0.003826174885034561 + 0.001 * 7.071570873260498
Epoch 810, val loss: 0.9457057118415833
Epoch 820, training loss: 0.010805431753396988 = 0.0037109891418367624 + 0.001 * 7.094442367553711
Epoch 820, val loss: 0.9495688080787659
Epoch 830, training loss: 0.010705935768783092 = 0.00360164069570601 + 0.001 * 7.104294300079346
Epoch 830, val loss: 0.953335702419281
Epoch 840, training loss: 0.010575422085821629 = 0.0034977479372173548 + 0.001 * 7.077673435211182
Epoch 840, val loss: 0.9570549726486206
Epoch 850, training loss: 0.010449484921991825 = 0.003398950444534421 + 0.001 * 7.050533771514893
Epoch 850, val loss: 0.9606754779815674
Epoch 860, training loss: 0.010378112085163593 = 0.0033049157354980707 + 0.001 * 7.073195934295654
Epoch 860, val loss: 0.9642273783683777
Epoch 870, training loss: 0.010282667353749275 = 0.003215349745005369 + 0.001 * 7.067317962646484
Epoch 870, val loss: 0.9677025675773621
Epoch 880, training loss: 0.010166976600885391 = 0.003129977732896805 + 0.001 * 7.036998271942139
Epoch 880, val loss: 0.9710980653762817
Epoch 890, training loss: 0.010085491463541985 = 0.0030485366005450487 + 0.001 * 7.036954402923584
Epoch 890, val loss: 0.9744442701339722
Epoch 900, training loss: 0.010063320398330688 = 0.0029707890935242176 + 0.001 * 7.092530727386475
Epoch 900, val loss: 0.9777054190635681
Epoch 910, training loss: 0.009958120994269848 = 0.0028965056408196688 + 0.001 * 7.061615467071533
Epoch 910, val loss: 0.9809170365333557
Epoch 920, training loss: 0.009884543716907501 = 0.0028255023062229156 + 0.001 * 7.059040546417236
Epoch 920, val loss: 0.9840442538261414
Epoch 930, training loss: 0.009845498949289322 = 0.002757548587396741 + 0.001 * 7.087950229644775
Epoch 930, val loss: 0.9871266484260559
Epoch 940, training loss: 0.009736120700836182 = 0.0026925154961645603 + 0.001 * 7.043604850769043
Epoch 940, val loss: 0.9901289343833923
Epoch 950, training loss: 0.00968346931040287 = 0.002630234230309725 + 0.001 * 7.053234100341797
Epoch 950, val loss: 0.9930751323699951
Epoch 960, training loss: 0.009625902399420738 = 0.0025705392472445965 + 0.001 * 7.055362224578857
Epoch 960, val loss: 0.9959834218025208
Epoch 970, training loss: 0.009561911225318909 = 0.0025132859591394663 + 0.001 * 7.048624515533447
Epoch 970, val loss: 0.9988207817077637
Epoch 980, training loss: 0.00947487261146307 = 0.0024583579506725073 + 0.001 * 7.016514778137207
Epoch 980, val loss: 1.0016071796417236
Epoch 990, training loss: 0.009429245255887508 = 0.0024056127294898033 + 0.001 * 7.023632049560547
Epoch 990, val loss: 1.00434410572052
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8402741170268846
The final CL Acc:0.80864, 0.01666, The final GNN Acc:0.83729, 0.00212
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10526])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9459444284439087 = 1.937347650527954 + 0.001 * 8.596831321716309
Epoch 0, val loss: 1.939823865890503
Epoch 10, training loss: 1.9357718229293823 = 1.9271750450134277 + 0.001 * 8.596734046936035
Epoch 10, val loss: 1.9299969673156738
Epoch 20, training loss: 1.9226514101028442 = 1.9140548706054688 + 0.001 * 8.5964937210083
Epoch 20, val loss: 1.9168199300765991
Epoch 30, training loss: 1.9039798974990845 = 1.8953839540481567 + 0.001 * 8.595932960510254
Epoch 30, val loss: 1.8977627754211426
Epoch 40, training loss: 1.877011775970459 = 1.8684172630310059 + 0.001 * 8.594489097595215
Epoch 40, val loss: 1.8706525564193726
Epoch 50, training loss: 1.8424580097198486 = 1.8338679075241089 + 0.001 * 8.590044021606445
Epoch 50, val loss: 1.8379580974578857
Epoch 60, training loss: 1.8109532594680786 = 1.8023802042007446 + 0.001 * 8.573002815246582
Epoch 60, val loss: 1.8112224340438843
Epoch 70, training loss: 1.7852559089660645 = 1.7767759561538696 + 0.001 * 8.479951858520508
Epoch 70, val loss: 1.7874397039413452
Epoch 80, training loss: 1.748434066772461 = 1.7402024269104004 + 0.001 * 8.23167610168457
Epoch 80, val loss: 1.7536779642105103
Epoch 90, training loss: 1.6966636180877686 = 1.6885242462158203 + 0.001 * 8.139333724975586
Epoch 90, val loss: 1.7094206809997559
Epoch 100, training loss: 1.6251237392425537 = 1.6170350313186646 + 0.001 * 8.088752746582031
Epoch 100, val loss: 1.648957371711731
Epoch 110, training loss: 1.541239619255066 = 1.5331822633743286 + 0.001 * 8.057373046875
Epoch 110, val loss: 1.5787314176559448
Epoch 120, training loss: 1.455973505973816 = 1.4479643106460571 + 0.001 * 8.009244918823242
Epoch 120, val loss: 1.5092942714691162
Epoch 130, training loss: 1.373080849647522 = 1.365228295326233 + 0.001 * 7.852509498596191
Epoch 130, val loss: 1.4440478086471558
Epoch 140, training loss: 1.2900588512420654 = 1.2823857069015503 + 0.001 * 7.673115253448486
Epoch 140, val loss: 1.3806813955307007
Epoch 150, training loss: 1.2054715156555176 = 1.1978837251663208 + 0.001 * 7.587759017944336
Epoch 150, val loss: 1.316978931427002
Epoch 160, training loss: 1.1215580701828003 = 1.1140462160110474 + 0.001 * 7.511820316314697
Epoch 160, val loss: 1.2548760175704956
Epoch 170, training loss: 1.0422358512878418 = 1.034744143486023 + 0.001 * 7.491660118103027
Epoch 170, val loss: 1.1975764036178589
Epoch 180, training loss: 0.969083309173584 = 0.9615975022315979 + 0.001 * 7.485801696777344
Epoch 180, val loss: 1.146544337272644
Epoch 190, training loss: 0.9004091024398804 = 0.89292973279953 + 0.001 * 7.479344844818115
Epoch 190, val loss: 1.1000218391418457
Epoch 200, training loss: 0.8326740860939026 = 0.8251990079879761 + 0.001 * 7.475060939788818
Epoch 200, val loss: 1.0547022819519043
Epoch 210, training loss: 0.7632670998573303 = 0.7557961344718933 + 0.001 * 7.470956802368164
Epoch 210, val loss: 1.008278250694275
Epoch 220, training loss: 0.6925418376922607 = 0.6850771307945251 + 0.001 * 7.464682102203369
Epoch 220, val loss: 0.9612464308738708
Epoch 230, training loss: 0.6239428520202637 = 0.6164882183074951 + 0.001 * 7.454621315002441
Epoch 230, val loss: 0.9170576333999634
Epoch 240, training loss: 0.5616071820259094 = 0.5541689991950989 + 0.001 * 7.438183784484863
Epoch 240, val loss: 0.8796200752258301
Epoch 250, training loss: 0.5074914693832397 = 0.5000665783882141 + 0.001 * 7.424863815307617
Epoch 250, val loss: 0.8509879112243652
Epoch 260, training loss: 0.4608476758003235 = 0.4534444808959961 + 0.001 * 7.403209209442139
Epoch 260, val loss: 0.8301610946655273
Epoch 270, training loss: 0.41948455572128296 = 0.41208991408348083 + 0.001 * 7.39463472366333
Epoch 270, val loss: 0.814693808555603
Epoch 280, training loss: 0.38097673654556274 = 0.373588502407074 + 0.001 * 7.388232707977295
Epoch 280, val loss: 0.8022242188453674
Epoch 290, training loss: 0.3434180021286011 = 0.33603328466415405 + 0.001 * 7.384725093841553
Epoch 290, val loss: 0.7909985780715942
Epoch 300, training loss: 0.3060225248336792 = 0.2986397445201874 + 0.001 * 7.382791996002197
Epoch 300, val loss: 0.7805383801460266
Epoch 310, training loss: 0.2693140208721161 = 0.26193422079086304 + 0.001 * 7.379804611206055
Epoch 310, val loss: 0.7715665698051453
Epoch 320, training loss: 0.2345903068780899 = 0.227207750082016 + 0.001 * 7.38254976272583
Epoch 320, val loss: 0.7650774717330933
Epoch 330, training loss: 0.20313797891139984 = 0.19576139748096466 + 0.001 * 7.376580715179443
Epoch 330, val loss: 0.7618907690048218
Epoch 340, training loss: 0.17567753791809082 = 0.16830278933048248 + 0.001 * 7.374755859375
Epoch 340, val loss: 0.7623209357261658
Epoch 350, training loss: 0.1521649807691574 = 0.1447916477918625 + 0.001 * 7.37332820892334
Epoch 350, val loss: 0.7663782238960266
Epoch 360, training loss: 0.13214394450187683 = 0.1247718408703804 + 0.001 * 7.372110366821289
Epoch 360, val loss: 0.7734853625297546
Epoch 370, training loss: 0.11510521173477173 = 0.10772328078746796 + 0.001 * 7.381933212280273
Epoch 370, val loss: 0.7830150127410889
Epoch 380, training loss: 0.10055822134017944 = 0.09318628162145615 + 0.001 * 7.371942520141602
Epoch 380, val loss: 0.7945078611373901
Epoch 390, training loss: 0.08815454691648483 = 0.0807836651802063 + 0.001 * 7.370882987976074
Epoch 390, val loss: 0.8074521422386169
Epoch 400, training loss: 0.07757852971553802 = 0.07020890712738037 + 0.001 * 7.369620323181152
Epoch 400, val loss: 0.8214432597160339
Epoch 410, training loss: 0.06857555359601974 = 0.061206985265016556 + 0.001 * 7.368570804595947
Epoch 410, val loss: 0.8361884355545044
Epoch 420, training loss: 0.06092701479792595 = 0.053557127714157104 + 0.001 * 7.369885444641113
Epoch 420, val loss: 0.8513646721839905
Epoch 430, training loss: 0.05443347617983818 = 0.047065988183021545 + 0.001 * 7.36748743057251
Epoch 430, val loss: 0.8667659759521484
Epoch 440, training loss: 0.048924557864665985 = 0.04155837371945381 + 0.001 * 7.366182804107666
Epoch 440, val loss: 0.8821727633476257
Epoch 450, training loss: 0.044245779514312744 = 0.036880575120449066 + 0.001 * 7.3652024269104
Epoch 450, val loss: 0.8973604440689087
Epoch 460, training loss: 0.040262527763843536 = 0.03289765492081642 + 0.001 * 7.364871978759766
Epoch 460, val loss: 0.9122788310050964
Epoch 470, training loss: 0.03685924783349037 = 0.02949536219239235 + 0.001 * 7.363885402679443
Epoch 470, val loss: 0.9268332719802856
Epoch 480, training loss: 0.033946242183446884 = 0.026576902717351913 + 0.001 * 7.369338035583496
Epoch 480, val loss: 0.9410002827644348
Epoch 490, training loss: 0.03142310678958893 = 0.024061482399702072 + 0.001 * 7.361623764038086
Epoch 490, val loss: 0.9547023177146912
Epoch 500, training loss: 0.02924528159201145 = 0.021882595494389534 + 0.001 * 7.362685203552246
Epoch 500, val loss: 0.9679431915283203
Epoch 510, training loss: 0.027356324717402458 = 0.01998629979789257 + 0.001 * 7.370024681091309
Epoch 510, val loss: 0.9807004332542419
Epoch 520, training loss: 0.025687213987112045 = 0.018327699974179268 + 0.001 * 7.359513282775879
Epoch 520, val loss: 0.9930126070976257
Epoch 530, training loss: 0.024229567497968674 = 0.016870001330971718 + 0.001 * 7.3595662117004395
Epoch 530, val loss: 1.0048857927322388
Epoch 540, training loss: 0.02294001542031765 = 0.015582852996885777 + 0.001 * 7.357161998748779
Epoch 540, val loss: 1.0162880420684814
Epoch 550, training loss: 0.021797316148877144 = 0.014441286213696003 + 0.001 * 7.356029033660889
Epoch 550, val loss: 1.0273152589797974
Epoch 560, training loss: 0.020782962441444397 = 0.013424575328826904 + 0.001 * 7.358387470245361
Epoch 560, val loss: 1.0379241704940796
Epoch 570, training loss: 0.019865501672029495 = 0.012515432201325893 + 0.001 * 7.350069522857666
Epoch 570, val loss: 1.0481423139572144
Epoch 580, training loss: 0.019049013033509254 = 0.011699398048222065 + 0.001 * 7.349615097045898
Epoch 580, val loss: 1.0580211877822876
Epoch 590, training loss: 0.01831153966486454 = 0.010964155197143555 + 0.001 * 7.347383499145508
Epoch 590, val loss: 1.0675785541534424
Epoch 600, training loss: 0.017644451931118965 = 0.010299469344317913 + 0.001 * 7.344981670379639
Epoch 600, val loss: 1.0767765045166016
Epoch 610, training loss: 0.017039766535162926 = 0.009696592576801777 + 0.001 * 7.343173027038574
Epoch 610, val loss: 1.085695743560791
Epoch 620, training loss: 0.01648925617337227 = 0.009148136712610722 + 0.001 * 7.34112024307251
Epoch 620, val loss: 1.0943305492401123
Epoch 630, training loss: 0.016000565141439438 = 0.008647708222270012 + 0.001 * 7.352856159210205
Epoch 630, val loss: 1.1026639938354492
Epoch 640, training loss: 0.015521341934800148 = 0.008189856074750423 + 0.001 * 7.331485748291016
Epoch 640, val loss: 1.1107511520385742
Epoch 650, training loss: 0.015104617923498154 = 0.007769836578518152 + 0.001 * 7.334781169891357
Epoch 650, val loss: 1.118574857711792
Epoch 660, training loss: 0.014714643359184265 = 0.007383626885712147 + 0.001 * 7.331015586853027
Epoch 660, val loss: 1.1261491775512695
Epoch 670, training loss: 0.014367720112204552 = 0.007027668412774801 + 0.001 * 7.340051174163818
Epoch 670, val loss: 1.1335163116455078
Epoch 680, training loss: 0.01403162069618702 = 0.006698885001242161 + 0.001 * 7.332735061645508
Epoch 680, val loss: 1.1406824588775635
Epoch 690, training loss: 0.013747954741120338 = 0.006394595839083195 + 0.001 * 7.353358268737793
Epoch 690, val loss: 1.1476051807403564
Epoch 700, training loss: 0.01343240961432457 = 0.00611239206045866 + 0.001 * 7.320016860961914
Epoch 700, val loss: 1.154356598854065
Epoch 710, training loss: 0.013183601200580597 = 0.005850150715559721 + 0.001 * 7.3334503173828125
Epoch 710, val loss: 1.160900592803955
Epoch 720, training loss: 0.012914618477225304 = 0.005605960264801979 + 0.001 * 7.308657169342041
Epoch 720, val loss: 1.1672375202178955
Epoch 730, training loss: 0.012684669345617294 = 0.0053782034665346146 + 0.001 * 7.306466102600098
Epoch 730, val loss: 1.1734230518341064
Epoch 740, training loss: 0.012477459385991096 = 0.005165386945009232 + 0.001 * 7.31207275390625
Epoch 740, val loss: 1.179417371749878
Epoch 750, training loss: 0.0122918039560318 = 0.0049662175588309765 + 0.001 * 7.32558536529541
Epoch 750, val loss: 1.185258388519287
Epoch 760, training loss: 0.012079796753823757 = 0.004779466427862644 + 0.001 * 7.30033016204834
Epoch 760, val loss: 1.190967321395874
Epoch 770, training loss: 0.011899586766958237 = 0.00460404297336936 + 0.001 * 7.295543670654297
Epoch 770, val loss: 1.1965242624282837
Epoch 780, training loss: 0.011744359508156776 = 0.004438959062099457 + 0.001 * 7.305399417877197
Epoch 780, val loss: 1.2019078731536865
Epoch 790, training loss: 0.011581994593143463 = 0.0042833974584937096 + 0.001 * 7.29859733581543
Epoch 790, val loss: 1.207190990447998
Epoch 800, training loss: 0.011428955011069775 = 0.004136550240218639 + 0.001 * 7.292404651641846
Epoch 800, val loss: 1.2123427391052246
Epoch 810, training loss: 0.01129211112856865 = 0.003997701685875654 + 0.001 * 7.29440975189209
Epoch 810, val loss: 1.2173569202423096
Epoch 820, training loss: 0.011159880086779594 = 0.0038661942817270756 + 0.001 * 7.293684959411621
Epoch 820, val loss: 1.222258448600769
Epoch 830, training loss: 0.011015214957296848 = 0.0037415705155581236 + 0.001 * 7.27364444732666
Epoch 830, val loss: 1.227067470550537
Epoch 840, training loss: 0.010938439518213272 = 0.0036234166473150253 + 0.001 * 7.315022945404053
Epoch 840, val loss: 1.2317638397216797
Epoch 850, training loss: 0.010778142139315605 = 0.0035111429169774055 + 0.001 * 7.266998767852783
Epoch 850, val loss: 1.2363674640655518
Epoch 860, training loss: 0.010674403049051762 = 0.003404340473935008 + 0.001 * 7.270062446594238
Epoch 860, val loss: 1.2408523559570312
Epoch 870, training loss: 0.010606668889522552 = 0.0033026759047061205 + 0.001 * 7.303992748260498
Epoch 870, val loss: 1.245261788368225
Epoch 880, training loss: 0.010490436106920242 = 0.0032058481592684984 + 0.001 * 7.284587860107422
Epoch 880, val loss: 1.2496322393417358
Epoch 890, training loss: 0.010375143960118294 = 0.0031136474572122097 + 0.001 * 7.261496543884277
Epoch 890, val loss: 1.2538331747055054
Epoch 900, training loss: 0.010308966971933842 = 0.0030256318859755993 + 0.001 * 7.283334732055664
Epoch 900, val loss: 1.2579947710037231
Epoch 910, training loss: 0.010188683867454529 = 0.0029415644239634275 + 0.001 * 7.247119426727295
Epoch 910, val loss: 1.2620805501937866
Epoch 920, training loss: 0.010171938687562943 = 0.002861323533579707 + 0.001 * 7.310615062713623
Epoch 920, val loss: 1.26607346534729
Epoch 930, training loss: 0.010058637708425522 = 0.002784727606922388 + 0.001 * 7.273909568786621
Epoch 930, val loss: 1.270005226135254
Epoch 940, training loss: 0.00999816507101059 = 0.0027116481214761734 + 0.001 * 7.286516189575195
Epoch 940, val loss: 1.2738330364227295
Epoch 950, training loss: 0.009924964979290962 = 0.0026418366469442844 + 0.001 * 7.283127784729004
Epoch 950, val loss: 1.2775566577911377
Epoch 960, training loss: 0.009829866699874401 = 0.00257499678991735 + 0.001 * 7.25486946105957
Epoch 960, val loss: 1.2812329530715942
Epoch 970, training loss: 0.009803857654333115 = 0.002510958584025502 + 0.001 * 7.292898654937744
Epoch 970, val loss: 1.2848602533340454
Epoch 980, training loss: 0.009689873084425926 = 0.0024496193509548903 + 0.001 * 7.24025297164917
Epoch 980, val loss: 1.2883769273757935
Epoch 990, training loss: 0.00962019618600607 = 0.002390799345448613 + 0.001 * 7.229396820068359
Epoch 990, val loss: 1.2918943166732788
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 1.938543677330017 = 1.929946780204773 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.9323970079421997
Epoch 10, training loss: 1.9298264980316162 = 1.9212297201156616 + 0.001 * 8.596802711486816
Epoch 10, val loss: 1.9235496520996094
Epoch 20, training loss: 1.9193289279937744 = 1.9107322692871094 + 0.001 * 8.596649169921875
Epoch 20, val loss: 1.9128080606460571
Epoch 30, training loss: 1.9048067331314087 = 1.8962104320526123 + 0.001 * 8.596282958984375
Epoch 30, val loss: 1.8981013298034668
Epoch 40, training loss: 1.8834091424942017 = 1.8748137950897217 + 0.001 * 8.5953369140625
Epoch 40, val loss: 1.8767820596694946
Epoch 50, training loss: 1.8531848192214966 = 1.8445922136306763 + 0.001 * 8.59261703491211
Epoch 50, val loss: 1.8478449583053589
Epoch 60, training loss: 1.817516565322876 = 1.8089330196380615 + 0.001 * 8.583529472351074
Epoch 60, val loss: 1.8161300420761108
Epoch 70, training loss: 1.7836941480636597 = 1.7751485109329224 + 0.001 * 8.545605659484863
Epoch 70, val loss: 1.7868468761444092
Epoch 80, training loss: 1.7420886754989624 = 1.7337948083877563 + 0.001 * 8.293822288513184
Epoch 80, val loss: 1.747512698173523
Epoch 90, training loss: 1.6839768886566162 = 1.6758675575256348 + 0.001 * 8.10927963256836
Epoch 90, val loss: 1.693981647491455
Epoch 100, training loss: 1.606368064880371 = 1.5983909368515015 + 0.001 * 7.977184772491455
Epoch 100, val loss: 1.6263526678085327
Epoch 110, training loss: 1.512534260749817 = 1.5047571659088135 + 0.001 * 7.777059555053711
Epoch 110, val loss: 1.5451655387878418
Epoch 120, training loss: 1.4087949991226196 = 1.4011884927749634 + 0.001 * 7.606496334075928
Epoch 120, val loss: 1.4571490287780762
Epoch 130, training loss: 1.3010892868041992 = 1.2935391664505005 + 0.001 * 7.55007266998291
Epoch 130, val loss: 1.3667570352554321
Epoch 140, training loss: 1.1921768188476562 = 1.1846647262573242 + 0.001 * 7.512055397033691
Epoch 140, val loss: 1.2777893543243408
Epoch 150, training loss: 1.0849868059158325 = 1.0774811506271362 + 0.001 * 7.505610466003418
Epoch 150, val loss: 1.1930073499679565
Epoch 160, training loss: 0.9830769300460815 = 0.9755769371986389 + 0.001 * 7.4999680519104
Epoch 160, val loss: 1.1144616603851318
Epoch 170, training loss: 0.889008641242981 = 0.8815141916275024 + 0.001 * 7.494474411010742
Epoch 170, val loss: 1.043717384338379
Epoch 180, training loss: 0.8042675852775574 = 0.796776294708252 + 0.001 * 7.491264820098877
Epoch 180, val loss: 0.981555163860321
Epoch 190, training loss: 0.7291889786720276 = 0.7217001914978027 + 0.001 * 7.488763809204102
Epoch 190, val loss: 0.9290866255760193
Epoch 200, training loss: 0.6626742482185364 = 0.6551876068115234 + 0.001 * 7.486647605895996
Epoch 200, val loss: 0.8861547112464905
Epoch 210, training loss: 0.6028060913085938 = 0.5953214168548584 + 0.001 * 7.484684944152832
Epoch 210, val loss: 0.851655125617981
Epoch 220, training loss: 0.5476634502410889 = 0.5401806235313416 + 0.001 * 7.482809066772461
Epoch 220, val loss: 0.8239098787307739
Epoch 230, training loss: 0.49599161744117737 = 0.48851072788238525 + 0.001 * 7.480899810791016
Epoch 230, val loss: 0.8011837005615234
Epoch 240, training loss: 0.4472768306732178 = 0.439797967672348 + 0.001 * 7.4788665771484375
Epoch 240, val loss: 0.78291916847229
Epoch 250, training loss: 0.40163710713386536 = 0.39416027069091797 + 0.001 * 7.476844310760498
Epoch 250, val loss: 0.7690609097480774
Epoch 260, training loss: 0.35941147804260254 = 0.35193657875061035 + 0.001 * 7.474903583526611
Epoch 260, val loss: 0.7598391771316528
Epoch 270, training loss: 0.32076749205589294 = 0.3132944107055664 + 0.001 * 7.473068714141846
Epoch 270, val loss: 0.755216658115387
Epoch 280, training loss: 0.28561273217201233 = 0.2781411409378052 + 0.001 * 7.471590042114258
Epoch 280, val loss: 0.7546021938323975
Epoch 290, training loss: 0.253675639629364 = 0.24620476365089417 + 0.001 * 7.470862865447998
Epoch 290, val loss: 0.7572970986366272
Epoch 300, training loss: 0.22469626367092133 = 0.2172269970178604 + 0.001 * 7.469263553619385
Epoch 300, val loss: 0.7626833319664001
Epoch 310, training loss: 0.1984938383102417 = 0.19102716445922852 + 0.001 * 7.466675758361816
Epoch 310, val loss: 0.7703946828842163
Epoch 320, training loss: 0.17497418820858002 = 0.16750913858413696 + 0.001 * 7.465052604675293
Epoch 320, val loss: 0.7799559235572815
Epoch 330, training loss: 0.15406270325183868 = 0.14660415053367615 + 0.001 * 7.458548545837402
Epoch 330, val loss: 0.7911116480827332
Epoch 340, training loss: 0.1356658786535263 = 0.12821349501609802 + 0.001 * 7.452386379241943
Epoch 340, val loss: 0.8036434650421143
Epoch 350, training loss: 0.11960799247026443 = 0.11216581612825394 + 0.001 * 7.4421772956848145
Epoch 350, val loss: 0.8173824548721313
Epoch 360, training loss: 0.10567495226860046 = 0.0982455462217331 + 0.001 * 7.42940616607666
Epoch 360, val loss: 0.832137942314148
Epoch 370, training loss: 0.09362993389368057 = 0.08621416240930557 + 0.001 * 7.415768146514893
Epoch 370, val loss: 0.847674548625946
Epoch 380, training loss: 0.08325599133968353 = 0.07583598047494888 + 0.001 * 7.420012474060059
Epoch 380, val loss: 0.8638016581535339
Epoch 390, training loss: 0.07430526614189148 = 0.06689661741256714 + 0.001 * 7.408650875091553
Epoch 390, val loss: 0.880385160446167
Epoch 400, training loss: 0.06658989191055298 = 0.059199947863817215 + 0.001 * 7.389942646026611
Epoch 400, val loss: 0.8972321152687073
Epoch 410, training loss: 0.05995113402605057 = 0.052570078521966934 + 0.001 * 7.381053924560547
Epoch 410, val loss: 0.914164662361145
Epoch 420, training loss: 0.0542345829308033 = 0.046852413564920425 + 0.001 * 7.382168769836426
Epoch 420, val loss: 0.9309804439544678
Epoch 430, training loss: 0.04927738383412361 = 0.041913773864507675 + 0.001 * 7.363609790802002
Epoch 430, val loss: 0.9476354718208313
Epoch 440, training loss: 0.0449962392449379 = 0.03763846680521965 + 0.001 * 7.3577728271484375
Epoch 440, val loss: 0.9640191197395325
Epoch 450, training loss: 0.04127471521496773 = 0.0339282751083374 + 0.001 * 7.346440315246582
Epoch 450, val loss: 0.9800455570220947
Epoch 460, training loss: 0.03808777779340744 = 0.030699681490659714 + 0.001 * 7.388094902038574
Epoch 460, val loss: 0.9956768155097961
Epoch 470, training loss: 0.03522052988409996 = 0.027881575748324394 + 0.001 * 7.338954925537109
Epoch 470, val loss: 1.0108672380447388
Epoch 480, training loss: 0.03274504840373993 = 0.02541353367269039 + 0.001 * 7.331516265869141
Epoch 480, val loss: 1.0255528688430786
Epoch 490, training loss: 0.03056579828262329 = 0.023244308307766914 + 0.001 * 7.321489334106445
Epoch 490, val loss: 1.0397591590881348
Epoch 500, training loss: 0.02865515649318695 = 0.021327901631593704 + 0.001 * 7.327253818511963
Epoch 500, val loss: 1.0535345077514648
Epoch 510, training loss: 0.026949046179652214 = 0.01962422952055931 + 0.001 * 7.32481575012207
Epoch 510, val loss: 1.0669277906417847
Epoch 520, training loss: 0.025420447811484337 = 0.018102314323186874 + 0.001 * 7.3181328773498535
Epoch 520, val loss: 1.080032467842102
Epoch 530, training loss: 0.024042434990406036 = 0.016738981008529663 + 0.001 * 7.30345344543457
Epoch 530, val loss: 1.0928775072097778
Epoch 540, training loss: 0.022926894947886467 = 0.015514962375164032 + 0.001 * 7.411932468414307
Epoch 540, val loss: 1.1053630113601685
Epoch 550, training loss: 0.021728532388806343 = 0.014414527453482151 + 0.001 * 7.314004421234131
Epoch 550, val loss: 1.1175768375396729
Epoch 560, training loss: 0.020713703706860542 = 0.013423129916191101 + 0.001 * 7.2905731201171875
Epoch 560, val loss: 1.1294987201690674
Epoch 570, training loss: 0.01982586830854416 = 0.012528331950306892 + 0.001 * 7.297534942626953
Epoch 570, val loss: 1.1410586833953857
Epoch 580, training loss: 0.01900479383766651 = 0.011719086207449436 + 0.001 * 7.285706996917725
Epoch 580, val loss: 1.1523284912109375
Epoch 590, training loss: 0.018270868808031082 = 0.010985546745359898 + 0.001 * 7.28532075881958
Epoch 590, val loss: 1.1633108854293823
Epoch 600, training loss: 0.017591217532753944 = 0.010319146327674389 + 0.001 * 7.272071361541748
Epoch 600, val loss: 1.1739751100540161
Epoch 610, training loss: 0.01699974201619625 = 0.009712479077279568 + 0.001 * 7.287261962890625
Epoch 610, val loss: 1.1843522787094116
Epoch 620, training loss: 0.01643497869372368 = 0.009159051813185215 + 0.001 * 7.2759270668029785
Epoch 620, val loss: 1.1944314241409302
Epoch 630, training loss: 0.01592942699790001 = 0.00865295808762312 + 0.001 * 7.276467800140381
Epoch 630, val loss: 1.2041876316070557
Epoch 640, training loss: 0.015468936413526535 = 0.008189208805561066 + 0.001 * 7.279727458953857
Epoch 640, val loss: 1.2136566638946533
Epoch 650, training loss: 0.015018502250313759 = 0.007763334549963474 + 0.001 * 7.255167007446289
Epoch 650, val loss: 1.22283935546875
Epoch 660, training loss: 0.014646421186625957 = 0.007371347863227129 + 0.001 * 7.275073051452637
Epoch 660, val loss: 1.2317548990249634
Epoch 670, training loss: 0.0142643041908741 = 0.007009759079664946 + 0.001 * 7.254545211791992
Epoch 670, val loss: 1.2404491901397705
Epoch 680, training loss: 0.013966243714094162 = 0.006675192154943943 + 0.001 * 7.291051387786865
Epoch 680, val loss: 1.2489122152328491
Epoch 690, training loss: 0.013626504689455032 = 0.006364775355905294 + 0.001 * 7.261728763580322
Epoch 690, val loss: 1.2571460008621216
Epoch 700, training loss: 0.013322199694812298 = 0.006075573619455099 + 0.001 * 7.246625900268555
Epoch 700, val loss: 1.2652041912078857
Epoch 710, training loss: 0.013045634143054485 = 0.005805172957479954 + 0.001 * 7.2404608726501465
Epoch 710, val loss: 1.2731255292892456
Epoch 720, training loss: 0.012807158753275871 = 0.005551923997700214 + 0.001 * 7.255234718322754
Epoch 720, val loss: 1.2808470726013184
Epoch 730, training loss: 0.012560253962874413 = 0.0053142886608839035 + 0.001 * 7.245965003967285
Epoch 730, val loss: 1.2884836196899414
Epoch 740, training loss: 0.01233750395476818 = 0.005090829450637102 + 0.001 * 7.246673583984375
Epoch 740, val loss: 1.295950174331665
Epoch 750, training loss: 0.01213672012090683 = 0.004880452062934637 + 0.001 * 7.25626802444458
Epoch 750, val loss: 1.3032945394515991
Epoch 760, training loss: 0.011929577216506004 = 0.004682484548538923 + 0.001 * 7.247091770172119
Epoch 760, val loss: 1.3104857206344604
Epoch 770, training loss: 0.011772784404456615 = 0.004495989065617323 + 0.001 * 7.276794910430908
Epoch 770, val loss: 1.3175886869430542
Epoch 780, training loss: 0.011558192782104015 = 0.0043206410482525826 + 0.001 * 7.237551212310791
Epoch 780, val loss: 1.3245240449905396
Epoch 790, training loss: 0.011389413848519325 = 0.00415562279522419 + 0.001 * 7.233790874481201
Epoch 790, val loss: 1.3313260078430176
Epoch 800, training loss: 0.011228622868657112 = 0.004000166431069374 + 0.001 * 7.228455543518066
Epoch 800, val loss: 1.337981104850769
Epoch 810, training loss: 0.011096921749413013 = 0.003853584174066782 + 0.001 * 7.243337154388428
Epoch 810, val loss: 1.344509243965149
Epoch 820, training loss: 0.010944359935820103 = 0.0037152471486479044 + 0.001 * 7.229112148284912
Epoch 820, val loss: 1.3508795499801636
Epoch 830, training loss: 0.010811885818839073 = 0.003584587015211582 + 0.001 * 7.227297782897949
Epoch 830, val loss: 1.3571758270263672
Epoch 840, training loss: 0.010710695758461952 = 0.0034610803704708815 + 0.001 * 7.249614715576172
Epoch 840, val loss: 1.3632832765579224
Epoch 850, training loss: 0.01054849661886692 = 0.003344220807775855 + 0.001 * 7.204275131225586
Epoch 850, val loss: 1.3693026304244995
Epoch 860, training loss: 0.010480059310793877 = 0.003233391558751464 + 0.001 * 7.246667861938477
Epoch 860, val loss: 1.3752233982086182
Epoch 870, training loss: 0.010334663093090057 = 0.003128206357359886 + 0.001 * 7.206455707550049
Epoch 870, val loss: 1.3810006380081177
Epoch 880, training loss: 0.010248704813420773 = 0.0030280097853392363 + 0.001 * 7.2206950187683105
Epoch 880, val loss: 1.3866989612579346
Epoch 890, training loss: 0.010136762633919716 = 0.002932708477601409 + 0.001 * 7.20405387878418
Epoch 890, val loss: 1.3923289775848389
Epoch 900, training loss: 0.010078133083879948 = 0.0028418356087058783 + 0.001 * 7.236297130584717
Epoch 900, val loss: 1.3978511095046997
Epoch 910, training loss: 0.009951751679182053 = 0.002755290362983942 + 0.001 * 7.196460723876953
Epoch 910, val loss: 1.4032641649246216
Epoch 920, training loss: 0.009887543506920338 = 0.002672736067324877 + 0.001 * 7.214807033538818
Epoch 920, val loss: 1.4085487127304077
Epoch 930, training loss: 0.00979672186076641 = 0.002593926154077053 + 0.001 * 7.202795028686523
Epoch 930, val loss: 1.4138177633285522
Epoch 940, training loss: 0.009694482199847698 = 0.002518570516258478 + 0.001 * 7.1759114265441895
Epoch 940, val loss: 1.4189354181289673
Epoch 950, training loss: 0.009626365266740322 = 0.0024464819580316544 + 0.001 * 7.179883003234863
Epoch 950, val loss: 1.4240045547485352
Epoch 960, training loss: 0.009566178545355797 = 0.0023774749133735895 + 0.001 * 7.1887030601501465
Epoch 960, val loss: 1.4289497137069702
Epoch 970, training loss: 0.009492749348282814 = 0.0023115063086152077 + 0.001 * 7.1812424659729
Epoch 970, val loss: 1.4338239431381226
Epoch 980, training loss: 0.009421762079000473 = 0.0022484215442091227 + 0.001 * 7.17333984375
Epoch 980, val loss: 1.4386340379714966
Epoch 990, training loss: 0.00935485027730465 = 0.0021881063003093004 + 0.001 * 7.166743755340576
Epoch 990, val loss: 1.4432847499847412
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 1.9509797096252441 = 1.9423828125 + 0.001 * 8.596840858459473
Epoch 0, val loss: 1.9365322589874268
Epoch 10, training loss: 1.941396713256836 = 1.9327999353408813 + 0.001 * 8.596800804138184
Epoch 10, val loss: 1.9266531467437744
Epoch 20, training loss: 1.9295916557312012 = 1.9209949970245361 + 0.001 * 8.596665382385254
Epoch 20, val loss: 1.914613127708435
Epoch 30, training loss: 1.9131156206130981 = 1.9045193195343018 + 0.001 * 8.596358299255371
Epoch 30, val loss: 1.8981666564941406
Epoch 40, training loss: 1.8890173435211182 = 1.8804216384887695 + 0.001 * 8.595645904541016
Epoch 40, val loss: 1.8745169639587402
Epoch 50, training loss: 1.8555898666381836 = 1.8469960689544678 + 0.001 * 8.593851089477539
Epoch 50, val loss: 1.8430099487304688
Epoch 60, training loss: 1.8182034492492676 = 1.809614896774292 + 0.001 * 8.588574409484863
Epoch 60, val loss: 1.8107908964157104
Epoch 70, training loss: 1.7867735624313354 = 1.7782038450241089 + 0.001 * 8.569665908813477
Epoch 70, val loss: 1.7858392000198364
Epoch 80, training loss: 1.750555396080017 = 1.7421034574508667 + 0.001 * 8.451973915100098
Epoch 80, val loss: 1.753921389579773
Epoch 90, training loss: 1.700379729270935 = 1.6921855211257935 + 0.001 * 8.194246292114258
Epoch 90, val loss: 1.7093957662582397
Epoch 100, training loss: 1.631134271621704 = 1.6230754852294922 + 0.001 * 8.05876636505127
Epoch 100, val loss: 1.6491789817810059
Epoch 110, training loss: 1.5426292419433594 = 1.5347561836242676 + 0.001 * 7.873066425323486
Epoch 110, val loss: 1.5741095542907715
Epoch 120, training loss: 1.4440110921859741 = 1.4363789558410645 + 0.001 * 7.632170677185059
Epoch 120, val loss: 1.492793321609497
Epoch 130, training loss: 1.3432540893554688 = 1.3356720209121704 + 0.001 * 7.582040309906006
Epoch 130, val loss: 1.4103306531906128
Epoch 140, training loss: 1.2421320676803589 = 1.234600305557251 + 0.001 * 7.531818389892578
Epoch 140, val loss: 1.329233169555664
Epoch 150, training loss: 1.1414909362792969 = 1.133985996246338 + 0.001 * 7.504919528961182
Epoch 150, val loss: 1.2497221231460571
Epoch 160, training loss: 1.0433399677276611 = 1.0358667373657227 + 0.001 * 7.473171710968018
Epoch 160, val loss: 1.1736913919448853
Epoch 170, training loss: 0.950165331363678 = 0.9427329897880554 + 0.001 * 7.4323530197143555
Epoch 170, val loss: 1.1032497882843018
Epoch 180, training loss: 0.8636347055435181 = 0.8562466502189636 + 0.001 * 7.388077259063721
Epoch 180, val loss: 1.0388089418411255
Epoch 190, training loss: 0.7845932841300964 = 0.7772420048713684 + 0.001 * 7.351259231567383
Epoch 190, val loss: 0.9806682467460632
Epoch 200, training loss: 0.713448166847229 = 0.7061107158660889 + 0.001 * 7.337430477142334
Epoch 200, val loss: 0.9294115304946899
Epoch 210, training loss: 0.6497554779052734 = 0.6424232721328735 + 0.001 * 7.332186698913574
Epoch 210, val loss: 0.8850433230400085
Epoch 220, training loss: 0.5921884179115295 = 0.5848591923713684 + 0.001 * 7.329202175140381
Epoch 220, val loss: 0.8473374247550964
Epoch 230, training loss: 0.5393130779266357 = 0.5319856405258179 + 0.001 * 7.327442646026611
Epoch 230, val loss: 0.8153668642044067
Epoch 240, training loss: 0.4898548722267151 = 0.48252883553504944 + 0.001 * 7.326051235198975
Epoch 240, val loss: 0.7878699898719788
Epoch 250, training loss: 0.44272738695144653 = 0.4354027211666107 + 0.001 * 7.324665069580078
Epoch 250, val loss: 0.764043927192688
Epoch 260, training loss: 0.3969351351261139 = 0.38961175084114075 + 0.001 * 7.323381423950195
Epoch 260, val loss: 0.7430359721183777
Epoch 270, training loss: 0.3519766926765442 = 0.3446543216705322 + 0.001 * 7.322364807128906
Epoch 270, val loss: 0.7242861986160278
Epoch 280, training loss: 0.30846667289733887 = 0.30114492774009705 + 0.001 * 7.321731090545654
Epoch 280, val loss: 0.7076552510261536
Epoch 290, training loss: 0.2678275406360626 = 0.26050591468811035 + 0.001 * 7.321617126464844
Epoch 290, val loss: 0.6936436891555786
Epoch 300, training loss: 0.23139913380146027 = 0.22407570481300354 + 0.001 * 7.323429584503174
Epoch 300, val loss: 0.6829652190208435
Epoch 310, training loss: 0.1998033970594406 = 0.19248045980930328 + 0.001 * 7.32293176651001
Epoch 310, val loss: 0.6761261820793152
Epoch 320, training loss: 0.17287388443946838 = 0.16555020213127136 + 0.001 * 7.323681354522705
Epoch 320, val loss: 0.6729925870895386
Epoch 330, training loss: 0.1500909924507141 = 0.14276616275310516 + 0.001 * 7.324825763702393
Epoch 330, val loss: 0.6731094121932983
Epoch 340, training loss: 0.13086198270320892 = 0.12353601306676865 + 0.001 * 7.325969696044922
Epoch 340, val loss: 0.675974428653717
Epoch 350, training loss: 0.114632748067379 = 0.10730390250682831 + 0.001 * 7.3288445472717285
Epoch 350, val loss: 0.6810774803161621
Epoch 360, training loss: 0.10090717673301697 = 0.0935777947306633 + 0.001 * 7.32938289642334
Epoch 360, val loss: 0.6879376173019409
Epoch 370, training loss: 0.08926358073949814 = 0.08193423599004745 + 0.001 * 7.329344272613525
Epoch 370, val loss: 0.6961666941642761
Epoch 380, training loss: 0.07935231178998947 = 0.0720219612121582 + 0.001 * 7.330347537994385
Epoch 380, val loss: 0.7053778767585754
Epoch 390, training loss: 0.07088840752840042 = 0.0635574534535408 + 0.001 * 7.330950736999512
Epoch 390, val loss: 0.7152907252311707
Epoch 400, training loss: 0.06364305317401886 = 0.05631152540445328 + 0.001 * 7.3315277099609375
Epoch 400, val loss: 0.7256863713264465
Epoch 410, training loss: 0.05743122845888138 = 0.05009540170431137 + 0.001 * 7.335824966430664
Epoch 410, val loss: 0.73637455701828
Epoch 420, training loss: 0.0520830973982811 = 0.04475056752562523 + 0.001 * 7.332531452178955
Epoch 420, val loss: 0.7471747994422913
Epoch 430, training loss: 0.04747713357210159 = 0.040144309401512146 + 0.001 * 7.332824230194092
Epoch 430, val loss: 0.7579783201217651
Epoch 440, training loss: 0.04349573701620102 = 0.03616296872496605 + 0.001 * 7.332768440246582
Epoch 440, val loss: 0.7686697840690613
Epoch 450, training loss: 0.04004320502281189 = 0.03271059691905975 + 0.001 * 7.332605838775635
Epoch 450, val loss: 0.7791844010353088
Epoch 460, training loss: 0.03703858703374863 = 0.029706288129091263 + 0.001 * 7.33229923248291
Epoch 460, val loss: 0.7894914746284485
Epoch 470, training loss: 0.03441333770751953 = 0.027082053944468498 + 0.001 * 7.331281661987305
Epoch 470, val loss: 0.7995814681053162
Epoch 480, training loss: 0.03211024031043053 = 0.02478068694472313 + 0.001 * 7.329554557800293
Epoch 480, val loss: 0.8094149231910706
Epoch 490, training loss: 0.03008371591567993 = 0.022754257544875145 + 0.001 * 7.329457759857178
Epoch 490, val loss: 0.8189892172813416
Epoch 500, training loss: 0.028292573988437653 = 0.020962901413440704 + 0.001 * 7.329671382904053
Epoch 500, val loss: 0.828308641910553
Epoch 510, training loss: 0.02669757604598999 = 0.019373148679733276 + 0.001 * 7.324427604675293
Epoch 510, val loss: 0.8373433947563171
Epoch 520, training loss: 0.025286024436354637 = 0.017957178875803947 + 0.001 * 7.3288445472717285
Epoch 520, val loss: 0.8461102247238159
Epoch 530, training loss: 0.024009941145777702 = 0.016691358759999275 + 0.001 * 7.318581581115723
Epoch 530, val loss: 0.8546141386032104
Epoch 540, training loss: 0.022875145077705383 = 0.015555919148027897 + 0.001 * 7.319226264953613
Epoch 540, val loss: 0.8628629446029663
Epoch 550, training loss: 0.021848497912287712 = 0.014533954672515392 + 0.001 * 7.314542293548584
Epoch 550, val loss: 0.8708686828613281
Epoch 560, training loss: 0.0209195613861084 = 0.013611123897135258 + 0.001 * 7.308436870574951
Epoch 560, val loss: 0.8786253333091736
Epoch 570, training loss: 0.020099949091672897 = 0.012775312177836895 + 0.001 * 7.324636936187744
Epoch 570, val loss: 0.8861523866653442
Epoch 580, training loss: 0.0193183571100235 = 0.01201613713055849 + 0.001 * 7.302219390869141
Epoch 580, val loss: 0.8934592008590698
Epoch 590, training loss: 0.018617480993270874 = 0.011324685998260975 + 0.001 * 7.292795658111572
Epoch 590, val loss: 0.9005588293075562
Epoch 600, training loss: 0.01798710972070694 = 0.010693269781768322 + 0.001 * 7.2938385009765625
Epoch 600, val loss: 0.9074265360832214
Epoch 610, training loss: 0.0173955000936985 = 0.010115273296833038 + 0.001 * 7.280227184295654
Epoch 610, val loss: 0.9141096472740173
Epoch 620, training loss: 0.016868840903043747 = 0.009584872983396053 + 0.001 * 7.2839674949646
Epoch 620, val loss: 0.9206001162528992
Epoch 630, training loss: 0.016395967453718185 = 0.009097074158489704 + 0.001 * 7.298892021179199
Epoch 630, val loss: 0.9269120693206787
Epoch 640, training loss: 0.015948019921779633 = 0.008647517301142216 + 0.001 * 7.300501823425293
Epoch 640, val loss: 0.9330537915229797
Epoch 650, training loss: 0.01548758614808321 = 0.008232316933572292 + 0.001 * 7.2552690505981445
Epoch 650, val loss: 0.9390010833740234
Epoch 660, training loss: 0.015117776580154896 = 0.007848129607737064 + 0.001 * 7.269646644592285
Epoch 660, val loss: 0.9447925090789795
Epoch 670, training loss: 0.014766465872526169 = 0.007491940166801214 + 0.001 * 7.274525165557861
Epoch 670, val loss: 0.9504275918006897
Epoch 680, training loss: 0.01440114714205265 = 0.007161086890846491 + 0.001 * 7.240060329437256
Epoch 680, val loss: 0.9559182524681091
Epoch 690, training loss: 0.014127524569630623 = 0.006853173952549696 + 0.001 * 7.274349689483643
Epoch 690, val loss: 0.9612604379653931
Epoch 700, training loss: 0.013812871649861336 = 0.006565994583070278 + 0.001 * 7.2468767166137695
Epoch 700, val loss: 0.9664575457572937
Epoch 710, training loss: 0.013541278429329395 = 0.006297410000115633 + 0.001 * 7.243867874145508
Epoch 710, val loss: 0.9715407490730286
Epoch 720, training loss: 0.013291046023368835 = 0.006045063026249409 + 0.001 * 7.245982646942139
Epoch 720, val loss: 0.9764927625656128
Epoch 730, training loss: 0.013073896989226341 = 0.005806657951325178 + 0.001 * 7.267239093780518
Epoch 730, val loss: 0.9813432693481445
Epoch 740, training loss: 0.012812494300305843 = 0.005580540746450424 + 0.001 * 7.231953144073486
Epoch 740, val loss: 0.9861069321632385
Epoch 750, training loss: 0.012622565031051636 = 0.005365706514567137 + 0.001 * 7.256858825683594
Epoch 750, val loss: 0.9908127188682556
Epoch 760, training loss: 0.012374404817819595 = 0.005161602050065994 + 0.001 * 7.212802410125732
Epoch 760, val loss: 0.9954425096511841
Epoch 770, training loss: 0.012178235687315464 = 0.004967670422047377 + 0.001 * 7.210565090179443
Epoch 770, val loss: 1.0000131130218506
Epoch 780, training loss: 0.012008795514702797 = 0.004783374257385731 + 0.001 * 7.22542142868042
Epoch 780, val loss: 1.0045576095581055
Epoch 790, training loss: 0.011822575703263283 = 0.00460792426019907 + 0.001 * 7.214651107788086
Epoch 790, val loss: 1.009037733078003
Epoch 800, training loss: 0.011657960712909698 = 0.0044404505752027035 + 0.001 * 7.217510223388672
Epoch 800, val loss: 1.0134919881820679
Epoch 810, training loss: 0.011479022912681103 = 0.004280238412320614 + 0.001 * 7.198784351348877
Epoch 810, val loss: 1.017920970916748
Epoch 820, training loss: 0.011321468278765678 = 0.004126483574509621 + 0.001 * 7.194983959197998
Epoch 820, val loss: 1.0223302841186523
Epoch 830, training loss: 0.01118597574532032 = 0.003979044500738382 + 0.001 * 7.2069315910339355
Epoch 830, val loss: 1.0267345905303955
Epoch 840, training loss: 0.011031745001673698 = 0.0038376206066459417 + 0.001 * 7.1941237449646
Epoch 840, val loss: 1.0310781002044678
Epoch 850, training loss: 0.010910483077168465 = 0.003702268237248063 + 0.001 * 7.20821475982666
Epoch 850, val loss: 1.0353832244873047
Epoch 860, training loss: 0.01075778342783451 = 0.0035727943759411573 + 0.001 * 7.184988975524902
Epoch 860, val loss: 1.0396339893341064
Epoch 870, training loss: 0.010641185566782951 = 0.0034488090313971043 + 0.001 * 7.192375659942627
Epoch 870, val loss: 1.0438510179519653
Epoch 880, training loss: 0.010554654523730278 = 0.0033302593510597944 + 0.001 * 7.224394798278809
Epoch 880, val loss: 1.0479964017868042
Epoch 890, training loss: 0.010414419695734978 = 0.0032171441707760096 + 0.001 * 7.197275638580322
Epoch 890, val loss: 1.0520923137664795
Epoch 900, training loss: 0.01028070505708456 = 0.0031090606935322285 + 0.001 * 7.17164421081543
Epoch 900, val loss: 1.056142807006836
Epoch 910, training loss: 0.010161670856177807 = 0.0030057986732572317 + 0.001 * 7.155871868133545
Epoch 910, val loss: 1.0601564645767212
Epoch 920, training loss: 0.010091731324791908 = 0.0029072382021695375 + 0.001 * 7.184493064880371
Epoch 920, val loss: 1.0640727281570435
Epoch 930, training loss: 0.010020171292126179 = 0.0028133096639066935 + 0.001 * 7.2068610191345215
Epoch 930, val loss: 1.0679852962493896
Epoch 940, training loss: 0.009897982701659203 = 0.0027237851172685623 + 0.001 * 7.174197196960449
Epoch 940, val loss: 1.071782112121582
Epoch 950, training loss: 0.009867275133728981 = 0.0026383325457572937 + 0.001 * 7.228942394256592
Epoch 950, val loss: 1.075529932975769
Epoch 960, training loss: 0.009732379578053951 = 0.002556869527325034 + 0.001 * 7.175509929656982
Epoch 960, val loss: 1.0792142152786255
Epoch 970, training loss: 0.009650442749261856 = 0.002479196060448885 + 0.001 * 7.17124605178833
Epoch 970, val loss: 1.0828279256820679
Epoch 980, training loss: 0.009572189301252365 = 0.0024051829241216183 + 0.001 * 7.167006492614746
Epoch 980, val loss: 1.0863646268844604
Epoch 990, training loss: 0.009489445947110653 = 0.0023345884401351213 + 0.001 * 7.154857158660889
Epoch 990, val loss: 1.0898665189743042
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8128624143384291
The final CL Acc:0.76667, 0.00800, The final GNN Acc:0.81392, 0.00522
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13248])
remove edge: torch.Size([2, 8056])
updated graph: torch.Size([2, 10748])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9490238428115845 = 1.9404269456863403 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.9419878721237183
Epoch 10, training loss: 1.9391230344772339 = 1.9305262565612793 + 0.001 * 8.596786499023438
Epoch 10, val loss: 1.9324250221252441
Epoch 20, training loss: 1.9269064664840698 = 1.9183099269866943 + 0.001 * 8.59659481048584
Epoch 20, val loss: 1.9202187061309814
Epoch 30, training loss: 1.9097694158554077 = 1.9011733531951904 + 0.001 * 8.596105575561523
Epoch 30, val loss: 1.902703881263733
Epoch 40, training loss: 1.8846051692962646 = 1.876010537147522 + 0.001 * 8.594680786132812
Epoch 40, val loss: 1.8769385814666748
Epoch 50, training loss: 1.8498594760894775 = 1.8412699699401855 + 0.001 * 8.589472770690918
Epoch 50, val loss: 1.8429111242294312
Epoch 60, training loss: 1.8117111921310425 = 1.8031458854675293 + 0.001 * 8.565258026123047
Epoch 60, val loss: 1.809144139289856
Epoch 70, training loss: 1.778388500213623 = 1.7699612379074097 + 0.001 * 8.427209854125977
Epoch 70, val loss: 1.7813488245010376
Epoch 80, training loss: 1.7349414825439453 = 1.7267225980758667 + 0.001 * 8.218901634216309
Epoch 80, val loss: 1.7423350811004639
Epoch 90, training loss: 1.6729977130889893 = 1.6649224758148193 + 0.001 * 8.075193405151367
Epoch 90, val loss: 1.6882456541061401
Epoch 100, training loss: 1.5894607305526733 = 1.5814881324768066 + 0.001 * 7.972650527954102
Epoch 100, val loss: 1.6173826456069946
Epoch 110, training loss: 1.4959758520126343 = 1.4881523847579956 + 0.001 * 7.8234124183654785
Epoch 110, val loss: 1.540421962738037
Epoch 120, training loss: 1.4045945405960083 = 1.3969062566757202 + 0.001 * 7.6882758140563965
Epoch 120, val loss: 1.4667949676513672
Epoch 130, training loss: 1.3174293041229248 = 1.3097374439239502 + 0.001 * 7.6919050216674805
Epoch 130, val loss: 1.3981449604034424
Epoch 140, training loss: 1.2311125993728638 = 1.2234470844268799 + 0.001 * 7.665542125701904
Epoch 140, val loss: 1.3314059972763062
Epoch 150, training loss: 1.1453137397766113 = 1.137666940689087 + 0.001 * 7.6467742919921875
Epoch 150, val loss: 1.2663965225219727
Epoch 160, training loss: 1.0614445209503174 = 1.0538256168365479 + 0.001 * 7.61892557144165
Epoch 160, val loss: 1.2038758993148804
Epoch 170, training loss: 0.9807519912719727 = 0.9731727242469788 + 0.001 * 7.5792555809021
Epoch 170, val loss: 1.1432982683181763
Epoch 180, training loss: 0.9037536382675171 = 0.8962234854698181 + 0.001 * 7.530137062072754
Epoch 180, val loss: 1.0842458009719849
Epoch 190, training loss: 0.8305395841598511 = 0.8230722546577454 + 0.001 * 7.467339992523193
Epoch 190, val loss: 1.0272046327590942
Epoch 200, training loss: 0.7622217535972595 = 0.754803478717804 + 0.001 * 7.41829252243042
Epoch 200, val loss: 0.9741997718811035
Epoch 210, training loss: 0.7002749443054199 = 0.6928942203521729 + 0.001 * 7.380748748779297
Epoch 210, val loss: 0.9280499219894409
Epoch 220, training loss: 0.6452831625938416 = 0.637926459312439 + 0.001 * 7.356701850891113
Epoch 220, val loss: 0.89069002866745
Epoch 230, training loss: 0.5968142747879028 = 0.589473307132721 + 0.001 * 7.340993404388428
Epoch 230, val loss: 0.8623825907707214
Epoch 240, training loss: 0.5541976690292358 = 0.546867847442627 + 0.001 * 7.329802513122559
Epoch 240, val loss: 0.8423362970352173
Epoch 250, training loss: 0.5169696807861328 = 0.5096428990364075 + 0.001 * 7.3267927169799805
Epoch 250, val loss: 0.8294091820716858
Epoch 260, training loss: 0.4844510555267334 = 0.4771266579627991 + 0.001 * 7.324384689331055
Epoch 260, val loss: 0.8223076462745667
Epoch 270, training loss: 0.4555334150791168 = 0.4482119083404541 + 0.001 * 7.321496486663818
Epoch 270, val loss: 0.819351077079773
Epoch 280, training loss: 0.428786963224411 = 0.4214682877063751 + 0.001 * 7.318687438964844
Epoch 280, val loss: 0.8187912106513977
Epoch 290, training loss: 0.4027591645717621 = 0.39544177055358887 + 0.001 * 7.317399978637695
Epoch 290, val loss: 0.8192193508148193
Epoch 300, training loss: 0.37632885575294495 = 0.36901620030403137 + 0.001 * 7.312648296356201
Epoch 300, val loss: 0.8198244571685791
Epoch 310, training loss: 0.3491347134113312 = 0.34182441234588623 + 0.001 * 7.3102898597717285
Epoch 310, val loss: 0.8204482197761536
Epoch 320, training loss: 0.32169225811958313 = 0.3143762946128845 + 0.001 * 7.315977096557617
Epoch 320, val loss: 0.8216316103935242
Epoch 330, training loss: 0.29474058747291565 = 0.28743165731430054 + 0.001 * 7.308933258056641
Epoch 330, val loss: 0.8237994909286499
Epoch 340, training loss: 0.26830345392227173 = 0.26100122928619385 + 0.001 * 7.302210807800293
Epoch 340, val loss: 0.8266072273254395
Epoch 350, training loss: 0.2414287030696869 = 0.23412863910198212 + 0.001 * 7.300060749053955
Epoch 350, val loss: 0.8293578028678894
Epoch 360, training loss: 0.2132890224456787 = 0.20598860085010529 + 0.001 * 7.300419807434082
Epoch 360, val loss: 0.8314669132232666
Epoch 370, training loss: 0.1846969574689865 = 0.1773933619260788 + 0.001 * 7.303599834442139
Epoch 370, val loss: 0.8336560726165771
Epoch 380, training loss: 0.15800879895687103 = 0.15070566534996033 + 0.001 * 7.303134441375732
Epoch 380, val loss: 0.8379311561584473
Epoch 390, training loss: 0.13519549369812012 = 0.1278969794511795 + 0.001 * 7.298506259918213
Epoch 390, val loss: 0.8458195328712463
Epoch 400, training loss: 0.11663319170475006 = 0.1093374639749527 + 0.001 * 7.295729637145996
Epoch 400, val loss: 0.8572784662246704
Epoch 410, training loss: 0.10173258930444717 = 0.09442123770713806 + 0.001 * 7.3113508224487305
Epoch 410, val loss: 0.8714932203292847
Epoch 420, training loss: 0.08962544053792953 = 0.08232264965772629 + 0.001 * 7.302787780761719
Epoch 420, val loss: 0.8874959349632263
Epoch 430, training loss: 0.07965463399887085 = 0.07235755026340485 + 0.001 * 7.297080993652344
Epoch 430, val loss: 0.9044553637504578
Epoch 440, training loss: 0.07132559269666672 = 0.06403084099292755 + 0.001 * 7.294753551483154
Epoch 440, val loss: 0.9218512177467346
Epoch 450, training loss: 0.06428699940443039 = 0.056994080543518066 + 0.001 * 7.292917251586914
Epoch 450, val loss: 0.9395239949226379
Epoch 460, training loss: 0.058295056223869324 = 0.05099116265773773 + 0.001 * 7.303891181945801
Epoch 460, val loss: 0.9571923017501831
Epoch 470, training loss: 0.053128667175769806 = 0.04583432897925377 + 0.001 * 7.294339179992676
Epoch 470, val loss: 0.9747136831283569
Epoch 480, training loss: 0.04867270216345787 = 0.041377000510692596 + 0.001 * 7.295700550079346
Epoch 480, val loss: 0.9919538497924805
Epoch 490, training loss: 0.04479987174272537 = 0.03750413656234741 + 0.001 * 7.295735836029053
Epoch 490, val loss: 1.0088229179382324
Epoch 500, training loss: 0.04141372814774513 = 0.03412182629108429 + 0.001 * 7.291901111602783
Epoch 500, val loss: 1.025303840637207
Epoch 510, training loss: 0.03844131901860237 = 0.031153613701462746 + 0.001 * 7.287704944610596
Epoch 510, val loss: 1.041343331336975
Epoch 520, training loss: 0.03583752363920212 = 0.028537752106785774 + 0.001 * 7.299772262573242
Epoch 520, val loss: 1.056917667388916
Epoch 530, training loss: 0.03353210166096687 = 0.026222962886095047 + 0.001 * 7.309138298034668
Epoch 530, val loss: 1.0720419883728027
Epoch 540, training loss: 0.03145251050591469 = 0.02416720800101757 + 0.001 * 7.285301208496094
Epoch 540, val loss: 1.0867902040481567
Epoch 550, training loss: 0.02960892766714096 = 0.022335361689329147 + 0.001 * 7.27356481552124
Epoch 550, val loss: 1.1010912656784058
Epoch 560, training loss: 0.02797466330230236 = 0.020697787404060364 + 0.001 * 7.276875019073486
Epoch 560, val loss: 1.114985704421997
Epoch 570, training loss: 0.026502056047320366 = 0.019229471683502197 + 0.001 * 7.272583484649658
Epoch 570, val loss: 1.1284778118133545
Epoch 580, training loss: 0.02518104948103428 = 0.017909131944179535 + 0.001 * 7.27191686630249
Epoch 580, val loss: 1.1415566205978394
Epoch 590, training loss: 0.02397734858095646 = 0.01671859249472618 + 0.001 * 7.258755207061768
Epoch 590, val loss: 1.1542303562164307
Epoch 600, training loss: 0.022891562432050705 = 0.015642134472727776 + 0.001 * 7.249427795410156
Epoch 600, val loss: 1.1665446758270264
Epoch 610, training loss: 0.021915752440690994 = 0.014666269533336163 + 0.001 * 7.249483585357666
Epoch 610, val loss: 1.1785049438476562
Epoch 620, training loss: 0.021057212725281715 = 0.013779404573142529 + 0.001 * 7.27780818939209
Epoch 620, val loss: 1.1900994777679443
Epoch 630, training loss: 0.020223811268806458 = 0.012971417978405952 + 0.001 * 7.25239372253418
Epoch 630, val loss: 1.2013556957244873
Epoch 640, training loss: 0.019467422738671303 = 0.01223358791321516 + 0.001 * 7.2338337898254395
Epoch 640, val loss: 1.2122790813446045
Epoch 650, training loss: 0.018805932253599167 = 0.011558208614587784 + 0.001 * 7.247723579406738
Epoch 650, val loss: 1.2228732109069824
Epoch 660, training loss: 0.018228260800242424 = 0.010938762687146664 + 0.001 * 7.289497375488281
Epoch 660, val loss: 1.2331480979919434
Epoch 670, training loss: 0.01760399341583252 = 0.010369308292865753 + 0.001 * 7.234684944152832
Epoch 670, val loss: 1.2431259155273438
Epoch 680, training loss: 0.017145249992609024 = 0.009844829328358173 + 0.001 * 7.30042028427124
Epoch 680, val loss: 1.2528027296066284
Epoch 690, training loss: 0.016615374013781548 = 0.009360738098621368 + 0.001 * 7.254635810852051
Epoch 690, val loss: 1.2622101306915283
Epoch 700, training loss: 0.016183558851480484 = 0.008913234807550907 + 0.001 * 7.270323753356934
Epoch 700, val loss: 1.2713572978973389
Epoch 710, training loss: 0.015732111409306526 = 0.00849878042936325 + 0.001 * 7.233330726623535
Epoch 710, val loss: 1.2802619934082031
Epoch 720, training loss: 0.015334827825427055 = 0.008114165626466274 + 0.001 * 7.220661640167236
Epoch 720, val loss: 1.2889043092727661
Epoch 730, training loss: 0.014977490529417992 = 0.007756671402603388 + 0.001 * 7.220818519592285
Epoch 730, val loss: 1.2973207235336304
Epoch 740, training loss: 0.014655344188213348 = 0.007423839997500181 + 0.001 * 7.231503486633301
Epoch 740, val loss: 1.3055063486099243
Epoch 750, training loss: 0.01433410495519638 = 0.0071135335601866245 + 0.001 * 7.220571041107178
Epoch 750, val loss: 1.3134849071502686
Epoch 760, training loss: 0.014046050608158112 = 0.006823675241321325 + 0.001 * 7.22237491607666
Epoch 760, val loss: 1.321233868598938
Epoch 770, training loss: 0.013780067674815655 = 0.006552649196237326 + 0.001 * 7.227417945861816
Epoch 770, val loss: 1.3288143873214722
Epoch 780, training loss: 0.01348317414522171 = 0.006298756692558527 + 0.001 * 7.184417247772217
Epoch 780, val loss: 1.3361746072769165
Epoch 790, training loss: 0.01327880285680294 = 0.006060643587261438 + 0.001 * 7.218159198760986
Epoch 790, val loss: 1.3433477878570557
Epoch 800, training loss: 0.01304528210312128 = 0.005837040487676859 + 0.001 * 7.2082414627075195
Epoch 800, val loss: 1.3503401279449463
Epoch 810, training loss: 0.012813873589038849 = 0.005626748315989971 + 0.001 * 7.187124252319336
Epoch 810, val loss: 1.3571572303771973
Epoch 820, training loss: 0.012623104266822338 = 0.005428777541965246 + 0.001 * 7.194326400756836
Epoch 820, val loss: 1.363815426826477
Epoch 830, training loss: 0.012438038364052773 = 0.005242141894996166 + 0.001 * 7.195896625518799
Epoch 830, val loss: 1.370291829109192
Epoch 840, training loss: 0.012238806113600731 = 0.005066043697297573 + 0.001 * 7.172761917114258
Epoch 840, val loss: 1.37661612033844
Epoch 850, training loss: 0.012074770405888557 = 0.00489967642351985 + 0.001 * 7.175093173980713
Epoch 850, val loss: 1.3828004598617554
Epoch 860, training loss: 0.011928425170481205 = 0.004742364399135113 + 0.001 * 7.186060428619385
Epoch 860, val loss: 1.388821005821228
Epoch 870, training loss: 0.011765791103243828 = 0.004593465477228165 + 0.001 * 7.172325134277344
Epoch 870, val loss: 1.3947080373764038
Epoch 880, training loss: 0.011662992648780346 = 0.00445238221436739 + 0.001 * 7.2106099128723145
Epoch 880, val loss: 1.4004719257354736
Epoch 890, training loss: 0.011496584862470627 = 0.004318558610975742 + 0.001 * 7.17802619934082
Epoch 890, val loss: 1.4060871601104736
Epoch 900, training loss: 0.011357996612787247 = 0.004191539250314236 + 0.001 * 7.166456699371338
Epoch 900, val loss: 1.4115924835205078
Epoch 910, training loss: 0.011252248659729958 = 0.004070838913321495 + 0.001 * 7.18140983581543
Epoch 910, val loss: 1.4169514179229736
Epoch 920, training loss: 0.011161349713802338 = 0.0039560808800160885 + 0.001 * 7.205268383026123
Epoch 920, val loss: 1.4221981763839722
Epoch 930, training loss: 0.011005817912518978 = 0.003846824634820223 + 0.001 * 7.158992767333984
Epoch 930, val loss: 1.4273122549057007
Epoch 940, training loss: 0.010948516428470612 = 0.0037427693605422974 + 0.001 * 7.205747127532959
Epoch 940, val loss: 1.4323347806930542
Epoch 950, training loss: 0.010805974714457989 = 0.003643549745902419 + 0.001 * 7.1624250411987305
Epoch 950, val loss: 1.4372378587722778
Epoch 960, training loss: 0.010698714293539524 = 0.003548945300281048 + 0.001 * 7.149768829345703
Epoch 960, val loss: 1.442033052444458
Epoch 970, training loss: 0.010605484247207642 = 0.0034585059620440006 + 0.001 * 7.146978378295898
Epoch 970, val loss: 1.4467283487319946
Epoch 980, training loss: 0.010540293529629707 = 0.0033720755018293858 + 0.001 * 7.16821813583374
Epoch 980, val loss: 1.4513194561004639
Epoch 990, training loss: 0.010453489609062672 = 0.0032894222531467676 + 0.001 * 7.164067268371582
Epoch 990, val loss: 1.455836534500122
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 1.9563543796539307 = 1.9477574825286865 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.9388571977615356
Epoch 10, training loss: 1.9464988708496094 = 1.9379020929336548 + 0.001 * 8.596816062927246
Epoch 10, val loss: 1.9290902614593506
Epoch 20, training loss: 1.9345831871032715 = 1.9259865283966064 + 0.001 * 8.596674919128418
Epoch 20, val loss: 1.9170749187469482
Epoch 30, training loss: 1.9180805683135986 = 1.9094842672348022 + 0.001 * 8.59635066986084
Epoch 30, val loss: 1.900172233581543
Epoch 40, training loss: 1.894003987312317 = 1.8854084014892578 + 0.001 * 8.595605850219727
Epoch 40, val loss: 1.8756210803985596
Epoch 50, training loss: 1.8599233627319336 = 1.8513298034667969 + 0.001 * 8.59359073638916
Epoch 50, val loss: 1.8421611785888672
Epoch 60, training loss: 1.8187627792358398 = 1.8101757764816284 + 0.001 * 8.586954116821289
Epoch 60, val loss: 1.8054451942443848
Epoch 70, training loss: 1.7806010246276855 = 1.7720401287078857 + 0.001 * 8.560864448547363
Epoch 70, val loss: 1.7762402296066284
Epoch 80, training loss: 1.739078402519226 = 1.7306692600250244 + 0.001 * 8.409097671508789
Epoch 80, val loss: 1.7418105602264404
Epoch 90, training loss: 1.6811548471450806 = 1.6730256080627441 + 0.001 * 8.129267692565918
Epoch 90, val loss: 1.6892168521881104
Epoch 100, training loss: 1.6024507284164429 = 1.5945155620574951 + 0.001 * 7.935153484344482
Epoch 100, val loss: 1.6180270910263062
Epoch 110, training loss: 1.5034719705581665 = 1.4957926273345947 + 0.001 * 7.679325103759766
Epoch 110, val loss: 1.531055212020874
Epoch 120, training loss: 1.3913315534591675 = 1.383834719657898 + 0.001 * 7.496862411499023
Epoch 120, val loss: 1.4364296197891235
Epoch 130, training loss: 1.274321436882019 = 1.266839623451233 + 0.001 * 7.48183012008667
Epoch 130, val loss: 1.3399651050567627
Epoch 140, training loss: 1.156782865524292 = 1.1493324041366577 + 0.001 * 7.450500965118408
Epoch 140, val loss: 1.244863748550415
Epoch 150, training loss: 1.042993426322937 = 1.0355536937713623 + 0.001 * 7.439709663391113
Epoch 150, val loss: 1.1531628370285034
Epoch 160, training loss: 0.9383102059364319 = 0.9308813810348511 + 0.001 * 7.428823947906494
Epoch 160, val loss: 1.0694583654403687
Epoch 170, training loss: 0.846882164478302 = 0.8394633531570435 + 0.001 * 7.418792247772217
Epoch 170, val loss: 0.9974240064620972
Epoch 180, training loss: 0.7699068784713745 = 0.7624982595443726 + 0.001 * 7.40863561630249
Epoch 180, val loss: 0.9383073449134827
Epoch 190, training loss: 0.7056052088737488 = 0.6982065439224243 + 0.001 * 7.3986639976501465
Epoch 190, val loss: 0.891349732875824
Epoch 200, training loss: 0.6503345966339111 = 0.6429450511932373 + 0.001 * 7.389535903930664
Epoch 200, val loss: 0.8531467914581299
Epoch 210, training loss: 0.6002699136734009 = 0.5928878784179688 + 0.001 * 7.382052898406982
Epoch 210, val loss: 0.8210654258728027
Epoch 220, training loss: 0.5527284145355225 = 0.545352578163147 + 0.001 * 7.375807762145996
Epoch 220, val loss: 0.7929500937461853
Epoch 230, training loss: 0.506525993347168 = 0.49915656447410583 + 0.001 * 7.369420528411865
Epoch 230, val loss: 0.7676151394844055
Epoch 240, training loss: 0.461689293384552 = 0.45432719588279724 + 0.001 * 7.362088203430176
Epoch 240, val loss: 0.7451759576797485
Epoch 250, training loss: 0.418867290019989 = 0.41151395440101624 + 0.001 * 7.3533501625061035
Epoch 250, val loss: 0.7260717153549194
Epoch 260, training loss: 0.37888041138648987 = 0.3715369999408722 + 0.001 * 7.343406677246094
Epoch 260, val loss: 0.7110162973403931
Epoch 270, training loss: 0.34246477484703064 = 0.33513739705085754 + 0.001 * 7.327369213104248
Epoch 270, val loss: 0.70046067237854
Epoch 280, training loss: 0.3099519610404968 = 0.30264633893966675 + 0.001 * 7.30561637878418
Epoch 280, val loss: 0.6946248412132263
Epoch 290, training loss: 0.28106358647346497 = 0.27377715706825256 + 0.001 * 7.286428451538086
Epoch 290, val loss: 0.6931735277175903
Epoch 300, training loss: 0.2549838721752167 = 0.24771203100681305 + 0.001 * 7.271848678588867
Epoch 300, val loss: 0.6953694224357605
Epoch 310, training loss: 0.2307581603527069 = 0.2234937846660614 + 0.001 * 7.264369010925293
Epoch 310, val loss: 0.7002623677253723
Epoch 320, training loss: 0.2077385038137436 = 0.2004760205745697 + 0.001 * 7.262477397918701
Epoch 320, val loss: 0.7070910334587097
Epoch 330, training loss: 0.18579743802547455 = 0.1785348355770111 + 0.001 * 7.2625956535339355
Epoch 330, val loss: 0.7154509425163269
Epoch 340, training loss: 0.1652751863002777 = 0.15801052749156952 + 0.001 * 7.264652729034424
Epoch 340, val loss: 0.7253727316856384
Epoch 350, training loss: 0.14663252234458923 = 0.13936643302440643 + 0.001 * 7.26608943939209
Epoch 350, val loss: 0.7368779182434082
Epoch 360, training loss: 0.13012222945690155 = 0.12285482883453369 + 0.001 * 7.267396450042725
Epoch 360, val loss: 0.7499164342880249
Epoch 370, training loss: 0.11571531742811203 = 0.10844675451517105 + 0.001 * 7.268561840057373
Epoch 370, val loss: 0.7641187310218811
Epoch 380, training loss: 0.10322141647338867 = 0.09595178812742233 + 0.001 * 7.269629001617432
Epoch 380, val loss: 0.779175341129303
Epoch 390, training loss: 0.09239622950553894 = 0.0851256400346756 + 0.001 * 7.270589828491211
Epoch 390, val loss: 0.7948407530784607
Epoch 400, training loss: 0.08299937099218369 = 0.07572793960571289 + 0.001 * 7.271428108215332
Epoch 400, val loss: 0.8108029961585999
Epoch 410, training loss: 0.07481958717107773 = 0.06754233688116074 + 0.001 * 7.277248382568359
Epoch 410, val loss: 0.8268914222717285
Epoch 420, training loss: 0.0676652118563652 = 0.06039169058203697 + 0.001 * 7.27352237701416
Epoch 420, val loss: 0.8430540561676025
Epoch 430, training loss: 0.06140151247382164 = 0.05412823706865311 + 0.001 * 7.273275852203369
Epoch 430, val loss: 0.8591095209121704
Epoch 440, training loss: 0.05590417608618736 = 0.048630449920892715 + 0.001 * 7.273725986480713
Epoch 440, val loss: 0.8749491572380066
Epoch 450, training loss: 0.051073718816041946 = 0.04379978030920029 + 0.001 * 7.273937702178955
Epoch 450, val loss: 0.8905378580093384
Epoch 460, training loss: 0.04682445526123047 = 0.03955145552754402 + 0.001 * 7.273000240325928
Epoch 460, val loss: 0.9058283567428589
Epoch 470, training loss: 0.0430842824280262 = 0.0358116440474987 + 0.001 * 7.272638320922852
Epoch 470, val loss: 0.920734167098999
Epoch 480, training loss: 0.03978586196899414 = 0.03251353278756142 + 0.001 * 7.2723307609558105
Epoch 480, val loss: 0.9352095127105713
Epoch 490, training loss: 0.036870233714580536 = 0.02959870547056198 + 0.001 * 7.271528720855713
Epoch 490, val loss: 0.9492945671081543
Epoch 500, training loss: 0.034285783767700195 = 0.02701639011502266 + 0.001 * 7.2693939208984375
Epoch 500, val loss: 0.9629459977149963
Epoch 510, training loss: 0.03198970854282379 = 0.024722665548324585 + 0.001 * 7.26704216003418
Epoch 510, val loss: 0.9761724472045898
Epoch 520, training loss: 0.02995184250175953 = 0.022680146619677544 + 0.001 * 7.271696090698242
Epoch 520, val loss: 0.9890011548995972
Epoch 530, training loss: 0.028111251071095467 = 0.020849263295531273 + 0.001 * 7.26198673248291
Epoch 530, val loss: 1.0015374422073364
Epoch 540, training loss: 0.026468681171536446 = 0.019202593713998795 + 0.001 * 7.266086578369141
Epoch 540, val loss: 1.0138626098632812
Epoch 550, training loss: 0.024986622855067253 = 0.01772579737007618 + 0.001 * 7.2608256340026855
Epoch 550, val loss: 1.0254286527633667
Epoch 560, training loss: 0.02365414798259735 = 0.016397906467318535 + 0.001 * 7.256241321563721
Epoch 560, val loss: 1.0367826223373413
Epoch 570, training loss: 0.02244758978486061 = 0.01520274393260479 + 0.001 * 7.244845867156982
Epoch 570, val loss: 1.0478578805923462
Epoch 580, training loss: 0.021379277110099792 = 0.014125645160675049 + 0.001 * 7.253632068634033
Epoch 580, val loss: 1.0586005449295044
Epoch 590, training loss: 0.020394988358020782 = 0.01315371785312891 + 0.001 * 7.241269111633301
Epoch 590, val loss: 1.0689548254013062
Epoch 600, training loss: 0.019506536424160004 = 0.012275039218366146 + 0.001 * 7.231497287750244
Epoch 600, val loss: 1.078984260559082
Epoch 610, training loss: 0.018748536705970764 = 0.011479183100163937 + 0.001 * 7.269352912902832
Epoch 610, val loss: 1.088734745979309
Epoch 620, training loss: 0.017977554351091385 = 0.010756933130323887 + 0.001 * 7.220620632171631
Epoch 620, val loss: 1.0981968641281128
Epoch 630, training loss: 0.017321143299341202 = 0.010099884122610092 + 0.001 * 7.221259593963623
Epoch 630, val loss: 1.1073799133300781
Epoch 640, training loss: 0.01672939956188202 = 0.00949983112514019 + 0.001 * 7.229568004608154
Epoch 640, val loss: 1.1163644790649414
Epoch 650, training loss: 0.01618078723549843 = 0.008949346840381622 + 0.001 * 7.231439590454102
Epoch 650, val loss: 1.1251176595687866
Epoch 660, training loss: 0.015642791986465454 = 0.008442533202469349 + 0.001 * 7.200259208679199
Epoch 660, val loss: 1.1337361335754395
Epoch 670, training loss: 0.01516276877373457 = 0.007974927313625813 + 0.001 * 7.187840938568115
Epoch 670, val loss: 1.1421411037445068
Epoch 680, training loss: 0.014722301624715328 = 0.00754284905269742 + 0.001 * 7.179452419281006
Epoch 680, val loss: 1.1503933668136597
Epoch 690, training loss: 0.014352494850754738 = 0.007143574301153421 + 0.001 * 7.208919525146484
Epoch 690, val loss: 1.1583830118179321
Epoch 700, training loss: 0.013961931690573692 = 0.00677406694740057 + 0.001 * 7.187864303588867
Epoch 700, val loss: 1.1661818027496338
Epoch 710, training loss: 0.013621397316455841 = 0.006431866437196732 + 0.001 * 7.189530849456787
Epoch 710, val loss: 1.1737868785858154
Epoch 720, training loss: 0.013274969533085823 = 0.006114658433943987 + 0.001 * 7.1603102684021
Epoch 720, val loss: 1.1811559200286865
Epoch 730, training loss: 0.012991286814212799 = 0.005820380989462137 + 0.001 * 7.170906066894531
Epoch 730, val loss: 1.1883111000061035
Epoch 740, training loss: 0.012755591422319412 = 0.005547142121940851 + 0.001 * 7.208448886871338
Epoch 740, val loss: 1.195222020149231
Epoch 750, training loss: 0.012457214295864105 = 0.005293321330100298 + 0.001 * 7.16389274597168
Epoch 750, val loss: 1.2019503116607666
Epoch 760, training loss: 0.012210831046104431 = 0.005057170055806637 + 0.001 * 7.153660774230957
Epoch 760, val loss: 1.2084789276123047
Epoch 770, training loss: 0.011995891109108925 = 0.0048370761796832085 + 0.001 * 7.158813953399658
Epoch 770, val loss: 1.2148133516311646
Epoch 780, training loss: 0.011771854013204575 = 0.004631758667528629 + 0.001 * 7.140094757080078
Epoch 780, val loss: 1.2209277153015137
Epoch 790, training loss: 0.011594008654356003 = 0.004440079443156719 + 0.001 * 7.153928279876709
Epoch 790, val loss: 1.2268812656402588
Epoch 800, training loss: 0.011418658308684826 = 0.0042608026415109634 + 0.001 * 7.15785551071167
Epoch 800, val loss: 1.2326600551605225
Epoch 810, training loss: 0.011235789395868778 = 0.004092974588274956 + 0.001 * 7.142814636230469
Epoch 810, val loss: 1.2382879257202148
Epoch 820, training loss: 0.011072980239987373 = 0.0039357757195830345 + 0.001 * 7.137204647064209
Epoch 820, val loss: 1.2437208890914917
Epoch 830, training loss: 0.010913005098700523 = 0.0037883142940700054 + 0.001 * 7.124691009521484
Epoch 830, val loss: 1.2490187883377075
Epoch 840, training loss: 0.010761601850390434 = 0.003649774705991149 + 0.001 * 7.1118268966674805
Epoch 840, val loss: 1.2541645765304565
Epoch 850, training loss: 0.01063413918018341 = 0.0035195150412619114 + 0.001 * 7.114623546600342
Epoch 850, val loss: 1.2591872215270996
Epoch 860, training loss: 0.010545848868787289 = 0.0033969655632972717 + 0.001 * 7.148882865905762
Epoch 860, val loss: 1.2641127109527588
Epoch 870, training loss: 0.010403322987258434 = 0.0032814990263432264 + 0.001 * 7.121823310852051
Epoch 870, val loss: 1.2688238620758057
Epoch 880, training loss: 0.010284426622092724 = 0.003172551980242133 + 0.001 * 7.111874103546143
Epoch 880, val loss: 1.2734559774398804
Epoch 890, training loss: 0.010230083018541336 = 0.003069627797231078 + 0.001 * 7.160455226898193
Epoch 890, val loss: 1.2779450416564941
Epoch 900, training loss: 0.010069645941257477 = 0.0029722994659096003 + 0.001 * 7.09734582901001
Epoch 900, val loss: 1.2823524475097656
Epoch 910, training loss: 0.009988503530621529 = 0.0028802375309169292 + 0.001 * 7.108265399932861
Epoch 910, val loss: 1.2866191864013672
Epoch 920, training loss: 0.009914858266711235 = 0.002793073421344161 + 0.001 * 7.121784687042236
Epoch 920, val loss: 1.2907822132110596
Epoch 930, training loss: 0.009802860207855701 = 0.0027104178443551064 + 0.001 * 7.092442035675049
Epoch 930, val loss: 1.2948342561721802
Epoch 940, training loss: 0.0097400126978755 = 0.002632016548886895 + 0.001 * 7.107995986938477
Epoch 940, val loss: 1.2987866401672363
Epoch 950, training loss: 0.00966087356209755 = 0.0025575377512723207 + 0.001 * 7.103335857391357
Epoch 950, val loss: 1.3026063442230225
Epoch 960, training loss: 0.009620512835681438 = 0.002486780984327197 + 0.001 * 7.133731365203857
Epoch 960, val loss: 1.30637788772583
Epoch 970, training loss: 0.009531045332551003 = 0.0024194777943193913 + 0.001 * 7.111567497253418
Epoch 970, val loss: 1.3100172281265259
Epoch 980, training loss: 0.009464399889111519 = 0.002355382777750492 + 0.001 * 7.1090168952941895
Epoch 980, val loss: 1.313569188117981
Epoch 990, training loss: 0.009389306418597698 = 0.0022943292278796434 + 0.001 * 7.094976902008057
Epoch 990, val loss: 1.317016839981079
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 1.933479905128479 = 1.9248830080032349 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.925740361213684
Epoch 10, training loss: 1.9239654541015625 = 1.915368676185608 + 0.001 * 8.596807479858398
Epoch 10, val loss: 1.9156103134155273
Epoch 20, training loss: 1.9121942520141602 = 1.9035975933074951 + 0.001 * 8.596656799316406
Epoch 20, val loss: 1.9030673503875732
Epoch 30, training loss: 1.8958768844604492 = 1.8872805833816528 + 0.001 * 8.596321105957031
Epoch 30, val loss: 1.8859180212020874
Epoch 40, training loss: 1.8723465204238892 = 1.86375093460083 + 0.001 * 8.59554672241211
Epoch 40, val loss: 1.8616389036178589
Epoch 50, training loss: 1.8402677774429321 = 1.831674337387085 + 0.001 * 8.593482971191406
Epoch 50, val loss: 1.8299283981323242
Epoch 60, training loss: 1.8033696413040161 = 1.794783115386963 + 0.001 * 8.586468696594238
Epoch 60, val loss: 1.7965115308761597
Epoch 70, training loss: 1.7655973434448242 = 1.7570418119430542 + 0.001 * 8.55555248260498
Epoch 70, val loss: 1.7641937732696533
Epoch 80, training loss: 1.7156128883361816 = 1.7072805166244507 + 0.001 * 8.33239459991455
Epoch 80, val loss: 1.7191630601882935
Epoch 90, training loss: 1.6458311080932617 = 1.6378039121627808 + 0.001 * 8.027222633361816
Epoch 90, val loss: 1.6571621894836426
Epoch 100, training loss: 1.554448127746582 = 1.5466594696044922 + 0.001 * 7.788618087768555
Epoch 100, val loss: 1.5797590017318726
Epoch 110, training loss: 1.4467307329177856 = 1.4390647411346436 + 0.001 * 7.66603946685791
Epoch 110, val loss: 1.4874651432037354
Epoch 120, training loss: 1.3274192810058594 = 1.3198232650756836 + 0.001 * 7.596004486083984
Epoch 120, val loss: 1.3854447603225708
Epoch 130, training loss: 1.200417160987854 = 1.1928948163986206 + 0.001 * 7.522325038909912
Epoch 130, val loss: 1.2779937982559204
Epoch 140, training loss: 1.0726993083953857 = 1.0652177333831787 + 0.001 * 7.48160982131958
Epoch 140, val loss: 1.1720293760299683
Epoch 150, training loss: 0.9534945487976074 = 0.9460262060165405 + 0.001 * 7.468341827392578
Epoch 150, val loss: 1.0758734941482544
Epoch 160, training loss: 0.8510485887527466 = 0.8435930609703064 + 0.001 * 7.455532550811768
Epoch 160, val loss: 0.996441662311554
Epoch 170, training loss: 0.7676119208335876 = 0.7601686120033264 + 0.001 * 7.443323135375977
Epoch 170, val loss: 0.9359303712844849
Epoch 180, training loss: 0.699629545211792 = 0.692207396030426 + 0.001 * 7.422176361083984
Epoch 180, val loss: 0.8912923336029053
Epoch 190, training loss: 0.6417208909988403 = 0.6343310475349426 + 0.001 * 7.389849662780762
Epoch 190, val loss: 0.8570570349693298
Epoch 200, training loss: 0.5895045399665833 = 0.582144021987915 + 0.001 * 7.360520362854004
Epoch 200, val loss: 0.8291537761688232
Epoch 210, training loss: 0.54059898853302 = 0.5332614779472351 + 0.001 * 7.337482452392578
Epoch 210, val loss: 0.8054251074790955
Epoch 220, training loss: 0.4942072629928589 = 0.48687559366226196 + 0.001 * 7.331663131713867
Epoch 220, val loss: 0.7853982448577881
Epoch 230, training loss: 0.45023009181022644 = 0.44290000200271606 + 0.001 * 7.330082416534424
Epoch 230, val loss: 0.7694898247718811
Epoch 240, training loss: 0.40863025188446045 = 0.4013006389141083 + 0.001 * 7.329619884490967
Epoch 240, val loss: 0.7577440738677979
Epoch 250, training loss: 0.3694148063659668 = 0.3620853126049042 + 0.001 * 7.3294878005981445
Epoch 250, val loss: 0.749865710735321
Epoch 260, training loss: 0.33270561695098877 = 0.32537636160850525 + 0.001 * 7.329251289367676
Epoch 260, val loss: 0.7451440691947937
Epoch 270, training loss: 0.2985870838165283 = 0.29125791788101196 + 0.001 * 7.329160690307617
Epoch 270, val loss: 0.7431055903434753
Epoch 280, training loss: 0.2670043706893921 = 0.25967517495155334 + 0.001 * 7.329183578491211
Epoch 280, val loss: 0.7431827187538147
Epoch 290, training loss: 0.2378765046596527 = 0.2305464893579483 + 0.001 * 7.330008506774902
Epoch 290, val loss: 0.7450282573699951
Epoch 300, training loss: 0.211223766207695 = 0.20389336347579956 + 0.001 * 7.330403804779053
Epoch 300, val loss: 0.7486202120780945
Epoch 310, training loss: 0.18715469539165497 = 0.17982488870620728 + 0.001 * 7.329801559448242
Epoch 310, val loss: 0.7541393637657166
Epoch 320, training loss: 0.16573050618171692 = 0.158400759100914 + 0.001 * 7.3297438621521
Epoch 320, val loss: 0.7615676522254944
Epoch 330, training loss: 0.146881103515625 = 0.13955187797546387 + 0.001 * 7.329232692718506
Epoch 330, val loss: 0.7708770632743835
Epoch 340, training loss: 0.13042953610420227 = 0.12309852987527847 + 0.001 * 7.3310089111328125
Epoch 340, val loss: 0.7818623185157776
Epoch 350, training loss: 0.11611855030059814 = 0.10878825932741165 + 0.001 * 7.330292224884033
Epoch 350, val loss: 0.7943412661552429
Epoch 360, training loss: 0.10367567092180252 = 0.09634751826524734 + 0.001 * 7.328155040740967
Epoch 360, val loss: 0.8079677224159241
Epoch 370, training loss: 0.09284701943397522 = 0.08552040159702301 + 0.001 * 7.326618671417236
Epoch 370, val loss: 0.8224000930786133
Epoch 380, training loss: 0.0834033414721489 = 0.07607876509428024 + 0.001 * 7.3245768547058105
Epoch 380, val loss: 0.8374105095863342
Epoch 390, training loss: 0.07515652477741241 = 0.06782853603363037 + 0.001 * 7.327989101409912
Epoch 390, val loss: 0.8527377247810364
Epoch 400, training loss: 0.06793195009231567 = 0.06061017885804176 + 0.001 * 7.321773529052734
Epoch 400, val loss: 0.8682260513305664
Epoch 410, training loss: 0.06160619482398033 = 0.05428723618388176 + 0.001 * 7.318957805633545
Epoch 410, val loss: 0.8836170434951782
Epoch 420, training loss: 0.05606088414788246 = 0.0487440824508667 + 0.001 * 7.316802978515625
Epoch 420, val loss: 0.8988598585128784
Epoch 430, training loss: 0.05119149759411812 = 0.043880630284547806 + 0.001 * 7.310866832733154
Epoch 430, val loss: 0.9138392210006714
Epoch 440, training loss: 0.046922750771045685 = 0.039610687643289566 + 0.001 * 7.312063694000244
Epoch 440, val loss: 0.9285292625427246
Epoch 450, training loss: 0.04316740483045578 = 0.035857316106557846 + 0.001 * 7.310086727142334
Epoch 450, val loss: 0.9427968263626099
Epoch 460, training loss: 0.039857205003499985 = 0.03255396708846092 + 0.001 * 7.30323600769043
Epoch 460, val loss: 0.9566310048103333
Epoch 470, training loss: 0.03695306181907654 = 0.029642285779118538 + 0.001 * 7.310775279998779
Epoch 470, val loss: 0.9700169563293457
Epoch 480, training loss: 0.03438706696033478 = 0.027070822194218636 + 0.001 * 7.3162431716918945
Epoch 480, val loss: 0.9829488396644592
Epoch 490, training loss: 0.032079219818115234 = 0.0247954074293375 + 0.001 * 7.283812999725342
Epoch 490, val loss: 0.9954178333282471
Epoch 500, training loss: 0.030068524181842804 = 0.022777272388339043 + 0.001 * 7.291250705718994
Epoch 500, val loss: 1.0074559450149536
Epoch 510, training loss: 0.02827332355082035 = 0.020982729271054268 + 0.001 * 7.290594100952148
Epoch 510, val loss: 1.019094467163086
Epoch 520, training loss: 0.02666148543357849 = 0.01938280463218689 + 0.001 * 7.278680801391602
Epoch 520, val loss: 1.0303394794464111
Epoch 530, training loss: 0.025231434032320976 = 0.01795280911028385 + 0.001 * 7.278624534606934
Epoch 530, val loss: 1.0411882400512695
Epoch 540, training loss: 0.0239311084151268 = 0.01667143404483795 + 0.001 * 7.259674549102783
Epoch 540, val loss: 1.051663875579834
Epoch 550, training loss: 0.02280941791832447 = 0.015520263463258743 + 0.001 * 7.289154529571533
Epoch 550, val loss: 1.061768889427185
Epoch 560, training loss: 0.021723641082644463 = 0.014483286067843437 + 0.001 * 7.240354537963867
Epoch 560, val loss: 1.0715274810791016
Epoch 570, training loss: 0.020819077268242836 = 0.013546819798648357 + 0.001 * 7.272256851196289
Epoch 570, val loss: 1.0809696912765503
Epoch 580, training loss: 0.019965609535574913 = 0.012698912993073463 + 0.001 * 7.266696453094482
Epoch 580, val loss: 1.090111255645752
Epoch 590, training loss: 0.019162509590387344 = 0.01192915253341198 + 0.001 * 7.233356475830078
Epoch 590, val loss: 1.098940372467041
Epoch 600, training loss: 0.018472086638212204 = 0.011228656396269798 + 0.001 * 7.243430137634277
Epoch 600, val loss: 1.1074910163879395
Epoch 610, training loss: 0.01787199266254902 = 0.010589645244181156 + 0.001 * 7.282346725463867
Epoch 610, val loss: 1.1157736778259277
Epoch 620, training loss: 0.017223412171006203 = 0.01000545546412468 + 0.001 * 7.21795654296875
Epoch 620, val loss: 1.1237695217132568
Epoch 630, training loss: 0.016678359359502792 = 0.009470146149396896 + 0.001 * 7.208213806152344
Epoch 630, val loss: 1.131545901298523
Epoch 640, training loss: 0.016219664365053177 = 0.008978522382676601 + 0.001 * 7.241141319274902
Epoch 640, val loss: 1.1390881538391113
Epoch 650, training loss: 0.01570819690823555 = 0.008525906130671501 + 0.001 * 7.182291030883789
Epoch 650, val loss: 1.1463935375213623
Epoch 660, training loss: 0.01531011052429676 = 0.008108075708150864 + 0.001 * 7.202034950256348
Epoch 660, val loss: 1.1535109281539917
Epoch 670, training loss: 0.014926903881132603 = 0.007721337955445051 + 0.001 * 7.205565452575684
Epoch 670, val loss: 1.160433053970337
Epoch 680, training loss: 0.01453641802072525 = 0.007362642325460911 + 0.001 * 7.1737751960754395
Epoch 680, val loss: 1.1671515703201294
Epoch 690, training loss: 0.014208236709237099 = 0.0070289913564920425 + 0.001 * 7.179244518280029
Epoch 690, val loss: 1.1736695766448975
Epoch 700, training loss: 0.013897299766540527 = 0.006717943586409092 + 0.001 * 7.179356098175049
Epoch 700, val loss: 1.1800227165222168
Epoch 710, training loss: 0.013608384877443314 = 0.006427406799048185 + 0.001 * 7.180978298187256
Epoch 710, val loss: 1.1861993074417114
Epoch 720, training loss: 0.013338176533579826 = 0.006155673880130053 + 0.001 * 7.182502269744873
Epoch 720, val loss: 1.1922599077224731
Epoch 730, training loss: 0.013082567602396011 = 0.005901226308196783 + 0.001 * 7.181341648101807
Epoch 730, val loss: 1.1981267929077148
Epoch 740, training loss: 0.012873901054263115 = 0.0056623355485498905 + 0.001 * 7.211565017700195
Epoch 740, val loss: 1.2038753032684326
Epoch 750, training loss: 0.012604565359652042 = 0.005438022781163454 + 0.001 * 7.166542053222656
Epoch 750, val loss: 1.2094616889953613
Epoch 760, training loss: 0.012385440990328789 = 0.00522737018764019 + 0.001 * 7.158070087432861
Epoch 760, val loss: 1.2149220705032349
Epoch 770, training loss: 0.012186243198812008 = 0.005029145162552595 + 0.001 * 7.157097816467285
Epoch 770, val loss: 1.2202292680740356
Epoch 780, training loss: 0.012005234137177467 = 0.004842519294470549 + 0.001 * 7.162714958190918
Epoch 780, val loss: 1.2254310846328735
Epoch 790, training loss: 0.011817678809165955 = 0.004666738212108612 + 0.001 * 7.15093994140625
Epoch 790, val loss: 1.2305060625076294
Epoch 800, training loss: 0.011641282588243484 = 0.004501007031649351 + 0.001 * 7.140274524688721
Epoch 800, val loss: 1.2354568243026733
Epoch 810, training loss: 0.011501334607601166 = 0.004344572313129902 + 0.001 * 7.15676212310791
Epoch 810, val loss: 1.2402911186218262
Epoch 820, training loss: 0.011365486308932304 = 0.004196896683424711 + 0.001 * 7.168588638305664
Epoch 820, val loss: 1.245032548904419
Epoch 830, training loss: 0.011210137978196144 = 0.004057326819747686 + 0.001 * 7.152811050415039
Epoch 830, val loss: 1.249665379524231
Epoch 840, training loss: 0.011089429259300232 = 0.003925291821360588 + 0.001 * 7.1641364097595215
Epoch 840, val loss: 1.2541929483413696
Epoch 850, training loss: 0.010935203172266483 = 0.0038004887755960226 + 0.001 * 7.134713649749756
Epoch 850, val loss: 1.2585856914520264
Epoch 860, training loss: 0.010813483968377113 = 0.0036824136041104794 + 0.001 * 7.131069660186768
Epoch 860, val loss: 1.2629151344299316
Epoch 870, training loss: 0.010728932917118073 = 0.003570467932149768 + 0.001 * 7.1584649085998535
Epoch 870, val loss: 1.2671160697937012
Epoch 880, training loss: 0.010606441646814346 = 0.0034642464015632868 + 0.001 * 7.142195224761963
Epoch 880, val loss: 1.2712624073028564
Epoch 890, training loss: 0.010475965216755867 = 0.0033635180443525314 + 0.001 * 7.112447261810303
Epoch 890, val loss: 1.2752794027328491
Epoch 900, training loss: 0.010373779572546482 = 0.003267876338213682 + 0.001 * 7.105902671813965
Epoch 900, val loss: 1.2792298793792725
Epoch 910, training loss: 0.010306067764759064 = 0.003177057486027479 + 0.001 * 7.129009246826172
Epoch 910, val loss: 1.2830849885940552
Epoch 920, training loss: 0.01020830124616623 = 0.0030906135216355324 + 0.001 * 7.117687702178955
Epoch 920, val loss: 1.286840558052063
Epoch 930, training loss: 0.010159749537706375 = 0.0030082170851528645 + 0.001 * 7.151532173156738
Epoch 930, val loss: 1.2905502319335938
Epoch 940, training loss: 0.010051795281469822 = 0.0029296947177499533 + 0.001 * 7.122100353240967
Epoch 940, val loss: 1.2941248416900635
Epoch 950, training loss: 0.009967837482690811 = 0.002854851773008704 + 0.001 * 7.112985610961914
Epoch 950, val loss: 1.2976981401443481
Epoch 960, training loss: 0.009883925318717957 = 0.002783461008220911 + 0.001 * 7.1004638671875
Epoch 960, val loss: 1.3011560440063477
Epoch 970, training loss: 0.009791692718863487 = 0.0027153075207024813 + 0.001 * 7.076385021209717
Epoch 970, val loss: 1.3045377731323242
Epoch 980, training loss: 0.009737256914377213 = 0.0026501608081161976 + 0.001 * 7.087095737457275
Epoch 980, val loss: 1.3078699111938477
Epoch 990, training loss: 0.009716200642287731 = 0.0025878946762531996 + 0.001 * 7.128305912017822
Epoch 990, val loss: 1.3111083507537842
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8397469688982605
The final CL Acc:0.78148, 0.02181, The final GNN Acc:0.83658, 0.00240
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11654])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10568])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9738316535949707 = 1.9652347564697266 + 0.001 * 8.596837997436523
Epoch 0, val loss: 1.9646185636520386
Epoch 10, training loss: 1.9624799489974976 = 1.953883171081543 + 0.001 * 8.596778869628906
Epoch 10, val loss: 1.9535129070281982
Epoch 20, training loss: 1.9480408430099487 = 1.9394443035125732 + 0.001 * 8.596586227416992
Epoch 20, val loss: 1.93850576877594
Epoch 30, training loss: 1.9272537231445312 = 1.918657660484314 + 0.001 * 8.596071243286133
Epoch 30, val loss: 1.9161503314971924
Epoch 40, training loss: 1.8964613676071167 = 1.887866735458374 + 0.001 * 8.594605445861816
Epoch 40, val loss: 1.8830029964447021
Epoch 50, training loss: 1.855287790298462 = 1.8466978073120117 + 0.001 * 8.589930534362793
Epoch 50, val loss: 1.8408677577972412
Epoch 60, training loss: 1.815883755683899 = 1.8073108196258545 + 0.001 * 8.572994232177734
Epoch 60, val loss: 1.8059511184692383
Epoch 70, training loss: 1.7889400720596313 = 1.7804375886917114 + 0.001 * 8.502470970153809
Epoch 70, val loss: 1.7864652872085571
Epoch 80, training loss: 1.75499427318573 = 1.7467135190963745 + 0.001 * 8.280730247497559
Epoch 80, val loss: 1.7602158784866333
Epoch 90, training loss: 1.7084742784500122 = 1.700294852256775 + 0.001 * 8.17947006225586
Epoch 90, val loss: 1.7218132019042969
Epoch 100, training loss: 1.641916275024414 = 1.6338298320770264 + 0.001 * 8.086447715759277
Epoch 100, val loss: 1.6655186414718628
Epoch 110, training loss: 1.5577203035354614 = 1.5497026443481445 + 0.001 * 8.017629623413086
Epoch 110, val loss: 1.5955337285995483
Epoch 120, training loss: 1.4688347578048706 = 1.4609160423278809 + 0.001 * 7.918768405914307
Epoch 120, val loss: 1.5240973234176636
Epoch 130, training loss: 1.3839613199234009 = 1.3762469291687012 + 0.001 * 7.714375019073486
Epoch 130, val loss: 1.4592417478561401
Epoch 140, training loss: 1.301743745803833 = 1.2940647602081299 + 0.001 * 7.678987503051758
Epoch 140, val loss: 1.3990204334259033
Epoch 150, training loss: 1.2191600799560547 = 1.2115285396575928 + 0.001 * 7.631580352783203
Epoch 150, val loss: 1.339188575744629
Epoch 160, training loss: 1.1360816955566406 = 1.1284877061843872 + 0.001 * 7.594015121459961
Epoch 160, val loss: 1.2794275283813477
Epoch 170, training loss: 1.0546565055847168 = 1.0470972061157227 + 0.001 * 7.5593061447143555
Epoch 170, val loss: 1.2209371328353882
Epoch 180, training loss: 0.9764938950538635 = 0.9689819812774658 + 0.001 * 7.511939525604248
Epoch 180, val loss: 1.165282130241394
Epoch 190, training loss: 0.9013938903808594 = 0.8939664959907532 + 0.001 * 7.427419662475586
Epoch 190, val loss: 1.111341953277588
Epoch 200, training loss: 0.8285739421844482 = 0.8212260603904724 + 0.001 * 7.347911357879639
Epoch 200, val loss: 1.059022307395935
Epoch 210, training loss: 0.7579759359359741 = 0.7506696581840515 + 0.001 * 7.3063063621521
Epoch 210, val loss: 1.0092623233795166
Epoch 220, training loss: 0.6908683180809021 = 0.6835781335830688 + 0.001 * 7.290189743041992
Epoch 220, val loss: 0.9641972184181213
Epoch 230, training loss: 0.6286600232124329 = 0.6213778853416443 + 0.001 * 7.28212308883667
Epoch 230, val loss: 0.926062285900116
Epoch 240, training loss: 0.5719720125198364 = 0.5646921396255493 + 0.001 * 7.279899597167969
Epoch 240, val loss: 0.8958693742752075
Epoch 250, training loss: 0.5206582546234131 = 0.5133793354034424 + 0.001 * 7.2789435386657715
Epoch 250, val loss: 0.8738566637039185
Epoch 260, training loss: 0.47419092059135437 = 0.466913104057312 + 0.001 * 7.277825355529785
Epoch 260, val loss: 0.8593014478683472
Epoch 270, training loss: 0.43204736709594727 = 0.42477133870124817 + 0.001 * 7.276035308837891
Epoch 270, val loss: 0.8510359525680542
Epoch 280, training loss: 0.39381298422813416 = 0.3865385055541992 + 0.001 * 7.274480819702148
Epoch 280, val loss: 0.847828209400177
Epoch 290, training loss: 0.35910889506340027 = 0.35183480381965637 + 0.001 * 7.274102210998535
Epoch 290, val loss: 0.8485386371612549
Epoch 300, training loss: 0.32740041613578796 = 0.32012736797332764 + 0.001 * 7.273037910461426
Epoch 300, val loss: 0.8520815968513489
Epoch 310, training loss: 0.29805508255958557 = 0.2907818853855133 + 0.001 * 7.273191928863525
Epoch 310, val loss: 0.8575093150138855
Epoch 320, training loss: 0.2703019380569458 = 0.26302921772003174 + 0.001 * 7.272716999053955
Epoch 320, val loss: 0.8638349771499634
Epoch 330, training loss: 0.243434339761734 = 0.23615984618663788 + 0.001 * 7.274500370025635
Epoch 330, val loss: 0.8706420660018921
Epoch 340, training loss: 0.21702709794044495 = 0.20975446701049805 + 0.001 * 7.272623062133789
Epoch 340, val loss: 0.8778136968612671
Epoch 350, training loss: 0.19123372435569763 = 0.18396106362342834 + 0.001 * 7.272655487060547
Epoch 350, val loss: 0.8852717876434326
Epoch 360, training loss: 0.1667734682559967 = 0.15949873626232147 + 0.001 * 7.2747392654418945
Epoch 360, val loss: 0.8938531875610352
Epoch 370, training loss: 0.14454610645771027 = 0.1372719705104828 + 0.001 * 7.27413272857666
Epoch 370, val loss: 0.9041266441345215
Epoch 380, training loss: 0.1251535415649414 = 0.11787979304790497 + 0.001 * 7.2737555503845215
Epoch 380, val loss: 0.9163446426391602
Epoch 390, training loss: 0.10865836590528488 = 0.10138527303934097 + 0.001 * 7.273090362548828
Epoch 390, val loss: 0.9303796887397766
Epoch 400, training loss: 0.09478530287742615 = 0.08750994503498077 + 0.001 * 7.275354385375977
Epoch 400, val loss: 0.9458402395248413
Epoch 410, training loss: 0.08315516263246536 = 0.07588139921426773 + 0.001 * 7.273764610290527
Epoch 410, val loss: 0.9623145461082458
Epoch 420, training loss: 0.07341507077217102 = 0.06614373624324799 + 0.001 * 7.271337032318115
Epoch 420, val loss: 0.9793750643730164
Epoch 430, training loss: 0.06525769829750061 = 0.057987045496702194 + 0.001 * 7.270655632019043
Epoch 430, val loss: 0.9965984225273132
Epoch 440, training loss: 0.058418504893779755 = 0.05114683508872986 + 0.001 * 7.271668910980225
Epoch 440, val loss: 1.0137380361557007
Epoch 450, training loss: 0.052664801478385925 = 0.04539579525589943 + 0.001 * 7.269006252288818
Epoch 450, val loss: 1.0305923223495483
Epoch 460, training loss: 0.047808676958084106 = 0.040543023496866226 + 0.001 * 7.265653610229492
Epoch 460, val loss: 1.0469820499420166
Epoch 470, training loss: 0.043692294508218765 = 0.036429263651371 + 0.001 * 7.263031482696533
Epoch 470, val loss: 1.0629007816314697
Epoch 480, training loss: 0.04020512104034424 = 0.03292444720864296 + 0.001 * 7.280674457550049
Epoch 480, val loss: 1.0782461166381836
Epoch 490, training loss: 0.037183526903390884 = 0.02992301806807518 + 0.001 * 7.260509967803955
Epoch 490, val loss: 1.0929877758026123
Epoch 500, training loss: 0.03459765017032623 = 0.027338190004229546 + 0.001 * 7.259459495544434
Epoch 500, val loss: 1.107176423072815
Epoch 510, training loss: 0.03235069289803505 = 0.025100141763687134 + 0.001 * 7.250552177429199
Epoch 510, val loss: 1.1207579374313354
Epoch 520, training loss: 0.03040170669555664 = 0.02315201424062252 + 0.001 * 7.249691009521484
Epoch 520, val loss: 1.1337740421295166
Epoch 530, training loss: 0.02868473343551159 = 0.021447764709591866 + 0.001 * 7.236968994140625
Epoch 530, val loss: 1.146278977394104
Epoch 540, training loss: 0.02724498324096203 = 0.019949477165937424 + 0.001 * 7.295506000518799
Epoch 540, val loss: 1.1582834720611572
Epoch 550, training loss: 0.02587113343179226 = 0.018626149743795395 + 0.001 * 7.244982719421387
Epoch 550, val loss: 1.1698534488677979
Epoch 560, training loss: 0.024683210998773575 = 0.017452223226428032 + 0.001 * 7.230986595153809
Epoch 560, val loss: 1.1809427738189697
Epoch 570, training loss: 0.02361997589468956 = 0.016406305134296417 + 0.001 * 7.21367073059082
Epoch 570, val loss: 1.1916100978851318
Epoch 580, training loss: 0.022707002237439156 = 0.01547055784612894 + 0.001 * 7.236443519592285
Epoch 580, val loss: 1.2018816471099854
Epoch 590, training loss: 0.021868402138352394 = 0.014630189165472984 + 0.001 * 7.238212585449219
Epoch 590, val loss: 1.211773157119751
Epoch 600, training loss: 0.021084409207105637 = 0.013872871175408363 + 0.001 * 7.211538314819336
Epoch 600, val loss: 1.2213001251220703
Epoch 610, training loss: 0.020396532490849495 = 0.013188091106712818 + 0.001 * 7.208441734313965
Epoch 610, val loss: 1.2304927110671997
Epoch 620, training loss: 0.019776053726673126 = 0.012566902674734592 + 0.001 * 7.209151744842529
Epoch 620, val loss: 1.2393685579299927
Epoch 630, training loss: 0.019214894622564316 = 0.01200173981487751 + 0.001 * 7.213155269622803
Epoch 630, val loss: 1.2479474544525146
Epoch 640, training loss: 0.018679199740290642 = 0.011486086994409561 + 0.001 * 7.193112373352051
Epoch 640, val loss: 1.2562609910964966
Epoch 650, training loss: 0.018212541937828064 = 0.01101433951407671 + 0.001 * 7.198201656341553
Epoch 650, val loss: 1.2642934322357178
Epoch 660, training loss: 0.017766643315553665 = 0.010581662878394127 + 0.001 * 7.1849799156188965
Epoch 660, val loss: 1.2720521688461304
Epoch 670, training loss: 0.017402566969394684 = 0.010183877311646938 + 0.001 * 7.218689441680908
Epoch 670, val loss: 1.2795766592025757
Epoch 680, training loss: 0.016989797353744507 = 0.009817284531891346 + 0.001 * 7.172512531280518
Epoch 680, val loss: 1.286868691444397
Epoch 690, training loss: 0.016660859808325768 = 0.009478739462792873 + 0.001 * 7.182120323181152
Epoch 690, val loss: 1.2939441204071045
Epoch 700, training loss: 0.016354525461792946 = 0.009165391325950623 + 0.001 * 7.189134120941162
Epoch 700, val loss: 1.300812840461731
Epoch 710, training loss: 0.016043907031416893 = 0.00887481402605772 + 0.001 * 7.169092178344727
Epoch 710, val loss: 1.3074709177017212
Epoch 720, training loss: 0.015781180933117867 = 0.00860484316945076 + 0.001 * 7.176337718963623
Epoch 720, val loss: 1.3139424324035645
Epoch 730, training loss: 0.015534637495875359 = 0.008353589102625847 + 0.001 * 7.181047439575195
Epoch 730, val loss: 1.3202273845672607
Epoch 740, training loss: 0.015285490080714226 = 0.00811935868114233 + 0.001 * 7.166131019592285
Epoch 740, val loss: 1.3263300657272339
Epoch 750, training loss: 0.015076547861099243 = 0.007900604046881199 + 0.001 * 7.175942897796631
Epoch 750, val loss: 1.3322702646255493
Epoch 760, training loss: 0.014874525368213654 = 0.007695994805544615 + 0.001 * 7.178529739379883
Epoch 760, val loss: 1.3380507230758667
Epoch 770, training loss: 0.014680612832307816 = 0.007504337001591921 + 0.001 * 7.176275730133057
Epoch 770, val loss: 1.3436776399612427
Epoch 780, training loss: 0.014486024156212807 = 0.00732454052194953 + 0.001 * 7.1614837646484375
Epoch 780, val loss: 1.349153757095337
Epoch 790, training loss: 0.014314593747258186 = 0.0071556526236236095 + 0.001 * 7.158940315246582
Epoch 790, val loss: 1.3544856309890747
Epoch 800, training loss: 0.014148643240332603 = 0.006996781565248966 + 0.001 * 7.151861667633057
Epoch 800, val loss: 1.359657645225525
Epoch 810, training loss: 0.01400300208479166 = 0.006847151555120945 + 0.001 * 7.155850410461426
Epoch 810, val loss: 1.3647032976150513
Epoch 820, training loss: 0.013858955353498459 = 0.006706058979034424 + 0.001 * 7.152895927429199
Epoch 820, val loss: 1.3696253299713135
Epoch 830, training loss: 0.013705309480428696 = 0.006572862155735493 + 0.001 * 7.132447242736816
Epoch 830, val loss: 1.374419927597046
Epoch 840, training loss: 0.013589022681117058 = 0.0064469738863408566 + 0.001 * 7.142048358917236
Epoch 840, val loss: 1.3790907859802246
Epoch 850, training loss: 0.013480046764016151 = 0.0063278633169829845 + 0.001 * 7.152182579040527
Epoch 850, val loss: 1.3836528062820435
Epoch 860, training loss: 0.013365843333303928 = 0.006215064786374569 + 0.001 * 7.150778293609619
Epoch 860, val loss: 1.3880903720855713
Epoch 870, training loss: 0.01324625313282013 = 0.006108107510954142 + 0.001 * 7.138145446777344
Epoch 870, val loss: 1.3924211263656616
Epoch 880, training loss: 0.0131382392719388 = 0.006006625015288591 + 0.001 * 7.131613731384277
Epoch 880, val loss: 1.3966412544250488
Epoch 890, training loss: 0.01306312344968319 = 0.005910218693315983 + 0.001 * 7.152904510498047
Epoch 890, val loss: 1.400753140449524
Epoch 900, training loss: 0.012942291796207428 = 0.005818558856844902 + 0.001 * 7.123732089996338
Epoch 900, val loss: 1.4047634601593018
Epoch 910, training loss: 0.012858575209975243 = 0.005731361918151379 + 0.001 * 7.127213478088379
Epoch 910, val loss: 1.4086828231811523
Epoch 920, training loss: 0.012788437306880951 = 0.005648325197398663 + 0.001 * 7.140111446380615
Epoch 920, val loss: 1.4125138521194458
Epoch 930, training loss: 0.012701952829957008 = 0.005569181405007839 + 0.001 * 7.132770538330078
Epoch 930, val loss: 1.4162507057189941
Epoch 940, training loss: 0.012611422687768936 = 0.005493698175996542 + 0.001 * 7.1177239418029785
Epoch 940, val loss: 1.4199037551879883
Epoch 950, training loss: 0.012547504156827927 = 0.0054216464050114155 + 0.001 * 7.125856876373291
Epoch 950, val loss: 1.4234873056411743
Epoch 960, training loss: 0.012478109449148178 = 0.005352830048650503 + 0.001 * 7.125278949737549
Epoch 960, val loss: 1.4269803762435913
Epoch 970, training loss: 0.012419508770108223 = 0.005287041887640953 + 0.001 * 7.1324663162231445
Epoch 970, val loss: 1.4304143190383911
Epoch 980, training loss: 0.012369656935334206 = 0.005224109627306461 + 0.001 * 7.1455464363098145
Epoch 980, val loss: 1.4337637424468994
Epoch 990, training loss: 0.012288773432374 = 0.00516386516392231 + 0.001 * 7.124907493591309
Epoch 990, val loss: 1.4370479583740234
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 1.9695937633514404 = 1.9609968662261963 + 0.001 * 8.596858024597168
Epoch 0, val loss: 1.9561055898666382
Epoch 10, training loss: 1.9596341848373413 = 1.9510374069213867 + 0.001 * 8.596819877624512
Epoch 10, val loss: 1.94643235206604
Epoch 20, training loss: 1.9477214813232422 = 1.9391248226165771 + 0.001 * 8.596674919128418
Epoch 20, val loss: 1.9342676401138306
Epoch 30, training loss: 1.931276559829712 = 1.9226802587509155 + 0.001 * 8.59630012512207
Epoch 30, val loss: 1.9170362949371338
Epoch 40, training loss: 1.9069143533706665 = 1.8983190059661865 + 0.001 * 8.595325469970703
Epoch 40, val loss: 1.8916722536087036
Epoch 50, training loss: 1.871420979499817 = 1.8628284931182861 + 0.001 * 8.592477798461914
Epoch 50, val loss: 1.8561428785324097
Epoch 60, training loss: 1.8275800943374634 = 1.8189975023269653 + 0.001 * 8.582582473754883
Epoch 60, val loss: 1.8163394927978516
Epoch 70, training loss: 1.7894062995910645 = 1.7808653116226196 + 0.001 * 8.541033744812012
Epoch 70, val loss: 1.7868192195892334
Epoch 80, training loss: 1.753675103187561 = 1.7453789710998535 + 0.001 * 8.296143531799316
Epoch 80, val loss: 1.7569481134414673
Epoch 90, training loss: 1.7051752805709839 = 1.697110652923584 + 0.001 * 8.06460189819336
Epoch 90, val loss: 1.7133722305297852
Epoch 100, training loss: 1.6389235258102417 = 1.6311322450637817 + 0.001 * 7.7912492752075195
Epoch 100, val loss: 1.6554627418518066
Epoch 110, training loss: 1.5536682605743408 = 1.5460551977157593 + 0.001 * 7.6130690574646
Epoch 110, val loss: 1.582038164138794
Epoch 120, training loss: 1.4546998739242554 = 1.4471453428268433 + 0.001 * 7.554514408111572
Epoch 120, val loss: 1.4995826482772827
Epoch 130, training loss: 1.3508203029632568 = 1.3432945013046265 + 0.001 * 7.525790214538574
Epoch 130, val loss: 1.416844129562378
Epoch 140, training loss: 1.247971534729004 = 1.2404662370681763 + 0.001 * 7.505249977111816
Epoch 140, val loss: 1.336665153503418
Epoch 150, training loss: 1.1491169929504395 = 1.1416330337524414 + 0.001 * 7.484002113342285
Epoch 150, val loss: 1.2609037160873413
Epoch 160, training loss: 1.0567705631256104 = 1.0493046045303345 + 0.001 * 7.465902328491211
Epoch 160, val loss: 1.1912295818328857
Epoch 170, training loss: 0.9722919464111328 = 0.9648404717445374 + 0.001 * 7.451467990875244
Epoch 170, val loss: 1.1280995607376099
Epoch 180, training loss: 0.8946695327758789 = 0.8872296810150146 + 0.001 * 7.439831733703613
Epoch 180, val loss: 1.0704421997070312
Epoch 190, training loss: 0.8220658302307129 = 0.814635694026947 + 0.001 * 7.430127143859863
Epoch 190, val loss: 1.0177862644195557
Epoch 200, training loss: 0.7529006600379944 = 0.7454783320426941 + 0.001 * 7.422339916229248
Epoch 200, val loss: 0.9695010781288147
Epoch 210, training loss: 0.6867387294769287 = 0.679321825504303 + 0.001 * 7.416896820068359
Epoch 210, val loss: 0.9248976111412048
Epoch 220, training loss: 0.6237292885780334 = 0.6163161396980286 + 0.001 * 7.413140773773193
Epoch 220, val loss: 0.8840427398681641
Epoch 230, training loss: 0.5639408230781555 = 0.5565311312675476 + 0.001 * 7.409710884094238
Epoch 230, val loss: 0.8470803499221802
Epoch 240, training loss: 0.507291853427887 = 0.49988576769828796 + 0.001 * 7.406102180480957
Epoch 240, val loss: 0.8136885166168213
Epoch 250, training loss: 0.4535793662071228 = 0.44617751240730286 + 0.001 * 7.4018402099609375
Epoch 250, val loss: 0.7844918370246887
Epoch 260, training loss: 0.4024730622768402 = 0.3950769603252411 + 0.001 * 7.396090507507324
Epoch 260, val loss: 0.7594722509384155
Epoch 270, training loss: 0.35406574606895447 = 0.3466762602329254 + 0.001 * 7.389473915100098
Epoch 270, val loss: 0.7392563223838806
Epoch 280, training loss: 0.3092372417449951 = 0.30186060070991516 + 0.001 * 7.376645088195801
Epoch 280, val loss: 0.7243197560310364
Epoch 290, training loss: 0.2691028416156769 = 0.2617431879043579 + 0.001 * 7.359654903411865
Epoch 290, val loss: 0.7151033878326416
Epoch 300, training loss: 0.23430891335010529 = 0.22696830332279205 + 0.001 * 7.340609073638916
Epoch 300, val loss: 0.7114154696464539
Epoch 310, training loss: 0.2047339379787445 = 0.19742053747177124 + 0.001 * 7.3134026527404785
Epoch 310, val loss: 0.7121797204017639
Epoch 320, training loss: 0.1797943264245987 = 0.17249630391597748 + 0.001 * 7.2980217933654785
Epoch 320, val loss: 0.7165923118591309
Epoch 330, training loss: 0.1587516963481903 = 0.15145964920520782 + 0.001 * 7.292051792144775
Epoch 330, val loss: 0.7238470315933228
Epoch 340, training loss: 0.1409011334180832 = 0.13362446427345276 + 0.001 * 7.27666711807251
Epoch 340, val loss: 0.7332608699798584
Epoch 350, training loss: 0.12569203972816467 = 0.11841349303722382 + 0.001 * 7.278539657592773
Epoch 350, val loss: 0.7442512512207031
Epoch 360, training loss: 0.11263265460729599 = 0.10535811632871628 + 0.001 * 7.27454137802124
Epoch 360, val loss: 0.7564736604690552
Epoch 370, training loss: 0.10135083645582199 = 0.09407780319452286 + 0.001 * 7.273031234741211
Epoch 370, val loss: 0.7695947289466858
Epoch 380, training loss: 0.09153889864683151 = 0.08426804095506668 + 0.001 * 7.270855903625488
Epoch 380, val loss: 0.7833994626998901
Epoch 390, training loss: 0.0829583927989006 = 0.07569029182195663 + 0.001 * 7.268097400665283
Epoch 390, val loss: 0.7976589798927307
Epoch 400, training loss: 0.07542992383241653 = 0.06815410405397415 + 0.001 * 7.275818347930908
Epoch 400, val loss: 0.8122740387916565
Epoch 410, training loss: 0.06877647340297699 = 0.061507437378168106 + 0.001 * 7.269032001495361
Epoch 410, val loss: 0.8270757794380188
Epoch 420, training loss: 0.06289426982402802 = 0.05562981590628624 + 0.001 * 7.26445198059082
Epoch 420, val loss: 0.8419995307922363
Epoch 430, training loss: 0.057688310742378235 = 0.050424784421920776 + 0.001 * 7.2635273933410645
Epoch 430, val loss: 0.8569470047950745
Epoch 440, training loss: 0.05308001488447189 = 0.04581329599022865 + 0.001 * 7.266717433929443
Epoch 440, val loss: 0.8718905448913574
Epoch 450, training loss: 0.04898976907134056 = 0.041729461401700974 + 0.001 * 7.260307788848877
Epoch 450, val loss: 0.8867104649543762
Epoch 460, training loss: 0.04537448659539223 = 0.0381152369081974 + 0.001 * 7.259249210357666
Epoch 460, val loss: 0.9013444185256958
Epoch 470, training loss: 0.042174361646175385 = 0.034916914999485016 + 0.001 * 7.257447242736816
Epoch 470, val loss: 0.9157683849334717
Epoch 480, training loss: 0.03934521600604057 = 0.03208570554852486 + 0.001 * 7.2595086097717285
Epoch 480, val loss: 0.9299091100692749
Epoch 490, training loss: 0.036831025034189224 = 0.029577167704701424 + 0.001 * 7.25385856628418
Epoch 490, val loss: 0.943710207939148
Epoch 500, training loss: 0.03461989760398865 = 0.02735150419175625 + 0.001 * 7.268393516540527
Epoch 500, val loss: 0.9571804404258728
Epoch 510, training loss: 0.03263615816831589 = 0.0253728237003088 + 0.001 * 7.263335704803467
Epoch 510, val loss: 0.9702327251434326
Epoch 520, training loss: 0.03085552528500557 = 0.023609619587659836 + 0.001 * 7.245904445648193
Epoch 520, val loss: 0.9829354882240295
Epoch 530, training loss: 0.02932233363389969 = 0.022034596651792526 + 0.001 * 7.287736892700195
Epoch 530, val loss: 0.9952428936958313
Epoch 540, training loss: 0.02787265181541443 = 0.02062448300421238 + 0.001 * 7.248167991638184
Epoch 540, val loss: 1.0071887969970703
Epoch 550, training loss: 0.026600733399391174 = 0.01935848593711853 + 0.001 * 7.242248058319092
Epoch 550, val loss: 1.0187348127365112
Epoch 560, training loss: 0.025452129542827606 = 0.01821869984269142 + 0.001 * 7.233428478240967
Epoch 560, val loss: 1.0299392938613892
Epoch 570, training loss: 0.02444772608578205 = 0.017189953476190567 + 0.001 * 7.257772922515869
Epoch 570, val loss: 1.040780782699585
Epoch 580, training loss: 0.023490848019719124 = 0.016258904710412025 + 0.001 * 7.231943607330322
Epoch 580, val loss: 1.0512714385986328
Epoch 590, training loss: 0.022688347846269608 = 0.015414160676300526 + 0.001 * 7.2741875648498535
Epoch 590, val loss: 1.061455249786377
Epoch 600, training loss: 0.02187078818678856 = 0.014645813964307308 + 0.001 * 7.224974155426025
Epoch 600, val loss: 1.0712984800338745
Epoch 610, training loss: 0.021183505654335022 = 0.013945373706519604 + 0.001 * 7.238131046295166
Epoch 610, val loss: 1.080861210823059
Epoch 620, training loss: 0.020521486178040504 = 0.0133053595200181 + 0.001 * 7.216126918792725
Epoch 620, val loss: 1.090123176574707
Epoch 630, training loss: 0.019942564889788628 = 0.012719275429844856 + 0.001 * 7.223289489746094
Epoch 630, val loss: 1.0991156101226807
Epoch 640, training loss: 0.019396629184484482 = 0.012181360274553299 + 0.001 * 7.215267658233643
Epoch 640, val loss: 1.107849359512329
Epoch 650, training loss: 0.01889800652861595 = 0.011686647310853004 + 0.001 * 7.211359977722168
Epoch 650, val loss: 1.1162856817245483
Epoch 660, training loss: 0.018444940447807312 = 0.01123076118528843 + 0.001 * 7.214178085327148
Epoch 660, val loss: 1.1244899034500122
Epoch 670, training loss: 0.01802126131951809 = 0.010809830389916897 + 0.001 * 7.211430072784424
Epoch 670, val loss: 1.1324491500854492
Epoch 680, training loss: 0.01761305332183838 = 0.010420410893857479 + 0.001 * 7.192641735076904
Epoch 680, val loss: 1.1401842832565308
Epoch 690, training loss: 0.017244990915060043 = 0.01005946658551693 + 0.001 * 7.185523509979248
Epoch 690, val loss: 1.1477067470550537
Epoch 700, training loss: 0.016919733956456184 = 0.009724066592752934 + 0.001 * 7.195667266845703
Epoch 700, val loss: 1.1550242900848389
Epoch 710, training loss: 0.01659265346825123 = 0.009411290287971497 + 0.001 * 7.181362628936768
Epoch 710, val loss: 1.1622000932693481
Epoch 720, training loss: 0.01634536124765873 = 0.009118250571191311 + 0.001 * 7.227109909057617
Epoch 720, val loss: 1.1692585945129395
Epoch 730, training loss: 0.016036152839660645 = 0.008843094110488892 + 0.001 * 7.193058967590332
Epoch 730, val loss: 1.1761925220489502
Epoch 740, training loss: 0.015764111652970314 = 0.008583977818489075 + 0.001 * 7.180133819580078
Epoch 740, val loss: 1.1830335855484009
Epoch 750, training loss: 0.015517357736825943 = 0.008339475840330124 + 0.001 * 7.177881240844727
Epoch 750, val loss: 1.1898059844970703
Epoch 760, training loss: 0.015272661112248898 = 0.008108571171760559 + 0.001 * 7.164089679718018
Epoch 760, val loss: 1.1964532136917114
Epoch 770, training loss: 0.015049698762595654 = 0.007890216074883938 + 0.001 * 7.159482479095459
Epoch 770, val loss: 1.2030401229858398
Epoch 780, training loss: 0.014847701415419579 = 0.007683323696255684 + 0.001 * 7.164377689361572
Epoch 780, val loss: 1.2095433473587036
Epoch 790, training loss: 0.01476367563009262 = 0.007487039547413588 + 0.001 * 7.27663516998291
Epoch 790, val loss: 1.215964674949646
Epoch 800, training loss: 0.014459331519901752 = 0.007300631143152714 + 0.001 * 7.158699989318848
Epoch 800, val loss: 1.2223377227783203
Epoch 810, training loss: 0.014297377318143845 = 0.007123181130737066 + 0.001 * 7.174195766448975
Epoch 810, val loss: 1.228643774986267
Epoch 820, training loss: 0.014143051579594612 = 0.0069539835676550865 + 0.001 * 7.189067840576172
Epoch 820, val loss: 1.234910488128662
Epoch 830, training loss: 0.013950785622000694 = 0.006792441941797733 + 0.001 * 7.1583428382873535
Epoch 830, val loss: 1.2411260604858398
Epoch 840, training loss: 0.013791132718324661 = 0.006638363469392061 + 0.001 * 7.152769088745117
Epoch 840, val loss: 1.2473394870758057
Epoch 850, training loss: 0.013633632101118565 = 0.006491539999842644 + 0.001 * 7.142091751098633
Epoch 850, val loss: 1.2534762620925903
Epoch 860, training loss: 0.013494343496859074 = 0.006351756397634745 + 0.001 * 7.142586708068848
Epoch 860, val loss: 1.2595432996749878
Epoch 870, training loss: 0.013373497873544693 = 0.006218759808689356 + 0.001 * 7.154737949371338
Epoch 870, val loss: 1.265539526939392
Epoch 880, training loss: 0.013239460065960884 = 0.006092436611652374 + 0.001 * 7.1470232009887695
Epoch 880, val loss: 1.2714377641677856
Epoch 890, training loss: 0.013111361302435398 = 0.005972287617623806 + 0.001 * 7.139073371887207
Epoch 890, val loss: 1.277287244796753
Epoch 900, training loss: 0.012998117133975029 = 0.0058579896576702595 + 0.001 * 7.140126705169678
Epoch 900, val loss: 1.2830089330673218
Epoch 910, training loss: 0.01291850209236145 = 0.005749118980020285 + 0.001 * 7.169382095336914
Epoch 910, val loss: 1.2886884212493896
Epoch 920, training loss: 0.01277063600718975 = 0.005645247176289558 + 0.001 * 7.125389099121094
Epoch 920, val loss: 1.2943257093429565
Epoch 930, training loss: 0.012669958174228668 = 0.005545986816287041 + 0.001 * 7.123971462249756
Epoch 930, val loss: 1.299877405166626
Epoch 940, training loss: 0.012556411325931549 = 0.005450839176774025 + 0.001 * 7.105571746826172
Epoch 940, val loss: 1.305454134941101
Epoch 950, training loss: 0.012494992464780807 = 0.005359498783946037 + 0.001 * 7.135493278503418
Epoch 950, val loss: 1.3110142946243286
Epoch 960, training loss: 0.012439388781785965 = 0.0052717141807079315 + 0.001 * 7.167674541473389
Epoch 960, val loss: 1.3166035413742065
Epoch 970, training loss: 0.012304377742111683 = 0.005187340080738068 + 0.001 * 7.117037296295166
Epoch 970, val loss: 1.3221791982650757
Epoch 980, training loss: 0.012260195799171925 = 0.0051059783436357975 + 0.001 * 7.15421724319458
Epoch 980, val loss: 1.3277841806411743
Epoch 990, training loss: 0.012163300067186356 = 0.005027479026466608 + 0.001 * 7.1358208656311035
Epoch 990, val loss: 1.3333886861801147
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 1.9617582559585571 = 1.953161358833313 + 0.001 * 8.596843719482422
Epoch 0, val loss: 1.9511610269546509
Epoch 10, training loss: 1.9512032270431519 = 1.9426064491271973 + 0.001 * 8.5968017578125
Epoch 10, val loss: 1.940602421760559
Epoch 20, training loss: 1.9380213022232056 = 1.9294246435165405 + 0.001 * 8.596653938293457
Epoch 20, val loss: 1.9273477792739868
Epoch 30, training loss: 1.9192306995391846 = 1.9106343984603882 + 0.001 * 8.596287727355957
Epoch 30, val loss: 1.90845787525177
Epoch 40, training loss: 1.8913649320602417 = 1.8827695846557617 + 0.001 * 8.595331192016602
Epoch 40, val loss: 1.8809322118759155
Epoch 50, training loss: 1.8532567024230957 = 1.844664454460144 + 0.001 * 8.592248916625977
Epoch 50, val loss: 1.8449498414993286
Epoch 60, training loss: 1.8128516674041748 = 1.8042722940444946 + 0.001 * 8.579394340515137
Epoch 60, val loss: 1.8105528354644775
Epoch 70, training loss: 1.7799900770187378 = 1.771479606628418 + 0.001 * 8.510467529296875
Epoch 70, val loss: 1.7842580080032349
Epoch 80, training loss: 1.7390393018722534 = 1.7308785915374756 + 0.001 * 8.160679817199707
Epoch 80, val loss: 1.748062014579773
Epoch 90, training loss: 1.682007908821106 = 1.6739497184753418 + 0.001 * 8.058219909667969
Epoch 90, val loss: 1.698290228843689
Epoch 100, training loss: 1.6065160036087036 = 1.5985034704208374 + 0.001 * 8.012566566467285
Epoch 100, val loss: 1.6352002620697021
Epoch 110, training loss: 1.5223416090011597 = 1.5143433809280396 + 0.001 * 7.998169898986816
Epoch 110, val loss: 1.5675190687179565
Epoch 120, training loss: 1.4417808055877686 = 1.433793544769287 + 0.001 * 7.987318992614746
Epoch 120, val loss: 1.5048269033432007
Epoch 130, training loss: 1.3669393062591553 = 1.358986258506775 + 0.001 * 7.95302677154541
Epoch 130, val loss: 1.4482221603393555
Epoch 140, training loss: 1.2944862842559814 = 1.2866557836532593 + 0.001 * 7.830557823181152
Epoch 140, val loss: 1.3953813314437866
Epoch 150, training loss: 1.2229310274124146 = 1.215238332748413 + 0.001 * 7.692697048187256
Epoch 150, val loss: 1.343500018119812
Epoch 160, training loss: 1.1530132293701172 = 1.1453614234924316 + 0.001 * 7.651844501495361
Epoch 160, val loss: 1.2938553094863892
Epoch 170, training loss: 1.0861148834228516 = 1.0785322189331055 + 0.001 * 7.582722187042236
Epoch 170, val loss: 1.2469297647476196
Epoch 180, training loss: 1.0238333940505981 = 1.01631760597229 + 0.001 * 7.515795707702637
Epoch 180, val loss: 1.2030282020568848
Epoch 190, training loss: 0.9669485688209534 = 0.9595054984092712 + 0.001 * 7.4430670738220215
Epoch 190, val loss: 1.1624842882156372
Epoch 200, training loss: 0.9144784808158875 = 0.9070735573768616 + 0.001 * 7.404930114746094
Epoch 200, val loss: 1.124536156654358
Epoch 210, training loss: 0.8637008666992188 = 0.8563006520271301 + 0.001 * 7.400214672088623
Epoch 210, val loss: 1.087795376777649
Epoch 220, training loss: 0.8117238879203796 = 0.8043264746665955 + 0.001 * 7.397388458251953
Epoch 220, val loss: 1.0507973432540894
Epoch 230, training loss: 0.7572534680366516 = 0.7498607635498047 + 0.001 * 7.3926920890808105
Epoch 230, val loss: 1.0128819942474365
Epoch 240, training loss: 0.7011005282402039 = 0.6937123537063599 + 0.001 * 7.388156890869141
Epoch 240, val loss: 0.9751952886581421
Epoch 250, training loss: 0.6452569365501404 = 0.6378737092018127 + 0.001 * 7.383212089538574
Epoch 250, val loss: 0.9396772384643555
Epoch 260, training loss: 0.5919890999794006 = 0.5846131443977356 + 0.001 * 7.375981330871582
Epoch 260, val loss: 0.9084780812263489
Epoch 270, training loss: 0.5429136157035828 = 0.5355494022369385 + 0.001 * 7.364206314086914
Epoch 270, val loss: 0.883342981338501
Epoch 280, training loss: 0.49825775623321533 = 0.49091145396232605 + 0.001 * 7.34630823135376
Epoch 280, val loss: 0.8643841743469238
Epoch 290, training loss: 0.4569694995880127 = 0.44964510202407837 + 0.001 * 7.324404716491699
Epoch 290, val loss: 0.8505697250366211
Epoch 300, training loss: 0.41745710372924805 = 0.4101596772670746 + 0.001 * 7.297432899475098
Epoch 300, val loss: 0.8401346206665039
Epoch 310, training loss: 0.3783760368824005 = 0.3710983097553253 + 0.001 * 7.277724266052246
Epoch 310, val loss: 0.831617534160614
Epoch 320, training loss: 0.33933693170547485 = 0.33207759261131287 + 0.001 * 7.259339809417725
Epoch 320, val loss: 0.8246365785598755
Epoch 330, training loss: 0.30114758014678955 = 0.29390454292297363 + 0.001 * 7.243044853210449
Epoch 330, val loss: 0.8194230198860168
Epoch 340, training loss: 0.2652263939380646 = 0.25799310207366943 + 0.001 * 7.233294486999512
Epoch 340, val loss: 0.8167868256568909
Epoch 350, training loss: 0.23279806971549988 = 0.22555913031101227 + 0.001 * 7.238941669464111
Epoch 350, val loss: 0.8174663782119751
Epoch 360, training loss: 0.20428472757339478 = 0.19706930220127106 + 0.001 * 7.215417861938477
Epoch 360, val loss: 0.8219416737556458
Epoch 370, training loss: 0.17955206334590912 = 0.17234572768211365 + 0.001 * 7.206334590911865
Epoch 370, val loss: 0.8298597931861877
Epoch 380, training loss: 0.15813978016376495 = 0.1509353369474411 + 0.001 * 7.204436779022217
Epoch 380, val loss: 0.8407860398292542
Epoch 390, training loss: 0.13954798877239227 = 0.13235239684581757 + 0.001 * 7.195587635040283
Epoch 390, val loss: 0.8541914820671082
Epoch 400, training loss: 0.12336690723896027 = 0.11617839336395264 + 0.001 * 7.188510894775391
Epoch 400, val loss: 0.8695252537727356
Epoch 410, training loss: 0.10925094783306122 = 0.1020684689283371 + 0.001 * 7.182476997375488
Epoch 410, val loss: 0.8861808180809021
Epoch 420, training loss: 0.09693647921085358 = 0.08975563943386078 + 0.001 * 7.180836200714111
Epoch 420, val loss: 0.903777539730072
Epoch 430, training loss: 0.0862075462937355 = 0.07902605086565018 + 0.001 * 7.181491851806641
Epoch 430, val loss: 0.9221165180206299
Epoch 440, training loss: 0.07688223570585251 = 0.06970521807670593 + 0.001 * 7.17701530456543
Epoch 440, val loss: 0.9408902525901794
Epoch 450, training loss: 0.0688130185008049 = 0.06163536757230759 + 0.001 * 7.177651882171631
Epoch 450, val loss: 0.959882378578186
Epoch 460, training loss: 0.061840903013944626 = 0.05466726794838905 + 0.001 * 7.1736345291137695
Epoch 460, val loss: 0.9789161682128906
Epoch 470, training loss: 0.05584084242582321 = 0.04866110533475876 + 0.001 * 7.179736614227295
Epoch 470, val loss: 0.9979282021522522
Epoch 480, training loss: 0.050663333386182785 = 0.043488144874572754 + 0.001 * 7.175189018249512
Epoch 480, val loss: 1.0168009996414185
Epoch 490, training loss: 0.0462048314511776 = 0.039033763110637665 + 0.001 * 7.171069145202637
Epoch 490, val loss: 1.0353578329086304
Epoch 500, training loss: 0.04236316308379173 = 0.03519488126039505 + 0.001 * 7.168282985687256
Epoch 500, val loss: 1.0535669326782227
Epoch 510, training loss: 0.039053529500961304 = 0.03188095614314079 + 0.001 * 7.172573089599609
Epoch 510, val loss: 1.0714082717895508
Epoch 520, training loss: 0.03618711605668068 = 0.02901426889002323 + 0.001 * 7.172845840454102
Epoch 520, val loss: 1.0887748003005981
Epoch 530, training loss: 0.033696673810482025 = 0.026527654379606247 + 0.001 * 7.169020175933838
Epoch 530, val loss: 1.1056663990020752
Epoch 540, training loss: 0.031531378626823425 = 0.02436365745961666 + 0.001 * 7.167718887329102
Epoch 540, val loss: 1.1220386028289795
Epoch 550, training loss: 0.029636956751346588 = 0.02247360721230507 + 0.001 * 7.163349151611328
Epoch 550, val loss: 1.137925624847412
Epoch 560, training loss: 0.02798360213637352 = 0.020816203206777573 + 0.001 * 7.1673994064331055
Epoch 560, val loss: 1.1533058881759644
Epoch 570, training loss: 0.02652045339345932 = 0.0193572249263525 + 0.001 * 7.163228511810303
Epoch 570, val loss: 1.168174147605896
Epoch 580, training loss: 0.025229429826140404 = 0.01806752011179924 + 0.001 * 7.161909580230713
Epoch 580, val loss: 1.1825765371322632
Epoch 590, training loss: 0.02408219501376152 = 0.016923023387789726 + 0.001 * 7.159171104431152
Epoch 590, val loss: 1.1964874267578125
Epoch 600, training loss: 0.02307230979204178 = 0.015903348103165627 + 0.001 * 7.168962478637695
Epoch 600, val loss: 1.2099095582962036
Epoch 610, training loss: 0.02215980552136898 = 0.014991598203778267 + 0.001 * 7.168206691741943
Epoch 610, val loss: 1.222888708114624
Epoch 620, training loss: 0.02133537270128727 = 0.014173413626849651 + 0.001 * 7.161959171295166
Epoch 620, val loss: 1.2354317903518677
Epoch 630, training loss: 0.020590590313076973 = 0.013436533510684967 + 0.001 * 7.154056072235107
Epoch 630, val loss: 1.2475651502609253
Epoch 640, training loss: 0.019930904731154442 = 0.01277068816125393 + 0.001 * 7.160215854644775
Epoch 640, val loss: 1.2593079805374146
Epoch 650, training loss: 0.019320940598845482 = 0.012167206034064293 + 0.001 * 7.15373420715332
Epoch 650, val loss: 1.2706741094589233
Epoch 660, training loss: 0.018768833950161934 = 0.011618603020906448 + 0.001 * 7.150230884552002
Epoch 660, val loss: 1.2816985845565796
Epoch 670, training loss: 0.018277790397405624 = 0.011118405498564243 + 0.001 * 7.159383773803711
Epoch 670, val loss: 1.2923822402954102
Epoch 680, training loss: 0.01781039871275425 = 0.010661155916750431 + 0.001 * 7.149242401123047
Epoch 680, val loss: 1.3027197122573853
Epoch 690, training loss: 0.017409052699804306 = 0.010242006741464138 + 0.001 * 7.167045593261719
Epoch 690, val loss: 1.3127408027648926
Epoch 700, training loss: 0.017004214227199554 = 0.009856872260570526 + 0.001 * 7.147341728210449
Epoch 700, val loss: 1.3224576711654663
Epoch 710, training loss: 0.01664837636053562 = 0.009502175264060497 + 0.001 * 7.146200180053711
Epoch 710, val loss: 1.3318970203399658
Epoch 720, training loss: 0.01631900481879711 = 0.00917475763708353 + 0.001 * 7.144247055053711
Epoch 720, val loss: 1.3410661220550537
Epoch 730, training loss: 0.016015401110053062 = 0.008871794678270817 + 0.001 * 7.143606662750244
Epoch 730, val loss: 1.3499910831451416
Epoch 740, training loss: 0.01573691889643669 = 0.008590951561927795 + 0.001 * 7.145967960357666
Epoch 740, val loss: 1.3586506843566895
Epoch 750, training loss: 0.015480481088161469 = 0.008330121636390686 + 0.001 * 7.1503586769104
Epoch 750, val loss: 1.3670762777328491
Epoch 760, training loss: 0.015234612859785557 = 0.00808744691312313 + 0.001 * 7.147165775299072
Epoch 760, val loss: 1.3752723932266235
Epoch 770, training loss: 0.015005039051175117 = 0.00786123238503933 + 0.001 * 7.1438069343566895
Epoch 770, val loss: 1.3832606077194214
Epoch 780, training loss: 0.014801517128944397 = 0.007649993058294058 + 0.001 * 7.151523590087891
Epoch 780, val loss: 1.3910084962844849
Epoch 790, training loss: 0.014595048502087593 = 0.007452452555298805 + 0.001 * 7.142596244812012
Epoch 790, val loss: 1.3985706567764282
Epoch 800, training loss: 0.014401180669665337 = 0.00726744020357728 + 0.001 * 7.133739948272705
Epoch 800, val loss: 1.4059319496154785
Epoch 810, training loss: 0.014229361899197102 = 0.007093850057572126 + 0.001 * 7.13551139831543
Epoch 810, val loss: 1.4130985736846924
Epoch 820, training loss: 0.014060372486710548 = 0.0069307298399508 + 0.001 * 7.129642486572266
Epoch 820, val loss: 1.4201009273529053
Epoch 830, training loss: 0.013907716609537601 = 0.006776296067982912 + 0.001 * 7.131420135498047
Epoch 830, val loss: 1.4269911050796509
Epoch 840, training loss: 0.013768453150987625 = 0.006630817428231239 + 0.001 * 7.137635231018066
Epoch 840, val loss: 1.4336837530136108
Epoch 850, training loss: 0.013625827617943287 = 0.006493141874670982 + 0.001 * 7.13268518447876
Epoch 850, val loss: 1.44026780128479
Epoch 860, training loss: 0.013496062718331814 = 0.006362603977322578 + 0.001 * 7.133458614349365
Epoch 860, val loss: 1.4466925859451294
Epoch 870, training loss: 0.0133680934086442 = 0.006238702684640884 + 0.001 * 7.129390239715576
Epoch 870, val loss: 1.4529962539672852
Epoch 880, training loss: 0.013255213387310505 = 0.006120880600064993 + 0.001 * 7.134332656860352
Epoch 880, val loss: 1.4591634273529053
Epoch 890, training loss: 0.013138079084455967 = 0.006008823402225971 + 0.001 * 7.129255294799805
Epoch 890, val loss: 1.4652595520019531
Epoch 900, training loss: 0.013027694076299667 = 0.005902119912207127 + 0.001 * 7.125574111938477
Epoch 900, val loss: 1.4712265729904175
Epoch 910, training loss: 0.012923303991556168 = 0.005800383631139994 + 0.001 * 7.12291955947876
Epoch 910, val loss: 1.4771063327789307
Epoch 920, training loss: 0.01282710861414671 = 0.005703445058315992 + 0.001 * 7.123663425445557
Epoch 920, val loss: 1.482873797416687
Epoch 930, training loss: 0.012729119509458542 = 0.0056108953431248665 + 0.001 * 7.118223190307617
Epoch 930, val loss: 1.488541841506958
Epoch 940, training loss: 0.012643537484109402 = 0.0055224657990038395 + 0.001 * 7.1210713386535645
Epoch 940, val loss: 1.4941399097442627
Epoch 950, training loss: 0.01255098171532154 = 0.005437890533357859 + 0.001 * 7.113090515136719
Epoch 950, val loss: 1.4996607303619385
Epoch 960, training loss: 0.012475050985813141 = 0.005356776528060436 + 0.001 * 7.118274211883545
Epoch 960, val loss: 1.5051268339157104
Epoch 970, training loss: 0.012400170788168907 = 0.005278893280774355 + 0.001 * 7.12127685546875
Epoch 970, val loss: 1.5105338096618652
Epoch 980, training loss: 0.01231161318719387 = 0.005203983746469021 + 0.001 * 7.10762882232666
Epoch 980, val loss: 1.5159045457839966
Epoch 990, training loss: 0.012242874130606651 = 0.005131847690790892 + 0.001 * 7.111025810241699
Epoch 990, val loss: 1.5212421417236328
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8091723774380601
The final CL Acc:0.75926, 0.02636, The final GNN Acc:0.80970, 0.00453
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13160])
remove edge: torch.Size([2, 7938])
updated graph: torch.Size([2, 10542])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9594038724899292 = 1.950806975364685 + 0.001 * 8.596857070922852
Epoch 0, val loss: 1.9584925174713135
Epoch 10, training loss: 1.949257254600525 = 1.9406604766845703 + 0.001 * 8.596822738647461
Epoch 10, val loss: 1.9484822750091553
Epoch 20, training loss: 1.9370397329330444 = 1.9284430742263794 + 0.001 * 8.59667682647705
Epoch 20, val loss: 1.9359623193740845
Epoch 30, training loss: 1.9200758934020996 = 1.9114795923233032 + 0.001 * 8.596329689025879
Epoch 30, val loss: 1.9181616306304932
Epoch 40, training loss: 1.8950797319412231 = 1.8864842653274536 + 0.001 * 8.595464706420898
Epoch 40, val loss: 1.8918651342391968
Epoch 50, training loss: 1.8591456413269043 = 1.8505526781082153 + 0.001 * 8.592913627624512
Epoch 50, val loss: 1.8552954196929932
Epoch 60, training loss: 1.8156399726867676 = 1.8070563077926636 + 0.001 * 8.583656311035156
Epoch 60, val loss: 1.8146594762802124
Epoch 70, training loss: 1.775352954864502 = 1.766809344291687 + 0.001 * 8.543627738952637
Epoch 70, val loss: 1.7806559801101685
Epoch 80, training loss: 1.7300409078598022 = 1.721704125404358 + 0.001 * 8.336793899536133
Epoch 80, val loss: 1.7406394481658936
Epoch 90, training loss: 1.6664326190948486 = 1.6583654880523682 + 0.001 * 8.06710147857666
Epoch 90, val loss: 1.6833868026733398
Epoch 100, training loss: 1.581450343132019 = 1.573507308959961 + 0.001 * 7.942981243133545
Epoch 100, val loss: 1.6083012819290161
Epoch 110, training loss: 1.4794039726257324 = 1.471591591835022 + 0.001 * 7.812394618988037
Epoch 110, val loss: 1.52228844165802
Epoch 120, training loss: 1.3721286058425903 = 1.3644315004348755 + 0.001 * 7.6971564292907715
Epoch 120, val loss: 1.435070514678955
Epoch 130, training loss: 1.2667295932769775 = 1.2590795755386353 + 0.001 * 7.650018215179443
Epoch 130, val loss: 1.3507288694381714
Epoch 140, training loss: 1.1645150184631348 = 1.1569441556930542 + 0.001 * 7.570802688598633
Epoch 140, val loss: 1.269202709197998
Epoch 150, training loss: 1.0661053657531738 = 1.058635950088501 + 0.001 * 7.469384670257568
Epoch 150, val loss: 1.1895267963409424
Epoch 160, training loss: 0.9728619456291199 = 0.9654361605644226 + 0.001 * 7.425796985626221
Epoch 160, val loss: 1.1134992837905884
Epoch 170, training loss: 0.8864841461181641 = 0.8790667653083801 + 0.001 * 7.417365550994873
Epoch 170, val loss: 1.042922854423523
Epoch 180, training loss: 0.8088238835334778 = 0.8014196157455444 + 0.001 * 7.404282093048096
Epoch 180, val loss: 0.9802393913269043
Epoch 190, training loss: 0.7407602071762085 = 0.733367383480072 + 0.001 * 7.392847537994385
Epoch 190, val loss: 0.9272463917732239
Epoch 200, training loss: 0.6809880137443542 = 0.6736039519309998 + 0.001 * 7.384036064147949
Epoch 200, val loss: 0.8838837742805481
Epoch 210, training loss: 0.6267049908638 = 0.6193298101425171 + 0.001 * 7.375153541564941
Epoch 210, val loss: 0.8478716015815735
Epoch 220, training loss: 0.575461208820343 = 0.5680946707725525 + 0.001 * 7.3665266036987305
Epoch 220, val loss: 0.8167146444320679
Epoch 230, training loss: 0.5261828303337097 = 0.5188243985176086 + 0.001 * 7.3584208488464355
Epoch 230, val loss: 0.7886795997619629
Epoch 240, training loss: 0.47897541522979736 = 0.471623957157135 + 0.001 * 7.351451396942139
Epoch 240, val loss: 0.7634932398796082
Epoch 250, training loss: 0.43432027101516724 = 0.4269745945930481 + 0.001 * 7.345676898956299
Epoch 250, val loss: 0.7412945628166199
Epoch 260, training loss: 0.39246705174446106 = 0.38512611389160156 + 0.001 * 7.340930461883545
Epoch 260, val loss: 0.722503125667572
Epoch 270, training loss: 0.3534432351589203 = 0.34610623121261597 + 0.001 * 7.337008953094482
Epoch 270, val loss: 0.706990122795105
Epoch 280, training loss: 0.3173649311065674 = 0.31003040075302124 + 0.001 * 7.334537506103516
Epoch 280, val loss: 0.694610595703125
Epoch 290, training loss: 0.2844204008579254 = 0.27708911895751953 + 0.001 * 7.331286907196045
Epoch 290, val loss: 0.6851992607116699
Epoch 300, training loss: 0.254619836807251 = 0.2472917139530182 + 0.001 * 7.328110218048096
Epoch 300, val loss: 0.678752064704895
Epoch 310, training loss: 0.22759269177913666 = 0.22026905417442322 + 0.001 * 7.323634624481201
Epoch 310, val loss: 0.6750277876853943
Epoch 320, training loss: 0.20270155370235443 = 0.19537882506847382 + 0.001 * 7.322724342346191
Epoch 320, val loss: 0.6735305190086365
Epoch 330, training loss: 0.17944218218326569 = 0.17212900519371033 + 0.001 * 7.313171863555908
Epoch 330, val loss: 0.6739025712013245
Epoch 340, training loss: 0.15778286755084991 = 0.15046800673007965 + 0.001 * 7.3148603439331055
Epoch 340, val loss: 0.6759329438209534
Epoch 350, training loss: 0.13806062936782837 = 0.13075891137123108 + 0.001 * 7.301711559295654
Epoch 350, val loss: 0.679784893989563
Epoch 360, training loss: 0.12067198753356934 = 0.1133827418088913 + 0.001 * 7.289246082305908
Epoch 360, val loss: 0.6854739785194397
Epoch 370, training loss: 0.10574349761009216 = 0.09845589846372604 + 0.001 * 7.287595272064209
Epoch 370, val loss: 0.6930351257324219
Epoch 380, training loss: 0.09306910634040833 = 0.08578281849622726 + 0.001 * 7.28628396987915
Epoch 380, val loss: 0.7022749185562134
Epoch 390, training loss: 0.08229625970125198 = 0.07502950727939606 + 0.001 * 7.266752243041992
Epoch 390, val loss: 0.7129464149475098
Epoch 400, training loss: 0.07313616573810577 = 0.06586318463087082 + 0.001 * 7.272983074188232
Epoch 400, val loss: 0.7247758507728577
Epoch 410, training loss: 0.06528157740831375 = 0.058035578578710556 + 0.001 * 7.245997905731201
Epoch 410, val loss: 0.7374212145805359
Epoch 420, training loss: 0.058587849140167236 = 0.051341455429792404 + 0.001 * 7.246392250061035
Epoch 420, val loss: 0.7505766153335571
Epoch 430, training loss: 0.05285688862204552 = 0.04561016708612442 + 0.001 * 7.246719837188721
Epoch 430, val loss: 0.7640076279640198
Epoch 440, training loss: 0.047937557101249695 = 0.040693189948797226 + 0.001 * 7.2443647384643555
Epoch 440, val loss: 0.7775290012359619
Epoch 450, training loss: 0.043707653880119324 = 0.03647082671523094 + 0.001 * 7.236828327178955
Epoch 450, val loss: 0.7910140752792358
Epoch 460, training loss: 0.04006359353661537 = 0.0328393392264843 + 0.001 * 7.224254608154297
Epoch 460, val loss: 0.8043789267539978
Epoch 470, training loss: 0.036936160176992416 = 0.029705790802836418 + 0.001 * 7.2303690910339355
Epoch 470, val loss: 0.8176097869873047
Epoch 480, training loss: 0.034208353608846664 = 0.02699136734008789 + 0.001 * 7.216987133026123
Epoch 480, val loss: 0.8305939435958862
Epoch 490, training loss: 0.031844742596149445 = 0.02462892234325409 + 0.001 * 7.215817928314209
Epoch 490, val loss: 0.8433189392089844
Epoch 500, training loss: 0.029779041185975075 = 0.022562868893146515 + 0.001 * 7.2161712646484375
Epoch 500, val loss: 0.8557422161102295
Epoch 510, training loss: 0.02794577181339264 = 0.020747147500514984 + 0.001 * 7.198624134063721
Epoch 510, val loss: 0.8678202033042908
Epoch 520, training loss: 0.026346586644649506 = 0.0191437266767025 + 0.001 * 7.202858924865723
Epoch 520, val loss: 0.8795628547668457
Epoch 530, training loss: 0.02492300607264042 = 0.017721449956297874 + 0.001 * 7.201556205749512
Epoch 530, val loss: 0.8909403085708618
Epoch 540, training loss: 0.023650240153074265 = 0.01645449548959732 + 0.001 * 7.195743560791016
Epoch 540, val loss: 0.9019753932952881
Epoch 550, training loss: 0.0225061047822237 = 0.015321443788707256 + 0.001 * 7.1846604347229
Epoch 550, val loss: 0.9126735329627991
Epoch 560, training loss: 0.021494794636964798 = 0.014304508455097675 + 0.001 * 7.190284729003906
Epoch 560, val loss: 0.9230133891105652
Epoch 570, training loss: 0.02056579478085041 = 0.013388478197157383 + 0.001 * 7.177315711975098
Epoch 570, val loss: 0.9330341815948486
Epoch 580, training loss: 0.019735027104616165 = 0.012560584582388401 + 0.001 * 7.174442291259766
Epoch 580, val loss: 0.942708432674408
Epoch 590, training loss: 0.018971174955368042 = 0.011809883639216423 + 0.001 * 7.16129207611084
Epoch 590, val loss: 0.9521045684814453
Epoch 600, training loss: 0.01828857883810997 = 0.011127153411507607 + 0.001 * 7.161425590515137
Epoch 600, val loss: 0.9611936211585999
Epoch 610, training loss: 0.01766938716173172 = 0.010504303500056267 + 0.001 * 7.165083408355713
Epoch 610, val loss: 0.9700258374214172
Epoch 620, training loss: 0.017092498019337654 = 0.009934570640325546 + 0.001 * 7.157927513122559
Epoch 620, val loss: 0.9785733819007874
Epoch 630, training loss: 0.016580691561102867 = 0.009411947801709175 + 0.001 * 7.168743133544922
Epoch 630, val loss: 0.9868757724761963
Epoch 640, training loss: 0.016080794855952263 = 0.008931083604693413 + 0.001 * 7.1497111320495605
Epoch 640, val loss: 0.9949744343757629
Epoch 650, training loss: 0.015651533380150795 = 0.008487558923661709 + 0.001 * 7.163974285125732
Epoch 650, val loss: 1.0028313398361206
Epoch 660, training loss: 0.015209514647722244 = 0.008077547885477543 + 0.001 * 7.131967067718506
Epoch 660, val loss: 1.010472297668457
Epoch 670, training loss: 0.01484681386500597 = 0.007697487715631723 + 0.001 * 7.149325847625732
Epoch 670, val loss: 1.0179249048233032
Epoch 680, training loss: 0.014475662261247635 = 0.007344735320657492 + 0.001 * 7.130927085876465
Epoch 680, val loss: 1.0251719951629639
Epoch 690, training loss: 0.014151053503155708 = 0.007016658317297697 + 0.001 * 7.13439416885376
Epoch 690, val loss: 1.03223717212677
Epoch 700, training loss: 0.013855962082743645 = 0.006710883695632219 + 0.001 * 7.145077705383301
Epoch 700, val loss: 1.0391230583190918
Epoch 710, training loss: 0.01354408822953701 = 0.006425355561077595 + 0.001 * 7.118732452392578
Epoch 710, val loss: 1.0458484888076782
Epoch 720, training loss: 0.013281389139592648 = 0.006158279720693827 + 0.001 * 7.123108863830566
Epoch 720, val loss: 1.0524064302444458
Epoch 730, training loss: 0.01303873397409916 = 0.005908295512199402 + 0.001 * 7.130438804626465
Epoch 730, val loss: 1.0588324069976807
Epoch 740, training loss: 0.012835030443966389 = 0.005674004089087248 + 0.001 * 7.1610260009765625
Epoch 740, val loss: 1.065092921257019
Epoch 750, training loss: 0.012566490098834038 = 0.005454283207654953 + 0.001 * 7.11220645904541
Epoch 750, val loss: 1.0712047815322876
Epoch 760, training loss: 0.012362495064735413 = 0.005247741471976042 + 0.001 * 7.114753246307373
Epoch 760, val loss: 1.0771875381469727
Epoch 770, training loss: 0.01219433918595314 = 0.005053125321865082 + 0.001 * 7.141213893890381
Epoch 770, val loss: 1.0830466747283936
Epoch 780, training loss: 0.011983403004705906 = 0.004869390744715929 + 0.001 * 7.114011764526367
Epoch 780, val loss: 1.0888067483901978
Epoch 790, training loss: 0.011833220720291138 = 0.0046949549578130245 + 0.001 * 7.138265132904053
Epoch 790, val loss: 1.0945093631744385
Epoch 800, training loss: 0.011629598215222359 = 0.004529201425611973 + 0.001 * 7.100396633148193
Epoch 800, val loss: 1.100128173828125
Epoch 810, training loss: 0.01147524081170559 = 0.004371190909296274 + 0.001 * 7.104050159454346
Epoch 810, val loss: 1.105741024017334
Epoch 820, training loss: 0.011310633271932602 = 0.004220325034111738 + 0.001 * 7.09030818939209
Epoch 820, val loss: 1.1112995147705078
Epoch 830, training loss: 0.011168299242854118 = 0.004075934179127216 + 0.001 * 7.092365264892578
Epoch 830, val loss: 1.1168535947799683
Epoch 840, training loss: 0.011017265729606152 = 0.003937638830393553 + 0.001 * 7.079626560211182
Epoch 840, val loss: 1.122389793395996
Epoch 850, training loss: 0.010909514501690865 = 0.003805471584200859 + 0.001 * 7.1040425300598145
Epoch 850, val loss: 1.1278611421585083
Epoch 860, training loss: 0.010764818638563156 = 0.003679310204461217 + 0.001 * 7.085508346557617
Epoch 860, val loss: 1.1333163976669312
Epoch 870, training loss: 0.010639756917953491 = 0.00355904852040112 + 0.001 * 7.0807085037231445
Epoch 870, val loss: 1.138709306716919
Epoch 880, training loss: 0.01053913775831461 = 0.0034441875759512186 + 0.001 * 7.094950199127197
Epoch 880, val loss: 1.1440484523773193
Epoch 890, training loss: 0.01040581800043583 = 0.003334582084789872 + 0.001 * 7.071235656738281
Epoch 890, val loss: 1.1493289470672607
Epoch 900, training loss: 0.010312222875654697 = 0.003230008529499173 + 0.001 * 7.08221435546875
Epoch 900, val loss: 1.1545659303665161
Epoch 910, training loss: 0.01022797729820013 = 0.003130345605313778 + 0.001 * 7.097631454467773
Epoch 910, val loss: 1.1597009897232056
Epoch 920, training loss: 0.01009614858776331 = 0.0030353646725416183 + 0.001 * 7.060783386230469
Epoch 920, val loss: 1.164770245552063
Epoch 930, training loss: 0.010009617544710636 = 0.0029448317363858223 + 0.001 * 7.064785480499268
Epoch 930, val loss: 1.169739007949829
Epoch 940, training loss: 0.009915120899677277 = 0.002858580555766821 + 0.001 * 7.056539535522461
Epoch 940, val loss: 1.1746256351470947
Epoch 950, training loss: 0.009827380068600178 = 0.0027763277757912874 + 0.001 * 7.051052093505859
Epoch 950, val loss: 1.1794445514678955
Epoch 960, training loss: 0.00974665954709053 = 0.0026979418471455574 + 0.001 * 7.048717498779297
Epoch 960, val loss: 1.1841261386871338
Epoch 970, training loss: 0.009670238941907883 = 0.002623258624225855 + 0.001 * 7.046980381011963
Epoch 970, val loss: 1.188753604888916
Epoch 980, training loss: 0.009606044739484787 = 0.0025520005729049444 + 0.001 * 7.054044246673584
Epoch 980, val loss: 1.1932934522628784
Epoch 990, training loss: 0.009549207054078579 = 0.0024839737452566624 + 0.001 * 7.065232753753662
Epoch 990, val loss: 1.1977384090423584
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 1.9546657800674438 = 1.9460688829421997 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.941033959388733
Epoch 10, training loss: 1.9439152479171753 = 1.9353184700012207 + 0.001 * 8.596783638000488
Epoch 10, val loss: 1.9307054281234741
Epoch 20, training loss: 1.9302221536636353 = 1.9216256141662598 + 0.001 * 8.596587181091309
Epoch 20, val loss: 1.9173009395599365
Epoch 30, training loss: 1.9108333587646484 = 1.9022372961044312 + 0.001 * 8.596111297607422
Epoch 30, val loss: 1.8981643915176392
Epoch 40, training loss: 1.8825807571411133 = 1.8739858865737915 + 0.001 * 8.594925880432129
Epoch 40, val loss: 1.8706119060516357
Epoch 50, training loss: 1.844114065170288 = 1.8355227708816528 + 0.001 * 8.591341018676758
Epoch 50, val loss: 1.8347091674804688
Epoch 60, training loss: 1.8023709058761597 = 1.7937934398651123 + 0.001 * 8.577522277832031
Epoch 60, val loss: 1.7991135120391846
Epoch 70, training loss: 1.7656197547912598 = 1.757108211517334 + 0.001 * 8.51152515411377
Epoch 70, val loss: 1.768189549446106
Epoch 80, training loss: 1.7177531719207764 = 1.7096178531646729 + 0.001 * 8.135313987731934
Epoch 80, val loss: 1.7236748933792114
Epoch 90, training loss: 1.651328444480896 = 1.6433411836624146 + 0.001 * 7.987212181091309
Epoch 90, val loss: 1.6641589403152466
Epoch 100, training loss: 1.5660427808761597 = 1.5581248998641968 + 0.001 * 7.917879104614258
Epoch 100, val loss: 1.5916111469268799
Epoch 110, training loss: 1.4719438552856445 = 1.464131474494934 + 0.001 * 7.812393665313721
Epoch 110, val loss: 1.5113728046417236
Epoch 120, training loss: 1.3792552947998047 = 1.371627688407898 + 0.001 * 7.627549648284912
Epoch 120, val loss: 1.4348090887069702
Epoch 130, training loss: 1.2894631624221802 = 1.281891107559204 + 0.001 * 7.572056293487549
Epoch 130, val loss: 1.361351490020752
Epoch 140, training loss: 1.1995081901550293 = 1.1919530630111694 + 0.001 * 7.555164813995361
Epoch 140, val loss: 1.289046287536621
Epoch 150, training loss: 1.1084060668945312 = 1.1008706092834473 + 0.001 * 7.535486221313477
Epoch 150, val loss: 1.2173629999160767
Epoch 160, training loss: 1.0181591510772705 = 1.0106480121612549 + 0.001 * 7.511195182800293
Epoch 160, val loss: 1.1475286483764648
Epoch 170, training loss: 0.9310472011566162 = 0.9235742688179016 + 0.001 * 7.4729509353637695
Epoch 170, val loss: 1.0803419351577759
Epoch 180, training loss: 0.8486039638519287 = 0.841183066368103 + 0.001 * 7.4209113121032715
Epoch 180, val loss: 1.0172760486602783
Epoch 190, training loss: 0.7719359993934631 = 0.7645799517631531 + 0.001 * 7.356023788452148
Epoch 190, val loss: 0.9591370820999146
Epoch 200, training loss: 0.7023674845695496 = 0.6950518488883972 + 0.001 * 7.3156352043151855
Epoch 200, val loss: 0.9081398844718933
Epoch 210, training loss: 0.6405408978462219 = 0.6332423686981201 + 0.001 * 7.298534393310547
Epoch 210, val loss: 0.865957498550415
Epoch 220, training loss: 0.585737407207489 = 0.5784534215927124 + 0.001 * 7.284005165100098
Epoch 220, val loss: 0.832435667514801
Epoch 230, training loss: 0.5365691184997559 = 0.5293039679527283 + 0.001 * 7.265137195587158
Epoch 230, val loss: 0.8067086338996887
Epoch 240, training loss: 0.4918358623981476 = 0.4845970869064331 + 0.001 * 7.23876428604126
Epoch 240, val loss: 0.7875853180885315
Epoch 250, training loss: 0.4506435692310333 = 0.4434267282485962 + 0.001 * 7.216848850250244
Epoch 250, val loss: 0.7737764716148376
Epoch 260, training loss: 0.41243264079093933 = 0.40522947907447815 + 0.001 * 7.203160762786865
Epoch 260, val loss: 0.7640687823295593
Epoch 270, training loss: 0.37712693214416504 = 0.36992985010147095 + 0.001 * 7.197070598602295
Epoch 270, val loss: 0.757914125919342
Epoch 280, training loss: 0.34497037529945374 = 0.33777427673339844 + 0.001 * 7.1960954666137695
Epoch 280, val loss: 0.7551047801971436
Epoch 290, training loss: 0.3160802721977234 = 0.308883935213089 + 0.001 * 7.196321964263916
Epoch 290, val loss: 0.7554121017456055
Epoch 300, training loss: 0.2902182936668396 = 0.2830217778682709 + 0.001 * 7.196506500244141
Epoch 300, val loss: 0.7583925724029541
Epoch 310, training loss: 0.26693105697631836 = 0.25973403453826904 + 0.001 * 7.197032451629639
Epoch 310, val loss: 0.7634493708610535
Epoch 320, training loss: 0.2456459105014801 = 0.2384483367204666 + 0.001 * 7.197577953338623
Epoch 320, val loss: 0.7699072360992432
Epoch 330, training loss: 0.22577911615371704 = 0.21858085691928864 + 0.001 * 7.1982645988464355
Epoch 330, val loss: 0.7771570682525635
Epoch 340, training loss: 0.20674753189086914 = 0.19954797625541687 + 0.001 * 7.199556827545166
Epoch 340, val loss: 0.784453809261322
Epoch 350, training loss: 0.18806158006191254 = 0.18086150288581848 + 0.001 * 7.200074195861816
Epoch 350, val loss: 0.7912784814834595
Epoch 360, training loss: 0.16946697235107422 = 0.1622660756111145 + 0.001 * 7.20089054107666
Epoch 360, val loss: 0.7973119020462036
Epoch 370, training loss: 0.15105105936527252 = 0.14384955167770386 + 0.001 * 7.201501846313477
Epoch 370, val loss: 0.8025544285774231
Epoch 380, training loss: 0.1332518607378006 = 0.12604811787605286 + 0.001 * 7.20374059677124
Epoch 380, val loss: 0.8072831034660339
Epoch 390, training loss: 0.11672050505876541 = 0.10951780527830124 + 0.001 * 7.202700614929199
Epoch 390, val loss: 0.8122206926345825
Epoch 400, training loss: 0.1020047590136528 = 0.09480182081460953 + 0.001 * 7.202940940856934
Epoch 400, val loss: 0.817696213722229
Epoch 410, training loss: 0.08926592022180557 = 0.08206302672624588 + 0.001 * 7.202894687652588
Epoch 410, val loss: 0.8240711092948914
Epoch 420, training loss: 0.07843104004859924 = 0.07122240960597992 + 0.001 * 7.2086262702941895
Epoch 420, val loss: 0.8314533829689026
Epoch 430, training loss: 0.06924115866422653 = 0.06203772500157356 + 0.001 * 7.203434467315674
Epoch 430, val loss: 0.8396235704421997
Epoch 440, training loss: 0.06145227700471878 = 0.05425059050321579 + 0.001 * 7.201684474945068
Epoch 440, val loss: 0.8483455181121826
Epoch 450, training loss: 0.05484167858958244 = 0.047640152275562286 + 0.001 * 7.201527118682861
Epoch 450, val loss: 0.8573963046073914
Epoch 460, training loss: 0.04921751469373703 = 0.04201650992035866 + 0.001 * 7.201004505157471
Epoch 460, val loss: 0.8665757775306702
Epoch 470, training loss: 0.04442087560892105 = 0.037221163511276245 + 0.001 * 7.199713230133057
Epoch 470, val loss: 0.8758158087730408
Epoch 480, training loss: 0.04032587260007858 = 0.03312908485531807 + 0.001 * 7.196789264678955
Epoch 480, val loss: 0.8850175142288208
Epoch 490, training loss: 0.036841731518507004 = 0.0296354778110981 + 0.001 * 7.206254959106445
Epoch 490, val loss: 0.8941451907157898
Epoch 500, training loss: 0.03383658081293106 = 0.026645008474588394 + 0.001 * 7.19157075881958
Epoch 500, val loss: 0.9031636118888855
Epoch 510, training loss: 0.03126319497823715 = 0.024073563516139984 + 0.001 * 7.189630508422852
Epoch 510, val loss: 0.9121360778808594
Epoch 520, training loss: 0.02905093878507614 = 0.021847151219844818 + 0.001 * 7.2037882804870605
Epoch 520, val loss: 0.9211123585700989
Epoch 530, training loss: 0.027097202837467194 = 0.019907504320144653 + 0.001 * 7.189698696136475
Epoch 530, val loss: 0.9300835132598877
Epoch 540, training loss: 0.025385642424225807 = 0.018209071829915047 + 0.001 * 7.176570892333984
Epoch 540, val loss: 0.9390283226966858
Epoch 550, training loss: 0.023905310779809952 = 0.016715772449970245 + 0.001 * 7.189538955688477
Epoch 550, val loss: 0.9478849172592163
Epoch 560, training loss: 0.02256610244512558 = 0.015398362651467323 + 0.001 * 7.1677398681640625
Epoch 560, val loss: 0.9566141963005066
Epoch 570, training loss: 0.021400948986411095 = 0.014231926761567593 + 0.001 * 7.1690216064453125
Epoch 570, val loss: 0.9652113318443298
Epoch 580, training loss: 0.020354818552732468 = 0.013195599429309368 + 0.001 * 7.159217834472656
Epoch 580, val loss: 0.9736369848251343
Epoch 590, training loss: 0.019443247467279434 = 0.012271581217646599 + 0.001 * 7.171666622161865
Epoch 590, val loss: 0.9819123148918152
Epoch 600, training loss: 0.018591957166790962 = 0.011445023119449615 + 0.001 * 7.146933555603027
Epoch 600, val loss: 0.989970326423645
Epoch 610, training loss: 0.017838936299085617 = 0.010702970437705517 + 0.001 * 7.135964393615723
Epoch 610, val loss: 0.9978340268135071
Epoch 620, training loss: 0.017179181799292564 = 0.010034533217549324 + 0.001 * 7.14464807510376
Epoch 620, val loss: 1.005502462387085
Epoch 630, training loss: 0.01656360924243927 = 0.009430468082427979 + 0.001 * 7.133141040802002
Epoch 630, val loss: 1.0129531621932983
Epoch 640, training loss: 0.01603427343070507 = 0.00888279639184475 + 0.001 * 7.151477336883545
Epoch 640, val loss: 1.0202151536941528
Epoch 650, training loss: 0.01552256103605032 = 0.008384816348552704 + 0.001 * 7.137744426727295
Epoch 650, val loss: 1.0272554159164429
Epoch 660, training loss: 0.01505403034389019 = 0.007930710911750793 + 0.001 * 7.123319149017334
Epoch 660, val loss: 1.0341142416000366
Epoch 670, training loss: 0.014624400064349174 = 0.007515391800552607 + 0.001 * 7.109007835388184
Epoch 670, val loss: 1.04079008102417
Epoch 680, training loss: 0.014252934604883194 = 0.007134307641535997 + 0.001 * 7.118627071380615
Epoch 680, val loss: 1.0472724437713623
Epoch 690, training loss: 0.013893289491534233 = 0.00678366981446743 + 0.001 * 7.109618663787842
Epoch 690, val loss: 1.0535681247711182
Epoch 700, training loss: 0.013565557077527046 = 0.006459671072661877 + 0.001 * 7.1058855056762695
Epoch 700, val loss: 1.0597091913223267
Epoch 710, training loss: 0.01327587105333805 = 0.006158605217933655 + 0.001 * 7.117265224456787
Epoch 710, val loss: 1.0657073259353638
Epoch 720, training loss: 0.012977387756109238 = 0.005877426825463772 + 0.001 * 7.099961280822754
Epoch 720, val loss: 1.0715192556381226
Epoch 730, training loss: 0.012735996395349503 = 0.005613518413156271 + 0.001 * 7.122477054595947
Epoch 730, val loss: 1.0772193670272827
Epoch 740, training loss: 0.012470336630940437 = 0.005365490447729826 + 0.001 * 7.1048455238342285
Epoch 740, val loss: 1.0827866792678833
Epoch 750, training loss: 0.012232353910803795 = 0.005132300779223442 + 0.001 * 7.100053310394287
Epoch 750, val loss: 1.0882372856140137
Epoch 760, training loss: 0.012005031108856201 = 0.0049129570834338665 + 0.001 * 7.092073917388916
Epoch 760, val loss: 1.0935587882995605
Epoch 770, training loss: 0.011833742260932922 = 0.004706652369350195 + 0.001 * 7.127089977264404
Epoch 770, val loss: 1.0987821817398071
Epoch 780, training loss: 0.011600909754633904 = 0.004513008985668421 + 0.001 * 7.087900638580322
Epoch 780, val loss: 1.103848934173584
Epoch 790, training loss: 0.011419760063290596 = 0.004331157077103853 + 0.001 * 7.0886030197143555
Epoch 790, val loss: 1.1088353395462036
Epoch 800, training loss: 0.011255405843257904 = 0.00416033947840333 + 0.001 * 7.095065593719482
Epoch 800, val loss: 1.113692045211792
Epoch 810, training loss: 0.011093340814113617 = 0.003999839536845684 + 0.001 * 7.093501091003418
Epoch 810, val loss: 1.1184403896331787
Epoch 820, training loss: 0.010945403948426247 = 0.00384898716583848 + 0.001 * 7.096416473388672
Epoch 820, val loss: 1.1230884790420532
Epoch 830, training loss: 0.010793808847665787 = 0.0037071555852890015 + 0.001 * 7.0866522789001465
Epoch 830, val loss: 1.1276086568832397
Epoch 840, training loss: 0.01068377960473299 = 0.003573730820789933 + 0.001 * 7.110048294067383
Epoch 840, val loss: 1.1320445537567139
Epoch 850, training loss: 0.010528111830353737 = 0.003448197618126869 + 0.001 * 7.07991361618042
Epoch 850, val loss: 1.1363493204116821
Epoch 860, training loss: 0.010406955145299435 = 0.0033299087081104517 + 0.001 * 7.077045917510986
Epoch 860, val loss: 1.1405657529830933
Epoch 870, training loss: 0.010313065722584724 = 0.0032183323055505753 + 0.001 * 7.094733715057373
Epoch 870, val loss: 1.1446894407272339
Epoch 880, training loss: 0.010195491835474968 = 0.0031129540875554085 + 0.001 * 7.082537651062012
Epoch 880, val loss: 1.1487224102020264
Epoch 890, training loss: 0.01008906215429306 = 0.0030130487866699696 + 0.001 * 7.076013565063477
Epoch 890, val loss: 1.1527228355407715
Epoch 900, training loss: 0.010038873180747032 = 0.0029178669210523367 + 0.001 * 7.121005535125732
Epoch 900, val loss: 1.1567294597625732
Epoch 910, training loss: 0.00990387424826622 = 0.0028268168680369854 + 0.001 * 7.077056884765625
Epoch 910, val loss: 1.1607671976089478
Epoch 920, training loss: 0.009813970886170864 = 0.0027392099145799875 + 0.001 * 7.074760437011719
Epoch 920, val loss: 1.1648967266082764
Epoch 930, training loss: 0.009728815406560898 = 0.002654676092788577 + 0.001 * 7.074138641357422
Epoch 930, val loss: 1.1691081523895264
Epoch 940, training loss: 0.009641503915190697 = 0.002573200035840273 + 0.001 * 7.068303108215332
Epoch 940, val loss: 1.1733992099761963
Epoch 950, training loss: 0.009560305625200272 = 0.0024948283098638058 + 0.001 * 7.06547737121582
Epoch 950, val loss: 1.1777594089508057
Epoch 960, training loss: 0.009479629807174206 = 0.00241954461671412 + 0.001 * 7.060085296630859
Epoch 960, val loss: 1.1821366548538208
Epoch 970, training loss: 0.00941565353423357 = 0.0023473117034882307 + 0.001 * 7.068341255187988
Epoch 970, val loss: 1.186527967453003
Epoch 980, training loss: 0.009353455156087875 = 0.0022781407460570335 + 0.001 * 7.075314044952393
Epoch 980, val loss: 1.1909277439117432
Epoch 990, training loss: 0.009273068979382515 = 0.002211990300565958 + 0.001 * 7.061079025268555
Epoch 990, val loss: 1.1952852010726929
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 1.9471906423568726 = 1.938593864440918 + 0.001 * 8.596835136413574
Epoch 0, val loss: 1.9289546012878418
Epoch 10, training loss: 1.9370509386062622 = 1.9284541606903076 + 0.001 * 8.596781730651855
Epoch 10, val loss: 1.9191306829452515
Epoch 20, training loss: 1.9247184991836548 = 1.9161219596862793 + 0.001 * 8.596595764160156
Epoch 20, val loss: 1.9068571329116821
Epoch 30, training loss: 1.90751314163208 = 1.8989169597625732 + 0.001 * 8.596166610717773
Epoch 30, val loss: 1.889580488204956
Epoch 40, training loss: 1.8823989629745483 = 1.873803734779358 + 0.001 * 8.595210075378418
Epoch 40, val loss: 1.8648276329040527
Epoch 50, training loss: 1.8477909564971924 = 1.8391985893249512 + 0.001 * 8.592415809631348
Epoch 50, val loss: 1.8325060606002808
Epoch 60, training loss: 1.8088489770889282 = 1.8002673387527466 + 0.001 * 8.58164119720459
Epoch 60, val loss: 1.8001632690429688
Epoch 70, training loss: 1.773556113243103 = 1.7650260925292969 + 0.001 * 8.529986381530762
Epoch 70, val loss: 1.7732371091842651
Epoch 80, training loss: 1.729404091835022 = 1.7211592197418213 + 0.001 * 8.244874000549316
Epoch 80, val loss: 1.734323501586914
Epoch 90, training loss: 1.666696310043335 = 1.6586247682571411 + 0.001 * 8.071499824523926
Epoch 90, val loss: 1.6777046918869019
Epoch 100, training loss: 1.5819528102874756 = 1.57407808303833 + 0.001 * 7.874754428863525
Epoch 100, val loss: 1.603958249092102
Epoch 110, training loss: 1.4817702770233154 = 1.4741261005401611 + 0.001 * 7.64415168762207
Epoch 110, val loss: 1.5205049514770508
Epoch 120, training loss: 1.3778700828552246 = 1.370299220085144 + 0.001 * 7.570910930633545
Epoch 120, val loss: 1.434738278388977
Epoch 130, training loss: 1.2748817205429077 = 1.2673754692077637 + 0.001 * 7.506290435791016
Epoch 130, val loss: 1.3512909412384033
Epoch 140, training loss: 1.17332124710083 = 1.1658653020858765 + 0.001 * 7.455965995788574
Epoch 140, val loss: 1.270320177078247
Epoch 150, training loss: 1.0743099451065063 = 1.0669002532958984 + 0.001 * 7.409720420837402
Epoch 150, val loss: 1.1935491561889648
Epoch 160, training loss: 0.9799541234970093 = 0.972589373588562 + 0.001 * 7.364742279052734
Epoch 160, val loss: 1.1218395233154297
Epoch 170, training loss: 0.8918976187705994 = 0.8845524191856384 + 0.001 * 7.345171928405762
Epoch 170, val loss: 1.0558898448944092
Epoch 180, training loss: 0.8106729388237 = 0.803335428237915 + 0.001 * 7.337512493133545
Epoch 180, val loss: 0.9959222674369812
Epoch 190, training loss: 0.7363120317459106 = 0.7289842367172241 + 0.001 * 7.327810287475586
Epoch 190, val loss: 0.9420287013053894
Epoch 200, training loss: 0.6684088706970215 = 0.661093533039093 + 0.001 * 7.315347194671631
Epoch 200, val loss: 0.8947538137435913
Epoch 210, training loss: 0.6059279441833496 = 0.598628580570221 + 0.001 * 7.299354553222656
Epoch 210, val loss: 0.8538473844528198
Epoch 220, training loss: 0.5476264357566833 = 0.5403470396995544 + 0.001 * 7.279391765594482
Epoch 220, val loss: 0.8187817335128784
Epoch 230, training loss: 0.49242350459098816 = 0.4851544201374054 + 0.001 * 7.269093990325928
Epoch 230, val loss: 0.7886674404144287
Epoch 240, training loss: 0.43961822986602783 = 0.4323863983154297 + 0.001 * 7.231821537017822
Epoch 240, val loss: 0.7628113627433777
Epoch 250, training loss: 0.38892245292663574 = 0.3817090690135956 + 0.001 * 7.213374614715576
Epoch 250, val loss: 0.7404399514198303
Epoch 260, training loss: 0.34037402272224426 = 0.33319351077079773 + 0.001 * 7.180509567260742
Epoch 260, val loss: 0.7215873003005981
Epoch 270, training loss: 0.2945968508720398 = 0.28742000460624695 + 0.001 * 7.176846981048584
Epoch 270, val loss: 0.7062555551528931
Epoch 280, training loss: 0.25248783826828003 = 0.24532108008861542 + 0.001 * 7.1667656898498535
Epoch 280, val loss: 0.6949032545089722
Epoch 290, training loss: 0.21503975987434387 = 0.2078869789838791 + 0.001 * 7.152774810791016
Epoch 290, val loss: 0.688088595867157
Epoch 300, training loss: 0.18290245532989502 = 0.17575259506702423 + 0.001 * 7.149864196777344
Epoch 300, val loss: 0.6859840750694275
Epoch 310, training loss: 0.15606807172298431 = 0.1489194929599762 + 0.001 * 7.14857292175293
Epoch 310, val loss: 0.6884151101112366
Epoch 320, training loss: 0.13400454819202423 = 0.1268678456544876 + 0.001 * 7.136696815490723
Epoch 320, val loss: 0.6947519779205322
Epoch 330, training loss: 0.1159777119755745 = 0.10884037613868713 + 0.001 * 7.137335777282715
Epoch 330, val loss: 0.7042381167411804
Epoch 340, training loss: 0.10119141638278961 = 0.09406150877475739 + 0.001 * 7.1299052238464355
Epoch 340, val loss: 0.7159656286239624
Epoch 350, training loss: 0.08901643753051758 = 0.08185421675443649 + 0.001 * 7.162220478057861
Epoch 350, val loss: 0.7292777299880981
Epoch 360, training loss: 0.07881073653697968 = 0.0716840997338295 + 0.001 * 7.126635551452637
Epoch 360, val loss: 0.7436790466308594
Epoch 370, training loss: 0.07026399672031403 = 0.06313973665237427 + 0.001 * 7.124259948730469
Epoch 370, val loss: 0.7587262392044067
Epoch 380, training loss: 0.06303885579109192 = 0.05591215193271637 + 0.001 * 7.126704692840576
Epoch 380, val loss: 0.7741994261741638
Epoch 390, training loss: 0.05687741935253143 = 0.049762312322854996 + 0.001 * 7.11510705947876
Epoch 390, val loss: 0.7898387908935547
Epoch 400, training loss: 0.05162222310900688 = 0.044503819197416306 + 0.001 * 7.118403911590576
Epoch 400, val loss: 0.8055118918418884
Epoch 410, training loss: 0.04711015895009041 = 0.03998482599854469 + 0.001 * 7.125332832336426
Epoch 410, val loss: 0.8210806846618652
Epoch 420, training loss: 0.043195728212594986 = 0.03608275577425957 + 0.001 * 7.112973690032959
Epoch 420, val loss: 0.8364675045013428
Epoch 430, training loss: 0.03980569168925285 = 0.0326971560716629 + 0.001 * 7.10853385925293
Epoch 430, val loss: 0.8516183495521545
Epoch 440, training loss: 0.03686375170946121 = 0.029746664687991142 + 0.001 * 7.117086410522461
Epoch 440, val loss: 0.8664446473121643
Epoch 450, training loss: 0.034270402044057846 = 0.027163732796907425 + 0.001 * 7.106670379638672
Epoch 450, val loss: 0.8809424638748169
Epoch 460, training loss: 0.03198833391070366 = 0.024893421679735184 + 0.001 * 7.094911575317383
Epoch 460, val loss: 0.8951089382171631
Epoch 470, training loss: 0.029985954985022545 = 0.022889984771609306 + 0.001 * 7.0959696769714355
Epoch 470, val loss: 0.9089226126670837
Epoch 480, training loss: 0.02821456454694271 = 0.021115032956004143 + 0.001 * 7.0995306968688965
Epoch 480, val loss: 0.9223213791847229
Epoch 490, training loss: 0.026627346873283386 = 0.019536565989255905 + 0.001 * 7.090781211853027
Epoch 490, val loss: 0.9353502988815308
Epoch 500, training loss: 0.025234371423721313 = 0.018127471208572388 + 0.001 * 7.106898784637451
Epoch 500, val loss: 0.9480400085449219
Epoch 510, training loss: 0.023948365822434425 = 0.016865206882357597 + 0.001 * 7.08315896987915
Epoch 510, val loss: 0.9603571891784668
Epoch 520, training loss: 0.022832456976175308 = 0.015730634331703186 + 0.001 * 7.101823329925537
Epoch 520, val loss: 0.9723056554794312
Epoch 530, training loss: 0.02178015559911728 = 0.014705866575241089 + 0.001 * 7.0742878913879395
Epoch 530, val loss: 0.9840051531791687
Epoch 540, training loss: 0.020847337320446968 = 0.013774965889751911 + 0.001 * 7.072371006011963
Epoch 540, val loss: 0.995453417301178
Epoch 550, training loss: 0.02001294493675232 = 0.012923022732138634 + 0.001 * 7.089922904968262
Epoch 550, val loss: 1.0067155361175537
Epoch 560, training loss: 0.019218740984797478 = 0.012140200473368168 + 0.001 * 7.078540325164795
Epoch 560, val loss: 1.0178158283233643
Epoch 570, training loss: 0.018490908667445183 = 0.011419919319450855 + 0.001 * 7.070988655090332
Epoch 570, val loss: 1.0287268161773682
Epoch 580, training loss: 0.017826996743679047 = 0.010757043957710266 + 0.001 * 7.06995153427124
Epoch 580, val loss: 1.0394749641418457
Epoch 590, training loss: 0.017227692529559135 = 0.010146782733500004 + 0.001 * 7.08090877532959
Epoch 590, val loss: 1.0500030517578125
Epoch 600, training loss: 0.016648700460791588 = 0.009584860876202583 + 0.001 * 7.063839912414551
Epoch 600, val loss: 1.0603060722351074
Epoch 610, training loss: 0.01613716036081314 = 0.009067189879715443 + 0.001 * 7.069969654083252
Epoch 610, val loss: 1.0703685283660889
Epoch 620, training loss: 0.015665890648961067 = 0.008589737117290497 + 0.001 * 7.076153755187988
Epoch 620, val loss: 1.080203890800476
Epoch 630, training loss: 0.015212979167699814 = 0.00814900454133749 + 0.001 * 7.063973903656006
Epoch 630, val loss: 1.0897901058197021
Epoch 640, training loss: 0.01480044610798359 = 0.0077416435815393925 + 0.001 * 7.058802604675293
Epoch 640, val loss: 1.099137544631958
Epoch 650, training loss: 0.014411669224500656 = 0.007364540360867977 + 0.001 * 7.047128677368164
Epoch 650, val loss: 1.108231544494629
Epoch 660, training loss: 0.014091966673731804 = 0.007015015929937363 + 0.001 * 7.076950550079346
Epoch 660, val loss: 1.117119312286377
Epoch 670, training loss: 0.013745428994297981 = 0.006690565962344408 + 0.001 * 7.054862022399902
Epoch 670, val loss: 1.1257673501968384
Epoch 680, training loss: 0.013444487005472183 = 0.006389038171619177 + 0.001 * 7.05544900894165
Epoch 680, val loss: 1.1342265605926514
Epoch 690, training loss: 0.013147260993719101 = 0.00610836548730731 + 0.001 * 7.038895130157471
Epoch 690, val loss: 1.1424453258514404
Epoch 700, training loss: 0.01289091445505619 = 0.005846690386533737 + 0.001 * 7.044223785400391
Epoch 700, val loss: 1.1504883766174316
Epoch 710, training loss: 0.012638142332434654 = 0.0056024654768407345 + 0.001 * 7.035676956176758
Epoch 710, val loss: 1.1583218574523926
Epoch 720, training loss: 0.01240427978336811 = 0.005374229047447443 + 0.001 * 7.030049800872803
Epoch 720, val loss: 1.1659756898880005
Epoch 730, training loss: 0.01220405101776123 = 0.0051606083288788795 + 0.001 * 7.0434417724609375
Epoch 730, val loss: 1.1734663248062134
Epoch 740, training loss: 0.011980313807725906 = 0.004960410296916962 + 0.001 * 7.019903182983398
Epoch 740, val loss: 1.1807501316070557
Epoch 750, training loss: 0.011854466050863266 = 0.0047725350596010685 + 0.001 * 7.081930637359619
Epoch 750, val loss: 1.1878540515899658
Epoch 760, training loss: 0.01162545196712017 = 0.0045960587449371815 + 0.001 * 7.029393196105957
Epoch 760, val loss: 1.1948187351226807
Epoch 770, training loss: 0.01148227509111166 = 0.004430007189512253 + 0.001 * 7.052267551422119
Epoch 770, val loss: 1.2016162872314453
Epoch 780, training loss: 0.011303065344691277 = 0.004273465368896723 + 0.001 * 7.029599666595459
Epoch 780, val loss: 1.208309531211853
Epoch 790, training loss: 0.011173149570822716 = 0.004125688690692186 + 0.001 * 7.047460556030273
Epoch 790, val loss: 1.2148232460021973
Epoch 800, training loss: 0.01099776104092598 = 0.003985871560871601 + 0.001 * 7.0118889808654785
Epoch 800, val loss: 1.2212824821472168
Epoch 810, training loss: 0.010870818980038166 = 0.0038532766047865152 + 0.001 * 7.017541885375977
Epoch 810, val loss: 1.2275664806365967
Epoch 820, training loss: 0.010743377730250359 = 0.0037271142937242985 + 0.001 * 7.016262531280518
Epoch 820, val loss: 1.2338637113571167
Epoch 830, training loss: 0.010609147138893604 = 0.0036068533081561327 + 0.001 * 7.002293586730957
Epoch 830, val loss: 1.240099549293518
Epoch 840, training loss: 0.01052132248878479 = 0.0034918556921184063 + 0.001 * 7.0294671058654785
Epoch 840, val loss: 1.2463270425796509
Epoch 850, training loss: 0.01038928423076868 = 0.0033818462397903204 + 0.001 * 7.007437229156494
Epoch 850, val loss: 1.252506971359253
Epoch 860, training loss: 0.010322031565010548 = 0.003276563948020339 + 0.001 * 7.045466899871826
Epoch 860, val loss: 1.2586727142333984
Epoch 870, training loss: 0.010189679451286793 = 0.0031757275573909283 + 0.001 * 7.013951778411865
Epoch 870, val loss: 1.26483952999115
Epoch 880, training loss: 0.010107551701366901 = 0.0030791766475886106 + 0.001 * 7.028375148773193
Epoch 880, val loss: 1.2709821462631226
Epoch 890, training loss: 0.009990012273192406 = 0.002986718900501728 + 0.001 * 7.003293037414551
Epoch 890, val loss: 1.2770576477050781
Epoch 900, training loss: 0.009913880378007889 = 0.0028983312658965588 + 0.001 * 7.015549182891846
Epoch 900, val loss: 1.2830969095230103
Epoch 910, training loss: 0.009817739948630333 = 0.002813719678670168 + 0.001 * 7.0040202140808105
Epoch 910, val loss: 1.2891286611557007
Epoch 920, training loss: 0.009727459400892258 = 0.002732891822233796 + 0.001 * 6.994566917419434
Epoch 920, val loss: 1.295031189918518
Epoch 930, training loss: 0.009673461318016052 = 0.0026555685326457024 + 0.001 * 7.017892360687256
Epoch 930, val loss: 1.3009417057037354
Epoch 940, training loss: 0.009574361145496368 = 0.0025816301349550486 + 0.001 * 6.992731094360352
Epoch 940, val loss: 1.3067189455032349
Epoch 950, training loss: 0.009519090875983238 = 0.0025109201669692993 + 0.001 * 7.008170127868652
Epoch 950, val loss: 1.3123871088027954
Epoch 960, training loss: 0.009440806694328785 = 0.0024433147627860308 + 0.001 * 6.997491359710693
Epoch 960, val loss: 1.317993402481079
Epoch 970, training loss: 0.00934384111315012 = 0.0023787112440913916 + 0.001 * 6.965129852294922
Epoch 970, val loss: 1.3235085010528564
Epoch 980, training loss: 0.00929967686533928 = 0.0023169098421931267 + 0.001 * 6.982766628265381
Epoch 980, val loss: 1.328940987586975
Epoch 990, training loss: 0.009254712611436844 = 0.002257760614156723 + 0.001 * 6.996952056884766
Epoch 990, val loss: 1.33432936668396
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8344754876120191
The final CL Acc:0.80123, 0.01666, The final GNN Acc:0.83641, 0.00203
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11662])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10634])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9469504356384277 = 1.9383535385131836 + 0.001 * 8.596860885620117
Epoch 0, val loss: 1.93623948097229
Epoch 10, training loss: 1.9364105463027954 = 1.9278137683868408 + 0.001 * 8.59681224822998
Epoch 10, val loss: 1.9260382652282715
Epoch 20, training loss: 1.923216462135315 = 1.91461980342865 + 0.001 * 8.596673965454102
Epoch 20, val loss: 1.912817358970642
Epoch 30, training loss: 1.904763102531433 = 1.8961666822433472 + 0.001 * 8.596376419067383
Epoch 30, val loss: 1.8939874172210693
Epoch 40, training loss: 1.8784358501434326 = 1.8698402643203735 + 0.001 * 8.595638275146484
Epoch 40, val loss: 1.8674145936965942
Epoch 50, training loss: 1.8443057537078857 = 1.835712194442749 + 0.001 * 8.59351634979248
Epoch 50, val loss: 1.8348662853240967
Epoch 60, training loss: 1.8110014200210571 = 1.8024152517318726 + 0.001 * 8.586136817932129
Epoch 60, val loss: 1.8070424795150757
Epoch 70, training loss: 1.7821351289749146 = 1.77358078956604 + 0.001 * 8.554312705993652
Epoch 70, val loss: 1.7835888862609863
Epoch 80, training loss: 1.7415512800216675 = 1.7332208156585693 + 0.001 * 8.330449104309082
Epoch 80, val loss: 1.747426152229309
Epoch 90, training loss: 1.684144139289856 = 1.6760327816009521 + 0.001 * 8.111320495605469
Epoch 90, val loss: 1.6966346502304077
Epoch 100, training loss: 1.605787992477417 = 1.597770094871521 + 0.001 * 8.017887115478516
Epoch 100, val loss: 1.6304376125335693
Epoch 110, training loss: 1.5127537250518799 = 1.5048956871032715 + 0.001 * 7.858092308044434
Epoch 110, val loss: 1.5525740385055542
Epoch 120, training loss: 1.414451003074646 = 1.406794786453247 + 0.001 * 7.656265735626221
Epoch 120, val loss: 1.4713565111160278
Epoch 130, training loss: 1.3152880668640137 = 1.3076329231262207 + 0.001 * 7.655172824859619
Epoch 130, val loss: 1.3927240371704102
Epoch 140, training loss: 1.2144570350646973 = 1.2068421840667725 + 0.001 * 7.6148295402526855
Epoch 140, val loss: 1.3155062198638916
Epoch 150, training loss: 1.113094687461853 = 1.1055132150650024 + 0.001 * 7.581457138061523
Epoch 150, val loss: 1.240789771080017
Epoch 160, training loss: 1.014150857925415 = 1.0066207647323608 + 0.001 * 7.530066967010498
Epoch 160, val loss: 1.1697933673858643
Epoch 170, training loss: 0.9208468198776245 = 0.9133744239807129 + 0.001 * 7.472384929656982
Epoch 170, val loss: 1.1036912202835083
Epoch 180, training loss: 0.8360102772712708 = 0.8285602331161499 + 0.001 * 7.450072765350342
Epoch 180, val loss: 1.0448628664016724
Epoch 190, training loss: 0.7611935138702393 = 0.7537544965744019 + 0.001 * 7.438992977142334
Epoch 190, val loss: 0.9949923157691956
Epoch 200, training loss: 0.6958781480789185 = 0.6884450912475586 + 0.001 * 7.433067321777344
Epoch 200, val loss: 0.954814076423645
Epoch 210, training loss: 0.6379708051681519 = 0.6305422186851501 + 0.001 * 7.428614139556885
Epoch 210, val loss: 0.9233450293540955
Epoch 220, training loss: 0.5853097438812256 = 0.5778861045837402 + 0.001 * 7.4236159324646
Epoch 220, val loss: 0.8994183540344238
Epoch 230, training loss: 0.5365791916847229 = 0.5291612148284912 + 0.001 * 7.4179887771606445
Epoch 230, val loss: 0.8820433020591736
Epoch 240, training loss: 0.4912731647491455 = 0.48386117815971375 + 0.001 * 7.411985397338867
Epoch 240, val loss: 0.8707870244979858
Epoch 250, training loss: 0.44923725724220276 = 0.44183114171028137 + 0.001 * 7.406109809875488
Epoch 250, val loss: 0.8650399446487427
Epoch 260, training loss: 0.4104166030883789 = 0.4030175507068634 + 0.001 * 7.399044036865234
Epoch 260, val loss: 0.8643625974655151
Epoch 270, training loss: 0.374708354473114 = 0.36731576919555664 + 0.001 * 7.392595291137695
Epoch 270, val loss: 0.8684445023536682
Epoch 280, training loss: 0.34181708097457886 = 0.3344392478466034 + 0.001 * 7.377838134765625
Epoch 280, val loss: 0.8766477704048157
Epoch 290, training loss: 0.31129932403564453 = 0.3039405941963196 + 0.001 * 7.358743667602539
Epoch 290, val loss: 0.8878450393676758
Epoch 300, training loss: 0.2825692296028137 = 0.27522972226142883 + 0.001 * 7.339513778686523
Epoch 300, val loss: 0.9011625647544861
Epoch 310, training loss: 0.2550472021102905 = 0.24772246181964874 + 0.001 * 7.32475471496582
Epoch 310, val loss: 0.9159396886825562
Epoch 320, training loss: 0.22842523455619812 = 0.22112298011779785 + 0.001 * 7.3022565841674805
Epoch 320, val loss: 0.9316630959510803
Epoch 330, training loss: 0.20292997360229492 = 0.19564174115657806 + 0.001 * 7.288235664367676
Epoch 330, val loss: 0.9482201337814331
Epoch 340, training loss: 0.17919617891311646 = 0.17191950976848602 + 0.001 * 7.276672840118408
Epoch 340, val loss: 0.9660547375679016
Epoch 350, training loss: 0.1578376591205597 = 0.15055571496486664 + 0.001 * 7.281946659088135
Epoch 350, val loss: 0.9854472875595093
Epoch 360, training loss: 0.13905486464500427 = 0.13177472352981567 + 0.001 * 7.280141830444336
Epoch 360, val loss: 1.0066217184066772
Epoch 370, training loss: 0.1227574571967125 = 0.1154811680316925 + 0.001 * 7.276289939880371
Epoch 370, val loss: 1.0295374393463135
Epoch 380, training loss: 0.1087012067437172 = 0.10142514109611511 + 0.001 * 7.276066780090332
Epoch 380, val loss: 1.0540064573287964
Epoch 390, training loss: 0.09660138934850693 = 0.08932963013648987 + 0.001 * 7.2717604637146
Epoch 390, val loss: 1.079415202140808
Epoch 400, training loss: 0.08619064092636108 = 0.07891836017370224 + 0.001 * 7.272279262542725
Epoch 400, val loss: 1.1053516864776611
Epoch 410, training loss: 0.07721540331840515 = 0.06994569301605225 + 0.001 * 7.269710540771484
Epoch 410, val loss: 1.1313960552215576
Epoch 420, training loss: 0.06947600096464157 = 0.06219767406582832 + 0.001 * 7.278327465057373
Epoch 420, val loss: 1.1572312116622925
Epoch 430, training loss: 0.06276904046535492 = 0.05549301207065582 + 0.001 * 7.276025295257568
Epoch 430, val loss: 1.182607650756836
Epoch 440, training loss: 0.05694552883505821 = 0.04968124255537987 + 0.001 * 7.264284610748291
Epoch 440, val loss: 1.2074334621429443
Epoch 450, training loss: 0.051898013800382614 = 0.04463421180844307 + 0.001 * 7.263801574707031
Epoch 450, val loss: 1.2315857410430908
Epoch 460, training loss: 0.047509610652923584 = 0.04024365544319153 + 0.001 * 7.265953540802002
Epoch 460, val loss: 1.2549598217010498
Epoch 470, training loss: 0.043677836656570435 = 0.03641645610332489 + 0.001 * 7.261379718780518
Epoch 470, val loss: 1.2775696516036987
Epoch 480, training loss: 0.040330179035663605 = 0.03307110443711281 + 0.001 * 7.259075164794922
Epoch 480, val loss: 1.2994264364242554
Epoch 490, training loss: 0.03739557042717934 = 0.030139362439513206 + 0.001 * 7.256208419799805
Epoch 490, val loss: 1.320505976676941
Epoch 500, training loss: 0.034815479069948196 = 0.027562130242586136 + 0.001 * 7.253349304199219
Epoch 500, val loss: 1.3408372402191162
Epoch 510, training loss: 0.0325382836163044 = 0.025289366021752357 + 0.001 * 7.248917102813721
Epoch 510, val loss: 1.3604573011398315
Epoch 520, training loss: 0.03053474798798561 = 0.02327856607735157 + 0.001 * 7.256180763244629
Epoch 520, val loss: 1.3793563842773438
Epoch 530, training loss: 0.028749505057930946 = 0.021493420004844666 + 0.001 * 7.25608491897583
Epoch 530, val loss: 1.3975908756256104
Epoch 540, training loss: 0.02714524418115616 = 0.019903330132365227 + 0.001 * 7.241913318634033
Epoch 540, val loss: 1.4151790142059326
Epoch 550, training loss: 0.02573036402463913 = 0.018482333049178123 + 0.001 * 7.248030185699463
Epoch 550, val loss: 1.4321562051773071
Epoch 560, training loss: 0.02445608377456665 = 0.017208309844136238 + 0.001 * 7.247774124145508
Epoch 560, val loss: 1.4485516548156738
Epoch 570, training loss: 0.023325154557824135 = 0.016062671318650246 + 0.001 * 7.2624831199646
Epoch 570, val loss: 1.464331030845642
Epoch 580, training loss: 0.022261805832386017 = 0.01502937451004982 + 0.001 * 7.232430934906006
Epoch 580, val loss: 1.4796034097671509
Epoch 590, training loss: 0.021322336047887802 = 0.014094620011746883 + 0.001 * 7.227715015411377
Epoch 590, val loss: 1.4943795204162598
Epoch 600, training loss: 0.020480433478951454 = 0.013246522285044193 + 0.001 * 7.23391056060791
Epoch 600, val loss: 1.5086475610733032
Epoch 610, training loss: 0.019695784896612167 = 0.012475088238716125 + 0.001 * 7.220695972442627
Epoch 610, val loss: 1.5224504470825195
Epoch 620, training loss: 0.01898927614092827 = 0.011771544814109802 + 0.001 * 7.2177300453186035
Epoch 620, val loss: 1.53580641746521
Epoch 630, training loss: 0.018338460475206375 = 0.011128239333629608 + 0.001 * 7.210219860076904
Epoch 630, val loss: 1.5487569570541382
Epoch 640, training loss: 0.01775801181793213 = 0.010538559406995773 + 0.001 * 7.219452381134033
Epoch 640, val loss: 1.5613224506378174
Epoch 650, training loss: 0.017221055924892426 = 0.009996788576245308 + 0.001 * 7.224266052246094
Epoch 650, val loss: 1.573500156402588
Epoch 660, training loss: 0.016752930358052254 = 0.009498023428022861 + 0.001 * 7.25490665435791
Epoch 660, val loss: 1.5853127241134644
Epoch 670, training loss: 0.016252342611551285 = 0.009037827141582966 + 0.001 * 7.21451473236084
Epoch 670, val loss: 1.5967507362365723
Epoch 680, training loss: 0.0158197320997715 = 0.008612338453531265 + 0.001 * 7.207393169403076
Epoch 680, val loss: 1.6078747510910034
Epoch 690, training loss: 0.015426816418766975 = 0.008218200877308846 + 0.001 * 7.208614826202393
Epoch 690, val loss: 1.6186704635620117
Epoch 700, training loss: 0.015041209757328033 = 0.007852458395063877 + 0.001 * 7.188751220703125
Epoch 700, val loss: 1.6291733980178833
Epoch 710, training loss: 0.014723717235028744 = 0.007512420415878296 + 0.001 * 7.211296558380127
Epoch 710, val loss: 1.6393717527389526
Epoch 720, training loss: 0.014417717233300209 = 0.007195767015218735 + 0.001 * 7.221950054168701
Epoch 720, val loss: 1.649299144744873
Epoch 730, training loss: 0.014089353382587433 = 0.006900364998728037 + 0.001 * 7.18898868560791
Epoch 730, val loss: 1.6589587926864624
Epoch 740, training loss: 0.013797146268188953 = 0.0066243549808859825 + 0.001 * 7.172791004180908
Epoch 740, val loss: 1.668359398841858
Epoch 750, training loss: 0.013577599078416824 = 0.0063660843297839165 + 0.001 * 7.211514472961426
Epoch 750, val loss: 1.677512288093567
Epoch 760, training loss: 0.013303140178322792 = 0.006124129518866539 + 0.001 * 7.17901086807251
Epoch 760, val loss: 1.6864417791366577
Epoch 770, training loss: 0.013057420030236244 = 0.005897109862416983 + 0.001 * 7.160309791564941
Epoch 770, val loss: 1.6951240301132202
Epoch 780, training loss: 0.012888536788523197 = 0.005683836527168751 + 0.001 * 7.204699993133545
Epoch 780, val loss: 1.7036182880401611
Epoch 790, training loss: 0.012652222067117691 = 0.005483202636241913 + 0.001 * 7.169018745422363
Epoch 790, val loss: 1.7118993997573853
Epoch 800, training loss: 0.01247289776802063 = 0.005294220056384802 + 0.001 * 7.178677082061768
Epoch 800, val loss: 1.719971776008606
Epoch 810, training loss: 0.01233468297868967 = 0.00511598028242588 + 0.001 * 7.21870231628418
Epoch 810, val loss: 1.7278629541397095
Epoch 820, training loss: 0.012151187285780907 = 0.004947719629853964 + 0.001 * 7.203466892242432
Epoch 820, val loss: 1.7355467081069946
Epoch 830, training loss: 0.011964753270149231 = 0.0047887456603348255 + 0.001 * 7.1760077476501465
Epoch 830, val loss: 1.7430435419082642
Epoch 840, training loss: 0.011790175922214985 = 0.004638312850147486 + 0.001 * 7.151862621307373
Epoch 840, val loss: 1.750395655632019
Epoch 850, training loss: 0.011633381247520447 = 0.004495830275118351 + 0.001 * 7.137550354003906
Epoch 850, val loss: 1.757560133934021
Epoch 860, training loss: 0.011491399258375168 = 0.004360750317573547 + 0.001 * 7.130648612976074
Epoch 860, val loss: 1.7645617723464966
Epoch 870, training loss: 0.01141695212572813 = 0.004232544451951981 + 0.001 * 7.1844072341918945
Epoch 870, val loss: 1.7714433670043945
Epoch 880, training loss: 0.011264439672231674 = 0.004110799636691809 + 0.001 * 7.153640270233154
Epoch 880, val loss: 1.778126835823059
Epoch 890, training loss: 0.01121828705072403 = 0.003995093982666731 + 0.001 * 7.2231926918029785
Epoch 890, val loss: 1.784663438796997
Epoch 900, training loss: 0.011035830713808537 = 0.003885010490193963 + 0.001 * 7.150819778442383
Epoch 900, val loss: 1.7910574674606323
Epoch 910, training loss: 0.010933934710919857 = 0.003780170576646924 + 0.001 * 7.153764247894287
Epoch 910, val loss: 1.7973246574401855
Epoch 920, training loss: 0.010854518041014671 = 0.0036802953109145164 + 0.001 * 7.174221992492676
Epoch 920, val loss: 1.8034754991531372
Epoch 930, training loss: 0.010816747322678566 = 0.0035850494168698788 + 0.001 * 7.2316975593566895
Epoch 930, val loss: 1.8094799518585205
Epoch 940, training loss: 0.010711888782680035 = 0.003494150238111615 + 0.001 * 7.217738628387451
Epoch 940, val loss: 1.8153353929519653
Epoch 950, training loss: 0.010540083050727844 = 0.003407331882044673 + 0.001 * 7.132750511169434
Epoch 950, val loss: 1.8210699558258057
Epoch 960, training loss: 0.010428931564092636 = 0.0033243580255657434 + 0.001 * 7.104572772979736
Epoch 960, val loss: 1.8267115354537964
Epoch 970, training loss: 0.010369367897510529 = 0.0032450021244585514 + 0.001 * 7.124365329742432
Epoch 970, val loss: 1.8322330713272095
Epoch 980, training loss: 0.010286157950758934 = 0.0031690727919340134 + 0.001 * 7.1170854568481445
Epoch 980, val loss: 1.8376559019088745
Epoch 990, training loss: 0.010191390290856361 = 0.003096343483775854 + 0.001 * 7.095046520233154
Epoch 990, val loss: 1.842954397201538
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 1.9664642810821533 = 1.9578675031661987 + 0.001 * 8.596819877624512
Epoch 0, val loss: 1.9627841711044312
Epoch 10, training loss: 1.956086277961731 = 1.9474895000457764 + 0.001 * 8.596763610839844
Epoch 10, val loss: 1.9525867700576782
Epoch 20, training loss: 1.9432545900344849 = 1.9346580505371094 + 0.001 * 8.596570014953613
Epoch 20, val loss: 1.9398258924484253
Epoch 30, training loss: 1.9254217147827148 = 1.9168256521224976 + 0.001 * 8.596111297607422
Epoch 30, val loss: 1.9219675064086914
Epoch 40, training loss: 1.8992446660995483 = 1.8906497955322266 + 0.001 * 8.594916343688965
Epoch 40, val loss: 1.8959672451019287
Epoch 50, training loss: 1.8627623319625854 = 1.8541710376739502 + 0.001 * 8.591316223144531
Epoch 50, val loss: 1.8610560894012451
Epoch 60, training loss: 1.8232574462890625 = 1.8146787881851196 + 0.001 * 8.578657150268555
Epoch 60, val loss: 1.8264715671539307
Epoch 70, training loss: 1.7939131259918213 = 1.7853853702545166 + 0.001 * 8.527786254882812
Epoch 70, val loss: 1.800893783569336
Epoch 80, training loss: 1.7601350545883179 = 1.7518720626831055 + 0.001 * 8.262985229492188
Epoch 80, val loss: 1.768270492553711
Epoch 90, training loss: 1.713728904724121 = 1.7056396007537842 + 0.001 * 8.089261054992676
Epoch 90, val loss: 1.7280733585357666
Epoch 100, training loss: 1.6484839916229248 = 1.6405318975448608 + 0.001 * 7.952077388763428
Epoch 100, val loss: 1.6738637685775757
Epoch 110, training loss: 1.5649133920669556 = 1.5571205615997314 + 0.001 * 7.79288911819458
Epoch 110, val loss: 1.6055864095687866
Epoch 120, training loss: 1.4731453657150269 = 1.465497374534607 + 0.001 * 7.648016929626465
Epoch 120, val loss: 1.5319340229034424
Epoch 130, training loss: 1.3821005821228027 = 1.3744879961013794 + 0.001 * 7.61260461807251
Epoch 130, val loss: 1.4602481126785278
Epoch 140, training loss: 1.2922558784484863 = 1.2847200632095337 + 0.001 * 7.535811901092529
Epoch 140, val loss: 1.3913044929504395
Epoch 150, training loss: 1.2022349834442139 = 1.1947838068008423 + 0.001 * 7.451117515563965
Epoch 150, val loss: 1.3226475715637207
Epoch 160, training loss: 1.1134283542633057 = 1.1060301065444946 + 0.001 * 7.398194789886475
Epoch 160, val loss: 1.2559067010879517
Epoch 170, training loss: 1.0289599895477295 = 1.0215768814086914 + 0.001 * 7.383090019226074
Epoch 170, val loss: 1.194008231163025
Epoch 180, training loss: 0.9505873918533325 = 0.9432061910629272 + 0.001 * 7.381183624267578
Epoch 180, val loss: 1.1381837129592896
Epoch 190, training loss: 0.8778066039085388 = 0.8704289197921753 + 0.001 * 7.377668380737305
Epoch 190, val loss: 1.0873069763183594
Epoch 200, training loss: 0.8097667098045349 = 0.8023896813392639 + 0.001 * 7.377015113830566
Epoch 200, val loss: 1.0402305126190186
Epoch 210, training loss: 0.7463601231575012 = 0.7389839887619019 + 0.001 * 7.376142978668213
Epoch 210, val loss: 0.9972694516181946
Epoch 220, training loss: 0.6875346899032593 = 0.6801597476005554 + 0.001 * 7.37492561340332
Epoch 220, val loss: 0.9589195847511292
Epoch 230, training loss: 0.6325166821479797 = 0.625143826007843 + 0.001 * 7.372827529907227
Epoch 230, val loss: 0.9256458282470703
Epoch 240, training loss: 0.5797336101531982 = 0.5723640322685242 + 0.001 * 7.369569778442383
Epoch 240, val loss: 0.8967970609664917
Epoch 250, training loss: 0.5275260806083679 = 0.5201610326766968 + 0.001 * 7.365056991577148
Epoch 250, val loss: 0.8714529275894165
Epoch 260, training loss: 0.47493308782577515 = 0.4675733745098114 + 0.001 * 7.359715938568115
Epoch 260, val loss: 0.8499532341957092
Epoch 270, training loss: 0.4224022626876831 = 0.41504961252212524 + 0.001 * 7.35265588760376
Epoch 270, val loss: 0.8334237933158875
Epoch 280, training loss: 0.37161797285079956 = 0.36427298188209534 + 0.001 * 7.344995975494385
Epoch 280, val loss: 0.8231710195541382
Epoch 290, training loss: 0.3247402310371399 = 0.31740519404411316 + 0.001 * 7.335024356842041
Epoch 290, val loss: 0.8199729919433594
Epoch 300, training loss: 0.28300973773002625 = 0.27568766474723816 + 0.001 * 7.322080135345459
Epoch 300, val loss: 0.8232603669166565
Epoch 310, training loss: 0.2464791238307953 = 0.23917987942695618 + 0.001 * 7.299239635467529
Epoch 310, val loss: 0.831714928150177
Epoch 320, training loss: 0.21474404633045197 = 0.20746397972106934 + 0.001 * 7.280069351196289
Epoch 320, val loss: 0.8441103100776672
Epoch 330, training loss: 0.18730729818344116 = 0.18004673719406128 + 0.001 * 7.260563373565674
Epoch 330, val loss: 0.8594295382499695
Epoch 340, training loss: 0.1636568307876587 = 0.1564154177904129 + 0.001 * 7.241408348083496
Epoch 340, val loss: 0.876667320728302
Epoch 350, training loss: 0.14329379796981812 = 0.13606998324394226 + 0.001 * 7.223821640014648
Epoch 350, val loss: 0.8951965570449829
Epoch 360, training loss: 0.12579195201396942 = 0.11857035011053085 + 0.001 * 7.221595764160156
Epoch 360, val loss: 0.9144927859306335
Epoch 370, training loss: 0.11072350293397903 = 0.10351596027612686 + 0.001 * 7.207540512084961
Epoch 370, val loss: 0.9342074394226074
Epoch 380, training loss: 0.09776569902896881 = 0.09056352078914642 + 0.001 * 7.202178001403809
Epoch 380, val loss: 0.9541860222816467
Epoch 390, training loss: 0.08663053810596466 = 0.07943560183048248 + 0.001 * 7.194939613342285
Epoch 390, val loss: 0.9742462038993835
Epoch 400, training loss: 0.07707514613866806 = 0.06988152116537094 + 0.001 * 7.193623065948486
Epoch 400, val loss: 0.9943141341209412
Epoch 410, training loss: 0.0688808411359787 = 0.06168276071548462 + 0.001 * 7.198083400726318
Epoch 410, val loss: 1.0142995119094849
Epoch 420, training loss: 0.06182944029569626 = 0.05464593693614006 + 0.001 * 7.183502674102783
Epoch 420, val loss: 1.0341318845748901
Epoch 430, training loss: 0.055784016847610474 = 0.04860164597630501 + 0.001 * 7.182370185852051
Epoch 430, val loss: 1.053653359413147
Epoch 440, training loss: 0.05059542506933212 = 0.04340179264545441 + 0.001 * 7.193630218505859
Epoch 440, val loss: 1.072798728942871
Epoch 450, training loss: 0.046103645116090775 = 0.038919396698474884 + 0.001 * 7.184249401092529
Epoch 450, val loss: 1.0914968252182007
Epoch 460, training loss: 0.04223332926630974 = 0.035045918077230453 + 0.001 * 7.187411308288574
Epoch 460, val loss: 1.1096841096878052
Epoch 470, training loss: 0.03886862099170685 = 0.031689610332250595 + 0.001 * 7.17901086807251
Epoch 470, val loss: 1.1273781061172485
Epoch 480, training loss: 0.03593854233622551 = 0.02877139113843441 + 0.001 * 7.167150497436523
Epoch 480, val loss: 1.1445280313491821
Epoch 490, training loss: 0.03338608518242836 = 0.026224687695503235 + 0.001 * 7.161396503448486
Epoch 490, val loss: 1.1611517667770386
Epoch 500, training loss: 0.031177891418337822 = 0.023993918672204018 + 0.001 * 7.183972358703613
Epoch 500, val loss: 1.1772111654281616
Epoch 510, training loss: 0.02918434701859951 = 0.022032635286450386 + 0.001 * 7.151711940765381
Epoch 510, val loss: 1.1927299499511719
Epoch 520, training loss: 0.027461811900138855 = 0.020301617681980133 + 0.001 * 7.160194396972656
Epoch 520, val loss: 1.2076691389083862
Epoch 530, training loss: 0.025922557339072227 = 0.01876791939139366 + 0.001 * 7.154637336730957
Epoch 530, val loss: 1.2220994234085083
Epoch 540, training loss: 0.02456209436058998 = 0.017403842881321907 + 0.001 * 7.1582512855529785
Epoch 540, val loss: 1.236002802848816
Epoch 550, training loss: 0.02333846315741539 = 0.016185974702239037 + 0.001 * 7.152487277984619
Epoch 550, val loss: 1.2494114637374878
Epoch 560, training loss: 0.02227473258972168 = 0.015094658359885216 + 0.001 * 7.1800737380981445
Epoch 560, val loss: 1.2623525857925415
Epoch 570, training loss: 0.021253446117043495 = 0.01411348581314087 + 0.001 * 7.139959335327148
Epoch 570, val loss: 1.2748041152954102
Epoch 580, training loss: 0.020372627303004265 = 0.013228298164904118 + 0.001 * 7.144329071044922
Epoch 580, val loss: 1.2868568897247314
Epoch 590, training loss: 0.019580107182264328 = 0.012427146546542645 + 0.001 * 7.15295934677124
Epoch 590, val loss: 1.2984938621520996
Epoch 600, training loss: 0.018841076642274857 = 0.01169990561902523 + 0.001 * 7.141171455383301
Epoch 600, val loss: 1.3097697496414185
Epoch 610, training loss: 0.018174149096012115 = 0.011037804186344147 + 0.001 * 7.136343479156494
Epoch 610, val loss: 1.3206777572631836
Epoch 620, training loss: 0.017559606581926346 = 0.010433315299451351 + 0.001 * 7.126291751861572
Epoch 620, val loss: 1.3312009572982788
Epoch 630, training loss: 0.017007537186145782 = 0.009879986755549908 + 0.001 * 7.127551078796387
Epoch 630, val loss: 1.341429591178894
Epoch 640, training loss: 0.016512831673026085 = 0.009372281841933727 + 0.001 * 7.140549659729004
Epoch 640, val loss: 1.3513063192367554
Epoch 650, training loss: 0.016044916585087776 = 0.008905294351279736 + 0.001 * 7.139622211456299
Epoch 650, val loss: 1.360905408859253
Epoch 660, training loss: 0.015606105327606201 = 0.008474777452647686 + 0.001 * 7.131328105926514
Epoch 660, val loss: 1.3702099323272705
Epoch 670, training loss: 0.01520224753767252 = 0.008076999336481094 + 0.001 * 7.125247955322266
Epoch 670, val loss: 1.3792285919189453
Epoch 680, training loss: 0.014836975373327732 = 0.007708733901381493 + 0.001 * 7.128241062164307
Epoch 680, val loss: 1.3879892826080322
Epoch 690, training loss: 0.014482267200946808 = 0.00736714992672205 + 0.001 * 7.11511754989624
Epoch 690, val loss: 1.3965181112289429
Epoch 700, training loss: 0.014163792133331299 = 0.0070496248081326485 + 0.001 * 7.114166736602783
Epoch 700, val loss: 1.4047963619232178
Epoch 710, training loss: 0.013882730156183243 = 0.006753683090209961 + 0.001 * 7.129047393798828
Epoch 710, val loss: 1.4128731489181519
Epoch 720, training loss: 0.013588900677859783 = 0.006477296352386475 + 0.001 * 7.1116042137146
Epoch 720, val loss: 1.420770525932312
Epoch 730, training loss: 0.013339923694729805 = 0.006217723246663809 + 0.001 * 7.122200012207031
Epoch 730, val loss: 1.4285913705825806
Epoch 740, training loss: 0.01307993195950985 = 0.005971590057015419 + 0.001 * 7.108342170715332
Epoch 740, val loss: 1.436477541923523
Epoch 750, training loss: 0.01283932849764824 = 0.005735623184591532 + 0.001 * 7.103705406188965
Epoch 750, val loss: 1.4445353746414185
Epoch 760, training loss: 0.01262315921485424 = 0.005508486647158861 + 0.001 * 7.11467170715332
Epoch 760, val loss: 1.4527567625045776
Epoch 770, training loss: 0.012394670397043228 = 0.005290240049362183 + 0.001 * 7.104430198669434
Epoch 770, val loss: 1.4611026048660278
Epoch 780, training loss: 0.012192318215966225 = 0.005081030540168285 + 0.001 * 7.111286640167236
Epoch 780, val loss: 1.4695277214050293
Epoch 790, training loss: 0.01199561357498169 = 0.004881357774138451 + 0.001 * 7.114255428314209
Epoch 790, val loss: 1.4779651165008545
Epoch 800, training loss: 0.011797839775681496 = 0.004691207781434059 + 0.001 * 7.106631278991699
Epoch 800, val loss: 1.486466646194458
Epoch 810, training loss: 0.011620117351412773 = 0.004510845988988876 + 0.001 * 7.10927152633667
Epoch 810, val loss: 1.4948680400848389
Epoch 820, training loss: 0.011445071548223495 = 0.00433979881927371 + 0.001 * 7.10527229309082
Epoch 820, val loss: 1.5032397508621216
Epoch 830, training loss: 0.011292526498436928 = 0.004177829250693321 + 0.001 * 7.114697456359863
Epoch 830, val loss: 1.511532187461853
Epoch 840, training loss: 0.011133356019854546 = 0.004024805501103401 + 0.001 * 7.10854959487915
Epoch 840, val loss: 1.519681692123413
Epoch 850, training loss: 0.01098400168120861 = 0.003880107309669256 + 0.001 * 7.103894233703613
Epoch 850, val loss: 1.5277405977249146
Epoch 860, training loss: 0.010834164917469025 = 0.0037433430552482605 + 0.001 * 7.090821743011475
Epoch 860, val loss: 1.535691499710083
Epoch 870, training loss: 0.010700846090912819 = 0.0036140079610049725 + 0.001 * 7.086837291717529
Epoch 870, val loss: 1.5434881448745728
Epoch 880, training loss: 0.010585097596049309 = 0.003491663606837392 + 0.001 * 7.093433856964111
Epoch 880, val loss: 1.5511770248413086
Epoch 890, training loss: 0.010467318817973137 = 0.0033758278004825115 + 0.001 * 7.091490268707275
Epoch 890, val loss: 1.5587140321731567
Epoch 900, training loss: 0.010347738862037659 = 0.0032662672456353903 + 0.001 * 7.0814714431762695
Epoch 900, val loss: 1.5660666227340698
Epoch 910, training loss: 0.010246317833662033 = 0.0031625174451619387 + 0.001 * 7.083800315856934
Epoch 910, val loss: 1.573341727256775
Epoch 920, training loss: 0.010158770717680454 = 0.003064167220145464 + 0.001 * 7.094603061676025
Epoch 920, val loss: 1.580461859703064
Epoch 930, training loss: 0.010048815980553627 = 0.0029708624351769686 + 0.001 * 7.077953338623047
Epoch 930, val loss: 1.5874086618423462
Epoch 940, training loss: 0.009969753213226795 = 0.00288235186599195 + 0.001 * 7.087401390075684
Epoch 940, val loss: 1.59425950050354
Epoch 950, training loss: 0.00988418236374855 = 0.0027982923202216625 + 0.001 * 7.08588981628418
Epoch 950, val loss: 1.6009670495986938
Epoch 960, training loss: 0.009802867658436298 = 0.002718346891924739 + 0.001 * 7.08452033996582
Epoch 960, val loss: 1.6075063943862915
Epoch 970, training loss: 0.009710093960165977 = 0.002642364939674735 + 0.001 * 7.0677289962768555
Epoch 970, val loss: 1.6139740943908691
Epoch 980, training loss: 0.009638982824981213 = 0.002569987904280424 + 0.001 * 7.068994522094727
Epoch 980, val loss: 1.6203234195709229
Epoch 990, training loss: 0.009583858773112297 = 0.0025009745731949806 + 0.001 * 7.082883834838867
Epoch 990, val loss: 1.6265153884887695
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 1.9334055185317993 = 1.9248087406158447 + 0.001 * 8.596829414367676
Epoch 0, val loss: 1.9123013019561768
Epoch 10, training loss: 1.923844337463379 = 1.9152475595474243 + 0.001 * 8.596785545349121
Epoch 10, val loss: 1.903120756149292
Epoch 20, training loss: 1.9121290445327759 = 1.9035323858261108 + 0.001 * 8.596613883972168
Epoch 20, val loss: 1.891913652420044
Epoch 30, training loss: 1.8956717252731323 = 1.8870755434036255 + 0.001 * 8.596171379089355
Epoch 30, val loss: 1.8765466213226318
Epoch 40, training loss: 1.8720035552978516 = 1.8634085655212402 + 0.001 * 8.595014572143555
Epoch 40, val loss: 1.8551808595657349
Epoch 50, training loss: 1.8415660858154297 = 1.8329745531082153 + 0.001 * 8.59158706665039
Epoch 50, val loss: 1.8297871351242065
Epoch 60, training loss: 1.811279296875 = 1.802700161933899 + 0.001 * 8.579087257385254
Epoch 60, val loss: 1.8074265718460083
Epoch 70, training loss: 1.782074213027954 = 1.7735573053359985 + 0.001 * 8.516937255859375
Epoch 70, val loss: 1.7833716869354248
Epoch 80, training loss: 1.7407853603363037 = 1.732584834098816 + 0.001 * 8.200553894042969
Epoch 80, val loss: 1.7449193000793457
Epoch 90, training loss: 1.6829962730407715 = 1.6748826503753662 + 0.001 * 8.113577842712402
Epoch 90, val loss: 1.6934937238693237
Epoch 100, training loss: 1.6072090864181519 = 1.5991355180740356 + 0.001 * 8.073585510253906
Epoch 100, val loss: 1.6290791034698486
Epoch 110, training loss: 1.5215317010879517 = 1.5134861469268799 + 0.001 * 8.045501708984375
Epoch 110, val loss: 1.5575405359268188
Epoch 120, training loss: 1.4348385334014893 = 1.426848292350769 + 0.001 * 7.990209579467773
Epoch 120, val loss: 1.4870073795318604
Epoch 130, training loss: 1.3495813608169556 = 1.3417448997497559 + 0.001 * 7.836490631103516
Epoch 130, val loss: 1.4198122024536133
Epoch 140, training loss: 1.265511155128479 = 1.2578470706939697 + 0.001 * 7.664142608642578
Epoch 140, val loss: 1.3558964729309082
Epoch 150, training loss: 1.182705283164978 = 1.1751230955123901 + 0.001 * 7.582188129425049
Epoch 150, val loss: 1.294904351234436
Epoch 160, training loss: 1.0991119146347046 = 1.0916484594345093 + 0.001 * 7.463493347167969
Epoch 160, val loss: 1.2346875667572021
Epoch 170, training loss: 1.0122965574264526 = 1.0048741102218628 + 0.001 * 7.4224371910095215
Epoch 170, val loss: 1.1717939376831055
Epoch 180, training loss: 0.922519862651825 = 0.9151021838188171 + 0.001 * 7.417688846588135
Epoch 180, val loss: 1.1058896780014038
Epoch 190, training loss: 0.8336369395256042 = 0.8262277245521545 + 0.001 * 7.409224033355713
Epoch 190, val loss: 1.0414531230926514
Epoch 200, training loss: 0.7502040266990662 = 0.7427989840507507 + 0.001 * 7.405040264129639
Epoch 200, val loss: 0.983447253704071
Epoch 210, training loss: 0.6745198965072632 = 0.6671188473701477 + 0.001 * 7.401035308837891
Epoch 210, val loss: 0.9350810647010803
Epoch 220, training loss: 0.6063705682754517 = 0.5989725589752197 + 0.001 * 7.397998809814453
Epoch 220, val loss: 0.8966584801673889
Epoch 230, training loss: 0.5445600152015686 = 0.5371644496917725 + 0.001 * 7.395576000213623
Epoch 230, val loss: 0.8666441440582275
Epoch 240, training loss: 0.48811808228492737 = 0.4807244539260864 + 0.001 * 7.3936309814453125
Epoch 240, val loss: 0.8432148694992065
Epoch 250, training loss: 0.4365469217300415 = 0.4291553795337677 + 0.001 * 7.391554355621338
Epoch 250, val loss: 0.8254250288009644
Epoch 260, training loss: 0.3897385895252228 = 0.382350355386734 + 0.001 * 7.388237476348877
Epoch 260, val loss: 0.8125052452087402
Epoch 270, training loss: 0.3476437032222748 = 0.34026235342025757 + 0.001 * 7.381335258483887
Epoch 270, val loss: 0.8041496276855469
Epoch 280, training loss: 0.3100152909755707 = 0.30264779925346375 + 0.001 * 7.367498397827148
Epoch 280, val loss: 0.7998735308647156
Epoch 290, training loss: 0.2763825058937073 = 0.2690369486808777 + 0.001 * 7.345547199249268
Epoch 290, val loss: 0.7990426421165466
Epoch 300, training loss: 0.24614550173282623 = 0.23882585763931274 + 0.001 * 7.319645404815674
Epoch 300, val loss: 0.8008481860160828
Epoch 310, training loss: 0.2187616527080536 = 0.21145552396774292 + 0.001 * 7.306130409240723
Epoch 310, val loss: 0.8048901557922363
Epoch 320, training loss: 0.19382575154304504 = 0.1865403950214386 + 0.001 * 7.285348892211914
Epoch 320, val loss: 0.8109000325202942
Epoch 330, training loss: 0.17117531597614288 = 0.1638951152563095 + 0.001 * 7.280197620391846
Epoch 330, val loss: 0.8187265992164612
Epoch 340, training loss: 0.1507623940706253 = 0.14348101615905762 + 0.001 * 7.281375408172607
Epoch 340, val loss: 0.8283457159996033
Epoch 350, training loss: 0.13257136940956116 = 0.12529069185256958 + 0.001 * 7.280675888061523
Epoch 350, val loss: 0.8396636247634888
Epoch 360, training loss: 0.11654900014400482 = 0.10926739126443863 + 0.001 * 7.281610488891602
Epoch 360, val loss: 0.8525334596633911
Epoch 370, training loss: 0.10256170481443405 = 0.09527891129255295 + 0.001 * 7.282794952392578
Epoch 370, val loss: 0.8666763305664062
Epoch 380, training loss: 0.09042453020811081 = 0.08314068615436554 + 0.001 * 7.283846855163574
Epoch 380, val loss: 0.881744921207428
Epoch 390, training loss: 0.0799373984336853 = 0.07265259325504303 + 0.001 * 7.284801483154297
Epoch 390, val loss: 0.8974366784095764
Epoch 400, training loss: 0.07090780138969421 = 0.06362222880125046 + 0.001 * 7.285574436187744
Epoch 400, val loss: 0.9136475324630737
Epoch 410, training loss: 0.06315910071134567 = 0.05587281659245491 + 0.001 * 7.286282539367676
Epoch 410, val loss: 0.930081844329834
Epoch 420, training loss: 0.056529197841882706 = 0.04923935607075691 + 0.001 * 7.28984260559082
Epoch 420, val loss: 0.9464943408966064
Epoch 430, training loss: 0.05085963383316994 = 0.04357162117958069 + 0.001 * 7.288013458251953
Epoch 430, val loss: 0.9627711772918701
Epoch 440, training loss: 0.0460185781121254 = 0.03873075917363167 + 0.001 * 7.287820339202881
Epoch 440, val loss: 0.9786289930343628
Epoch 450, training loss: 0.04187937080860138 = 0.034591466188430786 + 0.001 * 7.287905693054199
Epoch 450, val loss: 0.9940304160118103
Epoch 460, training loss: 0.038327544927597046 = 0.031039848923683167 + 0.001 * 7.287696361541748
Epoch 460, val loss: 1.0089287757873535
Epoch 470, training loss: 0.035268187522888184 = 0.02797660231590271 + 0.001 * 7.291585445404053
Epoch 470, val loss: 1.0232832431793213
Epoch 480, training loss: 0.03260979428887367 = 0.02532256953418255 + 0.001 * 7.287224769592285
Epoch 480, val loss: 1.0371484756469727
Epoch 490, training loss: 0.03030066192150116 = 0.02301381155848503 + 0.001 * 7.286850452423096
Epoch 490, val loss: 1.0504990816116333
Epoch 500, training loss: 0.028284825384616852 = 0.02099796198308468 + 0.001 * 7.286861896514893
Epoch 500, val loss: 1.063324213027954
Epoch 510, training loss: 0.026518452912569046 = 0.019231485202908516 + 0.001 * 7.286967754364014
Epoch 510, val loss: 1.0756410360336304
Epoch 520, training loss: 0.02496417984366417 = 0.017678020521998405 + 0.001 * 7.286159515380859
Epoch 520, val loss: 1.0874823331832886
Epoch 530, training loss: 0.023590661585330963 = 0.016306644305586815 + 0.001 * 7.284018039703369
Epoch 530, val loss: 1.0988390445709229
Epoch 540, training loss: 0.022379428148269653 = 0.015091349370777607 + 0.001 * 7.288077354431152
Epoch 540, val loss: 1.109757423400879
Epoch 550, training loss: 0.021293222904205322 = 0.014010440558195114 + 0.001 * 7.28278112411499
Epoch 550, val loss: 1.120238184928894
Epoch 560, training loss: 0.020326390862464905 = 0.013045188039541245 + 0.001 * 7.281201362609863
Epoch 560, val loss: 1.13033127784729
Epoch 570, training loss: 0.01946175843477249 = 0.012179354205727577 + 0.001 * 7.28240442276001
Epoch 570, val loss: 1.1400312185287476
Epoch 580, training loss: 0.018677901476621628 = 0.011399047449231148 + 0.001 * 7.2788543701171875
Epoch 580, val loss: 1.149397373199463
Epoch 590, training loss: 0.01796763762831688 = 0.010691811330616474 + 0.001 * 7.2758259773254395
Epoch 590, val loss: 1.158473253250122
Epoch 600, training loss: 0.01732981763780117 = 0.010046934708952904 + 0.001 * 7.2828826904296875
Epoch 600, val loss: 1.1672357320785522
Epoch 610, training loss: 0.016732851043343544 = 0.00945576373487711 + 0.001 * 7.2770867347717285
Epoch 610, val loss: 1.1757760047912598
Epoch 620, training loss: 0.01618824526667595 = 0.00891109462827444 + 0.001 * 7.277149677276611
Epoch 620, val loss: 1.184099793434143
Epoch 630, training loss: 0.015678711235523224 = 0.00840822421014309 + 0.001 * 7.270487308502197
Epoch 630, val loss: 1.1922296285629272
Epoch 640, training loss: 0.015209714882075787 = 0.0079434709623456 + 0.001 * 7.2662434577941895
Epoch 640, val loss: 1.2001454830169678
Epoch 650, training loss: 0.014802773483097553 = 0.007513790391385555 + 0.001 * 7.28898286819458
Epoch 650, val loss: 1.2078698873519897
Epoch 660, training loss: 0.014384923502802849 = 0.007116754073649645 + 0.001 * 7.2681684494018555
Epoch 660, val loss: 1.2153788805007935
Epoch 670, training loss: 0.014009936712682247 = 0.006749475374817848 + 0.001 * 7.26046085357666
Epoch 670, val loss: 1.2227368354797363
Epoch 680, training loss: 0.013664709404110909 = 0.0064095319248735905 + 0.001 * 7.255177021026611
Epoch 680, val loss: 1.2299031019210815
Epoch 690, training loss: 0.013356508687138557 = 0.006094760727137327 + 0.001 * 7.261747360229492
Epoch 690, val loss: 1.2368839979171753
Epoch 700, training loss: 0.013099553994834423 = 0.00580302020534873 + 0.001 * 7.296533584594727
Epoch 700, val loss: 1.2437164783477783
Epoch 710, training loss: 0.012777561321854591 = 0.0055318837985396385 + 0.001 * 7.245676517486572
Epoch 710, val loss: 1.2503366470336914
Epoch 720, training loss: 0.012520711869001389 = 0.005278957542032003 + 0.001 * 7.241754531860352
Epoch 720, val loss: 1.2568283081054688
Epoch 730, training loss: 0.012296192348003387 = 0.005041874013841152 + 0.001 * 7.254317760467529
Epoch 730, val loss: 1.2632356882095337
Epoch 740, training loss: 0.012054729275405407 = 0.004818145651370287 + 0.001 * 7.236583232879639
Epoch 740, val loss: 1.2695837020874023
Epoch 750, training loss: 0.01185738667845726 = 0.00460593868046999 + 0.001 * 7.251448154449463
Epoch 750, val loss: 1.2758474349975586
Epoch 760, training loss: 0.011645813472568989 = 0.004404797684401274 + 0.001 * 7.241015434265137
Epoch 760, val loss: 1.2820680141448975
Epoch 770, training loss: 0.011441841721534729 = 0.004213985521346331 + 0.001 * 7.227856159210205
Epoch 770, val loss: 1.2882273197174072
Epoch 780, training loss: 0.011308731511235237 = 0.0040333326905965805 + 0.001 * 7.275397777557373
Epoch 780, val loss: 1.2943404912948608
Epoch 790, training loss: 0.011104173958301544 = 0.0038630675990134478 + 0.001 * 7.241105556488037
Epoch 790, val loss: 1.3003637790679932
Epoch 800, training loss: 0.01090981625020504 = 0.00370244518853724 + 0.001 * 7.207370758056641
Epoch 800, val loss: 1.3063334226608276
Epoch 810, training loss: 0.010786744765937328 = 0.0035510980524122715 + 0.001 * 7.2356462478637695
Epoch 810, val loss: 1.3122570514678955
Epoch 820, training loss: 0.01061607152223587 = 0.003408924676477909 + 0.001 * 7.207146644592285
Epoch 820, val loss: 1.317999005317688
Epoch 830, training loss: 0.010506545193493366 = 0.0032753790728747845 + 0.001 * 7.231165885925293
Epoch 830, val loss: 1.3236819505691528
Epoch 840, training loss: 0.010353455320000648 = 0.0031499566975980997 + 0.001 * 7.203498363494873
Epoch 840, val loss: 1.3292046785354614
Epoch 850, training loss: 0.01025092601776123 = 0.0030321222729980946 + 0.001 * 7.218803405761719
Epoch 850, val loss: 1.3346362113952637
Epoch 860, training loss: 0.01011037826538086 = 0.002921456005424261 + 0.001 * 7.1889214515686035
Epoch 860, val loss: 1.3399163484573364
Epoch 870, training loss: 0.009994467720389366 = 0.002817362081259489 + 0.001 * 7.17710542678833
Epoch 870, val loss: 1.3451030254364014
Epoch 880, training loss: 0.009888610802590847 = 0.002719470765441656 + 0.001 * 7.169139862060547
Epoch 880, val loss: 1.35015070438385
Epoch 890, training loss: 0.009799518622457981 = 0.0026272872928529978 + 0.001 * 7.172231197357178
Epoch 890, val loss: 1.35512375831604
Epoch 900, training loss: 0.009693249128758907 = 0.00254043354652822 + 0.001 * 7.152815341949463
Epoch 900, val loss: 1.3599408864974976
Epoch 910, training loss: 0.009644204750657082 = 0.0024585211649537086 + 0.001 * 7.185683727264404
Epoch 910, val loss: 1.3646528720855713
Epoch 920, training loss: 0.009520135819911957 = 0.002381270984187722 + 0.001 * 7.138864517211914
Epoch 920, val loss: 1.369176983833313
Epoch 930, training loss: 0.009481898508965969 = 0.002308328403159976 + 0.001 * 7.173569679260254
Epoch 930, val loss: 1.3736817836761475
Epoch 940, training loss: 0.009379882365465164 = 0.002239558147266507 + 0.001 * 7.140324115753174
Epoch 940, val loss: 1.3779661655426025
Epoch 950, training loss: 0.009346754290163517 = 0.0021744875703006983 + 0.001 * 7.172266006469727
Epoch 950, val loss: 1.3821628093719482
Epoch 960, training loss: 0.009270921349525452 = 0.0021129969973117113 + 0.001 * 7.157923698425293
Epoch 960, val loss: 1.3862813711166382
Epoch 970, training loss: 0.009263814426958561 = 0.002054601674899459 + 0.001 * 7.209212303161621
Epoch 970, val loss: 1.3902689218521118
Epoch 980, training loss: 0.00911509059369564 = 0.001999232219532132 + 0.001 * 7.1158576011657715
Epoch 980, val loss: 1.3941242694854736
Epoch 990, training loss: 0.009043844416737556 = 0.001946716569364071 + 0.001 * 7.097127437591553
Epoch 990, val loss: 1.3979065418243408
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8070637849235636
The final CL Acc:0.75062, 0.01666, The final GNN Acc:0.81093, 0.00277
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13182])
remove edge: torch.Size([2, 7922])
updated graph: torch.Size([2, 10548])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9471986293792725 = 1.9386017322540283 + 0.001 * 8.596851348876953
Epoch 0, val loss: 1.9390203952789307
Epoch 10, training loss: 1.9381170272827148 = 1.9295202493667603 + 0.001 * 8.596793174743652
Epoch 10, val loss: 1.9302088022232056
Epoch 20, training loss: 1.9272524118423462 = 1.9186558723449707 + 0.001 * 8.596566200256348
Epoch 20, val loss: 1.9192782640457153
Epoch 30, training loss: 1.912430763244629 = 1.9038348197937012 + 0.001 * 8.595976829528809
Epoch 30, val loss: 1.9042824506759644
Epoch 40, training loss: 1.8909235000610352 = 1.882328987121582 + 0.001 * 8.594454765319824
Epoch 40, val loss: 1.882603645324707
Epoch 50, training loss: 1.8599923849105835 = 1.8514022827148438 + 0.001 * 8.590078353881836
Epoch 50, val loss: 1.8521913290023804
Epoch 60, training loss: 1.8201121091842651 = 1.811537504196167 + 0.001 * 8.57459545135498
Epoch 60, val loss: 1.8148528337478638
Epoch 70, training loss: 1.7783812284469604 = 1.7698699235916138 + 0.001 * 8.511298179626465
Epoch 70, val loss: 1.7777069807052612
Epoch 80, training loss: 1.733731985092163 = 1.725523829460144 + 0.001 * 8.208159446716309
Epoch 80, val loss: 1.737005591392517
Epoch 90, training loss: 1.6711375713348389 = 1.6630988121032715 + 0.001 * 8.038787841796875
Epoch 90, val loss: 1.6801354885101318
Epoch 100, training loss: 1.5879080295562744 = 1.5800195932388306 + 0.001 * 7.888455867767334
Epoch 100, val loss: 1.6087827682495117
Epoch 110, training loss: 1.486159086227417 = 1.4784139394760132 + 0.001 * 7.745199203491211
Epoch 110, val loss: 1.522735357284546
Epoch 120, training loss: 1.3757117986679077 = 1.3680261373519897 + 0.001 * 7.6856369972229
Epoch 120, val loss: 1.4285504817962646
Epoch 130, training loss: 1.2661622762680054 = 1.2585006952285767 + 0.001 * 7.661558151245117
Epoch 130, val loss: 1.3363128900527954
Epoch 140, training loss: 1.1634528636932373 = 1.1558293104171753 + 0.001 * 7.623561859130859
Epoch 140, val loss: 1.251914381980896
Epoch 150, training loss: 1.0703881978988647 = 1.0628063678741455 + 0.001 * 7.58183479309082
Epoch 150, val loss: 1.17718505859375
Epoch 160, training loss: 0.9864071607589722 = 0.9788858890533447 + 0.001 * 7.521279335021973
Epoch 160, val loss: 1.1121774911880493
Epoch 170, training loss: 0.9083876609802246 = 0.9009515643119812 + 0.001 * 7.436102867126465
Epoch 170, val loss: 1.0528295040130615
Epoch 180, training loss: 0.8337341547012329 = 0.8263524770736694 + 0.001 * 7.381651878356934
Epoch 180, val loss: 0.9961485862731934
Epoch 190, training loss: 0.7615048289299011 = 0.7541553378105164 + 0.001 * 7.349497318267822
Epoch 190, val loss: 0.9411659240722656
Epoch 200, training loss: 0.6925411224365234 = 0.6852272748947144 + 0.001 * 7.313831329345703
Epoch 200, val loss: 0.8889795541763306
Epoch 210, training loss: 0.6282422542572021 = 0.6209551095962524 + 0.001 * 7.287152290344238
Epoch 210, val loss: 0.8417748808860779
Epoch 220, training loss: 0.5690799355506897 = 0.5618188381195068 + 0.001 * 7.261082649230957
Epoch 220, val loss: 0.8011283874511719
Epoch 230, training loss: 0.5145757794380188 = 0.5073266625404358 + 0.001 * 7.249101161956787
Epoch 230, val loss: 0.767618715763092
Epoch 240, training loss: 0.46407264471054077 = 0.4568292796611786 + 0.001 * 7.243372917175293
Epoch 240, val loss: 0.7405880689620972
Epoch 250, training loss: 0.41707614064216614 = 0.409835547208786 + 0.001 * 7.240588188171387
Epoch 250, val loss: 0.7186428308486938
Epoch 260, training loss: 0.3732571601867676 = 0.36601823568344116 + 0.001 * 7.238913059234619
Epoch 260, val loss: 0.7005761861801147
Epoch 270, training loss: 0.33243051171302795 = 0.3251924216747284 + 0.001 * 7.238081932067871
Epoch 270, val loss: 0.685917854309082
Epoch 280, training loss: 0.29458197951316833 = 0.28734448552131653 + 0.001 * 7.237500190734863
Epoch 280, val loss: 0.6744776964187622
Epoch 290, training loss: 0.2598976492881775 = 0.25266146659851074 + 0.001 * 7.236174583435059
Epoch 290, val loss: 0.6660296320915222
Epoch 300, training loss: 0.22871026396751404 = 0.22147493064403534 + 0.001 * 7.235329627990723
Epoch 300, val loss: 0.6604103446006775
Epoch 310, training loss: 0.20124103128910065 = 0.19400709867477417 + 0.001 * 7.2339277267456055
Epoch 310, val loss: 0.6576009392738342
Epoch 320, training loss: 0.17745329439640045 = 0.170220285654068 + 0.001 * 7.2330145835876465
Epoch 320, val loss: 0.6574150919914246
Epoch 330, training loss: 0.15705518424510956 = 0.14982354640960693 + 0.001 * 7.231631755828857
Epoch 330, val loss: 0.6596221327781677
Epoch 340, training loss: 0.1396227777004242 = 0.1323898881673813 + 0.001 * 7.232882022857666
Epoch 340, val loss: 0.6637890338897705
Epoch 350, training loss: 0.12468516081571579 = 0.11745453625917435 + 0.001 * 7.230623722076416
Epoch 350, val loss: 0.6695772409439087
Epoch 360, training loss: 0.1118064597249031 = 0.10457915812730789 + 0.001 * 7.227297782897949
Epoch 360, val loss: 0.676616370677948
Epoch 370, training loss: 0.10062412172555923 = 0.09340264648199081 + 0.001 * 7.221476078033447
Epoch 370, val loss: 0.6846373081207275
Epoch 380, training loss: 0.09086309373378754 = 0.08364616334438324 + 0.001 * 7.21693229675293
Epoch 380, val loss: 0.6934385895729065
Epoch 390, training loss: 0.0823068916797638 = 0.07508987188339233 + 0.001 * 7.217017650604248
Epoch 390, val loss: 0.702803373336792
Epoch 400, training loss: 0.07476864755153656 = 0.06755801290273666 + 0.001 * 7.210637092590332
Epoch 400, val loss: 0.7126232981681824
Epoch 410, training loss: 0.06812820583581924 = 0.06090937554836273 + 0.001 * 7.218830585479736
Epoch 410, val loss: 0.7227919697761536
Epoch 420, training loss: 0.062227871268987656 = 0.05502789095044136 + 0.001 * 7.19998025894165
Epoch 420, val loss: 0.7332210540771484
Epoch 430, training loss: 0.057005174458026886 = 0.04981275647878647 + 0.001 * 7.192418575286865
Epoch 430, val loss: 0.7438136339187622
Epoch 440, training loss: 0.052362941205501556 = 0.045176275074481964 + 0.001 * 7.1866655349731445
Epoch 440, val loss: 0.7545143365859985
Epoch 450, training loss: 0.04823366552591324 = 0.041046902537345886 + 0.001 * 7.186761379241943
Epoch 450, val loss: 0.7652219533920288
Epoch 460, training loss: 0.044564083218574524 = 0.03736709803342819 + 0.001 * 7.1969828605651855
Epoch 460, val loss: 0.7759374380111694
Epoch 470, training loss: 0.041268713772296906 = 0.03408731892704964 + 0.001 * 7.181394577026367
Epoch 470, val loss: 0.7865725755691528
Epoch 480, training loss: 0.03834763169288635 = 0.031164418905973434 + 0.001 * 7.183212757110596
Epoch 480, val loss: 0.7970685958862305
Epoch 490, training loss: 0.03571872040629387 = 0.028557663783431053 + 0.001 * 7.161057472229004
Epoch 490, val loss: 0.8073691725730896
Epoch 500, training loss: 0.03338854759931564 = 0.026230722665786743 + 0.001 * 7.157824993133545
Epoch 500, val loss: 0.8174998164176941
Epoch 510, training loss: 0.03131522610783577 = 0.024151243269443512 + 0.001 * 7.163980960845947
Epoch 510, val loss: 0.8274657130241394
Epoch 520, training loss: 0.02945123426616192 = 0.022290373221039772 + 0.001 * 7.160860538482666
Epoch 520, val loss: 0.8372287750244141
Epoch 530, training loss: 0.027766386047005653 = 0.0206223763525486 + 0.001 * 7.144009113311768
Epoch 530, val loss: 0.8467868566513062
Epoch 540, training loss: 0.026268819347023964 = 0.019124360755085945 + 0.001 * 7.144458770751953
Epoch 540, val loss: 0.8561173677444458
Epoch 550, training loss: 0.024933189153671265 = 0.01777603104710579 + 0.001 * 7.157156944274902
Epoch 550, val loss: 0.8652145266532898
Epoch 560, training loss: 0.0237017460167408 = 0.01655994914472103 + 0.001 * 7.1417975425720215
Epoch 560, val loss: 0.8741139769554138
Epoch 570, training loss: 0.02259158343076706 = 0.015460683964192867 + 0.001 * 7.1308979988098145
Epoch 570, val loss: 0.8827678561210632
Epoch 580, training loss: 0.021591154858469963 = 0.014464851468801498 + 0.001 * 7.126303672790527
Epoch 580, val loss: 0.8912137150764465
Epoch 590, training loss: 0.020688457414507866 = 0.013560918159782887 + 0.001 * 7.127538681030273
Epoch 590, val loss: 0.8994563817977905
Epoch 600, training loss: 0.01986468955874443 = 0.012738537043333054 + 0.001 * 7.126152992248535
Epoch 600, val loss: 0.9074736833572388
Epoch 610, training loss: 0.019109804183244705 = 0.011988713406026363 + 0.001 * 7.121091365814209
Epoch 610, val loss: 0.9153032898902893
Epoch 620, training loss: 0.01842488907277584 = 0.011303454637527466 + 0.001 * 7.121434211730957
Epoch 620, val loss: 0.9229345917701721
Epoch 630, training loss: 0.017797403037548065 = 0.01067590992897749 + 0.001 * 7.121492385864258
Epoch 630, val loss: 0.9303767085075378
Epoch 640, training loss: 0.017220770940184593 = 0.01010003499686718 + 0.001 * 7.1207356452941895
Epoch 640, val loss: 0.9376166462898254
Epoch 650, training loss: 0.01668360084295273 = 0.009570514783263206 + 0.001 * 7.113086223602295
Epoch 650, val loss: 0.9446779489517212
Epoch 660, training loss: 0.016188617795705795 = 0.00908267218619585 + 0.001 * 7.105946063995361
Epoch 660, val loss: 0.9515611529350281
Epoch 670, training loss: 0.015754463151097298 = 0.008632384240627289 + 0.001 * 7.122078895568848
Epoch 670, val loss: 0.9582530856132507
Epoch 680, training loss: 0.015323919244110584 = 0.00821596197783947 + 0.001 * 7.107956886291504
Epoch 680, val loss: 0.9647843837738037
Epoch 690, training loss: 0.014929162338376045 = 0.007830201648175716 + 0.001 * 7.098960876464844
Epoch 690, val loss: 0.9711470007896423
Epoch 700, training loss: 0.014593668282032013 = 0.007472202181816101 + 0.001 * 7.121465682983398
Epoch 700, val loss: 0.9773467779159546
Epoch 710, training loss: 0.014241183176636696 = 0.007139482069760561 + 0.001 * 7.101700782775879
Epoch 710, val loss: 0.9834022521972656
Epoch 720, training loss: 0.013915155082941055 = 0.00682973163202405 + 0.001 * 7.085422515869141
Epoch 720, val loss: 0.9893093705177307
Epoch 730, training loss: 0.013632016256451607 = 0.00654089218005538 + 0.001 * 7.091123104095459
Epoch 730, val loss: 0.9950706362724304
Epoch 740, training loss: 0.01335464883595705 = 0.006271157879382372 + 0.001 * 7.08349084854126
Epoch 740, val loss: 1.000704050064087
Epoch 750, training loss: 0.013111380860209465 = 0.006018915679305792 + 0.001 * 7.092465400695801
Epoch 750, val loss: 1.0061945915222168
Epoch 760, training loss: 0.012864034622907639 = 0.005782673135399818 + 0.001 * 7.081361770629883
Epoch 760, val loss: 1.0115665197372437
Epoch 770, training loss: 0.012627750635147095 = 0.0055611347779631615 + 0.001 * 7.066615104675293
Epoch 770, val loss: 1.016805648803711
Epoch 780, training loss: 0.012431785464286804 = 0.005353111308068037 + 0.001 * 7.07867431640625
Epoch 780, val loss: 1.0219297409057617
Epoch 790, training loss: 0.01225627213716507 = 0.005157539155334234 + 0.001 * 7.098731994628906
Epoch 790, val loss: 1.0269262790679932
Epoch 800, training loss: 0.012073487043380737 = 0.004973488859832287 + 0.001 * 7.0999979972839355
Epoch 800, val loss: 1.0318149328231812
Epoch 810, training loss: 0.011881133541464806 = 0.0048000323586165905 + 0.001 * 7.081100940704346
Epoch 810, val loss: 1.0365917682647705
Epoch 820, training loss: 0.01170684676617384 = 0.0046364255249500275 + 0.001 * 7.070420742034912
Epoch 820, val loss: 1.0412654876708984
Epoch 830, training loss: 0.011534217745065689 = 0.004481873475015163 + 0.001 * 7.052343368530273
Epoch 830, val loss: 1.0458449125289917
Epoch 840, training loss: 0.01138781663030386 = 0.004335787612944841 + 0.001 * 7.052028656005859
Epoch 840, val loss: 1.050320029258728
Epoch 850, training loss: 0.011252827011048794 = 0.0041975174099206924 + 0.001 * 7.055309295654297
Epoch 850, val loss: 1.0546855926513672
Epoch 860, training loss: 0.01111796498298645 = 0.004066576715558767 + 0.001 * 7.051388263702393
Epoch 860, val loss: 1.0589770078659058
Epoch 870, training loss: 0.01098704244941473 = 0.003942409995943308 + 0.001 * 7.0446319580078125
Epoch 870, val loss: 1.0631695985794067
Epoch 880, training loss: 0.010869821533560753 = 0.0038245434407144785 + 0.001 * 7.045278072357178
Epoch 880, val loss: 1.0672634840011597
Epoch 890, training loss: 0.010765481740236282 = 0.003712614066898823 + 0.001 * 7.052867889404297
Epoch 890, val loss: 1.0712754726409912
Epoch 900, training loss: 0.010661391541361809 = 0.0036061827559024096 + 0.001 * 7.055208206176758
Epoch 900, val loss: 1.0751968622207642
Epoch 910, training loss: 0.010577306151390076 = 0.00350492587313056 + 0.001 * 7.0723795890808105
Epoch 910, val loss: 1.0790367126464844
Epoch 920, training loss: 0.010469034314155579 = 0.0034085263032466173 + 0.001 * 7.060507297515869
Epoch 920, val loss: 1.0827974081039429
Epoch 930, training loss: 0.010376574471592903 = 0.0033166473731398582 + 0.001 * 7.059926986694336
Epoch 930, val loss: 1.0864999294281006
Epoch 940, training loss: 0.010267172008752823 = 0.003229029243811965 + 0.001 * 7.038142204284668
Epoch 940, val loss: 1.0901049375534058
Epoch 950, training loss: 0.010180376470088959 = 0.0031453927513211966 + 0.001 * 7.034983158111572
Epoch 950, val loss: 1.093659520149231
Epoch 960, training loss: 0.010092643089592457 = 0.0030655311420559883 + 0.001 * 7.027111530303955
Epoch 960, val loss: 1.0971301794052124
Epoch 970, training loss: 0.01001991517841816 = 0.00298920925706625 + 0.001 * 7.03070592880249
Epoch 970, val loss: 1.1005421876907349
Epoch 980, training loss: 0.009936020709574223 = 0.002916232915595174 + 0.001 * 7.019787788391113
Epoch 980, val loss: 1.103872537612915
Epoch 990, training loss: 0.009857174009084702 = 0.0028464028146117926 + 0.001 * 7.010770797729492
Epoch 990, val loss: 1.107144832611084
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.9585665464401245 = 1.9499696493148804 + 0.001 * 8.596839904785156
Epoch 0, val loss: 1.9501456022262573
Epoch 10, training loss: 1.9481267929077148 = 1.9395300149917603 + 0.001 * 8.596787452697754
Epoch 10, val loss: 1.9393572807312012
Epoch 20, training loss: 1.9357171058654785 = 1.9271204471588135 + 0.001 * 8.596601486206055
Epoch 20, val loss: 1.926336646080017
Epoch 30, training loss: 1.9188650846481323 = 1.9102689027786255 + 0.001 * 8.596186637878418
Epoch 30, val loss: 1.9086573123931885
Epoch 40, training loss: 1.894357681274414 = 1.8857624530792236 + 0.001 * 8.595211029052734
Epoch 40, val loss: 1.883253812789917
Epoch 50, training loss: 1.8592987060546875 = 1.8507063388824463 + 0.001 * 8.59242057800293
Epoch 50, val loss: 1.8480932712554932
Epoch 60, training loss: 1.8162343502044678 = 1.8076525926589966 + 0.001 * 8.581786155700684
Epoch 60, val loss: 1.8076753616333008
Epoch 70, training loss: 1.7755707502365112 = 1.7670445442199707 + 0.001 * 8.526226997375488
Epoch 70, val loss: 1.7721798419952393
Epoch 80, training loss: 1.7317687273025513 = 1.7236053943634033 + 0.001 * 8.163376808166504
Epoch 80, val loss: 1.7312251329421997
Epoch 90, training loss: 1.6705925464630127 = 1.6626691818237305 + 0.001 * 7.923318862915039
Epoch 90, val loss: 1.6737090349197388
Epoch 100, training loss: 1.5886955261230469 = 1.5810036659240723 + 0.001 * 7.691835880279541
Epoch 100, val loss: 1.6001439094543457
Epoch 110, training loss: 1.4872992038726807 = 1.4798377752304077 + 0.001 * 7.4614033699035645
Epoch 110, val loss: 1.5113798379898071
Epoch 120, training loss: 1.375928282737732 = 1.3685613870620728 + 0.001 * 7.366842746734619
Epoch 120, val loss: 1.4160840511322021
Epoch 130, training loss: 1.2643170356750488 = 1.256970763206482 + 0.001 * 7.346228122711182
Epoch 130, val loss: 1.321565866470337
Epoch 140, training loss: 1.1563223600387573 = 1.148984432220459 + 0.001 * 7.337914943695068
Epoch 140, val loss: 1.231123447418213
Epoch 150, training loss: 1.0515484809875488 = 1.0442204475402832 + 0.001 * 7.327989101409912
Epoch 150, val loss: 1.1446588039398193
Epoch 160, training loss: 0.9507315754890442 = 0.9434107542037964 + 0.001 * 7.320831775665283
Epoch 160, val loss: 1.0627130270004272
Epoch 170, training loss: 0.8566868305206299 = 0.8493724465370178 + 0.001 * 7.31435489654541
Epoch 170, val loss: 0.9884914755821228
Epoch 180, training loss: 0.7724292874336243 = 0.7651209235191345 + 0.001 * 7.308352470397949
Epoch 180, val loss: 0.9242843985557556
Epoch 190, training loss: 0.6985681056976318 = 0.6912657618522644 + 0.001 * 7.3023481369018555
Epoch 190, val loss: 0.8708773851394653
Epoch 200, training loss: 0.6338431239128113 = 0.6265469193458557 + 0.001 * 7.296182632446289
Epoch 200, val loss: 0.8270853757858276
Epoch 210, training loss: 0.5766146183013916 = 0.5693249106407166 + 0.001 * 7.289694309234619
Epoch 210, val loss: 0.7914402484893799
Epoch 220, training loss: 0.5254291296005249 = 0.5181460976600647 + 0.001 * 7.283042907714844
Epoch 220, val loss: 0.7619483470916748
Epoch 230, training loss: 0.4789602756500244 = 0.4716838300228119 + 0.001 * 7.276442050933838
Epoch 230, val loss: 0.7368248105049133
Epoch 240, training loss: 0.4358159303665161 = 0.42854639887809753 + 0.001 * 7.269536972045898
Epoch 240, val loss: 0.7149032950401306
Epoch 250, training loss: 0.3947491943836212 = 0.38748738169670105 + 0.001 * 7.2618021965026855
Epoch 250, val loss: 0.6953080296516418
Epoch 260, training loss: 0.35515397787094116 = 0.34790098667144775 + 0.001 * 7.253004550933838
Epoch 260, val loss: 0.6778510212898254
Epoch 270, training loss: 0.3172677457332611 = 0.3100253939628601 + 0.001 * 7.242363929748535
Epoch 270, val loss: 0.6627959609031677
Epoch 280, training loss: 0.28166285157203674 = 0.2744305431842804 + 0.001 * 7.2323222160339355
Epoch 280, val loss: 0.6503634452819824
Epoch 290, training loss: 0.24854318797588348 = 0.2413306087255478 + 0.001 * 7.212573051452637
Epoch 290, val loss: 0.6403812766075134
Epoch 300, training loss: 0.21778714656829834 = 0.21058230102062225 + 0.001 * 7.20483922958374
Epoch 300, val loss: 0.6325552463531494
Epoch 310, training loss: 0.1894473284482956 = 0.18227148056030273 + 0.001 * 7.175845146179199
Epoch 310, val loss: 0.626798152923584
Epoch 320, training loss: 0.16401194036006927 = 0.1568455994129181 + 0.001 * 7.166344165802002
Epoch 320, val loss: 0.6231131553649902
Epoch 330, training loss: 0.14191550016403198 = 0.13478794693946838 + 0.001 * 7.127548694610596
Epoch 330, val loss: 0.6217327117919922
Epoch 340, training loss: 0.12328805029392242 = 0.11615156382322311 + 0.001 * 7.136482238769531
Epoch 340, val loss: 0.6227408647537231
Epoch 350, training loss: 0.10771435499191284 = 0.10060958564281464 + 0.001 * 7.104767322540283
Epoch 350, val loss: 0.6260076761245728
Epoch 360, training loss: 0.09477594494819641 = 0.0876818597316742 + 0.001 * 7.094082355499268
Epoch 360, val loss: 0.6311817765235901
Epoch 370, training loss: 0.0839972272515297 = 0.07688277959823608 + 0.001 * 7.114445686340332
Epoch 370, val loss: 0.6380115151405334
Epoch 380, training loss: 0.07488860934972763 = 0.06780470907688141 + 0.001 * 7.083898544311523
Epoch 380, val loss: 0.6460320949554443
Epoch 390, training loss: 0.06718754768371582 = 0.06012142822146416 + 0.001 * 7.066120624542236
Epoch 390, val loss: 0.6549897193908691
Epoch 400, training loss: 0.06063990667462349 = 0.05357840284705162 + 0.001 * 7.061502456665039
Epoch 400, val loss: 0.6646021604537964
Epoch 410, training loss: 0.055039845407009125 = 0.04797626659274101 + 0.001 * 7.063576698303223
Epoch 410, val loss: 0.6746519804000854
Epoch 420, training loss: 0.05020755156874657 = 0.04315590113401413 + 0.001 * 7.0516486167907715
Epoch 420, val loss: 0.6849585771560669
Epoch 430, training loss: 0.046103253960609436 = 0.03898811712861061 + 0.001 * 7.11513614654541
Epoch 430, val loss: 0.6954157948493958
Epoch 440, training loss: 0.04243316128849983 = 0.035368748009204865 + 0.001 * 7.064411640167236
Epoch 440, val loss: 0.7058744430541992
Epoch 450, training loss: 0.03926834464073181 = 0.032211367040872574 + 0.001 * 7.056978225708008
Epoch 450, val loss: 0.7162994742393494
Epoch 460, training loss: 0.03648483008146286 = 0.02944476716220379 + 0.001 * 7.0400614738464355
Epoch 460, val loss: 0.7265897393226624
Epoch 470, training loss: 0.034088362008333206 = 0.02701016329228878 + 0.001 * 7.078199863433838
Epoch 470, val loss: 0.7367703914642334
Epoch 480, training loss: 0.03190573677420616 = 0.024858925491571426 + 0.001 * 7.046810150146484
Epoch 480, val loss: 0.746774435043335
Epoch 490, training loss: 0.029985539615154266 = 0.02295059524476528 + 0.001 * 7.0349440574646
Epoch 490, val loss: 0.7565364837646484
Epoch 500, training loss: 0.028285067528486252 = 0.021251022815704346 + 0.001 * 7.034045219421387
Epoch 500, val loss: 0.7661949396133423
Epoch 510, training loss: 0.026768704876303673 = 0.019731692969799042 + 0.001 * 7.03701114654541
Epoch 510, val loss: 0.7755646705627441
Epoch 520, training loss: 0.025399118661880493 = 0.01836881972849369 + 0.001 * 7.030298709869385
Epoch 520, val loss: 0.7847480773925781
Epoch 530, training loss: 0.02417386882007122 = 0.017141975462436676 + 0.001 * 7.031893253326416
Epoch 530, val loss: 0.793710470199585
Epoch 540, training loss: 0.023056795820593834 = 0.016034377738833427 + 0.001 * 7.022418022155762
Epoch 540, val loss: 0.8024747371673584
Epoch 550, training loss: 0.022063273936510086 = 0.015031224116683006 + 0.001 * 7.032050609588623
Epoch 550, val loss: 0.8109883069992065
Epoch 560, training loss: 0.021140189841389656 = 0.014120013453066349 + 0.001 * 7.020175933837891
Epoch 560, val loss: 0.8192960023880005
Epoch 570, training loss: 0.020319834351539612 = 0.0132899833843112 + 0.001 * 7.029850482940674
Epoch 570, val loss: 0.8274105787277222
Epoch 580, training loss: 0.019551046192646027 = 0.012532039545476437 + 0.001 * 7.019006252288818
Epoch 580, val loss: 0.8353231549263
Epoch 590, training loss: 0.01885865069925785 = 0.011838181875646114 + 0.001 * 7.020468235015869
Epoch 590, val loss: 0.8430610299110413
Epoch 600, training loss: 0.018222585320472717 = 0.011201463639736176 + 0.001 * 7.021121978759766
Epoch 600, val loss: 0.8505989909172058
Epoch 610, training loss: 0.01762310042977333 = 0.010615905746817589 + 0.001 * 7.007194995880127
Epoch 610, val loss: 0.857970654964447
Epoch 620, training loss: 0.017106767743825912 = 0.010076322592794895 + 0.001 * 7.030445098876953
Epoch 620, val loss: 0.8651742339134216
Epoch 630, training loss: 0.016587160527706146 = 0.0095780985429883 + 0.001 * 7.00906229019165
Epoch 630, val loss: 0.8721922636032104
Epoch 640, training loss: 0.016127435490489006 = 0.00911711249500513 + 0.001 * 7.010322093963623
Epoch 640, val loss: 0.879063606262207
Epoch 650, training loss: 0.01569170504808426 = 0.008689445443451405 + 0.001 * 7.002260208129883
Epoch 650, val loss: 0.8857636451721191
Epoch 660, training loss: 0.015325210988521576 = 0.008291288278996944 + 0.001 * 7.0339226722717285
Epoch 660, val loss: 0.8923187851905823
Epoch 670, training loss: 0.014923006296157837 = 0.00791842583566904 + 0.001 * 7.004580497741699
Epoch 670, val loss: 0.8987709879875183
Epoch 680, training loss: 0.01458314061164856 = 0.007567394059151411 + 0.001 * 7.015746593475342
Epoch 680, val loss: 0.9051200151443481
Epoch 690, training loss: 0.014239853248000145 = 0.0072372653521597385 + 0.001 * 7.002587795257568
Epoch 690, val loss: 0.9113563895225525
Epoch 700, training loss: 0.013930020853877068 = 0.006926327012479305 + 0.001 * 7.003693103790283
Epoch 700, val loss: 0.9175096154212952
Epoch 710, training loss: 0.013628678396344185 = 0.006633274257183075 + 0.001 * 6.995403289794922
Epoch 710, val loss: 0.9235650897026062
Epoch 720, training loss: 0.013358336873352528 = 0.006357105448842049 + 0.001 * 7.0012311935424805
Epoch 720, val loss: 0.9295322895050049
Epoch 730, training loss: 0.013086343184113503 = 0.006097008939832449 + 0.001 * 6.9893341064453125
Epoch 730, val loss: 0.9353771805763245
Epoch 740, training loss: 0.012842809781432152 = 0.005852076690644026 + 0.001 * 6.9907331466674805
Epoch 740, val loss: 0.9411309361457825
Epoch 750, training loss: 0.01260652206838131 = 0.005621407181024551 + 0.001 * 6.985114097595215
Epoch 750, val loss: 0.946776807308197
Epoch 760, training loss: 0.01240125484764576 = 0.005404092837125063 + 0.001 * 6.997162342071533
Epoch 760, val loss: 0.952308714389801
Epoch 770, training loss: 0.012181725353002548 = 0.0051993378438055515 + 0.001 * 6.982387065887451
Epoch 770, val loss: 0.9577297568321228
Epoch 780, training loss: 0.011997949331998825 = 0.005006335210055113 + 0.001 * 6.991613864898682
Epoch 780, val loss: 0.9630430936813354
Epoch 790, training loss: 0.011808634735643864 = 0.004824311472475529 + 0.001 * 6.984323024749756
Epoch 790, val loss: 0.9682527780532837
Epoch 800, training loss: 0.011646157130599022 = 0.004652593284845352 + 0.001 * 6.993564128875732
Epoch 800, val loss: 0.9733556509017944
Epoch 810, training loss: 0.011474842205643654 = 0.004490464460104704 + 0.001 * 6.984376907348633
Epoch 810, val loss: 0.9783328771591187
Epoch 820, training loss: 0.01131081860512495 = 0.004337193910032511 + 0.001 * 6.973624229431152
Epoch 820, val loss: 0.9832431077957153
Epoch 830, training loss: 0.011168800294399261 = 0.00419222004711628 + 0.001 * 6.976579666137695
Epoch 830, val loss: 0.9880257844924927
Epoch 840, training loss: 0.011026697233319283 = 0.004054972901940346 + 0.001 * 6.971724033355713
Epoch 840, val loss: 0.992728590965271
Epoch 850, training loss: 0.010921258479356766 = 0.0039249686524271965 + 0.001 * 6.9962897300720215
Epoch 850, val loss: 0.9973154664039612
Epoch 860, training loss: 0.01077266689389944 = 0.0038017055485397577 + 0.001 * 6.970961093902588
Epoch 860, val loss: 1.0018278360366821
Epoch 870, training loss: 0.010651345364749432 = 0.0036847181618213654 + 0.001 * 6.9666266441345215
Epoch 870, val loss: 1.006245732307434
Epoch 880, training loss: 0.010547030717134476 = 0.003573571564629674 + 0.001 * 6.973458766937256
Epoch 880, val loss: 1.0105806589126587
Epoch 890, training loss: 0.01046740636229515 = 0.0034679181408137083 + 0.001 * 6.99948787689209
Epoch 890, val loss: 1.0148158073425293
Epoch 900, training loss: 0.010330929420888424 = 0.0033675346057862043 + 0.001 * 6.963394641876221
Epoch 900, val loss: 1.0189706087112427
Epoch 910, training loss: 0.010240232571959496 = 0.003272005822509527 + 0.001 * 6.968226909637451
Epoch 910, val loss: 1.0230259895324707
Epoch 920, training loss: 0.010147110559046268 = 0.003181006060913205 + 0.001 * 6.966104507446289
Epoch 920, val loss: 1.0270354747772217
Epoch 930, training loss: 0.010051232762634754 = 0.0030942487064749002 + 0.001 * 6.95698356628418
Epoch 930, val loss: 1.0309611558914185
Epoch 940, training loss: 0.009968496859073639 = 0.0030115016270428896 + 0.001 * 6.956995010375977
Epoch 940, val loss: 1.0347901582717896
Epoch 950, training loss: 0.009892189875245094 = 0.002932496601715684 + 0.001 * 6.959692478179932
Epoch 950, val loss: 1.038561463356018
Epoch 960, training loss: 0.00982174463570118 = 0.0028570126742124557 + 0.001 * 6.964731216430664
Epoch 960, val loss: 1.0422488451004028
Epoch 970, training loss: 0.009735203348100185 = 0.002784837270155549 + 0.001 * 6.9503655433654785
Epoch 970, val loss: 1.0458858013153076
Epoch 980, training loss: 0.009674408473074436 = 0.0027157976292073727 + 0.001 * 6.958610534667969
Epoch 980, val loss: 1.0494431257247925
Epoch 990, training loss: 0.009605791419744492 = 0.002649674192070961 + 0.001 * 6.956116676330566
Epoch 990, val loss: 1.0529425144195557
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 1.9527809619903564 = 1.9441840648651123 + 0.001 * 8.596845626831055
Epoch 0, val loss: 1.9314796924591064
Epoch 10, training loss: 1.942160964012146 = 1.9335641860961914 + 0.001 * 8.596796989440918
Epoch 10, val loss: 1.9212979078292847
Epoch 20, training loss: 1.9291152954101562 = 1.9205186367034912 + 0.001 * 8.596623420715332
Epoch 20, val loss: 1.9084111452102661
Epoch 30, training loss: 1.9109693765640259 = 1.9023730754852295 + 0.001 * 8.596261978149414
Epoch 30, val loss: 1.8903833627700806
Epoch 40, training loss: 1.8846205472946167 = 1.8760250806808472 + 0.001 * 8.595518112182617
Epoch 40, val loss: 1.8645814657211304
Epoch 50, training loss: 1.8488370180130005 = 1.8402433395385742 + 0.001 * 8.593706130981445
Epoch 50, val loss: 1.8313037157058716
Epoch 60, training loss: 1.8099102973937988 = 1.8013219833374023 + 0.001 * 8.588315963745117
Epoch 60, val loss: 1.7997781038284302
Epoch 70, training loss: 1.7761234045028687 = 1.7675553560256958 + 0.001 * 8.568103790283203
Epoch 70, val loss: 1.7757376432418823
Epoch 80, training loss: 1.7335909605026245 = 1.725138783454895 + 0.001 * 8.452184677124023
Epoch 80, val loss: 1.7395962476730347
Epoch 90, training loss: 1.6726025342941284 = 1.6645710468292236 + 0.001 * 8.031436920166016
Epoch 90, val loss: 1.6848868131637573
Epoch 100, training loss: 1.5908349752426147 = 1.5828804969787598 + 0.001 * 7.954452037811279
Epoch 100, val loss: 1.613504409790039
Epoch 110, training loss: 1.4947267770767212 = 1.486844539642334 + 0.001 * 7.882233142852783
Epoch 110, val loss: 1.5339142084121704
Epoch 120, training loss: 1.3966785669326782 = 1.3889755010604858 + 0.001 * 7.703041076660156
Epoch 120, val loss: 1.4541977643966675
Epoch 130, training loss: 1.2989178895950317 = 1.2914668321609497 + 0.001 * 7.451075077056885
Epoch 130, val loss: 1.3753092288970947
Epoch 140, training loss: 1.1989256143569946 = 1.1915175914764404 + 0.001 * 7.407989501953125
Epoch 140, val loss: 1.2950602769851685
Epoch 150, training loss: 1.0964133739471436 = 1.089053988456726 + 0.001 * 7.359442710876465
Epoch 150, val loss: 1.2127375602722168
Epoch 160, training loss: 0.9945530295372009 = 0.9872021079063416 + 0.001 * 7.350912570953369
Epoch 160, val loss: 1.1319193840026855
Epoch 170, training loss: 0.8967567086219788 = 0.8894126415252686 + 0.001 * 7.344078540802002
Epoch 170, val loss: 1.05559241771698
Epoch 180, training loss: 0.8050727844238281 = 0.7977356314659119 + 0.001 * 7.3371758460998535
Epoch 180, val loss: 0.985084593296051
Epoch 190, training loss: 0.7212979197502136 = 0.7139694690704346 + 0.001 * 7.328437805175781
Epoch 190, val loss: 0.921666145324707
Epoch 200, training loss: 0.6472778916358948 = 0.6399582028388977 + 0.001 * 7.319663047790527
Epoch 200, val loss: 0.8675432801246643
Epoch 210, training loss: 0.5837293267250061 = 0.5764184594154358 + 0.001 * 7.3108625411987305
Epoch 210, val loss: 0.8240135908126831
Epoch 220, training loss: 0.5295634269714355 = 0.5222612619400024 + 0.001 * 7.302169322967529
Epoch 220, val loss: 0.7909346222877502
Epoch 230, training loss: 0.4825281500816345 = 0.47523412108421326 + 0.001 * 7.294040203094482
Epoch 230, val loss: 0.7665760517120361
Epoch 240, training loss: 0.4401487112045288 = 0.4328620433807373 + 0.001 * 7.286677837371826
Epoch 240, val loss: 0.7485582828521729
Epoch 250, training loss: 0.4003269672393799 = 0.3930474519729614 + 0.001 * 7.279504299163818
Epoch 250, val loss: 0.7343985438346863
Epoch 260, training loss: 0.3616315722465515 = 0.3543609082698822 + 0.001 * 7.270657062530518
Epoch 260, val loss: 0.7221469283103943
Epoch 270, training loss: 0.32352566719055176 = 0.31626805663108826 + 0.001 * 7.257613182067871
Epoch 270, val loss: 0.7108585238456726
Epoch 280, training loss: 0.28619062900543213 = 0.2789528965950012 + 0.001 * 7.23774528503418
Epoch 280, val loss: 0.7001162767410278
Epoch 290, training loss: 0.250214159488678 = 0.24299539625644684 + 0.001 * 7.218767166137695
Epoch 290, val loss: 0.6903473734855652
Epoch 300, training loss: 0.21644015610218048 = 0.2092389017343521 + 0.001 * 7.2012505531311035
Epoch 300, val loss: 0.6822742223739624
Epoch 310, training loss: 0.18582722544670105 = 0.1786413937807083 + 0.001 * 7.185826778411865
Epoch 310, val loss: 0.6769228577613831
Epoch 320, training loss: 0.15910686552524567 = 0.1519312709569931 + 0.001 * 7.17559289932251
Epoch 320, val loss: 0.6750771999359131
Epoch 330, training loss: 0.1365043669939041 = 0.1293320208787918 + 0.001 * 7.172349452972412
Epoch 330, val loss: 0.6768031716346741
Epoch 340, training loss: 0.1177242174744606 = 0.11055317521095276 + 0.001 * 7.171039581298828
Epoch 340, val loss: 0.6817211508750916
Epoch 350, training loss: 0.10222337394952774 = 0.0950515866279602 + 0.001 * 7.171786785125732
Epoch 350, val loss: 0.6892101168632507
Epoch 360, training loss: 0.0894295945763588 = 0.08225693553686142 + 0.001 * 7.172660827636719
Epoch 360, val loss: 0.6986982226371765
Epoch 370, training loss: 0.07882211357355118 = 0.07164855301380157 + 0.001 * 7.173559665679932
Epoch 370, val loss: 0.7096432447433472
Epoch 380, training loss: 0.0699755847454071 = 0.06280077993869781 + 0.001 * 7.174802303314209
Epoch 380, val loss: 0.721653938293457
Epoch 390, training loss: 0.0625530257821083 = 0.05537743121385574 + 0.001 * 7.175593376159668
Epoch 390, val loss: 0.7343156337738037
Epoch 400, training loss: 0.05628760904073715 = 0.04911207780241966 + 0.001 * 7.175532817840576
Epoch 400, val loss: 0.7474180459976196
Epoch 410, training loss: 0.05096697062253952 = 0.04379117861390114 + 0.001 * 7.175789833068848
Epoch 410, val loss: 0.7607858180999756
Epoch 420, training loss: 0.04642125219106674 = 0.03924550116062164 + 0.001 * 7.175750732421875
Epoch 420, val loss: 0.7742263674736023
Epoch 430, training loss: 0.04251677170395851 = 0.0353403277695179 + 0.001 * 7.176443099975586
Epoch 430, val loss: 0.7875853776931763
Epoch 440, training loss: 0.03914429992437363 = 0.03196709230542183 + 0.001 * 7.177208423614502
Epoch 440, val loss: 0.8007811307907104
Epoch 450, training loss: 0.036214329302310944 = 0.02903791330754757 + 0.001 * 7.176414489746094
Epoch 450, val loss: 0.8137263655662537
Epoch 460, training loss: 0.03365838900208473 = 0.026481973007321358 + 0.001 * 7.176414966583252
Epoch 460, val loss: 0.8263963460922241
Epoch 470, training loss: 0.031417522579431534 = 0.0242411307990551 + 0.001 * 7.176391124725342
Epoch 470, val loss: 0.8387686014175415
Epoch 480, training loss: 0.029446236789226532 = 0.022267742082476616 + 0.001 * 7.178494453430176
Epoch 480, val loss: 0.8508338928222656
Epoch 490, training loss: 0.0276993028819561 = 0.020522363483905792 + 0.001 * 7.176938056945801
Epoch 490, val loss: 0.8625712990760803
Epoch 500, training loss: 0.026148546487092972 = 0.018972599878907204 + 0.001 * 7.175946235656738
Epoch 500, val loss: 0.8739847540855408
Epoch 510, training loss: 0.024768631905317307 = 0.017590994015336037 + 0.001 * 7.177638530731201
Epoch 510, val loss: 0.8850717544555664
Epoch 520, training loss: 0.023530609905719757 = 0.016354875639081 + 0.001 * 7.175734996795654
Epoch 520, val loss: 0.8958360552787781
Epoch 530, training loss: 0.022419994696974754 = 0.01524520292878151 + 0.001 * 7.17479133605957
Epoch 530, val loss: 0.9063059091567993
Epoch 540, training loss: 0.021420560777187347 = 0.014245796017348766 + 0.001 * 7.174764156341553
Epoch 540, val loss: 0.9164665937423706
Epoch 550, training loss: 0.020518828183412552 = 0.013342865742743015 + 0.001 * 7.175961494445801
Epoch 550, val loss: 0.926335334777832
Epoch 560, training loss: 0.01969735324382782 = 0.012524676509201527 + 0.001 * 7.172675609588623
Epoch 560, val loss: 0.9359112977981567
Epoch 570, training loss: 0.018954956904053688 = 0.011781145818531513 + 0.001 * 7.1738104820251465
Epoch 570, val loss: 0.9452117085456848
Epoch 580, training loss: 0.018274981528520584 = 0.01110371295362711 + 0.001 * 7.171267509460449
Epoch 580, val loss: 0.9542550444602966
Epoch 590, training loss: 0.017655882984399796 = 0.01048489473760128 + 0.001 * 7.170987606048584
Epoch 590, val loss: 0.9630438685417175
Epoch 600, training loss: 0.01709333062171936 = 0.009918206371366978 + 0.001 * 7.17512321472168
Epoch 600, val loss: 0.9716050624847412
Epoch 610, training loss: 0.0165683813393116 = 0.009398047812283039 + 0.001 * 7.170332908630371
Epoch 610, val loss: 0.9799282550811768
Epoch 620, training loss: 0.016087818890810013 = 0.008919542655348778 + 0.001 * 7.168275356292725
Epoch 620, val loss: 0.9880305528640747
Epoch 630, training loss: 0.015646904706954956 = 0.008478371426463127 + 0.001 * 7.168532848358154
Epoch 630, val loss: 0.9959158301353455
Epoch 640, training loss: 0.01523999311029911 = 0.008070814423263073 + 0.001 * 7.169178009033203
Epoch 640, val loss: 1.0035855770111084
Epoch 650, training loss: 0.0148625448346138 = 0.00769355334341526 + 0.001 * 7.1689910888671875
Epoch 650, val loss: 1.0110794305801392
Epoch 660, training loss: 0.014507274143397808 = 0.00734366849064827 + 0.001 * 7.163605213165283
Epoch 660, val loss: 1.0183556079864502
Epoch 670, training loss: 0.014181290753185749 = 0.00701861409470439 + 0.001 * 7.1626763343811035
Epoch 670, val loss: 1.0254545211791992
Epoch 680, training loss: 0.013877332210540771 = 0.006716086529195309 + 0.001 * 7.161245346069336
Epoch 680, val loss: 1.0323748588562012
Epoch 690, training loss: 0.013595538213849068 = 0.006434095092117786 + 0.001 * 7.161442756652832
Epoch 690, val loss: 1.039123296737671
Epoch 700, training loss: 0.013330258429050446 = 0.006170810200273991 + 0.001 * 7.15944766998291
Epoch 700, val loss: 1.0457149744033813
Epoch 710, training loss: 0.013084867969155312 = 0.005924605764448643 + 0.001 * 7.160262107849121
Epoch 710, val loss: 1.0521453619003296
Epoch 720, training loss: 0.012851014733314514 = 0.005694062449038029 + 0.001 * 7.156952381134033
Epoch 720, val loss: 1.0584343671798706
Epoch 730, training loss: 0.012638488784432411 = 0.00547786895185709 + 0.001 * 7.160619258880615
Epoch 730, val loss: 1.0645623207092285
Epoch 740, training loss: 0.01242772489786148 = 0.005274864379316568 + 0.001 * 7.152860164642334
Epoch 740, val loss: 1.070554256439209
Epoch 750, training loss: 0.012235605157911777 = 0.0050839995965361595 + 0.001 * 7.151605129241943
Epoch 750, val loss: 1.0764158964157104
Epoch 760, training loss: 0.012052025645971298 = 0.004904322326183319 + 0.001 * 7.147703170776367
Epoch 760, val loss: 1.082139015197754
Epoch 770, training loss: 0.011895092204213142 = 0.00473497249186039 + 0.001 * 7.160119533538818
Epoch 770, val loss: 1.0877243280410767
Epoch 780, training loss: 0.011724438518285751 = 0.004575198516249657 + 0.001 * 7.149239540100098
Epoch 780, val loss: 1.0932023525238037
Epoch 790, training loss: 0.011567237786948681 = 0.004424270708113909 + 0.001 * 7.1429667472839355
Epoch 790, val loss: 1.0985560417175293
Epoch 800, training loss: 0.011422866955399513 = 0.004281539469957352 + 0.001 * 7.141327381134033
Epoch 800, val loss: 1.103790044784546
Epoch 810, training loss: 0.011282067745923996 = 0.004146434832364321 + 0.001 * 7.135632514953613
Epoch 810, val loss: 1.1089197397232056
Epoch 820, training loss: 0.011154420673847198 = 0.0040184250101447105 + 0.001 * 7.135995864868164
Epoch 820, val loss: 1.1139413118362427
Epoch 830, training loss: 0.01103867869824171 = 0.0038970147725194693 + 0.001 * 7.141663551330566
Epoch 830, val loss: 1.1188452243804932
Epoch 840, training loss: 0.01091090776026249 = 0.0037817673292011023 + 0.001 * 7.1291399002075195
Epoch 840, val loss: 1.123650074005127
Epoch 850, training loss: 0.010804909281432629 = 0.003672269405797124 + 0.001 * 7.132639408111572
Epoch 850, val loss: 1.1283555030822754
Epoch 860, training loss: 0.010695649310946465 = 0.0035681319423019886 + 0.001 * 7.127517223358154
Epoch 860, val loss: 1.1329668760299683
Epoch 870, training loss: 0.010604370385408401 = 0.0034690198954194784 + 0.001 * 7.135349750518799
Epoch 870, val loss: 1.1374907493591309
Epoch 880, training loss: 0.010492527857422829 = 0.0033745868131518364 + 0.001 * 7.117940425872803
Epoch 880, val loss: 1.1419233083724976
Epoch 890, training loss: 0.010403517633676529 = 0.0032845819368958473 + 0.001 * 7.118935585021973
Epoch 890, val loss: 1.1462641954421997
Epoch 900, training loss: 0.010318642482161522 = 0.0031987286638468504 + 0.001 * 7.119913101196289
Epoch 900, val loss: 1.150527834892273
Epoch 910, training loss: 0.01022884901612997 = 0.003116753650829196 + 0.001 * 7.112094879150391
Epoch 910, val loss: 1.1546939611434937
Epoch 920, training loss: 0.010157441720366478 = 0.0030384513083845377 + 0.001 * 7.118989944458008
Epoch 920, val loss: 1.1587789058685303
Epoch 930, training loss: 0.010070106014609337 = 0.0029635983519256115 + 0.001 * 7.106507301330566
Epoch 930, val loss: 1.1628035306930542
Epoch 940, training loss: 0.010030033066868782 = 0.002891984535381198 + 0.001 * 7.1380486488342285
Epoch 940, val loss: 1.1667412519454956
Epoch 950, training loss: 0.00992858037352562 = 0.002823443152010441 + 0.001 * 7.105136871337891
Epoch 950, val loss: 1.1706181764602661
Epoch 960, training loss: 0.009883098304271698 = 0.0027577949222177267 + 0.001 * 7.125302791595459
Epoch 960, val loss: 1.1744133234024048
Epoch 970, training loss: 0.009797713719308376 = 0.002694864524528384 + 0.001 * 7.102848529815674
Epoch 970, val loss: 1.1781386137008667
Epoch 980, training loss: 0.009728847071528435 = 0.0026345145888626575 + 0.001 * 7.094331741333008
Epoch 980, val loss: 1.1818028688430786
Epoch 990, training loss: 0.009710986167192459 = 0.002576593542471528 + 0.001 * 7.134392738342285
Epoch 990, val loss: 1.1853774785995483
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8360569319978914
The final CL Acc:0.82222, 0.00907, The final GNN Acc:0.83553, 0.00114
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9588])
updated graph: torch.Size([2, 10610])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9469597339630127 = 1.9383628368377686 + 0.001 * 8.596869468688965
Epoch 0, val loss: 1.9346799850463867
Epoch 10, training loss: 1.9372048377990723 = 1.9286080598831177 + 0.001 * 8.596809387207031
Epoch 10, val loss: 1.9258301258087158
Epoch 20, training loss: 1.9251612424850464 = 1.9165645837783813 + 0.001 * 8.59661865234375
Epoch 20, val loss: 1.9146201610565186
Epoch 30, training loss: 1.9082063436508179 = 1.899610161781311 + 0.001 * 8.596146583557129
Epoch 30, val loss: 1.8985285758972168
Epoch 40, training loss: 1.8832781314849854 = 1.8746832609176636 + 0.001 * 8.594889640808105
Epoch 40, val loss: 1.875075101852417
Epoch 50, training loss: 1.849157691001892 = 1.840566635131836 + 0.001 * 8.591031074523926
Epoch 50, val loss: 1.8444514274597168
Epoch 60, training loss: 1.8121436834335327 = 1.8035675287246704 + 0.001 * 8.576133728027344
Epoch 60, val loss: 1.8140877485275269
Epoch 70, training loss: 1.7803157567977905 = 1.7718112468719482 + 0.001 * 8.504480361938477
Epoch 70, val loss: 1.7866671085357666
Epoch 80, training loss: 1.7403110265731812 = 1.7320882081985474 + 0.001 * 8.222807884216309
Epoch 80, val loss: 1.747659683227539
Epoch 90, training loss: 1.6847606897354126 = 1.6766440868377686 + 0.001 * 8.116595268249512
Epoch 90, val loss: 1.6974905729293823
Epoch 100, training loss: 1.6098576784133911 = 1.6018294095993042 + 0.001 * 8.0283203125
Epoch 100, val loss: 1.6346369981765747
Epoch 110, training loss: 1.5198231935501099 = 1.5119261741638184 + 0.001 * 7.8969831466674805
Epoch 110, val loss: 1.5612695217132568
Epoch 120, training loss: 1.423112392425537 = 1.4154618978500366 + 0.001 * 7.6505303382873535
Epoch 120, val loss: 1.4833531379699707
Epoch 130, training loss: 1.3246407508850098 = 1.3170937299728394 + 0.001 * 7.547049522399902
Epoch 130, val loss: 1.4058382511138916
Epoch 140, training loss: 1.2258132696151733 = 1.2183363437652588 + 0.001 * 7.476922988891602
Epoch 140, val loss: 1.330580711364746
Epoch 150, training loss: 1.127375841140747 = 1.1199285984039307 + 0.001 * 7.447236061096191
Epoch 150, val loss: 1.2573330402374268
Epoch 160, training loss: 1.0304017066955566 = 1.0229828357696533 + 0.001 * 7.418832778930664
Epoch 160, val loss: 1.1873046159744263
Epoch 170, training loss: 0.9373570084571838 = 0.9299595355987549 + 0.001 * 7.397453784942627
Epoch 170, val loss: 1.1225507259368896
Epoch 180, training loss: 0.8512987494468689 = 0.8439198136329651 + 0.001 * 7.3789544105529785
Epoch 180, val loss: 1.0653349161148071
Epoch 190, training loss: 0.7740061283111572 = 0.7666484117507935 + 0.001 * 7.357695579528809
Epoch 190, val loss: 1.017391324043274
Epoch 200, training loss: 0.7050102353096008 = 0.6976810097694397 + 0.001 * 7.329245567321777
Epoch 200, val loss: 0.9788419604301453
Epoch 210, training loss: 0.6424329280853271 = 0.635136067867279 + 0.001 * 7.296874046325684
Epoch 210, val loss: 0.948478102684021
Epoch 220, training loss: 0.5843223333358765 = 0.5770483016967773 + 0.001 * 7.274059295654297
Epoch 220, val loss: 0.9251744151115417
Epoch 230, training loss: 0.5292680263519287 = 0.5220080614089966 + 0.001 * 7.2599358558654785
Epoch 230, val loss: 0.9078840613365173
Epoch 240, training loss: 0.4763606786727905 = 0.46910926699638367 + 0.001 * 7.251406192779541
Epoch 240, val loss: 0.8955929279327393
Epoch 250, training loss: 0.4251207113265991 = 0.41787219047546387 + 0.001 * 7.248515605926514
Epoch 250, val loss: 0.8875399231910706
Epoch 260, training loss: 0.37564536929130554 = 0.3683958053588867 + 0.001 * 7.2495574951171875
Epoch 260, val loss: 0.8830841183662415
Epoch 270, training loss: 0.32870373129844666 = 0.32145577669143677 + 0.001 * 7.247950077056885
Epoch 270, val loss: 0.881775438785553
Epoch 280, training loss: 0.2852880656719208 = 0.2780393660068512 + 0.001 * 7.24869441986084
Epoch 280, val loss: 0.8835140466690063
Epoch 290, training loss: 0.24621549248695374 = 0.2389657199382782 + 0.001 * 7.2497782707214355
Epoch 290, val loss: 0.8883326649665833
Epoch 300, training loss: 0.21193310618400574 = 0.20468303561210632 + 0.001 * 7.250074863433838
Epoch 300, val loss: 0.8961167931556702
Epoch 310, training loss: 0.18248583376407623 = 0.17523464560508728 + 0.001 * 7.251185417175293
Epoch 310, val loss: 0.9066857099533081
Epoch 320, training loss: 0.1575849950313568 = 0.15033338963985443 + 0.001 * 7.251609802246094
Epoch 320, val loss: 0.9199112057685852
Epoch 330, training loss: 0.13673074543476105 = 0.1294717639684677 + 0.001 * 7.258982181549072
Epoch 330, val loss: 0.9354501962661743
Epoch 340, training loss: 0.11930685490369797 = 0.1120542660355568 + 0.001 * 7.252586841583252
Epoch 340, val loss: 0.952957272529602
Epoch 350, training loss: 0.10475021600723267 = 0.09749617427587509 + 0.001 * 7.254044532775879
Epoch 350, val loss: 0.9719265699386597
Epoch 360, training loss: 0.09253052622079849 = 0.08527719229459763 + 0.001 * 7.253334045410156
Epoch 360, val loss: 0.9919676780700684
Epoch 370, training loss: 0.08222247660160065 = 0.0749688670039177 + 0.001 * 7.253610610961914
Epoch 370, val loss: 1.0126826763153076
Epoch 380, training loss: 0.07347765564918518 = 0.06622427701950073 + 0.001 * 7.253380298614502
Epoch 380, val loss: 1.033754587173462
Epoch 390, training loss: 0.06602343171834946 = 0.058768000453710556 + 0.001 * 7.255433559417725
Epoch 390, val loss: 1.0548949241638184
Epoch 400, training loss: 0.0596313551068306 = 0.05237714946269989 + 0.001 * 7.254203796386719
Epoch 400, val loss: 1.075892448425293
Epoch 410, training loss: 0.05412675812840462 = 0.046873897314071655 + 0.001 * 7.252860069274902
Epoch 410, val loss: 1.0965917110443115
Epoch 420, training loss: 0.04936971887946129 = 0.04211561754345894 + 0.001 * 7.254101753234863
Epoch 420, val loss: 1.1169230937957764
Epoch 430, training loss: 0.04523669183254242 = 0.037984590977430344 + 0.001 * 7.252101421356201
Epoch 430, val loss: 1.136767864227295
Epoch 440, training loss: 0.041639696806669235 = 0.03437814861536026 + 0.001 * 7.2615485191345215
Epoch 440, val loss: 1.1561602354049683
Epoch 450, training loss: 0.03846333920955658 = 0.031210029497742653 + 0.001 * 7.253309726715088
Epoch 450, val loss: 1.1750824451446533
Epoch 460, training loss: 0.03566592186689377 = 0.028415804728865623 + 0.001 * 7.250118255615234
Epoch 460, val loss: 1.1935895681381226
Epoch 470, training loss: 0.033200182020664215 = 0.025944923982024193 + 0.001 * 7.255259037017822
Epoch 470, val loss: 1.2117156982421875
Epoch 480, training loss: 0.03100719302892685 = 0.023755978792905807 + 0.001 * 7.251213073730469
Epoch 480, val loss: 1.229414463043213
Epoch 490, training loss: 0.02906155399978161 = 0.0218130461871624 + 0.001 * 7.248507022857666
Epoch 490, val loss: 1.2466506958007812
Epoch 500, training loss: 0.027340855449438095 = 0.02008490450680256 + 0.001 * 7.255951404571533
Epoch 500, val loss: 1.2634358406066895
Epoch 510, training loss: 0.025792688131332397 = 0.018544292077422142 + 0.001 * 7.248395919799805
Epoch 510, val loss: 1.279727578163147
Epoch 520, training loss: 0.024411100894212723 = 0.01716727577149868 + 0.001 * 7.243825912475586
Epoch 520, val loss: 1.2955571413040161
Epoch 530, training loss: 0.023178212344646454 = 0.015933459624648094 + 0.001 * 7.244753360748291
Epoch 530, val loss: 1.3109050989151
Epoch 540, training loss: 0.022071240469813347 = 0.014825155958533287 + 0.001 * 7.246083736419678
Epoch 540, val loss: 1.325788974761963
Epoch 550, training loss: 0.021064933389425278 = 0.013826794922351837 + 0.001 * 7.238138198852539
Epoch 550, val loss: 1.3401957750320435
Epoch 560, training loss: 0.020180638879537582 = 0.012925151735544205 + 0.001 * 7.255486965179443
Epoch 560, val loss: 1.3541749715805054
Epoch 570, training loss: 0.019341692328453064 = 0.012108845636248589 + 0.001 * 7.232845306396484
Epoch 570, val loss: 1.367721676826477
Epoch 580, training loss: 0.018607117235660553 = 0.01136787049472332 + 0.001 * 7.239245891571045
Epoch 580, val loss: 1.380840539932251
Epoch 590, training loss: 0.017922189086675644 = 0.010693601332604885 + 0.001 * 7.228588104248047
Epoch 590, val loss: 1.3935770988464355
Epoch 600, training loss: 0.017309870570898056 = 0.010078324005007744 + 0.001 * 7.231545925140381
Epoch 600, val loss: 1.405920147895813
Epoch 610, training loss: 0.016738615930080414 = 0.009514729492366314 + 0.001 * 7.223886489868164
Epoch 610, val loss: 1.4179366827011108
Epoch 620, training loss: 0.01622341200709343 = 0.008995349518954754 + 0.001 * 7.228063106536865
Epoch 620, val loss: 1.4296793937683105
Epoch 630, training loss: 0.015754874795675278 = 0.008514201268553734 + 0.001 * 7.240674018859863
Epoch 630, val loss: 1.4411976337432861
Epoch 640, training loss: 0.015283827669918537 = 0.008067366667091846 + 0.001 * 7.216460704803467
Epoch 640, val loss: 1.4524809122085571
Epoch 650, training loss: 0.014889047481119633 = 0.007651873864233494 + 0.001 * 7.237173080444336
Epoch 650, val loss: 1.463564395904541
Epoch 660, training loss: 0.014484809711575508 = 0.007265535183250904 + 0.001 * 7.219273567199707
Epoch 660, val loss: 1.4744112491607666
Epoch 670, training loss: 0.014144856482744217 = 0.006906372494995594 + 0.001 * 7.23848295211792
Epoch 670, val loss: 1.485018253326416
Epoch 680, training loss: 0.013865400105714798 = 0.0065725212916731834 + 0.001 * 7.2928786277771
Epoch 680, val loss: 1.4953747987747192
Epoch 690, training loss: 0.013475501909852028 = 0.006262240465730429 + 0.001 * 7.213261127471924
Epoch 690, val loss: 1.5054689645767212
Epoch 700, training loss: 0.013169781304895878 = 0.005973654333502054 + 0.001 * 7.196126461029053
Epoch 700, val loss: 1.515290379524231
Epoch 710, training loss: 0.012896683998405933 = 0.005704981274902821 + 0.001 * 7.191702365875244
Epoch 710, val loss: 1.5248631238937378
Epoch 720, training loss: 0.012702688574790955 = 0.005454618018120527 + 0.001 * 7.248069763183594
Epoch 720, val loss: 1.53415846824646
Epoch 730, training loss: 0.012414699420332909 = 0.005221162922680378 + 0.001 * 7.193536758422852
Epoch 730, val loss: 1.5432095527648926
Epoch 740, training loss: 0.012227618135511875 = 0.005003172904253006 + 0.001 * 7.22444486618042
Epoch 740, val loss: 1.551978349685669
Epoch 750, training loss: 0.011993076652288437 = 0.004799435846507549 + 0.001 * 7.19364070892334
Epoch 750, val loss: 1.560504674911499
Epoch 760, training loss: 0.011797616258263588 = 0.004608780145645142 + 0.001 * 7.188835620880127
Epoch 760, val loss: 1.5687986612319946
Epoch 770, training loss: 0.011615661904215813 = 0.004430122207850218 + 0.001 * 7.185539722442627
Epoch 770, val loss: 1.5768381357192993
Epoch 780, training loss: 0.011451369151473045 = 0.004262499511241913 + 0.001 * 7.188868999481201
Epoch 780, val loss: 1.5846514701843262
Epoch 790, training loss: 0.011317380703985691 = 0.004105105996131897 + 0.001 * 7.212274551391602
Epoch 790, val loss: 1.5922348499298096
Epoch 800, training loss: 0.011144084855914116 = 0.0039571188390254974 + 0.001 * 7.1869659423828125
Epoch 800, val loss: 1.5996202230453491
Epoch 810, training loss: 0.011018622666597366 = 0.003817831166088581 + 0.001 * 7.200790882110596
Epoch 810, val loss: 1.6067649126052856
Epoch 820, training loss: 0.010850047692656517 = 0.0036866285372525454 + 0.001 * 7.163418769836426
Epoch 820, val loss: 1.6137478351593018
Epoch 830, training loss: 0.010720191523432732 = 0.003562820376828313 + 0.001 * 7.157370567321777
Epoch 830, val loss: 1.6205071210861206
Epoch 840, training loss: 0.010610578581690788 = 0.0034459219314157963 + 0.001 * 7.164656639099121
Epoch 840, val loss: 1.6271086931228638
Epoch 850, training loss: 0.010490505956113338 = 0.003335461253300309 + 0.001 * 7.155044078826904
Epoch 850, val loss: 1.6335194110870361
Epoch 860, training loss: 0.010371966287493706 = 0.0032309857197105885 + 0.001 * 7.140979766845703
Epoch 860, val loss: 1.6397285461425781
Epoch 870, training loss: 0.010274482890963554 = 0.003132027108222246 + 0.001 * 7.142455577850342
Epoch 870, val loss: 1.6458135843276978
Epoch 880, training loss: 0.010179271921515465 = 0.0030381130054593086 + 0.001 * 7.141158580780029
Epoch 880, val loss: 1.6517125368118286
Epoch 890, training loss: 0.010106874629855156 = 0.002948752138763666 + 0.001 * 7.158121585845947
Epoch 890, val loss: 1.6574467420578003
Epoch 900, training loss: 0.009998039342463017 = 0.0028635268099606037 + 0.001 * 7.134512424468994
Epoch 900, val loss: 1.6630429029464722
Epoch 910, training loss: 0.00995849259197712 = 0.0027818053495138884 + 0.001 * 7.176686763763428
Epoch 910, val loss: 1.6685246229171753
Epoch 920, training loss: 0.009856761433184147 = 0.0027030399069190025 + 0.001 * 7.153721332550049
Epoch 920, val loss: 1.6738183498382568
Epoch 930, training loss: 0.009747521951794624 = 0.002626878209412098 + 0.001 * 7.1206440925598145
Epoch 930, val loss: 1.6789915561676025
Epoch 940, training loss: 0.009668982587754726 = 0.002553099999204278 + 0.001 * 7.11588191986084
Epoch 940, val loss: 1.6840476989746094
Epoch 950, training loss: 0.009622490033507347 = 0.002481782343238592 + 0.001 * 7.140707015991211
Epoch 950, val loss: 1.6889556646347046
Epoch 960, training loss: 0.009555104188621044 = 0.0024129068478941917 + 0.001 * 7.142197132110596
Epoch 960, val loss: 1.693738341331482
Epoch 970, training loss: 0.009458471089601517 = 0.002346509601920843 + 0.001 * 7.111961841583252
Epoch 970, val loss: 1.6983768939971924
Epoch 980, training loss: 0.009393155574798584 = 0.002282668836414814 + 0.001 * 7.11048698425293
Epoch 980, val loss: 1.7029284238815308
Epoch 990, training loss: 0.009346210397779942 = 0.0022213109768927097 + 0.001 * 7.124898910522461
Epoch 990, val loss: 1.7073222398757935
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 1.9488239288330078 = 1.9402270317077637 + 0.001 * 8.596874237060547
Epoch 0, val loss: 1.9325531721115112
Epoch 10, training loss: 1.9386588335037231 = 1.9300620555877686 + 0.001 * 8.59682559967041
Epoch 10, val loss: 1.922424077987671
Epoch 20, training loss: 1.9261281490325928 = 1.9175314903259277 + 0.001 * 8.596686363220215
Epoch 20, val loss: 1.9100737571716309
Epoch 30, training loss: 1.9088590145111084 = 1.900262713432312 + 0.001 * 8.59635066986084
Epoch 30, val loss: 1.8934447765350342
Epoch 40, training loss: 1.884116291999817 = 1.8755208253860474 + 0.001 * 8.595444679260254
Epoch 40, val loss: 1.870390772819519
Epoch 50, training loss: 1.8508139848709106 = 1.8422214984893799 + 0.001 * 8.592432022094727
Epoch 50, val loss: 1.8414080142974854
Epoch 60, training loss: 1.814800500869751 = 1.8062201738357544 + 0.001 * 8.58034896850586
Epoch 60, val loss: 1.8137930631637573
Epoch 70, training loss: 1.7822500467300415 = 1.7737284898757935 + 0.001 * 8.521499633789062
Epoch 70, val loss: 1.7888996601104736
Epoch 80, training loss: 1.739572286605835 = 1.7312960624694824 + 0.001 * 8.276212692260742
Epoch 80, val loss: 1.7495607137680054
Epoch 90, training loss: 1.679587960243225 = 1.6714434623718262 + 0.001 * 8.14447021484375
Epoch 90, val loss: 1.6953469514846802
Epoch 100, training loss: 1.600054383277893 = 1.5920178890228271 + 0.001 * 8.03646469116211
Epoch 100, val loss: 1.6297506093978882
Epoch 110, training loss: 1.5092761516571045 = 1.5013668537139893 + 0.001 * 7.909257411956787
Epoch 110, val loss: 1.5555061101913452
Epoch 120, training loss: 1.4175848960876465 = 1.4098937511444092 + 0.001 * 7.691201210021973
Epoch 120, val loss: 1.4816792011260986
Epoch 130, training loss: 1.328276515007019 = 1.320624589920044 + 0.001 * 7.65186882019043
Epoch 130, val loss: 1.4112310409545898
Epoch 140, training loss: 1.23954439163208 = 1.2319505214691162 + 0.001 * 7.593899726867676
Epoch 140, val loss: 1.3439494371414185
Epoch 150, training loss: 1.1509222984313965 = 1.1433809995651245 + 0.001 * 7.541337966918945
Epoch 150, val loss: 1.279815435409546
Epoch 160, training loss: 1.0628126859664917 = 1.0553250312805176 + 0.001 * 7.48769474029541
Epoch 160, val loss: 1.2190982103347778
Epoch 170, training loss: 0.9759230017662048 = 0.968476414680481 + 0.001 * 7.446597576141357
Epoch 170, val loss: 1.1609822511672974
Epoch 180, training loss: 0.891494631767273 = 0.8840577006340027 + 0.001 * 7.436930179595947
Epoch 180, val loss: 1.1058889627456665
Epoch 190, training loss: 0.8108124732971191 = 0.803382933139801 + 0.001 * 7.4295525550842285
Epoch 190, val loss: 1.054453730583191
Epoch 200, training loss: 0.7352085709571838 = 0.7277874946594238 + 0.001 * 7.421067237854004
Epoch 200, val loss: 1.0083978176116943
Epoch 210, training loss: 0.6660253405570984 = 0.6586143374443054 + 0.001 * 7.410995006561279
Epoch 210, val loss: 0.9697802662849426
Epoch 220, training loss: 0.6036020517349243 = 0.5962041616439819 + 0.001 * 7.397902011871338
Epoch 220, val loss: 0.9401600360870361
Epoch 230, training loss: 0.5471233129501343 = 0.5397396683692932 + 0.001 * 7.383637428283691
Epoch 230, val loss: 0.9193626046180725
Epoch 240, training loss: 0.4956896901130676 = 0.488318532705307 + 0.001 * 7.371147632598877
Epoch 240, val loss: 0.9065080285072327
Epoch 250, training loss: 0.44858604669570923 = 0.4412273168563843 + 0.001 * 7.358718395233154
Epoch 250, val loss: 0.9001312255859375
Epoch 260, training loss: 0.4052046537399292 = 0.39785632491111755 + 0.001 * 7.348330974578857
Epoch 260, val loss: 0.8986438512802124
Epoch 270, training loss: 0.36508703231811523 = 0.35771599411964417 + 0.001 * 7.371033668518066
Epoch 270, val loss: 0.9006801843643188
Epoch 280, training loss: 0.3278115391731262 = 0.32047247886657715 + 0.001 * 7.3390703201293945
Epoch 280, val loss: 0.9057711362838745
Epoch 290, training loss: 0.2932743430137634 = 0.28593817353248596 + 0.001 * 7.336182594299316
Epoch 290, val loss: 0.9134604334831238
Epoch 300, training loss: 0.2613849341869354 = 0.25405168533325195 + 0.001 * 7.333235740661621
Epoch 300, val loss: 0.9235161542892456
Epoch 310, training loss: 0.23223264515399933 = 0.2249019742012024 + 0.001 * 7.330672264099121
Epoch 310, val loss: 0.9356074929237366
Epoch 320, training loss: 0.20592953264713287 = 0.19860176742076874 + 0.001 * 7.3277716636657715
Epoch 320, val loss: 0.9496501684188843
Epoch 330, training loss: 0.18247610330581665 = 0.17514720559120178 + 0.001 * 7.328903675079346
Epoch 330, val loss: 0.9655665755271912
Epoch 340, training loss: 0.16173629462718964 = 0.1544092744588852 + 0.001 * 7.327020645141602
Epoch 340, val loss: 0.982974112033844
Epoch 350, training loss: 0.14350800216197968 = 0.1361863911151886 + 0.001 * 7.321617126464844
Epoch 350, val loss: 1.0014899969100952
Epoch 360, training loss: 0.12754398584365845 = 0.12022300064563751 + 0.001 * 7.320977210998535
Epoch 360, val loss: 1.0207915306091309
Epoch 370, training loss: 0.1135919988155365 = 0.10627475380897522 + 0.001 * 7.317245006561279
Epoch 370, val loss: 1.040643572807312
Epoch 380, training loss: 0.10143095254898071 = 0.09411542117595673 + 0.001 * 7.315528392791748
Epoch 380, val loss: 1.060793399810791
Epoch 390, training loss: 0.09084319323301315 = 0.08352921903133392 + 0.001 * 7.313973903656006
Epoch 390, val loss: 1.080922245979309
Epoch 400, training loss: 0.08162692934274673 = 0.07431457191705704 + 0.001 * 7.3123555183410645
Epoch 400, val loss: 1.10097074508667
Epoch 410, training loss: 0.0736033022403717 = 0.06629108637571335 + 0.001 * 7.312214374542236
Epoch 410, val loss: 1.1207962036132812
Epoch 420, training loss: 0.06660930067300797 = 0.05929907411336899 + 0.001 * 7.310229301452637
Epoch 420, val loss: 1.1402629613876343
Epoch 430, training loss: 0.06051396206021309 = 0.053199540823698044 + 0.001 * 7.314422130584717
Epoch 430, val loss: 1.15929114818573
Epoch 440, training loss: 0.05517825484275818 = 0.04787116497755051 + 0.001 * 7.307091236114502
Epoch 440, val loss: 1.1778444051742554
Epoch 450, training loss: 0.0505123995244503 = 0.043209414929151535 + 0.001 * 7.30298376083374
Epoch 450, val loss: 1.1958746910095215
Epoch 460, training loss: 0.046437472105026245 = 0.03912297263741493 + 0.001 * 7.314499855041504
Epoch 460, val loss: 1.213338851928711
Epoch 470, training loss: 0.042839113622903824 = 0.03553313761949539 + 0.001 * 7.305976867675781
Epoch 470, val loss: 1.2302604913711548
Epoch 480, training loss: 0.039673734456300735 = 0.0323718897998333 + 0.001 * 7.301845073699951
Epoch 480, val loss: 1.2466486692428589
Epoch 490, training loss: 0.03688478842377663 = 0.029580596834421158 + 0.001 * 7.304192543029785
Epoch 490, val loss: 1.2624304294586182
Epoch 500, training loss: 0.03440475091338158 = 0.027106452733278275 + 0.001 * 7.298298358917236
Epoch 500, val loss: 1.2777162790298462
Epoch 510, training loss: 0.032198693603277206 = 0.024902300909161568 + 0.001 * 7.296391010284424
Epoch 510, val loss: 1.2925050258636475
Epoch 520, training loss: 0.030225621536374092 = 0.02292996272444725 + 0.001 * 7.295658111572266
Epoch 520, val loss: 1.3068879842758179
Epoch 530, training loss: 0.028450092300772667 = 0.021161293610930443 + 0.001 * 7.2887983322143555
Epoch 530, val loss: 1.3208680152893066
Epoch 540, training loss: 0.026868708431720734 = 0.01957293413579464 + 0.001 * 7.295773029327393
Epoch 540, val loss: 1.3345130681991577
Epoch 550, training loss: 0.025426482781767845 = 0.018144316971302032 + 0.001 * 7.282166004180908
Epoch 550, val loss: 1.3477840423583984
Epoch 560, training loss: 0.024137593805789948 = 0.01685740426182747 + 0.001 * 7.280188083648682
Epoch 560, val loss: 1.3607264757156372
Epoch 570, training loss: 0.022985944524407387 = 0.015695996582508087 + 0.001 * 7.289947986602783
Epoch 570, val loss: 1.373306155204773
Epoch 580, training loss: 0.021923143416643143 = 0.014645939692854881 + 0.001 * 7.27720308303833
Epoch 580, val loss: 1.385549783706665
Epoch 590, training loss: 0.020967409014701843 = 0.013694679364562035 + 0.001 * 7.272729873657227
Epoch 590, val loss: 1.397458791732788
Epoch 600, training loss: 0.020121056586503983 = 0.01283116266131401 + 0.001 * 7.2898945808410645
Epoch 600, val loss: 1.409005880355835
Epoch 610, training loss: 0.019314954057335854 = 0.01204570010304451 + 0.001 * 7.269254207611084
Epoch 610, val loss: 1.4202474355697632
Epoch 620, training loss: 0.018620282411575317 = 0.01132974959909916 + 0.001 * 7.290532112121582
Epoch 620, val loss: 1.4311379194259644
Epoch 630, training loss: 0.017927274107933044 = 0.010675783269107342 + 0.001 * 7.251491546630859
Epoch 630, val loss: 1.4417215585708618
Epoch 640, training loss: 0.017321964725852013 = 0.010077246464788914 + 0.001 * 7.244717597961426
Epoch 640, val loss: 1.451986312866211
Epoch 650, training loss: 0.01679128408432007 = 0.009528365917503834 + 0.001 * 7.262917518615723
Epoch 650, val loss: 1.4619581699371338
Epoch 660, training loss: 0.01626480370759964 = 0.009023986756801605 + 0.001 * 7.240816116333008
Epoch 660, val loss: 1.4716476202011108
Epoch 670, training loss: 0.01583375409245491 = 0.008559679612517357 + 0.001 * 7.274075031280518
Epoch 670, val loss: 1.4810429811477661
Epoch 680, training loss: 0.015351491048932076 = 0.008131428621709347 + 0.001 * 7.220061779022217
Epoch 680, val loss: 1.490157961845398
Epoch 690, training loss: 0.014962872490286827 = 0.007735743187367916 + 0.001 * 7.2271294593811035
Epoch 690, val loss: 1.4989992380142212
Epoch 700, training loss: 0.014640862122178078 = 0.007369483355432749 + 0.001 * 7.271378993988037
Epoch 700, val loss: 1.5075907707214355
Epoch 710, training loss: 0.0142399612814188 = 0.007029959931969643 + 0.001 * 7.210000991821289
Epoch 710, val loss: 1.5159193277359009
Epoch 720, training loss: 0.0139207998290658 = 0.006714633200317621 + 0.001 * 7.2061662673950195
Epoch 720, val loss: 1.5240120887756348
Epoch 730, training loss: 0.013631324283778667 = 0.006421249825507402 + 0.001 * 7.210073947906494
Epoch 730, val loss: 1.5318607091903687
Epoch 740, training loss: 0.013353094458580017 = 0.006147821433842182 + 0.001 * 7.205272674560547
Epoch 740, val loss: 1.5394842624664307
Epoch 750, training loss: 0.013095453381538391 = 0.005892479792237282 + 0.001 * 7.202972888946533
Epoch 750, val loss: 1.5468910932540894
Epoch 760, training loss: 0.012853986583650112 = 0.0056533352471888065 + 0.001 * 7.200651168823242
Epoch 760, val loss: 1.554078221321106
Epoch 770, training loss: 0.012680177576839924 = 0.0054289172403514385 + 0.001 * 7.251259803771973
Epoch 770, val loss: 1.5610945224761963
Epoch 780, training loss: 0.012436896562576294 = 0.005217712372541428 + 0.001 * 7.219183921813965
Epoch 780, val loss: 1.5678958892822266
Epoch 790, training loss: 0.012229479849338531 = 0.005018333438783884 + 0.001 * 7.211145877838135
Epoch 790, val loss: 1.574532151222229
Epoch 800, training loss: 0.011997243389487267 = 0.004829809069633484 + 0.001 * 7.167434215545654
Epoch 800, val loss: 1.5809760093688965
Epoch 810, training loss: 0.011827741749584675 = 0.0046513560228049755 + 0.001 * 7.176385402679443
Epoch 810, val loss: 1.58725106716156
Epoch 820, training loss: 0.011674677021801472 = 0.004482092335820198 + 0.001 * 7.19258451461792
Epoch 820, val loss: 1.59336519241333
Epoch 830, training loss: 0.011508718132972717 = 0.004321564454585314 + 0.001 * 7.1871538162231445
Epoch 830, val loss: 1.5992997884750366
Epoch 840, training loss: 0.011339966207742691 = 0.0041692922823131084 + 0.001 * 7.170673847198486
Epoch 840, val loss: 1.6050995588302612
Epoch 850, training loss: 0.011203624308109283 = 0.004024783615022898 + 0.001 * 7.178839683532715
Epoch 850, val loss: 1.6107221841812134
Epoch 860, training loss: 0.011057740077376366 = 0.0038875669706612825 + 0.001 * 7.170172691345215
Epoch 860, val loss: 1.6162042617797852
Epoch 870, training loss: 0.010926587507128716 = 0.0037574071902781725 + 0.001 * 7.169180393218994
Epoch 870, val loss: 1.6215250492095947
Epoch 880, training loss: 0.010799488052725792 = 0.003633887739852071 + 0.001 * 7.165599822998047
Epoch 880, val loss: 1.6266919374465942
Epoch 890, training loss: 0.010667171329259872 = 0.0035165692679584026 + 0.001 * 7.150602340698242
Epoch 890, val loss: 1.6317087411880493
Epoch 900, training loss: 0.010574445128440857 = 0.003405237104743719 + 0.001 * 7.1692070960998535
Epoch 900, val loss: 1.6365880966186523
Epoch 910, training loss: 0.01044437289237976 = 0.0032995787914842367 + 0.001 * 7.144793510437012
Epoch 910, val loss: 1.6413171291351318
Epoch 920, training loss: 0.010330392979085445 = 0.003199151484295726 + 0.001 * 7.131241321563721
Epoch 920, val loss: 1.645907998085022
Epoch 930, training loss: 0.010243073105812073 = 0.003103608964011073 + 0.001 * 7.139463424682617
Epoch 930, val loss: 1.6503791809082031
Epoch 940, training loss: 0.010150624439120293 = 0.003012749832123518 + 0.001 * 7.137874126434326
Epoch 940, val loss: 1.6547319889068604
Epoch 950, training loss: 0.010096214711666107 = 0.0029263028409332037 + 0.001 * 7.169911861419678
Epoch 950, val loss: 1.6589455604553223
Epoch 960, training loss: 0.009962055832147598 = 0.0028439522720873356 + 0.001 * 7.118103504180908
Epoch 960, val loss: 1.6630486249923706
Epoch 970, training loss: 0.009925956837832928 = 0.0027654608711600304 + 0.001 * 7.160495758056641
Epoch 970, val loss: 1.6670559644699097
Epoch 980, training loss: 0.009821614250540733 = 0.002690668450668454 + 0.001 * 7.130945205688477
Epoch 980, val loss: 1.6709452867507935
Epoch 990, training loss: 0.009731060825288296 = 0.0026193917728960514 + 0.001 * 7.111668586730957
Epoch 990, val loss: 1.6747121810913086
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 1.9509824514389038 = 1.9423855543136597 + 0.001 * 8.59687614440918
Epoch 0, val loss: 1.9486809968948364
Epoch 10, training loss: 1.9405317306518555 = 1.9319348335266113 + 0.001 * 8.596845626831055
Epoch 10, val loss: 1.9377950429916382
Epoch 20, training loss: 1.928176760673523 = 1.9195799827575684 + 0.001 * 8.59673023223877
Epoch 20, val loss: 1.9247140884399414
Epoch 30, training loss: 1.9117203950881958 = 1.9031239748001099 + 0.001 * 8.596426963806152
Epoch 30, val loss: 1.9073082208633423
Epoch 40, training loss: 1.8886219263076782 = 1.8800262212753296 + 0.001 * 8.59566879272461
Epoch 40, val loss: 1.8832746744155884
Epoch 50, training loss: 1.8574844598770142 = 1.8488909006118774 + 0.001 * 8.59351921081543
Epoch 50, val loss: 1.8523582220077515
Epoch 60, training loss: 1.8224596977233887 = 1.813873291015625 + 0.001 * 8.586408615112305
Epoch 60, val loss: 1.820823073387146
Epoch 70, training loss: 1.791455626487732 = 1.7828965187072754 + 0.001 * 8.559078216552734
Epoch 70, val loss: 1.795412540435791
Epoch 80, training loss: 1.7545149326324463 = 1.7461051940917969 + 0.001 * 8.409754753112793
Epoch 80, val loss: 1.7629735469818115
Epoch 90, training loss: 1.7015118598937988 = 1.6933410167694092 + 0.001 * 8.170854568481445
Epoch 90, val loss: 1.7182611227035522
Epoch 100, training loss: 1.6279940605163574 = 1.6199324131011963 + 0.001 * 8.061662673950195
Epoch 100, val loss: 1.6589834690093994
Epoch 110, training loss: 1.5357199907302856 = 1.527727484703064 + 0.001 * 7.992558002471924
Epoch 110, val loss: 1.5841481685638428
Epoch 120, training loss: 1.4344664812088013 = 1.4265809059143066 + 0.001 * 7.885571002960205
Epoch 120, val loss: 1.5026662349700928
Epoch 130, training loss: 1.3333396911621094 = 1.3255715370178223 + 0.001 * 7.768137454986572
Epoch 130, val loss: 1.42367684841156
Epoch 140, training loss: 1.2377156019210815 = 1.2299832105636597 + 0.001 * 7.732387065887451
Epoch 140, val loss: 1.353960394859314
Epoch 150, training loss: 1.1488734483718872 = 1.1411950588226318 + 0.001 * 7.678377628326416
Epoch 150, val loss: 1.293257236480713
Epoch 160, training loss: 1.0649540424346924 = 1.0573217868804932 + 0.001 * 7.632275581359863
Epoch 160, val loss: 1.238113522529602
Epoch 170, training loss: 0.9837648868560791 = 0.9761775732040405 + 0.001 * 7.587299823760986
Epoch 170, val loss: 1.185280203819275
Epoch 180, training loss: 0.9044642448425293 = 0.8969172239303589 + 0.001 * 7.5470290184021
Epoch 180, val loss: 1.1338391304016113
Epoch 190, training loss: 0.8275233507156372 = 0.8200011849403381 + 0.001 * 7.52215051651001
Epoch 190, val loss: 1.0842550992965698
Epoch 200, training loss: 0.7545336484909058 = 0.7470248341560364 + 0.001 * 7.508835792541504
Epoch 200, val loss: 1.0390421152114868
Epoch 210, training loss: 0.687421441078186 = 0.6799167990684509 + 0.001 * 7.504663467407227
Epoch 210, val loss: 1.0008641481399536
Epoch 220, training loss: 0.627494215965271 = 0.6199937462806702 + 0.001 * 7.500468730926514
Epoch 220, val loss: 0.9719138741493225
Epoch 230, training loss: 0.5751506686210632 = 0.5676552653312683 + 0.001 * 7.495398998260498
Epoch 230, val loss: 0.9528605341911316
Epoch 240, training loss: 0.5299697518348694 = 0.5224793553352356 + 0.001 * 7.4903740882873535
Epoch 240, val loss: 0.9436041712760925
Epoch 250, training loss: 0.490795373916626 = 0.4833109974861145 + 0.001 * 7.484376430511475
Epoch 250, val loss: 0.9421668648719788
Epoch 260, training loss: 0.4559853971004486 = 0.44850826263427734 + 0.001 * 7.4771294593811035
Epoch 260, val loss: 0.9460499286651611
Epoch 270, training loss: 0.42368561029434204 = 0.41622108221054077 + 0.001 * 7.464526176452637
Epoch 270, val loss: 0.9525974988937378
Epoch 280, training loss: 0.392159640789032 = 0.3847109377384186 + 0.001 * 7.4487128257751465
Epoch 280, val loss: 0.959915816783905
Epoch 290, training loss: 0.36016646027565 = 0.35273152589797974 + 0.001 * 7.434926509857178
Epoch 290, val loss: 0.9669166803359985
Epoch 300, training loss: 0.3272319734096527 = 0.3198243975639343 + 0.001 * 7.407575607299805
Epoch 300, val loss: 0.9734610319137573
Epoch 310, training loss: 0.29392343759536743 = 0.28649938106536865 + 0.001 * 7.424066543579102
Epoch 310, val loss: 0.9800660014152527
Epoch 320, training loss: 0.26116347312927246 = 0.25378021597862244 + 0.001 * 7.383265495300293
Epoch 320, val loss: 0.987353503704071
Epoch 330, training loss: 0.2300991714000702 = 0.22273066639900208 + 0.001 * 7.368500232696533
Epoch 330, val loss: 0.9960455894470215
Epoch 340, training loss: 0.20154106616973877 = 0.1941838264465332 + 0.001 * 7.357242107391357
Epoch 340, val loss: 1.0068870782852173
Epoch 350, training loss: 0.17608724534511566 = 0.16871292889118195 + 0.001 * 7.374310493469238
Epoch 350, val loss: 1.0202844142913818
Epoch 360, training loss: 0.15388718247413635 = 0.14653442800045013 + 0.001 * 7.352755069732666
Epoch 360, val loss: 1.0362420082092285
Epoch 370, training loss: 0.13487109541893005 = 0.12752555310726166 + 0.001 * 7.34553861618042
Epoch 370, val loss: 1.0544450283050537
Epoch 380, training loss: 0.11870630085468292 = 0.11136136949062347 + 0.001 * 7.34492826461792
Epoch 380, val loss: 1.0744200944900513
Epoch 390, training loss: 0.10497930645942688 = 0.09763382375240326 + 0.001 * 7.345482349395752
Epoch 390, val loss: 1.0956350564956665
Epoch 400, training loss: 0.09330065548419952 = 0.08595652133226395 + 0.001 * 7.3441362380981445
Epoch 400, val loss: 1.1176661252975464
Epoch 410, training loss: 0.08332911878824234 = 0.07599271833896637 + 0.001 * 7.33640193939209
Epoch 410, val loss: 1.1402260065078735
Epoch 420, training loss: 0.07478970289230347 = 0.06744884699583054 + 0.001 * 7.34085750579834
Epoch 420, val loss: 1.1630834341049194
Epoch 430, training loss: 0.0674247071146965 = 0.06008783355355263 + 0.001 * 7.336871147155762
Epoch 430, val loss: 1.1860688924789429
Epoch 440, training loss: 0.06104935705661774 = 0.053717613220214844 + 0.001 * 7.331744194030762
Epoch 440, val loss: 1.2089626789093018
Epoch 450, training loss: 0.05551091209053993 = 0.0481802374124527 + 0.001 * 7.3306732177734375
Epoch 450, val loss: 1.2316052913665771
Epoch 460, training loss: 0.05068277567625046 = 0.04334546625614166 + 0.001 * 7.337307929992676
Epoch 460, val loss: 1.2538517713546753
Epoch 470, training loss: 0.04644334316253662 = 0.03911174461245537 + 0.001 * 7.331599712371826
Epoch 470, val loss: 1.2756215333938599
Epoch 480, training loss: 0.04272521287202835 = 0.035393912345170975 + 0.001 * 7.331298828125
Epoch 480, val loss: 1.2968326807022095
Epoch 490, training loss: 0.039441272616386414 = 0.032120928168296814 + 0.001 * 7.320345401763916
Epoch 490, val loss: 1.3174666166305542
Epoch 500, training loss: 0.03656289726495743 = 0.02923257276415825 + 0.001 * 7.330326080322266
Epoch 500, val loss: 1.3375076055526733
Epoch 510, training loss: 0.03400751203298569 = 0.026678405702114105 + 0.001 * 7.329106330871582
Epoch 510, val loss: 1.3568888902664185
Epoch 520, training loss: 0.03172827884554863 = 0.024415461346507072 + 0.001 * 7.31281852722168
Epoch 520, val loss: 1.3755964040756226
Epoch 530, training loss: 0.029726756736636162 = 0.022406253963708878 + 0.001 * 7.320502758026123
Epoch 530, val loss: 1.3936411142349243
Epoch 540, training loss: 0.02794690430164337 = 0.020618584007024765 + 0.001 * 7.3283209800720215
Epoch 540, val loss: 1.411035180091858
Epoch 550, training loss: 0.026339691132307053 = 0.01902421936392784 + 0.001 * 7.315471172332764
Epoch 550, val loss: 1.427801251411438
Epoch 560, training loss: 0.024903979152441025 = 0.017598897218704224 + 0.001 * 7.305081844329834
Epoch 560, val loss: 1.4439510107040405
Epoch 570, training loss: 0.02362748607993126 = 0.01632143184542656 + 0.001 * 7.306054592132568
Epoch 570, val loss: 1.4594875574111938
Epoch 580, training loss: 0.02246634103357792 = 0.015173767693340778 + 0.001 * 7.29257345199585
Epoch 580, val loss: 1.4744120836257935
Epoch 590, training loss: 0.021427379921078682 = 0.014140011742711067 + 0.001 * 7.287367343902588
Epoch 590, val loss: 1.4887908697128296
Epoch 600, training loss: 0.02050979807972908 = 0.013206366449594498 + 0.001 * 7.303431034088135
Epoch 600, val loss: 1.5026291608810425
Epoch 610, training loss: 0.01966187357902527 = 0.012361238710582256 + 0.001 * 7.300633907318115
Epoch 610, val loss: 1.51597261428833
Epoch 620, training loss: 0.018888399004936218 = 0.011594245210289955 + 0.001 * 7.294154167175293
Epoch 620, val loss: 1.5287836790084839
Epoch 630, training loss: 0.018184449523687363 = 0.010896514169871807 + 0.001 * 7.287934303283691
Epoch 630, val loss: 1.5411549806594849
Epoch 640, training loss: 0.017541484907269478 = 0.010260418988764286 + 0.001 * 7.281064987182617
Epoch 640, val loss: 1.5530376434326172
Epoch 650, training loss: 0.016949668526649475 = 0.009679094888269901 + 0.001 * 7.27057409286499
Epoch 650, val loss: 1.5644983053207397
Epoch 660, training loss: 0.016421103850007057 = 0.00914666149765253 + 0.001 * 7.274442672729492
Epoch 660, val loss: 1.5755627155303955
Epoch 670, training loss: 0.015924902632832527 = 0.008657942526042461 + 0.001 * 7.266959190368652
Epoch 670, val loss: 1.5862265825271606
Epoch 680, training loss: 0.015479881316423416 = 0.008208327926695347 + 0.001 * 7.2715535163879395
Epoch 680, val loss: 1.59650456905365
Epoch 690, training loss: 0.015057563781738281 = 0.007793872617185116 + 0.001 * 7.26369047164917
Epoch 690, val loss: 1.6064510345458984
Epoch 700, training loss: 0.014675965532660484 = 0.007410763297230005 + 0.001 * 7.265202045440674
Epoch 700, val loss: 1.616089105606079
Epoch 710, training loss: 0.014319132082164288 = 0.0070556765422225 + 0.001 * 7.263455390930176
Epoch 710, val loss: 1.62541663646698
Epoch 720, training loss: 0.013981642201542854 = 0.0067262486554682255 + 0.001 * 7.255392551422119
Epoch 720, val loss: 1.6344120502471924
Epoch 730, training loss: 0.013657493516802788 = 0.0064198351465165615 + 0.001 * 7.2376580238342285
Epoch 730, val loss: 1.6431678533554077
Epoch 740, training loss: 0.013433009386062622 = 0.006134324707090855 + 0.001 * 7.298684597015381
Epoch 740, val loss: 1.651638388633728
Epoch 750, training loss: 0.013130003586411476 = 0.005867681931704283 + 0.001 * 7.262321472167969
Epoch 750, val loss: 1.659852147102356
Epoch 760, training loss: 0.012855110689997673 = 0.005618129391223192 + 0.001 * 7.236981391906738
Epoch 760, val loss: 1.6678440570831299
Epoch 770, training loss: 0.012602163478732109 = 0.005384270567446947 + 0.001 * 7.217893123626709
Epoch 770, val loss: 1.675625205039978
Epoch 780, training loss: 0.012392180040478706 = 0.005164780654013157 + 0.001 * 7.227398872375488
Epoch 780, val loss: 1.6831958293914795
Epoch 790, training loss: 0.012182017788290977 = 0.0049588438123464584 + 0.001 * 7.223174095153809
Epoch 790, val loss: 1.6905295848846436
Epoch 800, training loss: 0.01198151707649231 = 0.004765157122164965 + 0.001 * 7.216359615325928
Epoch 800, val loss: 1.6976879835128784
Epoch 810, training loss: 0.011807216331362724 = 0.004582928027957678 + 0.001 * 7.224288463592529
Epoch 810, val loss: 1.7046282291412354
Epoch 820, training loss: 0.011622571386396885 = 0.004411365836858749 + 0.001 * 7.211205005645752
Epoch 820, val loss: 1.7113789319992065
Epoch 830, training loss: 0.011457134038209915 = 0.004249856807291508 + 0.001 * 7.207276821136475
Epoch 830, val loss: 1.7179508209228516
Epoch 840, training loss: 0.011305361986160278 = 0.0040975045412778854 + 0.001 * 7.207857608795166
Epoch 840, val loss: 1.7243402004241943
Epoch 850, training loss: 0.011193943209946156 = 0.00395370414480567 + 0.001 * 7.240238666534424
Epoch 850, val loss: 1.7305445671081543
Epoch 860, training loss: 0.01102661993354559 = 0.003817783435806632 + 0.001 * 7.208836555480957
Epoch 860, val loss: 1.7366124391555786
Epoch 870, training loss: 0.010899893939495087 = 0.003689252305775881 + 0.001 * 7.210640907287598
Epoch 870, val loss: 1.7425044775009155
Epoch 880, training loss: 0.010785097256302834 = 0.0035676504485309124 + 0.001 * 7.2174458503723145
Epoch 880, val loss: 1.7482370138168335
Epoch 890, training loss: 0.01066991314291954 = 0.003452448872849345 + 0.001 * 7.217463970184326
Epoch 890, val loss: 1.7538111209869385
Epoch 900, training loss: 0.010522056370973587 = 0.003343218704685569 + 0.001 * 7.178837776184082
Epoch 900, val loss: 1.7592593431472778
Epoch 910, training loss: 0.01044198963791132 = 0.0032395783346146345 + 0.001 * 7.202410697937012
Epoch 910, val loss: 1.7645407915115356
Epoch 920, training loss: 0.010314317420125008 = 0.003141154535114765 + 0.001 * 7.173162937164307
Epoch 920, val loss: 1.7696861028671265
Epoch 930, training loss: 0.010248292237520218 = 0.0030476078391075134 + 0.001 * 7.200684070587158
Epoch 930, val loss: 1.7747142314910889
Epoch 940, training loss: 0.010145855136215687 = 0.0029586346354335546 + 0.001 * 7.187220573425293
Epoch 940, val loss: 1.7795979976654053
Epoch 950, training loss: 0.010083602741360664 = 0.0028740542475134134 + 0.001 * 7.209548473358154
Epoch 950, val loss: 1.7843440771102905
Epoch 960, training loss: 0.009971646592020988 = 0.0027936880942434072 + 0.001 * 7.177958011627197
Epoch 960, val loss: 1.7889676094055176
Epoch 970, training loss: 0.009925197809934616 = 0.002717106370255351 + 0.001 * 7.2080912590026855
Epoch 970, val loss: 1.7934468984603882
Epoch 980, training loss: 0.009809990413486958 = 0.0026440771762281656 + 0.001 * 7.165913105010986
Epoch 980, val loss: 1.79784095287323
Epoch 990, training loss: 0.009746109135448933 = 0.002574434271082282 + 0.001 * 7.1716742515563965
Epoch 990, val loss: 1.8021150827407837
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8028465998945704
The final CL Acc:0.74444, 0.02362, The final GNN Acc:0.80777, 0.00358
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13078])
remove edge: torch.Size([2, 7826])
updated graph: torch.Size([2, 10348])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.962049961090088 = 1.9534530639648438 + 0.001 * 8.5968656539917
Epoch 0, val loss: 1.9557820558547974
Epoch 10, training loss: 1.9521700143814087 = 1.943573236465454 + 0.001 * 8.59682846069336
Epoch 10, val loss: 1.946208119392395
Epoch 20, training loss: 1.9399628639221191 = 1.931366205215454 + 0.001 * 8.596689224243164
Epoch 20, val loss: 1.9338408708572388
Epoch 30, training loss: 1.9224649667739868 = 1.9138686656951904 + 0.001 * 8.596331596374512
Epoch 30, val loss: 1.915608525276184
Epoch 40, training loss: 1.8957862854003906 = 1.887190818786621 + 0.001 * 8.595449447631836
Epoch 40, val loss: 1.8875997066497803
Epoch 50, training loss: 1.8574777841567993 = 1.8488848209381104 + 0.001 * 8.592918395996094
Epoch 50, val loss: 1.8486207723617554
Epoch 60, training loss: 1.8131752014160156 = 1.804591417312622 + 0.001 * 8.583776473999023
Epoch 60, val loss: 1.8069690465927124
Epoch 70, training loss: 1.7754186391830444 = 1.766872763633728 + 0.001 * 8.545910835266113
Epoch 70, val loss: 1.7741608619689941
Epoch 80, training loss: 1.7314339876174927 = 1.72309410572052 + 0.001 * 8.339922904968262
Epoch 80, val loss: 1.7333701848983765
Epoch 90, training loss: 1.669551134109497 = 1.6614463329315186 + 0.001 * 8.104804992675781
Epoch 90, val loss: 1.675525188446045
Epoch 100, training loss: 1.5857805013656616 = 1.5779212713241577 + 0.001 * 7.8592529296875
Epoch 100, val loss: 1.5994807481765747
Epoch 110, training loss: 1.4826847314834595 = 1.4750041961669922 + 0.001 * 7.680495262145996
Epoch 110, val loss: 1.5092041492462158
Epoch 120, training loss: 1.370349645614624 = 1.3627476692199707 + 0.001 * 7.60202693939209
Epoch 120, val loss: 1.4125800132751465
Epoch 130, training loss: 1.2574225664138794 = 1.2498726844787598 + 0.001 * 7.549934387207031
Epoch 130, val loss: 1.3179097175598145
Epoch 140, training loss: 1.149245262145996 = 1.1417351961135864 + 0.001 * 7.510039329528809
Epoch 140, val loss: 1.2300105094909668
Epoch 150, training loss: 1.0493148565292358 = 1.0418440103530884 + 0.001 * 7.4708380699157715
Epoch 150, val loss: 1.1506682634353638
Epoch 160, training loss: 0.9584562182426453 = 0.9510173201560974 + 0.001 * 7.43891716003418
Epoch 160, val loss: 1.0795961618423462
Epoch 170, training loss: 0.8747962117195129 = 0.8673753142356873 + 0.001 * 7.420922756195068
Epoch 170, val loss: 1.015426754951477
Epoch 180, training loss: 0.7953441143035889 = 0.7879306674003601 + 0.001 * 7.413450717926025
Epoch 180, val loss: 0.9551841616630554
Epoch 190, training loss: 0.7185092568397522 = 0.7111004590988159 + 0.001 * 7.408817291259766
Epoch 190, val loss: 0.8977251052856445
Epoch 200, training loss: 0.6446560025215149 = 0.6372525691986084 + 0.001 * 7.403459548950195
Epoch 200, val loss: 0.8436474800109863
Epoch 210, training loss: 0.5752282738685608 = 0.5678316354751587 + 0.001 * 7.396658897399902
Epoch 210, val loss: 0.7943811416625977
Epoch 220, training loss: 0.5116320848464966 = 0.5042445659637451 + 0.001 * 7.387491226196289
Epoch 220, val loss: 0.7518754005432129
Epoch 230, training loss: 0.45454877614974976 = 0.44717174768447876 + 0.001 * 7.3770294189453125
Epoch 230, val loss: 0.7172260880470276
Epoch 240, training loss: 0.4037313163280487 = 0.3963664770126343 + 0.001 * 7.3648505210876465
Epoch 240, val loss: 0.6897249817848206
Epoch 250, training loss: 0.35829615592956543 = 0.3509474992752075 + 0.001 * 7.3486456871032715
Epoch 250, val loss: 0.6676304936408997
Epoch 260, training loss: 0.3172816336154938 = 0.3099478483200073 + 0.001 * 7.3337721824646
Epoch 260, val loss: 0.6495429277420044
Epoch 270, training loss: 0.28000766038894653 = 0.2726862132549286 + 0.001 * 7.321460723876953
Epoch 270, val loss: 0.6346669793128967
Epoch 280, training loss: 0.24616824090480804 = 0.23885637521743774 + 0.001 * 7.311871528625488
Epoch 280, val loss: 0.6227011680603027
Epoch 290, training loss: 0.21566393971443176 = 0.20837457478046417 + 0.001 * 7.289368152618408
Epoch 290, val loss: 0.6136623620986938
Epoch 300, training loss: 0.18855425715446472 = 0.18126682937145233 + 0.001 * 7.287420749664307
Epoch 300, val loss: 0.6073586940765381
Epoch 310, training loss: 0.16476207971572876 = 0.1574929803609848 + 0.001 * 7.269102573394775
Epoch 310, val loss: 0.6037536263465881
Epoch 320, training loss: 0.14412368834018707 = 0.13686586916446686 + 0.001 * 7.257813930511475
Epoch 320, val loss: 0.6024443507194519
Epoch 330, training loss: 0.12635423243045807 = 0.11909274756908417 + 0.001 * 7.261483669281006
Epoch 330, val loss: 0.6032012701034546
Epoch 340, training loss: 0.11108946800231934 = 0.10383861511945724 + 0.001 * 7.250853061676025
Epoch 340, val loss: 0.6057541370391846
Epoch 350, training loss: 0.0980115756392479 = 0.09077140688896179 + 0.001 * 7.240167140960693
Epoch 350, val loss: 0.6098158955574036
Epoch 360, training loss: 0.08682438731193542 = 0.07959425449371338 + 0.001 * 7.230134010314941
Epoch 360, val loss: 0.6151991486549377
Epoch 370, training loss: 0.07729937136173248 = 0.07004406303167343 + 0.001 * 7.255309104919434
Epoch 370, val loss: 0.6216748356819153
Epoch 380, training loss: 0.06911855190992355 = 0.061887435615062714 + 0.001 * 7.231115341186523
Epoch 380, val loss: 0.6290333867073059
Epoch 390, training loss: 0.06213715672492981 = 0.05491573363542557 + 0.001 * 7.221420764923096
Epoch 390, val loss: 0.6371005177497864
Epoch 400, training loss: 0.05616552010178566 = 0.04893994703888893 + 0.001 * 7.225571155548096
Epoch 400, val loss: 0.6457162499427795
Epoch 410, training loss: 0.05101746320724487 = 0.04380308464169502 + 0.001 * 7.214378356933594
Epoch 410, val loss: 0.654706597328186
Epoch 420, training loss: 0.04658326506614685 = 0.039370521903038025 + 0.001 * 7.212743282318115
Epoch 420, val loss: 0.6639326810836792
Epoch 430, training loss: 0.042764950543642044 = 0.03553050756454468 + 0.001 * 7.234442710876465
Epoch 430, val loss: 0.6732919812202454
Epoch 440, training loss: 0.039395153522491455 = 0.03218999132514 + 0.001 * 7.205160140991211
Epoch 440, val loss: 0.6826900243759155
Epoch 450, training loss: 0.036474648863077164 = 0.02927287109196186 + 0.001 * 7.201778411865234
Epoch 450, val loss: 0.6920190453529358
Epoch 460, training loss: 0.03391951695084572 = 0.026716064661741257 + 0.001 * 7.203453540802002
Epoch 460, val loss: 0.7012318968772888
Epoch 470, training loss: 0.03165882080793381 = 0.024466097354888916 + 0.001 * 7.192723751068115
Epoch 470, val loss: 0.7102853655815125
Epoch 480, training loss: 0.02966216951608658 = 0.022478671744465828 + 0.001 * 7.183496952056885
Epoch 480, val loss: 0.719153881072998
Epoch 490, training loss: 0.027907228097319603 = 0.020716961473226547 + 0.001 * 7.190265655517578
Epoch 490, val loss: 0.7278433442115784
Epoch 500, training loss: 0.026333849877119064 = 0.0191498976200819 + 0.001 * 7.183952808380127
Epoch 500, val loss: 0.7363213896751404
Epoch 510, training loss: 0.024928521364927292 = 0.017751449719071388 + 0.001 * 7.177070617675781
Epoch 510, val loss: 0.7445698976516724
Epoch 520, training loss: 0.02368701994419098 = 0.016499757766723633 + 0.001 * 7.187262058258057
Epoch 520, val loss: 0.7526008486747742
Epoch 530, training loss: 0.022549396380782127 = 0.015375589951872826 + 0.001 * 7.173806190490723
Epoch 530, val loss: 0.7604197859764099
Epoch 540, training loss: 0.021522488445043564 = 0.014362858608365059 + 0.001 * 7.159628868103027
Epoch 540, val loss: 0.7680140137672424
Epoch 550, training loss: 0.020623905584216118 = 0.013447835110127926 + 0.001 * 7.176069736480713
Epoch 550, val loss: 0.7754086852073669
Epoch 560, training loss: 0.019788702949881554 = 0.01261887513101101 + 0.001 * 7.169827938079834
Epoch 560, val loss: 0.7825953364372253
Epoch 570, training loss: 0.01902332529425621 = 0.011865820735692978 + 0.001 * 7.157503604888916
Epoch 570, val loss: 0.7895737290382385
Epoch 580, training loss: 0.018331781029701233 = 0.011179951950907707 + 0.001 * 7.151828765869141
Epoch 580, val loss: 0.7963690161705017
Epoch 590, training loss: 0.01770072430372238 = 0.010553664527833462 + 0.001 * 7.147059917449951
Epoch 590, val loss: 0.802970826625824
Epoch 600, training loss: 0.01713554747402668 = 0.009980357252061367 + 0.001 * 7.155190467834473
Epoch 600, val loss: 0.8093916177749634
Epoch 610, training loss: 0.016604240983724594 = 0.009454431012272835 + 0.001 * 7.149809837341309
Epoch 610, val loss: 0.8156272172927856
Epoch 620, training loss: 0.016114268451929092 = 0.008970849215984344 + 0.001 * 7.143418788909912
Epoch 620, val loss: 0.8217018246650696
Epoch 630, training loss: 0.01568112149834633 = 0.008525189012289047 + 0.001 * 7.155932426452637
Epoch 630, val loss: 0.8276029229164124
Epoch 640, training loss: 0.015253463760018349 = 0.008113705553114414 + 0.001 * 7.139758110046387
Epoch 640, val loss: 0.8333633542060852
Epoch 650, training loss: 0.014883209019899368 = 0.007732993457466364 + 0.001 * 7.150215148925781
Epoch 650, val loss: 0.8389686942100525
Epoch 660, training loss: 0.01450715959072113 = 0.007380071561783552 + 0.001 * 7.1270880699157715
Epoch 660, val loss: 0.8444133400917053
Epoch 670, training loss: 0.014185754582285881 = 0.007052394095808268 + 0.001 * 7.133360385894775
Epoch 670, val loss: 0.8497243523597717
Epoch 680, training loss: 0.013884276151657104 = 0.006747611798346043 + 0.001 * 7.136663913726807
Epoch 680, val loss: 0.8548932671546936
Epoch 690, training loss: 0.013590425252914429 = 0.006463618483394384 + 0.001 * 7.126806735992432
Epoch 690, val loss: 0.8599250912666321
Epoch 700, training loss: 0.013318859040737152 = 0.0061986190266907215 + 0.001 * 7.120240211486816
Epoch 700, val loss: 0.864842414855957
Epoch 710, training loss: 0.013080410659313202 = 0.005950897000730038 + 0.001 * 7.129513263702393
Epoch 710, val loss: 0.869633674621582
Epoch 720, training loss: 0.012832936830818653 = 0.005718981847167015 + 0.001 * 7.113954544067383
Epoch 720, val loss: 0.8743228316307068
Epoch 730, training loss: 0.012616580352187157 = 0.00550157530233264 + 0.001 * 7.115005016326904
Epoch 730, val loss: 0.8788911700248718
Epoch 740, training loss: 0.012417837977409363 = 0.005297529045492411 + 0.001 * 7.120308876037598
Epoch 740, val loss: 0.8833518028259277
Epoch 750, training loss: 0.012212950736284256 = 0.005105731077492237 + 0.001 * 7.107219219207764
Epoch 750, val loss: 0.8877095580101013
Epoch 760, training loss: 0.01204846054315567 = 0.004925212822854519 + 0.001 * 7.123247146606445
Epoch 760, val loss: 0.8919696807861328
Epoch 770, training loss: 0.011859440244734287 = 0.00475515890866518 + 0.001 * 7.104280948638916
Epoch 770, val loss: 0.8961313962936401
Epoch 780, training loss: 0.011694458313286304 = 0.00459469947963953 + 0.001 * 7.099758625030518
Epoch 780, val loss: 0.9001868963241577
Epoch 790, training loss: 0.011543236672878265 = 0.004443198908120394 + 0.001 * 7.100037574768066
Epoch 790, val loss: 0.9041595458984375
Epoch 800, training loss: 0.011392403393983841 = 0.00429998803883791 + 0.001 * 7.092414855957031
Epoch 800, val loss: 0.90804123878479
Epoch 810, training loss: 0.011273077689111233 = 0.004164401907473803 + 0.001 * 7.108675479888916
Epoch 810, val loss: 0.9118245244026184
Epoch 820, training loss: 0.011144602671265602 = 0.004035991616547108 + 0.001 * 7.108610153198242
Epoch 820, val loss: 0.9155330657958984
Epoch 830, training loss: 0.01100287213921547 = 0.0039141979068517685 + 0.001 * 7.088674545288086
Epoch 830, val loss: 0.9191542267799377
Epoch 840, training loss: 0.01087888516485691 = 0.003798589576035738 + 0.001 * 7.080295562744141
Epoch 840, val loss: 0.9227066040039062
Epoch 850, training loss: 0.010785875841975212 = 0.00368876033462584 + 0.001 * 7.0971150398254395
Epoch 850, val loss: 0.9261850118637085
Epoch 860, training loss: 0.010675891302525997 = 0.0035843453370034695 + 0.001 * 7.091545581817627
Epoch 860, val loss: 0.9295806884765625
Epoch 870, training loss: 0.010574538260698318 = 0.0034849978983402252 + 0.001 * 7.089540481567383
Epoch 870, val loss: 0.9329092502593994
Epoch 880, training loss: 0.010475954040884972 = 0.003390355035662651 + 0.001 * 7.085598468780518
Epoch 880, val loss: 0.9361646175384521
Epoch 890, training loss: 0.010368246585130692 = 0.0033001478295773268 + 0.001 * 7.068098068237305
Epoch 890, val loss: 0.9393572807312012
Epoch 900, training loss: 0.010299010202288628 = 0.003214086638763547 + 0.001 * 7.084923267364502
Epoch 900, val loss: 0.9424809813499451
Epoch 910, training loss: 0.010199450887739658 = 0.003131965873762965 + 0.001 * 7.0674848556518555
Epoch 910, val loss: 0.9455407857894897
Epoch 920, training loss: 0.01013156771659851 = 0.0030534998513758183 + 0.001 * 7.078067779541016
Epoch 920, val loss: 0.9485353827476501
Epoch 930, training loss: 0.010046320036053658 = 0.0029785102233290672 + 0.001 * 7.067809104919434
Epoch 930, val loss: 0.9514729976654053
Epoch 940, training loss: 0.009993880987167358 = 0.002906763693317771 + 0.001 * 7.0871171951293945
Epoch 940, val loss: 0.9543540477752686
Epoch 950, training loss: 0.009908424690365791 = 0.002838096348568797 + 0.001 * 7.070328235626221
Epoch 950, val loss: 0.9571678042411804
Epoch 960, training loss: 0.009835660457611084 = 0.002772305393591523 + 0.001 * 7.0633544921875
Epoch 960, val loss: 0.9599353075027466
Epoch 970, training loss: 0.009770049713551998 = 0.002709240186959505 + 0.001 * 7.060809135437012
Epoch 970, val loss: 0.9626545310020447
Epoch 980, training loss: 0.00971006229519844 = 0.0026487598661333323 + 0.001 * 7.0613017082214355
Epoch 980, val loss: 0.9653106927871704
Epoch 990, training loss: 0.009652232751250267 = 0.002590708900243044 + 0.001 * 7.061523914337158
Epoch 990, val loss: 0.9679304361343384
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 1.9684582948684692 = 1.959861397743225 + 0.001 * 8.596869468688965
Epoch 0, val loss: 1.959197998046875
Epoch 10, training loss: 1.957621693611145 = 1.9490249156951904 + 0.001 * 8.596808433532715
Epoch 10, val loss: 1.9482229948043823
Epoch 20, training loss: 1.9440497159957886 = 1.9354530572891235 + 0.001 * 8.596635818481445
Epoch 20, val loss: 1.9344111680984497
Epoch 30, training loss: 1.9250924587249756 = 1.9164962768554688 + 0.001 * 8.596219062805176
Epoch 30, val loss: 1.9151489734649658
Epoch 40, training loss: 1.8972903490066528 = 1.888695240020752 + 0.001 * 8.59510326385498
Epoch 40, val loss: 1.887175440788269
Epoch 50, training loss: 1.858535885810852 = 1.8499442338943481 + 0.001 * 8.591599464416504
Epoch 50, val loss: 1.8496818542480469
Epoch 60, training loss: 1.814354658126831 = 1.8057758808135986 + 0.001 * 8.57873821258545
Epoch 60, val loss: 1.8106706142425537
Epoch 70, training loss: 1.7760810852050781 = 1.7675565481185913 + 0.001 * 8.524479866027832
Epoch 70, val loss: 1.7797709703445435
Epoch 80, training loss: 1.7327176332473755 = 1.724470853805542 + 0.001 * 8.246816635131836
Epoch 80, val loss: 1.7410575151443481
Epoch 90, training loss: 1.6735854148864746 = 1.6654822826385498 + 0.001 * 8.103155136108398
Epoch 90, val loss: 1.6867698431015015
Epoch 100, training loss: 1.593479871749878 = 1.585498571395874 + 0.001 * 7.981270790100098
Epoch 100, val loss: 1.615012526512146
Epoch 110, training loss: 1.4968715906143188 = 1.4890413284301758 + 0.001 * 7.830316066741943
Epoch 110, val loss: 1.5324978828430176
Epoch 120, training loss: 1.3951002359390259 = 1.3874733448028564 + 0.001 * 7.626864910125732
Epoch 120, val loss: 1.4481220245361328
Epoch 130, training loss: 1.2934331893920898 = 1.2858421802520752 + 0.001 * 7.590958118438721
Epoch 130, val loss: 1.3659805059432983
Epoch 140, training loss: 1.1923789978027344 = 1.1848433017730713 + 0.001 * 7.535749912261963
Epoch 140, val loss: 1.285993218421936
Epoch 150, training loss: 1.0939239263534546 = 1.0864366292953491 + 0.001 * 7.487311840057373
Epoch 150, val loss: 1.2083261013031006
Epoch 160, training loss: 1.0005439519882202 = 0.9930930137634277 + 0.001 * 7.450880527496338
Epoch 160, val loss: 1.1350075006484985
Epoch 170, training loss: 0.9140605330467224 = 0.9066455364227295 + 0.001 * 7.414973258972168
Epoch 170, val loss: 1.0668363571166992
Epoch 180, training loss: 0.8355515599250793 = 0.8281635642051697 + 0.001 * 7.3879828453063965
Epoch 180, val loss: 1.0055651664733887
Epoch 190, training loss: 0.76527339220047 = 0.7578995823860168 + 0.001 * 7.373799800872803
Epoch 190, val loss: 0.9523594379425049
Epoch 200, training loss: 0.7027947902679443 = 0.6954330205917358 + 0.001 * 7.361749172210693
Epoch 200, val loss: 0.9078431725502014
Epoch 210, training loss: 0.6467040777206421 = 0.6393572688102722 + 0.001 * 7.3467864990234375
Epoch 210, val loss: 0.8715519905090332
Epoch 220, training loss: 0.5951943397521973 = 0.5878646373748779 + 0.001 * 7.329709529876709
Epoch 220, val loss: 0.8421741127967834
Epoch 230, training loss: 0.5469030141830444 = 0.5395838618278503 + 0.001 * 7.319161891937256
Epoch 230, val loss: 0.818091094493866
Epoch 240, training loss: 0.5011468529701233 = 0.49383899569511414 + 0.001 * 7.307856559753418
Epoch 240, val loss: 0.7980520129203796
Epoch 250, training loss: 0.457552045583725 = 0.4502502977848053 + 0.001 * 7.3017578125
Epoch 250, val loss: 0.7809526920318604
Epoch 260, training loss: 0.4158424437046051 = 0.4085453152656555 + 0.001 * 7.297130107879639
Epoch 260, val loss: 0.7659894227981567
Epoch 270, training loss: 0.3761206865310669 = 0.36881929636001587 + 0.001 * 7.301403045654297
Epoch 270, val loss: 0.7530074119567871
Epoch 280, training loss: 0.3388667404651642 = 0.3315761387348175 + 0.001 * 7.29058837890625
Epoch 280, val loss: 0.7421861886978149
Epoch 290, training loss: 0.3045752942562103 = 0.2972860634326935 + 0.001 * 7.2892303466796875
Epoch 290, val loss: 0.7340871691703796
Epoch 300, training loss: 0.27310362458229065 = 0.26581844687461853 + 0.001 * 7.285184860229492
Epoch 300, val loss: 0.7284578680992126
Epoch 310, training loss: 0.24373312294483185 = 0.23645217716693878 + 0.001 * 7.280951023101807
Epoch 310, val loss: 0.7246424555778503
Epoch 320, training loss: 0.21557126939296722 = 0.20829233527183533 + 0.001 * 7.278928279876709
Epoch 320, val loss: 0.7222486138343811
Epoch 330, training loss: 0.18829895555973053 = 0.18102268874645233 + 0.001 * 7.276273250579834
Epoch 330, val loss: 0.721264660358429
Epoch 340, training loss: 0.16261592507362366 = 0.15534156560897827 + 0.001 * 7.274352550506592
Epoch 340, val loss: 0.7221043705940247
Epoch 350, training loss: 0.13962571322917938 = 0.13235026597976685 + 0.001 * 7.275448799133301
Epoch 350, val loss: 0.7253648638725281
Epoch 360, training loss: 0.1200060322880745 = 0.11273681372404099 + 0.001 * 7.2692179679870605
Epoch 360, val loss: 0.7310121655464172
Epoch 370, training loss: 0.10373241454362869 = 0.09646493196487427 + 0.001 * 7.267481327056885
Epoch 370, val loss: 0.7385551333427429
Epoch 380, training loss: 0.0903349295258522 = 0.08306726813316345 + 0.001 * 7.26766300201416
Epoch 380, val loss: 0.7474068403244019
Epoch 390, training loss: 0.07927103340625763 = 0.07200022786855698 + 0.001 * 7.270803928375244
Epoch 390, val loss: 0.7571443319320679
Epoch 400, training loss: 0.0700516626238823 = 0.06279046088457108 + 0.001 * 7.261202812194824
Epoch 400, val loss: 0.7674025893211365
Epoch 410, training loss: 0.06232569366693497 = 0.05506697669625282 + 0.001 * 7.2587151527404785
Epoch 410, val loss: 0.7779681086540222
Epoch 420, training loss: 0.05580993741750717 = 0.048550717532634735 + 0.001 * 7.259220123291016
Epoch 420, val loss: 0.7886545658111572
Epoch 430, training loss: 0.05028584599494934 = 0.04302574694156647 + 0.001 * 7.260097026824951
Epoch 430, val loss: 0.7993344068527222
Epoch 440, training loss: 0.04558033123612404 = 0.0383194163441658 + 0.001 * 7.260913848876953
Epoch 440, val loss: 0.809906005859375
Epoch 450, training loss: 0.04154078662395477 = 0.03428655490279198 + 0.001 * 7.254232883453369
Epoch 450, val loss: 0.8203393816947937
Epoch 460, training loss: 0.03806415572762489 = 0.030810177326202393 + 0.001 * 7.253979682922363
Epoch 460, val loss: 0.830575704574585
Epoch 470, training loss: 0.0350496843457222 = 0.027799993753433228 + 0.001 * 7.249689102172852
Epoch 470, val loss: 0.840627908706665
Epoch 480, training loss: 0.03242894262075424 = 0.025184890255331993 + 0.001 * 7.2440505027771
Epoch 480, val loss: 0.8504601716995239
Epoch 490, training loss: 0.030198512598872185 = 0.022905677556991577 + 0.001 * 7.292834281921387
Epoch 490, val loss: 0.8600630164146423
Epoch 500, training loss: 0.028170112520456314 = 0.020912759006023407 + 0.001 * 7.257352828979492
Epoch 500, val loss: 0.8694058060646057
Epoch 510, training loss: 0.02640436962246895 = 0.019163867458701134 + 0.001 * 7.2405009269714355
Epoch 510, val loss: 0.8784970045089722
Epoch 520, training loss: 0.024861738085746765 = 0.017623038962483406 + 0.001 * 7.238699436187744
Epoch 520, val loss: 0.8873357176780701
Epoch 530, training loss: 0.023523345589637756 = 0.01626049540936947 + 0.001 * 7.262850761413574
Epoch 530, val loss: 0.8959424495697021
Epoch 540, training loss: 0.022276317700743675 = 0.015051322057843208 + 0.001 * 7.2249956130981445
Epoch 540, val loss: 0.9042897820472717
Epoch 550, training loss: 0.02119331993162632 = 0.013974171131849289 + 0.001 * 7.2191481590271
Epoch 550, val loss: 0.9123983383178711
Epoch 560, training loss: 0.02023358643054962 = 0.013011178933084011 + 0.001 * 7.222407817840576
Epoch 560, val loss: 0.9202904105186462
Epoch 570, training loss: 0.019367100670933723 = 0.012147286906838417 + 0.001 * 7.219812870025635
Epoch 570, val loss: 0.9279751181602478
Epoch 580, training loss: 0.018591105937957764 = 0.01136970054358244 + 0.001 * 7.221405506134033
Epoch 580, val loss: 0.9354339241981506
Epoch 590, training loss: 0.017885182052850723 = 0.010667507536709309 + 0.001 * 7.2176737785339355
Epoch 590, val loss: 0.9426625967025757
Epoch 600, training loss: 0.017269374802708626 = 0.010031350888311863 + 0.001 * 7.2380242347717285
Epoch 600, val loss: 0.9496985673904419
Epoch 610, training loss: 0.016668863594532013 = 0.00945329200476408 + 0.001 * 7.215571403503418
Epoch 610, val loss: 0.9565229415893555
Epoch 620, training loss: 0.016136329621076584 = 0.008926589973270893 + 0.001 * 7.209738731384277
Epoch 620, val loss: 0.963170051574707
Epoch 630, training loss: 0.015678854659199715 = 0.008445372804999352 + 0.001 * 7.233481407165527
Epoch 630, val loss: 0.9696396589279175
Epoch 640, training loss: 0.015205265022814274 = 0.008004617877304554 + 0.001 * 7.200646877288818
Epoch 640, val loss: 0.9759305715560913
Epoch 650, training loss: 0.014797857031226158 = 0.007599864155054092 + 0.001 * 7.197992324829102
Epoch 650, val loss: 0.9820605516433716
Epoch 660, training loss: 0.014420505613088608 = 0.007227291353046894 + 0.001 * 7.19321346282959
Epoch 660, val loss: 0.9880123734474182
Epoch 670, training loss: 0.01410929299890995 = 0.006883673369884491 + 0.001 * 7.225618839263916
Epoch 670, val loss: 0.9938169121742249
Epoch 680, training loss: 0.013754372484982014 = 0.00656601507216692 + 0.001 * 7.188356876373291
Epoch 680, val loss: 0.9994693398475647
Epoch 690, training loss: 0.013485041446983814 = 0.006271714344620705 + 0.001 * 7.213326930999756
Epoch 690, val loss: 1.0049787759780884
Epoch 700, training loss: 0.013185622170567513 = 0.005998483393341303 + 0.001 * 7.18713903427124
Epoch 700, val loss: 1.010351300239563
Epoch 710, training loss: 0.012927819043397903 = 0.005744266323745251 + 0.001 * 7.183552265167236
Epoch 710, val loss: 1.0155879259109497
Epoch 720, training loss: 0.012667989358305931 = 0.00550692155957222 + 0.001 * 7.161067962646484
Epoch 720, val loss: 1.0207208395004272
Epoch 730, training loss: 0.01244698092341423 = 0.005284249782562256 + 0.001 * 7.162731170654297
Epoch 730, val loss: 1.0258079767227173
Epoch 740, training loss: 0.0122201107442379 = 0.005074081011116505 + 0.001 * 7.146029949188232
Epoch 740, val loss: 1.030848503112793
Epoch 750, training loss: 0.012027030810713768 = 0.004874464124441147 + 0.001 * 7.152565956115723
Epoch 750, val loss: 1.0359281301498413
Epoch 760, training loss: 0.01183500699698925 = 0.00468430994078517 + 0.001 * 7.150696277618408
Epoch 760, val loss: 1.041019082069397
Epoch 770, training loss: 0.011650173924863338 = 0.0045030852779746056 + 0.001 * 7.147088527679443
Epoch 770, val loss: 1.046127438545227
Epoch 780, training loss: 0.011491816490888596 = 0.004330514930188656 + 0.001 * 7.1613006591796875
Epoch 780, val loss: 1.0512527227401733
Epoch 790, training loss: 0.011316606774926186 = 0.004166631028056145 + 0.001 * 7.149974822998047
Epoch 790, val loss: 1.0563596487045288
Epoch 800, training loss: 0.011153139173984528 = 0.00401110528036952 + 0.001 * 7.142033576965332
Epoch 800, val loss: 1.0614246129989624
Epoch 810, training loss: 0.011007239110767841 = 0.0038636436220258474 + 0.001 * 7.143594741821289
Epoch 810, val loss: 1.0664687156677246
Epoch 820, training loss: 0.010870345868170261 = 0.0037239189259707928 + 0.001 * 7.146426677703857
Epoch 820, val loss: 1.0714646577835083
Epoch 830, training loss: 0.010749067179858685 = 0.003591483924537897 + 0.001 * 7.157582759857178
Epoch 830, val loss: 1.0763978958129883
Epoch 840, training loss: 0.010607368312776089 = 0.003465999849140644 + 0.001 * 7.1413679122924805
Epoch 840, val loss: 1.081284999847412
Epoch 850, training loss: 0.010489353910088539 = 0.003347079735249281 + 0.001 * 7.142274379730225
Epoch 850, val loss: 1.0860850811004639
Epoch 860, training loss: 0.010358343832194805 = 0.003234387608245015 + 0.001 * 7.123955726623535
Epoch 860, val loss: 1.0908409357070923
Epoch 870, training loss: 0.010252620093524456 = 0.0031274983193725348 + 0.001 * 7.125121116638184
Epoch 870, val loss: 1.0955127477645874
Epoch 880, training loss: 0.010152933187782764 = 0.0030259524937719107 + 0.001 * 7.126980304718018
Epoch 880, val loss: 1.1001362800598145
Epoch 890, training loss: 0.01005521509796381 = 0.0029295124113559723 + 0.001 * 7.125702381134033
Epoch 890, val loss: 1.1046767234802246
Epoch 900, training loss: 0.009942471981048584 = 0.0028376851696521044 + 0.001 * 7.104786396026611
Epoch 900, val loss: 1.1091536283493042
Epoch 910, training loss: 0.009865074418485165 = 0.00275016319938004 + 0.001 * 7.114911079406738
Epoch 910, val loss: 1.1135869026184082
Epoch 920, training loss: 0.009802895598113537 = 0.0026664945762604475 + 0.001 * 7.1364006996154785
Epoch 920, val loss: 1.1179821491241455
Epoch 930, training loss: 0.009685923345386982 = 0.002586665563285351 + 0.001 * 7.099257469177246
Epoch 930, val loss: 1.1223068237304688
Epoch 940, training loss: 0.009592948481440544 = 0.0025103925727307796 + 0.001 * 7.082555294036865
Epoch 940, val loss: 1.126581072807312
Epoch 950, training loss: 0.009529082104563713 = 0.0024373442865908146 + 0.001 * 7.091737747192383
Epoch 950, val loss: 1.130819320678711
Epoch 960, training loss: 0.009493404068052769 = 0.002367388689890504 + 0.001 * 7.1260151863098145
Epoch 960, val loss: 1.1350014209747314
Epoch 970, training loss: 0.009410148486495018 = 0.0023003227543085814 + 0.001 * 7.109825134277344
Epoch 970, val loss: 1.1391404867172241
Epoch 980, training loss: 0.00935276597738266 = 0.0022360042203217745 + 0.001 * 7.116761684417725
Epoch 980, val loss: 1.1432266235351562
Epoch 990, training loss: 0.009262450970709324 = 0.0021743192337453365 + 0.001 * 7.088131427764893
Epoch 990, val loss: 1.1472702026367188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 1.9587702751159668 = 1.9501733779907227 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.9468492269515991
Epoch 10, training loss: 1.948359727859497 = 1.9397629499435425 + 0.001 * 8.59679889678955
Epoch 10, val loss: 1.936747670173645
Epoch 20, training loss: 1.9354586601257324 = 1.9268620014190674 + 0.001 * 8.596620559692383
Epoch 20, val loss: 1.923811674118042
Epoch 30, training loss: 1.9174253940582275 = 1.9088292121887207 + 0.001 * 8.596198081970215
Epoch 30, val loss: 1.905513048171997
Epoch 40, training loss: 1.8911443948745728 = 1.8825492858886719 + 0.001 * 8.595147132873535
Epoch 40, val loss: 1.8788859844207764
Epoch 50, training loss: 1.854766845703125 = 1.846174716949463 + 0.001 * 8.59216022491455
Epoch 50, val loss: 1.8434592485427856
Epoch 60, training loss: 1.8132826089859009 = 1.8047007322311401 + 0.001 * 8.581863403320312
Epoch 60, val loss: 1.8069101572036743
Epoch 70, training loss: 1.7767564058303833 = 1.76821768283844 + 0.001 * 8.538668632507324
Epoch 70, val loss: 1.7782795429229736
Epoch 80, training loss: 1.7337181568145752 = 1.7253916263580322 + 0.001 * 8.32658863067627
Epoch 80, val loss: 1.741136074066162
Epoch 90, training loss: 1.6726704835891724 = 1.6645206212997437 + 0.001 * 8.149863243103027
Epoch 90, val loss: 1.6866658926010132
Epoch 100, training loss: 1.5916454792022705 = 1.5836282968521118 + 0.001 * 8.017138481140137
Epoch 100, val loss: 1.6164653301239014
Epoch 110, training loss: 1.4980006217956543 = 1.4901236295700073 + 0.001 * 7.877012252807617
Epoch 110, val loss: 1.5395233631134033
Epoch 120, training loss: 1.402547836303711 = 1.394898533821106 + 0.001 * 7.649245738983154
Epoch 120, val loss: 1.461147665977478
Epoch 130, training loss: 1.3073798418045044 = 1.2998558282852173 + 0.001 * 7.524001121520996
Epoch 130, val loss: 1.384932279586792
Epoch 140, training loss: 1.2098307609558105 = 1.202378749847412 + 0.001 * 7.4520583152771
Epoch 140, val loss: 1.3073540925979614
Epoch 150, training loss: 1.1082195043563843 = 1.100797414779663 + 0.001 * 7.422104835510254
Epoch 150, val loss: 1.2265596389770508
Epoch 160, training loss: 1.005109190940857 = 0.9976939558982849 + 0.001 * 7.415194511413574
Epoch 160, val loss: 1.144960641860962
Epoch 170, training loss: 0.9051650762557983 = 0.8977556228637695 + 0.001 * 7.409466743469238
Epoch 170, val loss: 1.0665254592895508
Epoch 180, training loss: 0.8135075569152832 = 0.8061009049415588 + 0.001 * 7.406637191772461
Epoch 180, val loss: 0.9957024455070496
Epoch 190, training loss: 0.733261227607727 = 0.725857675075531 + 0.001 * 7.403532028198242
Epoch 190, val loss: 0.935469388961792
Epoch 200, training loss: 0.664313018321991 = 0.6569125652313232 + 0.001 * 7.4004292488098145
Epoch 200, val loss: 0.8865225911140442
Epoch 210, training loss: 0.6046878695487976 = 0.5972901582717896 + 0.001 * 7.397721767425537
Epoch 210, val loss: 0.8475250005722046
Epoch 220, training loss: 0.5523855090141296 = 0.5449897646903992 + 0.001 * 7.395722389221191
Epoch 220, val loss: 0.816817581653595
Epoch 230, training loss: 0.5060405731201172 = 0.4986461102962494 + 0.001 * 7.394443035125732
Epoch 230, val loss: 0.7927175164222717
Epoch 240, training loss: 0.4645584225654602 = 0.4571647047996521 + 0.001 * 7.393727779388428
Epoch 240, val loss: 0.774172306060791
Epoch 250, training loss: 0.42685315012931824 = 0.41945984959602356 + 0.001 * 7.393303394317627
Epoch 250, val loss: 0.7600532174110413
Epoch 260, training loss: 0.3919759690761566 = 0.38458311557769775 + 0.001 * 7.392851829528809
Epoch 260, val loss: 0.7495458126068115
Epoch 270, training loss: 0.35933205485343933 = 0.35193994641304016 + 0.001 * 7.392122268676758
Epoch 270, val loss: 0.7418836355209351
Epoch 280, training loss: 0.32872074842453003 = 0.32132989168167114 + 0.001 * 7.39085054397583
Epoch 280, val loss: 0.7366861701011658
Epoch 290, training loss: 0.30005913972854614 = 0.2926686108112335 + 0.001 * 7.390541076660156
Epoch 290, val loss: 0.7337546944618225
Epoch 300, training loss: 0.27306872606277466 = 0.2656819224357605 + 0.001 * 7.386794567108154
Epoch 300, val loss: 0.7328059077262878
Epoch 310, training loss: 0.24738863110542297 = 0.24000650644302368 + 0.001 * 7.382122993469238
Epoch 310, val loss: 0.7335926294326782
Epoch 320, training loss: 0.22270721197128296 = 0.21533280611038208 + 0.001 * 7.3744025230407715
Epoch 320, val loss: 0.7362629771232605
Epoch 330, training loss: 0.19914892315864563 = 0.19178436696529388 + 0.001 * 7.364559650421143
Epoch 330, val loss: 0.7409265041351318
Epoch 340, training loss: 0.17715880274772644 = 0.16980881989002228 + 0.001 * 7.3499884605407715
Epoch 340, val loss: 0.7478221654891968
Epoch 350, training loss: 0.15724340081214905 = 0.14991585910320282 + 0.001 * 7.327545166015625
Epoch 350, val loss: 0.7569321393966675
Epoch 360, training loss: 0.13965541124343872 = 0.13235574960708618 + 0.001 * 7.299657344818115
Epoch 360, val loss: 0.7681387662887573
Epoch 370, training loss: 0.12436699867248535 = 0.11705833673477173 + 0.001 * 7.30866003036499
Epoch 370, val loss: 0.7812203168869019
Epoch 380, training loss: 0.11105117201805115 = 0.10376346111297607 + 0.001 * 7.287710666656494
Epoch 380, val loss: 0.7955304980278015
Epoch 390, training loss: 0.09945017844438553 = 0.09217356145381927 + 0.001 * 7.276613712310791
Epoch 390, val loss: 0.810757577419281
Epoch 400, training loss: 0.08931219577789307 = 0.08203977346420288 + 0.001 * 7.272421836853027
Epoch 400, val loss: 0.8265866041183472
Epoch 410, training loss: 0.08042728900909424 = 0.07315950840711594 + 0.001 * 7.267778396606445
Epoch 410, val loss: 0.8426799774169922
Epoch 420, training loss: 0.07262541353702545 = 0.06536201387643814 + 0.001 * 7.263401985168457
Epoch 420, val loss: 0.8590166568756104
Epoch 430, training loss: 0.06576882302761078 = 0.05850748345255852 + 0.001 * 7.261340141296387
Epoch 430, val loss: 0.875331461429596
Epoch 440, training loss: 0.05976276844739914 = 0.05248689278960228 + 0.001 * 7.275875091552734
Epoch 440, val loss: 0.8915431499481201
Epoch 450, training loss: 0.054461680352687836 = 0.047203414142131805 + 0.001 * 7.258264541625977
Epoch 450, val loss: 0.9075285196304321
Epoch 460, training loss: 0.049816057085990906 = 0.04256525635719299 + 0.001 * 7.250800609588623
Epoch 460, val loss: 0.9231659173965454
Epoch 470, training loss: 0.0457470566034317 = 0.03848901763558388 + 0.001 * 7.25803804397583
Epoch 470, val loss: 0.9384773373603821
Epoch 480, training loss: 0.04214385524392128 = 0.03490026295185089 + 0.001 * 7.243590354919434
Epoch 480, val loss: 0.9533509612083435
Epoch 490, training loss: 0.03896995633840561 = 0.0317310132086277 + 0.001 * 7.2389445304870605
Epoch 490, val loss: 0.9677509665489197
Epoch 500, training loss: 0.03617451712489128 = 0.028922952711582184 + 0.001 * 7.251563549041748
Epoch 500, val loss: 0.9816802740097046
Epoch 510, training loss: 0.03365994244813919 = 0.026427336037158966 + 0.001 * 7.23260498046875
Epoch 510, val loss: 0.9951364994049072
Epoch 520, training loss: 0.03142588213086128 = 0.024202847853302956 + 0.001 * 7.223033905029297
Epoch 520, val loss: 1.0081508159637451
Epoch 530, training loss: 0.029460104182362556 = 0.022215234115719795 + 0.001 * 7.244870185852051
Epoch 530, val loss: 1.0207353830337524
Epoch 540, training loss: 0.027651915326714516 = 0.020435631275177002 + 0.001 * 7.216283321380615
Epoch 540, val loss: 1.032894253730774
Epoch 550, training loss: 0.02604023739695549 = 0.018836788833141327 + 0.001 * 7.20344877243042
Epoch 550, val loss: 1.0446720123291016
Epoch 560, training loss: 0.02459803782403469 = 0.017394348978996277 + 0.001 * 7.203688144683838
Epoch 560, val loss: 1.0561413764953613
Epoch 570, training loss: 0.023298777639865875 = 0.016090285032987595 + 0.001 * 7.208492279052734
Epoch 570, val loss: 1.0673059225082397
Epoch 580, training loss: 0.022168857976794243 = 0.01491048838943243 + 0.001 * 7.258368968963623
Epoch 580, val loss: 1.0782498121261597
Epoch 590, training loss: 0.0210530087351799 = 0.013843636959791183 + 0.001 * 7.209371566772461
Epoch 590, val loss: 1.0889085531234741
Epoch 600, training loss: 0.02006564661860466 = 0.01287844218313694 + 0.001 * 7.187203407287598
Epoch 600, val loss: 1.0993051528930664
Epoch 610, training loss: 0.019235506653785706 = 0.012004747055470943 + 0.001 * 7.2307586669921875
Epoch 610, val loss: 1.1094253063201904
Epoch 620, training loss: 0.01840784028172493 = 0.011213341727852821 + 0.001 * 7.1944990158081055
Epoch 620, val loss: 1.119282603263855
Epoch 630, training loss: 0.01767704263329506 = 0.010495609603822231 + 0.001 * 7.181432723999023
Epoch 630, val loss: 1.1288717985153198
Epoch 640, training loss: 0.017034336924552917 = 0.00984377134591341 + 0.001 * 7.1905646324157715
Epoch 640, val loss: 1.1381756067276
Epoch 650, training loss: 0.016425147652626038 = 0.009250432252883911 + 0.001 * 7.174715042114258
Epoch 650, val loss: 1.147232174873352
Epoch 660, training loss: 0.01588677242398262 = 0.008708649314939976 + 0.001 * 7.178122043609619
Epoch 660, val loss: 1.1560418605804443
Epoch 670, training loss: 0.015425319783389568 = 0.008212046697735786 + 0.001 * 7.213272571563721
Epoch 670, val loss: 1.1646337509155273
Epoch 680, training loss: 0.014921478927135468 = 0.007755485828965902 + 0.001 * 7.165992259979248
Epoch 680, val loss: 1.1729742288589478
Epoch 690, training loss: 0.014520907774567604 = 0.007334314752370119 + 0.001 * 7.1865925788879395
Epoch 690, val loss: 1.1811187267303467
Epoch 700, training loss: 0.014119699597358704 = 0.006944829598069191 + 0.001 * 7.174869537353516
Epoch 700, val loss: 1.1890552043914795
Epoch 710, training loss: 0.0137547068297863 = 0.006584025453776121 + 0.001 * 7.170680522918701
Epoch 710, val loss: 1.1967862844467163
Epoch 720, training loss: 0.013401921838521957 = 0.00624966761097312 + 0.001 * 7.1522536277771
Epoch 720, val loss: 1.204362154006958
Epoch 730, training loss: 0.01311902143061161 = 0.005939406808465719 + 0.001 * 7.1796135902404785
Epoch 730, val loss: 1.2117242813110352
Epoch 740, training loss: 0.01280074380338192 = 0.0056513152085244656 + 0.001 * 7.149428367614746
Epoch 740, val loss: 1.218910813331604
Epoch 750, training loss: 0.012525320053100586 = 0.005383568350225687 + 0.001 * 7.141750812530518
Epoch 750, val loss: 1.225928783416748
Epoch 760, training loss: 0.012288982048630714 = 0.005134595092386007 + 0.001 * 7.154386043548584
Epoch 760, val loss: 1.2327711582183838
Epoch 770, training loss: 0.01205351296812296 = 0.004902827087789774 + 0.001 * 7.1506853103637695
Epoch 770, val loss: 1.2394487857818604
Epoch 780, training loss: 0.01182912103831768 = 0.004686884116381407 + 0.001 * 7.142236709594727
Epoch 780, val loss: 1.2459893226623535
Epoch 790, training loss: 0.011651173233985901 = 0.004485566169023514 + 0.001 * 7.165606498718262
Epoch 790, val loss: 1.2523043155670166
Epoch 800, training loss: 0.011455368250608444 = 0.004297579638659954 + 0.001 * 7.1577887535095215
Epoch 800, val loss: 1.2584749460220337
Epoch 810, training loss: 0.011277541518211365 = 0.004121938720345497 + 0.001 * 7.155602931976318
Epoch 810, val loss: 1.2645193338394165
Epoch 820, training loss: 0.011092374101281166 = 0.003957620821893215 + 0.001 * 7.13475227355957
Epoch 820, val loss: 1.2703847885131836
Epoch 830, training loss: 0.010923918336629868 = 0.0038036692421883345 + 0.001 * 7.120248317718506
Epoch 830, val loss: 1.2760982513427734
Epoch 840, training loss: 0.010789236985147 = 0.0036593053955584764 + 0.001 * 7.1299309730529785
Epoch 840, val loss: 1.2816647291183472
Epoch 850, training loss: 0.010643254965543747 = 0.0035237304400652647 + 0.001 * 7.119524002075195
Epoch 850, val loss: 1.2870745658874512
Epoch 860, training loss: 0.010514108464121819 = 0.003396267769858241 + 0.001 * 7.11784029006958
Epoch 860, val loss: 1.2923603057861328
Epoch 870, training loss: 0.010406361892819405 = 0.003276356030255556 + 0.001 * 7.130005359649658
Epoch 870, val loss: 1.297513484954834
Epoch 880, training loss: 0.010286073200404644 = 0.0031634799670428038 + 0.001 * 7.122592449188232
Epoch 880, val loss: 1.3025398254394531
Epoch 890, training loss: 0.01018932368606329 = 0.0030570568051189184 + 0.001 * 7.132266998291016
Epoch 890, val loss: 1.307416319847107
Epoch 900, training loss: 0.010068142786622047 = 0.0029566397424787283 + 0.001 * 7.111502647399902
Epoch 900, val loss: 1.3121720552444458
Epoch 910, training loss: 0.009960100054740906 = 0.002861754037439823 + 0.001 * 7.09834623336792
Epoch 910, val loss: 1.3168179988861084
Epoch 920, training loss: 0.009897706098854542 = 0.002772035775706172 + 0.001 * 7.125670433044434
Epoch 920, val loss: 1.3213586807250977
Epoch 930, training loss: 0.00980348326265812 = 0.0026871769223362207 + 0.001 * 7.116305828094482
Epoch 930, val loss: 1.325787901878357
Epoch 940, training loss: 0.009728295728564262 = 0.0026067779399454594 + 0.001 * 7.121517658233643
Epoch 940, val loss: 1.3300939798355103
Epoch 950, training loss: 0.009626215323805809 = 0.002530553610995412 + 0.001 * 7.095661163330078
Epoch 950, val loss: 1.3343148231506348
Epoch 960, training loss: 0.009555578231811523 = 0.002458209404721856 + 0.001 * 7.097368240356445
Epoch 960, val loss: 1.3383891582489014
Epoch 970, training loss: 0.00947378110140562 = 0.0023894866462796926 + 0.001 * 7.084294319152832
Epoch 970, val loss: 1.3423858880996704
Epoch 980, training loss: 0.009422089904546738 = 0.0023241504095494747 + 0.001 * 7.097939491271973
Epoch 980, val loss: 1.3462902307510376
Epoch 990, training loss: 0.009359928779304028 = 0.0022619569208472967 + 0.001 * 7.0979719161987305
Epoch 990, val loss: 1.3501124382019043
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8371112282551397
The final CL Acc:0.80864, 0.00462, The final GNN Acc:0.83676, 0.00050
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11656])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10594])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9405730962753296 = 1.931976318359375 + 0.001 * 8.5968017578125
Epoch 0, val loss: 1.9240119457244873
Epoch 10, training loss: 1.931313395500183 = 1.9227166175842285 + 0.001 * 8.596747398376465
Epoch 10, val loss: 1.915649652481079
Epoch 20, training loss: 1.9200314283370972 = 1.9114348888397217 + 0.001 * 8.5965576171875
Epoch 20, val loss: 1.9050785303115845
Epoch 30, training loss: 1.9044419527053833 = 1.8958457708358765 + 0.001 * 8.596168518066406
Epoch 30, val loss: 1.8902682065963745
Epoch 40, training loss: 1.8819319009780884 = 1.8733365535736084 + 0.001 * 8.595314979553223
Epoch 40, val loss: 1.8690780401229858
Epoch 50, training loss: 1.8514155149459839 = 1.8428224325180054 + 0.001 * 8.593027114868164
Epoch 50, val loss: 1.8415120840072632
Epoch 60, training loss: 1.8176792860031128 = 1.8090938329696655 + 0.001 * 8.585454940795898
Epoch 60, val loss: 1.8132245540618896
Epoch 70, training loss: 1.7862601280212402 = 1.777706503868103 + 0.001 * 8.553624153137207
Epoch 70, val loss: 1.7866203784942627
Epoch 80, training loss: 1.7464137077331543 = 1.7380818128585815 + 0.001 * 8.33192253112793
Epoch 80, val loss: 1.7495049238204956
Epoch 90, training loss: 1.6904826164245605 = 1.6824270486831665 + 0.001 * 8.055534362792969
Epoch 90, val loss: 1.6986844539642334
Epoch 100, training loss: 1.6136366128921509 = 1.6057372093200684 + 0.001 * 7.8993635177612305
Epoch 100, val loss: 1.6328837871551514
Epoch 110, training loss: 1.517265796661377 = 1.5095535516738892 + 0.001 * 7.71221399307251
Epoch 110, val loss: 1.5516520738601685
Epoch 120, training loss: 1.4103258848190308 = 1.4026752710342407 + 0.001 * 7.650607585906982
Epoch 120, val loss: 1.4626933336257935
Epoch 130, training loss: 1.301012396812439 = 1.2933876514434814 + 0.001 * 7.624722957611084
Epoch 130, val loss: 1.3736404180526733
Epoch 140, training loss: 1.193724513053894 = 1.1861292123794556 + 0.001 * 7.595327854156494
Epoch 140, val loss: 1.290213942527771
Epoch 150, training loss: 1.09126615524292 = 1.0837221145629883 + 0.001 * 7.544043064117432
Epoch 150, val loss: 1.2141079902648926
Epoch 160, training loss: 0.9950574040412903 = 0.9875982403755188 + 0.001 * 7.459163665771484
Epoch 160, val loss: 1.1450018882751465
Epoch 170, training loss: 0.9062114953994751 = 0.8988174200057983 + 0.001 * 7.394060134887695
Epoch 170, val loss: 1.0830036401748657
Epoch 180, training loss: 0.8256364464759827 = 0.8182502388954163 + 0.001 * 7.386203765869141
Epoch 180, val loss: 1.0285203456878662
Epoch 190, training loss: 0.753697395324707 = 0.7463205456733704 + 0.001 * 7.3768229484558105
Epoch 190, val loss: 0.9823408126831055
Epoch 200, training loss: 0.6896613836288452 = 0.6822894215583801 + 0.001 * 7.371936798095703
Epoch 200, val loss: 0.94449383020401
Epoch 210, training loss: 0.6317631602287292 = 0.6243996620178223 + 0.001 * 7.363518238067627
Epoch 210, val loss: 0.9134061336517334
Epoch 220, training loss: 0.5782511234283447 = 0.5708985328674316 + 0.001 * 7.352566242218018
Epoch 220, val loss: 0.887346088886261
Epoch 230, training loss: 0.5278583765029907 = 0.5205222368240356 + 0.001 * 7.336134910583496
Epoch 230, val loss: 0.8645134568214417
Epoch 240, training loss: 0.4799211025238037 = 0.4726075232028961 + 0.001 * 7.31359338760376
Epoch 240, val loss: 0.8439818024635315
Epoch 250, training loss: 0.4343051016330719 = 0.4270167052745819 + 0.001 * 7.288407325744629
Epoch 250, val loss: 0.8258160948753357
Epoch 260, training loss: 0.3911066949367523 = 0.38383203744888306 + 0.001 * 7.274669170379639
Epoch 260, val loss: 0.8102156519889832
Epoch 270, training loss: 0.3504408299922943 = 0.3431878685951233 + 0.001 * 7.2529754638671875
Epoch 270, val loss: 0.7973300814628601
Epoch 280, training loss: 0.31246915459632874 = 0.30522236227989197 + 0.001 * 7.246777534484863
Epoch 280, val loss: 0.7867801785469055
Epoch 290, training loss: 0.27721405029296875 = 0.26997390389442444 + 0.001 * 7.240158557891846
Epoch 290, val loss: 0.7785964012145996
Epoch 300, training loss: 0.24462877213954926 = 0.23739254474639893 + 0.001 * 7.236222743988037
Epoch 300, val loss: 0.7727272510528564
Epoch 310, training loss: 0.214668408036232 = 0.20743605494499207 + 0.001 * 7.232349395751953
Epoch 310, val loss: 0.7690648436546326
Epoch 320, training loss: 0.1873633861541748 = 0.1801362931728363 + 0.001 * 7.227085113525391
Epoch 320, val loss: 0.7675312161445618
Epoch 330, training loss: 0.16284453868865967 = 0.15562225878238678 + 0.001 * 7.222271919250488
Epoch 330, val loss: 0.7682811617851257
Epoch 340, training loss: 0.14124278724193573 = 0.13401858508586884 + 0.001 * 7.2242045402526855
Epoch 340, val loss: 0.7714921832084656
Epoch 350, training loss: 0.12254329025745392 = 0.11533593386411667 + 0.001 * 7.207353591918945
Epoch 350, val loss: 0.7772836089134216
Epoch 360, training loss: 0.10663771629333496 = 0.09942857176065445 + 0.001 * 7.2091474533081055
Epoch 360, val loss: 0.7853601574897766
Epoch 370, training loss: 0.093211330473423 = 0.08602222800254822 + 0.001 * 7.1890997886657715
Epoch 370, val loss: 0.7952916026115417
Epoch 380, training loss: 0.08197027444839478 = 0.07477697730064392 + 0.001 * 7.193297863006592
Epoch 380, val loss: 0.8067872524261475
Epoch 390, training loss: 0.07253749668598175 = 0.0653529167175293 + 0.001 * 7.184577941894531
Epoch 390, val loss: 0.8194955587387085
Epoch 400, training loss: 0.06461039185523987 = 0.057438261806964874 + 0.001 * 7.172131061553955
Epoch 400, val loss: 0.8330279588699341
Epoch 410, training loss: 0.05793003365397453 = 0.05076208710670471 + 0.001 * 7.167945384979248
Epoch 410, val loss: 0.8470057845115662
Epoch 420, training loss: 0.05226648226380348 = 0.04510064423084259 + 0.001 * 7.165837287902832
Epoch 420, val loss: 0.8612357378005981
Epoch 430, training loss: 0.04744182154536247 = 0.04027701914310455 + 0.001 * 7.164801120758057
Epoch 430, val loss: 0.8755242228507996
Epoch 440, training loss: 0.04331306740641594 = 0.03614642098546028 + 0.001 * 7.166647434234619
Epoch 440, val loss: 0.8896648287773132
Epoch 450, training loss: 0.039761174470186234 = 0.03259061276912689 + 0.001 * 7.170562267303467
Epoch 450, val loss: 0.9035444259643555
Epoch 460, training loss: 0.03668113425374031 = 0.029514025896787643 + 0.001 * 7.16710901260376
Epoch 460, val loss: 0.9171429872512817
Epoch 470, training loss: 0.0340004488825798 = 0.02683909609913826 + 0.001 * 7.161353588104248
Epoch 470, val loss: 0.93036288022995
Epoch 480, training loss: 0.03166523203253746 = 0.024503031745553017 + 0.001 * 7.162201404571533
Epoch 480, val loss: 0.9431880712509155
Epoch 490, training loss: 0.029620304703712463 = 0.022453956305980682 + 0.001 * 7.166348457336426
Epoch 490, val loss: 0.9556301832199097
Epoch 500, training loss: 0.02780967950820923 = 0.02064915932714939 + 0.001 * 7.160519599914551
Epoch 500, val loss: 0.9676946401596069
Epoch 510, training loss: 0.026211220771074295 = 0.01905304752290249 + 0.001 * 7.158172607421875
Epoch 510, val loss: 0.979395866394043
Epoch 520, training loss: 0.024799518287181854 = 0.017636189237236977 + 0.001 * 7.163328647613525
Epoch 520, val loss: 0.9907512664794922
Epoch 530, training loss: 0.02353425696492195 = 0.016373703256249428 + 0.001 * 7.160553455352783
Epoch 530, val loss: 1.0017521381378174
Epoch 540, training loss: 0.022402483969926834 = 0.015244647860527039 + 0.001 * 7.157835006713867
Epoch 540, val loss: 1.0123705863952637
Epoch 550, training loss: 0.02139381878077984 = 0.014231584966182709 + 0.001 * 7.162233352661133
Epoch 550, val loss: 1.0226562023162842
Epoch 560, training loss: 0.020477168262004852 = 0.013319590128958225 + 0.001 * 7.157578468322754
Epoch 560, val loss: 1.032619833946228
Epoch 570, training loss: 0.019650308415293694 = 0.012495854869484901 + 0.001 * 7.154453754425049
Epoch 570, val loss: 1.0422866344451904
Epoch 580, training loss: 0.01890379935503006 = 0.011749436147511005 + 0.001 * 7.15436315536499
Epoch 580, val loss: 1.051639437675476
Epoch 590, training loss: 0.018228644505143166 = 0.011071175336837769 + 0.001 * 7.157469272613525
Epoch 590, val loss: 1.0607222318649292
Epoch 600, training loss: 0.017603937536478043 = 0.010453113354742527 + 0.001 * 7.150824069976807
Epoch 600, val loss: 1.0695050954818726
Epoch 610, training loss: 0.01703808084130287 = 0.00988829880952835 + 0.001 * 7.149781703948975
Epoch 610, val loss: 1.0780327320098877
Epoch 620, training loss: 0.01652703806757927 = 0.009370960295200348 + 0.001 * 7.1560773849487305
Epoch 620, val loss: 1.086319923400879
Epoch 630, training loss: 0.01604268327355385 = 0.00889584794640541 + 0.001 * 7.146833896636963
Epoch 630, val loss: 1.0943433046340942
Epoch 640, training loss: 0.01561522576957941 = 0.008458547294139862 + 0.001 * 7.156678199768066
Epoch 640, val loss: 1.1021552085876465
Epoch 650, training loss: 0.015208610333502293 = 0.008055109530687332 + 0.001 * 7.153500556945801
Epoch 650, val loss: 1.1097279787063599
Epoch 660, training loss: 0.01483130268752575 = 0.007682104129344225 + 0.001 * 7.149197578430176
Epoch 660, val loss: 1.1171191930770874
Epoch 670, training loss: 0.014478703960776329 = 0.007336298003792763 + 0.001 * 7.142405033111572
Epoch 670, val loss: 1.1242879629135132
Epoch 680, training loss: 0.014174891635775566 = 0.007014067843556404 + 0.001 * 7.160823822021484
Epoch 680, val loss: 1.1313655376434326
Epoch 690, training loss: 0.013861164450645447 = 0.006711529567837715 + 0.001 * 7.149634838104248
Epoch 690, val loss: 1.138391137123108
Epoch 700, training loss: 0.01357709988951683 = 0.006425235420465469 + 0.001 * 7.151864528656006
Epoch 700, val loss: 1.145408272743225
Epoch 710, training loss: 0.013297922909259796 = 0.006153783295303583 + 0.001 * 7.144139289855957
Epoch 710, val loss: 1.152448296546936
Epoch 720, training loss: 0.013033163733780384 = 0.005896450486034155 + 0.001 * 7.136713027954102
Epoch 720, val loss: 1.1594796180725098
Epoch 730, training loss: 0.012822303920984268 = 0.005653034429997206 + 0.001 * 7.16926908493042
Epoch 730, val loss: 1.1665117740631104
Epoch 740, training loss: 0.012558750808238983 = 0.005423412658274174 + 0.001 * 7.135338306427002
Epoch 740, val loss: 1.1734753847122192
Epoch 750, training loss: 0.01233999989926815 = 0.00520706083625555 + 0.001 * 7.132939338684082
Epoch 750, val loss: 1.1803306341171265
Epoch 760, training loss: 0.012134375981986523 = 0.0050034839659929276 + 0.001 * 7.130891799926758
Epoch 760, val loss: 1.1870715618133545
Epoch 770, training loss: 0.011948218569159508 = 0.004812079947441816 + 0.001 * 7.136138916015625
Epoch 770, val loss: 1.1937087774276733
Epoch 780, training loss: 0.011767448857426643 = 0.004632243420928717 + 0.001 * 7.135205268859863
Epoch 780, val loss: 1.2002439498901367
Epoch 790, training loss: 0.011590730398893356 = 0.004463165067136288 + 0.001 * 7.127564430236816
Epoch 790, val loss: 1.2066566944122314
Epoch 800, training loss: 0.01143312081694603 = 0.004304094240069389 + 0.001 * 7.129026412963867
Epoch 800, val loss: 1.2129508256912231
Epoch 810, training loss: 0.011286003515124321 = 0.00415437389165163 + 0.001 * 7.131629467010498
Epoch 810, val loss: 1.219115138053894
Epoch 820, training loss: 0.011143126524984837 = 0.004013319965451956 + 0.001 * 7.129806041717529
Epoch 820, val loss: 1.225150465965271
Epoch 830, training loss: 0.011004691012203693 = 0.0038802223280072212 + 0.001 * 7.1244683265686035
Epoch 830, val loss: 1.2310711145401
Epoch 840, training loss: 0.010888398624956608 = 0.0037545417435467243 + 0.001 * 7.133856773376465
Epoch 840, val loss: 1.236907720565796
Epoch 850, training loss: 0.010757789015769958 = 0.00363573944196105 + 0.001 * 7.122049808502197
Epoch 850, val loss: 1.2426166534423828
Epoch 860, training loss: 0.010678809136152267 = 0.0035233020316809416 + 0.001 * 7.1555070877075195
Epoch 860, val loss: 1.2482173442840576
Epoch 870, training loss: 0.010539762675762177 = 0.0034168115817010403 + 0.001 * 7.122951030731201
Epoch 870, val loss: 1.2537200450897217
Epoch 880, training loss: 0.010432366281747818 = 0.0033157856669276953 + 0.001 * 7.116580009460449
Epoch 880, val loss: 1.259124517440796
Epoch 890, training loss: 0.010337302461266518 = 0.0032198703847825527 + 0.001 * 7.117431163787842
Epoch 890, val loss: 1.2644262313842773
Epoch 900, training loss: 0.010243168100714684 = 0.0031286892481148243 + 0.001 * 7.114478588104248
Epoch 900, val loss: 1.2696352005004883
Epoch 910, training loss: 0.01016160100698471 = 0.003041923511773348 + 0.001 * 7.1196770668029785
Epoch 910, val loss: 1.2747448682785034
Epoch 920, training loss: 0.010089542716741562 = 0.002959306351840496 + 0.001 * 7.1302361488342285
Epoch 920, val loss: 1.2797809839248657
Epoch 930, training loss: 0.010007667355239391 = 0.00288062309846282 + 0.001 * 7.127043724060059
Epoch 930, val loss: 1.2847226858139038
Epoch 940, training loss: 0.009925547987222672 = 0.0028055559378117323 + 0.001 * 7.119991779327393
Epoch 940, val loss: 1.2895830869674683
Epoch 950, training loss: 0.009848184883594513 = 0.002733939327299595 + 0.001 * 7.114245891571045
Epoch 950, val loss: 1.2943755388259888
Epoch 960, training loss: 0.009777355939149857 = 0.002665524370968342 + 0.001 * 7.111830711364746
Epoch 960, val loss: 1.299060583114624
Epoch 970, training loss: 0.009699603542685509 = 0.0026001371443271637 + 0.001 * 7.099466323852539
Epoch 970, val loss: 1.3036965131759644
Epoch 980, training loss: 0.009670896455645561 = 0.002537619788199663 + 0.001 * 7.133275985717773
Epoch 980, val loss: 1.3082506656646729
Epoch 990, training loss: 0.009593529626727104 = 0.002477773232385516 + 0.001 * 7.115756511688232
Epoch 990, val loss: 1.3127144575119019
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 1.9588483572006226 = 1.9502514600753784 + 0.001 * 8.59684944152832
Epoch 0, val loss: 1.9591039419174194
Epoch 10, training loss: 1.9488043785095215 = 1.940207600593567 + 0.001 * 8.59682559967041
Epoch 10, val loss: 1.9492813348770142
Epoch 20, training loss: 1.9368809461593628 = 1.9282842874526978 + 0.001 * 8.596713066101074
Epoch 20, val loss: 1.9371867179870605
Epoch 30, training loss: 1.9206047058105469 = 1.912008285522461 + 0.001 * 8.59646224975586
Epoch 30, val loss: 1.920050024986267
Epoch 40, training loss: 1.8968271017074585 = 1.8882312774658203 + 0.001 * 8.59582805633545
Epoch 40, val loss: 1.8945449590682983
Epoch 50, training loss: 1.8633204698562622 = 1.8547264337539673 + 0.001 * 8.594002723693848
Epoch 50, val loss: 1.8592971563339233
Epoch 60, training loss: 1.825339674949646 = 1.8167518377304077 + 0.001 * 8.58777904510498
Epoch 60, val loss: 1.822172999382019
Epoch 70, training loss: 1.7945373058319092 = 1.7859760522842407 + 0.001 * 8.561302185058594
Epoch 70, val loss: 1.7942078113555908
Epoch 80, training loss: 1.7583369016647339 = 1.7499529123306274 + 0.001 * 8.383965492248535
Epoch 80, val loss: 1.761608600616455
Epoch 90, training loss: 1.7069392204284668 = 1.6988111734390259 + 0.001 * 8.12809944152832
Epoch 90, val loss: 1.7183845043182373
Epoch 100, training loss: 1.634702444076538 = 1.6266595125198364 + 0.001 * 8.042928695678711
Epoch 100, val loss: 1.658379077911377
Epoch 110, training loss: 1.5454744100570679 = 1.5374829769134521 + 0.001 * 7.991469860076904
Epoch 110, val loss: 1.583982229232788
Epoch 120, training loss: 1.4499125480651855 = 1.4420289993286133 + 0.001 * 7.883489608764648
Epoch 120, val loss: 1.505113124847412
Epoch 130, training loss: 1.3543552160263062 = 1.3466846942901611 + 0.001 * 7.67050313949585
Epoch 130, val loss: 1.4285564422607422
Epoch 140, training loss: 1.2599167823791504 = 1.2522839307785034 + 0.001 * 7.632875919342041
Epoch 140, val loss: 1.357047200202942
Epoch 150, training loss: 1.167137622833252 = 1.159590721130371 + 0.001 * 7.546956539154053
Epoch 150, val loss: 1.2914677858352661
Epoch 160, training loss: 1.0763949155807495 = 1.0689160823822021 + 0.001 * 7.478776454925537
Epoch 160, val loss: 1.2309398651123047
Epoch 170, training loss: 0.9883014559745789 = 0.980842649936676 + 0.001 * 7.458825588226318
Epoch 170, val loss: 1.173207402229309
Epoch 180, training loss: 0.9037204384803772 = 0.8962670564651489 + 0.001 * 7.453372955322266
Epoch 180, val loss: 1.1184245347976685
Epoch 190, training loss: 0.8240424394607544 = 0.8165954351425171 + 0.001 * 7.447018623352051
Epoch 190, val loss: 1.0669313669204712
Epoch 200, training loss: 0.7510105967521667 = 0.7435688972473145 + 0.001 * 7.4416961669921875
Epoch 200, val loss: 1.0207854509353638
Epoch 210, training loss: 0.6856328248977661 = 0.6781976819038391 + 0.001 * 7.435138702392578
Epoch 210, val loss: 0.9809016585350037
Epoch 220, training loss: 0.6274445056915283 = 0.6200177669525146 + 0.001 * 7.426758289337158
Epoch 220, val loss: 0.9471385478973389
Epoch 230, training loss: 0.5749527812004089 = 0.567539393901825 + 0.001 * 7.413365840911865
Epoch 230, val loss: 0.9183789491653442
Epoch 240, training loss: 0.5268555879592896 = 0.5194649696350098 + 0.001 * 7.39064359664917
Epoch 240, val loss: 0.8933578729629517
Epoch 250, training loss: 0.4823082685470581 = 0.47495007514953613 + 0.001 * 7.3582072257995605
Epoch 250, val loss: 0.8719068169593811
Epoch 260, training loss: 0.44076624512672424 = 0.4334351718425751 + 0.001 * 7.331063270568848
Epoch 260, val loss: 0.8541186451911926
Epoch 270, training loss: 0.401741623878479 = 0.3944289982318878 + 0.001 * 7.312631130218506
Epoch 270, val loss: 0.8399990200996399
Epoch 280, training loss: 0.3647722899913788 = 0.3574701249599457 + 0.001 * 7.30216646194458
Epoch 280, val loss: 0.8291002511978149
Epoch 290, training loss: 0.32937100529670715 = 0.3220749795436859 + 0.001 * 7.296037673950195
Epoch 290, val loss: 0.8208938837051392
Epoch 300, training loss: 0.295215368270874 = 0.2879222631454468 + 0.001 * 7.29310417175293
Epoch 300, val loss: 0.8150696158409119
Epoch 310, training loss: 0.262251079082489 = 0.2549602687358856 + 0.001 * 7.290799140930176
Epoch 310, val loss: 0.8117117285728455
Epoch 320, training loss: 0.23074041306972504 = 0.22345289587974548 + 0.001 * 7.287510395050049
Epoch 320, val loss: 0.8111507892608643
Epoch 330, training loss: 0.20131616294384003 = 0.1940283477306366 + 0.001 * 7.287818431854248
Epoch 330, val loss: 0.8137943744659424
Epoch 340, training loss: 0.17475394904613495 = 0.16746865212917328 + 0.001 * 7.285301685333252
Epoch 340, val loss: 0.8201704621315002
Epoch 350, training loss: 0.1515921950340271 = 0.14431092143058777 + 0.001 * 7.281267166137695
Epoch 350, val loss: 0.8301159739494324
Epoch 360, training loss: 0.13187268376350403 = 0.12458772957324982 + 0.001 * 7.2849555015563965
Epoch 360, val loss: 0.8433429002761841
Epoch 370, training loss: 0.11517086625099182 = 0.10789526998996735 + 0.001 * 7.275599479675293
Epoch 370, val loss: 0.8589628338813782
Epoch 380, training loss: 0.10101254284381866 = 0.09373853355646133 + 0.001 * 7.274007320404053
Epoch 380, val loss: 0.8763587474822998
Epoch 390, training loss: 0.08895785361528397 = 0.08169203251600266 + 0.001 * 7.265822410583496
Epoch 390, val loss: 0.894845187664032
Epoch 400, training loss: 0.078701451420784 = 0.07142448425292969 + 0.001 * 7.276965618133545
Epoch 400, val loss: 0.9139495491981506
Epoch 410, training loss: 0.06993093341588974 = 0.06267279386520386 + 0.001 * 7.258139133453369
Epoch 410, val loss: 0.9335561394691467
Epoch 420, training loss: 0.06246456131339073 = 0.055215269327163696 + 0.001 * 7.24929141998291
Epoch 420, val loss: 0.9534124135971069
Epoch 430, training loss: 0.05610083416104317 = 0.04886096343398094 + 0.001 * 7.239871978759766
Epoch 430, val loss: 0.973368227481842
Epoch 440, training loss: 0.05068717896938324 = 0.0434437170624733 + 0.001 * 7.243462562561035
Epoch 440, val loss: 0.9933078289031982
Epoch 450, training loss: 0.046067014336586 = 0.038816723972558975 + 0.001 * 7.250290393829346
Epoch 450, val loss: 1.0129616260528564
Epoch 460, training loss: 0.042073335498571396 = 0.034854404628276825 + 0.001 * 7.218930721282959
Epoch 460, val loss: 1.0323593616485596
Epoch 470, training loss: 0.0386686846613884 = 0.03144831210374832 + 0.001 * 7.220371246337891
Epoch 470, val loss: 1.051327109336853
Epoch 480, training loss: 0.035719987004995346 = 0.028508085757493973 + 0.001 * 7.211899280548096
Epoch 480, val loss: 1.0697969198226929
Epoch 490, training loss: 0.03316684812307358 = 0.025958511978387833 + 0.001 * 7.208337783813477
Epoch 490, val loss: 1.0877352952957153
Epoch 500, training loss: 0.03093978762626648 = 0.023737069219350815 + 0.001 * 7.202718734741211
Epoch 500, val loss: 1.1051064729690552
Epoch 510, training loss: 0.028988946229219437 = 0.021792050451040268 + 0.001 * 7.196896076202393
Epoch 510, val loss: 1.1218817234039307
Epoch 520, training loss: 0.027278028428554535 = 0.020081179216504097 + 0.001 * 7.196848392486572
Epoch 520, val loss: 1.1381169557571411
Epoch 530, training loss: 0.02576136589050293 = 0.018569447100162506 + 0.001 * 7.19191837310791
Epoch 530, val loss: 1.1537836790084839
Epoch 540, training loss: 0.02440757304430008 = 0.017227409407496452 + 0.001 * 7.180163383483887
Epoch 540, val loss: 1.1688776016235352
Epoch 550, training loss: 0.023218370974063873 = 0.01603095792233944 + 0.001 * 7.187411785125732
Epoch 550, val loss: 1.1834359169006348
Epoch 560, training loss: 0.022143730893731117 = 0.014960016123950481 + 0.001 * 7.183713912963867
Epoch 560, val loss: 1.197494387626648
Epoch 570, training loss: 0.021171756088733673 = 0.01399768702685833 + 0.001 * 7.174067974090576
Epoch 570, val loss: 1.2110595703125
Epoch 580, training loss: 0.020303314551711082 = 0.01312996819615364 + 0.001 * 7.173346042633057
Epoch 580, val loss: 1.2241636514663696
Epoch 590, training loss: 0.019518394023180008 = 0.01234484650194645 + 0.001 * 7.173547267913818
Epoch 590, val loss: 1.2368067502975464
Epoch 600, training loss: 0.018799489364027977 = 0.011632183566689491 + 0.001 * 7.167304992675781
Epoch 600, val loss: 1.2490736246109009
Epoch 610, training loss: 0.01815095730125904 = 0.010983288288116455 + 0.001 * 7.167668342590332
Epoch 610, val loss: 1.2609373331069946
Epoch 620, training loss: 0.01755915954709053 = 0.010390848852694035 + 0.001 * 7.168309688568115
Epoch 620, val loss: 1.2723997831344604
Epoch 630, training loss: 0.017013203352689743 = 0.009848433546721935 + 0.001 * 7.164770126342773
Epoch 630, val loss: 1.2835030555725098
Epoch 640, training loss: 0.016512855887413025 = 0.009350509382784367 + 0.001 * 7.162345886230469
Epoch 640, val loss: 1.2942545413970947
Epoch 650, training loss: 0.01604408212006092 = 0.008892268873751163 + 0.001 * 7.151812553405762
Epoch 650, val loss: 1.3047047853469849
Epoch 660, training loss: 0.015618743374943733 = 0.008469589985907078 + 0.001 * 7.149152755737305
Epoch 660, val loss: 1.3148248195648193
Epoch 670, training loss: 0.015230176970362663 = 0.008078881539404392 + 0.001 * 7.1512956619262695
Epoch 670, val loss: 1.3246424198150635
Epoch 680, training loss: 0.014861421659588814 = 0.007716979365795851 + 0.001 * 7.144441604614258
Epoch 680, val loss: 1.334182620048523
Epoch 690, training loss: 0.01453359704464674 = 0.007381090894341469 + 0.001 * 7.152505874633789
Epoch 690, val loss: 1.3434628248214722
Epoch 700, training loss: 0.014238717034459114 = 0.0070687104016542435 + 0.001 * 7.17000675201416
Epoch 700, val loss: 1.3524898290634155
Epoch 710, training loss: 0.013920895755290985 = 0.0067777130752801895 + 0.001 * 7.143181800842285
Epoch 710, val loss: 1.361249566078186
Epoch 720, training loss: 0.013650842942297459 = 0.006506126839667559 + 0.001 * 7.144715785980225
Epoch 720, val loss: 1.369804859161377
Epoch 730, training loss: 0.013400297611951828 = 0.006252255290746689 + 0.001 * 7.14804220199585
Epoch 730, val loss: 1.3780841827392578
Epoch 740, training loss: 0.013148048892617226 = 0.006014584563672543 + 0.001 * 7.133464336395264
Epoch 740, val loss: 1.3861671686172485
Epoch 750, training loss: 0.012952849268913269 = 0.005791725590825081 + 0.001 * 7.161122798919678
Epoch 750, val loss: 1.3940550088882446
Epoch 760, training loss: 0.012723935768008232 = 0.005582521669566631 + 0.001 * 7.141413688659668
Epoch 760, val loss: 1.4017162322998047
Epoch 770, training loss: 0.012544311583042145 = 0.005385831464082003 + 0.001 * 7.1584792137146
Epoch 770, val loss: 1.4092055559158325
Epoch 780, training loss: 0.012343100272119045 = 0.005200653336942196 + 0.001 * 7.142446517944336
Epoch 780, val loss: 1.4164916276931763
Epoch 790, training loss: 0.012155997566878796 = 0.005026091355830431 + 0.001 * 7.129905700683594
Epoch 790, val loss: 1.4236302375793457
Epoch 800, training loss: 0.011991473846137524 = 0.004861355293542147 + 0.001 * 7.130118370056152
Epoch 800, val loss: 1.4305784702301025
Epoch 810, training loss: 0.011828726157546043 = 0.004705679602921009 + 0.001 * 7.123046875
Epoch 810, val loss: 1.4373528957366943
Epoch 820, training loss: 0.011689648032188416 = 0.004558435175567865 + 0.001 * 7.1312127113342285
Epoch 820, val loss: 1.4439812898635864
Epoch 830, training loss: 0.011528229340910912 = 0.004419003147631884 + 0.001 * 7.109225749969482
Epoch 830, val loss: 1.4504458904266357
Epoch 840, training loss: 0.011419745162129402 = 0.004286819603294134 + 0.001 * 7.132925033569336
Epoch 840, val loss: 1.4567824602127075
Epoch 850, training loss: 0.011271664872765541 = 0.004161403048783541 + 0.001 * 7.110261917114258
Epoch 850, val loss: 1.462968349456787
Epoch 860, training loss: 0.011156290769577026 = 0.004042275249958038 + 0.001 * 7.114015102386475
Epoch 860, val loss: 1.4690150022506714
Epoch 870, training loss: 0.01104644499719143 = 0.0039290329441428185 + 0.001 * 7.1174116134643555
Epoch 870, val loss: 1.4749149084091187
Epoch 880, training loss: 0.0109630161896348 = 0.003821298945695162 + 0.001 * 7.141716957092285
Epoch 880, val loss: 1.4807090759277344
Epoch 890, training loss: 0.010840963572263718 = 0.003718707477673888 + 0.001 * 7.122255325317383
Epoch 890, val loss: 1.4863617420196533
Epoch 900, training loss: 0.010753965005278587 = 0.003620930714532733 + 0.001 * 7.133033752441406
Epoch 900, val loss: 1.4919170141220093
Epoch 910, training loss: 0.01062259916216135 = 0.0035276806447654963 + 0.001 * 7.0949177742004395
Epoch 910, val loss: 1.4973220825195312
Epoch 920, training loss: 0.010533676482737064 = 0.003438666695728898 + 0.001 * 7.0950093269348145
Epoch 920, val loss: 1.5026532411575317
Epoch 930, training loss: 0.01044590212404728 = 0.0033536332193762064 + 0.001 * 7.092268943786621
Epoch 930, val loss: 1.5078437328338623
Epoch 940, training loss: 0.01036667637526989 = 0.003272339468821883 + 0.001 * 7.094336986541748
Epoch 940, val loss: 1.5129618644714355
Epoch 950, training loss: 0.010289590805768967 = 0.0031945721711963415 + 0.001 * 7.095017910003662
Epoch 950, val loss: 1.5179493427276611
Epoch 960, training loss: 0.010218759998679161 = 0.0031201262027025223 + 0.001 * 7.0986328125
Epoch 960, val loss: 1.5228569507598877
Epoch 970, training loss: 0.010145489126443863 = 0.003048811573535204 + 0.001 * 7.096677303314209
Epoch 970, val loss: 1.5276466608047485
Epoch 980, training loss: 0.010103173553943634 = 0.002980473916977644 + 0.001 * 7.12269926071167
Epoch 980, val loss: 1.5323405265808105
Epoch 990, training loss: 0.009997794404625893 = 0.0029149376787245274 + 0.001 * 7.082856178283691
Epoch 990, val loss: 1.536952257156372
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.79388508170796
=== training gcn model ===
Epoch 0, training loss: 1.9481873512268066 = 1.939590573310852 + 0.001 * 8.596806526184082
Epoch 0, val loss: 1.9425641298294067
Epoch 10, training loss: 1.9385038614273071 = 1.9299070835113525 + 0.001 * 8.596760749816895
Epoch 10, val loss: 1.9324121475219727
Epoch 20, training loss: 1.9269603490829468 = 1.9183638095855713 + 0.001 * 8.596575736999512
Epoch 20, val loss: 1.920274257659912
Epoch 30, training loss: 1.9112869501113892 = 1.9026907682418823 + 0.001 * 8.59615707397461
Epoch 30, val loss: 1.9039157629013062
Epoch 40, training loss: 1.8887237310409546 = 1.8801285028457642 + 0.001 * 8.595267295837402
Epoch 40, val loss: 1.880613923072815
Epoch 50, training loss: 1.8575718402862549 = 1.848978877067566 + 0.001 * 8.593008041381836
Epoch 50, val loss: 1.8494685888290405
Epoch 60, training loss: 1.8225693702697754 = 1.8139835596084595 + 0.001 * 8.585814476013184
Epoch 60, val loss: 1.8168613910675049
Epoch 70, training loss: 1.793349027633667 = 1.7847932577133179 + 0.001 * 8.555801391601562
Epoch 70, val loss: 1.7912760972976685
Epoch 80, training loss: 1.75954008102417 = 1.7511968612670898 + 0.001 * 8.343167304992676
Epoch 80, val loss: 1.7617323398590088
Epoch 90, training loss: 1.7114474773406982 = 1.7033721208572388 + 0.001 * 8.075303077697754
Epoch 90, val loss: 1.7221041917800903
Epoch 100, training loss: 1.6440232992172241 = 1.6361280679702759 + 0.001 * 7.895228385925293
Epoch 100, val loss: 1.6673262119293213
Epoch 110, training loss: 1.5567715167999268 = 1.5490950345993042 + 0.001 * 7.676445960998535
Epoch 110, val loss: 1.5958677530288696
Epoch 120, training loss: 1.458436131477356 = 1.4508999586105347 + 0.001 * 7.536134719848633
Epoch 120, val loss: 1.5155118703842163
Epoch 130, training loss: 1.3584215641021729 = 1.3509489297866821 + 0.001 * 7.472647666931152
Epoch 130, val loss: 1.4348808526992798
Epoch 140, training loss: 1.2595847845077515 = 1.2521734237670898 + 0.001 * 7.411305904388428
Epoch 140, val loss: 1.3581711053848267
Epoch 150, training loss: 1.1630361080169678 = 1.1556577682495117 + 0.001 * 7.3783650398254395
Epoch 150, val loss: 1.2868309020996094
Epoch 160, training loss: 1.0702190399169922 = 1.0628626346588135 + 0.001 * 7.3564133644104
Epoch 160, val loss: 1.2213973999023438
Epoch 170, training loss: 0.9820830821990967 = 0.9747512936592102 + 0.001 * 7.331814765930176
Epoch 170, val loss: 1.1607199907302856
Epoch 180, training loss: 0.8990652561187744 = 0.8917669057846069 + 0.001 * 7.298367977142334
Epoch 180, val loss: 1.103628158569336
Epoch 190, training loss: 0.8220149278640747 = 0.8147515654563904 + 0.001 * 7.2633514404296875
Epoch 190, val loss: 1.050201177597046
Epoch 200, training loss: 0.7522479891777039 = 0.7450025081634521 + 0.001 * 7.245500087738037
Epoch 200, val loss: 1.0014954805374146
Epoch 210, training loss: 0.6903027296066284 = 0.683068037033081 + 0.001 * 7.234676837921143
Epoch 210, val loss: 0.9591962099075317
Epoch 220, training loss: 0.6350635886192322 = 0.6278331875801086 + 0.001 * 7.230422496795654
Epoch 220, val loss: 0.9236202836036682
Epoch 230, training loss: 0.5841405987739563 = 0.576909601688385 + 0.001 * 7.230996608734131
Epoch 230, val loss: 0.8938876390457153
Epoch 240, training loss: 0.5352686643600464 = 0.528036892414093 + 0.001 * 7.231744289398193
Epoch 240, val loss: 0.8684957027435303
Epoch 250, training loss: 0.48720481991767883 = 0.4799726903438568 + 0.001 * 7.232126712799072
Epoch 250, val loss: 0.845702052116394
Epoch 260, training loss: 0.4396597445011139 = 0.43242713809013367 + 0.001 * 7.232602119445801
Epoch 260, val loss: 0.8243711590766907
Epoch 270, training loss: 0.3927372694015503 = 0.38550394773483276 + 0.001 * 7.233314037322998
Epoch 270, val loss: 0.804355800151825
Epoch 280, training loss: 0.3467639982700348 = 0.3395299017429352 + 0.001 * 7.234099864959717
Epoch 280, val loss: 0.7862265110015869
Epoch 290, training loss: 0.30234280228614807 = 0.29510778188705444 + 0.001 * 7.235021114349365
Epoch 290, val loss: 0.7707331776618958
Epoch 300, training loss: 0.26052504777908325 = 0.25328898429870605 + 0.001 * 7.236068248748779
Epoch 300, val loss: 0.758287250995636
Epoch 310, training loss: 0.22264160215854645 = 0.21540436148643494 + 0.001 * 7.237235069274902
Epoch 310, val loss: 0.7495043873786926
Epoch 320, training loss: 0.18964892625808716 = 0.1824103742837906 + 0.001 * 7.238554954528809
Epoch 320, val loss: 0.7448544502258301
Epoch 330, training loss: 0.1616939902305603 = 0.15445327758789062 + 0.001 * 7.240715980529785
Epoch 330, val loss: 0.7441703081130981
Epoch 340, training loss: 0.138165682554245 = 0.13092437386512756 + 0.001 * 7.241301536560059
Epoch 340, val loss: 0.7467414736747742
Epoch 350, training loss: 0.11831644177436829 = 0.11107411980628967 + 0.001 * 7.242323875427246
Epoch 350, val loss: 0.7522503733634949
Epoch 360, training loss: 0.101697638630867 = 0.09445460140705109 + 0.001 * 7.2430338859558105
Epoch 360, val loss: 0.76027911901474
Epoch 370, training loss: 0.0879211351275444 = 0.08067624270915985 + 0.001 * 7.244892597198486
Epoch 370, val loss: 0.7703215479850769
Epoch 380, training loss: 0.07657130807638168 = 0.06932547688484192 + 0.001 * 7.24583101272583
Epoch 380, val loss: 0.7817742824554443
Epoch 390, training loss: 0.06722072511911392 = 0.059975553303956985 + 0.001 * 7.245174884796143
Epoch 390, val loss: 0.7941528558731079
Epoch 400, training loss: 0.05949262157082558 = 0.05224813148379326 + 0.001 * 7.244489669799805
Epoch 400, val loss: 0.8069911003112793
Epoch 410, training loss: 0.05307614803314209 = 0.045831792056560516 + 0.001 * 7.244357585906982
Epoch 410, val loss: 0.819988489151001
Epoch 420, training loss: 0.047721050679683685 = 0.04047538340091705 + 0.001 * 7.24566650390625
Epoch 420, val loss: 0.8329811692237854
Epoch 430, training loss: 0.043223630636930466 = 0.03597789630293846 + 0.001 * 7.24573278427124
Epoch 430, val loss: 0.8458400368690491
Epoch 440, training loss: 0.03941944241523743 = 0.032178036868572235 + 0.001 * 7.241404056549072
Epoch 440, val loss: 0.8584707379341125
Epoch 450, training loss: 0.03618292883038521 = 0.02894652634859085 + 0.001 * 7.236402988433838
Epoch 450, val loss: 0.8707907795906067
Epoch 460, training loss: 0.033421147614717484 = 0.026180578395724297 + 0.001 * 7.240568161010742
Epoch 460, val loss: 0.8827751874923706
Epoch 470, training loss: 0.031030800193548203 = 0.023798298090696335 + 0.001 * 7.232501029968262
Epoch 470, val loss: 0.8943823575973511
Epoch 480, training loss: 0.02896372601389885 = 0.021733827888965607 + 0.001 * 7.229897499084473
Epoch 480, val loss: 0.9056441783905029
Epoch 490, training loss: 0.027157314121723175 = 0.01993437297642231 + 0.001 * 7.222941875457764
Epoch 490, val loss: 0.9165140390396118
Epoch 500, training loss: 0.02560950443148613 = 0.01835721544921398 + 0.001 * 7.252288341522217
Epoch 500, val loss: 0.9270375967025757
Epoch 510, training loss: 0.024181149899959564 = 0.016967426985502243 + 0.001 * 7.2137227058410645
Epoch 510, val loss: 0.9371751546859741
Epoch 520, training loss: 0.022943982854485512 = 0.015736661851406097 + 0.001 * 7.207320213317871
Epoch 520, val loss: 0.9470047354698181
Epoch 530, training loss: 0.02185158059000969 = 0.014641507528722286 + 0.001 * 7.210072040557861
Epoch 530, val loss: 0.9564895033836365
Epoch 540, training loss: 0.02086326666176319 = 0.013662724755704403 + 0.001 * 7.2005414962768555
Epoch 540, val loss: 0.9656618237495422
Epoch 550, training loss: 0.019973326474428177 = 0.01278445590287447 + 0.001 * 7.188869476318359
Epoch 550, val loss: 0.974541425704956
Epoch 560, training loss: 0.019175788387656212 = 0.011993360705673695 + 0.001 * 7.182427883148193
Epoch 560, val loss: 0.9831287264823914
Epoch 570, training loss: 0.018467538058757782 = 0.011278060264885426 + 0.001 * 7.189478397369385
Epoch 570, val loss: 0.9914458394050598
Epoch 580, training loss: 0.017812538892030716 = 0.010629103519022465 + 0.001 * 7.183434963226318
Epoch 580, val loss: 0.999496579170227
Epoch 590, training loss: 0.017234979197382927 = 0.010038451291620731 + 0.001 * 7.196527481079102
Epoch 590, val loss: 1.0073219537734985
Epoch 600, training loss: 0.016676079481840134 = 0.009499291889369488 + 0.001 * 7.176786422729492
Epoch 600, val loss: 1.014893889427185
Epoch 610, training loss: 0.016178380697965622 = 0.009005767293274403 + 0.001 * 7.172613620758057
Epoch 610, val loss: 1.0222607851028442
Epoch 620, training loss: 0.015722226351499557 = 0.008552729152143002 + 0.001 * 7.169497489929199
Epoch 620, val loss: 1.0293995141983032
Epoch 630, training loss: 0.015297227539122105 = 0.008135808631777763 + 0.001 * 7.161418437957764
Epoch 630, val loss: 1.036340594291687
Epoch 640, training loss: 0.014917276799678802 = 0.007751211058348417 + 0.001 * 7.166064739227295
Epoch 640, val loss: 1.0430654287338257
Epoch 650, training loss: 0.014560679905116558 = 0.007395652588456869 + 0.001 * 7.165027141571045
Epoch 650, val loss: 1.0496207475662231
Epoch 660, training loss: 0.014216966927051544 = 0.007066220045089722 + 0.001 * 7.150746822357178
Epoch 660, val loss: 1.0559942722320557
Epoch 670, training loss: 0.013913005590438843 = 0.006760390475392342 + 0.001 * 7.152614116668701
Epoch 670, val loss: 1.0622066259384155
Epoch 680, training loss: 0.013633610680699348 = 0.0064759948290884495 + 0.001 * 7.1576151847839355
Epoch 680, val loss: 1.0682400465011597
Epoch 690, training loss: 0.0133588295429945 = 0.006210958119481802 + 0.001 * 7.1478705406188965
Epoch 690, val loss: 1.0741058588027954
Epoch 700, training loss: 0.013133898377418518 = 0.005963549483567476 + 0.001 * 7.170348644256592
Epoch 700, val loss: 1.0798413753509521
Epoch 710, training loss: 0.012873202562332153 = 0.005732124671339989 + 0.001 * 7.141077995300293
Epoch 710, val loss: 1.085410475730896
Epoch 720, training loss: 0.012674026191234589 = 0.005515186581760645 + 0.001 * 7.158839225769043
Epoch 720, val loss: 1.090880036354065
Epoch 730, training loss: 0.01245655957609415 = 0.0053111654706299305 + 0.001 * 7.1453938484191895
Epoch 730, val loss: 1.096228837966919
Epoch 740, training loss: 0.012255894020199776 = 0.005118353292346001 + 0.001 * 7.137540817260742
Epoch 740, val loss: 1.1015340089797974
Epoch 750, training loss: 0.012091755867004395 = 0.004935324192047119 + 0.001 * 7.156431674957275
Epoch 750, val loss: 1.1067856550216675
Epoch 760, training loss: 0.011896228417754173 = 0.004760902840644121 + 0.001 * 7.135325908660889
Epoch 760, val loss: 1.1120539903640747
Epoch 770, training loss: 0.011726530268788338 = 0.004594177473336458 + 0.001 * 7.132352352142334
Epoch 770, val loss: 1.1173219680786133
Epoch 780, training loss: 0.011561471968889236 = 0.004434698726981878 + 0.001 * 7.126773357391357
Epoch 780, val loss: 1.1226009130477905
Epoch 790, training loss: 0.01141505315899849 = 0.00428226962685585 + 0.001 * 7.13278341293335
Epoch 790, val loss: 1.1278823614120483
Epoch 800, training loss: 0.011258845217525959 = 0.004136932082474232 + 0.001 * 7.121912956237793
Epoch 800, val loss: 1.133113980293274
Epoch 810, training loss: 0.011130294762551785 = 0.003998450003564358 + 0.001 * 7.131844520568848
Epoch 810, val loss: 1.1383163928985596
Epoch 820, training loss: 0.010989450849592686 = 0.0038666098844259977 + 0.001 * 7.122840404510498
Epoch 820, val loss: 1.1434566974639893
Epoch 830, training loss: 0.010854559950530529 = 0.0037411064840853214 + 0.001 * 7.113452911376953
Epoch 830, val loss: 1.1485542058944702
Epoch 840, training loss: 0.010738799348473549 = 0.0036216704174876213 + 0.001 * 7.117128372192383
Epoch 840, val loss: 1.1535917520523071
Epoch 850, training loss: 0.010619455017149448 = 0.003508170135319233 + 0.001 * 7.1112847328186035
Epoch 850, val loss: 1.1585651636123657
Epoch 860, training loss: 0.010516687296330929 = 0.0034002382308244705 + 0.001 * 7.116448879241943
Epoch 860, val loss: 1.1634557247161865
Epoch 870, training loss: 0.010400492697954178 = 0.003297568531706929 + 0.001 * 7.10292387008667
Epoch 870, val loss: 1.1682560443878174
Epoch 880, training loss: 0.010325055569410324 = 0.003199892584234476 + 0.001 * 7.1251630783081055
Epoch 880, val loss: 1.1729971170425415
Epoch 890, training loss: 0.010209502652287483 = 0.0031071375124156475 + 0.001 * 7.102364540100098
Epoch 890, val loss: 1.1776351928710938
Epoch 900, training loss: 0.010168595239520073 = 0.0030188518576323986 + 0.001 * 7.149742603302002
Epoch 900, val loss: 1.1822266578674316
Epoch 910, training loss: 0.010035168379545212 = 0.002934824675321579 + 0.001 * 7.100343227386475
Epoch 910, val loss: 1.1867098808288574
Epoch 920, training loss: 0.009950734674930573 = 0.002854802878573537 + 0.001 * 7.095931529998779
Epoch 920, val loss: 1.1911183595657349
Epoch 930, training loss: 0.009895787574350834 = 0.002778541063889861 + 0.001 * 7.117246627807617
Epoch 930, val loss: 1.1954323053359985
Epoch 940, training loss: 0.009813176468014717 = 0.0027058597188442945 + 0.001 * 7.107316493988037
Epoch 940, val loss: 1.1996886730194092
Epoch 950, training loss: 0.009734716266393661 = 0.002636530203744769 + 0.001 * 7.0981855392456055
Epoch 950, val loss: 1.2038559913635254
Epoch 960, training loss: 0.009674481116235256 = 0.0025703401770442724 + 0.001 * 7.104140281677246
Epoch 960, val loss: 1.207937240600586
Epoch 970, training loss: 0.009594311937689781 = 0.0025070891715586185 + 0.001 * 7.087222576141357
Epoch 970, val loss: 1.2119518518447876
Epoch 980, training loss: 0.009548794478178024 = 0.0024466761387884617 + 0.001 * 7.102118492126465
Epoch 980, val loss: 1.215885043144226
Epoch 990, training loss: 0.009473113343119621 = 0.002388918539509177 + 0.001 * 7.084194183349609
Epoch 990, val loss: 1.2197420597076416
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.7996837111228255
The final CL Acc:0.74691, 0.01429, The final GNN Acc:0.80056, 0.00584
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13262])
remove edge: torch.Size([2, 7936])
updated graph: torch.Size([2, 10642])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.951276183128357 = 1.9426792860031128 + 0.001 * 8.596848487854004
Epoch 0, val loss: 1.9410436153411865
Epoch 10, training loss: 1.940973162651062 = 1.9323763847351074 + 0.001 * 8.59681510925293
Epoch 10, val loss: 1.9307359457015991
Epoch 20, training loss: 1.9283714294433594 = 1.9197747707366943 + 0.001 * 8.596683502197266
Epoch 20, val loss: 1.9179660081863403
Epoch 30, training loss: 1.910583257675171 = 1.901986837387085 + 0.001 * 8.5963773727417
Epoch 30, val loss: 1.8999903202056885
Epoch 40, training loss: 1.8842449188232422 = 1.875649333000183 + 0.001 * 8.595592498779297
Epoch 40, val loss: 1.8738367557525635
Epoch 50, training loss: 1.8479641675949097 = 1.839370846748352 + 0.001 * 8.593274116516113
Epoch 50, val loss: 1.839365839958191
Epoch 60, training loss: 1.8084321022033691 = 1.799846887588501 + 0.001 * 8.585245132446289
Epoch 60, val loss: 1.8050214052200317
Epoch 70, training loss: 1.7728759050369263 = 1.7643243074417114 + 0.001 * 8.551636695861816
Epoch 70, val loss: 1.7738844156265259
Epoch 80, training loss: 1.7253369092941284 = 1.7169742584228516 + 0.001 * 8.362646102905273
Epoch 80, val loss: 1.7276171445846558
Epoch 90, training loss: 1.658366084098816 = 1.6502313613891602 + 0.001 * 8.13467788696289
Epoch 90, val loss: 1.6660698652267456
Epoch 100, training loss: 1.5716402530670166 = 1.5635485649108887 + 0.001 * 8.09163761138916
Epoch 100, val loss: 1.5913583040237427
Epoch 110, training loss: 1.477566123008728 = 1.4694981575012207 + 0.001 * 8.06797981262207
Epoch 110, val loss: 1.5114152431488037
Epoch 120, training loss: 1.3883894681930542 = 1.380349040031433 + 0.001 * 8.040392875671387
Epoch 120, val loss: 1.4381892681121826
Epoch 130, training loss: 1.3083733320236206 = 1.3003976345062256 + 0.001 * 7.975654125213623
Epoch 130, val loss: 1.3752362728118896
Epoch 140, training loss: 1.2383352518081665 = 1.2305123805999756 + 0.001 * 7.822831630706787
Epoch 140, val loss: 1.323326587677002
Epoch 150, training loss: 1.1756274700164795 = 1.1678752899169922 + 0.001 * 7.75213098526001
Epoch 150, val loss: 1.2789417505264282
Epoch 160, training loss: 1.1140333414077759 = 1.106391429901123 + 0.001 * 7.641905784606934
Epoch 160, val loss: 1.2356635332107544
Epoch 170, training loss: 1.0480382442474365 = 1.0404808521270752 + 0.001 * 7.557438373565674
Epoch 170, val loss: 1.1877226829528809
Epoch 180, training loss: 0.9745582342147827 = 0.9670604467391968 + 0.001 * 7.497758388519287
Epoch 180, val loss: 1.132550597190857
Epoch 190, training loss: 0.8938311338424683 = 0.8863909840583801 + 0.001 * 7.440138816833496
Epoch 190, val loss: 1.0713331699371338
Epoch 200, training loss: 0.810164213180542 = 0.802763044834137 + 0.001 * 7.401156425476074
Epoch 200, val loss: 1.0085142850875854
Epoch 210, training loss: 0.7296920418739319 = 0.7223215103149414 + 0.001 * 7.370538234710693
Epoch 210, val loss: 0.9501871466636658
Epoch 220, training loss: 0.6573135256767273 = 0.649975597858429 + 0.001 * 7.337924480438232
Epoch 220, val loss: 0.9012309908866882
Epoch 230, training loss: 0.5948652625083923 = 0.587567925453186 + 0.001 * 7.297314167022705
Epoch 230, val loss: 0.8637062907218933
Epoch 240, training loss: 0.5414000153541565 = 0.5341320037841797 + 0.001 * 7.268004417419434
Epoch 240, val loss: 0.8367524147033691
Epoch 250, training loss: 0.49452248215675354 = 0.4872695505619049 + 0.001 * 7.25294303894043
Epoch 250, val loss: 0.8178258538246155
Epoch 260, training loss: 0.4515514671802521 = 0.44430381059646606 + 0.001 * 7.247661590576172
Epoch 260, val loss: 0.8041104078292847
Epoch 270, training loss: 0.4103738069534302 = 0.4031290113925934 + 0.001 * 7.244808197021484
Epoch 270, val loss: 0.7934370636940002
Epoch 280, training loss: 0.36997896432876587 = 0.3627362549304962 + 0.001 * 7.242705345153809
Epoch 280, val loss: 0.7846609354019165
Epoch 290, training loss: 0.33051058650016785 = 0.3232702612876892 + 0.001 * 7.240338325500488
Epoch 290, val loss: 0.7776840329170227
Epoch 300, training loss: 0.2928088903427124 = 0.28557103872299194 + 0.001 * 7.237844944000244
Epoch 300, val loss: 0.7728267312049866
Epoch 310, training loss: 0.25793248414993286 = 0.25069722533226013 + 0.001 * 7.235245227813721
Epoch 310, val loss: 0.7709965109825134
Epoch 320, training loss: 0.2266688197851181 = 0.2194337397813797 + 0.001 * 7.235083103179932
Epoch 320, val loss: 0.7727252244949341
Epoch 330, training loss: 0.19930019974708557 = 0.19206944108009338 + 0.001 * 7.230764865875244
Epoch 330, val loss: 0.7780538201332092
Epoch 340, training loss: 0.17567816376686096 = 0.16844984889030457 + 0.001 * 7.228307723999023
Epoch 340, val loss: 0.7866048216819763
Epoch 350, training loss: 0.15536586940288544 = 0.1481398642063141 + 0.001 * 7.226011276245117
Epoch 350, val loss: 0.7977631688117981
Epoch 360, training loss: 0.1378639042377472 = 0.13063916563987732 + 0.001 * 7.224743366241455
Epoch 360, val loss: 0.8107898235321045
Epoch 370, training loss: 0.12271197885274887 = 0.11549265682697296 + 0.001 * 7.219318389892578
Epoch 370, val loss: 0.8251824975013733
Epoch 380, training loss: 0.1095360741019249 = 0.10232087969779968 + 0.001 * 7.215195655822754
Epoch 380, val loss: 0.8406179547309875
Epoch 390, training loss: 0.09804791212081909 = 0.09082233160734177 + 0.001 * 7.225582122802734
Epoch 390, val loss: 0.8568132519721985
Epoch 400, training loss: 0.08796252310276031 = 0.08075548708438873 + 0.001 * 7.207032680511475
Epoch 400, val loss: 0.8735120892524719
Epoch 410, training loss: 0.0791281908750534 = 0.07192865014076233 + 0.001 * 7.1995439529418945
Epoch 410, val loss: 0.8905498385429382
Epoch 420, training loss: 0.07138193398714066 = 0.06418317556381226 + 0.001 * 7.198755741119385
Epoch 420, val loss: 0.9077982306480408
Epoch 430, training loss: 0.06457284837961197 = 0.057388994842767715 + 0.001 * 7.1838531494140625
Epoch 430, val loss: 0.9251496195793152
Epoch 440, training loss: 0.058610379695892334 = 0.05143192410469055 + 0.001 * 7.1784539222717285
Epoch 440, val loss: 0.9423309564590454
Epoch 450, training loss: 0.05337586998939514 = 0.046210117638111115 + 0.001 * 7.16575288772583
Epoch 450, val loss: 0.9592981338500977
Epoch 460, training loss: 0.0487905815243721 = 0.041633911430835724 + 0.001 * 7.156668186187744
Epoch 460, val loss: 0.9758240580558777
Epoch 470, training loss: 0.0448070727288723 = 0.03762185201048851 + 0.001 * 7.1852216720581055
Epoch 470, val loss: 0.9919517040252686
Epoch 480, training loss: 0.04125542566180229 = 0.034101858735084534 + 0.001 * 7.153566837310791
Epoch 480, val loss: 1.0075839757919312
Epoch 490, training loss: 0.03814766928553581 = 0.031008612364530563 + 0.001 * 7.1390581130981445
Epoch 490, val loss: 1.022759199142456
Epoch 500, training loss: 0.035420626401901245 = 0.028284689411520958 + 0.001 * 7.135936737060547
Epoch 500, val loss: 1.0373814105987549
Epoch 510, training loss: 0.033021435141563416 = 0.025880254805088043 + 0.001 * 7.141178131103516
Epoch 510, val loss: 1.0515520572662354
Epoch 520, training loss: 0.030880162492394447 = 0.02375205047428608 + 0.001 * 7.128111362457275
Epoch 520, val loss: 1.0651402473449707
Epoch 530, training loss: 0.028994742780923843 = 0.021863263100385666 + 0.001 * 7.131479263305664
Epoch 530, val loss: 1.0782700777053833
Epoch 540, training loss: 0.02731463685631752 = 0.02018234319984913 + 0.001 * 7.132293701171875
Epoch 540, val loss: 1.0908689498901367
Epoch 550, training loss: 0.025814101099967957 = 0.018683020025491714 + 0.001 * 7.131080150604248
Epoch 550, val loss: 1.103139877319336
Epoch 560, training loss: 0.024482712149620056 = 0.017341241240501404 + 0.001 * 7.141469478607178
Epoch 560, val loss: 1.114935040473938
Epoch 570, training loss: 0.023270804435014725 = 0.01613692194223404 + 0.001 * 7.133881092071533
Epoch 570, val loss: 1.1263108253479004
Epoch 580, training loss: 0.022179724648594856 = 0.01505295094102621 + 0.001 * 7.126773357391357
Epoch 580, val loss: 1.13724946975708
Epoch 590, training loss: 0.021194856613874435 = 0.01407474372535944 + 0.001 * 7.120113372802734
Epoch 590, val loss: 1.147790551185608
Epoch 600, training loss: 0.02032560296356678 = 0.013189620338380337 + 0.001 * 7.135982036590576
Epoch 600, val loss: 1.1580134630203247
Epoch 610, training loss: 0.01950787752866745 = 0.01238659955561161 + 0.001 * 7.121278285980225
Epoch 610, val loss: 1.1678624153137207
Epoch 620, training loss: 0.018774621188640594 = 0.01165624801069498 + 0.001 * 7.118372440338135
Epoch 620, val loss: 1.1773492097854614
Epoch 630, training loss: 0.018105117604136467 = 0.010990161448717117 + 0.001 * 7.114955902099609
Epoch 630, val loss: 1.1864858865737915
Epoch 640, training loss: 0.017502831295132637 = 0.010381254367530346 + 0.001 * 7.121576309204102
Epoch 640, val loss: 1.1953226327896118
Epoch 650, training loss: 0.01694454997777939 = 0.009823356755077839 + 0.001 * 7.121192932128906
Epoch 650, val loss: 1.2038415670394897
Epoch 660, training loss: 0.016428714618086815 = 0.00931110605597496 + 0.001 * 7.117608547210693
Epoch 660, val loss: 1.2120949029922485
Epoch 670, training loss: 0.015956219285726547 = 0.008839769288897514 + 0.001 * 7.116448879241943
Epoch 670, val loss: 1.2200599908828735
Epoch 680, training loss: 0.015522768720984459 = 0.008405168540775776 + 0.001 * 7.117599964141846
Epoch 680, val loss: 1.2277752161026
Epoch 690, training loss: 0.015137679874897003 = 0.00800361018627882 + 0.001 * 7.134068965911865
Epoch 690, val loss: 1.2352399826049805
Epoch 700, training loss: 0.014750629663467407 = 0.007631902117282152 + 0.001 * 7.118727207183838
Epoch 700, val loss: 1.2424653768539429
Epoch 710, training loss: 0.014398465864360332 = 0.00728716840967536 + 0.001 * 7.111297130584717
Epoch 710, val loss: 1.2494757175445557
Epoch 720, training loss: 0.014075085520744324 = 0.006966912653297186 + 0.001 * 7.1081719398498535
Epoch 720, val loss: 1.2562628984451294
Epoch 730, training loss: 0.013781201094388962 = 0.00666892109438777 + 0.001 * 7.112279891967773
Epoch 730, val loss: 1.2628525495529175
Epoch 740, training loss: 0.013506071642041206 = 0.0063912044279277325 + 0.001 * 7.114867210388184
Epoch 740, val loss: 1.2692492008209229
Epoch 750, training loss: 0.01323743537068367 = 0.006131965667009354 + 0.001 * 7.10546875
Epoch 750, val loss: 1.2754698991775513
Epoch 760, training loss: 0.012991074472665787 = 0.005889581050723791 + 0.001 * 7.101492881774902
Epoch 760, val loss: 1.2815114259719849
Epoch 770, training loss: 0.012767838314175606 = 0.005662616807967424 + 0.001 * 7.105221748352051
Epoch 770, val loss: 1.2873866558074951
Epoch 780, training loss: 0.012556701898574829 = 0.005449715536087751 + 0.001 * 7.106985569000244
Epoch 780, val loss: 1.2931045293807983
Epoch 790, training loss: 0.012359149754047394 = 0.005249358713626862 + 0.001 * 7.109790802001953
Epoch 790, val loss: 1.2986915111541748
Epoch 800, training loss: 0.012165119871497154 = 0.005059358663856983 + 0.001 * 7.105761528015137
Epoch 800, val loss: 1.3042196035385132
Epoch 810, training loss: 0.011978741735219955 = 0.004877559375017881 + 0.001 * 7.101181507110596
Epoch 810, val loss: 1.3097457885742188
Epoch 820, training loss: 0.01179899089038372 = 0.004702749662101269 + 0.001 * 7.096240997314453
Epoch 820, val loss: 1.3152583837509155
Epoch 830, training loss: 0.011635880917310715 = 0.004534709732979536 + 0.001 * 7.101171493530273
Epoch 830, val loss: 1.3207542896270752
Epoch 840, training loss: 0.011470139026641846 = 0.004373553674668074 + 0.001 * 7.096584320068359
Epoch 840, val loss: 1.3262155055999756
Epoch 850, training loss: 0.011310916393995285 = 0.004219529218971729 + 0.001 * 7.091386318206787
Epoch 850, val loss: 1.3316209316253662
Epoch 860, training loss: 0.011163683608174324 = 0.0040727523155510426 + 0.001 * 7.090931415557861
Epoch 860, val loss: 1.3369766473770142
Epoch 870, training loss: 0.011029188521206379 = 0.003933158237487078 + 0.001 * 7.096029758453369
Epoch 870, val loss: 1.3422430753707886
Epoch 880, training loss: 0.01089509204030037 = 0.0038006494287401438 + 0.001 * 7.094442367553711
Epoch 880, val loss: 1.3474122285842896
Epoch 890, training loss: 0.010768353939056396 = 0.0036750901490449905 + 0.001 * 7.093263626098633
Epoch 890, val loss: 1.3524259328842163
Epoch 900, training loss: 0.010652167722582817 = 0.00355613324791193 + 0.001 * 7.096034526824951
Epoch 900, val loss: 1.3573379516601562
Epoch 910, training loss: 0.010533493012189865 = 0.0034435074776411057 + 0.001 * 7.089984893798828
Epoch 910, val loss: 1.3620922565460205
Epoch 920, training loss: 0.010417480021715164 = 0.0033368112053722143 + 0.001 * 7.0806684494018555
Epoch 920, val loss: 1.3667398691177368
Epoch 930, training loss: 0.01033736951649189 = 0.003235712880268693 + 0.001 * 7.101655960083008
Epoch 930, val loss: 1.371240258216858
Epoch 940, training loss: 0.010232328437268734 = 0.003139855107292533 + 0.001 * 7.092473030090332
Epoch 940, val loss: 1.3756296634674072
Epoch 950, training loss: 0.01012496743351221 = 0.003048899117857218 + 0.001 * 7.076067924499512
Epoch 950, val loss: 1.3798792362213135
Epoch 960, training loss: 0.010049635544419289 = 0.0029624938033521175 + 0.001 * 7.087141990661621
Epoch 960, val loss: 1.3840429782867432
Epoch 970, training loss: 0.009966997429728508 = 0.002880411222577095 + 0.001 * 7.0865864753723145
Epoch 970, val loss: 1.3881006240844727
Epoch 980, training loss: 0.009872653521597385 = 0.0028023638296872377 + 0.001 * 7.070289134979248
Epoch 980, val loss: 1.3920283317565918
Epoch 990, training loss: 0.009799323044717312 = 0.0027280759532004595 + 0.001 * 7.071247100830078
Epoch 990, val loss: 1.3958711624145508
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8444913020558777
=== training gcn model ===
Epoch 0, training loss: 1.979487419128418 = 1.9708906412124634 + 0.001 * 8.596835136413574
Epoch 0, val loss: 1.980230689048767
Epoch 10, training loss: 1.9685394763946533 = 1.9599426984786987 + 0.001 * 8.596787452697754
Epoch 10, val loss: 1.9695343971252441
Epoch 20, training loss: 1.954890251159668 = 1.946293592453003 + 0.001 * 8.596626281738281
Epoch 20, val loss: 1.9555374383926392
Epoch 30, training loss: 1.9353904724121094 = 1.926794171333313 + 0.001 * 8.596240997314453
Epoch 30, val loss: 1.9351232051849365
Epoch 40, training loss: 1.9064195156097412 = 1.8978241682052612 + 0.001 * 8.595292091369629
Epoch 40, val loss: 1.9050211906433105
Epoch 50, training loss: 1.8652704954147339 = 1.8566778898239136 + 0.001 * 8.592555046081543
Epoch 50, val loss: 1.8638771772384644
Epoch 60, training loss: 1.8180904388427734 = 1.8095077276229858 + 0.001 * 8.582662582397461
Epoch 60, val loss: 1.8201919794082642
Epoch 70, training loss: 1.7803527116775513 = 1.7718124389648438 + 0.001 * 8.540213584899902
Epoch 70, val loss: 1.7872159481048584
Epoch 80, training loss: 1.7398245334625244 = 1.7315094470977783 + 0.001 * 8.31509780883789
Epoch 80, val loss: 1.7479519844055176
Epoch 90, training loss: 1.683225393295288 = 1.6750984191894531 + 0.001 * 8.12691879272461
Epoch 90, val loss: 1.695456624031067
Epoch 100, training loss: 1.6060572862625122 = 1.598144769668579 + 0.001 * 7.9125165939331055
Epoch 100, val loss: 1.6262506246566772
Epoch 110, training loss: 1.509546160697937 = 1.5018256902694702 + 0.001 * 7.720453262329102
Epoch 110, val loss: 1.5418580770492554
Epoch 120, training loss: 1.4056552648544312 = 1.3980342149734497 + 0.001 * 7.621062755584717
Epoch 120, val loss: 1.4533576965332031
Epoch 130, training loss: 1.3034635782241821 = 1.2958837747573853 + 0.001 * 7.579840660095215
Epoch 130, val loss: 1.3672703504562378
Epoch 140, training loss: 1.2034423351287842 = 1.195920705795288 + 0.001 * 7.52168607711792
Epoch 140, val loss: 1.2840696573257446
Epoch 150, training loss: 1.104349970817566 = 1.0968937873840332 + 0.001 * 7.456198692321777
Epoch 150, val loss: 1.202344536781311
Epoch 160, training loss: 1.0066908597946167 = 0.9992820024490356 + 0.001 * 7.40887451171875
Epoch 160, val loss: 1.1216555833816528
Epoch 170, training loss: 0.9130580425262451 = 0.9056727886199951 + 0.001 * 7.385273456573486
Epoch 170, val loss: 1.0445929765701294
Epoch 180, training loss: 0.8270138502120972 = 0.8196508884429932 + 0.001 * 7.362987518310547
Epoch 180, val loss: 0.9748392701148987
Epoch 190, training loss: 0.7509558200836182 = 0.7436157464981079 + 0.001 * 7.340051174163818
Epoch 190, val loss: 0.9154117703437805
Epoch 200, training loss: 0.6847114562988281 = 0.6773941516876221 + 0.001 * 7.31728458404541
Epoch 200, val loss: 0.8674269318580627
Epoch 210, training loss: 0.6262764930725098 = 0.6189766526222229 + 0.001 * 7.299830436706543
Epoch 210, val loss: 0.829857587814331
Epoch 220, training loss: 0.573406457901001 = 0.5661202073097229 + 0.001 * 7.286258220672607
Epoch 220, val loss: 0.800995409488678
Epoch 230, training loss: 0.5246009826660156 = 0.5173178911209106 + 0.001 * 7.28310489654541
Epoch 230, val loss: 0.7787704467773438
Epoch 240, training loss: 0.47893720865249634 = 0.4716571867465973 + 0.001 * 7.280025005340576
Epoch 240, val loss: 0.7614380717277527
Epoch 250, training loss: 0.4360259175300598 = 0.4287455081939697 + 0.001 * 7.280398845672607
Epoch 250, val loss: 0.7480859756469727
Epoch 260, training loss: 0.3957461714744568 = 0.3884652256965637 + 0.001 * 7.280939102172852
Epoch 260, val loss: 0.7384687662124634
Epoch 270, training loss: 0.3580417335033417 = 0.35075971484184265 + 0.001 * 7.282018661499023
Epoch 270, val loss: 0.7323611378669739
Epoch 280, training loss: 0.3227483034133911 = 0.3154650628566742 + 0.001 * 7.283247470855713
Epoch 280, val loss: 0.7297446727752686
Epoch 290, training loss: 0.289534330368042 = 0.28224968910217285 + 0.001 * 7.284641265869141
Epoch 290, val loss: 0.7297601699829102
Epoch 300, training loss: 0.2580997347831726 = 0.25081339478492737 + 0.001 * 7.286328315734863
Epoch 300, val loss: 0.7318909764289856
Epoch 310, training loss: 0.22842438519001007 = 0.2211332619190216 + 0.001 * 7.29111909866333
Epoch 310, val loss: 0.7356306314468384
Epoch 320, training loss: 0.20080862939357758 = 0.19351732730865479 + 0.001 * 7.291299819946289
Epoch 320, val loss: 0.7404793500900269
Epoch 330, training loss: 0.1757081001996994 = 0.16841544210910797 + 0.001 * 7.292659282684326
Epoch 330, val loss: 0.7464773654937744
Epoch 340, training loss: 0.1534392088651657 = 0.1461452692747116 + 0.001 * 7.293932914733887
Epoch 340, val loss: 0.7534375786781311
Epoch 350, training loss: 0.13403084874153137 = 0.12673541903495789 + 0.001 * 7.2954301834106445
Epoch 350, val loss: 0.7613731026649475
Epoch 360, training loss: 0.11729133129119873 = 0.10999464243650436 + 0.001 * 7.296688556671143
Epoch 360, val loss: 0.7701316475868225
Epoch 370, training loss: 0.10293661803007126 = 0.09563898295164108 + 0.001 * 7.297635078430176
Epoch 370, val loss: 0.779547929763794
Epoch 380, training loss: 0.09067050367593765 = 0.08336648344993591 + 0.001 * 7.304020881652832
Epoch 380, val loss: 0.7894108295440674
Epoch 390, training loss: 0.08019184321165085 = 0.07289174944162369 + 0.001 * 7.300093173980713
Epoch 390, val loss: 0.7995114922523499
Epoch 400, training loss: 0.07126084715127945 = 0.06396191567182541 + 0.001 * 7.2989301681518555
Epoch 400, val loss: 0.8097283244132996
Epoch 410, training loss: 0.06364909559488297 = 0.056350477039813995 + 0.001 * 7.298618316650391
Epoch 410, val loss: 0.8200552463531494
Epoch 420, training loss: 0.057159069925546646 = 0.04986105486750603 + 0.001 * 7.29801607131958
Epoch 420, val loss: 0.8303858637809753
Epoch 430, training loss: 0.05161775276064873 = 0.04431953653693199 + 0.001 * 7.298214435577393
Epoch 430, val loss: 0.8406846523284912
Epoch 440, training loss: 0.0468718446791172 = 0.03957657516002655 + 0.001 * 7.2952680587768555
Epoch 440, val loss: 0.8508992791175842
Epoch 450, training loss: 0.04280595853924751 = 0.035504236817359924 + 0.001 * 7.301721096038818
Epoch 450, val loss: 0.8610137104988098
Epoch 460, training loss: 0.03929007798433304 = 0.031995341181755066 + 0.001 * 7.294736385345459
Epoch 460, val loss: 0.8709722757339478
Epoch 470, training loss: 0.03625142574310303 = 0.02895965985953808 + 0.001 * 7.291764259338379
Epoch 470, val loss: 0.8807362914085388
Epoch 480, training loss: 0.033614616841077805 = 0.02632184885442257 + 0.001 * 7.292766571044922
Epoch 480, val loss: 0.8903015851974487
Epoch 490, training loss: 0.03130682185292244 = 0.024018313735723495 + 0.001 * 7.288508415222168
Epoch 490, val loss: 0.8996381163597107
Epoch 500, training loss: 0.02928255870938301 = 0.021995428949594498 + 0.001 * 7.287130355834961
Epoch 500, val loss: 0.9087702631950378
Epoch 510, training loss: 0.027491196990013123 = 0.020206885412335396 + 0.001 * 7.284310817718506
Epoch 510, val loss: 0.9177069067955017
Epoch 520, training loss: 0.02589286118745804 = 0.018613414838910103 + 0.001 * 7.279445171356201
Epoch 520, val loss: 0.9264556765556335
Epoch 530, training loss: 0.024461805820465088 = 0.017186295241117477 + 0.001 * 7.275510311126709
Epoch 530, val loss: 0.9350358247756958
Epoch 540, training loss: 0.02317928336560726 = 0.01590374857187271 + 0.001 * 7.275534629821777
Epoch 540, val loss: 0.9434532523155212
Epoch 550, training loss: 0.02202015370130539 = 0.014748858287930489 + 0.001 * 7.271295070648193
Epoch 550, val loss: 0.951691210269928
Epoch 560, training loss: 0.020995253697037697 = 0.013707546517252922 + 0.001 * 7.28770637512207
Epoch 560, val loss: 0.9597676396369934
Epoch 570, training loss: 0.020040472969412804 = 0.012767244130373001 + 0.001 * 7.273228168487549
Epoch 570, val loss: 0.9676469564437866
Epoch 580, training loss: 0.01918760873377323 = 0.011916917748749256 + 0.001 * 7.27069091796875
Epoch 580, val loss: 0.9753298163414001
Epoch 590, training loss: 0.01841142028570175 = 0.011146800592541695 + 0.001 * 7.26461935043335
Epoch 590, val loss: 0.9828431010246277
Epoch 600, training loss: 0.017705895006656647 = 0.010448015294969082 + 0.001 * 7.25787878036499
Epoch 600, val loss: 0.9901365041732788
Epoch 610, training loss: 0.01708066649734974 = 0.009812739677727222 + 0.001 * 7.267927169799805
Epoch 610, val loss: 0.9972660541534424
Epoch 620, training loss: 0.016481878235936165 = 0.00923407543450594 + 0.001 * 7.247802734375
Epoch 620, val loss: 1.0041940212249756
Epoch 630, training loss: 0.01598755083978176 = 0.008705860935151577 + 0.001 * 7.281689167022705
Epoch 630, val loss: 1.0109302997589111
Epoch 640, training loss: 0.015469432808458805 = 0.008222808130085468 + 0.001 * 7.24662446975708
Epoch 640, val loss: 1.017480492591858
Epoch 650, training loss: 0.015022099018096924 = 0.007780029904097319 + 0.001 * 7.242069244384766
Epoch 650, val loss: 1.0238590240478516
Epoch 660, training loss: 0.014639431610703468 = 0.007373388856649399 + 0.001 * 7.2660417556762695
Epoch 660, val loss: 1.0300734043121338
Epoch 670, training loss: 0.014229025691747665 = 0.0069992439821362495 + 0.001 * 7.229780673980713
Epoch 670, val loss: 1.0360990762710571
Epoch 680, training loss: 0.013881761580705643 = 0.006654195487499237 + 0.001 * 7.227566242218018
Epoch 680, val loss: 1.0419965982437134
Epoch 690, training loss: 0.013586767017841339 = 0.006335296202450991 + 0.001 * 7.251471042633057
Epoch 690, val loss: 1.0476938486099243
Epoch 700, training loss: 0.013291251845657825 = 0.006040207110345364 + 0.001 * 7.251044273376465
Epoch 700, val loss: 1.053234577178955
Epoch 710, training loss: 0.013069144450128078 = 0.005766597576439381 + 0.001 * 7.302546501159668
Epoch 710, val loss: 1.0586366653442383
Epoch 720, training loss: 0.012739483267068863 = 0.005512482952326536 + 0.001 * 7.226999282836914
Epoch 720, val loss: 1.0638632774353027
Epoch 730, training loss: 0.01253898162394762 = 0.005276050418615341 + 0.001 * 7.262930870056152
Epoch 730, val loss: 1.0689703226089478
Epoch 740, training loss: 0.012252159416675568 = 0.005055711138993502 + 0.001 * 7.196447372436523
Epoch 740, val loss: 1.0739201307296753
Epoch 750, training loss: 0.012063341215252876 = 0.004850046243518591 + 0.001 * 7.213294506072998
Epoch 750, val loss: 1.0787581205368042
Epoch 760, training loss: 0.011853666976094246 = 0.004657790530472994 + 0.001 * 7.195876598358154
Epoch 760, val loss: 1.0834559202194214
Epoch 770, training loss: 0.01166946068406105 = 0.00447780080139637 + 0.001 * 7.191659927368164
Epoch 770, val loss: 1.0880223512649536
Epoch 780, training loss: 0.011517674662172794 = 0.004309115931391716 + 0.001 * 7.208558559417725
Epoch 780, val loss: 1.0924575328826904
Epoch 790, training loss: 0.011337464675307274 = 0.004150806460529566 + 0.001 * 7.1866583824157715
Epoch 790, val loss: 1.096787452697754
Epoch 800, training loss: 0.011161437258124352 = 0.004002049099653959 + 0.001 * 7.159388065338135
Epoch 800, val loss: 1.1010199785232544
Epoch 810, training loss: 0.011038491502404213 = 0.003862101584672928 + 0.001 * 7.176389694213867
Epoch 810, val loss: 1.1051284074783325
Epoch 820, training loss: 0.010939814150333405 = 0.0037302798591554165 + 0.001 * 7.20953369140625
Epoch 820, val loss: 1.1091108322143555
Epoch 830, training loss: 0.010800093412399292 = 0.0036059378180652857 + 0.001 * 7.194155693054199
Epoch 830, val loss: 1.1130030155181885
Epoch 840, training loss: 0.010642215609550476 = 0.003488522721454501 + 0.001 * 7.153692245483398
Epoch 840, val loss: 1.1168439388275146
Epoch 850, training loss: 0.010576790198683739 = 0.0033775174524635077 + 0.001 * 7.199272632598877
Epoch 850, val loss: 1.1205543279647827
Epoch 860, training loss: 0.010393491014838219 = 0.0032725129276514053 + 0.001 * 7.120978355407715
Epoch 860, val loss: 1.124184250831604
Epoch 870, training loss: 0.010303177870810032 = 0.003173111705109477 + 0.001 * 7.13006591796875
Epoch 870, val loss: 1.1277470588684082
Epoch 880, training loss: 0.01022318098694086 = 0.003078901907429099 + 0.001 * 7.144278526306152
Epoch 880, val loss: 1.1311711072921753
Epoch 890, training loss: 0.010131694376468658 = 0.0029895459301769733 + 0.001 * 7.142148017883301
Epoch 890, val loss: 1.134568452835083
Epoch 900, training loss: 0.01011730171740055 = 0.0029046416748315096 + 0.001 * 7.21265983581543
Epoch 900, val loss: 1.137858510017395
Epoch 910, training loss: 0.009948799386620522 = 0.002824006602168083 + 0.001 * 7.124792098999023
Epoch 910, val loss: 1.1410696506500244
Epoch 920, training loss: 0.009852679446339607 = 0.0027472367510199547 + 0.001 * 7.105442047119141
Epoch 920, val loss: 1.1442300081253052
Epoch 930, training loss: 0.009777367115020752 = 0.002674142364412546 + 0.001 * 7.103224277496338
Epoch 930, val loss: 1.1472837924957275
Epoch 940, training loss: 0.00974897388368845 = 0.0026044759433716536 + 0.001 * 7.144497871398926
Epoch 940, val loss: 1.1502889394760132
Epoch 950, training loss: 0.009693131782114506 = 0.0025380223523825407 + 0.001 * 7.155109405517578
Epoch 950, val loss: 1.1532176733016968
Epoch 960, training loss: 0.009607740677893162 = 0.002474650274962187 + 0.001 * 7.133090019226074
Epoch 960, val loss: 1.1560916900634766
Epoch 970, training loss: 0.00950920581817627 = 0.002414072398096323 + 0.001 * 7.0951337814331055
Epoch 970, val loss: 1.158894419670105
Epoch 980, training loss: 0.009483183734118938 = 0.0023562100250273943 + 0.001 * 7.1269731521606445
Epoch 980, val loss: 1.1616493463516235
Epoch 990, training loss: 0.009425700642168522 = 0.002300902269780636 + 0.001 * 7.124797821044922
Epoch 990, val loss: 1.1643460988998413
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 1.9513049125671387 = 1.942708134651184 + 0.001 * 8.59681510925293
Epoch 0, val loss: 1.9450910091400146
Epoch 10, training loss: 1.9419996738433838 = 1.9334028959274292 + 0.001 * 8.596758842468262
Epoch 10, val loss: 1.9357173442840576
Epoch 20, training loss: 1.9305058717727661 = 1.9219093322753906 + 0.001 * 8.59654426574707
Epoch 20, val loss: 1.9234849214553833
Epoch 30, training loss: 1.9140818119049072 = 1.90548574924469 + 0.001 * 8.596050262451172
Epoch 30, val loss: 1.9054791927337646
Epoch 40, training loss: 1.8894107341766357 = 1.880815863609314 + 0.001 * 8.594820022583008
Epoch 40, val loss: 1.8783793449401855
Epoch 50, training loss: 1.8544118404388428 = 1.845820426940918 + 0.001 * 8.59142017364502
Epoch 50, val loss: 1.8411959409713745
Epoch 60, training loss: 1.814368486404419 = 1.8057883977890015 + 0.001 * 8.580078125
Epoch 60, val loss: 1.802846908569336
Epoch 70, training loss: 1.7788861989974976 = 1.770355224609375 + 0.001 * 8.530940055847168
Epoch 70, val loss: 1.7741219997406006
Epoch 80, training loss: 1.735435962677002 = 1.7272157669067383 + 0.001 * 8.220235824584961
Epoch 80, val loss: 1.7385928630828857
Epoch 90, training loss: 1.6745201349258423 = 1.666501760482788 + 0.001 * 8.018319129943848
Epoch 90, val loss: 1.6861125230789185
Epoch 100, training loss: 1.5923253297805786 = 1.5844792127609253 + 0.001 * 7.846131324768066
Epoch 100, val loss: 1.614119052886963
Epoch 110, training loss: 1.4949589967727661 = 1.487250804901123 + 0.001 * 7.708137512207031
Epoch 110, val loss: 1.5315624475479126
Epoch 120, training loss: 1.3935041427612305 = 1.3858669996261597 + 0.001 * 7.6371989250183105
Epoch 120, val loss: 1.4498080015182495
Epoch 130, training loss: 1.2930725812911987 = 1.2854894399642944 + 0.001 * 7.583096981048584
Epoch 130, val loss: 1.3729921579360962
Epoch 140, training loss: 1.1945295333862305 = 1.1870291233062744 + 0.001 * 7.500412464141846
Epoch 140, val loss: 1.300096869468689
Epoch 150, training loss: 1.1000417470932007 = 1.0926024913787842 + 0.001 * 7.439246654510498
Epoch 150, val loss: 1.2304826974868774
Epoch 160, training loss: 1.0123498439788818 = 1.004941463470459 + 0.001 * 7.408344268798828
Epoch 160, val loss: 1.1657404899597168
Epoch 170, training loss: 0.931681752204895 = 0.9242939949035645 + 0.001 * 7.387784481048584
Epoch 170, val loss: 1.1067670583724976
Epoch 180, training loss: 0.856661856174469 = 0.8492927551269531 + 0.001 * 7.369094371795654
Epoch 180, val loss: 1.0529881715774536
Epoch 190, training loss: 0.7865946888923645 = 0.7792461514472961 + 0.001 * 7.348543167114258
Epoch 190, val loss: 1.0036906003952026
Epoch 200, training loss: 0.7218199968338013 = 0.7145017981529236 + 0.001 * 7.318199634552002
Epoch 200, val loss: 0.9591069221496582
Epoch 210, training loss: 0.6623266935348511 = 0.6550352573394775 + 0.001 * 7.291428565979004
Epoch 210, val loss: 0.9197182059288025
Epoch 220, training loss: 0.6067743301391602 = 0.5995141863822937 + 0.001 * 7.26013708114624
Epoch 220, val loss: 0.8846975564956665
Epoch 230, training loss: 0.5534562468528748 = 0.5462120175361633 + 0.001 * 7.244222164154053
Epoch 230, val loss: 0.8529000878334045
Epoch 240, training loss: 0.5010600090026855 = 0.4938235580921173 + 0.001 * 7.23647928237915
Epoch 240, val loss: 0.8234800100326538
Epoch 250, training loss: 0.4493507444858551 = 0.4421176016330719 + 0.001 * 7.233145713806152
Epoch 250, val loss: 0.7968375086784363
Epoch 260, training loss: 0.39924103021621704 = 0.3920111358165741 + 0.001 * 7.229889392852783
Epoch 260, val loss: 0.7744895815849304
Epoch 270, training loss: 0.3521880507469177 = 0.3449614644050598 + 0.001 * 7.226585388183594
Epoch 270, val loss: 0.7578583359718323
Epoch 280, training loss: 0.3094210922718048 = 0.3021979033946991 + 0.001 * 7.223202705383301
Epoch 280, val loss: 0.7473559975624084
Epoch 290, training loss: 0.2715696692466736 = 0.26434987783432007 + 0.001 * 7.219793319702148
Epoch 290, val loss: 0.7424082159996033
Epoch 300, training loss: 0.2386266440153122 = 0.23140861093997955 + 0.001 * 7.218039512634277
Epoch 300, val loss: 0.7420454621315002
Epoch 310, training loss: 0.21019737422466278 = 0.20298399031162262 + 0.001 * 7.213388919830322
Epoch 310, val loss: 0.7451968193054199
Epoch 320, training loss: 0.1857767105102539 = 0.1785673350095749 + 0.001 * 7.209381103515625
Epoch 320, val loss: 0.7510417699813843
Epoch 330, training loss: 0.1648143082857132 = 0.15761080384254456 + 0.001 * 7.203502178192139
Epoch 330, val loss: 0.7589882612228394
Epoch 340, training loss: 0.14678041636943817 = 0.13958153128623962 + 0.001 * 7.198885917663574
Epoch 340, val loss: 0.7685182690620422
Epoch 350, training loss: 0.13120223581790924 = 0.12401105463504791 + 0.001 * 7.191174030303955
Epoch 350, val loss: 0.7793263792991638
Epoch 360, training loss: 0.1176876351237297 = 0.11050974577665329 + 0.001 * 7.1778883934021
Epoch 360, val loss: 0.791108250617981
Epoch 370, training loss: 0.10595313459634781 = 0.09875702857971191 + 0.001 * 7.196104049682617
Epoch 370, val loss: 0.8035739660263062
Epoch 380, training loss: 0.09565895050764084 = 0.08849222958087921 + 0.001 * 7.166718006134033
Epoch 380, val loss: 0.8165344595909119
Epoch 390, training loss: 0.08665395528078079 = 0.07950212806463242 + 0.001 * 7.151824951171875
Epoch 390, val loss: 0.8298465609550476
Epoch 400, training loss: 0.07874679565429688 = 0.07160606980323792 + 0.001 * 7.140725612640381
Epoch 400, val loss: 0.8433816432952881
Epoch 410, training loss: 0.07183867692947388 = 0.06465418636798859 + 0.001 * 7.184487819671631
Epoch 410, val loss: 0.8570826649665833
Epoch 420, training loss: 0.06566426903009415 = 0.05852235481142998 + 0.001 * 7.1419172286987305
Epoch 420, val loss: 0.8707583546638489
Epoch 430, training loss: 0.0602291040122509 = 0.05310049653053284 + 0.001 * 7.128606796264648
Epoch 430, val loss: 0.8844003081321716
Epoch 440, training loss: 0.055419422686100006 = 0.04829810932278633 + 0.001 * 7.121315002441406
Epoch 440, val loss: 0.8979122638702393
Epoch 450, training loss: 0.05115470290184021 = 0.044035471975803375 + 0.001 * 7.1192307472229
Epoch 450, val loss: 0.911243736743927
Epoch 460, training loss: 0.04736180976033211 = 0.04024410620331764 + 0.001 * 7.117703914642334
Epoch 460, val loss: 0.9243655800819397
Epoch 470, training loss: 0.043985117226839066 = 0.036865249276161194 + 0.001 * 7.119868755340576
Epoch 470, val loss: 0.9372510313987732
Epoch 480, training loss: 0.04096567630767822 = 0.03384837508201599 + 0.001 * 7.117300033569336
Epoch 480, val loss: 0.9498797655105591
Epoch 490, training loss: 0.03827032446861267 = 0.03114921785891056 + 0.001 * 7.121108055114746
Epoch 490, val loss: 0.9622232913970947
Epoch 500, training loss: 0.035848468542099 = 0.02872982993721962 + 0.001 * 7.11863899230957
Epoch 500, val loss: 0.9742476940155029
Epoch 510, training loss: 0.03367430344223976 = 0.026557104662060738 + 0.001 * 7.117198944091797
Epoch 510, val loss: 0.985977053642273
Epoch 520, training loss: 0.03171723335981369 = 0.02460196614265442 + 0.001 * 7.115268707275391
Epoch 520, val loss: 0.9974222779273987
Epoch 530, training loss: 0.0299538467079401 = 0.022839289158582687 + 0.001 * 7.11455774307251
Epoch 530, val loss: 1.0085481405258179
Epoch 540, training loss: 0.028372257947921753 = 0.021246865391731262 + 0.001 * 7.125392436981201
Epoch 540, val loss: 1.0193687677383423
Epoch 550, training loss: 0.0269197728484869 = 0.019805606454610825 + 0.001 * 7.114166736602783
Epoch 550, val loss: 1.0298957824707031
Epoch 560, training loss: 0.025611501187086105 = 0.018498733639717102 + 0.001 * 7.112768173217773
Epoch 560, val loss: 1.040116310119629
Epoch 570, training loss: 0.024423599243164062 = 0.017311275005340576 + 0.001 * 7.1123247146606445
Epoch 570, val loss: 1.0500317811965942
Epoch 580, training loss: 0.02334143966436386 = 0.01623029261827469 + 0.001 * 7.111146450042725
Epoch 580, val loss: 1.0596610307693481
Epoch 590, training loss: 0.022358227521181107 = 0.015244383364915848 + 0.001 * 7.113844394683838
Epoch 590, val loss: 1.0690243244171143
Epoch 600, training loss: 0.021453751251101494 = 0.014343452639877796 + 0.001 * 7.110298156738281
Epoch 600, val loss: 1.0781017541885376
Epoch 610, training loss: 0.020627284422516823 = 0.01351869571954012 + 0.001 * 7.108588218688965
Epoch 610, val loss: 1.0869170427322388
Epoch 620, training loss: 0.019871411845088005 = 0.012762202881276608 + 0.001 * 7.109209060668945
Epoch 620, val loss: 1.0954742431640625
Epoch 630, training loss: 0.019179610535502434 = 0.012067025527358055 + 0.001 * 7.112584590911865
Epoch 630, val loss: 1.103784203529358
Epoch 640, training loss: 0.018534842878580093 = 0.01142692007124424 + 0.001 * 7.10792350769043
Epoch 640, val loss: 1.1118707656860352
Epoch 650, training loss: 0.017943747341632843 = 0.010835806839168072 + 0.001 * 7.107939720153809
Epoch 650, val loss: 1.119734287261963
Epoch 660, training loss: 0.017391378059983253 = 0.010286783799529076 + 0.001 * 7.104593753814697
Epoch 660, val loss: 1.1274570226669312
Epoch 670, training loss: 0.016879219561815262 = 0.009773777797818184 + 0.001 * 7.105442523956299
Epoch 670, val loss: 1.1350486278533936
Epoch 680, training loss: 0.016400178894400597 = 0.00929296389222145 + 0.001 * 7.107213973999023
Epoch 680, val loss: 1.1425725221633911
Epoch 690, training loss: 0.015961293131113052 = 0.00884215161204338 + 0.001 * 7.119140625
Epoch 690, val loss: 1.149990439414978
Epoch 700, training loss: 0.015528099611401558 = 0.008419823832809925 + 0.001 * 7.108275890350342
Epoch 700, val loss: 1.1573045253753662
Epoch 710, training loss: 0.015126688405871391 = 0.008024435490369797 + 0.001 * 7.102252006530762
Epoch 710, val loss: 1.1645305156707764
Epoch 720, training loss: 0.014751299284398556 = 0.007654447574168444 + 0.001 * 7.096851348876953
Epoch 720, val loss: 1.1716370582580566
Epoch 730, training loss: 0.014411864802241325 = 0.00730830617249012 + 0.001 * 7.10355806350708
Epoch 730, val loss: 1.1786097288131714
Epoch 740, training loss: 0.014080977067351341 = 0.006984505336731672 + 0.001 * 7.096471786499023
Epoch 740, val loss: 1.1854482889175415
Epoch 750, training loss: 0.013779320754110813 = 0.006681488361209631 + 0.001 * 7.097832202911377
Epoch 750, val loss: 1.1921530961990356
Epoch 760, training loss: 0.013493342325091362 = 0.006397740449756384 + 0.001 * 7.095602035522461
Epoch 760, val loss: 1.198693037033081
Epoch 770, training loss: 0.013221628032624722 = 0.006131850183010101 + 0.001 * 7.08977746963501
Epoch 770, val loss: 1.2050833702087402
Epoch 780, training loss: 0.012982349842786789 = 0.005882511846721172 + 0.001 * 7.099837303161621
Epoch 780, val loss: 1.2113195657730103
Epoch 790, training loss: 0.012736842967569828 = 0.005648546852171421 + 0.001 * 7.088295936584473
Epoch 790, val loss: 1.2174134254455566
Epoch 800, training loss: 0.012560693547129631 = 0.005428779870271683 + 0.001 * 7.131913185119629
Epoch 800, val loss: 1.2233470678329468
Epoch 810, training loss: 0.012320080772042274 = 0.005222178064286709 + 0.001 * 7.097902774810791
Epoch 810, val loss: 1.2291312217712402
Epoch 820, training loss: 0.012112436816096306 = 0.005027804058045149 + 0.001 * 7.08463191986084
Epoch 820, val loss: 1.2347866296768188
Epoch 830, training loss: 0.011951509863138199 = 0.004844683688133955 + 0.001 * 7.106825351715088
Epoch 830, val loss: 1.2402726411819458
Epoch 840, training loss: 0.01175432838499546 = 0.004671936854720116 + 0.001 * 7.082391738891602
Epoch 840, val loss: 1.245629072189331
Epoch 850, training loss: 0.011585269123315811 = 0.004508802201598883 + 0.001 * 7.076466083526611
Epoch 850, val loss: 1.250883936882019
Epoch 860, training loss: 0.011442596092820168 = 0.004354423843324184 + 0.001 * 7.088172435760498
Epoch 860, val loss: 1.2560105323791504
Epoch 870, training loss: 0.011296603828668594 = 0.004208097234368324 + 0.001 * 7.08850622177124
Epoch 870, val loss: 1.261015772819519
Epoch 880, training loss: 0.011142878793179989 = 0.004069097340106964 + 0.001 * 7.0737810134887695
Epoch 880, val loss: 1.26591956615448
Epoch 890, training loss: 0.01101079210639 = 0.003936782944947481 + 0.001 * 7.074009418487549
Epoch 890, val loss: 1.2707300186157227
Epoch 900, training loss: 0.010884026065468788 = 0.003810576628893614 + 0.001 * 7.07344913482666
Epoch 900, val loss: 1.2754422426223755
Epoch 910, training loss: 0.010758873075246811 = 0.003690006211400032 + 0.001 * 7.068867206573486
Epoch 910, val loss: 1.2800548076629639
Epoch 920, training loss: 0.010659114457666874 = 0.0035746251232922077 + 0.001 * 7.084488868713379
Epoch 920, val loss: 1.2845731973648071
Epoch 930, training loss: 0.010549599304795265 = 0.003464164910838008 + 0.001 * 7.0854339599609375
Epoch 930, val loss: 1.2890236377716064
Epoch 940, training loss: 0.010434923693537712 = 0.0033584460616111755 + 0.001 * 7.076477527618408
Epoch 940, val loss: 1.2933578491210938
Epoch 950, training loss: 0.010348513722419739 = 0.0032571738120168447 + 0.001 * 7.091339588165283
Epoch 950, val loss: 1.2976157665252686
Epoch 960, training loss: 0.010228576138615608 = 0.003160282736644149 + 0.001 * 7.06829309463501
Epoch 960, val loss: 1.3017454147338867
Epoch 970, training loss: 0.010122162289917469 = 0.003067578189074993 + 0.001 * 7.054583549499512
Epoch 970, val loss: 1.305808186531067
Epoch 980, training loss: 0.010054847225546837 = 0.002978807780891657 + 0.001 * 7.076038837432861
Epoch 980, val loss: 1.3097563982009888
Epoch 990, training loss: 0.009950956329703331 = 0.00289383577182889 + 0.001 * 7.0571208000183105
Epoch 990, val loss: 1.3135864734649658
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8376383763837639
The final CL Acc:0.79877, 0.00630, The final GNN Acc:0.84063, 0.00287
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10528])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9489175081253052 = 1.9403207302093506 + 0.001 * 8.596816062927246
Epoch 0, val loss: 1.9323856830596924
Epoch 10, training loss: 1.939091444015503 = 1.9304946660995483 + 0.001 * 8.596769332885742
Epoch 10, val loss: 1.9228969812393188
Epoch 20, training loss: 1.927039623260498 = 1.9184430837631226 + 0.001 * 8.596598625183105
Epoch 20, val loss: 1.9108810424804688
Epoch 30, training loss: 1.9101334810256958 = 1.901537299156189 + 0.001 * 8.596221923828125
Epoch 30, val loss: 1.8938413858413696
Epoch 40, training loss: 1.8854066133499146 = 1.8768112659454346 + 0.001 * 8.595324516296387
Epoch 40, val loss: 1.8693066835403442
Epoch 50, training loss: 1.8514386415481567 = 1.8428460359573364 + 0.001 * 8.592646598815918
Epoch 50, val loss: 1.837448000907898
Epoch 60, training loss: 1.8147988319396973 = 1.8062164783477783 + 0.001 * 8.58230209350586
Epoch 60, val loss: 1.807452917098999
Epoch 70, training loss: 1.7839399576187134 = 1.775409460067749 + 0.001 * 8.530524253845215
Epoch 70, val loss: 1.7847787141799927
Epoch 80, training loss: 1.744903326034546 = 1.7366429567337036 + 0.001 * 8.260415077209473
Epoch 80, val loss: 1.7514785528182983
Epoch 90, training loss: 1.6903717517852783 = 1.68220853805542 + 0.001 * 8.163243293762207
Epoch 90, val loss: 1.705042839050293
Epoch 100, training loss: 1.6159766912460327 = 1.6079044342041016 + 0.001 * 8.072242736816406
Epoch 100, val loss: 1.6436609029769897
Epoch 110, training loss: 1.5276694297790527 = 1.5197490453720093 + 0.001 * 7.920413494110107
Epoch 110, val loss: 1.5720499753952026
Epoch 120, training loss: 1.4360473155975342 = 1.428417444229126 + 0.001 * 7.629922866821289
Epoch 120, val loss: 1.4985597133636475
Epoch 130, training loss: 1.3443584442138672 = 1.3368113040924072 + 0.001 * 7.5471978187561035
Epoch 130, val loss: 1.426457166671753
Epoch 140, training loss: 1.2515431642532349 = 1.2440775632858276 + 0.001 * 7.465551853179932
Epoch 140, val loss: 1.3544384241104126
Epoch 150, training loss: 1.1569288969039917 = 1.1494853496551514 + 0.001 * 7.443511486053467
Epoch 150, val loss: 1.2817178964614868
Epoch 160, training loss: 1.0613781213760376 = 1.0539430379867554 + 0.001 * 7.435102462768555
Epoch 160, val loss: 1.2087047100067139
Epoch 170, training loss: 0.9668542742729187 = 0.9594241380691528 + 0.001 * 7.430139541625977
Epoch 170, val loss: 1.13675057888031
Epoch 180, training loss: 0.8762243986129761 = 0.8687983155250549 + 0.001 * 7.426108360290527
Epoch 180, val loss: 1.0681575536727905
Epoch 190, training loss: 0.7926363945007324 = 0.7852155566215515 + 0.001 * 7.420816898345947
Epoch 190, val loss: 1.0060276985168457
Epoch 200, training loss: 0.7177351713180542 = 0.7103210687637329 + 0.001 * 7.414125919342041
Epoch 200, val loss: 0.952875554561615
Epoch 210, training loss: 0.6512339115142822 = 0.6438284516334534 + 0.001 * 7.405458450317383
Epoch 210, val loss: 0.9091393947601318
Epoch 220, training loss: 0.5917813181877136 = 0.5843872427940369 + 0.001 * 7.394069671630859
Epoch 220, val loss: 0.8737556338310242
Epoch 230, training loss: 0.537870466709137 = 0.5304921865463257 + 0.001 * 7.378273010253906
Epoch 230, val loss: 0.8452534079551697
Epoch 240, training loss: 0.4880817234516144 = 0.4807237684726715 + 0.001 * 7.357942581176758
Epoch 240, val loss: 0.8222580552101135
Epoch 250, training loss: 0.441221684217453 = 0.43388038873672485 + 0.001 * 7.341309070587158
Epoch 250, val loss: 0.8038055896759033
Epoch 260, training loss: 0.3964138627052307 = 0.3890933096408844 + 0.001 * 7.3205461502075195
Epoch 260, val loss: 0.7891683578491211
Epoch 270, training loss: 0.353318989276886 = 0.34601375460624695 + 0.001 * 7.305229663848877
Epoch 270, val loss: 0.7779229283332825
Epoch 280, training loss: 0.3120788037776947 = 0.30478429794311523 + 0.001 * 7.294501781463623
Epoch 280, val loss: 0.7699036598205566
Epoch 290, training loss: 0.2731982171535492 = 0.26591089367866516 + 0.001 * 7.287323951721191
Epoch 290, val loss: 0.7651785016059875
Epoch 300, training loss: 0.23730731010437012 = 0.23002374172210693 + 0.001 * 7.283563613891602
Epoch 300, val loss: 0.7638574838638306
Epoch 310, training loss: 0.2049952745437622 = 0.1977098435163498 + 0.001 * 7.285431385040283
Epoch 310, val loss: 0.7660361528396606
Epoch 320, training loss: 0.17659853398799896 = 0.16931910812854767 + 0.001 * 7.279430866241455
Epoch 320, val loss: 0.7714397311210632
Epoch 330, training loss: 0.15217635035514832 = 0.14489762485027313 + 0.001 * 7.278730392456055
Epoch 330, val loss: 0.7797687649726868
Epoch 340, training loss: 0.1314668357372284 = 0.12419186532497406 + 0.001 * 7.274974346160889
Epoch 340, val loss: 0.7905516624450684
Epoch 350, training loss: 0.11407379060983658 = 0.10677944123744965 + 0.001 * 7.294346332550049
Epoch 350, val loss: 0.8033407926559448
Epoch 360, training loss: 0.09944669157266617 = 0.0921710729598999 + 0.001 * 7.275620460510254
Epoch 360, val loss: 0.8175389766693115
Epoch 370, training loss: 0.08717204630374908 = 0.0799001082777977 + 0.001 * 7.271938800811768
Epoch 370, val loss: 0.832607090473175
Epoch 380, training loss: 0.07683731615543365 = 0.06956548243761063 + 0.001 * 7.271836280822754
Epoch 380, val loss: 0.8481920957565308
Epoch 390, training loss: 0.06811205297708511 = 0.060841917991638184 + 0.001 * 7.270137786865234
Epoch 390, val loss: 0.8640769124031067
Epoch 400, training loss: 0.06073629483580589 = 0.05346425622701645 + 0.001 * 7.272039413452148
Epoch 400, val loss: 0.8800751566886902
Epoch 410, training loss: 0.05448239669203758 = 0.0472133569419384 + 0.001 * 7.269040584564209
Epoch 410, val loss: 0.8960441946983337
Epoch 420, training loss: 0.0491747111082077 = 0.04190337657928467 + 0.001 * 7.271332263946533
Epoch 420, val loss: 0.9118439555168152
Epoch 430, training loss: 0.044642046093940735 = 0.03737741336226463 + 0.001 * 7.264632225036621
Epoch 430, val loss: 0.9273980259895325
Epoch 440, training loss: 0.04077398404479027 = 0.03350427374243736 + 0.001 * 7.269710063934326
Epoch 440, val loss: 0.9426354169845581
Epoch 450, training loss: 0.037432145327329636 = 0.030175842344760895 + 0.001 * 7.2563042640686035
Epoch 450, val loss: 0.9575130939483643
Epoch 460, training loss: 0.0345618799328804 = 0.027302363887429237 + 0.001 * 7.259516716003418
Epoch 460, val loss: 0.9719920754432678
Epoch 470, training loss: 0.03206360340118408 = 0.0248096976429224 + 0.001 * 7.25390625
Epoch 470, val loss: 0.9860268831253052
Epoch 480, training loss: 0.029884932562708855 = 0.02263592928647995 + 0.001 * 7.249002933502197
Epoch 480, val loss: 0.9996148943901062
Epoch 490, training loss: 0.027980055660009384 = 0.02072838693857193 + 0.001 * 7.251669406890869
Epoch 490, val loss: 1.0127506256103516
Epoch 500, training loss: 0.026279442012310028 = 0.019042624160647392 + 0.001 * 7.23681640625
Epoch 500, val loss: 1.0255682468414307
Epoch 510, training loss: 0.024805136024951935 = 0.01754588820040226 + 0.001 * 7.259246826171875
Epoch 510, val loss: 1.037947654724121
Epoch 520, training loss: 0.023446228355169296 = 0.016212956979870796 + 0.001 * 7.23327112197876
Epoch 520, val loss: 1.04994535446167
Epoch 530, training loss: 0.022243693470954895 = 0.015021826140582561 + 0.001 * 7.221867084503174
Epoch 530, val loss: 1.0615969896316528
Epoch 540, training loss: 0.021213840693235397 = 0.013954433612525463 + 0.001 * 7.2594075202941895
Epoch 540, val loss: 1.072866439819336
Epoch 550, training loss: 0.02022341825067997 = 0.012996084988117218 + 0.001 * 7.227332592010498
Epoch 550, val loss: 1.08378005027771
Epoch 560, training loss: 0.01934840716421604 = 0.012133238837122917 + 0.001 * 7.215167999267578
Epoch 560, val loss: 1.0943577289581299
Epoch 570, training loss: 0.01856740191578865 = 0.011354048736393452 + 0.001 * 7.213353633880615
Epoch 570, val loss: 1.1046044826507568
Epoch 580, training loss: 0.01786411739885807 = 0.010648766532540321 + 0.001 * 7.21535062789917
Epoch 580, val loss: 1.1145299673080444
Epoch 590, training loss: 0.01723586954176426 = 0.010008588433265686 + 0.001 * 7.227280616760254
Epoch 590, val loss: 1.1241422891616821
Epoch 600, training loss: 0.016645003110170364 = 0.00942620262503624 + 0.001 * 7.218799114227295
Epoch 600, val loss: 1.1334565877914429
Epoch 610, training loss: 0.016096506267786026 = 0.00889497995376587 + 0.001 * 7.2015252113342285
Epoch 610, val loss: 1.1424833536148071
Epoch 620, training loss: 0.015589991584420204 = 0.008409133180975914 + 0.001 * 7.180858612060547
Epoch 620, val loss: 1.1512540578842163
Epoch 630, training loss: 0.015154007822275162 = 0.007963813841342926 + 0.001 * 7.1901936531066895
Epoch 630, val loss: 1.1597645282745361
Epoch 640, training loss: 0.014744493179023266 = 0.007554801646620035 + 0.001 * 7.189691066741943
Epoch 640, val loss: 1.1680293083190918
Epoch 650, training loss: 0.014370739459991455 = 0.007178286090493202 + 0.001 * 7.192452430725098
Epoch 650, val loss: 1.1760414838790894
Epoch 660, training loss: 0.014007353223860264 = 0.006830944214016199 + 0.001 * 7.176408767700195
Epoch 660, val loss: 1.1838281154632568
Epoch 670, training loss: 0.013688821345567703 = 0.006509819068014622 + 0.001 * 7.179002285003662
Epoch 670, val loss: 1.191385269165039
Epoch 680, training loss: 0.013390405103564262 = 0.006212424021214247 + 0.001 * 7.177980899810791
Epoch 680, val loss: 1.1987324953079224
Epoch 690, training loss: 0.013101750984787941 = 0.0059364656917750835 + 0.001 * 7.165285587310791
Epoch 690, val loss: 1.2058689594268799
Epoch 700, training loss: 0.012873457744717598 = 0.00568000040948391 + 0.001 * 7.193456649780273
Epoch 700, val loss: 1.212820053100586
Epoch 710, training loss: 0.0126078175380826 = 0.005441127344965935 + 0.001 * 7.166689872741699
Epoch 710, val loss: 1.2195802927017212
Epoch 720, training loss: 0.012377199716866016 = 0.00521832937374711 + 0.001 * 7.158870220184326
Epoch 720, val loss: 1.226163625717163
Epoch 730, training loss: 0.012178786098957062 = 0.005010202061384916 + 0.001 * 7.168583869934082
Epoch 730, val loss: 1.2325692176818848
Epoch 740, training loss: 0.011962518095970154 = 0.004815476480871439 + 0.001 * 7.147040843963623
Epoch 740, val loss: 1.2388039827346802
Epoch 750, training loss: 0.011799551546573639 = 0.004633012227714062 + 0.001 * 7.166539192199707
Epoch 750, val loss: 1.2448790073394775
Epoch 760, training loss: 0.011615291237831116 = 0.004461810924112797 + 0.001 * 7.153480052947998
Epoch 760, val loss: 1.250808596611023
Epoch 770, training loss: 0.011448604054749012 = 0.004300983157008886 + 0.001 * 7.147620677947998
Epoch 770, val loss: 1.256592035293579
Epoch 780, training loss: 0.011295564472675323 = 0.004149761516600847 + 0.001 * 7.145802021026611
Epoch 780, val loss: 1.2622225284576416
Epoch 790, training loss: 0.011171886697411537 = 0.004007337614893913 + 0.001 * 7.164548397064209
Epoch 790, val loss: 1.267719030380249
Epoch 800, training loss: 0.011010544374585152 = 0.0038730427622795105 + 0.001 * 7.1375017166137695
Epoch 800, val loss: 1.2730894088745117
Epoch 810, training loss: 0.01088018249720335 = 0.003746253903955221 + 0.001 * 7.133928298950195
Epoch 810, val loss: 1.2783229351043701
Epoch 820, training loss: 0.010768224485218525 = 0.0036264359951019287 + 0.001 * 7.141788005828857
Epoch 820, val loss: 1.2834357023239136
Epoch 830, training loss: 0.010653932578861713 = 0.0035130491014569998 + 0.001 * 7.140882968902588
Epoch 830, val loss: 1.2884324789047241
Epoch 840, training loss: 0.010569322854280472 = 0.0034056801814585924 + 0.001 * 7.163642406463623
Epoch 840, val loss: 1.2933077812194824
Epoch 850, training loss: 0.010445969179272652 = 0.0033038898836821318 + 0.001 * 7.142078876495361
Epoch 850, val loss: 1.2980749607086182
Epoch 860, training loss: 0.010338847525417805 = 0.00320730684325099 + 0.001 * 7.131540298461914
Epoch 860, val loss: 1.3027310371398926
Epoch 870, training loss: 0.010263214819133282 = 0.0031155983451753855 + 0.001 * 7.147616386413574
Epoch 870, val loss: 1.307295560836792
Epoch 880, training loss: 0.010156621225178242 = 0.003028398146852851 + 0.001 * 7.128222942352295
Epoch 880, val loss: 1.3117471933364868
Epoch 890, training loss: 0.010075432248413563 = 0.002945446642115712 + 0.001 * 7.1299848556518555
Epoch 890, val loss: 1.3161182403564453
Epoch 900, training loss: 0.009986326098442078 = 0.002866471419110894 + 0.001 * 7.119853973388672
Epoch 900, val loss: 1.3203797340393066
Epoch 910, training loss: 0.009911724366247654 = 0.0027911928482353687 + 0.001 * 7.12053108215332
Epoch 910, val loss: 1.324557900428772
Epoch 920, training loss: 0.0098428251221776 = 0.0027194193098694086 + 0.001 * 7.123405933380127
Epoch 920, val loss: 1.3286395072937012
Epoch 930, training loss: 0.00975782610476017 = 0.0026509121526032686 + 0.001 * 7.106914043426514
Epoch 930, val loss: 1.3326358795166016
Epoch 940, training loss: 0.009709564968943596 = 0.0025854494888335466 + 0.001 * 7.124114990234375
Epoch 940, val loss: 1.3365495204925537
Epoch 950, training loss: 0.009639983996748924 = 0.0025228792801499367 + 0.001 * 7.117104530334473
Epoch 950, val loss: 1.3403825759887695
Epoch 960, training loss: 0.009573876857757568 = 0.0024630308616906404 + 0.001 * 7.110846042633057
Epoch 960, val loss: 1.3441282510757446
Epoch 970, training loss: 0.009517054073512554 = 0.0024057806003838778 + 0.001 * 7.111273288726807
Epoch 970, val loss: 1.3478033542633057
Epoch 980, training loss: 0.009460357949137688 = 0.0023508891463279724 + 0.001 * 7.109468460083008
Epoch 980, val loss: 1.3514076471328735
Epoch 990, training loss: 0.00942591018974781 = 0.00229834858328104 + 0.001 * 7.127561569213867
Epoch 990, val loss: 1.354920506477356
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 1.9445230960845947 = 1.9359261989593506 + 0.001 * 8.596839904785156
Epoch 0, val loss: 1.9353829622268677
Epoch 10, training loss: 1.9360612630844116 = 1.927464485168457 + 0.001 * 8.5968017578125
Epoch 10, val loss: 1.9271280765533447
Epoch 20, training loss: 1.9258379936218262 = 1.9172413349151611 + 0.001 * 8.596664428710938
Epoch 20, val loss: 1.9172439575195312
Epoch 30, training loss: 1.9116886854171753 = 1.903092384338379 + 0.001 * 8.596345901489258
Epoch 30, val loss: 1.9037669897079468
Epoch 40, training loss: 1.8907866477966309 = 1.8821910619735718 + 0.001 * 8.595616340637207
Epoch 40, val loss: 1.8843252658843994
Epoch 50, training loss: 1.8606092929840088 = 1.852015733718872 + 0.001 * 8.593606948852539
Epoch 50, val loss: 1.857436180114746
Epoch 60, training loss: 1.8232862949371338 = 1.8147001266479492 + 0.001 * 8.586201667785645
Epoch 60, val loss: 1.826311707496643
Epoch 70, training loss: 1.786765217781067 = 1.7782158851623535 + 0.001 * 8.549318313598633
Epoch 70, val loss: 1.7958369255065918
Epoch 80, training loss: 1.745914340019226 = 1.7376246452331543 + 0.001 * 8.289669036865234
Epoch 80, val loss: 1.7574137449264526
Epoch 90, training loss: 1.6891485452651978 = 1.6811319589614868 + 0.001 * 8.016582489013672
Epoch 90, val loss: 1.7071702480316162
Epoch 100, training loss: 1.6134803295135498 = 1.6055964231491089 + 0.001 * 7.883870601654053
Epoch 100, val loss: 1.64667809009552
Epoch 110, training loss: 1.522171974182129 = 1.514464259147644 + 0.001 * 7.707756519317627
Epoch 110, val loss: 1.5729951858520508
Epoch 120, training loss: 1.4236443042755127 = 1.416096568107605 + 0.001 * 7.547722816467285
Epoch 120, val loss: 1.4921802282333374
Epoch 130, training loss: 1.325412631034851 = 1.3179410696029663 + 0.001 * 7.4715352058410645
Epoch 130, val loss: 1.4118456840515137
Epoch 140, training loss: 1.2318757772445679 = 1.2244453430175781 + 0.001 * 7.430443286895752
Epoch 140, val loss: 1.3370743989944458
Epoch 150, training loss: 1.144545078277588 = 1.1371310949325562 + 0.001 * 7.41398286819458
Epoch 150, val loss: 1.2693276405334473
Epoch 160, training loss: 1.0631177425384521 = 1.0557111501693726 + 0.001 * 7.406582355499268
Epoch 160, val loss: 1.207740068435669
Epoch 170, training loss: 0.9864842295646667 = 0.9790809750556946 + 0.001 * 7.403234958648682
Epoch 170, val loss: 1.1503444910049438
Epoch 180, training loss: 0.913292646408081 = 0.9058927893638611 + 0.001 * 7.399843692779541
Epoch 180, val loss: 1.0950303077697754
Epoch 190, training loss: 0.8429635763168335 = 0.8355656266212463 + 0.001 * 7.397970676422119
Epoch 190, val loss: 1.0412616729736328
Epoch 200, training loss: 0.7759482860565186 = 0.7685510516166687 + 0.001 * 7.397211074829102
Epoch 200, val loss: 0.9901112914085388
Epoch 210, training loss: 0.7132102847099304 = 0.7058135867118835 + 0.001 * 7.396693229675293
Epoch 210, val loss: 0.9430977702140808
Epoch 220, training loss: 0.6550335884094238 = 0.6476378440856934 + 0.001 * 7.395751476287842
Epoch 220, val loss: 0.9023122191429138
Epoch 230, training loss: 0.6004101037979126 = 0.5930164456367493 + 0.001 * 7.393651008605957
Epoch 230, val loss: 0.8678964376449585
Epoch 240, training loss: 0.5477898716926575 = 0.5404006838798523 + 0.001 * 7.389200210571289
Epoch 240, val loss: 0.8389436602592468
Epoch 250, training loss: 0.4960521459579468 = 0.48867160081863403 + 0.001 * 7.380536079406738
Epoch 250, val loss: 0.8149070143699646
Epoch 260, training loss: 0.4450591206550598 = 0.43769341707229614 + 0.001 * 7.3657145500183105
Epoch 260, val loss: 0.7962563037872314
Epoch 270, training loss: 0.3958134949207306 = 0.3884667456150055 + 0.001 * 7.3467583656311035
Epoch 270, val loss: 0.7835897207260132
Epoch 280, training loss: 0.3497818112373352 = 0.34245413541793823 + 0.001 * 7.327687740325928
Epoch 280, val loss: 0.777314305305481
Epoch 290, training loss: 0.3081058859825134 = 0.30079570412635803 + 0.001 * 7.310175895690918
Epoch 290, val loss: 0.7771653532981873
Epoch 300, training loss: 0.27120235562324524 = 0.2638997435569763 + 0.001 * 7.302613258361816
Epoch 300, val loss: 0.7825292348861694
Epoch 310, training loss: 0.23886580765247345 = 0.23157420754432678 + 0.001 * 7.291600227355957
Epoch 310, val loss: 0.7925008535385132
Epoch 320, training loss: 0.21068376302719116 = 0.20339591801166534 + 0.001 * 7.287842750549316
Epoch 320, val loss: 0.8060081601142883
Epoch 330, training loss: 0.18616566061973572 = 0.17888344824314117 + 0.001 * 7.282207489013672
Epoch 330, val loss: 0.8221360445022583
Epoch 340, training loss: 0.1648550182580948 = 0.1575773060321808 + 0.001 * 7.277718544006348
Epoch 340, val loss: 0.8401232361793518
Epoch 350, training loss: 0.14631740748882294 = 0.1390434354543686 + 0.001 * 7.2739763259887695
Epoch 350, val loss: 0.8595230579376221
Epoch 360, training loss: 0.13017846643924713 = 0.12290719151496887 + 0.001 * 7.271268367767334
Epoch 360, val loss: 0.8798321485519409
Epoch 370, training loss: 0.11610352247953415 = 0.10883543640375137 + 0.001 * 7.26808500289917
Epoch 370, val loss: 0.9008683562278748
Epoch 380, training loss: 0.10382039099931717 = 0.09655403345823288 + 0.001 * 7.266357421875
Epoch 380, val loss: 0.9225565195083618
Epoch 390, training loss: 0.0930832251906395 = 0.0858234390616417 + 0.001 * 7.25978422164917
Epoch 390, val loss: 0.9447036385536194
Epoch 400, training loss: 0.08369820564985275 = 0.07644110172986984 + 0.001 * 7.257106304168701
Epoch 400, val loss: 0.9672250151634216
Epoch 410, training loss: 0.07547921687364578 = 0.06822900474071503 + 0.001 * 7.250210285186768
Epoch 410, val loss: 0.9899939894676208
Epoch 420, training loss: 0.06827827543020248 = 0.06102965772151947 + 0.001 * 7.248616695404053
Epoch 420, val loss: 1.0128346681594849
Epoch 430, training loss: 0.061956364661455154 = 0.05471329763531685 + 0.001 * 7.24306583404541
Epoch 430, val loss: 1.0356097221374512
Epoch 440, training loss: 0.056404899805784225 = 0.0491633303463459 + 0.001 * 7.241567611694336
Epoch 440, val loss: 1.0582243204116821
Epoch 450, training loss: 0.05151871591806412 = 0.04428260773420334 + 0.001 * 7.236107349395752
Epoch 450, val loss: 1.0805532932281494
Epoch 460, training loss: 0.0472194105386734 = 0.03998662158846855 + 0.001 * 7.232786655426025
Epoch 460, val loss: 1.102507472038269
Epoch 470, training loss: 0.04342589154839516 = 0.036203719675540924 + 0.001 * 7.222171306610107
Epoch 470, val loss: 1.1240086555480957
Epoch 480, training loss: 0.0401005819439888 = 0.03287116438150406 + 0.001 * 7.229415416717529
Epoch 480, val loss: 1.144953727722168
Epoch 490, training loss: 0.037141866981983185 = 0.029933486133813858 + 0.001 * 7.208379745483398
Epoch 490, val loss: 1.1653022766113281
Epoch 500, training loss: 0.03456395864486694 = 0.027341004461050034 + 0.001 * 7.222951889038086
Epoch 500, val loss: 1.185046672821045
Epoch 510, training loss: 0.032254576683044434 = 0.02505042590200901 + 0.001 * 7.204150676727295
Epoch 510, val loss: 1.2041431665420532
Epoch 520, training loss: 0.03020784817636013 = 0.02302221767604351 + 0.001 * 7.185629844665527
Epoch 520, val loss: 1.2225779294967651
Epoch 530, training loss: 0.028432536870241165 = 0.021222298964858055 + 0.001 * 7.210238456726074
Epoch 530, val loss: 1.240361213684082
Epoch 540, training loss: 0.026814449578523636 = 0.01962064392864704 + 0.001 * 7.193806171417236
Epoch 540, val loss: 1.2574713230133057
Epoch 550, training loss: 0.025389742106199265 = 0.018191395327448845 + 0.001 * 7.198346138000488
Epoch 550, val loss: 1.273988127708435
Epoch 560, training loss: 0.024082642048597336 = 0.01691209338605404 + 0.001 * 7.170549392700195
Epoch 560, val loss: 1.2899037599563599
Epoch 570, training loss: 0.022941388189792633 = 0.015763789415359497 + 0.001 * 7.177598476409912
Epoch 570, val loss: 1.3052639961242676
Epoch 580, training loss: 0.02191057987511158 = 0.014730357564985752 + 0.001 * 7.1802215576171875
Epoch 580, val loss: 1.3201053142547607
Epoch 590, training loss: 0.020961705595254898 = 0.013797339051961899 + 0.001 * 7.164365768432617
Epoch 590, val loss: 1.3343867063522339
Epoch 600, training loss: 0.020108671858906746 = 0.01295266393572092 + 0.001 * 7.156007289886475
Epoch 600, val loss: 1.3482168912887573
Epoch 610, training loss: 0.01936711184680462 = 0.012185840867459774 + 0.001 * 7.181271076202393
Epoch 610, val loss: 1.3615792989730835
Epoch 620, training loss: 0.018644209951162338 = 0.011487818323075771 + 0.001 * 7.15639066696167
Epoch 620, val loss: 1.3744909763336182
Epoch 630, training loss: 0.018007293343544006 = 0.01085070800036192 + 0.001 * 7.156585216522217
Epoch 630, val loss: 1.3869656324386597
Epoch 640, training loss: 0.017428943887352943 = 0.010267823934555054 + 0.001 * 7.161118984222412
Epoch 640, val loss: 1.399065375328064
Epoch 650, training loss: 0.016887396574020386 = 0.009733200073242188 + 0.001 * 7.154195785522461
Epoch 650, val loss: 1.4107611179351807
Epoch 660, training loss: 0.016382552683353424 = 0.009241747669875622 + 0.001 * 7.140805244445801
Epoch 660, val loss: 1.422078251838684
Epoch 670, training loss: 0.015938879922032356 = 0.008789037354290485 + 0.001 * 7.149842262268066
Epoch 670, val loss: 1.4330523014068604
Epoch 680, training loss: 0.015517003834247589 = 0.008371122181415558 + 0.001 * 7.145881175994873
Epoch 680, val loss: 1.4436891078948975
Epoch 690, training loss: 0.01514103077352047 = 0.007984497584402561 + 0.001 * 7.156533241271973
Epoch 690, val loss: 1.4540081024169922
Epoch 700, training loss: 0.01476735807955265 = 0.007625989615917206 + 0.001 * 7.141368389129639
Epoch 700, val loss: 1.464051604270935
Epoch 710, training loss: 0.014418652281165123 = 0.0072929090820252895 + 0.001 * 7.125743389129639
Epoch 710, val loss: 1.4737838506698608
Epoch 720, training loss: 0.014103144407272339 = 0.0069826338440179825 + 0.001 * 7.120509624481201
Epoch 720, val loss: 1.48325514793396
Epoch 730, training loss: 0.013827158138155937 = 0.006692678667604923 + 0.001 * 7.134478569030762
Epoch 730, val loss: 1.4924798011779785
Epoch 740, training loss: 0.01353834755718708 = 0.006420826073735952 + 0.001 * 7.117521286010742
Epoch 740, val loss: 1.501488208770752
Epoch 750, training loss: 0.013288411311805248 = 0.006165196653455496 + 0.001 * 7.123214244842529
Epoch 750, val loss: 1.5102993249893188
Epoch 760, training loss: 0.013045341707766056 = 0.005924226716160774 + 0.001 * 7.121114730834961
Epoch 760, val loss: 1.518915057182312
Epoch 770, training loss: 0.01281015295535326 = 0.005696540232747793 + 0.001 * 7.113612174987793
Epoch 770, val loss: 1.527371883392334
Epoch 780, training loss: 0.012590330094099045 = 0.005481230095028877 + 0.001 * 7.109099388122559
Epoch 780, val loss: 1.5356981754302979
Epoch 790, training loss: 0.012389812618494034 = 0.005277396645396948 + 0.001 * 7.1124162673950195
Epoch 790, val loss: 1.5438809394836426
Epoch 800, training loss: 0.012234953232109547 = 0.005084340926259756 + 0.001 * 7.150611877441406
Epoch 800, val loss: 1.5519309043884277
Epoch 810, training loss: 0.012005653232336044 = 0.004901378881186247 + 0.001 * 7.104274272918701
Epoch 810, val loss: 1.5598230361938477
Epoch 820, training loss: 0.011832339689135551 = 0.0047273533418774605 + 0.001 * 7.104986190795898
Epoch 820, val loss: 1.5675982236862183
Epoch 830, training loss: 0.011657429859042168 = 0.004560957662761211 + 0.001 * 7.096471786499023
Epoch 830, val loss: 1.5752475261688232
Epoch 840, training loss: 0.01149090938270092 = 0.004401142243295908 + 0.001 * 7.089766502380371
Epoch 840, val loss: 1.5827686786651611
Epoch 850, training loss: 0.011353028938174248 = 0.004247327335178852 + 0.001 * 7.105700969696045
Epoch 850, val loss: 1.5901774168014526
Epoch 860, training loss: 0.011184800416231155 = 0.0040994929149746895 + 0.001 * 7.0853071212768555
Epoch 860, val loss: 1.597441554069519
Epoch 870, training loss: 0.011075638234615326 = 0.003957700449973345 + 0.001 * 7.1179375648498535
Epoch 870, val loss: 1.6045842170715332
Epoch 880, training loss: 0.010910972021520138 = 0.0038220093119889498 + 0.001 * 7.088962078094482
Epoch 880, val loss: 1.6115951538085938
Epoch 890, training loss: 0.01080079935491085 = 0.003692371305078268 + 0.001 * 7.108427047729492
Epoch 890, val loss: 1.618465781211853
Epoch 900, training loss: 0.01065620593726635 = 0.0035685808397829533 + 0.001 * 7.087625026702881
Epoch 900, val loss: 1.6252269744873047
Epoch 910, training loss: 0.01053774543106556 = 0.0034506325609982014 + 0.001 * 7.087112903594971
Epoch 910, val loss: 1.6318720579147339
Epoch 920, training loss: 0.01041389349848032 = 0.003338310867547989 + 0.001 * 7.075582504272461
Epoch 920, val loss: 1.638375163078308
Epoch 930, training loss: 0.010301587171852589 = 0.003231349866837263 + 0.001 * 7.070237159729004
Epoch 930, val loss: 1.6447738409042358
Epoch 940, training loss: 0.010191213339567184 = 0.0031296112574636936 + 0.001 * 7.061601161956787
Epoch 940, val loss: 1.6510310173034668
Epoch 950, training loss: 0.010093597695231438 = 0.003032797947525978 + 0.001 * 7.0607991218566895
Epoch 950, val loss: 1.6571584939956665
Epoch 960, training loss: 0.010015632025897503 = 0.002940746722742915 + 0.001 * 7.07488489151001
Epoch 960, val loss: 1.663170576095581
Epoch 970, training loss: 0.00991545245051384 = 0.0028531828429549932 + 0.001 * 7.06226921081543
Epoch 970, val loss: 1.6690607070922852
Epoch 980, training loss: 0.009833582676947117 = 0.002769818063825369 + 0.001 * 7.0637640953063965
Epoch 980, val loss: 1.6748111248016357
Epoch 990, training loss: 0.009748989716172218 = 0.0026905168779194355 + 0.001 * 7.058472633361816
Epoch 990, val loss: 1.6804559230804443
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 1.951721429824829 = 1.9431246519088745 + 0.001 * 8.596783638000488
Epoch 0, val loss: 1.944366216659546
Epoch 10, training loss: 1.94170343875885 = 1.933106780052185 + 0.001 * 8.596715927124023
Epoch 10, val loss: 1.9341683387756348
Epoch 20, training loss: 1.9292035102844238 = 1.9206069707870483 + 0.001 * 8.596500396728516
Epoch 20, val loss: 1.9216301441192627
Epoch 30, training loss: 1.912033200263977 = 1.9034371376037598 + 0.001 * 8.596017837524414
Epoch 30, val loss: 1.9044196605682373
Epoch 40, training loss: 1.8874387741088867 = 1.878843903541565 + 0.001 * 8.594854354858398
Epoch 40, val loss: 1.8799062967300415
Epoch 50, training loss: 1.853806734085083 = 1.8452152013778687 + 0.001 * 8.591534614562988
Epoch 50, val loss: 1.8474962711334229
Epoch 60, training loss: 1.8156996965408325 = 1.8071199655532837 + 0.001 * 8.579754829406738
Epoch 60, val loss: 1.81353759765625
Epoch 70, training loss: 1.7808643579483032 = 1.7723411321640015 + 0.001 * 8.523263931274414
Epoch 70, val loss: 1.78435218334198
Epoch 80, training loss: 1.7396807670593262 = 1.731507420539856 + 0.001 * 8.173331260681152
Epoch 80, val loss: 1.7473915815353394
Epoch 90, training loss: 1.6834865808486938 = 1.6755610704421997 + 0.001 * 7.9255242347717285
Epoch 90, val loss: 1.6965605020523071
Epoch 100, training loss: 1.6075092554092407 = 1.5998032093048096 + 0.001 * 7.706014633178711
Epoch 100, val loss: 1.630281686782837
Epoch 110, training loss: 1.5123779773712158 = 1.5047712326049805 + 0.001 * 7.606708526611328
Epoch 110, val loss: 1.5490632057189941
Epoch 120, training loss: 1.405117154121399 = 1.3975485563278198 + 0.001 * 7.568624496459961
Epoch 120, val loss: 1.460005521774292
Epoch 130, training loss: 1.2941969633102417 = 1.2866790294647217 + 0.001 * 7.517923355102539
Epoch 130, val loss: 1.3687082529067993
Epoch 140, training loss: 1.1879297494888306 = 1.1804614067077637 + 0.001 * 7.468336582183838
Epoch 140, val loss: 1.282987356185913
Epoch 150, training loss: 1.0921491384506226 = 1.0847240686416626 + 0.001 * 7.425093650817871
Epoch 150, val loss: 1.208392858505249
Epoch 160, training loss: 1.0072317123413086 = 0.9998397827148438 + 0.001 * 7.391873359680176
Epoch 160, val loss: 1.1458420753479004
Epoch 170, training loss: 0.9306631088256836 = 0.923292338848114 + 0.001 * 7.370799541473389
Epoch 170, val loss: 1.0935765504837036
Epoch 180, training loss: 0.8596638441085815 = 0.8523033261299133 + 0.001 * 7.360528945922852
Epoch 180, val loss: 1.0485881567001343
Epoch 190, training loss: 0.7924858331680298 = 0.7851314544677734 + 0.001 * 7.354356288909912
Epoch 190, val loss: 1.0085234642028809
Epoch 200, training loss: 0.7282488346099854 = 0.720900297164917 + 0.001 * 7.34853458404541
Epoch 200, val loss: 0.9715983271598816
Epoch 210, training loss: 0.6665064096450806 = 0.6591644287109375 + 0.001 * 7.341989517211914
Epoch 210, val loss: 0.9376130104064941
Epoch 220, training loss: 0.6069244742393494 = 0.599591076374054 + 0.001 * 7.333384990692139
Epoch 220, val loss: 0.90646892786026
Epoch 230, training loss: 0.5493254661560059 = 0.5420017242431641 + 0.001 * 7.323728084564209
Epoch 230, val loss: 0.8781318068504333
Epoch 240, training loss: 0.4937831461429596 = 0.48647552728652954 + 0.001 * 7.307632923126221
Epoch 240, val loss: 0.8523500561714172
Epoch 250, training loss: 0.44073235988616943 = 0.4334428310394287 + 0.001 * 7.289514064788818
Epoch 250, val loss: 0.8294793963432312
Epoch 260, training loss: 0.3907528221607208 = 0.3834749758243561 + 0.001 * 7.277850151062012
Epoch 260, val loss: 0.8103750944137573
Epoch 270, training loss: 0.3442240357398987 = 0.33696532249450684 + 0.001 * 7.258718013763428
Epoch 270, val loss: 0.7953631281852722
Epoch 280, training loss: 0.3012544512748718 = 0.29400506615638733 + 0.001 * 7.2493896484375
Epoch 280, val loss: 0.7845057249069214
Epoch 290, training loss: 0.26178741455078125 = 0.25454550981521606 + 0.001 * 7.241891860961914
Epoch 290, val loss: 0.777182400226593
Epoch 300, training loss: 0.22591586410999298 = 0.21868190169334412 + 0.001 * 7.233957767486572
Epoch 300, val loss: 0.7733380198478699
Epoch 310, training loss: 0.19397515058517456 = 0.18674594163894653 + 0.001 * 7.229212284088135
Epoch 310, val loss: 0.7727641463279724
Epoch 320, training loss: 0.16627278923988342 = 0.15904644131660461 + 0.001 * 7.226354598999023
Epoch 320, val loss: 0.7755854725837708
Epoch 330, training loss: 0.14279183745384216 = 0.13557842373847961 + 0.001 * 7.213418483734131
Epoch 330, val loss: 0.7817705869674683
Epoch 340, training loss: 0.12320154160261154 = 0.11599361896514893 + 0.001 * 7.207921028137207
Epoch 340, val loss: 0.7908546328544617
Epoch 350, training loss: 0.10695341229438782 = 0.0997462049126625 + 0.001 * 7.207208633422852
Epoch 350, val loss: 0.8022339344024658
Epoch 360, training loss: 0.0934620201587677 = 0.08626634627580643 + 0.001 * 7.195670127868652
Epoch 360, val loss: 0.8152632713317871
Epoch 370, training loss: 0.08224222809076309 = 0.07505171746015549 + 0.001 * 7.190506935119629
Epoch 370, val loss: 0.8293163180351257
Epoch 380, training loss: 0.07287245988845825 = 0.06568483263254166 + 0.001 * 7.187626838684082
Epoch 380, val loss: 0.8439390063285828
Epoch 390, training loss: 0.06500983238220215 = 0.05782148987054825 + 0.001 * 7.188344955444336
Epoch 390, val loss: 0.8588495254516602
Epoch 400, training loss: 0.058349013328552246 = 0.051168955862522125 + 0.001 * 7.180055141448975
Epoch 400, val loss: 0.8738272190093994
Epoch 410, training loss: 0.05270281806588173 = 0.045518215745687485 + 0.001 * 7.184603214263916
Epoch 410, val loss: 0.8887178301811218
Epoch 420, training loss: 0.04786629229784012 = 0.04068810120224953 + 0.001 * 7.178191661834717
Epoch 420, val loss: 0.9034813046455383
Epoch 430, training loss: 0.04372129589319229 = 0.03654124215245247 + 0.001 * 7.180054664611816
Epoch 430, val loss: 0.9180129170417786
Epoch 440, training loss: 0.04013724625110626 = 0.03296441584825516 + 0.001 * 7.172828197479248
Epoch 440, val loss: 0.9322507381439209
Epoch 450, training loss: 0.03703142702579498 = 0.029865127056837082 + 0.001 * 7.166299819946289
Epoch 450, val loss: 0.9461566209793091
Epoch 460, training loss: 0.03433327004313469 = 0.027167314663529396 + 0.001 * 7.165954113006592
Epoch 460, val loss: 0.9597271084785461
Epoch 470, training loss: 0.03196500241756439 = 0.02480803057551384 + 0.001 * 7.156972408294678
Epoch 470, val loss: 0.9729408621788025
Epoch 480, training loss: 0.02990429475903511 = 0.022736337035894394 + 0.001 * 7.167957782745361
Epoch 480, val loss: 0.9857848882675171
Epoch 490, training loss: 0.028062226250767708 = 0.020909562706947327 + 0.001 * 7.152663707733154
Epoch 490, val loss: 0.9982646703720093
Epoch 500, training loss: 0.02645568735897541 = 0.019291911274194717 + 0.001 * 7.16377592086792
Epoch 500, val loss: 1.0103827714920044
Epoch 510, training loss: 0.02500038594007492 = 0.01785373128950596 + 0.001 * 7.1466546058654785
Epoch 510, val loss: 1.0221604108810425
Epoch 520, training loss: 0.023735152557492256 = 0.016570324078202248 + 0.001 * 7.164828300476074
Epoch 520, val loss: 1.0335966348648071
Epoch 530, training loss: 0.02257085219025612 = 0.015420728363096714 + 0.001 * 7.150123596191406
Epoch 530, val loss: 1.0447056293487549
Epoch 540, training loss: 0.021524937823414803 = 0.014387494884431362 + 0.001 * 7.137442588806152
Epoch 540, val loss: 1.0555020570755005
Epoch 550, training loss: 0.02058851160109043 = 0.013455655425786972 + 0.001 * 7.132855415344238
Epoch 550, val loss: 1.0659996271133423
Epoch 560, training loss: 0.019747428596019745 = 0.01261220034211874 + 0.001 * 7.135227680206299
Epoch 560, val loss: 1.0762168169021606
Epoch 570, training loss: 0.018976788967847824 = 0.011845367960631847 + 0.001 * 7.131419658660889
Epoch 570, val loss: 1.0861862897872925
Epoch 580, training loss: 0.018276404589414597 = 0.011144011281430721 + 0.001 * 7.132392406463623
Epoch 580, val loss: 1.0959432125091553
Epoch 590, training loss: 0.017644010484218597 = 0.010499190539121628 + 0.001 * 7.144819736480713
Epoch 590, val loss: 1.10551917552948
Epoch 600, training loss: 0.017030056565999985 = 0.009904669597744942 + 0.001 * 7.125387668609619
Epoch 600, val loss: 1.1149171590805054
Epoch 610, training loss: 0.016489163041114807 = 0.009355517104268074 + 0.001 * 7.1336445808410645
Epoch 610, val loss: 1.1241434812545776
Epoch 620, training loss: 0.01596410945057869 = 0.008848017081618309 + 0.001 * 7.116091251373291
Epoch 620, val loss: 1.1332138776779175
Epoch 630, training loss: 0.015502354130148888 = 0.008378872647881508 + 0.001 * 7.123480796813965
Epoch 630, val loss: 1.1420941352844238
Epoch 640, training loss: 0.01506124995648861 = 0.007944935001432896 + 0.001 * 7.116313934326172
Epoch 640, val loss: 1.1507850885391235
Epoch 650, training loss: 0.014657674357295036 = 0.007543249987065792 + 0.001 * 7.114424228668213
Epoch 650, val loss: 1.1593050956726074
Epoch 660, training loss: 0.014311568811535835 = 0.007171114906668663 + 0.001 * 7.140453815460205
Epoch 660, val loss: 1.1676414012908936
Epoch 670, training loss: 0.013933797366917133 = 0.006825962569564581 + 0.001 * 7.107834339141846
Epoch 670, val loss: 1.1758060455322266
Epoch 680, training loss: 0.013624252751469612 = 0.006505471654236317 + 0.001 * 7.118781089782715
Epoch 680, val loss: 1.1837894916534424
Epoch 690, training loss: 0.013306116685271263 = 0.006207612808793783 + 0.001 * 7.098504066467285
Epoch 690, val loss: 1.1916050910949707
Epoch 700, training loss: 0.013040637597441673 = 0.005930429324507713 + 0.001 * 7.110208511352539
Epoch 700, val loss: 1.1992435455322266
Epoch 710, training loss: 0.012782225385308266 = 0.005672105122357607 + 0.001 * 7.110119342803955
Epoch 710, val loss: 1.2067131996154785
Epoch 720, training loss: 0.012535292655229568 = 0.005431067198514938 + 0.001 * 7.104225158691406
Epoch 720, val loss: 1.214015245437622
Epoch 730, training loss: 0.012307276949286461 = 0.00520589342340827 + 0.001 * 7.101383686065674
Epoch 730, val loss: 1.2211508750915527
Epoch 740, training loss: 0.01209702156484127 = 0.004995337221771479 + 0.001 * 7.101683616638184
Epoch 740, val loss: 1.2281336784362793
Epoch 750, training loss: 0.011910723522305489 = 0.004798166453838348 + 0.001 * 7.112556457519531
Epoch 750, val loss: 1.2349565029144287
Epoch 760, training loss: 0.011707669124007225 = 0.004613302182406187 + 0.001 * 7.094367027282715
Epoch 760, val loss: 1.2416398525238037
Epoch 770, training loss: 0.011523889377713203 = 0.004439788870513439 + 0.001 * 7.084099769592285
Epoch 770, val loss: 1.2481609582901
Epoch 780, training loss: 0.011361287906765938 = 0.0042766910046339035 + 0.001 * 7.084596633911133
Epoch 780, val loss: 1.2545474767684937
Epoch 790, training loss: 0.011224270798265934 = 0.004123263992369175 + 0.001 * 7.101006507873535
Epoch 790, val loss: 1.2607855796813965
Epoch 800, training loss: 0.011066198348999023 = 0.003978734835982323 + 0.001 * 7.087462902069092
Epoch 800, val loss: 1.2668954133987427
Epoch 810, training loss: 0.01092392299324274 = 0.0038424551021307707 + 0.001 * 7.081467628479004
Epoch 810, val loss: 1.27286696434021
Epoch 820, training loss: 0.010814206674695015 = 0.00371388322673738 + 0.001 * 7.10032320022583
Epoch 820, val loss: 1.2787061929702759
Epoch 830, training loss: 0.01068160030990839 = 0.0035924045369029045 + 0.001 * 7.089195251464844
Epoch 830, val loss: 1.2844133377075195
Epoch 840, training loss: 0.010566862300038338 = 0.0034774632658809423 + 0.001 * 7.089398384094238
Epoch 840, val loss: 1.2900060415267944
Epoch 850, training loss: 0.010458170436322689 = 0.0033686186652630568 + 0.001 * 7.0895514488220215
Epoch 850, val loss: 1.295467495918274
Epoch 860, training loss: 0.010336175560951233 = 0.003265470266342163 + 0.001 * 7.070704936981201
Epoch 860, val loss: 1.3008185625076294
Epoch 870, training loss: 0.010266668163239956 = 0.0031675121281296015 + 0.001 * 7.099155426025391
Epoch 870, val loss: 1.3060640096664429
Epoch 880, training loss: 0.010138208977878094 = 0.0030744688119739294 + 0.001 * 7.063739776611328
Epoch 880, val loss: 1.311199426651001
Epoch 890, training loss: 0.010126376524567604 = 0.002985864644870162 + 0.001 * 7.140511512756348
Epoch 890, val loss: 1.3162351846694946
Epoch 900, training loss: 0.009976924397051334 = 0.0029016563203185797 + 0.001 * 7.075267314910889
Epoch 900, val loss: 1.3211634159088135
Epoch 910, training loss: 0.009917203336954117 = 0.0028212852776050568 + 0.001 * 7.095917701721191
Epoch 910, val loss: 1.325993299484253
Epoch 920, training loss: 0.009803259745240211 = 0.002744410652667284 + 0.001 * 7.058848857879639
Epoch 920, val loss: 1.3307349681854248
Epoch 930, training loss: 0.009769200347363949 = 0.0026709747035056353 + 0.001 * 7.098225116729736
Epoch 930, val loss: 1.3354055881500244
Epoch 940, training loss: 0.00965804886072874 = 0.0026008363347500563 + 0.001 * 7.057211875915527
Epoch 940, val loss: 1.3399758338928223
Epoch 950, training loss: 0.009609933942556381 = 0.0025336421094834805 + 0.001 * 7.076291561126709
Epoch 950, val loss: 1.3444633483886719
Epoch 960, training loss: 0.009508209303021431 = 0.002469391794875264 + 0.001 * 7.038816928863525
Epoch 960, val loss: 1.3488680124282837
Epoch 970, training loss: 0.009479746222496033 = 0.0024078066926449537 + 0.001 * 7.071938991546631
Epoch 970, val loss: 1.3531885147094727
Epoch 980, training loss: 0.009393898770213127 = 0.0023488467559218407 + 0.001 * 7.045051574707031
Epoch 980, val loss: 1.3574304580688477
Epoch 990, training loss: 0.009334007278084755 = 0.002292327582836151 + 0.001 * 7.0416789054870605
Epoch 990, val loss: 1.3615856170654297
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8118081180811808
The final CL Acc:0.78642, 0.01222, The final GNN Acc:0.80917, 0.00215
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13118])
remove edge: torch.Size([2, 8004])
updated graph: torch.Size([2, 10566])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9427058696746826 = 1.934109091758728 + 0.001 * 8.596818923950195
Epoch 0, val loss: 1.9320796728134155
Epoch 10, training loss: 1.9326049089431763 = 1.9240081310272217 + 0.001 * 8.59672737121582
Epoch 10, val loss: 1.9213900566101074
Epoch 20, training loss: 1.9198839664459229 = 1.911287546157837 + 0.001 * 8.596468925476074
Epoch 20, val loss: 1.9078859090805054
Epoch 30, training loss: 1.9018915891647339 = 1.8932956457138062 + 0.001 * 8.59591293334961
Epoch 30, val loss: 1.888890266418457
Epoch 40, training loss: 1.8758602142333984 = 1.8672655820846558 + 0.001 * 8.594579696655273
Epoch 40, val loss: 1.8618894815444946
Epoch 50, training loss: 1.8412226438522339 = 1.8326319456100464 + 0.001 * 8.590666770935059
Epoch 50, val loss: 1.8278526067733765
Epoch 60, training loss: 1.805139183998108 = 1.7965631484985352 + 0.001 * 8.575995445251465
Epoch 60, val loss: 1.7961424589157104
Epoch 70, training loss: 1.7706573009490967 = 1.7621546983718872 + 0.001 * 8.502552032470703
Epoch 70, val loss: 1.7667816877365112
Epoch 80, training loss: 1.723422884941101 = 1.7153003215789795 + 0.001 * 8.122536659240723
Epoch 80, val loss: 1.7240285873413086
Epoch 90, training loss: 1.6583786010742188 = 1.6504294872283936 + 0.001 * 7.949082374572754
Epoch 90, val loss: 1.665021538734436
Epoch 100, training loss: 1.5766830444335938 = 1.5688916444778442 + 0.001 * 7.791409969329834
Epoch 100, val loss: 1.5931507349014282
Epoch 110, training loss: 1.4890069961547852 = 1.4813799858093262 + 0.001 * 7.62697172164917
Epoch 110, val loss: 1.517677903175354
Epoch 120, training loss: 1.4028704166412354 = 1.3952946662902832 + 0.001 * 7.575743198394775
Epoch 120, val loss: 1.4474458694458008
Epoch 130, training loss: 1.316658854484558 = 1.3091050386428833 + 0.001 * 7.553779125213623
Epoch 130, val loss: 1.3800501823425293
Epoch 140, training loss: 1.2274972200393677 = 1.2199820280075073 + 0.001 * 7.515176296234131
Epoch 140, val loss: 1.3121740818023682
Epoch 150, training loss: 1.1358693838119507 = 1.1283962726593018 + 0.001 * 7.473104000091553
Epoch 150, val loss: 1.243565559387207
Epoch 160, training loss: 1.043952226638794 = 1.0365184545516968 + 0.001 * 7.43372917175293
Epoch 160, val loss: 1.1747342348098755
Epoch 170, training loss: 0.9542286992073059 = 0.9468267560005188 + 0.001 * 7.401915073394775
Epoch 170, val loss: 1.1066999435424805
Epoch 180, training loss: 0.8686667680740356 = 0.8612921237945557 + 0.001 * 7.374624252319336
Epoch 180, val loss: 1.0409501791000366
Epoch 190, training loss: 0.788603663444519 = 0.7812511920928955 + 0.001 * 7.3524956703186035
Epoch 190, val loss: 0.9797832369804382
Epoch 200, training loss: 0.7144774198532104 = 0.7071367502212524 + 0.001 * 7.340671539306641
Epoch 200, val loss: 0.9255710244178772
Epoch 210, training loss: 0.646461546421051 = 0.6391269564628601 + 0.001 * 7.334578514099121
Epoch 210, val loss: 0.8799389600753784
Epoch 220, training loss: 0.5846237540245056 = 0.5772939324378967 + 0.001 * 7.329813480377197
Epoch 220, val loss: 0.8438771963119507
Epoch 230, training loss: 0.5288125276565552 = 0.5214875936508179 + 0.001 * 7.324934005737305
Epoch 230, val loss: 0.8168656229972839
Epoch 240, training loss: 0.47838500142097473 = 0.4710661768913269 + 0.001 * 7.318835735321045
Epoch 240, val loss: 0.7974157929420471
Epoch 250, training loss: 0.4324198067188263 = 0.4251096844673157 + 0.001 * 7.310113430023193
Epoch 250, val loss: 0.7836012840270996
Epoch 260, training loss: 0.39020994305610657 = 0.3829133212566376 + 0.001 * 7.29661226272583
Epoch 260, val loss: 0.7742250561714172
Epoch 270, training loss: 0.35144490003585815 = 0.344169944524765 + 0.001 * 7.274966239929199
Epoch 270, val loss: 0.7690866589546204
Epoch 280, training loss: 0.31594523787498474 = 0.30870047211647034 + 0.001 * 7.244753360748291
Epoch 280, val loss: 0.7682112455368042
Epoch 290, training loss: 0.28353187441825867 = 0.2763102054595947 + 0.001 * 7.221659183502197
Epoch 290, val loss: 0.7710294127464294
Epoch 300, training loss: 0.2539651691913605 = 0.24675853550434113 + 0.001 * 7.206630229949951
Epoch 300, val loss: 0.7767106890678406
Epoch 310, training loss: 0.22700181603431702 = 0.21980883181095123 + 0.001 * 7.192983627319336
Epoch 310, val loss: 0.7847388982772827
Epoch 320, training loss: 0.20243282616138458 = 0.19523966312408447 + 0.001 * 7.193163871765137
Epoch 320, val loss: 0.7947563529014587
Epoch 330, training loss: 0.18011893332004547 = 0.1729271560907364 + 0.001 * 7.191773891448975
Epoch 330, val loss: 0.8065290451049805
Epoch 340, training loss: 0.15997569262981415 = 0.1527840942144394 + 0.001 * 7.191597938537598
Epoch 340, val loss: 0.8196372985839844
Epoch 350, training loss: 0.1419380009174347 = 0.1347474604845047 + 0.001 * 7.190537929534912
Epoch 350, val loss: 0.8337459564208984
Epoch 360, training loss: 0.1259361058473587 = 0.11874566227197647 + 0.001 * 7.190436840057373
Epoch 360, val loss: 0.848574161529541
Epoch 370, training loss: 0.11186543852090836 = 0.10467458516359329 + 0.001 * 7.190855026245117
Epoch 370, val loss: 0.8638507127761841
Epoch 380, training loss: 0.09957170486450195 = 0.09238055348396301 + 0.001 * 7.191150665283203
Epoch 380, val loss: 0.8794113397598267
Epoch 390, training loss: 0.08888930827379227 = 0.08169404417276382 + 0.001 * 7.195262432098389
Epoch 390, val loss: 0.8952122926712036
Epoch 400, training loss: 0.0796242505311966 = 0.0724310576915741 + 0.001 * 7.1931962966918945
Epoch 400, val loss: 0.911240816116333
Epoch 410, training loss: 0.07160743325948715 = 0.06441432982683182 + 0.001 * 7.1931023597717285
Epoch 410, val loss: 0.9273938536643982
Epoch 420, training loss: 0.0646711066365242 = 0.057479869574308395 + 0.001 * 7.191234588623047
Epoch 420, val loss: 0.943671464920044
Epoch 430, training loss: 0.0586693175137043 = 0.05147620290517807 + 0.001 * 7.193113803863525
Epoch 430, val loss: 0.9599360823631287
Epoch 440, training loss: 0.05346248671412468 = 0.04627079889178276 + 0.001 * 7.1916890144348145
Epoch 440, val loss: 0.9761281609535217
Epoch 450, training loss: 0.048935770988464355 = 0.04174576699733734 + 0.001 * 7.190001964569092
Epoch 450, val loss: 0.9921668171882629
Epoch 460, training loss: 0.044997263699769974 = 0.03780045360326767 + 0.001 * 7.196811199188232
Epoch 460, val loss: 1.0080076456069946
Epoch 470, training loss: 0.04154469817876816 = 0.03435005992650986 + 0.001 * 7.194637775421143
Epoch 470, val loss: 1.023533582687378
Epoch 480, training loss: 0.03851000964641571 = 0.03132304921746254 + 0.001 * 7.186961650848389
Epoch 480, val loss: 1.0387742519378662
Epoch 490, training loss: 0.035844478756189346 = 0.02865809015929699 + 0.001 * 7.186387538909912
Epoch 490, val loss: 1.0536319017410278
Epoch 500, training loss: 0.033488936722278595 = 0.026303786784410477 + 0.001 * 7.185150146484375
Epoch 500, val loss: 1.0681097507476807
Epoch 510, training loss: 0.0313996747136116 = 0.024216892197728157 + 0.001 * 7.1827802658081055
Epoch 510, val loss: 1.0821765661239624
Epoch 520, training loss: 0.029539279639720917 = 0.022359803318977356 + 0.001 * 7.179476737976074
Epoch 520, val loss: 1.0958741903305054
Epoch 530, training loss: 0.027886180207133293 = 0.020699333399534225 + 0.001 * 7.186845779418945
Epoch 530, val loss: 1.1091716289520264
Epoch 540, training loss: 0.026382723823189735 = 0.01920611597597599 + 0.001 * 7.176607608795166
Epoch 540, val loss: 1.1221568584442139
Epoch 550, training loss: 0.025031648576259613 = 0.01785712130367756 + 0.001 * 7.174527645111084
Epoch 550, val loss: 1.1348539590835571
Epoch 560, training loss: 0.023807678371667862 = 0.016635019332170486 + 0.001 * 7.172659397125244
Epoch 560, val loss: 1.1473475694656372
Epoch 570, training loss: 0.022694338113069534 = 0.015525884926319122 + 0.001 * 7.168453216552734
Epoch 570, val loss: 1.159555196762085
Epoch 580, training loss: 0.02169022150337696 = 0.014517698436975479 + 0.001 * 7.17252254486084
Epoch 580, val loss: 1.171504259109497
Epoch 590, training loss: 0.020755812525749207 = 0.013599843718111515 + 0.001 * 7.15596866607666
Epoch 590, val loss: 1.1831802129745483
Epoch 600, training loss: 0.019919222220778465 = 0.012762567959725857 + 0.001 * 7.156654357910156
Epoch 600, val loss: 1.1945687532424927
Epoch 610, training loss: 0.01917867735028267 = 0.011996309272944927 + 0.001 * 7.182368278503418
Epoch 610, val loss: 1.2056870460510254
Epoch 620, training loss: 0.018433894962072372 = 0.011291581206023693 + 0.001 * 7.142313480377197
Epoch 620, val loss: 1.2166296243667603
Epoch 630, training loss: 0.01779172755777836 = 0.010640661232173443 + 0.001 * 7.151065349578857
Epoch 630, val loss: 1.2273515462875366
Epoch 640, training loss: 0.01717435196042061 = 0.010038378648459911 + 0.001 * 7.13597297668457
Epoch 640, val loss: 1.237925410270691
Epoch 650, training loss: 0.016651656478643417 = 0.009481167420744896 + 0.001 * 7.1704888343811035
Epoch 650, val loss: 1.2482459545135498
Epoch 660, training loss: 0.016103683039546013 = 0.00896596908569336 + 0.001 * 7.13771390914917
Epoch 660, val loss: 1.258384346961975
Epoch 670, training loss: 0.015630802139639854 = 0.008489107713103294 + 0.001 * 7.141693592071533
Epoch 670, val loss: 1.2682825326919556
Epoch 680, training loss: 0.01517869159579277 = 0.008047680370509624 + 0.001 * 7.13101053237915
Epoch 680, val loss: 1.2779524326324463
Epoch 690, training loss: 0.014757612720131874 = 0.00763894896954298 + 0.001 * 7.1186628341674805
Epoch 690, val loss: 1.2873780727386475
Epoch 700, training loss: 0.014374096877872944 = 0.007260313723236322 + 0.001 * 7.11378288269043
Epoch 700, val loss: 1.2965823411941528
Epoch 710, training loss: 0.014022971503436565 = 0.0069091422483325005 + 0.001 * 7.113829135894775
Epoch 710, val loss: 1.3055393695831299
Epoch 720, training loss: 0.01371181569993496 = 0.006583264097571373 + 0.001 * 7.128551959991455
Epoch 720, val loss: 1.3142257928848267
Epoch 730, training loss: 0.013412242755293846 = 0.006280539091676474 + 0.001 * 7.131702899932861
Epoch 730, val loss: 1.322719693183899
Epoch 740, training loss: 0.013128994964063168 = 0.005998845677822828 + 0.001 * 7.130148887634277
Epoch 740, val loss: 1.3309719562530518
Epoch 750, training loss: 0.012888956815004349 = 0.005736475810408592 + 0.001 * 7.152480602264404
Epoch 750, val loss: 1.3390616178512573
Epoch 760, training loss: 0.012580364011228085 = 0.005491713993251324 + 0.001 * 7.088649749755859
Epoch 760, val loss: 1.3468867540359497
Epoch 770, training loss: 0.012372760102152824 = 0.005263104103505611 + 0.001 * 7.109655380249023
Epoch 770, val loss: 1.3545557260513306
Epoch 780, training loss: 0.012149451300501823 = 0.005049305036664009 + 0.001 * 7.10014533996582
Epoch 780, val loss: 1.362002968788147
Epoch 790, training loss: 0.011990715749561787 = 0.0048490590415894985 + 0.001 * 7.141656398773193
Epoch 790, val loss: 1.3692727088928223
Epoch 800, training loss: 0.011773036792874336 = 0.004661259241402149 + 0.001 * 7.111776828765869
Epoch 800, val loss: 1.3763463497161865
Epoch 810, training loss: 0.011572555638849735 = 0.004484833683818579 + 0.001 * 7.087721824645996
Epoch 810, val loss: 1.3832341432571411
Epoch 820, training loss: 0.011391935870051384 = 0.00431892741471529 + 0.001 * 7.073007583618164
Epoch 820, val loss: 1.3899691104888916
Epoch 830, training loss: 0.011242805048823357 = 0.004162708297371864 + 0.001 * 7.08009672164917
Epoch 830, val loss: 1.396528959274292
Epoch 840, training loss: 0.01108500361442566 = 0.004015265963971615 + 0.001 * 7.069736957550049
Epoch 840, val loss: 1.402890920639038
Epoch 850, training loss: 0.010947016067802906 = 0.003875797614455223 + 0.001 * 7.071218013763428
Epoch 850, val loss: 1.4091529846191406
Epoch 860, training loss: 0.010811712592840195 = 0.0037436920683830976 + 0.001 * 7.068020343780518
Epoch 860, val loss: 1.4152499437332153
Epoch 870, training loss: 0.01069609634578228 = 0.00361847341991961 + 0.001 * 7.077622890472412
Epoch 870, val loss: 1.4211716651916504
Epoch 880, training loss: 0.010563278570771217 = 0.0034994734451174736 + 0.001 * 7.063805103302002
Epoch 880, val loss: 1.4269862174987793
Epoch 890, training loss: 0.010459083132445812 = 0.0033862392883747816 + 0.001 * 7.072843074798584
Epoch 890, val loss: 1.4326545000076294
Epoch 900, training loss: 0.010366301983594894 = 0.0032785285729914904 + 0.001 * 7.087772846221924
Epoch 900, val loss: 1.4381505250930786
Epoch 910, training loss: 0.010244716890156269 = 0.0031759405974298716 + 0.001 * 7.0687761306762695
Epoch 910, val loss: 1.443590521812439
Epoch 920, training loss: 0.010144989006221294 = 0.0030782802496105433 + 0.001 * 7.066708087921143
Epoch 920, val loss: 1.4488623142242432
Epoch 930, training loss: 0.010027498006820679 = 0.002985358703881502 + 0.001 * 7.042138576507568
Epoch 930, val loss: 1.4540163278579712
Epoch 940, training loss: 0.009949961677193642 = 0.0028967426624149084 + 0.001 * 7.053218841552734
Epoch 940, val loss: 1.4590461254119873
Epoch 950, training loss: 0.009865541942417622 = 0.002812148304656148 + 0.001 * 7.0533928871154785
Epoch 950, val loss: 1.46397864818573
Epoch 960, training loss: 0.009786035865545273 = 0.0027314515318721533 + 0.001 * 7.054583549499512
Epoch 960, val loss: 1.468809962272644
Epoch 970, training loss: 0.009718246757984161 = 0.0026544765569269657 + 0.001 * 7.063769340515137
Epoch 970, val loss: 1.4734723567962646
Epoch 980, training loss: 0.00963149219751358 = 0.002580967266112566 + 0.001 * 7.0505242347717285
Epoch 980, val loss: 1.4780584573745728
Epoch 990, training loss: 0.009570942260324955 = 0.0025107774417847395 + 0.001 * 7.060164451599121
Epoch 990, val loss: 1.4825809001922607
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8334211913547708
=== training gcn model ===
Epoch 0, training loss: 1.9610775709152222 = 1.9524807929992676 + 0.001 * 8.596833229064941
Epoch 0, val loss: 1.957234501838684
Epoch 10, training loss: 1.9507412910461426 = 1.942144513130188 + 0.001 * 8.596776008605957
Epoch 10, val loss: 1.9463894367218018
Epoch 20, training loss: 1.9380295276641846 = 1.929432988166809 + 0.001 * 8.596597671508789
Epoch 20, val loss: 1.9329689741134644
Epoch 30, training loss: 1.9201606512069702 = 1.9115644693374634 + 0.001 * 8.596169471740723
Epoch 30, val loss: 1.9141780138015747
Epoch 40, training loss: 1.8939340114593506 = 1.8853389024734497 + 0.001 * 8.595165252685547
Epoch 40, val loss: 1.8869656324386597
Epoch 50, training loss: 1.85720956325531 = 1.8486171960830688 + 0.001 * 8.59237289428711
Epoch 50, val loss: 1.8503446578979492
Epoch 60, training loss: 1.8146950006484985 = 1.8061124086380005 + 0.001 * 8.582635879516602
Epoch 60, val loss: 1.811612844467163
Epoch 70, training loss: 1.7778868675231934 = 1.769345760345459 + 0.001 * 8.541051864624023
Epoch 70, val loss: 1.781509280204773
Epoch 80, training loss: 1.7353731393814087 = 1.727070689201355 + 0.001 * 8.30242919921875
Epoch 80, val loss: 1.7440478801727295
Epoch 90, training loss: 1.6748456954956055 = 1.6667553186416626 + 0.001 * 8.090420722961426
Epoch 90, val loss: 1.690385103225708
Epoch 100, training loss: 1.592702865600586 = 1.5846929550170898 + 0.001 * 8.009892463684082
Epoch 100, val loss: 1.6199989318847656
Epoch 110, training loss: 1.4918203353881836 = 1.4839415550231934 + 0.001 * 7.878782749176025
Epoch 110, val loss: 1.5361199378967285
Epoch 120, training loss: 1.3842593431472778 = 1.376550316810608 + 0.001 * 7.7090229988098145
Epoch 120, val loss: 1.4484285116195679
Epoch 130, training loss: 1.2783125638961792 = 1.2706540822982788 + 0.001 * 7.658442497253418
Epoch 130, val loss: 1.3633836507797241
Epoch 140, training loss: 1.1750367879867554 = 1.16741144657135 + 0.001 * 7.62534761428833
Epoch 140, val loss: 1.2809042930603027
Epoch 150, training loss: 1.0748980045318604 = 1.067337155342102 + 0.001 * 7.560849666595459
Epoch 150, val loss: 1.2019178867340088
Epoch 160, training loss: 0.9802769422531128 = 0.9728023409843445 + 0.001 * 7.474583148956299
Epoch 160, val loss: 1.1282814741134644
Epoch 170, training loss: 0.892629861831665 = 0.8852002620697021 + 0.001 * 7.429576396942139
Epoch 170, val loss: 1.0612213611602783
Epoch 180, training loss: 0.8122591376304626 = 0.8048428297042847 + 0.001 * 7.416332244873047
Epoch 180, val loss: 1.000761866569519
Epoch 190, training loss: 0.7396602630615234 = 0.7322549819946289 + 0.001 * 7.405275344848633
Epoch 190, val loss: 0.9475529193878174
Epoch 200, training loss: 0.675171971321106 = 0.6677765250205994 + 0.001 * 7.395458698272705
Epoch 200, val loss: 0.9023311138153076
Epoch 210, training loss: 0.618069589138031 = 0.6106846332550049 + 0.001 * 7.38496732711792
Epoch 210, val loss: 0.8653062582015991
Epoch 220, training loss: 0.5667489171028137 = 0.5593754649162292 + 0.001 * 7.373466491699219
Epoch 220, val loss: 0.8356540203094482
Epoch 230, training loss: 0.5196524858474731 = 0.5122924447059631 + 0.001 * 7.36007022857666
Epoch 230, val loss: 0.8122522234916687
Epoch 240, training loss: 0.47554609179496765 = 0.4681991934776306 + 0.001 * 7.346901893615723
Epoch 240, val loss: 0.7938174605369568
Epoch 250, training loss: 0.433464914560318 = 0.42613857984542847 + 0.001 * 7.326335430145264
Epoch 250, val loss: 0.7788566946983337
Epoch 260, training loss: 0.392863005399704 = 0.38555410504341125 + 0.001 * 7.308913707733154
Epoch 260, val loss: 0.7664153575897217
Epoch 270, training loss: 0.35361751914024353 = 0.34633341431617737 + 0.001 * 7.284091472625732
Epoch 270, val loss: 0.7558774352073669
Epoch 280, training loss: 0.3159279227256775 = 0.30866897106170654 + 0.001 * 7.25894021987915
Epoch 280, val loss: 0.7471314072608948
Epoch 290, training loss: 0.280073344707489 = 0.2728259861469269 + 0.001 * 7.2473649978637695
Epoch 290, val loss: 0.740075409412384
Epoch 300, training loss: 0.24631460011005402 = 0.23907871544361115 + 0.001 * 7.235879421234131
Epoch 300, val loss: 0.7348183393478394
Epoch 310, training loss: 0.21495839953422546 = 0.20772625505924225 + 0.001 * 7.23214054107666
Epoch 310, val loss: 0.7316737174987793
Epoch 320, training loss: 0.18642693758010864 = 0.17920123040676117 + 0.001 * 7.225701332092285
Epoch 320, val loss: 0.7310747504234314
Epoch 330, training loss: 0.1611490100622177 = 0.15392571687698364 + 0.001 * 7.223295211791992
Epoch 330, val loss: 0.7334000468254089
Epoch 340, training loss: 0.13931488990783691 = 0.13209953904151917 + 0.001 * 7.215354919433594
Epoch 340, val loss: 0.7387431263923645
Epoch 350, training loss: 0.12080903351306915 = 0.11359260976314545 + 0.001 * 7.216425895690918
Epoch 350, val loss: 0.7468158006668091
Epoch 360, training loss: 0.10524903982877731 = 0.09803759306669235 + 0.001 * 7.21144962310791
Epoch 360, val loss: 0.7573040723800659
Epoch 370, training loss: 0.09221236407756805 = 0.08498915284872055 + 0.001 * 7.223214149475098
Epoch 370, val loss: 0.769632875919342
Epoch 380, training loss: 0.08124509453773499 = 0.07402864098548889 + 0.001 * 7.216454982757568
Epoch 380, val loss: 0.783318042755127
Epoch 390, training loss: 0.07200503349304199 = 0.06479967385530472 + 0.001 * 7.205358028411865
Epoch 390, val loss: 0.7979524731636047
Epoch 400, training loss: 0.0642179548740387 = 0.05701064318418503 + 0.001 * 7.20731258392334
Epoch 400, val loss: 0.8130640983581543
Epoch 410, training loss: 0.057622030377388 = 0.05042140558362007 + 0.001 * 7.200623512268066
Epoch 410, val loss: 0.828385591506958
Epoch 420, training loss: 0.05202457681298256 = 0.04483196884393692 + 0.001 * 7.192606449127197
Epoch 420, val loss: 0.8437649011611938
Epoch 430, training loss: 0.047264911234378815 = 0.040072787553071976 + 0.001 * 7.192124366760254
Epoch 430, val loss: 0.8590003848075867
Epoch 440, training loss: 0.043190982192754745 = 0.036002907902002335 + 0.001 * 7.188075542449951
Epoch 440, val loss: 0.8739223480224609
Epoch 450, training loss: 0.039695385843515396 = 0.03250567987561226 + 0.001 * 7.189704895019531
Epoch 450, val loss: 0.8884764313697815
Epoch 460, training loss: 0.036668237298727036 = 0.029486065730452538 + 0.001 * 7.182172775268555
Epoch 460, val loss: 0.9026440382003784
Epoch 470, training loss: 0.034029770642519 = 0.026865478605031967 + 0.001 * 7.164290904998779
Epoch 470, val loss: 0.916385293006897
Epoch 480, training loss: 0.03176673501729965 = 0.02457945980131626 + 0.001 * 7.187276363372803
Epoch 480, val loss: 0.9297247529029846
Epoch 490, training loss: 0.02974402904510498 = 0.022575292736291885 + 0.001 * 7.168735980987549
Epoch 490, val loss: 0.9426398873329163
Epoch 500, training loss: 0.027970220893621445 = 0.020809827372431755 + 0.001 * 7.160394191741943
Epoch 500, val loss: 0.955146074295044
Epoch 510, training loss: 0.026400895789265633 = 0.01924744062125683 + 0.001 * 7.153454303741455
Epoch 510, val loss: 0.9672344326972961
Epoch 520, training loss: 0.025023076683282852 = 0.01785866916179657 + 0.001 * 7.1644062995910645
Epoch 520, val loss: 0.9789543151855469
Epoch 530, training loss: 0.023763442412018776 = 0.016619272530078888 + 0.001 * 7.144169807434082
Epoch 530, val loss: 0.9902939200401306
Epoch 540, training loss: 0.022652296349406242 = 0.015508720651268959 + 0.001 * 7.1435747146606445
Epoch 540, val loss: 1.0012809038162231
Epoch 550, training loss: 0.02166958525776863 = 0.014509985223412514 + 0.001 * 7.159600257873535
Epoch 550, val loss: 1.0119105577468872
Epoch 560, training loss: 0.020753154531121254 = 0.013608576729893684 + 0.001 * 7.144577503204346
Epoch 560, val loss: 1.022201657295227
Epoch 570, training loss: 0.019929444417357445 = 0.012792335823178291 + 0.001 * 7.137107849121094
Epoch 570, val loss: 1.0321837663650513
Epoch 580, training loss: 0.01918623596429825 = 0.012050929479300976 + 0.001 * 7.135306358337402
Epoch 580, val loss: 1.0418580770492554
Epoch 590, training loss: 0.018535727635025978 = 0.01137544121593237 + 0.001 * 7.160285949707031
Epoch 590, val loss: 1.0512421131134033
Epoch 600, training loss: 0.01790100708603859 = 0.0107583524659276 + 0.001 * 7.1426544189453125
Epoch 600, val loss: 1.0603388547897339
Epoch 610, training loss: 0.017329825088381767 = 0.01019294559955597 + 0.001 * 7.1368794441223145
Epoch 610, val loss: 1.0691661834716797
Epoch 620, training loss: 0.01680225506424904 = 0.009672963060438633 + 0.001 * 7.1292924880981445
Epoch 620, val loss: 1.0777685642242432
Epoch 630, training loss: 0.016321957111358643 = 0.009192424826323986 + 0.001 * 7.129532814025879
Epoch 630, val loss: 1.086203694343567
Epoch 640, training loss: 0.015895431861281395 = 0.008745665661990643 + 0.001 * 7.149765968322754
Epoch 640, val loss: 1.0945061445236206
Epoch 650, training loss: 0.015457786619663239 = 0.008328805677592754 + 0.001 * 7.128981113433838
Epoch 650, val loss: 1.102728009223938
Epoch 660, training loss: 0.015066818334162235 = 0.007938513532280922 + 0.001 * 7.128304481506348
Epoch 660, val loss: 1.1108827590942383
Epoch 670, training loss: 0.014698533341288567 = 0.007572693284600973 + 0.001 * 7.125840187072754
Epoch 670, val loss: 1.1189738512039185
Epoch 680, training loss: 0.014364175498485565 = 0.007229592185467482 + 0.001 * 7.13458251953125
Epoch 680, val loss: 1.1269954442977905
Epoch 690, training loss: 0.014039695262908936 = 0.006908176001161337 + 0.001 * 7.131518363952637
Epoch 690, val loss: 1.134943962097168
Epoch 700, training loss: 0.013740623369812965 = 0.006606917828321457 + 0.001 * 7.133705139160156
Epoch 700, val loss: 1.1427693367004395
Epoch 710, training loss: 0.013442862778902054 = 0.006324595771729946 + 0.001 * 7.1182661056518555
Epoch 710, val loss: 1.1505038738250732
Epoch 720, training loss: 0.01317555084824562 = 0.006059881765395403 + 0.001 * 7.115669250488281
Epoch 720, val loss: 1.1581002473831177
Epoch 730, training loss: 0.012928183190524578 = 0.005811621434986591 + 0.001 * 7.116561412811279
Epoch 730, val loss: 1.1655665636062622
Epoch 740, training loss: 0.012688130140304565 = 0.005578573793172836 + 0.001 * 7.109556674957275
Epoch 740, val loss: 1.172921895980835
Epoch 750, training loss: 0.01247473992407322 = 0.00535964872688055 + 0.001 * 7.115090847015381
Epoch 750, val loss: 1.1801502704620361
Epoch 760, training loss: 0.012271055951714516 = 0.005153922364115715 + 0.001 * 7.117132663726807
Epoch 760, val loss: 1.1872227191925049
Epoch 770, training loss: 0.012071168050169945 = 0.004960484802722931 + 0.001 * 7.110682964324951
Epoch 770, val loss: 1.1941518783569336
Epoch 780, training loss: 0.011902671307325363 = 0.004778391681611538 + 0.001 * 7.124279499053955
Epoch 780, val loss: 1.2009609937667847
Epoch 790, training loss: 0.011724278330802917 = 0.004606822971254587 + 0.001 * 7.11745548248291
Epoch 790, val loss: 1.2076075077056885
Epoch 800, training loss: 0.011551717296242714 = 0.004445088095963001 + 0.001 * 7.106628894805908
Epoch 800, val loss: 1.2141438722610474
Epoch 810, training loss: 0.011395696550607681 = 0.0042924038134515285 + 0.001 * 7.103291988372803
Epoch 810, val loss: 1.2205276489257812
Epoch 820, training loss: 0.011257410980761051 = 0.004148158244788647 + 0.001 * 7.109252452850342
Epoch 820, val loss: 1.2267827987670898
Epoch 830, training loss: 0.01111496239900589 = 0.0040118093602359295 + 0.001 * 7.103153228759766
Epoch 830, val loss: 1.2329108715057373
Epoch 840, training loss: 0.011002855375409126 = 0.003882763674482703 + 0.001 * 7.120090961456299
Epoch 840, val loss: 1.2388900518417358
Epoch 850, training loss: 0.01086905226111412 = 0.0037605357356369495 + 0.001 * 7.108515739440918
Epoch 850, val loss: 1.2447540760040283
Epoch 860, training loss: 0.010740291327238083 = 0.0036446042358875275 + 0.001 * 7.095686435699463
Epoch 860, val loss: 1.250488519668579
Epoch 870, training loss: 0.01062488742172718 = 0.0035345738288015127 + 0.001 * 7.090312957763672
Epoch 870, val loss: 1.2561137676239014
Epoch 880, training loss: 0.010533316060900688 = 0.003430078737437725 + 0.001 * 7.103237628936768
Epoch 880, val loss: 1.261641025543213
Epoch 890, training loss: 0.010433798655867577 = 0.0033307590056210756 + 0.001 * 7.103039264678955
Epoch 890, val loss: 1.2670289278030396
Epoch 900, training loss: 0.010319169610738754 = 0.0032362821511924267 + 0.001 * 7.082886695861816
Epoch 900, val loss: 1.272305965423584
Epoch 910, training loss: 0.010237791575491428 = 0.003146336181089282 + 0.001 * 7.091455459594727
Epoch 910, val loss: 1.2774920463562012
Epoch 920, training loss: 0.010176653042435646 = 0.0030606561340391636 + 0.001 * 7.11599588394165
Epoch 920, val loss: 1.282561182975769
Epoch 930, training loss: 0.01007828302681446 = 0.002978990552946925 + 0.001 * 7.099291801452637
Epoch 930, val loss: 1.2875139713287354
Epoch 940, training loss: 0.009991047903895378 = 0.002901108469814062 + 0.001 * 7.089939594268799
Epoch 940, val loss: 1.29238760471344
Epoch 950, training loss: 0.009906299412250519 = 0.0028267286252230406 + 0.001 * 7.079570293426514
Epoch 950, val loss: 1.2971488237380981
Epoch 960, training loss: 0.009866701439023018 = 0.0027556547429412603 + 0.001 * 7.11104679107666
Epoch 960, val loss: 1.3018180131912231
Epoch 970, training loss: 0.0097651407122612 = 0.002687741070985794 + 0.001 * 7.077399253845215
Epoch 970, val loss: 1.3063867092132568
Epoch 980, training loss: 0.009720172733068466 = 0.0026227529160678387 + 0.001 * 7.097419738769531
Epoch 980, val loss: 1.3108586072921753
Epoch 990, training loss: 0.009660987183451653 = 0.0025605736300349236 + 0.001 * 7.100412845611572
Epoch 990, val loss: 1.3152469396591187
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.845545598313126
=== training gcn model ===
Epoch 0, training loss: 1.9454907178878784 = 1.9368939399719238 + 0.001 * 8.596831321716309
Epoch 0, val loss: 1.9254640340805054
Epoch 10, training loss: 1.9355233907699585 = 1.926926612854004 + 0.001 * 8.596793174743652
Epoch 10, val loss: 1.9159016609191895
Epoch 20, training loss: 1.9233566522598267 = 1.9147599935531616 + 0.001 * 8.596633911132812
Epoch 20, val loss: 1.903794765472412
Epoch 30, training loss: 1.9062639474868774 = 1.897667646408081 + 0.001 * 8.5962553024292
Epoch 30, val loss: 1.88641357421875
Epoch 40, training loss: 1.881165623664856 = 1.872570276260376 + 0.001 * 8.595356941223145
Epoch 40, val loss: 1.8609933853149414
Epoch 50, training loss: 1.8465784788131714 = 1.8379855155944824 + 0.001 * 8.592976570129395
Epoch 50, val loss: 1.827566385269165
Epoch 60, training loss: 1.807837963104248 = 1.7992528676986694 + 0.001 * 8.585113525390625
Epoch 60, val loss: 1.7937504053115845
Epoch 70, training loss: 1.769571304321289 = 1.7610200643539429 + 0.001 * 8.551194190979004
Epoch 70, val loss: 1.7625430822372437
Epoch 80, training loss: 1.7184019088745117 = 1.7100809812545776 + 0.001 * 8.320889472961426
Epoch 80, val loss: 1.7178326845169067
Epoch 90, training loss: 1.6480118036270142 = 1.6399309635162354 + 0.001 * 8.080873489379883
Epoch 90, val loss: 1.653804898262024
Epoch 100, training loss: 1.5583370923995972 = 1.5503849983215332 + 0.001 * 7.952055931091309
Epoch 100, val loss: 1.573365569114685
Epoch 110, training loss: 1.4584027528762817 = 1.4506590366363525 + 0.001 * 7.743701934814453
Epoch 110, val loss: 1.4877917766571045
Epoch 120, training loss: 1.3545007705688477 = 1.3468365669250488 + 0.001 * 7.664231777191162
Epoch 120, val loss: 1.4017094373703003
Epoch 130, training loss: 1.2463871240615845 = 1.238728642463684 + 0.001 * 7.65847635269165
Epoch 130, val loss: 1.3143267631530762
Epoch 140, training loss: 1.1345758438110352 = 1.126933217048645 + 0.001 * 7.642578125
Epoch 140, val loss: 1.2250624895095825
Epoch 150, training loss: 1.0234559774398804 = 1.0158227682113647 + 0.001 * 7.633249282836914
Epoch 150, val loss: 1.1378053426742554
Epoch 160, training loss: 0.9193031191825867 = 0.9116798639297485 + 0.001 * 7.623270511627197
Epoch 160, val loss: 1.0583171844482422
Epoch 170, training loss: 0.8266969323158264 = 0.8190881013870239 + 0.001 * 7.608806610107422
Epoch 170, val loss: 0.9903559684753418
Epoch 180, training loss: 0.7467549443244934 = 0.7391661405563354 + 0.001 * 7.588832378387451
Epoch 180, val loss: 0.9356749653816223
Epoch 190, training loss: 0.6779136061668396 = 0.6703445911407471 + 0.001 * 7.568991184234619
Epoch 190, val loss: 0.8933987617492676
Epoch 200, training loss: 0.6176672577857971 = 0.610113263130188 + 0.001 * 7.553997993469238
Epoch 200, val loss: 0.86152583360672
Epoch 210, training loss: 0.5638697743415833 = 0.5563384294509888 + 0.001 * 7.531373977661133
Epoch 210, val loss: 0.8381452560424805
Epoch 220, training loss: 0.5152280330657959 = 0.5077378153800964 + 0.001 * 7.490227222442627
Epoch 220, val loss: 0.8216484785079956
Epoch 230, training loss: 0.47100281715393066 = 0.4635731875896454 + 0.001 * 7.429638385772705
Epoch 230, val loss: 0.8109726309776306
Epoch 240, training loss: 0.43073177337646484 = 0.4233425259590149 + 0.001 * 7.389248847961426
Epoch 240, val loss: 0.8053731918334961
Epoch 250, training loss: 0.39405396580696106 = 0.3866877853870392 + 0.001 * 7.366180419921875
Epoch 250, val loss: 0.8043437004089355
Epoch 260, training loss: 0.3606497347354889 = 0.3532921075820923 + 0.001 * 7.357636451721191
Epoch 260, val loss: 0.807491660118103
Epoch 270, training loss: 0.3301171660423279 = 0.3227670192718506 + 0.001 * 7.350135803222656
Epoch 270, val loss: 0.8141118884086609
Epoch 280, training loss: 0.3020050525665283 = 0.2946634292602539 + 0.001 * 7.341625213623047
Epoch 280, val loss: 0.8237370252609253
Epoch 290, training loss: 0.27585408091545105 = 0.2685278654098511 + 0.001 * 7.326201438903809
Epoch 290, val loss: 0.8362120389938354
Epoch 300, training loss: 0.2513556182384491 = 0.24404951930046082 + 0.001 * 7.306088924407959
Epoch 300, val loss: 0.8512570261955261
Epoch 310, training loss: 0.22838182747364044 = 0.2210875153541565 + 0.001 * 7.2943115234375
Epoch 310, val loss: 0.8687120079994202
Epoch 320, training loss: 0.20696412026882172 = 0.19968722760677338 + 0.001 * 7.276895046234131
Epoch 320, val loss: 0.8884323239326477
Epoch 330, training loss: 0.18724659085273743 = 0.17997340857982635 + 0.001 * 7.273177623748779
Epoch 330, val loss: 0.9104211330413818
Epoch 340, training loss: 0.16929295659065247 = 0.1620456576347351 + 0.001 * 7.2473015785217285
Epoch 340, val loss: 0.9345283508300781
Epoch 350, training loss: 0.15311452746391296 = 0.14586873352527618 + 0.001 * 7.24578857421875
Epoch 350, val loss: 0.9604910612106323
Epoch 360, training loss: 0.13858051598072052 = 0.13134169578552246 + 0.001 * 7.2388153076171875
Epoch 360, val loss: 0.9880436658859253
Epoch 370, training loss: 0.12556461989879608 = 0.1183260977268219 + 0.001 * 7.238522052764893
Epoch 370, val loss: 1.0166882276535034
Epoch 380, training loss: 0.11391302943229675 = 0.10668114572763443 + 0.001 * 7.2318830490112305
Epoch 380, val loss: 1.046034336090088
Epoch 390, training loss: 0.10349210351705551 = 0.09626026451587677 + 0.001 * 7.231839179992676
Epoch 390, val loss: 1.0759692192077637
Epoch 400, training loss: 0.09416739642620087 = 0.08692985773086548 + 0.001 * 7.237541198730469
Epoch 400, val loss: 1.106301188468933
Epoch 410, training loss: 0.08580432087182999 = 0.07857080549001694 + 0.001 * 7.233512878417969
Epoch 410, val loss: 1.1367920637130737
Epoch 420, training loss: 0.07830981910228729 = 0.07107800245285034 + 0.001 * 7.231815338134766
Epoch 420, val loss: 1.1673144102096558
Epoch 430, training loss: 0.07158657163381577 = 0.06436078250408173 + 0.001 * 7.225786209106445
Epoch 430, val loss: 1.1976832151412964
Epoch 440, training loss: 0.06557468324899673 = 0.0583440326154232 + 0.001 * 7.230648994445801
Epoch 440, val loss: 1.227718472480774
Epoch 450, training loss: 0.06018093600869179 = 0.052957817912101746 + 0.001 * 7.223116397857666
Epoch 450, val loss: 1.2573182582855225
Epoch 460, training loss: 0.0553499273955822 = 0.04813065379858017 + 0.001 * 7.219273567199707
Epoch 460, val loss: 1.2864335775375366
Epoch 470, training loss: 0.05102967098355293 = 0.04381393641233444 + 0.001 * 7.215733528137207
Epoch 470, val loss: 1.3150588274002075
Epoch 480, training loss: 0.04716172069311142 = 0.039953913539648056 + 0.001 * 7.207805156707764
Epoch 480, val loss: 1.343008279800415
Epoch 490, training loss: 0.04370827227830887 = 0.03650399297475815 + 0.001 * 7.204280376434326
Epoch 490, val loss: 1.3702502250671387
Epoch 500, training loss: 0.0406590960919857 = 0.03341987356543541 + 0.001 * 7.239223003387451
Epoch 500, val loss: 1.396756887435913
Epoch 510, training loss: 0.03787434473633766 = 0.03066176362335682 + 0.001 * 7.212581157684326
Epoch 510, val loss: 1.4224989414215088
Epoch 520, training loss: 0.03538781404495239 = 0.02819252200424671 + 0.001 * 7.195290565490723
Epoch 520, val loss: 1.4474587440490723
Epoch 530, training loss: 0.0331716313958168 = 0.025978190824389458 + 0.001 * 7.193439960479736
Epoch 530, val loss: 1.4716553688049316
Epoch 540, training loss: 0.03117680922150612 = 0.023987531661987305 + 0.001 * 7.189277172088623
Epoch 540, val loss: 1.4951362609863281
Epoch 550, training loss: 0.029413823038339615 = 0.022192705422639847 + 0.001 * 7.2211174964904785
Epoch 550, val loss: 1.5179271697998047
Epoch 560, training loss: 0.027754254639148712 = 0.02057063952088356 + 0.001 * 7.183614730834961
Epoch 560, val loss: 1.5400556325912476
Epoch 570, training loss: 0.02629733830690384 = 0.019101571291685104 + 0.001 * 7.195767402648926
Epoch 570, val loss: 1.5614893436431885
Epoch 580, training loss: 0.024945886805653572 = 0.017769256606698036 + 0.001 * 7.17663049697876
Epoch 580, val loss: 1.5822733640670776
Epoch 590, training loss: 0.02374289557337761 = 0.016559960320591927 + 0.001 * 7.182934284210205
Epoch 590, val loss: 1.6024209260940552
Epoch 600, training loss: 0.02262958511710167 = 0.015461290255188942 + 0.001 * 7.168294429779053
Epoch 600, val loss: 1.6219805479049683
Epoch 610, training loss: 0.021611951291561127 = 0.014461755752563477 + 0.001 * 7.15019416809082
Epoch 610, val loss: 1.6409416198730469
Epoch 620, training loss: 0.02074095606803894 = 0.013551252894103527 + 0.001 * 7.18970251083374
Epoch 620, val loss: 1.6593502759933472
Epoch 630, training loss: 0.019892513751983643 = 0.012721134349703789 + 0.001 * 7.1713786125183105
Epoch 630, val loss: 1.6771695613861084
Epoch 640, training loss: 0.019117118790745735 = 0.011963002383708954 + 0.001 * 7.154116630554199
Epoch 640, val loss: 1.694475531578064
Epoch 650, training loss: 0.018424861133098602 = 0.011269498616456985 + 0.001 * 7.155362129211426
Epoch 650, val loss: 1.7112524509429932
Epoch 660, training loss: 0.017795804888010025 = 0.010634197853505611 + 0.001 * 7.1616058349609375
Epoch 660, val loss: 1.7275254726409912
Epoch 670, training loss: 0.01720486395061016 = 0.010051126591861248 + 0.001 * 7.1537370681762695
Epoch 670, val loss: 1.743293046951294
Epoch 680, training loss: 0.016647662967443466 = 0.0095151886343956 + 0.001 * 7.132473468780518
Epoch 680, val loss: 1.7586236000061035
Epoch 690, training loss: 0.016182059422135353 = 0.009021642617881298 + 0.001 * 7.160416603088379
Epoch 690, val loss: 1.7734670639038086
Epoch 700, training loss: 0.01570526883006096 = 0.008566447533667088 + 0.001 * 7.138820171356201
Epoch 700, val loss: 1.7879011631011963
Epoch 710, training loss: 0.01526310108602047 = 0.008145560510456562 + 0.001 * 7.117539882659912
Epoch 710, val loss: 1.8018478155136108
Epoch 720, training loss: 0.014891336672008038 = 0.007755910512059927 + 0.001 * 7.135426044464111
Epoch 720, val loss: 1.8153809309005737
Epoch 730, training loss: 0.014509324915707111 = 0.007394850719720125 + 0.001 * 7.114473819732666
Epoch 730, val loss: 1.82854163646698
Epoch 740, training loss: 0.014213832095265388 = 0.007059618830680847 + 0.001 * 7.154212474822998
Epoch 740, val loss: 1.84129798412323
Epoch 750, training loss: 0.01388217881321907 = 0.006748187821358442 + 0.001 * 7.133990287780762
Epoch 750, val loss: 1.853783130645752
Epoch 760, training loss: 0.013580430299043655 = 0.006458211224526167 + 0.001 * 7.122218608856201
Epoch 760, val loss: 1.8658716678619385
Epoch 770, training loss: 0.013309432193636894 = 0.00618779007345438 + 0.001 * 7.121641635894775
Epoch 770, val loss: 1.8776487112045288
Epoch 780, training loss: 0.013048513792455196 = 0.005935212597250938 + 0.001 * 7.113300800323486
Epoch 780, val loss: 1.8890653848648071
Epoch 790, training loss: 0.012814674526453018 = 0.00569893280044198 + 0.001 * 7.115740776062012
Epoch 790, val loss: 1.9001754522323608
Epoch 800, training loss: 0.0125699732452631 = 0.005477655678987503 + 0.001 * 7.092317581176758
Epoch 800, val loss: 1.9110357761383057
Epoch 810, training loss: 0.012389520183205605 = 0.005270108114928007 + 0.001 * 7.119412422180176
Epoch 810, val loss: 1.9215829372406006
Epoch 820, training loss: 0.012164208106696606 = 0.005075248423963785 + 0.001 * 7.088959217071533
Epoch 820, val loss: 1.9318451881408691
Epoch 830, training loss: 0.011973781511187553 = 0.004892043769359589 + 0.001 * 7.081737518310547
Epoch 830, val loss: 1.9418443441390991
Epoch 840, training loss: 0.011805779300630093 = 0.004719582386314869 + 0.001 * 7.086196422576904
Epoch 840, val loss: 1.9515676498413086
Epoch 850, training loss: 0.011634687893092632 = 0.004557033535093069 + 0.001 * 7.077653884887695
Epoch 850, val loss: 1.961029052734375
Epoch 860, training loss: 0.011502588167786598 = 0.004403677303344011 + 0.001 * 7.098909854888916
Epoch 860, val loss: 1.9702683687210083
Epoch 870, training loss: 0.011356240138411522 = 0.004258803091943264 + 0.001 * 7.097436904907227
Epoch 870, val loss: 1.9792333841323853
Epoch 880, training loss: 0.011197047308087349 = 0.00412185862660408 + 0.001 * 7.075188159942627
Epoch 880, val loss: 1.9880151748657227
Epoch 890, training loss: 0.01109403558075428 = 0.003992255311459303 + 0.001 * 7.101779937744141
Epoch 890, val loss: 1.9965273141860962
Epoch 900, training loss: 0.010969368740916252 = 0.0038695153780281544 + 0.001 * 7.099853515625
Epoch 900, val loss: 2.0048959255218506
Epoch 910, training loss: 0.010838299989700317 = 0.003753126598894596 + 0.001 * 7.085172653198242
Epoch 910, val loss: 2.0129966735839844
Epoch 920, training loss: 0.010720308870077133 = 0.003642687341198325 + 0.001 * 7.0776214599609375
Epoch 920, val loss: 2.0209543704986572
Epoch 930, training loss: 0.010618994943797588 = 0.003537776181474328 + 0.001 * 7.081218242645264
Epoch 930, val loss: 2.028684139251709
Epoch 940, training loss: 0.010506274178624153 = 0.0034380280412733555 + 0.001 * 7.0682454109191895
Epoch 940, val loss: 2.036254405975342
Epoch 950, training loss: 0.010416004806756973 = 0.0033431376796215773 + 0.001 * 7.072866439819336
Epoch 950, val loss: 2.0436415672302246
Epoch 960, training loss: 0.010338776744902134 = 0.0032527500297874212 + 0.001 * 7.086026668548584
Epoch 960, val loss: 2.0508365631103516
Epoch 970, training loss: 0.010260478593409061 = 0.003166632493957877 + 0.001 * 7.093845844268799
Epoch 970, val loss: 2.0579206943511963
Epoch 980, training loss: 0.01015743799507618 = 0.0030844814609736204 + 0.001 * 7.072956562042236
Epoch 980, val loss: 2.064767599105835
Epoch 990, training loss: 0.010079900734126568 = 0.0030060813296586275 + 0.001 * 7.073819160461426
Epoch 990, val loss: 2.0715084075927734
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8313125988402742
The final CL Acc:0.78395, 0.01848, The final GNN Acc:0.83676, 0.00627
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11572])
remove edge: torch.Size([2, 9568])
updated graph: torch.Size([2, 10584])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9357624053955078 = 1.9271655082702637 + 0.001 * 8.596855163574219
Epoch 0, val loss: 1.9239106178283691
Epoch 10, training loss: 1.926604151725769 = 1.9180073738098145 + 0.001 * 8.596817016601562
Epoch 10, val loss: 1.9140688180923462
Epoch 20, training loss: 1.9155359268188477 = 1.9069392681121826 + 0.001 * 8.596688270568848
Epoch 20, val loss: 1.902044415473938
Epoch 30, training loss: 1.9002563953399658 = 1.8916599750518799 + 0.001 * 8.596378326416016
Epoch 30, val loss: 1.885497808456421
Epoch 40, training loss: 1.8780488967895508 = 1.8694533109664917 + 0.001 * 8.5956392288208
Epoch 40, val loss: 1.8618347644805908
Epoch 50, training loss: 1.8477085828781128 = 1.839114785194397 + 0.001 * 8.593791961669922
Epoch 50, val loss: 1.8312166929244995
Epoch 60, training loss: 1.8149445056915283 = 1.8063560724258423 + 0.001 * 8.588460922241211
Epoch 60, val loss: 1.8021807670593262
Epoch 70, training loss: 1.785499095916748 = 1.776929497718811 + 0.001 * 8.569611549377441
Epoch 70, val loss: 1.7801045179367065
Epoch 80, training loss: 1.7467522621154785 = 1.7382882833480835 + 0.001 * 8.46398639678955
Epoch 80, val loss: 1.7495684623718262
Epoch 90, training loss: 1.6922693252563477 = 1.6840786933898926 + 0.001 * 8.190641403198242
Epoch 90, val loss: 1.704576015472412
Epoch 100, training loss: 1.619751214981079 = 1.6116392612457275 + 0.001 * 8.111997604370117
Epoch 100, val loss: 1.64539635181427
Epoch 110, training loss: 1.535940408706665 = 1.527866244316101 + 0.001 * 8.074106216430664
Epoch 110, val loss: 1.5785250663757324
Epoch 120, training loss: 1.449958324432373 = 1.4419101476669312 + 0.001 * 8.04822826385498
Epoch 120, val loss: 1.5121461153030396
Epoch 130, training loss: 1.3640122413635254 = 1.3560261726379395 + 0.001 * 7.986062526702881
Epoch 130, val loss: 1.4489667415618896
Epoch 140, training loss: 1.2760465145111084 = 1.268264651298523 + 0.001 * 7.781914710998535
Epoch 140, val loss: 1.3852903842926025
Epoch 150, training loss: 1.186382532119751 = 1.178682565689087 + 0.001 * 7.69995641708374
Epoch 150, val loss: 1.3213465213775635
Epoch 160, training loss: 1.0975984334945679 = 1.0899714231491089 + 0.001 * 7.62704610824585
Epoch 160, val loss: 1.2587872743606567
Epoch 170, training loss: 1.012101411819458 = 1.0045216083526611 + 0.001 * 7.579841613769531
Epoch 170, val loss: 1.1983333826065063
Epoch 180, training loss: 0.9304553270339966 = 0.9229204058647156 + 0.001 * 7.534938335418701
Epoch 180, val loss: 1.1402685642242432
Epoch 190, training loss: 0.852119505405426 = 0.8446260094642639 + 0.001 * 7.493473529815674
Epoch 190, val loss: 1.0844948291778564
Epoch 200, training loss: 0.7769021987915039 = 0.7694430947303772 + 0.001 * 7.459095478057861
Epoch 200, val loss: 1.0319530963897705
Epoch 210, training loss: 0.705557107925415 = 0.6981381177902222 + 0.001 * 7.418986797332764
Epoch 210, val loss: 0.9842965602874756
Epoch 220, training loss: 0.6393035054206848 = 0.6319129467010498 + 0.001 * 7.390533924102783
Epoch 220, val loss: 0.9428790807723999
Epoch 230, training loss: 0.5788013339042664 = 0.571428656578064 + 0.001 * 7.372658729553223
Epoch 230, val loss: 0.908051609992981
Epoch 240, training loss: 0.5237311124801636 = 0.5163631439208984 + 0.001 * 7.367992401123047
Epoch 240, val loss: 0.8789882659912109
Epoch 250, training loss: 0.47305023670196533 = 0.46568402647972107 + 0.001 * 7.366212368011475
Epoch 250, val loss: 0.8542636632919312
Epoch 260, training loss: 0.42569500207901 = 0.4183298945426941 + 0.001 * 7.3651123046875
Epoch 260, val loss: 0.8330662250518799
Epoch 270, training loss: 0.38096511363983154 = 0.3736015260219574 + 0.001 * 7.363592147827148
Epoch 270, val loss: 0.8149344325065613
Epoch 280, training loss: 0.3386971652507782 = 0.3313348591327667 + 0.001 * 7.362316131591797
Epoch 280, val loss: 0.7997840046882629
Epoch 290, training loss: 0.2991061210632324 = 0.29174432158470154 + 0.001 * 7.361810684204102
Epoch 290, val loss: 0.7879456877708435
Epoch 300, training loss: 0.2624325752258301 = 0.2550722658634186 + 0.001 * 7.360306739807129
Epoch 300, val loss: 0.779566764831543
Epoch 310, training loss: 0.22891531884670258 = 0.22155603766441345 + 0.001 * 7.359279632568359
Epoch 310, val loss: 0.7750707268714905
Epoch 320, training loss: 0.19878366589546204 = 0.19142484664916992 + 0.001 * 7.358821868896484
Epoch 320, val loss: 0.7746415734291077
Epoch 330, training loss: 0.17218603193759918 = 0.16482506692409515 + 0.001 * 7.360968589782715
Epoch 330, val loss: 0.777995765209198
Epoch 340, training loss: 0.14910244941711426 = 0.14174573123455048 + 0.001 * 7.356720447540283
Epoch 340, val loss: 0.7846119403839111
Epoch 350, training loss: 0.12933923304080963 = 0.12198209017515182 + 0.001 * 7.357147693634033
Epoch 350, val loss: 0.7938600182533264
Epoch 360, training loss: 0.11254919320344925 = 0.10519498586654663 + 0.001 * 7.354208469390869
Epoch 360, val loss: 0.8050591349601746
Epoch 370, training loss: 0.09835460036993027 = 0.09099886566400528 + 0.001 * 7.355733394622803
Epoch 370, val loss: 0.8176653981208801
Epoch 380, training loss: 0.08636429905891418 = 0.07901277393102646 + 0.001 * 7.351527690887451
Epoch 380, val loss: 0.8313069939613342
Epoch 390, training loss: 0.07623423635959625 = 0.06888390332460403 + 0.001 * 7.350334167480469
Epoch 390, val loss: 0.8456143140792847
Epoch 400, training loss: 0.06765181571245193 = 0.06030289828777313 + 0.001 * 7.3489155769348145
Epoch 400, val loss: 0.8602755665779114
Epoch 410, training loss: 0.060367684811353683 = 0.05301780253648758 + 0.001 * 7.349881649017334
Epoch 410, val loss: 0.8751367926597595
Epoch 420, training loss: 0.054169028997421265 = 0.046824757009744644 + 0.001 * 7.344273567199707
Epoch 420, val loss: 0.8899984359741211
Epoch 430, training loss: 0.0489194430410862 = 0.04155157878994942 + 0.001 * 7.367863655090332
Epoch 430, val loss: 0.9047476053237915
Epoch 440, training loss: 0.04440055415034294 = 0.037051815539598465 + 0.001 * 7.348738193511963
Epoch 440, val loss: 0.919249415397644
Epoch 450, training loss: 0.04054515063762665 = 0.03320100158452988 + 0.001 * 7.344146728515625
Epoch 450, val loss: 0.9333927631378174
Epoch 460, training loss: 0.037232667207717896 = 0.02989349141716957 + 0.001 * 7.339174270629883
Epoch 460, val loss: 0.9471213221549988
Epoch 470, training loss: 0.03438026085495949 = 0.027041396126151085 + 0.001 * 7.338865756988525
Epoch 470, val loss: 0.9604073762893677
Epoch 480, training loss: 0.03190618008375168 = 0.024571189656853676 + 0.001 * 7.334988117218018
Epoch 480, val loss: 0.973271906375885
Epoch 490, training loss: 0.029756471514701843 = 0.022422131150960922 + 0.001 * 7.334339618682861
Epoch 490, val loss: 0.9856853485107422
Epoch 500, training loss: 0.027876058593392372 = 0.020543692633509636 + 0.001 * 7.332365989685059
Epoch 500, val loss: 0.9976617693901062
Epoch 510, training loss: 0.02622251585125923 = 0.018893985077738762 + 0.001 * 7.328529357910156
Epoch 510, val loss: 1.009238839149475
Epoch 520, training loss: 0.024765200912952423 = 0.01743852160871029 + 0.001 * 7.326679706573486
Epoch 520, val loss: 1.020391821861267
Epoch 530, training loss: 0.023473557084798813 = 0.01614883728325367 + 0.001 * 7.32472038269043
Epoch 530, val loss: 1.0311858654022217
Epoch 540, training loss: 0.022324780002236366 = 0.015001196414232254 + 0.001 * 7.323582649230957
Epoch 540, val loss: 1.0415955781936646
Epoch 550, training loss: 0.0213058739900589 = 0.013975915499031544 + 0.001 * 7.329958438873291
Epoch 550, val loss: 1.0516566038131714
Epoch 560, training loss: 0.0203884094953537 = 0.013056638650596142 + 0.001 * 7.331769943237305
Epoch 560, val loss: 1.061389446258545
Epoch 570, training loss: 0.01955866627395153 = 0.012229174375534058 + 0.001 * 7.329492092132568
Epoch 570, val loss: 1.0708036422729492
Epoch 580, training loss: 0.01880006305873394 = 0.011481717228889465 + 0.001 * 7.31834602355957
Epoch 580, val loss: 1.0799026489257812
Epoch 590, training loss: 0.018123261630535126 = 0.010804308578372002 + 0.001 * 7.318953514099121
Epoch 590, val loss: 1.0886999368667603
Epoch 600, training loss: 0.017503837123513222 = 0.010188538581132889 + 0.001 * 7.315298080444336
Epoch 600, val loss: 1.0972368717193604
Epoch 610, training loss: 0.016945019364356995 = 0.009627110324800014 + 0.001 * 7.317907810211182
Epoch 610, val loss: 1.1055119037628174
Epoch 620, training loss: 0.01645449735224247 = 0.009113793261349201 + 0.001 * 7.34070348739624
Epoch 620, val loss: 1.113538146018982
Epoch 630, training loss: 0.015954017639160156 = 0.00864318385720253 + 0.001 * 7.310833930969238
Epoch 630, val loss: 1.1213302612304688
Epoch 640, training loss: 0.015528513118624687 = 0.008210684172809124 + 0.001 * 7.317828178405762
Epoch 640, val loss: 1.1288896799087524
Epoch 650, training loss: 0.015138918533921242 = 0.007812251336872578 + 0.001 * 7.326666355133057
Epoch 650, val loss: 1.1362380981445312
Epoch 660, training loss: 0.014756688848137856 = 0.007444409187883139 + 0.001 * 7.31227970123291
Epoch 660, val loss: 1.1433717012405396
Epoch 670, training loss: 0.014424179680645466 = 0.007104051765054464 + 0.001 * 7.320127487182617
Epoch 670, val loss: 1.150305986404419
Epoch 680, training loss: 0.014098836109042168 = 0.00678853178396821 + 0.001 * 7.310304164886475
Epoch 680, val loss: 1.1570488214492798
Epoch 690, training loss: 0.01380419172346592 = 0.00649542361497879 + 0.001 * 7.308767318725586
Epoch 690, val loss: 1.1636216640472412
Epoch 700, training loss: 0.013532320037484169 = 0.00622268533334136 + 0.001 * 7.309633731842041
Epoch 700, val loss: 1.170011043548584
Epoch 710, training loss: 0.01325898990035057 = 0.0059684161096811295 + 0.001 * 7.2905731201171875
Epoch 710, val loss: 1.1762468814849854
Epoch 720, training loss: 0.013027673587203026 = 0.005730987526476383 + 0.001 * 7.296685695648193
Epoch 720, val loss: 1.182306170463562
Epoch 730, training loss: 0.012807313352823257 = 0.005508945789188147 + 0.001 * 7.298366546630859
Epoch 730, val loss: 1.1882243156433105
Epoch 740, training loss: 0.012592346407473087 = 0.0053008063696324825 + 0.001 * 7.291539669036865
Epoch 740, val loss: 1.1940046548843384
Epoch 750, training loss: 0.012391971424221992 = 0.005105263087898493 + 0.001 * 7.286708354949951
Epoch 750, val loss: 1.1996723413467407
Epoch 760, training loss: 0.012205246835947037 = 0.004921507090330124 + 0.001 * 7.2837395668029785
Epoch 760, val loss: 1.205198884010315
Epoch 770, training loss: 0.01203115563839674 = 0.004748400766402483 + 0.001 * 7.282754421234131
Epoch 770, val loss: 1.2105934619903564
Epoch 780, training loss: 0.011874713003635406 = 0.004585064481943846 + 0.001 * 7.289648056030273
Epoch 780, val loss: 1.215886116027832
Epoch 790, training loss: 0.011701421812176704 = 0.004430672619491816 + 0.001 * 7.270749092102051
Epoch 790, val loss: 1.221069097518921
Epoch 800, training loss: 0.011557754129171371 = 0.0042845504358410835 + 0.001 * 7.2732038497924805
Epoch 800, val loss: 1.2261459827423096
Epoch 810, training loss: 0.011431784369051456 = 0.004145954269915819 + 0.001 * 7.285829544067383
Epoch 810, val loss: 1.231137990951538
Epoch 820, training loss: 0.011312372982501984 = 0.0040143500082194805 + 0.001 * 7.298023223876953
Epoch 820, val loss: 1.236048698425293
Epoch 830, training loss: 0.011151490733027458 = 0.0038893346209079027 + 0.001 * 7.262155532836914
Epoch 830, val loss: 1.240875005722046
Epoch 840, training loss: 0.011036036536097527 = 0.0037702862173318863 + 0.001 * 7.265750408172607
Epoch 840, val loss: 1.2456347942352295
Epoch 850, training loss: 0.010901719331741333 = 0.00365691096521914 + 0.001 * 7.244808197021484
Epoch 850, val loss: 1.2503167390823364
Epoch 860, training loss: 0.010793564841151237 = 0.003548816777765751 + 0.001 * 7.244747161865234
Epoch 860, val loss: 1.2549335956573486
Epoch 870, training loss: 0.010694100521504879 = 0.0034458094742149115 + 0.001 * 7.248290538787842
Epoch 870, val loss: 1.259464979171753
Epoch 880, training loss: 0.010603290982544422 = 0.003347566118463874 + 0.001 * 7.255724906921387
Epoch 880, val loss: 1.2639257907867432
Epoch 890, training loss: 0.01050395704805851 = 0.003253870876505971 + 0.001 * 7.250085830688477
Epoch 890, val loss: 1.2683178186416626
Epoch 900, training loss: 0.010421492159366608 = 0.0031644615810364485 + 0.001 * 7.257030487060547
Epoch 900, val loss: 1.2726298570632935
Epoch 910, training loss: 0.010357793420553207 = 0.0030791321769356728 + 0.001 * 7.278660774230957
Epoch 910, val loss: 1.2768476009368896
Epoch 920, training loss: 0.010256371460855007 = 0.002997569041326642 + 0.001 * 7.2588019371032715
Epoch 920, val loss: 1.2810195684432983
Epoch 930, training loss: 0.010165058076381683 = 0.0029195265378803015 + 0.001 * 7.24553108215332
Epoch 930, val loss: 1.285128116607666
Epoch 940, training loss: 0.010071774944663048 = 0.002844704082235694 + 0.001 * 7.227070331573486
Epoch 940, val loss: 1.2891604900360107
Epoch 950, training loss: 0.010026226751506329 = 0.00277289398945868 + 0.001 * 7.253332138061523
Epoch 950, val loss: 1.2931550741195679
Epoch 960, training loss: 0.009931515902280807 = 0.0027036890387535095 + 0.001 * 7.227826118469238
Epoch 960, val loss: 1.2971309423446655
Epoch 970, training loss: 0.009871126152575016 = 0.002636790042743087 + 0.001 * 7.234335422515869
Epoch 970, val loss: 1.3010838031768799
Epoch 980, training loss: 0.009787697345018387 = 0.0025719329714775085 + 0.001 * 7.215764045715332
Epoch 980, val loss: 1.30498468875885
Epoch 990, training loss: 0.009742270223796368 = 0.0025089068803936243 + 0.001 * 7.233363151550293
Epoch 990, val loss: 1.3089436292648315
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 1.9448527097702026 = 1.9362558126449585 + 0.001 * 8.59685230255127
Epoch 0, val loss: 1.937036395072937
Epoch 10, training loss: 1.93563711643219 = 1.9270403385162354 + 0.001 * 8.596809387207031
Epoch 10, val loss: 1.9275434017181396
Epoch 20, training loss: 1.9242205619812012 = 1.9156239032745361 + 0.001 * 8.596674919128418
Epoch 20, val loss: 1.9157730340957642
Epoch 30, training loss: 1.9083044528961182 = 1.8997080326080322 + 0.001 * 8.596369743347168
Epoch 30, val loss: 1.899250864982605
Epoch 40, training loss: 1.88506019115448 = 1.8764644861221313 + 0.001 * 8.595648765563965
Epoch 40, val loss: 1.8754174709320068
Epoch 50, training loss: 1.8530853986740112 = 1.844491720199585 + 0.001 * 8.593735694885254
Epoch 50, val loss: 1.8442065715789795
Epoch 60, training loss: 1.817944884300232 = 1.8093572854995728 + 0.001 * 8.587621688842773
Epoch 60, val loss: 1.8137584924697876
Epoch 70, training loss: 1.7865327596664429 = 1.7779698371887207 + 0.001 * 8.562891960144043
Epoch 70, val loss: 1.7893399000167847
Epoch 80, training loss: 1.7471389770507812 = 1.7387299537658691 + 0.001 * 8.408974647521973
Epoch 80, val loss: 1.7559759616851807
Epoch 90, training loss: 1.692097783088684 = 1.6839568614959717 + 0.001 * 8.140959739685059
Epoch 90, val loss: 1.7090944051742554
Epoch 100, training loss: 1.6171674728393555 = 1.6091970205307007 + 0.001 * 7.970405578613281
Epoch 100, val loss: 1.647178053855896
Epoch 110, training loss: 1.5264936685562134 = 1.5187350511550903 + 0.001 * 7.758628845214844
Epoch 110, val loss: 1.5739307403564453
Epoch 120, training loss: 1.4279488325119019 = 1.420244574546814 + 0.001 * 7.704300403594971
Epoch 120, val loss: 1.4945950508117676
Epoch 130, training loss: 1.3244068622589111 = 1.3167134523391724 + 0.001 * 7.693424224853516
Epoch 130, val loss: 1.4115488529205322
Epoch 140, training loss: 1.2152594327926636 = 1.2075748443603516 + 0.001 * 7.68463659286499
Epoch 140, val loss: 1.325440764427185
Epoch 150, training loss: 1.102797031402588 = 1.0951164960861206 + 0.001 * 7.680496692657471
Epoch 150, val loss: 1.238146424293518
Epoch 160, training loss: 0.9935436248779297 = 0.9858700633049011 + 0.001 * 7.673581123352051
Epoch 160, val loss: 1.1557985544204712
Epoch 170, training loss: 0.8942244052886963 = 0.8865618705749512 + 0.001 * 7.6625075340271
Epoch 170, val loss: 1.0841227769851685
Epoch 180, training loss: 0.808021068572998 = 0.8003745675086975 + 0.001 * 7.6464762687683105
Epoch 180, val loss: 1.0259668827056885
Epoch 190, training loss: 0.7341025471687317 = 0.7264707684516907 + 0.001 * 7.631782531738281
Epoch 190, val loss: 0.9814881682395935
Epoch 200, training loss: 0.6695660948753357 = 0.6619390249252319 + 0.001 * 7.627081394195557
Epoch 200, val loss: 0.9485393166542053
Epoch 210, training loss: 0.611634373664856 = 0.6040127277374268 + 0.001 * 7.621669292449951
Epoch 210, val loss: 0.9247282147407532
Epoch 220, training loss: 0.5586737990379333 = 0.5510677099227905 + 0.001 * 7.606107711791992
Epoch 220, val loss: 0.9085575342178345
Epoch 230, training loss: 0.5099515318870544 = 0.5023807287216187 + 0.001 * 7.570790767669678
Epoch 230, val loss: 0.8993889093399048
Epoch 240, training loss: 0.46509796380996704 = 0.4575950503349304 + 0.001 * 7.502922534942627
Epoch 240, val loss: 0.8963448405265808
Epoch 250, training loss: 0.4239017069339752 = 0.41644486784935 + 0.001 * 7.456829071044922
Epoch 250, val loss: 0.8983840346336365
Epoch 260, training loss: 0.3861868679523468 = 0.37876608967781067 + 0.001 * 7.42077112197876
Epoch 260, val loss: 0.9050843119621277
Epoch 270, training loss: 0.35162001848220825 = 0.34421321749687195 + 0.001 * 7.406802177429199
Epoch 270, val loss: 0.9160655736923218
Epoch 280, training loss: 0.3195405900478363 = 0.3121650815010071 + 0.001 * 7.375520706176758
Epoch 280, val loss: 0.9304780960083008
Epoch 290, training loss: 0.289268434047699 = 0.28191426396369934 + 0.001 * 7.354161739349365
Epoch 290, val loss: 0.9474050998687744
Epoch 300, training loss: 0.2603605389595032 = 0.2530236840248108 + 0.001 * 7.336859226226807
Epoch 300, val loss: 0.9663317799568176
Epoch 310, training loss: 0.23287925124168396 = 0.2255561798810959 + 0.001 * 7.32307767868042
Epoch 310, val loss: 0.9869133830070496
Epoch 320, training loss: 0.20730972290039062 = 0.2000080645084381 + 0.001 * 7.30165433883667
Epoch 320, val loss: 1.0087705850601196
Epoch 330, training loss: 0.18417666852474213 = 0.17688190937042236 + 0.001 * 7.294759273529053
Epoch 330, val loss: 1.0315033197402954
Epoch 340, training loss: 0.1636485457420349 = 0.15635894238948822 + 0.001 * 7.2896013259887695
Epoch 340, val loss: 1.0551173686981201
Epoch 350, training loss: 0.14560112357139587 = 0.13831938803195953 + 0.001 * 7.28173303604126
Epoch 350, val loss: 1.079692006111145
Epoch 360, training loss: 0.12979798018932343 = 0.1225140169262886 + 0.001 * 7.283962249755859
Epoch 360, val loss: 1.1053472757339478
Epoch 370, training loss: 0.11595119535923004 = 0.10867279022932053 + 0.001 * 7.278408050537109
Epoch 370, val loss: 1.1320676803588867
Epoch 380, training loss: 0.10382401943206787 = 0.09655077010393143 + 0.001 * 7.273251056671143
Epoch 380, val loss: 1.1597284078598022
Epoch 390, training loss: 0.0931902825832367 = 0.08592057228088379 + 0.001 * 7.269712924957275
Epoch 390, val loss: 1.1882110834121704
Epoch 400, training loss: 0.08386141061782837 = 0.07659125328063965 + 0.001 * 7.270153045654297
Epoch 400, val loss: 1.217334270477295
Epoch 410, training loss: 0.07566210627555847 = 0.0683969035744667 + 0.001 * 7.265200614929199
Epoch 410, val loss: 1.2468091249465942
Epoch 420, training loss: 0.06845215708017349 = 0.06119392067193985 + 0.001 * 7.258237838745117
Epoch 420, val loss: 1.2764713764190674
Epoch 430, training loss: 0.062131889164447784 = 0.054859988391399384 + 0.001 * 7.271899700164795
Epoch 430, val loss: 1.306121826171875
Epoch 440, training loss: 0.05655074119567871 = 0.04928719997406006 + 0.001 * 7.263538837432861
Epoch 440, val loss: 1.3356363773345947
Epoch 450, training loss: 0.051644738763570786 = 0.044384345412254333 + 0.001 * 7.26039457321167
Epoch 450, val loss: 1.3649101257324219
Epoch 460, training loss: 0.04732280597090721 = 0.04007013142108917 + 0.001 * 7.252675533294678
Epoch 460, val loss: 1.3938519954681396
Epoch 470, training loss: 0.043533723801374435 = 0.03627030551433563 + 0.001 * 7.263417720794678
Epoch 470, val loss: 1.4222981929779053
Epoch 480, training loss: 0.04017326235771179 = 0.03292180970311165 + 0.001 * 7.251453876495361
Epoch 480, val loss: 1.4500973224639893
Epoch 490, training loss: 0.03722287341952324 = 0.029967451468110085 + 0.001 * 7.255420207977295
Epoch 490, val loss: 1.4772425889968872
Epoch 500, training loss: 0.034604642540216446 = 0.027357473969459534 + 0.001 * 7.247167587280273
Epoch 500, val loss: 1.5036892890930176
Epoch 510, training loss: 0.032292578369379044 = 0.025048399344086647 + 0.001 * 7.24417781829834
Epoch 510, val loss: 1.5293821096420288
Epoch 520, training loss: 0.03024621121585369 = 0.023001492023468018 + 0.001 * 7.2447190284729
Epoch 520, val loss: 1.5543162822723389
Epoch 530, training loss: 0.028426172211766243 = 0.02118237316608429 + 0.001 * 7.243798732757568
Epoch 530, val loss: 1.5784977674484253
Epoch 540, training loss: 0.02680155821144581 = 0.01956203766167164 + 0.001 * 7.239520072937012
Epoch 540, val loss: 1.6019409894943237
Epoch 550, training loss: 0.025378942489624023 = 0.018115464597940445 + 0.001 * 7.2634782791137695
Epoch 550, val loss: 1.6246460676193237
Epoch 560, training loss: 0.024054231122136116 = 0.016820453107357025 + 0.001 * 7.233777046203613
Epoch 560, val loss: 1.6466227769851685
Epoch 570, training loss: 0.022895975038409233 = 0.01565767452120781 + 0.001 * 7.23829984664917
Epoch 570, val loss: 1.6679295301437378
Epoch 580, training loss: 0.02186398208141327 = 0.014611384831368923 + 0.001 * 7.252597808837891
Epoch 580, val loss: 1.688575267791748
Epoch 590, training loss: 0.020894937217235565 = 0.013667058199644089 + 0.001 * 7.227879524230957
Epoch 590, val loss: 1.7085469961166382
Epoch 600, training loss: 0.020044077187776566 = 0.012812549248337746 + 0.001 * 7.231527328491211
Epoch 600, val loss: 1.7278996706008911
Epoch 610, training loss: 0.0192536823451519 = 0.012037334963679314 + 0.001 * 7.216348171234131
Epoch 610, val loss: 1.7466490268707275
Epoch 620, training loss: 0.018554292619228363 = 0.011332212947309017 + 0.001 * 7.222079753875732
Epoch 620, val loss: 1.764833688735962
Epoch 630, training loss: 0.017908377572894096 = 0.010689293034374714 + 0.001 * 7.219084739685059
Epoch 630, val loss: 1.7824794054031372
Epoch 640, training loss: 0.017317965626716614 = 0.01010154839605093 + 0.001 * 7.21641731262207
Epoch 640, val loss: 1.7995939254760742
Epoch 650, training loss: 0.016767648980021477 = 0.009563025087118149 + 0.001 * 7.204623699188232
Epoch 650, val loss: 1.8161860704421997
Epoch 660, training loss: 0.016294024884700775 = 0.009068506769835949 + 0.001 * 7.225518226623535
Epoch 660, val loss: 1.832296371459961
Epoch 670, training loss: 0.015822069719433784 = 0.008613414131104946 + 0.001 * 7.20865535736084
Epoch 670, val loss: 1.8479329347610474
Epoch 680, training loss: 0.015414765104651451 = 0.008193749003112316 + 0.001 * 7.221015930175781
Epoch 680, val loss: 1.8631350994110107
Epoch 690, training loss: 0.015011053532361984 = 0.007805879693478346 + 0.001 * 7.205173015594482
Epoch 690, val loss: 1.8779308795928955
Epoch 700, training loss: 0.014637377113103867 = 0.007446703501045704 + 0.001 * 7.190673351287842
Epoch 700, val loss: 1.8923276662826538
Epoch 710, training loss: 0.014317670837044716 = 0.007113115396350622 + 0.001 * 7.204554557800293
Epoch 710, val loss: 1.9063130617141724
Epoch 720, training loss: 0.01400265283882618 = 0.006802653893828392 + 0.001 * 7.19999885559082
Epoch 720, val loss: 1.9199693202972412
Epoch 730, training loss: 0.013720749877393246 = 0.006512777879834175 + 0.001 * 7.207971572875977
Epoch 730, val loss: 1.9333446025848389
Epoch 740, training loss: 0.013439929112792015 = 0.006240509916096926 + 0.001 * 7.199419021606445
Epoch 740, val loss: 1.9464378356933594
Epoch 750, training loss: 0.013188032433390617 = 0.0059835524298250675 + 0.001 * 7.204479217529297
Epoch 750, val loss: 1.9593851566314697
Epoch 760, training loss: 0.012934804894030094 = 0.005740076303482056 + 0.001 * 7.194728374481201
Epoch 760, val loss: 1.9722371101379395
Epoch 770, training loss: 0.012688801623880863 = 0.0055091180838644505 + 0.001 * 7.179683208465576
Epoch 770, val loss: 1.9849889278411865
Epoch 780, training loss: 0.01249171793460846 = 0.005290021654218435 + 0.001 * 7.201695919036865
Epoch 780, val loss: 1.9975858926773071
Epoch 790, training loss: 0.012258066795766354 = 0.005082414019852877 + 0.001 * 7.175652503967285
Epoch 790, val loss: 2.0101139545440674
Epoch 800, training loss: 0.012059550732374191 = 0.004885663744062185 + 0.001 * 7.173887252807617
Epoch 800, val loss: 2.0224382877349854
Epoch 810, training loss: 0.011861873790621758 = 0.004699481651186943 + 0.001 * 7.1623921394348145
Epoch 810, val loss: 2.034621477127075
Epoch 820, training loss: 0.011682765558362007 = 0.004523221403360367 + 0.001 * 7.159544467926025
Epoch 820, val loss: 2.046618700027466
Epoch 830, training loss: 0.011519530788064003 = 0.0043564834631979465 + 0.001 * 7.163046836853027
Epoch 830, val loss: 2.0584378242492676
Epoch 840, training loss: 0.011344926431775093 = 0.004198762588202953 + 0.001 * 7.1461639404296875
Epoch 840, val loss: 2.0700395107269287
Epoch 850, training loss: 0.011193507350981236 = 0.004049507435411215 + 0.001 * 7.1439995765686035
Epoch 850, val loss: 2.081463098526001
Epoch 860, training loss: 0.011051757261157036 = 0.003908252809196711 + 0.001 * 7.1435041427612305
Epoch 860, val loss: 2.0926997661590576
Epoch 870, training loss: 0.010912277735769749 = 0.0037744652945548296 + 0.001 * 7.13781213760376
Epoch 870, val loss: 2.103696346282959
Epoch 880, training loss: 0.010809030383825302 = 0.003647744655609131 + 0.001 * 7.161285400390625
Epoch 880, val loss: 2.1145517826080322
Epoch 890, training loss: 0.010679694823920727 = 0.0035275164991617203 + 0.001 * 7.152177810668945
Epoch 890, val loss: 2.1251637935638428
Epoch 900, training loss: 0.010553468950092793 = 0.0034135605674237013 + 0.001 * 7.1399078369140625
Epoch 900, val loss: 2.1355903148651123
Epoch 910, training loss: 0.0104346489533782 = 0.0033054372761398554 + 0.001 * 7.12921142578125
Epoch 910, val loss: 2.1458020210266113
Epoch 920, training loss: 0.010336305946111679 = 0.0032028481364250183 + 0.001 * 7.133457183837891
Epoch 920, val loss: 2.1558384895324707
Epoch 930, training loss: 0.010243255645036697 = 0.0031053870916366577 + 0.001 * 7.137868404388428
Epoch 930, val loss: 2.165691614151001
Epoch 940, training loss: 0.010147623717784882 = 0.0030127286445349455 + 0.001 * 7.134894371032715
Epoch 940, val loss: 2.175351142883301
Epoch 950, training loss: 0.01005209144204855 = 0.002924598753452301 + 0.001 * 7.127492427825928
Epoch 950, val loss: 2.184809684753418
Epoch 960, training loss: 0.009993279352784157 = 0.0028407410718500614 + 0.001 * 7.152538299560547
Epoch 960, val loss: 2.1941263675689697
Epoch 970, training loss: 0.009919200092554092 = 0.0027608799282461405 + 0.001 * 7.15831995010376
Epoch 970, val loss: 2.203260898590088
Epoch 980, training loss: 0.009830495342612267 = 0.002684909850358963 + 0.001 * 7.145585536956787
Epoch 980, val loss: 2.2121975421905518
Epoch 990, training loss: 0.009728342294692993 = 0.00261243199929595 + 0.001 * 7.115910530090332
Epoch 990, val loss: 2.2210192680358887
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 1.956709384918213 = 1.9481124877929688 + 0.001 * 8.596882820129395
Epoch 0, val loss: 1.9519144296646118
Epoch 10, training loss: 1.94734787940979 = 1.938750982284546 + 0.001 * 8.596845626831055
Epoch 10, val loss: 1.9423747062683105
Epoch 20, training loss: 1.935941219329834 = 1.9273444414138794 + 0.001 * 8.596731185913086
Epoch 20, val loss: 1.9307067394256592
Epoch 30, training loss: 1.9202384948730469 = 1.911642074584961 + 0.001 * 8.59643840789795
Epoch 30, val loss: 1.9147942066192627
Epoch 40, training loss: 1.8972489833831787 = 1.88865327835083 + 0.001 * 8.595648765563965
Epoch 40, val loss: 1.8918565511703491
Epoch 50, training loss: 1.8647340536117554 = 1.8561406135559082 + 0.001 * 8.593391418457031
Epoch 50, val loss: 1.8607584238052368
Epoch 60, training loss: 1.826568841934204 = 1.8179830312728882 + 0.001 * 8.585853576660156
Epoch 60, val loss: 1.8275251388549805
Epoch 70, training loss: 1.7939095497131348 = 1.7853525876998901 + 0.001 * 8.557015419006348
Epoch 70, val loss: 1.8017224073410034
Epoch 80, training loss: 1.7586942911148071 = 1.7503020763397217 + 0.001 * 8.39226245880127
Epoch 80, val loss: 1.7704311609268188
Epoch 90, training loss: 1.7094314098358154 = 1.7012087106704712 + 0.001 * 8.222694396972656
Epoch 90, val loss: 1.7276387214660645
Epoch 100, training loss: 1.640742301940918 = 1.63259756565094 + 0.001 * 8.144780158996582
Epoch 100, val loss: 1.670517086982727
Epoch 110, training loss: 1.553025722503662 = 1.5449517965316772 + 0.001 * 8.073883056640625
Epoch 110, val loss: 1.5989934206008911
Epoch 120, training loss: 1.458032250404358 = 1.4501070976257324 + 0.001 * 7.9252028465271
Epoch 120, val loss: 1.5233784914016724
Epoch 130, training loss: 1.364540457725525 = 1.3567763566970825 + 0.001 * 7.764056205749512
Epoch 130, val loss: 1.4511076211929321
Epoch 140, training loss: 1.2727302312850952 = 1.2650285959243774 + 0.001 * 7.701606750488281
Epoch 140, val loss: 1.3816317319869995
Epoch 150, training loss: 1.1812125444412231 = 1.1736111640930176 + 0.001 * 7.601351737976074
Epoch 150, val loss: 1.3130818605422974
Epoch 160, training loss: 1.092116117477417 = 1.0845568180084229 + 0.001 * 7.559275150299072
Epoch 160, val loss: 1.2470755577087402
Epoch 170, training loss: 1.0077577829360962 = 1.0002073049545288 + 0.001 * 7.55042028427124
Epoch 170, val loss: 1.18517005443573
Epoch 180, training loss: 0.9282771348953247 = 0.9207353591918945 + 0.001 * 7.541794300079346
Epoch 180, val loss: 1.1267645359039307
Epoch 190, training loss: 0.852367103099823 = 0.844835102558136 + 0.001 * 7.531975269317627
Epoch 190, val loss: 1.0707570314407349
Epoch 200, training loss: 0.7791564464569092 = 0.7716372013092041 + 0.001 * 7.519220352172852
Epoch 200, val loss: 1.0170586109161377
Epoch 210, training loss: 0.7091073989868164 = 0.7016093134880066 + 0.001 * 7.498068332672119
Epoch 210, val loss: 0.9668325781822205
Epoch 220, training loss: 0.6430789232254028 = 0.6356133818626404 + 0.001 * 7.465550899505615
Epoch 220, val loss: 0.9216277599334717
Epoch 230, training loss: 0.5815642476081848 = 0.5741212368011475 + 0.001 * 7.4430108070373535
Epoch 230, val loss: 0.8826901912689209
Epoch 240, training loss: 0.5243199467658997 = 0.5168984532356262 + 0.001 * 7.421472549438477
Epoch 240, val loss: 0.8504071235656738
Epoch 250, training loss: 0.47088685631752014 = 0.4634726941585541 + 0.001 * 7.414173603057861
Epoch 250, val loss: 0.8244240283966064
Epoch 260, training loss: 0.42104557156562805 = 0.41363346576690674 + 0.001 * 7.412102222442627
Epoch 260, val loss: 0.8042145371437073
Epoch 270, training loss: 0.37500429153442383 = 0.36759287118911743 + 0.001 * 7.411405086517334
Epoch 270, val loss: 0.7890335917472839
Epoch 280, training loss: 0.3330900967121124 = 0.32567930221557617 + 0.001 * 7.410804271697998
Epoch 280, val loss: 0.7783815860748291
Epoch 290, training loss: 0.2952575385570526 = 0.2878468930721283 + 0.001 * 7.410645008087158
Epoch 290, val loss: 0.7717536091804504
Epoch 300, training loss: 0.261080265045166 = 0.2536686956882477 + 0.001 * 7.411582946777344
Epoch 300, val loss: 0.7683755159378052
Epoch 310, training loss: 0.23006464540958405 = 0.22265410423278809 + 0.001 * 7.41054105758667
Epoch 310, val loss: 0.767532229423523
Epoch 320, training loss: 0.2019263654947281 = 0.19451546669006348 + 0.001 * 7.4108991622924805
Epoch 320, val loss: 0.7687689661979675
Epoch 330, training loss: 0.17664596438407898 = 0.16923584043979645 + 0.001 * 7.410125732421875
Epoch 330, val loss: 0.7718832492828369
Epoch 340, training loss: 0.1542995572090149 = 0.14689230918884277 + 0.001 * 7.407255172729492
Epoch 340, val loss: 0.7767909169197083
Epoch 350, training loss: 0.13486972451210022 = 0.12746194005012512 + 0.001 * 7.407790660858154
Epoch 350, val loss: 0.7835022211074829
Epoch 360, training loss: 0.11817444860935211 = 0.11076625436544418 + 0.001 * 7.408193588256836
Epoch 360, val loss: 0.7918922901153564
Epoch 370, training loss: 0.10390835255384445 = 0.09651260077953339 + 0.001 * 7.395750045776367
Epoch 370, val loss: 0.8017742037773132
Epoch 380, training loss: 0.091779924929142 = 0.08437701314687729 + 0.001 * 7.402912616729736
Epoch 380, val loss: 0.8128876686096191
Epoch 390, training loss: 0.08144233375787735 = 0.07405534386634827 + 0.001 * 7.386992454528809
Epoch 390, val loss: 0.8249583840370178
Epoch 400, training loss: 0.07265409827232361 = 0.06527376919984818 + 0.001 * 7.3803253173828125
Epoch 400, val loss: 0.8377567529678345
Epoch 410, training loss: 0.06517958641052246 = 0.05778889358043671 + 0.001 * 7.390689849853516
Epoch 410, val loss: 0.8510372042655945
Epoch 420, training loss: 0.05875920504331589 = 0.051388245075941086 + 0.001 * 7.370960712432861
Epoch 420, val loss: 0.8646596074104309
Epoch 430, training loss: 0.053260888904333115 = 0.04589538648724556 + 0.001 * 7.365500450134277
Epoch 430, val loss: 0.8784745931625366
Epoch 440, training loss: 0.0485263466835022 = 0.04116372764110565 + 0.001 * 7.362617492675781
Epoch 440, val loss: 0.89231938123703
Epoch 450, training loss: 0.04445355385541916 = 0.037072014063596725 + 0.001 * 7.381537914276123
Epoch 450, val loss: 0.9060558676719666
Epoch 460, training loss: 0.04087832570075989 = 0.03352188318967819 + 0.001 * 7.356441020965576
Epoch 460, val loss: 0.9196190237998962
Epoch 470, training loss: 0.03779047355055809 = 0.030431440100073814 + 0.001 * 7.359031677246094
Epoch 470, val loss: 0.9329590201377869
Epoch 480, training loss: 0.03509262949228287 = 0.027731528505682945 + 0.001 * 7.361102104187012
Epoch 480, val loss: 0.9460186958312988
Epoch 490, training loss: 0.032719552516937256 = 0.02536453679203987 + 0.001 * 7.355014801025391
Epoch 490, val loss: 0.9587711095809937
Epoch 500, training loss: 0.030627872794866562 = 0.023282857611775398 + 0.001 * 7.345015048980713
Epoch 500, val loss: 0.9711968302726746
Epoch 510, training loss: 0.028789089992642403 = 0.021445052698254585 + 0.001 * 7.344037055969238
Epoch 510, val loss: 0.983260452747345
Epoch 520, training loss: 0.02717081643640995 = 0.01981651410460472 + 0.001 * 7.354301452636719
Epoch 520, val loss: 0.9949696660041809
Epoch 530, training loss: 0.025706440210342407 = 0.01836807280778885 + 0.001 * 7.338367938995361
Epoch 530, val loss: 1.0063278675079346
Epoch 540, training loss: 0.02440785989165306 = 0.017075028270483017 + 0.001 * 7.332831859588623
Epoch 540, val loss: 1.0173633098602295
Epoch 550, training loss: 0.02327328734099865 = 0.015916742384433746 + 0.001 * 7.356544494628906
Epoch 550, val loss: 1.0280786752700806
Epoch 560, training loss: 0.022198989987373352 = 0.014875678345561028 + 0.001 * 7.3233113288879395
Epoch 560, val loss: 1.0384762287139893
Epoch 570, training loss: 0.021273069083690643 = 0.01393667608499527 + 0.001 * 7.336391925811768
Epoch 570, val loss: 1.0485681295394897
Epoch 580, training loss: 0.0204135961830616 = 0.013086950406432152 + 0.001 * 7.326646327972412
Epoch 580, val loss: 1.0583754777908325
Epoch 590, training loss: 0.019629675894975662 = 0.012315741740167141 + 0.001 * 7.313934326171875
Epoch 590, val loss: 1.0679025650024414
Epoch 600, training loss: 0.0189338568598032 = 0.01161352451890707 + 0.001 * 7.3203325271606445
Epoch 600, val loss: 1.0771540403366089
Epoch 610, training loss: 0.01830650120973587 = 0.010971284471452236 + 0.001 * 7.335217475891113
Epoch 610, val loss: 1.0861529111862183
Epoch 620, training loss: 0.01769564114511013 = 0.01038073655217886 + 0.001 * 7.314903736114502
Epoch 620, val loss: 1.0948938131332397
Epoch 630, training loss: 0.01714163087308407 = 0.009833958931267262 + 0.001 * 7.307672023773193
Epoch 630, val loss: 1.1033992767333984
Epoch 640, training loss: 0.016627298668026924 = 0.00932567473500967 + 0.001 * 7.301623344421387
Epoch 640, val loss: 1.1116982698440552
Epoch 650, training loss: 0.016155162826180458 = 0.00885260570794344 + 0.001 * 7.302556991577148
Epoch 650, val loss: 1.1198203563690186
Epoch 660, training loss: 0.01569957658648491 = 0.008412239141762257 + 0.001 * 7.287337303161621
Epoch 660, val loss: 1.127761960029602
Epoch 670, training loss: 0.01529137697070837 = 0.008002457208931446 + 0.001 * 7.288919448852539
Epoch 670, val loss: 1.1355284452438354
Epoch 680, training loss: 0.014913749881088734 = 0.007620995864272118 + 0.001 * 7.29275369644165
Epoch 680, val loss: 1.143122673034668
Epoch 690, training loss: 0.014541566371917725 = 0.007265875115990639 + 0.001 * 7.27569055557251
Epoch 690, val loss: 1.1505451202392578
Epoch 700, training loss: 0.014231224544346333 = 0.006935139186680317 + 0.001 * 7.296084880828857
Epoch 700, val loss: 1.1578106880187988
Epoch 710, training loss: 0.013898780569434166 = 0.006626875139772892 + 0.001 * 7.271905422210693
Epoch 710, val loss: 1.164916753768921
Epoch 720, training loss: 0.01363341324031353 = 0.006339307874441147 + 0.001 * 7.294105052947998
Epoch 720, val loss: 1.1718499660491943
Epoch 730, training loss: 0.013342026621103287 = 0.00607090862467885 + 0.001 * 7.271117687225342
Epoch 730, val loss: 1.1786335706710815
Epoch 740, training loss: 0.013080034404993057 = 0.005820106714963913 + 0.001 * 7.259927272796631
Epoch 740, val loss: 1.1852507591247559
Epoch 750, training loss: 0.012864340096712112 = 0.005585497245192528 + 0.001 * 7.278842926025391
Epoch 750, val loss: 1.1917364597320557
Epoch 760, training loss: 0.012621995061635971 = 0.005365811754018068 + 0.001 * 7.256183624267578
Epoch 760, val loss: 1.1980717182159424
Epoch 770, training loss: 0.012423805892467499 = 0.005159873981028795 + 0.001 * 7.2639312744140625
Epoch 770, val loss: 1.2042820453643799
Epoch 780, training loss: 0.012213249690830708 = 0.004966579377651215 + 0.001 * 7.246669769287109
Epoch 780, val loss: 1.2103466987609863
Epoch 790, training loss: 0.012038258835673332 = 0.004784954711794853 + 0.001 * 7.2533040046691895
Epoch 790, val loss: 1.2162812948226929
Epoch 800, training loss: 0.011874960735440254 = 0.004614136181771755 + 0.001 * 7.260824203491211
Epoch 800, val loss: 1.2220829725265503
Epoch 810, training loss: 0.011715125292539597 = 0.004453233443200588 + 0.001 * 7.261891841888428
Epoch 810, val loss: 1.2277424335479736
Epoch 820, training loss: 0.011534842662513256 = 0.004301552195101976 + 0.001 * 7.233290195465088
Epoch 820, val loss: 1.2333016395568848
Epoch 830, training loss: 0.01139812357723713 = 0.004158380441367626 + 0.001 * 7.239742279052734
Epoch 830, val loss: 1.2387220859527588
Epoch 840, training loss: 0.011252745985984802 = 0.004023111425340176 + 0.001 * 7.2296342849731445
Epoch 840, val loss: 1.244046688079834
Epoch 850, training loss: 0.011124655604362488 = 0.003895186586305499 + 0.001 * 7.22946834564209
Epoch 850, val loss: 1.2492276430130005
Epoch 860, training loss: 0.0109965018928051 = 0.003774042706936598 + 0.001 * 7.222458362579346
Epoch 860, val loss: 1.2543132305145264
Epoch 870, training loss: 0.010920044034719467 = 0.0036592509131878614 + 0.001 * 7.2607927322387695
Epoch 870, val loss: 1.2593002319335938
Epoch 880, training loss: 0.010764247737824917 = 0.0035503653343766928 + 0.001 * 7.213881969451904
Epoch 880, val loss: 1.264191746711731
Epoch 890, training loss: 0.01069232914596796 = 0.003446923103183508 + 0.001 * 7.245405673980713
Epoch 890, val loss: 1.268972635269165
Epoch 900, training loss: 0.010581662878394127 = 0.0033485759049654007 + 0.001 * 7.233086109161377
Epoch 900, val loss: 1.2736876010894775
Epoch 910, training loss: 0.01050950214266777 = 0.0032548632007092237 + 0.001 * 7.254638195037842
Epoch 910, val loss: 1.2783337831497192
Epoch 920, training loss: 0.010400299914181232 = 0.003165388246998191 + 0.001 * 7.23491096496582
Epoch 920, val loss: 1.2828667163848877
Epoch 930, training loss: 0.010307030752301216 = 0.003079821588471532 + 0.001 * 7.227208614349365
Epoch 930, val loss: 1.2873802185058594
Epoch 940, training loss: 0.010189914144575596 = 0.0029978242237120867 + 0.001 * 7.192090034484863
Epoch 940, val loss: 1.2918593883514404
Epoch 950, training loss: 0.010148552246391773 = 0.0029190208297222853 + 0.001 * 7.2295308113098145
Epoch 950, val loss: 1.29630446434021
Epoch 960, training loss: 0.010036449879407883 = 0.002843170426785946 + 0.001 * 7.19327974319458
Epoch 960, val loss: 1.300771951675415
Epoch 970, training loss: 0.009975123219192028 = 0.0027700569480657578 + 0.001 * 7.205065727233887
Epoch 970, val loss: 1.3052057027816772
Epoch 980, training loss: 0.009875824674963951 = 0.0026993965730071068 + 0.001 * 7.176427364349365
Epoch 980, val loss: 1.3096966743469238
Epoch 990, training loss: 0.009816483594477177 = 0.0026310246903449297 + 0.001 * 7.185458660125732
Epoch 990, val loss: 1.3141846656799316
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8044280442804429
The final CL Acc:0.77037, 0.02885, The final GNN Acc:0.80742, 0.00460
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13148])
remove edge: torch.Size([2, 7898])
updated graph: torch.Size([2, 10490])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9599353075027466 = 1.9513384103775024 + 0.001 * 8.59686279296875
Epoch 0, val loss: 1.9536980390548706
Epoch 10, training loss: 1.949570655822754 = 1.9409738779067993 + 0.001 * 8.596823692321777
Epoch 10, val loss: 1.943471908569336
Epoch 20, training loss: 1.9368213415145874 = 1.9282246828079224 + 0.001 * 8.596696853637695
Epoch 20, val loss: 1.930398941040039
Epoch 30, training loss: 1.9192414283752441 = 1.9106450080871582 + 0.001 * 8.596404075622559
Epoch 30, val loss: 1.9118496179580688
Epoch 40, training loss: 1.8937498331069946 = 1.885154128074646 + 0.001 * 8.595688819885254
Epoch 40, val loss: 1.8848562240600586
Epoch 50, training loss: 1.858275055885315 = 1.8496813774108887 + 0.001 * 8.59370231628418
Epoch 50, val loss: 1.8486648797988892
Epoch 60, training loss: 1.8171645402908325 = 1.808577299118042 + 0.001 * 8.587201118469238
Epoch 60, val loss: 1.8108545541763306
Epoch 70, training loss: 1.7810235023498535 = 1.772461175918579 + 0.001 * 8.56234073638916
Epoch 70, val loss: 1.7823675870895386
Epoch 80, training loss: 1.740143060684204 = 1.73171067237854 + 0.001 * 8.432354927062988
Epoch 80, val loss: 1.7479671239852905
Epoch 90, training loss: 1.6818394660949707 = 1.6736977100372314 + 0.001 * 8.14173412322998
Epoch 90, val loss: 1.6967051029205322
Epoch 100, training loss: 1.603803038597107 = 1.5957878828048706 + 0.001 * 8.015130996704102
Epoch 100, val loss: 1.6294474601745605
Epoch 110, training loss: 1.510604977607727 = 1.5026978254318237 + 0.001 * 7.907135486602783
Epoch 110, val loss: 1.5520386695861816
Epoch 120, training loss: 1.4139606952667236 = 1.4062345027923584 + 0.001 * 7.726251602172852
Epoch 120, val loss: 1.4734207391738892
Epoch 130, training loss: 1.3192819356918335 = 1.3116638660430908 + 0.001 * 7.618035316467285
Epoch 130, val loss: 1.398823857307434
Epoch 140, training loss: 1.226359486579895 = 1.2187920808792114 + 0.001 * 7.5674614906311035
Epoch 140, val loss: 1.326526165008545
Epoch 150, training loss: 1.134220838546753 = 1.1267508268356323 + 0.001 * 7.470027923583984
Epoch 150, val loss: 1.254409909248352
Epoch 160, training loss: 1.043515682220459 = 1.0361087322235107 + 0.001 * 7.406987190246582
Epoch 160, val loss: 1.183316707611084
Epoch 170, training loss: 0.9552022814750671 = 0.9478027820587158 + 0.001 * 7.3994903564453125
Epoch 170, val loss: 1.1140223741531372
Epoch 180, training loss: 0.8696174621582031 = 0.8622270822525024 + 0.001 * 7.390364646911621
Epoch 180, val loss: 1.047073483467102
Epoch 190, training loss: 0.7875104546546936 = 0.7801271677017212 + 0.001 * 7.383269786834717
Epoch 190, val loss: 0.9831627607345581
Epoch 200, training loss: 0.7106435894966125 = 0.7032656669616699 + 0.001 * 7.377937316894531
Epoch 200, val loss: 0.9245003461837769
Epoch 210, training loss: 0.6408035755157471 = 0.6334295868873596 + 0.001 * 7.373990058898926
Epoch 210, val loss: 0.8732512593269348
Epoch 220, training loss: 0.5781487226486206 = 0.5707783102989197 + 0.001 * 7.370413303375244
Epoch 220, val loss: 0.8297373652458191
Epoch 230, training loss: 0.5215014219284058 = 0.5141347050666809 + 0.001 * 7.36674165725708
Epoch 230, val loss: 0.79328453540802
Epoch 240, training loss: 0.46939948201179504 = 0.46203652024269104 + 0.001 * 7.362969398498535
Epoch 240, val loss: 0.7625008225440979
Epoch 250, training loss: 0.42063194513320923 = 0.4132727384567261 + 0.001 * 7.359208106994629
Epoch 250, val loss: 0.7361357808113098
Epoch 260, training loss: 0.3745456635951996 = 0.36719006299972534 + 0.001 * 7.355611324310303
Epoch 260, val loss: 0.7139126658439636
Epoch 270, training loss: 0.33107030391693115 = 0.32371777296066284 + 0.001 * 7.35253381729126
Epoch 270, val loss: 0.696029782295227
Epoch 280, training loss: 0.29058995842933655 = 0.28323978185653687 + 0.001 * 7.350162982940674
Epoch 280, val loss: 0.6825330257415771
Epoch 290, training loss: 0.25345250964164734 = 0.24610395729541779 + 0.001 * 7.348543167114258
Epoch 290, val loss: 0.6733453869819641
Epoch 300, training loss: 0.21989263594150543 = 0.2125449776649475 + 0.001 * 7.347653388977051
Epoch 300, val loss: 0.668359637260437
Epoch 310, training loss: 0.1900334656238556 = 0.18268601596355438 + 0.001 * 7.347456455230713
Epoch 310, val loss: 0.6672267317771912
Epoch 320, training loss: 0.1639438420534134 = 0.1565958708524704 + 0.001 * 7.3479719161987305
Epoch 320, val loss: 0.6696082353591919
Epoch 330, training loss: 0.14155755937099457 = 0.1342085599899292 + 0.001 * 7.348991870880127
Epoch 330, val loss: 0.6750603914260864
Epoch 340, training loss: 0.12264654785394669 = 0.1152963787317276 + 0.001 * 7.350165843963623
Epoch 340, val loss: 0.6828876733779907
Epoch 350, training loss: 0.10682383924722672 = 0.0994720533490181 + 0.001 * 7.351783275604248
Epoch 350, val loss: 0.6926224827766418
Epoch 360, training loss: 0.09361483156681061 = 0.08626135438680649 + 0.001 * 7.353479385375977
Epoch 360, val loss: 0.7037064433097839
Epoch 370, training loss: 0.08256874233484268 = 0.07521378993988037 + 0.001 * 7.354952812194824
Epoch 370, val loss: 0.7158032655715942
Epoch 380, training loss: 0.07329700887203217 = 0.06593954563140869 + 0.001 * 7.357460021972656
Epoch 380, val loss: 0.7285007834434509
Epoch 390, training loss: 0.06547316908836365 = 0.05811510235071182 + 0.001 * 7.358069896697998
Epoch 390, val loss: 0.7415124177932739
Epoch 400, training loss: 0.05883993208408356 = 0.051481083035469055 + 0.001 * 7.358846664428711
Epoch 400, val loss: 0.7545645236968994
Epoch 410, training loss: 0.053190842270851135 = 0.04582848772406578 + 0.001 * 7.3623552322387695
Epoch 410, val loss: 0.7675419449806213
Epoch 420, training loss: 0.04834762588143349 = 0.04098781943321228 + 0.001 * 7.359805583953857
Epoch 420, val loss: 0.7803340554237366
Epoch 430, training loss: 0.04418348893523216 = 0.03682320937514305 + 0.001 * 7.3602776527404785
Epoch 430, val loss: 0.7929036021232605
Epoch 440, training loss: 0.04058518633246422 = 0.033224936574697495 + 0.001 * 7.3602495193481445
Epoch 440, val loss: 0.8051896095275879
Epoch 450, training loss: 0.03746124356985092 = 0.030102336779236794 + 0.001 * 7.358906269073486
Epoch 450, val loss: 0.8171483874320984
Epoch 460, training loss: 0.03473775461316109 = 0.027380898594856262 + 0.001 * 7.356855392456055
Epoch 460, val loss: 0.8287516832351685
Epoch 470, training loss: 0.032354503870010376 = 0.02499939501285553 + 0.001 * 7.355109214782715
Epoch 470, val loss: 0.8399948477745056
Epoch 480, training loss: 0.03025890327990055 = 0.022906946018338203 + 0.001 * 7.351956844329834
Epoch 480, val loss: 0.8508954048156738
Epoch 490, training loss: 0.028407445177435875 = 0.021060865372419357 + 0.001 * 7.3465800285339355
Epoch 490, val loss: 0.8614349961280823
Epoch 500, training loss: 0.026767343282699585 = 0.0194235946983099 + 0.001 * 7.343748569488525
Epoch 500, val loss: 0.8715746402740479
Epoch 510, training loss: 0.02529604732990265 = 0.017959605902433395 + 0.001 * 7.336440086364746
Epoch 510, val loss: 0.8814789056777954
Epoch 520, training loss: 0.02397409826517105 = 0.01663857139647007 + 0.001 * 7.335526943206787
Epoch 520, val loss: 0.8911771178245544
Epoch 530, training loss: 0.022765886038541794 = 0.015440735034644604 + 0.001 * 7.325150966644287
Epoch 530, val loss: 0.9007071852684021
Epoch 540, training loss: 0.021659202873706818 = 0.014351623132824898 + 0.001 * 7.307579040527344
Epoch 540, val loss: 0.910182535648346
Epoch 550, training loss: 0.020767079666256905 = 0.01336185447871685 + 0.001 * 7.405224323272705
Epoch 550, val loss: 0.9195091724395752
Epoch 560, training loss: 0.019784055650234222 = 0.012462789192795753 + 0.001 * 7.32126522064209
Epoch 560, val loss: 0.9286372661590576
Epoch 570, training loss: 0.018958643078804016 = 0.011645115911960602 + 0.001 * 7.313527584075928
Epoch 570, val loss: 0.9375587105751038
Epoch 580, training loss: 0.01819944754242897 = 0.010900899767875671 + 0.001 * 7.298547267913818
Epoch 580, val loss: 0.9462987780570984
Epoch 590, training loss: 0.017520859837532043 = 0.010222889482975006 + 0.001 * 7.297969818115234
Epoch 590, val loss: 0.9548549056053162
Epoch 600, training loss: 0.01692267879843712 = 0.009605142287909985 + 0.001 * 7.317536354064941
Epoch 600, val loss: 0.9631554484367371
Epoch 610, training loss: 0.016329379752278328 = 0.009041561745107174 + 0.001 * 7.287817478179932
Epoch 610, val loss: 0.9712529182434082
Epoch 620, training loss: 0.015796739608049393 = 0.008526599034667015 + 0.001 * 7.270140647888184
Epoch 620, val loss: 0.9791383147239685
Epoch 630, training loss: 0.015316857025027275 = 0.008055341430008411 + 0.001 * 7.261515140533447
Epoch 630, val loss: 0.9867804646492004
Epoch 640, training loss: 0.014869541861116886 = 0.007623189594596624 + 0.001 * 7.246351718902588
Epoch 640, val loss: 0.9942235946655273
Epoch 650, training loss: 0.01451713778078556 = 0.007226136513054371 + 0.001 * 7.2910003662109375
Epoch 650, val loss: 1.0014703273773193
Epoch 660, training loss: 0.01411021314561367 = 0.00686053978279233 + 0.001 * 7.2496724128723145
Epoch 660, val loss: 1.008540391921997
Epoch 670, training loss: 0.013749664649367332 = 0.006523333489894867 + 0.001 * 7.226330280303955
Epoch 670, val loss: 1.0154058933258057
Epoch 680, training loss: 0.013457593508064747 = 0.006211692932993174 + 0.001 * 7.2459001541137695
Epoch 680, val loss: 1.0220811367034912
Epoch 690, training loss: 0.013140033930540085 = 0.005922831129282713 + 0.001 * 7.217202186584473
Epoch 690, val loss: 1.0285900831222534
Epoch 700, training loss: 0.01285938173532486 = 0.005654559470713139 + 0.001 * 7.204822540283203
Epoch 700, val loss: 1.0349318981170654
Epoch 710, training loss: 0.012689976021647453 = 0.005404718220233917 + 0.001 * 7.285257816314697
Epoch 710, val loss: 1.0411324501037598
Epoch 720, training loss: 0.012366749346256256 = 0.005171730648726225 + 0.001 * 7.195018768310547
Epoch 720, val loss: 1.0471965074539185
Epoch 730, training loss: 0.012153130024671555 = 0.0049538519233465195 + 0.001 * 7.199277877807617
Epoch 730, val loss: 1.053106427192688
Epoch 740, training loss: 0.011927837505936623 = 0.004749760963022709 + 0.001 * 7.178076267242432
Epoch 740, val loss: 1.0588805675506592
Epoch 750, training loss: 0.011802936904132366 = 0.004558132495731115 + 0.001 * 7.2448039054870605
Epoch 750, val loss: 1.0645434856414795
Epoch 760, training loss: 0.011587748304009438 = 0.004378038924187422 + 0.001 * 7.209709167480469
Epoch 760, val loss: 1.0700863599777222
Epoch 770, training loss: 0.011399934068322182 = 0.0042090569622814655 + 0.001 * 7.190876483917236
Epoch 770, val loss: 1.0754776000976562
Epoch 780, training loss: 0.011244485154747963 = 0.004050129093229771 + 0.001 * 7.194355487823486
Epoch 780, val loss: 1.0807729959487915
Epoch 790, training loss: 0.01107903104275465 = 0.0039005151484161615 + 0.001 * 7.178515434265137
Epoch 790, val loss: 1.0859357118606567
Epoch 800, training loss: 0.010919256135821342 = 0.0037594058085232973 + 0.001 * 7.159850120544434
Epoch 800, val loss: 1.0909725427627563
Epoch 810, training loss: 0.010788023471832275 = 0.0036261528730392456 + 0.001 * 7.161870002746582
Epoch 810, val loss: 1.0959326028823853
Epoch 820, training loss: 0.010640610009431839 = 0.003499570768326521 + 0.001 * 7.14103889465332
Epoch 820, val loss: 1.100788950920105
Epoch 830, training loss: 0.010550114326179028 = 0.0033797610085457563 + 0.001 * 7.170352935791016
Epoch 830, val loss: 1.1055032014846802
Epoch 840, training loss: 0.010422132909297943 = 0.0032658306881785393 + 0.001 * 7.156302452087402
Epoch 840, val loss: 1.1101689338684082
Epoch 850, training loss: 0.010303983464837074 = 0.0031573346350342035 + 0.001 * 7.14664888381958
Epoch 850, val loss: 1.1147701740264893
Epoch 860, training loss: 0.010197707451879978 = 0.0030539219733327627 + 0.001 * 7.14378547668457
Epoch 860, val loss: 1.1192880868911743
Epoch 870, training loss: 0.01008294802159071 = 0.0029551894403994083 + 0.001 * 7.127758026123047
Epoch 870, val loss: 1.1237208843231201
Epoch 880, training loss: 0.010007919743657112 = 0.0028611309826374054 + 0.001 * 7.146788597106934
Epoch 880, val loss: 1.1280633211135864
Epoch 890, training loss: 0.009920371696352959 = 0.0027716660406440496 + 0.001 * 7.148705005645752
Epoch 890, val loss: 1.1323490142822266
Epoch 900, training loss: 0.009838985279202461 = 0.0026865361724048853 + 0.001 * 7.152449131011963
Epoch 900, val loss: 1.1365511417388916
Epoch 910, training loss: 0.009750459343194962 = 0.002605163725093007 + 0.001 * 7.1452956199646
Epoch 910, val loss: 1.1406854391098022
Epoch 920, training loss: 0.009648241102695465 = 0.002527463249862194 + 0.001 * 7.120777606964111
Epoch 920, val loss: 1.1447643041610718
Epoch 930, training loss: 0.009577636606991291 = 0.002453429391607642 + 0.001 * 7.12420654296875
Epoch 930, val loss: 1.1487170457839966
Epoch 940, training loss: 0.009498736821115017 = 0.002382551785558462 + 0.001 * 7.116184711456299
Epoch 940, val loss: 1.1526494026184082
Epoch 950, training loss: 0.009465944021940231 = 0.002314828336238861 + 0.001 * 7.151115417480469
Epoch 950, val loss: 1.156495451927185
Epoch 960, training loss: 0.009367993101477623 = 0.00225002970546484 + 0.001 * 7.117962837219238
Epoch 960, val loss: 1.1602295637130737
Epoch 970, training loss: 0.009283813647925854 = 0.0021881551947444677 + 0.001 * 7.095658302307129
Epoch 970, val loss: 1.1639518737792969
Epoch 980, training loss: 0.009263593703508377 = 0.002128986641764641 + 0.001 * 7.134606838226318
Epoch 980, val loss: 1.1675796508789062
Epoch 990, training loss: 0.009207085706293583 = 0.002072359202429652 + 0.001 * 7.134726524353027
Epoch 990, val loss: 1.171166181564331
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8429098576700054
=== training gcn model ===
Epoch 0, training loss: 1.9528021812438965 = 1.9442052841186523 + 0.001 * 8.59685230255127
Epoch 0, val loss: 1.9412224292755127
Epoch 10, training loss: 1.9425233602523804 = 1.9339265823364258 + 0.001 * 8.596810340881348
Epoch 10, val loss: 1.9310047626495361
Epoch 20, training loss: 1.9298967123031616 = 1.9213000535964966 + 0.001 * 8.596660614013672
Epoch 20, val loss: 1.9179036617279053
Epoch 30, training loss: 1.9123311042785645 = 1.903734803199768 + 0.001 * 8.596327781677246
Epoch 30, val loss: 1.8994117975234985
Epoch 40, training loss: 1.8868452310562134 = 1.8782497644424438 + 0.001 * 8.595494270324707
Epoch 40, val loss: 1.8727277517318726
Epoch 50, training loss: 1.851624846458435 = 1.8430317640304565 + 0.001 * 8.593140602111816
Epoch 50, val loss: 1.83740234375
Epoch 60, training loss: 1.811659812927246 = 1.8030747175216675 + 0.001 * 8.585038185119629
Epoch 60, val loss: 1.801277756690979
Epoch 70, training loss: 1.7751421928405762 = 1.7665905952453613 + 0.001 * 8.551575660705566
Epoch 70, val loss: 1.771985650062561
Epoch 80, training loss: 1.7306386232376099 = 1.7222784757614136 + 0.001 * 8.36019229888916
Epoch 80, val loss: 1.7339621782302856
Epoch 90, training loss: 1.66791832447052 = 1.6597471237182617 + 0.001 * 8.171154975891113
Epoch 90, val loss: 1.6782124042510986
Epoch 100, training loss: 1.5839065313339233 = 1.5758720636367798 + 0.001 * 8.034419059753418
Epoch 100, val loss: 1.605339765548706
Epoch 110, training loss: 1.4830151796340942 = 1.4751677513122559 + 0.001 * 7.84744119644165
Epoch 110, val loss: 1.5193063020706177
Epoch 120, training loss: 1.3761197328567505 = 1.3685721158981323 + 0.001 * 7.547565937042236
Epoch 120, val loss: 1.4305757284164429
Epoch 130, training loss: 1.2700705528259277 = 1.2625757455825806 + 0.001 * 7.4948344230651855
Epoch 130, val loss: 1.3438223600387573
Epoch 140, training loss: 1.1676887273788452 = 1.160248041152954 + 0.001 * 7.440700531005859
Epoch 140, val loss: 1.2620868682861328
Epoch 150, training loss: 1.0717815160751343 = 1.0643553733825684 + 0.001 * 7.426089763641357
Epoch 150, val loss: 1.1859478950500488
Epoch 160, training loss: 0.9834461212158203 = 0.9760322570800781 + 0.001 * 7.413863182067871
Epoch 160, val loss: 1.1163846254348755
Epoch 170, training loss: 0.9016710519790649 = 0.8942632079124451 + 0.001 * 7.407844543457031
Epoch 170, val loss: 1.0517001152038574
Epoch 180, training loss: 0.8247185945510864 = 0.8173165917396545 + 0.001 * 7.4020256996154785
Epoch 180, val loss: 0.9902742505073547
Epoch 190, training loss: 0.7521020770072937 = 0.7447062134742737 + 0.001 * 7.395887851715088
Epoch 190, val loss: 0.9324845671653748
Epoch 200, training loss: 0.6845774054527283 = 0.6771878600120544 + 0.001 * 7.389543533325195
Epoch 200, val loss: 0.8800008296966553
Epoch 210, training loss: 0.6228477358818054 = 0.6154653429985046 + 0.001 * 7.3823723793029785
Epoch 210, val loss: 0.8345052003860474
Epoch 220, training loss: 0.5664985179901123 = 0.5591245889663696 + 0.001 * 7.373953819274902
Epoch 220, val loss: 0.7959532141685486
Epoch 230, training loss: 0.5146865844726562 = 0.5073226690292358 + 0.001 * 7.363888263702393
Epoch 230, val loss: 0.7634912133216858
Epoch 240, training loss: 0.46638771891593933 = 0.45903611183166504 + 0.001 * 7.351596355438232
Epoch 240, val loss: 0.7357817888259888
Epoch 250, training loss: 0.42085620760917664 = 0.41352027654647827 + 0.001 * 7.335930347442627
Epoch 250, val loss: 0.7120499610900879
Epoch 260, training loss: 0.37809374928474426 = 0.3707759976387024 + 0.001 * 7.317757606506348
Epoch 260, val loss: 0.6920515894889832
Epoch 270, training loss: 0.33875977993011475 = 0.3314623534679413 + 0.001 * 7.297432899475098
Epoch 270, val loss: 0.675754964351654
Epoch 280, training loss: 0.3035058379173279 = 0.29621338844299316 + 0.001 * 7.292434215545654
Epoch 280, val loss: 0.6628325581550598
Epoch 290, training loss: 0.27225324511528015 = 0.2649848461151123 + 0.001 * 7.268413066864014
Epoch 290, val loss: 0.6527906656265259
Epoch 300, training loss: 0.24416090548038483 = 0.23689396679401398 + 0.001 * 7.266942024230957
Epoch 300, val loss: 0.6446385979652405
Epoch 310, training loss: 0.21796557307243347 = 0.21070024371147156 + 0.001 * 7.265325546264648
Epoch 310, val loss: 0.6376509070396423
Epoch 320, training loss: 0.19269631803035736 = 0.18543022871017456 + 0.001 * 7.266088008880615
Epoch 320, val loss: 0.6312700510025024
Epoch 330, training loss: 0.16818398237228394 = 0.16091616451740265 + 0.001 * 7.267814636230469
Epoch 330, val loss: 0.6254991888999939
Epoch 340, training loss: 0.14515994489192963 = 0.13789087533950806 + 0.001 * 7.269064426422119
Epoch 340, val loss: 0.6211459636688232
Epoch 350, training loss: 0.12465739995241165 = 0.11738722771406174 + 0.001 * 7.270168781280518
Epoch 350, val loss: 0.6190211772918701
Epoch 360, training loss: 0.10720216482877731 = 0.09993106126785278 + 0.001 * 7.271101951599121
Epoch 360, val loss: 0.6197165846824646
Epoch 370, training loss: 0.09269094467163086 = 0.08541902154684067 + 0.001 * 7.271921157836914
Epoch 370, val loss: 0.6230131387710571
Epoch 380, training loss: 0.08067621290683746 = 0.07340355962514877 + 0.001 * 7.272649765014648
Epoch 380, val loss: 0.6284298300743103
Epoch 390, training loss: 0.07067247480154037 = 0.06339848041534424 + 0.001 * 7.273991107940674
Epoch 390, val loss: 0.63528972864151
Epoch 400, training loss: 0.062277790158987045 = 0.05500080808997154 + 0.001 * 7.27698278427124
Epoch 400, val loss: 0.6430830359458923
Epoch 410, training loss: 0.05521995574235916 = 0.04794475808739662 + 0.001 * 7.275198459625244
Epoch 410, val loss: 0.6515474319458008
Epoch 420, training loss: 0.04930544272065163 = 0.04203034192323685 + 0.001 * 7.275099277496338
Epoch 420, val loss: 0.6603825092315674
Epoch 430, training loss: 0.04434890300035477 = 0.037073567509651184 + 0.001 * 7.275337219238281
Epoch 430, val loss: 0.6695019602775574
Epoch 440, training loss: 0.04018927365541458 = 0.0329134464263916 + 0.001 * 7.275828838348389
Epoch 440, val loss: 0.6787304878234863
Epoch 450, training loss: 0.03668147325515747 = 0.02940523251891136 + 0.001 * 7.276242256164551
Epoch 450, val loss: 0.6879702210426331
Epoch 460, training loss: 0.033706072717905045 = 0.02643083781003952 + 0.001 * 7.275235176086426
Epoch 460, val loss: 0.697176992893219
Epoch 470, training loss: 0.03116742894053459 = 0.02389262057840824 + 0.001 * 7.274807929992676
Epoch 470, val loss: 0.7062604427337646
Epoch 480, training loss: 0.028987878933548927 = 0.02171242982149124 + 0.001 * 7.275448322296143
Epoch 480, val loss: 0.7152122855186462
Epoch 490, training loss: 0.027100015431642532 = 0.019827499985694885 + 0.001 * 7.272515773773193
Epoch 490, val loss: 0.7239851951599121
Epoch 500, training loss: 0.025459807366132736 = 0.01818791776895523 + 0.001 * 7.271888256072998
Epoch 500, val loss: 0.7325358986854553
Epoch 510, training loss: 0.024026675149798393 = 0.01675287075340748 + 0.001 * 7.2738037109375
Epoch 510, val loss: 0.7408397793769836
Epoch 520, training loss: 0.022761695086956024 = 0.015489839017391205 + 0.001 * 7.271856307983398
Epoch 520, val loss: 0.7489389777183533
Epoch 530, training loss: 0.021642126142978668 = 0.014372250065207481 + 0.001 * 7.269874572753906
Epoch 530, val loss: 0.7568151950836182
Epoch 540, training loss: 0.02064773440361023 = 0.013378430157899857 + 0.001 * 7.269303321838379
Epoch 540, val loss: 0.7644647359848022
Epoch 550, training loss: 0.019757872447371483 = 0.012490611523389816 + 0.001 * 7.267260551452637
Epoch 550, val loss: 0.7718662619590759
Epoch 560, training loss: 0.018957510590553284 = 0.011694050393998623 + 0.001 * 7.263460636138916
Epoch 560, val loss: 0.7790921926498413
Epoch 570, training loss: 0.018240004777908325 = 0.010976515710353851 + 0.001 * 7.26348876953125
Epoch 570, val loss: 0.7860809564590454
Epoch 580, training loss: 0.01759328320622444 = 0.010327835567295551 + 0.001 * 7.265447616577148
Epoch 580, val loss: 0.7928846478462219
Epoch 590, training loss: 0.017000261694192886 = 0.009739389643073082 + 0.001 * 7.260871887207031
Epoch 590, val loss: 0.7994869947433472
Epoch 600, training loss: 0.016460344195365906 = 0.009203762747347355 + 0.001 * 7.256581783294678
Epoch 600, val loss: 0.8058976531028748
Epoch 610, training loss: 0.015975210815668106 = 0.008714688010513783 + 0.001 * 7.26052188873291
Epoch 610, val loss: 0.8121464848518372
Epoch 620, training loss: 0.015521543100476265 = 0.008266880176961422 + 0.001 * 7.254662036895752
Epoch 620, val loss: 0.8182112574577332
Epoch 630, training loss: 0.01511484757065773 = 0.007855723612010479 + 0.001 * 7.259123802185059
Epoch 630, val loss: 0.8241006135940552
Epoch 640, training loss: 0.014718998223543167 = 0.007477217353880405 + 0.001 * 7.241780757904053
Epoch 640, val loss: 0.829851508140564
Epoch 650, training loss: 0.014373973943293095 = 0.0071279387921094894 + 0.001 * 7.246034622192383
Epoch 650, val loss: 0.8354296088218689
Epoch 660, training loss: 0.014063123613595963 = 0.006804923061281443 + 0.001 * 7.258200168609619
Epoch 660, val loss: 0.8408761620521545
Epoch 670, training loss: 0.013742592185735703 = 0.006505544763058424 + 0.001 * 7.2370476722717285
Epoch 670, val loss: 0.846190869808197
Epoch 680, training loss: 0.013502584770321846 = 0.006227499805390835 + 0.001 * 7.275084018707275
Epoch 680, val loss: 0.8513677716255188
Epoch 690, training loss: 0.013199364766478539 = 0.005968754179775715 + 0.001 * 7.2306108474731445
Epoch 690, val loss: 0.8564205169677734
Epoch 700, training loss: 0.01303020678460598 = 0.005727561190724373 + 0.001 * 7.302644729614258
Epoch 700, val loss: 0.8613443374633789
Epoch 710, training loss: 0.012743698433041573 = 0.005502319894731045 + 0.001 * 7.241378307342529
Epoch 710, val loss: 0.8661516904830933
Epoch 720, training loss: 0.012506311759352684 = 0.005291621200740337 + 0.001 * 7.214690685272217
Epoch 720, val loss: 0.8708392381668091
Epoch 730, training loss: 0.01233571209013462 = 0.005094233900308609 + 0.001 * 7.2414774894714355
Epoch 730, val loss: 0.8754122257232666
Epoch 740, training loss: 0.012147951871156693 = 0.004909041337668896 + 0.001 * 7.23891019821167
Epoch 740, val loss: 0.8798726201057434
Epoch 750, training loss: 0.011955242604017258 = 0.004735050722956657 + 0.001 * 7.22019100189209
Epoch 750, val loss: 0.8842424154281616
Epoch 760, training loss: 0.011789584532380104 = 0.004571334924548864 + 0.001 * 7.218249320983887
Epoch 760, val loss: 0.8885080218315125
Epoch 770, training loss: 0.011606553569436073 = 0.004417101386934519 + 0.001 * 7.189451694488525
Epoch 770, val loss: 0.8926723599433899
Epoch 780, training loss: 0.011461812071502209 = 0.004271598532795906 + 0.001 * 7.190213203430176
Epoch 780, val loss: 0.8967397212982178
Epoch 790, training loss: 0.011329686269164085 = 0.004134166054427624 + 0.001 * 7.195519924163818
Epoch 790, val loss: 0.9007371068000793
Epoch 800, training loss: 0.01119900494813919 = 0.004004189744591713 + 0.001 * 7.194814205169678
Epoch 800, val loss: 0.904640793800354
Epoch 810, training loss: 0.01109655387699604 = 0.0038811243139207363 + 0.001 * 7.215429306030273
Epoch 810, val loss: 0.9084621071815491
Epoch 820, training loss: 0.01092664897441864 = 0.003764448221772909 + 0.001 * 7.162199974060059
Epoch 820, val loss: 0.9121981859207153
Epoch 830, training loss: 0.010868881829082966 = 0.0036537402775138617 + 0.001 * 7.2151408195495605
Epoch 830, val loss: 0.9158700704574585
Epoch 840, training loss: 0.010722707957029343 = 0.0035485615953803062 + 0.001 * 7.17414665222168
Epoch 840, val loss: 0.9194596409797668
Epoch 850, training loss: 0.01060725748538971 = 0.003448524046689272 + 0.001 * 7.1587324142456055
Epoch 850, val loss: 0.9229902029037476
Epoch 860, training loss: 0.010526563972234726 = 0.0033532504457980394 + 0.001 * 7.173313617706299
Epoch 860, val loss: 0.9264664649963379
Epoch 870, training loss: 0.01042869221419096 = 0.0032624441664665937 + 0.001 * 7.166247844696045
Epoch 870, val loss: 0.9298669695854187
Epoch 880, training loss: 0.010370058007538319 = 0.003175825346261263 + 0.001 * 7.19423246383667
Epoch 880, val loss: 0.933193564414978
Epoch 890, training loss: 0.010273193940520287 = 0.003093088511377573 + 0.001 * 7.180105686187744
Epoch 890, val loss: 0.9364907145500183
Epoch 900, training loss: 0.010159452445805073 = 0.0030140087474137545 + 0.001 * 7.145442962646484
Epoch 900, val loss: 0.9397163391113281
Epoch 910, training loss: 0.01011616550385952 = 0.0029383660294115543 + 0.001 * 7.177799224853516
Epoch 910, val loss: 0.9428673386573792
Epoch 920, training loss: 0.010064153000712395 = 0.002865957096219063 + 0.001 * 7.198194980621338
Epoch 920, val loss: 0.9459837675094604
Epoch 930, training loss: 0.009938632138073444 = 0.002796522108837962 + 0.001 * 7.1421098709106445
Epoch 930, val loss: 0.9490448236465454
Epoch 940, training loss: 0.009862245060503483 = 0.002729862229898572 + 0.001 * 7.132382392883301
Epoch 940, val loss: 0.9520673751831055
Epoch 950, training loss: 0.009856138378381729 = 0.002665827516466379 + 0.001 * 7.190310001373291
Epoch 950, val loss: 0.955042839050293
Epoch 960, training loss: 0.009752387180924416 = 0.0026044417172670364 + 0.001 * 7.147945404052734
Epoch 960, val loss: 0.9579840898513794
Epoch 970, training loss: 0.009658033028244972 = 0.002545522293075919 + 0.001 * 7.1125102043151855
Epoch 970, val loss: 0.9608588218688965
Epoch 980, training loss: 0.009611023589968681 = 0.0024889346677809954 + 0.001 * 7.12208890914917
Epoch 980, val loss: 0.9636968374252319
Epoch 990, training loss: 0.009563169442117214 = 0.0024345326237380505 + 0.001 * 7.128636360168457
Epoch 990, val loss: 0.9664846062660217
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 1.9535733461380005 = 1.944976568222046 + 0.001 * 8.596817016601562
Epoch 0, val loss: 1.939346194267273
Epoch 10, training loss: 1.94382905960083 = 1.9352322816848755 + 0.001 * 8.596758842468262
Epoch 10, val loss: 1.9304786920547485
Epoch 20, training loss: 1.9320663213729858 = 1.9234697818756104 + 0.001 * 8.59653377532959
Epoch 20, val loss: 1.9193185567855835
Epoch 30, training loss: 1.9159624576568604 = 1.9073665142059326 + 0.001 * 8.595967292785645
Epoch 30, val loss: 1.903752088546753
Epoch 40, training loss: 1.8925825357437134 = 1.8839880228042603 + 0.001 * 8.59449291229248
Epoch 40, val loss: 1.8812803030014038
Epoch 50, training loss: 1.8593980073928833 = 1.8508081436157227 + 0.001 * 8.589871406555176
Epoch 50, val loss: 1.8506760597229004
Epoch 60, training loss: 1.8189849853515625 = 1.8104139566421509 + 0.001 * 8.571084022521973
Epoch 60, val loss: 1.8160648345947266
Epoch 70, training loss: 1.7783634662628174 = 1.7698920965194702 + 0.001 * 8.47140121459961
Epoch 70, val loss: 1.7830158472061157
Epoch 80, training loss: 1.731290578842163 = 1.7230440378189087 + 0.001 * 8.246488571166992
Epoch 80, val loss: 1.7404661178588867
Epoch 90, training loss: 1.6671409606933594 = 1.659165859222412 + 0.001 * 7.975096702575684
Epoch 90, val loss: 1.6808875799179077
Epoch 100, training loss: 1.581826090812683 = 1.5741387605667114 + 0.001 * 7.687376022338867
Epoch 100, val loss: 1.605649709701538
Epoch 110, training loss: 1.4797019958496094 = 1.4721912145614624 + 0.001 * 7.510775089263916
Epoch 110, val loss: 1.5193729400634766
Epoch 120, training loss: 1.371395468711853 = 1.3639485836029053 + 0.001 * 7.446855068206787
Epoch 120, val loss: 1.4313938617706299
Epoch 130, training loss: 1.2640674114227295 = 1.256692886352539 + 0.001 * 7.374492645263672
Epoch 130, val loss: 1.3473386764526367
Epoch 140, training loss: 1.16248619556427 = 1.1551648378372192 + 0.001 * 7.3213725090026855
Epoch 140, val loss: 1.2703481912612915
Epoch 150, training loss: 1.069526195526123 = 1.062231421470642 + 0.001 * 7.294803619384766
Epoch 150, val loss: 1.201261281967163
Epoch 160, training loss: 0.9862031936645508 = 0.9789287447929382 + 0.001 * 7.274454116821289
Epoch 160, val loss: 1.1400277614593506
Epoch 170, training loss: 0.9112768769264221 = 0.9040216207504272 + 0.001 * 7.255261421203613
Epoch 170, val loss: 1.0848997831344604
Epoch 180, training loss: 0.8424956798553467 = 0.8352519273757935 + 0.001 * 7.243723392486572
Epoch 180, val loss: 1.0344700813293457
Epoch 190, training loss: 0.7781001925468445 = 0.7708574533462524 + 0.001 * 7.2427496910095215
Epoch 190, val loss: 0.9873859286308289
Epoch 200, training loss: 0.7173860669136047 = 0.7101458311080933 + 0.001 * 7.240234375
Epoch 200, val loss: 0.9441882967948914
Epoch 210, training loss: 0.6600214838981628 = 0.65278160572052 + 0.001 * 7.239903926849365
Epoch 210, val loss: 0.9055858254432678
Epoch 220, training loss: 0.605501115322113 = 0.5982607007026672 + 0.001 * 7.240396022796631
Epoch 220, val loss: 0.8718215227127075
Epoch 230, training loss: 0.5531899333000183 = 0.5459486842155457 + 0.001 * 7.241222381591797
Epoch 230, val loss: 0.8424512147903442
Epoch 240, training loss: 0.5026738047599792 = 0.4954316020011902 + 0.001 * 7.242211818695068
Epoch 240, val loss: 0.8167513608932495
Epoch 250, training loss: 0.45380643010139465 = 0.44656333327293396 + 0.001 * 7.243110656738281
Epoch 250, val loss: 0.7944466471672058
Epoch 260, training loss: 0.4067266285419464 = 0.39948272705078125 + 0.001 * 7.243902683258057
Epoch 260, val loss: 0.7752734422683716
Epoch 270, training loss: 0.36178648471832275 = 0.3545420169830322 + 0.001 * 7.244474411010742
Epoch 270, val loss: 0.7594975829124451
Epoch 280, training loss: 0.31956133246421814 = 0.3123165965080261 + 0.001 * 7.244729042053223
Epoch 280, val loss: 0.7470890879631042
Epoch 290, training loss: 0.2807668447494507 = 0.2735222578048706 + 0.001 * 7.244571685791016
Epoch 290, val loss: 0.7379732728004456
Epoch 300, training loss: 0.2459314614534378 = 0.23868761956691742 + 0.001 * 7.243838310241699
Epoch 300, val loss: 0.7319108247756958
Epoch 310, training loss: 0.21522541344165802 = 0.20797982811927795 + 0.001 * 7.2455830574035645
Epoch 310, val loss: 0.7287266254425049
Epoch 320, training loss: 0.18843090534210205 = 0.18118974566459656 + 0.001 * 7.241166114807129
Epoch 320, val loss: 0.728164553642273
Epoch 330, training loss: 0.16517871618270874 = 0.15794096887111664 + 0.001 * 7.237748622894287
Epoch 330, val loss: 0.7300050854682922
Epoch 340, training loss: 0.14504267275333405 = 0.13780967891216278 + 0.001 * 7.232995986938477
Epoch 340, val loss: 0.7340174317359924
Epoch 350, training loss: 0.12764093279838562 = 0.12041357904672623 + 0.001 * 7.22735595703125
Epoch 350, val loss: 0.7399371862411499
Epoch 360, training loss: 0.11261855065822601 = 0.10539279133081436 + 0.001 * 7.225759506225586
Epoch 360, val loss: 0.7476404309272766
Epoch 370, training loss: 0.09963762760162354 = 0.09242807328701019 + 0.001 * 7.209551811218262
Epoch 370, val loss: 0.7569146156311035
Epoch 380, training loss: 0.08843619376420975 = 0.08123765140771866 + 0.001 * 7.198543548583984
Epoch 380, val loss: 0.7673563361167908
Epoch 390, training loss: 0.07877497375011444 = 0.07157782465219498 + 0.001 * 7.19714879989624
Epoch 390, val loss: 0.7787400484085083
Epoch 400, training loss: 0.07042655348777771 = 0.06324709206819534 + 0.001 * 7.179459571838379
Epoch 400, val loss: 0.790808379650116
Epoch 410, training loss: 0.06324440985918045 = 0.05607035756111145 + 0.001 * 7.1740546226501465
Epoch 410, val loss: 0.8033371567726135
Epoch 420, training loss: 0.0570538155734539 = 0.049890246242284775 + 0.001 * 7.163570404052734
Epoch 420, val loss: 0.8160209059715271
Epoch 430, training loss: 0.05172240734100342 = 0.044563595205545425 + 0.001 * 7.158811569213867
Epoch 430, val loss: 0.8287185430526733
Epoch 440, training loss: 0.04711630567908287 = 0.03996306285262108 + 0.001 * 7.153242111206055
Epoch 440, val loss: 0.841232180595398
Epoch 450, training loss: 0.04312331974506378 = 0.03597865626215935 + 0.001 * 7.1446614265441895
Epoch 450, val loss: 0.8534783720970154
Epoch 460, training loss: 0.039662934839725494 = 0.03251850605010986 + 0.001 * 7.14442777633667
Epoch 460, val loss: 0.8653829097747803
Epoch 470, training loss: 0.036669496446847916 = 0.029503233730793 + 0.001 * 7.166263580322266
Epoch 470, val loss: 0.8769875168800354
Epoch 480, training loss: 0.03400793299078941 = 0.026862338185310364 + 0.001 * 7.145593643188477
Epoch 480, val loss: 0.8881908059120178
Epoch 490, training loss: 0.031667135655879974 = 0.024536678567528725 + 0.001 * 7.13045597076416
Epoch 490, val loss: 0.8990369439125061
Epoch 500, training loss: 0.02960881032049656 = 0.022479720413684845 + 0.001 * 7.12908935546875
Epoch 500, val loss: 0.909517228603363
Epoch 510, training loss: 0.02779928222298622 = 0.020654380321502686 + 0.001 * 7.144901752471924
Epoch 510, val loss: 0.9196303486824036
Epoch 520, training loss: 0.02615613304078579 = 0.0190300103276968 + 0.001 * 7.126121997833252
Epoch 520, val loss: 0.9293894171714783
Epoch 530, training loss: 0.02472112514078617 = 0.017580410465598106 + 0.001 * 7.140714168548584
Epoch 530, val loss: 0.938762366771698
Epoch 540, training loss: 0.0234050415456295 = 0.016283731907606125 + 0.001 * 7.121309757232666
Epoch 540, val loss: 0.947809100151062
Epoch 550, training loss: 0.0222353246062994 = 0.015121163800358772 + 0.001 * 7.114161014556885
Epoch 550, val loss: 0.9564937949180603
Epoch 560, training loss: 0.021195128560066223 = 0.014076419174671173 + 0.001 * 7.118709087371826
Epoch 560, val loss: 0.9648525714874268
Epoch 570, training loss: 0.020259596407413483 = 0.01313527300953865 + 0.001 * 7.124321937561035
Epoch 570, val loss: 0.9728839993476868
Epoch 580, training loss: 0.019393283873796463 = 0.012285146862268448 + 0.001 * 7.10813570022583
Epoch 580, val loss: 0.980669379234314
Epoch 590, training loss: 0.0186329185962677 = 0.011515215039253235 + 0.001 * 7.117703914642334
Epoch 590, val loss: 0.9882033467292786
Epoch 600, training loss: 0.017929425463080406 = 0.010816246271133423 + 0.001 * 7.113178730010986
Epoch 600, val loss: 0.9954544901847839
Epoch 610, training loss: 0.017284931614995003 = 0.010180310346186161 + 0.001 * 7.104620456695557
Epoch 610, val loss: 1.0024707317352295
Epoch 620, training loss: 0.016705505549907684 = 0.009600291959941387 + 0.001 * 7.105213165283203
Epoch 620, val loss: 1.009247064590454
Epoch 630, training loss: 0.01616603136062622 = 0.009070088155567646 + 0.001 * 7.095943927764893
Epoch 630, val loss: 1.0158137083053589
Epoch 640, training loss: 0.01572958193719387 = 0.008584307506680489 + 0.001 * 7.1452741622924805
Epoch 640, val loss: 1.0221787691116333
Epoch 650, training loss: 0.015236804261803627 = 0.00813835859298706 + 0.001 * 7.098444938659668
Epoch 650, val loss: 1.0283008813858032
Epoch 660, training loss: 0.014828650280833244 = 0.007728172931820154 + 0.001 * 7.100477695465088
Epoch 660, val loss: 1.034239649772644
Epoch 670, training loss: 0.014451976865530014 = 0.007349985186010599 + 0.001 * 7.101990699768066
Epoch 670, val loss: 1.0400328636169434
Epoch 680, training loss: 0.014108166098594666 = 0.007000591605901718 + 0.001 * 7.107574462890625
Epoch 680, val loss: 1.045661211013794
Epoch 690, training loss: 0.013764100149273872 = 0.006677138619124889 + 0.001 * 7.08696174621582
Epoch 690, val loss: 1.0511224269866943
Epoch 700, training loss: 0.013480433262884617 = 0.006377214565873146 + 0.001 * 7.1032185554504395
Epoch 700, val loss: 1.0564311742782593
Epoch 710, training loss: 0.01319124922156334 = 0.006098642945289612 + 0.001 * 7.092606067657471
Epoch 710, val loss: 1.0615798234939575
Epoch 720, training loss: 0.012922607362270355 = 0.005839401390403509 + 0.001 * 7.0832061767578125
Epoch 720, val loss: 1.0665771961212158
Epoch 730, training loss: 0.012682726606726646 = 0.005597744602710009 + 0.001 * 7.084981918334961
Epoch 730, val loss: 1.0714435577392578
Epoch 740, training loss: 0.012447051703929901 = 0.005372175015509129 + 0.001 * 7.074876308441162
Epoch 740, val loss: 1.0761595964431763
Epoch 750, training loss: 0.012267122976481915 = 0.005161300767213106 + 0.001 * 7.1058220863342285
Epoch 750, val loss: 1.0807772874832153
Epoch 760, training loss: 0.01203991286456585 = 0.004963884595781565 + 0.001 * 7.0760273933410645
Epoch 760, val loss: 1.0852917432785034
Epoch 770, training loss: 0.011859862133860588 = 0.004778812173753977 + 0.001 * 7.081048965454102
Epoch 770, val loss: 1.0896799564361572
Epoch 780, training loss: 0.011678425595164299 = 0.004605043213814497 + 0.001 * 7.0733819007873535
Epoch 780, val loss: 1.093955397605896
Epoch 790, training loss: 0.011516937054693699 = 0.0044416505843400955 + 0.001 * 7.075285911560059
Epoch 790, val loss: 1.0981194972991943
Epoch 800, training loss: 0.011362934485077858 = 0.004287817981094122 + 0.001 * 7.07511568069458
Epoch 800, val loss: 1.1021943092346191
Epoch 810, training loss: 0.011222857981920242 = 0.004142848774790764 + 0.001 * 7.080008506774902
Epoch 810, val loss: 1.1061737537384033
Epoch 820, training loss: 0.011072252877056599 = 0.004006041679531336 + 0.001 * 7.066210746765137
Epoch 820, val loss: 1.1100434064865112
Epoch 830, training loss: 0.010950786992907524 = 0.0038768446538597345 + 0.001 * 7.073941707611084
Epoch 830, val loss: 1.1138250827789307
Epoch 840, training loss: 0.010831913910806179 = 0.003754666540771723 + 0.001 * 7.077247142791748
Epoch 840, val loss: 1.1175401210784912
Epoch 850, training loss: 0.010699216276407242 = 0.00363903702236712 + 0.001 * 7.060178756713867
Epoch 850, val loss: 1.1211271286010742
Epoch 860, training loss: 0.01058834046125412 = 0.0035294361878186464 + 0.001 * 7.058903694152832
Epoch 860, val loss: 1.124653935432434
Epoch 870, training loss: 0.010489878244698048 = 0.0034255057107657194 + 0.001 * 7.0643720626831055
Epoch 870, val loss: 1.128103494644165
Epoch 880, training loss: 0.010394038632512093 = 0.0033268104307353497 + 0.001 * 7.067228317260742
Epoch 880, val loss: 1.1314839124679565
Epoch 890, training loss: 0.010319755412638187 = 0.0032330346293747425 + 0.001 * 7.0867204666137695
Epoch 890, val loss: 1.1347885131835938
Epoch 900, training loss: 0.010207954794168472 = 0.0031438027508556843 + 0.001 * 7.064151287078857
Epoch 900, val loss: 1.1380292177200317
Epoch 910, training loss: 0.010102851316332817 = 0.003058904781937599 + 0.001 * 7.043945789337158
Epoch 910, val loss: 1.1411795616149902
Epoch 920, training loss: 0.010025501251220703 = 0.0029779919423162937 + 0.001 * 7.047509670257568
Epoch 920, val loss: 1.1442785263061523
Epoch 930, training loss: 0.009963775984942913 = 0.002900833496823907 + 0.001 * 7.0629425048828125
Epoch 930, val loss: 1.147325038909912
Epoch 940, training loss: 0.009893112815916538 = 0.0028271980118006468 + 0.001 * 7.065914154052734
Epoch 940, val loss: 1.1502900123596191
Epoch 950, training loss: 0.009803351014852524 = 0.002756870584562421 + 0.001 * 7.046480178833008
Epoch 950, val loss: 1.1531974077224731
Epoch 960, training loss: 0.009732598438858986 = 0.0026896344497799873 + 0.001 * 7.042963027954102
Epoch 960, val loss: 1.1560735702514648
Epoch 970, training loss: 0.009665408171713352 = 0.0026252989191561937 + 0.001 * 7.040108680725098
Epoch 970, val loss: 1.1588621139526367
Epoch 980, training loss: 0.009619934484362602 = 0.002563713351264596 + 0.001 * 7.056220531463623
Epoch 980, val loss: 1.1616045236587524
Epoch 990, training loss: 0.009572620503604412 = 0.0025046851951628923 + 0.001 * 7.067934989929199
Epoch 990, val loss: 1.1643154621124268
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8402741170268846
The final CL Acc:0.81852, 0.00302, The final GNN Acc:0.84080, 0.00155
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10558])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.974823236465454 = 1.9662264585494995 + 0.001 * 8.59678840637207
Epoch 0, val loss: 1.9645423889160156
Epoch 10, training loss: 1.9639018774032593 = 1.9553050994873047 + 0.001 * 8.596749305725098
Epoch 10, val loss: 1.9539217948913574
Epoch 20, training loss: 1.9507089853286743 = 1.9421124458312988 + 0.001 * 8.596586227416992
Epoch 20, val loss: 1.9406075477600098
Epoch 30, training loss: 1.9324698448181152 = 1.9238736629486084 + 0.001 * 8.59619426727295
Epoch 30, val loss: 1.9218215942382812
Epoch 40, training loss: 1.905879259109497 = 1.8972840309143066 + 0.001 * 8.595223426818848
Epoch 40, val loss: 1.894352674484253
Epoch 50, training loss: 1.869096040725708 = 1.8605035543441772 + 0.001 * 8.592499732971191
Epoch 50, val loss: 1.8574892282485962
Epoch 60, training loss: 1.828936219215393 = 1.8203531503677368 + 0.001 * 8.583023071289062
Epoch 60, val loss: 1.82089364528656
Epoch 70, training loss: 1.7997890710830688 = 1.7912511825561523 + 0.001 * 8.537919998168945
Epoch 70, val loss: 1.7975631952285767
Epoch 80, training loss: 1.7687606811523438 = 1.7604621648788452 + 0.001 * 8.29849624633789
Epoch 80, val loss: 1.7702372074127197
Epoch 90, training loss: 1.725744366645813 = 1.7176331281661987 + 0.001 * 8.111234664916992
Epoch 90, val loss: 1.732468843460083
Epoch 100, training loss: 1.6647779941558838 = 1.656959056854248 + 0.001 * 7.8189215660095215
Epoch 100, val loss: 1.6804765462875366
Epoch 110, training loss: 1.583371877670288 = 1.5756940841674805 + 0.001 * 7.67775297164917
Epoch 110, val loss: 1.6123594045639038
Epoch 120, training loss: 1.489816427230835 = 1.4821516275405884 + 0.001 * 7.664832592010498
Epoch 120, val loss: 1.5357805490493774
Epoch 130, training loss: 1.3925248384475708 = 1.3848685026168823 + 0.001 * 7.656342029571533
Epoch 130, val loss: 1.4583790302276611
Epoch 140, training loss: 1.292140245437622 = 1.284489631652832 + 0.001 * 7.650590896606445
Epoch 140, val loss: 1.3822391033172607
Epoch 150, training loss: 1.1872968673706055 = 1.179655909538269 + 0.001 * 7.640952110290527
Epoch 150, val loss: 1.3049155473709106
Epoch 160, training loss: 1.0798828601837158 = 1.0722650289535522 + 0.001 * 7.617784023284912
Epoch 160, val loss: 1.2281278371810913
Epoch 170, training loss: 0.9755561947822571 = 0.9679896235466003 + 0.001 * 7.566573619842529
Epoch 170, val loss: 1.1556217670440674
Epoch 180, training loss: 0.8803588151931763 = 0.8728879690170288 + 0.001 * 7.47083854675293
Epoch 180, val loss: 1.0918642282485962
Epoch 190, training loss: 0.7979305982589722 = 0.7905343770980835 + 0.001 * 7.396210193634033
Epoch 190, val loss: 1.0401111841201782
Epoch 200, training loss: 0.7282871007919312 = 0.7209516167640686 + 0.001 * 7.33551025390625
Epoch 200, val loss: 1.0006153583526611
Epoch 210, training loss: 0.6687588095664978 = 0.6614576578140259 + 0.001 * 7.3011345863342285
Epoch 210, val loss: 0.9716817140579224
Epoch 220, training loss: 0.6161960363388062 = 0.6089059114456177 + 0.001 * 7.290139675140381
Epoch 220, val loss: 0.9508824944496155
Epoch 230, training loss: 0.5682318806648254 = 0.5609631538391113 + 0.001 * 7.268739700317383
Epoch 230, val loss: 0.9362470507621765
Epoch 240, training loss: 0.5235785245895386 = 0.516318678855896 + 0.001 * 7.259864807128906
Epoch 240, val loss: 0.9266026616096497
Epoch 250, training loss: 0.48151496052742004 = 0.4742608666419983 + 0.001 * 7.254093647003174
Epoch 250, val loss: 0.9214028716087341
Epoch 260, training loss: 0.4417365789413452 = 0.4344857931137085 + 0.001 * 7.25079345703125
Epoch 260, val loss: 0.9204971790313721
Epoch 270, training loss: 0.40433815121650696 = 0.3970908224582672 + 0.001 * 7.2473297119140625
Epoch 270, val loss: 0.9240198135375977
Epoch 280, training loss: 0.36971983313560486 = 0.3624756932258606 + 0.001 * 7.244143486022949
Epoch 280, val loss: 0.9317655563354492
Epoch 290, training loss: 0.33821186423301697 = 0.3309726119041443 + 0.001 * 7.239262580871582
Epoch 290, val loss: 0.9433802366256714
Epoch 300, training loss: 0.3098195493221283 = 0.30257925391197205 + 0.001 * 7.240292072296143
Epoch 300, val loss: 0.958432137966156
Epoch 310, training loss: 0.28415077924728394 = 0.2769201099872589 + 0.001 * 7.230661869049072
Epoch 310, val loss: 0.9762873649597168
Epoch 320, training loss: 0.2606591284275055 = 0.2534272074699402 + 0.001 * 7.231910228729248
Epoch 320, val loss: 0.9962957501411438
Epoch 330, training loss: 0.23869642615318298 = 0.23147131502628326 + 0.001 * 7.225103855133057
Epoch 330, val loss: 1.0177816152572632
Epoch 340, training loss: 0.21768075227737427 = 0.2104606181383133 + 0.001 * 7.220138072967529
Epoch 340, val loss: 1.040253758430481
Epoch 350, training loss: 0.19720430672168732 = 0.18998584151268005 + 0.001 * 7.218461990356445
Epoch 350, val loss: 1.0634866952896118
Epoch 360, training loss: 0.17722709476947784 = 0.17001090943813324 + 0.001 * 7.216183662414551
Epoch 360, val loss: 1.0871365070343018
Epoch 370, training loss: 0.15813171863555908 = 0.1509234458208084 + 0.001 * 7.208271026611328
Epoch 370, val loss: 1.1112829446792603
Epoch 380, training loss: 0.14052218198776245 = 0.1332816779613495 + 0.001 * 7.240496635437012
Epoch 380, val loss: 1.1363482475280762
Epoch 390, training loss: 0.12469955533742905 = 0.11749398708343506 + 0.001 * 7.205566883087158
Epoch 390, val loss: 1.1626553535461426
Epoch 400, training loss: 0.11086028814315796 = 0.1036624163389206 + 0.001 * 7.1978678703308105
Epoch 400, val loss: 1.1903821229934692
Epoch 410, training loss: 0.09886237233877182 = 0.0916658341884613 + 0.001 * 7.196537017822266
Epoch 410, val loss: 1.2194314002990723
Epoch 420, training loss: 0.08852464705705643 = 0.0812908336520195 + 0.001 * 7.233815670013428
Epoch 420, val loss: 1.249516248703003
Epoch 430, training loss: 0.07950600981712341 = 0.0723169595003128 + 0.001 * 7.189047813415527
Epoch 430, val loss: 1.2801196575164795
Epoch 440, training loss: 0.07172735780477524 = 0.06454258412122726 + 0.001 * 7.184770107269287
Epoch 440, val loss: 1.3108787536621094
Epoch 450, training loss: 0.06497366726398468 = 0.057793207466602325 + 0.001 * 7.180455684661865
Epoch 450, val loss: 1.3413621187210083
Epoch 460, training loss: 0.05909791961312294 = 0.05192044377326965 + 0.001 * 7.177474498748779
Epoch 460, val loss: 1.3713847398757935
Epoch 470, training loss: 0.05398523434996605 = 0.04679751396179199 + 0.001 * 7.187719345092773
Epoch 470, val loss: 1.4007152318954468
Epoch 480, training loss: 0.04948486015200615 = 0.042319249361753464 + 0.001 * 7.165608882904053
Epoch 480, val loss: 1.4292685985565186
Epoch 490, training loss: 0.04558348283171654 = 0.03839361295104027 + 0.001 * 7.189870357513428
Epoch 490, val loss: 1.456979751586914
Epoch 500, training loss: 0.04210952669382095 = 0.034942589700222015 + 0.001 * 7.166934967041016
Epoch 500, val loss: 1.4838590621948242
Epoch 510, training loss: 0.039065584540367126 = 0.03190114349126816 + 0.001 * 7.164441108703613
Epoch 510, val loss: 1.5098531246185303
Epoch 520, training loss: 0.036377377808094025 = 0.029213130474090576 + 0.001 * 7.164247035980225
Epoch 520, val loss: 1.5349575281143188
Epoch 530, training loss: 0.03400164842605591 = 0.026831578463315964 + 0.001 * 7.170069694519043
Epoch 530, val loss: 1.559224247932434
Epoch 540, training loss: 0.03186322748661041 = 0.02471575327217579 + 0.001 * 7.147475242614746
Epoch 540, val loss: 1.582659363746643
Epoch 550, training loss: 0.029988117516040802 = 0.022830631583929062 + 0.001 * 7.157484531402588
Epoch 550, val loss: 1.6053053140640259
Epoch 560, training loss: 0.02831515669822693 = 0.02114640362560749 + 0.001 * 7.1687517166137695
Epoch 560, val loss: 1.62717604637146
Epoch 570, training loss: 0.026783106848597527 = 0.019637463614344597 + 0.001 * 7.14564323425293
Epoch 570, val loss: 1.6483147144317627
Epoch 580, training loss: 0.025417262688279152 = 0.018282035365700722 + 0.001 * 7.135227203369141
Epoch 580, val loss: 1.6686615943908691
Epoch 590, training loss: 0.02421322651207447 = 0.017061106860637665 + 0.001 * 7.152118682861328
Epoch 590, val loss: 1.6883565187454224
Epoch 600, training loss: 0.02311641350388527 = 0.015958134084939957 + 0.001 * 7.158277988433838
Epoch 600, val loss: 1.707383155822754
Epoch 610, training loss: 0.022092947736382484 = 0.014959042891860008 + 0.001 * 7.133904457092285
Epoch 610, val loss: 1.72574782371521
Epoch 620, training loss: 0.021194512024521828 = 0.014051686972379684 + 0.001 * 7.142824649810791
Epoch 620, val loss: 1.7435057163238525
Epoch 630, training loss: 0.020366422832012177 = 0.013225704431533813 + 0.001 * 7.140718460083008
Epoch 630, val loss: 1.760670781135559
Epoch 640, training loss: 0.019615717232227325 = 0.012472013011574745 + 0.001 * 7.143702983856201
Epoch 640, val loss: 1.777265191078186
Epoch 650, training loss: 0.018910985440015793 = 0.011782482266426086 + 0.001 * 7.128502368927002
Epoch 650, val loss: 1.7933539152145386
Epoch 660, training loss: 0.018274612724781036 = 0.011149669997394085 + 0.001 * 7.124942302703857
Epoch 660, val loss: 1.8089503049850464
Epoch 670, training loss: 0.01768634282052517 = 0.010567289777100086 + 0.001 * 7.119052410125732
Epoch 670, val loss: 1.8240530490875244
Epoch 680, training loss: 0.017167115584015846 = 0.010029957629740238 + 0.001 * 7.137156963348389
Epoch 680, val loss: 1.8387248516082764
Epoch 690, training loss: 0.016639675945043564 = 0.00953314732760191 + 0.001 * 7.106527328491211
Epoch 690, val loss: 1.8529623746871948
Epoch 700, training loss: 0.01618443801999092 = 0.009072394110262394 + 0.001 * 7.112044334411621
Epoch 700, val loss: 1.866819977760315
Epoch 710, training loss: 0.015783224254846573 = 0.008643935434520245 + 0.001 * 7.139288902282715
Epoch 710, val loss: 1.8802982568740845
Epoch 720, training loss: 0.015359349548816681 = 0.008244570344686508 + 0.001 * 7.114778995513916
Epoch 720, val loss: 1.8934659957885742
Epoch 730, training loss: 0.014968927949666977 = 0.00787169300019741 + 0.001 * 7.097234725952148
Epoch 730, val loss: 1.9063115119934082
Epoch 740, training loss: 0.014635531231760979 = 0.00752321956679225 + 0.001 * 7.112311840057373
Epoch 740, val loss: 1.9188419580459595
Epoch 750, training loss: 0.01431211456656456 = 0.007196915335953236 + 0.001 * 7.115199565887451
Epoch 750, val loss: 1.931077480316162
Epoch 760, training loss: 0.013984429650008678 = 0.006891188677400351 + 0.001 * 7.093240737915039
Epoch 760, val loss: 1.9430269002914429
Epoch 770, training loss: 0.01370763499289751 = 0.00660421559587121 + 0.001 * 7.103418827056885
Epoch 770, val loss: 1.9547204971313477
Epoch 780, training loss: 0.013441499322652817 = 0.006334517151117325 + 0.001 * 7.1069817543029785
Epoch 780, val loss: 1.9661774635314941
Epoch 790, training loss: 0.013190846890211105 = 0.006080894730985165 + 0.001 * 7.109951972961426
Epoch 790, val loss: 1.977341890335083
Epoch 800, training loss: 0.01293095201253891 = 0.005842253100126982 + 0.001 * 7.088698387145996
Epoch 800, val loss: 1.9882893562316895
Epoch 810, training loss: 0.012707047164440155 = 0.005617544520646334 + 0.001 * 7.089502334594727
Epoch 810, val loss: 1.9989795684814453
Epoch 820, training loss: 0.01249791495501995 = 0.00540571752935648 + 0.001 * 7.092196464538574
Epoch 820, val loss: 2.009455442428589
Epoch 830, training loss: 0.012287941761314869 = 0.005205905996263027 + 0.001 * 7.082035541534424
Epoch 830, val loss: 2.019704580307007
Epoch 840, training loss: 0.012090107426047325 = 0.005017266608774662 + 0.001 * 7.072841167449951
Epoch 840, val loss: 2.029733657836914
Epoch 850, training loss: 0.011910248547792435 = 0.00483927596360445 + 0.001 * 7.070972919464111
Epoch 850, val loss: 2.0395355224609375
Epoch 860, training loss: 0.011750558391213417 = 0.004670953378081322 + 0.001 * 7.079604625701904
Epoch 860, val loss: 2.0491416454315186
Epoch 870, training loss: 0.01159779354929924 = 0.004511669278144836 + 0.001 * 7.086123466491699
Epoch 870, val loss: 2.0585598945617676
Epoch 880, training loss: 0.011439353227615356 = 0.0043607656843960285 + 0.001 * 7.078586578369141
Epoch 880, val loss: 2.0677571296691895
Epoch 890, training loss: 0.011278010904788971 = 0.004217799752950668 + 0.001 * 7.060211181640625
Epoch 890, val loss: 2.0767805576324463
Epoch 900, training loss: 0.011145275086164474 = 0.004082095343619585 + 0.001 * 7.063179969787598
Epoch 900, val loss: 2.0856282711029053
Epoch 910, training loss: 0.011043617501854897 = 0.003953267820179462 + 0.001 * 7.090349197387695
Epoch 910, val loss: 2.09429669380188
Epoch 920, training loss: 0.01090310700237751 = 0.003831025678664446 + 0.001 * 7.072081565856934
Epoch 920, val loss: 2.102745532989502
Epoch 930, training loss: 0.010816440917551517 = 0.003714830381795764 + 0.001 * 7.10161018371582
Epoch 930, val loss: 2.1110360622406006
Epoch 940, training loss: 0.010723115876317024 = 0.0036045864690095186 + 0.001 * 7.118528842926025
Epoch 940, val loss: 2.119152784347534
Epoch 950, training loss: 0.010573704726994038 = 0.003499748185276985 + 0.001 * 7.07395601272583
Epoch 950, val loss: 2.1270642280578613
Epoch 960, training loss: 0.010458721779286861 = 0.00339997629635036 + 0.001 * 7.05874490737915
Epoch 960, val loss: 2.134831666946411
Epoch 970, training loss: 0.010373631492257118 = 0.0033048870973289013 + 0.001 * 7.068743705749512
Epoch 970, val loss: 2.1424295902252197
Epoch 980, training loss: 0.010273642838001251 = 0.003214150434359908 + 0.001 * 7.0594916343688965
Epoch 980, val loss: 2.1498959064483643
Epoch 990, training loss: 0.010230759158730507 = 0.0031274883076548576 + 0.001 * 7.103270053863525
Epoch 990, val loss: 2.1572141647338867
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.7986294148655773
=== training gcn model ===
Epoch 0, training loss: 1.96115243434906 = 1.952555537223816 + 0.001 * 8.596846580505371
Epoch 0, val loss: 1.9579936265945435
Epoch 10, training loss: 1.9501399993896484 = 1.9415432214736938 + 0.001 * 8.596805572509766
Epoch 10, val loss: 1.9468227624893188
Epoch 20, training loss: 1.936431884765625 = 1.92783522605896 + 0.001 * 8.596656799316406
Epoch 20, val loss: 1.9327740669250488
Epoch 30, training loss: 1.9169799089431763 = 1.9083836078643799 + 0.001 * 8.596314430236816
Epoch 30, val loss: 1.912881851196289
Epoch 40, training loss: 1.8887677192687988 = 1.8801722526550293 + 0.001 * 8.595499992370605
Epoch 40, val loss: 1.8844789266586304
Epoch 50, training loss: 1.8517885208129883 = 1.8431953191757202 + 0.001 * 8.593144416809082
Epoch 50, val loss: 1.8487287759780884
Epoch 60, training loss: 1.815665602684021 = 1.8070809841156006 + 0.001 * 8.58464527130127
Epoch 60, val loss: 1.8158090114593506
Epoch 70, training loss: 1.7862811088562012 = 1.7777328491210938 + 0.001 * 8.548206329345703
Epoch 70, val loss: 1.7873356342315674
Epoch 80, training loss: 1.7473564147949219 = 1.739029049873352 + 0.001 * 8.327310562133789
Epoch 80, val loss: 1.7513593435287476
Epoch 90, training loss: 1.6929792165756226 = 1.6848160028457642 + 0.001 * 8.163251876831055
Epoch 90, val loss: 1.7060551643371582
Epoch 100, training loss: 1.6192044019699097 = 1.6110998392105103 + 0.001 * 8.10461711883545
Epoch 100, val loss: 1.6464109420776367
Epoch 110, training loss: 1.533978819847107 = 1.525893211364746 + 0.001 * 8.085624694824219
Epoch 110, val loss: 1.5786842107772827
Epoch 120, training loss: 1.449889898300171 = 1.4418169260025024 + 0.001 * 8.07294750213623
Epoch 120, val loss: 1.5139280557632446
Epoch 130, training loss: 1.3718081712722778 = 1.3637642860412598 + 0.001 * 8.043845176696777
Epoch 130, val loss: 1.4557870626449585
Epoch 140, training loss: 1.2969244718551636 = 1.2890089750289917 + 0.001 * 7.915444374084473
Epoch 140, val loss: 1.4019919633865356
Epoch 150, training loss: 1.2221564054489136 = 1.2145121097564697 + 0.001 * 7.644320487976074
Epoch 150, val loss: 1.3498764038085938
Epoch 160, training loss: 1.1462969779968262 = 1.1387230157852173 + 0.001 * 7.574018955230713
Epoch 160, val loss: 1.2981770038604736
Epoch 170, training loss: 1.068052053451538 = 1.0605087280273438 + 0.001 * 7.543275833129883
Epoch 170, val loss: 1.2452431917190552
Epoch 180, training loss: 0.9870341420173645 = 0.9795006513595581 + 0.001 * 7.533475399017334
Epoch 180, val loss: 1.1899785995483398
Epoch 190, training loss: 0.904889702796936 = 0.8973668813705444 + 0.001 * 7.522817611694336
Epoch 190, val loss: 1.133082389831543
Epoch 200, training loss: 0.8248263001441956 = 0.8173112273216248 + 0.001 * 7.515059471130371
Epoch 200, val loss: 1.0768990516662598
Epoch 210, training loss: 0.7499074339866638 = 0.742401123046875 + 0.001 * 7.506300449371338
Epoch 210, val loss: 1.0243337154388428
Epoch 220, training loss: 0.6815955638885498 = 0.6740998029708862 + 0.001 * 7.49576473236084
Epoch 220, val loss: 0.9775264263153076
Epoch 230, training loss: 0.619221568107605 = 0.6117393374443054 + 0.001 * 7.482205390930176
Epoch 230, val loss: 0.9366968870162964
Epoch 240, training loss: 0.560986340045929 = 0.5535222887992859 + 0.001 * 7.464047431945801
Epoch 240, val loss: 0.901127815246582
Epoch 250, training loss: 0.5053900480270386 = 0.4979488253593445 + 0.001 * 7.441226005554199
Epoch 250, val loss: 0.8698964715003967
Epoch 260, training loss: 0.4517914354801178 = 0.44436946511268616 + 0.001 * 7.421961307525635
Epoch 260, val loss: 0.8431004881858826
Epoch 270, training loss: 0.40038180351257324 = 0.3929765820503235 + 0.001 * 7.405213356018066
Epoch 270, val loss: 0.8211553692817688
Epoch 280, training loss: 0.3516194224357605 = 0.34422117471694946 + 0.001 * 7.398232460021973
Epoch 280, val loss: 0.8037148714065552
Epoch 290, training loss: 0.3062416911125183 = 0.2988490164279938 + 0.001 * 7.392672538757324
Epoch 290, val loss: 0.7905464172363281
Epoch 300, training loss: 0.26496902108192444 = 0.25757941603660583 + 0.001 * 7.389606952667236
Epoch 300, val loss: 0.7812677621841431
Epoch 310, training loss: 0.22834806144237518 = 0.22096022963523865 + 0.001 * 7.387837886810303
Epoch 310, val loss: 0.7757153511047363
Epoch 320, training loss: 0.19650515913963318 = 0.18911801278591156 + 0.001 * 7.387141227722168
Epoch 320, val loss: 0.7737635374069214
Epoch 330, training loss: 0.16927923262119293 = 0.16189268231391907 + 0.001 * 7.386549949645996
Epoch 330, val loss: 0.7751624584197998
Epoch 340, training loss: 0.1461910456418991 = 0.13880528509616852 + 0.001 * 7.385759353637695
Epoch 340, val loss: 0.7793946862220764
Epoch 350, training loss: 0.12668617069721222 = 0.11930133402347565 + 0.001 * 7.384843349456787
Epoch 350, val loss: 0.7859188318252563
Epoch 360, training loss: 0.11023115366697311 = 0.1028466522693634 + 0.001 * 7.384498596191406
Epoch 360, val loss: 0.7943233847618103
Epoch 370, training loss: 0.09635338187217712 = 0.08896682411432266 + 0.001 * 7.386556148529053
Epoch 370, val loss: 0.804149329662323
Epoch 380, training loss: 0.08464350551366806 = 0.07725988328456879 + 0.001 * 7.383621692657471
Epoch 380, val loss: 0.8151213526725769
Epoch 390, training loss: 0.07476386427879333 = 0.06738398969173431 + 0.001 * 7.379875659942627
Epoch 390, val loss: 0.8268398642539978
Epoch 400, training loss: 0.06643029302358627 = 0.05904867500066757 + 0.001 * 7.38162088394165
Epoch 400, val loss: 0.8390454053878784
Epoch 410, training loss: 0.05938296765089035 = 0.052002403885126114 + 0.001 * 7.380564212799072
Epoch 410, val loss: 0.8514943718910217
Epoch 420, training loss: 0.0534103624522686 = 0.04603349789977074 + 0.001 * 7.376863479614258
Epoch 420, val loss: 0.8640686273574829
Epoch 430, training loss: 0.04834099858999252 = 0.04096298664808273 + 0.001 * 7.378009796142578
Epoch 430, val loss: 0.8766840696334839
Epoch 440, training loss: 0.04400680214166641 = 0.03663879260420799 + 0.001 * 7.368007183074951
Epoch 440, val loss: 0.8891939520835876
Epoch 450, training loss: 0.040299709886312485 = 0.03293518349528313 + 0.001 * 7.36452579498291
Epoch 450, val loss: 0.9015974998474121
Epoch 460, training loss: 0.03710751235485077 = 0.029748596251010895 + 0.001 * 7.358914852142334
Epoch 460, val loss: 0.9138112664222717
Epoch 470, training loss: 0.034352660179138184 = 0.026993177831172943 + 0.001 * 7.3594818115234375
Epoch 470, val loss: 0.925790548324585
Epoch 480, training loss: 0.03195181116461754 = 0.024599414318799973 + 0.001 * 7.352397441864014
Epoch 480, val loss: 0.937488853931427
Epoch 490, training loss: 0.029853075742721558 = 0.022509876638650894 + 0.001 * 7.343198776245117
Epoch 490, val loss: 0.9489526748657227
Epoch 500, training loss: 0.028022397309541702 = 0.020676886662840843 + 0.001 * 7.345510959625244
Epoch 500, val loss: 0.9601374864578247
Epoch 510, training loss: 0.02639983594417572 = 0.019061049446463585 + 0.001 * 7.338785648345947
Epoch 510, val loss: 0.9710027575492859
Epoch 520, training loss: 0.02496248111128807 = 0.01763051375746727 + 0.001 * 7.331966400146484
Epoch 520, val loss: 0.9815429449081421
Epoch 530, training loss: 0.02370578423142433 = 0.016358589753508568 + 0.001 * 7.347194671630859
Epoch 530, val loss: 0.9918169975280762
Epoch 540, training loss: 0.02252212166786194 = 0.015223400667309761 + 0.001 * 7.2987213134765625
Epoch 540, val loss: 1.0017988681793213
Epoch 550, training loss: 0.02150711603462696 = 0.014206289313733578 + 0.001 * 7.3008270263671875
Epoch 550, val loss: 1.0114792585372925
Epoch 560, training loss: 0.020579876378178596 = 0.013291645795106888 + 0.001 * 7.288229942321777
Epoch 560, val loss: 1.0208559036254883
Epoch 570, training loss: 0.0197483878582716 = 0.01246627513319254 + 0.001 * 7.2821125984191895
Epoch 570, val loss: 1.0299527645111084
Epoch 580, training loss: 0.019015388563275337 = 0.01171891763806343 + 0.001 * 7.296471118927002
Epoch 580, val loss: 1.0387829542160034
Epoch 590, training loss: 0.01833205670118332 = 0.011040120385587215 + 0.001 * 7.291935920715332
Epoch 590, val loss: 1.0473562479019165
Epoch 600, training loss: 0.017696762457489967 = 0.010421772487461567 + 0.001 * 7.274990081787109
Epoch 600, val loss: 1.0556833744049072
Epoch 610, training loss: 0.01712833344936371 = 0.009856895543634892 + 0.001 * 7.271438121795654
Epoch 610, val loss: 1.0637644529342651
Epoch 620, training loss: 0.016592107713222504 = 0.009339524433016777 + 0.001 * 7.2525835037231445
Epoch 620, val loss: 1.0716090202331543
Epoch 630, training loss: 0.0161192137748003 = 0.008864439092576504 + 0.001 * 7.254774570465088
Epoch 630, val loss: 1.079253077507019
Epoch 640, training loss: 0.015669889748096466 = 0.008427122607827187 + 0.001 * 7.242767333984375
Epoch 640, val loss: 1.0866674184799194
Epoch 650, training loss: 0.015280081890523434 = 0.008023680187761784 + 0.001 * 7.256401538848877
Epoch 650, val loss: 1.0938726663589478
Epoch 660, training loss: 0.014898116700351238 = 0.007650618441402912 + 0.001 * 7.247498035430908
Epoch 660, val loss: 1.100897192955017
Epoch 670, training loss: 0.014544210396707058 = 0.00730495061725378 + 0.001 * 7.239259243011475
Epoch 670, val loss: 1.107735514640808
Epoch 680, training loss: 0.014205282554030418 = 0.006984110455960035 + 0.001 * 7.221172332763672
Epoch 680, val loss: 1.1143643856048584
Epoch 690, training loss: 0.013913793489336967 = 0.006685751490294933 + 0.001 * 7.228042125701904
Epoch 690, val loss: 1.1208443641662598
Epoch 700, training loss: 0.013635365292429924 = 0.006407764740288258 + 0.001 * 7.227599620819092
Epoch 700, val loss: 1.127162218093872
Epoch 710, training loss: 0.013367393054068089 = 0.006148346699774265 + 0.001 * 7.219046115875244
Epoch 710, val loss: 1.133332371711731
Epoch 720, training loss: 0.013145137578248978 = 0.0059058391489088535 + 0.001 * 7.239297389984131
Epoch 720, val loss: 1.1393424272537231
Epoch 730, training loss: 0.01288713701069355 = 0.005678835324943066 + 0.001 * 7.208302021026611
Epoch 730, val loss: 1.1451804637908936
Epoch 740, training loss: 0.012688843533396721 = 0.005465997848659754 + 0.001 * 7.222845554351807
Epoch 740, val loss: 1.1508756875991821
Epoch 750, training loss: 0.012501927092671394 = 0.005266144406050444 + 0.001 * 7.235782146453857
Epoch 750, val loss: 1.1564539670944214
Epoch 760, training loss: 0.012293803505599499 = 0.005078237038105726 + 0.001 * 7.215566158294678
Epoch 760, val loss: 1.1618800163269043
Epoch 770, training loss: 0.012103848159313202 = 0.0049013299867510796 + 0.001 * 7.202517509460449
Epoch 770, val loss: 1.1671735048294067
Epoch 780, training loss: 0.011938082054257393 = 0.004734561312943697 + 0.001 * 7.203520774841309
Epoch 780, val loss: 1.172346591949463
Epoch 790, training loss: 0.011788347736001015 = 0.004577102605253458 + 0.001 * 7.211244583129883
Epoch 790, val loss: 1.177406668663025
Epoch 800, training loss: 0.01162042934447527 = 0.004428231157362461 + 0.001 * 7.192197799682617
Epoch 800, val loss: 1.1823457479476929
Epoch 810, training loss: 0.01149681955575943 = 0.004287200514227152 + 0.001 * 7.209619045257568
Epoch 810, val loss: 1.1872045993804932
Epoch 820, training loss: 0.011332431808114052 = 0.0041532875038683414 + 0.001 * 7.17914342880249
Epoch 820, val loss: 1.1919552087783813
Epoch 830, training loss: 0.0112067349255085 = 0.00402557710185647 + 0.001 * 7.181157112121582
Epoch 830, val loss: 1.1966450214385986
Epoch 840, training loss: 0.011096947826445103 = 0.003903136821463704 + 0.001 * 7.193810939788818
Epoch 840, val loss: 1.2012919187545776
Epoch 850, training loss: 0.010971528477966785 = 0.003785446286201477 + 0.001 * 7.186081886291504
Epoch 850, val loss: 1.2059619426727295
Epoch 860, training loss: 0.010872207581996918 = 0.0036717578768730164 + 0.001 * 7.200448989868164
Epoch 860, val loss: 1.2106373310089111
Epoch 870, training loss: 0.010750200599431992 = 0.003561657387763262 + 0.001 * 7.18854284286499
Epoch 870, val loss: 1.215314269065857
Epoch 880, training loss: 0.010656733065843582 = 0.003454923629760742 + 0.001 * 7.201808452606201
Epoch 880, val loss: 1.2199783325195312
Epoch 890, training loss: 0.01052506361156702 = 0.0033514683600515127 + 0.001 * 7.1735944747924805
Epoch 890, val loss: 1.2247092723846436
Epoch 900, training loss: 0.010436090640723705 = 0.0032514622434973717 + 0.001 * 7.184628009796143
Epoch 900, val loss: 1.2293685674667358
Epoch 910, training loss: 0.010318061336874962 = 0.0031551739666610956 + 0.001 * 7.162886619567871
Epoch 910, val loss: 1.234059453010559
Epoch 920, training loss: 0.010236043483018875 = 0.00306243565864861 + 0.001 * 7.173607349395752
Epoch 920, val loss: 1.238692283630371
Epoch 930, training loss: 0.010144281201064587 = 0.002973206341266632 + 0.001 * 7.171074390411377
Epoch 930, val loss: 1.2433134317398071
Epoch 940, training loss: 0.010037428699433804 = 0.002887507202103734 + 0.001 * 7.14992094039917
Epoch 940, val loss: 1.2479106187820435
Epoch 950, training loss: 0.009973032400012016 = 0.00280536781065166 + 0.001 * 7.167664051055908
Epoch 950, val loss: 1.2523938417434692
Epoch 960, training loss: 0.009928991086781025 = 0.0027266307733953 + 0.001 * 7.202360153198242
Epoch 960, val loss: 1.2568281888961792
Epoch 970, training loss: 0.009796692058444023 = 0.002651300746947527 + 0.001 * 7.14539098739624
Epoch 970, val loss: 1.2612521648406982
Epoch 980, training loss: 0.00971116591244936 = 0.0025791150983422995 + 0.001 * 7.132050514221191
Epoch 980, val loss: 1.2655463218688965
Epoch 990, training loss: 0.009645981714129448 = 0.0025099930353462696 + 0.001 * 7.135988712310791
Epoch 990, val loss: 1.2698094844818115
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 1.9441133737564087 = 1.935516595840454 + 0.001 * 8.596818923950195
Epoch 0, val loss: 1.9336963891983032
Epoch 10, training loss: 1.9347131252288818 = 1.9261163473129272 + 0.001 * 8.59677505493164
Epoch 10, val loss: 1.9242208003997803
Epoch 20, training loss: 1.9234426021575928 = 1.9148459434509277 + 0.001 * 8.596626281738281
Epoch 20, val loss: 1.9128915071487427
Epoch 30, training loss: 1.9078526496887207 = 1.8992563486099243 + 0.001 * 8.596261024475098
Epoch 30, val loss: 1.8974770307540894
Epoch 40, training loss: 1.8850653171539307 = 1.8764699697494507 + 0.001 * 8.595337867736816
Epoch 40, val loss: 1.8754804134368896
Epoch 50, training loss: 1.8532655239105225 = 1.8446729183197021 + 0.001 * 8.592551231384277
Epoch 50, val loss: 1.8460067510604858
Epoch 60, training loss: 1.8169310092926025 = 1.8083491325378418 + 0.001 * 8.581911087036133
Epoch 60, val loss: 1.8147469758987427
Epoch 70, training loss: 1.7846312522888184 = 1.7760975360870361 + 0.001 * 8.533703804016113
Epoch 70, val loss: 1.7871730327606201
Epoch 80, training loss: 1.745070219039917 = 1.736817717552185 + 0.001 * 8.252537727355957
Epoch 80, val loss: 1.7499803304672241
Epoch 90, training loss: 1.6888967752456665 = 1.6808308362960815 + 0.001 * 8.06597900390625
Epoch 90, val loss: 1.7003213167190552
Epoch 100, training loss: 1.6130441427230835 = 1.6051888465881348 + 0.001 * 7.85532283782959
Epoch 100, val loss: 1.6376146078109741
Epoch 110, training loss: 1.5198391675949097 = 1.5121865272521973 + 0.001 * 7.652588844299316
Epoch 110, val loss: 1.560725212097168
Epoch 120, training loss: 1.417259931564331 = 1.409647822380066 + 0.001 * 7.612137317657471
Epoch 120, val loss: 1.4764221906661987
Epoch 130, training loss: 1.3106110095977783 = 1.303039789199829 + 0.001 * 7.5711822509765625
Epoch 130, val loss: 1.3893204927444458
Epoch 140, training loss: 1.2004685401916504 = 1.1929470300674438 + 0.001 * 7.521517276763916
Epoch 140, val loss: 1.3021475076675415
Epoch 150, training loss: 1.08823561668396 = 1.080756664276123 + 0.001 * 7.478962421417236
Epoch 150, val loss: 1.2155969142913818
Epoch 160, training loss: 0.9785826802253723 = 0.9711149334907532 + 0.001 * 7.4677300453186035
Epoch 160, val loss: 1.1330206394195557
Epoch 170, training loss: 0.8773418664932251 = 0.8698785305023193 + 0.001 * 7.46331262588501
Epoch 170, val loss: 1.0593141317367554
Epoch 180, training loss: 0.7882957458496094 = 0.7808347940444946 + 0.001 * 7.460940361022949
Epoch 180, val loss: 0.998080313205719
Epoch 190, training loss: 0.7116469740867615 = 0.7041881680488586 + 0.001 * 7.458835124969482
Epoch 190, val loss: 0.9503877758979797
Epoch 200, training loss: 0.6454247236251831 = 0.6379685401916504 + 0.001 * 7.456193923950195
Epoch 200, val loss: 0.9148990511894226
Epoch 210, training loss: 0.5875742435455322 = 0.5801206231117249 + 0.001 * 7.4536237716674805
Epoch 210, val loss: 0.8894904851913452
Epoch 220, training loss: 0.5364083051681519 = 0.5289573073387146 + 0.001 * 7.45102596282959
Epoch 220, val loss: 0.8720150589942932
Epoch 230, training loss: 0.4904637932777405 = 0.4830159842967987 + 0.001 * 7.447807788848877
Epoch 230, val loss: 0.8604971766471863
Epoch 240, training loss: 0.44843819737434387 = 0.44099554419517517 + 0.001 * 7.442667007446289
Epoch 240, val loss: 0.853800356388092
Epoch 250, training loss: 0.4093784689903259 = 0.4019428789615631 + 0.001 * 7.43558931350708
Epoch 250, val loss: 0.8514593839645386
Epoch 260, training loss: 0.37287795543670654 = 0.3654547929763794 + 0.001 * 7.423147201538086
Epoch 260, val loss: 0.8532577157020569
Epoch 270, training loss: 0.3389003276824951 = 0.3314917981624603 + 0.001 * 7.408520221710205
Epoch 270, val loss: 0.8590734601020813
Epoch 280, training loss: 0.30750370025634766 = 0.3001127541065216 + 0.001 * 7.390944480895996
Epoch 280, val loss: 0.868622362613678
Epoch 290, training loss: 0.2785854935646057 = 0.27121081948280334 + 0.001 * 7.3746657371521
Epoch 290, val loss: 0.8814968466758728
Epoch 300, training loss: 0.25187984108924866 = 0.24451369047164917 + 0.001 * 7.366153240203857
Epoch 300, val loss: 0.8971309661865234
Epoch 310, training loss: 0.22706212103366852 = 0.21971096098423004 + 0.001 * 7.351161479949951
Epoch 310, val loss: 0.9150840640068054
Epoch 320, training loss: 0.20389224588871002 = 0.19654341042041779 + 0.001 * 7.3488359451293945
Epoch 320, val loss: 0.935012698173523
Epoch 330, training loss: 0.18224340677261353 = 0.17491236329078674 + 0.001 * 7.3310465812683105
Epoch 330, val loss: 0.9567633867263794
Epoch 340, training loss: 0.16219349205493927 = 0.15486624836921692 + 0.001 * 7.327246189117432
Epoch 340, val loss: 0.9800562262535095
Epoch 350, training loss: 0.14386053383350372 = 0.13653436303138733 + 0.001 * 7.326173305511475
Epoch 350, val loss: 1.0047845840454102
Epoch 360, training loss: 0.12735366821289062 = 0.12003891915082932 + 0.001 * 7.314745903015137
Epoch 360, val loss: 1.0308071374893188
Epoch 370, training loss: 0.1127280667424202 = 0.1054280623793602 + 0.001 * 7.300002098083496
Epoch 370, val loss: 1.057723045349121
Epoch 380, training loss: 0.09997212886810303 = 0.09266597777605057 + 0.001 * 7.306153774261475
Epoch 380, val loss: 1.0853042602539062
Epoch 390, training loss: 0.08890178054571152 = 0.08161245286464691 + 0.001 * 7.289327621459961
Epoch 390, val loss: 1.113042950630188
Epoch 400, training loss: 0.07936247438192368 = 0.07207445800304413 + 0.001 * 7.288018703460693
Epoch 400, val loss: 1.140605092048645
Epoch 410, training loss: 0.07113280147314072 = 0.06385867297649384 + 0.001 * 7.274127006530762
Epoch 410, val loss: 1.1678717136383057
Epoch 420, training loss: 0.06405839323997498 = 0.05678251385688782 + 0.001 * 7.275880813598633
Epoch 420, val loss: 1.1947358846664429
Epoch 430, training loss: 0.057942964136600494 = 0.05068066343665123 + 0.001 * 7.262301445007324
Epoch 430, val loss: 1.2210278511047363
Epoch 440, training loss: 0.05267249047756195 = 0.04541077837347984 + 0.001 * 7.261711597442627
Epoch 440, val loss: 1.24657142162323
Epoch 450, training loss: 0.0480961799621582 = 0.04084668681025505 + 0.001 * 7.249492168426514
Epoch 450, val loss: 1.271310567855835
Epoch 460, training loss: 0.04413200169801712 = 0.03688349947333336 + 0.001 * 7.248501300811768
Epoch 460, val loss: 1.2952545881271362
Epoch 470, training loss: 0.04067669063806534 = 0.03343116119503975 + 0.001 * 7.24553108215332
Epoch 470, val loss: 1.3183927536010742
Epoch 480, training loss: 0.037645675241947174 = 0.030414387583732605 + 0.001 * 7.231289386749268
Epoch 480, val loss: 1.3407310247421265
Epoch 490, training loss: 0.035018280148506165 = 0.027768636122345924 + 0.001 * 7.2496418952941895
Epoch 490, val loss: 1.3622180223464966
Epoch 500, training loss: 0.03266650438308716 = 0.025439899414777756 + 0.001 * 7.226603984832764
Epoch 500, val loss: 1.3829058408737183
Epoch 510, training loss: 0.030597174540162086 = 0.023382483050227165 + 0.001 * 7.214691638946533
Epoch 510, val loss: 1.402838945388794
Epoch 520, training loss: 0.02877253107726574 = 0.021558484062552452 + 0.001 * 7.214046001434326
Epoch 520, val loss: 1.422037124633789
Epoch 530, training loss: 0.027146106585860252 = 0.019935887306928635 + 0.001 * 7.210218906402588
Epoch 530, val loss: 1.4405583143234253
Epoch 540, training loss: 0.025687556713819504 = 0.01848764903843403 + 0.001 * 7.199908256530762
Epoch 540, val loss: 1.4583920240402222
Epoch 550, training loss: 0.024414338171482086 = 0.017190778627991676 + 0.001 * 7.223560333251953
Epoch 550, val loss: 1.4755804538726807
Epoch 560, training loss: 0.023226387798786163 = 0.016025779768824577 + 0.001 * 7.200608253479004
Epoch 560, val loss: 1.4921849966049194
Epoch 570, training loss: 0.0221724733710289 = 0.014975818805396557 + 0.001 * 7.1966552734375
Epoch 570, val loss: 1.5082112550735474
Epoch 580, training loss: 0.021222703158855438 = 0.014026710763573647 + 0.001 * 7.195992469787598
Epoch 580, val loss: 1.523706316947937
Epoch 590, training loss: 0.02035418152809143 = 0.013165398500859737 + 0.001 * 7.188783168792725
Epoch 590, val loss: 1.5386631488800049
Epoch 600, training loss: 0.01957409456372261 = 0.01237989217042923 + 0.001 * 7.194201469421387
Epoch 600, val loss: 1.5532143115997314
Epoch 610, training loss: 0.018850456923246384 = 0.011659392155706882 + 0.001 * 7.191063404083252
Epoch 610, val loss: 1.5674606561660767
Epoch 620, training loss: 0.018173879012465477 = 0.010995276272296906 + 0.001 * 7.178602695465088
Epoch 620, val loss: 1.5814810991287231
Epoch 630, training loss: 0.01756863109767437 = 0.010381808504462242 + 0.001 * 7.186822414398193
Epoch 630, val loss: 1.5953428745269775
Epoch 640, training loss: 0.01700012758374214 = 0.009814675897359848 + 0.001 * 7.185451507568359
Epoch 640, val loss: 1.6090224981307983
Epoch 650, training loss: 0.016475412994623184 = 0.009289857931435108 + 0.001 * 7.185554504394531
Epoch 650, val loss: 1.6224819421768188
Epoch 660, training loss: 0.015973243862390518 = 0.00880398415029049 + 0.001 * 7.169260501861572
Epoch 660, val loss: 1.6357338428497314
Epoch 670, training loss: 0.015526708215475082 = 0.00835356954485178 + 0.001 * 7.173137664794922
Epoch 670, val loss: 1.6487245559692383
Epoch 680, training loss: 0.015151342377066612 = 0.007935059256851673 + 0.001 * 7.216283321380615
Epoch 680, val loss: 1.6615163087844849
Epoch 690, training loss: 0.014713181182742119 = 0.007545255124568939 + 0.001 * 7.1679253578186035
Epoch 690, val loss: 1.6740851402282715
Epoch 700, training loss: 0.014352505095303059 = 0.0071812644600868225 + 0.001 * 7.171240329742432
Epoch 700, val loss: 1.6864750385284424
Epoch 710, training loss: 0.01401927974075079 = 0.00684098107740283 + 0.001 * 7.178298473358154
Epoch 710, val loss: 1.6986637115478516
Epoch 720, training loss: 0.013688158243894577 = 0.006522633600980043 + 0.001 * 7.165524482727051
Epoch 720, val loss: 1.7106471061706543
Epoch 730, training loss: 0.01339831855148077 = 0.006224650423973799 + 0.001 * 7.173667907714844
Epoch 730, val loss: 1.722441554069519
Epoch 740, training loss: 0.0131069365888834 = 0.0059455507434904575 + 0.001 * 7.1613850593566895
Epoch 740, val loss: 1.7340036630630493
Epoch 750, training loss: 0.012837816029787064 = 0.005683571100234985 + 0.001 * 7.154244899749756
Epoch 750, val loss: 1.745378017425537
Epoch 760, training loss: 0.012588100507855415 = 0.005436527542769909 + 0.001 * 7.151573181152344
Epoch 760, val loss: 1.7565380334854126
Epoch 770, training loss: 0.01235947571694851 = 0.005202696658670902 + 0.001 * 7.156778335571289
Epoch 770, val loss: 1.767524003982544
Epoch 780, training loss: 0.012120414525270462 = 0.004981024656444788 + 0.001 * 7.139389991760254
Epoch 780, val loss: 1.7783219814300537
Epoch 790, training loss: 0.01192131545394659 = 0.004771045409142971 + 0.001 * 7.150269508361816
Epoch 790, val loss: 1.7889600992202759
Epoch 800, training loss: 0.011722171679139137 = 0.004572418052703142 + 0.001 * 7.149753093719482
Epoch 800, val loss: 1.7994033098220825
Epoch 810, training loss: 0.0115294074639678 = 0.004384767729789019 + 0.001 * 7.144639492034912
Epoch 810, val loss: 1.8096531629562378
Epoch 820, training loss: 0.011342634446918964 = 0.004207768011838198 + 0.001 * 7.134866237640381
Epoch 820, val loss: 1.8197418451309204
Epoch 830, training loss: 0.011200781911611557 = 0.004040921106934547 + 0.001 * 7.159860610961914
Epoch 830, val loss: 1.8295925855636597
Epoch 840, training loss: 0.011025585234165192 = 0.0038837434258311987 + 0.001 * 7.141841411590576
Epoch 840, val loss: 1.8392751216888428
Epoch 850, training loss: 0.01086445152759552 = 0.003735689911991358 + 0.001 * 7.1287617683410645
Epoch 850, val loss: 1.848724365234375
Epoch 860, training loss: 0.010726768523454666 = 0.0035961847752332687 + 0.001 * 7.130583763122559
Epoch 860, val loss: 1.8579814434051514
Epoch 870, training loss: 0.010582826100289822 = 0.003464811947196722 + 0.001 * 7.118013858795166
Epoch 870, val loss: 1.8670334815979004
Epoch 880, training loss: 0.010473225265741348 = 0.003340977942571044 + 0.001 * 7.132246971130371
Epoch 880, val loss: 1.8758984804153442
Epoch 890, training loss: 0.010349201038479805 = 0.003224194748327136 + 0.001 * 7.125005722045898
Epoch 890, val loss: 1.8845419883728027
Epoch 900, training loss: 0.010231375694274902 = 0.0031140323262661695 + 0.001 * 7.117342948913574
Epoch 900, val loss: 1.8930039405822754
Epoch 910, training loss: 0.010116365738213062 = 0.0030100506264716387 + 0.001 * 7.1063151359558105
Epoch 910, val loss: 1.9012939929962158
Epoch 920, training loss: 0.010029478929936886 = 0.0029118007514625788 + 0.001 * 7.117677688598633
Epoch 920, val loss: 1.9094043970108032
Epoch 930, training loss: 0.009949272498488426 = 0.002818902488797903 + 0.001 * 7.130370140075684
Epoch 930, val loss: 1.917321801185608
Epoch 940, training loss: 0.00983535684645176 = 0.0027309919241815805 + 0.001 * 7.10436487197876
Epoch 940, val loss: 1.9250707626342773
Epoch 950, training loss: 0.00975792482495308 = 0.0026476974599063396 + 0.001 * 7.110227584838867
Epoch 950, val loss: 1.9326694011688232
Epoch 960, training loss: 0.009686581790447235 = 0.0025687392335385084 + 0.001 * 7.117842197418213
Epoch 960, val loss: 1.9400840997695923
Epoch 970, training loss: 0.009605149738490582 = 0.0024938953574746847 + 0.001 * 7.1112542152404785
Epoch 970, val loss: 1.9473743438720703
Epoch 980, training loss: 0.009546983055770397 = 0.0024228275287896395 + 0.001 * 7.124155044555664
Epoch 980, val loss: 1.954468846321106
Epoch 990, training loss: 0.009463218972086906 = 0.0023553441278636456 + 0.001 * 7.107874870300293
Epoch 990, val loss: 1.961461067199707
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.7965208223510807
The final CL Acc:0.74198, 0.03439, The final GNN Acc:0.79986, 0.00334
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13188])
remove edge: torch.Size([2, 8040])
updated graph: torch.Size([2, 10672])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9625221490859985 = 1.953925371170044 + 0.001 * 8.59677791595459
Epoch 0, val loss: 1.9545323848724365
Epoch 10, training loss: 1.9524685144424438 = 1.9438718557357788 + 0.001 * 8.596686363220215
Epoch 10, val loss: 1.9451273679733276
Epoch 20, training loss: 1.940083622932434 = 1.9314872026443481 + 0.001 * 8.596423149108887
Epoch 20, val loss: 1.9333864450454712
Epoch 30, training loss: 1.9228591918945312 = 1.914263367652893 + 0.001 * 8.595881462097168
Epoch 30, val loss: 1.9170736074447632
Epoch 40, training loss: 1.8974013328552246 = 1.888806700706482 + 0.001 * 8.594612121582031
Epoch 40, val loss: 1.8932362794876099
Epoch 50, training loss: 1.8603885173797607 = 1.8517975807189941 + 0.001 * 8.590928077697754
Epoch 50, val loss: 1.859570026397705
Epoch 60, training loss: 1.814300775527954 = 1.8057242631912231 + 0.001 * 8.576518058776855
Epoch 60, val loss: 1.8192954063415527
Epoch 70, training loss: 1.7697397470474243 = 1.761245608329773 + 0.001 * 8.494166374206543
Epoch 70, val loss: 1.7805439233779907
Epoch 80, training loss: 1.7216954231262207 = 1.7136560678482056 + 0.001 * 8.039352416992188
Epoch 80, val loss: 1.736299753189087
Epoch 90, training loss: 1.6559171676635742 = 1.6480495929718018 + 0.001 * 7.867589950561523
Epoch 90, val loss: 1.678784966468811
Epoch 100, training loss: 1.5697412490844727 = 1.5619910955429077 + 0.001 * 7.75013542175293
Epoch 100, val loss: 1.606182336807251
Epoch 110, training loss: 1.4665637016296387 = 1.4588823318481445 + 0.001 * 7.681411266326904
Epoch 110, val loss: 1.5202661752700806
Epoch 120, training loss: 1.3574308156967163 = 1.3497817516326904 + 0.001 * 7.649021625518799
Epoch 120, val loss: 1.4286798238754272
Epoch 130, training loss: 1.2532858848571777 = 1.2456729412078857 + 0.001 * 7.612889289855957
Epoch 130, val loss: 1.3417577743530273
Epoch 140, training loss: 1.1609408855438232 = 1.1533998250961304 + 0.001 * 7.541032791137695
Epoch 140, val loss: 1.265418291091919
Epoch 150, training loss: 1.0813939571380615 = 1.0739657878875732 + 0.001 * 7.428169250488281
Epoch 150, val loss: 1.2014354467391968
Epoch 160, training loss: 1.012394666671753 = 1.0050480365753174 + 0.001 * 7.346667766571045
Epoch 160, val loss: 1.1478960514068604
Epoch 170, training loss: 0.9498433470726013 = 0.9425065517425537 + 0.001 * 7.33681583404541
Epoch 170, val loss: 1.1005003452301025
Epoch 180, training loss: 0.889456570148468 = 0.8821341395378113 + 0.001 * 7.322452068328857
Epoch 180, val loss: 1.0549719333648682
Epoch 190, training loss: 0.8285646438598633 = 0.821262001991272 + 0.001 * 7.302649021148682
Epoch 190, val loss: 1.0088512897491455
Epoch 200, training loss: 0.7666125297546387 = 0.7593390941619873 + 0.001 * 7.273423671722412
Epoch 200, val loss: 0.9617698192596436
Epoch 210, training loss: 0.7050616145133972 = 0.6978244781494141 + 0.001 * 7.237109661102295
Epoch 210, val loss: 0.9156325459480286
Epoch 220, training loss: 0.6456416845321655 = 0.6384326219558716 + 0.001 * 7.209072113037109
Epoch 220, val loss: 0.872826874256134
Epoch 230, training loss: 0.5895707607269287 = 0.5823829174041748 + 0.001 * 7.187864303588867
Epoch 230, val loss: 0.8350731134414673
Epoch 240, training loss: 0.5371640920639038 = 0.5299891829490662 + 0.001 * 7.174932956695557
Epoch 240, val loss: 0.8030762672424316
Epoch 250, training loss: 0.4878943860530853 = 0.48072513937950134 + 0.001 * 7.169254779815674
Epoch 250, val loss: 0.7765257954597473
Epoch 260, training loss: 0.44073477387428284 = 0.4335702061653137 + 0.001 * 7.164562225341797
Epoch 260, val loss: 0.7545914649963379
Epoch 270, training loss: 0.3948797583580017 = 0.3877200186252594 + 0.001 * 7.159742832183838
Epoch 270, val loss: 0.7365912795066833
Epoch 280, training loss: 0.35033637285232544 = 0.34318068623542786 + 0.001 * 7.155698299407959
Epoch 280, val loss: 0.7219945788383484
Epoch 290, training loss: 0.30800819396972656 = 0.30085623264312744 + 0.001 * 7.151962757110596
Epoch 290, val loss: 0.710529088973999
Epoch 300, training loss: 0.2692142724990845 = 0.26206839084625244 + 0.001 * 7.145872116088867
Epoch 300, val loss: 0.7027844786643982
Epoch 310, training loss: 0.23493827879428864 = 0.22779613733291626 + 0.001 * 7.142136573791504
Epoch 310, val loss: 0.6989585161209106
Epoch 320, training loss: 0.20547744631767273 = 0.19833317399024963 + 0.001 * 7.1442742347717285
Epoch 320, val loss: 0.6990363001823425
Epoch 330, training loss: 0.18047752976417542 = 0.17333930730819702 + 0.001 * 7.138215065002441
Epoch 330, val loss: 0.7025850415229797
Epoch 340, training loss: 0.15931503474712372 = 0.15217943489551544 + 0.001 * 7.135597229003906
Epoch 340, val loss: 0.7088627815246582
Epoch 350, training loss: 0.1413203477859497 = 0.13418856263160706 + 0.001 * 7.131789684295654
Epoch 350, val loss: 0.7173144817352295
Epoch 360, training loss: 0.12591144442558289 = 0.11877158284187317 + 0.001 * 7.139858245849609
Epoch 360, val loss: 0.7273716330528259
Epoch 370, training loss: 0.11256711930036545 = 0.10543625801801682 + 0.001 * 7.130864143371582
Epoch 370, val loss: 0.7385355830192566
Epoch 380, training loss: 0.10094524919986725 = 0.09382466971874237 + 0.001 * 7.120580196380615
Epoch 380, val loss: 0.750607430934906
Epoch 390, training loss: 0.09078013896942139 = 0.08365830779075623 + 0.001 * 7.121832370758057
Epoch 390, val loss: 0.7631634473800659
Epoch 400, training loss: 0.08184865862131119 = 0.07473647594451904 + 0.001 * 7.112181186676025
Epoch 400, val loss: 0.776090145111084
Epoch 410, training loss: 0.07401658594608307 = 0.0668964609503746 + 0.001 * 7.120125770568848
Epoch 410, val loss: 0.7892262935638428
Epoch 420, training loss: 0.0671086460351944 = 0.06000165641307831 + 0.001 * 7.106989860534668
Epoch 420, val loss: 0.8024065494537354
Epoch 430, training loss: 0.061040617525577545 = 0.05393119156360626 + 0.001 * 7.1094255447387695
Epoch 430, val loss: 0.815570592880249
Epoch 440, training loss: 0.05568322539329529 = 0.048584017902612686 + 0.001 * 7.099205493927002
Epoch 440, val loss: 0.8286306858062744
Epoch 450, training loss: 0.050975147634744644 = 0.04387027025222778 + 0.001 * 7.1048784255981445
Epoch 450, val loss: 0.8415038585662842
Epoch 460, training loss: 0.0468059740960598 = 0.039711758494377136 + 0.001 * 7.09421443939209
Epoch 460, val loss: 0.8540858030319214
Epoch 470, training loss: 0.04313339293003082 = 0.03603965789079666 + 0.001 * 7.093733787536621
Epoch 470, val loss: 0.8663520812988281
Epoch 480, training loss: 0.039889805018901825 = 0.03279384225606918 + 0.001 * 7.095961093902588
Epoch 480, val loss: 0.8782894015312195
Epoch 490, training loss: 0.037009112536907196 = 0.02992173098027706 + 0.001 * 7.087381362915039
Epoch 490, val loss: 0.8898977637290955
Epoch 500, training loss: 0.03447798267006874 = 0.027376698330044746 + 0.001 * 7.101282596588135
Epoch 500, val loss: 0.9011878967285156
Epoch 510, training loss: 0.032203253358602524 = 0.025117233395576477 + 0.001 * 7.086018085479736
Epoch 510, val loss: 0.9121384620666504
Epoch 520, training loss: 0.030198879539966583 = 0.023107675835490227 + 0.001 * 7.091203689575195
Epoch 520, val loss: 0.9227384328842163
Epoch 530, training loss: 0.028398044407367706 = 0.021316705271601677 + 0.001 * 7.081338882446289
Epoch 530, val loss: 0.9330253601074219
Epoch 540, training loss: 0.02679901197552681 = 0.019716620445251465 + 0.001 * 7.082390308380127
Epoch 540, val loss: 0.9429523944854736
Epoch 550, training loss: 0.025365307927131653 = 0.018283693119883537 + 0.001 * 7.081614017486572
Epoch 550, val loss: 0.9525553584098816
Epoch 560, training loss: 0.0240786150097847 = 0.016997378319501877 + 0.001 * 7.081236362457275
Epoch 560, val loss: 0.9618546366691589
Epoch 570, training loss: 0.022916799411177635 = 0.015839658677577972 + 0.001 * 7.0771403312683105
Epoch 570, val loss: 0.9708619117736816
Epoch 580, training loss: 0.02189284935593605 = 0.014795012772083282 + 0.001 * 7.097835540771484
Epoch 580, val loss: 0.9795823693275452
Epoch 590, training loss: 0.020930981263518333 = 0.013850213959813118 + 0.001 * 7.080766677856445
Epoch 590, val loss: 0.9880518913269043
Epoch 600, training loss: 0.020070336759090424 = 0.012993530370295048 + 0.001 * 7.076805591583252
Epoch 600, val loss: 0.9962594509124756
Epoch 610, training loss: 0.019285425543785095 = 0.012214756570756435 + 0.001 * 7.070668697357178
Epoch 610, val loss: 1.0042403936386108
Epoch 620, training loss: 0.01857781410217285 = 0.01150493137538433 + 0.001 * 7.072882175445557
Epoch 620, val loss: 1.0119744539260864
Epoch 630, training loss: 0.01793782040476799 = 0.010855790227651596 + 0.001 * 7.082029819488525
Epoch 630, val loss: 1.0195012092590332
Epoch 640, training loss: 0.017332440242171288 = 0.010259026661515236 + 0.001 * 7.073413848876953
Epoch 640, val loss: 1.0268399715423584
Epoch 650, training loss: 0.016774222254753113 = 0.009706833399832249 + 0.001 * 7.067388534545898
Epoch 650, val loss: 1.0340265035629272
Epoch 660, training loss: 0.016261905431747437 = 0.009193829260766506 + 0.001 * 7.0680766105651855
Epoch 660, val loss: 1.041082501411438
Epoch 670, training loss: 0.015779957175254822 = 0.008716649375855923 + 0.001 * 7.0633063316345215
Epoch 670, val loss: 1.048019289970398
Epoch 680, training loss: 0.015339064411818981 = 0.00827261246740818 + 0.001 * 7.066451549530029
Epoch 680, val loss: 1.0548210144042969
Epoch 690, training loss: 0.014932380989193916 = 0.007859553210437298 + 0.001 * 7.072827339172363
Epoch 690, val loss: 1.0614840984344482
Epoch 700, training loss: 0.014544568955898285 = 0.007475315127521753 + 0.001 * 7.069253444671631
Epoch 700, val loss: 1.068019986152649
Epoch 710, training loss: 0.014196628704667091 = 0.007117938715964556 + 0.001 * 7.078689098358154
Epoch 710, val loss: 1.0744110345840454
Epoch 720, training loss: 0.01384849101305008 = 0.006785370409488678 + 0.001 * 7.0631208419799805
Epoch 720, val loss: 1.0806690454483032
Epoch 730, training loss: 0.013527028262615204 = 0.0064757405780255795 + 0.001 * 7.051287651062012
Epoch 730, val loss: 1.0867935419082642
Epoch 740, training loss: 0.013269728049635887 = 0.006187222898006439 + 0.001 * 7.0825042724609375
Epoch 740, val loss: 1.0927752256393433
Epoch 750, training loss: 0.01297767087817192 = 0.005918191745877266 + 0.001 * 7.059479236602783
Epoch 750, val loss: 1.0986273288726807
Epoch 760, training loss: 0.012715719640254974 = 0.005667063407599926 + 0.001 * 7.048656463623047
Epoch 760, val loss: 1.1043553352355957
Epoch 770, training loss: 0.012509710155427456 = 0.005432368256151676 + 0.001 * 7.077341556549072
Epoch 770, val loss: 1.1099517345428467
Epoch 780, training loss: 0.01226017251610756 = 0.005212839692831039 + 0.001 * 7.047332286834717
Epoch 780, val loss: 1.1154263019561768
Epoch 790, training loss: 0.012052202597260475 = 0.005007260013371706 + 0.001 * 7.0449419021606445
Epoch 790, val loss: 1.1207845211029053
Epoch 800, training loss: 0.011863818392157555 = 0.004814497195184231 + 0.001 * 7.049321174621582
Epoch 800, val loss: 1.126003623008728
Epoch 810, training loss: 0.011677926406264305 = 0.004633521661162376 + 0.001 * 7.044404029846191
Epoch 810, val loss: 1.131130337715149
Epoch 820, training loss: 0.011515723541378975 = 0.004463450517505407 + 0.001 * 7.052273273468018
Epoch 820, val loss: 1.1361297369003296
Epoch 830, training loss: 0.011341145262122154 = 0.004303432535380125 + 0.001 * 7.037712097167969
Epoch 830, val loss: 1.1410378217697144
Epoch 840, training loss: 0.011213188990950584 = 0.0041526989080011845 + 0.001 * 7.060489177703857
Epoch 840, val loss: 1.1458311080932617
Epoch 850, training loss: 0.011056584306061268 = 0.0040106079541146755 + 0.001 * 7.045976161956787
Epoch 850, val loss: 1.1505286693572998
Epoch 860, training loss: 0.010919874534010887 = 0.0038764846976846457 + 0.001 * 7.043389797210693
Epoch 860, val loss: 1.1551247835159302
Epoch 870, training loss: 0.010790770873427391 = 0.0037497698795050383 + 0.001 * 7.041000843048096
Epoch 870, val loss: 1.159628987312317
Epoch 880, training loss: 0.01067951787263155 = 0.0036299170460551977 + 0.001 * 7.049600601196289
Epoch 880, val loss: 1.1640349626541138
Epoch 890, training loss: 0.01054617390036583 = 0.003516456577926874 + 0.001 * 7.029716491699219
Epoch 890, val loss: 1.1683566570281982
Epoch 900, training loss: 0.010438757948577404 = 0.0034089309629052877 + 0.001 * 7.029826641082764
Epoch 900, val loss: 1.1725984811782837
Epoch 910, training loss: 0.01034105010330677 = 0.003306954400613904 + 0.001 * 7.034095287322998
Epoch 910, val loss: 1.1767441034317017
Epoch 920, training loss: 0.010234950110316277 = 0.0032101720571517944 + 0.001 * 7.024777412414551
Epoch 920, val loss: 1.180804967880249
Epoch 930, training loss: 0.010137196630239487 = 0.003118219319730997 + 0.001 * 7.018977165222168
Epoch 930, val loss: 1.18479323387146
Epoch 940, training loss: 0.010057243518531322 = 0.0030307823326438665 + 0.001 * 7.026460647583008
Epoch 940, val loss: 1.1886909008026123
Epoch 950, training loss: 0.009982062503695488 = 0.0029475530609488487 + 0.001 * 7.034509181976318
Epoch 950, val loss: 1.1925177574157715
Epoch 960, training loss: 0.00988187175244093 = 0.002868291223421693 + 0.001 * 7.013580322265625
Epoch 960, val loss: 1.196259617805481
Epoch 970, training loss: 0.00981675460934639 = 0.002792743733152747 + 0.001 * 7.024010181427002
Epoch 970, val loss: 1.1999317407608032
Epoch 980, training loss: 0.009740186855196953 = 0.0027206861414015293 + 0.001 * 7.019500732421875
Epoch 980, val loss: 1.2035305500030518
Epoch 990, training loss: 0.009671098552644253 = 0.002651904011145234 + 0.001 * 7.01919412612915
Epoch 990, val loss: 1.2070649862289429
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8313125988402742
=== training gcn model ===
Epoch 0, training loss: 1.9430983066558838 = 1.9345015287399292 + 0.001 * 8.596795082092285
Epoch 0, val loss: 1.9407840967178345
Epoch 10, training loss: 1.9335200786590576 = 1.9249234199523926 + 0.001 * 8.596710205078125
Epoch 10, val loss: 1.9304646253585815
Epoch 20, training loss: 1.921379804611206 = 1.9127833843231201 + 0.001 * 8.596452713012695
Epoch 20, val loss: 1.917155385017395
Epoch 30, training loss: 1.903889536857605 = 1.8952937126159668 + 0.001 * 8.595821380615234
Epoch 30, val loss: 1.8979485034942627
Epoch 40, training loss: 1.8776642084121704 = 1.8690699338912964 + 0.001 * 8.594223976135254
Epoch 40, val loss: 1.8696060180664062
Epoch 50, training loss: 1.8417246341705322 = 1.8331352472305298 + 0.001 * 8.589332580566406
Epoch 50, val loss: 1.8329719305038452
Epoch 60, training loss: 1.803900957107544 = 1.7953308820724487 + 0.001 * 8.570058822631836
Epoch 60, val loss: 1.798916220664978
Epoch 70, training loss: 1.7695449590682983 = 1.761091947555542 + 0.001 * 8.453007698059082
Epoch 70, val loss: 1.7707372903823853
Epoch 80, training loss: 1.7225168943405151 = 1.7144005298614502 + 0.001 * 8.116332054138184
Epoch 80, val loss: 1.7302939891815186
Epoch 90, training loss: 1.6570998430252075 = 1.6492362022399902 + 0.001 * 7.863643169403076
Epoch 90, val loss: 1.6737614870071411
Epoch 100, training loss: 1.5724222660064697 = 1.564736008644104 + 0.001 * 7.686241149902344
Epoch 100, val loss: 1.6031334400177002
Epoch 110, training loss: 1.4771054983139038 = 1.4694883823394775 + 0.001 * 7.617103576660156
Epoch 110, val loss: 1.5252596139907837
Epoch 120, training loss: 1.3799504041671753 = 1.3723851442337036 + 0.001 * 7.565309047698975
Epoch 120, val loss: 1.4471862316131592
Epoch 130, training loss: 1.2838584184646606 = 1.2763597965240479 + 0.001 * 7.49862813949585
Epoch 130, val loss: 1.3709431886672974
Epoch 140, training loss: 1.189220905303955 = 1.1817635297775269 + 0.001 * 7.457329750061035
Epoch 140, val loss: 1.2966406345367432
Epoch 150, training loss: 1.098974585533142 = 1.0915426015853882 + 0.001 * 7.431931972503662
Epoch 150, val loss: 1.226629376411438
Epoch 160, training loss: 1.0161738395690918 = 1.0087765455245972 + 0.001 * 7.397286415100098
Epoch 160, val loss: 1.16322660446167
Epoch 170, training loss: 0.9409343004226685 = 0.9335792660713196 + 0.001 * 7.3550286293029785
Epoch 170, val loss: 1.1059592962265015
Epoch 180, training loss: 0.8710307478904724 = 0.8637169003486633 + 0.001 * 7.313859939575195
Epoch 180, val loss: 1.0523022413253784
Epoch 190, training loss: 0.8041914105415344 = 0.7969171404838562 + 0.001 * 7.274240970611572
Epoch 190, val loss: 1.0003584623336792
Epoch 200, training loss: 0.7392281889915466 = 0.7319747805595398 + 0.001 * 7.253417015075684
Epoch 200, val loss: 0.9491779208183289
Epoch 210, training loss: 0.6760115623474121 = 0.6687683463096619 + 0.001 * 7.2432403564453125
Epoch 210, val loss: 0.8996102213859558
Epoch 220, training loss: 0.6146343946456909 = 0.6073946356773376 + 0.001 * 7.2397379875183105
Epoch 220, val loss: 0.8528122901916504
Epoch 230, training loss: 0.5549235939979553 = 0.5476852655410767 + 0.001 * 7.238338947296143
Epoch 230, val loss: 0.8096485137939453
Epoch 240, training loss: 0.4965333640575409 = 0.48929667472839355 + 0.001 * 7.2366943359375
Epoch 240, val loss: 0.7711156010627747
Epoch 250, training loss: 0.43961891531944275 = 0.43238362669944763 + 0.001 * 7.235275745391846
Epoch 250, val loss: 0.7372050285339355
Epoch 260, training loss: 0.3849804103374481 = 0.3777463138103485 + 0.001 * 7.234083652496338
Epoch 260, val loss: 0.7080208659172058
Epoch 270, training loss: 0.33378326892852783 = 0.326546847820282 + 0.001 * 7.236407279968262
Epoch 270, val loss: 0.6833711862564087
Epoch 280, training loss: 0.28699877858161926 = 0.2797647714614868 + 0.001 * 7.234013557434082
Epoch 280, val loss: 0.6631954312324524
Epoch 290, training loss: 0.24533244967460632 = 0.23809956014156342 + 0.001 * 7.232893943786621
Epoch 290, val loss: 0.6478261351585388
Epoch 300, training loss: 0.20904451608657837 = 0.2018113136291504 + 0.001 * 7.233203411102295
Epoch 300, val loss: 0.6375241279602051
Epoch 310, training loss: 0.17803511023521423 = 0.17079970240592957 + 0.001 * 7.235409736633301
Epoch 310, val loss: 0.6321129202842712
Epoch 320, training loss: 0.1519283503293991 = 0.14469359815120697 + 0.001 * 7.234750747680664
Epoch 320, val loss: 0.6311221122741699
Epoch 330, training loss: 0.13018903136253357 = 0.12295302003622055 + 0.001 * 7.236013412475586
Epoch 330, val loss: 0.633996844291687
Epoch 340, training loss: 0.11217621713876724 = 0.10494077950716019 + 0.001 * 7.2354350090026855
Epoch 340, val loss: 0.6399631500244141
Epoch 350, training loss: 0.09728250652551651 = 0.09004614502191544 + 0.001 * 7.236358165740967
Epoch 350, val loss: 0.6481549739837646
Epoch 360, training loss: 0.08494743704795837 = 0.0777110755443573 + 0.001 * 7.236362457275391
Epoch 360, val loss: 0.6578072905540466
Epoch 370, training loss: 0.07469454407691956 = 0.06745646148920059 + 0.001 * 7.238079071044922
Epoch 370, val loss: 0.668487012386322
Epoch 380, training loss: 0.06612846255302429 = 0.05889284238219261 + 0.001 * 7.235623359680176
Epoch 380, val loss: 0.6798323392868042
Epoch 390, training loss: 0.05894944444298744 = 0.05171143263578415 + 0.001 * 7.238009929656982
Epoch 390, val loss: 0.6915607452392578
Epoch 400, training loss: 0.05289710313081741 = 0.04566127061843872 + 0.001 * 7.235833168029785
Epoch 400, val loss: 0.7034907341003418
Epoch 410, training loss: 0.04778427258133888 = 0.04054183140397072 + 0.001 * 7.2424397468566895
Epoch 410, val loss: 0.7154537439346313
Epoch 420, training loss: 0.043425966054201126 = 0.03619162738323212 + 0.001 * 7.234336853027344
Epoch 420, val loss: 0.7273945212364197
Epoch 430, training loss: 0.03971252962946892 = 0.0324774906039238 + 0.001 * 7.235039710998535
Epoch 430, val loss: 0.7391829490661621
Epoch 440, training loss: 0.03651979938149452 = 0.029290344566106796 + 0.001 * 7.229454517364502
Epoch 440, val loss: 0.7507224678993225
Epoch 450, training loss: 0.03376784175634384 = 0.02653978392481804 + 0.001 * 7.228056907653809
Epoch 450, val loss: 0.7619950175285339
Epoch 460, training loss: 0.03138351067900658 = 0.02415345422923565 + 0.001 * 7.230057716369629
Epoch 460, val loss: 0.7729607820510864
Epoch 470, training loss: 0.029296111315488815 = 0.02207206003367901 + 0.001 * 7.224050045013428
Epoch 470, val loss: 0.7836184501647949
Epoch 480, training loss: 0.027480704709887505 = 0.020247194916009903 + 0.001 * 7.233509540557861
Epoch 480, val loss: 0.7939486503601074
Epoch 490, training loss: 0.02586216665804386 = 0.018639443442225456 + 0.001 * 7.222723007202148
Epoch 490, val loss: 0.8039891123771667
Epoch 500, training loss: 0.02443072199821472 = 0.017216596752405167 + 0.001 * 7.21412467956543
Epoch 500, val loss: 0.8137282133102417
Epoch 510, training loss: 0.0231665950268507 = 0.015951931476593018 + 0.001 * 7.214662551879883
Epoch 510, val loss: 0.8231939077377319
Epoch 520, training loss: 0.02203385904431343 = 0.01482325978577137 + 0.001 * 7.210598468780518
Epoch 520, val loss: 0.8323770761489868
Epoch 530, training loss: 0.021020010113716125 = 0.01381224300712347 + 0.001 * 7.207766056060791
Epoch 530, val loss: 0.8413049578666687
Epoch 540, training loss: 0.020107505843043327 = 0.012903221882879734 + 0.001 * 7.204283714294434
Epoch 540, val loss: 0.8499663472175598
Epoch 550, training loss: 0.019274037331342697 = 0.012083154171705246 + 0.001 * 7.190882682800293
Epoch 550, val loss: 0.8583876490592957
Epoch 560, training loss: 0.018550021573901176 = 0.011340952478349209 + 0.001 * 7.209068775177002
Epoch 560, val loss: 0.8665501475334167
Epoch 570, training loss: 0.017856217920780182 = 0.010667197406291962 + 0.001 * 7.189020156860352
Epoch 570, val loss: 0.874502420425415
Epoch 580, training loss: 0.017287204042077065 = 0.010053756646811962 + 0.001 * 7.233447551727295
Epoch 580, val loss: 0.8822314143180847
Epoch 590, training loss: 0.01667526178061962 = 0.009493790566921234 + 0.001 * 7.1814703941345215
Epoch 590, val loss: 0.8897494077682495
Epoch 600, training loss: 0.016207804903388023 = 0.008981280028820038 + 0.001 * 7.226524829864502
Epoch 600, val loss: 0.8970768451690674
Epoch 610, training loss: 0.015696853399276733 = 0.00851106084883213 + 0.001 * 7.185791969299316
Epoch 610, val loss: 0.9041971564292908
Epoch 620, training loss: 0.015273197554051876 = 0.008078668266534805 + 0.001 * 7.194529056549072
Epoch 620, val loss: 0.9111232757568359
Epoch 630, training loss: 0.014877460896968842 = 0.00768012972548604 + 0.001 * 7.197330474853516
Epoch 630, val loss: 0.9178794622421265
Epoch 640, training loss: 0.014469515532255173 = 0.007311968132853508 + 0.001 * 7.157547473907471
Epoch 640, val loss: 0.924462080001831
Epoch 650, training loss: 0.014187883585691452 = 0.006971209309995174 + 0.001 * 7.216674327850342
Epoch 650, val loss: 0.9308713674545288
Epoch 660, training loss: 0.01382480189204216 = 0.00665528466925025 + 0.001 * 7.1695170402526855
Epoch 660, val loss: 0.937126100063324
Epoch 670, training loss: 0.013512864708900452 = 0.00636186683550477 + 0.001 * 7.150997638702393
Epoch 670, val loss: 0.9432268738746643
Epoch 680, training loss: 0.013276517391204834 = 0.0060888007283210754 + 0.001 * 7.187716960906982
Epoch 680, val loss: 0.9491866230964661
Epoch 690, training loss: 0.012992640025913715 = 0.005834278650581837 + 0.001 * 7.158360958099365
Epoch 690, val loss: 0.9549980759620667
Epoch 700, training loss: 0.012759145349264145 = 0.00559672387316823 + 0.001 * 7.162420749664307
Epoch 700, val loss: 0.9606763124465942
Epoch 710, training loss: 0.012556183151900768 = 0.005374579690396786 + 0.001 * 7.181602954864502
Epoch 710, val loss: 0.9662229418754578
Epoch 720, training loss: 0.012289220467209816 = 0.005166572518646717 + 0.001 * 7.122647762298584
Epoch 720, val loss: 0.9716404676437378
Epoch 730, training loss: 0.01213478110730648 = 0.004971543326973915 + 0.001 * 7.16323709487915
Epoch 730, val loss: 0.9769278168678284
Epoch 740, training loss: 0.011979393661022186 = 0.004788426216691732 + 0.001 * 7.190967559814453
Epoch 740, val loss: 0.9820992946624756
Epoch 750, training loss: 0.011738434433937073 = 0.004616310819983482 + 0.001 * 7.122123718261719
Epoch 750, val loss: 0.9871527552604675
Epoch 760, training loss: 0.011595804244279861 = 0.004454278852790594 + 0.001 * 7.141524791717529
Epoch 760, val loss: 0.9920812249183655
Epoch 770, training loss: 0.011427788063883781 = 0.00430158618837595 + 0.001 * 7.126201629638672
Epoch 770, val loss: 0.9969197511672974
Epoch 780, training loss: 0.011267263442277908 = 0.004157498944550753 + 0.001 * 7.109764099121094
Epoch 780, val loss: 1.0016511678695679
Epoch 790, training loss: 0.011145776137709618 = 0.0040213861502707005 + 0.001 * 7.124389171600342
Epoch 790, val loss: 1.0062732696533203
Epoch 800, training loss: 0.011036046780645847 = 0.0038926945999264717 + 0.001 * 7.143352031707764
Epoch 800, val loss: 1.0107946395874023
Epoch 810, training loss: 0.010885541327297688 = 0.003770872252061963 + 0.001 * 7.114668846130371
Epoch 810, val loss: 1.0152226686477661
Epoch 820, training loss: 0.010831952095031738 = 0.0036554548423737288 + 0.001 * 7.176496505737305
Epoch 820, val loss: 1.0195790529251099
Epoch 830, training loss: 0.010652963072061539 = 0.0035459711216390133 + 0.001 * 7.106991767883301
Epoch 830, val loss: 1.0238244533538818
Epoch 840, training loss: 0.01054738461971283 = 0.0034420613665133715 + 0.001 * 7.10532283782959
Epoch 840, val loss: 1.0279929637908936
Epoch 850, training loss: 0.010452748276293278 = 0.0033433360513299704 + 0.001 * 7.10941219329834
Epoch 850, val loss: 1.0320719480514526
Epoch 860, training loss: 0.010354201309382915 = 0.003249452216550708 + 0.001 * 7.104748725891113
Epoch 860, val loss: 1.0360779762268066
Epoch 870, training loss: 0.010265577584505081 = 0.003160101128742099 + 0.001 * 7.105475902557373
Epoch 870, val loss: 1.0400009155273438
Epoch 880, training loss: 0.010167490690946579 = 0.003075018525123596 + 0.001 * 7.092471599578857
Epoch 880, val loss: 1.0438480377197266
Epoch 890, training loss: 0.010086053982377052 = 0.0029938933439552784 + 0.001 * 7.092159748077393
Epoch 890, val loss: 1.0476083755493164
Epoch 900, training loss: 0.009994013234972954 = 0.0029165144078433514 + 0.001 * 7.077498912811279
Epoch 900, val loss: 1.0513007640838623
Epoch 910, training loss: 0.009931603446602821 = 0.0028426414355635643 + 0.001 * 7.088961601257324
Epoch 910, val loss: 1.054931879043579
Epoch 920, training loss: 0.009870433248579502 = 0.0027720883954316378 + 0.001 * 7.098344326019287
Epoch 920, val loss: 1.0584814548492432
Epoch 930, training loss: 0.009827316738665104 = 0.00270461430773139 + 0.001 * 7.122702121734619
Epoch 930, val loss: 1.0619782209396362
Epoch 940, training loss: 0.009728094562888145 = 0.002640075981616974 + 0.001 * 7.088018894195557
Epoch 940, val loss: 1.0653965473175049
Epoch 950, training loss: 0.009666561149060726 = 0.0025782985612750053 + 0.001 * 7.08826208114624
Epoch 950, val loss: 1.0687363147735596
Epoch 960, training loss: 0.009577222168445587 = 0.002519126981496811 + 0.001 * 7.058094501495361
Epoch 960, val loss: 1.0720477104187012
Epoch 970, training loss: 0.009523929096758366 = 0.0024624010547995567 + 0.001 * 7.061527729034424
Epoch 970, val loss: 1.075273036956787
Epoch 980, training loss: 0.009483374655246735 = 0.002408013679087162 + 0.001 * 7.075361251831055
Epoch 980, val loss: 1.0784705877304077
Epoch 990, training loss: 0.009465259499847889 = 0.002355837495997548 + 0.001 * 7.109421253204346
Epoch 990, val loss: 1.081613540649414
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.9551175832748413 = 1.9465208053588867 + 0.001 * 8.5968017578125
Epoch 0, val loss: 1.9375327825546265
Epoch 10, training loss: 1.9449868202209473 = 1.9363900423049927 + 0.001 * 8.596738815307617
Epoch 10, val loss: 1.927689790725708
Epoch 20, training loss: 1.9325381517410278 = 1.9239416122436523 + 0.001 * 8.596535682678223
Epoch 20, val loss: 1.9152947664260864
Epoch 30, training loss: 1.914977788925171 = 1.9063817262649536 + 0.001 * 8.596061706542969
Epoch 30, val loss: 1.8977028131484985
Epoch 40, training loss: 1.8888936042785645 = 1.8802987337112427 + 0.001 * 8.594866752624512
Epoch 40, val loss: 1.8721566200256348
Epoch 50, training loss: 1.8525688648223877 = 1.8439775705337524 + 0.001 * 8.591288566589355
Epoch 50, val loss: 1.8384910821914673
Epoch 60, training loss: 1.8123705387115479 = 1.8037930727005005 + 0.001 * 8.577439308166504
Epoch 60, val loss: 1.8060553073883057
Epoch 70, training loss: 1.7774600982666016 = 1.7689464092254639 + 0.001 * 8.513667106628418
Epoch 70, val loss: 1.780531644821167
Epoch 80, training loss: 1.7343113422393799 = 1.726091980934143 + 0.001 * 8.219358444213867
Epoch 80, val loss: 1.7431366443634033
Epoch 90, training loss: 1.673823595046997 = 1.6657602787017822 + 0.001 * 8.063299179077148
Epoch 90, val loss: 1.6885327100753784
Epoch 100, training loss: 1.591450810432434 = 1.5835052728652954 + 0.001 * 7.945484161376953
Epoch 100, val loss: 1.6166380643844604
Epoch 110, training loss: 1.4944593906402588 = 1.4866470098495483 + 0.001 * 7.812340259552002
Epoch 110, val loss: 1.5364911556243896
Epoch 120, training loss: 1.3951523303985596 = 1.3875007629394531 + 0.001 * 7.6515655517578125
Epoch 120, val loss: 1.4578441381454468
Epoch 130, training loss: 1.2988442182540894 = 1.2912778854370117 + 0.001 * 7.566364288330078
Epoch 130, val loss: 1.3842055797576904
Epoch 140, training loss: 1.2058812379837036 = 1.1984010934829712 + 0.001 * 7.4801130294799805
Epoch 140, val loss: 1.3144536018371582
Epoch 150, training loss: 1.1178971529006958 = 1.1104618310928345 + 0.001 * 7.435327529907227
Epoch 150, val loss: 1.2487752437591553
Epoch 160, training loss: 1.037278175354004 = 1.0298652648925781 + 0.001 * 7.412900924682617
Epoch 160, val loss: 1.1897566318511963
Epoch 170, training loss: 0.9646978378295898 = 0.9573003053665161 + 0.001 * 7.397504806518555
Epoch 170, val loss: 1.1372369527816772
Epoch 180, training loss: 0.897920548915863 = 0.8905377984046936 + 0.001 * 7.382745265960693
Epoch 180, val loss: 1.0887014865875244
Epoch 190, training loss: 0.8336336612701416 = 0.826271116733551 + 0.001 * 7.362544059753418
Epoch 190, val loss: 1.0407322645187378
Epoch 200, training loss: 0.7696810364723206 = 0.7623462080955505 + 0.001 * 7.334832191467285
Epoch 200, val loss: 0.9919167757034302
Epoch 210, training loss: 0.7057778835296631 = 0.698477029800415 + 0.001 * 7.300857067108154
Epoch 210, val loss: 0.9424487948417664
Epoch 220, training loss: 0.6430716514587402 = 0.6357966065406799 + 0.001 * 7.275046348571777
Epoch 220, val loss: 0.894550621509552
Epoch 230, training loss: 0.5826952457427979 = 0.5754318237304688 + 0.001 * 7.2634077072143555
Epoch 230, val loss: 0.8500310182571411
Epoch 240, training loss: 0.5252403616905212 = 0.5179829597473145 + 0.001 * 7.2574143409729
Epoch 240, val loss: 0.8106760382652283
Epoch 250, training loss: 0.47103381156921387 = 0.4637787938117981 + 0.001 * 7.255002975463867
Epoch 250, val loss: 0.7775418758392334
Epoch 260, training loss: 0.4203656315803528 = 0.41311198472976685 + 0.001 * 7.253633499145508
Epoch 260, val loss: 0.7512959241867065
Epoch 270, training loss: 0.37360456585884094 = 0.366351455450058 + 0.001 * 7.2530999183654785
Epoch 270, val loss: 0.7325009107589722
Epoch 280, training loss: 0.3310832977294922 = 0.32383018732070923 + 0.001 * 7.253102779388428
Epoch 280, val loss: 0.7212585806846619
Epoch 290, training loss: 0.2929092049598694 = 0.2856557369232178 + 0.001 * 7.253468990325928
Epoch 290, val loss: 0.716842770576477
Epoch 300, training loss: 0.25889065861701965 = 0.2516365647315979 + 0.001 * 7.254087924957275
Epoch 300, val loss: 0.7181483507156372
Epoch 310, training loss: 0.22867636382579803 = 0.22142153978347778 + 0.001 * 7.254819869995117
Epoch 310, val loss: 0.7237401008605957
Epoch 320, training loss: 0.2019277960062027 = 0.1946699470281601 + 0.001 * 7.257848262786865
Epoch 320, val loss: 0.7326422929763794
Epoch 330, training loss: 0.1783302277326584 = 0.1710735261440277 + 0.001 * 7.256707191467285
Epoch 330, val loss: 0.744163990020752
Epoch 340, training loss: 0.1576187014579773 = 0.15036118030548096 + 0.001 * 7.257521629333496
Epoch 340, val loss: 0.7577309012413025
Epoch 350, training loss: 0.13954323530197144 = 0.13228550553321838 + 0.001 * 7.257724285125732
Epoch 350, val loss: 0.7728988528251648
Epoch 360, training loss: 0.12384581565856934 = 0.11658786237239838 + 0.001 * 7.257950782775879
Epoch 360, val loss: 0.7892977595329285
Epoch 370, training loss: 0.11026732623577118 = 0.103009432554245 + 0.001 * 7.2578911781311035
Epoch 370, val loss: 0.8066036701202393
Epoch 380, training loss: 0.09854874014854431 = 0.09128869324922562 + 0.001 * 7.2600483894348145
Epoch 380, val loss: 0.8244544267654419
Epoch 390, training loss: 0.08843068033456802 = 0.08117296546697617 + 0.001 * 7.257714748382568
Epoch 390, val loss: 0.8425992131233215
Epoch 400, training loss: 0.07968617975711823 = 0.07242920249700546 + 0.001 * 7.256975173950195
Epoch 400, val loss: 0.860838770866394
Epoch 410, training loss: 0.07210595905780792 = 0.06485040485858917 + 0.001 * 7.255551338195801
Epoch 410, val loss: 0.8789964318275452
Epoch 420, training loss: 0.06551341712474823 = 0.05826006829738617 + 0.001 * 7.253348350524902
Epoch 420, val loss: 0.8969804048538208
Epoch 430, training loss: 0.05976125970482826 = 0.05250934511423111 + 0.001 * 7.251914024353027
Epoch 430, val loss: 0.9147246479988098
Epoch 440, training loss: 0.05473579093813896 = 0.047474589198827744 + 0.001 * 7.261202335357666
Epoch 440, val loss: 0.9321518540382385
Epoch 450, training loss: 0.05029851943254471 = 0.04305368289351463 + 0.001 * 7.24483585357666
Epoch 450, val loss: 0.9491616487503052
Epoch 460, training loss: 0.04640413820743561 = 0.039160531014204025 + 0.001 * 7.243605136871338
Epoch 460, val loss: 0.9657348394393921
Epoch 470, training loss: 0.042957525700330734 = 0.03572170063853264 + 0.001 * 7.2358245849609375
Epoch 470, val loss: 0.9818328619003296
Epoch 480, training loss: 0.03991350159049034 = 0.03267509117722511 + 0.001 * 7.238409519195557
Epoch 480, val loss: 0.9974227547645569
Epoch 490, training loss: 0.037206538021564484 = 0.029968099668622017 + 0.001 * 7.238439083099365
Epoch 490, val loss: 1.0124856233596802
Epoch 500, training loss: 0.03478524833917618 = 0.027556011453270912 + 0.001 * 7.22923469543457
Epoch 500, val loss: 1.0270262956619263
Epoch 510, training loss: 0.032606542110443115 = 0.02540081925690174 + 0.001 * 7.205724239349365
Epoch 510, val loss: 1.0411100387573242
Epoch 520, training loss: 0.030678141862154007 = 0.02347026951611042 + 0.001 * 7.207871913909912
Epoch 520, val loss: 1.054697036743164
Epoch 530, training loss: 0.028953559696674347 = 0.021736623719334602 + 0.001 * 7.2169365882873535
Epoch 530, val loss: 1.0678372383117676
Epoch 540, training loss: 0.02740161307156086 = 0.02017594687640667 + 0.001 * 7.225666046142578
Epoch 540, val loss: 1.0805217027664185
Epoch 550, training loss: 0.025953935459256172 = 0.018767740577459335 + 0.001 * 7.18619441986084
Epoch 550, val loss: 1.0927613973617554
Epoch 560, training loss: 0.024671118706464767 = 0.017494183033704758 + 0.001 * 7.176934242248535
Epoch 560, val loss: 1.1045905351638794
Epoch 570, training loss: 0.023514525964856148 = 0.016339890658855438 + 0.001 * 7.1746344566345215
Epoch 570, val loss: 1.116015911102295
Epoch 580, training loss: 0.022462863475084305 = 0.015291455201804638 + 0.001 * 7.171408176422119
Epoch 580, val loss: 1.1270707845687866
Epoch 590, training loss: 0.021514106541872025 = 0.014337160624563694 + 0.001 * 7.176945686340332
Epoch 590, val loss: 1.137748122215271
Epoch 600, training loss: 0.02063414826989174 = 0.01346682757139206 + 0.001 * 7.167319297790527
Epoch 600, val loss: 1.1480932235717773
Epoch 610, training loss: 0.019838731735944748 = 0.012671559117734432 + 0.001 * 7.167171478271484
Epoch 610, val loss: 1.1580986976623535
Epoch 620, training loss: 0.019127685576677322 = 0.011943573132157326 + 0.001 * 7.184112071990967
Epoch 620, val loss: 1.1677827835083008
Epoch 630, training loss: 0.018444489687681198 = 0.011275963857769966 + 0.001 * 7.168526649475098
Epoch 630, val loss: 1.1771968603134155
Epoch 640, training loss: 0.017823003232479095 = 0.010662718676030636 + 0.001 * 7.160284519195557
Epoch 640, val loss: 1.1862996816635132
Epoch 650, training loss: 0.017264701426029205 = 0.0100983502343297 + 0.001 * 7.166350841522217
Epoch 650, val loss: 1.1951348781585693
Epoch 660, training loss: 0.016731059178709984 = 0.009577938355505466 + 0.001 * 7.153120994567871
Epoch 660, val loss: 1.2037041187286377
Epoch 670, training loss: 0.016254745423793793 = 0.009097285568714142 + 0.001 * 7.1574602127075195
Epoch 670, val loss: 1.2120469808578491
Epoch 680, training loss: 0.015810232609510422 = 0.008652596734464169 + 0.001 * 7.157634735107422
Epoch 680, val loss: 1.2201390266418457
Epoch 690, training loss: 0.0154075613245368 = 0.00824055727571249 + 0.001 * 7.167003631591797
Epoch 690, val loss: 1.2280133962631226
Epoch 700, training loss: 0.015022268518805504 = 0.00785816926509142 + 0.001 * 7.164098262786865
Epoch 700, val loss: 1.2356728315353394
Epoch 710, training loss: 0.014656702056527138 = 0.007502754218876362 + 0.001 * 7.153947830200195
Epoch 710, val loss: 1.243124008178711
Epoch 720, training loss: 0.014321422204375267 = 0.007171918172389269 + 0.001 * 7.149503231048584
Epoch 720, val loss: 1.250368595123291
Epoch 730, training loss: 0.014003325253725052 = 0.006863487418740988 + 0.001 * 7.139837265014648
Epoch 730, val loss: 1.2574306726455688
Epoch 740, training loss: 0.013727828860282898 = 0.006575556937605143 + 0.001 * 7.152271270751953
Epoch 740, val loss: 1.2643074989318848
Epoch 750, training loss: 0.013448022305965424 = 0.006306430324912071 + 0.001 * 7.141591548919678
Epoch 750, val loss: 1.2710304260253906
Epoch 760, training loss: 0.01319703459739685 = 0.006054537370800972 + 0.001 * 7.142496585845947
Epoch 760, val loss: 1.2775561809539795
Epoch 770, training loss: 0.01295759342610836 = 0.00581841915845871 + 0.001 * 7.13917350769043
Epoch 770, val loss: 1.283939003944397
Epoch 780, training loss: 0.012733101844787598 = 0.005596839357167482 + 0.001 * 7.136261940002441
Epoch 780, val loss: 1.2901815176010132
Epoch 790, training loss: 0.012537828646600246 = 0.0053886715322732925 + 0.001 * 7.14915657043457
Epoch 790, val loss: 1.2962597608566284
Epoch 800, training loss: 0.012326469644904137 = 0.005192817188799381 + 0.001 * 7.1336517333984375
Epoch 800, val loss: 1.3021870851516724
Epoch 810, training loss: 0.012144047766923904 = 0.005008407402783632 + 0.001 * 7.135639667510986
Epoch 810, val loss: 1.307987093925476
Epoch 820, training loss: 0.011962700635194778 = 0.004834518767893314 + 0.001 * 7.128181457519531
Epoch 820, val loss: 1.3136532306671143
Epoch 830, training loss: 0.01181041356176138 = 0.004670413676649332 + 0.001 * 7.1399993896484375
Epoch 830, val loss: 1.3191949129104614
Epoch 840, training loss: 0.011638887226581573 = 0.004515412263572216 + 0.001 * 7.123475074768066
Epoch 840, val loss: 1.3246129751205444
Epoch 850, training loss: 0.011482112109661102 = 0.004368853755295277 + 0.001 * 7.113257884979248
Epoch 850, val loss: 1.3298969268798828
Epoch 860, training loss: 0.011342845857143402 = 0.004230057820677757 + 0.001 * 7.11278772354126
Epoch 860, val loss: 1.3350753784179688
Epoch 870, training loss: 0.011211947537958622 = 0.004098611883819103 + 0.001 * 7.113335132598877
Epoch 870, val loss: 1.3401455879211426
Epoch 880, training loss: 0.011081046424806118 = 0.003973891492933035 + 0.001 * 7.107154369354248
Epoch 880, val loss: 1.3450936079025269
Epoch 890, training loss: 0.010968782007694244 = 0.003855518065392971 + 0.001 * 7.1132636070251465
Epoch 890, val loss: 1.3499643802642822
Epoch 900, training loss: 0.01086080726236105 = 0.0037430438678711653 + 0.001 * 7.117763519287109
Epoch 900, val loss: 1.3547275066375732
Epoch 910, training loss: 0.010746806859970093 = 0.003636074485257268 + 0.001 * 7.110732078552246
Epoch 910, val loss: 1.3593897819519043
Epoch 920, training loss: 0.010633887723088264 = 0.0035342739429324865 + 0.001 * 7.099613666534424
Epoch 920, val loss: 1.3639570474624634
Epoch 930, training loss: 0.010537487454712391 = 0.0034373095259070396 + 0.001 * 7.100177764892578
Epoch 930, val loss: 1.3684476613998413
Epoch 940, training loss: 0.010464431717991829 = 0.0033448729664087296 + 0.001 * 7.119558811187744
Epoch 940, val loss: 1.372833490371704
Epoch 950, training loss: 0.010374220088124275 = 0.0032566802110522985 + 0.001 * 7.117539405822754
Epoch 950, val loss: 1.37712562084198
Epoch 960, training loss: 0.010271256789565086 = 0.003172453725710511 + 0.001 * 7.0988030433654785
Epoch 960, val loss: 1.3813445568084717
Epoch 970, training loss: 0.010179842822253704 = 0.0030919683631509542 + 0.001 * 7.087874412536621
Epoch 970, val loss: 1.385482907295227
Epoch 980, training loss: 0.010101842693984509 = 0.0030149929225444794 + 0.001 * 7.086849212646484
Epoch 980, val loss: 1.3895289897918701
Epoch 990, training loss: 0.010027199983596802 = 0.002941326703876257 + 0.001 * 7.085873126983643
Epoch 990, val loss: 1.3935304880142212
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8434370057986295
The final CL Acc:0.81481, 0.02283, The final GNN Acc:0.83711, 0.00496
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11672])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10540])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.950137972831726 = 1.9415411949157715 + 0.001 * 8.59679889678955
Epoch 0, val loss: 1.9435399770736694
Epoch 10, training loss: 1.9408107995986938 = 1.9322140216827393 + 0.001 * 8.596766471862793
Epoch 10, val loss: 1.9341446161270142
Epoch 20, training loss: 1.9293965101242065 = 1.9207998514175415 + 0.001 * 8.5966215133667
Epoch 20, val loss: 1.9226868152618408
Epoch 30, training loss: 1.9134678840637207 = 1.9048715829849243 + 0.001 * 8.59628963470459
Epoch 30, val loss: 1.9067481756210327
Epoch 40, training loss: 1.8900065422058105 = 1.881411075592041 + 0.001 * 8.595466613769531
Epoch 40, val loss: 1.8833950757980347
Epoch 50, training loss: 1.8571281433105469 = 1.8485349416732788 + 0.001 * 8.593206405639648
Epoch 50, val loss: 1.851702094078064
Epoch 60, training loss: 1.8198792934417725 = 1.811293601989746 + 0.001 * 8.585734367370605
Epoch 60, val loss: 1.8183892965316772
Epoch 70, training loss: 1.788678765296936 = 1.7801225185394287 + 0.001 * 8.556221008300781
Epoch 70, val loss: 1.7917511463165283
Epoch 80, training loss: 1.7518222332000732 = 1.743438720703125 + 0.001 * 8.383511543273926
Epoch 80, val loss: 1.7569087743759155
Epoch 90, training loss: 1.6999685764312744 = 1.6918545961380005 + 0.001 * 8.11400032043457
Epoch 90, val loss: 1.7104988098144531
Epoch 100, training loss: 1.6296355724334717 = 1.6216332912445068 + 0.001 * 8.0023193359375
Epoch 100, val loss: 1.6510918140411377
Epoch 110, training loss: 1.5423505306243896 = 1.5345157384872437 + 0.001 * 7.834765911102295
Epoch 110, val loss: 1.579089641571045
Epoch 120, training loss: 1.4466774463653564 = 1.4390209913253784 + 0.001 * 7.656439304351807
Epoch 120, val loss: 1.5012750625610352
Epoch 130, training loss: 1.3483933210372925 = 1.3407561779022217 + 0.001 * 7.637141227722168
Epoch 130, val loss: 1.4232715368270874
Epoch 140, training loss: 1.2471760511398315 = 1.239598274230957 + 0.001 * 7.577755451202393
Epoch 140, val loss: 1.3454281091690063
Epoch 150, training loss: 1.1427664756774902 = 1.1352325677871704 + 0.001 * 7.533966541290283
Epoch 150, val loss: 1.2666058540344238
Epoch 160, training loss: 1.0375123023986816 = 1.029992938041687 + 0.001 * 7.519355297088623
Epoch 160, val loss: 1.1883509159088135
Epoch 170, training loss: 0.9362733960151672 = 0.9287604689598083 + 0.001 * 7.5129218101501465
Epoch 170, val loss: 1.1144517660140991
Epoch 180, training loss: 0.8444296717643738 = 0.8369258642196655 + 0.001 * 7.503805637359619
Epoch 180, val loss: 1.049225926399231
Epoch 190, training loss: 0.7648739218711853 = 0.7573804259300232 + 0.001 * 7.493508815765381
Epoch 190, val loss: 0.9955295324325562
Epoch 200, training loss: 0.6965856552124023 = 0.6891051530838013 + 0.001 * 7.480495929718018
Epoch 200, val loss: 0.9535816311836243
Epoch 210, training loss: 0.6363953948020935 = 0.6289277672767639 + 0.001 * 7.467601776123047
Epoch 210, val loss: 0.9217610955238342
Epoch 220, training loss: 0.5814798474311829 = 0.5740230679512024 + 0.001 * 7.456793308258057
Epoch 220, val loss: 0.8978614807128906
Epoch 230, training loss: 0.5302796959877014 = 0.5228303670883179 + 0.001 * 7.449310302734375
Epoch 230, val loss: 0.8799393177032471
Epoch 240, training loss: 0.48214107751846313 = 0.47469744086265564 + 0.001 * 7.44365119934082
Epoch 240, val loss: 0.8665440082550049
Epoch 250, training loss: 0.4367333948612213 = 0.42929530143737793 + 0.001 * 7.4380998611450195
Epoch 250, val loss: 0.8568316698074341
Epoch 260, training loss: 0.39397960901260376 = 0.3865470886230469 + 0.001 * 7.432530403137207
Epoch 260, val loss: 0.8508431315422058
Epoch 270, training loss: 0.35392099618911743 = 0.34649401903152466 + 0.001 * 7.426987171173096
Epoch 270, val loss: 0.8486091494560242
Epoch 280, training loss: 0.3166218101978302 = 0.3092006742954254 + 0.001 * 7.4211320877075195
Epoch 280, val loss: 0.8499967455863953
Epoch 290, training loss: 0.28198713064193726 = 0.27457645535469055 + 0.001 * 7.4106669425964355
Epoch 290, val loss: 0.854400098323822
Epoch 300, training loss: 0.2498307228088379 = 0.2424352765083313 + 0.001 * 7.395448684692383
Epoch 300, val loss: 0.8610562086105347
Epoch 310, training loss: 0.22015146911144257 = 0.2127755880355835 + 0.001 * 7.375875473022461
Epoch 310, val loss: 0.8697592616081238
Epoch 320, training loss: 0.19315607845783234 = 0.18578842282295227 + 0.001 * 7.367650508880615
Epoch 320, val loss: 0.8805068135261536
Epoch 330, training loss: 0.1690378487110138 = 0.16169464588165283 + 0.001 * 7.343202590942383
Epoch 330, val loss: 0.8934145569801331
Epoch 340, training loss: 0.1479388028383255 = 0.14062370359897614 + 0.001 * 7.315098285675049
Epoch 340, val loss: 0.9084250926971436
Epoch 350, training loss: 0.12979833781719208 = 0.12249011546373367 + 0.001 * 7.308226585388184
Epoch 350, val loss: 0.925189197063446
Epoch 360, training loss: 0.11433731764554977 = 0.10703655332326889 + 0.001 * 7.300761699676514
Epoch 360, val loss: 0.9433217644691467
Epoch 370, training loss: 0.1012047678232193 = 0.09391835331916809 + 0.001 * 7.286412715911865
Epoch 370, val loss: 0.9625709652900696
Epoch 380, training loss: 0.09006693959236145 = 0.08277888596057892 + 0.001 * 7.28805685043335
Epoch 380, val loss: 0.982562243938446
Epoch 390, training loss: 0.0805678442120552 = 0.07328978180885315 + 0.001 * 7.2780632972717285
Epoch 390, val loss: 1.0029898881912231
Epoch 400, training loss: 0.072441466152668 = 0.06517298519611359 + 0.001 * 7.268483638763428
Epoch 400, val loss: 1.023625373840332
Epoch 410, training loss: 0.0654606968164444 = 0.05820028483867645 + 0.001 * 7.2604079246521
Epoch 410, val loss: 1.044312596321106
Epoch 420, training loss: 0.059441305696964264 = 0.05218255892395973 + 0.001 * 7.258746147155762
Epoch 420, val loss: 1.0648858547210693
Epoch 430, training loss: 0.05423017591238022 = 0.0469675213098526 + 0.001 * 7.262653350830078
Epoch 430, val loss: 1.0851925611495972
Epoch 440, training loss: 0.049674294888973236 = 0.04242941364645958 + 0.001 * 7.244879245758057
Epoch 440, val loss: 1.105156421661377
Epoch 450, training loss: 0.04571019485592842 = 0.03846645727753639 + 0.001 * 7.243738174438477
Epoch 450, val loss: 1.1247230768203735
Epoch 460, training loss: 0.04222995042800903 = 0.034994132816791534 + 0.001 * 7.235819339752197
Epoch 460, val loss: 1.1438199281692505
Epoch 470, training loss: 0.039178166538476944 = 0.03194083645939827 + 0.001 * 7.237329006195068
Epoch 470, val loss: 1.1624594926834106
Epoch 480, training loss: 0.03648870065808296 = 0.029247654601931572 + 0.001 * 7.241046905517578
Epoch 480, val loss: 1.1806044578552246
Epoch 490, training loss: 0.03409786522388458 = 0.0268643107265234 + 0.001 * 7.233553886413574
Epoch 490, val loss: 1.1982241868972778
Epoch 500, training loss: 0.031973786652088165 = 0.024748435243964195 + 0.001 * 7.2253522872924805
Epoch 500, val loss: 1.21536123752594
Epoch 510, training loss: 0.03009229153394699 = 0.022864390164613724 + 0.001 * 7.227900981903076
Epoch 510, val loss: 1.231958031654358
Epoch 520, training loss: 0.02840472012758255 = 0.021181555464863777 + 0.001 * 7.223164081573486
Epoch 520, val loss: 1.2480616569519043
Epoch 530, training loss: 0.02688824199140072 = 0.019673744216561317 + 0.001 * 7.2144975662231445
Epoch 530, val loss: 1.2636903524398804
Epoch 540, training loss: 0.025534434244036674 = 0.01831822842359543 + 0.001 * 7.21620512008667
Epoch 540, val loss: 1.278838872909546
Epoch 550, training loss: 0.024303482845425606 = 0.017095383256673813 + 0.001 * 7.208099842071533
Epoch 550, val loss: 1.2935407161712646
Epoch 560, training loss: 0.023194940760731697 = 0.01598724164068699 + 0.001 * 7.207698822021484
Epoch 560, val loss: 1.3077622652053833
Epoch 570, training loss: 0.022174660116434097 = 0.014977171085774899 + 0.001 * 7.197487831115723
Epoch 570, val loss: 1.3215855360031128
Epoch 580, training loss: 0.021245358511805534 = 0.014050272293388844 + 0.001 * 7.1950860023498535
Epoch 580, val loss: 1.334959864616394
Epoch 590, training loss: 0.020394129678606987 = 0.013197224587202072 + 0.001 * 7.19690465927124
Epoch 590, val loss: 1.347963571548462
Epoch 600, training loss: 0.019600393250584602 = 0.012411020696163177 + 0.001 * 7.1893720626831055
Epoch 600, val loss: 1.3605691194534302
Epoch 610, training loss: 0.018883325159549713 = 0.01168596651405096 + 0.001 * 7.197357654571533
Epoch 610, val loss: 1.3727909326553345
Epoch 620, training loss: 0.01821006089448929 = 0.011017858982086182 + 0.001 * 7.192202568054199
Epoch 620, val loss: 1.3846248388290405
Epoch 630, training loss: 0.0175977423787117 = 0.010401585139334202 + 0.001 * 7.196157455444336
Epoch 630, val loss: 1.3961213827133179
Epoch 640, training loss: 0.01701495051383972 = 0.00983294565230608 + 0.001 * 7.182004928588867
Epoch 640, val loss: 1.4073024988174438
Epoch 650, training loss: 0.016502540558576584 = 0.009307780303061008 + 0.001 * 7.194759368896484
Epoch 650, val loss: 1.4181530475616455
Epoch 660, training loss: 0.015997307375073433 = 0.008822162635624409 + 0.001 * 7.175143718719482
Epoch 660, val loss: 1.4287278652191162
Epoch 670, training loss: 0.015549534931778908 = 0.008372191339731216 + 0.001 * 7.177343845367432
Epoch 670, val loss: 1.4390201568603516
Epoch 680, training loss: 0.01513303630053997 = 0.007954270578920841 + 0.001 * 7.178765773773193
Epoch 680, val loss: 1.4490598440170288
Epoch 690, training loss: 0.014730988070368767 = 0.007565416395664215 + 0.001 * 7.165571212768555
Epoch 690, val loss: 1.4588727951049805
Epoch 700, training loss: 0.014369206503033638 = 0.007202734239399433 + 0.001 * 7.1664719581604
Epoch 700, val loss: 1.4685087203979492
Epoch 710, training loss: 0.014035241678357124 = 0.00686400942504406 + 0.001 * 7.171232223510742
Epoch 710, val loss: 1.4779659509658813
Epoch 720, training loss: 0.013724371790885925 = 0.006546587683260441 + 0.001 * 7.177783489227295
Epoch 720, val loss: 1.4872846603393555
Epoch 730, training loss: 0.013416566886007786 = 0.006248940713703632 + 0.001 * 7.167625904083252
Epoch 730, val loss: 1.4964662790298462
Epoch 740, training loss: 0.013124244287610054 = 0.005969470366835594 + 0.001 * 7.154774188995361
Epoch 740, val loss: 1.5055183172225952
Epoch 750, training loss: 0.012872513383626938 = 0.005706739611923695 + 0.001 * 7.165772914886475
Epoch 750, val loss: 1.5143799781799316
Epoch 760, training loss: 0.01262301579117775 = 0.005459621548652649 + 0.001 * 7.163394451141357
Epoch 760, val loss: 1.523072361946106
Epoch 770, training loss: 0.012396598234772682 = 0.005226867273449898 + 0.001 * 7.169731140136719
Epoch 770, val loss: 1.5316405296325684
Epoch 780, training loss: 0.012160984799265862 = 0.005007442086935043 + 0.001 * 7.153542995452881
Epoch 780, val loss: 1.5400917530059814
Epoch 790, training loss: 0.011967256665229797 = 0.004800625145435333 + 0.001 * 7.166630744934082
Epoch 790, val loss: 1.548407793045044
Epoch 800, training loss: 0.011746720410883427 = 0.0046057249419391155 + 0.001 * 7.140995025634766
Epoch 800, val loss: 1.5565497875213623
Epoch 810, training loss: 0.011553950607776642 = 0.004421539139002562 + 0.001 * 7.132411479949951
Epoch 810, val loss: 1.564589500427246
Epoch 820, training loss: 0.01138962060213089 = 0.004247100092470646 + 0.001 * 7.142520427703857
Epoch 820, val loss: 1.5724847316741943
Epoch 830, training loss: 0.01122142095118761 = 0.004081510007381439 + 0.001 * 7.139910697937012
Epoch 830, val loss: 1.5803492069244385
Epoch 840, training loss: 0.011055553331971169 = 0.003924078773707151 + 0.001 * 7.131473541259766
Epoch 840, val loss: 1.5881085395812988
Epoch 850, training loss: 0.010913591831922531 = 0.003773725824430585 + 0.001 * 7.139865875244141
Epoch 850, val loss: 1.5958880186080933
Epoch 860, training loss: 0.010750670917332172 = 0.0036301815416663885 + 0.001 * 7.120489120483398
Epoch 860, val loss: 1.603713870048523
Epoch 870, training loss: 0.010637091472744942 = 0.003493138123303652 + 0.001 * 7.1439528465271
Epoch 870, val loss: 1.611556887626648
Epoch 880, training loss: 0.010498486459255219 = 0.003362214658409357 + 0.001 * 7.1362714767456055
Epoch 880, val loss: 1.6194640398025513
Epoch 890, training loss: 0.010390948504209518 = 0.0032371878623962402 + 0.001 * 7.153759956359863
Epoch 890, val loss: 1.6274116039276123
Epoch 900, training loss: 0.010240823030471802 = 0.003118132706731558 + 0.001 * 7.122690200805664
Epoch 900, val loss: 1.635346531867981
Epoch 910, training loss: 0.010136479511857033 = 0.0030047751497477293 + 0.001 * 7.131703853607178
Epoch 910, val loss: 1.6432890892028809
Epoch 920, training loss: 0.010069354437291622 = 0.002896863268688321 + 0.001 * 7.17249059677124
Epoch 920, val loss: 1.651170253753662
Epoch 930, training loss: 0.009895186871290207 = 0.002794270170852542 + 0.001 * 7.100915908813477
Epoch 930, val loss: 1.6590912342071533
Epoch 940, training loss: 0.009800350293517113 = 0.002696539042517543 + 0.001 * 7.103811264038086
Epoch 940, val loss: 1.6669973134994507
Epoch 950, training loss: 0.009721463546156883 = 0.002603468019515276 + 0.001 * 7.117995262145996
Epoch 950, val loss: 1.6748170852661133
Epoch 960, training loss: 0.009640581905841827 = 0.0025147744454443455 + 0.001 * 7.125807762145996
Epoch 960, val loss: 1.6826574802398682
Epoch 970, training loss: 0.009551153518259525 = 0.002430146560072899 + 0.001 * 7.121006488800049
Epoch 970, val loss: 1.6904308795928955
Epoch 980, training loss: 0.00947396457195282 = 0.0023492765612900257 + 0.001 * 7.124688148498535
Epoch 980, val loss: 1.6982156038284302
Epoch 990, training loss: 0.009386235848069191 = 0.002272089011967182 + 0.001 * 7.114146709442139
Epoch 990, val loss: 1.7059324979782104
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.7954665260938324
=== training gcn model ===
Epoch 0, training loss: 1.9569655656814575 = 1.948368787765503 + 0.001 * 8.596820831298828
Epoch 0, val loss: 1.9424015283584595
Epoch 10, training loss: 1.9463282823562622 = 1.9377315044403076 + 0.001 * 8.596736907958984
Epoch 10, val loss: 1.9313831329345703
Epoch 20, training loss: 1.9333540201187134 = 1.924757480621338 + 0.001 * 8.59652328491211
Epoch 20, val loss: 1.918049693107605
Epoch 30, training loss: 1.9158943891525269 = 1.9072983264923096 + 0.001 * 8.596040725708008
Epoch 30, val loss: 1.9001270532608032
Epoch 40, training loss: 1.891144037246704 = 1.8825491666793823 + 0.001 * 8.594892501831055
Epoch 40, val loss: 1.8750592470169067
Epoch 50, training loss: 1.8578463792800903 = 1.8492547273635864 + 0.001 * 8.591620445251465
Epoch 50, val loss: 1.8428722620010376
Epoch 60, training loss: 1.8215819597244263 = 1.8130018711090088 + 0.001 * 8.580096244812012
Epoch 60, val loss: 1.8119490146636963
Epoch 70, training loss: 1.7906838655471802 = 1.7821563482284546 + 0.001 * 8.527484893798828
Epoch 70, val loss: 1.7889394760131836
Epoch 80, training loss: 1.7548389434814453 = 1.746575117111206 + 0.001 * 8.263837814331055
Epoch 80, val loss: 1.759202241897583
Epoch 90, training loss: 1.7054290771484375 = 1.6973905563354492 + 0.001 * 8.038481712341309
Epoch 90, val loss: 1.716902256011963
Epoch 100, training loss: 1.6365348100662231 = 1.6287670135498047 + 0.001 * 7.7677412033081055
Epoch 100, val loss: 1.6584383249282837
Epoch 110, training loss: 1.546390175819397 = 1.5387845039367676 + 0.001 * 7.60561990737915
Epoch 110, val loss: 1.5840226411819458
Epoch 120, training loss: 1.4421463012695312 = 1.4346362352371216 + 0.001 * 7.510066986083984
Epoch 120, val loss: 1.500097393989563
Epoch 130, training loss: 1.3333609104156494 = 1.3259153366088867 + 0.001 * 7.445575714111328
Epoch 130, val loss: 1.4149264097213745
Epoch 140, training loss: 1.2263500690460205 = 1.2189418077468872 + 0.001 * 7.4082932472229
Epoch 140, val loss: 1.3326373100280762
Epoch 150, training loss: 1.1246113777160645 = 1.117228388786316 + 0.001 * 7.3829755783081055
Epoch 150, val loss: 1.2562165260314941
Epoch 160, training loss: 1.0293114185333252 = 1.021958589553833 + 0.001 * 7.3528056144714355
Epoch 160, val loss: 1.1869953870773315
Epoch 170, training loss: 0.94084233045578 = 0.933523416519165 + 0.001 * 7.318918228149414
Epoch 170, val loss: 1.1251609325408936
Epoch 180, training loss: 0.8597599864006042 = 0.8524647355079651 + 0.001 * 7.295258045196533
Epoch 180, val loss: 1.071058988571167
Epoch 190, training loss: 0.7867438793182373 = 0.7794618010520935 + 0.001 * 7.282071113586426
Epoch 190, val loss: 1.0248128175735474
Epoch 200, training loss: 0.7218140363693237 = 0.7145380973815918 + 0.001 * 7.2759575843811035
Epoch 200, val loss: 0.9865629076957703
Epoch 210, training loss: 0.6639193892478943 = 0.6566455364227295 + 0.001 * 7.273837566375732
Epoch 210, val loss: 0.9559114575386047
Epoch 220, training loss: 0.611244797706604 = 0.603972852230072 + 0.001 * 7.271970272064209
Epoch 220, val loss: 0.9316976070404053
Epoch 230, training loss: 0.5620752573013306 = 0.5548049211502075 + 0.001 * 7.270317077636719
Epoch 230, val loss: 0.9127373695373535
Epoch 240, training loss: 0.5153918862342834 = 0.5081236362457275 + 0.001 * 7.268251419067383
Epoch 240, val loss: 0.898274302482605
Epoch 250, training loss: 0.4709475040435791 = 0.46368226408958435 + 0.001 * 7.26524019241333
Epoch 250, val loss: 0.8879414796829224
Epoch 260, training loss: 0.42898568511009216 = 0.42172297835350037 + 0.001 * 7.262700080871582
Epoch 260, val loss: 0.881572961807251
Epoch 270, training loss: 0.39006054401397705 = 0.38280177116394043 + 0.001 * 7.258770942687988
Epoch 270, val loss: 0.879338800907135
Epoch 280, training loss: 0.354712575674057 = 0.347455233335495 + 0.001 * 7.257330417633057
Epoch 280, val loss: 0.8811306357383728
Epoch 290, training loss: 0.3230661451816559 = 0.3158152997493744 + 0.001 * 7.2508392333984375
Epoch 290, val loss: 0.8865429759025574
Epoch 300, training loss: 0.2947070300579071 = 0.28746020793914795 + 0.001 * 7.246816158294678
Epoch 300, val loss: 0.8947994709014893
Epoch 310, training loss: 0.26878178119659424 = 0.26154208183288574 + 0.001 * 7.239696025848389
Epoch 310, val loss: 0.9049718976020813
Epoch 320, training loss: 0.24427208304405212 = 0.23703736066818237 + 0.001 * 7.234725475311279
Epoch 320, val loss: 0.9162845611572266
Epoch 330, training loss: 0.2202739715576172 = 0.21304039657115936 + 0.001 * 7.233574867248535
Epoch 330, val loss: 0.9281848073005676
Epoch 340, training loss: 0.19639120995998383 = 0.18917106091976166 + 0.001 * 7.220146179199219
Epoch 340, val loss: 0.940666913986206
Epoch 350, training loss: 0.17310862243175507 = 0.1658926010131836 + 0.001 * 7.216024398803711
Epoch 350, val loss: 0.9539522528648376
Epoch 360, training loss: 0.15144702792167664 = 0.14423036575317383 + 0.001 * 7.216660976409912
Epoch 360, val loss: 0.968589723110199
Epoch 370, training loss: 0.1322389394044876 = 0.1250319629907608 + 0.001 * 7.20697546005249
Epoch 370, val loss: 0.9851490259170532
Epoch 380, training loss: 0.11585777252912521 = 0.10864654183387756 + 0.001 * 7.211230278015137
Epoch 380, val loss: 1.0037510395050049
Epoch 390, training loss: 0.10210444033145905 = 0.09489621967077255 + 0.001 * 7.208217620849609
Epoch 390, val loss: 1.0241891145706177
Epoch 400, training loss: 0.09059558063745499 = 0.08339723944664001 + 0.001 * 7.198341369628906
Epoch 400, val loss: 1.0459011793136597
Epoch 410, training loss: 0.0809459313750267 = 0.07374937832355499 + 0.001 * 7.1965508460998535
Epoch 410, val loss: 1.0686298608779907
Epoch 420, training loss: 0.07279636710882187 = 0.06560183316469193 + 0.001 * 7.194530487060547
Epoch 420, val loss: 1.091873288154602
Epoch 430, training loss: 0.0658598244190216 = 0.05866595730185509 + 0.001 * 7.193865776062012
Epoch 430, val loss: 1.1152756214141846
Epoch 440, training loss: 0.05991600826382637 = 0.05271576717495918 + 0.001 * 7.2002410888671875
Epoch 440, val loss: 1.1385185718536377
Epoch 450, training loss: 0.054760824888944626 = 0.047574035823345184 + 0.001 * 7.186787128448486
Epoch 450, val loss: 1.1615034341812134
Epoch 460, training loss: 0.0502861887216568 = 0.04310031235218048 + 0.001 * 7.185875415802002
Epoch 460, val loss: 1.184080719947815
Epoch 470, training loss: 0.04638751968741417 = 0.03918432071805 + 0.001 * 7.203198432922363
Epoch 470, val loss: 1.206168293952942
Epoch 480, training loss: 0.04293004050850868 = 0.03573821112513542 + 0.001 * 7.191828727722168
Epoch 480, val loss: 1.2276792526245117
Epoch 490, training loss: 0.03987618908286095 = 0.032691024243831635 + 0.001 * 7.185165882110596
Epoch 490, val loss: 1.2486374378204346
Epoch 500, training loss: 0.03717612475156784 = 0.029985344037413597 + 0.001 * 7.190782070159912
Epoch 500, val loss: 1.269006609916687
Epoch 510, training loss: 0.03476573899388313 = 0.027574362233281136 + 0.001 * 7.191375255584717
Epoch 510, val loss: 1.2887780666351318
Epoch 520, training loss: 0.03260292485356331 = 0.025419021025300026 + 0.001 * 7.183904647827148
Epoch 520, val loss: 1.3079676628112793
Epoch 530, training loss: 0.030688459053635597 = 0.023487277328968048 + 0.001 * 7.201180934906006
Epoch 530, val loss: 1.3265665769577026
Epoch 540, training loss: 0.028925945982336998 = 0.021751312538981438 + 0.001 * 7.174633026123047
Epoch 540, val loss: 1.3445812463760376
Epoch 550, training loss: 0.027374982833862305 = 0.020187661051750183 + 0.001 * 7.187321186065674
Epoch 550, val loss: 1.362009048461914
Epoch 560, training loss: 0.025948336347937584 = 0.018776152282953262 + 0.001 * 7.172183036804199
Epoch 560, val loss: 1.3789093494415283
Epoch 570, training loss: 0.024665266275405884 = 0.017499180510640144 + 0.001 * 7.166086196899414
Epoch 570, val loss: 1.3952947854995728
Epoch 580, training loss: 0.02351953648030758 = 0.016341567039489746 + 0.001 * 7.177968502044678
Epoch 580, val loss: 1.4111629724502563
Epoch 590, training loss: 0.022453272715210915 = 0.015290151350200176 + 0.001 * 7.163121700286865
Epoch 590, val loss: 1.4265549182891846
Epoch 600, training loss: 0.021493490785360336 = 0.01433336827903986 + 0.001 * 7.160122394561768
Epoch 600, val loss: 1.4414902925491333
Epoch 610, training loss: 0.020634427666664124 = 0.013461041264235973 + 0.001 * 7.1733856201171875
Epoch 610, val loss: 1.4559786319732666
Epoch 620, training loss: 0.019819321110844612 = 0.01266421191394329 + 0.001 * 7.155109405517578
Epoch 620, val loss: 1.47003972530365
Epoch 630, training loss: 0.019085705280303955 = 0.011935029178857803 + 0.001 * 7.150676727294922
Epoch 630, val loss: 1.4837043285369873
Epoch 640, training loss: 0.018467437475919724 = 0.011266570538282394 + 0.001 * 7.20086669921875
Epoch 640, val loss: 1.4969828128814697
Epoch 650, training loss: 0.017807655036449432 = 0.010652651078999043 + 0.001 * 7.155003547668457
Epoch 650, val loss: 1.509867548942566
Epoch 660, training loss: 0.01725025661289692 = 0.010087807662785053 + 0.001 * 7.162448406219482
Epoch 660, val loss: 1.5224146842956543
Epoch 670, training loss: 0.016715509817004204 = 0.009567164815962315 + 0.001 * 7.14834451675415
Epoch 670, val loss: 1.5346192121505737
Epoch 680, training loss: 0.016226613894104958 = 0.009086433798074722 + 0.001 * 7.140179634094238
Epoch 680, val loss: 1.5464980602264404
Epoch 690, training loss: 0.015777621418237686 = 0.008641794323921204 + 0.001 * 7.1358256340026855
Epoch 690, val loss: 1.5580415725708008
Epoch 700, training loss: 0.015374759212136269 = 0.00822988897562027 + 0.001 * 7.144870281219482
Epoch 700, val loss: 1.5692733526229858
Epoch 710, training loss: 0.015013245865702629 = 0.007847689092159271 + 0.001 * 7.165556907653809
Epoch 710, val loss: 1.5802175998687744
Epoch 720, training loss: 0.014628713950514793 = 0.00749251851812005 + 0.001 * 7.136195182800293
Epoch 720, val loss: 1.5908443927764893
Epoch 730, training loss: 0.014290284365415573 = 0.0071619777008891106 + 0.001 * 7.128305912017822
Epoch 730, val loss: 1.6012014150619507
Epoch 740, training loss: 0.013995576649904251 = 0.006853862199932337 + 0.001 * 7.141714572906494
Epoch 740, val loss: 1.6112998723983765
Epoch 750, training loss: 0.013690615072846413 = 0.006566283758729696 + 0.001 * 7.124330520629883
Epoch 750, val loss: 1.621140956878662
Epoch 760, training loss: 0.013416264206171036 = 0.0062974984757602215 + 0.001 * 7.118765354156494
Epoch 760, val loss: 1.6307144165039062
Epoch 770, training loss: 0.013173500075936317 = 0.006045917514711618 + 0.001 * 7.12758207321167
Epoch 770, val loss: 1.6400595903396606
Epoch 780, training loss: 0.013012893497943878 = 0.005810136441141367 + 0.001 * 7.202756881713867
Epoch 780, val loss: 1.649178385734558
Epoch 790, training loss: 0.012712296098470688 = 0.005588890984654427 + 0.001 * 7.123404502868652
Epoch 790, val loss: 1.6580417156219482
Epoch 800, training loss: 0.012513140216469765 = 0.005381041672080755 + 0.001 * 7.1320977210998535
Epoch 800, val loss: 1.6666948795318604
Epoch 810, training loss: 0.012295128777623177 = 0.005185537971556187 + 0.001 * 7.109590530395508
Epoch 810, val loss: 1.6751261949539185
Epoch 820, training loss: 0.012127559632062912 = 0.005001456011086702 + 0.001 * 7.12610387802124
Epoch 820, val loss: 1.6833606958389282
Epoch 830, training loss: 0.011939271353185177 = 0.004827914293855429 + 0.001 * 7.111356735229492
Epoch 830, val loss: 1.6913866996765137
Epoch 840, training loss: 0.011764680966734886 = 0.004664144478738308 + 0.001 * 7.100536823272705
Epoch 840, val loss: 1.6992061138153076
Epoch 850, training loss: 0.011608809232711792 = 0.004509455058723688 + 0.001 * 7.099353790283203
Epoch 850, val loss: 1.7068548202514648
Epoch 860, training loss: 0.01145586371421814 = 0.004363155458122492 + 0.001 * 7.092707633972168
Epoch 860, val loss: 1.7142916917800903
Epoch 870, training loss: 0.011333427391946316 = 0.004224691074341536 + 0.001 * 7.108736038208008
Epoch 870, val loss: 1.7215551137924194
Epoch 880, training loss: 0.011201674118638039 = 0.004093495663255453 + 0.001 * 7.108177661895752
Epoch 880, val loss: 1.7286391258239746
Epoch 890, training loss: 0.011061528697609901 = 0.0039691077545285225 + 0.001 * 7.09242057800293
Epoch 890, val loss: 1.735550045967102
Epoch 900, training loss: 0.010931126773357391 = 0.0038510668091475964 + 0.001 * 7.080059051513672
Epoch 900, val loss: 1.7422924041748047
Epoch 910, training loss: 0.010830937884747982 = 0.0037389365024864674 + 0.001 * 7.092000961303711
Epoch 910, val loss: 1.7488806247711182
Epoch 920, training loss: 0.010715766809880733 = 0.003632329171523452 + 0.001 * 7.083437442779541
Epoch 920, val loss: 1.7553002834320068
Epoch 930, training loss: 0.010617522522807121 = 0.0035308958031237125 + 0.001 * 7.086626052856445
Epoch 930, val loss: 1.761560082435608
Epoch 940, training loss: 0.010525809600949287 = 0.0034343302249908447 + 0.001 * 7.0914788246154785
Epoch 940, val loss: 1.7676904201507568
Epoch 950, training loss: 0.010428238660097122 = 0.00334231392480433 + 0.001 * 7.08592414855957
Epoch 950, val loss: 1.7736783027648926
Epoch 960, training loss: 0.010340974666178226 = 0.00325455772690475 + 0.001 * 7.086416244506836
Epoch 960, val loss: 1.7795312404632568
Epoch 970, training loss: 0.010261425748467445 = 0.0031708101741969585 + 0.001 * 7.090615272521973
Epoch 970, val loss: 1.785222053527832
Epoch 980, training loss: 0.010182302445173264 = 0.003090835176408291 + 0.001 * 7.091466903686523
Epoch 980, val loss: 1.7907965183258057
Epoch 990, training loss: 0.010106692090630531 = 0.0030144108459353447 + 0.001 * 7.092280864715576
Epoch 990, val loss: 1.7962350845336914
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8007380073800738
=== training gcn model ===
Epoch 0, training loss: 1.9746081829071045 = 1.96601140499115 + 0.001 * 8.59681510925293
Epoch 0, val loss: 1.9751379489898682
Epoch 10, training loss: 1.9640357494354248 = 1.9554389715194702 + 0.001 * 8.596763610839844
Epoch 10, val loss: 1.964013695716858
Epoch 20, training loss: 1.9509080648422241 = 1.9423115253448486 + 0.001 * 8.59659194946289
Epoch 20, val loss: 1.9500590562820435
Epoch 30, training loss: 1.9323912858963013 = 1.9237949848175049 + 0.001 * 8.596250534057617
Epoch 30, val loss: 1.9301215410232544
Epoch 40, training loss: 1.905036449432373 = 1.896440863609314 + 0.001 * 8.595526695251465
Epoch 40, val loss: 1.9005122184753418
Epoch 50, training loss: 1.8670793771743774 = 1.8584858179092407 + 0.001 * 8.593605995178223
Epoch 50, val loss: 1.860282301902771
Epoch 60, training loss: 1.8257135152816772 = 1.8171262741088867 + 0.001 * 8.587264060974121
Epoch 60, val loss: 1.819618821144104
Epoch 70, training loss: 1.7944881916046143 = 1.785927653312683 + 0.001 * 8.56058120727539
Epoch 70, val loss: 1.7928555011749268
Epoch 80, training loss: 1.76019287109375 = 1.7518510818481445 + 0.001 * 8.341789245605469
Epoch 80, val loss: 1.7636761665344238
Epoch 90, training loss: 1.712573766708374 = 1.704519510269165 + 0.001 * 8.054285049438477
Epoch 90, val loss: 1.7228938341140747
Epoch 100, training loss: 1.6462697982788086 = 1.6384612321853638 + 0.001 * 7.8085832595825195
Epoch 100, val loss: 1.6668201684951782
Epoch 110, training loss: 1.5604406595230103 = 1.552767038345337 + 0.001 * 7.673607349395752
Epoch 110, val loss: 1.5965443849563599
Epoch 120, training loss: 1.4628627300262451 = 1.455230474472046 + 0.001 * 7.632296085357666
Epoch 120, val loss: 1.5185774564743042
Epoch 130, training loss: 1.3631632328033447 = 1.3555922508239746 + 0.001 * 7.5710272789001465
Epoch 130, val loss: 1.4413756132125854
Epoch 140, training loss: 1.267263412475586 = 1.259761929512024 + 0.001 * 7.501484394073486
Epoch 140, val loss: 1.3696504831314087
Epoch 150, training loss: 1.178598403930664 = 1.1711623668670654 + 0.001 * 7.436089038848877
Epoch 150, val loss: 1.3055607080459595
Epoch 160, training loss: 1.0991991758346558 = 1.0918279886245728 + 0.001 * 7.371184349060059
Epoch 160, val loss: 1.2511829137802124
Epoch 170, training loss: 1.0300545692443848 = 1.0227298736572266 + 0.001 * 7.32465934753418
Epoch 170, val loss: 1.2072104215621948
Epoch 180, training loss: 0.9694454073905945 = 0.9621374607086182 + 0.001 * 7.307937145233154
Epoch 180, val loss: 1.1719070672988892
Epoch 190, training loss: 0.913550078868866 = 0.9062530994415283 + 0.001 * 7.296976566314697
Epoch 190, val loss: 1.1408674716949463
Epoch 200, training loss: 0.8583555817604065 = 0.8510651588439941 + 0.001 * 7.290426254272461
Epoch 200, val loss: 1.1102851629257202
Epoch 210, training loss: 0.800815761089325 = 0.7935298085212708 + 0.001 * 7.285934925079346
Epoch 210, val loss: 1.0773600339889526
Epoch 220, training loss: 0.7394661903381348 = 0.7321842312812805 + 0.001 * 7.281950950622559
Epoch 220, val loss: 1.0414113998413086
Epoch 230, training loss: 0.6747483611106873 = 0.6674702167510986 + 0.001 * 7.27813720703125
Epoch 230, val loss: 1.003631830215454
Epoch 240, training loss: 0.6083499789237976 = 0.6010712385177612 + 0.001 * 7.278713226318359
Epoch 240, val loss: 0.9666099548339844
Epoch 250, training loss: 0.5425581336021423 = 0.5352859497070312 + 0.001 * 7.272165298461914
Epoch 250, val loss: 0.9331953525543213
Epoch 260, training loss: 0.47948595881462097 = 0.4722178876399994 + 0.001 * 7.268059730529785
Epoch 260, val loss: 0.9055988788604736
Epoch 270, training loss: 0.4206759035587311 = 0.41340726613998413 + 0.001 * 7.268643856048584
Epoch 270, val loss: 0.8844053745269775
Epoch 280, training loss: 0.36705106496810913 = 0.35978782176971436 + 0.001 * 7.263247489929199
Epoch 280, val loss: 0.8692564368247986
Epoch 290, training loss: 0.31896793842315674 = 0.31170910596847534 + 0.001 * 7.258833408355713
Epoch 290, val loss: 0.8598542213439941
Epoch 300, training loss: 0.2763582468032837 = 0.26908931136131287 + 0.001 * 7.268933296203613
Epoch 300, val loss: 0.8555929064750671
Epoch 310, training loss: 0.23894041776657104 = 0.23168733716011047 + 0.001 * 7.253086566925049
Epoch 310, val loss: 0.8558592200279236
Epoch 320, training loss: 0.20638735592365265 = 0.19913797080516815 + 0.001 * 7.249382495880127
Epoch 320, val loss: 0.8599771857261658
Epoch 330, training loss: 0.17831100523471832 = 0.17103801667690277 + 0.001 * 7.272981643676758
Epoch 330, val loss: 0.8674795627593994
Epoch 340, training loss: 0.1542336642742157 = 0.14698351919651031 + 0.001 * 7.250148296356201
Epoch 340, val loss: 0.8776731491088867
Epoch 350, training loss: 0.13377241790294647 = 0.12653709948062897 + 0.001 * 7.235323905944824
Epoch 350, val loss: 0.8897772431373596
Epoch 360, training loss: 0.11651656776666641 = 0.10925112664699554 + 0.001 * 7.265443801879883
Epoch 360, val loss: 0.9032682180404663
Epoch 370, training loss: 0.10190194100141525 = 0.09467604011297226 + 0.001 * 7.225897312164307
Epoch 370, val loss: 0.9176890850067139
Epoch 380, training loss: 0.08962074667215347 = 0.08240018039941788 + 0.001 * 7.220564365386963
Epoch 380, val loss: 0.9326324462890625
Epoch 390, training loss: 0.0792696624994278 = 0.0720544382929802 + 0.001 * 7.215219974517822
Epoch 390, val loss: 0.9478113055229187
Epoch 400, training loss: 0.07056613266468048 = 0.0633164644241333 + 0.001 * 7.249666690826416
Epoch 400, val loss: 0.9629626274108887
Epoch 410, training loss: 0.0631253644824028 = 0.0559140108525753 + 0.001 * 7.2113518714904785
Epoch 410, val loss: 0.9780245423316956
Epoch 420, training loss: 0.0568385012447834 = 0.04962163046002388 + 0.001 * 7.216871738433838
Epoch 420, val loss: 0.9928401112556458
Epoch 430, training loss: 0.05148221179842949 = 0.04425328969955444 + 0.001 * 7.2289228439331055
Epoch 430, val loss: 1.0073752403259277
Epoch 440, training loss: 0.04687704145908356 = 0.039656855165958405 + 0.001 * 7.220186710357666
Epoch 440, val loss: 1.0215590000152588
Epoch 450, training loss: 0.04290349781513214 = 0.035703327506780624 + 0.001 * 7.200168609619141
Epoch 450, val loss: 1.0353350639343262
Epoch 460, training loss: 0.03950188308954239 = 0.03228658437728882 + 0.001 * 7.2153000831604
Epoch 460, val loss: 1.0486586093902588
Epoch 470, training loss: 0.036523159593343735 = 0.02932145819067955 + 0.001 * 7.2017011642456055
Epoch 470, val loss: 1.0615990161895752
Epoch 480, training loss: 0.033937904983758926 = 0.026736130937933922 + 0.001 * 7.2017741203308105
Epoch 480, val loss: 1.0740865468978882
Epoch 490, training loss: 0.0316678024828434 = 0.024472452700138092 + 0.001 * 7.19534969329834
Epoch 490, val loss: 1.0861488580703735
Epoch 500, training loss: 0.0296830665320158 = 0.022483423352241516 + 0.001 * 7.199642181396484
Epoch 500, val loss: 1.0977905988693237
Epoch 510, training loss: 0.027927814051508904 = 0.020726770162582397 + 0.001 * 7.201043128967285
Epoch 510, val loss: 1.1090068817138672
Epoch 520, training loss: 0.026371333748102188 = 0.019168773666024208 + 0.001 * 7.202559471130371
Epoch 520, val loss: 1.1198430061340332
Epoch 530, training loss: 0.02497623860836029 = 0.017781635746359825 + 0.001 * 7.194603443145752
Epoch 530, val loss: 1.130259394645691
Epoch 540, training loss: 0.023732401430606842 = 0.01654183119535446 + 0.001 * 7.190568923950195
Epoch 540, val loss: 1.1403604745864868
Epoch 550, training loss: 0.02261548489332199 = 0.015429630875587463 + 0.001 * 7.185853958129883
Epoch 550, val loss: 1.150144100189209
Epoch 560, training loss: 0.02162589132785797 = 0.014428514987230301 + 0.001 * 7.197375297546387
Epoch 560, val loss: 1.159582257270813
Epoch 570, training loss: 0.020716167986392975 = 0.01352456770837307 + 0.001 * 7.191599369049072
Epoch 570, val loss: 1.168694257736206
Epoch 580, training loss: 0.019889971241354942 = 0.012705802917480469 + 0.001 * 7.184167861938477
Epoch 580, val loss: 1.1774866580963135
Epoch 590, training loss: 0.019144661724567413 = 0.011961941607296467 + 0.001 * 7.182718753814697
Epoch 590, val loss: 1.1860253810882568
Epoch 600, training loss: 0.018511764705181122 = 0.011284084059298038 + 0.001 * 7.227680206298828
Epoch 600, val loss: 1.1942713260650635
Epoch 610, training loss: 0.017847619950771332 = 0.01066471915692091 + 0.001 * 7.182901382446289
Epoch 610, val loss: 1.2022559642791748
Epoch 620, training loss: 0.01727288030087948 = 0.010096341371536255 + 0.001 * 7.176538467407227
Epoch 620, val loss: 1.2099969387054443
Epoch 630, training loss: 0.016766853630542755 = 0.009571172297000885 + 0.001 * 7.195680141448975
Epoch 630, val loss: 1.2175085544586182
Epoch 640, training loss: 0.016266249120235443 = 0.009082745760679245 + 0.001 * 7.183502197265625
Epoch 640, val loss: 1.2248351573944092
Epoch 650, training loss: 0.01579723134636879 = 0.00862689595669508 + 0.001 * 7.170334815979004
Epoch 650, val loss: 1.2320208549499512
Epoch 660, training loss: 0.015369437634944916 = 0.008201205171644688 + 0.001 * 7.168232440948486
Epoch 660, val loss: 1.2390694618225098
Epoch 670, training loss: 0.01497383788228035 = 0.007803660351783037 + 0.001 * 7.170176982879639
Epoch 670, val loss: 1.2459241151809692
Epoch 680, training loss: 0.01460254192352295 = 0.00743276858702302 + 0.001 * 7.169773101806641
Epoch 680, val loss: 1.2526148557662964
Epoch 690, training loss: 0.014256658032536507 = 0.007086932193487883 + 0.001 * 7.169724941253662
Epoch 690, val loss: 1.259129285812378
Epoch 700, training loss: 0.013938158750534058 = 0.006764618679881096 + 0.001 * 7.173539638519287
Epoch 700, val loss: 1.2654706239700317
Epoch 710, training loss: 0.013619940727949142 = 0.00646411394700408 + 0.001 * 7.155827045440674
Epoch 710, val loss: 1.2716388702392578
Epoch 720, training loss: 0.013346225023269653 = 0.006183810997754335 + 0.001 * 7.162413597106934
Epoch 720, val loss: 1.2776447534561157
Epoch 730, training loss: 0.013091227039694786 = 0.005922181531786919 + 0.001 * 7.169044494628906
Epoch 730, val loss: 1.283476710319519
Epoch 740, training loss: 0.012874975800514221 = 0.005677810404449701 + 0.001 * 7.197164535522461
Epoch 740, val loss: 1.2891559600830078
Epoch 750, training loss: 0.012614400126039982 = 0.0054493071511387825 + 0.001 * 7.165092468261719
Epoch 750, val loss: 1.2946480512619019
Epoch 760, training loss: 0.012409960851073265 = 0.005235454998910427 + 0.001 * 7.174505710601807
Epoch 760, val loss: 1.3000133037567139
Epoch 770, training loss: 0.012207254767417908 = 0.005035047419369221 + 0.001 * 7.172206401824951
Epoch 770, val loss: 1.3052194118499756
Epoch 780, training loss: 0.012000467628240585 = 0.004847170319408178 + 0.001 * 7.153296947479248
Epoch 780, val loss: 1.3102850914001465
Epoch 790, training loss: 0.011811899021267891 = 0.0046707200817763805 + 0.001 * 7.141178131103516
Epoch 790, val loss: 1.3152120113372803
Epoch 800, training loss: 0.011645271442830563 = 0.004504865501075983 + 0.001 * 7.140405654907227
Epoch 800, val loss: 1.3200087547302246
Epoch 810, training loss: 0.011490171775221825 = 0.0043488070368766785 + 0.001 * 7.141365051269531
Epoch 810, val loss: 1.3246824741363525
Epoch 820, training loss: 0.011344512924551964 = 0.004201728384941816 + 0.001 * 7.1427836418151855
Epoch 820, val loss: 1.3292170763015747
Epoch 830, training loss: 0.011260788887739182 = 0.00406294921413064 + 0.001 * 7.19783878326416
Epoch 830, val loss: 1.3336431980133057
Epoch 840, training loss: 0.011079901829361916 = 0.0039319745264947414 + 0.001 * 7.1479268074035645
Epoch 840, val loss: 1.3379297256469727
Epoch 850, training loss: 0.010983413085341454 = 0.003808179870247841 + 0.001 * 7.175232887268066
Epoch 850, val loss: 1.3421423435211182
Epoch 860, training loss: 0.010815378278493881 = 0.0036910497583448887 + 0.001 * 7.124327659606934
Epoch 860, val loss: 1.3462271690368652
Epoch 870, training loss: 0.01071084663271904 = 0.0035801324993371964 + 0.001 * 7.130714416503906
Epoch 870, val loss: 1.3502190113067627
Epoch 880, training loss: 0.01062313374131918 = 0.0034749771002680063 + 0.001 * 7.14815616607666
Epoch 880, val loss: 1.3540631532669067
Epoch 890, training loss: 0.010491976514458656 = 0.0033752266317605972 + 0.001 * 7.116750240325928
Epoch 890, val loss: 1.3578654527664185
Epoch 900, training loss: 0.01041707955300808 = 0.003280473407357931 + 0.001 * 7.136605262756348
Epoch 900, val loss: 1.3615567684173584
Epoch 910, training loss: 0.010319787077605724 = 0.003190411254763603 + 0.001 * 7.129375457763672
Epoch 910, val loss: 1.3651511669158936
Epoch 920, training loss: 0.010223621502518654 = 0.003104677191004157 + 0.001 * 7.118943691253662
Epoch 920, val loss: 1.3686498403549194
Epoch 930, training loss: 0.010169249027967453 = 0.003023084718734026 + 0.001 * 7.1461639404296875
Epoch 930, val loss: 1.3720831871032715
Epoch 940, training loss: 0.01005303394049406 = 0.0029452729504555464 + 0.001 * 7.107760906219482
Epoch 940, val loss: 1.3754024505615234
Epoch 950, training loss: 0.009985333308577538 = 0.002871067263185978 + 0.001 * 7.114266395568848
Epoch 950, val loss: 1.378657579421997
Epoch 960, training loss: 0.009918704628944397 = 0.0028002001345157623 + 0.001 * 7.118504524230957
Epoch 960, val loss: 1.3818320035934448
Epoch 970, training loss: 0.009856228716671467 = 0.002732406137511134 + 0.001 * 7.1238226890563965
Epoch 970, val loss: 1.3849424123764038
Epoch 980, training loss: 0.009788349270820618 = 0.0026673406828194857 + 0.001 * 7.121008396148682
Epoch 980, val loss: 1.3879815340042114
Epoch 990, training loss: 0.009736558422446251 = 0.0026046386919915676 + 0.001 * 7.131918907165527
Epoch 990, val loss: 1.3909825086593628
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.7954665260938324
The final CL Acc:0.74444, 0.01814, The final GNN Acc:0.79722, 0.00249
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13206])
remove edge: torch.Size([2, 7916])
updated graph: torch.Size([2, 10566])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.965232491493225 = 1.9566357135772705 + 0.001 * 8.596813201904297
Epoch 0, val loss: 1.9613549709320068
Epoch 10, training loss: 1.9544614553451538 = 1.9458646774291992 + 0.001 * 8.596733093261719
Epoch 10, val loss: 1.9499834775924683
Epoch 20, training loss: 1.9407931566238403 = 1.9321966171264648 + 0.001 * 8.596524238586426
Epoch 20, val loss: 1.9354392290115356
Epoch 30, training loss: 1.9214509725570679 = 1.9128549098968506 + 0.001 * 8.596091270446777
Epoch 30, val loss: 1.9148966073989868
Epoch 40, training loss: 1.893211841583252 = 1.884616732597351 + 0.001 * 8.595067977905273
Epoch 40, val loss: 1.8852753639221191
Epoch 50, training loss: 1.8546921014785767 = 1.846099853515625 + 0.001 * 8.592217445373535
Epoch 50, val loss: 1.8466697931289673
Epoch 60, training loss: 1.8122063875198364 = 1.8036249876022339 + 0.001 * 8.581440925598145
Epoch 60, val loss: 1.8079984188079834
Epoch 70, training loss: 1.7749965190887451 = 1.7664703130722046 + 0.001 * 8.526195526123047
Epoch 70, val loss: 1.77682363986969
Epoch 80, training loss: 1.729516625404358 = 1.7213364839553833 + 0.001 * 8.180120468139648
Epoch 80, val loss: 1.7360807657241821
Epoch 90, training loss: 1.6657966375350952 = 1.657832145690918 + 0.001 * 7.964484214782715
Epoch 90, val loss: 1.6790103912353516
Epoch 100, training loss: 1.5822762250900269 = 1.574480652809143 + 0.001 * 7.795563697814941
Epoch 100, val loss: 1.6075795888900757
Epoch 110, training loss: 1.485691785812378 = 1.4780868291854858 + 0.001 * 7.604959487915039
Epoch 110, val loss: 1.5268856287002563
Epoch 120, training loss: 1.3866796493530273 = 1.3791240453720093 + 0.001 * 7.555551528930664
Epoch 120, val loss: 1.4432916641235352
Epoch 130, training loss: 1.2886404991149902 = 1.2810875177383423 + 0.001 * 7.552931785583496
Epoch 130, val loss: 1.3629848957061768
Epoch 140, training loss: 1.1901334524154663 = 1.1825898885726929 + 0.001 * 7.543581008911133
Epoch 140, val loss: 1.2842479944229126
Epoch 150, training loss: 1.0908434391021729 = 1.0833040475845337 + 0.001 * 7.5393595695495605
Epoch 150, val loss: 1.2061713933944702
Epoch 160, training loss: 0.9928610920906067 = 0.9853310585021973 + 0.001 * 7.53005838394165
Epoch 160, val loss: 1.1298307180404663
Epoch 170, training loss: 0.9002422094345093 = 0.8927265405654907 + 0.001 * 7.515646457672119
Epoch 170, val loss: 1.0586014986038208
Epoch 180, training loss: 0.8172159194946289 = 0.809722363948822 + 0.001 * 7.4935503005981445
Epoch 180, val loss: 0.9958809018135071
Epoch 190, training loss: 0.7456569671630859 = 0.7381958961486816 + 0.001 * 7.461067199707031
Epoch 190, val loss: 0.9433294534683228
Epoch 200, training loss: 0.6842345595359802 = 0.6768201589584351 + 0.001 * 7.414393424987793
Epoch 200, val loss: 0.9003696441650391
Epoch 210, training loss: 0.6298947930335999 = 0.622525155544281 + 0.001 * 7.369659900665283
Epoch 210, val loss: 0.8649901747703552
Epoch 220, training loss: 0.5797908306121826 = 0.5724472999572754 + 0.001 * 7.343516826629639
Epoch 220, val loss: 0.8353464603424072
Epoch 230, training loss: 0.5323150157928467 = 0.5249923467636108 + 0.001 * 7.322678089141846
Epoch 230, val loss: 0.8107488751411438
Epoch 240, training loss: 0.4869813919067383 = 0.479683518409729 + 0.001 * 7.29786491394043
Epoch 240, val loss: 0.7907549738883972
Epoch 250, training loss: 0.44378963112831116 = 0.43652549386024475 + 0.001 * 7.264136791229248
Epoch 250, val loss: 0.7750151753425598
Epoch 260, training loss: 0.4028257429599762 = 0.3955877125263214 + 0.001 * 7.238035202026367
Epoch 260, val loss: 0.7631921768188477
Epoch 270, training loss: 0.36415186524391174 = 0.3569325804710388 + 0.001 * 7.219276428222656
Epoch 270, val loss: 0.7549758553504944
Epoch 280, training loss: 0.3278256952762604 = 0.32061585783958435 + 0.001 * 7.2098307609558105
Epoch 280, val loss: 0.7499508261680603
Epoch 290, training loss: 0.2939315140247345 = 0.28671738505363464 + 0.001 * 7.214120864868164
Epoch 290, val loss: 0.7480015158653259
Epoch 300, training loss: 0.2625962197780609 = 0.255388468503952 + 0.001 * 7.2077436447143555
Epoch 300, val loss: 0.7490586042404175
Epoch 310, training loss: 0.23402367532253265 = 0.22681854665279388 + 0.001 * 7.205134391784668
Epoch 310, val loss: 0.7530247569084167
Epoch 320, training loss: 0.20832902193069458 = 0.2011243849992752 + 0.001 * 7.204636573791504
Epoch 320, val loss: 0.7598195672035217
Epoch 330, training loss: 0.18547041714191437 = 0.17826662957668304 + 0.001 * 7.203789710998535
Epoch 330, val loss: 0.7691938281059265
Epoch 340, training loss: 0.16529016196727753 = 0.1580871194601059 + 0.001 * 7.203039646148682
Epoch 340, val loss: 0.7808967232704163
Epoch 350, training loss: 0.14755262434482574 = 0.14034901559352875 + 0.001 * 7.2036051750183105
Epoch 350, val loss: 0.7946145534515381
Epoch 360, training loss: 0.13198035955429077 = 0.12477783113718033 + 0.001 * 7.2025275230407715
Epoch 360, val loss: 0.8099366426467896
Epoch 370, training loss: 0.11829791963100433 = 0.11109654605388641 + 0.001 * 7.20137357711792
Epoch 370, val loss: 0.826430082321167
Epoch 380, training loss: 0.1062629222869873 = 0.09906336665153503 + 0.001 * 7.199553489685059
Epoch 380, val loss: 0.8437696099281311
Epoch 390, training loss: 0.09566111117601395 = 0.08846399933099747 + 0.001 * 7.197109699249268
Epoch 390, val loss: 0.8616926670074463
Epoch 400, training loss: 0.08631150424480438 = 0.07911475747823715 + 0.001 * 7.196742534637451
Epoch 400, val loss: 0.8799616098403931
Epoch 410, training loss: 0.07804200053215027 = 0.07084734737873077 + 0.001 * 7.19465446472168
Epoch 410, val loss: 0.898354709148407
Epoch 420, training loss: 0.07070749998092651 = 0.06351426988840103 + 0.001 * 7.19323205947876
Epoch 420, val loss: 0.9166871309280396
Epoch 430, training loss: 0.06420166045427322 = 0.05700968578457832 + 0.001 * 7.191974639892578
Epoch 430, val loss: 0.9348645806312561
Epoch 440, training loss: 0.05842864140868187 = 0.05124419555068016 + 0.001 * 7.184445381164551
Epoch 440, val loss: 0.9527299404144287
Epoch 450, training loss: 0.05332982912659645 = 0.04614286869764328 + 0.001 * 7.1869611740112305
Epoch 450, val loss: 0.9701812863349915
Epoch 460, training loss: 0.0488155335187912 = 0.04163375124335289 + 0.001 * 7.181783199310303
Epoch 460, val loss: 0.9871576428413391
Epoch 470, training loss: 0.04482438042759895 = 0.037650611251592636 + 0.001 * 7.173768043518066
Epoch 470, val loss: 1.0036002397537231
Epoch 480, training loss: 0.04130154848098755 = 0.034132298082113266 + 0.001 * 7.169248104095459
Epoch 480, val loss: 1.0194752216339111
Epoch 490, training loss: 0.038222454488277435 = 0.03102368488907814 + 0.001 * 7.198768615722656
Epoch 490, val loss: 1.0347824096679688
Epoch 500, training loss: 0.03543815016746521 = 0.02827475219964981 + 0.001 * 7.163397312164307
Epoch 500, val loss: 1.0495011806488037
Epoch 510, training loss: 0.03299728408455849 = 0.025840681046247482 + 0.001 * 7.156601905822754
Epoch 510, val loss: 1.0636593103408813
Epoch 520, training loss: 0.030827974900603294 = 0.02368171513080597 + 0.001 * 7.146259307861328
Epoch 520, val loss: 1.0772548913955688
Epoch 530, training loss: 0.028940552845597267 = 0.02176305092871189 + 0.001 * 7.177501201629639
Epoch 530, val loss: 1.0902938842773438
Epoch 540, training loss: 0.027199268341064453 = 0.020053546875715256 + 0.001 * 7.145720958709717
Epoch 540, val loss: 1.102769136428833
Epoch 550, training loss: 0.02566572092473507 = 0.018521582707762718 + 0.001 * 7.144137859344482
Epoch 550, val loss: 1.1147849559783936
Epoch 560, training loss: 0.024273773655295372 = 0.017139488831162453 + 0.001 * 7.134284496307373
Epoch 560, val loss: 1.126453161239624
Epoch 570, training loss: 0.023047471418976784 = 0.01588950864970684 + 0.001 * 7.157962799072266
Epoch 570, val loss: 1.1378397941589355
Epoch 580, training loss: 0.021899152547121048 = 0.014758388511836529 + 0.001 * 7.1407647132873535
Epoch 580, val loss: 1.1489028930664062
Epoch 590, training loss: 0.020908297970891 = 0.013734349980950356 + 0.001 * 7.173947811126709
Epoch 590, val loss: 1.1596717834472656
Epoch 600, training loss: 0.019933026283979416 = 0.012806873768568039 + 0.001 * 7.1261515617370605
Epoch 600, val loss: 1.1701172590255737
Epoch 610, training loss: 0.019100230187177658 = 0.011966006830334663 + 0.001 * 7.134222507476807
Epoch 610, val loss: 1.1802420616149902
Epoch 620, training loss: 0.018315471708774567 = 0.011202717199921608 + 0.001 * 7.112753868103027
Epoch 620, val loss: 1.190062403678894
Epoch 630, training loss: 0.01761452853679657 = 0.010508735664188862 + 0.001 * 7.105792999267578
Epoch 630, val loss: 1.199573040008545
Epoch 640, training loss: 0.01699049025774002 = 0.009876673109829426 + 0.001 * 7.113816261291504
Epoch 640, val loss: 1.208770990371704
Epoch 650, training loss: 0.016408436000347137 = 0.009299928322434425 + 0.001 * 7.108508110046387
Epoch 650, val loss: 1.2176762819290161
Epoch 660, training loss: 0.015899542719125748 = 0.008772654458880424 + 0.001 * 7.126888275146484
Epoch 660, val loss: 1.2262959480285645
Epoch 670, training loss: 0.015412449836730957 = 0.008289829827845097 + 0.001 * 7.12261962890625
Epoch 670, val loss: 1.2346223592758179
Epoch 680, training loss: 0.014951908029615879 = 0.007846844382584095 + 0.001 * 7.105063438415527
Epoch 680, val loss: 1.2426952123641968
Epoch 690, training loss: 0.01453758031129837 = 0.007439552340656519 + 0.001 * 7.09802770614624
Epoch 690, val loss: 1.250497579574585
Epoch 700, training loss: 0.014166828244924545 = 0.007064389064908028 + 0.001 * 7.102438449859619
Epoch 700, val loss: 1.258055329322815
Epoch 710, training loss: 0.013828922063112259 = 0.006718170829117298 + 0.001 * 7.110750198364258
Epoch 710, val loss: 1.265367865562439
Epoch 720, training loss: 0.01349114254117012 = 0.006398068740963936 + 0.001 * 7.09307336807251
Epoch 720, val loss: 1.2724522352218628
Epoch 730, training loss: 0.013225995004177094 = 0.006101639475673437 + 0.001 * 7.124354839324951
Epoch 730, val loss: 1.2792991399765015
Epoch 740, training loss: 0.012925639748573303 = 0.0058266641572117805 + 0.001 * 7.09897518157959
Epoch 740, val loss: 1.285933256149292
Epoch 750, training loss: 0.012654656544327736 = 0.005571096669882536 + 0.001 * 7.083559513092041
Epoch 750, val loss: 1.292360544204712
Epoch 760, training loss: 0.012438138946890831 = 0.005333163309842348 + 0.001 * 7.10497522354126
Epoch 760, val loss: 1.2985918521881104
Epoch 770, training loss: 0.012198429554700851 = 0.005111353937536478 + 0.001 * 7.0870747566223145
Epoch 770, val loss: 1.304629921913147
Epoch 780, training loss: 0.011977734044194221 = 0.004904240369796753 + 0.001 * 7.073493957519531
Epoch 780, val loss: 1.3104805946350098
Epoch 790, training loss: 0.011780966073274612 = 0.004710521083325148 + 0.001 * 7.0704450607299805
Epoch 790, val loss: 1.3161499500274658
Epoch 800, training loss: 0.011625250801444054 = 0.004529062192887068 + 0.001 * 7.096187591552734
Epoch 800, val loss: 1.321658968925476
Epoch 810, training loss: 0.011434543877840042 = 0.004358922131359577 + 0.001 * 7.075622081756592
Epoch 810, val loss: 1.3270121812820435
Epoch 820, training loss: 0.011275878176093102 = 0.004199184477329254 + 0.001 * 7.076693058013916
Epoch 820, val loss: 1.3321934938430786
Epoch 830, training loss: 0.011124929413199425 = 0.004049008712172508 + 0.001 * 7.075920581817627
Epoch 830, val loss: 1.3372308015823364
Epoch 840, training loss: 0.010968858376145363 = 0.003907440230250359 + 0.001 * 7.061418056488037
Epoch 840, val loss: 1.342125415802002
Epoch 850, training loss: 0.010858794674277306 = 0.0037739197723567486 + 0.001 * 7.084874153137207
Epoch 850, val loss: 1.3468985557556152
Epoch 860, training loss: 0.010721753351390362 = 0.003647836158052087 + 0.001 * 7.073916912078857
Epoch 860, val loss: 1.3515293598175049
Epoch 870, training loss: 0.010585688054561615 = 0.0035286229103803635 + 0.001 * 7.057065486907959
Epoch 870, val loss: 1.3560380935668945
Epoch 880, training loss: 0.010474540293216705 = 0.003415745683014393 + 0.001 * 7.058794021606445
Epoch 880, val loss: 1.360433578491211
Epoch 890, training loss: 0.010386470705270767 = 0.003308715997263789 + 0.001 * 7.077754020690918
Epoch 890, val loss: 1.3647087812423706
Epoch 900, training loss: 0.010258901864290237 = 0.0032070986926555634 + 0.001 * 7.051802158355713
Epoch 900, val loss: 1.3688863515853882
Epoch 910, training loss: 0.010167340748012066 = 0.0031105901580303907 + 0.001 * 7.056750297546387
Epoch 910, val loss: 1.3729431629180908
Epoch 920, training loss: 0.01010671816766262 = 0.0030187794473022223 + 0.001 * 7.08793830871582
Epoch 920, val loss: 1.3769214153289795
Epoch 930, training loss: 0.00997469574213028 = 0.0029313224367797375 + 0.001 * 7.043372631072998
Epoch 930, val loss: 1.3807859420776367
Epoch 940, training loss: 0.00989406555891037 = 0.002847915980964899 + 0.001 * 7.046149253845215
Epoch 940, val loss: 1.384570837020874
Epoch 950, training loss: 0.009843230247497559 = 0.002768321894109249 + 0.001 * 7.074908256530762
Epoch 950, val loss: 1.3882724046707153
Epoch 960, training loss: 0.009742062538862228 = 0.0026923338882625103 + 0.001 * 7.049727916717529
Epoch 960, val loss: 1.3918757438659668
Epoch 970, training loss: 0.009663974866271019 = 0.0026196176186203957 + 0.001 * 7.044356822967529
Epoch 970, val loss: 1.395420789718628
Epoch 980, training loss: 0.00958685390651226 = 0.002549894619733095 + 0.001 * 7.036959171295166
Epoch 980, val loss: 1.3989055156707764
Epoch 990, training loss: 0.009549934417009354 = 0.0024829450994729996 + 0.001 * 7.066988945007324
Epoch 990, val loss: 1.4023447036743164
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.9692914485931396 = 1.960694670677185 + 0.001 * 8.596816062927246
Epoch 0, val loss: 1.9640116691589355
Epoch 10, training loss: 1.9585835933685303 = 1.9499868154525757 + 0.001 * 8.596736907958984
Epoch 10, val loss: 1.952891230583191
Epoch 20, training loss: 1.945373773574829 = 1.9367772340774536 + 0.001 * 8.596510887145996
Epoch 20, val loss: 1.9390751123428345
Epoch 30, training loss: 1.9269448518753052 = 1.9183489084243774 + 0.001 * 8.595991134643555
Epoch 30, val loss: 1.9198671579360962
Epoch 40, training loss: 1.8999379873275757 = 1.8913432359695435 + 0.001 * 8.594710350036621
Epoch 40, val loss: 1.8920842409133911
Epoch 50, training loss: 1.8619486093521118 = 1.8533575534820557 + 0.001 * 8.591017723083496
Epoch 50, val loss: 1.8547766208648682
Epoch 60, training loss: 1.818115234375 = 1.809537410736084 + 0.001 * 8.577874183654785
Epoch 60, val loss: 1.8157365322113037
Epoch 70, training loss: 1.7820502519607544 = 1.773532748222351 + 0.001 * 8.517537117004395
Epoch 70, val loss: 1.786594033241272
Epoch 80, training loss: 1.7434477806091309 = 1.7351980209350586 + 0.001 * 8.249713897705078
Epoch 80, val loss: 1.7512764930725098
Epoch 90, training loss: 1.688500165939331 = 1.6804041862487793 + 0.001 * 8.096014976501465
Epoch 90, val loss: 1.7012267112731934
Epoch 100, training loss: 1.612855076789856 = 1.6049445867538452 + 0.001 * 7.910436630249023
Epoch 100, val loss: 1.6347408294677734
Epoch 110, training loss: 1.516628623008728 = 1.5089266300201416 + 0.001 * 7.701937675476074
Epoch 110, val loss: 1.5518478155136108
Epoch 120, training loss: 1.4125808477401733 = 1.404959797859192 + 0.001 * 7.62108039855957
Epoch 120, val loss: 1.4643512964248657
Epoch 130, training loss: 1.3096519708633423 = 1.3020416498184204 + 0.001 * 7.610345840454102
Epoch 130, val loss: 1.3777583837509155
Epoch 140, training loss: 1.2076730728149414 = 1.2000839710235596 + 0.001 * 7.5890583992004395
Epoch 140, val loss: 1.2934969663619995
Epoch 150, training loss: 1.1066447496414185 = 1.099084734916687 + 0.001 * 7.560048580169678
Epoch 150, val loss: 1.2113932371139526
Epoch 160, training loss: 1.0087289810180664 = 1.0012145042419434 + 0.001 * 7.514493942260742
Epoch 160, val loss: 1.1333355903625488
Epoch 170, training loss: 0.9167619943618774 = 0.9093023538589478 + 0.001 * 7.4596638679504395
Epoch 170, val loss: 1.06089448928833
Epoch 180, training loss: 0.8331695795059204 = 0.8257585167884827 + 0.001 * 7.411033630371094
Epoch 180, val loss: 0.9959987998008728
Epoch 190, training loss: 0.7593643069267273 = 0.7519944906234741 + 0.001 * 7.3697967529296875
Epoch 190, val loss: 0.9401702284812927
Epoch 200, training loss: 0.6952086091041565 = 0.6878722906112671 + 0.001 * 7.3362956047058105
Epoch 200, val loss: 0.8938757181167603
Epoch 210, training loss: 0.6388683915138245 = 0.6315654516220093 + 0.001 * 7.302911758422852
Epoch 210, val loss: 0.856462836265564
Epoch 220, training loss: 0.587988555431366 = 0.5807210206985474 + 0.001 * 7.267521858215332
Epoch 220, val loss: 0.8266557455062866
Epoch 230, training loss: 0.5409033894538879 = 0.5336639881134033 + 0.001 * 7.2394208908081055
Epoch 230, val loss: 0.8032102584838867
Epoch 240, training loss: 0.49690505862236023 = 0.4896760880947113 + 0.001 * 7.228962421417236
Epoch 240, val loss: 0.785209059715271
Epoch 250, training loss: 0.45573845505714417 = 0.4485217332839966 + 0.001 * 7.216729164123535
Epoch 250, val loss: 0.771878182888031
Epoch 260, training loss: 0.41747674345970154 = 0.41026023030281067 + 0.001 * 7.216500282287598
Epoch 260, val loss: 0.7627816796302795
Epoch 270, training loss: 0.38241246342658997 = 0.37519925832748413 + 0.001 * 7.2132134437561035
Epoch 270, val loss: 0.7577815651893616
Epoch 280, training loss: 0.35083967447280884 = 0.3436281681060791 + 0.001 * 7.211517810821533
Epoch 280, val loss: 0.7565304040908813
Epoch 290, training loss: 0.32266294956207275 = 0.31545332074165344 + 0.001 * 7.209620475769043
Epoch 290, val loss: 0.7585287094116211
Epoch 300, training loss: 0.29733923077583313 = 0.2901325523853302 + 0.001 * 7.20668363571167
Epoch 300, val loss: 0.7631970643997192
Epoch 310, training loss: 0.27406689524650574 = 0.26686298847198486 + 0.001 * 7.203895092010498
Epoch 310, val loss: 0.769708514213562
Epoch 320, training loss: 0.25191614031791687 = 0.24471473693847656 + 0.001 * 7.201414108276367
Epoch 320, val loss: 0.7774173617362976
Epoch 330, training loss: 0.2299336940050125 = 0.22273299098014832 + 0.001 * 7.200703144073486
Epoch 330, val loss: 0.7855820059776306
Epoch 340, training loss: 0.20745818316936493 = 0.20026135444641113 + 0.001 * 7.1968255043029785
Epoch 340, val loss: 0.7935439944267273
Epoch 350, training loss: 0.18465352058410645 = 0.17745694518089294 + 0.001 * 7.196568965911865
Epoch 350, val loss: 0.8014167547225952
Epoch 360, training loss: 0.1626148223876953 = 0.15542148053646088 + 0.001 * 7.193345546722412
Epoch 360, val loss: 0.8099552392959595
Epoch 370, training loss: 0.1426047533750534 = 0.13541583716869354 + 0.001 * 7.188910007476807
Epoch 370, val loss: 0.8198179602622986
Epoch 380, training loss: 0.12529343366622925 = 0.11810647696256638 + 0.001 * 7.186953067779541
Epoch 380, val loss: 0.8315600752830505
Epoch 390, training loss: 0.11064951121807098 = 0.10346430540084839 + 0.001 * 7.185206413269043
Epoch 390, val loss: 0.8451842069625854
Epoch 400, training loss: 0.0983133316040039 = 0.0911368802189827 + 0.001 * 7.176450252532959
Epoch 400, val loss: 0.8602980375289917
Epoch 410, training loss: 0.08789347857236862 = 0.080715611577034 + 0.001 * 7.17786979675293
Epoch 410, val loss: 0.8764198422431946
Epoch 420, training loss: 0.07900252938270569 = 0.0718347430229187 + 0.001 * 7.167784690856934
Epoch 420, val loss: 0.8932063579559326
Epoch 430, training loss: 0.07137230038642883 = 0.06420222669839859 + 0.001 * 7.170076847076416
Epoch 430, val loss: 0.9102613925933838
Epoch 440, training loss: 0.0647604689002037 = 0.05759696289896965 + 0.001 * 7.163508415222168
Epoch 440, val loss: 0.9273545742034912
Epoch 450, training loss: 0.05900762230157852 = 0.05184735357761383 + 0.001 * 7.160266399383545
Epoch 450, val loss: 0.9443169236183167
Epoch 460, training loss: 0.05397646874189377 = 0.04682039096951485 + 0.001 * 7.156075477600098
Epoch 460, val loss: 0.9610522389411926
Epoch 470, training loss: 0.04955245554447174 = 0.04240839183330536 + 0.001 * 7.144062519073486
Epoch 470, val loss: 0.9774684309959412
Epoch 480, training loss: 0.04568028077483177 = 0.038523633033037186 + 0.001 * 7.156646728515625
Epoch 480, val loss: 0.9935258626937866
Epoch 490, training loss: 0.042244262993335724 = 0.035094358026981354 + 0.001 * 7.149904251098633
Epoch 490, val loss: 1.0091712474822998
Epoch 500, training loss: 0.03919818252325058 = 0.03205864503979683 + 0.001 * 7.139537334442139
Epoch 500, val loss: 1.0244028568267822
Epoch 510, training loss: 0.03649919480085373 = 0.029363999143242836 + 0.001 * 7.135195255279541
Epoch 510, val loss: 1.0392125844955444
Epoch 520, training loss: 0.034098997712135315 = 0.026966042816638947 + 0.001 * 7.132953643798828
Epoch 520, val loss: 1.0535939931869507
Epoch 530, training loss: 0.03198254853487015 = 0.024827122688293457 + 0.001 * 7.15542459487915
Epoch 530, val loss: 1.0675681829452515
Epoch 540, training loss: 0.030044708400964737 = 0.022915037348866463 + 0.001 * 7.129669666290283
Epoch 540, val loss: 1.081138014793396
Epoch 550, training loss: 0.028330765664577484 = 0.02120157703757286 + 0.001 * 7.129188060760498
Epoch 550, val loss: 1.0942941904067993
Epoch 560, training loss: 0.026788800954818726 = 0.01966245286166668 + 0.001 * 7.126348495483398
Epoch 560, val loss: 1.1070562601089478
Epoch 570, training loss: 0.025409307330846786 = 0.01827673800289631 + 0.001 * 7.132568359375
Epoch 570, val loss: 1.119409203529358
Epoch 580, training loss: 0.024148788303136826 = 0.017026687040925026 + 0.001 * 7.122100353240967
Epoch 580, val loss: 1.1313844919204712
Epoch 590, training loss: 0.02301677316427231 = 0.01589653268456459 + 0.001 * 7.120239734649658
Epoch 590, val loss: 1.1429766416549683
Epoch 600, training loss: 0.02200094424188137 = 0.01487243827432394 + 0.001 * 7.128505229949951
Epoch 600, val loss: 1.154207706451416
Epoch 610, training loss: 0.021067436784505844 = 0.0139422956854105 + 0.001 * 7.12514066696167
Epoch 610, val loss: 1.1650872230529785
Epoch 620, training loss: 0.020218007266521454 = 0.013095617294311523 + 0.001 * 7.122390270233154
Epoch 620, val loss: 1.1756490468978882
Epoch 630, training loss: 0.019453777000308037 = 0.01232326589524746 + 0.001 * 7.1305108070373535
Epoch 630, val loss: 1.1858989000320435
Epoch 640, training loss: 0.01873287744820118 = 0.011617284268140793 + 0.001 * 7.115593433380127
Epoch 640, val loss: 1.195849895477295
Epoch 650, training loss: 0.01808393932878971 = 0.010970627889037132 + 0.001 * 7.113311290740967
Epoch 650, val loss: 1.205513834953308
Epoch 660, training loss: 0.017520572990179062 = 0.01037716306746006 + 0.001 * 7.14340877532959
Epoch 660, val loss: 1.2148789167404175
Epoch 670, training loss: 0.016944916918873787 = 0.009831216186285019 + 0.001 * 7.113700866699219
Epoch 670, val loss: 1.2239693403244019
Epoch 680, training loss: 0.01643708348274231 = 0.009327523410320282 + 0.001 * 7.109559535980225
Epoch 680, val loss: 1.2328293323516846
Epoch 690, training loss: 0.015999257564544678 = 0.008860733360052109 + 0.001 * 7.138524055480957
Epoch 690, val loss: 1.241485357284546
Epoch 700, training loss: 0.015539351850748062 = 0.00842601153999567 + 0.001 * 7.113340377807617
Epoch 700, val loss: 1.2500019073486328
Epoch 710, training loss: 0.015122013166546822 = 0.008019250817596912 + 0.001 * 7.102761745452881
Epoch 710, val loss: 1.2584270238876343
Epoch 720, training loss: 0.014737012796103954 = 0.0076375254429876804 + 0.001 * 7.099486827850342
Epoch 720, val loss: 1.2667993307113647
Epoch 730, training loss: 0.014434892684221268 = 0.007278516888618469 + 0.001 * 7.156375408172607
Epoch 730, val loss: 1.2751206159591675
Epoch 740, training loss: 0.014046428725123405 = 0.006940258666872978 + 0.001 * 7.106170177459717
Epoch 740, val loss: 1.2833561897277832
Epoch 750, training loss: 0.01371719315648079 = 0.006620789412409067 + 0.001 * 7.096403121948242
Epoch 750, val loss: 1.2915054559707642
Epoch 760, training loss: 0.013409215956926346 = 0.006318673957139254 + 0.001 * 7.090542316436768
Epoch 760, val loss: 1.2995738983154297
Epoch 770, training loss: 0.0131427813321352 = 0.006033281330019236 + 0.001 * 7.109498977661133
Epoch 770, val loss: 1.3075453042984009
Epoch 780, training loss: 0.01286090537905693 = 0.005764098837971687 + 0.001 * 7.096806526184082
Epoch 780, val loss: 1.31538724899292
Epoch 790, training loss: 0.012602932751178741 = 0.005510349292308092 + 0.001 * 7.092582702636719
Epoch 790, val loss: 1.3231030702590942
Epoch 800, training loss: 0.012355649843811989 = 0.005271485541015863 + 0.001 * 7.084164619445801
Epoch 800, val loss: 1.3306952714920044
Epoch 810, training loss: 0.012128917500376701 = 0.0050467755645513535 + 0.001 * 7.082140922546387
Epoch 810, val loss: 1.3381375074386597
Epoch 820, training loss: 0.01193564385175705 = 0.004835418425500393 + 0.001 * 7.10022497177124
Epoch 820, val loss: 1.3454300165176392
Epoch 830, training loss: 0.011716347187757492 = 0.004636721219867468 + 0.001 * 7.079625606536865
Epoch 830, val loss: 1.3525691032409668
Epoch 840, training loss: 0.011541062965989113 = 0.004449889529496431 + 0.001 * 7.0911736488342285
Epoch 840, val loss: 1.3595460653305054
Epoch 850, training loss: 0.011375878006219864 = 0.00427418015897274 + 0.001 * 7.1016974449157715
Epoch 850, val loss: 1.3663674592971802
Epoch 860, training loss: 0.011183279566466808 = 0.004108823835849762 + 0.001 * 7.074455261230469
Epoch 860, val loss: 1.3730183839797974
Epoch 870, training loss: 0.011020814068615437 = 0.003953133709728718 + 0.001 * 7.0676798820495605
Epoch 870, val loss: 1.3795087337493896
Epoch 880, training loss: 0.010881390422582626 = 0.0038063719402998686 + 0.001 * 7.075018405914307
Epoch 880, val loss: 1.3858416080474854
Epoch 890, training loss: 0.010729992762207985 = 0.003667804878205061 + 0.001 * 7.062188148498535
Epoch 890, val loss: 1.392016887664795
Epoch 900, training loss: 0.010626869276165962 = 0.0035366013180464506 + 0.001 * 7.090267658233643
Epoch 900, val loss: 1.398044466972351
Epoch 910, training loss: 0.01050114817917347 = 0.003411763347685337 + 0.001 * 7.089384078979492
Epoch 910, val loss: 1.403952956199646
Epoch 920, training loss: 0.010363071225583553 = 0.0032922292593866587 + 0.001 * 7.0708417892456055
Epoch 920, val loss: 1.4097542762756348
Epoch 930, training loss: 0.010255610570311546 = 0.0031770227942615747 + 0.001 * 7.078587055206299
Epoch 930, val loss: 1.4154940843582153
Epoch 940, training loss: 0.010121175087988377 = 0.003065580502152443 + 0.001 * 7.055594444274902
Epoch 940, val loss: 1.4211889505386353
Epoch 950, training loss: 0.010019820183515549 = 0.002957563381642103 + 0.001 * 7.062256813049316
Epoch 950, val loss: 1.426829218864441
Epoch 960, training loss: 0.009935563430190086 = 0.0028529695700854063 + 0.001 * 7.0825934410095215
Epoch 960, val loss: 1.4324359893798828
Epoch 970, training loss: 0.00982317328453064 = 0.002751955296844244 + 0.001 * 7.0712175369262695
Epoch 970, val loss: 1.4379832744598389
Epoch 980, training loss: 0.009699579328298569 = 0.002654698910191655 + 0.001 * 7.044879913330078
Epoch 980, val loss: 1.4434810876846313
Epoch 990, training loss: 0.009634748101234436 = 0.0025613142643123865 + 0.001 * 7.073433876037598
Epoch 990, val loss: 1.4489158391952515
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 1.9547231197357178 = 1.9461263418197632 + 0.001 * 8.596793174743652
Epoch 0, val loss: 1.9517940282821655
Epoch 10, training loss: 1.944580078125 = 1.9359833002090454 + 0.001 * 8.596748352050781
Epoch 10, val loss: 1.941096305847168
Epoch 20, training loss: 1.9325990676879883 = 1.9240025281906128 + 0.001 * 8.596559524536133
Epoch 20, val loss: 1.9282324314117432
Epoch 30, training loss: 1.916284441947937 = 1.9076882600784302 + 0.001 * 8.596134185791016
Epoch 30, val loss: 1.910661220550537
Epoch 40, training loss: 1.8926924467086792 = 1.8840973377227783 + 0.001 * 8.5951566696167
Epoch 40, val loss: 1.8854029178619385
Epoch 50, training loss: 1.8595325946807861 = 1.8509401082992554 + 0.001 * 8.592513084411621
Epoch 50, val loss: 1.850748062133789
Epoch 60, training loss: 1.8196609020233154 = 1.8110774755477905 + 0.001 * 8.583427429199219
Epoch 60, val loss: 1.8117718696594238
Epoch 70, training loss: 1.7823668718338013 = 1.7738239765167236 + 0.001 * 8.542912483215332
Epoch 70, val loss: 1.7791521549224854
Epoch 80, training loss: 1.7402688264846802 = 1.731989860534668 + 0.001 * 8.278914451599121
Epoch 80, val loss: 1.743260383605957
Epoch 90, training loss: 1.679154396057129 = 1.6711561679840088 + 0.001 * 7.998172283172607
Epoch 90, val loss: 1.6906906366348267
Epoch 100, training loss: 1.594766616821289 = 1.5869426727294922 + 0.001 * 7.823892593383789
Epoch 100, val loss: 1.6182751655578613
Epoch 110, training loss: 1.4888508319854736 = 1.4812129735946655 + 0.001 * 7.637910842895508
Epoch 110, val loss: 1.5284854173660278
Epoch 120, training loss: 1.3727635145187378 = 1.3651840686798096 + 0.001 * 7.579466342926025
Epoch 120, val loss: 1.4317563772201538
Epoch 130, training loss: 1.2572970390319824 = 1.2497236728668213 + 0.001 * 7.573397636413574
Epoch 130, val loss: 1.3388428688049316
Epoch 140, training loss: 1.1476523876190186 = 1.1400948762893677 + 0.001 * 7.557482719421387
Epoch 140, val loss: 1.2539023160934448
Epoch 150, training loss: 1.045699954032898 = 1.0381559133529663 + 0.001 * 7.544023036956787
Epoch 150, val loss: 1.1764885187149048
Epoch 160, training loss: 0.9514966607093811 = 0.9439735412597656 + 0.001 * 7.523134231567383
Epoch 160, val loss: 1.1051956415176392
Epoch 170, training loss: 0.8651795387268066 = 0.8576952219009399 + 0.001 * 7.484311580657959
Epoch 170, val loss: 1.0389481782913208
Epoch 180, training loss: 0.7872838377952576 = 0.7798759937286377 + 0.001 * 7.407832145690918
Epoch 180, val loss: 0.9785308837890625
Epoch 190, training loss: 0.7180706858634949 = 0.7107847332954407 + 0.001 * 7.285929203033447
Epoch 190, val loss: 0.9253406524658203
Epoch 200, training loss: 0.6568686366081238 = 0.6496424078941345 + 0.001 * 7.226235866546631
Epoch 200, val loss: 0.8799681663513184
Epoch 210, training loss: 0.6015700101852417 = 0.5943720936775208 + 0.001 * 7.197891712188721
Epoch 210, val loss: 0.8421672582626343
Epoch 220, training loss: 0.5497666001319885 = 0.5425748229026794 + 0.001 * 7.191761016845703
Epoch 220, val loss: 0.8106690645217896
Epoch 230, training loss: 0.4997066557407379 = 0.4925157427787781 + 0.001 * 7.190908432006836
Epoch 230, val loss: 0.784135639667511
Epoch 240, training loss: 0.4507502019405365 = 0.44355958700180054 + 0.001 * 7.190628528594971
Epoch 240, val loss: 0.7614357471466064
Epoch 250, training loss: 0.40312305092811584 = 0.39593321084976196 + 0.001 * 7.189849853515625
Epoch 250, val loss: 0.7423627972602844
Epoch 260, training loss: 0.3575834035873413 = 0.35039445757865906 + 0.001 * 7.1889424324035645
Epoch 260, val loss: 0.7271531820297241
Epoch 270, training loss: 0.3150177001953125 = 0.30782943964004517 + 0.001 * 7.188258647918701
Epoch 270, val loss: 0.7163040637969971
Epoch 280, training loss: 0.27619853615760803 = 0.26901066303253174 + 0.001 * 7.1878790855407715
Epoch 280, val loss: 0.7101083993911743
Epoch 290, training loss: 0.24164943397045135 = 0.23446178436279297 + 0.001 * 7.187652587890625
Epoch 290, val loss: 0.7084749937057495
Epoch 300, training loss: 0.21147961914539337 = 0.2042916864156723 + 0.001 * 7.187933921813965
Epoch 300, val loss: 0.7111777663230896
Epoch 310, training loss: 0.18536867201328278 = 0.17818155884742737 + 0.001 * 7.187119007110596
Epoch 310, val loss: 0.7173969745635986
Epoch 320, training loss: 0.16281938552856445 = 0.15563102066516876 + 0.001 * 7.1883625984191895
Epoch 320, val loss: 0.7263757586479187
Epoch 330, training loss: 0.14331892132759094 = 0.13613249361515045 + 0.001 * 7.18643045425415
Epoch 330, val loss: 0.7372755408287048
Epoch 340, training loss: 0.1264238953590393 = 0.11923829466104507 + 0.001 * 7.185595989227295
Epoch 340, val loss: 0.7494052648544312
Epoch 350, training loss: 0.11177033185958862 = 0.10458540916442871 + 0.001 * 7.184922218322754
Epoch 350, val loss: 0.7623315453529358
Epoch 360, training loss: 0.09905478358268738 = 0.09187076985836029 + 0.001 * 7.184009552001953
Epoch 360, val loss: 0.7756907343864441
Epoch 370, training loss: 0.0880265012383461 = 0.08084411174058914 + 0.001 * 7.1823883056640625
Epoch 370, val loss: 0.7892515659332275
Epoch 380, training loss: 0.07847367227077484 = 0.07129223644733429 + 0.001 * 7.181435585021973
Epoch 380, val loss: 0.8028160333633423
Epoch 390, training loss: 0.07021155208349228 = 0.06302893906831741 + 0.001 * 7.182615756988525
Epoch 390, val loss: 0.8163185715675354
Epoch 400, training loss: 0.06306777894496918 = 0.055888522416353226 + 0.001 * 7.179256916046143
Epoch 400, val loss: 0.8296546339988708
Epoch 410, training loss: 0.0568976067006588 = 0.049720823764801025 + 0.001 * 7.176783561706543
Epoch 410, val loss: 0.8427292108535767
Epoch 420, training loss: 0.051567643880844116 = 0.044392310082912445 + 0.001 * 7.1753339767456055
Epoch 420, val loss: 0.8555366396903992
Epoch 430, training loss: 0.04695643112063408 = 0.039784759283065796 + 0.001 * 7.171671390533447
Epoch 430, val loss: 0.8680076599121094
Epoch 440, training loss: 0.04296473413705826 = 0.03579418733716011 + 0.001 * 7.1705451011657715
Epoch 440, val loss: 0.8801491856575012
Epoch 450, training loss: 0.03949613496661186 = 0.0323299914598465 + 0.001 * 7.166143894195557
Epoch 450, val loss: 0.8919523358345032
Epoch 460, training loss: 0.036481551826000214 = 0.029313884675502777 + 0.001 * 7.167668342590332
Epoch 460, val loss: 0.9034152030944824
Epoch 470, training loss: 0.03383966535329819 = 0.02667996473610401 + 0.001 * 7.159701824188232
Epoch 470, val loss: 0.9145265817642212
Epoch 480, training loss: 0.03153593838214874 = 0.024371787905693054 + 0.001 * 7.164150238037109
Epoch 480, val loss: 0.9252673983573914
Epoch 490, training loss: 0.029492231085896492 = 0.022341987118124962 + 0.001 * 7.150243282318115
Epoch 490, val loss: 0.9356760382652283
Epoch 500, training loss: 0.02769489958882332 = 0.02055068500339985 + 0.001 * 7.1442131996154785
Epoch 500, val loss: 0.9457442164421082
Epoch 510, training loss: 0.02610723115503788 = 0.018964247778058052 + 0.001 * 7.142983436584473
Epoch 510, val loss: 0.9555003046989441
Epoch 520, training loss: 0.02468801476061344 = 0.017554180696606636 + 0.001 * 7.133833408355713
Epoch 520, val loss: 0.9649387001991272
Epoch 530, training loss: 0.023425305262207985 = 0.01629655435681343 + 0.001 * 7.128750801086426
Epoch 530, val loss: 0.9740692377090454
Epoch 540, training loss: 0.02229267917573452 = 0.015170973725616932 + 0.001 * 7.121705055236816
Epoch 540, val loss: 0.9828993082046509
Epoch 550, training loss: 0.021279025822877884 = 0.014160236343741417 + 0.001 * 7.118790149688721
Epoch 550, val loss: 0.9914394617080688
Epoch 560, training loss: 0.020371045917272568 = 0.013249790295958519 + 0.001 * 7.1212544441223145
Epoch 560, val loss: 0.9997008442878723
Epoch 570, training loss: 0.019538946449756622 = 0.012427139095962048 + 0.001 * 7.111806869506836
Epoch 570, val loss: 1.007704734802246
Epoch 580, training loss: 0.018792375922203064 = 0.011681430973112583 + 0.001 * 7.110945701599121
Epoch 580, val loss: 1.015473484992981
Epoch 590, training loss: 0.018116790801286697 = 0.011003517545759678 + 0.001 * 7.1132731437683105
Epoch 590, val loss: 1.0229995250701904
Epoch 600, training loss: 0.017500633373856544 = 0.0103856660425663 + 0.001 * 7.114966869354248
Epoch 600, val loss: 1.0303057432174683
Epoch 610, training loss: 0.016933709383010864 = 0.009821032173931599 + 0.001 * 7.11267614364624
Epoch 610, val loss: 1.0373775959014893
Epoch 620, training loss: 0.01640945114195347 = 0.009303529746830463 + 0.001 * 7.105921745300293
Epoch 620, val loss: 1.0442355871200562
Epoch 630, training loss: 0.015928026288747787 = 0.00882754661142826 + 0.001 * 7.1004791259765625
Epoch 630, val loss: 1.0509064197540283
Epoch 640, training loss: 0.015483180060982704 = 0.008387908339500427 + 0.001 * 7.095271587371826
Epoch 640, val loss: 1.057387113571167
Epoch 650, training loss: 0.015106228180229664 = 0.0079796202480793 + 0.001 * 7.126607418060303
Epoch 650, val loss: 1.063769817352295
Epoch 660, training loss: 0.014694897457957268 = 0.007599486969411373 + 0.001 * 7.095410346984863
Epoch 660, val loss: 1.0699729919433594
Epoch 670, training loss: 0.014337742701172829 = 0.00724436528980732 + 0.001 * 7.093377590179443
Epoch 670, val loss: 1.0760802030563354
Epoch 680, training loss: 0.014004284515976906 = 0.0069119492545723915 + 0.001 * 7.092334270477295
Epoch 680, val loss: 1.0820527076721191
Epoch 690, training loss: 0.013690197840332985 = 0.006600654684007168 + 0.001 * 7.089543342590332
Epoch 690, val loss: 1.087897539138794
Epoch 700, training loss: 0.013398357667028904 = 0.006309102289378643 + 0.001 * 7.089254856109619
Epoch 700, val loss: 1.0936071872711182
Epoch 710, training loss: 0.01313088834285736 = 0.006035857833921909 + 0.001 * 7.095029830932617
Epoch 710, val loss: 1.099190592765808
Epoch 720, training loss: 0.012866408564150333 = 0.005779858212918043 + 0.001 * 7.086550235748291
Epoch 720, val loss: 1.1046373844146729
Epoch 730, training loss: 0.01261809654533863 = 0.005539811681956053 + 0.001 * 7.078284740447998
Epoch 730, val loss: 1.1099772453308105
Epoch 740, training loss: 0.012390924617648125 = 0.005314641632139683 + 0.001 * 7.076282978057861
Epoch 740, val loss: 1.1151963472366333
Epoch 750, training loss: 0.012176532298326492 = 0.005103382281959057 + 0.001 * 7.073149681091309
Epoch 750, val loss: 1.1202871799468994
Epoch 760, training loss: 0.011983184143900871 = 0.004904985893517733 + 0.001 * 7.078197479248047
Epoch 760, val loss: 1.1252634525299072
Epoch 770, training loss: 0.011800851672887802 = 0.004718522075563669 + 0.001 * 7.082329750061035
Epoch 770, val loss: 1.1301357746124268
Epoch 780, training loss: 0.011622106656432152 = 0.0045430827885866165 + 0.001 * 7.079023361206055
Epoch 780, val loss: 1.1348986625671387
Epoch 790, training loss: 0.011446647346019745 = 0.004377915058284998 + 0.001 * 7.068731307983398
Epoch 790, val loss: 1.1395454406738281
Epoch 800, training loss: 0.01129874400794506 = 0.004222280345857143 + 0.001 * 7.076463222503662
Epoch 800, val loss: 1.1441004276275635
Epoch 810, training loss: 0.01114451140165329 = 0.004075504839420319 + 0.001 * 7.06900691986084
Epoch 810, val loss: 1.148543357849121
Epoch 820, training loss: 0.01101228129118681 = 0.003936951979994774 + 0.001 * 7.075328826904297
Epoch 820, val loss: 1.1528832912445068
Epoch 830, training loss: 0.010875595733523369 = 0.0038060268852859735 + 0.001 * 7.069568157196045
Epoch 830, val loss: 1.1571252346038818
Epoch 840, training loss: 0.010753177106380463 = 0.0036822319962084293 + 0.001 * 7.070944309234619
Epoch 840, val loss: 1.1612781286239624
Epoch 850, training loss: 0.01064059603959322 = 0.0035650774370878935 + 0.001 * 7.0755181312561035
Epoch 850, val loss: 1.1653310060501099
Epoch 860, training loss: 0.010511262342333794 = 0.003454087534919381 + 0.001 * 7.057174205780029
Epoch 860, val loss: 1.1692852973937988
Epoch 870, training loss: 0.010404976084828377 = 0.0033488129265606403 + 0.001 * 7.0561628341674805
Epoch 870, val loss: 1.1731603145599365
Epoch 880, training loss: 0.010309483855962753 = 0.0032488987781107426 + 0.001 * 7.060585021972656
Epoch 880, val loss: 1.176958441734314
Epoch 890, training loss: 0.010209402069449425 = 0.003153993282467127 + 0.001 * 7.055408477783203
Epoch 890, val loss: 1.180654764175415
Epoch 900, training loss: 0.010129758156836033 = 0.0030637483578175306 + 0.001 * 7.066009521484375
Epoch 900, val loss: 1.1842793226242065
Epoch 910, training loss: 0.010043813847005367 = 0.0029779132455587387 + 0.001 * 7.0659003257751465
Epoch 910, val loss: 1.1878235340118408
Epoch 920, training loss: 0.00995751190930605 = 0.002896237885579467 + 0.001 * 7.061273574829102
Epoch 920, val loss: 1.1912950277328491
Epoch 930, training loss: 0.009877661243081093 = 0.002818403299897909 + 0.001 * 7.0592570304870605
Epoch 930, val loss: 1.1946972608566284
Epoch 940, training loss: 0.00980587862432003 = 0.0027442127466201782 + 0.001 * 7.0616655349731445
Epoch 940, val loss: 1.1980334520339966
Epoch 950, training loss: 0.00972271803766489 = 0.0026733994018286467 + 0.001 * 7.049318313598633
Epoch 950, val loss: 1.2012962102890015
Epoch 960, training loss: 0.009668469429016113 = 0.002605798887088895 + 0.001 * 7.0626702308654785
Epoch 960, val loss: 1.2044868469238281
Epoch 970, training loss: 0.009595915675163269 = 0.0025411811657249928 + 0.001 * 7.054734230041504
Epoch 970, val loss: 1.2076257467269897
Epoch 980, training loss: 0.009535344317555428 = 0.0024793818593025208 + 0.001 * 7.055962562561035
Epoch 980, val loss: 1.2106951475143433
Epoch 990, training loss: 0.009484951384365559 = 0.0024201907217502594 + 0.001 * 7.064760208129883
Epoch 990, val loss: 1.2137126922607422
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.845545598313126
The final CL Acc:0.80123, 0.01666, The final GNN Acc:0.84186, 0.00301
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11566])
remove edge: torch.Size([2, 9446])
updated graph: torch.Size([2, 10456])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9465324878692627 = 1.937935709953308 + 0.001 * 8.596837043762207
Epoch 0, val loss: 1.9360482692718506
Epoch 10, training loss: 1.9367221593856812 = 1.9281253814697266 + 0.001 * 8.596783638000488
Epoch 10, val loss: 1.9255144596099854
Epoch 20, training loss: 1.924569845199585 = 1.91597318649292 + 0.001 * 8.59662914276123
Epoch 20, val loss: 1.9123603105545044
Epoch 30, training loss: 1.9075602293014526 = 1.8989639282226562 + 0.001 * 8.596282958984375
Epoch 30, val loss: 1.8940379619598389
Epoch 40, training loss: 1.8828840255737305 = 1.874288558959961 + 0.001 * 8.595443725585938
Epoch 40, val loss: 1.8680368661880493
Epoch 50, training loss: 1.8496687412261963 = 1.8410756587982178 + 0.001 * 8.593099594116211
Epoch 50, val loss: 1.835125207901001
Epoch 60, training loss: 1.814913272857666 = 1.8063280582427979 + 0.001 * 8.585198402404785
Epoch 60, val loss: 1.8050861358642578
Epoch 70, training loss: 1.7859630584716797 = 1.7774156332015991 + 0.001 * 8.54743480682373
Epoch 70, val loss: 1.7835084199905396
Epoch 80, training loss: 1.7490589618682861 = 1.7407724857330322 + 0.001 * 8.286528587341309
Epoch 80, val loss: 1.7528467178344727
Epoch 90, training loss: 1.6971315145492554 = 1.6890066862106323 + 0.001 * 8.124855041503906
Epoch 90, val loss: 1.7077083587646484
Epoch 100, training loss: 1.626664161682129 = 1.618721604347229 + 0.001 * 7.942564487457275
Epoch 100, val loss: 1.6469614505767822
Epoch 110, training loss: 1.5426490306854248 = 1.534934163093567 + 0.001 * 7.714903354644775
Epoch 110, val loss: 1.5768331289291382
Epoch 120, training loss: 1.4539580345153809 = 1.446340560913086 + 0.001 * 7.6174492835998535
Epoch 120, val loss: 1.5055712461471558
Epoch 130, training loss: 1.3630890846252441 = 1.355545163154602 + 0.001 * 7.543980598449707
Epoch 130, val loss: 1.4343408346176147
Epoch 140, training loss: 1.2689422369003296 = 1.2614166736602783 + 0.001 * 7.525566577911377
Epoch 140, val loss: 1.3609585762023926
Epoch 150, training loss: 1.1715794801712036 = 1.164063572883606 + 0.001 * 7.515944480895996
Epoch 150, val loss: 1.2860180139541626
Epoch 160, training loss: 1.0738675594329834 = 1.0663633346557617 + 0.001 * 7.5041656494140625
Epoch 160, val loss: 1.2114713191986084
Epoch 170, training loss: 0.9792543649673462 = 0.97176194190979 + 0.001 * 7.492404937744141
Epoch 170, val loss: 1.1401010751724243
Epoch 180, training loss: 0.8898183107376099 = 0.8823428153991699 + 0.001 * 7.475502014160156
Epoch 180, val loss: 1.0738035440444946
Epoch 190, training loss: 0.8060094118118286 = 0.798561692237854 + 0.001 * 7.447699546813965
Epoch 190, val loss: 1.0125187635421753
Epoch 200, training loss: 0.7280526757240295 = 0.7206383943557739 + 0.001 * 7.414309501647949
Epoch 200, val loss: 0.9570149779319763
Epoch 210, training loss: 0.6559590697288513 = 0.6485746502876282 + 0.001 * 7.3844194412231445
Epoch 210, val loss: 0.9071556925773621
Epoch 220, training loss: 0.5895260572433472 = 0.5821545124053955 + 0.001 * 7.371535778045654
Epoch 220, val loss: 0.8639299273490906
Epoch 230, training loss: 0.5282538533210754 = 0.5208885669708252 + 0.001 * 7.365261554718018
Epoch 230, val loss: 0.827768862247467
Epoch 240, training loss: 0.4716864228248596 = 0.46432507038116455 + 0.001 * 7.361342906951904
Epoch 240, val loss: 0.7991607189178467
Epoch 250, training loss: 0.41951891779899597 = 0.412160724401474 + 0.001 * 7.358200550079346
Epoch 250, val loss: 0.7775599956512451
Epoch 260, training loss: 0.37152552604675293 = 0.36416926980018616 + 0.001 * 7.356256008148193
Epoch 260, val loss: 0.7618293762207031
Epoch 270, training loss: 0.32756519317626953 = 0.32021141052246094 + 0.001 * 7.353781700134277
Epoch 270, val loss: 0.7503638863563538
Epoch 280, training loss: 0.287567138671875 = 0.28021544218063354 + 0.001 * 7.351682186126709
Epoch 280, val loss: 0.7420632243156433
Epoch 290, training loss: 0.25147947669029236 = 0.2441260665655136 + 0.001 * 7.353404998779297
Epoch 290, val loss: 0.7364343404769897
Epoch 300, training loss: 0.21917690336704254 = 0.21182526648044586 + 0.001 * 7.351642608642578
Epoch 300, val loss: 0.7335286736488342
Epoch 310, training loss: 0.19051405787467957 = 0.1831657439470291 + 0.001 * 7.348320484161377
Epoch 310, val loss: 0.733089804649353
Epoch 320, training loss: 0.1653275191783905 = 0.15798227488994598 + 0.001 * 7.345245838165283
Epoch 320, val loss: 0.7351270318031311
Epoch 330, training loss: 0.14343689382076263 = 0.13609358668327332 + 0.001 * 7.343308448791504
Epoch 330, val loss: 0.7395660877227783
Epoch 340, training loss: 0.12460306286811829 = 0.1172584816813469 + 0.001 * 7.344583511352539
Epoch 340, val loss: 0.7464184165000916
Epoch 350, training loss: 0.10853012651205063 = 0.10118890553712845 + 0.001 * 7.341221332550049
Epoch 350, val loss: 0.7554868459701538
Epoch 360, training loss: 0.0948886051774025 = 0.08754680305719376 + 0.001 * 7.341799736022949
Epoch 360, val loss: 0.7663888931274414
Epoch 370, training loss: 0.08334383368492126 = 0.07600541412830353 + 0.001 * 7.33841609954834
Epoch 370, val loss: 0.778814435005188
Epoch 380, training loss: 0.07359588146209717 = 0.06625989824533463 + 0.001 * 7.335978984832764
Epoch 380, val loss: 0.7924787402153015
Epoch 390, training loss: 0.06535486876964569 = 0.05802632495760918 + 0.001 * 7.328543186187744
Epoch 390, val loss: 0.807104766368866
Epoch 400, training loss: 0.05840037390589714 = 0.05106089636683464 + 0.001 * 7.339478015899658
Epoch 400, val loss: 0.822406530380249
Epoch 410, training loss: 0.05248512700200081 = 0.045159295201301575 + 0.001 * 7.325832843780518
Epoch 410, val loss: 0.8380553722381592
Epoch 420, training loss: 0.047471195459365845 = 0.04014790803194046 + 0.001 * 7.3232879638671875
Epoch 420, val loss: 0.8537951707839966
Epoch 430, training loss: 0.043190985918045044 = 0.035875845700502396 + 0.001 * 7.3151397705078125
Epoch 430, val loss: 0.8694542050361633
Epoch 440, training loss: 0.03954168036580086 = 0.03221823275089264 + 0.001 * 7.323448181152344
Epoch 440, val loss: 0.8849369287490845
Epoch 450, training loss: 0.0363905243575573 = 0.029071567580103874 + 0.001 * 7.31895637512207
Epoch 450, val loss: 0.9001647233963013
Epoch 460, training loss: 0.033667970448732376 = 0.026352128013968468 + 0.001 * 7.315842628479004
Epoch 460, val loss: 0.915069580078125
Epoch 470, training loss: 0.031292058527469635 = 0.02399088814854622 + 0.001 * 7.301170349121094
Epoch 470, val loss: 0.9295756816864014
Epoch 480, training loss: 0.02924925833940506 = 0.021930810064077377 + 0.001 * 7.318448543548584
Epoch 480, val loss: 0.9436808228492737
Epoch 490, training loss: 0.027417948469519615 = 0.02012477070093155 + 0.001 * 7.293177604675293
Epoch 490, val loss: 0.9573560953140259
Epoch 500, training loss: 0.02581646293401718 = 0.01853405497968197 + 0.001 * 7.282408237457275
Epoch 500, val loss: 0.9706190228462219
Epoch 510, training loss: 0.0244061890989542 = 0.017127251252532005 + 0.001 * 7.278937339782715
Epoch 510, val loss: 0.9834775328636169
Epoch 520, training loss: 0.023154210299253464 = 0.01587769016623497 + 0.001 * 7.276519775390625
Epoch 520, val loss: 0.9959210157394409
Epoch 530, training loss: 0.022127041593194008 = 0.014763413928449154 + 0.001 * 7.363627910614014
Epoch 530, val loss: 1.0079615116119385
Epoch 540, training loss: 0.021030787378549576 = 0.013765893876552582 + 0.001 * 7.264892101287842
Epoch 540, val loss: 1.0196102857589722
Epoch 550, training loss: 0.0201470497995615 = 0.012869715690612793 + 0.001 * 7.277333736419678
Epoch 550, val loss: 1.030869960784912
Epoch 560, training loss: 0.019316384568810463 = 0.012061746791005135 + 0.001 * 7.254637241363525
Epoch 560, val loss: 1.0417795181274414
Epoch 570, training loss: 0.01871037296950817 = 0.011330877430737019 + 0.001 * 7.379494667053223
Epoch 570, val loss: 1.0523420572280884
Epoch 580, training loss: 0.017951523885130882 = 0.010667892172932625 + 0.001 * 7.283631324768066
Epoch 580, val loss: 1.0625497102737427
Epoch 590, training loss: 0.017308469861745834 = 0.01006458792835474 + 0.001 * 7.243880748748779
Epoch 590, val loss: 1.0724366903305054
Epoch 600, training loss: 0.016761336475610733 = 0.009513916447758675 + 0.001 * 7.247419357299805
Epoch 600, val loss: 1.0820214748382568
Epoch 610, training loss: 0.016270892694592476 = 0.009009943343698978 + 0.001 * 7.26094913482666
Epoch 610, val loss: 1.0913145542144775
Epoch 620, training loss: 0.015787523239850998 = 0.008547537960112095 + 0.001 * 7.239984512329102
Epoch 620, val loss: 1.1003190279006958
Epoch 630, training loss: 0.015374294482171535 = 0.008122104220092297 + 0.001 * 7.252190113067627
Epoch 630, val loss: 1.1090582609176636
Epoch 640, training loss: 0.014996739104390144 = 0.007729226257652044 + 0.001 * 7.26751184463501
Epoch 640, val loss: 1.117553949356079
Epoch 650, training loss: 0.014607498422265053 = 0.007364969234913588 + 0.001 * 7.242529392242432
Epoch 650, val loss: 1.1258128881454468
Epoch 660, training loss: 0.014298360794782639 = 0.007025087717920542 + 0.001 * 7.2732720375061035
Epoch 660, val loss: 1.1338715553283691
Epoch 670, training loss: 0.013945466838777065 = 0.006706674117594957 + 0.001 * 7.238792419433594
Epoch 670, val loss: 1.1417274475097656
Epoch 680, training loss: 0.013652494177222252 = 0.006407288368791342 + 0.001 * 7.245204925537109
Epoch 680, val loss: 1.1494231224060059
Epoch 690, training loss: 0.013355530798435211 = 0.006125660613179207 + 0.001 * 7.229869365692139
Epoch 690, val loss: 1.1569689512252808
Epoch 700, training loss: 0.013100400567054749 = 0.0058607421815395355 + 0.001 * 7.239657878875732
Epoch 700, val loss: 1.1643732786178589
Epoch 710, training loss: 0.012817459180951118 = 0.0056116534397006035 + 0.001 * 7.205804824829102
Epoch 710, val loss: 1.1716192960739136
Epoch 720, training loss: 0.012595725245773792 = 0.005377509631216526 + 0.001 * 7.218215465545654
Epoch 720, val loss: 1.1787171363830566
Epoch 730, training loss: 0.012375405058264732 = 0.005157536827027798 + 0.001 * 7.217867851257324
Epoch 730, val loss: 1.1856588125228882
Epoch 740, training loss: 0.012210464105010033 = 0.004950842820107937 + 0.001 * 7.259620666503906
Epoch 740, val loss: 1.1924442052841187
Epoch 750, training loss: 0.01195510197430849 = 0.004756672773510218 + 0.001 * 7.198428630828857
Epoch 750, val loss: 1.1990679502487183
Epoch 760, training loss: 0.011771868914365768 = 0.004574068821966648 + 0.001 * 7.1977996826171875
Epoch 760, val loss: 1.205536127090454
Epoch 770, training loss: 0.01160719245672226 = 0.004402343649417162 + 0.001 * 7.204848766326904
Epoch 770, val loss: 1.211854338645935
Epoch 780, training loss: 0.011445438489317894 = 0.004240704234689474 + 0.001 * 7.204733371734619
Epoch 780, val loss: 1.218017578125
Epoch 790, training loss: 0.011307199485599995 = 0.004088466987013817 + 0.001 * 7.2187323570251465
Epoch 790, val loss: 1.2240440845489502
Epoch 800, training loss: 0.01113029196858406 = 0.003944989759474993 + 0.001 * 7.185302257537842
Epoch 800, val loss: 1.2299259901046753
Epoch 810, training loss: 0.011003837920725346 = 0.003809609916061163 + 0.001 * 7.194227695465088
Epoch 810, val loss: 1.2356699705123901
Epoch 820, training loss: 0.010863551869988441 = 0.0036818464286625385 + 0.001 * 7.181705474853516
Epoch 820, val loss: 1.2412800788879395
Epoch 830, training loss: 0.010744171217083931 = 0.0035611677449196577 + 0.001 * 7.1830034255981445
Epoch 830, val loss: 1.2467541694641113
Epoch 840, training loss: 0.010633044876158237 = 0.0034470679238438606 + 0.001 * 7.185976505279541
Epoch 840, val loss: 1.2520877122879028
Epoch 850, training loss: 0.010533102788031101 = 0.0033390617463737726 + 0.001 * 7.194040298461914
Epoch 850, val loss: 1.257307767868042
Epoch 860, training loss: 0.010429791174829006 = 0.0032367610838264227 + 0.001 * 7.193029403686523
Epoch 860, val loss: 1.2623956203460693
Epoch 870, training loss: 0.010319355875253677 = 0.0031397801358252764 + 0.001 * 7.179574966430664
Epoch 870, val loss: 1.2673600912094116
Epoch 880, training loss: 0.010217293165624142 = 0.0030477538239210844 + 0.001 * 7.169539451599121
Epoch 880, val loss: 1.2722188234329224
Epoch 890, training loss: 0.010118773207068443 = 0.0029603904113173485 + 0.001 * 7.158381938934326
Epoch 890, val loss: 1.2769551277160645
Epoch 900, training loss: 0.010037100873887539 = 0.002877360675483942 + 0.001 * 7.159739971160889
Epoch 900, val loss: 1.2815874814987183
Epoch 910, training loss: 0.01000392809510231 = 0.002798432018607855 + 0.001 * 7.205496311187744
Epoch 910, val loss: 1.2861199378967285
Epoch 920, training loss: 0.009887057356536388 = 0.0027232696302235126 + 0.001 * 7.163787364959717
Epoch 920, val loss: 1.2905546426773071
Epoch 930, training loss: 0.00983813963830471 = 0.0026516893412917852 + 0.001 * 7.1864495277404785
Epoch 930, val loss: 1.2948848009109497
Epoch 940, training loss: 0.00974701065570116 = 0.0025834867265075445 + 0.001 * 7.1635236740112305
Epoch 940, val loss: 1.2991269826889038
Epoch 950, training loss: 0.009674033150076866 = 0.0025184035766869783 + 0.001 * 7.155629634857178
Epoch 950, val loss: 1.3032675981521606
Epoch 960, training loss: 0.00962282158434391 = 0.00245628971606493 + 0.001 * 7.166531085968018
Epoch 960, val loss: 1.3073123693466187
Epoch 970, training loss: 0.009567567147314548 = 0.002396977273747325 + 0.001 * 7.170589447021484
Epoch 970, val loss: 1.3112688064575195
Epoch 980, training loss: 0.009488152340054512 = 0.002340257866308093 + 0.001 * 7.147894382476807
Epoch 980, val loss: 1.3151452541351318
Epoch 990, training loss: 0.009421315975487232 = 0.0022860111203044653 + 0.001 * 7.135304927825928
Epoch 990, val loss: 1.3189314603805542
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.823405376910912
=== training gcn model ===
Epoch 0, training loss: 1.9495692253112793 = 1.9409723281860352 + 0.001 * 8.596837043762207
Epoch 0, val loss: 1.9412310123443604
Epoch 10, training loss: 1.9399932622909546 = 1.931396484375 + 0.001 * 8.596810340881348
Epoch 10, val loss: 1.9320340156555176
Epoch 20, training loss: 1.928604006767273 = 1.920007348060608 + 0.001 * 8.596694946289062
Epoch 20, val loss: 1.9207208156585693
Epoch 30, training loss: 1.9129602909088135 = 1.9043638706207275 + 0.001 * 8.596449851989746
Epoch 30, val loss: 1.904733657836914
Epoch 40, training loss: 1.8901151418685913 = 1.8815191984176636 + 0.001 * 8.595916748046875
Epoch 40, val loss: 1.8810985088348389
Epoch 50, training loss: 1.8581258058547974 = 1.8495311737060547 + 0.001 * 8.594609260559082
Epoch 50, val loss: 1.8487510681152344
Epoch 60, training loss: 1.82096266746521 = 1.812372088432312 + 0.001 * 8.59052848815918
Epoch 60, val loss: 1.8142379522323608
Epoch 70, training loss: 1.7879812717437744 = 1.7794073820114136 + 0.001 * 8.573905944824219
Epoch 70, val loss: 1.786728024482727
Epoch 80, training loss: 1.749497413635254 = 1.7410166263580322 + 0.001 * 8.480835914611816
Epoch 80, val loss: 1.7528705596923828
Epoch 90, training loss: 1.695541262626648 = 1.687412977218628 + 0.001 * 8.128332138061523
Epoch 90, val loss: 1.7054659128189087
Epoch 100, training loss: 1.6216834783554077 = 1.6137393712997437 + 0.001 * 7.944149494171143
Epoch 100, val loss: 1.6428625583648682
Epoch 110, training loss: 1.5276509523391724 = 1.51987624168396 + 0.001 * 7.774743556976318
Epoch 110, val loss: 1.564440369606018
Epoch 120, training loss: 1.4204285144805908 = 1.4127036333084106 + 0.001 * 7.724938869476318
Epoch 120, val loss: 1.4767017364501953
Epoch 130, training loss: 1.3076798915863037 = 1.2999727725982666 + 0.001 * 7.707119941711426
Epoch 130, val loss: 1.3865941762924194
Epoch 140, training loss: 1.196211576461792 = 1.1885218620300293 + 0.001 * 7.689699649810791
Epoch 140, val loss: 1.3006494045257568
Epoch 150, training loss: 1.091355800628662 = 1.0836905241012573 + 0.001 * 7.665222644805908
Epoch 150, val loss: 1.2227067947387695
Epoch 160, training loss: 0.9959729909896851 = 0.9883443117141724 + 0.001 * 7.628654479980469
Epoch 160, val loss: 1.1541705131530762
Epoch 170, training loss: 0.9101837873458862 = 0.9025938510894775 + 0.001 * 7.589925289154053
Epoch 170, val loss: 1.09388267993927
Epoch 180, training loss: 0.8329353332519531 = 0.82536381483078 + 0.001 * 7.571504592895508
Epoch 180, val loss: 1.0407744646072388
Epoch 190, training loss: 0.763303816318512 = 0.7557435035705566 + 0.001 * 7.5603132247924805
Epoch 190, val loss: 0.9941398501396179
Epoch 200, training loss: 0.7002394795417786 = 0.6926872134208679 + 0.001 * 7.552281856536865
Epoch 200, val loss: 0.9540817737579346
Epoch 210, training loss: 0.6420773267745972 = 0.6345317959785461 + 0.001 * 7.5455217361450195
Epoch 210, val loss: 0.9199837446212769
Epoch 220, training loss: 0.5869988203048706 = 0.5794606804847717 + 0.001 * 7.538130283355713
Epoch 220, val loss: 0.8915181159973145
Epoch 230, training loss: 0.5337774753570557 = 0.5262473225593567 + 0.001 * 7.530178070068359
Epoch 230, val loss: 0.8685497045516968
Epoch 240, training loss: 0.4822539985179901 = 0.4747335910797119 + 0.001 * 7.520396709442139
Epoch 240, val loss: 0.8513858914375305
Epoch 250, training loss: 0.4329555630683899 = 0.4254448413848877 + 0.001 * 7.510709762573242
Epoch 250, val loss: 0.8401232957839966
Epoch 260, training loss: 0.3865591585636139 = 0.37906596064567566 + 0.001 * 7.493194103240967
Epoch 260, val loss: 0.8348110914230347
Epoch 270, training loss: 0.3436388373374939 = 0.3361613154411316 + 0.001 * 7.477518558502197
Epoch 270, val loss: 0.8353232145309448
Epoch 280, training loss: 0.30450528860092163 = 0.2970372438430786 + 0.001 * 7.468031406402588
Epoch 280, val loss: 0.8416992425918579
Epoch 290, training loss: 0.2693120241165161 = 0.26186075806617737 + 0.001 * 7.451265335083008
Epoch 290, val loss: 0.8532230854034424
Epoch 300, training loss: 0.23805561661720276 = 0.23061519861221313 + 0.001 * 7.440423965454102
Epoch 300, val loss: 0.8692896962165833
Epoch 310, training loss: 0.21054871380329132 = 0.20311971008777618 + 0.001 * 7.4290056228637695
Epoch 310, val loss: 0.8893102407455444
Epoch 320, training loss: 0.18648986518383026 = 0.17907963693141937 + 0.001 * 7.4102349281311035
Epoch 320, val loss: 0.9126006364822388
Epoch 330, training loss: 0.16553892195224762 = 0.15813994407653809 + 0.001 * 7.398974895477295
Epoch 330, val loss: 0.9384037852287292
Epoch 340, training loss: 0.14733944833278656 = 0.1399383395910263 + 0.001 * 7.401105880737305
Epoch 340, val loss: 0.9660199284553528
Epoch 350, training loss: 0.13151982426643372 = 0.1241292655467987 + 0.001 * 7.390560150146484
Epoch 350, val loss: 0.9948580265045166
Epoch 360, training loss: 0.1177656427025795 = 0.11039455980062485 + 0.001 * 7.37108039855957
Epoch 360, val loss: 1.024389624595642
Epoch 370, training loss: 0.10580731928348541 = 0.09844668954610825 + 0.001 * 7.36063289642334
Epoch 370, val loss: 1.0541582107543945
Epoch 380, training loss: 0.0953843742609024 = 0.08802726864814758 + 0.001 * 7.357107162475586
Epoch 380, val loss: 1.0838695764541626
Epoch 390, training loss: 0.08626815676689148 = 0.07891436666250229 + 0.001 * 7.353789329528809
Epoch 390, val loss: 1.1132913827896118
Epoch 400, training loss: 0.07826696336269379 = 0.07091443240642548 + 0.001 * 7.352531909942627
Epoch 400, val loss: 1.142410397529602
Epoch 410, training loss: 0.07121233642101288 = 0.06386370956897736 + 0.001 * 7.348627090454102
Epoch 410, val loss: 1.1710764169692993
Epoch 420, training loss: 0.06497494876384735 = 0.05763018876314163 + 0.001 * 7.344758033752441
Epoch 420, val loss: 1.1992791891098022
Epoch 430, training loss: 0.05944518744945526 = 0.052104685455560684 + 0.001 * 7.3404998779296875
Epoch 430, val loss: 1.2270406484603882
Epoch 440, training loss: 0.05453095585107803 = 0.04719528928399086 + 0.001 * 7.335666179656982
Epoch 440, val loss: 1.2542674541473389
Epoch 450, training loss: 0.050160933285951614 = 0.042826347053050995 + 0.001 * 7.3345866203308105
Epoch 450, val loss: 1.2809865474700928
Epoch 460, training loss: 0.046267516911029816 = 0.03893478214740753 + 0.001 * 7.332735061645508
Epoch 460, val loss: 1.30717933177948
Epoch 470, training loss: 0.0427979975938797 = 0.0354672409594059 + 0.001 * 7.3307576179504395
Epoch 470, val loss: 1.3328527212142944
Epoch 480, training loss: 0.03970760852098465 = 0.03237709775567055 + 0.001 * 7.330511569976807
Epoch 480, val loss: 1.3579964637756348
Epoch 490, training loss: 0.03695333003997803 = 0.02962256595492363 + 0.001 * 7.330763339996338
Epoch 490, val loss: 1.3825486898422241
Epoch 500, training loss: 0.03448989614844322 = 0.027165954932570457 + 0.001 * 7.323940277099609
Epoch 500, val loss: 1.4064948558807373
Epoch 510, training loss: 0.03229750320315361 = 0.024973861873149872 + 0.001 * 7.3236403465271
Epoch 510, val loss: 1.429750680923462
Epoch 520, training loss: 0.030333546921610832 = 0.023015882819890976 + 0.001 * 7.31766414642334
Epoch 520, val loss: 1.452502965927124
Epoch 530, training loss: 0.028581885620951653 = 0.02126246504485607 + 0.001 * 7.319420337677002
Epoch 530, val loss: 1.4745478630065918
Epoch 540, training loss: 0.027004273608326912 = 0.019686490297317505 + 0.001 * 7.317782402038574
Epoch 540, val loss: 1.4959701299667358
Epoch 550, training loss: 0.025576692074537277 = 0.018264463171362877 + 0.001 * 7.312228679656982
Epoch 550, val loss: 1.5168051719665527
Epoch 560, training loss: 0.02428758144378662 = 0.016975944861769676 + 0.001 * 7.311635971069336
Epoch 560, val loss: 1.537039875984192
Epoch 570, training loss: 0.023112760856747627 = 0.015804968774318695 + 0.001 * 7.3077921867370605
Epoch 570, val loss: 1.5566308498382568
Epoch 580, training loss: 0.022043099626898766 = 0.014738518744707108 + 0.001 * 7.304581165313721
Epoch 580, val loss: 1.5756779909133911
Epoch 590, training loss: 0.02107435278594494 = 0.013767136260867119 + 0.001 * 7.307216167449951
Epoch 590, val loss: 1.5940338373184204
Epoch 600, training loss: 0.02017976902425289 = 0.012882255017757416 + 0.001 * 7.297513961791992
Epoch 600, val loss: 1.6118786334991455
Epoch 610, training loss: 0.01937350630760193 = 0.012074456550180912 + 0.001 * 7.299050331115723
Epoch 610, val loss: 1.6290614604949951
Epoch 620, training loss: 0.01863066852092743 = 0.01133674569427967 + 0.001 * 7.293923377990723
Epoch 620, val loss: 1.645669937133789
Epoch 630, training loss: 0.01795874908566475 = 0.01066241879016161 + 0.001 * 7.29633092880249
Epoch 630, val loss: 1.6616687774658203
Epoch 640, training loss: 0.017335254698991776 = 0.01004503108561039 + 0.001 * 7.29022216796875
Epoch 640, val loss: 1.6771317720413208
Epoch 650, training loss: 0.016777358949184418 = 0.009478767402470112 + 0.001 * 7.2985920906066895
Epoch 650, val loss: 1.6919962167739868
Epoch 660, training loss: 0.016257185488939285 = 0.00895867682993412 + 0.001 * 7.298507213592529
Epoch 660, val loss: 1.7063541412353516
Epoch 670, training loss: 0.01576176844537258 = 0.008480574935674667 + 0.001 * 7.281193733215332
Epoch 670, val loss: 1.720226764678955
Epoch 680, training loss: 0.015324318781495094 = 0.008039942942559719 + 0.001 * 7.2843756675720215
Epoch 680, val loss: 1.7335928678512573
Epoch 690, training loss: 0.014925366267561913 = 0.007633245550096035 + 0.001 * 7.292120456695557
Epoch 690, val loss: 1.7464871406555176
Epoch 700, training loss: 0.014537673443555832 = 0.007257662247866392 + 0.001 * 7.28001070022583
Epoch 700, val loss: 1.758915901184082
Epoch 710, training loss: 0.01418144442141056 = 0.006909900810569525 + 0.001 * 7.271543502807617
Epoch 710, val loss: 1.770891785621643
Epoch 720, training loss: 0.013857896439731121 = 0.0065873307175934315 + 0.001 * 7.270565509796143
Epoch 720, val loss: 1.7824499607086182
Epoch 730, training loss: 0.013558807782828808 = 0.00628785090520978 + 0.001 * 7.270956516265869
Epoch 730, val loss: 1.7937028408050537
Epoch 740, training loss: 0.013278940692543983 = 0.00600937707349658 + 0.001 * 7.269563674926758
Epoch 740, val loss: 1.8044508695602417
Epoch 750, training loss: 0.013008566573262215 = 0.005749986041337252 + 0.001 * 7.258580684661865
Epoch 750, val loss: 1.8148925304412842
Epoch 760, training loss: 0.01276314351707697 = 0.005507966037839651 + 0.001 * 7.255177021026611
Epoch 760, val loss: 1.8249382972717285
Epoch 770, training loss: 0.01258141826838255 = 0.00528178783133626 + 0.001 * 7.299630165100098
Epoch 770, val loss: 1.8346705436706543
Epoch 780, training loss: 0.012333560734987259 = 0.0050704763270914555 + 0.001 * 7.263084411621094
Epoch 780, val loss: 1.844043254852295
Epoch 790, training loss: 0.01212712936103344 = 0.004872729070484638 + 0.001 * 7.254400253295898
Epoch 790, val loss: 1.85311758518219
Epoch 800, training loss: 0.01193655002862215 = 0.004687224980443716 + 0.001 * 7.249324798583984
Epoch 800, val loss: 1.8619053363800049
Epoch 810, training loss: 0.011754235252737999 = 0.004512988030910492 + 0.001 * 7.241246700286865
Epoch 810, val loss: 1.8703932762145996
Epoch 820, training loss: 0.011589468456804752 = 0.004349135793745518 + 0.001 * 7.240332126617432
Epoch 820, val loss: 1.8786197900772095
Epoch 830, training loss: 0.011431609280407429 = 0.004194894805550575 + 0.001 * 7.2367143630981445
Epoch 830, val loss: 1.88656485080719
Epoch 840, training loss: 0.011299505829811096 = 0.004049563780426979 + 0.001 * 7.249941349029541
Epoch 840, val loss: 1.8942656517028809
Epoch 850, training loss: 0.011176817119121552 = 0.003912479616701603 + 0.001 * 7.264336585998535
Epoch 850, val loss: 1.9016895294189453
Epoch 860, training loss: 0.011022931896150112 = 0.0037831461522728205 + 0.001 * 7.239785671234131
Epoch 860, val loss: 1.9088948965072632
Epoch 870, training loss: 0.010891770012676716 = 0.0036609440576285124 + 0.001 * 7.230825424194336
Epoch 870, val loss: 1.9159129858016968
Epoch 880, training loss: 0.010788215324282646 = 0.003545243525877595 + 0.001 * 7.242971897125244
Epoch 880, val loss: 1.92267906665802
Epoch 890, training loss: 0.010662342421710491 = 0.0034357395488768816 + 0.001 * 7.226602077484131
Epoch 890, val loss: 1.929240345954895
Epoch 900, training loss: 0.010568185709416866 = 0.0033319368958473206 + 0.001 * 7.23624849319458
Epoch 900, val loss: 1.9356305599212646
Epoch 910, training loss: 0.010460942052304745 = 0.003233427880331874 + 0.001 * 7.227514266967773
Epoch 910, val loss: 1.941778540611267
Epoch 920, training loss: 0.010386666283011436 = 0.0031398788560181856 + 0.001 * 7.2467875480651855
Epoch 920, val loss: 1.9477604627609253
Epoch 930, training loss: 0.01026768796145916 = 0.0030510141514241695 + 0.001 * 7.216673374176025
Epoch 930, val loss: 1.9535845518112183
Epoch 940, training loss: 0.01017563883215189 = 0.0029664591420441866 + 0.001 * 7.209178924560547
Epoch 940, val loss: 1.9592047929763794
Epoch 950, training loss: 0.010089649818837643 = 0.0028859407175332308 + 0.001 * 7.203709125518799
Epoch 950, val loss: 1.9646538496017456
Epoch 960, training loss: 0.010007930919528008 = 0.0028092137072235346 + 0.001 * 7.198716640472412
Epoch 960, val loss: 1.9699368476867676
Epoch 970, training loss: 0.0099584786221385 = 0.0027360895182937384 + 0.001 * 7.222389221191406
Epoch 970, val loss: 1.9750852584838867
Epoch 980, training loss: 0.009917736053466797 = 0.0026663094758987427 + 0.001 * 7.251426696777344
Epoch 980, val loss: 1.9799827337265015
Epoch 990, training loss: 0.0097970487549901 = 0.0025998286437243223 + 0.001 * 7.197219371795654
Epoch 990, val loss: 1.9848304986953735
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8255139694254086
=== training gcn model ===
Epoch 0, training loss: 1.9515258073806763 = 1.9429289102554321 + 0.001 * 8.5968656539917
Epoch 0, val loss: 1.9400558471679688
Epoch 10, training loss: 1.9410598278045654 = 1.9324630498886108 + 0.001 * 8.596822738647461
Epoch 10, val loss: 1.9295681715011597
Epoch 20, training loss: 1.928357481956482 = 1.919760823249817 + 0.001 * 8.596701622009277
Epoch 20, val loss: 1.9167437553405762
Epoch 30, training loss: 1.9106987714767456 = 1.9021023511886597 + 0.001 * 8.596428871154785
Epoch 30, val loss: 1.8991622924804688
Epoch 40, training loss: 1.885047197341919 = 1.8764514923095703 + 0.001 * 8.595760345458984
Epoch 40, val loss: 1.8744959831237793
Epoch 50, training loss: 1.850604772567749 = 1.8420108556747437 + 0.001 * 8.593873977661133
Epoch 50, val loss: 1.8437178134918213
Epoch 60, training loss: 1.8153021335601807 = 1.8067148923873901 + 0.001 * 8.587191581726074
Epoch 60, val loss: 1.8164490461349487
Epoch 70, training loss: 1.7866491079330444 = 1.7780938148498535 + 0.001 * 8.555351257324219
Epoch 70, val loss: 1.7933961153030396
Epoch 80, training loss: 1.747945785522461 = 1.7396373748779297 + 0.001 * 8.308414459228516
Epoch 80, val loss: 1.7562144994735718
Epoch 90, training loss: 1.6931912899017334 = 1.68501877784729 + 0.001 * 8.17245864868164
Epoch 90, val loss: 1.7072186470031738
Epoch 100, training loss: 1.6195919513702393 = 1.6114912033081055 + 0.001 * 8.100702285766602
Epoch 100, val loss: 1.646191120147705
Epoch 110, training loss: 1.5371558666229248 = 1.5291181802749634 + 0.001 * 8.03766918182373
Epoch 110, val loss: 1.5787984132766724
Epoch 120, training loss: 1.4568419456481934 = 1.4489692449569702 + 0.001 * 7.872755527496338
Epoch 120, val loss: 1.5138366222381592
Epoch 130, training loss: 1.380901575088501 = 1.373140811920166 + 0.001 * 7.760821342468262
Epoch 130, val loss: 1.4530771970748901
Epoch 140, training loss: 1.3059660196304321 = 1.298232913017273 + 0.001 * 7.733047008514404
Epoch 140, val loss: 1.3943084478378296
Epoch 150, training loss: 1.2292863130569458 = 1.2215696573257446 + 0.001 * 7.716704368591309
Epoch 150, val loss: 1.3352192640304565
Epoch 160, training loss: 1.1489801406860352 = 1.141284704208374 + 0.001 * 7.695408344268799
Epoch 160, val loss: 1.274767279624939
Epoch 170, training loss: 1.0628546476364136 = 1.0551862716674805 + 0.001 * 7.66840124130249
Epoch 170, val loss: 1.2107954025268555
Epoch 180, training loss: 0.9706718921661377 = 0.9630452990531921 + 0.001 * 7.626586437225342
Epoch 180, val loss: 1.143060326576233
Epoch 190, training loss: 0.8764197826385498 = 0.8688581585884094 + 0.001 * 7.561596870422363
Epoch 190, val loss: 1.0753962993621826
Epoch 200, training loss: 0.7871400713920593 = 0.7796046733856201 + 0.001 * 7.535407066345215
Epoch 200, val loss: 1.0147563219070435
Epoch 210, training loss: 0.7078779935836792 = 0.700352668762207 + 0.001 * 7.52534294128418
Epoch 210, val loss: 0.9663740396499634
Epoch 220, training loss: 0.6395866870880127 = 0.6320650577545166 + 0.001 * 7.521653175354004
Epoch 220, val loss: 0.931551992893219
Epoch 230, training loss: 0.580826461315155 = 0.5733093023300171 + 0.001 * 7.51715087890625
Epoch 230, val loss: 0.9085581302642822
Epoch 240, training loss: 0.5296973586082458 = 0.5221871137619019 + 0.001 * 7.510270118713379
Epoch 240, val loss: 0.8948042392730713
Epoch 250, training loss: 0.48440852761268616 = 0.4769091308116913 + 0.001 * 7.499406814575195
Epoch 250, val loss: 0.8875252604484558
Epoch 260, training loss: 0.4435257911682129 = 0.4360406696796417 + 0.001 * 7.485134124755859
Epoch 260, val loss: 0.8848091959953308
Epoch 270, training loss: 0.40615424513816833 = 0.39869120717048645 + 0.001 * 7.463028907775879
Epoch 270, val loss: 0.8857666850090027
Epoch 280, training loss: 0.371853768825531 = 0.3644029200077057 + 0.001 * 7.450855731964111
Epoch 280, val loss: 0.8900569081306458
Epoch 290, training loss: 0.3403380811214447 = 0.3329128623008728 + 0.001 * 7.425217151641846
Epoch 290, val loss: 0.8975096940994263
Epoch 300, training loss: 0.31134024262428284 = 0.30392324924468994 + 0.001 * 7.416995048522949
Epoch 300, val loss: 0.9078742861747742
Epoch 310, training loss: 0.28442588448524475 = 0.27701428532600403 + 0.001 * 7.411604404449463
Epoch 310, val loss: 0.9207114577293396
Epoch 320, training loss: 0.2591882646083832 = 0.2517772614955902 + 0.001 * 7.4110107421875
Epoch 320, val loss: 0.9360744953155518
Epoch 330, training loss: 0.23529145121574402 = 0.22790099680423737 + 0.001 * 7.39045524597168
Epoch 330, val loss: 0.9544926285743713
Epoch 340, training loss: 0.21263369917869568 = 0.20524580776691437 + 0.001 * 7.387892246246338
Epoch 340, val loss: 0.9764612317085266
Epoch 350, training loss: 0.19128650426864624 = 0.18389564752578735 + 0.001 * 7.3908586502075195
Epoch 350, val loss: 1.0022445917129517
Epoch 360, training loss: 0.17149949073791504 = 0.16411976516246796 + 0.001 * 7.379722595214844
Epoch 360, val loss: 1.0319732427597046
Epoch 370, training loss: 0.15354880690574646 = 0.14617595076560974 + 0.001 * 7.37285041809082
Epoch 370, val loss: 1.0654213428497314
Epoch 380, training loss: 0.13757413625717163 = 0.13019943237304688 + 0.001 * 7.374707221984863
Epoch 380, val loss: 1.1021522283554077
Epoch 390, training loss: 0.12349217385053635 = 0.11612904071807861 + 0.001 * 7.363131046295166
Epoch 390, val loss: 1.1412123441696167
Epoch 400, training loss: 0.11110791563987732 = 0.10373634099960327 + 0.001 * 7.371576309204102
Epoch 400, val loss: 1.1816117763519287
Epoch 410, training loss: 0.1001390665769577 = 0.09277218580245972 + 0.001 * 7.36687707901001
Epoch 410, val loss: 1.222621202468872
Epoch 420, training loss: 0.09042828530073166 = 0.08305134624242783 + 0.001 * 7.376938343048096
Epoch 420, val loss: 1.2635425329208374
Epoch 430, training loss: 0.08177407830953598 = 0.07442620396614075 + 0.001 * 7.347870826721191
Epoch 430, val loss: 1.3040039539337158
Epoch 440, training loss: 0.07412244379520416 = 0.066775843501091 + 0.001 * 7.346601486206055
Epoch 440, val loss: 1.3437494039535522
Epoch 450, training loss: 0.0673416256904602 = 0.05999552831053734 + 0.001 * 7.346093654632568
Epoch 450, val loss: 1.3825010061264038
Epoch 460, training loss: 0.061354056000709534 = 0.05399424582719803 + 0.001 * 7.359811782836914
Epoch 460, val loss: 1.420058250427246
Epoch 470, training loss: 0.056032877415418625 = 0.04868560656905174 + 0.001 * 7.34727144241333
Epoch 470, val loss: 1.4562392234802246
Epoch 480, training loss: 0.05133097991347313 = 0.04398889094591141 + 0.001 * 7.342090129852295
Epoch 480, val loss: 1.4910298585891724
Epoch 490, training loss: 0.047166481614112854 = 0.03982903063297272 + 0.001 * 7.3374505043029785
Epoch 490, val loss: 1.5243780612945557
Epoch 500, training loss: 0.04349310323596001 = 0.036139506846666336 + 0.001 * 7.353597164154053
Epoch 500, val loss: 1.5563002824783325
Epoch 510, training loss: 0.04019048064947128 = 0.03286192566156387 + 0.001 * 7.328554630279541
Epoch 510, val loss: 1.586883783340454
Epoch 520, training loss: 0.03727582469582558 = 0.029945725575089455 + 0.001 * 7.330099582672119
Epoch 520, val loss: 1.616240382194519
Epoch 530, training loss: 0.03469228371977806 = 0.02734912745654583 + 0.001 * 7.343156337738037
Epoch 530, val loss: 1.6444718837738037
Epoch 540, training loss: 0.03236798942089081 = 0.025035375729203224 + 0.001 * 7.332613945007324
Epoch 540, val loss: 1.6717089414596558
Epoch 550, training loss: 0.030303623527288437 = 0.022972039878368378 + 0.001 * 7.331584453582764
Epoch 550, val loss: 1.6980196237564087
Epoch 560, training loss: 0.028442449867725372 = 0.021130040287971497 + 0.001 * 7.312408447265625
Epoch 560, val loss: 1.7235032320022583
Epoch 570, training loss: 0.02681417390704155 = 0.019482344388961792 + 0.001 * 7.331829071044922
Epoch 570, val loss: 1.7482235431671143
Epoch 580, training loss: 0.025331709533929825 = 0.0180074293166399 + 0.001 * 7.324280738830566
Epoch 580, val loss: 1.772222638130188
Epoch 590, training loss: 0.023998670279979706 = 0.016684096306562424 + 0.001 * 7.314573287963867
Epoch 590, val loss: 1.7955201864242554
Epoch 600, training loss: 0.02280133217573166 = 0.01549521367996931 + 0.001 * 7.306118488311768
Epoch 600, val loss: 1.818137764930725
Epoch 610, training loss: 0.021737225353717804 = 0.014425345696508884 + 0.001 * 7.311878204345703
Epoch 610, val loss: 1.840055227279663
Epoch 620, training loss: 0.020756280049681664 = 0.01346068549901247 + 0.001 * 7.295594692230225
Epoch 620, val loss: 1.8613582849502563
Epoch 630, training loss: 0.01988784596323967 = 0.01258891448378563 + 0.001 * 7.29893159866333
Epoch 630, val loss: 1.8820345401763916
Epoch 640, training loss: 0.019105304032564163 = 0.01179951336234808 + 0.001 * 7.305789470672607
Epoch 640, val loss: 1.902055025100708
Epoch 650, training loss: 0.0183719489723444 = 0.011083311401307583 + 0.001 * 7.288636684417725
Epoch 650, val loss: 1.9214601516723633
Epoch 660, training loss: 0.017722029238939285 = 0.010432025417685509 + 0.001 * 7.290004253387451
Epoch 660, val loss: 1.9402673244476318
Epoch 670, training loss: 0.01713326759636402 = 0.00983838364481926 + 0.001 * 7.294884204864502
Epoch 670, val loss: 1.9584863185882568
Epoch 680, training loss: 0.01659240573644638 = 0.009296114556491375 + 0.001 * 7.296289920806885
Epoch 680, val loss: 1.9761710166931152
Epoch 690, training loss: 0.016081662848591805 = 0.008799522183835506 + 0.001 * 7.282139778137207
Epoch 690, val loss: 1.9932879209518433
Epoch 700, training loss: 0.015637800097465515 = 0.008343707770109177 + 0.001 * 7.294093132019043
Epoch 700, val loss: 2.009922981262207
Epoch 710, training loss: 0.015217228792607784 = 0.007924471981823444 + 0.001 * 7.2927565574646
Epoch 710, val loss: 2.0260682106018066
Epoch 720, training loss: 0.014818023890256882 = 0.007537966128438711 + 0.001 * 7.280057430267334
Epoch 720, val loss: 2.0417017936706543
Epoch 730, training loss: 0.014454610645771027 = 0.007180951070040464 + 0.001 * 7.273659706115723
Epoch 730, val loss: 2.056884765625
Epoch 740, training loss: 0.01415901631116867 = 0.006850570440292358 + 0.001 * 7.308445930480957
Epoch 740, val loss: 2.0715763568878174
Epoch 750, training loss: 0.013827817514538765 = 0.0065444279462099075 + 0.001 * 7.283389091491699
Epoch 750, val loss: 2.0859076976776123
Epoch 760, training loss: 0.013516208156943321 = 0.006260087247937918 + 0.001 * 7.2561211585998535
Epoch 760, val loss: 2.099797248840332
Epoch 770, training loss: 0.013243024237453938 = 0.0059954444877803326 + 0.001 * 7.247579574584961
Epoch 770, val loss: 2.113300085067749
Epoch 780, training loss: 0.013032490387558937 = 0.00574874971061945 + 0.001 * 7.283740997314453
Epoch 780, val loss: 2.126434803009033
Epoch 790, training loss: 0.012804669328033924 = 0.005518416408449411 + 0.001 * 7.286252498626709
Epoch 790, val loss: 2.1392149925231934
Epoch 800, training loss: 0.01255815476179123 = 0.005303081590682268 + 0.001 * 7.255073070526123
Epoch 800, val loss: 2.151649236679077
Epoch 810, training loss: 0.012343370355665684 = 0.005101488437503576 + 0.001 * 7.241881370544434
Epoch 810, val loss: 2.1637494564056396
Epoch 820, training loss: 0.012154057621955872 = 0.0049125119112432 + 0.001 * 7.241545677185059
Epoch 820, val loss: 2.1755313873291016
Epoch 830, training loss: 0.011967883445322514 = 0.00473508657887578 + 0.001 * 7.232796669006348
Epoch 830, val loss: 2.186999797821045
Epoch 840, training loss: 0.011820834130048752 = 0.004568286705762148 + 0.001 * 7.252546787261963
Epoch 840, val loss: 2.1981825828552246
Epoch 850, training loss: 0.011645004153251648 = 0.004411275498569012 + 0.001 * 7.233727931976318
Epoch 850, val loss: 2.209012746810913
Epoch 860, training loss: 0.011514580808579922 = 0.004263309296220541 + 0.001 * 7.2512712478637695
Epoch 860, val loss: 2.2196478843688965
Epoch 870, training loss: 0.011361200362443924 = 0.004123680293560028 + 0.001 * 7.237519264221191
Epoch 870, val loss: 2.2299797534942627
Epoch 880, training loss: 0.011229408904910088 = 0.0039917281828820705 + 0.001 * 7.237680912017822
Epoch 880, val loss: 2.2401208877563477
Epoch 890, training loss: 0.01107818353921175 = 0.0038667412009090185 + 0.001 * 7.211441993713379
Epoch 890, val loss: 2.2499773502349854
Epoch 900, training loss: 0.010968352667987347 = 0.0037482031621038914 + 0.001 * 7.220149040222168
Epoch 900, val loss: 2.2596614360809326
Epoch 910, training loss: 0.010858324356377125 = 0.0036355112679302692 + 0.001 * 7.222812652587891
Epoch 910, val loss: 2.2691423892974854
Epoch 920, training loss: 0.010739685036242008 = 0.0035281546879559755 + 0.001 * 7.2115302085876465
Epoch 920, val loss: 2.2784671783447266
Epoch 930, training loss: 0.010622060857713223 = 0.0034255983773618937 + 0.001 * 7.196462154388428
Epoch 930, val loss: 2.2876651287078857
Epoch 940, training loss: 0.010515017434954643 = 0.0033273808658123016 + 0.001 * 7.187636375427246
Epoch 940, val loss: 2.296781301498413
Epoch 950, training loss: 0.010441936552524567 = 0.0032331726979464293 + 0.001 * 7.208763599395752
Epoch 950, val loss: 2.3058419227600098
Epoch 960, training loss: 0.010336748324334621 = 0.00314271985553205 + 0.001 * 7.194028377532959
Epoch 960, val loss: 2.314857244491577
Epoch 970, training loss: 0.010256254114210606 = 0.003055704990401864 + 0.001 * 7.200549125671387
Epoch 970, val loss: 2.3238260746002197
Epoch 980, training loss: 0.01016515027731657 = 0.0029720657039433718 + 0.001 * 7.193084239959717
Epoch 980, val loss: 2.3327534198760986
Epoch 990, training loss: 0.010106408968567848 = 0.0028915791772305965 + 0.001 * 7.214828968048096
Epoch 990, val loss: 2.341670274734497
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8202424881391671
The final CL Acc:0.77284, 0.03544, The final GNN Acc:0.82305, 0.00217
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13148])
remove edge: torch.Size([2, 7932])
updated graph: torch.Size([2, 10524])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9448997974395752 = 1.9363030195236206 + 0.001 * 8.596837043762207
Epoch 0, val loss: 1.9271310567855835
Epoch 10, training loss: 1.9342548847198486 = 1.925658106803894 + 0.001 * 8.596792221069336
Epoch 10, val loss: 1.9166176319122314
Epoch 20, training loss: 1.921257495880127 = 1.912660837173462 + 0.001 * 8.596634864807129
Epoch 20, val loss: 1.9033596515655518
Epoch 30, training loss: 1.9031767845153809 = 1.8945804834365845 + 0.001 * 8.596269607543945
Epoch 30, val loss: 1.884610652923584
Epoch 40, training loss: 1.8769398927688599 = 1.8683445453643799 + 0.001 * 8.595352172851562
Epoch 40, val loss: 1.8575471639633179
Epoch 50, training loss: 1.8418726921081543 = 1.8332799673080444 + 0.001 * 8.592768669128418
Epoch 50, val loss: 1.8233855962753296
Epoch 60, training loss: 1.8049284219741821 = 1.7963443994522095 + 0.001 * 8.584030151367188
Epoch 60, val loss: 1.7926688194274902
Epoch 70, training loss: 1.7695754766464233 = 1.761029839515686 + 0.001 * 8.545595169067383
Epoch 70, val loss: 1.76663339138031
Epoch 80, training loss: 1.721096396446228 = 1.7128208875656128 + 0.001 * 8.275504112243652
Epoch 80, val loss: 1.726083517074585
Epoch 90, training loss: 1.653681755065918 = 1.6456353664398193 + 0.001 * 8.046334266662598
Epoch 90, val loss: 1.6660908460617065
Epoch 100, training loss: 1.5673389434814453 = 1.5593796968460083 + 0.001 * 7.959193229675293
Epoch 100, val loss: 1.5917646884918213
Epoch 110, training loss: 1.4734435081481934 = 1.4656134843826294 + 0.001 * 7.829972743988037
Epoch 110, val loss: 1.5152708292007446
Epoch 120, training loss: 1.3795024156570435 = 1.371819019317627 + 0.001 * 7.683391094207764
Epoch 120, val loss: 1.4419376850128174
Epoch 130, training loss: 1.2842463254928589 = 1.276584267616272 + 0.001 * 7.662019729614258
Epoch 130, val loss: 1.3698959350585938
Epoch 140, training loss: 1.185476303100586 = 1.1778315305709839 + 0.001 * 7.644750118255615
Epoch 140, val loss: 1.2955158948898315
Epoch 150, training loss: 1.0840253829956055 = 1.0763976573944092 + 0.001 * 7.6277384757995605
Epoch 150, val loss: 1.219275951385498
Epoch 160, training loss: 0.9835290908813477 = 0.9759119749069214 + 0.001 * 7.617145538330078
Epoch 160, val loss: 1.1446830034255981
Epoch 170, training loss: 0.8887710571289062 = 0.8811654448509216 + 0.001 * 7.605614185333252
Epoch 170, val loss: 1.0758861303329468
Epoch 180, training loss: 0.8036959171295166 = 0.7961042523384094 + 0.001 * 7.591643810272217
Epoch 180, val loss: 1.0159331560134888
Epoch 190, training loss: 0.7299096584320068 = 0.7223333716392517 + 0.001 * 7.576281547546387
Epoch 190, val loss: 0.96619713306427
Epoch 200, training loss: 0.6662375926971436 = 0.6586739420890808 + 0.001 * 7.563652515411377
Epoch 200, val loss: 0.9261394143104553
Epoch 210, training loss: 0.6097832322120667 = 0.6022273898124695 + 0.001 * 7.555866718292236
Epoch 210, val loss: 0.8937531113624573
Epoch 220, training loss: 0.558063805103302 = 0.5505177974700928 + 0.001 * 7.54603385925293
Epoch 220, val loss: 0.8674831390380859
Epoch 230, training loss: 0.509747326374054 = 0.5022203326225281 + 0.001 * 7.5270161628723145
Epoch 230, val loss: 0.8467404246330261
Epoch 240, training loss: 0.46438276767730713 = 0.456892192363739 + 0.001 * 7.490562915802002
Epoch 240, val loss: 0.8309769034385681
Epoch 250, training loss: 0.42193081974983215 = 0.41446781158447266 + 0.001 * 7.4629950523376465
Epoch 250, val loss: 0.8200029730796814
Epoch 260, training loss: 0.38235601782798767 = 0.37492725253105164 + 0.001 * 7.428755283355713
Epoch 260, val loss: 0.8138682842254639
Epoch 270, training loss: 0.345492422580719 = 0.3380907475948334 + 0.001 * 7.401682376861572
Epoch 270, val loss: 0.8123098015785217
Epoch 280, training loss: 0.3110707104206085 = 0.30369117856025696 + 0.001 * 7.379528522491455
Epoch 280, val loss: 0.8145281076431274
Epoch 290, training loss: 0.27889779210090637 = 0.27154186367988586 + 0.001 * 7.355940818786621
Epoch 290, val loss: 0.8196721076965332
Epoch 300, training loss: 0.24895812571048737 = 0.24162620306015015 + 0.001 * 7.3319244384765625
Epoch 300, val loss: 0.827214777469635
Epoch 310, training loss: 0.2214033603668213 = 0.2140868902206421 + 0.001 * 7.316468238830566
Epoch 310, val loss: 0.836931586265564
Epoch 320, training loss: 0.19643686711788177 = 0.1891242116689682 + 0.001 * 7.312649726867676
Epoch 320, val loss: 0.8488653302192688
Epoch 330, training loss: 0.17414423823356628 = 0.16684077680110931 + 0.001 * 7.303459644317627
Epoch 330, val loss: 0.8630425930023193
Epoch 340, training loss: 0.1544969528913498 = 0.14719825983047485 + 0.001 * 7.298693656921387
Epoch 340, val loss: 0.8794085383415222
Epoch 350, training loss: 0.1373414695262909 = 0.13004037737846375 + 0.001 * 7.301084518432617
Epoch 350, val loss: 0.8978528380393982
Epoch 360, training loss: 0.12242203950881958 = 0.11512156575918198 + 0.001 * 7.300472259521484
Epoch 360, val loss: 0.9180723428726196
Epoch 370, training loss: 0.10945811867713928 = 0.10216041654348373 + 0.001 * 7.297703742980957
Epoch 370, val loss: 0.9395965337753296
Epoch 380, training loss: 0.09818194061517715 = 0.09088838845491409 + 0.001 * 7.293552875518799
Epoch 380, val loss: 0.96193927526474
Epoch 390, training loss: 0.08835507929325104 = 0.08106482028961182 + 0.001 * 7.290259838104248
Epoch 390, val loss: 0.9847965836524963
Epoch 400, training loss: 0.07978127151727676 = 0.0724853128194809 + 0.001 * 7.295958042144775
Epoch 400, val loss: 1.0078961849212646
Epoch 410, training loss: 0.07226765900850296 = 0.06497857719659805 + 0.001 * 7.289078235626221
Epoch 410, val loss: 1.0309609174728394
Epoch 420, training loss: 0.06568654626607895 = 0.05840011313557625 + 0.001 * 7.286431789398193
Epoch 420, val loss: 1.0538136959075928
Epoch 430, training loss: 0.05991232395172119 = 0.05262766778469086 + 0.001 * 7.284656047821045
Epoch 430, val loss: 1.076272964477539
Epoch 440, training loss: 0.054838284850120544 = 0.047555889934301376 + 0.001 * 7.282395362854004
Epoch 440, val loss: 1.0982763767242432
Epoch 450, training loss: 0.05037012696266174 = 0.043093208223581314 + 0.001 * 7.276919364929199
Epoch 450, val loss: 1.1197190284729004
Epoch 460, training loss: 0.046445950865745544 = 0.03916078060865402 + 0.001 * 7.285168170928955
Epoch 460, val loss: 1.140554666519165
Epoch 470, training loss: 0.042963311076164246 = 0.03568904101848602 + 0.001 * 7.2742695808410645
Epoch 470, val loss: 1.1607428789138794
Epoch 480, training loss: 0.03988564386963844 = 0.032617900520563126 + 0.001 * 7.26774263381958
Epoch 480, val loss: 1.180281162261963
Epoch 490, training loss: 0.03716861456632614 = 0.0298952404409647 + 0.001 * 7.273372650146484
Epoch 490, val loss: 1.1992063522338867
Epoch 500, training loss: 0.03474239632487297 = 0.02747582271695137 + 0.001 * 7.266573905944824
Epoch 500, val loss: 1.2175263166427612
Epoch 510, training loss: 0.03259801119565964 = 0.025320440530776978 + 0.001 * 7.277568817138672
Epoch 510, val loss: 1.2352553606033325
Epoch 520, training loss: 0.030650649219751358 = 0.02339567057788372 + 0.001 * 7.254977226257324
Epoch 520, val loss: 1.2523748874664307
Epoch 530, training loss: 0.028940733522176743 = 0.021672699600458145 + 0.001 * 7.268033981323242
Epoch 530, val loss: 1.2689191102981567
Epoch 540, training loss: 0.027376234531402588 = 0.02012624964118004 + 0.001 * 7.249985218048096
Epoch 540, val loss: 1.2848961353302002
Epoch 550, training loss: 0.025979286059737206 = 0.01873340643942356 + 0.001 * 7.24587869644165
Epoch 550, val loss: 1.3003511428833008
Epoch 560, training loss: 0.02471555769443512 = 0.017473692074418068 + 0.001 * 7.241864204406738
Epoch 560, val loss: 1.315324306488037
Epoch 570, training loss: 0.023571297526359558 = 0.016328245401382446 + 0.001 * 7.243052005767822
Epoch 570, val loss: 1.3299063444137573
Epoch 580, training loss: 0.022538598626852036 = 0.015281523577868938 + 0.001 * 7.257075786590576
Epoch 580, val loss: 1.3441585302352905
Epoch 590, training loss: 0.02155359648168087 = 0.01432186458259821 + 0.001 * 7.231730937957764
Epoch 590, val loss: 1.3581222295761108
Epoch 600, training loss: 0.020675770938396454 = 0.013440734706819057 + 0.001 * 7.235036373138428
Epoch 600, val loss: 1.3718382120132446
Epoch 610, training loss: 0.019885048270225525 = 0.012630954384803772 + 0.001 * 7.254093647003174
Epoch 610, val loss: 1.3852883577346802
Epoch 620, training loss: 0.019116351380944252 = 0.011886215768754482 + 0.001 * 7.230135440826416
Epoch 620, val loss: 1.3984676599502563
Epoch 630, training loss: 0.018416937440633774 = 0.011200597509741783 + 0.001 * 7.216340065002441
Epoch 630, val loss: 1.4113798141479492
Epoch 640, training loss: 0.01777959242463112 = 0.010568906553089619 + 0.001 * 7.210686206817627
Epoch 640, val loss: 1.4240226745605469
Epoch 650, training loss: 0.017218541353940964 = 0.009986397810280323 + 0.001 * 7.232143402099609
Epoch 650, val loss: 1.436384677886963
Epoch 660, training loss: 0.016673952341079712 = 0.009448773227632046 + 0.001 * 7.225177764892578
Epoch 660, val loss: 1.4484639167785645
Epoch 670, training loss: 0.01614825241267681 = 0.008952430449426174 + 0.001 * 7.195821762084961
Epoch 670, val loss: 1.4602493047714233
Epoch 680, training loss: 0.015732470899820328 = 0.008493438363075256 + 0.001 * 7.2390313148498535
Epoch 680, val loss: 1.4717528820037842
Epoch 690, training loss: 0.015286926180124283 = 0.008068566210567951 + 0.001 * 7.218359470367432
Epoch 690, val loss: 1.4829554557800293
Epoch 700, training loss: 0.014859231188893318 = 0.007674630731344223 + 0.001 * 7.184599876403809
Epoch 700, val loss: 1.4939028024673462
Epoch 710, training loss: 0.014511769637465477 = 0.007308993022888899 + 0.001 * 7.2027764320373535
Epoch 710, val loss: 1.5045585632324219
Epoch 720, training loss: 0.014148646965622902 = 0.006969278212636709 + 0.001 * 7.179368019104004
Epoch 720, val loss: 1.5149496793746948
Epoch 730, training loss: 0.01383169461041689 = 0.00665328511968255 + 0.001 * 7.178409099578857
Epoch 730, val loss: 1.5250853300094604
Epoch 740, training loss: 0.013532106764614582 = 0.00635876739397645 + 0.001 * 7.173338890075684
Epoch 740, val loss: 1.5349576473236084
Epoch 750, training loss: 0.013251326978206635 = 0.0060839238576591015 + 0.001 * 7.167402267456055
Epoch 750, val loss: 1.5445948839187622
Epoch 760, training loss: 0.013012636452913284 = 0.00582708977162838 + 0.001 * 7.185545921325684
Epoch 760, val loss: 1.5539933443069458
Epoch 770, training loss: 0.012757206335663795 = 0.005586746148765087 + 0.001 * 7.170459747314453
Epoch 770, val loss: 1.5631481409072876
Epoch 780, training loss: 0.012529954314231873 = 0.005361873656511307 + 0.001 * 7.168079853057861
Epoch 780, val loss: 1.5720793008804321
Epoch 790, training loss: 0.012305564247071743 = 0.005151179153472185 + 0.001 * 7.154384613037109
Epoch 790, val loss: 1.5807474851608276
Epoch 800, training loss: 0.012117015197873116 = 0.0049533722922205925 + 0.001 * 7.163642406463623
Epoch 800, val loss: 1.5892175436019897
Epoch 810, training loss: 0.011931534856557846 = 0.0047674053348600864 + 0.001 * 7.164129257202148
Epoch 810, val loss: 1.5974819660186768
Epoch 820, training loss: 0.01173404511064291 = 0.004592182580381632 + 0.001 * 7.141862392425537
Epoch 820, val loss: 1.6055585145950317
Epoch 830, training loss: 0.01158381812274456 = 0.0044266097247600555 + 0.001 * 7.157207489013672
Epoch 830, val loss: 1.6134577989578247
Epoch 840, training loss: 0.011453520506620407 = 0.00426923343911767 + 0.001 * 7.184286594390869
Epoch 840, val loss: 1.6212129592895508
Epoch 850, training loss: 0.011304091662168503 = 0.004118718206882477 + 0.001 * 7.185373783111572
Epoch 850, val loss: 1.6289341449737549
Epoch 860, training loss: 0.011114729568362236 = 0.003974469378590584 + 0.001 * 7.140259265899658
Epoch 860, val loss: 1.6365745067596436
Epoch 870, training loss: 0.010986270383000374 = 0.003835732350125909 + 0.001 * 7.150537490844727
Epoch 870, val loss: 1.644236445426941
Epoch 880, training loss: 0.01088397391140461 = 0.0037023774348199368 + 0.001 * 7.181596279144287
Epoch 880, val loss: 1.6518563032150269
Epoch 890, training loss: 0.010732446797192097 = 0.0035742975305765867 + 0.001 * 7.158148765563965
Epoch 890, val loss: 1.6594818830490112
Epoch 900, training loss: 0.01061946526169777 = 0.0034513582941144705 + 0.001 * 7.168107032775879
Epoch 900, val loss: 1.6671056747436523
Epoch 910, training loss: 0.010478829964995384 = 0.003333300817757845 + 0.001 * 7.145528793334961
Epoch 910, val loss: 1.6747443675994873
Epoch 920, training loss: 0.010338650085031986 = 0.003220213809981942 + 0.001 * 7.118436336517334
Epoch 920, val loss: 1.6823344230651855
Epoch 930, training loss: 0.010227885097265244 = 0.003112013218924403 + 0.001 * 7.115871906280518
Epoch 930, val loss: 1.6898835897445679
Epoch 940, training loss: 0.010138221085071564 = 0.0030085663311183453 + 0.001 * 7.129654884338379
Epoch 940, val loss: 1.6974109411239624
Epoch 950, training loss: 0.010040835477411747 = 0.0029096973594278097 + 0.001 * 7.131137847900391
Epoch 950, val loss: 1.7048821449279785
Epoch 960, training loss: 0.009936438873410225 = 0.0028152999002486467 + 0.001 * 7.121139049530029
Epoch 960, val loss: 1.7122663259506226
Epoch 970, training loss: 0.009858907200396061 = 0.0027252512518316507 + 0.001 * 7.133656024932861
Epoch 970, val loss: 1.719560980796814
Epoch 980, training loss: 0.009747936390340328 = 0.00263934931717813 + 0.001 * 7.108586311340332
Epoch 980, val loss: 1.7267355918884277
Epoch 990, training loss: 0.0096655348315835 = 0.0025574706960469484 + 0.001 * 7.1080641746521
Epoch 990, val loss: 1.7338274717330933
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 1.9524707794189453 = 1.9438738822937012 + 0.001 * 8.596853256225586
Epoch 0, val loss: 1.9485851526260376
Epoch 10, training loss: 1.9423047304153442 = 1.9337079524993896 + 0.001 * 8.596796035766602
Epoch 10, val loss: 1.9381033182144165
Epoch 20, training loss: 1.9295262098312378 = 1.9209295511245728 + 0.001 * 8.5966157913208
Epoch 20, val loss: 1.924973726272583
Epoch 30, training loss: 1.9115047454833984 = 1.9029085636138916 + 0.001 * 8.596176147460938
Epoch 30, val loss: 1.9066991806030273
Epoch 40, training loss: 1.885239839553833 = 1.8766448497772217 + 0.001 * 8.595032691955566
Epoch 40, val loss: 1.8804510831832886
Epoch 50, training loss: 1.8490912914276123 = 1.8404998779296875 + 0.001 * 8.59142780303955
Epoch 50, val loss: 1.8458386659622192
Epoch 60, training loss: 1.8088313341140747 = 1.8002545833587646 + 0.001 * 8.576703071594238
Epoch 60, val loss: 1.8106108903884888
Epoch 70, training loss: 1.7742202281951904 = 1.7657217979431152 + 0.001 * 8.498379707336426
Epoch 70, val loss: 1.7810595035552979
Epoch 80, training loss: 1.7315514087677002 = 1.723292350769043 + 0.001 * 8.259113311767578
Epoch 80, val loss: 1.741081953048706
Epoch 90, training loss: 1.6716612577438354 = 1.6635642051696777 + 0.001 * 8.09702205657959
Epoch 90, val loss: 1.6874479055404663
Epoch 100, training loss: 1.5906603336334229 = 1.5827609300613403 + 0.001 * 7.899372100830078
Epoch 100, val loss: 1.6182624101638794
Epoch 110, training loss: 1.4949142932891846 = 1.4873193502426147 + 0.001 * 7.594997406005859
Epoch 110, val loss: 1.5380200147628784
Epoch 120, training loss: 1.3965075016021729 = 1.3889782428741455 + 0.001 * 7.529202938079834
Epoch 120, val loss: 1.455805778503418
Epoch 130, training loss: 1.3001347780227661 = 1.2926490306854248 + 0.001 * 7.485775470733643
Epoch 130, val loss: 1.3772093057632446
Epoch 140, training loss: 1.2052747011184692 = 1.1978163719177246 + 0.001 * 7.458300590515137
Epoch 140, val loss: 1.3005313873291016
Epoch 150, training loss: 1.1121677160263062 = 1.1047340631484985 + 0.001 * 7.433709144592285
Epoch 150, val loss: 1.2262158393859863
Epoch 160, training loss: 1.0230683088302612 = 1.0156575441360474 + 0.001 * 7.410758972167969
Epoch 160, val loss: 1.1555876731872559
Epoch 170, training loss: 0.9399126768112183 = 0.9325278401374817 + 0.001 * 7.384864807128906
Epoch 170, val loss: 1.089722752571106
Epoch 180, training loss: 0.8631941080093384 = 0.8558386564254761 + 0.001 * 7.35545539855957
Epoch 180, val loss: 1.0288609266281128
Epoch 190, training loss: 0.7927945852279663 = 0.7854577302932739 + 0.001 * 7.3368706703186035
Epoch 190, val loss: 0.9728257060050964
Epoch 200, training loss: 0.7287379503250122 = 0.7214148640632629 + 0.001 * 7.3230719566345215
Epoch 200, val loss: 0.9222396612167358
Epoch 210, training loss: 0.6709778308868408 = 0.6636604070663452 + 0.001 * 7.317398548126221
Epoch 210, val loss: 0.8781587481498718
Epoch 220, training loss: 0.61854088306427 = 0.6112284064292908 + 0.001 * 7.312506198883057
Epoch 220, val loss: 0.8407285213470459
Epoch 230, training loss: 0.5699679255485535 = 0.5626602172851562 + 0.001 * 7.307707786560059
Epoch 230, val loss: 0.8090686798095703
Epoch 240, training loss: 0.5241876840591431 = 0.5168851017951965 + 0.001 * 7.302578449249268
Epoch 240, val loss: 0.7823573350906372
Epoch 250, training loss: 0.48074108362197876 = 0.4734402298927307 + 0.001 * 7.300854682922363
Epoch 250, val loss: 0.7599364519119263
Epoch 260, training loss: 0.43932685256004333 = 0.43203240633010864 + 0.001 * 7.294445514678955
Epoch 260, val loss: 0.7413674592971802
Epoch 270, training loss: 0.3996196985244751 = 0.3923281729221344 + 0.001 * 7.291515350341797
Epoch 270, val loss: 0.725845217704773
Epoch 280, training loss: 0.36136311292648315 = 0.3540745973587036 + 0.001 * 7.288525104522705
Epoch 280, val loss: 0.712699830532074
Epoch 290, training loss: 0.3246592879295349 = 0.3173709809780121 + 0.001 * 7.288315296173096
Epoch 290, val loss: 0.7015080451965332
Epoch 300, training loss: 0.28993406891822815 = 0.28264957666397095 + 0.001 * 7.284481048583984
Epoch 300, val loss: 0.6921988129615784
Epoch 310, training loss: 0.25769639015197754 = 0.2504168450832367 + 0.001 * 7.279536247253418
Epoch 310, val loss: 0.6850937008857727
Epoch 320, training loss: 0.2283667027950287 = 0.2210889309644699 + 0.001 * 7.27777624130249
Epoch 320, val loss: 0.6804511547088623
Epoch 330, training loss: 0.20214401185512543 = 0.19486723840236664 + 0.001 * 7.2767767906188965
Epoch 330, val loss: 0.6782180070877075
Epoch 340, training loss: 0.17902134358882904 = 0.17174698412418365 + 0.001 * 7.274362087249756
Epoch 340, val loss: 0.6783519387245178
Epoch 350, training loss: 0.15883509814739227 = 0.15156006813049316 + 0.001 * 7.27502965927124
Epoch 350, val loss: 0.6807882189750671
Epoch 360, training loss: 0.14130303263664246 = 0.13403214514255524 + 0.001 * 7.270884037017822
Epoch 360, val loss: 0.685333251953125
Epoch 370, training loss: 0.12611526250839233 = 0.11884055286645889 + 0.001 * 7.274711608886719
Epoch 370, val loss: 0.6917861104011536
Epoch 380, training loss: 0.11293089389801025 = 0.10565777122974396 + 0.001 * 7.2731194496154785
Epoch 380, val loss: 0.6997795104980469
Epoch 390, training loss: 0.10144855082035065 = 0.09417954087257385 + 0.001 * 7.269007682800293
Epoch 390, val loss: 0.7090400457382202
Epoch 400, training loss: 0.0914195105433464 = 0.0841526910662651 + 0.001 * 7.266816139221191
Epoch 400, val loss: 0.7193465232849121
Epoch 410, training loss: 0.08263138681650162 = 0.07536343485116959 + 0.001 * 7.267950057983398
Epoch 410, val loss: 0.7304767370223999
Epoch 420, training loss: 0.07489771395921707 = 0.06763231754302979 + 0.001 * 7.2653985023498535
Epoch 420, val loss: 0.7422496676445007
Epoch 430, training loss: 0.06808458268642426 = 0.060815878212451935 + 0.001 * 7.268702507019043
Epoch 430, val loss: 0.7544207572937012
Epoch 440, training loss: 0.062062740325927734 = 0.05480130761861801 + 0.001 * 7.261433124542236
Epoch 440, val loss: 0.7668917775154114
Epoch 450, training loss: 0.056750427931547165 = 0.04949061572551727 + 0.001 * 7.2598114013671875
Epoch 450, val loss: 0.7795491814613342
Epoch 460, training loss: 0.05206238850951195 = 0.04480041190981865 + 0.001 * 7.261975288391113
Epoch 460, val loss: 0.7922682762145996
Epoch 470, training loss: 0.04791666939854622 = 0.04065832495689392 + 0.001 * 7.25834321975708
Epoch 470, val loss: 0.80491042137146
Epoch 480, training loss: 0.044252581894397736 = 0.0369982086122036 + 0.001 * 7.2543721199035645
Epoch 480, val loss: 0.8174304962158203
Epoch 490, training loss: 0.04101742058992386 = 0.03376072272658348 + 0.001 * 7.256695747375488
Epoch 490, val loss: 0.8297122120857239
Epoch 500, training loss: 0.038149379193782806 = 0.030892109498381615 + 0.001 * 7.257269859313965
Epoch 500, val loss: 0.8417481184005737
Epoch 510, training loss: 0.035596273839473724 = 0.028345618396997452 + 0.001 * 7.250653266906738
Epoch 510, val loss: 0.8535205125808716
Epoch 520, training loss: 0.03333679586648941 = 0.026079999282956123 + 0.001 * 7.256795406341553
Epoch 520, val loss: 0.8649492859840393
Epoch 530, training loss: 0.03130989521741867 = 0.024059725925326347 + 0.001 * 7.250168800354004
Epoch 530, val loss: 0.8760195970535278
Epoch 540, training loss: 0.02949661761522293 = 0.02225395105779171 + 0.001 * 7.242665767669678
Epoch 540, val loss: 0.8868001699447632
Epoch 550, training loss: 0.027881119400262833 = 0.02063591405749321 + 0.001 * 7.245205879211426
Epoch 550, val loss: 0.8972739577293396
Epoch 560, training loss: 0.026426013559103012 = 0.019182821735739708 + 0.001 * 7.243190765380859
Epoch 560, val loss: 0.9074178338050842
Epoch 570, training loss: 0.02510790154337883 = 0.017873991280794144 + 0.001 * 7.233909606933594
Epoch 570, val loss: 0.9172784090042114
Epoch 580, training loss: 0.02392304316163063 = 0.01669093407690525 + 0.001 * 7.2321085929870605
Epoch 580, val loss: 0.9268662333488464
Epoch 590, training loss: 0.02286791056394577 = 0.01561663020402193 + 0.001 * 7.251279354095459
Epoch 590, val loss: 0.9361670613288879
Epoch 600, training loss: 0.021869787946343422 = 0.014637096785008907 + 0.001 * 7.232690811157227
Epoch 600, val loss: 0.9452226161956787
Epoch 610, training loss: 0.02097897417843342 = 0.0137411467730999 + 0.001 * 7.237827301025391
Epoch 610, val loss: 0.954042911529541
Epoch 620, training loss: 0.02015284262597561 = 0.012920061126351357 + 0.001 * 7.232780933380127
Epoch 620, val loss: 0.9626434445381165
Epoch 630, training loss: 0.019394859671592712 = 0.012166325002908707 + 0.001 * 7.22853422164917
Epoch 630, val loss: 0.9709851145744324
Epoch 640, training loss: 0.01869952864944935 = 0.011473532766103745 + 0.001 * 7.225996017456055
Epoch 640, val loss: 0.979123055934906
Epoch 650, training loss: 0.018052170053124428 = 0.01083611324429512 + 0.001 * 7.216056823730469
Epoch 650, val loss: 0.9870500564575195
Epoch 660, training loss: 0.017466261982917786 = 0.010249077342450619 + 0.001 * 7.217184543609619
Epoch 660, val loss: 0.9947706460952759
Epoch 670, training loss: 0.01692643016576767 = 0.009707875549793243 + 0.001 * 7.21855354309082
Epoch 670, val loss: 1.0022683143615723
Epoch 680, training loss: 0.01641879975795746 = 0.009207941591739655 + 0.001 * 7.21085786819458
Epoch 680, val loss: 1.0095641613006592
Epoch 690, training loss: 0.01595349796116352 = 0.008745552971959114 + 0.001 * 7.207944393157959
Epoch 690, val loss: 1.0166734457015991
Epoch 700, training loss: 0.015531962737441063 = 0.008317278698086739 + 0.001 * 7.214684009552002
Epoch 700, val loss: 1.0235804319381714
Epoch 710, training loss: 0.015139267779886723 = 0.007920114323496819 + 0.001 * 7.219152927398682
Epoch 710, val loss: 1.0303014516830444
Epoch 720, training loss: 0.014746884815394878 = 0.007551345508545637 + 0.001 * 7.1955389976501465
Epoch 720, val loss: 1.0368425846099854
Epoch 730, training loss: 0.014447404071688652 = 0.007208433002233505 + 0.001 * 7.23897123336792
Epoch 730, val loss: 1.0432158708572388
Epoch 740, training loss: 0.014078367501497269 = 0.006889164913445711 + 0.001 * 7.189202785491943
Epoch 740, val loss: 1.0493957996368408
Epoch 750, training loss: 0.01377624087035656 = 0.0065914723090827465 + 0.001 * 7.184767723083496
Epoch 750, val loss: 1.0554163455963135
Epoch 760, training loss: 0.013497551903128624 = 0.006313499063253403 + 0.001 * 7.18405294418335
Epoch 760, val loss: 1.0612832307815552
Epoch 770, training loss: 0.013237958773970604 = 0.0060536181554198265 + 0.001 * 7.184340000152588
Epoch 770, val loss: 1.0669963359832764
Epoch 780, training loss: 0.013003689236938953 = 0.005810316652059555 + 0.001 * 7.1933722496032715
Epoch 780, val loss: 1.07257878780365
Epoch 790, training loss: 0.012769961729645729 = 0.0055823600850999355 + 0.001 * 7.187601566314697
Epoch 790, val loss: 1.0780260562896729
Epoch 800, training loss: 0.01255557406693697 = 0.005368433427065611 + 0.001 * 7.187140464782715
Epoch 800, val loss: 1.0833410024642944
Epoch 810, training loss: 0.012339724227786064 = 0.005167435854673386 + 0.001 * 7.172287464141846
Epoch 810, val loss: 1.0885123014450073
Epoch 820, training loss: 0.012186123058199883 = 0.004978362005203962 + 0.001 * 7.207760334014893
Epoch 820, val loss: 1.0935614109039307
Epoch 830, training loss: 0.011976766400039196 = 0.004800260066986084 + 0.001 * 7.176506042480469
Epoch 830, val loss: 1.0984915494918823
Epoch 840, training loss: 0.011811919510364532 = 0.004632363561540842 + 0.001 * 7.179555892944336
Epoch 840, val loss: 1.103295087814331
Epoch 850, training loss: 0.011654533445835114 = 0.0044739083386957645 + 0.001 * 7.180624961853027
Epoch 850, val loss: 1.1079890727996826
Epoch 860, training loss: 0.011494478210806847 = 0.004324198234826326 + 0.001 * 7.170279502868652
Epoch 860, val loss: 1.112557291984558
Epoch 870, training loss: 0.011358575895428658 = 0.0041826386004686356 + 0.001 * 7.175936698913574
Epoch 870, val loss: 1.1170432567596436
Epoch 880, training loss: 0.011216394603252411 = 0.004048655275255442 + 0.001 * 7.167739391326904
Epoch 880, val loss: 1.1214195489883423
Epoch 890, training loss: 0.0110956821590662 = 0.0039217229932546616 + 0.001 * 7.173959255218506
Epoch 890, val loss: 1.1256963014602661
Epoch 900, training loss: 0.010969547554850578 = 0.0038014110177755356 + 0.001 * 7.168136119842529
Epoch 900, val loss: 1.129876732826233
Epoch 910, training loss: 0.010844936594367027 = 0.003687189193442464 + 0.001 * 7.157747268676758
Epoch 910, val loss: 1.1339566707611084
Epoch 920, training loss: 0.010724486783146858 = 0.003578704781830311 + 0.001 * 7.14578104019165
Epoch 920, val loss: 1.137937307357788
Epoch 930, training loss: 0.010636505670845509 = 0.003475547069683671 + 0.001 * 7.160958290100098
Epoch 930, val loss: 1.1418322324752808
Epoch 940, training loss: 0.010511809960007668 = 0.0033773905597627163 + 0.001 * 7.134418487548828
Epoch 940, val loss: 1.1456332206726074
Epoch 950, training loss: 0.010426015593111515 = 0.0032839092891663313 + 0.001 * 7.142106056213379
Epoch 950, val loss: 1.14937162399292
Epoch 960, training loss: 0.010331894271075726 = 0.00319482060149312 + 0.001 * 7.137073516845703
Epoch 960, val loss: 1.1530039310455322
Epoch 970, training loss: 0.010317955166101456 = 0.0031098665203899145 + 0.001 * 7.208087921142578
Epoch 970, val loss: 1.1565477848052979
Epoch 980, training loss: 0.010168633423745632 = 0.003028788371011615 + 0.001 * 7.1398444175720215
Epoch 980, val loss: 1.1600390672683716
Epoch 990, training loss: 0.010096846148371696 = 0.0029513759072870016 + 0.001 * 7.145469665527344
Epoch 990, val loss: 1.1634249687194824
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.9586936235427856 = 1.9500967264175415 + 0.001 * 8.596857070922852
Epoch 0, val loss: 1.9451237916946411
Epoch 10, training loss: 1.9479573965072632 = 1.9393606185913086 + 0.001 * 8.596817970275879
Epoch 10, val loss: 1.934800624847412
Epoch 20, training loss: 1.93448007106781 = 1.925883412361145 + 0.001 * 8.596677780151367
Epoch 20, val loss: 1.9214560985565186
Epoch 30, training loss: 1.9156330823898315 = 1.9070367813110352 + 0.001 * 8.596348762512207
Epoch 30, val loss: 1.9025225639343262
Epoch 40, training loss: 1.888291358947754 = 1.8796957731246948 + 0.001 * 8.595556259155273
Epoch 40, val loss: 1.8751482963562012
Epoch 50, training loss: 1.8507390022277832 = 1.8421456813812256 + 0.001 * 8.593315124511719
Epoch 50, val loss: 1.8390600681304932
Epoch 60, training loss: 1.8094643354415894 = 1.8008791208267212 + 0.001 * 8.585241317749023
Epoch 60, val loss: 1.8039116859436035
Epoch 70, training loss: 1.7751809358596802 = 1.766631841659546 + 0.001 * 8.549150466918945
Epoch 70, val loss: 1.7783023118972778
Epoch 80, training loss: 1.7326819896697998 = 1.7243634462356567 + 0.001 * 8.318483352661133
Epoch 80, val loss: 1.7413480281829834
Epoch 90, training loss: 1.6731458902359009 = 1.6650352478027344 + 0.001 * 8.110686302185059
Epoch 90, val loss: 1.6885054111480713
Epoch 100, training loss: 1.5945762395858765 = 1.586617112159729 + 0.001 * 7.959161758422852
Epoch 100, val loss: 1.6217515468597412
Epoch 110, training loss: 1.5037530660629272 = 1.496011734008789 + 0.001 * 7.741385459899902
Epoch 110, val loss: 1.5482933521270752
Epoch 120, training loss: 1.4112414121627808 = 1.4036424160003662 + 0.001 * 7.599040985107422
Epoch 120, val loss: 1.4738250970840454
Epoch 130, training loss: 1.318875789642334 = 1.3113678693771362 + 0.001 * 7.507908821105957
Epoch 130, val loss: 1.3997212648391724
Epoch 140, training loss: 1.2238215208053589 = 1.2163934707641602 + 0.001 * 7.428074836730957
Epoch 140, val loss: 1.3233376741409302
Epoch 150, training loss: 1.1246123313903809 = 1.1171903610229492 + 0.001 * 7.421915054321289
Epoch 150, val loss: 1.2428244352340698
Epoch 160, training loss: 1.0232394933700562 = 1.0158281326293945 + 0.001 * 7.411332607269287
Epoch 160, val loss: 1.1602548360824585
Epoch 170, training loss: 0.9240870475769043 = 0.9166796207427979 + 0.001 * 7.407444000244141
Epoch 170, val loss: 1.079207420349121
Epoch 180, training loss: 0.831413984298706 = 0.8240106105804443 + 0.001 * 7.403390884399414
Epoch 180, val loss: 1.0044821500778198
Epoch 190, training loss: 0.7475581765174866 = 0.7401609420776367 + 0.001 * 7.397216320037842
Epoch 190, val loss: 0.9387288689613342
Epoch 200, training loss: 0.672440767288208 = 0.6650524735450745 + 0.001 * 7.388279438018799
Epoch 200, val loss: 0.882704496383667
Epoch 210, training loss: 0.604364812374115 = 0.5969904661178589 + 0.001 * 7.374360084533691
Epoch 210, val loss: 0.8355045914649963
Epoch 220, training loss: 0.5414925217628479 = 0.5341395735740662 + 0.001 * 7.352951526641846
Epoch 220, val loss: 0.7950985431671143
Epoch 230, training loss: 0.48252227902412415 = 0.47518813610076904 + 0.001 * 7.334128379821777
Epoch 230, val loss: 0.7597745060920715
Epoch 240, training loss: 0.42683306336402893 = 0.4195300042629242 + 0.001 * 7.303067207336426
Epoch 240, val loss: 0.7290662527084351
Epoch 250, training loss: 0.37453779578208923 = 0.36725759506225586 + 0.001 * 7.28020715713501
Epoch 250, val loss: 0.7038468718528748
Epoch 260, training loss: 0.3259986639022827 = 0.31873053312301636 + 0.001 * 7.268124580383301
Epoch 260, val loss: 0.6844729781150818
Epoch 270, training loss: 0.2816172242164612 = 0.2743537724018097 + 0.001 * 7.2634501457214355
Epoch 270, val loss: 0.6708826422691345
Epoch 280, training loss: 0.24181419610977173 = 0.23455189168453217 + 0.001 * 7.2623114585876465
Epoch 280, val loss: 0.6627711057662964
Epoch 290, training loss: 0.20688213407993317 = 0.19962246716022491 + 0.001 * 7.259666442871094
Epoch 290, val loss: 0.6597267389297485
Epoch 300, training loss: 0.17689336836338043 = 0.16963402926921844 + 0.001 * 7.259335517883301
Epoch 300, val loss: 0.6612358689308167
Epoch 310, training loss: 0.15162929892539978 = 0.14436590671539307 + 0.001 * 7.263387680053711
Epoch 310, val loss: 0.6666744351387024
Epoch 320, training loss: 0.13058164715766907 = 0.12332189828157425 + 0.001 * 7.2597432136535645
Epoch 320, val loss: 0.6751726269721985
Epoch 330, training loss: 0.1131531223654747 = 0.10589240491390228 + 0.001 * 7.260714530944824
Epoch 330, val loss: 0.6859275698661804
Epoch 340, training loss: 0.09872318804264069 = 0.0914623886346817 + 0.001 * 7.260800361633301
Epoch 340, val loss: 0.6983409523963928
Epoch 350, training loss: 0.08674649149179459 = 0.079483762383461 + 0.001 * 7.262732028961182
Epoch 350, val loss: 0.7118610739707947
Epoch 360, training loss: 0.07675965130329132 = 0.06949712336063385 + 0.001 * 7.262529373168945
Epoch 360, val loss: 0.7260162234306335
Epoch 370, training loss: 0.068390853703022 = 0.06113003194332123 + 0.001 * 7.260819911956787
Epoch 370, val loss: 0.740439236164093
Epoch 380, training loss: 0.061342332512140274 = 0.054081860929727554 + 0.001 * 7.260472297668457
Epoch 380, val loss: 0.7549076676368713
Epoch 390, training loss: 0.05537375062704086 = 0.04811147600412369 + 0.001 * 7.26227331161499
Epoch 390, val loss: 0.7692572474479675
Epoch 400, training loss: 0.050285033881664276 = 0.04302629828453064 + 0.001 * 7.258735656738281
Epoch 400, val loss: 0.7833688259124756
Epoch 410, training loss: 0.04593080282211304 = 0.038669489324092865 + 0.001 * 7.261313438415527
Epoch 410, val loss: 0.7971634268760681
Epoch 420, training loss: 0.042173661291599274 = 0.03491675853729248 + 0.001 * 7.256900787353516
Epoch 420, val loss: 0.8105511665344238
Epoch 430, training loss: 0.03892311081290245 = 0.03166782483458519 + 0.001 * 7.255284309387207
Epoch 430, val loss: 0.8235527276992798
Epoch 440, training loss: 0.03609532117843628 = 0.028840146958827972 + 0.001 * 7.2551727294921875
Epoch 440, val loss: 0.836177408695221
Epoch 450, training loss: 0.033623747527599335 = 0.026367487385869026 + 0.001 * 7.256261825561523
Epoch 450, val loss: 0.8483849167823792
Epoch 460, training loss: 0.03144245594739914 = 0.024195147678256035 + 0.001 * 7.247306823730469
Epoch 460, val loss: 0.8601958751678467
Epoch 470, training loss: 0.0295201875269413 = 0.022278351709246635 + 0.001 * 7.24183464050293
Epoch 470, val loss: 0.8716013431549072
Epoch 480, training loss: 0.027820788323879242 = 0.020579809322953224 + 0.001 * 7.240979194641113
Epoch 480, val loss: 0.8826372623443604
Epoch 490, training loss: 0.02630995400249958 = 0.01906876638531685 + 0.001 * 7.241187572479248
Epoch 490, val loss: 0.8933197259902954
Epoch 500, training loss: 0.024948444217443466 = 0.01771923340857029 + 0.001 * 7.22921085357666
Epoch 500, val loss: 0.9036971926689148
Epoch 510, training loss: 0.023761548101902008 = 0.016509678214788437 + 0.001 * 7.251870632171631
Epoch 510, val loss: 0.9137274622917175
Epoch 520, training loss: 0.022659584879875183 = 0.015421770513057709 + 0.001 * 7.237813949584961
Epoch 520, val loss: 0.9234567880630493
Epoch 530, training loss: 0.021664245054125786 = 0.014440073631703854 + 0.001 * 7.2241716384887695
Epoch 530, val loss: 0.9328619837760925
Epoch 540, training loss: 0.020780522376298904 = 0.01355136837810278 + 0.001 * 7.229153633117676
Epoch 540, val loss: 0.9419981837272644
Epoch 550, training loss: 0.019965603947639465 = 0.012744500301778316 + 0.001 * 7.221102714538574
Epoch 550, val loss: 0.9508418440818787
Epoch 560, training loss: 0.01923702284693718 = 0.012009970843791962 + 0.001 * 7.22705078125
Epoch 560, val loss: 0.9594204425811768
Epoch 570, training loss: 0.018544334918260574 = 0.011339465156197548 + 0.001 * 7.204868793487549
Epoch 570, val loss: 0.9677319526672363
Epoch 580, training loss: 0.01791742444038391 = 0.01072581298649311 + 0.001 * 7.191612243652344
Epoch 580, val loss: 0.9757994413375854
Epoch 590, training loss: 0.01734990067780018 = 0.010162792168557644 + 0.001 * 7.187108039855957
Epoch 590, val loss: 0.9836310744285583
Epoch 600, training loss: 0.01682920567691326 = 0.009645051322877407 + 0.001 * 7.184154033660889
Epoch 600, val loss: 0.9912481307983398
Epoch 610, training loss: 0.016355544328689575 = 0.009167885407805443 + 0.001 * 7.187658309936523
Epoch 610, val loss: 0.9986594915390015
Epoch 620, training loss: 0.015919167548418045 = 0.008727186359465122 + 0.001 * 7.191980361938477
Epoch 620, val loss: 1.0058845281600952
Epoch 630, training loss: 0.015501468442380428 = 0.00831936951726675 + 0.001 * 7.182098388671875
Epoch 630, val loss: 1.012912392616272
Epoch 640, training loss: 0.01511392742395401 = 0.007941230200231075 + 0.001 * 7.172697067260742
Epoch 640, val loss: 1.0197428464889526
Epoch 650, training loss: 0.014765994623303413 = 0.00758999353274703 + 0.001 * 7.176000595092773
Epoch 650, val loss: 1.0264203548431396
Epoch 660, training loss: 0.014447249472141266 = 0.007263130974024534 + 0.001 * 7.184118747711182
Epoch 660, val loss: 1.0328980684280396
Epoch 670, training loss: 0.01413633394986391 = 0.006958472542464733 + 0.001 * 7.177861213684082
Epoch 670, val loss: 1.0392142534255981
Epoch 680, training loss: 0.013836636207997799 = 0.006674023345112801 + 0.001 * 7.162612438201904
Epoch 680, val loss: 1.0453646183013916
Epoch 690, training loss: 0.013622617349028587 = 0.0064080459997057915 + 0.001 * 7.214571475982666
Epoch 690, val loss: 1.0513553619384766
Epoch 700, training loss: 0.013328611850738525 = 0.0061589935794472694 + 0.001 * 7.169618606567383
Epoch 700, val loss: 1.0571871995925903
Epoch 710, training loss: 0.013095940463244915 = 0.005925457924604416 + 0.001 * 7.170482158660889
Epoch 710, val loss: 1.0628941059112549
Epoch 720, training loss: 0.01286664605140686 = 0.0057061403058469296 + 0.001 * 7.160505294799805
Epoch 720, val loss: 1.0684515237808228
Epoch 730, training loss: 0.012666960246860981 = 0.005499917082488537 + 0.001 * 7.1670427322387695
Epoch 730, val loss: 1.0738803148269653
Epoch 740, training loss: 0.012455129064619541 = 0.005305770318955183 + 0.001 * 7.14935827255249
Epoch 740, val loss: 1.0791711807250977
Epoch 750, training loss: 0.012295586988329887 = 0.005122788716107607 + 0.001 * 7.172798156738281
Epoch 750, val loss: 1.0843462944030762
Epoch 760, training loss: 0.012102501466870308 = 0.004950112663209438 + 0.001 * 7.152389049530029
Epoch 760, val loss: 1.0893802642822266
Epoch 770, training loss: 0.011959422379732132 = 0.004786984529346228 + 0.001 * 7.1724371910095215
Epoch 770, val loss: 1.0943183898925781
Epoch 780, training loss: 0.011776197701692581 = 0.004632716998457909 + 0.001 * 7.1434807777404785
Epoch 780, val loss: 1.0991209745407104
Epoch 790, training loss: 0.011624883860349655 = 0.004486679099500179 + 0.001 * 7.138205051422119
Epoch 790, val loss: 1.1038262844085693
Epoch 800, training loss: 0.011494968086481094 = 0.004348272457718849 + 0.001 * 7.146695613861084
Epoch 800, val loss: 1.1084181070327759
Epoch 810, training loss: 0.011349975131452084 = 0.004216997884213924 + 0.001 * 7.13297700881958
Epoch 810, val loss: 1.1129083633422852
Epoch 820, training loss: 0.011241555213928223 = 0.00409236503764987 + 0.001 * 7.149190425872803
Epoch 820, val loss: 1.1173018217086792
Epoch 830, training loss: 0.011105496436357498 = 0.003973929677158594 + 0.001 * 7.131566047668457
Epoch 830, val loss: 1.1216014623641968
Epoch 840, training loss: 0.01098688319325447 = 0.0038612945936620235 + 0.001 * 7.125588417053223
Epoch 840, val loss: 1.1257902383804321
Epoch 850, training loss: 0.010884483344852924 = 0.0037540781777352095 + 0.001 * 7.130404949188232
Epoch 850, val loss: 1.129910945892334
Epoch 860, training loss: 0.010797630064189434 = 0.003651927225291729 + 0.001 * 7.145702362060547
Epoch 860, val loss: 1.1339380741119385
Epoch 870, training loss: 0.010702180676162243 = 0.003554547904059291 + 0.001 * 7.147632598876953
Epoch 870, val loss: 1.1378620862960815
Epoch 880, training loss: 0.010591361671686172 = 0.0034616319462656975 + 0.001 * 7.129729270935059
Epoch 880, val loss: 1.141724705696106
Epoch 890, training loss: 0.010503365658223629 = 0.0033729069400578737 + 0.001 * 7.130458354949951
Epoch 890, val loss: 1.1454988718032837
Epoch 900, training loss: 0.010414320975542068 = 0.0032881107181310654 + 0.001 * 7.1262102127075195
Epoch 900, val loss: 1.1491823196411133
Epoch 910, training loss: 0.010324505157768726 = 0.0032070204615592957 + 0.001 * 7.1174845695495605
Epoch 910, val loss: 1.1527960300445557
Epoch 920, training loss: 0.010261244140565395 = 0.003129357472062111 + 0.001 * 7.1318864822387695
Epoch 920, val loss: 1.1563538312911987
Epoch 930, training loss: 0.010176701471209526 = 0.0030548227950930595 + 0.001 * 7.121878147125244
Epoch 930, val loss: 1.1598620414733887
Epoch 940, training loss: 0.010111880488693714 = 0.0029830315615981817 + 0.001 * 7.128848552703857
Epoch 940, val loss: 1.163343906402588
Epoch 950, training loss: 0.010020341724157333 = 0.002913582604378462 + 0.001 * 7.106759071350098
Epoch 950, val loss: 1.166806936264038
Epoch 960, training loss: 0.009981777518987656 = 0.002845920156687498 + 0.001 * 7.135856628417969
Epoch 960, val loss: 1.1703463792800903
Epoch 970, training loss: 0.009884705767035484 = 0.0027796346694231033 + 0.001 * 7.105071067810059
Epoch 970, val loss: 1.1739232540130615
Epoch 980, training loss: 0.00982669461518526 = 0.002714490285143256 + 0.001 * 7.112203598022461
Epoch 980, val loss: 1.1775922775268555
Epoch 990, training loss: 0.009739784523844719 = 0.0026509237941354513 + 0.001 * 7.088860511779785
Epoch 990, val loss: 1.181290626525879
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8313125988402742
The final CL Acc:0.80741, 0.01983, The final GNN Acc:0.83623, 0.00388
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11658])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10598])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9726334810256958 = 1.9640365839004517 + 0.001 * 8.596854209899902
Epoch 0, val loss: 1.9583868980407715
Epoch 10, training loss: 1.962837815284729 = 1.9542410373687744 + 0.001 * 8.596822738647461
Epoch 10, val loss: 1.949143648147583
Epoch 20, training loss: 1.9511945247650146 = 1.9425978660583496 + 0.001 * 8.596715927124023
Epoch 20, val loss: 1.9378198385238647
Epoch 30, training loss: 1.9354825019836426 = 1.926885962486267 + 0.001 * 8.596513748168945
Epoch 30, val loss: 1.922179102897644
Epoch 40, training loss: 1.9125009775161743 = 1.903904914855957 + 0.001 * 8.596120834350586
Epoch 40, val loss: 1.8992133140563965
Epoch 50, training loss: 1.8789618015289307 = 1.8703665733337402 + 0.001 * 8.59528636932373
Epoch 50, val loss: 1.8664487600326538
Epoch 60, training loss: 1.8358060121536255 = 1.8272128105163574 + 0.001 * 8.593188285827637
Epoch 60, val loss: 1.8274415731430054
Epoch 70, training loss: 1.794761300086975 = 1.7861747741699219 + 0.001 * 8.586503982543945
Epoch 70, val loss: 1.7954515218734741
Epoch 80, training loss: 1.758012056350708 = 1.7494577169418335 + 0.001 * 8.554380416870117
Epoch 80, val loss: 1.7656019926071167
Epoch 90, training loss: 1.7095444202423096 = 1.7012897729873657 + 0.001 * 8.25469970703125
Epoch 90, val loss: 1.7215386629104614
Epoch 100, training loss: 1.6438624858856201 = 1.6358222961425781 + 0.001 * 8.040177345275879
Epoch 100, val loss: 1.6630487442016602
Epoch 110, training loss: 1.558521032333374 = 1.5506969690322876 + 0.001 * 7.824082851409912
Epoch 110, val loss: 1.5890439748764038
Epoch 120, training loss: 1.4578428268432617 = 1.4502471685409546 + 0.001 * 7.595610618591309
Epoch 120, val loss: 1.5064927339553833
Epoch 130, training loss: 1.352372646331787 = 1.3448125123977661 + 0.001 * 7.560157775878906
Epoch 130, val loss: 1.423704743385315
Epoch 140, training loss: 1.2501935958862305 = 1.2426724433898926 + 0.001 * 7.52117919921875
Epoch 140, val loss: 1.346566915512085
Epoch 150, training loss: 1.154075264930725 = 1.1465765237808228 + 0.001 * 7.498708248138428
Epoch 150, val loss: 1.276732325553894
Epoch 160, training loss: 1.0634599924087524 = 1.055975317955017 + 0.001 * 7.484707355499268
Epoch 160, val loss: 1.2131619453430176
Epoch 170, training loss: 0.9771788120269775 = 0.9697096347808838 + 0.001 * 7.469178199768066
Epoch 170, val loss: 1.153770923614502
Epoch 180, training loss: 0.8950837254524231 = 0.8876312375068665 + 0.001 * 7.4525146484375
Epoch 180, val loss: 1.0984728336334229
Epoch 190, training loss: 0.8178105354309082 = 0.810378909111023 + 0.001 * 7.4316086769104
Epoch 190, val loss: 1.0475209951400757
Epoch 200, training loss: 0.745867908000946 = 0.7384639382362366 + 0.001 * 7.403970718383789
Epoch 200, val loss: 1.0012515783309937
Epoch 210, training loss: 0.6792668700218201 = 0.6718989610671997 + 0.001 * 7.367921352386475
Epoch 210, val loss: 0.960198163986206
Epoch 220, training loss: 0.61756432056427 = 0.6102160811424255 + 0.001 * 7.348224639892578
Epoch 220, val loss: 0.9242551922798157
Epoch 230, training loss: 0.5600089430809021 = 0.5526838898658752 + 0.001 * 7.325079917907715
Epoch 230, val loss: 0.8934511542320251
Epoch 240, training loss: 0.5057726502418518 = 0.4984562397003174 + 0.001 * 7.316400527954102
Epoch 240, val loss: 0.867483913898468
Epoch 250, training loss: 0.4541032016277313 = 0.44679105281829834 + 0.001 * 7.3121538162231445
Epoch 250, val loss: 0.845663845539093
Epoch 260, training loss: 0.40471988916397095 = 0.39741218090057373 + 0.001 * 7.307717800140381
Epoch 260, val loss: 0.8277705311775208
Epoch 270, training loss: 0.3579725921154022 = 0.35066935420036316 + 0.001 * 7.303224086761475
Epoch 270, val loss: 0.8139228224754333
Epoch 280, training loss: 0.3146507740020752 = 0.30735138058662415 + 0.001 * 7.299392223358154
Epoch 280, val loss: 0.8040547966957092
Epoch 290, training loss: 0.2754666209220886 = 0.2681735157966614 + 0.001 * 7.293111324310303
Epoch 290, val loss: 0.798211395740509
Epoch 300, training loss: 0.24076080322265625 = 0.23347236216068268 + 0.001 * 7.2884368896484375
Epoch 300, val loss: 0.7963211536407471
Epoch 310, training loss: 0.21044613420963287 = 0.20315797626972198 + 0.001 * 7.2881598472595215
Epoch 310, val loss: 0.7981294989585876
Epoch 320, training loss: 0.18419867753982544 = 0.1769099235534668 + 0.001 * 7.288747787475586
Epoch 320, val loss: 0.8032539486885071
Epoch 330, training loss: 0.16157402098178864 = 0.15429560840129852 + 0.001 * 7.278416633605957
Epoch 330, val loss: 0.8112607002258301
Epoch 340, training loss: 0.14213062822818756 = 0.1348610669374466 + 0.001 * 7.269560813903809
Epoch 340, val loss: 0.8216946721076965
Epoch 350, training loss: 0.12544749677181244 = 0.11817997694015503 + 0.001 * 7.267520427703857
Epoch 350, val loss: 0.8341459035873413
Epoch 360, training loss: 0.11113250255584717 = 0.10386231541633606 + 0.001 * 7.270188808441162
Epoch 360, val loss: 0.8481871485710144
Epoch 370, training loss: 0.09883533418178558 = 0.0915655642747879 + 0.001 * 7.269767761230469
Epoch 370, val loss: 0.8634324073791504
Epoch 380, training loss: 0.08825942128896713 = 0.08098986744880676 + 0.001 * 7.269554138183594
Epoch 380, val loss: 0.8794888257980347
Epoch 390, training loss: 0.07914603501558304 = 0.07187828421592712 + 0.001 * 7.267749786376953
Epoch 390, val loss: 0.8960300087928772
Epoch 400, training loss: 0.07127919793128967 = 0.06401406228542328 + 0.001 * 7.26513671875
Epoch 400, val loss: 0.9128104448318481
Epoch 410, training loss: 0.06447293609380722 = 0.05721277371048927 + 0.001 * 7.260159492492676
Epoch 410, val loss: 0.929564893245697
Epoch 420, training loss: 0.05857400596141815 = 0.05131665617227554 + 0.001 * 7.257350921630859
Epoch 420, val loss: 0.9461904764175415
Epoch 430, training loss: 0.05344759300351143 = 0.046191610395908356 + 0.001 * 7.255982398986816
Epoch 430, val loss: 0.9625550508499146
Epoch 440, training loss: 0.04898381605744362 = 0.0417255237698555 + 0.001 * 7.2582926750183105
Epoch 440, val loss: 0.9786341786384583
Epoch 450, training loss: 0.045077845454216 = 0.03782322257757187 + 0.001 * 7.254624366760254
Epoch 450, val loss: 0.9943499565124512
Epoch 460, training loss: 0.041657377034425735 = 0.034403156489133835 + 0.001 * 7.254221439361572
Epoch 460, val loss: 1.0096813440322876
Epoch 470, training loss: 0.038656800985336304 = 0.03139645606279373 + 0.001 * 7.260342597961426
Epoch 470, val loss: 1.024620532989502
Epoch 480, training loss: 0.03599236533045769 = 0.0287447702139616 + 0.001 * 7.247593879699707
Epoch 480, val loss: 1.0391823053359985
Epoch 490, training loss: 0.03364638239145279 = 0.02639845944941044 + 0.001 * 7.247921466827393
Epoch 490, val loss: 1.053355097770691
Epoch 500, training loss: 0.03155829384922981 = 0.02431565895676613 + 0.001 * 7.242634296417236
Epoch 500, val loss: 1.0671261548995972
Epoch 510, training loss: 0.02972692996263504 = 0.022461112588644028 + 0.001 * 7.2658162117004395
Epoch 510, val loss: 1.0805257558822632
Epoch 520, training loss: 0.028044790029525757 = 0.02080473303794861 + 0.001 * 7.240056991577148
Epoch 520, val loss: 1.0935266017913818
Epoch 530, training loss: 0.02656247466802597 = 0.019320618361234665 + 0.001 * 7.241855621337891
Epoch 530, val loss: 1.1061729192733765
Epoch 540, training loss: 0.02522111125290394 = 0.017986752092838287 + 0.001 * 7.234358310699463
Epoch 540, val loss: 1.1184604167938232
Epoch 550, training loss: 0.024015912786126137 = 0.016783332452178 + 0.001 * 7.232580184936523
Epoch 550, val loss: 1.1304253339767456
Epoch 560, training loss: 0.02292838878929615 = 0.015692204236984253 + 0.001 * 7.2361836433410645
Epoch 560, val loss: 1.1420892477035522
Epoch 570, training loss: 0.02193230390548706 = 0.014696955680847168 + 0.001 * 7.235348701477051
Epoch 570, val loss: 1.1534892320632935
Epoch 580, training loss: 0.02102118544280529 = 0.013785803690552711 + 0.001 * 7.235381603240967
Epoch 580, val loss: 1.1646770238876343
Epoch 590, training loss: 0.02017124369740486 = 0.012950228527188301 + 0.001 * 7.221014499664307
Epoch 590, val loss: 1.175622582435608
Epoch 600, training loss: 0.019403846934437752 = 0.01218304131180048 + 0.001 * 7.2208051681518555
Epoch 600, val loss: 1.1863075494766235
Epoch 610, training loss: 0.018694013357162476 = 0.011477097868919373 + 0.001 * 7.216915607452393
Epoch 610, val loss: 1.1967915296554565
Epoch 620, training loss: 0.018041392788290977 = 0.010826186276972294 + 0.001 * 7.215205669403076
Epoch 620, val loss: 1.207062840461731
Epoch 630, training loss: 0.01743815280497074 = 0.01022426038980484 + 0.001 * 7.213891506195068
Epoch 630, val loss: 1.2171354293823242
Epoch 640, training loss: 0.016878874972462654 = 0.009666858240962029 + 0.001 * 7.2120161056518555
Epoch 640, val loss: 1.2270491123199463
Epoch 650, training loss: 0.016357628628611565 = 0.009149680845439434 + 0.001 * 7.207947254180908
Epoch 650, val loss: 1.2367795705795288
Epoch 660, training loss: 0.015890534967184067 = 0.008669431321322918 + 0.001 * 7.221102714538574
Epoch 660, val loss: 1.246347188949585
Epoch 670, training loss: 0.015438131988048553 = 0.00822354294359684 + 0.001 * 7.214588165283203
Epoch 670, val loss: 1.2557576894760132
Epoch 680, training loss: 0.015024970285594463 = 0.007809270638972521 + 0.001 * 7.215699195861816
Epoch 680, val loss: 1.2649809122085571
Epoch 690, training loss: 0.014622580260038376 = 0.007424121722579002 + 0.001 * 7.198458194732666
Epoch 690, val loss: 1.2740545272827148
Epoch 700, training loss: 0.01426217146217823 = 0.007065914571285248 + 0.001 * 7.1962571144104
Epoch 700, val loss: 1.282951831817627
Epoch 710, training loss: 0.01393230352550745 = 0.006732632871717215 + 0.001 * 7.199670314788818
Epoch 710, val loss: 1.2916581630706787
Epoch 720, training loss: 0.013619071803987026 = 0.006422153674066067 + 0.001 * 7.19691801071167
Epoch 720, val loss: 1.3002406358718872
Epoch 730, training loss: 0.013326497748494148 = 0.006132781505584717 + 0.001 * 7.193716049194336
Epoch 730, val loss: 1.3085895776748657
Epoch 740, training loss: 0.013067461550235748 = 0.005862726829946041 + 0.001 * 7.204734802246094
Epoch 740, val loss: 1.3168171644210815
Epoch 750, training loss: 0.012787451967597008 = 0.005610507447272539 + 0.001 * 7.176943778991699
Epoch 750, val loss: 1.3248344659805298
Epoch 760, training loss: 0.012553472071886063 = 0.005374646279960871 + 0.001 * 7.1788249015808105
Epoch 760, val loss: 1.33270263671875
Epoch 770, training loss: 0.01232689805328846 = 0.005153908859938383 + 0.001 * 7.172988414764404
Epoch 770, val loss: 1.340412974357605
Epoch 780, training loss: 0.012129826471209526 = 0.004947049543261528 + 0.001 * 7.18277645111084
Epoch 780, val loss: 1.347944974899292
Epoch 790, training loss: 0.011943671852350235 = 0.004753006622195244 + 0.001 * 7.190664768218994
Epoch 790, val loss: 1.3553245067596436
Epoch 800, training loss: 0.0117379454895854 = 0.004570820368826389 + 0.001 * 7.1671247482299805
Epoch 800, val loss: 1.3625578880310059
Epoch 810, training loss: 0.011583544313907623 = 0.004399544559419155 + 0.001 * 7.183999061584473
Epoch 810, val loss: 1.3696365356445312
Epoch 820, training loss: 0.011428695172071457 = 0.004238421563059092 + 0.001 * 7.190273761749268
Epoch 820, val loss: 1.376559853553772
Epoch 830, training loss: 0.01126611977815628 = 0.004086629953235388 + 0.001 * 7.179489612579346
Epoch 830, val loss: 1.3833611011505127
Epoch 840, training loss: 0.011103572323918343 = 0.003943534567952156 + 0.001 * 7.160037040710449
Epoch 840, val loss: 1.389998197555542
Epoch 850, training loss: 0.010968191549181938 = 0.0038084194529801607 + 0.001 * 7.159771919250488
Epoch 850, val loss: 1.3965253829956055
Epoch 860, training loss: 0.010837176814675331 = 0.003680766560137272 + 0.001 * 7.156409740447998
Epoch 860, val loss: 1.4029110670089722
Epoch 870, training loss: 0.010761470533907413 = 0.0035599779803305864 + 0.001 * 7.201491832733154
Epoch 870, val loss: 1.4091819524765015
Epoch 880, training loss: 0.010596517473459244 = 0.0034458658192306757 + 0.001 * 7.150651454925537
Epoch 880, val loss: 1.41531240940094
Epoch 890, training loss: 0.010491514578461647 = 0.0033374964259564877 + 0.001 * 7.154018402099609
Epoch 890, val loss: 1.4213175773620605
Epoch 900, training loss: 0.01041522528976202 = 0.00323482952080667 + 0.001 * 7.180395603179932
Epoch 900, val loss: 1.4271858930587769
Epoch 910, training loss: 0.010271701961755753 = 0.0031373235397040844 + 0.001 * 7.134377479553223
Epoch 910, val loss: 1.4329646825790405
Epoch 920, training loss: 0.010181219317018986 = 0.00304476753808558 + 0.001 * 7.136451244354248
Epoch 920, val loss: 1.4386346340179443
Epoch 930, training loss: 0.010105067864060402 = 0.0029567545279860497 + 0.001 * 7.148312568664551
Epoch 930, val loss: 1.4441694021224976
Epoch 940, training loss: 0.010008912533521652 = 0.0028730477206408978 + 0.001 * 7.135864734649658
Epoch 940, val loss: 1.449612021446228
Epoch 950, training loss: 0.009928055107593536 = 0.002793333726003766 + 0.001 * 7.134721279144287
Epoch 950, val loss: 1.4549548625946045
Epoch 960, training loss: 0.009842793457210064 = 0.002717354102060199 + 0.001 * 7.125439167022705
Epoch 960, val loss: 1.4601942300796509
Epoch 970, training loss: 0.009776648133993149 = 0.002644957974553108 + 0.001 * 7.131689548492432
Epoch 970, val loss: 1.4653289318084717
Epoch 980, training loss: 0.009700175374746323 = 0.0025758983101695776 + 0.001 * 7.124277114868164
Epoch 980, val loss: 1.4703596830368042
Epoch 990, training loss: 0.009626884013414383 = 0.0025099576450884342 + 0.001 * 7.1169257164001465
Epoch 990, val loss: 1.4753204584121704
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8228782287822879
=== training gcn model ===
Epoch 0, training loss: 1.940930962562561 = 1.9323341846466064 + 0.001 * 8.596808433532715
Epoch 0, val loss: 1.9262982606887817
Epoch 10, training loss: 1.9311543703079224 = 1.9225575923919678 + 0.001 * 8.596758842468262
Epoch 10, val loss: 1.917237401008606
Epoch 20, training loss: 1.9191679954528809 = 1.9105714559555054 + 0.001 * 8.596593856811523
Epoch 20, val loss: 1.9058386087417603
Epoch 30, training loss: 1.902315616607666 = 1.8937194347381592 + 0.001 * 8.596226692199707
Epoch 30, val loss: 1.8897289037704468
Epoch 40, training loss: 1.8776222467422485 = 1.8690268993377686 + 0.001 * 8.595348358154297
Epoch 40, val loss: 1.866553544998169
Epoch 50, training loss: 1.8444132804870605 = 1.8358204364776611 + 0.001 * 8.592802047729492
Epoch 50, val loss: 1.8375352621078491
Epoch 60, training loss: 1.8098664283752441 = 1.8012831211090088 + 0.001 * 8.583356857299805
Epoch 60, val loss: 1.8114752769470215
Epoch 70, training loss: 1.7787419557571411 = 1.7702070474624634 + 0.001 * 8.534944534301758
Epoch 70, val loss: 1.7872042655944824
Epoch 80, training loss: 1.7360912561416626 = 1.727876901626587 + 0.001 * 8.214338302612305
Epoch 80, val loss: 1.7481094598770142
Epoch 90, training loss: 1.6757334470748901 = 1.6676572561264038 + 0.001 * 8.076224327087402
Epoch 90, val loss: 1.6948835849761963
Epoch 100, training loss: 1.5953413248062134 = 1.5873079299926758 + 0.001 * 8.03343677520752
Epoch 100, val loss: 1.6292102336883545
Epoch 110, training loss: 1.5042461156845093 = 1.49624502658844 + 0.001 * 8.001030921936035
Epoch 110, val loss: 1.5557692050933838
Epoch 120, training loss: 1.4117084741592407 = 1.403788447380066 + 0.001 * 7.919969081878662
Epoch 120, val loss: 1.482504963874817
Epoch 130, training loss: 1.3220224380493164 = 1.3142307996749878 + 0.001 * 7.791684627532959
Epoch 130, val loss: 1.4146977663040161
Epoch 140, training loss: 1.2355053424835205 = 1.2277497053146362 + 0.001 * 7.755681037902832
Epoch 140, val loss: 1.3518105745315552
Epoch 150, training loss: 1.1530801057815552 = 1.1454071998596191 + 0.001 * 7.67289924621582
Epoch 150, val loss: 1.2949090003967285
Epoch 160, training loss: 1.0763245820999146 = 1.0687429904937744 + 0.001 * 7.581535816192627
Epoch 160, val loss: 1.2445405721664429
Epoch 170, training loss: 1.0056856870651245 = 0.998138964176178 + 0.001 * 7.546771049499512
Epoch 170, val loss: 1.1994802951812744
Epoch 180, training loss: 0.9402600526809692 = 0.9327256083488464 + 0.001 * 7.534450531005859
Epoch 180, val loss: 1.1588730812072754
Epoch 190, training loss: 0.8779498934745789 = 0.8704280257225037 + 0.001 * 7.52187967300415
Epoch 190, val loss: 1.12067711353302
Epoch 200, training loss: 0.8164746761322021 = 0.8089614510536194 + 0.001 * 7.513212203979492
Epoch 200, val loss: 1.0837799310684204
Epoch 210, training loss: 0.7547900676727295 = 0.7472849488258362 + 0.001 * 7.505120754241943
Epoch 210, val loss: 1.0477502346038818
Epoch 220, training loss: 0.6934937834739685 = 0.6859965920448303 + 0.001 * 7.497184753417969
Epoch 220, val loss: 1.0133243799209595
Epoch 230, training loss: 0.6337520480155945 = 0.6262638568878174 + 0.001 * 7.488193988800049
Epoch 230, val loss: 0.9814140200614929
Epoch 240, training loss: 0.5766323804855347 = 0.5691540837287903 + 0.001 * 7.478302478790283
Epoch 240, val loss: 0.9534887075424194
Epoch 250, training loss: 0.5228254795074463 = 0.5153589844703674 + 0.001 * 7.466492176055908
Epoch 250, val loss: 0.930286169052124
Epoch 260, training loss: 0.47265055775642395 = 0.4651970863342285 + 0.001 * 7.453481674194336
Epoch 260, val loss: 0.9123222827911377
Epoch 270, training loss: 0.42599835991859436 = 0.41856563091278076 + 0.001 * 7.432737350463867
Epoch 270, val loss: 0.899371862411499
Epoch 280, training loss: 0.3825165629386902 = 0.3751075863838196 + 0.001 * 7.40898323059082
Epoch 280, val loss: 0.8910393714904785
Epoch 290, training loss: 0.34181544184684753 = 0.334439754486084 + 0.001 * 7.375676155090332
Epoch 290, val loss: 0.8866745829582214
Epoch 300, training loss: 0.30379876494407654 = 0.29643601179122925 + 0.001 * 7.362763404846191
Epoch 300, val loss: 0.8860529065132141
Epoch 310, training loss: 0.26854926347732544 = 0.2612040340900421 + 0.001 * 7.345231056213379
Epoch 310, val loss: 0.8890030384063721
Epoch 320, training loss: 0.2362624555826187 = 0.22894155979156494 + 0.001 * 7.320895195007324
Epoch 320, val loss: 0.8956242799758911
Epoch 330, training loss: 0.20710498094558716 = 0.1997910439968109 + 0.001 * 7.313934326171875
Epoch 330, val loss: 0.9058375954627991
Epoch 340, training loss: 0.1810930073261261 = 0.17378205060958862 + 0.001 * 7.310950756072998
Epoch 340, val loss: 0.9194619655609131
Epoch 350, training loss: 0.15814049541950226 = 0.15083758533000946 + 0.001 * 7.302906513214111
Epoch 350, val loss: 0.93604975938797
Epoch 360, training loss: 0.13807637989521027 = 0.1307765692472458 + 0.001 * 7.299811840057373
Epoch 360, val loss: 0.9550611972808838
Epoch 370, training loss: 0.1206846758723259 = 0.11337646842002869 + 0.001 * 7.308204174041748
Epoch 370, val loss: 0.975954532623291
Epoch 380, training loss: 0.10568347573280334 = 0.09838921576738358 + 0.001 * 7.294261932373047
Epoch 380, val loss: 0.9981883764266968
Epoch 390, training loss: 0.09284476190805435 = 0.08555004745721817 + 0.001 * 7.294715404510498
Epoch 390, val loss: 1.0211451053619385
Epoch 400, training loss: 0.08187936991453171 = 0.0745866596698761 + 0.001 * 7.2927069664001465
Epoch 400, val loss: 1.0444893836975098
Epoch 410, training loss: 0.07251901924610138 = 0.06523218750953674 + 0.001 * 7.286829471588135
Epoch 410, val loss: 1.067922592163086
Epoch 420, training loss: 0.06454796344041824 = 0.05724768340587616 + 0.001 * 7.300280570983887
Epoch 420, val loss: 1.0913273096084595
Epoch 430, training loss: 0.0577070415019989 = 0.050423070788383484 + 0.001 * 7.283970355987549
Epoch 430, val loss: 1.1145453453063965
Epoch 440, training loss: 0.05186571180820465 = 0.04458294436335564 + 0.001 * 7.282766819000244
Epoch 440, val loss: 1.137481927871704
Epoch 450, training loss: 0.0468602254986763 = 0.03957962989807129 + 0.001 * 7.280593395233154
Epoch 450, val loss: 1.159995675086975
Epoch 460, training loss: 0.04256043955683708 = 0.03528689220547676 + 0.001 * 7.273547172546387
Epoch 460, val loss: 1.1820056438446045
Epoch 470, training loss: 0.038895122706890106 = 0.03159604221582413 + 0.001 * 7.2990803718566895
Epoch 470, val loss: 1.2034236192703247
Epoch 480, training loss: 0.03569250553846359 = 0.02841421402990818 + 0.001 * 7.278292655944824
Epoch 480, val loss: 1.2242097854614258
Epoch 490, training loss: 0.03293604776263237 = 0.025662176311016083 + 0.001 * 7.273870944976807
Epoch 490, val loss: 1.244335412979126
Epoch 500, training loss: 0.030535107478499413 = 0.023272711783647537 + 0.001 * 7.262394905090332
Epoch 500, val loss: 1.2637747526168823
Epoch 510, training loss: 0.028468037024140358 = 0.021190095692873 + 0.001 * 7.27794075012207
Epoch 510, val loss: 1.282545804977417
Epoch 520, training loss: 0.026633216068148613 = 0.019367864355444908 + 0.001 * 7.265350818634033
Epoch 520, val loss: 1.300632119178772
Epoch 530, training loss: 0.025035101920366287 = 0.01776714064180851 + 0.001 * 7.267960071563721
Epoch 530, val loss: 1.3180770874023438
Epoch 540, training loss: 0.02360677160322666 = 0.016355516389012337 + 0.001 * 7.251255035400391
Epoch 540, val loss: 1.334883213043213
Epoch 550, training loss: 0.022381650283932686 = 0.01510582584887743 + 0.001 * 7.275824546813965
Epoch 550, val loss: 1.3510644435882568
Epoch 560, training loss: 0.02124316617846489 = 0.013995189219713211 + 0.001 * 7.247975826263428
Epoch 560, val loss: 1.3666722774505615
Epoch 570, training loss: 0.02026108279824257 = 0.013004404492676258 + 0.001 * 7.256679058074951
Epoch 570, val loss: 1.381729245185852
Epoch 580, training loss: 0.019358325749635696 = 0.012117361649870872 + 0.001 * 7.240963935852051
Epoch 580, val loss: 1.3962479829788208
Epoch 590, training loss: 0.018586192280054092 = 0.011320377700030804 + 0.001 * 7.265814304351807
Epoch 590, val loss: 1.4102643728256226
Epoch 600, training loss: 0.017844747751951218 = 0.01060200110077858 + 0.001 * 7.242746829986572
Epoch 600, val loss: 1.4238100051879883
Epoch 610, training loss: 0.017212072387337685 = 0.009952357970178127 + 0.001 * 7.259713649749756
Epoch 610, val loss: 1.4369031190872192
Epoch 620, training loss: 0.016592372208833694 = 0.009363099001348019 + 0.001 * 7.22927188873291
Epoch 620, val loss: 1.4495505094528198
Epoch 630, training loss: 0.01606505736708641 = 0.008827109821140766 + 0.001 * 7.2379469871521
Epoch 630, val loss: 1.461792230606079
Epoch 640, training loss: 0.015565721318125725 = 0.008338194340467453 + 0.001 * 7.227527141571045
Epoch 640, val loss: 1.4736380577087402
Epoch 650, training loss: 0.01512724719941616 = 0.007891089655458927 + 0.001 * 7.2361578941345215
Epoch 650, val loss: 1.4851231575012207
Epoch 660, training loss: 0.014700865373015404 = 0.007481168024241924 + 0.001 * 7.219696998596191
Epoch 660, val loss: 1.4962577819824219
Epoch 670, training loss: 0.014322707429528236 = 0.007104438729584217 + 0.001 * 7.218267917633057
Epoch 670, val loss: 1.5070574283599854
Epoch 680, training loss: 0.013968972489237785 = 0.006757456809282303 + 0.001 * 7.2115159034729
Epoch 680, val loss: 1.5175464153289795
Epoch 690, training loss: 0.01365474984049797 = 0.00643711956217885 + 0.001 * 7.217630386352539
Epoch 690, val loss: 1.5277173519134521
Epoch 700, training loss: 0.01335228979587555 = 0.0061408113688230515 + 0.001 * 7.211478233337402
Epoch 700, val loss: 1.5376207828521729
Epoch 710, training loss: 0.013077044859528542 = 0.00586614990606904 + 0.001 * 7.2108941078186035
Epoch 710, val loss: 1.5472334623336792
Epoch 720, training loss: 0.012824155390262604 = 0.005611097440123558 + 0.001 * 7.213057041168213
Epoch 720, val loss: 1.5565712451934814
Epoch 730, training loss: 0.01258503831923008 = 0.005373853258788586 + 0.001 * 7.211184024810791
Epoch 730, val loss: 1.5656602382659912
Epoch 740, training loss: 0.012359688058495522 = 0.0051527782343328 + 0.001 * 7.206910133361816
Epoch 740, val loss: 1.5745073556900024
Epoch 750, training loss: 0.012139490805566311 = 0.004946408327668905 + 0.001 * 7.193082332611084
Epoch 750, val loss: 1.5831184387207031
Epoch 760, training loss: 0.011943258345127106 = 0.004753493703901768 + 0.001 * 7.189764499664307
Epoch 760, val loss: 1.5915013551712036
Epoch 770, training loss: 0.011767564341425896 = 0.0045728436671197414 + 0.001 * 7.19472074508667
Epoch 770, val loss: 1.5996956825256348
Epoch 780, training loss: 0.0115886852145195 = 0.004403464961796999 + 0.001 * 7.1852192878723145
Epoch 780, val loss: 1.6076714992523193
Epoch 790, training loss: 0.011432589963078499 = 0.004244385287165642 + 0.001 * 7.188203811645508
Epoch 790, val loss: 1.6154447793960571
Epoch 800, training loss: 0.011286040768027306 = 0.004094887990504503 + 0.001 * 7.191153049468994
Epoch 800, val loss: 1.62302827835083
Epoch 810, training loss: 0.011139001697301865 = 0.003953986801207066 + 0.001 * 7.185014247894287
Epoch 810, val loss: 1.6304428577423096
Epoch 820, training loss: 0.01100563257932663 = 0.003821351332589984 + 0.001 * 7.184280872344971
Epoch 820, val loss: 1.637686848640442
Epoch 830, training loss: 0.010886591859161854 = 0.003695879364386201 + 0.001 * 7.1907124519348145
Epoch 830, val loss: 1.644769549369812
Epoch 840, training loss: 0.010742727667093277 = 0.0035774335265159607 + 0.001 * 7.165294170379639
Epoch 840, val loss: 1.6516873836517334
Epoch 850, training loss: 0.010629551485180855 = 0.0034654003102332354 + 0.001 * 7.164151191711426
Epoch 850, val loss: 1.6584382057189941
Epoch 860, training loss: 0.010534968227148056 = 0.003359225345775485 + 0.001 * 7.175742149353027
Epoch 860, val loss: 1.6650397777557373
Epoch 870, training loss: 0.010451678186655045 = 0.0032586061861366034 + 0.001 * 7.193071365356445
Epoch 870, val loss: 1.6715264320373535
Epoch 880, training loss: 0.01031643059104681 = 0.0031631209421902895 + 0.001 * 7.153309345245361
Epoch 880, val loss: 1.6778662204742432
Epoch 890, training loss: 0.01022840291261673 = 0.003072451101616025 + 0.001 * 7.155951023101807
Epoch 890, val loss: 1.68405020236969
Epoch 900, training loss: 0.010143108665943146 = 0.0029862693045288324 + 0.001 * 7.156838893890381
Epoch 900, val loss: 1.690110445022583
Epoch 910, training loss: 0.010056470520794392 = 0.002904236549511552 + 0.001 * 7.152234077453613
Epoch 910, val loss: 1.696073055267334
Epoch 920, training loss: 0.009999665431678295 = 0.0028260552790015936 + 0.001 * 7.173610210418701
Epoch 920, val loss: 1.7018972635269165
Epoch 930, training loss: 0.009895811788737774 = 0.002751496620476246 + 0.001 * 7.144314765930176
Epoch 930, val loss: 1.707607388496399
Epoch 940, training loss: 0.009837539866566658 = 0.002680394100025296 + 0.001 * 7.1571455001831055
Epoch 940, val loss: 1.713212251663208
Epoch 950, training loss: 0.009752320125699043 = 0.0026125600561499596 + 0.001 * 7.139759540557861
Epoch 950, val loss: 1.7187000513076782
Epoch 960, training loss: 0.009686311706900597 = 0.0025476673617959023 + 0.001 * 7.138644218444824
Epoch 960, val loss: 1.7240713834762573
Epoch 970, training loss: 0.009611489251255989 = 0.0024855893570929766 + 0.001 * 7.125899791717529
Epoch 970, val loss: 1.729332685470581
Epoch 980, training loss: 0.009552493691444397 = 0.0024262922815978527 + 0.001 * 7.126201629638672
Epoch 980, val loss: 1.7345181703567505
Epoch 990, training loss: 0.00951027125120163 = 0.0023694338742643595 + 0.001 * 7.140836715698242
Epoch 990, val loss: 1.7395901679992676
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8297311544544017
=== training gcn model ===
Epoch 0, training loss: 1.9599207639694214 = 1.9513239860534668 + 0.001 * 8.596831321716309
Epoch 0, val loss: 1.9528381824493408
Epoch 10, training loss: 1.949230670928955 = 1.9406338930130005 + 0.001 * 8.596789360046387
Epoch 10, val loss: 1.9417957067489624
Epoch 20, training loss: 1.936361312866211 = 1.927764654159546 + 0.001 * 8.59666919708252
Epoch 20, val loss: 1.9283850193023682
Epoch 30, training loss: 1.9188663959503174 = 1.9102699756622314 + 0.001 * 8.596405029296875
Epoch 30, val loss: 1.9100866317749023
Epoch 40, training loss: 1.8937432765960693 = 1.8851474523544312 + 0.001 * 8.59579086303711
Epoch 40, val loss: 1.8840913772583008
Epoch 50, training loss: 1.859204888343811 = 1.850610613822937 + 0.001 * 8.594246864318848
Epoch 50, val loss: 1.8496792316436768
Epoch 60, training loss: 1.8211925029754639 = 1.8126029968261719 + 0.001 * 8.589468955993652
Epoch 60, val loss: 1.815915584564209
Epoch 70, training loss: 1.7917031049728394 = 1.7831331491470337 + 0.001 * 8.569992065429688
Epoch 70, val loss: 1.7941007614135742
Epoch 80, training loss: 1.7578785419464111 = 1.7494479417800903 + 0.001 * 8.430558204650879
Epoch 80, val loss: 1.7665666341781616
Epoch 90, training loss: 1.7097524404525757 = 1.701550841331482 + 0.001 * 8.201546669006348
Epoch 90, val loss: 1.7257983684539795
Epoch 100, training loss: 1.6415801048278809 = 1.6335192918777466 + 0.001 * 8.060808181762695
Epoch 100, val loss: 1.668392300605774
Epoch 110, training loss: 1.554324984550476 = 1.5464468002319336 + 0.001 * 7.878206253051758
Epoch 110, val loss: 1.5964319705963135
Epoch 120, training loss: 1.4572036266326904 = 1.4494551420211792 + 0.001 * 7.748488903045654
Epoch 120, val loss: 1.5181764364242554
Epoch 130, training loss: 1.359069585800171 = 1.3513078689575195 + 0.001 * 7.761773586273193
Epoch 130, val loss: 1.4389572143554688
Epoch 140, training loss: 1.2596664428710938 = 1.2519185543060303 + 0.001 * 7.747934818267822
Epoch 140, val loss: 1.3610975742340088
Epoch 150, training loss: 1.1578007936477661 = 1.150057077407837 + 0.001 * 7.743677616119385
Epoch 150, val loss: 1.2830026149749756
Epoch 160, training loss: 1.0561785697937012 = 1.0484437942504883 + 0.001 * 7.734808921813965
Epoch 160, val loss: 1.2075718641281128
Epoch 170, training loss: 0.9598497152328491 = 0.952129602432251 + 0.001 * 7.720127582550049
Epoch 170, val loss: 1.138025164604187
Epoch 180, training loss: 0.8731914758682251 = 0.8654969930648804 + 0.001 * 7.694455623626709
Epoch 180, val loss: 1.0777488946914673
Epoch 190, training loss: 0.7982579469680786 = 0.7906100153923035 + 0.001 * 7.647952556610107
Epoch 190, val loss: 1.028494954109192
Epoch 200, training loss: 0.7345116138458252 = 0.7269366383552551 + 0.001 * 7.5749616622924805
Epoch 200, val loss: 0.9904565811157227
Epoch 210, training loss: 0.6795493960380554 = 0.6720144152641296 + 0.001 * 7.534996032714844
Epoch 210, val loss: 0.9622333645820618
Epoch 220, training loss: 0.6304018497467041 = 0.6228929758071899 + 0.001 * 7.508882522583008
Epoch 220, val loss: 0.9412957429885864
Epoch 230, training loss: 0.58460533618927 = 0.5771057605743408 + 0.001 * 7.499555587768555
Epoch 230, val loss: 0.9247294068336487
Epoch 240, training loss: 0.5403926968574524 = 0.532904863357544 + 0.001 * 7.487861156463623
Epoch 240, val loss: 0.9109624624252319
Epoch 250, training loss: 0.49673041701316833 = 0.48925691843032837 + 0.001 * 7.473492622375488
Epoch 250, val loss: 0.8996173739433289
Epoch 260, training loss: 0.4531404674053192 = 0.44568195939064026 + 0.001 * 7.458498001098633
Epoch 260, val loss: 0.8910406827926636
Epoch 270, training loss: 0.4095061421394348 = 0.40206944942474365 + 0.001 * 7.436705112457275
Epoch 270, val loss: 0.8857002258300781
Epoch 280, training loss: 0.3662528395652771 = 0.3588356673717499 + 0.001 * 7.417157173156738
Epoch 280, val loss: 0.8842170834541321
Epoch 290, training loss: 0.32438749074935913 = 0.3169863522052765 + 0.001 * 7.401147842407227
Epoch 290, val loss: 0.8872752785682678
Epoch 300, training loss: 0.2850498557090759 = 0.27766478061676025 + 0.001 * 7.385061264038086
Epoch 300, val loss: 0.895626962184906
Epoch 310, training loss: 0.2492554634809494 = 0.2418547421693802 + 0.001 * 7.400727272033691
Epoch 310, val loss: 0.9098675847053528
Epoch 320, training loss: 0.2174994796514511 = 0.21011726558208466 + 0.001 * 7.382210731506348
Epoch 320, val loss: 0.9298986196517944
Epoch 330, training loss: 0.1898323893547058 = 0.1824716478586197 + 0.001 * 7.360738754272461
Epoch 330, val loss: 0.9548506140708923
Epoch 340, training loss: 0.16589567065238953 = 0.15853460133075714 + 0.001 * 7.361063480377197
Epoch 340, val loss: 0.9834840893745422
Epoch 350, training loss: 0.14516271650791168 = 0.13782477378845215 + 0.001 * 7.337941646575928
Epoch 350, val loss: 1.014795184135437
Epoch 360, training loss: 0.12725438177585602 = 0.11992505937814713 + 0.001 * 7.329322338104248
Epoch 360, val loss: 1.0478233098983765
Epoch 370, training loss: 0.11181239783763885 = 0.10448131710290909 + 0.001 * 7.331077575683594
Epoch 370, val loss: 1.0818012952804565
Epoch 380, training loss: 0.09852014482021332 = 0.09120158851146698 + 0.001 * 7.318556785583496
Epoch 380, val loss: 1.1161671876907349
Epoch 390, training loss: 0.0871356874704361 = 0.07981962710618973 + 0.001 * 7.316059112548828
Epoch 390, val loss: 1.1505112648010254
Epoch 400, training loss: 0.07740767300128937 = 0.07008713483810425 + 0.001 * 7.320540904998779
Epoch 400, val loss: 1.1846637725830078
Epoch 410, training loss: 0.06908443570137024 = 0.061769988387823105 + 0.001 * 7.314446449279785
Epoch 410, val loss: 1.2184494733810425
Epoch 420, training loss: 0.061964813619852066 = 0.05465371534228325 + 0.001 * 7.311099052429199
Epoch 420, val loss: 1.2516305446624756
Epoch 430, training loss: 0.05584315210580826 = 0.048547085374593735 + 0.001 * 7.296065807342529
Epoch 430, val loss: 1.2841567993164062
Epoch 440, training loss: 0.05058044195175171 = 0.04329117387533188 + 0.001 * 7.289267063140869
Epoch 440, val loss: 1.3159857988357544
Epoch 450, training loss: 0.046039093285799026 = 0.03875543177127838 + 0.001 * 7.283660888671875
Epoch 450, val loss: 1.3470121622085571
Epoch 460, training loss: 0.04210065305233002 = 0.034831319004297256 + 0.001 * 7.26933479309082
Epoch 460, val loss: 1.3771692514419556
Epoch 470, training loss: 0.03870908170938492 = 0.0314275287091732 + 0.001 * 7.281554222106934
Epoch 470, val loss: 1.406429409980774
Epoch 480, training loss: 0.03573019802570343 = 0.028466032817959785 + 0.001 * 7.264166355133057
Epoch 480, val loss: 1.4347188472747803
Epoch 490, training loss: 0.033128999173641205 = 0.0258810855448246 + 0.001 * 7.247913837432861
Epoch 490, val loss: 1.4620610475540161
Epoch 500, training loss: 0.03086959384381771 = 0.023617465049028397 + 0.001 * 7.2521281242370605
Epoch 500, val loss: 1.4884545803070068
Epoch 510, training loss: 0.028887305408716202 = 0.02162836864590645 + 0.001 * 7.258935451507568
Epoch 510, val loss: 1.5139509439468384
Epoch 520, training loss: 0.02712472714483738 = 0.01987367495894432 + 0.001 * 7.251051425933838
Epoch 520, val loss: 1.5385072231292725
Epoch 530, training loss: 0.02556902915239334 = 0.018320420756936073 + 0.001 * 7.2486090660095215
Epoch 530, val loss: 1.5622260570526123
Epoch 540, training loss: 0.024173647165298462 = 0.016940666362643242 + 0.001 * 7.2329792976379395
Epoch 540, val loss: 1.5850822925567627
Epoch 550, training loss: 0.022939050570130348 = 0.015710560604929924 + 0.001 * 7.228489398956299
Epoch 550, val loss: 1.6071046590805054
Epoch 560, training loss: 0.0218571275472641 = 0.014610315673053265 + 0.001 * 7.246811866760254
Epoch 560, val loss: 1.6283680200576782
Epoch 570, training loss: 0.020842252299189568 = 0.013622959144413471 + 0.001 * 7.219293117523193
Epoch 570, val loss: 1.6488773822784424
Epoch 580, training loss: 0.019962847232818604 = 0.012733829207718372 + 0.001 * 7.229018688201904
Epoch 580, val loss: 1.668678641319275
Epoch 590, training loss: 0.01917251944541931 = 0.011930804699659348 + 0.001 * 7.241714000701904
Epoch 590, val loss: 1.687785267829895
Epoch 600, training loss: 0.018430553376674652 = 0.011203460395336151 + 0.001 * 7.227092266082764
Epoch 600, val loss: 1.7062472105026245
Epoch 610, training loss: 0.017789026722311974 = 0.010542592965066433 + 0.001 * 7.246432781219482
Epoch 610, val loss: 1.7241055965423584
Epoch 620, training loss: 0.017167262732982635 = 0.00994077417999506 + 0.001 * 7.22648811340332
Epoch 620, val loss: 1.7413932085037231
Epoch 630, training loss: 0.016600754112005234 = 0.009391121566295624 + 0.001 * 7.20963191986084
Epoch 630, val loss: 1.7581186294555664
Epoch 640, training loss: 0.01611717976629734 = 0.00888784322887659 + 0.001 * 7.229336738586426
Epoch 640, val loss: 1.774319052696228
Epoch 650, training loss: 0.015627754852175713 = 0.008426002226769924 + 0.001 * 7.201751708984375
Epoch 650, val loss: 1.7900025844573975
Epoch 660, training loss: 0.015187335200607777 = 0.008001214824616909 + 0.001 * 7.18612003326416
Epoch 660, val loss: 1.8052257299423218
Epoch 670, training loss: 0.01481080986559391 = 0.00760976318269968 + 0.001 * 7.201046466827393
Epoch 670, val loss: 1.8199853897094727
Epoch 680, training loss: 0.014434197917580605 = 0.007248170208185911 + 0.001 * 7.1860270500183105
Epoch 680, val loss: 1.8342748880386353
Epoch 690, training loss: 0.014098510146141052 = 0.0069135632365942 + 0.001 * 7.184946537017822
Epoch 690, val loss: 1.848201870918274
Epoch 700, training loss: 0.013790633529424667 = 0.0066032917238771915 + 0.001 * 7.187341213226318
Epoch 700, val loss: 1.8617017269134521
Epoch 710, training loss: 0.013498680666089058 = 0.006315065082162619 + 0.001 * 7.183615207672119
Epoch 710, val loss: 1.8748042583465576
Epoch 720, training loss: 0.01322629302740097 = 0.006046909373253584 + 0.001 * 7.179382801055908
Epoch 720, val loss: 1.887566328048706
Epoch 730, training loss: 0.012974675744771957 = 0.005796929355710745 + 0.001 * 7.177746295928955
Epoch 730, val loss: 1.8999545574188232
Epoch 740, training loss: 0.012745403684675694 = 0.005563540384173393 + 0.001 * 7.181862831115723
Epoch 740, val loss: 1.912022352218628
Epoch 750, training loss: 0.01250516064465046 = 0.005345309618860483 + 0.001 * 7.159850120544434
Epoch 750, val loss: 1.9237585067749023
Epoch 760, training loss: 0.012326860800385475 = 0.005140957422554493 + 0.001 * 7.185903072357178
Epoch 760, val loss: 1.9351907968521118
Epoch 770, training loss: 0.012151860632002354 = 0.00494929077103734 + 0.001 * 7.202569484710693
Epoch 770, val loss: 1.9463112354278564
Epoch 780, training loss: 0.011950677260756493 = 0.0047693997621536255 + 0.001 * 7.181277751922607
Epoch 780, val loss: 1.9571490287780762
Epoch 790, training loss: 0.011776156723499298 = 0.004600204061716795 + 0.001 * 7.175951957702637
Epoch 790, val loss: 1.9677108526229858
Epoch 800, training loss: 0.011608792468905449 = 0.0044409651309251785 + 0.001 * 7.167827129364014
Epoch 800, val loss: 1.978010892868042
Epoch 810, training loss: 0.011458737775683403 = 0.004290861077606678 + 0.001 * 7.16787576675415
Epoch 810, val loss: 1.9880563020706177
Epoch 820, training loss: 0.011289822869002819 = 0.0041491989977657795 + 0.001 * 7.140623569488525
Epoch 820, val loss: 1.997869849205017
Epoch 830, training loss: 0.01116439513862133 = 0.004015389829874039 + 0.001 * 7.149004936218262
Epoch 830, val loss: 2.00744366645813
Epoch 840, training loss: 0.011051702313125134 = 0.0038888731505721807 + 0.001 * 7.1628289222717285
Epoch 840, val loss: 2.0168025493621826
Epoch 850, training loss: 0.010896530002355576 = 0.0037691385950893164 + 0.001 * 7.1273908615112305
Epoch 850, val loss: 2.0259203910827637
Epoch 860, training loss: 0.010785303078591824 = 0.003655669977888465 + 0.001 * 7.129632949829102
Epoch 860, val loss: 2.0348620414733887
Epoch 870, training loss: 0.010700392536818981 = 0.003548082197085023 + 0.001 * 7.152309894561768
Epoch 870, val loss: 2.0435495376586914
Epoch 880, training loss: 0.010579398833215237 = 0.0034459340386092663 + 0.001 * 7.133464336395264
Epoch 880, val loss: 2.0520613193511963
Epoch 890, training loss: 0.01051273848861456 = 0.0033488457556813955 + 0.001 * 7.1638922691345215
Epoch 890, val loss: 2.0603950023651123
Epoch 900, training loss: 0.01039718184620142 = 0.0032565107103437185 + 0.001 * 7.140671253204346
Epoch 900, val loss: 2.0685057640075684
Epoch 910, training loss: 0.010298091918230057 = 0.0031686420552432537 + 0.001 * 7.129449844360352
Epoch 910, val loss: 2.0764620304107666
Epoch 920, training loss: 0.01021924614906311 = 0.0030849536415189505 + 0.001 * 7.134292125701904
Epoch 920, val loss: 2.0842459201812744
Epoch 930, training loss: 0.010157234966754913 = 0.0030051665380597115 + 0.001 * 7.152068138122559
Epoch 930, val loss: 2.091843843460083
Epoch 940, training loss: 0.010072178207337856 = 0.002929076785221696 + 0.001 * 7.143100738525391
Epoch 940, val loss: 2.0992612838745117
Epoch 950, training loss: 0.009983083233237267 = 0.002856430597603321 + 0.001 * 7.126652240753174
Epoch 950, val loss: 2.106560230255127
Epoch 960, training loss: 0.009908046573400497 = 0.002787018893286586 + 0.001 * 7.12102746963501
Epoch 960, val loss: 2.1136629581451416
Epoch 970, training loss: 0.00982710625976324 = 0.0027206602972000837 + 0.001 * 7.1064453125
Epoch 970, val loss: 2.1206436157226562
Epoch 980, training loss: 0.00976501777768135 = 0.002657172968611121 + 0.001 * 7.107844352722168
Epoch 980, val loss: 2.1274609565734863
Epoch 990, training loss: 0.009691573679447174 = 0.002596392761915922 + 0.001 * 7.095180988311768
Epoch 990, val loss: 2.1341328620910645
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8207696362677913
The final CL Acc:0.77531, 0.01364, The final GNN Acc:0.82446, 0.00383
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13164])
remove edge: torch.Size([2, 7906])
updated graph: torch.Size([2, 10514])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9441357851028442 = 1.9355390071868896 + 0.001 * 8.596809387207031
Epoch 0, val loss: 1.9413686990737915
Epoch 10, training loss: 1.934279203414917 = 1.9256824254989624 + 0.001 * 8.596759796142578
Epoch 10, val loss: 1.931014895439148
Epoch 20, training loss: 1.9224830865859985 = 1.913886547088623 + 0.001 * 8.596582412719727
Epoch 20, val loss: 1.918568730354309
Epoch 30, training loss: 1.9065440893173218 = 1.897947907447815 + 0.001 * 8.596192359924316
Epoch 30, val loss: 1.9018023014068604
Epoch 40, training loss: 1.883541226387024 = 1.8749459981918335 + 0.001 * 8.595267295837402
Epoch 40, val loss: 1.8779230117797852
Epoch 50, training loss: 1.8512731790542603 = 1.8426804542541504 + 0.001 * 8.592702865600586
Epoch 50, val loss: 1.845441460609436
Epoch 60, training loss: 1.8127351999282837 = 1.8041515350341797 + 0.001 * 8.58362865447998
Epoch 60, val loss: 1.809511661529541
Epoch 70, training loss: 1.775892972946167 = 1.7673523426055908 + 0.001 * 8.540616035461426
Epoch 70, val loss: 1.7777882814407349
Epoch 80, training loss: 1.7314566373825073 = 1.7231980562210083 + 0.001 * 8.25863265991211
Epoch 80, val loss: 1.7375577688217163
Epoch 90, training loss: 1.6684880256652832 = 1.6604644060134888 + 0.001 * 8.023660659790039
Epoch 90, val loss: 1.6806683540344238
Epoch 100, training loss: 1.5845935344696045 = 1.5766853094100952 + 0.001 * 7.908275604248047
Epoch 100, val loss: 1.608088731765747
Epoch 110, training loss: 1.4857538938522339 = 1.4779791831970215 + 0.001 * 7.774691581726074
Epoch 110, val loss: 1.525627851486206
Epoch 120, training loss: 1.382416844367981 = 1.374724268913269 + 0.001 * 7.6925368309021
Epoch 120, val loss: 1.4407496452331543
Epoch 130, training loss: 1.2799845933914185 = 1.2723183631896973 + 0.001 * 7.666227340698242
Epoch 130, val loss: 1.3578073978424072
Epoch 140, training loss: 1.180046796798706 = 1.1724424362182617 + 0.001 * 7.604391098022461
Epoch 140, val loss: 1.2775431871414185
Epoch 150, training loss: 1.0857888460159302 = 1.078282356262207 + 0.001 * 7.50645637512207
Epoch 150, val loss: 1.2024000883102417
Epoch 160, training loss: 1.0003265142440796 = 0.9929225444793701 + 0.001 * 7.404017448425293
Epoch 160, val loss: 1.1350644826889038
Epoch 170, training loss: 0.9234698414802551 = 0.91611248254776 + 0.001 * 7.357385635375977
Epoch 170, val loss: 1.075438141822815
Epoch 180, training loss: 0.8519277572631836 = 0.8446162343025208 + 0.001 * 7.311547756195068
Epoch 180, val loss: 1.0203076601028442
Epoch 190, training loss: 0.782576858997345 = 0.775305986404419 + 0.001 * 7.270859718322754
Epoch 190, val loss: 0.9670572876930237
Epoch 200, training loss: 0.7138503789901733 = 0.7065936326980591 + 0.001 * 7.256727695465088
Epoch 200, val loss: 0.9147779941558838
Epoch 210, training loss: 0.645696222782135 = 0.6384429931640625 + 0.001 * 7.253213882446289
Epoch 210, val loss: 0.8645114898681641
Epoch 220, training loss: 0.5790793299674988 = 0.5718292593955994 + 0.001 * 7.250059127807617
Epoch 220, val loss: 0.8181654214859009
Epoch 230, training loss: 0.5153248906135559 = 0.5080780982971191 + 0.001 * 7.246766567230225
Epoch 230, val loss: 0.7768533825874329
Epoch 240, training loss: 0.4557890295982361 = 0.448544442653656 + 0.001 * 7.2445759773254395
Epoch 240, val loss: 0.7413129210472107
Epoch 250, training loss: 0.4013538956642151 = 0.39411088824272156 + 0.001 * 7.243004322052002
Epoch 250, val loss: 0.7118945717811584
Epoch 260, training loss: 0.3522956669330597 = 0.3450535833835602 + 0.001 * 7.242077827453613
Epoch 260, val loss: 0.688692033290863
Epoch 270, training loss: 0.30847421288490295 = 0.3012325167655945 + 0.001 * 7.241706371307373
Epoch 270, val loss: 0.6713472008705139
Epoch 280, training loss: 0.2695288062095642 = 0.2622869908809662 + 0.001 * 7.241819381713867
Epoch 280, val loss: 0.6593109369277954
Epoch 290, training loss: 0.2350509613752365 = 0.22780859470367432 + 0.001 * 7.242360591888428
Epoch 290, val loss: 0.651528537273407
Epoch 300, training loss: 0.204708993434906 = 0.1974656879901886 + 0.001 * 7.243310451507568
Epoch 300, val loss: 0.64742511510849
Epoch 310, training loss: 0.17820827662944794 = 0.17096365988254547 + 0.001 * 7.244612216949463
Epoch 310, val loss: 0.6462697982788086
Epoch 320, training loss: 0.15525193512439728 = 0.14800578355789185 + 0.001 * 7.246151924133301
Epoch 320, val loss: 0.6477036476135254
Epoch 330, training loss: 0.13549911975860596 = 0.12825129926204681 + 0.001 * 7.247827529907227
Epoch 330, val loss: 0.6512659788131714
Epoch 340, training loss: 0.1185745969414711 = 0.11132505536079407 + 0.001 * 7.249542713165283
Epoch 340, val loss: 0.6564481854438782
Epoch 350, training loss: 0.10411112755537033 = 0.09685991704463959 + 0.001 * 7.251209259033203
Epoch 350, val loss: 0.6629173755645752
Epoch 360, training loss: 0.09176573157310486 = 0.08451205492019653 + 0.001 * 7.2536725997924805
Epoch 360, val loss: 0.6704264879226685
Epoch 370, training loss: 0.08123046159744263 = 0.07397592812776566 + 0.001 * 7.254530906677246
Epoch 370, val loss: 0.6787218451499939
Epoch 380, training loss: 0.07222860306501389 = 0.06497283279895782 + 0.001 * 7.255772590637207
Epoch 380, val loss: 0.6875980496406555
Epoch 390, training loss: 0.06452170014381409 = 0.057264894247055054 + 0.001 * 7.25680685043335
Epoch 390, val loss: 0.6969369053840637
Epoch 400, training loss: 0.05791860446333885 = 0.0506608821451664 + 0.001 * 7.2577223777771
Epoch 400, val loss: 0.7065860033035278
Epoch 410, training loss: 0.05225634574890137 = 0.04499823600053787 + 0.001 * 7.258111000061035
Epoch 410, val loss: 0.7164531350135803
Epoch 420, training loss: 0.047389522194862366 = 0.04013090580701828 + 0.001 * 7.258614540100098
Epoch 420, val loss: 0.7264317870140076
Epoch 430, training loss: 0.04318760707974434 = 0.0359291136264801 + 0.001 * 7.2584919929504395
Epoch 430, val loss: 0.7365415692329407
Epoch 440, training loss: 0.03954754397273064 = 0.032289400696754456 + 0.001 * 7.258142471313477
Epoch 440, val loss: 0.7467057108879089
Epoch 450, training loss: 0.036384645849466324 = 0.029127046465873718 + 0.001 * 7.25759744644165
Epoch 450, val loss: 0.7568710446357727
Epoch 460, training loss: 0.03362937271595001 = 0.026373011991381645 + 0.001 * 7.25636100769043
Epoch 460, val loss: 0.7669172883033752
Epoch 470, training loss: 0.03122306615114212 = 0.023968158289790154 + 0.001 * 7.254908561706543
Epoch 470, val loss: 0.7768357992172241
Epoch 480, training loss: 0.029116909950971603 = 0.021861793473362923 + 0.001 * 7.2551164627075195
Epoch 480, val loss: 0.7865889668464661
Epoch 490, training loss: 0.027261435985565186 = 0.02001090906560421 + 0.001 * 7.25052547454834
Epoch 490, val loss: 0.7961534857749939
Epoch 500, training loss: 0.025632638484239578 = 0.01837914064526558 + 0.001 * 7.2534966468811035
Epoch 500, val loss: 0.805523693561554
Epoch 510, training loss: 0.02418096736073494 = 0.0169355645775795 + 0.001 * 7.245401382446289
Epoch 510, val loss: 0.8146666288375854
Epoch 520, training loss: 0.022909725084900856 = 0.015654025599360466 + 0.001 * 7.255699157714844
Epoch 520, val loss: 0.8235716223716736
Epoch 530, training loss: 0.021741624921560287 = 0.014512347988784313 + 0.001 * 7.229275703430176
Epoch 530, val loss: 0.8322334885597229
Epoch 540, training loss: 0.02072528563439846 = 0.013491802848875523 + 0.001 * 7.233482837677002
Epoch 540, val loss: 0.8406540751457214
Epoch 550, training loss: 0.01980511285364628 = 0.012576579116284847 + 0.001 * 7.228532791137695
Epoch 550, val loss: 0.8488340377807617
Epoch 560, training loss: 0.018960896879434586 = 0.01175302267074585 + 0.001 * 7.2078728675842285
Epoch 560, val loss: 0.8567839860916138
Epoch 570, training loss: 0.0182355847209692 = 0.011009622365236282 + 0.001 * 7.225962162017822
Epoch 570, val loss: 0.8645175099372864
Epoch 580, training loss: 0.017558932304382324 = 0.010336566716432571 + 0.001 * 7.222364902496338
Epoch 580, val loss: 0.8720347285270691
Epoch 590, training loss: 0.016923334449529648 = 0.009725452400743961 + 0.001 * 7.19788122177124
Epoch 590, val loss: 0.8793431520462036
Epoch 600, training loss: 0.016353540122509003 = 0.009169033728539944 + 0.001 * 7.184505462646484
Epoch 600, val loss: 0.8864601850509644
Epoch 610, training loss: 0.01585150882601738 = 0.008661157451570034 + 0.001 * 7.190351486206055
Epoch 610, val loss: 0.8933634161949158
Epoch 620, training loss: 0.015394419431686401 = 0.008196450769901276 + 0.001 * 7.197968482971191
Epoch 620, val loss: 0.9000826478004456
Epoch 630, training loss: 0.014945654198527336 = 0.007770158350467682 + 0.001 * 7.1754961013793945
Epoch 630, val loss: 0.9066269993782043
Epoch 640, training loss: 0.014549070969223976 = 0.00737808970734477 + 0.001 * 7.170981407165527
Epoch 640, val loss: 0.9130005240440369
Epoch 650, training loss: 0.014199519529938698 = 0.007016666699200869 + 0.001 * 7.182852745056152
Epoch 650, val loss: 0.9192110896110535
Epoch 660, training loss: 0.013862872496247292 = 0.006682930514216423 + 0.001 * 7.1799421310424805
Epoch 660, val loss: 0.9252610802650452
Epoch 670, training loss: 0.013533501885831356 = 0.00637409370392561 + 0.001 * 7.159407615661621
Epoch 670, val loss: 0.9311587810516357
Epoch 680, training loss: 0.013243788853287697 = 0.006087745074182749 + 0.001 * 7.156043529510498
Epoch 680, val loss: 0.9369133710861206
Epoch 690, training loss: 0.012989089824259281 = 0.005821768660098314 + 0.001 * 7.167320728302002
Epoch 690, val loss: 0.9425274133682251
Epoch 700, training loss: 0.012731272727251053 = 0.005574297159910202 + 0.001 * 7.156975269317627
Epoch 700, val loss: 0.9480116963386536
Epoch 710, training loss: 0.012501796707510948 = 0.005343619734048843 + 0.001 * 7.158176422119141
Epoch 710, val loss: 0.9533580541610718
Epoch 720, training loss: 0.012277007102966309 = 0.005128233693540096 + 0.001 * 7.148772716522217
Epoch 720, val loss: 0.9585627913475037
Epoch 730, training loss: 0.012085694819688797 = 0.004926862195134163 + 0.001 * 7.15883207321167
Epoch 730, val loss: 0.9636664390563965
Epoch 740, training loss: 0.011887626722455025 = 0.004738300573080778 + 0.001 * 7.149325847625732
Epoch 740, val loss: 0.9686525464057922
Epoch 750, training loss: 0.011716742068529129 = 0.004561493173241615 + 0.001 * 7.155248641967773
Epoch 750, val loss: 0.9735167622566223
Epoch 760, training loss: 0.011536026373505592 = 0.004395443480461836 + 0.001 * 7.140583038330078
Epoch 760, val loss: 0.9782764315605164
Epoch 770, training loss: 0.011378217488527298 = 0.004239305388182402 + 0.001 * 7.138911247253418
Epoch 770, val loss: 0.9829352498054504
Epoch 780, training loss: 0.01122998259961605 = 0.004092311020940542 + 0.001 * 7.13767147064209
Epoch 780, val loss: 0.98748779296875
Epoch 790, training loss: 0.011090226471424103 = 0.003953793551772833 + 0.001 * 7.136433124542236
Epoch 790, val loss: 0.9919478893280029
Epoch 800, training loss: 0.010973544791340828 = 0.0038230970967561007 + 0.001 * 7.150447368621826
Epoch 800, val loss: 0.996299147605896
Epoch 810, training loss: 0.01083118375390768 = 0.003699628869071603 + 0.001 * 7.13155460357666
Epoch 810, val loss: 1.00058114528656
Epoch 820, training loss: 0.010712901130318642 = 0.0035828817635774612 + 0.001 * 7.130019187927246
Epoch 820, val loss: 1.0047422647476196
Epoch 830, training loss: 0.010598607361316681 = 0.0034723884891718626 + 0.001 * 7.126218795776367
Epoch 830, val loss: 1.008832573890686
Epoch 840, training loss: 0.010491224937140942 = 0.003367648459970951 + 0.001 * 7.1235761642456055
Epoch 840, val loss: 1.0128374099731445
Epoch 850, training loss: 0.010396979749202728 = 0.0032683005556464195 + 0.001 * 7.128678798675537
Epoch 850, val loss: 1.0167474746704102
Epoch 860, training loss: 0.010283024050295353 = 0.0031739589758217335 + 0.001 * 7.10906457901001
Epoch 860, val loss: 1.0205903053283691
Epoch 870, training loss: 0.010203417390584946 = 0.003084341296926141 + 0.001 * 7.119075775146484
Epoch 870, val loss: 1.0243536233901978
Epoch 880, training loss: 0.01010884065181017 = 0.002999109448865056 + 0.001 * 7.109731197357178
Epoch 880, val loss: 1.0280227661132812
Epoch 890, training loss: 0.010024520568549633 = 0.002918004058301449 + 0.001 * 7.106516361236572
Epoch 890, val loss: 1.0316460132598877
Epoch 900, training loss: 0.00994076393544674 = 0.0028407482896000147 + 0.001 * 7.100015163421631
Epoch 900, val loss: 1.035186529159546
Epoch 910, training loss: 0.009877189062535763 = 0.0027671013958752155 + 0.001 * 7.1100873947143555
Epoch 910, val loss: 1.038644790649414
Epoch 920, training loss: 0.009799336083233356 = 0.0026968501042574644 + 0.001 * 7.102485656738281
Epoch 920, val loss: 1.0420503616333008
Epoch 930, training loss: 0.009721970185637474 = 0.002629778580740094 + 0.001 * 7.092191219329834
Epoch 930, val loss: 1.0453935861587524
Epoch 940, training loss: 0.009657838381826878 = 0.002565727336332202 + 0.001 * 7.092111110687256
Epoch 940, val loss: 1.0486727952957153
Epoch 950, training loss: 0.009608526714146137 = 0.0025044772773981094 + 0.001 * 7.104049205780029
Epoch 950, val loss: 1.0518951416015625
Epoch 960, training loss: 0.009563707746565342 = 0.002445874037221074 + 0.001 * 7.117833614349365
Epoch 960, val loss: 1.0550479888916016
Epoch 970, training loss: 0.009493350982666016 = 0.0023897516075521708 + 0.001 * 7.1035990715026855
Epoch 970, val loss: 1.0581450462341309
Epoch 980, training loss: 0.009433647617697716 = 0.0023360000923275948 + 0.001 * 7.097647666931152
Epoch 980, val loss: 1.0611917972564697
Epoch 990, training loss: 0.009365660138428211 = 0.0022844793274998665 + 0.001 * 7.081180572509766
Epoch 990, val loss: 1.064183235168457
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8481481481481482
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 1.9482545852661133 = 1.9396578073501587 + 0.001 * 8.596818923950195
Epoch 0, val loss: 1.940229892730713
Epoch 10, training loss: 1.9384151697158813 = 1.9298183917999268 + 0.001 * 8.596760749816895
Epoch 10, val loss: 1.9306161403656006
Epoch 20, training loss: 1.9262189865112305 = 1.917622447013855 + 0.001 * 8.596565246582031
Epoch 20, val loss: 1.9185982942581177
Epoch 30, training loss: 1.9091272354125977 = 1.9005311727523804 + 0.001 * 8.596094131469727
Epoch 30, val loss: 1.9016607999801636
Epoch 40, training loss: 1.8840996026992798 = 1.875504732131958 + 0.001 * 8.594898223876953
Epoch 40, val loss: 1.8773940801620483
Epoch 50, training loss: 1.849685549736023 = 1.8410943746566772 + 0.001 * 8.591120719909668
Epoch 50, val loss: 1.8458194732666016
Epoch 60, training loss: 1.810988426208496 = 1.8024135828018188 + 0.001 * 8.574873924255371
Epoch 60, val loss: 1.813552737236023
Epoch 70, training loss: 1.774318814277649 = 1.7658436298370361 + 0.001 * 8.475157737731934
Epoch 70, val loss: 1.7826026678085327
Epoch 80, training loss: 1.7266045808792114 = 1.7184765338897705 + 0.001 * 8.1280517578125
Epoch 80, val loss: 1.7368545532226562
Epoch 90, training loss: 1.660502552986145 = 1.6525003910064697 + 0.001 * 8.00213623046875
Epoch 90, val loss: 1.675910472869873
Epoch 100, training loss: 1.5742298364639282 = 1.56634521484375 + 0.001 * 7.884660243988037
Epoch 100, val loss: 1.6022793054580688
Epoch 110, training loss: 1.4771478176116943 = 1.4694522619247437 + 0.001 * 7.695507049560547
Epoch 110, val loss: 1.5205011367797852
Epoch 120, training loss: 1.3808382749557495 = 1.3732439279556274 + 0.001 * 7.59439754486084
Epoch 120, val loss: 1.441743016242981
Epoch 130, training loss: 1.2883788347244263 = 1.2808399200439453 + 0.001 * 7.538918972015381
Epoch 130, val loss: 1.3675236701965332
Epoch 140, training loss: 1.1994684934616089 = 1.1919807195663452 + 0.001 * 7.487776279449463
Epoch 140, val loss: 1.2974348068237305
Epoch 150, training loss: 1.1134403944015503 = 1.1059798002243042 + 0.001 * 7.460607051849365
Epoch 150, val loss: 1.2310144901275635
Epoch 160, training loss: 1.0299190282821655 = 1.022475242614746 + 0.001 * 7.443784713745117
Epoch 160, val loss: 1.1671826839447021
Epoch 170, training loss: 0.9491287469863892 = 0.9416934847831726 + 0.001 * 7.435250282287598
Epoch 170, val loss: 1.105531930923462
Epoch 180, training loss: 0.8711704611778259 = 0.8637452125549316 + 0.001 * 7.425252914428711
Epoch 180, val loss: 1.04566490650177
Epoch 190, training loss: 0.796525239944458 = 0.7891112565994263 + 0.001 * 7.413987159729004
Epoch 190, val loss: 0.9883930087089539
Epoch 200, training loss: 0.7264120578765869 = 0.7190135717391968 + 0.001 * 7.39847993850708
Epoch 200, val loss: 0.935545802116394
Epoch 210, training loss: 0.6621047258377075 = 0.6547297239303589 + 0.001 * 7.374997138977051
Epoch 210, val loss: 0.8895189166069031
Epoch 220, training loss: 0.603906512260437 = 0.5965526700019836 + 0.001 * 7.35383939743042
Epoch 220, val loss: 0.8515328764915466
Epoch 230, training loss: 0.5511866807937622 = 0.5438624620437622 + 0.001 * 7.324196815490723
Epoch 230, val loss: 0.8214470148086548
Epoch 240, training loss: 0.5030539631843567 = 0.4957488477230072 + 0.001 * 7.305140495300293
Epoch 240, val loss: 0.7984480261802673
Epoch 250, training loss: 0.45859673619270325 = 0.45129409432411194 + 0.001 * 7.302651882171631
Epoch 250, val loss: 0.7814817428588867
Epoch 260, training loss: 0.417055606842041 = 0.4097646176815033 + 0.001 * 7.290982246398926
Epoch 260, val loss: 0.7690902948379517
Epoch 270, training loss: 0.37787550687789917 = 0.37058961391448975 + 0.001 * 7.285891532897949
Epoch 270, val loss: 0.7601500153541565
Epoch 280, training loss: 0.3407381772994995 = 0.3334559202194214 + 0.001 * 7.282250881195068
Epoch 280, val loss: 0.7538058161735535
Epoch 290, training loss: 0.3055984675884247 = 0.29831963777542114 + 0.001 * 7.278824329376221
Epoch 290, val loss: 0.7498329281806946
Epoch 300, training loss: 0.27266189455986023 = 0.2653845250606537 + 0.001 * 7.277381896972656
Epoch 300, val loss: 0.7482284903526306
Epoch 310, training loss: 0.24228551983833313 = 0.2350095957517624 + 0.001 * 7.2759199142456055
Epoch 310, val loss: 0.7492577433586121
Epoch 320, training loss: 0.21478720009326935 = 0.20751629769802094 + 0.001 * 7.270904541015625
Epoch 320, val loss: 0.7530495524406433
Epoch 330, training loss: 0.19026236236095428 = 0.1829918771982193 + 0.001 * 7.270488739013672
Epoch 330, val loss: 0.7595381140708923
Epoch 340, training loss: 0.1686212569475174 = 0.16135148704051971 + 0.001 * 7.269775390625
Epoch 340, val loss: 0.7683497071266174
Epoch 350, training loss: 0.14965206384658813 = 0.1423829197883606 + 0.001 * 7.269143104553223
Epoch 350, val loss: 0.7791539430618286
Epoch 360, training loss: 0.13308051228523254 = 0.12581558525562286 + 0.001 * 7.264923095703125
Epoch 360, val loss: 0.7914949655532837
Epoch 370, training loss: 0.11863736808300018 = 0.11137639731168747 + 0.001 * 7.2609686851501465
Epoch 370, val loss: 0.8051438927650452
Epoch 380, training loss: 0.1060587614774704 = 0.09879925847053528 + 0.001 * 7.25950288772583
Epoch 380, val loss: 0.8196969628334045
Epoch 390, training loss: 0.09509792178869247 = 0.08783770352602005 + 0.001 * 7.260215759277344
Epoch 390, val loss: 0.8348875045776367
Epoch 400, training loss: 0.08551988750696182 = 0.07826139032840729 + 0.001 * 7.258494853973389
Epoch 400, val loss: 0.8504838943481445
Epoch 410, training loss: 0.07712796330451965 = 0.06987544149160385 + 0.001 * 7.252523422241211
Epoch 410, val loss: 0.8663245439529419
Epoch 420, training loss: 0.06977420300245285 = 0.06252272427082062 + 0.001 * 7.251479625701904
Epoch 420, val loss: 0.882226288318634
Epoch 430, training loss: 0.06331530213356018 = 0.05606976896524429 + 0.001 * 7.245530128479004
Epoch 430, val loss: 0.8980123996734619
Epoch 440, training loss: 0.05764957517385483 = 0.05040489509701729 + 0.001 * 7.244678020477295
Epoch 440, val loss: 0.9135852456092834
Epoch 450, training loss: 0.05267609283328056 = 0.04542914405465126 + 0.001 * 7.246947765350342
Epoch 450, val loss: 0.9288583993911743
Epoch 460, training loss: 0.04829699546098709 = 0.04105428606271744 + 0.001 * 7.242708206176758
Epoch 460, val loss: 0.9438088536262512
Epoch 470, training loss: 0.04444137588143349 = 0.03720339387655258 + 0.001 * 7.237982273101807
Epoch 470, val loss: 0.9583833813667297
Epoch 480, training loss: 0.04103388264775276 = 0.03380831331014633 + 0.001 * 7.225569725036621
Epoch 480, val loss: 0.9725253582000732
Epoch 490, training loss: 0.03804325312376022 = 0.03080909140408039 + 0.001 * 7.23415994644165
Epoch 490, val loss: 0.986244797706604
Epoch 500, training loss: 0.03537360206246376 = 0.0281541645526886 + 0.001 * 7.2194366455078125
Epoch 500, val loss: 0.9995536804199219
Epoch 510, training loss: 0.033017922192811966 = 0.02579835243523121 + 0.001 * 7.219568252563477
Epoch 510, val loss: 1.0124601125717163
Epoch 520, training loss: 0.030911074951291084 = 0.02370309829711914 + 0.001 * 7.2079758644104
Epoch 520, val loss: 1.024971842765808
Epoch 530, training loss: 0.02906017377972603 = 0.021834595128893852 + 0.001 * 7.225578784942627
Epoch 530, val loss: 1.0371007919311523
Epoch 540, training loss: 0.027379563078284264 = 0.02016422525048256 + 0.001 * 7.215336799621582
Epoch 540, val loss: 1.0488743782043457
Epoch 550, training loss: 0.025872400030493736 = 0.01866718754172325 + 0.001 * 7.205212116241455
Epoch 550, val loss: 1.0602790117263794
Epoch 560, training loss: 0.024530621245503426 = 0.017322130501270294 + 0.001 * 7.20849084854126
Epoch 560, val loss: 1.0713458061218262
Epoch 570, training loss: 0.02331574819982052 = 0.016110792756080627 + 0.001 * 7.204955577850342
Epoch 570, val loss: 1.082052230834961
Epoch 580, training loss: 0.022208552807569504 = 0.015017272904515266 + 0.001 * 7.191279888153076
Epoch 580, val loss: 1.0924396514892578
Epoch 590, training loss: 0.02122567966580391 = 0.01402792427688837 + 0.001 * 7.197754383087158
Epoch 590, val loss: 1.1024906635284424
Epoch 600, training loss: 0.020311081781983376 = 0.013130700215697289 + 0.001 * 7.1803812980651855
Epoch 600, val loss: 1.1122468709945679
Epoch 610, training loss: 0.019486743956804276 = 0.01231524720788002 + 0.001 * 7.171496868133545
Epoch 610, val loss: 1.1217042207717896
Epoch 620, training loss: 0.018735934048891068 = 0.011572595685720444 + 0.001 * 7.163339138031006
Epoch 620, val loss: 1.1308661699295044
Epoch 630, training loss: 0.018071621656417847 = 0.010894780978560448 + 0.001 * 7.176839828491211
Epoch 630, val loss: 1.1397337913513184
Epoch 640, training loss: 0.017452429980039597 = 0.010274804197251797 + 0.001 * 7.1776251792907715
Epoch 640, val loss: 1.1483259201049805
Epoch 650, training loss: 0.016863670200109482 = 0.009706535376608372 + 0.001 * 7.157135009765625
Epoch 650, val loss: 1.1566624641418457
Epoch 660, training loss: 0.01634930819272995 = 0.0091846389696002 + 0.001 * 7.164668083190918
Epoch 660, val loss: 1.164764404296875
Epoch 670, training loss: 0.015846215188503265 = 0.008704490028321743 + 0.001 * 7.141724109649658
Epoch 670, val loss: 1.1726182699203491
Epoch 680, training loss: 0.015419304370880127 = 0.008261965587735176 + 0.001 * 7.157339096069336
Epoch 680, val loss: 1.180247187614441
Epoch 690, training loss: 0.015012676827609539 = 0.007853280752897263 + 0.001 * 7.159395694732666
Epoch 690, val loss: 1.1876492500305176
Epoch 700, training loss: 0.014621544629335403 = 0.007475194521248341 + 0.001 * 7.146349906921387
Epoch 700, val loss: 1.1948463916778564
Epoch 710, training loss: 0.014252508990466595 = 0.007124852854758501 + 0.001 * 7.127655982971191
Epoch 710, val loss: 1.2018393278121948
Epoch 720, training loss: 0.013946757651865482 = 0.0067996918223798275 + 0.001 * 7.14706563949585
Epoch 720, val loss: 1.2086361646652222
Epoch 730, training loss: 0.01362721249461174 = 0.006497382186353207 + 0.001 * 7.129829406738281
Epoch 730, val loss: 1.2152349948883057
Epoch 740, training loss: 0.013375706970691681 = 0.006215882021933794 + 0.001 * 7.159824371337891
Epoch 740, val loss: 1.2216562032699585
Epoch 750, training loss: 0.013111915439367294 = 0.005953280255198479 + 0.001 * 7.158635139465332
Epoch 750, val loss: 1.2278926372528076
Epoch 760, training loss: 0.012826669029891491 = 0.005707714706659317 + 0.001 * 7.118954181671143
Epoch 760, val loss: 1.2339825630187988
Epoch 770, training loss: 0.012600136920809746 = 0.005477442406117916 + 0.001 * 7.1226935386657715
Epoch 770, val loss: 1.2399119138717651
Epoch 780, training loss: 0.012368978932499886 = 0.0052611068822443485 + 0.001 * 7.107872009277344
Epoch 780, val loss: 1.2456920146942139
Epoch 790, training loss: 0.012189824134111404 = 0.005057112313807011 + 0.001 * 7.132711887359619
Epoch 790, val loss: 1.2513376474380493
Epoch 800, training loss: 0.011996002867817879 = 0.004864139948040247 + 0.001 * 7.131862640380859
Epoch 800, val loss: 1.2568646669387817
Epoch 810, training loss: 0.01182803325355053 = 0.004681096412241459 + 0.001 * 7.146935939788818
Epoch 810, val loss: 1.2622742652893066
Epoch 820, training loss: 0.011616995558142662 = 0.004507335368543863 + 0.001 * 7.109659194946289
Epoch 820, val loss: 1.2675966024398804
Epoch 830, training loss: 0.011463700793683529 = 0.004342199768871069 + 0.001 * 7.121500492095947
Epoch 830, val loss: 1.2728041410446167
Epoch 840, training loss: 0.011292511597275734 = 0.004185402300208807 + 0.001 * 7.1071085929870605
Epoch 840, val loss: 1.2778853178024292
Epoch 850, training loss: 0.011135256849229336 = 0.004036488477140665 + 0.001 * 7.09876823425293
Epoch 850, val loss: 1.282868504524231
Epoch 860, training loss: 0.011006445623934269 = 0.0038951069582253695 + 0.001 * 7.111338138580322
Epoch 860, val loss: 1.2877305746078491
Epoch 870, training loss: 0.010894328355789185 = 0.0037608847487717867 + 0.001 * 7.133443355560303
Epoch 870, val loss: 1.2925182580947876
Epoch 880, training loss: 0.010726513341069221 = 0.0036335012409836054 + 0.001 * 7.093011856079102
Epoch 880, val loss: 1.2971792221069336
Epoch 890, training loss: 0.010594391264021397 = 0.003512734780088067 + 0.001 * 7.081655979156494
Epoch 890, val loss: 1.3017336130142212
Epoch 900, training loss: 0.010476794093847275 = 0.003398129716515541 + 0.001 * 7.0786638259887695
Epoch 900, val loss: 1.3061943054199219
Epoch 910, training loss: 0.010373462922871113 = 0.003289321204647422 + 0.001 * 7.084141254425049
Epoch 910, val loss: 1.3105486631393433
Epoch 920, training loss: 0.010283421725034714 = 0.0031860352028161287 + 0.001 * 7.097386360168457
Epoch 920, val loss: 1.3148144483566284
Epoch 930, training loss: 0.01021229475736618 = 0.0030879289843142033 + 0.001 * 7.124365329742432
Epoch 930, val loss: 1.3189796209335327
Epoch 940, training loss: 0.01009648572653532 = 0.0029947629664093256 + 0.001 * 7.101722240447998
Epoch 940, val loss: 1.3230303525924683
Epoch 950, training loss: 0.010004708543419838 = 0.002906200708821416 + 0.001 * 7.098507881164551
Epoch 950, val loss: 1.32701575756073
Epoch 960, training loss: 0.0098820636048913 = 0.0028219709638506174 + 0.001 * 7.060092449188232
Epoch 960, val loss: 1.3308870792388916
Epoch 970, training loss: 0.009823421947658062 = 0.0027418460231274366 + 0.001 * 7.081575393676758
Epoch 970, val loss: 1.3346691131591797
Epoch 980, training loss: 0.009767306037247181 = 0.002665590262040496 + 0.001 * 7.101715087890625
Epoch 980, val loss: 1.3383678197860718
Epoch 990, training loss: 0.009672194719314575 = 0.002592936623841524 + 0.001 * 7.079258441925049
Epoch 990, val loss: 1.3419995307922363
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 1.9673703908920288 = 1.9587736129760742 + 0.001 * 8.596826553344727
Epoch 0, val loss: 1.9630911350250244
Epoch 10, training loss: 1.95693039894104 = 1.9483336210250854 + 0.001 * 8.596778869628906
Epoch 10, val loss: 1.9524972438812256
Epoch 20, training loss: 1.9441688060760498 = 1.9355721473693848 + 0.001 * 8.596601486206055
Epoch 20, val loss: 1.9394171237945557
Epoch 30, training loss: 1.926369071006775 = 1.917772889137268 + 0.001 * 8.596192359924316
Epoch 30, val loss: 1.9211846590042114
Epoch 40, training loss: 1.9000049829483032 = 1.8914097547531128 + 0.001 * 8.595272064208984
Epoch 40, val loss: 1.8945344686508179
Epoch 50, training loss: 1.8618674278259277 = 1.8532745838165283 + 0.001 * 8.592853546142578
Epoch 50, val loss: 1.857369065284729
Epoch 60, training loss: 1.8147492408752441 = 1.8061647415161133 + 0.001 * 8.58449649810791
Epoch 60, val loss: 1.815077781677246
Epoch 70, training loss: 1.7712271213531494 = 1.7626804113388062 + 0.001 * 8.546698570251465
Epoch 70, val loss: 1.779206395149231
Epoch 80, training loss: 1.7246555089950562 = 1.7163670063018799 + 0.001 * 8.288508415222168
Epoch 80, val loss: 1.736812710762024
Epoch 90, training loss: 1.659424901008606 = 1.6513950824737549 + 0.001 * 8.029840469360352
Epoch 90, val loss: 1.6771876811981201
Epoch 100, training loss: 1.573613166809082 = 1.5657854080200195 + 0.001 * 7.827700614929199
Epoch 100, val loss: 1.6028965711593628
Epoch 110, training loss: 1.4709272384643555 = 1.463215947151184 + 0.001 * 7.711236476898193
Epoch 110, val loss: 1.5177422761917114
Epoch 120, training loss: 1.3634238243103027 = 1.3557653427124023 + 0.001 * 7.658491611480713
Epoch 120, val loss: 1.4299497604370117
Epoch 130, training loss: 1.258926272392273 = 1.2513493299484253 + 0.001 * 7.576931953430176
Epoch 130, val loss: 1.3467371463775635
Epoch 140, training loss: 1.1617584228515625 = 1.1542447805404663 + 0.001 * 7.513639450073242
Epoch 140, val loss: 1.2703672647476196
Epoch 150, training loss: 1.074358582496643 = 1.0668774843215942 + 0.001 * 7.481156826019287
Epoch 150, val loss: 1.202344298362732
Epoch 160, training loss: 0.9971264600753784 = 0.9896681308746338 + 0.001 * 7.458354473114014
Epoch 160, val loss: 1.1432512998580933
Epoch 170, training loss: 0.9277324676513672 = 0.9202887415885925 + 0.001 * 7.443698406219482
Epoch 170, val loss: 1.0909159183502197
Epoch 180, training loss: 0.8627418875694275 = 0.8553059697151184 + 0.001 * 7.435935974121094
Epoch 180, val loss: 1.0420798063278198
Epoch 190, training loss: 0.7996177673339844 = 0.7921879887580872 + 0.001 * 7.429790496826172
Epoch 190, val loss: 0.9944343566894531
Epoch 200, training loss: 0.737416684627533 = 0.7299930453300476 + 0.001 * 7.423633098602295
Epoch 200, val loss: 0.94781893491745
Epoch 210, training loss: 0.6760023236274719 = 0.6685847043991089 + 0.001 * 7.417645454406738
Epoch 210, val loss: 0.9030546545982361
Epoch 220, training loss: 0.6151527762413025 = 0.6077407598495483 + 0.001 * 7.412025451660156
Epoch 220, val loss: 0.859993577003479
Epoch 230, training loss: 0.5542997717857361 = 0.5468928217887878 + 0.001 * 7.406931400299072
Epoch 230, val loss: 0.8188011050224304
Epoch 240, training loss: 0.4934297502040863 = 0.4860278069972992 + 0.001 * 7.401956081390381
Epoch 240, val loss: 0.7793176174163818
Epoch 250, training loss: 0.4334399998188019 = 0.4260428547859192 + 0.001 * 7.3971476554870605
Epoch 250, val loss: 0.742372453212738
Epoch 260, training loss: 0.37615835666656494 = 0.3687681555747986 + 0.001 * 7.390187740325928
Epoch 260, val loss: 0.7090850472450256
Epoch 270, training loss: 0.3235057592391968 = 0.31612417101860046 + 0.001 * 7.3815999031066895
Epoch 270, val loss: 0.681245744228363
Epoch 280, training loss: 0.2768004238605499 = 0.26942741870880127 + 0.001 * 7.373000621795654
Epoch 280, val loss: 0.6600818037986755
Epoch 290, training loss: 0.23646482825279236 = 0.22910307347774506 + 0.001 * 7.361752986907959
Epoch 290, val loss: 0.6456905603408813
Epoch 300, training loss: 0.2022578865289688 = 0.19490639865398407 + 0.001 * 7.35148286819458
Epoch 300, val loss: 0.6374821662902832
Epoch 310, training loss: 0.17360267043113708 = 0.16626094281673431 + 0.001 * 7.341726779937744
Epoch 310, val loss: 0.6344708204269409
Epoch 320, training loss: 0.14977046847343445 = 0.14244629442691803 + 0.001 * 7.324167251586914
Epoch 320, val loss: 0.6357234716415405
Epoch 330, training loss: 0.13002288341522217 = 0.1227053701877594 + 0.001 * 7.317511081695557
Epoch 330, val loss: 0.6404268741607666
Epoch 340, training loss: 0.11359786242246628 = 0.10630200058221817 + 0.001 * 7.295863628387451
Epoch 340, val loss: 0.6477504968643188
Epoch 350, training loss: 0.09988793730735779 = 0.0925985723733902 + 0.001 * 7.289366722106934
Epoch 350, val loss: 0.6570248007774353
Epoch 360, training loss: 0.08837220072746277 = 0.0810750350356102 + 0.001 * 7.297169208526611
Epoch 360, val loss: 0.6677160263061523
Epoch 370, training loss: 0.07859255373477936 = 0.07132214307785034 + 0.001 * 7.270413875579834
Epoch 370, val loss: 0.6793918609619141
Epoch 380, training loss: 0.07028580456972122 = 0.06302303075790405 + 0.001 * 7.262775897979736
Epoch 380, val loss: 0.691673219203949
Epoch 390, training loss: 0.06319306790828705 = 0.05593108758330345 + 0.001 * 7.261982440948486
Epoch 390, val loss: 0.704292893409729
Epoch 400, training loss: 0.057114310562610626 = 0.04984641820192337 + 0.001 * 7.2678937911987305
Epoch 400, val loss: 0.7170929908752441
Epoch 410, training loss: 0.051857538521289825 = 0.04460644721984863 + 0.001 * 7.251089096069336
Epoch 410, val loss: 0.7299417853355408
Epoch 420, training loss: 0.047309085726737976 = 0.04007982090115547 + 0.001 * 7.229262828826904
Epoch 420, val loss: 0.7426897287368774
Epoch 430, training loss: 0.0433923602104187 = 0.03615609556436539 + 0.001 * 7.236265182495117
Epoch 430, val loss: 0.755292534828186
Epoch 440, training loss: 0.039960067719221115 = 0.032743699848651886 + 0.001 * 7.216367721557617
Epoch 440, val loss: 0.7676897644996643
Epoch 450, training loss: 0.0369899682700634 = 0.029766008257865906 + 0.001 * 7.223960876464844
Epoch 450, val loss: 0.7798080444335938
Epoch 460, training loss: 0.03437585383653641 = 0.027158714830875397 + 0.001 * 7.217140197753906
Epoch 460, val loss: 0.7916638851165771
Epoch 470, training loss: 0.0320775993168354 = 0.02486780285835266 + 0.001 * 7.2097954750061035
Epoch 470, val loss: 0.8032073378562927
Epoch 480, training loss: 0.030052103102207184 = 0.022847522050142288 + 0.001 * 7.204581260681152
Epoch 480, val loss: 0.814439594745636
Epoch 490, training loss: 0.028277095407247543 = 0.021059416234493256 + 0.001 * 7.217678070068359
Epoch 490, val loss: 0.8253675103187561
Epoch 500, training loss: 0.02668497897684574 = 0.01947088912129402 + 0.001 * 7.214089393615723
Epoch 500, val loss: 0.8359792232513428
Epoch 510, training loss: 0.025249944999814034 = 0.018054762855172157 + 0.001 * 7.1951823234558105
Epoch 510, val loss: 0.8462589383125305
Epoch 520, training loss: 0.02399514615535736 = 0.016787972301244736 + 0.001 * 7.207172393798828
Epoch 520, val loss: 0.8562598824501038
Epoch 530, training loss: 0.022834260016679764 = 0.015651093795895576 + 0.001 * 7.18316650390625
Epoch 530, val loss: 0.8659340739250183
Epoch 540, training loss: 0.021824922412633896 = 0.014627648517489433 + 0.001 * 7.197274208068848
Epoch 540, val loss: 0.8753184676170349
Epoch 550, training loss: 0.020914064720273018 = 0.013703403063118458 + 0.001 * 7.210660934448242
Epoch 550, val loss: 0.8844283223152161
Epoch 560, training loss: 0.02006959170103073 = 0.012866230681538582 + 0.001 * 7.203361511230469
Epoch 560, val loss: 0.8932539224624634
Epoch 570, training loss: 0.019284648820757866 = 0.01210582535713911 + 0.001 * 7.178823471069336
Epoch 570, val loss: 0.9018216729164124
Epoch 580, training loss: 0.01860962063074112 = 0.011413159780204296 + 0.001 * 7.196460723876953
Epoch 580, val loss: 0.910153865814209
Epoch 590, training loss: 0.017971452325582504 = 0.010780619457364082 + 0.001 * 7.190832614898682
Epoch 590, val loss: 0.918257474899292
Epoch 600, training loss: 0.01736602559685707 = 0.010201416909694672 + 0.001 * 7.164608478546143
Epoch 600, val loss: 0.9261137843132019
Epoch 610, training loss: 0.01682962104678154 = 0.009669708088040352 + 0.001 * 7.159913063049316
Epoch 610, val loss: 0.933756947517395
Epoch 620, training loss: 0.016338715329766273 = 0.009180036373436451 + 0.001 * 7.15867805480957
Epoch 620, val loss: 0.9412106871604919
Epoch 630, training loss: 0.015892183408141136 = 0.008727234788239002 + 0.001 * 7.164947986602783
Epoch 630, val loss: 0.9484955072402954
Epoch 640, training loss: 0.015452619642019272 = 0.008306204341351986 + 0.001 * 7.146414279937744
Epoch 640, val loss: 0.9556476473808289
Epoch 650, training loss: 0.015064964070916176 = 0.007912619039416313 + 0.001 * 7.152345180511475
Epoch 650, val loss: 0.9627570509910583
Epoch 660, training loss: 0.014693474397063255 = 0.007543800864368677 + 0.001 * 7.1496734619140625
Epoch 660, val loss: 0.969800591468811
Epoch 670, training loss: 0.014361022971570492 = 0.007197974249720573 + 0.001 * 7.163048267364502
Epoch 670, val loss: 0.9768015742301941
Epoch 680, training loss: 0.014058711007237434 = 0.006873528938740492 + 0.001 * 7.185181617736816
Epoch 680, val loss: 0.9837337732315063
Epoch 690, training loss: 0.01372477412223816 = 0.0065691727213561535 + 0.001 * 7.155600547790527
Epoch 690, val loss: 0.9905916452407837
Epoch 700, training loss: 0.013463808223605156 = 0.006283893715590239 + 0.001 * 7.1799139976501465
Epoch 700, val loss: 0.9973737001419067
Epoch 710, training loss: 0.01317037083208561 = 0.006016617175191641 + 0.001 * 7.153753757476807
Epoch 710, val loss: 1.004042625427246
Epoch 720, training loss: 0.012915583327412605 = 0.005765840876847506 + 0.001 * 7.149742126464844
Epoch 720, val loss: 1.0106111764907837
Epoch 730, training loss: 0.012646310962736607 = 0.0055304765701293945 + 0.001 * 7.1158342361450195
Epoch 730, val loss: 1.0170589685440063
Epoch 740, training loss: 0.012453129515051842 = 0.005309388507157564 + 0.001 * 7.143740653991699
Epoch 740, val loss: 1.0234063863754272
Epoch 750, training loss: 0.012230665422976017 = 0.00510161928832531 + 0.001 * 7.1290459632873535
Epoch 750, val loss: 1.029632806777954
Epoch 760, training loss: 0.012027069926261902 = 0.004906267859041691 + 0.001 * 7.120802402496338
Epoch 760, val loss: 1.0357426404953003
Epoch 770, training loss: 0.011824816465377808 = 0.004722564946860075 + 0.001 * 7.102250576019287
Epoch 770, val loss: 1.0417288541793823
Epoch 780, training loss: 0.01166541688144207 = 0.004549644887447357 + 0.001 * 7.115772247314453
Epoch 780, val loss: 1.0475672483444214
Epoch 790, training loss: 0.011504894122481346 = 0.004386720713227987 + 0.001 * 7.118173122406006
Epoch 790, val loss: 1.0533297061920166
Epoch 800, training loss: 0.011363465338945389 = 0.004233002196997404 + 0.001 * 7.130462646484375
Epoch 800, val loss: 1.058966040611267
Epoch 810, training loss: 0.011212196201086044 = 0.004087862093001604 + 0.001 * 7.124334335327148
Epoch 810, val loss: 1.0644712448120117
Epoch 820, training loss: 0.011082658544182777 = 0.0039506349712610245 + 0.001 * 7.132023334503174
Epoch 820, val loss: 1.0698976516723633
Epoch 830, training loss: 0.01093652006238699 = 0.0038208418991416693 + 0.001 * 7.115677833557129
Epoch 830, val loss: 1.0751943588256836
Epoch 840, training loss: 0.01080002449452877 = 0.0036979925353080034 + 0.001 * 7.102031707763672
Epoch 840, val loss: 1.0803908109664917
Epoch 850, training loss: 0.010681705549359322 = 0.0035815765149891376 + 0.001 * 7.100128650665283
Epoch 850, val loss: 1.0854792594909668
Epoch 860, training loss: 0.010564010590314865 = 0.003471187548711896 + 0.001 * 7.092823028564453
Epoch 860, val loss: 1.0904595851898193
Epoch 870, training loss: 0.010464596562087536 = 0.0033664547372609377 + 0.001 * 7.098141670227051
Epoch 870, val loss: 1.0953432321548462
Epoch 880, training loss: 0.010355089791119099 = 0.003267006715759635 + 0.001 * 7.088082313537598
Epoch 880, val loss: 1.1001511812210083
Epoch 890, training loss: 0.010269459336996078 = 0.003172439057379961 + 0.001 * 7.097020149230957
Epoch 890, val loss: 1.1048178672790527
Epoch 900, training loss: 0.010176884941756725 = 0.003082468407228589 + 0.001 * 7.09441614151001
Epoch 900, val loss: 1.1094318628311157
Epoch 910, training loss: 0.010091233998537064 = 0.0029968409799039364 + 0.001 * 7.094393253326416
Epoch 910, val loss: 1.1139347553253174
Epoch 920, training loss: 0.010001800954341888 = 0.002915330696851015 + 0.001 * 7.086469650268555
Epoch 920, val loss: 1.1183301210403442
Epoch 930, training loss: 0.009901813231408596 = 0.002837604144588113 + 0.001 * 7.064208984375
Epoch 930, val loss: 1.1226710081100464
Epoch 940, training loss: 0.009833678603172302 = 0.002763455267995596 + 0.001 * 7.070222854614258
Epoch 940, val loss: 1.1268993616104126
Epoch 950, training loss: 0.009749457240104675 = 0.002692630747333169 + 0.001 * 7.056826591491699
Epoch 950, val loss: 1.131069302558899
Epoch 960, training loss: 0.00971007440239191 = 0.002625018358230591 + 0.001 * 7.085055828094482
Epoch 960, val loss: 1.1351248025894165
Epoch 970, training loss: 0.00966639630496502 = 0.0025603994727134705 + 0.001 * 7.105996608734131
Epoch 970, val loss: 1.1391528844833374
Epoch 980, training loss: 0.009568534791469574 = 0.0024985969066619873 + 0.001 * 7.0699381828308105
Epoch 980, val loss: 1.1430456638336182
Epoch 990, training loss: 0.009536233730614185 = 0.0024394530337303877 + 0.001 * 7.096780300140381
Epoch 990, val loss: 1.146898865699768
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8360569319978914
The final CL Acc:0.81852, 0.02419, The final GNN Acc:0.83746, 0.00131
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10586])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.963776707649231 = 1.9551799297332764 + 0.001 * 8.596807479858398
Epoch 0, val loss: 1.9599186182022095
Epoch 10, training loss: 1.953024983406067 = 1.9444282054901123 + 0.001 * 8.5967378616333
Epoch 10, val loss: 1.9495314359664917
Epoch 20, training loss: 1.9394862651824951 = 1.9308897256851196 + 0.001 * 8.596552848815918
Epoch 20, val loss: 1.9358028173446655
Epoch 30, training loss: 1.9204161167144775 = 1.9118199348449707 + 0.001 * 8.596174240112305
Epoch 30, val loss: 1.915919542312622
Epoch 40, training loss: 1.892621636390686 = 1.8840264081954956 + 0.001 * 8.595282554626465
Epoch 40, val loss: 1.8872414827346802
Epoch 50, training loss: 1.855496883392334 = 1.8469041585922241 + 0.001 * 8.592757225036621
Epoch 50, val loss: 1.851052165031433
Epoch 60, training loss: 1.8181369304656982 = 1.809552788734436 + 0.001 * 8.584183692932129
Epoch 60, val loss: 1.8187998533248901
Epoch 70, training loss: 1.7886992692947388 = 1.780150055885315 + 0.001 * 8.54922866821289
Epoch 70, val loss: 1.7946301698684692
Epoch 80, training loss: 1.7515305280685425 = 1.7432020902633667 + 0.001 * 8.328459739685059
Epoch 80, val loss: 1.761231541633606
Epoch 90, training loss: 1.7000749111175537 = 1.6919317245483398 + 0.001 * 8.143149375915527
Epoch 90, val loss: 1.7168669700622559
Epoch 100, training loss: 1.6291604042053223 = 1.6211130619049072 + 0.001 * 8.047340393066406
Epoch 100, val loss: 1.6579957008361816
Epoch 110, training loss: 1.5435693264007568 = 1.5356013774871826 + 0.001 * 7.968007564544678
Epoch 110, val loss: 1.5891680717468262
Epoch 120, training loss: 1.4552013874053955 = 1.4473849534988403 + 0.001 * 7.816493511199951
Epoch 120, val loss: 1.5216615200042725
Epoch 130, training loss: 1.3701989650726318 = 1.3624836206436157 + 0.001 * 7.715391635894775
Epoch 130, val loss: 1.4585093259811401
Epoch 140, training loss: 1.2879139184951782 = 1.280213475227356 + 0.001 * 7.700414180755615
Epoch 140, val loss: 1.399458885192871
Epoch 150, training loss: 1.206567645072937 = 1.1988999843597412 + 0.001 * 7.6676154136657715
Epoch 150, val loss: 1.3427678346633911
Epoch 160, training loss: 1.1274107694625854 = 1.1197806596755981 + 0.001 * 7.630119323730469
Epoch 160, val loss: 1.2887179851531982
Epoch 170, training loss: 1.052783489227295 = 1.0451912879943848 + 0.001 * 7.5922393798828125
Epoch 170, val loss: 1.2388620376586914
Epoch 180, training loss: 0.9835435748100281 = 0.9759995341300964 + 0.001 * 7.544060707092285
Epoch 180, val loss: 1.1930301189422607
Epoch 190, training loss: 0.9184245467185974 = 0.9109461307525635 + 0.001 * 7.478399276733398
Epoch 190, val loss: 1.1499472856521606
Epoch 200, training loss: 0.85533207654953 = 0.8479048609733582 + 0.001 * 7.427203178405762
Epoch 200, val loss: 1.1084476709365845
Epoch 210, training loss: 0.7930233478546143 = 0.7856184840202332 + 0.001 * 7.404850006103516
Epoch 210, val loss: 1.0682097673416138
Epoch 220, training loss: 0.7316619157791138 = 0.7242852449417114 + 0.001 * 7.376646995544434
Epoch 220, val loss: 1.030125617980957
Epoch 230, training loss: 0.6721585392951965 = 0.6647922396659851 + 0.001 * 7.36630392074585
Epoch 230, val loss: 0.9959223866462708
Epoch 240, training loss: 0.6153860092163086 = 0.6080414652824402 + 0.001 * 7.344568252563477
Epoch 240, val loss: 0.9673007726669312
Epoch 250, training loss: 0.5618401169776917 = 0.5545054078102112 + 0.001 * 7.334689617156982
Epoch 250, val loss: 0.9456664323806763
Epoch 260, training loss: 0.511698305606842 = 0.5043700933456421 + 0.001 * 7.3282389640808105
Epoch 260, val loss: 0.9315091371536255
Epoch 270, training loss: 0.46527835726737976 = 0.4579514265060425 + 0.001 * 7.326925277709961
Epoch 270, val loss: 0.9245352149009705
Epoch 280, training loss: 0.42285090684890747 = 0.415528804063797 + 0.001 * 7.322111129760742
Epoch 280, val loss: 0.9235016703605652
Epoch 290, training loss: 0.38420817255973816 = 0.37689030170440674 + 0.001 * 7.3178629875183105
Epoch 290, val loss: 0.9267975687980652
Epoch 300, training loss: 0.34852904081344604 = 0.3412134051322937 + 0.001 * 7.315645694732666
Epoch 300, val loss: 0.9331021308898926
Epoch 310, training loss: 0.3147139847278595 = 0.3073994815349579 + 0.001 * 7.314492702484131
Epoch 310, val loss: 0.9413393139839172
Epoch 320, training loss: 0.2819232940673828 = 0.274612158536911 + 0.001 * 7.311141014099121
Epoch 320, val loss: 0.9506698250770569
Epoch 330, training loss: 0.25004127621650696 = 0.24273242056369781 + 0.001 * 7.30886697769165
Epoch 330, val loss: 0.961132824420929
Epoch 340, training loss: 0.21979744732379913 = 0.2124880999326706 + 0.001 * 7.309342384338379
Epoch 340, val loss: 0.9730172753334045
Epoch 350, training loss: 0.192230686545372 = 0.18492412567138672 + 0.001 * 7.306563377380371
Epoch 350, val loss: 0.9867792129516602
Epoch 360, training loss: 0.16793391108512878 = 0.16062650084495544 + 0.001 * 7.3074164390563965
Epoch 360, val loss: 1.0025267601013184
Epoch 370, training loss: 0.14695735275745392 = 0.13965579867362976 + 0.001 * 7.301554203033447
Epoch 370, val loss: 1.0201566219329834
Epoch 380, training loss: 0.1289963275194168 = 0.12169977277517319 + 0.001 * 7.296553134918213
Epoch 380, val loss: 1.039518117904663
Epoch 390, training loss: 0.11363442242145538 = 0.10633829981088638 + 0.001 * 7.2961249351501465
Epoch 390, val loss: 1.0603059530258179
Epoch 400, training loss: 0.10046972334384918 = 0.09317807108163834 + 0.001 * 7.29165506362915
Epoch 400, val loss: 1.082129716873169
Epoch 410, training loss: 0.08915499597787857 = 0.08187052607536316 + 0.001 * 7.284471035003662
Epoch 410, val loss: 1.1045869588851929
Epoch 420, training loss: 0.07941800355911255 = 0.07212648540735245 + 0.001 * 7.2915167808532715
Epoch 420, val loss: 1.127421498298645
Epoch 430, training loss: 0.07099173963069916 = 0.06371432542800903 + 0.001 * 7.277417182922363
Epoch 430, val loss: 1.1503353118896484
Epoch 440, training loss: 0.06371793895959854 = 0.05644777789711952 + 0.001 * 7.270160675048828
Epoch 440, val loss: 1.1731640100479126
Epoch 450, training loss: 0.057531289756298065 = 0.050168223679065704 + 0.001 * 7.363065242767334
Epoch 450, val loss: 1.1957435607910156
Epoch 460, training loss: 0.05200779438018799 = 0.044736348092556 + 0.001 * 7.271447658538818
Epoch 460, val loss: 1.2179611921310425
Epoch 470, training loss: 0.04728315770626068 = 0.04003242403268814 + 0.001 * 7.250732421875
Epoch 470, val loss: 1.2397429943084717
Epoch 480, training loss: 0.043196260929107666 = 0.03595192730426788 + 0.001 * 7.244332313537598
Epoch 480, val loss: 1.261000633239746
Epoch 490, training loss: 0.03975573927164078 = 0.03240472450852394 + 0.001 * 7.351012706756592
Epoch 490, val loss: 1.2816646099090576
Epoch 500, training loss: 0.0365775041282177 = 0.029315004125237465 + 0.001 * 7.26249885559082
Epoch 500, val loss: 1.3017281293869019
Epoch 510, training loss: 0.03386615961790085 = 0.026615817099809647 + 0.001 * 7.2503437995910645
Epoch 510, val loss: 1.3212056159973145
Epoch 520, training loss: 0.031494010239839554 = 0.02425103262066841 + 0.001 * 7.242977142333984
Epoch 520, val loss: 1.3400250673294067
Epoch 530, training loss: 0.029405925422906876 = 0.022172406315803528 + 0.001 * 7.233518123626709
Epoch 530, val loss: 1.3581888675689697
Epoch 540, training loss: 0.02758539840579033 = 0.020339738577604294 + 0.001 * 7.245660305023193
Epoch 540, val loss: 1.3757041692733765
Epoch 550, training loss: 0.025964273139834404 = 0.01871861331164837 + 0.001 * 7.245659351348877
Epoch 550, val loss: 1.3926198482513428
Epoch 560, training loss: 0.02449989691376686 = 0.01728004217147827 + 0.001 * 7.219853401184082
Epoch 560, val loss: 1.4088749885559082
Epoch 570, training loss: 0.023216446861624718 = 0.01599905453622341 + 0.001 * 7.217392444610596
Epoch 570, val loss: 1.4245460033416748
Epoch 580, training loss: 0.02209523878991604 = 0.014854869805276394 + 0.001 * 7.240368366241455
Epoch 580, val loss: 1.4396438598632812
Epoch 590, training loss: 0.021042468026280403 = 0.013829465955495834 + 0.001 * 7.213001728057861
Epoch 590, val loss: 1.4542067050933838
Epoch 600, training loss: 0.020118435844779015 = 0.012907497584819794 + 0.001 * 7.210937976837158
Epoch 600, val loss: 1.4682859182357788
Epoch 610, training loss: 0.01931074634194374 = 0.012076081708073616 + 0.001 * 7.2346649169921875
Epoch 610, val loss: 1.4818577766418457
Epoch 620, training loss: 0.018544789403676987 = 0.011324231512844563 + 0.001 * 7.2205586433410645
Epoch 620, val loss: 1.4949548244476318
Epoch 630, training loss: 0.017824513837695122 = 0.01064241025596857 + 0.001 * 7.182103633880615
Epoch 630, val loss: 1.507568120956421
Epoch 640, training loss: 0.017239414155483246 = 0.010022337548434734 + 0.001 * 7.217075824737549
Epoch 640, val loss: 1.519789218902588
Epoch 650, training loss: 0.016657168045639992 = 0.009456935338675976 + 0.001 * 7.20023250579834
Epoch 650, val loss: 1.5315815210342407
Epoch 660, training loss: 0.01613944210112095 = 0.008940113708376884 + 0.001 * 7.199328422546387
Epoch 660, val loss: 1.542982816696167
Epoch 670, training loss: 0.01565629243850708 = 0.008466526865959167 + 0.001 * 7.189764499664307
Epoch 670, val loss: 1.5540452003479004
Epoch 680, training loss: 0.015221718698740005 = 0.008031588979065418 + 0.001 * 7.190129280090332
Epoch 680, val loss: 1.56474769115448
Epoch 690, training loss: 0.01479424349963665 = 0.007631288841366768 + 0.001 * 7.162954330444336
Epoch 690, val loss: 1.575105905532837
Epoch 700, training loss: 0.014445751905441284 = 0.007262053433805704 + 0.001 * 7.183697700500488
Epoch 700, val loss: 1.5851542949676514
Epoch 710, training loss: 0.014107382856309414 = 0.0069207740016281605 + 0.001 * 7.18660831451416
Epoch 710, val loss: 1.5948840379714966
Epoch 720, training loss: 0.013787001371383667 = 0.006604794412851334 + 0.001 * 7.182206153869629
Epoch 720, val loss: 1.6043236255645752
Epoch 730, training loss: 0.013475598767399788 = 0.006311655975878239 + 0.001 * 7.163942813873291
Epoch 730, val loss: 1.6134915351867676
Epoch 740, training loss: 0.013212181627750397 = 0.006039200816303492 + 0.001 * 7.172979831695557
Epoch 740, val loss: 1.6223629713058472
Epoch 750, training loss: 0.0129509586840868 = 0.005785600747913122 + 0.001 * 7.16535758972168
Epoch 750, val loss: 1.6310113668441772
Epoch 760, training loss: 0.01270793005824089 = 0.005549099296331406 + 0.001 * 7.1588311195373535
Epoch 760, val loss: 1.6393986940383911
Epoch 770, training loss: 0.012491435743868351 = 0.005328184925019741 + 0.001 * 7.16325044631958
Epoch 770, val loss: 1.6475011110305786
Epoch 780, training loss: 0.01227901503443718 = 0.005121568217873573 + 0.001 * 7.157445907592773
Epoch 780, val loss: 1.6554055213928223
Epoch 790, training loss: 0.012109313160181046 = 0.0049280207604169846 + 0.001 * 7.1812920570373535
Epoch 790, val loss: 1.6631181240081787
Epoch 800, training loss: 0.011891715228557587 = 0.004746474791318178 + 0.001 * 7.145240306854248
Epoch 800, val loss: 1.6705901622772217
Epoch 810, training loss: 0.011756809428334236 = 0.004575919825583696 + 0.001 * 7.180889129638672
Epoch 810, val loss: 1.6778498888015747
Epoch 820, training loss: 0.01156933419406414 = 0.004415544681251049 + 0.001 * 7.153789520263672
Epoch 820, val loss: 1.684924840927124
Epoch 830, training loss: 0.011385118588805199 = 0.00426454795524478 + 0.001 * 7.120570182800293
Epoch 830, val loss: 1.6918152570724487
Epoch 840, training loss: 0.011266006156802177 = 0.00412218552082777 + 0.001 * 7.143819808959961
Epoch 840, val loss: 1.6984918117523193
Epoch 850, training loss: 0.011175673454999924 = 0.003987840376794338 + 0.001 * 7.187832832336426
Epoch 850, val loss: 1.7050029039382935
Epoch 860, training loss: 0.010983680374920368 = 0.0038609227631241083 + 0.001 * 7.1227569580078125
Epoch 860, val loss: 1.7113823890686035
Epoch 870, training loss: 0.010865513235330582 = 0.003740881336852908 + 0.001 * 7.124631881713867
Epoch 870, val loss: 1.7175674438476562
Epoch 880, training loss: 0.010753780603408813 = 0.0036272190045565367 + 0.001 * 7.126561164855957
Epoch 880, val loss: 1.72358238697052
Epoch 890, training loss: 0.010631781071424484 = 0.0035194880329072475 + 0.001 * 7.112292766571045
Epoch 890, val loss: 1.72948157787323
Epoch 900, training loss: 0.010538075119256973 = 0.003417327767238021 + 0.001 * 7.120746612548828
Epoch 900, val loss: 1.7351834774017334
Epoch 910, training loss: 0.010438811033964157 = 0.0033203260973095894 + 0.001 * 7.118484020233154
Epoch 910, val loss: 1.7407634258270264
Epoch 920, training loss: 0.01037093997001648 = 0.003228116547688842 + 0.001 * 7.142823219299316
Epoch 920, val loss: 1.7461953163146973
Epoch 930, training loss: 0.010269556194543839 = 0.0031403799075633287 + 0.001 * 7.129175662994385
Epoch 930, val loss: 1.7515290975570679
Epoch 940, training loss: 0.01018056832253933 = 0.0030568952206522226 + 0.001 * 7.123672962188721
Epoch 940, val loss: 1.7567191123962402
Epoch 950, training loss: 0.010109912604093552 = 0.0029773570131510496 + 0.001 * 7.1325554847717285
Epoch 950, val loss: 1.7617727518081665
Epoch 960, training loss: 0.010020897723734379 = 0.0029014975298196077 + 0.001 * 7.1194000244140625
Epoch 960, val loss: 1.7666815519332886
Epoch 970, training loss: 0.009962365962564945 = 0.0028291523922234774 + 0.001 * 7.133213520050049
Epoch 970, val loss: 1.7715100049972534
Epoch 980, training loss: 0.009880207479000092 = 0.0027600382454693317 + 0.001 * 7.120169162750244
Epoch 980, val loss: 1.7762097120285034
Epoch 990, training loss: 0.009794040583074093 = 0.002693997463211417 + 0.001 * 7.100042819976807
Epoch 990, val loss: 1.7807797193527222
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8239325250395362
=== training gcn model ===
Epoch 0, training loss: 1.947479486465454 = 1.9388827085494995 + 0.001 * 8.596793174743652
Epoch 0, val loss: 1.9357175827026367
Epoch 10, training loss: 1.938232421875 = 1.9296356439590454 + 0.001 * 8.596732139587402
Epoch 10, val loss: 1.9271502494812012
Epoch 20, training loss: 1.926712155342102 = 1.9181156158447266 + 0.001 * 8.596522331237793
Epoch 20, val loss: 1.9160776138305664
Epoch 30, training loss: 1.9103158712387085 = 1.9017198085784912 + 0.001 * 8.596088409423828
Epoch 30, val loss: 1.8998639583587646
Epoch 40, training loss: 1.8857858180999756 = 1.8771907091140747 + 0.001 * 8.595073699951172
Epoch 40, val loss: 1.8755316734313965
Epoch 50, training loss: 1.8517537117004395 = 1.843161702156067 + 0.001 * 8.59199333190918
Epoch 50, val loss: 1.8433444499969482
Epoch 60, training loss: 1.8156777620315552 = 1.807098150253296 + 0.001 * 8.579639434814453
Epoch 60, val loss: 1.8131537437438965
Epoch 70, training loss: 1.7865813970565796 = 1.7780637741088867 + 0.001 * 8.517593383789062
Epoch 70, val loss: 1.7906410694122314
Epoch 80, training loss: 1.7489467859268188 = 1.7407461404800415 + 0.001 * 8.2006254196167
Epoch 80, val loss: 1.7574998140335083
Epoch 90, training loss: 1.695855975151062 = 1.6877981424331665 + 0.001 * 8.057848930358887
Epoch 90, val loss: 1.7115066051483154
Epoch 100, training loss: 1.6239075660705566 = 1.6159712076187134 + 0.001 * 7.936402320861816
Epoch 100, val loss: 1.6531331539154053
Epoch 110, training loss: 1.5404362678527832 = 1.532675862312317 + 0.001 * 7.760376453399658
Epoch 110, val loss: 1.587719440460205
Epoch 120, training loss: 1.456993818283081 = 1.4493731260299683 + 0.001 * 7.620715618133545
Epoch 120, val loss: 1.5244845151901245
Epoch 130, training loss: 1.377137303352356 = 1.3695650100708008 + 0.001 * 7.5723419189453125
Epoch 130, val loss: 1.4658254384994507
Epoch 140, training loss: 1.2970997095108032 = 1.2896336317062378 + 0.001 * 7.466123104095459
Epoch 140, val loss: 1.4081957340240479
Epoch 150, training loss: 1.2147754430770874 = 1.2073856592178345 + 0.001 * 7.389821529388428
Epoch 150, val loss: 1.3496509790420532
Epoch 160, training loss: 1.1313068866729736 = 1.1239434480667114 + 0.001 * 7.3633880615234375
Epoch 160, val loss: 1.2913742065429688
Epoch 170, training loss: 1.049476981163025 = 1.0421332120895386 + 0.001 * 7.343781471252441
Epoch 170, val loss: 1.2358102798461914
Epoch 180, training loss: 0.9708969593048096 = 0.9635683298110962 + 0.001 * 7.328647136688232
Epoch 180, val loss: 1.1840224266052246
Epoch 190, training loss: 0.8954094052314758 = 0.8880890607833862 + 0.001 * 7.3203511238098145
Epoch 190, val loss: 1.1351913213729858
Epoch 200, training loss: 0.8221113085746765 = 0.8147946000099182 + 0.001 * 7.316681385040283
Epoch 200, val loss: 1.088762879371643
Epoch 210, training loss: 0.7508583664894104 = 0.7435433864593506 + 0.001 * 7.314979553222656
Epoch 210, val loss: 1.0453975200653076
Epoch 220, training loss: 0.6828736662864685 = 0.6755599975585938 + 0.001 * 7.3136467933654785
Epoch 220, val loss: 1.006187915802002
Epoch 230, training loss: 0.6201807260513306 = 0.6128678917884827 + 0.001 * 7.31281852722168
Epoch 230, val loss: 0.9729296565055847
Epoch 240, training loss: 0.5637850761413574 = 0.5564728379249573 + 0.001 * 7.312265396118164
Epoch 240, val loss: 0.9466200470924377
Epoch 250, training loss: 0.51298987865448 = 0.5056780576705933 + 0.001 * 7.311817646026611
Epoch 250, val loss: 0.9263358116149902
Epoch 260, training loss: 0.46599841117858887 = 0.4586864709854126 + 0.001 * 7.311948776245117
Epoch 260, val loss: 0.9101312756538391
Epoch 270, training loss: 0.4211178421974182 = 0.4138065278530121 + 0.001 * 7.311328411102295
Epoch 270, val loss: 0.8962894678115845
Epoch 280, training loss: 0.37756454944610596 = 0.370253324508667 + 0.001 * 7.31121301651001
Epoch 280, val loss: 0.8837900757789612
Epoch 290, training loss: 0.33568060398101807 = 0.32836949825286865 + 0.001 * 7.31109619140625
Epoch 290, val loss: 0.872590959072113
Epoch 300, training loss: 0.29652267694473267 = 0.28921157121658325 + 0.001 * 7.3111114501953125
Epoch 300, val loss: 0.8632489442825317
Epoch 310, training loss: 0.2610538601875305 = 0.25374147295951843 + 0.001 * 7.312382221221924
Epoch 310, val loss: 0.8565391898155212
Epoch 320, training loss: 0.2295726090669632 = 0.22226089239120483 + 0.001 * 7.311715126037598
Epoch 320, val loss: 0.8529962301254272
Epoch 330, training loss: 0.20186448097229004 = 0.19455182552337646 + 0.001 * 7.312659740447998
Epoch 330, val loss: 0.8528566360473633
Epoch 340, training loss: 0.17753338813781738 = 0.17022083699703217 + 0.001 * 7.312544345855713
Epoch 340, val loss: 0.8557599186897278
Epoch 350, training loss: 0.1562102884054184 = 0.14889781177043915 + 0.001 * 7.312478065490723
Epoch 350, val loss: 0.8613196015357971
Epoch 360, training loss: 0.13758108019828796 = 0.13026896119117737 + 0.001 * 7.312119007110596
Epoch 360, val loss: 0.869076669216156
Epoch 370, training loss: 0.12135963141918182 = 0.11404500901699066 + 0.001 * 7.314621925354004
Epoch 370, val loss: 0.8786250352859497
Epoch 380, training loss: 0.10726334154605865 = 0.0999520942568779 + 0.001 * 7.311246395111084
Epoch 380, val loss: 0.8894601464271545
Epoch 390, training loss: 0.09503993391990662 = 0.08772820234298706 + 0.001 * 7.311732292175293
Epoch 390, val loss: 0.9012659192085266
Epoch 400, training loss: 0.08445222675800323 = 0.07714298367500305 + 0.001 * 7.30924129486084
Epoch 400, val loss: 0.9137364029884338
Epoch 410, training loss: 0.07529392838478088 = 0.06798645853996277 + 0.001 * 7.307467937469482
Epoch 410, val loss: 0.9266282320022583
Epoch 420, training loss: 0.06738287210464478 = 0.06007491052150726 + 0.001 * 7.307959079742432
Epoch 420, val loss: 0.9397860765457153
Epoch 430, training loss: 0.060547858476638794 = 0.05324602499604225 + 0.001 * 7.3018341064453125
Epoch 430, val loss: 0.9530698657035828
Epoch 440, training loss: 0.054650116711854935 = 0.04735236242413521 + 0.001 * 7.297754764556885
Epoch 440, val loss: 0.9663119912147522
Epoch 450, training loss: 0.04955830052495003 = 0.04226614162325859 + 0.001 * 7.292158126831055
Epoch 450, val loss: 0.9794393181800842
Epoch 460, training loss: 0.04515886679291725 = 0.03787237033247948 + 0.001 * 7.286497592926025
Epoch 460, val loss: 0.9922822117805481
Epoch 470, training loss: 0.041353076696395874 = 0.03406988084316254 + 0.001 * 7.283195972442627
Epoch 470, val loss: 1.0048280954360962
Epoch 480, training loss: 0.03804812580347061 = 0.030771777033805847 + 0.001 * 7.276348114013672
Epoch 480, val loss: 1.017001748085022
Epoch 490, training loss: 0.0352092906832695 = 0.02790272794663906 + 0.001 * 7.30656099319458
Epoch 490, val loss: 1.028791069984436
Epoch 500, training loss: 0.032679956406354904 = 0.025398297235369682 + 0.001 * 7.281658172607422
Epoch 500, val loss: 1.0402013063430786
Epoch 510, training loss: 0.030476268380880356 = 0.0232053454965353 + 0.001 * 7.27092170715332
Epoch 510, val loss: 1.051209568977356
Epoch 520, training loss: 0.02852446958422661 = 0.021278953179717064 + 0.001 * 7.245516777038574
Epoch 520, val loss: 1.0618313550949097
Epoch 530, training loss: 0.026818109676241875 = 0.01957911252975464 + 0.001 * 7.238997459411621
Epoch 530, val loss: 1.0720534324645996
Epoch 540, training loss: 0.025290487334132195 = 0.018069980666041374 + 0.001 * 7.22050666809082
Epoch 540, val loss: 1.0819138288497925
Epoch 550, training loss: 0.023955686017870903 = 0.016719721257686615 + 0.001 * 7.235964298248291
Epoch 550, val loss: 1.091490626335144
Epoch 560, training loss: 0.02271181531250477 = 0.015505760908126831 + 0.001 * 7.2060546875
Epoch 560, val loss: 1.1007803678512573
Epoch 570, training loss: 0.021654417738318443 = 0.01441092137247324 + 0.001 * 7.243495464324951
Epoch 570, val loss: 1.1097859144210815
Epoch 580, training loss: 0.02062247321009636 = 0.013422198593616486 + 0.001 * 7.200274467468262
Epoch 580, val loss: 1.1185016632080078
Epoch 590, training loss: 0.019781963899731636 = 0.012527759186923504 + 0.001 * 7.254204750061035
Epoch 590, val loss: 1.126909613609314
Epoch 600, training loss: 0.018917487934231758 = 0.011717773042619228 + 0.001 * 7.199714660644531
Epoch 600, val loss: 1.1349880695343018
Epoch 610, training loss: 0.01816560886800289 = 0.010982810519635677 + 0.001 * 7.182797908782959
Epoch 610, val loss: 1.1427903175354004
Epoch 620, training loss: 0.01750892587006092 = 0.0103141563013196 + 0.001 * 7.194769859313965
Epoch 620, val loss: 1.1503642797470093
Epoch 630, training loss: 0.016898099333047867 = 0.009703033603727818 + 0.001 * 7.195065021514893
Epoch 630, val loss: 1.1576566696166992
Epoch 640, training loss: 0.016339385882019997 = 0.009140389040112495 + 0.001 * 7.198996543884277
Epoch 640, val loss: 1.1647006273269653
Epoch 650, training loss: 0.015788672491908073 = 0.008620211854577065 + 0.001 * 7.168459892272949
Epoch 650, val loss: 1.1715087890625
Epoch 660, training loss: 0.015311570838093758 = 0.00813865102827549 + 0.001 * 7.172919273376465
Epoch 660, val loss: 1.1780920028686523
Epoch 670, training loss: 0.014853550121188164 = 0.007693352177739143 + 0.001 * 7.160197734832764
Epoch 670, val loss: 1.18447744846344
Epoch 680, training loss: 0.014416872523725033 = 0.007281665690243244 + 0.001 * 7.135206699371338
Epoch 680, val loss: 1.1906381845474243
Epoch 690, training loss: 0.014036063104867935 = 0.006901243235915899 + 0.001 * 7.134819507598877
Epoch 690, val loss: 1.1966280937194824
Epoch 700, training loss: 0.013688063248991966 = 0.006549620535224676 + 0.001 * 7.1384429931640625
Epoch 700, val loss: 1.202399730682373
Epoch 710, training loss: 0.013395298272371292 = 0.006224364973604679 + 0.001 * 7.170932769775391
Epoch 710, val loss: 1.207995891571045
Epoch 720, training loss: 0.013053925707936287 = 0.00592349749058485 + 0.001 * 7.130427837371826
Epoch 720, val loss: 1.2134315967559814
Epoch 730, training loss: 0.012809725478291512 = 0.005644859746098518 + 0.001 * 7.164865970611572
Epoch 730, val loss: 1.2186682224273682
Epoch 740, training loss: 0.012563386932015419 = 0.005386498291045427 + 0.001 * 7.176888942718506
Epoch 740, val loss: 1.2237352132797241
Epoch 750, training loss: 0.012254253961145878 = 0.0051465206779539585 + 0.001 * 7.107732772827148
Epoch 750, val loss: 1.2286758422851562
Epoch 760, training loss: 0.012043098919093609 = 0.004923396278172731 + 0.001 * 7.119702339172363
Epoch 760, val loss: 1.2334357500076294
Epoch 770, training loss: 0.011876779608428478 = 0.0047156293876469135 + 0.001 * 7.161149978637695
Epoch 770, val loss: 1.2380353212356567
Epoch 780, training loss: 0.011633848771452904 = 0.004521897993981838 + 0.001 * 7.111950397491455
Epoch 780, val loss: 1.242522120475769
Epoch 790, training loss: 0.0114823579788208 = 0.004340991377830505 + 0.001 * 7.141366481781006
Epoch 790, val loss: 1.2468364238739014
Epoch 800, training loss: 0.011286204680800438 = 0.004171912558376789 + 0.001 * 7.114292144775391
Epoch 800, val loss: 1.251054048538208
Epoch 810, training loss: 0.011144354939460754 = 0.004013580735772848 + 0.001 * 7.130774021148682
Epoch 810, val loss: 1.2551360130310059
Epoch 820, training loss: 0.010992453433573246 = 0.0038651181384921074 + 0.001 * 7.127335071563721
Epoch 820, val loss: 1.2591246366500854
Epoch 830, training loss: 0.010812293738126755 = 0.0037257522344589233 + 0.001 * 7.086541652679443
Epoch 830, val loss: 1.2629560232162476
Epoch 840, training loss: 0.010709518566727638 = 0.0035947291180491447 + 0.001 * 7.11478853225708
Epoch 840, val loss: 1.2666873931884766
Epoch 850, training loss: 0.010541149415075779 = 0.003471429692581296 + 0.001 * 7.069719314575195
Epoch 850, val loss: 1.2702959775924683
Epoch 860, training loss: 0.01044920738786459 = 0.003355248598381877 + 0.001 * 7.093958854675293
Epoch 860, val loss: 1.2738298177719116
Epoch 870, training loss: 0.010333515703678131 = 0.0032456666231155396 + 0.001 * 7.087848663330078
Epoch 870, val loss: 1.2772349119186401
Epoch 880, training loss: 0.010231280699372292 = 0.003142161527648568 + 0.001 * 7.089118957519531
Epoch 880, val loss: 1.2805520296096802
Epoch 890, training loss: 0.010135535150766373 = 0.003044313285499811 + 0.001 * 7.091221332550049
Epoch 890, val loss: 1.2837892770767212
Epoch 900, training loss: 0.010017411783337593 = 0.0029517500661313534 + 0.001 * 7.065661907196045
Epoch 900, val loss: 1.2868905067443848
Epoch 910, training loss: 0.009937186725437641 = 0.002864031819626689 + 0.001 * 7.073154926300049
Epoch 910, val loss: 1.2899339199066162
Epoch 920, training loss: 0.009828034788370132 = 0.002780852373689413 + 0.001 * 7.047182083129883
Epoch 920, val loss: 1.2928804159164429
Epoch 930, training loss: 0.009785919450223446 = 0.0027019199915230274 + 0.001 * 7.083999156951904
Epoch 930, val loss: 1.2957017421722412
Epoch 940, training loss: 0.009697658941149712 = 0.0026269773952662945 + 0.001 * 7.070681571960449
Epoch 940, val loss: 1.2985031604766846
Epoch 950, training loss: 0.009610207751393318 = 0.002555704675614834 + 0.001 * 7.054502487182617
Epoch 950, val loss: 1.3011873960494995
Epoch 960, training loss: 0.009627381339669228 = 0.0024878638796508312 + 0.001 * 7.139516830444336
Epoch 960, val loss: 1.3038297891616821
Epoch 970, training loss: 0.009470265358686447 = 0.0024233560543507338 + 0.001 * 7.046909332275391
Epoch 970, val loss: 1.3063503503799438
Epoch 980, training loss: 0.009416216984391212 = 0.0023619201965630054 + 0.001 * 7.054296970367432
Epoch 980, val loss: 1.308819055557251
Epoch 990, training loss: 0.009354357607662678 = 0.0023033327888697386 + 0.001 * 7.051024913787842
Epoch 990, val loss: 1.311205267906189
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8265682656826568
=== training gcn model ===
Epoch 0, training loss: 1.9789174795150757 = 1.970320701599121 + 0.001 * 8.596796035766602
Epoch 0, val loss: 1.9717520475387573
Epoch 10, training loss: 1.9680358171463013 = 1.9594390392303467 + 0.001 * 8.596750259399414
Epoch 10, val loss: 1.9613147974014282
Epoch 20, training loss: 1.9547321796417236 = 1.9461356401443481 + 0.001 * 8.59658432006836
Epoch 20, val loss: 1.9480714797973633
Epoch 30, training loss: 1.936136245727539 = 1.9275400638580322 + 0.001 * 8.596224784851074
Epoch 30, val loss: 1.9290764331817627
Epoch 40, training loss: 1.9085419178009033 = 1.8999465703964233 + 0.001 * 8.59537124633789
Epoch 40, val loss: 1.9006965160369873
Epoch 50, training loss: 1.8695182800292969 = 1.8609251976013184 + 0.001 * 8.593063354492188
Epoch 50, val loss: 1.8617922067642212
Epoch 60, training loss: 1.8263591527938843 = 1.817773699760437 + 0.001 * 8.585503578186035
Epoch 60, val loss: 1.8229693174362183
Epoch 70, training loss: 1.7960516214370728 = 1.787498950958252 + 0.001 * 8.552663803100586
Epoch 70, val loss: 1.799880862236023
Epoch 80, training loss: 1.7654799222946167 = 1.7571532726287842 + 0.001 * 8.326665878295898
Epoch 80, val loss: 1.7737420797348022
Epoch 90, training loss: 1.7226788997650146 = 1.7144908905029297 + 0.001 * 8.188007354736328
Epoch 90, val loss: 1.7383753061294556
Epoch 100, training loss: 1.6618365049362183 = 1.6537338495254517 + 0.001 * 8.102676391601562
Epoch 100, val loss: 1.6897368431091309
Epoch 110, training loss: 1.5822007656097412 = 1.5741395950317383 + 0.001 * 8.06116008758545
Epoch 110, val loss: 1.6264493465423584
Epoch 120, training loss: 1.4956567287445068 = 1.4876214265823364 + 0.001 * 8.0352783203125
Epoch 120, val loss: 1.560691237449646
Epoch 130, training loss: 1.413985252380371 = 1.4060088396072388 + 0.001 * 7.976415157318115
Epoch 130, val loss: 1.5006771087646484
Epoch 140, training loss: 1.3382560014724731 = 1.3305232524871826 + 0.001 * 7.732747554779053
Epoch 140, val loss: 1.4476841688156128
Epoch 150, training loss: 1.26604163646698 = 1.258438229560852 + 0.001 * 7.603382587432861
Epoch 150, val loss: 1.3984612226486206
Epoch 160, training loss: 1.1959238052368164 = 1.188399076461792 + 0.001 * 7.5247673988342285
Epoch 160, val loss: 1.351880669593811
Epoch 170, training loss: 1.126565933227539 = 1.119098424911499 + 0.001 * 7.467506408691406
Epoch 170, val loss: 1.3076080083847046
Epoch 180, training loss: 1.055654764175415 = 1.0482028722763062 + 0.001 * 7.451833724975586
Epoch 180, val loss: 1.2629387378692627
Epoch 190, training loss: 0.9813225269317627 = 0.9738776087760925 + 0.001 * 7.444910526275635
Epoch 190, val loss: 1.2152093648910522
Epoch 200, training loss: 0.903355598449707 = 0.8959182500839233 + 0.001 * 7.437320232391357
Epoch 200, val loss: 1.1640361547470093
Epoch 210, training loss: 0.8226510882377625 = 0.815223217010498 + 0.001 * 7.427891254425049
Epoch 210, val loss: 1.1105009317398071
Epoch 220, training loss: 0.7410088777542114 = 0.7335941195487976 + 0.001 * 7.414732933044434
Epoch 220, val loss: 1.0567320585250854
Epoch 230, training loss: 0.6614695191383362 = 0.6540738940238953 + 0.001 * 7.3956475257873535
Epoch 230, val loss: 1.0060051679611206
Epoch 240, training loss: 0.5875281691551208 = 0.5801600813865662 + 0.001 * 7.368070125579834
Epoch 240, val loss: 0.9620146751403809
Epoch 250, training loss: 0.5213004946708679 = 0.513956606388092 + 0.001 * 7.343909740447998
Epoch 250, val loss: 0.9273244738578796
Epoch 260, training loss: 0.46301573514938354 = 0.4556887149810791 + 0.001 * 7.327012538909912
Epoch 260, val loss: 0.9020243883132935
Epoch 270, training loss: 0.41176530718803406 = 0.4044489860534668 + 0.001 * 7.316319465637207
Epoch 270, val loss: 0.8847538232803345
Epoch 280, training loss: 0.3661862313747406 = 0.35887888073921204 + 0.001 * 7.307343006134033
Epoch 280, val loss: 0.8739209771156311
Epoch 290, training loss: 0.3249490559101105 = 0.3176460564136505 + 0.001 * 7.302985191345215
Epoch 290, val loss: 0.8679114580154419
Epoch 300, training loss: 0.28708815574645996 = 0.27978330850601196 + 0.001 * 7.3048415184021
Epoch 300, val loss: 0.8658861517906189
Epoch 310, training loss: 0.25200119614601135 = 0.24470677971839905 + 0.001 * 7.294417381286621
Epoch 310, val loss: 0.8668121695518494
Epoch 320, training loss: 0.21958909928798676 = 0.21229766309261322 + 0.001 * 7.291438102722168
Epoch 320, val loss: 0.8702628016471863
Epoch 330, training loss: 0.19007891416549683 = 0.18278108537197113 + 0.001 * 7.2978339195251465
Epoch 330, val loss: 0.8761197924613953
Epoch 340, training loss: 0.16383442282676697 = 0.1565389633178711 + 0.001 * 7.295462608337402
Epoch 340, val loss: 0.8843843340873718
Epoch 350, training loss: 0.14108766615390778 = 0.13380062580108643 + 0.001 * 7.287041664123535
Epoch 350, val loss: 0.8948042392730713
Epoch 360, training loss: 0.12177034467458725 = 0.114481620490551 + 0.001 * 7.288723468780518
Epoch 360, val loss: 0.9071138501167297
Epoch 370, training loss: 0.10556076467037201 = 0.09826760739088058 + 0.001 * 7.293156623840332
Epoch 370, val loss: 0.9210553765296936
Epoch 380, training loss: 0.09202513843774796 = 0.08473175764083862 + 0.001 * 7.293381214141846
Epoch 380, val loss: 0.9361740946769714
Epoch 390, training loss: 0.08073567599058151 = 0.07344350963830948 + 0.001 * 7.292162895202637
Epoch 390, val loss: 0.9519968628883362
Epoch 400, training loss: 0.07130378484725952 = 0.06401219964027405 + 0.001 * 7.291585445404053
Epoch 400, val loss: 0.9682437181472778
Epoch 410, training loss: 0.06339731067419052 = 0.056108199059963226 + 0.001 * 7.289107799530029
Epoch 410, val loss: 0.984585702419281
Epoch 420, training loss: 0.05674602836370468 = 0.0494597963988781 + 0.001 * 7.286230087280273
Epoch 420, val loss: 1.0008803606033325
Epoch 430, training loss: 0.05112799257040024 = 0.04384426400065422 + 0.001 * 7.283728122711182
Epoch 430, val loss: 1.0169808864593506
Epoch 440, training loss: 0.04636680707335472 = 0.03907933831214905 + 0.001 * 7.287469863891602
Epoch 440, val loss: 1.0327731370925903
Epoch 450, training loss: 0.04229757562279701 = 0.035015471279621124 + 0.001 * 7.282104015350342
Epoch 450, val loss: 1.0482289791107178
Epoch 460, training loss: 0.03880966827273369 = 0.031532131135463715 + 0.001 * 7.277535438537598
Epoch 460, val loss: 1.0632613897323608
Epoch 470, training loss: 0.03581925109028816 = 0.028531353920698166 + 0.001 * 7.287896633148193
Epoch 470, val loss: 1.0778390169143677
Epoch 480, training loss: 0.03320499509572983 = 0.02593296952545643 + 0.001 * 7.272023677825928
Epoch 480, val loss: 1.0919677019119263
Epoch 490, training loss: 0.030942140147089958 = 0.02367165870964527 + 0.001 * 7.270481109619141
Epoch 490, val loss: 1.105652928352356
Epoch 500, training loss: 0.028968581929802895 = 0.021694153547286987 + 0.001 * 7.274428367614746
Epoch 500, val loss: 1.1188961267471313
Epoch 510, training loss: 0.027223780751228333 = 0.01995674893260002 + 0.001 * 7.267032623291016
Epoch 510, val loss: 1.1316975355148315
Epoch 520, training loss: 0.02568615972995758 = 0.018423255532979965 + 0.001 * 7.262902736663818
Epoch 520, val loss: 1.1440895795822144
Epoch 530, training loss: 0.0243271104991436 = 0.017063699662685394 + 0.001 * 7.263410568237305
Epoch 530, val loss: 1.1560794115066528
Epoch 540, training loss: 0.023113952949643135 = 0.015853337943553925 + 0.001 * 7.26061487197876
Epoch 540, val loss: 1.1676788330078125
Epoch 550, training loss: 0.022023038938641548 = 0.014771411195397377 + 0.001 * 7.251626968383789
Epoch 550, val loss: 1.1789109706878662
Epoch 560, training loss: 0.0210484117269516 = 0.013800615444779396 + 0.001 * 7.247796535491943
Epoch 560, val loss: 1.189788579940796
Epoch 570, training loss: 0.020178884267807007 = 0.012926340103149414 + 0.001 * 7.252544403076172
Epoch 570, val loss: 1.2003315687179565
Epoch 580, training loss: 0.019396953284740448 = 0.012136347591876984 + 0.001 * 7.260604381561279
Epoch 580, val loss: 1.2105491161346436
Epoch 590, training loss: 0.018673773854970932 = 0.011420209892094135 + 0.001 * 7.253562927246094
Epoch 590, val loss: 1.2204488515853882
Epoch 600, training loss: 0.01801314391195774 = 0.010769026353955269 + 0.001 * 7.244117259979248
Epoch 600, val loss: 1.2300533056259155
Epoch 610, training loss: 0.017411144450306892 = 0.010175173170864582 + 0.001 * 7.235971450805664
Epoch 610, val loss: 1.2393766641616821
Epoch 620, training loss: 0.016861777752637863 = 0.00963213387876749 + 0.001 * 7.22964334487915
Epoch 620, val loss: 1.2484345436096191
Epoch 630, training loss: 0.016352403908967972 = 0.009134262800216675 + 0.001 * 7.2181396484375
Epoch 630, val loss: 1.2572306394577026
Epoch 640, training loss: 0.015890296548604965 = 0.008676701225340366 + 0.001 * 7.213594436645508
Epoch 640, val loss: 1.2657784223556519
Epoch 650, training loss: 0.015477078035473824 = 0.008255145512521267 + 0.001 * 7.2219319343566895
Epoch 650, val loss: 1.274094820022583
Epoch 660, training loss: 0.01509106531739235 = 0.007865948602557182 + 0.001 * 7.225115776062012
Epoch 660, val loss: 1.2821775674819946
Epoch 670, training loss: 0.014706009067595005 = 0.007505829446017742 + 0.001 * 7.200179100036621
Epoch 670, val loss: 1.2900679111480713
Epoch 680, training loss: 0.014371935278177261 = 0.007171838544309139 + 0.001 * 7.20009708404541
Epoch 680, val loss: 1.2977571487426758
Epoch 690, training loss: 0.014066137373447418 = 0.0068613942712545395 + 0.001 * 7.204743385314941
Epoch 690, val loss: 1.3052482604980469
Epoch 700, training loss: 0.013793790712952614 = 0.006571226753294468 + 0.001 * 7.222563743591309
Epoch 700, val loss: 1.3126009702682495
Epoch 710, training loss: 0.01349713560193777 = 0.006297675892710686 + 0.001 * 7.199459552764893
Epoch 710, val loss: 1.3198962211608887
Epoch 720, training loss: 0.013218754902482033 = 0.006038071122020483 + 0.001 * 7.1806840896606445
Epoch 720, val loss: 1.3271558284759521
Epoch 730, training loss: 0.012985044158995152 = 0.005791194271296263 + 0.001 * 7.193849563598633
Epoch 730, val loss: 1.3343877792358398
Epoch 740, training loss: 0.012741127982735634 = 0.005556661635637283 + 0.001 * 7.184465408325195
Epoch 740, val loss: 1.3415695428848267
Epoch 750, training loss: 0.012515561655163765 = 0.005334204062819481 + 0.001 * 7.1813578605651855
Epoch 750, val loss: 1.3486847877502441
Epoch 760, training loss: 0.012327433563768864 = 0.005123695824295282 + 0.001 * 7.203737258911133
Epoch 760, val loss: 1.3557153940200806
Epoch 770, training loss: 0.012138212099671364 = 0.004924707114696503 + 0.001 * 7.213504791259766
Epoch 770, val loss: 1.3626328706741333
Epoch 780, training loss: 0.011930342763662338 = 0.004736808594316244 + 0.001 * 7.1935343742370605
Epoch 780, val loss: 1.3694368600845337
Epoch 790, training loss: 0.011753004044294357 = 0.004559459630399942 + 0.001 * 7.193543910980225
Epoch 790, val loss: 1.3761241436004639
Epoch 800, training loss: 0.011580612510442734 = 0.004392111208289862 + 0.001 * 7.18850040435791
Epoch 800, val loss: 1.3826570510864258
Epoch 810, training loss: 0.011390570551156998 = 0.004234205465763807 + 0.001 * 7.156365394592285
Epoch 810, val loss: 1.3890537023544312
Epoch 820, training loss: 0.011249817907810211 = 0.004085153341293335 + 0.001 * 7.1646647453308105
Epoch 820, val loss: 1.3953144550323486
Epoch 830, training loss: 0.011136065237224102 = 0.003944437485188246 + 0.001 * 7.191627502441406
Epoch 830, val loss: 1.4014406204223633
Epoch 840, training loss: 0.011000072583556175 = 0.003811493283137679 + 0.001 * 7.1885786056518555
Epoch 840, val loss: 1.4074498414993286
Epoch 850, training loss: 0.01085948757827282 = 0.003685788717120886 + 0.001 * 7.173698902130127
Epoch 850, val loss: 1.41335129737854
Epoch 860, training loss: 0.010735653340816498 = 0.0035668620839715004 + 0.001 * 7.168790817260742
Epoch 860, val loss: 1.419127345085144
Epoch 870, training loss: 0.010616867803037167 = 0.0034542358480393887 + 0.001 * 7.162631511688232
Epoch 870, val loss: 1.424774169921875
Epoch 880, training loss: 0.010488148778676987 = 0.003347532358020544 + 0.001 * 7.140616416931152
Epoch 880, val loss: 1.4303009510040283
Epoch 890, training loss: 0.010381080210208893 = 0.0032463495153933764 + 0.001 * 7.134730815887451
Epoch 890, val loss: 1.4357084035873413
Epoch 900, training loss: 0.010287636891007423 = 0.0031503071077167988 + 0.001 * 7.137330055236816
Epoch 900, val loss: 1.4410091638565063
Epoch 910, training loss: 0.010187124833464622 = 0.003059046808630228 + 0.001 * 7.128077983856201
Epoch 910, val loss: 1.4462119340896606
Epoch 920, training loss: 0.01011342741549015 = 0.0029722508043050766 + 0.001 * 7.141175746917725
Epoch 920, val loss: 1.4513124227523804
Epoch 930, training loss: 0.0100290197879076 = 0.0028896424919366837 + 0.001 * 7.139377117156982
Epoch 930, val loss: 1.4562897682189941
Epoch 940, training loss: 0.009939529933035374 = 0.00281098997220397 + 0.001 * 7.128539562225342
Epoch 940, val loss: 1.46116304397583
Epoch 950, training loss: 0.009863517247140408 = 0.0027360031381249428 + 0.001 * 7.127513885498047
Epoch 950, val loss: 1.4659541845321655
Epoch 960, training loss: 0.009792047552764416 = 0.0026644649915397167 + 0.001 * 7.12758207321167
Epoch 960, val loss: 1.4706438779830933
Epoch 970, training loss: 0.009725863113999367 = 0.0025961173232644796 + 0.001 * 7.1297454833984375
Epoch 970, val loss: 1.4752482175827026
Epoch 980, training loss: 0.009669388644397259 = 0.0025307973846793175 + 0.001 * 7.1385908126831055
Epoch 980, val loss: 1.4797577857971191
Epoch 990, training loss: 0.009579864330589771 = 0.0024683072697371244 + 0.001 * 7.111556529998779
Epoch 990, val loss: 1.4842100143432617
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8228782287822879
The final CL Acc:0.73580, 0.02518, The final GNN Acc:0.82446, 0.00155
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13184])
remove edge: torch.Size([2, 7962])
updated graph: torch.Size([2, 10590])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9394605159759521 = 1.9308637380599976 + 0.001 * 8.5968017578125
Epoch 0, val loss: 1.9200035333633423
Epoch 10, training loss: 1.9301592111587524 = 1.9215624332427979 + 0.001 * 8.596735000610352
Epoch 10, val loss: 1.9112135171890259
Epoch 20, training loss: 1.9186509847640991 = 1.9100544452667236 + 0.001 * 8.596539497375488
Epoch 20, val loss: 1.899946689605713
Epoch 30, training loss: 1.9023172855377197 = 1.8937212228775024 + 0.001 * 8.596104621887207
Epoch 30, val loss: 1.8836270570755005
Epoch 40, training loss: 1.878225564956665 = 1.8696304559707642 + 0.001 * 8.595121383666992
Epoch 40, val loss: 1.8597480058670044
Epoch 50, training loss: 1.8452694416046143 = 1.836676836013794 + 0.001 * 8.59264087677002
Epoch 50, val loss: 1.8288973569869995
Epoch 60, training loss: 1.8092889785766602 = 1.800704002380371 + 0.001 * 8.584975242614746
Epoch 60, val loss: 1.7999975681304932
Epoch 70, training loss: 1.7745616436004639 = 1.7660104036331177 + 0.001 * 8.551288604736328
Epoch 70, val loss: 1.7743124961853027
Epoch 80, training loss: 1.7274978160858154 = 1.719200849533081 + 0.001 * 8.29694938659668
Epoch 80, val loss: 1.7335350513458252
Epoch 90, training loss: 1.6619833707809448 = 1.6539033651351929 + 0.001 * 8.080063819885254
Epoch 90, val loss: 1.6739656925201416
Epoch 100, training loss: 1.5762975215911865 = 1.5683448314666748 + 0.001 * 7.952645778656006
Epoch 100, val loss: 1.5980615615844727
Epoch 110, training loss: 1.479433298110962 = 1.471673846244812 + 0.001 * 7.759399890899658
Epoch 110, val loss: 1.5166789293289185
Epoch 120, training loss: 1.3801251649856567 = 1.372443675994873 + 0.001 * 7.6815104484558105
Epoch 120, val loss: 1.4388041496276855
Epoch 130, training loss: 1.2796963453292847 = 1.2720303535461426 + 0.001 * 7.666022777557373
Epoch 130, val loss: 1.3625147342681885
Epoch 140, training loss: 1.1782951354980469 = 1.1706595420837402 + 0.001 * 7.635609149932861
Epoch 140, val loss: 1.285606026649475
Epoch 150, training loss: 1.078285813331604 = 1.0706816911697388 + 0.001 * 7.60409688949585
Epoch 150, val loss: 1.2098031044006348
Epoch 160, training loss: 0.9829297661781311 = 0.9753677248954773 + 0.001 * 7.562062740325928
Epoch 160, val loss: 1.1387134790420532
Epoch 170, training loss: 0.8949414491653442 = 0.8874378204345703 + 0.001 * 7.503609657287598
Epoch 170, val loss: 1.075746774673462
Epoch 180, training loss: 0.8159008622169495 = 0.8084675073623657 + 0.001 * 7.4333600997924805
Epoch 180, val loss: 1.0217461585998535
Epoch 190, training loss: 0.7464222311973572 = 0.7390461564064026 + 0.001 * 7.3760528564453125
Epoch 190, val loss: 0.976877748966217
Epoch 200, training loss: 0.685631513595581 = 0.6782951354980469 + 0.001 * 7.33639669418335
Epoch 200, val loss: 0.9403476715087891
Epoch 210, training loss: 0.6313045024871826 = 0.6239876747131348 + 0.001 * 7.31680154800415
Epoch 210, val loss: 0.9105290174484253
Epoch 220, training loss: 0.5812145471572876 = 0.5739066004753113 + 0.001 * 7.307969093322754
Epoch 220, val loss: 0.885850191116333
Epoch 230, training loss: 0.5340683460235596 = 0.526764988899231 + 0.001 * 7.303337097167969
Epoch 230, val loss: 0.8653874397277832
Epoch 240, training loss: 0.489376425743103 = 0.48207658529281616 + 0.001 * 7.299844741821289
Epoch 240, val loss: 0.8489378094673157
Epoch 250, training loss: 0.44691216945648193 = 0.43961596488952637 + 0.001 * 7.296211242675781
Epoch 250, val loss: 0.8365317583084106
Epoch 260, training loss: 0.40650326013565063 = 0.39921140670776367 + 0.001 * 7.291857719421387
Epoch 260, val loss: 0.8282835483551025
Epoch 270, training loss: 0.36822474002838135 = 0.3609350919723511 + 0.001 * 7.289645671844482
Epoch 270, val loss: 0.8244074583053589
Epoch 280, training loss: 0.3324292004108429 = 0.3251444101333618 + 0.001 * 7.284783363342285
Epoch 280, val loss: 0.8250182271003723
Epoch 290, training loss: 0.2994924783706665 = 0.29221028089523315 + 0.001 * 7.282201766967773
Epoch 290, val loss: 0.8301793932914734
Epoch 300, training loss: 0.2695019841194153 = 0.2622242867946625 + 0.001 * 7.277704238891602
Epoch 300, val loss: 0.8396034240722656
Epoch 310, training loss: 0.24232076108455658 = 0.23504510521888733 + 0.001 * 7.275652885437012
Epoch 310, val loss: 0.8526426553726196
Epoch 320, training loss: 0.21771402657032013 = 0.21044592559337616 + 0.001 * 7.268102169036865
Epoch 320, val loss: 0.8688293099403381
Epoch 330, training loss: 0.1954624056816101 = 0.18819618225097656 + 0.001 * 7.26621675491333
Epoch 330, val loss: 0.8878249526023865
Epoch 340, training loss: 0.17535394430160522 = 0.16809587180614471 + 0.001 * 7.258073329925537
Epoch 340, val loss: 0.9093596339225769
Epoch 350, training loss: 0.15720035135746002 = 0.14994919300079346 + 0.001 * 7.251151084899902
Epoch 350, val loss: 0.9330627918243408
Epoch 360, training loss: 0.14080238342285156 = 0.13355901837348938 + 0.001 * 7.2433695793151855
Epoch 360, val loss: 0.9586347937583923
Epoch 370, training loss: 0.12600292265415192 = 0.11876147985458374 + 0.001 * 7.2414374351501465
Epoch 370, val loss: 0.985729992389679
Epoch 380, training loss: 0.11265823990106583 = 0.1054290309548378 + 0.001 * 7.2292070388793945
Epoch 380, val loss: 1.0140902996063232
Epoch 390, training loss: 0.10069244354963303 = 0.09347186237573624 + 0.001 * 7.2205810546875
Epoch 390, val loss: 1.0434801578521729
Epoch 400, training loss: 0.09003693610429764 = 0.08281826972961426 + 0.001 * 7.218663215637207
Epoch 400, val loss: 1.0736193656921387
Epoch 410, training loss: 0.08060672879219055 = 0.07340041548013687 + 0.001 * 7.206312656402588
Epoch 410, val loss: 1.104101538658142
Epoch 420, training loss: 0.07234200090169907 = 0.06513610482215881 + 0.001 * 7.205894470214844
Epoch 420, val loss: 1.1345828771591187
Epoch 430, training loss: 0.06511838734149933 = 0.05792848765850067 + 0.001 * 7.1898956298828125
Epoch 430, val loss: 1.164677619934082
Epoch 440, training loss: 0.058863259851932526 = 0.05166758969426155 + 0.001 * 7.195671558380127
Epoch 440, val loss: 1.1941174268722534
Epoch 450, training loss: 0.05342938378453255 = 0.04623778536915779 + 0.001 * 7.191599369049072
Epoch 450, val loss: 1.2226574420928955
Epoch 460, training loss: 0.048712193965911865 = 0.04152718186378479 + 0.001 * 7.185009956359863
Epoch 460, val loss: 1.2501972913742065
Epoch 470, training loss: 0.04461143538355827 = 0.03743385523557663 + 0.001 * 7.177578449249268
Epoch 470, val loss: 1.276713490486145
Epoch 480, training loss: 0.04104429855942726 = 0.033867448568344116 + 0.001 * 7.176848411560059
Epoch 480, val loss: 1.3022618293762207
Epoch 490, training loss: 0.037924446165561676 = 0.030750425532460213 + 0.001 * 7.174018383026123
Epoch 490, val loss: 1.3268232345581055
Epoch 500, training loss: 0.035181839019060135 = 0.028017448261380196 + 0.001 * 7.164390563964844
Epoch 500, val loss: 1.3504638671875
Epoch 510, training loss: 0.0327770970761776 = 0.025613395497202873 + 0.001 * 7.16370153427124
Epoch 510, val loss: 1.373213529586792
Epoch 520, training loss: 0.03066307306289673 = 0.02349197492003441 + 0.001 * 7.171098709106445
Epoch 520, val loss: 1.3951027393341064
Epoch 530, training loss: 0.028785282745957375 = 0.021614205092191696 + 0.001 * 7.171077728271484
Epoch 530, val loss: 1.416172981262207
Epoch 540, training loss: 0.02710643783211708 = 0.019947022199630737 + 0.001 * 7.159414768218994
Epoch 540, val loss: 1.4364430904388428
Epoch 550, training loss: 0.025617187842726707 = 0.018462197855114937 + 0.001 * 7.154989242553711
Epoch 550, val loss: 1.4559921026229858
Epoch 560, training loss: 0.02429785393178463 = 0.01713571697473526 + 0.001 * 7.162136554718018
Epoch 560, val loss: 1.4748188257217407
Epoch 570, training loss: 0.023109015077352524 = 0.01594700664281845 + 0.001 * 7.162008285522461
Epoch 570, val loss: 1.4929662942886353
Epoch 580, training loss: 0.022035835310816765 = 0.014878644607961178 + 0.001 * 7.157190322875977
Epoch 580, val loss: 1.5105104446411133
Epoch 590, training loss: 0.02106373757123947 = 0.01391573529690504 + 0.001 * 7.148002624511719
Epoch 590, val loss: 1.5274351835250854
Epoch 600, training loss: 0.020191261544823647 = 0.01304542738944292 + 0.001 * 7.145834445953369
Epoch 600, val loss: 1.543791651725769
Epoch 610, training loss: 0.01940632425248623 = 0.012256791815161705 + 0.001 * 7.149532318115234
Epoch 610, val loss: 1.559600830078125
Epoch 620, training loss: 0.018681282177567482 = 0.011540009640157223 + 0.001 * 7.14127254486084
Epoch 620, val loss: 1.574860692024231
Epoch 630, training loss: 0.01803455501794815 = 0.010886802338063717 + 0.001 * 7.14775276184082
Epoch 630, val loss: 1.5896083116531372
Epoch 640, training loss: 0.01743798702955246 = 0.010290097445249557 + 0.001 * 7.147890090942383
Epoch 640, val loss: 1.6038600206375122
Epoch 650, training loss: 0.016880592331290245 = 0.009743636474013329 + 0.001 * 7.136955261230469
Epoch 650, val loss: 1.617673635482788
Epoch 660, training loss: 0.01637897454202175 = 0.009242040105164051 + 0.001 * 7.136934757232666
Epoch 660, val loss: 1.6310328245162964
Epoch 670, training loss: 0.015927758067846298 = 0.00878064800053835 + 0.001 * 7.147109031677246
Epoch 670, val loss: 1.643975853919983
Epoch 680, training loss: 0.015481590293347836 = 0.008355248719453812 + 0.001 * 7.126341342926025
Epoch 680, val loss: 1.6565409898757935
Epoch 690, training loss: 0.015091417357325554 = 0.0079622408375144 + 0.001 * 7.129175662994385
Epoch 690, val loss: 1.6687049865722656
Epoch 700, training loss: 0.014720411039888859 = 0.00759840989485383 + 0.001 * 7.122000694274902
Epoch 700, val loss: 1.6805202960968018
Epoch 710, training loss: 0.014403369277715683 = 0.0072609554044902325 + 0.001 * 7.14241361618042
Epoch 710, val loss: 1.6920080184936523
Epoch 720, training loss: 0.014083023183047771 = 0.0069474163465201855 + 0.001 * 7.135606288909912
Epoch 720, val loss: 1.7031232118606567
Epoch 730, training loss: 0.01377811748534441 = 0.006655573379248381 + 0.001 * 7.122543811798096
Epoch 730, val loss: 1.713948130607605
Epoch 740, training loss: 0.013518963009119034 = 0.006383479572832584 + 0.001 * 7.135483741760254
Epoch 740, val loss: 1.7244669198989868
Epoch 750, training loss: 0.013239381834864616 = 0.006129366345703602 + 0.001 * 7.110015392303467
Epoch 750, val loss: 1.7346587181091309
Epoch 760, training loss: 0.013011638075113297 = 0.005891718436032534 + 0.001 * 7.1199188232421875
Epoch 760, val loss: 1.744585394859314
Epoch 770, training loss: 0.012788929976522923 = 0.005669112782925367 + 0.001 * 7.119816780090332
Epoch 770, val loss: 1.7542215585708618
Epoch 780, training loss: 0.012577904388308525 = 0.005460336338728666 + 0.001 * 7.117568016052246
Epoch 780, val loss: 1.763617753982544
Epoch 790, training loss: 0.012378405779600143 = 0.005264226347208023 + 0.001 * 7.114179611206055
Epoch 790, val loss: 1.7727488279342651
Epoch 800, training loss: 0.012181859463453293 = 0.005079751834273338 + 0.001 * 7.102107524871826
Epoch 800, val loss: 1.781643033027649
Epoch 810, training loss: 0.01202036626636982 = 0.004906045272946358 + 0.001 * 7.114321231842041
Epoch 810, val loss: 1.7903003692626953
Epoch 820, training loss: 0.01183487381786108 = 0.004742267541587353 + 0.001 * 7.092606067657471
Epoch 820, val loss: 1.7987574338912964
Epoch 830, training loss: 0.011673801578581333 = 0.004587667994201183 + 0.001 * 7.0861334800720215
Epoch 830, val loss: 1.807006597518921
Epoch 840, training loss: 0.011528212577104568 = 0.004441551864147186 + 0.001 * 7.086659908294678
Epoch 840, val loss: 1.8150168657302856
Epoch 850, training loss: 0.011422354727983475 = 0.004303325898945332 + 0.001 * 7.119028091430664
Epoch 850, val loss: 1.822848916053772
Epoch 860, training loss: 0.011275144293904305 = 0.004172418266534805 + 0.001 * 7.102725028991699
Epoch 860, val loss: 1.8304657936096191
Epoch 870, training loss: 0.011153541505336761 = 0.004048324655741453 + 0.001 * 7.105216026306152
Epoch 870, val loss: 1.837913990020752
Epoch 880, training loss: 0.011026952415704727 = 0.003930567763745785 + 0.001 * 7.096384048461914
Epoch 880, val loss: 1.8451873064041138
Epoch 890, training loss: 0.010925717651844025 = 0.0038187315221875906 + 0.001 * 7.106986045837402
Epoch 890, val loss: 1.8522788286209106
Epoch 900, training loss: 0.010819094255566597 = 0.003712414763867855 + 0.001 * 7.1066789627075195
Epoch 900, val loss: 1.8592082262039185
Epoch 910, training loss: 0.010692760348320007 = 0.003611271735280752 + 0.001 * 7.081488609313965
Epoch 910, val loss: 1.8659769296646118
Epoch 920, training loss: 0.010605080984532833 = 0.00351496203802526 + 0.001 * 7.090118885040283
Epoch 920, val loss: 1.872568130493164
Epoch 930, training loss: 0.010499769821763039 = 0.0034231599420309067 + 0.001 * 7.076610088348389
Epoch 930, val loss: 1.8790305852890015
Epoch 940, training loss: 0.010410989634692669 = 0.003335605375468731 + 0.001 * 7.075384140014648
Epoch 940, val loss: 1.8853265047073364
Epoch 950, training loss: 0.010316038504242897 = 0.003252037800848484 + 0.001 * 7.064000129699707
Epoch 950, val loss: 1.891514539718628
Epoch 960, training loss: 0.010232431814074516 = 0.003172211581841111 + 0.001 * 7.060220241546631
Epoch 960, val loss: 1.897538423538208
Epoch 970, training loss: 0.01017705537378788 = 0.003095911582931876 + 0.001 * 7.081143856048584
Epoch 970, val loss: 1.9034597873687744
Epoch 980, training loss: 0.01009360421448946 = 0.0030229375697672367 + 0.001 * 7.070666313171387
Epoch 980, val loss: 1.90923011302948
Epoch 990, training loss: 0.010045846924185753 = 0.0029530739411711693 + 0.001 * 7.092772006988525
Epoch 990, val loss: 1.9148874282836914
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 1.9607129096984863 = 1.9521161317825317 + 0.001 * 8.59679889678955
Epoch 0, val loss: 1.9452589750289917
Epoch 10, training loss: 1.9503473043441772 = 1.9417505264282227 + 0.001 * 8.596725463867188
Epoch 10, val loss: 1.9352306127548218
Epoch 20, training loss: 1.937272310256958 = 1.9286757707595825 + 0.001 * 8.596517562866211
Epoch 20, val loss: 1.9221354722976685
Epoch 30, training loss: 1.9187947511672974 = 1.91019868850708 + 0.001 * 8.59608268737793
Epoch 30, val loss: 1.9032737016677856
Epoch 40, training loss: 1.891530990600586 = 1.882935881614685 + 0.001 * 8.595108985900879
Epoch 40, val loss: 1.8756542205810547
Epoch 50, training loss: 1.8536609411239624 = 1.8450684547424316 + 0.001 * 8.59244441986084
Epoch 50, val loss: 1.8390693664550781
Epoch 60, training loss: 1.8116440773010254 = 1.80306077003479 + 0.001 * 8.58330249786377
Epoch 60, val loss: 1.8027434349060059
Epoch 70, training loss: 1.7765196561813354 = 1.7679781913757324 + 0.001 * 8.541478157043457
Epoch 70, val loss: 1.7748833894729614
Epoch 80, training loss: 1.7342901229858398 = 1.7260597944259644 + 0.001 * 8.230341911315918
Epoch 80, val loss: 1.7365586757659912
Epoch 90, training loss: 1.6752090454101562 = 1.6672872304916382 + 0.001 * 7.921837329864502
Epoch 90, val loss: 1.682174801826477
Epoch 100, training loss: 1.5954104661941528 = 1.5877257585525513 + 0.001 * 7.684656620025635
Epoch 100, val loss: 1.6106369495391846
Epoch 110, training loss: 1.499265432357788 = 1.4916746616363525 + 0.001 * 7.590756893157959
Epoch 110, val loss: 1.526798129081726
Epoch 120, training loss: 1.396614670753479 = 1.3890328407287598 + 0.001 * 7.581818103790283
Epoch 120, val loss: 1.438453197479248
Epoch 130, training loss: 1.2899218797683716 = 1.2823517322540283 + 0.001 * 7.570140838623047
Epoch 130, val loss: 1.3482762575149536
Epoch 140, training loss: 1.1775106191635132 = 1.1699578762054443 + 0.001 * 7.552801132202148
Epoch 140, val loss: 1.2548192739486694
Epoch 150, training loss: 1.0611865520477295 = 1.0536623001098633 + 0.001 * 7.524207592010498
Epoch 150, val loss: 1.1589676141738892
Epoch 160, training loss: 0.9480957984924316 = 0.9406241774559021 + 0.001 * 7.471625328063965
Epoch 160, val loss: 1.0680357217788696
Epoch 170, training loss: 0.8466173410415649 = 0.8392094373703003 + 0.001 * 7.407887935638428
Epoch 170, val loss: 0.9898653626441956
Epoch 180, training loss: 0.7608567476272583 = 0.7534623146057129 + 0.001 * 7.394420623779297
Epoch 180, val loss: 0.9285268783569336
Epoch 190, training loss: 0.6892672777175903 = 0.6818815469741821 + 0.001 * 7.385721206665039
Epoch 190, val loss: 0.882685661315918
Epoch 200, training loss: 0.6278892755508423 = 0.6205116510391235 + 0.001 * 7.377594947814941
Epoch 200, val loss: 0.8482845425605774
Epoch 210, training loss: 0.5734481811523438 = 0.5660774111747742 + 0.001 * 7.370776176452637
Epoch 210, val loss: 0.8221795558929443
Epoch 220, training loss: 0.5240744948387146 = 0.5167108178138733 + 0.001 * 7.363675117492676
Epoch 220, val loss: 0.802065372467041
Epoch 230, training loss: 0.4787517189979553 = 0.47139620780944824 + 0.001 * 7.355522155761719
Epoch 230, val loss: 0.7865160703659058
Epoch 240, training loss: 0.43682610988616943 = 0.4294840395450592 + 0.001 * 7.342075347900391
Epoch 240, val loss: 0.775022029876709
Epoch 250, training loss: 0.39791467785835266 = 0.3905957043170929 + 0.001 * 7.318977355957031
Epoch 250, val loss: 0.7674406170845032
Epoch 260, training loss: 0.36184975504875183 = 0.3545604646205902 + 0.001 * 7.289304733276367
Epoch 260, val loss: 0.7638764381408691
Epoch 270, training loss: 0.3286019563674927 = 0.3213389217853546 + 0.001 * 7.263035774230957
Epoch 270, val loss: 0.764338493347168
Epoch 280, training loss: 0.298060804605484 = 0.29079657793045044 + 0.001 * 7.264227867126465
Epoch 280, val loss: 0.7685084939002991
Epoch 290, training loss: 0.2699819505214691 = 0.262748122215271 + 0.001 * 7.233820915222168
Epoch 290, val loss: 0.7758163809776306
Epoch 300, training loss: 0.2442825883626938 = 0.23704996705055237 + 0.001 * 7.232614994049072
Epoch 300, val loss: 0.7860634922981262
Epoch 310, training loss: 0.22081291675567627 = 0.21358327567577362 + 0.001 * 7.229640007019043
Epoch 310, val loss: 0.7990509867668152
Epoch 320, training loss: 0.19946332275867462 = 0.1922355443239212 + 0.001 * 7.227774143218994
Epoch 320, val loss: 0.814634382724762
Epoch 330, training loss: 0.18010838329792023 = 0.1728818267583847 + 0.001 * 7.226555824279785
Epoch 330, val loss: 0.8325642943382263
Epoch 340, training loss: 0.16259224712848663 = 0.15536750853061676 + 0.001 * 7.224736213684082
Epoch 340, val loss: 0.8524969816207886
Epoch 350, training loss: 0.1467834711074829 = 0.13956092298030853 + 0.001 * 7.222545623779297
Epoch 350, val loss: 0.8741387724876404
Epoch 360, training loss: 0.13257478177547455 = 0.12534506618976593 + 0.001 * 7.2297186851501465
Epoch 360, val loss: 0.897153377532959
Epoch 370, training loss: 0.11982344090938568 = 0.11260467767715454 + 0.001 * 7.218761444091797
Epoch 370, val loss: 0.9212247133255005
Epoch 380, training loss: 0.1084371954202652 = 0.10122261941432953 + 0.001 * 7.214572429656982
Epoch 380, val loss: 0.9460893273353577
Epoch 390, training loss: 0.0982828214764595 = 0.09107347577810287 + 0.001 * 7.209347724914551
Epoch 390, val loss: 0.9715177416801453
Epoch 400, training loss: 0.0892479345202446 = 0.08203579485416412 + 0.001 * 7.2121381759643555
Epoch 400, val loss: 0.9973285794258118
Epoch 410, training loss: 0.08119641989469528 = 0.07399515807628632 + 0.001 * 7.201261043548584
Epoch 410, val loss: 1.0233018398284912
Epoch 420, training loss: 0.07404400408267975 = 0.06684757024049759 + 0.001 * 7.196437358856201
Epoch 420, val loss: 1.049168586730957
Epoch 430, training loss: 0.06768748909235 = 0.06049887835979462 + 0.001 * 7.1886115074157715
Epoch 430, val loss: 1.0748603343963623
Epoch 440, training loss: 0.06205775961279869 = 0.05486138164997101 + 0.001 * 7.196378231048584
Epoch 440, val loss: 1.1002440452575684
Epoch 450, training loss: 0.057032015174627304 = 0.049856171011924744 + 0.001 * 7.17584228515625
Epoch 450, val loss: 1.125190258026123
Epoch 460, training loss: 0.052574947476387024 = 0.045409176498651505 + 0.001 * 7.165771007537842
Epoch 460, val loss: 1.149593710899353
Epoch 470, training loss: 0.048643261194229126 = 0.041455138474702835 + 0.001 * 7.188121795654297
Epoch 470, val loss: 1.1734390258789062
Epoch 480, training loss: 0.045087844133377075 = 0.037936098873615265 + 0.001 * 7.151743412017822
Epoch 480, val loss: 1.1966367959976196
Epoch 490, training loss: 0.04194527119398117 = 0.03479992598295212 + 0.001 * 7.1453447341918945
Epoch 490, val loss: 1.2192051410675049
Epoch 500, training loss: 0.039141811430454254 = 0.032000720500946045 + 0.001 * 7.141091346740723
Epoch 500, val loss: 1.241059422492981
Epoch 510, training loss: 0.03662905469536781 = 0.029497388750314713 + 0.001 * 7.131667137145996
Epoch 510, val loss: 1.262238621711731
Epoch 520, training loss: 0.034411512315273285 = 0.027253899723291397 + 0.001 * 7.15761137008667
Epoch 520, val loss: 1.282745599746704
Epoch 530, training loss: 0.03235873579978943 = 0.02523801475763321 + 0.001 * 7.120721340179443
Epoch 530, val loss: 1.3026201725006104
Epoch 540, training loss: 0.030556660145521164 = 0.023422900587320328 + 0.001 * 7.13375997543335
Epoch 540, val loss: 1.3218834400177002
Epoch 550, training loss: 0.028913967311382294 = 0.021784961223602295 + 0.001 * 7.129006385803223
Epoch 550, val loss: 1.3405085802078247
Epoch 560, training loss: 0.027415744960308075 = 0.020304307341575623 + 0.001 * 7.111437797546387
Epoch 560, val loss: 1.3585275411605835
Epoch 570, training loss: 0.026067934930324554 = 0.018963078036904335 + 0.001 * 7.104856967926025
Epoch 570, val loss: 1.3759865760803223
Epoch 580, training loss: 0.024847934022545815 = 0.01774481125175953 + 0.001 * 7.103121757507324
Epoch 580, val loss: 1.392884373664856
Epoch 590, training loss: 0.023743297904729843 = 0.01663581281900406 + 0.001 * 7.107485771179199
Epoch 590, val loss: 1.409277319908142
Epoch 600, training loss: 0.022758379578590393 = 0.015624462626874447 + 0.001 * 7.13391637802124
Epoch 600, val loss: 1.4251539707183838
Epoch 610, training loss: 0.021824032068252563 = 0.014700429514050484 + 0.001 * 7.123601913452148
Epoch 610, val loss: 1.4405485391616821
Epoch 620, training loss: 0.020954594016075134 = 0.013854489661753178 + 0.001 * 7.100104808807373
Epoch 620, val loss: 1.4554728269577026
Epoch 630, training loss: 0.020175017416477203 = 0.013078686781227589 + 0.001 * 7.096329689025879
Epoch 630, val loss: 1.4699785709381104
Epoch 640, training loss: 0.019451912492513657 = 0.012365883216261864 + 0.001 * 7.086028099060059
Epoch 640, val loss: 1.4840208292007446
Epoch 650, training loss: 0.018801286816596985 = 0.011709792539477348 + 0.001 * 7.091493129730225
Epoch 650, val loss: 1.49765944480896
Epoch 660, training loss: 0.018188834190368652 = 0.011104881763458252 + 0.001 * 7.0839524269104
Epoch 660, val loss: 1.5109068155288696
Epoch 670, training loss: 0.017646262422204018 = 0.010546193458139896 + 0.001 * 7.10006856918335
Epoch 670, val loss: 1.5237584114074707
Epoch 680, training loss: 0.017124000936746597 = 0.010029418393969536 + 0.001 * 7.094583034515381
Epoch 680, val loss: 1.5362471342086792
Epoch 690, training loss: 0.01664293371140957 = 0.009550546295940876 + 0.001 * 7.092386722564697
Epoch 690, val loss: 1.5483698844909668
Epoch 700, training loss: 0.01619688794016838 = 0.009106139652431011 + 0.001 * 7.090747833251953
Epoch 700, val loss: 1.5601742267608643
Epoch 710, training loss: 0.015768736600875854 = 0.008693002164363861 + 0.001 * 7.075734615325928
Epoch 710, val loss: 1.5716263055801392
Epoch 720, training loss: 0.01538490317761898 = 0.008308395743370056 + 0.001 * 7.076506614685059
Epoch 720, val loss: 1.5827730894088745
Epoch 730, training loss: 0.015022067353129387 = 0.00794984120875597 + 0.001 * 7.072226047515869
Epoch 730, val loss: 1.5935981273651123
Epoch 740, training loss: 0.014699315652251244 = 0.007615142967551947 + 0.001 * 7.084171772003174
Epoch 740, val loss: 1.6041455268859863
Epoch 750, training loss: 0.01438509114086628 = 0.007302214857190847 + 0.001 * 7.082876205444336
Epoch 750, val loss: 1.6143889427185059
Epoch 760, training loss: 0.01410445012152195 = 0.0070093125104904175 + 0.001 * 7.095137596130371
Epoch 760, val loss: 1.6243723630905151
Epoch 770, training loss: 0.013815291225910187 = 0.0067346831783652306 + 0.001 * 7.0806074142456055
Epoch 770, val loss: 1.6340835094451904
Epoch 780, training loss: 0.013535657897591591 = 0.0064767454750835896 + 0.001 * 7.05891227722168
Epoch 780, val loss: 1.6435693502426147
Epoch 790, training loss: 0.01328868418931961 = 0.006234032101929188 + 0.001 * 7.054652214050293
Epoch 790, val loss: 1.652803897857666
Epoch 800, training loss: 0.013063615188002586 = 0.006004995666444302 + 0.001 * 7.058619022369385
Epoch 800, val loss: 1.6618961095809937
Epoch 810, training loss: 0.012835761532187462 = 0.00578793091699481 + 0.001 * 7.047829627990723
Epoch 810, val loss: 1.6708194017410278
Epoch 820, training loss: 0.012638600543141365 = 0.005581288132816553 + 0.001 * 7.057312488555908
Epoch 820, val loss: 1.6796491146087646
Epoch 830, training loss: 0.012428434565663338 = 0.005383938550949097 + 0.001 * 7.044496059417725
Epoch 830, val loss: 1.6884576082229614
Epoch 840, training loss: 0.01224108599126339 = 0.005195255391299725 + 0.001 * 7.045830726623535
Epoch 840, val loss: 1.697204828262329
Epoch 850, training loss: 0.012057395651936531 = 0.005014816299080849 + 0.001 * 7.042579650878906
Epoch 850, val loss: 1.7058898210525513
Epoch 860, training loss: 0.011886857450008392 = 0.0048422073014080524 + 0.001 * 7.044649124145508
Epoch 860, val loss: 1.714566946029663
Epoch 870, training loss: 0.011718281544744968 = 0.004677155986428261 + 0.001 * 7.041125297546387
Epoch 870, val loss: 1.7231652736663818
Epoch 880, training loss: 0.011580375023186207 = 0.004519485868513584 + 0.001 * 7.060888767242432
Epoch 880, val loss: 1.7316765785217285
Epoch 890, training loss: 0.011416330002248287 = 0.004369066096842289 + 0.001 * 7.0472636222839355
Epoch 890, val loss: 1.7401390075683594
Epoch 900, training loss: 0.011280027218163013 = 0.004225678276270628 + 0.001 * 7.054348468780518
Epoch 900, val loss: 1.7484595775604248
Epoch 910, training loss: 0.011118205264210701 = 0.0040891049429774284 + 0.001 * 7.02910041809082
Epoch 910, val loss: 1.7566736936569214
Epoch 920, training loss: 0.011033332906663418 = 0.0039589968509972095 + 0.001 * 7.07433557510376
Epoch 920, val loss: 1.7647430896759033
Epoch 930, training loss: 0.010897394269704819 = 0.0038351465482264757 + 0.001 * 7.062247276306152
Epoch 930, val loss: 1.7726516723632812
Epoch 940, training loss: 0.010758878663182259 = 0.0037174357566982508 + 0.001 * 7.041442394256592
Epoch 940, val loss: 1.7804759740829468
Epoch 950, training loss: 0.010621301829814911 = 0.003605353645980358 + 0.001 * 7.015947341918945
Epoch 950, val loss: 1.7881407737731934
Epoch 960, training loss: 0.010524949990212917 = 0.003498597303405404 + 0.001 * 7.026352405548096
Epoch 960, val loss: 1.7956559658050537
Epoch 970, training loss: 0.01046315860003233 = 0.0033968703355640173 + 0.001 * 7.066287994384766
Epoch 970, val loss: 1.8030142784118652
Epoch 980, training loss: 0.010341660119593143 = 0.00329984282143414 + 0.001 * 7.041816711425781
Epoch 980, val loss: 1.8102692365646362
Epoch 990, training loss: 0.010221168398857117 = 0.0032071052119135857 + 0.001 * 7.014062881469727
Epoch 990, val loss: 1.817430019378662
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8323668950975225
=== training gcn model ===
Epoch 0, training loss: 1.9589715003967285 = 1.950374722480774 + 0.001 * 8.596796989440918
Epoch 0, val loss: 1.9466232061386108
Epoch 10, training loss: 1.9490302801132202 = 1.9404335021972656 + 0.001 * 8.596731185913086
Epoch 10, val loss: 1.936890721321106
Epoch 20, training loss: 1.937007188796997 = 1.9284106492996216 + 0.001 * 8.596529006958008
Epoch 20, val loss: 1.9247938394546509
Epoch 30, training loss: 1.9205716848373413 = 1.911975622177124 + 0.001 * 8.596096992492676
Epoch 30, val loss: 1.908203125
Epoch 40, training loss: 1.8966245651245117 = 1.8880294561386108 + 0.001 * 8.595121383666992
Epoch 40, val loss: 1.8841887712478638
Epoch 50, training loss: 1.8625093698501587 = 1.853916883468628 + 0.001 * 8.5924654006958
Epoch 50, val loss: 1.851149559020996
Epoch 60, training loss: 1.820989727973938 = 1.8124067783355713 + 0.001 * 8.582967758178711
Epoch 60, val loss: 1.8142646551132202
Epoch 70, training loss: 1.7809301614761353 = 1.772391676902771 + 0.001 * 8.53844928741455
Epoch 70, val loss: 1.781983494758606
Epoch 80, training loss: 1.7358306646347046 = 1.7275683879852295 + 0.001 * 8.262219429016113
Epoch 80, val loss: 1.7425258159637451
Epoch 90, training loss: 1.6753417253494263 = 1.6673591136932373 + 0.001 * 7.982665538787842
Epoch 90, val loss: 1.6868690252304077
Epoch 100, training loss: 1.5959522724151611 = 1.5881173610687256 + 0.001 * 7.834875583648682
Epoch 100, val loss: 1.616437554359436
Epoch 110, training loss: 1.5003933906555176 = 1.492763638496399 + 0.001 * 7.629800319671631
Epoch 110, val loss: 1.535133719444275
Epoch 120, training loss: 1.3969619274139404 = 1.3894455432891846 + 0.001 * 7.516366481781006
Epoch 120, val loss: 1.4503321647644043
Epoch 130, training loss: 1.2941184043884277 = 1.2866268157958984 + 0.001 * 7.491633415222168
Epoch 130, val loss: 1.3692132234573364
Epoch 140, training loss: 1.1955353021621704 = 1.1880804300308228 + 0.001 * 7.454888343811035
Epoch 140, val loss: 1.2927889823913574
Epoch 150, training loss: 1.1034162044525146 = 1.0959993600845337 + 0.001 * 7.4168195724487305
Epoch 150, val loss: 1.2224420309066772
Epoch 160, training loss: 1.018697738647461 = 1.011319875717163 + 0.001 * 7.377915859222412
Epoch 160, val loss: 1.1578598022460938
Epoch 170, training loss: 0.941359281539917 = 0.9340164661407471 + 0.001 * 7.34278678894043
Epoch 170, val loss: 1.0993362665176392
Epoch 180, training loss: 0.8700094223022461 = 0.8626983165740967 + 0.001 * 7.3111252784729
Epoch 180, val loss: 1.0461058616638184
Epoch 190, training loss: 0.8031492829322815 = 0.7958722710609436 + 0.001 * 7.276996612548828
Epoch 190, val loss: 0.9977809190750122
Epoch 200, training loss: 0.7398895621299744 = 0.7326462268829346 + 0.001 * 7.243358612060547
Epoch 200, val loss: 0.9541111588478088
Epoch 210, training loss: 0.6801590323448181 = 0.6729350090026855 + 0.001 * 7.224038124084473
Epoch 210, val loss: 0.9148905277252197
Epoch 220, training loss: 0.6244611740112305 = 0.6172450184822083 + 0.001 * 7.216170787811279
Epoch 220, val loss: 0.8804510235786438
Epoch 230, training loss: 0.5734818577766418 = 0.5662673115730286 + 0.001 * 7.214543342590332
Epoch 230, val loss: 0.851479172706604
Epoch 240, training loss: 0.5275658965110779 = 0.5203514099121094 + 0.001 * 7.2144975662231445
Epoch 240, val loss: 0.8283466696739197
Epoch 250, training loss: 0.48634180426597595 = 0.4791278839111328 + 0.001 * 7.2139058113098145
Epoch 250, val loss: 0.8107663989067078
Epoch 260, training loss: 0.4486834704875946 = 0.4414709508419037 + 0.001 * 7.2125091552734375
Epoch 260, val loss: 0.7975759506225586
Epoch 270, training loss: 0.41305914521217346 = 0.40584930777549744 + 0.001 * 7.209841728210449
Epoch 270, val loss: 0.787257194519043
Epoch 280, training loss: 0.3779231905937195 = 0.3707177937030792 + 0.001 * 7.205389499664307
Epoch 280, val loss: 0.7783648371696472
Epoch 290, training loss: 0.3422428369522095 = 0.33504417538642883 + 0.001 * 7.198648929595947
Epoch 290, val loss: 0.7700976729393005
Epoch 300, training loss: 0.30584898591041565 = 0.29865923523902893 + 0.001 * 7.189743518829346
Epoch 300, val loss: 0.762198269367218
Epoch 310, training loss: 0.2696312367916107 = 0.2624515891075134 + 0.001 * 7.179644584655762
Epoch 310, val loss: 0.7550915479660034
Epoch 320, training loss: 0.23510907590389252 = 0.22794121503829956 + 0.001 * 7.167856216430664
Epoch 320, val loss: 0.7498610019683838
Epoch 330, training loss: 0.20371048152446747 = 0.1965569704771042 + 0.001 * 7.153506755828857
Epoch 330, val loss: 0.7476666569709778
Epoch 340, training loss: 0.17621266841888428 = 0.16903699934482574 + 0.001 * 7.175671577453613
Epoch 340, val loss: 0.7490574717521667
Epoch 350, training loss: 0.15256604552268982 = 0.14542177319526672 + 0.001 * 7.144269943237305
Epoch 350, val loss: 0.7540692090988159
Epoch 360, training loss: 0.13253287971019745 = 0.12539394199848175 + 0.001 * 7.138934135437012
Epoch 360, val loss: 0.7622193694114685
Epoch 370, training loss: 0.115627221763134 = 0.10849224030971527 + 0.001 * 7.134983062744141
Epoch 370, val loss: 0.7728829383850098
Epoch 380, training loss: 0.10138773173093796 = 0.09425418823957443 + 0.001 * 7.133543491363525
Epoch 380, val loss: 0.7853827476501465
Epoch 390, training loss: 0.08938559889793396 = 0.08225367963314056 + 0.001 * 7.131916522979736
Epoch 390, val loss: 0.7990299463272095
Epoch 400, training loss: 0.07923973351716995 = 0.07210889458656311 + 0.001 * 7.130836009979248
Epoch 400, val loss: 0.8133450746536255
Epoch 410, training loss: 0.0706307590007782 = 0.06350122392177582 + 0.001 * 7.12953519821167
Epoch 410, val loss: 0.8278468251228333
Epoch 420, training loss: 0.06330085545778275 = 0.056169141083955765 + 0.001 * 7.131716728210449
Epoch 420, val loss: 0.8422932028770447
Epoch 430, training loss: 0.057027436792850494 = 0.04990050196647644 + 0.001 * 7.1269354820251465
Epoch 430, val loss: 0.8565167784690857
Epoch 440, training loss: 0.0516488291323185 = 0.044524069875478745 + 0.001 * 7.124759197235107
Epoch 440, val loss: 0.8704800605773926
Epoch 450, training loss: 0.047029007226228714 = 0.03989815339446068 + 0.001 * 7.130855083465576
Epoch 450, val loss: 0.8841003775596619
Epoch 460, training loss: 0.043023962527513504 = 0.03590471297502518 + 0.001 * 7.119250297546387
Epoch 460, val loss: 0.8973587155342102
Epoch 470, training loss: 0.0395611897110939 = 0.032444629818201065 + 0.001 * 7.116561412811279
Epoch 470, val loss: 0.910264253616333
Epoch 480, training loss: 0.03654962033033371 = 0.02943487837910652 + 0.001 * 7.11474084854126
Epoch 480, val loss: 0.9227739572525024
Epoch 490, training loss: 0.033916376531124115 = 0.026805732399225235 + 0.001 * 7.110645771026611
Epoch 490, val loss: 0.9348979592323303
Epoch 500, training loss: 0.03161773085594177 = 0.024496061727404594 + 0.001 * 7.12166690826416
Epoch 500, val loss: 0.9466320276260376
Epoch 510, training loss: 0.029565438628196716 = 0.022453749552369118 + 0.001 * 7.11168909072876
Epoch 510, val loss: 0.9579306244850159
Epoch 520, training loss: 0.027744131162762642 = 0.02063867263495922 + 0.001 * 7.105458736419678
Epoch 520, val loss: 0.9688208699226379
Epoch 530, training loss: 0.026120901107788086 = 0.019020363688468933 + 0.001 * 7.100536823272705
Epoch 530, val loss: 0.9792999029159546
Epoch 540, training loss: 0.024674002081155777 = 0.017574016004800797 + 0.001 * 7.099985599517822
Epoch 540, val loss: 0.9893985390663147
Epoch 550, training loss: 0.023379867896437645 = 0.016278589144349098 + 0.001 * 7.101278305053711
Epoch 550, val loss: 0.9990887641906738
Epoch 560, training loss: 0.022213734686374664 = 0.015115931630134583 + 0.001 * 7.097803115844727
Epoch 560, val loss: 1.0084327459335327
Epoch 570, training loss: 0.021161356940865517 = 0.014070109464228153 + 0.001 * 7.091247081756592
Epoch 570, val loss: 1.017414927482605
Epoch 580, training loss: 0.020223380997776985 = 0.013127117417752743 + 0.001 * 7.096263885498047
Epoch 580, val loss: 1.0260564088821411
Epoch 590, training loss: 0.019362783059477806 = 0.01227483432739973 + 0.001 * 7.087948799133301
Epoch 590, val loss: 1.0343611240386963
Epoch 600, training loss: 0.018589524552226067 = 0.011502629145979881 + 0.001 * 7.086894989013672
Epoch 600, val loss: 1.0423780679702759
Epoch 610, training loss: 0.017893560230731964 = 0.010801357217133045 + 0.001 * 7.092202186584473
Epoch 610, val loss: 1.0500872135162354
Epoch 620, training loss: 0.017257269471883774 = 0.010163022205233574 + 0.001 * 7.094247341156006
Epoch 620, val loss: 1.0575083494186401
Epoch 630, training loss: 0.016666894778609276 = 0.009580626152455807 + 0.001 * 7.086268901824951
Epoch 630, val loss: 1.06466543674469
Epoch 640, training loss: 0.016130957752466202 = 0.009048002772033215 + 0.001 * 7.082954406738281
Epoch 640, val loss: 1.0715779066085815
Epoch 650, training loss: 0.015641000121831894 = 0.008559783920645714 + 0.001 * 7.081216335296631
Epoch 650, val loss: 1.0782290697097778
Epoch 660, training loss: 0.015190629288554192 = 0.008111309260129929 + 0.001 * 7.079319000244141
Epoch 660, val loss: 1.0846558809280396
Epoch 670, training loss: 0.01479120273143053 = 0.007698433473706245 + 0.001 * 7.092769145965576
Epoch 670, val loss: 1.0908693075180054
Epoch 680, training loss: 0.014399364590644836 = 0.007317259442061186 + 0.001 * 7.082104682922363
Epoch 680, val loss: 1.0968830585479736
Epoch 690, training loss: 0.014040259644389153 = 0.006963962689042091 + 0.001 * 7.076297283172607
Epoch 690, val loss: 1.1027491092681885
Epoch 700, training loss: 0.013711190782487392 = 0.006634542718529701 + 0.001 * 7.076647758483887
Epoch 700, val loss: 1.1085395812988281
Epoch 710, training loss: 0.013408562168478966 = 0.006325854454189539 + 0.001 * 7.082707405090332
Epoch 710, val loss: 1.11428701877594
Epoch 720, training loss: 0.013111025094985962 = 0.006035839207470417 + 0.001 * 7.075185298919678
Epoch 720, val loss: 1.119957447052002
Epoch 730, training loss: 0.01283414289355278 = 0.005763218272477388 + 0.001 * 7.070923805236816
Epoch 730, val loss: 1.1255908012390137
Epoch 740, training loss: 0.012592148035764694 = 0.005507133435457945 + 0.001 * 7.085014343261719
Epoch 740, val loss: 1.131110668182373
Epoch 750, training loss: 0.012335194274783134 = 0.005266774911433458 + 0.001 * 7.068418979644775
Epoch 750, val loss: 1.1365785598754883
Epoch 760, training loss: 0.012109648436307907 = 0.005041210446506739 + 0.001 * 7.068437099456787
Epoch 760, val loss: 1.1418943405151367
Epoch 770, training loss: 0.011903662234544754 = 0.004829526413232088 + 0.001 * 7.074134826660156
Epoch 770, val loss: 1.1470927000045776
Epoch 780, training loss: 0.011697147972881794 = 0.004630857612937689 + 0.001 * 7.066289901733398
Epoch 780, val loss: 1.152170181274414
Epoch 790, training loss: 0.011513009667396545 = 0.00444437563419342 + 0.001 * 7.068633079528809
Epoch 790, val loss: 1.1571229696273804
Epoch 800, training loss: 0.011330378241837025 = 0.004269277211278677 + 0.001 * 7.061100482940674
Epoch 800, val loss: 1.1619453430175781
Epoch 810, training loss: 0.011165762320160866 = 0.004104756284505129 + 0.001 * 7.061005592346191
Epoch 810, val loss: 1.1666375398635864
Epoch 820, training loss: 0.01101420633494854 = 0.003950098529458046 + 0.001 * 7.0641069412231445
Epoch 820, val loss: 1.1712177991867065
Epoch 830, training loss: 0.01086774468421936 = 0.0038046417757868767 + 0.001 * 7.063102722167969
Epoch 830, val loss: 1.175659418106079
Epoch 840, training loss: 0.01072896271944046 = 0.003667736193165183 + 0.001 * 7.0612263679504395
Epoch 840, val loss: 1.1800026893615723
Epoch 850, training loss: 0.010604681447148323 = 0.0035387289244681597 + 0.001 * 7.065952301025391
Epoch 850, val loss: 1.1842321157455444
Epoch 860, training loss: 0.010472784750163555 = 0.0034170665312558413 + 0.001 * 7.055717468261719
Epoch 860, val loss: 1.1883502006530762
Epoch 870, training loss: 0.010355041362345219 = 0.0033022190909832716 + 0.001 * 7.052822113037109
Epoch 870, val loss: 1.192351222038269
Epoch 880, training loss: 0.010243517346680164 = 0.003193731652572751 + 0.001 * 7.049785614013672
Epoch 880, val loss: 1.19625985622406
Epoch 890, training loss: 0.010145405307412148 = 0.003091164631769061 + 0.001 * 7.0542402267456055
Epoch 890, val loss: 1.2000635862350464
Epoch 900, training loss: 0.010046259500086308 = 0.0029941375833004713 + 0.001 * 7.052121162414551
Epoch 900, val loss: 1.2037612199783325
Epoch 910, training loss: 0.00996281299740076 = 0.0029022356029599905 + 0.001 * 7.060576915740967
Epoch 910, val loss: 1.2073630094528198
Epoch 920, training loss: 0.009866269305348396 = 0.0028151078149676323 + 0.001 * 7.051161766052246
Epoch 920, val loss: 1.2108961343765259
Epoch 930, training loss: 0.009779242798686028 = 0.002732441294938326 + 0.001 * 7.0468010902404785
Epoch 930, val loss: 1.2143168449401855
Epoch 940, training loss: 0.00970371998846531 = 0.002653938252478838 + 0.001 * 7.04978084564209
Epoch 940, val loss: 1.2176573276519775
Epoch 950, training loss: 0.009625259786844254 = 0.0025793330278247595 + 0.001 * 7.045926570892334
Epoch 950, val loss: 1.2209326028823853
Epoch 960, training loss: 0.009555131196975708 = 0.0025083827786147594 + 0.001 * 7.04674768447876
Epoch 960, val loss: 1.224088191986084
Epoch 970, training loss: 0.009479053318500519 = 0.002440872136503458 + 0.001 * 7.038180828094482
Epoch 970, val loss: 1.227208137512207
Epoch 980, training loss: 0.00944444164633751 = 0.0023765554651618004 + 0.001 * 7.067885875701904
Epoch 980, val loss: 1.2302615642547607
Epoch 990, training loss: 0.009355593472719193 = 0.002315288642421365 + 0.001 * 7.040304183959961
Epoch 990, val loss: 1.2332123517990112
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8392198207696363
The final CL Acc:0.78272, 0.02014, The final GNN Acc:0.83623, 0.00287
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11556])
remove edge: torch.Size([2, 9518])
updated graph: torch.Size([2, 10518])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9518818855285645 = 1.9432851076126099 + 0.001 * 8.596817016601562
Epoch 0, val loss: 1.945922613143921
Epoch 10, training loss: 1.9419150352478027 = 1.9333182573318481 + 0.001 * 8.596780776977539
Epoch 10, val loss: 1.9353854656219482
Epoch 20, training loss: 1.9299578666687012 = 1.9213612079620361 + 0.001 * 8.596628189086914
Epoch 20, val loss: 1.9225744009017944
Epoch 30, training loss: 1.9132710695266724 = 1.904674768447876 + 0.001 * 8.59626293182373
Epoch 30, val loss: 1.904657006263733
Epoch 40, training loss: 1.8887879848480225 = 1.8801926374435425 + 0.001 * 8.595356941223145
Epoch 40, val loss: 1.8785756826400757
Epoch 50, training loss: 1.8547852039337158 = 1.846192479133606 + 0.001 * 8.592686653137207
Epoch 50, val loss: 1.8438297510147095
Epoch 60, training loss: 1.8172612190246582 = 1.8086780309677124 + 0.001 * 8.583218574523926
Epoch 60, val loss: 1.8091039657592773
Epoch 70, training loss: 1.7869820594787598 = 1.778445839881897 + 0.001 * 8.536205291748047
Epoch 70, val loss: 1.7845706939697266
Epoch 80, training loss: 1.7499383687973022 = 1.7416423559188843 + 0.001 * 8.296058654785156
Epoch 80, val loss: 1.7531425952911377
Epoch 90, training loss: 1.6971640586853027 = 1.6890147924423218 + 0.001 * 8.149235725402832
Epoch 90, val loss: 1.7088356018066406
Epoch 100, training loss: 1.623698353767395 = 1.6157187223434448 + 0.001 * 7.979587078094482
Epoch 100, val loss: 1.648036003112793
Epoch 110, training loss: 1.532463788986206 = 1.5246840715408325 + 0.001 * 7.779715061187744
Epoch 110, val loss: 1.5740786790847778
Epoch 120, training loss: 1.4339412450790405 = 1.4262621402740479 + 0.001 * 7.67913293838501
Epoch 120, val loss: 1.4954437017440796
Epoch 130, training loss: 1.3345203399658203 = 1.3269143104553223 + 0.001 * 7.606064319610596
Epoch 130, val loss: 1.4176539182662964
Epoch 140, training loss: 1.234425663948059 = 1.226880669593811 + 0.001 * 7.54494047164917
Epoch 140, val loss: 1.3406797647476196
Epoch 150, training loss: 1.1346126794815063 = 1.1270941495895386 + 0.001 * 7.518492698669434
Epoch 150, val loss: 1.265509843826294
Epoch 160, training loss: 1.0379592180252075 = 1.0304549932479858 + 0.001 * 7.5042033195495605
Epoch 160, val loss: 1.1946625709533691
Epoch 170, training loss: 0.9463008046150208 = 0.9388046860694885 + 0.001 * 7.496096611022949
Epoch 170, val loss: 1.128612756729126
Epoch 180, training loss: 0.8592853546142578 = 0.851794421672821 + 0.001 * 7.490911960601807
Epoch 180, val loss: 1.065714955329895
Epoch 190, training loss: 0.7765170335769653 = 0.7690321207046509 + 0.001 * 7.484931468963623
Epoch 190, val loss: 1.0052965879440308
Epoch 200, training loss: 0.6991204619407654 = 0.6916443109512329 + 0.001 * 7.476126670837402
Epoch 200, val loss: 0.9488078951835632
Epoch 210, training loss: 0.6289567351341248 = 0.6214948892593384 + 0.001 * 7.461869239807129
Epoch 210, val loss: 0.8989891409873962
Epoch 220, training loss: 0.5667538642883301 = 0.5593088269233704 + 0.001 * 7.4450273513793945
Epoch 220, val loss: 0.8580172657966614
Epoch 230, training loss: 0.5115379095077515 = 0.5041149258613586 + 0.001 * 7.423004150390625
Epoch 230, val loss: 0.8260724544525146
Epoch 240, training loss: 0.46165353059768677 = 0.45424845814704895 + 0.001 * 7.405065536499023
Epoch 240, val loss: 0.8018404245376587
Epoch 250, training loss: 0.4160254895687103 = 0.40863290429115295 + 0.001 * 7.392583847045898
Epoch 250, val loss: 0.7839316725730896
Epoch 260, training loss: 0.37455886602401733 = 0.36717191338539124 + 0.001 * 7.3869428634643555
Epoch 260, val loss: 0.7716693878173828
Epoch 270, training loss: 0.3377523124217987 = 0.3303699195384979 + 0.001 * 7.3824028968811035
Epoch 270, val loss: 0.7647390961647034
Epoch 280, training loss: 0.30590057373046875 = 0.29851841926574707 + 0.001 * 7.3821611404418945
Epoch 280, val loss: 0.7627576589584351
Epoch 290, training loss: 0.2786079943180084 = 0.271228551864624 + 0.001 * 7.379448413848877
Epoch 290, val loss: 0.764582097530365
Epoch 300, training loss: 0.2549973130226135 = 0.24760515987873077 + 0.001 * 7.392138957977295
Epoch 300, val loss: 0.7692841291427612
Epoch 310, training loss: 0.2339419424533844 = 0.2265634387731552 + 0.001 * 7.378505706787109
Epoch 310, val loss: 0.7758629322052002
Epoch 320, training loss: 0.2143871933221817 = 0.20701269805431366 + 0.001 * 7.374490737915039
Epoch 320, val loss: 0.7835389971733093
Epoch 330, training loss: 0.19535966217517853 = 0.18798936903476715 + 0.001 * 7.370289325714111
Epoch 330, val loss: 0.7918140888214111
Epoch 340, training loss: 0.1762043982744217 = 0.16882944107055664 + 0.001 * 7.3749566078186035
Epoch 340, val loss: 0.8001596331596375
Epoch 350, training loss: 0.1567641943693161 = 0.14939996600151062 + 0.001 * 7.364221096038818
Epoch 350, val loss: 0.8085441589355469
Epoch 360, training loss: 0.13757239282131195 = 0.13021299242973328 + 0.001 * 7.359403133392334
Epoch 360, val loss: 0.8171339631080627
Epoch 370, training loss: 0.11950232833623886 = 0.11214042454957962 + 0.001 * 7.361902236938477
Epoch 370, val loss: 0.8262098431587219
Epoch 380, training loss: 0.10333726555109024 = 0.09597809612751007 + 0.001 * 7.359171390533447
Epoch 380, val loss: 0.8360223174095154
Epoch 390, training loss: 0.08946669846773148 = 0.08212189376354218 + 0.001 * 7.344802379608154
Epoch 390, val loss: 0.846750020980835
Epoch 400, training loss: 0.07788980007171631 = 0.07054993510246277 + 0.001 * 7.339860916137695
Epoch 400, val loss: 0.8583277463912964
Epoch 410, training loss: 0.0683252364397049 = 0.06098948419094086 + 0.001 * 7.335755825042725
Epoch 410, val loss: 0.8703790903091431
Epoch 420, training loss: 0.06043148413300514 = 0.05310233309864998 + 0.001 * 7.32914924621582
Epoch 420, val loss: 0.8827356100082397
Epoch 430, training loss: 0.05392472445964813 = 0.04655967280268669 + 0.001 * 7.365049362182617
Epoch 430, val loss: 0.8951505422592163
Epoch 440, training loss: 0.04841802641749382 = 0.04109688475728035 + 0.001 * 7.321141719818115
Epoch 440, val loss: 0.9074832201004028
Epoch 450, training loss: 0.04382160305976868 = 0.03650113195180893 + 0.001 * 7.320472240447998
Epoch 450, val loss: 0.9196748733520508
Epoch 460, training loss: 0.039921849966049194 = 0.0326058454811573 + 0.001 * 7.316002368927002
Epoch 460, val loss: 0.9315536022186279
Epoch 470, training loss: 0.036595746874809265 = 0.02928163856267929 + 0.001 * 7.314105987548828
Epoch 470, val loss: 0.9432010054588318
Epoch 480, training loss: 0.03374094143509865 = 0.02642761915922165 + 0.001 * 7.313321113586426
Epoch 480, val loss: 0.9545197486877441
Epoch 490, training loss: 0.031277090311050415 = 0.023963525891304016 + 0.001 * 7.313562870025635
Epoch 490, val loss: 0.9655528664588928
Epoch 500, training loss: 0.02914108708500862 = 0.021825240924954414 + 0.001 * 7.315846920013428
Epoch 500, val loss: 0.9762671589851379
Epoch 510, training loss: 0.027268219739198685 = 0.01996033638715744 + 0.001 * 7.3078837394714355
Epoch 510, val loss: 0.9866577386856079
Epoch 520, training loss: 0.025623733177781105 = 0.01832572929561138 + 0.001 * 7.298004150390625
Epoch 520, val loss: 0.9967463612556458
Epoch 530, training loss: 0.024209748953580856 = 0.01688653789460659 + 0.001 * 7.323211193084717
Epoch 530, val loss: 1.0065152645111084
Epoch 540, training loss: 0.02291080541908741 = 0.015613855794072151 + 0.001 * 7.2969489097595215
Epoch 540, val loss: 1.0160011053085327
Epoch 550, training loss: 0.02178916707634926 = 0.01448353286832571 + 0.001 * 7.305634498596191
Epoch 550, val loss: 1.025179386138916
Epoch 560, training loss: 0.02076302096247673 = 0.013475470244884491 + 0.001 * 7.287550449371338
Epoch 560, val loss: 1.0340704917907715
Epoch 570, training loss: 0.019872358068823814 = 0.012573027051985264 + 0.001 * 7.299331188201904
Epoch 570, val loss: 1.0426818132400513
Epoch 580, training loss: 0.019055886194109917 = 0.01176209095865488 + 0.001 * 7.293794631958008
Epoch 580, val loss: 1.0510376691818237
Epoch 590, training loss: 0.018326397985219955 = 0.011030813679099083 + 0.001 * 7.295583724975586
Epoch 590, val loss: 1.059105396270752
Epoch 600, training loss: 0.017656885087490082 = 0.010369152761995792 + 0.001 * 7.2877326011657715
Epoch 600, val loss: 1.0669738054275513
Epoch 610, training loss: 0.01705688238143921 = 0.009768604300916195 + 0.001 * 7.2882771492004395
Epoch 610, val loss: 1.0746006965637207
Epoch 620, training loss: 0.016506044194102287 = 0.009221886284649372 + 0.001 * 7.284157752990723
Epoch 620, val loss: 1.0819720029830933
Epoch 630, training loss: 0.01600799523293972 = 0.008722791448235512 + 0.001 * 7.28520393371582
Epoch 630, val loss: 1.0891363620758057
Epoch 640, training loss: 0.01555098406970501 = 0.008265872485935688 + 0.001 * 7.285111427307129
Epoch 640, val loss: 1.0960884094238281
Epoch 650, training loss: 0.015118081122636795 = 0.007846512831747532 + 0.001 * 7.2715678215026855
Epoch 650, val loss: 1.1028562784194946
Epoch 660, training loss: 0.014724047854542732 = 0.007460670080035925 + 0.001 * 7.263377666473389
Epoch 660, val loss: 1.1094284057617188
Epoch 670, training loss: 0.01436286885291338 = 0.007104872725903988 + 0.001 * 7.25799560546875
Epoch 670, val loss: 1.115822672843933
Epoch 680, training loss: 0.01405566930770874 = 0.006776147987693548 + 0.001 * 7.279520511627197
Epoch 680, val loss: 1.1220344305038452
Epoch 690, training loss: 0.013742582872509956 = 0.006471715867519379 + 0.001 * 7.270866870880127
Epoch 690, val loss: 1.1280853748321533
Epoch 700, training loss: 0.013453977182507515 = 0.006189271807670593 + 0.001 * 7.264705181121826
Epoch 700, val loss: 1.1339877843856812
Epoch 710, training loss: 0.013186905533075333 = 0.0059266951866447926 + 0.001 * 7.260209560394287
Epoch 710, val loss: 1.1397013664245605
Epoch 720, training loss: 0.012927383184432983 = 0.005682126618921757 + 0.001 * 7.2452569007873535
Epoch 720, val loss: 1.145300269126892
Epoch 730, training loss: 0.012704424560070038 = 0.005453949794173241 + 0.001 * 7.250474452972412
Epoch 730, val loss: 1.1507511138916016
Epoch 740, training loss: 0.012507707811892033 = 0.0052407654002308846 + 0.001 * 7.266942024230957
Epoch 740, val loss: 1.1560544967651367
Epoch 750, training loss: 0.012285223230719566 = 0.005041214637458324 + 0.001 * 7.244007587432861
Epoch 750, val loss: 1.1612423658370972
Epoch 760, training loss: 0.012101208791136742 = 0.004854198079556227 + 0.001 * 7.247009754180908
Epoch 760, val loss: 1.1663028001785278
Epoch 770, training loss: 0.011913551017642021 = 0.00467865914106369 + 0.001 * 7.234891414642334
Epoch 770, val loss: 1.17122220993042
Epoch 780, training loss: 0.011750331148505211 = 0.0045136637054383755 + 0.001 * 7.236667156219482
Epoch 780, val loss: 1.1760581731796265
Epoch 790, training loss: 0.011585451662540436 = 0.004358396399766207 + 0.001 * 7.227055549621582
Epoch 790, val loss: 1.1807408332824707
Epoch 800, training loss: 0.011440311558544636 = 0.004212075378745794 + 0.001 * 7.228235721588135
Epoch 800, val loss: 1.1853452920913696
Epoch 810, training loss: 0.011309051886200905 = 0.004074030090123415 + 0.001 * 7.235021591186523
Epoch 810, val loss: 1.189821481704712
Epoch 820, training loss: 0.011172967031598091 = 0.003943628165870905 + 0.001 * 7.229339122772217
Epoch 820, val loss: 1.1942179203033447
Epoch 830, training loss: 0.011045055463910103 = 0.0038203191943466663 + 0.001 * 7.224736213684082
Epoch 830, val loss: 1.198520541191101
Epoch 840, training loss: 0.010915293358266354 = 0.0037036011926829815 + 0.001 * 7.211691856384277
Epoch 840, val loss: 1.2026968002319336
Epoch 850, training loss: 0.010806678794324398 = 0.0035930019803345203 + 0.001 * 7.213676452636719
Epoch 850, val loss: 1.2068077325820923
Epoch 860, training loss: 0.010701126419007778 = 0.0034881143365055323 + 0.001 * 7.213011741638184
Epoch 860, val loss: 1.2108112573623657
Epoch 870, training loss: 0.010604629293084145 = 0.0033885203301906586 + 0.001 * 7.216109275817871
Epoch 870, val loss: 1.2147420644760132
Epoch 880, training loss: 0.01048805471509695 = 0.0032938767690211535 + 0.001 * 7.194177627563477
Epoch 880, val loss: 1.2186105251312256
Epoch 890, training loss: 0.010418896563351154 = 0.003203863278031349 + 0.001 * 7.215033054351807
Epoch 890, val loss: 1.2223652601242065
Epoch 900, training loss: 0.010341589339077473 = 0.0031181692611426115 + 0.001 * 7.223419666290283
Epoch 900, val loss: 1.2260627746582031
Epoch 910, training loss: 0.010246358811855316 = 0.0030365418642759323 + 0.001 * 7.2098164558410645
Epoch 910, val loss: 1.2296712398529053
Epoch 920, training loss: 0.010208705440163612 = 0.002958690281957388 + 0.001 * 7.250014305114746
Epoch 920, val loss: 1.2332227230072021
Epoch 930, training loss: 0.01008142065256834 = 0.002884423127397895 + 0.001 * 7.196997165679932
Epoch 930, val loss: 1.2366782426834106
Epoch 940, training loss: 0.010027311742305756 = 0.0028135040774941444 + 0.001 * 7.213806629180908
Epoch 940, val loss: 1.2400909662246704
Epoch 950, training loss: 0.009933305904269218 = 0.002745718229562044 + 0.001 * 7.187587738037109
Epoch 950, val loss: 1.243417501449585
Epoch 960, training loss: 0.00985334999859333 = 0.002680913545191288 + 0.001 * 7.172436714172363
Epoch 960, val loss: 1.2466750144958496
Epoch 970, training loss: 0.009781088680028915 = 0.0026189028285443783 + 0.001 * 7.1621856689453125
Epoch 970, val loss: 1.2498862743377686
Epoch 980, training loss: 0.009730430319905281 = 0.002559493063017726 + 0.001 * 7.1709370613098145
Epoch 980, val loss: 1.2530263662338257
Epoch 990, training loss: 0.009655613452196121 = 0.0025026067160069942 + 0.001 * 7.153006553649902
Epoch 990, val loss: 1.2560770511627197
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 1.9355725049972534 = 1.9269757270812988 + 0.001 * 8.596831321716309
Epoch 0, val loss: 1.925889253616333
Epoch 10, training loss: 1.9261583089828491 = 1.9175615310668945 + 0.001 * 8.596790313720703
Epoch 10, val loss: 1.9163694381713867
Epoch 20, training loss: 1.9148139953613281 = 1.906217336654663 + 0.001 * 8.596630096435547
Epoch 20, val loss: 1.9048244953155518
Epoch 30, training loss: 1.8992393016815186 = 1.8906431198120117 + 0.001 * 8.596223831176758
Epoch 30, val loss: 1.889087438583374
Epoch 40, training loss: 1.8769946098327637 = 1.8683993816375732 + 0.001 * 8.59524154663086
Epoch 40, val loss: 1.867148995399475
Epoch 50, training loss: 1.8468412160873413 = 1.8382484912872314 + 0.001 * 8.592670440673828
Epoch 50, val loss: 1.8387774229049683
Epoch 60, training loss: 1.8124465942382812 = 1.8038619756698608 + 0.001 * 8.58462905883789
Epoch 60, val loss: 1.808919072151184
Epoch 70, training loss: 1.7783231735229492 = 1.7697757482528687 + 0.001 * 8.547446250915527
Epoch 70, val loss: 1.7806051969528198
Epoch 80, training loss: 1.733370065689087 = 1.725097894668579 + 0.001 * 8.272154808044434
Epoch 80, val loss: 1.7405468225479126
Epoch 90, training loss: 1.6696531772613525 = 1.661546230316162 + 0.001 * 8.106954574584961
Epoch 90, val loss: 1.6841659545898438
Epoch 100, training loss: 1.5856623649597168 = 1.5776244401931763 + 0.001 * 8.037947654724121
Epoch 100, val loss: 1.613072395324707
Epoch 110, training loss: 1.4887521266937256 = 1.4807895421981812 + 0.001 * 7.962610721588135
Epoch 110, val loss: 1.534061074256897
Epoch 120, training loss: 1.387876033782959 = 1.3800735473632812 + 0.001 * 7.802534580230713
Epoch 120, val loss: 1.4551465511322021
Epoch 130, training loss: 1.2879351377487183 = 1.2802660465240479 + 0.001 * 7.669119834899902
Epoch 130, val loss: 1.38083016872406
Epoch 140, training loss: 1.1920510530471802 = 1.1844813823699951 + 0.001 * 7.569622039794922
Epoch 140, val loss: 1.3138301372528076
Epoch 150, training loss: 1.1029086112976074 = 1.0953917503356934 + 0.001 * 7.516888618469238
Epoch 150, val loss: 1.254981517791748
Epoch 160, training loss: 1.0212546586990356 = 1.0137434005737305 + 0.001 * 7.5112714767456055
Epoch 160, val loss: 1.2038021087646484
Epoch 170, training loss: 0.9458796977996826 = 0.9383705854415894 + 0.001 * 7.509134292602539
Epoch 170, val loss: 1.1579922437667847
Epoch 180, training loss: 0.8745757341384888 = 0.8670708537101746 + 0.001 * 7.504870891571045
Epoch 180, val loss: 1.1148803234100342
Epoch 190, training loss: 0.8050746321678162 = 0.7975742816925049 + 0.001 * 7.500372886657715
Epoch 190, val loss: 1.0726141929626465
Epoch 200, training loss: 0.7359244227409363 = 0.728428840637207 + 0.001 * 7.495608806610107
Epoch 200, val loss: 1.0296409130096436
Epoch 210, training loss: 0.6669262051582336 = 0.6594362854957581 + 0.001 * 7.489943504333496
Epoch 210, val loss: 0.9865869879722595
Epoch 220, training loss: 0.5987790822982788 = 0.5912965536117554 + 0.001 * 7.4825005531311035
Epoch 220, val loss: 0.9446786642074585
Epoch 230, training loss: 0.5326185822486877 = 0.5251461863517761 + 0.001 * 7.472413539886475
Epoch 230, val loss: 0.9053306579589844
Epoch 240, training loss: 0.46990588307380676 = 0.4624488651752472 + 0.001 * 7.4570231437683105
Epoch 240, val loss: 0.8697232007980347
Epoch 250, training loss: 0.41202452778816223 = 0.40459108352661133 + 0.001 * 7.433440685272217
Epoch 250, val loss: 0.8394694924354553
Epoch 260, training loss: 0.35976213216781616 = 0.35234197974205017 + 0.001 * 7.420159816741943
Epoch 260, val loss: 0.8154808282852173
Epoch 270, training loss: 0.31325626373291016 = 0.3058551251888275 + 0.001 * 7.401135444641113
Epoch 270, val loss: 0.7979188561439514
Epoch 280, training loss: 0.2723269760608673 = 0.26493728160858154 + 0.001 * 7.389704704284668
Epoch 280, val loss: 0.7861998677253723
Epoch 290, training loss: 0.2366480678319931 = 0.22926771640777588 + 0.001 * 7.380344390869141
Epoch 290, val loss: 0.7793127298355103
Epoch 300, training loss: 0.20579788088798523 = 0.19842128455638885 + 0.001 * 7.3765950202941895
Epoch 300, val loss: 0.7763793468475342
Epoch 310, training loss: 0.17925740778446198 = 0.17188692092895508 + 0.001 * 7.370491981506348
Epoch 310, val loss: 0.7765564918518066
Epoch 320, training loss: 0.1564546525478363 = 0.1490904688835144 + 0.001 * 7.364187717437744
Epoch 320, val loss: 0.7793448567390442
Epoch 330, training loss: 0.13684456050395966 = 0.12948079407215118 + 0.001 * 7.3637614250183105
Epoch 330, val loss: 0.7843216061592102
Epoch 340, training loss: 0.11993978917598724 = 0.1125912293791771 + 0.001 * 7.348559856414795
Epoch 340, val loss: 0.7911626100540161
Epoch 350, training loss: 0.10538452118635178 = 0.09803993999958038 + 0.001 * 7.344579219818115
Epoch 350, val loss: 0.7995277047157288
Epoch 360, training loss: 0.09285841137170792 = 0.08551632612943649 + 0.001 * 7.342085361480713
Epoch 360, val loss: 0.809185802936554
Epoch 370, training loss: 0.08208024501800537 = 0.07475276291370392 + 0.001 * 7.327483177185059
Epoch 370, val loss: 0.8198500275611877
Epoch 380, training loss: 0.07285741716623306 = 0.06551748514175415 + 0.001 * 7.339931488037109
Epoch 380, val loss: 0.831264078617096
Epoch 390, training loss: 0.06492329388856888 = 0.05760432407259941 + 0.001 * 7.318972110748291
Epoch 390, val loss: 0.843216598033905
Epoch 400, training loss: 0.058139655739068985 = 0.05083182081580162 + 0.001 * 7.307834625244141
Epoch 400, val loss: 0.8555028438568115
Epoch 410, training loss: 0.05235042795538902 = 0.045036546885967255 + 0.001 * 7.313880443572998
Epoch 410, val loss: 0.8679863214492798
Epoch 420, training loss: 0.047386761754751205 = 0.040074560791254044 + 0.001 * 7.312201023101807
Epoch 420, val loss: 0.8805480003356934
Epoch 430, training loss: 0.04312190040946007 = 0.03582049906253815 + 0.001 * 7.301401138305664
Epoch 430, val loss: 0.8930795192718506
Epoch 440, training loss: 0.039459485560655594 = 0.03216313570737839 + 0.001 * 7.296348571777344
Epoch 440, val loss: 0.905478835105896
Epoch 450, training loss: 0.036298029124736786 = 0.029002824798226357 + 0.001 * 7.2952046394348145
Epoch 450, val loss: 0.9177427887916565
Epoch 460, training loss: 0.033549293875694275 = 0.02625768631696701 + 0.001 * 7.291606903076172
Epoch 460, val loss: 0.9297605156898499
Epoch 470, training loss: 0.03115556389093399 = 0.023863257840275764 + 0.001 * 7.292304992675781
Epoch 470, val loss: 0.9414904117584229
Epoch 480, training loss: 0.029082544147968292 = 0.021767599508166313 + 0.001 * 7.314944744110107
Epoch 480, val loss: 0.9529435038566589
Epoch 490, training loss: 0.02721061185002327 = 0.01992749236524105 + 0.001 * 7.283120155334473
Epoch 490, val loss: 0.9640827178955078
Epoch 500, training loss: 0.025594014674425125 = 0.01830640807747841 + 0.001 * 7.287605285644531
Epoch 500, val loss: 0.9749144315719604
Epoch 510, training loss: 0.024165784940123558 = 0.016873445361852646 + 0.001 * 7.29233980178833
Epoch 510, val loss: 0.9854112267494202
Epoch 520, training loss: 0.02287771925330162 = 0.015602285973727703 + 0.001 * 7.275432109832764
Epoch 520, val loss: 0.9956085085868835
Epoch 530, training loss: 0.021741105243563652 = 0.014470543712377548 + 0.001 * 7.270561695098877
Epoch 530, val loss: 1.0055196285247803
Epoch 540, training loss: 0.020723434165120125 = 0.013459320180118084 + 0.001 * 7.264113903045654
Epoch 540, val loss: 1.0151413679122925
Epoch 550, training loss: 0.019823594018816948 = 0.012552724219858646 + 0.001 * 7.270869731903076
Epoch 550, val loss: 1.0244724750518799
Epoch 560, training loss: 0.01902781054377556 = 0.011737305670976639 + 0.001 * 7.2905049324035645
Epoch 560, val loss: 1.033516764640808
Epoch 570, training loss: 0.01825764961540699 = 0.011001485399901867 + 0.001 * 7.256163597106934
Epoch 570, val loss: 1.042319655418396
Epoch 580, training loss: 0.0175955630838871 = 0.01033528707921505 + 0.001 * 7.260276794433594
Epoch 580, val loss: 1.050850749015808
Epoch 590, training loss: 0.01698465272784233 = 0.009730339981615543 + 0.001 * 7.254312038421631
Epoch 590, val loss: 1.0591453313827515
Epoch 600, training loss: 0.016462020576000214 = 0.009179538115859032 + 0.001 * 7.282483100891113
Epoch 600, val loss: 1.0672039985656738
Epoch 610, training loss: 0.01592652127146721 = 0.008676684461534023 + 0.001 * 7.24983549118042
Epoch 610, val loss: 1.0750175714492798
Epoch 620, training loss: 0.015469847247004509 = 0.00821647234261036 + 0.001 * 7.2533745765686035
Epoch 620, val loss: 1.0826281309127808
Epoch 630, training loss: 0.015040416270494461 = 0.007794080302119255 + 0.001 * 7.246335983276367
Epoch 630, val loss: 1.0900170803070068
Epoch 640, training loss: 0.014647630974650383 = 0.007405527867376804 + 0.001 * 7.242102146148682
Epoch 640, val loss: 1.0972089767456055
Epoch 650, training loss: 0.01429644413292408 = 0.0070473141968250275 + 0.001 * 7.2491302490234375
Epoch 650, val loss: 1.1041862964630127
Epoch 660, training loss: 0.013969294726848602 = 0.006716424133628607 + 0.001 * 7.252870559692383
Epoch 660, val loss: 1.110973834991455
Epoch 670, training loss: 0.013663334771990776 = 0.0064101070165634155 + 0.001 * 7.2532267570495605
Epoch 670, val loss: 1.1176122426986694
Epoch 680, training loss: 0.013361241668462753 = 0.006125954911112785 + 0.001 * 7.235286712646484
Epoch 680, val loss: 1.1240419149398804
Epoch 690, training loss: 0.013102423399686813 = 0.005861887242645025 + 0.001 * 7.240535736083984
Epoch 690, val loss: 1.1303205490112305
Epoch 700, training loss: 0.01285349577665329 = 0.005616095382720232 + 0.001 * 7.237400531768799
Epoch 700, val loss: 1.136441707611084
Epoch 710, training loss: 0.012618077918887138 = 0.0053868768736720085 + 0.001 * 7.231201171875
Epoch 710, val loss: 1.1423835754394531
Epoch 720, training loss: 0.012404071167111397 = 0.005172773730009794 + 0.001 * 7.231296539306641
Epoch 720, val loss: 1.1482031345367432
Epoch 730, training loss: 0.012195693328976631 = 0.0049724699929356575 + 0.001 * 7.2232232093811035
Epoch 730, val loss: 1.1538619995117188
Epoch 740, training loss: 0.011999895796179771 = 0.004784786608070135 + 0.001 * 7.215108871459961
Epoch 740, val loss: 1.159397840499878
Epoch 750, training loss: 0.011828935705125332 = 0.004608705639839172 + 0.001 * 7.220229625701904
Epoch 750, val loss: 1.1647933721542358
Epoch 760, training loss: 0.011676302179694176 = 0.0044432878494262695 + 0.001 * 7.2330145835876465
Epoch 760, val loss: 1.170042634010315
Epoch 770, training loss: 0.011508967727422714 = 0.004287674557417631 + 0.001 * 7.221292972564697
Epoch 770, val loss: 1.175220251083374
Epoch 780, training loss: 0.011342538520693779 = 0.004141105338931084 + 0.001 * 7.201432704925537
Epoch 780, val loss: 1.1802324056625366
Epoch 790, training loss: 0.011219467036426067 = 0.004002884961664677 + 0.001 * 7.21658182144165
Epoch 790, val loss: 1.1851316690444946
Epoch 800, training loss: 0.01109173521399498 = 0.0038724043406546116 + 0.001 * 7.219329833984375
Epoch 800, val loss: 1.1899480819702148
Epoch 810, training loss: 0.01097121275961399 = 0.003749071853235364 + 0.001 * 7.222140312194824
Epoch 810, val loss: 1.1946227550506592
Epoch 820, training loss: 0.010846590623259544 = 0.0036323927342891693 + 0.001 * 7.214198112487793
Epoch 820, val loss: 1.1992216110229492
Epoch 830, training loss: 0.010711895301938057 = 0.003521847538650036 + 0.001 * 7.190046787261963
Epoch 830, val loss: 1.2037023305892944
Epoch 840, training loss: 0.010642250999808311 = 0.003417049767449498 + 0.001 * 7.225200653076172
Epoch 840, val loss: 1.2080997228622437
Epoch 850, training loss: 0.010518116876482964 = 0.0033175924327224493 + 0.001 * 7.200523853302002
Epoch 850, val loss: 1.2123634815216064
Epoch 860, training loss: 0.010421987622976303 = 0.0032231223303824663 + 0.001 * 7.1988654136657715
Epoch 860, val loss: 1.2165708541870117
Epoch 870, training loss: 0.01033807173371315 = 0.0031332848593592644 + 0.001 * 7.204786777496338
Epoch 870, val loss: 1.220673680305481
Epoch 880, training loss: 0.010247579775750637 = 0.003047836711630225 + 0.001 * 7.199742794036865
Epoch 880, val loss: 1.2247116565704346
Epoch 890, training loss: 0.01017497107386589 = 0.0029664449393749237 + 0.001 * 7.208525657653809
Epoch 890, val loss: 1.2286468744277954
Epoch 900, training loss: 0.010060850530862808 = 0.0028888497035950422 + 0.001 * 7.172000408172607
Epoch 900, val loss: 1.232499122619629
Epoch 910, training loss: 0.009982980787754059 = 0.002814825624227524 + 0.001 * 7.168154716491699
Epoch 910, val loss: 1.2362726926803589
Epoch 920, training loss: 0.009917672723531723 = 0.00274416315369308 + 0.001 * 7.173509120941162
Epoch 920, val loss: 1.2399717569351196
Epoch 930, training loss: 0.00984252244234085 = 0.002676695352420211 + 0.001 * 7.165826320648193
Epoch 930, val loss: 1.2435975074768066
Epoch 940, training loss: 0.009772494435310364 = 0.0026122136041522026 + 0.001 * 7.160280704498291
Epoch 940, val loss: 1.2471429109573364
Epoch 950, training loss: 0.009738100692629814 = 0.0025505274534225464 + 0.001 * 7.187573432922363
Epoch 950, val loss: 1.2506486177444458
Epoch 960, training loss: 0.009672190062701702 = 0.0024914753157645464 + 0.001 * 7.1807146072387695
Epoch 960, val loss: 1.2540292739868164
Epoch 970, training loss: 0.009592516347765923 = 0.002434920286759734 + 0.001 * 7.157595634460449
Epoch 970, val loss: 1.2573747634887695
Epoch 980, training loss: 0.009576713666319847 = 0.002380711492151022 + 0.001 * 7.196002006530762
Epoch 980, val loss: 1.2606562376022339
Epoch 990, training loss: 0.009492412209510803 = 0.002328726928681135 + 0.001 * 7.163684368133545
Epoch 990, val loss: 1.263895869255066
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 1.9492188692092896 = 1.9406219720840454 + 0.001 * 8.596837997436523
Epoch 0, val loss: 1.9325554370880127
Epoch 10, training loss: 1.9389350414276123 = 1.9303382635116577 + 0.001 * 8.596786499023438
Epoch 10, val loss: 1.9229027032852173
Epoch 20, training loss: 1.9263637065887451 = 1.91776704788208 + 0.001 * 8.596630096435547
Epoch 20, val loss: 1.9105710983276367
Epoch 30, training loss: 1.9091302156448364 = 1.90053391456604 + 0.001 * 8.59628963470459
Epoch 30, val loss: 1.8932174444198608
Epoch 40, training loss: 1.884580135345459 = 1.8759845495224 + 0.001 * 8.595563888549805
Epoch 40, val loss: 1.8686647415161133
Epoch 50, training loss: 1.85169517993927 = 1.8431013822555542 + 0.001 * 8.593801498413086
Epoch 50, val loss: 1.8373817205429077
Epoch 60, training loss: 1.8164994716644287 = 1.8079109191894531 + 0.001 * 8.58857536315918
Epoch 60, val loss: 1.8078229427337646
Epoch 70, training loss: 1.7868225574493408 = 1.778254508972168 + 0.001 * 8.568045616149902
Epoch 70, val loss: 1.7856273651123047
Epoch 80, training loss: 1.7502456903457642 = 1.7418113946914673 + 0.001 * 8.434350967407227
Epoch 80, val loss: 1.754036545753479
Epoch 90, training loss: 1.6982496976852417 = 1.690086841583252 + 0.001 * 8.162802696228027
Epoch 90, val loss: 1.7083576917648315
Epoch 100, training loss: 1.6260462999343872 = 1.6179872751235962 + 0.001 * 8.059049606323242
Epoch 100, val loss: 1.6467020511627197
Epoch 110, training loss: 1.5362414121627808 = 1.528281807899475 + 0.001 * 7.959587097167969
Epoch 110, val loss: 1.5727540254592896
Epoch 120, training loss: 1.4380537271499634 = 1.4302586317062378 + 0.001 * 7.795041561126709
Epoch 120, val loss: 1.4932725429534912
Epoch 130, training loss: 1.3376882076263428 = 1.3300033807754517 + 0.001 * 7.684812545776367
Epoch 130, val loss: 1.41501784324646
Epoch 140, training loss: 1.2377336025238037 = 1.2300890684127808 + 0.001 * 7.6445207595825195
Epoch 140, val loss: 1.339957594871521
Epoch 150, training loss: 1.1393669843673706 = 1.1317979097366333 + 0.001 * 7.569041728973389
Epoch 150, val loss: 1.2682222127914429
Epoch 160, training loss: 1.0433019399642944 = 1.035782814025879 + 0.001 * 7.519134521484375
Epoch 160, val loss: 1.200179934501648
Epoch 170, training loss: 0.9498492479324341 = 0.942342221736908 + 0.001 * 7.507054805755615
Epoch 170, val loss: 1.1349204778671265
Epoch 180, training loss: 0.8587648868560791 = 0.8512610793113708 + 0.001 * 7.503780364990234
Epoch 180, val loss: 1.071374535560608
Epoch 190, training loss: 0.7699991464614868 = 0.7624987363815308 + 0.001 * 7.500381946563721
Epoch 190, val loss: 1.0093750953674316
Epoch 200, training loss: 0.6846271753311157 = 0.6771298050880432 + 0.001 * 7.497340679168701
Epoch 200, val loss: 0.9498811960220337
Epoch 210, training loss: 0.6048774719238281 = 0.5973838567733765 + 0.001 * 7.493610858917236
Epoch 210, val loss: 0.8954333662986755
Epoch 220, training loss: 0.5328291654586792 = 0.5253404974937439 + 0.001 * 7.488696575164795
Epoch 220, val loss: 0.8488898873329163
Epoch 230, training loss: 0.46959686279296875 = 0.46211522817611694 + 0.001 * 7.481642246246338
Epoch 230, val loss: 0.8119548559188843
Epoch 240, training loss: 0.4151766002178192 = 0.40770646929740906 + 0.001 * 7.470141887664795
Epoch 240, val loss: 0.7845069169998169
Epoch 250, training loss: 0.3690619170665741 = 0.3616137206554413 + 0.001 * 7.448193073272705
Epoch 250, val loss: 0.7657583951950073
Epoch 260, training loss: 0.3303334712982178 = 0.3229139447212219 + 0.001 * 7.419517993927002
Epoch 260, val loss: 0.7545660734176636
Epoch 270, training loss: 0.2977251410484314 = 0.29033079743385315 + 0.001 * 7.394347667694092
Epoch 270, val loss: 0.7494957447052002
Epoch 280, training loss: 0.26979508996009827 = 0.2624278962612152 + 0.001 * 7.367184638977051
Epoch 280, val loss: 0.7491626739501953
Epoch 290, training loss: 0.24515408277511597 = 0.23779508471488953 + 0.001 * 7.358996868133545
Epoch 290, val loss: 0.7523776292800903
Epoch 300, training loss: 0.22254423797130585 = 0.21519437432289124 + 0.001 * 7.349864482879639
Epoch 300, val loss: 0.7580550909042358
Epoch 310, training loss: 0.20104442536830902 = 0.19370000064373016 + 0.001 * 7.3444294929504395
Epoch 310, val loss: 0.7655535936355591
Epoch 320, training loss: 0.18022918701171875 = 0.17289312183856964 + 0.001 * 7.336061477661133
Epoch 320, val loss: 0.7742889523506165
Epoch 330, training loss: 0.16024798154830933 = 0.15290838479995728 + 0.001 * 7.339595317840576
Epoch 330, val loss: 0.7841160893440247
Epoch 340, training loss: 0.14159530401229858 = 0.134271502494812 + 0.001 * 7.323801517486572
Epoch 340, val loss: 0.7950390577316284
Epoch 350, training loss: 0.12484163045883179 = 0.11752880364656448 + 0.001 * 7.312828540802002
Epoch 350, val loss: 0.8070785403251648
Epoch 360, training loss: 0.11023547500371933 = 0.10293160378932953 + 0.001 * 7.303872585296631
Epoch 360, val loss: 0.8203723430633545
Epoch 370, training loss: 0.09771829098463058 = 0.09041842818260193 + 0.001 * 7.29986572265625
Epoch 370, val loss: 0.8346657752990723
Epoch 380, training loss: 0.0870450958609581 = 0.07975247502326965 + 0.001 * 7.29262113571167
Epoch 380, val loss: 0.8497291207313538
Epoch 390, training loss: 0.07793600112199783 = 0.07064894586801529 + 0.001 * 7.287054538726807
Epoch 390, val loss: 0.8654311299324036
Epoch 400, training loss: 0.07012419402599335 = 0.06283723562955856 + 0.001 * 7.286957740783691
Epoch 400, val loss: 0.8814464211463928
Epoch 410, training loss: 0.06338180601596832 = 0.05609307438135147 + 0.001 * 7.2887349128723145
Epoch 410, val loss: 0.8976003527641296
Epoch 420, training loss: 0.057522762566804886 = 0.05023575201630592 + 0.001 * 7.28701114654541
Epoch 420, val loss: 0.9136778116226196
Epoch 430, training loss: 0.052407413721084595 = 0.04512423649430275 + 0.001 * 7.283176898956299
Epoch 430, val loss: 0.9295414686203003
Epoch 440, training loss: 0.047928180545568466 = 0.04064841940999031 + 0.001 * 7.279759883880615
Epoch 440, val loss: 0.9450948238372803
Epoch 450, training loss: 0.044000573456287384 = 0.03672178089618683 + 0.001 * 7.2787933349609375
Epoch 450, val loss: 0.9603163003921509
Epoch 460, training loss: 0.04054614529013634 = 0.03327194228768349 + 0.001 * 7.274204254150391
Epoch 460, val loss: 0.9751385450363159
Epoch 470, training loss: 0.037510037422180176 = 0.030238304287195206 + 0.001 * 7.271730899810791
Epoch 470, val loss: 0.9895870685577393
Epoch 480, training loss: 0.0348433256149292 = 0.027566971257328987 + 0.001 * 7.276355266571045
Epoch 480, val loss: 1.0036565065383911
Epoch 490, training loss: 0.03247716650366783 = 0.02521071955561638 + 0.001 * 7.266447067260742
Epoch 490, val loss: 1.017317295074463
Epoch 500, training loss: 0.030396485701203346 = 0.023128075525164604 + 0.001 * 7.2684102058410645
Epoch 500, val loss: 1.0306034088134766
Epoch 510, training loss: 0.028547413647174835 = 0.021282583475112915 + 0.001 * 7.264830112457275
Epoch 510, val loss: 1.0434930324554443
Epoch 520, training loss: 0.026924002915620804 = 0.019642524421215057 + 0.001 * 7.281477928161621
Epoch 520, val loss: 1.055975079536438
Epoch 530, training loss: 0.025444667786359787 = 0.018180837854743004 + 0.001 * 7.263828754425049
Epoch 530, val loss: 1.068083643913269
Epoch 540, training loss: 0.0241325031965971 = 0.01687423326075077 + 0.001 * 7.258269786834717
Epoch 540, val loss: 1.0798083543777466
Epoch 550, training loss: 0.022966040298342705 = 0.015702711418271065 + 0.001 * 7.263329029083252
Epoch 550, val loss: 1.0911669731140137
Epoch 560, training loss: 0.021905887871980667 = 0.014649298042058945 + 0.001 * 7.256588459014893
Epoch 560, val loss: 1.1021555662155151
Epoch 570, training loss: 0.02097485400736332 = 0.013699119910597801 + 0.001 * 7.275733947753906
Epoch 570, val loss: 1.112815022468567
Epoch 580, training loss: 0.02008826844394207 = 0.012839791364967823 + 0.001 * 7.248476505279541
Epoch 580, val loss: 1.1231212615966797
Epoch 590, training loss: 0.01930784434080124 = 0.012060503475368023 + 0.001 * 7.247341156005859
Epoch 590, val loss: 1.1330982446670532
Epoch 600, training loss: 0.018612561747431755 = 0.011351779103279114 + 0.001 * 7.260781764984131
Epoch 600, val loss: 1.1427781581878662
Epoch 610, training loss: 0.017949972301721573 = 0.010705622844398022 + 0.001 * 7.244349479675293
Epoch 610, val loss: 1.1521711349487305
Epoch 620, training loss: 0.01735745184123516 = 0.010115050710737705 + 0.001 * 7.242400169372559
Epoch 620, val loss: 1.1612919569015503
Epoch 630, training loss: 0.016812006011605263 = 0.009573988616466522 + 0.001 * 7.238016605377197
Epoch 630, val loss: 1.1701422929763794
Epoch 640, training loss: 0.016306469216942787 = 0.009077155962586403 + 0.001 * 7.229313373565674
Epoch 640, val loss: 1.1787043809890747
Epoch 650, training loss: 0.015854377299547195 = 0.008619902655482292 + 0.001 * 7.234475135803223
Epoch 650, val loss: 1.1870406866073608
Epoch 660, training loss: 0.015426366589963436 = 0.008198190480470657 + 0.001 * 7.228175640106201
Epoch 660, val loss: 1.1951478719711304
Epoch 670, training loss: 0.015041645616292953 = 0.007808531168848276 + 0.001 * 7.233114242553711
Epoch 670, val loss: 1.2030247449874878
Epoch 680, training loss: 0.014673100784420967 = 0.007447740063071251 + 0.001 * 7.225360870361328
Epoch 680, val loss: 1.2106901407241821
Epoch 690, training loss: 0.014346394687891006 = 0.007113054394721985 + 0.001 * 7.233339309692383
Epoch 690, val loss: 1.218130111694336
Epoch 700, training loss: 0.014031969010829926 = 0.006802053190767765 + 0.001 * 7.229916095733643
Epoch 700, val loss: 1.225385308265686
Epoch 710, training loss: 0.013733603991568089 = 0.0065125515684485435 + 0.001 * 7.221052169799805
Epoch 710, val loss: 1.2324484586715698
Epoch 720, training loss: 0.013465534895658493 = 0.00624263659119606 + 0.001 * 7.222898483276367
Epoch 720, val loss: 1.239329218864441
Epoch 730, training loss: 0.01320297084748745 = 0.00599053455516696 + 0.001 * 7.212436199188232
Epoch 730, val loss: 1.2460285425186157
Epoch 740, training loss: 0.012973679229617119 = 0.0057547083124518394 + 0.001 * 7.218970775604248
Epoch 740, val loss: 1.2525535821914673
Epoch 750, training loss: 0.012746129184961319 = 0.005533834453672171 + 0.001 * 7.212294101715088
Epoch 750, val loss: 1.258934497833252
Epoch 760, training loss: 0.012547170743346214 = 0.005326641257852316 + 0.001 * 7.220528602600098
Epoch 760, val loss: 1.2651517391204834
Epoch 770, training loss: 0.012331830337643623 = 0.00513206422328949 + 0.001 * 7.199766159057617
Epoch 770, val loss: 1.2712153196334839
Epoch 780, training loss: 0.012160662561655045 = 0.004949058406054974 + 0.001 * 7.21160364151001
Epoch 780, val loss: 1.277105689048767
Epoch 790, training loss: 0.011974899098277092 = 0.0047767371870577335 + 0.001 * 7.198162078857422
Epoch 790, val loss: 1.282899022102356
Epoch 800, training loss: 0.011809676885604858 = 0.004614261444658041 + 0.001 * 7.195415496826172
Epoch 800, val loss: 1.2885469198226929
Epoch 810, training loss: 0.011652044951915741 = 0.004460914060473442 + 0.001 * 7.191130638122559
Epoch 810, val loss: 1.2940622568130493
Epoch 820, training loss: 0.011506004258990288 = 0.004315993748605251 + 0.001 * 7.1900105476379395
Epoch 820, val loss: 1.2994506359100342
Epoch 830, training loss: 0.011365794576704502 = 0.004178914707154036 + 0.001 * 7.186879634857178
Epoch 830, val loss: 1.3047370910644531
Epoch 840, training loss: 0.011238633655011654 = 0.004049116279929876 + 0.001 * 7.189517021179199
Epoch 840, val loss: 1.30989670753479
Epoch 850, training loss: 0.011110617779195309 = 0.003926076460629702 + 0.001 * 7.184540748596191
Epoch 850, val loss: 1.3149670362472534
Epoch 860, training loss: 0.011017689481377602 = 0.003809371031820774 + 0.001 * 7.208318710327148
Epoch 860, val loss: 1.3199083805084229
Epoch 870, training loss: 0.010872773826122284 = 0.0036985566839575768 + 0.001 * 7.174217224121094
Epoch 870, val loss: 1.32474946975708
Epoch 880, training loss: 0.010765556246042252 = 0.003593216650187969 + 0.001 * 7.17233943939209
Epoch 880, val loss: 1.329498052597046
Epoch 890, training loss: 0.010665560141205788 = 0.0034930165857076645 + 0.001 * 7.172543525695801
Epoch 890, val loss: 1.3341383934020996
Epoch 900, training loss: 0.01057451032102108 = 0.0033976107370108366 + 0.001 * 7.176898956298828
Epoch 900, val loss: 1.3387031555175781
Epoch 910, training loss: 0.010487777180969715 = 0.003306708065792918 + 0.001 * 7.181068420410156
Epoch 910, val loss: 1.343147873878479
Epoch 920, training loss: 0.010394202545285225 = 0.003220025449991226 + 0.001 * 7.174176216125488
Epoch 920, val loss: 1.3475178480148315
Epoch 930, training loss: 0.010307066142559052 = 0.003137284191325307 + 0.001 * 7.169781684875488
Epoch 930, val loss: 1.3518155813217163
Epoch 940, training loss: 0.010260598734021187 = 0.00305828545242548 + 0.001 * 7.20231294631958
Epoch 940, val loss: 1.3559942245483398
Epoch 950, training loss: 0.010153917595744133 = 0.0029828017577528954 + 0.001 * 7.171115875244141
Epoch 950, val loss: 1.3601250648498535
Epoch 960, training loss: 0.010113928467035294 = 0.002910612616688013 + 0.001 * 7.203315258026123
Epoch 960, val loss: 1.3641276359558105
Epoch 970, training loss: 0.00999270286411047 = 0.0028415273409336805 + 0.001 * 7.151175022125244
Epoch 970, val loss: 1.3681026697158813
Epoch 980, training loss: 0.009932158514857292 = 0.0027753638569265604 + 0.001 * 7.156794548034668
Epoch 980, val loss: 1.371970534324646
Epoch 990, training loss: 0.009862234815955162 = 0.0027119696605950594 + 0.001 * 7.150265216827393
Epoch 990, val loss: 1.375779151916504
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.816025303110174
The final CL Acc:0.77407, 0.02181, The final GNN Acc:0.81620, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13150])
remove edge: torch.Size([2, 7826])
updated graph: torch.Size([2, 10420])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9464141130447388 = 1.9378172159194946 + 0.001 * 8.596848487854004
Epoch 0, val loss: 1.937597393989563
Epoch 10, training loss: 1.9363220930099487 = 1.9277253150939941 + 0.001 * 8.596803665161133
Epoch 10, val loss: 1.9270069599151611
Epoch 20, training loss: 1.9235790967941284 = 1.9149824380874634 + 0.001 * 8.596650123596191
Epoch 20, val loss: 1.9134256839752197
Epoch 30, training loss: 1.905380368232727 = 1.8967840671539307 + 0.001 * 8.59626579284668
Epoch 30, val loss: 1.8940963745117188
Epoch 40, training loss: 1.878646969795227 = 1.870051622390747 + 0.001 * 8.595349311828613
Epoch 40, val loss: 1.8664653301239014
Epoch 50, training loss: 1.8428065776824951 = 1.8342136144638062 + 0.001 * 8.592970848083496
Epoch 50, val loss: 1.8317924737930298
Epoch 60, training loss: 1.8055155277252197 = 1.7969298362731934 + 0.001 * 8.585675239562988
Epoch 60, val loss: 1.8000417947769165
Epoch 70, training loss: 1.77146577835083 = 1.7629107236862183 + 0.001 * 8.55510425567627
Epoch 70, val loss: 1.772870659828186
Epoch 80, training loss: 1.724232792854309 = 1.7158743143081665 + 0.001 * 8.358488082885742
Epoch 80, val loss: 1.7313190698623657
Epoch 90, training loss: 1.6571980714797974 = 1.6490237712860107 + 0.001 * 8.174304008483887
Epoch 90, val loss: 1.6718000173568726
Epoch 100, training loss: 1.571146011352539 = 1.5630439519882202 + 0.001 * 8.101999282836914
Epoch 100, val loss: 1.5994945764541626
Epoch 110, training loss: 1.4787973165512085 = 1.4707435369491577 + 0.001 * 8.053762435913086
Epoch 110, val loss: 1.52467942237854
Epoch 120, training loss: 1.3884496688842773 = 1.380503535270691 + 0.001 * 7.946192264556885
Epoch 120, val loss: 1.453888177871704
Epoch 130, training loss: 1.298789620399475 = 1.2910876274108887 + 0.001 * 7.701989650726318
Epoch 130, val loss: 1.3857656717300415
Epoch 140, training loss: 1.208475112915039 = 1.200784683227539 + 0.001 * 7.690396308898926
Epoch 140, val loss: 1.3185369968414307
Epoch 150, training loss: 1.1183323860168457 = 1.1106678247451782 + 0.001 * 7.664521217346191
Epoch 150, val loss: 1.2528427839279175
Epoch 160, training loss: 1.030205249786377 = 1.022559404373169 + 0.001 * 7.645837783813477
Epoch 160, val loss: 1.1888784170150757
Epoch 170, training loss: 0.9451439380645752 = 0.9375190138816833 + 0.001 * 7.62495231628418
Epoch 170, val loss: 1.1268234252929688
Epoch 180, training loss: 0.8640894293785095 = 0.8564945459365845 + 0.001 * 7.594869136810303
Epoch 180, val loss: 1.066848635673523
Epoch 190, training loss: 0.7885216474533081 = 0.7809752225875854 + 0.001 * 7.546401023864746
Epoch 190, val loss: 1.0112762451171875
Epoch 200, training loss: 0.7200843095779419 = 0.7125836610794067 + 0.001 * 7.500646114349365
Epoch 200, val loss: 0.9629892110824585
Epoch 210, training loss: 0.6591118574142456 = 0.651621401309967 + 0.001 * 7.49043607711792
Epoch 210, val loss: 0.9235005378723145
Epoch 220, training loss: 0.6041852831840515 = 0.5967017412185669 + 0.001 * 7.483553886413574
Epoch 220, val loss: 0.8922364115715027
Epoch 230, training loss: 0.553402304649353 = 0.5459263920783997 + 0.001 * 7.475913047790527
Epoch 230, val loss: 0.8673593401908875
Epoch 240, training loss: 0.5054345726966858 = 0.49796807765960693 + 0.001 * 7.466472148895264
Epoch 240, val loss: 0.8471791744232178
Epoch 250, training loss: 0.45965200662612915 = 0.45219454169273376 + 0.001 * 7.457462310791016
Epoch 250, val loss: 0.830939531326294
Epoch 260, training loss: 0.41606858372688293 = 0.4086237847805023 + 0.001 * 7.444791793823242
Epoch 260, val loss: 0.8189458847045898
Epoch 270, training loss: 0.3749712407588959 = 0.36754310131073 + 0.001 * 7.428131103515625
Epoch 270, val loss: 0.8118389248847961
Epoch 280, training loss: 0.33671635389328003 = 0.3293118178844452 + 0.001 * 7.404534816741943
Epoch 280, val loss: 0.8098430633544922
Epoch 290, training loss: 0.30155307054519653 = 0.2941710352897644 + 0.001 * 7.382026195526123
Epoch 290, val loss: 0.8126454949378967
Epoch 300, training loss: 0.26953279972076416 = 0.26215171813964844 + 0.001 * 7.381078720092773
Epoch 300, val loss: 0.8198614716529846
Epoch 310, training loss: 0.2404993325471878 = 0.2331615388393402 + 0.001 * 7.33779764175415
Epoch 310, val loss: 0.8308621048927307
Epoch 320, training loss: 0.2143988162279129 = 0.20706714689731598 + 0.001 * 7.331667423248291
Epoch 320, val loss: 0.8450083136558533
Epoch 330, training loss: 0.19104041159152985 = 0.18372200429439545 + 0.001 * 7.318411350250244
Epoch 330, val loss: 0.8617758750915527
Epoch 340, training loss: 0.1702752709388733 = 0.16296349465847015 + 0.001 * 7.311779499053955
Epoch 340, val loss: 0.8806716203689575
Epoch 350, training loss: 0.1519380509853363 = 0.14462630450725555 + 0.001 * 7.311748504638672
Epoch 350, val loss: 0.9013937711715698
Epoch 360, training loss: 0.13581416010856628 = 0.12850672006607056 + 0.001 * 7.307444095611572
Epoch 360, val loss: 0.9236070513725281
Epoch 370, training loss: 0.12168839573860168 = 0.11438494175672531 + 0.001 * 7.303456783294678
Epoch 370, val loss: 0.9469761848449707
Epoch 380, training loss: 0.10933685302734375 = 0.10203512758016586 + 0.001 * 7.301724433898926
Epoch 380, val loss: 0.971221387386322
Epoch 390, training loss: 0.09854105114936829 = 0.0912342444062233 + 0.001 * 7.306809425354004
Epoch 390, val loss: 0.9961028099060059
Epoch 400, training loss: 0.0890829935669899 = 0.08177969604730606 + 0.001 * 7.303300380706787
Epoch 400, val loss: 1.0213044881820679
Epoch 410, training loss: 0.08079095184803009 = 0.07349223643541336 + 0.001 * 7.298717021942139
Epoch 410, val loss: 1.0465528964996338
Epoch 420, training loss: 0.07351730018854141 = 0.06621649861335754 + 0.001 * 7.3008012771606445
Epoch 420, val loss: 1.0717395544052124
Epoch 430, training loss: 0.06711949408054352 = 0.05981907993555069 + 0.001 * 7.300410270690918
Epoch 430, val loss: 1.0966994762420654
Epoch 440, training loss: 0.06147675961256027 = 0.0541817843914032 + 0.001 * 7.294976711273193
Epoch 440, val loss: 1.1213093996047974
Epoch 450, training loss: 0.056499846279621124 = 0.049203287810087204 + 0.001 * 7.296557426452637
Epoch 450, val loss: 1.1454392671585083
Epoch 460, training loss: 0.05209200829267502 = 0.04479677602648735 + 0.001 * 7.295232772827148
Epoch 460, val loss: 1.1690820455551147
Epoch 470, training loss: 0.048177581280469894 = 0.04088781774044037 + 0.001 * 7.289763450622559
Epoch 470, val loss: 1.19217050075531
Epoch 480, training loss: 0.04470298811793327 = 0.037412650883197784 + 0.001 * 7.2903361320495605
Epoch 480, val loss: 1.214684009552002
Epoch 490, training loss: 0.041602328419685364 = 0.034316595643758774 + 0.001 * 7.285733222961426
Epoch 490, val loss: 1.236642599105835
Epoch 500, training loss: 0.03883617743849754 = 0.031552720814943314 + 0.001 * 7.2834577560424805
Epoch 500, val loss: 1.2579988241195679
Epoch 510, training loss: 0.03638586029410362 = 0.029080534353852272 + 0.001 * 7.305325031280518
Epoch 510, val loss: 1.2787703275680542
Epoch 520, training loss: 0.034148599952459335 = 0.026864925399422646 + 0.001 * 7.283673286437988
Epoch 520, val loss: 1.2989624738693237
Epoch 530, training loss: 0.032151445746421814 = 0.024875130504369736 + 0.001 * 7.276313781738281
Epoch 530, val loss: 1.3185853958129883
Epoch 540, training loss: 0.030355917289853096 = 0.023084472864866257 + 0.001 * 7.271443843841553
Epoch 540, val loss: 1.3376150131225586
Epoch 550, training loss: 0.02874450944364071 = 0.021469833329319954 + 0.001 * 7.2746758460998535
Epoch 550, val loss: 1.3561129570007324
Epoch 560, training loss: 0.027287373319268227 = 0.02001100406050682 + 0.001 * 7.276368618011475
Epoch 560, val loss: 1.3740875720977783
Epoch 570, training loss: 0.02595915086567402 = 0.018690112978219986 + 0.001 * 7.269037246704102
Epoch 570, val loss: 1.3915263414382935
Epoch 580, training loss: 0.02475792169570923 = 0.017491508275270462 + 0.001 * 7.266413688659668
Epoch 580, val loss: 1.4084689617156982
Epoch 590, training loss: 0.023660823702812195 = 0.01640167087316513 + 0.001 * 7.259152412414551
Epoch 590, val loss: 1.424932837486267
Epoch 600, training loss: 0.022662902250885963 = 0.015408671461045742 + 0.001 * 7.25423002243042
Epoch 600, val loss: 1.4409323930740356
Epoch 610, training loss: 0.021780475974082947 = 0.014502063393592834 + 0.001 * 7.27841329574585
Epoch 610, val loss: 1.4564546346664429
Epoch 620, training loss: 0.020931433886289597 = 0.013672742992639542 + 0.001 * 7.258691310882568
Epoch 620, val loss: 1.471543550491333
Epoch 630, training loss: 0.0201546810567379 = 0.012912471778690815 + 0.001 * 7.2422099113464355
Epoch 630, val loss: 1.4862163066864014
Epoch 640, training loss: 0.019475281238555908 = 0.012214182876050472 + 0.001 * 7.261096954345703
Epoch 640, val loss: 1.5004963874816895
Epoch 650, training loss: 0.018819376826286316 = 0.011571641080081463 + 0.001 * 7.247735023498535
Epoch 650, val loss: 1.5143686532974243
Epoch 660, training loss: 0.018230104818940163 = 0.010979312472045422 + 0.001 * 7.250791549682617
Epoch 660, val loss: 1.5278425216674805
Epoch 670, training loss: 0.01766262948513031 = 0.010432297363877296 + 0.001 * 7.2303314208984375
Epoch 670, val loss: 1.5409904718399048
Epoch 680, training loss: 0.01716863177716732 = 0.00992625579237938 + 0.001 * 7.242376327514648
Epoch 680, val loss: 1.553756594657898
Epoch 690, training loss: 0.01669350638985634 = 0.009457340463995934 + 0.001 * 7.236165523529053
Epoch 690, val loss: 1.5662025213241577
Epoch 700, training loss: 0.01626429706811905 = 0.00902203656733036 + 0.001 * 7.242260932922363
Epoch 700, val loss: 1.578307867050171
Epoch 710, training loss: 0.015839362516999245 = 0.008617374114692211 + 0.001 * 7.221988201141357
Epoch 710, val loss: 1.5901036262512207
Epoch 720, training loss: 0.015459466725587845 = 0.008240575902163982 + 0.001 * 7.21889066696167
Epoch 720, val loss: 1.601591944694519
Epoch 730, training loss: 0.015099633485078812 = 0.007889238186180592 + 0.001 * 7.210394859313965
Epoch 730, val loss: 1.612795114517212
Epoch 740, training loss: 0.014771625399589539 = 0.007561129052191973 + 0.001 * 7.210495948791504
Epoch 740, val loss: 1.6237070560455322
Epoch 750, training loss: 0.014470504596829414 = 0.007254294119775295 + 0.001 * 7.21621036529541
Epoch 750, val loss: 1.634343147277832
Epoch 760, training loss: 0.014168337918817997 = 0.006966973189264536 + 0.001 * 7.201364517211914
Epoch 760, val loss: 1.6447299718856812
Epoch 770, training loss: 0.01388557069003582 = 0.006697577890008688 + 0.001 * 7.187993049621582
Epoch 770, val loss: 1.6548634767532349
Epoch 780, training loss: 0.013641845434904099 = 0.006444680038839579 + 0.001 * 7.197164535522461
Epoch 780, val loss: 1.664752721786499
Epoch 790, training loss: 0.013401385396718979 = 0.006206980440765619 + 0.001 * 7.194404125213623
Epoch 790, val loss: 1.6743972301483154
Epoch 800, training loss: 0.0131600396707654 = 0.005983317736536264 + 0.001 * 7.176721572875977
Epoch 800, val loss: 1.6838328838348389
Epoch 810, training loss: 0.01295953057706356 = 0.005772591568529606 + 0.001 * 7.186938762664795
Epoch 810, val loss: 1.693023681640625
Epoch 820, training loss: 0.012759223580360413 = 0.005573841277509928 + 0.001 * 7.1853814125061035
Epoch 820, val loss: 1.7020213603973389
Epoch 830, training loss: 0.012563366442918777 = 0.005386169999837875 + 0.001 * 7.177196025848389
Epoch 830, val loss: 1.7108242511749268
Epoch 840, training loss: 0.012379629537463188 = 0.005208778195083141 + 0.001 * 7.17085075378418
Epoch 840, val loss: 1.71942937374115
Epoch 850, training loss: 0.012211782857775688 = 0.0050409589894115925 + 0.001 * 7.170823097229004
Epoch 850, val loss: 1.727841854095459
Epoch 860, training loss: 0.012046225368976593 = 0.004882019013166428 + 0.001 * 7.164206504821777
Epoch 860, val loss: 1.7360745668411255
Epoch 870, training loss: 0.011908406391739845 = 0.004731355234980583 + 0.001 * 7.177050590515137
Epoch 870, val loss: 1.7441033124923706
Epoch 880, training loss: 0.011746278032660484 = 0.004588386509567499 + 0.001 * 7.157890796661377
Epoch 880, val loss: 1.7519490718841553
Epoch 890, training loss: 0.011609306558966637 = 0.004452630877494812 + 0.001 * 7.156675338745117
Epoch 890, val loss: 1.7596683502197266
Epoch 900, training loss: 0.011484416201710701 = 0.004323587752878666 + 0.001 * 7.160828113555908
Epoch 900, val loss: 1.767195463180542
Epoch 910, training loss: 0.01135600358247757 = 0.004200873896479607 + 0.001 * 7.1551289558410645
Epoch 910, val loss: 1.7745881080627441
Epoch 920, training loss: 0.011247936636209488 = 0.004084032028913498 + 0.001 * 7.163903713226318
Epoch 920, val loss: 1.7818185091018677
Epoch 930, training loss: 0.011132407933473587 = 0.003972737584263086 + 0.001 * 7.159669399261475
Epoch 930, val loss: 1.788926362991333
Epoch 940, training loss: 0.011030861176550388 = 0.003866594983264804 + 0.001 * 7.1642656326293945
Epoch 940, val loss: 1.795858383178711
Epoch 950, training loss: 0.010928238742053509 = 0.0037653124891221523 + 0.001 * 7.162925720214844
Epoch 950, val loss: 1.802646279335022
Epoch 960, training loss: 0.010829446837306023 = 0.0036686144303530455 + 0.001 * 7.160832405090332
Epoch 960, val loss: 1.8093299865722656
Epoch 970, training loss: 0.010716384276747704 = 0.0035762148909270763 + 0.001 * 7.140169143676758
Epoch 970, val loss: 1.8158595561981201
Epoch 980, training loss: 0.010646575130522251 = 0.003487879876047373 + 0.001 * 7.158694744110107
Epoch 980, val loss: 1.822252869606018
Epoch 990, training loss: 0.010553177446126938 = 0.0034033358097076416 + 0.001 * 7.149841785430908
Epoch 990, val loss: 1.8285139799118042
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 1.940070629119873 = 1.9314738512039185 + 0.001 * 8.596832275390625
Epoch 0, val loss: 1.9316010475158691
Epoch 10, training loss: 1.9300899505615234 = 1.9214931726455688 + 0.001 * 8.596776962280273
Epoch 10, val loss: 1.9214156866073608
Epoch 20, training loss: 1.9182398319244385 = 1.9096431732177734 + 0.001 * 8.596614837646484
Epoch 20, val loss: 1.9091308116912842
Epoch 30, training loss: 1.9021356105804443 = 1.893539309501648 + 0.001 * 8.596288681030273
Epoch 30, val loss: 1.8924092054367065
Epoch 40, training loss: 1.8787951469421387 = 1.8701995611190796 + 0.001 * 8.595555305480957
Epoch 40, val loss: 1.8683809041976929
Epoch 50, training loss: 1.8462649583816528 = 1.8376713991165161 + 0.001 * 8.59361743927002
Epoch 50, val loss: 1.8360739946365356
Epoch 60, training loss: 1.8086555004119873 = 1.8000682592391968 + 0.001 * 8.587263107299805
Epoch 60, val loss: 1.8019604682922363
Epoch 70, training loss: 1.7730381488800049 = 1.7644779682159424 + 0.001 * 8.5601224899292
Epoch 70, val loss: 1.7723604440689087
Epoch 80, training loss: 1.727417230606079 = 1.7190382480621338 + 0.001 * 8.37900161743164
Epoch 80, val loss: 1.7319164276123047
Epoch 90, training loss: 1.6625226736068726 = 1.654424786567688 + 0.001 * 8.097853660583496
Epoch 90, val loss: 1.67390775680542
Epoch 100, training loss: 1.5767344236373901 = 1.5688276290893555 + 0.001 * 7.906790733337402
Epoch 100, val loss: 1.600406289100647
Epoch 110, training loss: 1.4753432273864746 = 1.4676578044891357 + 0.001 * 7.685408115386963
Epoch 110, val loss: 1.5154908895492554
Epoch 120, training loss: 1.3682126998901367 = 1.3606151342391968 + 0.001 * 7.5976057052612305
Epoch 120, val loss: 1.4269551038742065
Epoch 130, training loss: 1.2604089975357056 = 1.2528820037841797 + 0.001 * 7.5270185470581055
Epoch 130, val loss: 1.3385467529296875
Epoch 140, training loss: 1.1550381183624268 = 1.147560715675354 + 0.001 * 7.4774580001831055
Epoch 140, val loss: 1.2529845237731934
Epoch 150, training loss: 1.054828405380249 = 1.0473713874816895 + 0.001 * 7.4570088386535645
Epoch 150, val loss: 1.1728897094726562
Epoch 160, training loss: 0.9616909623146057 = 0.9542425870895386 + 0.001 * 7.44834566116333
Epoch 160, val loss: 1.1001919507980347
Epoch 170, training loss: 0.8758857846260071 = 0.8684461712837219 + 0.001 * 7.439599990844727
Epoch 170, val loss: 1.0347552299499512
Epoch 180, training loss: 0.797333836555481 = 0.789901614189148 + 0.001 * 7.4322333335876465
Epoch 180, val loss: 0.9765002131462097
Epoch 190, training loss: 0.7263637781143188 = 0.7189412117004395 + 0.001 * 7.4225544929504395
Epoch 190, val loss: 0.9257800579071045
Epoch 200, training loss: 0.6631758809089661 = 0.6557682156562805 + 0.001 * 7.407654762268066
Epoch 200, val loss: 0.8835319876670837
Epoch 210, training loss: 0.6073110103607178 = 0.5999279618263245 + 0.001 * 7.3830389976501465
Epoch 210, val loss: 0.8497995734214783
Epoch 220, training loss: 0.5576361417770386 = 0.550277590751648 + 0.001 * 7.358558177947998
Epoch 220, val loss: 0.8236373662948608
Epoch 230, training loss: 0.5127382278442383 = 0.5054060816764832 + 0.001 * 7.332132816314697
Epoch 230, val loss: 0.8035370707511902
Epoch 240, training loss: 0.4713495373725891 = 0.46403416991233826 + 0.001 * 7.315360069274902
Epoch 240, val loss: 0.7880493998527527
Epoch 250, training loss: 0.43249979615211487 = 0.42519107460975647 + 0.001 * 7.308727741241455
Epoch 250, val loss: 0.7759278416633606
Epoch 260, training loss: 0.3956701457500458 = 0.38836583495140076 + 0.001 * 7.304305076599121
Epoch 260, val loss: 0.766843855381012
Epoch 270, training loss: 0.3607117831707001 = 0.3534102439880371 + 0.001 * 7.301535606384277
Epoch 270, val loss: 0.7608041167259216
Epoch 280, training loss: 0.3276858329772949 = 0.32038697600364685 + 0.001 * 7.298846244812012
Epoch 280, val loss: 0.7578796744346619
Epoch 290, training loss: 0.2966853082180023 = 0.2893892526626587 + 0.001 * 7.296046257019043
Epoch 290, val loss: 0.757885217666626
Epoch 300, training loss: 0.2677266597747803 = 0.2604314386844635 + 0.001 * 7.295206069946289
Epoch 300, val loss: 0.7605851292610168
Epoch 310, training loss: 0.24073299765586853 = 0.23343370854854584 + 0.001 * 7.299286842346191
Epoch 310, val loss: 0.7655941247940063
Epoch 320, training loss: 0.21555066108703613 = 0.20826098322868347 + 0.001 * 7.289677619934082
Epoch 320, val loss: 0.7725449204444885
Epoch 330, training loss: 0.192149817943573 = 0.1848648339509964 + 0.001 * 7.284977436065674
Epoch 330, val loss: 0.7808831930160522
Epoch 340, training loss: 0.1705910563468933 = 0.16330958902835846 + 0.001 * 7.281463146209717
Epoch 340, val loss: 0.7905289530754089
Epoch 350, training loss: 0.1509810984134674 = 0.14370253682136536 + 0.001 * 7.278557300567627
Epoch 350, val loss: 0.8012820482254028
Epoch 360, training loss: 0.13340167701244354 = 0.12611985206604004 + 0.001 * 7.28183126449585
Epoch 360, val loss: 0.813007116317749
Epoch 370, training loss: 0.1178046390414238 = 0.11053194850683212 + 0.001 * 7.2726874351501465
Epoch 370, val loss: 0.8255704045295715
Epoch 380, training loss: 0.10410396754741669 = 0.09683296829462051 + 0.001 * 7.270996570587158
Epoch 380, val loss: 0.8390352129936218
Epoch 390, training loss: 0.09214191138744354 = 0.08487717807292938 + 0.001 * 7.26473331451416
Epoch 390, val loss: 0.8530596494674683
Epoch 400, training loss: 0.081747867166996 = 0.07448971271514893 + 0.001 * 7.258152961730957
Epoch 400, val loss: 0.8673631548881531
Epoch 410, training loss: 0.07276320457458496 = 0.06548508256673813 + 0.001 * 7.278125286102295
Epoch 410, val loss: 0.8817536234855652
Epoch 420, training loss: 0.0649518221616745 = 0.05769392102956772 + 0.001 * 7.257896900177002
Epoch 420, val loss: 0.8960564136505127
Epoch 430, training loss: 0.05821197107434273 = 0.050964634865522385 + 0.001 * 7.2473368644714355
Epoch 430, val loss: 0.9101728200912476
Epoch 440, training loss: 0.05243407562375069 = 0.04516054317355156 + 0.001 * 7.273531913757324
Epoch 440, val loss: 0.9239836931228638
Epoch 450, training loss: 0.0473901741206646 = 0.0401570200920105 + 0.001 * 7.233154296875
Epoch 450, val loss: 0.9374149441719055
Epoch 460, training loss: 0.04307163506746292 = 0.03584093227982521 + 0.001 * 7.2307024002075195
Epoch 460, val loss: 0.9504178762435913
Epoch 470, training loss: 0.039341650903224945 = 0.03211294487118721 + 0.001 * 7.228704452514648
Epoch 470, val loss: 0.9629971981048584
Epoch 480, training loss: 0.0361015647649765 = 0.02888595499098301 + 0.001 * 7.2156081199646
Epoch 480, val loss: 0.975125253200531
Epoch 490, training loss: 0.033317361027002335 = 0.02608487382531166 + 0.001 * 7.232486724853516
Epoch 490, val loss: 0.9868393540382385
Epoch 500, training loss: 0.03086335025727749 = 0.02364625781774521 + 0.001 * 7.2170915603637695
Epoch 500, val loss: 0.998108446598053
Epoch 510, training loss: 0.02873990125954151 = 0.021516218781471252 + 0.001 * 7.223681926727295
Epoch 510, val loss: 1.0089529752731323
Epoch 520, training loss: 0.02686874382197857 = 0.019649075344204903 + 0.001 * 7.219667911529541
Epoch 520, val loss: 1.019385814666748
Epoch 530, training loss: 0.025219600647687912 = 0.0180068202316761 + 0.001 * 7.212779521942139
Epoch 530, val loss: 1.029427409172058
Epoch 540, training loss: 0.0237557515501976 = 0.01655685342848301 + 0.001 * 7.198897838592529
Epoch 540, val loss: 1.0390851497650146
Epoch 550, training loss: 0.022470511496067047 = 0.015271909534931183 + 0.001 * 7.198602676391602
Epoch 550, val loss: 1.0483895540237427
Epoch 560, training loss: 0.02133122645318508 = 0.01412911619991064 + 0.001 * 7.202109336853027
Epoch 560, val loss: 1.0573461055755615
Epoch 570, training loss: 0.020305784419178963 = 0.013109374791383743 + 0.001 * 7.196408748626709
Epoch 570, val loss: 1.0659751892089844
Epoch 580, training loss: 0.019391430541872978 = 0.012196255847811699 + 0.001 * 7.195173740386963
Epoch 580, val loss: 1.0742539167404175
Epoch 590, training loss: 0.018564261496067047 = 0.011376019567251205 + 0.001 * 7.188241481781006
Epoch 590, val loss: 1.0822436809539795
Epoch 600, training loss: 0.017823684960603714 = 0.010636944323778152 + 0.001 * 7.186739444732666
Epoch 600, val loss: 1.0899238586425781
Epoch 610, training loss: 0.017159562557935715 = 0.00996893085539341 + 0.001 * 7.190630912780762
Epoch 610, val loss: 1.0973138809204102
Epoch 620, training loss: 0.016540009528398514 = 0.009363433346152306 + 0.001 * 7.176576137542725
Epoch 620, val loss: 1.104435682296753
Epoch 630, training loss: 0.01599702425301075 = 0.008813089691102505 + 0.001 * 7.183934211730957
Epoch 630, val loss: 1.111315369606018
Epoch 640, training loss: 0.015486964955925941 = 0.008311602286994457 + 0.001 * 7.1753621101379395
Epoch 640, val loss: 1.1179604530334473
Epoch 650, training loss: 0.015043569728732109 = 0.007853468880057335 + 0.001 * 7.19010066986084
Epoch 650, val loss: 1.1243666410446167
Epoch 660, training loss: 0.01460267510265112 = 0.00743396021425724 + 0.001 * 7.16871452331543
Epoch 660, val loss: 1.1305700540542603
Epoch 670, training loss: 0.014224423095583916 = 0.007048939820379019 + 0.001 * 7.175483226776123
Epoch 670, val loss: 1.136587381362915
Epoch 680, training loss: 0.013867868110537529 = 0.006694768089801073 + 0.001 * 7.173099517822266
Epoch 680, val loss: 1.1423758268356323
Epoch 690, training loss: 0.013538362458348274 = 0.00636829761788249 + 0.001 * 7.170064449310303
Epoch 690, val loss: 1.148019790649414
Epoch 700, training loss: 0.013256951235234737 = 0.006066768430173397 + 0.001 * 7.190182685852051
Epoch 700, val loss: 1.1534615755081177
Epoch 710, training loss: 0.012957064434885979 = 0.005787703674286604 + 0.001 * 7.169360637664795
Epoch 710, val loss: 1.1587440967559814
Epoch 720, training loss: 0.0126928286626935 = 0.005528721492737532 + 0.001 * 7.164106845855713
Epoch 720, val loss: 1.1638448238372803
Epoch 730, training loss: 0.012450015172362328 = 0.0052873520180583 + 0.001 * 7.162662506103516
Epoch 730, val loss: 1.1688162088394165
Epoch 740, training loss: 0.01222805306315422 = 0.005060716066509485 + 0.001 * 7.167336940765381
Epoch 740, val loss: 1.1736592054367065
Epoch 750, training loss: 0.012011224403977394 = 0.0048464080318808556 + 0.001 * 7.164815425872803
Epoch 750, val loss: 1.1783697605133057
Epoch 760, training loss: 0.011793779209256172 = 0.004643086344003677 + 0.001 * 7.150691986083984
Epoch 760, val loss: 1.1829689741134644
Epoch 770, training loss: 0.011605177074670792 = 0.00445016473531723 + 0.001 * 7.155012130737305
Epoch 770, val loss: 1.1874290704727173
Epoch 780, training loss: 0.011418882757425308 = 0.004267378710210323 + 0.001 * 7.151503562927246
Epoch 780, val loss: 1.191781759262085
Epoch 790, training loss: 0.011251959018409252 = 0.004094610922038555 + 0.001 * 7.157347679138184
Epoch 790, val loss: 1.1960147619247437
Epoch 800, training loss: 0.011096024885773659 = 0.0039315782487392426 + 0.001 * 7.164445877075195
Epoch 800, val loss: 1.2001140117645264
Epoch 810, training loss: 0.010931218974292278 = 0.0037779388949275017 + 0.001 * 7.153279781341553
Epoch 810, val loss: 1.2041252851486206
Epoch 820, training loss: 0.010783812031149864 = 0.003633244661614299 + 0.001 * 7.150567054748535
Epoch 820, val loss: 1.2080258131027222
Epoch 830, training loss: 0.010674561373889446 = 0.0034970075357705355 + 0.001 * 7.177553176879883
Epoch 830, val loss: 1.2118152379989624
Epoch 840, training loss: 0.010507926344871521 = 0.003368759760633111 + 0.001 * 7.139166355133057
Epoch 840, val loss: 1.2155189514160156
Epoch 850, training loss: 0.010388685390353203 = 0.003247999818995595 + 0.001 * 7.140685558319092
Epoch 850, val loss: 1.2191146612167358
Epoch 860, training loss: 0.010265205055475235 = 0.0031342473812401295 + 0.001 * 7.130957126617432
Epoch 860, val loss: 1.22262442111969
Epoch 870, training loss: 0.010155151598155499 = 0.0030270323622971773 + 0.001 * 7.128118991851807
Epoch 870, val loss: 1.2260299921035767
Epoch 880, training loss: 0.0100610526278615 = 0.00292588141746819 + 0.001 * 7.135170936584473
Epoch 880, val loss: 1.2293477058410645
Epoch 890, training loss: 0.009960324503481388 = 0.0028304215520620346 + 0.001 * 7.1299028396606445
Epoch 890, val loss: 1.2325851917266846
Epoch 900, training loss: 0.00991328526288271 = 0.002740263007581234 + 0.001 * 7.1730217933654785
Epoch 900, val loss: 1.235731601715088
Epoch 910, training loss: 0.009779548272490501 = 0.002655073767527938 + 0.001 * 7.124474048614502
Epoch 910, val loss: 1.2387930154800415
Epoch 920, training loss: 0.009711370803415775 = 0.0025744482409209013 + 0.001 * 7.1369218826293945
Epoch 920, val loss: 1.2417738437652588
Epoch 930, training loss: 0.009622829034924507 = 0.0024981151800602674 + 0.001 * 7.12471342086792
Epoch 930, val loss: 1.2446860074996948
Epoch 940, training loss: 0.009546278044581413 = 0.0024257691111415625 + 0.001 * 7.120508193969727
Epoch 940, val loss: 1.2475178241729736
Epoch 950, training loss: 0.009477706626057625 = 0.002357164863497019 + 0.001 * 7.120542049407959
Epoch 950, val loss: 1.2502796649932861
Epoch 960, training loss: 0.009405070915818214 = 0.0022920402698218822 + 0.001 * 7.113029956817627
Epoch 960, val loss: 1.2529469728469849
Epoch 970, training loss: 0.00935065746307373 = 0.002230151556432247 + 0.001 * 7.1205058097839355
Epoch 970, val loss: 1.2555416822433472
Epoch 980, training loss: 0.009288346394896507 = 0.0021713203750550747 + 0.001 * 7.117025852203369
Epoch 980, val loss: 1.2580883502960205
Epoch 990, training loss: 0.009227525442838669 = 0.002115330658853054 + 0.001 * 7.112195014953613
Epoch 990, val loss: 1.2605533599853516
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 1.9601778984069824 = 1.9515810012817383 + 0.001 * 8.596843719482422
Epoch 0, val loss: 1.9411240816116333
Epoch 10, training loss: 1.948907732963562 = 1.9403109550476074 + 0.001 * 8.596793174743652
Epoch 10, val loss: 1.9297932386398315
Epoch 20, training loss: 1.9350494146347046 = 1.9264527559280396 + 0.001 * 8.596630096435547
Epoch 20, val loss: 1.9156068563461304
Epoch 30, training loss: 1.9158684015274048 = 1.9072721004486084 + 0.001 * 8.596242904663086
Epoch 30, val loss: 1.896032691001892
Epoch 40, training loss: 1.8882226943969727 = 1.8796273469924927 + 0.001 * 8.595300674438477
Epoch 40, val loss: 1.868399977684021
Epoch 50, training loss: 1.8511790037155151 = 1.8425862789154053 + 0.001 * 8.592682838439941
Epoch 50, val loss: 1.8338298797607422
Epoch 60, training loss: 1.8124676942825317 = 1.8038837909698486 + 0.001 * 8.58388900756836
Epoch 60, val loss: 1.8029680252075195
Epoch 70, training loss: 1.780788540840149 = 1.7722430229187012 + 0.001 * 8.545557975769043
Epoch 70, val loss: 1.7807409763336182
Epoch 80, training loss: 1.7407933473587036 = 1.732467770576477 + 0.001 * 8.325590133666992
Epoch 80, val loss: 1.7464343309402466
Epoch 90, training loss: 1.6846158504486084 = 1.676430344581604 + 0.001 * 8.185467720031738
Epoch 90, val loss: 1.6958948373794556
Epoch 100, training loss: 1.608455777168274 = 1.600374460220337 + 0.001 * 8.081257820129395
Epoch 100, val loss: 1.6294795274734497
Epoch 110, training loss: 1.5212664604187012 = 1.513266682624817 + 0.001 * 7.999786376953125
Epoch 110, val loss: 1.557299256324768
Epoch 120, training loss: 1.436024785041809 = 1.4281984567642212 + 0.001 * 7.826336860656738
Epoch 120, val loss: 1.4902833700180054
Epoch 130, training loss: 1.3552899360656738 = 1.3476563692092896 + 0.001 * 7.633533000946045
Epoch 130, val loss: 1.429090976715088
Epoch 140, training loss: 1.2751712799072266 = 1.2675762176513672 + 0.001 * 7.595120429992676
Epoch 140, val loss: 1.368096947669983
Epoch 150, training loss: 1.1914101839065552 = 1.1838635206222534 + 0.001 * 7.546669960021973
Epoch 150, val loss: 1.3031933307647705
Epoch 160, training loss: 1.103672742843628 = 1.0961544513702393 + 0.001 * 7.518245220184326
Epoch 160, val loss: 1.2348451614379883
Epoch 170, training loss: 1.0147044658660889 = 1.0072051286697388 + 0.001 * 7.499308109283447
Epoch 170, val loss: 1.1656047105789185
Epoch 180, training loss: 0.9281689524650574 = 0.9206825494766235 + 0.001 * 7.486382961273193
Epoch 180, val loss: 1.0986229181289673
Epoch 190, training loss: 0.8471075296401978 = 0.8396270871162415 + 0.001 * 7.480452537536621
Epoch 190, val loss: 1.0361220836639404
Epoch 200, training loss: 0.7735405564308167 = 0.7660647034645081 + 0.001 * 7.47583532333374
Epoch 200, val loss: 0.9797201752662659
Epoch 210, training loss: 0.7080125212669373 = 0.7005419731140137 + 0.001 * 7.470555782318115
Epoch 210, val loss: 0.9302268624305725
Epoch 220, training loss: 0.6491875052452087 = 0.6417229771614075 + 0.001 * 7.464541912078857
Epoch 220, val loss: 0.8872936367988586
Epoch 230, training loss: 0.5943019390106201 = 0.586844801902771 + 0.001 * 7.457150936126709
Epoch 230, val loss: 0.8496662378311157
Epoch 240, training loss: 0.5409196019172668 = 0.5334722399711609 + 0.001 * 7.447385787963867
Epoch 240, val loss: 0.8161705732345581
Epoch 250, training loss: 0.48797744512557983 = 0.4805401563644409 + 0.001 * 7.4372878074646
Epoch 250, val loss: 0.786442756652832
Epoch 260, training loss: 0.4362076222896576 = 0.4287858307361603 + 0.001 * 7.421791076660156
Epoch 260, val loss: 0.761144757270813
Epoch 270, training loss: 0.38718748092651367 = 0.3797878324985504 + 0.001 * 7.399649143218994
Epoch 270, val loss: 0.7413490414619446
Epoch 280, training loss: 0.3420938551425934 = 0.3347240090370178 + 0.001 * 7.36983585357666
Epoch 280, val loss: 0.7273529767990112
Epoch 290, training loss: 0.301400750875473 = 0.2940489649772644 + 0.001 * 7.35179328918457
Epoch 290, val loss: 0.7185234427452087
Epoch 300, training loss: 0.26496484875679016 = 0.2576477527618408 + 0.001 * 7.31709623336792
Epoch 300, val loss: 0.7138522863388062
Epoch 310, training loss: 0.23257265985012054 = 0.2252529263496399 + 0.001 * 7.319729804992676
Epoch 310, val loss: 0.712371826171875
Epoch 320, training loss: 0.20392881333827972 = 0.1966322362422943 + 0.001 * 7.296579360961914
Epoch 320, val loss: 0.7135229110717773
Epoch 330, training loss: 0.17888258397579193 = 0.17159615457057953 + 0.001 * 7.2864298820495605
Epoch 330, val loss: 0.7169179916381836
Epoch 340, training loss: 0.15719427168369293 = 0.14991143345832825 + 0.001 * 7.282834529876709
Epoch 340, val loss: 0.7222979664802551
Epoch 350, training loss: 0.13850414752960205 = 0.13122491538524628 + 0.001 * 7.279236793518066
Epoch 350, val loss: 0.729402482509613
Epoch 360, training loss: 0.12242663651704788 = 0.1151403859257698 + 0.001 * 7.286251068115234
Epoch 360, val loss: 0.7379803657531738
Epoch 370, training loss: 0.10855255275964737 = 0.10126965492963791 + 0.001 * 7.28289794921875
Epoch 370, val loss: 0.7477165460586548
Epoch 380, training loss: 0.09655702859163284 = 0.08927536755800247 + 0.001 * 7.28165864944458
Epoch 380, val loss: 0.7583831548690796
Epoch 390, training loss: 0.08615218102931976 = 0.07887299358844757 + 0.001 * 7.279188632965088
Epoch 390, val loss: 0.7696816325187683
Epoch 400, training loss: 0.07710693031549454 = 0.06982937455177307 + 0.001 * 7.277552604675293
Epoch 400, val loss: 0.7814630270004272
Epoch 410, training loss: 0.0692419707775116 = 0.0619574673473835 + 0.001 * 7.2845048904418945
Epoch 410, val loss: 0.7935703992843628
Epoch 420, training loss: 0.06238068267703056 = 0.05509865656495094 + 0.001 * 7.282024383544922
Epoch 420, val loss: 0.8058741092681885
Epoch 430, training loss: 0.05640464276075363 = 0.049124814569950104 + 0.001 * 7.279829025268555
Epoch 430, val loss: 0.818289577960968
Epoch 440, training loss: 0.051200319081544876 = 0.04392414540052414 + 0.001 * 7.276173114776611
Epoch 440, val loss: 0.8307579755783081
Epoch 450, training loss: 0.046676959842443466 = 0.03939880430698395 + 0.001 * 7.27815580368042
Epoch 450, val loss: 0.8432230353355408
Epoch 460, training loss: 0.0427340604364872 = 0.03545830026268959 + 0.001 * 7.275759696960449
Epoch 460, val loss: 0.8556538224220276
Epoch 470, training loss: 0.03929965943098068 = 0.032023075968027115 + 0.001 * 7.276581287384033
Epoch 470, val loss: 0.8679917454719543
Epoch 480, training loss: 0.03630337864160538 = 0.029023338109254837 + 0.001 * 7.280040264129639
Epoch 480, val loss: 0.8801867961883545
Epoch 490, training loss: 0.0336778350174427 = 0.02639756165444851 + 0.001 * 7.280272006988525
Epoch 490, val loss: 0.8921927809715271
Epoch 500, training loss: 0.031364597380161285 = 0.02409283258020878 + 0.001 * 7.27176570892334
Epoch 500, val loss: 0.9040130972862244
Epoch 510, training loss: 0.02933488041162491 = 0.022063959389925003 + 0.001 * 7.27092170715332
Epoch 510, val loss: 0.9155418872833252
Epoch 520, training loss: 0.027546651661396027 = 0.020271891728043556 + 0.001 * 7.2747602462768555
Epoch 520, val loss: 0.9268414378166199
Epoch 530, training loss: 0.025950349867343903 = 0.01868389919400215 + 0.001 * 7.266450881958008
Epoch 530, val loss: 0.9378722310066223
Epoch 540, training loss: 0.024537205696105957 = 0.0172720979899168 + 0.001 * 7.26510763168335
Epoch 540, val loss: 0.9485942721366882
Epoch 550, training loss: 0.023275267332792282 = 0.016012966632843018 + 0.001 * 7.262301445007324
Epoch 550, val loss: 0.9590310454368591
Epoch 560, training loss: 0.02216447703540325 = 0.014886260032653809 + 0.001 * 7.278216361999512
Epoch 560, val loss: 0.9691817760467529
Epoch 570, training loss: 0.021137893199920654 = 0.013874836266040802 + 0.001 * 7.263057231903076
Epoch 570, val loss: 0.979012131690979
Epoch 580, training loss: 0.020230857655405998 = 0.012963361106812954 + 0.001 * 7.267495632171631
Epoch 580, val loss: 0.9886048436164856
Epoch 590, training loss: 0.01939954049885273 = 0.012138775549829006 + 0.001 * 7.2607645988464355
Epoch 590, val loss: 0.9979346394538879
Epoch 600, training loss: 0.01864444464445114 = 0.011390242725610733 + 0.001 * 7.254200458526611
Epoch 600, val loss: 1.0070080757141113
Epoch 610, training loss: 0.017969509586691856 = 0.010707786306738853 + 0.001 * 7.261723518371582
Epoch 610, val loss: 1.0159090757369995
Epoch 620, training loss: 0.017333950847387314 = 0.010083400644361973 + 0.001 * 7.25054931640625
Epoch 620, val loss: 1.0246447324752808
Epoch 630, training loss: 0.016757532954216003 = 0.00951029546558857 + 0.001 * 7.247236251831055
Epoch 630, val loss: 1.0332388877868652
Epoch 640, training loss: 0.016237594187259674 = 0.008982905186712742 + 0.001 * 7.2546892166137695
Epoch 640, val loss: 1.0416501760482788
Epoch 650, training loss: 0.01574038341641426 = 0.008496819995343685 + 0.001 * 7.243562698364258
Epoch 650, val loss: 1.0499454736709595
Epoch 660, training loss: 0.015288043767213821 = 0.008047972805798054 + 0.001 * 7.240070343017578
Epoch 660, val loss: 1.0580575466156006
Epoch 670, training loss: 0.014882560819387436 = 0.0076329405419528484 + 0.001 * 7.24962043762207
Epoch 670, val loss: 1.0660035610198975
Epoch 680, training loss: 0.014487683773040771 = 0.00724880350753665 + 0.001 * 7.238879680633545
Epoch 680, val loss: 1.0737937688827515
Epoch 690, training loss: 0.014131507836282253 = 0.006892758421599865 + 0.001 * 7.238749027252197
Epoch 690, val loss: 1.081403374671936
Epoch 700, training loss: 0.01379699818789959 = 0.006562371272593737 + 0.001 * 7.234626770019531
Epoch 700, val loss: 1.0888819694519043
Epoch 710, training loss: 0.013490205630660057 = 0.006255441810935736 + 0.001 * 7.234764099121094
Epoch 710, val loss: 1.0961432456970215
Epoch 720, training loss: 0.01319081149995327 = 0.0059698717668652534 + 0.001 * 7.220939636230469
Epoch 720, val loss: 1.1032484769821167
Epoch 730, training loss: 0.012926010414958 = 0.0057039218954741955 + 0.001 * 7.222087860107422
Epoch 730, val loss: 1.1102021932601929
Epoch 740, training loss: 0.012712636962532997 = 0.0054558562114834785 + 0.001 * 7.25678014755249
Epoch 740, val loss: 1.1169863939285278
Epoch 750, training loss: 0.012437726370990276 = 0.005224301945418119 + 0.001 * 7.213424205780029
Epoch 750, val loss: 1.123609185218811
Epoch 760, training loss: 0.01222182996571064 = 0.005007852800190449 + 0.001 * 7.213977336883545
Epoch 760, val loss: 1.1300616264343262
Epoch 770, training loss: 0.012033803388476372 = 0.00480534927919507 + 0.001 * 7.228453636169434
Epoch 770, val loss: 1.1363927125930786
Epoch 780, training loss: 0.011812375858426094 = 0.004615640267729759 + 0.001 * 7.196735858917236
Epoch 780, val loss: 1.1425516605377197
Epoch 790, training loss: 0.011640045791864395 = 0.00443771667778492 + 0.001 * 7.202329158782959
Epoch 790, val loss: 1.1485803127288818
Epoch 800, training loss: 0.011491871438920498 = 0.004270675126463175 + 0.001 * 7.221196174621582
Epoch 800, val loss: 1.154497504234314
Epoch 810, training loss: 0.01133454404771328 = 0.004113718401640654 + 0.001 * 7.220824718475342
Epoch 810, val loss: 1.1602174043655396
Epoch 820, training loss: 0.011175207793712616 = 0.003966100513935089 + 0.001 * 7.209107398986816
Epoch 820, val loss: 1.1658623218536377
Epoch 830, training loss: 0.011027824133634567 = 0.0038270382210612297 + 0.001 * 7.200785160064697
Epoch 830, val loss: 1.1713601350784302
Epoch 840, training loss: 0.010877791792154312 = 0.0036958963610231876 + 0.001 * 7.181895732879639
Epoch 840, val loss: 1.176705002784729
Epoch 850, training loss: 0.010762208141386509 = 0.0035720940213650465 + 0.001 * 7.190113544464111
Epoch 850, val loss: 1.181973934173584
Epoch 860, training loss: 0.010648339055478573 = 0.0034551348071545362 + 0.001 * 7.193203449249268
Epoch 860, val loss: 1.1870695352554321
Epoch 870, training loss: 0.010552509687840939 = 0.003344592871144414 + 0.001 * 7.207916736602783
Epoch 870, val loss: 1.1921128034591675
Epoch 880, training loss: 0.010429917834699154 = 0.0032399625051766634 + 0.001 * 7.18995475769043
Epoch 880, val loss: 1.1970094442367554
Epoch 890, training loss: 0.010303175076842308 = 0.0031408618669956923 + 0.001 * 7.1623125076293945
Epoch 890, val loss: 1.2018314599990845
Epoch 900, training loss: 0.01021665334701538 = 0.003046897239983082 + 0.001 * 7.1697564125061035
Epoch 900, val loss: 1.206491470336914
Epoch 910, training loss: 0.010149913839995861 = 0.0029577447567135096 + 0.001 * 7.192169189453125
Epoch 910, val loss: 1.2111155986785889
Epoch 920, training loss: 0.010082205757498741 = 0.0028730719350278378 + 0.001 * 7.209133625030518
Epoch 920, val loss: 1.2155754566192627
Epoch 930, training loss: 0.009954608976840973 = 0.0027925404720008373 + 0.001 * 7.162068843841553
Epoch 930, val loss: 1.22000253200531
Epoch 940, training loss: 0.009880740195512772 = 0.002715953392907977 + 0.001 * 7.164786338806152
Epoch 940, val loss: 1.2242769002914429
Epoch 950, training loss: 0.009785919450223446 = 0.0026430690195411444 + 0.001 * 7.142849922180176
Epoch 950, val loss: 1.2285363674163818
Epoch 960, training loss: 0.009724106639623642 = 0.0025736154057085514 + 0.001 * 7.150491237640381
Epoch 960, val loss: 1.2326382398605347
Epoch 970, training loss: 0.009646197780966759 = 0.0025073715951293707 + 0.001 * 7.138825416564941
Epoch 970, val loss: 1.2366976737976074
Epoch 980, training loss: 0.009592555463314056 = 0.0024441732093691826 + 0.001 * 7.148381233215332
Epoch 980, val loss: 1.2406796216964722
Epoch 990, training loss: 0.00953444093465805 = 0.0023838342167437077 + 0.001 * 7.150606632232666
Epoch 990, val loss: 1.2445162534713745
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8392198207696363
The final CL Acc:0.79259, 0.02400, The final GNN Acc:0.83904, 0.00108
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10510])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9644604921340942 = 1.95586359500885 + 0.001 * 8.596843719482422
Epoch 0, val loss: 1.952307105064392
Epoch 10, training loss: 1.9537508487701416 = 1.945154070854187 + 0.001 * 8.596795082092285
Epoch 10, val loss: 1.9421474933624268
Epoch 20, training loss: 1.940587043762207 = 1.931990385055542 + 0.001 * 8.596624374389648
Epoch 20, val loss: 1.9292317628860474
Epoch 30, training loss: 1.9222545623779297 = 1.9136583805084229 + 0.001 * 8.596238136291504
Epoch 30, val loss: 1.9109309911727905
Epoch 40, training loss: 1.895363450050354 = 1.8867682218551636 + 0.001 * 8.595258712768555
Epoch 40, val loss: 1.8843029737472534
Epoch 50, training loss: 1.8581316471099854 = 1.8495393991470337 + 0.001 * 8.592267036437988
Epoch 50, val loss: 1.849109172821045
Epoch 60, training loss: 1.818253755569458 = 1.8096729516983032 + 0.001 * 8.580759048461914
Epoch 60, val loss: 1.81598961353302
Epoch 70, training loss: 1.7883992195129395 = 1.7798688411712646 + 0.001 * 8.530333518981934
Epoch 70, val loss: 1.7940630912780762
Epoch 80, training loss: 1.7527426481246948 = 1.7445229291915894 + 0.001 * 8.219681739807129
Epoch 80, val loss: 1.7627371549606323
Epoch 90, training loss: 1.7035515308380127 = 1.6954981088638306 + 0.001 * 8.053478240966797
Epoch 90, val loss: 1.7202354669570923
Epoch 100, training loss: 1.6346832513809204 = 1.6267445087432861 + 0.001 * 7.93872594833374
Epoch 100, val loss: 1.663597583770752
Epoch 110, training loss: 1.5475454330444336 = 1.5397714376449585 + 0.001 * 7.773942947387695
Epoch 110, val loss: 1.5935101509094238
Epoch 120, training loss: 1.4535727500915527 = 1.4459091424942017 + 0.001 * 7.6636528968811035
Epoch 120, val loss: 1.5201729536056519
Epoch 130, training loss: 1.3609236478805542 = 1.3532991409301758 + 0.001 * 7.624452590942383
Epoch 130, val loss: 1.4496409893035889
Epoch 140, training loss: 1.2711799144744873 = 1.2636420726776123 + 0.001 * 7.537786960601807
Epoch 140, val loss: 1.3842090368270874
Epoch 150, training loss: 1.1845316886901855 = 1.177130937576294 + 0.001 * 7.400702953338623
Epoch 150, val loss: 1.3234437704086304
Epoch 160, training loss: 1.1035481691360474 = 1.0962364673614502 + 0.001 * 7.311753273010254
Epoch 160, val loss: 1.2688039541244507
Epoch 170, training loss: 1.0312998294830322 = 1.0240122079849243 + 0.001 * 7.287578105926514
Epoch 170, val loss: 1.222360372543335
Epoch 180, training loss: 0.9679622650146484 = 0.9606838822364807 + 0.001 * 7.278385162353516
Epoch 180, val loss: 1.1834008693695068
Epoch 190, training loss: 0.9106325507164001 = 0.9033586382865906 + 0.001 * 7.273884296417236
Epoch 190, val loss: 1.1492691040039062
Epoch 200, training loss: 0.8552171587944031 = 0.8479472994804382 + 0.001 * 7.269841194152832
Epoch 200, val loss: 1.1165680885314941
Epoch 210, training loss: 0.7981932163238525 = 0.7909282445907593 + 0.001 * 7.26496696472168
Epoch 210, val loss: 1.0826489925384521
Epoch 220, training loss: 0.7379932999610901 = 0.7307335734367371 + 0.001 * 7.259714126586914
Epoch 220, val loss: 1.0465565919876099
Epoch 230, training loss: 0.6753796935081482 = 0.6681238412857056 + 0.001 * 7.2558441162109375
Epoch 230, val loss: 1.0097122192382812
Epoch 240, training loss: 0.6131684184074402 = 0.6059190630912781 + 0.001 * 7.249375343322754
Epoch 240, val loss: 0.97502201795578
Epoch 250, training loss: 0.5546693205833435 = 0.5474268198013306 + 0.001 * 7.2424774169921875
Epoch 250, val loss: 0.9456895589828491
Epoch 260, training loss: 0.5017927885055542 = 0.49455776810646057 + 0.001 * 7.235048294067383
Epoch 260, val loss: 0.9235550165176392
Epoch 270, training loss: 0.45436158776283264 = 0.44713371992111206 + 0.001 * 7.227880001068115
Epoch 270, val loss: 0.9084361791610718
Epoch 280, training loss: 0.41088080406188965 = 0.4036620557308197 + 0.001 * 7.218760013580322
Epoch 280, val loss: 0.8983162045478821
Epoch 290, training loss: 0.36994174122810364 = 0.36271730065345764 + 0.001 * 7.2244462966918945
Epoch 290, val loss: 0.8912652730941772
Epoch 300, training loss: 0.3309391736984253 = 0.3237345218658447 + 0.001 * 7.2046589851379395
Epoch 300, val loss: 0.8862826228141785
Epoch 310, training loss: 0.29417484998703003 = 0.28697919845581055 + 0.001 * 7.195640563964844
Epoch 310, val loss: 0.8832083344459534
Epoch 320, training loss: 0.2603192627429962 = 0.25312986969947815 + 0.001 * 7.189404487609863
Epoch 320, val loss: 0.8822410702705383
Epoch 330, training loss: 0.22993725538253784 = 0.22274602949619293 + 0.001 * 7.191228866577148
Epoch 330, val loss: 0.8840533494949341
Epoch 340, training loss: 0.20322750508785248 = 0.1960471272468567 + 0.001 * 7.1803741455078125
Epoch 340, val loss: 0.889199435710907
Epoch 350, training loss: 0.1801350712776184 = 0.17296338081359863 + 0.001 * 7.171693325042725
Epoch 350, val loss: 0.8978002667427063
Epoch 360, training loss: 0.16031543910503387 = 0.15314477682113647 + 0.001 * 7.170657634735107
Epoch 360, val loss: 0.9094604849815369
Epoch 370, training loss: 0.1432667374610901 = 0.13610504567623138 + 0.001 * 7.1616950035095215
Epoch 370, val loss: 0.9235994219779968
Epoch 380, training loss: 0.12853650748729706 = 0.12138191610574722 + 0.001 * 7.154594421386719
Epoch 380, val loss: 0.9392222166061401
Epoch 390, training loss: 0.11572466045618057 = 0.10857747495174408 + 0.001 * 7.147184371948242
Epoch 390, val loss: 0.9557399153709412
Epoch 400, training loss: 0.104521244764328 = 0.09737981110811234 + 0.001 * 7.141434192657471
Epoch 400, val loss: 0.9728618264198303
Epoch 410, training loss: 0.09468819200992584 = 0.0875435546040535 + 0.001 * 7.144636631011963
Epoch 410, val loss: 0.990257203578949
Epoch 420, training loss: 0.08601115643978119 = 0.07887347787618637 + 0.001 * 7.137674331665039
Epoch 420, val loss: 1.0080074071884155
Epoch 430, training loss: 0.07832477986812592 = 0.07120388746261597 + 0.001 * 7.120894432067871
Epoch 430, val loss: 1.02587890625
Epoch 440, training loss: 0.07152137905359268 = 0.06440088152885437 + 0.001 * 7.1204962730407715
Epoch 440, val loss: 1.0438940525054932
Epoch 450, training loss: 0.06546219438314438 = 0.05835607647895813 + 0.001 * 7.106114864349365
Epoch 450, val loss: 1.0619696378707886
Epoch 460, training loss: 0.06007537990808487 = 0.052975527942180634 + 0.001 * 7.099853038787842
Epoch 460, val loss: 1.0800739526748657
Epoch 470, training loss: 0.05526562035083771 = 0.04817887395620346 + 0.001 * 7.086746692657471
Epoch 470, val loss: 1.0981773138046265
Epoch 480, training loss: 0.05098611116409302 = 0.04389570653438568 + 0.001 * 7.090404510498047
Epoch 480, val loss: 1.1161785125732422
Epoch 490, training loss: 0.04714106023311615 = 0.04006500542163849 + 0.001 * 7.076053619384766
Epoch 490, val loss: 1.1339994668960571
Epoch 500, training loss: 0.043710894882678986 = 0.03663258254528046 + 0.001 * 7.078310966491699
Epoch 500, val loss: 1.1515775918960571
Epoch 510, training loss: 0.040634866803884506 = 0.033555060625076294 + 0.001 * 7.079804420471191
Epoch 510, val loss: 1.1689093112945557
Epoch 520, training loss: 0.03786395117640495 = 0.03079339675605297 + 0.001 * 7.070555210113525
Epoch 520, val loss: 1.1859501600265503
Epoch 530, training loss: 0.03537888079881668 = 0.028313037008047104 + 0.001 * 7.065842628479004
Epoch 530, val loss: 1.2026660442352295
Epoch 540, training loss: 0.033146608620882034 = 0.02608368918299675 + 0.001 * 7.062918186187744
Epoch 540, val loss: 1.2190518379211426
Epoch 550, training loss: 0.03114193305373192 = 0.024079004302620888 + 0.001 * 7.062928199768066
Epoch 550, val loss: 1.2350705862045288
Epoch 560, training loss: 0.029343506321310997 = 0.022275729104876518 + 0.001 * 7.067777156829834
Epoch 560, val loss: 1.2507551908493042
Epoch 570, training loss: 0.02772335894405842 = 0.02065252512693405 + 0.001 * 7.070833683013916
Epoch 570, val loss: 1.2660603523254395
Epoch 580, training loss: 0.026253415271639824 = 0.019190283492207527 + 0.001 * 7.063131809234619
Epoch 580, val loss: 1.280996322631836
Epoch 590, training loss: 0.024929212406277657 = 0.017871126532554626 + 0.001 * 7.058084964752197
Epoch 590, val loss: 1.295580506324768
Epoch 600, training loss: 0.02373471111059189 = 0.016679663211107254 + 0.001 * 7.055046558380127
Epoch 600, val loss: 1.3097413778305054
Epoch 610, training loss: 0.022666804492473602 = 0.015601702965795994 + 0.001 * 7.0651021003723145
Epoch 610, val loss: 1.3235418796539307
Epoch 620, training loss: 0.021680649369955063 = 0.014624657109379768 + 0.001 * 7.055991172790527
Epoch 620, val loss: 1.3369275331497192
Epoch 630, training loss: 0.020790042355656624 = 0.013737201690673828 + 0.001 * 7.052839756011963
Epoch 630, val loss: 1.349931240081787
Epoch 640, training loss: 0.01998113840818405 = 0.012929324992001057 + 0.001 * 7.051814079284668
Epoch 640, val loss: 1.3625919818878174
Epoch 650, training loss: 0.01924804225564003 = 0.01219235174357891 + 0.001 * 7.055691242218018
Epoch 650, val loss: 1.3748668432235718
Epoch 660, training loss: 0.018569039180874825 = 0.011518610641360283 + 0.001 * 7.05042839050293
Epoch 660, val loss: 1.3867442607879639
Epoch 670, training loss: 0.01794833317399025 = 0.010901347734034061 + 0.001 * 7.046984672546387
Epoch 670, val loss: 1.3982794284820557
Epoch 680, training loss: 0.017386434599757195 = 0.010334468446671963 + 0.001 * 7.051965236663818
Epoch 680, val loss: 1.4095032215118408
Epoch 690, training loss: 0.01686311513185501 = 0.00981281790882349 + 0.001 * 7.050297737121582
Epoch 690, val loss: 1.4203985929489136
Epoch 700, training loss: 0.01637684553861618 = 0.009331805631518364 + 0.001 * 7.045040130615234
Epoch 700, val loss: 1.4309828281402588
Epoch 710, training loss: 0.015934428200125694 = 0.008887416683137417 + 0.001 * 7.047011852264404
Epoch 710, val loss: 1.4412765502929688
Epoch 720, training loss: 0.01552190724760294 = 0.00847605150192976 + 0.001 * 7.045855522155762
Epoch 720, val loss: 1.4513036012649536
Epoch 730, training loss: 0.015136072412133217 = 0.008094580844044685 + 0.001 * 7.04149055480957
Epoch 730, val loss: 1.4610316753387451
Epoch 740, training loss: 0.014796516858041286 = 0.0077401865273714066 + 0.001 * 7.05633020401001
Epoch 740, val loss: 1.4705026149749756
Epoch 750, training loss: 0.014452577568590641 = 0.007410348393023014 + 0.001 * 7.042228698730469
Epoch 750, val loss: 1.4797239303588867
Epoch 760, training loss: 0.014139305800199509 = 0.007102909963577986 + 0.001 * 7.036395072937012
Epoch 760, val loss: 1.4886844158172607
Epoch 770, training loss: 0.01385195180773735 = 0.006815905217081308 + 0.001 * 7.036046028137207
Epoch 770, val loss: 1.4973996877670288
Epoch 780, training loss: 0.01358345802873373 = 0.00654754089191556 + 0.001 * 7.035916805267334
Epoch 780, val loss: 1.5059146881103516
Epoch 790, training loss: 0.013336019590497017 = 0.006296220235526562 + 0.001 * 7.039799690246582
Epoch 790, val loss: 1.5141830444335938
Epoch 800, training loss: 0.013103367760777473 = 0.006060530431568623 + 0.001 * 7.042837142944336
Epoch 800, val loss: 1.5222539901733398
Epoch 810, training loss: 0.012877307832241058 = 0.005839199293404818 + 0.001 * 7.038108825683594
Epoch 810, val loss: 1.530116319656372
Epoch 820, training loss: 0.012660056352615356 = 0.005631089210510254 + 0.001 * 7.028966903686523
Epoch 820, val loss: 1.5378036499023438
Epoch 830, training loss: 0.012476366013288498 = 0.0054351696744561195 + 0.001 * 7.041195869445801
Epoch 830, val loss: 1.545304298400879
Epoch 840, training loss: 0.012277700006961823 = 0.005250504240393639 + 0.001 * 7.027194976806641
Epoch 840, val loss: 1.5525927543640137
Epoch 850, training loss: 0.012106352485716343 = 0.005076236091554165 + 0.001 * 7.030116081237793
Epoch 850, val loss: 1.5597285032272339
Epoch 860, training loss: 0.011947371065616608 = 0.004911602009087801 + 0.001 * 7.035768508911133
Epoch 860, val loss: 1.5667086839675903
Epoch 870, training loss: 0.011778531596064568 = 0.004755892790853977 + 0.001 * 7.02263879776001
Epoch 870, val loss: 1.5734977722167969
Epoch 880, training loss: 0.011629320681095123 = 0.004608483053743839 + 0.001 * 7.020837306976318
Epoch 880, val loss: 1.580151081085205
Epoch 890, training loss: 0.011518911458551884 = 0.004468793515115976 + 0.001 * 7.050117492675781
Epoch 890, val loss: 1.586662769317627
Epoch 900, training loss: 0.011364859528839588 = 0.004336296580731869 + 0.001 * 7.028562545776367
Epoch 900, val loss: 1.5929951667785645
Epoch 910, training loss: 0.011228739283978939 = 0.004210507497191429 + 0.001 * 7.018231391906738
Epoch 910, val loss: 1.5992062091827393
Epoch 920, training loss: 0.011111396364867687 = 0.004090947564691305 + 0.001 * 7.020448684692383
Epoch 920, val loss: 1.6052757501602173
Epoch 930, training loss: 0.010996481403708458 = 0.003977236803621054 + 0.001 * 7.01924467086792
Epoch 930, val loss: 1.611224889755249
Epoch 940, training loss: 0.010904177092015743 = 0.003868984756991267 + 0.001 * 7.035192012786865
Epoch 940, val loss: 1.6170319318771362
Epoch 950, training loss: 0.010789117775857449 = 0.003765844041481614 + 0.001 * 7.02327299118042
Epoch 950, val loss: 1.6227141618728638
Epoch 960, training loss: 0.010684091597795486 = 0.0036675126757472754 + 0.001 * 7.016578197479248
Epoch 960, val loss: 1.62827467918396
Epoch 970, training loss: 0.010599223896861076 = 0.003573697991669178 + 0.001 * 7.02552604675293
Epoch 970, val loss: 1.63372004032135
Epoch 980, training loss: 0.010501288808882236 = 0.0034841198939830065 + 0.001 * 7.0171685218811035
Epoch 980, val loss: 1.6390740871429443
Epoch 990, training loss: 0.010403471998870373 = 0.0033985271584242582 + 0.001 * 7.004944801330566
Epoch 990, val loss: 1.6442718505859375
Epoch 1000, training loss: 0.010324116796255112 = 0.003316675079986453 + 0.001 * 7.007441520690918
Epoch 1000, val loss: 1.6494029760360718
Epoch 1010, training loss: 0.010245811194181442 = 0.003238360397517681 + 0.001 * 7.007450103759766
Epoch 1010, val loss: 1.654403805732727
Epoch 1020, training loss: 0.010164403356611729 = 0.003163338638842106 + 0.001 * 7.001064300537109
Epoch 1020, val loss: 1.6593226194381714
Epoch 1030, training loss: 0.010118203237652779 = 0.0030913727823644876 + 0.001 * 7.026830196380615
Epoch 1030, val loss: 1.664157748222351
Epoch 1040, training loss: 0.010032501071691513 = 0.003022091696038842 + 0.001 * 7.010408878326416
Epoch 1040, val loss: 1.6688538789749146
Epoch 1050, training loss: 0.0099667739123106 = 0.0029548669699579477 + 0.001 * 7.011906623840332
Epoch 1050, val loss: 1.6735496520996094
Epoch 1060, training loss: 0.009893659502267838 = 0.002889077877625823 + 0.001 * 7.004580974578857
Epoch 1060, val loss: 1.6782065629959106
Epoch 1070, training loss: 0.009823266416788101 = 0.0028242592234164476 + 0.001 * 6.999007225036621
Epoch 1070, val loss: 1.682915210723877
Epoch 1080, training loss: 0.009760141372680664 = 0.0027603437192738056 + 0.001 * 6.999797821044922
Epoch 1080, val loss: 1.6877421140670776
Epoch 1090, training loss: 0.009711386635899544 = 0.002697435673326254 + 0.001 * 7.013950347900391
Epoch 1090, val loss: 1.6927108764648438
Epoch 1100, training loss: 0.009632760658860207 = 0.002635715063661337 + 0.001 * 6.997045516967773
Epoch 1100, val loss: 1.697799801826477
Epoch 1110, training loss: 0.009571057744324207 = 0.0025754536036401987 + 0.001 * 6.995604038238525
Epoch 1110, val loss: 1.7030302286148071
Epoch 1120, training loss: 0.009505817666649818 = 0.0025168550200760365 + 0.001 * 6.988962173461914
Epoch 1120, val loss: 1.7083425521850586
Epoch 1130, training loss: 0.009480391629040241 = 0.0024600098840892315 + 0.001 * 7.020381450653076
Epoch 1130, val loss: 1.7136814594268799
Epoch 1140, training loss: 0.00938732735812664 = 0.0024050131905823946 + 0.001 * 6.982314109802246
Epoch 1140, val loss: 1.719090223312378
Epoch 1150, training loss: 0.009347003884613514 = 0.0023518146481364965 + 0.001 * 6.995189189910889
Epoch 1150, val loss: 1.7244902849197388
Epoch 1160, training loss: 0.009304855018854141 = 0.002300445456057787 + 0.001 * 7.004409313201904
Epoch 1160, val loss: 1.7299667596817017
Epoch 1170, training loss: 0.009249843657016754 = 0.0022508534602820873 + 0.001 * 6.998990058898926
Epoch 1170, val loss: 1.7353471517562866
Epoch 1180, training loss: 0.009201633743941784 = 0.002203005598857999 + 0.001 * 6.99862813949585
Epoch 1180, val loss: 1.7407779693603516
Epoch 1190, training loss: 0.0091438302770257 = 0.002156828297302127 + 0.001 * 6.987001419067383
Epoch 1190, val loss: 1.7461944818496704
Epoch 1200, training loss: 0.009095615707337856 = 0.0021122992038726807 + 0.001 * 6.983315944671631
Epoch 1200, val loss: 1.7515896558761597
Epoch 1210, training loss: 0.009056653827428818 = 0.0020693596452474594 + 0.001 * 6.987293243408203
Epoch 1210, val loss: 1.7569384574890137
Epoch 1220, training loss: 0.009004796855151653 = 0.002027965849265456 + 0.001 * 6.97683048248291
Epoch 1220, val loss: 1.762290120124817
Epoch 1230, training loss: 0.008961393497884274 = 0.001988019095733762 + 0.001 * 6.973374366760254
Epoch 1230, val loss: 1.767585277557373
Epoch 1240, training loss: 0.008935537189245224 = 0.0019495075102895498 + 0.001 * 6.986029624938965
Epoch 1240, val loss: 1.7728254795074463
Epoch 1250, training loss: 0.008918294683098793 = 0.0019124220125377178 + 0.001 * 7.0058722496032715
Epoch 1250, val loss: 1.7780088186264038
Epoch 1260, training loss: 0.008852159604430199 = 0.0018766741268336773 + 0.001 * 6.975484848022461
Epoch 1260, val loss: 1.783139705657959
Epoch 1270, training loss: 0.008822827599942684 = 0.0018421827116981149 + 0.001 * 6.980644702911377
Epoch 1270, val loss: 1.7881642580032349
Epoch 1280, training loss: 0.008798380382359028 = 0.0018089225050061941 + 0.001 * 6.989457607269287
Epoch 1280, val loss: 1.7932385206222534
Epoch 1290, training loss: 0.008808611892163754 = 0.0017768122488632798 + 0.001 * 7.031798839569092
Epoch 1290, val loss: 1.7981773614883423
Epoch 1300, training loss: 0.008715552277863026 = 0.0017458185320720077 + 0.001 * 6.969733715057373
Epoch 1300, val loss: 1.803044319152832
Epoch 1310, training loss: 0.008689720183610916 = 0.0017158970003947616 + 0.001 * 6.973823070526123
Epoch 1310, val loss: 1.8078925609588623
Epoch 1320, training loss: 0.008644608780741692 = 0.0016869865357875824 + 0.001 * 6.9576215744018555
Epoch 1320, val loss: 1.812633752822876
Epoch 1330, training loss: 0.008630982600152493 = 0.0016590519808232784 + 0.001 * 6.971930503845215
Epoch 1330, val loss: 1.8173854351043701
Epoch 1340, training loss: 0.008619114756584167 = 0.0016320431604981422 + 0.001 * 6.987071514129639
Epoch 1340, val loss: 1.8220162391662598
Epoch 1350, training loss: 0.008564267307519913 = 0.0016059466870501637 + 0.001 * 6.958320617675781
Epoch 1350, val loss: 1.82658052444458
Epoch 1360, training loss: 0.008561721071600914 = 0.0015807037707418203 + 0.001 * 6.981016635894775
Epoch 1360, val loss: 1.8310571908950806
Epoch 1370, training loss: 0.008516471832990646 = 0.0015562984626740217 + 0.001 * 6.960172653198242
Epoch 1370, val loss: 1.835531234741211
Epoch 1380, training loss: 0.008484529331326485 = 0.001532735419459641 + 0.001 * 6.951793670654297
Epoch 1380, val loss: 1.8398891687393188
Epoch 1390, training loss: 0.008476688526570797 = 0.0015099492156878114 + 0.001 * 6.966739177703857
Epoch 1390, val loss: 1.8441715240478516
Epoch 1400, training loss: 0.008449861779808998 = 0.001487897476181388 + 0.001 * 6.961963653564453
Epoch 1400, val loss: 1.8483965396881104
Epoch 1410, training loss: 0.008462290279567242 = 0.0014665721682831645 + 0.001 * 6.995718002319336
Epoch 1410, val loss: 1.8525454998016357
Epoch 1420, training loss: 0.008388567715883255 = 0.001445931033231318 + 0.001 * 6.942636489868164
Epoch 1420, val loss: 1.8566471338272095
Epoch 1430, training loss: 0.008390320464968681 = 0.001425937982276082 + 0.001 * 6.964382171630859
Epoch 1430, val loss: 1.8606809377670288
Epoch 1440, training loss: 0.008363758213818073 = 0.0014065724099054933 + 0.001 * 6.9571852684021
Epoch 1440, val loss: 1.864668607711792
Epoch 1450, training loss: 0.008341548964381218 = 0.001387814641930163 + 0.001 * 6.953733444213867
Epoch 1450, val loss: 1.8685879707336426
Epoch 1460, training loss: 0.008330135606229305 = 0.0013696422101929784 + 0.001 * 6.960493087768555
Epoch 1460, val loss: 1.872376561164856
Epoch 1470, training loss: 0.008302314206957817 = 0.0013520396314561367 + 0.001 * 6.950274467468262
Epoch 1470, val loss: 1.8761675357818604
Epoch 1480, training loss: 0.008275649510324001 = 0.0013349719811230898 + 0.001 * 6.940677642822266
Epoch 1480, val loss: 1.8798633813858032
Epoch 1490, training loss: 0.008267595432698727 = 0.0013184153940528631 + 0.001 * 6.949179649353027
Epoch 1490, val loss: 1.883520245552063
Epoch 1500, training loss: 0.00825167540460825 = 0.0013023632345721126 + 0.001 * 6.94931173324585
Epoch 1500, val loss: 1.8871418237686157
Epoch 1510, training loss: 0.00823122262954712 = 0.001286769867874682 + 0.001 * 6.94445276260376
Epoch 1510, val loss: 1.8907051086425781
Epoch 1520, training loss: 0.008224338293075562 = 0.001271658344194293 + 0.001 * 6.95267915725708
Epoch 1520, val loss: 1.8941489458084106
Epoch 1530, training loss: 0.008193258196115494 = 0.0012569930404424667 + 0.001 * 6.936264991760254
Epoch 1530, val loss: 1.897613763809204
Epoch 1540, training loss: 0.00818682461977005 = 0.001242753816768527 + 0.001 * 6.944070339202881
Epoch 1540, val loss: 1.9009130001068115
Epoch 1550, training loss: 0.008232258260250092 = 0.0012289336882531643 + 0.001 * 7.003323554992676
Epoch 1550, val loss: 1.9042959213256836
Epoch 1560, training loss: 0.008138180710375309 = 0.0012155150761827826 + 0.001 * 6.922665119171143
Epoch 1560, val loss: 1.9075312614440918
Epoch 1570, training loss: 0.00814246665686369 = 0.0012025042669847608 + 0.001 * 6.939962387084961
Epoch 1570, val loss: 1.9107073545455933
Epoch 1580, training loss: 0.00814935564994812 = 0.0011898704105988145 + 0.001 * 6.959484577178955
Epoch 1580, val loss: 1.9139091968536377
Epoch 1590, training loss: 0.008108442649245262 = 0.0011776243336498737 + 0.001 * 6.9308180809021
Epoch 1590, val loss: 1.9169803857803345
Epoch 1600, training loss: 0.008113549090921879 = 0.0011657305294647813 + 0.001 * 6.947817802429199
Epoch 1600, val loss: 1.9199962615966797
Epoch 1610, training loss: 0.00810172501951456 = 0.0011541524436324835 + 0.001 * 6.947572708129883
Epoch 1610, val loss: 1.9229704141616821
Epoch 1620, training loss: 0.008079756051301956 = 0.0011428818106651306 + 0.001 * 6.936873912811279
Epoch 1620, val loss: 1.9258289337158203
Epoch 1630, training loss: 0.008071478456258774 = 0.0011319193290546536 + 0.001 * 6.939558982849121
Epoch 1630, val loss: 1.928782343864441
Epoch 1640, training loss: 0.008040842600166798 = 0.0011212483514100313 + 0.001 * 6.9195942878723145
Epoch 1640, val loss: 1.9316519498825073
Epoch 1650, training loss: 0.00809019897133112 = 0.0011108636390417814 + 0.001 * 6.979334831237793
Epoch 1650, val loss: 1.9344853162765503
Epoch 1660, training loss: 0.008018339984118938 = 0.0011007292196154594 + 0.001 * 6.9176106452941895
Epoch 1660, val loss: 1.9371980428695679
Epoch 1670, training loss: 0.00802948884665966 = 0.0010908605763688684 + 0.001 * 6.93862771987915
Epoch 1670, val loss: 1.939916729927063
Epoch 1680, training loss: 0.007983886636793613 = 0.001081252470612526 + 0.001 * 6.9026336669921875
Epoch 1680, val loss: 1.9425822496414185
Epoch 1690, training loss: 0.007984848693013191 = 0.0010719073470681906 + 0.001 * 6.9129414558410645
Epoch 1690, val loss: 1.9451653957366943
Epoch 1700, training loss: 0.008016645908355713 = 0.0010628035524860024 + 0.001 * 6.953841686248779
Epoch 1700, val loss: 1.9477039575576782
Epoch 1710, training loss: 0.007958954200148582 = 0.0010539471404626966 + 0.001 * 6.905006408691406
Epoch 1710, val loss: 1.9502230882644653
Epoch 1720, training loss: 0.007961845025420189 = 0.0010453414870426059 + 0.001 * 6.916502952575684
Epoch 1720, val loss: 1.9526488780975342
Epoch 1730, training loss: 0.00794897135347128 = 0.001036952598951757 + 0.001 * 6.912018299102783
Epoch 1730, val loss: 1.9550586938858032
Epoch 1740, training loss: 0.007936148904263973 = 0.001028780941851437 + 0.001 * 6.90736722946167
Epoch 1740, val loss: 1.9574958086013794
Epoch 1750, training loss: 0.00794441532343626 = 0.0010208158055320382 + 0.001 * 6.923598766326904
Epoch 1750, val loss: 1.9598138332366943
Epoch 1760, training loss: 0.007925797253847122 = 0.001013058703392744 + 0.001 * 6.91273832321167
Epoch 1760, val loss: 1.96212637424469
Epoch 1770, training loss: 0.00791134126484394 = 0.0010054829763248563 + 0.001 * 6.905857563018799
Epoch 1770, val loss: 1.9644129276275635
Epoch 1780, training loss: 0.007919377647340298 = 0.000998114817775786 + 0.001 * 6.921262741088867
Epoch 1780, val loss: 1.9667057991027832
Epoch 1790, training loss: 0.007937410846352577 = 0.0009909329237416387 + 0.001 * 6.946477890014648
Epoch 1790, val loss: 1.9687689542770386
Epoch 1800, training loss: 0.007889625616371632 = 0.000983935547992587 + 0.001 * 6.9056901931762695
Epoch 1800, val loss: 1.970987319946289
Epoch 1810, training loss: 0.007876502349972725 = 0.0009771008044481277 + 0.001 * 6.899401664733887
Epoch 1810, val loss: 1.973140001296997
Epoch 1820, training loss: 0.007862068712711334 = 0.000970436551142484 + 0.001 * 6.891631603240967
Epoch 1820, val loss: 1.9751824140548706
Epoch 1830, training loss: 0.007875082083046436 = 0.0009639281779527664 + 0.001 * 6.911153793334961
Epoch 1830, val loss: 1.9772931337356567
Epoch 1840, training loss: 0.007866312749683857 = 0.0009575590956956148 + 0.001 * 6.908753395080566
Epoch 1840, val loss: 1.9793025255203247
Epoch 1850, training loss: 0.007839023135602474 = 0.0009513433324173093 + 0.001 * 6.887679100036621
Epoch 1850, val loss: 1.981283187866211
Epoch 1860, training loss: 0.007834220305085182 = 0.0009452695958316326 + 0.001 * 6.888950824737549
Epoch 1860, val loss: 1.9832690954208374
Epoch 1870, training loss: 0.007819111458957195 = 0.0009393179789185524 + 0.001 * 6.879793167114258
Epoch 1870, val loss: 1.9852057695388794
Epoch 1880, training loss: 0.0078043341636657715 = 0.0009335107752121985 + 0.001 * 6.870822906494141
Epoch 1880, val loss: 1.9871233701705933
Epoch 1890, training loss: 0.007836618460714817 = 0.0009278292418457568 + 0.001 * 6.908788681030273
Epoch 1890, val loss: 1.9889730215072632
Epoch 1900, training loss: 0.007800152059644461 = 0.0009222640073858202 + 0.001 * 6.877887725830078
Epoch 1900, val loss: 1.9907892942428589
Epoch 1910, training loss: 0.0077947513200342655 = 0.0009168320102617145 + 0.001 * 6.877918720245361
Epoch 1910, val loss: 1.9925529956817627
Epoch 1920, training loss: 0.007856305688619614 = 0.0009115274297073483 + 0.001 * 6.944777965545654
Epoch 1920, val loss: 1.9943406581878662
Epoch 1930, training loss: 0.00780407153069973 = 0.0009063186589628458 + 0.001 * 6.89775276184082
Epoch 1930, val loss: 1.9960781335830688
Epoch 1940, training loss: 0.00782859604805708 = 0.0009012329392135143 + 0.001 * 6.92736291885376
Epoch 1940, val loss: 1.9977455139160156
Epoch 1950, training loss: 0.007778545375913382 = 0.0008962455322034657 + 0.001 * 6.882299423217773
Epoch 1950, val loss: 1.9994562864303589
Epoch 1960, training loss: 0.007776970975100994 = 0.0008913679630495608 + 0.001 * 6.885602951049805
Epoch 1960, val loss: 2.0010414123535156
Epoch 1970, training loss: 0.00778944231569767 = 0.0008865828858688474 + 0.001 * 6.902859210968018
Epoch 1970, val loss: 2.0027241706848145
Epoch 1980, training loss: 0.007763439789414406 = 0.0008819156792014837 + 0.001 * 6.881524085998535
Epoch 1980, val loss: 2.004230260848999
Epoch 1990, training loss: 0.007749832235276699 = 0.0008773335139267147 + 0.001 * 6.872498512268066
Epoch 1990, val loss: 2.0058281421661377
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 1.950141429901123 = 1.941544532775879 + 0.001 * 8.5968599319458
Epoch 0, val loss: 1.9376808404922485
Epoch 10, training loss: 1.940845012664795 = 1.9322482347488403 + 0.001 * 8.596820831298828
Epoch 10, val loss: 1.9289767742156982
Epoch 20, training loss: 1.929319143295288 = 1.920722484588623 + 0.001 * 8.596689224243164
Epoch 20, val loss: 1.917810082435608
Epoch 30, training loss: 1.9130668640136719 = 1.9044705629348755 + 0.001 * 8.596354484558105
Epoch 30, val loss: 1.9018489122390747
Epoch 40, training loss: 1.8889498710632324 = 1.880354404449463 + 0.001 * 8.595443725585938
Epoch 40, val loss: 1.878359317779541
Epoch 50, training loss: 1.8550385236740112 = 1.846445918083191 + 0.001 * 8.592630386352539
Epoch 50, val loss: 1.846813440322876
Epoch 60, training loss: 1.8174232244491577 = 1.8088409900665283 + 0.001 * 8.582208633422852
Epoch 60, val loss: 1.8158129453659058
Epoch 70, training loss: 1.7875595092773438 = 1.7790182828903198 + 0.001 * 8.541193008422852
Epoch 70, val loss: 1.7932968139648438
Epoch 80, training loss: 1.7518020868301392 = 1.7434790134429932 + 0.001 * 8.32303524017334
Epoch 80, val loss: 1.7611266374588013
Epoch 90, training loss: 1.7016088962554932 = 1.693598747253418 + 0.001 * 8.01015853881836
Epoch 90, val loss: 1.717092514038086
Epoch 100, training loss: 1.6317063570022583 = 1.6237529516220093 + 0.001 * 7.953382968902588
Epoch 100, val loss: 1.6586955785751343
Epoch 110, training loss: 1.5439586639404297 = 1.536048412322998 + 0.001 * 7.910261154174805
Epoch 110, val loss: 1.588280439376831
Epoch 120, training loss: 1.4483835697174072 = 1.4405405521392822 + 0.001 * 7.84300422668457
Epoch 120, val loss: 1.512913465499878
Epoch 130, training loss: 1.3515126705169678 = 1.3438411951065063 + 0.001 * 7.671422004699707
Epoch 130, val loss: 1.4377018213272095
Epoch 140, training loss: 1.2528117895126343 = 1.2453129291534424 + 0.001 * 7.49887752532959
Epoch 140, val loss: 1.3622255325317383
Epoch 150, training loss: 1.151323676109314 = 1.1438812017440796 + 0.001 * 7.442488193511963
Epoch 150, val loss: 1.285914421081543
Epoch 160, training loss: 1.049621820449829 = 1.042235255241394 + 0.001 * 7.3865556716918945
Epoch 160, val loss: 1.2109507322311401
Epoch 170, training loss: 0.9523307681083679 = 0.9449646472930908 + 0.001 * 7.366125106811523
Epoch 170, val loss: 1.1405174732208252
Epoch 180, training loss: 0.8633102178573608 = 0.8559673428535461 + 0.001 * 7.342864036560059
Epoch 180, val loss: 1.0770224332809448
Epoch 190, training loss: 0.7847931385040283 = 0.7774718999862671 + 0.001 * 7.321255683898926
Epoch 190, val loss: 1.0221941471099854
Epoch 200, training loss: 0.7169508934020996 = 0.7096452713012695 + 0.001 * 7.305614948272705
Epoch 200, val loss: 0.9768275618553162
Epoch 210, training loss: 0.6575307250022888 = 0.6502336859703064 + 0.001 * 7.297042369842529
Epoch 210, val loss: 0.9396870732307434
Epoch 220, training loss: 0.6031138896942139 = 0.5958219170570374 + 0.001 * 7.292001247406006
Epoch 220, val loss: 0.9084834456443787
Epoch 230, training loss: 0.5508548021316528 = 0.5435686707496643 + 0.001 * 7.286106586456299
Epoch 230, val loss: 0.8809963464736938
Epoch 240, training loss: 0.4994743764400482 = 0.49219533801078796 + 0.001 * 7.279026031494141
Epoch 240, val loss: 0.8564662337303162
Epoch 250, training loss: 0.4492616057395935 = 0.44199028611183167 + 0.001 * 7.271316051483154
Epoch 250, val loss: 0.8351901769638062
Epoch 260, training loss: 0.401399165391922 = 0.3941362798213959 + 0.001 * 7.262895107269287
Epoch 260, val loss: 0.8181432485580444
Epoch 270, training loss: 0.3571285307407379 = 0.3498755395412445 + 0.001 * 7.252990245819092
Epoch 270, val loss: 0.8059948682785034
Epoch 280, training loss: 0.31718650460243225 = 0.3099474608898163 + 0.001 * 7.239037036895752
Epoch 280, val loss: 0.7987235188484192
Epoch 290, training loss: 0.2816753089427948 = 0.2744540870189667 + 0.001 * 7.2212114334106445
Epoch 290, val loss: 0.7958040237426758
Epoch 300, training loss: 0.25012925267219543 = 0.242931067943573 + 0.001 * 7.198187351226807
Epoch 300, val loss: 0.7966204881668091
Epoch 310, training loss: 0.22191666066646576 = 0.21473483741283417 + 0.001 * 7.181826591491699
Epoch 310, val loss: 0.8003658652305603
Epoch 320, training loss: 0.19655150175094604 = 0.18939708173274994 + 0.001 * 7.154422283172607
Epoch 320, val loss: 0.8064629435539246
Epoch 330, training loss: 0.17383214831352234 = 0.16669538617134094 + 0.001 * 7.136760234832764
Epoch 330, val loss: 0.8144846558570862
Epoch 340, training loss: 0.15369802713394165 = 0.146566241979599 + 0.001 * 7.131789684295654
Epoch 340, val loss: 0.8240652680397034
Epoch 350, training loss: 0.13604149222373962 = 0.12891225516796112 + 0.001 * 7.129239082336426
Epoch 350, val loss: 0.8348994851112366
Epoch 360, training loss: 0.12069143354892731 = 0.11356571316719055 + 0.001 * 7.125716686248779
Epoch 360, val loss: 0.8467753529548645
Epoch 370, training loss: 0.10740743577480316 = 0.10028151422739029 + 0.001 * 7.125921726226807
Epoch 370, val loss: 0.8595348000526428
Epoch 380, training loss: 0.09592165797948837 = 0.0887962207198143 + 0.001 * 7.125433921813965
Epoch 380, val loss: 0.8729326128959656
Epoch 390, training loss: 0.08598828315734863 = 0.07886005192995071 + 0.001 * 7.128231048583984
Epoch 390, val loss: 0.8869800567626953
Epoch 400, training loss: 0.07738269865512848 = 0.07025627791881561 + 0.001 * 7.126424312591553
Epoch 400, val loss: 0.9014662504196167
Epoch 410, training loss: 0.06992346793413162 = 0.06279826909303665 + 0.001 * 7.125198841094971
Epoch 410, val loss: 0.9162511229515076
Epoch 420, training loss: 0.06344985216856003 = 0.056324541568756104 + 0.001 * 7.125311374664307
Epoch 420, val loss: 0.9311310648918152
Epoch 430, training loss: 0.05782579258084297 = 0.050700362771749496 + 0.001 * 7.125431060791016
Epoch 430, val loss: 0.946074903011322
Epoch 440, training loss: 0.05293416231870651 = 0.04580559581518173 + 0.001 * 7.128565788269043
Epoch 440, val loss: 0.9608569145202637
Epoch 450, training loss: 0.048658017069101334 = 0.04153280705213547 + 0.001 * 7.125208854675293
Epoch 450, val loss: 0.9754015803337097
Epoch 460, training loss: 0.044915564358234406 = 0.037789925932884216 + 0.001 * 7.125636577606201
Epoch 460, val loss: 0.9896466135978699
Epoch 470, training loss: 0.0416235625743866 = 0.03449820727109909 + 0.001 * 7.125356197357178
Epoch 470, val loss: 1.0036214590072632
Epoch 480, training loss: 0.03871653601527214 = 0.03159154951572418 + 0.001 * 7.12498664855957
Epoch 480, val loss: 1.0172293186187744
Epoch 490, training loss: 0.036139391362667084 = 0.02901429869234562 + 0.001 * 7.125092029571533
Epoch 490, val loss: 1.0305263996124268
Epoch 500, training loss: 0.03385067731142044 = 0.026720445603132248 + 0.001 * 7.130233287811279
Epoch 500, val loss: 1.0434731245040894
Epoch 510, training loss: 0.031797464936971664 = 0.02467195875942707 + 0.001 * 7.125506401062012
Epoch 510, val loss: 1.0561121702194214
Epoch 520, training loss: 0.02996155247092247 = 0.022836625576019287 + 0.001 * 7.1249260902404785
Epoch 520, val loss: 1.068429946899414
Epoch 530, training loss: 0.02831181138753891 = 0.021187784150242805 + 0.001 * 7.124027729034424
Epoch 530, val loss: 1.0804091691970825
Epoch 540, training loss: 0.026828348636627197 = 0.01970258727669716 + 0.001 * 7.12576150894165
Epoch 540, val loss: 1.0920592546463013
Epoch 550, training loss: 0.025484517216682434 = 0.01836128905415535 + 0.001 * 7.123228549957275
Epoch 550, val loss: 1.1034654378890991
Epoch 560, training loss: 0.02426946721971035 = 0.01714731939136982 + 0.001 * 7.122147560119629
Epoch 560, val loss: 1.1145635843276978
Epoch 570, training loss: 0.023167185485363007 = 0.01604606769979 + 0.001 * 7.121116638183594
Epoch 570, val loss: 1.1253715753555298
Epoch 580, training loss: 0.02216818369925022 = 0.015045012347400188 + 0.001 * 7.123171329498291
Epoch 580, val loss: 1.135908842086792
Epoch 590, training loss: 0.02125507965683937 = 0.014133096672594547 + 0.001 * 7.121983528137207
Epoch 590, val loss: 1.1461827754974365
Epoch 600, training loss: 0.020419398322701454 = 0.013300532475113869 + 0.001 * 7.118865489959717
Epoch 600, val loss: 1.1561973094940186
Epoch 610, training loss: 0.019656723365187645 = 0.012539004907011986 + 0.001 * 7.117718696594238
Epoch 610, val loss: 1.1659554243087769
Epoch 620, training loss: 0.018965529277920723 = 0.01184108853340149 + 0.001 * 7.1244401931762695
Epoch 620, val loss: 1.17546546459198
Epoch 630, training loss: 0.018315594643354416 = 0.01120029203593731 + 0.001 * 7.115301132202148
Epoch 630, val loss: 1.1847268342971802
Epoch 640, training loss: 0.017725156620144844 = 0.010610819794237614 + 0.001 * 7.114336967468262
Epoch 640, val loss: 1.193752646446228
Epoch 650, training loss: 0.01718924008309841 = 0.010067649185657501 + 0.001 * 7.121590614318848
Epoch 650, val loss: 1.202560544013977
Epoch 660, training loss: 0.016674768179655075 = 0.00956620555371046 + 0.001 * 7.108563423156738
Epoch 660, val loss: 1.2111486196517944
Epoch 670, training loss: 0.016208793967962265 = 0.009102494455873966 + 0.001 * 7.10629940032959
Epoch 670, val loss: 1.2195284366607666
Epoch 680, training loss: 0.015781356021761894 = 0.008672952651977539 + 0.001 * 7.108402729034424
Epoch 680, val loss: 1.2276992797851562
Epoch 690, training loss: 0.015384890139102936 = 0.008274449035525322 + 0.001 * 7.110440731048584
Epoch 690, val loss: 1.2356843948364258
Epoch 700, training loss: 0.01501537673175335 = 0.007904107682406902 + 0.001 * 7.111268997192383
Epoch 700, val loss: 1.243464469909668
Epoch 710, training loss: 0.0146616380661726 = 0.00755944661796093 + 0.001 * 7.10219144821167
Epoch 710, val loss: 1.251045823097229
Epoch 720, training loss: 0.014329144731163979 = 0.007238264195621014 + 0.001 * 7.090880870819092
Epoch 720, val loss: 1.2584272623062134
Epoch 730, training loss: 0.014040421694517136 = 0.006938460282981396 + 0.001 * 7.1019606590271
Epoch 730, val loss: 1.2656444311141968
Epoch 740, training loss: 0.013748434372246265 = 0.006658222526311874 + 0.001 * 7.090211391448975
Epoch 740, val loss: 1.272675633430481
Epoch 750, training loss: 0.01347250398248434 = 0.006395899225026369 + 0.001 * 7.07660436630249
Epoch 750, val loss: 1.2795294523239136
Epoch 760, training loss: 0.013267753645777702 = 0.006150004453957081 + 0.001 * 7.117748260498047
Epoch 760, val loss: 1.2862422466278076
Epoch 770, training loss: 0.012995563447475433 = 0.005919189658015966 + 0.001 * 7.07637357711792
Epoch 770, val loss: 1.2927700281143188
Epoch 780, training loss: 0.012782122939825058 = 0.005702320486307144 + 0.001 * 7.079802513122559
Epoch 780, val loss: 1.2991639375686646
Epoch 790, training loss: 0.012564308941364288 = 0.005498266313225031 + 0.001 * 7.066041946411133
Epoch 790, val loss: 1.3054094314575195
Epoch 800, training loss: 0.012370110489428043 = 0.005306086968630552 + 0.001 * 7.064023017883301
Epoch 800, val loss: 1.3115170001983643
Epoch 810, training loss: 0.012181184254586697 = 0.005124875344336033 + 0.001 * 7.056308746337891
Epoch 810, val loss: 1.317480444908142
Epoch 820, training loss: 0.012029477395117283 = 0.00495380163192749 + 0.001 * 7.0756754875183105
Epoch 820, val loss: 1.3233319520950317
Epoch 830, training loss: 0.011862692423164845 = 0.004792159888893366 + 0.001 * 7.070532321929932
Epoch 830, val loss: 1.3290241956710815
Epoch 840, training loss: 0.011749894358217716 = 0.004639251157641411 + 0.001 * 7.110642910003662
Epoch 840, val loss: 1.3345935344696045
Epoch 850, training loss: 0.01153418980538845 = 0.004494476597756147 + 0.001 * 7.039712905883789
Epoch 850, val loss: 1.340043306350708
Epoch 860, training loss: 0.01139017567038536 = 0.004357276484370232 + 0.001 * 7.032899379730225
Epoch 860, val loss: 1.3453843593597412
Epoch 870, training loss: 0.0112694026902318 = 0.004227114375680685 + 0.001 * 7.042287826538086
Epoch 870, val loss: 1.350626826286316
Epoch 880, training loss: 0.011155441403388977 = 0.004103528335690498 + 0.001 * 7.051913261413574
Epoch 880, val loss: 1.3557242155075073
Epoch 890, training loss: 0.011012887582182884 = 0.003986078780144453 + 0.001 * 7.02680778503418
Epoch 890, val loss: 1.3607443571090698
Epoch 900, training loss: 0.010951154865324497 = 0.0038743868935853243 + 0.001 * 7.076767444610596
Epoch 900, val loss: 1.3656415939331055
Epoch 910, training loss: 0.010793234221637249 = 0.0037680596578866243 + 0.001 * 7.025174140930176
Epoch 910, val loss: 1.3704344034194946
Epoch 920, training loss: 0.010692465119063854 = 0.0036667741369456053 + 0.001 * 7.02569055557251
Epoch 920, val loss: 1.3751375675201416
Epoch 930, training loss: 0.010593998245894909 = 0.0035701931919902563 + 0.001 * 7.023804664611816
Epoch 930, val loss: 1.3797250986099243
Epoch 940, training loss: 0.010496038943529129 = 0.00347802578471601 + 0.001 * 7.018013000488281
Epoch 940, val loss: 1.3842304944992065
Epoch 950, training loss: 0.01041087880730629 = 0.0033899073023349047 + 0.001 * 7.020970821380615
Epoch 950, val loss: 1.388653039932251
Epoch 960, training loss: 0.010363411158323288 = 0.003305369522422552 + 0.001 * 7.058041095733643
Epoch 960, val loss: 1.3929990530014038
Epoch 970, training loss: 0.010234195739030838 = 0.003223647829145193 + 0.001 * 7.010547161102295
Epoch 970, val loss: 1.3973238468170166
Epoch 980, training loss: 0.01017742045223713 = 0.003143874229863286 + 0.001 * 7.03354549407959
Epoch 980, val loss: 1.4016530513763428
Epoch 990, training loss: 0.010086390189826488 = 0.003065556986257434 + 0.001 * 7.0208330154418945
Epoch 990, val loss: 1.4059892892837524
Epoch 1000, training loss: 0.009996701031923294 = 0.002988348715007305 + 0.001 * 7.008351802825928
Epoch 1000, val loss: 1.4103807210922241
Epoch 1010, training loss: 0.0099666528403759 = 0.0029122731648385525 + 0.001 * 7.054379463195801
Epoch 1010, val loss: 1.414825201034546
Epoch 1020, training loss: 0.00983658991754055 = 0.002837348962202668 + 0.001 * 6.999240875244141
Epoch 1020, val loss: 1.419325351715088
Epoch 1030, training loss: 0.009775045327842236 = 0.00276357214897871 + 0.001 * 7.011472702026367
Epoch 1030, val loss: 1.4238667488098145
Epoch 1040, training loss: 0.009692773222923279 = 0.002690979978069663 + 0.001 * 7.001792907714844
Epoch 1040, val loss: 1.4285231828689575
Epoch 1050, training loss: 0.009624995291233063 = 0.002619732404127717 + 0.001 * 7.005262851715088
Epoch 1050, val loss: 1.4332019090652466
Epoch 1060, training loss: 0.00954589992761612 = 0.0025499374605715275 + 0.001 * 6.995962142944336
Epoch 1060, val loss: 1.4379332065582275
Epoch 1070, training loss: 0.00947326049208641 = 0.0024817015510052443 + 0.001 * 6.99155855178833
Epoch 1070, val loss: 1.4427040815353394
Epoch 1080, training loss: 0.009408834390342236 = 0.0024151713587343693 + 0.001 * 6.9936628341674805
Epoch 1080, val loss: 1.4475165605545044
Epoch 1090, training loss: 0.0093524856492877 = 0.0023505957797169685 + 0.001 * 7.001889705657959
Epoch 1090, val loss: 1.4522981643676758
Epoch 1100, training loss: 0.00927683711051941 = 0.0022881310433149338 + 0.001 * 6.988705158233643
Epoch 1100, val loss: 1.457148790359497
Epoch 1110, training loss: 0.009225982241332531 = 0.002227655379101634 + 0.001 * 6.998326778411865
Epoch 1110, val loss: 1.4619804620742798
Epoch 1120, training loss: 0.009178591892123222 = 0.0021692574955523014 + 0.001 * 7.009334087371826
Epoch 1120, val loss: 1.4667917490005493
Epoch 1130, training loss: 0.009114743210375309 = 0.002112826332449913 + 0.001 * 7.001916408538818
Epoch 1130, val loss: 1.4716118574142456
Epoch 1140, training loss: 0.00904332846403122 = 0.00205839890986681 + 0.001 * 6.984928607940674
Epoch 1140, val loss: 1.4764071702957153
Epoch 1150, training loss: 0.00898775551468134 = 0.002005901886150241 + 0.001 * 6.981853485107422
Epoch 1150, val loss: 1.4811540842056274
Epoch 1160, training loss: 0.008937245234847069 = 0.0019554602913558483 + 0.001 * 6.981784343719482
Epoch 1160, val loss: 1.4858278036117554
Epoch 1170, training loss: 0.008908150717616081 = 0.0019069468835368752 + 0.001 * 7.001203536987305
Epoch 1170, val loss: 1.490460991859436
Epoch 1180, training loss: 0.008836139924824238 = 0.0018602838972583413 + 0.001 * 6.975855350494385
Epoch 1180, val loss: 1.4951030015945435
Epoch 1190, training loss: 0.008791913278400898 = 0.0018152926350012422 + 0.001 * 6.976620197296143
Epoch 1190, val loss: 1.499678373336792
Epoch 1200, training loss: 0.008779075928032398 = 0.0017718486487865448 + 0.001 * 7.007226943969727
Epoch 1200, val loss: 1.504218339920044
Epoch 1210, training loss: 0.008712466806173325 = 0.0017299432074651122 + 0.001 * 6.982523441314697
Epoch 1210, val loss: 1.5087766647338867
Epoch 1220, training loss: 0.008670318871736526 = 0.001689433236606419 + 0.001 * 6.9808855056762695
Epoch 1220, val loss: 1.5133177042007446
Epoch 1230, training loss: 0.008641154505312443 = 0.0016503159422427416 + 0.001 * 6.990838527679443
Epoch 1230, val loss: 1.517785668373108
Epoch 1240, training loss: 0.008596831932663918 = 0.0016127096023410559 + 0.001 * 6.984122276306152
Epoch 1240, val loss: 1.522260308265686
Epoch 1250, training loss: 0.008537768386304379 = 0.0015765801072120667 + 0.001 * 6.961187839508057
Epoch 1250, val loss: 1.5266295671463013
Epoch 1260, training loss: 0.008543784730136395 = 0.0015417227987200022 + 0.001 * 7.002061367034912
Epoch 1260, val loss: 1.5309334993362427
Epoch 1270, training loss: 0.008475402370095253 = 0.0015080883167684078 + 0.001 * 6.967313289642334
Epoch 1270, val loss: 1.5352494716644287
Epoch 1280, training loss: 0.008467307314276695 = 0.001475650118663907 + 0.001 * 6.991657257080078
Epoch 1280, val loss: 1.5394723415374756
Epoch 1290, training loss: 0.008408652618527412 = 0.0014444206608459353 + 0.001 * 6.964231967926025
Epoch 1290, val loss: 1.543642282485962
Epoch 1300, training loss: 0.008387504145503044 = 0.001414382946677506 + 0.001 * 6.97312068939209
Epoch 1300, val loss: 1.5477412939071655
Epoch 1310, training loss: 0.008344417437911034 = 0.001385438023135066 + 0.001 * 6.95897912979126
Epoch 1310, val loss: 1.5517598390579224
Epoch 1320, training loss: 0.00832135509699583 = 0.001357512199319899 + 0.001 * 6.963842391967773
Epoch 1320, val loss: 1.5557416677474976
Epoch 1330, training loss: 0.008323944173753262 = 0.0013305452885106206 + 0.001 * 6.993398189544678
Epoch 1330, val loss: 1.559684157371521
Epoch 1340, training loss: 0.008288676850497723 = 0.0013045873492956161 + 0.001 * 6.984089374542236
Epoch 1340, val loss: 1.563607931137085
Epoch 1350, training loss: 0.008233021013438702 = 0.0012794820358976722 + 0.001 * 6.95353889465332
Epoch 1350, val loss: 1.5674598217010498
Epoch 1360, training loss: 0.00821862742304802 = 0.0012553271371871233 + 0.001 * 6.9633002281188965
Epoch 1360, val loss: 1.5712379217147827
Epoch 1370, training loss: 0.008184902369976044 = 0.0012320668902248144 + 0.001 * 6.9528350830078125
Epoch 1370, val loss: 1.5749760866165161
Epoch 1380, training loss: 0.00820110458880663 = 0.0012096791760995984 + 0.001 * 6.991425037384033
Epoch 1380, val loss: 1.5786316394805908
Epoch 1390, training loss: 0.008150228299200535 = 0.001188076101243496 + 0.001 * 6.962152004241943
Epoch 1390, val loss: 1.5822465419769287
Epoch 1400, training loss: 0.00813431665301323 = 0.0011673092376440763 + 0.001 * 6.967007160186768
Epoch 1400, val loss: 1.5857328176498413
Epoch 1410, training loss: 0.008097682148218155 = 0.001147319097071886 + 0.001 * 6.950362682342529
Epoch 1410, val loss: 1.5891996622085571
Epoch 1420, training loss: 0.008077939040958881 = 0.0011280893813818693 + 0.001 * 6.9498491287231445
Epoch 1420, val loss: 1.5925852060317993
Epoch 1430, training loss: 0.008092055097222328 = 0.00110956362914294 + 0.001 * 6.9824910163879395
Epoch 1430, val loss: 1.5958898067474365
Epoch 1440, training loss: 0.008063102141022682 = 0.001091737300157547 + 0.001 * 6.971364974975586
Epoch 1440, val loss: 1.599136233329773
Epoch 1450, training loss: 0.008024646900594234 = 0.0010745692998170853 + 0.001 * 6.950077056884766
Epoch 1450, val loss: 1.6023198366165161
Epoch 1460, training loss: 0.007998339831829071 = 0.0010580159723758698 + 0.001 * 6.940323352813721
Epoch 1460, val loss: 1.6054188013076782
Epoch 1470, training loss: 0.007997258566319942 = 0.0010420373873785138 + 0.001 * 6.955220699310303
Epoch 1470, val loss: 1.608474612236023
Epoch 1480, training loss: 0.007966330274939537 = 0.0010266099125146866 + 0.001 * 6.939720630645752
Epoch 1480, val loss: 1.6114643812179565
Epoch 1490, training loss: 0.007948104292154312 = 0.0010117419296875596 + 0.001 * 6.936361789703369
Epoch 1490, val loss: 1.614423394203186
Epoch 1500, training loss: 0.007975981570780277 = 0.0009973699925467372 + 0.001 * 6.978611469268799
Epoch 1500, val loss: 1.6172618865966797
Epoch 1510, training loss: 0.007921148091554642 = 0.0009835031814873219 + 0.001 * 6.937644004821777
Epoch 1510, val loss: 1.6200965642929077
Epoch 1520, training loss: 0.007893466390669346 = 0.0009701586677692831 + 0.001 * 6.923307418823242
Epoch 1520, val loss: 1.6228220462799072
Epoch 1530, training loss: 0.007883759215474129 = 0.0009572952403686941 + 0.001 * 6.926464080810547
Epoch 1530, val loss: 1.6255147457122803
Epoch 1540, training loss: 0.007886717095971107 = 0.0009449052740819752 + 0.001 * 6.941811561584473
Epoch 1540, val loss: 1.6280877590179443
Epoch 1550, training loss: 0.007864563725888729 = 0.0009329667082056403 + 0.001 * 6.9315972328186035
Epoch 1550, val loss: 1.6306513547897339
Epoch 1560, training loss: 0.007863236591219902 = 0.0009214202291332185 + 0.001 * 6.9418158531188965
Epoch 1560, val loss: 1.633135437965393
Epoch 1570, training loss: 0.007832608185708523 = 0.0009102638578042388 + 0.001 * 6.922343730926514
Epoch 1570, val loss: 1.635568380355835
Epoch 1580, training loss: 0.007825405336916447 = 0.000899444508831948 + 0.001 * 6.925960063934326
Epoch 1580, val loss: 1.6379444599151611
Epoch 1590, training loss: 0.007808018010109663 = 0.0008890334283933043 + 0.001 * 6.9189839363098145
Epoch 1590, val loss: 1.6402713060379028
Epoch 1600, training loss: 0.007813913747668266 = 0.0008789680432528257 + 0.001 * 6.934945106506348
Epoch 1600, val loss: 1.6425199508666992
Epoch 1610, training loss: 0.007794634439051151 = 0.0008692466071806848 + 0.001 * 6.925387382507324
Epoch 1610, val loss: 1.6447625160217285
Epoch 1620, training loss: 0.007803552784025669 = 0.0008598110871389508 + 0.001 * 6.943741321563721
Epoch 1620, val loss: 1.6469380855560303
Epoch 1630, training loss: 0.007758637424558401 = 0.0008506831945851445 + 0.001 * 6.907954216003418
Epoch 1630, val loss: 1.6490492820739746
Epoch 1640, training loss: 0.007760418578982353 = 0.0008418482611887157 + 0.001 * 6.918570041656494
Epoch 1640, val loss: 1.6511647701263428
Epoch 1650, training loss: 0.0077946423552930355 = 0.0008332878933288157 + 0.001 * 6.9613542556762695
Epoch 1650, val loss: 1.6531710624694824
Epoch 1660, training loss: 0.007731068413704634 = 0.0008250501123256981 + 0.001 * 6.906017780303955
Epoch 1660, val loss: 1.6551653146743774
Epoch 1670, training loss: 0.007737499196082354 = 0.00081705889897421 + 0.001 * 6.920439720153809
Epoch 1670, val loss: 1.657129168510437
Epoch 1680, training loss: 0.007749014534056187 = 0.0008093041833490133 + 0.001 * 6.939709663391113
Epoch 1680, val loss: 1.6590303182601929
Epoch 1690, training loss: 0.007707910146564245 = 0.0008018043008632958 + 0.001 * 6.9061055183410645
Epoch 1690, val loss: 1.660909652709961
Epoch 1700, training loss: 0.007754337973892689 = 0.0007945182733237743 + 0.001 * 6.959819316864014
Epoch 1700, val loss: 1.662727952003479
Epoch 1710, training loss: 0.0077348449267446995 = 0.0007874886505305767 + 0.001 * 6.9473557472229
Epoch 1710, val loss: 1.6644599437713623
Epoch 1720, training loss: 0.007711948361247778 = 0.0007806913345120847 + 0.001 * 6.9312567710876465
Epoch 1720, val loss: 1.6661980152130127
Epoch 1730, training loss: 0.007669171784073114 = 0.0007740838336758316 + 0.001 * 6.895087718963623
Epoch 1730, val loss: 1.667932152748108
Epoch 1740, training loss: 0.007691793609410524 = 0.0007676774985156953 + 0.001 * 6.9241156578063965
Epoch 1740, val loss: 1.6695373058319092
Epoch 1750, training loss: 0.007661683950573206 = 0.0007614648202434182 + 0.001 * 6.900218963623047
Epoch 1750, val loss: 1.671217441558838
Epoch 1760, training loss: 0.00766542786732316 = 0.000755448651034385 + 0.001 * 6.909978866577148
Epoch 1760, val loss: 1.672778606414795
Epoch 1770, training loss: 0.007715035695582628 = 0.000749597791582346 + 0.001 * 6.965437412261963
Epoch 1770, val loss: 1.67429518699646
Epoch 1780, training loss: 0.007649603299796581 = 0.0007439014734700322 + 0.001 * 6.905701637268066
Epoch 1780, val loss: 1.675823450088501
Epoch 1790, training loss: 0.007622190751135349 = 0.0007383764605037868 + 0.001 * 6.883814334869385
Epoch 1790, val loss: 1.6773067712783813
Epoch 1800, training loss: 0.007642111741006374 = 0.0007330311927944422 + 0.001 * 6.9090800285339355
Epoch 1800, val loss: 1.6787577867507935
Epoch 1810, training loss: 0.00766001595184207 = 0.0007278263219632208 + 0.001 * 6.932189464569092
Epoch 1810, val loss: 1.6801235675811768
Epoch 1820, training loss: 0.0076187774538993835 = 0.0007228107424452901 + 0.001 * 6.895966529846191
Epoch 1820, val loss: 1.6815022230148315
Epoch 1830, training loss: 0.007598530501127243 = 0.0007179151871241629 + 0.001 * 6.880615234375
Epoch 1830, val loss: 1.6828739643096924
Epoch 1840, training loss: 0.007593202404677868 = 0.000713150599040091 + 0.001 * 6.880051612854004
Epoch 1840, val loss: 1.684140920639038
Epoch 1850, training loss: 0.007606349885463715 = 0.0007085324614308774 + 0.001 * 6.897817134857178
Epoch 1850, val loss: 1.6854525804519653
Epoch 1860, training loss: 0.007627701386809349 = 0.0007040344644337893 + 0.001 * 6.923666954040527
Epoch 1860, val loss: 1.6866588592529297
Epoch 1870, training loss: 0.00758771225810051 = 0.0006996865849941969 + 0.001 * 6.888025283813477
Epoch 1870, val loss: 1.6878745555877686
Epoch 1880, training loss: 0.007621347438544035 = 0.0006954522687010467 + 0.001 * 6.9258952140808105
Epoch 1880, val loss: 1.68905770778656
Epoch 1890, training loss: 0.007590873166918755 = 0.0006913268007338047 + 0.001 * 6.899546146392822
Epoch 1890, val loss: 1.690242886543274
Epoch 1900, training loss: 0.007596048526465893 = 0.0006873288657516241 + 0.001 * 6.908719539642334
Epoch 1900, val loss: 1.6913139820098877
Epoch 1910, training loss: 0.007594472728669643 = 0.0006834573578089476 + 0.001 * 6.911015033721924
Epoch 1910, val loss: 1.6924169063568115
Epoch 1920, training loss: 0.007559063844382763 = 0.0006797101814299822 + 0.001 * 6.879353046417236
Epoch 1920, val loss: 1.6934751272201538
Epoch 1930, training loss: 0.007578973658382893 = 0.00067610148107633 + 0.001 * 6.902871608734131
Epoch 1930, val loss: 1.6944845914840698
Epoch 1940, training loss: 0.007582683581858873 = 0.0006725775892846286 + 0.001 * 6.9101057052612305
Epoch 1940, val loss: 1.695525050163269
Epoch 1950, training loss: 0.0075665912590920925 = 0.0006691591697745025 + 0.001 * 6.89743185043335
Epoch 1950, val loss: 1.6964421272277832
Epoch 1960, training loss: 0.00754969660192728 = 0.0006658121710643172 + 0.001 * 6.883883953094482
Epoch 1960, val loss: 1.6974692344665527
Epoch 1970, training loss: 0.007529699243605137 = 0.0006625500973314047 + 0.001 * 6.867148399353027
Epoch 1970, val loss: 1.6983191967010498
Epoch 1980, training loss: 0.007554774172604084 = 0.000659363460727036 + 0.001 * 6.895410537719727
Epoch 1980, val loss: 1.699259638786316
Epoch 1990, training loss: 0.00752128753811121 = 0.000656259071547538 + 0.001 * 6.865027904510498
Epoch 1990, val loss: 1.7001479864120483
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 1.9698359966278076 = 1.961239218711853 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.9676952362060547
Epoch 10, training loss: 1.9592856168746948 = 1.9506888389587402 + 0.001 * 8.596783638000488
Epoch 10, val loss: 1.9568973779678345
Epoch 20, training loss: 1.9463796615600586 = 1.9377830028533936 + 0.001 * 8.596599578857422
Epoch 20, val loss: 1.9436557292938232
Epoch 30, training loss: 1.9283039569854736 = 1.9197077751159668 + 0.001 * 8.596193313598633
Epoch 30, val loss: 1.9251458644866943
Epoch 40, training loss: 1.901707649230957 = 1.8931124210357666 + 0.001 * 8.595250129699707
Epoch 40, val loss: 1.8981736898422241
Epoch 50, training loss: 1.8644883632659912 = 1.855895757675171 + 0.001 * 8.592567443847656
Epoch 50, val loss: 1.8618075847625732
Epoch 60, training loss: 1.8229396343231201 = 1.814356803894043 + 0.001 * 8.582839012145996
Epoch 60, val loss: 1.824235439300537
Epoch 70, training loss: 1.7908605337142944 = 1.7823164463043213 + 0.001 * 8.544123649597168
Epoch 70, val loss: 1.7960617542266846
Epoch 80, training loss: 1.7579671144485474 = 1.7496627569198608 + 0.001 * 8.30440616607666
Epoch 80, val loss: 1.7642173767089844
Epoch 90, training loss: 1.7120121717453003 = 1.7039543390274048 + 0.001 * 8.057784080505371
Epoch 90, val loss: 1.7243379354476929
Epoch 100, training loss: 1.647922396659851 = 1.6399792432785034 + 0.001 * 7.943208694458008
Epoch 100, val loss: 1.6713553667068481
Epoch 110, training loss: 1.5635377168655396 = 1.5557314157485962 + 0.001 * 7.80624532699585
Epoch 110, val loss: 1.6023091077804565
Epoch 120, training loss: 1.4672273397445679 = 1.4596071243286133 + 0.001 * 7.620259761810303
Epoch 120, val loss: 1.5236434936523438
Epoch 130, training loss: 1.3681330680847168 = 1.3606972694396973 + 0.001 * 7.435783386230469
Epoch 130, val loss: 1.4431359767913818
Epoch 140, training loss: 1.2699319124221802 = 1.2625771760940552 + 0.001 * 7.354704856872559
Epoch 140, val loss: 1.3666131496429443
Epoch 150, training loss: 1.172084927558899 = 1.1648396253585815 + 0.001 * 7.245311737060547
Epoch 150, val loss: 1.2924467325210571
Epoch 160, training loss: 1.0740113258361816 = 1.066809058189392 + 0.001 * 7.202270030975342
Epoch 160, val loss: 1.2206381559371948
Epoch 170, training loss: 0.9768837690353394 = 0.9696895480155945 + 0.001 * 7.1942267417907715
Epoch 170, val loss: 1.1512936353683472
Epoch 180, training loss: 0.8839025497436523 = 0.8767181038856506 + 0.001 * 7.1844563484191895
Epoch 180, val loss: 1.0866143703460693
Epoch 190, training loss: 0.7992132306098938 = 0.7920413613319397 + 0.001 * 7.171874046325684
Epoch 190, val loss: 1.0294058322906494
Epoch 200, training loss: 0.7253304123878479 = 0.7181747555732727 + 0.001 * 7.1556525230407715
Epoch 200, val loss: 0.9815429449081421
Epoch 210, training loss: 0.6614522337913513 = 0.654316782951355 + 0.001 * 7.135433197021484
Epoch 210, val loss: 0.9425138235092163
Epoch 220, training loss: 0.6046896576881409 = 0.5975719690322876 + 0.001 * 7.117701530456543
Epoch 220, val loss: 0.9102762937545776
Epoch 230, training loss: 0.5522000789642334 = 0.5450915694236755 + 0.001 * 7.1085028648376465
Epoch 230, val loss: 0.8829209208488464
Epoch 240, training loss: 0.5022039413452148 = 0.495098739862442 + 0.001 * 7.105186462402344
Epoch 240, val loss: 0.8592662811279297
Epoch 250, training loss: 0.4540098011493683 = 0.44690585136413574 + 0.001 * 7.103940486907959
Epoch 250, val loss: 0.8385078310966492
Epoch 260, training loss: 0.40758752822875977 = 0.4004838466644287 + 0.001 * 7.1036858558654785
Epoch 260, val loss: 0.8202791810035706
Epoch 270, training loss: 0.3633013367652893 = 0.3561979830265045 + 0.001 * 7.103358268737793
Epoch 270, val loss: 0.8043187260627747
Epoch 280, training loss: 0.32152044773101807 = 0.3144156336784363 + 0.001 * 7.104817867279053
Epoch 280, val loss: 0.7903926968574524
Epoch 290, training loss: 0.2826652526855469 = 0.2755607068538666 + 0.001 * 7.10455322265625
Epoch 290, val loss: 0.7785645127296448
Epoch 300, training loss: 0.24712541699409485 = 0.24002113938331604 + 0.001 * 7.1042799949646
Epoch 300, val loss: 0.7689831256866455
Epoch 310, training loss: 0.21518762409687042 = 0.20808275043964386 + 0.001 * 7.1048784255981445
Epoch 310, val loss: 0.7617642879486084
Epoch 320, training loss: 0.18697355687618256 = 0.17986805737018585 + 0.001 * 7.10550594329834
Epoch 320, val loss: 0.7571728825569153
Epoch 330, training loss: 0.16244681179523468 = 0.1553390920162201 + 0.001 * 7.1077165603637695
Epoch 330, val loss: 0.75534588098526
Epoch 340, training loss: 0.1413610875606537 = 0.1342531442642212 + 0.001 * 7.107949733734131
Epoch 340, val loss: 0.7563396692276001
Epoch 350, training loss: 0.1233505979180336 = 0.1162426695227623 + 0.001 * 7.10792875289917
Epoch 350, val loss: 0.7599803805351257
Epoch 360, training loss: 0.10801967978477478 = 0.10091082751750946 + 0.001 * 7.108850479125977
Epoch 360, val loss: 0.7659748792648315
Epoch 370, training loss: 0.09498817473649979 = 0.0878782868385315 + 0.001 * 7.109889984130859
Epoch 370, val loss: 0.7739176154136658
Epoch 380, training loss: 0.08392204344272614 = 0.07681110501289368 + 0.001 * 7.110938549041748
Epoch 380, val loss: 0.7834025025367737
Epoch 390, training loss: 0.07452845573425293 = 0.06741800159215927 + 0.001 * 7.1104536056518555
Epoch 390, val loss: 0.7940959930419922
Epoch 400, training loss: 0.06656068563461304 = 0.059449851512908936 + 0.001 * 7.1108317375183105
Epoch 400, val loss: 0.8056837916374207
Epoch 410, training loss: 0.05979902297258377 = 0.052686598151922226 + 0.001 * 7.112422943115234
Epoch 410, val loss: 0.8178091049194336
Epoch 420, training loss: 0.0540434829890728 = 0.046932101249694824 + 0.001 * 7.111380577087402
Epoch 420, val loss: 0.8302505612373352
Epoch 430, training loss: 0.04913117736577988 = 0.04201754927635193 + 0.001 * 7.1136274337768555
Epoch 430, val loss: 0.8427793383598328
Epoch 440, training loss: 0.04491133242845535 = 0.037799712270498276 + 0.001 * 7.111619472503662
Epoch 440, val loss: 0.8553007245063782
Epoch 450, training loss: 0.041273679584264755 = 0.03416220471262932 + 0.001 * 7.111474514007568
Epoch 450, val loss: 0.8677024245262146
Epoch 460, training loss: 0.03811890259385109 = 0.031008856371045113 + 0.001 * 7.110045433044434
Epoch 460, val loss: 0.8799031972885132
Epoch 470, training loss: 0.03537120297551155 = 0.028260929509997368 + 0.001 * 7.11027193069458
Epoch 470, val loss: 0.8918551802635193
Epoch 480, training loss: 0.03296465426683426 = 0.02585475705564022 + 0.001 * 7.109895706176758
Epoch 480, val loss: 0.9035281538963318
Epoch 490, training loss: 0.030847817659378052 = 0.02373792603611946 + 0.001 * 7.109890460968018
Epoch 490, val loss: 0.9149349927902222
Epoch 500, training loss: 0.028978221118450165 = 0.021867355331778526 + 0.001 * 7.110865116119385
Epoch 500, val loss: 0.9260232448577881
Epoch 510, training loss: 0.027313323691487312 = 0.020206455141305923 + 0.001 * 7.106867790222168
Epoch 510, val loss: 0.9368004202842712
Epoch 520, training loss: 0.02582903392612934 = 0.01872328296303749 + 0.001 * 7.10575008392334
Epoch 520, val loss: 0.9472938179969788
Epoch 530, training loss: 0.02449466660618782 = 0.017390824854373932 + 0.001 * 7.1038408279418945
Epoch 530, val loss: 0.9575338959693909
Epoch 540, training loss: 0.023291178047657013 = 0.016188887879252434 + 0.001 * 7.102288722991943
Epoch 540, val loss: 0.9675739407539368
Epoch 550, training loss: 0.022213296964764595 = 0.015101774595677853 + 0.001 * 7.111522674560547
Epoch 550, val loss: 0.977432131767273
Epoch 560, training loss: 0.021221619099378586 = 0.014116240665316582 + 0.001 * 7.105379104614258
Epoch 560, val loss: 0.9870901703834534
Epoch 570, training loss: 0.02032063715159893 = 0.013220868073403835 + 0.001 * 7.099769115447998
Epoch 570, val loss: 0.9965516328811646
Epoch 580, training loss: 0.019509168341755867 = 0.012405535206198692 + 0.001 * 7.10363245010376
Epoch 580, val loss: 1.0057867765426636
Epoch 590, training loss: 0.018761470913887024 = 0.011661911383271217 + 0.001 * 7.099560260772705
Epoch 590, val loss: 1.0148266553878784
Epoch 600, training loss: 0.018076779320836067 = 0.010982497595250607 + 0.001 * 7.09428071975708
Epoch 600, val loss: 1.0236303806304932
Epoch 610, training loss: 0.017473390325903893 = 0.010360599495470524 + 0.001 * 7.112790107727051
Epoch 610, val loss: 1.0322036743164062
Epoch 620, training loss: 0.016885049641132355 = 0.009790275245904922 + 0.001 * 7.09477424621582
Epoch 620, val loss: 1.0405514240264893
Epoch 630, training loss: 0.01635698415338993 = 0.009266256354749203 + 0.001 * 7.090727806091309
Epoch 630, val loss: 1.048681378364563
Epoch 640, training loss: 0.01587047055363655 = 0.008783937431871891 + 0.001 * 7.086531639099121
Epoch 640, val loss: 1.0566004514694214
Epoch 650, training loss: 0.015422052703797817 = 0.008339226245880127 + 0.001 * 7.082826137542725
Epoch 650, val loss: 1.0643030405044556
Epoch 660, training loss: 0.015012269839644432 = 0.007928499951958656 + 0.001 * 7.08376932144165
Epoch 660, val loss: 1.0717792510986328
Epoch 670, training loss: 0.014627760276198387 = 0.007548506837338209 + 0.001 * 7.07925271987915
Epoch 670, val loss: 1.079069972038269
Epoch 680, training loss: 0.014276470988988876 = 0.007196424528956413 + 0.001 * 7.0800461769104
Epoch 680, val loss: 1.0861555337905884
Epoch 690, training loss: 0.01394631341099739 = 0.00686962204053998 + 0.001 * 7.076691150665283
Epoch 690, val loss: 1.093047022819519
Epoch 700, training loss: 0.013638926669955254 = 0.0065658134408295155 + 0.001 * 7.073112964630127
Epoch 700, val loss: 1.0997552871704102
Epoch 710, training loss: 0.013348953798413277 = 0.006282922346144915 + 0.001 * 7.066030502319336
Epoch 710, val loss: 1.106291651725769
Epoch 720, training loss: 0.013083215802907944 = 0.0060190982185304165 + 0.001 * 7.064117431640625
Epoch 720, val loss: 1.1126435995101929
Epoch 730, training loss: 0.012844089418649673 = 0.005772668868303299 + 0.001 * 7.071420669555664
Epoch 730, val loss: 1.1188409328460693
Epoch 740, training loss: 0.012601181864738464 = 0.0055419569835066795 + 0.001 * 7.0592241287231445
Epoch 740, val loss: 1.1248903274536133
Epoch 750, training loss: 0.012414902448654175 = 0.005325476173311472 + 0.001 * 7.089426040649414
Epoch 750, val loss: 1.130789041519165
Epoch 760, training loss: 0.012173017486929893 = 0.005122091621160507 + 0.001 * 7.050925254821777
Epoch 760, val loss: 1.1365365982055664
Epoch 770, training loss: 0.012013830244541168 = 0.004930443130433559 + 0.001 * 7.0833868980407715
Epoch 770, val loss: 1.1421748399734497
Epoch 780, training loss: 0.011791077442467213 = 0.004749621730297804 + 0.001 * 7.041455268859863
Epoch 780, val loss: 1.1476714611053467
Epoch 790, training loss: 0.011655922047793865 = 0.004578650929033756 + 0.001 * 7.077270984649658
Epoch 790, val loss: 1.1530627012252808
Epoch 800, training loss: 0.011486522853374481 = 0.004416989162564278 + 0.001 * 7.069533348083496
Epoch 800, val loss: 1.1583391427993774
Epoch 810, training loss: 0.011304227635264397 = 0.004263851325958967 + 0.001 * 7.040376663208008
Epoch 810, val loss: 1.1635011434555054
Epoch 820, training loss: 0.011174356564879417 = 0.004118544049561024 + 0.001 * 7.055812358856201
Epoch 820, val loss: 1.1685749292373657
Epoch 830, training loss: 0.011016209609806538 = 0.003980698063969612 + 0.001 * 7.035511016845703
Epoch 830, val loss: 1.1735564470291138
Epoch 840, training loss: 0.010890336707234383 = 0.0038497333880513906 + 0.001 * 7.040603160858154
Epoch 840, val loss: 1.178438663482666
Epoch 850, training loss: 0.010772996582090855 = 0.0037253752816468477 + 0.001 * 7.04762077331543
Epoch 850, val loss: 1.1832334995269775
Epoch 860, training loss: 0.010648086667060852 = 0.0036072104703634977 + 0.001 * 7.040875434875488
Epoch 860, val loss: 1.1879240274429321
Epoch 870, training loss: 0.010584456846117973 = 0.003494884818792343 + 0.001 * 7.089571475982666
Epoch 870, val loss: 1.1925593614578247
Epoch 880, training loss: 0.010419972240924835 = 0.0033881133422255516 + 0.001 * 7.031857967376709
Epoch 880, val loss: 1.19705069065094
Epoch 890, training loss: 0.010315031744539738 = 0.0032865784596651793 + 0.001 * 7.0284528732299805
Epoch 890, val loss: 1.2014970779418945
Epoch 900, training loss: 0.010224144905805588 = 0.0031899490859359503 + 0.001 * 7.034195423126221
Epoch 900, val loss: 1.2058416604995728
Epoch 910, training loss: 0.010126699693500996 = 0.00309795537032187 + 0.001 * 7.028743743896484
Epoch 910, val loss: 1.2100887298583984
Epoch 920, training loss: 0.010025713592767715 = 0.0030103521421551704 + 0.001 * 7.0153608322143555
Epoch 920, val loss: 1.2142717838287354
Epoch 930, training loss: 0.009983778931200504 = 0.002926815301179886 + 0.001 * 7.0569634437561035
Epoch 930, val loss: 1.2184029817581177
Epoch 940, training loss: 0.00985865481197834 = 0.0028472193516790867 + 0.001 * 7.011435031890869
Epoch 940, val loss: 1.2223858833312988
Epoch 950, training loss: 0.009826595894992352 = 0.002771212486550212 + 0.001 * 7.05538272857666
Epoch 950, val loss: 1.226362705230713
Epoch 960, training loss: 0.009714131243526936 = 0.00269863149151206 + 0.001 * 7.015499591827393
Epoch 960, val loss: 1.2302006483078003
Epoch 970, training loss: 0.009672217071056366 = 0.0026292987167835236 + 0.001 * 7.0429182052612305
Epoch 970, val loss: 1.2340338230133057
Epoch 980, training loss: 0.009580818004906178 = 0.0025630590971559286 + 0.001 * 7.017758369445801
Epoch 980, val loss: 1.237730622291565
Epoch 990, training loss: 0.009518399834632874 = 0.0024997550062835217 + 0.001 * 7.018643856048584
Epoch 990, val loss: 1.2413969039916992
Epoch 1000, training loss: 0.009450460784137249 = 0.002439136616885662 + 0.001 * 7.011323928833008
Epoch 1000, val loss: 1.2449951171875
Epoch 1010, training loss: 0.009364338591694832 = 0.002381140599027276 + 0.001 * 6.983197212219238
Epoch 1010, val loss: 1.2485276460647583
Epoch 1020, training loss: 0.00932496227324009 = 0.0023255711421370506 + 0.001 * 6.999391078948975
Epoch 1020, val loss: 1.25199294090271
Epoch 1030, training loss: 0.009264146909117699 = 0.002272299723699689 + 0.001 * 6.991847038269043
Epoch 1030, val loss: 1.255395770072937
Epoch 1040, training loss: 0.00922640971839428 = 0.002221232745796442 + 0.001 * 7.005177021026611
Epoch 1040, val loss: 1.258756399154663
Epoch 1050, training loss: 0.009150762110948563 = 0.002172245644032955 + 0.001 * 6.978515625
Epoch 1050, val loss: 1.2620097398757935
Epoch 1060, training loss: 0.009120517410337925 = 0.0021252327132970095 + 0.001 * 6.995284557342529
Epoch 1060, val loss: 1.2652555704116821
Epoch 1070, training loss: 0.009064098820090294 = 0.002080136677250266 + 0.001 * 6.983961582183838
Epoch 1070, val loss: 1.2684024572372437
Epoch 1080, training loss: 0.009013627655804157 = 0.0020367789547890425 + 0.001 * 6.976848125457764
Epoch 1080, val loss: 1.271512746810913
Epoch 1090, training loss: 0.008984205313026905 = 0.0019950582645833492 + 0.001 * 6.989146709442139
Epoch 1090, val loss: 1.2745550870895386
Epoch 1100, training loss: 0.008925322443246841 = 0.0019549280405044556 + 0.001 * 6.970394134521484
Epoch 1100, val loss: 1.277563214302063
Epoch 1110, training loss: 0.00893473345786333 = 0.0019162950338795781 + 0.001 * 7.01843786239624
Epoch 1110, val loss: 1.2805273532867432
Epoch 1120, training loss: 0.008849726058542728 = 0.001879154471680522 + 0.001 * 6.970571517944336
Epoch 1120, val loss: 1.2834004163742065
Epoch 1130, training loss: 0.008805772289633751 = 0.001843385980464518 + 0.001 * 6.962385654449463
Epoch 1130, val loss: 1.2862653732299805
Epoch 1140, training loss: 0.008776050992310047 = 0.0018089248333126307 + 0.001 * 6.967125415802002
Epoch 1140, val loss: 1.2890675067901611
Epoch 1150, training loss: 0.008752354420721531 = 0.0017757242312654853 + 0.001 * 6.976630210876465
Epoch 1150, val loss: 1.2917859554290771
Epoch 1160, training loss: 0.008708301931619644 = 0.001743727014400065 + 0.001 * 6.964574813842773
Epoch 1160, val loss: 1.294508934020996
Epoch 1170, training loss: 0.008669559843838215 = 0.0017128734616562724 + 0.001 * 6.956686496734619
Epoch 1170, val loss: 1.2971482276916504
Epoch 1180, training loss: 0.008661204017698765 = 0.001683106878772378 + 0.001 * 6.978096961975098
Epoch 1180, val loss: 1.2997732162475586
Epoch 1190, training loss: 0.008663689717650414 = 0.0016543602105230093 + 0.001 * 7.009329319000244
Epoch 1190, val loss: 1.3023122549057007
Epoch 1200, training loss: 0.00858494732528925 = 0.0016266332240775228 + 0.001 * 6.958313465118408
Epoch 1200, val loss: 1.3048646450042725
Epoch 1210, training loss: 0.008568467572331429 = 0.0015998331364244223 + 0.001 * 6.968633651733398
Epoch 1210, val loss: 1.3073283433914185
Epoch 1220, training loss: 0.008533982560038567 = 0.0015739321243017912 + 0.001 * 6.960050106048584
Epoch 1220, val loss: 1.3097879886627197
Epoch 1230, training loss: 0.0085225161164999 = 0.0015488832723349333 + 0.001 * 6.9736328125
Epoch 1230, val loss: 1.3121867179870605
Epoch 1240, training loss: 0.008484157733619213 = 0.001524672145023942 + 0.001 * 6.9594855308532715
Epoch 1240, val loss: 1.3145407438278198
Epoch 1250, training loss: 0.008484993129968643 = 0.0015012436779215932 + 0.001 * 6.983748912811279
Epoch 1250, val loss: 1.3168697357177734
Epoch 1260, training loss: 0.008430607616901398 = 0.0014786090468987823 + 0.001 * 6.951998233795166
Epoch 1260, val loss: 1.3191455602645874
Epoch 1270, training loss: 0.008414634503424168 = 0.0014566940953955054 + 0.001 * 6.957940101623535
Epoch 1270, val loss: 1.3214106559753418
Epoch 1280, training loss: 0.008386189118027687 = 0.0014354813611134887 + 0.001 * 6.950707912445068
Epoch 1280, val loss: 1.3236030340194702
Epoch 1290, training loss: 0.008347407914698124 = 0.0014149242779240012 + 0.001 * 6.932483673095703
Epoch 1290, val loss: 1.3257596492767334
Epoch 1300, training loss: 0.008371330797672272 = 0.0013950151624158025 + 0.001 * 6.976315498352051
Epoch 1300, val loss: 1.327941656112671
Epoch 1310, training loss: 0.008304291404783726 = 0.0013757176930084825 + 0.001 * 6.928573131561279
Epoch 1310, val loss: 1.330018162727356
Epoch 1320, training loss: 0.008302153088152409 = 0.0013570076553151011 + 0.001 * 6.945145130157471
Epoch 1320, val loss: 1.3321282863616943
Epoch 1330, training loss: 0.00827027764171362 = 0.0013388673542067409 + 0.001 * 6.93140983581543
Epoch 1330, val loss: 1.3341357707977295
Epoch 1340, training loss: 0.008296189829707146 = 0.0013212572084739804 + 0.001 * 6.9749321937561035
Epoch 1340, val loss: 1.336213231086731
Epoch 1350, training loss: 0.008245388977229595 = 0.001304177800193429 + 0.001 * 6.941211223602295
Epoch 1350, val loss: 1.3381704092025757
Epoch 1360, training loss: 0.008232368156313896 = 0.0012876010732725263 + 0.001 * 6.944766521453857
Epoch 1360, val loss: 1.3401529788970947
Epoch 1370, training loss: 0.008227625861763954 = 0.0012715040938928723 + 0.001 * 6.956121444702148
Epoch 1370, val loss: 1.3420323133468628
Epoch 1380, training loss: 0.008202522993087769 = 0.0012558662565425038 + 0.001 * 6.946656703948975
Epoch 1380, val loss: 1.3439464569091797
Epoch 1390, training loss: 0.008192774839699268 = 0.001240675337612629 + 0.001 * 6.952099323272705
Epoch 1390, val loss: 1.3458400964736938
Epoch 1400, training loss: 0.008166301995515823 = 0.0012259086361154914 + 0.001 * 6.940393447875977
Epoch 1400, val loss: 1.3476459980010986
Epoch 1410, training loss: 0.008135821670293808 = 0.001211562193930149 + 0.001 * 6.924259185791016
Epoch 1410, val loss: 1.3494915962219238
Epoch 1420, training loss: 0.008167312480509281 = 0.0011976092355325818 + 0.001 * 6.969703197479248
Epoch 1420, val loss: 1.3512444496154785
Epoch 1430, training loss: 0.008127219043672085 = 0.0011840671068057418 + 0.001 * 6.943151473999023
Epoch 1430, val loss: 1.3530436754226685
Epoch 1440, training loss: 0.008088987320661545 = 0.0011709117097780108 + 0.001 * 6.9180755615234375
Epoch 1440, val loss: 1.3547643423080444
Epoch 1450, training loss: 0.008059720508754253 = 0.0011581334983929992 + 0.001 * 6.901586532592773
Epoch 1450, val loss: 1.356492519378662
Epoch 1460, training loss: 0.008044566959142685 = 0.001145723508670926 + 0.001 * 6.898842811584473
Epoch 1460, val loss: 1.3581784963607788
Epoch 1470, training loss: 0.008084646426141262 = 0.0011336607858538628 + 0.001 * 6.950985431671143
Epoch 1470, val loss: 1.3598511219024658
Epoch 1480, training loss: 0.008029752410948277 = 0.001121926004998386 + 0.001 * 6.9078264236450195
Epoch 1480, val loss: 1.3614661693572998
Epoch 1490, training loss: 0.008017035201191902 = 0.0011104980949312449 + 0.001 * 6.906537055969238
Epoch 1490, val loss: 1.3630907535552979
Epoch 1500, training loss: 0.008009194396436214 = 0.0010993580799549818 + 0.001 * 6.9098358154296875
Epoch 1500, val loss: 1.3646504878997803
Epoch 1510, training loss: 0.007982906885445118 = 0.0010885143419727683 + 0.001 * 6.894392490386963
Epoch 1510, val loss: 1.3662467002868652
Epoch 1520, training loss: 0.007988614961504936 = 0.0010779532603919506 + 0.001 * 6.910660743713379
Epoch 1520, val loss: 1.3677973747253418
Epoch 1530, training loss: 0.007998189888894558 = 0.0010676641250029206 + 0.001 * 6.930525302886963
Epoch 1530, val loss: 1.3693175315856934
Epoch 1540, training loss: 0.007957300171256065 = 0.0010576279601082206 + 0.001 * 6.899672031402588
Epoch 1540, val loss: 1.37082040309906
Epoch 1550, training loss: 0.007945171557366848 = 0.001047856523655355 + 0.001 * 6.89731502532959
Epoch 1550, val loss: 1.3722846508026123
Epoch 1560, training loss: 0.007962675765156746 = 0.0010383205953985453 + 0.001 * 6.924354553222656
Epoch 1560, val loss: 1.3738150596618652
Epoch 1570, training loss: 0.007968795485794544 = 0.0010290114441886544 + 0.001 * 6.939784049987793
Epoch 1570, val loss: 1.375248908996582
Epoch 1580, training loss: 0.00792439840734005 = 0.0010199510725215077 + 0.001 * 6.904446601867676
Epoch 1580, val loss: 1.3766555786132812
Epoch 1590, training loss: 0.007954714819788933 = 0.0010111060691997409 + 0.001 * 6.943608283996582
Epoch 1590, val loss: 1.3781168460845947
Epoch 1600, training loss: 0.007890470325946808 = 0.001002489123493433 + 0.001 * 6.887981414794922
Epoch 1600, val loss: 1.3794846534729004
Epoch 1610, training loss: 0.007896663621068 = 0.0009940647287294269 + 0.001 * 6.902598857879639
Epoch 1610, val loss: 1.380923867225647
Epoch 1620, training loss: 0.007942311465740204 = 0.000985836610198021 + 0.001 * 6.956474304199219
Epoch 1620, val loss: 1.3822541236877441
Epoch 1630, training loss: 0.007852878421545029 = 0.0009778117528185248 + 0.001 * 6.87506628036499
Epoch 1630, val loss: 1.3835700750350952
Epoch 1640, training loss: 0.007863539271056652 = 0.0009699799702502787 + 0.001 * 6.893558979034424
Epoch 1640, val loss: 1.3849284648895264
Epoch 1650, training loss: 0.007845683954656124 = 0.0009623304358683527 + 0.001 * 6.883353233337402
Epoch 1650, val loss: 1.3862451314926147
Epoch 1660, training loss: 0.007880770601332188 = 0.0009548607049509883 + 0.001 * 6.925909996032715
Epoch 1660, val loss: 1.3874908685684204
Epoch 1670, training loss: 0.007827891036868095 = 0.0009475704282522202 + 0.001 * 6.8803205490112305
Epoch 1670, val loss: 1.3888094425201416
Epoch 1680, training loss: 0.007804399821907282 = 0.0009404650190845132 + 0.001 * 6.863934516906738
Epoch 1680, val loss: 1.390128493309021
Epoch 1690, training loss: 0.007815766148269176 = 0.0009335215436294675 + 0.001 * 6.88224458694458
Epoch 1690, val loss: 1.3912866115570068
Epoch 1700, training loss: 0.007802451960742474 = 0.0009267377317883074 + 0.001 * 6.87571382522583
Epoch 1700, val loss: 1.3925855159759521
Epoch 1710, training loss: 0.007829837501049042 = 0.0009200998465530574 + 0.001 * 6.9097371101379395
Epoch 1710, val loss: 1.393802523612976
Epoch 1720, training loss: 0.007794128730893135 = 0.0009136090520769358 + 0.001 * 6.880518913269043
Epoch 1720, val loss: 1.3950251340866089
Epoch 1730, training loss: 0.007783605717122555 = 0.0009072594111785293 + 0.001 * 6.876345634460449
Epoch 1730, val loss: 1.3961310386657715
Epoch 1740, training loss: 0.007759455591440201 = 0.0009010625071823597 + 0.001 * 6.858392715454102
Epoch 1740, val loss: 1.3973689079284668
Epoch 1750, training loss: 0.007749410346150398 = 0.0008949987823143601 + 0.001 * 6.8544111251831055
Epoch 1750, val loss: 1.398566722869873
Epoch 1760, training loss: 0.007811542134732008 = 0.0008890572935342789 + 0.001 * 6.922484397888184
Epoch 1760, val loss: 1.3996721506118774
Epoch 1770, training loss: 0.007783406879752874 = 0.0008832616731524467 + 0.001 * 6.900145053863525
Epoch 1770, val loss: 1.4007911682128906
Epoch 1780, training loss: 0.007769884541630745 = 0.0008775877067819238 + 0.001 * 6.89229679107666
Epoch 1780, val loss: 1.4019241333007812
Epoch 1790, training loss: 0.007739728782325983 = 0.0008720305631868541 + 0.001 * 6.8676981925964355
Epoch 1790, val loss: 1.4030663967132568
Epoch 1800, training loss: 0.00776545749977231 = 0.0008665836649015546 + 0.001 * 6.898873805999756
Epoch 1800, val loss: 1.4041759967803955
Epoch 1810, training loss: 0.007727907970547676 = 0.0008612499223090708 + 0.001 * 6.866657733917236
Epoch 1810, val loss: 1.4052543640136719
Epoch 1820, training loss: 0.007724017836153507 = 0.0008560291607864201 + 0.001 * 6.867988109588623
Epoch 1820, val loss: 1.406309962272644
Epoch 1830, training loss: 0.0077454885467886925 = 0.0008509132312610745 + 0.001 * 6.894575119018555
Epoch 1830, val loss: 1.4073634147644043
Epoch 1840, training loss: 0.007680982351303101 = 0.0008458983502350748 + 0.001 * 6.835083484649658
Epoch 1840, val loss: 1.4084463119506836
Epoch 1850, training loss: 0.007692020386457443 = 0.0008409904548898339 + 0.001 * 6.851029872894287
Epoch 1850, val loss: 1.409531593322754
Epoch 1860, training loss: 0.007678869180381298 = 0.0008361735381186008 + 0.001 * 6.842695236206055
Epoch 1860, val loss: 1.4105559587478638
Epoch 1870, training loss: 0.007717956323176622 = 0.0008314521401189268 + 0.001 * 6.88650369644165
Epoch 1870, val loss: 1.4115701913833618
Epoch 1880, training loss: 0.007660308387130499 = 0.0008268234087154269 + 0.001 * 6.833484649658203
Epoch 1880, val loss: 1.4125641584396362
Epoch 1890, training loss: 0.007649629842489958 = 0.0008222893811762333 + 0.001 * 6.827340126037598
Epoch 1890, val loss: 1.413598656654358
Epoch 1900, training loss: 0.007657795213162899 = 0.0008178343414328992 + 0.001 * 6.83996057510376
Epoch 1900, val loss: 1.4146356582641602
Epoch 1910, training loss: 0.007701323367655277 = 0.0008134599775075912 + 0.001 * 6.8878631591796875
Epoch 1910, val loss: 1.4155676364898682
Epoch 1920, training loss: 0.007647854276001453 = 0.0008091669878922403 + 0.001 * 6.838686943054199
Epoch 1920, val loss: 1.4165871143341064
Epoch 1930, training loss: 0.007639755494892597 = 0.0008049511234275997 + 0.001 * 6.834804058074951
Epoch 1930, val loss: 1.4175331592559814
Epoch 1940, training loss: 0.0076207565143704414 = 0.0008008207660168409 + 0.001 * 6.819935321807861
Epoch 1940, val loss: 1.4185513257980347
Epoch 1950, training loss: 0.007639940362423658 = 0.0007967679994180799 + 0.001 * 6.843172073364258
Epoch 1950, val loss: 1.4194713830947876
Epoch 1960, training loss: 0.0076238480396568775 = 0.0007927985861897469 + 0.001 * 6.831048965454102
Epoch 1960, val loss: 1.4203916788101196
Epoch 1970, training loss: 0.007652633357793093 = 0.0007889002445153892 + 0.001 * 6.863732814788818
Epoch 1970, val loss: 1.421427845954895
Epoch 1980, training loss: 0.007641696371138096 = 0.000785078969784081 + 0.001 * 6.856616973876953
Epoch 1980, val loss: 1.4222317934036255
Epoch 1990, training loss: 0.007642194628715515 = 0.0007813174743205309 + 0.001 * 6.860876560211182
Epoch 1990, val loss: 1.4232535362243652
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8065366367949395
The final CL Acc:0.75926, 0.00302, The final GNN Acc:0.80917, 0.00188
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13172])
remove edge: torch.Size([2, 7896])
updated graph: torch.Size([2, 10512])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9460725784301758 = 1.9374756813049316 + 0.001 * 8.59683895111084
Epoch 0, val loss: 1.9371672868728638
Epoch 10, training loss: 1.9360586404800415 = 1.927461862564087 + 0.001 * 8.59677505493164
Epoch 10, val loss: 1.928013563156128
Epoch 20, training loss: 1.9238650798797607 = 1.9152685403823853 + 0.001 * 8.596559524536133
Epoch 20, val loss: 1.9166537523269653
Epoch 30, training loss: 1.9070439338684082 = 1.898447871208191 + 0.001 * 8.596002578735352
Epoch 30, val loss: 1.9006917476654053
Epoch 40, training loss: 1.8826874494552612 = 1.874092936515808 + 0.001 * 8.594454765319824
Epoch 40, val loss: 1.8776473999023438
Epoch 50, training loss: 1.8490699529647827 = 1.840480923652649 + 0.001 * 8.589056015014648
Epoch 50, val loss: 1.8470025062561035
Epoch 60, training loss: 1.8099286556243896 = 1.801364541053772 + 0.001 * 8.564160346984863
Epoch 60, val loss: 1.8138757944107056
Epoch 70, training loss: 1.772659182548523 = 1.7642433643341064 + 0.001 * 8.415762901306152
Epoch 70, val loss: 1.7825840711593628
Epoch 80, training loss: 1.7272807359695435 = 1.7192299365997314 + 0.001 * 8.050749778747559
Epoch 80, val loss: 1.739647388458252
Epoch 90, training loss: 1.663669466972351 = 1.6557321548461914 + 0.001 * 7.937298774719238
Epoch 90, val loss: 1.682125449180603
Epoch 100, training loss: 1.5791471004486084 = 1.571314811706543 + 0.001 * 7.832306385040283
Epoch 100, val loss: 1.6116925477981567
Epoch 110, training loss: 1.479983925819397 = 1.4723039865493774 + 0.001 * 7.6799211502075195
Epoch 110, val loss: 1.5304977893829346
Epoch 120, training loss: 1.375752568244934 = 1.368274450302124 + 0.001 * 7.478176593780518
Epoch 120, val loss: 1.4449126720428467
Epoch 130, training loss: 1.272689938545227 = 1.2652850151062012 + 0.001 * 7.404972553253174
Epoch 130, val loss: 1.3606865406036377
Epoch 140, training loss: 1.1738461256027222 = 1.1665083169937134 + 0.001 * 7.33777379989624
Epoch 140, val loss: 1.2803936004638672
Epoch 150, training loss: 1.0837006568908691 = 1.0764211416244507 + 0.001 * 7.279482841491699
Epoch 150, val loss: 1.2081973552703857
Epoch 160, training loss: 1.0057519674301147 = 0.9985149502754211 + 0.001 * 7.236957550048828
Epoch 160, val loss: 1.1477646827697754
Epoch 170, training loss: 0.9385781288146973 = 0.9313613176345825 + 0.001 * 7.2168097496032715
Epoch 170, val loss: 1.097366452217102
Epoch 180, training loss: 0.8771823048591614 = 0.8699714541435242 + 0.001 * 7.210860729217529
Epoch 180, val loss: 1.0520317554473877
Epoch 190, training loss: 0.8162680864334106 = 0.8090643882751465 + 0.001 * 7.2037177085876465
Epoch 190, val loss: 1.0067217350006104
Epoch 200, training loss: 0.7523677349090576 = 0.7451764941215515 + 0.001 * 7.191240310668945
Epoch 200, val loss: 0.958129346370697
Epoch 210, training loss: 0.6847548484802246 = 0.677582323551178 + 0.001 * 7.172521591186523
Epoch 210, val loss: 0.9066532850265503
Epoch 220, training loss: 0.6151557564735413 = 0.6080105900764465 + 0.001 * 7.145157337188721
Epoch 220, val loss: 0.854604959487915
Epoch 230, training loss: 0.5461238622665405 = 0.5390037298202515 + 0.001 * 7.1201372146606445
Epoch 230, val loss: 0.8053380846977234
Epoch 240, training loss: 0.48006388545036316 = 0.4729689061641693 + 0.001 * 7.094964504241943
Epoch 240, val loss: 0.761544942855835
Epoch 250, training loss: 0.41875505447387695 = 0.41168081760406494 + 0.001 * 7.074246883392334
Epoch 250, val loss: 0.7255369424819946
Epoch 260, training loss: 0.36329299211502075 = 0.356233149766922 + 0.001 * 7.059842586517334
Epoch 260, val loss: 0.6982683539390564
Epoch 270, training loss: 0.31415650248527527 = 0.30710598826408386 + 0.001 * 7.050521373748779
Epoch 270, val loss: 0.6792868971824646
Epoch 280, training loss: 0.27136048674583435 = 0.2643141448497772 + 0.001 * 7.0463433265686035
Epoch 280, val loss: 0.6675972938537598
Epoch 290, training loss: 0.23463337123394012 = 0.22759266197681427 + 0.001 * 7.040709495544434
Epoch 290, val loss: 0.6619356274604797
Epoch 300, training loss: 0.20350277423858643 = 0.19646726548671722 + 0.001 * 7.03550386428833
Epoch 300, val loss: 0.6613836288452148
Epoch 310, training loss: 0.17732277512550354 = 0.17029041051864624 + 0.001 * 7.032357692718506
Epoch 310, val loss: 0.6646528244018555
Epoch 320, training loss: 0.15537336468696594 = 0.14834241569042206 + 0.001 * 7.030943870544434
Epoch 320, val loss: 0.670930802822113
Epoch 330, training loss: 0.1369464099407196 = 0.12991996109485626 + 0.001 * 7.0264410972595215
Epoch 330, val loss: 0.6794326901435852
Epoch 340, training loss: 0.12142263352870941 = 0.11439760774374008 + 0.001 * 7.025022506713867
Epoch 340, val loss: 0.6895061135292053
Epoch 350, training loss: 0.10826738178730011 = 0.1012449637055397 + 0.001 * 7.0224175453186035
Epoch 350, val loss: 0.7007124423980713
Epoch 360, training loss: 0.0970509797334671 = 0.09002763032913208 + 0.001 * 7.023351669311523
Epoch 360, val loss: 0.7125176787376404
Epoch 370, training loss: 0.08741694688796997 = 0.08039466291666031 + 0.001 * 7.022284984588623
Epoch 370, val loss: 0.7246960401535034
Epoch 380, training loss: 0.07908176630735397 = 0.07206302881240845 + 0.001 * 7.018736839294434
Epoch 380, val loss: 0.7369627952575684
Epoch 390, training loss: 0.07182805240154266 = 0.06481058150529861 + 0.001 * 7.017467021942139
Epoch 390, val loss: 0.7492340803146362
Epoch 400, training loss: 0.06547947973012924 = 0.058462899178266525 + 0.001 * 7.016580581665039
Epoch 400, val loss: 0.7613304257392883
Epoch 410, training loss: 0.059894561767578125 = 0.05287918820977211 + 0.001 * 7.015372276306152
Epoch 410, val loss: 0.7732477188110352
Epoch 420, training loss: 0.05496261641383171 = 0.047948285937309265 + 0.001 * 7.014329433441162
Epoch 420, val loss: 0.7849445343017578
Epoch 430, training loss: 0.050597257912158966 = 0.04358082637190819 + 0.001 * 7.01643180847168
Epoch 430, val loss: 0.7964902520179749
Epoch 440, training loss: 0.04671439900994301 = 0.03970140218734741 + 0.001 * 7.012998104095459
Epoch 440, val loss: 0.8078081011772156
Epoch 450, training loss: 0.04325933754444122 = 0.03624759241938591 + 0.001 * 7.011744499206543
Epoch 450, val loss: 0.818951427936554
Epoch 460, training loss: 0.04017789289355278 = 0.03316722437739372 + 0.001 * 7.01066780090332
Epoch 460, val loss: 0.8299155235290527
Epoch 470, training loss: 0.037439439445734024 = 0.030416009947657585 + 0.001 * 7.0234293937683105
Epoch 470, val loss: 0.8407021760940552
Epoch 480, training loss: 0.03496328741312027 = 0.027954889461398125 + 0.001 * 7.008399486541748
Epoch 480, val loss: 0.851340651512146
Epoch 490, training loss: 0.03275743126869202 = 0.02575063705444336 + 0.001 * 7.006794452667236
Epoch 490, val loss: 0.8618043661117554
Epoch 500, training loss: 0.030781187117099762 = 0.02377389371395111 + 0.001 * 7.007293224334717
Epoch 500, val loss: 0.8721114993095398
Epoch 510, training loss: 0.029005423188209534 = 0.02199879102408886 + 0.001 * 7.006632328033447
Epoch 510, val loss: 0.8822424411773682
Epoch 520, training loss: 0.02740774117410183 = 0.020402397960424423 + 0.001 * 7.005342960357666
Epoch 520, val loss: 0.8922001719474792
Epoch 530, training loss: 0.025974730029702187 = 0.01896442100405693 + 0.001 * 7.010309219360352
Epoch 530, val loss: 0.9019246697425842
Epoch 540, training loss: 0.024670623242855072 = 0.017666766420006752 + 0.001 * 7.003857135772705
Epoch 540, val loss: 0.9114350080490112
Epoch 550, training loss: 0.02349138632416725 = 0.016493501141667366 + 0.001 * 6.9978837966918945
Epoch 550, val loss: 0.9207415580749512
Epoch 560, training loss: 0.022428665310144424 = 0.015430312603712082 + 0.001 * 6.998352527618408
Epoch 560, val loss: 0.9298549890518188
Epoch 570, training loss: 0.02147096022963524 = 0.014464895240962505 + 0.001 * 7.006063938140869
Epoch 570, val loss: 0.9387496113777161
Epoch 580, training loss: 0.02058444358408451 = 0.013586455024778843 + 0.001 * 6.997988224029541
Epoch 580, val loss: 0.9474388360977173
Epoch 590, training loss: 0.01977730542421341 = 0.012785526923835278 + 0.001 * 6.991779327392578
Epoch 590, val loss: 0.955924928188324
Epoch 600, training loss: 0.019044511020183563 = 0.012053589336574078 + 0.001 * 6.9909210205078125
Epoch 600, val loss: 0.9642196297645569
Epoch 610, training loss: 0.018386362120509148 = 0.011383364908397198 + 0.001 * 7.002997398376465
Epoch 610, val loss: 0.9723048210144043
Epoch 620, training loss: 0.01775733381509781 = 0.010768349282443523 + 0.001 * 6.988985061645508
Epoch 620, val loss: 0.9802237153053284
Epoch 630, training loss: 0.01718241535127163 = 0.010202647186815739 + 0.001 * 6.979767322540283
Epoch 630, val loss: 0.9879411458969116
Epoch 640, training loss: 0.01667090691626072 = 0.009680580347776413 + 0.001 * 6.990326404571533
Epoch 640, val loss: 0.9954982995986938
Epoch 650, training loss: 0.016182882711291313 = 0.009196026250720024 + 0.001 * 6.986855983734131
Epoch 650, val loss: 1.0028566122055054
Epoch 660, training loss: 0.01572331227362156 = 0.008743510581552982 + 0.001 * 6.979801654815674
Epoch 660, val loss: 1.010019302368164
Epoch 670, training loss: 0.015287944115698338 = 0.008319479413330555 + 0.001 * 6.968464374542236
Epoch 670, val loss: 1.0170031785964966
Epoch 680, training loss: 0.014887663535773754 = 0.007921840064227581 + 0.001 * 6.965823173522949
Epoch 680, val loss: 1.0238139629364014
Epoch 690, training loss: 0.014517227187752724 = 0.007549076806753874 + 0.001 * 6.968150615692139
Epoch 690, val loss: 1.0304230451583862
Epoch 700, training loss: 0.014161648228764534 = 0.007199941668659449 + 0.001 * 6.961705684661865
Epoch 700, val loss: 1.0368717908859253
Epoch 710, training loss: 0.013850344344973564 = 0.006873056758195162 + 0.001 * 6.977287769317627
Epoch 710, val loss: 1.0431426763534546
Epoch 720, training loss: 0.013527176342904568 = 0.0065671661868691444 + 0.001 * 6.960010051727295
Epoch 720, val loss: 1.0492452383041382
Epoch 730, training loss: 0.01323363371193409 = 0.00628088740631938 + 0.001 * 6.952746391296387
Epoch 730, val loss: 1.0552037954330444
Epoch 740, training loss: 0.012981608510017395 = 0.006012808997184038 + 0.001 * 6.968799114227295
Epoch 740, val loss: 1.060995101928711
Epoch 750, training loss: 0.012736404314637184 = 0.005761883221566677 + 0.001 * 6.974521160125732
Epoch 750, val loss: 1.066645622253418
Epoch 760, training loss: 0.012508140876889229 = 0.005526838358491659 + 0.001 * 6.981302261352539
Epoch 760, val loss: 1.0721253156661987
Epoch 770, training loss: 0.012253297492861748 = 0.005306493490934372 + 0.001 * 6.946803092956543
Epoch 770, val loss: 1.0775176286697388
Epoch 780, training loss: 0.012050097808241844 = 0.005099651403725147 + 0.001 * 6.950446128845215
Epoch 780, val loss: 1.0827635526657104
Epoch 790, training loss: 0.01186698954552412 = 0.004905314184725285 + 0.001 * 6.96167516708374
Epoch 790, val loss: 1.0878746509552002
Epoch 800, training loss: 0.011683791875839233 = 0.004722552374005318 + 0.001 * 6.961238861083984
Epoch 800, val loss: 1.0928934812545776
Epoch 810, training loss: 0.011496484279632568 = 0.004550538957118988 + 0.001 * 6.945944309234619
Epoch 810, val loss: 1.0977604389190674
Epoch 820, training loss: 0.011344284750521183 = 0.0043885051272809505 + 0.001 * 6.955779075622559
Epoch 820, val loss: 1.1025270223617554
Epoch 830, training loss: 0.01116321049630642 = 0.004235764499753714 + 0.001 * 6.927445888519287
Epoch 830, val loss: 1.1071797609329224
Epoch 840, training loss: 0.011017033830285072 = 0.004091604147106409 + 0.001 * 6.925429344177246
Epoch 840, val loss: 1.1117222309112549
Epoch 850, training loss: 0.010903710499405861 = 0.003955414053052664 + 0.001 * 6.948295593261719
Epoch 850, val loss: 1.1161510944366455
Epoch 860, training loss: 0.01075158640742302 = 0.003826627042144537 + 0.001 * 6.9249587059021
Epoch 860, val loss: 1.1204880475997925
Epoch 870, training loss: 0.010635069571435452 = 0.003704751143231988 + 0.001 * 6.9303178787231445
Epoch 870, val loss: 1.1247336864471436
Epoch 880, training loss: 0.01052091270685196 = 0.0035892832092940807 + 0.001 * 6.931629180908203
Epoch 880, val loss: 1.1288827657699585
Epoch 890, training loss: 0.010411551222205162 = 0.003479823935776949 + 0.001 * 6.931726932525635
Epoch 890, val loss: 1.132918119430542
Epoch 900, training loss: 0.010321349836885929 = 0.0033759605139493942 + 0.001 * 6.9453887939453125
Epoch 900, val loss: 1.1368916034698486
Epoch 910, training loss: 0.010198631323873997 = 0.003277322743088007 + 0.001 * 6.9213080406188965
Epoch 910, val loss: 1.140729546546936
Epoch 920, training loss: 0.01009448803961277 = 0.0031835564877837896 + 0.001 * 6.910931587219238
Epoch 920, val loss: 1.1445149183273315
Epoch 930, training loss: 0.010018737986683846 = 0.003094363259151578 + 0.001 * 6.924374580383301
Epoch 930, val loss: 1.1482127904891968
Epoch 940, training loss: 0.009916771203279495 = 0.0030094515532255173 + 0.001 * 6.90731954574585
Epoch 940, val loss: 1.1518224477767944
Epoch 950, training loss: 0.009850970469415188 = 0.002928541274741292 + 0.001 * 6.922428607940674
Epoch 950, val loss: 1.155358076095581
Epoch 960, training loss: 0.009770742617547512 = 0.002851414028555155 + 0.001 * 6.919328212738037
Epoch 960, val loss: 1.158812165260315
Epoch 970, training loss: 0.0096838204190135 = 0.0027778286021202803 + 0.001 * 6.905991554260254
Epoch 970, val loss: 1.1621992588043213
Epoch 980, training loss: 0.009655973874032497 = 0.002707547275349498 + 0.001 * 6.948426723480225
Epoch 980, val loss: 1.1655099391937256
Epoch 990, training loss: 0.009571637026965618 = 0.0026403984520584345 + 0.001 * 6.931238174438477
Epoch 990, val loss: 1.1687337160110474
Epoch 1000, training loss: 0.009474835358560085 = 0.0025761821307241917 + 0.001 * 6.898653030395508
Epoch 1000, val loss: 1.171905517578125
Epoch 1010, training loss: 0.009424594230949879 = 0.002514767227694392 + 0.001 * 6.909826278686523
Epoch 1010, val loss: 1.1749825477600098
Epoch 1020, training loss: 0.009381968528032303 = 0.0024559353478252888 + 0.001 * 6.9260334968566895
Epoch 1020, val loss: 1.1780335903167725
Epoch 1030, training loss: 0.009309070184826851 = 0.0023995796218514442 + 0.001 * 6.909489631652832
Epoch 1030, val loss: 1.1810246706008911
Epoch 1040, training loss: 0.009233389981091022 = 0.002345549175515771 + 0.001 * 6.887840747833252
Epoch 1040, val loss: 1.1839255094528198
Epoch 1050, training loss: 0.009198188781738281 = 0.0022937338799238205 + 0.001 * 6.904455184936523
Epoch 1050, val loss: 1.1867868900299072
Epoch 1060, training loss: 0.009131666272878647 = 0.0022439835593104362 + 0.001 * 6.88768196105957
Epoch 1060, val loss: 1.189589023590088
Epoch 1070, training loss: 0.009092608466744423 = 0.0021962339524179697 + 0.001 * 6.896373748779297
Epoch 1070, val loss: 1.192327618598938
Epoch 1080, training loss: 0.00905101653188467 = 0.002150380751118064 + 0.001 * 6.900635719299316
Epoch 1080, val loss: 1.1950042247772217
Epoch 1090, training loss: 0.008994613774120808 = 0.002106308238580823 + 0.001 * 6.888305187225342
Epoch 1090, val loss: 1.1976265907287598
Epoch 1100, training loss: 0.008952522650361061 = 0.002063911873847246 + 0.001 * 6.888609886169434
Epoch 1100, val loss: 1.2002016305923462
Epoch 1110, training loss: 0.008927533403038979 = 0.002023125533014536 + 0.001 * 6.904407978057861
Epoch 1110, val loss: 1.2027422189712524
Epoch 1120, training loss: 0.008899180218577385 = 0.001983861206099391 + 0.001 * 6.915318965911865
Epoch 1120, val loss: 1.2051938772201538
Epoch 1130, training loss: 0.008843014016747475 = 0.0019460339099168777 + 0.001 * 6.896979808807373
Epoch 1130, val loss: 1.207636833190918
Epoch 1140, training loss: 0.008801968768239021 = 0.0019095800817012787 + 0.001 * 6.892388820648193
Epoch 1140, val loss: 1.2100048065185547
Epoch 1150, training loss: 0.008758630603551865 = 0.0018744409317150712 + 0.001 * 6.884189128875732
Epoch 1150, val loss: 1.2123152017593384
Epoch 1160, training loss: 0.008734493516385555 = 0.0018405583687126637 + 0.001 * 6.893934726715088
Epoch 1160, val loss: 1.214612364768982
Epoch 1170, training loss: 0.008679231628775597 = 0.0018078726716339588 + 0.001 * 6.871358871459961
Epoch 1170, val loss: 1.216840147972107
Epoch 1180, training loss: 0.008647012524306774 = 0.0017763142241165042 + 0.001 * 6.87069845199585
Epoch 1180, val loss: 1.2190358638763428
Epoch 1190, training loss: 0.008614543825387955 = 0.0017458515940234065 + 0.001 * 6.868692398071289
Epoch 1190, val loss: 1.2211954593658447
Epoch 1200, training loss: 0.008605855517089367 = 0.0017164219170808792 + 0.001 * 6.88943338394165
Epoch 1200, val loss: 1.2232921123504639
Epoch 1210, training loss: 0.008573056198656559 = 0.0016879918985068798 + 0.001 * 6.885064125061035
Epoch 1210, val loss: 1.2253714799880981
Epoch 1220, training loss: 0.008542615920305252 = 0.0016605141572654247 + 0.001 * 6.882101058959961
Epoch 1220, val loss: 1.2273979187011719
Epoch 1230, training loss: 0.008528834208846092 = 0.001633940264582634 + 0.001 * 6.894893169403076
Epoch 1230, val loss: 1.2293858528137207
Epoch 1240, training loss: 0.008518524467945099 = 0.0016082137590274215 + 0.001 * 6.9103102684021
Epoch 1240, val loss: 1.2313653230667114
Epoch 1250, training loss: 0.0084436209872365 = 0.001583352335728705 + 0.001 * 6.860268592834473
Epoch 1250, val loss: 1.2332669496536255
Epoch 1260, training loss: 0.008424640633165836 = 0.0015592832351103425 + 0.001 * 6.865356922149658
Epoch 1260, val loss: 1.2351571321487427
Epoch 1270, training loss: 0.008424736559391022 = 0.0015359566314145923 + 0.001 * 6.888780117034912
Epoch 1270, val loss: 1.2370187044143677
Epoch 1280, training loss: 0.008386161178350449 = 0.0015133593697100878 + 0.001 * 6.872801303863525
Epoch 1280, val loss: 1.2388205528259277
Epoch 1290, training loss: 0.008346347138285637 = 0.001491465838626027 + 0.001 * 6.854881286621094
Epoch 1290, val loss: 1.2406021356582642
Epoch 1300, training loss: 0.008331822231411934 = 0.0014702333137392998 + 0.001 * 6.861588954925537
Epoch 1300, val loss: 1.24234139919281
Epoch 1310, training loss: 0.008334091864526272 = 0.001449640840291977 + 0.001 * 6.884450912475586
Epoch 1310, val loss: 1.2440601587295532
Epoch 1320, training loss: 0.008307783864438534 = 0.0014296888839453459 + 0.001 * 6.87809419631958
Epoch 1320, val loss: 1.2457250356674194
Epoch 1330, training loss: 0.008264419622719288 = 0.0014103164430707693 + 0.001 * 6.854102611541748
Epoch 1330, val loss: 1.2473570108413696
Epoch 1340, training loss: 0.008260371163487434 = 0.001391537836752832 + 0.001 * 6.868833065032959
Epoch 1340, val loss: 1.248931646347046
Epoch 1350, training loss: 0.008256074041128159 = 0.0013732847291976213 + 0.001 * 6.882789134979248
Epoch 1350, val loss: 1.2505195140838623
Epoch 1360, training loss: 0.008207161910831928 = 0.0013555990299209952 + 0.001 * 6.8515625
Epoch 1360, val loss: 1.2520354986190796
Epoch 1370, training loss: 0.008184662088751793 = 0.00133840914350003 + 0.001 * 6.84625244140625
Epoch 1370, val loss: 1.2535480260849
Epoch 1380, training loss: 0.008191430941224098 = 0.0013216998195275664 + 0.001 * 6.8697309494018555
Epoch 1380, val loss: 1.2550110816955566
Epoch 1390, training loss: 0.008177434094250202 = 0.001305449171923101 + 0.001 * 6.871984958648682
Epoch 1390, val loss: 1.2564661502838135
Epoch 1400, training loss: 0.008132334798574448 = 0.001289662905037403 + 0.001 * 6.842670917510986
Epoch 1400, val loss: 1.2578589916229248
Epoch 1410, training loss: 0.00812210701406002 = 0.0012743037659674883 + 0.001 * 6.847803115844727
Epoch 1410, val loss: 1.2592517137527466
Epoch 1420, training loss: 0.00811702385544777 = 0.0012593708233907819 + 0.001 * 6.85765266418457
Epoch 1420, val loss: 1.2605875730514526
Epoch 1430, training loss: 0.008093353360891342 = 0.0012448353227227926 + 0.001 * 6.848517894744873
Epoch 1430, val loss: 1.261931300163269
Epoch 1440, training loss: 0.008084987290203571 = 0.0012306866701692343 + 0.001 * 6.854300022125244
Epoch 1440, val loss: 1.263230800628662
Epoch 1450, training loss: 0.00805419310927391 = 0.001216922770254314 + 0.001 * 6.837270259857178
Epoch 1450, val loss: 1.2645151615142822
Epoch 1460, training loss: 0.008038759231567383 = 0.0012035255786031485 + 0.001 * 6.835233211517334
Epoch 1460, val loss: 1.2657756805419922
Epoch 1470, training loss: 0.008018113672733307 = 0.0011904831044375896 + 0.001 * 6.827630043029785
Epoch 1470, val loss: 1.2670164108276367
Epoch 1480, training loss: 0.008012840524315834 = 0.0011777793988585472 + 0.001 * 6.835060119628906
Epoch 1480, val loss: 1.268225073814392
Epoch 1490, training loss: 0.008017949759960175 = 0.0011653985129669309 + 0.001 * 6.852550506591797
Epoch 1490, val loss: 1.2694320678710938
Epoch 1500, training loss: 0.008012764155864716 = 0.0011533324141055346 + 0.001 * 6.859431266784668
Epoch 1500, val loss: 1.2705886363983154
Epoch 1510, training loss: 0.007989482954144478 = 0.0011415776098147035 + 0.001 * 6.847905158996582
Epoch 1510, val loss: 1.2717305421829224
Epoch 1520, training loss: 0.007962463423609734 = 0.0011301101185381413 + 0.001 * 6.832353591918945
Epoch 1520, val loss: 1.2728809118270874
Epoch 1530, training loss: 0.00793817825615406 = 0.0011189280776306987 + 0.001 * 6.819250106811523
Epoch 1530, val loss: 1.2739850282669067
Epoch 1540, training loss: 0.007936771027743816 = 0.0011080328840762377 + 0.001 * 6.828737735748291
Epoch 1540, val loss: 1.2750729322433472
Epoch 1550, training loss: 0.00793218333274126 = 0.0010974042816087604 + 0.001 * 6.834779262542725
Epoch 1550, val loss: 1.2761532068252563
Epoch 1560, training loss: 0.007924252189695835 = 0.001087025972083211 + 0.001 * 6.837225437164307
Epoch 1560, val loss: 1.2771916389465332
Epoch 1570, training loss: 0.007904051803052425 = 0.0010769085492938757 + 0.001 * 6.827142715454102
Epoch 1570, val loss: 1.2782337665557861
Epoch 1580, training loss: 0.00790636520832777 = 0.001067032921127975 + 0.001 * 6.83933162689209
Epoch 1580, val loss: 1.2792201042175293
Epoch 1590, training loss: 0.007894879207015038 = 0.0010573714971542358 + 0.001 * 6.837507724761963
Epoch 1590, val loss: 1.2802252769470215
Epoch 1600, training loss: 0.007866892032325268 = 0.0010479543125256896 + 0.001 * 6.8189377784729
Epoch 1600, val loss: 1.2811933755874634
Epoch 1610, training loss: 0.007877008989453316 = 0.0010387393413111567 + 0.001 * 6.838269233703613
Epoch 1610, val loss: 1.282152533531189
Epoch 1620, training loss: 0.007867882959544659 = 0.0010297500994056463 + 0.001 * 6.838132858276367
Epoch 1620, val loss: 1.2830810546875
Epoch 1630, training loss: 0.007889145985245705 = 0.0010209534084424376 + 0.001 * 6.868192672729492
Epoch 1630, val loss: 1.284009337425232
Epoch 1640, training loss: 0.007857180200517178 = 0.0010123811662197113 + 0.001 * 6.844798564910889
Epoch 1640, val loss: 1.2848904132843018
Epoch 1650, training loss: 0.007835941389203072 = 0.0010039883200079203 + 0.001 * 6.8319525718688965
Epoch 1650, val loss: 1.2857909202575684
Epoch 1660, training loss: 0.007817327044904232 = 0.0009957844158634543 + 0.001 * 6.821542263031006
Epoch 1660, val loss: 1.2866815328598022
Epoch 1670, training loss: 0.007808230817317963 = 0.0009877609554678202 + 0.001 * 6.820469379425049
Epoch 1670, val loss: 1.2875149250030518
Epoch 1680, training loss: 0.007814565673470497 = 0.000979915028437972 + 0.001 * 6.834650039672852
Epoch 1680, val loss: 1.2883573770523071
Epoch 1690, training loss: 0.00781550258398056 = 0.0009722473914735019 + 0.001 * 6.843255043029785
Epoch 1690, val loss: 1.2892029285430908
Epoch 1700, training loss: 0.007785205263644457 = 0.000964741746429354 + 0.001 * 6.820463180541992
Epoch 1700, val loss: 1.290002703666687
Epoch 1710, training loss: 0.007787799928337336 = 0.0009574071737006307 + 0.001 * 6.830392360687256
Epoch 1710, val loss: 1.2907923460006714
Epoch 1720, training loss: 0.007793529890477657 = 0.000950210087466985 + 0.001 * 6.843319416046143
Epoch 1720, val loss: 1.2915657758712769
Epoch 1730, training loss: 0.007764453999698162 = 0.0009431962389498949 + 0.001 * 6.8212571144104
Epoch 1730, val loss: 1.2923306226730347
Epoch 1740, training loss: 0.007749498821794987 = 0.0009363096323795617 + 0.001 * 6.813188552856445
Epoch 1740, val loss: 1.2930614948272705
Epoch 1750, training loss: 0.00773458369076252 = 0.000929567264392972 + 0.001 * 6.805016040802002
Epoch 1750, val loss: 1.2937994003295898
Epoch 1760, training loss: 0.007713225204497576 = 0.0009229608112946153 + 0.001 * 6.790264129638672
Epoch 1760, val loss: 1.2945019006729126
Epoch 1770, training loss: 0.007718916516751051 = 0.0009164908551611006 + 0.001 * 6.802425384521484
Epoch 1770, val loss: 1.2951980829238892
Epoch 1780, training loss: 0.007716859690845013 = 0.0009101477917283773 + 0.001 * 6.806711196899414
Epoch 1780, val loss: 1.2958815097808838
Epoch 1790, training loss: 0.007750208955258131 = 0.0009039309807121754 + 0.001 * 6.846277713775635
Epoch 1790, val loss: 1.2965562343597412
Epoch 1800, training loss: 0.007738709915429354 = 0.0008978428668342531 + 0.001 * 6.840866565704346
Epoch 1800, val loss: 1.2972023487091064
Epoch 1810, training loss: 0.007689754478633404 = 0.0008918915409594774 + 0.001 * 6.797863006591797
Epoch 1810, val loss: 1.2978358268737793
Epoch 1820, training loss: 0.007701387628912926 = 0.0008860446396283805 + 0.001 * 6.815342903137207
Epoch 1820, val loss: 1.2984919548034668
Epoch 1830, training loss: 0.007701801136136055 = 0.0008803296368569136 + 0.001 * 6.821470737457275
Epoch 1830, val loss: 1.299073338508606
Epoch 1840, training loss: 0.007691951934248209 = 0.0008746954845264554 + 0.001 * 6.817256450653076
Epoch 1840, val loss: 1.2996989488601685
Epoch 1850, training loss: 0.007672150153666735 = 0.0008691888069733977 + 0.001 * 6.8029608726501465
Epoch 1850, val loss: 1.3002684116363525
Epoch 1860, training loss: 0.007651442661881447 = 0.00086377770639956 + 0.001 * 6.787664413452148
Epoch 1860, val loss: 1.3008723258972168
Epoch 1870, training loss: 0.007645990699529648 = 0.0008584839524701238 + 0.001 * 6.787506580352783
Epoch 1870, val loss: 1.3014171123504639
Epoch 1880, training loss: 0.007636967580765486 = 0.0008532850188203156 + 0.001 * 6.783681869506836
Epoch 1880, val loss: 1.3019812107086182
Epoch 1890, training loss: 0.007645046804100275 = 0.0008481700206175447 + 0.001 * 6.796876430511475
Epoch 1890, val loss: 1.3025310039520264
Epoch 1900, training loss: 0.007632633671164513 = 0.0008431655005551875 + 0.001 * 6.789467811584473
Epoch 1900, val loss: 1.3030624389648438
Epoch 1910, training loss: 0.0076445238664746284 = 0.0008382327505387366 + 0.001 * 6.806290626525879
Epoch 1910, val loss: 1.30360746383667
Epoch 1920, training loss: 0.0076611400581896305 = 0.0008334098383784294 + 0.001 * 6.82772970199585
Epoch 1920, val loss: 1.3041043281555176
Epoch 1930, training loss: 0.007627793122082949 = 0.0008286619558930397 + 0.001 * 6.799130916595459
Epoch 1930, val loss: 1.3046036958694458
Epoch 1940, training loss: 0.007614308502525091 = 0.0008240212337113917 + 0.001 * 6.790287017822266
Epoch 1940, val loss: 1.3050905466079712
Epoch 1950, training loss: 0.0077137392945587635 = 0.0008194500696845353 + 0.001 * 6.894288539886475
Epoch 1950, val loss: 1.3055707216262817
Epoch 1960, training loss: 0.0076347882859408855 = 0.0008149657514877617 + 0.001 * 6.819821834564209
Epoch 1960, val loss: 1.306046485900879
Epoch 1970, training loss: 0.007609378546476364 = 0.0008105466840788722 + 0.001 * 6.798831462860107
Epoch 1970, val loss: 1.306480050086975
Epoch 1980, training loss: 0.007580379489809275 = 0.0008062056149356067 + 0.001 * 6.774173736572266
Epoch 1980, val loss: 1.3069413900375366
Epoch 1990, training loss: 0.007577703800052404 = 0.0008019409142434597 + 0.001 * 6.775762557983398
Epoch 1990, val loss: 1.3074015378952026
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 1.9453907012939453 = 1.9367939233779907 + 0.001 * 8.596837043762207
Epoch 0, val loss: 1.924621820449829
Epoch 10, training loss: 1.935128092765808 = 1.9265313148498535 + 0.001 * 8.596771240234375
Epoch 10, val loss: 1.914366364479065
Epoch 20, training loss: 1.9221975803375244 = 1.913601040840149 + 0.001 * 8.596517562866211
Epoch 20, val loss: 1.9010002613067627
Epoch 30, training loss: 1.9036674499511719 = 1.8950716257095337 + 0.001 * 8.595824241638184
Epoch 30, val loss: 1.881614327430725
Epoch 40, training loss: 1.876531720161438 = 1.8679378032684326 + 0.001 * 8.593860626220703
Epoch 40, val loss: 1.8537979125976562
Epoch 50, training loss: 1.8409130573272705 = 1.8323255777359009 + 0.001 * 8.58751392364502
Epoch 50, val loss: 1.819985270500183
Epoch 60, training loss: 1.80519700050354 = 1.7966344356536865 + 0.001 * 8.562582969665527
Epoch 60, val loss: 1.7915397882461548
Epoch 70, training loss: 1.7689249515533447 = 1.7604957818984985 + 0.001 * 8.429163932800293
Epoch 70, val loss: 1.7639020681381226
Epoch 80, training loss: 1.7187741994857788 = 1.7107206583023071 + 0.001 * 8.053484916687012
Epoch 80, val loss: 1.721675992012024
Epoch 90, training loss: 1.6498401165008545 = 1.6418273448944092 + 0.001 * 8.012724876403809
Epoch 90, val loss: 1.6615447998046875
Epoch 100, training loss: 1.5655606985092163 = 1.5575737953186035 + 0.001 * 7.986847877502441
Epoch 100, val loss: 1.5897884368896484
Epoch 110, training loss: 1.4796051979064941 = 1.4716728925704956 + 0.001 * 7.932305335998535
Epoch 110, val loss: 1.5205914974212646
Epoch 120, training loss: 1.3959335088729858 = 1.3881767988204956 + 0.001 * 7.756724834442139
Epoch 120, val loss: 1.4574323892593384
Epoch 130, training loss: 1.3133423328399658 = 1.3058290481567383 + 0.001 * 7.513279438018799
Epoch 130, val loss: 1.398358702659607
Epoch 140, training loss: 1.230804443359375 = 1.22331702709198 + 0.001 * 7.487465858459473
Epoch 140, val loss: 1.3406323194503784
Epoch 150, training loss: 1.1486839056015015 = 1.1412653923034668 + 0.001 * 7.418468475341797
Epoch 150, val loss: 1.2837653160095215
Epoch 160, training loss: 1.0690758228302002 = 1.061685562133789 + 0.001 * 7.390233993530273
Epoch 160, val loss: 1.2293466329574585
Epoch 170, training loss: 0.9929114580154419 = 0.9855449199676514 + 0.001 * 7.366508483886719
Epoch 170, val loss: 1.1776831150054932
Epoch 180, training loss: 0.9198367595672607 = 0.9125006198883057 + 0.001 * 7.336167335510254
Epoch 180, val loss: 1.1277884244918823
Epoch 190, training loss: 0.8490463495254517 = 0.8417493104934692 + 0.001 * 7.297020435333252
Epoch 190, val loss: 1.07893705368042
Epoch 200, training loss: 0.7806628942489624 = 0.7733980417251587 + 0.001 * 7.264847278594971
Epoch 200, val loss: 1.0311208963394165
Epoch 210, training loss: 0.7160953283309937 = 0.7088419198989868 + 0.001 * 7.253426551818848
Epoch 210, val loss: 0.9865156412124634
Epoch 220, training loss: 0.6562002897262573 = 0.648949146270752 + 0.001 * 7.251140117645264
Epoch 220, val loss: 0.9465721845626831
Epoch 230, training loss: 0.6004639267921448 = 0.5932177305221558 + 0.001 * 7.246223449707031
Epoch 230, val loss: 0.9124826788902283
Epoch 240, training loss: 0.5477044582366943 = 0.5404655337333679 + 0.001 * 7.2389373779296875
Epoch 240, val loss: 0.8841358423233032
Epoch 250, training loss: 0.49678221344947815 = 0.48955169320106506 + 0.001 * 7.2305097579956055
Epoch 250, val loss: 0.8610162138938904
Epoch 260, training loss: 0.4470440149307251 = 0.43982061743736267 + 0.001 * 7.223388671875
Epoch 260, val loss: 0.842378556728363
Epoch 270, training loss: 0.3985934853553772 = 0.39138439297676086 + 0.001 * 7.2090911865234375
Epoch 270, val loss: 0.8280068039894104
Epoch 280, training loss: 0.3526870012283325 = 0.34549543261528015 + 0.001 * 7.191555976867676
Epoch 280, val loss: 0.8181357979774475
Epoch 290, training loss: 0.3109053373336792 = 0.3037208914756775 + 0.001 * 7.184459209442139
Epoch 290, val loss: 0.8132153153419495
Epoch 300, training loss: 0.2739960849285126 = 0.26684656739234924 + 0.001 * 7.149507522583008
Epoch 300, val loss: 0.8133058547973633
Epoch 310, training loss: 0.2418738454580307 = 0.23474904894828796 + 0.001 * 7.124800682067871
Epoch 310, val loss: 0.8179510831832886
Epoch 320, training loss: 0.2140754610300064 = 0.20697236061096191 + 0.001 * 7.103106498718262
Epoch 320, val loss: 0.8262768983840942
Epoch 330, training loss: 0.19004718959331512 = 0.18295753002166748 + 0.001 * 7.089662551879883
Epoch 330, val loss: 0.837310791015625
Epoch 340, training loss: 0.1692010909318924 = 0.1621302366256714 + 0.001 * 7.070857524871826
Epoch 340, val loss: 0.8504711389541626
Epoch 350, training loss: 0.15102998912334442 = 0.14396724104881287 + 0.001 * 7.062743663787842
Epoch 350, val loss: 0.8650198578834534
Epoch 360, training loss: 0.13510239124298096 = 0.128041073679924 + 0.001 * 7.06131649017334
Epoch 360, val loss: 0.8805414438247681
Epoch 370, training loss: 0.12107657641172409 = 0.11402300745248795 + 0.001 * 7.053569316864014
Epoch 370, val loss: 0.8967723846435547
Epoch 380, training loss: 0.1087215393781662 = 0.10165971517562866 + 0.001 * 7.0618205070495605
Epoch 380, val loss: 0.9134104251861572
Epoch 390, training loss: 0.09780322760343552 = 0.09075196087360382 + 0.001 * 7.051268100738525
Epoch 390, val loss: 0.9303849935531616
Epoch 400, training loss: 0.08817937225103378 = 0.08113288134336472 + 0.001 * 7.0464935302734375
Epoch 400, val loss: 0.9474923014640808
Epoch 410, training loss: 0.07970631867647171 = 0.07266304641962051 + 0.001 * 7.043274402618408
Epoch 410, val loss: 0.9645556211471558
Epoch 420, training loss: 0.07227155566215515 = 0.065211720764637 + 0.001 * 7.0598368644714355
Epoch 420, val loss: 0.9815508723258972
Epoch 430, training loss: 0.06570246815681458 = 0.05865880101919174 + 0.001 * 7.043669700622559
Epoch 430, val loss: 0.9983945488929749
Epoch 440, training loss: 0.0599270798265934 = 0.05289172753691673 + 0.001 * 7.035350799560547
Epoch 440, val loss: 1.0150296688079834
Epoch 450, training loss: 0.05484519526362419 = 0.04781269282102585 + 0.001 * 7.032501220703125
Epoch 450, val loss: 1.0313715934753418
Epoch 460, training loss: 0.05038289353251457 = 0.04333411157131195 + 0.001 * 7.048781871795654
Epoch 460, val loss: 1.04732346534729
Epoch 470, training loss: 0.04640587419271469 = 0.03937757760286331 + 0.001 * 7.028295040130615
Epoch 470, val loss: 1.0629092454910278
Epoch 480, training loss: 0.0429038405418396 = 0.03587503731250763 + 0.001 * 7.028804779052734
Epoch 480, val loss: 1.0781000852584839
Epoch 490, training loss: 0.03979167342185974 = 0.03276776894927025 + 0.001 * 7.023905277252197
Epoch 490, val loss: 1.0928651094436646
Epoch 500, training loss: 0.03702801093459129 = 0.030006064102053642 + 0.001 * 7.021945953369141
Epoch 500, val loss: 1.1072107553482056
Epoch 510, training loss: 0.03456694632768631 = 0.027547147125005722 + 0.001 * 7.01979923248291
Epoch 510, val loss: 1.1211843490600586
Epoch 520, training loss: 0.03237133473157883 = 0.025353441014885902 + 0.001 * 7.017893314361572
Epoch 520, val loss: 1.134742259979248
Epoch 530, training loss: 0.030414067208766937 = 0.02339252270758152 + 0.001 * 7.021543502807617
Epoch 530, val loss: 1.1479380130767822
Epoch 540, training loss: 0.028653932735323906 = 0.021636130288243294 + 0.001 * 7.0178022384643555
Epoch 540, val loss: 1.1607218980789185
Epoch 550, training loss: 0.027078110724687576 = 0.020059822127223015 + 0.001 * 7.018287658691406
Epoch 550, val loss: 1.1731477975845337
Epoch 560, training loss: 0.02565949037671089 = 0.018642378970980644 + 0.001 * 7.017111301422119
Epoch 560, val loss: 1.1851733922958374
Epoch 570, training loss: 0.024381915107369423 = 0.017365077510476112 + 0.001 * 7.016837120056152
Epoch 570, val loss: 1.196842074394226
Epoch 580, training loss: 0.023221544921398163 = 0.016211478039622307 + 0.001 * 7.010067462921143
Epoch 580, val loss: 1.2081587314605713
Epoch 590, training loss: 0.022177129983901978 = 0.015167204663157463 + 0.001 * 7.009925365447998
Epoch 590, val loss: 1.2191412448883057
Epoch 600, training loss: 0.021235667169094086 = 0.014219723641872406 + 0.001 * 7.0159430503845215
Epoch 600, val loss: 1.2297736406326294
Epoch 610, training loss: 0.020368164405226707 = 0.013358156196773052 + 0.001 * 7.010007858276367
Epoch 610, val loss: 1.240126609802246
Epoch 620, training loss: 0.019577816128730774 = 0.01257307454943657 + 0.001 * 7.004741191864014
Epoch 620, val loss: 1.2501428127288818
Epoch 630, training loss: 0.01886521652340889 = 0.011856062337756157 + 0.001 * 7.009153842926025
Epoch 630, val loss: 1.2598687410354614
Epoch 640, training loss: 0.018202276900410652 = 0.011199740692973137 + 0.001 * 7.002536296844482
Epoch 640, val loss: 1.2693077325820923
Epoch 650, training loss: 0.0175996795296669 = 0.01059770304709673 + 0.001 * 7.00197696685791
Epoch 650, val loss: 1.278451681137085
Epoch 660, training loss: 0.017045967280864716 = 0.010044317692518234 + 0.001 * 7.001648426055908
Epoch 660, val loss: 1.2873494625091553
Epoch 670, training loss: 0.016543688252568245 = 0.009534596465528011 + 0.001 * 7.009091854095459
Epoch 670, val loss: 1.2960110902786255
Epoch 680, training loss: 0.016063503921031952 = 0.009064167737960815 + 0.001 * 6.999334812164307
Epoch 680, val loss: 1.3044315576553345
Epoch 690, training loss: 0.01563333347439766 = 0.008629151619970798 + 0.001 * 7.004181385040283
Epoch 690, val loss: 1.3126063346862793
Epoch 700, training loss: 0.015240412205457687 = 0.008226134814321995 + 0.001 * 7.014277458190918
Epoch 700, val loss: 1.3205626010894775
Epoch 710, training loss: 0.014846212230622768 = 0.007852156646549702 + 0.001 * 6.994055271148682
Epoch 710, val loss: 1.328307032585144
Epoch 720, training loss: 0.014500552788376808 = 0.007504496723413467 + 0.001 * 6.996056079864502
Epoch 720, val loss: 1.3358395099639893
Epoch 730, training loss: 0.014200814068317413 = 0.007180753163993359 + 0.001 * 7.0200605392456055
Epoch 730, val loss: 1.343163251876831
Epoch 740, training loss: 0.013873210176825523 = 0.006878833286464214 + 0.001 * 6.994376182556152
Epoch 740, val loss: 1.3502956628799438
Epoch 750, training loss: 0.013587728142738342 = 0.006596833001822233 + 0.001 * 6.990894794464111
Epoch 750, val loss: 1.357246994972229
Epoch 760, training loss: 0.013323202729225159 = 0.00633303401991725 + 0.001 * 6.99016809463501
Epoch 760, val loss: 1.364020824432373
Epoch 770, training loss: 0.013088185340166092 = 0.006085903849452734 + 0.001 * 7.002281665802002
Epoch 770, val loss: 1.3706004619598389
Epoch 780, training loss: 0.012845240533351898 = 0.0058540888130664825 + 0.001 * 6.991151809692383
Epoch 780, val loss: 1.3770442008972168
Epoch 790, training loss: 0.012640981003642082 = 0.005636375397443771 + 0.001 * 7.004604816436768
Epoch 790, val loss: 1.383301854133606
Epoch 800, training loss: 0.012427803128957748 = 0.005431631579995155 + 0.001 * 6.996171474456787
Epoch 800, val loss: 1.3894094228744507
Epoch 810, training loss: 0.0122278593480587 = 0.0052388496696949005 + 0.001 * 6.989009380340576
Epoch 810, val loss: 1.3953896760940552
Epoch 820, training loss: 0.012047307565808296 = 0.0050571090541779995 + 0.001 * 6.990198135375977
Epoch 820, val loss: 1.4011969566345215
Epoch 830, training loss: 0.011867349036037922 = 0.004885578993707895 + 0.001 * 6.981769561767578
Epoch 830, val loss: 1.4068877696990967
Epoch 840, training loss: 0.011712566018104553 = 0.004723509307950735 + 0.001 * 6.9890570640563965
Epoch 840, val loss: 1.4124293327331543
Epoch 850, training loss: 0.011553911492228508 = 0.0045702154748141766 + 0.001 * 6.983695983886719
Epoch 850, val loss: 1.4178467988967896
Epoch 860, training loss: 0.011418062262237072 = 0.004425094462931156 + 0.001 * 6.99296760559082
Epoch 860, val loss: 1.423125147819519
Epoch 870, training loss: 0.011269068345427513 = 0.004287551622837782 + 0.001 * 6.981515884399414
Epoch 870, val loss: 1.4283015727996826
Epoch 880, training loss: 0.011149510741233826 = 0.004157095681875944 + 0.001 * 6.992414474487305
Epoch 880, val loss: 1.4333455562591553
Epoch 890, training loss: 0.011017747223377228 = 0.004033232107758522 + 0.001 * 6.9845147132873535
Epoch 890, val loss: 1.4382808208465576
Epoch 900, training loss: 0.010891273617744446 = 0.003915536217391491 + 0.001 * 6.97573709487915
Epoch 900, val loss: 1.4430999755859375
Epoch 910, training loss: 0.010781290009617805 = 0.0038035947363823652 + 0.001 * 6.977694511413574
Epoch 910, val loss: 1.447831630706787
Epoch 920, training loss: 0.010670640505850315 = 0.0036970365326851606 + 0.001 * 6.97360372543335
Epoch 920, val loss: 1.4524357318878174
Epoch 930, training loss: 0.010569091886281967 = 0.0035955191124230623 + 0.001 * 6.973572254180908
Epoch 930, val loss: 1.4569414854049683
Epoch 940, training loss: 0.0104791559278965 = 0.0034987388644367456 + 0.001 * 6.980416774749756
Epoch 940, val loss: 1.4613566398620605
Epoch 950, training loss: 0.010387563146650791 = 0.003406399628147483 + 0.001 * 6.981163024902344
Epoch 950, val loss: 1.4656747579574585
Epoch 960, training loss: 0.010293979197740555 = 0.003318232949823141 + 0.001 * 6.975745677947998
Epoch 960, val loss: 1.4698981046676636
Epoch 970, training loss: 0.01021319068968296 = 0.003233998315408826 + 0.001 * 6.979191780090332
Epoch 970, val loss: 1.4740393161773682
Epoch 980, training loss: 0.010122501291334629 = 0.003153464524075389 + 0.001 * 6.969036102294922
Epoch 980, val loss: 1.4780818223953247
Epoch 990, training loss: 0.010043871589004993 = 0.0030764094553887844 + 0.001 * 6.967461585998535
Epoch 990, val loss: 1.4820393323898315
Epoch 1000, training loss: 0.01001239288598299 = 0.003002638230100274 + 0.001 * 7.009754180908203
Epoch 1000, val loss: 1.4859150648117065
Epoch 1010, training loss: 0.009896923787891865 = 0.002931957133114338 + 0.001 * 6.964966297149658
Epoch 1010, val loss: 1.4897149801254272
Epoch 1020, training loss: 0.009830758906900883 = 0.0028642204124480486 + 0.001 * 6.966538429260254
Epoch 1020, val loss: 1.4934245347976685
Epoch 1030, training loss: 0.009775841608643532 = 0.002799240406602621 + 0.001 * 6.9766011238098145
Epoch 1030, val loss: 1.4970768690109253
Epoch 1040, training loss: 0.009699897840619087 = 0.0027368715964257717 + 0.001 * 6.96302604675293
Epoch 1040, val loss: 1.5006517171859741
Epoch 1050, training loss: 0.009639204479753971 = 0.0026769915129989386 + 0.001 * 6.962212562561035
Epoch 1050, val loss: 1.5041202306747437
Epoch 1060, training loss: 0.009593029506504536 = 0.0026194502133876085 + 0.001 * 6.973579406738281
Epoch 1060, val loss: 1.507552146911621
Epoch 1070, training loss: 0.009523997083306313 = 0.002564135007560253 + 0.001 * 6.959861755371094
Epoch 1070, val loss: 1.5108997821807861
Epoch 1080, training loss: 0.009489333257079124 = 0.0025109495036303997 + 0.001 * 6.978384017944336
Epoch 1080, val loss: 1.514166235923767
Epoch 1090, training loss: 0.009426859207451344 = 0.002459763316437602 + 0.001 * 6.967095375061035
Epoch 1090, val loss: 1.5174187421798706
Epoch 1100, training loss: 0.0093690799549222 = 0.0024104879703372717 + 0.001 * 6.958591938018799
Epoch 1100, val loss: 1.5205438137054443
Epoch 1110, training loss: 0.009321304969489574 = 0.0023630138020962477 + 0.001 * 6.9582905769348145
Epoch 1110, val loss: 1.5236248970031738
Epoch 1120, training loss: 0.009279378689825535 = 0.0023172793444246054 + 0.001 * 6.962099075317383
Epoch 1120, val loss: 1.526684284210205
Epoch 1130, training loss: 0.009237866848707199 = 0.0022731851786375046 + 0.001 * 6.964681148529053
Epoch 1130, val loss: 1.529624104499817
Epoch 1140, training loss: 0.0091922702267766 = 0.0022306570317596197 + 0.001 * 6.961613178253174
Epoch 1140, val loss: 1.5325477123260498
Epoch 1150, training loss: 0.009140027686953545 = 0.00218963879160583 + 0.001 * 6.950388431549072
Epoch 1150, val loss: 1.5353935956954956
Epoch 1160, training loss: 0.00911340955644846 = 0.0021500398870557547 + 0.001 * 6.963369369506836
Epoch 1160, val loss: 1.5381885766983032
Epoch 1170, training loss: 0.009064335376024246 = 0.0021117916330695152 + 0.001 * 6.95254373550415
Epoch 1170, val loss: 1.5409185886383057
Epoch 1180, training loss: 0.009017523378133774 = 0.0020748451352119446 + 0.001 * 6.942678451538086
Epoch 1180, val loss: 1.5436227321624756
Epoch 1190, training loss: 0.0090089226141572 = 0.0020391379948705435 + 0.001 * 6.969784259796143
Epoch 1190, val loss: 1.546242594718933
Epoch 1200, training loss: 0.008958341553807259 = 0.002004622481763363 + 0.001 * 6.953718662261963
Epoch 1200, val loss: 1.5488401651382446
Epoch 1210, training loss: 0.00891704298555851 = 0.0019712415523827076 + 0.001 * 6.945801258087158
Epoch 1210, val loss: 1.551353096961975
Epoch 1220, training loss: 0.00888669490814209 = 0.0019389368826523423 + 0.001 * 6.947757244110107
Epoch 1220, val loss: 1.5538517236709595
Epoch 1230, training loss: 0.00884978100657463 = 0.0019076736643910408 + 0.001 * 6.942107200622559
Epoch 1230, val loss: 1.5562807321548462
Epoch 1240, training loss: 0.008823197335004807 = 0.0018774154596030712 + 0.001 * 6.945781707763672
Epoch 1240, val loss: 1.558664321899414
Epoch 1250, training loss: 0.008807794190943241 = 0.0018480998696759343 + 0.001 * 6.959693908691406
Epoch 1250, val loss: 1.5610113143920898
Epoch 1260, training loss: 0.00877414271235466 = 0.001819712109863758 + 0.001 * 6.954430103302002
Epoch 1260, val loss: 1.5633186101913452
Epoch 1270, training loss: 0.008737406693398952 = 0.0017921922262758017 + 0.001 * 6.94521427154541
Epoch 1270, val loss: 1.5655794143676758
Epoch 1280, training loss: 0.008710692636668682 = 0.0017655051779001951 + 0.001 * 6.945187568664551
Epoch 1280, val loss: 1.567805528640747
Epoch 1290, training loss: 0.00868330616503954 = 0.0017396261682733893 + 0.001 * 6.943679332733154
Epoch 1290, val loss: 1.5699706077575684
Epoch 1300, training loss: 0.008665397763252258 = 0.0017145228339359164 + 0.001 * 6.9508748054504395
Epoch 1300, val loss: 1.5721068382263184
Epoch 1310, training loss: 0.00862678699195385 = 0.001690163742750883 + 0.001 * 6.936623573303223
Epoch 1310, val loss: 1.5741995573043823
Epoch 1320, training loss: 0.008611011318862438 = 0.00166650942992419 + 0.001 * 6.944501876831055
Epoch 1320, val loss: 1.5762451887130737
Epoch 1330, training loss: 0.008574073202908039 = 0.001643541851080954 + 0.001 * 6.930530548095703
Epoch 1330, val loss: 1.5782486200332642
Epoch 1340, training loss: 0.008551038801670074 = 0.0016212445916607976 + 0.001 * 6.929793357849121
Epoch 1340, val loss: 1.5802174806594849
Epoch 1350, training loss: 0.00853025447577238 = 0.0015995786525309086 + 0.001 * 6.930675506591797
Epoch 1350, val loss: 1.5821506977081299
Epoch 1360, training loss: 0.008531211875379086 = 0.0015785255236551166 + 0.001 * 6.952686309814453
Epoch 1360, val loss: 1.5840555429458618
Epoch 1370, training loss: 0.008488230407238007 = 0.00155805260874331 + 0.001 * 6.930177688598633
Epoch 1370, val loss: 1.5859071016311646
Epoch 1380, training loss: 0.008460096083581448 = 0.0015381497796624899 + 0.001 * 6.921945571899414
Epoch 1380, val loss: 1.5877346992492676
Epoch 1390, training loss: 0.008461268618702888 = 0.0015188069082796574 + 0.001 * 6.9424614906311035
Epoch 1390, val loss: 1.5895415544509888
Epoch 1400, training loss: 0.008420359343290329 = 0.0014999804552644491 + 0.001 * 6.9203782081604
Epoch 1400, val loss: 1.591263771057129
Epoch 1410, training loss: 0.008402195759117603 = 0.0014816608745604753 + 0.001 * 6.920534610748291
Epoch 1410, val loss: 1.592954397201538
Epoch 1420, training loss: 0.008393857628107071 = 0.0014638400170952082 + 0.001 * 6.930017471313477
Epoch 1420, val loss: 1.5946418046951294
Epoch 1430, training loss: 0.00836723018437624 = 0.0014464861014857888 + 0.001 * 6.920743942260742
Epoch 1430, val loss: 1.5962687730789185
Epoch 1440, training loss: 0.008354529738426208 = 0.0014295961009338498 + 0.001 * 6.924932956695557
Epoch 1440, val loss: 1.5978875160217285
Epoch 1450, training loss: 0.008333906531333923 = 0.001413151971064508 + 0.001 * 6.9207539558410645
Epoch 1450, val loss: 1.5994865894317627
Epoch 1460, training loss: 0.008322294801473618 = 0.001397122978232801 + 0.001 * 6.925171852111816
Epoch 1460, val loss: 1.6010010242462158
Epoch 1470, training loss: 0.008326595649123192 = 0.0013815085403621197 + 0.001 * 6.945087432861328
Epoch 1470, val loss: 1.6025291681289673
Epoch 1480, training loss: 0.008308999240398407 = 0.0013662980636581779 + 0.001 * 6.94270133972168
Epoch 1480, val loss: 1.6040289402008057
Epoch 1490, training loss: 0.008287379518151283 = 0.0013514617457985878 + 0.001 * 6.935916900634766
Epoch 1490, val loss: 1.6054757833480835
Epoch 1500, training loss: 0.00824929028749466 = 0.0013369946973398328 + 0.001 * 6.912295341491699
Epoch 1500, val loss: 1.6068687438964844
Epoch 1510, training loss: 0.008243928663432598 = 0.0013228891184553504 + 0.001 * 6.921039581298828
Epoch 1510, val loss: 1.608277678489685
Epoch 1520, training loss: 0.008223064243793488 = 0.0013091336004436016 + 0.001 * 6.913930416107178
Epoch 1520, val loss: 1.609629511833191
Epoch 1530, training loss: 0.008206909522414207 = 0.0012957083526998758 + 0.001 * 6.911200523376465
Epoch 1530, val loss: 1.6109832525253296
Epoch 1540, training loss: 0.008221287280321121 = 0.0012826219899579883 + 0.001 * 6.938665390014648
Epoch 1540, val loss: 1.6122905015945435
Epoch 1550, training loss: 0.008178905583918095 = 0.0012698324862867594 + 0.001 * 6.9090728759765625
Epoch 1550, val loss: 1.613561987876892
Epoch 1560, training loss: 0.008173910900950432 = 0.0012573602143675089 + 0.001 * 6.916550636291504
Epoch 1560, val loss: 1.614807367324829
Epoch 1570, training loss: 0.008143224753439426 = 0.0012451809598132968 + 0.001 * 6.898043632507324
Epoch 1570, val loss: 1.616020679473877
Epoch 1580, training loss: 0.008129422552883625 = 0.001233282033354044 + 0.001 * 6.896140098571777
Epoch 1580, val loss: 1.6172246932983398
Epoch 1590, training loss: 0.008173773065209389 = 0.0012216747272759676 + 0.001 * 6.9520978927612305
Epoch 1590, val loss: 1.6184319257736206
Epoch 1600, training loss: 0.008106209337711334 = 0.0012103280751034617 + 0.001 * 6.895881175994873
Epoch 1600, val loss: 1.6195621490478516
Epoch 1610, training loss: 0.008102494291961193 = 0.0011992445215582848 + 0.001 * 6.903249263763428
Epoch 1610, val loss: 1.620704174041748
Epoch 1620, training loss: 0.008080625906586647 = 0.0011884077684953809 + 0.001 * 6.892218112945557
Epoch 1620, val loss: 1.6217906475067139
Epoch 1630, training loss: 0.008098606020212173 = 0.0011778376065194607 + 0.001 * 6.9207682609558105
Epoch 1630, val loss: 1.6228702068328857
Epoch 1640, training loss: 0.008073595352470875 = 0.0011674808338284492 + 0.001 * 6.906114101409912
Epoch 1640, val loss: 1.623909831047058
Epoch 1650, training loss: 0.008077934384346008 = 0.0011573605006560683 + 0.001 * 6.920573711395264
Epoch 1650, val loss: 1.6249349117279053
Epoch 1660, training loss: 0.008037608116865158 = 0.0011474592611193657 + 0.001 * 6.890148639678955
Epoch 1660, val loss: 1.6259615421295166
Epoch 1670, training loss: 0.008046944625675678 = 0.001137775951065123 + 0.001 * 6.909168243408203
Epoch 1670, val loss: 1.626963496208191
Epoch 1680, training loss: 0.008018073625862598 = 0.0011283035855740309 + 0.001 * 6.889769554138184
Epoch 1680, val loss: 1.6279022693634033
Epoch 1690, training loss: 0.008037001825869083 = 0.001119030057452619 + 0.001 * 6.917971611022949
Epoch 1690, val loss: 1.628844976425171
Epoch 1700, training loss: 0.008021491579711437 = 0.001109970617108047 + 0.001 * 6.911520481109619
Epoch 1700, val loss: 1.6298059225082397
Epoch 1710, training loss: 0.007991920225322247 = 0.001101084752008319 + 0.001 * 6.890834808349609
Epoch 1710, val loss: 1.6306999921798706
Epoch 1720, training loss: 0.007982593961060047 = 0.0010924217058345675 + 0.001 * 6.890172004699707
Epoch 1720, val loss: 1.6315829753875732
Epoch 1730, training loss: 0.007964522577822208 = 0.0010839112801477313 + 0.001 * 6.880610942840576
Epoch 1730, val loss: 1.6324868202209473
Epoch 1740, training loss: 0.007967031560838223 = 0.0010755795519798994 + 0.001 * 6.891451358795166
Epoch 1740, val loss: 1.6333361864089966
Epoch 1750, training loss: 0.007999889552593231 = 0.001067437813617289 + 0.001 * 6.932451248168945
Epoch 1750, val loss: 1.6341363191604614
Epoch 1760, training loss: 0.007967681623995304 = 0.0010594538180157542 + 0.001 * 6.908227443695068
Epoch 1760, val loss: 1.6349589824676514
Epoch 1770, training loss: 0.007935547269880772 = 0.0010516226757317781 + 0.001 * 6.88392448425293
Epoch 1770, val loss: 1.6357749700546265
Epoch 1780, training loss: 0.00791992899030447 = 0.0010439599864184856 + 0.001 * 6.8759684562683105
Epoch 1780, val loss: 1.636541724205017
Epoch 1790, training loss: 0.007907749153673649 = 0.0010364747140556574 + 0.001 * 6.871273994445801
Epoch 1790, val loss: 1.6373307704925537
Epoch 1800, training loss: 0.007921334356069565 = 0.0010290953796356916 + 0.001 * 6.892239093780518
Epoch 1800, val loss: 1.638059139251709
Epoch 1810, training loss: 0.007905028760433197 = 0.0010218933457508683 + 0.001 * 6.883134841918945
Epoch 1810, val loss: 1.6388027667999268
Epoch 1820, training loss: 0.007911867462098598 = 0.0010148201836273074 + 0.001 * 6.89704704284668
Epoch 1820, val loss: 1.6394940614700317
Epoch 1830, training loss: 0.007923639379441738 = 0.0010079059284180403 + 0.001 * 6.9157328605651855
Epoch 1830, val loss: 1.6402171850204468
Epoch 1840, training loss: 0.007862331345677376 = 0.0010010912083089352 + 0.001 * 6.861239910125732
Epoch 1840, val loss: 1.6408805847167969
Epoch 1850, training loss: 0.007861838676035404 = 0.00099442177452147 + 0.001 * 6.867416858673096
Epoch 1850, val loss: 1.6415780782699585
Epoch 1860, training loss: 0.007855076342821121 = 0.0009878816781565547 + 0.001 * 6.867194652557373
Epoch 1860, val loss: 1.642196536064148
Epoch 1870, training loss: 0.007877431809902191 = 0.0009814571822062135 + 0.001 * 6.895974159240723
Epoch 1870, val loss: 1.6428961753845215
Epoch 1880, training loss: 0.0078515550121665 = 0.0009751581237651408 + 0.001 * 6.876396179199219
Epoch 1880, val loss: 1.6434800624847412
Epoch 1890, training loss: 0.007844682782888412 = 0.0009689772850833833 + 0.001 * 6.875705242156982
Epoch 1890, val loss: 1.6441069841384888
Epoch 1900, training loss: 0.00785725750029087 = 0.0009628973784856498 + 0.001 * 6.894359588623047
Epoch 1900, val loss: 1.6446959972381592
Epoch 1910, training loss: 0.007869850844144821 = 0.0009569376707077026 + 0.001 * 6.9129133224487305
Epoch 1910, val loss: 1.645346999168396
Epoch 1920, training loss: 0.007805804722011089 = 0.0009510782547295094 + 0.001 * 6.854726314544678
Epoch 1920, val loss: 1.645880103111267
Epoch 1930, training loss: 0.0077968500554561615 = 0.0009453479433432221 + 0.001 * 6.851501941680908
Epoch 1930, val loss: 1.6464591026306152
Epoch 1940, training loss: 0.007803315296769142 = 0.0009397207759320736 + 0.001 * 6.863594055175781
Epoch 1940, val loss: 1.6469908952713013
Epoch 1950, training loss: 0.00781853124499321 = 0.0009341716067865491 + 0.001 * 6.884358882904053
Epoch 1950, val loss: 1.647574782371521
Epoch 1960, training loss: 0.007783460896462202 = 0.0009287061402574182 + 0.001 * 6.854754447937012
Epoch 1960, val loss: 1.6480475664138794
Epoch 1970, training loss: 0.007771429605782032 = 0.0009233489399775863 + 0.001 * 6.848080158233643
Epoch 1970, val loss: 1.6485857963562012
Epoch 1980, training loss: 0.007787409238517284 = 0.0009180848719552159 + 0.001 * 6.869324207305908
Epoch 1980, val loss: 1.6490906476974487
Epoch 1990, training loss: 0.007782524451613426 = 0.0009129318641498685 + 0.001 * 6.869592189788818
Epoch 1990, val loss: 1.649590015411377
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 1.968598484992981 = 1.9600015878677368 + 0.001 * 8.59684944152832
Epoch 0, val loss: 1.9552077054977417
Epoch 10, training loss: 1.958263874053955 = 1.9496670961380005 + 0.001 * 8.596780776977539
Epoch 10, val loss: 1.945072889328003
Epoch 20, training loss: 1.945387601852417 = 1.9367910623550415 + 0.001 * 8.596535682678223
Epoch 20, val loss: 1.9324421882629395
Epoch 30, training loss: 1.9271732568740845 = 1.9185773134231567 + 0.001 * 8.595921516418457
Epoch 30, val loss: 1.9143527746200562
Epoch 40, training loss: 1.89999258518219 = 1.8913981914520264 + 0.001 * 8.594367027282715
Epoch 40, val loss: 1.8876962661743164
Epoch 50, training loss: 1.8613016605377197 = 1.8527119159698486 + 0.001 * 8.589706420898438
Epoch 50, val loss: 1.851433277130127
Epoch 60, training loss: 1.8164371252059937 = 1.8078649044036865 + 0.001 * 8.572168350219727
Epoch 60, val loss: 1.8134708404541016
Epoch 70, training loss: 1.7778819799423218 = 1.7693883180618286 + 0.001 * 8.493701934814453
Epoch 70, val loss: 1.783408761024475
Epoch 80, training loss: 1.733954906463623 = 1.7257956266403198 + 0.001 * 8.159276008605957
Epoch 80, val loss: 1.7422361373901367
Epoch 90, training loss: 1.672808289527893 = 1.6647216081619263 + 0.001 * 8.086649894714355
Epoch 90, val loss: 1.68369460105896
Epoch 100, training loss: 1.5914100408554077 = 1.5833945274353027 + 0.001 * 8.015460014343262
Epoch 100, val loss: 1.611369252204895
Epoch 110, training loss: 1.4965311288833618 = 1.4886029958724976 + 0.001 * 7.928188800811768
Epoch 110, val loss: 1.5314708948135376
Epoch 120, training loss: 1.3997735977172852 = 1.392055869102478 + 0.001 * 7.717689037322998
Epoch 120, val loss: 1.450776219367981
Epoch 130, training loss: 1.3065232038497925 = 1.2990145683288574 + 0.001 * 7.508677005767822
Epoch 130, val loss: 1.375044345855713
Epoch 140, training loss: 1.2163429260253906 = 1.2088918685913086 + 0.001 * 7.451003551483154
Epoch 140, val loss: 1.3036861419677734
Epoch 150, training loss: 1.1281945705413818 = 1.12080717086792 + 0.001 * 7.387448310852051
Epoch 150, val loss: 1.2349419593811035
Epoch 160, training loss: 1.0416982173919678 = 1.0343613624572754 + 0.001 * 7.336859226226807
Epoch 160, val loss: 1.1679285764694214
Epoch 170, training loss: 0.9570878744125366 = 0.9497847557067871 + 0.001 * 7.303138256072998
Epoch 170, val loss: 1.1023095846176147
Epoch 180, training loss: 0.875385046005249 = 0.868094801902771 + 0.001 * 7.290247917175293
Epoch 180, val loss: 1.0387427806854248
Epoch 190, training loss: 0.7980802059173584 = 0.7907997965812683 + 0.001 * 7.280407428741455
Epoch 190, val loss: 0.9786341786384583
Epoch 200, training loss: 0.7263282537460327 = 0.7190613150596619 + 0.001 * 7.266931056976318
Epoch 200, val loss: 0.9239751100540161
Epoch 210, training loss: 0.660254955291748 = 0.6530054807662964 + 0.001 * 7.249499797821045
Epoch 210, val loss: 0.8761399984359741
Epoch 220, training loss: 0.5989579558372498 = 0.591731607913971 + 0.001 * 7.2263503074646
Epoch 220, val loss: 0.8354277610778809
Epoch 230, training loss: 0.5411139726638794 = 0.5339157581329346 + 0.001 * 7.1982340812683105
Epoch 230, val loss: 0.8017811179161072
Epoch 240, training loss: 0.48623669147491455 = 0.479068785905838 + 0.001 * 7.167904376983643
Epoch 240, val loss: 0.7745227217674255
Epoch 250, training loss: 0.4347193241119385 = 0.4275764524936676 + 0.001 * 7.14287805557251
Epoch 250, val loss: 0.7530170679092407
Epoch 260, training loss: 0.3873324692249298 = 0.3802090287208557 + 0.001 * 7.123427391052246
Epoch 260, val loss: 0.7366480827331543
Epoch 270, training loss: 0.3444575369358063 = 0.3373446464538574 + 0.001 * 7.112878799438477
Epoch 270, val loss: 0.7250000834465027
Epoch 280, training loss: 0.3058503270149231 = 0.29874134063720703 + 0.001 * 7.108978271484375
Epoch 280, val loss: 0.7173793315887451
Epoch 290, training loss: 0.2709409296512604 = 0.26383358240127563 + 0.001 * 7.107354164123535
Epoch 290, val loss: 0.7128219604492188
Epoch 300, training loss: 0.23925501108169556 = 0.23215080797672272 + 0.001 * 7.104209899902344
Epoch 300, val loss: 0.7104294896125793
Epoch 310, training loss: 0.21059474349021912 = 0.20349201560020447 + 0.001 * 7.1027350425720215
Epoch 310, val loss: 0.7097294330596924
Epoch 320, training loss: 0.18498541414737701 = 0.17788296937942505 + 0.001 * 7.102449417114258
Epoch 320, val loss: 0.7107129096984863
Epoch 330, training loss: 0.16244858503341675 = 0.1553489714860916 + 0.001 * 7.099617004394531
Epoch 330, val loss: 0.7134271264076233
Epoch 340, training loss: 0.14289617538452148 = 0.1357964277267456 + 0.001 * 7.099754333496094
Epoch 340, val loss: 0.7179244756698608
Epoch 350, training loss: 0.12609043717384338 = 0.11899325251579285 + 0.001 * 7.097182750701904
Epoch 350, val loss: 0.7241836786270142
Epoch 360, training loss: 0.11170840263366699 = 0.1046118214726448 + 0.001 * 7.0965800285339355
Epoch 360, val loss: 0.7320101857185364
Epoch 370, training loss: 0.09941048920154572 = 0.0923103466629982 + 0.001 * 7.100140571594238
Epoch 370, val loss: 0.7412054538726807
Epoch 380, training loss: 0.08886654675006866 = 0.08176762610673904 + 0.001 * 7.098918437957764
Epoch 380, val loss: 0.7514821290969849
Epoch 390, training loss: 0.07979904860258102 = 0.07270368933677673 + 0.001 * 7.095361232757568
Epoch 390, val loss: 0.7625918388366699
Epoch 400, training loss: 0.07197470217943192 = 0.06488291919231415 + 0.001 * 7.091780662536621
Epoch 400, val loss: 0.7743226885795593
Epoch 410, training loss: 0.06520456075668335 = 0.05811072885990143 + 0.001 * 7.0938334465026855
Epoch 410, val loss: 0.7864589095115662
Epoch 420, training loss: 0.05932379141449928 = 0.052227746695280075 + 0.001 * 7.09604549407959
Epoch 420, val loss: 0.7988274693489075
Epoch 430, training loss: 0.05419406667351723 = 0.047102391719818115 + 0.001 * 7.091676235198975
Epoch 430, val loss: 0.8113113045692444
Epoch 440, training loss: 0.04971258342266083 = 0.04262450337409973 + 0.001 * 7.088078498840332
Epoch 440, val loss: 0.8238061666488647
Epoch 450, training loss: 0.045800503343343735 = 0.038702722638845444 + 0.001 * 7.097779273986816
Epoch 450, val loss: 0.8362400531768799
Epoch 460, training loss: 0.042346928268671036 = 0.0352591909468174 + 0.001 * 7.087737083435059
Epoch 460, val loss: 0.848556399345398
Epoch 470, training loss: 0.03931586071848869 = 0.03222658485174179 + 0.001 * 7.08927583694458
Epoch 470, val loss: 0.8607093095779419
Epoch 480, training loss: 0.03663907200098038 = 0.02954811416566372 + 0.001 * 7.090956687927246
Epoch 480, val loss: 0.8726852536201477
Epoch 490, training loss: 0.03426172956824303 = 0.02717009373009205 + 0.001 * 7.091634273529053
Epoch 490, val loss: 0.8844506740570068
Epoch 500, training loss: 0.032135020941495895 = 0.02504783682525158 + 0.001 * 7.087182998657227
Epoch 500, val loss: 0.8960395455360413
Epoch 510, training loss: 0.03022965043783188 = 0.023145822808146477 + 0.001 * 7.083827018737793
Epoch 510, val loss: 0.907474160194397
Epoch 520, training loss: 0.028515182435512543 = 0.021436501294374466 + 0.001 * 7.078681468963623
Epoch 520, val loss: 0.9187368750572205
Epoch 530, training loss: 0.026973312720656395 = 0.019897043704986572 + 0.001 * 7.076268196105957
Epoch 530, val loss: 0.9298221468925476
Epoch 540, training loss: 0.0256138164550066 = 0.01850825548171997 + 0.001 * 7.105560302734375
Epoch 540, val loss: 0.9407238960266113
Epoch 550, training loss: 0.02432822436094284 = 0.017252745106816292 + 0.001 * 7.075479507446289
Epoch 550, val loss: 0.9514334201812744
Epoch 560, training loss: 0.02319050207734108 = 0.016114015132188797 + 0.001 * 7.076485633850098
Epoch 560, val loss: 0.9619477391242981
Epoch 570, training loss: 0.02214674837887287 = 0.015076011419296265 + 0.001 * 7.070736408233643
Epoch 570, val loss: 0.9722967147827148
Epoch 580, training loss: 0.021201372146606445 = 0.014125294983386993 + 0.001 * 7.076077461242676
Epoch 580, val loss: 0.9825254082679749
Epoch 590, training loss: 0.020321976393461227 = 0.01325270626693964 + 0.001 * 7.06926965713501
Epoch 590, val loss: 0.992637574672699
Epoch 600, training loss: 0.01952217146754265 = 0.012451158836483955 + 0.001 * 7.071012020111084
Epoch 600, val loss: 1.0026291608810425
Epoch 610, training loss: 0.01878497004508972 = 0.011714750900864601 + 0.001 * 7.070218563079834
Epoch 610, val loss: 1.0124975442886353
Epoch 620, training loss: 0.018100690096616745 = 0.011038017459213734 + 0.001 * 7.062673091888428
Epoch 620, val loss: 1.0222004652023315
Epoch 630, training loss: 0.017518088221549988 = 0.010415948927402496 + 0.001 * 7.102139949798584
Epoch 630, val loss: 1.0317388772964478
Epoch 640, training loss: 0.016905460506677628 = 0.009843703359365463 + 0.001 * 7.061755657196045
Epoch 640, val loss: 1.0411032438278198
Epoch 650, training loss: 0.01637904904782772 = 0.009316639043390751 + 0.001 * 7.062410354614258
Epoch 650, val loss: 1.0502822399139404
Epoch 660, training loss: 0.01588963158428669 = 0.008830585516989231 + 0.001 * 7.059045314788818
Epoch 660, val loss: 1.0592741966247559
Epoch 670, training loss: 0.015429915860295296 = 0.008381779305636883 + 0.001 * 7.048136234283447
Epoch 670, val loss: 1.068074107170105
Epoch 680, training loss: 0.015032156370580196 = 0.00796679500490427 + 0.001 * 7.065361022949219
Epoch 680, val loss: 1.076675534248352
Epoch 690, training loss: 0.014630386605858803 = 0.007582586724311113 + 0.001 * 7.047799587249756
Epoch 690, val loss: 1.0851176977157593
Epoch 700, training loss: 0.014269525185227394 = 0.007226263638585806 + 0.001 * 7.043261528015137
Epoch 700, val loss: 1.0933547019958496
Epoch 710, training loss: 0.013955813832581043 = 0.006895371712744236 + 0.001 * 7.060441970825195
Epoch 710, val loss: 1.1014410257339478
Epoch 720, training loss: 0.013638718985021114 = 0.00658765435218811 + 0.001 * 7.051064491271973
Epoch 720, val loss: 1.1093333959579468
Epoch 730, training loss: 0.013343382626771927 = 0.006300961598753929 + 0.001 * 7.042421340942383
Epoch 730, val loss: 1.1170802116394043
Epoch 740, training loss: 0.013066371902823448 = 0.006033491343259811 + 0.001 * 7.032879829406738
Epoch 740, val loss: 1.1246347427368164
Epoch 750, training loss: 0.012809064239263535 = 0.0057836235500872135 + 0.001 * 7.025440216064453
Epoch 750, val loss: 1.1320308446884155
Epoch 760, training loss: 0.012581882998347282 = 0.005549891851842403 + 0.001 * 7.0319905281066895
Epoch 760, val loss: 1.1392675638198853
Epoch 770, training loss: 0.012377074919641018 = 0.005330963060259819 + 0.001 * 7.046111583709717
Epoch 770, val loss: 1.1463449001312256
Epoch 780, training loss: 0.01216322835534811 = 0.005125644151121378 + 0.001 * 7.037583827972412
Epoch 780, val loss: 1.153278112411499
Epoch 790, training loss: 0.011950990185141563 = 0.004932826384902 + 0.001 * 7.018163681030273
Epoch 790, val loss: 1.1600629091262817
Epoch 800, training loss: 0.011782637797296047 = 0.0047515397891402245 + 0.001 * 7.031097888946533
Epoch 800, val loss: 1.1667083501815796
Epoch 810, training loss: 0.01159158255904913 = 0.0045808907598257065 + 0.001 * 7.0106916427612305
Epoch 810, val loss: 1.173221468925476
Epoch 820, training loss: 0.011436207219958305 = 0.004420051351189613 + 0.001 * 7.016156196594238
Epoch 820, val loss: 1.179600715637207
Epoch 830, training loss: 0.011280912905931473 = 0.004268201999366283 + 0.001 * 7.012711048126221
Epoch 830, val loss: 1.1858456134796143
Epoch 840, training loss: 0.011124786920845509 = 0.0041245692409574986 + 0.001 * 7.000217437744141
Epoch 840, val loss: 1.191983938217163
Epoch 850, training loss: 0.011004108935594559 = 0.003988550044596195 + 0.001 * 7.01555871963501
Epoch 850, val loss: 1.1980078220367432
Epoch 860, training loss: 0.010870292782783508 = 0.0038595213554799557 + 0.001 * 7.010771751403809
Epoch 860, val loss: 1.2039239406585693
Epoch 870, training loss: 0.010724728927016258 = 0.0037369225174188614 + 0.001 * 6.9878058433532715
Epoch 870, val loss: 1.2097480297088623
Epoch 880, training loss: 0.010614888742566109 = 0.0036203782074153423 + 0.001 * 6.994510650634766
Epoch 880, val loss: 1.2154638767242432
Epoch 890, training loss: 0.010519977658987045 = 0.0035093859769403934 + 0.001 * 7.010591983795166
Epoch 890, val loss: 1.221100926399231
Epoch 900, training loss: 0.010397396050393581 = 0.0034035760909318924 + 0.001 * 6.993819713592529
Epoch 900, val loss: 1.2266557216644287
Epoch 910, training loss: 0.010281998664140701 = 0.003302362747490406 + 0.001 * 6.979636192321777
Epoch 910, val loss: 1.232141375541687
Epoch 920, training loss: 0.010178565979003906 = 0.003205542918294668 + 0.001 * 6.9730224609375
Epoch 920, val loss: 1.237563967704773
Epoch 930, training loss: 0.010104261338710785 = 0.003112888429313898 + 0.001 * 6.991372585296631
Epoch 930, val loss: 1.2429149150848389
Epoch 940, training loss: 0.010008920915424824 = 0.003024220233783126 + 0.001 * 6.984700679779053
Epoch 940, val loss: 1.2481930255889893
Epoch 950, training loss: 0.009913267567753792 = 0.0029392109718173742 + 0.001 * 6.974056720733643
Epoch 950, val loss: 1.253426194190979
Epoch 960, training loss: 0.00983236450701952 = 0.0028577230405062437 + 0.001 * 6.974640846252441
Epoch 960, val loss: 1.2586002349853516
Epoch 970, training loss: 0.009751453064382076 = 0.002779660513624549 + 0.001 * 6.971792221069336
Epoch 970, val loss: 1.2637332677841187
Epoch 980, training loss: 0.009680794551968575 = 0.002704853657633066 + 0.001 * 6.975940704345703
Epoch 980, val loss: 1.268781065940857
Epoch 990, training loss: 0.009600820019841194 = 0.002633064053952694 + 0.001 * 6.9677557945251465
Epoch 990, val loss: 1.2737668752670288
Epoch 1000, training loss: 0.009553350508213043 = 0.0025640896055847406 + 0.001 * 6.989260673522949
Epoch 1000, val loss: 1.2787222862243652
Epoch 1010, training loss: 0.009465055540204048 = 0.002497799461707473 + 0.001 * 6.967255592346191
Epoch 1010, val loss: 1.2836205959320068
Epoch 1020, training loss: 0.009403681382536888 = 0.0024340820964425802 + 0.001 * 6.96959924697876
Epoch 1020, val loss: 1.2884389162063599
Epoch 1030, training loss: 0.009341091848909855 = 0.0023730003740638494 + 0.001 * 6.9680914878845215
Epoch 1030, val loss: 1.2931727170944214
Epoch 1040, training loss: 0.009266466833651066 = 0.0023144162259995937 + 0.001 * 6.95205020904541
Epoch 1040, val loss: 1.2978551387786865
Epoch 1050, training loss: 0.009237086400389671 = 0.002258077496662736 + 0.001 * 6.979008674621582
Epoch 1050, val loss: 1.3025016784667969
Epoch 1060, training loss: 0.009151803329586983 = 0.0022038505412638187 + 0.001 * 6.9479522705078125
Epoch 1060, val loss: 1.3070555925369263
Epoch 1070, training loss: 0.009107566438615322 = 0.002151705091819167 + 0.001 * 6.9558610916137695
Epoch 1070, val loss: 1.3115640878677368
Epoch 1080, training loss: 0.009063542820513248 = 0.0021015494130551815 + 0.001 * 6.961993217468262
Epoch 1080, val loss: 1.3160022497177124
Epoch 1090, training loss: 0.009015413001179695 = 0.0020533541683107615 + 0.001 * 6.962058067321777
Epoch 1090, val loss: 1.320407509803772
Epoch 1100, training loss: 0.008954446762800217 = 0.0020069582387804985 + 0.001 * 6.947488307952881
Epoch 1100, val loss: 1.3246983289718628
Epoch 1110, training loss: 0.008926630020141602 = 0.0019622487016022205 + 0.001 * 6.964380741119385
Epoch 1110, val loss: 1.3289448022842407
Epoch 1120, training loss: 0.008876154199242592 = 0.0019192382460460067 + 0.001 * 6.956915855407715
Epoch 1120, val loss: 1.3331762552261353
Epoch 1130, training loss: 0.00882240105420351 = 0.0018777818186208606 + 0.001 * 6.9446187019348145
Epoch 1130, val loss: 1.337328553199768
Epoch 1140, training loss: 0.008779805153608322 = 0.0018378958338871598 + 0.001 * 6.941909313201904
Epoch 1140, val loss: 1.3413729667663574
Epoch 1150, training loss: 0.008749298751354218 = 0.001799477729946375 + 0.001 * 6.949820041656494
Epoch 1150, val loss: 1.3453730344772339
Epoch 1160, training loss: 0.008705778047442436 = 0.0017624900210648775 + 0.001 * 6.9432878494262695
Epoch 1160, val loss: 1.3493387699127197
Epoch 1170, training loss: 0.00869082286953926 = 0.0017268422525376081 + 0.001 * 6.963980197906494
Epoch 1170, val loss: 1.3532503843307495
Epoch 1180, training loss: 0.008621425367891788 = 0.0016924573574215174 + 0.001 * 6.928967475891113
Epoch 1180, val loss: 1.3571016788482666
Epoch 1190, training loss: 0.008593285456299782 = 0.0016592862084507942 + 0.001 * 6.933999061584473
Epoch 1190, val loss: 1.360848069190979
Epoch 1200, training loss: 0.008555565029382706 = 0.0016273348592221737 + 0.001 * 6.928229808807373
Epoch 1200, val loss: 1.3645637035369873
Epoch 1210, training loss: 0.008535717613995075 = 0.0015965448692440987 + 0.001 * 6.939172267913818
Epoch 1210, val loss: 1.3682278394699097
Epoch 1220, training loss: 0.0084891552105546 = 0.00156683090608567 + 0.001 * 6.922323703765869
Epoch 1220, val loss: 1.3718103170394897
Epoch 1230, training loss: 0.008458347991108894 = 0.001538188080303371 + 0.001 * 6.920159816741943
Epoch 1230, val loss: 1.3753594160079956
Epoch 1240, training loss: 0.008441195823252201 = 0.0015104993944987655 + 0.001 * 6.930696487426758
Epoch 1240, val loss: 1.3788504600524902
Epoch 1250, training loss: 0.008425869047641754 = 0.00148375378921628 + 0.001 * 6.942114353179932
Epoch 1250, val loss: 1.3822656869888306
Epoch 1260, training loss: 0.008380920626223087 = 0.0014579693088307977 + 0.001 * 6.922950744628906
Epoch 1260, val loss: 1.3856523036956787
Epoch 1270, training loss: 0.008344011381268501 = 0.0014330651611089706 + 0.001 * 6.910946369171143
Epoch 1270, val loss: 1.3889532089233398
Epoch 1280, training loss: 0.008336407132446766 = 0.0014090302865952253 + 0.001 * 6.9273762702941895
Epoch 1280, val loss: 1.3922007083892822
Epoch 1290, training loss: 0.008289676159620285 = 0.0013858433812856674 + 0.001 * 6.903832912445068
Epoch 1290, val loss: 1.3953888416290283
Epoch 1300, training loss: 0.008282052353024483 = 0.0013634375063702464 + 0.001 * 6.918614864349365
Epoch 1300, val loss: 1.3985283374786377
Epoch 1310, training loss: 0.008254624903202057 = 0.0013417761074379086 + 0.001 * 6.912848472595215
Epoch 1310, val loss: 1.4016218185424805
Epoch 1320, training loss: 0.008227072656154633 = 0.0013207951560616493 + 0.001 * 6.906277179718018
Epoch 1320, val loss: 1.4046509265899658
Epoch 1330, training loss: 0.008231639862060547 = 0.0013004883658140898 + 0.001 * 6.931151390075684
Epoch 1330, val loss: 1.4076344966888428
Epoch 1340, training loss: 0.008192943409085274 = 0.0012808428145945072 + 0.001 * 6.912100791931152
Epoch 1340, val loss: 1.410595417022705
Epoch 1350, training loss: 0.008181879296898842 = 0.0012617976171895862 + 0.001 * 6.92008113861084
Epoch 1350, val loss: 1.4134997129440308
Epoch 1360, training loss: 0.008143706247210503 = 0.0012433682568371296 + 0.001 * 6.900337219238281
Epoch 1360, val loss: 1.4162945747375488
Epoch 1370, training loss: 0.008134694769978523 = 0.001225557061843574 + 0.001 * 6.909137725830078
Epoch 1370, val loss: 1.4191210269927979
Epoch 1380, training loss: 0.008155718445777893 = 0.0012082959292456508 + 0.001 * 6.947422504425049
Epoch 1380, val loss: 1.4218565225601196
Epoch 1390, training loss: 0.00808594562113285 = 0.0011915754294022918 + 0.001 * 6.894370079040527
Epoch 1390, val loss: 1.4245332479476929
Epoch 1400, training loss: 0.008078142069280148 = 0.0011753755388781428 + 0.001 * 6.902766227722168
Epoch 1400, val loss: 1.4272010326385498
Epoch 1410, training loss: 0.008063898421823978 = 0.0011596778640523553 + 0.001 * 6.904220104217529
Epoch 1410, val loss: 1.4297685623168945
Epoch 1420, training loss: 0.00804573018103838 = 0.0011445002164691687 + 0.001 * 6.901229381561279
Epoch 1420, val loss: 1.4323431253433228
Epoch 1430, training loss: 0.008014913648366928 = 0.0011297703022137284 + 0.001 * 6.885142803192139
Epoch 1430, val loss: 1.4348424673080444
Epoch 1440, training loss: 0.008016284555196762 = 0.0011154888197779655 + 0.001 * 6.900795936584473
Epoch 1440, val loss: 1.437307596206665
Epoch 1450, training loss: 0.007987315766513348 = 0.0011016405187547207 + 0.001 * 6.885674953460693
Epoch 1450, val loss: 1.4397318363189697
Epoch 1460, training loss: 0.00796121172606945 = 0.0010882148053497076 + 0.001 * 6.872996807098389
Epoch 1460, val loss: 1.4421124458312988
Epoch 1470, training loss: 0.0079555818811059 = 0.001075180945917964 + 0.001 * 6.880400657653809
Epoch 1470, val loss: 1.4444228410720825
Epoch 1480, training loss: 0.007966512814164162 = 0.0010625417344272137 + 0.001 * 6.903970718383789
Epoch 1480, val loss: 1.4467401504516602
Epoch 1490, training loss: 0.007942389696836472 = 0.0010502641089260578 + 0.001 * 6.892125606536865
Epoch 1490, val loss: 1.4490094184875488
Epoch 1500, training loss: 0.007929342798888683 = 0.001038335612975061 + 0.001 * 6.891006946563721
Epoch 1500, val loss: 1.451200008392334
Epoch 1510, training loss: 0.007917865179479122 = 0.001026748912408948 + 0.001 * 6.891116142272949
Epoch 1510, val loss: 1.4533977508544922
Epoch 1520, training loss: 0.007908753119409084 = 0.0010155144846066833 + 0.001 * 6.893238544464111
Epoch 1520, val loss: 1.455562710762024
Epoch 1530, training loss: 0.007889265194535255 = 0.0010046017123386264 + 0.001 * 6.884663105010986
Epoch 1530, val loss: 1.4576246738433838
Epoch 1540, training loss: 0.007891206070780754 = 0.0009940080344676971 + 0.001 * 6.89719820022583
Epoch 1540, val loss: 1.4597166776657104
Epoch 1550, training loss: 0.007847500964999199 = 0.000983706908300519 + 0.001 * 6.86379337310791
Epoch 1550, val loss: 1.4617409706115723
Epoch 1560, training loss: 0.00783576350659132 = 0.0009737039217725396 + 0.001 * 6.862059116363525
Epoch 1560, val loss: 1.4637095928192139
Epoch 1570, training loss: 0.007859818637371063 = 0.0009639935451559722 + 0.001 * 6.895824432373047
Epoch 1570, val loss: 1.4656713008880615
Epoch 1580, training loss: 0.007825526408851147 = 0.0009545751963742077 + 0.001 * 6.870950698852539
Epoch 1580, val loss: 1.4676363468170166
Epoch 1590, training loss: 0.0077938297763466835 = 0.0009453955572098494 + 0.001 * 6.848433494567871
Epoch 1590, val loss: 1.4694879055023193
Epoch 1600, training loss: 0.007837129756808281 = 0.0009364593424834311 + 0.001 * 6.900669574737549
Epoch 1600, val loss: 1.47130286693573
Epoch 1610, training loss: 0.007832124829292297 = 0.0009277881472371519 + 0.001 * 6.904336929321289
Epoch 1610, val loss: 1.4731974601745605
Epoch 1620, training loss: 0.007787751965224743 = 0.0009193690493702888 + 0.001 * 6.868382453918457
Epoch 1620, val loss: 1.4749205112457275
Epoch 1630, training loss: 0.007812196854501963 = 0.0009111494873650372 + 0.001 * 6.9010467529296875
Epoch 1630, val loss: 1.4766652584075928
Epoch 1640, training loss: 0.007744626607745886 = 0.0009031742811203003 + 0.001 * 6.841452121734619
Epoch 1640, val loss: 1.4784283638000488
Epoch 1650, training loss: 0.007774742785841227 = 0.0008953940705396235 + 0.001 * 6.879348278045654
Epoch 1650, val loss: 1.4801065921783447
Epoch 1660, training loss: 0.007766229100525379 = 0.0008878143853507936 + 0.001 * 6.878414154052734
Epoch 1660, val loss: 1.4817867279052734
Epoch 1670, training loss: 0.0077513777650892735 = 0.0008804223616607487 + 0.001 * 6.870954990386963
Epoch 1670, val loss: 1.483400821685791
Epoch 1680, training loss: 0.007714851293712854 = 0.0008732160204090178 + 0.001 * 6.841635227203369
Epoch 1680, val loss: 1.485007643699646
Epoch 1690, training loss: 0.00775371165946126 = 0.0008662042091600597 + 0.001 * 6.887507438659668
Epoch 1690, val loss: 1.4866300821304321
Epoch 1700, training loss: 0.007705725729465485 = 0.0008593650418333709 + 0.001 * 6.846360683441162
Epoch 1700, val loss: 1.4881811141967773
Epoch 1710, training loss: 0.007702379487454891 = 0.0008526711608283222 + 0.001 * 6.849708080291748
Epoch 1710, val loss: 1.4896780252456665
Epoch 1720, training loss: 0.007688961457461119 = 0.0008461819379590452 + 0.001 * 6.842779159545898
Epoch 1720, val loss: 1.4912093877792358
Epoch 1730, training loss: 0.007693501189351082 = 0.0008398582576774061 + 0.001 * 6.853642463684082
Epoch 1730, val loss: 1.4926731586456299
Epoch 1740, training loss: 0.007681387010961771 = 0.0008336918544955552 + 0.001 * 6.8476948738098145
Epoch 1740, val loss: 1.494150996208191
Epoch 1750, training loss: 0.007685855496674776 = 0.0008276671869680285 + 0.001 * 6.858187675476074
Epoch 1750, val loss: 1.4955778121948242
Epoch 1760, training loss: 0.0077244373969733715 = 0.0008218053844757378 + 0.001 * 6.9026312828063965
Epoch 1760, val loss: 1.4969863891601562
Epoch 1770, training loss: 0.007664770353585482 = 0.0008160759462043643 + 0.001 * 6.84869384765625
Epoch 1770, val loss: 1.4983561038970947
Epoch 1780, training loss: 0.007639686577022076 = 0.000810490280855447 + 0.001 * 6.829195976257324
Epoch 1780, val loss: 1.499756932258606
Epoch 1790, training loss: 0.0076424493454396725 = 0.0008050404721871018 + 0.001 * 6.837408542633057
Epoch 1790, val loss: 1.5010603666305542
Epoch 1800, training loss: 0.0076520852744579315 = 0.0007997188367880881 + 0.001 * 6.852365970611572
Epoch 1800, val loss: 1.5023713111877441
Epoch 1810, training loss: 0.007656103000044823 = 0.0007945203105919063 + 0.001 * 6.8615827560424805
Epoch 1810, val loss: 1.503696322441101
Epoch 1820, training loss: 0.0076554687693715096 = 0.0007894320297054946 + 0.001 * 6.866036415100098
Epoch 1820, val loss: 1.5049329996109009
Epoch 1830, training loss: 0.007633191533386707 = 0.0007844679639674723 + 0.001 * 6.848723411560059
Epoch 1830, val loss: 1.5062071084976196
Epoch 1840, training loss: 0.007662585470825434 = 0.0007795924320816994 + 0.001 * 6.882992744445801
Epoch 1840, val loss: 1.5074589252471924
Epoch 1850, training loss: 0.0075903767719864845 = 0.0007748323259875178 + 0.001 * 6.815544128417969
Epoch 1850, val loss: 1.508679747581482
Epoch 1860, training loss: 0.007590052671730518 = 0.0007701732683926821 + 0.001 * 6.819879531860352
Epoch 1860, val loss: 1.509865164756775
Epoch 1870, training loss: 0.007695462089031935 = 0.0007656286470592022 + 0.001 * 6.929832935333252
Epoch 1870, val loss: 1.5110784769058228
Epoch 1880, training loss: 0.007580570410937071 = 0.0007612039335072041 + 0.001 * 6.819365978240967
Epoch 1880, val loss: 1.5121968984603882
Epoch 1890, training loss: 0.007608600426465273 = 0.0007568529108539224 + 0.001 * 6.851747512817383
Epoch 1890, val loss: 1.513350486755371
Epoch 1900, training loss: 0.0076076677069067955 = 0.0007526174304075539 + 0.001 * 6.8550496101379395
Epoch 1900, val loss: 1.51443612575531
Epoch 1910, training loss: 0.007573109585791826 = 0.000748469668906182 + 0.001 * 6.824639797210693
Epoch 1910, val loss: 1.5155980587005615
Epoch 1920, training loss: 0.0075680650770664215 = 0.0007444102666340768 + 0.001 * 6.823654651641846
Epoch 1920, val loss: 1.516599178314209
Epoch 1930, training loss: 0.007552630268037319 = 0.0007404318312183022 + 0.001 * 6.812198162078857
Epoch 1930, val loss: 1.5177249908447266
Epoch 1940, training loss: 0.007551934570074081 = 0.0007365565397776663 + 0.001 * 6.815377712249756
Epoch 1940, val loss: 1.5187572240829468
Epoch 1950, training loss: 0.007579413708299398 = 0.0007327645434997976 + 0.001 * 6.846648693084717
Epoch 1950, val loss: 1.5197455883026123
Epoch 1960, training loss: 0.007527266163378954 = 0.0007290738285519183 + 0.001 * 6.798192024230957
Epoch 1960, val loss: 1.5208103656768799
Epoch 1970, training loss: 0.007537489756941795 = 0.0007254467345774174 + 0.001 * 6.812042713165283
Epoch 1970, val loss: 1.5217647552490234
Epoch 1980, training loss: 0.007615657988935709 = 0.0007218887913040817 + 0.001 * 6.893769264221191
Epoch 1980, val loss: 1.5227279663085938
Epoch 1990, training loss: 0.007518925238400698 = 0.0007184070418588817 + 0.001 * 6.800518035888672
Epoch 1990, val loss: 1.5237281322479248
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.845018450184502
The final CL Acc:0.81111, 0.00605, The final GNN Acc:0.84080, 0.00310
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11652])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10626])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9510552883148193 = 1.9424583911895752 + 0.001 * 8.5968599319458
Epoch 0, val loss: 1.935028076171875
Epoch 10, training loss: 1.9413344860076904 = 1.9327377080917358 + 0.001 * 8.596822738647461
Epoch 10, val loss: 1.925405502319336
Epoch 20, training loss: 1.9293972253799438 = 1.9208005666732788 + 0.001 * 8.596673965454102
Epoch 20, val loss: 1.9130473136901855
Epoch 30, training loss: 1.9127647876739502 = 1.9041684865951538 + 0.001 * 8.596307754516602
Epoch 30, val loss: 1.8954311609268188
Epoch 40, training loss: 1.8885517120361328 = 1.8799563646316528 + 0.001 * 8.595396041870117
Epoch 40, val loss: 1.8698911666870117
Epoch 50, training loss: 1.8555680513381958 = 1.8469752073287964 + 0.001 * 8.592790603637695
Epoch 50, val loss: 1.836633563041687
Epoch 60, training loss: 1.8204741477966309 = 1.811890721321106 + 0.001 * 8.583443641662598
Epoch 60, val loss: 1.805753469467163
Epoch 70, training loss: 1.7931679487228394 = 1.7846267223358154 + 0.001 * 8.541252136230469
Epoch 70, val loss: 1.7864420413970947
Epoch 80, training loss: 1.759464979171753 = 1.7512017488479614 + 0.001 * 8.263276100158691
Epoch 80, val loss: 1.7604906558990479
Epoch 90, training loss: 1.7118700742721558 = 1.703856348991394 + 0.001 * 8.013714790344238
Epoch 90, val loss: 1.7218905687332153
Epoch 100, training loss: 1.646008849143982 = 1.6381449699401855 + 0.001 * 7.8639302253723145
Epoch 100, val loss: 1.6683307886123657
Epoch 110, training loss: 1.5625332593917847 = 1.5548871755599976 + 0.001 * 7.646080493927002
Epoch 110, val loss: 1.6008145809173584
Epoch 120, training loss: 1.4691826105117798 = 1.4617222547531128 + 0.001 * 7.460407733917236
Epoch 120, val loss: 1.5243568420410156
Epoch 130, training loss: 1.3721001148223877 = 1.3647109270095825 + 0.001 * 7.389138221740723
Epoch 130, val loss: 1.44668710231781
Epoch 140, training loss: 1.2708196640014648 = 1.263474702835083 + 0.001 * 7.344930648803711
Epoch 140, val loss: 1.3674081563949585
Epoch 150, training loss: 1.1644415855407715 = 1.1571221351623535 + 0.001 * 7.319488048553467
Epoch 150, val loss: 1.2850470542907715
Epoch 160, training loss: 1.0566537380218506 = 1.0493637323379517 + 0.001 * 7.289957523345947
Epoch 160, val loss: 1.2026830911636353
Epoch 170, training loss: 0.9544170498847961 = 0.9471538066864014 + 0.001 * 7.263250350952148
Epoch 170, val loss: 1.1254627704620361
Epoch 180, training loss: 0.8641138672828674 = 0.8568688631057739 + 0.001 * 7.244976997375488
Epoch 180, val loss: 1.0583317279815674
Epoch 190, training loss: 0.7887908816337585 = 0.7815549969673157 + 0.001 * 7.235859394073486
Epoch 190, val loss: 1.0035161972045898
Epoch 200, training loss: 0.727739691734314 = 0.7205089330673218 + 0.001 * 7.230752944946289
Epoch 200, val loss: 0.9610893130302429
Epoch 210, training loss: 0.6777629256248474 = 0.6705361604690552 + 0.001 * 7.226759910583496
Epoch 210, val loss: 0.9293009638786316
Epoch 220, training loss: 0.6346721649169922 = 0.6274499297142029 + 0.001 * 7.222249507904053
Epoch 220, val loss: 0.9051703810691833
Epoch 230, training loss: 0.5946542024612427 = 0.5874376893043518 + 0.001 * 7.216493606567383
Epoch 230, val loss: 0.8857762217521667
Epoch 240, training loss: 0.5548213720321655 = 0.5476126074790955 + 0.001 * 7.208762168884277
Epoch 240, val loss: 0.8687148094177246
Epoch 250, training loss: 0.5135639309883118 = 0.5063661932945251 + 0.001 * 7.197742938995361
Epoch 250, val loss: 0.8526239395141602
Epoch 260, training loss: 0.4707513153553009 = 0.46357062458992004 + 0.001 * 7.180694103240967
Epoch 260, val loss: 0.8379184007644653
Epoch 270, training loss: 0.42764607071876526 = 0.42048850655555725 + 0.001 * 7.1575727462768555
Epoch 270, val loss: 0.8257180452346802
Epoch 280, training loss: 0.3861418664455414 = 0.3790147304534912 + 0.001 * 7.127137660980225
Epoch 280, val loss: 0.8174716234207153
Epoch 290, training loss: 0.34772321581840515 = 0.3406180143356323 + 0.001 * 7.105213165283203
Epoch 290, val loss: 0.8135669827461243
Epoch 300, training loss: 0.3129536211490631 = 0.30586734414100647 + 0.001 * 7.086269855499268
Epoch 300, val loss: 0.8138833045959473
Epoch 310, training loss: 0.28161415457725525 = 0.2745334506034851 + 0.001 * 7.0807085037231445
Epoch 310, val loss: 0.8178010582923889
Epoch 320, training loss: 0.25304415822029114 = 0.2459641844034195 + 0.001 * 7.079967021942139
Epoch 320, val loss: 0.8243326544761658
Epoch 330, training loss: 0.22660894691944122 = 0.21952766180038452 + 0.001 * 7.081290245056152
Epoch 330, val loss: 0.8327566385269165
Epoch 340, training loss: 0.2019452303647995 = 0.1948617547750473 + 0.001 * 7.083470821380615
Epoch 340, val loss: 0.8428890705108643
Epoch 350, training loss: 0.17900124192237854 = 0.17191588878631592 + 0.001 * 7.085353374481201
Epoch 350, val loss: 0.8544958829879761
Epoch 360, training loss: 0.15795668959617615 = 0.15086989104747772 + 0.001 * 7.0868048667907715
Epoch 360, val loss: 0.8677482008934021
Epoch 370, training loss: 0.13901245594024658 = 0.13192437589168549 + 0.001 * 7.088085174560547
Epoch 370, val loss: 0.8825117349624634
Epoch 380, training loss: 0.12222042679786682 = 0.11513117700815201 + 0.001 * 7.089250564575195
Epoch 380, val loss: 0.8986682295799255
Epoch 390, training loss: 0.10750465840101242 = 0.10041367262601852 + 0.001 * 7.090981960296631
Epoch 390, val loss: 0.9158755540847778
Epoch 400, training loss: 0.09471573680639267 = 0.08762364089488983 + 0.001 * 7.092095375061035
Epoch 400, val loss: 0.9338753819465637
Epoch 410, training loss: 0.08366715162992477 = 0.07657477259635925 + 0.001 * 7.09237813949585
Epoch 410, val loss: 0.9522358179092407
Epoch 420, training loss: 0.07416009902954102 = 0.0670667290687561 + 0.001 * 7.093367576599121
Epoch 420, val loss: 0.9705172181129456
Epoch 430, training loss: 0.06599585711956024 = 0.05890187993645668 + 0.001 * 7.0939764976501465
Epoch 430, val loss: 0.9887101650238037
Epoch 440, training loss: 0.05900155380368233 = 0.05190553143620491 + 0.001 * 7.09602165222168
Epoch 440, val loss: 1.0065866708755493
Epoch 450, training loss: 0.05301086604595184 = 0.04591520130634308 + 0.001 * 7.095664978027344
Epoch 450, val loss: 1.0241665840148926
Epoch 460, training loss: 0.04788004979491234 = 0.04078414663672447 + 0.001 * 7.095903396606445
Epoch 460, val loss: 1.0413175821304321
Epoch 470, training loss: 0.04347941279411316 = 0.0363839715719223 + 0.001 * 7.095441818237305
Epoch 470, val loss: 1.058012843132019
Epoch 480, training loss: 0.039701901376247406 = 0.032602619379758835 + 0.001 * 7.09928035736084
Epoch 480, val loss: 1.0741649866104126
Epoch 490, training loss: 0.03643929585814476 = 0.029343631118535995 + 0.001 * 7.095664024353027
Epoch 490, val loss: 1.0898138284683228
Epoch 500, training loss: 0.03362071514129639 = 0.026524947956204414 + 0.001 * 7.095765113830566
Epoch 500, val loss: 1.1049110889434814
Epoch 510, training loss: 0.031173842027783394 = 0.02407771721482277 + 0.001 * 7.09612512588501
Epoch 510, val loss: 1.1194486618041992
Epoch 520, training loss: 0.029039686545729637 = 0.02194417640566826 + 0.001 * 7.0955095291137695
Epoch 520, val loss: 1.1334587335586548
Epoch 530, training loss: 0.027171380817890167 = 0.02007589302957058 + 0.001 * 7.095486640930176
Epoch 530, val loss: 1.1469799280166626
Epoch 540, training loss: 0.025528032332658768 = 0.018432864919304848 + 0.001 * 7.095166206359863
Epoch 540, val loss: 1.1599808931350708
Epoch 550, training loss: 0.024076776579022408 = 0.01698201335966587 + 0.001 * 7.094762325286865
Epoch 550, val loss: 1.172441840171814
Epoch 560, training loss: 0.022790418937802315 = 0.015695590525865555 + 0.001 * 7.0948286056518555
Epoch 560, val loss: 1.1843980550765991
Epoch 570, training loss: 0.021645143628120422 = 0.014550499618053436 + 0.001 * 7.094643592834473
Epoch 570, val loss: 1.195931315422058
Epoch 580, training loss: 0.0206291601061821 = 0.013527545146644115 + 0.001 * 7.101614475250244
Epoch 580, val loss: 1.2069954872131348
Epoch 590, training loss: 0.019704895094037056 = 0.012610350735485554 + 0.001 * 7.094543933868408
Epoch 590, val loss: 1.217654824256897
Epoch 600, training loss: 0.018878037109971046 = 0.011785153299570084 + 0.001 * 7.092883110046387
Epoch 600, val loss: 1.2279012203216553
Epoch 610, training loss: 0.01813170686364174 = 0.011040390469133854 + 0.001 * 7.091316223144531
Epoch 610, val loss: 1.2378047704696655
Epoch 620, training loss: 0.01747172325849533 = 0.010366316884756088 + 0.001 * 7.105405807495117
Epoch 620, val loss: 1.2473022937774658
Epoch 630, training loss: 0.016847852617502213 = 0.00975442212074995 + 0.001 * 7.093430519104004
Epoch 630, val loss: 1.2564846277236938
Epoch 640, training loss: 0.016288159415125847 = 0.009197419509291649 + 0.001 * 7.090740203857422
Epoch 640, val loss: 1.2653385400772095
Epoch 650, training loss: 0.015777284279465675 = 0.0086890310049057 + 0.001 * 7.088253498077393
Epoch 650, val loss: 1.2738722562789917
Epoch 660, training loss: 0.015332313254475594 = 0.008223839104175568 + 0.001 * 7.108473300933838
Epoch 660, val loss: 1.2821574211120605
Epoch 670, training loss: 0.014885556884109974 = 0.007797116879373789 + 0.001 * 7.088439464569092
Epoch 670, val loss: 1.2901211977005005
Epoch 680, training loss: 0.014492979273200035 = 0.007404844742268324 + 0.001 * 7.088134288787842
Epoch 680, val loss: 1.2978557348251343
Epoch 690, training loss: 0.014129554852843285 = 0.007043438497930765 + 0.001 * 7.086116313934326
Epoch 690, val loss: 1.3053100109100342
Epoch 700, training loss: 0.013801245018839836 = 0.006709750276058912 + 0.001 * 7.091494560241699
Epoch 700, val loss: 1.312529444694519
Epoch 710, training loss: 0.013492832891643047 = 0.006401048041880131 + 0.001 * 7.091784477233887
Epoch 710, val loss: 1.3195043802261353
Epoch 720, training loss: 0.01320150401443243 = 0.006114823278039694 + 0.001 * 7.0866804122924805
Epoch 720, val loss: 1.326277494430542
Epoch 730, training loss: 0.01293603703379631 = 0.005848495755344629 + 0.001 * 7.087540626525879
Epoch 730, val loss: 1.332885503768921
Epoch 740, training loss: 0.012684309855103493 = 0.005599027033895254 + 0.001 * 7.085282325744629
Epoch 740, val loss: 1.3393373489379883
Epoch 750, training loss: 0.012448761612176895 = 0.00536304758861661 + 0.001 * 7.085714340209961
Epoch 750, val loss: 1.3456616401672363
Epoch 760, training loss: 0.012220960110425949 = 0.005138363689184189 + 0.001 * 7.082596778869629
Epoch 760, val loss: 1.3519694805145264
Epoch 770, training loss: 0.01200235728174448 = 0.004923978354781866 + 0.001 * 7.078378677368164
Epoch 770, val loss: 1.3580574989318848
Epoch 780, training loss: 0.011797615326941013 = 0.004719818942248821 + 0.001 * 7.07779598236084
Epoch 780, val loss: 1.363987922668457
Epoch 790, training loss: 0.011598324403166771 = 0.0045260838232934475 + 0.001 * 7.072240829467773
Epoch 790, val loss: 1.3697936534881592
Epoch 800, training loss: 0.011448632925748825 = 0.004342461470514536 + 0.001 * 7.106171131134033
Epoch 800, val loss: 1.375379204750061
Epoch 810, training loss: 0.011238716542720795 = 0.0041688960045576096 + 0.001 * 7.069820404052734
Epoch 810, val loss: 1.3808385133743286
Epoch 820, training loss: 0.011062887497246265 = 0.00400504469871521 + 0.001 * 7.057842254638672
Epoch 820, val loss: 1.3861528635025024
Epoch 830, training loss: 0.010919728316366673 = 0.0038504975382238626 + 0.001 * 7.069230556488037
Epoch 830, val loss: 1.3912783861160278
Epoch 840, training loss: 0.010761059820652008 = 0.0037050903774797916 + 0.001 * 7.055969715118408
Epoch 840, val loss: 1.396345615386963
Epoch 850, training loss: 0.010648530907928944 = 0.0035680471919476986 + 0.001 * 7.080483436584473
Epoch 850, val loss: 1.401256799697876
Epoch 860, training loss: 0.010476301424205303 = 0.0034388566855341196 + 0.001 * 7.037444114685059
Epoch 860, val loss: 1.4059844017028809
Epoch 870, training loss: 0.01037349458783865 = 0.003317324910312891 + 0.001 * 7.056169509887695
Epoch 870, val loss: 1.410642147064209
Epoch 880, training loss: 0.010244951583445072 = 0.003202861174941063 + 0.001 * 7.042089939117432
Epoch 880, val loss: 1.4151346683502197
Epoch 890, training loss: 0.010133320465683937 = 0.003095150925219059 + 0.001 * 7.0381693840026855
Epoch 890, val loss: 1.4194871187210083
Epoch 900, training loss: 0.010024046525359154 = 0.002993630478158593 + 0.001 * 7.0304155349731445
Epoch 900, val loss: 1.4238141775131226
Epoch 910, training loss: 0.009945409372448921 = 0.0028978106565773487 + 0.001 * 7.047598838806152
Epoch 910, val loss: 1.427944302558899
Epoch 920, training loss: 0.009856383316218853 = 0.0028074888978153467 + 0.001 * 7.04889440536499
Epoch 920, val loss: 1.4319926500320435
Epoch 930, training loss: 0.009738749824464321 = 0.002722193719819188 + 0.001 * 7.016555309295654
Epoch 930, val loss: 1.435926079750061
Epoch 940, training loss: 0.009715531952679157 = 0.002641531638801098 + 0.001 * 7.073999881744385
Epoch 940, val loss: 1.4398165941238403
Epoch 950, training loss: 0.009562439285218716 = 0.002565201371908188 + 0.001 * 6.997237682342529
Epoch 950, val loss: 1.4435232877731323
Epoch 960, training loss: 0.009476274251937866 = 0.0024929672945290804 + 0.001 * 6.983306884765625
Epoch 960, val loss: 1.4472169876098633
Epoch 970, training loss: 0.009422553703188896 = 0.0024245199747383595 + 0.001 * 6.99803352355957
Epoch 970, val loss: 1.4508076906204224
Epoch 980, training loss: 0.00938406866043806 = 0.002359541831538081 + 0.001 * 7.024526596069336
Epoch 980, val loss: 1.454289436340332
Epoch 990, training loss: 0.009294138289988041 = 0.0022979031782597303 + 0.001 * 6.996234893798828
Epoch 990, val loss: 1.4575836658477783
Epoch 1000, training loss: 0.009231493808329105 = 0.0022393320687115192 + 0.001 * 6.992161273956299
Epoch 1000, val loss: 1.4610263109207153
Epoch 1010, training loss: 0.009235252626240253 = 0.0021835973020642996 + 0.001 * 7.051655292510986
Epoch 1010, val loss: 1.464337944984436
Epoch 1020, training loss: 0.009097821079194546 = 0.0021306618582457304 + 0.001 * 6.967158794403076
Epoch 1020, val loss: 1.4674055576324463
Epoch 1030, training loss: 0.009070933796465397 = 0.0020801681093871593 + 0.001 * 6.990765571594238
Epoch 1030, val loss: 1.4705522060394287
Epoch 1040, training loss: 0.00903217401355505 = 0.0020320075564086437 + 0.001 * 7.000165939331055
Epoch 1040, val loss: 1.4735811948776245
Epoch 1050, training loss: 0.008945111185312271 = 0.001986090559512377 + 0.001 * 6.959020137786865
Epoch 1050, val loss: 1.476489543914795
Epoch 1060, training loss: 0.008905421942472458 = 0.0019420708995312452 + 0.001 * 6.963350772857666
Epoch 1060, val loss: 1.4794498682022095
Epoch 1070, training loss: 0.008871527388691902 = 0.001899983617477119 + 0.001 * 6.971543312072754
Epoch 1070, val loss: 1.482182264328003
Epoch 1080, training loss: 0.00881143007427454 = 0.001859785057604313 + 0.001 * 6.9516448974609375
Epoch 1080, val loss: 1.4849603176116943
Epoch 1090, training loss: 0.008782406337559223 = 0.0018214354058727622 + 0.001 * 6.960970401763916
Epoch 1090, val loss: 1.4875264167785645
Epoch 1100, training loss: 0.008755180984735489 = 0.0017847210401669145 + 0.001 * 6.970459938049316
Epoch 1100, val loss: 1.4902697801589966
Epoch 1110, training loss: 0.008696023374795914 = 0.0017495579086244106 + 0.001 * 6.946465492248535
Epoch 1110, val loss: 1.4928765296936035
Epoch 1120, training loss: 0.00869834516197443 = 0.0017158541595563293 + 0.001 * 6.9824910163879395
Epoch 1120, val loss: 1.4954215288162231
Epoch 1130, training loss: 0.008644748479127884 = 0.0016835220158100128 + 0.001 * 6.961225986480713
Epoch 1130, val loss: 1.4977909326553345
Epoch 1140, training loss: 0.008611423894762993 = 0.001652459497563541 + 0.001 * 6.958963871002197
Epoch 1140, val loss: 1.5002232789993286
Epoch 1150, training loss: 0.008591624908149242 = 0.0016225946601480246 + 0.001 * 6.969029903411865
Epoch 1150, val loss: 1.5026448965072632
Epoch 1160, training loss: 0.00852998998016119 = 0.0015938953729346395 + 0.001 * 6.936094284057617
Epoch 1160, val loss: 1.5049508810043335
Epoch 1170, training loss: 0.008555326610803604 = 0.0015662764199078083 + 0.001 * 6.989049911499023
Epoch 1170, val loss: 1.5071905851364136
Epoch 1180, training loss: 0.008494212292134762 = 0.0015397340757772326 + 0.001 * 6.954477787017822
Epoch 1180, val loss: 1.5094767808914185
Epoch 1190, training loss: 0.008479306474328041 = 0.001514228293672204 + 0.001 * 6.9650774002075195
Epoch 1190, val loss: 1.511692762374878
Epoch 1200, training loss: 0.008412618190050125 = 0.00148964102845639 + 0.001 * 6.922976970672607
Epoch 1200, val loss: 1.5138052701950073
Epoch 1210, training loss: 0.00839919038116932 = 0.00146596843842417 + 0.001 * 6.933221340179443
Epoch 1210, val loss: 1.515913724899292
Epoch 1220, training loss: 0.008391805924475193 = 0.0014431467279791832 + 0.001 * 6.9486589431762695
Epoch 1220, val loss: 1.5179494619369507
Epoch 1230, training loss: 0.008324520662426949 = 0.0014211601810529828 + 0.001 * 6.903360366821289
Epoch 1230, val loss: 1.5199247598648071
Epoch 1240, training loss: 0.008327644318342209 = 0.0013999376678839326 + 0.001 * 6.927706241607666
Epoch 1240, val loss: 1.522003173828125
Epoch 1250, training loss: 0.008298220112919807 = 0.0013794517144560814 + 0.001 * 6.918768405914307
Epoch 1250, val loss: 1.5238940715789795
Epoch 1260, training loss: 0.008282413706183434 = 0.0013596556382253766 + 0.001 * 6.922757625579834
Epoch 1260, val loss: 1.5258221626281738
Epoch 1270, training loss: 0.008259755559265614 = 0.0013405400095507503 + 0.001 * 6.919215679168701
Epoch 1270, val loss: 1.5276715755462646
Epoch 1280, training loss: 0.008219921961426735 = 0.0013220474356785417 + 0.001 * 6.897873878479004
Epoch 1280, val loss: 1.5295220613479614
Epoch 1290, training loss: 0.008205464109778404 = 0.001304194563999772 + 0.001 * 6.90126895904541
Epoch 1290, val loss: 1.5313688516616821
Epoch 1300, training loss: 0.008196589536964893 = 0.0012868890771642327 + 0.001 * 6.909700393676758
Epoch 1300, val loss: 1.5330427885055542
Epoch 1310, training loss: 0.00821886770427227 = 0.0012700562365353107 + 0.001 * 6.948810577392578
Epoch 1310, val loss: 1.5348422527313232
Epoch 1320, training loss: 0.00815514288842678 = 0.0012537213042378426 + 0.001 * 6.901421070098877
Epoch 1320, val loss: 1.5364506244659424
Epoch 1330, training loss: 0.008160840719938278 = 0.0012379448162391782 + 0.001 * 6.922895908355713
Epoch 1330, val loss: 1.538138508796692
Epoch 1340, training loss: 0.008115106262266636 = 0.0012225339887663722 + 0.001 * 6.892572402954102
Epoch 1340, val loss: 1.539855718612671
Epoch 1350, training loss: 0.00813775323331356 = 0.0012075849808752537 + 0.001 * 6.9301676750183105
Epoch 1350, val loss: 1.5415431261062622
Epoch 1360, training loss: 0.008087520487606525 = 0.0011930601904168725 + 0.001 * 6.8944597244262695
Epoch 1360, val loss: 1.543107032775879
Epoch 1370, training loss: 0.008091254159808159 = 0.0011789491400122643 + 0.001 * 6.9123053550720215
Epoch 1370, val loss: 1.5446912050247192
Epoch 1380, training loss: 0.00806134007871151 = 0.0011651826789602637 + 0.001 * 6.8961567878723145
Epoch 1380, val loss: 1.5462570190429688
Epoch 1390, training loss: 0.008061712607741356 = 0.0011517482344061136 + 0.001 * 6.909964084625244
Epoch 1390, val loss: 1.5479241609573364
Epoch 1400, training loss: 0.008033279329538345 = 0.0011386754922568798 + 0.001 * 6.894603252410889
Epoch 1400, val loss: 1.5495232343673706
Epoch 1410, training loss: 0.00805220752954483 = 0.001125893322750926 + 0.001 * 6.926313877105713
Epoch 1410, val loss: 1.5510035753250122
Epoch 1420, training loss: 0.008006319403648376 = 0.0011134587693959475 + 0.001 * 6.892859935760498
Epoch 1420, val loss: 1.552562952041626
Epoch 1430, training loss: 0.00800693966448307 = 0.0011013300390914083 + 0.001 * 6.905609607696533
Epoch 1430, val loss: 1.5541822910308838
Epoch 1440, training loss: 0.007975010201334953 = 0.0010895292507484555 + 0.001 * 6.885480880737305
Epoch 1440, val loss: 1.5555872917175293
Epoch 1450, training loss: 0.007967725396156311 = 0.0010779708391055465 + 0.001 * 6.889754295349121
Epoch 1450, val loss: 1.557209849357605
Epoch 1460, training loss: 0.00795332808047533 = 0.001066709286533296 + 0.001 * 6.886618137359619
Epoch 1460, val loss: 1.5586836338043213
Epoch 1470, training loss: 0.00795537419617176 = 0.0010557149071246386 + 0.001 * 6.899659156799316
Epoch 1470, val loss: 1.5601882934570312
Epoch 1480, training loss: 0.007921846583485603 = 0.0010449797846376896 + 0.001 * 6.876865863800049
Epoch 1480, val loss: 1.5615888833999634
Epoch 1490, training loss: 0.007917741313576698 = 0.0010345650371164083 + 0.001 * 6.883175849914551
Epoch 1490, val loss: 1.5630896091461182
Epoch 1500, training loss: 0.007874930277466774 = 0.0010244138538837433 + 0.001 * 6.8505167961120605
Epoch 1500, val loss: 1.5645456314086914
Epoch 1510, training loss: 0.007938597351312637 = 0.0010145345004275441 + 0.001 * 6.924062252044678
Epoch 1510, val loss: 1.5659277439117432
Epoch 1520, training loss: 0.007896068505942822 = 0.0010048820404335856 + 0.001 * 6.891186714172363
Epoch 1520, val loss: 1.56735360622406
Epoch 1530, training loss: 0.007949848659336567 = 0.000995470560155809 + 0.001 * 6.9543776512146
Epoch 1530, val loss: 1.5687601566314697
Epoch 1540, training loss: 0.007849372923374176 = 0.0009863307932391763 + 0.001 * 6.863041877746582
Epoch 1540, val loss: 1.56995689868927
Epoch 1550, training loss: 0.007825345732271671 = 0.0009774068603292108 + 0.001 * 6.847938060760498
Epoch 1550, val loss: 1.5714755058288574
Epoch 1560, training loss: 0.007844692096114159 = 0.0009687021956779063 + 0.001 * 6.8759894371032715
Epoch 1560, val loss: 1.5727754831314087
Epoch 1570, training loss: 0.007875743322074413 = 0.0009602062636986375 + 0.001 * 6.915536403656006
Epoch 1570, val loss: 1.5741147994995117
Epoch 1580, training loss: 0.00782771036028862 = 0.0009519498562440276 + 0.001 * 6.875760078430176
Epoch 1580, val loss: 1.5753599405288696
Epoch 1590, training loss: 0.007790304254740477 = 0.0009439002606086433 + 0.001 * 6.846404075622559
Epoch 1590, val loss: 1.5767148733139038
Epoch 1600, training loss: 0.007774546276777983 = 0.0009360479889437556 + 0.001 * 6.838497638702393
Epoch 1600, val loss: 1.5780178308486938
Epoch 1610, training loss: 0.007778555154800415 = 0.0009283669060096145 + 0.001 * 6.8501877784729
Epoch 1610, val loss: 1.5792218446731567
Epoch 1620, training loss: 0.007772358600050211 = 0.0009208507835865021 + 0.001 * 6.851507663726807
Epoch 1620, val loss: 1.5804905891418457
Epoch 1630, training loss: 0.0078110662288963795 = 0.0009135378641076386 + 0.001 * 6.897527694702148
Epoch 1630, val loss: 1.5816807746887207
Epoch 1640, training loss: 0.00773924496024847 = 0.0009063621982932091 + 0.001 * 6.832882404327393
Epoch 1640, val loss: 1.5829157829284668
Epoch 1650, training loss: 0.007739131338894367 = 0.0008993625524453819 + 0.001 * 6.839768409729004
Epoch 1650, val loss: 1.5841646194458008
Epoch 1660, training loss: 0.007770088035613298 = 0.0008925218135118484 + 0.001 * 6.877565860748291
Epoch 1660, val loss: 1.5853959321975708
Epoch 1670, training loss: 0.0077517153695225716 = 0.0008858797955326736 + 0.001 * 6.865835189819336
Epoch 1670, val loss: 1.5865286588668823
Epoch 1680, training loss: 0.007780737709254026 = 0.0008793713641352952 + 0.001 * 6.901365756988525
Epoch 1680, val loss: 1.58761465549469
Epoch 1690, training loss: 0.007707341108471155 = 0.0008730175322853029 + 0.001 * 6.834323406219482
Epoch 1690, val loss: 1.5887045860290527
Epoch 1700, training loss: 0.007731226738542318 = 0.000866830290760845 + 0.001 * 6.864396095275879
Epoch 1700, val loss: 1.5900453329086304
Epoch 1710, training loss: 0.007730943150818348 = 0.0008607953786849976 + 0.001 * 6.870147228240967
Epoch 1710, val loss: 1.5909512042999268
Epoch 1720, training loss: 0.007694496773183346 = 0.0008548668120056391 + 0.001 * 6.839629173278809
Epoch 1720, val loss: 1.5921809673309326
Epoch 1730, training loss: 0.007669238373637199 = 0.0008490949403494596 + 0.001 * 6.82014274597168
Epoch 1730, val loss: 1.5932384729385376
Epoch 1740, training loss: 0.00769845163449645 = 0.0008434703340753913 + 0.001 * 6.854980945587158
Epoch 1740, val loss: 1.5942646265029907
Epoch 1750, training loss: 0.007642213720828295 = 0.0008379719220101833 + 0.001 * 6.80424165725708
Epoch 1750, val loss: 1.595304012298584
Epoch 1760, training loss: 0.0076719908975064754 = 0.0008326165261678398 + 0.001 * 6.83937406539917
Epoch 1760, val loss: 1.5964666604995728
Epoch 1770, training loss: 0.007702538277953863 = 0.0008274166029877961 + 0.001 * 6.875121116638184
Epoch 1770, val loss: 1.597435474395752
Epoch 1780, training loss: 0.007661687210202217 = 0.0008222971810027957 + 0.001 * 6.839389801025391
Epoch 1780, val loss: 1.5983377695083618
Epoch 1790, training loss: 0.007689278572797775 = 0.0008173019741661847 + 0.001 * 6.871975898742676
Epoch 1790, val loss: 1.5994709730148315
Epoch 1800, training loss: 0.007659101393073797 = 0.0008124229498207569 + 0.001 * 6.846678256988525
Epoch 1800, val loss: 1.6003625392913818
Epoch 1810, training loss: 0.007651092484593391 = 0.0008076393860392272 + 0.001 * 6.8434529304504395
Epoch 1810, val loss: 1.6012588739395142
Epoch 1820, training loss: 0.007618111092597246 = 0.0008029596065171063 + 0.001 * 6.815151214599609
Epoch 1820, val loss: 1.602287769317627
Epoch 1830, training loss: 0.007632122375071049 = 0.0007983851828612387 + 0.001 * 6.833736896514893
Epoch 1830, val loss: 1.6032469272613525
Epoch 1840, training loss: 0.007593212649226189 = 0.0007939318311400712 + 0.001 * 6.799280166625977
Epoch 1840, val loss: 1.6040147542953491
Epoch 1850, training loss: 0.007598854601383209 = 0.0007895918097347021 + 0.001 * 6.809262275695801
Epoch 1850, val loss: 1.6050913333892822
Epoch 1860, training loss: 0.007629305124282837 = 0.0007853152928873897 + 0.001 * 6.843989372253418
Epoch 1860, val loss: 1.6058435440063477
Epoch 1870, training loss: 0.007610440254211426 = 0.0007811369141563773 + 0.001 * 6.829302787780762
Epoch 1870, val loss: 1.6067843437194824
Epoch 1880, training loss: 0.007567829918116331 = 0.0007770268130116165 + 0.001 * 6.790802955627441
Epoch 1880, val loss: 1.6076679229736328
Epoch 1890, training loss: 0.007584943901747465 = 0.0007729981443844736 + 0.001 * 6.81194543838501
Epoch 1890, val loss: 1.6085518598556519
Epoch 1900, training loss: 0.0075782896019518375 = 0.000769073492847383 + 0.001 * 6.809215545654297
Epoch 1900, val loss: 1.609342336654663
Epoch 1910, training loss: 0.0075587863102555275 = 0.0007652363274246454 + 0.001 * 6.79355001449585
Epoch 1910, val loss: 1.6102055311203003
Epoch 1920, training loss: 0.007611446548253298 = 0.0007614848436787724 + 0.001 * 6.849961280822754
Epoch 1920, val loss: 1.6111022233963013
Epoch 1930, training loss: 0.00755787268280983 = 0.0007578139775432646 + 0.001 * 6.800058364868164
Epoch 1930, val loss: 1.6117128133773804
Epoch 1940, training loss: 0.007562635466456413 = 0.0007542220409959555 + 0.001 * 6.808413505554199
Epoch 1940, val loss: 1.6125761270523071
Epoch 1950, training loss: 0.007593159563839436 = 0.0007507144473493099 + 0.001 * 6.842444896697998
Epoch 1950, val loss: 1.6133263111114502
Epoch 1960, training loss: 0.00756039097905159 = 0.0007472569704987109 + 0.001 * 6.813133716583252
Epoch 1960, val loss: 1.6140148639678955
Epoch 1970, training loss: 0.007528707850724459 = 0.0007438743487000465 + 0.001 * 6.784832954406738
Epoch 1970, val loss: 1.614851951599121
Epoch 1980, training loss: 0.007532945834100246 = 0.0007405637297779322 + 0.001 * 6.79238224029541
Epoch 1980, val loss: 1.6155340671539307
Epoch 1990, training loss: 0.007589599583297968 = 0.0007373058469966054 + 0.001 * 6.852293491363525
Epoch 1990, val loss: 1.616413950920105
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 1.9505846500396729 = 1.9419877529144287 + 0.001 * 8.596843719482422
Epoch 0, val loss: 1.9344871044158936
Epoch 10, training loss: 1.9409093856811523 = 1.9323126077651978 + 0.001 * 8.596793174743652
Epoch 10, val loss: 1.924321174621582
Epoch 20, training loss: 1.9289592504501343 = 1.9203625917434692 + 0.001 * 8.596602439880371
Epoch 20, val loss: 1.91143798828125
Epoch 30, training loss: 1.9120179414749146 = 1.9034218788146973 + 0.001 * 8.596115112304688
Epoch 30, val loss: 1.8932851552963257
Epoch 40, training loss: 1.8871196508407593 = 1.878524899482727 + 0.001 * 8.594786643981934
Epoch 40, val loss: 1.8671624660491943
Epoch 50, training loss: 1.8531222343444824 = 1.8445318937301636 + 0.001 * 8.590291023254395
Epoch 50, val loss: 1.8334766626358032
Epoch 60, training loss: 1.8166313171386719 = 1.8080604076385498 + 0.001 * 8.570849418640137
Epoch 60, val loss: 1.8023918867111206
Epoch 70, training loss: 1.7870817184448242 = 1.7786120176315308 + 0.001 * 8.46967601776123
Epoch 70, val loss: 1.7821446657180786
Epoch 80, training loss: 1.750561237335205 = 1.742509126663208 + 0.001 * 8.052072525024414
Epoch 80, val loss: 1.7531213760375977
Epoch 90, training loss: 1.6993881464004517 = 1.6914507150650024 + 0.001 * 7.937459468841553
Epoch 90, val loss: 1.7088606357574463
Epoch 100, training loss: 1.6289913654327393 = 1.6212247610092163 + 0.001 * 7.766559600830078
Epoch 100, val loss: 1.6476799249649048
Epoch 110, training loss: 1.5408610105514526 = 1.5332437753677368 + 0.001 * 7.617201805114746
Epoch 110, val loss: 1.5738521814346313
Epoch 120, training loss: 1.4422235488891602 = 1.4346519708633423 + 0.001 * 7.571616172790527
Epoch 120, val loss: 1.493730068206787
Epoch 130, training loss: 1.3379651308059692 = 1.330438494682312 + 0.001 * 7.526588439941406
Epoch 130, val loss: 1.4108690023422241
Epoch 140, training loss: 1.2286937236785889 = 1.2212549448013306 + 0.001 * 7.438765048980713
Epoch 140, val loss: 1.3260174989700317
Epoch 150, training loss: 1.1166460514068604 = 1.1093106269836426 + 0.001 * 7.335444927215576
Epoch 150, val loss: 1.241204857826233
Epoch 160, training loss: 1.0071135759353638 = 0.9998229742050171 + 0.001 * 7.290647506713867
Epoch 160, val loss: 1.160089373588562
Epoch 170, training loss: 0.9063290357589722 = 0.8990474343299866 + 0.001 * 7.281593322753906
Epoch 170, val loss: 1.0888019800186157
Epoch 180, training loss: 0.818437397480011 = 0.8111679553985596 + 0.001 * 7.269468307495117
Epoch 180, val loss: 1.0306429862976074
Epoch 190, training loss: 0.7437432408332825 = 0.7364822626113892 + 0.001 * 7.260961532592773
Epoch 190, val loss: 0.9859128594398499
Epoch 200, training loss: 0.6797864437103271 = 0.6725337505340576 + 0.001 * 7.25269889831543
Epoch 200, val loss: 0.9526602029800415
Epoch 210, training loss: 0.6233466863632202 = 0.6161034107208252 + 0.001 * 7.243254661560059
Epoch 210, val loss: 0.9279959201812744
Epoch 220, training loss: 0.571719229221344 = 0.5644872188568115 + 0.001 * 7.232013702392578
Epoch 220, val loss: 0.9095158576965332
Epoch 230, training loss: 0.5231396555900574 = 0.5159205198287964 + 0.001 * 7.219140529632568
Epoch 230, val loss: 0.8961066007614136
Epoch 240, training loss: 0.47664332389831543 = 0.4694375991821289 + 0.001 * 7.2057271003723145
Epoch 240, val loss: 0.887348473072052
Epoch 250, training loss: 0.43199896812438965 = 0.424813449382782 + 0.001 * 7.185530662536621
Epoch 250, val loss: 0.8835731148719788
Epoch 260, training loss: 0.3895314633846283 = 0.3823518753051758 + 0.001 * 7.179602146148682
Epoch 260, val loss: 0.8849350214004517
Epoch 270, training loss: 0.34967491030693054 = 0.3425241708755493 + 0.001 * 7.150751113891602
Epoch 270, val loss: 0.891568660736084
Epoch 280, training loss: 0.3128555417060852 = 0.30571821331977844 + 0.001 * 7.137324333190918
Epoch 280, val loss: 0.9032649397850037
Epoch 290, training loss: 0.27923911809921265 = 0.27210840582847595 + 0.001 * 7.13071346282959
Epoch 290, val loss: 0.9195348024368286
Epoch 300, training loss: 0.24875199794769287 = 0.24162890017032623 + 0.001 * 7.123092174530029
Epoch 300, val loss: 0.9398117065429688
Epoch 310, training loss: 0.22130517661571503 = 0.21418584883213043 + 0.001 * 7.119322299957275
Epoch 310, val loss: 0.9634738564491272
Epoch 320, training loss: 0.19676285982131958 = 0.1896408200263977 + 0.001 * 7.122039794921875
Epoch 320, val loss: 0.9900676608085632
Epoch 330, training loss: 0.17495977878570557 = 0.16784338653087616 + 0.001 * 7.116398334503174
Epoch 330, val loss: 1.019001841545105
Epoch 340, training loss: 0.15572212636470795 = 0.1486065536737442 + 0.001 * 7.115573406219482
Epoch 340, val loss: 1.0496132373809814
Epoch 350, training loss: 0.13881637156009674 = 0.13170123100280762 + 0.001 * 7.115133285522461
Epoch 350, val loss: 1.0814858675003052
Epoch 360, training loss: 0.12401127815246582 = 0.1168968677520752 + 0.001 * 7.1144118309021
Epoch 360, val loss: 1.1141231060028076
Epoch 370, training loss: 0.11107096076011658 = 0.10395561903715134 + 0.001 * 7.115339756011963
Epoch 370, val loss: 1.1471704244613647
Epoch 380, training loss: 0.09975960105657578 = 0.09264782816171646 + 0.001 * 7.111774921417236
Epoch 380, val loss: 1.1803150177001953
Epoch 390, training loss: 0.0898749902844429 = 0.08276397734880447 + 0.001 * 7.111012935638428
Epoch 390, val loss: 1.2132011651992798
Epoch 400, training loss: 0.08123208582401276 = 0.0741148591041565 + 0.001 * 7.117227554321289
Epoch 400, val loss: 1.2457259893417358
Epoch 410, training loss: 0.0736452043056488 = 0.06653811782598495 + 0.001 * 7.107085704803467
Epoch 410, val loss: 1.2776458263397217
Epoch 420, training loss: 0.0669981837272644 = 0.059891458600759506 + 0.001 * 7.106728553771973
Epoch 420, val loss: 1.308890700340271
Epoch 430, training loss: 0.06115521490573883 = 0.05405138432979584 + 0.001 * 7.103828430175781
Epoch 430, val loss: 1.33933687210083
Epoch 440, training loss: 0.056027039885520935 = 0.04891312122344971 + 0.001 * 7.113920211791992
Epoch 440, val loss: 1.3689517974853516
Epoch 450, training loss: 0.05148715525865555 = 0.04438434913754463 + 0.001 * 7.1028056144714355
Epoch 450, val loss: 1.3976988792419434
Epoch 460, training loss: 0.04747912287712097 = 0.04038562253117561 + 0.001 * 7.093499660491943
Epoch 460, val loss: 1.425567865371704
Epoch 470, training loss: 0.04394075646996498 = 0.03684823215007782 + 0.001 * 7.092525005340576
Epoch 470, val loss: 1.4525271654129028
Epoch 480, training loss: 0.04080256447196007 = 0.03371346369385719 + 0.001 * 7.089099407196045
Epoch 480, val loss: 1.4785958528518677
Epoch 490, training loss: 0.03802094608545303 = 0.030930038541555405 + 0.001 * 7.090906143188477
Epoch 490, val loss: 1.503745436668396
Epoch 500, training loss: 0.03553570806980133 = 0.028452903032302856 + 0.001 * 7.082804203033447
Epoch 500, val loss: 1.5280345678329468
Epoch 510, training loss: 0.03331470862030983 = 0.026243718340992928 + 0.001 * 7.070990562438965
Epoch 510, val loss: 1.5514791011810303
Epoch 520, training loss: 0.03133324161171913 = 0.02426893264055252 + 0.001 * 7.064310073852539
Epoch 520, val loss: 1.574114441871643
Epoch 530, training loss: 0.029560526832938194 = 0.022499574348330498 + 0.001 * 7.060952186584473
Epoch 530, val loss: 1.5958970785140991
Epoch 540, training loss: 0.027964815497398376 = 0.02091052383184433 + 0.001 * 7.054291248321533
Epoch 540, val loss: 1.6169506311416626
Epoch 550, training loss: 0.026547666639089584 = 0.019479719921946526 + 0.001 * 7.0679473876953125
Epoch 550, val loss: 1.6372182369232178
Epoch 560, training loss: 0.025251545011997223 = 0.018188290297985077 + 0.001 * 7.063253402709961
Epoch 560, val loss: 1.6567813158035278
Epoch 570, training loss: 0.024066109210252762 = 0.01701963134109974 + 0.001 * 7.046478271484375
Epoch 570, val loss: 1.675634503364563
Epoch 580, training loss: 0.023060832172632217 = 0.015959348529577255 + 0.001 * 7.101484298706055
Epoch 580, val loss: 1.6938655376434326
Epoch 590, training loss: 0.022049160674214363 = 0.014995550736784935 + 0.001 * 7.053609848022461
Epoch 590, val loss: 1.711470603942871
Epoch 600, training loss: 0.021150698885321617 = 0.014117307960987091 + 0.001 * 7.033390522003174
Epoch 600, val loss: 1.7284793853759766
Epoch 610, training loss: 0.020374147221446037 = 0.013314974494278431 + 0.001 * 7.0591721534729
Epoch 610, val loss: 1.7449368238449097
Epoch 620, training loss: 0.01960875280201435 = 0.01258047204464674 + 0.001 * 7.028279781341553
Epoch 620, val loss: 1.7609105110168457
Epoch 630, training loss: 0.01893448829650879 = 0.011906571686267853 + 0.001 * 7.0279154777526855
Epoch 630, val loss: 1.7763280868530273
Epoch 640, training loss: 0.018320400267839432 = 0.01128685288131237 + 0.001 * 7.033546447753906
Epoch 640, val loss: 1.7912534475326538
Epoch 650, training loss: 0.017735188826918602 = 0.010715793818235397 + 0.001 * 7.019395351409912
Epoch 650, val loss: 1.8057388067245483
Epoch 660, training loss: 0.01720794104039669 = 0.010188608430325985 + 0.001 * 7.0193328857421875
Epoch 660, val loss: 1.819814682006836
Epoch 670, training loss: 0.016724977642297745 = 0.00970110297203064 + 0.001 * 7.023873805999756
Epoch 670, val loss: 1.8334555625915527
Epoch 680, training loss: 0.016269564628601074 = 0.009249459952116013 + 0.001 * 7.020103931427002
Epoch 680, val loss: 1.8466789722442627
Epoch 690, training loss: 0.015865350142121315 = 0.008830273523926735 + 0.001 * 7.03507661819458
Epoch 690, val loss: 1.8595308065414429
Epoch 700, training loss: 0.015458405017852783 = 0.008440583012998104 + 0.001 * 7.017821788787842
Epoch 700, val loss: 1.8719732761383057
Epoch 710, training loss: 0.01510251872241497 = 0.008077628910541534 + 0.001 * 7.02488899230957
Epoch 710, val loss: 1.884115219116211
Epoch 720, training loss: 0.01474490761756897 = 0.0077390954829752445 + 0.001 * 7.00581169128418
Epoch 720, val loss: 1.8958998918533325
Epoch 730, training loss: 0.014463672414422035 = 0.007422872819006443 + 0.001 * 7.040799617767334
Epoch 730, val loss: 1.9073829650878906
Epoch 740, training loss: 0.014151297509670258 = 0.00712712062522769 + 0.001 * 7.024176120758057
Epoch 740, val loss: 1.9185062646865845
Epoch 750, training loss: 0.013855879195034504 = 0.006850090343505144 + 0.001 * 7.005788326263428
Epoch 750, val loss: 1.9293791055679321
Epoch 760, training loss: 0.013592744246125221 = 0.0065902229398489 + 0.001 * 7.002520561218262
Epoch 760, val loss: 1.9399423599243164
Epoch 770, training loss: 0.01336183026432991 = 0.00634611863642931 + 0.001 * 7.015710830688477
Epoch 770, val loss: 1.9502724409103394
Epoch 780, training loss: 0.013122265227138996 = 0.0061165569350123405 + 0.001 * 7.005707740783691
Epoch 780, val loss: 1.9602835178375244
Epoch 790, training loss: 0.012934889644384384 = 0.005900438409298658 + 0.001 * 7.034450531005859
Epoch 790, val loss: 1.9700652360916138
Epoch 800, training loss: 0.01269911602139473 = 0.005696740932762623 + 0.001 * 7.002374172210693
Epoch 800, val loss: 1.9796240329742432
Epoch 810, training loss: 0.012504339218139648 = 0.005504507105797529 + 0.001 * 6.999831199645996
Epoch 810, val loss: 1.9889165163040161
Epoch 820, training loss: 0.012324540875852108 = 0.00532284751534462 + 0.001 * 7.001693248748779
Epoch 820, val loss: 1.998020887374878
Epoch 830, training loss: 0.012151727452874184 = 0.005151057615876198 + 0.001 * 7.000669002532959
Epoch 830, val loss: 2.0068631172180176
Epoch 840, training loss: 0.011980801820755005 = 0.004988460801541805 + 0.001 * 6.992341041564941
Epoch 840, val loss: 2.0155134201049805
Epoch 850, training loss: 0.011829119175672531 = 0.004834368824958801 + 0.001 * 6.994750499725342
Epoch 850, val loss: 2.0239787101745605
Epoch 860, training loss: 0.011695259250700474 = 0.0046882168389856815 + 0.001 * 7.007041931152344
Epoch 860, val loss: 2.0322320461273193
Epoch 870, training loss: 0.01153992023319006 = 0.004549451172351837 + 0.001 * 6.990468502044678
Epoch 870, val loss: 2.0402798652648926
Epoch 880, training loss: 0.01139807514846325 = 0.0044175852090120316 + 0.001 * 6.980489730834961
Epoch 880, val loss: 2.0481715202331543
Epoch 890, training loss: 0.011269841343164444 = 0.004292171448469162 + 0.001 * 6.977670192718506
Epoch 890, val loss: 2.055859088897705
Epoch 900, training loss: 0.011164810508489609 = 0.004172803834080696 + 0.001 * 6.992006301879883
Epoch 900, val loss: 2.0633997917175293
Epoch 910, training loss: 0.011035152710974216 = 0.0040590763092041016 + 0.001 * 6.976076126098633
Epoch 910, val loss: 2.070746421813965
Epoch 920, training loss: 0.010935395956039429 = 0.003950656857341528 + 0.001 * 6.984738349914551
Epoch 920, val loss: 2.0779531002044678
Epoch 930, training loss: 0.010834917426109314 = 0.00384720996953547 + 0.001 * 6.987707138061523
Epoch 930, val loss: 2.0850071907043457
Epoch 940, training loss: 0.010731634683907032 = 0.003748436225578189 + 0.001 * 6.9831976890563965
Epoch 940, val loss: 2.0919032096862793
Epoch 950, training loss: 0.01062154769897461 = 0.0036540695000439882 + 0.001 * 6.967478275299072
Epoch 950, val loss: 2.098611354827881
Epoch 960, training loss: 0.010534638538956642 = 0.0035638476256281137 + 0.001 * 6.970790863037109
Epoch 960, val loss: 2.1051981449127197
Epoch 970, training loss: 0.010440314188599586 = 0.0034775412641465664 + 0.001 * 6.962772846221924
Epoch 970, val loss: 2.1116387844085693
Epoch 980, training loss: 0.01039110403507948 = 0.0033949152566492558 + 0.001 * 6.996188640594482
Epoch 980, val loss: 2.1179513931274414
Epoch 990, training loss: 0.010282071307301521 = 0.003315769834443927 + 0.001 * 6.966301441192627
Epoch 990, val loss: 2.1241021156311035
Epoch 1000, training loss: 0.010223884135484695 = 0.003239918267354369 + 0.001 * 6.9839653968811035
Epoch 1000, val loss: 2.1301684379577637
Epoch 1010, training loss: 0.010152137838304043 = 0.0031671824399381876 + 0.001 * 6.984954833984375
Epoch 1010, val loss: 2.1360666751861572
Epoch 1020, training loss: 0.010071773082017899 = 0.003097382141277194 + 0.001 * 6.974390506744385
Epoch 1020, val loss: 2.141873836517334
Epoch 1030, training loss: 0.009988770820200443 = 0.003030360210686922 + 0.001 * 6.958410263061523
Epoch 1030, val loss: 2.1475162506103516
Epoch 1040, training loss: 0.009937090799212456 = 0.002965975319966674 + 0.001 * 6.971115589141846
Epoch 1040, val loss: 2.1531150341033936
Epoch 1050, training loss: 0.009889370761811733 = 0.002904085209593177 + 0.001 * 6.98528528213501
Epoch 1050, val loss: 2.1585474014282227
Epoch 1060, training loss: 0.009824812412261963 = 0.002844571601599455 + 0.001 * 6.9802398681640625
Epoch 1060, val loss: 2.163877248764038
Epoch 1070, training loss: 0.009739032946527004 = 0.0027873103972524405 + 0.001 * 6.951722145080566
Epoch 1070, val loss: 2.1690714359283447
Epoch 1080, training loss: 0.009687062352895737 = 0.002732196357101202 + 0.001 * 6.954865455627441
Epoch 1080, val loss: 2.174189805984497
Epoch 1090, training loss: 0.009636511094868183 = 0.0026791137643158436 + 0.001 * 6.957396984100342
Epoch 1090, val loss: 2.17919659614563
Epoch 1100, training loss: 0.00962500087916851 = 0.0026279683224856853 + 0.001 * 6.997032642364502
Epoch 1100, val loss: 2.1841607093811035
Epoch 1110, training loss: 0.009526201523840427 = 0.0025786494370549917 + 0.001 * 6.947551727294922
Epoch 1110, val loss: 2.188944101333618
Epoch 1120, training loss: 0.009495105594396591 = 0.002531103091314435 + 0.001 * 6.96400260925293
Epoch 1120, val loss: 2.1936261653900146
Epoch 1130, training loss: 0.009436983615159988 = 0.0024852128699421883 + 0.001 * 6.951770305633545
Epoch 1130, val loss: 2.198241710662842
Epoch 1140, training loss: 0.009376141242682934 = 0.002440922660753131 + 0.001 * 6.935218334197998
Epoch 1140, val loss: 2.202763557434082
Epoch 1150, training loss: 0.00933081191033125 = 0.002398155862465501 + 0.001 * 6.932655334472656
Epoch 1150, val loss: 2.207226514816284
Epoch 1160, training loss: 0.009324900805950165 = 0.0023568454198539257 + 0.001 * 6.96805477142334
Epoch 1160, val loss: 2.2115705013275146
Epoch 1170, training loss: 0.009285558946430683 = 0.002316912403330207 + 0.001 * 6.968646049499512
Epoch 1170, val loss: 2.215811014175415
Epoch 1180, training loss: 0.009236158803105354 = 0.0022783190943300724 + 0.001 * 6.957839012145996
Epoch 1180, val loss: 2.219972848892212
Epoch 1190, training loss: 0.009196897968649864 = 0.0022409972734749317 + 0.001 * 6.955900192260742
Epoch 1190, val loss: 2.224088191986084
Epoch 1200, training loss: 0.009141596034169197 = 0.002204903867095709 + 0.001 * 6.936692237854004
Epoch 1200, val loss: 2.2280704975128174
Epoch 1210, training loss: 0.009103875607252121 = 0.0021699643693864346 + 0.001 * 6.933910369873047
Epoch 1210, val loss: 2.2320010662078857
Epoch 1220, training loss: 0.009060212410986423 = 0.002136145019903779 + 0.001 * 6.924067497253418
Epoch 1220, val loss: 2.2358720302581787
Epoch 1230, training loss: 0.009043371304869652 = 0.0021034013479948044 + 0.001 * 6.939969539642334
Epoch 1230, val loss: 2.2396671772003174
Epoch 1240, training loss: 0.0090002017095685 = 0.002071683993563056 + 0.001 * 6.9285173416137695
Epoch 1240, val loss: 2.2433371543884277
Epoch 1250, training loss: 0.008981284685432911 = 0.0020409475546330214 + 0.001 * 6.940337181091309
Epoch 1250, val loss: 2.246981620788574
Epoch 1260, training loss: 0.008924976922571659 = 0.0020111443009227514 + 0.001 * 6.913832187652588
Epoch 1260, val loss: 2.2505252361297607
Epoch 1270, training loss: 0.008903300389647484 = 0.0019822465255856514 + 0.001 * 6.921053409576416
Epoch 1270, val loss: 2.2540009021759033
Epoch 1280, training loss: 0.00888795591890812 = 0.001954213250428438 + 0.001 * 6.933743000030518
Epoch 1280, val loss: 2.2574002742767334
Epoch 1290, training loss: 0.008858822286128998 = 0.0019269991898909211 + 0.001 * 6.931822299957275
Epoch 1290, val loss: 2.2607481479644775
Epoch 1300, training loss: 0.00885247066617012 = 0.0019006074871867895 + 0.001 * 6.951862812042236
Epoch 1300, val loss: 2.2640368938446045
Epoch 1310, training loss: 0.008812898769974709 = 0.0018749871524050832 + 0.001 * 6.937911033630371
Epoch 1310, val loss: 2.2671756744384766
Epoch 1320, training loss: 0.0087675666436553 = 0.0018501064041629434 + 0.001 * 6.917459487915039
Epoch 1320, val loss: 2.2703044414520264
Epoch 1330, training loss: 0.008746054023504257 = 0.001825941726565361 + 0.001 * 6.920112133026123
Epoch 1330, val loss: 2.273411989212036
Epoch 1340, training loss: 0.008769242092967033 = 0.0018024594755843282 + 0.001 * 6.966782093048096
Epoch 1340, val loss: 2.276430368423462
Epoch 1350, training loss: 0.008703808300197124 = 0.001779639278538525 + 0.001 * 6.924168586730957
Epoch 1350, val loss: 2.2793469429016113
Epoch 1360, training loss: 0.008675038814544678 = 0.0017574378289282322 + 0.001 * 6.917600631713867
Epoch 1360, val loss: 2.2822394371032715
Epoch 1370, training loss: 0.008637097664177418 = 0.0017358532641083002 + 0.001 * 6.901243686676025
Epoch 1370, val loss: 2.2850565910339355
Epoch 1380, training loss: 0.0086243050172925 = 0.001714851357974112 + 0.001 * 6.909453392028809
Epoch 1380, val loss: 2.2878363132476807
Epoch 1390, training loss: 0.008584802970290184 = 0.0016944159287959337 + 0.001 * 6.890387058258057
Epoch 1390, val loss: 2.2905728816986084
Epoch 1400, training loss: 0.008615530095994473 = 0.0016745233442634344 + 0.001 * 6.941006660461426
Epoch 1400, val loss: 2.2932658195495605
Epoch 1410, training loss: 0.008570373989641666 = 0.0016551523003727198 + 0.001 * 6.915221214294434
Epoch 1410, val loss: 2.295819044113159
Epoch 1420, training loss: 0.008531277067959309 = 0.0016362922033295035 + 0.001 * 6.894984722137451
Epoch 1420, val loss: 2.2983529567718506
Epoch 1430, training loss: 0.00852150283753872 = 0.0016179373487830162 + 0.001 * 6.903565406799316
Epoch 1430, val loss: 2.3008456230163574
Epoch 1440, training loss: 0.008483868092298508 = 0.0016000563045963645 + 0.001 * 6.883811950683594
Epoch 1440, val loss: 2.303313970565796
Epoch 1450, training loss: 0.0084753492847085 = 0.001582635217346251 + 0.001 * 6.89271354675293
Epoch 1450, val loss: 2.305680990219116
Epoch 1460, training loss: 0.008459053002297878 = 0.0015656454488635063 + 0.001 * 6.893407344818115
Epoch 1460, val loss: 2.308058023452759
Epoch 1470, training loss: 0.00844678282737732 = 0.001549092005006969 + 0.001 * 6.897690773010254
Epoch 1470, val loss: 2.3103222846984863
Epoch 1480, training loss: 0.00844565313309431 = 0.001532953348942101 + 0.001 * 6.912699222564697
Epoch 1480, val loss: 2.312540054321289
Epoch 1490, training loss: 0.008428633213043213 = 0.0015172145795077085 + 0.001 * 6.911418437957764
Epoch 1490, val loss: 2.3147449493408203
Epoch 1500, training loss: 0.00840611569583416 = 0.0015018575359135866 + 0.001 * 6.904257774353027
Epoch 1500, val loss: 2.3168675899505615
Epoch 1510, training loss: 0.008383430540561676 = 0.0014868766302242875 + 0.001 * 6.8965535163879395
Epoch 1510, val loss: 2.3189523220062256
Epoch 1520, training loss: 0.008362999185919762 = 0.0014722681371495128 + 0.001 * 6.890730381011963
Epoch 1520, val loss: 2.320991277694702
Epoch 1530, training loss: 0.008330995216965675 = 0.0014579929411411285 + 0.001 * 6.873002052307129
Epoch 1530, val loss: 2.3229422569274902
Epoch 1540, training loss: 0.008318636566400528 = 0.001444065710529685 + 0.001 * 6.874570846557617
Epoch 1540, val loss: 2.324949026107788
Epoch 1550, training loss: 0.008313708938658237 = 0.0014304638607427478 + 0.001 * 6.88324499130249
Epoch 1550, val loss: 2.3268496990203857
Epoch 1560, training loss: 0.008310182020068169 = 0.0014171822695061564 + 0.001 * 6.892999172210693
Epoch 1560, val loss: 2.3287460803985596
Epoch 1570, training loss: 0.008314970880746841 = 0.001404203474521637 + 0.001 * 6.910767555236816
Epoch 1570, val loss: 2.3304736614227295
Epoch 1580, training loss: 0.008259494788944721 = 0.0013915227027609944 + 0.001 * 6.867971897125244
Epoch 1580, val loss: 2.3322579860687256
Epoch 1590, training loss: 0.008240464143455029 = 0.0013791313394904137 + 0.001 * 6.861332416534424
Epoch 1590, val loss: 2.3339786529541016
Epoch 1600, training loss: 0.008249281905591488 = 0.0013670262414962053 + 0.001 * 6.882255554199219
Epoch 1600, val loss: 2.335747241973877
Epoch 1610, training loss: 0.00822384376078844 = 0.0013551936717703938 + 0.001 * 6.868649959564209
Epoch 1610, val loss: 2.3373711109161377
Epoch 1620, training loss: 0.00820096954703331 = 0.0013436288572847843 + 0.001 * 6.8573408126831055
Epoch 1620, val loss: 2.3389217853546143
Epoch 1630, training loss: 0.008199959062039852 = 0.0013323213206604123 + 0.001 * 6.867637634277344
Epoch 1630, val loss: 2.340485095977783
Epoch 1640, training loss: 0.008185142651200294 = 0.0013212589547038078 + 0.001 * 6.8638834953308105
Epoch 1640, val loss: 2.3421077728271484
Epoch 1650, training loss: 0.008183605037629604 = 0.0013104365207254887 + 0.001 * 6.873167991638184
Epoch 1650, val loss: 2.343611001968384
Epoch 1660, training loss: 0.008225745521485806 = 0.001299853902310133 + 0.001 * 6.925891399383545
Epoch 1660, val loss: 2.3450491428375244
Epoch 1670, training loss: 0.00815040897578001 = 0.0012894774554297328 + 0.001 * 6.860931396484375
Epoch 1670, val loss: 2.3464157581329346
Epoch 1680, training loss: 0.008137914352118969 = 0.0012793351197615266 + 0.001 * 6.858578681945801
Epoch 1680, val loss: 2.347811222076416
Epoch 1690, training loss: 0.008121567778289318 = 0.001269404892809689 + 0.001 * 6.8521623611450195
Epoch 1690, val loss: 2.3492238521575928
Epoch 1700, training loss: 0.008113451302051544 = 0.0012596787419170141 + 0.001 * 6.853772163391113
Epoch 1700, val loss: 2.350557565689087
Epoch 1710, training loss: 0.008096044883131981 = 0.0012501454912126064 + 0.001 * 6.8458991050720215
Epoch 1710, val loss: 2.351806640625
Epoch 1720, training loss: 0.008078763261437416 = 0.0012408148031681776 + 0.001 * 6.837948322296143
Epoch 1720, val loss: 2.3529982566833496
Epoch 1730, training loss: 0.008086650632321835 = 0.001231672358699143 + 0.001 * 6.854978084564209
Epoch 1730, val loss: 2.354330062866211
Epoch 1740, training loss: 0.008091910742223263 = 0.0012227095430716872 + 0.001 * 6.869201183319092
Epoch 1740, val loss: 2.355478525161743
Epoch 1750, training loss: 0.008121434599161148 = 0.0012139416066929698 + 0.001 * 6.907492637634277
Epoch 1750, val loss: 2.356663703918457
Epoch 1760, training loss: 0.008069305680692196 = 0.0012053370010107756 + 0.001 * 6.863968372344971
Epoch 1760, val loss: 2.35780668258667
Epoch 1770, training loss: 0.00803357269614935 = 0.0011968988692387938 + 0.001 * 6.836673259735107
Epoch 1770, val loss: 2.358818531036377
Epoch 1780, training loss: 0.008038162253797054 = 0.0011886266293004155 + 0.001 * 6.8495354652404785
Epoch 1780, val loss: 2.3599579334259033
Epoch 1790, training loss: 0.008042498491704464 = 0.0011805201647803187 + 0.001 * 6.861978530883789
Epoch 1790, val loss: 2.36100697517395
Epoch 1800, training loss: 0.008057856932282448 = 0.0011725731892511249 + 0.001 * 6.885283470153809
Epoch 1800, val loss: 2.362025260925293
Epoch 1810, training loss: 0.008050506934523582 = 0.0011647648643702269 + 0.001 * 6.8857421875
Epoch 1810, val loss: 2.362894296646118
Epoch 1820, training loss: 0.008009479381144047 = 0.0011571060167625546 + 0.001 * 6.852373123168945
Epoch 1820, val loss: 2.363769054412842
Epoch 1830, training loss: 0.007988810539245605 = 0.0011495841899886727 + 0.001 * 6.839226245880127
Epoch 1830, val loss: 2.3647146224975586
Epoch 1840, training loss: 0.007968162186443806 = 0.0011422105599194765 + 0.001 * 6.825951099395752
Epoch 1840, val loss: 2.3657162189483643
Epoch 1850, training loss: 0.007978731766343117 = 0.0011349719716235995 + 0.001 * 6.843760013580322
Epoch 1850, val loss: 2.3666210174560547
Epoch 1860, training loss: 0.007977509871125221 = 0.0011278516612946987 + 0.001 * 6.849658489227295
Epoch 1860, val loss: 2.367372512817383
Epoch 1870, training loss: 0.00797849427908659 = 0.0011208647629246116 + 0.001 * 6.857629299163818
Epoch 1870, val loss: 2.3681344985961914
Epoch 1880, training loss: 0.00797452311962843 = 0.001114011975005269 + 0.001 * 6.86051082611084
Epoch 1880, val loss: 2.3689470291137695
Epoch 1890, training loss: 0.007963800802826881 = 0.001107267220504582 + 0.001 * 6.856533050537109
Epoch 1890, val loss: 2.3696985244750977
Epoch 1900, training loss: 0.00794477853924036 = 0.0011006563436239958 + 0.001 * 6.84412145614624
Epoch 1900, val loss: 2.370392322540283
Epoch 1910, training loss: 0.007894596084952354 = 0.0010941507061943412 + 0.001 * 6.800445079803467
Epoch 1910, val loss: 2.3710992336273193
Epoch 1920, training loss: 0.00790893193334341 = 0.0010877643944695592 + 0.001 * 6.8211669921875
Epoch 1920, val loss: 2.371795892715454
Epoch 1930, training loss: 0.007925434038043022 = 0.00108148658182472 + 0.001 * 6.843946933746338
Epoch 1930, val loss: 2.3724563121795654
Epoch 1940, training loss: 0.007938389666378498 = 0.001075311447493732 + 0.001 * 6.863077640533447
Epoch 1940, val loss: 2.373072624206543
Epoch 1950, training loss: 0.007924502715468407 = 0.001069237245246768 + 0.001 * 6.855265140533447
Epoch 1950, val loss: 2.373555898666382
Epoch 1960, training loss: 0.007900942116975784 = 0.001063257222995162 + 0.001 * 6.837684154510498
Epoch 1960, val loss: 2.3741495609283447
Epoch 1970, training loss: 0.00787398125976324 = 0.0010573730105534196 + 0.001 * 6.816607475280762
Epoch 1970, val loss: 2.374699831008911
Epoch 1980, training loss: 0.00789837259799242 = 0.0010515882167965174 + 0.001 * 6.846783638000488
Epoch 1980, val loss: 2.3752408027648926
Epoch 1990, training loss: 0.00790059007704258 = 0.0010459022596478462 + 0.001 * 6.8546881675720215
Epoch 1990, val loss: 2.375767230987549
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 1.9740525484085083 = 1.9654556512832642 + 0.001 * 8.596855163574219
Epoch 0, val loss: 1.9706170558929443
Epoch 10, training loss: 1.9635424613952637 = 1.954945683479309 + 0.001 * 8.596813201904297
Epoch 10, val loss: 1.9603312015533447
Epoch 20, training loss: 1.950527548789978 = 1.941930890083313 + 0.001 * 8.596668243408203
Epoch 20, val loss: 1.9471408128738403
Epoch 30, training loss: 1.9321457147598267 = 1.9235494136810303 + 0.001 * 8.596334457397461
Epoch 30, val loss: 1.928003191947937
Epoch 40, training loss: 1.9047988653182983 = 1.8962033987045288 + 0.001 * 8.595452308654785
Epoch 40, val loss: 1.8995184898376465
Epoch 50, training loss: 1.8666491508483887 = 1.8580564260482788 + 0.001 * 8.592700004577637
Epoch 50, val loss: 1.8611557483673096
Epoch 60, training loss: 1.825492024421692 = 1.8169094324111938 + 0.001 * 8.582562446594238
Epoch 60, val loss: 1.823869228363037
Epoch 70, training loss: 1.7950431108474731 = 1.7865004539489746 + 0.001 * 8.5426607131958
Epoch 70, val loss: 1.7996195554733276
Epoch 80, training loss: 1.7611641883850098 = 1.752834439277649 + 0.001 * 8.329728126525879
Epoch 80, val loss: 1.7698540687561035
Epoch 90, training loss: 1.715048909187317 = 1.7069851160049438 + 0.001 * 8.063736915588379
Epoch 90, val loss: 1.7295550107955933
Epoch 100, training loss: 1.6502741575241089 = 1.6422834396362305 + 0.001 * 7.990663528442383
Epoch 100, val loss: 1.67451012134552
Epoch 110, training loss: 1.5661134719848633 = 1.558220386505127 + 0.001 * 7.893110275268555
Epoch 110, val loss: 1.6051486730575562
Epoch 120, training loss: 1.4720031023025513 = 1.4642807245254517 + 0.001 * 7.72239351272583
Epoch 120, val loss: 1.5290342569351196
Epoch 130, training loss: 1.3777263164520264 = 1.3701897859573364 + 0.001 * 7.536553859710693
Epoch 130, val loss: 1.4548670053482056
Epoch 140, training loss: 1.2853435277938843 = 1.27789306640625 + 0.001 * 7.450444221496582
Epoch 140, val loss: 1.3842047452926636
Epoch 150, training loss: 1.1947146654129028 = 1.1873544454574585 + 0.001 * 7.36025333404541
Epoch 150, val loss: 1.3160673379898071
Epoch 160, training loss: 1.1084342002868652 = 1.101142168045044 + 0.001 * 7.292057037353516
Epoch 160, val loss: 1.2525634765625
Epoch 170, training loss: 1.0293331146240234 = 1.0220754146575928 + 0.001 * 7.25771951675415
Epoch 170, val loss: 1.1947723627090454
Epoch 180, training loss: 0.9578319191932678 = 0.9505981802940369 + 0.001 * 7.2337646484375
Epoch 180, val loss: 1.143237590789795
Epoch 190, training loss: 0.891910195350647 = 0.8847033977508545 + 0.001 * 7.206826210021973
Epoch 190, val loss: 1.0959817171096802
Epoch 200, training loss: 0.828760027885437 = 0.8215852975845337 + 0.001 * 7.174726486206055
Epoch 200, val loss: 1.0506144762039185
Epoch 210, training loss: 0.7662851214408875 = 0.7591411471366882 + 0.001 * 7.143950462341309
Epoch 210, val loss: 1.005969762802124
Epoch 220, training loss: 0.7036766409873962 = 0.6965475082397461 + 0.001 * 7.129143238067627
Epoch 220, val loss: 0.9619672894477844
Epoch 230, training loss: 0.6411436200141907 = 0.6340241432189941 + 0.001 * 7.119501113891602
Epoch 230, val loss: 0.9193419814109802
Epoch 240, training loss: 0.5790970921516418 = 0.5719785690307617 + 0.001 * 7.118504524230957
Epoch 240, val loss: 0.8791051506996155
Epoch 250, training loss: 0.5178949236869812 = 0.5107764005661011 + 0.001 * 7.118495464324951
Epoch 250, val loss: 0.8417989015579224
Epoch 260, training loss: 0.4583788812160492 = 0.4512609839439392 + 0.001 * 7.117904186248779
Epoch 260, val loss: 0.8079042434692383
Epoch 270, training loss: 0.40171998739242554 = 0.3946031630039215 + 0.001 * 7.1168107986450195
Epoch 270, val loss: 0.7777622938156128
Epoch 280, training loss: 0.3490470051765442 = 0.34193155169487 + 0.001 * 7.115448951721191
Epoch 280, val loss: 0.7516699433326721
Epoch 290, training loss: 0.30117639899253845 = 0.29406237602233887 + 0.001 * 7.114020347595215
Epoch 290, val loss: 0.7298205494880676
Epoch 300, training loss: 0.25854265689849854 = 0.25143003463745117 + 0.001 * 7.112621784210205
Epoch 300, val loss: 0.7121085524559021
Epoch 310, training loss: 0.22128111124038696 = 0.21416977047920227 + 0.001 * 7.111335277557373
Epoch 310, val loss: 0.6984164714813232
Epoch 320, training loss: 0.18929381668567657 = 0.1821836531162262 + 0.001 * 7.110163688659668
Epoch 320, val loss: 0.6885241866111755
Epoch 330, training loss: 0.16227370500564575 = 0.15516237914562225 + 0.001 * 7.111320972442627
Epoch 330, val loss: 0.6822735667228699
Epoch 340, training loss: 0.1397123634815216 = 0.13260392844676971 + 0.001 * 7.108432769775391
Epoch 340, val loss: 0.6793325543403625
Epoch 350, training loss: 0.12099723517894745 = 0.11388940364122391 + 0.001 * 7.107834815979004
Epoch 350, val loss: 0.6793201565742493
Epoch 360, training loss: 0.10549104958772659 = 0.09838426858186722 + 0.001 * 7.106783390045166
Epoch 360, val loss: 0.6816918849945068
Epoch 370, training loss: 0.09260918945074081 = 0.08550316840410233 + 0.001 * 7.106023788452148
Epoch 370, val loss: 0.6859123706817627
Epoch 380, training loss: 0.08185677230358124 = 0.07475104182958603 + 0.001 * 7.105731010437012
Epoch 380, val loss: 0.6914644837379456
Epoch 390, training loss: 0.0728292241692543 = 0.06572532653808594 + 0.001 * 7.103899955749512
Epoch 390, val loss: 0.6980278491973877
Epoch 400, training loss: 0.06520703434944153 = 0.0581040158867836 + 0.001 * 7.103016376495361
Epoch 400, val loss: 0.7052955031394958
Epoch 410, training loss: 0.05873929336667061 = 0.05163232609629631 + 0.001 * 7.106967449188232
Epoch 410, val loss: 0.7129461169242859
Epoch 420, training loss: 0.05320962890982628 = 0.046106599271297455 + 0.001 * 7.103030681610107
Epoch 420, val loss: 0.7208613753318787
Epoch 430, training loss: 0.04846656695008278 = 0.041365042328834534 + 0.001 * 7.101524829864502
Epoch 430, val loss: 0.7288593053817749
Epoch 440, training loss: 0.04437486082315445 = 0.03727545961737633 + 0.001 * 7.099400043487549
Epoch 440, val loss: 0.7368729114532471
Epoch 450, training loss: 0.04082787409424782 = 0.0337294302880764 + 0.001 * 7.098444938659668
Epoch 450, val loss: 0.7448221445083618
Epoch 460, training loss: 0.03773818537592888 = 0.0306350439786911 + 0.001 * 7.103140354156494
Epoch 460, val loss: 0.7526600360870361
Epoch 470, training loss: 0.03501587733626366 = 0.027918366715312004 + 0.001 * 7.09751033782959
Epoch 470, val loss: 0.7603450417518616
Epoch 480, training loss: 0.032618433237075806 = 0.025522232055664062 + 0.001 * 7.0962018966674805
Epoch 480, val loss: 0.7679087519645691
Epoch 490, training loss: 0.030497802421450615 = 0.023401638492941856 + 0.001 * 7.096163272857666
Epoch 490, val loss: 0.7752947211265564
Epoch 500, training loss: 0.028613360598683357 = 0.021519403904676437 + 0.001 * 7.09395694732666
Epoch 500, val loss: 0.7825137376785278
Epoch 510, training loss: 0.026948750019073486 = 0.019844507798552513 + 0.001 * 7.104240894317627
Epoch 510, val loss: 0.7895477414131165
Epoch 520, training loss: 0.02544424869120121 = 0.018349891528487206 + 0.001 * 7.094356536865234
Epoch 520, val loss: 0.796393871307373
Epoch 530, training loss: 0.02410557121038437 = 0.017012998461723328 + 0.001 * 7.092571258544922
Epoch 530, val loss: 0.8030749559402466
Epoch 540, training loss: 0.022906102240085602 = 0.01581409014761448 + 0.001 * 7.092011451721191
Epoch 540, val loss: 0.8095594048500061
Epoch 550, training loss: 0.02182672545313835 = 0.014736073091626167 + 0.001 * 7.090652942657471
Epoch 550, val loss: 0.815881073474884
Epoch 560, training loss: 0.020853862166404724 = 0.013764183036983013 + 0.001 * 7.08967924118042
Epoch 560, val loss: 0.8220680952072144
Epoch 570, training loss: 0.019975343719124794 = 0.01288567017763853 + 0.001 * 7.0896735191345215
Epoch 570, val loss: 0.8280494213104248
Epoch 580, training loss: 0.019181067124009132 = 0.012089437805116177 + 0.001 * 7.091629505157471
Epoch 580, val loss: 0.8338959813117981
Epoch 590, training loss: 0.018456408753991127 = 0.01136587094515562 + 0.001 * 7.0905375480651855
Epoch 590, val loss: 0.8395407795906067
Epoch 600, training loss: 0.017795125022530556 = 0.010706668719649315 + 0.001 * 7.088456630706787
Epoch 600, val loss: 0.8450577855110168
Epoch 610, training loss: 0.01719125360250473 = 0.010104612447321415 + 0.001 * 7.086641311645508
Epoch 610, val loss: 0.850418210029602
Epoch 620, training loss: 0.016651302576065063 = 0.009553495794534683 + 0.001 * 7.097806930541992
Epoch 620, val loss: 0.855631411075592
Epoch 630, training loss: 0.016134710982441902 = 0.009047993458807468 + 0.001 * 7.08671760559082
Epoch 630, val loss: 0.8607149124145508
Epoch 640, training loss: 0.015668384730815887 = 0.008583218790590763 + 0.001 * 7.085165023803711
Epoch 640, val loss: 0.8656604290008545
Epoch 650, training loss: 0.01524432934820652 = 0.008154993876814842 + 0.001 * 7.089334487915039
Epoch 650, val loss: 0.8704786896705627
Epoch 660, training loss: 0.014845708385109901 = 0.007759641390293837 + 0.001 * 7.086067199707031
Epoch 660, val loss: 0.8751552700996399
Epoch 670, training loss: 0.014482347294688225 = 0.007393925916403532 + 0.001 * 7.08842134475708
Epoch 670, val loss: 0.8797101378440857
Epoch 680, training loss: 0.014136619865894318 = 0.007054985035210848 + 0.001 * 7.081634998321533
Epoch 680, val loss: 0.8841658234596252
Epoch 690, training loss: 0.013822060078382492 = 0.006740332581102848 + 0.001 * 7.081726551055908
Epoch 690, val loss: 0.8885122537612915
Epoch 700, training loss: 0.013552054762840271 = 0.006447731051594019 + 0.001 * 7.104323387145996
Epoch 700, val loss: 0.8927345871925354
Epoch 710, training loss: 0.013254680670797825 = 0.006175261922180653 + 0.001 * 7.079418182373047
Epoch 710, val loss: 0.8968630433082581
Epoch 720, training loss: 0.01299969106912613 = 0.005921078380197287 + 0.001 * 7.078612327575684
Epoch 720, val loss: 0.9008951187133789
Epoch 730, training loss: 0.012771280482411385 = 0.005683593917638063 + 0.001 * 7.087686538696289
Epoch 730, val loss: 0.9048346281051636
Epoch 740, training loss: 0.012536836788058281 = 0.005461389664560556 + 0.001 * 7.075446128845215
Epoch 740, val loss: 0.9086751937866211
Epoch 750, training loss: 0.012326091527938843 = 0.005253178533166647 + 0.001 * 7.07291316986084
Epoch 750, val loss: 0.912424623966217
Epoch 760, training loss: 0.012137848883867264 = 0.0050578247755765915 + 0.001 * 7.080023765563965
Epoch 760, val loss: 0.916091799736023
Epoch 770, training loss: 0.011946332640945911 = 0.004874273668974638 + 0.001 * 7.07205867767334
Epoch 770, val loss: 0.9196920990943909
Epoch 780, training loss: 0.011770267970860004 = 0.004701588302850723 + 0.001 * 7.068679332733154
Epoch 780, val loss: 0.9232025742530823
Epoch 790, training loss: 0.011648201383650303 = 0.0045389458537101746 + 0.001 * 7.109255313873291
Epoch 790, val loss: 0.9266466498374939
Epoch 800, training loss: 0.011449906975030899 = 0.0043856739066541195 + 0.001 * 7.064232349395752
Epoch 800, val loss: 0.9299828410148621
Epoch 810, training loss: 0.011306259781122208 = 0.004240972455590963 + 0.001 * 7.065286636352539
Epoch 810, val loss: 0.9332721829414368
Epoch 820, training loss: 0.011172935366630554 = 0.004104210063815117 + 0.001 * 7.0687255859375
Epoch 820, val loss: 0.9365032911300659
Epoch 830, training loss: 0.011027691885828972 = 0.003974819555878639 + 0.001 * 7.052872657775879
Epoch 830, val loss: 0.9396663308143616
Epoch 840, training loss: 0.010913826525211334 = 0.003852267749607563 + 0.001 * 7.061557769775391
Epoch 840, val loss: 0.9427406191825867
Epoch 850, training loss: 0.01078250166028738 = 0.00373612972907722 + 0.001 * 7.0463714599609375
Epoch 850, val loss: 0.9457704424858093
Epoch 860, training loss: 0.010679712519049644 = 0.0036260082852095366 + 0.001 * 7.053703784942627
Epoch 860, val loss: 0.948746383190155
Epoch 870, training loss: 0.010582273826003075 = 0.003521448466926813 + 0.001 * 7.060825347900391
Epoch 870, val loss: 0.9516206979751587
Epoch 880, training loss: 0.010473519563674927 = 0.003422053763642907 + 0.001 * 7.0514655113220215
Epoch 880, val loss: 0.9544622302055359
Epoch 890, training loss: 0.010394829325377941 = 0.0033275082241743803 + 0.001 * 7.067320823669434
Epoch 890, val loss: 0.9572381973266602
Epoch 900, training loss: 0.01027965173125267 = 0.00323748798109591 + 0.001 * 7.042163848876953
Epoch 900, val loss: 0.9599610567092896
Epoch 910, training loss: 0.010188359767198563 = 0.0031516561284661293 + 0.001 * 7.036703109741211
Epoch 910, val loss: 0.9626323580741882
Epoch 920, training loss: 0.010133879259228706 = 0.0030696592293679714 + 0.001 * 7.0642194747924805
Epoch 920, val loss: 0.9652807116508484
Epoch 930, training loss: 0.010040330700576305 = 0.002991131506860256 + 0.001 * 7.049198627471924
Epoch 930, val loss: 0.9678364992141724
Epoch 940, training loss: 0.009929561987519264 = 0.0029154461808502674 + 0.001 * 7.014115333557129
Epoch 940, val loss: 0.9703748822212219
Epoch 950, training loss: 0.009889147244393826 = 0.002842085435986519 + 0.001 * 7.047061443328857
Epoch 950, val loss: 0.9729132652282715
Epoch 960, training loss: 0.00979587808251381 = 0.0027706818655133247 + 0.001 * 7.025195598602295
Epoch 960, val loss: 0.9753745794296265
Epoch 970, training loss: 0.00970919243991375 = 0.0027008242905139923 + 0.001 * 7.008368492126465
Epoch 970, val loss: 0.9778781533241272
Epoch 980, training loss: 0.009663533419370651 = 0.002632523188367486 + 0.001 * 7.031009674072266
Epoch 980, val loss: 0.9803430438041687
Epoch 990, training loss: 0.00957402028143406 = 0.002565824892371893 + 0.001 * 7.008194446563721
Epoch 990, val loss: 0.9828250408172607
Epoch 1000, training loss: 0.009520389139652252 = 0.002500881440937519 + 0.001 * 7.019507884979248
Epoch 1000, val loss: 0.985297441482544
Epoch 1010, training loss: 0.00950025487691164 = 0.0024377561639994383 + 0.001 * 7.062498569488525
Epoch 1010, val loss: 0.9877732992172241
Epoch 1020, training loss: 0.009387793019413948 = 0.0023765789810568094 + 0.001 * 7.011213779449463
Epoch 1020, val loss: 0.9902149438858032
Epoch 1030, training loss: 0.00943001452833414 = 0.0023172800429165363 + 0.001 * 7.112734317779541
Epoch 1030, val loss: 0.9926770329475403
Epoch 1040, training loss: 0.00924934446811676 = 0.0022600707598030567 + 0.001 * 6.989274024963379
Epoch 1040, val loss: 0.9950810074806213
Epoch 1050, training loss: 0.009198238141834736 = 0.0022047285456210375 + 0.001 * 6.993509292602539
Epoch 1050, val loss: 0.9974805116653442
Epoch 1060, training loss: 0.009151292033493519 = 0.0021512547973543406 + 0.001 * 7.00003719329834
Epoch 1060, val loss: 0.9998124837875366
Epoch 1070, training loss: 0.009090641513466835 = 0.0020995098166167736 + 0.001 * 6.99113130569458
Epoch 1070, val loss: 1.002156376838684
Epoch 1080, training loss: 0.009062699973583221 = 0.0020494733471423388 + 0.001 * 7.01322603225708
Epoch 1080, val loss: 1.004460096359253
Epoch 1090, training loss: 0.008990854024887085 = 0.002001082058995962 + 0.001 * 6.989771842956543
Epoch 1090, val loss: 1.0067265033721924
Epoch 1100, training loss: 0.008942920714616776 = 0.0019542493391782045 + 0.001 * 6.98867130279541
Epoch 1100, val loss: 1.0089753866195679
Epoch 1110, training loss: 0.008901998400688171 = 0.0019089713459834456 + 0.001 * 6.9930267333984375
Epoch 1110, val loss: 1.0111838579177856
Epoch 1120, training loss: 0.008832069113850594 = 0.0018651745049282908 + 0.001 * 6.966894626617432
Epoch 1120, val loss: 1.0133569240570068
Epoch 1130, training loss: 0.008786492049694061 = 0.0018228498520329595 + 0.001 * 6.963642120361328
Epoch 1130, val loss: 1.0155285596847534
Epoch 1140, training loss: 0.008859437890350819 = 0.0017819585045799613 + 0.001 * 7.077479362487793
Epoch 1140, val loss: 1.0176470279693604
Epoch 1150, training loss: 0.008703726343810558 = 0.0017425116384401917 + 0.001 * 6.961214542388916
Epoch 1150, val loss: 1.0197008848190308
Epoch 1160, training loss: 0.008689040318131447 = 0.0017043439438566566 + 0.001 * 6.984696388244629
Epoch 1160, val loss: 1.0217950344085693
Epoch 1170, training loss: 0.008628368377685547 = 0.0016674939543008804 + 0.001 * 6.960874557495117
Epoch 1170, val loss: 1.0237923860549927
Epoch 1180, training loss: 0.008593607693910599 = 0.0016318776179105043 + 0.001 * 6.961730003356934
Epoch 1180, val loss: 1.0257818698883057
Epoch 1190, training loss: 0.008553692139685154 = 0.001597526133991778 + 0.001 * 6.956165790557861
Epoch 1190, val loss: 1.0277336835861206
Epoch 1200, training loss: 0.00852502416819334 = 0.0015643383376300335 + 0.001 * 6.960685729980469
Epoch 1200, val loss: 1.0296998023986816
Epoch 1210, training loss: 0.008495153859257698 = 0.0015323238912969828 + 0.001 * 6.962830066680908
Epoch 1210, val loss: 1.0315979719161987
Epoch 1220, training loss: 0.008447723463177681 = 0.001501503516919911 + 0.001 * 6.946219444274902
Epoch 1220, val loss: 1.0334826707839966
Epoch 1230, training loss: 0.008423357270658016 = 0.0014717810554429889 + 0.001 * 6.951575756072998
Epoch 1230, val loss: 1.0353459119796753
Epoch 1240, training loss: 0.008405928499996662 = 0.0014431659365072846 + 0.001 * 6.962761878967285
Epoch 1240, val loss: 1.0371745824813843
Epoch 1250, training loss: 0.008407067507505417 = 0.0014155521057546139 + 0.001 * 6.991515159606934
Epoch 1250, val loss: 1.038977861404419
Epoch 1260, training loss: 0.008337156847119331 = 0.0013889966066926718 + 0.001 * 6.948159694671631
Epoch 1260, val loss: 1.0407408475875854
Epoch 1270, training loss: 0.008323071524500847 = 0.0013633675407618284 + 0.001 * 6.9597039222717285
Epoch 1270, val loss: 1.042522668838501
Epoch 1280, training loss: 0.008304007351398468 = 0.0013386863283813 + 0.001 * 6.965321063995361
Epoch 1280, val loss: 1.0442363023757935
Epoch 1290, training loss: 0.008288032375276089 = 0.0013148298021405935 + 0.001 * 6.973202705383301
Epoch 1290, val loss: 1.0459407567977905
Epoch 1300, training loss: 0.008240454830229282 = 0.0012918557040393353 + 0.001 * 6.948598861694336
Epoch 1300, val loss: 1.0476114749908447
Epoch 1310, training loss: 0.008212147280573845 = 0.0012696745106950402 + 0.001 * 6.942471981048584
Epoch 1310, val loss: 1.0492419004440308
Epoch 1320, training loss: 0.008235186338424683 = 0.0012482879683375359 + 0.001 * 6.986897945404053
Epoch 1320, val loss: 1.0508843660354614
Epoch 1330, training loss: 0.008159383200109005 = 0.0012277016649022698 + 0.001 * 6.931680679321289
Epoch 1330, val loss: 1.0524492263793945
Epoch 1340, training loss: 0.008120500482618809 = 0.0012078348081558943 + 0.001 * 6.912665367126465
Epoch 1340, val loss: 1.0540037155151367
Epoch 1350, training loss: 0.008136983960866928 = 0.0011886553838849068 + 0.001 * 6.948328495025635
Epoch 1350, val loss: 1.0555583238601685
Epoch 1360, training loss: 0.008101402781903744 = 0.0011701361509039998 + 0.001 * 6.9312663078308105
Epoch 1360, val loss: 1.0570534467697144
Epoch 1370, training loss: 0.00808259192854166 = 0.001152269309386611 + 0.001 * 6.930322647094727
Epoch 1370, val loss: 1.058510184288025
Epoch 1380, training loss: 0.008094667457044125 = 0.001135025406256318 + 0.001 * 6.959641933441162
Epoch 1380, val loss: 1.0599666833877563
Epoch 1390, training loss: 0.00804223120212555 = 0.0011184486793354154 + 0.001 * 6.923781871795654
Epoch 1390, val loss: 1.061474323272705
Epoch 1400, training loss: 0.008053528144955635 = 0.0011024739360436797 + 0.001 * 6.951053619384766
Epoch 1400, val loss: 1.0628947019577026
Epoch 1410, training loss: 0.00802789255976677 = 0.0010870638070628047 + 0.001 * 6.940828323364258
Epoch 1410, val loss: 1.0642633438110352
Epoch 1420, training loss: 0.007994305342435837 = 0.001072168000973761 + 0.001 * 6.922137260437012
Epoch 1420, val loss: 1.0656523704528809
Epoch 1430, training loss: 0.007971898652613163 = 0.0010578010696917772 + 0.001 * 6.914096832275391
Epoch 1430, val loss: 1.0670239925384521
Epoch 1440, training loss: 0.007964137941598892 = 0.001043909345753491 + 0.001 * 6.920228481292725
Epoch 1440, val loss: 1.068334937095642
Epoch 1450, training loss: 0.007961500436067581 = 0.0010304838651791215 + 0.001 * 6.931015968322754
Epoch 1450, val loss: 1.0696213245391846
Epoch 1460, training loss: 0.007931976579129696 = 0.0010174695635214448 + 0.001 * 6.914506435394287
Epoch 1460, val loss: 1.0709155797958374
Epoch 1470, training loss: 0.00791946891695261 = 0.0010049262782558799 + 0.001 * 6.9145426750183105
Epoch 1470, val loss: 1.072148323059082
Epoch 1480, training loss: 0.007900257594883442 = 0.0009927765931934118 + 0.001 * 6.907480239868164
Epoch 1480, val loss: 1.0733492374420166
Epoch 1490, training loss: 0.00788709707558155 = 0.00098102365154773 + 0.001 * 6.906073093414307
Epoch 1490, val loss: 1.0745410919189453
Epoch 1500, training loss: 0.007922745309770107 = 0.0009696245542727411 + 0.001 * 6.953120708465576
Epoch 1500, val loss: 1.0757102966308594
Epoch 1510, training loss: 0.007870947010815144 = 0.0009586024680174887 + 0.001 * 6.912344455718994
Epoch 1510, val loss: 1.076880693435669
Epoch 1520, training loss: 0.007839797995984554 = 0.0009479445870965719 + 0.001 * 6.891853332519531
Epoch 1520, val loss: 1.0779740810394287
Epoch 1530, training loss: 0.007842552848160267 = 0.0009376396774314344 + 0.001 * 6.904913425445557
Epoch 1530, val loss: 1.0791212320327759
Epoch 1540, training loss: 0.007846644148230553 = 0.0009276373311877251 + 0.001 * 6.91900634765625
Epoch 1540, val loss: 1.0801692008972168
Epoch 1550, training loss: 0.007830292917788029 = 0.0009179593180306256 + 0.001 * 6.912333011627197
Epoch 1550, val loss: 1.081202507019043
Epoch 1560, training loss: 0.007796135731041431 = 0.0009085622732527554 + 0.001 * 6.887572765350342
Epoch 1560, val loss: 1.082248330116272
Epoch 1570, training loss: 0.007806466892361641 = 0.0008994635427370667 + 0.001 * 6.907002925872803
Epoch 1570, val loss: 1.0832926034927368
Epoch 1580, training loss: 0.007855920121073723 = 0.0008906211005523801 + 0.001 * 6.965298652648926
Epoch 1580, val loss: 1.0842703580856323
Epoch 1590, training loss: 0.007779574953019619 = 0.0008820445509627461 + 0.001 * 6.8975300788879395
Epoch 1590, val loss: 1.0852402448654175
Epoch 1600, training loss: 0.007826671004295349 = 0.0008737118332646787 + 0.001 * 6.952959060668945
Epoch 1600, val loss: 1.0862147808074951
Epoch 1610, training loss: 0.007761356886476278 = 0.0008656500722281635 + 0.001 * 6.895706653594971
Epoch 1610, val loss: 1.0871129035949707
Epoch 1620, training loss: 0.007763109635561705 = 0.000857811770401895 + 0.001 * 6.90529727935791
Epoch 1620, val loss: 1.0880413055419922
Epoch 1630, training loss: 0.0077299028635025024 = 0.0008502028649672866 + 0.001 * 6.87969970703125
Epoch 1630, val loss: 1.0889246463775635
Epoch 1640, training loss: 0.007727250922471285 = 0.000842830806504935 + 0.001 * 6.884419918060303
Epoch 1640, val loss: 1.0898267030715942
Epoch 1650, training loss: 0.007728932425379753 = 0.0008356531034223735 + 0.001 * 6.893279075622559
Epoch 1650, val loss: 1.0906543731689453
Epoch 1660, training loss: 0.0077257780358195305 = 0.0008287073578685522 + 0.001 * 6.897070407867432
Epoch 1660, val loss: 1.0914885997772217
Epoch 1670, training loss: 0.0077436985448002815 = 0.0008219300652854145 + 0.001 * 6.9217681884765625
Epoch 1670, val loss: 1.092278003692627
Epoch 1680, training loss: 0.0076911561191082 = 0.0008153829257935286 + 0.001 * 6.875772476196289
Epoch 1680, val loss: 1.0930982828140259
Epoch 1690, training loss: 0.007733995094895363 = 0.0008090020855888724 + 0.001 * 6.924992561340332
Epoch 1690, val loss: 1.0938844680786133
Epoch 1700, training loss: 0.007684510201215744 = 0.000802813854534179 + 0.001 * 6.881695747375488
Epoch 1700, val loss: 1.0946205854415894
Epoch 1710, training loss: 0.007692521903663874 = 0.0007967923884280026 + 0.001 * 6.895729064941406
Epoch 1710, val loss: 1.0953433513641357
Epoch 1720, training loss: 0.007683413103222847 = 0.0007909381529316306 + 0.001 * 6.89247465133667
Epoch 1720, val loss: 1.0960601568222046
Epoch 1730, training loss: 0.007676067296415567 = 0.0007852519047446549 + 0.001 * 6.890814781188965
Epoch 1730, val loss: 1.0967566967010498
Epoch 1740, training loss: 0.007694666739553213 = 0.0007797029102221131 + 0.001 * 6.914963722229004
Epoch 1740, val loss: 1.097438097000122
Epoch 1750, training loss: 0.007643410935997963 = 0.0007743191090412438 + 0.001 * 6.869091510772705
Epoch 1750, val loss: 1.0980939865112305
Epoch 1760, training loss: 0.007639963645488024 = 0.0007690737838856876 + 0.001 * 6.870889663696289
Epoch 1760, val loss: 1.0987428426742554
Epoch 1770, training loss: 0.007625881116837263 = 0.0007639601826667786 + 0.001 * 6.8619208335876465
Epoch 1770, val loss: 1.099374532699585
Epoch 1780, training loss: 0.007666617166250944 = 0.0007589614251628518 + 0.001 * 6.907655715942383
Epoch 1780, val loss: 1.0999841690063477
Epoch 1790, training loss: 0.007612043060362339 = 0.0007541140075773001 + 0.001 * 6.857928276062012
Epoch 1790, val loss: 1.1005603075027466
Epoch 1800, training loss: 0.007685374468564987 = 0.0007493755547329783 + 0.001 * 6.935998439788818
Epoch 1800, val loss: 1.1011803150177002
Epoch 1810, training loss: 0.007585467770695686 = 0.0007447744137607515 + 0.001 * 6.84069299697876
Epoch 1810, val loss: 1.1016992330551147
Epoch 1820, training loss: 0.007602316793054342 = 0.0007402689079754055 + 0.001 * 6.8620476722717285
Epoch 1820, val loss: 1.1022841930389404
Epoch 1830, training loss: 0.0075925663113594055 = 0.0007358944858424366 + 0.001 * 6.8566718101501465
Epoch 1830, val loss: 1.1027960777282715
Epoch 1840, training loss: 0.007582468446344137 = 0.0007316201808862388 + 0.001 * 6.850848197937012
Epoch 1840, val loss: 1.1033761501312256
Epoch 1850, training loss: 0.007588769309222698 = 0.0007274405215866864 + 0.001 * 6.861328125
Epoch 1850, val loss: 1.1038031578063965
Epoch 1860, training loss: 0.007575334515422583 = 0.0007233633659780025 + 0.001 * 6.851970672607422
Epoch 1860, val loss: 1.104365587234497
Epoch 1870, training loss: 0.007588941603899002 = 0.000719394301995635 + 0.001 * 6.869547367095947
Epoch 1870, val loss: 1.1047754287719727
Epoch 1880, training loss: 0.007568749599158764 = 0.00071551906876266 + 0.001 * 6.853229999542236
Epoch 1880, val loss: 1.1052627563476562
Epoch 1890, training loss: 0.007544924970716238 = 0.0007117208442650735 + 0.001 * 6.8332037925720215
Epoch 1890, val loss: 1.1056827306747437
Epoch 1900, training loss: 0.007560753729194403 = 0.0007080226205289364 + 0.001 * 6.852730751037598
Epoch 1900, val loss: 1.1061667203903198
Epoch 1910, training loss: 0.007561258971691132 = 0.0007044153171591461 + 0.001 * 6.8568434715271
Epoch 1910, val loss: 1.1065735816955566
Epoch 1920, training loss: 0.007552131544798613 = 0.0007008826942183077 + 0.001 * 6.851248741149902
Epoch 1920, val loss: 1.107027292251587
Epoch 1930, training loss: 0.007539126556366682 = 0.0006974290008656681 + 0.001 * 6.8416972160339355
Epoch 1930, val loss: 1.107352375984192
Epoch 1940, training loss: 0.007555769290775061 = 0.0006940519087947905 + 0.001 * 6.861717224121094
Epoch 1940, val loss: 1.1077635288238525
Epoch 1950, training loss: 0.0076695894822478294 = 0.0006907536881044507 + 0.001 * 6.978835582733154
Epoch 1950, val loss: 1.1081829071044922
Epoch 1960, training loss: 0.007529269903898239 = 0.0006875505787320435 + 0.001 * 6.841718673706055
Epoch 1960, val loss: 1.1085158586502075
Epoch 1970, training loss: 0.007524736225605011 = 0.0006844073650427163 + 0.001 * 6.840328216552734
Epoch 1970, val loss: 1.1088734865188599
Epoch 1980, training loss: 0.007537060417234898 = 0.0006813412765040994 + 0.001 * 6.855719089508057
Epoch 1980, val loss: 1.1092385053634644
Epoch 1990, training loss: 0.007517935708165169 = 0.0006783455610275269 + 0.001 * 6.839589595794678
Epoch 1990, val loss: 1.1095951795578003
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8065366367949395
The final CL Acc:0.76667, 0.03143, The final GNN Acc:0.81075, 0.00394
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13130])
remove edge: torch.Size([2, 7978])
updated graph: torch.Size([2, 10552])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9469131231307983 = 1.9383163452148438 + 0.001 * 8.596829414367676
Epoch 0, val loss: 1.9328974485397339
Epoch 10, training loss: 1.9373018741607666 = 1.928705096244812 + 0.001 * 8.596719741821289
Epoch 10, val loss: 1.9231687784194946
Epoch 20, training loss: 1.9253844022750854 = 1.9167879819869995 + 0.001 * 8.596410751342773
Epoch 20, val loss: 1.9113388061523438
Epoch 30, training loss: 1.9089299440383911 = 1.9003342390060425 + 0.001 * 8.595667839050293
Epoch 30, val loss: 1.8950103521347046
Epoch 40, training loss: 1.8849588632583618 = 1.8763651847839355 + 0.001 * 8.593623161315918
Epoch 40, val loss: 1.871603012084961
Epoch 50, training loss: 1.8512390851974487 = 1.8426525592803955 + 0.001 * 8.586564064025879
Epoch 50, val loss: 1.840036153793335
Epoch 60, training loss: 1.810433030128479 = 1.8018792867660522 + 0.001 * 8.553691864013672
Epoch 60, val loss: 1.8051564693450928
Epoch 70, training loss: 1.7692540884017944 = 1.7609227895736694 + 0.001 * 8.331294059753418
Epoch 70, val loss: 1.7724391222000122
Epoch 80, training loss: 1.7200424671173096 = 1.71211838722229 + 0.001 * 7.924091815948486
Epoch 80, val loss: 1.7292208671569824
Epoch 90, training loss: 1.6530438661575317 = 1.6453818082809448 + 0.001 * 7.662067413330078
Epoch 90, val loss: 1.6674343347549438
Epoch 100, training loss: 1.5650101900100708 = 1.5575531721115112 + 0.001 * 7.457034111022949
Epoch 100, val loss: 1.5885710716247559
Epoch 110, training loss: 1.4611179828643799 = 1.4537720680236816 + 0.001 * 7.345947265625
Epoch 110, val loss: 1.500180959701538
Epoch 120, training loss: 1.3516587018966675 = 1.3443529605865479 + 0.001 * 7.305763244628906
Epoch 120, val loss: 1.4099006652832031
Epoch 130, training loss: 1.2443194389343262 = 1.2370432615280151 + 0.001 * 7.276185989379883
Epoch 130, val loss: 1.3238993883132935
Epoch 140, training loss: 1.1439496278762817 = 1.1367061138153076 + 0.001 * 7.243494033813477
Epoch 140, val loss: 1.2446316480636597
Epoch 150, training loss: 1.052858591079712 = 1.045653223991394 + 0.001 * 7.205399513244629
Epoch 150, val loss: 1.1731419563293457
Epoch 160, training loss: 0.9707589149475098 = 0.9635844826698303 + 0.001 * 7.174436569213867
Epoch 160, val loss: 1.1088811159133911
Epoch 170, training loss: 0.8955557942390442 = 0.8884044289588928 + 0.001 * 7.151361465454102
Epoch 170, val loss: 1.0503783226013184
Epoch 180, training loss: 0.8251856565475464 = 0.8180559277534485 + 0.001 * 7.129729747772217
Epoch 180, val loss: 0.9955980181694031
Epoch 190, training loss: 0.7586321234703064 = 0.7515226006507874 + 0.001 * 7.109496593475342
Epoch 190, val loss: 0.9441761374473572
Epoch 200, training loss: 0.6960532069206238 = 0.6889582276344299 + 0.001 * 7.094990253448486
Epoch 200, val loss: 0.8962110280990601
Epoch 210, training loss: 0.6379541158676147 = 0.6308650374412537 + 0.001 * 7.089079856872559
Epoch 210, val loss: 0.8528590798377991
Epoch 220, training loss: 0.5844217538833618 = 0.5773348808288574 + 0.001 * 7.086895942687988
Epoch 220, val loss: 0.8150853514671326
Epoch 230, training loss: 0.5350354909896851 = 0.5279499888420105 + 0.001 * 7.085501670837402
Epoch 230, val loss: 0.7828198075294495
Epoch 240, training loss: 0.4892156422138214 = 0.4821304678916931 + 0.001 * 7.085186004638672
Epoch 240, val loss: 0.7554437518119812
Epoch 250, training loss: 0.44631075859069824 = 0.4392262399196625 + 0.001 * 7.084508419036865
Epoch 250, val loss: 0.7321497797966003
Epoch 260, training loss: 0.4055895209312439 = 0.39850592613220215 + 0.001 * 7.083608627319336
Epoch 260, val loss: 0.7122153043746948
Epoch 270, training loss: 0.3664964735507965 = 0.3594137728214264 + 0.001 * 7.0826873779296875
Epoch 270, val loss: 0.6949110627174377
Epoch 280, training loss: 0.3288673758506775 = 0.32178565859794617 + 0.001 * 7.081721782684326
Epoch 280, val loss: 0.6799383759498596
Epoch 290, training loss: 0.2930448055267334 = 0.2859640419483185 + 0.001 * 7.080766201019287
Epoch 290, val loss: 0.6674911975860596
Epoch 300, training loss: 0.2597121298313141 = 0.25263139605522156 + 0.001 * 7.080729961395264
Epoch 300, val loss: 0.6579842567443848
Epoch 310, training loss: 0.22950078547000885 = 0.22242170572280884 + 0.001 * 7.079078197479248
Epoch 310, val loss: 0.651854932308197
Epoch 320, training loss: 0.20270393788814545 = 0.1956256479024887 + 0.001 * 7.078290939331055
Epoch 320, val loss: 0.6491760611534119
Epoch 330, training loss: 0.1792529821395874 = 0.17217573523521423 + 0.001 * 7.077244758605957
Epoch 330, val loss: 0.6497200131416321
Epoch 340, training loss: 0.15884996950626373 = 0.15177293121814728 + 0.001 * 7.07704496383667
Epoch 340, val loss: 0.653165340423584
Epoch 350, training loss: 0.1411297768354416 = 0.13405396044254303 + 0.001 * 7.075812339782715
Epoch 350, val loss: 0.6591418981552124
Epoch 360, training loss: 0.12571930885314941 = 0.11864525079727173 + 0.001 * 7.074060440063477
Epoch 360, val loss: 0.6670748591423035
Epoch 370, training loss: 0.11229091137647629 = 0.10521730780601501 + 0.001 * 7.073601245880127
Epoch 370, val loss: 0.6764888167381287
Epoch 380, training loss: 0.1005658209323883 = 0.09349417686462402 + 0.001 * 7.0716423988342285
Epoch 380, val loss: 0.6869377493858337
Epoch 390, training loss: 0.09030113369226456 = 0.0832306519150734 + 0.001 * 7.070478439331055
Epoch 390, val loss: 0.6981194019317627
Epoch 400, training loss: 0.08128687739372253 = 0.07422101497650146 + 0.001 * 7.065860748291016
Epoch 400, val loss: 0.7098127007484436
Epoch 410, training loss: 0.07336753606796265 = 0.06630121171474457 + 0.001 * 7.066322326660156
Epoch 410, val loss: 0.7218594551086426
Epoch 420, training loss: 0.06639871746301651 = 0.0593385174870491 + 0.001 * 7.060197830200195
Epoch 420, val loss: 0.7341366410255432
Epoch 430, training loss: 0.060275349766016006 = 0.05321744456887245 + 0.001 * 7.0579047203063965
Epoch 430, val loss: 0.7464953660964966
Epoch 440, training loss: 0.05490156263113022 = 0.04783572256565094 + 0.001 * 7.0658392906188965
Epoch 440, val loss: 0.758922278881073
Epoch 450, training loss: 0.05015336349606514 = 0.04310370609164238 + 0.001 * 7.049656867980957
Epoch 450, val loss: 0.771262526512146
Epoch 460, training loss: 0.045984044671058655 = 0.03893866389989853 + 0.001 * 7.045382499694824
Epoch 460, val loss: 0.783496618270874
Epoch 470, training loss: 0.04232163727283478 = 0.035268910229206085 + 0.001 * 7.052727222442627
Epoch 470, val loss: 0.7956287860870361
Epoch 480, training loss: 0.03907096013426781 = 0.03202803432941437 + 0.001 * 7.042923927307129
Epoch 480, val loss: 0.8074327707290649
Epoch 490, training loss: 0.036192625761032104 = 0.02915835753083229 + 0.001 * 7.034268856048584
Epoch 490, val loss: 0.8189605474472046
Epoch 500, training loss: 0.033705275505781174 = 0.026610473170876503 + 0.001 * 7.094802379608154
Epoch 500, val loss: 0.8302218914031982
Epoch 510, training loss: 0.03139180690050125 = 0.02434355393052101 + 0.001 * 7.048252582550049
Epoch 510, val loss: 0.8410385847091675
Epoch 520, training loss: 0.029353633522987366 = 0.022322749719023705 + 0.001 * 7.030882835388184
Epoch 520, val loss: 0.8515655398368835
Epoch 530, training loss: 0.027544721961021423 = 0.020518356934189796 + 0.001 * 7.026365756988525
Epoch 530, val loss: 0.861770510673523
Epoch 540, training loss: 0.025936929509043694 = 0.018904604017734528 + 0.001 * 7.032325267791748
Epoch 540, val loss: 0.8716205954551697
Epoch 550, training loss: 0.024483827874064445 = 0.017458444461226463 + 0.001 * 7.025383472442627
Epoch 550, val loss: 0.8811442852020264
Epoch 560, training loss: 0.02317841351032257 = 0.016159968450665474 + 0.001 * 7.018443584442139
Epoch 560, val loss: 0.8903716802597046
Epoch 570, training loss: 0.022022657096385956 = 0.014991936273872852 + 0.001 * 7.030719757080078
Epoch 570, val loss: 0.8993160724639893
Epoch 580, training loss: 0.0209506805986166 = 0.01393924467265606 + 0.001 * 7.0114359855651855
Epoch 580, val loss: 0.9079732298851013
Epoch 590, training loss: 0.020000465214252472 = 0.012988347560167313 + 0.001 * 7.0121169090271
Epoch 590, val loss: 0.916316568851471
Epoch 600, training loss: 0.019139409065246582 = 0.012127681635320187 + 0.001 * 7.011726379394531
Epoch 600, val loss: 0.9244369268417358
Epoch 610, training loss: 0.018367817625403404 = 0.011347055435180664 + 0.001 * 7.020761489868164
Epoch 610, val loss: 0.9322584271430969
Epoch 620, training loss: 0.01765868067741394 = 0.010637649334967136 + 0.001 * 7.021029949188232
Epoch 620, val loss: 0.9398425817489624
Epoch 630, training loss: 0.016997992992401123 = 0.009991604834794998 + 0.001 * 7.006387233734131
Epoch 630, val loss: 0.9472054243087769
Epoch 640, training loss: 0.016400573775172234 = 0.009402009658515453 + 0.001 * 6.998563766479492
Epoch 640, val loss: 0.954322874546051
Epoch 650, training loss: 0.01585594192147255 = 0.008862844668328762 + 0.001 * 6.993096351623535
Epoch 650, val loss: 0.9612370133399963
Epoch 660, training loss: 0.015367220155894756 = 0.008368861861526966 + 0.001 * 6.998357772827148
Epoch 660, val loss: 0.9679540395736694
Epoch 670, training loss: 0.014934279024600983 = 0.00791548378765583 + 0.001 * 7.018794536590576
Epoch 670, val loss: 0.9744605422019958
Epoch 680, training loss: 0.014485507272183895 = 0.007498601917177439 + 0.001 * 6.986905097961426
Epoch 680, val loss: 0.9807870388031006
Epoch 690, training loss: 0.014101951383054256 = 0.007114565931260586 + 0.001 * 6.987385272979736
Epoch 690, val loss: 0.9869228601455688
Epoch 700, training loss: 0.013741257600486279 = 0.006760149262845516 + 0.001 * 6.98110818862915
Epoch 700, val loss: 0.9928668141365051
Epoch 710, training loss: 0.013417833484709263 = 0.006432510446757078 + 0.001 * 6.98532247543335
Epoch 710, val loss: 0.998660147190094
Epoch 720, training loss: 0.013121794909238815 = 0.006129135377705097 + 0.001 * 6.992659091949463
Epoch 720, val loss: 1.004287600517273
Epoch 730, training loss: 0.012820899486541748 = 0.005847723223268986 + 0.001 * 6.973175525665283
Epoch 730, val loss: 1.0097453594207764
Epoch 740, training loss: 0.012553561478853226 = 0.005586239043623209 + 0.001 * 6.967321872711182
Epoch 740, val loss: 1.0150561332702637
Epoch 750, training loss: 0.012311176396906376 = 0.0053429268300533295 + 0.001 * 6.968249320983887
Epoch 750, val loss: 1.0202562808990479
Epoch 760, training loss: 0.012085947208106518 = 0.005116227548569441 + 0.001 * 6.969719409942627
Epoch 760, val loss: 1.0253015756607056
Epoch 770, training loss: 0.011888615787029266 = 0.004904678091406822 + 0.001 * 6.983937740325928
Epoch 770, val loss: 1.0301741361618042
Epoch 780, training loss: 0.011682208627462387 = 0.004706972744315863 + 0.001 * 6.975235462188721
Epoch 780, val loss: 1.034959316253662
Epoch 790, training loss: 0.011480728164315224 = 0.004521962720900774 + 0.001 * 6.958765029907227
Epoch 790, val loss: 1.03957998752594
Epoch 800, training loss: 0.011322004720568657 = 0.004348600283265114 + 0.001 * 6.973403453826904
Epoch 800, val loss: 1.044101357460022
Epoch 810, training loss: 0.01114935614168644 = 0.004185967613011599 + 0.001 * 6.963387966156006
Epoch 810, val loss: 1.0485092401504517
Epoch 820, training loss: 0.011019108816981316 = 0.004033184610307217 + 0.001 * 6.985924243927002
Epoch 820, val loss: 1.0527974367141724
Epoch 830, training loss: 0.01085459440946579 = 0.0038894969038665295 + 0.001 * 6.965097427368164
Epoch 830, val loss: 1.0569839477539062
Epoch 840, training loss: 0.01070545706897974 = 0.0037541971541941166 + 0.001 * 6.951259613037109
Epoch 840, val loss: 1.061042070388794
Epoch 850, training loss: 0.010579351335763931 = 0.0036266595125198364 + 0.001 * 6.952691555023193
Epoch 850, val loss: 1.0649880170822144
Epoch 860, training loss: 0.010457310825586319 = 0.0035063622053712606 + 0.001 * 6.950948715209961
Epoch 860, val loss: 1.0688817501068115
Epoch 870, training loss: 0.010337634943425655 = 0.0033926961477845907 + 0.001 * 6.944938659667969
Epoch 870, val loss: 1.072663426399231
Epoch 880, training loss: 0.010246769525110722 = 0.003285202430561185 + 0.001 * 6.96156644821167
Epoch 880, val loss: 1.0763399600982666
Epoch 890, training loss: 0.0101369833573699 = 0.0031834628898650408 + 0.001 * 6.95352029800415
Epoch 890, val loss: 1.0799278020858765
Epoch 900, training loss: 0.010025804862380028 = 0.00308707682415843 + 0.001 * 6.938727378845215
Epoch 900, val loss: 1.083476185798645
Epoch 910, training loss: 0.009946459904313087 = 0.0029956575017422438 + 0.001 * 6.950802326202393
Epoch 910, val loss: 1.0869202613830566
Epoch 920, training loss: 0.009855949319899082 = 0.0029088875744491816 + 0.001 * 6.947061538696289
Epoch 920, val loss: 1.0902668237686157
Epoch 930, training loss: 0.009777076542377472 = 0.0028264587745070457 + 0.001 * 6.950617790222168
Epoch 930, val loss: 1.0935574769973755
Epoch 940, training loss: 0.00968489795923233 = 0.0027480842545628548 + 0.001 * 6.9368133544921875
Epoch 940, val loss: 1.0967837572097778
Epoch 950, training loss: 0.00961711909621954 = 0.002673516748473048 + 0.001 * 6.943602085113525
Epoch 950, val loss: 1.0998879671096802
Epoch 960, training loss: 0.009549327194690704 = 0.0026025166735053062 + 0.001 * 6.946809768676758
Epoch 960, val loss: 1.10297691822052
Epoch 970, training loss: 0.009498439729213715 = 0.0025348507333546877 + 0.001 * 6.963588714599609
Epoch 970, val loss: 1.1059331893920898
Epoch 980, training loss: 0.009401662275195122 = 0.0024703387171030045 + 0.001 * 6.9313225746154785
Epoch 980, val loss: 1.1088855266571045
Epoch 990, training loss: 0.009342487901449203 = 0.0024087654892355204 + 0.001 * 6.933722496032715
Epoch 990, val loss: 1.1117455959320068
Epoch 1000, training loss: 0.00927012786269188 = 0.0023499643430113792 + 0.001 * 6.920162677764893
Epoch 1000, val loss: 1.1145282983779907
Epoch 1010, training loss: 0.009216628968715668 = 0.002293761121109128 + 0.001 * 6.922867298126221
Epoch 1010, val loss: 1.117277979850769
Epoch 1020, training loss: 0.009197719395160675 = 0.0022400112356990576 + 0.001 * 6.957707405090332
Epoch 1020, val loss: 1.1199486255645752
Epoch 1030, training loss: 0.00910383090376854 = 0.0021885945461690426 + 0.001 * 6.915236473083496
Epoch 1030, val loss: 1.1225879192352295
Epoch 1040, training loss: 0.009072976186871529 = 0.0021393452771008015 + 0.001 * 6.93363094329834
Epoch 1040, val loss: 1.1251378059387207
Epoch 1050, training loss: 0.00902131199836731 = 0.002092188224196434 + 0.001 * 6.929123878479004
Epoch 1050, val loss: 1.1276252269744873
Epoch 1060, training loss: 0.008971251547336578 = 0.0020469625014811754 + 0.001 * 6.924288749694824
Epoch 1060, val loss: 1.1300853490829468
Epoch 1070, training loss: 0.008915049955248833 = 0.002003574511036277 + 0.001 * 6.91147518157959
Epoch 1070, val loss: 1.1324716806411743
Epoch 1080, training loss: 0.008885541930794716 = 0.001961935544386506 + 0.001 * 6.923605918884277
Epoch 1080, val loss: 1.134814739227295
Epoch 1090, training loss: 0.008848157711327076 = 0.0019219324458390474 + 0.001 * 6.926225185394287
Epoch 1090, val loss: 1.1371256113052368
Epoch 1100, training loss: 0.00878869742155075 = 0.0018835058435797691 + 0.001 * 6.905190944671631
Epoch 1100, val loss: 1.1393626928329468
Epoch 1110, training loss: 0.008760608732700348 = 0.0018465595785528421 + 0.001 * 6.91404914855957
Epoch 1110, val loss: 1.141552209854126
Epoch 1120, training loss: 0.00873132050037384 = 0.0018110332312062383 + 0.001 * 6.920286655426025
Epoch 1120, val loss: 1.1437329053878784
Epoch 1130, training loss: 0.00868549570441246 = 0.0017768453108146787 + 0.001 * 6.908649921417236
Epoch 1130, val loss: 1.1458219289779663
Epoch 1140, training loss: 0.008664797991514206 = 0.0017439464572817087 + 0.001 * 6.920851230621338
Epoch 1140, val loss: 1.1478849649429321
Epoch 1150, training loss: 0.008624089881777763 = 0.0017122558783739805 + 0.001 * 6.911833763122559
Epoch 1150, val loss: 1.149909496307373
Epoch 1160, training loss: 0.008609320037066936 = 0.0016817201394587755 + 0.001 * 6.9275994300842285
Epoch 1160, val loss: 1.1518406867980957
Epoch 1170, training loss: 0.008557228371500969 = 0.0016522847581654787 + 0.001 * 6.904942989349365
Epoch 1170, val loss: 1.1537878513336182
Epoch 1180, training loss: 0.008528652600944042 = 0.0016239025862887502 + 0.001 * 6.904749870300293
Epoch 1180, val loss: 1.155686855316162
Epoch 1190, training loss: 0.00849162321537733 = 0.0015965100610628724 + 0.001 * 6.89511251449585
Epoch 1190, val loss: 1.157518982887268
Epoch 1200, training loss: 0.008482296951115131 = 0.0015700614312663674 + 0.001 * 6.912235260009766
Epoch 1200, val loss: 1.1593399047851562
Epoch 1210, training loss: 0.008433125913143158 = 0.0015445294557139277 + 0.001 * 6.888596057891846
Epoch 1210, val loss: 1.161104679107666
Epoch 1220, training loss: 0.00841296836733818 = 0.0015198547625914216 + 0.001 * 6.893113136291504
Epoch 1220, val loss: 1.1628221273422241
Epoch 1230, training loss: 0.008380481973290443 = 0.001496004406362772 + 0.001 * 6.884477138519287
Epoch 1230, val loss: 1.1645398139953613
Epoch 1240, training loss: 0.008372596465051174 = 0.0014729511458426714 + 0.001 * 6.89964485168457
Epoch 1240, val loss: 1.1662274599075317
Epoch 1250, training loss: 0.008326765149831772 = 0.001450654468499124 + 0.001 * 6.876110553741455
Epoch 1250, val loss: 1.1678229570388794
Epoch 1260, training loss: 0.008311431854963303 = 0.0014290808467194438 + 0.001 * 6.882350921630859
Epoch 1260, val loss: 1.169441819190979
Epoch 1270, training loss: 0.008303938433527946 = 0.0014081904664635658 + 0.001 * 6.895747184753418
Epoch 1270, val loss: 1.1710174083709717
Epoch 1280, training loss: 0.008274036459624767 = 0.0013879629550501704 + 0.001 * 6.886073112487793
Epoch 1280, val loss: 1.1725594997406006
Epoch 1290, training loss: 0.008260182105004787 = 0.0013683701399713755 + 0.001 * 6.891811847686768
Epoch 1290, val loss: 1.1740577220916748
Epoch 1300, training loss: 0.0082344776019454 = 0.0013493937440216541 + 0.001 * 6.8850836753845215
Epoch 1300, val loss: 1.1755032539367676
Epoch 1310, training loss: 0.008232169784605503 = 0.0013310060603544116 + 0.001 * 6.901163578033447
Epoch 1310, val loss: 1.1769660711288452
Epoch 1320, training loss: 0.008204396814107895 = 0.001313179382123053 + 0.001 * 6.891217231750488
Epoch 1320, val loss: 1.178382158279419
Epoch 1330, training loss: 0.008222954347729683 = 0.001295898575335741 + 0.001 * 6.9270548820495605
Epoch 1330, val loss: 1.1797518730163574
Epoch 1340, training loss: 0.00814120750874281 = 0.0012791331391781569 + 0.001 * 6.86207389831543
Epoch 1340, val loss: 1.1811168193817139
Epoch 1350, training loss: 0.008134773932397366 = 0.0012628522235900164 + 0.001 * 6.871921062469482
Epoch 1350, val loss: 1.1824437379837036
Epoch 1360, training loss: 0.008119890466332436 = 0.0012470509391278028 + 0.001 * 6.872838973999023
Epoch 1360, val loss: 1.1837502717971802
Epoch 1370, training loss: 0.008085878565907478 = 0.0012317007640376687 + 0.001 * 6.854177474975586
Epoch 1370, val loss: 1.185020923614502
Epoch 1380, training loss: 0.008108583278954029 = 0.0012167994864284992 + 0.001 * 6.891783237457275
Epoch 1380, val loss: 1.1862810850143433
Epoch 1390, training loss: 0.008070116862654686 = 0.0012023249873891473 + 0.001 * 6.867791652679443
Epoch 1390, val loss: 1.187508463859558
Epoch 1400, training loss: 0.00809498317539692 = 0.0011882624821737409 + 0.001 * 6.906720161437988
Epoch 1400, val loss: 1.1886787414550781
Epoch 1410, training loss: 0.008026772178709507 = 0.0011746113887056708 + 0.001 * 6.852160930633545
Epoch 1410, val loss: 1.1898664236068726
Epoch 1420, training loss: 0.00802439171820879 = 0.001161332125775516 + 0.001 * 6.8630595207214355
Epoch 1420, val loss: 1.1910006999969482
Epoch 1430, training loss: 0.008016964420676231 = 0.001148425042629242 + 0.001 * 6.8685383796691895
Epoch 1430, val loss: 1.1921098232269287
Epoch 1440, training loss: 0.008009054698050022 = 0.0011358739575371146 + 0.001 * 6.873180866241455
Epoch 1440, val loss: 1.1932493448257446
Epoch 1450, training loss: 0.007967868819832802 = 0.0011236565187573433 + 0.001 * 6.844211578369141
Epoch 1450, val loss: 1.194306492805481
Epoch 1460, training loss: 0.007966013625264168 = 0.0011117489775642753 + 0.001 * 6.854264259338379
Epoch 1460, val loss: 1.1953470706939697
Epoch 1470, training loss: 0.007943752221763134 = 0.0011001609964296222 + 0.001 * 6.843591213226318
Epoch 1470, val loss: 1.1963927745819092
Epoch 1480, training loss: 0.007929878309369087 = 0.0010888808174058795 + 0.001 * 6.840997695922852
Epoch 1480, val loss: 1.1973878145217896
Epoch 1490, training loss: 0.007950926199555397 = 0.0010778760770335793 + 0.001 * 6.873049736022949
Epoch 1490, val loss: 1.1983752250671387
Epoch 1500, training loss: 0.007921479642391205 = 0.0010671811178326607 + 0.001 * 6.8542985916137695
Epoch 1500, val loss: 1.1993350982666016
Epoch 1510, training loss: 0.007937069982290268 = 0.0010567419230937958 + 0.001 * 6.8803277015686035
Epoch 1510, val loss: 1.2003052234649658
Epoch 1520, training loss: 0.007883572019636631 = 0.0010465884115546942 + 0.001 * 6.836983680725098
Epoch 1520, val loss: 1.2012059688568115
Epoch 1530, training loss: 0.007881003431975842 = 0.0010366867063567042 + 0.001 * 6.8443169593811035
Epoch 1530, val loss: 1.2021015882492065
Epoch 1540, training loss: 0.00786287896335125 = 0.001027036807499826 + 0.001 * 6.835841655731201
Epoch 1540, val loss: 1.2030261754989624
Epoch 1550, training loss: 0.007853254675865173 = 0.0010176204377785325 + 0.001 * 6.835633754730225
Epoch 1550, val loss: 1.2038443088531494
Epoch 1560, training loss: 0.007871369831264019 = 0.001008430146612227 + 0.001 * 6.862939357757568
Epoch 1560, val loss: 1.2047361135482788
Epoch 1570, training loss: 0.007836084812879562 = 0.0009994611609727144 + 0.001 * 6.836623191833496
Epoch 1570, val loss: 1.2055418491363525
Epoch 1580, training loss: 0.007823457010090351 = 0.000990709406323731 + 0.001 * 6.832747459411621
Epoch 1580, val loss: 1.2063915729522705
Epoch 1590, training loss: 0.007825103588402271 = 0.0009821666171774268 + 0.001 * 6.8429365158081055
Epoch 1590, val loss: 1.2071936130523682
Epoch 1600, training loss: 0.0078057413920760155 = 0.0009738310473039746 + 0.001 * 6.831910133361816
Epoch 1600, val loss: 1.2079784870147705
Epoch 1610, training loss: 0.007816153578460217 = 0.0009656884358264506 + 0.001 * 6.850464820861816
Epoch 1610, val loss: 1.2087371349334717
Epoch 1620, training loss: 0.0078016468323767185 = 0.0009577321470715106 + 0.001 * 6.84391450881958
Epoch 1620, val loss: 1.2094911336898804
Epoch 1630, training loss: 0.007773078046739101 = 0.0009499562438577414 + 0.001 * 6.823121070861816
Epoch 1630, val loss: 1.2102235555648804
Epoch 1640, training loss: 0.007761961780488491 = 0.0009423621231690049 + 0.001 * 6.819599628448486
Epoch 1640, val loss: 1.2109349966049194
Epoch 1650, training loss: 0.007765875197947025 = 0.0009349412866868079 + 0.001 * 6.830933570861816
Epoch 1650, val loss: 1.2116585969924927
Epoch 1660, training loss: 0.007761488668620586 = 0.0009276933851651847 + 0.001 * 6.833795070648193
Epoch 1660, val loss: 1.2123253345489502
Epoch 1670, training loss: 0.007729926612228155 = 0.000920597929507494 + 0.001 * 6.809328556060791
Epoch 1670, val loss: 1.2130075693130493
Epoch 1680, training loss: 0.007742178626358509 = 0.0009136656881310046 + 0.001 * 6.828512668609619
Epoch 1680, val loss: 1.2136911153793335
Epoch 1690, training loss: 0.0077973660081624985 = 0.0009068944491446018 + 0.001 * 6.890471458435059
Epoch 1690, val loss: 1.2143231630325317
Epoch 1700, training loss: 0.007717509754002094 = 0.0009002731530927122 + 0.001 * 6.817236423492432
Epoch 1700, val loss: 1.2149684429168701
Epoch 1710, training loss: 0.007704165764153004 = 0.0008937938837334514 + 0.001 * 6.810371398925781
Epoch 1710, val loss: 1.2155838012695312
Epoch 1720, training loss: 0.007693854160606861 = 0.000887446862179786 + 0.001 * 6.8064069747924805
Epoch 1720, val loss: 1.2161943912506104
Epoch 1730, training loss: 0.007699457462877035 = 0.0008812378509901464 + 0.001 * 6.818219184875488
Epoch 1730, val loss: 1.2167823314666748
Epoch 1740, training loss: 0.007687814068049192 = 0.0008751570130698383 + 0.001 * 6.812656879425049
Epoch 1740, val loss: 1.2173964977264404
Epoch 1750, training loss: 0.007691662758588791 = 0.0008691963157616556 + 0.001 * 6.822465896606445
Epoch 1750, val loss: 1.2179474830627441
Epoch 1760, training loss: 0.007684529758989811 = 0.0008633543620817363 + 0.001 * 6.8211750984191895
Epoch 1760, val loss: 1.218496322631836
Epoch 1770, training loss: 0.0076912771910429 = 0.0008576416876167059 + 0.001 * 6.833635330200195
Epoch 1770, val loss: 1.2190783023834229
Epoch 1780, training loss: 0.007698772009462118 = 0.0008520426345057786 + 0.001 * 6.846728801727295
Epoch 1780, val loss: 1.219624400138855
Epoch 1790, training loss: 0.007651586085557938 = 0.0008465521386824548 + 0.001 * 6.8050336837768555
Epoch 1790, val loss: 1.220138430595398
Epoch 1800, training loss: 0.007657384034246206 = 0.0008411724702455103 + 0.001 * 6.816211223602295
Epoch 1800, val loss: 1.2206611633300781
Epoch 1810, training loss: 0.007629746105521917 = 0.0008359026396647096 + 0.001 * 6.7938432693481445
Epoch 1810, val loss: 1.2212060689926147
Epoch 1820, training loss: 0.007634191308170557 = 0.0008307386888191104 + 0.001 * 6.803452014923096
Epoch 1820, val loss: 1.221704363822937
Epoch 1830, training loss: 0.007673834916204214 = 0.0008256701985374093 + 0.001 * 6.8481645584106445
Epoch 1830, val loss: 1.2222118377685547
Epoch 1840, training loss: 0.007613117806613445 = 0.0008206882630474865 + 0.001 * 6.792428970336914
Epoch 1840, val loss: 1.22267484664917
Epoch 1850, training loss: 0.007613200694322586 = 0.0008158044656738639 + 0.001 * 6.797395706176758
Epoch 1850, val loss: 1.2231584787368774
Epoch 1860, training loss: 0.007647345773875713 = 0.0008110157796181738 + 0.001 * 6.836329936981201
Epoch 1860, val loss: 1.2236008644104004
Epoch 1870, training loss: 0.007611447013914585 = 0.0008063184795901179 + 0.001 * 6.80512809753418
Epoch 1870, val loss: 1.2240633964538574
Epoch 1880, training loss: 0.007604263257235289 = 0.0008017136715352535 + 0.001 * 6.802549362182617
Epoch 1880, val loss: 1.2245362997055054
Epoch 1890, training loss: 0.007621620781719685 = 0.000797197048086673 + 0.001 * 6.824423313140869
Epoch 1890, val loss: 1.224955677986145
Epoch 1900, training loss: 0.007588126230984926 = 0.0007927597616799176 + 0.001 * 6.795365810394287
Epoch 1900, val loss: 1.2253907918930054
Epoch 1910, training loss: 0.007595649920403957 = 0.0007884062943048775 + 0.001 * 6.807243347167969
Epoch 1910, val loss: 1.2258059978485107
Epoch 1920, training loss: 0.007604057900607586 = 0.0007841225014999509 + 0.001 * 6.819935321807861
Epoch 1920, val loss: 1.226240634918213
Epoch 1930, training loss: 0.0075693330727517605 = 0.0007799250306561589 + 0.001 * 6.789407730102539
Epoch 1930, val loss: 1.2265890836715698
Epoch 1940, training loss: 0.007570135872811079 = 0.0007757893763482571 + 0.001 * 6.794346332550049
Epoch 1940, val loss: 1.2270442247390747
Epoch 1950, training loss: 0.007592142093926668 = 0.0007717387052252889 + 0.001 * 6.820403099060059
Epoch 1950, val loss: 1.2274184226989746
Epoch 1960, training loss: 0.0075507317669689655 = 0.0007677561370655894 + 0.001 * 6.782975196838379
Epoch 1960, val loss: 1.2277905941009521
Epoch 1970, training loss: 0.00754551449790597 = 0.0007638385286554694 + 0.001 * 6.781675338745117
Epoch 1970, val loss: 1.2282100915908813
Epoch 1980, training loss: 0.0075713032856583595 = 0.0007599942036904395 + 0.001 * 6.811308860778809
Epoch 1980, val loss: 1.2285842895507812
Epoch 1990, training loss: 0.007565675303339958 = 0.0007562097162008286 + 0.001 * 6.809465408325195
Epoch 1990, val loss: 1.2289856672286987
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8302583025830259
=== training gcn model ===
Epoch 0, training loss: 1.9794330596923828 = 1.9708362817764282 + 0.001 * 8.596795082092285
Epoch 0, val loss: 1.9702060222625732
Epoch 10, training loss: 1.9680668115615845 = 1.9594701528549194 + 0.001 * 8.59671401977539
Epoch 10, val loss: 1.9585427045822144
Epoch 20, training loss: 1.9537272453308105 = 1.9451308250427246 + 0.001 * 8.596420288085938
Epoch 20, val loss: 1.9435449838638306
Epoch 30, training loss: 1.933197259902954 = 1.9246015548706055 + 0.001 * 8.595683097839355
Epoch 30, val loss: 1.9220178127288818
Epoch 40, training loss: 1.902595043182373 = 1.8940014839172363 + 0.001 * 8.59361743927002
Epoch 40, val loss: 1.8903504610061646
Epoch 50, training loss: 1.8598190546035767 = 1.8512325286865234 + 0.001 * 8.586475372314453
Epoch 50, val loss: 1.8480324745178223
Epoch 60, training loss: 1.8134983777999878 = 1.8049415349960327 + 0.001 * 8.556816101074219
Epoch 60, val loss: 1.8070412874221802
Epoch 70, training loss: 1.7786433696746826 = 1.7702406644821167 + 0.001 * 8.4027099609375
Epoch 70, val loss: 1.780073642730713
Epoch 80, training loss: 1.7389369010925293 = 1.7309297323226929 + 0.001 * 8.007162094116211
Epoch 80, val loss: 1.7457612752914429
Epoch 90, training loss: 1.6842918395996094 = 1.6763818264007568 + 0.001 * 7.910017490386963
Epoch 90, val loss: 1.6971664428710938
Epoch 100, training loss: 1.6089774370193481 = 1.6011857986450195 + 0.001 * 7.791678428649902
Epoch 100, val loss: 1.6313631534576416
Epoch 110, training loss: 1.5163915157318115 = 1.508750081062317 + 0.001 * 7.641427516937256
Epoch 110, val loss: 1.5532759428024292
Epoch 120, training loss: 1.4207024574279785 = 1.4132143259048462 + 0.001 * 7.488121509552002
Epoch 120, val loss: 1.4757829904556274
Epoch 130, training loss: 1.3302533626556396 = 1.3228466510772705 + 0.001 * 7.406665802001953
Epoch 130, val loss: 1.4037022590637207
Epoch 140, training loss: 1.2442877292633057 = 1.2369563579559326 + 0.001 * 7.331423759460449
Epoch 140, val loss: 1.336328148841858
Epoch 150, training loss: 1.1624360084533691 = 1.1551613807678223 + 0.001 * 7.274653911590576
Epoch 150, val loss: 1.2720178365707397
Epoch 160, training loss: 1.0864198207855225 = 1.0791891813278198 + 0.001 * 7.230680465698242
Epoch 160, val loss: 1.2122869491577148
Epoch 170, training loss: 1.0178701877593994 = 1.0106748342514038 + 0.001 * 7.195343971252441
Epoch 170, val loss: 1.1591579914093018
Epoch 180, training loss: 0.9563432335853577 = 0.9491698741912842 + 0.001 * 7.173356056213379
Epoch 180, val loss: 1.1124303340911865
Epoch 190, training loss: 0.8991581797599792 = 0.8919934034347534 + 0.001 * 7.164755344390869
Epoch 190, val loss: 1.0700322389602661
Epoch 200, training loss: 0.8428992033004761 = 0.8357398509979248 + 0.001 * 7.159339427947998
Epoch 200, val loss: 1.0293221473693848
Epoch 210, training loss: 0.785308837890625 = 0.7781572937965393 + 0.001 * 7.151557445526123
Epoch 210, val loss: 0.9883444309234619
Epoch 220, training loss: 0.7259156703948975 = 0.7187747359275818 + 0.001 * 7.140909194946289
Epoch 220, val loss: 0.9467009902000427
Epoch 230, training loss: 0.6658980846405029 = 0.6587722897529602 + 0.001 * 7.1257829666137695
Epoch 230, val loss: 0.9054610133171082
Epoch 240, training loss: 0.6076365113258362 = 0.6005337834358215 + 0.001 * 7.102753162384033
Epoch 240, val loss: 0.8664541840553284
Epoch 250, training loss: 0.5536845326423645 = 0.5466044545173645 + 0.001 * 7.080102443695068
Epoch 250, val loss: 0.8320184946060181
Epoch 260, training loss: 0.5055831074714661 = 0.49852895736694336 + 0.001 * 7.054135799407959
Epoch 260, val loss: 0.8036686778068542
Epoch 270, training loss: 0.4631490409374237 = 0.45612284541130066 + 0.001 * 7.02618408203125
Epoch 270, val loss: 0.7815982699394226
Epoch 280, training loss: 0.42500463128089905 = 0.4179914891719818 + 0.001 * 7.013151168823242
Epoch 280, val loss: 0.7644402384757996
Epoch 290, training loss: 0.3895057141780853 = 0.3825084865093231 + 0.001 * 6.997231960296631
Epoch 290, val loss: 0.750676691532135
Epoch 300, training loss: 0.35544079542160034 = 0.3484509289264679 + 0.001 * 6.989872932434082
Epoch 300, val loss: 0.7389297485351562
Epoch 310, training loss: 0.3221561014652252 = 0.31517159938812256 + 0.001 * 6.984513282775879
Epoch 310, val loss: 0.7285757660865784
Epoch 320, training loss: 0.28946733474731445 = 0.2824869453907013 + 0.001 * 6.9803900718688965
Epoch 320, val loss: 0.7194141745567322
Epoch 330, training loss: 0.2577362358570099 = 0.2507582902908325 + 0.001 * 6.977942943572998
Epoch 330, val loss: 0.7114228010177612
Epoch 340, training loss: 0.22770041227340698 = 0.22072777152061462 + 0.001 * 6.972646236419678
Epoch 340, val loss: 0.7049819231033325
Epoch 350, training loss: 0.20027515292167664 = 0.19330483675003052 + 0.001 * 6.970317840576172
Epoch 350, val loss: 0.7007747292518616
Epoch 360, training loss: 0.17600199580192566 = 0.1690271943807602 + 0.001 * 6.9748053550720215
Epoch 360, val loss: 0.6992555260658264
Epoch 370, training loss: 0.15485113859176636 = 0.14788281917572021 + 0.001 * 6.968323707580566
Epoch 370, val loss: 0.7004985213279724
Epoch 380, training loss: 0.136494979262352 = 0.1295263171195984 + 0.001 * 6.968667030334473
Epoch 380, val loss: 0.7043519020080566
Epoch 390, training loss: 0.12052488327026367 = 0.11355634033679962 + 0.001 * 6.968541622161865
Epoch 390, val loss: 0.7104183435440063
Epoch 400, training loss: 0.1065966933965683 = 0.0996289923787117 + 0.001 * 6.967698097229004
Epoch 400, val loss: 0.718172013759613
Epoch 410, training loss: 0.09448198974132538 = 0.08750609308481216 + 0.001 * 6.975893020629883
Epoch 410, val loss: 0.7272738814353943
Epoch 420, training loss: 0.08393754810094833 = 0.07696926593780518 + 0.001 * 6.968282699584961
Epoch 420, val loss: 0.7372990250587463
Epoch 430, training loss: 0.07479996234178543 = 0.06783138960599899 + 0.001 * 6.968570232391357
Epoch 430, val loss: 0.7479302883148193
Epoch 440, training loss: 0.06690645217895508 = 0.059937961399555206 + 0.001 * 6.968490123748779
Epoch 440, val loss: 0.7590005397796631
Epoch 450, training loss: 0.060103416442871094 = 0.05313490703701973 + 0.001 * 6.968509197235107
Epoch 450, val loss: 0.7704078555107117
Epoch 460, training loss: 0.05425294116139412 = 0.04727946221828461 + 0.001 * 6.973479270935059
Epoch 460, val loss: 0.7820553183555603
Epoch 470, training loss: 0.04920879751443863 = 0.042239997535943985 + 0.001 * 6.968799114227295
Epoch 470, val loss: 0.7938392758369446
Epoch 480, training loss: 0.044865142554044724 = 0.037896253168582916 + 0.001 * 6.9688897132873535
Epoch 480, val loss: 0.8057023882865906
Epoch 490, training loss: 0.04110988974571228 = 0.03414205089211464 + 0.001 * 6.967838764190674
Epoch 490, val loss: 0.8175874948501587
Epoch 500, training loss: 0.03785616159439087 = 0.030886469408869743 + 0.001 * 6.969690322875977
Epoch 500, val loss: 0.8294616341590881
Epoch 510, training loss: 0.035020068287849426 = 0.028052659705281258 + 0.001 * 6.967409610748291
Epoch 510, val loss: 0.8412660956382751
Epoch 520, training loss: 0.03254256770014763 = 0.025576502084732056 + 0.001 * 6.966065406799316
Epoch 520, val loss: 0.8529344201087952
Epoch 530, training loss: 0.030370693653821945 = 0.023404793813824654 + 0.001 * 6.965900421142578
Epoch 530, val loss: 0.8644112348556519
Epoch 540, training loss: 0.02846141904592514 = 0.021492503583431244 + 0.001 * 6.96891450881958
Epoch 540, val loss: 0.8756890296936035
Epoch 550, training loss: 0.02676444686949253 = 0.019802045077085495 + 0.001 * 6.962401390075684
Epoch 550, val loss: 0.886738657951355
Epoch 560, training loss: 0.025263473391532898 = 0.01830209605395794 + 0.001 * 6.96137809753418
Epoch 560, val loss: 0.8975421786308289
Epoch 570, training loss: 0.023927226662635803 = 0.016966307535767555 + 0.001 * 6.9609198570251465
Epoch 570, val loss: 0.9081081748008728
Epoch 580, training loss: 0.022732334211468697 = 0.015772605314850807 + 0.001 * 6.959728717803955
Epoch 580, val loss: 0.9184085130691528
Epoch 590, training loss: 0.021659579128026962 = 0.014702333137392998 + 0.001 * 6.957246780395508
Epoch 590, val loss: 0.928425133228302
Epoch 600, training loss: 0.02069452777504921 = 0.013739488087594509 + 0.001 * 6.955040454864502
Epoch 600, val loss: 0.9381964802742004
Epoch 610, training loss: 0.019848428666591644 = 0.012870477512478828 + 0.001 * 6.977951526641846
Epoch 610, val loss: 0.9477006793022156
Epoch 620, training loss: 0.019053174182772636 = 0.012083839625120163 + 0.001 * 6.969334602355957
Epoch 620, val loss: 0.9569480419158936
Epoch 630, training loss: 0.018321586772799492 = 0.011369715444743633 + 0.001 * 6.951871395111084
Epoch 630, val loss: 0.9659571051597595
Epoch 640, training loss: 0.017668871209025383 = 0.010719615034759045 + 0.001 * 6.94925594329834
Epoch 640, val loss: 0.9747074842453003
Epoch 650, training loss: 0.017073381692171097 = 0.010126201435923576 + 0.001 * 6.947180271148682
Epoch 650, val loss: 0.9832285642623901
Epoch 660, training loss: 0.01653062179684639 = 0.009583181701600552 + 0.001 * 6.947440147399902
Epoch 660, val loss: 0.9915199279785156
Epoch 670, training loss: 0.016034651547670364 = 0.009085058234632015 + 0.001 * 6.949593544006348
Epoch 670, val loss: 0.9995759129524231
Epoch 680, training loss: 0.015573794022202492 = 0.008627117611467838 + 0.001 * 6.946676731109619
Epoch 680, val loss: 1.0074303150177002
Epoch 690, training loss: 0.01514711044728756 = 0.008205207996070385 + 0.001 * 6.941902160644531
Epoch 690, val loss: 1.015071153640747
Epoch 700, training loss: 0.014753825031220913 = 0.007815598510205746 + 0.001 * 6.938226222991943
Epoch 700, val loss: 1.0225108861923218
Epoch 710, training loss: 0.014401714317500591 = 0.007455111015588045 + 0.001 * 6.946602821350098
Epoch 710, val loss: 1.0297616720199585
Epoch 720, training loss: 0.014065640047192574 = 0.00712093198671937 + 0.001 * 6.94470739364624
Epoch 720, val loss: 1.0368295907974243
Epoch 730, training loss: 0.0137525275349617 = 0.006810549646615982 + 0.001 * 6.9419779777526855
Epoch 730, val loss: 1.0437264442443848
Epoch 740, training loss: 0.013456933200359344 = 0.006521738599985838 + 0.001 * 6.935194969177246
Epoch 740, val loss: 1.0504345893859863
Epoch 750, training loss: 0.013179507106542587 = 0.006252526771277189 + 0.001 * 6.926980018615723
Epoch 750, val loss: 1.0569920539855957
Epoch 760, training loss: 0.012936456128954887 = 0.006001209374517202 + 0.001 * 6.935246467590332
Epoch 760, val loss: 1.0633795261383057
Epoch 770, training loss: 0.012711203657090664 = 0.005766218528151512 + 0.001 * 6.9449849128723145
Epoch 770, val loss: 1.069632649421692
Epoch 780, training loss: 0.012474561110138893 = 0.00554615305736661 + 0.001 * 6.928407669067383
Epoch 780, val loss: 1.0757217407226562
Epoch 790, training loss: 0.012265272438526154 = 0.005339771043509245 + 0.001 * 6.925500392913818
Epoch 790, val loss: 1.0816774368286133
Epoch 800, training loss: 0.012064799666404724 = 0.005145956762135029 + 0.001 * 6.918842792510986
Epoch 800, val loss: 1.0874885320663452
Epoch 810, training loss: 0.011881520971655846 = 0.0049637118354439735 + 0.001 * 6.917808532714844
Epoch 810, val loss: 1.0931651592254639
Epoch 820, training loss: 0.01171968411654234 = 0.004792138934135437 + 0.001 * 6.927545070648193
Epoch 820, val loss: 1.098720908164978
Epoch 830, training loss: 0.01155576016753912 = 0.004630415700376034 + 0.001 * 6.925343990325928
Epoch 830, val loss: 1.10414457321167
Epoch 840, training loss: 0.011390110477805138 = 0.004477791488170624 + 0.001 * 6.912319183349609
Epoch 840, val loss: 1.109459638595581
Epoch 850, training loss: 0.011254522018134594 = 0.004333588760346174 + 0.001 * 6.920932769775391
Epoch 850, val loss: 1.1146515607833862
Epoch 860, training loss: 0.01113059837371111 = 0.004197168629616499 + 0.001 * 6.93342924118042
Epoch 860, val loss: 1.1197359561920166
Epoch 870, training loss: 0.01099410280585289 = 0.0040680235251784325 + 0.001 * 6.9260783195495605
Epoch 870, val loss: 1.124720811843872
Epoch 880, training loss: 0.010854465886950493 = 0.003945621196180582 + 0.001 * 6.908844947814941
Epoch 880, val loss: 1.1295859813690186
Epoch 890, training loss: 0.010734757408499718 = 0.0038295025005936623 + 0.001 * 6.905254364013672
Epoch 890, val loss: 1.1343568563461304
Epoch 900, training loss: 0.010623488575220108 = 0.003719230182468891 + 0.001 * 6.904257774353027
Epoch 900, val loss: 1.139024257659912
Epoch 910, training loss: 0.010537120513617992 = 0.0036144047044217587 + 0.001 * 6.922715663909912
Epoch 910, val loss: 1.143600344657898
Epoch 920, training loss: 0.01043039932847023 = 0.003514692187309265 + 0.001 * 6.915706157684326
Epoch 920, val loss: 1.1480865478515625
Epoch 930, training loss: 0.010320760309696198 = 0.0034197461791336536 + 0.001 * 6.9010138511657715
Epoch 930, val loss: 1.1524755954742432
Epoch 940, training loss: 0.010234139859676361 = 0.003329274244606495 + 0.001 * 6.904865264892578
Epoch 940, val loss: 1.156785249710083
Epoch 950, training loss: 0.010142575949430466 = 0.0032429953571408987 + 0.001 * 6.899580478668213
Epoch 950, val loss: 1.1610037088394165
Epoch 960, training loss: 0.010056550614535809 = 0.003160657826811075 + 0.001 * 6.89589262008667
Epoch 960, val loss: 1.1651527881622314
Epoch 970, training loss: 0.009979451075196266 = 0.0030820213723927736 + 0.001 * 6.8974289894104
Epoch 970, val loss: 1.1692063808441162
Epoch 980, training loss: 0.009908938780426979 = 0.00300685060210526 + 0.001 * 6.902088165283203
Epoch 980, val loss: 1.1731929779052734
Epoch 990, training loss: 0.009848760440945625 = 0.002934962511062622 + 0.001 * 6.913797855377197
Epoch 990, val loss: 1.1771141290664673
Epoch 1000, training loss: 0.009765523485839367 = 0.0028661510441452265 + 0.001 * 6.899372100830078
Epoch 1000, val loss: 1.1809524297714233
Epoch 1010, training loss: 0.009697037748992443 = 0.002800254849717021 + 0.001 * 6.896782875061035
Epoch 1010, val loss: 1.1847273111343384
Epoch 1020, training loss: 0.00966613832861185 = 0.0027371100150048733 + 0.001 * 6.929028034210205
Epoch 1020, val loss: 1.1884249448776245
Epoch 1030, training loss: 0.009575909934937954 = 0.002676559379324317 + 0.001 * 6.899350643157959
Epoch 1030, val loss: 1.1920722723007202
Epoch 1040, training loss: 0.009512531571090221 = 0.0026184648741036654 + 0.001 * 6.89406681060791
Epoch 1040, val loss: 1.1956428289413452
Epoch 1050, training loss: 0.00945741031318903 = 0.002562691690400243 + 0.001 * 6.894718647003174
Epoch 1050, val loss: 1.1991522312164307
Epoch 1060, training loss: 0.00939624197781086 = 0.0025091138668358326 + 0.001 * 6.88712739944458
Epoch 1060, val loss: 1.202601671218872
Epoch 1070, training loss: 0.009373298846185207 = 0.002457620808854699 + 0.001 * 6.915678024291992
Epoch 1070, val loss: 1.2059880495071411
Epoch 1080, training loss: 0.009298980236053467 = 0.002408101223409176 + 0.001 * 6.890878677368164
Epoch 1080, val loss: 1.209326982498169
Epoch 1090, training loss: 0.009241366758942604 = 0.0023604577872902155 + 0.001 * 6.880908489227295
Epoch 1090, val loss: 1.2125957012176514
Epoch 1100, training loss: 0.009198207408189774 = 0.0023145978339016438 + 0.001 * 6.883608818054199
Epoch 1100, val loss: 1.215820074081421
Epoch 1110, training loss: 0.009147902950644493 = 0.002270433586090803 + 0.001 * 6.877469539642334
Epoch 1110, val loss: 1.2189806699752808
Epoch 1120, training loss: 0.009132646024227142 = 0.0022278716787695885 + 0.001 * 6.904774188995361
Epoch 1120, val loss: 1.2220972776412964
Epoch 1130, training loss: 0.009076248854398727 = 0.002186843426898122 + 0.001 * 6.889405250549316
Epoch 1130, val loss: 1.2251613140106201
Epoch 1140, training loss: 0.00902671180665493 = 0.002147275721654296 + 0.001 * 6.8794355392456055
Epoch 1140, val loss: 1.2281694412231445
Epoch 1150, training loss: 0.009010282345116138 = 0.0021090952213853598 + 0.001 * 6.901186943054199
Epoch 1150, val loss: 1.2311303615570068
Epoch 1160, training loss: 0.008965253829956055 = 0.002072246978059411 + 0.001 * 6.893006324768066
Epoch 1160, val loss: 1.2340524196624756
Epoch 1170, training loss: 0.008917022496461868 = 0.0020366557873785496 + 0.001 * 6.88036584854126
Epoch 1170, val loss: 1.2369097471237183
Epoch 1180, training loss: 0.008876355364918709 = 0.002002277411520481 + 0.001 * 6.874078273773193
Epoch 1180, val loss: 1.239736795425415
Epoch 1190, training loss: 0.008843956515192986 = 0.0019690534099936485 + 0.001 * 6.874903202056885
Epoch 1190, val loss: 1.2425241470336914
Epoch 1200, training loss: 0.008825710043311119 = 0.0019369263900443912 + 0.001 * 6.888782978057861
Epoch 1200, val loss: 1.2452510595321655
Epoch 1210, training loss: 0.008775856345891953 = 0.0019058531615883112 + 0.001 * 6.8700032234191895
Epoch 1210, val loss: 1.2479532957077026
Epoch 1220, training loss: 0.00873991847038269 = 0.0018757905345410109 + 0.001 * 6.8641276359558105
Epoch 1220, val loss: 1.2506119012832642
Epoch 1230, training loss: 0.008712234906852245 = 0.0018466918263584375 + 0.001 * 6.865542888641357
Epoch 1230, val loss: 1.2532296180725098
Epoch 1240, training loss: 0.008690117858350277 = 0.0018185110529884696 + 0.001 * 6.871606349945068
Epoch 1240, val loss: 1.2558051347732544
Epoch 1250, training loss: 0.008677475154399872 = 0.001791225979104638 + 0.001 * 6.88624906539917
Epoch 1250, val loss: 1.2583457231521606
Epoch 1260, training loss: 0.008645214140415192 = 0.0017647831700742245 + 0.001 * 6.880430698394775
Epoch 1260, val loss: 1.2608351707458496
Epoch 1270, training loss: 0.008616257458925247 = 0.0017391516594216228 + 0.001 * 6.877105236053467
Epoch 1270, val loss: 1.2633068561553955
Epoch 1280, training loss: 0.008573259226977825 = 0.0017143053701147437 + 0.001 * 6.858953475952148
Epoch 1280, val loss: 1.2657294273376465
Epoch 1290, training loss: 0.008556727319955826 = 0.0016902013448998332 + 0.001 * 6.866526126861572
Epoch 1290, val loss: 1.2681236267089844
Epoch 1300, training loss: 0.008533759973943233 = 0.0016668100142851472 + 0.001 * 6.866949558258057
Epoch 1300, val loss: 1.27046799659729
Epoch 1310, training loss: 0.00850182306021452 = 0.0016441207844763994 + 0.001 * 6.857701778411865
Epoch 1310, val loss: 1.27280592918396
Epoch 1320, training loss: 0.008484486490488052 = 0.001622091280296445 + 0.001 * 6.8623948097229
Epoch 1320, val loss: 1.2750940322875977
Epoch 1330, training loss: 0.008482073433697224 = 0.0016006870428100228 + 0.001 * 6.8813862800598145
Epoch 1330, val loss: 1.2773442268371582
Epoch 1340, training loss: 0.008451396599411964 = 0.0015799084212630987 + 0.001 * 6.871487617492676
Epoch 1340, val loss: 1.2795993089675903
Epoch 1350, training loss: 0.008409874513745308 = 0.0015597051242366433 + 0.001 * 6.850168704986572
Epoch 1350, val loss: 1.2817912101745605
Epoch 1360, training loss: 0.008399018086493015 = 0.0015400754055008292 + 0.001 * 6.858942031860352
Epoch 1360, val loss: 1.2839702367782593
Epoch 1370, training loss: 0.008383212611079216 = 0.0015209767734631896 + 0.001 * 6.8622355461120605
Epoch 1370, val loss: 1.2861188650131226
Epoch 1380, training loss: 0.008354758843779564 = 0.0015023925807327032 + 0.001 * 6.852365970611572
Epoch 1380, val loss: 1.2882319688796997
Epoch 1390, training loss: 0.008349831216037273 = 0.0014843267854303122 + 0.001 * 6.865503787994385
Epoch 1390, val loss: 1.2903354167938232
Epoch 1400, training loss: 0.0083168251439929 = 0.0014667259529232979 + 0.001 * 6.850099086761475
Epoch 1400, val loss: 1.2923892736434937
Epoch 1410, training loss: 0.008318457752466202 = 0.0014496109215542674 + 0.001 * 6.868846416473389
Epoch 1410, val loss: 1.2944365739822388
Epoch 1420, training loss: 0.00829659029841423 = 0.0014329226687550545 + 0.001 * 6.863667011260986
Epoch 1420, val loss: 1.2964210510253906
Epoch 1430, training loss: 0.008266662247478962 = 0.0014166675973683596 + 0.001 * 6.84999418258667
Epoch 1430, val loss: 1.2984259128570557
Epoch 1440, training loss: 0.00827222503721714 = 0.001400829409249127 + 0.001 * 6.871395587921143
Epoch 1440, val loss: 1.3003885746002197
Epoch 1450, training loss: 0.008238735608756542 = 0.001385366078466177 + 0.001 * 6.853369235992432
Epoch 1450, val loss: 1.3023161888122559
Epoch 1460, training loss: 0.008209271356463432 = 0.0013702925061807036 + 0.001 * 6.838978290557861
Epoch 1460, val loss: 1.3042458295822144
Epoch 1470, training loss: 0.008202780038118362 = 0.0013555856421589851 + 0.001 * 6.847194671630859
Epoch 1470, val loss: 1.3061583042144775
Epoch 1480, training loss: 0.008198695257306099 = 0.0013412206899374723 + 0.001 * 6.857473850250244
Epoch 1480, val loss: 1.3080356121063232
Epoch 1490, training loss: 0.00817450974136591 = 0.0013271791394799948 + 0.001 * 6.847330093383789
Epoch 1490, val loss: 1.309883713722229
Epoch 1500, training loss: 0.008160235360264778 = 0.0013134664623066783 + 0.001 * 6.846768379211426
Epoch 1500, val loss: 1.3117468357086182
Epoch 1510, training loss: 0.008170322515070438 = 0.001300063799135387 + 0.001 * 6.870258331298828
Epoch 1510, val loss: 1.3135513067245483
Epoch 1520, training loss: 0.008141162805259228 = 0.0012869586935266852 + 0.001 * 6.854203701019287
Epoch 1520, val loss: 1.3153802156448364
Epoch 1530, training loss: 0.008114405907690525 = 0.0012741215759888291 + 0.001 * 6.84028434753418
Epoch 1530, val loss: 1.317164659500122
Epoch 1540, training loss: 0.008091028779745102 = 0.001261563622392714 + 0.001 * 6.829464912414551
Epoch 1540, val loss: 1.3189443349838257
Epoch 1550, training loss: 0.008078078739345074 = 0.0012492428068071604 + 0.001 * 6.828835487365723
Epoch 1550, val loss: 1.3207162618637085
Epoch 1560, training loss: 0.008084544911980629 = 0.0012371608754619956 + 0.001 * 6.847383975982666
Epoch 1560, val loss: 1.3224661350250244
Epoch 1570, training loss: 0.008058713749051094 = 0.0012253320310264826 + 0.001 * 6.833381175994873
Epoch 1570, val loss: 1.3242002725601196
Epoch 1580, training loss: 0.008037427440285683 = 0.0012137226294726133 + 0.001 * 6.823704242706299
Epoch 1580, val loss: 1.3259254693984985
Epoch 1590, training loss: 0.008035387843847275 = 0.001202322542667389 + 0.001 * 6.833065509796143
Epoch 1590, val loss: 1.3276406526565552
Epoch 1600, training loss: 0.008046599105000496 = 0.0011911189649254084 + 0.001 * 6.855480194091797
Epoch 1600, val loss: 1.3293571472167969
Epoch 1610, training loss: 0.008028772659599781 = 0.0011801028158515692 + 0.001 * 6.848669528961182
Epoch 1610, val loss: 1.3310426473617554
Epoch 1620, training loss: 0.007988416589796543 = 0.0011692775879055262 + 0.001 * 6.819139003753662
Epoch 1620, val loss: 1.332751750946045
Epoch 1630, training loss: 0.008002671413123608 = 0.0011586627224460244 + 0.001 * 6.844007968902588
Epoch 1630, val loss: 1.3344463109970093
Epoch 1640, training loss: 0.00796492863446474 = 0.0011482362169772387 + 0.001 * 6.816691875457764
Epoch 1640, val loss: 1.3361095190048218
Epoch 1650, training loss: 0.007957708090543747 = 0.001138007384724915 + 0.001 * 6.819700717926025
Epoch 1650, val loss: 1.3377922773361206
Epoch 1660, training loss: 0.007958780974149704 = 0.001127935480326414 + 0.001 * 6.830844879150391
Epoch 1660, val loss: 1.3394337892532349
Epoch 1670, training loss: 0.007978271692991257 = 0.0011180423898622394 + 0.001 * 6.860229015350342
Epoch 1670, val loss: 1.3411014080047607
Epoch 1680, training loss: 0.007935739122331142 = 0.0011083007557317615 + 0.001 * 6.8274383544921875
Epoch 1680, val loss: 1.3427202701568604
Epoch 1690, training loss: 0.007908971980214119 = 0.0010987146524712443 + 0.001 * 6.8102569580078125
Epoch 1690, val loss: 1.3443726301193237
Epoch 1700, training loss: 0.007901443168520927 = 0.0010892648715525866 + 0.001 * 6.812177658081055
Epoch 1700, val loss: 1.3459941148757935
Epoch 1710, training loss: 0.007911945693194866 = 0.0010799545561894774 + 0.001 * 6.831990718841553
Epoch 1710, val loss: 1.3476375341415405
Epoch 1720, training loss: 0.007880753837525845 = 0.0010707835899665952 + 0.001 * 6.809969902038574
Epoch 1720, val loss: 1.3492341041564941
Epoch 1730, training loss: 0.007882457226514816 = 0.0010617626830935478 + 0.001 * 6.820694446563721
Epoch 1730, val loss: 1.35088312625885
Epoch 1740, training loss: 0.007858537137508392 = 0.001052859821356833 + 0.001 * 6.8056769371032715
Epoch 1740, val loss: 1.3524889945983887
Epoch 1750, training loss: 0.007872572168707848 = 0.0010440764017403126 + 0.001 * 6.828495025634766
Epoch 1750, val loss: 1.3541085720062256
Epoch 1760, training loss: 0.007860245183110237 = 0.0010354346595704556 + 0.001 * 6.824810028076172
Epoch 1760, val loss: 1.3557230234146118
Epoch 1770, training loss: 0.007832453586161137 = 0.001026929123327136 + 0.001 * 6.805523872375488
Epoch 1770, val loss: 1.357320785522461
Epoch 1780, training loss: 0.007833253592252731 = 0.0010185566497966647 + 0.001 * 6.814696788787842
Epoch 1780, val loss: 1.3589478731155396
Epoch 1790, training loss: 0.00784360896795988 = 0.0010103139793500304 + 0.001 * 6.833294868469238
Epoch 1790, val loss: 1.3605293035507202
Epoch 1800, training loss: 0.007815439254045486 = 0.0010022209025919437 + 0.001 * 6.813218116760254
Epoch 1800, val loss: 1.3620960712432861
Epoch 1810, training loss: 0.0078056808561086655 = 0.0009942561155185103 + 0.001 * 6.811424255371094
Epoch 1810, val loss: 1.3636926412582397
Epoch 1820, training loss: 0.0078040300868451595 = 0.0009864087915048003 + 0.001 * 6.817620754241943
Epoch 1820, val loss: 1.3652622699737549
Epoch 1830, training loss: 0.0077953459694981575 = 0.0009786731097847223 + 0.001 * 6.8166728019714355
Epoch 1830, val loss: 1.3668076992034912
Epoch 1840, training loss: 0.007777846418321133 = 0.0009710886515676975 + 0.001 * 6.80675745010376
Epoch 1840, val loss: 1.3683916330337524
Epoch 1850, training loss: 0.007756128441542387 = 0.0009636083268560469 + 0.001 * 6.792520046234131
Epoch 1850, val loss: 1.369926929473877
Epoch 1860, training loss: 0.007782986853271723 = 0.000956262752879411 + 0.001 * 6.826723575592041
Epoch 1860, val loss: 1.3714529275894165
Epoch 1870, training loss: 0.00774262472987175 = 0.0009490413358435035 + 0.001 * 6.793582916259766
Epoch 1870, val loss: 1.3729740381240845
Epoch 1880, training loss: 0.007770242635160685 = 0.0009419539128430188 + 0.001 * 6.8282880783081055
Epoch 1880, val loss: 1.3745357990264893
Epoch 1890, training loss: 0.0077565694227814674 = 0.0009349900647066534 + 0.001 * 6.8215789794921875
Epoch 1890, val loss: 1.3760294914245605
Epoch 1900, training loss: 0.007742638699710369 = 0.0009281301172450185 + 0.001 * 6.814508438110352
Epoch 1900, val loss: 1.3774948120117188
Epoch 1910, training loss: 0.0077313086949288845 = 0.0009214127785526216 + 0.001 * 6.8098955154418945
Epoch 1910, val loss: 1.3790085315704346
Epoch 1920, training loss: 0.007719932124018669 = 0.0009147705277428031 + 0.001 * 6.805160999298096
Epoch 1920, val loss: 1.380509853363037
Epoch 1930, training loss: 0.007704086601734161 = 0.0009082542965188622 + 0.001 * 6.79583215713501
Epoch 1930, val loss: 1.381959080696106
Epoch 1940, training loss: 0.007702860049903393 = 0.0009018287528306246 + 0.001 * 6.80103063583374
Epoch 1940, val loss: 1.3834556341171265
Epoch 1950, training loss: 0.007714168168604374 = 0.0008955085650086403 + 0.001 * 6.81865930557251
Epoch 1950, val loss: 1.3848719596862793
Epoch 1960, training loss: 0.0076773324981331825 = 0.0008892967598512769 + 0.001 * 6.7880353927612305
Epoch 1960, val loss: 1.386328935623169
Epoch 1970, training loss: 0.007684665732085705 = 0.0008831883314996958 + 0.001 * 6.801476955413818
Epoch 1970, val loss: 1.387792944908142
Epoch 1980, training loss: 0.007688810583204031 = 0.0008771740831434727 + 0.001 * 6.811635971069336
Epoch 1980, val loss: 1.3891831636428833
Epoch 1990, training loss: 0.007654582615941763 = 0.0008712654816918075 + 0.001 * 6.7833170890808105
Epoch 1990, val loss: 1.3906224966049194
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 1.9509027004241943 = 1.9423059225082397 + 0.001 * 8.596827507019043
Epoch 0, val loss: 1.9495474100112915
Epoch 10, training loss: 1.9413782358169556 = 1.932781457901001 + 0.001 * 8.596780776977539
Epoch 10, val loss: 1.9403808116912842
Epoch 20, training loss: 1.929643154144287 = 1.921046495437622 + 0.001 * 8.596598625183105
Epoch 20, val loss: 1.9286108016967773
Epoch 30, training loss: 1.9130454063415527 = 1.904449224472046 + 0.001 * 8.596144676208496
Epoch 30, val loss: 1.9115550518035889
Epoch 40, training loss: 1.888511300086975 = 1.8799163103103638 + 0.001 * 8.595005989074707
Epoch 40, val loss: 1.8864271640777588
Epoch 50, training loss: 1.8541754484176636 = 1.8455836772918701 + 0.001 * 8.59172248840332
Epoch 50, val loss: 1.8523701429367065
Epoch 60, training loss: 1.8142682313919067 = 1.8056882619857788 + 0.001 * 8.579913139343262
Epoch 60, val loss: 1.815578818321228
Epoch 70, training loss: 1.7769293785095215 = 1.768399715423584 + 0.001 * 8.529638290405273
Epoch 70, val loss: 1.783068060874939
Epoch 80, training loss: 1.730535626411438 = 1.7223002910614014 + 0.001 * 8.235342979431152
Epoch 80, val loss: 1.7407140731811523
Epoch 90, training loss: 1.6649422645568848 = 1.657038927078247 + 0.001 * 7.903388023376465
Epoch 90, val loss: 1.6817587614059448
Epoch 100, training loss: 1.5768353939056396 = 1.5690470933914185 + 0.001 * 7.788243770599365
Epoch 100, val loss: 1.6037951707839966
Epoch 110, training loss: 1.4720982313156128 = 1.4644551277160645 + 0.001 * 7.643066883087158
Epoch 110, val loss: 1.5140925645828247
Epoch 120, training loss: 1.3608853816986084 = 1.3533387184143066 + 0.001 * 7.546606540679932
Epoch 120, val loss: 1.4201831817626953
Epoch 130, training loss: 1.2482293844223022 = 1.240694284439087 + 0.001 * 7.535043716430664
Epoch 130, val loss: 1.3280926942825317
Epoch 140, training loss: 1.138237714767456 = 1.130728840827942 + 0.001 * 7.50887393951416
Epoch 140, val loss: 1.241572380065918
Epoch 150, training loss: 1.0356576442718506 = 1.028173565864563 + 0.001 * 7.484065532684326
Epoch 150, val loss: 1.163251519203186
Epoch 160, training loss: 0.9425559639930725 = 0.9351022839546204 + 0.001 * 7.453685283660889
Epoch 160, val loss: 1.0937275886535645
Epoch 170, training loss: 0.8586307168006897 = 0.8512188196182251 + 0.001 * 7.41187047958374
Epoch 170, val loss: 1.0317307710647583
Epoch 180, training loss: 0.7835283279418945 = 0.7761560678482056 + 0.001 * 7.372282028198242
Epoch 180, val loss: 0.9769854545593262
Epoch 190, training loss: 0.7170636057853699 = 0.7097103595733643 + 0.001 * 7.353274345397949
Epoch 190, val loss: 0.9303510785102844
Epoch 200, training loss: 0.6582371592521667 = 0.6508930921554565 + 0.001 * 7.344091892242432
Epoch 200, val loss: 0.8918574452400208
Epoch 210, training loss: 0.6052263379096985 = 0.597896933555603 + 0.001 * 7.329387187957764
Epoch 210, val loss: 0.8608847260475159
Epoch 220, training loss: 0.5563448667526245 = 0.5490375757217407 + 0.001 * 7.307298183441162
Epoch 220, val loss: 0.8365341424942017
Epoch 230, training loss: 0.5106492042541504 = 0.5033842921257019 + 0.001 * 7.264919281005859
Epoch 230, val loss: 0.8180261850357056
Epoch 240, training loss: 0.4678518772125244 = 0.4606476426124573 + 0.001 * 7.204235553741455
Epoch 240, val loss: 0.8047147393226624
Epoch 250, training loss: 0.42799055576324463 = 0.42083901166915894 + 0.001 * 7.151536464691162
Epoch 250, val loss: 0.7959499955177307
Epoch 260, training loss: 0.3909914791584015 = 0.38387998938560486 + 0.001 * 7.111502647399902
Epoch 260, val loss: 0.7913745045661926
Epoch 270, training loss: 0.35661259293556213 = 0.3495280146598816 + 0.001 * 7.08458137512207
Epoch 270, val loss: 0.7905681133270264
Epoch 280, training loss: 0.3244054317474365 = 0.31733182072639465 + 0.001 * 7.073601722717285
Epoch 280, val loss: 0.7930036783218384
Epoch 290, training loss: 0.2939322590827942 = 0.28686240315437317 + 0.001 * 7.069862365722656
Epoch 290, val loss: 0.798149585723877
Epoch 300, training loss: 0.2650375962257385 = 0.25797221064567566 + 0.001 * 7.065379619598389
Epoch 300, val loss: 0.8056280016899109
Epoch 310, training loss: 0.23787787556648254 = 0.23081448674201965 + 0.001 * 7.0633955001831055
Epoch 310, val loss: 0.815289318561554
Epoch 320, training loss: 0.21275514364242554 = 0.20568978786468506 + 0.001 * 7.065356731414795
Epoch 320, val loss: 0.8271127939224243
Epoch 330, training loss: 0.1899050772190094 = 0.18284255266189575 + 0.001 * 7.0625224113464355
Epoch 330, val loss: 0.840946614742279
Epoch 340, training loss: 0.16942010819911957 = 0.1623571217060089 + 0.001 * 7.06298303604126
Epoch 340, val loss: 0.8567649126052856
Epoch 350, training loss: 0.1512337327003479 = 0.144172802567482 + 0.001 * 7.0609331130981445
Epoch 350, val loss: 0.8743975758552551
Epoch 360, training loss: 0.13519951701164246 = 0.12813939154148102 + 0.001 * 7.0601325035095215
Epoch 360, val loss: 0.8936132788658142
Epoch 370, training loss: 0.12113141268491745 = 0.11406978964805603 + 0.001 * 7.061622142791748
Epoch 370, val loss: 0.9140758514404297
Epoch 380, training loss: 0.10881218314170837 = 0.10175132006406784 + 0.001 * 7.06086540222168
Epoch 380, val loss: 0.9353847503662109
Epoch 390, training loss: 0.09802573919296265 = 0.09096671640872955 + 0.001 * 7.059020042419434
Epoch 390, val loss: 0.9571553468704224
Epoch 400, training loss: 0.0885852798819542 = 0.0815124362707138 + 0.001 * 7.0728440284729
Epoch 400, val loss: 0.9791362881660461
Epoch 410, training loss: 0.08027482777833939 = 0.07321176677942276 + 0.001 * 7.063057899475098
Epoch 410, val loss: 1.001091480255127
Epoch 420, training loss: 0.07297324389219284 = 0.06591370701789856 + 0.001 * 7.0595383644104
Epoch 420, val loss: 1.0228437185287476
Epoch 430, training loss: 0.06654532253742218 = 0.05948744714260101 + 0.001 * 7.057878017425537
Epoch 430, val loss: 1.044277548789978
Epoch 440, training loss: 0.06087840348482132 = 0.05382193252444267 + 0.001 * 7.056469440460205
Epoch 440, val loss: 1.0652662515640259
Epoch 450, training loss: 0.05587712302803993 = 0.048820946365594864 + 0.001 * 7.0561747550964355
Epoch 450, val loss: 1.0857222080230713
Epoch 460, training loss: 0.051454976201057434 = 0.044399574398994446 + 0.001 * 7.055399417877197
Epoch 460, val loss: 1.1056121587753296
Epoch 470, training loss: 0.04754098504781723 = 0.04048379883170128 + 0.001 * 7.057185649871826
Epoch 470, val loss: 1.1249264478683472
Epoch 480, training loss: 0.04406365007162094 = 0.03700954467058182 + 0.001 * 7.054103374481201
Epoch 480, val loss: 1.1436362266540527
Epoch 490, training loss: 0.040975578129291534 = 0.03392020985484123 + 0.001 * 7.055368423461914
Epoch 490, val loss: 1.1617653369903564
Epoch 500, training loss: 0.03821735456585884 = 0.031166676431894302 + 0.001 * 7.050678730010986
Epoch 500, val loss: 1.1793370246887207
Epoch 510, training loss: 0.035751938819885254 = 0.028701577335596085 + 0.001 * 7.05035924911499
Epoch 510, val loss: 1.1964399814605713
Epoch 520, training loss: 0.03353384882211685 = 0.02648361213505268 + 0.001 * 7.050235271453857
Epoch 520, val loss: 1.2131544351577759
Epoch 530, training loss: 0.03153393417596817 = 0.02448219619691372 + 0.001 * 7.051737308502197
Epoch 530, val loss: 1.2295162677764893
Epoch 540, training loss: 0.029719002544879913 = 0.02267385460436344 + 0.001 * 7.0451483726501465
Epoch 540, val loss: 1.2455430030822754
Epoch 550, training loss: 0.02808048017323017 = 0.021038182079792023 + 0.001 * 7.042297840118408
Epoch 550, val loss: 1.2612030506134033
Epoch 560, training loss: 0.02659604325890541 = 0.019557423889636993 + 0.001 * 7.038618087768555
Epoch 560, val loss: 1.2764887809753418
Epoch 570, training loss: 0.025266382843255997 = 0.018215691670775414 + 0.001 * 7.050691604614258
Epoch 570, val loss: 1.2913877964019775
Epoch 580, training loss: 0.024035517126321793 = 0.01699841022491455 + 0.001 * 7.037106513977051
Epoch 580, val loss: 1.3059018850326538
Epoch 590, training loss: 0.022923950105905533 = 0.01589254103600979 + 0.001 * 7.03140926361084
Epoch 590, val loss: 1.3200031518936157
Epoch 600, training loss: 0.02192278578877449 = 0.014886289834976196 + 0.001 * 7.036496162414551
Epoch 600, val loss: 1.3337045907974243
Epoch 610, training loss: 0.02099771797657013 = 0.013969236053526402 + 0.001 * 7.0284810066223145
Epoch 610, val loss: 1.3470107316970825
Epoch 620, training loss: 0.020158063620328903 = 0.01313198171555996 + 0.001 * 7.026081562042236
Epoch 620, val loss: 1.3599210977554321
Epoch 630, training loss: 0.019391359761357307 = 0.012365495786070824 + 0.001 * 7.0258636474609375
Epoch 630, val loss: 1.3724510669708252
Epoch 640, training loss: 0.018679331988096237 = 0.01166038028895855 + 0.001 * 7.018951892852783
Epoch 640, val loss: 1.3846698999404907
Epoch 650, training loss: 0.01804269850254059 = 0.011007838882505894 + 0.001 * 7.034859657287598
Epoch 650, val loss: 1.3966516256332397
Epoch 660, training loss: 0.01742217317223549 = 0.010401919484138489 + 0.001 * 7.020253658294678
Epoch 660, val loss: 1.408405065536499
Epoch 670, training loss: 0.016853293403983116 = 0.009838731959462166 + 0.001 * 7.014561176300049
Epoch 670, val loss: 1.4199358224868774
Epoch 680, training loss: 0.01633206382393837 = 0.009315370582044125 + 0.001 * 7.016693592071533
Epoch 680, val loss: 1.4312353134155273
Epoch 690, training loss: 0.01584733836352825 = 0.008829179219901562 + 0.001 * 7.01815938949585
Epoch 690, val loss: 1.4422962665557861
Epoch 700, training loss: 0.015389420092105865 = 0.008377551101148129 + 0.001 * 7.011868476867676
Epoch 700, val loss: 1.4531036615371704
Epoch 710, training loss: 0.014971977099776268 = 0.007957950234413147 + 0.001 * 7.014026641845703
Epoch 710, val loss: 1.4636483192443848
Epoch 720, training loss: 0.014582844451069832 = 0.0075679682195186615 + 0.001 * 7.014876365661621
Epoch 720, val loss: 1.4739454984664917
Epoch 730, training loss: 0.014217093586921692 = 0.007205250207334757 + 0.001 * 7.011842727661133
Epoch 730, val loss: 1.4840023517608643
Epoch 740, training loss: 0.01387699879705906 = 0.006867651361972094 + 0.001 * 7.009346961975098
Epoch 740, val loss: 1.4938262701034546
Epoch 750, training loss: 0.013557018712162971 = 0.0065531344152987 + 0.001 * 7.003884315490723
Epoch 750, val loss: 1.5034092664718628
Epoch 760, training loss: 0.013278374448418617 = 0.006259872578084469 + 0.001 * 7.0185017585754395
Epoch 760, val loss: 1.5127547979354858
Epoch 770, training loss: 0.013002948835492134 = 0.005986171308904886 + 0.001 * 7.016777515411377
Epoch 770, val loss: 1.5218675136566162
Epoch 780, training loss: 0.0127329770475626 = 0.0057304329238832 + 0.001 * 7.002544403076172
Epoch 780, val loss: 1.5307444334030151
Epoch 790, training loss: 0.012503005564212799 = 0.005491203628480434 + 0.001 * 7.011801719665527
Epoch 790, val loss: 1.5394034385681152
Epoch 800, training loss: 0.012283025309443474 = 0.005267218220978975 + 0.001 * 7.015806674957275
Epoch 800, val loss: 1.5478463172912598
Epoch 810, training loss: 0.012060742825269699 = 0.005057260859757662 + 0.001 * 7.003481864929199
Epoch 810, val loss: 1.5560781955718994
Epoch 820, training loss: 0.011861399747431278 = 0.004860240966081619 + 0.001 * 7.001158237457275
Epoch 820, val loss: 1.5641155242919922
Epoch 830, training loss: 0.011671625077724457 = 0.0046751610934734344 + 0.001 * 6.996464252471924
Epoch 830, val loss: 1.5719531774520874
Epoch 840, training loss: 0.011497742496430874 = 0.004501136019825935 + 0.001 * 6.996606349945068
Epoch 840, val loss: 1.5795984268188477
Epoch 850, training loss: 0.011332877911627293 = 0.0043373205699026585 + 0.001 * 6.995556831359863
Epoch 850, val loss: 1.5870505571365356
Epoch 860, training loss: 0.011186014860868454 = 0.004182933829724789 + 0.001 * 7.00308084487915
Epoch 860, val loss: 1.594317078590393
Epoch 870, training loss: 0.011034644208848476 = 0.004037327133119106 + 0.001 * 6.997316837310791
Epoch 870, val loss: 1.6014128923416138
Epoch 880, training loss: 0.010892382822930813 = 0.0038998383097350597 + 0.001 * 6.992544174194336
Epoch 880, val loss: 1.6083307266235352
Epoch 890, training loss: 0.010776366107165813 = 0.0037699041422456503 + 0.001 * 7.006462097167969
Epoch 890, val loss: 1.6150838136672974
Epoch 900, training loss: 0.010650180280208588 = 0.0036470009945333004 + 0.001 * 7.00317907333374
Epoch 900, val loss: 1.6216628551483154
Epoch 910, training loss: 0.010521519929170609 = 0.0035306529607623816 + 0.001 * 6.990866184234619
Epoch 910, val loss: 1.6280890703201294
Epoch 920, training loss: 0.010400833562016487 = 0.0034203878603875637 + 0.001 * 6.98044490814209
Epoch 920, val loss: 1.6343648433685303
Epoch 930, training loss: 0.01032086182385683 = 0.003315787296742201 + 0.001 * 7.0050740242004395
Epoch 930, val loss: 1.6404922008514404
Epoch 940, training loss: 0.010216200724244118 = 0.003216516925022006 + 0.001 * 6.999683856964111
Epoch 940, val loss: 1.646475911140442
Epoch 950, training loss: 0.010108308866620064 = 0.0031222195830196142 + 0.001 * 6.986088752746582
Epoch 950, val loss: 1.6523046493530273
Epoch 960, training loss: 0.010019742883741856 = 0.0030325616244226694 + 0.001 * 6.987180709838867
Epoch 960, val loss: 1.6580075025558472
Epoch 970, training loss: 0.009921664372086525 = 0.002947235247120261 + 0.001 * 6.974429130554199
Epoch 970, val loss: 1.6635806560516357
Epoch 980, training loss: 0.009841689839959145 = 0.002865979913622141 + 0.001 * 6.975708961486816
Epoch 980, val loss: 1.669027328491211
Epoch 990, training loss: 0.009761344641447067 = 0.0027885690797120333 + 0.001 * 6.972774982452393
Epoch 990, val loss: 1.6743499040603638
Epoch 1000, training loss: 0.009687863290309906 = 0.002714757341891527 + 0.001 * 6.973104953765869
Epoch 1000, val loss: 1.679556965827942
Epoch 1010, training loss: 0.009612723253667355 = 0.00264432723633945 + 0.001 * 6.968395233154297
Epoch 1010, val loss: 1.6846630573272705
Epoch 1020, training loss: 0.0095437690615654 = 0.002577073173597455 + 0.001 * 6.966695785522461
Epoch 1020, val loss: 1.6896495819091797
Epoch 1030, training loss: 0.009481998160481453 = 0.002512821927666664 + 0.001 * 6.969175338745117
Epoch 1030, val loss: 1.6945252418518066
Epoch 1040, training loss: 0.009421894326806068 = 0.0024513951502740383 + 0.001 * 6.970499515533447
Epoch 1040, val loss: 1.6993112564086914
Epoch 1050, training loss: 0.009359209798276424 = 0.0023926349822431803 + 0.001 * 6.966574668884277
Epoch 1050, val loss: 1.7039803266525269
Epoch 1060, training loss: 0.009303181432187557 = 0.0023363798391073942 + 0.001 * 6.966801643371582
Epoch 1060, val loss: 1.7085424661636353
Epoch 1070, training loss: 0.009257879108190536 = 0.002282485831528902 + 0.001 * 6.975393295288086
Epoch 1070, val loss: 1.7130118608474731
Epoch 1080, training loss: 0.009220374748110771 = 0.0022308258339762688 + 0.001 * 6.989549160003662
Epoch 1080, val loss: 1.717378854751587
Epoch 1090, training loss: 0.009153991937637329 = 0.002181303920224309 + 0.001 * 6.972687721252441
Epoch 1090, val loss: 1.7216596603393555
Epoch 1100, training loss: 0.009091141633689404 = 0.0021338004153221846 + 0.001 * 6.957341194152832
Epoch 1100, val loss: 1.7258362770080566
Epoch 1110, training loss: 0.00904710590839386 = 0.002088188659399748 + 0.001 * 6.958916664123535
Epoch 1110, val loss: 1.7299305200576782
Epoch 1120, training loss: 0.009045889601111412 = 0.0020443724934011698 + 0.001 * 7.001516819000244
Epoch 1120, val loss: 1.733946442604065
Epoch 1130, training loss: 0.008957773447036743 = 0.002002258785068989 + 0.001 * 6.955514430999756
Epoch 1130, val loss: 1.7378593683242798
Epoch 1140, training loss: 0.008914894424378872 = 0.001961752073839307 + 0.001 * 6.953142166137695
Epoch 1140, val loss: 1.7417103052139282
Epoch 1150, training loss: 0.008891831152141094 = 0.0019227436278015375 + 0.001 * 6.969087600708008
Epoch 1150, val loss: 1.745484471321106
Epoch 1160, training loss: 0.008833827450871468 = 0.0018850906053557992 + 0.001 * 6.948736667633057
Epoch 1160, val loss: 1.749178171157837
Epoch 1170, training loss: 0.008792742155492306 = 0.0018486535409465432 + 0.001 * 6.944088459014893
Epoch 1170, val loss: 1.7528263330459595
Epoch 1180, training loss: 0.008758447133004665 = 0.001813177834264934 + 0.001 * 6.9452691078186035
Epoch 1180, val loss: 1.7564537525177002
Epoch 1190, training loss: 0.008718844503164291 = 0.0017784603405743837 + 0.001 * 6.940383434295654
Epoch 1190, val loss: 1.760082721710205
Epoch 1200, training loss: 0.008692373521625996 = 0.0017443486722186208 + 0.001 * 6.948024272918701
Epoch 1200, val loss: 1.7637221813201904
Epoch 1210, training loss: 0.008665484376251698 = 0.0017106819432228804 + 0.001 * 6.954802513122559
Epoch 1210, val loss: 1.767377257347107
Epoch 1220, training loss: 0.008623800240457058 = 0.0016774166142567992 + 0.001 * 6.946383476257324
Epoch 1220, val loss: 1.7710950374603271
Epoch 1230, training loss: 0.008608933538198471 = 0.0016445284709334373 + 0.001 * 6.964405059814453
Epoch 1230, val loss: 1.7748476266860962
Epoch 1240, training loss: 0.0086145531386137 = 0.0016121533699333668 + 0.001 * 7.002399444580078
Epoch 1240, val loss: 1.7786402702331543
Epoch 1250, training loss: 0.0085142208263278 = 0.0015803086571395397 + 0.001 * 6.9339118003845215
Epoch 1250, val loss: 1.7824420928955078
Epoch 1260, training loss: 0.008486087433993816 = 0.0015490910736843944 + 0.001 * 6.9369964599609375
Epoch 1260, val loss: 1.7862529754638672
Epoch 1270, training loss: 0.008445448242127895 = 0.0015185534721240401 + 0.001 * 6.926894664764404
Epoch 1270, val loss: 1.7900614738464355
Epoch 1280, training loss: 0.008417177014052868 = 0.0014887272845953703 + 0.001 * 6.9284491539001465
Epoch 1280, val loss: 1.793853759765625
Epoch 1290, training loss: 0.008407910354435444 = 0.0014596368419006467 + 0.001 * 6.948273181915283
Epoch 1290, val loss: 1.7976326942443848
Epoch 1300, training loss: 0.008389272727072239 = 0.0014313444262370467 + 0.001 * 6.95792818069458
Epoch 1300, val loss: 1.8013900518417358
Epoch 1310, training loss: 0.008330849930644035 = 0.0014038137160241604 + 0.001 * 6.927036285400391
Epoch 1310, val loss: 1.8051024675369263
Epoch 1320, training loss: 0.008306415751576424 = 0.0013771087396889925 + 0.001 * 6.929306507110596
Epoch 1320, val loss: 1.8087795972824097
Epoch 1330, training loss: 0.008270079270005226 = 0.001351214130409062 + 0.001 * 6.9188642501831055
Epoch 1330, val loss: 1.8124064207077026
Epoch 1340, training loss: 0.008243565447628498 = 0.0013261090498417616 + 0.001 * 6.917455673217773
Epoch 1340, val loss: 1.815996527671814
Epoch 1350, training loss: 0.00821744929999113 = 0.0013017647434026003 + 0.001 * 6.915684223175049
Epoch 1350, val loss: 1.819545030593872
Epoch 1360, training loss: 0.008193954825401306 = 0.0012781573459506035 + 0.001 * 6.915797710418701
Epoch 1360, val loss: 1.8230509757995605
Epoch 1370, training loss: 0.008173702284693718 = 0.0012552618281915784 + 0.001 * 6.918440341949463
Epoch 1370, val loss: 1.826516032218933
Epoch 1380, training loss: 0.00818067230284214 = 0.001233072835020721 + 0.001 * 6.947598934173584
Epoch 1380, val loss: 1.8299269676208496
Epoch 1390, training loss: 0.008146818727254868 = 0.0012116092257201672 + 0.001 * 6.935209274291992
Epoch 1390, val loss: 1.8332643508911133
Epoch 1400, training loss: 0.008109960705041885 = 0.0011908551678061485 + 0.001 * 6.919105052947998
Epoch 1400, val loss: 1.8365672826766968
Epoch 1410, training loss: 0.008090037852525711 = 0.0011707532685250044 + 0.001 * 6.919284343719482
Epoch 1410, val loss: 1.839798092842102
Epoch 1420, training loss: 0.008061070926487446 = 0.0011513207573443651 + 0.001 * 6.909749507904053
Epoch 1420, val loss: 1.842981219291687
Epoch 1430, training loss: 0.008045707829296589 = 0.0011324926745146513 + 0.001 * 6.913214683532715
Epoch 1430, val loss: 1.8460909128189087
Epoch 1440, training loss: 0.008012781850993633 = 0.001114271697588265 + 0.001 * 6.898509979248047
Epoch 1440, val loss: 1.8491650819778442
Epoch 1450, training loss: 0.007997813634574413 = 0.0010966135887429118 + 0.001 * 6.901199817657471
Epoch 1450, val loss: 1.8521744012832642
Epoch 1460, training loss: 0.007984334602952003 = 0.00107950484380126 + 0.001 * 6.904829025268555
Epoch 1460, val loss: 1.8551483154296875
Epoch 1470, training loss: 0.007986124604940414 = 0.0010629164753481746 + 0.001 * 6.923207759857178
Epoch 1470, val loss: 1.8580636978149414
Epoch 1480, training loss: 0.007972394116222858 = 0.001046833349391818 + 0.001 * 6.925560474395752
Epoch 1480, val loss: 1.860917568206787
Epoch 1490, training loss: 0.007959539070725441 = 0.0010312682716175914 + 0.001 * 6.92827033996582
Epoch 1490, val loss: 1.863721489906311
Epoch 1500, training loss: 0.007924181409180164 = 0.0010162235703319311 + 0.001 * 6.907957553863525
Epoch 1500, val loss: 1.8664458990097046
Epoch 1510, training loss: 0.007891819812357426 = 0.001001636148430407 + 0.001 * 6.890183448791504
Epoch 1510, val loss: 1.8691167831420898
Epoch 1520, training loss: 0.007878562435507774 = 0.0009875103132799268 + 0.001 * 6.891051769256592
Epoch 1520, val loss: 1.8717362880706787
Epoch 1530, training loss: 0.007867908105254173 = 0.0009738499647937715 + 0.001 * 6.894057750701904
Epoch 1530, val loss: 1.8742876052856445
Epoch 1540, training loss: 0.007859150879085064 = 0.0009606450912542641 + 0.001 * 6.898505210876465
Epoch 1540, val loss: 1.8767768144607544
Epoch 1550, training loss: 0.007860720157623291 = 0.0009478674619458616 + 0.001 * 6.912852764129639
Epoch 1550, val loss: 1.8792133331298828
Epoch 1560, training loss: 0.007850423455238342 = 0.000935479358304292 + 0.001 * 6.914943695068359
Epoch 1560, val loss: 1.8816038370132446
Epoch 1570, training loss: 0.007829807698726654 = 0.0009234941098839045 + 0.001 * 6.906313419342041
Epoch 1570, val loss: 1.8839565515518188
Epoch 1580, training loss: 0.007805094588547945 = 0.000911922543309629 + 0.001 * 6.893171787261963
Epoch 1580, val loss: 1.8862590789794922
Epoch 1590, training loss: 0.007781273685395718 = 0.0009007243206724524 + 0.001 * 6.88054895401001
Epoch 1590, val loss: 1.8884809017181396
Epoch 1600, training loss: 0.007764084730297327 = 0.0008898804080672562 + 0.001 * 6.874203681945801
Epoch 1600, val loss: 1.890665888786316
Epoch 1610, training loss: 0.007752209901809692 = 0.0008793650195002556 + 0.001 * 6.872844696044922
Epoch 1610, val loss: 1.8927873373031616
Epoch 1620, training loss: 0.007740981411188841 = 0.0008691821130923927 + 0.001 * 6.871798992156982
Epoch 1620, val loss: 1.894869089126587
Epoch 1630, training loss: 0.007733555510640144 = 0.0008592910598963499 + 0.001 * 6.874264240264893
Epoch 1630, val loss: 1.8968946933746338
Epoch 1640, training loss: 0.007725373841822147 = 0.0008497104863636196 + 0.001 * 6.8756632804870605
Epoch 1640, val loss: 1.8988622426986694
Epoch 1650, training loss: 0.007728558033704758 = 0.0008404398686252534 + 0.001 * 6.888117790222168
Epoch 1650, val loss: 1.9007513523101807
Epoch 1660, training loss: 0.007719633635133505 = 0.0008314645965583622 + 0.001 * 6.8881683349609375
Epoch 1660, val loss: 1.902582049369812
Epoch 1670, training loss: 0.007699884008616209 = 0.0008227886282838881 + 0.001 * 6.8770952224731445
Epoch 1670, val loss: 1.9043503999710083
Epoch 1680, training loss: 0.007747656665742397 = 0.0008143790764734149 + 0.001 * 6.933277130126953
Epoch 1680, val loss: 1.9060571193695068
Epoch 1690, training loss: 0.007704617455601692 = 0.0008062581764534116 + 0.001 * 6.898359298706055
Epoch 1690, val loss: 1.9077266454696655
Epoch 1700, training loss: 0.007669663056731224 = 0.0007983788382261992 + 0.001 * 6.871284008026123
Epoch 1700, val loss: 1.909342646598816
Epoch 1710, training loss: 0.007655208930373192 = 0.000790745485574007 + 0.001 * 6.8644633293151855
Epoch 1710, val loss: 1.9109128713607788
Epoch 1720, training loss: 0.007642791606485844 = 0.0007833323907107115 + 0.001 * 6.8594584465026855
Epoch 1720, val loss: 1.9124635457992554
Epoch 1730, training loss: 0.007677422370761633 = 0.0007761557353660464 + 0.001 * 6.901266098022461
Epoch 1730, val loss: 1.9139792919158936
Epoch 1740, training loss: 0.0076706851832568645 = 0.000769177102483809 + 0.001 * 6.90150785446167
Epoch 1740, val loss: 1.9154229164123535
Epoch 1750, training loss: 0.0076765622943639755 = 0.000762430252507329 + 0.001 * 6.9141316413879395
Epoch 1750, val loss: 1.916856288909912
Epoch 1760, training loss: 0.007617234718054533 = 0.0007558806100860238 + 0.001 * 6.861353874206543
Epoch 1760, val loss: 1.9182393550872803
Epoch 1770, training loss: 0.007605754304677248 = 0.0007495196186937392 + 0.001 * 6.856234550476074
Epoch 1770, val loss: 1.919572114944458
Epoch 1780, training loss: 0.007605388760566711 = 0.0007433620630763471 + 0.001 * 6.862026214599609
Epoch 1780, val loss: 1.9208686351776123
Epoch 1790, training loss: 0.007611023727804422 = 0.0007373829721473157 + 0.001 * 6.873640537261963
Epoch 1790, val loss: 1.9221161603927612
Epoch 1800, training loss: 0.007623761426657438 = 0.0007315639522857964 + 0.001 * 6.892197132110596
Epoch 1800, val loss: 1.9233192205429077
Epoch 1810, training loss: 0.007611779496073723 = 0.0007259331177920103 + 0.001 * 6.885846138000488
Epoch 1810, val loss: 1.9245038032531738
Epoch 1820, training loss: 0.007603703998029232 = 0.000720432261005044 + 0.001 * 6.88327169418335
Epoch 1820, val loss: 1.92562735080719
Epoch 1830, training loss: 0.007609175518155098 = 0.0007151011377573013 + 0.001 * 6.894073963165283
Epoch 1830, val loss: 1.9267643690109253
Epoch 1840, training loss: 0.0075822751969099045 = 0.0007099000504240394 + 0.001 * 6.872374534606934
Epoch 1840, val loss: 1.9278051853179932
Epoch 1850, training loss: 0.0076230186969041824 = 0.0007048599072732031 + 0.001 * 6.918158531188965
Epoch 1850, val loss: 1.9288533926010132
Epoch 1860, training loss: 0.007555627264082432 = 0.0006999749457463622 + 0.001 * 6.85565185546875
Epoch 1860, val loss: 1.9298436641693115
Epoch 1870, training loss: 0.0075493743643164635 = 0.0006952207768335938 + 0.001 * 6.854153156280518
Epoch 1870, val loss: 1.9307856559753418
Epoch 1880, training loss: 0.007555758114904165 = 0.0006906259222887456 + 0.001 * 6.865131855010986
Epoch 1880, val loss: 1.9317294359207153
Epoch 1890, training loss: 0.007559527643024921 = 0.0006861329893581569 + 0.001 * 6.87339448928833
Epoch 1890, val loss: 1.9326255321502686
Epoch 1900, training loss: 0.007536164950579405 = 0.0006817726534791291 + 0.001 * 6.854391574859619
Epoch 1900, val loss: 1.9335137605667114
Epoch 1910, training loss: 0.007510575465857983 = 0.0006775303045287728 + 0.001 * 6.83304500579834
Epoch 1910, val loss: 1.9343546628952026
Epoch 1920, training loss: 0.007507347036153078 = 0.0006734059425070882 + 0.001 * 6.8339409828186035
Epoch 1920, val loss: 1.9351682662963867
Epoch 1930, training loss: 0.007499707862734795 = 0.0006693857721984386 + 0.001 * 6.830321788787842
Epoch 1930, val loss: 1.9359488487243652
Epoch 1940, training loss: 0.007495087571442127 = 0.00066547398455441 + 0.001 * 6.82961368560791
Epoch 1940, val loss: 1.9367164373397827
Epoch 1950, training loss: 0.007492834702134132 = 0.0006616541650146246 + 0.001 * 6.831180572509766
Epoch 1950, val loss: 1.9374550580978394
Epoch 1960, training loss: 0.007485825568437576 = 0.0006579254404641688 + 0.001 * 6.827899932861328
Epoch 1960, val loss: 1.9381747245788574
Epoch 1970, training loss: 0.007561568170785904 = 0.0006542936316691339 + 0.001 * 6.90727424621582
Epoch 1970, val loss: 1.938879370689392
Epoch 1980, training loss: 0.007495235651731491 = 0.000650734466034919 + 0.001 * 6.844500541687012
Epoch 1980, val loss: 1.9395136833190918
Epoch 1990, training loss: 0.007480580359697342 = 0.0006473016110248864 + 0.001 * 6.833278179168701
Epoch 1990, val loss: 1.9401588439941406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8318397469688983
The final CL Acc:0.79753, 0.01222, The final GNN Acc:0.83483, 0.00538
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11648])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10624])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9526573419570923 = 1.9440604448318481 + 0.001 * 8.596855163574219
Epoch 0, val loss: 1.9373435974121094
Epoch 10, training loss: 1.9431623220443726 = 1.934565544128418 + 0.001 * 8.596769332885742
Epoch 10, val loss: 1.927546501159668
Epoch 20, training loss: 1.9313409328460693 = 1.9227443933486938 + 0.001 * 8.596482276916504
Epoch 20, val loss: 1.9151502847671509
Epoch 30, training loss: 1.9149984121322632 = 1.9064027070999146 + 0.001 * 8.595720291137695
Epoch 30, val loss: 1.8980674743652344
Epoch 40, training loss: 1.891114354133606 = 1.8825207948684692 + 0.001 * 8.593609809875488
Epoch 40, val loss: 1.8735624551773071
Epoch 50, training loss: 1.8577557802200317 = 1.8491688966751099 + 0.001 * 8.586824417114258
Epoch 50, val loss: 1.8411741256713867
Epoch 60, training loss: 1.8206654787063599 = 1.81210458278656 + 0.001 * 8.560873031616211
Epoch 60, val loss: 1.8099050521850586
Epoch 70, training loss: 1.7897266149520874 = 1.781300663948059 + 0.001 * 8.425930976867676
Epoch 70, val loss: 1.7883228063583374
Epoch 80, training loss: 1.7535748481750488 = 1.7454169988632202 + 0.001 * 8.15790843963623
Epoch 80, val loss: 1.7599868774414062
Epoch 90, training loss: 1.7033311128616333 = 1.6953186988830566 + 0.001 * 8.01240348815918
Epoch 90, val loss: 1.7175830602645874
Epoch 100, training loss: 1.6339380741119385 = 1.626125454902649 + 0.001 * 7.812653541564941
Epoch 100, val loss: 1.6589728593826294
Epoch 110, training loss: 1.5491260290145874 = 1.5415092706680298 + 0.001 * 7.6167683601379395
Epoch 110, val loss: 1.5902605056762695
Epoch 120, training loss: 1.4600703716278076 = 1.4526174068450928 + 0.001 * 7.4530110359191895
Epoch 120, val loss: 1.5226528644561768
Epoch 130, training loss: 1.3740553855895996 = 1.3666809797286987 + 0.001 * 7.374354362487793
Epoch 130, val loss: 1.4605721235275269
Epoch 140, training loss: 1.2912029027938843 = 1.283889889717102 + 0.001 * 7.312978744506836
Epoch 140, val loss: 1.4035110473632812
Epoch 150, training loss: 1.210835337638855 = 1.2035571336746216 + 0.001 * 7.278250217437744
Epoch 150, val loss: 1.3502211570739746
Epoch 160, training loss: 1.1343004703521729 = 1.1270365715026855 + 0.001 * 7.263945579528809
Epoch 160, val loss: 1.300060749053955
Epoch 170, training loss: 1.0635086297988892 = 1.056249976158142 + 0.001 * 7.258621692657471
Epoch 170, val loss: 1.2537997961044312
Epoch 180, training loss: 0.9992271065711975 = 0.9919719099998474 + 0.001 * 7.255221843719482
Epoch 180, val loss: 1.2115402221679688
Epoch 190, training loss: 0.9404481649398804 = 0.9331954717636108 + 0.001 * 7.252677917480469
Epoch 190, val loss: 1.1720144748687744
Epoch 200, training loss: 0.8848841786384583 = 0.8776336908340454 + 0.001 * 7.2505106925964355
Epoch 200, val loss: 1.1331589221954346
Epoch 210, training loss: 0.8302221298217773 = 0.8229740262031555 + 0.001 * 7.248120307922363
Epoch 210, val loss: 1.0931735038757324
Epoch 220, training loss: 0.7751402854919434 = 0.7678954005241394 + 0.001 * 7.244914531707764
Epoch 220, val loss: 1.051512598991394
Epoch 230, training loss: 0.7197263240814209 = 0.7124852538108826 + 0.001 * 7.241049766540527
Epoch 230, val loss: 1.0093586444854736
Epoch 240, training loss: 0.664889395236969 = 0.6576526761054993 + 0.001 * 7.236710071563721
Epoch 240, val loss: 0.9686042070388794
Epoch 250, training loss: 0.6111814975738525 = 0.6039493083953857 + 0.001 * 7.232215404510498
Epoch 250, val loss: 0.9309229254722595
Epoch 260, training loss: 0.5584545731544495 = 0.5512279272079468 + 0.001 * 7.226630210876465
Epoch 260, val loss: 0.8965470790863037
Epoch 270, training loss: 0.5059971809387207 = 0.49877873063087463 + 0.001 * 7.218447685241699
Epoch 270, val loss: 0.8645827174186707
Epoch 280, training loss: 0.45311519503593445 = 0.4459010064601898 + 0.001 * 7.214180946350098
Epoch 280, val loss: 0.8340412974357605
Epoch 290, training loss: 0.3997497260570526 = 0.39255276322364807 + 0.001 * 7.196958065032959
Epoch 290, val loss: 0.8050030469894409
Epoch 300, training loss: 0.3468625247478485 = 0.33968618512153625 + 0.001 * 7.176327705383301
Epoch 300, val loss: 0.7789477705955505
Epoch 310, training loss: 0.29650068283081055 = 0.28934532403945923 + 0.001 * 7.1553568840026855
Epoch 310, val loss: 0.7581767439842224
Epoch 320, training loss: 0.2510426640510559 = 0.24391323328018188 + 0.001 * 7.1294355392456055
Epoch 320, val loss: 0.7445576190948486
Epoch 330, training loss: 0.2122678905725479 = 0.20515064895153046 + 0.001 * 7.117248058319092
Epoch 330, val loss: 0.7383321523666382
Epoch 340, training loss: 0.1804102659225464 = 0.17331552505493164 + 0.001 * 7.094733715057373
Epoch 340, val loss: 0.7386578917503357
Epoch 350, training loss: 0.15458515286445618 = 0.14748863875865936 + 0.001 * 7.096515655517578
Epoch 350, val loss: 0.744390606880188
Epoch 360, training loss: 0.13351720571517944 = 0.12643858790397644 + 0.001 * 7.078618049621582
Epoch 360, val loss: 0.7540900111198425
Epoch 370, training loss: 0.11619957536458969 = 0.10912624001502991 + 0.001 * 7.0733323097229
Epoch 370, val loss: 0.7664070725440979
Epoch 380, training loss: 0.1018102765083313 = 0.09473878145217896 + 0.001 * 7.071496486663818
Epoch 380, val loss: 0.7807076573371887
Epoch 390, training loss: 0.08972631394863129 = 0.08266126364469528 + 0.001 * 7.0650529861450195
Epoch 390, val loss: 0.7962192893028259
Epoch 400, training loss: 0.0795024111866951 = 0.0724371150135994 + 0.001 * 7.065296173095703
Epoch 400, val loss: 0.8125112652778625
Epoch 410, training loss: 0.07078880071640015 = 0.06373018771409988 + 0.001 * 7.058615207672119
Epoch 410, val loss: 0.8292137384414673
Epoch 420, training loss: 0.06334433704614639 = 0.05628636106848717 + 0.001 * 7.057979106903076
Epoch 420, val loss: 0.8459776639938354
Epoch 430, training loss: 0.05696699395775795 = 0.04991219565272331 + 0.001 * 7.054797172546387
Epoch 430, val loss: 0.8627063035964966
Epoch 440, training loss: 0.051509540528059006 = 0.044451821595430374 + 0.001 * 7.0577192306518555
Epoch 440, val loss: 0.8791608214378357
Epoch 450, training loss: 0.0468200147151947 = 0.039769478142261505 + 0.001 * 7.050534248352051
Epoch 450, val loss: 0.8951860666275024
Epoch 460, training loss: 0.042796727269887924 = 0.035745780915021896 + 0.001 * 7.05094575881958
Epoch 460, val loss: 0.910735011100769
Epoch 470, training loss: 0.03933477774262428 = 0.03227666765451431 + 0.001 * 7.058111190795898
Epoch 470, val loss: 0.9258438944816589
Epoch 480, training loss: 0.03632533922791481 = 0.029273496940732002 + 0.001 * 7.05184268951416
Epoch 480, val loss: 0.9403687715530396
Epoch 490, training loss: 0.03371051698923111 = 0.026661526411771774 + 0.001 * 7.0489912033081055
Epoch 490, val loss: 0.9543787240982056
Epoch 500, training loss: 0.03142690658569336 = 0.02437848597764969 + 0.001 * 7.048421382904053
Epoch 500, val loss: 0.9679228663444519
Epoch 510, training loss: 0.029417037963867188 = 0.022372348234057426 + 0.001 * 7.044689655303955
Epoch 510, val loss: 0.9810532331466675
Epoch 520, training loss: 0.02764149010181427 = 0.020599152892827988 + 0.001 * 7.042336940765381
Epoch 520, val loss: 0.9938234686851501
Epoch 530, training loss: 0.02606513351202011 = 0.019023079425096512 + 0.001 * 7.042054653167725
Epoch 530, val loss: 1.0063042640686035
Epoch 540, training loss: 0.024672003462910652 = 0.01761592924594879 + 0.001 * 7.056074142456055
Epoch 540, val loss: 1.0185027122497559
Epoch 550, training loss: 0.02339388057589531 = 0.016355078667402267 + 0.001 * 7.038801193237305
Epoch 550, val loss: 1.0304336547851562
Epoch 560, training loss: 0.022263675928115845 = 0.01522163487970829 + 0.001 * 7.04203987121582
Epoch 560, val loss: 1.04208242893219
Epoch 570, training loss: 0.02123960480093956 = 0.014199958182871342 + 0.001 * 7.039645195007324
Epoch 570, val loss: 1.0534565448760986
Epoch 580, training loss: 0.020307540893554688 = 0.013276832178235054 + 0.001 * 7.0307087898254395
Epoch 580, val loss: 1.0645569562911987
Epoch 590, training loss: 0.019499193876981735 = 0.0124403340741992 + 0.001 * 7.058858394622803
Epoch 590, val loss: 1.0753867626190186
Epoch 600, training loss: 0.018705755472183228 = 0.011679131537675858 + 0.001 * 7.026623249053955
Epoch 600, val loss: 1.0859296321868896
Epoch 610, training loss: 0.018009008839726448 = 0.010981233790516853 + 0.001 * 7.027774810791016
Epoch 610, val loss: 1.0962598323822021
Epoch 620, training loss: 0.017392776906490326 = 0.010337010957300663 + 0.001 * 7.0557661056518555
Epoch 620, val loss: 1.106410026550293
Epoch 630, training loss: 0.016772648319602013 = 0.009741159155964851 + 0.001 * 7.031488418579102
Epoch 630, val loss: 1.1163971424102783
Epoch 640, training loss: 0.016225650906562805 = 0.009189805947244167 + 0.001 * 7.035844326019287
Epoch 640, val loss: 1.1261917352676392
Epoch 650, training loss: 0.015698952600359917 = 0.008680121973156929 + 0.001 * 7.0188307762146
Epoch 650, val loss: 1.1358131170272827
Epoch 660, training loss: 0.015245618298649788 = 0.008209116756916046 + 0.001 * 7.036501884460449
Epoch 660, val loss: 1.1452486515045166
Epoch 670, training loss: 0.014802468940615654 = 0.0077739194966852665 + 0.001 * 7.028549671173096
Epoch 670, val loss: 1.154432773590088
Epoch 680, training loss: 0.01438981108367443 = 0.007371717132627964 + 0.001 * 7.018093109130859
Epoch 680, val loss: 1.1634066104888916
Epoch 690, training loss: 0.014032608829438686 = 0.006999724544584751 + 0.001 * 7.032884120941162
Epoch 690, val loss: 1.1721665859222412
Epoch 700, training loss: 0.013676155358552933 = 0.006655301433056593 + 0.001 * 7.020853042602539
Epoch 700, val loss: 1.1806831359863281
Epoch 710, training loss: 0.013353239744901657 = 0.0063357967883348465 + 0.001 * 7.017442226409912
Epoch 710, val loss: 1.1890177726745605
Epoch 720, training loss: 0.013069059699773788 = 0.006037787068635225 + 0.001 * 7.031272888183594
Epoch 720, val loss: 1.1971461772918701
Epoch 730, training loss: 0.012781895697116852 = 0.005759247113019228 + 0.001 * 7.022648334503174
Epoch 730, val loss: 1.2051587104797363
Epoch 740, training loss: 0.012511671520769596 = 0.005498010199517012 + 0.001 * 7.013660907745361
Epoch 740, val loss: 1.212967038154602
Epoch 750, training loss: 0.012281213887035847 = 0.005252278875559568 + 0.001 * 7.028934478759766
Epoch 750, val loss: 1.2206013202667236
Epoch 760, training loss: 0.012030821293592453 = 0.005021137185394764 + 0.001 * 7.009684085845947
Epoch 760, val loss: 1.2281402349472046
Epoch 770, training loss: 0.011806735768914223 = 0.004803585819900036 + 0.001 * 7.003149032592773
Epoch 770, val loss: 1.235538363456726
Epoch 780, training loss: 0.011600399389863014 = 0.004598874598741531 + 0.001 * 7.001524448394775
Epoch 780, val loss: 1.242781639099121
Epoch 790, training loss: 0.01140781119465828 = 0.004406298510730267 + 0.001 * 7.0015130043029785
Epoch 790, val loss: 1.2498961687088013
Epoch 800, training loss: 0.011225891299545765 = 0.004224820993840694 + 0.001 * 7.001070022583008
Epoch 800, val loss: 1.2568860054016113
Epoch 810, training loss: 0.011053802445530891 = 0.004053132608532906 + 0.001 * 7.000669002532959
Epoch 810, val loss: 1.2638099193572998
Epoch 820, training loss: 0.010893991217017174 = 0.003889573970809579 + 0.001 * 7.0044169425964355
Epoch 820, val loss: 1.2707041501998901
Epoch 830, training loss: 0.010736409574747086 = 0.003732290817424655 + 0.001 * 7.004117965698242
Epoch 830, val loss: 1.2776055335998535
Epoch 840, training loss: 0.010574793443083763 = 0.0035799522884190083 + 0.001 * 6.994840145111084
Epoch 840, val loss: 1.2845655679702759
Epoch 850, training loss: 0.010433414950966835 = 0.003431534394621849 + 0.001 * 7.001880168914795
Epoch 850, val loss: 1.2916427850723267
Epoch 860, training loss: 0.01029843557626009 = 0.003286845050752163 + 0.001 * 7.011590003967285
Epoch 860, val loss: 1.2988450527191162
Epoch 870, training loss: 0.010133066214621067 = 0.003146205563098192 + 0.001 * 6.986860275268555
Epoch 870, val loss: 1.3060815334320068
Epoch 880, training loss: 0.010008851066231728 = 0.0030100850854068995 + 0.001 * 6.99876594543457
Epoch 880, val loss: 1.3134715557098389
Epoch 890, training loss: 0.0098726786673069 = 0.0028789984062314034 + 0.001 * 6.993679523468018
Epoch 890, val loss: 1.3208986520767212
Epoch 900, training loss: 0.009739099070429802 = 0.0027534516993910074 + 0.001 * 6.985647201538086
Epoch 900, val loss: 1.328359603881836
Epoch 910, training loss: 0.009634187445044518 = 0.002633725991472602 + 0.001 * 7.000461578369141
Epoch 910, val loss: 1.335801601409912
Epoch 920, training loss: 0.009507856331765652 = 0.0025201269891113043 + 0.001 * 6.987728595733643
Epoch 920, val loss: 1.3431564569473267
Epoch 930, training loss: 0.009405882097780704 = 0.002412587869912386 + 0.001 * 6.993293762207031
Epoch 930, val loss: 1.350461483001709
Epoch 940, training loss: 0.00929117389023304 = 0.0023111295886337757 + 0.001 * 6.980043888092041
Epoch 940, val loss: 1.357724905014038
Epoch 950, training loss: 0.009194557555019855 = 0.0022154764737933874 + 0.001 * 6.979081153869629
Epoch 950, val loss: 1.3648667335510254
Epoch 960, training loss: 0.009111573919653893 = 0.0021253922022879124 + 0.001 * 6.986180782318115
Epoch 960, val loss: 1.3718947172164917
Epoch 970, training loss: 0.009020108729600906 = 0.002040696796029806 + 0.001 * 6.9794111251831055
Epoch 970, val loss: 1.3787792921066284
Epoch 980, training loss: 0.008964767679572105 = 0.001961073139682412 + 0.001 * 7.0036940574646
Epoch 980, val loss: 1.3855997323989868
Epoch 990, training loss: 0.008867746219038963 = 0.001886283396743238 + 0.001 * 6.981462478637695
Epoch 990, val loss: 1.392208456993103
Epoch 1000, training loss: 0.008795089088380337 = 0.0018159648170694709 + 0.001 * 6.979124069213867
Epoch 1000, val loss: 1.3987916707992554
Epoch 1010, training loss: 0.008718390949070454 = 0.0017498962115496397 + 0.001 * 6.968494415283203
Epoch 1010, val loss: 1.4051932096481323
Epoch 1020, training loss: 0.00868185143917799 = 0.0016877588350325823 + 0.001 * 6.994091987609863
Epoch 1020, val loss: 1.4114691019058228
Epoch 1030, training loss: 0.008602351881563663 = 0.0016293376684188843 + 0.001 * 6.973013877868652
Epoch 1030, val loss: 1.4175474643707275
Epoch 1040, training loss: 0.008593631908297539 = 0.0015743550611659884 + 0.001 * 7.0192766189575195
Epoch 1040, val loss: 1.4235137701034546
Epoch 1050, training loss: 0.00849088467657566 = 0.0015226395335048437 + 0.001 * 6.968245029449463
Epoch 1050, val loss: 1.4293259382247925
Epoch 1060, training loss: 0.00844605639576912 = 0.0014738934114575386 + 0.001 * 6.97216272354126
Epoch 1060, val loss: 1.435028314590454
Epoch 1070, training loss: 0.008397616446018219 = 0.0014279517345130444 + 0.001 * 6.969664573669434
Epoch 1070, val loss: 1.4406036138534546
Epoch 1080, training loss: 0.008349252864718437 = 0.001384642207995057 + 0.001 * 6.9646100997924805
Epoch 1080, val loss: 1.4460115432739258
Epoch 1090, training loss: 0.008305076509714127 = 0.0013437372399494052 + 0.001 * 6.961338996887207
Epoch 1090, val loss: 1.4513113498687744
Epoch 1100, training loss: 0.008266013115644455 = 0.0013051350833848119 + 0.001 * 6.960877418518066
Epoch 1100, val loss: 1.4564695358276367
Epoch 1110, training loss: 0.008255851455032825 = 0.0012686687987297773 + 0.001 * 6.987182140350342
Epoch 1110, val loss: 1.461506962776184
Epoch 1120, training loss: 0.008208772167563438 = 0.001234183320775628 + 0.001 * 6.974588394165039
Epoch 1120, val loss: 1.46639883518219
Epoch 1130, training loss: 0.008159508928656578 = 0.001201538136228919 + 0.001 * 6.957970142364502
Epoch 1130, val loss: 1.4711962938308716
Epoch 1140, training loss: 0.008130553178489208 = 0.0011706469813361764 + 0.001 * 6.959906101226807
Epoch 1140, val loss: 1.4758890867233276
Epoch 1150, training loss: 0.008096177130937576 = 0.0011413772590458393 + 0.001 * 6.954799652099609
Epoch 1150, val loss: 1.480392336845398
Epoch 1160, training loss: 0.008086192421615124 = 0.0011136223329231143 + 0.001 * 6.972569465637207
Epoch 1160, val loss: 1.4848695993423462
Epoch 1170, training loss: 0.008030972443521023 = 0.001087349490262568 + 0.001 * 6.943622589111328
Epoch 1170, val loss: 1.48915433883667
Epoch 1180, training loss: 0.008021240122616291 = 0.001062390161678195 + 0.001 * 6.9588494300842285
Epoch 1180, val loss: 1.4933966398239136
Epoch 1190, training loss: 0.00798445288091898 = 0.0010386994108557701 + 0.001 * 6.94575309753418
Epoch 1190, val loss: 1.497473955154419
Epoch 1200, training loss: 0.008001916110515594 = 0.0010161532554775476 + 0.001 * 6.985762596130371
Epoch 1200, val loss: 1.5015246868133545
Epoch 1210, training loss: 0.00794466957449913 = 0.0009947591461241245 + 0.001 * 6.949909687042236
Epoch 1210, val loss: 1.5054333209991455
Epoch 1220, training loss: 0.007961057126522064 = 0.0009743656264618039 + 0.001 * 6.986691474914551
Epoch 1220, val loss: 1.5092999935150146
Epoch 1230, training loss: 0.00789594929665327 = 0.0009549804381094873 + 0.001 * 6.940968990325928
Epoch 1230, val loss: 1.513006329536438
Epoch 1240, training loss: 0.007884997874498367 = 0.0009365063160657883 + 0.001 * 6.94849157333374
Epoch 1240, val loss: 1.5166337490081787
Epoch 1250, training loss: 0.007867559790611267 = 0.0009188850526697934 + 0.001 * 6.94867467880249
Epoch 1250, val loss: 1.520212173461914
Epoch 1260, training loss: 0.007863103412091732 = 0.0009021107689477503 + 0.001 * 6.960992336273193
Epoch 1260, val loss: 1.523661494255066
Epoch 1270, training loss: 0.007831459864974022 = 0.0008860735688358545 + 0.001 * 6.945385932922363
Epoch 1270, val loss: 1.5270541906356812
Epoch 1280, training loss: 0.007812931202352047 = 0.0008707887609489262 + 0.001 * 6.942142009735107
Epoch 1280, val loss: 1.530329704284668
Epoch 1290, training loss: 0.007792615797370672 = 0.0008561997092328966 + 0.001 * 6.936415672302246
Epoch 1290, val loss: 1.5335360765457153
Epoch 1300, training loss: 0.0077742463909089565 = 0.0008422771352343261 + 0.001 * 6.931969165802002
Epoch 1300, val loss: 1.5366805791854858
Epoch 1310, training loss: 0.007775669917464256 = 0.0008289618417620659 + 0.001 * 6.946707725524902
Epoch 1310, val loss: 1.5396714210510254
Epoch 1320, training loss: 0.0077439043670892715 = 0.0008162203012034297 + 0.001 * 6.9276838302612305
Epoch 1320, val loss: 1.542690634727478
Epoch 1330, training loss: 0.0077364519238471985 = 0.0008040478569455445 + 0.001 * 6.932403564453125
Epoch 1330, val loss: 1.5455865859985352
Epoch 1340, training loss: 0.0077259959653019905 = 0.0007924071978777647 + 0.001 * 6.933588027954102
Epoch 1340, val loss: 1.548383116722107
Epoch 1350, training loss: 0.007708292454481125 = 0.0007812663679942489 + 0.001 * 6.92702579498291
Epoch 1350, val loss: 1.5511794090270996
Epoch 1360, training loss: 0.007724785711616278 = 0.0007706090691499412 + 0.001 * 6.954176425933838
Epoch 1360, val loss: 1.5538150072097778
Epoch 1370, training loss: 0.007696561049669981 = 0.000760377966798842 + 0.001 * 6.936182975769043
Epoch 1370, val loss: 1.5564526319503784
Epoch 1380, training loss: 0.00768687017261982 = 0.0007505870889872313 + 0.001 * 6.936283111572266
Epoch 1380, val loss: 1.5590035915374756
Epoch 1390, training loss: 0.007671946194022894 = 0.00074117595795542 + 0.001 * 6.930769920349121
Epoch 1390, val loss: 1.5615451335906982
Epoch 1400, training loss: 0.007665333338081837 = 0.0007321201846934855 + 0.001 * 6.933212757110596
Epoch 1400, val loss: 1.5639938116073608
Epoch 1410, training loss: 0.007641398347914219 = 0.0007234206423163414 + 0.001 * 6.917977333068848
Epoch 1410, val loss: 1.56645929813385
Epoch 1420, training loss: 0.0076627605594694614 = 0.0007150580058805645 + 0.001 * 6.947702407836914
Epoch 1420, val loss: 1.5688045024871826
Epoch 1430, training loss: 0.007631776854395866 = 0.0007070884457789361 + 0.001 * 6.92468786239624
Epoch 1430, val loss: 1.5710012912750244
Epoch 1440, training loss: 0.007636737078428268 = 0.00069943827111274 + 0.001 * 6.937298774719238
Epoch 1440, val loss: 1.5732924938201904
Epoch 1450, training loss: 0.007616613060235977 = 0.0006921099266037345 + 0.001 * 6.924502849578857
Epoch 1450, val loss: 1.5753819942474365
Epoch 1460, training loss: 0.007610688451677561 = 0.0006850684876553714 + 0.001 * 6.925619602203369
Epoch 1460, val loss: 1.5775119066238403
Epoch 1470, training loss: 0.007599023636430502 = 0.0006782850250601768 + 0.001 * 6.920738220214844
Epoch 1470, val loss: 1.57952082157135
Epoch 1480, training loss: 0.007573068141937256 = 0.000671784277074039 + 0.001 * 6.901283264160156
Epoch 1480, val loss: 1.5815268754959106
Epoch 1490, training loss: 0.0075714681297540665 = 0.0006655107135884464 + 0.001 * 6.905957221984863
Epoch 1490, val loss: 1.5834985971450806
Epoch 1500, training loss: 0.007560773752629757 = 0.0006594721344299614 + 0.001 * 6.90130090713501
Epoch 1500, val loss: 1.585385799407959
Epoch 1510, training loss: 0.007557040546089411 = 0.0006536515429615974 + 0.001 * 6.903388500213623
Epoch 1510, val loss: 1.5872572660446167
Epoch 1520, training loss: 0.0075604356825351715 = 0.0006480427691712976 + 0.001 * 6.912392616271973
Epoch 1520, val loss: 1.589102029800415
Epoch 1530, training loss: 0.007549187634140253 = 0.0006426502368412912 + 0.001 * 6.906537055969238
Epoch 1530, val loss: 1.5908452272415161
Epoch 1540, training loss: 0.007550262846052647 = 0.0006374576478265226 + 0.001 * 6.91280460357666
Epoch 1540, val loss: 1.5925811529159546
Epoch 1550, training loss: 0.007535598706454039 = 0.0006324718124233186 + 0.001 * 6.9031267166137695
Epoch 1550, val loss: 1.594245195388794
Epoch 1560, training loss: 0.007514876313507557 = 0.0006276874337345362 + 0.001 * 6.887188911437988
Epoch 1560, val loss: 1.5959041118621826
Epoch 1570, training loss: 0.0075193545781075954 = 0.0006230936269275844 + 0.001 * 6.896260738372803
Epoch 1570, val loss: 1.5974451303482056
Epoch 1580, training loss: 0.007503158412873745 = 0.0006186753162182868 + 0.001 * 6.8844828605651855
Epoch 1580, val loss: 1.5989986658096313
Epoch 1590, training loss: 0.007525665685534477 = 0.0006144252838566899 + 0.001 * 6.911240100860596
Epoch 1590, val loss: 1.6005271673202515
Epoch 1600, training loss: 0.007511403411626816 = 0.0006103226915001869 + 0.001 * 6.90108060836792
Epoch 1600, val loss: 1.6019456386566162
Epoch 1610, training loss: 0.007500617764890194 = 0.0006063713226467371 + 0.001 * 6.8942461013793945
Epoch 1610, val loss: 1.6034268140792847
Epoch 1620, training loss: 0.007484807167202234 = 0.0006025582551956177 + 0.001 * 6.882248401641846
Epoch 1620, val loss: 1.6047924757003784
Epoch 1630, training loss: 0.0074782161973416805 = 0.0005988716147840023 + 0.001 * 6.879344463348389
Epoch 1630, val loss: 1.606157660484314
Epoch 1640, training loss: 0.007496360689401627 = 0.0005953249637968838 + 0.001 * 6.901035308837891
Epoch 1640, val loss: 1.6075104475021362
Epoch 1650, training loss: 0.007520588580518961 = 0.0005919007817283273 + 0.001 * 6.928687572479248
Epoch 1650, val loss: 1.608750820159912
Epoch 1660, training loss: 0.007465612608939409 = 0.0005885743303224444 + 0.001 * 6.87703800201416
Epoch 1660, val loss: 1.6100152730941772
Epoch 1670, training loss: 0.007463366724550724 = 0.0005853690672665834 + 0.001 * 6.877997398376465
Epoch 1670, val loss: 1.61130690574646
Epoch 1680, training loss: 0.007493038661777973 = 0.0005822775419801474 + 0.001 * 6.910760402679443
Epoch 1680, val loss: 1.6124736070632935
Epoch 1690, training loss: 0.007487317081540823 = 0.0005792998126707971 + 0.001 * 6.908016681671143
Epoch 1690, val loss: 1.6136728525161743
Epoch 1700, training loss: 0.007478481624275446 = 0.0005764003144577146 + 0.001 * 6.90208101272583
Epoch 1700, val loss: 1.614740252494812
Epoch 1710, training loss: 0.007451923564076424 = 0.0005736260791309178 + 0.001 * 6.878296852111816
Epoch 1710, val loss: 1.615919589996338
Epoch 1720, training loss: 0.007462226320058107 = 0.0005709350225515664 + 0.001 * 6.891290664672852
Epoch 1720, val loss: 1.6170140504837036
Epoch 1730, training loss: 0.007463535293936729 = 0.0005683370400220156 + 0.001 * 6.895198345184326
Epoch 1730, val loss: 1.6180652379989624
Epoch 1740, training loss: 0.007473347708582878 = 0.0005658215959556401 + 0.001 * 6.907525539398193
Epoch 1740, val loss: 1.6189872026443481
Epoch 1750, training loss: 0.00743444450199604 = 0.0005633949185721576 + 0.001 * 6.871049404144287
Epoch 1750, val loss: 1.6200706958770752
Epoch 1760, training loss: 0.007414672058075666 = 0.00056103290989995 + 0.001 * 6.853638648986816
Epoch 1760, val loss: 1.6210325956344604
Epoch 1770, training loss: 0.007447472307831049 = 0.0005587430787272751 + 0.001 * 6.888728618621826
Epoch 1770, val loss: 1.6219984292984009
Epoch 1780, training loss: 0.0074424780905246735 = 0.0005565285100601614 + 0.001 * 6.88594913482666
Epoch 1780, val loss: 1.6228779554367065
Epoch 1790, training loss: 0.007452578283846378 = 0.0005543746519833803 + 0.001 * 6.898202896118164
Epoch 1790, val loss: 1.6238079071044922
Epoch 1800, training loss: 0.00744453864172101 = 0.000552286219317466 + 0.001 * 6.892251968383789
Epoch 1800, val loss: 1.6246702671051025
Epoch 1810, training loss: 0.0074520595371723175 = 0.0005502608837559819 + 0.001 * 6.901798248291016
Epoch 1810, val loss: 1.6255924701690674
Epoch 1820, training loss: 0.007419447880238295 = 0.0005482943379320204 + 0.001 * 6.871153354644775
Epoch 1820, val loss: 1.6263954639434814
Epoch 1830, training loss: 0.0074018631130456924 = 0.0005463884444907308 + 0.001 * 6.855474472045898
Epoch 1830, val loss: 1.6273080110549927
Epoch 1840, training loss: 0.007421828340739012 = 0.0005445358692668378 + 0.001 * 6.877292156219482
Epoch 1840, val loss: 1.6280890703201294
Epoch 1850, training loss: 0.0074111539870500565 = 0.0005427587311714888 + 0.001 * 6.86839485168457
Epoch 1850, val loss: 1.6289454698562622
Epoch 1860, training loss: 0.007400752045214176 = 0.0005410390440374613 + 0.001 * 6.859712600708008
Epoch 1860, val loss: 1.6296378374099731
Epoch 1870, training loss: 0.007397923152893782 = 0.0005393659812398255 + 0.001 * 6.858556747436523
Epoch 1870, val loss: 1.6304607391357422
Epoch 1880, training loss: 0.007392081432044506 = 0.0005377433262765408 + 0.001 * 6.854337692260742
Epoch 1880, val loss: 1.631172776222229
Epoch 1890, training loss: 0.0074252476915717125 = 0.0005361837102100253 + 0.001 * 6.889063358306885
Epoch 1890, val loss: 1.6319211721420288
Epoch 1900, training loss: 0.007387508638203144 = 0.0005346834077499807 + 0.001 * 6.852824687957764
Epoch 1900, val loss: 1.6325162649154663
Epoch 1910, training loss: 0.007400617469102144 = 0.0005332059226930141 + 0.001 * 6.867411136627197
Epoch 1910, val loss: 1.633320689201355
Epoch 1920, training loss: 0.007441761903464794 = 0.0005318031180649996 + 0.001 * 6.909958362579346
Epoch 1920, val loss: 1.63395357131958
Epoch 1930, training loss: 0.007385012693703175 = 0.0005304281366989017 + 0.001 * 6.854584217071533
Epoch 1930, val loss: 1.6345878839492798
Epoch 1940, training loss: 0.007389155216515064 = 0.000529083248693496 + 0.001 * 6.860071659088135
Epoch 1940, val loss: 1.635283350944519
Epoch 1950, training loss: 0.007380012422800064 = 0.0005277934833429754 + 0.001 * 6.8522186279296875
Epoch 1950, val loss: 1.6358779668807983
Epoch 1960, training loss: 0.007372746709734201 = 0.0005265194340609014 + 0.001 * 6.846226692199707
Epoch 1960, val loss: 1.6365256309509277
Epoch 1970, training loss: 0.007374920416623354 = 0.0005252753617241979 + 0.001 * 6.849644660949707
Epoch 1970, val loss: 1.6371288299560547
Epoch 1980, training loss: 0.007411052472889423 = 0.0005240710452198982 + 0.001 * 6.886981010437012
Epoch 1980, val loss: 1.6377009153366089
Epoch 1990, training loss: 0.007361064199358225 = 0.0005228801746852696 + 0.001 * 6.838183403015137
Epoch 1990, val loss: 1.638236403465271
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 1.9734563827514648 = 1.9648594856262207 + 0.001 * 8.596877098083496
Epoch 0, val loss: 1.9617334604263306
Epoch 10, training loss: 1.9623258113861084 = 1.9537290334701538 + 0.001 * 8.596835136413574
Epoch 10, val loss: 1.9499236345291138
Epoch 20, training loss: 1.9486184120178223 = 1.9400217533111572 + 0.001 * 8.596668243408203
Epoch 20, val loss: 1.9351576566696167
Epoch 30, training loss: 1.9298750162124634 = 1.9212788343429565 + 0.001 * 8.596240043640137
Epoch 30, val loss: 1.9149595499038696
Epoch 40, training loss: 1.9030426740646362 = 1.8944475650787354 + 0.001 * 8.595163345336914
Epoch 40, val loss: 1.8865331411361694
Epoch 50, training loss: 1.866052269935608 = 1.8574600219726562 + 0.001 * 8.592191696166992
Epoch 50, val loss: 1.849098801612854
Epoch 60, training loss: 1.8243359327316284 = 1.8157531023025513 + 0.001 * 8.582819938659668
Epoch 60, val loss: 1.8115875720977783
Epoch 70, training loss: 1.791859745979309 = 1.7833096981048584 + 0.001 * 8.550027847290039
Epoch 70, val loss: 1.7884384393692017
Epoch 80, training loss: 1.759412169456482 = 1.7510374784469604 + 0.001 * 8.374650001525879
Epoch 80, val loss: 1.7637544870376587
Epoch 90, training loss: 1.7146365642547607 = 1.706541895866394 + 0.001 * 8.0946683883667
Epoch 90, val loss: 1.7254321575164795
Epoch 100, training loss: 1.6535345315933228 = 1.6455680131912231 + 0.001 * 7.966466426849365
Epoch 100, val loss: 1.6725519895553589
Epoch 110, training loss: 1.5728628635406494 = 1.5650614500045776 + 0.001 * 7.801451683044434
Epoch 110, val loss: 1.604699969291687
Epoch 120, training loss: 1.4784215688705444 = 1.4707448482513428 + 0.001 * 7.6767258644104
Epoch 120, val loss: 1.5264719724655151
Epoch 130, training loss: 1.3801467418670654 = 1.3725601434707642 + 0.001 * 7.586574077606201
Epoch 130, val loss: 1.4468190670013428
Epoch 140, training loss: 1.2842597961425781 = 1.2767143249511719 + 0.001 * 7.545454025268555
Epoch 140, val loss: 1.369480013847351
Epoch 150, training loss: 1.1934378147125244 = 1.1859060525894165 + 0.001 * 7.53176212310791
Epoch 150, val loss: 1.2972739934921265
Epoch 160, training loss: 1.1082350015640259 = 1.1007170677185059 + 0.001 * 7.517883777618408
Epoch 160, val loss: 1.2291487455368042
Epoch 170, training loss: 1.027740240097046 = 1.0202453136444092 + 0.001 * 7.494915962219238
Epoch 170, val loss: 1.1645609140396118
Epoch 180, training loss: 0.9508335590362549 = 0.9433714151382446 + 0.001 * 7.462121486663818
Epoch 180, val loss: 1.1030117273330688
Epoch 190, training loss: 0.8766523599624634 = 0.8692335486412048 + 0.001 * 7.418783664703369
Epoch 190, val loss: 1.0444653034210205
Epoch 200, training loss: 0.8049979209899902 = 0.7976261377334595 + 0.001 * 7.37180757522583
Epoch 200, val loss: 0.9895492196083069
Epoch 210, training loss: 0.7364916205406189 = 0.7291490435600281 + 0.001 * 7.342559814453125
Epoch 210, val loss: 0.9392247796058655
Epoch 220, training loss: 0.6721029877662659 = 0.6647680997848511 + 0.001 * 7.334867000579834
Epoch 220, val loss: 0.8946321606636047
Epoch 230, training loss: 0.6123564839363098 = 0.6050310134887695 + 0.001 * 7.325464725494385
Epoch 230, val loss: 0.8565199375152588
Epoch 240, training loss: 0.5567778944969177 = 0.5494639873504639 + 0.001 * 7.313932418823242
Epoch 240, val loss: 0.8243069052696228
Epoch 250, training loss: 0.5044485330581665 = 0.49714720249176025 + 0.001 * 7.301337242126465
Epoch 250, val loss: 0.797767698764801
Epoch 260, training loss: 0.4545471668243408 = 0.447259783744812 + 0.001 * 7.2873711585998535
Epoch 260, val loss: 0.7763591408729553
Epoch 270, training loss: 0.40656453371047974 = 0.39928901195526123 + 0.001 * 7.275531768798828
Epoch 270, val loss: 0.7592533826828003
Epoch 280, training loss: 0.36051642894744873 = 0.3532577455043793 + 0.001 * 7.258681297302246
Epoch 280, val loss: 0.7456557750701904
Epoch 290, training loss: 0.3169253468513489 = 0.30968034267425537 + 0.001 * 7.245001792907715
Epoch 290, val loss: 0.735599935054779
Epoch 300, training loss: 0.2765446603298187 = 0.2693186104297638 + 0.001 * 7.226038932800293
Epoch 300, val loss: 0.7293366193771362
Epoch 310, training loss: 0.24008908867835999 = 0.2328670769929886 + 0.001 * 7.222009658813477
Epoch 310, val loss: 0.7271206974983215
Epoch 320, training loss: 0.20793280005455017 = 0.20073020458221436 + 0.001 * 7.202588081359863
Epoch 320, val loss: 0.728667676448822
Epoch 330, training loss: 0.18010841310024261 = 0.17291006445884705 + 0.001 * 7.198353290557861
Epoch 330, val loss: 0.7338054776191711
Epoch 340, training loss: 0.156326562166214 = 0.14912773668766022 + 0.001 * 7.1988205909729
Epoch 340, val loss: 0.7421151995658875
Epoch 350, training loss: 0.13610315322875977 = 0.12891145050525665 + 0.001 * 7.191701889038086
Epoch 350, val loss: 0.7530065774917603
Epoch 360, training loss: 0.11897124350070953 = 0.11178135126829147 + 0.001 * 7.189891338348389
Epoch 360, val loss: 0.7659832835197449
Epoch 370, training loss: 0.10446368157863617 = 0.09727555513381958 + 0.001 * 7.188124656677246
Epoch 370, val loss: 0.780558705329895
Epoch 380, training loss: 0.0921718180179596 = 0.08498085290193558 + 0.001 * 7.190961837768555
Epoch 380, val loss: 0.7962201237678528
Epoch 390, training loss: 0.08173574507236481 = 0.07454551756381989 + 0.001 * 7.190226078033447
Epoch 390, val loss: 0.8126523494720459
Epoch 400, training loss: 0.07286739349365234 = 0.065677210688591 + 0.001 * 7.190185546875
Epoch 400, val loss: 0.82953280210495
Epoch 410, training loss: 0.06531710922718048 = 0.05812538042664528 + 0.001 * 7.19172477722168
Epoch 410, val loss: 0.8465823531150818
Epoch 420, training loss: 0.05887047201395035 = 0.05167848616838455 + 0.001 * 7.19198751449585
Epoch 420, val loss: 0.8635268807411194
Epoch 430, training loss: 0.05334903299808502 = 0.04615103080868721 + 0.001 * 7.198001384735107
Epoch 430, val loss: 0.8802657127380371
Epoch 440, training loss: 0.04858320206403732 = 0.041388966143131256 + 0.001 * 7.1942338943481445
Epoch 440, val loss: 0.8967382311820984
Epoch 450, training loss: 0.04446079954504967 = 0.037265777587890625 + 0.001 * 7.1950225830078125
Epoch 450, val loss: 0.9128724932670593
Epoch 460, training loss: 0.04087772220373154 = 0.03368178382515907 + 0.001 * 7.195937156677246
Epoch 460, val loss: 0.9286735653877258
Epoch 470, training loss: 0.03775196522474289 = 0.030556080862879753 + 0.001 * 7.195885181427002
Epoch 470, val loss: 0.9440785050392151
Epoch 480, training loss: 0.03501783311367035 = 0.02782069332897663 + 0.001 * 7.197140216827393
Epoch 480, val loss: 0.9590839743614197
Epoch 490, training loss: 0.03261559084057808 = 0.02541910670697689 + 0.001 * 7.1964850425720215
Epoch 490, val loss: 0.9736910462379456
Epoch 500, training loss: 0.0305025652050972 = 0.02330358698964119 + 0.001 * 7.198977947235107
Epoch 500, val loss: 0.9878212213516235
Epoch 510, training loss: 0.02862953208386898 = 0.02143346704542637 + 0.001 * 7.196064472198486
Epoch 510, val loss: 1.0015268325805664
Epoch 520, training loss: 0.026973824948072433 = 0.01977466605603695 + 0.001 * 7.199159622192383
Epoch 520, val loss: 1.0148080587387085
Epoch 530, training loss: 0.02549397386610508 = 0.01829867623746395 + 0.001 * 7.1952972412109375
Epoch 530, val loss: 1.0276508331298828
Epoch 540, training loss: 0.02417958527803421 = 0.016980769112706184 + 0.001 * 7.198816299438477
Epoch 540, val loss: 1.040063738822937
Epoch 550, training loss: 0.023001233115792274 = 0.015800099819898605 + 0.001 * 7.201132774353027
Epoch 550, val loss: 1.0520744323730469
Epoch 560, training loss: 0.02193557098507881 = 0.014739231206476688 + 0.001 * 7.1963396072387695
Epoch 560, val loss: 1.063693642616272
Epoch 570, training loss: 0.02097749151289463 = 0.013783030211925507 + 0.001 * 7.194460391998291
Epoch 570, val loss: 1.0749350786209106
Epoch 580, training loss: 0.02011420950293541 = 0.012918486259877682 + 0.001 * 7.195722579956055
Epoch 580, val loss: 1.0858287811279297
Epoch 590, training loss: 0.019328368827700615 = 0.012134646996855736 + 0.001 * 7.193721294403076
Epoch 590, val loss: 1.0963943004608154
Epoch 600, training loss: 0.018625512719154358 = 0.011422155424952507 + 0.001 * 7.203357696533203
Epoch 600, val loss: 1.1066228151321411
Epoch 610, training loss: 0.017965883016586304 = 0.010772613808512688 + 0.001 * 7.193268299102783
Epoch 610, val loss: 1.1165237426757812
Epoch 620, training loss: 0.01736782304942608 = 0.010178981348872185 + 0.001 * 7.188841342926025
Epoch 620, val loss: 1.1261180639266968
Epoch 630, training loss: 0.016828028485178947 = 0.009635056369006634 + 0.001 * 7.192971229553223
Epoch 630, val loss: 1.1354522705078125
Epoch 640, training loss: 0.016323644667863846 = 0.009135564789175987 + 0.001 * 7.188079833984375
Epoch 640, val loss: 1.1445186138153076
Epoch 650, training loss: 0.01586311124265194 = 0.008675836026668549 + 0.001 * 7.187274932861328
Epoch 650, val loss: 1.1533092260360718
Epoch 660, training loss: 0.015450229868292809 = 0.008251874707639217 + 0.001 * 7.198355197906494
Epoch 660, val loss: 1.161894679069519
Epoch 670, training loss: 0.01505313254892826 = 0.007860049605369568 + 0.001 * 7.193082332611084
Epoch 670, val loss: 1.1702128648757935
Epoch 680, training loss: 0.014683234505355358 = 0.007497233804315329 + 0.001 * 7.186000347137451
Epoch 680, val loss: 1.1783283948898315
Epoch 690, training loss: 0.014344236813485622 = 0.007160621229559183 + 0.001 * 7.183615207672119
Epoch 690, val loss: 1.1862291097640991
Epoch 700, training loss: 0.014027921482920647 = 0.006847742013633251 + 0.001 * 7.180179119110107
Epoch 700, val loss: 1.193926453590393
Epoch 710, training loss: 0.013740492053329945 = 0.00655645877122879 + 0.001 * 7.184032917022705
Epoch 710, val loss: 1.2014360427856445
Epoch 720, training loss: 0.013476630672812462 = 0.006284827366471291 + 0.001 * 7.191802978515625
Epoch 720, val loss: 1.208743929862976
Epoch 730, training loss: 0.013206366449594498 = 0.0060311309061944485 + 0.001 * 7.175235271453857
Epoch 730, val loss: 1.2158970832824707
Epoch 740, training loss: 0.01297597587108612 = 0.005793818272650242 + 0.001 * 7.182157516479492
Epoch 740, val loss: 1.2228703498840332
Epoch 750, training loss: 0.012753807939589024 = 0.00557149201631546 + 0.001 * 7.182315349578857
Epoch 750, val loss: 1.2296510934829712
Epoch 760, training loss: 0.012539179995656013 = 0.005362941883504391 + 0.001 * 7.176237106323242
Epoch 760, val loss: 1.2363243103027344
Epoch 770, training loss: 0.012345725670456886 = 0.005167040042579174 + 0.001 * 7.178684711456299
Epoch 770, val loss: 1.2428126335144043
Epoch 780, training loss: 0.01215658150613308 = 0.004982794169336557 + 0.001 * 7.173786640167236
Epoch 780, val loss: 1.249172329902649
Epoch 790, training loss: 0.011976722627878189 = 0.004809298552572727 + 0.001 * 7.167423248291016
Epoch 790, val loss: 1.255380392074585
Epoch 800, training loss: 0.01181034930050373 = 0.004645729903131723 + 0.001 * 7.164618968963623
Epoch 800, val loss: 1.2614597082138062
Epoch 810, training loss: 0.01165601797401905 = 0.004491344094276428 + 0.001 * 7.164673805236816
Epoch 810, val loss: 1.2674132585525513
Epoch 820, training loss: 0.011506970971822739 = 0.004345470108091831 + 0.001 * 7.161500453948975
Epoch 820, val loss: 1.2732393741607666
Epoch 830, training loss: 0.011362147517502308 = 0.004207496996968985 + 0.001 * 7.1546502113342285
Epoch 830, val loss: 1.2789387702941895
Epoch 840, training loss: 0.011261342093348503 = 0.004076838493347168 + 0.001 * 7.184502601623535
Epoch 840, val loss: 1.2845351696014404
Epoch 850, training loss: 0.011112062260508537 = 0.003953052684664726 + 0.001 * 7.1590094566345215
Epoch 850, val loss: 1.2900090217590332
Epoch 860, training loss: 0.011005477979779243 = 0.003835601033642888 + 0.001 * 7.169877052307129
Epoch 860, val loss: 1.2953530550003052
Epoch 870, training loss: 0.01087542250752449 = 0.0037240898236632347 + 0.001 * 7.151332378387451
Epoch 870, val loss: 1.3005887269973755
Epoch 880, training loss: 0.010778148658573627 = 0.003618143731728196 + 0.001 * 7.160004615783691
Epoch 880, val loss: 1.305734634399414
Epoch 890, training loss: 0.010671683587133884 = 0.0035173404030501842 + 0.001 * 7.1543426513671875
Epoch 890, val loss: 1.3107763528823853
Epoch 900, training loss: 0.010612343437969685 = 0.0034213780891150236 + 0.001 * 7.190964698791504
Epoch 900, val loss: 1.315732479095459
Epoch 910, training loss: 0.010484070517122746 = 0.0033299627248197794 + 0.001 * 7.154107093811035
Epoch 910, val loss: 1.3205715417861938
Epoch 920, training loss: 0.010418030433356762 = 0.0032428130507469177 + 0.001 * 7.175217151641846
Epoch 920, val loss: 1.3253519535064697
Epoch 930, training loss: 0.010309905745089054 = 0.0031596480403095484 + 0.001 * 7.150257587432861
Epoch 930, val loss: 1.3300074338912964
Epoch 940, training loss: 0.010246618650853634 = 0.00308025605045259 + 0.001 * 7.166362285614014
Epoch 940, val loss: 1.3345921039581299
Epoch 950, training loss: 0.010133840143680573 = 0.0030043814331293106 + 0.001 * 7.129458427429199
Epoch 950, val loss: 1.3390812873840332
Epoch 960, training loss: 0.010080648586153984 = 0.0029318416491150856 + 0.001 * 7.148806095123291
Epoch 960, val loss: 1.3435217142105103
Epoch 970, training loss: 0.010009477846324444 = 0.002862445078790188 + 0.001 * 7.147032260894775
Epoch 970, val loss: 1.3478505611419678
Epoch 980, training loss: 0.009918536990880966 = 0.002795998938381672 + 0.001 * 7.122537612915039
Epoch 980, val loss: 1.3521015644073486
Epoch 990, training loss: 0.00986923836171627 = 0.002732354449108243 + 0.001 * 7.13688325881958
Epoch 990, val loss: 1.3563051223754883
Epoch 1000, training loss: 0.009821603074669838 = 0.0026713479310274124 + 0.001 * 7.150254726409912
Epoch 1000, val loss: 1.3604117631912231
Epoch 1010, training loss: 0.00974836852401495 = 0.002612840849906206 + 0.001 * 7.13552713394165
Epoch 1010, val loss: 1.364452600479126
Epoch 1020, training loss: 0.009661023505032063 = 0.0025566909462213516 + 0.001 * 7.104332447052002
Epoch 1020, val loss: 1.368407964706421
Epoch 1030, training loss: 0.009614775888621807 = 0.002502779709175229 + 0.001 * 7.111995697021484
Epoch 1030, val loss: 1.3723373413085938
Epoch 1040, training loss: 0.009614265523850918 = 0.002450989792123437 + 0.001 * 7.163275241851807
Epoch 1040, val loss: 1.3761558532714844
Epoch 1050, training loss: 0.009511865675449371 = 0.0024011977948248386 + 0.001 * 7.1106672286987305
Epoch 1050, val loss: 1.3798993825912476
Epoch 1060, training loss: 0.009455263614654541 = 0.0023533208295702934 + 0.001 * 7.101943016052246
Epoch 1060, val loss: 1.383671760559082
Epoch 1070, training loss: 0.009397817775607109 = 0.0023072604089975357 + 0.001 * 7.09055757522583
Epoch 1070, val loss: 1.387276291847229
Epoch 1080, training loss: 0.009349919855594635 = 0.002262930851429701 + 0.001 * 7.086988925933838
Epoch 1080, val loss: 1.390877366065979
Epoch 1090, training loss: 0.0093153091147542 = 0.002220224356278777 + 0.001 * 7.095084190368652
Epoch 1090, val loss: 1.3944311141967773
Epoch 1100, training loss: 0.00927980151027441 = 0.002179073868319392 + 0.001 * 7.100727081298828
Epoch 1100, val loss: 1.3978450298309326
Epoch 1110, training loss: 0.009239992126822472 = 0.0021394044160842896 + 0.001 * 7.100587368011475
Epoch 1110, val loss: 1.4012854099273682
Epoch 1120, training loss: 0.009185224771499634 = 0.0021011442877352238 + 0.001 * 7.084080696105957
Epoch 1120, val loss: 1.4046815633773804
Epoch 1130, training loss: 0.009146745316684246 = 0.0020642015151679516 + 0.001 * 7.08254337310791
Epoch 1130, val loss: 1.4079208374023438
Epoch 1140, training loss: 0.009098385460674763 = 0.0020285025238990784 + 0.001 * 7.069882392883301
Epoch 1140, val loss: 1.4112013578414917
Epoch 1150, training loss: 0.009115910157561302 = 0.001993925077840686 + 0.001 * 7.121984481811523
Epoch 1150, val loss: 1.4144381284713745
Epoch 1160, training loss: 0.009053261950612068 = 0.0019603604450821877 + 0.001 * 7.09290075302124
Epoch 1160, val loss: 1.4175783395767212
Epoch 1170, training loss: 0.009030047804117203 = 0.0019275989616289735 + 0.001 * 7.102447986602783
Epoch 1170, val loss: 1.4207829236984253
Epoch 1180, training loss: 0.008975616656243801 = 0.0018954244442284107 + 0.001 * 7.0801920890808105
Epoch 1180, val loss: 1.4239805936813354
Epoch 1190, training loss: 0.008959277532994747 = 0.001863656798377633 + 0.001 * 7.095620632171631
Epoch 1190, val loss: 1.4271832704544067
Epoch 1200, training loss: 0.00890086218714714 = 0.0018320985836908221 + 0.001 * 7.068763732910156
Epoch 1200, val loss: 1.4303573369979858
Epoch 1210, training loss: 0.008885100483894348 = 0.0018006720347329974 + 0.001 * 7.084428310394287
Epoch 1210, val loss: 1.4336647987365723
Epoch 1220, training loss: 0.00884136650711298 = 0.001769334077835083 + 0.001 * 7.0720319747924805
Epoch 1220, val loss: 1.4370002746582031
Epoch 1230, training loss: 0.008801104500889778 = 0.0017381389625370502 + 0.001 * 7.0629658699035645
Epoch 1230, val loss: 1.4404772520065308
Epoch 1240, training loss: 0.008777636103332043 = 0.0017071545589715242 + 0.001 * 7.070481300354004
Epoch 1240, val loss: 1.4438941478729248
Epoch 1250, training loss: 0.008727507665753365 = 0.0016764906467869878 + 0.001 * 7.051016807556152
Epoch 1250, val loss: 1.4473774433135986
Epoch 1260, training loss: 0.00867983978241682 = 0.0016462436178699136 + 0.001 * 7.033595561981201
Epoch 1260, val loss: 1.4509750604629517
Epoch 1270, training loss: 0.008665383793413639 = 0.001616487861610949 + 0.001 * 7.048895835876465
Epoch 1270, val loss: 1.4545584917068481
Epoch 1280, training loss: 0.008690198883414268 = 0.0015873482916504145 + 0.001 * 7.102850437164307
Epoch 1280, val loss: 1.4580729007720947
Epoch 1290, training loss: 0.008623041212558746 = 0.0015588864916935563 + 0.001 * 7.064154624938965
Epoch 1290, val loss: 1.4616470336914062
Epoch 1300, training loss: 0.00856064073741436 = 0.0015310770832002163 + 0.001 * 7.0295634269714355
Epoch 1300, val loss: 1.4652941226959229
Epoch 1310, training loss: 0.00853186659514904 = 0.0015039800200611353 + 0.001 * 7.027885913848877
Epoch 1310, val loss: 1.4688243865966797
Epoch 1320, training loss: 0.008513904176652431 = 0.0014776465250179172 + 0.001 * 7.036257266998291
Epoch 1320, val loss: 1.4723293781280518
Epoch 1330, training loss: 0.008472763933241367 = 0.00145209941547364 + 0.001 * 7.020664215087891
Epoch 1330, val loss: 1.4758529663085938
Epoch 1340, training loss: 0.008431017398834229 = 0.0014272992266342044 + 0.001 * 7.00371789932251
Epoch 1340, val loss: 1.4793144464492798
Epoch 1350, training loss: 0.008427252992987633 = 0.0014032438630238175 + 0.001 * 7.024008750915527
Epoch 1350, val loss: 1.4827133417129517
Epoch 1360, training loss: 0.0084066241979599 = 0.0013800146989524364 + 0.001 * 7.026609420776367
Epoch 1360, val loss: 1.4860926866531372
Epoch 1370, training loss: 0.008403252810239792 = 0.0013575409539043903 + 0.001 * 7.045711994171143
Epoch 1370, val loss: 1.4894537925720215
Epoch 1380, training loss: 0.00834138598293066 = 0.0013357623247429729 + 0.001 * 7.0056233406066895
Epoch 1380, val loss: 1.4926753044128418
Epoch 1390, training loss: 0.008335283026099205 = 0.001314680092036724 + 0.001 * 7.020602703094482
Epoch 1390, val loss: 1.4959323406219482
Epoch 1400, training loss: 0.008314048871397972 = 0.0012943214969709516 + 0.001 * 7.0197272300720215
Epoch 1400, val loss: 1.4991815090179443
Epoch 1410, training loss: 0.008310035802423954 = 0.00127466453704983 + 0.001 * 7.035370826721191
Epoch 1410, val loss: 1.5021950006484985
Epoch 1420, training loss: 0.008286469615995884 = 0.0012556653236970305 + 0.001 * 7.03080415725708
Epoch 1420, val loss: 1.5053002834320068
Epoch 1430, training loss: 0.008246870711445808 = 0.0012372740311548114 + 0.001 * 7.009596824645996
Epoch 1430, val loss: 1.508313536643982
Epoch 1440, training loss: 0.008221364580094814 = 0.0012194921728223562 + 0.001 * 7.0018720626831055
Epoch 1440, val loss: 1.511319875717163
Epoch 1450, training loss: 0.008221113122999668 = 0.001202272018417716 + 0.001 * 7.018840312957764
Epoch 1450, val loss: 1.5142804384231567
Epoch 1460, training loss: 0.008169868029654026 = 0.0011856331257149577 + 0.001 * 6.984234809875488
Epoch 1460, val loss: 1.5171129703521729
Epoch 1470, training loss: 0.008160868659615517 = 0.0011695385910570621 + 0.001 * 6.991329193115234
Epoch 1470, val loss: 1.5199823379516602
Epoch 1480, training loss: 0.008218674920499325 = 0.0011539647821336985 + 0.001 * 7.064709663391113
Epoch 1480, val loss: 1.5227386951446533
Epoch 1490, training loss: 0.008109764195978642 = 0.001138952560722828 + 0.001 * 6.970811367034912
Epoch 1490, val loss: 1.5254045724868774
Epoch 1500, training loss: 0.008107149973511696 = 0.0011243656044825912 + 0.001 * 6.982783794403076
Epoch 1500, val loss: 1.5281401872634888
Epoch 1510, training loss: 0.008090600371360779 = 0.0011102361604571342 + 0.001 * 6.980363368988037
Epoch 1510, val loss: 1.5308117866516113
Epoch 1520, training loss: 0.008122636936604977 = 0.0010965561959892511 + 0.001 * 7.02608060836792
Epoch 1520, val loss: 1.533364176750183
Epoch 1530, training loss: 0.008073353208601475 = 0.0010832947446033359 + 0.001 * 6.990058422088623
Epoch 1530, val loss: 1.5359445810317993
Epoch 1540, training loss: 0.008061159402132034 = 0.001070474972948432 + 0.001 * 6.990684509277344
Epoch 1540, val loss: 1.5384421348571777
Epoch 1550, training loss: 0.008020315319299698 = 0.0010580553207546473 + 0.001 * 6.962259292602539
Epoch 1550, val loss: 1.5409066677093506
Epoch 1560, training loss: 0.00800850335508585 = 0.0010460148332640529 + 0.001 * 6.962488174438477
Epoch 1560, val loss: 1.5434575080871582
Epoch 1570, training loss: 0.008083394728600979 = 0.0010343012399971485 + 0.001 * 7.049093246459961
Epoch 1570, val loss: 1.5457487106323242
Epoch 1580, training loss: 0.008001954294741154 = 0.0010230301413685083 + 0.001 * 6.978923797607422
Epoch 1580, val loss: 1.5480928421020508
Epoch 1590, training loss: 0.007960409857332706 = 0.0010119985090568662 + 0.001 * 6.948410987854004
Epoch 1590, val loss: 1.550481915473938
Epoch 1600, training loss: 0.007978197187185287 = 0.0010013470891863108 + 0.001 * 6.976849555969238
Epoch 1600, val loss: 1.5526641607284546
Epoch 1610, training loss: 0.008077067323029041 = 0.0009910321095958352 + 0.001 * 7.086034774780273
Epoch 1610, val loss: 1.5548659563064575
Epoch 1620, training loss: 0.007938619703054428 = 0.0009809996699914336 + 0.001 * 6.957619667053223
Epoch 1620, val loss: 1.556976318359375
Epoch 1630, training loss: 0.007916603237390518 = 0.0009712878963910043 + 0.001 * 6.945315361022949
Epoch 1630, val loss: 1.5592403411865234
Epoch 1640, training loss: 0.007939253933727741 = 0.0009618384065106511 + 0.001 * 6.977415084838867
Epoch 1640, val loss: 1.5613632202148438
Epoch 1650, training loss: 0.007932967506349087 = 0.0009526786743663251 + 0.001 * 6.980288505554199
Epoch 1650, val loss: 1.5632668733596802
Epoch 1660, training loss: 0.007880248129367828 = 0.0009437540429644287 + 0.001 * 6.936493873596191
Epoch 1660, val loss: 1.5654027462005615
Epoch 1670, training loss: 0.00788275059312582 = 0.0009350533364340663 + 0.001 * 6.947696685791016
Epoch 1670, val loss: 1.5673904418945312
Epoch 1680, training loss: 0.007871274836361408 = 0.0009266195120289922 + 0.001 * 6.94465446472168
Epoch 1680, val loss: 1.5694326162338257
Epoch 1690, training loss: 0.007872255519032478 = 0.0009184022201225162 + 0.001 * 6.953853130340576
Epoch 1690, val loss: 1.5713030099868774
Epoch 1700, training loss: 0.00785268284380436 = 0.0009104398777708411 + 0.001 * 6.942242622375488
Epoch 1700, val loss: 1.5733033418655396
Epoch 1710, training loss: 0.007911610417068005 = 0.000902647094335407 + 0.001 * 7.008963584899902
Epoch 1710, val loss: 1.575162410736084
Epoch 1720, training loss: 0.007810657378286123 = 0.0008951135678216815 + 0.001 * 6.915543556213379
Epoch 1720, val loss: 1.5769684314727783
Epoch 1730, training loss: 0.007809392176568508 = 0.0008877695072442293 + 0.001 * 6.921622276306152
Epoch 1730, val loss: 1.578887939453125
Epoch 1740, training loss: 0.007840404286980629 = 0.0008806100813671947 + 0.001 * 6.959793567657471
Epoch 1740, val loss: 1.5806845426559448
Epoch 1750, training loss: 0.007846442982554436 = 0.0008736622403375804 + 0.001 * 6.972780704498291
Epoch 1750, val loss: 1.5823630094528198
Epoch 1760, training loss: 0.007794844452291727 = 0.0008668955415487289 + 0.001 * 6.927948474884033
Epoch 1760, val loss: 1.5841490030288696
Epoch 1770, training loss: 0.007773176301270723 = 0.0008602848974987864 + 0.001 * 6.912890911102295
Epoch 1770, val loss: 1.5859311819076538
Epoch 1780, training loss: 0.00786577071994543 = 0.0008538276888430119 + 0.001 * 7.0119428634643555
Epoch 1780, val loss: 1.5875262022018433
Epoch 1790, training loss: 0.007798881269991398 = 0.0008475536596961319 + 0.001 * 6.951327323913574
Epoch 1790, val loss: 1.589219331741333
Epoch 1800, training loss: 0.007762637920677662 = 0.0008414348703809083 + 0.001 * 6.921202659606934
Epoch 1800, val loss: 1.5909161567687988
Epoch 1810, training loss: 0.0077715241350233555 = 0.0008354265009984374 + 0.001 * 6.936097145080566
Epoch 1810, val loss: 1.592553973197937
Epoch 1820, training loss: 0.007777138147503138 = 0.000829625700134784 + 0.001 * 6.947512149810791
Epoch 1820, val loss: 1.5941654443740845
Epoch 1830, training loss: 0.007772168610244989 = 0.0008239393937401474 + 0.001 * 6.94822883605957
Epoch 1830, val loss: 1.5957109928131104
Epoch 1840, training loss: 0.007730979472398758 = 0.000818415719550103 + 0.001 * 6.912563800811768
Epoch 1840, val loss: 1.5972779989242554
Epoch 1850, training loss: 0.007736690808087587 = 0.0008130043861456215 + 0.001 * 6.9236860275268555
Epoch 1850, val loss: 1.5989112854003906
Epoch 1860, training loss: 0.007765220478177071 = 0.0008077441598288715 + 0.001 * 6.9574761390686035
Epoch 1860, val loss: 1.6003559827804565
Epoch 1870, training loss: 0.007738854736089706 = 0.000802568974904716 + 0.001 * 6.936285495758057
Epoch 1870, val loss: 1.6017807722091675
Epoch 1880, training loss: 0.007708929944783449 = 0.0007975315675139427 + 0.001 * 6.911397933959961
Epoch 1880, val loss: 1.603391170501709
Epoch 1890, training loss: 0.007692086044698954 = 0.0007926443940959871 + 0.001 * 6.899441242218018
Epoch 1890, val loss: 1.604783058166504
Epoch 1900, training loss: 0.007677705492824316 = 0.0007878273027017713 + 0.001 * 6.889877796173096
Epoch 1900, val loss: 1.6062785387039185
Epoch 1910, training loss: 0.007735890336334705 = 0.000783160503488034 + 0.001 * 6.95272970199585
Epoch 1910, val loss: 1.6077467203140259
Epoch 1920, training loss: 0.007693714462220669 = 0.0007785698981024325 + 0.001 * 6.915144443511963
Epoch 1920, val loss: 1.609012246131897
Epoch 1930, training loss: 0.007683027535676956 = 0.0007740826113149524 + 0.001 * 6.908944606781006
Epoch 1930, val loss: 1.6104689836502075
Epoch 1940, training loss: 0.00768757751211524 = 0.000769679609220475 + 0.001 * 6.917897701263428
Epoch 1940, val loss: 1.6118887662887573
Epoch 1950, training loss: 0.0076936217956244946 = 0.000765366421546787 + 0.001 * 6.928255081176758
Epoch 1950, val loss: 1.6132559776306152
Epoch 1960, training loss: 0.007683170028030872 = 0.0007611720939166844 + 0.001 * 6.921997547149658
Epoch 1960, val loss: 1.6145669221878052
Epoch 1970, training loss: 0.007640599738806486 = 0.000757040863391012 + 0.001 * 6.88355827331543
Epoch 1970, val loss: 1.615893006324768
Epoch 1980, training loss: 0.007627713028341532 = 0.0007530244183726609 + 0.001 * 6.874688148498535
Epoch 1980, val loss: 1.6172682046890259
Epoch 1990, training loss: 0.007646700832992792 = 0.0007490955176763237 + 0.001 * 6.897604942321777
Epoch 1990, val loss: 1.618625521659851
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8228782287822879
=== training gcn model ===
Epoch 0, training loss: 1.9590489864349365 = 1.9504520893096924 + 0.001 * 8.596868515014648
Epoch 0, val loss: 1.9575685262680054
Epoch 10, training loss: 1.9486069679260254 = 1.9400101900100708 + 0.001 * 8.596823692321777
Epoch 10, val loss: 1.9469528198242188
Epoch 20, training loss: 1.935880422592163 = 1.927283763885498 + 0.001 * 8.596657752990723
Epoch 20, val loss: 1.9338237047195435
Epoch 30, training loss: 1.9183257818222046 = 1.9097295999526978 + 0.001 * 8.596235275268555
Epoch 30, val loss: 1.9157195091247559
Epoch 40, training loss: 1.8930102586746216 = 1.8844151496887207 + 0.001 * 8.595049858093262
Epoch 40, val loss: 1.8900375366210938
Epoch 50, training loss: 1.858764886856079 = 1.8501735925674438 + 0.001 * 8.591245651245117
Epoch 50, val loss: 1.85689115524292
Epoch 60, training loss: 1.8224598169326782 = 1.8138837814331055 + 0.001 * 8.576054573059082
Epoch 60, val loss: 1.8248611688613892
Epoch 70, training loss: 1.7934880256652832 = 1.784989595413208 + 0.001 * 8.498382568359375
Epoch 70, val loss: 1.7992006540298462
Epoch 80, training loss: 1.7585363388061523 = 1.7504050731658936 + 0.001 * 8.131312370300293
Epoch 80, val loss: 1.765884518623352
Epoch 90, training loss: 1.7091163396835327 = 1.7010682821273804 + 0.001 * 8.048104286193848
Epoch 90, val loss: 1.7228459119796753
Epoch 100, training loss: 1.6398661136627197 = 1.6318851709365845 + 0.001 * 7.980915546417236
Epoch 100, val loss: 1.6646864414215088
Epoch 110, training loss: 1.556403398513794 = 1.5485011339187622 + 0.001 * 7.90226936340332
Epoch 110, val loss: 1.5960328578948975
Epoch 120, training loss: 1.4714868068695068 = 1.4637682437896729 + 0.001 * 7.718539237976074
Epoch 120, val loss: 1.5271079540252686
Epoch 130, training loss: 1.3901008367538452 = 1.3825746774673462 + 0.001 * 7.5261969566345215
Epoch 130, val loss: 1.4619296789169312
Epoch 140, training loss: 1.310203194618225 = 1.3026912212371826 + 0.001 * 7.5120086669921875
Epoch 140, val loss: 1.3985764980316162
Epoch 150, training loss: 1.229189157485962 = 1.2217204570770264 + 0.001 * 7.468757152557373
Epoch 150, val loss: 1.3358489274978638
Epoch 160, training loss: 1.1470574140548706 = 1.139601230621338 + 0.001 * 7.456215858459473
Epoch 160, val loss: 1.2747334241867065
Epoch 170, training loss: 1.0648983716964722 = 1.05745267868042 + 0.001 * 7.4457478523254395
Epoch 170, val loss: 1.21630859375
Epoch 180, training loss: 0.9833418726921082 = 0.9759145975112915 + 0.001 * 7.427278518676758
Epoch 180, val loss: 1.1597589254379272
Epoch 190, training loss: 0.9034618735313416 = 0.8960614204406738 + 0.001 * 7.400430202484131
Epoch 190, val loss: 1.1053760051727295
Epoch 200, training loss: 0.8268924951553345 = 0.8195227980613708 + 0.001 * 7.369699954986572
Epoch 200, val loss: 1.054634690284729
Epoch 210, training loss: 0.75548255443573 = 0.7481356263160706 + 0.001 * 7.346940517425537
Epoch 210, val loss: 1.008996844291687
Epoch 220, training loss: 0.6903088092803955 = 0.6829872727394104 + 0.001 * 7.321531772613525
Epoch 220, val loss: 0.9699254035949707
Epoch 230, training loss: 0.6309005618095398 = 0.6236111521720886 + 0.001 * 7.289435863494873
Epoch 230, val loss: 0.93718022108078
Epoch 240, training loss: 0.5759913921356201 = 0.568733811378479 + 0.001 * 7.257593154907227
Epoch 240, val loss: 0.9098076820373535
Epoch 250, training loss: 0.5247011780738831 = 0.5174748301506042 + 0.001 * 7.226321220397949
Epoch 250, val loss: 0.8875454664230347
Epoch 260, training loss: 0.47696295380592346 = 0.4697507619857788 + 0.001 * 7.212179183959961
Epoch 260, val loss: 0.8709635138511658
Epoch 270, training loss: 0.4329930245876312 = 0.4257946014404297 + 0.001 * 7.198422431945801
Epoch 270, val loss: 0.8608529567718506
Epoch 280, training loss: 0.3928530216217041 = 0.38566258549690247 + 0.001 * 7.1904497146606445
Epoch 280, val loss: 0.8574310541152954
Epoch 290, training loss: 0.3563077449798584 = 0.349119633436203 + 0.001 * 7.18811559677124
Epoch 290, val loss: 0.8599067330360413
Epoch 300, training loss: 0.3229326605796814 = 0.31574612855911255 + 0.001 * 7.1865234375
Epoch 300, val loss: 0.8674637079238892
Epoch 310, training loss: 0.2923267185688019 = 0.28514185547828674 + 0.001 * 7.184854507446289
Epoch 310, val loss: 0.879223108291626
Epoch 320, training loss: 0.2642193138599396 = 0.2570386528968811 + 0.001 * 7.180655479431152
Epoch 320, val loss: 0.8945842385292053
Epoch 330, training loss: 0.23840805888175964 = 0.23122967779636383 + 0.001 * 7.1783833503723145
Epoch 330, val loss: 0.9127764701843262
Epoch 340, training loss: 0.21473407745361328 = 0.20755617320537567 + 0.001 * 7.177906036376953
Epoch 340, val loss: 0.9330453276634216
Epoch 350, training loss: 0.19304561614990234 = 0.18586860597133636 + 0.001 * 7.1770100593566895
Epoch 350, val loss: 0.9550114274024963
Epoch 360, training loss: 0.1732141077518463 = 0.1660429835319519 + 0.001 * 7.171126365661621
Epoch 360, val loss: 0.9781680703163147
Epoch 370, training loss: 0.1551431119441986 = 0.14797444641590118 + 0.001 * 7.168669700622559
Epoch 370, val loss: 1.0021263360977173
Epoch 380, training loss: 0.1387348771095276 = 0.13156667351722717 + 0.001 * 7.168209552764893
Epoch 380, val loss: 1.0265605449676514
Epoch 390, training loss: 0.1239432767033577 = 0.11676923930644989 + 0.001 * 7.174035549163818
Epoch 390, val loss: 1.0512197017669678
Epoch 400, training loss: 0.1106882318854332 = 0.10352069139480591 + 0.001 * 7.167538642883301
Epoch 400, val loss: 1.0758198499679565
Epoch 410, training loss: 0.0989023894071579 = 0.09174028784036636 + 0.001 * 7.162099838256836
Epoch 410, val loss: 1.1001338958740234
Epoch 420, training loss: 0.08848574012517929 = 0.08132639527320862 + 0.001 * 7.159345626831055
Epoch 420, val loss: 1.1240310668945312
Epoch 430, training loss: 0.07932470738887787 = 0.07216808199882507 + 0.001 * 7.156628608703613
Epoch 430, val loss: 1.147423505783081
Epoch 440, training loss: 0.07131148129701614 = 0.06414827704429626 + 0.001 * 7.163206100463867
Epoch 440, val loss: 1.1703224182128906
Epoch 450, training loss: 0.06429870426654816 = 0.05714750289916992 + 0.001 * 7.151198863983154
Epoch 450, val loss: 1.1925415992736816
Epoch 460, training loss: 0.058196306228637695 = 0.05104758217930794 + 0.001 * 7.148723602294922
Epoch 460, val loss: 1.2140544652938843
Epoch 470, training loss: 0.05289231240749359 = 0.04573553055524826 + 0.001 * 7.15678071975708
Epoch 470, val loss: 1.2348792552947998
Epoch 480, training loss: 0.04825524240732193 = 0.04110823944211006 + 0.001 * 7.147003650665283
Epoch 480, val loss: 1.2549973726272583
Epoch 490, training loss: 0.04421260952949524 = 0.037072595208883286 + 0.001 * 7.140015602111816
Epoch 490, val loss: 1.2743761539459229
Epoch 500, training loss: 0.04071209579706192 = 0.03354700654745102 + 0.001 * 7.1650896072387695
Epoch 500, val loss: 1.293096661567688
Epoch 510, training loss: 0.03759973123669624 = 0.030459469184279442 + 0.001 * 7.140261173248291
Epoch 510, val loss: 1.311036229133606
Epoch 520, training loss: 0.03488821163773537 = 0.027749044820666313 + 0.001 * 7.139166355133057
Epoch 520, val loss: 1.3283637762069702
Epoch 530, training loss: 0.032497867941856384 = 0.02536342293024063 + 0.001 * 7.134444713592529
Epoch 530, val loss: 1.3450621366500854
Epoch 540, training loss: 0.03038625977933407 = 0.02325824461877346 + 0.001 * 7.12801456451416
Epoch 540, val loss: 1.3612278699874878
Epoch 550, training loss: 0.0285380557179451 = 0.021395739167928696 + 0.001 * 7.142316818237305
Epoch 550, val loss: 1.3768219947814941
Epoch 560, training loss: 0.026862874627113342 = 0.0197430532425642 + 0.001 * 7.119821548461914
Epoch 560, val loss: 1.391912817955017
Epoch 570, training loss: 0.02539670839905739 = 0.018272120505571365 + 0.001 * 7.1245880126953125
Epoch 570, val loss: 1.4064966440200806
Epoch 580, training loss: 0.02408393658697605 = 0.0169590562582016 + 0.001 * 7.124880313873291
Epoch 580, val loss: 1.420557975769043
Epoch 590, training loss: 0.022901063784956932 = 0.01578330434858799 + 0.001 * 7.117758750915527
Epoch 590, val loss: 1.4341824054718018
Epoch 600, training loss: 0.021848559379577637 = 0.014727617613971233 + 0.001 * 7.120942115783691
Epoch 600, val loss: 1.4473737478256226
Epoch 610, training loss: 0.020890135318040848 = 0.013776895590126514 + 0.001 * 7.113239288330078
Epoch 610, val loss: 1.460131049156189
Epoch 620, training loss: 0.020030148327350616 = 0.012918207794427872 + 0.001 * 7.111940860748291
Epoch 620, val loss: 1.4724918603897095
Epoch 630, training loss: 0.019264791160821915 = 0.012140467762947083 + 0.001 * 7.124321937561035
Epoch 630, val loss: 1.4844990968704224
Epoch 640, training loss: 0.018551189452409744 = 0.01143408939242363 + 0.001 * 7.117098808288574
Epoch 640, val loss: 1.4961096048355103
Epoch 650, training loss: 0.017899969592690468 = 0.010790773667395115 + 0.001 * 7.109195232391357
Epoch 650, val loss: 1.507401466369629
Epoch 660, training loss: 0.017302948981523514 = 0.010203389450907707 + 0.001 * 7.0995588302612305
Epoch 660, val loss: 1.5183547735214233
Epoch 670, training loss: 0.016761653125286102 = 0.009665699675679207 + 0.001 * 7.095953941345215
Epoch 670, val loss: 1.5289760828018188
Epoch 680, training loss: 0.016270684078335762 = 0.009172381833195686 + 0.001 * 7.098301410675049
Epoch 680, val loss: 1.5393105745315552
Epoch 690, training loss: 0.01581525057554245 = 0.008718745782971382 + 0.001 * 7.096505165100098
Epoch 690, val loss: 1.5493327379226685
Epoch 700, training loss: 0.015392124652862549 = 0.008300618268549442 + 0.001 * 7.091505527496338
Epoch 700, val loss: 1.5590713024139404
Epoch 710, training loss: 0.015015584416687489 = 0.007914429530501366 + 0.001 * 7.101154327392578
Epoch 710, val loss: 1.568562388420105
Epoch 720, training loss: 0.014644185081124306 = 0.00755699910223484 + 0.001 * 7.087186336517334
Epoch 720, val loss: 1.5778034925460815
Epoch 730, training loss: 0.014307881705462933 = 0.007225541397929192 + 0.001 * 7.082339763641357
Epoch 730, val loss: 1.5868374109268188
Epoch 740, training loss: 0.014012010768055916 = 0.00691754836589098 + 0.001 * 7.094461917877197
Epoch 740, val loss: 1.5956509113311768
Epoch 750, training loss: 0.013723818585276604 = 0.0066309054382145405 + 0.001 * 7.0929131507873535
Epoch 750, val loss: 1.6042448282241821
Epoch 760, training loss: 0.013446364551782608 = 0.006363593507558107 + 0.001 * 7.082771301269531
Epoch 760, val loss: 1.6126065254211426
Epoch 770, training loss: 0.013200373388826847 = 0.006113850045949221 + 0.001 * 7.086523056030273
Epoch 770, val loss: 1.6208069324493408
Epoch 780, training loss: 0.012952353805303574 = 0.0058800335973501205 + 0.001 * 7.072319507598877
Epoch 780, val loss: 1.6288217306137085
Epoch 790, training loss: 0.01274433545768261 = 0.005660399328917265 + 0.001 * 7.083935260772705
Epoch 790, val loss: 1.6366603374481201
Epoch 800, training loss: 0.012536978349089622 = 0.005453443620353937 + 0.001 * 7.0835347175598145
Epoch 800, val loss: 1.6443455219268799
Epoch 810, training loss: 0.01233042124658823 = 0.00525760930031538 + 0.001 * 7.072811603546143
Epoch 810, val loss: 1.6519488096237183
Epoch 820, training loss: 0.01215360313653946 = 0.005071514286100864 + 0.001 * 7.082087993621826
Epoch 820, val loss: 1.6593838930130005
Epoch 830, training loss: 0.011971823871135712 = 0.004894141107797623 + 0.001 * 7.077682018280029
Epoch 830, val loss: 1.6667265892028809
Epoch 840, training loss: 0.01179889403283596 = 0.004724852275103331 + 0.001 * 7.074041843414307
Epoch 840, val loss: 1.6739314794540405
Epoch 850, training loss: 0.011637527495622635 = 0.004563410300761461 + 0.001 * 7.0741167068481445
Epoch 850, val loss: 1.6810510158538818
Epoch 860, training loss: 0.01147700846195221 = 0.0044095865450799465 + 0.001 * 7.0674214363098145
Epoch 860, val loss: 1.688017725944519
Epoch 870, training loss: 0.011332765221595764 = 0.004263094160705805 + 0.001 * 7.069670677185059
Epoch 870, val loss: 1.6948919296264648
Epoch 880, training loss: 0.011179599910974503 = 0.004123596008867025 + 0.001 * 7.056003093719482
Epoch 880, val loss: 1.7016369104385376
Epoch 890, training loss: 0.01106558833271265 = 0.003990848548710346 + 0.001 * 7.074739456176758
Epoch 890, val loss: 1.708248257637024
Epoch 900, training loss: 0.010928954929113388 = 0.003864559344947338 + 0.001 * 7.064395904541016
Epoch 900, val loss: 1.7147337198257446
Epoch 910, training loss: 0.010794405825436115 = 0.0037444881163537502 + 0.001 * 7.049917221069336
Epoch 910, val loss: 1.7210761308670044
Epoch 920, training loss: 0.01067863404750824 = 0.0036302865482866764 + 0.001 * 7.048346996307373
Epoch 920, val loss: 1.7273083925247192
Epoch 930, training loss: 0.010578246787190437 = 0.0035216500982642174 + 0.001 * 7.056595802307129
Epoch 930, val loss: 1.7334250211715698
Epoch 940, training loss: 0.010479350574314594 = 0.00341829564422369 + 0.001 * 7.061054706573486
Epoch 940, val loss: 1.739410161972046
Epoch 950, training loss: 0.010379099287092686 = 0.0033199586905539036 + 0.001 * 7.059140205383301
Epoch 950, val loss: 1.7452507019042969
Epoch 960, training loss: 0.010285272262990475 = 0.0032264261972159147 + 0.001 * 7.058845520019531
Epoch 960, val loss: 1.7509838342666626
Epoch 970, training loss: 0.01017975714057684 = 0.0031373954843729734 + 0.001 * 7.042361259460449
Epoch 970, val loss: 1.756624460220337
Epoch 980, training loss: 0.010097619146108627 = 0.0030525936745107174 + 0.001 * 7.045024871826172
Epoch 980, val loss: 1.7621771097183228
Epoch 990, training loss: 0.010037948377430439 = 0.0029718002770096064 + 0.001 * 7.066147804260254
Epoch 990, val loss: 1.7675827741622925
Epoch 1000, training loss: 0.009942935779690742 = 0.002894769422709942 + 0.001 * 7.048165798187256
Epoch 1000, val loss: 1.7728996276855469
Epoch 1010, training loss: 0.009863121435046196 = 0.002821260131895542 + 0.001 * 7.041861534118652
Epoch 1010, val loss: 1.7781155109405518
Epoch 1020, training loss: 0.00979846902191639 = 0.0027511008083820343 + 0.001 * 7.047367572784424
Epoch 1020, val loss: 1.783227562904358
Epoch 1030, training loss: 0.009732536971569061 = 0.002684060949832201 + 0.001 * 7.048476219177246
Epoch 1030, val loss: 1.7882927656173706
Epoch 1040, training loss: 0.009660262614488602 = 0.0026199519634246826 + 0.001 * 7.040309906005859
Epoch 1040, val loss: 1.793201208114624
Epoch 1050, training loss: 0.009581467136740685 = 0.002558651380240917 + 0.001 * 7.022814750671387
Epoch 1050, val loss: 1.7980761528015137
Epoch 1060, training loss: 0.009554911404848099 = 0.0024998229928314686 + 0.001 * 7.055088520050049
Epoch 1060, val loss: 1.8029162883758545
Epoch 1070, training loss: 0.009467908181250095 = 0.002443434903398156 + 0.001 * 7.024473190307617
Epoch 1070, val loss: 1.8076375722885132
Epoch 1080, training loss: 0.009419068694114685 = 0.002389190485700965 + 0.001 * 7.02987813949585
Epoch 1080, val loss: 1.8123880624771118
Epoch 1090, training loss: 0.009354809299111366 = 0.002337162382900715 + 0.001 * 7.0176472663879395
Epoch 1090, val loss: 1.8170267343521118
Epoch 1100, training loss: 0.00930559728294611 = 0.002287116600200534 + 0.001 * 7.01848030090332
Epoch 1100, val loss: 1.8215827941894531
Epoch 1110, training loss: 0.009280873462557793 = 0.0022388568613678217 + 0.001 * 7.04201602935791
Epoch 1110, val loss: 1.826127052307129
Epoch 1120, training loss: 0.009232188574969769 = 0.0021922460291534662 + 0.001 * 7.039941787719727
Epoch 1120, val loss: 1.8305363655090332
Epoch 1130, training loss: 0.00917672086507082 = 0.0021470352075994015 + 0.001 * 7.0296854972839355
Epoch 1130, val loss: 1.8349758386611938
Epoch 1140, training loss: 0.009154045023024082 = 0.002103119855746627 + 0.001 * 7.050924777984619
Epoch 1140, val loss: 1.8393818140029907
Epoch 1150, training loss: 0.009101442992687225 = 0.0020603942684829235 + 0.001 * 7.041048526763916
Epoch 1150, val loss: 1.8436871767044067
Epoch 1160, training loss: 0.009033016860485077 = 0.0020187050104141235 + 0.001 * 7.014310836791992
Epoch 1160, val loss: 1.848052740097046
Epoch 1170, training loss: 0.008987612091004848 = 0.0019778809510171413 + 0.001 * 7.009730815887451
Epoch 1170, val loss: 1.8524898290634155
Epoch 1180, training loss: 0.008944245055317879 = 0.0019378992728888988 + 0.001 * 7.006344795227051
Epoch 1180, val loss: 1.8567789793014526
Epoch 1190, training loss: 0.008901599794626236 = 0.0018987399525940418 + 0.001 * 7.002860069274902
Epoch 1190, val loss: 1.8612675666809082
Epoch 1200, training loss: 0.00885699037462473 = 0.001860358752310276 + 0.001 * 6.996631145477295
Epoch 1200, val loss: 1.865626335144043
Epoch 1210, training loss: 0.008822232484817505 = 0.0018227752298116684 + 0.001 * 6.999456882476807
Epoch 1210, val loss: 1.8700357675552368
Epoch 1220, training loss: 0.008791597560048103 = 0.0017860119696706533 + 0.001 * 7.005585193634033
Epoch 1220, val loss: 1.874479055404663
Epoch 1230, training loss: 0.008757601492106915 = 0.0017501047113910317 + 0.001 * 7.007496356964111
Epoch 1230, val loss: 1.878828763961792
Epoch 1240, training loss: 0.008720980025827885 = 0.001715181046165526 + 0.001 * 7.005798816680908
Epoch 1240, val loss: 1.8832714557647705
Epoch 1250, training loss: 0.008668280206620693 = 0.0016812082612887025 + 0.001 * 6.987071514129639
Epoch 1250, val loss: 1.887662649154663
Epoch 1260, training loss: 0.008652896620333195 = 0.001648121397010982 + 0.001 * 7.004774570465088
Epoch 1260, val loss: 1.8920483589172363
Epoch 1270, training loss: 0.008637450635433197 = 0.0016159627120941877 + 0.001 * 7.021487236022949
Epoch 1270, val loss: 1.896362066268921
Epoch 1280, training loss: 0.00858842022716999 = 0.0015847979811951518 + 0.001 * 7.003622055053711
Epoch 1280, val loss: 1.9007152318954468
Epoch 1290, training loss: 0.008535074070096016 = 0.001554623944684863 + 0.001 * 6.980449676513672
Epoch 1290, val loss: 1.9050973653793335
Epoch 1300, training loss: 0.008522196672856808 = 0.0015253846067935228 + 0.001 * 6.996811389923096
Epoch 1300, val loss: 1.9093996286392212
Epoch 1310, training loss: 0.008493699133396149 = 0.0014970668125897646 + 0.001 * 6.996631622314453
Epoch 1310, val loss: 1.9136244058609009
Epoch 1320, training loss: 0.00846492126584053 = 0.0014697372680529952 + 0.001 * 6.99518346786499
Epoch 1320, val loss: 1.917785882949829
Epoch 1330, training loss: 0.00843801349401474 = 0.0014432483585551381 + 0.001 * 6.99476432800293
Epoch 1330, val loss: 1.9219269752502441
Epoch 1340, training loss: 0.00841592252254486 = 0.0014177135890349746 + 0.001 * 6.998208522796631
Epoch 1340, val loss: 1.9260848760604858
Epoch 1350, training loss: 0.00842611026018858 = 0.001393027720041573 + 0.001 * 7.033082008361816
Epoch 1350, val loss: 1.930140495300293
Epoch 1360, training loss: 0.008342050947248936 = 0.001369199831970036 + 0.001 * 6.972850799560547
Epoch 1360, val loss: 1.9340853691101074
Epoch 1370, training loss: 0.008305761031806469 = 0.0013461578637361526 + 0.001 * 6.9596028327941895
Epoch 1370, val loss: 1.9380712509155273
Epoch 1380, training loss: 0.008290699683129787 = 0.0013238813262432814 + 0.001 * 6.966818332672119
Epoch 1380, val loss: 1.9419840574264526
Epoch 1390, training loss: 0.008282856084406376 = 0.0013023160863667727 + 0.001 * 6.980539321899414
Epoch 1390, val loss: 1.9458003044128418
Epoch 1400, training loss: 0.008305980823934078 = 0.0012814243091270328 + 0.001 * 7.0245561599731445
Epoch 1400, val loss: 1.9495840072631836
Epoch 1410, training loss: 0.008242160081863403 = 0.0012612355640158057 + 0.001 * 6.980924606323242
Epoch 1410, val loss: 1.9532902240753174
Epoch 1420, training loss: 0.008203353732824326 = 0.0012417149264365435 + 0.001 * 6.961638927459717
Epoch 1420, val loss: 1.9569313526153564
Epoch 1430, training loss: 0.008195479400455952 = 0.0012228671694174409 + 0.001 * 6.972611427307129
Epoch 1430, val loss: 1.9605236053466797
Epoch 1440, training loss: 0.008199140429496765 = 0.0012046063784509897 + 0.001 * 6.994533538818359
Epoch 1440, val loss: 1.9640268087387085
Epoch 1450, training loss: 0.008165871724486351 = 0.001186912413686514 + 0.001 * 6.978959560394287
Epoch 1450, val loss: 1.9674967527389526
Epoch 1460, training loss: 0.008126871660351753 = 0.0011698288144543767 + 0.001 * 6.957042694091797
Epoch 1460, val loss: 1.9708963632583618
Epoch 1470, training loss: 0.008112623356282711 = 0.0011532983044162393 + 0.001 * 6.959324836730957
Epoch 1470, val loss: 1.974258542060852
Epoch 1480, training loss: 0.008109968155622482 = 0.0011373225133866072 + 0.001 * 6.972645282745361
Epoch 1480, val loss: 1.9775965213775635
Epoch 1490, training loss: 0.008099507540464401 = 0.0011218327563256025 + 0.001 * 6.97767448425293
Epoch 1490, val loss: 1.9808074235916138
Epoch 1500, training loss: 0.008050058037042618 = 0.0011068377643823624 + 0.001 * 6.943220615386963
Epoch 1500, val loss: 1.984023928642273
Epoch 1510, training loss: 0.008059966377913952 = 0.0010923286899924278 + 0.001 * 6.967637538909912
Epoch 1510, val loss: 1.9871749877929688
Epoch 1520, training loss: 0.008027130737900734 = 0.0010782470926642418 + 0.001 * 6.948883056640625
Epoch 1520, val loss: 1.990309238433838
Epoch 1530, training loss: 0.008026672527194023 = 0.0010645746951922774 + 0.001 * 6.96209716796875
Epoch 1530, val loss: 1.9933996200561523
Epoch 1540, training loss: 0.00801540445536375 = 0.0010513425804674625 + 0.001 * 6.964061737060547
Epoch 1540, val loss: 1.9963414669036865
Epoch 1550, training loss: 0.007986211217939854 = 0.0010385209461674094 + 0.001 * 6.947690486907959
Epoch 1550, val loss: 1.9992707967758179
Epoch 1560, training loss: 0.007966823875904083 = 0.0010261139832437038 + 0.001 * 6.940709590911865
Epoch 1560, val loss: 2.002211570739746
Epoch 1570, training loss: 0.007964052259922028 = 0.0010141130769625306 + 0.001 * 6.9499382972717285
Epoch 1570, val loss: 2.005056142807007
Epoch 1580, training loss: 0.007947497069835663 = 0.0010024678194895387 + 0.001 * 6.945028781890869
Epoch 1580, val loss: 2.007874011993408
Epoch 1590, training loss: 0.007956326939165592 = 0.0009911969536915421 + 0.001 * 6.965129852294922
Epoch 1590, val loss: 2.0106306076049805
Epoch 1600, training loss: 0.007936539128422737 = 0.000980239943601191 + 0.001 * 6.956298351287842
Epoch 1600, val loss: 2.0133774280548096
Epoch 1610, training loss: 0.007963246665894985 = 0.0009696378838270903 + 0.001 * 6.993607997894287
Epoch 1610, val loss: 2.0160269737243652
Epoch 1620, training loss: 0.007890338078141212 = 0.00095934554701671 + 0.001 * 6.930992603302002
Epoch 1620, val loss: 2.0185739994049072
Epoch 1630, training loss: 0.007864726707339287 = 0.0009493588586337864 + 0.001 * 6.915367603302002
Epoch 1630, val loss: 2.0211987495422363
Epoch 1640, training loss: 0.007853911258280277 = 0.0009396412060596049 + 0.001 * 6.91426944732666
Epoch 1640, val loss: 2.023709535598755
Epoch 1650, training loss: 0.00786728598177433 = 0.0009302184917032719 + 0.001 * 6.937067031860352
Epoch 1650, val loss: 2.026177406311035
Epoch 1660, training loss: 0.007846014574170113 = 0.000921035127248615 + 0.001 * 6.924979209899902
Epoch 1660, val loss: 2.028682231903076
Epoch 1670, training loss: 0.007856479845941067 = 0.0009121153852902353 + 0.001 * 6.944364547729492
Epoch 1670, val loss: 2.0311014652252197
Epoch 1680, training loss: 0.007825139909982681 = 0.0009034716640599072 + 0.001 * 6.92166805267334
Epoch 1680, val loss: 2.033527374267578
Epoch 1690, training loss: 0.007802205625921488 = 0.000895054021384567 + 0.001 * 6.907151222229004
Epoch 1690, val loss: 2.035799741744995
Epoch 1700, training loss: 0.007799388840794563 = 0.0008868781151250005 + 0.001 * 6.912510395050049
Epoch 1700, val loss: 2.0381314754486084
Epoch 1710, training loss: 0.00778750516474247 = 0.0008789454586803913 + 0.001 * 6.908559322357178
Epoch 1710, val loss: 2.0403923988342285
Epoch 1720, training loss: 0.007783864624798298 = 0.0008712255512364209 + 0.001 * 6.9126386642456055
Epoch 1720, val loss: 2.04266357421875
Epoch 1730, training loss: 0.007809547707438469 = 0.0008637087303213775 + 0.001 * 6.945838928222656
Epoch 1730, val loss: 2.0447914600372314
Epoch 1740, training loss: 0.007763673551380634 = 0.0008563963929191232 + 0.001 * 6.9072771072387695
Epoch 1740, val loss: 2.047025203704834
Epoch 1750, training loss: 0.0077549004927277565 = 0.0008492874912917614 + 0.001 * 6.905612468719482
Epoch 1750, val loss: 2.0491340160369873
Epoch 1760, training loss: 0.007774262689054012 = 0.0008423711988143623 + 0.001 * 6.931891441345215
Epoch 1760, val loss: 2.0513033866882324
Epoch 1770, training loss: 0.007748931180685759 = 0.0008356185862794518 + 0.001 * 6.9133124351501465
Epoch 1770, val loss: 2.0533828735351562
Epoch 1780, training loss: 0.007724156137555838 = 0.0008290917612612247 + 0.001 * 6.895063877105713
Epoch 1780, val loss: 2.0554304122924805
Epoch 1790, training loss: 0.007728316821157932 = 0.0008227229700423777 + 0.001 * 6.905593395233154
Epoch 1790, val loss: 2.0574915409088135
Epoch 1800, training loss: 0.007730444893240929 = 0.0008164854370988905 + 0.001 * 6.91395902633667
Epoch 1800, val loss: 2.0594637393951416
Epoch 1810, training loss: 0.007734491024166346 = 0.0008104778826236725 + 0.001 * 6.924012660980225
Epoch 1810, val loss: 2.0614352226257324
Epoch 1820, training loss: 0.007767191622406244 = 0.0008046143921092153 + 0.001 * 6.962576866149902
Epoch 1820, val loss: 2.063291311264038
Epoch 1830, training loss: 0.007718562614172697 = 0.0007989088189788163 + 0.001 * 6.919653415679932
Epoch 1830, val loss: 2.065178155899048
Epoch 1840, training loss: 0.007719625718891621 = 0.0007934195455163717 + 0.001 * 6.926206111907959
Epoch 1840, val loss: 2.0669848918914795
Epoch 1850, training loss: 0.007671759929507971 = 0.0007880223565734923 + 0.001 * 6.883737087249756
Epoch 1850, val loss: 2.068815231323242
Epoch 1860, training loss: 0.007684994488954544 = 0.0007827738299965858 + 0.001 * 6.902220249176025
Epoch 1860, val loss: 2.07063627243042
Epoch 1870, training loss: 0.00768292834982276 = 0.0007776540005579591 + 0.001 * 6.905273914337158
Epoch 1870, val loss: 2.0724403858184814
Epoch 1880, training loss: 0.007677271030843258 = 0.0007726596086286008 + 0.001 * 6.904611110687256
Epoch 1880, val loss: 2.074110746383667
Epoch 1890, training loss: 0.007665506564080715 = 0.000767806894145906 + 0.001 * 6.897699356079102
Epoch 1890, val loss: 2.0757899284362793
Epoch 1900, training loss: 0.007668888196349144 = 0.0007630373584106565 + 0.001 * 6.905850410461426
Epoch 1900, val loss: 2.077510118484497
Epoch 1910, training loss: 0.0076550585217773914 = 0.0007583976839669049 + 0.001 * 6.896660327911377
Epoch 1910, val loss: 2.0791709423065186
Epoch 1920, training loss: 0.0076911961659789085 = 0.0007539023063145578 + 0.001 * 6.937293529510498
Epoch 1920, val loss: 2.080732822418213
Epoch 1930, training loss: 0.00763543788343668 = 0.0007495001773349941 + 0.001 * 6.885937690734863
Epoch 1930, val loss: 2.0822408199310303
Epoch 1940, training loss: 0.007613704074174166 = 0.0007452354184351861 + 0.001 * 6.868468284606934
Epoch 1940, val loss: 2.083812952041626
Epoch 1950, training loss: 0.0076190815307199955 = 0.000741038762498647 + 0.001 * 6.878042221069336
Epoch 1950, val loss: 2.0853166580200195
Epoch 1960, training loss: 0.007618526462465525 = 0.0007369195809587836 + 0.001 * 6.881606578826904
Epoch 1960, val loss: 2.0868070125579834
Epoch 1970, training loss: 0.00762970931828022 = 0.0007329249638132751 + 0.001 * 6.89678430557251
Epoch 1970, val loss: 2.0883424282073975
Epoch 1980, training loss: 0.007604838348925114 = 0.0007290014182217419 + 0.001 * 6.875836372375488
Epoch 1980, val loss: 2.0896544456481934
Epoch 1990, training loss: 0.007600806187838316 = 0.0007251904462464154 + 0.001 * 6.875615119934082
Epoch 1990, val loss: 2.0911922454833984
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8218239325250396
The final CL Acc:0.76296, 0.01983, The final GNN Acc:0.81761, 0.00672
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13256])
remove edge: torch.Size([2, 8030])
updated graph: torch.Size([2, 10730])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9512865543365479 = 1.9426896572113037 + 0.001 * 8.59684944152832
Epoch 0, val loss: 1.9406006336212158
Epoch 10, training loss: 1.9415411949157715 = 1.932944416999817 + 0.001 * 8.596793174743652
Epoch 10, val loss: 1.9310232400894165
Epoch 20, training loss: 1.929835319519043 = 1.921238660812378 + 0.001 * 8.596598625183105
Epoch 20, val loss: 1.9195330142974854
Epoch 30, training loss: 1.9138613939285278 = 1.9052653312683105 + 0.001 * 8.596109390258789
Epoch 30, val loss: 1.904062271118164
Epoch 40, training loss: 1.8908817768096924 = 1.8822870254516602 + 0.001 * 8.594808578491211
Epoch 40, val loss: 1.8821851015090942
Epoch 50, training loss: 1.8584442138671875 = 1.8498538732528687 + 0.001 * 8.590368270874023
Epoch 50, val loss: 1.852343201637268
Epoch 60, training loss: 1.818434476852417 = 1.809863567352295 + 0.001 * 8.570948600769043
Epoch 60, val loss: 1.8175631761550903
Epoch 70, training loss: 1.7787880897521973 = 1.7703121900558472 + 0.001 * 8.475933074951172
Epoch 70, val loss: 1.783637523651123
Epoch 80, training loss: 1.734721302986145 = 1.7266032695770264 + 0.001 * 8.118048667907715
Epoch 80, val loss: 1.7406375408172607
Epoch 90, training loss: 1.673172950744629 = 1.6652393341064453 + 0.001 * 7.9335618019104
Epoch 90, val loss: 1.68088698387146
Epoch 100, training loss: 1.5913689136505127 = 1.5835847854614258 + 0.001 * 7.784087181091309
Epoch 100, val loss: 1.607589840888977
Epoch 110, training loss: 1.4945564270019531 = 1.4868974685668945 + 0.001 * 7.65894889831543
Epoch 110, val loss: 1.5232188701629639
Epoch 120, training loss: 1.3924026489257812 = 1.3849005699157715 + 0.001 * 7.5020222663879395
Epoch 120, val loss: 1.4342509508132935
Epoch 130, training loss: 1.293081283569336 = 1.2856876850128174 + 0.001 * 7.3936052322387695
Epoch 130, val loss: 1.3508377075195312
Epoch 140, training loss: 1.197685718536377 = 1.1903973817825317 + 0.001 * 7.288350582122803
Epoch 140, val loss: 1.2731006145477295
Epoch 150, training loss: 1.1043468713760376 = 1.097109317779541 + 0.001 * 7.237559795379639
Epoch 150, val loss: 1.1989260911941528
Epoch 160, training loss: 1.0113253593444824 = 1.0040968656539917 + 0.001 * 7.228536605834961
Epoch 160, val loss: 1.126541018486023
Epoch 170, training loss: 0.9193295240402222 = 0.9121055006980896 + 0.001 * 7.2240118980407715
Epoch 170, val loss: 1.0560617446899414
Epoch 180, training loss: 0.8310238122940063 = 0.8238036036491394 + 0.001 * 7.220222473144531
Epoch 180, val loss: 0.9895784854888916
Epoch 190, training loss: 0.7492082715034485 = 0.7419916987419128 + 0.001 * 7.216548442840576
Epoch 190, val loss: 0.9296934008598328
Epoch 200, training loss: 0.6759047508239746 = 0.6686933040618896 + 0.001 * 7.211432933807373
Epoch 200, val loss: 0.8779265284538269
Epoch 210, training loss: 0.6119949221611023 = 0.6047888994216919 + 0.001 * 7.206023693084717
Epoch 210, val loss: 0.835877001285553
Epoch 220, training loss: 0.5571265816688538 = 0.5499271154403687 + 0.001 * 7.199443817138672
Epoch 220, val loss: 0.8032988905906677
Epoch 230, training loss: 0.5097770690917969 = 0.5025861263275146 + 0.001 * 7.19096040725708
Epoch 230, val loss: 0.7788255214691162
Epoch 240, training loss: 0.4677835702896118 = 0.4606051743030548 + 0.001 * 7.178390026092529
Epoch 240, val loss: 0.7601558566093445
Epoch 250, training loss: 0.42889851331710815 = 0.42173632979393005 + 0.001 * 7.162171363830566
Epoch 250, val loss: 0.745114266872406
Epoch 260, training loss: 0.39120158553123474 = 0.3840511739253998 + 0.001 * 7.150423526763916
Epoch 260, val loss: 0.7319177389144897
Epoch 270, training loss: 0.3534446358680725 = 0.346314400434494 + 0.001 * 7.130247592926025
Epoch 270, val loss: 0.719631552696228
Epoch 280, training loss: 0.3154778778553009 = 0.30836233496665955 + 0.001 * 7.115541458129883
Epoch 280, val loss: 0.7082821130752563
Epoch 290, training loss: 0.27834591269493103 = 0.27123862504959106 + 0.001 * 7.107275485992432
Epoch 290, val loss: 0.6987780332565308
Epoch 300, training loss: 0.24375805258750916 = 0.23665930330753326 + 0.001 * 7.098755836486816
Epoch 300, val loss: 0.6920648217201233
Epoch 310, training loss: 0.21309249103069305 = 0.20600064098834991 + 0.001 * 7.09184455871582
Epoch 310, val loss: 0.6887947916984558
Epoch 320, training loss: 0.1867508590221405 = 0.17966580390930176 + 0.001 * 7.085054397583008
Epoch 320, val loss: 0.6889432668685913
Epoch 330, training loss: 0.16436539590358734 = 0.15727375447750092 + 0.001 * 7.091643810272217
Epoch 330, val loss: 0.6917712092399597
Epoch 340, training loss: 0.14527098834514618 = 0.13819214701652527 + 0.001 * 7.078835964202881
Epoch 340, val loss: 0.6966206431388855
Epoch 350, training loss: 0.12890522181987762 = 0.12183137238025665 + 0.001 * 7.073849201202393
Epoch 350, val loss: 0.7029391527175903
Epoch 360, training loss: 0.11478064209222794 = 0.10771037638187408 + 0.001 * 7.070263862609863
Epoch 360, val loss: 0.7102843523025513
Epoch 370, training loss: 0.10251123458147049 = 0.09544306993484497 + 0.001 * 7.068165302276611
Epoch 370, val loss: 0.7183949947357178
Epoch 380, training loss: 0.09180039912462234 = 0.08473493158817291 + 0.001 * 7.0654683113098145
Epoch 380, val loss: 0.7271701097488403
Epoch 390, training loss: 0.08242058008909225 = 0.07535495609045029 + 0.001 * 7.065627098083496
Epoch 390, val loss: 0.7365342378616333
Epoch 400, training loss: 0.07419069111347198 = 0.06712672859430313 + 0.001 * 7.063965320587158
Epoch 400, val loss: 0.7463957667350769
Epoch 410, training loss: 0.0669635757803917 = 0.05990450456738472 + 0.001 * 7.059070110321045
Epoch 410, val loss: 0.756674587726593
Epoch 420, training loss: 0.06062577664852142 = 0.05356752499938011 + 0.001 * 7.058251857757568
Epoch 420, val loss: 0.7672384977340698
Epoch 430, training loss: 0.055066920816898346 = 0.04801030084490776 + 0.001 * 7.056619644165039
Epoch 430, val loss: 0.7779646515846252
Epoch 440, training loss: 0.05019310116767883 = 0.04313984140753746 + 0.001 * 7.053260803222656
Epoch 440, val loss: 0.788781464099884
Epoch 450, training loss: 0.04593456909060478 = 0.0388716384768486 + 0.001 * 7.062929153442383
Epoch 450, val loss: 0.7995656132698059
Epoch 460, training loss: 0.04218452423810959 = 0.035129718482494354 + 0.001 * 7.054807186126709
Epoch 460, val loss: 0.810202419757843
Epoch 470, training loss: 0.03889578580856323 = 0.03184651583433151 + 0.001 * 7.049269199371338
Epoch 470, val loss: 0.8206789493560791
Epoch 480, training loss: 0.03600763529539108 = 0.028962314128875732 + 0.001 * 7.0453200340271
Epoch 480, val loss: 0.8309323191642761
Epoch 490, training loss: 0.033503420650959015 = 0.026424584910273552 + 0.001 * 7.078836917877197
Epoch 490, val loss: 0.8409846425056458
Epoch 500, training loss: 0.03123122826218605 = 0.024187909439206123 + 0.001 * 7.043319225311279
Epoch 500, val loss: 0.8507498502731323
Epoch 510, training loss: 0.02925211936235428 = 0.022211255505681038 + 0.001 * 7.040862560272217
Epoch 510, val loss: 0.8602363467216492
Epoch 520, training loss: 0.02749529853463173 = 0.02045983448624611 + 0.001 * 7.035463333129883
Epoch 520, val loss: 0.8694387078285217
Epoch 530, training loss: 0.025937601923942566 = 0.018903853371739388 + 0.001 * 7.033747673034668
Epoch 530, val loss: 0.8784001469612122
Epoch 540, training loss: 0.02455221861600876 = 0.01751767098903656 + 0.001 * 7.034546852111816
Epoch 540, val loss: 0.8871067762374878
Epoch 550, training loss: 0.023313703015446663 = 0.016278568655252457 + 0.001 * 7.0351338386535645
Epoch 550, val loss: 0.8955671787261963
Epoch 560, training loss: 0.02219584584236145 = 0.015166166238486767 + 0.001 * 7.029678821563721
Epoch 560, val loss: 0.9037825465202332
Epoch 570, training loss: 0.02119780331850052 = 0.014160715974867344 + 0.001 * 7.037086009979248
Epoch 570, val loss: 0.9117587804794312
Epoch 580, training loss: 0.020271088927984238 = 0.013246232643723488 + 0.001 * 7.0248565673828125
Epoch 580, val loss: 0.9195444583892822
Epoch 590, training loss: 0.019434873014688492 = 0.012411883100867271 + 0.001 * 7.022988796234131
Epoch 590, val loss: 0.9271643757820129
Epoch 600, training loss: 0.018675705417990685 = 0.011649339459836483 + 0.001 * 7.026365756988525
Epoch 600, val loss: 0.9345883131027222
Epoch 610, training loss: 0.017972897738218307 = 0.010951672680675983 + 0.001 * 7.021225452423096
Epoch 610, val loss: 0.9418275356292725
Epoch 620, training loss: 0.01735103689134121 = 0.010312705300748348 + 0.001 * 7.038331031799316
Epoch 620, val loss: 0.9488437175750732
Epoch 630, training loss: 0.016752148047089577 = 0.009727017022669315 + 0.001 * 7.025130271911621
Epoch 630, val loss: 0.9556622505187988
Epoch 640, training loss: 0.01620638370513916 = 0.009189601987600327 + 0.001 * 7.016780376434326
Epoch 640, val loss: 0.9623008370399475
Epoch 650, training loss: 0.015706269070506096 = 0.008695750497281551 + 0.001 * 7.010518550872803
Epoch 650, val loss: 0.9687691926956177
Epoch 660, training loss: 0.01524842157959938 = 0.008241242729127407 + 0.001 * 7.00717830657959
Epoch 660, val loss: 0.9750434160232544
Epoch 670, training loss: 0.014835452660918236 = 0.007822341285645962 + 0.001 * 7.013111114501953
Epoch 670, val loss: 0.9811542630195618
Epoch 680, training loss: 0.014438420534133911 = 0.0074358438141644 + 0.001 * 7.00257682800293
Epoch 680, val loss: 0.9871533513069153
Epoch 690, training loss: 0.01408645510673523 = 0.00707859406247735 + 0.001 * 7.0078606605529785
Epoch 690, val loss: 0.9929277300834656
Epoch 700, training loss: 0.013750877231359482 = 0.0067477901466190815 + 0.001 * 7.003086566925049
Epoch 700, val loss: 0.9985657930374146
Epoch 710, training loss: 0.013439401052892208 = 0.006440997123718262 + 0.001 * 6.998403549194336
Epoch 710, val loss: 1.0040794610977173
Epoch 720, training loss: 0.013182828202843666 = 0.006155975628644228 + 0.001 * 7.026851654052734
Epoch 720, val loss: 1.0094069242477417
Epoch 730, training loss: 0.012882139533758163 = 0.005890834145247936 + 0.001 * 6.991304397583008
Epoch 730, val loss: 1.0146163702011108
Epoch 740, training loss: 0.012634748592972755 = 0.005643710494041443 + 0.001 * 6.991037845611572
Epoch 740, val loss: 1.0196895599365234
Epoch 750, training loss: 0.012407698668539524 = 0.005413058213889599 + 0.001 * 6.994640350341797
Epoch 750, val loss: 1.0246338844299316
Epoch 760, training loss: 0.012192429974675179 = 0.005197442602366209 + 0.001 * 6.994986534118652
Epoch 760, val loss: 1.0294350385665894
Epoch 770, training loss: 0.011995425447821617 = 0.00499561196193099 + 0.001 * 6.999813079833984
Epoch 770, val loss: 1.0341272354125977
Epoch 780, training loss: 0.011803111992776394 = 0.004806381184607744 + 0.001 * 6.996730327606201
Epoch 780, val loss: 1.0386886596679688
Epoch 790, training loss: 0.01162484847009182 = 0.004628791473805904 + 0.001 * 6.996056079864502
Epoch 790, val loss: 1.0431469678878784
Epoch 800, training loss: 0.011449519544839859 = 0.004461875185370445 + 0.001 * 6.987644195556641
Epoch 800, val loss: 1.047499656677246
Epoch 810, training loss: 0.011283360421657562 = 0.004304811824113131 + 0.001 * 6.978548526763916
Epoch 810, val loss: 1.0517380237579346
Epoch 820, training loss: 0.01114325225353241 = 0.004156844224780798 + 0.001 * 6.986407279968262
Epoch 820, val loss: 1.0558795928955078
Epoch 830, training loss: 0.010993571020662785 = 0.004017295315861702 + 0.001 * 6.976275444030762
Epoch 830, val loss: 1.0599137544631958
Epoch 840, training loss: 0.010875418782234192 = 0.003885546699166298 + 0.001 * 6.989871978759766
Epoch 840, val loss: 1.0638954639434814
Epoch 850, training loss: 0.010756676085293293 = 0.0037610125727951527 + 0.001 * 6.995663166046143
Epoch 850, val loss: 1.0677738189697266
Epoch 860, training loss: 0.010620375163853168 = 0.0036432084161788225 + 0.001 * 6.977166652679443
Epoch 860, val loss: 1.071560263633728
Epoch 870, training loss: 0.010500183328986168 = 0.0035315928980708122 + 0.001 * 6.968590259552002
Epoch 870, val loss: 1.0752626657485962
Epoch 880, training loss: 0.010407639667391777 = 0.0034257599618285894 + 0.001 * 6.981879234313965
Epoch 880, val loss: 1.0788586139678955
Epoch 890, training loss: 0.01029230561107397 = 0.003325331024825573 + 0.001 * 6.966974258422852
Epoch 890, val loss: 1.082390546798706
Epoch 900, training loss: 0.010199928656220436 = 0.003229923080652952 + 0.001 * 6.970005035400391
Epoch 900, val loss: 1.085839867591858
Epoch 910, training loss: 0.010102227330207825 = 0.0031392360106110573 + 0.001 * 6.962990760803223
Epoch 910, val loss: 1.0892068147659302
Epoch 920, training loss: 0.010009894147515297 = 0.0030529554933309555 + 0.001 * 6.956938743591309
Epoch 920, val loss: 1.0924967527389526
Epoch 930, training loss: 0.009931037202477455 = 0.0029708005022257566 + 0.001 * 6.960236549377441
Epoch 930, val loss: 1.0957173109054565
Epoch 940, training loss: 0.009860978461802006 = 0.0028925163205713034 + 0.001 * 6.968461513519287
Epoch 940, val loss: 1.0988777875900269
Epoch 950, training loss: 0.009796529076993465 = 0.002817870583385229 + 0.001 * 6.978658199310303
Epoch 950, val loss: 1.1020464897155762
Epoch 960, training loss: 0.009722733870148659 = 0.0027466267347335815 + 0.001 * 6.976106643676758
Epoch 960, val loss: 1.105075716972351
Epoch 970, training loss: 0.00964055210351944 = 0.002678597578778863 + 0.001 * 6.961954116821289
Epoch 970, val loss: 1.1080455780029297
Epoch 980, training loss: 0.00956258736550808 = 0.0026135523803532124 + 0.001 * 6.949034690856934
Epoch 980, val loss: 1.1109583377838135
Epoch 990, training loss: 0.009507485665380955 = 0.0025513169821351767 + 0.001 * 6.9561686515808105
Epoch 990, val loss: 1.1137988567352295
Epoch 1000, training loss: 0.009441439062356949 = 0.002491690218448639 + 0.001 * 6.9497480392456055
Epoch 1000, val loss: 1.1165615320205688
Epoch 1010, training loss: 0.009408324025571346 = 0.0024344725534319878 + 0.001 * 6.973851203918457
Epoch 1010, val loss: 1.1193126440048218
Epoch 1020, training loss: 0.009344012476503849 = 0.002379458164796233 + 0.001 * 6.964554309844971
Epoch 1020, val loss: 1.1220099925994873
Epoch 1030, training loss: 0.009275184944272041 = 0.002326349727809429 + 0.001 * 6.948834419250488
Epoch 1030, val loss: 1.124642014503479
Epoch 1040, training loss: 0.009217369370162487 = 0.0022748077753931284 + 0.001 * 6.942561626434326
Epoch 1040, val loss: 1.127246379852295
Epoch 1050, training loss: 0.009168813936412334 = 0.002224529627710581 + 0.001 * 6.944283962249756
Epoch 1050, val loss: 1.129823088645935
Epoch 1060, training loss: 0.009123132564127445 = 0.0021752826869487762 + 0.001 * 6.947849750518799
Epoch 1060, val loss: 1.1323599815368652
Epoch 1070, training loss: 0.009057432413101196 = 0.002126842737197876 + 0.001 * 6.930588722229004
Epoch 1070, val loss: 1.1349347829818726
Epoch 1080, training loss: 0.009027178399264812 = 0.0020790761336684227 + 0.001 * 6.948101997375488
Epoch 1080, val loss: 1.137444019317627
Epoch 1090, training loss: 0.008975167758762836 = 0.002031936775892973 + 0.001 * 6.943230628967285
Epoch 1090, val loss: 1.140051007270813
Epoch 1100, training loss: 0.008930995129048824 = 0.0019853233825415373 + 0.001 * 6.945671558380127
Epoch 1100, val loss: 1.1426079273223877
Epoch 1110, training loss: 0.008889606222510338 = 0.0019393085967749357 + 0.001 * 6.9502973556518555
Epoch 1110, val loss: 1.1452475786209106
Epoch 1120, training loss: 0.008829797618091106 = 0.0018937973072752357 + 0.001 * 6.936000347137451
Epoch 1120, val loss: 1.1478742361068726
Epoch 1130, training loss: 0.00877403374761343 = 0.0018488637870177627 + 0.001 * 6.925169467926025
Epoch 1130, val loss: 1.1505568027496338
Epoch 1140, training loss: 0.008731098845601082 = 0.0018046889454126358 + 0.001 * 6.92641019821167
Epoch 1140, val loss: 1.1532632112503052
Epoch 1150, training loss: 0.008694701828062534 = 0.0017613951349630952 + 0.001 * 6.933306694030762
Epoch 1150, val loss: 1.1560156345367432
Epoch 1160, training loss: 0.008638106286525726 = 0.001719108666293323 + 0.001 * 6.918997287750244
Epoch 1160, val loss: 1.1587666273117065
Epoch 1170, training loss: 0.008604324422776699 = 0.001677903812378645 + 0.001 * 6.926420211791992
Epoch 1170, val loss: 1.1615474224090576
Epoch 1180, training loss: 0.008589358069002628 = 0.0016378307482227683 + 0.001 * 6.951527118682861
Epoch 1180, val loss: 1.1643062829971313
Epoch 1190, training loss: 0.008525569923222065 = 0.0015989824896678329 + 0.001 * 6.9265875816345215
Epoch 1190, val loss: 1.167001724243164
Epoch 1200, training loss: 0.008484830148518085 = 0.0015613826690241694 + 0.001 * 6.923447132110596
Epoch 1200, val loss: 1.1697248220443726
Epoch 1210, training loss: 0.008434119634330273 = 0.0015250917058438063 + 0.001 * 6.909028053283691
Epoch 1210, val loss: 1.1724441051483154
Epoch 1220, training loss: 0.008439319208264351 = 0.0014901035465300083 + 0.001 * 6.949214935302734
Epoch 1220, val loss: 1.1751222610473633
Epoch 1230, training loss: 0.008378077298402786 = 0.0014564255252480507 + 0.001 * 6.921651840209961
Epoch 1230, val loss: 1.1777479648590088
Epoch 1240, training loss: 0.008329739794135094 = 0.0014239575248211622 + 0.001 * 6.9057817459106445
Epoch 1240, val loss: 1.1803247928619385
Epoch 1250, training loss: 0.008295680396258831 = 0.0013927769614383578 + 0.001 * 6.902903079986572
Epoch 1250, val loss: 1.1827948093414307
Epoch 1260, training loss: 0.008272035047411919 = 0.001362804090604186 + 0.001 * 6.9092302322387695
Epoch 1260, val loss: 1.1853439807891846
Epoch 1270, training loss: 0.008246315643191338 = 0.0013340803561732173 + 0.001 * 6.912234783172607
Epoch 1270, val loss: 1.1878280639648438
Epoch 1280, training loss: 0.008210076950490475 = 0.0013064973754808307 + 0.001 * 6.9035797119140625
Epoch 1280, val loss: 1.1903201341629028
Epoch 1290, training loss: 0.0081810113042593 = 0.001279982621781528 + 0.001 * 6.901028633117676
Epoch 1290, val loss: 1.1925945281982422
Epoch 1300, training loss: 0.008153806440532207 = 0.001254507340490818 + 0.001 * 6.899298667907715
Epoch 1300, val loss: 1.1949468851089478
Epoch 1310, training loss: 0.008130825124680996 = 0.00123005046043545 + 0.001 * 6.900774002075195
Epoch 1310, val loss: 1.1972696781158447
Epoch 1320, training loss: 0.008112765848636627 = 0.0012065491173416376 + 0.001 * 6.906216621398926
Epoch 1320, val loss: 1.199510097503662
Epoch 1330, training loss: 0.008078765124082565 = 0.0011839671060442924 + 0.001 * 6.8947978019714355
Epoch 1330, val loss: 1.2017333507537842
Epoch 1340, training loss: 0.008061932399868965 = 0.0011623569298535585 + 0.001 * 6.8995747566223145
Epoch 1340, val loss: 1.2039068937301636
Epoch 1350, training loss: 0.008034741505980492 = 0.0011416006600484252 + 0.001 * 6.89314079284668
Epoch 1350, val loss: 1.205979824066162
Epoch 1360, training loss: 0.008030213415622711 = 0.0011216269340366125 + 0.001 * 6.908586502075195
Epoch 1360, val loss: 1.2079914808273315
Epoch 1370, training loss: 0.007999076507985592 = 0.001102439477108419 + 0.001 * 6.896636962890625
Epoch 1370, val loss: 1.2099323272705078
Epoch 1380, training loss: 0.00798862986266613 = 0.0010839949827641249 + 0.001 * 6.904634475708008
Epoch 1380, val loss: 1.2120097875595093
Epoch 1390, training loss: 0.007991401478648186 = 0.001066260039806366 + 0.001 * 6.925141334533691
Epoch 1390, val loss: 1.2137593030929565
Epoch 1400, training loss: 0.00793898943811655 = 0.001049183076247573 + 0.001 * 6.889806270599365
Epoch 1400, val loss: 1.215701699256897
Epoch 1410, training loss: 0.007935043424367905 = 0.0010327311465516686 + 0.001 * 6.9023118019104
Epoch 1410, val loss: 1.2175151109695435
Epoch 1420, training loss: 0.007905788719654083 = 0.0010169277666136622 + 0.001 * 6.888860702514648
Epoch 1420, val loss: 1.2192597389221191
Epoch 1430, training loss: 0.007883747108280659 = 0.0010017423192039132 + 0.001 * 6.882004737854004
Epoch 1430, val loss: 1.2209728956222534
Epoch 1440, training loss: 0.007857195101678371 = 0.000987109960988164 + 0.001 * 6.8700852394104
Epoch 1440, val loss: 1.222699761390686
Epoch 1450, training loss: 0.007861163467168808 = 0.0009729990852065384 + 0.001 * 6.888164520263672
Epoch 1450, val loss: 1.2242956161499023
Epoch 1460, training loss: 0.00782429613173008 = 0.0009594016592018306 + 0.001 * 6.864894390106201
Epoch 1460, val loss: 1.2259571552276611
Epoch 1470, training loss: 0.00782496016472578 = 0.0009463037131354213 + 0.001 * 6.878656387329102
Epoch 1470, val loss: 1.2274644374847412
Epoch 1480, training loss: 0.0078020766377449036 = 0.0009336594957858324 + 0.001 * 6.868417263031006
Epoch 1480, val loss: 1.2290209531784058
Epoch 1490, training loss: 0.0078115141950547695 = 0.0009214404853992164 + 0.001 * 6.890073776245117
Epoch 1490, val loss: 1.2305203676223755
Epoch 1500, training loss: 0.007774076424539089 = 0.0009096760768443346 + 0.001 * 6.864400386810303
Epoch 1500, val loss: 1.2319579124450684
Epoch 1510, training loss: 0.007792464457452297 = 0.0008982965955510736 + 0.001 * 6.894167423248291
Epoch 1510, val loss: 1.2334140539169312
Epoch 1520, training loss: 0.007757408544421196 = 0.0008873101905919611 + 0.001 * 6.870098114013672
Epoch 1520, val loss: 1.2347286939620972
Epoch 1530, training loss: 0.0077492231503129005 = 0.0008766811806708574 + 0.001 * 6.872541904449463
Epoch 1530, val loss: 1.2361589670181274
Epoch 1540, training loss: 0.007773234974592924 = 0.0008664215565659106 + 0.001 * 6.906813144683838
Epoch 1540, val loss: 1.2374598979949951
Epoch 1550, training loss: 0.007708266843110323 = 0.0008564948220737278 + 0.001 * 6.851771354675293
Epoch 1550, val loss: 1.2387584447860718
Epoch 1560, training loss: 0.0077075413428246975 = 0.0008468941668979824 + 0.001 * 6.860647201538086
Epoch 1560, val loss: 1.239932656288147
Epoch 1570, training loss: 0.00769036915153265 = 0.0008376154000870883 + 0.001 * 6.852753639221191
Epoch 1570, val loss: 1.2412058115005493
Epoch 1580, training loss: 0.007685029413551092 = 0.0008286231895908713 + 0.001 * 6.856405735015869
Epoch 1580, val loss: 1.242390513420105
Epoch 1590, training loss: 0.007673701271414757 = 0.0008199461153708398 + 0.001 * 6.853754997253418
Epoch 1590, val loss: 1.2435613870620728
Epoch 1600, training loss: 0.007659692782908678 = 0.0008115124655887485 + 0.001 * 6.848179817199707
Epoch 1600, val loss: 1.244665503501892
Epoch 1610, training loss: 0.007669515907764435 = 0.0008033587364479899 + 0.001 * 6.866156578063965
Epoch 1610, val loss: 1.2457859516143799
Epoch 1620, training loss: 0.007664921227842569 = 0.0007954706670716405 + 0.001 * 6.869450569152832
Epoch 1620, val loss: 1.2468751668930054
Epoch 1630, training loss: 0.007628664840012789 = 0.0007878023898229003 + 0.001 * 6.840861797332764
Epoch 1630, val loss: 1.2479743957519531
Epoch 1640, training loss: 0.007632918655872345 = 0.0007803792832419276 + 0.001 * 6.8525390625
Epoch 1640, val loss: 1.2489198446273804
Epoch 1650, training loss: 0.007612286601215601 = 0.0007731965160928667 + 0.001 * 6.839089870452881
Epoch 1650, val loss: 1.2500100135803223
Epoch 1660, training loss: 0.007605289574712515 = 0.0007662385469302535 + 0.001 * 6.839050769805908
Epoch 1660, val loss: 1.250936508178711
Epoch 1670, training loss: 0.007597540505230427 = 0.0007594914641231298 + 0.001 * 6.838048934936523
Epoch 1670, val loss: 1.2518798112869263
Epoch 1680, training loss: 0.00758421840146184 = 0.0007529372232966125 + 0.001 * 6.8312811851501465
Epoch 1680, val loss: 1.2528860569000244
Epoch 1690, training loss: 0.00758043397217989 = 0.0007466021343134344 + 0.001 * 6.833831310272217
Epoch 1690, val loss: 1.2537175416946411
Epoch 1700, training loss: 0.007605566177517176 = 0.0007404395728372037 + 0.001 * 6.865126132965088
Epoch 1700, val loss: 1.2545639276504517
Epoch 1710, training loss: 0.007580938749015331 = 0.0007344392943195999 + 0.001 * 6.846499443054199
Epoch 1710, val loss: 1.2554740905761719
Epoch 1720, training loss: 0.0075677125714719296 = 0.0007286139880307019 + 0.001 * 6.83909797668457
Epoch 1720, val loss: 1.256330966949463
Epoch 1730, training loss: 0.007579687982797623 = 0.0007229606853798032 + 0.001 * 6.856727123260498
Epoch 1730, val loss: 1.2570854425430298
Epoch 1740, training loss: 0.007580653764307499 = 0.0007174744387157261 + 0.001 * 6.863178730010986
Epoch 1740, val loss: 1.2579010725021362
Epoch 1750, training loss: 0.007554333191365004 = 0.0007121177623048425 + 0.001 * 6.842215061187744
Epoch 1750, val loss: 1.2586898803710938
Epoch 1760, training loss: 0.007532481104135513 = 0.0007069066632539034 + 0.001 * 6.8255743980407715
Epoch 1760, val loss: 1.2593976259231567
Epoch 1770, training loss: 0.007552461698651314 = 0.0007018277538008988 + 0.001 * 6.85063362121582
Epoch 1770, val loss: 1.2602685689926147
Epoch 1780, training loss: 0.007534728851169348 = 0.0006968840607441962 + 0.001 * 6.837844371795654
Epoch 1780, val loss: 1.2609182596206665
Epoch 1790, training loss: 0.007504218723624945 = 0.000692083325702697 + 0.001 * 6.812134742736816
Epoch 1790, val loss: 1.2615936994552612
Epoch 1800, training loss: 0.007527315057814121 = 0.0006874188547953963 + 0.001 * 6.839895725250244
Epoch 1800, val loss: 1.262312889099121
Epoch 1810, training loss: 0.0075589437037706375 = 0.0006828628829680383 + 0.001 * 6.876080513000488
Epoch 1810, val loss: 1.2630252838134766
Epoch 1820, training loss: 0.0075010028667747974 = 0.000678445678204298 + 0.001 * 6.822556972503662
Epoch 1820, val loss: 1.2636773586273193
Epoch 1830, training loss: 0.007511679548770189 = 0.0006741394172422588 + 0.001 * 6.837540149688721
Epoch 1830, val loss: 1.2643083333969116
Epoch 1840, training loss: 0.007492346689105034 = 0.0006699483492411673 + 0.001 * 6.8223981857299805
Epoch 1840, val loss: 1.2648942470550537
Epoch 1850, training loss: 0.00749453529715538 = 0.0006658583879470825 + 0.001 * 6.828676700592041
Epoch 1850, val loss: 1.2655433416366577
Epoch 1860, training loss: 0.0074916137382388115 = 0.0006618723273277283 + 0.001 * 6.82974100112915
Epoch 1860, val loss: 1.2661349773406982
Epoch 1870, training loss: 0.007510792929679155 = 0.000657981785479933 + 0.001 * 6.852810859680176
Epoch 1870, val loss: 1.2667484283447266
Epoch 1880, training loss: 0.0074638063088059425 = 0.0006541917682625353 + 0.001 * 6.809614181518555
Epoch 1880, val loss: 1.2672380208969116
Epoch 1890, training loss: 0.007469438016414642 = 0.0006504969205707312 + 0.001 * 6.81894063949585
Epoch 1890, val loss: 1.2678664922714233
Epoch 1900, training loss: 0.007497848477214575 = 0.0006468889187090099 + 0.001 * 6.850959300994873
Epoch 1900, val loss: 1.2683894634246826
Epoch 1910, training loss: 0.007452569901943207 = 0.0006433700327761471 + 0.001 * 6.809199333190918
Epoch 1910, val loss: 1.268897294998169
Epoch 1920, training loss: 0.00746974628418684 = 0.0006399148260243237 + 0.001 * 6.829831123352051
Epoch 1920, val loss: 1.2693310976028442
Epoch 1930, training loss: 0.0074510048143565655 = 0.0006365428562276065 + 0.001 * 6.814461708068848
Epoch 1930, val loss: 1.2698954343795776
Epoch 1940, training loss: 0.007442611735314131 = 0.0006332453922368586 + 0.001 * 6.809366226196289
Epoch 1940, val loss: 1.2703584432601929
Epoch 1950, training loss: 0.007450399454683065 = 0.000630015681963414 + 0.001 * 6.820383548736572
Epoch 1950, val loss: 1.270780086517334
Epoch 1960, training loss: 0.007443638984113932 = 0.0006268502911552787 + 0.001 * 6.816788196563721
Epoch 1960, val loss: 1.2712743282318115
Epoch 1970, training loss: 0.007415961008518934 = 0.0006237393245100975 + 0.001 * 6.792221546173096
Epoch 1970, val loss: 1.2717337608337402
Epoch 1980, training loss: 0.007423812989145517 = 0.0006207149126566947 + 0.001 * 6.803097724914551
Epoch 1980, val loss: 1.2722440958023071
Epoch 1990, training loss: 0.007429648190736771 = 0.0006177534814924002 + 0.001 * 6.811893939971924
Epoch 1990, val loss: 1.2726318836212158
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 1.9618831872940063 = 1.9532862901687622 + 0.001 * 8.5968599319458
Epoch 0, val loss: 1.9601677656173706
Epoch 10, training loss: 1.9512871503829956 = 1.942690372467041 + 0.001 * 8.596792221069336
Epoch 10, val loss: 1.9489370584487915
Epoch 20, training loss: 1.9384377002716064 = 1.929841160774231 + 0.001 * 8.596575736999512
Epoch 20, val loss: 1.934936285018921
Epoch 30, training loss: 1.9208993911743164 = 1.9123034477233887 + 0.001 * 8.59597396850586
Epoch 30, val loss: 1.9156763553619385
Epoch 40, training loss: 1.8955862522125244 = 1.88699209690094 + 0.001 * 8.594185829162598
Epoch 40, val loss: 1.8881123065948486
Epoch 50, training loss: 1.8603644371032715 = 1.8517765998840332 + 0.001 * 8.587860107421875
Epoch 50, val loss: 1.8512638807296753
Epoch 60, training loss: 1.8197729587554932 = 1.8112107515335083 + 0.001 * 8.562195777893066
Epoch 60, val loss: 1.812770962715149
Epoch 70, training loss: 1.7854013442993164 = 1.776949405670166 + 0.001 * 8.451891899108887
Epoch 70, val loss: 1.7845724821090698
Epoch 80, training loss: 1.7482799291610718 = 1.7400325536727905 + 0.001 * 8.247380256652832
Epoch 80, val loss: 1.7529007196426392
Epoch 90, training loss: 1.6962623596191406 = 1.6881828308105469 + 0.001 * 8.079578399658203
Epoch 90, val loss: 1.7068558931350708
Epoch 100, training loss: 1.6253825426101685 = 1.6175692081451416 + 0.001 * 7.813323497772217
Epoch 100, val loss: 1.6445640325546265
Epoch 110, training loss: 1.5362670421600342 = 1.528646469116211 + 0.001 * 7.620543956756592
Epoch 110, val loss: 1.567861557006836
Epoch 120, training loss: 1.4380747079849243 = 1.430479884147644 + 0.001 * 7.594769477844238
Epoch 120, val loss: 1.4842509031295776
Epoch 130, training loss: 1.3371866941452026 = 1.32962167263031 + 0.001 * 7.565058708190918
Epoch 130, val loss: 1.3997206687927246
Epoch 140, training loss: 1.2322860956192017 = 1.224750280380249 + 0.001 * 7.5357561111450195
Epoch 140, val loss: 1.3118867874145508
Epoch 150, training loss: 1.1239229440689087 = 1.1164332628250122 + 0.001 * 7.489641189575195
Epoch 150, val loss: 1.2220500707626343
Epoch 160, training loss: 1.0166093111038208 = 1.0091837644577026 + 0.001 * 7.425576210021973
Epoch 160, val loss: 1.1336179971694946
Epoch 170, training loss: 0.9166629910469055 = 0.9092906713485718 + 0.001 * 7.372291564941406
Epoch 170, val loss: 1.0534107685089111
Epoch 180, training loss: 0.8280364871025085 = 0.820690929889679 + 0.001 * 7.345562934875488
Epoch 180, val loss: 0.9857210516929626
Epoch 190, training loss: 0.7511889934539795 = 0.7438617944717407 + 0.001 * 7.327192783355713
Epoch 190, val loss: 0.931335985660553
Epoch 200, training loss: 0.6841070652008057 = 0.6767899394035339 + 0.001 * 7.317103385925293
Epoch 200, val loss: 0.8889209628105164
Epoch 210, training loss: 0.623979926109314 = 0.6166719198226929 + 0.001 * 7.308006286621094
Epoch 210, val loss: 0.8560166954994202
Epoch 220, training loss: 0.5687679052352905 = 0.5614693760871887 + 0.001 * 7.298537254333496
Epoch 220, val loss: 0.8307426571846008
Epoch 230, training loss: 0.5177275538444519 = 0.5104333162307739 + 0.001 * 7.294208526611328
Epoch 230, val loss: 0.8119527101516724
Epoch 240, training loss: 0.47055163979530334 = 0.46327054500579834 + 0.001 * 7.281095504760742
Epoch 240, val loss: 0.7987530827522278
Epoch 250, training loss: 0.42701178789138794 = 0.419742226600647 + 0.001 * 7.269562721252441
Epoch 250, val loss: 0.79022616147995
Epoch 260, training loss: 0.3868902325630188 = 0.37963342666625977 + 0.001 * 7.256800651550293
Epoch 260, val loss: 0.7860698699951172
Epoch 270, training loss: 0.3500494360923767 = 0.34280717372894287 + 0.001 * 7.242259502410889
Epoch 270, val loss: 0.7864113450050354
Epoch 280, training loss: 0.3162711262702942 = 0.30904850363731384 + 0.001 * 7.222616672515869
Epoch 280, val loss: 0.7910240292549133
Epoch 290, training loss: 0.28526532649993896 = 0.27804872393608093 + 0.001 * 7.2165961265563965
Epoch 290, val loss: 0.7993854284286499
Epoch 300, training loss: 0.25670021772384644 = 0.2495075762271881 + 0.001 * 7.192626476287842
Epoch 300, val loss: 0.8111414313316345
Epoch 310, training loss: 0.2304013967514038 = 0.2232157588005066 + 0.001 * 7.185634613037109
Epoch 310, val loss: 0.8259249925613403
Epoch 320, training loss: 0.20624609291553497 = 0.19907543063163757 + 0.001 * 7.17066764831543
Epoch 320, val loss: 0.8435041308403015
Epoch 330, training loss: 0.1842155158519745 = 0.17704442143440247 + 0.001 * 7.1710944175720215
Epoch 330, val loss: 0.8635010123252869
Epoch 340, training loss: 0.16426825523376465 = 0.15710510313510895 + 0.001 * 7.163147926330566
Epoch 340, val loss: 0.8856236934661865
Epoch 350, training loss: 0.1463780552148819 = 0.1392153948545456 + 0.001 * 7.1626667976379395
Epoch 350, val loss: 0.9094546437263489
Epoch 360, training loss: 0.13046708703041077 = 0.12330689281225204 + 0.001 * 7.160186290740967
Epoch 360, val loss: 0.9347257614135742
Epoch 370, training loss: 0.11645295470952988 = 0.10929230600595474 + 0.001 * 7.160645008087158
Epoch 370, val loss: 0.9609624743461609
Epoch 380, training loss: 0.10419357568025589 = 0.09703139960765839 + 0.001 * 7.162173271179199
Epoch 380, val loss: 0.9878103733062744
Epoch 390, training loss: 0.0935351699590683 = 0.08636833727359772 + 0.001 * 7.166830539703369
Epoch 390, val loss: 1.0148208141326904
Epoch 400, training loss: 0.08428367972373962 = 0.07712103426456451 + 0.001 * 7.162642002105713
Epoch 400, val loss: 1.0416486263275146
Epoch 410, training loss: 0.07625923305749893 = 0.06910213083028793 + 0.001 * 7.157102584838867
Epoch 410, val loss: 1.0680629014968872
Epoch 420, training loss: 0.0692879855632782 = 0.06213032081723213 + 0.001 * 7.157663345336914
Epoch 420, val loss: 1.0938884019851685
Epoch 430, training loss: 0.06320475041866302 = 0.05604678392410278 + 0.001 * 7.157963752746582
Epoch 430, val loss: 1.1191450357437134
Epoch 440, training loss: 0.057869959622621536 = 0.05071447417140007 + 0.001 * 7.155486583709717
Epoch 440, val loss: 1.143804669380188
Epoch 450, training loss: 0.053180426359176636 = 0.04602295160293579 + 0.001 * 7.157473087310791
Epoch 450, val loss: 1.1677435636520386
Epoch 460, training loss: 0.04904497042298317 = 0.04188360646367073 + 0.001 * 7.161362648010254
Epoch 460, val loss: 1.1909968852996826
Epoch 470, training loss: 0.04537461698055267 = 0.038221124559640884 + 0.001 * 7.153493404388428
Epoch 470, val loss: 1.2135396003723145
Epoch 480, training loss: 0.04212367534637451 = 0.034971415996551514 + 0.001 * 7.152259826660156
Epoch 480, val loss: 1.2353891134262085
Epoch 490, training loss: 0.03923966363072395 = 0.03207573667168617 + 0.001 * 7.163928031921387
Epoch 490, val loss: 1.2566266059875488
Epoch 500, training loss: 0.03664153441786766 = 0.029484832659363747 + 0.001 * 7.156702995300293
Epoch 500, val loss: 1.2773890495300293
Epoch 510, training loss: 0.03431175649166107 = 0.027157139033079147 + 0.001 * 7.154617786407471
Epoch 510, val loss: 1.2977063655853271
Epoch 520, training loss: 0.032209835946559906 = 0.02506076544523239 + 0.001 * 7.149072170257568
Epoch 520, val loss: 1.3175938129425049
Epoch 530, training loss: 0.030334139242768288 = 0.02317015454173088 + 0.001 * 7.163984298706055
Epoch 530, val loss: 1.3370195627212524
Epoch 540, training loss: 0.02861087955534458 = 0.021463381126523018 + 0.001 * 7.14749813079834
Epoch 540, val loss: 1.3559892177581787
Epoch 550, training loss: 0.027065452188253403 = 0.01992064341902733 + 0.001 * 7.144808769226074
Epoch 550, val loss: 1.374451994895935
Epoch 560, training loss: 0.025680705904960632 = 0.018524538725614548 + 0.001 * 7.156167507171631
Epoch 560, val loss: 1.3923662900924683
Epoch 570, training loss: 0.024408238008618355 = 0.017259621992707253 + 0.001 * 7.14861536026001
Epoch 570, val loss: 1.409873127937317
Epoch 580, training loss: 0.023253321647644043 = 0.016111962497234344 + 0.001 * 7.141358375549316
Epoch 580, val loss: 1.4268330335617065
Epoch 590, training loss: 0.02221362665295601 = 0.01506904698908329 + 0.001 * 7.144580364227295
Epoch 590, val loss: 1.4433131217956543
Epoch 600, training loss: 0.02125181071460247 = 0.014119921252131462 + 0.001 * 7.131889343261719
Epoch 600, val loss: 1.4592849016189575
Epoch 610, training loss: 0.020420057699084282 = 0.013254666700959206 + 0.001 * 7.165390968322754
Epoch 610, val loss: 1.4747960567474365
Epoch 620, training loss: 0.019598830491304398 = 0.012464560568332672 + 0.001 * 7.134270668029785
Epoch 620, val loss: 1.4898524284362793
Epoch 630, training loss: 0.018875684589147568 = 0.011741618625819683 + 0.001 * 7.1340651512146
Epoch 630, val loss: 1.5044454336166382
Epoch 640, training loss: 0.01820477657020092 = 0.011078985407948494 + 0.001 * 7.125791072845459
Epoch 640, val loss: 1.5186214447021484
Epoch 650, training loss: 0.017598804086446762 = 0.010470656678080559 + 0.001 * 7.128148078918457
Epoch 650, val loss: 1.5323625802993774
Epoch 660, training loss: 0.017032481729984283 = 0.009911270812153816 + 0.001 * 7.121211051940918
Epoch 660, val loss: 1.5457229614257812
Epoch 670, training loss: 0.016526715829968452 = 0.00939595140516758 + 0.001 * 7.130764007568359
Epoch 670, val loss: 1.5586931705474854
Epoch 680, training loss: 0.016054507344961166 = 0.008920329622924328 + 0.001 * 7.134176731109619
Epoch 680, val loss: 1.571303129196167
Epoch 690, training loss: 0.015608598478138447 = 0.008480693213641644 + 0.001 * 7.127904891967773
Epoch 690, val loss: 1.5835342407226562
Epoch 700, training loss: 0.015196790918707848 = 0.008073617704212666 + 0.001 * 7.123172283172607
Epoch 700, val loss: 1.595441222190857
Epoch 710, training loss: 0.014819467440247536 = 0.007696081884205341 + 0.001 * 7.123385906219482
Epoch 710, val loss: 1.6069705486297607
Epoch 720, training loss: 0.01445173379033804 = 0.007345424499362707 + 0.001 * 7.106308937072754
Epoch 720, val loss: 1.6181782484054565
Epoch 730, training loss: 0.01413840800523758 = 0.0070191542617976665 + 0.001 * 7.119253158569336
Epoch 730, val loss: 1.6290762424468994
Epoch 740, training loss: 0.013827672228217125 = 0.0067152054980397224 + 0.001 * 7.112466335296631
Epoch 740, val loss: 1.6396664381027222
Epoch 750, training loss: 0.013536341488361359 = 0.006431657820940018 + 0.001 * 7.104683876037598
Epoch 750, val loss: 1.6499742269515991
Epoch 760, training loss: 0.013263262808322906 = 0.006166711449623108 + 0.001 * 7.096550941467285
Epoch 760, val loss: 1.6599899530410767
Epoch 770, training loss: 0.013017028570175171 = 0.005918846931308508 + 0.001 * 7.09818172454834
Epoch 770, val loss: 1.669732928276062
Epoch 780, training loss: 0.012799855321645737 = 0.005686473101377487 + 0.001 * 7.113381862640381
Epoch 780, val loss: 1.6792720556259155
Epoch 790, training loss: 0.012575270608067513 = 0.005468131508678198 + 0.001 * 7.1071391105651855
Epoch 790, val loss: 1.6885098218917847
Epoch 800, training loss: 0.012355070561170578 = 0.005261755082756281 + 0.001 * 7.093315124511719
Epoch 800, val loss: 1.6975598335266113
Epoch 810, training loss: 0.012154003605246544 = 0.005065638571977615 + 0.001 * 7.088365077972412
Epoch 810, val loss: 1.7064522504806519
Epoch 820, training loss: 0.011983240023255348 = 0.004878401756286621 + 0.001 * 7.1048383712768555
Epoch 820, val loss: 1.7151927947998047
Epoch 830, training loss: 0.011783277615904808 = 0.004699500277638435 + 0.001 * 7.083776473999023
Epoch 830, val loss: 1.7238177061080933
Epoch 840, training loss: 0.01160566695034504 = 0.004528510384261608 + 0.001 * 7.077155590057373
Epoch 840, val loss: 1.732310175895691
Epoch 850, training loss: 0.011465644463896751 = 0.004365458618849516 + 0.001 * 7.100185394287109
Epoch 850, val loss: 1.7407052516937256
Epoch 860, training loss: 0.011302286759018898 = 0.004210349638015032 + 0.001 * 7.0919365882873535
Epoch 860, val loss: 1.7489360570907593
Epoch 870, training loss: 0.0111361313611269 = 0.00406278669834137 + 0.001 * 7.073343753814697
Epoch 870, val loss: 1.7570457458496094
Epoch 880, training loss: 0.01100892759859562 = 0.003922571893781424 + 0.001 * 7.086355686187744
Epoch 880, val loss: 1.765015721321106
Epoch 890, training loss: 0.01088701281696558 = 0.0037896193098276854 + 0.001 * 7.09739351272583
Epoch 890, val loss: 1.7728372812271118
Epoch 900, training loss: 0.010745870880782604 = 0.0036634861025959253 + 0.001 * 7.0823845863342285
Epoch 900, val loss: 1.780444860458374
Epoch 910, training loss: 0.010600940324366093 = 0.003543802769854665 + 0.001 * 7.057137489318848
Epoch 910, val loss: 1.787927269935608
Epoch 920, training loss: 0.010482046753168106 = 0.003430199110880494 + 0.001 * 7.051846981048584
Epoch 920, val loss: 1.7952367067337036
Epoch 930, training loss: 0.010374736972153187 = 0.0033221987541764975 + 0.001 * 7.05253791809082
Epoch 930, val loss: 1.8023864030838013
Epoch 940, training loss: 0.010273866355419159 = 0.0032192105427384377 + 0.001 * 7.054655075073242
Epoch 940, val loss: 1.8093628883361816
Epoch 950, training loss: 0.010175671428442001 = 0.003120808396488428 + 0.001 * 7.054862022399902
Epoch 950, val loss: 1.8161696195602417
Epoch 960, training loss: 0.01010675448924303 = 0.0030267357360571623 + 0.001 * 7.080018043518066
Epoch 960, val loss: 1.8227853775024414
Epoch 970, training loss: 0.010009069927036762 = 0.002936703385785222 + 0.001 * 7.072365760803223
Epoch 970, val loss: 1.8292508125305176
Epoch 980, training loss: 0.009903168305754662 = 0.0028503774665296078 + 0.001 * 7.052790641784668
Epoch 980, val loss: 1.8355381488800049
Epoch 990, training loss: 0.009803743101656437 = 0.002767747500911355 + 0.001 * 7.0359954833984375
Epoch 990, val loss: 1.8416906595230103
Epoch 1000, training loss: 0.0097628403455019 = 0.002688696375116706 + 0.001 * 7.074143409729004
Epoch 1000, val loss: 1.8476719856262207
Epoch 1010, training loss: 0.009663429111242294 = 0.00261298893019557 + 0.001 * 7.050439834594727
Epoch 1010, val loss: 1.8535088300704956
Epoch 1020, training loss: 0.009581356309354305 = 0.0025404824409633875 + 0.001 * 7.0408735275268555
Epoch 1020, val loss: 1.8591960668563843
Epoch 1030, training loss: 0.00951120164245367 = 0.002471216721460223 + 0.001 * 7.039984226226807
Epoch 1030, val loss: 1.864778757095337
Epoch 1040, training loss: 0.009468531236052513 = 0.0024049850180745125 + 0.001 * 7.0635457038879395
Epoch 1040, val loss: 1.8702096939086914
Epoch 1050, training loss: 0.009385466575622559 = 0.0023418681230396032 + 0.001 * 7.04359769821167
Epoch 1050, val loss: 1.87552011013031
Epoch 1060, training loss: 0.009312506765127182 = 0.0022815712727606297 + 0.001 * 7.030935287475586
Epoch 1060, val loss: 1.8806383609771729
Epoch 1070, training loss: 0.009248746559023857 = 0.002223894465714693 + 0.001 * 7.0248517990112305
Epoch 1070, val loss: 1.8856487274169922
Epoch 1080, training loss: 0.009192245081067085 = 0.0021688099950551987 + 0.001 * 7.023434638977051
Epoch 1080, val loss: 1.890541911125183
Epoch 1090, training loss: 0.009139242582023144 = 0.0021161381155252457 + 0.001 * 7.023104190826416
Epoch 1090, val loss: 1.8953394889831543
Epoch 1100, training loss: 0.00907776691019535 = 0.0020658359862864017 + 0.001 * 7.011929988861084
Epoch 1100, val loss: 1.9000102281570435
Epoch 1110, training loss: 0.009034823626279831 = 0.0020177976693958044 + 0.001 * 7.017025470733643
Epoch 1110, val loss: 1.9045473337173462
Epoch 1120, training loss: 0.009004674851894379 = 0.0019718448165804148 + 0.001 * 7.032829284667969
Epoch 1120, val loss: 1.9090040922164917
Epoch 1130, training loss: 0.008952955715358257 = 0.0019279478583484888 + 0.001 * 7.025007724761963
Epoch 1130, val loss: 1.9133481979370117
Epoch 1140, training loss: 0.008950358256697655 = 0.0018859211122617126 + 0.001 * 7.064436912536621
Epoch 1140, val loss: 1.9175727367401123
Epoch 1150, training loss: 0.008886168710887432 = 0.0018456747056916356 + 0.001 * 7.040493488311768
Epoch 1150, val loss: 1.92172372341156
Epoch 1160, training loss: 0.008815347217023373 = 0.0018071101512759924 + 0.001 * 7.008236408233643
Epoch 1160, val loss: 1.9257597923278809
Epoch 1170, training loss: 0.008782261051237583 = 0.0017701068427413702 + 0.001 * 7.012153625488281
Epoch 1170, val loss: 1.9297114610671997
Epoch 1180, training loss: 0.008748354390263557 = 0.0017346248496323824 + 0.001 * 7.013729572296143
Epoch 1180, val loss: 1.9335700273513794
Epoch 1190, training loss: 0.008726172149181366 = 0.0017005600966513157 + 0.001 * 7.025611877441406
Epoch 1190, val loss: 1.9373310804367065
Epoch 1200, training loss: 0.00867072306573391 = 0.0016679855762049556 + 0.001 * 7.002737522125244
Epoch 1200, val loss: 1.941024899482727
Epoch 1210, training loss: 0.008626208640635014 = 0.0016366502968594432 + 0.001 * 6.98955774307251
Epoch 1210, val loss: 1.944611668586731
Epoch 1220, training loss: 0.008629833348095417 = 0.0016064316732808948 + 0.001 * 7.023401260375977
Epoch 1220, val loss: 1.9481476545333862
Epoch 1230, training loss: 0.008590343408286572 = 0.0015775816282257438 + 0.001 * 7.01276159286499
Epoch 1230, val loss: 1.9516092538833618
Epoch 1240, training loss: 0.008524286560714245 = 0.0015497973654419184 + 0.001 * 6.974488735198975
Epoch 1240, val loss: 1.9549827575683594
Epoch 1250, training loss: 0.008522159419953823 = 0.0015230017015710473 + 0.001 * 6.999157428741455
Epoch 1250, val loss: 1.9582784175872803
Epoch 1260, training loss: 0.008463890291750431 = 0.0014971642522141337 + 0.001 * 6.966725826263428
Epoch 1260, val loss: 1.961506962776184
Epoch 1270, training loss: 0.00846312940120697 = 0.0014722603373229504 + 0.001 * 6.990868091583252
Epoch 1270, val loss: 1.9646331071853638
Epoch 1280, training loss: 0.008444240316748619 = 0.0014483596896752715 + 0.001 * 6.995880603790283
Epoch 1280, val loss: 1.9677250385284424
Epoch 1290, training loss: 0.008430886082351208 = 0.001425286172889173 + 0.001 * 7.005599498748779
Epoch 1290, val loss: 1.970732569694519
Epoch 1300, training loss: 0.008386960253119469 = 0.001403085421770811 + 0.001 * 6.983874320983887
Epoch 1300, val loss: 1.9737062454223633
Epoch 1310, training loss: 0.00834833923727274 = 0.001381609821692109 + 0.001 * 6.966729164123535
Epoch 1310, val loss: 1.9765607118606567
Epoch 1320, training loss: 0.008343270979821682 = 0.001360945519991219 + 0.001 * 6.982325077056885
Epoch 1320, val loss: 1.9793665409088135
Epoch 1330, training loss: 0.008344697766005993 = 0.0013409664388746023 + 0.001 * 7.003730773925781
Epoch 1330, val loss: 1.9820959568023682
Epoch 1340, training loss: 0.008275830186903477 = 0.0013217817759141326 + 0.001 * 6.954048156738281
Epoch 1340, val loss: 1.984785795211792
Epoch 1350, training loss: 0.008279130794107914 = 0.0013031744165346026 + 0.001 * 6.975956439971924
Epoch 1350, val loss: 1.987385869026184
Epoch 1360, training loss: 0.008246103301644325 = 0.0012851916253566742 + 0.001 * 6.960910797119141
Epoch 1360, val loss: 1.9899382591247559
Epoch 1370, training loss: 0.008251208811998367 = 0.0012678473722189665 + 0.001 * 6.983360767364502
Epoch 1370, val loss: 1.9924137592315674
Epoch 1380, training loss: 0.008210398256778717 = 0.0012510570231825113 + 0.001 * 6.959341049194336
Epoch 1380, val loss: 1.9948914051055908
Epoch 1390, training loss: 0.008201103657484055 = 0.0012348488671705127 + 0.001 * 6.966254234313965
Epoch 1390, val loss: 1.9972763061523438
Epoch 1400, training loss: 0.008165369741618633 = 0.0012191366404294968 + 0.001 * 6.946232795715332
Epoch 1400, val loss: 1.999627947807312
Epoch 1410, training loss: 0.008160131052136421 = 0.0012038975255563855 + 0.001 * 6.956233501434326
Epoch 1410, val loss: 2.0018973350524902
Epoch 1420, training loss: 0.00813599955290556 = 0.0011892527109012008 + 0.001 * 6.946746349334717
Epoch 1420, val loss: 2.0041403770446777
Epoch 1430, training loss: 0.008118715137243271 = 0.0011749843833968043 + 0.001 * 6.94373083114624
Epoch 1430, val loss: 2.00628662109375
Epoch 1440, training loss: 0.00813278742134571 = 0.0011612153612077236 + 0.001 * 6.971571922302246
Epoch 1440, val loss: 2.008478879928589
Epoch 1450, training loss: 0.008085023611783981 = 0.001147935981862247 + 0.001 * 6.937087535858154
Epoch 1450, val loss: 2.0105090141296387
Epoch 1460, training loss: 0.0080850999802351 = 0.0011349489213898778 + 0.001 * 6.950150489807129
Epoch 1460, val loss: 2.0125539302825928
Epoch 1470, training loss: 0.008153130300343037 = 0.0011224576737731695 + 0.001 * 7.030672073364258
Epoch 1470, val loss: 2.0145740509033203
Epoch 1480, training loss: 0.008055536076426506 = 0.0011103044962510467 + 0.001 * 6.9452314376831055
Epoch 1480, val loss: 2.0164713859558105
Epoch 1490, training loss: 0.008057554252445698 = 0.0010984297841787338 + 0.001 * 6.9591240882873535
Epoch 1490, val loss: 2.0183894634246826
Epoch 1500, training loss: 0.007999594323337078 = 0.0010870049009099603 + 0.001 * 6.912589073181152
Epoch 1500, val loss: 2.020228624343872
Epoch 1510, training loss: 0.008012935519218445 = 0.0010758626740425825 + 0.001 * 6.937072277069092
Epoch 1510, val loss: 2.022026538848877
Epoch 1520, training loss: 0.007996970787644386 = 0.0010650294134393334 + 0.001 * 6.93194055557251
Epoch 1520, val loss: 2.0238258838653564
Epoch 1530, training loss: 0.008013655431568623 = 0.0010545375989750028 + 0.001 * 6.9591169357299805
Epoch 1530, val loss: 2.025559902191162
Epoch 1540, training loss: 0.007951490581035614 = 0.0010443266946822405 + 0.001 * 6.907163619995117
Epoch 1540, val loss: 2.027256965637207
Epoch 1550, training loss: 0.007942444644868374 = 0.00103440519887954 + 0.001 * 6.90803861618042
Epoch 1550, val loss: 2.0289247035980225
Epoch 1560, training loss: 0.007943994365632534 = 0.00102478195913136 + 0.001 * 6.9192118644714355
Epoch 1560, val loss: 2.0305583477020264
Epoch 1570, training loss: 0.00793472956866026 = 0.0010154423071071506 + 0.001 * 6.919287204742432
Epoch 1570, val loss: 2.0321457386016846
Epoch 1580, training loss: 0.007916889153420925 = 0.0010063437512144446 + 0.001 * 6.910545349121094
Epoch 1580, val loss: 2.0336861610412598
Epoch 1590, training loss: 0.007915804162621498 = 0.000997487804852426 + 0.001 * 6.91831636428833
Epoch 1590, val loss: 2.0352296829223633
Epoch 1600, training loss: 0.00790792889893055 = 0.0009888423373922706 + 0.001 * 6.919086456298828
Epoch 1600, val loss: 2.0367283821105957
Epoch 1610, training loss: 0.007873336784541607 = 0.0009804900037124753 + 0.001 * 6.89284610748291
Epoch 1610, val loss: 2.038166046142578
Epoch 1620, training loss: 0.007882383652031422 = 0.0009723526309244335 + 0.001 * 6.910030841827393
Epoch 1620, val loss: 2.039613723754883
Epoch 1630, training loss: 0.007877327501773834 = 0.0009644023957662284 + 0.001 * 6.912924289703369
Epoch 1630, val loss: 2.0409891605377197
Epoch 1640, training loss: 0.007880655117332935 = 0.0009566291118972003 + 0.001 * 6.924025535583496
Epoch 1640, val loss: 2.0423741340637207
Epoch 1650, training loss: 0.007866114377975464 = 0.0009491469245404005 + 0.001 * 6.916966915130615
Epoch 1650, val loss: 2.0437042713165283
Epoch 1660, training loss: 0.007827682420611382 = 0.0009418000117875636 + 0.001 * 6.8858819007873535
Epoch 1660, val loss: 2.0450069904327393
Epoch 1670, training loss: 0.007834086194634438 = 0.0009346271399408579 + 0.001 * 6.899458408355713
Epoch 1670, val loss: 2.0462746620178223
Epoch 1680, training loss: 0.007821861654520035 = 0.000927653512917459 + 0.001 * 6.89420747756958
Epoch 1680, val loss: 2.047527313232422
Epoch 1690, training loss: 0.007816975936293602 = 0.0009208497940562665 + 0.001 * 6.8961262702941895
Epoch 1690, val loss: 2.048724412918091
Epoch 1700, training loss: 0.007797043304890394 = 0.0009142089984379709 + 0.001 * 6.882833957672119
Epoch 1700, val loss: 2.0499353408813477
Epoch 1710, training loss: 0.007806343957781792 = 0.000907793000806123 + 0.001 * 6.898550510406494
Epoch 1710, val loss: 2.051083564758301
Epoch 1720, training loss: 0.00782185047864914 = 0.0009014669922180474 + 0.001 * 6.920383453369141
Epoch 1720, val loss: 2.052208662033081
Epoch 1730, training loss: 0.007812418509274721 = 0.0008952988428063691 + 0.001 * 6.917119026184082
Epoch 1730, val loss: 2.0533523559570312
Epoch 1740, training loss: 0.007788625545799732 = 0.0008892905316315591 + 0.001 * 6.89933443069458
Epoch 1740, val loss: 2.0543994903564453
Epoch 1750, training loss: 0.0077632125467062 = 0.0008834663312882185 + 0.001 * 6.8797454833984375
Epoch 1750, val loss: 2.0554592609405518
Epoch 1760, training loss: 0.007771745789796114 = 0.0008777251350693405 + 0.001 * 6.8940205574035645
Epoch 1760, val loss: 2.0564916133880615
Epoch 1770, training loss: 0.007760579232126474 = 0.000872140284627676 + 0.001 * 6.888438701629639
Epoch 1770, val loss: 2.0574986934661865
Epoch 1780, training loss: 0.007776993326842785 = 0.0008667028159834445 + 0.001 * 6.910290241241455
Epoch 1780, val loss: 2.058504343032837
Epoch 1790, training loss: 0.007755758706480265 = 0.0008613335667178035 + 0.001 * 6.894424915313721
Epoch 1790, val loss: 2.0594465732574463
Epoch 1800, training loss: 0.007789403200149536 = 0.000856126775033772 + 0.001 * 6.933276176452637
Epoch 1800, val loss: 2.060401439666748
Epoch 1810, training loss: 0.007741337176412344 = 0.0008510435000061989 + 0.001 * 6.890293121337891
Epoch 1810, val loss: 2.0613033771514893
Epoch 1820, training loss: 0.007710963487625122 = 0.0008460599929094315 + 0.001 * 6.864902973175049
Epoch 1820, val loss: 2.06215500831604
Epoch 1830, training loss: 0.007712865248322487 = 0.0008411669405177236 + 0.001 * 6.871697902679443
Epoch 1830, val loss: 2.0630645751953125
Epoch 1840, training loss: 0.007696385495364666 = 0.0008364320965483785 + 0.001 * 6.8599534034729
Epoch 1840, val loss: 2.063871145248413
Epoch 1850, training loss: 0.007698479108512402 = 0.0008317820029333234 + 0.001 * 6.866696834564209
Epoch 1850, val loss: 2.064718723297119
Epoch 1860, training loss: 0.00771133229136467 = 0.0008271883125416934 + 0.001 * 6.884143829345703
Epoch 1860, val loss: 2.065516233444214
Epoch 1870, training loss: 0.007738785818219185 = 0.0008227003854699433 + 0.001 * 6.9160847663879395
Epoch 1870, val loss: 2.0663158893585205
Epoch 1880, training loss: 0.007685210555791855 = 0.0008183190366253257 + 0.001 * 6.866890907287598
Epoch 1880, val loss: 2.0670697689056396
Epoch 1890, training loss: 0.007669918704777956 = 0.0008140030549839139 + 0.001 * 6.855915546417236
Epoch 1890, val loss: 2.067826271057129
Epoch 1900, training loss: 0.00765323406085372 = 0.0008097723475657403 + 0.001 * 6.843461513519287
Epoch 1900, val loss: 2.0685791969299316
Epoch 1910, training loss: 0.007713256869465113 = 0.0008056468213908374 + 0.001 * 6.907609939575195
Epoch 1910, val loss: 2.0692830085754395
Epoch 1920, training loss: 0.007683626376092434 = 0.0008015876519493759 + 0.001 * 6.882038593292236
Epoch 1920, val loss: 2.0700111389160156
Epoch 1930, training loss: 0.007686019875109196 = 0.000797569751739502 + 0.001 * 6.888449668884277
Epoch 1930, val loss: 2.0706963539123535
Epoch 1940, training loss: 0.007663927040994167 = 0.0007936760666780174 + 0.001 * 6.870250701904297
Epoch 1940, val loss: 2.0713889598846436
Epoch 1950, training loss: 0.007673454936593771 = 0.0007898442563600838 + 0.001 * 6.883610248565674
Epoch 1950, val loss: 2.0720231533050537
Epoch 1960, training loss: 0.007666307035833597 = 0.000786059012170881 + 0.001 * 6.880247592926025
Epoch 1960, val loss: 2.0726945400238037
Epoch 1970, training loss: 0.007664903067052364 = 0.0007823830237612128 + 0.001 * 6.882519721984863
Epoch 1970, val loss: 2.0733094215393066
Epoch 1980, training loss: 0.0076589882373809814 = 0.0007787516224198043 + 0.001 * 6.880236625671387
Epoch 1980, val loss: 2.073939085006714
Epoch 1990, training loss: 0.0076308418065309525 = 0.0007751710363663733 + 0.001 * 6.85567045211792
Epoch 1990, val loss: 2.0745632648468018
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.971671223640442 = 1.9630743265151978 + 0.001 * 8.596855163574219
Epoch 0, val loss: 1.959959864616394
Epoch 10, training loss: 1.9605733156204224 = 1.9519765377044678 + 0.001 * 8.596779823303223
Epoch 10, val loss: 1.9482651948928833
Epoch 20, training loss: 1.9463828802108765 = 1.937786340713501 + 0.001 * 8.596572875976562
Epoch 20, val loss: 1.9333776235580444
Epoch 30, training loss: 1.9261291027069092 = 1.917533040046692 + 0.001 * 8.596081733703613
Epoch 30, val loss: 1.9123023748397827
Epoch 40, training loss: 1.896154761314392 = 1.8875600099563599 + 0.001 * 8.59473705291748
Epoch 40, val loss: 1.881751298904419
Epoch 50, training loss: 1.855131983757019 = 1.8465417623519897 + 0.001 * 8.59017562866211
Epoch 50, val loss: 1.8425260782241821
Epoch 60, training loss: 1.812328577041626 = 1.803757905960083 + 0.001 * 8.57072639465332
Epoch 60, val loss: 1.806876301765442
Epoch 70, training loss: 1.7785230875015259 = 1.7700468301773071 + 0.001 * 8.476214408874512
Epoch 70, val loss: 1.7811692953109741
Epoch 80, training loss: 1.735883355140686 = 1.7277674674987793 + 0.001 * 8.115880012512207
Epoch 80, val loss: 1.743032693862915
Epoch 90, training loss: 1.6774840354919434 = 1.6694809198379517 + 0.001 * 8.003105163574219
Epoch 90, val loss: 1.6896241903305054
Epoch 100, training loss: 1.597882628440857 = 1.5900700092315674 + 0.001 * 7.812582015991211
Epoch 100, val loss: 1.6190744638442993
Epoch 110, training loss: 1.5043425559997559 = 1.4967288970947266 + 0.001 * 7.613705158233643
Epoch 110, val loss: 1.538432002067566
Epoch 120, training loss: 1.4100350141525269 = 1.4024900197982788 + 0.001 * 7.544963359832764
Epoch 120, val loss: 1.4586644172668457
Epoch 130, training loss: 1.317482829093933 = 1.30996572971344 + 0.001 * 7.517118453979492
Epoch 130, val loss: 1.3825621604919434
Epoch 140, training loss: 1.222267508506775 = 1.2147915363311768 + 0.001 * 7.476022243499756
Epoch 140, val loss: 1.30546236038208
Epoch 150, training loss: 1.122819185256958 = 1.1153903007507324 + 0.001 * 7.428909778594971
Epoch 150, val loss: 1.2252285480499268
Epoch 160, training loss: 1.0219780206680298 = 1.0145927667617798 + 0.001 * 7.385241985321045
Epoch 160, val loss: 1.1449519395828247
Epoch 170, training loss: 0.925750195980072 = 0.9184017777442932 + 0.001 * 7.348426342010498
Epoch 170, val loss: 1.069579839706421
Epoch 180, training loss: 0.8408480286598206 = 0.8335354328155518 + 0.001 * 7.31256628036499
Epoch 180, val loss: 1.0058146715164185
Epoch 190, training loss: 0.7704188823699951 = 0.7631320953369141 + 0.001 * 7.286776542663574
Epoch 190, val loss: 0.9560448527336121
Epoch 200, training loss: 0.71308434009552 = 0.705805778503418 + 0.001 * 7.278548240661621
Epoch 200, val loss: 0.9191407561302185
Epoch 210, training loss: 0.6646619439125061 = 0.6573876142501831 + 0.001 * 7.274302959442139
Epoch 210, val loss: 0.8912718892097473
Epoch 220, training loss: 0.6207595467567444 = 0.6134909987449646 + 0.001 * 7.26852560043335
Epoch 220, val loss: 0.8682928681373596
Epoch 230, training loss: 0.5781142711639404 = 0.5708551406860352 + 0.001 * 7.259159088134766
Epoch 230, val loss: 0.8473668098449707
Epoch 240, training loss: 0.5348916053771973 = 0.527645468711853 + 0.001 * 7.246157169342041
Epoch 240, val loss: 0.8269738554954529
Epoch 250, training loss: 0.4908443093299866 = 0.48361408710479736 + 0.001 * 7.230223178863525
Epoch 250, val loss: 0.8074475526809692
Epoch 260, training loss: 0.4470725953578949 = 0.4398641288280487 + 0.001 * 7.208477973937988
Epoch 260, val loss: 0.7905604839324951
Epoch 270, training loss: 0.40513449907302856 = 0.39794737100601196 + 0.001 * 7.187119960784912
Epoch 270, val loss: 0.7781540155410767
Epoch 280, training loss: 0.3662782311439514 = 0.35911551117897034 + 0.001 * 7.162720680236816
Epoch 280, val loss: 0.7710580229759216
Epoch 290, training loss: 0.3308941721916199 = 0.3237511217594147 + 0.001 * 7.143041610717773
Epoch 290, val loss: 0.7689481973648071
Epoch 300, training loss: 0.29854699969291687 = 0.2914101481437683 + 0.001 * 7.136837005615234
Epoch 300, val loss: 0.7705951929092407
Epoch 310, training loss: 0.26848629117012024 = 0.2613639235496521 + 0.001 * 7.122370719909668
Epoch 310, val loss: 0.7748593091964722
Epoch 320, training loss: 0.2402086853981018 = 0.23309165239334106 + 0.001 * 7.117027759552002
Epoch 320, val loss: 0.7811965942382812
Epoch 330, training loss: 0.21362222731113434 = 0.20651070773601532 + 0.001 * 7.111521244049072
Epoch 330, val loss: 0.7893173694610596
Epoch 340, training loss: 0.18901076912879944 = 0.18189464509487152 + 0.001 * 7.116127014160156
Epoch 340, val loss: 0.799189031124115
Epoch 350, training loss: 0.16672514379024506 = 0.15961629152297974 + 0.001 * 7.108847618103027
Epoch 350, val loss: 0.8109356760978699
Epoch 360, training loss: 0.147017702460289 = 0.13991060853004456 + 0.001 * 7.107091426849365
Epoch 360, val loss: 0.8245143294334412
Epoch 370, training loss: 0.12989091873168945 = 0.12278596311807632 + 0.001 * 7.104950904846191
Epoch 370, val loss: 0.8397126197814941
Epoch 380, training loss: 0.11518264561891556 = 0.10807841271162033 + 0.001 * 7.104232311248779
Epoch 380, val loss: 0.8562349677085876
Epoch 390, training loss: 0.10262203961610794 = 0.0955154150724411 + 0.001 * 7.106622695922852
Epoch 390, val loss: 0.8737595677375793
Epoch 400, training loss: 0.09189015626907349 = 0.08478346467018127 + 0.001 * 7.10669469833374
Epoch 400, val loss: 0.8918907046318054
Epoch 410, training loss: 0.08268150687217712 = 0.07557818293571472 + 0.001 * 7.103322982788086
Epoch 410, val loss: 0.9103595018386841
Epoch 420, training loss: 0.0747440755367279 = 0.06763912737369537 + 0.001 * 7.104950904846191
Epoch 420, val loss: 0.9289644956588745
Epoch 430, training loss: 0.06785575300455093 = 0.06075237691402435 + 0.001 * 7.103377819061279
Epoch 430, val loss: 0.9475380182266235
Epoch 440, training loss: 0.06184811890125275 = 0.05474594235420227 + 0.001 * 7.102175235748291
Epoch 440, val loss: 0.9659116268157959
Epoch 450, training loss: 0.05658561363816261 = 0.04948260262608528 + 0.001 * 7.103009223937988
Epoch 450, val loss: 0.9840616583824158
Epoch 460, training loss: 0.05195832997560501 = 0.04485116899013519 + 0.001 * 7.107161521911621
Epoch 460, val loss: 1.001969337463379
Epoch 470, training loss: 0.04786604270339012 = 0.04076336324214935 + 0.001 * 7.102678298950195
Epoch 470, val loss: 1.0195544958114624
Epoch 480, training loss: 0.04424674063920975 = 0.037143856287002563 + 0.001 * 7.1028852462768555
Epoch 480, val loss: 1.036778450012207
Epoch 490, training loss: 0.04103228449821472 = 0.03393169865012169 + 0.001 * 7.1005859375
Epoch 490, val loss: 1.0535790920257568
Epoch 500, training loss: 0.038175068795681 = 0.03107362426817417 + 0.001 * 7.101444244384766
Epoch 500, val loss: 1.0699641704559326
Epoch 510, training loss: 0.035621266812086105 = 0.028521526604890823 + 0.001 * 7.099740982055664
Epoch 510, val loss: 1.0859819650650024
Epoch 520, training loss: 0.03333225101232529 = 0.026232434436678886 + 0.001 * 7.099816799163818
Epoch 520, val loss: 1.101674199104309
Epoch 530, training loss: 0.03127790987491608 = 0.024172388017177582 + 0.001 * 7.105523586273193
Epoch 530, val loss: 1.117028832435608
Epoch 540, training loss: 0.029417719691991806 = 0.02231591008603573 + 0.001 * 7.101809024810791
Epoch 540, val loss: 1.1320880651474
Epoch 550, training loss: 0.027744293212890625 = 0.020641352981328964 + 0.001 * 7.102940559387207
Epoch 550, val loss: 1.1468515396118164
Epoch 560, training loss: 0.02622740902006626 = 0.019130192697048187 + 0.001 * 7.0972161293029785
Epoch 560, val loss: 1.1613136529922485
Epoch 570, training loss: 0.024860523641109467 = 0.017765475437045097 + 0.001 * 7.095048904418945
Epoch 570, val loss: 1.1754429340362549
Epoch 580, training loss: 0.023642010986804962 = 0.01653170958161354 + 0.001 * 7.1103010177612305
Epoch 580, val loss: 1.1892540454864502
Epoch 590, training loss: 0.022507425397634506 = 0.01541487779468298 + 0.001 * 7.092547416687012
Epoch 590, val loss: 1.202723741531372
Epoch 600, training loss: 0.021494856104254723 = 0.014402179047465324 + 0.001 * 7.092676162719727
Epoch 600, val loss: 1.2158321142196655
Epoch 610, training loss: 0.020572375506162643 = 0.01348233874887228 + 0.001 * 7.090035438537598
Epoch 610, val loss: 1.2286072969436646
Epoch 620, training loss: 0.01975211314857006 = 0.012645391747355461 + 0.001 * 7.106720447540283
Epoch 620, val loss: 1.241032361984253
Epoch 630, training loss: 0.018973715603351593 = 0.011882402002811432 + 0.001 * 7.091313362121582
Epoch 630, val loss: 1.2531208992004395
Epoch 640, training loss: 0.018274737522006035 = 0.011185450479388237 + 0.001 * 7.089286804199219
Epoch 640, val loss: 1.2648847103118896
Epoch 650, training loss: 0.01763550192117691 = 0.010547670535743237 + 0.001 * 7.087830543518066
Epoch 650, val loss: 1.276322364807129
Epoch 660, training loss: 0.017045456916093826 = 0.009962943382561207 + 0.001 * 7.082512855529785
Epoch 660, val loss: 1.2874417304992676
Epoch 670, training loss: 0.016515551134943962 = 0.009425822645425797 + 0.001 * 7.089728832244873
Epoch 670, val loss: 1.29823637008667
Epoch 680, training loss: 0.016019757837057114 = 0.0089315390214324 + 0.001 * 7.088217735290527
Epoch 680, val loss: 1.3087130784988403
Epoch 690, training loss: 0.015560565516352654 = 0.008475867100059986 + 0.001 * 7.08469820022583
Epoch 690, val loss: 1.318897008895874
Epoch 700, training loss: 0.015136144123971462 = 0.008055020123720169 + 0.001 * 7.0811238288879395
Epoch 700, val loss: 1.3288030624389648
Epoch 710, training loss: 0.014762010425329208 = 0.007665738929063082 + 0.001 * 7.096271514892578
Epoch 710, val loss: 1.3384188413619995
Epoch 720, training loss: 0.014396502636373043 = 0.007305026054382324 + 0.001 * 7.0914764404296875
Epoch 720, val loss: 1.3477720022201538
Epoch 730, training loss: 0.014052660204470158 = 0.006970243062824011 + 0.001 * 7.082417011260986
Epoch 730, val loss: 1.356866717338562
Epoch 740, training loss: 0.013734432868659496 = 0.00665903277695179 + 0.001 * 7.075399875640869
Epoch 740, val loss: 1.3657190799713135
Epoch 750, training loss: 0.01344181690365076 = 0.006369295995682478 + 0.001 * 7.072520732879639
Epoch 750, val loss: 1.3743312358856201
Epoch 760, training loss: 0.013168105855584145 = 0.006099154241383076 + 0.001 * 7.068950653076172
Epoch 760, val loss: 1.3827130794525146
Epoch 770, training loss: 0.012928349897265434 = 0.005846893880516291 + 0.001 * 7.081455707550049
Epoch 770, val loss: 1.3908801078796387
Epoch 780, training loss: 0.012677868828177452 = 0.005610999651253223 + 0.001 * 7.066868305206299
Epoch 780, val loss: 1.3988069295883179
Epoch 790, training loss: 0.01245497353374958 = 0.005390133708715439 + 0.001 * 7.0648393630981445
Epoch 790, val loss: 1.406530737876892
Epoch 800, training loss: 0.012259034439921379 = 0.005183082073926926 + 0.001 * 7.075952529907227
Epoch 800, val loss: 1.414054036140442
Epoch 810, training loss: 0.012046738527715206 = 0.004988707136362791 + 0.001 * 7.05803108215332
Epoch 810, val loss: 1.4213708639144897
Epoch 820, training loss: 0.011880187317728996 = 0.0048060109838843346 + 0.001 * 7.074175834655762
Epoch 820, val loss: 1.4285149574279785
Epoch 830, training loss: 0.011706922203302383 = 0.004634105134755373 + 0.001 * 7.072816371917725
Epoch 830, val loss: 1.4354777336120605
Epoch 840, training loss: 0.011537058278918266 = 0.0044721569865942 + 0.001 * 7.064900875091553
Epoch 840, val loss: 1.4422576427459717
Epoch 850, training loss: 0.011391831561923027 = 0.004319434054195881 + 0.001 * 7.072397232055664
Epoch 850, val loss: 1.4488801956176758
Epoch 860, training loss: 0.0112476646900177 = 0.004175231792032719 + 0.001 * 7.072432994842529
Epoch 860, val loss: 1.4553394317626953
Epoch 870, training loss: 0.01110488548874855 = 0.00403895927593112 + 0.001 * 7.065926551818848
Epoch 870, val loss: 1.4616261720657349
Epoch 880, training loss: 0.01095678098499775 = 0.003910045605152845 + 0.001 * 7.046734809875488
Epoch 880, val loss: 1.4677824974060059
Epoch 890, training loss: 0.010855932720005512 = 0.0037879657465964556 + 0.001 * 7.067966938018799
Epoch 890, val loss: 1.473786473274231
Epoch 900, training loss: 0.010732367634773254 = 0.0036722663789987564 + 0.001 * 7.060101509094238
Epoch 900, val loss: 1.4796509742736816
Epoch 910, training loss: 0.010638386011123657 = 0.0035625314339995384 + 0.001 * 7.075854301452637
Epoch 910, val loss: 1.4853705167770386
Epoch 920, training loss: 0.01050109975039959 = 0.003458338789641857 + 0.001 * 7.042760848999023
Epoch 920, val loss: 1.4909554719924927
Epoch 930, training loss: 0.01040725689381361 = 0.003359326394274831 + 0.001 * 7.0479302406311035
Epoch 930, val loss: 1.4964171648025513
Epoch 940, training loss: 0.010363630950450897 = 0.003265164792537689 + 0.001 * 7.098465442657471
Epoch 940, val loss: 1.5017470121383667
Epoch 950, training loss: 0.010198771953582764 = 0.0031755289528518915 + 0.001 * 7.023242473602295
Epoch 950, val loss: 1.506942629814148
Epoch 960, training loss: 0.010129010304808617 = 0.003090162528678775 + 0.001 * 7.03884744644165
Epoch 960, val loss: 1.512032389640808
Epoch 970, training loss: 0.010055712424218655 = 0.0030087793711572886 + 0.001 * 7.046932697296143
Epoch 970, val loss: 1.516991376876831
Epoch 980, training loss: 0.009969267062842846 = 0.0029311387334018946 + 0.001 * 7.03812837600708
Epoch 980, val loss: 1.5218597650527954
Epoch 990, training loss: 0.009892149828374386 = 0.002857030602172017 + 0.001 * 7.035118579864502
Epoch 990, val loss: 1.5265965461730957
Epoch 1000, training loss: 0.00982297956943512 = 0.0027862410061061382 + 0.001 * 7.03673791885376
Epoch 1000, val loss: 1.5312306880950928
Epoch 1010, training loss: 0.009734909981489182 = 0.002718576230108738 + 0.001 * 7.016333103179932
Epoch 1010, val loss: 1.5357582569122314
Epoch 1020, training loss: 0.009696681052446365 = 0.002653847448527813 + 0.001 * 7.042832851409912
Epoch 1020, val loss: 1.5402114391326904
Epoch 1030, training loss: 0.009690196253359318 = 0.002591897500678897 + 0.001 * 7.0982985496521
Epoch 1030, val loss: 1.544556736946106
Epoch 1040, training loss: 0.009578639641404152 = 0.0025325710885226727 + 0.001 * 7.0460686683654785
Epoch 1040, val loss: 1.5487879514694214
Epoch 1050, training loss: 0.009488900192081928 = 0.0024757287465035915 + 0.001 * 7.013171195983887
Epoch 1050, val loss: 1.5529578924179077
Epoch 1060, training loss: 0.009444010443985462 = 0.0024211846757680178 + 0.001 * 7.022825717926025
Epoch 1060, val loss: 1.5570541620254517
Epoch 1070, training loss: 0.009369904175400734 = 0.002368730492889881 + 0.001 * 7.001173973083496
Epoch 1070, val loss: 1.5610684156417847
Epoch 1080, training loss: 0.009352022781968117 = 0.0023181287106126547 + 0.001 * 7.033894062042236
Epoch 1080, val loss: 1.5650050640106201
Epoch 1090, training loss: 0.00928534846752882 = 0.0022690407931804657 + 0.001 * 7.016307353973389
Epoch 1090, val loss: 1.5688915252685547
Epoch 1100, training loss: 0.009239397011697292 = 0.002221030183136463 + 0.001 * 7.01836633682251
Epoch 1100, val loss: 1.572752833366394
Epoch 1110, training loss: 0.009187186136841774 = 0.0021742559038102627 + 0.001 * 7.012930393218994
Epoch 1110, val loss: 1.5765697956085205
Epoch 1120, training loss: 0.009132029488682747 = 0.0021287330891937017 + 0.001 * 7.0032958984375
Epoch 1120, val loss: 1.5803337097167969
Epoch 1130, training loss: 0.009103958494961262 = 0.0020841804798692465 + 0.001 * 7.019777774810791
Epoch 1130, val loss: 1.5840977430343628
Epoch 1140, training loss: 0.009023191407322884 = 0.0020409405697137117 + 0.001 * 6.982250213623047
Epoch 1140, val loss: 1.587803840637207
Epoch 1150, training loss: 0.008973538875579834 = 0.0019989176653325558 + 0.001 * 6.974621295928955
Epoch 1150, val loss: 1.591444730758667
Epoch 1160, training loss: 0.008957096375524998 = 0.001958247972652316 + 0.001 * 6.998847961425781
Epoch 1160, val loss: 1.5950565338134766
Epoch 1170, training loss: 0.00892370380461216 = 0.0019187623402103782 + 0.001 * 7.004940986633301
Epoch 1170, val loss: 1.5986329317092896
Epoch 1180, training loss: 0.008880381472408772 = 0.0018806018633767962 + 0.001 * 6.999779224395752
Epoch 1180, val loss: 1.6021549701690674
Epoch 1190, training loss: 0.00884738564491272 = 0.0018436468672007322 + 0.001 * 7.003738880157471
Epoch 1190, val loss: 1.6056102514266968
Epoch 1200, training loss: 0.008794727735221386 = 0.0018077727872878313 + 0.001 * 6.986954689025879
Epoch 1200, val loss: 1.6090314388275146
Epoch 1210, training loss: 0.008778266608715057 = 0.0017730649560689926 + 0.001 * 7.0052008628845215
Epoch 1210, val loss: 1.6124411821365356
Epoch 1220, training loss: 0.008712506853044033 = 0.0017396219773218036 + 0.001 * 6.972884654998779
Epoch 1220, val loss: 1.6157796382904053
Epoch 1230, training loss: 0.008686361834406853 = 0.0017073657363653183 + 0.001 * 6.978995323181152
Epoch 1230, val loss: 1.619060754776001
Epoch 1240, training loss: 0.00870498176664114 = 0.0016762192826718092 + 0.001 * 7.028762340545654
Epoch 1240, val loss: 1.6223182678222656
Epoch 1250, training loss: 0.008607798255980015 = 0.0016461426857858896 + 0.001 * 6.961655139923096
Epoch 1250, val loss: 1.6255137920379639
Epoch 1260, training loss: 0.008600810542702675 = 0.0016170842573046684 + 0.001 * 6.983725547790527
Epoch 1260, val loss: 1.6286550760269165
Epoch 1270, training loss: 0.008587999269366264 = 0.0015890137292444706 + 0.001 * 6.998985767364502
Epoch 1270, val loss: 1.6317551136016846
Epoch 1280, training loss: 0.008503267541527748 = 0.001561895478516817 + 0.001 * 6.941371440887451
Epoch 1280, val loss: 1.6347782611846924
Epoch 1290, training loss: 0.0085176145657897 = 0.001535678980872035 + 0.001 * 6.981935501098633
Epoch 1290, val loss: 1.6377836465835571
Epoch 1300, training loss: 0.008546685799956322 = 0.0015103563200682402 + 0.001 * 7.0363287925720215
Epoch 1300, val loss: 1.640715479850769
Epoch 1310, training loss: 0.008441539481282234 = 0.0014859242364764214 + 0.001 * 6.9556145668029785
Epoch 1310, val loss: 1.6435887813568115
Epoch 1320, training loss: 0.008419210091233253 = 0.0014623227762058377 + 0.001 * 6.9568867683410645
Epoch 1320, val loss: 1.646423578262329
Epoch 1330, training loss: 0.008398625999689102 = 0.001439483487047255 + 0.001 * 6.959142208099365
Epoch 1330, val loss: 1.6492165327072144
Epoch 1340, training loss: 0.00835506059229374 = 0.001417376915924251 + 0.001 * 6.937683582305908
Epoch 1340, val loss: 1.6519421339035034
Epoch 1350, training loss: 0.008346029557287693 = 0.0013960466021671891 + 0.001 * 6.949982643127441
Epoch 1350, val loss: 1.6546140909194946
Epoch 1360, training loss: 0.008303290233016014 = 0.001375409308820963 + 0.001 * 6.927881240844727
Epoch 1360, val loss: 1.6572283506393433
Epoch 1370, training loss: 0.008333000354468822 = 0.00135538459289819 + 0.001 * 6.9776153564453125
Epoch 1370, val loss: 1.6598169803619385
Epoch 1380, training loss: 0.008296005427837372 = 0.0013359846780076623 + 0.001 * 6.960020065307617
Epoch 1380, val loss: 1.6623551845550537
Epoch 1390, training loss: 0.008278626948595047 = 0.001317246351391077 + 0.001 * 6.9613800048828125
Epoch 1390, val loss: 1.6648489236831665
Epoch 1400, training loss: 0.008219743147492409 = 0.0012991400435566902 + 0.001 * 6.920603275299072
Epoch 1400, val loss: 1.6672767400741577
Epoch 1410, training loss: 0.008221217431128025 = 0.0012816122034564614 + 0.001 * 6.939605236053467
Epoch 1410, val loss: 1.6696593761444092
Epoch 1420, training loss: 0.00824039988219738 = 0.0012646508403122425 + 0.001 * 6.9757490158081055
Epoch 1420, val loss: 1.6720194816589355
Epoch 1430, training loss: 0.008189532905817032 = 0.0012482148595154285 + 0.001 * 6.941318035125732
Epoch 1430, val loss: 1.6742857694625854
Epoch 1440, training loss: 0.008130794391036034 = 0.0012322691036388278 + 0.001 * 6.898525238037109
Epoch 1440, val loss: 1.6765462160110474
Epoch 1450, training loss: 0.008143041282892227 = 0.0012168161338195205 + 0.001 * 6.926224708557129
Epoch 1450, val loss: 1.6787621974945068
Epoch 1460, training loss: 0.008164428174495697 = 0.0012018169509246945 + 0.001 * 6.962610721588135
Epoch 1460, val loss: 1.6809357404708862
Epoch 1470, training loss: 0.008132398128509521 = 0.0011872920440509915 + 0.001 * 6.945106029510498
Epoch 1470, val loss: 1.683058261871338
Epoch 1480, training loss: 0.008087844587862492 = 0.0011732019484043121 + 0.001 * 6.914642333984375
Epoch 1480, val loss: 1.6851426362991333
Epoch 1490, training loss: 0.008048659190535545 = 0.0011595427058637142 + 0.001 * 6.8891167640686035
Epoch 1490, val loss: 1.6871973276138306
Epoch 1500, training loss: 0.008065350353717804 = 0.0011462594848126173 + 0.001 * 6.919090747833252
Epoch 1500, val loss: 1.6892238855361938
Epoch 1510, training loss: 0.008055987767875195 = 0.0011333607835695148 + 0.001 * 6.922626495361328
Epoch 1510, val loss: 1.6911948919296265
Epoch 1520, training loss: 0.00809021107852459 = 0.0011208211071789265 + 0.001 * 6.969388961791992
Epoch 1520, val loss: 1.693104863166809
Epoch 1530, training loss: 0.008027387782931328 = 0.0011086422018706799 + 0.001 * 6.918745517730713
Epoch 1530, val loss: 1.6950221061706543
Epoch 1540, training loss: 0.007983817718923092 = 0.0010968055576086044 + 0.001 * 6.887012004852295
Epoch 1540, val loss: 1.6968954801559448
Epoch 1550, training loss: 0.00803924910724163 = 0.001085285097360611 + 0.001 * 6.9539642333984375
Epoch 1550, val loss: 1.6987462043762207
Epoch 1560, training loss: 0.008039619773626328 = 0.001074099214747548 + 0.001 * 6.965519905090332
Epoch 1560, val loss: 1.700540542602539
Epoch 1570, training loss: 0.007957260124385357 = 0.0010632304474711418 + 0.001 * 6.894029140472412
Epoch 1570, val loss: 1.7023022174835205
Epoch 1580, training loss: 0.007937030866742134 = 0.0010526467813178897 + 0.001 * 6.8843841552734375
Epoch 1580, val loss: 1.7040228843688965
Epoch 1590, training loss: 0.007937240414321423 = 0.0010423416970297694 + 0.001 * 6.894898891448975
Epoch 1590, val loss: 1.705722451210022
Epoch 1600, training loss: 0.00793488323688507 = 0.0010322958696633577 + 0.001 * 6.902587413787842
Epoch 1600, val loss: 1.7073924541473389
Epoch 1610, training loss: 0.0079205222427845 = 0.0010225222213193774 + 0.001 * 6.898000240325928
Epoch 1610, val loss: 1.709043025970459
Epoch 1620, training loss: 0.0079110749065876 = 0.001013019122183323 + 0.001 * 6.8980560302734375
Epoch 1620, val loss: 1.710635781288147
Epoch 1630, training loss: 0.007900376804172993 = 0.0010037864558398724 + 0.001 * 6.896590232849121
Epoch 1630, val loss: 1.7122350931167603
Epoch 1640, training loss: 0.007871614769101143 = 0.0009947963990271091 + 0.001 * 6.876818656921387
Epoch 1640, val loss: 1.71376633644104
Epoch 1650, training loss: 0.007860124111175537 = 0.0009860421996563673 + 0.001 * 6.874081134796143
Epoch 1650, val loss: 1.715287685394287
Epoch 1660, training loss: 0.007868774235248566 = 0.0009775091893970966 + 0.001 * 6.891265392303467
Epoch 1660, val loss: 1.7167718410491943
Epoch 1670, training loss: 0.007892404682934284 = 0.0009691892773844302 + 0.001 * 6.923214912414551
Epoch 1670, val loss: 1.7182214260101318
Epoch 1680, training loss: 0.007838458754122257 = 0.0009610745473764837 + 0.001 * 6.877384185791016
Epoch 1680, val loss: 1.7196327447891235
Epoch 1690, training loss: 0.007839943282306194 = 0.0009531548130325973 + 0.001 * 6.8867878913879395
Epoch 1690, val loss: 1.7210731506347656
Epoch 1700, training loss: 0.0078087737783789635 = 0.0009454117389395833 + 0.001 * 6.863361835479736
Epoch 1700, val loss: 1.7224305868148804
Epoch 1710, training loss: 0.007817994803190231 = 0.0009378647664561868 + 0.001 * 6.880129337310791
Epoch 1710, val loss: 1.723823070526123
Epoch 1720, training loss: 0.0078077903017401695 = 0.0009305097046308219 + 0.001 * 6.877280235290527
Epoch 1720, val loss: 1.725132942199707
Epoch 1730, training loss: 0.0077711534686386585 = 0.0009233292075805366 + 0.001 * 6.8478240966796875
Epoch 1730, val loss: 1.7264388799667358
Epoch 1740, training loss: 0.007820376195013523 = 0.0009163023787550628 + 0.001 * 6.904073238372803
Epoch 1740, val loss: 1.7277214527130127
Epoch 1750, training loss: 0.007796980906277895 = 0.0009094501147046685 + 0.001 * 6.88753080368042
Epoch 1750, val loss: 1.7290059328079224
Epoch 1760, training loss: 0.007784735877066851 = 0.0009027430205605924 + 0.001 * 6.881992340087891
Epoch 1760, val loss: 1.7302273511886597
Epoch 1770, training loss: 0.007737061008810997 = 0.0008961950079537928 + 0.001 * 6.840865612030029
Epoch 1770, val loss: 1.731450080871582
Epoch 1780, training loss: 0.007762427441775799 = 0.0008897992665879428 + 0.001 * 6.872628211975098
Epoch 1780, val loss: 1.7326722145080566
Epoch 1790, training loss: 0.007747088558971882 = 0.0008835463668219745 + 0.001 * 6.863542079925537
Epoch 1790, val loss: 1.7338461875915527
Epoch 1800, training loss: 0.007787992246448994 = 0.000877438869792968 + 0.001 * 6.910552978515625
Epoch 1800, val loss: 1.7349646091461182
Epoch 1810, training loss: 0.007757565937936306 = 0.0008714873110875487 + 0.001 * 6.886078357696533
Epoch 1810, val loss: 1.736086368560791
Epoch 1820, training loss: 0.007730476558208466 = 0.0008656749268993735 + 0.001 * 6.864801406860352
Epoch 1820, val loss: 1.7372040748596191
Epoch 1830, training loss: 0.007720484863966703 = 0.0008599996799603105 + 0.001 * 6.860485076904297
Epoch 1830, val loss: 1.7383004426956177
Epoch 1840, training loss: 0.0077246688306331635 = 0.0008544534794054925 + 0.001 * 6.870214939117432
Epoch 1840, val loss: 1.7393722534179688
Epoch 1850, training loss: 0.007726040203124285 = 0.0008490214240737259 + 0.001 * 6.877018451690674
Epoch 1850, val loss: 1.740418553352356
Epoch 1860, training loss: 0.007689443416893482 = 0.0008437228971160948 + 0.001 * 6.845720291137695
Epoch 1860, val loss: 1.7414494752883911
Epoch 1870, training loss: 0.00767869409173727 = 0.0008385229739360511 + 0.001 * 6.840170860290527
Epoch 1870, val loss: 1.7424578666687012
Epoch 1880, training loss: 0.007736504077911377 = 0.0008334384765475988 + 0.001 * 6.9030656814575195
Epoch 1880, val loss: 1.7434486150741577
Epoch 1890, training loss: 0.007692262995988131 = 0.0008284514187835157 + 0.001 * 6.863811016082764
Epoch 1890, val loss: 1.74441659450531
Epoch 1900, training loss: 0.00765632651746273 = 0.0008235623245127499 + 0.001 * 6.832763671875
Epoch 1900, val loss: 1.745375633239746
Epoch 1910, training loss: 0.0076441895216703415 = 0.0008187826024368405 + 0.001 * 6.825406551361084
Epoch 1910, val loss: 1.746352195739746
Epoch 1920, training loss: 0.007651019841432571 = 0.0008140866993926466 + 0.001 * 6.83693265914917
Epoch 1920, val loss: 1.747287392616272
Epoch 1930, training loss: 0.007649468723684549 = 0.0008094778168015182 + 0.001 * 6.839990615844727
Epoch 1930, val loss: 1.748192310333252
Epoch 1940, training loss: 0.007626639679074287 = 0.0008049772586673498 + 0.001 * 6.821662425994873
Epoch 1940, val loss: 1.749106526374817
Epoch 1950, training loss: 0.007651876658201218 = 0.0008005780982784927 + 0.001 * 6.8512983322143555
Epoch 1950, val loss: 1.7499815225601196
Epoch 1960, training loss: 0.007675370667129755 = 0.0007962551317177713 + 0.001 * 6.879115104675293
Epoch 1960, val loss: 1.7508565187454224
Epoch 1970, training loss: 0.007657793816179037 = 0.0007920399075374007 + 0.001 * 6.865753650665283
Epoch 1970, val loss: 1.7516857385635376
Epoch 1980, training loss: 0.007594326511025429 = 0.0007879032054916024 + 0.001 * 6.806423187255859
Epoch 1980, val loss: 1.7525242567062378
Epoch 1990, training loss: 0.0075951069593429565 = 0.0007838658639229834 + 0.001 * 6.8112406730651855
Epoch 1990, val loss: 1.7533471584320068
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8439641539272537
The final CL Acc:0.79136, 0.02607, The final GNN Acc:0.83957, 0.00317
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10578])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9768518209457397 = 1.9682549238204956 + 0.001 * 8.596837997436523
Epoch 0, val loss: 1.9680668115615845
Epoch 10, training loss: 1.9665032625198364 = 1.9579064846038818 + 0.001 * 8.596768379211426
Epoch 10, val loss: 1.957940697669983
Epoch 20, training loss: 1.953784704208374 = 1.9451881647109985 + 0.001 * 8.596541404724121
Epoch 20, val loss: 1.9449926614761353
Epoch 30, training loss: 1.9360312223434448 = 1.9274351596832275 + 0.001 * 8.596003532409668
Epoch 30, val loss: 1.9265530109405518
Epoch 40, training loss: 1.9094557762145996 = 1.9008612632751465 + 0.001 * 8.594541549682617
Epoch 40, val loss: 1.8990198373794556
Epoch 50, training loss: 1.8710424900054932 = 1.862452507019043 + 0.001 * 8.589947700500488
Epoch 50, val loss: 1.8605815172195435
Epoch 60, training loss: 1.8261265754699707 = 1.8175544738769531 + 0.001 * 8.572148323059082
Epoch 60, val loss: 1.8202043771743774
Epoch 70, training loss: 1.791304588317871 = 1.7828155755996704 + 0.001 * 8.489060401916504
Epoch 70, val loss: 1.7944839000701904
Epoch 80, training loss: 1.7562187910079956 = 1.7480970621109009 + 0.001 * 8.121774673461914
Epoch 80, val loss: 1.7656548023223877
Epoch 90, training loss: 1.7080152034759521 = 1.7001392841339111 + 0.001 * 7.875903606414795
Epoch 90, val loss: 1.7239289283752441
Epoch 100, training loss: 1.6416125297546387 = 1.6339446306228638 + 0.001 * 7.6678547859191895
Epoch 100, val loss: 1.667265772819519
Epoch 110, training loss: 1.5539392232894897 = 1.5463330745697021 + 0.001 * 7.6061553955078125
Epoch 110, val loss: 1.5933616161346436
Epoch 120, training loss: 1.4514226913452148 = 1.4438384771347046 + 0.001 * 7.58420991897583
Epoch 120, val loss: 1.5081558227539062
Epoch 130, training loss: 1.343677282333374 = 1.3361084461212158 + 0.001 * 7.568878650665283
Epoch 130, val loss: 1.4205201864242554
Epoch 140, training loss: 1.2368801832199097 = 1.2293351888656616 + 0.001 * 7.545012474060059
Epoch 140, val loss: 1.3361321687698364
Epoch 150, training loss: 1.133898377418518 = 1.1264039278030396 + 0.001 * 7.494426250457764
Epoch 150, val loss: 1.2571628093719482
Epoch 160, training loss: 1.036704659461975 = 1.0292940139770508 + 0.001 * 7.410624980926514
Epoch 160, val loss: 1.1851378679275513
Epoch 170, training loss: 0.9473382234573364 = 0.939979612827301 + 0.001 * 7.358623504638672
Epoch 170, val loss: 1.121214747428894
Epoch 180, training loss: 0.8676255941390991 = 0.8602810502052307 + 0.001 * 7.344552040100098
Epoch 180, val loss: 1.0665502548217773
Epoch 190, training loss: 0.7986898422241211 = 0.7913528680801392 + 0.001 * 7.336994647979736
Epoch 190, val loss: 1.0223984718322754
Epoch 200, training loss: 0.7402929663658142 = 0.7329612970352173 + 0.001 * 7.331679821014404
Epoch 200, val loss: 0.9881694912910461
Epoch 210, training loss: 0.6905599236488342 = 0.6832370758056641 + 0.001 * 7.3228678703308105
Epoch 210, val loss: 0.9629390239715576
Epoch 220, training loss: 0.6466113328933716 = 0.6392980217933655 + 0.001 * 7.313302040100098
Epoch 220, val loss: 0.9445838928222656
Epoch 230, training loss: 0.6056306958198547 = 0.5983309745788574 + 0.001 * 7.299699783325195
Epoch 230, val loss: 0.9308638572692871
Epoch 240, training loss: 0.5654854774475098 = 0.558201789855957 + 0.001 * 7.28369140625
Epoch 240, val loss: 0.9199149012565613
Epoch 250, training loss: 0.5251791477203369 = 0.5179054141044617 + 0.001 * 7.2737627029418945
Epoch 250, val loss: 0.9109295010566711
Epoch 260, training loss: 0.4848708510398865 = 0.4776145815849304 + 0.001 * 7.2562689781188965
Epoch 260, val loss: 0.904332160949707
Epoch 270, training loss: 0.4455537497997284 = 0.438313364982605 + 0.001 * 7.24038553237915
Epoch 270, val loss: 0.9010965824127197
Epoch 280, training loss: 0.40850409865379333 = 0.40127113461494446 + 0.001 * 7.2329583168029785
Epoch 280, val loss: 0.9023140668869019
Epoch 290, training loss: 0.37459149956703186 = 0.3673703074455261 + 0.001 * 7.22120475769043
Epoch 290, val loss: 0.9087135791778564
Epoch 300, training loss: 0.34405517578125 = 0.3368258476257324 + 0.001 * 7.229341983795166
Epoch 300, val loss: 0.9199879169464111
Epoch 310, training loss: 0.3166349232196808 = 0.3094319701194763 + 0.001 * 7.202939510345459
Epoch 310, val loss: 0.9353930354118347
Epoch 320, training loss: 0.29191112518310547 = 0.28472384810447693 + 0.001 * 7.187280654907227
Epoch 320, val loss: 0.9542783498764038
Epoch 330, training loss: 0.26931336522102356 = 0.262076199054718 + 0.001 * 7.237152576446533
Epoch 330, val loss: 0.9759361147880554
Epoch 340, training loss: 0.24792268872261047 = 0.24074432253837585 + 0.001 * 7.178373336791992
Epoch 340, val loss: 0.9997706413269043
Epoch 350, training loss: 0.22710087895393372 = 0.2199370414018631 + 0.001 * 7.163841724395752
Epoch 350, val loss: 1.0252840518951416
Epoch 360, training loss: 0.20618534088134766 = 0.19902697205543518 + 0.001 * 7.158375263214111
Epoch 360, val loss: 1.0519158840179443
Epoch 370, training loss: 0.18521425127983093 = 0.17805776000022888 + 0.001 * 7.156489372253418
Epoch 370, val loss: 1.0797513723373413
Epoch 380, training loss: 0.16498342156410217 = 0.1578308492898941 + 0.001 * 7.152565956115723
Epoch 380, val loss: 1.1094505786895752
Epoch 390, training loss: 0.14645077288150787 = 0.13928498327732086 + 0.001 * 7.165794849395752
Epoch 390, val loss: 1.1417691707611084
Epoch 400, training loss: 0.13004329800605774 = 0.12291913479566574 + 0.001 * 7.1241631507873535
Epoch 400, val loss: 1.1767977476119995
Epoch 410, training loss: 0.1158531978726387 = 0.10872850567102432 + 0.001 * 7.124689102172852
Epoch 410, val loss: 1.213887333869934
Epoch 420, training loss: 0.10358259081840515 = 0.09645618498325348 + 0.001 * 7.126401901245117
Epoch 420, val loss: 1.2523211240768433
Epoch 430, training loss: 0.09294939786195755 = 0.08581668138504028 + 0.001 * 7.132718086242676
Epoch 430, val loss: 1.2913925647735596
Epoch 440, training loss: 0.08367534726858139 = 0.07656512409448624 + 0.001 * 7.110219478607178
Epoch 440, val loss: 1.330519437789917
Epoch 450, training loss: 0.07561470568180084 = 0.0685090646147728 + 0.001 * 7.105642318725586
Epoch 450, val loss: 1.3694376945495605
Epoch 460, training loss: 0.06859473139047623 = 0.0614810511469841 + 0.001 * 7.113682270050049
Epoch 460, val loss: 1.407704472541809
Epoch 470, training loss: 0.062433503568172455 = 0.055338382720947266 + 0.001 * 7.095120429992676
Epoch 470, val loss: 1.4453799724578857
Epoch 480, training loss: 0.057046517729759216 = 0.04995923116803169 + 0.001 * 7.087287425994873
Epoch 480, val loss: 1.4823029041290283
Epoch 490, training loss: 0.05235559865832329 = 0.0452299565076828 + 0.001 * 7.125641822814941
Epoch 490, val loss: 1.5182759761810303
Epoch 500, training loss: 0.048141419887542725 = 0.04105573147535324 + 0.001 * 7.085687160491943
Epoch 500, val loss: 1.5534700155258179
Epoch 510, training loss: 0.04445374757051468 = 0.03735986351966858 + 0.001 * 7.09388542175293
Epoch 510, val loss: 1.5878586769104004
Epoch 520, training loss: 0.041173942387104034 = 0.0340811125934124 + 0.001 * 7.092828273773193
Epoch 520, val loss: 1.6214650869369507
Epoch 530, training loss: 0.03824863210320473 = 0.03116741217672825 + 0.001 * 7.081220626831055
Epoch 530, val loss: 1.6541409492492676
Epoch 540, training loss: 0.03564804792404175 = 0.028573863208293915 + 0.001 * 7.074183940887451
Epoch 540, val loss: 1.6859862804412842
Epoch 550, training loss: 0.03332992643117905 = 0.0262589231133461 + 0.001 * 7.071002006530762
Epoch 550, val loss: 1.7169387340545654
Epoch 560, training loss: 0.0312701091170311 = 0.024187352508306503 + 0.001 * 7.082755088806152
Epoch 560, val loss: 1.7470968961715698
Epoch 570, training loss: 0.029418926686048508 = 0.022329330444335938 + 0.001 * 7.089596271514893
Epoch 570, val loss: 1.7764968872070312
Epoch 580, training loss: 0.027735944837331772 = 0.0206600371748209 + 0.001 * 7.075907230377197
Epoch 580, val loss: 1.8051419258117676
Epoch 590, training loss: 0.026236698031425476 = 0.019157415255904198 + 0.001 * 7.079282283782959
Epoch 590, val loss: 1.8330687284469604
Epoch 600, training loss: 0.024862516671419144 = 0.017802344635128975 + 0.001 * 7.0601725578308105
Epoch 600, val loss: 1.8602421283721924
Epoch 610, training loss: 0.023648541420698166 = 0.016578160226345062 + 0.001 * 7.070380687713623
Epoch 610, val loss: 1.8866947889328003
Epoch 620, training loss: 0.02254989556968212 = 0.015470358543097973 + 0.001 * 7.0795369148254395
Epoch 620, val loss: 1.912429928779602
Epoch 630, training loss: 0.021543020382523537 = 0.0144658787176013 + 0.001 * 7.077141284942627
Epoch 630, val loss: 1.937464714050293
Epoch 640, training loss: 0.020629215985536575 = 0.013553588651120663 + 0.001 * 7.07562780380249
Epoch 640, val loss: 1.9618104696273804
Epoch 650, training loss: 0.01979343593120575 = 0.012723280116915703 + 0.001 * 7.070156574249268
Epoch 650, val loss: 1.9854763746261597
Epoch 660, training loss: 0.01901659369468689 = 0.011966032907366753 + 0.001 * 7.050560474395752
Epoch 660, val loss: 2.0084733963012695
Epoch 670, training loss: 0.018321797251701355 = 0.011274157091975212 + 0.001 * 7.0476393699646
Epoch 670, val loss: 2.0307812690734863
Epoch 680, training loss: 0.017714418470859528 = 0.010640869848430157 + 0.001 * 7.073549270629883
Epoch 680, val loss: 2.0525200366973877
Epoch 690, training loss: 0.017132893204689026 = 0.010060117579996586 + 0.001 * 7.072775363922119
Epoch 690, val loss: 2.073650360107422
Epoch 700, training loss: 0.01657905802130699 = 0.009526490233838558 + 0.001 * 7.052567005157471
Epoch 700, val loss: 2.0941669940948486
Epoch 710, training loss: 0.0160750150680542 = 0.009035314433276653 + 0.001 * 7.039701461791992
Epoch 710, val loss: 2.114129066467285
Epoch 720, training loss: 0.015622057020664215 = 0.008582311682403088 + 0.001 * 7.039744853973389
Epoch 720, val loss: 2.133509874343872
Epoch 730, training loss: 0.015219812281429768 = 0.0081637566909194 + 0.001 * 7.056055068969727
Epoch 730, val loss: 2.152402400970459
Epoch 740, training loss: 0.014819703064858913 = 0.007776367012411356 + 0.001 * 7.043335914611816
Epoch 740, val loss: 2.1707868576049805
Epoch 750, training loss: 0.01446598768234253 = 0.00741706695407629 + 0.001 * 7.048920154571533
Epoch 750, val loss: 2.188697338104248
Epoch 760, training loss: 0.014108531177043915 = 0.007083304226398468 + 0.001 * 7.02522611618042
Epoch 760, val loss: 2.2061097621917725
Epoch 770, training loss: 0.01379941776394844 = 0.006772542838007212 + 0.001 * 7.02687406539917
Epoch 770, val loss: 2.2230939865112305
Epoch 780, training loss: 0.013529583811759949 = 0.006482404191046953 + 0.001 * 7.047179222106934
Epoch 780, val loss: 2.2396411895751953
Epoch 790, training loss: 0.013254107907414436 = 0.006210668478161097 + 0.001 * 7.043438911437988
Epoch 790, val loss: 2.255821943283081
Epoch 800, training loss: 0.012989703565835953 = 0.005955390632152557 + 0.001 * 7.0343122482299805
Epoch 800, val loss: 2.2716360092163086
Epoch 810, training loss: 0.012729490175843239 = 0.005714987870305777 + 0.001 * 7.01450252532959
Epoch 810, val loss: 2.287102460861206
Epoch 820, training loss: 0.012520022690296173 = 0.005488249938935041 + 0.001 * 7.031772136688232
Epoch 820, val loss: 2.302285671234131
Epoch 830, training loss: 0.012316087260842323 = 0.005274090450257063 + 0.001 * 7.041996955871582
Epoch 830, val loss: 2.3171746730804443
Epoch 840, training loss: 0.012113159522414207 = 0.005071765277534723 + 0.001 * 7.041394233703613
Epoch 840, val loss: 2.3318119049072266
Epoch 850, training loss: 0.011885718442499638 = 0.0048805102705955505 + 0.001 * 7.0052080154418945
Epoch 850, val loss: 2.3461496829986572
Epoch 860, training loss: 0.011707336641848087 = 0.004699735902249813 + 0.001 * 7.0076003074646
Epoch 860, val loss: 2.3602850437164307
Epoch 870, training loss: 0.011527067050337791 = 0.00452877301722765 + 0.001 * 6.998293399810791
Epoch 870, val loss: 2.3740997314453125
Epoch 880, training loss: 0.01137735415250063 = 0.004367099609225988 + 0.001 * 7.010254383087158
Epoch 880, val loss: 2.38767671585083
Epoch 890, training loss: 0.011223831214010715 = 0.004214156419038773 + 0.001 * 7.009674549102783
Epoch 890, val loss: 2.4009974002838135
Epoch 900, training loss: 0.01107280794531107 = 0.00406940933316946 + 0.001 * 7.003398418426514
Epoch 900, val loss: 2.4140658378601074
Epoch 910, training loss: 0.010928662493824959 = 0.003932335879653692 + 0.001 * 6.996326446533203
Epoch 910, val loss: 2.426933765411377
Epoch 920, training loss: 0.010813681408762932 = 0.0038024524692445993 + 0.001 * 7.011229038238525
Epoch 920, val loss: 2.439545154571533
Epoch 930, training loss: 0.0106887798756361 = 0.0036794247571378946 + 0.001 * 7.009355068206787
Epoch 930, val loss: 2.451916217803955
Epoch 940, training loss: 0.010551437735557556 = 0.003562747035175562 + 0.001 * 6.98868989944458
Epoch 940, val loss: 2.464055061340332
Epoch 950, training loss: 0.01046118326485157 = 0.003452037461102009 + 0.001 * 7.009145259857178
Epoch 950, val loss: 2.475956916809082
Epoch 960, training loss: 0.010330431163311005 = 0.003346910933032632 + 0.001 * 6.983520030975342
Epoch 960, val loss: 2.4876677989959717
Epoch 970, training loss: 0.010243074968457222 = 0.003247010987251997 + 0.001 * 6.996063232421875
Epoch 970, val loss: 2.4991259574890137
Epoch 980, training loss: 0.010144898667931557 = 0.0031520489137619734 + 0.001 * 6.992849349975586
Epoch 980, val loss: 2.5103743076324463
Epoch 990, training loss: 0.010127296671271324 = 0.003061688970774412 + 0.001 * 7.065607070922852
Epoch 990, val loss: 2.5214130878448486
Epoch 1000, training loss: 0.009968751110136509 = 0.0029757379088550806 + 0.001 * 6.993012428283691
Epoch 1000, val loss: 2.532254934310913
Epoch 1010, training loss: 0.009887448512017727 = 0.0028938190080225468 + 0.001 * 6.993628978729248
Epoch 1010, val loss: 2.54289174079895
Epoch 1020, training loss: 0.009807262569665909 = 0.0028157667256891727 + 0.001 * 6.991495132446289
Epoch 1020, val loss: 2.55338978767395
Epoch 1030, training loss: 0.009727519936859608 = 0.0027412765193730593 + 0.001 * 6.986242771148682
Epoch 1030, val loss: 2.5636425018310547
Epoch 1040, training loss: 0.009644770063459873 = 0.0026702475734055042 + 0.001 * 6.974522113800049
Epoch 1040, val loss: 2.573718786239624
Epoch 1050, training loss: 0.009560735896229744 = 0.0026024016551673412 + 0.001 * 6.958334445953369
Epoch 1050, val loss: 2.5836052894592285
Epoch 1060, training loss: 0.009502209722995758 = 0.002537573454901576 + 0.001 * 6.964635848999023
Epoch 1060, val loss: 2.59332537651062
Epoch 1070, training loss: 0.009490588679909706 = 0.00247560883872211 + 0.001 * 7.014979839324951
Epoch 1070, val loss: 2.6028525829315186
Epoch 1080, training loss: 0.00939929112792015 = 0.0024162977933883667 + 0.001 * 6.982993125915527
Epoch 1080, val loss: 2.612206220626831
Epoch 1090, training loss: 0.009344283491373062 = 0.0023595665115863085 + 0.001 * 6.984716892242432
Epoch 1090, val loss: 2.6214003562927246
Epoch 1100, training loss: 0.009269917383790016 = 0.002305213827639818 + 0.001 * 6.964703559875488
Epoch 1100, val loss: 2.6304171085357666
Epoch 1110, training loss: 0.009212560951709747 = 0.0022530946880578995 + 0.001 * 6.959465980529785
Epoch 1110, val loss: 2.639272689819336
Epoch 1120, training loss: 0.009191323071718216 = 0.0022031497210264206 + 0.001 * 6.988173007965088
Epoch 1120, val loss: 2.648000717163086
Epoch 1130, training loss: 0.00913064181804657 = 0.002155209891498089 + 0.001 * 6.975430965423584
Epoch 1130, val loss: 2.6565160751342773
Epoch 1140, training loss: 0.009061697870492935 = 0.0021092318929731846 + 0.001 * 6.952465534210205
Epoch 1140, val loss: 2.6649422645568848
Epoch 1150, training loss: 0.009032880887389183 = 0.0020651016384363174 + 0.001 * 6.967779159545898
Epoch 1150, val loss: 2.673222780227661
Epoch 1160, training loss: 0.008969743736088276 = 0.0020226798951625824 + 0.001 * 6.947063446044922
Epoch 1160, val loss: 2.681321620941162
Epoch 1170, training loss: 0.008930318057537079 = 0.001981907058507204 + 0.001 * 6.948410987854004
Epoch 1170, val loss: 2.6892964839935303
Epoch 1180, training loss: 0.008891012519598007 = 0.0019427199149504304 + 0.001 * 6.948291778564453
Epoch 1180, val loss: 2.6971302032470703
Epoch 1190, training loss: 0.00886537879705429 = 0.0019050291739404202 + 0.001 * 6.960348606109619
Epoch 1190, val loss: 2.7048418521881104
Epoch 1200, training loss: 0.008830231614410877 = 0.001868772436864674 + 0.001 * 6.961458683013916
Epoch 1200, val loss: 2.712400436401367
Epoch 1210, training loss: 0.008785904385149479 = 0.0018338497029617429 + 0.001 * 6.952054500579834
Epoch 1210, val loss: 2.7198565006256104
Epoch 1220, training loss: 0.008739813230931759 = 0.0018002244178205729 + 0.001 * 6.9395880699157715
Epoch 1220, val loss: 2.727201223373413
Epoch 1230, training loss: 0.008696931414306164 = 0.001767817186191678 + 0.001 * 6.929113864898682
Epoch 1230, val loss: 2.7343688011169434
Epoch 1240, training loss: 0.008678974583745003 = 0.001736561767756939 + 0.001 * 6.942412376403809
Epoch 1240, val loss: 2.7414355278015137
Epoch 1250, training loss: 0.008691523224115372 = 0.001706439652480185 + 0.001 * 6.985083103179932
Epoch 1250, val loss: 2.74837064743042
Epoch 1260, training loss: 0.008634138852357864 = 0.0016773497918620706 + 0.001 * 6.956788539886475
Epoch 1260, val loss: 2.7552080154418945
Epoch 1270, training loss: 0.008595127612352371 = 0.0016492996364831924 + 0.001 * 6.945827960968018
Epoch 1270, val loss: 2.76196551322937
Epoch 1280, training loss: 0.008555686101317406 = 0.00162219419144094 + 0.001 * 6.9334917068481445
Epoch 1280, val loss: 2.7685461044311523
Epoch 1290, training loss: 0.008528011851012707 = 0.0015960315940901637 + 0.001 * 6.931979656219482
Epoch 1290, val loss: 2.7750356197357178
Epoch 1300, training loss: 0.008490286767482758 = 0.0015707719139754772 + 0.001 * 6.919515132904053
Epoch 1300, val loss: 2.7814407348632812
Epoch 1310, training loss: 0.008463681675493717 = 0.001546345534734428 + 0.001 * 6.9173359870910645
Epoch 1310, val loss: 2.7877423763275146
Epoch 1320, training loss: 0.008444651030004025 = 0.001522740232758224 + 0.001 * 6.921910285949707
Epoch 1320, val loss: 2.7939445972442627
Epoch 1330, training loss: 0.008451190777122974 = 0.001499899080954492 + 0.001 * 6.951291561126709
Epoch 1330, val loss: 2.8000242710113525
Epoch 1340, training loss: 0.008397730067372322 = 0.001477787271142006 + 0.001 * 6.919942855834961
Epoch 1340, val loss: 2.8060688972473145
Epoch 1350, training loss: 0.0083733219653368 = 0.0014563902514055371 + 0.001 * 6.916931629180908
Epoch 1350, val loss: 2.8119547367095947
Epoch 1360, training loss: 0.008361905813217163 = 0.001435670885257423 + 0.001 * 6.926234722137451
Epoch 1360, val loss: 2.817781686782837
Epoch 1370, training loss: 0.008349515497684479 = 0.001415603794157505 + 0.001 * 6.933910846710205
Epoch 1370, val loss: 2.8235108852386475
Epoch 1380, training loss: 0.008327607996761799 = 0.0013961682561784983 + 0.001 * 6.9314398765563965
Epoch 1380, val loss: 2.829138994216919
Epoch 1390, training loss: 0.00833870843052864 = 0.0013773026876151562 + 0.001 * 6.961404800415039
Epoch 1390, val loss: 2.834681749343872
Epoch 1400, training loss: 0.008285552263259888 = 0.0013590658782050014 + 0.001 * 6.926486492156982
Epoch 1400, val loss: 2.8401684761047363
Epoch 1410, training loss: 0.008268170058727264 = 0.0013413520064204931 + 0.001 * 6.926817417144775
Epoch 1410, val loss: 2.845521926879883
Epoch 1420, training loss: 0.00825587660074234 = 0.001324199722148478 + 0.001 * 6.931676387786865
Epoch 1420, val loss: 2.8508572578430176
Epoch 1430, training loss: 0.008223077282309532 = 0.0013075312599539757 + 0.001 * 6.91554594039917
Epoch 1430, val loss: 2.8560497760772705
Epoch 1440, training loss: 0.008210893720388412 = 0.001291372231207788 + 0.001 * 6.919520854949951
Epoch 1440, val loss: 2.861206293106079
Epoch 1450, training loss: 0.008188361302018166 = 0.0012756692012771964 + 0.001 * 6.912692070007324
Epoch 1450, val loss: 2.8662736415863037
Epoch 1460, training loss: 0.00817611813545227 = 0.0012604306684806943 + 0.001 * 6.915687084197998
Epoch 1460, val loss: 2.8712072372436523
Epoch 1470, training loss: 0.008153660222887993 = 0.0012456463882699609 + 0.001 * 6.908013820648193
Epoch 1470, val loss: 2.8761184215545654
Epoch 1480, training loss: 0.008148156106472015 = 0.0012312732869759202 + 0.001 * 6.916882514953613
Epoch 1480, val loss: 2.8809685707092285
Epoch 1490, training loss: 0.008154748938977718 = 0.0012173076393082738 + 0.001 * 6.937440872192383
Epoch 1490, val loss: 2.8857505321502686
Epoch 1500, training loss: 0.008116430602967739 = 0.0012037374544888735 + 0.001 * 6.912692546844482
Epoch 1500, val loss: 2.890421152114868
Epoch 1510, training loss: 0.00811475608497858 = 0.0011905247811228037 + 0.001 * 6.924230575561523
Epoch 1510, val loss: 2.8950328826904297
Epoch 1520, training loss: 0.008119042962789536 = 0.001177700236439705 + 0.001 * 6.941341876983643
Epoch 1520, val loss: 2.8996551036834717
Epoch 1530, training loss: 0.008077221922576427 = 0.001165211433544755 + 0.001 * 6.9120097160339355
Epoch 1530, val loss: 2.9041264057159424
Epoch 1540, training loss: 0.00807532761245966 = 0.0011530730407685041 + 0.001 * 6.9222540855407715
Epoch 1540, val loss: 2.90852689743042
Epoch 1550, training loss: 0.008038945496082306 = 0.0011412565363571048 + 0.001 * 6.897688865661621
Epoch 1550, val loss: 2.9129061698913574
Epoch 1560, training loss: 0.008038321509957314 = 0.001129749114625156 + 0.001 * 6.908571720123291
Epoch 1560, val loss: 2.9171547889709473
Epoch 1570, training loss: 0.008043055422604084 = 0.0011185618350282311 + 0.001 * 6.924492835998535
Epoch 1570, val loss: 2.9214298725128174
Epoch 1580, training loss: 0.007998158223927021 = 0.0011076356749981642 + 0.001 * 6.890522480010986
Epoch 1580, val loss: 2.925600051879883
Epoch 1590, training loss: 0.007981311529874802 = 0.001097009633667767 + 0.001 * 6.88430118560791
Epoch 1590, val loss: 2.929699659347534
Epoch 1600, training loss: 0.007998757995665073 = 0.0010866582160815597 + 0.001 * 6.912099361419678
Epoch 1600, val loss: 2.933776378631592
Epoch 1610, training loss: 0.007989511825144291 = 0.0010765554616227746 + 0.001 * 6.9129557609558105
Epoch 1610, val loss: 2.9377636909484863
Epoch 1620, training loss: 0.007968942634761333 = 0.0010667335009202361 + 0.001 * 6.902209281921387
Epoch 1620, val loss: 2.941713809967041
Epoch 1630, training loss: 0.007965211756527424 = 0.001057141344062984 + 0.001 * 6.908070087432861
Epoch 1630, val loss: 2.9456117153167725
Epoch 1640, training loss: 0.007925723679363728 = 0.0010478049516677856 + 0.001 * 6.877918243408203
Epoch 1640, val loss: 2.949406623840332
Epoch 1650, training loss: 0.007910216227173805 = 0.0010386856738477945 + 0.001 * 6.871530532836914
Epoch 1650, val loss: 2.953199863433838
Epoch 1660, training loss: 0.007924593985080719 = 0.0010298063280060887 + 0.001 * 6.894787311553955
Epoch 1660, val loss: 2.956899881362915
Epoch 1670, training loss: 0.007901694625616074 = 0.0010211351327598095 + 0.001 * 6.880558967590332
Epoch 1670, val loss: 2.9605445861816406
Epoch 1680, training loss: 0.007879672572016716 = 0.0010126651031896472 + 0.001 * 6.867007255554199
Epoch 1680, val loss: 2.964162588119507
Epoch 1690, training loss: 0.007892237976193428 = 0.0010044206865131855 + 0.001 * 6.887816905975342
Epoch 1690, val loss: 2.967677116394043
Epoch 1700, training loss: 0.007881342433393002 = 0.0009963486809283495 + 0.001 * 6.884993553161621
Epoch 1700, val loss: 2.971224069595337
Epoch 1710, training loss: 0.007878229953348637 = 0.000988480867817998 + 0.001 * 6.889749050140381
Epoch 1710, val loss: 2.9746463298797607
Epoch 1720, training loss: 0.007870701141655445 = 0.0009807896567508578 + 0.001 * 6.889911651611328
Epoch 1720, val loss: 2.978025197982788
Epoch 1730, training loss: 0.007842347957193851 = 0.0009733001934364438 + 0.001 * 6.86904764175415
Epoch 1730, val loss: 2.9814093112945557
Epoch 1740, training loss: 0.007844234816730022 = 0.0009659663191996515 + 0.001 * 6.878268718719482
Epoch 1740, val loss: 2.984722375869751
Epoch 1750, training loss: 0.007832922972738743 = 0.0009587949025444686 + 0.001 * 6.874127388000488
Epoch 1750, val loss: 2.98799729347229
Epoch 1760, training loss: 0.007830190472304821 = 0.0009517863509245217 + 0.001 * 6.878404140472412
Epoch 1760, val loss: 2.9912025928497314
Epoch 1770, training loss: 0.007826244458556175 = 0.0009449259960092604 + 0.001 * 6.881318092346191
Epoch 1770, val loss: 2.9943203926086426
Epoch 1780, training loss: 0.007834811694920063 = 0.0009382300195284188 + 0.001 * 6.896581649780273
Epoch 1780, val loss: 2.9974236488342285
Epoch 1790, training loss: 0.007805257104337215 = 0.0009316807263530791 + 0.001 * 6.8735761642456055
Epoch 1790, val loss: 3.0005476474761963
Epoch 1800, training loss: 0.007803027983754873 = 0.0009252724121324718 + 0.001 * 6.877755641937256
Epoch 1800, val loss: 3.0036094188690186
Epoch 1810, training loss: 0.007793382741510868 = 0.000919002341106534 + 0.001 * 6.874379634857178
Epoch 1810, val loss: 3.0066452026367188
Epoch 1820, training loss: 0.007783766835927963 = 0.0009128556703217328 + 0.001 * 6.87091064453125
Epoch 1820, val loss: 3.0095694065093994
Epoch 1830, training loss: 0.0077644092962145805 = 0.0009068486397154629 + 0.001 * 6.857560157775879
Epoch 1830, val loss: 3.012521266937256
Epoch 1840, training loss: 0.007774676661938429 = 0.0009009702480398118 + 0.001 * 6.873705863952637
Epoch 1840, val loss: 3.015397071838379
Epoch 1850, training loss: 0.007760828360915184 = 0.0008952072239480913 + 0.001 * 6.8656206130981445
Epoch 1850, val loss: 3.018240451812744
Epoch 1860, training loss: 0.007753687910735607 = 0.0008895676583051682 + 0.001 * 6.864120006561279
Epoch 1860, val loss: 3.0209808349609375
Epoch 1870, training loss: 0.007743779104202986 = 0.0008840425871312618 + 0.001 * 6.85973596572876
Epoch 1870, val loss: 3.0237607955932617
Epoch 1880, training loss: 0.007733616977930069 = 0.0008786398102529347 + 0.001 * 6.854976654052734
Epoch 1880, val loss: 3.026458263397217
Epoch 1890, training loss: 0.007732768543064594 = 0.0008733411086723208 + 0.001 * 6.859426975250244
Epoch 1890, val loss: 3.0291967391967773
Epoch 1900, training loss: 0.0077468352392315865 = 0.000868152012117207 + 0.001 * 6.878683090209961
Epoch 1900, val loss: 3.031818151473999
Epoch 1910, training loss: 0.007745554205030203 = 0.0008630658849142492 + 0.001 * 6.882487773895264
Epoch 1910, val loss: 3.0344252586364746
Epoch 1920, training loss: 0.007692985702306032 = 0.0008580619469285011 + 0.001 * 6.834923267364502
Epoch 1920, val loss: 3.0369858741760254
Epoch 1930, training loss: 0.007716955617070198 = 0.0008531660423614085 + 0.001 * 6.863789081573486
Epoch 1930, val loss: 3.039517641067505
Epoch 1940, training loss: 0.007717748638242483 = 0.0008483718847855926 + 0.001 * 6.869376182556152
Epoch 1940, val loss: 3.0420424938201904
Epoch 1950, training loss: 0.007721802685409784 = 0.0008436552016064525 + 0.001 * 6.878147125244141
Epoch 1950, val loss: 3.0445268154144287
Epoch 1960, training loss: 0.007714217994362116 = 0.000839047715999186 + 0.001 * 6.8751702308654785
Epoch 1960, val loss: 3.0469987392425537
Epoch 1970, training loss: 0.0076941195875406265 = 0.000834508566185832 + 0.001 * 6.859610557556152
Epoch 1970, val loss: 3.0493621826171875
Epoch 1980, training loss: 0.007713921833783388 = 0.0008300692425109446 + 0.001 * 6.883852481842041
Epoch 1980, val loss: 3.051748752593994
Epoch 1990, training loss: 0.007688397541642189 = 0.0008257196168415248 + 0.001 * 6.862677574157715
Epoch 1990, val loss: 3.054159641265869
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 1.9533027410507202 = 1.9447059631347656 + 0.001 * 8.596832275390625
Epoch 0, val loss: 1.9430406093597412
Epoch 10, training loss: 1.942564845085144 = 1.9339680671691895 + 0.001 * 8.596769332885742
Epoch 10, val loss: 1.932977318763733
Epoch 20, training loss: 1.9289270639419556 = 1.92033052444458 + 0.001 * 8.596593856811523
Epoch 20, val loss: 1.9196996688842773
Epoch 30, training loss: 1.909740924835205 = 1.9011447429656982 + 0.001 * 8.596210479736328
Epoch 30, val loss: 1.900808334350586
Epoch 40, training loss: 1.882082462310791 = 1.8734872341156006 + 0.001 * 8.595247268676758
Epoch 40, val loss: 1.8740911483764648
Epoch 50, training loss: 1.845862865447998 = 1.837270736694336 + 0.001 * 8.592174530029297
Epoch 50, val loss: 1.8410286903381348
Epoch 60, training loss: 1.8098076581954956 = 1.8012278079986572 + 0.001 * 8.579806327819824
Epoch 60, val loss: 1.8114386796951294
Epoch 70, training loss: 1.7792876958847046 = 1.770770788192749 + 0.001 * 8.516928672790527
Epoch 70, val loss: 1.7854809761047363
Epoch 80, training loss: 1.7374552488327026 = 1.7293238639831543 + 0.001 * 8.131377220153809
Epoch 80, val loss: 1.7469698190689087
Epoch 90, training loss: 1.6796610355377197 = 1.6716691255569458 + 0.001 * 7.991896152496338
Epoch 90, val loss: 1.6968998908996582
Epoch 100, training loss: 1.6041897535324097 = 1.596232295036316 + 0.001 * 7.957457542419434
Epoch 100, val loss: 1.6348037719726562
Epoch 110, training loss: 1.5208313465118408 = 1.512891173362732 + 0.001 * 7.940158367156982
Epoch 110, val loss: 1.565819263458252
Epoch 120, training loss: 1.4388147592544556 = 1.4309111833572388 + 0.001 * 7.903584003448486
Epoch 120, val loss: 1.4981569051742554
Epoch 130, training loss: 1.3601735830307007 = 1.3524022102355957 + 0.001 * 7.7713775634765625
Epoch 130, val loss: 1.4336615800857544
Epoch 140, training loss: 1.2823776006698608 = 1.2749615907669067 + 0.001 * 7.415960788726807
Epoch 140, val loss: 1.370728611946106
Epoch 150, training loss: 1.2035152912139893 = 1.1961445808410645 + 0.001 * 7.370665073394775
Epoch 150, val loss: 1.3097792863845825
Epoch 160, training loss: 1.1227060556411743 = 1.1153764724731445 + 0.001 * 7.3296356201171875
Epoch 160, val loss: 1.2504165172576904
Epoch 170, training loss: 1.039480209350586 = 1.0321781635284424 + 0.001 * 7.302095890045166
Epoch 170, val loss: 1.1908100843429565
Epoch 180, training loss: 0.9544572234153748 = 0.9471674561500549 + 0.001 * 7.289746284484863
Epoch 180, val loss: 1.131607174873352
Epoch 190, training loss: 0.869560956954956 = 0.8622764945030212 + 0.001 * 7.284457206726074
Epoch 190, val loss: 1.0736141204833984
Epoch 200, training loss: 0.7875932455062866 = 0.7803128361701965 + 0.001 * 7.280410289764404
Epoch 200, val loss: 1.0197466611862183
Epoch 210, training loss: 0.7111716866493225 = 0.7038954496383667 + 0.001 * 7.276247501373291
Epoch 210, val loss: 0.9721287488937378
Epoch 220, training loss: 0.6419864296913147 = 0.6347132921218872 + 0.001 * 7.27310848236084
Epoch 220, val loss: 0.9326478838920593
Epoch 230, training loss: 0.5803253650665283 = 0.5730544924736023 + 0.001 * 7.270888328552246
Epoch 230, val loss: 0.901756227016449
Epoch 240, training loss: 0.5252730846405029 = 0.5180038809776306 + 0.001 * 7.269185543060303
Epoch 240, val loss: 0.8786458373069763
Epoch 250, training loss: 0.4752948582172394 = 0.4680272340774536 + 0.001 * 7.267619609832764
Epoch 250, val loss: 0.8616573810577393
Epoch 260, training loss: 0.42881232500076294 = 0.42154672741889954 + 0.001 * 7.265585899353027
Epoch 260, val loss: 0.8491808176040649
Epoch 270, training loss: 0.3846154808998108 = 0.37735337018966675 + 0.001 * 7.262112617492676
Epoch 270, val loss: 0.8401317000389099
Epoch 280, training loss: 0.3421545624732971 = 0.33489909768104553 + 0.001 * 7.255458354949951
Epoch 280, val loss: 0.8336594104766846
Epoch 290, training loss: 0.3014999032020569 = 0.2942555248737335 + 0.001 * 7.24437141418457
Epoch 290, val loss: 0.8300501108169556
Epoch 300, training loss: 0.2631989121437073 = 0.255968302488327 + 0.001 * 7.2305989265441895
Epoch 300, val loss: 0.8291370272636414
Epoch 310, training loss: 0.22799800336360931 = 0.22078865766525269 + 0.001 * 7.209339141845703
Epoch 310, val loss: 0.8309561014175415
Epoch 320, training loss: 0.19658827781677246 = 0.1893893927335739 + 0.001 * 7.198881149291992
Epoch 320, val loss: 0.8354876637458801
Epoch 330, training loss: 0.16925272345542908 = 0.16207003593444824 + 0.001 * 7.1826934814453125
Epoch 330, val loss: 0.8424708247184753
Epoch 340, training loss: 0.14587950706481934 = 0.13870811462402344 + 0.001 * 7.171398639678955
Epoch 340, val loss: 0.8513752818107605
Epoch 350, training loss: 0.12611012160778046 = 0.11894038319587708 + 0.001 * 7.169742107391357
Epoch 350, val loss: 0.8619482517242432
Epoch 360, training loss: 0.10945641994476318 = 0.10229554027318954 + 0.001 * 7.1608757972717285
Epoch 360, val loss: 0.8737979531288147
Epoch 370, training loss: 0.0954672172665596 = 0.0883069708943367 + 0.001 * 7.160248279571533
Epoch 370, val loss: 0.886541485786438
Epoch 380, training loss: 0.08371686190366745 = 0.07655935734510422 + 0.001 * 7.157506465911865
Epoch 380, val loss: 0.899955153465271
Epoch 390, training loss: 0.07384854555130005 = 0.06669364869594574 + 0.001 * 7.154899597167969
Epoch 390, val loss: 0.913702130317688
Epoch 400, training loss: 0.06555264443159103 = 0.05839722231030464 + 0.001 * 7.155418395996094
Epoch 400, val loss: 0.9275957942008972
Epoch 410, training loss: 0.05855703726410866 = 0.051408007740974426 + 0.001 * 7.149029731750488
Epoch 410, val loss: 0.9415163397789001
Epoch 420, training loss: 0.0526493564248085 = 0.04550052434206009 + 0.001 * 7.148830413818359
Epoch 420, val loss: 0.9554485082626343
Epoch 430, training loss: 0.047633275389671326 = 0.04048794507980347 + 0.001 * 7.145328998565674
Epoch 430, val loss: 0.9691516757011414
Epoch 440, training loss: 0.04335426911711693 = 0.03621499612927437 + 0.001 * 7.139273166656494
Epoch 440, val loss: 0.982653021812439
Epoch 450, training loss: 0.03968331217765808 = 0.03255421668291092 + 0.001 * 7.129095554351807
Epoch 450, val loss: 0.9957977533340454
Epoch 460, training loss: 0.036523643881082535 = 0.02939853072166443 + 0.001 * 7.125113010406494
Epoch 460, val loss: 1.0086883306503296
Epoch 470, training loss: 0.033782001584768295 = 0.026657773181796074 + 0.001 * 7.124228000640869
Epoch 470, val loss: 1.0212633609771729
Epoch 480, training loss: 0.03137527406215668 = 0.02426544763147831 + 0.001 * 7.109827995300293
Epoch 480, val loss: 1.0334311723709106
Epoch 490, training loss: 0.029276669025421143 = 0.022168109193444252 + 0.001 * 7.1085591316223145
Epoch 490, val loss: 1.0452792644500732
Epoch 500, training loss: 0.027420593425631523 = 0.020321784541010857 + 0.001 * 7.098809242248535
Epoch 500, val loss: 1.0567626953125
Epoch 510, training loss: 0.025781644508242607 = 0.018690407276153564 + 0.001 * 7.0912370681762695
Epoch 510, val loss: 1.0679720640182495
Epoch 520, training loss: 0.02432907186448574 = 0.017244089394807816 + 0.001 * 7.084982395172119
Epoch 520, val loss: 1.0787276029586792
Epoch 530, training loss: 0.023035500198602676 = 0.01595751941204071 + 0.001 * 7.077981472015381
Epoch 530, val loss: 1.08919358253479
Epoch 540, training loss: 0.021891698241233826 = 0.014809361658990383 + 0.001 * 7.082336902618408
Epoch 540, val loss: 1.0993235111236572
Epoch 550, training loss: 0.02086082100868225 = 0.013781603425741196 + 0.001 * 7.07921838760376
Epoch 550, val loss: 1.1091554164886475
Epoch 560, training loss: 0.019932124763727188 = 0.012858431786298752 + 0.001 * 7.073692321777344
Epoch 560, val loss: 1.118695616722107
Epoch 570, training loss: 0.019092820584774017 = 0.012026481330394745 + 0.001 * 7.06633996963501
Epoch 570, val loss: 1.1279411315917969
Epoch 580, training loss: 0.018344396725296974 = 0.011274510063230991 + 0.001 * 7.069886207580566
Epoch 580, val loss: 1.1368809938430786
Epoch 590, training loss: 0.017657669261097908 = 0.010593018494546413 + 0.001 * 7.064650058746338
Epoch 590, val loss: 1.1455750465393066
Epoch 600, training loss: 0.017031950876116753 = 0.009973568841814995 + 0.001 * 7.0583815574646
Epoch 600, val loss: 1.1539826393127441
Epoch 610, training loss: 0.016490627080202103 = 0.009409024380147457 + 0.001 * 7.081602573394775
Epoch 610, val loss: 1.1621047258377075
Epoch 620, training loss: 0.015951281413435936 = 0.008893243968486786 + 0.001 * 7.058036804199219
Epoch 620, val loss: 1.1700907945632935
Epoch 630, training loss: 0.015475578606128693 = 0.008420722559094429 + 0.001 * 7.0548553466796875
Epoch 630, val loss: 1.1777865886688232
Epoch 640, training loss: 0.015044737607240677 = 0.007986815646290779 + 0.001 * 7.057921409606934
Epoch 640, val loss: 1.185276746749878
Epoch 650, training loss: 0.014642703346908092 = 0.007587440777570009 + 0.001 * 7.055262088775635
Epoch 650, val loss: 1.1925393342971802
Epoch 660, training loss: 0.014277368783950806 = 0.007219091523438692 + 0.001 * 7.058277130126953
Epoch 660, val loss: 1.199623703956604
Epoch 670, training loss: 0.013926951214671135 = 0.006878666114062071 + 0.001 * 7.048285007476807
Epoch 670, val loss: 1.2065147161483765
Epoch 680, training loss: 0.01364530622959137 = 0.006563378963619471 + 0.001 * 7.081926345825195
Epoch 680, val loss: 1.2132083177566528
Epoch 690, training loss: 0.013323487713932991 = 0.006270797923207283 + 0.001 * 7.052690029144287
Epoch 690, val loss: 1.2197542190551758
Epoch 700, training loss: 0.013044469058513641 = 0.005998665001243353 + 0.001 * 7.045803070068359
Epoch 700, val loss: 1.2261189222335815
Epoch 710, training loss: 0.012787893414497375 = 0.005744827911257744 + 0.001 * 7.043065071105957
Epoch 710, val loss: 1.2323176860809326
Epoch 720, training loss: 0.012553837150335312 = 0.005507179535925388 + 0.001 * 7.046657562255859
Epoch 720, val loss: 1.2383947372436523
Epoch 730, training loss: 0.012336693704128265 = 0.00528367143124342 + 0.001 * 7.053021430969238
Epoch 730, val loss: 1.244360089302063
Epoch 740, training loss: 0.012112250551581383 = 0.0050727613270282745 + 0.001 * 7.039489269256592
Epoch 740, val loss: 1.2502278089523315
Epoch 750, training loss: 0.011906051076948643 = 0.0048732212744653225 + 0.001 * 7.032829284667969
Epoch 750, val loss: 1.2560495138168335
Epoch 760, training loss: 0.011726919561624527 = 0.004683952312916517 + 0.001 * 7.042967319488525
Epoch 760, val loss: 1.2618036270141602
Epoch 770, training loss: 0.011544722132384777 = 0.0045044575817883015 + 0.001 * 7.040264129638672
Epoch 770, val loss: 1.267535924911499
Epoch 780, training loss: 0.011368410661816597 = 0.004334297031164169 + 0.001 * 7.03411340713501
Epoch 780, val loss: 1.2732264995574951
Epoch 790, training loss: 0.011202609166502953 = 0.0041731917299330235 + 0.001 * 7.029416561126709
Epoch 790, val loss: 1.2788488864898682
Epoch 800, training loss: 0.011054648086428642 = 0.004020709078758955 + 0.001 * 7.033938884735107
Epoch 800, val loss: 1.2844005823135376
Epoch 810, training loss: 0.010917903855443 = 0.0038764961063861847 + 0.001 * 7.041407108306885
Epoch 810, val loss: 1.2898914813995361
Epoch 820, training loss: 0.010764316655695438 = 0.0037402217276394367 + 0.001 * 7.024094581604004
Epoch 820, val loss: 1.295275092124939
Epoch 830, training loss: 0.010656718164682388 = 0.003611353226006031 + 0.001 * 7.0453643798828125
Epoch 830, val loss: 1.3005751371383667
Epoch 840, training loss: 0.010512561537325382 = 0.003489603055641055 + 0.001 * 7.022958278656006
Epoch 840, val loss: 1.3058364391326904
Epoch 850, training loss: 0.010415589436888695 = 0.003374484134837985 + 0.001 * 7.041104793548584
Epoch 850, val loss: 1.3109264373779297
Epoch 860, training loss: 0.010285204276442528 = 0.00326569308526814 + 0.001 * 7.019510746002197
Epoch 860, val loss: 1.3159854412078857
Epoch 870, training loss: 0.010194946080446243 = 0.0031627933494746685 + 0.001 * 7.03215217590332
Epoch 870, val loss: 1.320866346359253
Epoch 880, training loss: 0.010092023760080338 = 0.0030653735157102346 + 0.001 * 7.0266499519348145
Epoch 880, val loss: 1.3256999254226685
Epoch 890, training loss: 0.00998950656503439 = 0.0029730757232755423 + 0.001 * 7.016430377960205
Epoch 890, val loss: 1.33043372631073
Epoch 900, training loss: 0.009900175034999847 = 0.0028855614364147186 + 0.001 * 7.014613628387451
Epoch 900, val loss: 1.335060477256775
Epoch 910, training loss: 0.009813029319047928 = 0.0028025407809764147 + 0.001 * 7.010488033294678
Epoch 910, val loss: 1.3395967483520508
Epoch 920, training loss: 0.00974326767027378 = 0.0027237068861722946 + 0.001 * 7.019559860229492
Epoch 920, val loss: 1.3440290689468384
Epoch 930, training loss: 0.009655695408582687 = 0.002648785011842847 + 0.001 * 7.0069098472595215
Epoch 930, val loss: 1.34837806224823
Epoch 940, training loss: 0.009601120837032795 = 0.0025775067042559385 + 0.001 * 7.023613452911377
Epoch 940, val loss: 1.3526263236999512
Epoch 950, training loss: 0.009522054344415665 = 0.0025096461176872253 + 0.001 * 7.012407302856445
Epoch 950, val loss: 1.356796145439148
Epoch 960, training loss: 0.009458553045988083 = 0.0024449860211461782 + 0.001 * 7.013566493988037
Epoch 960, val loss: 1.3608824014663696
Epoch 970, training loss: 0.009382043033838272 = 0.00238334434106946 + 0.001 * 6.998698711395264
Epoch 970, val loss: 1.364870548248291
Epoch 980, training loss: 0.009325031191110611 = 0.002324536442756653 + 0.001 * 7.000494003295898
Epoch 980, val loss: 1.3687782287597656
Epoch 990, training loss: 0.009263571351766586 = 0.0022683697752654552 + 0.001 * 6.995201587677002
Epoch 990, val loss: 1.3726210594177246
Epoch 1000, training loss: 0.009223729372024536 = 0.0022146846167743206 + 0.001 * 7.009044170379639
Epoch 1000, val loss: 1.3763986825942993
Epoch 1010, training loss: 0.009166447445750237 = 0.002163338242098689 + 0.001 * 7.003108501434326
Epoch 1010, val loss: 1.380088210105896
Epoch 1020, training loss: 0.009119759313762188 = 0.0021142226178199053 + 0.001 * 7.005536079406738
Epoch 1020, val loss: 1.3837099075317383
Epoch 1030, training loss: 0.009070474654436111 = 0.002067175228148699 + 0.001 * 7.003298759460449
Epoch 1030, val loss: 1.3872489929199219
Epoch 1040, training loss: 0.009011615067720413 = 0.002022079424932599 + 0.001 * 6.989534854888916
Epoch 1040, val loss: 1.3907285928726196
Epoch 1050, training loss: 0.008991514332592487 = 0.0019788227509707212 + 0.001 * 7.012691020965576
Epoch 1050, val loss: 1.394142508506775
Epoch 1060, training loss: 0.008929066359996796 = 0.0019373279064893723 + 0.001 * 6.9917378425598145
Epoch 1060, val loss: 1.3974900245666504
Epoch 1070, training loss: 0.008884744718670845 = 0.0018974683480337262 + 0.001 * 6.98727560043335
Epoch 1070, val loss: 1.4007617235183716
Epoch 1080, training loss: 0.00886251125484705 = 0.001859171548858285 + 0.001 * 7.0033392906188965
Epoch 1080, val loss: 1.4039969444274902
Epoch 1090, training loss: 0.008809718303382397 = 0.0018223706865683198 + 0.001 * 6.98734712600708
Epoch 1090, val loss: 1.4071807861328125
Epoch 1100, training loss: 0.008802895434200764 = 0.0017869778675958514 + 0.001 * 7.015917778015137
Epoch 1100, val loss: 1.410264253616333
Epoch 1110, training loss: 0.008738473989069462 = 0.0017529473407194018 + 0.001 * 6.9855265617370605
Epoch 1110, val loss: 1.41335129737854
Epoch 1120, training loss: 0.008727386593818665 = 0.0017201497685164213 + 0.001 * 7.007236957550049
Epoch 1120, val loss: 1.4163204431533813
Epoch 1130, training loss: 0.008677296340465546 = 0.0016885583754628897 + 0.001 * 6.9887375831604
Epoch 1130, val loss: 1.419286847114563
Epoch 1140, training loss: 0.008659702725708485 = 0.0016581256641075015 + 0.001 * 7.0015764236450195
Epoch 1140, val loss: 1.42217218875885
Epoch 1150, training loss: 0.008604076690971851 = 0.001628795056603849 + 0.001 * 6.975281238555908
Epoch 1150, val loss: 1.4250013828277588
Epoch 1160, training loss: 0.00857621431350708 = 0.0016004778444766998 + 0.001 * 6.975735664367676
Epoch 1160, val loss: 1.427790880203247
Epoch 1170, training loss: 0.008563985116779804 = 0.0015731288585811853 + 0.001 * 6.990856170654297
Epoch 1170, val loss: 1.4305644035339355
Epoch 1180, training loss: 0.008525961078703403 = 0.0015467741759493947 + 0.001 * 6.979186058044434
Epoch 1180, val loss: 1.4332290887832642
Epoch 1190, training loss: 0.008503016084432602 = 0.0015213084407150745 + 0.001 * 6.981707572937012
Epoch 1190, val loss: 1.4358967542648315
Epoch 1200, training loss: 0.008479051291942596 = 0.0014967056922614574 + 0.001 * 6.982345104217529
Epoch 1200, val loss: 1.438476800918579
Epoch 1210, training loss: 0.008464318700134754 = 0.0014729163376614451 + 0.001 * 6.9914021492004395
Epoch 1210, val loss: 1.4410301446914673
Epoch 1220, training loss: 0.008424808271229267 = 0.0014499189564958215 + 0.001 * 6.974889278411865
Epoch 1220, val loss: 1.443564772605896
Epoch 1230, training loss: 0.008396378718316555 = 0.0014276570873335004 + 0.001 * 6.96872091293335
Epoch 1230, val loss: 1.446013331413269
Epoch 1240, training loss: 0.008404871448874474 = 0.0014060696121305227 + 0.001 * 6.998801231384277
Epoch 1240, val loss: 1.4484753608703613
Epoch 1250, training loss: 0.008361383341252804 = 0.0013851624680683017 + 0.001 * 6.976220607757568
Epoch 1250, val loss: 1.4508568048477173
Epoch 1260, training loss: 0.008377271704375744 = 0.0013648431049659848 + 0.001 * 7.012428283691406
Epoch 1260, val loss: 1.4532191753387451
Epoch 1270, training loss: 0.008307594805955887 = 0.001345094875432551 + 0.001 * 6.962499618530273
Epoch 1270, val loss: 1.455575704574585
Epoch 1280, training loss: 0.008297056891024113 = 0.001325826975516975 + 0.001 * 6.9712300300598145
Epoch 1280, val loss: 1.4578574895858765
Epoch 1290, training loss: 0.008284861221909523 = 0.001307001686654985 + 0.001 * 6.977859020233154
Epoch 1290, val loss: 1.4601670503616333
Epoch 1300, training loss: 0.008240235038101673 = 0.0012885223841294646 + 0.001 * 6.951712131500244
Epoch 1300, val loss: 1.462440848350525
Epoch 1310, training loss: 0.00824237335473299 = 0.0012702923268079758 + 0.001 * 6.972080707550049
Epoch 1310, val loss: 1.4647074937820435
Epoch 1320, training loss: 0.008206269703805447 = 0.0012522462056949735 + 0.001 * 6.954023361206055
Epoch 1320, val loss: 1.4670004844665527
Epoch 1330, training loss: 0.008193767629563808 = 0.0012343872804194689 + 0.001 * 6.959380149841309
Epoch 1330, val loss: 1.4692413806915283
Epoch 1340, training loss: 0.00820851605385542 = 0.001216607866808772 + 0.001 * 6.991907596588135
Epoch 1340, val loss: 1.4715713262557983
Epoch 1350, training loss: 0.008150476962327957 = 0.0011989575577899814 + 0.001 * 6.95151948928833
Epoch 1350, val loss: 1.4738662242889404
Epoch 1360, training loss: 0.008190273307263851 = 0.0011813793098554015 + 0.001 * 7.0088934898376465
Epoch 1360, val loss: 1.4761509895324707
Epoch 1370, training loss: 0.008122703991830349 = 0.0011639372678473592 + 0.001 * 6.958766460418701
Epoch 1370, val loss: 1.4785069227218628
Epoch 1380, training loss: 0.008123468607664108 = 0.0011466508731245995 + 0.001 * 6.9768171310424805
Epoch 1380, val loss: 1.4807806015014648
Epoch 1390, training loss: 0.008078751154243946 = 0.00112958496902138 + 0.001 * 6.949165344238281
Epoch 1390, val loss: 1.4831308126449585
Epoch 1400, training loss: 0.008071175776422024 = 0.0011127212783321738 + 0.001 * 6.958454608917236
Epoch 1400, val loss: 1.4853895902633667
Epoch 1410, training loss: 0.008057994768023491 = 0.0010961425723508 + 0.001 * 6.961852073669434
Epoch 1410, val loss: 1.4877394437789917
Epoch 1420, training loss: 0.008028725162148476 = 0.00107998785097152 + 0.001 * 6.948736667633057
Epoch 1420, val loss: 1.4899619817733765
Epoch 1430, training loss: 0.008004468865692616 = 0.0010641132248565555 + 0.001 * 6.9403557777404785
Epoch 1430, val loss: 1.492201328277588
Epoch 1440, training loss: 0.007990997284650803 = 0.0010485855164006352 + 0.001 * 6.942411422729492
Epoch 1440, val loss: 1.4944322109222412
Epoch 1450, training loss: 0.007977276109158993 = 0.0010334348771721125 + 0.001 * 6.943840980529785
Epoch 1450, val loss: 1.496626615524292
Epoch 1460, training loss: 0.007986444048583508 = 0.0010186521103605628 + 0.001 * 6.967791557312012
Epoch 1460, val loss: 1.4988067150115967
Epoch 1470, training loss: 0.007933846674859524 = 0.0010042436188086867 + 0.001 * 6.929603099822998
Epoch 1470, val loss: 1.5009734630584717
Epoch 1480, training loss: 0.00793500803411007 = 0.0009902107995003462 + 0.001 * 6.944796562194824
Epoch 1480, val loss: 1.5030639171600342
Epoch 1490, training loss: 0.007930395193397999 = 0.0009765286813490093 + 0.001 * 6.953866481781006
Epoch 1490, val loss: 1.505179524421692
Epoch 1500, training loss: 0.007917272858321667 = 0.0009633238660171628 + 0.001 * 6.953948497772217
Epoch 1500, val loss: 1.507200837135315
Epoch 1510, training loss: 0.007924799807369709 = 0.000950473069678992 + 0.001 * 6.9743266105651855
Epoch 1510, val loss: 1.5091781616210938
Epoch 1520, training loss: 0.007870852947235107 = 0.0009380652918480337 + 0.001 * 6.932787895202637
Epoch 1520, val loss: 1.511228322982788
Epoch 1530, training loss: 0.007874341681599617 = 0.0009259952930733562 + 0.001 * 6.948346138000488
Epoch 1530, val loss: 1.5130648612976074
Epoch 1540, training loss: 0.007858894765377045 = 0.0009142602793872356 + 0.001 * 6.944634437561035
Epoch 1540, val loss: 1.5150902271270752
Epoch 1550, training loss: 0.007831374183297157 = 0.0009028820204548538 + 0.001 * 6.928491592407227
Epoch 1550, val loss: 1.5169026851654053
Epoch 1560, training loss: 0.00786226149648428 = 0.0008918377570807934 + 0.001 * 6.970423221588135
Epoch 1560, val loss: 1.518723964691162
Epoch 1570, training loss: 0.007817511446774006 = 0.0008811530424281955 + 0.001 * 6.9363579750061035
Epoch 1570, val loss: 1.5206105709075928
Epoch 1580, training loss: 0.007805051282048225 = 0.0008707721717655659 + 0.001 * 6.934278964996338
Epoch 1580, val loss: 1.5222848653793335
Epoch 1590, training loss: 0.007796094287186861 = 0.0008607044001109898 + 0.001 * 6.935389518737793
Epoch 1590, val loss: 1.5241044759750366
Epoch 1600, training loss: 0.0077698915265500546 = 0.000850927026476711 + 0.001 * 6.91896390914917
Epoch 1600, val loss: 1.5257266759872437
Epoch 1610, training loss: 0.007765831891447306 = 0.0008414378389716148 + 0.001 * 6.924393653869629
Epoch 1610, val loss: 1.527474045753479
Epoch 1620, training loss: 0.0077432673424482346 = 0.0008322616922669113 + 0.001 * 6.91100549697876
Epoch 1620, val loss: 1.5290719270706177
Epoch 1630, training loss: 0.007775665260851383 = 0.0008233822300098836 + 0.001 * 6.952282428741455
Epoch 1630, val loss: 1.5306495428085327
Epoch 1640, training loss: 0.007730448618531227 = 0.0008147975895553827 + 0.001 * 6.915650367736816
Epoch 1640, val loss: 1.5322784185409546
Epoch 1650, training loss: 0.007714397739619017 = 0.0008064701105467975 + 0.001 * 6.907927513122559
Epoch 1650, val loss: 1.5337512493133545
Epoch 1660, training loss: 0.00775910122320056 = 0.000798382970970124 + 0.001 * 6.960718154907227
Epoch 1660, val loss: 1.5353212356567383
Epoch 1670, training loss: 0.007701063062995672 = 0.0007905321544967592 + 0.001 * 6.9105305671691895
Epoch 1670, val loss: 1.5367746353149414
Epoch 1680, training loss: 0.007685923017561436 = 0.0007829113746993244 + 0.001 * 6.903011322021484
Epoch 1680, val loss: 1.5381852388381958
Epoch 1690, training loss: 0.007692109793424606 = 0.0007755050901323557 + 0.001 * 6.916604518890381
Epoch 1690, val loss: 1.53964364528656
Epoch 1700, training loss: 0.00768880732357502 = 0.0007683386793360114 + 0.001 * 6.920468330383301
Epoch 1700, val loss: 1.541016936302185
Epoch 1710, training loss: 0.007680043578147888 = 0.000761350616812706 + 0.001 * 6.918692588806152
Epoch 1710, val loss: 1.542358160018921
Epoch 1720, training loss: 0.007662451360374689 = 0.0007545783882960677 + 0.001 * 6.907872676849365
Epoch 1720, val loss: 1.5437262058258057
Epoch 1730, training loss: 0.007668983656913042 = 0.0007479719934053719 + 0.001 * 6.921010971069336
Epoch 1730, val loss: 1.5449997186660767
Epoch 1740, training loss: 0.007665947545319796 = 0.0007415724685415626 + 0.001 * 6.924374580383301
Epoch 1740, val loss: 1.546355128288269
Epoch 1750, training loss: 0.007643171586096287 = 0.0007354012341238558 + 0.001 * 6.907770156860352
Epoch 1750, val loss: 1.5474979877471924
Epoch 1760, training loss: 0.007625157944858074 = 0.0007294020033441484 + 0.001 * 6.895755767822266
Epoch 1760, val loss: 1.5487561225891113
Epoch 1770, training loss: 0.007626093924045563 = 0.0007236010860651731 + 0.001 * 6.902492046356201
Epoch 1770, val loss: 1.5499238967895508
Epoch 1780, training loss: 0.007611733861267567 = 0.0007179616950452328 + 0.001 * 6.893771648406982
Epoch 1780, val loss: 1.5510380268096924
Epoch 1790, training loss: 0.007643047254532576 = 0.0007124895928427577 + 0.001 * 6.9305572509765625
Epoch 1790, val loss: 1.5522123575210571
Epoch 1800, training loss: 0.007609889376908541 = 0.0007071918807923794 + 0.001 * 6.9026970863342285
Epoch 1800, val loss: 1.5533109903335571
Epoch 1810, training loss: 0.007597820833325386 = 0.0007020225166343153 + 0.001 * 6.8957977294921875
Epoch 1810, val loss: 1.554323434829712
Epoch 1820, training loss: 0.007611740846186876 = 0.000697003270033747 + 0.001 * 6.914737224578857
Epoch 1820, val loss: 1.5554383993148804
Epoch 1830, training loss: 0.00765202147886157 = 0.0006921222084201872 + 0.001 * 6.959898948669434
Epoch 1830, val loss: 1.556465983390808
Epoch 1840, training loss: 0.00758797163143754 = 0.0006873706006444991 + 0.001 * 6.900600910186768
Epoch 1840, val loss: 1.55744469165802
Epoch 1850, training loss: 0.007582235615700483 = 0.0006827422766946256 + 0.001 * 6.8994927406311035
Epoch 1850, val loss: 1.5584416389465332
Epoch 1860, training loss: 0.007562914863228798 = 0.0006782487034797668 + 0.001 * 6.8846659660339355
Epoch 1860, val loss: 1.5594062805175781
Epoch 1870, training loss: 0.007587726693600416 = 0.0006739091477356851 + 0.001 * 6.913816928863525
Epoch 1870, val loss: 1.5603450536727905
Epoch 1880, training loss: 0.007573670707643032 = 0.0006696967175230384 + 0.001 * 6.903973579406738
Epoch 1880, val loss: 1.5612269639968872
Epoch 1890, training loss: 0.007544331252574921 = 0.0006656052428297698 + 0.001 * 6.878726005554199
Epoch 1890, val loss: 1.562102198600769
Epoch 1900, training loss: 0.007576342672109604 = 0.0006616055616177619 + 0.001 * 6.914736747741699
Epoch 1900, val loss: 1.5629689693450928
Epoch 1910, training loss: 0.007579379249364138 = 0.0006577114691026509 + 0.001 * 6.921667098999023
Epoch 1910, val loss: 1.563841462135315
Epoch 1920, training loss: 0.007544014602899551 = 0.0006539178430102766 + 0.001 * 6.890096187591553
Epoch 1920, val loss: 1.5646138191223145
Epoch 1930, training loss: 0.007554230745881796 = 0.0006501851021312177 + 0.001 * 6.904045104980469
Epoch 1930, val loss: 1.565481424331665
Epoch 1940, training loss: 0.0075395493768155575 = 0.0006465598708018661 + 0.001 * 6.892989158630371
Epoch 1940, val loss: 1.5662463903427124
Epoch 1950, training loss: 0.007523670326918364 = 0.0006430124631151557 + 0.001 * 6.880657196044922
Epoch 1950, val loss: 1.5670212507247925
Epoch 1960, training loss: 0.007516852580010891 = 0.0006395441014319658 + 0.001 * 6.877308368682861
Epoch 1960, val loss: 1.5678095817565918
Epoch 1970, training loss: 0.007557970006018877 = 0.0006361623527482152 + 0.001 * 6.921807289123535
Epoch 1970, val loss: 1.568528413772583
Epoch 1980, training loss: 0.007529709953814745 = 0.0006328662275336683 + 0.001 * 6.896843433380127
Epoch 1980, val loss: 1.569309949874878
Epoch 1990, training loss: 0.007556389085948467 = 0.000629655783995986 + 0.001 * 6.926733016967773
Epoch 1990, val loss: 1.5699695348739624
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 1.94245445728302 = 1.9338575601577759 + 0.001 * 8.596850395202637
Epoch 0, val loss: 1.9346184730529785
Epoch 10, training loss: 1.933068871498108 = 1.9244720935821533 + 0.001 * 8.596795082092285
Epoch 10, val loss: 1.924570083618164
Epoch 20, training loss: 1.9215431213378906 = 1.9129464626312256 + 0.001 * 8.596624374389648
Epoch 20, val loss: 1.912017822265625
Epoch 30, training loss: 1.9056015014648438 = 1.897005319595337 + 0.001 * 8.596219062805176
Epoch 30, val loss: 1.8945666551589966
Epoch 40, training loss: 1.8823210000991821 = 1.8737257719039917 + 0.001 * 8.595206260681152
Epoch 40, val loss: 1.869408369064331
Epoch 50, training loss: 1.8505915403366089 = 1.8419992923736572 + 0.001 * 8.592269897460938
Epoch 50, val loss: 1.8367173671722412
Epoch 60, training loss: 1.8175206184387207 = 1.8089390993118286 + 0.001 * 8.581504821777344
Epoch 60, val loss: 1.8067773580551147
Epoch 70, training loss: 1.7901172637939453 = 1.7815847396850586 + 0.001 * 8.53248405456543
Epoch 70, val loss: 1.785673975944519
Epoch 80, training loss: 1.7544193267822266 = 1.7461910247802734 + 0.001 * 8.2283296585083
Epoch 80, val loss: 1.7574833631515503
Epoch 90, training loss: 1.7036364078521729 = 1.6956089735031128 + 0.001 * 8.027436256408691
Epoch 90, val loss: 1.7152520418167114
Epoch 100, training loss: 1.6344908475875854 = 1.6265263557434082 + 0.001 * 7.964478492736816
Epoch 100, val loss: 1.657757043838501
Epoch 110, training loss: 1.5540364980697632 = 1.5461400747299194 + 0.001 * 7.896462440490723
Epoch 110, val loss: 1.592289686203003
Epoch 120, training loss: 1.4731067419052124 = 1.4653573036193848 + 0.001 * 7.749396800994873
Epoch 120, val loss: 1.5264191627502441
Epoch 130, training loss: 1.3942633867263794 = 1.3866994380950928 + 0.001 * 7.5639119148254395
Epoch 130, val loss: 1.4639081954956055
Epoch 140, training loss: 1.3137288093566895 = 1.306227445602417 + 0.001 * 7.501407623291016
Epoch 140, val loss: 1.4033360481262207
Epoch 150, training loss: 1.2291169166564941 = 1.2217116355895996 + 0.001 * 7.405325412750244
Epoch 150, val loss: 1.341023564338684
Epoch 160, training loss: 1.142491102218628 = 1.1351585388183594 + 0.001 * 7.332548141479492
Epoch 160, val loss: 1.2787444591522217
Epoch 170, training loss: 1.057929515838623 = 1.0506348609924316 + 0.001 * 7.294604301452637
Epoch 170, val loss: 1.2195837497711182
Epoch 180, training loss: 0.9778063297271729 = 0.9705254435539246 + 0.001 * 7.2808942794799805
Epoch 180, val loss: 1.1652977466583252
Epoch 190, training loss: 0.9013274312019348 = 0.8940543532371521 + 0.001 * 7.273049354553223
Epoch 190, val loss: 1.1142277717590332
Epoch 200, training loss: 0.8260363936424255 = 0.8187755942344666 + 0.001 * 7.2607927322387695
Epoch 200, val loss: 1.0648808479309082
Epoch 210, training loss: 0.7505767941474915 = 0.7433333992958069 + 0.001 * 7.243377208709717
Epoch 210, val loss: 1.0150234699249268
Epoch 220, training loss: 0.6760422587394714 = 0.6688222289085388 + 0.001 * 7.220029830932617
Epoch 220, val loss: 0.965957522392273
Epoch 230, training loss: 0.6046877503395081 = 0.5974878072738647 + 0.001 * 7.199965953826904
Epoch 230, val loss: 0.9200448393821716
Epoch 240, training loss: 0.5380468964576721 = 0.53086256980896 + 0.001 * 7.184311389923096
Epoch 240, val loss: 0.8803706765174866
Epoch 250, training loss: 0.4768202304840088 = 0.469642698764801 + 0.001 * 7.177530288696289
Epoch 250, val loss: 0.8485190868377686
Epoch 260, training loss: 0.42126837372779846 = 0.41409334540367126 + 0.001 * 7.175016403198242
Epoch 260, val loss: 0.8249602317810059
Epoch 270, training loss: 0.3713010847568512 = 0.3641265332698822 + 0.001 * 7.174557209014893
Epoch 270, val loss: 0.8082377314567566
Epoch 280, training loss: 0.3263373076915741 = 0.3191624581813812 + 0.001 * 7.174862861633301
Epoch 280, val loss: 0.7968995571136475
Epoch 290, training loss: 0.2856767773628235 = 0.2784990072250366 + 0.001 * 7.177758693695068
Epoch 290, val loss: 0.7898995280265808
Epoch 300, training loss: 0.24882471561431885 = 0.24164816737174988 + 0.001 * 7.176547050476074
Epoch 300, val loss: 0.7864972352981567
Epoch 310, training loss: 0.21564322710037231 = 0.2084653526544571 + 0.001 * 7.177876949310303
Epoch 310, val loss: 0.786382257938385
Epoch 320, training loss: 0.1862366795539856 = 0.17905892431735992 + 0.001 * 7.177755355834961
Epoch 320, val loss: 0.7893471121788025
Epoch 330, training loss: 0.16070325672626495 = 0.1535251885652542 + 0.001 * 7.178062438964844
Epoch 330, val loss: 0.7951327562332153
Epoch 340, training loss: 0.13894270360469818 = 0.13176071643829346 + 0.001 * 7.181979656219482
Epoch 340, val loss: 0.8032835125923157
Epoch 350, training loss: 0.12062159180641174 = 0.11344217509031296 + 0.001 * 7.179419040679932
Epoch 350, val loss: 0.813370406627655
Epoch 360, training loss: 0.10529627650976181 = 0.09811879694461823 + 0.001 * 7.1774773597717285
Epoch 360, val loss: 0.8249474763870239
Epoch 370, training loss: 0.09249062091112137 = 0.08531457930803299 + 0.001 * 7.176042079925537
Epoch 370, val loss: 0.8375391364097595
Epoch 380, training loss: 0.08176440745592117 = 0.07458885759115219 + 0.001 * 7.175546169281006
Epoch 380, val loss: 0.8508105278015137
Epoch 390, training loss: 0.07274225354194641 = 0.06556734442710876 + 0.001 * 7.174910545349121
Epoch 390, val loss: 0.8645333647727966
Epoch 400, training loss: 0.06511504203081131 = 0.057943280786275864 + 0.001 * 7.171762466430664
Epoch 400, val loss: 0.878512442111969
Epoch 410, training loss: 0.05863815173506737 = 0.05146710202097893 + 0.001 * 7.17104959487915
Epoch 410, val loss: 0.8925149440765381
Epoch 420, training loss: 0.05310782790184021 = 0.04593691602349281 + 0.001 * 7.170912742614746
Epoch 420, val loss: 0.906467854976654
Epoch 430, training loss: 0.04835937172174454 = 0.04119031876325607 + 0.001 * 7.1690521240234375
Epoch 430, val loss: 0.9202243089675903
Epoch 440, training loss: 0.044264137744903564 = 0.037096913903951645 + 0.001 * 7.167224407196045
Epoch 440, val loss: 0.9337322115898132
Epoch 450, training loss: 0.040715038776397705 = 0.03355012834072113 + 0.001 * 7.164911270141602
Epoch 450, val loss: 0.9469766616821289
Epoch 460, training loss: 0.037625424563884735 = 0.030463023111224174 + 0.001 * 7.162402153015137
Epoch 460, val loss: 0.9599571824073792
Epoch 470, training loss: 0.03492466360330582 = 0.02776401676237583 + 0.001 * 7.160646438598633
Epoch 470, val loss: 0.9726201891899109
Epoch 480, training loss: 0.032555487006902695 = 0.025395000353455544 + 0.001 * 7.160487651824951
Epoch 480, val loss: 0.9849146008491516
Epoch 490, training loss: 0.030465640127658844 = 0.023307137191295624 + 0.001 * 7.15850305557251
Epoch 490, val loss: 0.9969072341918945
Epoch 500, training loss: 0.02862374112010002 = 0.021459972485899925 + 0.001 * 7.163768291473389
Epoch 500, val loss: 1.0085110664367676
Epoch 510, training loss: 0.02697461098432541 = 0.019819866865873337 + 0.001 * 7.154743671417236
Epoch 510, val loss: 1.019768238067627
Epoch 520, training loss: 0.025509929284453392 = 0.01835818961262703 + 0.001 * 7.15173864364624
Epoch 520, val loss: 1.0306806564331055
Epoch 530, training loss: 0.024213388562202454 = 0.01705101877450943 + 0.001 * 7.1623687744140625
Epoch 530, val loss: 1.0412678718566895
Epoch 540, training loss: 0.023023873567581177 = 0.01587812788784504 + 0.001 * 7.145744323730469
Epoch 540, val loss: 1.0515533685684204
Epoch 550, training loss: 0.021982571110129356 = 0.0148224588483572 + 0.001 * 7.160111427307129
Epoch 550, val loss: 1.0615060329437256
Epoch 560, training loss: 0.021012941375374794 = 0.01386937778443098 + 0.001 * 7.143563747406006
Epoch 560, val loss: 1.0711476802825928
Epoch 570, training loss: 0.020146720111370087 = 0.013006414286792278 + 0.001 * 7.1403045654296875
Epoch 570, val loss: 1.0805068016052246
Epoch 580, training loss: 0.019392095506191254 = 0.012222839519381523 + 0.001 * 7.169254779815674
Epoch 580, val loss: 1.0895715951919556
Epoch 590, training loss: 0.018645135685801506 = 0.01150944922119379 + 0.001 * 7.135685920715332
Epoch 590, val loss: 1.098376750946045
Epoch 600, training loss: 0.017987310886383057 = 0.010858402587473392 + 0.001 * 7.128909111022949
Epoch 600, val loss: 1.1069111824035645
Epoch 610, training loss: 0.017418941482901573 = 0.01026277244091034 + 0.001 * 7.1561689376831055
Epoch 610, val loss: 1.1152033805847168
Epoch 620, training loss: 0.01684187352657318 = 0.009716548956930637 + 0.001 * 7.1253252029418945
Epoch 620, val loss: 1.1232435703277588
Epoch 630, training loss: 0.016336476430296898 = 0.009214501827955246 + 0.001 * 7.121974945068359
Epoch 630, val loss: 1.1310712099075317
Epoch 640, training loss: 0.015869177877902985 = 0.00875192042440176 + 0.001 * 7.117258071899414
Epoch 640, val loss: 1.1386547088623047
Epoch 650, training loss: 0.015444097109138966 = 0.008324266411364079 + 0.001 * 7.119830131530762
Epoch 650, val loss: 1.146056056022644
Epoch 660, training loss: 0.015037231147289276 = 0.007926329970359802 + 0.001 * 7.11090087890625
Epoch 660, val loss: 1.1532565355300903
Epoch 670, training loss: 0.01467175967991352 = 0.007553390692919493 + 0.001 * 7.118368148803711
Epoch 670, val loss: 1.1602802276611328
Epoch 680, training loss: 0.014323011040687561 = 0.007202965207397938 + 0.001 * 7.120046138763428
Epoch 680, val loss: 1.167176365852356
Epoch 690, training loss: 0.013969127088785172 = 0.006873137317597866 + 0.001 * 7.095989227294922
Epoch 690, val loss: 1.173919439315796
Epoch 700, training loss: 0.013652256689965725 = 0.006562917493283749 + 0.001 * 7.089338779449463
Epoch 700, val loss: 1.1805222034454346
Epoch 710, training loss: 0.01337246224284172 = 0.0062714326195418835 + 0.001 * 7.101029396057129
Epoch 710, val loss: 1.1869909763336182
Epoch 720, training loss: 0.013098672032356262 = 0.0059977928176522255 + 0.001 * 7.100879192352295
Epoch 720, val loss: 1.193320631980896
Epoch 730, training loss: 0.01282467134296894 = 0.005740918219089508 + 0.001 * 7.083752155303955
Epoch 730, val loss: 1.199521780014038
Epoch 740, training loss: 0.012585815042257309 = 0.005499465391039848 + 0.001 * 7.086349010467529
Epoch 740, val loss: 1.205625295639038
Epoch 750, training loss: 0.012342378497123718 = 0.00527216587215662 + 0.001 * 7.0702128410339355
Epoch 750, val loss: 1.2116376161575317
Epoch 760, training loss: 0.012193232774734497 = 0.005057693924754858 + 0.001 * 7.135538578033447
Epoch 760, val loss: 1.2175792455673218
Epoch 770, training loss: 0.011920921504497528 = 0.004855107516050339 + 0.001 * 7.065813064575195
Epoch 770, val loss: 1.223475694656372
Epoch 780, training loss: 0.011747429147362709 = 0.004663336556404829 + 0.001 * 7.084092617034912
Epoch 780, val loss: 1.2293238639831543
Epoch 790, training loss: 0.011546948924660683 = 0.004481690935790539 + 0.001 * 7.065258026123047
Epoch 790, val loss: 1.2351070642471313
Epoch 800, training loss: 0.011402806267142296 = 0.004309571348130703 + 0.001 * 7.093235015869141
Epoch 800, val loss: 1.2408519983291626
Epoch 810, training loss: 0.011213330551981926 = 0.004146589897572994 + 0.001 * 7.066740989685059
Epoch 810, val loss: 1.2465567588806152
Epoch 820, training loss: 0.011050211265683174 = 0.003992327954620123 + 0.001 * 7.057883262634277
Epoch 820, val loss: 1.2521475553512573
Epoch 830, training loss: 0.010903121903538704 = 0.003846358507871628 + 0.001 * 7.056763172149658
Epoch 830, val loss: 1.2576614618301392
Epoch 840, training loss: 0.01076087448745966 = 0.0037082030903548002 + 0.001 * 7.052671432495117
Epoch 840, val loss: 1.2630963325500488
Epoch 850, training loss: 0.010625524446368217 = 0.0035773352719843388 + 0.001 * 7.048189163208008
Epoch 850, val loss: 1.2684656381607056
Epoch 860, training loss: 0.010511542670428753 = 0.0034534994047135115 + 0.001 * 7.058042526245117
Epoch 860, val loss: 1.2737059593200684
Epoch 870, training loss: 0.010371034033596516 = 0.003336163004860282 + 0.001 * 7.034870624542236
Epoch 870, val loss: 1.2788320779800415
Epoch 880, training loss: 0.01025925762951374 = 0.0032250171061605215 + 0.001 * 7.034240245819092
Epoch 880, val loss: 1.2838754653930664
Epoch 890, training loss: 0.010179828852415085 = 0.003119648899883032 + 0.001 * 7.060179710388184
Epoch 890, val loss: 1.2888087034225464
Epoch 900, training loss: 0.01004383061081171 = 0.0030198199674487114 + 0.001 * 7.024010181427002
Epoch 900, val loss: 1.293668508529663
Epoch 910, training loss: 0.009960026480257511 = 0.0029250646475702524 + 0.001 * 7.034961223602295
Epoch 910, val loss: 1.2983932495117188
Epoch 920, training loss: 0.009876303374767303 = 0.0028351498767733574 + 0.001 * 7.041152477264404
Epoch 920, val loss: 1.303066372871399
Epoch 930, training loss: 0.009771941229701042 = 0.0027497641276568174 + 0.001 * 7.022177219390869
Epoch 930, val loss: 1.3076001405715942
Epoch 940, training loss: 0.009685693308711052 = 0.0026686887722462416 + 0.001 * 7.017004013061523
Epoch 940, val loss: 1.3120468854904175
Epoch 950, training loss: 0.009661908261477947 = 0.002591578522697091 + 0.001 * 7.070329666137695
Epoch 950, val loss: 1.3164037466049194
Epoch 960, training loss: 0.009525533765554428 = 0.002518276683986187 + 0.001 * 7.007256984710693
Epoch 960, val loss: 1.3206565380096436
Epoch 970, training loss: 0.009453904815018177 = 0.0024485066533088684 + 0.001 * 7.005397796630859
Epoch 970, val loss: 1.3248144388198853
Epoch 980, training loss: 0.009402213618159294 = 0.002382040023803711 + 0.001 * 7.0201735496521
Epoch 980, val loss: 1.3289026021957397
Epoch 990, training loss: 0.00934646837413311 = 0.0023187019396573305 + 0.001 * 7.027766227722168
Epoch 990, val loss: 1.3328875303268433
Epoch 1000, training loss: 0.009262085892260075 = 0.00225832243449986 + 0.001 * 7.003762722015381
Epoch 1000, val loss: 1.3367795944213867
Epoch 1010, training loss: 0.009217864833772182 = 0.0022006926592439413 + 0.001 * 7.017171382904053
Epoch 1010, val loss: 1.3406089544296265
Epoch 1020, training loss: 0.009142044931650162 = 0.00214571226388216 + 0.001 * 6.996331691741943
Epoch 1020, val loss: 1.3443336486816406
Epoch 1030, training loss: 0.009092392399907112 = 0.0020932003390043974 + 0.001 * 6.9991912841796875
Epoch 1030, val loss: 1.3479756116867065
Epoch 1040, training loss: 0.009026862680912018 = 0.002043041167780757 + 0.001 * 6.983820915222168
Epoch 1040, val loss: 1.3515386581420898
Epoch 1050, training loss: 0.008975082077085972 = 0.0019950675778090954 + 0.001 * 6.980014324188232
Epoch 1050, val loss: 1.355021357536316
Epoch 1060, training loss: 0.00893657561391592 = 0.0019491839921101928 + 0.001 * 6.987390995025635
Epoch 1060, val loss: 1.3584251403808594
Epoch 1070, training loss: 0.008905312046408653 = 0.001905255252495408 + 0.001 * 7.000056266784668
Epoch 1070, val loss: 1.3617578744888306
Epoch 1080, training loss: 0.008871322497725487 = 0.0018631762359291315 + 0.001 * 7.008146286010742
Epoch 1080, val loss: 1.3650141954421997
Epoch 1090, training loss: 0.008825111202895641 = 0.0018228736007586122 + 0.001 * 7.002237319946289
Epoch 1090, val loss: 1.3681912422180176
Epoch 1100, training loss: 0.008782053366303444 = 0.0017842203378677368 + 0.001 * 6.997832298278809
Epoch 1100, val loss: 1.3713032007217407
Epoch 1110, training loss: 0.008710050955414772 = 0.0017471511382609606 + 0.001 * 6.962899208068848
Epoch 1110, val loss: 1.3743592500686646
Epoch 1120, training loss: 0.008676892146468163 = 0.0017115428345277905 + 0.001 * 6.965349197387695
Epoch 1120, val loss: 1.3773181438446045
Epoch 1130, training loss: 0.008692939765751362 = 0.001677337335422635 + 0.001 * 7.015601634979248
Epoch 1130, val loss: 1.3802497386932373
Epoch 1140, training loss: 0.008637058548629284 = 0.0016445356886833906 + 0.001 * 6.992522239685059
Epoch 1140, val loss: 1.3830997943878174
Epoch 1150, training loss: 0.00864043552428484 = 0.0016129401046782732 + 0.001 * 7.02749490737915
Epoch 1150, val loss: 1.385882019996643
Epoch 1160, training loss: 0.008538976311683655 = 0.001582612982019782 + 0.001 * 6.956363201141357
Epoch 1160, val loss: 1.3886163234710693
Epoch 1170, training loss: 0.00854489952325821 = 0.001553395763039589 + 0.001 * 6.991503715515137
Epoch 1170, val loss: 1.3912655115127563
Epoch 1180, training loss: 0.00848725438117981 = 0.0015253140591084957 + 0.001 * 6.961939334869385
Epoch 1180, val loss: 1.3939052820205688
Epoch 1190, training loss: 0.008538850583136082 = 0.0014982187421992421 + 0.001 * 7.040631294250488
Epoch 1190, val loss: 1.396456003189087
Epoch 1200, training loss: 0.008438658900558949 = 0.001472183270379901 + 0.001 * 6.966475009918213
Epoch 1200, val loss: 1.3989704847335815
Epoch 1210, training loss: 0.008455283008515835 = 0.0014470408204942942 + 0.001 * 7.008242130279541
Epoch 1210, val loss: 1.401406168937683
Epoch 1220, training loss: 0.008373240008950233 = 0.0014228380750864744 + 0.001 * 6.950401306152344
Epoch 1220, val loss: 1.4038050174713135
Epoch 1230, training loss: 0.008366899564862251 = 0.001399465138092637 + 0.001 * 6.967434406280518
Epoch 1230, val loss: 1.4061460494995117
Epoch 1240, training loss: 0.008320916444063187 = 0.0013769264332950115 + 0.001 * 6.9439897537231445
Epoch 1240, val loss: 1.4084601402282715
Epoch 1250, training loss: 0.008297612890601158 = 0.001355162588879466 + 0.001 * 6.942449569702148
Epoch 1250, val loss: 1.4107030630111694
Epoch 1260, training loss: 0.008273839950561523 = 0.001334123546257615 + 0.001 * 6.939716339111328
Epoch 1260, val loss: 1.4129009246826172
Epoch 1270, training loss: 0.008249987848103046 = 0.0013138424837961793 + 0.001 * 6.936145305633545
Epoch 1270, val loss: 1.4150687456130981
Epoch 1280, training loss: 0.008244466036558151 = 0.0012942272005602717 + 0.001 * 6.950238227844238
Epoch 1280, val loss: 1.4171714782714844
Epoch 1290, training loss: 0.008198326453566551 = 0.0012752542970702052 + 0.001 * 6.923072338104248
Epoch 1290, val loss: 1.4192537069320679
Epoch 1300, training loss: 0.00825408473610878 = 0.0012568983947858214 + 0.001 * 6.997185707092285
Epoch 1300, val loss: 1.4212749004364014
Epoch 1310, training loss: 0.008185258135199547 = 0.001239183358848095 + 0.001 * 6.94607400894165
Epoch 1310, val loss: 1.4232616424560547
Epoch 1320, training loss: 0.008205010555684566 = 0.0012219975469633937 + 0.001 * 6.983013153076172
Epoch 1320, val loss: 1.4251896142959595
Epoch 1330, training loss: 0.008148129098117352 = 0.0012053853133693337 + 0.001 * 6.94274377822876
Epoch 1330, val loss: 1.4270975589752197
Epoch 1340, training loss: 0.008141625672578812 = 0.001189278089441359 + 0.001 * 6.952347278594971
Epoch 1340, val loss: 1.4289577007293701
Epoch 1350, training loss: 0.008112804964184761 = 0.0011737007880583405 + 0.001 * 6.939104080200195
Epoch 1350, val loss: 1.4307842254638672
Epoch 1360, training loss: 0.008078709244728088 = 0.0011585975298658013 + 0.001 * 6.920111656188965
Epoch 1360, val loss: 1.4325627088546753
Epoch 1370, training loss: 0.008070905692875385 = 0.001143954461440444 + 0.001 * 6.926950931549072
Epoch 1370, val loss: 1.4343098402023315
Epoch 1380, training loss: 0.008067070506513119 = 0.001129750395193696 + 0.001 * 6.937320232391357
Epoch 1380, val loss: 1.4360406398773193
Epoch 1390, training loss: 0.00807851180434227 = 0.0011159884743392467 + 0.001 * 6.962522506713867
Epoch 1390, val loss: 1.4377111196517944
Epoch 1400, training loss: 0.008044499903917313 = 0.0011026128195226192 + 0.001 * 6.941887378692627
Epoch 1400, val loss: 1.4393681287765503
Epoch 1410, training loss: 0.008007673546671867 = 0.001089674886316061 + 0.001 * 6.917998313903809
Epoch 1410, val loss: 1.4409695863723755
Epoch 1420, training loss: 0.007987597957253456 = 0.001077089225873351 + 0.001 * 6.910508155822754
Epoch 1420, val loss: 1.442548394203186
Epoch 1430, training loss: 0.007990919053554535 = 0.001064866897650063 + 0.001 * 6.926052093505859
Epoch 1430, val loss: 1.4441019296646118
Epoch 1440, training loss: 0.007961294613778591 = 0.0010529905557632446 + 0.001 * 6.908303737640381
Epoch 1440, val loss: 1.44560968875885
Epoch 1450, training loss: 0.007946396246552467 = 0.0010414631105959415 + 0.001 * 6.904933452606201
Epoch 1450, val loss: 1.447102665901184
Epoch 1460, training loss: 0.007958104833960533 = 0.0010302509181201458 + 0.001 * 6.927854061126709
Epoch 1460, val loss: 1.4485522508621216
Epoch 1470, training loss: 0.00791635550558567 = 0.0010193610796704888 + 0.001 * 6.896993637084961
Epoch 1470, val loss: 1.4499934911727905
Epoch 1480, training loss: 0.007905639708042145 = 0.001008764491416514 + 0.001 * 6.89687442779541
Epoch 1480, val loss: 1.4513874053955078
Epoch 1490, training loss: 0.00793427973985672 = 0.000998465926386416 + 0.001 * 6.9358134269714355
Epoch 1490, val loss: 1.4527779817581177
Epoch 1500, training loss: 0.007900403812527657 = 0.0009884396567940712 + 0.001 * 6.91196346282959
Epoch 1500, val loss: 1.454126238822937
Epoch 1510, training loss: 0.007872899062931538 = 0.0009786963928490877 + 0.001 * 6.894202709197998
Epoch 1510, val loss: 1.4554438591003418
Epoch 1520, training loss: 0.007860076613724232 = 0.0009692080202512443 + 0.001 * 6.890868663787842
Epoch 1520, val loss: 1.4567525386810303
Epoch 1530, training loss: 0.007871038280427456 = 0.0009599831537343562 + 0.001 * 6.911054611206055
Epoch 1530, val loss: 1.4580259323120117
Epoch 1540, training loss: 0.007850410416722298 = 0.0009510000236332417 + 0.001 * 6.899410247802734
Epoch 1540, val loss: 1.4592961072921753
Epoch 1550, training loss: 0.00784324761480093 = 0.000942255777772516 + 0.001 * 6.900991916656494
Epoch 1550, val loss: 1.460518717765808
Epoch 1560, training loss: 0.007828583009541035 = 0.0009337401716038585 + 0.001 * 6.894842624664307
Epoch 1560, val loss: 1.4617254734039307
Epoch 1570, training loss: 0.007819910533726215 = 0.0009254335309378803 + 0.001 * 6.894476413726807
Epoch 1570, val loss: 1.4629157781600952
Epoch 1580, training loss: 0.007831813767552376 = 0.0009173367288894951 + 0.001 * 6.9144768714904785
Epoch 1580, val loss: 1.4640909433364868
Epoch 1590, training loss: 0.007794437929987907 = 0.0009094551205635071 + 0.001 * 6.884982585906982
Epoch 1590, val loss: 1.465240716934204
Epoch 1600, training loss: 0.007795021869242191 = 0.0009017785196192563 + 0.001 * 6.893242835998535
Epoch 1600, val loss: 1.4663749933242798
Epoch 1610, training loss: 0.00779785867780447 = 0.0008943004067987204 + 0.001 * 6.903558254241943
Epoch 1610, val loss: 1.467474341392517
Epoch 1620, training loss: 0.0077917976304888725 = 0.0008869985467754304 + 0.001 * 6.90479850769043
Epoch 1620, val loss: 1.4685653448104858
Epoch 1630, training loss: 0.007760132662951946 = 0.0008798718918114901 + 0.001 * 6.880259990692139
Epoch 1630, val loss: 1.4696494340896606
Epoch 1640, training loss: 0.007771494332700968 = 0.0008729267865419388 + 0.001 * 6.898567199707031
Epoch 1640, val loss: 1.4706826210021973
Epoch 1650, training loss: 0.007751527242362499 = 0.0008661353494971991 + 0.001 * 6.885391712188721
Epoch 1650, val loss: 1.4717355966567993
Epoch 1660, training loss: 0.007786931935697794 = 0.0008595132967457175 + 0.001 * 6.927418231964111
Epoch 1660, val loss: 1.4727399349212646
Epoch 1670, training loss: 0.00775567814707756 = 0.0008530507329851389 + 0.001 * 6.902626991271973
Epoch 1670, val loss: 1.4737536907196045
Epoch 1680, training loss: 0.007713751867413521 = 0.000846737006213516 + 0.001 * 6.867014408111572
Epoch 1680, val loss: 1.4747297763824463
Epoch 1690, training loss: 0.00771744130179286 = 0.0008405803819186985 + 0.001 * 6.876860618591309
Epoch 1690, val loss: 1.4757018089294434
Epoch 1700, training loss: 0.007723326329141855 = 0.0008345578098669648 + 0.001 * 6.888768196105957
Epoch 1700, val loss: 1.4766497611999512
Epoch 1710, training loss: 0.007746873423457146 = 0.0008286829688586295 + 0.001 * 6.918190002441406
Epoch 1710, val loss: 1.477586269378662
Epoch 1720, training loss: 0.007723180111497641 = 0.0008229486411437392 + 0.001 * 6.900230884552002
Epoch 1720, val loss: 1.478487491607666
Epoch 1730, training loss: 0.007697699125856161 = 0.00081732461694628 + 0.001 * 6.880374431610107
Epoch 1730, val loss: 1.4794005155563354
Epoch 1740, training loss: 0.007681294344365597 = 0.0008118452387861907 + 0.001 * 6.869449138641357
Epoch 1740, val loss: 1.4802830219268799
Epoch 1750, training loss: 0.007674588821828365 = 0.0008064788416959345 + 0.001 * 6.868109703063965
Epoch 1750, val loss: 1.4811457395553589
Epoch 1760, training loss: 0.0076803541742265224 = 0.0008012323523871601 + 0.001 * 6.879121780395508
Epoch 1760, val loss: 1.4820029735565186
Epoch 1770, training loss: 0.007657288573682308 = 0.0007960951770655811 + 0.001 * 6.8611931800842285
Epoch 1770, val loss: 1.4828476905822754
Epoch 1780, training loss: 0.007648042403161526 = 0.00079106300836429 + 0.001 * 6.856978893280029
Epoch 1780, val loss: 1.483681321144104
Epoch 1790, training loss: 0.007648913189768791 = 0.0007861389894969761 + 0.001 * 6.862773895263672
Epoch 1790, val loss: 1.4844892024993896
Epoch 1800, training loss: 0.007662824355065823 = 0.0007813169504515827 + 0.001 * 6.881507396697998
Epoch 1800, val loss: 1.485296607017517
Epoch 1810, training loss: 0.007654087617993355 = 0.0007766032358631492 + 0.001 * 6.87748384475708
Epoch 1810, val loss: 1.4860990047454834
Epoch 1820, training loss: 0.007630985230207443 = 0.0007719991263002157 + 0.001 * 6.858985424041748
Epoch 1820, val loss: 1.4868621826171875
Epoch 1830, training loss: 0.007622392848134041 = 0.0007674724329262972 + 0.001 * 6.854920387268066
Epoch 1830, val loss: 1.4876188039779663
Epoch 1840, training loss: 0.007659594994038343 = 0.0007630540640093386 + 0.001 * 6.896540641784668
Epoch 1840, val loss: 1.4883568286895752
Epoch 1850, training loss: 0.007630136329680681 = 0.0007587149739265442 + 0.001 * 6.871420860290527
Epoch 1850, val loss: 1.4891172647476196
Epoch 1860, training loss: 0.007630037609487772 = 0.0007544805412180722 + 0.001 * 6.875556468963623
Epoch 1860, val loss: 1.4898262023925781
Epoch 1870, training loss: 0.007619923911988735 = 0.0007503224769607186 + 0.001 * 6.869601249694824
Epoch 1870, val loss: 1.4905418157577515
Epoch 1880, training loss: 0.007596390321850777 = 0.0007462484063580632 + 0.001 * 6.850141525268555
Epoch 1880, val loss: 1.4912289381027222
Epoch 1890, training loss: 0.0076241265051066875 = 0.0007422392372973263 + 0.001 * 6.881886959075928
Epoch 1890, val loss: 1.4919211864471436
Epoch 1900, training loss: 0.00760142644867301 = 0.0007383277988992631 + 0.001 * 6.86309814453125
Epoch 1900, val loss: 1.4925881624221802
Epoch 1910, training loss: 0.007593166083097458 = 0.0007344682235270739 + 0.001 * 6.858697414398193
Epoch 1910, val loss: 1.4932609796524048
Epoch 1920, training loss: 0.007610679604113102 = 0.0007306970655918121 + 0.001 * 6.879981994628906
Epoch 1920, val loss: 1.4939154386520386
Epoch 1930, training loss: 0.007578551769256592 = 0.0007269973284564912 + 0.001 * 6.851553916931152
Epoch 1930, val loss: 1.4945752620697021
Epoch 1940, training loss: 0.007583637721836567 = 0.0007233739597722888 + 0.001 * 6.860263347625732
Epoch 1940, val loss: 1.4951962232589722
Epoch 1950, training loss: 0.007617535535246134 = 0.0007197927334345877 + 0.001 * 6.897742748260498
Epoch 1950, val loss: 1.4958099126815796
Epoch 1960, training loss: 0.007571342866867781 = 0.0007162996917031705 + 0.001 * 6.855042934417725
Epoch 1960, val loss: 1.496442198753357
Epoch 1970, training loss: 0.00755410036072135 = 0.0007128605502657592 + 0.001 * 6.8412394523620605
Epoch 1970, val loss: 1.4970343112945557
Epoch 1980, training loss: 0.007548416033387184 = 0.0007094896282069385 + 0.001 * 6.838926315307617
Epoch 1980, val loss: 1.4976387023925781
Epoch 1990, training loss: 0.00755187775939703 = 0.0007061856449581683 + 0.001 * 6.845691680908203
Epoch 1990, val loss: 1.498221516609192
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8181338956246705
The final CL Acc:0.75926, 0.01814, The final GNN Acc:0.81778, 0.00050
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13186])
remove edge: torch.Size([2, 7964])
updated graph: torch.Size([2, 10594])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.955165982246399 = 1.9465690851211548 + 0.001 * 8.59685230255127
Epoch 0, val loss: 1.9497036933898926
Epoch 10, training loss: 1.9453251361846924 = 1.9367283582687378 + 0.001 * 8.596796035766602
Epoch 10, val loss: 1.940466284751892
Epoch 20, training loss: 1.9332903623580933 = 1.9246937036514282 + 0.001 * 8.596606254577637
Epoch 20, val loss: 1.9286266565322876
Epoch 30, training loss: 1.9164462089538574 = 1.9078500270843506 + 0.001 * 8.596150398254395
Epoch 30, val loss: 1.9117431640625
Epoch 40, training loss: 1.8915661573410034 = 1.882971167564392 + 0.001 * 8.594943046569824
Epoch 40, val loss: 1.8867974281311035
Epoch 50, training loss: 1.8561614751815796 = 1.8475704193115234 + 0.001 * 8.591036796569824
Epoch 50, val loss: 1.8521826267242432
Epoch 60, training loss: 1.8143588304519653 = 1.8057833909988403 + 0.001 * 8.575460433959961
Epoch 60, val loss: 1.8138022422790527
Epoch 70, training loss: 1.776853322982788 = 1.768357515335083 + 0.001 * 8.495813369750977
Epoch 70, val loss: 1.780416488647461
Epoch 80, training loss: 1.7335832118988037 = 1.7254831790924072 + 0.001 * 8.100008964538574
Epoch 80, val loss: 1.7393085956573486
Epoch 90, training loss: 1.672868013381958 = 1.6649121046066284 + 0.001 * 7.955949306488037
Epoch 90, val loss: 1.6842663288116455
Epoch 100, training loss: 1.5923129320144653 = 1.5845082998275757 + 0.001 * 7.80458927154541
Epoch 100, val loss: 1.6160136461257935
Epoch 110, training loss: 1.4964537620544434 = 1.4888534545898438 + 0.001 * 7.600350856781006
Epoch 110, val loss: 1.5348412990570068
Epoch 120, training loss: 1.3946441411972046 = 1.387142300605774 + 0.001 * 7.501870155334473
Epoch 120, val loss: 1.4490026235580444
Epoch 130, training loss: 1.2920070886611938 = 1.284563660621643 + 0.001 * 7.443373680114746
Epoch 130, val loss: 1.3644996881484985
Epoch 140, training loss: 1.1901017427444458 = 1.1827349662780762 + 0.001 * 7.366821765899658
Epoch 140, val loss: 1.2830755710601807
Epoch 150, training loss: 1.0895434617996216 = 1.0822458267211914 + 0.001 * 7.297659397125244
Epoch 150, val loss: 1.2037856578826904
Epoch 160, training loss: 0.9911121129989624 = 0.9838412404060364 + 0.001 * 7.270849704742432
Epoch 160, val loss: 1.1273273229599
Epoch 170, training loss: 0.8964747786521912 = 0.8892117142677307 + 0.001 * 7.263075828552246
Epoch 170, val loss: 1.0542939901351929
Epoch 180, training loss: 0.8083099722862244 = 0.8010560870170593 + 0.001 * 7.253861427307129
Epoch 180, val loss: 0.9869155287742615
Epoch 190, training loss: 0.7288515567779541 = 0.7216096520423889 + 0.001 * 7.241888046264648
Epoch 190, val loss: 0.927288830280304
Epoch 200, training loss: 0.6589169502258301 = 0.6516941785812378 + 0.001 * 7.2227888107299805
Epoch 200, val loss: 0.8768138289451599
Epoch 210, training loss: 0.5976204872131348 = 0.590419590473175 + 0.001 * 7.200896739959717
Epoch 210, val loss: 0.8356972932815552
Epoch 220, training loss: 0.5434562563896179 = 0.5362690687179565 + 0.001 * 7.187195301055908
Epoch 220, val loss: 0.803288459777832
Epoch 230, training loss: 0.4950178265571594 = 0.48784175515174866 + 0.001 * 7.176064491271973
Epoch 230, val loss: 0.7783641815185547
Epoch 240, training loss: 0.4512661099433899 = 0.44409501552581787 + 0.001 * 7.1711015701293945
Epoch 240, val loss: 0.759595513343811
Epoch 250, training loss: 0.41149577498435974 = 0.4043278992176056 + 0.001 * 7.167860984802246
Epoch 250, val loss: 0.7459366321563721
Epoch 260, training loss: 0.37519213557243347 = 0.36802640557289124 + 0.001 * 7.165738582611084
Epoch 260, val loss: 0.7365067601203918
Epoch 270, training loss: 0.34188708662986755 = 0.3347233235836029 + 0.001 * 7.163767337799072
Epoch 270, val loss: 0.7306740880012512
Epoch 280, training loss: 0.3111806809902191 = 0.30401885509490967 + 0.001 * 7.161829948425293
Epoch 280, val loss: 0.7279494404792786
Epoch 290, training loss: 0.2826886475086212 = 0.2755287289619446 + 0.001 * 7.159915447235107
Epoch 290, val loss: 0.7278336882591248
Epoch 300, training loss: 0.2560286819934845 = 0.2488674372434616 + 0.001 * 7.161255836486816
Epoch 300, val loss: 0.7297865748405457
Epoch 310, training loss: 0.23081038892269135 = 0.2236536294221878 + 0.001 * 7.156753063201904
Epoch 310, val loss: 0.7334704995155334
Epoch 320, training loss: 0.2068387120962143 = 0.19968312978744507 + 0.001 * 7.155576705932617
Epoch 320, val loss: 0.7383926510810852
Epoch 330, training loss: 0.18413357436656952 = 0.17698031663894653 + 0.001 * 7.153253078460693
Epoch 330, val loss: 0.7444538474082947
Epoch 340, training loss: 0.16295020282268524 = 0.15579672157764435 + 0.001 * 7.153478622436523
Epoch 340, val loss: 0.7516636848449707
Epoch 350, training loss: 0.14366327226161957 = 0.13651525974273682 + 0.001 * 7.148007392883301
Epoch 350, val loss: 0.7599863409996033
Epoch 360, training loss: 0.12656547129154205 = 0.11941122263669968 + 0.001 * 7.1542510986328125
Epoch 360, val loss: 0.7693448662757874
Epoch 370, training loss: 0.11168179661035538 = 0.10453744232654572 + 0.001 * 7.144356727600098
Epoch 370, val loss: 0.7797068953514099
Epoch 380, training loss: 0.09888655692338943 = 0.0917428731918335 + 0.001 * 7.143684387207031
Epoch 380, val loss: 0.7909507751464844
Epoch 390, training loss: 0.08791390806436539 = 0.08077992498874664 + 0.001 * 7.133981704711914
Epoch 390, val loss: 0.8028929829597473
Epoch 400, training loss: 0.07850687205791473 = 0.07138212025165558 + 0.001 * 7.124751091003418
Epoch 400, val loss: 0.815261721611023
Epoch 410, training loss: 0.07043134421110153 = 0.06330788135528564 + 0.001 * 7.123465061187744
Epoch 410, val loss: 0.8279014229774475
Epoch 420, training loss: 0.06347693502902985 = 0.056350190192461014 + 0.001 * 7.126742362976074
Epoch 420, val loss: 0.8406228423118591
Epoch 430, training loss: 0.05745526775717735 = 0.050334442406892776 + 0.001 * 7.120826244354248
Epoch 430, val loss: 0.8532373309135437
Epoch 440, training loss: 0.05222037062048912 = 0.04511640965938568 + 0.001 * 7.103959560394287
Epoch 440, val loss: 0.8657416701316833
Epoch 450, training loss: 0.047675780951976776 = 0.04057655110955238 + 0.001 * 7.099231243133545
Epoch 450, val loss: 0.8780065178871155
Epoch 460, training loss: 0.04371372610330582 = 0.03661586716771126 + 0.001 * 7.09785795211792
Epoch 460, val loss: 0.8900750279426575
Epoch 470, training loss: 0.04024619236588478 = 0.0331505611538887 + 0.001 * 7.095632076263428
Epoch 470, val loss: 0.9018364548683167
Epoch 480, training loss: 0.03720403090119362 = 0.030110839754343033 + 0.001 * 7.093191623687744
Epoch 480, val loss: 0.9132732152938843
Epoch 490, training loss: 0.034527234733104706 = 0.027436792850494385 + 0.001 * 7.0904436111450195
Epoch 490, val loss: 0.9243910908699036
Epoch 500, training loss: 0.032167401164770126 = 0.025077998638153076 + 0.001 * 7.089402198791504
Epoch 500, val loss: 0.9351624846458435
Epoch 510, training loss: 0.03009081818163395 = 0.022991545498371124 + 0.001 * 7.099271774291992
Epoch 510, val loss: 0.9455956220626831
Epoch 520, training loss: 0.028233017772436142 = 0.02114161290228367 + 0.001 * 7.091404438018799
Epoch 520, val loss: 0.9557281732559204
Epoch 530, training loss: 0.026587655767798424 = 0.019496362656354904 + 0.001 * 7.0912933349609375
Epoch 530, val loss: 0.9654611349105835
Epoch 540, training loss: 0.02511853724718094 = 0.018028857186436653 + 0.001 * 7.0896806716918945
Epoch 540, val loss: 0.9748981595039368
Epoch 550, training loss: 0.02380133979022503 = 0.01671600714325905 + 0.001 * 7.085332870483398
Epoch 550, val loss: 0.984053909778595
Epoch 560, training loss: 0.02263285219669342 = 0.015538277104496956 + 0.001 * 7.094574928283691
Epoch 560, val loss: 0.9929253458976746
Epoch 570, training loss: 0.021571997553110123 = 0.014478770084679127 + 0.001 * 7.093227863311768
Epoch 570, val loss: 1.0014967918395996
Epoch 580, training loss: 0.020607711747288704 = 0.013523412868380547 + 0.001 * 7.084298610687256
Epoch 580, val loss: 1.0098183155059814
Epoch 590, training loss: 0.019740715622901917 = 0.012659656815230846 + 0.001 * 7.081058025360107
Epoch 590, val loss: 1.0178519487380981
Epoch 600, training loss: 0.018968647345900536 = 0.01187663059681654 + 0.001 * 7.092016696929932
Epoch 600, val loss: 1.0256434679031372
Epoch 610, training loss: 0.01824510656297207 = 0.011165255680680275 + 0.001 * 7.079850196838379
Epoch 610, val loss: 1.0331836938858032
Epoch 620, training loss: 0.017600437626242638 = 0.010517202317714691 + 0.001 * 7.083234786987305
Epoch 620, val loss: 1.0404775142669678
Epoch 630, training loss: 0.017003584653139114 = 0.00992544461041689 + 0.001 * 7.078139305114746
Epoch 630, val loss: 1.0475670099258423
Epoch 640, training loss: 0.016461731866002083 = 0.009383907541632652 + 0.001 * 7.077824592590332
Epoch 640, val loss: 1.054431438446045
Epoch 650, training loss: 0.015975816175341606 = 0.008887222036719322 + 0.001 * 7.088594436645508
Epoch 650, val loss: 1.0610886812210083
Epoch 660, training loss: 0.01550490502268076 = 0.008430680260062218 + 0.001 * 7.074224472045898
Epoch 660, val loss: 1.0675472021102905
Epoch 670, training loss: 0.015085538849234581 = 0.008010150864720345 + 0.001 * 7.075387001037598
Epoch 670, val loss: 1.0738155841827393
Epoch 680, training loss: 0.014694525860249996 = 0.0076220049522817135 + 0.001 * 7.072520732879639
Epoch 680, val loss: 1.0798786878585815
Epoch 690, training loss: 0.014337250962853432 = 0.0072630625218153 + 0.001 * 7.074187755584717
Epoch 690, val loss: 1.0858033895492554
Epoch 700, training loss: 0.01400907151401043 = 0.006930501200258732 + 0.001 * 7.078570365905762
Epoch 700, val loss: 1.0915275812149048
Epoch 710, training loss: 0.013698150403797626 = 0.006621828302741051 + 0.001 * 7.076321601867676
Epoch 710, val loss: 1.0970925092697144
Epoch 720, training loss: 0.013406273908913136 = 0.006334941368550062 + 0.001 * 7.071331977844238
Epoch 720, val loss: 1.1025209426879883
Epoch 730, training loss: 0.013142948038876057 = 0.0060677295550704 + 0.001 * 7.075218200683594
Epoch 730, val loss: 1.1077752113342285
Epoch 740, training loss: 0.012887238524854183 = 0.005818405654281378 + 0.001 * 7.0688323974609375
Epoch 740, val loss: 1.1128917932510376
Epoch 750, training loss: 0.012646831572055817 = 0.005585190840065479 + 0.001 * 7.061640739440918
Epoch 750, val loss: 1.1178940534591675
Epoch 760, training loss: 0.012431079521775246 = 0.0053666215389966965 + 0.001 * 7.064457416534424
Epoch 760, val loss: 1.1227635145187378
Epoch 770, training loss: 0.012225211597979069 = 0.005161461420357227 + 0.001 * 7.06374979019165
Epoch 770, val loss: 1.1275004148483276
Epoch 780, training loss: 0.012031684629619122 = 0.004968480672687292 + 0.001 * 7.063203811645508
Epoch 780, val loss: 1.1321418285369873
Epoch 790, training loss: 0.011854707263410091 = 0.004786519333720207 + 0.001 * 7.068187713623047
Epoch 790, val loss: 1.1366862058639526
Epoch 800, training loss: 0.011689389124512672 = 0.004614587873220444 + 0.001 * 7.074800491333008
Epoch 800, val loss: 1.1411480903625488
Epoch 810, training loss: 0.011510682292282581 = 0.004451820161193609 + 0.001 * 7.05886173248291
Epoch 810, val loss: 1.1455254554748535
Epoch 820, training loss: 0.011349514126777649 = 0.004297432955354452 + 0.001 * 7.0520806312561035
Epoch 820, val loss: 1.1498061418533325
Epoch 830, training loss: 0.011206265538930893 = 0.004150718450546265 + 0.001 * 7.05554723739624
Epoch 830, val loss: 1.154018759727478
Epoch 840, training loss: 0.011080997064709663 = 0.004011308308690786 + 0.001 * 7.069687843322754
Epoch 840, val loss: 1.1581538915634155
Epoch 850, training loss: 0.010937397368252277 = 0.003878879826515913 + 0.001 * 7.058516979217529
Epoch 850, val loss: 1.1621835231781006
Epoch 860, training loss: 0.010821067728102207 = 0.0037529764231294394 + 0.001 * 7.068090915679932
Epoch 860, val loss: 1.1661534309387207
Epoch 870, training loss: 0.010679783299565315 = 0.003633109387010336 + 0.001 * 7.0466742515563965
Epoch 870, val loss: 1.1700196266174316
Epoch 880, training loss: 0.010582530871033669 = 0.003518895246088505 + 0.001 * 7.063634872436523
Epoch 880, val loss: 1.1738089323043823
Epoch 890, training loss: 0.01046828180551529 = 0.003410025266930461 + 0.001 * 7.058256149291992
Epoch 890, val loss: 1.177528738975525
Epoch 900, training loss: 0.010353307239711285 = 0.003306279657408595 + 0.001 * 7.047027111053467
Epoch 900, val loss: 1.1811238527297974
Epoch 910, training loss: 0.010276507586240768 = 0.0032073738984763622 + 0.001 * 7.069133758544922
Epoch 910, val loss: 1.1846685409545898
Epoch 920, training loss: 0.010151417925953865 = 0.00311302300542593 + 0.001 * 7.038394451141357
Epoch 920, val loss: 1.1881299018859863
Epoch 930, training loss: 0.010078271850943565 = 0.0030229822732508183 + 0.001 * 7.055288791656494
Epoch 930, val loss: 1.1914836168289185
Epoch 940, training loss: 0.009975108318030834 = 0.0029371764976531267 + 0.001 * 7.037931442260742
Epoch 940, val loss: 1.1948060989379883
Epoch 950, training loss: 0.00988917425274849 = 0.00285531859844923 + 0.001 * 7.03385591506958
Epoch 950, val loss: 1.198028802871704
Epoch 960, training loss: 0.009815553203225136 = 0.002777127083390951 + 0.001 * 7.038426399230957
Epoch 960, val loss: 1.2011725902557373
Epoch 970, training loss: 0.009732987731695175 = 0.002702486701309681 + 0.001 * 7.030500411987305
Epoch 970, val loss: 1.204256296157837
Epoch 980, training loss: 0.009654330089688301 = 0.00263128150254488 + 0.001 * 7.023047924041748
Epoch 980, val loss: 1.2072527408599854
Epoch 990, training loss: 0.00958605669438839 = 0.0025631962344050407 + 0.001 * 7.022859573364258
Epoch 990, val loss: 1.2101871967315674
Epoch 1000, training loss: 0.009552652947604656 = 0.002498079091310501 + 0.001 * 7.0545735359191895
Epoch 1000, val loss: 1.213041067123413
Epoch 1010, training loss: 0.009474212303757668 = 0.0024358208756893873 + 0.001 * 7.03839111328125
Epoch 1010, val loss: 1.2158622741699219
Epoch 1020, training loss: 0.009404761716723442 = 0.002376226708292961 + 0.001 * 7.028534889221191
Epoch 1020, val loss: 1.2185920476913452
Epoch 1030, training loss: 0.009363550692796707 = 0.0023192516528069973 + 0.001 * 7.044299125671387
Epoch 1030, val loss: 1.2212703227996826
Epoch 1040, training loss: 0.009287184104323387 = 0.0022646989673376083 + 0.001 * 7.022485256195068
Epoch 1040, val loss: 1.2238837480545044
Epoch 1050, training loss: 0.009230757132172585 = 0.0022124566603451967 + 0.001 * 7.0183000564575195
Epoch 1050, val loss: 1.226423740386963
Epoch 1060, training loss: 0.009177466854453087 = 0.0021623452194035053 + 0.001 * 7.015121936798096
Epoch 1060, val loss: 1.2289212942123413
Epoch 1070, training loss: 0.009154404513537884 = 0.0021143131889402866 + 0.001 * 7.040091037750244
Epoch 1070, val loss: 1.231364130973816
Epoch 1080, training loss: 0.009090975858271122 = 0.0020682939793914557 + 0.001 * 7.022681713104248
Epoch 1080, val loss: 1.2337186336517334
Epoch 1090, training loss: 0.009064669720828533 = 0.0020241306629031897 + 0.001 * 7.040538787841797
Epoch 1090, val loss: 1.2360360622406006
Epoch 1100, training loss: 0.008996004238724709 = 0.001981819747015834 + 0.001 * 7.01418399810791
Epoch 1100, val loss: 1.238280177116394
Epoch 1110, training loss: 0.008949963375926018 = 0.0019412635592743754 + 0.001 * 7.008699417114258
Epoch 1110, val loss: 1.2404781579971313
Epoch 1120, training loss: 0.00893042329698801 = 0.001902310992591083 + 0.001 * 7.028111934661865
Epoch 1120, val loss: 1.2426283359527588
Epoch 1130, training loss: 0.008892228826880455 = 0.0018648516852408648 + 0.001 * 7.027376651763916
Epoch 1130, val loss: 1.2447338104248047
Epoch 1140, training loss: 0.008866495452821255 = 0.001828797161579132 + 0.001 * 7.037697792053223
Epoch 1140, val loss: 1.2467957735061646
Epoch 1150, training loss: 0.008817070163786411 = 0.00179414008744061 + 0.001 * 7.022929668426514
Epoch 1150, val loss: 1.248786211013794
Epoch 1160, training loss: 0.008761998265981674 = 0.0017607908230274916 + 0.001 * 7.00120735168457
Epoch 1160, val loss: 1.2507686614990234
Epoch 1170, training loss: 0.008750170469284058 = 0.001728636329062283 + 0.001 * 7.021533489227295
Epoch 1170, val loss: 1.2526811361312866
Epoch 1180, training loss: 0.008704353123903275 = 0.0016976413317024708 + 0.001 * 7.006711959838867
Epoch 1180, val loss: 1.2545464038848877
Epoch 1190, training loss: 0.008681931532919407 = 0.0016677721869200468 + 0.001 * 7.014159202575684
Epoch 1190, val loss: 1.2563989162445068
Epoch 1200, training loss: 0.00862865149974823 = 0.001638968475162983 + 0.001 * 6.989683151245117
Epoch 1200, val loss: 1.2581934928894043
Epoch 1210, training loss: 0.008603313937783241 = 0.0016111808363348246 + 0.001 * 6.992132663726807
Epoch 1210, val loss: 1.2599446773529053
Epoch 1220, training loss: 0.008591720834374428 = 0.001584361889399588 + 0.001 * 7.007358074188232
Epoch 1220, val loss: 1.261652946472168
Epoch 1230, training loss: 0.00854245200753212 = 0.0015584981301799417 + 0.001 * 6.983953475952148
Epoch 1230, val loss: 1.2633411884307861
Epoch 1240, training loss: 0.008529171347618103 = 0.001533527160063386 + 0.001 * 6.9956440925598145
Epoch 1240, val loss: 1.2649863958358765
Epoch 1250, training loss: 0.008541077375411987 = 0.0015093889087438583 + 0.001 * 7.031688213348389
Epoch 1250, val loss: 1.2665770053863525
Epoch 1260, training loss: 0.008488972671329975 = 0.0014860605588182807 + 0.001 * 7.002911567687988
Epoch 1260, val loss: 1.2681761980056763
Epoch 1270, training loss: 0.008448776789009571 = 0.001463525928556919 + 0.001 * 6.985250473022461
Epoch 1270, val loss: 1.2696778774261475
Epoch 1280, training loss: 0.008482009172439575 = 0.0014417578931897879 + 0.001 * 7.040250778198242
Epoch 1280, val loss: 1.2712076902389526
Epoch 1290, training loss: 0.008410314098000526 = 0.0014207499334588647 + 0.001 * 6.989563465118408
Epoch 1290, val loss: 1.2726448774337769
Epoch 1300, training loss: 0.0083820391446352 = 0.0014004585100337863 + 0.001 * 6.98158073425293
Epoch 1300, val loss: 1.27407705783844
Epoch 1310, training loss: 0.008346707560122013 = 0.0013808014336973429 + 0.001 * 6.965906143188477
Epoch 1310, val loss: 1.275435209274292
Epoch 1320, training loss: 0.008334696292877197 = 0.0013617840595543385 + 0.001 * 6.972911357879639
Epoch 1320, val loss: 1.2768104076385498
Epoch 1330, training loss: 0.008330278098583221 = 0.0013433772837743163 + 0.001 * 6.986900806427002
Epoch 1330, val loss: 1.2781341075897217
Epoch 1340, training loss: 0.008303393609821796 = 0.0013255446683615446 + 0.001 * 6.977849006652832
Epoch 1340, val loss: 1.2794320583343506
Epoch 1350, training loss: 0.008304595947265625 = 0.0013082209043204784 + 0.001 * 6.99637508392334
Epoch 1350, val loss: 1.2807073593139648
Epoch 1360, training loss: 0.008270570077002048 = 0.0012914225226268172 + 0.001 * 6.979147434234619
Epoch 1360, val loss: 1.2819433212280273
Epoch 1370, training loss: 0.008257112465798855 = 0.001275129383429885 + 0.001 * 6.981983184814453
Epoch 1370, val loss: 1.283176302909851
Epoch 1380, training loss: 0.008250102400779724 = 0.0012593286810442805 + 0.001 * 6.990773677825928
Epoch 1380, val loss: 1.2843408584594727
Epoch 1390, training loss: 0.00818771030753851 = 0.001243992941454053 + 0.001 * 6.943717002868652
Epoch 1390, val loss: 1.285508632659912
Epoch 1400, training loss: 0.00819298718124628 = 0.001229101326316595 + 0.001 * 6.963885307312012
Epoch 1400, val loss: 1.2866476774215698
Epoch 1410, training loss: 0.008186880499124527 = 0.0012146559311076999 + 0.001 * 6.972224712371826
Epoch 1410, val loss: 1.2877806425094604
Epoch 1420, training loss: 0.008181055076420307 = 0.0012006462784484029 + 0.001 * 6.980408191680908
Epoch 1420, val loss: 1.2888597249984741
Epoch 1430, training loss: 0.008143862709403038 = 0.0011870263842865825 + 0.001 * 6.956835746765137
Epoch 1430, val loss: 1.2899272441864014
Epoch 1440, training loss: 0.008133269846439362 = 0.001173775759525597 + 0.001 * 6.959493637084961
Epoch 1440, val loss: 1.2909669876098633
Epoch 1450, training loss: 0.008136480115354061 = 0.0011608896311372519 + 0.001 * 6.975590229034424
Epoch 1450, val loss: 1.2919902801513672
Epoch 1460, training loss: 0.00808623991906643 = 0.0011483777780085802 + 0.001 * 6.937861919403076
Epoch 1460, val loss: 1.2929768562316895
Epoch 1470, training loss: 0.00807162281125784 = 0.0011361981742084026 + 0.001 * 6.935424327850342
Epoch 1470, val loss: 1.293977975845337
Epoch 1480, training loss: 0.008081567473709583 = 0.0011243446497246623 + 0.001 * 6.9572224617004395
Epoch 1480, val loss: 1.2948824167251587
Epoch 1490, training loss: 0.008062658831477165 = 0.0011128210462629795 + 0.001 * 6.9498372077941895
Epoch 1490, val loss: 1.2958338260650635
Epoch 1500, training loss: 0.008069374598562717 = 0.0011016089702025056 + 0.001 * 6.967764854431152
Epoch 1500, val loss: 1.2967323064804077
Epoch 1510, training loss: 0.008078953251242638 = 0.0010907307732850313 + 0.001 * 6.988222122192383
Epoch 1510, val loss: 1.2976430654525757
Epoch 1520, training loss: 0.008032523095607758 = 0.0010801730677485466 + 0.001 * 6.95235013961792
Epoch 1520, val loss: 1.2984856367111206
Epoch 1530, training loss: 0.008027526549994946 = 0.001069876248948276 + 0.001 * 6.9576497077941895
Epoch 1530, val loss: 1.299361228942871
Epoch 1540, training loss: 0.008057430386543274 = 0.0010598358931019902 + 0.001 * 6.997593879699707
Epoch 1540, val loss: 1.300171136856079
Epoch 1550, training loss: 0.007980589754879475 = 0.0010500388452783227 + 0.001 * 6.9305500984191895
Epoch 1550, val loss: 1.3009878396987915
Epoch 1560, training loss: 0.007990706712007523 = 0.001040479983203113 + 0.001 * 6.950226783752441
Epoch 1560, val loss: 1.301810383796692
Epoch 1570, training loss: 0.007992327213287354 = 0.001031173742376268 + 0.001 * 6.961153030395508
Epoch 1570, val loss: 1.3025530576705933
Epoch 1580, training loss: 0.008012104779481888 = 0.001022086013108492 + 0.001 * 6.990017890930176
Epoch 1580, val loss: 1.303313136100769
Epoch 1590, training loss: 0.007943493314087391 = 0.0010132142342627048 + 0.001 * 6.930278778076172
Epoch 1590, val loss: 1.3040462732315063
Epoch 1600, training loss: 0.007921027019619942 = 0.0010045641101896763 + 0.001 * 6.916461944580078
Epoch 1600, val loss: 1.3047932386398315
Epoch 1610, training loss: 0.007897122763097286 = 0.0009961171308532357 + 0.001 * 6.901004791259766
Epoch 1610, val loss: 1.3055002689361572
Epoch 1620, training loss: 0.007900682277977467 = 0.0009878752753138542 + 0.001 * 6.912806510925293
Epoch 1620, val loss: 1.3061926364898682
Epoch 1630, training loss: 0.007899279706180096 = 0.000979832373559475 + 0.001 * 6.91944694519043
Epoch 1630, val loss: 1.3069027662277222
Epoch 1640, training loss: 0.007863646373152733 = 0.0009719922090880573 + 0.001 * 6.891653537750244
Epoch 1640, val loss: 1.3075501918792725
Epoch 1650, training loss: 0.007855499163269997 = 0.0009643345256336033 + 0.001 * 6.891164302825928
Epoch 1650, val loss: 1.3082151412963867
Epoch 1660, training loss: 0.00792662613093853 = 0.0009568496607244015 + 0.001 * 6.969775676727295
Epoch 1660, val loss: 1.3088434934616089
Epoch 1670, training loss: 0.007930842228233814 = 0.0009495436097495258 + 0.001 * 6.981297969818115
Epoch 1670, val loss: 1.3094836473464966
Epoch 1680, training loss: 0.007846280932426453 = 0.0009424296440556645 + 0.001 * 6.90385103225708
Epoch 1680, val loss: 1.3100271224975586
Epoch 1690, training loss: 0.007838459685444832 = 0.000935479998588562 + 0.001 * 6.902979850769043
Epoch 1690, val loss: 1.3106383085250854
Epoch 1700, training loss: 0.007824000902473927 = 0.0009286853019148111 + 0.001 * 6.895315647125244
Epoch 1700, val loss: 1.3112101554870605
Epoch 1710, training loss: 0.007813841104507446 = 0.0009220336796715856 + 0.001 * 6.8918070793151855
Epoch 1710, val loss: 1.3118118047714233
Epoch 1720, training loss: 0.007845684885978699 = 0.0009155194857157767 + 0.001 * 6.930164813995361
Epoch 1720, val loss: 1.3123693466186523
Epoch 1730, training loss: 0.007855149917304516 = 0.0009091479587368667 + 0.001 * 6.946002006530762
Epoch 1730, val loss: 1.3128962516784668
Epoch 1740, training loss: 0.00784418173134327 = 0.0009029079228639603 + 0.001 * 6.941273212432861
Epoch 1740, val loss: 1.313455581665039
Epoch 1750, training loss: 0.007784784771502018 = 0.0008967980975285172 + 0.001 * 6.887986660003662
Epoch 1750, val loss: 1.3139575719833374
Epoch 1760, training loss: 0.007791660726070404 = 0.0008908260497264564 + 0.001 * 6.900834560394287
Epoch 1760, val loss: 1.3145068883895874
Epoch 1770, training loss: 0.007885763421654701 = 0.0008849892183206975 + 0.001 * 7.000774383544922
Epoch 1770, val loss: 1.3150007724761963
Epoch 1780, training loss: 0.007760259322822094 = 0.0008792569860816002 + 0.001 * 6.881001949310303
Epoch 1780, val loss: 1.3154839277267456
Epoch 1790, training loss: 0.0077479565516114235 = 0.0008736567688174546 + 0.001 * 6.874299049377441
Epoch 1790, val loss: 1.315983772277832
Epoch 1800, training loss: 0.007752765901386738 = 0.0008681618492119014 + 0.001 * 6.884603977203369
Epoch 1800, val loss: 1.316466212272644
Epoch 1810, training loss: 0.007755130995064974 = 0.0008627889328636229 + 0.001 * 6.892341613769531
Epoch 1810, val loss: 1.3169270753860474
Epoch 1820, training loss: 0.007765236776322126 = 0.0008575212559662759 + 0.001 * 6.907715320587158
Epoch 1820, val loss: 1.317396640777588
Epoch 1830, training loss: 0.007765784859657288 = 0.0008523769793100655 + 0.001 * 6.913407325744629
Epoch 1830, val loss: 1.3177741765975952
Epoch 1840, training loss: 0.007804649416357279 = 0.0008473162306472659 + 0.001 * 6.957332611083984
Epoch 1840, val loss: 1.3182313442230225
Epoch 1850, training loss: 0.007698479574173689 = 0.000842347159050405 + 0.001 * 6.8561320304870605
Epoch 1850, val loss: 1.3186243772506714
Epoch 1860, training loss: 0.007730439770966768 = 0.0008374662138521671 + 0.001 * 6.89297342300415
Epoch 1860, val loss: 1.3190882205963135
Epoch 1870, training loss: 0.007713365834206343 = 0.000832693069241941 + 0.001 * 6.880672454833984
Epoch 1870, val loss: 1.319433569908142
Epoch 1880, training loss: 0.007713532540947199 = 0.0008279819739982486 + 0.001 * 6.885550022125244
Epoch 1880, val loss: 1.3199124336242676
Epoch 1890, training loss: 0.007688228506594896 = 0.0008233740809373558 + 0.001 * 6.864854335784912
Epoch 1890, val loss: 1.3202534914016724
Epoch 1900, training loss: 0.007672427222132683 = 0.0008188363863155246 + 0.001 * 6.853590488433838
Epoch 1900, val loss: 1.3206875324249268
Epoch 1910, training loss: 0.007746887393295765 = 0.0008143879240378737 + 0.001 * 6.932498931884766
Epoch 1910, val loss: 1.3210338354110718
Epoch 1920, training loss: 0.007686270400881767 = 0.0008100115810520947 + 0.001 * 6.876258373260498
Epoch 1920, val loss: 1.321433186531067
Epoch 1930, training loss: 0.007723573129624128 = 0.0008057109662331641 + 0.001 * 6.9178619384765625
Epoch 1930, val loss: 1.3218142986297607
Epoch 1940, training loss: 0.007676276378333569 = 0.0008014963823370636 + 0.001 * 6.87477970123291
Epoch 1940, val loss: 1.3221156597137451
Epoch 1950, training loss: 0.007659966591745615 = 0.0007973687606863678 + 0.001 * 6.862597465515137
Epoch 1950, val loss: 1.32247793674469
Epoch 1960, training loss: 0.007661568466573954 = 0.0007932926528155804 + 0.001 * 6.8682756423950195
Epoch 1960, val loss: 1.3228296041488647
Epoch 1970, training loss: 0.007645886857062578 = 0.0007892918074503541 + 0.001 * 6.856595039367676
Epoch 1970, val loss: 1.3231823444366455
Epoch 1980, training loss: 0.007660871837288141 = 0.0007853480638004839 + 0.001 * 6.875523567199707
Epoch 1980, val loss: 1.3235177993774414
Epoch 1990, training loss: 0.0077208359725773335 = 0.0007814858108758926 + 0.001 * 6.93934965133667
Epoch 1990, val loss: 1.3238078355789185
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.952201008796692 = 1.9436042308807373 + 0.001 * 8.59682846069336
Epoch 0, val loss: 1.9457778930664062
Epoch 10, training loss: 1.9413660764694214 = 1.9327692985534668 + 0.001 * 8.59673023223877
Epoch 10, val loss: 1.935136079788208
Epoch 20, training loss: 1.9274711608886719 = 1.918874740600586 + 0.001 * 8.596419334411621
Epoch 20, val loss: 1.9212151765823364
Epoch 30, training loss: 1.9075822830200195 = 1.8989866971969604 + 0.001 * 8.595603942871094
Epoch 30, val loss: 1.9014137983322144
Epoch 40, training loss: 1.8784641027450562 = 1.8698707818984985 + 0.001 * 8.593351364135742
Epoch 40, val loss: 1.8731704950332642
Epoch 50, training loss: 1.839500069618225 = 1.8309141397476196 + 0.001 * 8.585938453674316
Epoch 50, val loss: 1.8379268646240234
Epoch 60, training loss: 1.7997499704360962 = 1.7911934852600098 + 0.001 * 8.556513786315918
Epoch 60, val loss: 1.8060672283172607
Epoch 70, training loss: 1.7646925449371338 = 1.756272554397583 + 0.001 * 8.419937133789062
Epoch 70, val loss: 1.7762027978897095
Epoch 80, training loss: 1.7171168327331543 = 1.7090625762939453 + 0.001 * 8.054262161254883
Epoch 80, val loss: 1.731643795967102
Epoch 90, training loss: 1.6515852212905884 = 1.6435993909835815 + 0.001 * 7.985776901245117
Epoch 90, val loss: 1.6741769313812256
Epoch 100, training loss: 1.5685961246490479 = 1.5606440305709839 + 0.001 * 7.952060222625732
Epoch 100, val loss: 1.6054493188858032
Epoch 110, training loss: 1.481030821800232 = 1.4731019735336304 + 0.001 * 7.928853511810303
Epoch 110, val loss: 1.5347840785980225
Epoch 120, training loss: 1.4000087976455688 = 1.392128348350525 + 0.001 * 7.880417823791504
Epoch 120, val loss: 1.470529317855835
Epoch 130, training loss: 1.3244061470031738 = 1.3166661262512207 + 0.001 * 7.739991664886475
Epoch 130, val loss: 1.4117001295089722
Epoch 140, training loss: 1.2504907846450806 = 1.242984652519226 + 0.001 * 7.5061421394348145
Epoch 140, val loss: 1.3550457954406738
Epoch 150, training loss: 1.1761505603790283 = 1.168688178062439 + 0.001 * 7.462412357330322
Epoch 150, val loss: 1.2987605333328247
Epoch 160, training loss: 1.0996185541152954 = 1.09221613407135 + 0.001 * 7.402466297149658
Epoch 160, val loss: 1.240864872932434
Epoch 170, training loss: 1.0190166234970093 = 1.011675238609314 + 0.001 * 7.341440200805664
Epoch 170, val loss: 1.1783933639526367
Epoch 180, training loss: 0.9341084361076355 = 0.926819384098053 + 0.001 * 7.289067268371582
Epoch 180, val loss: 1.1108031272888184
Epoch 190, training loss: 0.8479583263397217 = 0.8406888246536255 + 0.001 * 7.269526958465576
Epoch 190, val loss: 1.0404256582260132
Epoch 200, training loss: 0.765673041343689 = 0.7584066987037659 + 0.001 * 7.266371250152588
Epoch 200, val loss: 0.9731802344322205
Epoch 210, training loss: 0.6911196112632751 = 0.6838564872741699 + 0.001 * 7.263140678405762
Epoch 210, val loss: 0.9134620428085327
Epoch 220, training loss: 0.6250916123390198 = 0.617831289768219 + 0.001 * 7.260299205780029
Epoch 220, val loss: 0.8633994460105896
Epoch 230, training loss: 0.5661584734916687 = 0.5589023232460022 + 0.001 * 7.256160736083984
Epoch 230, val loss: 0.8225998878479004
Epoch 240, training loss: 0.5126551985740662 = 0.5054038166999817 + 0.001 * 7.251400470733643
Epoch 240, val loss: 0.7896154522895813
Epoch 250, training loss: 0.46372467279434204 = 0.4564792215824127 + 0.001 * 7.245451927185059
Epoch 250, val loss: 0.7629669904708862
Epoch 260, training loss: 0.4192624092102051 = 0.4120248258113861 + 0.001 * 7.237579822540283
Epoch 260, val loss: 0.7418515682220459
Epoch 270, training loss: 0.379256933927536 = 0.372030645608902 + 0.001 * 7.226294994354248
Epoch 270, val loss: 0.7258968949317932
Epoch 280, training loss: 0.34327757358551025 = 0.33606967329978943 + 0.001 * 7.207895755767822
Epoch 280, val loss: 0.7145878076553345
Epoch 290, training loss: 0.31046804785728455 = 0.3032809793949127 + 0.001 * 7.187069416046143
Epoch 290, val loss: 0.7070534229278564
Epoch 300, training loss: 0.2798497974872589 = 0.2726990282535553 + 0.001 * 7.15076208114624
Epoch 300, val loss: 0.7024306654930115
Epoch 310, training loss: 0.2506992518901825 = 0.2435777634382248 + 0.001 * 7.12150239944458
Epoch 310, val loss: 0.6999643445014954
Epoch 320, training loss: 0.22281961143016815 = 0.2157164216041565 + 0.001 * 7.103190898895264
Epoch 320, val loss: 0.6992990374565125
Epoch 330, training loss: 0.19664567708969116 = 0.18955175578594208 + 0.001 * 7.093917369842529
Epoch 330, val loss: 0.700452983379364
Epoch 340, training loss: 0.17289264500141144 = 0.16580404341220856 + 0.001 * 7.088598728179932
Epoch 340, val loss: 0.7035542130470276
Epoch 350, training loss: 0.15200020372867584 = 0.14491499960422516 + 0.001 * 7.085207462310791
Epoch 350, val loss: 0.7086696028709412
Epoch 360, training loss: 0.13396218419075012 = 0.1268826425075531 + 0.001 * 7.079535484313965
Epoch 360, val loss: 0.7157984375953674
Epoch 370, training loss: 0.11849403381347656 = 0.11141994595527649 + 0.001 * 7.07409143447876
Epoch 370, val loss: 0.7246565222740173
Epoch 380, training loss: 0.10524551570415497 = 0.09815774857997894 + 0.001 * 7.087768077850342
Epoch 380, val loss: 0.7349017858505249
Epoch 390, training loss: 0.09383074939250946 = 0.08675913512706757 + 0.001 * 7.071617603302002
Epoch 390, val loss: 0.7461332082748413
Epoch 400, training loss: 0.0840006023645401 = 0.07693848013877869 + 0.001 * 7.0621232986450195
Epoch 400, val loss: 0.7580821514129639
Epoch 410, training loss: 0.07551216334104538 = 0.06845292448997498 + 0.001 * 7.059238910675049
Epoch 410, val loss: 0.7705423831939697
Epoch 420, training loss: 0.06816049665212631 = 0.06110546365380287 + 0.001 * 7.0550336837768555
Epoch 420, val loss: 0.7833115458488464
Epoch 430, training loss: 0.061779096722602844 = 0.05472763255238533 + 0.001 * 7.051465034484863
Epoch 430, val loss: 0.7963084578514099
Epoch 440, training loss: 0.05623085796833038 = 0.04917759820818901 + 0.001 * 7.053257942199707
Epoch 440, val loss: 0.8093914985656738
Epoch 450, training loss: 0.05138195678591728 = 0.04433278366923332 + 0.001 * 7.049172878265381
Epoch 450, val loss: 0.8224918842315674
Epoch 460, training loss: 0.04713965207338333 = 0.040090855211019516 + 0.001 * 7.0487961769104
Epoch 460, val loss: 0.8355708122253418
Epoch 470, training loss: 0.04341201111674309 = 0.03636573627591133 + 0.001 * 7.046274185180664
Epoch 470, val loss: 0.8485202193260193
Epoch 480, training loss: 0.0401294007897377 = 0.03308478742837906 + 0.001 * 7.044614315032959
Epoch 480, val loss: 0.8613116145133972
Epoch 490, training loss: 0.03723042458295822 = 0.030186686664819717 + 0.001 * 7.043738842010498
Epoch 490, val loss: 0.8739058971405029
Epoch 500, training loss: 0.03466818854212761 = 0.02761979028582573 + 0.001 * 7.048399448394775
Epoch 500, val loss: 0.8862501382827759
Epoch 510, training loss: 0.0323847234249115 = 0.025340760126709938 + 0.001 * 7.043964862823486
Epoch 510, val loss: 0.8983606696128845
Epoch 520, training loss: 0.03035431168973446 = 0.023312171921133995 + 0.001 * 7.042139053344727
Epoch 520, val loss: 0.9102053046226501
Epoch 530, training loss: 0.028543740510940552 = 0.021502045914530754 + 0.001 * 7.041693687438965
Epoch 530, val loss: 0.921777069568634
Epoch 540, training loss: 0.026924677193164825 = 0.01988300122320652 + 0.001 * 7.041675567626953
Epoch 540, val loss: 0.9330631494522095
Epoch 550, training loss: 0.02547203004360199 = 0.018431266769766808 + 0.001 * 7.040763854980469
Epoch 550, val loss: 0.9440327882766724
Epoch 560, training loss: 0.02416663058102131 = 0.017126137390732765 + 0.001 * 7.040493488311768
Epoch 560, val loss: 0.9547176957130432
Epoch 570, training loss: 0.022998452186584473 = 0.01594993844628334 + 0.001 * 7.048514366149902
Epoch 570, val loss: 0.9651089310646057
Epoch 580, training loss: 0.021926600486040115 = 0.014887377619743347 + 0.001 * 7.0392231941223145
Epoch 580, val loss: 0.9752059578895569
Epoch 590, training loss: 0.02096462808549404 = 0.013925190083682537 + 0.001 * 7.039437294006348
Epoch 590, val loss: 0.9850229024887085
Epoch 600, training loss: 0.020090242847800255 = 0.013051801361143589 + 0.001 * 7.038440704345703
Epoch 600, val loss: 0.9945391416549683
Epoch 610, training loss: 0.019296642392873764 = 0.012257120572030544 + 0.001 * 7.039522647857666
Epoch 610, val loss: 1.0038079023361206
Epoch 620, training loss: 0.018572166562080383 = 0.011532462202012539 + 0.001 * 7.0397047996521
Epoch 620, val loss: 1.0128031969070435
Epoch 630, training loss: 0.017908239737153053 = 0.010870191268622875 + 0.001 * 7.038048267364502
Epoch 630, val loss: 1.0215489864349365
Epoch 640, training loss: 0.017303118482232094 = 0.010263659991323948 + 0.001 * 7.03945779800415
Epoch 640, val loss: 1.0300512313842773
Epoch 650, training loss: 0.016743944957852364 = 0.009707019664347172 + 0.001 * 7.036924362182617
Epoch 650, val loss: 1.0383063554763794
Epoch 660, training loss: 0.016231685876846313 = 0.009195193648338318 + 0.001 * 7.036491870880127
Epoch 660, val loss: 1.0463407039642334
Epoch 670, training loss: 0.015760276466608047 = 0.008723660372197628 + 0.001 * 7.03661584854126
Epoch 670, val loss: 1.0541399717330933
Epoch 680, training loss: 0.015322213061153889 = 0.008288430981338024 + 0.001 * 7.0337815284729
Epoch 680, val loss: 1.061724305152893
Epoch 690, training loss: 0.014919795095920563 = 0.007885927334427834 + 0.001 * 7.033867835998535
Epoch 690, val loss: 1.069096565246582
Epoch 700, training loss: 0.014549851417541504 = 0.007512886542826891 + 0.001 * 7.0369648933410645
Epoch 700, val loss: 1.076279878616333
Epoch 710, training loss: 0.014200491830706596 = 0.007165755610913038 + 0.001 * 7.034736156463623
Epoch 710, val loss: 1.0832698345184326
Epoch 720, training loss: 0.013873681426048279 = 0.006841014139354229 + 0.001 * 7.032667636871338
Epoch 720, val loss: 1.090088963508606
Epoch 730, training loss: 0.013566110283136368 = 0.006535336375236511 + 0.001 * 7.030773639678955
Epoch 730, val loss: 1.0967737436294556
Epoch 740, training loss: 0.013276392593979836 = 0.006246522534638643 + 0.001 * 7.029869556427002
Epoch 740, val loss: 1.1033401489257812
Epoch 750, training loss: 0.013004357926547527 = 0.005973390303552151 + 0.001 * 7.0309672355651855
Epoch 750, val loss: 1.1098008155822754
Epoch 760, training loss: 0.012749619781970978 = 0.005715313833206892 + 0.001 * 7.034305095672607
Epoch 760, val loss: 1.1161754131317139
Epoch 770, training loss: 0.012502111494541168 = 0.005471741780638695 + 0.001 * 7.030369758605957
Epoch 770, val loss: 1.1224136352539062
Epoch 780, training loss: 0.012269293889403343 = 0.005242136772722006 + 0.001 * 7.027156829833984
Epoch 780, val loss: 1.12856125831604
Epoch 790, training loss: 0.012052247300744057 = 0.005025858525186777 + 0.001 * 7.026388645172119
Epoch 790, val loss: 1.1345949172973633
Epoch 800, training loss: 0.011850111186504364 = 0.004822245333343744 + 0.001 * 7.027865886688232
Epoch 800, val loss: 1.1405084133148193
Epoch 810, training loss: 0.011655382812023163 = 0.004630607087165117 + 0.001 * 7.024775981903076
Epoch 810, val loss: 1.1463056802749634
Epoch 820, training loss: 0.011476784944534302 = 0.0044502331875264645 + 0.001 * 7.026551723480225
Epoch 820, val loss: 1.1519994735717773
Epoch 830, training loss: 0.011304449290037155 = 0.004280406516045332 + 0.001 * 7.02404260635376
Epoch 830, val loss: 1.15758216381073
Epoch 840, training loss: 0.01114783063530922 = 0.004120443947613239 + 0.001 * 7.027386665344238
Epoch 840, val loss: 1.1630463600158691
Epoch 850, training loss: 0.010993648320436478 = 0.003969775978475809 + 0.001 * 7.023871421813965
Epoch 850, val loss: 1.1683818101882935
Epoch 860, training loss: 0.010853000916540623 = 0.003827764419838786 + 0.001 * 7.025236129760742
Epoch 860, val loss: 1.1736139059066772
Epoch 870, training loss: 0.010714566335082054 = 0.003693781327456236 + 0.001 * 7.020784854888916
Epoch 870, val loss: 1.1787625551223755
Epoch 880, training loss: 0.01058797538280487 = 0.0035672690719366074 + 0.001 * 7.0207061767578125
Epoch 880, val loss: 1.1837674379348755
Epoch 890, training loss: 0.010469832457602024 = 0.00344772613607347 + 0.001 * 7.022106170654297
Epoch 890, val loss: 1.1886751651763916
Epoch 900, training loss: 0.01035207137465477 = 0.003334691282361746 + 0.001 * 7.017380237579346
Epoch 900, val loss: 1.193482518196106
Epoch 910, training loss: 0.010244104079902172 = 0.0032277258578687906 + 0.001 * 7.016377925872803
Epoch 910, val loss: 1.198188304901123
Epoch 920, training loss: 0.010144135914742947 = 0.0031264096032828093 + 0.001 * 7.017726421356201
Epoch 920, val loss: 1.202784776687622
Epoch 930, training loss: 0.010045541450381279 = 0.0030303646344691515 + 0.001 * 7.015176296234131
Epoch 930, val loss: 1.2072906494140625
Epoch 940, training loss: 0.009963438846170902 = 0.0029392687138170004 + 0.001 * 7.024169445037842
Epoch 940, val loss: 1.2117058038711548
Epoch 950, training loss: 0.009869803674519062 = 0.0028528186958283186 + 0.001 * 7.016984939575195
Epoch 950, val loss: 1.2160090208053589
Epoch 960, training loss: 0.009792995639145374 = 0.002770689083263278 + 0.001 * 7.022306442260742
Epoch 960, val loss: 1.2202330827713013
Epoch 970, training loss: 0.009714031592011452 = 0.0026926067657768726 + 0.001 * 7.021424293518066
Epoch 970, val loss: 1.2243622541427612
Epoch 980, training loss: 0.009628059342503548 = 0.002618306316435337 + 0.001 * 7.00975227355957
Epoch 980, val loss: 1.2284024953842163
Epoch 990, training loss: 0.009556867182254791 = 0.0025475425645709038 + 0.001 * 7.009324073791504
Epoch 990, val loss: 1.2323485612869263
Epoch 1000, training loss: 0.009489146061241627 = 0.002480102004483342 + 0.001 * 7.009044170379639
Epoch 1000, val loss: 1.2362263202667236
Epoch 1010, training loss: 0.00943324901163578 = 0.0024158041924238205 + 0.001 * 7.017445087432861
Epoch 1010, val loss: 1.2400141954421997
Epoch 1020, training loss: 0.009367660619318485 = 0.002354448428377509 + 0.001 * 7.013211727142334
Epoch 1020, val loss: 1.2437407970428467
Epoch 1030, training loss: 0.009300914593040943 = 0.002295864513143897 + 0.001 * 7.005050182342529
Epoch 1030, val loss: 1.2473628520965576
Epoch 1040, training loss: 0.009256361052393913 = 0.0022398950532078743 + 0.001 * 7.016465663909912
Epoch 1040, val loss: 1.2509276866912842
Epoch 1050, training loss: 0.009196245111525059 = 0.002186373807489872 + 0.001 * 7.009871006011963
Epoch 1050, val loss: 1.2544364929199219
Epoch 1060, training loss: 0.009141316637396812 = 0.0021351741161197424 + 0.001 * 7.0061421394348145
Epoch 1060, val loss: 1.2578506469726562
Epoch 1070, training loss: 0.009090963751077652 = 0.002086159074679017 + 0.001 * 7.004804611206055
Epoch 1070, val loss: 1.2612107992172241
Epoch 1080, training loss: 0.009047264233231544 = 0.0020392034202814102 + 0.001 * 7.008060455322266
Epoch 1080, val loss: 1.2644978761672974
Epoch 1090, training loss: 0.009004461579024792 = 0.001994210062548518 + 0.001 * 7.010251522064209
Epoch 1090, val loss: 1.267729640007019
Epoch 1100, training loss: 0.008950854651629925 = 0.0019510680576786399 + 0.001 * 6.999786376953125
Epoch 1100, val loss: 1.270889163017273
Epoch 1110, training loss: 0.008916867896914482 = 0.0019096762407571077 + 0.001 * 7.007191181182861
Epoch 1110, val loss: 1.2739908695220947
Epoch 1120, training loss: 0.00887257419526577 = 0.0018699312349781394 + 0.001 * 7.002642631530762
Epoch 1120, val loss: 1.2770450115203857
Epoch 1130, training loss: 0.00882750190794468 = 0.0018317622598260641 + 0.001 * 6.995738983154297
Epoch 1130, val loss: 1.2800211906433105
Epoch 1140, training loss: 0.00880524329841137 = 0.0017950788605958223 + 0.001 * 7.0101637840271
Epoch 1140, val loss: 1.2829495668411255
Epoch 1150, training loss: 0.008759102784097195 = 0.001759811188094318 + 0.001 * 6.999290943145752
Epoch 1150, val loss: 1.285841941833496
Epoch 1160, training loss: 0.008720642887055874 = 0.0017258875304833055 + 0.001 * 6.994754791259766
Epoch 1160, val loss: 1.2886513471603394
Epoch 1170, training loss: 0.008699115365743637 = 0.0016932389698922634 + 0.001 * 7.005876541137695
Epoch 1170, val loss: 1.2914198637008667
Epoch 1180, training loss: 0.008654658682644367 = 0.0016618092777207494 + 0.001 * 6.992848873138428
Epoch 1180, val loss: 1.2941323518753052
Epoch 1190, training loss: 0.008621800690889359 = 0.0016315345419570804 + 0.001 * 6.990265846252441
Epoch 1190, val loss: 1.296792984008789
Epoch 1200, training loss: 0.008593488484621048 = 0.0016023502685129642 + 0.001 * 6.991138458251953
Epoch 1200, val loss: 1.2994023561477661
Epoch 1210, training loss: 0.008584863506257534 = 0.001574213383719325 + 0.001 * 7.010649681091309
Epoch 1210, val loss: 1.3019717931747437
Epoch 1220, training loss: 0.008546215482056141 = 0.0015470912912860513 + 0.001 * 6.999124050140381
Epoch 1220, val loss: 1.3045077323913574
Epoch 1230, training loss: 0.008513236418366432 = 0.001520912628620863 + 0.001 * 6.99232292175293
Epoch 1230, val loss: 1.3069572448730469
Epoch 1240, training loss: 0.008478039875626564 = 0.0014956413069739938 + 0.001 * 6.982398509979248
Epoch 1240, val loss: 1.3093963861465454
Epoch 1250, training loss: 0.008465725928544998 = 0.001471242867410183 + 0.001 * 6.994482517242432
Epoch 1250, val loss: 1.3117756843566895
Epoch 1260, training loss: 0.008435133844614029 = 0.0014476883225142956 + 0.001 * 6.987445831298828
Epoch 1260, val loss: 1.3141132593154907
Epoch 1270, training loss: 0.008410865440964699 = 0.0014249219093471766 + 0.001 * 6.985942840576172
Epoch 1270, val loss: 1.3164007663726807
Epoch 1280, training loss: 0.008389301598072052 = 0.0014029191806912422 + 0.001 * 6.986382007598877
Epoch 1280, val loss: 1.3186712265014648
Epoch 1290, training loss: 0.00836277473717928 = 0.001381632057018578 + 0.001 * 6.981142044067383
Epoch 1290, val loss: 1.320902943611145
Epoch 1300, training loss: 0.0083396527916193 = 0.0013610315509140491 + 0.001 * 6.978621006011963
Epoch 1300, val loss: 1.3230763673782349
Epoch 1310, training loss: 0.008328759111464024 = 0.001341092400252819 + 0.001 * 6.987666606903076
Epoch 1310, val loss: 1.3252073526382446
Epoch 1320, training loss: 0.008310464210808277 = 0.001321788877248764 + 0.001 * 6.988675117492676
Epoch 1320, val loss: 1.327322006225586
Epoch 1330, training loss: 0.008281322196125984 = 0.0013031002599745989 + 0.001 * 6.978221416473389
Epoch 1330, val loss: 1.3293853998184204
Epoch 1340, training loss: 0.008287779986858368 = 0.0012849941849708557 + 0.001 * 7.0027852058410645
Epoch 1340, val loss: 1.3314090967178345
Epoch 1350, training loss: 0.008251947350800037 = 0.0012674492318183184 + 0.001 * 6.984497547149658
Epoch 1350, val loss: 1.3334276676177979
Epoch 1360, training loss: 0.008219699375331402 = 0.0012504359474405646 + 0.001 * 6.969263553619385
Epoch 1360, val loss: 1.335375428199768
Epoch 1370, training loss: 0.008223709650337696 = 0.0012339407112449408 + 0.001 * 6.9897685050964355
Epoch 1370, val loss: 1.337295651435852
Epoch 1380, training loss: 0.008191840723156929 = 0.001217963290400803 + 0.001 * 6.973876953125
Epoch 1380, val loss: 1.3391884565353394
Epoch 1390, training loss: 0.0081793749704957 = 0.0012024512980133295 + 0.001 * 6.976922988891602
Epoch 1390, val loss: 1.3410414457321167
Epoch 1400, training loss: 0.008162262849509716 = 0.0011874050833284855 + 0.001 * 6.974857330322266
Epoch 1400, val loss: 1.3428853750228882
Epoch 1410, training loss: 0.008145882748067379 = 0.0011727961245924234 + 0.001 * 6.973085880279541
Epoch 1410, val loss: 1.3446699380874634
Epoch 1420, training loss: 0.008132708258926868 = 0.0011586121981963515 + 0.001 * 6.974095821380615
Epoch 1420, val loss: 1.3464555740356445
Epoch 1430, training loss: 0.008116675540804863 = 0.0011448321165516973 + 0.001 * 6.9718427658081055
Epoch 1430, val loss: 1.3481818437576294
Epoch 1440, training loss: 0.008095264434814453 = 0.0011314438888803124 + 0.001 * 6.963819980621338
Epoch 1440, val loss: 1.349901556968689
Epoch 1450, training loss: 0.008085135370492935 = 0.0011184313334524632 + 0.001 * 6.966704368591309
Epoch 1450, val loss: 1.3515772819519043
Epoch 1460, training loss: 0.00807238183915615 = 0.0011057840893045068 + 0.001 * 6.966597080230713
Epoch 1460, val loss: 1.3532309532165527
Epoch 1470, training loss: 0.008096877485513687 = 0.0010934898164123297 + 0.001 * 7.003386974334717
Epoch 1470, val loss: 1.3548420667648315
Epoch 1480, training loss: 0.008051269687712193 = 0.0010815600398927927 + 0.001 * 6.9697089195251465
Epoch 1480, val loss: 1.3564622402191162
Epoch 1490, training loss: 0.008034300059080124 = 0.0010699423728510737 + 0.001 * 6.964357376098633
Epoch 1490, val loss: 1.3580175638198853
Epoch 1500, training loss: 0.008020672015845776 = 0.0010586435673758388 + 0.001 * 6.9620280265808105
Epoch 1500, val loss: 1.359580159187317
Epoch 1510, training loss: 0.008011095225811005 = 0.0010476476745679975 + 0.001 * 6.963447093963623
Epoch 1510, val loss: 1.3611090183258057
Epoch 1520, training loss: 0.00799266342073679 = 0.001036938512697816 + 0.001 * 6.955724239349365
Epoch 1520, val loss: 1.362606167793274
Epoch 1530, training loss: 0.007990015670657158 = 0.0010265088640153408 + 0.001 * 6.96350622177124
Epoch 1530, val loss: 1.364061713218689
Epoch 1540, training loss: 0.007965191267430782 = 0.001016357447952032 + 0.001 * 6.948833465576172
Epoch 1540, val loss: 1.3655301332473755
Epoch 1550, training loss: 0.0080085638910532 = 0.0010064509697258472 + 0.001 * 7.002112865447998
Epoch 1550, val loss: 1.366979718208313
Epoch 1560, training loss: 0.007951696403324604 = 0.0009968273807317019 + 0.001 * 6.954868316650391
Epoch 1560, val loss: 1.3684016466140747
Epoch 1570, training loss: 0.007945835590362549 = 0.0009874362731352448 + 0.001 * 6.958399295806885
Epoch 1570, val loss: 1.3697670698165894
Epoch 1580, training loss: 0.007931050844490528 = 0.0009782736888155341 + 0.001 * 6.95277738571167
Epoch 1580, val loss: 1.3711508512496948
Epoch 1590, training loss: 0.00792816374450922 = 0.0009693405590951443 + 0.001 * 6.958822727203369
Epoch 1590, val loss: 1.3724919557571411
Epoch 1600, training loss: 0.007926528342068195 = 0.0009606228559277952 + 0.001 * 6.96590518951416
Epoch 1600, val loss: 1.3738363981246948
Epoch 1610, training loss: 0.00790016632527113 = 0.0009521317551843822 + 0.001 * 6.948034286499023
Epoch 1610, val loss: 1.3751329183578491
Epoch 1620, training loss: 0.00789000652730465 = 0.0009438394336029887 + 0.001 * 6.946166515350342
Epoch 1620, val loss: 1.3764331340789795
Epoch 1630, training loss: 0.00788195338100195 = 0.0009357422241009772 + 0.001 * 6.946210861206055
Epoch 1630, val loss: 1.3776988983154297
Epoch 1640, training loss: 0.007871336303651333 = 0.0009278415818698704 + 0.001 * 6.94349479675293
Epoch 1640, val loss: 1.378969669342041
Epoch 1650, training loss: 0.007870987989008427 = 0.0009201267384923995 + 0.001 * 6.950860977172852
Epoch 1650, val loss: 1.380218505859375
Epoch 1660, training loss: 0.007853747345507145 = 0.000912601943127811 + 0.001 * 6.941144943237305
Epoch 1660, val loss: 1.3814231157302856
Epoch 1670, training loss: 0.007868076674640179 = 0.0009052474633790553 + 0.001 * 6.962829113006592
Epoch 1670, val loss: 1.3826391696929932
Epoch 1680, training loss: 0.00784354843199253 = 0.0008980676648207009 + 0.001 * 6.9454803466796875
Epoch 1680, val loss: 1.38384211063385
Epoch 1690, training loss: 0.00788265373557806 = 0.0008910372271202505 + 0.001 * 6.991616249084473
Epoch 1690, val loss: 1.3850023746490479
Epoch 1700, training loss: 0.00782956276088953 = 0.000884193170350045 + 0.001 * 6.945368766784668
Epoch 1700, val loss: 1.386177659034729
Epoch 1710, training loss: 0.00781475380063057 = 0.0008774888701736927 + 0.001 * 6.9372639656066895
Epoch 1710, val loss: 1.3873188495635986
Epoch 1720, training loss: 0.007823211140930653 = 0.0008709423709660769 + 0.001 * 6.952268123626709
Epoch 1720, val loss: 1.3884624242782593
Epoch 1730, training loss: 0.007804814726114273 = 0.0008645537891425192 + 0.001 * 6.940260887145996
Epoch 1730, val loss: 1.3895726203918457
Epoch 1740, training loss: 0.00781664252281189 = 0.0008582976879552007 + 0.001 * 6.958343982696533
Epoch 1740, val loss: 1.3906818628311157
Epoch 1750, training loss: 0.007778996601700783 = 0.0008521958952769637 + 0.001 * 6.92680025100708
Epoch 1750, val loss: 1.3917614221572876
Epoch 1760, training loss: 0.007797308266162872 = 0.0008462220430374146 + 0.001 * 6.951086044311523
Epoch 1760, val loss: 1.392839789390564
Epoch 1770, training loss: 0.007765619084239006 = 0.0008403772371821105 + 0.001 * 6.925241470336914
Epoch 1770, val loss: 1.3939024209976196
Epoch 1780, training loss: 0.007796939928084612 = 0.0008346520480699837 + 0.001 * 6.962287425994873
Epoch 1780, val loss: 1.3949707746505737
Epoch 1790, training loss: 0.007767883595079184 = 0.0008290620753541589 + 0.001 * 6.938821315765381
Epoch 1790, val loss: 1.3960115909576416
Epoch 1800, training loss: 0.007759518921375275 = 0.000823583803139627 + 0.001 * 6.935934543609619
Epoch 1800, val loss: 1.3970115184783936
Epoch 1810, training loss: 0.007743452675640583 = 0.0008182236342690885 + 0.001 * 6.925228595733643
Epoch 1810, val loss: 1.398024082183838
Epoch 1820, training loss: 0.00773369986563921 = 0.0008129681809805334 + 0.001 * 6.920731067657471
Epoch 1820, val loss: 1.3990291357040405
Epoch 1830, training loss: 0.007766833063215017 = 0.0008078081300482154 + 0.001 * 6.959024429321289
Epoch 1830, val loss: 1.400012493133545
Epoch 1840, training loss: 0.007727860938757658 = 0.0008027505245991051 + 0.001 * 6.92510986328125
Epoch 1840, val loss: 1.4010099172592163
Epoch 1850, training loss: 0.00772881880402565 = 0.0007978070061653852 + 0.001 * 6.93101167678833
Epoch 1850, val loss: 1.4019685983657837
Epoch 1860, training loss: 0.00770795252174139 = 0.0007929514977149665 + 0.001 * 6.915000915527344
Epoch 1860, val loss: 1.4029340744018555
Epoch 1870, training loss: 0.007702425122261047 = 0.0007882014033384621 + 0.001 * 6.9142231941223145
Epoch 1870, val loss: 1.4038832187652588
Epoch 1880, training loss: 0.007696385495364666 = 0.0007835397263988853 + 0.001 * 6.912845134735107
Epoch 1880, val loss: 1.4048396348953247
Epoch 1890, training loss: 0.007709605619311333 = 0.0007789746741764247 + 0.001 * 6.930630683898926
Epoch 1890, val loss: 1.4057470560073853
Epoch 1900, training loss: 0.007703083101660013 = 0.0007745003676973283 + 0.001 * 6.928582191467285
Epoch 1900, val loss: 1.406679391860962
Epoch 1910, training loss: 0.007712891791015863 = 0.0007701121503487229 + 0.001 * 6.942779541015625
Epoch 1910, val loss: 1.4075909852981567
Epoch 1920, training loss: 0.007692055311053991 = 0.0007658060058020055 + 0.001 * 6.926249027252197
Epoch 1920, val loss: 1.4084936380386353
Epoch 1930, training loss: 0.007678416091948748 = 0.0007615851936861873 + 0.001 * 6.916830539703369
Epoch 1930, val loss: 1.4093772172927856
Epoch 1940, training loss: 0.007655705325305462 = 0.0007574388291686773 + 0.001 * 6.898266315460205
Epoch 1940, val loss: 1.4102561473846436
Epoch 1950, training loss: 0.0076603577472269535 = 0.0007533688913099468 + 0.001 * 6.906988620758057
Epoch 1950, val loss: 1.411133885383606
Epoch 1960, training loss: 0.007665805984288454 = 0.0007493813754990697 + 0.001 * 6.91642427444458
Epoch 1960, val loss: 1.4120049476623535
Epoch 1970, training loss: 0.007688907906413078 = 0.0007454645237885416 + 0.001 * 6.9434428215026855
Epoch 1970, val loss: 1.4128718376159668
Epoch 1980, training loss: 0.007655712775886059 = 0.0007416270091198385 + 0.001 * 6.914085388183594
Epoch 1980, val loss: 1.4137085676193237
Epoch 1990, training loss: 0.00765976682305336 = 0.0007378290756605566 + 0.001 * 6.921937465667725
Epoch 1990, val loss: 1.4145517349243164
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 1.9617211818695068 = 1.9531242847442627 + 0.001 * 8.596841812133789
Epoch 0, val loss: 1.960510015487671
Epoch 10, training loss: 1.9521700143814087 = 1.943573236465454 + 0.001 * 8.596787452697754
Epoch 10, val loss: 1.9505133628845215
Epoch 20, training loss: 1.9404650926589966 = 1.931868553161621 + 0.001 * 8.596567153930664
Epoch 20, val loss: 1.9381169080734253
Epoch 30, training loss: 1.9238773584365845 = 1.9152814149856567 + 0.001 * 8.595985412597656
Epoch 30, val loss: 1.9206812381744385
Epoch 40, training loss: 1.8989671468734741 = 1.8903728723526 + 0.001 * 8.594322204589844
Epoch 40, val loss: 1.894932508468628
Epoch 50, training loss: 1.8629233837127686 = 1.8543347120285034 + 0.001 * 8.588617324829102
Epoch 50, val loss: 1.8591535091400146
Epoch 60, training loss: 1.8201884031295776 = 1.8116230964660645 + 0.001 * 8.565276145935059
Epoch 60, val loss: 1.8203608989715576
Epoch 70, training loss: 1.783708930015564 = 1.775251030921936 + 0.001 * 8.457887649536133
Epoch 70, val loss: 1.790282130241394
Epoch 80, training loss: 1.7441939115524292 = 1.7360541820526123 + 0.001 * 8.139699935913086
Epoch 80, val loss: 1.7546254396438599
Epoch 90, training loss: 1.689626693725586 = 1.681652545928955 + 0.001 * 7.974196434020996
Epoch 90, val loss: 1.7050262689590454
Epoch 100, training loss: 1.6146739721298218 = 1.6069567203521729 + 0.001 * 7.717193603515625
Epoch 100, val loss: 1.6395152807235718
Epoch 110, training loss: 1.5191988945007324 = 1.5116313695907593 + 0.001 * 7.567550182342529
Epoch 110, val loss: 1.5584371089935303
Epoch 120, training loss: 1.4102591276168823 = 1.4026975631713867 + 0.001 * 7.56151008605957
Epoch 120, val loss: 1.4668041467666626
Epoch 130, training loss: 1.2946932315826416 = 1.2871614694595337 + 0.001 * 7.531821250915527
Epoch 130, val loss: 1.3715696334838867
Epoch 140, training loss: 1.1760550737380981 = 1.1685585975646973 + 0.001 * 7.496495246887207
Epoch 140, val loss: 1.2736612558364868
Epoch 150, training loss: 1.058376431465149 = 1.0509380102157593 + 0.001 * 7.438385486602783
Epoch 150, val loss: 1.1776623725891113
Epoch 160, training loss: 0.9475527405738831 = 0.9401695728302002 + 0.001 * 7.383191108703613
Epoch 160, val loss: 1.0881320238113403
Epoch 170, training loss: 0.8483482003211975 = 0.8409853577613831 + 0.001 * 7.362825393676758
Epoch 170, val loss: 1.009605884552002
Epoch 180, training loss: 0.7626088857650757 = 0.755264401435852 + 0.001 * 7.344466209411621
Epoch 180, val loss: 0.9447129964828491
Epoch 190, training loss: 0.6893051862716675 = 0.6819734573364258 + 0.001 * 7.331738471984863
Epoch 190, val loss: 0.8937520384788513
Epoch 200, training loss: 0.6256963014602661 = 0.6183770298957825 + 0.001 * 7.319243431091309
Epoch 200, val loss: 0.8550493121147156
Epoch 210, training loss: 0.5689722895622253 = 0.5616674423217773 + 0.001 * 7.304831504821777
Epoch 210, val loss: 0.8259723782539368
Epoch 220, training loss: 0.5173403024673462 = 0.5100485682487488 + 0.001 * 7.291745185852051
Epoch 220, val loss: 0.8042848706245422
Epoch 230, training loss: 0.46982213854789734 = 0.46254658699035645 + 0.001 * 7.275552272796631
Epoch 230, val loss: 0.788297712802887
Epoch 240, training loss: 0.4258561432361603 = 0.4185965657234192 + 0.001 * 7.25958251953125
Epoch 240, val loss: 0.7768417000770569
Epoch 250, training loss: 0.38519227504730225 = 0.3779473602771759 + 0.001 * 7.2449188232421875
Epoch 250, val loss: 0.7693243026733398
Epoch 260, training loss: 0.34766972064971924 = 0.34044149518013 + 0.001 * 7.2282185554504395
Epoch 260, val loss: 0.7657444477081299
Epoch 270, training loss: 0.31315699219703674 = 0.3059403598308563 + 0.001 * 7.216622352600098
Epoch 270, val loss: 0.7661563158035278
Epoch 280, training loss: 0.2815091609954834 = 0.2742992639541626 + 0.001 * 7.2098870277404785
Epoch 280, val loss: 0.7701917290687561
Epoch 290, training loss: 0.2525547444820404 = 0.2453528344631195 + 0.001 * 7.201913833618164
Epoch 290, val loss: 0.7774176001548767
Epoch 300, training loss: 0.22615256905555725 = 0.21895526349544525 + 0.001 * 7.197310924530029
Epoch 300, val loss: 0.7875036001205444
Epoch 310, training loss: 0.2021675407886505 = 0.19497716426849365 + 0.001 * 7.190369129180908
Epoch 310, val loss: 0.8001481890678406
Epoch 320, training loss: 0.18048907816410065 = 0.17330732941627502 + 0.001 * 7.181746482849121
Epoch 320, val loss: 0.8148607015609741
Epoch 330, training loss: 0.1610388606786728 = 0.15386098623275757 + 0.001 * 7.1778764724731445
Epoch 330, val loss: 0.8313438892364502
Epoch 340, training loss: 0.14372462034225464 = 0.13654854893684387 + 0.001 * 7.176066875457764
Epoch 340, val loss: 0.8492117524147034
Epoch 350, training loss: 0.12842491269111633 = 0.12124919891357422 + 0.001 * 7.175713539123535
Epoch 350, val loss: 0.8681244850158691
Epoch 360, training loss: 0.11497583240270615 = 0.10780282318592072 + 0.001 * 7.1730055809021
Epoch 360, val loss: 0.8877601027488708
Epoch 370, training loss: 0.10318488627672195 = 0.09601481258869171 + 0.001 * 7.17007303237915
Epoch 370, val loss: 0.907918393611908
Epoch 380, training loss: 0.09285543859004974 = 0.0856938362121582 + 0.001 * 7.161602973937988
Epoch 380, val loss: 0.928438663482666
Epoch 390, training loss: 0.08380953222513199 = 0.07665469497442245 + 0.001 * 7.154839992523193
Epoch 390, val loss: 0.949027955532074
Epoch 400, training loss: 0.07587777078151703 = 0.06873250007629395 + 0.001 * 7.145268440246582
Epoch 400, val loss: 0.9695414304733276
Epoch 410, training loss: 0.06892247498035431 = 0.06178177148103714 + 0.001 * 7.140706539154053
Epoch 410, val loss: 0.9897858500480652
Epoch 420, training loss: 0.0628158375620842 = 0.05567409470677376 + 0.001 * 7.1417412757873535
Epoch 420, val loss: 1.0096644163131714
Epoch 430, training loss: 0.05744245648384094 = 0.05029649659991264 + 0.001 * 7.145961284637451
Epoch 430, val loss: 1.0290863513946533
Epoch 440, training loss: 0.05268629267811775 = 0.04555271193385124 + 0.001 * 7.133579730987549
Epoch 440, val loss: 1.0479527711868286
Epoch 450, training loss: 0.04850361496210098 = 0.04136018082499504 + 0.001 * 7.143432140350342
Epoch 450, val loss: 1.066208004951477
Epoch 460, training loss: 0.044788211584091187 = 0.03764786943793297 + 0.001 * 7.140341281890869
Epoch 460, val loss: 1.083822250366211
Epoch 470, training loss: 0.041487351059913635 = 0.03435451164841652 + 0.001 * 7.132839679718018
Epoch 470, val loss: 1.1008179187774658
Epoch 480, training loss: 0.03855637460947037 = 0.03142658248543739 + 0.001 * 7.1297926902771
Epoch 480, val loss: 1.117200255393982
Epoch 490, training loss: 0.03593915328383446 = 0.02881859801709652 + 0.001 * 7.120553970336914
Epoch 490, val loss: 1.1329530477523804
Epoch 500, training loss: 0.033634115010499954 = 0.026490941643714905 + 0.001 * 7.143172264099121
Epoch 500, val loss: 1.1480587720870972
Epoch 510, training loss: 0.03153321146965027 = 0.024409601464867592 + 0.001 * 7.123607635498047
Epoch 510, val loss: 1.1625514030456543
Epoch 520, training loss: 0.029647860676050186 = 0.02254476211965084 + 0.001 * 7.103099346160889
Epoch 520, val loss: 1.1764403581619263
Epoch 530, training loss: 0.027972426265478134 = 0.020870337262749672 + 0.001 * 7.10208797454834
Epoch 530, val loss: 1.1897724866867065
Epoch 540, training loss: 0.02646695449948311 = 0.019363803789019585 + 0.001 * 7.103151321411133
Epoch 540, val loss: 1.202552318572998
Epoch 550, training loss: 0.02510528638958931 = 0.01800559088587761 + 0.001 * 7.099695682525635
Epoch 550, val loss: 1.214795470237732
Epoch 560, training loss: 0.023874441161751747 = 0.01677863672375679 + 0.001 * 7.095803737640381
Epoch 560, val loss: 1.2265310287475586
Epoch 570, training loss: 0.02277401275932789 = 0.015667853876948357 + 0.001 * 7.106158256530762
Epoch 570, val loss: 1.2377958297729492
Epoch 580, training loss: 0.021754521876573563 = 0.014660270884633064 + 0.001 * 7.094249725341797
Epoch 580, val loss: 1.248595118522644
Epoch 590, training loss: 0.020849380642175674 = 0.013744097203016281 + 0.001 * 7.105283737182617
Epoch 590, val loss: 1.2589644193649292
Epoch 600, training loss: 0.020003248006105423 = 0.012907803989946842 + 0.001 * 7.095443248748779
Epoch 600, val loss: 1.2689467668533325
Epoch 610, training loss: 0.01923171617090702 = 0.012139271013438702 + 0.001 * 7.09244441986084
Epoch 610, val loss: 1.2785851955413818
Epoch 620, training loss: 0.0185307078063488 = 0.01142896618694067 + 0.001 * 7.101741313934326
Epoch 620, val loss: 1.2879444360733032
Epoch 630, training loss: 0.01785082370042801 = 0.010770493187010288 + 0.001 * 7.0803303718566895
Epoch 630, val loss: 1.297059416770935
Epoch 640, training loss: 0.01725165732204914 = 0.010159874334931374 + 0.001 * 7.091783046722412
Epoch 640, val loss: 1.305884838104248
Epoch 650, training loss: 0.016683755442500114 = 0.009593769907951355 + 0.001 * 7.089984893798828
Epoch 650, val loss: 1.3144149780273438
Epoch 660, training loss: 0.016151830554008484 = 0.009069077670574188 + 0.001 * 7.0827531814575195
Epoch 660, val loss: 1.3226759433746338
Epoch 670, training loss: 0.015667002648115158 = 0.008582894690334797 + 0.001 * 7.084106922149658
Epoch 670, val loss: 1.3306665420532227
Epoch 680, training loss: 0.015207020565867424 = 0.008132115006446838 + 0.001 * 7.0749053955078125
Epoch 680, val loss: 1.3384177684783936
Epoch 690, training loss: 0.01478189043700695 = 0.0077139269560575485 + 0.001 * 7.067962646484375
Epoch 690, val loss: 1.3458938598632812
Epoch 700, training loss: 0.01439734362065792 = 0.007325427606701851 + 0.001 * 7.071915149688721
Epoch 700, val loss: 1.3531461954116821
Epoch 710, training loss: 0.014029290527105331 = 0.006963904481381178 + 0.001 * 7.065385341644287
Epoch 710, val loss: 1.3601547479629517
Epoch 720, training loss: 0.013695042580366135 = 0.006627229508012533 + 0.001 * 7.067812442779541
Epoch 720, val loss: 1.366958498954773
Epoch 730, training loss: 0.013368474319577217 = 0.006309313699603081 + 0.001 * 7.0591607093811035
Epoch 730, val loss: 1.3736947774887085
Epoch 740, training loss: 0.01309506967663765 = 0.006012872792780399 + 0.001 * 7.082196235656738
Epoch 740, val loss: 1.3800116777420044
Epoch 750, training loss: 0.012830222025513649 = 0.005735459737479687 + 0.001 * 7.094762325286865
Epoch 750, val loss: 1.386398434638977
Epoch 760, training loss: 0.012538332492113113 = 0.005476920399814844 + 0.001 * 7.061412334442139
Epoch 760, val loss: 1.392490029335022
Epoch 770, training loss: 0.012288369238376617 = 0.005235074553638697 + 0.001 * 7.053293704986572
Epoch 770, val loss: 1.39842689037323
Epoch 780, training loss: 0.01206322479993105 = 0.005008786916732788 + 0.001 * 7.054437637329102
Epoch 780, val loss: 1.4041647911071777
Epoch 790, training loss: 0.011862162500619888 = 0.004796885419636965 + 0.001 * 7.065276622772217
Epoch 790, val loss: 1.4096572399139404
Epoch 800, training loss: 0.01164967194199562 = 0.004598530940711498 + 0.001 * 7.051140785217285
Epoch 800, val loss: 1.4149833917617798
Epoch 810, training loss: 0.011467400938272476 = 0.0044124918058514595 + 0.001 * 7.054908275604248
Epoch 810, val loss: 1.420164942741394
Epoch 820, training loss: 0.011282363906502724 = 0.00423783203586936 + 0.001 * 7.044530868530273
Epoch 820, val loss: 1.4251275062561035
Epoch 830, training loss: 0.011115798726677895 = 0.004073758143931627 + 0.001 * 7.04203987121582
Epoch 830, val loss: 1.4299782514572144
Epoch 840, training loss: 0.010964613407850266 = 0.003919653128832579 + 0.001 * 7.0449604988098145
Epoch 840, val loss: 1.434648036956787
Epoch 850, training loss: 0.010814775712788105 = 0.0037747183814644814 + 0.001 * 7.040057182312012
Epoch 850, val loss: 1.4391849040985107
Epoch 860, training loss: 0.010679972358047962 = 0.0036382710095494986 + 0.001 * 7.041701316833496
Epoch 860, val loss: 1.44356369972229
Epoch 870, training loss: 0.010546088218688965 = 0.0035096199717372656 + 0.001 * 7.036467552185059
Epoch 870, val loss: 1.4478061199188232
Epoch 880, training loss: 0.010433688759803772 = 0.00338819925673306 + 0.001 * 7.045489311218262
Epoch 880, val loss: 1.451918601989746
Epoch 890, training loss: 0.01032092422246933 = 0.003273514099419117 + 0.001 * 7.047409534454346
Epoch 890, val loss: 1.4559078216552734
Epoch 900, training loss: 0.010207461193203926 = 0.003165037604048848 + 0.001 * 7.042423248291016
Epoch 900, val loss: 1.4597545862197876
Epoch 910, training loss: 0.010110653936862946 = 0.0030624158680438995 + 0.001 * 7.0482378005981445
Epoch 910, val loss: 1.463502049446106
Epoch 920, training loss: 0.010023984126746655 = 0.0029653755482286215 + 0.001 * 7.058608055114746
Epoch 920, val loss: 1.4671467542648315
Epoch 930, training loss: 0.009918760508298874 = 0.002873470541089773 + 0.001 * 7.045289516448975
Epoch 930, val loss: 1.4706300497055054
Epoch 940, training loss: 0.009812980890274048 = 0.002786362776532769 + 0.001 * 7.026618003845215
Epoch 940, val loss: 1.4740593433380127
Epoch 950, training loss: 0.009722501039505005 = 0.0027037051040679216 + 0.001 * 7.018795967102051
Epoch 950, val loss: 1.4773396253585815
Epoch 960, training loss: 0.00964448880404234 = 0.0026252232491970062 + 0.001 * 7.019265174865723
Epoch 960, val loss: 1.4805711507797241
Epoch 970, training loss: 0.009567303583025932 = 0.0025506457313895226 + 0.001 * 7.016656875610352
Epoch 970, val loss: 1.483651041984558
Epoch 980, training loss: 0.009501047432422638 = 0.0024796724319458008 + 0.001 * 7.021374702453613
Epoch 980, val loss: 1.4866830110549927
Epoch 990, training loss: 0.009456923231482506 = 0.0024120062589645386 + 0.001 * 7.044916152954102
Epoch 990, val loss: 1.489579439163208
Epoch 1000, training loss: 0.009368320927023888 = 0.0023475619964301586 + 0.001 * 7.020758628845215
Epoch 1000, val loss: 1.4924522638320923
Epoch 1010, training loss: 0.00930635817348957 = 0.002286020200699568 + 0.001 * 7.0203375816345215
Epoch 1010, val loss: 1.4951914548873901
Epoch 1020, training loss: 0.009238047525286674 = 0.0022272286005318165 + 0.001 * 7.010818004608154
Epoch 1020, val loss: 1.4979138374328613
Epoch 1030, training loss: 0.009183809161186218 = 0.0021708703134208918 + 0.001 * 7.012938976287842
Epoch 1030, val loss: 1.5005298852920532
Epoch 1040, training loss: 0.009122094139456749 = 0.002116807736456394 + 0.001 * 7.005286693572998
Epoch 1040, val loss: 1.5031033754348755
Epoch 1050, training loss: 0.009108595550060272 = 0.002064728643745184 + 0.001 * 7.0438666343688965
Epoch 1050, val loss: 1.5056376457214355
Epoch 1060, training loss: 0.009015980176627636 = 0.0020144120790064335 + 0.001 * 7.001567840576172
Epoch 1060, val loss: 1.5080907344818115
Epoch 1070, training loss: 0.008992623537778854 = 0.001965705770999193 + 0.001 * 7.026917934417725
Epoch 1070, val loss: 1.5105316638946533
Epoch 1080, training loss: 0.008913833647966385 = 0.001918485388159752 + 0.001 * 6.9953484535217285
Epoch 1080, val loss: 1.512910008430481
Epoch 1090, training loss: 0.008898192085325718 = 0.0018726774724200368 + 0.001 * 7.025514602661133
Epoch 1090, val loss: 1.5153088569641113
Epoch 1100, training loss: 0.008847415447235107 = 0.0018282157834619284 + 0.001 * 7.019199371337891
Epoch 1100, val loss: 1.517641544342041
Epoch 1110, training loss: 0.00878127384930849 = 0.001785094733349979 + 0.001 * 6.99617862701416
Epoch 1110, val loss: 1.5199564695358276
Epoch 1120, training loss: 0.008736430667340755 = 0.0017432894092053175 + 0.001 * 6.993140697479248
Epoch 1120, val loss: 1.5222742557525635
Epoch 1130, training loss: 0.00869290716946125 = 0.0017028376460075378 + 0.001 * 6.99006986618042
Epoch 1130, val loss: 1.5245288610458374
Epoch 1140, training loss: 0.008663374930620193 = 0.0016636591171845794 + 0.001 * 6.999715805053711
Epoch 1140, val loss: 1.5268301963806152
Epoch 1150, training loss: 0.008655224926769733 = 0.0016257602255791426 + 0.001 * 7.0294647216796875
Epoch 1150, val loss: 1.5290849208831787
Epoch 1160, training loss: 0.008597858250141144 = 0.0015891470247879624 + 0.001 * 7.0087103843688965
Epoch 1160, val loss: 1.5312696695327759
Epoch 1170, training loss: 0.008574974723160267 = 0.0015538530424237251 + 0.001 * 7.021121501922607
Epoch 1170, val loss: 1.5335108041763306
Epoch 1180, training loss: 0.008544588461518288 = 0.001519773039035499 + 0.001 * 7.024815559387207
Epoch 1180, val loss: 1.5356329679489136
Epoch 1190, training loss: 0.008489920757710934 = 0.0014870604500174522 + 0.001 * 7.002860069274902
Epoch 1190, val loss: 1.5378073453903198
Epoch 1200, training loss: 0.008462327532470226 = 0.001455464051105082 + 0.001 * 7.006863117218018
Epoch 1200, val loss: 1.539880394935608
Epoch 1210, training loss: 0.00839900504797697 = 0.0014250760432332754 + 0.001 * 6.973928928375244
Epoch 1210, val loss: 1.5419785976409912
Epoch 1220, training loss: 0.008376984857022762 = 0.001395822037011385 + 0.001 * 6.9811625480651855
Epoch 1220, val loss: 1.5440270900726318
Epoch 1230, training loss: 0.008352207951247692 = 0.0013676985399797559 + 0.001 * 6.984508514404297
Epoch 1230, val loss: 1.546042561531067
Epoch 1240, training loss: 0.008314288221299648 = 0.0013406621292233467 + 0.001 * 6.973625659942627
Epoch 1240, val loss: 1.5480073690414429
Epoch 1250, training loss: 0.008298907428979874 = 0.0013146200217306614 + 0.001 * 6.984286785125732
Epoch 1250, val loss: 1.5499153137207031
Epoch 1260, training loss: 0.008255893364548683 = 0.0012895771069452167 + 0.001 * 6.966316223144531
Epoch 1260, val loss: 1.551832675933838
Epoch 1270, training loss: 0.00824500247836113 = 0.0012654196470975876 + 0.001 * 6.979582786560059
Epoch 1270, val loss: 1.5536726713180542
Epoch 1280, training loss: 0.008240779861807823 = 0.0012421718565747142 + 0.001 * 6.998607635498047
Epoch 1280, val loss: 1.5555301904678345
Epoch 1290, training loss: 0.008178884163498878 = 0.0012197669129818678 + 0.001 * 6.9591169357299805
Epoch 1290, val loss: 1.5572923421859741
Epoch 1300, training loss: 0.008174357004463673 = 0.0011982310097664595 + 0.001 * 6.976125240325928
Epoch 1300, val loss: 1.5590786933898926
Epoch 1310, training loss: 0.008137800730764866 = 0.0011774672893807292 + 0.001 * 6.960332870483398
Epoch 1310, val loss: 1.560748815536499
Epoch 1320, training loss: 0.008115272969007492 = 0.0011574402451515198 + 0.001 * 6.957832336425781
Epoch 1320, val loss: 1.5624462366104126
Epoch 1330, training loss: 0.008114159107208252 = 0.0011381222866475582 + 0.001 * 6.976036071777344
Epoch 1330, val loss: 1.5640977621078491
Epoch 1340, training loss: 0.008103695698082447 = 0.0011194872204214334 + 0.001 * 6.984208106994629
Epoch 1340, val loss: 1.5656555891036987
Epoch 1350, training loss: 0.008073557168245316 = 0.0011015983764082193 + 0.001 * 6.971958160400391
Epoch 1350, val loss: 1.5672537088394165
Epoch 1360, training loss: 0.008046750910580158 = 0.0010843201307579875 + 0.001 * 6.962430477142334
Epoch 1360, val loss: 1.5687814950942993
Epoch 1370, training loss: 0.008028985932469368 = 0.0010676635429263115 + 0.001 * 6.96132230758667
Epoch 1370, val loss: 1.5702691078186035
Epoch 1380, training loss: 0.008016921579837799 = 0.0010516017209738493 + 0.001 * 6.965319633483887
Epoch 1380, val loss: 1.5717412233352661
Epoch 1390, training loss: 0.008009434677660465 = 0.0010360846063122153 + 0.001 * 6.9733500480651855
Epoch 1390, val loss: 1.5731323957443237
Epoch 1400, training loss: 0.008014598861336708 = 0.0010212144115939736 + 0.001 * 6.993383884429932
Epoch 1400, val loss: 1.5745352506637573
Epoch 1410, training loss: 0.007962465286254883 = 0.0010068233823403716 + 0.001 * 6.955641269683838
Epoch 1410, val loss: 1.5758216381072998
Epoch 1420, training loss: 0.007932542823255062 = 0.0009929586667567492 + 0.001 * 6.939584255218506
Epoch 1420, val loss: 1.5771362781524658
Epoch 1430, training loss: 0.007918208837509155 = 0.0009795452933758497 + 0.001 * 6.938663005828857
Epoch 1430, val loss: 1.578407645225525
Epoch 1440, training loss: 0.007947605103254318 = 0.0009665527613833547 + 0.001 * 6.981051445007324
Epoch 1440, val loss: 1.5796430110931396
Epoch 1450, training loss: 0.007964526303112507 = 0.0009540293249301612 + 0.001 * 7.010496616363525
Epoch 1450, val loss: 1.580880880355835
Epoch 1460, training loss: 0.007889504544436932 = 0.0009418754489161074 + 0.001 * 6.947628498077393
Epoch 1460, val loss: 1.5820136070251465
Epoch 1470, training loss: 0.007865022867918015 = 0.0009301451500505209 + 0.001 * 6.934877395629883
Epoch 1470, val loss: 1.583158254623413
Epoch 1480, training loss: 0.007879863493144512 = 0.000918769568670541 + 0.001 * 6.961093902587891
Epoch 1480, val loss: 1.5842801332473755
Epoch 1490, training loss: 0.007858414202928543 = 0.000907793000806123 + 0.001 * 6.950620651245117
Epoch 1490, val loss: 1.5853815078735352
Epoch 1500, training loss: 0.007821205072104931 = 0.0008971370989456773 + 0.001 * 6.924067497253418
Epoch 1500, val loss: 1.586390495300293
Epoch 1510, training loss: 0.007825148291885853 = 0.0008868318400345743 + 0.001 * 6.938316345214844
Epoch 1510, val loss: 1.5874546766281128
Epoch 1520, training loss: 0.007809363305568695 = 0.0008768239058554173 + 0.001 * 6.932538986206055
Epoch 1520, val loss: 1.5884239673614502
Epoch 1530, training loss: 0.00780253391712904 = 0.0008671394316479564 + 0.001 * 6.935393810272217
Epoch 1530, val loss: 1.5893971920013428
Epoch 1540, training loss: 0.007790002040565014 = 0.0008577487897127867 + 0.001 * 6.932253360748291
Epoch 1540, val loss: 1.590348720550537
Epoch 1550, training loss: 0.007782419677823782 = 0.000848617113661021 + 0.001 * 6.933802604675293
Epoch 1550, val loss: 1.5912349224090576
Epoch 1560, training loss: 0.007791776210069656 = 0.0008398087229579687 + 0.001 * 6.951967239379883
Epoch 1560, val loss: 1.592122197151184
Epoch 1570, training loss: 0.007774048950523138 = 0.0008312965510413051 + 0.001 * 6.942751884460449
Epoch 1570, val loss: 1.592988133430481
Epoch 1580, training loss: 0.007806497160345316 = 0.0008229975937865674 + 0.001 * 6.983499050140381
Epoch 1580, val loss: 1.5937702655792236
Epoch 1590, training loss: 0.007753521203994751 = 0.0008150087087415159 + 0.001 * 6.938512325286865
Epoch 1590, val loss: 1.5945926904678345
Epoch 1600, training loss: 0.007740102708339691 = 0.0008072066120803356 + 0.001 * 6.932895660400391
Epoch 1600, val loss: 1.5953216552734375
Epoch 1610, training loss: 0.00771991116926074 = 0.0007996553322300315 + 0.001 * 6.920255184173584
Epoch 1610, val loss: 1.5961062908172607
Epoch 1620, training loss: 0.007728530094027519 = 0.0007923034718260169 + 0.001 * 6.9362263679504395
Epoch 1620, val loss: 1.5968273878097534
Epoch 1630, training loss: 0.0077043576166033745 = 0.0007851826376281679 + 0.001 * 6.919174671173096
Epoch 1630, val loss: 1.5975407361984253
Epoch 1640, training loss: 0.007703801617026329 = 0.0007782578468322754 + 0.001 * 6.925543308258057
Epoch 1640, val loss: 1.5982494354248047
Epoch 1650, training loss: 0.00775712076574564 = 0.0007715086103416979 + 0.001 * 6.985611915588379
Epoch 1650, val loss: 1.5988852977752686
Epoch 1660, training loss: 0.007685431279242039 = 0.0007650029147043824 + 0.001 * 6.920428276062012
Epoch 1660, val loss: 1.5995343923568726
Epoch 1670, training loss: 0.007708671968430281 = 0.0007586670690216124 + 0.001 * 6.950004577636719
Epoch 1670, val loss: 1.6001564264297485
Epoch 1680, training loss: 0.007736884523183107 = 0.0007524805841967463 + 0.001 * 6.984403610229492
Epoch 1680, val loss: 1.6007124185562134
Epoch 1690, training loss: 0.007650444284081459 = 0.0007465004455298185 + 0.001 * 6.9039435386657715
Epoch 1690, val loss: 1.6013057231903076
Epoch 1700, training loss: 0.007648545317351818 = 0.0007406649529002607 + 0.001 * 6.907879829406738
Epoch 1700, val loss: 1.6018749475479126
Epoch 1710, training loss: 0.007639746647328138 = 0.0007349794032052159 + 0.001 * 6.904767036437988
Epoch 1710, val loss: 1.6023879051208496
Epoch 1720, training loss: 0.007679946720600128 = 0.0007294543902389705 + 0.001 * 6.9504923820495605
Epoch 1720, val loss: 1.602952003479004
Epoch 1730, training loss: 0.007670399267226458 = 0.0007240542327053845 + 0.001 * 6.946344375610352
Epoch 1730, val loss: 1.6034414768218994
Epoch 1740, training loss: 0.007611463312059641 = 0.0007188493036665022 + 0.001 * 6.892613887786865
Epoch 1740, val loss: 1.6039059162139893
Epoch 1750, training loss: 0.007602821569889784 = 0.0007137617212720215 + 0.001 * 6.889059543609619
Epoch 1750, val loss: 1.6043530702590942
Epoch 1760, training loss: 0.007605386432260275 = 0.0007088080164976418 + 0.001 * 6.896577835083008
Epoch 1760, val loss: 1.6048028469085693
Epoch 1770, training loss: 0.007594236638396978 = 0.0007039699121378362 + 0.001 * 6.890266418457031
Epoch 1770, val loss: 1.6052278280258179
Epoch 1780, training loss: 0.007626022212207317 = 0.0006992376293055713 + 0.001 * 6.926784515380859
Epoch 1780, val loss: 1.6056245565414429
Epoch 1790, training loss: 0.007595321629196405 = 0.0006946196081116796 + 0.001 * 6.900701999664307
Epoch 1790, val loss: 1.6060154438018799
Epoch 1800, training loss: 0.007565937936306 = 0.0006901302258484066 + 0.001 * 6.875807762145996
Epoch 1800, val loss: 1.6064001321792603
Epoch 1810, training loss: 0.007586419582366943 = 0.0006857389817014337 + 0.001 * 6.9006805419921875
Epoch 1810, val loss: 1.6067478656768799
Epoch 1820, training loss: 0.007581520825624466 = 0.0006814801599830389 + 0.001 * 6.900040149688721
Epoch 1820, val loss: 1.6071059703826904
Epoch 1830, training loss: 0.007554836105555296 = 0.0006773055647499859 + 0.001 * 6.877530097961426
Epoch 1830, val loss: 1.6074022054672241
Epoch 1840, training loss: 0.007567429915070534 = 0.0006732371402904391 + 0.001 * 6.894192218780518
Epoch 1840, val loss: 1.6077519655227661
Epoch 1850, training loss: 0.0075973523780703545 = 0.000669257075060159 + 0.001 * 6.928094863891602
Epoch 1850, val loss: 1.6080374717712402
Epoch 1860, training loss: 0.007567875552922487 = 0.0006653853342868388 + 0.001 * 6.902490139007568
Epoch 1860, val loss: 1.6082961559295654
Epoch 1870, training loss: 0.007576035335659981 = 0.0006616121390834451 + 0.001 * 6.914422988891602
Epoch 1870, val loss: 1.608596920967102
Epoch 1880, training loss: 0.007546612061560154 = 0.0006579040782526135 + 0.001 * 6.888707637786865
Epoch 1880, val loss: 1.6088216304779053
Epoch 1890, training loss: 0.007533497177064419 = 0.0006543152849189937 + 0.001 * 6.879181861877441
Epoch 1890, val loss: 1.6090672016143799
Epoch 1900, training loss: 0.007537700701504946 = 0.0006507700309157372 + 0.001 * 6.886930465698242
Epoch 1900, val loss: 1.6092963218688965
Epoch 1910, training loss: 0.0075266798958182335 = 0.0006473019602708519 + 0.001 * 6.879377841949463
Epoch 1910, val loss: 1.6095165014266968
Epoch 1920, training loss: 0.0075525203719735146 = 0.0006439350545406342 + 0.001 * 6.908585071563721
Epoch 1920, val loss: 1.6097344160079956
Epoch 1930, training loss: 0.007507938891649246 = 0.0006406228058040142 + 0.001 * 6.867315769195557
Epoch 1930, val loss: 1.6099097728729248
Epoch 1940, training loss: 0.007516865152865648 = 0.0006373938522301614 + 0.001 * 6.8794708251953125
Epoch 1940, val loss: 1.6100709438323975
Epoch 1950, training loss: 0.007559920195490122 = 0.0006342167616821826 + 0.001 * 6.925703048706055
Epoch 1950, val loss: 1.6102148294448853
Epoch 1960, training loss: 0.0075040156953036785 = 0.0006311346660368145 + 0.001 * 6.872880935668945
Epoch 1960, val loss: 1.610381841659546
Epoch 1970, training loss: 0.007487743627279997 = 0.0006281017558649182 + 0.001 * 6.8596415519714355
Epoch 1970, val loss: 1.6105551719665527
Epoch 1980, training loss: 0.007506968453526497 = 0.0006251151207834482 + 0.001 * 6.881853103637695
Epoch 1980, val loss: 1.6106504201889038
Epoch 1990, training loss: 0.007517307996749878 = 0.0006222305237315595 + 0.001 * 6.895077228546143
Epoch 1990, val loss: 1.6107910871505737
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8376383763837639
The final CL Acc:0.79630, 0.01684, The final GNN Acc:0.83641, 0.00108
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10516])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9633978605270386 = 1.954801082611084 + 0.001 * 8.596817970275879
Epoch 0, val loss: 1.9520410299301147
Epoch 10, training loss: 1.952909231185913 = 1.9443124532699585 + 0.001 * 8.596753120422363
Epoch 10, val loss: 1.9417921304702759
Epoch 20, training loss: 1.9398188591003418 = 1.9312223196029663 + 0.001 * 8.59656810760498
Epoch 20, val loss: 1.928441047668457
Epoch 30, training loss: 1.9211406707763672 = 1.91254460811615 + 0.001 * 8.59611988067627
Epoch 30, val loss: 1.9089950323104858
Epoch 40, training loss: 1.8934584856033325 = 1.8848636150360107 + 0.001 * 8.594864845275879
Epoch 40, val loss: 1.880468726158142
Epoch 50, training loss: 1.8559901714324951 = 1.8473997116088867 + 0.001 * 8.590513229370117
Epoch 50, val loss: 1.8438621759414673
Epoch 60, training loss: 1.8190745115280151 = 1.8105024099349976 + 0.001 * 8.572094917297363
Epoch 60, val loss: 1.8126119375228882
Epoch 70, training loss: 1.7919094562530518 = 1.7834184169769287 + 0.001 * 8.49106216430664
Epoch 70, val loss: 1.7923318147659302
Epoch 80, training loss: 1.7579362392425537 = 1.7497882843017578 + 0.001 * 8.147979736328125
Epoch 80, val loss: 1.7637441158294678
Epoch 90, training loss: 1.711768627166748 = 1.7037242650985718 + 0.001 * 8.044328689575195
Epoch 90, val loss: 1.7236968278884888
Epoch 100, training loss: 1.646734595298767 = 1.638784646987915 + 0.001 * 7.94991397857666
Epoch 100, val loss: 1.6677312850952148
Epoch 110, training loss: 1.564393401145935 = 1.5565292835235596 + 0.001 * 7.864139556884766
Epoch 110, val loss: 1.5987156629562378
Epoch 120, training loss: 1.4744638204574585 = 1.4667260646820068 + 0.001 * 7.737810134887695
Epoch 120, val loss: 1.527159571647644
Epoch 130, training loss: 1.3863459825515747 = 1.3787320852279663 + 0.001 * 7.613901138305664
Epoch 130, val loss: 1.4601913690567017
Epoch 140, training loss: 1.3026400804519653 = 1.295043706893921 + 0.001 * 7.596332550048828
Epoch 140, val loss: 1.4011589288711548
Epoch 150, training loss: 1.222298264503479 = 1.214746117591858 + 0.001 * 7.55211877822876
Epoch 150, val loss: 1.3472158908843994
Epoch 160, training loss: 1.1429500579833984 = 1.1354329586029053 + 0.001 * 7.517076015472412
Epoch 160, val loss: 1.2943679094314575
Epoch 170, training loss: 1.0626026391983032 = 1.0551263093948364 + 0.001 * 7.476380825042725
Epoch 170, val loss: 1.239951252937317
Epoch 180, training loss: 0.9812617301940918 = 0.9738314747810364 + 0.001 * 7.430235385894775
Epoch 180, val loss: 1.1830898523330688
Epoch 190, training loss: 0.9014573097229004 = 0.8940671682357788 + 0.001 * 7.390120029449463
Epoch 190, val loss: 1.1257764101028442
Epoch 200, training loss: 0.8266592621803284 = 0.8192985653877258 + 0.001 * 7.360722064971924
Epoch 200, val loss: 1.0718530416488647
Epoch 210, training loss: 0.7597646713256836 = 0.7524271607398987 + 0.001 * 7.33752965927124
Epoch 210, val loss: 1.024598240852356
Epoch 220, training loss: 0.7016993761062622 = 0.6943720579147339 + 0.001 * 7.327315807342529
Epoch 220, val loss: 0.9860417246818542
Epoch 230, training loss: 0.6509807705879211 = 0.6436604261398315 + 0.001 * 7.320352554321289
Epoch 230, val loss: 0.9553937911987305
Epoch 240, training loss: 0.6046768426895142 = 0.5973662734031677 + 0.001 * 7.3105878829956055
Epoch 240, val loss: 0.9301526546478271
Epoch 250, training loss: 0.5597240924835205 = 0.5524259209632874 + 0.001 * 7.298186302185059
Epoch 250, val loss: 0.9077616930007935
Epoch 260, training loss: 0.5140355229377747 = 0.5067517757415771 + 0.001 * 7.283743858337402
Epoch 260, val loss: 0.8867286443710327
Epoch 270, training loss: 0.46702665090560913 = 0.4597623348236084 + 0.001 * 7.264330863952637
Epoch 270, val loss: 0.8671588897705078
Epoch 280, training loss: 0.41922348737716675 = 0.41198334097862244 + 0.001 * 7.240133285522461
Epoch 280, val loss: 0.8496570587158203
Epoch 290, training loss: 0.3717138171195984 = 0.364501029253006 + 0.001 * 7.2127766609191895
Epoch 290, val loss: 0.8346298933029175
Epoch 300, training loss: 0.3257969617843628 = 0.31860974431037903 + 0.001 * 7.1872172355651855
Epoch 300, val loss: 0.8227350115776062
Epoch 310, training loss: 0.28285834193229675 = 0.2757044732570648 + 0.001 * 7.153871059417725
Epoch 310, val loss: 0.8140371441841125
Epoch 320, training loss: 0.24410423636436462 = 0.2369702011346817 + 0.001 * 7.134034633636475
Epoch 320, val loss: 0.8086035847663879
Epoch 330, training loss: 0.21008563041687012 = 0.20296180248260498 + 0.001 * 7.123822212219238
Epoch 330, val loss: 0.8062857389450073
Epoch 340, training loss: 0.1807323396205902 = 0.1736256629228592 + 0.001 * 7.106681823730469
Epoch 340, val loss: 0.8068767786026001
Epoch 350, training loss: 0.15567155182361603 = 0.14856839179992676 + 0.001 * 7.103152751922607
Epoch 350, val loss: 0.8098529577255249
Epoch 360, training loss: 0.1343875229358673 = 0.12729275226593018 + 0.001 * 7.094763278961182
Epoch 360, val loss: 0.8148250579833984
Epoch 370, training loss: 0.11637468636035919 = 0.10928728431463242 + 0.001 * 7.087404251098633
Epoch 370, val loss: 0.8214129209518433
Epoch 380, training loss: 0.10116083174943924 = 0.09407752752304077 + 0.001 * 7.083302021026611
Epoch 380, val loss: 0.8293386697769165
Epoch 390, training loss: 0.08832579106092453 = 0.0812440812587738 + 0.001 * 7.081712245941162
Epoch 390, val loss: 0.8383182883262634
Epoch 400, training loss: 0.07750643789768219 = 0.0704306811094284 + 0.001 * 7.075753211975098
Epoch 400, val loss: 0.8481404781341553
Epoch 410, training loss: 0.06840112805366516 = 0.06132735684514046 + 0.001 * 7.0737690925598145
Epoch 410, val loss: 0.8586737513542175
Epoch 420, training loss: 0.06073036789894104 = 0.05366024374961853 + 0.001 * 7.070122718811035
Epoch 420, val loss: 0.8697239756584167
Epoch 430, training loss: 0.05426658317446709 = 0.047197505831718445 + 0.001 * 7.069075584411621
Epoch 430, val loss: 0.8811502456665039
Epoch 440, training loss: 0.04880927503108978 = 0.041737113147974014 + 0.001 * 7.072160243988037
Epoch 440, val loss: 0.892775297164917
Epoch 450, training loss: 0.04418283700942993 = 0.03710990026593208 + 0.001 * 7.072936058044434
Epoch 450, val loss: 0.9044498205184937
Epoch 460, training loss: 0.04024340212345123 = 0.033173900097608566 + 0.001 * 7.069500923156738
Epoch 460, val loss: 0.9160999655723572
Epoch 470, training loss: 0.03687600418925285 = 0.029809962958097458 + 0.001 * 7.066040992736816
Epoch 470, val loss: 0.9276667237281799
Epoch 480, training loss: 0.03399144113063812 = 0.02692079357802868 + 0.001 * 7.070649147033691
Epoch 480, val loss: 0.939030647277832
Epoch 490, training loss: 0.0314941480755806 = 0.02442629635334015 + 0.001 * 7.067850112915039
Epoch 490, val loss: 0.9501948356628418
Epoch 500, training loss: 0.029325800016522408 = 0.022261569276452065 + 0.001 * 7.064229965209961
Epoch 500, val loss: 0.9611138105392456
Epoch 510, training loss: 0.02743743360042572 = 0.020373933017253876 + 0.001 * 7.063499450683594
Epoch 510, val loss: 0.9717737436294556
Epoch 520, training loss: 0.025781352072954178 = 0.01871969923377037 + 0.001 * 7.061653137207031
Epoch 520, val loss: 0.9821348786354065
Epoch 530, training loss: 0.024323252961039543 = 0.017263077199459076 + 0.001 * 7.060175895690918
Epoch 530, val loss: 0.992228090763092
Epoch 540, training loss: 0.02303503267467022 = 0.01597454957664013 + 0.001 * 7.060482501983643
Epoch 540, val loss: 1.0020339488983154
Epoch 550, training loss: 0.02189258299767971 = 0.014829639345407486 + 0.001 * 7.062943935394287
Epoch 550, val loss: 1.0115984678268433
Epoch 560, training loss: 0.02086794003844261 = 0.013807900249958038 + 0.001 * 7.060040473937988
Epoch 560, val loss: 1.020874261856079
Epoch 570, training loss: 0.019951412454247475 = 0.012892410159111023 + 0.001 * 7.059001922607422
Epoch 570, val loss: 1.0298869609832764
Epoch 580, training loss: 0.019126202911138535 = 0.01206901203840971 + 0.001 * 7.05718994140625
Epoch 580, val loss: 1.0386114120483398
Epoch 590, training loss: 0.018383478745818138 = 0.011325701139867306 + 0.001 * 7.0577778816223145
Epoch 590, val loss: 1.0470850467681885
Epoch 600, training loss: 0.01770637370646 = 0.010652448050677776 + 0.001 * 7.053925037384033
Epoch 600, val loss: 1.0553150177001953
Epoch 610, training loss: 0.017092524096369743 = 0.01004074141383171 + 0.001 * 7.051782131195068
Epoch 610, val loss: 1.0633046627044678
Epoch 620, training loss: 0.016538025811314583 = 0.00948325265198946 + 0.001 * 7.05477237701416
Epoch 620, val loss: 1.071073293685913
Epoch 630, training loss: 0.01602407544851303 = 0.008973579853773117 + 0.001 * 7.0504961013793945
Epoch 630, val loss: 1.0786254405975342
Epoch 640, training loss: 0.015556384809315205 = 0.008506041951477528 + 0.001 * 7.050342559814453
Epoch 640, val loss: 1.085965871810913
Epoch 650, training loss: 0.015126983635127544 = 0.008075629360973835 + 0.001 * 7.051353931427002
Epoch 650, val loss: 1.0931341648101807
Epoch 660, training loss: 0.014724559150636196 = 0.007677902467548847 + 0.001 * 7.046656131744385
Epoch 660, val loss: 1.1001302003860474
Epoch 670, training loss: 0.01435435563325882 = 0.007308855652809143 + 0.001 * 7.0455002784729
Epoch 670, val loss: 1.1069777011871338
Epoch 680, training loss: 0.014019649475812912 = 0.0069652399979531765 + 0.001 * 7.054409027099609
Epoch 680, val loss: 1.1136797666549683
Epoch 690, training loss: 0.013692034408450127 = 0.006644491571933031 + 0.001 * 7.047542572021484
Epoch 690, val loss: 1.1202378273010254
Epoch 700, training loss: 0.013387739658355713 = 0.006344655994325876 + 0.001 * 7.043083190917969
Epoch 700, val loss: 1.1266545057296753
Epoch 710, training loss: 0.013115816749632359 = 0.006064049433916807 + 0.001 * 7.051766872406006
Epoch 710, val loss: 1.132936954498291
Epoch 720, training loss: 0.012842070311307907 = 0.005801632534712553 + 0.001 * 7.0404372215271
Epoch 720, val loss: 1.1390796899795532
Epoch 730, training loss: 0.012597369030117989 = 0.005555931478738785 + 0.001 * 7.04143762588501
Epoch 730, val loss: 1.1450637578964233
Epoch 740, training loss: 0.01236177608370781 = 0.005325600504875183 + 0.001 * 7.036175727844238
Epoch 740, val loss: 1.1509076356887817
Epoch 750, training loss: 0.012159030884504318 = 0.00510964123532176 + 0.001 * 7.049389839172363
Epoch 750, val loss: 1.1566082239151
Epoch 760, training loss: 0.011946985498070717 = 0.004907061345875263 + 0.001 * 7.039924144744873
Epoch 760, val loss: 1.1621813774108887
Epoch 770, training loss: 0.011750435456633568 = 0.004716785158962011 + 0.001 * 7.033649921417236
Epoch 770, val loss: 1.1676157712936401
Epoch 780, training loss: 0.011569196358323097 = 0.004537911154329777 + 0.001 * 7.031284332275391
Epoch 780, val loss: 1.172911524772644
Epoch 790, training loss: 0.011405972763895988 = 0.004369641188532114 + 0.001 * 7.0363311767578125
Epoch 790, val loss: 1.178084135055542
Epoch 800, training loss: 0.011238782666623592 = 0.004211278632283211 + 0.001 * 7.027503490447998
Epoch 800, val loss: 1.1831477880477905
Epoch 810, training loss: 0.01108734030276537 = 0.0040620616637170315 + 0.001 * 7.025278091430664
Epoch 810, val loss: 1.1880948543548584
Epoch 820, training loss: 0.010950244963169098 = 0.003921323921531439 + 0.001 * 7.028921127319336
Epoch 820, val loss: 1.1928985118865967
Epoch 830, training loss: 0.010832227766513824 = 0.003788471221923828 + 0.001 * 7.043756008148193
Epoch 830, val loss: 1.1976089477539062
Epoch 840, training loss: 0.01068984903395176 = 0.0036630257964134216 + 0.001 * 7.0268235206604
Epoch 840, val loss: 1.2022018432617188
Epoch 850, training loss: 0.010567395947873592 = 0.0035443741362541914 + 0.001 * 7.023021697998047
Epoch 850, val loss: 1.2066929340362549
Epoch 860, training loss: 0.0104564493522048 = 0.0034320810809731483 + 0.001 * 7.024367809295654
Epoch 860, val loss: 1.2111024856567383
Epoch 870, training loss: 0.010349364951252937 = 0.003325683530420065 + 0.001 * 7.023681640625
Epoch 870, val loss: 1.2153891324996948
Epoch 880, training loss: 0.010253856889903545 = 0.0032248140778392553 + 0.001 * 7.029042720794678
Epoch 880, val loss: 1.2195957899093628
Epoch 890, training loss: 0.010156775824725628 = 0.0031290811020880938 + 0.001 * 7.027694225311279
Epoch 890, val loss: 1.2236907482147217
Epoch 900, training loss: 0.010062477551400661 = 0.003038175404071808 + 0.001 * 7.024302005767822
Epoch 900, val loss: 1.2277231216430664
Epoch 910, training loss: 0.009982448071241379 = 0.002951760543510318 + 0.001 * 7.03068733215332
Epoch 910, val loss: 1.231661319732666
Epoch 920, training loss: 0.00988410972058773 = 0.002869598101824522 + 0.001 * 7.0145111083984375
Epoch 920, val loss: 1.235519528388977
Epoch 930, training loss: 0.009800145402550697 = 0.0027913593221455812 + 0.001 * 7.008786201477051
Epoch 930, val loss: 1.239280104637146
Epoch 940, training loss: 0.00972308311611414 = 0.002716815797612071 + 0.001 * 7.0062665939331055
Epoch 940, val loss: 1.2429852485656738
Epoch 950, training loss: 0.009663715958595276 = 0.00264574121683836 + 0.001 * 7.017974853515625
Epoch 950, val loss: 1.246605634689331
Epoch 960, training loss: 0.009584507904946804 = 0.002577932085841894 + 0.001 * 7.006575584411621
Epoch 960, val loss: 1.2501602172851562
Epoch 970, training loss: 0.009518759325146675 = 0.0025132102891802788 + 0.001 * 7.005548477172852
Epoch 970, val loss: 1.253627896308899
Epoch 980, training loss: 0.009463170543313026 = 0.0024513660464435816 + 0.001 * 7.011804580688477
Epoch 980, val loss: 1.2570258378982544
Epoch 990, training loss: 0.009395526722073555 = 0.0023922468535602093 + 0.001 * 7.003279209136963
Epoch 990, val loss: 1.2603687047958374
Epoch 1000, training loss: 0.009339452721178532 = 0.002335677621886134 + 0.001 * 7.003774642944336
Epoch 1000, val loss: 1.2636319398880005
Epoch 1010, training loss: 0.009288829751312733 = 0.0022815472912043333 + 0.001 * 7.007282257080078
Epoch 1010, val loss: 1.2668333053588867
Epoch 1020, training loss: 0.009246976114809513 = 0.002229694975540042 + 0.001 * 7.0172810554504395
Epoch 1020, val loss: 1.2699803113937378
Epoch 1030, training loss: 0.009181654080748558 = 0.0021799986716359854 + 0.001 * 7.001654624938965
Epoch 1030, val loss: 1.2730419635772705
Epoch 1040, training loss: 0.009131978265941143 = 0.0021323594264686108 + 0.001 * 6.9996185302734375
Epoch 1040, val loss: 1.276062250137329
Epoch 1050, training loss: 0.009090827777981758 = 0.0020866342820227146 + 0.001 * 7.0041937828063965
Epoch 1050, val loss: 1.2790067195892334
Epoch 1060, training loss: 0.009040728211402893 = 0.002042714273557067 + 0.001 * 6.998013496398926
Epoch 1060, val loss: 1.2819159030914307
Epoch 1070, training loss: 0.009005620144307613 = 0.002000523963943124 + 0.001 * 7.005095481872559
Epoch 1070, val loss: 1.2847421169281006
Epoch 1080, training loss: 0.008966085501015186 = 0.0019599944353103638 + 0.001 * 7.0060906410217285
Epoch 1080, val loss: 1.2875373363494873
Epoch 1090, training loss: 0.00891043245792389 = 0.001920995069667697 + 0.001 * 6.989436626434326
Epoch 1090, val loss: 1.2902742624282837
Epoch 1100, training loss: 0.008893275633454323 = 0.0018834852380678058 + 0.001 * 7.009789943695068
Epoch 1100, val loss: 1.2929788827896118
Epoch 1110, training loss: 0.008843018673360348 = 0.001847412553615868 + 0.001 * 6.995605945587158
Epoch 1110, val loss: 1.2955961227416992
Epoch 1120, training loss: 0.008796506561338902 = 0.0018126564100384712 + 0.001 * 6.983850002288818
Epoch 1120, val loss: 1.2982063293457031
Epoch 1130, training loss: 0.008759777061641216 = 0.0017791623249650002 + 0.001 * 6.980614185333252
Epoch 1130, val loss: 1.3007405996322632
Epoch 1140, training loss: 0.008737378753721714 = 0.0017468832666054368 + 0.001 * 6.990495204925537
Epoch 1140, val loss: 1.3032501935958862
Epoch 1150, training loss: 0.008700445294380188 = 0.0017157640540972352 + 0.001 * 6.984680652618408
Epoch 1150, val loss: 1.3056985139846802
Epoch 1160, training loss: 0.00866780150681734 = 0.0016857264563441277 + 0.001 * 6.982074737548828
Epoch 1160, val loss: 1.3081260919570923
Epoch 1170, training loss: 0.00864204578101635 = 0.001656767912209034 + 0.001 * 6.9852776527404785
Epoch 1170, val loss: 1.3104991912841797
Epoch 1180, training loss: 0.008605108596384525 = 0.0016288086771965027 + 0.001 * 6.97629976272583
Epoch 1180, val loss: 1.3128257989883423
Epoch 1190, training loss: 0.008586165495216846 = 0.0016018039314076304 + 0.001 * 6.98436164855957
Epoch 1190, val loss: 1.3151236772537231
Epoch 1200, training loss: 0.008552813902497292 = 0.0015757372602820396 + 0.001 * 6.977076053619385
Epoch 1200, val loss: 1.3173772096633911
Epoch 1210, training loss: 0.008535659871995449 = 0.001550523447804153 + 0.001 * 6.98513650894165
Epoch 1210, val loss: 1.319583773612976
Epoch 1220, training loss: 0.00850063469260931 = 0.001526155392639339 + 0.001 * 6.974478721618652
Epoch 1220, val loss: 1.3217809200286865
Epoch 1230, training loss: 0.008483135141432285 = 0.001502578379586339 + 0.001 * 6.980556488037109
Epoch 1230, val loss: 1.3239061832427979
Epoch 1240, training loss: 0.008446378633379936 = 0.0014797690091654658 + 0.001 * 6.966609001159668
Epoch 1240, val loss: 1.3260207176208496
Epoch 1250, training loss: 0.008427833206951618 = 0.0014576844405382872 + 0.001 * 6.97014856338501
Epoch 1250, val loss: 1.3280874490737915
Epoch 1260, training loss: 0.008414259180426598 = 0.001436329446732998 + 0.001 * 6.97792911529541
Epoch 1260, val loss: 1.330137014389038
Epoch 1270, training loss: 0.008393869735300541 = 0.001415621372871101 + 0.001 * 6.978248596191406
Epoch 1270, val loss: 1.3321212530136108
Epoch 1280, training loss: 0.008365368470549583 = 0.0013955788454040885 + 0.001 * 6.969789028167725
Epoch 1280, val loss: 1.3340978622436523
Epoch 1290, training loss: 0.008334715850651264 = 0.0013761409791186452 + 0.001 * 6.9585747718811035
Epoch 1290, val loss: 1.3360530138015747
Epoch 1300, training loss: 0.008319933898746967 = 0.0013572967145591974 + 0.001 * 6.962636470794678
Epoch 1300, val loss: 1.3379616737365723
Epoch 1310, training loss: 0.008298124186694622 = 0.0013390311505645514 + 0.001 * 6.959092617034912
Epoch 1310, val loss: 1.3398325443267822
Epoch 1320, training loss: 0.008279236033558846 = 0.0013213404454290867 + 0.001 * 6.957895755767822
Epoch 1320, val loss: 1.3416779041290283
Epoch 1330, training loss: 0.0082649365067482 = 0.0013041611528024077 + 0.001 * 6.960774898529053
Epoch 1330, val loss: 1.3434977531433105
Epoch 1340, training loss: 0.008246850222349167 = 0.00128749362193048 + 0.001 * 6.959356307983398
Epoch 1340, val loss: 1.3452801704406738
Epoch 1350, training loss: 0.00823383778333664 = 0.0012713123578578234 + 0.001 * 6.962525367736816
Epoch 1350, val loss: 1.3470380306243896
Epoch 1360, training loss: 0.008240539580583572 = 0.0012556001311168075 + 0.001 * 6.9849395751953125
Epoch 1360, val loss: 1.348784327507019
Epoch 1370, training loss: 0.008194453082978725 = 0.001240324811078608 + 0.001 * 6.954128265380859
Epoch 1370, val loss: 1.3505172729492188
Epoch 1380, training loss: 0.00819212943315506 = 0.0012254792964085937 + 0.001 * 6.966649532318115
Epoch 1380, val loss: 1.3521780967712402
Epoch 1390, training loss: 0.008192704059183598 = 0.0012110749958083034 + 0.001 * 6.98162841796875
Epoch 1390, val loss: 1.3538697957992554
Epoch 1400, training loss: 0.008153034374117851 = 0.001197052071802318 + 0.001 * 6.955981731414795
Epoch 1400, val loss: 1.3554863929748535
Epoch 1410, training loss: 0.008126803673803806 = 0.001183440675958991 + 0.001 * 6.943362236022949
Epoch 1410, val loss: 1.3571207523345947
Epoch 1420, training loss: 0.008113889954984188 = 0.0011702001793310046 + 0.001 * 6.943689346313477
Epoch 1420, val loss: 1.3586915731430054
Epoch 1430, training loss: 0.008098535239696503 = 0.0011573117226362228 + 0.001 * 6.94122314453125
Epoch 1430, val loss: 1.3602850437164307
Epoch 1440, training loss: 0.008084501139819622 = 0.0011447774013504386 + 0.001 * 6.939723491668701
Epoch 1440, val loss: 1.361808180809021
Epoch 1450, training loss: 0.008089140057563782 = 0.0011325805680826306 + 0.001 * 6.956559181213379
Epoch 1450, val loss: 1.363329291343689
Epoch 1460, training loss: 0.008073863573372364 = 0.0011207169154658914 + 0.001 * 6.953146457672119
Epoch 1460, val loss: 1.3648273944854736
Epoch 1470, training loss: 0.00805625505745411 = 0.0011091511696577072 + 0.001 * 6.947103977203369
Epoch 1470, val loss: 1.366286277770996
Epoch 1480, training loss: 0.008036920800805092 = 0.0010978768114000559 + 0.001 * 6.9390435218811035
Epoch 1480, val loss: 1.3677356243133545
Epoch 1490, training loss: 0.008030300959944725 = 0.0010868918616324663 + 0.001 * 6.943408489227295
Epoch 1490, val loss: 1.369187831878662
Epoch 1500, training loss: 0.008012054488062859 = 0.001076187239959836 + 0.001 * 6.935866832733154
Epoch 1500, val loss: 1.3705925941467285
Epoch 1510, training loss: 0.008010859601199627 = 0.0010657762177288532 + 0.001 * 6.945083141326904
Epoch 1510, val loss: 1.3719795942306519
Epoch 1520, training loss: 0.007998313754796982 = 0.0010556207271292806 + 0.001 * 6.942692756652832
Epoch 1520, val loss: 1.3733443021774292
Epoch 1530, training loss: 0.007987341843545437 = 0.0010457067983224988 + 0.001 * 6.941634654998779
Epoch 1530, val loss: 1.3747057914733887
Epoch 1540, training loss: 0.007968277670443058 = 0.0010360355954617262 + 0.001 * 6.932241916656494
Epoch 1540, val loss: 1.3760355710983276
Epoch 1550, training loss: 0.00797771941870451 = 0.0010266005992889404 + 0.001 * 6.951118469238281
Epoch 1550, val loss: 1.377383828163147
Epoch 1560, training loss: 0.007958786562085152 = 0.0010174139169976115 + 0.001 * 6.941372871398926
Epoch 1560, val loss: 1.3786933422088623
Epoch 1570, training loss: 0.007974796928465366 = 0.0010084511013701558 + 0.001 * 6.966345310211182
Epoch 1570, val loss: 1.3799934387207031
Epoch 1580, training loss: 0.007966803386807442 = 0.0009997003944590688 + 0.001 * 6.967102527618408
Epoch 1580, val loss: 1.3812202215194702
Epoch 1590, training loss: 0.007937676273286343 = 0.0009911921806633472 + 0.001 * 6.946483612060547
Epoch 1590, val loss: 1.3825087547302246
Epoch 1600, training loss: 0.007914261892437935 = 0.0009828663896769285 + 0.001 * 6.931395053863525
Epoch 1600, val loss: 1.3837233781814575
Epoch 1610, training loss: 0.007924873381853104 = 0.0009747492149472237 + 0.001 * 6.950124263763428
Epoch 1610, val loss: 1.3849608898162842
Epoch 1620, training loss: 0.007880238816142082 = 0.0009668159764260054 + 0.001 * 6.913422107696533
Epoch 1620, val loss: 1.3861896991729736
Epoch 1630, training loss: 0.007887009531259537 = 0.0009590661502443254 + 0.001 * 6.927943229675293
Epoch 1630, val loss: 1.387353539466858
Epoch 1640, training loss: 0.007871679961681366 = 0.0009514786652289331 + 0.001 * 6.920200824737549
Epoch 1640, val loss: 1.388572335243225
Epoch 1650, training loss: 0.007861213758587837 = 0.0009440690628252923 + 0.001 * 6.917144775390625
Epoch 1650, val loss: 1.3897215127944946
Epoch 1660, training loss: 0.007850833237171173 = 0.0009368417668156326 + 0.001 * 6.9139909744262695
Epoch 1660, val loss: 1.3909128904342651
Epoch 1670, training loss: 0.007846556603908539 = 0.0009297722135670483 + 0.001 * 6.916783809661865
Epoch 1670, val loss: 1.3920108079910278
Epoch 1680, training loss: 0.0078422911465168 = 0.0009228409035131335 + 0.001 * 6.919449806213379
Epoch 1680, val loss: 1.3931556940078735
Epoch 1690, training loss: 0.007834024727344513 = 0.0009160706540569663 + 0.001 * 6.917953968048096
Epoch 1690, val loss: 1.3942606449127197
Epoch 1700, training loss: 0.007859502919018269 = 0.0009094590786844492 + 0.001 * 6.950043201446533
Epoch 1700, val loss: 1.3953955173492432
Epoch 1710, training loss: 0.007819288410246372 = 0.0009029967477545142 + 0.001 * 6.9162917137146
Epoch 1710, val loss: 1.3964802026748657
Epoch 1720, training loss: 0.007840324193239212 = 0.0008966705645434558 + 0.001 * 6.943653583526611
Epoch 1720, val loss: 1.3975481986999512
Epoch 1730, training loss: 0.007814817130565643 = 0.0008904764545150101 + 0.001 * 6.924340724945068
Epoch 1730, val loss: 1.3986350297927856
Epoch 1740, training loss: 0.007833974435925484 = 0.0008844167459756136 + 0.001 * 6.949557781219482
Epoch 1740, val loss: 1.3996702432632446
Epoch 1750, training loss: 0.007812414783984423 = 0.0008784844540059566 + 0.001 * 6.933929920196533
Epoch 1750, val loss: 1.400752067565918
Epoch 1760, training loss: 0.007801122963428497 = 0.0008726714877411723 + 0.001 * 6.928451061248779
Epoch 1760, val loss: 1.401772141456604
Epoch 1770, training loss: 0.0077707828022539616 = 0.0008669790695421398 + 0.001 * 6.90380334854126
Epoch 1770, val loss: 1.4027997255325317
Epoch 1780, training loss: 0.007788787595927715 = 0.0008613996324129403 + 0.001 * 6.927387237548828
Epoch 1780, val loss: 1.4038244485855103
Epoch 1790, training loss: 0.0077902209013700485 = 0.0008559357956983149 + 0.001 * 6.934284687042236
Epoch 1790, val loss: 1.404822587966919
Epoch 1800, training loss: 0.007791726849973202 = 0.0008506026351824403 + 0.001 * 6.941123962402344
Epoch 1800, val loss: 1.4058316946029663
Epoch 1810, training loss: 0.007743248250335455 = 0.0008453619084320962 + 0.001 * 6.897885799407959
Epoch 1810, val loss: 1.4067986011505127
Epoch 1820, training loss: 0.007743153255432844 = 0.0008402279927395284 + 0.001 * 6.90292501449585
Epoch 1820, val loss: 1.4077638387680054
Epoch 1830, training loss: 0.0077282339334487915 = 0.0008351979777216911 + 0.001 * 6.893035411834717
Epoch 1830, val loss: 1.408760905265808
Epoch 1840, training loss: 0.0077387490309774876 = 0.0008302638307213783 + 0.001 * 6.908484935760498
Epoch 1840, val loss: 1.409663438796997
Epoch 1850, training loss: 0.007758913096040487 = 0.0008254229906015098 + 0.001 * 6.933489799499512
Epoch 1850, val loss: 1.4106323719024658
Epoch 1860, training loss: 0.007717527914792299 = 0.0008206812781281769 + 0.001 * 6.896846294403076
Epoch 1860, val loss: 1.4115737676620483
Epoch 1870, training loss: 0.007707525044679642 = 0.0008160326979123056 + 0.001 * 6.891491889953613
Epoch 1870, val loss: 1.4124870300292969
Epoch 1880, training loss: 0.007706569507718086 = 0.0008114647353067994 + 0.001 * 6.89510440826416
Epoch 1880, val loss: 1.413385033607483
Epoch 1890, training loss: 0.007733698468655348 = 0.0008069815230555832 + 0.001 * 6.926716327667236
Epoch 1890, val loss: 1.4142955541610718
Epoch 1900, training loss: 0.007714597042649984 = 0.0008025665883906186 + 0.001 * 6.91202974319458
Epoch 1900, val loss: 1.4151997566223145
Epoch 1910, training loss: 0.007718643173575401 = 0.000798242399469018 + 0.001 * 6.920400619506836
Epoch 1910, val loss: 1.4160425662994385
Epoch 1920, training loss: 0.007691215258091688 = 0.0007940001087263227 + 0.001 * 6.897214889526367
Epoch 1920, val loss: 1.4169491529464722
Epoch 1930, training loss: 0.0076695834286510944 = 0.0007898332551121712 + 0.001 * 6.879749774932861
Epoch 1930, val loss: 1.4177947044372559
Epoch 1940, training loss: 0.007677909452468157 = 0.0007857443415559828 + 0.001 * 6.892164707183838
Epoch 1940, val loss: 1.4186716079711914
Epoch 1950, training loss: 0.007657257840037346 = 0.0007817292353138328 + 0.001 * 6.875528335571289
Epoch 1950, val loss: 1.419521450996399
Epoch 1960, training loss: 0.0076755499467253685 = 0.0007777877617627382 + 0.001 * 6.897762298583984
Epoch 1960, val loss: 1.4203526973724365
Epoch 1970, training loss: 0.007661201525479555 = 0.0007739243446849287 + 0.001 * 6.887276649475098
Epoch 1970, val loss: 1.4212101697921753
Epoch 1980, training loss: 0.0076661971397697926 = 0.000770123559050262 + 0.001 * 6.896073341369629
Epoch 1980, val loss: 1.4220324754714966
Epoch 1990, training loss: 0.007638083770871162 = 0.0007663877331651747 + 0.001 * 6.8716959953308105
Epoch 1990, val loss: 1.422864317893982
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 1.9531030654907227 = 1.944506287574768 + 0.001 * 8.596795082092285
Epoch 0, val loss: 1.9436355829238892
Epoch 10, training loss: 1.9434099197387695 = 1.934813141822815 + 0.001 * 8.59672737121582
Epoch 10, val loss: 1.9343607425689697
Epoch 20, training loss: 1.9313275814056396 = 1.9227310419082642 + 0.001 * 8.59649658203125
Epoch 20, val loss: 1.9221464395523071
Epoch 30, training loss: 1.9140070676803589 = 1.9054111242294312 + 0.001 * 8.595945358276367
Epoch 30, val loss: 1.9041892290115356
Epoch 40, training loss: 1.888161063194275 = 1.8795666694641113 + 0.001 * 8.594436645507812
Epoch 40, val loss: 1.877511978149414
Epoch 50, training loss: 1.852534532546997 = 1.8439452648162842 + 0.001 * 8.589224815368652
Epoch 50, val loss: 1.8426865339279175
Epoch 60, training loss: 1.815791130065918 = 1.8072254657745361 + 0.001 * 8.565696716308594
Epoch 60, val loss: 1.8112695217132568
Epoch 70, training loss: 1.7873402833938599 = 1.7789008617401123 + 0.001 * 8.439471244812012
Epoch 70, val loss: 1.7892488241195679
Epoch 80, training loss: 1.7505704164505005 = 1.7424485683441162 + 0.001 * 8.12182331085205
Epoch 80, val loss: 1.757391333580017
Epoch 90, training loss: 1.6992450952529907 = 1.691285490989685 + 0.001 * 7.959585666656494
Epoch 90, val loss: 1.7133904695510864
Epoch 100, training loss: 1.6273860931396484 = 1.6196093559265137 + 0.001 * 7.776788234710693
Epoch 100, val loss: 1.654257893562317
Epoch 110, training loss: 1.5385427474975586 = 1.5309449434280396 + 0.001 * 7.5977959632873535
Epoch 110, val loss: 1.5832669734954834
Epoch 120, training loss: 1.4448798894882202 = 1.437324047088623 + 0.001 * 7.555803298950195
Epoch 120, val loss: 1.5097798109054565
Epoch 130, training loss: 1.3531687259674072 = 1.3456472158432007 + 0.001 * 7.5214972496032715
Epoch 130, val loss: 1.4406472444534302
Epoch 140, training loss: 1.2621122598648071 = 1.254652976989746 + 0.001 * 7.459228038787842
Epoch 140, val loss: 1.373771071434021
Epoch 150, training loss: 1.1708612442016602 = 1.1635066270828247 + 0.001 * 7.354574203491211
Epoch 150, val loss: 1.3082785606384277
Epoch 160, training loss: 1.0816640853881836 = 1.0744062662124634 + 0.001 * 7.25786018371582
Epoch 160, val loss: 1.2458158731460571
Epoch 170, training loss: 0.9971708655357361 = 0.9899759292602539 + 0.001 * 7.194963455200195
Epoch 170, val loss: 1.1881461143493652
Epoch 180, training loss: 0.9183799028396606 = 0.9112194180488586 + 0.001 * 7.160468578338623
Epoch 180, val loss: 1.135322093963623
Epoch 190, training loss: 0.8456521034240723 = 0.838506817817688 + 0.001 * 7.145275592803955
Epoch 190, val loss: 1.0873968601226807
Epoch 200, training loss: 0.7795177698135376 = 0.7723821997642517 + 0.001 * 7.135563373565674
Epoch 200, val loss: 1.0451960563659668
Epoch 210, training loss: 0.7204888463020325 = 0.7133584022521973 + 0.001 * 7.130444526672363
Epoch 210, val loss: 1.0097122192382812
Epoch 220, training loss: 0.6679903268814087 = 0.6608629822731018 + 0.001 * 7.1273393630981445
Epoch 220, val loss: 0.9806948900222778
Epoch 230, training loss: 0.6202349662780762 = 0.6131101846694946 + 0.001 * 7.124790668487549
Epoch 230, val loss: 0.9567887783050537
Epoch 240, training loss: 0.5750886797904968 = 0.5679672360420227 + 0.001 * 7.121425151824951
Epoch 240, val loss: 0.9363183975219727
Epoch 250, training loss: 0.5310237407684326 = 0.5239049792289734 + 0.001 * 7.118741035461426
Epoch 250, val loss: 0.9181593656539917
Epoch 260, training loss: 0.48752740025520325 = 0.48041099309921265 + 0.001 * 7.116406440734863
Epoch 260, val loss: 0.902424156665802
Epoch 270, training loss: 0.4450942873954773 = 0.43798136711120605 + 0.001 * 7.11293363571167
Epoch 270, val loss: 0.8896363973617554
Epoch 280, training loss: 0.4044645428657532 = 0.397354394197464 + 0.001 * 7.110143661499023
Epoch 280, val loss: 0.8805184960365295
Epoch 290, training loss: 0.3662380278110504 = 0.3591324985027313 + 0.001 * 7.105535507202148
Epoch 290, val loss: 0.8753798604011536
Epoch 300, training loss: 0.3304775357246399 = 0.32337772846221924 + 0.001 * 7.099795341491699
Epoch 300, val loss: 0.8738517761230469
Epoch 310, training loss: 0.2968386113643646 = 0.28974342346191406 + 0.001 * 7.095191478729248
Epoch 310, val loss: 0.8752615451812744
Epoch 320, training loss: 0.2648969888687134 = 0.25780805945396423 + 0.001 * 7.088929176330566
Epoch 320, val loss: 0.8786047101020813
Epoch 330, training loss: 0.23462115228176117 = 0.22753480076789856 + 0.001 * 7.086355686187744
Epoch 330, val loss: 0.8836183547973633
Epoch 340, training loss: 0.20638693869113922 = 0.19930541515350342 + 0.001 * 7.0815205574035645
Epoch 340, val loss: 0.8902451395988464
Epoch 350, training loss: 0.18079762160778046 = 0.17371530830860138 + 0.001 * 7.082306385040283
Epoch 350, val loss: 0.8986226916313171
Epoch 360, training loss: 0.15829585492610931 = 0.15120479464530945 + 0.001 * 7.091066837310791
Epoch 360, val loss: 0.9088991284370422
Epoch 370, training loss: 0.13886305689811707 = 0.13178527355194092 + 0.001 * 7.077788829803467
Epoch 370, val loss: 0.9209678769111633
Epoch 380, training loss: 0.12225156277418137 = 0.1151759922504425 + 0.001 * 7.075568199157715
Epoch 380, val loss: 0.9344161748886108
Epoch 390, training loss: 0.10807334631681442 = 0.10099875926971436 + 0.001 * 7.074588775634766
Epoch 390, val loss: 0.9489343762397766
Epoch 400, training loss: 0.09596691280603409 = 0.08889289200305939 + 0.001 * 7.074023246765137
Epoch 400, val loss: 0.9640935659408569
Epoch 410, training loss: 0.08561079204082489 = 0.07853735983371735 + 0.001 * 7.073428153991699
Epoch 410, val loss: 0.9795689582824707
Epoch 420, training loss: 0.07673032581806183 = 0.06965595483779907 + 0.001 * 7.074373722076416
Epoch 420, val loss: 0.9951169490814209
Epoch 430, training loss: 0.06908875703811646 = 0.062017571181058884 + 0.001 * 7.071182727813721
Epoch 430, val loss: 1.0105732679367065
Epoch 440, training loss: 0.06250504404306412 = 0.05543122440576553 + 0.001 * 7.073816776275635
Epoch 440, val loss: 1.025838017463684
Epoch 450, training loss: 0.05680476129055023 = 0.049735188484191895 + 0.001 * 7.069572448730469
Epoch 450, val loss: 1.0407382249832153
Epoch 460, training loss: 0.051864661276340485 = 0.04479576647281647 + 0.001 * 7.068894386291504
Epoch 460, val loss: 1.0553385019302368
Epoch 470, training loss: 0.047584168612957 = 0.04049856215715408 + 0.001 * 7.085606098175049
Epoch 470, val loss: 1.0695794820785522
Epoch 480, training loss: 0.04381592944264412 = 0.036747898906469345 + 0.001 * 7.06803035736084
Epoch 480, val loss: 1.0834494829177856
Epoch 490, training loss: 0.04053383320569992 = 0.033462900668382645 + 0.001 * 7.0709309577941895
Epoch 490, val loss: 1.09689462184906
Epoch 500, training loss: 0.037641845643520355 = 0.030575599521398544 + 0.001 * 7.066244602203369
Epoch 500, val loss: 1.1099412441253662
Epoch 510, training loss: 0.035093531012535095 = 0.02802830934524536 + 0.001 * 7.065220355987549
Epoch 510, val loss: 1.1226078271865845
Epoch 520, training loss: 0.032836951315402985 = 0.025772416964173317 + 0.001 * 7.064535140991211
Epoch 520, val loss: 1.1348119974136353
Epoch 530, training loss: 0.0308319553732872 = 0.023768514394760132 + 0.001 * 7.063441753387451
Epoch 530, val loss: 1.1466200351715088
Epoch 540, training loss: 0.029055751860141754 = 0.021982716396450996 + 0.001 * 7.073035717010498
Epoch 540, val loss: 1.1580513715744019
Epoch 550, training loss: 0.027453146874904633 = 0.020385824143886566 + 0.001 * 7.067322254180908
Epoch 550, val loss: 1.1690168380737305
Epoch 560, training loss: 0.026016248390078545 = 0.018953274935483932 + 0.001 * 7.062973499298096
Epoch 560, val loss: 1.1796244382858276
Epoch 570, training loss: 0.024724900722503662 = 0.01766396500170231 + 0.001 * 7.060935974121094
Epoch 570, val loss: 1.1898621320724487
Epoch 580, training loss: 0.02356741949915886 = 0.016500132158398628 + 0.001 * 7.067287445068359
Epoch 580, val loss: 1.1997275352478027
Epoch 590, training loss: 0.022511962801218033 = 0.015446703881025314 + 0.001 * 7.065258026123047
Epoch 590, val loss: 1.2092562913894653
Epoch 600, training loss: 0.02154707908630371 = 0.014490588568150997 + 0.001 * 7.056490421295166
Epoch 600, val loss: 1.2184464931488037
Epoch 610, training loss: 0.020685788244009018 = 0.013620617799460888 + 0.001 * 7.065169811248779
Epoch 610, val loss: 1.2273156642913818
Epoch 620, training loss: 0.019880929961800575 = 0.012827196158468723 + 0.001 * 7.0537333488464355
Epoch 620, val loss: 1.2358845472335815
Epoch 630, training loss: 0.019153399392962456 = 0.012101789005100727 + 0.001 * 7.051610469818115
Epoch 630, val loss: 1.2441743612289429
Epoch 640, training loss: 0.018499087542295456 = 0.011437103152275085 + 0.001 * 7.061983585357666
Epoch 640, val loss: 1.2522011995315552
Epoch 650, training loss: 0.017879711464047432 = 0.010826676152646542 + 0.001 * 7.053034782409668
Epoch 650, val loss: 1.2599661350250244
Epoch 660, training loss: 0.017317062243819237 = 0.01026496384292841 + 0.001 * 7.052098274230957
Epoch 660, val loss: 1.2674713134765625
Epoch 670, training loss: 0.016792086884379387 = 0.009746975265443325 + 0.001 * 7.045111656188965
Epoch 670, val loss: 1.2747530937194824
Epoch 680, training loss: 0.01632656157016754 = 0.009268460795283318 + 0.001 * 7.058101177215576
Epoch 680, val loss: 1.2817782163619995
Epoch 690, training loss: 0.01587907411158085 = 0.008825637400150299 + 0.001 * 7.053436756134033
Epoch 690, val loss: 1.288605809211731
Epoch 700, training loss: 0.015457951463758945 = 0.008415017277002335 + 0.001 * 7.042933940887451
Epoch 700, val loss: 1.295224666595459
Epoch 710, training loss: 0.015088102780282497 = 0.008033637888729572 + 0.001 * 7.054464340209961
Epoch 710, val loss: 1.301650881767273
Epoch 720, training loss: 0.014722520485520363 = 0.0076787350699305534 + 0.001 * 7.043785095214844
Epoch 720, val loss: 1.307858943939209
Epoch 730, training loss: 0.01438925415277481 = 0.007347959093749523 + 0.001 * 7.041294097900391
Epoch 730, val loss: 1.3138900995254517
Epoch 740, training loss: 0.014089562930166721 = 0.007039238698780537 + 0.001 * 7.050323963165283
Epoch 740, val loss: 1.319754958152771
Epoch 750, training loss: 0.013788091950118542 = 0.006750612985342741 + 0.001 * 7.037478446960449
Epoch 750, val loss: 1.3254555463790894
Epoch 760, training loss: 0.01352732814848423 = 0.006480450741946697 + 0.001 * 7.046876430511475
Epoch 760, val loss: 1.3309822082519531
Epoch 770, training loss: 0.013261593878269196 = 0.006227240432053804 + 0.001 * 7.034352779388428
Epoch 770, val loss: 1.3363996744155884
Epoch 780, training loss: 0.013024862855672836 = 0.0059896428138017654 + 0.001 * 7.035219669342041
Epoch 780, val loss: 1.3416097164154053
Epoch 790, training loss: 0.012800916098058224 = 0.005766396876424551 + 0.001 * 7.034518718719482
Epoch 790, val loss: 1.3467164039611816
Epoch 800, training loss: 0.012587664648890495 = 0.005556423682719469 + 0.001 * 7.031239986419678
Epoch 800, val loss: 1.3516290187835693
Epoch 810, training loss: 0.012383054941892624 = 0.00535865593701601 + 0.001 * 7.024399280548096
Epoch 810, val loss: 1.3564767837524414
Epoch 820, training loss: 0.012201843783259392 = 0.005172152072191238 + 0.001 * 7.029690742492676
Epoch 820, val loss: 1.3611369132995605
Epoch 830, training loss: 0.012022299692034721 = 0.004996039438992739 + 0.001 * 7.026259422302246
Epoch 830, val loss: 1.3657336235046387
Epoch 840, training loss: 0.01186906173825264 = 0.004829446319490671 + 0.001 * 7.039615631103516
Epoch 840, val loss: 1.3701810836791992
Epoch 850, training loss: 0.011702354066073895 = 0.004671432543545961 + 0.001 * 7.03092098236084
Epoch 850, val loss: 1.3745536804199219
Epoch 860, training loss: 0.011540313251316547 = 0.004520868416875601 + 0.001 * 7.019444465637207
Epoch 860, val loss: 1.3788403272628784
Epoch 870, training loss: 0.011413860134780407 = 0.004376559983938932 + 0.001 * 7.037299633026123
Epoch 870, val loss: 1.3831360340118408
Epoch 880, training loss: 0.011254798620939255 = 0.0042377980425953865 + 0.001 * 7.0169997215271
Epoch 880, val loss: 1.3873987197875977
Epoch 890, training loss: 0.011133009567856789 = 0.004104096908122301 + 0.001 * 7.028912544250488
Epoch 890, val loss: 1.3916696310043335
Epoch 900, training loss: 0.010990612208843231 = 0.003975266125053167 + 0.001 * 7.015345573425293
Epoch 900, val loss: 1.3959449529647827
Epoch 910, training loss: 0.01086927019059658 = 0.0038513371255248785 + 0.001 * 7.017932891845703
Epoch 910, val loss: 1.400208592414856
Epoch 920, training loss: 0.010745331645011902 = 0.0037323383148759604 + 0.001 * 7.012992858886719
Epoch 920, val loss: 1.404466152191162
Epoch 930, training loss: 0.010654968209564686 = 0.0036182235926389694 + 0.001 * 7.036744117736816
Epoch 930, val loss: 1.408645749092102
Epoch 940, training loss: 0.010530440136790276 = 0.00350896455347538 + 0.001 * 7.021475791931152
Epoch 940, val loss: 1.4128140211105347
Epoch 950, training loss: 0.010431533679366112 = 0.0034044506028294563 + 0.001 * 7.027082443237305
Epoch 950, val loss: 1.4168967008590698
Epoch 960, training loss: 0.0103255994617939 = 0.0033045809250324965 + 0.001 * 7.0210185050964355
Epoch 960, val loss: 1.4209535121917725
Epoch 970, training loss: 0.010227464139461517 = 0.0032091462053358555 + 0.001 * 7.018318176269531
Epoch 970, val loss: 1.4248874187469482
Epoch 980, training loss: 0.010124840773642063 = 0.0031180279329419136 + 0.001 * 7.006812572479248
Epoch 980, val loss: 1.4287936687469482
Epoch 990, training loss: 0.010042639449238777 = 0.0030309834983199835 + 0.001 * 7.011655330657959
Epoch 990, val loss: 1.4326244592666626
Epoch 1000, training loss: 0.009951724670827389 = 0.002947863657027483 + 0.001 * 7.0038604736328125
Epoch 1000, val loss: 1.4363348484039307
Epoch 1010, training loss: 0.009877155534923077 = 0.0028684574645012617 + 0.001 * 7.008697986602783
Epoch 1010, val loss: 1.4399570226669312
Epoch 1020, training loss: 0.00979086197912693 = 0.002792585175484419 + 0.001 * 6.998276710510254
Epoch 1020, val loss: 1.4435465335845947
Epoch 1030, training loss: 0.009729092940688133 = 0.002720063552260399 + 0.001 * 7.009028434753418
Epoch 1030, val loss: 1.4470434188842773
Epoch 1040, training loss: 0.00966367032378912 = 0.0026507845614105463 + 0.001 * 7.012885093688965
Epoch 1040, val loss: 1.450494408607483
Epoch 1050, training loss: 0.009586690925061703 = 0.002584544476121664 + 0.001 * 7.002146244049072
Epoch 1050, val loss: 1.4538251161575317
Epoch 1060, training loss: 0.009540067985653877 = 0.0025211593601852655 + 0.001 * 7.0189080238342285
Epoch 1060, val loss: 1.457055926322937
Epoch 1070, training loss: 0.009462088346481323 = 0.00246048835106194 + 0.001 * 7.0015997886657715
Epoch 1070, val loss: 1.4602679014205933
Epoch 1080, training loss: 0.009386234916746616 = 0.002402408281341195 + 0.001 * 6.983826637268066
Epoch 1080, val loss: 1.4633418321609497
Epoch 1090, training loss: 0.009331876412034035 = 0.002346768043935299 + 0.001 * 6.985108375549316
Epoch 1090, val loss: 1.4664006233215332
Epoch 1100, training loss: 0.00929717905819416 = 0.0022934486623853445 + 0.001 * 7.003730297088623
Epoch 1100, val loss: 1.4693642854690552
Epoch 1110, training loss: 0.009252814576029778 = 0.0022423265036195517 + 0.001 * 7.0104875564575195
Epoch 1110, val loss: 1.4722685813903809
Epoch 1120, training loss: 0.00917864590883255 = 0.002193269319832325 + 0.001 * 6.985375881195068
Epoch 1120, val loss: 1.475108027458191
Epoch 1130, training loss: 0.009149219840765 = 0.0021461977157741785 + 0.001 * 7.003021717071533
Epoch 1130, val loss: 1.4778649806976318
Epoch 1140, training loss: 0.009088139981031418 = 0.002100998302921653 + 0.001 * 6.9871416091918945
Epoch 1140, val loss: 1.4805817604064941
Epoch 1150, training loss: 0.009039992466568947 = 0.0020575837697833776 + 0.001 * 6.982408046722412
Epoch 1150, val loss: 1.4832020998001099
Epoch 1160, training loss: 0.009005162864923477 = 0.002015868201851845 + 0.001 * 6.98929500579834
Epoch 1160, val loss: 1.4857909679412842
Epoch 1170, training loss: 0.008950811810791492 = 0.0019757261034101248 + 0.001 * 6.975085258483887
Epoch 1170, val loss: 1.4882844686508179
Epoch 1180, training loss: 0.008913218043744564 = 0.0019371008966118097 + 0.001 * 6.976117134094238
Epoch 1180, val loss: 1.4907146692276
Epoch 1190, training loss: 0.008897158317267895 = 0.0018999085295945406 + 0.001 * 6.997249126434326
Epoch 1190, val loss: 1.493133544921875
Epoch 1200, training loss: 0.00885852798819542 = 0.0018641318893060088 + 0.001 * 6.994395732879639
Epoch 1200, val loss: 1.4954668283462524
Epoch 1210, training loss: 0.008811218664050102 = 0.0018296577036380768 + 0.001 * 6.981560707092285
Epoch 1210, val loss: 1.4977247714996338
Epoch 1220, training loss: 0.008763343095779419 = 0.001796450582332909 + 0.001 * 6.966891765594482
Epoch 1220, val loss: 1.4999510049819946
Epoch 1230, training loss: 0.008726040832698345 = 0.0017644253093749285 + 0.001 * 6.961615562438965
Epoch 1230, val loss: 1.502122163772583
Epoch 1240, training loss: 0.008698618970811367 = 0.0017335170414298773 + 0.001 * 6.965101718902588
Epoch 1240, val loss: 1.5042462348937988
Epoch 1250, training loss: 0.008685053326189518 = 0.0017036786302924156 + 0.001 * 6.981374263763428
Epoch 1250, val loss: 1.5063260793685913
Epoch 1260, training loss: 0.008628816343843937 = 0.0016748860944062471 + 0.001 * 6.953929901123047
Epoch 1260, val loss: 1.508363962173462
Epoch 1270, training loss: 0.0086343539878726 = 0.0016470957780256867 + 0.001 * 6.987257480621338
Epoch 1270, val loss: 1.51033353805542
Epoch 1280, training loss: 0.00859951414167881 = 0.0016202490078285336 + 0.001 * 6.979264736175537
Epoch 1280, val loss: 1.5123218297958374
Epoch 1290, training loss: 0.008555572479963303 = 0.0015943162143230438 + 0.001 * 6.96125602722168
Epoch 1290, val loss: 1.5141832828521729
Epoch 1300, training loss: 0.008536173030734062 = 0.0015692461747676134 + 0.001 * 6.966926574707031
Epoch 1300, val loss: 1.5160412788391113
Epoch 1310, training loss: 0.008501986972987652 = 0.0015450067585334182 + 0.001 * 6.956979751586914
Epoch 1310, val loss: 1.5178312063217163
Epoch 1320, training loss: 0.008489959873259068 = 0.0015215413877740502 + 0.001 * 6.968418121337891
Epoch 1320, val loss: 1.5196099281311035
Epoch 1330, training loss: 0.008448733016848564 = 0.0014988200273364782 + 0.001 * 6.9499125480651855
Epoch 1330, val loss: 1.5213308334350586
Epoch 1340, training loss: 0.00844270084053278 = 0.0014768370892852545 + 0.001 * 6.965863227844238
Epoch 1340, val loss: 1.523021936416626
Epoch 1350, training loss: 0.008470730856060982 = 0.0014555244706571102 + 0.001 * 7.0152058601379395
Epoch 1350, val loss: 1.5246957540512085
Epoch 1360, training loss: 0.008394293487071991 = 0.001434900681488216 + 0.001 * 6.959392547607422
Epoch 1360, val loss: 1.5262879133224487
Epoch 1370, training loss: 0.008359209634363651 = 0.0014149084454402328 + 0.001 * 6.944301128387451
Epoch 1370, val loss: 1.5278539657592773
Epoch 1380, training loss: 0.008333765901625156 = 0.0013955025933682919 + 0.001 * 6.938262939453125
Epoch 1380, val loss: 1.5294133424758911
Epoch 1390, training loss: 0.008355258032679558 = 0.0013766871998086572 + 0.001 * 6.978570461273193
Epoch 1390, val loss: 1.5309258699417114
Epoch 1400, training loss: 0.008305402472615242 = 0.0013584165135398507 + 0.001 * 6.946985721588135
Epoch 1400, val loss: 1.5324138402938843
Epoch 1410, training loss: 0.008291738107800484 = 0.0013406742364168167 + 0.001 * 6.951064109802246
Epoch 1410, val loss: 1.5338777303695679
Epoch 1420, training loss: 0.008265193551778793 = 0.0013234196230769157 + 0.001 * 6.941773891448975
Epoch 1420, val loss: 1.5352783203125
Epoch 1430, training loss: 0.008253364823758602 = 0.0013066219398751855 + 0.001 * 6.946742057800293
Epoch 1430, val loss: 1.5366904735565186
Epoch 1440, training loss: 0.00824197567999363 = 0.0012902364833280444 + 0.001 * 6.9517388343811035
Epoch 1440, val loss: 1.5380663871765137
Epoch 1450, training loss: 0.008237473666667938 = 0.001274236710742116 + 0.001 * 6.9632368087768555
Epoch 1450, val loss: 1.5393935441970825
Epoch 1460, training loss: 0.008207332342863083 = 0.0012585436925292015 + 0.001 * 6.948788642883301
Epoch 1460, val loss: 1.5407507419586182
Epoch 1470, training loss: 0.008174268528819084 = 0.001243141246959567 + 0.001 * 6.931127071380615
Epoch 1470, val loss: 1.5420371294021606
Epoch 1480, training loss: 0.008174433372914791 = 0.00122794508934021 + 0.001 * 6.946487903594971
Epoch 1480, val loss: 1.5432939529418945
Epoch 1490, training loss: 0.008164371363818645 = 0.0012129273964092135 + 0.001 * 6.951444149017334
Epoch 1490, val loss: 1.5445401668548584
Epoch 1500, training loss: 0.008127064444124699 = 0.0011980051640421152 + 0.001 * 6.92905855178833
Epoch 1500, val loss: 1.5457229614257812
Epoch 1510, training loss: 0.00809984840452671 = 0.001183203305117786 + 0.001 * 6.91664457321167
Epoch 1510, val loss: 1.546922206878662
Epoch 1520, training loss: 0.008087255991995335 = 0.001168484683148563 + 0.001 * 6.918771266937256
Epoch 1520, val loss: 1.5480949878692627
Epoch 1530, training loss: 0.008065893314778805 = 0.00115382659714669 + 0.001 * 6.912066459655762
Epoch 1530, val loss: 1.5492843389511108
Epoch 1540, training loss: 0.008050682954490185 = 0.0011392106534913182 + 0.001 * 6.911471843719482
Epoch 1540, val loss: 1.5503770112991333
Epoch 1550, training loss: 0.008066374808549881 = 0.0011246484937146306 + 0.001 * 6.941725730895996
Epoch 1550, val loss: 1.5515493154525757
Epoch 1560, training loss: 0.008030679076910019 = 0.0011101915733888745 + 0.001 * 6.920487403869629
Epoch 1560, val loss: 1.5527000427246094
Epoch 1570, training loss: 0.008045155555009842 = 0.0010958292987197638 + 0.001 * 6.9493255615234375
Epoch 1570, val loss: 1.5538543462753296
Epoch 1580, training loss: 0.008038136176764965 = 0.0010816280264407396 + 0.001 * 6.956507682800293
Epoch 1580, val loss: 1.5549741983413696
Epoch 1590, training loss: 0.008006169460713863 = 0.0010675884550437331 + 0.001 * 6.938580513000488
Epoch 1590, val loss: 1.5560253858566284
Epoch 1600, training loss: 0.00795659888535738 = 0.001053755753673613 + 0.001 * 6.9028425216674805
Epoch 1600, val loss: 1.5570827722549438
Epoch 1610, training loss: 0.007954955101013184 = 0.0010401238687336445 + 0.001 * 6.914830684661865
Epoch 1610, val loss: 1.5581573247909546
Epoch 1620, training loss: 0.007928645238280296 = 0.0010267480975016952 + 0.001 * 6.901896953582764
Epoch 1620, val loss: 1.5592542886734009
Epoch 1630, training loss: 0.007910639978945255 = 0.001013667439110577 + 0.001 * 6.896971702575684
Epoch 1630, val loss: 1.5603421926498413
Epoch 1640, training loss: 0.007920578122138977 = 0.0010008978424593806 + 0.001 * 6.919680118560791
Epoch 1640, val loss: 1.5613504648208618
Epoch 1650, training loss: 0.007891750894486904 = 0.000988473417237401 + 0.001 * 6.9032769203186035
Epoch 1650, val loss: 1.562336802482605
Epoch 1660, training loss: 0.00788632407784462 = 0.0009763778652995825 + 0.001 * 6.909945487976074
Epoch 1660, val loss: 1.5633918046951294
Epoch 1670, training loss: 0.007856749929487705 = 0.0009646045509725809 + 0.001 * 6.892144680023193
Epoch 1670, val loss: 1.5643811225891113
Epoch 1680, training loss: 0.007864177227020264 = 0.0009531209361739457 + 0.001 * 6.911056041717529
Epoch 1680, val loss: 1.5652631521224976
Epoch 1690, training loss: 0.007885623723268509 = 0.0009419585112482309 + 0.001 * 6.943665027618408
Epoch 1690, val loss: 1.5662634372711182
Epoch 1700, training loss: 0.007844364270567894 = 0.0009310821769759059 + 0.001 * 6.9132819175720215
Epoch 1700, val loss: 1.567262053489685
Epoch 1710, training loss: 0.007826946675777435 = 0.0009204983944073319 + 0.001 * 6.906447410583496
Epoch 1710, val loss: 1.568085789680481
Epoch 1720, training loss: 0.007809457369148731 = 0.0009101819596253335 + 0.001 * 6.899274826049805
Epoch 1720, val loss: 1.5689047574996948
Epoch 1730, training loss: 0.0077822767198085785 = 0.0009001741418614984 + 0.001 * 6.8821024894714355
Epoch 1730, val loss: 1.5698678493499756
Epoch 1740, training loss: 0.007771986536681652 = 0.0008904286078177392 + 0.001 * 6.881557464599609
Epoch 1740, val loss: 1.5707529783248901
Epoch 1750, training loss: 0.007770255673676729 = 0.0008809551363810897 + 0.001 * 6.8892998695373535
Epoch 1750, val loss: 1.5716561079025269
Epoch 1760, training loss: 0.007774003781378269 = 0.0008717479067854583 + 0.001 * 6.902255535125732
Epoch 1760, val loss: 1.5724101066589355
Epoch 1770, training loss: 0.007747909985482693 = 0.0008627907373011112 + 0.001 * 6.8851189613342285
Epoch 1770, val loss: 1.5732038021087646
Epoch 1780, training loss: 0.007740079425275326 = 0.0008540599374100566 + 0.001 * 6.886019229888916
Epoch 1780, val loss: 1.5739887952804565
Epoch 1790, training loss: 0.0077215577475726604 = 0.0008455721545033157 + 0.001 * 6.875985622406006
Epoch 1790, val loss: 1.5748151540756226
Epoch 1800, training loss: 0.007712634280323982 = 0.0008373111486434937 + 0.001 * 6.8753228187561035
Epoch 1800, val loss: 1.5754209756851196
Epoch 1810, training loss: 0.007697022054344416 = 0.0008292553829960525 + 0.001 * 6.867766380310059
Epoch 1810, val loss: 1.5762732028961182
Epoch 1820, training loss: 0.0077257039956748486 = 0.0008214459521695971 + 0.001 * 6.904257774353027
Epoch 1820, val loss: 1.5771011114120483
Epoch 1830, training loss: 0.0076900627464056015 = 0.0008138548582792282 + 0.001 * 6.87620735168457
Epoch 1830, val loss: 1.5777506828308105
Epoch 1840, training loss: 0.007668455131351948 = 0.0008065279689617455 + 0.001 * 6.861926555633545
Epoch 1840, val loss: 1.5784099102020264
Epoch 1850, training loss: 0.007645152974873781 = 0.0007994281477294862 + 0.001 * 6.845724582672119
Epoch 1850, val loss: 1.5790561437606812
Epoch 1860, training loss: 0.007647817954421043 = 0.0007925396203063428 + 0.001 * 6.855278015136719
Epoch 1860, val loss: 1.5796836614608765
Epoch 1870, training loss: 0.007660447619855404 = 0.0007858527824282646 + 0.001 * 6.874594688415527
Epoch 1870, val loss: 1.5803042650222778
Epoch 1880, training loss: 0.007669185753911734 = 0.0007793150725774467 + 0.001 * 6.889870643615723
Epoch 1880, val loss: 1.580960988998413
Epoch 1890, training loss: 0.007655754219740629 = 0.0007729902863502502 + 0.001 * 6.882763385772705
Epoch 1890, val loss: 1.5815984010696411
Epoch 1900, training loss: 0.007641189265996218 = 0.0007667937316000462 + 0.001 * 6.874395370483398
Epoch 1900, val loss: 1.5821661949157715
Epoch 1910, training loss: 0.007614491041749716 = 0.000760762020945549 + 0.001 * 6.853728771209717
Epoch 1910, val loss: 1.58280611038208
Epoch 1920, training loss: 0.007624237798154354 = 0.0007549007423222065 + 0.001 * 6.8693366050720215
Epoch 1920, val loss: 1.5833622217178345
Epoch 1930, training loss: 0.0075955139473080635 = 0.0007491980213671923 + 0.001 * 6.846315860748291
Epoch 1930, val loss: 1.583937406539917
Epoch 1940, training loss: 0.0075912210159003735 = 0.0007436120067723095 + 0.001 * 6.84760856628418
Epoch 1940, val loss: 1.5844390392303467
Epoch 1950, training loss: 0.007635024841874838 = 0.0007382090552709997 + 0.001 * 6.896815299987793
Epoch 1950, val loss: 1.5849878787994385
Epoch 1960, training loss: 0.0076270941644907 = 0.000732932414393872 + 0.001 * 6.894161701202393
Epoch 1960, val loss: 1.5855623483657837
Epoch 1970, training loss: 0.007607316132634878 = 0.0007278085686266422 + 0.001 * 6.879507064819336
Epoch 1970, val loss: 1.5860011577606201
Epoch 1980, training loss: 0.00757561344653368 = 0.0007228259928524494 + 0.001 * 6.852787017822266
Epoch 1980, val loss: 1.5866186618804932
Epoch 1990, training loss: 0.007569098845124245 = 0.0007179866079241037 + 0.001 * 6.851112365722656
Epoch 1990, val loss: 1.5870113372802734
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 1.9503555297851562 = 1.9417587518692017 + 0.001 * 8.596826553344727
Epoch 0, val loss: 1.940588116645813
Epoch 10, training loss: 1.9404906034469604 = 1.9318938255310059 + 0.001 * 8.59675407409668
Epoch 10, val loss: 1.9304174184799194
Epoch 20, training loss: 1.9278295040130615 = 1.919232964515686 + 0.001 * 8.596513748168945
Epoch 20, val loss: 1.9172872304916382
Epoch 30, training loss: 1.909698247909546 = 1.9011023044586182 + 0.001 * 8.595959663391113
Epoch 30, val loss: 1.8986082077026367
Epoch 40, training loss: 1.8834588527679443 = 1.8748642206192017 + 0.001 * 8.594578742980957
Epoch 40, val loss: 1.872144341468811
Epoch 50, training loss: 1.8489201068878174 = 1.8403297662734985 + 0.001 * 8.590313911437988
Epoch 50, val loss: 1.8392919301986694
Epoch 60, training loss: 1.8137731552124023 = 1.8052003383636475 + 0.001 * 8.572821617126465
Epoch 60, val loss: 1.8094314336776733
Epoch 70, training loss: 1.7849682569503784 = 1.776490569114685 + 0.001 * 8.477738380432129
Epoch 70, val loss: 1.7861406803131104
Epoch 80, training loss: 1.7475582361221313 = 1.7394472360610962 + 0.001 * 8.111008644104004
Epoch 80, val loss: 1.7541732788085938
Epoch 90, training loss: 1.693969488143921 = 1.6859681606292725 + 0.001 * 8.001337051391602
Epoch 90, val loss: 1.7101893424987793
Epoch 100, training loss: 1.6208521127700806 = 1.6129316091537476 + 0.001 * 7.920558452606201
Epoch 100, val loss: 1.6520134210586548
Epoch 110, training loss: 1.5334771871566772 = 1.5256800651550293 + 0.001 * 7.797175407409668
Epoch 110, val loss: 1.583970546722412
Epoch 120, training loss: 1.4438602924346924 = 1.4362374544143677 + 0.001 * 7.622802734375
Epoch 120, val loss: 1.5169587135314941
Epoch 130, training loss: 1.3569526672363281 = 1.3494189977645874 + 0.001 * 7.5337018966674805
Epoch 130, val loss: 1.4539859294891357
Epoch 140, training loss: 1.2708674669265747 = 1.2634261846542358 + 0.001 * 7.441333293914795
Epoch 140, val loss: 1.3939253091812134
Epoch 150, training loss: 1.1850495338439941 = 1.1777336597442627 + 0.001 * 7.315814018249512
Epoch 150, val loss: 1.335418939590454
Epoch 160, training loss: 1.1028465032577515 = 1.0956238508224487 + 0.001 * 7.222659111022949
Epoch 160, val loss: 1.2806955575942993
Epoch 170, training loss: 1.0278306007385254 = 1.020658016204834 + 0.001 * 7.172550678253174
Epoch 170, val loss: 1.2316182851791382
Epoch 180, training loss: 0.9604530930519104 = 0.9532924294471741 + 0.001 * 7.160634994506836
Epoch 180, val loss: 1.187412142753601
Epoch 190, training loss: 0.8981903195381165 = 0.8910320401191711 + 0.001 * 7.15825080871582
Epoch 190, val loss: 1.1454037427902222
Epoch 200, training loss: 0.8370783925056458 = 0.8299217224121094 + 0.001 * 7.156679630279541
Epoch 200, val loss: 1.1025855541229248
Epoch 210, training loss: 0.7734599113464355 = 0.7663046717643738 + 0.001 * 7.1552205085754395
Epoch 210, val loss: 1.0568050146102905
Epoch 220, training loss: 0.7054654955863953 = 0.6983118057250977 + 0.001 * 7.153692245483398
Epoch 220, val loss: 1.007407546043396
Epoch 230, training loss: 0.6337537169456482 = 0.6266017556190491 + 0.001 * 7.151952743530273
Epoch 230, val loss: 0.9559666514396667
Epoch 240, training loss: 0.5609920620918274 = 0.5538420677185059 + 0.001 * 7.149966239929199
Epoch 240, val loss: 0.9059262871742249
Epoch 250, training loss: 0.4903823137283325 = 0.4832345247268677 + 0.001 * 7.147801399230957
Epoch 250, val loss: 0.860918402671814
Epoch 260, training loss: 0.4243975281715393 = 0.41725194454193115 + 0.001 * 7.145586967468262
Epoch 260, val loss: 0.8231399059295654
Epoch 270, training loss: 0.364613801240921 = 0.3574702739715576 + 0.001 * 7.1435394287109375
Epoch 270, val loss: 0.7934532165527344
Epoch 280, training loss: 0.3117715120315552 = 0.30462974309921265 + 0.001 * 7.141770839691162
Epoch 280, val loss: 0.7714484930038452
Epoch 290, training loss: 0.26618942618370056 = 0.25904616713523865 + 0.001 * 7.1432647705078125
Epoch 290, val loss: 0.756756067276001
Epoch 300, training loss: 0.22755563259124756 = 0.2204153835773468 + 0.001 * 7.14024543762207
Epoch 300, val loss: 0.7484728097915649
Epoch 310, training loss: 0.19516794383525848 = 0.1880292445421219 + 0.001 * 7.138701915740967
Epoch 310, val loss: 0.745537281036377
Epoch 320, training loss: 0.16815710067749023 = 0.1610189825296402 + 0.001 * 7.138122081756592
Epoch 320, val loss: 0.7469008564949036
Epoch 330, training loss: 0.14561285078525543 = 0.13847477734088898 + 0.001 * 7.13808012008667
Epoch 330, val loss: 0.751648485660553
Epoch 340, training loss: 0.12670151889324188 = 0.11956321448087692 + 0.001 * 7.13830041885376
Epoch 340, val loss: 0.7589550614356995
Epoch 350, training loss: 0.11074463278055191 = 0.10360591113567352 + 0.001 * 7.138721466064453
Epoch 350, val loss: 0.7681941986083984
Epoch 360, training loss: 0.09721715748310089 = 0.09007791429758072 + 0.001 * 7.139242172241211
Epoch 360, val loss: 0.7787488698959351
Epoch 370, training loss: 0.08570939302444458 = 0.07856956869363785 + 0.001 * 7.139820098876953
Epoch 370, val loss: 0.7902495265007019
Epoch 380, training loss: 0.07590263336896896 = 0.06876180320978165 + 0.001 * 7.1408281326293945
Epoch 380, val loss: 0.8024255633354187
Epoch 390, training loss: 0.0675392746925354 = 0.06039823591709137 + 0.001 * 7.141036510467529
Epoch 390, val loss: 0.8150516152381897
Epoch 400, training loss: 0.06040428951382637 = 0.05326260253787041 + 0.001 * 7.141687870025635
Epoch 400, val loss: 0.8279968500137329
Epoch 410, training loss: 0.0543120838701725 = 0.04716990515589714 + 0.001 * 7.142178535461426
Epoch 410, val loss: 0.8410851359367371
Epoch 420, training loss: 0.049103621393442154 = 0.04196097329258919 + 0.001 * 7.142647743225098
Epoch 420, val loss: 0.8541542887687683
Epoch 430, training loss: 0.04464193060994148 = 0.03749876841902733 + 0.001 * 7.143160820007324
Epoch 430, val loss: 0.867171585559845
Epoch 440, training loss: 0.040809910744428635 = 0.033665984869003296 + 0.001 * 7.143925189971924
Epoch 440, val loss: 0.8800061345100403
Epoch 450, training loss: 0.03750482574105263 = 0.030360780656337738 + 0.001 * 7.144046306610107
Epoch 450, val loss: 0.8926002979278564
Epoch 460, training loss: 0.034640613943338394 = 0.02749631740152836 + 0.001 * 7.1442975997924805
Epoch 460, val loss: 0.904899001121521
Epoch 470, training loss: 0.03214762732386589 = 0.025003205984830856 + 0.001 * 7.144420146942139
Epoch 470, val loss: 0.9169310331344604
Epoch 480, training loss: 0.029966697096824646 = 0.022822201251983643 + 0.001 * 7.144495010375977
Epoch 480, val loss: 0.9286457300186157
Epoch 490, training loss: 0.028050687164068222 = 0.020905597135424614 + 0.001 * 7.145089149475098
Epoch 490, val loss: 0.9400912523269653
Epoch 500, training loss: 0.026359591633081436 = 0.01921466924250126 + 0.001 * 7.14492130279541
Epoch 500, val loss: 0.9512816071510315
Epoch 510, training loss: 0.02486199140548706 = 0.017717255279421806 + 0.001 * 7.144735336303711
Epoch 510, val loss: 0.9621630907058716
Epoch 520, training loss: 0.0235346257686615 = 0.016386695206165314 + 0.001 * 7.147931098937988
Epoch 520, val loss: 0.9727640748023987
Epoch 530, training loss: 0.022345570847392082 = 0.015200558118522167 + 0.001 * 7.145012855529785
Epoch 530, val loss: 0.9831029772758484
Epoch 540, training loss: 0.021284194663167 = 0.014139574021100998 + 0.001 * 7.144620418548584
Epoch 540, val loss: 0.9931300282478333
Epoch 550, training loss: 0.020336370915174484 = 0.013187597505748272 + 0.001 * 7.148772239685059
Epoch 550, val loss: 1.002867579460144
Epoch 560, training loss: 0.0194756630808115 = 0.012330735102295876 + 0.001 * 7.144927024841309
Epoch 560, val loss: 1.0123622417449951
Epoch 570, training loss: 0.01870151050388813 = 0.011557345278561115 + 0.001 * 7.144164562225342
Epoch 570, val loss: 1.0215338468551636
Epoch 580, training loss: 0.018000774085521698 = 0.010857033543288708 + 0.001 * 7.143739223480225
Epoch 580, val loss: 1.0304925441741943
Epoch 590, training loss: 0.017364507541060448 = 0.010221075266599655 + 0.001 * 7.143432140350342
Epoch 590, val loss: 1.0391710996627808
Epoch 600, training loss: 0.016797423362731934 = 0.009641963988542557 + 0.001 * 7.155459403991699
Epoch 600, val loss: 1.047597885131836
Epoch 610, training loss: 0.01625913195312023 = 0.009113306179642677 + 0.001 * 7.1458258628845215
Epoch 610, val loss: 1.0557913780212402
Epoch 620, training loss: 0.015772812068462372 = 0.008629354648292065 + 0.001 * 7.14345645904541
Epoch 620, val loss: 1.0637952089309692
Epoch 630, training loss: 0.015327474102377892 = 0.008185219019651413 + 0.001 * 7.142254829406738
Epoch 630, val loss: 1.071515679359436
Epoch 640, training loss: 0.014918213710188866 = 0.0077766841277480125 + 0.001 * 7.141529083251953
Epoch 640, val loss: 1.079108476638794
Epoch 650, training loss: 0.014545135200023651 = 0.007400040049105883 + 0.001 * 7.145094394683838
Epoch 650, val loss: 1.0864362716674805
Epoch 660, training loss: 0.014193560928106308 = 0.00705206673592329 + 0.001 * 7.141493320465088
Epoch 660, val loss: 1.0935890674591064
Epoch 670, training loss: 0.01387082226574421 = 0.006729921326041222 + 0.001 * 7.140900135040283
Epoch 670, val loss: 1.1005719900131226
Epoch 680, training loss: 0.013577735982835293 = 0.006431073881685734 + 0.001 * 7.146661758422852
Epoch 680, val loss: 1.1073395013809204
Epoch 690, training loss: 0.013297376222908497 = 0.00615328224375844 + 0.001 * 7.1440935134887695
Epoch 690, val loss: 1.113983154296875
Epoch 700, training loss: 0.013033313676714897 = 0.005894551053643227 + 0.001 * 7.138762474060059
Epoch 700, val loss: 1.120425820350647
Epoch 710, training loss: 0.01278967410326004 = 0.005653089843690395 + 0.001 * 7.136584281921387
Epoch 710, val loss: 1.1267664432525635
Epoch 720, training loss: 0.012562062591314316 = 0.005427208263427019 + 0.001 * 7.134853363037109
Epoch 720, val loss: 1.1329188346862793
Epoch 730, training loss: 0.012355825863778591 = 0.005215466022491455 + 0.001 * 7.140359401702881
Epoch 730, val loss: 1.1389639377593994
Epoch 740, training loss: 0.012153173796832561 = 0.005016590468585491 + 0.001 * 7.136582851409912
Epoch 740, val loss: 1.144851565361023
Epoch 750, training loss: 0.011959383264183998 = 0.0048293438740074635 + 0.001 * 7.130039691925049
Epoch 750, val loss: 1.1506391763687134
Epoch 760, training loss: 0.01178019493818283 = 0.004652680829167366 + 0.001 * 7.127513885498047
Epoch 760, val loss: 1.1563055515289307
Epoch 770, training loss: 0.011641602963209152 = 0.004485732410103083 + 0.001 * 7.15587043762207
Epoch 770, val loss: 1.1618826389312744
Epoch 780, training loss: 0.011459225788712502 = 0.004327909555286169 + 0.001 * 7.131315231323242
Epoch 780, val loss: 1.167344093322754
Epoch 790, training loss: 0.011308702640235424 = 0.004178433679044247 + 0.001 * 7.130268573760986
Epoch 790, val loss: 1.1727021932601929
Epoch 800, training loss: 0.011159326881170273 = 0.004036648198962212 + 0.001 * 7.122678279876709
Epoch 800, val loss: 1.1779907941818237
Epoch 810, training loss: 0.011032012291252613 = 0.003902062540873885 + 0.001 * 7.12994909286499
Epoch 810, val loss: 1.1831996440887451
Epoch 820, training loss: 0.010890948586165905 = 0.0037742741405963898 + 0.001 * 7.116673946380615
Epoch 820, val loss: 1.188300371170044
Epoch 830, training loss: 0.010772711597383022 = 0.003652767976745963 + 0.001 * 7.119943141937256
Epoch 830, val loss: 1.1933434009552002
Epoch 840, training loss: 0.010656725615262985 = 0.003537251614034176 + 0.001 * 7.119473457336426
Epoch 840, val loss: 1.1982800960540771
Epoch 850, training loss: 0.010531194508075714 = 0.003426850773394108 + 0.001 * 7.104343414306641
Epoch 850, val loss: 1.2031521797180176
Epoch 860, training loss: 0.010468273423612118 = 0.0033210618421435356 + 0.001 * 7.147211074829102
Epoch 860, val loss: 1.2079750299453735
Epoch 870, training loss: 0.010330742225050926 = 0.0032195670064538717 + 0.001 * 7.111174583435059
Epoch 870, val loss: 1.2127238512039185
Epoch 880, training loss: 0.01024121604859829 = 0.0031218973454087973 + 0.001 * 7.119318008422852
Epoch 880, val loss: 1.2174568176269531
Epoch 890, training loss: 0.010121887549757957 = 0.0030280756764113903 + 0.001 * 7.09381103515625
Epoch 890, val loss: 1.2221348285675049
Epoch 900, training loss: 0.010022241622209549 = 0.0029379611369222403 + 0.001 * 7.084280014038086
Epoch 900, val loss: 1.2267837524414062
Epoch 910, training loss: 0.00997407827526331 = 0.0028514566365629435 + 0.001 * 7.122621059417725
Epoch 910, val loss: 1.231425404548645
Epoch 920, training loss: 0.009873732924461365 = 0.002768584294244647 + 0.001 * 7.105147838592529
Epoch 920, val loss: 1.2360390424728394
Epoch 930, training loss: 0.009827314876019955 = 0.0026892346795648336 + 0.001 * 7.138079643249512
Epoch 930, val loss: 1.240558385848999
Epoch 940, training loss: 0.00970449112355709 = 0.0026133465580642223 + 0.001 * 7.09114408493042
Epoch 940, val loss: 1.245017647743225
Epoch 950, training loss: 0.009637721814215183 = 0.0025406701024621725 + 0.001 * 7.097051620483398
Epoch 950, val loss: 1.2494633197784424
Epoch 960, training loss: 0.009609600529074669 = 0.002471241634339094 + 0.001 * 7.1383585929870605
Epoch 960, val loss: 1.2538717985153198
Epoch 970, training loss: 0.009480725042521954 = 0.0024049580097198486 + 0.001 * 7.075766563415527
Epoch 970, val loss: 1.2582182884216309
Epoch 980, training loss: 0.009474319405853748 = 0.002341661136597395 + 0.001 * 7.132658004760742
Epoch 980, val loss: 1.262510061264038
Epoch 990, training loss: 0.009352035820484161 = 0.0022812564857304096 + 0.001 * 7.0707783699035645
Epoch 990, val loss: 1.2667441368103027
Epoch 1000, training loss: 0.009271608665585518 = 0.0022235126234591007 + 0.001 * 7.048095226287842
Epoch 1000, val loss: 1.2709344625473022
Epoch 1010, training loss: 0.009296822361648083 = 0.0021683371160179377 + 0.001 * 7.128484725952148
Epoch 1010, val loss: 1.2750709056854248
Epoch 1020, training loss: 0.009168749675154686 = 0.002115646842867136 + 0.001 * 7.053102970123291
Epoch 1020, val loss: 1.279159426689148
Epoch 1030, training loss: 0.009135439991950989 = 0.0020651640370488167 + 0.001 * 7.070276260375977
Epoch 1030, val loss: 1.2831450700759888
Epoch 1040, training loss: 0.009076491929590702 = 0.0020168900955468416 + 0.001 * 7.059601783752441
Epoch 1040, val loss: 1.2871190309524536
Epoch 1050, training loss: 0.00901356153190136 = 0.0019706846214830875 + 0.001 * 7.042876243591309
Epoch 1050, val loss: 1.2910101413726807
Epoch 1060, training loss: 0.008986616507172585 = 0.0019264458678662777 + 0.001 * 7.060169696807861
Epoch 1060, val loss: 1.2948992252349854
Epoch 1070, training loss: 0.008921908214688301 = 0.0018840915290638804 + 0.001 * 7.037816047668457
Epoch 1070, val loss: 1.2987020015716553
Epoch 1080, training loss: 0.00888949353247881 = 0.0018434918019920588 + 0.001 * 7.046001434326172
Epoch 1080, val loss: 1.3024183511734009
Epoch 1090, training loss: 0.008872407488524914 = 0.0018045614706352353 + 0.001 * 7.067845821380615
Epoch 1090, val loss: 1.3061381578445435
Epoch 1100, training loss: 0.008809751830995083 = 0.0017671765526756644 + 0.001 * 7.042574882507324
Epoch 1100, val loss: 1.3097773790359497
Epoch 1110, training loss: 0.008756729774177074 = 0.0017313029384240508 + 0.001 * 7.025426387786865
Epoch 1110, val loss: 1.313383936882019
Epoch 1120, training loss: 0.008722053840756416 = 0.0016968707786872983 + 0.001 * 7.025182723999023
Epoch 1120, val loss: 1.3169265985488892
Epoch 1130, training loss: 0.008685044012963772 = 0.0016638104571029544 + 0.001 * 7.021233081817627
Epoch 1130, val loss: 1.3203933238983154
Epoch 1140, training loss: 0.008710387162864208 = 0.0016320268623530865 + 0.001 * 7.078360080718994
Epoch 1140, val loss: 1.3238928318023682
Epoch 1150, training loss: 0.008627542294561863 = 0.0016014504944905639 + 0.001 * 7.026091575622559
Epoch 1150, val loss: 1.3272637128829956
Epoch 1160, training loss: 0.00859547033905983 = 0.0015720209339633584 + 0.001 * 7.023448944091797
Epoch 1160, val loss: 1.3305631875991821
Epoch 1170, training loss: 0.008565870113670826 = 0.0015436861431226134 + 0.001 * 7.022183895111084
Epoch 1170, val loss: 1.3338866233825684
Epoch 1180, training loss: 0.008535904809832573 = 0.0015164081705734134 + 0.001 * 7.019495964050293
Epoch 1180, val loss: 1.3371440172195435
Epoch 1190, training loss: 0.008501281030476093 = 0.0014901255490258336 + 0.001 * 7.011154651641846
Epoch 1190, val loss: 1.3403198719024658
Epoch 1200, training loss: 0.008474995382130146 = 0.0014648012584075332 + 0.001 * 7.010193824768066
Epoch 1200, val loss: 1.3434885740280151
Epoch 1210, training loss: 0.008472757413983345 = 0.0014403710374608636 + 0.001 * 7.03238582611084
Epoch 1210, val loss: 1.346594214439392
Epoch 1220, training loss: 0.008430367335677147 = 0.0014167979825288057 + 0.001 * 7.013569355010986
Epoch 1220, val loss: 1.3496665954589844
Epoch 1230, training loss: 0.008410087786614895 = 0.0013940598582848907 + 0.001 * 7.016027450561523
Epoch 1230, val loss: 1.3526747226715088
Epoch 1240, training loss: 0.008384358137845993 = 0.0013721210416406393 + 0.001 * 7.012237071990967
Epoch 1240, val loss: 1.3556703329086304
Epoch 1250, training loss: 0.008356324397027493 = 0.001350918086245656 + 0.001 * 7.005406379699707
Epoch 1250, val loss: 1.3585643768310547
Epoch 1260, training loss: 0.008345214650034904 = 0.0013304511085152626 + 0.001 * 7.014763832092285
Epoch 1260, val loss: 1.361457109451294
Epoch 1270, training loss: 0.008339940570294857 = 0.0013106701662763953 + 0.001 * 7.029270172119141
Epoch 1270, val loss: 1.3643410205841064
Epoch 1280, training loss: 0.008313775062561035 = 0.0012915190309286118 + 0.001 * 7.0222554206848145
Epoch 1280, val loss: 1.3671544790267944
Epoch 1290, training loss: 0.008279475383460522 = 0.001273014466278255 + 0.001 * 7.006460666656494
Epoch 1290, val loss: 1.369942307472229
Epoch 1300, training loss: 0.0082472525537014 = 0.0012551158433780074 + 0.001 * 6.9921369552612305
Epoch 1300, val loss: 1.3726990222930908
Epoch 1310, training loss: 0.00824063178151846 = 0.001237784163095057 + 0.001 * 7.002846717834473
Epoch 1310, val loss: 1.3754163980484009
Epoch 1320, training loss: 0.008215866051614285 = 0.0012210271088406444 + 0.001 * 6.994839191436768
Epoch 1320, val loss: 1.3781120777130127
Epoch 1330, training loss: 0.008196411654353142 = 0.001204793923534453 + 0.001 * 6.991617679595947
Epoch 1330, val loss: 1.3807107210159302
Epoch 1340, training loss: 0.008201134391129017 = 0.0011890647001564503 + 0.001 * 7.012069225311279
Epoch 1340, val loss: 1.3833242654800415
Epoch 1350, training loss: 0.008178047835826874 = 0.0011738240718841553 + 0.001 * 7.004223346710205
Epoch 1350, val loss: 1.3859124183654785
Epoch 1360, training loss: 0.00816976185888052 = 0.0011590670328587294 + 0.001 * 7.01069450378418
Epoch 1360, val loss: 1.3884375095367432
Epoch 1370, training loss: 0.008129333145916462 = 0.001144736772403121 + 0.001 * 6.984595775604248
Epoch 1370, val loss: 1.390950322151184
Epoch 1380, training loss: 0.008110415190458298 = 0.0011308676330372691 + 0.001 * 6.979547023773193
Epoch 1380, val loss: 1.3934022188186646
Epoch 1390, training loss: 0.008105612359941006 = 0.0011174193350598216 + 0.001 * 6.988193035125732
Epoch 1390, val loss: 1.3958925008773804
Epoch 1400, training loss: 0.008107283152639866 = 0.0011043723206967115 + 0.001 * 7.002910614013672
Epoch 1400, val loss: 1.3983100652694702
Epoch 1410, training loss: 0.008082181215286255 = 0.0010917091276496649 + 0.001 * 6.990471363067627
Epoch 1410, val loss: 1.4006720781326294
Epoch 1420, training loss: 0.008096469566226006 = 0.0010794379049912095 + 0.001 * 7.017031192779541
Epoch 1420, val loss: 1.4030581712722778
Epoch 1430, training loss: 0.008045004680752754 = 0.0010674902005121112 + 0.001 * 6.977514266967773
Epoch 1430, val loss: 1.40535306930542
Epoch 1440, training loss: 0.00803785864263773 = 0.0010558991925790906 + 0.001 * 6.981959342956543
Epoch 1440, val loss: 1.4076615571975708
Epoch 1450, training loss: 0.00800175592303276 = 0.0010446326341480017 + 0.001 * 6.957123279571533
Epoch 1450, val loss: 1.4099451303482056
Epoch 1460, training loss: 0.0079894308000803 = 0.0010336794657632709 + 0.001 * 6.955750942230225
Epoch 1460, val loss: 1.412153720855713
Epoch 1470, training loss: 0.007990814745426178 = 0.0010230669286102057 + 0.001 * 6.967747211456299
Epoch 1470, val loss: 1.4143677949905396
Epoch 1480, training loss: 0.008006461896002293 = 0.001012735883705318 + 0.001 * 6.9937262535095215
Epoch 1480, val loss: 1.4165422916412354
Epoch 1490, training loss: 0.008000091649591923 = 0.001002721837721765 + 0.001 * 6.997369289398193
Epoch 1490, val loss: 1.4186993837356567
Epoch 1500, training loss: 0.007949675433337688 = 0.0009929490042850375 + 0.001 * 6.95672607421875
Epoch 1500, val loss: 1.4208106994628906
Epoch 1510, training loss: 0.007942193187773228 = 0.0009834746597334743 + 0.001 * 6.958718299865723
Epoch 1510, val loss: 1.4229367971420288
Epoch 1520, training loss: 0.007973316125571728 = 0.0009742418769747019 + 0.001 * 6.9990739822387695
Epoch 1520, val loss: 1.425036072731018
Epoch 1530, training loss: 0.00797381903976202 = 0.000965276500210166 + 0.001 * 7.008542060852051
Epoch 1530, val loss: 1.4270999431610107
Epoch 1540, training loss: 0.007892766036093235 = 0.0009565447107888758 + 0.001 * 6.936221122741699
Epoch 1540, val loss: 1.4290907382965088
Epoch 1550, training loss: 0.007901960983872414 = 0.0009480565204285085 + 0.001 * 6.953904628753662
Epoch 1550, val loss: 1.4311130046844482
Epoch 1560, training loss: 0.007909247651696205 = 0.000939783698413521 + 0.001 * 6.96946382522583
Epoch 1560, val loss: 1.4330683946609497
Epoch 1570, training loss: 0.007905415259301662 = 0.0009317296789959073 + 0.001 * 6.973684787750244
Epoch 1570, val loss: 1.4350451231002808
Epoch 1580, training loss: 0.007933300919830799 = 0.0009238933562301099 + 0.001 * 7.009407043457031
Epoch 1580, val loss: 1.436965823173523
Epoch 1590, training loss: 0.007883548736572266 = 0.000916295510251075 + 0.001 * 6.967252731323242
Epoch 1590, val loss: 1.4388046264648438
Epoch 1600, training loss: 0.007854673080146313 = 0.0009088944061659276 + 0.001 * 6.9457783699035645
Epoch 1600, val loss: 1.4406955242156982
Epoch 1610, training loss: 0.007826674729585648 = 0.0009017128613777459 + 0.001 * 6.924961566925049
Epoch 1610, val loss: 1.4425147771835327
Epoch 1620, training loss: 0.007861953228712082 = 0.0008947224705480039 + 0.001 * 6.967230796813965
Epoch 1620, val loss: 1.4443897008895874
Epoch 1630, training loss: 0.007848715409636497 = 0.0008879065862856805 + 0.001 * 6.960808277130127
Epoch 1630, val loss: 1.446158528327942
Epoch 1640, training loss: 0.007824612781405449 = 0.0008812667219899595 + 0.001 * 6.94334602355957
Epoch 1640, val loss: 1.4479182958602905
Epoch 1650, training loss: 0.007816369645297527 = 0.0008747952524572611 + 0.001 * 6.941573619842529
Epoch 1650, val loss: 1.449629306793213
Epoch 1660, training loss: 0.0078060016967356205 = 0.0008684955537319183 + 0.001 * 6.937505722045898
Epoch 1660, val loss: 1.4513866901397705
Epoch 1670, training loss: 0.007808381225913763 = 0.0008623372996225953 + 0.001 * 6.946043491363525
Epoch 1670, val loss: 1.4531208276748657
Epoch 1680, training loss: 0.007914640009403229 = 0.000856324506457895 + 0.001 * 7.058315277099609
Epoch 1680, val loss: 1.4548404216766357
Epoch 1690, training loss: 0.007764829322695732 = 0.0008504824945703149 + 0.001 * 6.914346694946289
Epoch 1690, val loss: 1.4564794301986694
Epoch 1700, training loss: 0.007768912240862846 = 0.0008447925793007016 + 0.001 * 6.924119472503662
Epoch 1700, val loss: 1.4581477642059326
Epoch 1710, training loss: 0.007765553891658783 = 0.0008392834570258856 + 0.001 * 6.926270008087158
Epoch 1710, val loss: 1.459783673286438
Epoch 1720, training loss: 0.007746904157102108 = 0.0008338841143995523 + 0.001 * 6.91301965713501
Epoch 1720, val loss: 1.4613471031188965
Epoch 1730, training loss: 0.007745212409645319 = 0.0008286306401714683 + 0.001 * 6.916581153869629
Epoch 1730, val loss: 1.4629507064819336
Epoch 1740, training loss: 0.0077942912466824055 = 0.0008234960259869695 + 0.001 * 6.970795154571533
Epoch 1740, val loss: 1.4645298719406128
Epoch 1750, training loss: 0.0077480836771428585 = 0.0008184631005860865 + 0.001 * 6.929620265960693
Epoch 1750, val loss: 1.4660700559616089
Epoch 1760, training loss: 0.007704597432166338 = 0.0008135423413477838 + 0.001 * 6.891054630279541
Epoch 1760, val loss: 1.4676095247268677
Epoch 1770, training loss: 0.007720589637756348 = 0.0008087365422397852 + 0.001 * 6.9118523597717285
Epoch 1770, val loss: 1.4691957235336304
Epoch 1780, training loss: 0.0077135199680924416 = 0.0008040373795665801 + 0.001 * 6.909482479095459
Epoch 1780, val loss: 1.4706670045852661
Epoch 1790, training loss: 0.007725869305431843 = 0.0007994258194230497 + 0.001 * 6.926443099975586
Epoch 1790, val loss: 1.472213864326477
Epoch 1800, training loss: 0.007700218819081783 = 0.0007949209539219737 + 0.001 * 6.90529727935791
Epoch 1800, val loss: 1.4737024307250977
Epoch 1810, training loss: 0.00783558376133442 = 0.0007905113161541522 + 0.001 * 7.045071601867676
Epoch 1810, val loss: 1.4751638174057007
Epoch 1820, training loss: 0.007710283622145653 = 0.0007862015045247972 + 0.001 * 6.924081802368164
Epoch 1820, val loss: 1.476596713066101
Epoch 1830, training loss: 0.007691896986216307 = 0.0007819858728908002 + 0.001 * 6.909910678863525
Epoch 1830, val loss: 1.4780386686325073
Epoch 1840, training loss: 0.007665995974093676 = 0.0007778270519338548 + 0.001 * 6.8881683349609375
Epoch 1840, val loss: 1.4795191287994385
Epoch 1850, training loss: 0.0076674483716487885 = 0.0007737746345810592 + 0.001 * 6.893673419952393
Epoch 1850, val loss: 1.4809249639511108
Epoch 1860, training loss: 0.007667133584618568 = 0.0007697763503529131 + 0.001 * 6.897356986999512
Epoch 1860, val loss: 1.4823994636535645
Epoch 1870, training loss: 0.007651232182979584 = 0.0007658671238459647 + 0.001 * 6.885365009307861
Epoch 1870, val loss: 1.4838314056396484
Epoch 1880, training loss: 0.00767087284475565 = 0.0007620232063345611 + 0.001 * 6.908849716186523
Epoch 1880, val loss: 1.4852019548416138
Epoch 1890, training loss: 0.007655327208340168 = 0.0007582750404253602 + 0.001 * 6.897051811218262
Epoch 1890, val loss: 1.4866185188293457
Epoch 1900, training loss: 0.007626320235431194 = 0.0007545812986791134 + 0.001 * 6.871738433837891
Epoch 1900, val loss: 1.4879603385925293
Epoch 1910, training loss: 0.007653273642063141 = 0.0007509870338253677 + 0.001 * 6.902286529541016
Epoch 1910, val loss: 1.4893721342086792
Epoch 1920, training loss: 0.0076599931344389915 = 0.0007474696612916887 + 0.001 * 6.91252326965332
Epoch 1920, val loss: 1.4907844066619873
Epoch 1930, training loss: 0.007650493178516626 = 0.000744019343983382 + 0.001 * 6.906473636627197
Epoch 1930, val loss: 1.492079496383667
Epoch 1940, training loss: 0.007647888269275427 = 0.0007406327640637755 + 0.001 * 6.907255172729492
Epoch 1940, val loss: 1.4934371709823608
Epoch 1950, training loss: 0.007618024945259094 = 0.0007373197004199028 + 0.001 * 6.880704879760742
Epoch 1950, val loss: 1.4947446584701538
Epoch 1960, training loss: 0.007617206312716007 = 0.0007340693846344948 + 0.001 * 6.883136749267578
Epoch 1960, val loss: 1.496078372001648
Epoch 1970, training loss: 0.007599529344588518 = 0.0007308704080060124 + 0.001 * 6.868658542633057
Epoch 1970, val loss: 1.4973688125610352
Epoch 1980, training loss: 0.007602442987263203 = 0.0007277409313246608 + 0.001 * 6.874701499938965
Epoch 1980, val loss: 1.4987143278121948
Epoch 1990, training loss: 0.007584606762975454 = 0.0007246923632919788 + 0.001 * 6.859914302825928
Epoch 1990, val loss: 1.499989628791809
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8139167105956774
The final CL Acc:0.75309, 0.02270, The final GNN Acc:0.81620, 0.00163
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13118])
remove edge: torch.Size([2, 7966])
updated graph: torch.Size([2, 10528])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.9643545150756836 = 1.955757737159729 + 0.001 * 8.596834182739258
Epoch 0, val loss: 1.9504339694976807
Epoch 10, training loss: 1.9534045457839966 = 1.944807767868042 + 0.001 * 8.596755981445312
Epoch 10, val loss: 1.9399974346160889
Epoch 20, training loss: 1.9399564266204834 = 1.931359887123108 + 0.001 * 8.59652328491211
Epoch 20, val loss: 1.9266160726547241
Epoch 30, training loss: 1.9214237928390503 = 1.9128278493881226 + 0.001 * 8.595951080322266
Epoch 30, val loss: 1.9076875448226929
Epoch 40, training loss: 1.8946064710617065 = 1.886012077331543 + 0.001 * 8.594390869140625
Epoch 40, val loss: 1.8801894187927246
Epoch 50, training loss: 1.8577498197555542 = 1.8491605520248413 + 0.001 * 8.589323997497559
Epoch 50, val loss: 1.8436681032180786
Epoch 60, training loss: 1.8161767721176147 = 1.807608723640442 + 0.001 * 8.568074226379395
Epoch 60, val loss: 1.8068451881408691
Epoch 70, training loss: 1.7789956331253052 = 1.7705386877059937 + 0.001 * 8.456896781921387
Epoch 70, val loss: 1.778369426727295
Epoch 80, training loss: 1.7345329523086548 = 1.7264833450317383 + 0.001 * 8.049622535705566
Epoch 80, val loss: 1.7414565086364746
Epoch 90, training loss: 1.673645257949829 = 1.6657496690750122 + 0.001 * 7.89561128616333
Epoch 90, val loss: 1.6870282888412476
Epoch 100, training loss: 1.5916495323181152 = 1.5839108228683472 + 0.001 * 7.738712310791016
Epoch 100, val loss: 1.614783525466919
Epoch 110, training loss: 1.4954721927642822 = 1.4878840446472168 + 0.001 * 7.588127136230469
Epoch 110, val loss: 1.5341747999191284
Epoch 120, training loss: 1.3983464241027832 = 1.3908017873764038 + 0.001 * 7.544676303863525
Epoch 120, val loss: 1.4553534984588623
Epoch 130, training loss: 1.3046681880950928 = 1.2971282005310059 + 0.001 * 7.540036201477051
Epoch 130, val loss: 1.3815457820892334
Epoch 140, training loss: 1.2114112377166748 = 1.203896164894104 + 0.001 * 7.515079021453857
Epoch 140, val loss: 1.307435393333435
Epoch 150, training loss: 1.118208885192871 = 1.1107248067855835 + 0.001 * 7.484130859375
Epoch 150, val loss: 1.2338091135025024
Epoch 160, training loss: 1.0272607803344727 = 1.0198285579681396 + 0.001 * 7.432265281677246
Epoch 160, val loss: 1.1635034084320068
Epoch 170, training loss: 0.9406194686889648 = 0.9332636594772339 + 0.001 * 7.355827808380127
Epoch 170, val loss: 1.0988513231277466
Epoch 180, training loss: 0.8585693836212158 = 0.8512842655181885 + 0.001 * 7.285101413726807
Epoch 180, val loss: 1.0392502546310425
Epoch 190, training loss: 0.7814714908599854 = 0.7742275595664978 + 0.001 * 7.24390983581543
Epoch 190, val loss: 0.9840342402458191
Epoch 200, training loss: 0.7102786898612976 = 0.7030619382858276 + 0.001 * 7.216750621795654
Epoch 200, val loss: 0.9338700175285339
Epoch 210, training loss: 0.6452215909957886 = 0.6380153894424438 + 0.001 * 7.206195831298828
Epoch 210, val loss: 0.8894824981689453
Epoch 220, training loss: 0.5854628682136536 = 0.5782631635665894 + 0.001 * 7.1996989250183105
Epoch 220, val loss: 0.8508432507514954
Epoch 230, training loss: 0.5300132036209106 = 0.5228205919265747 + 0.001 * 7.192605495452881
Epoch 230, val loss: 0.8184087872505188
Epoch 240, training loss: 0.4784194827079773 = 0.47123271226882935 + 0.001 * 7.186781406402588
Epoch 240, val loss: 0.7922781705856323
Epoch 250, training loss: 0.4305337071418762 = 0.4233573079109192 + 0.001 * 7.176393508911133
Epoch 250, val loss: 0.7721487879753113
Epoch 260, training loss: 0.38621968030929565 = 0.3790537714958191 + 0.001 * 7.1659159660339355
Epoch 260, val loss: 0.7572519779205322
Epoch 270, training loss: 0.34535089135169983 = 0.33819320797920227 + 0.001 * 7.157689571380615
Epoch 270, val loss: 0.7469749450683594
Epoch 280, training loss: 0.30787888169288635 = 0.30073288083076477 + 0.001 * 7.145993709564209
Epoch 280, val loss: 0.7409723997116089
Epoch 290, training loss: 0.2738666236400604 = 0.26672878861427307 + 0.001 * 7.137820720672607
Epoch 290, val loss: 0.7390632033348083
Epoch 300, training loss: 0.24335956573486328 = 0.23623168468475342 + 0.001 * 7.127875328063965
Epoch 300, val loss: 0.7410218119621277
Epoch 310, training loss: 0.21632049977779388 = 0.20920738577842712 + 0.001 * 7.1131086349487305
Epoch 310, val loss: 0.7464587092399597
Epoch 320, training loss: 0.19257694482803345 = 0.18546999990940094 + 0.001 * 7.106940746307373
Epoch 320, val loss: 0.7548922896385193
Epoch 330, training loss: 0.17180787026882172 = 0.16470804810523987 + 0.001 * 7.099820613861084
Epoch 330, val loss: 0.765718400478363
Epoch 340, training loss: 0.15364626049995422 = 0.14655667543411255 + 0.001 * 7.089582443237305
Epoch 340, val loss: 0.7784487009048462
Epoch 350, training loss: 0.1377469003200531 = 0.1306721419095993 + 0.001 * 7.074762344360352
Epoch 350, val loss: 0.79255211353302
Epoch 360, training loss: 0.12381616234779358 = 0.11674264818429947 + 0.001 * 7.073515892028809
Epoch 360, val loss: 0.8076794743537903
Epoch 370, training loss: 0.11156119406223297 = 0.1044984757900238 + 0.001 * 7.062714576721191
Epoch 370, val loss: 0.8235928416252136
Epoch 380, training loss: 0.1007687970995903 = 0.09370836615562439 + 0.001 * 7.060431480407715
Epoch 380, val loss: 0.840046763420105
Epoch 390, training loss: 0.09122966229915619 = 0.0841754600405693 + 0.001 * 7.054199695587158
Epoch 390, val loss: 0.8569226861000061
Epoch 400, training loss: 0.08278325200080872 = 0.07573653012514114 + 0.001 * 7.046717643737793
Epoch 400, val loss: 0.8740221858024597
Epoch 410, training loss: 0.07529700547456741 = 0.06825581192970276 + 0.001 * 7.04119348526001
Epoch 410, val loss: 0.8912376165390015
Epoch 420, training loss: 0.06865570694208145 = 0.06161782518029213 + 0.001 * 7.037883758544922
Epoch 420, val loss: 0.9085155129432678
Epoch 430, training loss: 0.06276819109916687 = 0.05572494491934776 + 0.001 * 7.0432448387146
Epoch 430, val loss: 0.9257197976112366
Epoch 440, training loss: 0.05752822011709213 = 0.05049106478691101 + 0.001 * 7.037156105041504
Epoch 440, val loss: 0.9428365230560303
Epoch 450, training loss: 0.05287375673651695 = 0.04584067687392235 + 0.001 * 7.033078193664551
Epoch 450, val loss: 0.9597530961036682
Epoch 460, training loss: 0.04874516278505325 = 0.041706934571266174 + 0.001 * 7.038228511810303
Epoch 460, val loss: 0.9764263033866882
Epoch 470, training loss: 0.04506776109337807 = 0.038030169904232025 + 0.001 * 7.03759241104126
Epoch 470, val loss: 0.9928348064422607
Epoch 480, training loss: 0.04178773984313011 = 0.034757182002067566 + 0.001 * 7.030559062957764
Epoch 480, val loss: 1.0089236497879028
Epoch 490, training loss: 0.038869813084602356 = 0.0318404920399189 + 0.001 * 7.029322624206543
Epoch 490, val loss: 1.0246978998184204
Epoch 500, training loss: 0.0362711101770401 = 0.029237767681479454 + 0.001 * 7.03334379196167
Epoch 500, val loss: 1.0401381254196167
Epoch 510, training loss: 0.033946774899959564 = 0.026911797001957893 + 0.001 * 7.034976482391357
Epoch 510, val loss: 1.055199146270752
Epoch 520, training loss: 0.03185834363102913 = 0.024830177426338196 + 0.001 * 7.028167247772217
Epoch 520, val loss: 1.069881796836853
Epoch 530, training loss: 0.029990356415510178 = 0.022963901981711388 + 0.001 * 7.026453495025635
Epoch 530, val loss: 1.0842036008834839
Epoch 540, training loss: 0.028312135487794876 = 0.02128760516643524 + 0.001 * 7.02453088760376
Epoch 540, val loss: 1.0981714725494385
Epoch 550, training loss: 0.02680184319615364 = 0.019778624176979065 + 0.001 * 7.023218154907227
Epoch 550, val loss: 1.1117348670959473
Epoch 560, training loss: 0.025438867509365082 = 0.01841741055250168 + 0.001 * 7.021457672119141
Epoch 560, val loss: 1.1249245405197144
Epoch 570, training loss: 0.024219375103712082 = 0.017187055200338364 + 0.001 * 7.03231954574585
Epoch 570, val loss: 1.1377670764923096
Epoch 580, training loss: 0.0230918787419796 = 0.016072627156972885 + 0.001 * 7.019251346588135
Epoch 580, val loss: 1.1502652168273926
Epoch 590, training loss: 0.02208526059985161 = 0.015060934238135815 + 0.001 * 7.024325847625732
Epoch 590, val loss: 1.1624277830123901
Epoch 600, training loss: 0.02115858905017376 = 0.014140511862933636 + 0.001 * 7.0180768966674805
Epoch 600, val loss: 1.174261450767517
Epoch 610, training loss: 0.02031709998846054 = 0.01330130361020565 + 0.001 * 7.015797138214111
Epoch 610, val loss: 1.1857720613479614
Epoch 620, training loss: 0.019574154168367386 = 0.012534549459815025 + 0.001 * 7.039604187011719
Epoch 620, val loss: 1.1969506740570068
Epoch 630, training loss: 0.018854409456253052 = 0.011832568794488907 + 0.001 * 7.021839618682861
Epoch 630, val loss: 1.2078166007995605
Epoch 640, training loss: 0.018208812922239304 = 0.01118854247033596 + 0.001 * 7.020270347595215
Epoch 640, val loss: 1.2183799743652344
Epoch 650, training loss: 0.017613086849451065 = 0.010596471838653088 + 0.001 * 7.0166144371032715
Epoch 650, val loss: 1.2286494970321655
Epoch 660, training loss: 0.01706276834011078 = 0.010051112622022629 + 0.001 * 7.011655807495117
Epoch 660, val loss: 1.2386525869369507
Epoch 670, training loss: 0.016555655747652054 = 0.009547693654894829 + 0.001 * 7.007960796356201
Epoch 670, val loss: 1.2484021186828613
Epoch 680, training loss: 0.016090668737888336 = 0.009081931784749031 + 0.001 * 7.008735656738281
Epoch 680, val loss: 1.2579014301300049
Epoch 690, training loss: 0.01565922610461712 = 0.00864998809993267 + 0.001 * 7.009238243103027
Epoch 690, val loss: 1.2671540975570679
Epoch 700, training loss: 0.01525186002254486 = 0.008248180150985718 + 0.001 * 7.003680229187012
Epoch 700, val loss: 1.27618408203125
Epoch 710, training loss: 0.014876814559102058 = 0.00787316169589758 + 0.001 * 7.003653049468994
Epoch 710, val loss: 1.2850265502929688
Epoch 720, training loss: 0.014527563005685806 = 0.007522054016590118 + 0.001 * 7.005508899688721
Epoch 720, val loss: 1.2936612367630005
Epoch 730, training loss: 0.014194086194038391 = 0.007192528806626797 + 0.001 * 7.001556396484375
Epoch 730, val loss: 1.3021299839019775
Epoch 740, training loss: 0.013888286426663399 = 0.006882674526423216 + 0.001 * 7.005610942840576
Epoch 740, val loss: 1.3104504346847534
Epoch 750, training loss: 0.013594923540949821 = 0.006591140292584896 + 0.001 * 7.003782272338867
Epoch 750, val loss: 1.3186278343200684
Epoch 760, training loss: 0.013317619450390339 = 0.006316626910120249 + 0.001 * 7.000992298126221
Epoch 760, val loss: 1.3266593217849731
Epoch 770, training loss: 0.013051887974143028 = 0.006058061961084604 + 0.001 * 6.993825912475586
Epoch 770, val loss: 1.3345413208007812
Epoch 780, training loss: 0.012824330478906631 = 0.005814408417791128 + 0.001 * 7.009921073913574
Epoch 780, val loss: 1.3422846794128418
Epoch 790, training loss: 0.01258070208132267 = 0.005584773141890764 + 0.001 * 6.99592924118042
Epoch 790, val loss: 1.3498785495758057
Epoch 800, training loss: 0.012360019609332085 = 0.005368302576243877 + 0.001 * 6.991717338562012
Epoch 800, val loss: 1.357336401939392
Epoch 810, training loss: 0.012160135433077812 = 0.005164202768355608 + 0.001 * 6.995932579040527
Epoch 810, val loss: 1.3646515607833862
Epoch 820, training loss: 0.011965997517108917 = 0.004971660673618317 + 0.001 * 6.9943366050720215
Epoch 820, val loss: 1.371824860572815
Epoch 830, training loss: 0.011776394210755825 = 0.0047898744232952595 + 0.001 * 6.9865193367004395
Epoch 830, val loss: 1.3788622617721558
Epoch 840, training loss: 0.011599971912801266 = 0.004618173930794001 + 0.001 * 6.981797695159912
Epoch 840, val loss: 1.3857570886611938
Epoch 850, training loss: 0.011437218636274338 = 0.004455905873328447 + 0.001 * 6.981312274932861
Epoch 850, val loss: 1.3925176858901978
Epoch 860, training loss: 0.011290578171610832 = 0.004302434157580137 + 0.001 * 6.9881439208984375
Epoch 860, val loss: 1.3991395235061646
Epoch 870, training loss: 0.011146324686706066 = 0.00415718462318182 + 0.001 * 6.989139556884766
Epoch 870, val loss: 1.4056276082992554
Epoch 880, training loss: 0.010997110046446323 = 0.0040196701884269714 + 0.001 * 6.9774394035339355
Epoch 880, val loss: 1.4119848012924194
Epoch 890, training loss: 0.01087351143360138 = 0.0038893527816981077 + 0.001 * 6.984158515930176
Epoch 890, val loss: 1.4182121753692627
Epoch 900, training loss: 0.010742105543613434 = 0.0037657616194337606 + 0.001 * 6.976343154907227
Epoch 900, val loss: 1.4243102073669434
Epoch 910, training loss: 0.010645396076142788 = 0.0036484571173787117 + 0.001 * 6.996938705444336
Epoch 910, val loss: 1.4302921295166016
Epoch 920, training loss: 0.010519309900701046 = 0.003537081414833665 + 0.001 * 6.9822282791137695
Epoch 920, val loss: 1.4361538887023926
Epoch 930, training loss: 0.010408016853034496 = 0.003431250574067235 + 0.001 * 6.9767656326293945
Epoch 930, val loss: 1.4418913125991821
Epoch 940, training loss: 0.010297518223524094 = 0.003330594627186656 + 0.001 * 6.966923713684082
Epoch 940, val loss: 1.4475125074386597
Epoch 950, training loss: 0.010200685821473598 = 0.0032347890082746744 + 0.001 * 6.9658966064453125
Epoch 950, val loss: 1.453025221824646
Epoch 960, training loss: 0.010121053084731102 = 0.0031435457058250904 + 0.001 * 6.9775071144104
Epoch 960, val loss: 1.4584238529205322
Epoch 970, training loss: 0.01002455223351717 = 0.003056648653000593 + 0.001 * 6.967903137207031
Epoch 970, val loss: 1.4637161493301392
Epoch 980, training loss: 0.009944010525941849 = 0.002973809139803052 + 0.001 * 6.970201015472412
Epoch 980, val loss: 1.4689010381698608
Epoch 990, training loss: 0.009861823171377182 = 0.002894769888371229 + 0.001 * 6.967052936553955
Epoch 990, val loss: 1.4739876985549927
Epoch 1000, training loss: 0.009781872853636742 = 0.0028192847967147827 + 0.001 * 6.962588310241699
Epoch 1000, val loss: 1.4789735078811646
Epoch 1010, training loss: 0.00972488522529602 = 0.0027471769135445356 + 0.001 * 6.977707862854004
Epoch 1010, val loss: 1.4838502407073975
Epoch 1020, training loss: 0.009633543901145458 = 0.0026782273780554533 + 0.001 * 6.955316066741943
Epoch 1020, val loss: 1.4886360168457031
Epoch 1030, training loss: 0.009569002315402031 = 0.002612275769934058 + 0.001 * 6.956726551055908
Epoch 1030, val loss: 1.493337869644165
Epoch 1040, training loss: 0.009507013484835625 = 0.002549140015617013 + 0.001 * 6.957873344421387
Epoch 1040, val loss: 1.4979453086853027
Epoch 1050, training loss: 0.009439840912818909 = 0.002488667145371437 + 0.001 * 6.951173305511475
Epoch 1050, val loss: 1.50246000289917
Epoch 1060, training loss: 0.009389680810272694 = 0.002430714201182127 + 0.001 * 6.958966255187988
Epoch 1060, val loss: 1.5068995952606201
Epoch 1070, training loss: 0.009328116662800312 = 0.0023751326370984316 + 0.001 * 6.952983379364014
Epoch 1070, val loss: 1.511253833770752
Epoch 1080, training loss: 0.009271032176911831 = 0.0023218102287501097 + 0.001 * 6.949221611022949
Epoch 1080, val loss: 1.5155248641967773
Epoch 1090, training loss: 0.00921604037284851 = 0.0022706170566380024 + 0.001 * 6.945422649383545
Epoch 1090, val loss: 1.5197175741195679
Epoch 1100, training loss: 0.009177826344966888 = 0.0022214404307305813 + 0.001 * 6.956386089324951
Epoch 1100, val loss: 1.5238436460494995
Epoch 1110, training loss: 0.00911746732890606 = 0.0021741653326898813 + 0.001 * 6.943301677703857
Epoch 1110, val loss: 1.5278897285461426
Epoch 1120, training loss: 0.009077288210391998 = 0.002128677209839225 + 0.001 * 6.948610782623291
Epoch 1120, val loss: 1.5318659543991089
Epoch 1130, training loss: 0.009045474231243134 = 0.0020848626736551523 + 0.001 * 6.960611343383789
Epoch 1130, val loss: 1.535763144493103
Epoch 1140, training loss: 0.008990423753857613 = 0.002042595762759447 + 0.001 * 6.9478278160095215
Epoch 1140, val loss: 1.539603590965271
Epoch 1150, training loss: 0.00894571840763092 = 0.002001803368330002 + 0.001 * 6.943915367126465
Epoch 1150, val loss: 1.543376088142395
Epoch 1160, training loss: 0.00890614464879036 = 0.001962357899174094 + 0.001 * 6.943786144256592
Epoch 1160, val loss: 1.5470978021621704
Epoch 1170, training loss: 0.008865414187312126 = 0.0019241677364334464 + 0.001 * 6.941246032714844
Epoch 1170, val loss: 1.5507687330245972
Epoch 1180, training loss: 0.00884953048080206 = 0.001887104008346796 + 0.001 * 6.96242618560791
Epoch 1180, val loss: 1.5544242858886719
Epoch 1190, training loss: 0.008780542761087418 = 0.0018511383095756173 + 0.001 * 6.929403781890869
Epoch 1190, val loss: 1.5580133199691772
Epoch 1200, training loss: 0.008753081783652306 = 0.0018161725020036101 + 0.001 * 6.936909198760986
Epoch 1200, val loss: 1.5615681409835815
Epoch 1210, training loss: 0.00871378555893898 = 0.0017821491928771138 + 0.001 * 6.931636333465576
Epoch 1210, val loss: 1.565093994140625
Epoch 1220, training loss: 0.008677830919623375 = 0.0017489830497652292 + 0.001 * 6.928847312927246
Epoch 1220, val loss: 1.5685954093933105
Epoch 1230, training loss: 0.008641707710921764 = 0.0017166098114103079 + 0.001 * 6.925097465515137
Epoch 1230, val loss: 1.572083592414856
Epoch 1240, training loss: 0.008610122837126255 = 0.001685012481175363 + 0.001 * 6.925110340118408
Epoch 1240, val loss: 1.5755597352981567
Epoch 1250, training loss: 0.008611657656729221 = 0.0016542051453143358 + 0.001 * 6.957452297210693
Epoch 1250, val loss: 1.5790128707885742
Epoch 1260, training loss: 0.008583024144172668 = 0.001624125288799405 + 0.001 * 6.958898544311523
Epoch 1260, val loss: 1.5824487209320068
Epoch 1270, training loss: 0.008538667112588882 = 0.0015947853680700064 + 0.001 * 6.943881511688232
Epoch 1270, val loss: 1.5858588218688965
Epoch 1280, training loss: 0.008499697782099247 = 0.0015661496436223388 + 0.001 * 6.933547496795654
Epoch 1280, val loss: 1.5892606973648071
Epoch 1290, training loss: 0.00847134180366993 = 0.001538267475552857 + 0.001 * 6.9330735206604
Epoch 1290, val loss: 1.5926337242126465
Epoch 1300, training loss: 0.00843245629221201 = 0.0015110670356079936 + 0.001 * 6.921388626098633
Epoch 1300, val loss: 1.595991849899292
Epoch 1310, training loss: 0.008403646759688854 = 0.0014845941914245486 + 0.001 * 6.9190521240234375
Epoch 1310, val loss: 1.599316120147705
Epoch 1320, training loss: 0.008373748511075974 = 0.0014588135527446866 + 0.001 * 6.914934158325195
Epoch 1320, val loss: 1.6026197671890259
Epoch 1330, training loss: 0.008351394906640053 = 0.0014337047468870878 + 0.001 * 6.917690277099609
Epoch 1330, val loss: 1.6058803796768188
Epoch 1340, training loss: 0.008343355730175972 = 0.0014092832570895553 + 0.001 * 6.934072494506836
Epoch 1340, val loss: 1.6091084480285645
Epoch 1350, training loss: 0.008293588645756245 = 0.001385558512993157 + 0.001 * 6.908030033111572
Epoch 1350, val loss: 1.6122878789901733
Epoch 1360, training loss: 0.008305612951517105 = 0.0013624918647110462 + 0.001 * 6.94312047958374
Epoch 1360, val loss: 1.615431308746338
Epoch 1370, training loss: 0.008259586989879608 = 0.001340080052614212 + 0.001 * 6.919506072998047
Epoch 1370, val loss: 1.6185191869735718
Epoch 1380, training loss: 0.008227836340665817 = 0.0013183619594201446 + 0.001 * 6.9094743728637695
Epoch 1380, val loss: 1.6215713024139404
Epoch 1390, training loss: 0.00819602981209755 = 0.0012972706463187933 + 0.001 * 6.898758888244629
Epoch 1390, val loss: 1.6245869398117065
Epoch 1400, training loss: 0.008180460892617702 = 0.001276814378798008 + 0.001 * 6.903645992279053
Epoch 1400, val loss: 1.6275490522384644
Epoch 1410, training loss: 0.008182201534509659 = 0.0012569866375997663 + 0.001 * 6.9252142906188965
Epoch 1410, val loss: 1.630465030670166
Epoch 1420, training loss: 0.008170309476554394 = 0.0012377214152365923 + 0.001 * 6.93258810043335
Epoch 1420, val loss: 1.633341670036316
Epoch 1430, training loss: 0.008123443461954594 = 0.001218997873365879 + 0.001 * 6.904445171356201
Epoch 1430, val loss: 1.6361769437789917
Epoch 1440, training loss: 0.008095640689134598 = 0.0012008219491690397 + 0.001 * 6.894818305969238
Epoch 1440, val loss: 1.6389700174331665
Epoch 1450, training loss: 0.008074762299656868 = 0.0011832277523353696 + 0.001 * 6.891533851623535
Epoch 1450, val loss: 1.6417186260223389
Epoch 1460, training loss: 0.008068121038377285 = 0.0011661850148811936 + 0.001 * 6.901935577392578
Epoch 1460, val loss: 1.644407868385315
Epoch 1470, training loss: 0.008062190376222134 = 0.0011496623046696186 + 0.001 * 6.912527561187744
Epoch 1470, val loss: 1.647080421447754
Epoch 1480, training loss: 0.008051278069615364 = 0.001133615500293672 + 0.001 * 6.917662620544434
Epoch 1480, val loss: 1.6496946811676025
Epoch 1490, training loss: 0.008021943271160126 = 0.0011180943110957742 + 0.001 * 6.903848648071289
Epoch 1490, val loss: 1.652239441871643
Epoch 1500, training loss: 0.007985861040651798 = 0.0011030330788344145 + 0.001 * 6.882827281951904
Epoch 1500, val loss: 1.6547688245773315
Epoch 1510, training loss: 0.007990546524524689 = 0.0010884292423725128 + 0.001 * 6.9021172523498535
Epoch 1510, val loss: 1.6572355031967163
Epoch 1520, training loss: 0.008027498610317707 = 0.0010742454323917627 + 0.001 * 6.953253269195557
Epoch 1520, val loss: 1.65965735912323
Epoch 1530, training loss: 0.007967140525579453 = 0.0010605343850329518 + 0.001 * 6.906605243682861
Epoch 1530, val loss: 1.6620218753814697
Epoch 1540, training loss: 0.007931374944746494 = 0.0010472454596310854 + 0.001 * 6.884129524230957
Epoch 1540, val loss: 1.6643455028533936
Epoch 1550, training loss: 0.007924759760499 = 0.001034345361404121 + 0.001 * 6.890414237976074
Epoch 1550, val loss: 1.6666401624679565
Epoch 1560, training loss: 0.007901577278971672 = 0.0010218089446425438 + 0.001 * 6.879767417907715
Epoch 1560, val loss: 1.6688661575317383
Epoch 1570, training loss: 0.007898653857409954 = 0.0010096317855641246 + 0.001 * 6.889021396636963
Epoch 1570, val loss: 1.6710596084594727
Epoch 1580, training loss: 0.007897410541772842 = 0.0009978313464671373 + 0.001 * 6.899579048156738
Epoch 1580, val loss: 1.6732020378112793
Epoch 1590, training loss: 0.007870130240917206 = 0.0009863587329164147 + 0.001 * 6.8837714195251465
Epoch 1590, val loss: 1.675301194190979
Epoch 1600, training loss: 0.00785721093416214 = 0.0009752018377184868 + 0.001 * 6.882008075714111
Epoch 1600, val loss: 1.6773735284805298
Epoch 1610, training loss: 0.007843422703444958 = 0.0009643806843087077 + 0.001 * 6.87904167175293
Epoch 1610, val loss: 1.679383397102356
Epoch 1620, training loss: 0.0078566400334239 = 0.0009538637241348624 + 0.001 * 6.90277624130249
Epoch 1620, val loss: 1.6813594102859497
Epoch 1630, training loss: 0.007863922975957394 = 0.0009436232503503561 + 0.001 * 6.920299053192139
Epoch 1630, val loss: 1.68328857421875
Epoch 1640, training loss: 0.00783214345574379 = 0.0009336952934972942 + 0.001 * 6.898447513580322
Epoch 1640, val loss: 1.6851781606674194
Epoch 1650, training loss: 0.007805258966982365 = 0.0009240189683623612 + 0.001 * 6.881239414215088
Epoch 1650, val loss: 1.6870324611663818
Epoch 1660, training loss: 0.007775220554322004 = 0.0009146194206550717 + 0.001 * 6.86060094833374
Epoch 1660, val loss: 1.6888515949249268
Epoch 1670, training loss: 0.007767745293676853 = 0.0009054678957909346 + 0.001 * 6.862277030944824
Epoch 1670, val loss: 1.6906243562698364
Epoch 1680, training loss: 0.007771458011120558 = 0.0008965587476268411 + 0.001 * 6.874898910522461
Epoch 1680, val loss: 1.6923507452011108
Epoch 1690, training loss: 0.007754824124276638 = 0.0008878992521204054 + 0.001 * 6.86692476272583
Epoch 1690, val loss: 1.6940466165542603
Epoch 1700, training loss: 0.007738086394965649 = 0.0008794815512374043 + 0.001 * 6.858604431152344
Epoch 1700, val loss: 1.6957128047943115
Epoch 1710, training loss: 0.007729758508503437 = 0.0008712965645827353 + 0.001 * 6.858461856842041
Epoch 1710, val loss: 1.6973445415496826
Epoch 1720, training loss: 0.007784957066178322 = 0.0008633092511445284 + 0.001 * 6.921647071838379
Epoch 1720, val loss: 1.698948621749878
Epoch 1730, training loss: 0.007769029121845961 = 0.0008555753738619387 + 0.001 * 6.913453578948975
Epoch 1730, val loss: 1.700485110282898
Epoch 1740, training loss: 0.007726287469267845 = 0.000848012394271791 + 0.001 * 6.878274440765381
Epoch 1740, val loss: 1.7019996643066406
Epoch 1750, training loss: 0.00769832544028759 = 0.0008406940614804626 + 0.001 * 6.857630729675293
Epoch 1750, val loss: 1.7034780979156494
Epoch 1760, training loss: 0.007691860198974609 = 0.0008335437742061913 + 0.001 * 6.858315944671631
Epoch 1760, val loss: 1.704911470413208
Epoch 1770, training loss: 0.007678259629756212 = 0.0008265952346846461 + 0.001 * 6.851664066314697
Epoch 1770, val loss: 1.7062957286834717
Epoch 1780, training loss: 0.0076949517242610455 = 0.0008198357536457479 + 0.001 * 6.875115394592285
Epoch 1780, val loss: 1.7076761722564697
Epoch 1790, training loss: 0.00769452378153801 = 0.0008132123621180654 + 0.001 * 6.881311416625977
Epoch 1790, val loss: 1.7090237140655518
Epoch 1800, training loss: 0.007675420492887497 = 0.0008067904855124652 + 0.001 * 6.8686299324035645
Epoch 1800, val loss: 1.7103224992752075
Epoch 1810, training loss: 0.007642968092113733 = 0.0008004960254766047 + 0.001 * 6.842471599578857
Epoch 1810, val loss: 1.7116247415542603
Epoch 1820, training loss: 0.00764411361888051 = 0.000794374558608979 + 0.001 * 6.849739074707031
Epoch 1820, val loss: 1.7128652334213257
Epoch 1830, training loss: 0.007638579234480858 = 0.0007883876096457243 + 0.001 * 6.850191116333008
Epoch 1830, val loss: 1.7140696048736572
Epoch 1840, training loss: 0.007640293333679438 = 0.0007825700449757278 + 0.001 * 6.857723236083984
Epoch 1840, val loss: 1.7152535915374756
Epoch 1850, training loss: 0.00763468025252223 = 0.00077689258614555 + 0.001 * 6.857787609100342
Epoch 1850, val loss: 1.7163982391357422
Epoch 1860, training loss: 0.007630782667547464 = 0.0007713469094596803 + 0.001 * 6.859435558319092
Epoch 1860, val loss: 1.7175012826919556
Epoch 1870, training loss: 0.007594152819365263 = 0.0007659489056095481 + 0.001 * 6.8282036781311035
Epoch 1870, val loss: 1.71859872341156
Epoch 1880, training loss: 0.007626661099493504 = 0.0007606643484905362 + 0.001 * 6.865996360778809
Epoch 1880, val loss: 1.7196532487869263
Epoch 1890, training loss: 0.00759522058069706 = 0.0007555113988928497 + 0.001 * 6.8397088050842285
Epoch 1890, val loss: 1.7206780910491943
Epoch 1900, training loss: 0.007617074530571699 = 0.0007505026878789067 + 0.001 * 6.866571426391602
Epoch 1900, val loss: 1.7216627597808838
Epoch 1910, training loss: 0.007635420188307762 = 0.0007455938030034304 + 0.001 * 6.889825820922852
Epoch 1910, val loss: 1.7226485013961792
Epoch 1920, training loss: 0.007603479083627462 = 0.0007408392266370356 + 0.001 * 6.862639427185059
Epoch 1920, val loss: 1.723578929901123
Epoch 1930, training loss: 0.007575016003102064 = 0.0007361550233326852 + 0.001 * 6.838860511779785
Epoch 1930, val loss: 1.7245104312896729
Epoch 1940, training loss: 0.007573879323899746 = 0.000731597887352109 + 0.001 * 6.842281341552734
Epoch 1940, val loss: 1.7253834009170532
Epoch 1950, training loss: 0.007561102043837309 = 0.0007271271315403283 + 0.001 * 6.833974361419678
Epoch 1950, val loss: 1.726250171661377
Epoch 1960, training loss: 0.007548851426690817 = 0.0007227745954878628 + 0.001 * 6.826076507568359
Epoch 1960, val loss: 1.7270954847335815
Epoch 1970, training loss: 0.007546720560640097 = 0.0007184991845861077 + 0.001 * 6.828221321105957
Epoch 1970, val loss: 1.7279138565063477
Epoch 1980, training loss: 0.007572546601295471 = 0.0007143138791434467 + 0.001 * 6.858232498168945
Epoch 1980, val loss: 1.728740930557251
Epoch 1990, training loss: 0.007539689540863037 = 0.0007102160016074777 + 0.001 * 6.829473495483398
Epoch 1990, val loss: 1.7295007705688477
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8313125988402742
=== training gcn model ===
Epoch 0, training loss: 1.9475672245025635 = 1.9389704465866089 + 0.001 * 8.596806526184082
Epoch 0, val loss: 1.9339110851287842
Epoch 10, training loss: 1.9375470876693726 = 1.928950309753418 + 0.001 * 8.59673023223877
Epoch 10, val loss: 1.9234477281570435
Epoch 20, training loss: 1.92489755153656 = 1.9163010120391846 + 0.001 * 8.5964994430542
Epoch 20, val loss: 1.9102838039398193
Epoch 30, training loss: 1.9068206548690796 = 1.8982247114181519 + 0.001 * 8.595932960510254
Epoch 30, val loss: 1.8916873931884766
Epoch 40, training loss: 1.8802298307418823 = 1.8716354370117188 + 0.001 * 8.594417572021484
Epoch 40, val loss: 1.8649650812149048
Epoch 50, training loss: 1.8442453145980835 = 1.835655689239502 + 0.001 * 8.589593887329102
Epoch 50, val loss: 1.8311328887939453
Epoch 60, training loss: 1.8061516284942627 = 1.7975811958312988 + 0.001 * 8.570395469665527
Epoch 60, val loss: 1.8000917434692383
Epoch 70, training loss: 1.7726380825042725 = 1.7641706466674805 + 0.001 * 8.46743106842041
Epoch 70, val loss: 1.774767279624939
Epoch 80, training loss: 1.7277191877365112 = 1.7196440696716309 + 0.001 * 8.075175285339355
Epoch 80, val loss: 1.7351086139678955
Epoch 90, training loss: 1.6649682521820068 = 1.6569788455963135 + 0.001 * 7.989434242248535
Epoch 90, val loss: 1.679489254951477
Epoch 100, training loss: 1.5815653800964355 = 1.5736379623413086 + 0.001 * 7.927469730377197
Epoch 100, val loss: 1.6083338260650635
Epoch 110, training loss: 1.4851361513137817 = 1.4773054122924805 + 0.001 * 7.830770492553711
Epoch 110, val loss: 1.5265564918518066
Epoch 120, training loss: 1.386013388633728 = 1.3784006834030151 + 0.001 * 7.612731456756592
Epoch 120, val loss: 1.444594144821167
Epoch 130, training loss: 1.2881717681884766 = 1.2806862592697144 + 0.001 * 7.485544681549072
Epoch 130, val loss: 1.3653932809829712
Epoch 140, training loss: 1.1904982328414917 = 1.1830334663391113 + 0.001 * 7.464772701263428
Epoch 140, val loss: 1.2880972623825073
Epoch 150, training loss: 1.0924186706542969 = 1.084999918937683 + 0.001 * 7.418728828430176
Epoch 150, val loss: 1.2117143869400024
Epoch 160, training loss: 0.9954111576080322 = 0.9880449771881104 + 0.001 * 7.366156101226807
Epoch 160, val loss: 1.1370517015457153
Epoch 170, training loss: 0.9030037522315979 = 0.8956863880157471 + 0.001 * 7.317348003387451
Epoch 170, val loss: 1.0662047863006592
Epoch 180, training loss: 0.8196054100990295 = 0.8123207092285156 + 0.001 * 7.284693717956543
Epoch 180, val loss: 1.0028347969055176
Epoch 190, training loss: 0.7479226589202881 = 0.7406708002090454 + 0.001 * 7.251852035522461
Epoch 190, val loss: 0.9497272372245789
Epoch 200, training loss: 0.6876500248908997 = 0.6804383993148804 + 0.001 * 7.211628437042236
Epoch 200, val loss: 0.9073446393013
Epoch 210, training loss: 0.635793924331665 = 0.6286154389381409 + 0.001 * 7.178488731384277
Epoch 210, val loss: 0.8734197616577148
Epoch 220, training loss: 0.5884625315666199 = 0.5812945365905762 + 0.001 * 7.167967796325684
Epoch 220, val loss: 0.844738781452179
Epoch 230, training loss: 0.5427747368812561 = 0.5356147289276123 + 0.001 * 7.159996509552002
Epoch 230, val loss: 0.8189533948898315
Epoch 240, training loss: 0.4975080192089081 = 0.49035120010375977 + 0.001 * 7.156826972961426
Epoch 240, val loss: 0.7952783703804016
Epoch 250, training loss: 0.45281508564949036 = 0.44565916061401367 + 0.001 * 7.155933856964111
Epoch 250, val loss: 0.7741076946258545
Epoch 260, training loss: 0.40968403220176697 = 0.4025292992591858 + 0.001 * 7.1547346115112305
Epoch 260, val loss: 0.7563902735710144
Epoch 270, training loss: 0.36920613050460815 = 0.36205288767814636 + 0.001 * 7.153235912322998
Epoch 270, val loss: 0.7429634928703308
Epoch 280, training loss: 0.3321710228919983 = 0.32501929998397827 + 0.001 * 7.151719093322754
Epoch 280, val loss: 0.7338526844978333
Epoch 290, training loss: 0.2987494170665741 = 0.2915988862514496 + 0.001 * 7.150537490844727
Epoch 290, val loss: 0.7284940481185913
Epoch 300, training loss: 0.26851603388786316 = 0.26136621832847595 + 0.001 * 7.149820804595947
Epoch 300, val loss: 0.7261152267456055
Epoch 310, training loss: 0.24080392718315125 = 0.23365440964698792 + 0.001 * 7.1495184898376465
Epoch 310, val loss: 0.7257410287857056
Epoch 320, training loss: 0.2150696963071823 = 0.20792023837566376 + 0.001 * 7.1494526863098145
Epoch 320, val loss: 0.726681113243103
Epoch 330, training loss: 0.19107849895954132 = 0.18392905592918396 + 0.001 * 7.149448394775391
Epoch 330, val loss: 0.7284788489341736
Epoch 340, training loss: 0.16889531910419464 = 0.16174568235874176 + 0.001 * 7.149637699127197
Epoch 340, val loss: 0.7310597896575928
Epoch 350, training loss: 0.14876185357570648 = 0.14161251485347748 + 0.001 * 7.149335861206055
Epoch 350, val loss: 0.7345188856124878
Epoch 360, training loss: 0.13088682293891907 = 0.12373752892017365 + 0.001 * 7.149295806884766
Epoch 360, val loss: 0.7389978766441345
Epoch 370, training loss: 0.1153002381324768 = 0.1081516221165657 + 0.001 * 7.148612022399902
Epoch 370, val loss: 0.7445040941238403
Epoch 380, training loss: 0.10187109559774399 = 0.09472297132015228 + 0.001 * 7.148125648498535
Epoch 380, val loss: 0.7510121464729309
Epoch 390, training loss: 0.09037072211503983 = 0.08322303742170334 + 0.001 * 7.147681713104248
Epoch 390, val loss: 0.758385956287384
Epoch 400, training loss: 0.08053852617740631 = 0.07339130342006683 + 0.001 * 7.147221565246582
Epoch 400, val loss: 0.7664004564285278
Epoch 410, training loss: 0.072120301425457 = 0.0649750605225563 + 0.001 * 7.1452436447143555
Epoch 410, val loss: 0.7748607397079468
Epoch 420, training loss: 0.06489450484514236 = 0.05775035545229912 + 0.001 * 7.144145965576172
Epoch 420, val loss: 0.7835761308670044
Epoch 430, training loss: 0.058674585074186325 = 0.051530871540308 + 0.001 * 7.143711566925049
Epoch 430, val loss: 0.792396068572998
Epoch 440, training loss: 0.053301408886909485 = 0.04615890234708786 + 0.001 * 7.1425042152404785
Epoch 440, val loss: 0.8012163639068604
Epoch 450, training loss: 0.04864276200532913 = 0.04150259867310524 + 0.001 * 7.140161991119385
Epoch 450, val loss: 0.8099740743637085
Epoch 460, training loss: 0.04459559917449951 = 0.03745412826538086 + 0.001 * 7.141468524932861
Epoch 460, val loss: 0.8185924291610718
Epoch 470, training loss: 0.041061289608478546 = 0.03392365202307701 + 0.001 * 7.137636661529541
Epoch 470, val loss: 0.827070415019989
Epoch 480, training loss: 0.037969693541526794 = 0.030835449695587158 + 0.001 * 7.1342453956604
Epoch 480, val loss: 0.8353702425956726
Epoch 490, training loss: 0.03525706008076668 = 0.028125349432229996 + 0.001 * 7.131710529327393
Epoch 490, val loss: 0.8434619307518005
Epoch 500, training loss: 0.03287295997142792 = 0.025739042088389397 + 0.001 * 7.1339192390441895
Epoch 500, val loss: 0.8513625860214233
Epoch 510, training loss: 0.03076179325580597 = 0.023631565272808075 + 0.001 * 7.130227565765381
Epoch 510, val loss: 0.8590792417526245
Epoch 520, training loss: 0.02889082580804825 = 0.021764585748314857 + 0.001 * 7.126240253448486
Epoch 520, val loss: 0.8665856122970581
Epoch 530, training loss: 0.027224687859416008 = 0.02010459639132023 + 0.001 * 7.120090961456299
Epoch 530, val loss: 0.8739121556282043
Epoch 540, training loss: 0.025743193924427032 = 0.018624264746904373 + 0.001 * 7.1189284324646
Epoch 540, val loss: 0.8810622096061707
Epoch 550, training loss: 0.024415800347924232 = 0.017299752682447433 + 0.001 * 7.1160478591918945
Epoch 550, val loss: 0.8880178332328796
Epoch 560, training loss: 0.02321845106780529 = 0.016111044213175774 + 0.001 * 7.1074066162109375
Epoch 560, val loss: 0.8947827219963074
Epoch 570, training loss: 0.022144543007016182 = 0.015040935017168522 + 0.001 * 7.103607177734375
Epoch 570, val loss: 0.9013553261756897
Epoch 580, training loss: 0.021184347569942474 = 0.014074685983359814 + 0.001 * 7.109661102294922
Epoch 580, val loss: 0.9077432751655579
Epoch 590, training loss: 0.02030634507536888 = 0.013199849985539913 + 0.001 * 7.106494426727295
Epoch 590, val loss: 0.9139492511749268
Epoch 600, training loss: 0.019505495205521584 = 0.012405628338456154 + 0.001 * 7.099865913391113
Epoch 600, val loss: 0.9199822545051575
Epoch 610, training loss: 0.01877402700483799 = 0.011682669632136822 + 0.001 * 7.0913567543029785
Epoch 610, val loss: 0.9258539080619812
Epoch 620, training loss: 0.0181419774889946 = 0.01102286297827959 + 0.001 * 7.119114398956299
Epoch 620, val loss: 0.9315552115440369
Epoch 630, training loss: 0.017504505813121796 = 0.01041923463344574 + 0.001 * 7.085270404815674
Epoch 630, val loss: 0.9371066093444824
Epoch 640, training loss: 0.016946416348218918 = 0.009865712374448776 + 0.001 * 7.080704212188721
Epoch 640, val loss: 0.9424991607666016
Epoch 650, training loss: 0.016433047130703926 = 0.009356988593935966 + 0.001 * 7.0760579109191895
Epoch 650, val loss: 0.9477462768554688
Epoch 660, training loss: 0.015952974557876587 = 0.00888839177787304 + 0.001 * 7.0645833015441895
Epoch 660, val loss: 0.952850878238678
Epoch 670, training loss: 0.015509214252233505 = 0.00845541711896658 + 0.001 * 7.053797245025635
Epoch 670, val loss: 0.9578264951705933
Epoch 680, training loss: 0.015114663168787956 = 0.008052725344896317 + 0.001 * 7.06193733215332
Epoch 680, val loss: 0.962654709815979
Epoch 690, training loss: 0.014736490324139595 = 0.007673350628465414 + 0.001 * 7.063138961791992
Epoch 690, val loss: 0.967379629611969
Epoch 700, training loss: 0.014362737536430359 = 0.007311847992241383 + 0.001 * 7.050889492034912
Epoch 700, val loss: 0.9720273613929749
Epoch 710, training loss: 0.01401616632938385 = 0.006965999957174063 + 0.001 * 7.05016565322876
Epoch 710, val loss: 0.9766086339950562
Epoch 720, training loss: 0.013696620240807533 = 0.006635922007262707 + 0.001 * 7.06069803237915
Epoch 720, val loss: 0.9811276793479919
Epoch 730, training loss: 0.013388090766966343 = 0.006322409026324749 + 0.001 * 7.065681457519531
Epoch 730, val loss: 0.9855824708938599
Epoch 740, training loss: 0.01305641420185566 = 0.006026026792824268 + 0.001 * 7.0303874015808105
Epoch 740, val loss: 0.9899762272834778
Epoch 750, training loss: 0.012789273634552956 = 0.005746764596551657 + 0.001 * 7.042508602142334
Epoch 750, val loss: 0.9943199753761292
Epoch 760, training loss: 0.012535725720226765 = 0.0054843430407345295 + 0.001 * 7.051382541656494
Epoch 760, val loss: 0.9985848665237427
Epoch 770, training loss: 0.012265574187040329 = 0.005238307174295187 + 0.001 * 7.027266979217529
Epoch 770, val loss: 1.0027800798416138
Epoch 780, training loss: 0.012047137133777142 = 0.005007803440093994 + 0.001 * 7.039333343505859
Epoch 780, val loss: 1.0068914890289307
Epoch 790, training loss: 0.011811809614300728 = 0.004791943822056055 + 0.001 * 7.019865036010742
Epoch 790, val loss: 1.0109381675720215
Epoch 800, training loss: 0.011599617078900337 = 0.004589837044477463 + 0.001 * 7.009779930114746
Epoch 800, val loss: 1.0149099826812744
Epoch 810, training loss: 0.01141069084405899 = 0.004400579258799553 + 0.001 * 7.010110855102539
Epoch 810, val loss: 1.0188043117523193
Epoch 820, training loss: 0.011239162646234035 = 0.004223255906254053 + 0.001 * 7.01590633392334
Epoch 820, val loss: 1.0226244926452637
Epoch 830, training loss: 0.011095745489001274 = 0.00405702693387866 + 0.001 * 7.038717746734619
Epoch 830, val loss: 1.0263736248016357
Epoch 840, training loss: 0.010922831483185291 = 0.0039010951295495033 + 0.001 * 7.021736145019531
Epoch 840, val loss: 1.0300471782684326
Epoch 850, training loss: 0.010764277540147305 = 0.003754681907594204 + 0.001 * 7.0095953941345215
Epoch 850, val loss: 1.0336363315582275
Epoch 860, training loss: 0.010640022344887257 = 0.003617087146267295 + 0.001 * 7.022934436798096
Epoch 860, val loss: 1.037164568901062
Epoch 870, training loss: 0.010495668277144432 = 0.003487653099000454 + 0.001 * 7.00801420211792
Epoch 870, val loss: 1.040608286857605
Epoch 880, training loss: 0.010370966047048569 = 0.0033658065367490053 + 0.001 * 7.0051589012146
Epoch 880, val loss: 1.0439826250076294
Epoch 890, training loss: 0.010262705385684967 = 0.003250973066315055 + 0.001 * 7.01173210144043
Epoch 890, val loss: 1.0472841262817383
Epoch 900, training loss: 0.01014665700495243 = 0.0031426772475242615 + 0.001 * 7.003979682922363
Epoch 900, val loss: 1.050507664680481
Epoch 910, training loss: 0.010025468654930592 = 0.003040441544726491 + 0.001 * 6.985026836395264
Epoch 910, val loss: 1.053663969039917
Epoch 920, training loss: 0.009923446923494339 = 0.002943821484223008 + 0.001 * 6.9796247482299805
Epoch 920, val loss: 1.0567623376846313
Epoch 930, training loss: 0.009838391095399857 = 0.0028524231165647507 + 0.001 * 6.98596715927124
Epoch 930, val loss: 1.0597902536392212
Epoch 940, training loss: 0.009738849475979805 = 0.002765899058431387 + 0.001 * 6.972950458526611
Epoch 940, val loss: 1.062755823135376
Epoch 950, training loss: 0.009674390777945518 = 0.002683907514438033 + 0.001 * 6.990483283996582
Epoch 950, val loss: 1.0656522512435913
Epoch 960, training loss: 0.009587661363184452 = 0.0026061434764415026 + 0.001 * 6.981517314910889
Epoch 960, val loss: 1.0684882402420044
Epoch 970, training loss: 0.009502158500254154 = 0.002532323356717825 + 0.001 * 6.969834804534912
Epoch 970, val loss: 1.0712484121322632
Epoch 980, training loss: 0.009439878165721893 = 0.002462175441905856 + 0.001 * 6.977702617645264
Epoch 980, val loss: 1.0739610195159912
Epoch 990, training loss: 0.009378223679959774 = 0.0023954559583216906 + 0.001 * 6.982767105102539
Epoch 990, val loss: 1.0766135454177856
Epoch 1000, training loss: 0.00929869432002306 = 0.002331971190869808 + 0.001 * 6.9667229652404785
Epoch 1000, val loss: 1.079206109046936
Epoch 1010, training loss: 0.009243895299732685 = 0.002271501114591956 + 0.001 * 6.97239351272583
Epoch 1010, val loss: 1.0817509889602661
Epoch 1020, training loss: 0.009182857349514961 = 0.0022138776257634163 + 0.001 * 6.968979358673096
Epoch 1020, val loss: 1.0842318534851074
Epoch 1030, training loss: 0.009111001156270504 = 0.002158907474949956 + 0.001 * 6.952093601226807
Epoch 1030, val loss: 1.0866601467132568
Epoch 1040, training loss: 0.009083977900445461 = 0.002106432570144534 + 0.001 * 6.977544784545898
Epoch 1040, val loss: 1.0890454053878784
Epoch 1050, training loss: 0.009043723344802856 = 0.002056333003565669 + 0.001 * 6.98738956451416
Epoch 1050, val loss: 1.0913724899291992
Epoch 1060, training loss: 0.008985933847725391 = 0.002008437644690275 + 0.001 * 6.9774956703186035
Epoch 1060, val loss: 1.0936530828475952
Epoch 1070, training loss: 0.00895233266055584 = 0.0019626333378255367 + 0.001 * 6.989698886871338
Epoch 1070, val loss: 1.0958788394927979
Epoch 1080, training loss: 0.008892737329006195 = 0.0019187917932868004 + 0.001 * 6.973945617675781
Epoch 1080, val loss: 1.0980619192123413
Epoch 1090, training loss: 0.008820151910185814 = 0.0018768213922157884 + 0.001 * 6.94333028793335
Epoch 1090, val loss: 1.1001906394958496
Epoch 1100, training loss: 0.008787882514297962 = 0.001836597453802824 + 0.001 * 6.951284885406494
Epoch 1100, val loss: 1.1022766828536987
Epoch 1110, training loss: 0.008752805180847645 = 0.001798040815629065 + 0.001 * 6.954764366149902
Epoch 1110, val loss: 1.1043176651000977
Epoch 1120, training loss: 0.008726072497665882 = 0.001761063700541854 + 0.001 * 6.96500825881958
Epoch 1120, val loss: 1.1063038110733032
Epoch 1130, training loss: 0.008679811842739582 = 0.0017255544662475586 + 0.001 * 6.954257011413574
Epoch 1130, val loss: 1.1082487106323242
Epoch 1140, training loss: 0.00866676215082407 = 0.0016914467560127378 + 0.001 * 6.975314617156982
Epoch 1140, val loss: 1.1101627349853516
Epoch 1150, training loss: 0.008630676195025444 = 0.0016586895799264312 + 0.001 * 6.971986293792725
Epoch 1150, val loss: 1.112023115158081
Epoch 1160, training loss: 0.008577564731240273 = 0.0016271882923319936 + 0.001 * 6.950376033782959
Epoch 1160, val loss: 1.1138331890106201
Epoch 1170, training loss: 0.008522944524884224 = 0.0015968969091773033 + 0.001 * 6.9260478019714355
Epoch 1170, val loss: 1.1156173944473267
Epoch 1180, training loss: 0.008498433977365494 = 0.0015677447663620114 + 0.001 * 6.930688381195068
Epoch 1180, val loss: 1.117350459098816
Epoch 1190, training loss: 0.008510520681738853 = 0.0015396659728139639 + 0.001 * 6.97085428237915
Epoch 1190, val loss: 1.1190409660339355
Epoch 1200, training loss: 0.008440263569355011 = 0.001512623392045498 + 0.001 * 6.927640438079834
Epoch 1200, val loss: 1.120701551437378
Epoch 1210, training loss: 0.008427275344729424 = 0.0014865631237626076 + 0.001 * 6.940711498260498
Epoch 1210, val loss: 1.122329831123352
Epoch 1220, training loss: 0.008410265669226646 = 0.0014614355750381947 + 0.001 * 6.948829174041748
Epoch 1220, val loss: 1.1239280700683594
Epoch 1230, training loss: 0.008355189114809036 = 0.0014372039586305618 + 0.001 * 6.917984485626221
Epoch 1230, val loss: 1.125457763671875
Epoch 1240, training loss: 0.008343994617462158 = 0.0014138336991891265 + 0.001 * 6.9301605224609375
Epoch 1240, val loss: 1.1269891262054443
Epoch 1250, training loss: 0.008330363780260086 = 0.0013912743888795376 + 0.001 * 6.939089298248291
Epoch 1250, val loss: 1.1284769773483276
Epoch 1260, training loss: 0.0083099864423275 = 0.0013694956433027983 + 0.001 * 6.940490245819092
Epoch 1260, val loss: 1.129927635192871
Epoch 1270, training loss: 0.008281904272735119 = 0.0013484429800882936 + 0.001 * 6.933460712432861
Epoch 1270, val loss: 1.1313552856445312
Epoch 1280, training loss: 0.008238887414336205 = 0.0013281037099659443 + 0.001 * 6.910783767700195
Epoch 1280, val loss: 1.1327358484268188
Epoch 1290, training loss: 0.008219192735850811 = 0.0013084474485367537 + 0.001 * 6.910744667053223
Epoch 1290, val loss: 1.1341079473495483
Epoch 1300, training loss: 0.0081993592903018 = 0.0012894386891275644 + 0.001 * 6.9099202156066895
Epoch 1300, val loss: 1.1354472637176514
Epoch 1310, training loss: 0.008203493431210518 = 0.0012710494920611382 + 0.001 * 6.932444095611572
Epoch 1310, val loss: 1.1367294788360596
Epoch 1320, training loss: 0.008173171430826187 = 0.0012532495893537998 + 0.001 * 6.919920921325684
Epoch 1320, val loss: 1.138030767440796
Epoch 1330, training loss: 0.008174055255949497 = 0.0012360226828604937 + 0.001 * 6.938032150268555
Epoch 1330, val loss: 1.1392549276351929
Epoch 1340, training loss: 0.008125567808747292 = 0.0012193312868475914 + 0.001 * 6.90623664855957
Epoch 1340, val loss: 1.1404865980148315
Epoch 1350, training loss: 0.008096620440483093 = 0.0012031674850732088 + 0.001 * 6.893452167510986
Epoch 1350, val loss: 1.1417016983032227
Epoch 1360, training loss: 0.00810406357049942 = 0.0011874889023602009 + 0.001 * 6.916574478149414
Epoch 1360, val loss: 1.1428651809692383
Epoch 1370, training loss: 0.008073139935731888 = 0.0011723092757165432 + 0.001 * 6.900830268859863
Epoch 1370, val loss: 1.144002079963684
Epoch 1380, training loss: 0.008057027123868465 = 0.0011575788957998157 + 0.001 * 6.899448394775391
Epoch 1380, val loss: 1.145140528678894
Epoch 1390, training loss: 0.008051939308643341 = 0.0011433049803599715 + 0.001 * 6.908634185791016
Epoch 1390, val loss: 1.1462445259094238
Epoch 1400, training loss: 0.008039269596338272 = 0.001129436306655407 + 0.001 * 6.909832954406738
Epoch 1400, val loss: 1.1473302841186523
Epoch 1410, training loss: 0.008050943724811077 = 0.001115994295105338 + 0.001 * 6.9349493980407715
Epoch 1410, val loss: 1.1483560800552368
Epoch 1420, training loss: 0.007997053675353527 = 0.0011029395973309875 + 0.001 * 6.894114017486572
Epoch 1420, val loss: 1.1493825912475586
Epoch 1430, training loss: 0.007998288609087467 = 0.0010902663925662637 + 0.001 * 6.908021450042725
Epoch 1430, val loss: 1.1503961086273193
Epoch 1440, training loss: 0.007995735853910446 = 0.0010779529111459851 + 0.001 * 6.917782306671143
Epoch 1440, val loss: 1.1513702869415283
Epoch 1450, training loss: 0.00795833207666874 = 0.0010659934487193823 + 0.001 * 6.892338752746582
Epoch 1450, val loss: 1.1523206233978271
Epoch 1460, training loss: 0.007940651848912239 = 0.001054374617524445 + 0.001 * 6.886277198791504
Epoch 1460, val loss: 1.1532577276229858
Epoch 1470, training loss: 0.007920731790363789 = 0.0010430774418637156 + 0.001 * 6.877654075622559
Epoch 1470, val loss: 1.1541787385940552
Epoch 1480, training loss: 0.007934462279081345 = 0.001032100641168654 + 0.001 * 6.902360916137695
Epoch 1480, val loss: 1.1550668478012085
Epoch 1490, training loss: 0.00792967714369297 = 0.0010214120848104358 + 0.001 * 6.908265113830566
Epoch 1490, val loss: 1.1559242010116577
Epoch 1500, training loss: 0.007898172363638878 = 0.0010110246948897839 + 0.001 * 6.887146949768066
Epoch 1500, val loss: 1.1567950248718262
Epoch 1510, training loss: 0.007887388579547405 = 0.0010009134421125054 + 0.001 * 6.886474609375
Epoch 1510, val loss: 1.1576292514801025
Epoch 1520, training loss: 0.007883084006607533 = 0.0009910691296681762 + 0.001 * 6.892014503479004
Epoch 1520, val loss: 1.1584628820419312
Epoch 1530, training loss: 0.007859393022954464 = 0.0009814862860366702 + 0.001 * 6.877906322479248
Epoch 1530, val loss: 1.1592669486999512
Epoch 1540, training loss: 0.00787919107824564 = 0.0009721547248773277 + 0.001 * 6.907035827636719
Epoch 1540, val loss: 1.160078525543213
Epoch 1550, training loss: 0.007849213667213917 = 0.0009630720014683902 + 0.001 * 6.886140823364258
Epoch 1550, val loss: 1.1608107089996338
Epoch 1560, training loss: 0.007822593674063683 = 0.0009542179177515209 + 0.001 * 6.868375778198242
Epoch 1560, val loss: 1.161591649055481
Epoch 1570, training loss: 0.007814643904566765 = 0.0009455950930714607 + 0.001 * 6.869049072265625
Epoch 1570, val loss: 1.1623455286026
Epoch 1580, training loss: 0.0078055886551737785 = 0.0009371885098516941 + 0.001 * 6.868399620056152
Epoch 1580, val loss: 1.163069486618042
Epoch 1590, training loss: 0.00779805239289999 = 0.0009289938025176525 + 0.001 * 6.869058132171631
Epoch 1590, val loss: 1.1637678146362305
Epoch 1600, training loss: 0.00780691159889102 = 0.0009209944983012974 + 0.001 * 6.885916709899902
Epoch 1600, val loss: 1.1644870042800903
Epoch 1610, training loss: 0.0077919433824718 = 0.0009132099221460521 + 0.001 * 6.878733158111572
Epoch 1610, val loss: 1.1651625633239746
Epoch 1620, training loss: 0.007775343954563141 = 0.0009056078852154315 + 0.001 * 6.8697357177734375
Epoch 1620, val loss: 1.165831446647644
Epoch 1630, training loss: 0.007776942569762468 = 0.0008981972932815552 + 0.001 * 6.878745079040527
Epoch 1630, val loss: 1.166487455368042
Epoch 1640, training loss: 0.007784176152199507 = 0.0008909516618587077 + 0.001 * 6.893224239349365
Epoch 1640, val loss: 1.167123556137085
Epoch 1650, training loss: 0.007766122929751873 = 0.0008838992798700929 + 0.001 * 6.882223606109619
Epoch 1650, val loss: 1.1677078008651733
Epoch 1660, training loss: 0.007733428385108709 = 0.0008769963169470429 + 0.001 * 6.85643196105957
Epoch 1660, val loss: 1.1683528423309326
Epoch 1670, training loss: 0.007719747722148895 = 0.0008702591294422746 + 0.001 * 6.849488258361816
Epoch 1670, val loss: 1.1689577102661133
Epoch 1680, training loss: 0.00772919412702322 = 0.0008636730490252376 + 0.001 * 6.865520477294922
Epoch 1680, val loss: 1.1695377826690674
Epoch 1690, training loss: 0.00770053593441844 = 0.0008572394726797938 + 0.001 * 6.843296051025391
Epoch 1690, val loss: 1.1701278686523438
Epoch 1700, training loss: 0.00769539549946785 = 0.0008509608451277018 + 0.001 * 6.8444342613220215
Epoch 1700, val loss: 1.1707115173339844
Epoch 1710, training loss: 0.007694571744650602 = 0.0008448209264315665 + 0.001 * 6.849750518798828
Epoch 1710, val loss: 1.1712568998336792
Epoch 1720, training loss: 0.007685805670917034 = 0.0008388241403736174 + 0.001 * 6.846981048583984
Epoch 1720, val loss: 1.1717811822891235
Epoch 1730, training loss: 0.0076840524561703205 = 0.000832968857139349 + 0.001 * 6.851083278656006
Epoch 1730, val loss: 1.1723257303237915
Epoch 1740, training loss: 0.007705193944275379 = 0.0008272307459264994 + 0.001 * 6.877962589263916
Epoch 1740, val loss: 1.1728556156158447
Epoch 1750, training loss: 0.0076835607178509235 = 0.000821638444904238 + 0.001 * 6.861921787261963
Epoch 1750, val loss: 1.173316478729248
Epoch 1760, training loss: 0.007665812503546476 = 0.0008161524310708046 + 0.001 * 6.8496599197387695
Epoch 1760, val loss: 1.1738563776016235
Epoch 1770, training loss: 0.007652167696505785 = 0.0008107964531518519 + 0.001 * 6.841371059417725
Epoch 1770, val loss: 1.1743360757827759
Epoch 1780, training loss: 0.0076755378395318985 = 0.0008055561920627952 + 0.001 * 6.869981288909912
Epoch 1780, val loss: 1.1747944355010986
Epoch 1790, training loss: 0.007675745990127325 = 0.0008004242554306984 + 0.001 * 6.875321388244629
Epoch 1790, val loss: 1.1752617359161377
Epoch 1800, training loss: 0.007625367026776075 = 0.0007953957538120449 + 0.001 * 6.8299713134765625
Epoch 1800, val loss: 1.1757228374481201
Epoch 1810, training loss: 0.007625545375049114 = 0.0007904756930656731 + 0.001 * 6.835069179534912
Epoch 1810, val loss: 1.1761709451675415
Epoch 1820, training loss: 0.0076649384573102 = 0.00078565877629444 + 0.001 * 6.879279613494873
Epoch 1820, val loss: 1.1766074895858765
Epoch 1830, training loss: 0.007628713734447956 = 0.0007809402886778116 + 0.001 * 6.847773551940918
Epoch 1830, val loss: 1.1769863367080688
Epoch 1840, training loss: 0.007614535745233297 = 0.0007763099274598062 + 0.001 * 6.838225364685059
Epoch 1840, val loss: 1.1774561405181885
Epoch 1850, training loss: 0.007619969546794891 = 0.0007717747939750552 + 0.001 * 6.848194122314453
Epoch 1850, val loss: 1.1778415441513062
Epoch 1860, training loss: 0.007616820745170116 = 0.0007673308718949556 + 0.001 * 6.849489688873291
Epoch 1860, val loss: 1.1782130002975464
Epoch 1870, training loss: 0.007612728979438543 = 0.0007629821193404496 + 0.001 * 6.849746227264404
Epoch 1870, val loss: 1.1786298751831055
Epoch 1880, training loss: 0.00759250158444047 = 0.0007587142172269523 + 0.001 * 6.833787441253662
Epoch 1880, val loss: 1.1790395975112915
Epoch 1890, training loss: 0.007632773369550705 = 0.0007545264670625329 + 0.001 * 6.878246784210205
Epoch 1890, val loss: 1.1793766021728516
Epoch 1900, training loss: 0.007605590391904116 = 0.0007504178211092949 + 0.001 * 6.855172157287598
Epoch 1900, val loss: 1.1797600984573364
Epoch 1910, training loss: 0.0075911409221589565 = 0.0007464055088348687 + 0.001 * 6.844735145568848
Epoch 1910, val loss: 1.180119276046753
Epoch 1920, training loss: 0.007570411544293165 = 0.0007424576324410737 + 0.001 * 6.827953338623047
Epoch 1920, val loss: 1.180470585823059
Epoch 1930, training loss: 0.007567096501588821 = 0.0007385849021375179 + 0.001 * 6.8285112380981445
Epoch 1930, val loss: 1.1808252334594727
Epoch 1940, training loss: 0.007570091634988785 = 0.0007347927894443274 + 0.001 * 6.835298538208008
Epoch 1940, val loss: 1.181154727935791
Epoch 1950, training loss: 0.007586026564240456 = 0.0007310595829039812 + 0.001 * 6.854966640472412
Epoch 1950, val loss: 1.181473970413208
Epoch 1960, training loss: 0.007596518378704786 = 0.0007274080417118967 + 0.001 * 6.869110107421875
Epoch 1960, val loss: 1.1818053722381592
Epoch 1970, training loss: 0.007539994083344936 = 0.0007238064426928759 + 0.001 * 6.816186904907227
Epoch 1970, val loss: 1.1821277141571045
Epoch 1980, training loss: 0.007555521093308926 = 0.0007202844135463238 + 0.001 * 6.835236549377441
Epoch 1980, val loss: 1.1824373006820679
Epoch 1990, training loss: 0.007547092158347368 = 0.0007168175070546567 + 0.001 * 6.8302741050720215
Epoch 1990, val loss: 1.1827445030212402
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 1.9395636320114136 = 1.930966854095459 + 0.001 * 8.596786499023438
Epoch 0, val loss: 1.9307574033737183
Epoch 10, training loss: 1.9300569295883179 = 1.9214602708816528 + 0.001 * 8.596710205078125
Epoch 10, val loss: 1.920904278755188
Epoch 20, training loss: 1.918370246887207 = 1.909773826599121 + 0.001 * 8.596420288085938
Epoch 20, val loss: 1.9088003635406494
Epoch 30, training loss: 1.9019873142242432 = 1.8933916091918945 + 0.001 * 8.595709800720215
Epoch 30, val loss: 1.8920128345489502
Epoch 40, training loss: 1.8780041933059692 = 1.8694103956222534 + 0.001 * 8.593809127807617
Epoch 40, val loss: 1.8678274154663086
Epoch 50, training loss: 1.8450459241867065 = 1.8364583253860474 + 0.001 * 8.587615013122559
Epoch 50, val loss: 1.8360316753387451
Epoch 60, training loss: 1.80730140209198 = 1.7987393140792847 + 0.001 * 8.562032699584961
Epoch 60, val loss: 1.802680253982544
Epoch 70, training loss: 1.7692999839782715 = 1.7608652114868164 + 0.001 * 8.4347505569458
Epoch 70, val loss: 1.7708511352539062
Epoch 80, training loss: 1.7202067375183105 = 1.712189793586731 + 0.001 * 8.016928672790527
Epoch 80, val loss: 1.7270736694335938
Epoch 90, training loss: 1.6528505086898804 = 1.645032286643982 + 0.001 * 7.818226337432861
Epoch 90, val loss: 1.665999412536621
Epoch 100, training loss: 1.5640473365783691 = 1.5564243793487549 + 0.001 * 7.622960090637207
Epoch 100, val loss: 1.588266134262085
Epoch 110, training loss: 1.4597073793411255 = 1.4521921873092651 + 0.001 * 7.5152058601379395
Epoch 110, val loss: 1.4996217489242554
Epoch 120, training loss: 1.347906231880188 = 1.3404240608215332 + 0.001 * 7.482189655303955
Epoch 120, val loss: 1.4079097509384155
Epoch 130, training loss: 1.2355000972747803 = 1.2280540466308594 + 0.001 * 7.44609260559082
Epoch 130, val loss: 1.3176249265670776
Epoch 140, training loss: 1.128956913948059 = 1.121559977531433 + 0.001 * 7.396988868713379
Epoch 140, val loss: 1.234420657157898
Epoch 150, training loss: 1.033435344696045 = 1.0260930061340332 + 0.001 * 7.342352867126465
Epoch 150, val loss: 1.1619936227798462
Epoch 160, training loss: 0.9494598507881165 = 0.9421696066856384 + 0.001 * 7.290273189544678
Epoch 160, val loss: 1.0999037027359009
Epoch 170, training loss: 0.874064564704895 = 0.8668211102485657 + 0.001 * 7.243475437164307
Epoch 170, val loss: 1.0450345277786255
Epoch 180, training loss: 0.8047980070114136 = 0.797588586807251 + 0.001 * 7.209394454956055
Epoch 180, val loss: 0.9955271482467651
Epoch 190, training loss: 0.74072265625 = 0.7335357666015625 + 0.001 * 7.186903953552246
Epoch 190, val loss: 0.9512293934822083
Epoch 200, training loss: 0.6815447807312012 = 0.6743797659873962 + 0.001 * 7.164984703063965
Epoch 200, val loss: 0.9127781987190247
Epoch 210, training loss: 0.6265074610710144 = 0.6193646788597107 + 0.001 * 7.142759799957275
Epoch 210, val loss: 0.8803784251213074
Epoch 220, training loss: 0.5743576884269714 = 0.5672317743301392 + 0.001 * 7.125941753387451
Epoch 220, val loss: 0.8531913757324219
Epoch 230, training loss: 0.523983895778656 = 0.5168684124946594 + 0.001 * 7.115473747253418
Epoch 230, val loss: 0.8297349214553833
Epoch 240, training loss: 0.4748806953430176 = 0.46776846051216125 + 0.001 * 7.112245559692383
Epoch 240, val loss: 0.8092235922813416
Epoch 250, training loss: 0.4271664619445801 = 0.4200553596019745 + 0.001 * 7.111087322235107
Epoch 250, val loss: 0.7917748093605042
Epoch 260, training loss: 0.38130584359169006 = 0.37419506907463074 + 0.001 * 7.110772609710693
Epoch 260, val loss: 0.7776755690574646
Epoch 270, training loss: 0.337775856256485 = 0.3306652903556824 + 0.001 * 7.1105546951293945
Epoch 270, val loss: 0.7671873569488525
Epoch 280, training loss: 0.2969370484352112 = 0.28982633352279663 + 0.001 * 7.110717296600342
Epoch 280, val loss: 0.7603822350502014
Epoch 290, training loss: 0.2591690421104431 = 0.25205767154693604 + 0.001 * 7.111371040344238
Epoch 290, val loss: 0.7570176720619202
Epoch 300, training loss: 0.2249971181154251 = 0.21788433194160461 + 0.001 * 7.11277961730957
Epoch 300, val loss: 0.7570787072181702
Epoch 310, training loss: 0.1949332058429718 = 0.18781974911689758 + 0.001 * 7.113457202911377
Epoch 310, val loss: 0.7605857849121094
Epoch 320, training loss: 0.16914315521717072 = 0.16202834248542786 + 0.001 * 7.114809989929199
Epoch 320, val loss: 0.767291247844696
Epoch 330, training loss: 0.14736376702785492 = 0.14024761319160461 + 0.001 * 7.1161603927612305
Epoch 330, val loss: 0.7769068479537964
Epoch 340, training loss: 0.12906613945960999 = 0.12194830924272537 + 0.001 * 7.117824077606201
Epoch 340, val loss: 0.788959264755249
Epoch 350, training loss: 0.11367816478013992 = 0.10655909031629562 + 0.001 * 7.119072914123535
Epoch 350, val loss: 0.8028882741928101
Epoch 360, training loss: 0.10069034993648529 = 0.0935700312256813 + 0.001 * 7.1203203201293945
Epoch 360, val loss: 0.8182222247123718
Epoch 370, training loss: 0.08968129754066467 = 0.08256004750728607 + 0.001 * 7.12125301361084
Epoch 370, val loss: 0.8344771862030029
Epoch 380, training loss: 0.08031559735536575 = 0.07319322973489761 + 0.001 * 7.122364521026611
Epoch 380, val loss: 0.8513069152832031
Epoch 390, training loss: 0.0723174512386322 = 0.0651940405368805 + 0.001 * 7.123407363891602
Epoch 390, val loss: 0.8683302998542786
Epoch 400, training loss: 0.0654568150639534 = 0.05833311751484871 + 0.001 * 7.1236982345581055
Epoch 400, val loss: 0.885319709777832
Epoch 410, training loss: 0.05954470485448837 = 0.05241948738694191 + 0.001 * 7.125218391418457
Epoch 410, val loss: 0.9020884037017822
Epoch 420, training loss: 0.054421789944171906 = 0.047296736389398575 + 0.001 * 7.125053405761719
Epoch 420, val loss: 0.9184983372688293
Epoch 430, training loss: 0.04996239393949509 = 0.04283686354756355 + 0.001 * 7.1255292892456055
Epoch 430, val loss: 0.9344840049743652
Epoch 440, training loss: 0.046060483902692795 = 0.03893517702817917 + 0.001 * 7.125307083129883
Epoch 440, val loss: 0.9500323534011841
Epoch 450, training loss: 0.04263216257095337 = 0.03550637513399124 + 0.001 * 7.125785827636719
Epoch 450, val loss: 0.9651042819023132
Epoch 460, training loss: 0.03960546851158142 = 0.03248092904686928 + 0.001 * 7.124538898468018
Epoch 460, val loss: 0.979705810546875
Epoch 470, training loss: 0.0369255393743515 = 0.029801102355122566 + 0.001 * 7.12443733215332
Epoch 470, val loss: 0.9938539862632751
Epoch 480, training loss: 0.034547630697488785 = 0.027419552206993103 + 0.001 * 7.128076553344727
Epoch 480, val loss: 1.0075490474700928
Epoch 490, training loss: 0.03241993486881256 = 0.02529650740325451 + 0.001 * 7.123428821563721
Epoch 490, val loss: 1.0208253860473633
Epoch 500, training loss: 0.030521146953105927 = 0.02339817024767399 + 0.001 * 7.122977256774902
Epoch 500, val loss: 1.0336700677871704
Epoch 510, training loss: 0.028817767277359962 = 0.02169598639011383 + 0.001 * 7.121780872344971
Epoch 510, val loss: 1.0461000204086304
Epoch 520, training loss: 0.02728637494146824 = 0.02016574516892433 + 0.001 * 7.12062931060791
Epoch 520, val loss: 1.0581367015838623
Epoch 530, training loss: 0.025906769558787346 = 0.018786149099469185 + 0.001 * 7.120620250701904
Epoch 530, val loss: 1.0697904825210571
Epoch 540, training loss: 0.024659959599375725 = 0.017539381980895996 + 0.001 * 7.120577335357666
Epoch 540, val loss: 1.0810863971710205
Epoch 550, training loss: 0.02352536842226982 = 0.016409853473305702 + 0.001 * 7.11551570892334
Epoch 550, val loss: 1.0920469760894775
Epoch 560, training loss: 0.022507231682538986 = 0.015384065918624401 + 0.001 * 7.123166561126709
Epoch 560, val loss: 1.1026561260223389
Epoch 570, training loss: 0.021583691239356995 = 0.014450343325734138 + 0.001 * 7.133347511291504
Epoch 570, val loss: 1.1129446029663086
Epoch 580, training loss: 0.020713770762085915 = 0.013598471879959106 + 0.001 * 7.115299224853516
Epoch 580, val loss: 1.1229227781295776
Epoch 590, training loss: 0.019930565729737282 = 0.012819747440516949 + 0.001 * 7.1108174324035645
Epoch 590, val loss: 1.1326031684875488
Epoch 600, training loss: 0.019213464111089706 = 0.012106400914490223 + 0.001 * 7.107063293457031
Epoch 600, val loss: 1.141989827156067
Epoch 610, training loss: 0.01855449751019478 = 0.011451532132923603 + 0.001 * 7.102963924407959
Epoch 610, val loss: 1.1511032581329346
Epoch 620, training loss: 0.01794767566025257 = 0.010849145241081715 + 0.001 * 7.098530292510986
Epoch 620, val loss: 1.1599442958831787
Epoch 630, training loss: 0.017452379688620567 = 0.01029394380748272 + 0.001 * 7.158435821533203
Epoch 630, val loss: 1.1685285568237305
Epoch 640, training loss: 0.01689719595015049 = 0.00978122279047966 + 0.001 * 7.115972995758057
Epoch 640, val loss: 1.1768523454666138
Epoch 650, training loss: 0.016399625688791275 = 0.00930699147284031 + 0.001 * 7.092634201049805
Epoch 650, val loss: 1.184935450553894
Epoch 660, training loss: 0.01595858484506607 = 0.00886763259768486 + 0.001 * 7.0909528732299805
Epoch 660, val loss: 1.1927868127822876
Epoch 670, training loss: 0.015544410794973373 = 0.008459869772195816 + 0.001 * 7.084540843963623
Epoch 670, val loss: 1.2004135847091675
Epoch 680, training loss: 0.015161262825131416 = 0.0080808000639081 + 0.001 * 7.08046293258667
Epoch 680, val loss: 1.2078289985656738
Epoch 690, training loss: 0.01481715589761734 = 0.007727855816483498 + 0.001 * 7.08929967880249
Epoch 690, val loss: 1.2150355577468872
Epoch 700, training loss: 0.01447289064526558 = 0.007398699875921011 + 0.001 * 7.074190616607666
Epoch 700, val loss: 1.2220579385757446
Epoch 710, training loss: 0.014159350655972958 = 0.007091324310749769 + 0.001 * 7.068026065826416
Epoch 710, val loss: 1.2288858890533447
Epoch 720, training loss: 0.013863461092114449 = 0.006803866475820541 + 0.001 * 7.05959415435791
Epoch 720, val loss: 1.2355167865753174
Epoch 730, training loss: 0.013605795800685883 = 0.006534678861498833 + 0.001 * 7.0711164474487305
Epoch 730, val loss: 1.2419888973236084
Epoch 740, training loss: 0.013341700658202171 = 0.006282273679971695 + 0.001 * 7.059427261352539
Epoch 740, val loss: 1.248284101486206
Epoch 750, training loss: 0.013106062076985836 = 0.0060452925972640514 + 0.001 * 7.060769081115723
Epoch 750, val loss: 1.2544118165969849
Epoch 760, training loss: 0.012892913073301315 = 0.005822532810270786 + 0.001 * 7.070379257202148
Epoch 760, val loss: 1.2603873014450073
Epoch 770, training loss: 0.012659014202654362 = 0.005612861830741167 + 0.001 * 7.046152114868164
Epoch 770, val loss: 1.2662006616592407
Epoch 780, training loss: 0.012511884793639183 = 0.0054153152741491795 + 0.001 * 7.096568584442139
Epoch 780, val loss: 1.2718753814697266
Epoch 790, training loss: 0.012281857430934906 = 0.005228952970355749 + 0.001 * 7.0529046058654785
Epoch 790, val loss: 1.2774080038070679
Epoch 800, training loss: 0.01209370419383049 = 0.00505298376083374 + 0.001 * 7.040719985961914
Epoch 800, val loss: 1.2828091382980347
Epoch 810, training loss: 0.011939751915633678 = 0.0048866537399590015 + 0.001 * 7.053097724914551
Epoch 810, val loss: 1.288071870803833
Epoch 820, training loss: 0.01176423765718937 = 0.004729259293526411 + 0.001 * 7.034977436065674
Epoch 820, val loss: 1.2932215929031372
Epoch 830, training loss: 0.01165984570980072 = 0.004580236040055752 + 0.001 * 7.079609394073486
Epoch 830, val loss: 1.2982257604599
Epoch 840, training loss: 0.011462237685918808 = 0.004438937176018953 + 0.001 * 7.0233001708984375
Epoch 840, val loss: 1.3031227588653564
Epoch 850, training loss: 0.011339696124196053 = 0.004304872825741768 + 0.001 * 7.034822940826416
Epoch 850, val loss: 1.3079228401184082
Epoch 860, training loss: 0.011207355186343193 = 0.0041775573045015335 + 0.001 * 7.029797077178955
Epoch 860, val loss: 1.3125966787338257
Epoch 870, training loss: 0.011068541556596756 = 0.0040565598756074905 + 0.001 * 7.011981964111328
Epoch 870, val loss: 1.3171697854995728
Epoch 880, training loss: 0.010984255000948906 = 0.0039414772763848305 + 0.001 * 7.042778015136719
Epoch 880, val loss: 1.3216313123703003
Epoch 890, training loss: 0.010840699076652527 = 0.0038318652659654617 + 0.001 * 7.008832931518555
Epoch 890, val loss: 1.3260104656219482
Epoch 900, training loss: 0.01074468344449997 = 0.003727463772520423 + 0.001 * 7.017219543457031
Epoch 900, val loss: 1.3302823305130005
Epoch 910, training loss: 0.010654068551957607 = 0.003627914236858487 + 0.001 * 7.026153564453125
Epoch 910, val loss: 1.3344587087631226
Epoch 920, training loss: 0.010553817264735699 = 0.003532892558723688 + 0.001 * 7.0209245681762695
Epoch 920, val loss: 1.338560938835144
Epoch 930, training loss: 0.010462638922035694 = 0.003442192217335105 + 0.001 * 7.020446300506592
Epoch 930, val loss: 1.3425421714782715
Epoch 940, training loss: 0.0103626549243927 = 0.0033554958645254374 + 0.001 * 7.0071587562561035
Epoch 940, val loss: 1.3464680910110474
Epoch 950, training loss: 0.010321189649403095 = 0.0032726225908845663 + 0.001 * 7.048566818237305
Epoch 950, val loss: 1.3502928018569946
Epoch 960, training loss: 0.010202573612332344 = 0.0031933195423334837 + 0.001 * 7.00925350189209
Epoch 960, val loss: 1.3540335893630981
Epoch 970, training loss: 0.010113142430782318 = 0.003117398824542761 + 0.001 * 6.995743274688721
Epoch 970, val loss: 1.3577098846435547
Epoch 980, training loss: 0.010051559656858444 = 0.003044667188078165 + 0.001 * 7.006892204284668
Epoch 980, val loss: 1.3613005876541138
Epoch 990, training loss: 0.009974362328648567 = 0.0029749395325779915 + 0.001 * 6.999423027038574
Epoch 990, val loss: 1.3648301362991333
Epoch 1000, training loss: 0.009938686154782772 = 0.0029080617241561413 + 0.001 * 7.030623912811279
Epoch 1000, val loss: 1.3682912588119507
Epoch 1010, training loss: 0.009822119027376175 = 0.0028438863810151815 + 0.001 * 6.978231906890869
Epoch 1010, val loss: 1.3716516494750977
Epoch 1020, training loss: 0.00975846778601408 = 0.0027822835836559534 + 0.001 * 6.976183891296387
Epoch 1020, val loss: 1.374958872795105
Epoch 1030, training loss: 0.009707186371088028 = 0.002723085228353739 + 0.001 * 6.984101295471191
Epoch 1030, val loss: 1.3782111406326294
Epoch 1040, training loss: 0.009646865539252758 = 0.0026661951560527086 + 0.001 * 6.98067045211792
Epoch 1040, val loss: 1.3813836574554443
Epoch 1050, training loss: 0.009598596952855587 = 0.0026114911306649446 + 0.001 * 6.987105369567871
Epoch 1050, val loss: 1.384490728378296
Epoch 1060, training loss: 0.009549298323690891 = 0.0025588294956833124 + 0.001 * 6.990468502044678
Epoch 1060, val loss: 1.3875677585601807
Epoch 1070, training loss: 0.009537339210510254 = 0.0025081655476242304 + 0.001 * 7.029172897338867
Epoch 1070, val loss: 1.3905726671218872
Epoch 1080, training loss: 0.009453514590859413 = 0.002459355629980564 + 0.001 * 6.99415922164917
Epoch 1080, val loss: 1.3935009241104126
Epoch 1090, training loss: 0.009416485205292702 = 0.002412336878478527 + 0.001 * 7.004148483276367
Epoch 1090, val loss: 1.396382212638855
Epoch 1100, training loss: 0.009355150163173676 = 0.0023669793736189604 + 0.001 * 6.988170146942139
Epoch 1100, val loss: 1.3992332220077515
Epoch 1110, training loss: 0.009311104193329811 = 0.002323276363313198 + 0.001 * 6.987826824188232
Epoch 1110, val loss: 1.4019895792007446
Epoch 1120, training loss: 0.009264357388019562 = 0.0022810897789895535 + 0.001 * 6.983267784118652
Epoch 1120, val loss: 1.4047285318374634
Epoch 1130, training loss: 0.009204257279634476 = 0.002240384230390191 + 0.001 * 6.963872909545898
Epoch 1130, val loss: 1.4074039459228516
Epoch 1140, training loss: 0.009174682199954987 = 0.0022010812535881996 + 0.001 * 6.9736008644104
Epoch 1140, val loss: 1.4100242853164673
Epoch 1150, training loss: 0.00913748238235712 = 0.0021631077397614717 + 0.001 * 6.974374294281006
Epoch 1150, val loss: 1.4126267433166504
Epoch 1160, training loss: 0.009105858393013477 = 0.002126419683918357 + 0.001 * 6.979438781738281
Epoch 1160, val loss: 1.4151536226272583
Epoch 1170, training loss: 0.00904910359531641 = 0.0020909542217850685 + 0.001 * 6.958148956298828
Epoch 1170, val loss: 1.417633295059204
Epoch 1180, training loss: 0.009042836725711823 = 0.0020566778257489204 + 0.001 * 6.98615837097168
Epoch 1180, val loss: 1.4200786352157593
Epoch 1190, training loss: 0.00896516628563404 = 0.0020234796684235334 + 0.001 * 6.941686630249023
Epoch 1190, val loss: 1.4224737882614136
Epoch 1200, training loss: 0.008970667608082294 = 0.00199139560572803 + 0.001 * 6.979271411895752
Epoch 1200, val loss: 1.4248292446136475
Epoch 1210, training loss: 0.008909990079700947 = 0.0019602952525019646 + 0.001 * 6.949694633483887
Epoch 1210, val loss: 1.427152156829834
Epoch 1220, training loss: 0.008881766349077225 = 0.0019301916472613811 + 0.001 * 6.951574325561523
Epoch 1220, val loss: 1.429436206817627
Epoch 1230, training loss: 0.008845285512506962 = 0.001901033567264676 + 0.001 * 6.944251537322998
Epoch 1230, val loss: 1.4316797256469727
Epoch 1240, training loss: 0.0088103748857975 = 0.0018727811984717846 + 0.001 * 6.93759298324585
Epoch 1240, val loss: 1.4338724613189697
Epoch 1250, training loss: 0.00879855826497078 = 0.0018453868106007576 + 0.001 * 6.953171253204346
Epoch 1250, val loss: 1.4360477924346924
Epoch 1260, training loss: 0.008758977986872196 = 0.0018188103567808867 + 0.001 * 6.94016695022583
Epoch 1260, val loss: 1.4381754398345947
Epoch 1270, training loss: 0.008719420060515404 = 0.001793048926629126 + 0.001 * 6.926371097564697
Epoch 1270, val loss: 1.4402660131454468
Epoch 1280, training loss: 0.00870490912348032 = 0.0017680557211861014 + 0.001 * 6.936853408813477
Epoch 1280, val loss: 1.442335605621338
Epoch 1290, training loss: 0.008709738031029701 = 0.0017438015202060342 + 0.001 * 6.965935707092285
Epoch 1290, val loss: 1.4443509578704834
Epoch 1300, training loss: 0.008648566901683807 = 0.0017202245071530342 + 0.001 * 6.928341388702393
Epoch 1300, val loss: 1.4463685750961304
Epoch 1310, training loss: 0.008637059479951859 = 0.0016973483143374324 + 0.001 * 6.939711093902588
Epoch 1310, val loss: 1.4483388662338257
Epoch 1320, training loss: 0.008595587685704231 = 0.001675136387348175 + 0.001 * 6.9204511642456055
Epoch 1320, val loss: 1.4502708911895752
Epoch 1330, training loss: 0.00856864545494318 = 0.0016535409959033132 + 0.001 * 6.915103912353516
Epoch 1330, val loss: 1.4521572589874268
Epoch 1340, training loss: 0.008560816757380962 = 0.0016325600445270538 + 0.001 * 6.928256511688232
Epoch 1340, val loss: 1.4540473222732544
Epoch 1350, training loss: 0.00853993184864521 = 0.0016121627995744348 + 0.001 * 6.927768707275391
Epoch 1350, val loss: 1.4558802843093872
Epoch 1360, training loss: 0.008515244349837303 = 0.0015923120081424713 + 0.001 * 6.922932147979736
Epoch 1360, val loss: 1.457695484161377
Epoch 1370, training loss: 0.008508898317813873 = 0.0015729903243482113 + 0.001 * 6.935907363891602
Epoch 1370, val loss: 1.4594898223876953
Epoch 1380, training loss: 0.008472277782857418 = 0.0015541563043370843 + 0.001 * 6.918121337890625
Epoch 1380, val loss: 1.4612394571304321
Epoch 1390, training loss: 0.008477824740111828 = 0.0015357807278633118 + 0.001 * 6.942043781280518
Epoch 1390, val loss: 1.4630038738250732
Epoch 1400, training loss: 0.008444810286164284 = 0.0015178031753748655 + 0.001 * 6.92700719833374
Epoch 1400, val loss: 1.4646917581558228
Epoch 1410, training loss: 0.008407381363213062 = 0.0015001429710537195 + 0.001 * 6.907238006591797
Epoch 1410, val loss: 1.4663900136947632
Epoch 1420, training loss: 0.008429421111941338 = 0.001482723979279399 + 0.001 * 6.946697235107422
Epoch 1420, val loss: 1.4680479764938354
Epoch 1430, training loss: 0.008391543291509151 = 0.0014654818223789334 + 0.001 * 6.926061153411865
Epoch 1430, val loss: 1.4696966409683228
Epoch 1440, training loss: 0.008389991708099842 = 0.0014483610866591334 + 0.001 * 6.9416303634643555
Epoch 1440, val loss: 1.4713040590286255
Epoch 1450, training loss: 0.0083609065040946 = 0.001431227196007967 + 0.001 * 6.929678916931152
Epoch 1450, val loss: 1.4729523658752441
Epoch 1460, training loss: 0.008344175294041634 = 0.0014140509301796556 + 0.001 * 6.930124282836914
Epoch 1460, val loss: 1.4745168685913086
Epoch 1470, training loss: 0.008320891298353672 = 0.001396742882207036 + 0.001 * 6.924148082733154
Epoch 1470, val loss: 1.4761232137680054
Epoch 1480, training loss: 0.008323310874402523 = 0.001379396882839501 + 0.001 * 6.94391393661499
Epoch 1480, val loss: 1.4776878356933594
Epoch 1490, training loss: 0.008264921605587006 = 0.001361988252028823 + 0.001 * 6.902932643890381
Epoch 1490, val loss: 1.4792985916137695
Epoch 1500, training loss: 0.008243887685239315 = 0.001344658900052309 + 0.001 * 6.899228572845459
Epoch 1500, val loss: 1.4808586835861206
Epoch 1510, training loss: 0.00823021586984396 = 0.0013273863587528467 + 0.001 * 6.902829647064209
Epoch 1510, val loss: 1.4824588298797607
Epoch 1520, training loss: 0.00823962315917015 = 0.0013102259254083037 + 0.001 * 6.929396629333496
Epoch 1520, val loss: 1.4840422868728638
Epoch 1530, training loss: 0.008215462788939476 = 0.0012932488461956382 + 0.001 * 6.922214031219482
Epoch 1530, val loss: 1.485594391822815
Epoch 1540, training loss: 0.008177604526281357 = 0.0012765227584168315 + 0.001 * 6.901081562042236
Epoch 1540, val loss: 1.4871751070022583
Epoch 1550, training loss: 0.00814594142138958 = 0.00126005825586617 + 0.001 * 6.88588285446167
Epoch 1550, val loss: 1.488736629486084
Epoch 1560, training loss: 0.008120615035295486 = 0.0012438652338460088 + 0.001 * 6.876749515533447
Epoch 1560, val loss: 1.4902926683425903
Epoch 1570, training loss: 0.008131340146064758 = 0.001227937056683004 + 0.001 * 6.903403282165527
Epoch 1570, val loss: 1.4918229579925537
Epoch 1580, training loss: 0.008109369315207005 = 0.0012123397318646312 + 0.001 * 6.897029399871826
Epoch 1580, val loss: 1.4933520555496216
Epoch 1590, training loss: 0.008093584328889847 = 0.0011970670893788338 + 0.001 * 6.896517276763916
Epoch 1590, val loss: 1.4948912858963013
Epoch 1600, training loss: 0.008106516674160957 = 0.0011821333318948746 + 0.001 * 6.924383163452148
Epoch 1600, val loss: 1.4963723421096802
Epoch 1610, training loss: 0.008105259388685226 = 0.0011675310088321567 + 0.001 * 6.937727928161621
Epoch 1610, val loss: 1.4978712797164917
Epoch 1620, training loss: 0.008050546050071716 = 0.0011533083161339164 + 0.001 * 6.897237300872803
Epoch 1620, val loss: 1.4993126392364502
Epoch 1630, training loss: 0.008039860986173153 = 0.0011394962202757597 + 0.001 * 6.900364398956299
Epoch 1630, val loss: 1.500731110572815
Epoch 1640, training loss: 0.008013846352696419 = 0.0011260673636570573 + 0.001 * 6.8877787590026855
Epoch 1640, val loss: 1.502142310142517
Epoch 1650, training loss: 0.00799521617591381 = 0.001112980768084526 + 0.001 * 6.882235527038574
Epoch 1650, val loss: 1.5035068988800049
Epoch 1660, training loss: 0.007978527806699276 = 0.0011002847459167242 + 0.001 * 6.878242492675781
Epoch 1660, val loss: 1.504873514175415
Epoch 1670, training loss: 0.007976885885000229 = 0.0010879545006901026 + 0.001 * 6.8889312744140625
Epoch 1670, val loss: 1.5061848163604736
Epoch 1680, training loss: 0.007997251115739346 = 0.0010760016739368439 + 0.001 * 6.921248912811279
Epoch 1680, val loss: 1.5075043439865112
Epoch 1690, training loss: 0.007942876778542995 = 0.001064432435669005 + 0.001 * 6.878443717956543
Epoch 1690, val loss: 1.5087414979934692
Epoch 1700, training loss: 0.007953548803925514 = 0.0010531987063586712 + 0.001 * 6.900349140167236
Epoch 1700, val loss: 1.5100018978118896
Epoch 1710, training loss: 0.007972534745931625 = 0.0010422499617561698 + 0.001 * 6.93028450012207
Epoch 1710, val loss: 1.5112422704696655
Epoch 1720, training loss: 0.007905751466751099 = 0.0010315892286598682 + 0.001 * 6.874161720275879
Epoch 1720, val loss: 1.5123963356018066
Epoch 1730, training loss: 0.00792628899216652 = 0.0010212176712229848 + 0.001 * 6.905071258544922
Epoch 1730, val loss: 1.513609766960144
Epoch 1740, training loss: 0.007935301400721073 = 0.0010111037408933043 + 0.001 * 6.924197196960449
Epoch 1740, val loss: 1.5147374868392944
Epoch 1750, training loss: 0.007894505746662617 = 0.0010012569837272167 + 0.001 * 6.893248558044434
Epoch 1750, val loss: 1.515829086303711
Epoch 1760, training loss: 0.00790300965309143 = 0.0009916872950270772 + 0.001 * 6.911321640014648
Epoch 1760, val loss: 1.5169692039489746
Epoch 1770, training loss: 0.007852241396903992 = 0.000982369645498693 + 0.001 * 6.869871616363525
Epoch 1770, val loss: 1.5179871320724487
Epoch 1780, training loss: 0.007830764167010784 = 0.0009733063634485006 + 0.001 * 6.857457637786865
Epoch 1780, val loss: 1.5190701484680176
Epoch 1790, training loss: 0.007818496786057949 = 0.0009645023965276778 + 0.001 * 6.853993892669678
Epoch 1790, val loss: 1.5200978517532349
Epoch 1800, training loss: 0.007882043719291687 = 0.0009559395257383585 + 0.001 * 6.926103591918945
Epoch 1800, val loss: 1.5211039781570435
Epoch 1810, training loss: 0.0077828005887568 = 0.0009476359700784087 + 0.001 * 6.8351640701293945
Epoch 1810, val loss: 1.5220845937728882
Epoch 1820, training loss: 0.007798485457897186 = 0.0009395603556185961 + 0.001 * 6.858924865722656
Epoch 1820, val loss: 1.5230733156204224
Epoch 1830, training loss: 0.00777630927041173 = 0.0009316798532381654 + 0.001 * 6.844629287719727
Epoch 1830, val loss: 1.52400803565979
Epoch 1840, training loss: 0.007763068191707134 = 0.0009240091312676668 + 0.001 * 6.8390583992004395
Epoch 1840, val loss: 1.5249462127685547
Epoch 1850, training loss: 0.007795293815433979 = 0.0009165095980279148 + 0.001 * 6.8787841796875
Epoch 1850, val loss: 1.525805115699768
Epoch 1860, training loss: 0.00777283962816 = 0.0009092281106859446 + 0.001 * 6.863610744476318
Epoch 1860, val loss: 1.5266971588134766
Epoch 1870, training loss: 0.0077383676543831825 = 0.0009021212463267148 + 0.001 * 6.836246013641357
Epoch 1870, val loss: 1.527577519416809
Epoch 1880, training loss: 0.007764942944049835 = 0.00089520268375054 + 0.001 * 6.869740009307861
Epoch 1880, val loss: 1.528428077697754
Epoch 1890, training loss: 0.007752456236630678 = 0.0008884822018444538 + 0.001 * 6.863973617553711
Epoch 1890, val loss: 1.5292315483093262
Epoch 1900, training loss: 0.007728547789156437 = 0.0008819352369755507 + 0.001 * 6.846611976623535
Epoch 1900, val loss: 1.5300172567367554
Epoch 1910, training loss: 0.007762182503938675 = 0.0008755324524827302 + 0.001 * 6.8866496086120605
Epoch 1910, val loss: 1.5308303833007812
Epoch 1920, training loss: 0.00773509219288826 = 0.0008692804840393364 + 0.001 * 6.865811347961426
Epoch 1920, val loss: 1.5315618515014648
Epoch 1930, training loss: 0.007696528919041157 = 0.000863152788951993 + 0.001 * 6.833375453948975
Epoch 1930, val loss: 1.5323249101638794
Epoch 1940, training loss: 0.007702572271227837 = 0.0008571806247346103 + 0.001 * 6.845391273498535
Epoch 1940, val loss: 1.5330455303192139
Epoch 1950, training loss: 0.007689990568906069 = 0.0008513476350344718 + 0.001 * 6.838642597198486
Epoch 1950, val loss: 1.5337748527526855
Epoch 1960, training loss: 0.007673337124288082 = 0.0008456518989987671 + 0.001 * 6.8276848793029785
Epoch 1960, val loss: 1.5344994068145752
Epoch 1970, training loss: 0.007686372846364975 = 0.0008400724618695676 + 0.001 * 6.84630012512207
Epoch 1970, val loss: 1.5351601839065552
Epoch 1980, training loss: 0.007670063991099596 = 0.0008346210815943778 + 0.001 * 6.835442543029785
Epoch 1980, val loss: 1.535844087600708
Epoch 1990, training loss: 0.007649184204638004 = 0.0008293171413242817 + 0.001 * 6.81986665725708
Epoch 1990, val loss: 1.5364996194839478
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.838165524512388
The final CL Acc:0.80988, 0.00462, The final GNN Acc:0.83711, 0.00437
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10566])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.953977346420288 = 1.945380449295044 + 0.001 * 8.596869468688965
Epoch 0, val loss: 1.940022587776184
Epoch 10, training loss: 1.9433501958847046 = 1.9347532987594604 + 0.001 * 8.596839904785156
Epoch 10, val loss: 1.9290889501571655
Epoch 20, training loss: 1.9301912784576416 = 1.921594500541687 + 0.001 * 8.596725463867188
Epoch 20, val loss: 1.915614366531372
Epoch 30, training loss: 1.9117964506149292 = 1.9032000303268433 + 0.001 * 8.59646987915039
Epoch 30, val loss: 1.897027850151062
Epoch 40, training loss: 1.8852193355560303 = 1.876623511314392 + 0.001 * 8.595820426940918
Epoch 40, val loss: 1.8707338571548462
Epoch 50, training loss: 1.8500549793243408 = 1.841461181640625 + 0.001 * 8.593850135803223
Epoch 50, val loss: 1.8379217386245728
Epoch 60, training loss: 1.815136432647705 = 1.8065497875213623 + 0.001 * 8.58658504486084
Epoch 60, val loss: 1.8092243671417236
Epoch 70, training loss: 1.7870162725448608 = 1.7784626483917236 + 0.001 * 8.553645133972168
Epoch 70, val loss: 1.7871577739715576
Epoch 80, training loss: 1.7482585906982422 = 1.7399271726608276 + 0.001 * 8.331409454345703
Epoch 80, val loss: 1.7524772882461548
Epoch 90, training loss: 1.692628264427185 = 1.6845439672470093 + 0.001 * 8.084242820739746
Epoch 90, val loss: 1.7032930850982666
Epoch 100, training loss: 1.616971492767334 = 1.6089277267456055 + 0.001 * 8.043718338012695
Epoch 100, val loss: 1.6400463581085205
Epoch 110, training loss: 1.530845284461975 = 1.5228163003921509 + 0.001 * 8.029010772705078
Epoch 110, val loss: 1.5711618661880493
Epoch 120, training loss: 1.4464839696884155 = 1.4384782314300537 + 0.001 * 8.005712509155273
Epoch 120, val loss: 1.5064250230789185
Epoch 130, training loss: 1.3664782047271729 = 1.3585654497146606 + 0.001 * 7.912813186645508
Epoch 130, val loss: 1.4457626342773438
Epoch 140, training loss: 1.286365032196045 = 1.278658151626587 + 0.001 * 7.706934452056885
Epoch 140, val loss: 1.385283350944519
Epoch 150, training loss: 1.2046352624893188 = 1.196942687034607 + 0.001 * 7.69256591796875
Epoch 150, val loss: 1.3239402770996094
Epoch 160, training loss: 1.1219885349273682 = 1.1143412590026855 + 0.001 * 7.647250175476074
Epoch 160, val loss: 1.2634997367858887
Epoch 170, training loss: 1.0397111177444458 = 1.0320926904678345 + 0.001 * 7.618436336517334
Epoch 170, val loss: 1.204256534576416
Epoch 180, training loss: 0.9589847922325134 = 0.9514034986495972 + 0.001 * 7.581268787384033
Epoch 180, val loss: 1.1465785503387451
Epoch 190, training loss: 0.8812371492385864 = 0.873689591884613 + 0.001 * 7.547560691833496
Epoch 190, val loss: 1.0914591550827026
Epoch 200, training loss: 0.8079423308372498 = 0.8004120588302612 + 0.001 * 7.5302934646606445
Epoch 200, val loss: 1.0408591032028198
Epoch 210, training loss: 0.740212619304657 = 0.7327054738998413 + 0.001 * 7.507170677185059
Epoch 210, val loss: 0.9964383840560913
Epoch 220, training loss: 0.6779239177703857 = 0.6704582571983337 + 0.001 * 7.465686798095703
Epoch 220, val loss: 0.9592981338500977
Epoch 230, training loss: 0.6198111176490784 = 0.6123762130737305 + 0.001 * 7.434915065765381
Epoch 230, val loss: 0.9293885827064514
Epoch 240, training loss: 0.5646548867225647 = 0.557248592376709 + 0.001 * 7.406285762786865
Epoch 240, val loss: 0.9060438275337219
Epoch 250, training loss: 0.5123340487480164 = 0.5049368143081665 + 0.001 * 7.397233486175537
Epoch 250, val loss: 0.8893483281135559
Epoch 260, training loss: 0.46335363388061523 = 0.45596083998680115 + 0.001 * 7.39277982711792
Epoch 260, val loss: 0.8796755075454712
Epoch 270, training loss: 0.41835087537765503 = 0.41095787286758423 + 0.001 * 7.393002033233643
Epoch 270, val loss: 0.8772349953651428
Epoch 280, training loss: 0.3775520324707031 = 0.3701655864715576 + 0.001 * 7.386450290679932
Epoch 280, val loss: 0.8816874027252197
Epoch 290, training loss: 0.340624064207077 = 0.3332423269748688 + 0.001 * 7.3817338943481445
Epoch 290, val loss: 0.8919325470924377
Epoch 300, training loss: 0.30683907866477966 = 0.2994609773159027 + 0.001 * 7.378105640411377
Epoch 300, val loss: 0.9065840840339661
Epoch 310, training loss: 0.27544617652893066 = 0.2680709958076477 + 0.001 * 7.375188827514648
Epoch 310, val loss: 0.9247891902923584
Epoch 320, training loss: 0.24609658122062683 = 0.2387268841266632 + 0.001 * 7.369699478149414
Epoch 320, val loss: 0.9463447332382202
Epoch 330, training loss: 0.21895544230937958 = 0.21158906817436218 + 0.001 * 7.366378307342529
Epoch 330, val loss: 0.9707998633384705
Epoch 340, training loss: 0.19426949322223663 = 0.18689899146556854 + 0.001 * 7.370507717132568
Epoch 340, val loss: 0.9975571036338806
Epoch 350, training loss: 0.17215953767299652 = 0.16479122638702393 + 0.001 * 7.368309020996094
Epoch 350, val loss: 1.0257949829101562
Epoch 360, training loss: 0.15254142880439758 = 0.14519329369068146 + 0.001 * 7.348130702972412
Epoch 360, val loss: 1.0550177097320557
Epoch 370, training loss: 0.1352633684873581 = 0.12792859971523285 + 0.001 * 7.334761619567871
Epoch 370, val loss: 1.0849775075912476
Epoch 380, training loss: 0.12016107141971588 = 0.1128007248044014 + 0.001 * 7.36034631729126
Epoch 380, val loss: 1.1155436038970947
Epoch 390, training loss: 0.10693640261888504 = 0.0995989739894867 + 0.001 * 7.337430477142334
Epoch 390, val loss: 1.1466574668884277
Epoch 400, training loss: 0.0954355001449585 = 0.08811301738023758 + 0.001 * 7.322485446929932
Epoch 400, val loss: 1.1784552335739136
Epoch 410, training loss: 0.08545821160078049 = 0.07813887298107147 + 0.001 * 7.3193359375
Epoch 410, val loss: 1.2106280326843262
Epoch 420, training loss: 0.07680340856313705 = 0.06949549168348312 + 0.001 * 7.307918548583984
Epoch 420, val loss: 1.243069052696228
Epoch 430, training loss: 0.06930246204137802 = 0.061996545642614365 + 0.001 * 7.3059163093566895
Epoch 430, val loss: 1.2755056619644165
Epoch 440, training loss: 0.06277699768543243 = 0.05547362565994263 + 0.001 * 7.303369522094727
Epoch 440, val loss: 1.3075904846191406
Epoch 450, training loss: 0.057086750864982605 = 0.04978703334927559 + 0.001 * 7.299716472625732
Epoch 450, val loss: 1.3392925262451172
Epoch 460, training loss: 0.052114587277173996 = 0.04481971636414528 + 0.001 * 7.2948713302612305
Epoch 460, val loss: 1.3705575466156006
Epoch 470, training loss: 0.04776288941502571 = 0.04047376662492752 + 0.001 * 7.28912353515625
Epoch 470, val loss: 1.4011626243591309
Epoch 480, training loss: 0.04394783452153206 = 0.036664869636297226 + 0.001 * 7.282965660095215
Epoch 480, val loss: 1.4311997890472412
Epoch 490, training loss: 0.04059891402721405 = 0.03332012519240379 + 0.001 * 7.278790473937988
Epoch 490, val loss: 1.4603863954544067
Epoch 500, training loss: 0.037655286490917206 = 0.030377022922039032 + 0.001 * 7.278262615203857
Epoch 500, val loss: 1.488844633102417
Epoch 510, training loss: 0.03506837785243988 = 0.02778192050755024 + 0.001 * 7.286458492279053
Epoch 510, val loss: 1.5165199041366577
Epoch 520, training loss: 0.03276680037379265 = 0.02548748441040516 + 0.001 * 7.279316425323486
Epoch 520, val loss: 1.543284296989441
Epoch 530, training loss: 0.030725523829460144 = 0.023453963920474052 + 0.001 * 7.2715606689453125
Epoch 530, val loss: 1.5693614482879639
Epoch 540, training loss: 0.02891778200864792 = 0.021646510809659958 + 0.001 * 7.271270751953125
Epoch 540, val loss: 1.5945401191711426
Epoch 550, training loss: 0.02730550616979599 = 0.020035414025187492 + 0.001 * 7.270092010498047
Epoch 550, val loss: 1.6190160512924194
Epoch 560, training loss: 0.025861624628305435 = 0.01859521120786667 + 0.001 * 7.266412258148193
Epoch 560, val loss: 1.6426669359207153
Epoch 570, training loss: 0.024573983624577522 = 0.017303751781582832 + 0.001 * 7.270231246948242
Epoch 570, val loss: 1.6655899286270142
Epoch 580, training loss: 0.02340037003159523 = 0.01614234782755375 + 0.001 * 7.258021354675293
Epoch 580, val loss: 1.687807559967041
Epoch 590, training loss: 0.022349948063492775 = 0.015094950795173645 + 0.001 * 7.254997253417969
Epoch 590, val loss: 1.7093455791473389
Epoch 600, training loss: 0.02140272781252861 = 0.014147757552564144 + 0.001 * 7.254969120025635
Epoch 600, val loss: 1.7302316427230835
Epoch 610, training loss: 0.02055356465280056 = 0.013288825750350952 + 0.001 * 7.264739036560059
Epoch 610, val loss: 1.7504161596298218
Epoch 620, training loss: 0.019762277603149414 = 0.012507886625826359 + 0.001 * 7.254390239715576
Epoch 620, val loss: 1.770022988319397
Epoch 630, training loss: 0.019044043496251106 = 0.011796027421951294 + 0.001 * 7.248016357421875
Epoch 630, val loss: 1.7890101671218872
Epoch 640, training loss: 0.018398268148303032 = 0.011145531199872494 + 0.001 * 7.2527360916137695
Epoch 640, val loss: 1.8074531555175781
Epoch 650, training loss: 0.017806226387619972 = 0.010549687780439854 + 0.001 * 7.2565388679504395
Epoch 650, val loss: 1.8253668546676636
Epoch 660, training loss: 0.01725146360695362 = 0.010002664290368557 + 0.001 * 7.248798847198486
Epoch 660, val loss: 1.842751145362854
Epoch 670, training loss: 0.016730235889554024 = 0.009499323554337025 + 0.001 * 7.230912685394287
Epoch 670, val loss: 1.8596673011779785
Epoch 680, training loss: 0.016262182965874672 = 0.009035207331180573 + 0.001 * 7.226974964141846
Epoch 680, val loss: 1.87608802318573
Epoch 690, training loss: 0.01583082228899002 = 0.00860636867582798 + 0.001 * 7.224453926086426
Epoch 690, val loss: 1.8920704126358032
Epoch 700, training loss: 0.015433279797434807 = 0.008209376595914364 + 0.001 * 7.223902225494385
Epoch 700, val loss: 1.907618522644043
Epoch 710, training loss: 0.015084621496498585 = 0.00784118939191103 + 0.001 * 7.243431568145752
Epoch 710, val loss: 1.9227895736694336
Epoch 720, training loss: 0.014709997922182083 = 0.0074991220608353615 + 0.001 * 7.210875511169434
Epoch 720, val loss: 1.937545895576477
Epoch 730, training loss: 0.014423897489905357 = 0.0071807443164289 + 0.001 * 7.243152141571045
Epoch 730, val loss: 1.9519859552383423
Epoch 740, training loss: 0.014093473553657532 = 0.006883925758302212 + 0.001 * 7.209547519683838
Epoch 740, val loss: 1.9659981727600098
Epoch 750, training loss: 0.013841668143868446 = 0.006606779992580414 + 0.001 * 7.23488712310791
Epoch 750, val loss: 1.9796359539031982
Epoch 760, training loss: 0.013553889468312263 = 0.006347597576677799 + 0.001 * 7.206291675567627
Epoch 760, val loss: 1.9930227994918823
Epoch 770, training loss: 0.013323514722287655 = 0.006104881409555674 + 0.001 * 7.21863317489624
Epoch 770, val loss: 2.006040334701538
Epoch 780, training loss: 0.0130971260368824 = 0.005877220071852207 + 0.001 * 7.219905376434326
Epoch 780, val loss: 2.018718719482422
Epoch 790, training loss: 0.012865381315350533 = 0.005663445219397545 + 0.001 * 7.201935291290283
Epoch 790, val loss: 2.031162738800049
Epoch 800, training loss: 0.012691990472376347 = 0.005462413188070059 + 0.001 * 7.22957706451416
Epoch 800, val loss: 2.0432891845703125
Epoch 810, training loss: 0.012474406510591507 = 0.005273138172924519 + 0.001 * 7.201267719268799
Epoch 810, val loss: 2.0550944805145264
Epoch 820, training loss: 0.0122930146753788 = 0.0050947405397892 + 0.001 * 7.198273181915283
Epoch 820, val loss: 2.0666697025299072
Epoch 830, training loss: 0.012123075313866138 = 0.004926385823637247 + 0.001 * 7.196689128875732
Epoch 830, val loss: 2.077984571456909
Epoch 840, training loss: 0.011935291811823845 = 0.0047673252411186695 + 0.001 * 7.167965888977051
Epoch 840, val loss: 2.0890591144561768
Epoch 850, training loss: 0.01181352324783802 = 0.0046168966218829155 + 0.001 * 7.196625709533691
Epoch 850, val loss: 2.099902629852295
Epoch 860, training loss: 0.011663677170872688 = 0.004474481102079153 + 0.001 * 7.1891961097717285
Epoch 860, val loss: 2.110524892807007
Epoch 870, training loss: 0.01154189370572567 = 0.004339525941759348 + 0.001 * 7.202366828918457
Epoch 870, val loss: 2.120896816253662
Epoch 880, training loss: 0.011383593082427979 = 0.0042115068063139915 + 0.001 * 7.172085285186768
Epoch 880, val loss: 2.1309876441955566
Epoch 890, training loss: 0.011257877573370934 = 0.004089968744665384 + 0.001 * 7.167908191680908
Epoch 890, val loss: 2.1409547328948975
Epoch 900, training loss: 0.01116412878036499 = 0.003974466118961573 + 0.001 * 7.189662933349609
Epoch 900, val loss: 2.150724172592163
Epoch 910, training loss: 0.01101725921034813 = 0.003864601254463196 + 0.001 * 7.152657985687256
Epoch 910, val loss: 2.1602296829223633
Epoch 920, training loss: 0.010909772478044033 = 0.0037600144278258085 + 0.001 * 7.149757385253906
Epoch 920, val loss: 2.169574499130249
Epoch 930, training loss: 0.01083105057477951 = 0.003660371992737055 + 0.001 * 7.17067813873291
Epoch 930, val loss: 2.178708076477051
Epoch 940, training loss: 0.010730305686593056 = 0.003565378487110138 + 0.001 * 7.164926528930664
Epoch 940, val loss: 2.1876649856567383
Epoch 950, training loss: 0.010645267553627491 = 0.0034747524186968803 + 0.001 * 7.1705145835876465
Epoch 950, val loss: 2.1965014934539795
Epoch 960, training loss: 0.010545979253947735 = 0.0033882043790072203 + 0.001 * 7.157774448394775
Epoch 960, val loss: 2.2050981521606445
Epoch 970, training loss: 0.010446708649396896 = 0.0033054957166314125 + 0.001 * 7.141211986541748
Epoch 970, val loss: 2.2135708332061768
Epoch 980, training loss: 0.010366412810981274 = 0.0032264140900224447 + 0.001 * 7.139997959136963
Epoch 980, val loss: 2.2218515872955322
Epoch 990, training loss: 0.01030801236629486 = 0.0031507222447544336 + 0.001 * 7.157289981842041
Epoch 990, val loss: 2.230010986328125
Epoch 1000, training loss: 0.010222859680652618 = 0.0030782422982156277 + 0.001 * 7.144617080688477
Epoch 1000, val loss: 2.2379887104034424
Epoch 1010, training loss: 0.010153387673199177 = 0.003008796600624919 + 0.001 * 7.144590377807617
Epoch 1010, val loss: 2.2458012104034424
Epoch 1020, training loss: 0.01009888295084238 = 0.002942226128652692 + 0.001 * 7.156656742095947
Epoch 1020, val loss: 2.253546714782715
Epoch 1030, training loss: 0.010009948164224625 = 0.002878371626138687 + 0.001 * 7.131576061248779
Epoch 1030, val loss: 2.261060953140259
Epoch 1040, training loss: 0.009974843822419643 = 0.0028170777950435877 + 0.001 * 7.157765865325928
Epoch 1040, val loss: 2.268540143966675
Epoch 1050, training loss: 0.009875568561255932 = 0.002758205635473132 + 0.001 * 7.1173624992370605
Epoch 1050, val loss: 2.275790214538574
Epoch 1060, training loss: 0.00985717587172985 = 0.002701654564589262 + 0.001 * 7.155521392822266
Epoch 1060, val loss: 2.2829995155334473
Epoch 1070, training loss: 0.009799320250749588 = 0.0026472732424736023 + 0.001 * 7.1520466804504395
Epoch 1070, val loss: 2.29002046585083
Epoch 1080, training loss: 0.009709213860332966 = 0.002594968071207404 + 0.001 * 7.114245891571045
Epoch 1080, val loss: 2.2969019412994385
Epoch 1090, training loss: 0.009666246362030506 = 0.0025446147192269564 + 0.001 * 7.121631145477295
Epoch 1090, val loss: 2.3037490844726562
Epoch 1100, training loss: 0.009636537171900272 = 0.0024961342569440603 + 0.001 * 7.140402793884277
Epoch 1100, val loss: 2.3104500770568848
Epoch 1110, training loss: 0.009555217809975147 = 0.002449435880407691 + 0.001 * 7.105781555175781
Epoch 1110, val loss: 2.3170273303985596
Epoch 1120, training loss: 0.009519972838461399 = 0.0024044355377554893 + 0.001 * 7.115537166595459
Epoch 1120, val loss: 2.3234753608703613
Epoch 1130, training loss: 0.009464170783758163 = 0.0023610396310687065 + 0.001 * 7.103130340576172
Epoch 1130, val loss: 2.329831838607788
Epoch 1140, training loss: 0.009426554664969444 = 0.002319180406630039 + 0.001 * 7.107373237609863
Epoch 1140, val loss: 2.3361449241638184
Epoch 1150, training loss: 0.009400146082043648 = 0.0022787731140851974 + 0.001 * 7.121372222900391
Epoch 1150, val loss: 2.3423075675964355
Epoch 1160, training loss: 0.00935294572263956 = 0.0022397737484425306 + 0.001 * 7.113171577453613
Epoch 1160, val loss: 2.3483426570892334
Epoch 1170, training loss: 0.00931603740900755 = 0.002202108269557357 + 0.001 * 7.11392879486084
Epoch 1170, val loss: 2.354306221008301
Epoch 1180, training loss: 0.009263655170798302 = 0.0021657245233654976 + 0.001 * 7.097929954528809
Epoch 1180, val loss: 2.3602232933044434
Epoch 1190, training loss: 0.009219212457537651 = 0.0021305459085851908 + 0.001 * 7.088665962219238
Epoch 1190, val loss: 2.365999460220337
Epoch 1200, training loss: 0.009191030636429787 = 0.002096527488902211 + 0.001 * 7.094502925872803
Epoch 1200, val loss: 2.3717031478881836
Epoch 1210, training loss: 0.009169798344373703 = 0.002063618740066886 + 0.001 * 7.106179714202881
Epoch 1210, val loss: 2.3772950172424316
Epoch 1220, training loss: 0.00913085788488388 = 0.0020317756570875645 + 0.001 * 7.099081516265869
Epoch 1220, val loss: 2.3828251361846924
Epoch 1230, training loss: 0.009099800139665604 = 0.0020009444560855627 + 0.001 * 7.098855495452881
Epoch 1230, val loss: 2.3882737159729004
Epoch 1240, training loss: 0.009055515751242638 = 0.001971077872440219 + 0.001 * 7.084437847137451
Epoch 1240, val loss: 2.3936870098114014
Epoch 1250, training loss: 0.009032093919813633 = 0.0019421533215790987 + 0.001 * 7.089940071105957
Epoch 1250, val loss: 2.3989739418029785
Epoch 1260, training loss: 0.009000785648822784 = 0.0019141297088935971 + 0.001 * 7.086655616760254
Epoch 1260, val loss: 2.4042487144470215
Epoch 1270, training loss: 0.008984883315861225 = 0.0018869629129767418 + 0.001 * 7.097919940948486
Epoch 1270, val loss: 2.4093663692474365
Epoch 1280, training loss: 0.008944679982960224 = 0.001860627788119018 + 0.001 * 7.084052085876465
Epoch 1280, val loss: 2.4144387245178223
Epoch 1290, training loss: 0.008930298499763012 = 0.0018350697355344892 + 0.001 * 7.09522819519043
Epoch 1290, val loss: 2.41943621635437
Epoch 1300, training loss: 0.008888039737939835 = 0.0018102617468684912 + 0.001 * 7.07777738571167
Epoch 1300, val loss: 2.424328327178955
Epoch 1310, training loss: 0.008871599100530148 = 0.0017861890373751521 + 0.001 * 7.085409641265869
Epoch 1310, val loss: 2.4292664527893066
Epoch 1320, training loss: 0.008874370716512203 = 0.0017628141213208437 + 0.001 * 7.111556053161621
Epoch 1320, val loss: 2.434074878692627
Epoch 1330, training loss: 0.008815821260213852 = 0.001740113366395235 + 0.001 * 7.07570743560791
Epoch 1330, val loss: 2.4387688636779785
Epoch 1340, training loss: 0.008780749514698982 = 0.001718052546493709 + 0.001 * 7.062696933746338
Epoch 1340, val loss: 2.4434609413146973
Epoch 1350, training loss: 0.008754384703934193 = 0.0016966148978099227 + 0.001 * 7.057769775390625
Epoch 1350, val loss: 2.4480955600738525
Epoch 1360, training loss: 0.008745106868445873 = 0.0016757744597271085 + 0.001 * 7.069331645965576
Epoch 1360, val loss: 2.452650547027588
Epoch 1370, training loss: 0.008728387765586376 = 0.0016554994508624077 + 0.001 * 7.072887897491455
Epoch 1370, val loss: 2.4571502208709717
Epoch 1380, training loss: 0.008706149645149708 = 0.0016357647255063057 + 0.001 * 7.070384502410889
Epoch 1380, val loss: 2.461576223373413
Epoch 1390, training loss: 0.008694827556610107 = 0.0016165527049452066 + 0.001 * 7.078274726867676
Epoch 1390, val loss: 2.4659602642059326
Epoch 1400, training loss: 0.0086746197193861 = 0.001597841503098607 + 0.001 * 7.076777458190918
Epoch 1400, val loss: 2.4703660011291504
Epoch 1410, training loss: 0.00864587351679802 = 0.001579591422341764 + 0.001 * 7.066282272338867
Epoch 1410, val loss: 2.4746334552764893
Epoch 1420, training loss: 0.008672382682561874 = 0.0015617928002029657 + 0.001 * 7.110589504241943
Epoch 1420, val loss: 2.478989839553833
Epoch 1430, training loss: 0.00861060805618763 = 0.0015444069867953658 + 0.001 * 7.066201210021973
Epoch 1430, val loss: 2.483139991760254
Epoch 1440, training loss: 0.008573479019105434 = 0.0015274204779416323 + 0.001 * 7.046058177947998
Epoch 1440, val loss: 2.487363576889038
Epoch 1450, training loss: 0.008560137823224068 = 0.0015107912477105856 + 0.001 * 7.049346446990967
Epoch 1450, val loss: 2.4915287494659424
Epoch 1460, training loss: 0.008528041653335094 = 0.0014945092843845487 + 0.001 * 7.033531665802002
Epoch 1460, val loss: 2.495652675628662
Epoch 1470, training loss: 0.008538840338587761 = 0.001478568185120821 + 0.001 * 7.060271739959717
Epoch 1470, val loss: 2.4997591972351074
Epoch 1480, training loss: 0.008519359864294529 = 0.001462928717955947 + 0.001 * 7.056430816650391
Epoch 1480, val loss: 2.503817081451416
Epoch 1490, training loss: 0.008519037626683712 = 0.0014475648058578372 + 0.001 * 7.071472644805908
Epoch 1490, val loss: 2.5078840255737305
Epoch 1500, training loss: 0.008523436263203621 = 0.0014324520016089082 + 0.001 * 7.0909833908081055
Epoch 1500, val loss: 2.511953115463257
Epoch 1510, training loss: 0.008440682664513588 = 0.0014175634132698178 + 0.001 * 7.023118495941162
Epoch 1510, val loss: 2.51594614982605
Epoch 1520, training loss: 0.008439631201326847 = 0.0014029069570824504 + 0.001 * 7.036724090576172
Epoch 1520, val loss: 2.5200343132019043
Epoch 1530, training loss: 0.00844626035541296 = 0.0013884564395993948 + 0.001 * 7.057803153991699
Epoch 1530, val loss: 2.5241012573242188
Epoch 1540, training loss: 0.008412227965891361 = 0.001374169485643506 + 0.001 * 7.038058280944824
Epoch 1540, val loss: 2.5281429290771484
Epoch 1550, training loss: 0.008397253230214119 = 0.0013600500533357263 + 0.001 * 7.03720235824585
Epoch 1550, val loss: 2.532196521759033
Epoch 1560, training loss: 0.00842762179672718 = 0.0013461117632687092 + 0.001 * 7.081509590148926
Epoch 1560, val loss: 2.5363407135009766
Epoch 1570, training loss: 0.00838618353009224 = 0.0013323510065674782 + 0.001 * 7.053832054138184
Epoch 1570, val loss: 2.5404951572418213
Epoch 1580, training loss: 0.008379422128200531 = 0.0013187454314902425 + 0.001 * 7.060676574707031
Epoch 1580, val loss: 2.544527769088745
Epoch 1590, training loss: 0.008346585556864738 = 0.0013052958529442549 + 0.001 * 7.041289329528809
Epoch 1590, val loss: 2.548750162124634
Epoch 1600, training loss: 0.008325068280100822 = 0.0012919947039335966 + 0.001 * 7.0330729484558105
Epoch 1600, val loss: 2.5530052185058594
Epoch 1610, training loss: 0.00830064807087183 = 0.001278818235732615 + 0.001 * 7.021829128265381
Epoch 1610, val loss: 2.5571441650390625
Epoch 1620, training loss: 0.008270484395325184 = 0.001265796134248376 + 0.001 * 7.004687786102295
Epoch 1620, val loss: 2.5614240169525146
Epoch 1630, training loss: 0.00829493347555399 = 0.001252915128134191 + 0.001 * 7.042017936706543
Epoch 1630, val loss: 2.5657246112823486
Epoch 1640, training loss: 0.008309976197779179 = 0.0012401960557326674 + 0.001 * 7.069779872894287
Epoch 1640, val loss: 2.569939136505127
Epoch 1650, training loss: 0.008252128027379513 = 0.001227635657414794 + 0.001 * 7.024491786956787
Epoch 1650, val loss: 2.5741469860076904
Epoch 1660, training loss: 0.008219487965106964 = 0.001215223572216928 + 0.001 * 7.0042643547058105
Epoch 1660, val loss: 2.5783729553222656
Epoch 1670, training loss: 0.008253627456724644 = 0.001202980405651033 + 0.001 * 7.050646781921387
Epoch 1670, val loss: 2.5827584266662598
Epoch 1680, training loss: 0.00823846273124218 = 0.0011908621527254581 + 0.001 * 7.047600269317627
Epoch 1680, val loss: 2.5869503021240234
Epoch 1690, training loss: 0.008195869624614716 = 0.0011789803393185139 + 0.001 * 7.016888618469238
Epoch 1690, val loss: 2.5912370681762695
Epoch 1700, training loss: 0.008240425027906895 = 0.0011672655818983912 + 0.001 * 7.073159217834473
Epoch 1700, val loss: 2.5955770015716553
Epoch 1710, training loss: 0.008196326903998852 = 0.00115576374810189 + 0.001 * 7.040562629699707
Epoch 1710, val loss: 2.5998525619506836
Epoch 1720, training loss: 0.00815582275390625 = 0.0011444195406511426 + 0.001 * 7.011402606964111
Epoch 1720, val loss: 2.604079484939575
Epoch 1730, training loss: 0.008141668513417244 = 0.0011332782451063395 + 0.001 * 7.008389472961426
Epoch 1730, val loss: 2.608421802520752
Epoch 1740, training loss: 0.008108972571790218 = 0.0011223192559555173 + 0.001 * 6.9866533279418945
Epoch 1740, val loss: 2.612694501876831
Epoch 1750, training loss: 0.008103148080408573 = 0.001111541991122067 + 0.001 * 6.991605281829834
Epoch 1750, val loss: 2.617025852203369
Epoch 1760, training loss: 0.008109940215945244 = 0.0011009435402229428 + 0.001 * 7.008996486663818
Epoch 1760, val loss: 2.6212339401245117
Epoch 1770, training loss: 0.008113979361951351 = 0.0010905724484473467 + 0.001 * 7.023406982421875
Epoch 1770, val loss: 2.6254289150238037
Epoch 1780, training loss: 0.008087176829576492 = 0.0010803896002471447 + 0.001 * 7.006786823272705
Epoch 1780, val loss: 2.629671096801758
Epoch 1790, training loss: 0.008058792911469936 = 0.00107041175942868 + 0.001 * 6.988381385803223
Epoch 1790, val loss: 2.633864164352417
Epoch 1800, training loss: 0.0080372653901577 = 0.0010606336873024702 + 0.001 * 6.9766316413879395
Epoch 1800, val loss: 2.6381587982177734
Epoch 1810, training loss: 0.008068186230957508 = 0.001051058410666883 + 0.001 * 7.017127990722656
Epoch 1810, val loss: 2.642350912094116
Epoch 1820, training loss: 0.00802692025899887 = 0.0010416628792881966 + 0.001 * 6.985257625579834
Epoch 1820, val loss: 2.6464765071868896
Epoch 1830, training loss: 0.00803526584059 = 0.0010324468603357673 + 0.001 * 7.002818584442139
Epoch 1830, val loss: 2.650649070739746
Epoch 1840, training loss: 0.008041251450777054 = 0.001023435383103788 + 0.001 * 7.017816066741943
Epoch 1840, val loss: 2.6547162532806396
Epoch 1850, training loss: 0.008051891811192036 = 0.001014605863019824 + 0.001 * 7.037285804748535
Epoch 1850, val loss: 2.6588430404663086
Epoch 1860, training loss: 0.008007694967091084 = 0.001005958067253232 + 0.001 * 7.001736640930176
Epoch 1860, val loss: 2.662790298461914
Epoch 1870, training loss: 0.00798715464770794 = 0.000997497234493494 + 0.001 * 6.989657402038574
Epoch 1870, val loss: 2.666757583618164
Epoch 1880, training loss: 0.00796008761972189 = 0.0009892068337649107 + 0.001 * 6.970880508422852
Epoch 1880, val loss: 2.6707515716552734
Epoch 1890, training loss: 0.007940717972815037 = 0.000981088844127953 + 0.001 * 6.959629058837891
Epoch 1890, val loss: 2.674755096435547
Epoch 1900, training loss: 0.0079538244754076 = 0.0009731592144817114 + 0.001 * 6.9806647300720215
Epoch 1900, val loss: 2.6787192821502686
Epoch 1910, training loss: 0.007966478355228901 = 0.0009654053137637675 + 0.001 * 7.001072883605957
Epoch 1910, val loss: 2.6825954914093018
Epoch 1920, training loss: 0.007960755378007889 = 0.0009578208555467427 + 0.001 * 7.002933979034424
Epoch 1920, val loss: 2.686405897140503
Epoch 1930, training loss: 0.00792721938341856 = 0.0009503859328106046 + 0.001 * 6.976833343505859
Epoch 1930, val loss: 2.690225601196289
Epoch 1940, training loss: 0.007924915291368961 = 0.0009431178914383054 + 0.001 * 6.981797218322754
Epoch 1940, val loss: 2.69406795501709
Epoch 1950, training loss: 0.007886783219873905 = 0.0009360135882161558 + 0.001 * 6.950769424438477
Epoch 1950, val loss: 2.6978015899658203
Epoch 1960, training loss: 0.007920508272945881 = 0.000929049332626164 + 0.001 * 6.991458892822266
Epoch 1960, val loss: 2.7014129161834717
Epoch 1970, training loss: 0.007896838709712029 = 0.0009222537046298385 + 0.001 * 6.974584579467773
Epoch 1970, val loss: 2.705260992050171
Epoch 1980, training loss: 0.00790880061686039 = 0.0009155934094451368 + 0.001 * 6.993206977844238
Epoch 1980, val loss: 2.7089855670928955
Epoch 1990, training loss: 0.007892011664807796 = 0.0009090747917070985 + 0.001 * 6.982936859130859
Epoch 1990, val loss: 2.7125179767608643
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 1.9661697149276733 = 1.9575728178024292 + 0.001 * 8.596842765808105
Epoch 0, val loss: 1.9642571210861206
Epoch 10, training loss: 1.955546498298645 = 1.9469497203826904 + 0.001 * 8.596760749816895
Epoch 10, val loss: 1.9529857635498047
Epoch 20, training loss: 1.942246913909912 = 1.9336503744125366 + 0.001 * 8.596541404724121
Epoch 20, val loss: 1.938715934753418
Epoch 30, training loss: 1.9235014915466309 = 1.9149054288864136 + 0.001 * 8.5960054397583
Epoch 30, val loss: 1.9185841083526611
Epoch 40, training loss: 1.8960264921188354 = 1.8874318599700928 + 0.001 * 8.594630241394043
Epoch 40, val loss: 1.8896321058273315
Epoch 50, training loss: 1.8589848279953003 = 1.8503944873809814 + 0.001 * 8.590361595153809
Epoch 50, val loss: 1.8526194095611572
Epoch 60, training loss: 1.8218560218811035 = 1.8132823705673218 + 0.001 * 8.573665618896484
Epoch 60, val loss: 1.8201631307601929
Epoch 70, training loss: 1.7952617406845093 = 1.7867603302001953 + 0.001 * 8.501463890075684
Epoch 70, val loss: 1.7998567819595337
Epoch 80, training loss: 1.7623897790908813 = 1.754168152809143 + 0.001 * 8.221671104431152
Epoch 80, val loss: 1.7715401649475098
Epoch 90, training loss: 1.7169463634490967 = 1.708819031715393 + 0.001 * 8.127311706542969
Epoch 90, val loss: 1.7318922281265259
Epoch 100, training loss: 1.6515074968338013 = 1.6435188055038452 + 0.001 * 7.988664627075195
Epoch 100, val loss: 1.6764254570007324
Epoch 110, training loss: 1.565894365310669 = 1.5580964088439941 + 0.001 * 7.797942161560059
Epoch 110, val loss: 1.6058200597763062
Epoch 120, training loss: 1.469947338104248 = 1.4623382091522217 + 0.001 * 7.609115123748779
Epoch 120, val loss: 1.5294747352600098
Epoch 130, training loss: 1.3738349676132202 = 1.3662954568862915 + 0.001 * 7.539503574371338
Epoch 130, val loss: 1.4554704427719116
Epoch 140, training loss: 1.2795193195343018 = 1.2720739841461182 + 0.001 * 7.445388317108154
Epoch 140, val loss: 1.3842629194259644
Epoch 150, training loss: 1.1866955757141113 = 1.179327130317688 + 0.001 * 7.368473529815674
Epoch 150, val loss: 1.3142544031143188
Epoch 160, training loss: 1.097511887550354 = 1.0901756286621094 + 0.001 * 7.336206436157227
Epoch 160, val loss: 1.2476108074188232
Epoch 170, training loss: 1.0149261951446533 = 1.007599115371704 + 0.001 * 7.327050685882568
Epoch 170, val loss: 1.1874371767044067
Epoch 180, training loss: 0.939243733882904 = 0.9319238662719727 + 0.001 * 7.3198933601379395
Epoch 180, val loss: 1.1339397430419922
Epoch 190, training loss: 0.8689072728157043 = 0.8615925312042236 + 0.001 * 7.31473445892334
Epoch 190, val loss: 1.0851879119873047
Epoch 200, training loss: 0.8025743961334229 = 0.7952635884284973 + 0.001 * 7.3107991218566895
Epoch 200, val loss: 1.0395429134368896
Epoch 210, training loss: 0.7404247522354126 = 0.7331185936927795 + 0.001 * 7.306151390075684
Epoch 210, val loss: 0.9972699284553528
Epoch 220, training loss: 0.6832383275032043 = 0.675937831401825 + 0.001 * 7.300492763519287
Epoch 220, val loss: 0.9593783617019653
Epoch 230, training loss: 0.6306756138801575 = 0.6233817934989929 + 0.001 * 7.293829917907715
Epoch 230, val loss: 0.9256414771080017
Epoch 240, training loss: 0.5810384750366211 = 0.5737514495849609 + 0.001 * 7.287016868591309
Epoch 240, val loss: 0.895424485206604
Epoch 250, training loss: 0.5324855446815491 = 0.5252065062522888 + 0.001 * 7.279055595397949
Epoch 250, val loss: 0.8682254552841187
Epoch 260, training loss: 0.484205961227417 = 0.4769357144832611 + 0.001 * 7.270251750946045
Epoch 260, val loss: 0.8444175720214844
Epoch 270, training loss: 0.4367046356201172 = 0.4294436573982239 + 0.001 * 7.26096773147583
Epoch 270, val loss: 0.8251523971557617
Epoch 280, training loss: 0.3912859857082367 = 0.384038507938385 + 0.001 * 7.247485637664795
Epoch 280, val loss: 0.8115721940994263
Epoch 290, training loss: 0.34915047883987427 = 0.3419110178947449 + 0.001 * 7.239459037780762
Epoch 290, val loss: 0.803917646408081
Epoch 300, training loss: 0.31075319647789 = 0.3035126030445099 + 0.001 * 7.240592002868652
Epoch 300, val loss: 0.8016319274902344
Epoch 310, training loss: 0.27576693892478943 = 0.26854652166366577 + 0.001 * 7.220410346984863
Epoch 310, val loss: 0.8032557368278503
Epoch 320, training loss: 0.24375663697719574 = 0.23654213547706604 + 0.001 * 7.214506149291992
Epoch 320, val loss: 0.8078620433807373
Epoch 330, training loss: 0.21442967653274536 = 0.20721930265426636 + 0.001 * 7.210380554199219
Epoch 330, val loss: 0.8147311806678772
Epoch 340, training loss: 0.18776030838489532 = 0.1805475503206253 + 0.001 * 7.212764263153076
Epoch 340, val loss: 0.8234421610832214
Epoch 350, training loss: 0.1638532280921936 = 0.15664927661418915 + 0.001 * 7.203952312469482
Epoch 350, val loss: 0.833832323551178
Epoch 360, training loss: 0.14276553690433502 = 0.13556522130966187 + 0.001 * 7.200319290161133
Epoch 360, val loss: 0.8457581996917725
Epoch 370, training loss: 0.12437914311885834 = 0.11718207597732544 + 0.001 * 7.197066307067871
Epoch 370, val loss: 0.8587082624435425
Epoch 380, training loss: 0.10849551111459732 = 0.1012992113828659 + 0.001 * 7.196296691894531
Epoch 380, val loss: 0.8723231554031372
Epoch 390, training loss: 0.09482545405626297 = 0.08763185888528824 + 0.001 * 7.193592071533203
Epoch 390, val loss: 0.8860859274864197
Epoch 400, training loss: 0.0831260085105896 = 0.07592792809009552 + 0.001 * 7.198083400726318
Epoch 400, val loss: 0.8999205231666565
Epoch 410, training loss: 0.0731610655784607 = 0.06597374379634857 + 0.001 * 7.187319755554199
Epoch 410, val loss: 0.9136903882026672
Epoch 420, training loss: 0.06471683830022812 = 0.057532500475645065 + 0.001 * 7.184338092803955
Epoch 420, val loss: 0.9274638295173645
Epoch 430, training loss: 0.05759840086102486 = 0.05039481073617935 + 0.001 * 7.20358943939209
Epoch 430, val loss: 0.9411522746086121
Epoch 440, training loss: 0.051536258310079575 = 0.04435507208108902 + 0.001 * 7.181184768676758
Epoch 440, val loss: 0.954866886138916
Epoch 450, training loss: 0.046414557844400406 = 0.03924320265650749 + 0.001 * 7.171353340148926
Epoch 450, val loss: 0.9685696363449097
Epoch 460, training loss: 0.04210042208433151 = 0.03489958122372627 + 0.001 * 7.200841903686523
Epoch 460, val loss: 0.9820750951766968
Epoch 470, training loss: 0.03836989402770996 = 0.031198173761367798 + 0.001 * 7.171720504760742
Epoch 470, val loss: 0.9953089952468872
Epoch 480, training loss: 0.0351950079202652 = 0.028031261637806892 + 0.001 * 7.163745880126953
Epoch 480, val loss: 1.0082117319107056
Epoch 490, training loss: 0.032466839998960495 = 0.0253105778247118 + 0.001 * 7.1562628746032715
Epoch 490, val loss: 1.0207678079605103
Epoch 500, training loss: 0.03011227585375309 = 0.022961923852562904 + 0.001 * 7.150351524353027
Epoch 500, val loss: 1.0329633951187134
Epoch 510, training loss: 0.028071468695998192 = 0.020924516022205353 + 0.001 * 7.146951675415039
Epoch 510, val loss: 1.0448323488235474
Epoch 520, training loss: 0.026312608271837234 = 0.0191484484821558 + 0.001 * 7.164159774780273
Epoch 520, val loss: 1.0563243627548218
Epoch 530, training loss: 0.024754934012889862 = 0.017592843621969223 + 0.001 * 7.162090301513672
Epoch 530, val loss: 1.0674766302108765
Epoch 540, training loss: 0.023374831303954124 = 0.016223760321736336 + 0.001 * 7.1510701179504395
Epoch 540, val loss: 1.0782520771026611
Epoch 550, training loss: 0.02213318832218647 = 0.01501263864338398 + 0.001 * 7.12054967880249
Epoch 550, val loss: 1.0886342525482178
Epoch 560, training loss: 0.021056169643998146 = 0.013935920782387257 + 0.001 * 7.120248794555664
Epoch 560, val loss: 1.098700761795044
Epoch 570, training loss: 0.020093385130167007 = 0.01297339703887701 + 0.001 * 7.1199870109558105
Epoch 570, val loss: 1.1084027290344238
Epoch 580, training loss: 0.01924562081694603 = 0.01210818625986576 + 0.001 * 7.137434005737305
Epoch 580, val loss: 1.1178383827209473
Epoch 590, training loss: 0.01842990145087242 = 0.01132610160857439 + 0.001 * 7.1037983894348145
Epoch 590, val loss: 1.1269594430923462
Epoch 600, training loss: 0.01774291880428791 = 0.010616030544042587 + 0.001 * 7.126888275146484
Epoch 600, val loss: 1.1358144283294678
Epoch 610, training loss: 0.017075873911380768 = 0.009969616308808327 + 0.001 * 7.106257438659668
Epoch 610, val loss: 1.1444156169891357
Epoch 620, training loss: 0.016477376222610474 = 0.009379955008625984 + 0.001 * 7.097421169281006
Epoch 620, val loss: 1.1527283191680908
Epoch 630, training loss: 0.01593262143433094 = 0.008841012604534626 + 0.001 * 7.091609001159668
Epoch 630, val loss: 1.1608213186264038
Epoch 640, training loss: 0.015459183603525162 = 0.008347486145794392 + 0.001 * 7.111696720123291
Epoch 640, val loss: 1.1686723232269287
Epoch 650, training loss: 0.014987688511610031 = 0.007894917391240597 + 0.001 * 7.092770576477051
Epoch 650, val loss: 1.1762957572937012
Epoch 660, training loss: 0.014565037563443184 = 0.007479147519916296 + 0.001 * 7.085890293121338
Epoch 660, val loss: 1.1837060451507568
Epoch 670, training loss: 0.014212140813469887 = 0.007096501998603344 + 0.001 * 7.115638256072998
Epoch 670, val loss: 1.190893530845642
Epoch 680, training loss: 0.013838028535246849 = 0.006743703968822956 + 0.001 * 7.094324111938477
Epoch 680, val loss: 1.1978857517242432
Epoch 690, training loss: 0.013485811650753021 = 0.0064178695902228355 + 0.001 * 7.067942142486572
Epoch 690, val loss: 1.2046427726745605
Epoch 700, training loss: 0.01317751593887806 = 0.006116325035691261 + 0.001 * 7.061190128326416
Epoch 700, val loss: 1.211243748664856
Epoch 710, training loss: 0.01292526163160801 = 0.00583679461851716 + 0.001 * 7.088466644287109
Epoch 710, val loss: 1.217681884765625
Epoch 720, training loss: 0.012671222910284996 = 0.00557729322463274 + 0.001 * 7.093928813934326
Epoch 720, val loss: 1.2239347696304321
Epoch 730, training loss: 0.012412085197865963 = 0.005336003843694925 + 0.001 * 7.076080799102783
Epoch 730, val loss: 1.2299785614013672
Epoch 740, training loss: 0.01215386763215065 = 0.005111260339617729 + 0.001 * 7.042606353759766
Epoch 740, val loss: 1.2358648777008057
Epoch 750, training loss: 0.011969106271862984 = 0.004901594948023558 + 0.001 * 7.067511558532715
Epoch 750, val loss: 1.24165940284729
Epoch 760, training loss: 0.01177210733294487 = 0.004705748055130243 + 0.001 * 7.066359043121338
Epoch 760, val loss: 1.2472668886184692
Epoch 770, training loss: 0.011560877785086632 = 0.004522508475929499 + 0.001 * 7.0383687019348145
Epoch 770, val loss: 1.2527215480804443
Epoch 780, training loss: 0.011424158699810505 = 0.004350842908024788 + 0.001 * 7.073315620422363
Epoch 780, val loss: 1.2580814361572266
Epoch 790, training loss: 0.011272500269114971 = 0.0041898153722286224 + 0.001 * 7.082684516906738
Epoch 790, val loss: 1.2632302045822144
Epoch 800, training loss: 0.011083995923399925 = 0.00403854064643383 + 0.001 * 7.045455455780029
Epoch 800, val loss: 1.2682913541793823
Epoch 810, training loss: 0.010928962379693985 = 0.0038962571416050196 + 0.001 * 7.032705307006836
Epoch 810, val loss: 1.2732611894607544
Epoch 820, training loss: 0.0107977744191885 = 0.003762272885069251 + 0.001 * 7.035501480102539
Epoch 820, val loss: 1.2780612707138062
Epoch 830, training loss: 0.010662349872291088 = 0.0036359592340886593 + 0.001 * 7.026390075683594
Epoch 830, val loss: 1.2827457189559937
Epoch 840, training loss: 0.010548150166869164 = 0.003516734577715397 + 0.001 * 7.031415939331055
Epoch 840, val loss: 1.2873457670211792
Epoch 850, training loss: 0.010444436222314835 = 0.0034040850587189198 + 0.001 * 7.040351390838623
Epoch 850, val loss: 1.2918248176574707
Epoch 860, training loss: 0.010323951952159405 = 0.0032975554931908846 + 0.001 * 7.026395797729492
Epoch 860, val loss: 1.296129584312439
Epoch 870, training loss: 0.010239220224320889 = 0.0031966911628842354 + 0.001 * 7.0425286293029785
Epoch 870, val loss: 1.3004194498062134
Epoch 880, training loss: 0.010144397616386414 = 0.0031011125538498163 + 0.001 * 7.0432844161987305
Epoch 880, val loss: 1.3046058416366577
Epoch 890, training loss: 0.010024606250226498 = 0.003010432468727231 + 0.001 * 7.01417350769043
Epoch 890, val loss: 1.3086479902267456
Epoch 900, training loss: 0.0099737998098135 = 0.002924339147284627 + 0.001 * 7.049460411071777
Epoch 900, val loss: 1.312638521194458
Epoch 910, training loss: 0.009866396896541119 = 0.0028424973133951426 + 0.001 * 7.023899078369141
Epoch 910, val loss: 1.3165160417556763
Epoch 920, training loss: 0.009817015379667282 = 0.002764661330729723 + 0.001 * 7.052353382110596
