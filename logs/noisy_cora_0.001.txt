Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0001, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11672])
remove edge: torch.Size([2, 9546])
updated graph: torch.Size([2, 10662])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9560389518737793 = 1.9474420547485352 + 0.001 * 8.596857070922852
Epoch 0, val loss: 1.9505759477615356
Epoch 10, training loss: 1.9511699676513672 = 1.942573070526123 + 0.001 * 8.596845626831055
Epoch 10, val loss: 1.9454731941223145
Epoch 20, training loss: 1.9461326599121094 = 1.9375358819961548 + 0.001 * 8.596796035766602
Epoch 20, val loss: 1.9400655031204224
Epoch 30, training loss: 1.940365195274353 = 1.931768536567688 + 0.001 * 8.596697807312012
Epoch 30, val loss: 1.9337775707244873
Epoch 40, training loss: 1.9333876371383667 = 1.9247910976409912 + 0.001 * 8.596517562866211
Epoch 40, val loss: 1.9261201620101929
Epoch 50, training loss: 1.9246325492858887 = 1.9160363674163818 + 0.001 * 8.596179962158203
Epoch 50, val loss: 1.9165124893188477
Epoch 60, training loss: 1.9134396314620972 = 1.904844045639038 + 0.001 * 8.595529556274414
Epoch 60, val loss: 1.9043127298355103
Epoch 70, training loss: 1.899213433265686 = 1.890619158744812 + 0.001 * 8.594247817993164
Epoch 70, val loss: 1.8890107870101929
Epoch 80, training loss: 1.8816760778427124 = 1.8730844259262085 + 0.001 * 8.591668128967285
Epoch 80, val loss: 1.8705861568450928
Epoch 90, training loss: 1.8613405227661133 = 1.8527544736862183 + 0.001 * 8.586084365844727
Epoch 90, val loss: 1.8501412868499756
Epoch 100, training loss: 1.8398493528366089 = 1.831276535987854 + 0.001 * 8.572819709777832
Epoch 100, val loss: 1.830154299736023
Epoch 110, training loss: 1.819696307182312 = 1.8111579418182373 + 0.001 * 8.538421630859375
Epoch 110, val loss: 1.8135398626327515
Epoch 120, training loss: 1.801956057548523 = 1.7935254573822021 + 0.001 * 8.43054485321045
Epoch 120, val loss: 1.800698161125183
Epoch 130, training loss: 1.7845134735107422 = 1.77633798122406 + 0.001 * 8.17549991607666
Epoch 130, val loss: 1.7882786989212036
Epoch 140, training loss: 1.764817237854004 = 1.7566906213760376 + 0.001 * 8.126628875732422
Epoch 140, val loss: 1.772852897644043
Epoch 150, training loss: 1.7412043809890747 = 1.7331478595733643 + 0.001 * 8.056574821472168
Epoch 150, val loss: 1.7533622980117798
Epoch 160, training loss: 1.7125046253204346 = 1.7045726776123047 + 0.001 * 7.931941986083984
Epoch 160, val loss: 1.7292429208755493
Epoch 170, training loss: 1.6779627799987793 = 1.6702008247375488 + 0.001 * 7.761896133422852
Epoch 170, val loss: 1.7002453804016113
Epoch 180, training loss: 1.6379234790802002 = 1.6302934885025024 + 0.001 * 7.629995822906494
Epoch 180, val loss: 1.6665502786636353
Epoch 190, training loss: 1.593247890472412 = 1.585685133934021 + 0.001 * 7.562794208526611
Epoch 190, val loss: 1.628998041152954
Epoch 200, training loss: 1.545035719871521 = 1.5375088453292847 + 0.001 * 7.526882171630859
Epoch 200, val loss: 1.5889993906021118
Epoch 210, training loss: 1.494152545928955 = 1.486661672592163 + 0.001 * 7.490830898284912
Epoch 210, val loss: 1.547803282737732
Epoch 220, training loss: 1.4413090944290161 = 1.4338676929473877 + 0.001 * 7.441351890563965
Epoch 220, val loss: 1.5058735609054565
Epoch 230, training loss: 1.3867106437683105 = 1.3793292045593262 + 0.001 * 7.381392002105713
Epoch 230, val loss: 1.4631249904632568
Epoch 240, training loss: 1.33053719997406 = 1.323215126991272 + 0.001 * 7.3220534324646
Epoch 240, val loss: 1.4193594455718994
Epoch 250, training loss: 1.2730330228805542 = 1.2657499313354492 + 0.001 * 7.283067226409912
Epoch 250, val loss: 1.3743349313735962
Epoch 260, training loss: 1.2146461009979248 = 1.2073785066604614 + 0.001 * 7.2676167488098145
Epoch 260, val loss: 1.3284416198730469
Epoch 270, training loss: 1.1561123132705688 = 1.1488516330718994 + 0.001 * 7.26062536239624
Epoch 270, val loss: 1.282474160194397
Epoch 280, training loss: 1.0983134508132935 = 1.0910611152648926 + 0.001 * 7.252357006072998
Epoch 280, val loss: 1.2371517419815063
Epoch 290, training loss: 1.0421526432037354 = 1.0349102020263672 + 0.001 * 7.242441177368164
Epoch 290, val loss: 1.1934882402420044
Epoch 300, training loss: 0.9884621500968933 = 0.9812326431274414 + 0.001 * 7.229516506195068
Epoch 300, val loss: 1.1522883176803589
Epoch 310, training loss: 0.93792325258255 = 0.9307098984718323 + 0.001 * 7.213357925415039
Epoch 310, val loss: 1.1143980026245117
Epoch 320, training loss: 0.8909762501716614 = 0.8837779760360718 + 0.001 * 7.198283672332764
Epoch 320, val loss: 1.079986810684204
Epoch 330, training loss: 0.8477509021759033 = 0.8405677080154419 + 0.001 * 7.183182716369629
Epoch 330, val loss: 1.0492323637008667
Epoch 340, training loss: 0.8081179261207581 = 0.800944983959198 + 0.001 * 7.17291259765625
Epoch 340, val loss: 1.0219806432724
Epoch 350, training loss: 0.7717674374580383 = 0.7646058797836304 + 0.001 * 7.16152811050415
Epoch 350, val loss: 0.9979448318481445
Epoch 360, training loss: 0.7382885217666626 = 0.7311362624168396 + 0.001 * 7.152283191680908
Epoch 360, val loss: 0.9768562316894531
Epoch 370, training loss: 0.7072310447692871 = 0.7000867128372192 + 0.001 * 7.144350051879883
Epoch 370, val loss: 0.9582997560501099
Epoch 380, training loss: 0.6781529188156128 = 0.6710138320922852 + 0.001 * 7.139077186584473
Epoch 380, val loss: 0.9418676495552063
Epoch 390, training loss: 0.6506507992744446 = 0.6435173749923706 + 0.001 * 7.133400917053223
Epoch 390, val loss: 0.9271802306175232
Epoch 400, training loss: 0.624373197555542 = 0.617245614528656 + 0.001 * 7.127572536468506
Epoch 400, val loss: 0.9140251278877258
Epoch 410, training loss: 0.5990320444107056 = 0.5919085144996643 + 0.001 * 7.123522758483887
Epoch 410, val loss: 0.902208685874939
Epoch 420, training loss: 0.5743934512138367 = 0.5672730207443237 + 0.001 * 7.1204328536987305
Epoch 420, val loss: 0.8914645910263062
Epoch 430, training loss: 0.5503090620040894 = 0.543192446231842 + 0.001 * 7.116607666015625
Epoch 430, val loss: 0.881822943687439
Epoch 440, training loss: 0.5266574025154114 = 0.5195441246032715 + 0.001 * 7.113284111022949
Epoch 440, val loss: 0.873184323310852
Epoch 450, training loss: 0.5033726096153259 = 0.4962630271911621 + 0.001 * 7.10955286026001
Epoch 450, val loss: 0.865623414516449
Epoch 460, training loss: 0.48044878244400024 = 0.47334209084510803 + 0.001 * 7.106703281402588
Epoch 460, val loss: 0.8591393828392029
Epoch 470, training loss: 0.4579257369041443 = 0.4508206844329834 + 0.001 * 7.1050591468811035
Epoch 470, val loss: 0.8538573384284973
Epoch 480, training loss: 0.435836523771286 = 0.4287342131137848 + 0.001 * 7.102303981781006
Epoch 480, val loss: 0.8498592376708984
Epoch 490, training loss: 0.41425830125808716 = 0.4071584641933441 + 0.001 * 7.099822044372559
Epoch 490, val loss: 0.8471834063529968
Epoch 500, training loss: 0.39326003193855286 = 0.3861626088619232 + 0.001 * 7.097411632537842
Epoch 500, val loss: 0.8458269834518433
Epoch 510, training loss: 0.37288907170295715 = 0.3657929301261902 + 0.001 * 7.096155166625977
Epoch 510, val loss: 0.8458674550056458
Epoch 520, training loss: 0.35317471623420715 = 0.34608110785484314 + 0.001 * 7.093620777130127
Epoch 520, val loss: 0.847229540348053
Epoch 530, training loss: 0.33414193987846375 = 0.3270511031150818 + 0.001 * 7.090841293334961
Epoch 530, val loss: 0.849878191947937
Epoch 540, training loss: 0.31580063700675964 = 0.308711975812912 + 0.001 * 7.088654041290283
Epoch 540, val loss: 0.8537741303443909
Epoch 550, training loss: 0.29816630482673645 = 0.291079044342041 + 0.001 * 7.087251663208008
Epoch 550, val loss: 0.8587712645530701
Epoch 560, training loss: 0.28127098083496094 = 0.2741863429546356 + 0.001 * 7.084626197814941
Epoch 560, val loss: 0.8648260831832886
Epoch 570, training loss: 0.26514479517936707 = 0.25806063413619995 + 0.001 * 7.0841522216796875
Epoch 570, val loss: 0.8719226121902466
Epoch 580, training loss: 0.24979674816131592 = 0.24271486699581146 + 0.001 * 7.081876754760742
Epoch 580, val loss: 0.879934549331665
Epoch 590, training loss: 0.23523926734924316 = 0.22816000878810883 + 0.001 * 7.079261302947998
Epoch 590, val loss: 0.8887678384780884
Epoch 600, training loss: 0.22147619724273682 = 0.21439848840236664 + 0.001 * 7.077703475952148
Epoch 600, val loss: 0.8983829617500305
Epoch 610, training loss: 0.20849359035491943 = 0.20141953229904175 + 0.001 * 7.074060440063477
Epoch 610, val loss: 0.908730149269104
Epoch 620, training loss: 0.19627702236175537 = 0.18920622766017914 + 0.001 * 7.070798397064209
Epoch 620, val loss: 0.9197613596916199
Epoch 630, training loss: 0.1848127692937851 = 0.17773041129112244 + 0.001 * 7.082361221313477
Epoch 630, val loss: 0.9314156770706177
Epoch 640, training loss: 0.17403091490268707 = 0.16696298122406006 + 0.001 * 7.067937850952148
Epoch 640, val loss: 0.9436243176460266
Epoch 650, training loss: 0.16393740475177765 = 0.15687182545661926 + 0.001 * 7.06558084487915
Epoch 650, val loss: 0.9563097953796387
Epoch 660, training loss: 0.15448105335235596 = 0.14741910994052887 + 0.001 * 7.061944007873535
Epoch 660, val loss: 0.9694352746009827
Epoch 670, training loss: 0.14564360678195953 = 0.13857164978981018 + 0.001 * 7.0719523429870605
Epoch 670, val loss: 0.9829410314559937
Epoch 680, training loss: 0.13734731078147888 = 0.1302930861711502 + 0.001 * 7.054224491119385
Epoch 680, val loss: 0.996795117855072
Epoch 690, training loss: 0.12959955632686615 = 0.12254822999238968 + 0.001 * 7.0513200759887695
Epoch 690, val loss: 1.0109552145004272
Epoch 700, training loss: 0.12235354632139206 = 0.11530394107103348 + 0.001 * 7.049602508544922
Epoch 700, val loss: 1.0253947973251343
Epoch 710, training loss: 0.11559324711561203 = 0.10852750390768051 + 0.001 * 7.065739631652832
Epoch 710, val loss: 1.0400173664093018
Epoch 720, training loss: 0.10923141986131668 = 0.1021893173456192 + 0.001 * 7.0420989990234375
Epoch 720, val loss: 1.0548248291015625
Epoch 730, training loss: 0.10330324620008469 = 0.09626320749521255 + 0.001 * 7.040036201477051
Epoch 730, val loss: 1.0697970390319824
Epoch 740, training loss: 0.09775983542203903 = 0.09072423726320267 + 0.001 * 7.0355987548828125
Epoch 740, val loss: 1.0848942995071411
Epoch 750, training loss: 0.09258243441581726 = 0.08554749190807343 + 0.001 * 7.034938812255859
Epoch 750, val loss: 1.1000850200653076
Epoch 760, training loss: 0.08775077760219574 = 0.08071088045835495 + 0.001 * 7.039895534515381
Epoch 760, val loss: 1.115330457687378
Epoch 770, training loss: 0.08322567492723465 = 0.07619200646877289 + 0.001 * 7.033666133880615
Epoch 770, val loss: 1.1306384801864624
Epoch 780, training loss: 0.07899423688650131 = 0.0719698891043663 + 0.001 * 7.02434778213501
Epoch 780, val loss: 1.1459732055664062
Epoch 790, training loss: 0.07507940381765366 = 0.06802500039339066 + 0.001 * 7.054400444030762
Epoch 790, val loss: 1.1613199710845947
Epoch 800, training loss: 0.07137202471494675 = 0.06433874368667603 + 0.001 * 7.033280849456787
Epoch 800, val loss: 1.1766548156738281
Epoch 810, training loss: 0.06791584938764572 = 0.060893625020980835 + 0.001 * 7.022222995758057
Epoch 810, val loss: 1.1919679641723633
Epoch 820, training loss: 0.0646933913230896 = 0.05767323076725006 + 0.001 * 7.02016019821167
Epoch 820, val loss: 1.207228660583496
Epoch 830, training loss: 0.06168123707175255 = 0.054661963135004044 + 0.001 * 7.0192742347717285
Epoch 830, val loss: 1.2223880290985107
Epoch 840, training loss: 0.058863550424575806 = 0.05184505879878998 + 0.001 * 7.018491268157959
Epoch 840, val loss: 1.2374603748321533
Epoch 850, training loss: 0.056217025965452194 = 0.049209047108888626 + 0.001 * 7.00797700881958
Epoch 850, val loss: 1.2524168491363525
Epoch 860, training loss: 0.053756676614284515 = 0.04674117639660835 + 0.001 * 7.015499591827393
Epoch 860, val loss: 1.2672492265701294
Epoch 870, training loss: 0.05144374072551727 = 0.04442981258034706 + 0.001 * 7.013928413391113
Epoch 870, val loss: 1.281929850578308
Epoch 880, training loss: 0.049267783761024475 = 0.04226385056972504 + 0.001 * 7.003931999206543
Epoch 880, val loss: 1.2964422702789307
Epoch 890, training loss: 0.04725215584039688 = 0.04023308306932449 + 0.001 * 7.019071102142334
Epoch 890, val loss: 1.310788631439209
Epoch 900, training loss: 0.0453425794839859 = 0.03832821175456047 + 0.001 * 7.0143656730651855
Epoch 900, val loss: 1.3249410390853882
Epoch 910, training loss: 0.04354139789938927 = 0.03654053434729576 + 0.001 * 7.000864028930664
Epoch 910, val loss: 1.3389207124710083
Epoch 920, training loss: 0.04186059534549713 = 0.034861885011196136 + 0.001 * 6.9987101554870605
Epoch 920, val loss: 1.352686882019043
Epoch 930, training loss: 0.040282923728227615 = 0.033284708857536316 + 0.001 * 6.998214244842529
Epoch 930, val loss: 1.3662686347961426
Epoch 940, training loss: 0.038810089230537415 = 0.03180215135216713 + 0.001 * 7.007939338684082
Epoch 940, val loss: 1.3796478509902954
Epoch 950, training loss: 0.03740619868040085 = 0.030407670885324478 + 0.001 * 6.998525619506836
Epoch 950, val loss: 1.3928321599960327
Epoch 960, training loss: 0.036085788160562515 = 0.029095178470015526 + 0.001 * 6.990607738494873
Epoch 960, val loss: 1.4057928323745728
Epoch 970, training loss: 0.03485730290412903 = 0.027859050780534744 + 0.001 * 6.9982500076293945
Epoch 970, val loss: 1.4185857772827148
Epoch 980, training loss: 0.033680204302072525 = 0.026694033294916153 + 0.001 * 6.986169338226318
Epoch 980, val loss: 1.431165337562561
Epoch 990, training loss: 0.03257734328508377 = 0.025594374164938927 + 0.001 * 6.982968330383301
Epoch 990, val loss: 1.4435362815856934
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 1.9634758234024048 = 1.9548789262771606 + 0.001 * 8.596839904785156
Epoch 0, val loss: 1.9484800100326538
Epoch 10, training loss: 1.9582313299179077 = 1.9496345520019531 + 0.001 * 8.596831321716309
Epoch 10, val loss: 1.9432241916656494
Epoch 20, training loss: 1.9526349306106567 = 1.9440381526947021 + 0.001 * 8.596790313720703
Epoch 20, val loss: 1.937591314315796
Epoch 30, training loss: 1.9461019039154053 = 1.9375052452087402 + 0.001 * 8.596710205078125
Epoch 30, val loss: 1.9309812784194946
Epoch 40, training loss: 1.9380848407745361 = 1.9294883012771606 + 0.001 * 8.596562385559082
Epoch 40, val loss: 1.9228280782699585
Epoch 50, training loss: 1.9279669523239136 = 1.9193706512451172 + 0.001 * 8.59628677368164
Epoch 50, val loss: 1.9125436544418335
Epoch 60, training loss: 1.9150770902633667 = 1.9064812660217285 + 0.001 * 8.595766067504883
Epoch 60, val loss: 1.8995630741119385
Epoch 70, training loss: 1.8988112211227417 = 1.8902164697647095 + 0.001 * 8.594749450683594
Epoch 70, val loss: 1.8834974765777588
Epoch 80, training loss: 1.8790606260299683 = 1.870468020439148 + 0.001 * 8.592643737792969
Epoch 80, val loss: 1.8645588159561157
Epoch 90, training loss: 1.85659658908844 = 1.848008632659912 + 0.001 * 8.587902069091797
Epoch 90, val loss: 1.8439682722091675
Epoch 100, training loss: 1.8334698677062988 = 1.8248932361602783 + 0.001 * 8.57659912109375
Epoch 100, val loss: 1.824144721031189
Epoch 110, training loss: 1.8121507167816162 = 1.8036030530929565 + 0.001 * 8.547693252563477
Epoch 110, val loss: 1.8073650598526
Epoch 120, training loss: 1.792843222618103 = 1.784381628036499 + 0.001 * 8.461602210998535
Epoch 120, val loss: 1.7930219173431396
Epoch 130, training loss: 1.772812008857727 = 1.7646352052688599 + 0.001 * 8.176839828491211
Epoch 130, val loss: 1.7777962684631348
Epoch 140, training loss: 1.7499033212661743 = 1.7418291568756104 + 0.001 * 8.074187278747559
Epoch 140, val loss: 1.7591898441314697
Epoch 150, training loss: 1.7224829196929932 = 1.7144596576690674 + 0.001 * 8.023268699645996
Epoch 150, val loss: 1.736160397529602
Epoch 160, training loss: 1.6893630027770996 = 1.6813933849334717 + 0.001 * 7.969597816467285
Epoch 160, val loss: 1.707903265953064
Epoch 170, training loss: 1.6503829956054688 = 1.6424753665924072 + 0.001 * 7.9076128005981445
Epoch 170, val loss: 1.6748136281967163
Epoch 180, training loss: 1.6065434217453003 = 1.5987131595611572 + 0.001 * 7.830298900604248
Epoch 180, val loss: 1.6384068727493286
Epoch 190, training loss: 1.5597974061965942 = 1.5520925521850586 + 0.001 * 7.704887390136719
Epoch 190, val loss: 1.6004325151443481
Epoch 200, training loss: 1.5121501684188843 = 1.504645824432373 + 0.001 * 7.504384994506836
Epoch 200, val loss: 1.5624730587005615
Epoch 210, training loss: 1.4649823904037476 = 1.45766019821167 + 0.001 * 7.322219371795654
Epoch 210, val loss: 1.5260698795318604
Epoch 220, training loss: 1.4190845489501953 = 1.4117952585220337 + 0.001 * 7.289330005645752
Epoch 220, val loss: 1.4912909269332886
Epoch 230, training loss: 1.374393105506897 = 1.3671362400054932 + 0.001 * 7.2568583488464355
Epoch 230, val loss: 1.4580810070037842
Epoch 240, training loss: 1.3306714296340942 = 1.3234319686889648 + 0.001 * 7.239430904388428
Epoch 240, val loss: 1.4262477159500122
Epoch 250, training loss: 1.2874860763549805 = 1.2802541255950928 + 0.001 * 7.2320075035095215
Epoch 250, val loss: 1.395569920539856
Epoch 260, training loss: 1.2445095777511597 = 1.237280249595642 + 0.001 * 7.229305744171143
Epoch 260, val loss: 1.365543007850647
Epoch 270, training loss: 1.2013872861862183 = 1.194159984588623 + 0.001 * 7.2273054122924805
Epoch 270, val loss: 1.3358361721038818
Epoch 280, training loss: 1.1577953100204468 = 1.150568962097168 + 0.001 * 7.2263970375061035
Epoch 280, val loss: 1.30601966381073
Epoch 290, training loss: 1.1134324073791504 = 1.1062068939208984 + 0.001 * 7.225496292114258
Epoch 290, val loss: 1.2755874395370483
Epoch 300, training loss: 1.0682326555252075 = 1.0610085725784302 + 0.001 * 7.224055290222168
Epoch 300, val loss: 1.2443727254867554
Epoch 310, training loss: 1.0223957300186157 = 1.0151735544204712 + 0.001 * 7.222125053405762
Epoch 310, val loss: 1.2121919393539429
Epoch 320, training loss: 0.9763135313987732 = 0.9690939784049988 + 0.001 * 7.219579219818115
Epoch 320, val loss: 1.1795176267623901
Epoch 330, training loss: 0.9305722713470459 = 0.9233561754226685 + 0.001 * 7.21612024307251
Epoch 330, val loss: 1.1467982530593872
Epoch 340, training loss: 0.8859484791755676 = 0.8787367939949036 + 0.001 * 7.211693286895752
Epoch 340, val loss: 1.1146435737609863
Epoch 350, training loss: 0.843040406703949 = 0.8358343243598938 + 0.001 * 7.20609712600708
Epoch 350, val loss: 1.0838569402694702
Epoch 360, training loss: 0.8022128939628601 = 0.7950140237808228 + 0.001 * 7.198888301849365
Epoch 360, val loss: 1.0546051263809204
Epoch 370, training loss: 0.7635831832885742 = 0.7563936710357666 + 0.001 * 7.189539432525635
Epoch 370, val loss: 1.0274957418441772
Epoch 380, training loss: 0.7271127104759216 = 0.7199353575706482 + 0.001 * 7.1773810386657715
Epoch 380, val loss: 1.0024033784866333
Epoch 390, training loss: 0.6925178170204163 = 0.6853559017181396 + 0.001 * 7.161893367767334
Epoch 390, val loss: 0.9792661070823669
Epoch 400, training loss: 0.659493625164032 = 0.6523474454879761 + 0.001 * 7.146165370941162
Epoch 400, val loss: 0.958048939704895
Epoch 410, training loss: 0.6277318000793457 = 0.6206017732620239 + 0.001 * 7.130049228668213
Epoch 410, val loss: 0.9385876655578613
Epoch 420, training loss: 0.596985936164856 = 0.5898733139038086 + 0.001 * 7.1126275062561035
Epoch 420, val loss: 0.9208334684371948
Epoch 430, training loss: 0.5670919418334961 = 0.5599938631057739 + 0.001 * 7.098080158233643
Epoch 430, val loss: 0.9048840999603271
Epoch 440, training loss: 0.5379636883735657 = 0.5308780670166016 + 0.001 * 7.085638999938965
Epoch 440, val loss: 0.8905603885650635
Epoch 450, training loss: 0.5096004605293274 = 0.5025234222412109 + 0.001 * 7.0770344734191895
Epoch 450, val loss: 0.8779376149177551
Epoch 460, training loss: 0.48205113410949707 = 0.47498342394828796 + 0.001 * 7.067724704742432
Epoch 460, val loss: 0.8670498728752136
Epoch 470, training loss: 0.4553948640823364 = 0.4483335614204407 + 0.001 * 7.061306953430176
Epoch 470, val loss: 0.85786372423172
Epoch 480, training loss: 0.429720401763916 = 0.42266330122947693 + 0.001 * 7.057091236114502
Epoch 480, val loss: 0.8503807783126831
Epoch 490, training loss: 0.40510398149490356 = 0.39805182814598083 + 0.001 * 7.052160263061523
Epoch 490, val loss: 0.8444045186042786
Epoch 500, training loss: 0.38160431385040283 = 0.3745530843734741 + 0.001 * 7.051220893859863
Epoch 500, val loss: 0.83990877866745
Epoch 510, training loss: 0.35923901200294495 = 0.3521907329559326 + 0.001 * 7.04827356338501
Epoch 510, val loss: 0.836713969707489
Epoch 520, training loss: 0.33800646662712097 = 0.33096441626548767 + 0.001 * 7.042062759399414
Epoch 520, val loss: 0.8348355889320374
Epoch 530, training loss: 0.31789645552635193 = 0.3108571767807007 + 0.001 * 7.039271354675293
Epoch 530, val loss: 0.8340864777565002
Epoch 540, training loss: 0.2988770604133606 = 0.29184144735336304 + 0.001 * 7.035619258880615
Epoch 540, val loss: 0.8344230651855469
Epoch 550, training loss: 0.28091689944267273 = 0.2738802134990692 + 0.001 * 7.0366902351379395
Epoch 550, val loss: 0.8357602953910828
Epoch 560, training loss: 0.2639782130718231 = 0.25693848729133606 + 0.001 * 7.039731502532959
Epoch 560, val loss: 0.8380600810050964
Epoch 570, training loss: 0.24800902605056763 = 0.24097926914691925 + 0.001 * 7.029758453369141
Epoch 570, val loss: 0.8412979245185852
Epoch 580, training loss: 0.23298226296901703 = 0.2259582281112671 + 0.001 * 7.024041652679443
Epoch 580, val loss: 0.8453832268714905
Epoch 590, training loss: 0.21886201202869415 = 0.21183742582798004 + 0.001 * 7.024586200714111
Epoch 590, val loss: 0.8502890467643738
Epoch 600, training loss: 0.20559976994991302 = 0.19857776165008545 + 0.001 * 7.022014141082764
Epoch 600, val loss: 0.8559711575508118
Epoch 610, training loss: 0.19315670430660248 = 0.1861373633146286 + 0.001 * 7.019339084625244
Epoch 610, val loss: 0.8623775243759155
Epoch 620, training loss: 0.1814926117658615 = 0.17447780072689056 + 0.001 * 7.014814376831055
Epoch 620, val loss: 0.8694571852684021
Epoch 630, training loss: 0.170577272772789 = 0.16356207430362701 + 0.001 * 7.015194892883301
Epoch 630, val loss: 0.877133309841156
Epoch 640, training loss: 0.1603602170944214 = 0.15334945917129517 + 0.001 * 7.010763645172119
Epoch 640, val loss: 0.8853753209114075
Epoch 650, training loss: 0.15081021189689636 = 0.14380022883415222 + 0.001 * 7.009989261627197
Epoch 650, val loss: 0.8941059112548828
Epoch 660, training loss: 0.1418885439634323 = 0.13487470149993896 + 0.001 * 7.013847827911377
Epoch 660, val loss: 0.9032864570617676
Epoch 670, training loss: 0.13354867696762085 = 0.1265387386083603 + 0.001 * 7.009944915771484
Epoch 670, val loss: 0.9128670692443848
Epoch 680, training loss: 0.12576428055763245 = 0.11875828355550766 + 0.001 * 7.005990505218506
Epoch 680, val loss: 0.9227797985076904
Epoch 690, training loss: 0.11850272119045258 = 0.11150109767913818 + 0.001 * 7.001621723175049
Epoch 690, val loss: 0.9330172538757324
Epoch 700, training loss: 0.11175955832004547 = 0.10473519563674927 + 0.001 * 7.024360656738281
Epoch 700, val loss: 0.9435166716575623
Epoch 710, training loss: 0.10543786734342575 = 0.09843143075704575 + 0.001 * 7.006434917449951
Epoch 710, val loss: 0.9542803764343262
Epoch 720, training loss: 0.09955931454896927 = 0.09255954623222351 + 0.001 * 6.99976921081543
Epoch 720, val loss: 0.9652324318885803
Epoch 730, training loss: 0.09408842772245407 = 0.08709131926298141 + 0.001 * 6.997105598449707
Epoch 730, val loss: 0.9763478636741638
Epoch 740, training loss: 0.08900096267461777 = 0.08199935406446457 + 0.001 * 7.001611232757568
Epoch 740, val loss: 0.9875839948654175
Epoch 750, training loss: 0.08425262570381165 = 0.07725761830806732 + 0.001 * 6.9950032234191895
Epoch 750, val loss: 0.9989219307899475
Epoch 760, training loss: 0.07983705401420593 = 0.07284052670001984 + 0.001 * 6.996524810791016
Epoch 760, val loss: 1.0103226900100708
Epoch 770, training loss: 0.0757199302315712 = 0.06872577965259552 + 0.001 * 6.9941511154174805
Epoch 770, val loss: 1.0217617750167847
Epoch 780, training loss: 0.07188647985458374 = 0.0648919865489006 + 0.001 * 6.994494438171387
Epoch 780, val loss: 1.0332539081573486
Epoch 790, training loss: 0.06831039488315582 = 0.06131916493177414 + 0.001 * 6.99122953414917
Epoch 790, val loss: 1.044729471206665
Epoch 800, training loss: 0.06497790664434433 = 0.057988353073596954 + 0.001 * 6.989556312561035
Epoch 800, val loss: 1.0561931133270264
Epoch 810, training loss: 0.0618712417781353 = 0.05488184094429016 + 0.001 * 6.989401340484619
Epoch 810, val loss: 1.0676301717758179
Epoch 820, training loss: 0.05897800624370575 = 0.05198350176215172 + 0.001 * 6.994502544403076
Epoch 820, val loss: 1.079020380973816
Epoch 830, training loss: 0.05626863241195679 = 0.0492786169052124 + 0.001 * 6.990016460418701
Epoch 830, val loss: 1.0903332233428955
Epoch 840, training loss: 0.05373863875865936 = 0.046752430498600006 + 0.001 * 6.986209869384766
Epoch 840, val loss: 1.1015559434890747
Epoch 850, training loss: 0.05137554556131363 = 0.04439183697104454 + 0.001 * 6.983706951141357
Epoch 850, val loss: 1.1126928329467773
Epoch 860, training loss: 0.04916977882385254 = 0.04218476265668869 + 0.001 * 6.985015392303467
Epoch 860, val loss: 1.1237304210662842
Epoch 870, training loss: 0.04710466414690018 = 0.040119875222444534 + 0.001 * 6.984787940979004
Epoch 870, val loss: 1.1346409320831299
Epoch 880, training loss: 0.04516791179776192 = 0.03818696364760399 + 0.001 * 6.980947017669678
Epoch 880, val loss: 1.1454546451568604
Epoch 890, training loss: 0.04336053133010864 = 0.036376409232616425 + 0.001 * 6.984122276306152
Epoch 890, val loss: 1.1561596393585205
Epoch 900, training loss: 0.041664183139801025 = 0.034679707139730453 + 0.001 * 6.984475612640381
Epoch 900, val loss: 1.166746973991394
Epoch 910, training loss: 0.040070682764053345 = 0.03308838605880737 + 0.001 * 6.982296466827393
Epoch 910, val loss: 1.177210807800293
Epoch 920, training loss: 0.038575686514377594 = 0.031594887375831604 + 0.001 * 6.980800151824951
Epoch 920, val loss: 1.1875718832015991
Epoch 930, training loss: 0.03716636449098587 = 0.030190737918019295 + 0.001 * 6.9756269454956055
Epoch 930, val loss: 1.1977815628051758
Epoch 940, training loss: 0.03584957867860794 = 0.028867924585938454 + 0.001 * 6.981652736663818
Epoch 940, val loss: 1.20786714553833
Epoch 950, training loss: 0.034596916288137436 = 0.02761954255402088 + 0.001 * 6.97737455368042
Epoch 950, val loss: 1.2178486585617065
Epoch 960, training loss: 0.03341367840766907 = 0.026440635323524475 + 0.001 * 6.9730424880981445
Epoch 960, val loss: 1.2277095317840576
Epoch 970, training loss: 0.032297827303409576 = 0.025326967239379883 + 0.001 * 6.970860004425049
Epoch 970, val loss: 1.2374589443206787
Epoch 980, training loss: 0.03126088157296181 = 0.024274792522192 + 0.001 * 6.986087322235107
Epoch 980, val loss: 1.2470893859863281
Epoch 990, training loss: 0.030253197997808456 = 0.02328074909746647 + 0.001 * 6.972448348999023
Epoch 990, val loss: 1.2565892934799194
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 1.941144347190857 = 1.9325474500656128 + 0.001 * 8.596846580505371
Epoch 0, val loss: 1.9273812770843506
Epoch 10, training loss: 1.9362525939941406 = 1.927655816078186 + 0.001 * 8.596837043762207
Epoch 10, val loss: 1.922410249710083
Epoch 20, training loss: 1.9310022592544556 = 1.922405481338501 + 0.001 * 8.596790313720703
Epoch 20, val loss: 1.917051911354065
Epoch 30, training loss: 1.9249365329742432 = 1.9163398742675781 + 0.001 * 8.596707344055176
Epoch 30, val loss: 1.9108810424804688
Epoch 40, training loss: 1.91756272315979 = 1.9089661836624146 + 0.001 * 8.596564292907715
Epoch 40, val loss: 1.903416395187378
Epoch 50, training loss: 1.9083375930786133 = 1.899741291999817 + 0.001 * 8.59631061553955
Epoch 50, val loss: 1.8941411972045898
Epoch 60, training loss: 1.8966898918151855 = 1.8880940675735474 + 0.001 * 8.59586238861084
Epoch 60, val loss: 1.8825488090515137
Epoch 70, training loss: 1.8821772336959839 = 1.873582124710083 + 0.001 * 8.595062255859375
Epoch 70, val loss: 1.86836576461792
Epoch 80, training loss: 1.8648518323898315 = 1.8562582731246948 + 0.001 * 8.593578338623047
Epoch 80, val loss: 1.8519967794418335
Epoch 90, training loss: 1.8456928730010986 = 1.8371021747589111 + 0.001 * 8.590642929077148
Epoch 90, val loss: 1.834774136543274
Epoch 100, training loss: 1.8262237310409546 = 1.8176393508911133 + 0.001 * 8.584403991699219
Epoch 100, val loss: 1.818480134010315
Epoch 110, training loss: 1.8071500062942505 = 1.7985804080963135 + 0.001 * 8.569605827331543
Epoch 110, val loss: 1.8035024404525757
Epoch 120, training loss: 1.7871136665344238 = 1.7785857915878296 + 0.001 * 8.527894020080566
Epoch 120, val loss: 1.7878369092941284
Epoch 130, training loss: 1.7638121843338013 = 1.7554346323013306 + 0.001 * 8.377581596374512
Epoch 130, val loss: 1.7688814401626587
Epoch 140, training loss: 1.735347867012024 = 1.7273391485214233 + 0.001 * 8.008691787719727
Epoch 140, val loss: 1.7450523376464844
Epoch 150, training loss: 1.7011271715164185 = 1.693182110786438 + 0.001 * 7.945004940032959
Epoch 150, val loss: 1.7156062126159668
Epoch 160, training loss: 1.660667896270752 = 1.6527725458145142 + 0.001 * 7.895336627960205
Epoch 160, val loss: 1.6809725761413574
Epoch 170, training loss: 1.6152468919754028 = 1.6073980331420898 + 0.001 * 7.848876476287842
Epoch 170, val loss: 1.6428091526031494
Epoch 180, training loss: 1.5669068098068237 = 1.5591250658035278 + 0.001 * 7.781700134277344
Epoch 180, val loss: 1.603076696395874
Epoch 190, training loss: 1.5180078744888306 = 1.5103356838226318 + 0.001 * 7.67222261428833
Epoch 190, val loss: 1.5642307996749878
Epoch 200, training loss: 1.4699475765228271 = 1.4624249935150146 + 0.001 * 7.522606372833252
Epoch 200, val loss: 1.5273375511169434
Epoch 210, training loss: 1.4230918884277344 = 1.4156583547592163 + 0.001 * 7.4335503578186035
Epoch 210, val loss: 1.4926724433898926
Epoch 220, training loss: 1.3772013187408447 = 1.3697690963745117 + 0.001 * 7.432278156280518
Epoch 220, val loss: 1.4597831964492798
Epoch 230, training loss: 1.3320698738098145 = 1.3246586322784424 + 0.001 * 7.411187648773193
Epoch 230, val loss: 1.428496241569519
Epoch 240, training loss: 1.2876958847045898 = 1.2802939414978027 + 0.001 * 7.401941776275635
Epoch 240, val loss: 1.398612380027771
Epoch 250, training loss: 1.2442060708999634 = 1.2368155717849731 + 0.001 * 7.390466213226318
Epoch 250, val loss: 1.3701744079589844
Epoch 260, training loss: 1.2018828392028809 = 1.1945068836212158 + 0.001 * 7.37596321105957
Epoch 260, val loss: 1.3429986238479614
Epoch 270, training loss: 1.1607784032821655 = 1.1534223556518555 + 0.001 * 7.356050491333008
Epoch 270, val loss: 1.317210078239441
Epoch 280, training loss: 1.1208610534667969 = 1.1135286092758179 + 0.001 * 7.332432746887207
Epoch 280, val loss: 1.2923978567123413
Epoch 290, training loss: 1.0820482969284058 = 1.0747398138046265 + 0.001 * 7.308442115783691
Epoch 290, val loss: 1.2681623697280884
Epoch 300, training loss: 1.0442239046096802 = 1.0369353294372559 + 0.001 * 7.288529396057129
Epoch 300, val loss: 1.2445098161697388
Epoch 310, training loss: 1.0072569847106934 = 0.9999830722808838 + 0.001 * 7.273943901062012
Epoch 310, val loss: 1.221235752105713
Epoch 320, training loss: 0.9710155129432678 = 0.9637531042098999 + 0.001 * 7.262421607971191
Epoch 320, val loss: 1.198006510734558
Epoch 330, training loss: 0.9353545904159546 = 0.9281032085418701 + 0.001 * 7.251399040222168
Epoch 330, val loss: 1.1750539541244507
Epoch 340, training loss: 0.9001551866531372 = 0.8929153084754944 + 0.001 * 7.239903926849365
Epoch 340, val loss: 1.1520696878433228
Epoch 350, training loss: 0.8653604984283447 = 0.8581297993659973 + 0.001 * 7.230705261230469
Epoch 350, val loss: 1.129278540611267
Epoch 360, training loss: 0.8309639096260071 = 0.8237427473068237 + 0.001 * 7.221173286437988
Epoch 360, val loss: 1.106560230255127
Epoch 370, training loss: 0.7970342636108398 = 0.7898222208023071 + 0.001 * 7.212055206298828
Epoch 370, val loss: 1.0841597318649292
Epoch 380, training loss: 0.7637109160423279 = 0.7565085887908936 + 0.001 * 7.202327728271484
Epoch 380, val loss: 1.062255859375
Epoch 390, training loss: 0.7311977744102478 = 0.7240042090415955 + 0.001 * 7.193580627441406
Epoch 390, val loss: 1.0411970615386963
Epoch 400, training loss: 0.6997044682502747 = 0.6925226449966431 + 0.001 * 7.181825637817383
Epoch 400, val loss: 1.0214332342147827
Epoch 410, training loss: 0.669408917427063 = 0.6622372269630432 + 0.001 * 7.171703815460205
Epoch 410, val loss: 1.0030813217163086
Epoch 420, training loss: 0.6404263973236084 = 0.6332678198814392 + 0.001 * 7.158592224121094
Epoch 420, val loss: 0.9865977168083191
Epoch 430, training loss: 0.6128106713294983 = 0.6056637763977051 + 0.001 * 7.14691686630249
Epoch 430, val loss: 0.9719485640525818
Epoch 440, training loss: 0.586533784866333 = 0.5793992280960083 + 0.001 * 7.134546279907227
Epoch 440, val loss: 0.9592626690864563
Epoch 450, training loss: 0.5615320205688477 = 0.5544072389602661 + 0.001 * 7.1248087882995605
Epoch 450, val loss: 0.9485236406326294
Epoch 460, training loss: 0.5377157926559448 = 0.5306037664413452 + 0.001 * 7.112052917480469
Epoch 460, val loss: 0.9396044015884399
Epoch 470, training loss: 0.5149860978126526 = 0.5078876614570618 + 0.001 * 7.098443508148193
Epoch 470, val loss: 0.9324230551719666
Epoch 480, training loss: 0.493257075548172 = 0.48616543412208557 + 0.001 * 7.091639995574951
Epoch 480, val loss: 0.9268031716346741
Epoch 490, training loss: 0.4724462628364563 = 0.46535757184028625 + 0.001 * 7.088698863983154
Epoch 490, val loss: 0.9225611090660095
Epoch 500, training loss: 0.4524790644645691 = 0.44540339708328247 + 0.001 * 7.0756611824035645
Epoch 500, val loss: 0.9196345806121826
Epoch 510, training loss: 0.4333341717720032 = 0.42627057433128357 + 0.001 * 7.063610076904297
Epoch 510, val loss: 0.91790771484375
Epoch 520, training loss: 0.4150182008743286 = 0.4079400897026062 + 0.001 * 7.078106880187988
Epoch 520, val loss: 0.9172899127006531
Epoch 530, training loss: 0.39744824171066284 = 0.39039263129234314 + 0.001 * 7.055624961853027
Epoch 530, val loss: 0.9177154898643494
Epoch 540, training loss: 0.38065749406814575 = 0.3736085295677185 + 0.001 * 7.048974990844727
Epoch 540, val loss: 0.9191331267356873
Epoch 550, training loss: 0.3646068871021271 = 0.3575620651245117 + 0.001 * 7.04482364654541
Epoch 550, val loss: 0.9213930368423462
Epoch 560, training loss: 0.3492879569530487 = 0.3422253131866455 + 0.001 * 7.06264066696167
Epoch 560, val loss: 0.9244323372840881
Epoch 570, training loss: 0.33461490273475647 = 0.32756999135017395 + 0.001 * 7.044900894165039
Epoch 570, val loss: 0.9282144904136658
Epoch 580, training loss: 0.3205878734588623 = 0.31355011463165283 + 0.001 * 7.037746906280518
Epoch 580, val loss: 0.9326264262199402
Epoch 590, training loss: 0.30714505910873413 = 0.3001105487346649 + 0.001 * 7.034515857696533
Epoch 590, val loss: 0.9376174807548523
Epoch 600, training loss: 0.29422488808631897 = 0.2871951758861542 + 0.001 * 7.029702663421631
Epoch 600, val loss: 0.9429898858070374
Epoch 610, training loss: 0.2817758321762085 = 0.27474331855773926 + 0.001 * 7.032516956329346
Epoch 610, val loss: 0.9486864805221558
Epoch 620, training loss: 0.26971542835235596 = 0.2626909017562866 + 0.001 * 7.024519443511963
Epoch 620, val loss: 0.9546214938163757
Epoch 630, training loss: 0.25800445675849915 = 0.2509625256061554 + 0.001 * 7.041927337646484
Epoch 630, val loss: 0.9606493711471558
Epoch 640, training loss: 0.24650587141513824 = 0.23947888612747192 + 0.001 * 7.026979446411133
Epoch 640, val loss: 0.9666557312011719
Epoch 650, training loss: 0.23517358303070068 = 0.22815121710300446 + 0.001 * 7.0223612785339355
Epoch 650, val loss: 0.972475528717041
Epoch 660, training loss: 0.2239142507314682 = 0.2168930172920227 + 0.001 * 7.021233081817627
Epoch 660, val loss: 0.9779866337776184
Epoch 670, training loss: 0.2126588672399521 = 0.20563487708568573 + 0.001 * 7.023988246917725
Epoch 670, val loss: 0.9831008315086365
Epoch 680, training loss: 0.20132990181446075 = 0.19431157410144806 + 0.001 * 7.018332481384277
Epoch 680, val loss: 0.9877786040306091
Epoch 690, training loss: 0.18991103768348694 = 0.18289105594158173 + 0.001 * 7.019975185394287
Epoch 690, val loss: 0.991986095905304
Epoch 700, training loss: 0.17840567231178284 = 0.17138735949993134 + 0.001 * 7.018317222595215
Epoch 700, val loss: 0.9957268834114075
Epoch 710, training loss: 0.16687750816345215 = 0.1598636955022812 + 0.001 * 7.013810157775879
Epoch 710, val loss: 0.9991636872291565
Epoch 720, training loss: 0.15546558797359467 = 0.1484481543302536 + 0.001 * 7.017428874969482
Epoch 720, val loss: 1.0024205446243286
Epoch 730, training loss: 0.14430171251296997 = 0.13728821277618408 + 0.001 * 7.013497352600098
Epoch 730, val loss: 1.0057533979415894
Epoch 740, training loss: 0.13354717195034027 = 0.126535564661026 + 0.001 * 7.011600494384766
Epoch 740, val loss: 1.0093539953231812
Epoch 750, training loss: 0.12334650754928589 = 0.11632303148508072 + 0.001 * 7.023477554321289
Epoch 750, val loss: 1.0133312940597534
Epoch 760, training loss: 0.11376725137233734 = 0.10675594955682755 + 0.001 * 7.011302947998047
Epoch 760, val loss: 1.0178778171539307
Epoch 770, training loss: 0.10490988194942474 = 0.0979023203253746 + 0.001 * 7.007561206817627
Epoch 770, val loss: 1.0230634212493896
Epoch 780, training loss: 0.09679669886827469 = 0.08977814018726349 + 0.001 * 7.018558025360107
Epoch 780, val loss: 1.0288686752319336
Epoch 790, training loss: 0.08938321471214294 = 0.08237042278051376 + 0.001 * 7.012794017791748
Epoch 790, val loss: 1.03528892993927
Epoch 800, training loss: 0.08265306800603867 = 0.07565105706453323 + 0.001 * 7.002009391784668
Epoch 800, val loss: 1.0422407388687134
Epoch 810, training loss: 0.07659932971000671 = 0.06957844644784927 + 0.001 * 7.020885467529297
Epoch 810, val loss: 1.0496654510498047
Epoch 820, training loss: 0.07111524045467377 = 0.06410682201385498 + 0.001 * 7.00841760635376
Epoch 820, val loss: 1.057475209236145
Epoch 830, training loss: 0.06618048250675201 = 0.059182941913604736 + 0.001 * 6.9975433349609375
Epoch 830, val loss: 1.0656051635742188
Epoch 840, training loss: 0.06175761669874191 = 0.05475578084588051 + 0.001 * 7.001833915710449
Epoch 840, val loss: 1.0739012956619263
Epoch 850, training loss: 0.05776683986186981 = 0.050772711634635925 + 0.001 * 6.994125843048096
Epoch 850, val loss: 1.08232581615448
Epoch 860, training loss: 0.054181747138500214 = 0.04718649387359619 + 0.001 * 6.995251178741455
Epoch 860, val loss: 1.0908350944519043
Epoch 870, training loss: 0.05094708129763603 = 0.04395388439297676 + 0.001 * 6.993196964263916
Epoch 870, val loss: 1.0993765592575073
Epoch 880, training loss: 0.048030540347099304 = 0.04103486239910126 + 0.001 * 6.995676517486572
Epoch 880, val loss: 1.1079177856445312
Epoch 890, training loss: 0.04538634791970253 = 0.03839430585503578 + 0.001 * 6.992040157318115
Epoch 890, val loss: 1.1164157390594482
Epoch 900, training loss: 0.04299447685480118 = 0.03600066900253296 + 0.001 * 6.993808746337891
Epoch 900, val loss: 1.124812364578247
Epoch 910, training loss: 0.040814608335494995 = 0.03382568061351776 + 0.001 * 6.988927364349365
Epoch 910, val loss: 1.1331920623779297
Epoch 920, training loss: 0.03883235529065132 = 0.03184463828802109 + 0.001 * 6.987716197967529
Epoch 920, val loss: 1.1414707899093628
Epoch 930, training loss: 0.03702940046787262 = 0.030035430565476418 + 0.001 * 6.99397087097168
Epoch 930, val loss: 1.1496754884719849
Epoch 940, training loss: 0.03536061942577362 = 0.028379537165164948 + 0.001 * 6.981081485748291
Epoch 940, val loss: 1.15775728225708
Epoch 950, training loss: 0.033860087394714355 = 0.026859896257519722 + 0.001 * 7.000190258026123
Epoch 950, val loss: 1.1657267808914185
Epoch 960, training loss: 0.03244815394282341 = 0.025462206453084946 + 0.001 * 6.985945701599121
Epoch 960, val loss: 1.1735787391662598
Epoch 970, training loss: 0.03114849142730236 = 0.024173574522137642 + 0.001 * 6.974916458129883
Epoch 970, val loss: 1.181300163269043
Epoch 980, training loss: 0.02997042052447796 = 0.022982992231845856 + 0.001 * 6.987427711486816
Epoch 980, val loss: 1.188933253288269
Epoch 990, training loss: 0.028855420649051666 = 0.021881122142076492 + 0.001 * 6.974297523498535
Epoch 990, val loss: 1.1963868141174316
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8039008961518187
The final CL Acc:0.74321, 0.01222, The final GNN Acc:0.80531, 0.00163
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13090])
remove edge: torch.Size([2, 7998])
updated graph: torch.Size([2, 10532])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1.965043067932129 = 1.9564461708068848 + 0.001 * 8.596851348876953
Epoch 0, val loss: 1.958119511604309
Epoch 10, training loss: 1.9599802494049072 = 1.951383352279663 + 0.001 * 8.596839904785156
Epoch 10, val loss: 1.952790379524231
Epoch 20, training loss: 1.954659104347229 = 1.9460623264312744 + 0.001 * 8.596794128417969
Epoch 20, val loss: 1.9472479820251465
Epoch 30, training loss: 1.9485559463500977 = 1.9399592876434326 + 0.001 * 8.596708297729492
Epoch 30, val loss: 1.9409769773483276
Epoch 40, training loss: 1.941152572631836 = 1.9325560331344604 + 0.001 * 8.596553802490234
Epoch 40, val loss: 1.93342125415802
Epoch 50, training loss: 1.9318267107009888 = 1.9232304096221924 + 0.001 * 8.596283912658691
Epoch 50, val loss: 1.9239280223846436
Epoch 60, training loss: 1.9198168516159058 = 1.9112210273742676 + 0.001 * 8.595793724060059
Epoch 60, val loss: 1.9117398262023926
Epoch 70, training loss: 1.9043437242507935 = 1.8957488536834717 + 0.001 * 8.594854354858398
Epoch 70, val loss: 1.896056056022644
Epoch 80, training loss: 1.8847382068634033 = 1.876145362854004 + 0.001 * 8.592891693115234
Epoch 80, val loss: 1.876462697982788
Epoch 90, training loss: 1.8611210584640503 = 1.8525326251983643 + 0.001 * 8.588421821594238
Epoch 90, val loss: 1.8534897565841675
Epoch 100, training loss: 1.8350104093551636 = 1.8264330625534058 + 0.001 * 8.57731819152832
Epoch 100, val loss: 1.8291399478912354
Epoch 110, training loss: 1.809090495109558 = 1.8005423545837402 + 0.001 * 8.548152923583984
Epoch 110, val loss: 1.8063833713531494
Epoch 120, training loss: 1.7851747274398804 = 1.7767071723937988 + 0.001 * 8.467573165893555
Epoch 120, val loss: 1.7865006923675537
Epoch 130, training loss: 1.7613654136657715 = 1.7531743049621582 + 0.001 * 8.19115924835205
Epoch 130, val loss: 1.7668365240097046
Epoch 140, training loss: 1.7347034215927124 = 1.726667046546936 + 0.001 * 8.036409378051758
Epoch 140, val loss: 1.7440532445907593
Epoch 150, training loss: 1.703108310699463 = 1.6951712369918823 + 0.001 * 7.937049865722656
Epoch 150, val loss: 1.7165558338165283
Epoch 160, training loss: 1.6653963327407837 = 1.657589316368103 + 0.001 * 7.806985855102539
Epoch 160, val loss: 1.6836808919906616
Epoch 170, training loss: 1.6210384368896484 = 1.6133832931518555 + 0.001 * 7.65514612197876
Epoch 170, val loss: 1.6452609300613403
Epoch 180, training loss: 1.5706785917282104 = 1.5631808042526245 + 0.001 * 7.497774124145508
Epoch 180, val loss: 1.6021978855133057
Epoch 190, training loss: 1.5162477493286133 = 1.5088684558868408 + 0.001 * 7.379296779632568
Epoch 190, val loss: 1.5562371015548706
Epoch 200, training loss: 1.4596151113510132 = 1.4522795677185059 + 0.001 * 7.335552215576172
Epoch 200, val loss: 1.5090973377227783
Epoch 210, training loss: 1.4025076627731323 = 1.3951995372772217 + 0.001 * 7.308085918426514
Epoch 210, val loss: 1.462421178817749
Epoch 220, training loss: 1.3461401462554932 = 1.3388593196868896 + 0.001 * 7.280876159667969
Epoch 220, val loss: 1.4169987440109253
Epoch 230, training loss: 1.2910809516906738 = 1.2838258743286133 + 0.001 * 7.255123615264893
Epoch 230, val loss: 1.3729132413864136
Epoch 240, training loss: 1.2376022338867188 = 1.2303727865219116 + 0.001 * 7.229430198669434
Epoch 240, val loss: 1.3305282592773438
Epoch 250, training loss: 1.1860846281051636 = 1.1788787841796875 + 0.001 * 7.205794334411621
Epoch 250, val loss: 1.2899656295776367
Epoch 260, training loss: 1.137069821357727 = 1.1298826932907104 + 0.001 * 7.187182426452637
Epoch 260, val loss: 1.251804232597351
Epoch 270, training loss: 1.0908880233764648 = 1.0837150812149048 + 0.001 * 7.172923564910889
Epoch 270, val loss: 1.216269850730896
Epoch 280, training loss: 1.0475666522979736 = 1.0404059886932373 + 0.001 * 7.160614967346191
Epoch 280, val loss: 1.1831363439559937
Epoch 290, training loss: 1.0068840980529785 = 0.9997365474700928 + 0.001 * 7.147572040557861
Epoch 290, val loss: 1.152095913887024
Epoch 300, training loss: 0.9684181213378906 = 0.9612857699394226 + 0.001 * 7.132364273071289
Epoch 300, val loss: 1.122657060623169
Epoch 310, training loss: 0.9316544532775879 = 0.9245390892028809 + 0.001 * 7.115342617034912
Epoch 310, val loss: 1.0944840908050537
Epoch 320, training loss: 0.896112322807312 = 0.8890144228935242 + 0.001 * 7.097921371459961
Epoch 320, val loss: 1.0670896768569946
Epoch 330, training loss: 0.8614106774330139 = 0.8543286919593811 + 0.001 * 7.081997871398926
Epoch 330, val loss: 1.0402308702468872
Epoch 340, training loss: 0.8272606730461121 = 0.8201903104782104 + 0.001 * 7.070371627807617
Epoch 340, val loss: 1.0136091709136963
Epoch 350, training loss: 0.793514609336853 = 0.786451518535614 + 0.001 * 7.0631184577941895
Epoch 350, val loss: 0.9873575568199158
Epoch 360, training loss: 0.760132908821106 = 0.7530743479728699 + 0.001 * 7.058558940887451
Epoch 360, val loss: 0.9614057540893555
Epoch 370, training loss: 0.727138340473175 = 0.7200830578804016 + 0.001 * 7.05530309677124
Epoch 370, val loss: 0.9360429644584656
Epoch 380, training loss: 0.694571316242218 = 0.6875183582305908 + 0.001 * 7.052946090698242
Epoch 380, val loss: 0.9114139676094055
Epoch 390, training loss: 0.6624715328216553 = 0.655420184135437 + 0.001 * 7.051326274871826
Epoch 390, val loss: 0.8877050280570984
Epoch 400, training loss: 0.6309288740158081 = 0.6238787174224854 + 0.001 * 7.050185203552246
Epoch 400, val loss: 0.8650006055831909
Epoch 410, training loss: 0.6000449657440186 = 0.5929957628250122 + 0.001 * 7.049222469329834
Epoch 410, val loss: 0.8435097932815552
Epoch 420, training loss: 0.5699520111083984 = 0.5629002451896667 + 0.001 * 7.051773548126221
Epoch 420, val loss: 0.8233388662338257
Epoch 430, training loss: 0.5407912135124207 = 0.5337421298027039 + 0.001 * 7.049072265625
Epoch 430, val loss: 0.8047809600830078
Epoch 440, training loss: 0.5127071738243103 = 0.5056591033935547 + 0.001 * 7.048093795776367
Epoch 440, val loss: 0.7878407835960388
Epoch 450, training loss: 0.48578742146492004 = 0.4787399172782898 + 0.001 * 7.047500133514404
Epoch 450, val loss: 0.7725566029548645
Epoch 460, training loss: 0.4600455164909363 = 0.4529982805252075 + 0.001 * 7.047223091125488
Epoch 460, val loss: 0.7588986158370972
Epoch 470, training loss: 0.43544456362724304 = 0.4283974766731262 + 0.001 * 7.047093391418457
Epoch 470, val loss: 0.7467219233512878
Epoch 480, training loss: 0.41190242767333984 = 0.4048553705215454 + 0.001 * 7.0470452308654785
Epoch 480, val loss: 0.7359243631362915
Epoch 490, training loss: 0.3893163800239563 = 0.38226932287216187 + 0.001 * 7.047058582305908
Epoch 490, val loss: 0.7263277173042297
Epoch 500, training loss: 0.36758333444595337 = 0.36053621768951416 + 0.001 * 7.04710578918457
Epoch 500, val loss: 0.717735230922699
Epoch 510, training loss: 0.3466343879699707 = 0.33958694338798523 + 0.001 * 7.047445297241211
Epoch 510, val loss: 0.7100341320037842
Epoch 520, training loss: 0.32643750309944153 = 0.31939011812210083 + 0.001 * 7.047393321990967
Epoch 520, val loss: 0.703167736530304
Epoch 530, training loss: 0.30699244141578674 = 0.2999449074268341 + 0.001 * 7.0475382804870605
Epoch 530, val loss: 0.6970564723014832
Epoch 540, training loss: 0.2883361876010895 = 0.2812883257865906 + 0.001 * 7.047860145568848
Epoch 540, val loss: 0.6916981339454651
Epoch 550, training loss: 0.2705203592777252 = 0.26347237825393677 + 0.001 * 7.047969818115234
Epoch 550, val loss: 0.687110185623169
Epoch 560, training loss: 0.2535955011844635 = 0.24654671549797058 + 0.001 * 7.048776149749756
Epoch 560, val loss: 0.6832727789878845
Epoch 570, training loss: 0.23759448528289795 = 0.2305462807416916 + 0.001 * 7.048203468322754
Epoch 570, val loss: 0.6801620125770569
Epoch 580, training loss: 0.22253470122814178 = 0.21548622846603394 + 0.001 * 7.048471450805664
Epoch 580, val loss: 0.6778136491775513
Epoch 590, training loss: 0.20841103792190552 = 0.2013622671365738 + 0.001 * 7.048778057098389
Epoch 590, val loss: 0.6762160658836365
Epoch 600, training loss: 0.19520361721515656 = 0.18815433979034424 + 0.001 * 7.049281120300293
Epoch 600, val loss: 0.6752979159355164
Epoch 610, training loss: 0.18288573622703552 = 0.17583665251731873 + 0.001 * 7.049086093902588
Epoch 610, val loss: 0.6750369071960449
Epoch 620, training loss: 0.17142115533351898 = 0.16437022387981415 + 0.001 * 7.050928115844727
Epoch 620, val loss: 0.6753997802734375
Epoch 630, training loss: 0.1607578992843628 = 0.15370824933052063 + 0.001 * 7.049651622772217
Epoch 630, val loss: 0.6763445138931274
Epoch 640, training loss: 0.15085135400295258 = 0.1438017636537552 + 0.001 * 7.049586772918701
Epoch 640, val loss: 0.6778395771980286
Epoch 650, training loss: 0.14165125787258148 = 0.13460202515125275 + 0.001 * 7.049236297607422
Epoch 650, val loss: 0.6798190474510193
Epoch 660, training loss: 0.13311167061328888 = 0.12606245279312134 + 0.001 * 7.04921293258667
Epoch 660, val loss: 0.6822563409805298
Epoch 670, training loss: 0.12518541514873505 = 0.11813593655824661 + 0.001 * 7.0494771003723145
Epoch 670, val loss: 0.6850903034210205
Epoch 680, training loss: 0.11783096939325333 = 0.1107809841632843 + 0.001 * 7.049984931945801
Epoch 680, val loss: 0.6883184313774109
Epoch 690, training loss: 0.11100588738918304 = 0.1039542704820633 + 0.001 * 7.051612854003906
Epoch 690, val loss: 0.6919039487838745
Epoch 700, training loss: 0.10466594994068146 = 0.09761688113212585 + 0.001 * 7.049067497253418
Epoch 700, val loss: 0.6957957148551941
Epoch 710, training loss: 0.09878192842006683 = 0.09173284471035004 + 0.001 * 7.049084663391113
Epoch 710, val loss: 0.6999711394309998
Epoch 720, training loss: 0.09331610053777695 = 0.0862678661942482 + 0.001 * 7.04823637008667
Epoch 720, val loss: 0.7043913006782532
Epoch 730, training loss: 0.08823980391025543 = 0.08119183778762817 + 0.001 * 7.047968864440918
Epoch 730, val loss: 0.7090423107147217
Epoch 740, training loss: 0.08352524787187576 = 0.07647518813610077 + 0.001 * 7.0500569343566895
Epoch 740, val loss: 0.7138775587081909
Epoch 750, training loss: 0.07913804799318314 = 0.07209081947803497 + 0.001 * 7.047226905822754
Epoch 750, val loss: 0.7188993692398071
Epoch 760, training loss: 0.07505925744771957 = 0.06801234185695648 + 0.001 * 7.046917915344238
Epoch 760, val loss: 0.7240657210350037
Epoch 770, training loss: 0.07126396149396896 = 0.06421752274036407 + 0.001 * 7.046440601348877
Epoch 770, val loss: 0.7293629050254822
Epoch 780, training loss: 0.0677310973405838 = 0.060685522854328156 + 0.001 * 7.045577526092529
Epoch 780, val loss: 0.7347869277000427
Epoch 790, training loss: 0.06444091349840164 = 0.057396404445171356 + 0.001 * 7.044509410858154
Epoch 790, val loss: 0.740303099155426
Epoch 800, training loss: 0.06137995421886444 = 0.05433178320527077 + 0.001 * 7.0481696128845215
Epoch 800, val loss: 0.7459000945091248
Epoch 810, training loss: 0.0585198812186718 = 0.05147462338209152 + 0.001 * 7.045256614685059
Epoch 810, val loss: 0.7515509724617004
Epoch 820, training loss: 0.05585167557001114 = 0.04880935326218605 + 0.001 * 7.042320728302002
Epoch 820, val loss: 0.757250964641571
Epoch 830, training loss: 0.05336445942521095 = 0.04632134363055229 + 0.001 * 7.043115139007568
Epoch 830, val loss: 0.7629919648170471
Epoch 840, training loss: 0.05103781446814537 = 0.043997325003147125 + 0.001 * 7.040490627288818
Epoch 840, val loss: 0.7687566876411438
Epoch 850, training loss: 0.048865221440792084 = 0.04182533547282219 + 0.001 * 7.039885520935059
Epoch 850, val loss: 0.7745391130447388
Epoch 860, training loss: 0.04683363065123558 = 0.03979390114545822 + 0.001 * 7.039730072021484
Epoch 860, val loss: 0.7803307771682739
Epoch 870, training loss: 0.04492896795272827 = 0.03789268061518669 + 0.001 * 7.0362868309021
Epoch 870, val loss: 0.7861244678497314
Epoch 880, training loss: 0.04314734786748886 = 0.03611212223768234 + 0.001 * 7.035223960876465
Epoch 880, val loss: 0.7918953895568848
Epoch 890, training loss: 0.041477903723716736 = 0.034443654119968414 + 0.001 * 7.034250736236572
Epoch 890, val loss: 0.7976420521736145
Epoch 900, training loss: 0.03990998864173889 = 0.03287920355796814 + 0.001 * 7.03078556060791
Epoch 900, val loss: 0.8033714890480042
Epoch 910, training loss: 0.03844298794865608 = 0.031410910189151764 + 0.001 * 7.032077789306641
Epoch 910, val loss: 0.8090673089027405
Epoch 920, training loss: 0.03705913573503494 = 0.030031718313694 + 0.001 * 7.027415752410889
Epoch 920, val loss: 0.8147165775299072
Epoch 930, training loss: 0.035759203135967255 = 0.028735326603055 + 0.001 * 7.023877143859863
Epoch 930, val loss: 0.820330798625946
Epoch 940, training loss: 0.0345422737300396 = 0.027515649795532227 + 0.001 * 7.026623249053955
Epoch 940, val loss: 0.8259002566337585
Epoch 950, training loss: 0.03339704871177673 = 0.026367371901869774 + 0.001 * 7.029675483703613
Epoch 950, val loss: 0.8314283490180969
Epoch 960, training loss: 0.03230202943086624 = 0.02528553642332554 + 0.001 * 7.016493320465088
Epoch 960, val loss: 0.8369039297103882
Epoch 970, training loss: 0.03127908706665039 = 0.024265490472316742 + 0.001 * 7.013595104217529
Epoch 970, val loss: 0.8423275351524353
Epoch 980, training loss: 0.03032374382019043 = 0.023302799090743065 + 0.001 * 7.020944595336914
Epoch 980, val loss: 0.8476965427398682
Epoch 990, training loss: 0.02940407022833824 = 0.02239355817437172 + 0.001 * 7.010512828826904
Epoch 990, val loss: 0.8530009984970093
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 1.9557186365127563 = 1.9471218585968018 + 0.001 * 8.596813201904297
Epoch 0, val loss: 1.948125958442688
Epoch 10, training loss: 1.9507293701171875 = 1.942132592201233 + 0.001 * 8.596799850463867
Epoch 10, val loss: 1.943196177482605
Epoch 20, training loss: 1.9454327821731567 = 1.9368360042572021 + 0.001 * 8.596741676330566
Epoch 20, val loss: 1.9378067255020142
Epoch 30, training loss: 1.9392503499984741 = 1.930653691291809 + 0.001 * 8.596630096435547
Epoch 30, val loss: 1.9314303398132324
Epoch 40, training loss: 1.9316747188568115 = 1.9230782985687256 + 0.001 * 8.596435546875
Epoch 40, val loss: 1.9235141277313232
Epoch 50, training loss: 1.922013521194458 = 1.9134174585342407 + 0.001 * 8.596095085144043
Epoch 50, val loss: 1.9133673906326294
Epoch 60, training loss: 1.909476637840271 = 1.9008811712265015 + 0.001 * 8.595480918884277
Epoch 60, val loss: 1.9002127647399902
Epoch 70, training loss: 1.8933401107788086 = 1.8847458362579346 + 0.001 * 8.594322204589844
Epoch 70, val loss: 1.8834540843963623
Epoch 80, training loss: 1.8734091520309448 = 1.8648171424865723 + 0.001 * 8.591950416564941
Epoch 80, val loss: 1.863145112991333
Epoch 90, training loss: 1.850343108177185 = 1.8417565822601318 + 0.001 * 8.586531639099121
Epoch 90, val loss: 1.8404580354690552
Epoch 100, training loss: 1.8261549472808838 = 1.8175822496414185 + 0.001 * 8.572712898254395
Epoch 100, val loss: 1.8179353475570679
Epoch 110, training loss: 1.8031210899353027 = 1.7945879697799683 + 0.001 * 8.533136367797852
Epoch 110, val loss: 1.7979069948196411
Epoch 120, training loss: 1.7810986042022705 = 1.7727099657058716 + 0.001 * 8.38859748840332
Epoch 120, val loss: 1.7798062562942505
Epoch 130, training loss: 1.7573928833007812 = 1.7493462562561035 + 0.001 * 8.046683311462402
Epoch 130, val loss: 1.7605805397033691
Epoch 140, training loss: 1.7299772500991821 = 1.7220392227172852 + 0.001 * 7.938069820404053
Epoch 140, val loss: 1.7376995086669922
Epoch 150, training loss: 1.6970034837722778 = 1.6892445087432861 + 0.001 * 7.758934020996094
Epoch 150, val loss: 1.7096655368804932
Epoch 160, training loss: 1.6577268838882446 = 1.65016770362854 + 0.001 * 7.559144020080566
Epoch 160, val loss: 1.6758685111999512
Epoch 170, training loss: 1.6127997636795044 = 1.6054080724716187 + 0.001 * 7.391668319702148
Epoch 170, val loss: 1.6371086835861206
Epoch 180, training loss: 1.563716173171997 = 1.5564486980438232 + 0.001 * 7.2674713134765625
Epoch 180, val loss: 1.5950239896774292
Epoch 190, training loss: 1.512510895729065 = 1.5053075551986694 + 0.001 * 7.203321933746338
Epoch 190, val loss: 1.551862120628357
Epoch 200, training loss: 1.4609066247940063 = 1.45372474193573 + 0.001 * 7.181881904602051
Epoch 200, val loss: 1.5094727277755737
Epoch 210, training loss: 1.4099338054656982 = 1.402768850326538 + 0.001 * 7.164997100830078
Epoch 210, val loss: 1.4688224792480469
Epoch 220, training loss: 1.3599363565444946 = 1.352785348892212 + 0.001 * 7.150954723358154
Epoch 220, val loss: 1.4300222396850586
Epoch 230, training loss: 1.310749888420105 = 1.3036068677902222 + 0.001 * 7.143022060394287
Epoch 230, val loss: 1.392751932144165
Epoch 240, training loss: 1.262205958366394 = 1.2550675868988037 + 0.001 * 7.138329982757568
Epoch 240, val loss: 1.3567109107971191
Epoch 250, training loss: 1.2142255306243896 = 1.207091212272644 + 0.001 * 7.1342997550964355
Epoch 250, val loss: 1.3215018510818481
Epoch 260, training loss: 1.1667892932891846 = 1.1596596240997314 + 0.001 * 7.129638671875
Epoch 260, val loss: 1.286731481552124
Epoch 270, training loss: 1.1200602054595947 = 1.11293625831604 + 0.001 * 7.124000072479248
Epoch 270, val loss: 1.2522908449172974
Epoch 280, training loss: 1.074151873588562 = 1.0670349597930908 + 0.001 * 7.1169514656066895
Epoch 280, val loss: 1.2182202339172363
Epoch 290, training loss: 1.0291415452957153 = 1.0220333337783813 + 0.001 * 7.108199119567871
Epoch 290, val loss: 1.1843831539154053
Epoch 300, training loss: 0.9851212501525879 = 0.9780236482620239 + 0.001 * 7.097578048706055
Epoch 300, val loss: 1.150839924812317
Epoch 310, training loss: 0.942177951335907 = 0.9350927472114563 + 0.001 * 7.085214614868164
Epoch 310, val loss: 1.1175997257232666
Epoch 320, training loss: 0.9004219770431519 = 0.89335036277771 + 0.001 * 7.071625232696533
Epoch 320, val loss: 1.084842324256897
Epoch 330, training loss: 0.8599933981895447 = 0.8529355525970459 + 0.001 * 7.057849884033203
Epoch 330, val loss: 1.0528655052185059
Epoch 340, training loss: 0.8210245966911316 = 0.8139784336090088 + 0.001 * 7.046163558959961
Epoch 340, val loss: 1.0217643976211548
Epoch 350, training loss: 0.7836311459541321 = 0.776593029499054 + 0.001 * 7.038129806518555
Epoch 350, val loss: 0.9920009970664978
Epoch 360, training loss: 0.7478759288787842 = 0.7408444881439209 + 0.001 * 7.03141450881958
Epoch 360, val loss: 0.9637237787246704
Epoch 370, training loss: 0.7137451767921448 = 0.7067179679870605 + 0.001 * 7.027214527130127
Epoch 370, val loss: 0.9371649622917175
Epoch 380, training loss: 0.6811524629592896 = 0.6741279363632202 + 0.001 * 7.024518013000488
Epoch 380, val loss: 0.9123528599739075
Epoch 390, training loss: 0.6499606966972351 = 0.6429376602172852 + 0.001 * 7.023019790649414
Epoch 390, val loss: 0.8893270492553711
Epoch 400, training loss: 0.6200262308120728 = 0.613003671169281 + 0.001 * 7.022580623626709
Epoch 400, val loss: 0.8680223822593689
Epoch 410, training loss: 0.5912281274795532 = 0.5842064619064331 + 0.001 * 7.021660327911377
Epoch 410, val loss: 0.8484103083610535
Epoch 420, training loss: 0.5634572505950928 = 0.55643630027771 + 0.001 * 7.020979404449463
Epoch 420, val loss: 0.8303956985473633
Epoch 430, training loss: 0.5366219282150269 = 0.5296017527580261 + 0.001 * 7.02019739151001
Epoch 430, val loss: 0.8139452338218689
Epoch 440, training loss: 0.5106644630432129 = 0.5036436915397644 + 0.001 * 7.020751476287842
Epoch 440, val loss: 0.7990008592605591
Epoch 450, training loss: 0.4855123460292816 = 0.4784928858280182 + 0.001 * 7.019448280334473
Epoch 450, val loss: 0.7853939533233643
Epoch 460, training loss: 0.4611131250858307 = 0.4540942907333374 + 0.001 * 7.018826484680176
Epoch 460, val loss: 0.7730517387390137
Epoch 470, training loss: 0.43741336464881897 = 0.4303949773311615 + 0.001 * 7.018388748168945
Epoch 470, val loss: 0.7618573307991028
Epoch 480, training loss: 0.41437670588493347 = 0.4073585569858551 + 0.001 * 7.018155574798584
Epoch 480, val loss: 0.7516642212867737
Epoch 490, training loss: 0.39198485016822815 = 0.38496553897857666 + 0.001 * 7.0192975997924805
Epoch 490, val loss: 0.7423928380012512
Epoch 500, training loss: 0.37023448944091797 = 0.363217294216156 + 0.001 * 7.017209529876709
Epoch 500, val loss: 0.733963131904602
Epoch 510, training loss: 0.3491513133049011 = 0.34213438630104065 + 0.001 * 7.016940593719482
Epoch 510, val loss: 0.7263250946998596
Epoch 520, training loss: 0.32877057790756226 = 0.3217540681362152 + 0.001 * 7.0165205001831055
Epoch 520, val loss: 0.7194589972496033
Epoch 530, training loss: 0.30913907289505005 = 0.30212289094924927 + 0.001 * 7.016170978546143
Epoch 530, val loss: 0.713373064994812
Epoch 540, training loss: 0.29030755162239075 = 0.28329017758369446 + 0.001 * 7.017369747161865
Epoch 540, val loss: 0.708084762096405
Epoch 550, training loss: 0.27232062816619873 = 0.26530423760414124 + 0.001 * 7.01638126373291
Epoch 550, val loss: 0.703583836555481
Epoch 560, training loss: 0.25522512197494507 = 0.24820978939533234 + 0.001 * 7.0153326988220215
Epoch 560, val loss: 0.6999165415763855
Epoch 570, training loss: 0.23905245959758759 = 0.23203697800636292 + 0.001 * 7.015476226806641
Epoch 570, val loss: 0.6970533132553101
Epoch 580, training loss: 0.2238166183233261 = 0.21680213510990143 + 0.001 * 7.014482498168945
Epoch 580, val loss: 0.6950258016586304
Epoch 590, training loss: 0.20952296257019043 = 0.2025081217288971 + 0.001 * 7.014845848083496
Epoch 590, val loss: 0.6937861442565918
Epoch 600, training loss: 0.19615834951400757 = 0.18914340436458588 + 0.001 * 7.014939785003662
Epoch 600, val loss: 0.6933170557022095
Epoch 610, training loss: 0.1836928129196167 = 0.1766786426305771 + 0.001 * 7.01417350769043
Epoch 610, val loss: 0.6935560703277588
Epoch 620, training loss: 0.1720886379480362 = 0.1650756299495697 + 0.001 * 7.013003826141357
Epoch 620, val loss: 0.6944534182548523
Epoch 630, training loss: 0.16130349040031433 = 0.1542903184890747 + 0.001 * 7.013172149658203
Epoch 630, val loss: 0.6959962248802185
Epoch 640, training loss: 0.15128746628761292 = 0.14427584409713745 + 0.001 * 7.011627674102783
Epoch 640, val loss: 0.6981234550476074
Epoch 650, training loss: 0.14199432730674744 = 0.1349833458662033 + 0.001 * 7.010985851287842
Epoch 650, val loss: 0.7007637023925781
Epoch 660, training loss: 0.13337509334087372 = 0.126363143324852 + 0.001 * 7.0119452476501465
Epoch 660, val loss: 0.7038779854774475
Epoch 670, training loss: 0.12537389993667603 = 0.11836595088243484 + 0.001 * 7.007942199707031
Epoch 670, val loss: 0.7074224948883057
Epoch 680, training loss: 0.11795109510421753 = 0.11094547063112259 + 0.001 * 7.005624294281006
Epoch 680, val loss: 0.7113494277000427
Epoch 690, training loss: 0.11107219010591507 = 0.10405943542718887 + 0.001 * 7.012753486633301
Epoch 690, val loss: 0.7155971527099609
Epoch 700, training loss: 0.10467333346605301 = 0.0976669117808342 + 0.001 * 7.006420135498047
Epoch 700, val loss: 0.7201434969902039
Epoch 710, training loss: 0.09873135387897491 = 0.09172990173101425 + 0.001 * 7.001451015472412
Epoch 710, val loss: 0.724966824054718
Epoch 720, training loss: 0.09321113675832748 = 0.0862131342291832 + 0.001 * 6.99799919128418
Epoch 720, val loss: 0.7299967408180237
Epoch 730, training loss: 0.08807960897684097 = 0.08108408004045486 + 0.001 * 6.995525360107422
Epoch 730, val loss: 0.7352089881896973
Epoch 740, training loss: 0.08330895006656647 = 0.07631343603134155 + 0.001 * 6.995517253875732
Epoch 740, val loss: 0.7405999898910522
Epoch 750, training loss: 0.07887115329504013 = 0.07187403738498688 + 0.001 * 6.99711799621582
Epoch 750, val loss: 0.7461163997650146
Epoch 760, training loss: 0.07472866028547287 = 0.06774085015058517 + 0.001 * 6.98780632019043
Epoch 760, val loss: 0.7517504096031189
Epoch 770, training loss: 0.0708736777305603 = 0.06389172375202179 + 0.001 * 6.9819560050964355
Epoch 770, val loss: 0.7574907541275024
Epoch 780, training loss: 0.0672854483127594 = 0.06030576676130295 + 0.001 * 6.979683876037598
Epoch 780, val loss: 0.7633169889450073
Epoch 790, training loss: 0.0639374703168869 = 0.05696389451622963 + 0.001 * 6.973578453063965
Epoch 790, val loss: 0.7691970467567444
Epoch 800, training loss: 0.06081855297088623 = 0.05384838953614235 + 0.001 * 6.970161437988281
Epoch 800, val loss: 0.7751547694206238
Epoch 810, training loss: 0.05790619179606438 = 0.05094247683882713 + 0.001 * 6.963714122772217
Epoch 810, val loss: 0.7811530828475952
Epoch 820, training loss: 0.055212944746017456 = 0.04823102802038193 + 0.001 * 6.981915473937988
Epoch 820, val loss: 0.7871699333190918
Epoch 830, training loss: 0.05265894532203674 = 0.04570021852850914 + 0.001 * 6.958724498748779
Epoch 830, val loss: 0.7932319641113281
Epoch 840, training loss: 0.050310224294662476 = 0.043337058275938034 + 0.001 * 6.973165988922119
Epoch 840, val loss: 0.7993003726005554
Epoch 850, training loss: 0.04808260127902031 = 0.04112963750958443 + 0.001 * 6.9529619216918945
Epoch 850, val loss: 0.805393636226654
Epoch 860, training loss: 0.046005334705114365 = 0.03906658664345741 + 0.001 * 6.938747406005859
Epoch 860, val loss: 0.8114655017852783
Epoch 870, training loss: 0.04409511759877205 = 0.03713732957839966 + 0.001 * 6.957788467407227
Epoch 870, val loss: 0.8175302147865295
Epoch 880, training loss: 0.0422675795853138 = 0.035332322120666504 + 0.001 * 6.935255527496338
Epoch 880, val loss: 0.8235693573951721
Epoch 890, training loss: 0.04057793319225311 = 0.03364218771457672 + 0.001 * 6.935745716094971
Epoch 890, val loss: 0.8295765519142151
Epoch 900, training loss: 0.03899121284484863 = 0.03205855190753937 + 0.001 * 6.932660102844238
Epoch 900, val loss: 0.8355494737625122
Epoch 910, training loss: 0.037511419504880905 = 0.030573831871151924 + 0.001 * 6.937588691711426
Epoch 910, val loss: 0.8414775729179382
Epoch 920, training loss: 0.03611566126346588 = 0.029180938377976418 + 0.001 * 6.934720993041992
Epoch 920, val loss: 0.847353458404541
Epoch 930, training loss: 0.034790992736816406 = 0.027873247861862183 + 0.001 * 6.917743682861328
Epoch 930, val loss: 0.8531854152679443
Epoch 940, training loss: 0.033561013638973236 = 0.026644714176654816 + 0.001 * 6.916299819946289
Epoch 940, val loss: 0.8589659929275513
Epoch 950, training loss: 0.03239651024341583 = 0.02548970840871334 + 0.001 * 6.906802177429199
Epoch 950, val loss: 0.8646742105484009
Epoch 960, training loss: 0.03133298456668854 = 0.024402931332588196 + 0.001 * 6.930052280426025
Epoch 960, val loss: 0.8703423142433167
Epoch 970, training loss: 0.03028983250260353 = 0.02337963506579399 + 0.001 * 6.9101972579956055
Epoch 970, val loss: 0.8759469389915466
Epoch 980, training loss: 0.029324961826205254 = 0.022415319457650185 + 0.001 * 6.909642696380615
Epoch 980, val loss: 0.8814830780029297
Epoch 990, training loss: 0.02840658649802208 = 0.02150597982108593 + 0.001 * 6.900606632232666
Epoch 990, val loss: 0.8869588971138
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 1.9685436487197876 = 1.9599467515945435 + 0.001 * 8.596856117248535
Epoch 0, val loss: 1.9603606462478638
Epoch 10, training loss: 1.963132619857788 = 1.954535722732544 + 0.001 * 8.596846580505371
Epoch 10, val loss: 1.9552329778671265
Epoch 20, training loss: 1.9573391675949097 = 1.948742389678955 + 0.001 * 8.59680461883545
Epoch 20, val loss: 1.9496263265609741
Epoch 30, training loss: 1.9505385160446167 = 1.941941738128662 + 0.001 * 8.59672737121582
Epoch 30, val loss: 1.9429527521133423
Epoch 40, training loss: 1.9421523809432983 = 1.9335558414459229 + 0.001 * 8.596595764160156
Epoch 40, val loss: 1.934640645980835
Epoch 50, training loss: 1.9314656257629395 = 1.9228692054748535 + 0.001 * 8.596369743347168
Epoch 50, val loss: 1.92399001121521
Epoch 60, training loss: 1.9176621437072754 = 1.9090662002563477 + 0.001 * 8.595967292785645
Epoch 60, val loss: 1.9102152585983276
Epoch 70, training loss: 1.8999810218811035 = 1.891385793685913 + 0.001 * 8.595227241516113
Epoch 70, val loss: 1.8926547765731812
Epoch 80, training loss: 1.8780639171600342 = 1.8694701194763184 + 0.001 * 8.59375
Epoch 80, val loss: 1.8711493015289307
Epoch 90, training loss: 1.8522919416427612 = 1.8437014818191528 + 0.001 * 8.590510368347168
Epoch 90, val loss: 1.8465547561645508
Epoch 100, training loss: 1.824376106262207 = 1.8157933950424194 + 0.001 * 8.582704544067383
Epoch 100, val loss: 1.8209458589553833
Epoch 110, training loss: 1.7970497608184814 = 1.788487195968628 + 0.001 * 8.562529563903809
Epoch 110, val loss: 1.7969807386398315
Epoch 120, training loss: 1.7714263200759888 = 1.762920618057251 + 0.001 * 8.505708694458008
Epoch 120, val loss: 1.7750276327133179
Epoch 130, training loss: 1.7447487115859985 = 1.7364439964294434 + 0.001 * 8.304679870605469
Epoch 130, val loss: 1.7519054412841797
Epoch 140, training loss: 1.7136917114257812 = 1.7057082653045654 + 0.001 * 7.983408451080322
Epoch 140, val loss: 1.724834680557251
Epoch 150, training loss: 1.6769416332244873 = 1.6690475940704346 + 0.001 * 7.894017696380615
Epoch 150, val loss: 1.6928859949111938
Epoch 160, training loss: 1.6337116956710815 = 1.6259305477142334 + 0.001 * 7.781106948852539
Epoch 160, val loss: 1.6558274030685425
Epoch 170, training loss: 1.5839948654174805 = 1.576333999633789 + 0.001 * 7.660909652709961
Epoch 170, val loss: 1.613616704940796
Epoch 180, training loss: 1.5290672779083252 = 1.521510124206543 + 0.001 * 7.55717134475708
Epoch 180, val loss: 1.5671364068984985
Epoch 190, training loss: 1.471279263496399 = 1.4637854099273682 + 0.001 * 7.493815898895264
Epoch 190, val loss: 1.5181502103805542
Epoch 200, training loss: 1.4123101234436035 = 1.4048515558242798 + 0.001 * 7.458539962768555
Epoch 200, val loss: 1.4683253765106201
Epoch 210, training loss: 1.3534879684448242 = 1.3460620641708374 + 0.001 * 7.425867557525635
Epoch 210, val loss: 1.4189058542251587
Epoch 220, training loss: 1.295578122138977 = 1.2881834506988525 + 0.001 * 7.394679546356201
Epoch 220, val loss: 1.37028169631958
Epoch 230, training loss: 1.238963007926941 = 1.231598973274231 + 0.001 * 7.363986492156982
Epoch 230, val loss: 1.3231217861175537
Epoch 240, training loss: 1.1837445497512817 = 1.1764066219329834 + 0.001 * 7.3379411697387695
Epoch 240, val loss: 1.2777656316757202
Epoch 250, training loss: 1.1299911737442017 = 1.122672438621521 + 0.001 * 7.318786144256592
Epoch 250, val loss: 1.2342077493667603
Epoch 260, training loss: 1.0778350830078125 = 1.0705302953720093 + 0.001 * 7.304842948913574
Epoch 260, val loss: 1.1924474239349365
Epoch 270, training loss: 1.0273237228393555 = 1.0200321674346924 + 0.001 * 7.291546821594238
Epoch 270, val loss: 1.1524978876113892
Epoch 280, training loss: 0.9785033464431763 = 0.9712260365486145 + 0.001 * 7.2773261070251465
Epoch 280, val loss: 1.1140974760055542
Epoch 290, training loss: 0.931532084941864 = 0.9242689609527588 + 0.001 * 7.2631516456604
Epoch 290, val loss: 1.0775787830352783
Epoch 300, training loss: 0.8866109848022461 = 0.8793607950210571 + 0.001 * 7.250185966491699
Epoch 300, val loss: 1.042787790298462
Epoch 310, training loss: 0.8439981341362 = 0.8367591500282288 + 0.001 * 7.238974094390869
Epoch 310, val loss: 1.010056495666504
Epoch 320, training loss: 0.8038749098777771 = 0.7966461181640625 + 0.001 * 7.22879695892334
Epoch 320, val loss: 0.9795364737510681
Epoch 330, training loss: 0.766299843788147 = 0.7590813636779785 + 0.001 * 7.218505859375
Epoch 330, val loss: 0.9513859748840332
Epoch 340, training loss: 0.7311831712722778 = 0.7239760756492615 + 0.001 * 7.207096576690674
Epoch 340, val loss: 0.9255648255348206
Epoch 350, training loss: 0.6983014941215515 = 0.6911068558692932 + 0.001 * 7.194664478302002
Epoch 350, val loss: 0.9020821452140808
Epoch 360, training loss: 0.6673495769500732 = 0.660167932510376 + 0.001 * 7.181624412536621
Epoch 360, val loss: 0.8807263970375061
Epoch 370, training loss: 0.6380133032798767 = 0.6308448314666748 + 0.001 * 7.168464183807373
Epoch 370, val loss: 0.8613106608390808
Epoch 380, training loss: 0.6100192666053772 = 0.6028643250465393 + 0.001 * 7.154922962188721
Epoch 380, val loss: 0.8437209129333496
Epoch 390, training loss: 0.583154022693634 = 0.5760111212730408 + 0.001 * 7.142888069152832
Epoch 390, val loss: 0.8277148604393005
Epoch 400, training loss: 0.5572559833526611 = 0.5501250624656677 + 0.001 * 7.1308979988098145
Epoch 400, val loss: 0.813186764717102
Epoch 410, training loss: 0.5322305560112 = 0.5251093506813049 + 0.001 * 7.121191501617432
Epoch 410, val loss: 0.7999506592750549
Epoch 420, training loss: 0.5080270767211914 = 0.5009124875068665 + 0.001 * 7.114570617675781
Epoch 420, val loss: 0.7879619002342224
Epoch 430, training loss: 0.4846085011959076 = 0.4775002598762512 + 0.001 * 7.108245372772217
Epoch 430, val loss: 0.777126133441925
Epoch 440, training loss: 0.4619612395763397 = 0.4548608362674713 + 0.001 * 7.100390434265137
Epoch 440, val loss: 0.7674292325973511
Epoch 450, training loss: 0.44006550312042236 = 0.43297138810157776 + 0.001 * 7.094106197357178
Epoch 450, val loss: 0.7588332295417786
Epoch 460, training loss: 0.4189002811908722 = 0.41180747747421265 + 0.001 * 7.092790603637695
Epoch 460, val loss: 0.7513155341148376
Epoch 470, training loss: 0.3984206020832062 = 0.3913344740867615 + 0.001 * 7.086124897003174
Epoch 470, val loss: 0.74482262134552
Epoch 480, training loss: 0.37860167026519775 = 0.3715217709541321 + 0.001 * 7.079894065856934
Epoch 480, val loss: 0.7392722964286804
Epoch 490, training loss: 0.3594164550304413 = 0.35234057903289795 + 0.001 * 7.075873374938965
Epoch 490, val loss: 0.7345650792121887
Epoch 500, training loss: 0.3408524692058563 = 0.3337809145450592 + 0.001 * 7.071552753448486
Epoch 500, val loss: 0.7306665778160095
Epoch 510, training loss: 0.3229129910469055 = 0.31584617495536804 + 0.001 * 7.066807270050049
Epoch 510, val loss: 0.727539598941803
Epoch 520, training loss: 0.30562373995780945 = 0.29856038093566895 + 0.001 * 7.06334924697876
Epoch 520, val loss: 0.7251327633857727
Epoch 530, training loss: 0.28901469707489014 = 0.28195226192474365 + 0.001 * 7.06243371963501
Epoch 530, val loss: 0.7234523296356201
Epoch 540, training loss: 0.2731102406978607 = 0.2660537660121918 + 0.001 * 7.0564703941345215
Epoch 540, val loss: 0.7225081920623779
Epoch 550, training loss: 0.2579471170902252 = 0.2508925199508667 + 0.001 * 7.05458927154541
Epoch 550, val loss: 0.7222492694854736
Epoch 560, training loss: 0.24354040622711182 = 0.23648953437805176 + 0.001 * 7.0508713722229
Epoch 560, val loss: 0.7226807475090027
Epoch 570, training loss: 0.22990572452545166 = 0.22286075353622437 + 0.001 * 7.044970989227295
Epoch 570, val loss: 0.7237873673439026
Epoch 580, training loss: 0.21704712510108948 = 0.2100069671869278 + 0.001 * 7.040158748626709
Epoch 580, val loss: 0.7255657911300659
Epoch 590, training loss: 0.20495949685573578 = 0.19791723787784576 + 0.001 * 7.0422539710998535
Epoch 590, val loss: 0.7279980182647705
Epoch 600, training loss: 0.19361735880374908 = 0.18657395243644714 + 0.001 * 7.0434041023254395
Epoch 600, val loss: 0.7310551404953003
Epoch 610, training loss: 0.18298611044883728 = 0.1759525090456009 + 0.001 * 7.033596038818359
Epoch 610, val loss: 0.734709620475769
Epoch 620, training loss: 0.173047736287117 = 0.16602066159248352 + 0.001 * 7.027076721191406
Epoch 620, val loss: 0.7389093041419983
Epoch 630, training loss: 0.16376255452632904 = 0.15673953294754028 + 0.001 * 7.023014545440674
Epoch 630, val loss: 0.7435898184776306
Epoch 640, training loss: 0.15512478351593018 = 0.14807134866714478 + 0.001 * 7.053436756134033
Epoch 640, val loss: 0.7487200498580933
Epoch 650, training loss: 0.14699608087539673 = 0.13997553288936615 + 0.001 * 7.020543098449707
Epoch 650, val loss: 0.7542365193367004
Epoch 660, training loss: 0.13942505419254303 = 0.13241106271743774 + 0.001 * 7.0139946937561035
Epoch 660, val loss: 0.7600866556167603
Epoch 670, training loss: 0.1323520541191101 = 0.12533846497535706 + 0.001 * 7.013582229614258
Epoch 670, val loss: 0.7662239074707031
Epoch 680, training loss: 0.12572833895683289 = 0.11871936917304993 + 0.001 * 7.00896692276001
Epoch 680, val loss: 0.7725990414619446
Epoch 690, training loss: 0.11953085660934448 = 0.11251579225063324 + 0.001 * 7.015066623687744
Epoch 690, val loss: 0.7791832685470581
Epoch 700, training loss: 0.11369989812374115 = 0.10669323056936264 + 0.001 * 7.0066657066345215
Epoch 700, val loss: 0.7859324216842651
Epoch 710, training loss: 0.10822143405675888 = 0.10122153908014297 + 0.001 * 6.999893665313721
Epoch 710, val loss: 0.7928224205970764
Epoch 720, training loss: 0.10306898504495621 = 0.09607226401567459 + 0.001 * 6.996723175048828
Epoch 720, val loss: 0.7998468279838562
Epoch 730, training loss: 0.09823434054851532 = 0.09122315794229507 + 0.001 * 7.01118278503418
Epoch 730, val loss: 0.8070017695426941
Epoch 740, training loss: 0.09365137666463852 = 0.08665340393781662 + 0.001 * 6.9979729652404785
Epoch 740, val loss: 0.814280092716217
Epoch 750, training loss: 0.08933170884847641 = 0.08234427124261856 + 0.001 * 6.987438201904297
Epoch 750, val loss: 0.8216384649276733
Epoch 760, training loss: 0.08528119325637817 = 0.07827959209680557 + 0.001 * 7.001596927642822
Epoch 760, val loss: 0.829064130783081
Epoch 770, training loss: 0.0814296305179596 = 0.07444467395544052 + 0.001 * 6.984957695007324
Epoch 770, val loss: 0.8365378975868225
Epoch 780, training loss: 0.0778110921382904 = 0.0708257332444191 + 0.001 * 6.985357284545898
Epoch 780, val loss: 0.8440343141555786
Epoch 790, training loss: 0.07439734041690826 = 0.06740956753492355 + 0.001 * 6.987771987915039
Epoch 790, val loss: 0.8515522480010986
Epoch 800, training loss: 0.0711677074432373 = 0.0641840249300003 + 0.001 * 6.983684539794922
Epoch 800, val loss: 0.8590789437294006
Epoch 810, training loss: 0.06812813133001328 = 0.06113732233643532 + 0.001 * 6.990807056427002
Epoch 810, val loss: 0.8666096329689026
Epoch 820, training loss: 0.06523836404085159 = 0.058258794248104095 + 0.001 * 6.979572772979736
Epoch 820, val loss: 0.8741413354873657
Epoch 830, training loss: 0.06251015514135361 = 0.055539123713970184 + 0.001 * 6.971034526824951
Epoch 830, val loss: 0.8816595673561096
Epoch 840, training loss: 0.059952493757009506 = 0.052968814969062805 + 0.001 * 6.98367977142334
Epoch 840, val loss: 0.8891533017158508
Epoch 850, training loss: 0.05751562491059303 = 0.05053914710879326 + 0.001 * 6.976478099822998
Epoch 850, val loss: 0.8966217041015625
Epoch 860, training loss: 0.05521327257156372 = 0.04824206978082657 + 0.001 * 6.971203327178955
Epoch 860, val loss: 0.9040452241897583
Epoch 870, training loss: 0.05304142087697983 = 0.046069707721471786 + 0.001 * 6.971712589263916
Epoch 870, val loss: 0.9114363789558411
Epoch 880, training loss: 0.05097975581884384 = 0.044014766812324524 + 0.001 * 6.964990615844727
Epoch 880, val loss: 0.9187682271003723
Epoch 890, training loss: 0.04906182736158371 = 0.04207057133316994 + 0.001 * 6.991253852844238
Epoch 890, val loss: 0.9260550141334534
Epoch 900, training loss: 0.04719507694244385 = 0.0402311272919178 + 0.001 * 6.963947296142578
Epoch 900, val loss: 0.9332828521728516
Epoch 910, training loss: 0.04545183852314949 = 0.038490284234285355 + 0.001 * 6.96155309677124
Epoch 910, val loss: 0.940435528755188
Epoch 920, training loss: 0.0438014380633831 = 0.03684229403734207 + 0.001 * 6.959144115447998
Epoch 920, val loss: 0.9475129842758179
Epoch 930, training loss: 0.0422392413020134 = 0.03528178110718727 + 0.001 * 6.957460880279541
Epoch 930, val loss: 0.9545260071754456
Epoch 940, training loss: 0.04077409952878952 = 0.03380347788333893 + 0.001 * 6.9706220626831055
Epoch 940, val loss: 0.961474597454071
Epoch 950, training loss: 0.03936291113495827 = 0.032402776181697845 + 0.001 * 6.960134029388428
Epoch 950, val loss: 0.9683430790901184
Epoch 960, training loss: 0.03802965581417084 = 0.031075159087777138 + 0.001 * 6.954495429992676
Epoch 960, val loss: 0.9751362204551697
Epoch 970, training loss: 0.036777615547180176 = 0.02981659770011902 + 0.001 * 6.961019515991211
Epoch 970, val loss: 0.9818502068519592
Epoch 980, training loss: 0.035579465329647064 = 0.028623059391975403 + 0.001 * 6.956406593322754
Epoch 980, val loss: 0.9884870648384094
Epoch 990, training loss: 0.03444965183734894 = 0.027490777894854546 + 0.001 * 6.958871841430664
Epoch 990, val loss: 0.995041012763977
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8429098576700054
The final CL Acc:0.81481, 0.00302, The final GNN Acc:0.83887, 0.00287
